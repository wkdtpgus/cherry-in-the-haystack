"""
테스트용 샘플 텍스트
LLM 관련 책/논문에서 발췌한 문단들 (한국어)
"""

SAMPLE_PARAGRAPHS = [
    # 1. Attention 개념
    {
        "text": """
어텐션 메커니즘은 모델이 출력의 각 요소를 생성할 때 입력 시퀀스의 서로 다른 부분에
집중할 수 있게 해주는 핵심적인 신경망 구성 요소다. 전통적인 시퀀스-투-시퀀스 모델이
전체 입력을 고정 길이 벡터로 압축하는 것과 달리, 어텐션은 디코더가 모든 인코더 은닉
상태를 되돌아보고 각 디코딩 단계에서 가장 관련성 높은 부분에 선택적으로 집중할 수
있게 한다. 이러한 메커니즘은 특히 긴 시퀀스를 처리할 때 정보 손실을 방지하는 데
효과적이다. 어텐션 스코어는 쿼리(Query)와 키(Key) 벡터 간의 유사도를 계산하여
산출되며, 이 스코어를 소프트맥스 함수를 통해 정규화한 후 밸류(Value) 벡터와 가중
합산하여 최종 출력을 생성한다. 이 과정에서 모델은 입력의 어느 부분이 현재 출력에
가장 중요한지를 학습하게 되며, 이는 기계 번역, 텍스트 요약, 질의응답 등 다양한
자연어 처리 태스크에서 성능 향상에 크게 기여했다. 셀프 어텐션(Self-Attention)은
이 개념을 확장하여 동일한 시퀀스 내의 요소들 간의 관계를 모델링하는 데 사용된다.
""".strip(),
        "source": {"book_id": "llm-basics", "book_title": "LLM 기초", "page": 45, "paragraph": 2}
    },

    # 2. Transformer 정의
    {
        "text": """
트랜스포머는 순환(recurrence)과 합성곱(convolution)을 완전히 배제하고 셀프 어텐션
메커니즘에만 전적으로 의존하는 혁신적인 신경망 아키텍처다. 2017년 Vaswani 등이
"Attention Is All You Need" 논문에서 처음 소개했으며, 이후 GPT, BERT, T5, LLaMA 등
대부분의 현대 대형 언어 모델의 기반이 되었다. 트랜스포머 아키텍처는 인코더와 디코더
스택으로 구성되며, 각 레이어는 멀티헤드 셀프 어텐션(Multi-Head Self-Attention)과
위치별 피드포워드 네트워크(Position-wise Feed-Forward Network)로 이루어져 있다.
멀티헤드 어텐션은 여러 개의 어텐션 헤드를 병렬로 실행하여 입력의 다양한 측면을
동시에 포착할 수 있게 해준다. 또한 잔차 연결(Residual Connection)과 레이어 정규화
(Layer Normalization)를 적용하여 깊은 네트워크의 학습 안정성을 확보한다. 위치
정보는 사인/코사인 함수 기반의 포지셔널 인코딩(Positional Encoding)을 통해 주입되어,
순환 구조 없이도 시퀀스의 순서 정보를 모델이 인식할 수 있게 한다. 이 아키텍처의
병렬화 가능성은 GPU를 활용한 대규모 학습을 가능하게 하여 현재의 LLM 혁명을 이끌었다.
""".strip(),
        "source": {"book_id": "transformer-guide", "book_title": "트랜스포머 가이드", "page": 12, "paragraph": 1}
    },

    # 3. LoRA 기법
    {
        "text": """
LoRA(Low-Rank Adaptation)는 사전학습된 모델의 가중치를 동결하고 트랜스포머 아키텍처의
각 레이어에 학습 가능한 저순위 분해 행렬을 주입하는 파라미터 효율적 파인튜닝 기법이다.
기존의 전체 파인튜닝(Full Fine-tuning)이 모델의 모든 파라미터를 업데이트해야 하는
반면, LoRA는 원본 가중치 행렬 W에 대해 W + BA 형태의 저순위 업데이트만 학습한다.
여기서 B와 A는 각각 d×r과 r×k 차원의 행렬이며, r(랭크)은 보통 4~64 정도의 작은
값으로 설정된다. 이 접근법은 학습 가능한 파라미터 수를 10,000배 이상, GPU 메모리
요구량을 3배 이상 줄이면서도 전체 파인튜닝과 비슷하거나 더 나은 성능을 달성할 수
있다. LoRA의 또 다른 장점은 태스크별로 서로 다른 어댑터를 학습하여 하나의 기본
모델에서 여러 태스크를 처리할 수 있다는 것이다. 추론 시에는 학습된 BA를 원본
가중치에 병합하여 추가적인 지연 없이 사용할 수 있다. QLoRA는 LoRA에 4비트 양자화를
결합하여 메모리 효율성을 더욱 높인 변형이며, 이를 통해 소비자용 GPU에서도 대형
언어 모델의 파인튜닝이 가능해졌다.
""".strip(),
        "source": {"book_id": "peft-handbook", "book_title": "PEFT 핸드북", "page": 78, "paragraph": 3}
    },

    # 4. 프롬프트 엔지니어링
    {
        "text": """
프롬프트 엔지니어링은 대형 언어 모델에서 원하는 출력을 얻기 위해 입력 프롬프트를
설계하고 최적화하는 기술이자 새롭게 부상하는 학문 분야다. 효과적인 프롬프트는 명확한
지시사항, 관련 컨텍스트, 예시(few-shot learning), 그리고 출력 형식 지정을 포함한다.
제로샷(Zero-shot) 프롬프팅은 예시 없이 직접 태스크를 수행하도록 요청하는 방식이고,
퓨샷(Few-shot) 프롬프팅은 몇 가지 입출력 예시를 제공하여 모델이 패턴을 학습하도록
유도하는 방식이다. Chain-of-Thought(CoT) 프롬프팅은 복잡한 추론 문제를 해결할 때
중간 사고 과정을 명시적으로 생성하도록 유도하여 정확도를 크게 향상시킨다. 잘 설계된
프롬프트는 모델의 파인튜닝 없이도 성능을 크게 향상시킬 수 있으며, 이는 특히 API
기반으로 제공되는 폐쇄형 모델을 사용할 때 유일한 커스터마이징 방법이 된다. 프롬프트
템플릿, 역할 부여(role-playing), 출력 형식 제약 등 다양한 기법이 실무에서 활용되며,
최근에는 자동 프롬프트 최적화(Automatic Prompt Optimization)와 같이 프롬프트 자체를
자동으로 개선하는 연구도 활발히 진행되고 있다.
""".strip(),
        "source": {"book_id": "prompt-engineering", "book_title": "프롬프트 엔지니어링", "page": 23, "paragraph": 1}
    },

    # 5. RAG (Retrieval-Augmented Generation)
    {
        "text": """
RAG(Retrieval-Augmented Generation, 검색 증강 생성)는 대형 언어 모델의 생성 능력과
외부 지식 검색을 결합한 하이브리드 아키텍처다. 순수 생성 모델이 학습 데이터에만
의존하여 환각(hallucination)이나 outdated 정보 문제를 겪는 반면, RAG는 실시간으로
관련 문서를 검색하여 이러한 한계를 극복한다. RAG 파이프라인은 크게 인덱싱, 검색,
생성의 세 단계로 구성된다. 인덱싱 단계에서는 문서를 청크로 분할하고 임베딩 모델을
통해 벡터로 변환한 후 벡터 데이터베이스에 저장한다. 검색 단계에서는 사용자 질의를
동일한 임베딩 모델로 벡터화하여 유사도 검색을 수행하고, 가장 관련성 높은 문서들을
추출한다. 생성 단계에서는 검색된 문서들을 컨텍스트로 프롬프트에 포함시켜 LLM이
이를 참조하여 응답을 생성하도록 한다. 이 아키텍처의 장점은 모델 재학습 없이
지식베이스를 업데이트할 수 있고, 출처를 명시하여 답변의 신뢰성을 높일 수 있다는
것이다. 최근에는 하이브리드 검색, 리랭킹, 쿼리 확장 등 다양한 기법이 RAG 성능을
향상시키기 위해 연구되고 있다.
""".strip(),
        "source": {"book_id": "rag-patterns", "book_title": "RAG 패턴", "page": 56, "paragraph": 2}
    },

    # 6. 평가 주도 개발 (EvDD)
    {
        "text": """
평가 주도 개발(Evaluation-Driven Development, EvDD)은 AI 애플리케이션을 구현하기 전에
평가 기준을 먼저 정의하는 체계적인 개발 방법론이다. 소프트웨어 공학의 테스트 주도
개발(TDD)과 유사하게, EvDD는 먼저 성공 기준과 평가 메트릭을 설정한 후 이를 충족하는
방향으로 개발을 진행한다. LLM 애플리케이션은 출력이 비결정적(non-deterministic)이고
품질 평가가 주관적일 수 있어 전통적인 단위 테스트만으로는 품질 보장이 어렵다.
EvDD는 이러한 특성을 고려하여 정량적 메트릭(정확도, F1 점수, BLEU 등)과 정성적
평가(인간 평가, LLM-as-Judge)를 결합한 종합적인 평가 프레임워크를 구축한다. 평가
데이터셋은 엣지 케이스, 적대적 입력, 다양한 사용 시나리오를 포함하도록 신중하게
설계되어야 한다. 또한 회귀 테스트를 통해 모델 업데이트나 프롬프트 변경 시 기존
성능이 저하되지 않는지 지속적으로 모니터링한다. 이 접근법은 LLM 애플리케이션의
품질을 체계적으로 관리하고, 프로덕션 환경에서의 예기치 않은 동작을 사전에 방지하는
데 효과적이다. CI/CD 파이프라인에 평가 단계를 통합하여 자동화된 품질 게이트를
구현하는 것이 EvDD의 모범 사례로 권장된다.
""".strip(),
        "source": {"book_id": "llm-engineering", "book_title": "LLM 엔지니어링", "page": 134, "paragraph": 4}
    },
]


def get_sample(index: int = 0) -> dict:
    """인덱스로 샘플 가져오기"""
    return SAMPLE_PARAGRAPHS[index % len(SAMPLE_PARAGRAPHS)]


def get_all_samples() -> list[dict]:
    """모든 샘플 반환"""
    return SAMPLE_PARAGRAPHS
