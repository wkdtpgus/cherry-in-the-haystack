**업데이트**: 전체 강연은 지금 YouTube에서 시청하실 수 있습니다! 슬라이드는 여기에서 확인하세요: https://docs.google.com/presentation/d/1sZqMAoIJDxz79cbC5ap5v9jknYH4Aa9cFFaWL8Rids4/edit?usp=sharing Andrej의 강연에 대한 많은 사람들의 큰 관심은 여전히 뜨겁습니다. 당시 YC AI 스타트업 스쿨(YC AI Startup School)에서 있었던 강연은 "앞으로 몇 주 안에" 공개될 예정이었고, 아쉽게도 모든 이가 현장에 직접 참석할 수는 없었으며, 강연 내용이 구식이 될 수도 있다는 우려도 있었습니다. 아무도 팬캠(fancam)을 녹화한 것 같지 않았지만, 피프리서치(PeepResearch)™를 사용하여 강연에 대한 모든 트윗을 취합하고, 훌륭한 필기자(마지막 슬라이드에 출처 표기)들의 힌트를 활용하여 주요 내용을 재구성했습니다. 그때 약속드렸던 바와 같이, 전체 강연 영상과 스크립트가 공개된 지금, 저희는 이 글을 통해 Andrej Karpathy의 통찰을 더욱 깊이 있게 분석하고 주석을 추가했습니다. **업데이트**: 슬라이드가 이제 전체 스크립트(transcript)와 동기화(synced)되었습니다. 읽어보시려면 아래로 스크롤하여 슬라이드를 확인하세요.

**Part 0: 소프트웨어 3.0(Software 3.0) - 프롬프트(Prompt)가 이제 프로그램(Program)이다**

우리는 "AI 엔지니어(AI Engineer)의 부상(Rise of The AI Engineer)"에서 소프트웨어 3.0(Software 3.0)에 대해 처음 논의했지만, 이는 소프트웨어 2.0(Software 2.0) 에세이와 "가장 뜨거운 새로운 프로그래밍 언어(programming language)는 영어다"라는 말의 명백한 결과입니다. Andrej는 원래 테슬라(Tesla)에서 소프트웨어 1.0(Software 1.0)을 대체하고 있는 현상을 관찰하며 소프트웨어 2.0(Software 2.0) 에세이를 작성했습니다. 그리고 이제 그는 소프트웨어 3.0(Software 3.0) 시대에 맞춰 그 개념을 업데이트하기 위해 다시 돌아왔습니다.

제가 했던 것처럼 소프트웨어 2.0(Software 2.0) 차트를 수정하는 대신, Andrej는 소프트웨어 1.0/2.0/3.0의 혼합 및 공존을 보여주는 새로운 다이어그램을 선보이며 "소프트웨어 3.0(Software 3.0)이 1.0/2.0을 잠식하고 있다" 그리고 "엄청난 양의 소프트웨어(software)가 다시 작성될 것"이라고 언급했습니다.

Andrej는 여전히 프로그램(program)의 핵심으로서 프롬프트(prompt)에 집중하고 있으며, 이는 2023년 당시에도 논의의 여지가 있었고, 현재도 여전히 중요한 관점입니다. 최근 몇 년간 AI 엔지니어(AI Engineer)의 역할이 단순히 프롬프트 엔지니어링(Prompt Engineering)을 넘어 시스템 설계와 모델 최적화 등 광범위한 영역으로 확장되고 있음을 고려할 때, 소프트웨어 3.0의 "1+2=3" 변형은 이러한 변화를 잘 설명합니다.

**Part 1: LLM(Large Language Model)은 새로운 컴퓨터(Computer)다**

LLM(Large Language Model)은 유틸리티(Utility)와 같다
LLM(Large Language Model)은 반도체 팹(Fab)과 유사한 역할을 한다.
LLM(Large Language Model)은 OS(Operating System)와 같다
LLM(Large Language Model)은 시분할 메인프레임(Timeshare Mainframe)과 같은 특성을 보입니다… 비록 그가 "파워 투 더 피플(Power to the People)"에서 주장하듯이, LLM(Large Language Model)은 또한 값비싼 최첨단 기술(frontier tech)의 일반적인 흐름과는 다른 특이한 역전 현상을 보이기도 합니다. 이는 클라우드(cloud) 기반 AI에서 개인/프라이빗 AI(Personal/Private AI)로의 전환 가능성을 시사합니다. 최근 엑소랩스(Exolabs)와 애플 MLX(Apple MLX)의 작업에서 개인 컴퓨팅 v2(Personal Computing v2)의 징후가 더욱 뚜렷해지고 있습니다.

Part 1 요약:

**Part 2: LLM 심리학(LLM Psychology)**

LLM(Large Language Model)은 "사람의 영혼"과 같습니다. 즉, 일종의 창발적 "심리학"을 가진 사람들의 확률적 시뮬레이션(stochastic simulation)입니다.

Andrej는 현재 LLM(Large Language Model)이 인간을 시뮬레이션(simulate)하는 방식에서 나타나는 두 가지 주요 난제를 강조합니다.

**들쭉날쭉한 지능(Jagged Intelligence)** (https://x.com/karpathy/status/1816531576228053133): 제가 생각해낸 이 단어는 최첨단 LLM(state-of-the-art LLM)이 매우 인상적인 작업(예: 복잡한 수학 문제 해결)을 수행할 수 있으면서도 동시에 매우 어리석은 문제로 고군분투하는 (이상하고 직관적이지 않은) 사실을 설명합니다. 예를 들어, 이틀 전의 예시입니다. 9.11과 9.9 중 어떤 숫자가 더 큰가요? 틀렸습니다. LLM은 인간과 달리 지식과 문제 해결 능력이 선형적으로(linearly) 함께 발전하지 않으며, 특정 영역에서는 탁월하지만 다른 영역에서는 치명적인 실패를 보이는 경우가 많습니다. 어느 부분이 강하고 약한지 명확하지 않을 때가 많지만, 경험을 통해 어느 정도 직관을 개발할 수 있습니다. 최근 연구에서는 이러한 들쭉날쭉한 지능(Jagged Intelligence) 문제를 해결하기 위해 다양한 접근 방식이 시도되고 있습니다. 단순히 스케일링(scaling)을 넘어 스택(stack) 전반에 걸친 아키텍처 개선과 학습 데이터의 질적 향상이 중요하게 부각되고 있습니다. 개인적으로 저는 이것들이 근본적인 문제라고 생각하지 않습니다. 이것들은 스케일링(scaling)뿐만 아니라 스택(stack) 전반에 걸쳐 더 많은 작업을 요구합니다. 현재 가장 큰 문제 중 하나는 '인지적 자기 지식(cognitive self-knowledge)'의 부족입니다. 이는 모델 후처리 학습(model post-training)에서 인간 라벨러(human labeler)를 모방하는 것보다 더 정교한 접근 방식을 요구합니다. 라마 3.1(Llama 3.1) 논문의 환각(hallucination) 완화 섹션(https://x.com/karpathy/status/1816171241809797335)에서 그 예시를 확인할 수 있습니다. 현재로서는 특히 프로덕션 환경(production setting)에서 이것을 인지해야 합니다. LLM(Large Language Model)을 잘하는 작업에 사용하되, 들쭉날쭉한 부분에 주의하고 휴먼 인 더 루프(human in the loop)를 유지하세요.

**전향성 기억상실증(Anterograde Amnesia)** (https://x.com/karpathy/status/1930003172246073412): 저는 LLM(Large Language Model)이 전향성 기억상실증(Anterograde Amnesia)을 가진 동료와 같다고 설명하고 싶습니다. 즉, 학습이 끝나면 장기적인 지식이나 전문성을 통합하거나 구축하지 못하고, 그들이 가진 것은 단기 기억(short-term memory)(컨텍스트 윈도우(context window))뿐입니다. 이러한 상태에서는 관계를 구축하거나 작업을 수행하기 어렵습니다. 이러한 결함에 대한 첫 번째 완화 시도로 ChatGPT의 메모리(Memory) 기능이 등장했지만, 이는 앞으로 구현될 더 발전된 기능의 원시적인 형태일 뿐입니다. 이로 인해 그는 새로운 학습 패러다임(learning paradigm)의 필요성을 제안했습니다 (https://x.com/karpathy/status/1921368644069765486). 우리는 LLM(Large Language Model) 학습을 위한 (적어도 하나의) 주요 패러다임(paradigm)을 놓치고 있습니다. 무엇이라고 불러야 할지 확실하지 않지만, 아마 이름이 있을 수도 있습니다. 시스템 프롬프트 학습(system prompt learning)일까요? 기존의 사전 학습(Pretraining)은 지식 습득에, 미세 조정(Finetuning)은 특정 행동 패턴 학습에 중점을 두었습니다. 그러나 많은 인간 학습은 파라미터(parameter) 변경보다는 시스템 프롬프트(system prompt)의 변화와 유사하게 작동합니다. 즉, 문제를 해결하고 그 경험을 바탕으로 다음번에는 '이런 종류의 문제에는 이런 접근 방식이 효과적일 것 같다'와 같이 스스로 '메모'하는 방식입니다. 이는 특정 사용자 데이터를 저장하는 것을 넘어, 일반적인 문제 해결 전략을 기억하는 '메모리(Memory)' 기능과 더 가깝습니다. LLM(Large Language Model)은 말 그대로 메멘토(Memento)의 남자와 같지만, 우리는 아직 그들에게 스크래치패드(scratchpad)를 주지 않았습니다. 이 새로운 패러다임(paradigm)은 지식 기반 '검토' 단계를 활용하여 보상 스케일러(reward scaler)보다 훨씬 더 강력하고 데이터 효율적인 피드백 채널(feedback channel)을 제공할 수 있습니다. 제 생각에는 이것이 강화 학습(Reinforcement Learning)을 통해 가중치(weight)에 내재화되어야 하는 종류의 문제 해결 지식이 아니며, 적어도 즉시/독점적으로는 아닙니다. 그리고 인간 엔지니어(engineer)가 시스템 프롬프트(system prompt)를 수동으로 작성하는 것에서 나와서도 안 됩니다. 대신, 이는 시스템 프롬프트 학습(System Prompt learning)을 통해 이루어져야 합니다. 이는 수정(correction)과 경사 하강법(gradient descent)의 차이를 제외하면 강화 학습(Reinforcement Learning)과 유사한 설정입니다. LLM(Large Language Model) 시스템 프롬프트(system prompt)의 상당 부분은 시스템 프롬프트 학습(system prompt learning)을 통해 자체적으로 생성될 수 있으며, 이는 LLM이 문제 해결 방법에 대한 지침을 스스로 작성하는 것과 같은 형태를 띠게 될 것입니다. 이러한 접근 방식이 성공한다면, 이는 강력한 새로운 학습 패러다임(learning paradigm)이 될 것입니다. 아직 해결해야 할 많은 세부 사항이 남아 있습니다(수정은 어떻게 작동하는가? 수정 시스템을 학습할 수/해야 하는가? 인간이 하는 것처럼 명시적인 시스템 텍스트에서 습관적 가중치로 지식을 점진적으로 어떻게 이동시키는가? 등).

Part 2 요약:

**Part 3: 부분 자율성(Partial Autonomy)**

우리는 아이언맨 슈트(Iron Man Suit) 비유를 좋아합니다. 이 슈트는 두 가지 유용한 방식으로 우리를 확장합니다.

*   **증강(Augmentation)**: 사용자에게 힘, 도구, 센서(sensor) 및 정보를 제공
*   **자율성(Autonomy)**: 슈트는 종종 자체적인 의지를 가지고 프롬프트(prompt) 없이 행동을 취함

이러한 원칙에 따라 AI 제품을 어떻게 설계할 수 있을까요?

**Part 3a: 자율성 슬라이더(Autonomy Slider)**

자율성 슬라이더(Autonomy Slider)는 컨텍스트(context)에 대한 자율성(autonomy) 수준을 선택할 수 있게 해주는 중요한 개념입니다. 예:

*   커서(Cursor): Tab -> cmd+K -> Cmd+L -> Cmd+I (에이전트 모드(agent mode))
*   퍼플렉시티(Perplexity): 검색 -> 연구 -> 심층 연구
*   테슬라 오토파일럿(Tesla Autopilot): 레벨 1에서 레벨 4

**Part 3b: 인간-AI 생성-검증 루프(Human-AI Generation-Verification Loop)**

생성 <-> 검증 주기에서 우리는 부분 자율성(partial autonomy)을 위한 완벽한 워크플로우(workflow)를 필요로 합니다. 루프(loop)가 빠를수록 더 효과적입니다.

*   **검증 개선**: 쉽고 빠르게 승리하게 만드세요.
*   **생성 개선**: AI의 고삐를 바싹 죄세요.

**Part 3c: 데모-제품 간극(Demo-Product Gap)**

우리가 **부분** 자율성(partial autonomy)을 중요하게 여기는 이유는, 작동하는 데모(demo)와 실제로 신뢰할 수 있는 제품 사이에는 여전히 상당한 간극이 존재하기 때문입니다. 그는 2014년에 웨이모(Waymo) 프로토타입(prototype)을 아무런 개입 없이 탔던 경험을 회상하며 자율 주행(self-driving)이 "도래했다"고 생각했지만, 여전히 해결해야 할 많은 문제가 남아 있었다고 말합니다.

"데모(demo)는 works.any()이고, 제품은 works.all()이다."

**Part 4: 바이브 코딩(Vibe Coding)**

수많은 스타트업(startup)을 탄생시킨 트윗은 이제 자체 위키백과(Wikipedia) 페이지를 가지고 있습니다!

그러나 여전히 많은 문제들이 남아있습니다. 메뉴젠(MenuGen)을 바이브 코딩(Vibe coding)하는 과정에서, 그는 로컬 코드(local code)를 실행한 직후 AI 가속 기능이 사라지는 현상을 경험했습니다. 이는 AI 기반 개발 환경이 실제 프로덕션 워크플로우에 통합될 때 직면하는 현실적인 과제를 보여줍니다. 특히, 2025년 현재 웹 애플리케이션(web application) 구축의 복잡성은 여전하며, 웹 개발 전문가(webdev expert)들이 기존의 파편화된 서비스들을 통해 여전히 핵심 역할을 하고 있습니다. 이러한 환경은 AI의 원활한 접근을 어렵게 만들기도 합니다. 불쌍한 클럭(Clerk)은 부정적인 언급을 받았고, 버셀(Vercel)의 @leerob은 긍정적인 언급을 받았습니다. 이는 각 플랫폼의 문서화 접근 방식이 인간 개발자와 에이전트(agent) 중 어느 쪽에 더 친화적인지를 단적으로 보여주는 사례로 해석될 수 있습니다.

**Part 5: 에이전트(Agent)를 위해 구축하라**

결론적으로, 도구 제작자(toolmaker)들은 "디지털 정보의 새로운 소비자/조작자 범주"가 존재한다는 사실을 인지해야 합니다.

1.  인간(GUI(Graphical User Interface))
2.  컴퓨터(API(Application Program Interface))
3.  **새로운 것**: 에이전트(Agent) <- 컴퓨터… 하지만 인간과 유사함

구체적으로, llms.txt와 같은 접근 방식이 효과적인 이유는 HTML이 LLM(Large Language Model)에게는 파싱(parsing)하기 쉽지 않기 때문입니다. 그는 또한 우리가 라이트닝 팟(lightning pod)에서 다루었던 기트인제스트(Gitingest)와 코그니션(Cognition)의 딥위키(DeepWiki)와 같은 "컨텍스트 빌더(Context builder)"를 언급했습니다.

**마무리 / 요약**

이것은 에이전트(Agent)의 10년입니다. AGI(Artificial General Intelligence) 2027과 같은 과장된 예측이나 작동하지 않는 화려한 데모(demo)보다는, 부분 자율성(partial autonomy), 맞춤형 GUI(custom GUI) 및 자율성 슬라이더(autonomy slider)를 갖춘 실용적인 솔루션이 더욱 중요해질 것입니다. 소프트웨어 3.0(Software 3.0)이 소프트웨어 1/2(Software 1/2)을 잠식하고 있다는 것, 그들의 유틸리티(Utility)/팹(Fab)/OS(Operating System) 특성이 그들의 운명을 결정할 것이라는 것, 생성기-검증기 루프(generator-verifier loop)를 개선하고, **에이전트(Agent)를 위해 구축하라 🤖**는 것을 기억하세요.

LS 구독자를 위한 전체 슬라이드는 여기 :) 아래 링크