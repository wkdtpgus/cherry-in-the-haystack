(출처: [2, 5, 14]) 빠르게 변화하는 연구 분야에서, 디코더 전용 트랜스포머(decoder-only transformer) 아키텍처는 대규모 언어 모델(LLM) 연구에서 몇 안 되는 지속적인 핵심 요소 중 하나로 남아 있습니다. 이 아키텍처는 원래 GPT 모델이 제안된 이래로 사용되어 왔으며, 효율성 향상을 위한 사소한 조정 외에는 거의 변함없이 유지되었습니다. 그러나 이 아키텍처에 대해 탐구될 가장 의미 있는 수정 중 하나는 전문가 혼합(Mixture-of-Experts, MoE) 레이어입니다.

"MoE 아키텍처를 사용하면 밀집 모델(dense model)이 일반적으로 달성하는 것보다 모델 품질과 추론 효율성(inference efficiency) 사이에서 더 나은 절충점(tradeoff)을 얻을 수 있습니다." - [11]에서

MoE 기반 LLM은 모델 아키텍처에 희소성(sparsity)을 도입하여, 총 매개변수(total parameters) 수를 기준으로 모델 크기를 계산 비용(compute costs)의 상응하는 증가 없이 크게 늘릴 수 있도록 합니다. Grok [9] 및 DeepSeek-v3 [15]와 같은 최신 모델에서 성공적으로 채택된 이 수정은 극도로 큰 모델의 탐색을 더 다루기 쉽고 계산 효율적으로 만듭니다. 이 개요에서는 MoE의 기본 사항을 배우고 이 아이디어가 최근 어떻게 적용되어 더 강력한 LLM을 만들었는지 탐구할 것입니다.

## LLM을 위한 MoE의 기본 사항

MoE 아키텍처를 이해하려면 먼저 디코더 전용 트랜스포머의 기본 구조와 MoE가 이 구조에 어떻게 통합되는지 파악해야 합니다. LLM은 텍스트를 입력으로 받아 토큰화(tokenization) 및 벡터화(vectorization) 과정을 거쳐 모델의 어휘(vocabulary) 내 토큰 임베딩(token embedding)으로 변환됩니다. 이러한 토큰 임베딩은 트랜스포머 블록의 각 서브 레이어를 통과하며, 이때 피드포워드 네트워크(Feed-Forward Network, FFN)는 각 토큰의 표현을 독립적으로 변환하는 핵심 역할을 합니다.

MoE는 바로 이 FFN 부분에 희소성(sparsity)과 조건부 계산(conditional computation)이라는 새로운 패러다임을 도입합니다. 기존의 밀집(dense) FFN이 모든 입력 토큰에 대해 동일한 가중치 집합을 사용하여 연산을 수행하는 반면, MoE는 입력 토큰의 특성에 따라 특정 전문가(expert)들만 활성화하여 계산 효율성을 극대화합니다. 이는 모델이 특정 유형의 정보나 패턴을 처리하는 데 특화된 여러 "전문가"를 가질 수 있게 함으로써, 전체 모델의 표현 능력(representational capacity)을 크게 확장합니다.

MoE는 이론적 기반을 마련한 선구적인 논문들을 통해 발전해왔습니다. 특히, 희소하게 게이팅된 전문가 혼합 레이어(The Sparsely-Gated Mixture-of-Experts Layer) [1]는 MoE 레이어의 핵심 개념과 라우팅 메커니즘을 제안했으며, 스위치 트랜스포머(Switch Transformers) [2]는 이를 대규모 언어 모델에 성공적으로 적용하여 훈련 효율성을 획기적으로 개선했습니다. 또한, 안정적이고 전이 가능한 전문가 혼합(Stable and Transferable Mixture-of-Experts, ST-MoE) [3]은 MoE 모델의 훈련 안정성을 높이는 데 기여했습니다. 이러한 연구들은 MoE가 LLM의 확장성과 효율성을 동시에 달성할 수 있는 중요한 아키텍처임을 입증했습니다.

## "전문가(experts)"란 무엇인가요?

디코더 전용 트랜스포머 아키텍처에서 MoE에 의해 이루어지는 주요 수정은 트랜스포머 블록의 피드포워드 구성 요소 내에 있습니다. 표준 아키텍처에서는 단일 피드포워드 신경망(feed-forward neural network) — 일반적으로 비선형 활성화(non-linear activation)가 중간에 있는 두 개의 피드포워드 레이어로 구성됨 —을 통해 모든 토큰이 개별적으로 전달됩니다. 아래를 참조하십시오.

```
[Image: Standard feed-forward network in a transformer block]
```

MoE는 이 블록 구조를 약간 수정합니다. 블록의 피드포워드 구성 요소 내에 단일 피드포워드 네트워크를 갖는 대신, 우리는 여러 개의 피드포워드 네트워크를 생성하며, 각 네트워크는 자체적인 독립적인 가중치(independent weights)를 가집니다. 우리는 이들 각 네트워크를 "전문가(expert)"라고 부릅니다. 예를 들어, MoE 기반 LLM은 각 피드포워드 서브 레이어에 8개의 독립적인 전문가를 가질 수 있습니다.

```
[Image: Experts within a transformer layer]
```

트랜스포머 레이어 내의 전문가들은 위에서 보여준 대로 정의될 수 있습니다. 한 레이어에 N 개의 전문가가 있으며, i 번째 전문가는 E_i 라는 표기법으로 참조할 수 있습니다.

### MoE 기반 트랜스포머 아키텍처 구현

MoE 기반 디코더 전용 트랜스포머 아키텍처를 구현하려면, 트랜스포머 블록 내의 피드포워드 레이어를 여러 전문가로 구성된 MoE 레이어로 대체합니다. 각 전문가는 기존 FFN과 동일한 내부 구조를 가지지만, 고유한 가중치를 학습합니다. 이처럼 여러 전문가를 배치함으로써 모델은 다양한 유형의 입력에 대해 특화된 처리를 수행할 수 있게 됩니다.

중요한 점은 모든 트랜스포머 블록에 MoE 레이어를 적용할 필요는 없다는 것입니다. 대부분의 MoE 기반 LLM은 "인터리브된(interleaved)" 접근 방식을 사용하는데, 이는 P 번째 레이어마다 MoE 레이어를 배치하고 그 외의 레이어는 표준 FFN을 유지하는 방식입니다. 예를 들어, P를 2로 설정하면 한 레이어는 MoE, 다음 레이어는 FFN, 그 다음 레이어는 다시 MoE가 됩니다. 이러한 인터리브 방식은 모델의 성능과 계산 효율성 사이에서 최적의 균형을 찾는 데 필수적입니다. 모든 레이어에 MoE를 적용하면 총 매개변수 수가 지나치게 증가하고 통신 오버헤드(communication overhead)가 커져 비효율적일 수 있기 때문입니다. 적절한 인터리브 전략은 모델의 확장성을 유지하면서도 MoE의 이점을 효과적으로 활용할 수 있게 합니다.

## 효율적인 토큰 라우팅 메커니즘

MoE 기반 아키텍처의 주요 이점 중 하나는 효율성이지만, 전문가만 사용하는 것으로는 효율성이 향상되지 않습니다! 사실, 모델의 각 레이어에 더 많은 전문가를 추가하면 모델의 총 매개변수 수 — 그리고 필요한 계산량 —가 크게 증가합니다. 아키텍처를 더 효율적으로 만들기 위해서는 각 레이어에서 사용될 전문가들을 희소하게(sparsely) 선택해야 합니다!

**전문가 선택.** d 차원 토큰 벡터(token vector)로 표현되는 단일 토큰을 고려해 봅시다. 우리의 목표는 이 토큰을 처리할 전문가들의 부분 집합(크기 k)을 선택하는 것입니다. MoE 문헌에서는 일반적으로 토큰이 이 전문가들에게 "라우팅(routed)"될 것이라고 말합니다. 이 라우팅 작업을 계산하고 최적화할 알고리즘이 필요합니다.

가장 간단한 라우팅 알고리즘은 토큰 벡터에 선형 변환(linear transformation)을 적용하여 N 크기(즉, 전문가 수)의 벡터를 형성하는 것입니다. 그런 다음, 소프트맥스 함수(softmax function)를 적용하여 우리 토큰에 대한 전문가 집합에 걸쳐 확률 분포(probability distribution)를 형성할 수 있습니다. 이 분포를 사용하여 분포에서 상위 K 개의 전문가를 단순히 선택함으로써 우리 토큰이 라우팅되어야 할 전문가를 선택할 수 있습니다.

```
[Image: Computing output of routing mechanism]
```

이 라우팅 전략은 오늘날 우리가 사용하는 희소 MoE 레이어 구조를 제안한 논문인 [1]에서 사용되었습니다. 위를 참조하십시오. 그러나 이러한 라우팅 메커니즘은 전문가의 균형 잡힌 선택을 명시적으로 장려하지 않습니다. 이러한 이유로, 모델은 아래에서 설명하는 바와 같이 전문가 레이어를 완전히 그리고 균일하게 활용하는 대신, 모든 토큰에 대해 동일한 소수의 전문가를 반복적으로 선택하는 상태로 수렴할 가능성이 높습니다. 이 현상은 일반적으로 "라우팅 붕괴(routing collapse)"라고 불립니다.

"게이팅 네트워크(gating network)는 항상 동일한 소수의 전문가에 대해 큰 가중치를 생성하는 상태로 수렴하는 경향이 있습니다. 이러한 불균형은 선호되는 전문가들이 더 빠르게 훈련되고 따라서 게이팅 네트워크에 의해 더욱 선택되기 때문에 자가 강화됩니다." - [1]에서

**활성 매개변수(Active parameters).** MoE 레이어 내에서 각 토큰을 처리하기 위해 전문가의 부분 집합만 선택하기 때문에, MoE 문헌에는 "활성(active)" 매개변수라는 개념이 있습니다. 간단히 말해, 주어진 토큰을 처리할 때 MoE 모델의 전체 매개변수 중 작은 부분 — 각 MoE 레이어에서 선택된 전문가들에 의해 결정됨 —만 활성화됩니다. 결과적으로 MoE에 의해 수행되는 총 계산량은 전체 매개변수 수가 아닌 활성 매개변수 수에 비례합니다.

### 라우팅 전략의 발전과 과제

초기 MoE 모델들은 단순한 Top-K 라우팅 방식을 사용했지만, 이는 "라우팅 붕괴"와 같은 문제로 인해 모든 전문가가 고르게 활용되지 못하는 결과를 초래했습니다. 이를 해결하기 위해 다양한 라우팅 전략이 연구되었습니다. 예를 들어, 일부 연구는 라우터 네트워크 자체를 학습 가능한 파라미터로 간주하여, 토큰의 특성을 더 잘 반영하는 라우팅 결정을 내리도록 유도합니다. 또한, Top-K 선택 시 전문가별 편향(bias)을 동적으로 조절하여 특정 전문가에게 토큰이 몰리는 현상을 완화하려는 시도도 있습니다.

라우팅의 또 다른 중요한 과제는 전문가 전문화(expert specialization)입니다. 이상적으로는 각 전문가가 특정 유형의 정보(예: 코드, 수학, 특정 언어)에 특화되기를 바라지만, 실제로는 토큰 ID에 기반한 컨텍스트 독립적인 라우팅 패턴이 관찰되기도 합니다 [12]. 이는 동일한 토큰이 어떤 문맥에 있든 항상 동일한 전문가에게 라우팅되는 경향을 의미하며, 모델의 표현 능력을 제한할 수 있습니다. 따라서 라우팅 메커니즘이 더 의미 있는 방식으로 전문가를 활용하도록 유도하는 연구가 지속적으로 필요합니다.

## MoE 훈련의 안정성 및 균형 유지

MoE 모델의 훈련은 밀집 모델보다 복잡하며, 전문가들의 고른 활용과 훈련 안정성 유지가 핵심 과제입니다. 이를 위해 다양한 보조 손실(auxiliary losses)과 전략이 개발되었습니다.

### 보조 손실을 통한 전문가 로드 밸런싱

전문가 로드 밸런싱은 모든 전문가가 훈련 중에 충분한 수의 토큰을 처리하도록 보장하여, 일부 전문가만 과도하게 활용되고 다른 전문가들은 "굶는" 현상(starvation)을 방지하는 것을 목표로 합니다. 이는 모델의 전반적인 학습 효율성과 성능에 직접적인 영향을 미칩니다.

훈련 중에 전문가의 균형 잡힌 선택을 장려하기 위해, 우리는 모델이 각 전문가를 균일하게 활용하도록 보상하는 추가적인 제약 조건(constraint)을 훈련 손실(training loss)에 추가할 수 있습니다. [1]에서는 각 전문가에 대한 "중요도(importance)" 점수를 정의함으로써 이를 수행합니다. 중요도 점수는 라우팅 메커니즘에 의해 각 전문가에 대해 예측된 확률을 기반으로 합니다.

```
[Image: Details of computing the importance loss]
```

**중요도 손실 계산 세부 정보 (출처: [1])**

**중요도 손실(Importance Loss)**은 각 전문가에 대한 토큰 할당 확률의 균형을 맞추는 데 중점을 둡니다 [1]. 라우팅 메커니즘이 예측하는 각 전문가의 확률 분포를 기반으로 '중요도' 점수를 계산하고, 이 점수들이 고르게 분포되도록 손실을 추가합니다. 하지만 이 손실만으로는 실제 토큰 할당의 불균형을 완전히 해소하기 어렵습니다. 특정 전문가에게 높은 확률을 할당받는 소수의 토큰이 존재하더라도, 실제로는 적은 토큰만 처리하게 될 수 있기 때문입니다.

**로드 밸런싱(Load balancing).** 위에서 설명한 중요도 손실이 유용하더라도, 전문가들에게 동일한 중요도가 할당되었다고 해서 토큰이 균일하게 라우팅된다는 의미는 아닙니다. 예를 들어, 전문가들은 다음과 같은 경우에 동일한 중요도를 가질 수 있습니다:

*   매우 높은 확률을 할당하는 소수의 토큰.
*   훨씬 더 많은 수의 낮은 확률을 할당하는 토큰.

결과적으로, 중요도 손실을 사용하더라도 각 전문가에게 전달되는 토큰의 수는 여전히 매우 불균일할 수 있으며, 이는 과도한 메모리 사용과 MoE 2 의 전반적인 효율성 저하로 이어질 수 있습니다.

```
[Image: The expert load balancing loss]
```

**전문가 로드 밸런싱 손실 (출처: [2])**

**로드 밸런싱 손실(Load Balancing Loss)** [2]은 이러한 한계를 보완하기 위해 도입되었습니다. 이 손실은 각 전문가에게 할당된 라우터 확률의 비율과 실제로 각 전문가에게 전달된 토큰의 비율이라는 두 가지 요소를 동시에 고려합니다. 이 두 비율이 서로 유사하도록 유도함으로써, 라우터가 전문가들에게 균일한 확률을 할당할 뿐만 아니라 실제로 토큰들이 고르게 분배되도록 장려합니다. 이는 하드웨어 활용률을 높이고 메모리 과부하를 방지하는 데 필수적입니다. 이 문제를 해결하기 위해, 우리는 전문가 중요도와 로드 밸런싱(각 전문가 간 토큰의 균등 라우팅으로 정의됨)을 모두 포착하는 단일 보조 손실 항(auxiliary loss term)(위에 표시됨)을 생성할 수 있습니다. 이러한 접근 방식은 [2]에서 제안되었으며, 저자들은 두 가지 양을 고려하는 손실을 생성합니다:

*   각 전문가에게 할당된 라우터 확률의 비율 3 .
*   각 전문가에게 전달된 토큰의 비율.

이 두 양을 각각의 N 차원 벡터에 저장하면, 이 두 벡터의 내적(dot product) 4 을 취하여 단일 손실 항을 생성할 수 있습니다. 결과 손실은 전문가들이 균일한 확률과 로드 밸런싱을 받을 때 최소화되며, 따라서 단일 보조 손실 항 내에서 우리의 두 가지 목표를 모두 포착합니다!

```
[Image: The router-z loss]
```

**라우터-z 손실 (출처: [3])**

**라우터 z-손실(Router z-loss)** [3]은 라우터의 훈련 안정성을 향상시키는 데 초점을 맞춥니다. 라우터의 로짓(logits) 값이 너무 커지면 소프트맥스 함수 적용 시 부동 소수점 오차(floating-point error)로 인해 훈련이 불안정해질 수 있습니다. 라우터 z-손실은 이러한 로짓의 크기를 제한하여 훈련 과정을 안정화합니다. 이 손실은 로드 밸런싱과는 다른 목적을 가지므로, 일반적으로 로드 밸런싱 손실과 함께 사용되어 MoE 훈련의 전반적인 견고성을 높입니다.

"라우터는 float32 정밀도로 전문가들에 대한 확률 분포를 계산합니다. 그러나 가장 큰 규모에서는 이것이 신뢰할 수 있는 훈련을 제공하기에 불충분하다는 것을 발견했습니다." - [3]에서

라우터가 더 작은 로짓을 예측하도록 장려하기 위해, 위에 표시된 손실 항을 사용할 수 있습니다. 이 손실이 라우터의 로짓을 정규화하는 데만 초점을 맞추고 로드 밸런싱을 수행하지 않는다는 점을 감안할 때, 우리는 일반적으로 라우터 z-손실을 [2]에서 제안된 보조 로드 밸런싱 손실과 함께 사용합니다. 이 두 손실은 LLM의 표준 언어 모델링 손실 위에 추가됩니다. 아래를 참조하십시오.

```
[Image: Combined loss function for MoE training]
```

이러한 보조 손실들은 MoE 훈련의 복잡성을 관리하고 모델이 잠재력을 최대한 발휘할 수 있도록 돕는 중요한 도구입니다. 최근에는 보조 손실 없이도 로드 밸런싱을 달성하려는 시도도 있으며, 이는 전문가별 편향 항(bias term)을 동적으로 조정하여 전문가 활용도를 균형 있게 만드는 방식입니다 [15].

### 전문가 용량(Expert Capacity)

```
[Image: Expert capacity diagram]
```

(출처: [2])

**전문가 용량.** 각 전문가에 대해 설정하는 고정 배치 크기를 형식화하기 위해 전문가 용량을 정의할 수 있습니다. 전문가 용량은 아래와 같이 정의됩니다.

```
[Mathematical formula for expert capacity]
```

전문가 용량은 각 전문가에게 보낼 수 있는 배치 내 토큰의 최대 수를 정의합니다. 전문가에게 라우팅된 토큰 수가 전문가 용량을 초과하면, 우리는 이 추가 토큰들을 "드롭(drop)"합니다. 더 구체적으로 말하면, 이 토큰들에 대한 계산은 수행하지 않고, 트랜스포머의 잔여 연결(residual connection)을 통해 그 표현이 다음 레이어로 직접 흐르도록 합니다.

"하드웨어 활용을 개선하기 위해, 희소 모델의 대부분의 구현은 각 전문가에 대해 정적 배치 크기를 가집니다. 전문가 용량은 각 전문가에게 라우팅될 수 있는 토큰의 수를 의미합니다. 이 용량을 초과하면 오버플로우된 토큰들은… 잔여 연결을 통해 다음 레이어로 전달됩니다." - [3]에서

### 용량 계수 최적화 및 동적 할당

전문가 용량은 `용량 계수(capacity factor)`를 통해 조절됩니다. 용량 계수 1은 토큰이 전문가들 사이에 완벽하게 균형 잡힌 방식으로 라우팅될 때 각 전문가가 처리할 수 있는 최대 토큰 수를 의미합니다. 그러나 실제 훈련 환경에서는 토큰 라우팅이 완벽하게 균일하지 않으므로, 용량 계수를 1보다 높게 설정하여 토큰 불균형에 대한 버퍼를 제공하는 것이 일반적입니다. 예를 들어, 용량 계수를 1.25로 설정하면 각 전문가가 평균보다 25% 더 많은 토큰을 처리할 수 있는 여유를 가집니다.

용량 계수를 너무 낮게 설정하면 드롭되는 토큰의 수가 많아져 모델 성능이 저하될 수 있으며, 너무 높게 설정하면 메모리 사용량이 증가하고 계산 효율성이 떨어집니다. 따라서 훈련 안정성과 효율성을 동시에 고려하여 최적의 용량 계수를 찾는 것이 중요합니다. 흥미롭게도, MoE 모델은 비교적 낮은 용량 계수에서도 잘 작동하는 경향이 있으며 [2, 3], 이는 MoE의 희소성 이점을 잘 활용할 수 있음을 시사합니다.

```
[Image: Capacity factor vs. dropped tokens]
```

(출처: [2])

**용량 계수는 어떻게 설정할까요?** 흥미롭게도, MoE 모델은 비교적 낮은 용량 계수에서도 잘 작동하는 경향이 있습니다 [2, 3]. 위를 참조하십시오. 그러나 훈련 실행에 영향을 미치지 않도록 드롭되는 토큰의 수가 너무 많지 않도록 해야 합니다 (즉, 이는 경험적으로 수행될 수 있습니다). 또한 훈련과 추론에 다른 용량 계수를 사용할 수 있습니다. 예를 들어, ST-MoE [3]는 훈련 중에는 1.25의 용량 계수를 사용하고 평가 중에는 2.0의 용량 계수를 사용합니다.

최근에는 정적인 용량 계수 대신, 토큰 분포의 동적 변화에 따라 전문가 용량을 유연하게 조절하는 `동적 용량 할당(dynamic capacity allocation)` 연구도 진행되고 있습니다. 이는 전문가 활용도를 실시간으로 최적화하여 훈련 및 추론 효율성을 더욱 높일 수 있는 잠재력을 가집니다.

## MoE 레이어의 출력 계산 및 고급 기법

MoE 레이어의 최종 출력은 라우팅 메커니즘을 통해 선택된 전문가들의 출력을 결합하여 얻어집니다. 이 과정은 모델의 표현력을 결정하는 중요한 부분입니다.

### MoE 레이어 출력 계산의 기본

```
[Image: Computing output of an MoE layer]
```

**MoE 레이어의 출력 계산**

라우터의 출력을 얻으면, 최종 출력을 다음과 같이 계산합니다:

*   토큰을 선택된 전문가들에게 보냅니다.
*   이 토큰들에 대한 전문가들의 출력을 계산합니다.
*   전문가 출력의 가중 평균(weighted average)을 취하며, 가중치는 라우터에 의해 각 전문가에게 할당된 확률입니다.

```
[Mathematical formula for MoE layer output]
```

위 방정식에서, 우리는 단일 토큰에 대한 MoE 레이어의 출력을 계산하는 과정을 형식화했습니다. 이 토큰의 출력은 K 개의 활성 전문가 각각의 출력에 대한 가중 평균입니다. 이 가중 평균은 각 전문가의 기여도를 조절하여 최종 출력을 형성하며, 이는 조건부 계산의 핵심입니다.

### 공유 전문가(Shared experts)

**공유 전문가(Shared experts)**는 MoE 문헌 [14, 15]에서 비교적 최근에 도입된 아이디어입니다. 아이디어는 간단합니다: 우리는 두 그룹의 전문가를 가집니다 — 공유 전문가와 라우팅된 전문가(routed experts). 모든 토큰은 항상 공유 전문가들을 통과합니다. 토큰은 일반적인 MoE 라우팅 메커니즘에 따라 라우팅된 전문가들을 통과합니다. 공유 전문가에 대한 이 아이디어는 아래에 묘사되어 있으며, MoE 레이어 내의 전문가 부분 집합에만 라우팅이 적용됨을 볼 수 있습니다. 일반적으로 공유 전문가의 수는 라우팅된 전문가의 수보다 적어야 합니다 — 공유 전문가의 수를 늘리면 MoE의 희소성 이점(sparsity benefits)이 저하됩니다.

```
[Image: Shared vs. routed experts]
```

**공유 전문가 대 라우팅된 전문가 (출처: [14])**

공유 전문가를 사용하는 동기는 전문가들 간의 중복 정보(redundant information) 양을 최소화하는 것입니다. 공유 전문가 집합을 가짐으로써, 네트워크가 동일한 정보를 여러 다른 전문가에 걸쳐 복제할 필요 없이 이 전문가들 내에 공유 정보를 저장할 수 있도록 합니다. 공유 전문가가 있는 MoE 레이어의 출력을 계산하려면, 단순히 공유 전문가의 출력을 일반적인 라우팅된 출력에 추가합니다. 아래를 참조하십시오.

```
[Mathematical formula for MoE layer output with shared experts]
```

**공유 전문가가 있는 MoE 레이어의 출력 계산**

### 계층적 MoE 및 동적 MoE

최근 연구에서는 MoE의 개념을 더욱 확장하여 `계층적 MoE(Hierarchical MoE)`와 `동적 MoE(Dynamic MoE)`와 같은 고급 기법들이 탐구되고 있습니다. 계층적 MoE는 전문가들을 여러 계층으로 구성하여, 첫 번째 계층의 라우터가 광범위한 전문가 그룹을 선택하고, 다음 계층의 라우터가 그 그룹 내에서 더 세분화된 전문가를 선택하는 방식입니다. 이는 모델이 더욱 복잡한 의사결정 과정을 통해 토큰을 처리하고, 전문가의 전문화 수준을 다단계로 조절할 수 있게 합니다.

`동적 MoE`는 모델이 훈련 또는 추론 중에 전문가의 수, 구성 또는 라우팅 규칙을 동적으로 변경하는 것을 허용합니다. 예를 들어, 특정 작업이나 입력에 따라 필요한 만큼의 전문가만 활성화하거나, 훈련 초기에는 모든 전문가를 사용하다가 점차 불필요한 전문가를 제거하여 모델을 경량화하는 방식 등이 있습니다. 이러한 유연성은 MoE 모델의 효율성을 극대화하고, 다양한 시나리오에 더욱 효과적으로 적응할 수 있도록 합니다.

## 모두 종합하기: MoE 레이어를 가진 디코더 전용 LLM

```
[Image: A full MoE block in a transformer]
```

**트랜스포머 내의 전체 MoE 블록 (출처: [2])**

MoE 레이어의 전체 묘사는 위에 제공됩니다. MoE에서는 표준 디코더 전용 트랜스포머의 블록 구조를 피드포워드 네트워크를 전문가 레이어로 대체함으로써 수정합니다. 간단히 말해, 이 전문가 레이어는 원래 피드포워드 네트워크의 여러 독립적인 복사본을 포함합니다. 특히, MoE 레이어 내의 이 모든 구성 요소 — 일반 레이어(들), 전문가들, 그리고 라우팅 메커니즘 —는 경사 하강법(gradient descent)을 통해 공동으로 훈련됩니다. 각 토큰에 대해, 우리는 라우팅 메커니즘을 통해 어떤 전문가를 사용할지 선택할 수 있으며, 이는 일반적으로 토큰 벡터의 간단한 선형 변환을 통해 구현됩니다. 이를 종합하면, MoE의 수정된 블록 구조는 다음을 포함합니다:

*   셀프 어텐션 레이어(self-attention layer).
*   잔여 연결과 정규화 연산(normalization operation).
*   토큰을 전문가에게 라우팅하는 것을 결정하는 라우팅 메커니즘.
*   여러 독립적인 피드포워드 네트워크를 가진 전문가 레이어.
*   각 토큰에 대한 전문가 레이어의 최종 출력에 적용되는 최종 추가 및 정규화 연산.

수정된 블록 구조를 제외하고는 트랜스포머 아키텍처는 동일하게 유지됩니다. 또한 트랜스포머의 P 번째 블록만 MoE 레이어를 사용하도록 변환하며 — 다른 블록은 변경되지 않습니다. 일부 MoE는 모든 레이어에 전문가를 사용하지만, 실제로는 P 를 2, 4 또는 심지어 6으로 설정하는 것이 일반적입니다. 이 방법은 MoE LLM이 소비하는 총 매개변수 수를 제어하는 데 유용할 수 있습니다.

## MoE의 장점과 도전 과제

이제 MoE의 기본 사항을 이해했으니, 궁금할 수 있습니다: 왜 밀집 모델 대신 MoE를 사용하고 싶을까요? MoE의 가장 큰 장점은 효율성이지만, 이 모델들에는 주목할 만한 단점도 있습니다. MoE의 가장 중요한 장단점 몇 가지를 빠르게 살펴보겠습니다.

### MoE의 장점.

LLM은 규모의 이점을 얻습니다 — 더 큰 모델과 더 큰 데이터셋은 더 나은 성능으로 이어집니다. 그러나 LLM을 확장하는 데는 비용이 따릅니다! MoE의 주요 이점 중 하나는 확장과 관련된 문제를 회피하는 능력입니다 — 이들은 토큰당 고정된 계산 비용(computational cost)으로 모델의 크기를 늘릴 수 있도록 합니다. 이런 식으로, 밀집 모델에만 국한된다면 불가능했을 더 큰 모델을 훈련할 수 있습니다. 언어 모델링 영역에서, 이러한 희소 모델의 추가 매개변수와 표현 능력(representational capacity)은 큰 차이를 만듭니다.

"LLM이 점점 더 보편화됨에 따라, 계산 자원을 비례적으로 늘리지 않고 성능을 향상시키는 것은 중요한 과제입니다." - [12]에서

MoE의 계산 이점은 (논쟁의 여지가 있지만) 추론 중에 가장 큰 영향을 미칩니다. MoE 모델은 총 매개변수 수 면에서 크기 때문에, 이러한 매개변수를 저장할 수 있는 충분한 수의 GPU가 필요합니다. 그러나 각 토큰을 처리할 때 이 매개변수 중 고정된 부분만 사용하므로 계산 효율성이 크게 향상됩니다. 낮은 배치 크기에서는 추론이 더 빠르고, 큰 배치 크기에서는 처리량(throughput)이 더 높습니다 [5]. 흥미롭게도 MoE는 훈련에도 더 효율적입니다. 예를 들어, 스위치 트랜스포머는 MoE 아키텍처 사용으로 7배의 사전 훈련 속도 향상을 보고했습니다 [2]. 아래를 참조하십시오.

```
[Image: Training speedup with MoE]
```

(출처: [2])

### MoE의 확장된 이점: 다중 모달 및 강건성

MoE는 언어 모델링 외에도 다양한 영역에서 확장된 이점을 제공합니다. 특히, `다중 모달(multimodal)` LLM에서 MoE는 텍스트, 이미지, 오디오 등 여러 양식(modality)의 데이터를 처리하는 데 특화된 전문가를 가질 수 있게 합니다. 예를 들어, 이미지 관련 전문가와 텍스트 관련 전문가를 분리하여 운영함으로써, 모델은 각 양식에 대한 깊이 있는 이해를 유지하면서도 전체적인 표현 능력을 향상시킬 수 있습니다. 이는 복잡한 다중 모달 작업을 효율적으로 처리하는 데 기여합니다.

또한, MoE는 모델의 `강건성(robustness)`을 향상시키는 데 도움을 줄 수 있습니다. 특정 전문가가 오염되거나 오작동하더라도, 다른 전문가들이 여전히 올바른 정보를 처리할 수 있으므로, 단일 밀집 모델보다 오류에 덜 취약할 수 있습니다. 이는 시스템의 신뢰성을 높이는 데 중요한 역할을 합니다. 또한, MoE는 특정 지식이나 기능을 특정 전문가에 `분리(isolate)`하여 저장할 수 있는 잠재력을 제공하며, 이는 모델의 해석 가능성(interpretability)을 높이고 특정 지식 영역을 업데이트하거나 수정하는 것을 용이하게 할 수 있습니다.

### MoE 훈련 및 배포의 난점과 해결 방안

**MoE 사용의 단점.** 이러한 이점에도 불구하고 MoE는 또한 다음과 같습니다:

*   훈련 중 불안정성에 취약합니다.
*   미세 조정(finetune)하기 어렵습니다 (즉, 과적합(overfitting) 문제로 인해).
*   낮은/혼합 정밀도 훈련 기술(low / mixed precision training techniques)에 민감합니다.
*   하이퍼파라미터(hyperparameter) 설정(예: 가중치 초기화(weight initialization))에 민감합니다.

간단히 말해, MoE를 최대한 활용하려면 더 많은 부가 기능과 미세 조정이 필요합니다. 이러한 이유로, MoE는 모든 시나리오에서 최선의 선택이 아닐 수 있습니다. 예를 들어, 특정 작업에서 LLM을 미세 조정하려는 경우 밀집 모델이 더 쉬운 선택일 수 있습니다. 그러나 제대로 사용할 수 있다면 MoE는 다양한 이점을 가집니다.

MoE는 많은 장점에도 불구하고 훈련 및 배포 과정에서 여러 도전 과제에 직면합니다.

*   **훈련 불안정성**: MoE 모델은 라우팅 메커니즘과 전문가 간의 상호작용으로 인해 훈련이 불안정해지기 쉽습니다. 특히 `콜드 스타트(cold start)` 문제(초기 훈련 단계에서 전문가들이 제대로 전문화되지 않아 라우팅이 비효율적인 문제)나 라우팅 붕괴 현상이 발생할 수 있습니다. 이를 해결하기 위해 정교한 초기화 전략, 보조 손실, 그리고 훈련 과정에서 라우터를 점진적으로 활성화하는 기법 등이 연구되고 있습니다.
*   **미세 조정(Fine-tuning)의 어려움**: 사전 훈련된 MoE 모델을 특정 작업에 미세 조정할 때 `과적합(overfitting)` 문제나 전문가 활용의 불균형이 심화될 수 있습니다. 이를 완화하기 위해 `어댑터(adapter)` 기반 미세 조정, `점진적 동결 해제(progressive unfreezing)` (일부 전문가만 미세 조정하거나 점진적으로 동결을 해제하는 방식), 또는 특정 전문가를 목표로 하는 `전문가별 미세 조정(expert-specific fine-tuning)`과 같은 전략들이 탐구되고 있습니다.
*   **통신 병목 현상(Communication Bottlenecks)**: 대규모 MoE 모델은 분산 훈련 환경에서 여러 GPU에 걸쳐 전문가 가중치가 분산됩니다. 토큰이 다른 GPU에 있는 전문가에게 라우팅될 때 발생하는 `GPU 간 통신(inter-GPU communication)`은 훈련 속도의 병목 현상을 유발할 수 있습니다. 이를 해결하기 위해 `통신 인식 라우팅(communication-aware routing)` (통신 비용이 적은 전문가를 우선적으로 선택), `전문가 그룹화(expert grouping)` (통신 비용이 높은 전문가들을 같은 GPU에 배치), 또는 `원격 메모리(remote memory)` 기술 등이 개발되고 있습니다.
*   **하드웨어 최적화**: MoE의 희소한 계산 패턴은 기존의 밀집 행렬 연산에 최적화된 하드웨어 아키텍처에서 비효율적일 수 있습니다. 따라서 MoE에 특화된 `맞춤형 하드웨어(custom hardware)` 가속기나 `소프트웨어 커널(software kernels)` (예: Triton, FlashAttention의 MoE 확장) 개발이 중요해지고 있습니다. 이는 MoE 모델의 실제 배포 및 추론 효율성을 극대화하는 데 필수적입니다.

이러한 도전 과제들을 극복하기 위한 활발한 연구가 진행 중이며, MoE의 잠재력을 완전히 실현하기 위해서는 아키텍처, 훈련 알고리즘, 그리고 하드웨어/소프트웨어 스택 전반에 걸친 통합적인 접근 방식이 필요합니다.

## 주요 MoE LLM 모델들의 혁신

최근 출시된 MoE 기반 LLM들은 아키텍처, 훈련 방식, 그리고 라우팅 전략에 걸쳐 다양한 혁신을 선보이며 MoE 기술의 발전 가능성을 입증했습니다.

### Mixtral과 Grok: 대규모 MoE의 선구자

```
[Image: Mixtral performance comparison]
```

(출처: [5])

Mixtral 8×7B (일명 Mixtral of Experts)는 영어, 프랑스어, 이탈리아어, 독일어, 스페인어에 능통한 오픈 소스 Mistral-7B 모델 [6]의 MoE 기반 확장입니다. 이 두 모델은 Apache 2.0 라이선스 하에 오픈 가중치(open weights)를 가지며, 모델에 대한 세부 정보를 제공하는 해당 기술 보고서도 있습니다. Mixtral은 Mistral의 모든 레이어를 8개의 전문가를 가진 전문가 레이어로 변환합니다. 이 전문가들 중 두 개가 각 토큰에 대해 활성화되며, 총 470억 개의 매개변수와 130억 개의 활성 매개변수를 가진 모델을 생성합니다. 이 모델은 또한 32K의 컨텍스트 길이(context length)를 가지며, 이는 비-MoE 모델보다 4배 더 큽니다. 위 그림에서 볼 수 있듯이, Mixtral은 전반적으로 Mistral을 능가하며 특히 코드 생성, 수학 및 다국어 벤치마크에서 뛰어난 성능을 보이며, 일부 경우에는 더 큰 LLaMA-2-70B 모델의 성능을 초과하기도 합니다. 그룹화된 쿼리 어텐션(GQA) [7]과 슬라이딩 윈도우 어텐션(SWA) [8]과 같은 효율적인 어텐션 메커니즘을 채택하여 긴 컨텍스트 처리 능력도 강화했습니다.

```
[Image: GQA diagram]
```

(출처: [7])

**Mistral-7B 아키텍처.** Mixtral 8×7B의 기본 LLM 아키텍처는 Mistral-7B [6]의 아키텍처 설정과 정확히 일치하는 디코더 전용 트랜스포머입니다. 표준 디코더 전용 LLM 아키텍처와 비교하여, Mistral-7B에 의해 몇 가지 변경 사항이 있습니다:

*   **그룹화된 쿼리 어텐션(Grouped-Query Attention, GQA) [7]**: 효율성을 개선하기 위해 셀프 어텐션 헤드(self-attention heads) 그룹 간에 키(key) 및 값(value) 투영(projections)을 공유합니다. 위를 참조하십시오.
*   **슬라이딩 윈도우 어텐션(Sliding Window Attention, SWA) [8]**: 각 토큰에 대해 크기 W 의 고정된 윈도우(window)에 걸쳐 (마스크된) 셀프 어텐션을 계산하여 LLM이 감소된 추론 비용 5 으로 임의 길이의 시퀀스를 처리할 수 있도록 합니다. 아래를 참조하십시오.

```
[Image: Sliding Window Attention diagram]
```

SWA를 사용하기 때문에, 모델은 롤링 버퍼(rolling buffer) / 순환 캐시(circular caches)와 같은 방법을 사용하여 KV 캐시(KV cache)를 더 메모리 효율적으로 만들거나 청크된 프리필(chunked prefill)을 사용하여 추론 속도를 높일 수 있습니다. Mixtral 8×7B는 동일한 아키텍처 관례를 채택합니다.

(출처: [6])

**추가 세부 정보.** 이전에 언급했듯이, Mixtral은 LLM의 모든 레이어를 전문가 레이어로 변환합니다. 각 전문가 레이어 내에서는 모든 토큰에 대해 선형 레이어(linear layer)의 상위 K 로짓에 소프트맥스를 취하는 간단한 라우팅 메커니즘이 채택됩니다 — 이는 이 개요의 시작 부분에서 논의된 라우팅 메커니즘과 일치합니다. 아래를 참조하십시오.

```
[Image: Mixtral routing mechanism]
```

(출처: [5])

[5]의 저자들은 Mixtral이 다국어 코퍼스(multilingual corpus)에 걸쳐 사전 훈련되어 모델이 여러 언어를 이해할 수 있다고 언급합니다. 아래에서 볼 수 있듯이, Mixtral은 다국어 벤치마크에서 LLaMA 모델을 보편적으로 능가합니다.

```
[Image: Mixtral multilingual performance]
```

(출처: [5])

**라우팅 분석.** 논문을 마무리하기 위해, [5]의 저자들은 여러 도메인에 걸쳐 토큰에 대해 전문가가 어떻게 선택되는지에 대한 상세한 분석을 수행하여 해석 가능한 패턴을 추론할 수 있는지 확인합니다. The Pile 내의 다양한 주제 영역에 대해 다른 전문가에게 할당된 토큰 분포를 플로팅할 때, 토큰 할당에서 명확한 패턴은 나타나지 않습니다. 아래를 참조하십시오.

```
[Image: Token distribution across experts by topic]
```

(출처: [5])

그러나 MoE는 일부 구조화된 동작을 보입니다. 예를 들어, 파이썬 코드의 "self"와 영어의 "Question"이라는 단어는 — 비록 여러 토큰으로 구성되어 있지만 — 종종 동일한 전문가를 통해 라우팅됩니다. 유사하게, 코드의 들여쓰기 토큰(indentation tokens)은 일반적으로 동일한 전문가에게 보내지며, 연속적인 시퀀스(consecutive sequences) — 서로 가까이 있는 토큰들의 시퀀스 —는 일반적으로 동일한 전문가에게 보내집니다. 아래를 참조하십시오. 이러한 결과는 i) 전문가가 주제별로 전문화되지 않지만 ii) MoE의 라우팅 메커니즘이 모델 입력의 구문(syntax) 또는 내용과 관련하여 일부 구조화된 동작을 따른다는 것을 나타냅니다.

```
[Image: Structured routing behavior]
```

(출처: [5])

**확장.** Mixtral 이후, Mixtral-8×22B 라는 더 큰 버전의 모델이 출시되었습니다. 이 모델은 총 1410억 개의 매개변수와 390억 개의 활성 매개변수를 가지며, 원래 Mixtral 모델보다 약 3배 더 큽니다. Mixtral-8×22B는 코딩 및 수학 작업에 특히 능숙하며, 64K로 확장된 컨텍스트 길이를 가지고 있고, 함수 호출(function calling)을 기본적으로 수행할 수 있습니다. Mixtral-8×22B의 다른 오픈 모델 대비 주요 이점은 아래에 요약되어 있습니다.

```
[Image: Mixtral-8x22B benefits summary]
```

( 출처 )

xAI의 Grok [9, 10] 역시 MoE 기반 LLM의 대표적인 사례로, 3140억 개의 매개변수 중 약 700~800억 개의 활성 매개변수를 갖는 대규모 모델입니다. Grok은 특히 추론 능력과 긴 컨텍스트 이해 능력에서 강점을 보이며, "건초 더미 속 바늘 찾기(needle in a haystack)" 테스트에서 128K 토큰의 시퀀스를 완벽하게 처리하는 능력을 입증했습니다. Grok의 연속적인 버전(Grok-1.5, Grok-2) 출시는 MoE 모델이 빠르게 발전하고 있음을 보여줍니다. 모델에 대한 상세한 기술 보고서는 없지만, MoE 기반 LLM의 가장 주목할 만한 최근 사례 중 하나는 xAI의 Grok입니다. 초기 Grok-1 모델은 2024년 초에 출시되었습니다. 연구원들은 이 모델이 각 토큰에 대해 25%의 가중치가 활성화되는 3140억 개의 매개변수를 가진 MoE(즉, 약 700억~800억 개의 활성 매개변수)라고 밝혔습니다. Grok-1의 아키텍처와 기본 모델 가중치는 Apache 2.0 라이선스 하에 오픈 소스화되었습니다. 그러나 이는 사전 훈련된 기본 모델이며, 모델의 후처리 훈련(post training) 과정에 대한 세부 정보는 제공되지 않았습니다.

```
[Image: Grok-1.5 performance comparison]
```

(출처: [10])

**Grok-1.5 [10].** Grok-1 6 의 초기 출시 직후, 더 나은 추론(reasoning) 및 긴 컨텍스트 이해(long context understanding) 기능을 갖춘 후속 버전의 모델이 발표되었습니다. 예를 들어, Grok-1.5는 수학 및 코딩 관련 작업에서 훨씬 더 나은 성능을 보입니다. 위를 참조하십시오.

"이 모델은 컨텍스트 윈도우(context window)가 확장됨에 따라 지시 따르기(instruction-following) 능력을 유지하면서 더 길고 복잡한 프롬프트(prompts)를 처리할 수 있습니다." - [10]에서

Grok-1.5는 건초 더미 속 바늘 찾기 테스트(needle in a haystack test)에서 완벽한 검색(retrieval)으로 최대 128K 토큰의 시퀀스를 처리할 수 있습니다. 아래를 참조하십시오. 저자들은 또한 모델이 많은 컨텍스트가 주어졌을 때 견고한 지시 따르기 능력을 유지한다고 언급하는데, 이는 순수한 검색 7 에 비해 긴 컨텍스트 능력의 훨씬 더 좋은 신호입니다.

```
[Image: Grok-1.5 needle in a haystack test]
```

(출처: [10])

Grok-1.5와 Grok-1이 이렇게 짧은 간격으로 출시되었다는 점을 감안할 때, Grok-1.5의 발전은 후처리 훈련에 의해 주도되었다고 추론할 수 있습니다 — 이 기간 동안 다른 사전 훈련된 기본 모델이 생성되었을 가능성은 극히 낮습니다.

**Grok-2.** 최근에는 Grok-2가 출시되었는데, 이는 챗봇 아레나(Chatbot Arena)에서 측정된 바와 같이 추론, 코딩 및 채팅 기능이 향상되었습니다. Grok-2는 또한 다양한 다른 작은 개선 사항(예: 도구 사용, 검색, 사실성 등)을 가지고 있으며, Grok-2의 증류 버전(distilled version)인 Grok-2-mini가 주 모델과 함께 출시되었습니다. 그러나 Grok-2의 아키텍처에 대한 공개된 세부 정보는 공유되지 않았습니다 — 이 모델은 처음부터 훈련되었을 가능성이 높으며 MoE 기반일 수도 있고 아닐 수도 있습니다.

### DBRX: 세분화된 전문가와 데이터 품질의 중요성

```
[Image: DBRX performance comparison]
```

(출처: [11])

Databricks Mosaic이 출시한 DBRX [11]는 `세분화된 MoE(fine-grained MoE)`의 이점을 극대화한 모델입니다. 모델의 두 가지 버전 — 기본 모델(DBRX base)과 미세 조정된 모델(DBRX Instruct) —이 오픈 라이선스(즉, Databricks 오픈 모델 라이선스) 하에 출시되었습니다. DBRX는 다음 사양을 가진 MoE 기반 LLM입니다:

*   360억 개의 활성 매개변수를 가진 총 1320억 개의 매개변수.
*   각 MoE 레이어에 16개의 전문가가 있으며, 각 토큰에 대해 4개의 전문가가 활성화됩니다.
*   최적화된 텍스트 12조 토큰으로 사전 훈련되었습니다.
*   사전 훈련 효율성에서 4배 향상.

각 MoE 레이어에 16개의 전문가를 두지만, 각 개별 전문가는 더 작게 설계하여 총 1320억 개의 매개변수 중 360억 개의 활성 매개변수를 가집니다. 이는 라우터가 선택할 수 있는 전문가 조합의 수를 크게 늘려 모델 품질 향상에 기여합니다. 가장 주목할 만한 점은 DBRX가 "세분화된(fine-grained)" MoE 모델이라는 것입니다. 즉, 이 모델은 각 MoE 레이어에 더 많은 수의 전문가를 사용하지만, 각 개별 전문가는 더 작습니다. 참고로, Mixtral과 Grok-1 모두 각 MoE 레이어 내에 8개의 전문가를 포함하며 — 이 중 두 개는 주어진 토큰에 대해 활성화됩니다. 세분화된 전문가를 사용함으로써, 각 MoE 레이어는 선택할 수 있는 더 많은 전문가 조합(특히 65배 더 많음)을 가지며, 이는 [11]에서 품질을 향상시키는 것으로 밝혀졌습니다.

**훈련 데이터.** DBRX의 사전 훈련 데이터셋은 매우 크지만 8 , [11]의 저자들은 데이터의 품질을 개선하는 데도 상당한 투자를 했습니다. 결과적으로 DBRX의 통계적 훈련 효율성(statistical training efficiency)은 일반적인 경우보다 높습니다 — 더 적은 토큰으로 더 높은 정확도를 달성하기 때문에 훈련이 더 빠릅니다. 더 구체적으로, [11]의 저자들은 새로운 데이터가 토큰당 2배 더 효율적이라고 추정하는데, 이는 절반 이하의 토큰으로 훈련하고도 동일한 수준의 성능을 달성할 수 있음을 의미합니다. 이 주장은 새로운 모델의 사전 훈련 데이터가 단독으로 미치는 영향을 테스트함으로써 검증되었습니다 (즉, 다른 사전 훈련 데이터를 가진 고정 모델을 사용).

DBRX의 또 다른 핵심 혁신은 `고품질 훈련 데이터셋`과 `커리큘럼 학습(curriculum learning)` 전략입니다. [11]의 연구진은 데이터의 품질 개선에 막대한 투자를 했으며, 훈련 과정 전반에 걸쳐 데이터 혼합을 동적으로 변경하는 커리큘럼 학습을 적용하여 통계적 훈련 효율성(statistical training efficiency)을 크게 높였습니다. 이는 더 적은 토큰으로도 높은 정확도를 달성하게 하여, DBRX가 이전 모델보다 4배 더 적은 계산량으로 훈련될 수 있도록 했습니다.

"단독으로 볼 때, 더 나은 사전 훈련 데이터는 모델 품질에 상당한 영향을 미쳤습니다. 우리는 DBRX 사전 훈련 데이터를 사용하여 1조 토큰으로 7B 모델을 훈련했습니다. 이는 MPT-7B의 30.9%에 비해 Databricks Gauntlet에서 39.0%에 도달했습니다." - [11]에서

또한, DBRX를 훈련하기 위해 커리큘럼 학습(curriculum learning)이 사용됩니다 — 사전 훈련 데이터의 혼합은 사전 훈련 과정 전반에 걸쳐 동적으로 변경됩니다. 이 커리큘럼 학습 전략의 세부 사항은 나중에 이 논문에서 설명되었습니다. DBRX가 사용하는 커리큘럼 학습 전략은 웹 크롤링(web-crawling)을 통해 얻은 데이터에 비해 품질이 더 높기 때문에 훈련 후반부에 더 작고 도메인별 데이터셋을 단순히 업샘플링(upsamples)합니다. 이 간단한 커리큘럼 학습 전략은 어려운 벤치마크에서 성능을 크게 향상시키는 것으로 밝혀졌습니다. 아래를 참조하십시오.

```
[Image: DBRX curriculum learning performance]
```

( 출처 )

**토크나이저(Tokenizer) 및 컨텍스트 윈도우(context window).** DBRX는 32K의 컨텍스트 길이를 가지며 GPT-4 토크나이저(tiktoken을 통해 사용 가능)를 사용합니다. 저자들에 따르면, GPT-4 토크나이저는 주로 성능 때문에 선택되었습니다. 이 토크나이저는 큰 어휘를 가지고 있으며 토큰 효율성이 매우 높아, 동일한 양의 텍스트를 더 적은 토큰으로 표현함으로써 디코딩(decoding) 및 훈련 속도를 자연스럽게 향상시킵니다.

```
[Image: DBRX tokenizer efficiency]
```

(출처: [11])

**효율성 이점.** DBRX의 제안은 사전 훈련 효율성 면에서 큰 개선을 가져옵니다. 지금까지 배운 것 외에도, [11]에서 언급된 효율성 향상의 추가적인 원천이 몇 가지 있습니다:

*   MoE 아키텍처는 소규모 실험에서 훈련 중 1.7배 더 적은 FLOPS를 필요로 하는 것으로 밝혀졌습니다.
*   디코더 전용 아키텍처에 대한 다른 수정 사항(즉, RoPE , GLU 활성화(GLU activation) 및 GQA ).
*   "더 나은 최적화 전략(optimization strategies)".

모든 데이터, 아키텍처 및 최적화 변경 사항을 고려할 때, DBRX의 종단 간(end-to-end) 훈련 프로세스는 이전 모델에 사용된 사전 훈련 파이프라인(pretraining pipeline)과 비교하여 4배 더 적은 계산량을 필요로 합니다. 이 수치를 결정하기 위해, [11]의 저자들은 DBRX의 더 작은 변형 모델을 이전 MPT-7B 모델과 비교하여, 더 작은 DBRX 모델이 훈련 중 3.7배 더 적은 FLOPS를 사용하면서 Databricks Gauntlet에서 유사한 성능을 달성한다는 것을 발견했습니다.

```
[Image: DBRX training efficiency comparison]
```

(출처: [21])

DBRX는 또한 추론 효율성(inference efficiency) 개선을 가져오는데 — 로드 테스트(load tests)에서 사용자당 초당 150 토큰으로 LLaMA-2-70B보다 최대 2배 더 빠릅니다. 이러한 측정은 TensorRT-LLM 및 16비트 정밀도(precision)를 사용하는 최적화된 서빙 인프라(serving infrastructure)를 사용하여 이루어졌으며, 이는 매우 빠릅니다. DBRX의 MoE 아키텍처는 비교적 적은 수의 활성 매개변수로 인해 추론 효율성에도 도움이 됩니다. 예를 들어, DBRX는 총 매개변수 및 활성 매개변수 모두에서 Grok-1 크기의 40%입니다.

"전문가 혼합 모델을 훈련하는 것은 어렵습니다. 우리는 DBRX급 모델을 효율적인 방식으로 반복적으로 훈련할 수 있을 만큼 견고한 파이프라인을 구축하기 위해 다양한 과학적 및 성능적 과제를 극복해야 했습니다." - [11]에서

MoE 훈련은 일반적으로 훈련 중 발생하는 불안정성, 통신 병목 현상(communication bottlenecks) 등으로 인해 어렵습니다. 그러나 DBRX는 [11]에 설명된 최적화된 사전 훈련 전략 덕분에 안정성, 효율성 및 성능 면에서 인상적인 결과를 달성합니다. 특히, 이러한 결과를 가능하게 하는 단일 변경 또는 발전은 없습니다. DBRX가 사용하는 인상적인 사전 훈련 파이프라인은 수많은 작고 실용적인 변경 사항에 의해 가능해졌습니다.

**경험적 평가.** 다른 오픈 LLM과 비교할 때, DBRX-Instruct는 Mixtral과 비교하여 복합 벤치마크에서 큰 차이로 더 나은 성능을 달성합니다. 아래를 참조하십시오. DBRX는 범용 LLM임에도 불구하고 인상적인 프로그래밍 기술을 가지고 있으며, Grok-1 (크기가 두 배 이상!)과 CodeLLaMA-70B와 같은 전문 코딩 모델까지 능가합니다. DBRX는 추론 및 수학 기반 작업에서도 좋은 성능을 보입니다.

```
[Image: DBRX-Instruct vs. other open LLMs]
```

(출처: [11])

폐쇄형 모델과 비교할 때, DBRX는 GPT-3.5의 성능을 능가하며 Gemini-1.0 Pro 와 경쟁할 만합니다. Gemini-1.0 Pro는 GSM8K에서만 DBRX보다 뛰어난 성능을 보이며, Mixtral-Medium은 고려되는 몇 가지 특정 작업에서 더 나은 성능을 보입니다. 아래를 참조하십시오. 높은 수준에서 볼 때, DBRX는 프로그래밍, 수학, 일반 지식, 상식 추론(commonsense reasoning) 및 검색/RAG(retrieval / RAG)에 능숙한 것으로 보입니다.

```
[Image: DBRX-Instruct vs. closed models]
```

(출처: [21])

### OpenMoE: 투명한 연구와 라우팅 분석의 중요성

OpenMoE [12]는 진정한 오픈 소스 MoE 모델의 부족 문제를 해결하기 위해 6억 5천만 개에서 340억 개에 이르는 다양한 크기의 MoE LLM 제품군을 공개했습니다. 이 프로젝트는 MoE 모델의 설계 선택(예: 인터리브된 MoE 레이어의 스트라이드)과 훈련 데이터 구성에 대한 상세한 분석을 제공하며, 특히 라우팅 동역학(routing dynamics)에 대한 심층적인 연구를 수행했습니다. OpenMoE는 전문가들이 특정 도메인에 전문화되는 경향이 적고, 토큰 라우팅이 주로 토큰 ID에 의해 결정되는 `컨텍스트 독립적 전문화(Context-Independent Specialization)` 현상을 밝혀냈습니다. 비록 SOTA 성능을 달성하지는 못했지만, OpenMoE는 MoE 연구의 투명성을 높이고 후속 연구를 위한 중요한 기반을 제공했다는 점에서 큰 의미를 가집니다. 언어 모델링 영역에서 MoE의 성공에도 불구하고, 코드, 정보, 데이터, 가중치 등이 모두 공개적으로 공유되는 진정한 오픈 소스 MoE의 수는 상대적으로 적습니다. 이 문제를 해결하기 위해 OpenMoE [12]는 6억 5천만 개에서 340억 개의 매개변수에 이르는 디코더 전용 MoE LLM 제품군을 훈련하기 위한 대규모 노력을 수행합니다. 이 모델들은 다양한 세분성(granularity)(즉, 16개 또는 32개의 전문가)을 가진 세분화된 전문가를 채택합니다. 이 노력의 결과는 [12]에 문서화되어 있으며 모든 모델은 공개적으로 공유됩니다. 저자들은 또한 그들의 결과를 재현하는 데 사용할 수 있는 잘 문서화된 코드 저장소(code repository)를 제공합니다.

[OpenMoE 저장소](https://github.com/OpenMoE/OpenMoE)

"모든 레이어에 MoE를 사용하는 것은 라우팅 중에 더 많은 계산 오버헤드(computational overhead)를 발생시키고 인터리브된 MoE 사용보다 더 나쁜 비용-효율성 절충점을 유발합니다." - [12]에서

**설계 선택.** OpenMoE 모델은 ST-MoE [3]의 설정을 채택하며, 동일한 라우팅 메커니즘과 활성 전문가 수(즉, k = 2 )를 포함합니다. 저자들은 4번째 또는 6번째 트랜스포머 블록만 MoE 레이어로 변환하기로 선택했으며, 더 큰 스트라이드가 비용과 효율성 면에서 더 나은 절충점을 제공한다는 것을 발견했습니다. OpenMoE에 사용된 사전 훈련 데이터셋은 코드의 높은 분포를 포함합니다. 사실, 사전 훈련 초기 단계에서는 코드가 데이터셋의 50% 이상을 차지했지만, 이 비율은 나중에 훈련에서 최적이 아니라는 이유로 조정되었습니다. 아래를 참조하십시오. 정렬(alignment)을 위해, OpenMoE는 사전 훈련 후 WildChat의 데이터를 사용하여 SFT(Supervised Fine-Tuning)를 거쳐 더 나은 지시 따르기 능력을 유도합니다.

```
[Image: OpenMoE pretraining data composition]
```

(출처: [12])

**라우팅 동역학(Routing dynamics).** OpenMoE의 주요 기여 중 하나는 모델 내에서 이루어진 라우팅 결정에 대한 상세한 분석입니다. 먼저, 이전 연구 [5]에서 보여진 결과와 유사하게 — 전문가들은 특정 도메인에 전문화되는 경향이 없음을 알 수 있습니다. 아래를 참조하십시오.

```
[Image: OpenMoE expert specialization by domain]
```

(출처: [12])

그러나 아래 그림에서 볼 수 있듯이, 자연어 및 특정 작업에 걸쳐 어느 정도의 전문가 전문화(expert specialization)가 나타납니다.

```
[Image: OpenMoE expert specialization by language/task]
```

(출처: [12])

그러나 이 경향을 더 깊이 파고들면, 토큰 라우팅의 동역학은 주로 토큰 ID(token ID)에 의해 결정된다는 것을 알 수 있습니다. 즉, 동일한 토큰은 그 토큰이 존재하는 컨텍스트와 상관없이 거의 항상 동일한 전문가에게 라우팅됩니다. 이 패턴은 [12]에서 "컨텍스트 독립적 전문화(Context-Independent Specialization)"라고 불립니다.

"이것은 매우 흥미로운 발견인데, 동일한 토큰 ID를 가진 토큰들이 다른 문장에서 매우 다양한 컨텍스트를 가지기 때문입니다. 예를 들어, 토큰 'an'은 'an apple' 또는 'another'의 일부일 수 있습니다. 그러나 이 모든 토큰들은 소수의 고정된 전문가에 대해 매우 강력한 전문화를 보입니다." - [12]에서

흥미롭게도, 전문가들은 선호하는 토큰에서 관찰 가능한 패턴을 가집니다. 아래를 참조하십시오. 예를 들어, "have", "has", "had"는 모두 동일한 전문가에게 라우팅되는 반면, 한 전문가는 "=", "and", "\n" 토큰을 받습니다 — 코딩 언어 내에서 매우 흔한 토큰들입니다. [12]에서 우리는 이러한 라우팅 패턴이 사전 훈련의 초기 단계에서 확고해지며 훈련 후반에는 거의 변하지 않는다는 것을 알 수 있습니다.

```
[Image: OpenMoE expert token preferences]
```

(출처: [12])

**라우팅 문제.** [12]에서 관찰된 라우팅 패턴 외에도, OpenMoE 모델이 성능을 손상시킬 수 있는 일부 라우팅 동작을 보인다는 것을 알 수 있습니다. 예를 들어, 모델은 시퀀스 후반에 토큰을 드롭하는 경향이 있는데, 이는 긴 시퀀스 작업(예: 다중 턴 채팅)의 성능을 손상시킬 수 있습니다.

```
[Image: OpenMoE multi-turn chat performance]
```

**OpenMoE 모델은 다중 턴 채팅 문제에서 더 나쁜 성능을 보입니다 (출처: [12])**

라우팅 동역학이 사전 훈련 과정의 초기 단계에서 고정되기 때문에, 이러한 동작은 후처리 훈련 중에 수정하기 어렵습니다. 사실, OpenMoE 모델은 사전 훈련과 SFT 9 중 데이터 간의 도메인 격차(domain gap)로 인해 일반적으로 어려움을 겪는 것으로 관찰됩니다 — 데이터 구성의 차이로 인해 토큰 라우팅 동역학이 불규칙해집니다. 이러한 문제를 해결하기 위해, [12]의 저자들은 지시 따르기 데이터를 사전 훈련 데이터셋에 혼합할 것을 권장합니다.

**모델 평가.** 전반적으로, OpenMoE 모델은 MoE LLM 중에서 새로운 최첨단 성능(state-of-the-art performance)을 달성하지 못했습니다 — [12]의 저자들은 이 사실을 공개적으로 밝히고 OpenMoE 모델의 성능이 더 나은 설계를 통해 크게 향상될 수 있음을 인정합니다. OpenMoE 모델의 더 큰 기여는 그들의 투명성입니다. [12]에서 공개적으로 공유된 세부 정보와 아티팩트(artifacts)는 이 주제에 대한 추가 연구를 수행하는 데 필요한 자원을 제공함으로써 MoE에 대한 오픈 연구 노력을 가속화할 수 있습니다.

### DeepSeek 시리즈: 아키텍처 및 훈련 효율성의 정점

최근 제안된 DeepSeek MoE 모델들, DeepSeek-v2 [14]와 DeepSeek-v3 [15]를 포함하여, 다양한 이유로 LLM 연구 커뮤니티 내에서 큰 반향을 일으켰습니다:

*   그들의 가중치는 공개적으로 공유됩니다.
*   많은 세부 정보를 공유하는 기술 보고서가 함께 제공됩니다.
*   그들의 성능은 인상적이며 — 많은 폐쇄형 모델과 동등합니다.
*   그들의 훈련 비용은 상당히 합리적입니다.

우리가 보게 될 것처럼, DeepSeek 모델은 훈련 효율성과 다운스트림 성능(downstream performance)을 모두 극대화하는 다양한 독특한 설계 선택을 합니다. DeepSeek-v2 [14]— 210억 개의 활성 매개변수를 가진 2360억 개의 매개변수 MoE —는 이후 DeepSeek-V3 모델에서 사용된 MoE 아키텍처를 제안합니다. DeepSeek MoE 모델은 성능과 효율성을 높이기 위해 기본 트랜스포머 블록을 약간 수정한다는 점에서 이전 모델들과는 다소 다릅니다. 아래에서 볼 수 있듯이, DeepSeek-v2 모델은 — 좋은 성능을 보이는 것 외에도 — 훈련 및 추론 효율성 관점에서 상당히 인상적이며, 훨씬 더 큰 DeepSeek-v3 모델의 강력한 출발점이 됩니다.

DeepSeek-v2 [14]와 DeepSeek-v3 [15]는 MoE 아키텍처와 훈련 효율성 측면에서 여러 독특한 설계 선택을 선보였습니다. DeepSeek-v2는 2360억 개의 매개변수 중 210억 개의 활성 매개변수를 가지며, `멀티 헤드 잠재 어텐션(Multi-head latent attention, MLA)`을 채택하여 KV 캐시 메모리 사용량을 93% 이상 절감했습니다. 또한, 세분화된 전문가와 공유 전문가를 결합한 독특한 MoE 레이어 구조를 사용하여 중복 정보를 최소화하고 전문화를 장려했습니다.

```
[Image: DeepSeek-v2 efficiency comparison]
```

(출처: [14])

**멀티 헤드 잠재 어텐션(Multi-head latent attention, MLA).** 표준 멀티 헤드 어텐션(multi-headed attention) 대신, DeepSeek-v2는 효율적인 어텐션 변형인 MLA를 채택합니다. 멀티 쿼리 어텐션(multi-query attention) 또는 그룹화된 쿼리 어텐션과 유사하게, MLA는 모델의 KV 캐시가 소비하는 메모리를 최소화하는 것을 목표로 합니다. 그러나 다른 효율적인 어텐션 변형과 달리, MLA는 상당한 성능 절충점(performance tradeoff)을 가지지 않습니다.

```
[Image: MLA mechanism]
```

(출처: [14])

특히, 이러한 메모리 효율성 증가는 모든 키(key) 및 값(value) 벡터를 훨씬 더 작은 (잠재) 벡터로 표현할 수 있게 하는 저랭크(low-rank) 공동 투영(joint projection)을 통해 달성됩니다. 위를 참조하십시오. 이 벡터를 업샘플링(upsample)할 수 있습니다 — 단순히 선형적으로 투영하여 여러 개의 더 큰 벡터를 형성함 — 전체 키 및 값 벡터를 복원하기 위해, 그러나 우리는 KV 캐시에 잠재 벡터만 저장하면 되므로 메모리 소비를 크게 줄일 수 있습니다. MLA를 채택하면 DeepSeek-v2의 KV 캐시 크기가 670억 개의 매개변수를 가진 밀집 모델에 비해 93% 이상 감소합니다.

**DeepSeek MoE 아키텍처.** MLA 사용 외에도, DeepSeek 모델은 독특한 MoE 레이어 구조를 채택합니다. DBRX와 유사하게, 이 모델들은 세분화된 전문가를 사용합니다. 그러나 이 전문가들 중 일부는 공유됩니다. 이러한 구조를 채택하는 동기는 전문가들 간의 중복 정보를 최소화하면서 더 많은 수의 전문가들 사이에서 전문화를 장려하는 것입니다. DeepSeek 모델이 사용하는 블록 구조의 전체 개략도는 아래에 제공됩니다.

```
[Image: DeepSeek MoE block structure]
```

(출처: [14])

[14]의 저자들은 DeepSeek-v2가 사용하는 세분화된 전문가를 처리하기 위한 흥미로운 로드 밸런싱 전략도 채택합니다. [2]에서 제안된 보조 로드 밸런싱 손실을 사용하는 것 외에도, DeepSeek-v2는 분산 훈련(distributed training) 중 장치 간 통신 균형을 맞추는 것을 목표로 하는 두 가지 보조 손실 항을 가집니다.

```
[Mathematical formula for device-level load balancing auxiliary loss]
```

**장치 수준 로드 밸런싱 보조 손실 (출처: [14])**

세분화된 전문가를 사용하는 것은 각 토큰을 더 많은 수의 전문가에게 전달해야 함을 의미합니다. 분산 훈련 설정에서, 전문가들은 다른 장치에 있을 수 있으며 각 장치에는 여러 전문가가 상주합니다. 장치 간 통신과 계산이 균형을 이루도록 보장하기 위해, i) 전문가를 상주하는 장치별로 그룹화하고 ii) MoE가 장치별로 균형 잡힌 라우팅을 수행하도록 장려하는 추가 보조 손실이 필요합니다. 예를 들어, 위에 표시된 보조 손실은 장치 간 균형 잡힌 계산을 장려합니다. [14]에는 장치 간 균형 잡힌 통신을 장려하기 위한 추가 손실도 제안되어 있습니다.

```
[Image: DeepSeek-v3 performance comparison]
```

(출처: [15])

DeepSeek-v3 [15]는 DeepSeek-v2 10 의 훨씬 더 큰 버전으로, 총 6710억 개의 매개변수와 370억 개의 활성 매개변수를 가집니다. 이 더 큰 모델은 14.8조 토큰으로 구성된 방대한 코퍼스에서 사전 훈련되었습니다. 사전 훈련 후, 다단계 후처리 훈련 파이프라인(multi-phase post training pipeline)이 적용됩니다:

*   모델은 먼저 두 단계의 컨텍스트 확장 절차를 거치는데, SFT를 통해 최대 컨텍스트 길이가 32K가 되도록 미세 조정된 다음, 다시 128K의 컨텍스트 길이를 가지도록 미세 조정됩니다.
*   컨텍스트 확장 후, 모델은 인간의 선호도에 맞추기 위해 추가 SFT 및 RLHF(Reinforcement Learning from Human Feedback)를 거칩니다.
*   최근 제안된 R1 추론 모델의 기능도 후처리 훈련 중에 DeepSeek-v2에 증류(distilled)됩니다.

최종 DeepSeek-v3 모델은 폐쇄형 모델을 능가하며 심지어 최고의 폐쇄형 LLM과 유사한 성능을 달성합니다. 위를 참조하십시오. DeepSeek-v3는 또한 MoE의 훈련 및 로드 밸런싱 전략에 여러 수정을 가하여 모델의 훈련 프로세스를 효율적이고 안정적으로 만듭니다.

DeepSeek-v3는 DeepSeek-v2의 확장 버전으로, 6710억 개의 매개변수와 370억 개의 활성 매개변수를 가지며 14.8조 토큰의 방대한 코퍼스에서 사전 훈련되었습니다. DeepSeek-v3의 주요 혁신 중 하나는 `다중 토큰 예측(Multi-Token Prediction, MTP)` 훈련 목표입니다. 이는 시퀀스 내 각 토큰에 대해 여러 미래 토큰을 예측함으로써 모델에 더 풍부한 훈련 신호를 제공하여 훈련 효율성과 성능을 향상시켰습니다. 또한, DeepSeek-v3는 보조 손실 없이 전문가별 편향 항을 조절하는 로드 밸런싱 전략과 `FP8 혼합 정밀도 훈련(FP8 mixed precision training)`을 활용하여 훈련 비용을 획기적으로 절감했습니다.

"뛰어난 성능에도 불구하고, DeepSeek-V3는 전체 훈련을 위해 2.788M H800 GPU 시간만 필요합니다. 또한, 훈련 과정은 놀랍도록 안정적입니다… 우리는 복구 불가능한 손실 급증을 경험하거나 롤백(rollbacks)을 수행하지 않았습니다." - [15]에서

DeepSeek-v3의 아키텍처는 그 전작에서 영감을 받았습니다. 예를 들어, MLA, 세분화된 전문가 및 공유 전문가는 모두 DeepSeek-v3에서 사용됩니다. 그러나 DeepSeek-v2와 달리 DeepSeek-v3는 다중 토큰 예측(Multi-Token Prediction, MTP) 훈련 목표를 사용합니다. 이 목표는 LLM 훈련에 거의 보편적으로 사용되는 지도 학습(supervised), 교차 엔트로피(cross entropy) 기반 다음 토큰 예측(next token prediction) 목표의 확장입니다. 시퀀스 내 각 토큰에 대해 다음 토큰을 예측하는 대신, MTP는 D 개의 미래 토큰을 예측합니다. 이러한 예측은 모델 아키텍처에 추가된 일련의 추가 모듈(additional modules)에 의해 순차적으로 이루어집니다. 아래를 참조하십시오.

```
[Image: Multi-Token Prediction (MTP) objective]
```

(출처: [15])

여러 미래 토큰이 예측되면, 우리는 교차 엔트로피 손실을 정상적으로 적용할 수 있습니다. MTP를 통해 예측된 여러 미래 토큰에 이 손실을 적용하면 모델에 더 풍부한 훈련 신호(training signal)를 제공하여 훈련 효율성과 전반적인 성능을 향상시킵니다. 더 나아가, MTP에 사용되는 이러한 추가 모듈은 추측 디코딩(speculative decoding)을 통해 추론 효율성을 향상시키는 데도 사용될 수 있습니다. 그러나 [15]의 저자들은 MTP 전략이 순전히 모델 성능 향상을 위해 사용되며 — 추가 모듈은 훈련 후 폐기된다고 명시합니다.

```
[Mathematical formula for auxiliary-loss-free load balancing strategy]
```

**보조 손실 없는 로드 밸런싱 전략 (출처: [15])**

DeepSeek-v3는 또한 보조 손실 없는 로드 밸런싱 전략을 사용하는데, 이는 단순히 상위 K 전문가 선택에 전문가별 편향 항(per-expert bias term)을 추가합니다. 위를 참조하십시오. 각 반복마다, 각 전문가에 대한 편향 항은 해당 전문가가 각각 과소 로드(underloaded)되었는지 과부하(overloaded)되었는지에 따라 고정된 계수 γ 만큼 증가하거나 감소합니다. 중요하게도, 이러한 편향은 상위 K 전문가를 선택할 때만 사용되며 — 라우터 내에서 전문가 확률 계산에 영향을 미치지 않습니다. 이 접근 방식은 MoE 내에서 전문가 활용을 효과적으로 균형 있게 만들고 로드 밸런싱 손실 사용으로 인한 성능 저하를 제거하는 것으로 밝혀졌습니다. 그러나 [15]의 저자들은 DeepSeek-v3를 훈련할 때 여전히 보조 로드 밸런싱 손실(매우 낮은 스케일링 계수(scaling factor)와 함께)을 사용한다고 언급합니다.

```
[Image: DeepSeek-v3 training cost]
```

(출처: [15])

**훈련 효율성.** 위에 설명된 전략들의 효율성 및 성능 이점 덕분에, DeepSeek-v3는 믿을 수 없을 정도로 경제적입니다. 게다가, 이 모델은 새로운 FP8 혼합 정밀도 훈련 프레임워크(FP8 mixed precision training framework)를 사용하여 훈련되었으며, 이는 대규모 LLM을 위한 8비트 훈련의 첫 번째 검증을 의미합니다. 총체적으로, 최종 모델 훈련 비용은 약 560만 달러로 추정되었습니다 11 . 위를 참조하십시오. 요약하자면, DeepSeek-v3는 다음과 같습니다:

*   매우 경제적인 방식으로 훈련되었으며 (FP8 훈련 및 MTP와 같은 여러 새로운 발전과 함께!)
*   오픈 모델로서는 믿을 수 없을 정도로 인상적이며 — 심지어 최고의 폐쇄형 LLM과도 매우 경쟁적입니다.
*   여러 새로운 수정 사항을 포함하는 흥미로운 MoE 아키텍처를 기반으로 합니다.

**추론.** DeepSeek-v3는 또한 최근 출시된 오픈 추론 모델인 DeepSeek-R1 [13]의 기본 모델 역할을 합니다. 간단히 말해, R1은 OpenAI가 최근 탐구한 o1 스타일 모델의 오픈 복제본입니다. 상세한 기술 보고서에서 설명된 바와 같이, 이 모델은 순수한 강화 학습(reinforcement learning)을 사용하여 극도로 긴 사고의 사슬(chains of thought)을 만들어 복잡한 (검증 가능한) 추론 작업을 해결하는 방법을 학습합니다. 아래 그림에서 볼 수 있듯이, R1의 성능은 특히 오픈 모델로서는 상당히 인상적입니다. 그러나 R1의 기능은 믿을 수 없을 정도로 유능한 기본 모델에 먼저 접근할 수 없었다면 불가능했을 것입니다.

```
[Image: DeepSeek-R1 performance]
```

(출처: [13])

## MoE의 미래 전망

MoE는 LLM의 확장성과 효율성을 동시에 해결하는 핵심 기술로 자리매김하고 있습니다. 앞으로 MoE 연구는 다음과 같은 방향으로 발전할 것으로 예상됩니다.

*   **완전 동적 MoE**: 현재의 MoE 모델은 여전히 정적인 전문가 수와 라우팅 규칙에 의존하는 경향이 있습니다. 미래에는 모델이 입력의 복잡성이나 작업의 요구 사항에 따라 전문가의 수, 구성, 또는 라우팅 메커니즘을 `완전히 동적으로 조절`하는 방향으로 발전할 것입니다. 이는 모델이 자원을 더욱 효율적으로 사용하고, 다양한 시나리오에 유연하게 대응할 수 있게 할 것입니다.
*   **MoE 전용 하드웨어 및 소프트웨어 스택**: MoE의 희소한 계산 패턴은 기존 하드웨어에서 비효율적일 수 있으므로, `MoE에 최적화된 프로세서(MoE-specific processors)`나 `가속기 아키텍처` 개발이 가속화될 것입니다. 또한, MoE 훈련 및 추론을 위한 `저수준 소프트웨어 라이브러리`와 `컴파일러 최적화`가 더욱 발전하여 개발자들이 MoE를 쉽게 활용할 수 있는 환경이 조성될 것입니다.
*   **멀티모달 및 멀티태스킹 MoE**: MoE는 텍스트뿐만 아니라 이미지, 오디오, 비디오 등 다양한 양식의 데이터를 처리하는 `멀티모달 모델`에 더욱 효과적으로 통합될 것입니다. 각 양식이나 특정 작업에 특화된 전문가를 활용하여 `멀티태스킹(multi-tasking)` 능력을 강화하고, 모델의 일반화 성능을 향상시킬 수 있습니다.
*   **경량 MoE 및 엣지 디바이스 배포**: 대규모 MoE 모델은 여전히 많은 리소스를 필요로 하지만, `경량화된 MoE 아키텍처`나 `지식 증류(knowledge distillation)` 기법을 통해 MoE의 이점을 `엣지 디바이스(edge devices)`에서도 활용하려는 연구가 활발해질 것입니다. 이는 MoE 기술의 적용 범위를 더욱 확장할 것입니다.

## 결론

전문가 혼합(MoE) 아키텍처는 대규모 언어 모델의 지속적인 발전을 위한 핵심 동력으로 부상했습니다. 이들은 계산 자원의 급격한 증가 없이 모델의 크기와 표현 능력을 확장하고, 훈련 및 추론 효율성을 향상시키는 독특한 이점을 제공합니다. 초기 MoE 모델이 겪었던 복잡성과 훈련 불안정성이라는 과제들은 지난 몇 년간의 연구를 통해 상당 부분 해결되었습니다. 정교한 라우팅 알고리즘, 효과적인 보조 손실, 그리고 DeepSeek과 같은 모델에서 선보인 혁신적인 훈련 전략들은 MoE를 실용적이고 강력한 기술로 변화시켰습니다.

MoE는 언어 모델링에 특히 적합한 많은 이점을 가집니다. 이들은 계산량의 급격한 증가 없이 더 큰 규모의 탐색을 가능하게 하고, 훈련 비용을 줄이며, 효율적으로 호스팅될 수 있습니다. 희소성이라는 아이디어가 기계 학습 문헌 내에서 오랫동안 존재해 왔지만, MoE는 희소성의 특히 영향력 있는 구현(instantiation)입니다. 이들은 현대 하드웨어와 호환되며 GPU에서 실용적으로 구현될 수 있는 방식으로 희소성을 활용합니다. 흥미롭게도, 초기 MoE 변형 모델들은 복잡성, 불안정성 및 사용의 어려움으로 인해 채택에 어려움을 겪었습니다. 그러나 이 개요에서 본 발전들은 MoE를 실용적이고 영향력 있는 것으로 만들었습니다 — 디코더 전용 트랜스포머 아키텍처의 간단하고 유망한 확장입니다.

Mixtral, Grok, DBRX, DeepSeek 시리즈와 같은 최신 MoE LLM들은 성능, 효율성, 그리고 확장성 측면에서 밀집 모델의 한계를 뛰어넘는 인상적인 결과를 보여주었습니다. 이들은 단순한 매개변수 증가를 넘어, 조건부 계산이라는 패러다임을 통해 LLM이 더욱 지능적이고 다양한 작업에 능숙해질 수 있음을 입증했습니다. MoE는 이제 LLM 연구의 주류 아키텍처 중 하나로 확고히 자리 잡았으며, 앞으로 더욱 발전된 형태로 AI 기술의 미래를 이끌어갈 것입니다.

뉴스레터가 처음이신가요? 안녕하세요! 저는 Cameron R. Wolfe 이고, 넷플릭스(Netflix)의 딥러닝(Deep Learning) 박사이자 머신러닝 과학자(Machine Learning Scientist)입니다. 이것은 독자들이 AI 연구의 중요한 주제를 더 잘 이해하도록 돕는 딥(러닝) 포커스 뉴스레터입니다. 뉴스레터가 마음에 드시면 구독하고, 공유하거나, X와 LinkedIn에서 저를 팔로우해주세요!

[구독]

## 참고 문헌

[1] Shazeer, Noam, et al. "터무니없이 큰 신경망: 희소하게 게이팅된 전문가 혼합 레이어(Outrageously large neural networks: The sparsely-gated mixture-of-experts layer)." arXiv preprint arXiv:1701.06538 (2017).
[2] Fedus, William, Barret Zoph, and Noam Shazeer. "스위치 트랜스포머: 간단하고 효율적인 희소성으로 조 단위 매개변수 모델로 확장(Switch transformers: Scaling to trillion parameter models with simple and efficient sparsity)." Journal of Machine Learning Research 23.120 (2022): 1-39.
[3] Zoph, Barret, et al. "ST-MoE: 안정적이고 전이 가능한 희소 전문가 모델 설계(St-moe: Designing stable and transferable sparse expert models)." arXiv preprint arXiv:2202.08906 (2022).
[5] Jiang, Albert Q., et al. "전문가들의 믹스트랄(Mixtral of experts)." arXiv preprint arXiv:2401.04088 (2024).
[6] Jiang, Albert Q., et al. "미스트랄 7B(Mistral 7B)." arXiv preprint arXiv:2310.06825 (2023).
[7] Ainslie, Joshua, et al. "GQA: 멀티 헤드 체크포인트에서 일반화된 멀티 쿼리 트랜스포머 모델 훈련(GQA: Training Generalized Multi-Query Transformer Models from Multi-Head Checkpoints)." arXiv preprint arXiv:2305.13245 (2023).
[8] Beltagy, Iz, Matthew E. Peters, and Arman Cohan. "롱포머: 긴 문서 트랜스포머(Longformer: The long-document transformer)." arXiv preprint arXiv:2004.05150 (2020).
[9] xAI. “Grok-1 오픈 릴리스(Open Release of Grok-1)” https://x.ai/blog/grok-os (2024).
[10] xAI. “Grok-1.5 발표(Announcing Grok-1.5)” https://x.ai/blog/grok-1.5 (2024).
[11] Mosaic Research (Databricks). “DBRX 소개: 새로운 최첨단 오픈 LLM(Introducing DBRX: A New State-of-the-Art Open LLM)” https://www.databricks.com/blog/introducing-dbrx-new-state-art-open-llm (2024).
[12] Xue, Fuzhao, et al. "OpenMoE: 오픈 전문가 혼합 언어 모델에 대한 초기 노력(Openmoe: An early effort on open mixture-of-experts language models)." arXiv preprint arXiv:2402.01739 (2024).
[13] Guo, Daya, et al. "DeepSeek-R1: 강화 학습을 통한 LLM의 추론 능력 장려(DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning)." arXiv preprint arXiv:2501.12948 (2025).
[14] Liu, Aixin, et al. "DeepSeek-v2: 강력하고 경제적이며 효율적인 전문가 혼합 언어 모델(Deepseek-v2: A strong, economical, and efficient mixture-of-experts language model)." arXiv preprint arXiv:2405.04434 (2024).
[15] Liu, Aixin, et al. "DeepSeek-v3 기술 보고서(Deepseek-v3 technical report)." arXiv preprint arXiv:2412.19437 (2024).