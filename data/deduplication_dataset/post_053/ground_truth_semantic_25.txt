(출처: [2, 5, 14]) 끊임없이 발전하는 연구 영역에서, 단방향 변환기(decoder-only transformer) 구조는 거대 언어 모델(LLM) 연구의 핵심 기둥 중 하나로 확고히 자리 잡고 있습니다. 이 설계는 최초의 GPT 모델이 소개된 이후로 꾸준히 활용되어 왔으며, 성능 개선을 위한 미세한 조정 외에는 그 근본적인 형태가 크게 변하지 않았습니다. 그러나 이 구조에 적용될 수 있는 가장 중요한 변화 중 하나는 바로 전문가 혼합(Mixture-of-Experts, MoE) 계층의 도입입니다.

"MoE 아키텍처는 밀집 모델(dense model)이 통상적으로 달성하는 것보다 모델의 성능과 추론 과정의 효율성(inference efficiency) 사이에서 더 유리한 균형점(tradeoff)을 제공합니다." - [11]에서

MoE 기반 LLM은 모델의 구조에 희소성(sparsity) 원리를 적용하여, 전체 매개변수(total parameters)의 양을 계산 비용(compute costs)의 비례적 증가 없이 대폭 확장할 수 있도록 합니다. Grok [9]와 DeepSeek-v3 [15] 같은 최신 모델에서 성공적으로 구현된 이 혁신은 극도로 방대한 모델을 개발하고 운용하는 과정을 훨씬 더 실현 가능하고 계산적으로 효율적인 방식으로 전환시켰습니다. 본 개요에서는 MoE의 핵심 원리를 학습하고, 이 개념이 최근 어떻게 강력한 LLM을 구축하는 데 활용되었는지 탐구할 것입니다. 또한, MoE가 현대 AI 연구에서 왜 그토록 중요한 위치를 차지하게 되었는지, 그리고 그 기술적 배경과 미래 전망에 대해서도 심도 있게 다룰 예정입니다.

## LLM을 위한 MoE의 핵심 개념

본 개요에서 논의할 MoE 기반 LLM들은 단방향 변환기(decoder-only transformer) 구조를 기반으로 합니다. 해당 구조의 세부 사항은 이 글에서 깊이 다루지 않지만, 익숙하지 않다면 관련 자료를 참고하시기 바랍니다. 단방향 변환기는 정규화(normalization)(예: 층 정규화(layer normalization) 또는 RMS 층 정규화(RMS layer normalization)), 마스크가 적용된 다중 헤드 자기 주의(masked multi-headed self-attention) 또는 피드포워드 변환(feed-forward transformation), 그리고 잔여 연결(residual connection)을 포함하는 반복적인 구성 단위로 이루어져 있습니다. 아래 그림을 참조하십시오.

```
[Image: Decoder-only transformer architecture diagram]
```

**단방향 변환기 구조**

이 부분에서는 MoE의 기본 원리를 상세히 설명할 것입니다. 이 설명은 i) 표준 MoE 계층을 제안하고 ii) 이 아이디어를 변환기(transformer) 구조에 적용하도록 확장한 선구적인 연구 논문들을 바탕으로 합니다. 이들 논문은 다음과 같습니다:

*   **희소하게 게이팅된 전문가 혼합 계층(The Sparsely-Gated Mixture-of-Experts Layer) [1]**
*   **스위치 트랜스포머(Switch Transformers) [2]**
*   **안정적이고 전이 가능한 전문가 혼합(Stable and Transferable Mixture-of-Experts, ST-MoE) [3]**

이들 논문과 MoE 구조의 역사적 배경에 대한 더 깊이 있는 분석은 아래의 해당 아이디어들에 대한 상세한 개요에서 찾아볼 수 있습니다.

**전문가 혼합(MoE): 조건부 계산(Conditional Computation)의 탄생과 부상**
Cameron R. Wolfe, Ph.D. · 2024년 3월 18일
[전체 이야기 읽기]

**간략한 배경 지식.** MoE와 라우팅 알고리즘(routing algorithms)을 이해하기 위해서는 먼저 디코더 변환기(decoder-transformer) (및 각 계층)의 입력 데이터 구조를 파악해야 합니다. 물론 LLM은 텍스트를 입력으로 받지만, 이 텍스트는 모델이 처리하기 전에 광범위한 전처리를 거칩니다. 첫째로 텍스트는 토큰화(tokenized)됩니다 (아래 그림 참조) — 즉, 불연속적인 토큰(discrete tokens)들의 목록으로 변환됩니다. 이러한 토큰들은 단어와 하위 단어(sub-words) 단위를 포함합니다. LLM은 미리 정의되고 훈련된 고정된 토큰 집합을 가지고 있으며, 이를 모델의 "어휘(vocabulary)"라고 부릅니다. 어휘의 크기는 모델마다 상이하지만, 대략 64,000개에서 256,000개 사이의 토큰 크기가 일반적입니다. 이러한 토큰화 과정은 인간 언어의 복잡성을 모델이 이해할 수 있는 이산적인 단위로 분해하는 첫걸음이며, LLM이 문맥을 파악하고 의미를 생성하는 데 필수적인 기반을 제공합니다.

```
[Image: Tokenizing and vectoring text for an LLM]
```

**LLM을 위한 텍스트 토큰화 및 벡터화**

텍스트가 토큰으로 변환된 후, 입력 내의 각 토큰은 벡터 형태로 표현됩니다. 어휘집을 갖는 것 외에도, LLM은 어휘에 포함된 모든 토큰에 대한 (훈련된 1 ) 벡터 임베딩(vector embedding)을 저장하는 토큰 임베딩 계층(token embedding layer)을 포함합니다. 이 계층에서 각 토큰에 해당하는 벡터를 추출하여 입력 행렬(input matrix)을 구성할 수 있습니다. 만약 각 토큰 임베딩이 d 차원이고 입력에 총 C개의 토큰이 있다면, 이 입력 행렬의 전체 크기는 C x d가 됩니다. 아래 그림을 참조하십시오. 이러한 임베딩은 단순히 고유한 식별자가 아니라, 각 토큰의 의미적, 문법적 특성을 고차원 공간에 압축하여 표현함으로써 모델이 단어 간의 관계를 학습하고 복잡한 언어 패턴을 인식할 수 있도록 돕습니다.

```
[Image: Input matrix of token vectors]
```

**토큰 벡터의 입력 행렬**

변환기(transformer)의 각 계층 — 그리고 모든 변환기 블록(transformer block) 내의 각 하위 계층(sub-layer) —은 이러한 입력의 차원(크기)을 보존합니다. 결과적으로, 변환기의 모든 피드포워드(feed-forward) 또는 어텐션 모듈(attention module)에 대한 입력(및 출력)은 이와 동일한 크기의 행렬입니다! 이 균일한 크기 유지는 정보 흐름의 일관성을 보장하며, 복잡한 계산이 여러 계층을 거쳐도 데이터 구조가 안정적으로 유지되도록 합니다.

## "전문가(experts)"란 무엇인가요?

단방향 변환기 구조에서 MoE가 도입하는 핵심적인 변형은 변환기 블록의 피드포워드 구성 요소 내에 있습니다. 표준 구조에서는 비선형 활성화(non-linear activation) 함수가 중간에 삽입된 두 개의 피드포워드 계층으로 일반적으로 이루어진 단일 피드포워드 신경망(feed-forward neural network)을 통해 모든 토큰이 개별적으로 처리됩니다. 아래 그림을 참조하십시오. 이 단일 경로는 모든 입력 토큰에 대해 동일한 변환을 적용하며, 이는 모델의 표현 능력을 제한할 수 있습니다.

```
[Image: Standard feed-forward network in a transformer block]
```

MoE는 이 블록 구조를 미묘하게 변경합니다. 블록의 피드포워드 요소 내에 하나의 피드포워드 네트워크를 두는 대신, 우리는 여러 개의 피드포워드 네트워크를 생성하며, 각 네트워크는 자신만의 독립적인 가중치(independent weights)를 가집니다. 우리는 이들 각각의 네트워크를 "전문가(expert)"라고 칭합니다. 예를 들어, MoE 기반 LLM은 각 피드포워드 하위 계층에 8개의 독립적인 전문가를 포함할 수 있습니다. 이러한 다수의 전문가들은 모델이 입력 데이터의 다양한 측면에 대해 전문화된 지식을 학습할 수 있도록 하며, 이는 모델의 전체적인 용량과 유연성을 크게 향상시킵니다.

```
[Image: Experts within a transformer layer]
```

변환기 계층 내의 전문가들은 위에서 제시된 방식대로 정의될 수 있습니다. 한 계층에 N개의 전문가가 존재하며, i번째 전문가는 E_i라는 표기법으로 언급될 수 있습니다. 각 전문가는 특정 유형의 패턴이나 정보 처리에 특화되어, 전체 시스템이 훨씬 더 복잡하고 미묘한 작업을 수행할 수 있도록 합니다.

## MoE 기반 변환기 구성.

MoE 기반 단방향 변환기 구조를 구축하려면, 변환기의 피드포워드 계층을 MoE — 즉, 전문가 — 계층으로 전환하기만 하면 됩니다. MoE 계층 내의 각 전문가는 해당 계층의 원래 피드포워드 네트워크와 동일한 구조를 가집니다 — 우리는 단순히 원래 피드포워드 네트워크의 여러 독립적인 복사본을 배치하는 것입니다. 아래 그림을 참조하십시오. 이 과정은 모델의 기본 계산 단위를 유지하면서도, 각 계산 단계에서 다양한 전문성을 활용할 수 있는 잠재력을 부여합니다.

```
[Image: Adding experts to a decoder-only transformer block]
```

**단방향 변환기 블록에 전문가 추가 (출처: [2])**

그러나 변환기의 모든 피드포워드 계층에 전문가를 사용할 필요는 없습니다. 대부분의 MoE 기반 LLM은 P의 간격(stride)을 사용하는데, 이는 P번째 계층마다 전문가 계층으로 변환되고 다른 계층은 본래의 형태로 유지됨을 의미합니다 — 이들을 "인터리브된(interleaved)" MoE 계층이라고 합니다. 이 접근 방식은 결과 모델의 성능과 효율성 사이에서 더 우수한 균형을 달성하는 데 활용될 수 있습니다. 모든 계층에 전문가를 두는 것은 계산 비용을 불필요하게 증가시킬 수 있으므로, 전략적인 배치를 통해 최적의 효과를 얻는 것이 중요합니다.

## 라우팅 알고리즘(Routing Algorithms)

MoE 기반 구조의 핵심적인 이점 중 하나는 효율성이지만, 단순히 전문가를 많이 사용하는 것만으로는 효율성이 증대되지 않습니다! 사실, 모델의 각 계층에 더 많은 전문가를 추가하면 모델의 총 매개변수 수 — 그리고 필요한 계산량 —가 상당히 증가합니다. 구조를 더 효율적으로 만들기 위해서는 각 계층에서 활용될 전문가들을 희소하게(sparsely) 선별해야 합니다! 즉, 모든 토큰이 모든 전문가를 거치게 하는 것이 아니라, 특정 토큰에 가장 적합한 소수의 전문가만을 활성화시키는 방식이 필요합니다.

**전문가 선별.** d 차원 토큰 벡터(token vector)로 표현되는 단일 토큰을 가정해 봅시다. 우리의 목표는 이 토큰을 처리할 전문가들의 부분 집합(크기 k)을 결정하는 것입니다. MoE 문헌에서는 일반적으로 토큰이 이 전문가들에게 "경로 지정(routed)"될 것이라고 표현합니다. 이러한 경로 지정 작업을 계산하고 최적화할 수 있는 알고리즘이 필수적입니다.

가장 기본적인 라우팅 알고리즘은 토큰 벡터에 선형 변환(linear transformation)을 적용하여 N 크기(즉, 전문가 수)의 벡터를 생성하는 것입니다. 그 다음, 소프트맥스 함수(softmax function)를 적용하여 우리 토큰에 대한 전문가 집합에 걸쳐 확률 분포(probability distribution)를 형성할 수 있습니다. 이 분포를 활용하여 상위 K개의 전문가를 단순히 선택함으로써 우리 토큰이 경로 지정되어야 할 전문가를 결정할 수 있습니다.

```
[Image: Computing output of routing mechanism]
```

**라우팅 메커니즘의 출력 계산**

이러한 라우팅 전략은 오늘날 우리가 사용하는 희소 MoE 계층 구조를 제안한 논문 [1]에서 처음 사용되었습니다. 위 그림을 참조하십시오. 그러나 이러한 라우팅 메커니즘은 전문가들의 균형 잡힌 선택을 명시적으로 촉진하지 않습니다. 이로 인해, 모델은 아래에서 설명하는 바와 같이 전문가 계층을 완전히 그리고 균일하게 활용하는 대신, 모든 토큰에 대해 동일한 소수의 전문가를 반복적으로 선택하는 경향이 발생할 수 있습니다. 이러한 현상은 통상적으로 "라우팅 붕괴(routing collapse)"라고 불립니다. 이는 모델이 특정 전문가에게 과도하게 의존하게 되어 다른 전문가들의 잠재력을 충분히 활용하지 못하게 만들 수 있습니다.

"게이팅 네트워크(gating network)는 항상 동일한 소수의 전문가에 대해 높은 가중치를 생성하는 상태로 수렴하는 경향이 있습니다. 이러한 불균형은 선호되는 전문가들이 더 빨리 훈련되고 따라서 게이팅 네트워크에 의해 더욱 선택되기 때문에 자가 강화됩니다." - [1]에서

**활성 매개변수(Active parameters).** MoE 계층 내에서 각 토큰을 처리하기 위해 전문가의 부분 집합만 선택하기 때문에, MoE 문헌에는 "활성(active)" 매개변수라는 개념이 존재합니다. 간단히 말해, 특정 토큰을 처리할 때 MoE 모델의 전체 매개변수 중 작은 부분 — 각 MoE 계층에서 선택된 전문가들에 의해 결정됨 —만이 실제로 활성화됩니다. 결과적으로 MoE에 의해 수행되는 총 계산량은 전체 매개변수 수가 아닌 활성 매개변수 수에 비례하게 됩니다. 이는 모델의 용량은 크지만, 실제 계산 부하는 훨씬 낮게 유지될 수 있음을 의미하며, MoE의 핵심적인 효율성 이점으로 작용합니다.

## 보조 손실(Auxiliary Losses) 및 전문가 부하 분산(Expert Load Balancing)

훈련 과정에서 전문가들의 균형 잡힌 활용을 촉진하기 위해, 우리는 모델이 각 전문가를 고르게 사용하도록 보상하는 추가적인 제약 조건(constraint)을 훈련 손실(training loss)에 통합할 수 있습니다. [1]에서는 각 전문가에 대한 "중요도(importance)" 점수를 정의함으로써 이를 달성합니다. 중요도 점수는 라우팅 메커니즘에 의해 각 전문가에 대해 예측된 확률을 기반으로 계산됩니다. 이러한 추가 손실 항은 모델이 특정 전문가에만 의존하는 경향을 완화하고 모든 전문가가 고루 훈련될 수 있도록 유도합니다.

```
[Image: Details of computing the importance loss]
```

**중요도 손실 계산 세부 정보 (출처: [1])**

데이터 배치(batch)가 주어졌을 때, 배치 내의 모든 토큰에 걸쳐 각 전문가에게 할당된 확률의 합을 구하여 중요도를 산출합니다. 위 그림을 참조하십시오. 그런 다음, 이러한 확률이 균형을 이루는지 확인하기 위해 전문가 중요도 점수의 제곱 변동 계수(squared coefficient of variation, CV)를 계산할 수 있습니다. 간단히 말해, 모든 전문가가 유사한 중요도 점수를 가질 경우 CV는 작은 값을 가지며, 그렇지 않을 경우 큰 값을 가집니다. 이어서, 위에서 제시된 중요도 손실을 표준 언어 모델링 손실(language modeling loss)에 추가하여 새로운 (정규화된) 훈련 목표(training objective)를 구성할 수 있습니다. 이 추가적인 중요도 손실 항은 MoE가 훈련 과정 전반에 걸쳐 전문가들에게 동등한 확률을 할당하도록 돕습니다.

**부하 분산(Load balancing).** 위에서 설명된 중요도 손실이 유용함에도 불구하고, 전문가들에게 동일한 중요도가 부여되었다고 해서 토큰이 균등하게 경로 지정된다는 것을 의미하지는 않습니다. 예를 들어, 전문가들은 다음과 같은 상황에서 유사한 중요도를 가질 수 있습니다:

*   매우 높은 확률을 할당하는 소수의 토큰.
*   훨씬 더 많은 수의 낮은 확률을 할당하는 토큰.

결과적으로, 중요도 손실을 사용하더라도 각 전문가에게 전달되는 토큰의 수는 여전히 매우 불균형할 수 있으며, 이는 과도한 메모리 사용과 MoE 2 의 전반적인 효율성 저하로 이어질 수 있습니다. 즉, 일부 전문가는 과부하되고 다른 전문가는 거의 사용되지 않는 현상이 발생할 수 있습니다.

```
[Image: The expert load balancing loss]
```

**전문가 부하 분산 손실 (출처: [2])**

이러한 문제를 해결하기 위해, 우리는 전문가 중요도와 부하 분산(각 전문가 간 토큰의 균등 경로 지정으로 정의됨)을 모두 포착하는 단일 보조 손실 항(auxiliary loss term)(위에 표시됨)을 개발할 수 있습니다. 이러한 접근 방식은 [2]에서 제안되었으며, 저자들은 두 가지 양을 고려하는 손실을 생성합니다:

*   각 전문가에게 할당된 라우터 확률의 비율 3 .
*   각 전문가에게 전달된 토큰의 비율.

이 두 양을 각각 N차원 벡터에 저장한 후, 이 두 벡터의 내적(dot product) 4 을 계산하여 단일 손실 항을 생성할 수 있습니다. 결과적인 손실은 전문가들이 균일한 확률과 부하 분산을 받을 때 최소화되며, 따라서 단일 보조 손실 항 내에서 우리의 두 가지 목표를 모두 효과적으로 달성합니다!

```
[Image: The router-z loss]
```

**라우터 z-손실 (출처: [3])**

**라우터 z-손실(Router z-loss).** 위에서 설명된 보조 부하 분산 손실은 MoE 문헌 전반에 걸쳐 널리 사용되지만, [3]의 저자들은 훈련 안정성(training stability)을 더욱 향상시킬 수 있는 라우터 z-손실(router z-loss)이라는 추가 보조 손실 항을 제시합니다. 라우터 z-손실은 라우팅 메커니즘에 의해 예측되는 로짓(logits)의 크기를 제한합니다 — 이는 확률이 아닌, 소프트맥스(softmax) 함수가 적용되기 전의 값입니다. 이상적으로는 이 로짓들이 너무 커지는 것을 원치 않습니다. 그러나 이 로짓들은 라우터의 (지수) 소프트맥스(softmax) 함수를 통과할 때 매우 커질 수 있으며 — 이는 전체 ( float32 ) 정밀도(precision)를 사용할 때조차 훈련 과정을 불안정하게 만들 수 있는 반올림 오류(round-off errors)로 이어집니다.

"라우터는 float32 정밀도로 전문가들에 대한 확률 분포를 계산합니다. 그러나 가장 큰 규모에서는 이것이 신뢰할 수 있는 훈련을 제공하기에 불충분하다는 것을 발견했습니다." - [3]에서

라우터가 더 작은 로짓을 예측하도록 유도하기 위해, 위에 제시된 손실 항을 사용할 수 있습니다. 이 손실이 라우터의 로짓을 정규화하는 데만 초점을 맞추고 부하 분산을 수행하지 않는다는 점을 고려할 때, 우리는 일반적으로 라우터 z-손실을 [2]에서 제안된 보조 부하 분산 손실과 함께 사용합니다. 이 두 손실은 LLM의 표준 언어 모델링 손실 위에 추가됩니다. 아래 그림을 참조하십시오. 이러한 결합된 접근 방식은 모델의 안정성과 효율성을 동시에 향상시키는 데 기여합니다.

```
[Image: Combined loss function for MoE training]
```

## 전문가 처리 용량(Expert Capacity)

MoE 계층에서 수행되는 계산은 훈련 및 추론(inference) 중에 발생하는 경로 지정 결정으로 인해 동적인 특성을 가집니다. 그러나 희소 모델(sparse models)의 대부분의 실제 구현을 살펴보면, 일반적으로 고정된 배치 크기(static batch sizes)를 사용합니다 — 이는 하드웨어 활용(hardware utilization)을 개선하는 데 효과적인 방법입니다.

```
[Image: Expert capacity diagram]
```

(출처: [2])

**전문가 처리 용량.** 각 전문가에게 할당되는 고정된 배치 크기를 명확히 하기 위해 전문가 처리 용량(expert capacity)을 정의할 수 있습니다. 전문가 처리 용량은 아래와 같이 정의됩니다.

```
[Mathematical formula for expert capacity]
```

전문가 처리 용량은 각 전문가에게 전달될 수 있는 배치 내 토큰의 최대 개수를 규정합니다. 만약 특정 전문가에게 경로 지정된 토큰의 수가 전문가 처리 용량을 초과하면, 우리는 이 추가 토큰들을 "버립니다(drop)". 더 구체적으로 말하면, 이러한 토큰들에 대한 계산은 수행하지 않고, 변환기(transformer)의 잔여 연결(residual connection)을 통해 그 표현이 다음 계층으로 직접 흐르도록 합니다.

"하드웨어 활용을 개선하기 위해, 희소 모델의 대부분의 구현은 각 전문가에 대해 정적 배치 크기를 가집니다. 전문가 용량은 각 전문가에게 라우팅될 수 있는 토큰의 수를 의미합니다. 이 용량을 초과하면 오버플로우된 토큰들은… 잔여 연결을 통해 다음 레이어로 전달됩니다." - [3]에서

전문가 처리 용량은 용량 계수(capacity factor) 설정을 통해 조절됩니다. 용량 계수가 1이라는 것은 토큰이 전문가들 사이에 완벽하게 균형 잡힌 방식으로 경로 지정됨을 의미합니다. 반대로, 용량 계수를 1보다 높게 설정하면 전문가들 간의 토큰 불균형을 수용하기 위한 추가적인 여유 공간(buffer)을 제공합니다. 그러나 이는 비용(예: 더 높은 메모리 사용량과 낮은 효율성)을 수반합니다. 적절한 용량 계수 설정은 모델의 성능과 자원 활용 사이의 섬세한 균형을 맞추는 데 필수적입니다.

```
[Image: Capacity factor vs. dropped tokens]
```

(출처: [2])

**용량 계수는 어떻게 설정할까요?** 흥미롭게도, MoE 모델은 상대적으로 낮은 용량 계수에서도 잘 작동하는 경향이 있습니다 [2, 3]. 위 그림을 참조하십시오. 그러나 훈련 과정에 부정적인 영향을 미치지 않도록 버려지는 토큰의 수가 너무 많지 않도록 보장해야 합니다 (즉, 이는 경험적으로 결정될 수 있습니다). 또한 훈련과 추론에 서로 다른 용량 계수를 적용할 수 있습니다. 예를 들어, ST-MoE [3]는 훈련 중에는 1.25의 용량 계수를 사용하고 평가 중에는 2.0의 용량 계수를 활용합니다. 이러한 유연성은 다양한 운영 환경에서 모델의 효율성을 최적화하는 데 도움이 됩니다.

## MoE 계층의 출력 계산

```
[Image: Computing output of an MoE layer]
```

**MoE 계층의 출력 계산**

라우터의 결과값을 얻으면, 최종 출력은 다음과 같이 계산됩니다:

*   토큰을 선정된 전문가들에게 보냅니다.
*   이 토큰들에 대한 전문가들의 결과값을 산출합니다.
*   전문가 결과값의 가중 평균(weighted average)을 취하며, 이때 가중치는 라우터에 의해 각 전문가에게 할당된 확률입니다.

```
[Mathematical formula for MoE layer output]
```

위의 수학적 표현에서, 우리는 단일 토큰에 대한 MoE 계층의 최종 결과값을 계산하는 과정을 공식화했습니다. 이 토큰의 최종 출력은 K개의 활성화된 전문가 각각의 출력에 대한 가중 평균으로 구성됩니다. 이는 각 전문가의 기여도를 그 중요도에 따라 조절하여 최종 결론을 도출하는 방식입니다.

**공유 전문가(Shared experts)**는 MoE 문헌 [14, 15]에서 비교적 최근에 소개된 개념입니다. 이 아이디어는 간단합니다: 우리는 두 그룹의 전문가를 가집니다 — 공유 전문가와 경로 지정된 전문가(routed experts). 모든 토큰은 항상 공유 전문가들을 거쳐 처리됩니다. 반면, 토큰은 일반적인 MoE 경로 지정 메커니즘에 따라 경로 지정된 전문가들을 통과합니다. 공유 전문가에 대한 이 개념은 아래에 그림으로 나타나 있으며, MoE 계층 내의 전문가 부분 집합에만 경로 지정이 적용됨을 확인할 수 있습니다. 일반적으로 공유 전문가의 수는 경로 지정된 전문가의 수보다 적어야 합니다 — 공유 전문가의 수를 늘리면 MoE의 희소성 이점(sparsity benefits)이 감소하기 때문입니다.

```
[Image: Shared vs. routed experts]
```

**공유 전문가 대 경로 지정된 전문가 (출처: [14])**

공유 전문가를 사용하는 동기는 전문가들 간의 중복 정보(redundant information)의 양을 최소화하는 것입니다. 공유 전문가 집합을 가짐으로써, 네트워크가 동일한 정보를 여러 다른 전문가에 걸쳐 복제할 필요 없이 이 전문가들 내에 공유 정보를 저장할 수 있도록 합니다. 공유 전문가가 있는 MoE 계층의 출력을 계산하려면, 단순히 공유 전문가의 출력을 일반적인 경로 지정된 출력에 추가합니다. 아래 그림을 참조하십시오. 이 방식은 모델의 효율성을 높이면서도 전반적인 지식의 응집력을 유지하는 데 기여합니다.

```
[Mathematical formula for MoE layer output with shared experts]
```

**공유 전문가가 있는 MoE 계층의 출력 계산**

## 전체 그림: MoE 계층을 포함한 단방향 LLM

```
[Image: A full MoE block in a transformer]
```

**변환기 내의 완전한 MoE 블록 (출처: [2])**

MoE 계층의 완전한 도해는 위에 제시되어 있습니다. MoE에서는 표준 단방향 변환기(decoder-only transformer)의 블록 구조를 피드포워드 네트워크를 전문가 계층으로 대체함으로써 수정합니다. 간단히 말해, 이 전문가 계층은 본래의 피드포워드 네트워크의 여러 독립적인 복사본을 포함합니다. 특히, MoE 계층 내의 이 모든 구성 요소 — 일반 계층(들), 전문가들, 그리고 경로 지정 메커니즘 —는 경사 하강법(gradient descent)을 통해 공동으로 훈련됩니다. 각 토큰에 대해, 우리는 경로 지정 메커니즘을 통해 어떤 전문가를 활용할지 선택할 수 있으며, 이는 일반적으로 토큰 벡터의 간단한 선형 변환을 통해 구현됩니다. 이를 종합하면, MoE의 수정된 블록 구조는 다음을 포함합니다:

*   자기 주의(self-attention) 계층.
*   잔여 연결(residual connection)과 정규화(normalization) 연산.
*   토큰을 전문가에게 경로 지정하는 것을 결정하는 경로 지정 메커니즘.
*   여러 독립적인 피드포워드 네트워크를 가진 전문가 계층.
*   각 토큰에 대한 전문가 계층의 최종 출력에 적용되는 최종 추가 및 정규화 연산.

수정된 블록 구조를 제외하고는 변환기 구조는 동일하게 유지됩니다. 또한 변환기의 P번째 블록만 MoE 계층을 사용하도록 변환하며 — 다른 블록은 변경되지 않습니다. 일부 MoE는 모든 계층에 전문가를 사용하지만, 실제로는 P를 2, 4 또는 심지어 6으로 설정하는 것이 일반적입니다. 이 방법은 MoE LLM이 소비하는 총 매개변수 수를 제어하는 데 유용할 수 있습니다. 이러한 설계 선택은 모델의 계산 복잡성과 성능 사이의 최적의 균형을 찾는 데 중요합니다.

## MoE 활용의 장점과 한계

이제 MoE의 기본적인 개념들을 파악했으니, 아마도 "왜 밀집 모델 대신 MoE를 사용해야 할까?"라는 의문이 생길 수 있습니다. MoE의 가장 큰 장점은 효율성이지만, 이 모델들에는 고려해야 할 단점도 존재합니다. MoE의 가장 중요한 장점과 단점 몇 가지를 간략히 살펴보겠습니다.

**MoE의 장점.** LLM은 규모의 이점을 얻습니다 — 더 큰 모델과 더 큰 데이터셋은 더 나은 성능으로 이어집니다. 그러나 LLM을 확장하는 데는 막대한 비용이 따릅니다! MoE의 핵심적인 이점 중 하나는 규모 확장과 관련된 어려움을 회피하는 능력입니다 — 이들은 토큰당 고정된 계산 비용(computational cost)으로 모델의 크기를 늘릴 수 있도록 합니다. 이러한 방식으로, 밀집 모델에만 의존했다면 불가능했을 더 방대한 모델을 훈련할 수 있습니다. 언어 모델링 영역에서, 이러한 희소 모델의 추가적인 매개변수와 표현 능력(representational capacity)은 성능에 지대한 영향을 미칩니다.

"LLM이 점점 더 보편화됨에 따라, 계산 자원을 비례적으로 늘리지 않고 성능을 향상시키는 것은 중요한 과제입니다." - [12]에서

MoE의 계산적 이점은 (논란의 여지가 있지만) 추론 과정에서 가장 크게 발휘됩니다. MoE 모델은 총 매개변수 수 면에서 거대하기 때문에, 이러한 매개변수를 저장할 수 있는 충분한 수의 GPU가 필요합니다. 그러나 각 토큰을 처리할 때는 이 매개변수 중 고정된 부분만 사용되므로 계산 효율성이 크게 향상됩니다. 낮은 배치 크기에서는 추론이 더 신속하며, 큰 배치 크기에서는 처리량(throughput)이 더 높아집니다 [5]. 흥미롭게도 MoE는 훈련에도 더 효율적입니다. 예를 들어, 스위치 트랜스포머는 MoE 구조 사용으로 7배의 사전 훈련 속도 향상을 보고했습니다 [2]. 아래 그림을 참조하십시오.

```
[Image: Training speedup with MoE]
```

(출처: [2])

**MoE 활용의 단점.** 이러한 장점에도 불구하고 MoE는 또한 다음과 같은 특성을 가집니다:

*   훈련 중 불안정성(instability)에 취약합니다.
*   미세 조정(finetune)하기 어렵습니다 (즉, 과적합(overfitting) 문제로 인해).
*   낮은/혼합 정밀도 훈련 기술(low / mixed precision training techniques)에 민감합니다.
*   하이퍼파라미터(hyperparameter) 설정(예: 가중치 초기화(weight initialization))에 민감합니다.

간단히 말해, MoE를 최대한 활용하려면 더 많은 보완적인 기능과 정교한 조정이 요구됩니다. 이러한 이유로, MoE는 모든 상황에서 최적의 선택이 아닐 수 있습니다. 예를 들어, 특정 작업에서 LLM을 미세 조정하려는 경우 밀집 모델이 더 쉬운 대안이 될 수 있습니다. 그러나 제대로 활용할 수 있다면 MoE는 다양한 이점을 제공합니다. 이러한 복잡성에도 불구하고, MoE는 그 잠재력 때문에 계속해서 활발한 연구 대상이 되고 있습니다.

## 전문가 혼합 언어 모델(Mixture-of-Experts Language Models)

이제 MoE의 가장 중요하고 근본적인 개념들을 이해했으니, 이 개념들이 언어 모델링 영역에서 어떻게 적용되었는지 더 깊이 탐구해 보겠습니다. LLM이 규모 증가의 이점을 얻는다는 사실 때문에, MoE는 LLM 연구 내에서 광범위하게 채택되어 큰 성공을 거두었습니다.

### Mixtral of Experts [5]

```
[Image: Mixtral performance comparison]
```

(출처: [5])

Mixtral 8×7B (별칭: Mixtral of Experts)는 영어, 프랑스어, 이탈리아어, 독일어, 스페인어에 능통한 오픈 소스 Mistral-7B 모델 [6]의 MoE 기반 확장판입니다. 이 두 모델은 Apache 2.0 라이선스 하에 공개 가중치(open weights)를 제공하며, 모델에 대한 상세한 정보를 담은 기술 보고서도 함께 제공됩니다. Mixtral은 Mistral의 모든 계층을 8개의 전문가를 가진 전문가 계층으로 변환합니다. 이 전문가들 중 두 개가 각 토큰에 대해 활성화되며, 총 470억 개의 매개변수와 130억 개의 활성 매개변수를 가진 모델을 생성합니다. 이 모델은 또한 32K의 컨텍스트 길이(context length)를 가지는데, 이는 비-MoE 모델보다 4배 더 큰 수치입니다. 위 그림에서 볼 수 있듯이, Mixtral은 전반적으로 Mistral을 능가하며 특히 코드 생성, 수학 및 다국어 벤치마크에서 뛰어난 성능을 보이며, 일부 경우에는 더 큰 LLaMA-2-70B 모델의 성능을 초과하기도 합니다.

```
[Image: GQA diagram]
```

(출처: [7])

**Mistral-7B 구조.** Mixtral 8×7B의 기반이 되는 LLM 구조는 Mistral-7B [6]의 구조 설정과 정확히 일치하는 단방향 변환기(decoder-only transformer)입니다. 표준 단방향 LLM 구조와 비교하여, Mistral-7B에는 몇 가지 변경 사항이 있습니다:

*   **그룹화된 쿼리 어텐션(Grouped-Query Attention, GQA) [7]**: 효율성을 개선하기 위해 자기 주의 헤드(self-attention heads) 그룹 간에 키(key) 및 값(value) 투영(projections)을 공유합니다. 위 그림을 참조하십시오. 이는 특히 추론 시 메모리 사용량을 줄이는 데 기여합니다.
*   **슬라이딩 윈도우 어텐션(Sliding Window Attention, SWA) [8]**: 각 토큰에 대해 크기 W의 고정된 윈도우(window)에 걸쳐 (마스크된) 자기 주의를 계산하여 LLM이 감소된 추론 비용 5 으로 임의 길이의 시퀀스를 처리할 수 있도록 합니다. 아래 그림을 참조하십시오. 이는 긴 컨텍스트 처리에 있어 효율적인 대안을 제공합니다.

```
[Image: Sliding Window Attention diagram]
```

SWA를 사용하기 때문에, 모델은 롤링 버퍼(rolling buffer) / 순환 캐시(circular caches)와 같은 방법을 활용하여 KV 캐시(KV cache)를 더 메모리 효율적으로 만들거나 청크된 프리필(chunked prefill)을 사용하여 추론 속도를 높일 수 있습니다. Mixtral 8×7B는 동일한 구조적 관례를 따릅니다.

(출처: [6])

**추가 상세 정보.** 이전에 언급했듯이, Mixtral은 LLM의 모든 계층을 전문가 계층으로 변환합니다. 각 전문가 계층 내에서는 모든 토큰에 대해 선형 계층(linear layer)의 상위 K 로짓(logits)에 소프트맥스(softmax)를 취하는 간단한 경로 지정 메커니즘이 채택됩니다 — 이는 본 개요의 시작 부분에서 논의된 경로 지정 메커니즘과 일치합니다. 아래 그림을 참조하십시오.

```
[Image: Mixtral routing mechanism]
```

(출처: [5])

[5]의 저자들은 Mixtral이 다국어 코퍼스(multilingual corpus)에 걸쳐 사전 훈련되어 모델이 여러 언어를 이해할 수 있다고 명시합니다. 아래에서 볼 수 있듯이, Mixtral은 다국어 벤치마크에서 LLaMA 모델을 보편적으로 능가합니다.

```
[Image: Mixtral multilingual performance]
```

(출처: [5])

**경로 지정 분석.** 논문을 마무리하며, [5]의 저자들은 여러 도메인에 걸쳐 토큰에 대해 전문가가 어떻게 선택되는지에 대한 상세한 분석을 수행하여 해석 가능한 패턴을 추론할 수 있는지 확인합니다. The Pile 내의 다양한 주제 영역에 대해 다른 전문가에게 할당된 토큰 분포를 그래프로 나타낼 때, 토큰 할당에서 명확한 주제별 패턴은 나타나지 않습니다. 아래 그림을 참조하십시오. 이는 전문가들이 광범위한 지식을 공유하거나, 그 전문화가 주제 경계를 넘어서는 더 추상적인 수준에서 이루어질 수 있음을 시사합니다.

```
[Image: Token distribution across experts by topic]
```

(출처: [5])

그러나 MoE는 일부 구조화된 동작을 보입니다. 예를 들어, 파이썬 코드의 "self"와 영어의 "Question"이라는 단어는 — 비록 여러 토큰으로 구성되어 있지만 — 종종 동일한 전문가를 통해 경로 지정됩니다. 유사하게, 코드의 들여쓰기 토큰(indentation tokens)은 일반적으로 동일한 전문가에게 보내지며, 연속적인 시퀀스(consecutive sequences) — 서로 가까이 있는 토큰들의 시퀀스 —는 일반적으로 동일한 전문가에게 보내집니다. 아래 그림을 참조하십시오. 이러한 결과는 i) 전문가가 주제별로 전문화되지 않지만 ii) MoE의 경로 지정 메커니즘이 모델 입력의 구문(syntax) 또는 내용과 관련하여 일부 구조화된 동작을 따른다는 것을 나타냅니다. 이는 전문가들이 언어의 구조적 특성이나 특정 유형의 토큰 패턴을 처리하는 데 특화될 수 있음을 보여줍니다.

```
[Image: Structured routing behavior]
```

(출처: [5])

**확장.** Mixtral 출시 이후, Mixtral-8×22B라는 더 큰 규모의 모델이 공개되었습니다. 이 모델은 총 1410억 개의 매개변수와 390억 개의 활성 매개변수를 가지며, 이는 원래 Mixtral 모델보다 약 3배 더 큰 규모입니다. Mixtral-8×22B는 특히 코딩 및 수학 작업에 뛰어난 능력을 보이며, 64K로 확장된 컨텍스트 길이(context length)를 가지고 있고, 함수 호출(function calling) 기능을 기본적으로 지원합니다. Mixtral-8×22B의 다른 오픈 모델 대비 주요 이점은 아래에 요약되어 있습니다. 이러한 확장된 버전은 MoE 아키텍처의 스케일링 잠재력을 명확하게 보여줍니다.

```
[Image: Mixtral-8x22B benefits summary]
```

( 출처 )

### Grok (from xAI ) [9]

모델에 대한 상세한 기술 보고서는 공개되지 않았지만, MoE 기반 LLM의 가장 주목할 만한 최근 사례 중 하나는 xAI의 Grok입니다. 초기 Grok-1 모델은 2024년 초에 출시되었습니다. 연구원들은 이 모델이 각 토큰에 대해 25%의 가중치가 활성화되는 3140억 개의 매개변수를 가진 MoE(즉, 약 700억~800억 개의 활성 매개변수)라고 밝혔습니다. Grok-1의 구조와 핵심 모델 가중치는 Apache 2.0 라이선스 하에 오픈 소스화되었습니다. 그러나 이는 사전 훈련된 기본 모델이며, 모델의 후처리 훈련(post training) 과정에 대한 세부 정보는 제공되지 않았습니다. 이는 모델의 실제 성능을 이해하는 데 중요한 부분이 누락되었음을 의미합니다.

```
[Image: Grok-1.5 performance comparison]
```

(출처: [10])

**Grok-1.5 [10].** Grok-1 6 의 초기 공개 직후, 더 향상된 추론(reasoning) 및 긴 컨텍스트 이해(long context understanding) 기능을 갖춘 후속 모델 버전이 발표되었습니다. 예를 들어, Grok-1.5는 수학 및 코딩 관련 작업에서 훨씬 더 우수한 성능을 보입니다. 위 그림을 참조하십시오.

"이 모델은 컨텍스트 윈도우(context window)가 확장됨에 따라 지시 따르기(instruction-following) 능력을 유지하면서 더 길고 복잡한 프롬프트(prompts)를 처리할 수 있습니다." - [10]에서

Grok-1.5는 건초 더미 속 바늘 찾기 테스트(needle in a haystack test)에서 완벽한 검색(retrieval) 성능으로 최대 128K 토큰의 시퀀스를 처리할 수 있습니다. 아래 그림을 참조하십시오. 저자들은 또한 모델이 많은 컨텍스트가 주어졌을 때도 견고한 지시 따르기 능력을 유지한다고 언급하는데, 이는 순수한 검색 7 에 비해 긴 컨텍스트 능력의 훨씬 더 신뢰할 만한 지표입니다.

```
[Image: Grok-1.5 needle in a haystack test]
```

(출처: [10])

Grok-1.5와 Grok-1이 이렇게 짧은 간격으로 출시되었다는 점을 고려할 때, Grok-1.5의 개선 사항은 후처리 훈련에 의해 주도되었다고 추론할 수 있습니다 — 이 기간 동안 다른 사전 훈련된 기본 모델이 생성되었을 가능성은 극히 낮습니다. 이는 기본 모델의 잠재력을 최대한 끌어내기 위한 미세 조정과 최적화의 중요성을 보여줍니다.

**Grok-2.** 최근에는 Grok-2가 출시되었는데, 이는 챗봇 아레나(Chatbot Arena)에서 측정된 바와 같이 추론, 코딩 및 채팅 기능이 향상되었습니다. Grok-2는 또한 다양한 다른 작은 개선 사항(예: 도구 사용, 검색, 사실성 등)을 포함하며, Grok-2의 정제된 버전(distilled version)인 Grok-2-mini가 주 모델과 함께 공개되었습니다. 그러나 Grok-2의 구조에 대한 공개된 세부 정보는 공유되지 않았습니다 — 이 모델은 처음부터 훈련되었을 가능성이 높으며 MoE 기반일 수도 있고 아닐 수도 있습니다. 이러한 정보의 부족은 모델의 내부 작동 방식에 대한 추측을 낳습니다.

### DBRX (from Mosaic ) [11]

```
[Image: DBRX performance comparison]
```

(출처: [11])

DBRX는 Mosaic에서 출시한 오픈 LLM 시리즈의 가장 최신 모델입니다. 모델의 두 가지 버전 — 기본 모델(DBRX base)과 미세 조정된 모델(DBRX Instruct) —이 오픈 라이선스(즉, Databricks 오픈 모델 라이선스) 하에 공개되었습니다. DBRX는 다음 사양을 가진 MoE 기반 LLM입니다:

*   360억 개의 활성 매개변수를 가진 총 1320억 개의 매개변수.
*   각 MoE 계층에 16개의 전문가가 있으며, 각 토큰에 대해 4개의 전문가가 활성화됩니다.
*   최적화된 텍스트 12조 토큰으로 사전 훈련되었습니다.
*   사전 훈련 효율성에서 4배 향상.

가장 주목할 만한 점은 DBRX가 "세분화된(fine-grained)" MoE 모델이라는 것입니다. 즉, 이 모델은 각 MoE 계층에 더 많은 수의 전문가를 사용하지만, 각 개별 전문가는 더 작습니다. 참고로, Mixtral과 Grok-1 모두 각 MoE 계층 내에 8개의 전문가를 포함하며 — 이 중 두 개는 주어진 토큰에 대해 활성화됩니다. 세분화된 전문가를 사용함으로써, 각 MoE 계층은 선택할 수 있는 더 많은 전문가 조합(특히 65배 더 많음)을 가지며, 이는 [11]에서 모델 품질을 향상시키는 것으로 밝혀졌습니다.

**훈련 데이터.** DBRX의 사전 훈련 데이터셋은 매우 방대하지만 8 , [11]의 저자들은 데이터의 품질을 높이는 데도 상당한 노력을 기울였습니다. 결과적으로 DBRX의 통계적 훈련 효율성(statistical training efficiency)은 일반적인 경우보다 높습니다 — 더 적은 토큰으로 더 높은 정확도를 달성하기 때문에 훈련이 더 신속합니다. 더 구체적으로, [11]의 저자들은 새로운 데이터가 토큰당 2배 더 효율적이라고 추정하는데, 이는 절반 이하의 토큰으로 훈련하고도 동일한 수준의 성능을 달성할 수 있음을 의미합니다. 이 주장은 새로운 모델의 사전 훈련 데이터가 단독으로 미치는 영향을 테스트함으로써 검증되었습니다 (즉, 다른 사전 훈련 데이터를 가진 고정 모델을 사용).

"단독으로 볼 때, 더 나은 사전 훈련 데이터는 모델 품질에 상당한 영향을 미쳤습니다. 우리는 DBRX 사전 훈련 데이터를 사용하여 1조 토큰으로 7B 모델을 훈련했습니다. 이는 MPT-7B의 30.9%에 비해 Databricks Gauntlet에서 39.0%에 도달했습니다." - [11]에서

또한, DBRX를 훈련하기 위해 커리큘럼 학습(curriculum learning)이 적용됩니다 — 사전 훈련 데이터의 혼합은 사전 훈련 과정 전반에 걸쳐 동적으로 변화합니다. 이 커리큘럼 학습 전략의 세부 사항은 나중에 이 논문에서 설명되었습니다. DBRX가 사용하는 커리큘럼 학습 전략은 웹 크롤링(web-crawling)을 통해 얻은 데이터에 비해 품질이 더 높기 때문에 훈련 후반부에 더 작고 도메인별 데이터셋을 단순히 업샘플링(upsamples)합니다. 이 간단한 커리큘럼 학습 전략은 어려운 벤치마크에서 성능을 크게 향상시키는 것으로 밝혀졌습니다. 아래 그림을 참조하십시오.

```
[Image: DBRX curriculum learning performance]
```

( 출처 )

**토크나이저(Tokenizer) 및 컨텍스트 윈도우(context window).** DBRX는 32K의 컨텍스트 길이를 가지며 GPT-4 토크나이저(tiktoken을 통해 사용 가능)를 채택합니다. 저자들에 따르면, GPT-4 토크나이저는 주로 성능 때문에 선택되었습니다. 이 토크나이저는 큰 어휘를 가지고 있으며 토큰 효율성이 매우 높아, 동일한 양의 텍스트를 더 적은 토큰으로 표현함으로써 디코딩(decoding) 및 훈련 속도를 자연스럽게 향상시킵니다. 이는 모델의 전반적인 효율성에 크게 기여합니다.

```
[Image: DBRX tokenizer efficiency]
```

(출처: [11])

**효율성 이점.** DBRX의 제안은 사전 훈련 효율성 측면에서 상당한 개선을 가져옵니다. 지금까지 배운 것 외에도, [11]에서 언급된 효율성 향상의 추가적인 요인들이 몇 가지 있습니다:

*   MoE 구조는 소규모 실험에서 훈련 중 1.7배 더 적은 FLOPS를 요구하는 것으로 나타났습니다.
*   단방향 구조에 대한 다른 수정 사항(즉, RoPE , GLU 활성화(GLU activation) 및 GQA ).
*   "더 나은 최적화 전략(optimization strategies)".

모든 데이터, 구조 및 최적화 변경 사항을 고려할 때, DBRX의 종단 간(end-to-end) 훈련 프로세스는 이전 모델에 사용된 사전 훈련 파이프라인(pretraining pipeline)과 비교하여 4배 더 적은 계산량을 필요로 합니다. 이 수치를 결정하기 위해, [11]의 저자들은 DBRX의 더 작은 변형 모델을 이전 MPT-7B 모델과 비교하여, 더 작은 DBRX 모델이 훈련 중 3.7배 더 적은 FLOPS를 사용하면서 Databricks Gauntlet에서 유사한 성능을 달성한다는 것을 발견했습니다.

```
[Image: DBRX training efficiency comparison]
```

(출처: [21])

DBRX는 또한 추론 효율성(inference efficiency) 개선을 가져오는데 — 부하 테스트(load tests)에서 사용자당 초당 150 토큰으로 LLaMA-2-70B보다 최대 2배 더 빠릅니다. 이러한 측정은 TensorRT-LLM 및 16비트 정밀도(precision)를 사용하는 최적화된 서빙 인프라(serving infrastructure)를 활용하여 이루어졌으며, 이는 매우 빠른 속도를 의미합니다. DBRX의 MoE 구조는 비교적 적은 수의 활성 매개변수로 인해 추론 효율성에도 기여합니다. 예를 들어, DBRX는 총 매개변수 및 활성 매개변수 모두에서 Grok-1 크기의 40%에 해당합니다.

"전문가 혼합 모델을 훈련하는 것은 어렵습니다. 우리는 DBRX급 모델을 효율적인 방식으로 반복적으로 훈련할 수 있을 만큼 견고한 파이프라인을 구축하기 위해 다양한 과학적 및 성능적 과제를 극복해야 했습니다." - [11]에서

MoE 훈련은 일반적으로 훈련 중 발생하는 불안정성, 통신 병목 현상(communication bottlenecks) 등으로 인해 어려운 것으로 알려져 있습니다. 그러나 DBRX는 [11]에 설명된 최적화된 사전 훈련 전략 덕분에 안정성, 효율성 및 성능 면에서 인상적인 결과를 달성합니다. 특히, 이러한 결과를 가능하게 하는 단일 변경 또는 발전은 없습니다. DBRX가 사용하는 인상적인 사전 훈련 파이프라인은 수많은 작고 실용적인 변경 사항들의 통합에 의해 가능해졌습니다.

**경험적 평가.** 다른 오픈 LLM과 비교할 때, DBRX-Instruct는 Mixtral과 비교하여 복합 벤치마크에서 큰 차이로 더 나은 성능을 달성합니다. 아래 그림을 참조하십시오. DBRX는 범용 LLM임에도 불구하고 인상적인 프로그래밍 기술을 보유하고 있으며, Grok-1 (크기가 두 배 이상!)과 CodeLLaMA-70B와 같은 전문 코딩 모델까지 능가합니다. DBRX는 추론 및 수학 기반 작업에서도 뛰어난 성능을 보입니다.

```
[Image: DBRX-Instruct vs. other open LLMs]
```

(출처: [11])

폐쇄형 모델과 비교할 때, DBRX는 GPT-3.5의 성능을 능가하며 Gemini-1.0 Pro 와 경쟁할 만합니다. Gemini-1.0 Pro는 GSM8K에서만 DBRX보다 우수한 성능을 보이며, Mixtral-Medium은 고려되는 몇 가지 특정 작업에서 더 나은 성능을 보입니다. 아래 그림을 참조하십시오. 높은 수준에서 볼 때, DBRX는 프로그래밍, 수학, 일반 지식, 상식 추론(commonsense reasoning) 및 검색/RAG(retrieval / RAG)에 능숙한 것으로 보입니다.

```
[Image: DBRX-Instruct vs. closed models]
```

(출처: [21])

### OpenMoE: 오픈 전문가 혼합 언어 모델에 대한 초기 노력 [12]

언어 모델링 영역에서 MoE의 성공에도 불구하고, 코드, 정보, 데이터, 가중치 등이 모두 공개적으로 공유되는 진정한 오픈 소스 MoE의 수는 상대적으로 적습니다. 이 문제를 해결하기 위해 OpenMoE [12]는 6억 5천만 개에서 340억 개의 매개변수에 이르는 단방향 MoE LLM 제품군을 훈련하기 위한 대규모 노력을 수행합니다. 이 모델들은 다양한 세분성(granularity)(즉, 16개 또는 32개의 전문가)을 가진 세분화된 전문가를 채택합니다. 이 노력의 결과는 [12]에 문서화되어 있으며 모든 모델은 공개적으로 공유됩니다. 저자들은 또한 그들의 결과를 재현하는 데 사용할 수 있는 잘 문서화된 코드 저장소(code repository)를 제공합니다.

[OpenMoE 저장소](https://github.com/OpenMoE/OpenMoE)

"모든 계층에 MoE를 사용하는 것은 경로 지정 중에 더 많은 계산 오버헤드(computational overhead)를 유발하고 인터리브된 MoE 사용보다 더 나쁜 비용-효율성 절충점을 초래합니다." - [12]에서

**설계 선택.** OpenMoE 모델은 ST-MoE [3]의 설정을 채택하며, 동일한 경로 지정 메커니즘과 활성 전문가 수(즉, k = 2 )를 포함합니다. 저자들은 4번째 또는 6번째 변환기 블록만 MoE 계층으로 변환하기로 결정했으며, 더 큰 간격(stride)이 비용과 효율성 측면에서 더 나은 균형을 제공한다는 것을 발견했습니다. OpenMoE에 사용된 사전 훈련 데이터셋은 코드의 높은 분포를 포함합니다. 사실, 사전 훈련 초기 단계에서는 코드가 데이터셋의 50% 이상을 차지했지만, 이 비율은 나중에 훈련에서 최적이 아니라는 이유로 조정되었습니다. 아래 그림을 참조하십시오. 정렬(alignment)을 위해, OpenMoE는 사전 훈련 후 WildChat의 데이터를 사용하여 SFT(Supervised Fine-Tuning)를 거쳐 더 나은 지시 따르기 능력을 유도합니다.

```
[Image: OpenMoE pretraining data composition]
```

(출처: [12])

**경로 지정 동역학(Routing dynamics).** OpenMoE의 주요 기여 중 하나는 모델 내에서 이루어진 경로 지정 결정에 대한 상세한 분석입니다. 먼저, 이전 연구 [5]에서 나타난 결과와 유사하게 — 전문가들은 특정 도메인에 전문화되는 경향이 없음을 알 수 있습니다. 아래 그림을 참조하십시오. 이는 전문가들이 광범위한 지식을 처리하거나, 전문화가 더 추상적인 수준에서 이루어질 수 있음을 시사합니다.

```
[Image: OpenMoE expert specialization by domain]
```

(출처: [12])

그러나 아래 그림에서 볼 수 있듯이, 자연어 및 특정 작업에 걸쳐 어느 정도의 전문가 전문화(expert specialization)가 나타납니다.

```
[Image: OpenMoE expert specialization by language/task]
```

(출처: [12])

그러나 이 경향을 더 깊이 파고들면, 토큰 경로 지정의 동역학은 주로 토큰 ID(token ID)에 의해 결정된다는 것을 알 수 있습니다. 즉, 동일한 토큰은 그 토큰이 존재하는 컨텍스트와 상관없이 거의 항상 동일한 전문가에게 경로 지정됩니다. 이 패턴은 [12]에서 "컨텍스트 독립적 전문화(Context-Independent Specialization)"라고 불립니다.

"이것은 매우 흥미로운 발견인데, 동일한 토큰 ID를 가진 토큰들이 다른 문장에서 매우 다양한 컨텍스트를 가지기 때문입니다. 예를 들어, 토큰 'an'은 'an apple' 또는 'another'의 일부일 수 있습니다. 그러나 이 모든 토큰들은 소수의 고정된 전문가에 대해 매우 강력한 전문화를 보입니다." - [12]에서

흥미롭게도, 전문가들은 선호하는 토큰에서 관찰 가능한 패턴을 가집니다. 아래 그림을 참조하십시오. 예를 들어, "have", "has", "had"는 모두 동일한 전문가에게 경로 지정되는 반면, 한 전문가는 "=", "and", "\n" 토큰을 받습니다 — 코딩 언어 내에서 매우 흔한 토큰들입니다. [12]에서 우리는 이러한 경로 지정 패턴이 사전 훈련의 초기 단계에서 확고해지며 훈련 후반에는 거의 변하지 않는다는 것을 알 수 있습니다.

```
[Image: OpenMoE expert token preferences]
```

(출처: [12])

**경로 지정 문제.** [12]에서 관찰된 경로 지정 패턴 외에도, OpenMoE 모델이 성능을 저해할 수 있는 일부 경로 지정 동작을 보인다는 것을 알 수 있습니다. 예를 들어, 모델은 시퀀스 후반에 토큰을 버리는(dropping) 경향이 있는데, 이는 긴 시퀀스 작업(예: 다중 턴 채팅)의 성능을 손상시킬 수 있습니다.

```
[Image: OpenMoE multi-turn chat performance]
```

**OpenMoE 모델은 다중 턴 채팅 문제에서 더 나쁜 성능을 보입니다 (출처: [12])**

경로 지정 동역학이 사전 훈련 과정의 초기 단계에서 고정되기 때문에, 이러한 동작은 후처리 훈련 중에 수정하기 어렵습니다. 사실, OpenMoE 모델은 사전 훈련과 SFT 9 중 데이터 간의 도메인 격차(domain gap)로 인해 일반적으로 어려움을 겪는 것으로 관찰됩니다 — 데이터 구성의 차이로 인해 토큰 경로 지정 동역학이 불규칙해집니다. 이러한 문제를 해결하기 위해, [12]의 저자들은 지시 따르기 데이터를 사전 훈련 데이터셋에 혼합할 것을 권장합니다.

**모델 평가.** 전반적으로, OpenMoE 모델은 MoE LLM 중에서 새로운 최첨단 성능(state-of-the-art performance)을 달성하지 못했습니다 — [12]의 저자들은 이 사실을 공개적으로 밝히고 OpenMoE 모델의 성능이 더 나은 설계를 통해 크게 향상될 수 있음을 인정합니다. OpenMoE 모델의 더 큰 기여는 그들의 투명성입니다. [12]에서 공개적으로 공유된 세부 정보와 아티팩트(artifacts)는 이 주제에 대한 추가 연구를 수행하는 데 필요한 자원을 제공함으로써 MoE에 대한 오픈 연구 노력을 가속화할 수 있습니다.

### DeepSeek-v2 [14] 및 DeepSeek-V3 [15]

최근 제안된 DeepSeek MoE 모델들, DeepSeek-v2 [14]와 DeepSeek-v3 [15]를 포함하여, 다양한 이유로 LLM 연구 커뮤니티 내에서 큰 반향을 일으켰습니다:

*   그들의 가중치는 공개적으로 공유됩니다.
*   많은 세부 정보를 공유하는 기술 보고서가 함께 제공됩니다.
*   그들의 성능은 인상적이며 — 많은 폐쇄형 모델과 동등합니다.
*   그들의 훈련 비용은 상당히 합리적입니다.

우리가 보게 될 것처럼, DeepSeek 모델은 훈련 효율성과 다운스트림 성능(downstream performance)을 모두 극대화하는 다양한 독특한 설계 선택을 합니다. DeepSeek-v2 [14]— 210억 개의 활성 매개변수를 가진 2360억 개의 매개변수 MoE —는 이후 DeepSeek-V3 모델에서 사용된 MoE 구조를 제안합니다. DeepSeek MoE 모델은 성능과 효율성을 높이기 위해 기본 변환기 블록을 약간 수정한다는 점에서 이전 모델들과는 다소 다릅니다. 아래에서 볼 수 있듯이, DeepSeek-v2 모델은 — 좋은 성능을 보이는 것 외에도 — 훈련 및 추론 효율성 관점에서 상당히 인상적이며, 훨씬 더 큰 DeepSeek-v3 모델의 강력한 출발점이 됩니다.

```
[Image: DeepSeek-v2 efficiency comparison]
```

(출처: [14])

**멀티 헤드 잠재 어텐션(Multi-head latent attention, MLA).** 표준 멀티 헤드 어텐션(multi-headed attention) 대신, DeepSeek-v2는 효율적인 어텐션 변형인 MLA를 채택합니다. 멀티 쿼리 어텐션(multi-query attention) 또는 그룹화된 쿼리 어텐션과 유사하게, MLA는 모델의 KV 캐시(KV cache)가 소비하는 메모리를 최소화하는 것을 목표로 합니다. 그러나 다른 효율적인 어텐션 변형과 달리, MLA는 상당한 성능 절충점(performance tradeoff)을 가지지 않습니다.

```
[Image: MLA mechanism]
```

(출처: [14])

특히, 이러한 메모리 효율성 증가는 모든 키(key) 및 값(value) 벡터를 훨씬 더 작은 (잠재) 벡터로 표현할 수 있게 하는 저랭크(low-rank) 공동 투영(joint projection)을 통해 달성됩니다. 위 그림을 참조하십시오. 이 벡터를 업샘플링(upsample)할 수 있습니다 — 단순히 선형적으로 투영하여 여러 개의 더 큰 벡터를 형성함 — 전체 키 및 값 벡터를 복원하기 위해, 그러나 우리는 KV 캐시에 잠재 벡터만 저장하면 되므로 메모리 소비를 크게 줄일 수 있습니다. MLA를 채택하면 DeepSeek-v2의 KV 캐시 크기가 670억 개의 매개변수를 가진 밀집 모델에 비해 93% 이상 감소합니다.

**DeepSeek MoE 구조.** MLA 사용 외에도, DeepSeek 모델은 독특한 MoE 계층 구조를 채택합니다. DBRX와 유사하게, 이 모델들은 세분화된 전문가를 사용합니다. 그러나 이 전문가들 중 일부는 공유됩니다. 이러한 구조를 채택하는 동기는 전문가들 간의 중복 정보를 최소화하면서 더 많은 수의 전문가들 사이에서 전문화를 촉진하는 것입니다. DeepSeek 모델이 사용하는 블록 구조의 전체 개략도는 아래에 제공됩니다.

```
[Image: DeepSeek MoE block structure]
```

(출처: [14])

[14]의 저자들은 DeepSeek-v2가 사용하는 세분화된 전문가를 처리하기 위한 흥미로운 부하 분산 전략도 채택합니다. [2]에서 제안된 보조 부하 분산 손실을 사용하는 것 외에도, DeepSeek-v2는 분산 훈련(distributed training) 중 장치 간 통신 균형을 맞추는 것을 목표로 하는 두 가지 보조 손실 항을 가집니다.

```
[Mathematical formula for device-level load balancing auxiliary loss]
```

**장치 수준 부하 분산 보조 손실 (출처: [14])**

세분화된 전문가를 사용하는 것은 각 토큰을 더 많은 수의 전문가에게 전달해야 함을 의미합니다. 분산 훈련 설정에서, 전문가들은 다른 장치에 있을 수 있으며 각 장치에는 여러 전문가가 상주합니다. 장치 간 통신과 계산이 균형을 이루도록 보장하기 위해, i) 전문가를 상주하는 장치별로 그룹화하고 ii) MoE가 장치별로 균형 잡힌 경로 지정을 수행하도록 유도하는 추가 보조 손실이 필요합니다. 예를 들어, 위에 표시된 보조 손실은 장치 간 균형 잡힌 계산을 촉진합니다. [14]에는 장치 간 균형 잡힌 통신을 촉진하기 위한 추가 손실도 제안되어 있습니다.

```
[Image: DeepSeek-v3 performance comparison]
```

(출처: [15])

DeepSeek-v3 [15]는 DeepSeek-v2 10 의 훨씬 더 큰 버전으로, 총 6710억 개의 매개변수와 370억 개의 활성 매개변수를 가집니다. 이 더 큰 모델은 14.8조 토큰으로 구성된 방대한 코퍼스에서 사전 훈련되었습니다. 사전 훈련 후, 다단계 후처리 훈련 파이프라인(multi-phase post training pipeline)이 적용됩니다:

*   모델은 먼저 두 단계의 컨텍스트 확장 절차를 거치는데, SFT를 통해 최대 컨텍스트 길이가 32K가 되도록 미세 조정된 다음, 다시 128K의 컨텍스트 길이를 가지도록 미세 조정됩니다.
*   컨텍스트 확장 후, 모델은 인간의 선호도에 맞추기 위해 추가 SFT 및 RLHF(Reinforcement Learning from Human Feedback)를 거칩니다.
*   최근 제안된 R1 추론 모델의 기능도 후처리 훈련 중에 DeepSeek-v2에 증류(distilled)됩니다.

최종 DeepSeek-v3 모델은 폐쇄형 모델을 능가하며 심지어 최고의 폐쇄형 LLM과 유사한 성능을 달성합니다. 위 그림을 참조하십시오. DeepSeek-v3는 또한 MoE의 훈련 및 부하 분산 전략에 여러 수정을 가하여 모델의 훈련 프로세스를 효율적이고 안정적으로 만듭니다.

"뛰어난 성능에도 불구하고, DeepSeek-V3는 전체 훈련을 위해 2.788M H800 GPU 시간만 필요합니다. 또한, 훈련 과정은 놀라울 정도로 안정적입니다… 우리는 복구 불가능한 손실 급증을 경험하거나 롤백(rollbacks)을 수행하지 않았습니다." - [15]에서

DeepSeek-v3의 구조는 그 전작에서 영감을 받았습니다. 예를 들어, MLA, 세분화된 전문가 및 공유 전문가는 모두 DeepSeek-v3에서 사용됩니다. 그러나 DeepSeek-v2와 달리 DeepSeek-v3는 다중 토큰 예측(Multi-Token Prediction, MTP) 훈련 목표를 사용합니다. 이 목표는 LLM 훈련에 거의 보편적으로 사용되는 지도 학습(supervised), 교차 엔트로피(cross entropy) 기반 다음 토큰 예측(next token prediction) 목표의 확장입니다. 시퀀스 내 각 토큰에 대해 다음 토큰을 예측하는 대신, MTP는 D개의 미래 토큰을 예측합니다. 이러한 예측은 모델 구조에 추가된 일련의 추가 모듈(additional modules)에 의해 순차적으로 이루어집니다. 아래 그림을 참조하십시오.

```
[Image: Multi-Token Prediction (MTP) objective]
```

(출처: [15])

여러 미래 토큰이 예측되면, 우리는 교차 엔트로피 손실을 정상적으로 적용할 수 있습니다. MTP를 통해 예측된 여러 미래 토큰에 이 손실을 적용하면 모델에 더 풍부한 훈련 신호(training signal)를 제공하여 훈련 효율성과 전반적인 성능을 향상시킵니다. 더 나아가, MTP에 사용되는 이러한 추가 모듈은 추측 디코딩(speculative decoding)을 통해 추론 효율성을 향상시키는 데도 사용될 수 있습니다. 그러나 [15]의 저자들은 MTP 전략이 순전히 모델 성능 향상을 위해 사용되며 — 추가 모듈은 훈련 후 폐기된다고 명시합니다.

```
[Mathematical formula for auxiliary-loss-free load balancing strategy]
```

**보조 손실 없는 부하 분산 전략 (출처: [15])**

DeepSeek-v3는 또한 보조 손실 없는 부하 분산 전략을 사용하는데, 이는 단순히 상위 K 전문가 선택에 전문가별 편향 항(per-expert bias term)을 추가합니다. 위 그림을 참조하십시오. 각 반복마다, 각 전문가에 대한 편향 항은 해당 전문가가 각각 과소 부하(underloaded)되었는지 과부하(overloaded)되었는지에 따라 고정된 계수 γ만큼 증가하거나 감소합니다. 중요하게도, 이러한 편향은 상위 K 전문가를 선택할 때만 사용되며 — 라우터 내에서 전문가 확률 계산에 영향을 미치지 않습니다. 이 접근 방식은 MoE 내에서 전문가 활용을 효과적으로 균형 있게 만들고 부하 분산 손실 사용으로 인한 성능 저하를 제거하는 것으로 밝혀졌습니다. 그러나 [15]의 저자들은 DeepSeek-v3를 훈련할 때 여전히 보조 부하 분산 손실(매우 낮은 스케일링 계수(scaling factor)와 함께)을 사용한다고 언급합니다.

```
[Image: DeepSeek-v3 training cost]
```

(출처: [15])

**훈련 효율성.** 위에 설명된 전략들의 효율성 및 성능 이점 덕분에, DeepSeek-v3는 믿을 수 없을 정도로 경제적입니다. 게다가, 이 모델은 새로운 FP8 혼합 정밀도 훈련 프레임워크(FP8 mixed precision training framework)를 사용하여 훈련되었으며, 이는 대규모 LLM을 위한 8비트 훈련의 첫 번째 검증을 의미합니다. 총체적으로, 최종 모델 훈련 비용은 약 560만 달러로 추정되었습니다 11 . 위 그림을 참조하십시오. 요약하자면, DeepSeek-v3는 다음과 같습니다:

*   매우 경제적인 방식으로 훈련되었으며 (FP8 훈련 및 MTP와 같은 여러 새로운 발전과 함께!)
*   오픈 모델로서는 믿을 수 없을 정도로 인상적이며 — 심지어 최고의 폐쇄형 LLM과도 매우 경쟁적입니다.
*   여러 새로운 수정 사항을 포함하는 흥미로운 MoE 구조를 기반으로 합니다.

**추론.** DeepSeek-v3는 또한 최근 출시된 오픈 추론 모델인 DeepSeek-R1 [13]의 기반 모델 역할을 합니다. 간단히 말해, R1은 OpenAI가 최근 탐구한 o1 스타일 모델의 오픈 복제본입니다. 상세한 기술 보고서에서 설명된 바와 같이, 이 모델은 순수한 강화 학습(reinforcement learning)을 사용하여 극도로 긴 사고의 사슬(chains of thought)을 만들어 복잡한 (검증 가능한) 추론 작업을 해결하는 방법을 학습합니다. 아래 그림에서 볼 수 있듯이, R1의 성능은 특히 오픈 모델로서는 상당히 인상적입니다. 그러나 R1의 기능은 믿을 수 없을 정도로 유능한 기반 모델에 먼저 접근할 수 없었다면 불가능했을 것입니다.

```
[Image: DeepSeek-R1 performance]
```

(출처: [13])

## 최종 고찰

MoE는 언어 모델링에 특히 적합한 많은 이점을 제공합니다. 이들은 계산량의 급격한 증가 없이 더 큰 규모의 탐색을 가능하게 하고, 훈련 비용을 절감하며, 효율적으로 호스팅될 수 있습니다. 희소성이라는 아이디어가 기계 학습 문헌 내에서 오랫동안 존재해 왔지만, MoE는 희소성의 특히 영향력 있는 구현(instantiation)입니다. 이들은 현대 하드웨어와 호환되며 GPU에서 실용적으로 구현될 수 있는 방식으로 희소성을 활용합니다. 흥미롭게도, 초기 MoE 변형 모델들은 복잡성, 불안정성 및 사용의 어려움으로 인해 채택에 난관을 겪었습니다. 그러나 본 개요에서 살펴본 발전들은 MoE를 실용적이고 영향력 있는 기술로 변모시켰습니다 — 이는 단방향 변환기 구조의 간단하면서도 유망한 확장입니다. 앞으로 MoE 기술은 더욱 발전하여, 모델의 지능과 효율성을 한 단계 더 끌어올리는 중요한 역할을 할 것으로 기대됩니다.

뉴스레터가 처음이신가요? 안녕하세요! 저는 Cameron R. Wolfe 이고, 넷플릭스(Netflix)의 딥러닝(Deep Learning) 박사이자 머신러닝 과학자(Machine Learning Scientist)입니다. 이것은 독자들이 AI 연구의 중요한 주제를 더 잘 이해하도록 돕는 딥(러닝) 포커스 뉴스레터입니다. 뉴스레터가 마음에 드시면 구독하고, 공유하거나, X와 LinkedIn에서 저를 팔로우해주세요!

[구독]

## 참고 문헌

[1] Shazeer, Noam, et al. "터무니없이 큰 신경망: 희소하게 게이팅된 전문가 혼합 레이어(Outrageously large neural networks: The sparsely-gated mixture-of-experts layer)." arXiv preprint arXiv:1701.06538 (2017).
[2] Fedus, William, Barret Zoph, and Noam Shazeer. "스위치 트랜스포머: 간단하고 효율적인 희소성으로 조 단위 매개변수 모델로 확장(Switch transformers: Scaling to trillion parameter models with simple and efficient sparsity)." Journal of Machine Learning Research 23.120 (2022): 1-39.
[3] Zoph, Barret, et al. "ST-MoE: 안정적이고 전이 가능한 희소 전문가 모델 설계(St-moe: Designing stable and transferable sparse expert models)." arXiv preprint arXiv:2202.08906 (2022).
[5] Jiang, Albert Q., et al. "전문가들의 믹스트랄(Mixtral of experts)." arXiv preprint arXiv:2401.04088 (2024).
[6] Jiang, Albert Q., et al. "미스트랄 7B(Mistral 7B)." arXiv preprint arXiv:2310.06825 (2023).
[7] Ainslie, Joshua, et al. "GQA: 멀티 헤드 체크포인트에서 일반화된 멀티 쿼리 트랜스포머 모델 훈련(GQA: Training Generalized Multi-Query Transformer Models from Multi-Head Checkpoints)." arXiv preprint arXiv:2305.13245 (2023).
[8] Beltagy, Iz, Matthew E. Peters, and Arman Cohan. "롱포머: 긴 문서 트랜스포머(Longformer: The long-document transformer)." arXiv preprint arXiv:2004.05150 (2020).
[9] xAI. “Grok-1 오픈 릴리스(Open Release of Grok-1)” https://x.ai/blog/grok-os (2024).
[10] xAI. “Grok-1.5 발표(Announcing Grok-1.5)” https://x.ai/blog/grok-1.5 (2024).
[11] Mosaic Research (Databricks). “DBRX 소개: 새로운 최첨단 오픈 LLM(Introducing DBRX: A New State-of-the-Art Open LLM)” https://www.databricks.com/blog/introducing-dbrx-new-state-art-open-llm (2024).
[12] Xue, Fuzhao, et al. "OpenMoE: 오픈 전문가 혼합 언어 모델에 대한 초기 노력(Openmoe: An early effort on open mixture-of-experts language models)." arXiv preprint arXiv:2402.01739 (2024).
[13] Guo, Daya, et al. "DeepSeek-R1: 강화 학습을 통한 LLM의 추론 능력 장려(DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning)." arXiv preprint arXiv:2501.12948 (2025).
[14] Liu, Aixin, et al. "DeepSeek-v2: 강력하고 경제적이며 효율적인 전문가 혼합 언어 모델(Deepseek-v2: A strong, economical, and efficient mixture-of-experts language model)." arXiv preprint arXiv:2405.04434 (2024).
[15] Liu, Aixin, et al. "DeepSeek-v3 기술 보고서(Deepseek-v3 technical report)." arXiv preprint arXiv:2412.19437 (2024).

1 다시 말해, 이 임베딩 계층의 내용은 다른 모델 매개변수와 마찬가지로 훈련 과정 전반에 걸쳐 (경사 하강법을 통해) 업데이트됩니다.
2 하드웨어(예: GPU 활용, 처리량 등) 및 통계적(예: 모델이 데이터로부터 얼마나 빨리 학습하는지) 효율성을 모두 개선하기 위해, 우리는 각 데이터 배치에서 모든 전문가에게 전달되는 토큰 수가 비교적 균일하기를 원합니다.
3 이 양은 우리의 경로 지정 알고리즘에 의해 예측되며, 따라서 미분 가능합니다. 따라서 각 전문가에게 보내지는 토큰의 비율 자체는 미분 가능한 양이 아니더라도 손실 함수 전체는 미분 가능합니다.
4 또한 전문가 수가 증가함에 따라 손실이 일정하게 유지되도록 이 손실 항에 N을 곱해야 합니다.
5 SWA가 각 토큰이 시퀀스 내에서 몇 개의 토큰만 "보도록" 제한하는 것처럼 보일 수 있습니다. 그러나 k개의 연속적인 SWA 계층을 서로 쌓으면 각 토큰의 유효 컨텍스트 윈도우(effective context window)가 증가합니다. 현재 토큰의 표현은 실제로는 그 앞에 오는 k⋅W개의 토큰에 의해 영향을 받습니다.
6 이 두 모델은 모두 2024년 3월에 약 10일 간격으로 출시되었습니다!
7 건초 더미 속 바늘 찾기 테스트는 LLM이 컨텍스트 내에서 정보를 검색하는 능력을 테스트합니다. 그러나 모델이 건초 더미 속 바늘 찾기 테스트에서 완벽한 점수를 받더라도 여전히 좋지 않은 긴 컨텍스트 능력을 가질 수 있습니다. 예를 들어, 긴 컨텍스트가 주어졌을 때 지시 따르기 또는 추론 능력이 훨씬 더 나빠질 수 있습니다.
8 참고로, 동일 팀에서 출시된 이전 모델들 — MPT-7B 및 MPT-30B —은 1조 토큰의 텍스트로만 사전 훈련되었습니다.
9 사전 훈련과 비교하여 SFT 중에는 훨씬 다른 스타일의 데이터를 접하게 됩니다. 예를 들어, 다중 턴 채팅 데이터, 지시 템플릿 등이 있습니다.
10 이 모델들 사이에 DeepSeek은 DeepSeek-v2.5라는 중간 모델도 출시했습니다. 자세한 내용은 여기를 참조하십시오.
11 이 추정치는 H800 GPU 시간당 2달러의 임대 가격을 가정하여 이루어졌습니다. 또한, 이는 최종 모델 훈련의 순수 계산 비용만을 반영하며, 추가 비용이나 실험은 제외됩니다. DeepSeek-v3 훈련의 실제 총 비용은 이 보고된 수치보다 훨씬 클 것이 분명합니다.