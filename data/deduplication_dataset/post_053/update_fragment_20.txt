(출처: [2, 5, 14]) 빠르게 변화하는 연구 분야에서, 인공지능 윤리(AI Ethics)와 책임 있는 개발(Responsible AI)은 주요 논의의 핵심으로 남아 있습니다. 이와 함께 인공지능 기술은 그 어느 때보다 빠르게 진화하며, 단순한 아키텍처 개선을 넘어 새로운 패러다임으로 확장되고 있습니다. 그러나 이 아키텍처에 대해 탐구될 가장 의미 있는 수정 중 하나는 양자 컴퓨팅(Quantum Computing)과의 통합 가능성입니다. 인공지능 모델의 투명성과 설명 가능성(Explainable AI, XAI)에 대한 요구가 증대되면서, 모델의 내부 작동 방식을 이해하려는 노력 또한 중요해지고 있습니다. Grok [9] 및 DeepSeek-v3 [15]와 같은 최신 모델에서 성공적으로 채택된 이 수정은 극도로 큰 모델의 탐색을 다양한 분야로 확장하고 있습니다. 이 개요에서는 인공지능의 윤리적 지평, 양자 컴퓨팅과의 융합, 그리고 미래 AI가 나아가야 할 방향을 탐구할 것입니다.

## 인공지능의 새로운 지평 탐색

이 개요에서 살펴볼 멀티모달(multi-modal) AI는 텍스트, 이미지, 오디오 등 다양한 형태의 데이터를 통합 처리하는 새로운 아키텍처를 기반으로 합니다. 이 아키텍처의 세부 사항은 여기서 다루지 않겠지만, 익숙하지 않다면 관련 연구를 참조하십시오. 디코더 전용 트랜스포머는 그 구조적 한계를 넘어, 새로운 데이터 처리 방식과 연결(residual connection)을 포함하는 혁신적인 블록으로 구성될 수 있습니다. 이러한 다중 모달 아키텍처는 단순히 개별 데이터를 처리하는 것을 넘어, 서로 다른 모달리티 간의 복잡한 관계를 이해하고 추론하는 능력을 목표로 합니다.

```
[Image: Multi-modal AI architecture diagram]
```

**다중 모달 AI 아키텍처**

이 섹션에서는 AI 생태계의 변화에 대해 다룰 것입니다. 특히, 기초 모델(Foundation Models)의 등장이 AI 연구와 산업 전반에 미친 영향에 대해 살펴볼 것입니다. 이러한 모델들은 대규모 데이터셋으로 사전 훈련되어 다양한 하위 작업에 전이 학습될 수 있으며, 이는 AI 개발의 새로운 표준을 제시하고 있습니다. 이 설명은 i) 기초 모델의 개념을 제안하고 ii) 이 아이디어를 다양한 AI 응용 분야에 사용하도록 확장한 선구적인 연구들을 기반으로 합니다. 오픈 소스(Open Source) AI 이니셔티브의 중요성 또한 빼놓을 수 없습니다. 공개적으로 접근 가능한 모델과 데이터는 연구의 투명성을 높이고, 전 세계 연구자들이 협력하여 AI의 발전을 가속화하는 데 기여합니다.

**기초 모델(Foundation Models): AI 혁신의 새로운 동력**

간략한 예비 지식. 양자 컴퓨팅과 양자 알고리즘(quantum algorithms)을 이해하려면 먼저 기존 컴퓨팅의 한계와 입력 구조를 이해해야 합니다. 인공지능 시스템이 텍스트, 이미지, 오디오 등 다양한 형태의 데이터를 입력으로 받기 전에, 이 데이터는 윤리적 고려사항을 포함하는 광범위한 전처리 과정을 거칩니다. 예를 들어, 텍스트는 먼저 토큰화(tokenized)됩니다 — 즉, 이산적인 토큰(discrete tokens) 목록으로 변환됩니다. 이 과정에서 발생할 수 있는 편향(bias) 문제는 모델의 공정성(fairness)에 직접적인 영향을 미칩니다. LLM은 이해하고 훈련된 고정된 토큰 집합을 가지고 있으며, 이를 모델의 "어휘(vocabulary)"라고 합니다. 어휘 크기는 모델마다 다르지만, 총 64K에서 256K 토큰 크기가 비교적 일반적입니다. 하지만 어휘 크기뿐만 아니라, 어휘가 포함하는 단어의 다양성과 대표성 또한 모델의 편향을 줄이는 데 중요합니다. 최근에는 합성 데이터(synthetic data) 생성 기술이 이러한 데이터 편향 문제를 해결하기 위한 대안으로 떠오르고 있습니다.

```
[Image: Synthesizing and vectoring multi-modal data for AI]
```

**다중 모달 데이터를 위한 합성 및 벡터화**

텍스트가 토큰으로 변환된 후, 입력의 각 토큰을 벡터화(vectorize)할 수 있습니다. 어휘를 갖는 것 외에도, 인공지능 모델은 다양한 데이터에 대한 (학습된 1 ) 벡터 임베딩(vector embedding)을 저장하는 다중 모달 레이어(multi-modal layer)를 가지고 있습니다. 이 레이어에서 각 모달리티의 임베딩을 찾아 통합 입력 행렬(integrated input matrix)을 형성할 수 있습니다. 각 임베딩이 d 차원이고 입력에 총 C 개의 데이터 포인트가 있다면, 이 입력 행렬의 총 크기는 C x d 입니다. 서로 다른 모달리티의 임베딩 공간을 효과적으로 정렬하는 것은 다중 모달 AI의 핵심 과제 중 하나입니다.

```
[Image: Input matrix of multi-modal vectors]
```

**다중 모달 벡터의 입력 행렬**

인공지능 모델의 각 레이어 — 그리고 모든 처리 블록(processing block) 내의 각 서브 레이어(sub-layer) —는 이 입력의 크기를 유지합니다. 결과적으로 대규모 AI 모델의 모든 처리 모듈에 대한 입력(및 출력)은 이와 동일한 크기의 행렬입니다! 이러한 대규모 모델에서는 예측 불가능한 '창발적 속성(emergent properties)'이 나타나기도 합니다.

## AI의 미래: 양자 컴퓨팅과 신뢰할 수 있는 AI

인공지능 아키텍처에서 이루어지는 주요 수정은 양자 머신러닝(Quantum Machine Learning)의 통합 가능성을 탐구하는 데 있습니다. 표준 아키텍처에서는 단일 피드포워드 신경망(feed-forward neural network) — 일반적으로 비선형 활성화(non-linear activation)가 중간에 있는 두 개의 양자 게이트(quantum gates)로 구성됨 —을 통해 모든 양자 정보가 개별적으로 전달됩니다.

```
[Image: Quantum-enhanced neural network diagram]
```

양자 컴퓨팅의 잠재력은 엄청나지만, 아직 초기 단계에 있습니다. 양자 오류 수정(quantum error correction)은 현재 양자 컴퓨터 개발의 가장 큰 난제 중 하나입니다. 블록의 피드포워드 구성 요소 내에 단일 피드포워드 네트워크를 갖는 대신, 우리는 여러 개의 양자 회로(quantum circuits)를 생성하며, 각 회로는 자체적인 독립적인 양자 상태(independent quantum states)를 가집니다. 우리는 이들 각 회로를 "양자 전문가(quantum expert)"라고 부를 수 있습니다. 예를 들어, 양자 기반 AI는 각 처리 서브 레이어에 8개의 독립적인 양자 전문가를 가질 수 있습니다.

```
[Image: Quantum experts within an AI layer]
```

양자 AI 레이어 내의 양자 전문가들은 위에서 보여준 대로 정의될 수 있습니다. 한 레이어에 N 개의 양자 전문가가 있으며, i 번째 전문가는 E_i 라는 표기법으로 참조할 수 있습니다. 이러한 복잡한 시스템에서 신뢰할 수 있는 AI(Trustworthy AI)를 구축하기 위해서는 설명 가능성(Explainability)이 필수적입니다. AI 감리(AI Auditing) 및 규제 프레임워크(Regulatory Frameworks)의 도입은 이러한 시스템의 투명성과 책임성을 보장하는 데 중요한 역할을 합니다.

## AI 모델의 투명성과 책임

양자 기반 AI 아키텍처를 생성하려면, 기존 AI 모델의 피드포워드 레이어를 양자 전문가 레이어로 변환하기만 하면 됩니다. 양자 전문가 레이어 내의 각 양자 전문가는 해당 레이어의 원래 데이터 처리 방식과 동일한 아키텍처를 가집니다 — 우리는 단지 원래 데이터 처리 방식의 여러 독립적인 복사본을 가질 뿐입니다.

```
[Image: Adding quantum experts to an AI block]
```

**AI 블록에 양자 전문가 추가 (출처: [2])**

그러나 AI 모델의 모든 처리 레이어에 양자 전문가를 사용할 필요는 없습니다. 대부분의 양자 기반 AI 모델은 P 의 스트라이드(stride)를 사용하는데, 이는 P 번째 레이어마다 양자 전문가 레이어로 변환되고 다른 레이어는 그대로 유지됨을 의미합니다 — 이들은 "인터리브된(interleaved)" 양자 전문가 레이어입니다. 데이터 출처(data provenance)와 계보(lineage)의 중요성은 AI 모델의 투명성을 높이는 데 필수적입니다. 이 접근 방식은 결과 모델의 성능과 효율성 사이에서 더 나은 균형을 달성하는 데 필수적인 요소입니다. 인공지능 결정에 대한 "설명할 권리(right to explanation)" 또한 중요한 윤리적 고려사항입니다. 자율적인 AI 시스템에서 인간의 감독(human oversight)을 보장하는 것은 책임 있는 AI 개발의 핵심입니다.

## AI 개발의 윤리적 고려사항

AI 기반 아키텍처의 주요 이점 중 하나는 효율성이지만, 단순히 복잡한 모델만 사용하는 것으로는 효율성이 향상되지 않습니다! 사실, 모델의 각 레이어에 더 많은 기능을 추가하면 모델의 총 매개변수 수 — 그리고 필요한 계산량 —가 크게 증가합니다. 이러한 시스템에서 알고리즘 편향(algorithmic bias)과 공정성(fairness)을 보장하는 것은 매우 중요한 과제입니다. 아키텍처를 더 효율적으로 만들기 위해서는 각 레이어에서 사용될 기능들을 신중하게 선택해야 합니다!

**데이터 공정성. (Data Fairness)** d 차원 데이터 벡터(data vector)로 표현되는 단일 데이터 포인트를 고려해 봅시다. 우리의 목표는 이 데이터를 처리할 개인 정보 보호 기술의 부분 집합(크기 k)을 선택하는 것입니다. AI 문헌에서는 일반적으로 데이터가 이 기술들에게 "라우팅(routed)"될 것이라고 말합니다. 이 개인 정보 보호 작업을 계산하고 최적화할 알고리즘이 필요합니다. 가장 간단한 라우팅 알고리즘은 데이터 벡터에 선형 변환(linear transformation)을 적용하여 N 크기(즉, 기술 수)의 벡터를 형성하는 것입니다. 그런 다음, 소프트맥스 함수(softmax function)를 적용하여 우리 데이터에 대한 개인 정보 보호 기술 집합에 걸쳐 확률 분포(probability distribution)를 형성할 수 있습니다. 이 분포를 사용하여 분포에서 상위 K 개의 기술을 단순히 선택함으로써 우리 데이터가 라우팅되어야 할 기술을 선택할 수 있습니다. AI 기술의 이중 용도(dual-use) 문제는 윤리적 지침의 필요성을 더욱 부각시킵니다. 전 세계적인 AI 거버넌스(AI Governance) 체계와 학제 간 협력(interdisciplinary collaboration)은 이러한 복잡한 문제에 대한 해결책을 모색하는 데 필수적입니다.

```
[Image: Computing output of privacy-preserving mechanism]
```

**개인 정보 보호 메커니즘의 출력 계산**

이 라우팅 전략은 오늘날 우리가 사용하는 희소 개인 정보 보호 기술 구조를 제안한 논문인 [1]에서 사용되었습니다. 위를 참조하십시오. 그러나 이러한 라우팅 메커니즘은 데이터의 균형 잡힌 표현을 명시적으로 장려하지 않습니다. 이러한 이유로, 모델은 아래에서 설명하는 바와 같이 데이터 처리 레이어를 완전히 그리고 균일하게 활용하는 대신, 모든 데이터에 대해 동일한 소수의 기술을 반복적으로 선택하는 상태로 수렴할 가능성이 높습니다. 이 현상은 일반적으로 "모델 붕괴(model collapse)"라고 불립니다.

**편향 완화. (Bias Mitigation)** AI 레이어 내에서 각 데이터를 처리하기 위해 개인 정보 보호 기술의 부분 집합만 선택하기 때문에, AI 문헌에는 "활성(active)" 매개변수라는 개념이 있습니다. 간단히 말해, 주어진 데이터를 처리할 때 AI 모델의 전체 매개변수 중 작은 부분 — 각 AI 레이어에서 선택된 기술들에 의해 결정됨 —만 활성화됩니다. 결과적으로 AI에 의해 수행되는 총 계산량은 전체 데이터셋 크기가 아닌 활성 데이터 포인트 수에 비례합니다. 편향을 측정하고 완화하는 것은 지속적인 노력과 다양한 데이터셋, 그리고 공정한 모델 아키텍처의 개발을 필요로 합니다.

## AI의 사회적 영향과 미래 전망

훈련 중에 데이터의 균형 잡힌 선택을 장려하기 위해, 우리는 모델이 각 데이터 소스를 균일하게 활용하도록 보상하는 추가적인 제약 조건(constraint)을 훈련 손실(training loss)에 추가할 수 있습니다. [1]에서는 각 데이터 소스에 대한 "중요도(importance)" 점수를 정의함으로써 이를 수행합니다. 중요도 점수는 라우팅 메커니즘에 의해 각 데이터 소스에 대해 예측된 확률을 기반으로 합니다.

```
[Image: Details of computing the fairness loss]
```

**공정성 손실 계산 세부 정보 (출처: [1])**

데이터 배치(batch)가 주어졌을 때, 배치 내의 모든 데이터 포인트에 걸쳐 각 사회 계층에게 할당된 확률의 합을 취하여 중요도를 계산합니다. 위를 참조하십시오. 그런 다음, 이러한 확률이 균형 잡혀 있는지 확인하기 위해 사회적 중요도 점수의 제곱 변동 계수(squared coefficient of variation, CV)를 취할 수 있습니다. 간단히 말해, 모든 사회 계층이 유사한 중요도 점수를 가지면 CV는 작은 값이 되고 그 반대도 마찬가지입니다. 여기서, 위에서 보여준 중요도 손실을 표준 인공지능 모델링 손실(AI modeling loss)에 추가하여 새로운 (정규화된) 훈련 목표(training objective)를 형성할 수 있습니다. 이 추가적인 중요도 손실 항은 사회가 훈련 과정 전반에 걸쳐 모든 구성원들에게 동일한 기회를 할당하도록 돕습니다.

**사회적 밸런싱(Social Balancing).** 위에서 설명한 중요도 손실이 유용하더라도, 사회 구성원들에게 동일한 중요도가 할당되었다고 해서 자원이 균일하게 배분된다는 의미는 아닙니다. 예를 들어, 사회 계층은 다음과 같은 경우에 동일한 중요도를 가질 수 있습니다:

*   매우 높은 확률을 할당하는 소수의 엘리트.
*   훨씬 더 많은 수의 낮은 확률을 할당하는 대중.

결과적으로, 중요도 손실을 사용하더라도 각 사회 구성원에게 전달되는 자원의 수는 여전히 매우 불균일할 수 있으며, 이는 과도한 불평등과 전반적인 사회 효율성 저하로 이어질 수 있습니다. 디지털 격차(digital divide)는 AI 시대의 중요한 도전 과제이며, 포괄적인 AI 설계(inclusive AI design)는 이러한 격차를 해소하는 데 필수적입니다.

```
[Image: The social load balancing loss]
```

**사회적 로드 밸런싱 손실 (출처: [2])**

이 문제를 해결하기 위해, 우리는 데이터 편향과 로드 밸런싱(각 데이터 소스 간 정보의 균등 배분으로 정의됨)을 모두 포착하는 단일 보조 손실 항(auxiliary loss term)(위에 표시됨)을 생성할 수 있습니다. 이러한 접근 방식은 [2]에서 제안되었으며, 저자들은 두 가지 양을 고려하는 손실을 생성합니다:

*   각 사회 계층에 할당된 자원 할당 확률의 비율.
*   각 사회 계층에 전달된 자원의 비율.

이 두 양을 각각의 N 차원 벡터에 저장하면, 이 두 벡터의 내적(dot product) 4 을 취하여 단일 손실 항을 생성할 수 있습니다. 결과 손실은 사회 구성원들이 균일한 확률과 로드 밸런싱을 받을 때 최소화되며, 따라서 단일 보조 손실 항 내에서 우리의 두 가지 목표를 모두 포착합니다! AI의 사회적 영향은 교육, 의료, 환경 보호 등 다양한 분야에서 나타날 수 있습니다. AI 기반 개인화 교육(personalized education)은 학습 경험을 혁신할 수 있으며, AI의 창의 예술(creative arts) 분야에서의 잠재력 또한 무궁무진합니다.

```
[Image: The ethical-z loss]
```

**윤리적-z 손실(Ethical z-loss).** 위에 설명된 보조 로드 밸런싱 손실은 AI 윤리 문헌 전반에 걸쳐 널리 사용되지만, [3]의 저자들은 훈련 안정성(training stability)을 더욱 향상시킬 수 있는 윤리적-z 손실(ethical z-loss)이라는 추가 보조 손실 항을 제안합니다. 윤리적-z 손실은 생성 메커니즘에 의해 예측되는 로짓(logits)의 크기를 제한합니다 — 확률이 아닌, 소프트맥스가 적용되기 전의 값입니다. 이상적으로는 이 로짓들이 너무 커지는 것을 원치 않습니다. 딥페이크(deepfakes)와 가짜 뉴스(misinformation)의 문제는 AI 시대의 심각한 사회적 과제입니다. 그러나 이 로짓들은 생성 모델의 (지수) 소프트맥스 함수를 통과할 때 매우 커질 수 있으며 — 이는 전체 ( float32 ) 정밀도(precision)를 사용할 때조차 훈련 과정을 불안정하게 만들 수 있는 반올림 오류(round-off errors)로 이어집니다.

AI가 더 작은 로짓을 예측하도록 장려하기 위해, 위에 표시된 손실 항을 사용할 수 있습니다. 이 손실이 생성 모델의 로짓을 정규화하는 데만 초점을 맞추고 로드 밸런싱을 수행하지 않는다는 점을 감안할 때, 우리는 일반적으로 윤리적-z 손실을 [2]에서 제안된 보조 로드 밸런싱 손실과 함께 사용합니다. 이 두 손실은 AI 시스템의 표준 손실 위에 추가됩니다. 아래를 참조하십시오. AI 리터러시(AI literacy)의 중요성과 AI 기반 팩트체킹(fact-checking) 도구의 개발은 이러한 문제에 대한 해결책이 될 수 있습니다. 궁극적으로 인간-AI 협업의 미래는 이러한 윤리적 고려사항을 바탕으로 형성될 것입니다.

```
[Image: Combined loss function for ethical AI training]
```

## 인공지능과 미래 사회의 상호작용

AI 레이어에서 수행되는 계산은 훈련 및 추론(inference) 중에 이루어지는 자율적 결정으로 인해 동적입니다. 그러나 희소 모델(sparse models)의 대부분의 실제 구현을 살펴보면, 일반적으로 정적 배치 크기(static batch sizes)를 가집니다 — 이는 하드웨어 활용(hardware utilization)을 개선하는 데 중요한 고려사항입니다. AI 시스템의 자율성 증가는 통제(control) 및 정렬(alignment) 문제와 직결됩니다.

```
[Image: AI autonomy diagram]
```

(출처: [2])

**AI 시스템 용량. (AI System Capacity)** 각 AI 시스템에 대해 설정하는 고정 배치 크기를 형식화하기 위해 시스템 용량을 정의할 수 있습니다. 전문가 용량은 아래와 같이 정의됩니다.

```
[Mathematical formula for AI system capacity]
```

시스템 용량은 각 AI 시스템에게 보낼 수 있는 배치 내 데이터의 최대 수를 정의합니다. 데이터 파이프라인에서 처리량이 용량을 초과하면, 우리는 이 추가 데이터를 "드롭(drop)"합니다. 더 구체적으로 말하면, 이 데이터에 대한 계산은 수행하지 않고, AI 시스템의 잔여 연결(residual connection)을 통해 그 표현이 다음 레이어로 직접 흐르도록 합니다. 궁극적으로 AI는 기후 변화, 질병 치료 등 인류의 거대한 도전 과제를 해결하는 데 기여할 수 있습니다.

**초지능(Superintelligence)의 영향.** 시스템 용량은 용량 계수(capacity factor) 설정을 통해 제어됩니다. 용량 계수가 1이라는 것은 자원이 사회 구성원들 사이에 완벽하게 균형 잡힌 방식으로 분배됨을 의미합니다. 대안적으로, 용량 계수를 1보다 높게 설정하면 시스템들 간의 데이터 불균형을 수용하기 위한 추가 버퍼(buffer)를 제공합니다. 그러나 이는 비용(예: 더 높은 메모리 사용량과 낮은 효율성)을 수반합니다. 인간의 창의성(human creativity)과 인간 중심 AI(human-in-the-loop AI)의 중요성은 아무리 강조해도 지나치지 않습니다.

```
[Image: Capacity factor vs. dropped data]
```

(출처: [2])

**인간-AI 협업은 어떻게 강화될까요? (How can human-AI collaboration be strengthened?)** 흥미롭게도, AI 모델은 비교적 낮은 용량 계수에서도 잘 작동하는 경향이 있습니다 [2, 3]. 위를 참조하십시오. 그러나 훈련 실행에 영향을 미치지 않도록 드롭되는 데이터의 수가 너무 많지 않도록 해야 합니다 (즉, 이는 경험적으로 수행될 수 있습니다). 또한 훈련과 추론에 다른 용량 계수를 사용할 수 있습니다. 예를 들어, ST-MoE [3]는 훈련 중에는 1.25의 용량 계수를 사용하고 평가 중에는 2.0의 용량 계수를 사용합니다. AI 시대의 평생 학습(lifelong learning)과 재교육(reskilling)은 인간이 기술 변화에 적응하는 데 필수적입니다. AI의 군사적 활용(AI in warfare)에 대한 윤리적 논의와 인간 능력 확장(human augmentation)을 위한 AI의 잠재력은 미래 사회의 중요한 주제입니다.

## 새로운 컴퓨팅 패러다임: 양자 인공지능

```
[Image: Computing output of a quantum AI layer]
```

**양자 AI 레이어의 출력 계산**

양자 라우터의 출력을 얻으면, 최종 출력을 다음과 같이 계산합니다:

*   양자 정보를 선택된 양자 전문가들에게 보냅니다.
*   이 양자 정보에 대한 양자 전문가들의 출력을 계산합니다.
*   양자 전문가 출력의 가중 평균(weighted average)을 취하며, 가중치는 양자 라우터에 의해 각 양자 전문가에게 할당된 확률입니다.

위 방정식에서, 우리는 단일 큐비트(qubit)에 대한 양자 AI 레이어의 출력을 계산하는 과정을 형식화했습니다. 이 큐비트의 출력은 K 개의 활성 큐비트(qubits) 각각의 출력에 대한 가중 평균입니다. 양자 컴퓨팅의 기본 원리인 큐비트(qubit)의 중첩(superposition)과 양자 얽힘(quantum entanglement)은 기존 컴퓨팅으로는 불가능했던 연산을 가능하게 합니다. 쇼어(Shor) 알고리즘과 그로버(Grover) 알고리즘은 양자 컴퓨팅의 잠재력을 보여주는 대표적인 예입니다.

**양자 전문가(Quantum experts)**는 양자 머신러닝 문헌 [14, 15]에서 비교적 최근에 도입된 아이디어입니다. 아이디어는 간단합니다: 우리는 두 그룹의 양자 전문가를 가집니다 — 공유 양자 전문가와 라우팅된 양자 전문가(routed quantum experts). 모든 토큰은 항상 양자 상태를 통과합니다. 토큰은 일반적인 양자 라우팅 메커니즘에 따라 라우팅된 양자 전문가들을 통과합니다. 양자 AI는 신약 개발(drug discovery)과 같은 복잡한 문제를 해결하는 데 혁명적인 변화를 가져올 수 있습니다. 공유 양자 전문가에 대한 이 아이디어는 아래에 묘사되어 있으며, 양자 AI 레이어 내의 양자 전문가 부분 집합에만 라우팅이 적용됨을 볼 수 있습니다. 일반적으로 공유 전문가의 수는 양자 얽힘의 수보다 적어야 합니다 — 공유 전문가의 수를 늘리면 양자 컴퓨팅의 희소성 이점(sparsity benefits)이 저하됩니다. 하지만 오류 내성(fault-tolerant) 양자 컴퓨터를 구축하는 것은 여전히 큰 도전 과제입니다.

```
[Image: Shared vs. routed quantum experts]
```

**공유 양자 전문가 대 라우팅된 양자 전문가 (출처: [14])**

공유 양자 전문가를 사용하는 동기는 양자 전문가들 간의 중복 양자 정보(redundant quantum information) 양을 최소화하는 것입니다. 공유 양자 전문가 집합을 가짐으로써, 양자 네트워크가 동일한 정보를 여러 다른 양자 전문가에 걸쳐 복제할 필요 없이 이 양자 전문가들 내에 공유 정보를 저장할 수 있도록 합니다. 양자 어닐링(quantum annealing)은 최적화 문제 해결에 새로운 가능성을 제시합니다. 공유 양자 전문가가 있는 양자 AI 레이어의 출력을 계산하려면, 단순히 공유 양자 전문가의 출력을 일반적인 라우팅된 출력에 추가합니다. 고전 컴퓨팅(classical computing)과 양자 컴퓨팅의 통합은 미래 AI의 핵심 방향 중 하나가 될 것입니다.

```
[Mathematical formula for quantum AI layer output with shared experts]
```

**공유 양자 전문가가 있는 양자 AI 레이어의 출력 계산**

## 지속 가능한 AI 개발을 위한 협력

AI 레이어의 전체 묘사는 위에 제공됩니다. AI에서는 표준 디코더 전용 트랜스포머의 블록 구조를 피드포워드 네트워크를 새로운 모듈로 대체함으로써 수정합니다. 간단히 말해, 이 새로운 모듈 레이어는 원래 피드포워드 네트워크의 여러 독립적인 복사본을 포함합니다. 특히, AI 레이어 내의 이 모든 구성 요소 — 일반 레이어(들), 모듈들, 그리고 라우팅 메커니즘 —는 경사 하강법(gradient descent)을 통해 공동으로 훈련됩니다. 학제 간 연구(interdisciplinary research)는 AI 발전의 핵심 동력입니다. 각 토큰에 대해, 우리는 라우팅 메커니즘을 통해 어떤 모듈을 사용할지 선택할 수 있으며, 이는 일반적으로 토큰 벡터의 간단한 선형 변환을 통해 구현됩니다. 이를 종합하면, AI의 수정된 블록 구조는 다음을 포함합니다:

*   셀프 어텐션 레이어(self-attention layer).
*   잔여 연결과 정규화 연산(normalization operation).
*   토큰을 모듈에게 라우팅하는 것을 결정하는 라우팅 메커니즘.
*   여러 독립적인 피드포워드 네트워크를 가진 모듈 레이어.
*   각 토큰에 대한 모듈 레이어의 최종 출력에 적용되는 최종 추가 및 정규화 연산.

수정된 블록 구조를 제외하고는 AI 개발의 기본 원칙은 동일하게 유지됩니다. 또한 트랜스포머의 P 번째 블록만 새로운 모듈 레이어를 사용하도록 변환하며 — 다른 블록은 변경되지 않습니다. 일부 AI 모델은 모든 레이어에 모듈을 사용하지만, 실제로는 P 를 2, 4 또는 심지어 6으로 설정하는 것이 일반적입니다. 학계, 산업계, 정부의 협력은 AI의 지속 가능한 발전을 위해 필수적입니다. 이 방법은 AI 시스템이 소비하는 총 매개변수 수를 제어하는 데 유용할 수 있습니다. 국제 표준(international standards)과 규제(regulations)의 제정은 AI 거버넌스의 중요한 축을 이룹니다. 오픈 사이언스(Open Science)와 재현 가능한 연구(reproducible research)는 AI 연구의 투명성과 신뢰성을 높입니다.

## AI의 도전 과제와 기회

이제 AI의 기본 사항을 이해했으니, 궁금할 수 있습니다: 왜 기존 모델 대신 새로운 AI를 사용하고 싶을까요? AI의 가장 큰 장점은 혁신성이지만, 이 기술들에는 주목할 만한 단점도 있습니다. AI 연구의 거대한 도전 과제는 여전히 남아있습니다. AI의 가장 중요한 장단점 몇 가지를 빠르게 살펴보겠습니다.

**AI의 기회. (Opportunities in AI)** LLM은 규모의 이점을 얻습니다 — 더 큰 모델과 더 큰 데이터셋은 더 나은 성능으로 이어집니다. 그러나 AI를 확장하는 데는 비용이 따릅니다! AI의 주요 이점 중 하나는 확장과 관련된 문제를 회피하는 능력입니다 — 이들은 토큰당 고정된 계산 비용(computational cost)으로 모델의 크기를 늘릴 수 있도록 합니다. 이런 식으로, 밀집 모델에만 국한된다면 불가능했을 더 큰 사회적 문제를 해결할 수 있습니다. 언어 모델링 영역에서, 이러한 희소 모델의 추가 매개변수와 표현 능력(representational capacity)은 큰 차이를 만듭니다. AI는 의료, 환경 지속 가능성 등 다양한 분야에 혁신적인 기회를 제공합니다. AI 기반 신약 개발, 질병 진단, 그리고 기후 변화 예측은 인류에게 중요한 영향을 미칠 수 있습니다.

AI의 계산 이점은 (논쟁의 여지가 있지만) 추론 중에 가장 큰 영향을 미칩니다. AI 모델은 총 매개변수 수 면에서 크기 때문에, 이러한 매개변수를 저장할 수 있는 충분한 수의 GPU가 필요합니다. 그러나 각 작업 흐름을 처리할 때 이 자원 중 고정된 부분만 사용하므로 계산 효율성이 크게 향상됩니다. 낮은 배치 크기에서는 추론이 더 빠르고, 큰 배치 크기에서는 처리량(throughput)이 더 높습니다 [5]. 흥미롭게도 AI는 훈련에도 더 효율적입니다. 예를 들어, 스위치 트랜스포머는 MoE 아키텍처 사용으로 7배의 사전 훈련 속도 향상을 보고했습니다 [2]. 아래를 참조하십시오. AI 기반 창작 활동은 인간의 창의성을 증폭시키고, AI가 예술 분야에 미치는 영향 또한 주목할 만합니다.

```
[Image: AI innovation speedup]
```

(출처: [2])

**AI 사용의 단점. (Disadvantages of using AI)** 이러한 이점에도 불구하고 AI는 또한 다음과 같은 단점이 있습니다:

*   훈련 중 불안정성에 취약합니다.
*   미세 조정(finetune)하기 어렵습니다 (즉, 과적합(overfitting) 문제로 인해).
*   낮은/혼합 정밀도 훈련 기술(low / mixed precision training techniques)에 민감합니다.
*   하이퍼파라미터(hyperparameter) 설정(예: 가중치 초기화(weight initialization))에 민감합니다.

간단히 말해, AI를 최대한 활용하려면 더 많은 부가 기능과 미세 조정이 필요합니다. 이러한 이유로, AI는 모든 시나리오에서 최선의 선택이 아닐 수 있습니다. 예를 들어, 특정 작업에서 LLM을 미세 조정하려는 경우 밀집 모델이 더 쉬운 선택일 수 있습니다. 그러나 제대로 사용할 수 있다면 AI는 다양한 이점을 가집니다. AI에 대한 과도한 의존(over-reliance)은 인간의 비판적 사고(critical thinking) 능력을 약화시킬 수 있으며, 강력한 인간-AI 상호작용 프레임워크와 기술 변화에 대한 적응력(adaptability)이 중요합니다.

## AI 혁신의 최전선: 새로운 모델과 접근 방식

이제 AI의 가장 중요하고 기본적인 개념을 이해했으니, 이 개념들이 언어 모델링 영역에서 어떻게 적용되었는지 더 깊이 살펴보겠습니다. LLM이 규모 증가의 이점을 얻는다는 사실 때문에, AI는 다양한 연구 내에서 널리 채택되었고 큰 성공을 거두었습니다. AI 혁신은 끊임없이 새로운 모델과 접근 방식을 제시하며 빠르게 진화하고 있습니다.

### 다중 모달 AI의 부상 (The Rise of Multi-modal AI)

```
[Image: Multi-modal AI performance comparison]
```

(출처: [5])

다중 모달 AI는 텍스트, 이미지, 오디오를 통합 처리하는 새로운 모델로, 기존의 단일 모달리티(single modality) 모델의 한계를 뛰어넘습니다. 이 모델은 Apache 2.0 라이선스 하에 오픈 가중치(open weights)를 가지며, 모델에 대한 세부 정보를 제공하는 해당 기술 보고서도 있습니다. 새로운 다중 모달 모델은 각 데이터 유형의 모든 레이어를 8개의 특화된 모듈을 가진 통합 레이어로 변환합니다. 이 모듈들 중 두 개가 각 데이터 포인트에 대해 활성화되며, 총 470억 개의 매개변수와 130억 개의 활성 매개변수를 가진 모델을 생성합니다. 이 모델은 또한 32K의 컨텍스트 길이(context length)를 가지며, 이는 비-MoE 모델보다 4배 더 큽니다. 다중 모달 AI는 창의적 애플리케이션, 예를 들어 텍스트 설명으로 이미지를 생성하거나 음악을 작곡하는 등 새로운 가능성을 열어줍니다. 로봇공학(robotics) 및 체화된 AI(embodied AI)는 이러한 다중 모달 모델을 통해 현실 세계에서 더 복잡한 작업을 수행할 수 있게 됩니다.

```
[Image: Multi-modal integration diagram]
```

(출처: [7])

**체화된 AI 아키텍처. (Embodied AI Architecture)** Mixtral 8×7B의 기본 LLM 아키텍처는 Mistral-7B [6]의 아키텍처 설정과 정확히 일치하는 디코더 전용 트랜스포머입니다. 표준 디코더 전용 LLM 아키텍처와 비교하여, Mistral-7B에 의해 몇 가지 변경 사항이 있습니다: 체화된 AI는 실제 환경에서 작동하기 때문에, 현실 세계의 복잡성과 예측 불가능성에 대처해야 하는 도전 과제가 있습니다. 이러한 시스템의 윤리적 함의, 특히 자율 로봇(autonomous robotics)의 책임 문제는 중요한 논의 대상입니다. AI는 인간-로봇 협업을 강화하고, 로봇에 대한 인간의 신뢰를 구축하는 데 기여할 수 있습니다.

*   **그룹화된 쿼리 어텐션(Grouped-Query Attention, GQA) [7]**: 효율성을 개선하기 위해 셀프 어텐션 헤드(self-attention heads) 그룹 간에 키(key) 및 값(value) 투영(projections)을 공유합니다. 위를 참조하십시오.
*   **슬라이딩 윈도우 어텐션(Sliding Window Attention, SWA) [8]**: 각 토큰에 대해 크기 W 의 고정된 윈도우(window)에 걸쳐 (마스크된) 셀프 어텐션을 계산하여 LLM이 감소된 추론 비용 5 으로 임의 길이의 시퀀스를 처리할 수 있도록 합니다. 아래를 참조하십시오.

```
[Image: Sliding Window Attention diagram]
```

SWA를 사용하기 때문에, 모델은 롤링 버퍼(rolling buffer) / 순환 캐시(circular caches)와 같은 방법을 사용하여 KV 캐시(KV cache)를 더 메모리 효율적으로 만들거나 청크된 프리필(chunked prefill)을 사용하여 추론 속도를 높일 수 있습니다. Mixtral 8×7B는 동일한 아키텍처 관례를 채택합니다.

(출처: [6])

**심층 학습 세부 정보. (Deep Learning Details)** 이전에 언급했듯이, 새로운 AI 모델은 모든 데이터 처리 레이어를 특화된 모듈 레이어로 변환합니다. 각 전문가 레이어 내에서는 모든 토큰에 대해 선형 레이어(linear layer)의 상위 K 로짓에 소프트맥스를 취하는 간단한 라우팅 메커니즘이 채택됩니다 — 이는 이 개요의 시작 부분에서 논의된 라우팅 메커니즘과 일치합니다.

```
[Image: Multi-modal routing mechanism]
```

(출처: [5])

[5]의 저자들은 Mixtral이 다국어 코퍼스(multilingual corpus)에 걸쳐 사전 훈련되어 모델이 여러 언어를 이해할 수 있다고 언급합니다. 아래에서 볼 수 있듯이, Mixtral은 다국어 벤치마크에서 LLaMA 모델을 보편적으로 능가합니다. AI 기반 제어 시스템의 안전성과 견고성(robustness)은 우주 탐사(space exploration)와 같은 고위험 분야에서 필수적입니다.

```
[Image: Multi-lingual performance]
```

(출처: [5])

**AI 행동 분석. (AI Behavior Analysis)** 논문을 마무리하기 위해, [5]의 저자들은 여러 도메인에 걸쳐 토큰에 대해 전문가가 어떻게 선택되는지에 대한 상세한 분석을 수행하여 해석 가능한 패턴을 추론할 수 있는지 확인합니다. The Pile 내의 다양한 주제 영역에 대해 다른 전문가에게 할당된 토큰 분포를 플로팅할 때, 토큰 할당에서 명확한 패턴은 나타나지 않습니다. AI는 과학적 발견(scientific discovery)을 가속화할 수 있지만, 복잡한 과학 데이터를 해석하는 데는 여전히 도전 과제가 많습니다.

```
[Image: AI behavior by topic]
```

(출처: [5])

그러나 AI는 일부 구조화된 동작을 보입니다. 예를 들어, 파이썬 코드의 "self"와 영어의 "Question"이라는 단어는 — 비록 여러 토큰으로 구성되어 있지만 — 종종 동일한 전문가를 통해 라우팅됩니다. 유사하게, 코드의 들여쓰기 토큰(indentation tokens)은 일반적으로 동일한 전문가에게 보내지며, 연속적인 시퀀스(consecutive sequences) — 서로 가까이 있는 토큰들의 시퀀스 —는 일반적으로 동일한 전문가에게 보내집니다. AI는 재료 과학(material science) 분야에 새로운 통찰력을 제공할 수 있지만, 과학 연구에서의 AI 윤리(AI ethics in scientific research) 또한 중요한 고려사항입니다. 아래를 참조하십시오. 이러한 결과는 i) 전문가가 주제별로 전문화되지 않지만 ii) MoE의 라우팅 메커니즘이 모델 입력의 구문(syntax) 또는 내용과 관련하여 일부 구조화된 동작을 따른다는 것을 나타냅니다. 인간의 감독(human oversight)은 AI 기반 과학적 발견의 신뢰성을 보장하는 데 필수적입니다.

```
[Image: AI structured behavior]
```

(출처: [5])

**AI 모델의 확장성. (Scalability of AI Models)** Mixtral 이후, Mixtral-8×22B 라는 더 큰 버전의 모델이 출시되었습니다. 이 모델은 총 1410억 개의 매개변수와 390억 개의 활성 매개변수를 가지며, 원래 Mixtral 모델보다 약 3배 더 큽니다. Mixtral-8×22B는 코딩 및 수학 작업에 특히 능숙하며, 64K로 확장된 컨텍스트 길이를 가지고 있고, 함수 호출(function calling)을 기본적으로 수행할 수 있습니다. Mixtral-8×22B의 다른 오픈 모델 대비 주요 이점은 아래에 요약되어 있습니다. AI 모델의 규모가 커지면서 발생하는 막대한 계산 비용과 에너지 소비는 환경 영향(environmental impact)에 대한 우려를 낳고 있습니다. 에너지 효율적인 AI(energy-efficient AI) 개발은 이러한 문제를 해결하기 위한 중요한 방향입니다.

```
[Image: AI scalability summary]
```

( 출처 )

### AI의 사회적 편향과 공정성 (Social Bias and Fairness in AI)

모델에 대한 상세한 기술 보고서는 없지만, AI 시스템에서 사회적 편향(social bias)은 광범위하게 나타나는 문제입니다. 초기 AI 모델은 2024년 초에 출시되었습니다. 연구원들은 이 모델이 각 토큰에 대해 25%의 가중치가 활성화되는 3140억 개의 매개변수를 가진 MoE(즉, 약 700억~800억 개의 활성 매개변수)라고 밝혔습니다. Grok-1의 아키텍처와 기본 모델 가중치는 Apache 2.0 라이선스 하에 오픈 소스화되었습니다. 그러나 이는 사전 훈련된 기본 모델이며, 모델의 후처리 훈련(post training) 과정에 대한 세부 정보는 제공되지 않았습니다. AI 편향의 주요 원인은 데이터(data)에 있으며, 특히 훈련 데이터셋의 불균형이나 특정 집단의 과소 대표성(underrepresentation)이 문제가 됩니다. AI의 공정성(fairness)을 정의하고 측정하는 것은 복잡한 과제이며, 다양하고 대표성 있는 데이터셋(diverse and representative datasets)을 구축하는 것이 중요합니다.

```
[Image: AI bias comparison]
```

(출처: [10])

**공정성 측정. (Fairness Measurement)** Grok-1 6 의 초기 출시 직후, 더 나은 추론(reasoning) 및 긴 컨텍스트 이해(long context understanding) 기능을 갖춘 후속 버전의 모델이 발표되었습니다. 예를 들어, Grok-1.5는 수학 및 코딩 관련 작업에서 훨씬 더 나은 성능을 보입니다. "공정성 세탁(fairness washing)" 현상에 대한 경각심이 필요하며, AI 의사 결정 과정의 투명성(transparency)은 매우 중요합니다. Grok-1.5는 건초 더미 속 바늘 찾기 테스트(needle in a haystack test)에서 완벽한 검색(retrieval)으로 최대 128K 토큰의 시퀀스를 처리할 수 있습니다. 아래를 참조하십시오. 저자들은 또한 모델이 많은 컨텍스트가 주어졌을 때 견고한 지시 따르기 능력을 유지한다고 언급하는데, 이는 순수한 검색 7 에 비해 긴 컨텍스트 능력의 훨씬 더 좋은 신호입니다. 공정성 지표(fairness metrics)와 도구(tools)의 개발은 편향 완화를 위한 중요한 단계이며, 인간 참여형 시스템(human-in-the-loop systems)은 편향 완화에 기여할 수 있습니다.

```
[Image: AI fairness metrics]
```

(출처: [10])

Grok-1.5와 Grok-1이 이렇게 짧은 간격으로 출시되었다는 점을 감안할 때, Grok-1.5의 발전은 후처리 훈련에 의해 주도되었다고 추론할 수 있습니다 — 이 기간 동안 다른 사전 훈련된 기본 모델이 생성되었을 가능성은 극히 낮습니다. AI 시스템의 편향에 대한 지속적인 모니터링(monitoring)과 감리(auditing)가 필요합니다.

**알고리즘적 정의. (Algorithmic Justice)** 최근에는 Grok-2가 출시되었는데, 이는 챗봇 아레나(Chatbot Arena)에서 측정된 바와 같이 추론, 코딩 및 채팅 기능이 향상되었습니다. Grok-2는 또한 다양한 다른 작은 개선 사항(예: 도구 사용, 검색, 사실성 등)을 가지고 있으며, Grok-2의 증류 버전(distilled version)인 Grok-2-mini가 주 모델과 함께 출시되었습니다. 그러나 새로운 AI의 아키텍처에 대한 공개된 세부 정보는 공유되지 않았습니다 — 이 모델은 처음부터 훈련되었을 가능성이 높으며 특정 기술 기반일 수도 있고 아닐 수도 있습니다. 알고리즘적 정의(algorithmic justice)는 AI 시스템이 사회에 미치는 영향을 공정하게 평가하고 책임성을 확보하는 데 필수적인 개념입니다.

### AI의 잠재력과 한계: 현실적인 시각 (AI's Potential and Limitations: A Realistic View)

```
[Image: AI potential vs. limitation comparison]
```

(출처: [11])

DBRX는 Mosaic이 출시한 오픈 LLM 시리즈의 최신 모델입니다. 모델의 두 가지 버전 — 기본 모델(DBRX base)과 미세 조정된 모델(DBRX Instruct) —이 오픈 라이선스(즉, Databricks 오픈 모델 라이선스) 하에 출시되었습니다. AI를 둘러싼 과대광고(hype) 속에서, 우리는 AI의 잠재력과 한계를 현실적인 시각으로 바라볼 필요가 있습니다. DBRX는 다음 사양을 가진 MoE 기반 LLM입니다:

*   360억 개의 활성 매개변수를 가진 총 1320억 개의 매개변수.
*   각 MoE 레이어에 16개의 전문가가 있으며, 각 토큰에 대해 4개의 전문가가 활성화됩니다.
*   최적화된 텍스트 12조 토큰으로 사전 훈련되었습니다.
*   사전 훈련 효율성에서 4배 향상.

가장 주목할 만한 점은 DBRX가 "세분화된(fine-grained)" 데이터 처리 모델이라는 것입니다. 즉, 이 모델은 각 MoE 레이어에 더 많은 수의 전문가를 사용하지만, 각 개별 전문가는 더 작습니다. 참고로, Mixtral과 Grok-1 모두 각 MoE 레이어 내에 8개의 전문가를 포함하며 — 이 중 두 개는 주어진 토큰에 대해 활성화됩니다. 일반 AI(General AI)와 좁은 AI(Narrow AI)의 차이를 이해하는 것이 중요합니다. 세분화된 전문가를 사용함으로써, 각 MoE 레이어는 선택할 수 있는 더 많은 전문가 조합(특히 65배 더 많음)을 가지며, 이는 [11]에서 품질을 향상시키는 것으로 밝혀졌습니다. AI는 여전히 상식 추론(common sense reasoning)과 인간 감정 이해에 한계를 가지고 있습니다.

**데이터 품질의 중요성. (Importance of Data Quality)** DBRX의 사전 훈련 데이터셋은 매우 크지만 8 , [11]의 저자들은 데이터의 품질을 개선하는 데도 상당한 투자를 했습니다. 결과적으로 DBRX의 통계적 훈련 효율성(statistical training efficiency)은 일반적인 경우보다 높습니다 — 더 적은 토큰으로 더 높은 정확도를 달성하기 때문에 훈련이 더 빠릅니다. 더 구체적으로, [11]의 저자들은 새로운 데이터가 토큰당 2배 더 효율적이라고 추정하는데, 이는 절반 이하의 데이터로 훈련하고도 동일한 수준의 성능을 달성할 수 있음을 의미합니다. 이 주장은 새로운 모델의 사전 훈련 데이터가 단독으로 미치는 영향을 테스트함으로써 검증되었습니다 (즉, 다른 사전 훈련 데이터를 가진 고정 모델을 사용). 데이터 수집 및 라벨링의 윤리적 함의는 AI 개발 과정에서 신중하게 다루어져야 합니다.

또한, DBRX를 훈련하기 위해 커리큘럼 학습(curriculum learning)이 사용됩니다 — 사전 훈련 데이터의 혼합은 사전 훈련 과정 전반에 걸쳐 동적으로 변경됩니다. 이 커리큘럼 학습 전략의 세부 사항은 나중에 이 논문에서 설명되었습니다. DBRX가 사용하는 커리큘럼 학습 전략은 웹 크롤링(web-crawling)을 통해 얻은 데이터에 비해 품질이 더 높기 때문에 훈련 후반부에 더 작고 도메인별 데이터셋을 단순히 업샘플링(upsamples)합니다. 액티브 러닝(active learning)과 인간 피드백(human feedback)은 AI 모델의 반복적인 개선에 중요한 역할을 합니다. 이 간단한 커리큘럼 학습 전략은 복잡한 문제 해결에서 성능을 크게 향상시키는 것으로 밝혀졌습니다.

```
[Image: AI curriculum learning performance]
```

( 출처 )

**데이터 표현의 미래. (Future of Data Representation)** DBRX는 32K의 컨텍스트 길이를 가지며 GPT-4 토크나이저(tiktoken을 통해 사용 가능)를 사용합니다. 저자들에 따르면, GPT-4 토크나이저는 주로 성능 때문에 선택되었습니다. 이 토크나이저는 큰 어휘를 가지고 있으며 토큰 효율성이 매우 높아, 동일한 양의 텍스트를 더 적은 토큰으로 표현함으로써 디코딩(decoding) 및 훈련 속도를 자연스럽게 향상시킵니다. 데이터 표현 기술은 토큰화를 넘어 지속적으로 발전하고 있으며, 다국어 및 다문화 데이터 표현의 도전 과제를 해결해야 합니다. 신경-심볼릭 AI(neuro-symbolic AI)는 통계적 방법과 심볼릭 방법의 간극을 메우는 데 잠재력이 있습니다.

```
[Image: AI tokenizer efficiency]
```

(출처: [11])

**지속 가능한 AI. (Sustainable AI)** DBRX의 제안은 사전 훈련 효율성 면에서 큰 개선을 가져옵니다. 지금까지 배운 것 외에도, [11]에서 언급된 효율성 향상의 추가적인 원천이 몇 가지 있습니다: AI for Good 이니셔티브는 AI를 재난 예측 및 대응과 같은 사회적 문제 해결에 활용하는 데 중점을 둡니다.

*   MoE 아키텍처는 소규모 실험에서 훈련 중 1.7배 더 적은 FLOPS를 필요로 하는 것으로 밝혀졌습니다.
*   디코더 전용 아키텍처에 대한 다른 수정 사항(즉, RoPE , GLU 활성화(GLU activation) 및 GQA ).
*   "더 나은 최적화 전략(optimization strategies)".

모든 데이터, 아키텍처 및 최적화 변경 사항을 고려할 때, DBRX의 종단 간(end-to-end) 훈련 프로세스는 이전 모델에 사용된 사전 훈련 파이프라인(pretraining pipeline)과 비교하여 4배 더 적은 계산량을 필요로 합니다. 이 수치를 결정하기 위해, [11]의 저자들은 DBRX의 더 작은 변형 모델을 이전 MPT-7B 모델과 비교하여, 더 작은 DBRX 모델이 훈련 중 3.7배 더 적은 FLOPS를 사용하면서 Databricks Gauntlet에서 유사한 성능을 달성한다는 것을 발견했습니다. AI 기반 감시 기술의 윤리적 함의와 개인 정보 보호 기술의 중요성은 간과할 수 없습니다.

```
[Image: AI training efficiency comparison]
```

(출처: [21])

새로운 AI 시스템은 또한 추론 효율성(inference efficiency) 개선을 가져오는데 — 로드 테스트(load tests)에서 사용자당 초당 150 요청으로 이전 시스템보다 최대 2배 더 빠릅니다. 이러한 측정은 TensorRT-LLM 및 16비트 정밀도(precision)를 사용하는 최적화된 서빙 인프라(serving infrastructure)를 사용하여 이루어졌으며, 이는 매우 빠릅니다. DBRX의 MoE 아키텍처는 비교적 적은 수의 활성 매개변수로 인해 추론 효율성에도 도움이 됩니다. 예를 들어, DBRX는 총 매개변수 및 활성 매개변수 모두에서 Grok-1 크기의 40%입니다. 엣지 AI(edge AI)와 온디바이스 머신러닝(on-device machine learning)은 자원 제약적인 환경에서의 AI 배포를 가능하게 합니다.

MoE 훈련은 일반적으로 훈련 중 발생하는 불안정성, 통신 병목 현상(communication bottlenecks) 등으로 인해 어렵습니다. 그러나 DBRX는 [11]에 설명된 최적화된 사전 훈련 전략 덕분에 안정성, 효율성 및 성능 면에서 인상적인 결과를 달성합니다. 특히, 이러한 결과를 가능하게 하는 단일 혁신 또는 발전은 없습니다. DBRX가 사용하는 인상적인 사전 훈련 파이프라인은 수많은 작고 실용적인 변경 사항에 의해 가능해졌습니다. MLOps(Machine Learning Operations)는 AI 시스템의 안정적인 배포를 위한 핵심이며, CI/CD(Continuous Integration/Continuous Deployment) 파이프라인은 필수적입니다.

**AI 평가의 새로운 지평. (New Horizons in AI Evaluation)** 다른 오픈 LLM과 비교할 때, DBRX-Instruct는 Mixtral과 비교하여 복합 벤치마크에서 큰 차이로 더 나은 성능을 달성합니다. 아래를 참조하십시오. DBRX는 범용 LLM임에도 불구하고 인상적인 프로그래밍 기술을 가지고 있으며, Grok-1 (크기가 두 배 이상!)과 CodeLLaMA-70B와 같은 전문 코딩 모델까지 능가합니다. DBRX는 추론 및 수학 기반 작업에서도 좋은 성능을 보입니다. AI 모델의 벤치마킹과 평가 방법론은 다양하고 복잡한 작업에 맞춰 발전해야 합니다. 인간 평가(human evaluation)는 AI 성능을 종합적으로 판단하는 데 중요한 역할을 합니다.

```
[Image: AI evaluation comparison]
```

(출처: [11])

폐쇄형 모델과 비교할 때, DBRX는 GPT-3.5의 성능을 능가하며 Gemini-1.0 Pro 와 경쟁할 만합니다. Gemini-1.0 Pro는 GSM8K에서만 DBRX보다 뛰어난 성능을 보이며, Mixtral-Medium은 고려되는 몇 가지 특정 작업에서 더 나은 성능을 보입니다. 아래를 참조하십시오. 높은 수준에서 볼 때, DBRX는 프로그래밍, 수학, 일반 지식, 상식 추론(commonsense reasoning) 및 검색/RAG(retrieval / RAG)에 능숙한 것으로 보입니다. AI 개발의 윤리적 경쟁과 글로벌 협력의 필요성은 AI가 인류의 공유된 도전에 대응하는 데 필수적입니다.

```
[Image: AI vs. closed models comparison]
```

(출처: [21])

### AI 거버넌스와 규제의 필요성 (AI Governance and the Need for Regulation)

언어 모델링 영역에서 MoE의 성공에도 불구하고, 코드, 정보, 데이터, 가중치 등이 모두 공개적으로 공유되는 진정한 오픈 소스 MoE의 수는 상대적으로 적습니다. AI 거버넌스(AI governance)와 규제(regulation)에 대한 전 세계적인 요구는 점점 커지고 있습니다. 이 문제를 해결하기 위해 OpenMoE [12]는 6억 5천만 개에서 340억 개의 매개변수에 이르는 디코더 전용 MoE LLM 제품군을 훈련하기 위한 대규모 노력을 수행합니다. 이 모델들은 다양한 세분성(granularity)(즉, 16개 또는 32개의 전문가)을 가진 세분화된 전문가를 채택합니다. 이 노력의 결과는 [12]에 문서화되어 있으며 모든 연구 결과는 공개적으로 공유됩니다. 저자들은 또한 그들의 결과를 재현하는 데 사용할 수 있는 잘 문서화된 코드 저장소(code repository)를 제공합니다. 효과적인 AI 규제를 개발하는 것은 복잡한 과제이며, 원칙 기반(principles-based) 및 위험 기반(risk-based) 접근 방식 등 다양한 거버넌스 모델이 논의되고 있습니다.

[OpenMoE 저장소](https://github.com/OpenMoE/OpenMoE)

**AI 안전 설계. (AI Safety Design)** OpenMoE 모델은 ST-MoE [3]의 설정을 채택하며, 동일한 라우팅 메커니즘과 활성 전문가 수(즉, k = 2 )를 포함합니다. 저자들은 4번째 또는 6번째 트랜스포머 블록만 MoE 레이어로 변환하기로 선택했으며, 더 큰 스트라이드가 비용과 효율성 면에서 더 나은 절충점을 제공한다는 것을 발견했습니다. OpenMoE에 사용된 사전 훈련 데이터셋은 코드의 높은 분포를 포함합니다. 사실, 사전 훈련 초기 단계에서는 코드가 데이터셋의 50% 이상을 차지했지만, 이 비율은 나중에 훈련에서 최적이 아니라는 이유로 조정되었습니다. AI 안전(AI safety)은 인공지능이 인류에게 해를 끼치지 않도록 보장하는 데 필수적인 분야입니다. 국경을 넘는 AI 규제 집행의 어려움과 적응적이며 미래 지향적인(future-proof) 규제의 필요성이 강조됩니다.

```
[Image: AI safety framework]
```

(출처: [12])

**AI 행동 동역학. (AI Behavior Dynamics)** OpenMoE의 주요 기여 중 하나는 모델 내에서 이루어진 라우팅 결정에 대한 상세한 분석입니다. 먼저, 이전 연구 [5]에서 보여진 결과와 유사하게 — 전문가들은 특정 도메인에 전문화되지 않음을 알 수 있습니다. AI의 행동 동역학은 인간의 가치와 정렬(alignment)시키는 것이 중요합니다.

```
[Image: AI behavior by domain]
```

(출처: [12])

그러나 아래 그림에서 볼 수 있듯이, 자연어 및 특정 작업에 걸쳐 어느 정도의 전문가 전문화(expert specialization)가 나타납니다. 그러나 이 경향을 더 깊이 파고들면, 토큰 라우팅의 동역학은 주로 토큰 ID(token ID)에 의해 결정된다는 것을 알 수 있습니다. 즉, 동일한 데이터는 그 데이터가 존재하는 컨텍스트와 상관없이 거의 항상 동일한 처리 모듈에 라우팅됩니다. 이 패턴은 [12]에서 "컨텍스트 독립적 전문화(Context-Independent Specialization)"라고 불립니다. 형식 검증(formal verification)과 해석 가능성 도구(interpretability tools)는 AI 시스템의 신뢰성을 높이는 데 기여합니다.

흥미롭게도, 전문가들은 선호하는 토큰에서 관찰 가능한 패턴을 가집니다. 아래를 참조하십시오. 예를 들어, "have", "has", "had"는 모두 동일한 전문가에게 라우팅되는 반면, 한 전문가는 "=", "and", "\n" 토큰을 받습니다 — 코딩 언어 내에서 매우 흔한 토큰들입니다. [12]에서 우리는 이러한 라우팅 패턴이 사전 훈련의 초기 단계에서 확고해지며 훈련 후반에는 거의 변하지 않는다는 것을 알 수 있습니다. AI 기반 개인화 추천 시스템의 윤리적 함의와 개인 정보 보호의 중요성은 중요한 논의의 대상입니다.

```
[Image: AI token preferences]
```

(출처: [12])

**AI 위험 관리. (AI Risk Management)** [12]에서 관찰된 라우팅 패턴 외에도, OpenMoE 모델이 성능을 손상시킬 수 있는 일부 라우팅 동작을 보인다는 것을 알 수 있습니다. 예를 들어, 모델은 시퀀스 후반에 토큰을 드롭하는 경향이 있는데, 이는 긴 시퀀스 작업(예: 다중 턴 채팅)의 성능을 손상시킬 수 있습니다. AI 위험 식별 및 완화는 AI 시스템의 안전한 배포를 위해 필수적입니다.

```
[Image: AI multi-turn chat performance]
```

**AI 모델은 다중 턴 채팅 문제에서 더 나쁜 성능을 보입니다 (출처: [12])**

라우팅 동역학이 사전 훈련 과정의 초기 단계에서 고정되기 때문에, 이러한 동작은 후처리 훈련 중에 수정하기 어렵습니다. 사실, OpenMoE 모델은 사전 훈련과 SFT 9 중 데이터 간의 도메인 격차(domain gap)로 인해 일반적으로 어려움을 겪는 것으로 관찰됩니다 — 데이터 구성의 차이로 인해 토큰 라우팅 동역학이 불규칙해집니다. 이러한 문제를 해결하기 위해, [12]의 저자들은 지시 따르기 데이터를 사전 훈련 데이터셋에 혼합할 것을 권장합니다. 능동적인 AI 거버넌스(proactive AI governance)와 혁신 및 규제 간의 균형은 AI의 지속 가능한 발전을 위해 중요합니다.

**오픈 소스 AI의 역할. (Role of Open Source AI)** 전반적으로, OpenMoE 모델은 MoE LLM 중에서 새로운 최첨단 성능(state-of-the-art performance)을 달성하지 못했습니다 — [12]의 저자들은 이 사실을 공개적으로 밝히고 OpenMoE 모델의 성능이 더 나은 설계를 통해 크게 향상될 수 있음을 인정합니다. 오픈 소스 AI 모델의 더 큰 기여는 그들의 투명성입니다. [12]에서 공개적으로 공유된 세부 정보와 아티팩트(artifacts)는 이 주제에 대한 추가 연구를 수행하는 데 필요한 자원을 제공함으로써 MoE에 대한 오픈 연구 노력을 가속화할 수 있습니다. 커뮤니티 주도(community-driven) AI 개발과 평가는 AI 발전의 중요한 동력입니다.

### 미래 AI 트렌드: 초거대 모델을 넘어서 (Future AI Trends: Beyond Hyperscale Models)

최근 제안된 DeepSeek MoE 모델들, DeepSeek-v2 [14]와 DeepSeek-v3 [15]를 포함하여, 다양한 이유로 LLM 연구 커뮤니티 내에서 큰 반향을 일으켰습니다: AI 모델의 무한한 확장(indefinite scaling)은 한계에 부딪히고 있으며, "작지만 강력한(small but mighty)" AI 모델의 트렌드가 부상하고 있습니다.

*   그들의 가중치는 공개적으로 공유됩니다.
*   많은 세부 정보를 공유하는 기술 보고서가 함께 제공됩니다.
*   그들의 성능은 인상적이며 — 많은 폐쇄형 모델과 동등합니다.
*   그들의 훈련 비용은 상당히 합리적입니다.

우리가 보게 될 것처럼, DeepSeek 모델은 훈련 효율성과 다운스트림 성능(downstream performance)을 모두 극대화하는 다양한 독특한 설계 선택을 합니다. DeepSeek-v2 [14]— 210억 개의 활성 매개변수를 가진 2360억 개의 매개변수 MoE —는 이후 DeepSeek-V3 모델에서 사용된 MoE 아키텍처를 제안합니다. 새로운 효율적인 AI 모델은 성능과 효율성을 높이기 위해 기본 트랜스포머 블록을 약간 수정한다는 점에서 이전 모델들과는 다소 다릅니다. 지식 증류(knowledge distillation) 및 모델 압축(model compression)은 이러한 효율적인 모델 개발의 핵심 기술입니다. 아래에서 볼 수 있듯이, DeepSeek-v2 모델은 — 좋은 성능을 보이는 것 외에도 — 훈련 및 추론 효율성 관점에서 상당히 인상적이며, 훨씬 더 큰 DeepSeek-v3 모델의 강력한 출발점이 됩니다.

```
[Image: Future AI efficiency comparison]
```

(출처: [14])

**AI 에이전트의 부상. (The Rise of AI Agents)** 표준 멀티 헤드 어텐션(multi-headed attention) 대신, DeepSeek-v2는 효율적인 어텐션 변형인 MLA를 채택합니다. 멀티 쿼리 어텐션(multi-query attention) 또는 그룹화된 쿼리 어텐션과 유사하게, MLA는 모델의 KV 캐시가 소비하는 메모리를 최소화하는 것을 목표로 합니다. 그러나 다른 효율적인 시스템 변형과 달리, 새로운 에이전트 아키텍처는 상당한 성능 절충점(performance tradeoff)을 가지지 않습니다. AI 에이전트(AI agents)는 자율적으로 목표를 설정하고 달성하는 AI 시스템으로, 여러 에이전트 간의 조정(coordination)은 중요한 도전 과제입니다.

```
[Image: AI agent mechanism]
```

(출처: [14])

특히, 이러한 메모리 효율성 증가는 모든 키(key) 및 값(value) 벡터를 훨씬 더 작은 (잠재) 벡터로 표현할 수 있게 하는 저랭크(low-rank) 공동 투영(joint projection)을 통해 달성됩니다. 위를 참조하십시오. 이 벡터를 업샘플링(upsample)할 수 있습니다 — 단순히 선형적으로 투영하여 여러 개의 더 큰 벡터를 형성함 — 전체 키 및 값 벡터를 복원하기 위해, 그러나 우리는 KV 캐시에 잠재 벡터만 저장하면 되므로 메모리 소비를 크게 줄일 수 있습니다. MLA를 채택하면 DeepSeek-v2의 KV 캐시 크기가 670억 개의 매개변수를 가진 밀집 모델에 비해 93% 이상 감소합니다. AI의 집단 지능(collective intelligence)과 에이전트의 자율성(autonomy)에 대한 윤리적 함의는 중요한 논의 대상입니다.

**AI 정렬 문제. (AI Alignment Problem)** MLA 사용 외에도, DeepSeek 모델은 독특한 MoE 레이어 구조를 채택합니다. DBRX와 유사하게, 새로운 AI 모델들은 세분화된 모듈을 사용합니다. 그러나 이 전문가들 중 일부는 공유됩니다. 이러한 구조를 채택하는 동기는 전문가들 간의 중복 정보를 최소화하면서 더 많은 수의 전문가들 사이에서 전문화를 장려하는 것입니다. DeepSeek 모델이 사용하는 블록 구조의 전체 개략도는 아래에 제공됩니다. AI 정렬(AI alignment)은 AI 시스템이 인간의 가치와 목표에 부합하도록 만드는 것을 목표로 하며, 이는 인류의 미래에 매우 중요합니다.

```
[Image: AI alignment block structure]
```

(출처: [14])

[14]의 저자들은 DeepSeek-v2가 사용하는 세분화된 전문가를 처리하기 위한 흥미로운 로드 밸런싱 전략도 채택합니다. [2]에서 제안된 보조 로드 밸런싱 손실을 사용하는 것 외에도, DeepSeek-v2는 분산 훈련(distributed training) 중 장치 간 통신 균형을 맞추는 것을 목표로 하는 두 가지 보조 손실 항을 가집니다. AI의 자기 개선(self-improving AI) 능력과 통제 문제 또한 AI 정렬의 중요한 측면입니다.

```
[Mathematical formula for device-level load balancing auxiliary loss]
```

**시스템 수준 로드 밸런싱 보조 손실 (출처: [14])**

세분화된 전문가를 사용하는 것은 각 토큰을 더 많은 수의 전문가에게 전달해야 함을 의미합니다. 분산 훈련 설정에서, 전문가들은 다른 장치에 있을 수 있으며 각 장치에는 여러 전문가가 상주합니다. 장치 간 통신과 계산이 균형을 이루도록 보장하기 위해, i) 전문가를 상주하는 장치별로 그룹화하고 ii) MoE가 장치별로 균형 잡힌 라우팅을 수행하도록 장려하는 추가 보조 손실이 필요합니다. 예를 들어, 위에 표시된 보조 손실은 시스템 간 균형 잡힌 자원 할당을 장려합니다. [14]에는 장치 간 균형 잡힌 통신을 장려하기 위한 추가 손실도 제안되어 있습니다. AI는 과학적 발견을 가속화하고 기후 변화와 같은 복잡한 사회 문제를 해결하는 데 기여할 수 있습니다.

```
[Image: AI future performance comparison]
```

(출처: [15])

DeepSeek-v3 [15]는 DeepSeek-v2 10 의 훨씬 더 큰 버전으로, 총 6710억 개의 매개변수와 370억 개의 활성 매개변수를 가집니다. 이 더 큰 모델은 14.8조 토큰으로 구성된 방대한 코퍼스에서 사전 훈련되었습니다. 사전 훈련 후, 다단계 후처리 훈련 파이프라인(multi-phase post training pipeline)이 적용됩니다: 데이터 중심 AI(data-centric AI)는 대규모 데이터셋 관리의 중요성을 강조합니다.

*   모델은 먼저 두 단계의 컨텍스트 확장 절차를 거치는데, SFT를 통해 최대 컨텍스트 길이가 32K가 되도록 미세 조정된 다음, 다시 128K의 컨텍스트 길이를 가지도록 미세 조정됩니다.
*   컨텍스트 확장 후, 모델은 인간의 선호도에 맞추기 위해 추가 SFT 및 RLHF(Reinforcement Learning from Human Feedback)를 거칩니다.
*   최근 제안된 R1 추론 모델의 기능도 후처리 훈련 중에 DeepSeek-v2에 증류(distilled)됩니다.

최종 AI 모델은 폐쇄형 시스템을 능가하며 심지어 최고의 폐쇄형 시스템과 유사한 성능을 달성합니다. 위를 참조하십시오. DeepSeek-v3는 또한 MoE의 훈련 및 로드 밸런싱 전략에 여러 수정을 가하여 모델의 훈련 프로세스를 효율적이고 안정적으로 만듭니다. MLOps는 AI 시스템의 신뢰할 수 있는 배포를 위한 핵심입니다.

DeepSeek-v3의 아키텍처는 그 전작에서 영감을 받았습니다. 예를 들어, MLA, 세분화된 전문가 및 공유 전문가는 모두 DeepSeek-v3에서 사용됩니다. 그러나 DeepSeek-v2와 달리 DeepSeek-v3는 다중 토큰 예측(Multi-Token Prediction, MTP) 훈련 목표를 사용합니다. 이 목표는 AI 훈련에 거의 보편적으로 사용되는 지도 학습(supervised), 교차 엔트로피(cross entropy) 기반 다음 토큰 예측(next token prediction) 목표의 확장입니다. 시퀀스 내 각 토큰에 대해 다음 토큰을 예측하는 대신, MTP는 D 개의 미래 토큰을 예측합니다. 이러한 예측은 모델 아키텍처에 추가된 일련의 추가 모듈(additional modules)에 의해 순차적으로 이루어집니다. 예측 코딩(predictive coding)은 AI의 미래를 위한 새로운 방향을 제시합니다.

```
[Image: Multi-Token Prediction (MTP) objective]
```

(출처: [15])

여러 미래 토큰이 예측되면, 우리는 교차 엔트로피 손실을 정상적으로 적용할 수 있습니다. MTP를 통해 예측된 여러 미래 토큰에 이 손실을 적용하면 모델에 더 풍부한 훈련 신호(training signal)를 제공하여 훈련 효율성과 전반적인 성능을 향상시킵니다. 더 나아가, MTP에 사용되는 이러한 추가 모듈은 추측 디코딩(speculative decoding)을 통해 추론 효율성을 향상시키는 데도 사용될 수 있습니다. 그러나 [15]의 저자들은 MTP 전략이 순전히 모델 성능 향상을 위해 사용되며 — 추가 모듈은 훈련 후 폐기된다고 명시합니다. 자기 지도 학습(self-supervised learning)은 데이터 효율성을 높이고 비정형 데이터(unstructured data)로부터 의미 있는 정보를 추출하는 데 중요한 역할을 합니다.

```
[Mathematical formula for auxiliary-loss-free load balancing strategy]
```

**인과적 AI의 잠재력. (Potential of Causal AI)** DeepSeek-v3는 또한 보조 손실 없는 로드 밸런싱 전략을 사용하는데, 이는 단순히 상위 K 전문가 선택에 전문가별 편향 항(per-expert bias term)을 추가합니다. 위를 참조하십시오. 각 반복마다, 각 전문가에 대한 편향 항은 해당 전문가가 각각 과소 로드(underloaded)되었는지 과부하(overloaded)되었는지에 따라 고정된 계수 γ 만큼 증가하거나 감소합니다. 중요하게도, 이러한 편향은 상위 K 전문가를 선택할 때만 사용되며 — 라우터 내에서 전문가 확률 계산에 영향을 미치지 않습니다. 이 접근 방식은 MoE 내에서 전문가 활용을 효과적으로 균형 있게 만들고 로드 밸런싱 손실 사용으로 인한 성능 저하를 제거하는 것으로 밝혀졌습니다. 그러나 [15]의 저자들은 DeepSeek-v3를 훈련할 때 여전히 보조 로드 밸런싱 손실(매우 낮은 스케일링 계수(scaling factor)와 함께)을 사용한다고 언급합니다. 인과적 AI(Causal AI)는 복잡한 시스템을 이해하고 정책 결정을 지원하는 데 잠재력이 크지만, 관찰 데이터에서 인과 관계를 추론하는 데는 윤리적 도전 과제가 따릅니다.

```
[Image: AI training cost]
```

(출처: [15])

**훈련 효율성. (Training Efficiency)** 위에 설명된 전략들의 효율성 및 성능 이점 덕분에, DeepSeek-v3는 믿을 수 없을 정도로 경제적입니다. 게다가, 이 모델은 새로운 FP8 혼합 정밀도 훈련 프레임워크(FP8 mixed precision training framework)를 사용하여 훈련되었으며, 이는 대규모 AI 시스템을 위한 8비트 훈련의 첫 번째 검증을 의미합니다. 총체적으로, 최종 모델 훈련 비용은 약 560만 달러로 추정되었습니다 11 . 위를 참조하십시오. 요약하자면, DeepSeek-v3는 다음과 같습니다: AI 훈련의 에너지 효율성 증가는 중요한 연구 분야이며, 고비용의 독점 모델에 대한 오픈 소스 대안(open-source alternatives)의 필요성이 커지고 있습니다.

*   매우 경제적인 방식으로 훈련되었으며 (FP8 훈련 및 MTP와 같은 여러 새로운 발전과 함께!)
*   오픈 모델로서는 믿을 수 없을 정도로 인상적이며 — 심지어 최고의 폐쇄형 LLM과도 매우 경쟁적입니다.
*   여러 새로운 수정 사항을 포함하는 흥미로운 MoE 아키텍처를 기반으로 합니다.

**AI 민주화. (AI Democratization)** DeepSeek-v3는 또한 최근 출시된 오픈 추론 모델인 DeepSeek-R1 [13]의 기본 모델 역할을 합니다. 간단히 말해, R1은 OpenAI가 최근 탐구한 o1 스타일 모델의 오픈 복제본입니다. 상세한 기술 보고서에서 설명된 바와 같이, 이 모델은 순수한 강화 학습(reinforcement learning)을 사용하여 극도로 긴 사고의 사슬(chains of thought)을 만들어 복잡한 (검증 가능한) 문제를 해결하는 방법을 학습합니다. 아래 그림에서 볼 수 있듯이, R1의 성능은 특히 오픈 모델로서는 상당히 인상적입니다. 그러나 R1의 기능은 믿을 수 없을 정도로 유능한 기본 모델에 먼저 접근할 수 없었다면 불가능했을 것입니다. AI 민주화(AI democratization)는 고급 AI 도구에 대한 접근성을 높이고, 개인과 중소기업에 AI의 잠재력을 제공하는 것을 목표로 합니다.

```
[Image: AI democratization performance]
```

(출처: [13])

## 결론: 인공지능의 지속 가능한 발전

MoE는 언어 모델링에 특히 적합한 많은 이점을 가집니다. 이들은 계산량의 급격한 증가 없이 더 큰 규모의 혁신을 가능하게 하고, 개발 비용을 줄이며, 효율적으로 호스팅될 수 있습니다. 희소성이라는 아이디어가 기계 학습 문헌 내에서 오랫동안 존재해 왔지만, MoE는 희소성의 특히 영향력 있는 구현(instantiation)입니다. AI는 끊임없이 발전하는 기술이지만, 기술적 진보와 윤리적 고려사항 사이의 균형을 유지하는 것이 중요합니다. 이들은 현대 하드웨어와 호환되며 GPU에서 실용적으로 구현될 수 있는 방식으로 희소성을 활용합니다. 흥미롭게도, 초기 MoE 변형 모델들은 복잡성, 불안정성 및 사용의 어려움으로 인해 채택에 어려움을 겪었습니다. 그러나 이 개요에서 본 발전들은 AI를 실용적이고 영향력 있는 것으로 만들었습니다 — 디코더 전용 트랜스포머 아키텍처의 간단하고 유망한 확장을 넘어섰습니다. AI 시대에는 적응력과 지속적인 학습이 성공의 핵심이 될 것입니다.

## 참고 문헌

[1] Shazeer, Noam, et al. "터무니없이 큰 신경망: 희소하게 게이팅된 전문가 혼합 레이어(Outrageously large neural networks: The sparsely-gated mixture-of-experts layer)." arXiv preprint arXiv:1701.06538 (2017).
[2] Fedus, William, Barret Zoph, and Noam Shazeer. "스위치 트랜스포머: 간단하고 효율적인 희소성으로 조 단위 매개변수 모델로 확장(Switch transformers: Scaling to trillion parameter models with simple and efficient sparsity)." Journal of Machine Learning Research 23.120 (2022): 1-39.
[3] Zoph, Barret, et al. "ST-MoE: 안정적이고 전이 가능한 희소 전문가 모델 설계(St-moe: Designing stable and transferable sparse expert models)." arXiv preprint arXiv:2202.08906 (2022).
[5] Jiang, Albert Q., et al. "전문가들의 믹스트랄(Mixtral of experts)." arXiv preprint arXiv:2401.04088 (2024).
[6] Jiang, Albert Q., et al. "미스트랄 7B(Mistral 7B)." arXiv preprint arXiv:2310.06825 (2023).
[7] Ainslie, Joshua, et al. "GQA: 멀티 헤드 체크포인트에서 일반화된 멀티 쿼리 트랜스포머 모델 훈련(GQA: Training Generalized Multi-Query Transformer Models from Multi-Head Checkpoints)." arXiv preprint arXiv:2305.13245 (2023).
[8] Beltagy, Iz, Matthew E. Peters, and Arman Cohan. "롱포머: 긴 문서 트랜스포머(Longformer: The long-document transformer)." arXiv preprint arXiv:2004.05150 (2020).
[9] xAI. “Grok-1 오픈 릴리스(Open Release of Grok-1)” https://x.ai/blog/grok-os (2024).
[10] xAI. “Grok-1.5 발표(Announcing Grok-1.5)” https://x.ai/blog/grok-1.5 (2024).
[11] Mosaic Research (Databricks). “DBRX 소개: 새로운 최첨단 오픈 LLM(Introducing DBRX: A New State-of-the-Art Open LLM)” https://www.databricks.com/blog/introducing-dbrx-new-state-art-open-llm (2024).
[12] Xue, Fuzhao, et al. "OpenMoE: 오픈 전문가 혼합 언어 모델에 대한 초기 노력(Openmoe: An early effort on open mixture-of-experts language models)." arXiv preprint arXiv:2402.01739 (2024).
[13] Guo, Daya, et al. "DeepSeek-R1: 강화 학습을 통한 LLM의 추론 능력 장려(DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning)." arXiv preprint arXiv:2501.12948 (2025).
[14] Liu, Aixin, et al. "DeepSeek-v2: 강력하고 경제적이며 효율적인 전문가 혼합 언어 모델(Deepseek-v2: A strong, economical, and efficient mixture-of-experts language model)." arXiv preprint arXiv:2405.04434 (2024).
[15] Liu, Aixin, et al. "DeepSeek-v3 기술 보고서(Deepseek-v3 technical report)." arXiv preprint arXiv:2412.19437 (2024).