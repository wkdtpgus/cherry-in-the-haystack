## 대규모 언어 모델(LLM)을 위한 전문가 혼합(MoE) 아키텍처 심층 분석: 최신 동향과 발전

(출처: [2, 5, 14]) 급변하는 연구 영역에서, 디코더 전용 트랜스포머 구조는 대규모 언어 모델(LLM) 분야에서 변함없이 중요한 구성 요소로 자리 잡고 있습니다. 최초의 GPT 모델이 제안된 이후로 이 구조는 소소한 효율성 개선을 제외하고는 근본적인 변화 없이 지속적으로 활용되어 왔습니다. 하지만 이 아키텍처에 적용될 수 있는 가장 핵심적인 혁신 중 하나는 바로 전문가 혼합(Mixture-of-Experts, MoE) 계층입니다.

"MoE 아키텍처를 사용하면 밀집 모델(dense model)이 일반적으로 달성하는 것보다 모델 품질과 추론 효율성(inference efficiency) 사이에서 더 나은 절충점(tradeoff)을 얻을 수 있습니다." - [11]에서

MoE 기반 LLM은 모델 아키텍처에 희소성(sparsity)을 도입하여, 총 매개변수(total parameters) 수를 기준으로 모델 크기를 계산 비용(compute costs)의 상응하는 증가 없이 크게 늘릴 수 있도록 합니다. Grok [9] 및 DeepSeek-v3 [15]와 같은 최신 모델에서 성공적으로 채택된 이 수정은 극도로 큰 모델의 탐색을 더 다루기 쉽고 계산 효율적으로 만듭니다. 이 개요에서는 MoE의 기본 사항을 배우고 이 아이디어가 최근 어떻게 적용되어 더 강력한 LLM을 만들었는지 탐구할 것입니다. 최근 몇 년간 MoE는 단순한 연구 개념을 넘어 실제 제품과 오픈 소스 이니셔티브에 광범위하게 채택되며 그 실용성을 입증했습니다. 특히, 방대한 매개변수 수를 가진 모델을 효율적으로 훈련하고 배포하는 데 있어 MoE는 핵심적인 기술로 부상했습니다. 이는 모델의 성능을 유지하거나 향상시키면서도, 운영 비용을 절감하고 더 넓은 범위의 하드웨어에서 LLM을 구동할 수 있게 하는 중요한 진전입니다. 이러한 흐름은 인공지능 기술의 대중화와 접근성 향상에 크게 기여하고 있습니다.

## LLM을 위한 MoE의 기본 사항

본 글에서 다루는 MoE 기반 LLM은 디코더 전용 트랜스포머 구조를 근간으로 합니다. 해당 아키텍처에 대한 상세한 설명은 생략하겠지만, 관련 지식이 부족하다면 다른 자료를 참고하시기 바랍니다. 디코더 전용 트랜스포머는 정규화(예: 레이어 정규화 또는 RMS 레이어 정규화), 마스크된 멀티 헤드 셀프 어텐션, 피드포워드 변환, 그리고 잔여 연결을 포함하는 반복적인 모듈들로 이루어져 있습니다. 아래 그림을 참조하십시오.

```
[Image: Decoder-only transformer architecture diagram]
```

**디코더 전용 트랜스포머 아키텍처**

이 섹션에서는 MoE의 기본 사항을 다룰 것입니다. 이 설명은 i) 표준 MoE 레이어를 제안하고 ii) 이 아이디어를 트랜스포머 아키텍처에 사용하도록 확장한 선구적인 논문들을 기반으로 합니다. 해당 논문들은 다음과 같습니다:

*   **희소하게 게이팅된 전문가 혼합 레이어(The Sparsely-Gated Mixture-of-Experts Layer) [1]**
*   **스위치 트랜스포머(Switch Transformers) [2]**
*   **안정적이고 전이 가능한 전문가 혼합(Stable and Transferable Mixture-of-Experts, ST-MoE) [3]**

이 논문들과 MoE 아키텍처의 기원에 대한 더 자세한 분석은 아래의 이 아이디어들에 대한 더 상세한 개요를 참조하십시오.

**전문가 혼합(MoE): 조건부 계산(Conditional Computation)의 탄생과 부상**
Cameron R. Wolfe, Ph.D. · 2024년 3월 18일
[전체 이야기 읽기]

**간략한 예비 지식.** MoE와 라우팅 알고리즘(routing algorithms)을 이해하려면 먼저 디코더 트랜스포머(decoder-transformer) (및 각 레이어)의 입력 구조를 이해해야 합니다. 물론 LLM은 텍스트를 입력으로 받지만, 이 텍스트는 LLM이 실제로 보기 전에 광범위한 처리를 거칩니다. 먼저 텍스트는 토큰화(tokenized)됩니다 (아래 참조) — 즉, 이산적인 토큰(discrete tokens) 목록으로 변환됩니다. 이 토큰들은 단어와 하위 단어(sub-words)입니다. LLM은 이해하고 훈련된 고정된 토큰 집합을 가지고 있으며, 이를 모델의 "어휘(vocabulary)"라고 합니다. 어휘 크기는 모델마다 다르지만, 총 64K에서 256K 토큰 크기가 비교적 일반적입니다. 밀집 모델(Dense models)은 입력 데이터의 모든 부분에 대해 동일한 가중치 집합을 적용하여 계산을 수행합니다. 이는 모든 입력 토큰이 모델의 모든 매개변수를 통과한다는 것을 의미합니다. 반면, MoE와 같은 희소 모델(sparse models)은 입력의 특정 부분에만 활성화되는 가중치(즉, 전문가)를 가집니다. 이러한 "조건부 계산(conditional computation)"은 특정 입력에 대해 전체 모델의 일부만 활성화되도록 하여, 계산 비용을 절감하면서도 모델의 전체 용량(capacity)을 크게 늘릴 수 있도록 합니다. 이는 마치 특정 문제에 대해 가장 적합한 전문가를 선별하여 조언을 구하는 것과 유사합니다. 결과적으로 MoE는 모델이 훨씬 더 많은 지식을 학습하고 저장할 수 있게 하면서도, 실제 추론 시에는 필요한 계산량을 효율적으로 관리할 수 있게 해줍니다.

```
[Image: Tokenizing and vectoring text for an LLM]
```

**LLM을 위한 텍스트 토큰화 및 벡터화**

텍스트가 토큰 형태로 전환되면, 각 입력 토큰은 벡터 형태로 변환될 수 있습니다. LLM은 고유한 어휘를 보유하고 있으며, 이 어휘 내의 모든 토큰에 대해 학습된 벡터 임베딩을 저장하는 토큰 임베딩 계층을 포함합니다. 이 계층을 통해 각 토큰에 해당하는 벡터를 추출하여 입력 행렬을 구성할 수 있습니다. 만약 각 토큰 임베딩이 d 차원이고 입력에 총 C개의 토큰이 존재한다면, 이 입력 행렬의 전체 크기는 C x d가 됩니다. 아래 그림을 참고하십시오.

```
[Image: Input matrix of token vectors]
```

**토큰 벡터의 입력 행렬**

트랜스포머의 각 레이어 — 그리고 모든 트랜스포머 블록(transformer block) 내의 각 서브 레이어(sub-layer) —는 이 입력의 크기를 유지합니다. 결과적으로 트랜스포머의 모든 피드포워드(feed-forward) 또는 어텐션 모듈(attention module)에 대한 입력(및 출력)은 이와 동일한 크기의 행렬입니다!

## "전문가(experts)"란 무엇인가요?

디코더 전용 트랜스포머 구조에서 MoE가 도입하는 핵심적인 변화는 트랜스포머 블록의 피드포워드 요소 내부에 있습니다. 일반적인 구조에서는 모든 토큰이 단일 피드포워드 신경망(Feed-Forward Neural Network, FFN)을 거치는데, 이 FFN은 대개 비선형 활성화 함수를 사이에 둔 두 개의 선형 계층으로 구성됩니다. 아래 그림을 확인해 주십시오.

```
[Image: Standard feed-forward network in a transformer block]
```

MoE는 이 블록 구조를 약간 수정합니다. 블록의 피드포워드 구성 요소 내에 단일 피드포워드 네트워크를 갖는 대신, 우리는 여러 개의 피드포워드 네트워크를 생성하며, 각 네트워크는 자체적인 독립적인 가중치(independent weights)를 가집니다. 우리는 이들 각 네트워크를 "전문가(expert)"라고 부릅니다. 예를 들어, MoE 기반 LLM은 각 피드포워드 서브 레이어에 8개의 독립적인 전문가를 가질 수 있습니다.

```
[Image: Experts within a transformer layer]
```

트랜스포머 레이어 내의 전문가들은 위에서 보여준 대로 정의될 수 있습니다. 한 레이어에 N 개의 전문가가 있으며, i 번째 전문가는 E_i 라는 표기법으로 참조할 수 있습니다.

## MoE 기반 트랜스포머 생성.

MoE를 활용한 디코더 전용 트랜스포머 구조를 구축하기 위해서는, 트랜스포머의 피드포워드 계층을 MoE, 즉 전문가 계층으로 전환하는 과정이 필요합니다. MoE 계층에 포함된 각 전문가는 해당 계층의 기존 피드포워드 네트워크와 동일한 내부 구조를 가지며, 이는 본질적으로 기존 네트워크의 여러 독립적인 사본을 생성하는 것과 같습니다. 아래 그림을 참고하십시오.

```
[Image: Adding experts to a decoder-only transformer block]
```

**디코더 전용 트랜스포머 블록에 전문가 추가 (출처: [2])**

그러나 트랜스포머의 모든 피드포워드 레이어에 전문가를 사용할 필요는 없습니다. 대부분의 MoE 기반 LLM은 P 의 스트라이드(stride)를 사용하는데, 이는 P 번째 레이어마다 전문가 레이어로 변환되고 다른 레이어는 그대로 유지됨을 의미합니다 — 이들은 "인터리브된(interleaved)" MoE 레이어입니다. 이 접근 방식은 결과 모델의 성능과 효율성 사이에서 더 나은 균형을 달성하는 데 사용될 수 있습니다.

## 라우팅 알고리즘(Routing Algorithms)

MoE 기반 아키텍처의 주요 이점 중 하나는 효율성이지만, 전문가만 사용하는 것으로는 효율성이 향상되지 않습니다! 사실, 모델의 각 레이어에 더 많은 전문가를 추가하면 모델의 총 매개변수 수 — 그리고 필요한 계산량 —가 크게 증가합니다. 아키텍처를 더 효율적으로 만들기 위해서는 각 레이어에서 사용될 전문가들을 희소하게(sparsely) 선택해야 합니다!

**전문가 선택.** d 차원 토큰 벡터(token vector)로 표현되는 단일 토큰을 고려해 봅시다. 우리의 목표는 이 토큰을 처리할 전문가들의 부분 집합(크기 k)을 선택하는 것입니다. MoE 문헌에서는 일반적으로 토큰이 이 전문가들에게 "라우팅(routed)"될 것이라고 말합니다. 이 라우팅 작업을 계산하고 최적화할 알고리즘이 필요합니다.

가장 간단한 라우팅 알고리즘은 토큰 벡터에 선형 변환(linear transformation)을 적용하여 N 크기(즉, 전문가 수)의 벡터를 형성하는 것입니다. 그런 다음, 소프트맥스 함수(softmax function)를 적용하여 우리 토큰에 대한 전문가 집합에 걸쳐 확률 분포(probability distribution)를 형성할 수 있습니다. 이 분포를 사용하여 분포에서 상위 K 개의 전문가를 단순히 선택함으로써 우리 토큰이 라우팅되어야 할 전문가를 선택할 수 있습니다.

```
[Image: Computing output of routing mechanism]
```

**라우팅 메커니즘의 출력 계산**

현재 활용되는 희소 MoE 계층 구조를 처음 제시한 [1] 논문에서 이 라우팅 방식이 채택되었습니다. 위 그림을 참고하십시오. 하지만 이러한 라우팅 방식은 전문가들의 고른 선택을 명시적으로 유도하지 않습니다. 결과적으로, 모델은 아래에서 설명하듯이 전문가 계층을 고르게 사용하기보다는, 모든 토큰에 대해 특정 소수의 전문가만을 반복적으로 선택하는 경향을 보이며 수렴할 수 있습니다. 이러한 현상을 흔히 "라우팅 붕괴(routing collapse)"라고 지칭합니다.

"게이팅 네트워크(gating network)는 항상 동일한 소수의 전문가에 대해 큰 가중치를 생성하는 상태로 수렴하는 경향이 있습니다. 이러한 불균형은 선호되는 전문가들이 더 빠르게 훈련되고 따라서 게이팅 네트워크에 의해 더욱 선택되기 때문에 자가 강화됩니다." - [1]에서

라우팅 알고리즘의 발전은 MoE 모델의 실제 성능에 지대한 영향을 미칩니다. 초기에는 간단한 상위 K 선택 방식이 주로 사용되었지만, 이는 특정 전문가에게만 트래픽이 집중되는 '핫 전문가(hot expert)' 문제를 야기할 수 있었습니다. 이를 해결하기 위해 동적 라우팅(dynamic routing)이나 계층적 라우팅(hierarchical routing)과 같은 보다 정교한 방법들이 연구되고 있습니다. 동적 라우팅은 모델이 훈련 과정에서 라우팅 결정을 유연하게 조정하도록 학습하며, 계층적 라우팅은 먼저 넓은 범주의 전문가 그룹을 선택한 다음, 그 안에서 세부 전문가를 선택하는 방식으로 효율성을 높입니다. 그러나 이러한 고급 라우팅 방식은 그 자체로 계산 오버헤드를 발생시킬 수 있으며, 이는 MoE의 핵심 이점인 효율성과 상충될 수 있습니다. 따라서 라우팅 알고리즘의 설계는 모델의 전체적인 효율성과 성능을 결정하는 중요한 요소입니다.

**활성 매개변수(Active parameters).** MoE 레이어 내에서 각 토큰을 처리하기 위해 전문가의 부분 집합만 선택하기 때문에, MoE 문헌에는 "활성(active)" 매개변수라는 개념이 있습니다. 간단히 말해, 주어진 토큰을 처리할 때 MoE 모델의 전체 매개변수 중 작은 부분 — 각 MoE 레이어에서 선택된 전문가들에 의해 결정됨 —만 활성화됩니다. 결과적으로 MoE에 의해 수행되는 총 계산량은 전체 매개변수 수가 아닌 활성 매개변수 수에 비례합니다.

## 보조 손실(Auxiliary Losses) 및 전문가 로드 밸런싱(Expert Load Balancing)

모델 훈련 과정에서 전문가들이 고르게 선택되도록 유도하기 위해, 각 전문가의 균등한 활용을 장려하는 추가적인 제약 조건을 훈련 손실에 통합할 수 있습니다. [1] 논문에서는 이를 위해 각 전문가의 "중요도" 점수를 정의하는 방식을 사용합니다. 이 중요도 점수는 라우팅 메커니즘이 각 전문가에 대해 예측한 확률에 근거합니다.

```
[Image: Details of computing the importance loss]
```

**중요도 손실 계산 세부 정보 (출처: [1])**

데이터 배치(batch)가 주어졌을 때, 배치 내의 모든 토큰에 걸쳐 각 전문가에게 할당된 확률의 합을 취하여 중요도를 계산합니다. 위를 참조하십시오. 그런 다음, 이러한 확률이 균형 잡혀 있는지 확인하기 위해 전문가 중요도 점수의 제곱 변동 계수(squared coefficient of variation, CV)를 취할 수 있습니다. 간단히 말해, 모든 전문가가 유사한 중요도 점수를 가지면 CV는 작은 값이 되고 그 반대도 마찬가지입니다. 여기서, 위에서 보여준 중요도 손실을 표준 언어 모델링 손실(language modeling loss)에 추가하여 새로운 (정규화된) 훈련 목표(training objective)를 형성할 수 있습니다. 이 추가적인 중요도 손실 항은 MoE가 훈련 과정 전반에 걸쳐 전문가들에게 동일한 확률을 할당하도록 돕습니다. 이러한 보조 손실 항들은 MoE 모델의 훈련 안정성을 높이는 데 필수적이지만, 동시에 여러 손실 항 간의 균형을 맞추는 것이 중요합니다. 중요도 손실, 로드 밸런싱 손실, 라우터 z-손실 등 다양한 보조 손실들은 각각 다른 목적을 가지며, 이들의 가중치(scaling factor)를 적절히 설정하는 것은 하이퍼파라미터 튜닝의 중요한 부분이 됩니다. 너무 강한 보조 손실은 모델의 주요 학습 목표(예: 언어 모델링)를 방해할 수 있으며, 너무 약하면 라우팅 붕괴와 같은 문제가 발생할 수 있습니다. 최근 연구에서는 이러한 손실 항들의 가중치를 훈련 단계에 따라 동적으로 조정하는 적응형 스케일링(adaptive scaling) 기법도 탐구되고 있습니다. 이는 훈련 초기 단계에서는 전문가 활용의 균형을 강력하게 유도하고, 후반 단계에서는 주요 작업 성능에 집중하도록 하는 방식입니다.

**로드 밸런싱(Load balancing).** 위에서 설명한 중요도 손실이 유용하더라도, 전문가들에게 동일한 중요도가 할당되었다고 해서 토큰이 균일하게 라우팅된다는 의미는 아닙니다. 예를 들어, 전문가들은 다음과 같은 경우에 동일한 중요도를 가질 수 있습니다:

*   매우 높은 확률을 할당하는 소수의 토큰.
*   훨씬 더 많은 수의 낮은 확률을 할당하는 토큰.

결과적으로, 중요도 손실을 사용하더라도 각 전문가에게 전달되는 토큰의 수는 여전히 매우 불균일할 수 있으며, 이는 과도한 메모리 사용과 MoE 2 의 전반적인 효율성 저하로 이어질 수 있습니다.

```
[Image: The expert load balancing loss]
```

**전문가 로드 밸런싱 손실 (출처: [2])**

이 문제를 해결하기 위해, 우리는 전문가 중요도와 로드 밸런싱(각 전문가 간 토큰의 균등 라우팅으로 정의됨)을 모두 포착하는 단일 보조 손실 항(auxiliary loss term)(위에 표시됨)을 생성할 수 있습니다. 이러한 접근 방식은 [2]에서 제안되었으며, 저자들은 두 가지 양을 고려하는 손실을 생성합니다:

*   각 전문가에게 할당된 라우터 확률의 비율 3 .
*   각 전문가에게 전달된 토큰의 비율.

이 두 양을 각각의 N 차원 벡터에 저장하면, 이 두 벡터의 내적(dot product) 4 을 취하여 단일 손실 항을 생성할 수 있습니다. 결과 손실은 전문가들이 균일한 확률과 로드 밸런싱을 받을 때 최소화되며, 따라서 단일 보조 손실 항 내에서 우리의 두 가지 목표를 모두 포착합니다!

```
[Image: The router-z loss]
```

**라우터-z 손실 (출처: [3])**

**라우터 z-손실(Router z-loss).** 위에 설명된 보조 로드 밸런싱 손실은 MoE 문헌 전반에 걸쳐 널리 사용되지만, [3]의 저자들은 훈련 안정성(training stability)을 더욱 향상시킬 수 있는 라우터 z-손실(router z-loss)이라는 추가 보조 손실 항을 제안합니다. 라우터 z-손실은 라우팅 메커니즘에 의해 예측되는 로짓(logits)의 크기를 제한합니다 — 확률이 아닌, 소프트맥스가 적용되기 전의 값입니다. 이상적으로는 이 로짓들이 너무 커지는 것을 원치 않습니다. 그러나 이 로짓들은 라우터의 (지수) 소프트맥스 함수를 통과할 때 매우 커질 수 있으며 — 이는 전체 ( float32 ) 정밀도(precision)를 사용할 때조차 훈련 과정을 불안정하게 만들 수 있는 반올림 오류(round-off errors)로 이어집니다.

"라우터는 float32 정밀도로 전문가들에 대한 확률 분포를 계산합니다. 그러나 가장 큰 규모에서는 이것이 신뢰할 수 있는 훈련을 제공하기에 불충분하다는 것을 발견했습니다." - [3]에서

라우터가 더 작은 로짓을 예측하도록 장려하기 위해, 위에 표시된 손실 항을 사용할 수 있습니다. 이 손실이 라우터의 로짓을 정규화하는 데만 초점을 맞추고 로드 밸런싱을 수행하지 않는다는 점을 감안할 때, 우리는 일반적으로 라우터 z-손실을 [2]에서 제안된 보조 로드 밸런싱 손실과 함께 사용합니다. 이 두 손실은 LLM의 표준 언어 모델링 손실 위에 추가됩니다. 아래를 참조하십시오.

```
[Image: Combined loss function for MoE training]
```

## 전문가 용량(Expert Capacity)

MoE 레이어에서 수행되는 계산은 훈련 및 추론(inference) 중에 이루어지는 라우팅 결정으로 인해 동적입니다. 그러나 희소 모델(sparse models)의 대부분의 실제 구현을 살펴보면, 일반적으로 정적 배치 크기(static batch sizes)를 가집니다 — 이는 하드웨어 활용(hardware utilization)을 개선하는 데 유용한 방법입니다.

```
[Image: Expert capacity diagram]
```

(출처: [2])

**전문가 용량.** 각 전문가에게 할당되는 고정된 배치 크기를 명확히 하기 위해, 우리는 '전문가 용량'이라는 개념을 도입할 수 있습니다. 이 전문가 용량은 다음 공식과 같이 정의됩니다.

```
[Mathematical formula for expert capacity]
```

전문가 용량은 각 전문가에게 보낼 수 있는 배치 내 토큰의 최대 수를 정의합니다. 전문가에게 라우팅된 토큰 수가 전문가 용량을 초과하면, 우리는 이 추가 토큰들을 "드롭(drop)"합니다. 더 구체적으로 말하면, 이 토큰들에 대한 계산은 수행하지 않고, 트랜스포머의 잔여 연결(residual connection)을 통해 그 표현이 다음 레이어로 직접 흐르도록 합니다.

"하드웨어 활용을 개선하기 위해, 희소 모델의 대부분의 구현은 각 전문가에 대해 정적 배치 크기를 가집니다. 전문가 용량은 각 전문가에게 라우팅될 수 있는 토큰의 수를 의미합니다. 이 용량을 초과하면 오버플로우된 토큰들은… 잔여 연결을 통해 다음 레이어로 전달됩니다." - [3]에서

전문가 용량은 MoE 모델의 하드웨어 효율성에 직접적인 영향을 미칩니다. 특히 분산 환경에서 여러 GPU에 걸쳐 전문가들이 분산될 때, 각 GPU의 메모리 용량과 통신 대역폭은 전문가 용량 설정의 중요한 고려 사항이 됩니다. 용량 계수를 높게 설정하면 드롭되는 토큰의 수가 줄어들어 모델 성능에는 좋지만, 각 전문가가 처리해야 할 최대 토큰 수가 늘어나 GPU 메모리 사용량이 증가하고, 이는 결국 더 큰 배치 크기를 처리하는 데 제약을 줄 수 있습니다. 반대로 용량 계수가 너무 낮으면 많은 토큰이 드롭되어 모델이 충분한 정보를 처리하지 못하고 성능 저하로 이어질 수 있습니다. 따라서 하드웨어 제약 조건과 모델 성능 요구 사항 사이의 최적점을 찾는 것이 중요합니다.

전문가 용량은 용량 계수(capacity factor) 설정을 통해 제어됩니다. 용량 계수가 1이라는 것은 토큰이 전문가들 사이에 완벽하게 균형 잡힌 방식으로 라우팅됨을 의미합니다. 대안적으로, 용량 계수를 1보다 높게 설정하면 전문가들 간의 토큰 불균형을 수용하기 위한 추가 버퍼(buffer)를 제공합니다. 그러나 이는 비용(예: 더 높은 메모리 사용량과 낮은 효율성)을 수반합니다.

```
[Image: Capacity factor vs. dropped tokens]
```

(출처: [2])

**용량 계수는 어떻게 설정할까요?** 흥미롭게도, MoE 모델은 비교적 낮은 용량 계수에서도 잘 작동하는 경향이 있습니다 [2, 3]. 위를 참조하십시오. 그러나 훈련 실행에 영향을 미치지 않도록 드롭되는 토큰의 수가 너무 많지 않도록 해야 합니다 (즉, 이는 경험적으로 수행될 수 있습니다). 또한 훈련과 추론에 다른 용량 계수를 사용할 수 있습니다. 예를 들어, ST-MoE [3]는 훈련 중에는 1.25의 용량 계수를 사용하고 평가 중에는 2.0의 용량 계수를 사용합니다.

## MoE 레이어의 출력 계산

```
[Image: Computing output of an MoE layer]
```

**MoE 레이어의 출력 계산**

라우터로부터 결과값을 획득하면, 최종 결과는 다음 단계에 따라 산출됩니다:

*   해당 토큰을 선택된 전문가들에게 전달합니다.
*   각 전문가가 해당 토큰에 대한 결과물을 도출합니다.
*   라우터가 각 전문가에게 부여한 확률을 가중치로 사용하여 전문가 결과물들의 가중 평균을 구합니다.

```
[Mathematical formula for MoE layer output]
```

위 방정식에서, 우리는 단일 토큰에 대한 MoE 레이어의 출력을 계산하는 과정을 형식화했습니다. 이 토큰의 출력은 K 개의 활성 전문가 각각의 출력에 대한 가중 평균입니다.

**공유 전문가(Shared experts)**는 MoE 문헌 [14, 15]에서 비교적 최근에 도입된 아이디어입니다. 아이디어는 간단합니다: 우리는 두 그룹의 전문가를 가집니다 — 공유 전문가와 라우팅된 전문가(routed experts). 모든 토큰은 항상 공유 전문가들을 통과합니다. 토큰은 일반적인 MoE 라우팅 메커니즘에 따라 라우팅된 전문가들을 통과합니다. 공유 전문가에 대한 이 아이디어는 아래에 묘사되어 있으며, MoE 레이어 내의 전문가 부분 집합에만 라우팅이 적용됨을 볼 수 있습니다. 일반적으로 공유 전문가의 수는 라우팅된 전문가의 수보다 적어야 합니다 — 공유 전문가의 수를 늘리면 MoE의 희소성 이점(sparsity benefits)이 저하됩니다. 공유 전문가의 도입은 모델의 학습 효율성과 일반화 능력에 긍정적인 영향을 미칠 수 있습니다. 특정 정보나 기능이 여러 전문가에게 중복되어 학습되는 비효율성을 줄이고, 모든 입력이 반드시 거쳐야 하는 핵심적인 지식이나 처리 단계를 공유 전문가가 담당함으로써 모델의 견고성(robustness)을 높일 수 있습니다. 예를 들어, 기본적인 문법 규칙이나 일반적인 의미론적 패턴과 같은 보편적인 지식은 공유 전문가가 처리하고, 특정 도메인이나 복잡한 추론과 관련된 세부적인 지식은 라우팅된 전문가들이 담당하도록 분리할 수 있습니다. 이는 모델이 보다 구조화된 방식으로 지식을 습득하고 활용하도록 돕습니다.

```
[Image: Shared vs. routed experts]
```

**공유 전문가 대 라우팅된 전문가 (출처: [14])**

공유 전문가를 사용하는 동기는 전문가들 간의 중복 정보(redundant information) 양을 최소화하는 것입니다. 공유 전문가 집합을 가짐으로써, 네트워크가 동일한 정보를 여러 다른 전문가에 걸쳐 복제할 필요 없이 이 전문가들 내에 공유 정보를 저장할 수 있도록 합니다. 공유 전문가가 있는 MoE 레이어의 출력을 계산하려면, 단순히 공유 전문가의 출력을 일반적인 라우팅된 출력에 추가합니다. 아래를 참조하십시오.

```
[Mathematical formula for MoE layer output with shared experts]
```

**공유 전문가가 있는 MoE 레이어의 출력 계산**

## 모두 종합하기: MoE 레이어를 가진 디코더 전용 LLM

```
[Image: A full MoE block in a transformer]
```

**트랜스포머 내의 전체 MoE 블록 (출처: [2])**

MoE 레이어의 전체 묘사는 위에 제공됩니다. MoE에서는 표준 디코더 전용 트랜스포머의 블록 구조를 피드포워드 네트워크를 전문가 레이어로 대체함으로써 수정합니다. 간단히 말해, 이 전문가 레이어는 원래 피드포워드 네트워크의 여러 독립적인 복사본을 포함합니다. 특히, MoE 레이어 내의 이 모든 구성 요소 — 일반 레이어(들), 전문가들, 그리고 라우팅 메커니즘 —는 경사 하강법(gradient descent)을 통해 공동으로 훈련됩니다. 각 토큰에 대해, 우리는 라우팅 메커니즘을 통해 어떤 전문가를 사용할지 선택할 수 있으며, 이는 일반적으로 토큰 벡터의 간단한 선형 변환을 통해 구현됩니다. 이를 종합하면, MoE의 수정된 블록 구조는 다음을 포함합니다:

*   셀프 어텐션 레이어(self-attention layer).
*   잔여 연결과 정규화 연산(normalization operation).
*   토큰을 전문가에게 라우팅하는 것을 결정하는 라우팅 메커니즘.
*   여러 독립적인 피드포워드 네트워크를 가진 전문가 레이어.
*   각 토큰에 대한 전문가 레이어의 최종 출력에 적용되는 최종 추가 및 정규화 연산.

수정된 블록 구조를 제외하고는 트랜스포머 아키텍처는 동일하게 유지됩니다. 또한 트랜스포머의 P 번째 블록만 MoE 레이어를 사용하도록 변환하며 — 다른 블록은 변경되지 않습니다. 일부 MoE는 모든 레이어에 전문가를 사용하지만, 실제로는 P 를 2, 4 또는 심지어 6으로 설정하는 것이 일반적입니다. 이 방법은 MoE LLM이 소비하는 총 매개변수 수를 제어하는 데 유용할 수 있습니다.

## MoE 사용의 장단점

이제 MoE의 기본 사항을 이해했으니, 궁금할 수 있습니다: 왜 밀집 모델 대신 MoE를 사용하고 싶을까요? MoE의 가장 큰 장점은 효율성이지만, 이 모델들에는 주목할 만한 단점도 있습니다. MoE의 가장 중요한 장단점 몇 가지를 빠르게 살펴보겠습니다.

**MoE의 장점.** LLM은 규모의 이점을 얻습니다 — 더 큰 모델과 더 큰 데이터셋은 더 나은 성능으로 이어집니다. 그러나 LLM을 확장하는 데는 비용이 따릅니다! MoE의 주요 이점 중 하나는 확장과 관련된 문제를 회피하는 능력입니다 — 이들은 토큰당 고정된 계산 비용(computational cost)으로 모델의 크기를 늘릴 수 있도록 합니다. 이런 식으로, 밀집 모델에만 국한된다면 불가능했을 더 큰 모델을 훈련할 수 있습니다. 언어 모델링 영역에서, 이러한 희소 모델의 추가 매개변수와 표현 능력(representational capacity)은 큰 차이를 만듭니다.

"LLM이 점점 더 보편화됨에 따라, 계산 자원을 비례적으로 늘리지 않고 성능을 향상시키는 것은 중요한 과제입니다." - [12]에서

MoE의 계산 이점은 (논쟁의 여지가 있지만) 추론 중에 가장 큰 영향을 미칩니다. MoE 모델은 총 매개변수 수 면에서 크기 때문에, 이러한 매개변수를 저장할 수 있는 충분한 수의 GPU가 필요합니다. 그러나 각 토큰을 처리할 때 이 매개변수 중 고정된 부분만 사용하므로 계산 효율성이 크게 향상됩니다. 낮은 배치 크기에서는 추론이 더 빠르고, 큰 배치 크기에서는 처리량(throughput)이 더 높습니다 [5]. 흥미롭게도 MoE는 훈련에도 더 효율적입니다. 예를 들어, 스위치 트랜스포머는 MoE 아키텍처 사용으로 7배의 사전 훈련 속도 향상을 보고했습니다 [2]. 아래를 참조하십시오.

```
[Image: Training speedup with MoE]
```

(출처: [2])

**MoE 사용의 단점.** 이러한 이점에도 불구하고 MoE는 또한 다음과 같습니다:

*   훈련 중 불안정성에 취약합니다.
*   미세 조정(finetune)하기 어렵습니다 (즉, 과적합(overfitting) 문제로 인해).
*   낮은/혼합 정밀도 훈련 기술(low / mixed precision training techniques)에 민감합니다.
*   하이퍼파라미터(hyperparameter) 설정(예: 가중치 초기화(weight initialization))에 민감합니다.

간단히 말해, MoE를 최대한 활용하려면 더 많은 부가 기능과 미세 조정이 필요합니다. 이러한 이유로, MoE는 모든 시나리오에서 최선의 선택이 아닐 수 있습니다. 예를 들어, 특정 작업에서 LLM을 미세 조정하려는 경우 밀집 모델이 더 쉬운 선택일 수 있습니다. 그러나 제대로 사용할 수 있다면 MoE는 다양한 이점을 가집니다. MoE 모델의 배포 및 서비스는 밀집 모델에 비해 복잡성이 증가할 수 있습니다. 특히, 활성화되는 전문가들이 여러 장치(예: GPU)에 분산되어 있을 때 발생하는 통신 병목 현상(communication bottlenecks)은 추론 지연 시간(latency)에 큰 영향을 미칠 수 있습니다. 각 토큰이 라우팅될 때마다 필요한 전문가들의 가중치를 불러오고 계산 결과를 취합하는 과정에서 데이터 전송이 발생하며, 이는 고속 인터커넥트(interconnect)가 필수적임을 의미합니다. 또한, MoE 모델은 일반적으로 훨씬 더 많은 총 매개변수를 가지므로, 모델 가중치를 저장하는 데 더 많은 메모리가 필요합니다. 이는 클라우드 환경에서 GPU 인스턴스를 선택하거나 온프레미스(on-premise) 하드웨어를 설계할 때 중요한 고려 사항이 됩니다. 이러한 엔지니어링적 도전에도 불구하고, MoE는 그 성능과 효율성 잠재력 덕분에 활발히 연구되고 개선되고 있습니다.

## 전문가 혼합 언어 모델(Mixture-of-Experts Language Models)

이제 MoE의 가장 중요하고 기본적인 개념을 이해했으니, 이 개념들이 언어 모델링 영역에서 어떻게 적용되었는지 더 깊이 살펴보겠습니다. LLM이 규모 증가의 이점을 얻는다는 사실 때문에, MoE는 LLM 연구 내에서 널리 채택되었고 큰 성공을 거두었습니다.

### Mixtral of Experts [5]

```
[Image: Mixtral performance comparison]
```

(출처: [5])

Mixtral 8×7B (일명 전문가들의 믹스트랄)은 Mistral-7B 모델 [6]을 기반으로 하며, 영어, 프랑스어, 이탈리아어, 독일어, 스페인어 등 다국어에 능숙한 오픈 소스 MoE 확장 모델입니다. 이 두 모델은 아파치 2.0(Apache 2.0) 라이선스 하에 개방형 가중치를 제공하며, 모델에 대한 상세 정보는 기술 보고서에서 확인할 수 있습니다. Mixtral은 Mistral의 모든 계층을 8개의 전문가를 포함하는 전문가 계층으로 변환합니다. 각 토큰에 대해 이 중 두 개의 전문가가 활성화되며, 이는 총 470억 개의 매개변수와 130억 개의 활성 매개변수를 가진 모델을 탄생시킵니다. 또한 이 모델은 32K의 컨텍스트 길이를 지원하는데, 이는 비-MoE 모델보다 4배 더 긴 수준입니다. 위 그래프에서 나타나듯이, Mixtral은 전반적으로 Mistral을 능가하며, 특히 코드 생성, 수학, 그리고 다국어 벤치마크에서 탁월한 성과를 보여 일부 경우 더 큰 LLaMA-2-70B 모델의 성능을 초과하기도 합니다.

```
[Image: GQA diagram]
```

(출처: [7])

**Mistral-7B 아키텍처.** Mixtral 8×7B의 기본 LLM 아키텍처는 Mistral-7B [6]의 아키텍처 설정과 정확히 일치하는 디코더 전용 트랜스포머입니다. 표준 디코더 전용 LLM 아키텍처와 비교하여, Mistral-7B에 의해 몇 가지 변경 사항이 있습니다:

*   **그룹화된 쿼리 어텐션(Grouped-Query Attention, GQA) [7]**: 효율성을 개선하기 위해 셀프 어텐션 헤드(self-attention heads) 그룹 간에 키(key) 및 값(value) 투영(projections)을 공유합니다. 위를 참조하십시오.
*   **슬라이딩 윈도우 어텐션(Sliding Window Attention, SWA) [8]**: 각 토큰에 대해 크기 W 의 고정된 윈도우(window)에 걸쳐 (마스크된) 셀프 어텐션을 계산하여 LLM이 감소된 추론 비용 5 으로 임의 길이의 시퀀스를 처리할 수 있도록 합니다. 아래를 참조하십시오.

```
[Image: Sliding Window Attention diagram]
```

SWA를 사용하기 때문에, 모델은 롤링 버퍼(rolling buffer) / 순환 캐시(circular caches)와 같은 방법을 사용하여 KV 캐시(KV cache)를 더 메모리 효율적으로 만들거나 청크된 프리필(chunked prefill)을 사용하여 추론 속도를 높일 수 있습니다. Mixtral 8×7B는 동일한 아키텍처 관례를 채택합니다.

(출처: [6])

**추가 세부 정보.** 이전에 언급했듯이, Mixtral은 LLM의 모든 레이어를 전문가 레이어로 변환합니다. 각 전문가 레이어 내에서는 모든 토큰에 대해 선형 레이어(linear layer)의 상위 K 로짓에 소프트맥스를 취하는 간단한 라우팅 메커니즘이 채택됩니다 — 이는 이 개요의 시작 부분에서 논의된 라우팅 메커니즘과 일치합니다. 아래를 참조하십시오.

```
[Image: Mixtral routing mechanism]
```

(출처: [5])

[5]의 저자들은 Mixtral이 다국어 코퍼스(multilingual corpus)에 걸쳐 사전 훈련되어 모델이 여러 언어를 이해할 수 있다고 언급합니다. 아래에서 볼 수 있듯이, Mixtral은 다국어 벤치마크에서 LLaMA 모델을 보편적으로 능가합니다.

```
[Image: Mixtral multilingual performance]
```

(출처: [5])

**라우팅 분석.** 논문을 마무리하기 위해, [5]의 저자들은 여러 도메인에 걸쳐 토큰에 대해 전문가가 어떻게 선택되는지에 대한 상세한 분석을 수행하여 해석 가능한 패턴을 추론할 수 있는지 확인합니다. The Pile 내의 다양한 주제 영역에 대해 다른 전문가에게 할당된 토큰 분포를 플로팅할 때, 토큰 할당에서 명확한 패턴은 나타나지 않습니다. 아래를 참조하십시오.

```
[Image: Token distribution across experts by topic]
```

(출처: [5])

그러나 MoE는 일부 구조화된 동작을 보입니다. 예를 들어, 파이썬 코드의 "self"와 영어의 "Question"이라는 단어는 — 비록 여러 토큰으로 구성되어 있지만 — 종종 동일한 전문가를 통해 라우팅됩니다. 유사하게, 코드의 들여쓰기 토큰(indentation tokens)은 일반적으로 동일한 전문가에게 보내지며, 연속적인 시퀀스(consecutive sequences) — 서로 가까이 있는 토큰들의 시퀀스 —는 일반적으로 동일한 전문가에게 보내집니다. 아래를 참조하십시오. 이러한 결과는 i) 전문가가 주제별로 전문화되지 않지만 ii) MoE의 라우팅 메커니즘이 모델 입력의 구문(syntax) 또는 내용과 관련하여 일부 구조화된 동작을 따른다는 것을 나타냅니다.

```
[Image: Structured routing behavior]
```

(출처: [5])

**확장.** Mixtral 이후, Mixtral-8×22B 라는 더 큰 버전의 모델이 출시되었습니다. 이 모델은 총 1410억 개의 매개변수와 390억 개의 활성 매개변수를 가지며, 원래 Mixtral 모델보다 약 3배 더 큽니다. Mixtral-8×22B는 코딩 및 수학 작업에 특히 능숙하며, 64K로 확장된 컨텍스트 길이를 가지고 있고, 함수 호출(function calling)을 기본적으로 수행할 수 있습니다. Mixtral-8×22B의 다른 오픈 모델 대비 주요 이점은 아래에 요약되어 있습니다.

```
[Image: Mixtral-8x22B benefits summary]
```

( 출처 )

### Grok (from xAI ) [9]

모델에 대한 상세한 기술 보고서는 없지만, MoE 기반 LLM의 가장 주목할 만한 최근 사례 중 하나는 xAI의 Grok입니다. 초기 Grok-1 모델은 2024년 초에 출시되었습니다. 연구원들은 이 모델이 각 토큰에 대해 25%의 가중치가 활성화되는 3140억 개의 매개변수를 가진 MoE(즉, 약 700억~800억 개의 활성 매개변수)라고 밝혔습니다. Grok-1의 아키텍처와 기본 모델 가중치는 Apache 2.0 라이선스 하에 오픈 소스화되었습니다. 그러나 이는 사전 훈련된 기본 모델이며, 모델의 후처리 훈련(post training) 과정에 대한 세부 정보는 제공되지 않았습니다.

```
[Image: Grok-1.5 performance comparison]
```

(출처: [10])

**Grok-1.5 [10].** Grok-1 6 의 초기 출시 직후, 더 나은 추론(reasoning) 및 긴 컨텍스트 이해(long context understanding) 기능을 갖춘 후속 버전의 모델이 발표되었습니다. 예를 들어, Grok-1.5는 수학 및 코딩 관련 작업에서 훨씬 더 좋은 성능을 보입니다. 위를 참조하십시오.

"이 모델은 컨텍스트 윈도우(context window)가 확장됨에 따라 지시 따르기(instruction-following) 능력을 유지하면서 더 길고 복잡한 프롬프트(prompts)를 처리할 수 있습니다." - [10]에서

Grok-1.5는 건초 더미 속 바늘 찾기 테스트(needle in a haystack test)에서 완벽한 검색(retrieval)으로 최대 128K 토큰의 시퀀스를 처리할 수 있습니다. 아래를 참조하십시오. 저자들은 또한 모델이 많은 컨텍스트가 주어졌을 때 견고한 지시 따르기 능력을 유지한다고 언급하는데, 이는 순수한 검색 7 에 비해 긴 컨텍스트 능력의 훨씬 더 좋은 신호입니다.

```
[Image: Grok-1.5 needle in a haystack test]
```

(출처: [10])

Grok-1.5와 Grok-1이 이렇게 짧은 간격으로 출시되었다는 점을 감안할 때, Grok-1.5의 발전은 후처리 훈련에 의해 주도되었다고 추론할 수 있습니다 — 이 기간 동안 다른 사전 훈련된 기본 모델이 생성되었을 가능성은 극히 낮습니다.

**Grok-2.** 최근에는 Grok-2가 출시되었는데, 이는 챗봇 아레나(Chatbot Arena)에서 측정된 바와 같이 추론, 코딩 및 채팅 기능이 향상되었습니다. Grok-2는 또한 다양한 다른 작은 개선 사항(예: 도구 사용, 검색, 사실성 등)을 가지고 있으며, Grok-2의 증류 버전(distilled version)인 Grok-2-mini가 주 모델과 함께 출시되었습니다. 그러나 Grok-2의 아키텍처에 대한 공개된 세부 정보는 공유되지 않았습니다 — 이 모델은 처음부터 훈련되었을 가능성이 높으며 MoE 기반일 수도 있고 아닐 수도 있습니다. Grok 모델은 xAI의 설립 이념인 "재미있고 반항적인(fun and rebellious)" 태도를 반영하는 독특한 대화 스타일로 주목받았습니다. 이는 단순히 기술적 성능을 넘어, 모델의 개성과 사용자 경험 측면에서 LLM이 나아갈 수 있는 새로운 방향을 제시했습니다. Grok-2의 아키텍처 세부 정보가 아직 공개되지 않았지만, Grok-1이 MoE를 성공적으로 활용했음을 감안할 때, Grok-2 역시 MoE 기반일 가능성이 높습니다. 특히, 추론 및 코딩 능력의 향상은 더 많은 전문가 용량(expert capacity)이나 개선된 라우팅 메커니즘을 통해 달성되었을 수 있습니다. Grok의 사례는 MoE가 단순한 성능 향상을 넘어, 모델의 특정 '페르소나'나 특성을 구현하는 데에도 기여할 수 있음을 시사합니다.

### DBRX (from Mosaic ) [11]

```
[Image: DBRX performance comparison]
```

(출처: [11])

DBRX는 Mosaic이 출시한 오픈 LLM 시리즈의 최신 모델입니다. 모델의 두 가지 버전 — 기본 모델(DBRX base)과 미세 조정된 모델(DBRX Instruct) —이 오픈 라이선스(즉, Databricks 오픈 모델 라이선스) 하에 출시되었습니다. DBRX는 다음 사양을 가진 MoE 기반 LLM입니다:

*   360억 개의 활성 매개변수를 가진 총 1320억 개의 매개변수.
*   각 MoE 레이어에 16개의 전문가가 있으며, 각 토큰에 대해 4개의 전문가가 활성화됩니다.
*   최적화된 텍스트 12조 토큰으로 사전 훈련되었습니다.
*   사전 훈련 효율성에서 4배 향상.

가장 주목할 만한 점은 DBRX가 "세분화된(fine-grained)" MoE 모델이라는 것입니다. 즉, 이 모델은 각 MoE 레이어에 더 많은 수의 전문가를 사용하지만, 각 개별 전문가는 더 작습니다. 참고로, Mixtral과 Grok-1 모두 각 MoE 레이어 내에 8개의 전문가를 포함하며 — 이 중 두 개는 주어진 토큰에 대해 활성화됩니다. 세분화된 전문가를 사용함으로써, 각 MoE 레이어는 선택할 수 있는 더 많은 전문가 조합(특히 65배 더 많음)을 가지며, 이는 [11]에서 품질을 향상시키는 것으로 밝혀졌습니다. DBRX의 "세분화된 MoE(fine-grained MoE)" 접근 방식은 MoE 아키텍처의 한계를 극복하려는 중요한 시도입니다. 기존 MoE 모델은 소수의 큰 전문가를 사용하여 특정 작업에 대한 전문화를 유도했지만, 이는 전문가 간의 중복 학습을 야기하거나 특정 전문가가 과부하되는 문제를 일으킬 수 있었습니다. DBRX는 더 많은 수의 작고 세분화된 전문가를 도입함으로써, 각 전문가가 더 미묘한 특징이나 작업에 특화될 수 있는 기회를 제공합니다. 예를 들어, 하나의 큰 전문가가 "프로그래밍"이라는 광범위한 영역을 담당하는 대신, DBRX는 "파이썬 문법", "자바스크립트 함수", "SQL 쿼리 최적화"와 같이 더 구체적인 하위 영역에 특화된 여러 작은 전문가를 가질 수 있습니다. 이러한 세분화는 모델이 주어진 입력에 대해 훨씬 더 정교하고 적절한 전문가 조합을 선택할 수 있게 하여, 전반적인 모델의 품질과 효율성을 향상시킵니다. 이는 LLM이 복잡하고 다양한 작업에 대해 더 유연하게 대응할 수 있도록 돕는 진보적인 설계입니다.

**훈련 데이터.** DBRX의 사전 훈련 데이터셋은 매우 크지만 8 , [11]의 저자들은 데이터의 품질을 개선하는 데도 상당한 투자를 했습니다. 결과적으로 DBRX의 통계적 훈련 효율성(statistical training efficiency)은 일반적인 경우보다 높습니다 — 더 적은 토큰으로 더 높은 정확도를 달성하기 때문에 훈련이 더 빠릅니다. 더 구체적으로, [11]의 저자들은 새로운 데이터가 토큰당 2배 더 효율적이라고 추정하는데, 이는 절반 이하의 토큰으로 훈련하고도 동일한 수준의 성능을 달성할 수 있음을 의미합니다. 이 주장은 새로운 모델의 사전 훈련 데이터가 단독으로 미치는 영향을 테스트함으로써 검증되었습니다 (즉, 다른 사전 훈련 데이터를 가진 고정 모델을 사용).

"단독으로 볼 때, 더 나은 사전 훈련 데이터는 모델 품질에 상당한 영향을 미쳤습니다. 우리는 DBRX 사전 훈련 데이터를 사용하여 1조 토큰으로 7B 모델을 훈련했습니다. 이는 MPT-7B의 30.9%에 비해 Databricks Gauntlet에서 39.0%에 도달했습니다." - [11]에서

또한, DBRX를 훈련하기 위해 커리큘럼 학습(curriculum learning)이 사용됩니다 — 사전 훈련 데이터의 혼합은 사전 훈련 과정 전반에 걸쳐 동적으로 변경됩니다. 이 커리큘럼 학습 전략의 세부 사항은 나중에 이 논문에서 설명되었습니다. DBRX가 사용하는 커리큘럼 학습 전략은 웹 크롤링(web-crawling)을 통해 얻은 데이터에 비해 품질이 더 높기 때문에 훈련 후반부에 더 작고 도메인별 데이터셋을 단순히 업샘플링(upsamples)합니다. 이 간단한 커리큘럼 학습 전략은 어려운 벤치마크에서 성능을 크게 향상시키는 것으로 밝혀졌습니다. 아래를 참조하십시오.

```
[Image: DBRX curriculum learning performance]
```

( 출처 )

**토크나이저(Tokenizer) 및 컨텍스트 윈도우(context window).** DBRX는 32K의 컨텍스트 길이를 가지며 GPT-4 토크나이저(tiktoken을 통해 사용 가능)를 사용합니다。저자들에 따르면, GPT-4 토크나이저는 주로 성능 때문에 선택되었습니다. 이 토크나이저는 큰 어휘를 가지고 있으며 토큰 효율성이 매우 높아, 동일한 양의 텍스트를 더 적은 토큰으로 표현함으로써 디코딩(decoding) 및 훈련 속도를 자연스럽게 향상시킵니다.

```
[Image: DBRX tokenizer efficiency]
```

(출처: [11])

**효율성 이점.** DBRX의 제안은 사전 훈련 효율성 면에서 큰 개선을 가져옵니다. 지금까지 배운 것 외에도, [11]에서 언급된 효율성 향상의 추가적인 원천이 몇 가지 있습니다:

*   MoE 아키텍처는 소규모 실험에서 훈련 중 1.7배 더 적은 FLOPS를 필요로 하는 것으로 밝혀졌습니다.
*   디코더 전용 아키텍처에 대한 다른 수정 사항(즉, RoPE , GLU 활성화(GLU activation) 및 GQA ).
*   "더 나은 최적화 전략(optimization strategies)".

모든 데이터, 아키텍처 및 최적화 변경 사항을 고려할 때, DBRX의 종단 간(end-to-end) 훈련 프로세스는 이전 모델에 사용된 사전 훈련 파이프라인(pretraining pipeline)과 비교하여 4배 더 적은 계산량을 필요로 합니다. 이 수치를 결정하기 위해, [11]의 저자들은 DBRX의 더 작은 변형 모델을 이전 MPT-7B 모델과 비교하여, 더 작은 DBRX 모델이 훈련 중 3.7배 더 적은 FLOPS를 사용하면서 Databricks Gauntlet에서 유사한 성능을 달성한다는 것을 발견했습니다。

```
[Image: DBRX training efficiency comparison]
```

(출처: [21])

DBRX는 또한 추론 효율성(inference efficiency) 개선을 가져오는데 — 로드 테스트(load tests)에서 사용자당 초당 150 토큰으로 LLaMA-2-70B보다 최대 2배 더 빠릅니다. 이러한 측정은 TensorRT-LLM 및 16비트 정밀도(precision)를 사용하는 최적화된 서빙 인프라(serving infrastructure)를 사용하여 이루어졌으며, 이는 매우 빠릅니다. DBRX의 MoE 아키텍처는 비교적 적은 수의 활성 매개변수로 인해 추론 효율성에도 도움이 됩니다. 예를 들어, DBRX는 총 매개변수 및 활성 매개변수 모두에서 Grok-1 크기의 40%입니다.

"전문가 혼합 모델을 훈련하는 것은 어렵습니다. 우리는 DBRX급 모델을 효율적인 방식으로 반복적으로 훈련할 수 있을 만큼 견고한 파이프라인을 구축하기 위해 다양한 과학적 및 성능적 과제를 극복해야 했습니다." - [11]에서

MoE 훈련은 일반적으로 훈련 중 발생하는 불안정성, 통신 병목 현상 등으로 인해 어렵습니다. 그러나 DBRX는 [11]에 설명된 최적화된 사전 훈련 전략 덕분에 안정성, 효율성 및 성능 면에서 인상적인 결과를 달성합니다. 특히, 이러한 결과를 가능하게 하는 단일 변경 또는 발전은 없습니다. DBRX가 사용하는 인상적인 사전 훈련 파이프라인은 수많은 작고 실용적인 변경 사항에 의해 가능해졌습니다.

**경험적 평가.** 다른 오픈 LLM과 비교할 때, DBRX-Instruct는 Mixtral과 비교하여 복합 벤치마크에서 큰 차이로 더 나은 성능을 달성합니다. 아래를 참조하십시오. DBRX는 범용 LLM임에도 불구하고 인상적인 프로그래밍 기술을 가지고 있으며, Grok-1 (크기가 두 배 이상!)과 CodeLLaMA-70B와 같은 전문 코딩 모델까지 능가합니다. DBRX는 추론 및 수학 기반 작업에서도 좋은 성능을 보입니다.

```
[Image: DBRX-Instruct vs. other open LLMs]
```

(출처: [11])

폐쇄형 모델과 비교할 때, DBRX는 GPT-3.5의 성능을 능가하며 Gemini-1.0 Pro 와 경쟁할 만합니다. Gemini-1.0 Pro는 GSM8K에서만 DBRX보다 뛰어난 성능을 보이며, Mixtral-Medium은 고려되는 몇 가지 특정 작업에서 더 나은 성능을 보입니다. 아래를 참조하십시오. 높은 수준에서 볼 때, DBRX는 프로그래밍, 수학, 일반 지식, 상식 추론(commonsense reasoning) 및 검색/RAG(retrieval / RAG)에 능숙한 것으로 보입니다.

```
[Image: DBRX-Instruct vs. closed models]
```

(출처: [21])

### OpenMoE: 오픈 전문가 혼합 언어 모델에 대한 초기 노력 [12]

언어 모델링 영역에서 MoE의 성공에도 불구하고, 코드, 정보, 데이터, 가중치 등이 모두 공개적으로 공유되는 진정한 오픈 소스 MoE의 수는 상대적으로 적습니다. 이 문제를 해결하기 위해 OpenMoE [12]는 6억 5천만 개에서 340억 개의 매개변수에 이르는 디코더 전용 MoE LLM 제품군을 훈련하기 위한 대규모 노력을 수행합니다. 이 모델들은 다양한 세분성(granularity)(즉, 16개 또는 32개의 전문가)을 가진 세분화된 전문가를 채택합니다. 이 노력의 결과는 [12]에 문서화되어 있으며 모든 모델은 공개적으로 공유됩니다. 저자들은 또한 그들의 결과를 재현하는 데 사용할 수 있는 잘 문서화된 코드 저장소(code repository)를 제공합니다.

[OpenMoE 저장소](https://github.com/OpenMoE/OpenMoE)

"모든 레이어에 MoE를 사용하는 것은 라우팅 중에 더 많은 계산 오버헤드(computational overhead)를 발생시키고 인터리브된 MoE 사용보다 더 나쁜 비용-효율성 절충점을 유발합니다." - [12]에서

**설계 선택.** OpenMoE 모델은 ST-MoE [3]의 설정을 채택하며, 동일한 라우팅 메커니즘과 활성 전문가 수(즉, k = 2 )를 포함합니다. 저자들은 4번째 또는 6번째 트랜스포머 블록만 MoE 레이어로 변환하기로 선택했으며, 더 큰 스트라이드가 비용과 효율성 면에서 더 나은 절충점을 제공한다는 것을 발견했습니다. OpenMoE에 사용된 사전 훈련 데이터셋은 코드의 높은 분포를 포함합니다. 사실, 사전 훈련 초기 단계에서는 코드가 데이터셋의 50% 이상을 차지했지만, 이 비율은 나중에 훈련에서 최적이 아니라는 이유로 조정되었습니다. 아래를 참조하십시오. 정렬(alignment)을 위해, OpenMoE는 사전 훈련 후 WildChat의 데이터를 사용하여 SFT(Supervised Fine-Tuning)를 거쳐 더 나은 지시 따르기 능력을 유도합니다.

```
[Image: OpenMoE pretraining data composition]
```

(출처: [12])

**라우팅 동역학(Routing dynamics).** OpenMoE의 주요 기여 중 하나는 모델 내에서 이루어진 라우팅 결정에 대한 상세한 분석입니다. 먼저, 이전 연구 [5]에서 보여진 결과와 유사하게 — 전문가들은 특정 도메인에 전문화되는 경향이 없음을 알 수 있습니다. 아래를 참조하십시오.

```
[Image: OpenMoE expert specialization by domain]
```

(출처: [12])

그러나 아래 그림에서 볼 수 있듯이, 자연어 및 특정 작업에 걸쳐 어느 정도의 전문가 전문화(expert specialization)가 나타납니다.

```
[Image: OpenMoE expert specialization by language/task]
```

(출처: [12])

그러나 이 경향을 더 깊이 파고들면, 토큰 라우팅의 동역학은 주로 토큰 ID(token ID)에 의해 결정된다는 것을 알 수 있습니다. 즉, 동일한 토큰은 그 토큰이 존재하는 컨텍스트와 상관없이 거의 항상 동일한 전문가에게 라우팅됩니다. 이 패턴은 [12]에서 "컨텍스트 독립적 전문화(Context-Independent Specialization)"라고 불립니다.

"이것은 매우 흥미로운 발견인데, 동일한 토큰 ID를 가진 토큰들이 다른 문장에서 매우 다양한 컨텍스트를 가지기 때문입니다. 예를 들어, 토큰 'an'은 'an apple' 또는 'another'의 일부일 수 있습니다. 그러나 이 모든 토큰들은 소수의 고정된 전문가에 대해 매우 강력한 전문화를 보입니다." - [12]에서

흥미롭게도, 전문가들은 선호하는 토큰에서 관찰 가능한 패턴을 가집니다. 아래를 참조하십시오. 예를 들어, "have", "has", "had"는 모두 동일한 전문가에게 라우팅되는 반면, 한 전문가는 "=", "and", "\n" 토큰을 받습니다 — 코딩 언어 내에서 매우 흔한 토큰들입니다. [12]에서 우리는 이러한 라우팅 패턴이 사전 훈련의 초기 단계에서 확고해지며 훈련 후반에는 거의 변하지 않는다는 것을 알 수 있습니다.

```
[Image: OpenMoE expert token preferences]
```

(출처: [12])

**라우팅 문제.** [12]에서 관찰된 라우팅 패턴 외에도, OpenMoE 모델이 성능을 손상시킬 수 있는 일부 라우팅 동작을 보인다는 것을 알 수 있습니다. 예를 들어, 모델은 시퀀스 후반에 토큰을 드롭하는 경향이 있는데, 이는 긴 시퀀스 작업(예: 다중 턴 채팅)의 성능을 손상시킬 수 있습니다.

```
[Image: OpenMoE multi-turn chat performance]
```

**OpenMoE 모델은 다중 턴 채팅 문제에서 더 나쁜 성능을 보입니다 (출처: [12])**

라우팅 동역학이 사전 훈련 과정의 초기 단계에서 고정되기 때문에, 이러한 동작은 후처리 훈련 중에 수정하기 어렵습니다. 사실, OpenMoE 모델은 사전 훈련과 SFT 9 중 데이터 간의 도메인 격차(domain gap)로 인해 일반적으로 어려움을 겪는 것으로 관찰됩니다 — 데이터 구성의 차이로 인해 토큰 라우팅 동역학이 불규칙해집니다. 이러한 문제를 해결하기 위해, [12]의 저자들은 지시 따르기 데이터를 사전 훈련 데이터셋에 혼합할 것을 권장합니다. OpenMoE 프로젝트는 MoE 연구의 투명성과 재현성(reproducibility)을 높이는 데 크게 기여했습니다. 기존의 많은 LLM 연구가 폐쇄적으로 진행되거나 상세한 구현 정보를 공개하지 않는 경향이 있었던 반면, OpenMoE는 모델 가중치뿐만 아니라 훈련 코드와 상세한 분석 결과를 공유함으로써 다른 연구자들이 MoE 아키텍처를 탐구하고 개선할 수 있는 귀중한 기반을 마련했습니다. 이는 오픈 사이언스(open science)의 정신을 구현하며, 커뮤니티 전체의 발전 속도를 가속화하는 중요한 역할을 합니다. 특히, "컨텍스트 독립적 전문화"와 같은 흥미로운 라우팅 동역학에 대한 발견은 MoE 모델이 어떻게 지식을 조직하고 활용하는지에 대한 심층적인 이해를 제공하며, 향후 더 효율적이고 안정적인 MoE 설계를 위한 단서를 제공합니다.

**모델 평가.** 전반적으로, OpenMoE 모델은 MoE LLM 중에서 새로운 최첨단 성능(state-of-the-art performance)을 달성하지 못했습니다 — [12]의 저자들은 이 사실을 공개적으로 밝히고 OpenMoE 모델의 성능이 더 나은 설계를 통해 크게 향상될 수 있음을 인정합니다. OpenMoE 모델의 더 큰 기여는 그들의 투명성입니다. [12]에서 공개적으로 공유된 세부 정보와 아티팩트(artifacts)는 이 주제에 대한 추가 연구를 수행하는 데 필요한 자원을 제공함으로써 MoE에 대한 오픈 연구 노력을 가속화할 수 있습니다.

### DeepSeek-v2 [14] 및 DeepSeek-V3 [15]

최근 제안된 DeepSeek MoE 모델들, DeepSeek-v2 [14]와 DeepSeek-v3 [15]를 포함하여, 다양한 이유로 LLM 연구 커뮤니티 내에서 큰 반향을 일으켰습니다:

*   그들의 가중치는 공개적으로 공유됩니다.
*   많은 세부 정보를 공유하는 기술 보고서가 함께 제공됩니다.
*   그들의 성능은 인상적이며 — 많은 폐쇄형 모델과 동등합니다.
*   그들의 훈련 비용은 상당히 합리적입니다.

우리가 보게 될 것처럼, DeepSeek 모델은 훈련 효율성과 다운스트림 성능(downstream performance)을 모두 극대화하는 다양한 독특한 설계 선택을 합니다. DeepSeek-v2 [14]— 210억 개의 활성 매개변수를 가진 2360억 개의 매개변수 MoE —는 이후 DeepSeek-V3 모델에서 사용된 MoE 아키텍처를 제안합니다. DeepSeek MoE 모델은 성능과 효율성을 높이기 위해 기본 트랜스포머 블록을 약간 수정한다는 점에서 이전 모델들과는 다소 다릅니다. 아래에서 볼 수 있듯이, DeepSeek-v2 모델은 — 좋은 성능을 보이는 것 외에도 — 훈련 및 추론 효율성 관점에서 상당히 인상적이며, 훨씬 더 큰 DeepSeek-v3 모델의 강력한 출발점이 됩니다.

```
[Image: DeepSeek-v2 efficiency comparison]
```

(출처: [14])

**멀티 헤드 잠재 어텐션(Multi-head latent attention, MLA).** 표준 멀티 헤드 어텐션(multi-headed attention) 대신, DeepSeek-v2는 효율적인 어텐션 변형인 MLA를 채택합니다. 멀티 쿼리 어텐션(multi-query attention) 또는 그룹화된 쿼리 어텐션과 유사하게, MLA는 모델의 KV 캐시가 소비하는 메모리를 최소화하는 것을 목표로 합니다. 그러나 다른 효율적인 어텐션 변형과 달리, MLA는 상당한 성능 절충점(performance tradeoff)을 가지지 않습니다.

```
[Image: MLA mechanism]
```

(출처: [14])

특히, 이러한 메모리 효율성 증가는 모든 키(key) 및 값(value) 벡터를 훨씬 더 작은 (잠재) 벡터로 표현할 수 있게 하는 저랭크(low-rank) 공동 투영(joint projection)을 통해 달성됩니다. 위를 참조하십시오. 이 벡터를 업샘플링(upsample)할 수 있습니다 — 단순히 선형적으로 투영하여 여러 개의 더 큰 벡터를 형성함 — 전체 키 및 값 벡터를 복원하기 위해, 그러나 우리는 KV 캐시에 잠재 벡터만 저장하면 되므로 메모리 소비를 크게 줄일 수 있습니다. MLA를 채택하면 DeepSeek-v2의 KV 캐시 크기가 670억 개의 매개변수를 가진 밀집 모델에 비해 93% 이상 감소합니다. 멀티 헤드 잠재 어텐션(MLA)은 긴 컨텍스트 길이(long context length)를 다루는 데 있어 매우 중요한 혁신입니다. LLM의 컨텍스트 윈도우가 길어질수록 KV 캐시의 메모리 요구량이 기하급수적으로 증가하여 추론 비용이 비싸지는 문제가 발생합니다. MLA는 이러한 KV 캐시의 크기를 획기적으로 줄임으로써, 모델이 훨씬 더 긴 텍스트를 효율적으로 처리할 수 있게 합니다. 예를 들어, 법률 문서 분석, 장문의 코드 이해, 또는 긴 대화 기록 요약과 같은 작업에서 MLA는 메모리 제약 없이 모델이 광범위한 정보를 활용할 수 있도록 지원합니다. 이는 실제 응용 분야에서 LLM의 유용성을 크게 확장하는 핵심 기술입니다.

**DeepSeek MoE 아키텍처.** MLA 사용 외에도, DeepSeek 모델은 독특한 MoE 레이어 구조를 채택합니다. DBRX와 유사하게, 이 모델들은 세분화된 전문가를 사용합니다. 그러나 이 전문가들 중 일부는 공유됩니다. 이러한 구조를 채택하는 동기는 전문가들 간의 중복 정보를 최소화하면서 더 많은 수의 전문가들 사이에서 전문화를 장려하는 것입니다. DeepSeek 모델이 사용하는 블록 구조의 전체 개략도는 아래에 제공됩니다.

```
[Image: DeepSeek MoE block structure]
```

(출처: [14])

[14]의 저자들은 DeepSeek-v2가 사용하는 세분화된 전문가를 처리하기 위한 흥미로운 로드 밸런싱 전략도 채택합니다. [2]에서 제안된 보조 로드 밸런싱 손실을 사용하는 것 외에도, DeepSeek-v2는 분산 훈련(distributed training) 중 장치 간 통신 균형을 맞추는 것을 목표로 하는 두 가지 보조 손실 항을 가집니다.

```
[Mathematical formula for device-level load balancing auxiliary loss]
```

**장치 수준 로드 밸런싱 보조 손실 (출처: [14])**

세분화된 전문가를 사용하는 것은 각 토큰을 더 많은 수의 전문가에게 전달해야 함을 의미합니다. 분산 훈련 설정에서, 전문가들은 다른 장치에 있을 수 있으며 각 장치에는 여러 전문가가 상주합니다. 장치 간 통신과 계산이 균형을 이루도록 보장하기 위해, i) 전문가를 상주하는 장치별로 그룹화하고 ii) MoE가 장치별로 균형 잡힌 라우팅을 수행하도록 장려하는 추가 보조 손실이 필요합니다. 예를 들어, 위에 표시된 보조 손실은 장치 간 균형 잡힌 계산을 장려합니다. [14]에는 장치 간 균형 잡힌 통신을 장려하기 위한 추가 손실도 제안되어 있습니다.

```
[Image: DeepSeek-v3 performance comparison]
```

(출처: [15])

DeepSeek-v3 [15]는 DeepSeek-v2 10 의 훨씬 더 큰 버전으로, 총 6710억 개의 매개변수와 370억 개의 활성 매개변수를 가집니다. 이 더 큰 모델은 14.8조 토큰으로 구성된 방대한 코퍼스에서 사전 훈련되었습니다. 사전 훈련 후, 다단계 후처리 훈련 파이프라인(multi-phase post training pipeline)이 적용됩니다:

*   모델은 먼저 두 단계의 컨텍스트 확장 절차를 거치는데, SFT를 통해 최대 컨텍스트 길이가 32K가 되도록 미세 조정된 다음, 다시 128K의 컨텍스트 길이를 가지도록 미세 조정됩니다.
*   컨텍스트 확장 후, 모델은 인간의 선호도에 맞추기 위해 추가 SFT 및 RLHF(Reinforcement Learning from Human Feedback)를 거칩니다.
*   최근 제안된 R1 추론 모델의 기능도 후처리 훈련 중에 DeepSeek-v2에 증류(distilled)됩니다.

최종 DeepSeek-v3 모델은 폐쇄형 모델을 능가하며 심지어 최고의 폐쇄형 LLM과 유사한 성능을 달성합니다. 위를 참조하십시오. DeepSeek-v3는 또한 MoE의 훈련 및 로드 밸런싱 전략에 여러 수정을 가하여 모델의 훈련 프로세스를 효율적이고 안정적으로 만듭니다.

"뛰어난 성능에도 불구하고, DeepSeek-V3는 전체 훈련을 위해 2.788M H800 GPU 시간만 필요합니다. 또한, 훈련 과정은 놀랍도록 안정적입니다… 우리는 복구 불가능한 손실 급증을 경험하거나 롤백(rollbacks)을 수행하지 않았습니다." - [15]에서

DeepSeek-v3의 아키텍처는 그 전작에서 영감을 받았습니다. 예를 들어, MLA, 세분화된 전문가 및 공유 전문가는 모두 DeepSeek-v3에서 사용됩니다. 그러나 DeepSeek-v2와 달리 DeepSeek-v3는 다중 토큰 예측(Multi-Token Prediction, MTP) 훈련 목표를 사용합니다. 이 목표는 LLM 훈련에 거의 보편적으로 사용되는 지도 학습(supervised), 교차 엔트로피(cross entropy) 기반 다음 토큰 예측(next token prediction) 목표의 확장입니다. 시퀀스 내 각 토큰에 대해 다음 토큰을 예측하는 대신, MTP는 D 개의 미래 토큰을 예측합니다. 이러한 예측은 모델 아키텍처에 추가된 일련의 추가 모듈(additional modules)에 의해 순차적으로 이루어집니다. 아래를 참조하십시오.

```
[Image: Multi-Token Prediction (MTP) objective]
```

(출처: [15])

여러 미래 토큰이 예측되면, 우리는 교차 엔트로피 손실을 정상적으로 적용할 수 있습니다. MTP를 통해 예측된 여러 미래 토큰에 이 손실을 적용하면 모델에 더 풍부한 훈련 신호(training signal)를 제공하여 훈련 효율성과 전반적인 성능을 향상시킵니다. 더 나아가, MTP에 사용되는 이러한 추가 모듈은 추측 디코딩(speculative decoding)을 통해 추론 효율성을 향상시키는 데도 사용될 수 있습니다. 그러나 [15]의 저자들은 MTP 전략이 순전히 모델 성능 향상을 위해 사용되며 — 추가 모듈은 훈련 후 폐기된다고 명시합니다. 다중 토큰 예측(MTP) 목표는 LLM 훈련의 패러다임을 바꿀 수 있는 잠재력을 가지고 있습니다. 기존의 다음 토큰 예측(next token prediction) 방식은 한 번에 하나의 토큰만 예측하므로, 모델이 장기적인 의존성(long-term dependencies)을 학습하거나 문맥 전체의 일관성을 유지하는 데 어려움을 겪을 수 있습니다. MTP는 여러 미래 토큰을 동시에 또는 순차적으로 예측함으로써, 모델이 더 넓은 범위의 문맥을 고려하고 더 풍부한 의미론적 정보를 학습하도록 유도합니다. 이는 모델의 예측 정확도를 높일 뿐만 아니라, 훈련 과정에서 더 효율적인 학습 신호를 제공하여 전반적인 수렴 속도를 향상시킬 수 있습니다. MTP의 성공적인 적용은 LLM의 훈련 비용을 절감하고, 더 강력한 모델을 개발하는 데 중요한 기여를 할 것입니다.

```
[Mathematical formula for auxiliary-loss-free load balancing strategy]
```

**보조 손실 없는 로드 밸런싱 전략 (출처: [15])**

DeepSeek-v3는 또한 보조 손실 없는 로드 밸런싱 전략을 사용하는데, 이는 단순히 상위 K 전문가 선택에 전문가별 편향 항(per-expert bias term)을 추가합니다. 위를 참조하십시오. 각 반복마다, 각 전문가에 대한 편향 항은 해당 전문가가 각각 과소 로드(underloaded)되었는지 과부하(overloaded)되었는지에 따라 고정된 계수 γ 만큼 증가하거나 감소합니다. 중요하게도, 이러한 편향은 상위 K 전문가를 선택할 때만 사용되며 — 라우터 내에서 전문가 확률 계산에 영향을 미치지 않습니다. 이 접근 방식은 MoE 내에서 전문가 활용을 효과적으로 균형 있게 만들고 로드 밸런싱 손실 사용으로 인한 성능 저하를 제거하는 것으로 밝혀졌습니다. 그러나 [15]의 저자들은 DeepSeek-v3를 훈련할 때 여전히 보조 로드 밸런싱 손실(매우 낮은 스케일링 계수(scaling factor)와 함께)을 사용한다고 언급합니다.

```
[Image: DeepSeek-v3 training cost]
```

(출처: [15])

**훈련 효율성.** 위에 설명된 전략들의 효율성 및 성능 이점 덕분에, DeepSeek-v3는 믿을 수 없을 정도로 경제적입니다. 게다가, 이 모델은 새로운 FP8 혼합 정밀도 훈련 프레임워크(FP8 mixed precision training framework)를 사용하여 훈련되었으며, 이는 대규모 LLM을 위한 8비트 훈련의 첫 번째 검증을 의미합니다. 총체적으로, 최종 모델 훈련 비용은 약 560만 달러로 추정되었습니다 11 . 위를 참조하십시오. 요약하자면, DeepSeek-v3는 다음과 같습니다:

*   매우 경제적인 방식으로 훈련되었으며 (FP8 훈련 및 MTP와 같은 여러 새로운 발전과 함께!)
*   오픈 모델로서는 믿을 수 없을 정도로 인상적이며 — 심지어 최고의 폐쇄형 LLM과도 매우 경쟁적입니다.
*   여러 새로운 수정 사항을 포함하는 흥미로운 MoE 아키텍처를 기반으로 합니다.

**추론.** DeepSeek-v3는 또한 최근 출시된 오픈 추론 모델인 DeepSeek-R1 [13]의 기본 모델 역할을 합니다. 간단히 말해, R1은 OpenAI가 최근 탐구한 o1 스타일 모델의 오픈 복제본입니다. 상세한 기술 보고서에서 설명된 바와 같이, 이 모델은 순수한 강화 학습(reinforcement learning)을 사용하여 극도로 긴 사고의 사슬(chains of thought)을 만들어 복잡한 (검증 가능한) 추론 작업을 해결하는 방법을 학습합니다. 아래 그림에서 볼 수 있듯이, R1의 성능은 특히 오픈 모델로서는 상당히 인상적입니다. 그러나 R1의 기능은 믿을 수 없을 정도로 유능한 기본 모델에 먼저 접근할 수 없었다면 불가능했을 것입니다.

```
[Image: DeepSeek-R1 performance]
```

(출처: [13])

## 최종 생각

MoE는 언어 모델 분야에 특히 유리한 수많은 장점을 제공합니다. 이 기술은 계산량의 급격한 증대 없이도 모델 규모를 확장할 수 있게 하며, 훈련 비용을 절감하고, 효율적인 서비스 운영을 가능하게 합니다. 희소성이라는 개념은 머신러닝 분야에 오랫동안 존재해 왔지만, MoE는 그중에서도 특히 강력하고 실질적인 구현 사례로 평가됩니다. MoE는 현대 컴퓨팅 환경, 특히 GPU와 호환되며 실제 적용 가능한 방식으로 희소성을 활용합니다. 초기 MoE 모델들은 복잡성과 불안정성, 그리고 활용의 어려움으로 인해 광범위한 채택에 난항을 겪었으나, 본 글에서 살펴본 최근의 발전들은 MoE를 실용적이고 파급력 있는 기술로 변모시켰습니다. 이는 디코더 전용 트랜스포머 아키텍처의 간단하면서도 매우 유망한 확장이라 할 수 있습니다. MoE 아키텍처는 LLM의 미래 발전에 있어 핵심적인 역할을 계속할 것으로 보입니다. 특히, 멀티모달(multimodal) LLM이나 에이전트 기반 AI 시스템과 같이 더욱 복잡한 AI 모델에서 MoE의 조건부 계산 능력은 더욱 빛을 발할 것입니다. 개별 전문가들이 특정 양식(modality)이나 특정 작업 단계에 특화되어, 전체 시스템이 훨씬 더 효율적이고 유연하게 작동하도록 설계할 수 있습니다. 또한, 동적 MoE(dynamic MoE)와 같이 훈련이나 추론 중에 활성화되는 전문가의 수를 동적으로 조절하는 연구는 자원 활용의 효율성을 한층 더 끌어올릴 잠재력을 가지고 있습니다. MoE는 단순한 모델 크기 확장을 넘어, 인공지능이 복잡한 현실 세계의 문제를 해결하는 데 필요한 지능과 효율성을 동시에 확보하는 중요한 도구로 진화하고 있습니다.

뉴스레터가 처음이신가요? 안녕하세요! 저는 Cameron R. Wolfe 이고, 넷플릭스(Netflix)의 딥러닝(Deep Learning) 박사이자 머신러닝 과학자(Machine Learning Scientist)입니다. 이것은 독자들이 AI 연구의 중요한 주제를 더 잘 이해하도록 돕는 딥(러닝) 포커스 뉴스레터입니다. 뉴스레터가 마음에 드시면 구독하고, 공유하거나, X와 LinkedIn에서 저를 팔로우해주세요!

[구독]

## 참고 문헌

[1] Shazeer, Noam, et al. "터무니없이 큰 신경망: 희소하게 게이팅된 전문가 혼합 레이어(Outrageously large neural networks: The sparsely-gated mixture-of-experts layer)." arXiv preprint arXiv:1701.06538 (2017).
[2] Fedus, William, Barret Zoph, and Noam Shazeer. "스위치 트랜스포머: 간단하고 효율적인 희소성으로 조 단위 매개변수 모델로 확장(Switch transformers: Scaling to trillion parameter models with simple and efficient sparsity)." Journal of Machine Learning Research 23.120 (2022): 1-39.
[3] Zoph, Barret, et al. "ST-MoE: 안정적이고 전이 가능한 희소 전문가 모델 설계(St-moe: Designing stable and transferable sparse expert models)." arXiv preprint arXiv:2202.08906 (2022).
[5] Jiang, Albert Q., et al. "전문가들의 믹스트랄(Mixtral of experts)." arXiv preprint arXiv:2401.04088 (2024).
[6] Jiang, Albert Q., et al. "미스트랄 7B(Mistral 7B)." arXiv preprint arXiv:2310.06825 (2023).
[7] Ainslie, Joshua, et al. "GQA: 멀티 헤드 체크포인트에서 일반화된 멀티 쿼리 트랜스포머 모델 훈련(GQA: Training Generalized Multi-Query Transformer Models from Multi-Head Checkpoints)." arXiv preprint arXiv:2305.13245 (2023).
[8] Beltagy, Iz, Matthew E. Peters, and Arman Cohan. "롱포머: 긴 문서 트랜스포머(Longformer: The long-document transformer)." arXiv preprint arXiv:2004.05150 (2020).
[9] xAI. “Grok-1 오픈 릴리스(Open Release of Grok-1)” https://x.ai/blog/grok-os (2024).
[10] xAI. “Grok-1.5 발표(Announcing Grok-1.5)” https://x.ai/blog/grok-1.5 (2024).
[11] Mosaic Research (Databricks). “DBRX 소개: 새로운 최첨단 오픈 LLM(Introducing DBRX: A New State-of-the-Art Open LLM)” https://www.databricks.com/blog/introducing-dbrx-new-state-art-open-llm (2024).
[12] Xue, Fuzhao, et al. "OpenMoE: 오픈 전문가 혼합 언어 모델에 대한 초기 노력(Openmoe: An early effort on open mixture-of-experts language models)." arXiv preprint arXiv:2402.01739 (2024).
[13] Guo, Daya, et al. "DeepSeek-R1: 강화 학습을 통한 LLM의 추론 능력 장려(DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning)." arXiv preprint arXiv:2501.12948 (2025).
[14] Liu, Aixin, et al. "DeepSeek-v2: 강력하고 경제적이며 효율적인 전문가 혼합 언어 모델(Deepseek-v2: A strong, economical, and efficient mixture-of-experts language model)." arXiv preprint arXiv:2405.04434 (2024).
[15] Liu, Aixin, et al. "DeepSeek-v3 기술 보고서(Deepseek-v3 technical report)." arXiv preprint arXiv:2412.19437 (2024).

1 다시 말해, 이 임베딩 레이어의 내용은 다른 모델 매개변수와 마찬가지로 훈련 과정 전반에 걸쳐 (경사 하강법을 통해) 업데이트됩니다.
2 하드웨어(예: GPU 활용, 처리량 등) 및 통계적(예: 모델이 데이터로부터 얼마나 빨리 학습하는지) 효율성을 모두 개선하기 위해, 우리는 각 데이터 배치에서 모든 전문가에게 전달되는 토큰 수가 비교적 균일하기를 원합니다.
3 이 양은 우리의 라우팅 알고리즘에 의해 예측되며, 따라서 미분 가능합니다. 따라서 각 전문가에게 보내지는 토큰의 비율 자체는 미분 가능한 양이 아니더라도 손실 함수 전체는 미분 가능합니다.
4 또한 전문가 수가 증가함에 따라 손실이 일정하게 유지되도록 이 손실 항에 N을 곱해야 합니다.
5 SWA가 각 토큰이 시퀀스 내에서 몇 개의 토큰만 "보도록" 제한하는 것처럼 보일 수 있습니다. 그러나 k 개의 연속적인 SWA 레이어를 서로 쌓으면 각 토큰의 유효 컨텍스트 윈도우(effective context window)가 증가합니다. 현재 토큰의 표현은 실제로는 그 앞에 오는 k⋅W 개의 토큰에 의해 영향을 받습니다.
6 이 두 모델은 모두 2024년 3월에 약 10일 간격으로 출시되었습니다!
7 건초 더미 속 바늘 찾기 테스트는 LLM이 컨텍스트 내에서 정보를 검색하는 능력을 테스트합니다. 그러나 모델이 건초 더미 속 바늘 찾기 테스트에서 완벽한 점수를 받더라도 여전히 좋지 않은 긴 컨텍스트 능력을 가질 수 있습니다. 예를 들어, 긴 컨텍스트가 주어졌을 때 지시 따르기 또는 추론 능력이 훨씬 더 나빠질 수 있습니다.
8 참고로, 동일 팀에서 출시된 이전 모델들 — MPT-7B 및 MPT-30B —은 1조 토큰의 텍스트로만 사전 훈련되었습니다.
9 사전 훈련과 비교하여 SFT 중에는 훨씬 다른 스타일의 데이터를 접하게 됩니다. 예를 들어, 다중 턴 채팅 데이터, 지시 템플릿 등이 있습니다.
10 이 모델들 사이에 DeepSeek은 DeepSeek-v2.5라는 중간 모델도 출시했습니다. 자세한 내용은 여기를 참조하십시오.
11 이 추정치는 H800 GPU 시간당 2달러의 임대 가격을 가정하여 이루어졌습니다. 또한, 이는 최종 모델 훈련의 순수 계산 비용만을 반영하며, 추가 비용이나 실험은 제외됩니다. DeepSeek-v3 훈련의 실제 총 비용은 이 보고된 수치보다 훨씬 클 것이 분명합니다.