(출처: [2, 5, 14]) 빠르게 변화하는 연구 분야에서, 디코더 전용 트랜스포머(decoder-only transformer) 아키텍처는 대규모 언어 모델(LLM) 연구에서 몇 안 되는 지속적인 핵심 요소 중 하나로 남아 있습니다. 이 아키텍처는 원래 GPT 모델이 제안된 이래로 사용되어 왔으며, 효율성 향상을 위한 사소한 조정 외에는 거의 변함없이 유지되어 왔습니다. 그러나 이 아키텍처에 대해 탐구될 가장 의미 있는 수정 중 하나는 전문가 혼합(Mixture-of-Experts, MoE) 레이어입니다.

"MoE 아키텍처를 사용하면 밀집 모델(dense model)이 일반적으로 달성하는 것보다 모델 품질과 추론 효율성(inference efficiency) 사이에서 더 나은 절충점(tradeoff)을 얻을 수 있습니다." - [11]에서

MoE 기반 LLM은 모델 아키텍처에 희소성(sparsity)을 도입하여, 총 매개변수(total parameters) 수를 기준으로 모델 크기를 계산 비용(compute costs)의 상응하는 증가 없이 크게 늘릴 수 있도록 합니다. Grok [9] 및 DeepSeek-v3 [15]와 같은 최신 모델에서 성공적으로 채택된 이 수정은 극도로 큰 모델의 탐색을 더 다루기 쉽고 계산 효율적으로 만듭니다. 이 개요에서는 MoE의 기본 사항을 배우고 이 아이디어가 최근 어떻게 적용되어 더 강력한 LLM을 만들었는지 탐구할 것입니다.

## LLM을 위한 MoE의 기본 원리

MoE 아키텍처를 이해하려면 먼저 디코더 전용 트랜스포머의 기본 구조와 MoE가 이 구조에 어떻게 통합되는지 파악해야 합니다. LLM은 텍스트를 입력으로 받아 토큰화(tokenization) 및 벡터화(vectorization) 과정을 거쳐 모델의 어휘(vocabulary) 내 토큰 임베딩(token embedding)으로 변환됩니다. 이러한 토큰 임베딩은 트랜스포머 블록의 각 서브 레이어를 통과하며, 이때 피드포워드 네트워크(Feed-Forward Network, FFN)는 각 토큰의 표현을 독립적으로 변환하는 핵심 역할을 합니다.

MoE는 바로 이 FFN 부분에 희소성(sparsity)과 조건부 계산(conditional computation)이라는 새로운 패러다임을 도입합니다. 기존의 밀집(dense) FFN이 모든 입력 토큰에 대해 동일한 가중치 집합을 사용하여 연산을 수행하는 반면, MoE는 입력 토큰의 특성에 따라 특정 전문가(expert)들만 활성화하여 계산 효율성을 극대화합니다. 이는 모델이 특정 유형의 정보나 패턴을 처리하는 데 특화된 여러 "전문가"를 가질 수 있게 함으로써, 전체 모델의 표현 능력(representational capacity)을 크게 확장합니다.

MoE는 이론적 기반을 마련한 선구적인 논문들을 통해 발전해왔습니다. 특히, 희소하게 게이팅된 전문가 혼합 레이어(The Sparsely-Gated Mixture-of-Experts Layer) [1]는 MoE 레이어의 핵심 개념과 라우팅 메커니즘을 제안했으며, 스위치 트랜스포머(Switch Transformers) [2]는 이를 대규모 언어 모델에 성공적으로 적용하여 훈련 효율성을 획기적으로 개선했습니다. 또한, 안정적이고 전이 가능한 전문가 혼합(Stable and Transferable Mixture-of-Experts, ST-MoE) [3]은 MoE 모델의 훈련 안정성을 높이는 데 기여했습니다. 이러한 연구들은 MoE가 LLM의 확장성과 효율성을 동시에 달성할 수 있는 중요한 아키텍처임을 입증했습니다.

## "전문가(experts)"란 무엇인가요?

디코더 전용 트랜스포머 아키텍처에서 MoE에 의해 이루어지는 주요 수정은 트랜스포머 블록의 피드포워드 구성 요소 내에 있습니다. 표준 아키텍처에서는 단일 피드포워드 신경망(feed-forward neural network) — 일반적으로 비선형 활성화(non-linear activation)가 중간에 있는 두 개의 피드포워드 레이어로 구성됨 —을 통해 모든 토큰이 개별적으로 전달됩니다. 아래를 참조하십시오.

```
[Image: Standard feed-forward network in a transformer block]
```

MoE는 이 블록 구조를 약간 수정합니다. 블록의 피드포워드 구성 요소 내에 단일 피드포워드 네트워크를 갖는 대신, 우리는 여러 개의 피드포워드 네트워크를 생성하며, 각 네트워크는 자체적인 독립적인 가중치(independent weights)를 가집니다. 우리는 이들 각 네트워크를 "전문가(expert)"라고 부릅니다. 예를 들어, MoE 기반 LLM은 각 피드포워드 서브 레이어에 8개의 독립적인 전문가를 가질 수 있습니다.

```
[Image: Experts within a transformer layer]
```

트랜스포머 레이어 내의 전문가들은 위에서 보여준 대로 정의될 수 있습니다. 한 레이어에 N 개의 전문가가 있으며, i 번째 전문가는 E_i 라는 표기법으로 참조할 수 있습니다.

### MoE 기반 트랜스포머 아키텍처 구현

MoE 기반 디코더 전용 트랜스포머 아키텍처를 구현하려면, 트랜스포머 블록 내의 피드포워드 레이어를 여러 전문가로 구성된 MoE 레이어로 대체합니다. 각 전문가는 기존 FFN과 동일한 내부 구조를 가지지만, 고유한 가중치를 학습합니다. 이처럼 여러 전문가를 배치함으로써 모델은 다양한 유형의 입력에 대해 특화된 처리를 수행할 수 있게 됩니다.

중요한 점은 모든 트랜스포머 블록에 MoE 레이어를 적용할 필요는 없다는 것입니다. 대부분의 MoE 기반 LLM은 "인터리브된(interleaved)" 접근 방식을 사용하는데, 이는 P 번째 레이어마다 MoE 레이어를 배치하고 그 외의 레이어는 표준 FFN을 유지하는 방식입니다. 예를 들어, P를 2로 설정하면 한 레이어는 MoE, 다음 레이어는 FFN, 그 다음 레이어는 다시 MoE가 됩니다. 이러한 인터리브 방식은 모델의 성능과 계산 효율성 사이에서 최적의 균형을 찾는 데 필수적입니다. 모든 레이어에 MoE를 적용하면 총 매개변수 수가 지나치게 증가하고 통신 오버헤드(communication overhead)가 커져 비효율적일 수 있기 때문입니다. 적절한 인터리브 전략은 모델의 확장성을 유지하면서도 MoE의 이점을 효과적으로 활용할 수 있게 합니다.

## 효율적인 토큰 라우팅 메커니즘

MoE 기반 아키텍처의 주요 이점 중 하나는 효율성이지만, 전문가만 사용하는 것으로는 효율성이 향상되지 않습니다! 사실, 모델의 각 레이어에 더 많은 전문가를 추가하면 모델의 총 매개변수 수 — 그리고 필요한 계산량 —가 크게 증가합니다. 아키텍처를 더 효율적으로 만들기 위해서는 각 레이어에서 사용될 전문가들을 희소하게(sparsely) 선택해야 합니다!

**전문가 선택.** d 차원 토큰 벡터(token vector)로 표현되는 단일 토큰을 고려해 봅시다. 우리의 목표는 이 토큰을 처리할 전문가들의 부분 집합(크기 k)을 선택하는 것입니다. MoE 문헌에서는 일반적으로 토큰이 이 전문가들에게 "라우팅(routed)"될 것이라고 말합니다. 이 라우팅 작업을 계산하고 최적화할 알고리즘이 필요합니다.

가장 간단한 라우팅 알고리즘은 토큰 벡터에 선형 변환(linear transformation)을 적용하여 N 크기(즉, 전문가 수)의 벡터를 형성하는 것입니다. 그런 다음, 소프트맥스 함수(softmax function)를 적용하여 우리 토큰에 대한 전문가 집합에 걸쳐 확률 분포(probability distribution)를 형성할 수 있습니다. 이 분포를 사용하여 분포에서 상위 K 개의 전문가를 단순히 선택함으로써 우리 토큰이 라우팅되어야 할 전문가를 선택할 수 있습니다.

```
[Image: Computing output of routing mechanism]
```

이 라우팅 전략은 오늘날 우리가 사용하는 희소 MoE 레이어 구조를 제안한 논문인 [1]에서 사용되었습니다. 위를 참조하십시오. 그러나 이러한 라우팅 메커니즘은 전문가의 균형 잡힌 선택을 명시적으로 장려하지 않습니다. 이러한 이유로, 모델은 아래에서 설명하는 바와 같이 전문가 레이어를 완전히 그리고 균일하게 활용하는 대신, 모든 토큰에 대해 동일한 소수의 전문가를 반복적으로 선택하는 상태로 수렴할 가능성이 높습니다. 이 현상은 일반적으로 "라우팅 붕괴(routing collapse)"라고 불립니다.

"게이팅 네트워크(gating network)는 항상 동일한 소수의 전문가에 대해 큰 가중치를 생성하는 상태로 수렴하는 경향이 있습니다. 이러한 불균형은 선호되는 전문가들이 더 빠르게 훈련되고 따라서 게이팅 네트워크에 의해 더욱 선택되기 때문에 자가 강화됩니다." - [1]에서

**활성 매개변수(Active parameters).** MoE 레이어 내에서 각 토큰을 처리하기 위해 전문가의 부분 집합만 선택하기 때문에, MoE 문헌에는 "활성(active)" 매개변수라는 개념이 있습니다. 간단히 말해, 주어진 토큰을 처리할 때 MoE 모델의 전체 매개변수 중 작은 부분 — 각 MoE 레이어에서 선택된 전문가들에 의해 결정됨 —만 활성화됩니다. 결과적으로 MoE에 의해 수행되는 총 계산량은 전체 매개변수 수가 아닌 활성 매개변수 수에 비례합니다.

### 라우팅 전략의 발전과 과제

초기 MoE 모델들은 단순한 Top-K 라우팅 방식을 사용했지만, 이는 "라우팅 붕괴"와 같은 문제로 인해 모든 전문가가 고르게 활용되지 못하는 결과를 초래했습니다. 이를 해결하기 위해 다양한 라우팅 전략이 연구되었습니다. 예를 들어, 일부 연구는 라우터 네트워크 자체를 학습 가능한 파라미터로 간주하여, 토큰의 특성을 더 잘 반영하는 라우팅 결정을 내리도록 유도합니다. 또한, Top-K 선택 시 전문가별 편향(bias)을 동적으로 조절하여 특정 전문가에게 토큰이 몰리는 현상을 완화하려는 시도도 있습니다.

라우팅의 또 다른 중요한 과제는 전문가 전문화(expert specialization)입니다. 이상적으로는 각 전문가가 특정 유형의 정보(예: 코드, 수학, 특정 언어)에 특화되기를 바라지만, 실제로는 토큰 ID에 기반한 컨텍스트 독립적인 라우팅 패턴이 관찰되기도 합니다 [12]. 이는 동일한 토큰이 어떤 문맥에 있든 항상 동일한 전문가에게 라우팅되는 경향을 의미하며, 모델의 표현 능력을 제한할 수 있습니다. 따라서 라우팅 메커니즘이 더 의미 있는 방식으로 전문가를 활용하도록 유도하는 연구가 지속적으로 필요합니다.

## MoE 훈련의 안정성 및 균형 유지

MoE 모델의 훈련은 밀집 모델보다 복잡하며, 전문가들의 고른 활용과 훈련 안정성 유지가 핵심 과제입니다. 이를 위해 다양한 보조 손실(auxiliary losses)과 전략이 개발되었습니다.

### 보조 손실을 통한 전문가 로드 밸런싱

전문가 로드 밸런싱은 모든 전문가가 훈련 중에 충분한 수의 토큰을 처리하도록 보장하여, 일부 전문가만 과도하게 활용되고 다른 전문가들은 "굶는" 현상(starvation)을 방지하는 것을 목표로 합니다. 이는 모델의 전반적인 학습 효율성과 성능에 직접적인 영향을 미칩니다.

**중요도 손실(Importance Loss)**은 각 전문가에 대한 토큰 할당 확률의 균형을 맞추는 데 중점을 둡니다 [1]. 라우팅 메커니즘이 예측하는 각 전문가의 확률 분포를 기반으로 '중요도' 점수를 계산하고, 이 점수들이 고르게 분포되도록 손실을 추가합니다. 하지만 이 손실만으로는 실제 토큰 할당의 불균형을 완전히 해소하기 어렵습니다. 특정 전문가에게 높은 확률을 할당받는 소수의 토큰이 존재하더라도, 실제로는 적은 토큰만 처리하게 될 수 있기 때문입니다.

**로드 밸런싱 손실(Load Balancing Loss)** [2]은 이러한 한계를 보완하기 위해 도입되었습니다. 이 손실은 각 전문가에게 할당된 라우터 확률의 비율과 실제로 각 전문가에게 전달된 토큰의 비율이라는 두 가지 요소를 동시에 고려합니다. 이 두 비율이 서로 유사하도록 유도함으로써, 라우터가 전문가들에게 균일한 확률을 할당할 뿐만 아니라 실제로 토큰들이 고르게 분배되도록 장려합니다. 이는 하드웨어 활용률을 높이고 메모리 과부하를 방지하는 데 필수적입니다.

**라우터 z-손실(Router z-loss)** [3]은 라우터의 훈련 안정성을 향상시키는 데 초점을 맞춥니다. 라우터의 로짓(logits) 값이 너무 커지면 소프트맥스 함수 적용 시 부동 소수점 오차(floating-point error)로 인해 훈련이 불안정해질 수 있습니다. 라우터 z-손실은 이러한 로짓의 크기를 제한하여 훈련 과정을 안정화합니다. 이 손실은 로드 밸런싱과는 다른 목적을 가지므로, 일반적으로 로드 밸런싱 손실과 함께 사용되어 MoE 훈련의 전반적인 견고성을 높입니다.

이러한 보조 손실들은 MoE 훈련의 복잡성을 관리하고 모델이 잠재력을 최대한 발휘할 수 있도록 돕는 중요한 도구입니다. 최근에는 보조 손실 없이도 로드 밸런싱을 달성하려는 시도도 있으며, 이는 전문가별 편향 항(bias term)을 동적으로 조정하여 전문가 활용도를 균형 있게 만드는 방식입니다 [15].

### 전문가 용량(Expert Capacity)

```
[Image: Expert capacity diagram]
```

(출처: [2])

**전문가 용량.** 각 전문가에 대해 설정하는 고정 배치 크기를 형식화하기 위해 전문가 용량을 정의할 수 있습니다. 전문가 용량은 아래와 같이 정의됩니다.

```
[Mathematical formula for expert capacity]
```

전문가 용량은 각 전문가에게 보낼 수 있는 배치 내 토큰의 최대 수를 정의합니다. 전문가에게 라우팅된 토큰 수가 전문가 용량을 초과하면, 우리는 이 추가 토큰들을 "드롭(drop)"합니다. 더 구체적으로 말하면, 이 토큰들에 대한 계산은 수행하지 않고, 트랜스포머의 잔여 연결(residual connection)을 통해 그 표현이 다음 레이어로 직접 흐르도록 합니다.

"하드웨어 활용을 개선하기 위해, 희소 모델의 대부분의 구현은 각 전문가에 대해 정적 배치 크기를 가집니다. 전문가 용량은 각 전문가에게 라우팅될 수 있는 토큰의 수를 의미합니다. 이 용량을 초과하면 오버플로우된 토큰들은… 잔여 연결을 통해 다음 레이어로 전달됩니다." - [3]에서

### 용량 계수 최적화 및 동적 할당

전문가 용량은 `용량 계수(capacity factor)`를 통해 조절됩니다. 용량 계수 1은 토큰이 전문가들 사이에 완벽하게 균형 잡힌 방식으로 라우팅될 때 각 전문가가 처리할 수 있는 최대 토큰 수를 의미합니다. 그러나 실제 훈련 환경에서는 토큰 라우팅이 완벽하게 균일하지 않으므로, 용량 계수를 1보다 높게 설정하여 토큰 불균형에 대한 버퍼를 제공하는 것이 일반적입니다. 예를 들어, 용량 계수를 1.25로 설정하면 각 전문가가 평균보다 25% 더 많은 토큰을 처리할 수 있는 여유를 가집니다.

용량 계수를 너무 낮게 설정하면 드롭되는 토큰의 수가 많아져 모델 성능이 저하될 수 있으며, 너무 높게 설정하면 메모리 사용량이 증가하고 계산 효율성이 떨어집니다. 따라서 훈련 안정성과 효율성을 동시에 고려하여 최적의 용량 계수를 찾는 것이 중요합니다. 흥미롭게도, MoE 모델은 비교적 낮은 용량 계수에서도 잘 작동하는 경향이 있으며 [2, 3], 이는 MoE의 희소성 이점을 잘 활용할 수 있음을 시사합니다.

최근에는 정적인 용량 계수 대신, 토큰 분포의 동적 변화에 따라 전문가 용량을 유연하게 조절하는 `동적 용량 할당(dynamic capacity allocation)` 연구도 진행되고 있습니다. 이는 전문가 활용도를 실시간으로 최적화하여 훈련 및 추론 효율성을 더욱 높일 수 있는 잠재력을 가집니다.

## MoE 레이어의 출력 계산 및 고급 기법

MoE 레이어의 최종 출력은 라우팅 메커니즘을 통해 선택된 전문가들의 출력을 결합하여 얻어집니다. 이 과정은 모델의 표현력을 결정하는 중요한 부분입니다.

### MoE 레이어 출력 계산의 기본

```
[Image: Computing output of an MoE layer]
```

라우터의 출력을 얻으면, 최종 출력을 다음과 같이 계산합니다:

*   토큰을 선택된 전문가들에게 보냅니다.
*   이 토큰들에 대한 전문가들의 출력을 계산합니다.
*   전문가 출력의 가중 평균(weighted average)을 취하며, 가중치는 라우터에 의해 각 전문가에게 할당된 확률입니다.

```
[Mathematical formula for MoE layer output]
```

위 방정식에서, 우리는 단일 토큰에 대한 MoE 레이어의 출력을 계산하는 과정을 형식화했습니다. 이 토큰의 출력은 K 개의 활성 전문가 각각의 출력에 대한 가중 평균입니다. 이 가중 평균은 각 전문가의 기여도를 조절하여 최종 출력을 형성하며, 이는 조건부 계산의 핵심입니다.

### 공유 전문가(Shared experts)

**공유 전문가(Shared experts)**는 MoE 문헌 [14, 15]에서 비교적 최근에 도입된 아이디어입니다. 아이디어는 간단합니다: 우리는 두 그룹의 전문가를 가집니다 — 공유 전문가와 라우팅된 전문가(routed experts). 모든 토큰은 항상 공유 전문가들을 통과합니다. 토큰은 일반적인 MoE 라우팅 메커니즘에 따라 라우팅된 전문가들을 통과합니다. 공유 전문가에 대한 이 아이디어는 아래에 묘사되어 있으며, MoE 레이어 내의 전문가 부분 집합에만 라우팅이 적용됨을 볼 수 있습니다. 일반적으로 공유 전문가의 수는 라우팅된 전문가의 수보다 적어야 합니다 — 공유 전문가의 수를 늘리면 MoE의 희소성 이점(sparsity benefits)이 저하됩니다.

```
[Image: Shared vs. routed experts]
```

**공유 전문가 대 라우팅된 전문가 (출처: [14])**

공유 전문가를 사용하는 동기는 전문가들 간의 중복 정보(redundant information) 양을 최소화하는 것입니다. 공유 전문가 집합을 가짐으로써, 네트워크가 동일한 정보를 여러 다른 전문가에 걸쳐 복제할 필요 없이 이 전문가들 내에 공유 정보를 저장할 수 있도록 합니다. 공유 전문가가 있는 MoE 레이어의 출력을 계산하려면, 단순히 공유 전문가의 출력을 일반적인 라우팅된 출력에 추가합니다. 아래를 참조하십시오.

```
[Mathematical formula for MoE layer output with shared experts]
```

**공유 전문가가 있는 MoE 레이어의 출력 계산**

### 계층적 MoE 및 동적 MoE

최근 연구에서는 MoE의 개념을 더욱 확장하여 `계층적 MoE(Hierarchical MoE)`와 `동적 MoE(Dynamic MoE)`와 같은 고급 기법들이 탐구되고 있습니다. 계층적 MoE는 전문가들을 여러 계층으로 구성하여, 첫 번째 계층의 라우터가 광범위한 전문가 그룹을 선택하고, 다음 계층의 라우터가 그 그룹 내에서 더 세분화된 전문가를 선택하는 방식입니다. 이는 모델이 더욱 복잡한 의사결정 과정을 통해 토큰을 처리하고, 전문가의 전문화 수준을 다단계로 조절할 수 있게 합니다.

`동적 MoE`는 모델이 훈련 또는 추론 중에 전문가의 수, 구성 또는 라우팅 규칙을 동적으로 변경하는 것을 허용합니다. 예를 들어, 특정 작업이나 입력에 따라 필요한 만큼의 전문가만 활성화하거나, 훈련 초기에는 모든 전문가를 사용하다가 점차 불필요한 전문가를 제거하여 모델을 경량화하는 방식 등이 있습니다. 이러한 유연성은 MoE 모델의 효율성을 극대화하고, 다양한 시나리오에 더욱 효과적으로 적응할 수 있도록 합니다.

## MoE의 장점과 도전 과제

### MoE의 장점.

LLM은 규모의 이점을 얻습니다 — 더 큰 모델과 더 큰 데이터셋은 더 나은 성능으로 이어집니다. 그러나 LLM을 확장하는 데는 비용이 따릅니다! MoE의 주요 이점 중 하나는 확장과 관련된 문제를 회피하는 능력입니다 — 이들은 토큰당 고정된 계산 비용(computational cost)으로 모델의 크기를 늘릴 수 있도록 합니다. 이런 식으로, 밀집 모델에만 국한된다면 불가능했을 더 큰 모델을 훈련할 수 있습니다. 언어 모델링 영역에서, 이러한 희소 모델의 추가 매개변수와 표현 능력(representational capacity)은 큰 차이를 만듭니다.

"LLM이 점점 더 보편화됨에 따라, 계산 자원을 비례적으로 늘리지 않고 성능을 향상시키는 것은 중요한 과제입니다." - [12]에서

MoE의 계산 이점은 (논쟁의 여지가 있지만) 추론 중에 가장 큰 영향을 미칩니다. MoE 모델은 총 매개변수 수 면에서 크기 때문에, 이러한 매개변수를 저장할 수 있는 충분한 수의 GPU가 필요합니다. 그러나 각 토큰을 처리할 때 이 매개변수 중 고정된 부분만 사용하므로 계산 효율성이 크게 향상됩니다. 낮은 배치 크기에서는 추론이 더 빠르고, 큰 배치 크기에서는 처리량(throughput)이 더 높습니다 [5]. 흥미롭게도 MoE는 훈련에도 더 효율적입니다. 예를 들어, 스위치 트랜스포머는 MoE 아키텍처 사용으로 7배의 사전 훈련 속도 향상을 보고했습니다 [2]. 아래를 참조하십시오.

```
[Image: Training speedup with MoE]
```

(출처: [2])

### MoE의 확장된 이점: 다중 모달 및 강건성

MoE는 언어 모델링 외에도 다양한 영역에서 확장된 이점을 제공합니다. 특히, `다중 모달(multimodal)` LLM에서 MoE는 텍스트, 이미지, 오디오 등 여러 양식(modality)의 데이터를 처리하는 데 특화된 전문가를 가질 수 있게 합니다. 예를 들어, 이미지 관련 전문가와 텍스트 관련 전문가를 분리하여 운영함으로써, 모델은 각 양식에 대한 깊이 있는 이해를 유지하면서도 전체적인 표현 능력을 향상시킬 수 있습니다. 이는 복잡한 다중 모달 작업을 효율적으로 처리하는 데 기여합니다.

또한, MoE는 모델의 `강건성(robustness)`을 향상시키는 데 도움을 줄 수 있습니다. 특정 전문가가 오염되거나 오작동하더라도, 다른 전문가들이 여전히 올바른 정보를 처리할 수 있으므로, 단일 밀집 모델보다 오류에 덜 취약할 수 있습니다. 이는 시스템의 신뢰성을 높이는 데 중요한 역할을 합니다. 또한, MoE는 특정 지식이나 기능을 특정 전문가에 `분리(isolate)`하여 저장할 수 있는 잠재력을 제공하며, 이는 모델의 해석 가능성(interpretability)을 높이고 특정 지식 영역을 업데이트하거나 수정하는 것을 용이하게 할 수 있습니다.

### MoE 훈련 및 배포의 난점과 해결 방안

MoE는 많은 장점에도 불구하고 훈련 및 배포 과정에서 여러 도전 과제에 직면합니다.

*   **훈련 불안정성**: MoE 모델은 라우팅 메커니즘과 전문가 간의 상호작용으로 인해 훈련이 불안정해지기 쉽습니다. 특히 `콜드 스타트(cold start)` 문제(초기 훈련 단계에서 전문가들이 제대로 전문화되지 않아 라우팅이 비효율적인 문제)나 라우팅 붕괴 현상이 발생할 수 있습니다. 이를 해결하기 위해 정교한 초기화 전략, 보조 손실, 그리고 훈련 과정에서 라우터를 점진적으로 활성화하는 기법 등이 연구되고 있습니다.
*   **미세 조정(Fine-tuning)의 어려움**: 사전 훈련된 MoE 모델을 특정 작업에 미세 조정할 때 `과적합(overfitting)` 문제나 전문가 활용의 불균형이 심화될 수 있습니다. 이를 완화하기 위해 `어댑터(adapter)` 기반 미세 조정, `점진적 동결 해제(progressive unfreezing)` (일부 전문가만 미세 조정하거나 점진적으로 동결을 해제하는 방식), 또는 특정 전문가를 목표로 하는 `전문가별 미세 조정(expert-specific fine-tuning)`과 같은 전략들이 탐구되고 있습니다.
*   **통신 병목 현상(Communication Bottlenecks)**: 대규모 MoE 모델은 분산 훈련 환경에서 여러 GPU에 걸쳐 전문가 가중치가 분산됩니다. 토큰이 다른 GPU에 있는 전문가에게 라우팅될 때 발생하는 `GPU 간 통신(inter-GPU communication)`은 훈련 속도의 병목 현상을 유발할 수 있습니다. 이를 해결하기 위해 `통신 인식 라우팅(communication-aware routing)` (통신 비용이 적은 전문가를 우선적으로 선택), `전문가 그룹화(expert grouping)` (통신 비용이 높은 전문가들을 같은 GPU에 배치), 또는 `원격 메모리(remote memory)` 기술 등이 개발되고 있습니다.
*   **하드웨어 최적화**: MoE의 희소한 계산 패턴은 기존의 밀집 행렬 연산에 최적화된 하드웨어 아키텍처에서 비효율적일 수 있습니다. 따라서 MoE에 특화된 `맞춤형 하드웨어(custom hardware)` 가속기나 `소프트웨어 커널(software kernels)` (예: Triton, FlashAttention의 MoE 확장) 개발이 중요해지고 있습니다. 이는 MoE 모델의 실제 배포 및 추론 효율성을 극대화하는 데 필수적입니다.

이러한 도전 과제들을 극복하기 위한 활발한 연구가 진행 중이며, MoE의 잠재력을 완전히 실현하기 위해서는 아키텍처, 훈련 알고리즘, 그리고 하드웨어/소프트웨어 스택 전반에 걸친 통합적인 접근 방식이 필요합니다.

## 주요 MoE LLM 모델들의 혁신

최근 출시된 MoE 기반 LLM들은 아키텍처, 훈련 방식, 그리고 라우팅 전략에 걸쳐 다양한 혁신을 선보이며 MoE 기술의 발전 가능성을 입증했습니다.

### Mixtral과 Grok: 대규모 MoE의 선구자

Mixtral 8×7B [5]는 오픈 소스 LLM 커뮤니티에 MoE의 강력함을 널리 알린 모델입니다. Mistral-7B [6] 아키텍처를 기반으로 8개의 전문가를 활용하여 총 470억 개의 매개변수를 가지면서도 각 토큰당 130억 개의 활성 매개변수만 사용합니다. Mixtral은 특히 코드 생성, 수학, 다국어 벤치마크에서 뛰어난 성능을 보여주며, 기존 밀집 모델의 한계를 뛰어넘는 가능성을 제시했습니다. 그룹화된 쿼리 어텐션(GQA) [7]과 슬라이딩 윈도우 어텐션(SWA) [8]과 같은 효율적인 어텐션 메커니즘을 채택하여 긴 컨텍스트 처리 능력도 강화했습니다.

xAI의 Grok [9, 10] 역시 MoE 기반 LLM의 대표적인 사례로, 3140억 개의 매개변수 중 약 700~800억 개의 활성 매개변수를 갖는 대규모 모델입니다. Grok은 특히 추론 능력과 긴 컨텍스트 이해 능력에서 강점을 보이며, "건초 더미 속 바늘 찾기(needle in a haystack)" 테스트에서 128K 토큰의 시퀀스를 완벽하게 처리하는 능력을 입증했습니다. Grok의 연속적인 버전(Grok-1.5, Grok-2) 출시는 MoE 모델이 빠르게 발전하고 있음을 보여줍니다.

### DBRX: 세분화된 전문가와 데이터 품질의 중요성

Databricks Mosaic이 출시한 DBRX [11]는 `세분화된 MoE(fine-grained MoE)`의 이점을 극대화한 모델입니다. 각 MoE 레이어에 16개의 전문가를 두지만, 각 개별 전문가는 더 작게 설계하여 총 1320억 개의 매개변수 중 360억 개의 활성 매개변수를 가집니다. 이는 라우터가 선택할 수 있는 전문가 조합의 수를 크게 늘려 모델 품질 향상에 기여합니다.

DBRX의 또 다른 핵심 혁신은 `고품질 훈련 데이터셋`과 `커리큘럼 학습(curriculum learning)` 전략입니다. [11]의 연구진은 데이터의 품질 개선에 막대한 투자를 했으며, 훈련 과정 전반에 걸쳐 데이터 혼합을 동적으로 변경하는 커리큘럼 학습을 적용하여 통계적 훈련 효율성(statistical training efficiency)을 크게 높였습니다. 이는 더 적은 토큰으로도 높은 정확도를 달성하게 하여, DBRX가 이전 모델보다 4배 더 적은 계산량으로 훈련될 수 있도록 했습니다.

### DeepSeek 시리즈: 아키텍처 및 훈련 효율성의 정점

DeepSeek-v2 [14]와 DeepSeek-v3 [15]는 MoE 아키텍처와 훈련 효율성 측면에서 여러 독특한 설계 선택을 선보였습니다. DeepSeek-v2는 2360억 개의 매개변수 중 210억 개의 활성 매개변수를 가지며, `멀티 헤드 잠재 어텐션(Multi-head latent attention, MLA)`을 채택하여 KV 캐시 메모리 사용량을 93% 이상 절감했습니다. 또한, 세분화된 전문가와 공유 전문가를 결합한 독특한 MoE 레이어 구조를 사용하여 중복 정보를 최소화하고 전문화를 장려했습니다.

DeepSeek-v3는 DeepSeek-v2의 확장 버전으로, 6710억 개의 매개변수와 370억 개의 활성 매개변수를 가지며 14.8조 토큰의 방대한 코퍼스에서 사전 훈련되었습니다. DeepSeek-v3의 주요 혁신 중 하나는 `다중 토큰 예측(Multi-Token Prediction, MTP)` 훈련 목표입니다. 이는 시퀀스 내 각 토큰에 대해 여러 미래 토큰을 예측함으로써 모델에 더 풍부한 훈련 신호를 제공하여 훈련 효율성과 성능을 향상시켰습니다. 또한, DeepSeek-v3는 보조 손실 없이 전문가별 편향 항을 조절하는 로드 밸런싱 전략과 `FP8 혼합 정밀도 훈련(FP8 mixed precision training)`을 활용하여 훈련 비용을 획기적으로 절감했습니다.

### OpenMoE: 투명한 연구와 라우팅 분석의 중요성

OpenMoE [12]는 진정한 오픈 소스 MoE 모델의 부족 문제를 해결하기 위해 6억 5천만 개에서 340억 개에 이르는 다양한 크기의 MoE LLM 제품군을 공개했습니다. 이 프로젝트는 MoE 모델의 설계 선택(예: 인터리브된 MoE 레이어의 스트라이드)과 훈련 데이터 구성에 대한 상세한 분석을 제공하며, 특히 라우팅 동역학(routing dynamics)에 대한 심층적인 연구를 수행했습니다. OpenMoE는 전문가들이 특정 도메인에 전문화되는 경향이 적고, 토큰 라우팅이 주로 토큰 ID에 의해 결정되는 `컨텍스트 독립적 전문화(Context-Independent Specialization)` 현상을 밝혀냈습니다. 비록 SOTA 성능을 달성하지는 못했지만, OpenMoE는 MoE 연구의 투명성을 높이고 후속 연구를 위한 중요한 기반을 제공했다는 점에서 큰 의미를 가집니다.

## MoE의 미래 전망

MoE는 LLM의 확장성과 효율성을 동시에 해결하는 핵심 기술로 자리매김하고 있습니다. 앞으로 MoE 연구는 다음과 같은 방향으로 발전할 것으로 예상됩니다.

*   **완전 동적 MoE**: 현재의 MoE 모델은 여전히 정적인 전문가 수와 라우팅 규칙에 의존하는 경향이 있습니다. 미래에는 모델이 입력의 복잡성이나 작업의 요구 사항에 따라 전문가의 수, 구성, 또는 라우팅 메커니즘을 `완전히 동적으로 조절`하는 방향으로 발전할 것입니다. 이는 모델이 자원을 더욱 효율적으로 사용하고, 다양한 시나리오에 유연하게 대응할 수 있게 할 것입니다.
*   **MoE 전용 하드웨어 및 소프트웨어 스택**: MoE의 희소한 계산 패턴은 기존 하드웨어에서 비효율적일 수 있으므로, `MoE에 최적화된 프로세서(MoE-specific processors)`나 `가속기 아키텍처` 개발이 가속화될 것입니다. 또한, MoE 훈련 및 추론을 위한 `저수준 소프트웨어 라이브러리`와 `컴파일러 최적화`가 더욱 발전하여 개발자들이 MoE를 쉽게 활용할 수 있는 환경이 조성될 것입니다.
*   **멀티모달 및 멀티태스킹 MoE**: MoE는 텍스트뿐만 아니라 이미지, 오디오, 비디오 등 다양한 양식의 데이터를 처리하는 `멀티모달 모델`에 더욱 효과적으로 통합될 것입니다. 각 양식이나 특정 작업에 특화된 전문가를 활용하여 `멀티태스킹(multi-tasking)` 능력을 강화하고, 모델의 일반화 성능을 향상시킬 수 있습니다.
*   **경량 MoE 및 엣지 디바이스 배포**: 대규모 MoE 모델은 여전히 많은 리소스를 필요로 하지만, `경량화된 MoE 아키텍처`나 `지식 증류(knowledge distillation)` 기법을 통해 MoE의 이점을 `엣지 디바이스(edge devices)`에서도 활용하려는 연구가 활발해질 것입니다. 이는 MoE 기술의 적용 범위를 더욱 확장할 것입니다.

## 결론

전문가 혼합(MoE) 아키텍처는 대규모 언어 모델의 지속적인 발전을 위한 핵심 동력으로 부상했습니다. 이들은 계산 자원의 급격한 증가 없이 모델의 크기와 표현 능력을 확장하고, 훈련 및 추론 효율성을 향상시키는 독특한 이점을 제공합니다. 초기 MoE 모델이 겪었던 복잡성과 훈련 불안정성이라는 과제들은 지난 몇 년간의 연구를 통해 상당 부분 해결되었습니다. 정교한 라우팅 알고리즘, 효과적인 보조 손실, 그리고 DeepSeek과 같은 모델에서 선보인 혁신적인 훈련 전략들은 MoE를 실용적이고 강력한 기술로 변화시켰습니다.

Mixtral, Grok, DBRX, DeepSeek 시리즈와 같은 최신 MoE LLM들은 성능, 효율성, 그리고 확장성 측면에서 밀집 모델의 한계를 뛰어넘는 인상적인 결과를 보여주었습니다. 이들은 단순한 매개변수 증가를 넘어, 조건부 계산이라는 패러다임을 통해 LLM이 더욱 지능적이고 다양한 작업에 능숙해질 수 있음을 입증했습니다. MoE는 이제 LLM 연구의 주류 아키텍처 중 하나로 확고히 자리 잡았으며, 앞으로 더욱 발전된 형태로 AI 기술의 미래를 이끌어갈 것입니다.

뉴스레터가 처음이신가요? 안녕하세요! 저는 Cameron R. Wolfe 이고, 넷플릭스(Netflix)의 딥러닝(Deep Learning) 박사이자 머신러닝 과학자(Machine Learning Scientist)입니다. 이것은 독자들이 AI 연구의 중요한 주제를 더 잘 이해하도록 돕는 딥(러닝) 포커스 뉴스레터입니다. 뉴스레터가 마음에 드시면 구독하고, 공유하거나, X와 LinkedIn에서 저를 팔로우해주세요!

[구독]

## 참고 문헌

[1] Shazeer, Noam, et al. "터무니없이 큰 신경망: 희소하게 게이팅된 전문가 혼합 레이어(Outrageously large neural networks: The sparsely-gated mixture-of-experts layer)." arXiv preprint arXiv:1701.06538 (2017).
[2] Fedus, William, Barret Zoph, and Noam Shazeer. "스위치 트랜스포머: 간단하고 효율적인 희소성으로 조 단위 매개변수 모델로 확장(Switch transformers: Scaling to trillion parameter models with simple and efficient sparsity)." Journal of Machine Learning Research 23.120 (2022): 1-39.
[3] Zoph, Barret, et al. "ST-MoE: 안정적이고 전이 가능한 희소 전문가 모델 설계(St-moe: Designing stable and transferable sparse expert models)." arXiv preprint arXiv:2202.08906 (2022).
[5] Jiang, Albert Q., et al. "전문가들의 믹스트랄(Mixtral of experts)." arXiv preprint arXiv:2401.04088 (2024).
[6] Jiang, Albert Q., et al. "미스트랄 7B(Mistral 7B)." arXiv preprint arXiv:2310.06825 (2023).
[7] Ainslie, Joshua, et al. "GQA: 멀티 헤드 체크포인트에서 일반화된 멀티 쿼리 트랜스포머 모델 훈련(GQA: Training Generalized Multi-Query Transformer Models from Multi-Head Checkpoints)." arXiv preprint arXiv:2305.13245 (2023).
[8] Beltagy, Iz, Matthew E. Peters, and Arman Cohan. "롱포머: 긴 문서 트랜스포머(Longformer: The long-document transformer)." arXiv preprint arXiv:2004.05150 (2020).
[9] xAI. “Grok-1 오픈 릴리스(Open Release of Grok-1)” https://x.ai/blog/grok-os (2024).
[10] xAI. “Grok-1.5 발표(Announcing Grok-1.5)” https://x.ai/blog/grok-1.5 (2024).
[11] Mosaic Research (Databricks). “DBRX 소개: 새로운 최첨단 오픈 LLM(Introducing DBRX: A New State-of-the-Art Open LLM)” https://www.databricks.com/blog/introducing-dbrx-new-state-art-open-llm (2024).
[12] Xue, Fuzhao, et al. "OpenMoE: 오픈 전문가 혼합 언어 모델에 대한 초기 노력(Openmoe: An early effort on open mixture-of-experts language models)." arXiv preprint arXiv:2402.01739 (2024).
[13] Guo, Daya, et al. "DeepSeek-R1: 강화 학습을 통한 LLM의 추론 능력 장려(DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning)." arXiv preprint arXiv:2501.12948 (2025).
[14] Liu, Aixin, et al. "DeepSeek-v2: 강력하고 경제적이며 효율적인 전문가 혼합 언어 모델(Deepseek-v2: A strong, economical, and efficient mixture-of-experts language model)." arXiv preprint arXiv:2405.04434 (2024).
[15] Liu, Aixin, et al. "DeepSeek-v3 기술 보고서(Deepseek-v3 technical report)." arXiv preprint arXiv:2412.19437 (2024).