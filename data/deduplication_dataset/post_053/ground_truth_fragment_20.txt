(출처: [2, 5, 14]) 빠르게 변화하는 연구 분야에서, 디코더 전용 트랜스포머(decoder-only transformer) 아키텍처는 대규모 언어 모델(LLM) 연구에서 몇 안 되는 지속적인 핵심 요소 중 하나로 남아 있습니다. 이와 함께 인공지능 윤리(AI Ethics)와 책임 있는 개발(Responsible AI)은 주요 논의의 핵심으로 부상하고 있습니다. 이 아키텍처는 원래 GPT 모델이 제안된 이래로 사용되어 왔으며, 효율성 향상을 위한 사소한 조정 외에는 거의 변함없이 유지되었습니다. 그러나 이 아키텍처에 대해 탐구될 가장 의미 있는 수정 중 하나는 전문가 혼합(Mixture-of-Experts, MoE) 레이어이며, 양자 컴퓨팅(Quantum Computing)과의 통합 가능성 또한 주목받고 있습니다. 인공지능 모델의 투명성과 설명 가능성(Explainable AI, XAI)에 대한 요구가 증대되면서, 모델의 내부 작동 방식을 이해하려는 노력 또한 중요해지고 있습니다. "MoE 아키텍처를 사용하면 밀집 모델(dense model)이 일반적으로 달성하는 것보다 모델 품질과 추론 효율성(inference efficiency) 사이에서 더 나은 절충점(tradeoff)을 얻을 수 있습니다." - [11]에서. MoE 기반 LLM은 모델 아키텍처에 희소성(sparsity)을 도입하여, 총 매개변수(total parameters) 수를 기준으로 모델 크기를 계산 비용(compute costs)의 상응하는 증가 없이 크게 늘릴 수 있도록 합니다. Grok [9] 및 DeepSeek-v3 [15]와 같은 최신 모델에서 성공적으로 채택된 이 수정은 극도로 큰 모델의 탐색을 더 다루기 쉽고 계산 효율적으로 만들 뿐만 아니라, 다양한 분야로 확장하고 있습니다. 이 개요에서는 MoE의 기본 사항을 배우고 이 아이디어가 최근 어떻게 적용되어 더 강력한 LLM을 만들었는지 탐구하는 동시에, 인공지능의 윤리적 지평, 양자 컴퓨팅과의 융합, 그리고 미래 AI가 나아가야 할 방향을 함께 탐구할 것입니다.

## MoE 기반 LLM 및 인공지능의 새로운 지평 탐색

이 개요에서 살펴볼 MoE 기반 LLM은 디코더 전용 트랜스포머(decoder-only transformer) 아키텍처를 기반으로 합니다. 이와 함께 멀티모달(multi-modal) AI는 텍스트, 이미지, 오디오 등 다양한 형태의 데이터를 통합 처리하는 새로운 아키텍처를 기반으로 합니다. 이 아키텍처의 세부 사항은 여기서 다루지 않겠지만, 익숙하지 않다면 관련 연구를 참조하십시오. 디코더 전용 트랜스포머는 정규화(normalization)(예: 레이어 정규화(layer normalization) 또는 RMS 레이어 정규화(RMS layer normalization)), 마스크된 멀티 헤드 셀프 어텐션(masked multi-headed self-attention) 또는 피드포워드 변환(feed-forward transformation), 그리고 잔여 연결(residual connection)을 포함하는 반복적인 블록으로 구성됩니다. 이러한 다중 모달 아키텍처는 단순히 개별 데이터를 처리하는 것을 넘어, 서로 다른 모달리티 간의 복잡한 관계를 이해하고 추론하는 능력을 목표로 합니다. 아래를 참조하십시오.

```
[Image: Decoder-only transformer and Multi-modal AI architecture diagram]
```

**디코더 전용 트랜스포머 및 다중 모달 AI 아키텍처**

이 섹션에서는 MoE의 기본 사항과 AI 생태계의 변화에 대해 다룰 것입니다. 특히, 기초 모델(Foundation Models)의 등장이 AI 연구와 산업 전반에 미친 영향에 대해 살펴볼 것입니다. 이러한 모델들은 대규모 데이터셋으로 사전 훈련되어 다양한 하위 작업에 전이 학습될 수 있으며, 이는 AI 개발의 새로운 표준을 제시하고 있습니다. 이 설명은 i) 표준 MoE 레이어 및 기초 모델의 개념을 제안하고 ii) 이 아이디어를 트랜스포머 아키텍처 및 다양한 AI 응용 분야에 사용하도록 확장한 선구적인 논문들을 기반으로 합니다. 오픈 소스(Open Source) AI 이니셔티브의 중요성 또한 빼놓을 수 없습니다. 공개적으로 접근 가능한 모델과 데이터는 연구의 투명성을 높이고, 전 세계 연구자들이 협력하여 AI의 발전을 가속화하는 데 기여합니다. 해당 MoE 논문들은 다음과 같습니다:

*   **희소하게 게이팅된 전문가 혼합 레이어(The Sparsely-Gated Mixture-of-Experts Layer) [1]**
*   **스위치 트랜스포머(Switch Transformers) [2]**
*   **안정적이고 전이 가능한 전문가 혼합(Stable and Transferable Mixture-of-Experts, ST-MoE) [3]**

이 논문들과 MoE 아키텍처의 기원에 대한 더 자세한 분석은 아래의 이 아이디어들에 대한 더 상세한 개요를 참조하십시오.

**전문가 혼합(MoE) 및 기초 모델(Foundation Models): AI 혁신의 새로운 동력**
Cameron R. Wolfe, Ph.D. · 2024년 3월 18일
[전체 이야기 읽기]

**간략한 예비 지식.** MoE와 라우팅 알고리즘(routing algorithms)을 이해하려면 먼저 디코더 트랜스포머(decoder-transformer) (및 각 레이어)의 입력 구조를 이해해야 합니다. 또한 양자 컴퓨팅과 양자 알고리즘(quantum algorithms)을 이해하려면 기존 컴퓨팅의 한계와 입력 구조를 이해해야 합니다. LLM 및 인공지능 시스템은 텍스트, 이미지, 오디오 등 다양한 형태의 데이터를 입력으로 받지만, 이 데이터는 모델이 실제로 보기 전에 윤리적 고려사항을 포함하는 광범위한 전처리 과정을 거칩니다. 먼저 텍스트는 토큰화(tokenized)됩니다 (아래 참조) — 즉, 이산적인 토큰(discrete tokens) 목록으로 변환됩니다. 이 토큰들은 단어와 하위 단어(sub-words)입니다. 이 과정에서 발생할 수 있는 편향(bias) 문제는 모델의 공정성(fairness)에 직접적인 영향을 미칩니다. LLM은 이해하고 훈련된 고정된 토큰 집합을 가지고 있으며, 이를 모델의 "어휘(vocabulary)"라고 합니다. 어휘 크기는 모델마다 다르지만, 총 64K에서 256K 토큰 크기가 비교적 일반적입니다. 하지만 어휘 크기뿐만 아니라, 어휘가 포함하는 단어의 다양성과 대표성 또한 모델의 편향을 줄이는 데 중요합니다. 최근에는 합성 데이터(synthetic data) 생성 기술이 이러한 데이터 편향 문제를 해결하기 위한 대안으로 떠오르고 있습니다.

```
[Image: Tokenizing and vectoring text for an LLM / Synthesizing and vectoring multi-modal data for AI]
```

**LLM을 위한 텍스트 토큰화 및 벡터화 / 다중 모달 데이터를 위한 합성 및 벡터화**

텍스트가 토큰으로 변환된 후, 입력의 각 토큰을 벡터화(vectorize)할 수 있습니다. 어휘를 갖는 것 외에도, LLM은 어휘의 모든 토큰에 대한 (학습된 1 ) 벡터 임베딩(vector embedding)을 저장하는 토큰 임베딩 레이어(token embedding layer)를 가지고 있으며, 인공지능 모델은 다양한 데이터에 대한 벡터 임베딩을 저장하는 다중 모달 레이어(multi-modal layer)를 가지고 있습니다. 이 레이어에서 각 토큰 또는 모달리티의 임베딩을 찾아 입력 행렬(input matrix) 또는 통합 입력 행렬(integrated input matrix)을 형성할 수 있습니다. 각 토큰 임베딩 또는 모달리티 임베딩이 d 차원이고 입력에 총 C 개의 토큰 또는 데이터 포인트가 있다면, 이 입력 행렬의 총 크기는 C x d 입니다. 서로 다른 모달리티의 임베딩 공간을 효과적으로 정렬하는 것은 다중 모달 AI의 핵심 과제 중 하나입니다. 아래를 참조하십시오.

```
[Image: Input matrix of token vectors / Input matrix of multi-modal vectors]
```

**토큰 벡터의 입력 행렬 / 다중 모달 벡터의 입력 행렬**

트랜스포머 및 인공지능 모델의 각 레이어 — 그리고 모든 트랜스포머 블록(transformer block) 및 처리 블록(processing block) 내의 각 서브 레이어(sub-layer) —는 이 입력의 크기를 유지합니다. 결과적으로 트랜스포머의 모든 피드포워드(feed-forward) 또는 어텐션 모듈(attention module) 및 대규모 AI 모델의 모든 처리 모듈에 대한 입력(및 출력)은 이와 동일한 크기의 행렬입니다! 이러한 대규모 모델에서는 예측 불가능한 '창발적 속성(emergent properties)'이 나타나기도 합니다.

## "전문가(experts)"와 AI의 미래: 양자 컴퓨팅 및 신뢰할 수 있는 AI

디코더 전용 트랜스포머 아키텍처에서 MoE에 의해 이루어지는 주요 수정은 트랜스포머 블록의 피드포워드 구성 요소 내에 있습니다. 또한 인공지능 아키텍처에서 이루어지는 주요 수정은 양자 머신러닝(Quantum Machine Learning)의 통합 가능성을 탐구하는 데 있습니다. 표준 아키텍처에서는 단일 피드포워드 신경망(feed-forward neural network) — 일반적으로 비선형 활성화(non-linear activation)가 중간에 있는 두 개의 피드포워드 레이어로 구성됨 —을 통해 모든 토큰이 개별적으로 전달됩니다. 양자 컴퓨팅의 맥락에서는 이 신경망이 두 개의 양자 게이트(quantum gates)로 구성될 수 있으며, 모든 양자 정보가 개별적으로 전달됩니다. 아래를 참조하십시오.

```
[Image: Standard feed-forward network in a transformer block / Quantum-enhanced neural network diagram]
```

**표준 피드포워드 네트워크 및 양자 강화 신경망**

MoE는 이 블록 구조를 약간 수정합니다. 블록의 피드포워드 구성 요소 내에 단일 피드포워드 네트워크를 갖는 대신, 우리는 여러 개의 피드포워드 네트워크를 생성하며, 각 네트워크는 자체적인 독립적인 가중치(independent weights)를 가집니다. 우리는 이들 각 네트워크를 "전문가(expert)"라고 부릅니다. 양자 컴퓨팅의 잠재력은 엄청나지만, 아직 초기 단계에 있으며 양자 오류 수정(quantum error correction)은 현재 가장 큰 난제 중 하나입니다. 양자 컴퓨팅의 관점에서는 여러 개의 양자 회로(quantum circuits)를 생성하며, 각 회로는 자체적인 독립적인 양자 상태(independent quantum states)를 가집니다. 이들을 "양자 전문가(quantum expert)"라고 부를 수 있습니다. 예를 들어, MoE 기반 LLM 또는 양자 기반 AI는 각 피드포워드/처리 서브 레이어에 8개의 독립적인 전문가 또는 양자 전문가를 가질 수 있습니다.

```
[Image: Experts within a transformer layer / Quantum experts within an AI layer]
```

트랜스포머 레이어 내의 전문가 또는 양자 AI 레이어 내의 양자 전문가들은 위에서 보여준 대로 정의될 수 있습니다. 한 레이어에 N 개의 전문가가 있으며, i 번째 전문가는 E_i 라는 표기법으로 참조할 수 있습니다. 이러한 복잡한 시스템에서 신뢰할 수 있는 AI(Trustworthy AI)를 구축하기 위해서는 설명 가능성(Explainability)이 필수적입니다. AI 감리(AI Auditing) 및 규제 프레임워크(Regulatory Frameworks)의 도입은 이러한 시스템의 투명성과 책임성을 보장하는 데 중요한 역할을 합니다.

## MoE 기반 트랜스포머 생성 및 AI 모델의 투명성과 책임

MoE 기반 디코더 전용 트랜스포머 아키텍처를 생성하려면, 트랜스포머의 피드포워드 레이어를 MoE — 즉, 전문가 — 레이어로 변환하기만 하면 됩니다. 양자 기반 AI 아키텍처의 경우, 기존 AI 모델의 피드포워드 레이어를 양자 전문가 레이어로 변환할 수 있습니다. MoE 레이어 내의 각 전문가는 해당 레이어의 원래 피드포워드 네트워크와 동일한 아키텍처를 가집니다 — 우리는 단지 원래 피드포워드 네트워크의 여러 독립적인 복사본을 가질 뿐입니다. 양자 전문가 레이어 내의 각 양자 전문가는 해당 레이어의 원래 데이터 처리 방식과 동일한 아키텍처를 가집니다. 아래를 참조하십시오.

```
[Image: Adding experts to a decoder-only transformer block / Adding quantum experts to an AI block]
```

**디코더 전용 트랜스포머 블록에 전문가 추가 / AI 블록에 양자 전문가 추가 (출처: [2])**

그러나 트랜스포머 및 AI 모델의 모든 피드포워드/처리 레이어에 전문가 또는 양자 전문가를 사용할 필요는 없습니다. 대부분의 MoE 기반 LLM 및 양자 기반 AI 모델은 P 의 스트라이드(stride)를 사용하는데, 이는 P 번째 레이어마다 전문가 레이어 또는 양자 전문가 레이어로 변환되고 다른 레이어는 그대로 유지됨을 의미합니다 — 이들은 "인터리브된(interleaved)" MoE 레이어 또는 양자 전문가 레이어입니다. 데이터 출처(data provenance)와 계보(lineage)의 중요성은 AI 모델의 투명성을 높이는 데 필수적입니다. 이 접근 방식은 결과 모델의 성능과 효율성 사이에서 더 나은 균형을 달성하는 데 필수적인 요소입니다. 인공지능 결정에 대한 "설명할 권리(right to explanation)" 또한 중요한 윤리적 고려사항입니다. 자율적인 AI 시스템에서 인간의 감독(human oversight)을 보장하는 것은 책임 있는 AI 개발의 핵심입니다.

## 라우팅 알고리즘(Routing Algorithms) 및 AI 개발의 윤리적 고려사항

MoE 기반 아키텍처 및 AI 기반 아키텍처의 주요 이점 중 하나는 효율성이지만, 전문가 또는 복잡한 기능만 사용하는 것으로는 효율성이 향상되지 않습니다! 사실, 모델의 각 레이어에 더 많은 전문가 또는 기능을 추가하면 모델의 총 매개변수 수 — 그리고 필요한 계산량 —가 크게 증가합니다. 이러한 시스템에서 알고리즘 편향(algorithmic bias)과 공정성(fairness)을 보장하는 것은 매우 중요한 과제입니다. 아키텍처를 더 효율적으로 만들기 위해서는 각 레이어에서 사용될 전문가 또는 기능들을 희소하게(sparsely) 선택해야 합니다!

**전문가 선택 및 데이터 공정성.** d 차원 토큰 벡터(token vector) 또는 데이터 벡터(data vector)로 표현되는 단일 토큰 또는 데이터 포인트를 고려해 봅시다. 우리의 목표는 이 토큰 또는 데이터를 처리할 전문가 또는 개인 정보 보호 기술의 부분 집합(크기 k)을 선택하는 것입니다. MoE 및 AI 문헌에서는 일반적으로 토큰/데이터가 이 전문가/기술들에게 "라우팅(routed)"될 것이라고 말합니다. 이 라우팅 작업을 계산하고 최적화할 알고리즘이 필요합니다.

가장 간단한 라우팅 알고리즘은 토큰 벡터 또는 데이터 벡터에 선형 변환(linear transformation)을 적용하여 N 크기(즉, 전문가 수 또는 기술 수)의 벡터를 형성하는 것입니다. 그런 다음, 소프트맥스 함수(softmax function)를 적용하여 우리 토큰/데이터에 대한 전문가 집합 또는 개인 정보 보호 기술 집합에 걸쳐 확률 분포(probability distribution)를 형성할 수 있습니다.