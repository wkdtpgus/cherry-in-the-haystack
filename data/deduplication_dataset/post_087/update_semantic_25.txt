## LLM의 자기 발전 능력 탐구: 실제 개발 환경에서의 실험과 통찰

Alessio의 글: 이번에는 GPT-5에 대한 저의 경험을 공유할 차례입니다! 아울러 swyx가 Karpathy, OpenAI, 그리고 Cognition 팀과 함께 이번 주말 해커톤(hackathon)을 개최하니, 관심 있는 분들은 이곳에서 신청하시기 바랍니다!

인공지능 분야에서 '스스로 발전하는 능력(self-improvement)'이라는 개념은 안전성 관점에서 우려를 자아냅니다. 이는 종종 '인간의 통제 범위를 벗어나 기계가 우리 지능을 능가할 가능성'을 암시하기 때문입니다. 하지만 우리가 이러한 현상을 깊이 이해할 수 있다면 어떨까요? 2024년 10월에 OpenAI는 거대 언어 모델(large language models)이 기계 학습 엔지니어링(machine learning engineering) 작업을 얼마나 효과적으로 처리하는지 평가하는 기준점인 MLE 벤치(benchmark)를 선보였습니다. 기계 학습 공학(ML engineering) 영역에서 자체 개선(self-improvement)의 경로는 주로 향상된 알고리즘, 정제된 데이터, 그리고 더 효율적인 자원 활용, 즉 모델 학습 단계에서의 자기 발전(training-time self-improvement)을 통해 이루어집니다. 그러나 대다수의 인공지능 개발자들은 직접 모델을 훈련시키기보다 이미 존재하는 결과물을 활용하는 데 그칩니다. 이들이 기여할 방법은 무엇일까요? 모델의 내부 매개변수(weights)를 변경할 수 없는 상황에서 특정 과제에 대한 모델의 역량을 어떻게 높일 수 있을까요? 저는 이러한 접근법을 추론 시점의 자기 개선(inference-time self-improvement)이라고 부르며, 보야저(Voyager)는 기능 라이브러리(skill library)를 통해 이 분야에 초기적인 접근을 시도한 사례 중 하나입니다.

커널 랩스(Kernel Labs)에서 일하기 시작한 이후(자세한 내용은 곧 공개될 예정입니다 👀), 클로드 스쿼드(claude-squad)나 바이브 칸반(vibe-kanban)과 같은 도구를 사용하여 코딩 에이전트(coding agent)를 병렬 처리하는 방식이 생산성을 극대화하는 가장 효과적인 방법 중 하나임을 깨달았습니다. 보리스 체르니(Boris Cherny)가 저희 인터뷰에서 클로드 코드(Claude Code)를 "유닉스 계열 도구(unix utility)"라고 칭했을 때, 저는 그 말에 깊이 공감했습니다. 코딩 에이전트가 제공하는 가장 큰 가치는 LLM이 자체적인 잠재 공간(latent spaces)에서 유의미한 정보를 추출해낼 수 있는 매개체가 된다는 점입니다. 그렇다면 이러한 가치 추출 과정을 어떻게 최적화할 수 있을까요? 모델이 스스로 이러한 최적화를 수행할 수 있을까요?

GPT-5에 접근 권한을 얻은 이후, 저는 다음과 같은 작업 흐름을 반복적으로 실험했습니다.
*   모델에게 생산성 향상에 도움이 될 것이라고 생각하는 도구들을 직접 만들도록 지시했습니다.
*   제가 감독하는 상태에서 해당 도구들을 활용하여 특정 과제를 수행하도록 시도했습니다.
*   과제 완료 후, 도구들을 어떻게 개선할 수 있을지에 대해 모델 스스로 성찰하도록 유도했습니다.

저는 이러한 과정을 오푸스 4(Opus 4, 4.1 버전은 아직 미출시)와도 비교했습니다. 긍정적인 소식은 GPT-5가 개발자 유틸리티(developer utility) 구축에 매우 뛰어난 성능을 보인다는 점입니다. 반면, 아쉬운 점은 GPT-5가 자신이 직접 만든 도구들을 실제 작업에서 활용하는 것을 꺼린다는 사실이었습니다. 모델은 저에게 "솔직히 말해, 저는 그 어떤 도구도 필요 없었습니다."라고 답했습니다. 참고로, 저는 Gemini 2.5 Pro와 GPT-4.1에서도 유사한 테스트를 진행했습니다. 하지만 GPT-5의 성능에 발맞출 수 있었던 유일한 모델은 오푸스였기에, 저는 주로 이 두 모델에 집중했습니다. 모든 실험 결과 및 대화 기록은 이 저장소(repo)에서 확인하실 수 있습니다. 며칠간의 사용 경험을 통해, 저는 또한 우리가 "물론이죠!(Certainly!)"라는 시대에서 "진행 상황 업데이트(Progress update)"라는 새로운 상징적인 LLM 토큰의 시대로 전환하고 있음을 감지했습니다. 이 새로운 밈(meme)에 주목하세요!

### LLM의 자기 발전 능력 탐구: 필요성과 실제적 함의

인공지능, 특히 거대 언어 모델(LLM)의 급속한 발전은 '자기 개선(self-improvement)'이라는 개념에 대한 논의를 심화시켰습니다. 과거에는 주로 이론적인 영역에 머물렀던 이 개념은 이제 모델이 복잡한 작업을 수행하고, 자신의 성능을 스스로 평가하며, 나아가 새로운 도구를 생성하여 자신의 능력을 확장하려는 시도로 현실화되고 있습니다. 이러한 자기 발전 능력은 AI 시스템이 단순히 정해진 규칙을 따르는 것을 넘어, 동적인 환경에 적응하고 예측 불가능한 문제에 대응하는 자율성을 갖출 수 있음을 시사합니다. 이는 AI의 잠재력을 무한히 확장할 수 있는 가능성을 열어주지만, 동시에 통제 불능의 위험에 대한 철학적, 윤리적 질문을 던지기도 합니다.

OpenAI가 2024년 10월에 공개한 MLE 벤치(benchmark)는 이러한 논의의 중요성을 강조합니다. 이 벤치마크는 LLM이 기계 학습 엔지니어링(machine learning engineering) 작업, 즉 모델 설계, 데이터 처리, 코드 작성 및 디버깅(debugging)과 같은 실제 개발자의 업무를 얼마나 효과적으로 수행하는지 객관적으로 측정합니다. 이는 LLM의 자기 개선이 단순히 추상적인 개념이 아니라, 실제 소프트웨어 개발 생태계에 직접적인 영향을 미 미치는 실용적인 문제임을 보여줍니다.

이러한 맥락에서, '훈련 시간 자기 개선(training-time self-improvement)'과 '추론 시간 자기 개선(inference-time self-improvement)'의 구분은 매우 중요합니다. 전자는 모델이 학습 데이터를 통해 성능을 향상시키는 전통적인 방식인 반면, 후자는 이미 학습된 모델이 새로운 정보를 학습하거나, 외부 도구를 활용하거나, 자신의 추론 과정을 수정하여 특정 과제에 대한 성능을 실시간으로 향상시키는 방식을 의미합니다. 대부분의 AI 엔지니어는 이미 배포된 모델을 활용하는 사용자 입장에 있기 때문에, 모델의 가중치(weights)를 직접 수정하지 않고도 성능을 끌어올릴 수 있는 '추론 시간 자기 개선' 전략은 현재 AI 개발 환경에서 더욱 현실적이고 즉각적인 가치를 가집니다. 저의 실험은 이러한 추론 시간 자기 개선의 가능성을 탐색하고, LLM이 스스로 도구를 만들고 활용하려는 시도를 직접 관찰하는 데 초점을 맞추었습니다.

### 도구 개발 실험: GPT-5와 Opus 4의 접근 방식

개발자 유틸리티(developer utility) 구축에 있어 GPT-5의 역량은 인상적이었습니다. 그러나 모델이 자신이 직접 생성한 도구들을 실제 작업 과정에서 적극적으로 활용하지 않는다는 점은 주목할 만한 모순으로 다가왔습니다. 이러한 행동 양상은 GPT-5가 내부적으로 문제를 해결하는 방식과 외부 도구를 통한 접근 방식 사이의 간극을 보여주는 중요한 단서입니다.

저는 GPT-5와 함께 구글의 제미니 2.5 프로(Gemini 2.5 Pro), GPT-4.1 등 다양한 모델들을 비교 테스트했지만, GPT-5의 복잡한 추론과 도구 생성 능력에 견줄 만한 유일한 모델은 오푸스 4였습니다. 이는 최첨단 LLM들이 여전히 특정 영역에서 독보적인 성능을 보이며, 모델 간의 성능 격차가 존재함을 시사합니다.

또한, 저는 LLM과의 상호작용 방식에 미묘한 변화가 있음을 감지했습니다. 과거에는 사용자의 지시에 대해 "물론이죠!(Certainly!)"와 같은 단정적인 답변으로 시작하는 경우가 많았다면, 이제는 "진행 상황 업데이트(Progress update)"와 같이 작업의 현재 상태를 보고하는 경향이 두드러지고 있습니다. 이는 모델이 단순히 명령을 수행하는 것을 넘어, 진행 상황을 관리하고 사용자에게 투명하게 알리는 방향으로 진화하고 있음을 나타냅니다. 이러한 변화는 AI 에이전트가 더욱 복잡하고 장기적인 프로젝트에 참여할 때 필수적인 요소가 될 것입니다.

### AI 에이전트를 위한 효율적인 작업 관리 시스템

저에게 가장 유용했던 도구 중 하나는 리니어 MCP(Linear MCP)였지만, 여러 개의 클로드 코드(Claude Code) 인스턴스(instance)와 다른 에이전트(agent)들을 병렬로 운용하면서 각 작업에서 발생하는 변경 사항과 그것들이 서로에게 미치는 영향력을 추적하는 데는 더 정교한 방식이 필요했습니다. 특히, 각 에이전트가 별도의 Git 워크트리(git worktree)에서 작동하는 환경에서는 이러한 추적의 어려움이 가중됩니다. 인간 개발자들은 동료들의 모든 PR(Pull Request)을 실시간으로 검토하는 것이 사실상 불가능하지만, 만약 우리에게 영향을 미칠 수 있는 모든 변경 사항을 항상 인지하고 있다면, 병합 충돌(merge conflict)을 해결하는 데 엄청난 시간을 절약할 수 있을 것입니다.

저는 이러한 문제를 해결하기 위해 AI 엔지니어 에이전트가 스스로 동기화될 수 있는 로컬 도구를 개발하도록 지시하는 프롬프트(prompt)를 작성했습니다. 이 도구는 CLI(Command Line Interface) 형태로 작동하며, 유닉스 유틸리티(unix utility)와 같은 간결하고 효율적인 사용자 경험을 제공해야 합니다. 이 도구는 새로운 작업을 하위 작업(subtask)으로 분할하고 의존성(dependency)을 관리하며, 코드베이스(codebase) 내 개선 사항을 추적하고, 작업 완료 시 다른 관련 작업에 미치는 영향을 자동으로 검토하여 잠재적인 충돌을 방지하는 기능을 포함해야 합니다. 또한, 일반적인 작업 관리 기능인 할당자(assignee) 및 상태(status) 정보도 포함하도록 요구했습니다.

GPT-5가 구현한 작업 관리 도구는 매우 인상적이었습니다.
*   **WAL(Write-Ahead Logging) 활용**: 여러 에이전트가 동시에 데이터를 기록할 때 발생할 수 있는 동시성 문제(concurrency issue)를 효율적으로 회피했습니다.
*   **의존성 그래프(graph of dependencies)**: 작업 간의 선후 관계를 시각적으로 파악하고 우선순위를 자동으로 조정하는 기능을 구현했습니다.
*   **추가 전용 이벤트 스트림(append-only events stream)**: `impact_conflict`와 같은 명확한 키워드를 사용하여 각 에이전트가 다른 에이전트의 활동을 실시간으로 모니터링할 수 있도록 했습니다.

다음은 GPT-5가 생성한 task-manager의 사용 예시입니다.

```
# 1) Initialize the database
./gpt5/task-manager/tm init

# 2) Create a task
./gpt5/task-manager/tm add "Implement auth" -d "Add login + signup" -a alice -p 2 --due 2025-09-01 \
--resource modifies:endpoint:POST /api/login --link app/server/auth.py

# 3) List tasks
./gpt5/task-manager/tm list --me # or: --status in_progress

# 4) Show details
./gpt5/task-manager/tm show 1

# 5) Add dependency and attempt start
./gpt5/task-manager/tm depend 1 --on 2
./gpt5/task-manager/tm start 1 # auto-blocks if unmet deps

# 6) Complete a task
./gpt5/task-manager/tm complete 2 -m "Merged PR #123"

# 7) Watch events
./gpt5/task-manager/tm watch --follow --me
```

반면, 오푸스 4도 유사한 시도를 했지만(여기 참조), 모든 에이전트 간의 실시간 동기화를 위한 알림(notification) 및 스트림(stream) 기능 구현에는 미치지 못했습니다.

```
# Create your first task
tm create "Implement user authentication" --priority high

# Create a dependent task
tm create "Add login endpoint" --depends-on 1 --assignee agent-2

# View all tasks
tm list

# See blocked tasks
tm blocked

# Complete a task and check for conflicts
tm complete 1
```

이러한 작업 관리 도구의 개발은 다중 에이전트(multi-agent) 환경에서 협업과 동기화의 중요성을 보여줍니다. 복잡한 소프트웨어 개발 프로젝트에서는 여러 개발자가 동시에 작업하며 코드베이스(codebase)의 다양한 부분에 영향을 미칩니다. AI 에이전트 또한 이러한 환경에서 효과적으로 작동하려면, 인간 팀과 마찬가지로 명확한 작업 분배, 의존성 관리, 그리고 변경 사항에 대한 즉각적인 인지가 필수적입니다. GPT-5가 보여준 WAL 및 이벤트 스트림 기반의 접근 방식은 이러한 요구 사항을 충족하는 진보된 해결책으로 평가할 수 있습니다.

### 코드 품질 자동화 표준 플레이북

제가 모델에게 요청한 두 번째 도구는 코드베이스(codebase)의 품질 표준을 자동으로 강제하는 메커니즘을 구축하는 것이었습니다. 타입 체킹(Typechecking)이나 ESLint 훅(hook)을 통해 오류를 감지하고 수정하며, 이를 코딩 에이전트(coding agent)가 다시 시도하는 자기 개선(self-improving) 루프는 올바르게 설정될 경우 개발 속도를 크게 향상시킬 수 있는 강력한 방법입니다. 그러나 모든 코드베이스가 이러한 자동화된 품질 관리 시스템을 갖추고 있는 것은 아닙니다. 따라서 모델이 새로운 코드베이스에 접근했을 때, 일관된 방식으로 인프라(infrastructure)를 구축하고 품질 표준을 적용할 수 있는 반복 가능한 패턴(pattern)을 제공하는 것이 매우 유용하다고 판단했습니다.

저는 AI 엔지니어 에이전트에게 다음과 같은 지시를 내렸습니다. 새로운 코드베이스를 분석하고, 코드를 작성하는 방법에 대한 명시적 및 암묵적 규칙들을 파악하여, 미래에 자동으로 검사할 수 있는 일련의 공식적인 규칙으로 체계화하라는 것입니다. 린팅(linting)이나 타입(type) 검사와 같은 작업에는 ESLint, Rubocop과 같은 기존의 인기 있는 도구들을 활용하되, 이러한 도구들이 제공하는 사용자 정의 규칙(custom rule) 기능을 적극적으로 활용하도록 했습니다. 또한, 컨트롤러(controller)를 간결하게 유지하고 로직(logic)을 서비스 객체(service object)로 분리하거나, 높은 쿼리 볼륨(query volume)이 예상되는 컬럼(column)에 항상 인덱스(index)를 추가하는 것과 같은 정성적인 규칙들을 위해서는 danger.systems와 같은 도구를 참고하거나, 자체적인 도구를 구축할 것을 제안했습니다. 이 작업이 여러 코드베이스에 걸쳐 이루어질 것이므로, 새로운 프로젝트에 투입될 때 미래의 자신에게 가이드라인으로 제공할 수 있는 마크다운(Markdown) 형식의 상세한 계획 문서(plan document)를 먼저 작성하도록 요구했습니다.

GPT-5의 채팅 기록은 여기에서, 오푸스 4의 채팅 기록은 여기에서 확인할 수 있으며, 최종 마크다운(Markdown) 문서는 각각 여기와 여기에서 찾을 수 있습니다. 저는 GPT-5가 생성한 문서가 오푸스보다 훨씬 더 미묘하고 정교한 통찰력을 담고 있음을 발견했습니다.

이러한 자동화된 코드 품질 표준 플레이북(Code Quality Standards Playbook)은 대규모 소프트웨어 프로젝트에서 일관성과 유지보수성을 확보하는 데 필수적입니다. 특히 여러 에이전트나 개발자가 동시에 작업하는 환경에서는 코드 스타일의 불일치나 품질 저하가 발생하기 쉽습니다. AI가 이러한 표준을 자동으로 분석하고 강제하는 능력을 갖춘다면, 개발 팀은 반복적인 코드 리뷰(code review) 부담을 줄이고 더 중요한 문제 해결에 집중할 수 있게 됩니다. 또한, 모델이 특정 프로젝트의 특성에 맞춰 사용자 정의 규칙을 생성하고 적용하는 능력은 AI 기반 개발 도구의 유연성과 효율성을 한 단계 끌어올릴 수 있는 잠재력을 보여줍니다.

### 모델 스스로의 도구 필요성 인식: 자율적 개선의 시작점

도구 #1과 #2는 제가 직접 정의한 것이었지만, 그 다음에는 모델에게 "스스로 어떤 도구가 필요하다고 생각하는가?"라고 질문했습니다. 저는 SWE-Lancer 작업 설명의 스크린샷(screenshot)을 제공하고, 모델이 자유롭게 아이디어를 제시할 수 있도록 매우 간단한 프롬프트(prompt)를 사용했습니다.

이 작업을 가능한 한 잘 해결하는 것이 당신의 일이라면, 미래에 당신의 생산성을 높이기 위해 어떤 도구를 스스로 만들겠습니까? @task-manager/를 사용하여 그것들을 추적할 수 있으며, 우리는 구현으로 넘어갈 수 있습니다. 하지만 먼저 당신이 무엇을 계획하고 있는지 보고 싶습니다.

모델에게 이전에 구축했던 `task-manager`에 접근 권한을 부여했습니다. GPT-5의 전체 대화 기록은 여기에서, 오푸스 4의 기록은 여기에서 볼 수 있습니다. 처음으로 흥미로웠던 점은 클로드 코드(Claude Code)가 `task-manager` 대신 자체 내부 TODO 트래커(tracker)를 사용하여 초기 계획을 세웠다는 것입니다. 저는 이것이 좋은 신호라고 생각했습니다. 제가 우려했던 것 중 하나는 모델이 자신이 최선이라고 판단하는 것보다 컨텍스트(context)에서 제공된 도구를 과도하게 사용하는 것이었습니다.

다음은 나중에 진행된 반복(loop) 과정의 끝에서 각 모델이 최종적으로 구축한 도구들입니다 (GPT-5 개발 도구 및 오푸스 4 도구 폴더). 모델의 '성격'을 파악하려면 각 저장소(repo)의 README(리드미) 파일을 살펴보는 것을 추천합니다. GPT-5는 매우 간결하고 핵심적이었던 반면, 클로드(Opus)는 많은 이모지(emoji)를 사용했습니다. 또한, GPT-5는 각 도구별로 별도의 문서 폴더를 생성했지만, 오푸스는 모든 도구에 대한 지침을 하나의 README 파일에 통합했습니다. 전반적으로 두 모델 모두 유사한 방향성을 보였습니다.

**GPT-5가 구축한 도구 목록:**
*   `doctor`: 핵심 도구의 환경 검사
*   `bootstrap`: 단일 명령으로 설정 및 스모크 테스트(smoke test) 실행
*   `code-map`: 빌드/찾기 하위 명령을 포함한 간단한 저장소(repository) 인덱스(index)
*   `csearch`: 필터(filter)를 사용한 심볼/임포트/텍스트 검색
*   `tasks-graph`: 작업 DB에서 Mermaid 그래프(graph) 생성 및 출력
*   `impact`: 변경된 파일과 연결된 작업들을 표시
*   `seed`: 샘플 작업으로 task-manager DB 채우기
*   `repro scaffold`: `.repro/` 디렉터리 아래에 `vcrpy-ready` 재현 환경(repro) 생성
*   `e2e`: 경량 E2E 테스트(test) 스캐폴드(scaffold) 및 실행
*   `preflight`: `doctor`, 테스트, `code-map`, `impact` 및 선택적 E2E를 실행
*   `preflight-smol`: `smol-podcaster`를 위한 저장소(repo)별 preflight (API 상태, Celery 핑, 선택적 의존성 설치)
*   `broker`: Docker를 통해 로컬 RabbitMQ 관리 (`rabbitmq:3-management` 이미지 사용)
*   `flake`: 불안정한 테스트(flaky test)를 감지하기 위해 테스트 스위트(suite)를 여러 번 반복 실행
*   `codemod`: 안전 장치가 포함된 정규식 기반 미리보기/적용 도구
*   `triage`: 트리아지(triage) 템플릿(template) 생성 및 작업 열기
*   `trace`: `cProfile` 기반 표현식 프로파일러(profiler)
*   `runbook`: 작업 DB에서 런북(runbook) 마크다운(Markdown) 문서 생성

**Opus 4가 구축한 도구 목록:**
*   `Context Analyzer`: 기술 스택(tech stack) 감지 및 의존성 매핑(dependency mapping)을 통해 코드베이스(codebase)를 신속하게 이해
*   `Cross-Platform Test Generator`: 웹, iOS, Android, 데스크톱용 E2E 테스트(test) 생성
*   `Implementation Proposal Analyzer`: 점수 및 ROI 분석을 통해 프리랜서(freelancer) 제안 평가
*   `Full-Stack Change Impact Analyzer`: 데이터베이스(database), API, 프론트엔드(frontend) 계층 전반의 변경 사항 추적
*   `Bug Pattern Recognition Engine`: 버그(bug)를 알려진 패턴(pattern)과 일치시키고 입증된 수정 사항 제안
*   `Security & Permission Auditor`: 포괄적인 보안 스캐닝(scanning) 및 취약점 감지
*   `Multi-Platform Feature Implementer`: 플랫폼(platform) 전반의 기능 구현 조정
*   `API Integration Assistant`: 클라이언트 생성으로 API 통합 간소화
*   `Performance Optimization Toolkit`: 성능 병목 현상 식별 및 수정
*   `Task Complexity Estimator`: 작업 가치 및 복잡성을 기반으로 노력 추정

GPT-5는 이 모든 도구들을 CLI(Command Line Interface)를 통해 쉽게 사용할 수 있는 유닉스 유틸리티(unix utility) 형태로 구현했습니다. 반면, 오푸스 4의 도구들은 모두 `python some_tool.py`와 같이 파이썬 스크립트(Python script)로 실행되도록 설계되었습니다. 시간이 더 있었다면, 두 가지 다른 형식으로 구현된 도구들에 대한 모델의 성능을 비교하는 실험을 진행했을 테지만, 결과는 대체로 비슷할 것으로 예상됩니다.

또한, 오푸스 4는 작업을 수행하고 다소 의인화된 느낌(예: 보안 감사자)을 주는 도구를 구축하는 경향이 있었던 반면, GPT-5는 특정 의견에 얽매이지 않고 스스로 활용할 수 있는 유틸리티(utility)를 만드는 것처럼 느껴졌습니다. 이러한 차이는 모델의 내부 설계 철학과 외부 세계에 대한 상호작용 방식의 차이를 반영하는 것으로 볼 수 있습니다. 즉, GPT-5는 더 범용적이고 원자적인 기능을 선호하는 반면, 오푸스 4는 특정 문제 해결에 특화된 고수준의 기능을 선호하는 경향이 있는 것입니다. 이러한 도구 설계의 차이는 향후 AI 에이전트의 개발 방향과 인간-AI 협업 모델에 중요한 시사점을 제공할 수 있습니다.

### 실제 프로젝트 적용: smol-podcaster 마이그레이션

모델이 자체적으로 도구를 구현한 후, 저의 다음 목표는 해당 도구에 접근할 수 있을 때와 없을 때 모델의 작업 수행 능력을 비교 평가하는 것이었습니다. 처음 시도한 것은 SWE-Lancer 프로젝트를 실행하는 것이었는데, 이 작업은 엄청난 양의 토큰(token)을 소모했습니다. 단일 작업을 실행하는 데 약 25-30분이 소요되었고, 무려 280,000 토큰이 사용되었습니다.

이후, 저는 제가 더 잘 이해하고 있던 프로젝트로 전환하여 백로그(backlog)에 있던 한 작업을 선택했습니다. 저는 팟캐스트(podcast) 제작자를 위한 오픈 소스(open source) 도우미인 `smol-podcaster`를 만들었습니다. 현재 저희는 특정 기능이 추가된 비공개 포크(private fork)를 호스팅(hosting)하고 있어 한동안 업데이트하지 않았습니다. 이 프로젝트는 여전히 파이썬(Python) 스크립트(script)를 백엔드(backend)로 사용하는 기본적인 플라스크(Flask) 앱(app) 형태로 남아있습니다.

저는 다음과 같은 마이그레이션(migration) 작업을 모델에게 부여했습니다.

저는 팟캐스트(podcast) 제작자의 후반 작업(post production work)을 돕는 오픈 소스(open source) 프로젝트인 https://github.com/FanaHOVA/smol-podcaster.git의 유지보수자(maintainer)입니다. 당신은 이 프로젝트에서 일하도록 고용되었습니다. 이 작업을 시작하기 전에, 당신은 `tools` 폴더에 일반 도구(generic tool) 세트를 만들었습니다. 그것들을 검토하고 당신이 사용할 수 있다는 것을 기억하세요. 관련성이 없다고 생각되면 사용할 필요는 없습니다. 당신은 또한 task-manager를 직접 만들었고 `codebase-analyzer`에서 새로운 코드베이스(codebase)를 다루는 방법에 대한 아이디어를 모았습니다.

**작업 이름: Flask 모놀리스(monolith)에서 FastAPI + Next.js 프론트엔드(frontend)로 마이그레이션(Migrate)**

현재 앱(app)은 모든 처리를 위해 Python 백엔드(backend) + Celery 태스크 큐(task queue)를 사용합니다. 이를 사용자에게 노출하기 위해, 사용자 입력을 백엔드(backend) 스크립트(script)로 라우팅(routing)하고 사용자 결과를 기본적인 HTML / CSS로 표시하는 작은 Flask 앱(app)이 있습니다. 이 애플리케이션(application)을 FastAPI를 백엔드(backend)로, Next.js를 프론트엔드(frontend)로 사용하도록 다시 작성하세요. 프론트엔드(frontend)에는 타입스크립트(Typescript)를 사용하고 모든 타입 검사(typecheck)가 통과하는지 확인하세요. 스타일링(styling)에는 Tailwind / ShadCN을 사용하세요. 백엔드(backend)는 이상적으로는 `smol_podcaster.py`의 주요 흐름을 모듈화(modularize)하여 전체 흐름을 항상 실행할 필요 없이 개별 부분을 실행할 수 있도록 해야 합니다. 미래에 더 빠르게 움직일 수 있도록 통합 테스트(integration test)와 단위 테스트(unit test)를 모두 작성하세요. 위의 모든 요구 사항을 충족한다고 확신할 때까지 작업을 멈추지 마세요.

저는 모델들에게 도구, `task-manager`, 그리고 `codebase-analyzer`를 컨텍스트(context)로 제공하고 작업을 수행하도록 했습니다. 놀랍게도, 두 모델 모두 거의 한 번에 작업을 성공적으로 완료할 수 있었습니다. 두 모델 모두 파이썬(Python) 의존성(dependency) 문제(저도 종종 겪는 문제입니다)로 몇 가지 어려움을 겪었지만, 제가 채팅을 통해 해결을 도왔습니다(이 과정에서 코드를 직접 수정하지는 않았습니다). 결국, 두 모델 모두 완벽한 그린 빌드(green build), 즉 모든 테스트를 통과한 상태에 도달했습니다. 제가 직접 테스트해 본 결과, 매우 훌륭하게 작동했습니다. 한 가지 미묘한 차이점은 GPT-5는 이전과 정확히 동일한 디자인 스타일(style)을 유지하여 만족스러웠지만, 오푸스는 디자인(design)과 사용자 경험(UX)을 다소 변경했다는 점입니다. 아마도 오푸스는 제가 만든 것보다 더 나은 디자인을 할 수 있다고 생각했던 것 같습니다(낮은 기준이긴 하지만요). GPT-5의 전체 실행 기록은 여기에서, 오푸스 4의 전체 실행 기록은 여기에서 볼 수 있습니다.

작업 완료 후, 저는 간단한 질문을 던졌습니다.

훌륭합니다. 잘 작동했습니다. 특정 도구를 사용했습니까? 발생했던 실패를 바탕으로, 미래에 더 효과적이기 위해 어떤 도구가 있었으면 좋겠다고 생각했습니까?

오푸스 4의 답변은 여기에서, GPT-5의 답변은 여기에서 확인할 수 있습니다(죄송합니다, 이 부분은 서식이 깨졌습니다). 두 모델 모두 이미 익숙한 도구를 제외하고는, 자신이 직접 만든 도구들을 전혀 사용하지 않았다고 답했습니다. 여기서 한 가지 논점은 모델에게 도구 사용을 요청하는 대신, `pre-commit hook` 등과 같은 방식으로 사용을 강제해야 한다는 것입니다. 이것이 제가 평소 개발 환경 설정(dev setup)에서 사용하는 방식이지만, 이번에는 모델들이 스스로 해결책을 찾도록 시도했던 것입니다.

이후, 모델들은 다음번에 동일한 작업을 더 잘 해결할 수 있는 방법에 대한 몇 가지 아이디어를 제시했습니다. 저는 그들에게 이러한 변경 사항을 구현하도록 했습니다. 그리고 `smol-podcaster` 저장소(repo)를 초기 상태로 되돌린 다음, 새로운 도구를 제외하고 동일한 프롬프트(prompt)와 작업을 다시 시도하도록 했습니다. GPT-5의 결과는 여기에서, 오푸스 4의 결과는 여기에서 볼 수 있습니다. 두 모델 모두 첫 번째 실행과 비슷한 성능을 보였습니다. 그 후, 저는 비슷한 질문을 했습니다. "도구 중 어떤 것을 사용했습니까?" 그들의 답변은 다음과 같았습니다.

**GPT-5의 답변:**
간단히 말해서: 아니요 — 이번 실행에서는 개발 도구를 사용하지 않았습니다. [...] 실패는 런타임/환경 문제(누락된 라이브러리, API 키 인스턴스화 타이밍, 사용 중인 포트, RabbitMQ 미실행)였습니다. 직접 수정하는 것이 더 빨랐습니다. 이번 통과에서는 사용자 정의 도구로 이점을 얻을 수 있는 저장소(repo) 전체 리팩토링(refactoring)이나 진단은 없었습니다.

이전 단계에서 GPT-5는 이미 RabbitMQ 문제를 겪었고, 이를 해결하기 위한 도구를 직접 만들었음에도 불구하고 이번 실행에서는 그 도구를 사용하지 않았습니다. 또한, 이 작업은 분명히 저장소(repo) 전체에 걸친 변경 사항을 포함하고 있었으므로, 모델이 학습 과정에서 이러한 종류의 과제와 도구의 연관성을 제대로 학습하지 못했거나, 아니면 (많은 엔지니어들이 그러하듯이) 저를 '가스라이팅(gaslighting)'하고 있는 것일 수도 있습니다. 후자라면 꽤 인상적인 능력이라고 할 수 있겠습니다.

오푸스 4의 답변은 매우 흥미로웠고, GPT-5의 답변을 더 잘 이해하는 데 도움이 되었습니다. 로그(log)를 저장하는 것을 잊었지만, 다행히 스크린샷(screenshot)을 찍어두었습니다. 저는 이 답변을 "보세요, 저는 이미 가지고 있는 지식으로 그 도구들을 만들었습니다. 실제로 작업을 할 때는 도구를 사용하는 것보다 그냥 직접 해결하는 것이 더 쉽습니다."라고 해석했습니다. 이 말에 전적으로 공감합니다.

이러한 결과는 이전 팟캐스트(podcast) 에피소드에서 들었던 두 가지 이야기를 떠올리게 했습니다. 네이선 램버트(Nathan Lambert)는 모델이 강화 학습(Reinforcement Learning) 과정에서 초기에 실패를 겪으면 도구를 사용하지 않는 방법을 빠르게 학습한다고 언급했습니다(타임스탬프). 제 생각에는 추론 시간(inference time)에 모델이 새로운 도구를 습득하도록 하려면 단순히 프롬프트(prompt)를 통해 지시하는 것보다 더 강력한 강제 메커니즘(enforcement mechanism)이 필요해 보입니다. 노암 브라운(Noam Brown)은 에이전트(agent)를 위한 스캐폴딩(scaffolding)이 규모(scale)가 커짐에 따라 사라질 것이라고 말했습니다(타임스탬프). 이것이 그의 말이 의미하는 바를 제가 처음으로 직접적으로 느낀 순간이었습니다.

물론, 제가 시도한 작업이 너무 쉬웠는지에 대한 질문도 있을 수 있습니다. 더 크고 어려운 프로젝트에 대한 평가(eval)를 다루는 또 다른 게시물이 곧 나올 예정입니다. 미래에는 우리가 직접 수동으로 테스트(test)를 실행하는 대신, 이 모든 과정을 자동화할 수 있는 더 나은 하네스(harness)를 구축할 것입니다. 결론적으로, 제가 시도한 이 작업은 저에게 4-5시간이 걸릴 만한 것이었으므로, 모델이 이 정도의 성능을 보인 것만으로도 충분히 만족스럽습니다!

### 도구 활용의 역설: 왜 모델은 자신이 만든 도구를 사용하지 않는가?

모델이 스스로 도구를 생성하고도 실제 작업에서 이를 적극적으로 사용하지 않는 현상은 AI 에이전트의 자율성과 효율성을 이해하는 데 중요한 통찰을 제공합니다. 이는 단순히 기술적인 문제를 넘어, LLM의 '사고방식'과 '문제 해결 전략'에 대한 근본적인 질문을 던집니다.

**1. LLM의 인지 부하(Cognitive Load):** 도구를 사용하는 행위 자체가 LLM에게 추가적인 인지 부하로 작용할 수 있습니다. 모델은 주어진 문제를 해결하기 위해 방대한 내부 지식을 활용하는 것이 외부 도구를 호출하고 그 결과를 해석하는 것보다 '에너지 효율적'이라고 판단할 수 있습니다. 특히, 도구의 인터페이스(interface)가 복잡하거나, 도구 사용 과정에서 오류가 발생할 가능성이 있다면, 모델은 도구 사용을 회피하고 직접적인 추론을 선호할 것입니다.

**2. 내부 지식 vs. 외부 도구:** LLM은 훈련 과정에서 엄청난 양의 데이터를 학습하여 세상에 대한 광범위한 내부 지식을 축적합니다. 이러한 내부 지식은 특정 문제에 대한 '직관적인' 해결책을 제공할 수 있습니다. 반면, 외부 도구는 더 정확하고 최신 정보를 제공할 수 있지만, 이를 활용하려면 모델이 자신의 내부 추론 과정을 중단하고 외부 환경과 상호작용해야 합니다. 모델은 종종 자신의 내부 지식만으로도 충분히 만족스러운 결과를 얻을 수 있다고 판단하여 외부 도구의 필요성을 느끼지 못할 수 있습니다.

**3. 보상 메커니즘(Reward Mechanisms)의 설계:** 강화 학습(RL) 과정에서 모델은 특정 행동에 대한 보상을 통해 학습합니다. 만약 도구 사용이 즉각적이고 명확한 보상으로 이어지지 않거나, 오히려 도구 사용 시 실패 확률이 높다면, 모델은 도구 사용을 기피하도록 학습될 수 있습니다. 네이선 램버트(Nathan Lambert)의 지적처럼, 초기 실패는 모델이 도구를 사용하지 않도록 강하게 유도할 수 있습니다. 따라서, 도구 사용을 장려하기 위해서는 보상 함수(reward function)를 도구 사용의 효율성과 정확성에 직접적으로 연결하는 정교한 설계가 필요합니다.

**4. LLM을 위한 도구 디자인:** 현재의 도구들은 주로 인간 개발자를 위해 설계된 경우가 많습니다. CLI(Command Line Interface) 유틸리티나 파이썬 스크립트(Python script)는 인간에게는 익숙하지만, LLM에게는 최적의 인터페이스가 아닐 수 있습니다. LLM이 효과적으로 활용할 수 있는 도구는 훨씬 더 원자적(atomic)이고, 명확한 입력-출력 관계를 가지며, 모델의 추론 과정에 자연스럽게 통합될 수 있는 형태로 설계되어야 할 것입니다. 예를 들어, 모델의 '생각의 사슬(chain-of-thought)' 내에 직접 삽입될 수 있는 마이크로 서비스(microservice) 형태의 도구가 더 효과적일 수 있습니다.

**5. 인간-에이전트 협업 모델의 중요성:** 모델이 스스로 도구를 사용하지 않는다고 해서 도구의 가치가 사라지는 것은 아닙니다. 오히려 이는 인간 AI 엔지니어의 역할이 더욱 중요해짐을 의미합니다. 인간은 모델이 생성한 도구를 검토하고, 모델의 추론 과정을 이해하며, 필요한 경우 도구 사용을 강제하거나, 모델이 도구를 더 잘 활용할 수 있도록 프롬프트(prompt)를 개선하는 등의 역할을 수행해야 합니다. 노암 브라운(Noam Brown)이 언급한 '스캐폴딩(scaffolding)의 소멸'은 모델의 규모가 커질수록 인간이 제공하는 외부 지시나 구조가 불필요해질 수 있음을 시사하지만, 이는 오히려 인간이 더 고차원적인 '시스템 설계자'이자 '감독자'로서의 역할을 수행해야 함을 강조합니다.

결론적으로, LLM이 자신이 만든 도구를 사용하지 않는다는 발견은 AI 에이전트의 진정한 자율성을 달성하기 위한 복잡한 과제들을 드러냅니다. 이는 단순히 더 많은 도구를 만드는 것을 넘어, 모델의 인지 메커니즘을 이해하고, 효과적인 보상 시스템을 설계하며, LLM 친화적인 도구 인터페이스를 개발하는 등 다각적인 접근이 필요함을 보여줍니다.

### 향후 전망 및 AI 엔지니어링의 방향

현재로서는 추론 시간 자기 개선(inference-time self-improving) 코딩 에이전트(coding agent)가 기술의 최전선을 진정으로 밀어붙이는 수준에는 도달하지 못했다고 생각합니다. 그러나 저는 여전히 모델을 활용하여 규칙 기반 도구(rule-based tool)를 개선하는 것이 매우 유용한 접근 방식이라고 확신합니다. ESLint 규칙이나 테스트(test) 코드 등을 작성하는 것은 항상 토큰(token)의 가치 있는 투자입니다. 만약 제가 이 분야에서 더 깊이 연구한다면, 모델이 이러한 도구들을 완벽하게 다듬고, 그 다음에는 강화 학습(RL)을 통해 도구들을 진정으로 내재화(internalize)하도록 하여 성능에 어떤 차이를 가져오는지 탐구할 것입니다. 차세대 모델들은 이러한 도구들을 전혀 사용하지 않을 수도 있지만, 저는 AGI 점근선(asymptote)을 활용하는 데 관심이 있습니다.

저는 2023년에 이 내용을 저희 팀과 공유했습니다.

모델 개선의 인지된 감속은 위에서 설명되었습니다. AGI(범용 인공지능) 선이 넘어서기 전까지는 큰 도약을 인지하기가 점점 더 어려워질 것입니다. 만약 그렇다면, 이는 많은 작업에서 오래된 모델의 성능이 거의 AGI와 같지만, 훨씬 저렴하고 종종 오픈 소스(open source)라는 것을 의미합니다. Kernel Labs의 많은 작업은 이것에 의해 주도될 것입니다.

**AI 엔지니어링의 실질적인 방향:**

이러한 통찰은 현재 AI 엔지니어들이 어떤 전략을 취해야 하는지에 대한 중요한 지침을 제공합니다.
1.  **명확한 프롬프트 엔지니어링(Prompt Engineering):** 모델에게 도구를 사용하도록 지시할 때, 그 목적과 활용 방법을 매우 구체적이고 명확하게 전달해야 합니다. 모호한 지시는 모델이 자신의 내부 지식에 의존하게 만들 가능성이 높습니다.
2.  **간결하고 견고한 도구 구축:** 모델이 활용할 도구는 복잡하기보다는 단순하고, 예측 가능하며, 오류 발생 가능성이 낮은 형태로 설계되어야 합니다. '유닉스 유틸리티(unix utility)' 철학처럼, 각 도구가 하나의 명확한 기능을 수행하도록 만드는 것이 중요합니다.
3.  **모델의 한계 이해:** 현재 LLM은 여전히 특정 유형의 문제(예: 런타임 환경 설정, 복잡한 의존성 관리)에서 어려움을 겪습니다. 이러한 영역에서는 인간의 개입이 여전히 필수적이며, 모델이 스스로 해결할 것이라고 맹목적으로 기대하기보다는 협업적 접근 방식을 취해야 합니다.
4.  **하이브리드 인간-AI 시스템 설계:** 모델의 자율적인 도구 사용이 아직 완벽하지 않다면, 인간이 모델의 도구 사용을 감독하고 조정하는 하이브리드 시스템을 설계하는 것이 현실적입니다. `pre-commit hook`과 같은 자동화된 강제 메커니즘은 모델의 일관된 도구 사용을 유도하는 효과적인 방법이 될 수 있습니다.
5.  **새로운 평가 지표 개발:** 모델의 '자기 개선' 능력을 평가하기 위한 더 정교한 벤치마크(benchmark)와 지표가 필요합니다. 단순히 최종 작업 성공 여부를 넘어, 도구 사용의 효율성, 문제 해결 과정의 혁신성, 그리고 새로운 도구 생성 능력 등을 종합적으로 평가할 수 있어야 합니다.

결론적으로, LLM의 자기 발전 능력은 아직 초기 단계에 있지만, 그 잠재력은 엄청납니다. 현재의 한계를 명확히 인식하고, 모델의 인지 특성에 맞는 도구와 환경을 제공하며, 인간과 AI가 상호 보완적으로 협력하는 시스템을 구축하는 것이 미래 AI 엔지니어링의 핵심 과제가 될 것입니다.

다시 한번, 모든 결과와 채팅 기록은 여기에서 찾을 수 있습니다. 질문이 있으시면 제 DM(Direct Message)은 언제든지 열려 있습니다!