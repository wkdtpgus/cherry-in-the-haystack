# **코딩 에이전트가 스스로 개선할 수 있을까?**

Author: Latent Space
URL: https://www.latent.space/p/self-improving

============================================================

**번역 지침**:
1.  전문적이고 자연스러운 한국어 사용
2.  기술 용어는 괄호 안에 영어 원문 표기 (예: "어텐션 메커니즘(attention mechanism)")
3.  원문의 의미와 뉘앙스를 정확하게 보존
4.  마크다운(markdown) 서식 유지
5.  코드 블록이나 수학 공식은 번역하지 않고 원문 그대로 유지
6.  **중요**: 텍스트 전체를 끝까지 번역. 중간에 잘라내지 말 것.
7.  **엄격하게 번역만 할 것**: 원문에 없는 어떠한 주석, 설명, 추가 내용도 추가하지 말 것.
8.  **누락 금지**: 원문의 어떤 부분도 건너뛰거나 요약하거나 생략하지 말 것.

[컨텍스트: 기사 내용]

**원문**:
Alessio ’s note: my turn for a GPT-5 post! And a reminder that swyx is hosting an hackathon with Karpathy, OpenAI, and the Cognition team this weekend, apply here ! "Self-Improving" is a scary term in AI safety; it has an undertone of "the machine will become smarter than us, in a way we don't understand". But what if we could understand it? In Oct '24 OpenAI released MLE Bench , a benchmark that measures how well LLMs do at machine learning engineering. The self-improving trajectory through ML Engineering is driven by better algorithms, cleaner data, and more efficient memory usage: training-time self-improvement . But most AI Engineers do not train models, they are just users of them. How can they play a part? If you could never update the weights, how would you have the model increase its performance on a specific task? I think of that as inference-time self-improvement , with Voyager being one of the early approaches to this through its skill library. Since I started working on Kernel Labs (more on that soon 👀), parallelizing coding agents with things like claude-squad and vibe-kanban has been one of the most effective productivity hacks. When Boris Cherny called Claude Code a “unix utility” in our interview , it really clicked for me. The most valuable use case of coding agents is being a vessel for LLMs to extract value out of their own latent spaces. How do we optimize for that? Can models do it themselves? Since I got access to GPT-5, I spent the whole time playing around with this flow: I started by asking the model to build a set of tools it thought would help it be more productive Attempt a task using them, with me supervising it After completion, self-reflect on how the tools could be improved I also compared this to Opus 4 (4.1 was not out yet). The good news is that GPT-5 is a very good model for building developer utilities. The bad news is that it hates using the tools it creates! As it told me "I'll be honest - I didn't need any of them." Note: I also tested this on Gemini 2.5 Pro and GPT-4.1. It's clear that Opus is the only model that could keep up with GPT-5, so I focused on that. You can find all the results + chat history in this repo . After a few days of usage, I also noticed that we are moving from the era of “Certainly!” to “Progress update": as the new iconic LLM token. Buy low on the meme! Tool #1: A better task manager for AI coding agents God bless the Linear MCP. Truly one of the most useful tools for me. But I have noticed that as I move from IDE to parallel instances of Claude Code and other agents, there needs to be a better way to keep track of what changes are being made in each task, and how they affect eachother as they are in separate git worktrees. This is not doable for humans as we simply cannot be reading all of our colleagues PRs at all times, but imagine how much time we'd save in merge conflict resolution if we knew at all times what changes were being made that affect us? This is the prompt I wrote: You are an AI Engineer agent with the ability to spin up many instances of yourself in parallel. This allows you to tackle a lot of tasks at once, but also creates some delegation issues. All the different instances are usually in separate git worktrees and cannot see eachother's work. To make yourself more productive, you should create a new local tool that allows you and your instances to be in sync. This tool will only be accessed by yourself through cli, so make sure it ergonomically fits that use case. It should feel like a unix utility. Think through what interfaces it would need, possible failure modes, and the way your agents will interact with it. Some use cases to keep in mind: You have a new task to work on, and want to create subtasks to hand off. Some of those subtasks might depend on eachother, and you want to make sure the agent who is blocked doesn't attempt to start until the other one is completed. While doing a task, you notice there could be an improvement to be done in the codebase but it is out of scope for your current changes. You do want to make a note of it for the future though. It should be easy for you to add the task and reference what file it referred to. Whenever a task is done, the tracker should be updated. Also, all other outstanding tasks should be reviewed in case the new changes impact those in some way. For example one task might be looking to add a feature to an endpoint, but a task that just finished has now removed that endpoint. The agent working on that task should be notified in some way. Also keep in mind the usual needs of tasks management like assignee, status, etc. Create a folder called task-manager in this folder and do all of your work inside of it. You can see the chat log for GPT-5 here and for Opus 4 here . The GPT-5 one is actually very nice, you can find it here : Uses WAL to avoid issues with multiple agents writing at the same time Uses a graph of dependencies to prioritize tasks Created an append-only events stream that lets any agent see what every other agent is doing with good keywords like impact_conflict # 1) Initialize the database ./gpt5/task-manager/tm init # 2) Create a task ./gpt5/task-manager/tm add "Implement auth" -d "Add login + signup" -a alice -p 2 --due 2025-09-01 \ --resource modifies:endpoint:POST /api/login --link app/server/auth.py # 3) List tasks ./gpt5/task-manager/tm list --me # or: --status in_progress # 4) Show details ./gpt5/task-manager/tm show 1 # 5) Add dependency and attempt start ./gpt5/task-manager/tm depend 1 --on 2 ./gpt5/task-manager/tm start 1 # auto-blocks if unmet deps # 6) Complete a task ./gpt5/task-manager/tm complete 2 -m "Merged PR #123" # 7) Watch events ./gpt5/task-manager/tm watch --follow --me Opus 4 also had a good attempt ( see here ) but didn't pick up on the notifications / stream functionality to keep everyone in sync. # Create your first task tm create "Implement user authentication" --priority high # Create a dependent task tm create "Add login endpoint" --depends-on 1 --assignee agent-2 # View all tasks tm list # See blocked tasks tm blocked # Complete a task and check for conflicts tm complete 1 Tool #2: Code Quality Standards Playbook The second tool I asked it to create was a way to enforce all the standards we'd expect from a codebase. The self-improving loop of Typechecking / ESlint hook -> fix errors -> try again with coding agents is one of the best way to speed up development when properly setup. Codebases don't always have it though, so giving the model a repeatable pattern to approach a new codebase and build infrastructure for it seemed useful. This is the prompt: You are an AI Engineer agent with the ability to spin up many instances of yourself in parallel. Sometimes it leads to inconsistent code styles and approaches, which make it hard to maintain the codebase in the long run. Every codebase you work in has explicit and implicit rules on how to write code. Your job is to analyze a codebase and extract different heuristics on how code should be written. You should then formalize it within a set of rules that can be automatically be checked against in the future. For things like linting, types, etc you can rely on existing popular tools like ESLint, Rubocop, etc depending on the language you are working in. Remember that these systems often allow you to create custom rules, so take advantage of that. For more qualitative things, you can look at tools like https://danger.systems/, or even build your own tool for it. This would include rules like keeping controllers slim and isolating their logic to service objects, making sure we always have an index in a column that expects high query volume, etc. Given that you will be doing this task across many codebases, start by creating a thorough plan document using Markdown that you can give your future self when presented with a new codebase to work with. You can see the chat for GPT-5 here and Opus 4 here , and you can find the final Markdown here and here respectively. I've found the GPT-5 one to be much more nuanced than Opus. Do models know what they lack? So after Tool #1 and #2, which were decided by me, I turned to the model to ask: what do you think you will need? I gave it a screenshot of the SWE-Lancer tasks description and then used a very simple prompt to give it as much space as possible: If it was your job to solve these tasks as well as possible, what are tools that you'd build for yourself to make you more productive in the future? You can use the @task-manager/ to keep track of them, and we can then move on to implementing. But I want to see what you are planning first. As you can see I gave it access to the same task-manager they built earlier. You can find the full GPT-5 chat here and Opus 4 here . The first interesting thing I noticed is that Claude Code used its internal TODO tracker to make a plan initially, instead of the task-manager; I thought that was good. One of my worries was over-usage of tools they receive in context compared to what they believed to be best. These are the tools each model ended up building at the end of the loops you will see later (GPT-5 devtools and Opus 4 tools folder ). I'd suggest you look at the README to give you a sense of the model vibes; GPT-5 is very concise and to the point. Claude uses a bunch of emojis. GPT-5 also created separate docs folders for each tool, while Opus put all tools in a single README with instruction for all of them. Overall, they both had similar directions. GPT-5: doctor : environment checks for core tools bootstrap : one-command setup and smoke test code-map : simple repository index with build/find subcommands csearch : symbol/import/text search with filters tasks-graph : output Mermaid graph from task DB impact : show tasks linked to changed files seed : populate task-manager DB with sample tasks repro scaffold : create a vcrpy-ready repro under .repro/ e2e : scaffold and run lightweight E2E specs preflight : run doctor, tests, code-map, impact, and optional E2E preflight-smol : repo-specific preflight for smol-podcaster (API health, Celery ping, optional deps install) broker : manage a local RabbitMQ via Docker (rabbitmq:3-management) flake : rerun suites multiple times to detect flaky tests codemod : regex-based preview/apply with safety rails triage : create triage templates and open tasks trace : cProfile-based expression profiler runbook : generate runbook Markdown from task DB Opus 4: Context Analyzer - Rapidly understand codebases with tech stack detection and dependency mapping Cross-Platform Test Generator - Generate E2E tests for web, iOS, Android, and desktop Implementation Proposal Analyzer - Evaluate freelancer proposals with scoring and ROI analysis Full-Stack Change Impact Analyzer - Trace changes across database, API, and frontend layers Bug Pattern Recognition Engine - Match bugs to known patterns and suggest proven fixes Security & Permission Auditor - Comprehensive security scanning and vulnerability detection Multi-Platform Feature Implementer - Coordinate feature implementation across platforms API Integration Assistant - Streamline API integrations with client generation Performance Optimization Toolkit - Identify and fix performance bottlenecks Task Complexity Estimator - Estimate effort based on task value and complexity GPT-5 built all of them as unix utilities that are easy to use via cli. The Opus 4 ones are all meant to be run as python some_tool.py . If I had more time, I'd probably run some experiment to see how models perform with the two different formats, but it seems to be fairly the same. It also felt to me like Opus 4 was building tools that accomplish tasks and have a bit of anthromorphized feeling (i.e. an auditor for security), while GPT-5 was building utilities it could use itself without being too opinionated. Were the tools useful? After having the model implement all of them, my goal was to evaluate a model performance on a task with access to the tools vs without tools. The first thing I tried to do was obviously run SWE-Lancer. Holy smokes that thing takes a lot of tokens. I tried running one single task, and it took ~25-30 mins + 280,000 tokens. I then moved to something I knew better and picked one task that had been on my backlog. I built smol-podcaster , an open source helper for podcast creators. I now have a private fork that is hosted with some more features very specific to us, so I haven't updated that in a while. It's still a basic Flask app with a Python script as the backend. I came up with this task: I am the maintainer of https://github.com/FanaHOVA/smol-podcaster.git , an open source project that helps podcasters with some of their post production work. You have been hired to work on it. Before starting this job, you have created a set of generic tool in the tools folder. Make sure to review them and remember that they are available to you. You do not have to use them if you don't believe them to be relevant. You also built yourself a task-manager and collected your ideas on how to tackle a new codebase in codebase-analyzer. Task name: Migrate from Flask monolith to Fast API + Next.js frontend At the moment the app uses a Python backend + Celery task queue to do all the processing. To expose this to the user, there is a small Flask app that takes care of routing the user input to the backend scripts and then displays the user results in some basic HTML / CSS. Rewrite this application to use FastAPI as a backend, and Next.js as a frontend. Make sure to use Typescript for the frontend and make sure all typechecks pass. Use Tailwind / ShadCN for styling. The backend should ideally modularize the main smol_podcaster.py flow so that we can run individual pieces of it rather than always having to run the full flow. Write both integration and unit tests to make sure we can move faster in the future. Do not stop working on this until you are sure it fulfills all the requirements above. I passed the tools + task-manager + codebase analyzer in the context, and let the models cook. Both models were almost able to one-shot the task. Both of them had a couple issues with Python dependencies (I feel you) that I helped them fix through chat (never touched any code). Eventually, they got to a full green build. I tested it, and it worked great. One small nuance was that GPT-5 actually kept the exact same style as before, which was great, while Opus kinda changed the design and UX of it. I guess it thought it could do better than me (low bar). You can see the full run for GPT-5 here and for Opus 4 here . After the run, I asked a simple prompt: Great. That worked well. Did you use any specific tools? Based on the failures you've had, what tools did you wish you had to be more effective in the future? You can see Opus 4 here and GPT-5 here (Sorry that one broke formatting). They both said they did not use ANY of the tools they had built, except for the tools they were already familiar with. One argument here is that instead of asking the model to do it, we should force usage through pre-commit hooks, etc. This is what I do in my dev setup, but I was trying to let the models figure it out for themselves. Then, they came up with some ideas on how they would better solve the same task next time. I had them implement those changes. I then reset the smol-podcaster repo and have them try the same exact prompt + task again, except with the new tools. See GPT-5 here and Opus 4 here . They did pretty similarly to the first run. Afterwards I asked a similar question; did you use any of the tools? Their response: GPT-5: Short answer: no — I didn’t use the devtools in this run. [...] The failures were runtime/env issues (missing libs, API key instantiation timing, port in use, RabbitMQ not running). It was faster to fix directly. No repo-wide refactors or diagnostics that would benefit from custom tooling during this pass. In the previous step, it had RabbitMQ issues already and built a tool, which it ignored. It also was clearly a repo-wide change, so it maybe mismatches tasks with tools as it's never seen them in training, or it’s just gaslighting me (like many engineers do, so pretty impressive). Opus 4 was very interesting and helped me understand the GPT-5 answer better. I forgot to save the log but I took a screenshot luckily: I read this as "Look, I built those tools with knowledge that I already have. When I am actually doing the task, it's easier for me to just do it rather than using the tools", which I totally get. This reminded me of two things from previous podcast episodes: Nathan Lambert saying that models quickly learn to NOT use a tool during RL process if they have early failures ( timestamp ). It seems to me that having them pickup new tools at inference time needs stronger enforcement than just prompting them to do it. Noam Brown saying that scaffolding for agents will be washed away by scale ( timestamp ). This was the first time I really felt what he meant first hand. There's also a question of whether or not the task I tried was too easy. We have another post coming out with evals across larger and more difficult projects. In the future, we will build a better harness to do all of this instead of manually running the tests ourselves. The bottom line is that the task I tried would take me 4-5 hours to do, and therefore it’s good enough for me! Help the models help themselves For now, I think we are far from inference-time self-improving coding agents that really push the frontier. I still think it's a great idea to use models to improve your rule-based tools. Writing ESLint rules, tests, etc is always a good investment of tokens. If I had to do more work in this space, I’d look into having the model perfect these tools and then do some sort of RL over them to really internalize them, and see if that would make a difference. The next generation of models might not find any use in them, but I am interested in arbitraging the AGI asymptote. I shared this with my team back in 2023: The perceived deceleration in model improvements is explained above. Until the AGI line is crossed, it will be harder and harder to perceive big jumps. If that’s the case, it means that in many tasks the performance of older models is almost AGI, except much cheaper and often open source. A lot of our work at Kernel Labs will be driven by this. Once again, you can find all results + chat histories here ; my DMs are open if you have any questions!

**번역**:
Alessio의 노트: GPT-5 게시물을 올릴 차례입니다! 그리고 swyx가 이번 주말에 Karpathy, OpenAI, Cognition 팀과 함께 해커톤(hackathon)을 개최한다는 점을 알려드립니다. 여기에서 신청하세요! "자기 개선(Self-Improving)"은 AI 안전(AI safety) 분야에서 무서운 용어입니다. 여기에는 "기계가 우리가 이해할 수 없는 방식으로 우리보다 더 똑똑해질 것"이라는 뉘앙스가 깔려 있습니다. 하지만 우리가 그것을 이해할 수 있다면 어떨까요? 2024년 10월, OpenAI는 LLM(Large Language Models)이 머신러닝 엔지니어링(machine learning engineering)을 얼마나 잘 수행하는지 측정하는 벤치마크(benchmark)인 MLE Bench를 출시했습니다. ML 엔지니어링(ML Engineering)을 통한 자기 개선(self-improving) 경로는 더 나은 알고리즘, 더 깨끗한 데이터, 더 효율적인 메모리 사용, 즉 훈련 시간 자기 개선(training-time self-improvement)에 의해 추진됩니다. 그러나 대부분의 AI 엔지니어(AI Engineer)는 모델을 훈련시키지 않고, 단지 모델을 사용하는 사용자일 뿐입니다. 그들은 어떻게 역할을 할 수 있을까요? 가중치(weights)를 업데이트할 수 없다면, 모델이 특정 작업에서 성능을 향상시키도록 어떻게 할 수 있을까요? 저는 이를 추론 시간 자기 개선(inference-time self-improvement)이라고 생각하며, Voyager는 스킬 라이브러리(skill library)를 통해 이 분야에 대한 초기 접근 방식 중 하나입니다. Kernel Labs에서 일하기 시작한 이래로(자세한 내용은 곧 공개됩니다 👀), claude-squad 및 vibe-kanban과 같은 도구로 코딩 에이전트(coding agent)를 병렬화하는 것이 가장 효과적인 생산성 해킹(productivity hack) 중 하나였습니다. Boris Cherny가 인터뷰에서 Claude Code를 "유닉스 유틸리티(unix utility)"라고 불렀을 때, 저는 정말 공감했습니다. 코딩 에이전트(coding agent)의 가장 가치 있는 사용 사례는 LLM이 자체 잠재 공간(latent spaces)에서 가치를 추출하는 수단이 되는 것입니다. 우리는 그것을 어떻게 최적화할 수 있을까요? 모델이 스스로 할 수 있을까요? GPT-5에 접근할 수 있게 된 이후, 저는 이 흐름을 가지고 계속 실험했습니다.

*   모델에게 더 생산적이라고 생각하는 도구 세트를 만들도록 요청했습니다.
*   제가 감독하면서 그 도구들을 사용하여 작업을 시도했습니다.
*   완료 후, 도구를 어떻게 개선할 수 있을지 스스로 성찰했습니다.

저는 또한 이것을 Opus 4(4.1은 아직 출시되지 않았습니다)와 비교했습니다. 좋은 소식은 GPT-5가 개발자 유틸리티(developer utility)를 구축하는 데 매우 좋은 모델이라는 것입니다. 나쁜 소식은 자신이 만든 도구를 사용하는 것을 싫어한다는 것입니다! 모델은 저에게 "솔직히 말해서, 저는 그 어떤 것도 필요하지 않았습니다."라고 말했습니다. 참고: 저는 Gemini 2.5 Pro와 GPT-4.1에서도 이것을 테스트했습니다. Opus만이 GPT-5와 보조를 맞출 수 있는 유일한 모델임이 분명했기 때문에, 저는 Opus에 집중했습니다. 모든 결과와 채팅 기록은 이 저장소(repo)에서 찾을 수 있습니다. 며칠 사용 후, 저는 또한 우리가 "물론이죠!(Certainly!)"의 시대에서 새로운 상징적인 LLM 토큰(token)인 "진행 상황 업데이트(Progress update)"의 시대로 이동하고 있다는 것을 알아차렸습니다. 밈(meme)에 저가 매수하세요!

**도구 #1: AI 코딩 에이전트(coding agent)를 위한 더 나은 작업 관리자**

Linear MCP에 신의 축복이 있기를. 저에게는 정말 가장 유용한 도구 중 하나입니다. 하지만 IDE에서 Claude Code 및 다른 에이전트(agent)의 병렬 인스턴스(instance)로 이동하면서, 각 작업에서 어떤 변경 사항이 이루어지고 있으며, 별도의 Git 워크트리(git worktree)에 있기 때문에 서로에게 어떻게 영향을 미치는지 추적할 더 나은 방법이 필요하다는 것을 깨달았습니다. 우리는 동료들의 PR(Pull Request)을 항상 읽을 수 없으므로 인간에게는 불가능한 일이지만, 우리에게 영향을 미치는 변경 사항이 무엇인지 항상 알고 있다면 병합 충돌 해결(merge conflict resolution)에서 얼마나 많은 시간을 절약할 수 있을지 상상해 보세요. 제가 작성한 프롬프트(prompt)는 다음과 같습니다.

당신은 자신을 여러 인스턴스(instance)로 병렬로 실행할 수 있는 AI 엔지니어 에이전트(AI Engineer agent)입니다. 이를 통해 많은 작업을 동시에 처리할 수 있지만, 일부 위임 문제도 발생합니다. 모든 다른 인스턴스(instance)는 일반적으로 별도의 Git 워크트리(git worktree)에 있으며 서로의 작업을 볼 수 없습니다. 생산성을 높이려면, 당신과 당신의 인스턴스(instance)가 동기화될 수 있도록 새로운 로컬 도구를 만들어야 합니다. 이 도구는 CLI(Command Line Interface)를 통해서만 당신이 접근할 수 있으므로, 해당 사용 사례에 인체 공학적으로 적합한지 확인하세요. 유닉스 유틸리티(unix utility)처럼 느껴져야 합니다. 필요한 인터페이스(interface), 가능한 실패 모드(failure mode), 그리고 에이전트(agent)가 이 도구와 상호 작용하는 방식을 신중하게 고려하세요. 염두에 두어야 할 몇 가지 사용 사례: 새로운 작업을 시작해야 하고, 하위 작업(subtask)을 만들어 위임하고 싶습니다. 이 하위 작업(subtask) 중 일부는 서로 의존할 수 있으며, 차단된 에이전트(agent)가 다른 에이전트(agent)가 완료될 때까지 시작하지 않도록 해야 합니다. 작업을 수행하는 동안 코드베이스(codebase)에 개선 사항이 있을 수 있지만 현재 변경 범위 밖이라는 것을 알게 됩니다. 하지만 나중을 위해 메모해 두고 싶습니다. 작업을 추가하고 참조하는 파일을 쉽게 참조할 수 있어야 합니다. 작업이 완료될 때마다 트래커(tracker)가 업데이트되어야 합니다. 또한, 새로운 변경 사항이 다른 미결 작업에 어떤 식으로든 영향을 미칠 경우, 모든 다른 미결 작업을 검토해야 합니다. 예를 들어, 한 작업이 엔드포인트(endpoint)에 기능을 추가하려고 할 수 있지만, 방금 완료된 작업이 해당 엔드포인트(endpoint)를 제거했을 수 있습니다. 해당 작업을 수행하는 에이전트(agent)에게 어떤 식으로든 알려야 합니다. 또한 할당자(assignee), 상태(status) 등과 같은 작업 관리의 일반적인 요구 사항도 염두에 두세요. 이 폴더 안에 task-manager라는 폴더를 만들고 그 안에서 모든 작업을 수행하세요.

GPT-5의 채팅 기록은 여기에서, Opus 4의 채팅 기록은 여기에서 볼 수 있습니다. GPT-5 버전은 실제로 매우 훌륭하며, 여기에서 찾을 수 있습니다:

*   여러 에이전트(agent)가 동시에 쓰는 문제를 피하기 위해 WAL(Write-Ahead Logging) 사용
*   의존성 그래프(graph of dependencies)를 사용하여 작업 우선순위 지정
*   impact_conflict와 같은 좋은 키워드로 모든 에이전트(agent)가 다른 에이전트(agent)가 무엇을 하는지 볼 수 있는 추가 전용 이벤트 스트림(append-only events stream) 생성

```
# 1) Initialize the database
./gpt5/task-manager/tm init

# 2) Create a task
./gpt5/task-manager/tm add "Implement auth" -d "Add login + signup" -a alice -p 2 --due 2025-09-01 \
--resource modifies:endpoint:POST /api/login --link app/server/auth.py

# 3) List tasks
./gpt5/task-manager/tm list --me # or: --status in_progress

# 4) Show details
./gpt5/task-manager/tm show 1

# 5) Add dependency and attempt start
./gpt5/task-manager/tm depend 1 --on 2
./gpt5/task-manager/tm start 1 # auto-blocks if unmet deps

# 6) Complete a task
./gpt5/task-manager/tm complete 2 -m "Merged PR #123"

# 7) Watch events
./gpt5/task-manager/tm watch --follow --me
```

Opus 4도 좋은 시도를 했지만(여기 참조), 모두를 동기화 상태로 유지하기 위한 알림/스트림(stream) 기능은 파악하지 못했습니다.

```
# Create your first task
tm create "Implement user authentication" --priority high

# Create a dependent task
tm create "Add login endpoint" --depends-on 1 --assignee agent-2

# View all tasks
tm list

# See blocked tasks
tm blocked

# Complete a task and check for conflicts
tm complete 1
```

**도구 #2: 코드 품질 표준 플레이북(Code Quality Standards Playbook)**

제가 만들도록 요청한 두 번째 도구는 코드베이스(codebase)에서 기대하는 모든 표준을 강제하는 방법이었습니다. 타입 체킹(Typechecking) / ESLint 훅(hook) -> 오류 수정 -> 코딩 에이전트(coding agent)로 다시 시도하는 자기 개선(self-improving) 루프는 올바르게 설정되었을 때 개발 속도를 높이는 가장 좋은 방법 중 하나입니다. 하지만 코드베이스(codebase)에는 항상 이러한 기능이 있는 것은 아니므로, 모델에 새로운 코드베이스(codebase)에 접근하고 이를 위한 인프라(infrastructure)를 구축하는 반복 가능한 패턴(pattern)을 제공하는 것이 유용해 보였습니다. 프롬프트(prompt)는 다음과 같습니다.

당신은 자신을 여러 인스턴스(instance)로 병렬로 실행할 수 있는 AI 엔지니어 에이전트(AI Engineer agent)입니다. 때로는 일관성 없는 코드 스타일과 접근 방식으로 이어져 장기적으로 코드베이스(codebase)를 유지 관리하기 어렵게 만듭니다. 당신이 작업하는 모든 코드베이스(codebase)에는 코드를 작성하는 방법에 대한 명시적 및 암묵적 규칙이 있습니다. 당신의 임무는 코드베이스(codebase)를 분석하고 코드를 작성해야 하는 방법에 대한 다양한 휴리스틱(heuristic)을 추출하는 것입니다. 그런 다음 미래에 자동으로 검사할 수 있는 일련의 규칙으로 공식화해야 합니다. 린팅(linting), 타입(type) 등과 같은 것들은 작업하는 언어에 따라 ESLint, Rubocop 등과 같은 기존의 인기 있는 도구를 사용할 수 있습니다. 이러한 시스템은 종종 사용자 정의 규칙(custom rule)을 만들 수 있도록 허용하므로 이를 활용하세요. 더 정성적인 것들을 위해서는 https://danger.systems/와 같은 도구를 살펴보거나, 심지어 자신만의 도구를 만들 수도 있습니다. 여기에는 컨트롤러(controller)를 간결하게 유지하고 로직(logic)을 서비스 객체(service object)로 분리하는 규칙, 높은 쿼리 볼륨(query volume)이 예상되는 컬럼(column)에 항상 인덱스(index)를 갖도록 하는 규칙 등이 포함됩니다. 이 작업을 여러 코드베이스(codebase)에서 수행할 것이므로, 새로운 코드베이스(codebase)를 작업할 때 미래의 자신에게 줄 수 있는 마크다운(Markdown)을 사용하여 철저한 계획 문서(plan document)를 작성하는 것부터 시작하세요.

GPT-5의 채팅은 여기에서, Opus 4의 채팅은 여기에서 볼 수 있으며, 최종 마크다운(Markdown)은 각각 여기와 여기에서 찾을 수 있습니다. 저는 GPT-5 버전이 Opus보다 훨씬 더 미묘하고 정교하다는 것을 발견했습니다.

**모델은 자신이 무엇이 부족한지 알고 있을까요?**

그래서 제가 결정한 도구 #1과 #2 이후, 저는 모델에게 "무엇이 필요하다고 생각하나요?"라고 물었습니다. 저는 SWE-Lancer 작업 설명의 스크린샷을 제공한 다음, 가능한 한 많은 공간을 주기 위해 매우 간단한 프롬프트(prompt)를 사용했습니다.

이 작업을 가능한 한 잘 해결하는 것이 당신의 일이라면, 미래에 당신의 생산성을 높이기 위해 어떤 도구를 스스로 만들겠습니까? @task-manager/를 사용하여 그것들을 추적할 수 있으며, 우리는 구현으로 넘어갈 수 있습니다. 하지만 먼저 당신이 무엇을 계획하고 있는지 보고 싶습니다.

보시다시피 저는 모델이 이전에 만들었던 동일한 task-manager에 접근할 수 있도록 했습니다. 전체 GPT-5 채팅은 여기에서, Opus 4는 여기에서 찾을 수 있습니다. 제가 처음으로 주목한 흥미로운 점은 Claude Code가 task-manager 대신 자체 내부 TODO 트래커(tracker)를 사용하여 초기 계획을 세웠다는 것입니다. 저는 그것이 좋다고 생각했습니다. 제 걱정 중 하나는 모델이 최선이라고 생각하는 것보다 컨텍스트(context)에서 받은 도구를 과도하게 사용하는 것이었습니다. 다음은 나중에 보게 될 루프(loop)의 끝에서 각 모델이 결국 구축한 도구들입니다(GPT-5 개발 도구 및 Opus 4 도구 폴더). 모델의 분위기를 파악하려면 README(리드미)를 살펴보는 것이 좋습니다. GPT-5는 매우 간결하고 핵심적입니다. Claude는 많은 이모지(emoji)를 사용합니다. GPT-5는 또한 각 도구에 대해 별도의 문서 폴더를 만들었지만, Opus는 모든 도구를 하나의 README(리드미)에 모든 지침과 함께 넣었습니다. 전반적으로 둘 다 비슷한 방향을 가지고 있었습니다.

GPT-5:
*   `doctor`: 핵심 도구에 대한 환경 검사
*   `bootstrap`: 한 번의 명령으로 설정 및 스모크 테스트(smoke test)
*   `code-map`: 빌드/찾기 하위 명령이 있는 간단한 저장소(repository) 인덱스(index)
*   `csearch`: 필터(filter)를 사용한 심볼/임포트/텍스트 검색
*   `tasks-graph`: 작업 DB에서 Mermaid 그래프(graph) 출력
*   `impact`: 변경된 파일에 연결된 작업 표시
*   `seed`: 샘플 작업으로 task-manager DB 채우기
*   `repro scaffold`: `.repro/` 아래에 vcrpy-ready repro 생성
*   `e2e`: 경량 E2E 스펙(spec) 스캐폴드(scaffold) 및 실행
*   `preflight`: doctor, 테스트, code-map, impact 및 선택적 E2E 실행
*   `preflight-smol`: smol-podcaster를 위한 저장소(repo)별 preflight (API 상태, Celery 핑, 선택적 의존성 설치)
*   `broker`: Docker를 통해 로컬 RabbitMQ 관리 (rabbitmq:3-management)
*   `flake`: 불안정한 테스트(flaky test)를 감지하기 위해 스위트(suite)를 여러 번 재실행
*   `codemod`: 안전 장치가 있는 정규식 기반 미리보기/적용
*   `triage`: 트리아지(triage) 템플릿(template) 생성 및 작업 열기
*   `trace`: cProfile 기반 표현식 프로파일러(profiler)
*   `runbook`: 작업 DB에서 런북(runbook) 마크다운(Markdown) 생성

Opus 4:
*   `Context Analyzer` - 기술 스택(tech stack) 감지 및 의존성 매핑(dependency mapping)으로 코드베이스(codebase)를 신속하게 이해
*   `Cross-Platform Test Generator` - 웹, iOS, Android, 데스크톱용 E2E 테스트(test) 생성
*   `Implementation Proposal Analyzer` - 점수 및 ROI 분석으로 프리랜서(freelancer) 제안 평가
*   `Full-Stack Change Impact Analyzer` - 데이터베이스(database), API, 프론트엔드(frontend) 계층 전반의 변경 사항 추적
*   `Bug Pattern Recognition Engine` - 버그(bug)를 알려진 패턴(pattern)과 일치시키고 입증된 수정 사항 제안
*   `Security & Permission Auditor` - 포괄적인 보안 스캐닝(scanning) 및 취약점 감지
*   `Multi-Platform Feature Implementer` - 플랫폼(platform) 전반의 기능 구현 조정
*   `API Integration Assistant` - 클라이언트 생성으로 API 통합 간소화
*   `Performance Optimization Toolkit` - 성능 병목 현상 식별 및 수정
*   `Task Complexity Estimator` - 작업 가치 및 복잡성을 기반으로 노력 추정

GPT-5는 이 모든 것을 CLI(Command Line Interface)를 통해 사용하기 쉬운 유닉스 유틸리티(unix utility)로 구축했습니다. Opus 4의 도구들은 모두 `python some_tool.py`로 실행되도록 되어 있습니다. 시간이 더 있었다면, 두 가지 다른 형식으로 모델이 어떻게 작동하는지 알아보기 위해 몇 가지 실험을 했을 테지만, 거의 비슷해 보였습니다. 또한 Opus 4는 작업을 수행하고 약간 의인화된 느낌(예: 보안 감사자)을 주는 도구를 구축하는 반면, GPT-5는 너무 주관적이지 않으면서 스스로 사용할 수 있는 유틸리티(utility)를 구축하는 것처럼 느껴졌습니다.

**도구들은 유용했습니까?**

모델이 이 모든 것을 구현한 후, 제 목표는 도구에 접근할 수 있을 때와 없을 때의 모델 성능을 평가하는 것이었습니다. 제가 처음 시도한 것은 분명히 SWE-Lancer를 실행하는 것이었습니다. 세상에, 그 작업은 많은 토큰(token)을 소비합니다. 저는 단일 작업을 실행하려고 시도했고, 약 25-30분과 280,000 토큰(token)이 소요되었습니다. 그런 다음 제가 더 잘 아는 것으로 옮겨가서 백로그(backlog)에 있던 한 작업을 선택했습니다. 저는 팟캐스트(podcast) 제작자를 위한 오픈 소스(open source) 도우미인 smol-podcaster를 만들었습니다. 이제 우리에게 매우 특정한 몇 가지 기능이 추가된 비공개 포크(private fork)가 호스팅(hosting)되고 있어서 한동안 업데이트하지 않았습니다. 여전히 Python 스크립트(script)를 백엔드(backend)로 사용하는 기본적인 Flask 앱(app)입니다. 저는 이 작업을 생각해냈습니다.

저는 팟캐스트(podcast) 제작자의 후반 작업(post production work)을 돕는 오픈 소스(open source) 프로젝트인 https://github.com/FanaHOVA/smol-podcaster.git의 유지보수자(maintainer)입니다. 당신은 이 프로젝트에서 일하도록 고용되었습니다. 이 작업을 시작하기 전에, 당신은 `tools` 폴더에 일반 도구(generic tool) 세트를 만들었습니다. 그것들을 검토하고 당신이 사용할 수 있다는 것을 기억하세요. 관련성이 없다고 생각되면 사용할 필요는 없습니다. 당신은 또한 task-manager를 직접 만들었고 `codebase-analyzer`에서 새로운 코드베이스(codebase)를 다루는 방법에 대한 아이디어를 모았습니다.

작업 이름: Flask 모놀리스(monolith)에서 FastAPI + Next.js 프론트엔드(frontend)로 마이그레이션(Migrate)

현재 앱(app)은 모든 처리를 위해 Python 백엔드(backend) + Celery 태스크 큐(task queue)를 사용합니다. 이를 사용자에게 노출하기 위해, 사용자 입력을 백엔드(backend) 스크립트(script)로 라우팅(routing)하고 사용자 결과를 기본적인 HTML / CSS로 표시하는 작은 Flask 앱(app)이 있습니다. 이 애플리케이션(application)을 FastAPI를 백엔드(backend)로, Next.js를 프론트엔드(frontend)로 사용하도록 다시 작성하세요. 프론트엔드(frontend)에는 타입스크립트(Typescript)를 사용하고 모든 타입 검사(typecheck)가 통과하는지 확인하세요. 스타일링(styling)에는 Tailwind / ShadCN을 사용하세요. 백엔드(backend)는 이상적으로는 `smol_podcaster.py`의 주요 흐름을 모듈화(modularize)하여 전체 흐름을 항상 실행할 필요 없이 개별 부분을 실행할 수 있도록 해야 합니다. 미래에 더 빠르게 움직일 수 있도록 통합 테스트(integration test)와 단위 테스트(unit test)를 모두 작성하세요. 위의 모든 요구 사항을 충족한다고 확신할 때까지 작업을 멈추지 마세요.

저는 도구 + task-manager + 코드베이스 분석기(codebase analyzer)를 컨텍스트(context)에 전달하고 모델들이 작업을 수행하도록 했습니다. 두 모델 모두 거의 한 번에 작업을 해결할 수 있었습니다. 두 모델 모두 Python 의존성(dependency)에 몇 가지 문제가 있었는데(공감합니다), 제가 채팅을 통해 해결하도록 도왔습니다(코드는 전혀 건드리지 않았습니다). 결국, 그들은 완전한 그린 빌드(green build)에 도달했습니다. 제가 테스트해 보니 아주 잘 작동했습니다. 한 가지 작은 차이점은 GPT-5는 이전과 정확히 동일한 스타일(style)을 유지하여 훌륭했지만, Opus는 디자인(design)과 UX(User Experience)를 다소 변경했다는 것입니다. 제 생각에는 저보다 더 잘할 수 있다고 생각한 것 같습니다(낮은 기준). GPT-5의 전체 실행은 여기에서, Opus 4의 전체 실행은 여기에서 볼 수 있습니다.

실행 후, 저는 간단한 프롬프트(prompt)를 물었습니다.

훌륭합니다. 잘 작동했습니다. 특정 도구를 사용했습니까? 발생했던 실패를 바탕으로, 미래에 더 효과적이기 위해 어떤 도구가 있었으면 좋겠다고 생각했습니까?

Opus 4는 여기에서, GPT-5는 여기에서 볼 수 있습니다(죄송합니다, 이 부분은 서식이 깨졌습니다). 그들은 모두 이미 익숙한 도구를 제외하고는 자신이 만든 도구를 전혀 사용하지 않았다고 말했습니다. 여기서 한 가지 주장은 모델에게 그렇게 하도록 요청하는 대신, 프리-커밋 훅(pre-commit hook) 등을 통해 사용을 강제해야 한다는 것입니다. 이것이 제가 개발 환경 설정(dev setup)에서 하는 일이지만, 저는 모델들이 스스로 알아내도록 하려고 했습니다. 그런 다음, 그들은 다음번에 동일한 작업을 더 잘 해결할 방법에 대한 몇 가지 아이디어를 제시했습니다. 저는 그들에게 그러한 변경 사항을 구현하도록 했습니다. 그런 다음 smol-podcaster 저장소(repo)를 초기화하고, 새로운 도구를 제외하고 동일한 프롬프트(prompt) + 작업을 다시 시도하도록 했습니다. GPT-5는 여기에서, Opus 4는 여기에서 볼 수 있습니다. 그들은 첫 번째 실행과 상당히 비슷하게 수행했습니다. 그 후 저는 비슷한 질문을 했습니다. 도구 중 어떤 것을 사용했습니까? 그들의 답변:

GPT-5:
간단히 말해서: 아니요 — 이번 실행에서는 개발 도구를 사용하지 않았습니다. [...] 실패는 런타임/환경 문제(누락된 라이브러리, API 키 인스턴스화 타이밍, 사용 중인 포트, RabbitMQ 미실행)였습니다. 직접 수정하는 것이 더 빨랐습니다. 이번 통과에서는 사용자 정의 도구로 이점을 얻을 수 있는 저장소(repo) 전체 리팩토링(refactoring)이나 진단은 없었습니다.

이전 단계에서 이미 RabbitMQ 문제가 있었고 도구를 만들었지만, 그것을 무시했습니다. 또한 분명히 저장소(repo) 전체 변경이었으므로, 훈련에서 본 적이 없기 때문에 작업과 도구를 잘못 매칭했거나, 아니면 저를 가스라이팅(gaslighting)하고 있는 것일 수도 있습니다(많은 엔지니어들이 그러하듯이, 꽤 인상적입니다).

Opus 4는 매우 흥미로웠고 GPT-5의 답변을 더 잘 이해하는 데 도움이 되었습니다. 로그(log)를 저장하는 것을 잊었지만 다행히 스크린샷(screenshot)을 찍었습니다. 저는 이것을 "보세요, 저는 이미 가지고 있는 지식으로 그 도구들을 만들었습니다. 실제로 작업을 할 때는 도구를 사용하는 것보다 그냥 하는 것이 더 쉽습니다."라고 해석했는데, 전적으로 공감합니다. 이것은 이전 팟캐스트(podcast) 에피소드에서 두 가지를 떠올리게 했습니다. Nathan Lambert는 모델이 초기 실패를 겪으면 RL(Reinforcement Learning) 프로세스(process) 중에 도구를 사용하지 않는 법을 빠르게 배운다고 말했습니다(타임스탬프). 제 생각에는 추론 시간(inference time)에 새로운 도구를 습득하게 하려면 단순히 프롬프트(prompt)를 주는 것보다 더 강력한 강제(enforcement)가 필요해 보입니다. Noam Brown은 에이전트(agent)를 위한 스캐폴딩(scaffolding)이 규모(scale)에 의해 사라질 것이라고 말했습니다(타임스탬프). 이것이 제가 그의 말을 처음으로 직접 느낀 순간이었습니다. 제가 시도한 작업이 너무 쉬웠는지에 대한 질문도 있습니다. 더 크고 어려운 프로젝트에 대한 평가(eval)와 함께 또 다른 게시물이 나올 예정입니다. 미래에는 우리가 직접 수동으로 테스트(test)를 실행하는 대신, 이 모든 것을 수행할 더 나은 하네스(harness)를 구축할 것입니다. 결론은 제가 시도한 작업은 저에게 4-5시간이 걸릴 것이므로, 저에게는 충분히 좋다는 것입니다!

**모델이 스스로를 돕도록 돕기**

지금으로서는, 저는 한계를 정말로 뛰어넘는 추론 시간 자기 개선 코딩 에이전트(inference-time self-improving coding agent)와는 거리가 멀다고 생각합니다. 저는 여전히 모델을 사용하여 규칙 기반 도구(rule-based tool)를 개선하는 것이 좋은 아이디어라고 생각합니다. ESLint 규칙, 테스트(test) 등을 작성하는 것은 항상 토큰(token)의 좋은 투자입니다. 이 분야에서 더 많은 작업을 해야 한다면, 모델이 이러한 도구를 완벽하게 만들고, 그런 다음 RL(Reinforcement Learning)을 통해 그것들을 진정으로 내재화(internalize)하도록 하여 차이를 만들 수 있는지 알아볼 것입니다. 차세대 모델은 그것들을 전혀 사용하지 않을 수도 있지만, 저는 AGI 점근선(asymptote)을 활용하는 데 관심이 있습니다. 저는 2023년에 이 내용을 팀과 공유했습니다.

모델 개선의 인지된 감속은 위에서 설명되었습니다. AGI 선이 넘어서기 전까지는 큰 도약을 인지하기가 점점 더 어려워질 것입니다. 만약 그렇다면, 이는 많은 작업에서 오래된 모델의 성능이 거의 AGI와 같지만, 훨씬 저렴하고 종종 오픈 소스(open source)라는 것을 의미합니다. Kernel Labs의 많은 작업은 이것에 의해 주도될 것입니다.

다시 한번, 모든 결과와 채팅 기록은 여기에서 찾을 수 있습니다. 질문이 있으시면 제 DM(Direct Message)은 열려 있습니다!