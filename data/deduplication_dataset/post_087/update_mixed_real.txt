Alessio의 노트: GPT-5에 대한 새로운 게시물을 올릴 시간입니다! 그리고 swyx가 작년 주말에 Karpathy, OpenAI, Cognition 팀과 함께 주최했던 해커톤(hackathon)에 대한 소식을 다시 한번 상기시켜 드립니다. 당시 신청은 여기에서 할 수 있었습니다. "자기 개선(Self-Improving)"이라는 용어는 AI 안전(AI safety) 분야에서 여전히 논란의 여지가 있는 용어입니다. 이 용어는 "기계가 우리가 이해할 수 없는 방식으로 우리보다 더 똑똑해질 것"이라는 어두운 뉘앙스를 내포하고 있습니다. 그러나 만약 우리가 그것을 이해할 수 있다면 어떨까요? 지난 2024년 10월, OpenAI는 LLM(Large Language Models)이 머신러닝 엔지니어링(machine learning engineering)을 얼마나 잘 수행하는지 측정하는 벤치마크(benchmark)인 MLE Bench를 공개했습니다. ML 엔지니어링(ML Engineering)을 통한 모델의 자기 개선(self-improving) 경로는 더 나은 알고리즘, 더 깨끗한 데이터, 그리고 효율적인 메모리 사용, 즉 훈련 시간 자기 개선(training-time self-improvement)에 의해 촉진됩니다. 하지만 대부분의 AI 엔지니어(AI Engineer)는 직접 모델을 훈련시키기보다는, 단지 모델을 사용하는 사용자일 뿐입니다. 이러한 사용자들은 모델의 자기 개선 과정에 어떻게 기여할 수 있을까요? 만약 가중치(weights)를 직접 업데이트할 수 없다면, 모델이 특정 작업에서 성능을 향상시키도록 어떻게 이끌 수 있을까요? 저는 이 개념을 추론 시간 자기 개선(inference-time self-improvement)이라고 생각하며, Voyager는 스킬 라이브러리(skill library)를 통해 이 분야에 대한 초기 접근 방식 중 하나로 주목받았습니다.

제가 Kernel Labs에서 일하기 시작한 이래로(현재 저희 팀은 다양한 최첨단 연구를 진행하고 있습니다), claude-squad 및 vibe-kanban과 같은 도구들을 활용하여 코딩 에이전트(coding agent)를 병렬화하는 방식은 가장 효과적인 생산성 향상(productivity hack) 전략 중 하나로 자리 잡았습니다. Boris Cherny가 과거 인터뷰에서 Claude Code를 "유닉스 유틸리티(unix utility)"라고 묘사했을 때, 저는 그 말에 깊이 공감했습니다. 코딩 에이전트(coding agent)의 가장 중요한 활용 사례는 LLM이 자체 잠재 공간(latent spaces)으로부터 가치를 추출할 수 있는 매개체가 되는 것입니다. 그렇다면 우리는 이러한 과정을 어떻게 최적화할 수 있을까요? 모델 스스로 이 작업을 수행할 수 있을까요?

GPT-5에 대한 접근 권한을 얻은 후, 저는 다음 흐름에 따라 실험을 진행하는 데 많은 시간을 할애했습니다.

*   모델에게 더 생산적이라고 생각하는 도구 세트를 만들도록 요청했습니다.
*   제가 감독하면서 그 도구들을 사용하여 작업을 시도했습니다.
*   작업 완료 후, 도구를 어떻게 개선할 수 있을지 스스로 성찰했습니다.

저는 또한 당시 최신 버전이었던 Opus 4(4.1은 아직 출시되지 않았습니다)와 GPT-5의 성능을 비교했습니다. 긍정적인 소식은 GPT-5가 개발자 유틸리티(developer utility)를 구축하는 데 매우 뛰어난 모델이라는 점입니다. 하지만 아쉬운 점은, GPT-5가 자신이 직접 만든 도구를 사용하는 것을 꺼려한다는 것입니다! 모델은 저에게 "솔직히 말해서, 저는 그 어떤 것도 필요하지 않았습니다."라고 답변했습니다. 참고로, 저는 이 실험을 Gemini 2.5 Pro와 GPT-4.1에서도 진행했습니다. 당시에는 Opus만이 GPT-5와 견줄 만한 성능을 보여주었기 때문에, 저는 주로 Opus와의 비교에 집중했습니다. 모든 실험 결과와 채팅 기록은 이 저장소(repo)에서 확인하실 수 있습니다. 몇 주간의 사용 후, 저는 또한 LLM의 상징적인 토큰(token)이 과거의 "물론이죠!(Certainly!)" 시대에서 "진행 상황 업데이트(Progress update)"의 시대로 변화하고 있다는 점을 발견했습니다. 이러한 변화는 밈(meme) 관점에서도 주목할 만한 가치가 있습니다!

**도구 #1: AI 코딩 에이전트(coding agent)를 위한 진화된 작업 관리자**

Linear MCP는 여전히 저에게 가장 유용한 도구 중 하나입니다. 하지만 IDE 환경에서 Claude Code와 같은 다른 에이전트(agent)의 병렬 인스턴스(instance)로 작업 흐름이 확장되면서, 각 작업에서 발생하는 변경 사항과 이들이 별도의 Git 워크트리(git worktree) 내에서 서로에게 미치는 영향을 효율적으로 추적할 수 있는 더 나은 방법의 필요성을 절감했습니다. 인간이 동료들의 모든 PR(Pull Request)을 항상 검토하는 것은 현실적으로 불가능하지만, 만약 우리에게 영향을 미치는 변경 사항을 항상 파악할 수 있다면 병합 충돌 해결(merge conflict resolution)에 소요되는 시간을 얼마나 절약할 수 있을지 상상해 보세요. 제가 AI 에이전트에게 제시했던 프롬프트(prompt)는 다음과 같습니다.

당신은 자신을 여러 인스턴스(instance)로 병렬로 실행할 수 있는 AI 엔지니어 에이전트(AI Engineer agent)입니다. 이 능력은 당신이 많은 작업을 동시에 처리할 수 있게 하지만, 동시에 일부 위임 문제도 야기합니다. 모든 개별 인스턴스(instance)는 일반적으로 별도의 Git 워크트리(git worktree)에 존재하며 서로의 작업을 직접 볼 수 없습니다. 생산성을 극대화하기 위해, 당신과 당신의 인스턴스(instance)가 항상 동기화될 수 있도록 새로운 로컬 도구를 개발해야 합니다. 이 도구는 오직 CLI(Command Line Interface)를 통해서만 당신이 접근할 수 있도록 설계되어야 하며, 해당 사용 사례에 인체 공학적으로 적합하게 만들어져야 합니다. 마치 유닉스 유틸리티(unix utility)처럼 느껴져야 합니다. 필요한 인터페이스(interface), 발생 가능한 실패 모드(failure mode), 그리고 에이전트(agent)가 이 도구와 상호 작용하는 방식에 대해 심도 있게 고민해 보세요. 다음은 염두에 두어야 할 몇 가지 핵심 사용 사례입니다. 첫째, 새로운 작업을 시작해야 할 때, 이를 하위 작업(subtask)으로 분할하여 다른 에이전트에게 위임하고 싶을 수 있습니다. 이때 일부 하위 작업(subtask)은 서로 의존 관계를 가질 수 있으므로, 선행 작업이 완료될 때까지 차단된 에이전트(agent)가 작업을 시작하지 않도록 보장해야 합니다. 둘째, 작업을 수행하는 도중 코드베이스(codebase)에 개선의 여지가 있지만, 현재 변경 범위 밖의 사항임을 발견할 수 있습니다. 이러한 사항은 나중을 위해 반드시 메모해 두어야 합니다. 작업을 추가하고 해당 파일에 대한 참조를 쉽게 추가할 수 있어야 합니다. 셋째, 작업이 완료될 때마다 트래커(tracker)가 즉시 업데이트되어야 합니다. 또한, 새로운 변경 사항이 다른 미결 작업에 어떤 식으로든 영향을 미칠 가능성이 있으므로, 모든 다른 미결 작업을 검토해야 합니다. 예를 들어, 한 작업이 특정 엔드포인트(endpoint)에 기능을 추가하려 했으나, 방금 완료된 다른 작업이 해당 엔드포인트(endpoint)를 제거했을 수 있습니다. 이 경우 해당 작업을 수행하는 에이전트(agent)에게 반드시 알려야 합니다. 마지막으로, 할당자(assignee), 상태(status) 등과 같은 일반적인 작업 관리 요구 사항도 고려해야 합니다. 이 지시를 받은 폴더 안에 `task-manager`라는 새 폴더를 만들고 그 안에서 모든 개발 작업을 진행하세요.

GPT-5의 채팅 기록은 여기에서, Opus 4의 채팅 기록은 여기에서 확인하실 수 있습니다. GPT-5 버전은 실제로 매우 인상적이며, 여기에서 그 결과물을 찾을 수 있습니다.

*   여러 에이전트(agent)가 동시에 쓰는 문제를 방지하기 위해 WAL(Write-Ahead Logging)을 사용합니다.
*   의존성 그래프(graph of dependencies)를 활용하여 작업의 우선순위를 지능적으로 지정합니다.
*   `impact_conflict`와 같은 명확한 키워드를 통해 모든 에이전트(agent)가 다른 에이전트(agent)가 수행하는 작업을 실시간으로 확인할 수 있는 추가 전용 이벤트 스트림(append-only events stream)을 생성했습니다.

```
# 1) Initialize the database
./gpt5/task-manager/tm init

# 2) Create a task
./gpt5/task-manager/tm add "Implement auth" -d "Add login + signup" -a alice -p 2 --due 2025-09-01 \
--resource modifies:endpoint:POST /api/login --link app/server/auth.py

# 3) List tasks
./gpt5/task-manager/tm list --me # or: --status in_progress

# 4) Show details
./gpt5/task-manager/tm show 1

# 5) Add dependency and attempt start
./gpt5/task-manager/tm depend 1 --on 2
./gpt5/task-manager/tm start 1 # auto-blocks if unmet deps

# 6) Complete a task
./gpt5/task-manager/tm complete 2 -m "Merged PR #123"

# 7) Watch events
./gpt5/task-manager/tm watch --follow --me
```

Opus 4 또한 좋은 시도를 보였지만(여기 참조), 모든 에이전트(agent)를 동기화 상태로 유지하기 위한 알림/스트림(stream) 기능은 충분히 구현하지 못했습니다.

```
# Create your first task
tm create "Implement user authentication" --priority high

# Create a dependent task
tm create "Add login endpoint" --depends-on 1 --assignee agent-2

# View all tasks
tm list

# See blocked tasks
tm blocked

# Complete a task and check for conflicts
tm complete 1
```

**도구 #2: 코드 품질 표준 플레이북(Code Quality Standards Playbook)의 중요성**

제가 모델에게 만들도록 요청한 두 번째 도구는 코드베이스(codebase)에서 기대하는 모든 표준을 강제하는 체계적인 방법이었습니다. 타입 체킹(Typechecking) / ESLint 훅(hook)을 통해 오류를 수정하고 코딩 에이전트(coding agent)로 다시 시도하는 자기 개선(self-improving) 루프는 올바르게 설정되었을 때 개발 속도를 획기적으로 가속화하는 가장 효과적인 방법 중 하나입니다. 그러나 모든 코드베이스(codebase)가 이러한 기능을 항상 갖추고 있는 것은 아니므로, 모델에 새로운 코드베이스(codebase)에 접근하고 이를 위한 인프라(infrastructure)를 구축하는 반복 가능한 패턴(pattern)을 제공하는 것이 매우 유용하다고 판단했습니다. 제가 제시한 프롬프트(prompt)는 다음과 같습니다.

당신은 자신을 여러 인스턴스(instance)로 병렬로 실행할 수 있는 AI 엔지니어 에이전트(AI Engineer agent)입니다. 때로는 이러한 병렬 작업이 일관성 없는 코드 스타일과 접근 방식으로 이어져 장기적으로 코드베이스(codebase)를 유지 관리하는 데 어려움을 초래할 수 있습니다. 당신이 작업하는 모든 코드베이스(codebase)에는 코드를 작성하는 방법에 대한 명시적 및 암묵적 규칙이 존재합니다. 당신의 주요 임무는 코드베이스(codebase)를 심층적으로 분석하고, 코드를 작성해야 하는 방법에 대한 다양한 휴리스틱(heuristic)을 추출하는 것입니다. 그런 다음, 추출된 휴리스틱을 미래에 자동으로 검사할 수 있는 일련의 규칙으로 공식화해야 합니다. 린팅(linting), 타입(type) 검사 등과 같은 일반적인 사항의 경우, 당신이 작업하는 언어에 따라 ESLint, Rubocop 등과 같은 기존의 인기 있는 도구를 활용할 수 있습니다. 이러한 시스템은 종종 사용자 정의 규칙(custom rule)을 생성할 수 있는 기능을 제공하므로, 이를 적극적으로 활용하세요. 더 정성적인 코드 품질 측면을 위해서는 https://danger.systems/와 같은 도구를 탐색하거나, 심지어 당신만의 맞춤형 도구를 개발할 수도 있습니다. 이러한 규칙에는 컨트롤러(controller)를 간결하게 유지하고 로직(logic)을 서비스 객체(service object)로 분리하는 방법, 높은 쿼리 볼륨(query volume)이 예상되는 컬럼(column)에 항상 인덱스(index)를 갖도록 보장하는 방법 등이 포함될 수 있습니다. 이 작업을 여러 코드베이스(codebase)에 걸쳐 수행하게 될 것이므로, 새로운 코드베이스(codebase)를 작업할 때 미래의 당신 자신에게 제공할 수 있는 마크다운(Markdown) 형식의 철저한 계획 문서(plan document)를 작성하는 것부터 시작하세요.

GPT-5의 채팅 기록은 여기에서, Opus 4의 채팅 기록은 여기에서 볼 수 있으며, 최종 마크다운(Markdown)은 각각 여기와 여기에서 찾을 수 있습니다. 저는 GPT-5 버전이 Opus보다 훨씬 더 미묘하고 정교한 접근 방식을 보였다는 것을 발견했습니다. 이는 AI 모델이 코드 품질 표준에 대한 깊이 있는 이해를 기반으로 더욱 실용적인 가이드를 제공할 수 있음을 시사합니다.

**모델은 자신이 무엇이 부족한지 알고 있을까요? 진화하는 자기 성찰 능력**

도구 #1과 #2는 제가 직접 결정하여 모델에게 만들도록 지시했지만, 그 이후에는 모델에게 직접 "무엇이 필요하다고 생각하나요?"라고 물어보았습니다. 저는 SWE-Lancer 작업 설명의 스크린샷을 제공한 다음, 모델이 자유롭게 생각할 수 있는 최대한의 여지를 주기 위해 매우 간단한 프롬프트(prompt)를 사용했습니다.

이 작업을 가능한 한 최선으로 해결하는 것이 당신의 임무라면, 미래에 당신의 생산성을 높이기 위해 어떤 도구를 스스로 구축하시겠습니까? `@task-manager/`를 활용하여 이 도구들을 추적할 수 있으며, 이후 구현 단계로 넘어갈 수 있습니다. 하지만 먼저 당신의 계획을 보고 싶습니다.

보시다시피, 저는 모델에게 이전에 직접 구축했던 동일한 `task-manager`에 접근할 수 있도록 허용했습니다. 전체 GPT-5 채팅은 여기에서, Opus 4는 여기에서 찾을 수 있습니다. 제가 처음으로 주목한 흥미로운 점은 Claude Code가 `task-manager` 대신 자체 내부 TODO 트래커(tracker)를 사용하여 초기 계획을 세웠다는 것입니다. 저는 이것이 모델이 자신의 익숙한 방식을 선호하는 자연스러운 경향을 보여준다고 생각했습니다. 제 걱정 중 하나는 모델이 최선이라고 생각하는 것보다 컨텍스트(context)에서 받은 도구를 과도하게 사용하는 것이었습니다. 다음은 나중에 보게 될 루프(loop)의 끝에서 각 모델이 최종적으로 구축한 도구들입니다(GPT-5 개발 도구 및 Opus 4 도구 폴더). 모델의 특성을 파악하려면 README(리드미)를 살펴보는 것이 좋습니다. GPT-5는 매우 간결하고 핵심적입니다. 반면 Claude는 많은 이모지(emoji)를 사용했습니다. GPT-5는 또한 각 도구에 대해 별도의 문서 폴더를 생성했지만, Opus는 모든 도구를 하나의 README(리드미)에 모든 지침과 함께 통합했습니다. 전반적으로, 두 모델 모두 유사한 방향성을 가지고 있었습니다.

GPT-5:
*   `doctor`: 핵심 도구에 대한 환경 검사 및 설정 확인
*   `bootstrap`: 한 번의 명령으로 개발 환경 설정 및 스모크 테스트(smoke test) 수행
*   `code-map`: 빌드/찾기 하위 명령을 포함한 간단한 저장소(repository) 인덱스(index) 생성
*   `csearch`: 필터(filter) 기능을 갖춘 심볼/임포트/텍스트 검색 유틸리티
*   `tasks-graph`: 작업 DB의 의존성 관계를 Mermaid 그래프(graph)로 시각화
*   `impact`: 변경된 파일에 연결된 작업 목록 표시
*   `seed`: 샘플 작업을 통해 `task-manager` DB를 채우는 기능
*   `repro scaffold`: `.repro/` 디렉토리 아래에 `vcrpy-ready` 재현 환경 생성
*   `e2e`: 경량 E2E 스펙(spec) 스캐폴드(scaffold) 및 실행
*   `preflight`: `doctor`, 테스트, `code-map`, `impact` 및 선택적 E2E를 포함한 사전 점검 실행
*   `preflight-smol`: `smol-podcaster` 프로젝트를 위한 저장소(repo)별 `preflight` (API 상태, Celery 핑, 선택적 의존성 설치 포함)
*   `broker`: Docker를 통해 로컬 RabbitMQ 관리 (rabbitmq:3-management 이미지 사용)
*   `flake`: 불안정한 테스트(flaky test)를 감지하기 위해 테스트 스위트(suite)를 여러 번 재실행
*   `codemod`: 안전 장치를 갖춘 정규식 기반 코드 미리보기/적용 도구
*   `triage`: 트리아지(triage) 템플릿(template) 생성 및 관련 작업 열기
*   `trace`: `cProfile` 기반 표현식 프로파일러(profiler)
*   `runbook`: 작업 DB에서 런북(runbook) 마크다운(Markdown) 문서 생성

Opus 4:
*   `Context Analyzer`: 기술 스택(tech stack) 감지 및 의존성 매핑(dependency mapping)을 통해 코드베이스(codebase)를 신속하게 이해
*   `Cross-Platform Test Generator`: 웹, iOS, Android, 데스크톱 환경을 위한 E2E 테스트(test) 생성
*   `Implementation Proposal Analyzer`: 점수 및 ROI 분석을 통해 프리랜서(freelancer) 제안 평가
*   `Full-Stack Change Impact Analyzer`: 데이터베이스(database), API, 프론트엔드(frontend) 계층 전반의 변경 사항 추적
*   `Bug Pattern Recognition Engine`: 버그(bug)를 알려진 패턴(pattern)과 일치시키고 검증된 수정 사항 제안
*   `Security & Permission Auditor`: 포괄적인 보안 스캐닝(scanning) 및 취약점 감지 기능 제공
*   `Multi-Platform Feature Implementer`: 다양한 플랫폼(platform) 전반의 기능 구현을 조율하고 관리
*   `API Integration Assistant`: 클라이언트(client) 생성을 통해 API 통합 프로세스 간소화
*   `Performance Optimization Toolkit`: 성능 병목 현상 식별 및 효과적인 수정 방안 제시
*   `Task Complexity Estimator`: 작업의 가치와 복잡성을 기반으로 필요한 노력 추정

GPT-5는 이 모든 도구를 CLI(Command Line Interface)를 통해 쉽게 사용할 수 있는 유닉스 유틸리티(unix utility) 형태로 구축했습니다. 반면 Opus 4의 도구들은 모두 `python some_tool.py`와 같이 파이썬 스크립트(script)로 실행되도록 설계되었습니다. 만약 더 많은 시간이 있었다면, 두 가지 다른 형식으로 모델이 어떻게 작동하는지 알아보기 위해 몇 가지 실험을 했을 테지만, 결과는 크게 다르지 않을 것으로 보였습니다. 또한 Opus 4는 작업을 수행하고 약간 의인화된 느낌(예: 보안 감사자)을 주는 도구를 구축하는 반면, GPT-5는 너무 주관적이지 않으면서 스스로 사용할 수 있는 유틸리티(utility)를 구축하는 것처럼 느껴졌습니다. 이는 모델이 도구를 인식하고 활용하는 방식에 대한 흥미로운 차이를 보여줍니다.

**도구들은 실제 작업에 유용했습니까?**

모델이 이 모든 도구를 구현한 후, 제 목표는 도구에 접근할 수 있을 때와 없을 때의 모델 성능을 평가하는 것이었습니다. 제가 처음 시도한 것은 물론 SWE-Lancer를 실행하는 것이었습니다. 세상에, 그 작업은 엄청난 양의 토큰(token)을 소비했습니다. 단일 작업을 실행하는 데 약 25-30분과 280,000 토큰(token)이 소요되었습니다. 그 후 저는 제가 더 잘 아는 영역으로 옮겨가서 백로그(backlog)에 있던 한 작업을 선택했습니다. 저는 팟캐스트(podcast) 제작자를 위한 오픈 소스(open source) 도우미인 smol-podcaster를 만들었습니다. 현재는 저희에게 매우 특정한 몇 가지 기능이 추가된 비공개 포크(private fork)가 호스팅(hosting)되고 있어서 한동안 업데이트하지 않았습니다. 이 앱(app)은 여전히 Python 스크립트(script)를 백엔드(backend)로 사용하는 기본적인 Flask 앱(app)입니다. 저는 다음과 같은 작업을 모델에게 제시했습니다.

저는 팟캐스트(podcast) 제작자의 후반 작업(post production work)을 돕는 오픈 소스(open source) 프로젝트인 https://github.com/FanaHOVA/smol-podcaster.git의 유지보수자(maintainer)입니다. 당신은 이 프로젝트에서 일하도록 고용되었습니다. 이 작업을 시작하기 전에, 당신은 `tools` 폴더에 일반 도구(generic tool) 세트를 만들었습니다. 그것들을 검토하고 당신이 사용할 수 있다는 것을 기억하세요. 관련성이 없다고 생각되면 사용할 필요는 없습니다. 당신은 또한 task-manager를 직접 만들었고 `codebase-analyzer`에서 새로운 코드베이스(codebase)를 다루는 방법에 대한 아이디어를 모았습니다.

작업 이름: Flask 모놀리스(monolith)에서 FastAPI + Next.js 프론트엔드(frontend)로 마이그레이션(Migrate)

현재 앱(app)은 모든 처리를 위해 Python 백엔드(backend) + Celery 태스크 큐(task queue)를 사용합니다. 이를 사용자에게 노출하기 위해, 사용자 입력을 백엔드(backend) 스크립트(script)로 라우팅(routing)하고 사용자 결과를 기본적인 HTML / CSS로 표시하는 작은 Flask 앱(app)이 있습니다. 이 애플리케이션(application)을 FastAPI를 백엔드(backend)로, Next.js를 프론트엔드(frontend)로 사용하도록 다시 작성하세요. 프론트엔드(frontend)에는 타입스크립트(Typescript)를 사용하고 모든 타입 검사(typecheck)가 통과하는지 확인하세요. 스타일링(styling)에는 Tailwind / ShadCN을 사용하세요. 백엔드(backend)는 이상적으로는 `smol_podcaster.py`의 주요 흐름을 모듈화(modularize)하여 전체 흐름을 항상 실행할 필요 없이 개별 부분을 실행할 수 있도록 해야 합니다. 미래에 더 빠르게 움직일 수 있도록 통합 테스트(integration test)와 단위 테스트(unit test)를 모두 작성하세요. 위의 모든 요구 사항을 충족한다고 확신할 때까지 작업을 멈추지 마세요.

저는 도구 + task-manager + 코드베이스 분석기(codebase analyzer)를 컨텍스트(context)에 전달하고 모델들이 작업을 수행하도록 했습니다. 두 모델 모두 거의 한 번에 작업을 해결할 수 있었습니다. 두 모델 모두 Python 의존성(dependency)에 몇 가지 문제가 있었는데(저도 공감합니다), 제가 채팅을 통해 해결하도록 도왔습니다(코드는 전혀 건드리지 않았습니다). 결국, 그들은 완전한 그린 빌드(green build)에 도달했습니다. 제가 테스트해 보니 아주 잘 작동했습니다. 한 가지 작은 차이점은 GPT-5는 이전과 정확히 동일한 스타일(style)을 유지하여 훌륭했지만, Opus는 디자인(design)과 UX(User Experience)를 다소 변경했다는 것입니다. 제 생각에는 Opus가 저보다 더 잘할 수 있다고 생각한 것 같습니다(낮은 기준이긴 하지만요). GPT-5의 전체 실행은 여기에서, Opus 4의 전체 실행은 여기에서 볼 수 있습니다.

실행 후, 저는 간단한 프롬프트(prompt)를 물었습니다.

훌륭합니다. 잘 작동했습니다. 특정 도구를 사용했습니까? 발생했던 실패를 바탕으로, 미래에 더 효과적이기 위해 어떤 도구가 있었으면 좋겠다고 생각했습니까?

Opus 4의 답변은 여기에서, GPT-5의 답변은 여기에서 볼 수 있습니다(죄송합니다, 이 부분은 서식이 깨졌습니다). 그들은 모두 이미 익숙한 도구를 제외하고는 자신이 만든 도구를 전혀 사용하지 않았다고 말했습니다. 여기서 한 가지 주장은 모델에게 그렇게 하도록 요청하는 대신, 프리-커밋 훅(pre-commit hook) 등을 통해 도구 사용을 강제해야 한다는 것입니다. 이것이 제가 개발 환경 설정(dev setup)에서 하는 일이지만, 저는 모델들이 스스로 알아내도록 하려고 했습니다. 그런 다음, 그들은 다음번에 동일한 작업을 더 잘 해결할 방법에 대한 몇 가지 아이디어를 제시했습니다. 저는 그들에게 그러한 변경 사항을 구현하도록 했습니다. 그런 다음 smol-podcaster 저장소(repo)를 초기화하고, 새로운 도구를 포함하여 동일한 프롬프트(prompt) + 작업을 다시 시도하도록 했습니다. GPT-5는 여기에서, Opus 4는 여기에서 볼 수 있습니다. 그들은 첫 번째 실행과 상당히 비슷하게 수행했습니다. 이후 저는 비슷한 질문을 던졌습니다. "도구 중 어떤 것을 사용했습니까?" 그들의 답변은 다음과 같았습니다.

GPT-5:
간단히 말해서: 아니요 — 이번 실행에서는 개발 도구를 사용하지 않았습니다. [...] 실패는 런타임/환경 문제(누락된 라이브러리, API 키 인스턴스화 타이밍, 사용 중인 포트, RabbitMQ 미실행)였습니다. 직접 수정하는 것이 더 빨랐습니다. 이번 통과에서는 사용자 정의 도구로 이점을 얻을 수 있는 저장소(repo) 전체 리팩토링(refactoring)이나 진단은 없었습니다.

이전 단계에서 이미 RabbitMQ 문제가 있었고 도구를 만들었지만, 그것을 무시했습니다. 또한 분명히 저장소(repo) 전체 변경이었으므로, 훈련에서 본 적이 없기 때문에 작업과 도구를 잘못 매칭했거나, 아니면 저를 가스라이팅(gaslighting)하고 있는 것일 수도 있습니다(많은 엔지니어들이 그러하듯이, 꽤 인상적입니다).

Opus 4의 답변은 매우 흥미로웠고 GPT-5의 답변을 더 잘 이해하는 데 도움이 되었습니다. 로그(log)를 저장하는 것을 잊었지만 다행히 스크린샷(screenshot)을 찍었습니다. 저는 이것을 "보세요, 저는 이미 가지고 있는 지식으로 그 도구들을 만들었습니다. 실제로 작업을 할 때는 도구를 사용하는 것보다 그냥 하는 것이 더 쉽습니다."라고 해석했는데, 전적으로 공감합니다. 이러한 현상은 과거 팟캐스트(podcast) 에피소드에서 언급되었던 두 가지를 떠올리게 했습니다. Nathan Lambert는 모델이 초기 실패를 겪으면 RL(Reinforcement Learning) 프로세스(process) 중에 도구를 사용하지 않는 법을 빠르게 배운다고 말했습니다(타임스탬프). 제 생각에는 추론 시간(inference time)에 새로운 도구를 습득하게 하려면 단순히 프롬프트(prompt)를 주는 것보다 더 강력한 강제(enforcement)가 필요해 보입니다. Noam Brown은 에이전트(agent)를 위한 스캐폴딩(scaffolding)이 규모(scale)에 의해 사라질 것이라고 말했습니다(타임스탬프). 이것이 제가 그의 말을 처음으로 직접 피부로 느낀 순간이었습니다. 제가 시도한 작업이 너무 쉬웠는지에 대한 의문도 여전히 남아 있습니다. 더 크고 어려운 프로젝트에 대한 평가(eval) 결과와 함께 또 다른 게시물이 곧 나올 예정입니다. 미래에는 우리가 직접 수동으로 테스트(test)를 실행하는 대신, 이 모든 과정을 자동화할 수 있는 더 나은 하네스(harness)를 구축할 것입니다. 결론적으로, 제가 시도한 작업은 저에게 4-5시간이 걸릴 것이므로, AI 모델이 이 정도의 작업을 수행할 수 있다는 것만으로도 저에게는 충분히 만족스러운 결과입니다!

**모델이 스스로를 돕도록 돕기: 미래를 위한 전략**

현재로서는, 저는 한계를 정말로 뛰어넘는 추론 시간 자기 개선 코딩 에이전트(inference-time self-improving coding agent)의 등장은 아직 먼 이야기라고 생각합니다. 하지만 저는 여전히 모델을 활용하여 규칙 기반 도구(rule-based tool)를 개선하는 것이 매우 유효하고 좋은 아이디어라고 믿습니다. ESLint 규칙, 테스트(test) 등을 작성하는 것은 항상 토큰(token)의 가치 있는 투자입니다. 만약 이 분야에서 더 많은 연구를 진행한다면, 저는 모델이 이러한 도구를 완벽하게 만들고, 그런 다음 강화 학습(Reinforcement Learning)을 통해 그것들을 진정으로 내재화(internalize)하도록 하여 실질적인 차이를 만들어낼 수 있는지 탐구할 것입니다. 차세대 모델은 이러한 도구들을 전혀 사용하지 않을 수도 있지만, 저는 AGI 점근선(asymptote)을 활용하는 데 깊은 관심을 가지고 있습니다. 저는 2023년에 이 내용을 저희 팀과 공유했습니다.

모델 개선의 인지된 감속은 위에서 설명되었습니다. AGI 선이 넘어서기 전까지는 큰 도약을 인지하기가 점점 더 어려워질 것입니다. 만약 그렇다면, 이는 많은 작업에서 오래된 모델의 성능이 거의 AGI와 같지만, 훨씬 저렴하고 종종 오픈 소스(open source)라는 것을 의미합니다. Kernel Labs의 많은 작업은 이러한 철학에 기반하여 진행될 것입니다.

다시 한번, 모든 결과와 채팅 기록은 여기에서 찾을 수 있습니다. 질문이 있으시면 제 DM(Direct Message)은 언제든지 열려 있습니다!