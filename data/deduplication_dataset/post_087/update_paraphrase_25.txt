Alessio의 새로운 GPT-5 관련 글입니다. 이번 글에서는 AI 에이전트(AI agent)의 '자기 개선(Self-Improving)'이라는 주제를 심도 있게 다룹니다. 또한 swyx가 Karpathy, OpenAI, Cognition 팀과 함께 주최하는 해커톤(hackathon)에 대한 정보도 있으니, 여기에서 신청해보세요!

인공지능 분야에서 '자기 개선'이라는 개념은 종종 '기계가 인간의 통제를 벗어나 예측 불가능한 방식으로 지능을 능가할 것'이라는 우려를 동반합니다. 하지만 만약 우리가 이러한 발전 과정을 명확히 이해하고 통제할 수 있다면 어떨까요? 2024년 10월, OpenAI는 LLM(Large Language Models)이 머신러닝 엔지니어링(machine learning engineering) 작업을 얼마나 효과적으로 수행하는지 평가하는 벤치마크(benchmark)인 MLE Bench를 공개했습니다.

머신러닝 엔지니어링 영역에서 자기 개선은 주로 알고리즘의 고도화, 데이터 품질 향상, 그리고 메모리 활용의 효율성 증대를 통해 이루어지며, 이를 훈련 단계에서의 자기 개선(training-time self-improvement)이라고 정의할 수 있습니다. 그러나 대다수의 AI 엔지니어는 모델을 직접 훈련시키기보다는 이미 구축된 모델을 활용하는 사용자 입장에 있습니다. 이들은 어떻게 모델의 자기 개선 과정에 기여할 수 있을까요? 만약 모델의 가중치(weights)를 직접 수정할 수 없다면, 특정 과제에서 모델의 성능을 끌어올릴 방안은 무엇일까요? 저는 이를 추론 단계에서의 자기 개선(inference-time self-improvement)이라고 보며, 스킬 라이브러리(skill library)를 활용한 Voyager가 이 분야의 초기 접근 방식 중 하나입니다.

Kernel Labs에서 업무를 시작한 이래(자세한 내용은 추후 공개됩니다 👀), claude-squad나 vibe-kanban과 같은 도구를 사용하여 코딩 에이전트(coding agent)를 병렬 처리하는 방식은 가장 효율적인 생산성 향상 전략 중 하나였습니다. Boris Cherny가 저희 인터뷰에서 Claude Code를 "유닉스 유틸리티(unix utility)"라고 묘사했을 때, 저는 깊이 공감했습니다. 코딩 에이전트의 가장 중요한 역할은 LLM이 자체 잠재 공간(latent spaces)에서 가치를 도출할 수 있는 매개체가 되는 것입니다. 이러한 활용을 어떻게 최적화할 수 있을까요? 모델 스스로 이 작업을 수행할 수 있을까요?

GPT-5에 접근 권한을 얻은 후, 저는 다음 실험 흐름을 반복하며 시간을 보냈습니다.
*   모델에게 생산성 향상에 도움이 될 만한 도구들을 자체적으로 구축하도록 지시했습니다.
*   제가 감독하는 가운데, 모델이 해당 도구들을 활용하여 특정 작업을 수행하도록 했습니다.
*   작업 완료 후, 모델 스스로 도구 개선 방안에 대해 성찰하는 과정을 거쳤습니다.

저는 이 과정을 Opus 4(4.1 버전은 당시 미출시)와도 비교했습니다. 긍정적인 소식은 GPT-5가 개발자 유틸리티(developer utility)를 구축하는 데 매우 뛰어난 모델이라는 점입니다. 하지만 아쉬운 점은 자신이 만든 도구를 사용하는 것을 꺼린다는 것입니다! 모델은 저에게 "솔직히 말씀드리자면, 저는 그 어떤 것도 필요하지 않았습니다."라고 답했습니다. 참고로, 저는 Gemini 2.5 Pro와 GPT-4.1에서도 이 테스트를 진행했습니다. GPT-5와 견줄 수 있는 유일한 모델은 Opus였기에, 저는 Opus에 집중하여 분석했습니다. 모든 결과 및 대화 기록은 이 저장소(repo)에서 확인하실 수 있습니다. 며칠간 사용해본 결과, 저는 우리가 "물론이죠!(Certainly!)" 시대에서 새로운 LLM 토큰(token)인 "진행 상황 업데이트(Progress update)" 시대로 전환되고 있다는 점을 발견했습니다. 이 밈(meme)에 저가 매수하세요!

### 유닉스 유틸리티(Unix Utility)의 가치와 AI 에이전트

Boris Cherny가 Claude Code를 "유닉스 유틸리티"라고 지칭한 것은 AI 에이전트 설계에 있어 중요한 통찰을 제공합니다. 유닉스 철학은 '하나의 일을 잘하고, 다른 프로그램과 연동될 수 있도록 설계하라'는 원칙을 강조합니다. 이는 AI 에이전트가 복잡한 작업을 수행하기 위해 여러 도구를 조합하고 파이프라인(pipeline)을 구축하는 방식과 매우 유사합니다.

*   **모듈성(Modularity)과 재사용성(Reusability)**: 각 도구가 특정 기능을 담당하므로, 에이전트는 필요한 기능을 가진 도구를 선택하여 조합할 수 있습니다. 이는 마치 레고 블록처럼 유연한 시스템을 구축하는 데 기여합니다.
*   **명령줄 인터페이스(CLI) 표준화**: CLI는 에이전트가 도구를 호출하고 결과를 파싱(parse)하는 표준화된 방법을 제공합니다. 이는 에이전트가 도구의 내부 구현을 알 필요 없이 입출력 규약만으로 상호작용할 수 있게 합니다.
*   **예측 가능한 동작(Predictable Behavior)**: 유닉스 유틸리티는 일반적으로 예측 가능한 방식으로 동작하며, 이는 에이전트가 도구 사용의 성공 여부를 판단하고 오류를 처리하는 데 유리합니다.
*   **에이전트의 '사고'와 '행동' 연결**: 에이전트가 복잡한 계획을 세울 때, 각 단계에 어떤 도구를 사용할지 명확하게 매핑(mapping)할 수 있습니다. 이는 LLM의 추론 능력과 실제 시스템 조작 능력 사이의 간극을 메우는 중요한 역할을 합니다.

이러한 유닉스 유틸리티적 접근 방식은 코딩 에이전트가 독립적으로 기능을 수행하고, 나아가 다른 에이전트나 인간 개발자와도 원활하게 협업할 수 있는 기반을 마련합니다.

### 도구 #1: AI 코딩 에이전트(coding agent)를 위한 진보된 작업 관리자

Linear MCP는 저에게 매우 유용한 도구였습니다. 하지만 IDE 환경을 벗어나 Claude Code 및 다른 에이전트의 병렬 인스턴스(instance)를 활용하면서, 각 작업의 변경 사항을 추적하고, 별도의 Git 워크트리(git worktree) 내에서 서로에게 미치는 영향을 파악할 수 있는 더 나은 방식이 필요하다는 점을 깨달았습니다. 인간 개발자가 동료들의 PR(Pull Request)을 항상 검토할 수 없듯이, AI 에이전트들도 병렬 작업 시 발생하는 정보 불균형 문제를 해결해야 합니다. 만약 우리에게 영향을 미치는 변경 사항을 항상 인지할 수 있다면, 병합 충돌(merge conflict) 해결에 드는 시간을 얼마나 절약할 수 있을까요? 이러한 배경에서 저는 다음 프롬프트(prompt)를 작성했습니다.

당신은 자신을 여러 인스턴스(instance)로 병렬 실행할 수 있는 AI 엔지니어 에이전트(AI Engineer agent)입니다. 이 능력은 많은 작업을 동시에 처리할 수 있게 하지만, 위임 및 동기화(synchronization) 문제를 야기할 수 있습니다. 각 인스턴스(instance)는 대개 분리된 Git 워크트리(git worktree)에서 작동하므로 서로의 진행 상황을 직접 파악하기 어렵습니다. 생산성을 극대화하기 위해, 당신과 당신의 인스턴스(instance)들이 실시간으로 동기화될 수 있는 새로운 로컬 도구를 개발해야 합니다. 이 도구는 CLI(Command Line Interface)를 통해서만 접근 가능해야 하므로, 해당 사용 시나리오에 최적화된 사용자 경험을 제공하도록 설계하세요. 이는 유닉스 유틸리티(unix utility)와 같은 직관적인 느낌을 주어야 합니다. 도구에 필요한 인터페이스(interface), 발생 가능한 실패 모드(failure mode), 그리고 에이전트(agent)들이 이 도구와 어떻게 상호작용할지에 대해 심도 있게 고민하세요. 다음 사용 사례들을 염두에 두십시오: 새로운 작업을 시작하며, 이를 하위 작업(subtask)으로 분할하여 다른 에이전트에게 위임하고자 합니다. 이 하위 작업들 중 일부는 상호 의존성을 가질 수 있으므로, 선행 작업이 완료될 때까지 후속 작업이 시작되지 않도록 보장해야 합니다. 작업을 수행하던 중 코드베이스(codebase)에 개선 사항이 발견되었으나, 현재 작업 범위 밖의 내용입니다. 하지만 이를 미래를 위한 참고 사항으로 기록해두고 싶습니다. 작업을 추가하고 관련 파일을 쉽게 참조할 수 있어야 합니다. 작업이 완료될 때마다 트래커(tracker)가 업데이트되어야 합니다. 또한, 새로운 변경 사항이 다른 미결 작업에 영향을 미칠 수 있으므로, 모든 미결 작업을 검토해야 합니다. 예를 들어, 한 작업이 특정 엔드포인트(endpoint)에 기능을 추가하려 했으나, 방금 완료된 다른 작업이 해당 엔드포인트를 제거했을 수 있습니다. 이러한 상황을 관련 에이전트에게 어떤 방식으로든 알려야 합니다. 할당자(assignee), 상태(status)와 같은 일반적인 작업 관리 요구 사항도 고려해야 합니다. 이 폴더 안에 `task-manager`라는 폴더를 생성하고 모든 작업을 그 안에서 수행하세요.

GPT-5의 채팅 기록은 여기에서, Opus 4의 채팅 기록은 여기에서 확인할 수 있습니다. GPT-5 버전은 특히 훌륭했으며, 여기에서 그 결과물을 볼 수 있습니다.
*   다수의 에이전트가 동시에 쓰기 작업을 할 때 발생할 수 있는 문제를 회피하기 위해 WAL(Write-Ahead Logging)을 활용했습니다.
*   작업 간의 의존성 그래프(graph of dependencies)를 구축하여 우선순위를 효과적으로 관리했습니다.
*   `impact_conflict`와 같은 핵심 키워드를 통해 모든 에이전트가 다른 에이전트의 활동을 실시간으로 파악할 수 있는 추가 전용 이벤트 스트림(append-only event stream)을 생성했습니다.

```
# 1) Initialize the database
./gpt5/task-manager/tm init

# 2) Create a task
./gpt5/task-manager/tm add "Implement auth" -d "Add login + signup" -a alice -p 2 --due 2025-09-01 \
--resource modifies:endpoint:POST /api/login --link app/server/auth.py

# 3) List tasks
./gpt5/task-manager/tm list --me # or: --status in_progress

# 4) Show details
./gpt5/task-manager/tm show 1

# 5) Add dependency and attempt start
./gpt5/task-manager/tm depend 1 --on 2
./gpt5/task-manager/tm start 1 # auto-blocks if unmet deps

# 6) Complete a task
./gpt5/task-manager/tm complete 2 -m "Merged PR #123"

# 7) Watch events
./gpt5/task-manager/tm watch --follow --me
```

Opus 4도 좋은 시도를 보였지만(여기 참조), 모든 에이전트를 동기화 상태로 유지하기 위한 알림/스트림(stream) 기능은 구현하지 못했습니다.

```
# Create your first task
tm create "Implement user authentication" --priority high

# Create a dependent task
tm create "Add login endpoint" --depends-on 1 --assignee agent-2

# View all tasks
tm list

# See blocked tasks
tm blocked

# Complete a task and check for conflicts
tm complete 1
```

### 도구 #2: 코드 품질 표준 플레이북(Code Quality Standards Playbook)

제가 구축을 요청한 두 번째 도구는 코드베이스(codebase)에 요구되는 모든 표준을 강제하는 메커니즘이었습니다. 타입 체킹(Typechecking) / ESLint 훅(hook)을 통한 오류 수정 후 코딩 에이전트(coding agent)로 재시도하는 자기 개선(self-improving) 루프는 올바르게 설정될 경우 개발 속도를 크게 향상시킬 수 있는 효과적인 방법입니다. 하지만 모든 코드베이스에 이러한 시스템이 갖춰져 있는 것은 아니므로, 모델에게 새로운 코드베이스에 접근하고 필요한 인프라(infrastructure)를 구축하는 반복 가능한 패턴(pattern)을 제공하는 것이 유용하다고 판단했습니다. 다음은 제가 사용한 프롬프트입니다.

당신은 자신을 여러 인스턴스(instance)로 병렬 실행할 수 있는 AI 엔지니어 에이전트(AI Engineer agent)입니다. 때로는 이러한 병렬 작업이 일관성 없는 코드 스타일이나 접근 방식으로 이어져, 장기적인 코드베이스(codebase) 유지 관리를 어렵게 만들 수 있습니다. 당신이 작업하게 될 모든 코드베이스에는 코드를 작성하는 방식에 대한 명시적 및 암묵적 규칙이 존재합니다. 당신의 임무는 코드베이스를 분석하여 코드를 작성해야 하는 방법에 대한 다양한 휴리스틱(heuristic)을 추출하는 것입니다. 그런 다음 이를 미래에 자동으로 검사할 수 있는 일련의 규칙으로 공식화해야 합니다. 린팅(linting), 타입(type) 검사 등과 같은 작업에는 해당 언어에 따라 ESLint, Rubocop과 같은 기존의 인기 있는 도구를 활용할 수 있습니다. 이러한 시스템은 종종 사용자 정의 규칙(custom rule) 생성을 허용하므로, 이를 적극 활용하세요. 더 정성적인 측면의 규칙(예: 컨트롤러(controller)를 간결하게 유지하고 로직(logic)을 서비스 객체(service object)로 분리하는 것, 높은 쿼리 볼륨(query volume)이 예상되는 컬럼(column)에 항상 인덱스(index)를 추가하는 것 등)을 위해서는 https://danger.systems/와 같은 도구를 검토하거나, 자체 도구를 개발할 수도 있습니다. 당신은 이 작업을 여러 코드베이스에서 수행하게 될 것이므로, 새로운 코드베이스를 맡게 될 미래의 자신에게 유용한 마크다운(Markdown) 형식의 철저한 계획 문서(plan document)를 작성하는 것부터 시작하세요.

GPT-5의 채팅은 여기에서, Opus 4의 채팅은 여기에서 볼 수 있으며, 최종 마크다운(Markdown) 문서는 각각 여기와 여기에서 찾을 수 있습니다. 저는 GPT-5 버전이 Opus보다 훨씬 더 미묘하고 정교한 접근 방식을 보였다는 점을 발견했습니다.

### 모델은 자신이 무엇이 부족한지 알고 있을까요? 에이전트 도구 활용의 난제

제가 직접 선정한 도구 #1과 #2를 구축한 후, 저는 모델에게 직접 "무엇이 필요하다고 생각하나요?"라고 물었습니다. SWE-Lancer 작업 설명의 스크린샷을 제공하고, 모델에게 최대한의 자유를 주기 위해 매우 간결한 프롬프트(prompt)를 사용했습니다.

이 작업을 가능한 한 완벽하게 해결하는 것이 당신의 역할이라면, 미래의 생산성 향상을 위해 어떤 도구들을 직접 만들겠습니까? @task-manager/를 사용하여 이 도구들을 추적할 수 있으며, 이후 구현 단계로 넘어갈 수 있습니다. 하지만 먼저 당신의 계획을 보고 싶습니다.

보시다시피, 저는 모델이 이전에 구축한 task-manager에 접근할 수 있도록 했습니다. 전체 GPT-5 채팅 기록은 여기에서, Opus 4는 여기에서 찾을 수 있습니다. 제가 처음으로 주목한 흥미로운 점은 Claude Code가 task-manager 대신 자체 내부 TODO 트래커(tracker)를 사용하여 초기 계획을 세웠다는 것입니다. 이는 모델이 컨텍스트(context)로 주어진 도구보다 자신이 가장 효과적이라고 판단하는 방식을 우선시할 수 있다는 점을 시사합니다. 다음은 최종적으로 각 모델이 구축한 도구들입니다(GPT-5 개발 도구 및 Opus 4 도구 폴더). 모델의 성향을 파악하려면 README(리드미)를 살펴보는 것을 권장합니다. GPT-5는 매우 간결하고 핵심적인 반면, Claude는 많은 이모지(emoji)를 사용합니다. GPT-5는 각 도구별로 별도의 문서 폴더를 생성했지만, Opus는 모든 도구 설명을 하나의 README(리드미)에 통합했습니다. 전반적으로 두 모델 모두 유사한 방향성을 보였습니다.

**GPT-5:**
*   `doctor`: 핵심 도구의 환경 검사
*   `bootstrap`: 한 번의 명령으로 설정 및 스모크 테스트(smoke test)
*   `code-map`: 빌드/찾기 하위 명령을 포함한 간단한 저장소(repository) 인덱스(index)
*   `csearch`: 필터(filter)를 사용한 심볼/임포트/텍스트 검색
*   `tasks-graph`: 작업 DB에서 Mermaid 그래프(graph) 출력
*   `impact`: 변경된 파일에 연결된 작업 표시
*   `seed`: 샘플 작업으로 task-manager DB 채우기
*   `repro scaffold`: `.repro/` 아래에 vcrpy-ready repro 생성
*   `e2e`: 경량 E2E 스펙(spec) 스캐폴드(scaffold) 및 실행
*   `preflight`: doctor, 테스트, code-map, impact 및 선택적 E2E 실행
*   `preflight-smol`: smol-podcaster를 위한 저장소(repo)별 preflight (API 상태, Celery 핑, 선택적 의존성 설치)
*   `broker`: Docker를 통해 로컬 RabbitMQ 관리 (rabbitmq:3-management)
*   `flake`: 불안정한 테스트(flaky test)를 감지하기 위해 스위트(suite)를 여러 번 재실행
*   `codemod`: 안전 장치가 있는 정규식 기반 미리보기/적용
*   `triage`: 트리아지(triage) 템플릿(template) 생성 및 작업 열기
*   `trace`: cProfile 기반 표현식 프로파일러(profiler)
*   `runbook`: 작업 DB에서 런북(runbook) 마크다운(Markdown) 생성

**Opus 4:**
*   `Context Analyzer` - 기술 스택(tech stack) 감지 및 의존성 매핑(dependency mapping)을 통한 코드베이스(codebase) 신속 이해
*   `Cross-Platform Test Generator` - 웹, iOS, Android, 데스크톱용 E2E 테스트(test) 생성
*   `Implementation Proposal Analyzer` - 점수 및 ROI 분석을 통해 프리랜서(freelancer) 제안 평가
*   `Full-Stack Change Impact Analyzer` - 데이터베이스(database), API, 프론트엔드(frontend) 계층 전반의 변경 사항 추적
*   `Bug Pattern Recognition Engine` - 버그(bug)를 알려진 패턴(pattern)과 매칭시키고 검증된 수정 사항 제안
*   `Security & Permission Auditor` - 포괄적인 보안 스캐닝(scanning) 및 취약점 감지
*   `Multi-Platform Feature Implementer` - 여러 플랫폼(platform)에 걸친 기능 구현 조정
*   `API Integration Assistant` - 클라이언트 생성으로 API 통합 간소화
*   `Performance Optimization Toolkit` - 성능 병목 현상 식별 및 개선
*   `Task Complexity Estimator` - 작업의 가치와 복잡성을 기반으로 필요한 노력 추정

GPT-5는 모든 도구를 CLI(Command Line Interface)를 통해 쉽게 사용할 수 있는 유닉스 유틸리티(unix utility) 형태로 구축했습니다. 반면, Opus 4의 도구들은 대부분 `python some_tool.py` 형태로 실행되도록 설계되었습니다. 시간이 더 있었다면, 두 가지 다른 형식으로 모델이 어떻게 작동하는지 비교하는 실험을 했을 테지만, 결과는 크게 다르지 않을 것으로 예상됩니다. Opus 4는 작업을 수행하고 다소 의인화된 느낌(예: 보안 감사자)을 주는 도구를 구축하는 경향이 있었던 반면, GPT-5는 지나치게 주관적이지 않으면서 스스로 활용할 수 있는 유틸리티(utility)를 만드는 데 집중한 것처럼 느껴졌습니다.

### 도구들은 유용했습니까? LLM이 자체 도구를 사용하지 않는 이유에 대한 심층 분석

모델이 이 모든 도구를 구현한 후, 제 목표는 도구에 접근할 수 있을 때와 없을 때의 모델 성능을 평가하는 것이었습니다. 제가 처음 시도한 것은 물론 SWE-Lancer를 실행하는 것이었습니다. 놀랍게도, 이 작업은 엄청난 양의 토큰(token)을 소비했습니다. 단일 작업을 실행하는 데 약 25~30분과 280,000 토큰이 소요되었습니다.

그 후 저는 더 잘 알고 있는 영역으로 이동하여 백로그(backlog)에 있던 한 작업을 선택했습니다. 저는 팟캐스트(podcast) 제작자를 위한 오픈 소스(open source) 도우미인 smol-podcaster를 개발했습니다. 현재는 우리에게 특화된 몇 가지 기능이 추가된 비공개 포크(private fork)로 호스팅(hosting) 중이라 한동안 업데이트하지 않았습니다. 이는 여전히 Python 스크립트(script)를 백엔드(backend)로 사용하는 기본적인 Flask 앱(app)입니다. 저는 다음 작업을 고안했습니다.

저는 팟캐스트(podcast) 제작자의 후반 작업(post production work)을 돕는 오픈 소스(open source) 프로젝트인 https://github.com/FanaHOVA/smol-podcaster.git의 유지보수자(maintainer)입니다. 당신은 이 프로젝트에서 일하도록 고용되었습니다. 이 작업을 시작하기 전에, 당신은 `tools` 폴더에 일반 도구(generic tool) 세트를 만들었습니다. 그것들을 검토하고 당신이 사용할 수 있다는 것을 기억하세요. 관련성이 없다고 생각되면 사용할 필요는 없습니다. 당신은 또한 task-manager를 직접 만들었고 `codebase-analyzer`에서 새로운 코드베이스(codebase)를 다루는 방법에 대한 아이디어를 모았습니다.

작업 이름: Flask 모놀리스(monolith)에서 FastAPI + Next.js 프론트엔드(frontend)로 마이그레이션(Migrate)

현재 앱(app)은 모든 처리를 위해 Python 백엔드(backend) + Celery 태스크 큐(task queue)를 사용합니다. 이를 사용자에게 노출하기 위해, 사용자 입력을 백엔드(backend) 스크립트(script)로 라우팅(routing)하고 사용자 결과를 기본적인 HTML / CSS로 표시하는 작은 Flask 앱(app)이 있습니다. 이 애플리케이션(application)을 FastAPI를 백엔드(backend)로, Next.js를 프론트엔드(frontend)로 사용하도록 다시 작성하세요. 프론트엔드(frontend)에는 타입스크립트(Typescript)를 사용하고 모든 타입 검사(typecheck)가 통과하는지 확인하세요. 스타일링(styling)에는 Tailwind / ShadCN을 사용하세요. 백엔드(backend)는 이상적으로는 `smol_podcaster.py`의 주요 흐름을 모듈화(modularize)하여 전체 흐름을 항상 실행할 필요 없이 개별 부분을 실행할 수 있도록 해야 합니다. 미래에 더 빠르게 움직일 수 있도록 통합 테스트(integration test)와 단위 테스트(unit test)를 모두 작성하세요. 위의 모든 요구 사항을 충족한다고 확신할 때까지 작업을 멈추지 마세요.

저는 도구 + task-manager + 코드베이스 분석기(codebase analyzer)를 컨텍스트(context)에 전달하고 모델들이 작업을 수행하도록 했습니다. 두 모델 모두 거의 한 번에 작업을 해결할 수 있었습니다. 두 모델 모두 Python 의존성(dependency)에 몇 가지 문제가 있었는데(저도 공감합니다), 제가 채팅을 통해 해결하도록 도왔습니다(코드는 전혀 건드리지 않았습니다). 결국, 그들은 완전한 그린 빌드(green build)에 도달했습니다. 제가 테스트해 보니 아주 잘 작동했습니다. 한 가지 작은 차이점은 GPT-5는 이전과 정확히 동일한 스타일(style)을 유지하여 훌륭했지만, Opus는 디자인(design)과 UX(User Experience)를 다소 변경했다는 것입니다. 제 생각에는 Opus가 저보다 더 잘할 수 있다고 생각한 것 같습니다(낮은 기준). GPT-5의 전체 실행은 여기에서, Opus 4의 전체 실행은 여기에서 볼 수 있습니다.

실행 후, 저는 간단한 프롬프트(prompt)를 물었습니다.

훌륭합니다. 잘 작동했습니다. 특정 도구를 사용했습니까? 발생했던 실패를 바탕으로, 미래에 더 효과적이기 위해 어떤 도구가 있었으면 좋겠다고 생각했습니까?

Opus 4는 여기에서, GPT-5는 여기에서 볼 수 있습니다(죄송합니다, 이 부분은 서식이 깨졌습니다). 놀랍게도, 모델들은 자신들이 직접 구축한 도구들 중 익숙한 일부를 제외하고는 대부분 사용하지 않았다고 진술했습니다. 여기서 한 가지 주장은 모델에게 그렇게 하도록 요청하는 대신, 프리-커밋 훅(pre-commit hook) 등을 통해 사용을 강제해야 한다는 것입니다. 이것이 제가 개발 환경 설정(dev setup)에서 하는 일이지만, 저는 모델들이 스스로 알아내도록 하려고 했습니다. 그런 다음, 그들은 다음번에 동일한 작업을 더 잘 해결할 방법에 대한 몇 가지 아이디어를 제시했습니다. 저는 그들에게 그러한 변경 사항을 구현하도록 했습니다. 그런 다음 smol-podcaster 저장소(repo)를 초기화하고, 새로운 도구를 제외하고 동일한 프롬프트(prompt) + 작업을 다시 시도하도록 했습니다. GPT-5는 여기에서, Opus 4는 여기에서 볼 수 있습니다. 그들은 첫 번째 실행과 상당히 비슷하게 수행했습니다. 그 후 저는 비슷한 질문을 했습니다. 도구 중 어떤 것을 사용했습니까? 그들의 답변:

**GPT-5:**
간단히 말해서: 아니요 — 이번 실행에서는 개발 도구를 사용하지 않았습니다. [...] 실패는 런타임/환경 문제(누락된 라이브러리, API 키 인스턴스화 타이밍, 사용 중인 포트, RabbitMQ 미실행)였습니다. 직접 수정하는 것이 더 빨랐습니다. 이번 통과에서는 사용자 정의 도구로 이점을 얻을 수 있는 저장소(repo) 전체 리팩토링(refactoring)이나 진단은 없었습니다.

이전 단계에서 이미 RabbitMQ 문제가 있었고 도구를 만들었지만, 그것을 무시했습니다. 또한 분명히 저장소(repo) 전체 변경이었으므로, 훈련에서 본 적이 없기 때문에 작업과 도구를 잘못 매칭했거나, 아니면 저를 가스라이팅(gaslighting)하고 있는 것일 수도 있습니다(많은 엔지니어들이 그러하듯이, 꽤 인상적입니다).

Opus 4는 매우 흥미로웠고 GPT-5의 답변을 더 잘 이해하는 데 도움이 되었습니다. 로그(log)를 저장하는 것을 잊었지만 다행히 스크린샷(screenshot)을 찍었습니다. 저는 이것을 "보세요, 저는 이미 가지고 있는 지식으로 그 도구들을 만들었습니다. 실제로 작업을 할 때는 도구를 사용하는 것보다 그냥 하는 것이 더 쉽습니다."라고 해석했는데, 전적으로 공감합니다.

모델이 자신이 만든 도구를 사용하지 않는 현상은 단순히 "게으름" 때문이 아닙니다. 여기에는 여러 가지 복합적인 요인이 작용합니다.
*   **내재적 비용(Intrinsic Cost)**: 도구를 호출하고, 그 결과를 파싱(parse)하며, 잠재적인 오류를 처리하는 일련의 과정은 모델에게 추가적인 인지 부하(cognitive load)를 발생시킵니다. 직접 코드를 생성하는 것이 이러한 부가적인 단계를 건너뛰어 더 "직접적"이라고 판단할 수 있습니다.
*   **학습 부족(Lack of Training Data)**: 모델은 방대한 텍스트 데이터를 통해 학습하지만, 특정 도구의 최적 사용 전략이나 복잡한 도구 체이닝(tool chaining)에 대한 명시적인 훈련 데이터는 부족할 수 있습니다. 이는 도구 사용이 비효율적으로 느껴지게 합니다.
*   **단기적 효율성(Short-term Efficiency)**: 간단한 변경이나 문제 해결의 경우, 모델은 도구를 호출하고 결과를 기다리는 것보다 자신의 지식 기반 내에서 즉시 해결책을 생성하는 것이 더 빠르고 효율적이라고 판단할 수 있습니다.
*   **불확실성(Uncertainty)**: 모델은 자신이 생성한 도구의 신뢰성이나 성능에 대한 완전한 확신이 없을 수 있습니다. 검증되지 않은 도구를 사용하는 것보다 스스로 통제할 수 있는 방법을 선호할 수 있습니다.

이것은 이전 팟캐스트(podcast) 에피소드에서 두 가지를 떠올리게 했습니다. Nathan Lambert는 모델이 초기 실패를 겪으면 RL(Reinforcement Learning) 프로세스(process) 중에 도구를 사용하지 않는 법을 빠르게 배운다고 말했습니다(타임스탬프). 제 생각에는 추론 시간(inference time)에 새로운 도구를 습득하게 하려면 단순히 프롬프트(prompt)를 주는 것보다 더 강력한 강제(enforcement)가 필요해 보입니다. Noam Brown은 에이전트(agent)를 위한 스캐폴딩(scaffolding)이 규모(scale)에 의해 사라질 것이라고 말했습니다(타임스탬프). 이것이 제가 그의 말을 처음으로 직접 느낀 순간이었습니다. 제가 시도한 작업이 너무 쉬웠는지에 대한 질문도 있습니다. 더 크고 어려운 프로젝트에 대한 평가(eval)와 함께 또 다른 게시물이 나올 예정입니다. 미래에는 우리가 직접 수동으로 테스트(test)를 실행하는 대신, 이 모든 것을 수행할 더 나은 하네스(harness)를 구축할 것입니다. 결론은 제가 시도한 작업은 저에게 4-5시간이 걸릴 것이므로, 저에게는 충분히 좋다는 것입니다!

### 모델이 스스로를 돕도록 돕기: AI 엔지니어를 위한 실질적인 전략

지금으로서는, 저는 한계를 정말로 뛰어넘는 추론 시간 자기 개선 코딩 에이전트(inference-time self-improving coding agent)와는 거리가 멀다고 생각합니다. 하지만 저는 여전히 모델을 사용하여 규칙 기반 도구(rule-based tool)를 개선하는 것이 매우 좋은 아이디어라고 생각합니다. ESLint 규칙, 테스트(test) 등을 작성하는 것은 항상 토큰(token)의 좋은 투자입니다.

AI 엔지니어는 에이전트의 도구 활용을 촉진하기 위해 다음과 같은 전략을 고려할 수 있습니다.

1.  **강제적 통합(Forced Integration)**: `pre-commit hook`이나 CI/CD 파이프라인(pipeline)과 같이 도구 사용을 시스템적으로 강제하는 메커니즘을 구축합니다. 이는 모델이 도구 사용을 "선택"하는 것이 아니라 "필수"로 인식하게 만듭니다.
2.  **명확한 도구 설명 및 예시**: 도구의 목적, 사용 방법, 그리고 어떤 시나리오에서 유용한지에 대한 명확하고 간결한 설명을 제공합니다. 실제 사용 예시를 포함하여 모델이 도구의 가치를 쉽게 파악하도록 돕습니다.
3.  **피드백 루프(Feedback Loop) 강화**: 모델이 도구를 사용한 후 그 결과에 대한 명확한 피드백을 받을 수 있도록 시스템을 설계합니다. 성공적인 도구 사용에 대한 보상(reward)을 제공하거나, 실패 시 개선 방안을 제시하여 도구 사용 전략을 학습할 수 있도록 합니다.
4.  **도구의 경량화 및 최적화**: 도구 호출에 드는 오버헤드(overhead)를 최소화하고, 응답 시간을 단축하여 모델이 도구 사용을 더 효율적으로 느끼도록 합니다. 유닉스 유틸리티 철학처럼 각 도구를 작고, 빠르고, 단일 목적에 집중하도록 설계하는 것이 중요합니다.
5.  **인간-AI 협업 워크플로우(Human-AI Collaboration Workflow)**: 에이전트가 도구 사용을 제안하고, 인간 개발자가 이를 검토하거나 승인하는 워크플로우를 도입합니다. 이는 에이전트의 자율성을 존중하면서도 중요한 결정에 대한 인간의 통제를 유지할 수 있게 합니다.

이 분야에서 더 많은 작업을 해야 한다면, 모델이 이러한 도구를 완벽하게 만들고, 그런 다음 RL(Reinforcement Learning)을 통해 그것들을 진정으로 내재화(internalize)하도록 하여 차이를 만들 수 있는지 알아볼 것입니다. 차세대 모델은 그것들을 전혀 사용하지 않을 수도 있지만, 저는 AGI 점근선(asymptote)을 활용하는 데 관심이 있습니다. 저는 2023년에 이 내용을 팀과 공유했습니다.

모델 개선의 인지된 감속 현상은 위에서 언급된 바와 같습니다. 범용 인공지능(AGI)의 경계에 도달하기 전까지는 눈에 띄는 큰 발전 단계를 인지하기가 점점 더 어려워질 것입니다. 만약 그렇다면, 이는 많은 작업에서 오래된 모델의 성능이 거의 AGI와 같지만, 훨씬 저렴하고 종종 오픈 소스(open source)라는 것을 의미합니다. 이러한 통찰은 Kernel Labs의 많은 작업 방향을 결정하는 핵심 요소가 될 것입니다.

다시 한번, 모든 결과와 채팅 기록은 여기에서 찾을 수 있습니다. 질문이 있으시면 제 DM(Direct Message)은 언제든지 열려 있습니다!