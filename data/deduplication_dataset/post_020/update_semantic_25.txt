지난 한 달간 인공지능 분야에서는 수많은 중대한 발전이 전개되었습니다. 애플의 온디바이스(on-device) 거대 언어 모델(LLM) 통합 발표, 엔비디아의 대규모 네모트론(Nemotron) 계열 모델 공개, FlashAttention-3의 등판, 그리고 구글의 Gemma 2 출시 등 다채로운 소식들이 쏟아져 나왔습니다. 이러한 주요 사건들은 이미 여러 언론 매체를 통해 독자 여러분께 충분히 알려졌으리라 생각합니다. 거대 언어 모델(LLM) 생태계는 전례 없는 속도로 진화하고 있으며, 매일 새로운 모델과 기술들이 등장하고 있습니다. 이러한 변화의 물결 속에서, 어떤 기술이 진정으로 혁신적이며 지속 가능한 영향을 미 미칠지 파악하는 것은 쉽지 않은 일입니다. 이에 본고에서는 LLM 학습의 핵심 기반 기술인 명령어 미세 조정(instruction finetuning)에 초점을 맞춘 최신 연구 동향을 심층적으로 탐색해보고자 합니다. 특히, 데이터 효율성, 모델 성능 향상, 그리고 실용적인 적용 가능성에 중점을 둔 연구들을 다룰 것입니다.

이 글에서 다룰 핵심 내용은 다음과 같습니다:
*   명령어 미세 조정을 위한 데이터 구축의 혁신적이고 경제적인 접근법
*   기반부터 시작하는 명령어 미세 조정 기법의 심층 분석
*   명령어 기반 데이터를 활용한 LLM 사전 학습의 새로운 패러다임
*   Gemma 2 모델의 주요 개선점 및 아키텍처적 특징 분석
*   지난 6월 공개된 주목할 만한 기타 연구 성과들

흥미로운 통찰력을 얻으시길 바랍니다!

## 1. 기반부터 정렬 데이터(Alignment Data) 생성하기

"The Magpie: Alignment Data Synthesis from Scratch by Prompting Aligned LLMs with Nothing" 논문은 LLM 명령어 미세 조정(instruction finetuning)을 위한 고품질 데이터셋을 생성하는 기발하고 효율적인 방법을 제시합니다. 이 접근 방식이 완전히 새로운 이론적 통찰력을 제공하는 것은 아니지만, 실제 적용 가능성이 매우 높은 흥미로운 실용적 활용 사례 중 하나로 주목받고 있습니다.

### 1.1 무(無)의 상태에서 명령어 데이터 집합(Instruction Dataset) 구축하기

이 명령어 데이터 생성 기법은 여타 접근 방식과 달리 전적으로 자동화가 가능하며, 선행 질문이나 구체적인 지시사항이 요구되지 않는다는 점에서 차별화됩니다. 해당 논문의 제목이 암시하듯, 이 방법론은 '무(無)'의 상태에서 명령어 데이터 집합을 만들어낼 수 있도록 합니다. 오직 필요한 것은 국지적으로 가동되는 Llama 3 8B 모델뿐입니다. 이는 소규모 연구팀이나 개인 개발자에게도 고품질 데이터셋 접근성을 크게 향상시킨다는 점에서 비용 효율적이고 민주적인 접근법이라 할 수 있습니다.

아래 도식은 이 방법의 작동 원리를 요약합니다.

명령어 미세 조정을 위한 합성 데이터셋을 생성하는 Magpie 방법론의 주석이 달린 도식.
이 도식은 Magpie 논문의 도식을 기반으로 합니다: https://arxiv.org/abs/2406.08464

기본적으로, 위 그림에서 볼 수 있듯이, 우리는 Llama 3 8B Instruct 모델에 사전 정의된 쿼리 템플릿(pre-query template)으로 프롬프트(prompt)를 제공하기만 하면, 모델이 우리를 위한 명령어를 생성합니다. 그 다음, 생성된 명령어를 다시 LLM에 입력하면, 모델이 응답을 생성하는 방식입니다. 이 과정을 수천 번 반복하면, 명령어 미세 조정을 위한 방대한 데이터셋을 확보할 수 있습니다. (선택적으로, LLM을 활용하여 생성된 명령어-응답 쌍(instruction-response pairs)의 품질을 필터링하는 과정을 추가할 수 있습니다.) 이 "아무것도 없는" 상태에서의 시작은 초기 시드(seed) 프롬프트 없이도 모델 자체의 언어 이해 능력을 활용하여 다양한 질문과 답변을 자율적으로 생성해낸다는 점에서 혁신적입니다.

### 1.2 데이터셋 품질 및 성능 우위

주목할 만한 사실은, 연구진이 자체적으로 생성한 명령어 데이터 집합을 활용하여 Llama 3 8B 기반 모델을 오직 명령어 미세 조정만으로(강화 학습 기반 선호도 조정이나 DPO 기법 없이) 훈련시켰을 때, Meta AI의 원조 Llama 2 8B Instruct 모델의 성능을 상회한다는 점을 입증했다는 것입니다. 이는 다음 도표에서 확인할 수 있습니다. 이는 합성 데이터의 품질이 RLHF나 DPO와 같은 복잡하고 자원 소모적인 선호도 학습 과정 없이도 실제 모델의 성능을 크게 향상시킬 수 있음을 시사합니다.

Magpie가 생성한 명령어 데이터셋으로 미세 조정된 Llama 3 8B 기본 모델이 오리지널 Llama 3 8B Instruct 모델을 능가합니다.
Magpie 논문의 주석이 달린 도식을 기반으로 합니다: https://arxiv.org/abs/2406.08464

위 그림에 나타난 Magpie의 인상적인 결과는 단 30만 개의 샘플(samples)로 달성되었습니다. 이에 비해, 오리지널 Llama 3 Instruct 모델은 1억 개의 샘플로 미세 조정 및 정렬(aligned)되었다는 점을 고려하면, Magpie 방법론의 데이터 효율성은 가히 혁신적이라 할 수 있습니다. 이는 고품질의 합성 데이터가 방대한 양의 수동으로 레이블링된 데이터를 대체할 수 있음을 보여주는 강력한 증거입니다.

### 1.3 데이터셋 생성 로컬에서 실행하기

처음에는 회의적이었지만, 직접 구현해 보았습니다. 정말 작동합니다! 여기에서 Ollama를 사용한 제 재구현(reimplementation)을 찾을 수 있으며, 이는 MacBook Air에서도 로컬로 잘 실행됩니다. 이러한 접근 방식은 클라우드 자원에 대한 의존도를 줄이고, 개인 개발자나 소규모 팀이 자체적으로 LLM을 실험하고 개선할 수 있는 문을 열어줍니다.

로컬에서 실행되는 Magpie 방법의 재구현 코드 스크린샷.
코드는 여기에서 확인할 수 있습니다.

### 1.4 추가 세부 정보: Air vs. Pro 버전 및 다양성

저자들은 두 가지 데이터셋 세트를 만들었습니다: Llama 3 70B Instruct 모델을 사용한 "Pro" 버전과 Llama 3 8B Instruct 모델을 사용한 "Air" 버전입니다. Pro 버전은 더 강력한 모델을 기반으로 하기에, 생성되는 데이터의 복잡성이나 품질에서 미묘한 차이를 보일 수 있습니다.

이전 그림에서 보았듯이, Magpie-Pro가 생성한 데이터셋은 Llama 3 8B 기본 모델을 명령어 미세 조정(instruction-finetune)하는 데 사용될 때 Magpie-Air 데이터셋에 비해 약간 더 강력한 모델을 만듭니다. 이는 더 큰 모델이 더 정교하고 다양한 명령어를 생성할 수 있음을 시사합니다.

아래 그림은 LLM을 통해 평가된 데이터셋 품질 및 난이도에 대한 추가 비교를 보여줍니다.

Magpie 논문의 주석이 달린 플롯(plots)으로, Air 및 Pro 데이터셋의 상대적인 데이터셋 품질과 난이도를 보여줍니다.

위 그림에서 볼 수 있듯이, Air 및 Pro 데이터셋의 품질은 대략 동등합니다. 또한, Alpaca 데이터셋이 이들과 어떻게 비교되는지 보는 것도 흥미로웠을 것입니다. (Magpie 데이터가 Alpaca보다 훨씬 고품질이라는 가정이 있지만, 참조 지점(reference point)이 있다면 흥미로울 것입니다.) 더욱이, 이 논문은 이 데이터셋의 폭 또는 다양성(breadth or diversity)이 Alpaca, Evol Instruct, UltraChat와 같은 다른 인기 있는 명령어 미세 조정 데이터셋보다 훨씬 크다는 분석을 포함합니다. 이는 Magpie가 단순한 데이터 증강을 넘어, 모델의 일반화 능력을 향상시킬 수 있는 광범위한 시나리오를 커버한다는 의미입니다.

또한, 다른 명령어 미세 조정 데이터셋으로 훈련된 모델과 비교했을 때, Magpie-Pro로 미세 조정된 모델도 매우 유리하게 비교됩니다. 이는 Magpie 방식이 합성 데이터 생성의 새로운 표준을 제시할 수 있음을 보여줍니다.

### 1.5 결론: Magpie의 잠재력과 한계

총체적으로 판단할 때, Magpie는 한편으로는 그 유효성에서 매력적이며, 다른 한편으로는 실용적 가치가 높은 흥미로운 응용 사례로 판단됩니다. 향후 일반적인 명령어 데이터 집합을 구성함에 있어 매력적이고, 단순하며, 경제적인 대안으로 분명히 검토될 가치가 있습니다. 특히, 자원 제약이 있는 환경에서 고품질의 미세 조정 데이터를 확보하는 데 있어 Magpie는 강력한 솔루션이 될 수 있습니다.

하지만, 합성 데이터가 갖는 내재적인 한계도 고려해야 합니다. 생성 모델의 편향이 데이터에 반영될 수 있으며, 특정 도메인에 대한 깊이 있는 이해나 인간적인 섬세함이 요구되는 작업에서는 여전히 실제 데이터의 필요성이 제기될 수 있습니다. 그럼에도 불구하고, Magpie는 합성 데이터 생성 기술의 발전을 통해 LLM 개발의 접근성을 넓히고, 새로운 연구 방향을 제시했다는 점에서 그 의의가 큽니다. 앞으로 Magpie와 같은 기술이 어떻게 발전하여 이러한 한계를 극복하고, 더욱 다양한 응용 분야에서 활용될지 주목됩니다.

## 2. 기반부터 명령어 미세 조정(Instruction Finetuning) 구현하기

거대 언어 모델(LLM)의 명령어 미세 조정 절차를 심층적으로 탐구하려는 분들을 위해, 마침내 Manning 웹사이트에 LLM 명령어 미세 조정에 관한 제 7장이 공개되었음을 알려드리게 되어 기쁩니다. 이 장은 단순한 이론 설명을 넘어, 실제 작동하는 미세 조정 파이프라인을 직접 구축하는 실질적인 경험을 제공합니다.

이 장은 본 저서에서 가장 방대한 분량으로, 명령어 미세 조정 파이프라인(pipeline)을 기초부터 직접 구축하는 방안을 제시합니다. 이는 입력 데이터의 형식화, 맞춤형 콜레이트(collate) 기능을 활용한 배치 처리(batching), 패딩 토큰(padding tokens)의 마스킹(masking), 실제 학습 반복(training loop) 구현, 그리고 맞춤형 평가 집합을 통한 미세 조정된 LLM의 응답 품질 평가에 이르는 전 과정을 포괄합니다. (연습 문제에는 프롬프트 스타일(prompt styles) 변경, 명령어 마스킹(instruction masking), LoRA 추가가 포함됩니다.)

이러한 '처음부터' 구현 방식은 단순한 라이브러리 사용을 넘어, 명령어 미세 조정의 각 단계에서 발생하는 문제점과 해결책을 깊이 있게 이해하는 데 필수적입니다. 특히, 모델의 내부 작동 방식과 데이터 흐름을 명확히 파악함으로써, 특정 애플리케이션에 최적화된 미세 조정 전략을 수립하는 데 필요한 기반 지식을 제공합니다. 또한, 기존 프레임워크의 한계를 넘어선 커스터마이징(customizing) 가능성을 열어줍니다.

즐거운 코딩 되시길 바랍니다!

제 책 "Build a Large Language Model From Scratch"의 7장 개요.
보충 코드 자료는 GitHub에서 여기에서 확인할 수 있습니다.

추신: 이 장은 또한 마지막 장이며, 출판사는 현재 인쇄 버전의 레이아웃(layouts)을 준비 중입니다. 책의 완성 단계에 접어들면서, 독자들이 LLM 구축의 전 과정을 일관된 흐름으로 이해할 수 있도록 구성하는 데 많은 노력을 기울였습니다.

### 2.1 명령어 미세 조정 구현의 도전 과제와 해법

명령어 미세 조정을 직접 구현하는 과정은 단순히 코드를 작성하는 것을 넘어 다양한 기술적 도전 과제를 수반합니다. 예를 들어, 대규모 데이터셋을 효율적으로 처리하기 위한 데이터 파이프라인(data pipeline) 최적화, 메모리 관리, 그리고 분산 학습 환경 설정 등이 있습니다. 특히, 다양한 길이의 시퀀스를 처리하기 위한 패딩(padding) 및 마스킹(masking) 전략은 모델의 학습 효율성과 성능에 직접적인 영향을 미칩니다.

또한, 명령어 미세 조정은 '재앙적 망각(catastrophic forgetting)'과 같은 문제를 야기할 수 있습니다. 이는 모델이 새로운 명령어 데이터에 적응하면서 기존에 학습했던 일반적인 지식이나 능력을 잃어버리는 현상입니다. 이를 완화하기 위해 LoRA(Low-Rank Adaptation)와 같은 효율적인 파인튜닝 기법을 통합하는 것이 중요합니다. LoRA는 적은 수의 추가 매개변수만을 학습하여 원본 모델의 가중치를 보존하면서도 새로운 작업에 대한 성능을 효과적으로 향상시킬 수 있습니다. 본 장에서는 이러한 실질적인 문제점과 그에 대한 효과적인 해결 방안들을 함께 다루어, 독자들이 실제 환경에서 강력한 LLM을 개발할 수 있도록 돕습니다.

## 3. 명령어 기반 데이터로 LLM 사전 학습(Pretraining)

"Instruction Pre-Training: Language Models are Supervised Multitask Learners" (https://arxiv.org/abs/2406.14491) 논문에서 연구자들은 원시 텍스트(raw text) 대신 인공적으로 생성된 명령어-응답 쌍(synthetic instruction-response pairs)을 포함시킴으로써, 거대 언어 모델(LLM)의 사전 학습(pretraining) 과정을 더욱 효율적으로 개선할 수 있는지에 대한 가능성을 탐구합니다. (여기서 "원시 텍스트(raw text)"는 특정 형식으로 재처리되지 않은 책, 웹사이트, 논문 등의 텍스트를 의미합니다.) 이 연구는 LLM이 단순히 텍스트 예측을 넘어, 다양한 작업을 수행하는 '지도형 다중 작업 학습자(supervised multitask learner)'로서의 잠재력을 극대화하는 새로운 패러다임을 제시합니다.

일반 사전 학습(regular pretraining)(상단)과 제안된 명령어 사전 학습(instruction pretraining) 접근 방식(하단)의 비교 (https://arxiv.abs/2406.14491의 주석이 달린 도식을 통해)

구체적으로, 연구자들은 이 작업을 위해 특별히 미세 조정된 LLM인 "명령어 합성기(instruction synthesizer)"를 통해 원본 훈련 코퍼스(corpus) 자체에서 명령어-응답 데이터(instruction-response data)를 생성하는 실험을 진행합니다. (이것이 원본 텍스트를 명령어 데이터로 형식화하는 것을 제안하는 첫 번째 논문은 아닙니다. 떠오르는 또 다른 작업은 "Genie: Achieving Human Parity in Content-Grounded Datasets Generation" (https://arxiv.org/abs/2401.14367)입니다. 또한 몇 달 전 사전 학습 중에 명령어 데이터를 사용하는 또 다른 논문이나 블로그 게시물을 본 기억이 있습니다. 이 방법을 동료들과 논의했지만, 아쉽게도 참조를 찾을 수 없었습니다. 그럼에도 불구하고, 여기서 논의된 논문은 로컬에서 실행되는 공개적으로 사용 가능한 LLM을 기반으로 하며 사전 학습(pretraining)과 연속 사전 학습(continual pretraining)을 모두 다루기 때문에 특히 흥미롭습니다.)

### 3.1 명령어 합성기(Instruction Synthesizer)의 역할과 구축

사전 학습 및 연속 사전 학습 결과에 대해 자세히 알아보기 전에, 이 방법의 핵심 구성 요소인 명령어 합성기(instruction synthesizer)에 대해 이야기해 봅시다. 이 합성기는 원시 텍스트를 다양한 형식의 명령어-응답 쌍으로 변환하는 역할을 합니다.

이것은 공개적으로 사용 가능한 Mistral 7B v0.1 LLM(작년에 제가 여기에서 썼던 내용: https://magazine.sebastianraschka.com/i/138555764/mistral-b)으로, 원본 텍스트에서 명령어-응답 쌍을 생성하도록 미세 조정되었습니다. 이 합성기를 미세 조정하기 위해 연구자들은 질문과 답변이 연결된 위키피디아(Wikipedia)의 구절로 구성된 HotpotQA (https://arxiv.org/abs/1809.09600)와 같은 데이터셋을 사용합니다. 이를 통해 저자들은 상식 추론(commonsense reasoning), 감성 분석(sentiment analysis), 수학 문제(math problems) 등 다양한 작업이 다루어지도록 합니다. 합성기는 단순히 텍스트를 재구성하는 것을 넘어, 텍스트의 의미를 이해하고 그로부터 다양한 유형의 질문과 그에 대한 정확한 답변을 생성하는 능력을 갖추도록 훈련됩니다.

명령어 합성기(instruction synthesizer)의 입력 및 출력 데이터 (https://arxiv.abs/2406.14491의 주석이 달린 도식을 통해)

이 명령어 합성기(instruction synthesizer)가 개발되면 (즉, 미세 조정되면), 대상 LLM을 사전 학습하기 위한 입력 데이터(input data)를 생성하는 데 사용될 수 있습니다. 명령어 합성기에 대한 마지막 주목할 만한 세부 사항은 아래 그림과 같이 여러 원본 텍스트(T n )와 명령어-응답 쌍(I n ⊕ R n )이 퓨샷 예제(few-shot examples)로 연결된다는 것입니다. 이는 합성기가 새로운 원본 텍스트를 처리할 때, 이미 학습된 패턴을 활용하여 더욱 효과적인 명령어-응답 쌍을 생성할 수 있도록 돕습니다.

명령어 합성기(instruction synthesizer)를 미세 조정하고 사용하는 명령어 데이터(instruction-data) 형식 (https://arxiv.abs/2406.14491의 주석이 달린 도식을 통해)

### 3.2 명령어 데이터를 활용한 사전 학습(Pretraining) 성능 분석

이제 명령어-응답 쌍을 생성하는 방법에 대해 논의했으니, 흥미로운 부분인 이 증강된 데이터셋(augmented dataset)에서 모델이 얼마나 잘 훈련되는지에 대해 알아봅시다. 이 섹션에서는 명령어 사전 학습이 모델의 초기 능력 형성에 미치는 영향을 중점적으로 다룹니다.

첫 번째 결과 세트는 처음부터 훈련된 두 개의 작은 모델을 살펴봅니다: 5억 개의 매개변수(parameters)와 13억 개의 매개변수(parameters) (둘 다 Mistral 아키텍처(architecture) 기반).

처음부터 모델을 훈련하는 데 사용된 3가지 다른 사전 학습(pretraining) 접근 방식 비교 (https://arxiv.abs/2406.14491의 주석이 달린 표)

위 표에서 볼 수 있듯이, 제안된 명령어 사전 학습(instruction pretraining) 접근 방식( **Instruct PT** )을 통해 훈련된 모델은 대부분의 벤치마크(benchmark) 작업에서 가장 좋은 성능을 보입니다(값이 높을수록 좋습니다). 그러나 합성된 명령어-응답 쌍이 포함되었기 때문에 Vanilla PT 접근 방식보다 더 많은 토큰(tokens)을 보았다는 점에 유의하십시오. 따라서 저자들은 원본 텍스트와 합성기 훈련에 사용된 명령어 데이터를 모두 포함하는 데이터 혼합(data mix)으로 훈련된 모델인 Mix PT 비교를 포함했습니다. 이 비교는 명령어 데이터의 양뿐만 아니라 그 '성격' 또한 중요하다는 점을 시사합니다.

이 비교를 통해 우리는 단순히 어떤 명령어 데이터를 사용하는 것만으로는 차이가 나지 않는다는 것을 알 수 있습니다. Instruct PT가 대부분의 작업에서 Mix PT보다 더 나은 성능을 보인다는 사실은 명령어-응답 데이터의 특성(즉, 원본 데이터와 관련된 명령어-응답 데이터)이 차이를 만든다는 것을 보여줍니다. (저자들은 동일한 수의 토큰을 사용하여 모든 실험을 수행했습니다.) 이는 모델이 단순히 텍스트를 예측하는 것을 넘어, 특정 작업을 수행하도록 '지도'되는 과정에서 더 깊이 있는 언어 이해와 추론 능력을 학습한다는 것을 의미합니다.

또한, Instruct PT로 사전 학습된 모델에는 또 다른 장점이 있습니다. 아래 그림에서 볼 수 있듯이, 나중에 명령어 미세 조정될 때 더 많이 개선됩니다. 이는 명령어 사전 학습이 모델의 '학습 가능성(learnability)'을 향상시켜, 후속 미세 조정 단계에서 더 큰 성능 향상을 이끌어낼 수 있음을 보여줍니다.

전통적인 사전 학습 패러다임(Vanilla PT) 또는 명령어 사전 학습(instruction pretraining)으로 사전 학습된 LLM 미세 조정(finetuning) (https://arxiv.abs/2406.14491의 주석이 달린 그림)

### 3.3 명령어 데이터를 활용한 연속 사전 학습(Continual Pretraining)

처음부터 사전 학습하는 것은 LLM이 처음 만들어지는 방식이기 때문에 흥미롭습니다. 그러나 실무자들은 연속 사전 학습(continual pretraining)과 미세 조정(finetuning)에 더 많은 관심을 가질 것이라고 생각합니다. 여기서 연속 사전 학습은 기존의 사전 학습된 모델을 가져와 새로운 도메인 데이터(domain data)로 추가로 사전 학습하는 것을 의미합니다. 예를 들어, 일반 텍스트 코퍼스(text corpus)로 훈련된 Llama 3 8B 기본 모델을 금융, 의료, 법률 또는 기타 특정 도메인에 맞게 조정하고 싶다고 생각해 보십시오.

아래 표는 연구자들이 사전 학습된 Llama 3 8B 기본 모델에 명령어 사전 학습 방법을 적용했을 때 얻은 결과를 요약합니다. 구체적으로, 그들은 생의학 텍스트(biomedical texts)와 금융 텍스트(finance texts) 모두로 연속 사전 학습을 수행했습니다.

연속 사전 학습(continual pretraining)에 사용된 3가지 다른 사전 학습(pretraining) 접근 방식 비교 (https://arxiv.abs/2406.14491의 주석이 달린 표)

위 표를 보면, 명령어 사전 학습 접근 방식( **Instruct PT** )이 바닐라 사전 학습(vanilla pretraining)( **Vanilla PT** ) 접근 방식(여기서는 기본 모델의 일반적인 연속 사전 학습을 의미)보다 분명히 우수한 성능을 보인다는 것을 알 수 있습니다. Llama 3 70B 기본 모델은 참조용으로 포함되었으며, 이는 작은 전문화된 모델이 더 큰 일반 모델을 능가할 수 있음을 보여주기 위한 것이라고 생각합니다. 이는 명령어 기반 연속 사전 학습이 특정 도메인에 대한 모델의 이해도와 전문성을 크게 높일 수 있음을 시사하며, 도메인 특화 LLM 개발에 있어 중요한 방법론이 될 수 있습니다.

### 3.4 결론: 명령어 사전 학습의 효율성과 미래

LLM 사전 학습 파이프라인을 누군가에게 설명할 때마다, 그들은 그 단순함과 이것이 오늘날 LLM을 훈련하는 데 여전히 일반적으로 사용되는 방식이라는 사실에 놀라곤 합니다. 그런 의미에서 명령어 사전 학습 접근 방식은 상당히 신선합니다. 이 방법은 단순히 텍스트를 예측하는 것을 넘어, 모델이 실제 작업을 수행하는 능력, 즉 '명령어를 따르는 능력'을 초기부터 내재화하도록 훈련시킵니다. 이는 모델의 범용성과 적응성을 크게 향상시키는 결과를 가져옵니다.

한 가지 주의할 점은 대규모 사전 학습 코퍼스(corpora)의 경우, 명령어 증강 코퍼스(instruction-augmented corpora)를 생성하는 데 여전히 비용이 많이 들 수 있다는 것입니다. 그러나 생성된 데이터의 좋은 점은 일단 생성되면 여러 다른 프로젝트에서 재사용될 수 있다는 것입니다. 또한, 명령어 합성기 자체의 성능이 향상됨에 따라, 이러한 데이터 생성 비용은 점차 감소할 가능성이 높습니다. 명령어 사전 학습은 LLM이 단순히 '언어 모델'을 넘어 '지능형 에이전트(intelligent agent)'로 진화하는 데 중요한 발판을 마련할 것입니다.

## 4. Gemma 2: 효율성과 성능의 조화

구글이 새롭게 선보인 Gemma 2 모델에 대한 언급을 빼놓을 수 없습니다. 이 모델은 지난 한 달간 공개된 모델 중 가장 주목할 만한 출시작으로 평가됩니다. 그러나 순수한 규모 면에서는 엔비디아의 Nemotron-4 340B가 최고를 차지합니다 (https://arxiv.org/abs/2406.11704). Gemma 2는 2.6B, 9B, 27B 매개변수(parameter) 버전으로 제공되며, 특히 소형 및 중형 모델에서 뛰어난 성능을 보여주며 실용적인 적용 가능성을 넓혔습니다.

핵심적인 주제는 학습 데이터 집합의 규모를 무조건적으로 증대시키기보다, 오히려 상대적으로 작고 효율적인 거대 언어 모델(LLM)을 개발하는 데 중점을 둔 기술들을 탐구하는 데 있습니다. 구체적으로, 구글 연구팀은 2.6B 및 9B 매개변수 모델을 만들기 위해 세 가지 주요 아키텍처(architectural) 및 훈련 선택을 혼합합니다: 슬라이딩 윈도우 어텐션(sliding window attention), 그룹 쿼리 어텐션(grouped-query attention), 그리고 지식 증류(knowledge distillation)입니다. 이러한 기술들은 모델의 효율성을 극대화하면서도 경쟁력 있는 성능을 유지하는 데 기여합니다.

### 4.1 슬라이딩 윈도우 어텐션(Sliding Window Attention)의 효율성

슬라이딩 윈도우 어텐션(sliding window attention)(예: Mistral에 의해 대중화됨)은 고정된 크기의 어텐션 블록(attention block)을 사용하는 기술로, 아래 그림과 같이 현재 토큰(token)이 모든 이전 토큰이 아닌 특정 수의 이전 토큰에만 어텐션(attention)할 수 있도록 합니다. 이는 전체 시퀀스에 대한 어텐션 계산 비용을 크게 줄여, 긴 컨텍스트(long context)를 처리하면서도 메모리 사용량과 연산 시간을 절감하는 데 효과적입니다.

슬라이딩 윈도우 어텐션(sliding window attention)을 설명하는 https://arxiv.abs/2310.06825의 주석이 달린 그림.

Gemma 2의 경우, 저자들은 일반 어텐션(regular attention)과 슬라이딩 윈도우 어텐션 레이어(layers)를 번갈아 사용했습니다. 슬라이딩 어텐션 블록 크기는 4096 토큰(tokens)이었고, 총 블록 크기는 8192 토큰에 걸쳐 있었습니다. 이러한 하이브리드(hybrid) 접근 방식은 지역적(local) 정보와 전역적(global) 정보를 모두 효율적으로 활용하여 모델의 성능을 최적화합니다.

슬라이딩 윈도우 어텐션은 주로 계산 성능(computational performance)을 향상시키는 데 사용되며, 연구자들은 또한 추론(inference) 중에 블록 크기(block size)를 줄일 때 혼란도(perplexity)에 거의 눈에 띄지 않는 차이가 있음을 보여주는 작은 절제 연구(ablation study)를 포함했습니다. 이는 유연한 컨텍스트 길이 조절이 가능함을 의미하며, 다양한 하드웨어 환경에서 효율적인 배포를 가능하게 합니다.

Gemma 2 기술 보고서의 절제 연구(ablation study)는 슬라이딩 윈도우(sliding window)의 블록 크기(block size) 감소가 추론(inference) 중 9B 매개변수(parameter) 모델의 모델링 성능(modeling performance)에 거의 영향을 미치지 않음을 보여줍니다. (GPU 메모리(memory) 개선을 나란히 보는 것도 흥미로웠을 것입니다.)

### 4.2 그룹 쿼리 어텐션(Group-query attention)의 최적화

그룹 쿼리 어텐션(Group-query attention)(Llama 2 및 3에서처럼)은 다중 쿼리 어텐션(multi-query attention)의 더 일반화된 형태로 간주될 수 있습니다. 이 뒤에 있는 동기는 여러 쿼리 헤드(Query heads)에 대해 동일한 키(Keys) 및 값(Values) 헤드(heads)를 공유하여 훈련 가능한 매개변수(trainable parameters)의 수를 줄이고, 따라서 계산 요구 사항(computational requirements)을 낮추는 것입니다. 이는 특히 추론 시 KV 캐시(cache) 메모리 사용량을 크게 줄여, 더 긴 컨텍스트를 처리하거나 더 많은 동시 요청을 처리할 수 있게 합니다.

Ainslie et al. 2023의 주석이 달린 그림

기존의 다중 헤드 어텐션(Multi-Head Attention, MHA)은 각 헤드마다 독립적인 쿼리, 키, 값 프로젝션(projection)을 사용합니다. 반면, 다중 쿼리 어텐션(Multi-Query Attention, MQA)은 모든 쿼리 헤드가 하나의 키 및 값 프로젝션을 공유합니다. 그룹 쿼리 어텐션(GQA)은 MHA와 MQA의 중간 형태로, 여러 쿼리 헤드가 소수의 키 및 값 프로젝션을 공유하는 방식입니다. 이로 인해 MHA보다 메모리 효율적이고 MQA보다 성능 저하가 적다는 장점을 가집니다.

### 4.3 지식 증류(Knowledge Distillation)를 통한 소형 모델 강화

지식 증류(Knowledge distillation)의 일반적인 아이디어(MiniLLM, https://arxiv.org/abs/2306.08543에서처럼)는 더 큰 모델(교사 모델)에서 더 작은 모델(학생 모델)로 지식을 전달하는 것입니다. 여기서는 27B(교사) 모델을 처음부터 훈련한 다음, 더 큰 교사 모델의 출력(outputs)을 사용하여 더 작은 2B 및 9B(학생) 모델을 훈련했습니다. 27B 모델은 지식 증류를 사용하지 않았지만, 더 작은 모델을 위한 "교사" 역할을 하기 위해 처음부터 훈련되었습니다. 이 과정은 학생 모델이 교사 모델의 예측 분포를 모방하도록 학습하여, 작고 효율적인 모델이 대형 모델의 복잡한 지식을 효과적으로 흡수할 수 있도록 합니다.

제 책 "Machine Learning Q and AI"에서 컴퓨터 비전(computer vision) 맥락의 지식 증류(knowledge distillation) 개요.
LLM 맥락에서는 이미지 대신 텍스트를, 클래스 레이블(class labels) 대신 예측된 토큰(predicted tokens)을 생각하십시오.

지식 증류는 모델 압축(model compression)의 핵심 전략 중 하나로, 제한된 컴퓨팅 자원에서 대형 모델의 성능을 재현해야 하는 온디바이스(on-device) 또는 엣지(edge) 환경에 특히 유용합니다. Gemma 2의 경우, 이 기술을 통해 2B 및 9B 모델이 27B 모델의 추론 능력을 상당 부분 계승하면서도 훨씬 적은 자원으로 운영될 수 있게 됩니다.

### 4.4 기타 흥미로운 아키텍처(Architecture) 세부 정보

이 논문에는 다른 흥미로운 정보들이 많이 포함되어 있습니다. 예를 들어, Gemma 2의 한 가지 특징은 비교적 큰 어휘 크기(vocabulary size)입니다: 256,000 토큰(tokens). 이는 첫 번째 Gemma 모델과 유사하지만, Llama 3 어휘(128,000)의 두 배, Phi-3 어휘(32,000)의 8배라는 점에서 여전히 주목할 만합니다.

LLM의 어휘 크기(vocabulary size)는 모델이 인식하고 생성할 수 있는 고유한 토큰(단어, 서브워드 또는 문자)의 수를 나타냅니다. LLM에서 큰 어휘 크기(vocabulary size)는 단어와 개념의 더 나은 커버리지(coverage), 다국어 콘텐츠(multilingual content)의 향상된 처리, 그리고 토큰화 아티팩트(tokenization artifacts) 감소를 가능하게 합니다. 그러나 큰 어휘 크기는 또한 모델 크기 증가 및 더 큰 임베딩(embedding) 및 출력 레이어(output layers)로 인한 잠재적으로 느린 추론(inference)과 같은 트레이드오프(trade-offs)를 동반합니다. (이러한 단점을 상쇄하는 데 슬라이딩 윈도우 어텐션(sliding window attention)과 다중 쿼리 어텐션 메커니즘(multi-query attention mechanism)이 중요합니다.) Gemma 2는 이러한 단점들을 효율적인 어텐션 메커니즘으로 상쇄하며, 다양한 언어와 복잡한 텍스트를 더 정교하게 처리할 수 있는 기반을 마련합니다.

또한 이전에 본 적이 없는 기술인 "로짓 캐핑(logit capping)"에 대한 흥미로운 섹션도 있습니다. 기본적으로, 이는 로짓 값(logit values)을 특정 범위 내로 유지하기 위한 최소-최대 정규화(min-max normalizing) 및 클리핑(clipping)의 한 형태입니다. 이는 훈련 중 안정성(stability)과 기울기 흐름(gradient flow)을 개선하기 위한 것으로 추정됩니다. 로짓 값은 모델의 최종 출력 확률을 결정하는 중요한 요소이므로, 이 값의 범위를 제한함으로써 모델이 과도하게 확신하거나 불안정한 예측을 내놓는 것을 방지하고, 학습 과정의 수렴을 돕습니다.

`logits ← soft_cap ∗ tanh(logits/soft_cap).`

또한, 이 논문은 이에 대한 자세한 내용을 많이 제공하지는 않지만, 다른 하이퍼파라미터(hyperparameters)를 가진 여러 실행의 모델을 결합하기 위해 모델 병합 기술(model merging techniques)을 활용합니다. (그러나 관심 있는 독자들은 Gemma 2가 이를 위해 사용하는 WARP: On the Benefits of Weight Averaged Rewarded Policies 에서 더 자세히 읽을 수 있습니다.) 모델 병합은 여러 모델의 장점을 결합하여 단일 모델보다 뛰어난 성능을 달성하거나, 특정 작업에 특화된 모델을 생성하는 데 사용될 수 있습니다.

모델링 성능(modeling performance) 측면에서 Gemma 2는 3배 더 큰 Llama 3 70B만큼 거의 좋으며, 이전 Qwen 1.5 32B 모델을 능가합니다. 더 최근의 Qwen 2 모델과의 비교를 보는 것도 흥미로울 것입니다.

공개적으로 사용 가능한 가중치(weights)를 가진 두 가지 인기 모델인 Llama 3와 Qwen 1.5의 비교. (Gemma 2 기술 보고서의 주석이 달린 표).

개인적으로, Gemma 2 보고서가 일부 아키텍처(architectural) 선택에 대한 절제 연구(ablation studies)를 포함한다는 점이 하이라이트입니다. 이는 한때 학술 연구에서 당연한 것이었지만, LLM 연구에서는 점점 더 드물어지고 있습니다. 이러한 절제 연구는 특정 구성 요소가 모델 성능에 미치는 영향을 체계적으로 분석하여, LLM 설계의 투명성과 이해도를 높이는 데 기여합니다.

Gemma 2 기술 보고서에 포함된 절제 연구(ablation studies) 중 하나의 예. 여기서 "wide"는 28개 레이어(layers)와 24,576의 중간 크기(intermediate size)를 가진 모델을 의미하고, "deep"은 42개 레이어(layers)와 14,336의 중간 크기(intermediate size)를 가진 아키텍처(architecture)를 의미합니다.

### 4.5 결론: Gemma 2의 전략적 위치

구글에서 이렇게 비교적 상세한 기술 보고서를 보는 것은 신선합니다. 모델 자체에 관해서는, 대중적 합의에 따르면 Gemma 2는 오늘날 단일 GPU 사용 사례(single-GPU use cases)에 가장 유능한 모델일 것입니다. 이는 접근성과 비용 효율성 측면에서 개인 개발자나 소규모 기업에게 매우 매력적인 옵션이 됩니다. 더 큰 모델의 경우, Llama 3 70B와 Qwen 2 72B가 강력한 경쟁자로 남아 있습니다. Gemma 2의 전략은 단순히 가장 큰 모델을 만드는 것이 아니라, 최신 아키텍처 혁신과 효율적인 훈련 기법을 통해 다양한 규모에서 최적의 성능을 제공하는 데 있습니다. 이러한 접근 방식은 LLM 기술의 상업적 및 개인적 활용 범위를 넓히는 데 크게 기여할 것으로 예상됩니다.

## Ahead of AI 지원하기

Ahead of AI는 어떠한 직접적인 보상도 없이 오직 개인적인 열정으로 운영되는 프로젝트입니다. 그럼에도 불구하고 저의 작업을 지지하고 싶으신 분들께서는 저의 저서를 구매해주시면 큰 힘이 될 것입니다. 이 책들이 통찰력 있고 유익하다고 생각하시면, 친구와 동료들에게 자유롭게 추천해 주십시오. 잠시 시간을 내어 Amazon에 Machine Learning Q and AI 또는 Machine Learning with PyTorch and Scikit-Learn에 대한 리뷰를 남겨주시면 큰 도움이 될 것입니다! 여러분의 지원은 저에게 큰 의미이며, 이 여정을 계속하는 데 엄청난 도움이 됩니다. 감사합니다!

## 5. 6월의 다른 흥미로운 연구 논문들: 최신 동향 분석

아래는 지난 6월 제가 우연히 발견한 다른 흥미로운 논문들의 목록입니다. 이 목록의 길이를 고려하여, 제가 특히 흥미롭다고 생각한 20개에는 별표(*)를 표시했습니다. 그러나 이 목록과 그 주석은 전적으로 제 관심사와 제 프로젝트와의 관련성을 기반으로 한다는 점을 유의하십시오. 6월의 연구 동향은 데이터 생성, 모델 효율성, 정렬 및 신뢰성, 그리고 새로운 아키텍처 탐색이라는 몇 가지 주요 테마로 요약될 수 있습니다.

**Scaling Synthetic Data Creation with 1,000,000,000 Personas** by Chan, Wang, Yu, et al. (6월 28일), https://arxiv.org/abs/2406.20094
이 연구는 거대 언어 모델(LLM)을 활용하여 자동적으로 구축된 방대한 페르소나 집합인 페르소나 허브를 기반으로, 다양한 종류의 인공 데이터를 생성하는 페르소나 주도형 데이터 합성 방법론을 제시합니다. 이러한 대규모 페르소나 기반 데이터는 모델의 일반화 능력을 향상시키고, 특정 사용자 그룹에 특화된 응답을 생성하는 데 중요한 역할을 할 수 있습니다. 이 페르소나 허브는 전 세계 인구의 약 13%를 나타냅니다.

**LLM Critics Help Catch LLM Bugs** by McAleese, Pokorny, Ceron Uribe, et al. (6월 28일), https://arxiv.org/abs/2407.00215
이 연구는 인간 피드백 기반 강화 학습(RLHF)을 활용하여 모델이 생성한 코드를 평가하는 데 인간을 보조하는 '비평가' 모델을 개발하고, 코드 오류에 대한 자연어 형태의 피드백을 생성하도록 LLM을 훈련시켰으며, 여러 작업에서 오류를 찾아내는 데 있어 그 유효성을 실증합니다. 이는 LLM이 소프트웨어 개발 프로세스에서 디버깅(debugging) 보조 도구로서의 잠재력을 가지고 있음을 보여줍니다.

**Direct Preference Knowledge Distillation for Large Language Models** by Li, Gu, Dong, et al. (6월 28일), https://arxiv.org/abs/2406.19774
DPKD는 LLM을 위한 지식 증류(Knowledge Distillation)를 두 단계 프로세스(process)로 재구성합니다. 첫째, 암묵적 보상(implicit reward)과 역 KL 발산(reverse KL divergence)을 결합한 목표를 최적화하고, 둘째, 학생 모델(student model)의 출력보다 교사 모델(teacher model)의 출력에 대한 선호도 확률(preference probability)을 향상시킵니다.

**Changing Answer Order Can Decrease MMLU Accuracy** by Gupta, Pantoja, Ross, et al. (6월 27일), https://arxiv.org/abs/2406.19470
이 연구는 LLM을 위한 MMLU 벤치마크(benchmark)에서 정확도 측정의 견고성(robustness)을 조사하며, 답변 레이블(label) 내용을 섞는 것이 모델(model) 전반에 걸쳐 정확도(accuracy) 감소를 초래하고, 민감도(sensitivity)가 다양하다는 것을 밝혀냅니다. 이는 벤치마크 설계의 중요성과 LLM 평가의 섬세함을 강조합니다.

**From Artificial Needles to Real Haystacks: Improving Retrieval Capabilities in LLMs by Finetuning on Synthetic Data** by Xiong, Papageorgiou, Lee, and Papailiopoulos (6월 27일), https://arxiv.org/abs/2406.19292
이 연구는 LLM의 긴 컨텍스트(long-context) 정보 검색 및 추론 능력(reasoning capabilities)을 향상시키기 위해 숫자 키-값 검색 작업(numerical key-value retrieval tasks)의 합성 데이터셋(synthetic dataset)을 사용하는 미세 조정(finetuning) 접근 방식을 제안합니다.

**Dataset Size Recovery from LoRA Weights** by Salama, Kahana, Horwitz, and Hoshen (6월 27일), https://arxiv.org/abs/2406.19395
이 연구는 LoRA 행렬의 노름(norm)과 스펙트럼(spectrum)을 분석하여 LoRA를 사용하여 비전 모델(vision model)을 미세 조정(finetuning)하는 데 사용된 이미지(image) 수를 복구하는 방법을 소개합니다.

**Step-DPO: Step-wise Preference Optimization for Long-chain Reasoning of LLMs** by Azerbayev, Shao, Lin, et al. (6월 26일), https://arxiv.org/abs/2406.18629
이 논문은 LLM의 수학 문제 해결에서 개별 추론 단계(reasoning steps)를 최적화하는 방법인 Step-DPO를 소개하며, 이를 위해 맞춤형 10K 단계별 선호도 쌍 데이터셋(step-wise preference pair dataset)을 사용합니다.

**RouteLLM: Learning to Route LLMs with Preference Data** by Ong, Amjad, et al. (6월 26일), https://arxiv.org/abs/2406.18665
이 연구는 추론(inference) 시 더 강력한 LLM과 더 약한 LLM 사이에서 동적으로 선택하여 비용-성능 트레이드오프(cost-performance trade-offs)를 최적화하는 효율적인 라우터 모델(router models)을 제안합니다. 이는 LLM 배포의 경제성을 크게 향상시킬 수 있습니다.

**\* A Closer Look into Mixture-of-Experts in Large Language Models** by Zhang, Liu, Patel, et al. (6월 26일), https://arxiv.org/abs/2406.18219
이 연구는 Mixture-of-Experts (MoE) LLM의 내부 작동 방식을 살펴보고 뉴런(neuron) 동작, 전문가 선택 기준(expert selection criteria) 및 레이어(layer) 전반의 전문가 다양성(expert diversity)에 대한 통찰력을 공유하며, 이러한 관찰을 기반으로 MoE 설계 및 구현에 대한 실용적인 제안을 제공합니다. MoE는 미래 LLM 스케일링의 핵심 기술로, 효율적인 자원 활용과 성능 확장을 가능하게 합니다.

**\* Following Length Constraints in Instructions** by Yuan, Kulikov, Yu, et al. (6월 25일), https://arxiv.org/abs/2406.17744
이 연구는 추론(inference) 시 사용자가 지정한 길이 제약(length constraints)을 따를 수 있는 LLM을 훈련하는 방법을 소개하며, 모델 평가(model evaluation)의 길이 편향(length bias)을 해결하고 길이 제어 작업(length-controlled tasks)에서 표준 명령어 팔로잉 모델(instruction-following models)보다 뛰어난 성능을 보입니다. 이는 뉴스 요약이나 특정 분량의 보고서 작성 등 실제 콘텐츠 생성 애플리케이션에서 매우 중요한 기능입니다.

**LongIns: A Challenging Long-context Instruction-based Exam for LLMs** by Shaham, Bai, An, et al. (6월 25일), https://arxiv.org/abs/2406.17588
LongIns는 LLM의 긴 컨텍스트(long-context) 능력(capabilities)을 평가하기 위한 새로운 벤치마크(benchmark)로, 검색 및 추론 능력(reasoning abilities)을 평가하기 위해 세 가지 설정(setting)을 사용합니다.

**\* The FineWeb Datasets: Decanting the Web for the Finest Text Data at Scale** by He, Wang, Shen, et al. (6월 25일), https://arxiv.org/abs/2406.17557
이 보고서는 Common Crawl에서 파생된 15조 토큰(token) 데이터셋(dataset)인 FineWeb과 1.3조 토큰(token) 교육 서브셋(educational subset)인 FineWeb-Edu를 소개합니다. 이러한 고품질의 대규모 데이터셋은 차세대 LLM 훈련의 기반이 되며, 모델의 성능과 일반화 능력에 지대한 영향을 미칩니다.

**Adam-mini: Use Fewer Learning Rates To Gain More** by Zhang, Chen, Li, et al . (6월 24일), https://arxiv.org/abs/2406.16793
Adam-mini는 학습률(learning rate) 자원을 전략적으로 줄이고, 헤시안 구조(Hessian structure)를 기반으로 매개변수(parameter)를 분할하며, 매개변수 블록(parameter blocks)에 최적화된 단일 학습률(learning rate)을 할당하여 AdamW와 유사하거나 더 나은 성능을 달성하면서 메모리(memory)를 45-50% 적게 사용하는 제안된 최적화기(optimizer)입니다.

**WARP: On the Benefits of Weight Averaged Rewarded Policies** by Ramé, Ferret, Vieillard, et al. (6월 24일), https://arxiv.org/abs/2406.16768
이 논문은 LLM을 위한 새로운 정렬 전략(alignment strategy)을 소개하며, 세 단계에서 정책(policy)을 병합합니다: 동적 KL 정규화(dynamic KL regularization)를 위한 지수 이동 평균(exponential moving average) 사용, 독립적으로 미세 조정(fine-tuned)된 정책(policy)의 구형 보간(spherical interpolation), 그리고 초기화(initialization)를 통한 선형 보간(linear interpolation)입니다.

**Sparser is Faster and Less is More: Efficient Sparse Attention for Long-Range Transformers** by Lou, Jia, Zheng, and Tu (6월 24일), https://arxiv.org/abs/2406.16747
저자들은 자기회귀 트랜스포머(autoregressive Transformers)를 위한 새로운 희소 어텐션 메커니즘(sparse attention mechanism)을 제안하며, 스코어링 네트워크(scoring network)와 미분 가능한 top-k 마스크 연산자(differentiable top-k mask operator)를 사용하여 쿼리(query)당 일정한 수의 KV 쌍을 선택함으로써 선형 시간 복잡도(linear time complexity)와 일정한 메모리 사용량(constant memory footprint)을 달성합니다.

**Efficient Continual Pre-training by Mitigating the Stability Gap** by Wang, Hu, Xiong, et al. (6월 21일), https://arxiv.org/abs/2406.14833
이 연구는 LLM의 연속 사전 훈련(continual pretraining)을 개선하기 위한 세 가지 전략을 제안합니다: 서브셋(subset)에 대한 여러 에포크(epoch), 고품질 데이터(high-quality data)에 집중, 그리고 사전 훈련 데이터(pretraining data)와 유사한 혼합(mixture) 사용입니다.

**MoA: Mixture of Sparse Attention for Automatic Large Language Model Compression** by Fu, Huang, Ning, et al. (6월 21일), https://arxiv.org/abs/2406.14909
Mixture of Attention (MoA)은 LLM에서 다양한 모델 구성 요소(model components) 및 입력 길이(input lengths)에 대한 희소 어텐션 패턴(sparse attention patterns)을 자동으로 최적화하여, 균일 희소 어텐션(uniform sparse attention) 접근 방식보다 컨텍스트 길이(context length), 정확도(accuracy) 및 효율성(efficiency)을 향상시킵니다.

**LongRAG: Enhancing Retrieval-Augmented Generation with Long-context LLMs** by Jiang, Ma, Chen, et al. (6월 21일), https://arxiv.org/abs/2406.15319
LongRAG는 4K 토큰(token) 검색 단위(retrieval units)와 긴 컨텍스트(long-context) LLM을 사용하여 답변을 추출하는 새로운 RAG 프레임워크(framework)를 소개하며, 이는 추가 훈련 없이 검색 성능(retrieval performance)을 향상시키고 질문-답변 작업(question-answering tasks)에서 최첨단 결과(state-of-the-art results)를 달성합니다.

**\* A Tale of Trust and Accuracy: Base vs. Instruct LLMs in RAG Systems** by Cuconasu, Trappolini, Tonellotto, et al. (6월 21일), https://arxiv.org/abs/2406.14972
이 연구는 기본 LLM이 검색 증강 생성(Retrieval Augmented Generation, RAG) 작업에서 명령어 튜닝(instruction-tuned) 모델보다 우수한 성능을 보인다는 것을 입증함으로써 기존의 통념에 도전합니다. 이는 RAG 시스템 설계에서 기본 모델의 잠재력을 재평가하게 합니다.

**Can LLMs Learn by Teaching? A Preliminary Study** by Ning, Wang, Li, Lin, et al. (6월 20일), https://arxiv.org/abs/2406.14629
저자들은 LLM에서 "가르치면서 배우기(Learning by Teaching)"를 구현하기 위한 세 가지 방법을 개발하고 테스트합니다. 이는 학생 피드백(student feedback) 관찰, 피드백(feedback)으로부터 학습, 반복 학습(iterative learning)과 같이 다양한 수준에서 인간의 교육 프로세스(teaching processes)를 모방하여 추가적인 인간 생성 데이터(human-produced data)나 더 강력한 모델(model)에 의존하지 않고 모델 성능(model performance)을 향상시킵니다.

**\* Instruction Pre-Training: Language Models are Supervised Multitask Learners** by Cheng, Gu, Huang, et al. (6월 20일), https://arxiv.org/abs/2406.14491
이 연구는 합성적으로 생성된 명령어-응답 쌍(instruction-response pairs)으로 원본 코퍼스(raw corpora)를 증강하는 LLM의 지도 다중 작업 사전 훈련(supervised multitask pretraining)을 위한 프레임워크(framework)를 소개합니다. 이는 본고에서 심층적으로 다룬 주제로, LLM의 학습 패러다임을 변화시키는 중요한 연구입니다.

**\* Can Long-Context Language Models Subsume Retrieval, RAG, SQL, and More?** by Wu, Zhang, Johnson, et al. (6월 19일), https://arxiv.org/abs/2406.13121
이 연구는 수백만 개의 토큰(token)을 요구하는 작업에서 긴 컨텍스트(long-context) LLM을 평가하기 위한 벤치마크(benchmark)를 소개하며, 이러한 긴 컨텍스트 LLM이 인컨텍스트 검색(in-context retrieval) 및 추론 작업(reasoning tasks)에서 전문화된 검색 및 RAG 시스템(RAG systems)과 경쟁할 수 있음을 보여줍니다. 이는 LLM이 점점 더 복잡한 정보 처리 작업을 단독으로 수행할 수 있음을 시사합니다.

**Judging the Judges: Evaluating Alignment and Vulnerabilities in LLMs-as-Judges** by Ye, Turpin, Li, He, et al. (6월 18일), https://arxiv.org/abs/2406.12624
이 논문은 TriviaQA를 벤치마크(benchmark)로 사용하여 LLM-as-a-judge 패러다임(paradigm)을 평가하며, 9개의 심사 모델(judge models)과 9개의 시험 응시 모델(exam taker models)을 인간 주석(human annotations)과 비교합니다. 그 결과, 높은 인간 정렬(human alignment)을 가진 모델이 반드시 시험 응시 모델(exam taker models)을 순위 매기는 데 가장 적합하지는 않다는 것을 밝혀냅니다.

**From RAGs to Rich Parameters: Probing How Language Models Utilize External Knowledge Over Parametric Information for Factual Queries** by Wadhwa, Seetharaman, Aggarwal, et al. (6월 18일), https://arxiv.org/abs/2406.12824
저자들은 LLM에서 검색 증강 생성(Retrieval Augmented Generation, RAG)의 메커니즘(mechanics)을 조사하여, 모델이 질문에 답할 때 매개변수 기억(parametric memory)보다는 검색된 컨텍스트 정보(retrieved context information)에 주로 의존하며, 다양한 모델 계열(model families)에서 지름길 행동(shortcut behavior)을 보인다는 것을 밝혀냅니다.

**Self-MoE: Towards Compositional Large Language Models with Self-Specialized Experts** by Kang, Karlinsky, and Luo, et al. (6월 17일), https://arxiv.org/abs/2406.12034
이 논문은 모놀리식(monolithic) LLM을 MiXSE(MiXture of Self-specialized Experts)라는 모듈식 시스템(modular system)으로 변환하는 방법을 소개하며, 자체 생성된 합성 데이터(self-generated synthetic data)를 사용하여 공유 기본 LLM 및 자체 최적화된 라우팅(routing)을 갖춘 전문화된 전문가 모듈(specialized expert modules)을 생성합니다.

**Measuring memorization in RLHF for code completion** by Pappu, Porter, Shumailov, and Hayes (6월 17일), https://arxiv.org/abs/2406.11715
이 연구는 코드 완성 작업(code completion tasks)에 초점을 맞춰 LLM의 데이터 기억(data memorization)에 대한 인간 피드백을 통한 강화 학습(Reinforcement Learning with Human Feedback, RLHF)의 영향을 조사하며, RLHF가 직접 미세 조정(finetuning)에 비해 보상 모델링(reward modeling) 및 강화 학습(reinforcement learning)에 사용된 데이터(data)의 기억(memorization)을 줄이지만, 초기 미세 조정(finetuning) 단계의 기억(memorization)은 대체로 보존한다는 것을 발견합니다.

**HARE: HumAn pRiors, a key to small language model Efficiency** by Zhang, Jin, Ge, et al. (6월 17일), https://arxiv.org/abs/2406.11410
이 연구는 벤치마크(benchmark) 데이터 유출을 피하면서 의미론적 다양성(semantic diversity)과 데이터 품질 일관성(data quality consistency)에 초점을 맞춘 소형 언어 모델(Small Language Models, SLMs)의 데이터 구성에서 인간 사전 지식(human priors)을 활용하는 원칙을 제안합니다.

**Iterative Length-Regularized Direct Preference Optimization: A Case Study on Improving 7B Language Models to GPT-4 Level** by Kim, Lee, Park, et al. (6월 17일), https://arxiv.org/abs/2406.11817
이 연구는 반복적인 길이 정규화 직접 선호도 최적화(iterative length-regularized Direct Preference Optimization, iLR-DPO)를 소개하며, 이는 응답의 장황함(response verbosity)을 제어하면서 인간의 선호도(human preferences)에 대한 LLM 정렬(alignment)을 개선하는 방법입니다.

**Unveiling Encoder-Free Vision-Language Models** by Choi, Yoon, Lee, et al . (6월 17일), https://arxiv.org/abs/2406.11832
이 연구는 통합된 디코더(decoder)에서 시각 및 텍스트 입력(visual and textual inputs)을 직접 처리하는 인코더 없는 비전-언어 모델(encoder-free vision-language model, VLM)을 제시합니다.

**\* DeepSeek-Coder-V2: Breaking the Barrier of Closed-Source Models in Code Intelligence** by Zhu, Wang, Lee, et al. (6월 17일), https://arxiv.org/abs/2406.11931
DeepSeek-Coder-V2는 추가 6조 토큰(token)에 대한 연속 사전 훈련(continued pretraining)을 통해 코딩 작업(coding tasks)에서 GPT4-Turbo 수준의 성능을 달성하는 오픈 소스(open-source) Mixture-of-Experts 코드 LLM입니다. 이는 오픈 소스 커뮤니티에 강력한 코드 생성 및 이해 능력을 제공하며, 코드 LLM의 새로운 기준을 제시합니다.

**Tokenization Falling Short: The Curse of Tokenization** by Nguyen, Kim, Patel, et al. (6월 17일), https://arxiv.org/abs/2406.11687
이 연구는 복잡한 문제 해결(complex problem solving), 토큰 구조 탐색(token structure probing), 그리고 타이포그래피(typographical) 변화에 대한 복원력(resilience)에서 LLM의 성능을 조사함으로써 LLM의 "토큰화의 저주(curse of tokenization)"를 탐구합니다. 그 결과, 모델 크기(model size)를 확장하는 것이 도움이 되지만, LLM은 토큰화로 인한 편향(tokenization-induced biases)에 여전히 취약하다는 것을 밝혀냅니다.

**DataComp-LM: In Search of the Next Generation of Training Sets for Language Models** by Li, Fang, Smyrnis, et al. (6월 17일), https://arxiv.org/abs/2406.11794
저자들은 언어 모델 훈련에서 데이터셋 큐레이션(dataset curation) 전략을 실험하기 위한 표준화된 테스트베드(testbed)를 제공하며, 여기에는 240조 토큰(token) 코퍼스(corpus), 사전 훈련 레시피(pretraining recipes), 그리고 53개의 다운스트림 평가(downstream evaluations)가 포함됩니다.

**\* Nemotron-4 340B Technical Report** by Unknown Authors at NVIDIA (6월 17일), https://arxiv.org/abs/2406.11704
이 기술 보고서는 다양한 벤치마크(benchmark)에서 경쟁력 있는 성능을 보이고 합성 데이터 생성(synthetic data generation)에 탁월하며, 추가 연구 개발을 위해 데이터 생성 파이프라인(data generation pipeline)을 오픈 소스(open-sourcing)로 공개하는 NVIDIA의 Nemotron-4 340B 모델 제품군(model family) 출시와 함께 제공됩니다. NVIDIA가 LLM 파운데이션 모델 시장에 본격적으로 진입하고 있음을 알리는 중요한 신호입니다.

**mDPO: Conditional Preference Optimization for Multimodal Large Language Models** by Wang, Zhou, Huang, et al. (6월 17일), https://arxiv.org/abs/2406.11839
mDPO는 언어 선호도(language preferences)와 함께 이미지 선호도(image preference)를 최적화하고 선택된 응답에 대한 가능성 감소(likelihood decrease)를 방지하기 위해 보상 앵커(reward anchor)를 도입함으로써 다중 모달 DPO(multimodal DPO)의 무조건적 선호도 문제(unconditional preference problem)를 해결합니다.

**\* How Do Large Language Models Acquire Factual Knowledge During Pretraining?** by Chang, Park, Ye, et al. (6월 17일), https://arxiv.org/abs/2406.11813
이 연구는 LLM이 사전 학습 과정에서 사실적 지식을 어떻게 습득하는지에 대한 근본적인 질문을 탐구합니다. 이는 모델의 지식 표현과 추론 능력을 이해하는 데 중요한 통찰력을 제공합니다.

**Task Me Anything** by Zhang, Huang, Ma, et al. (6월 17일), https://arxiv.org/abs/2406.11775
Task-Me-Anything은 방대한 이미지(image) 및 비디오(video) 분류 체계(taxonomy)에서 작업 인스턴스(task instances)를 프로그래밍 방식으로 생성하여 다중 모달 언어 모델(multimodal language models)을 위한 맞춤형 벤치마크(benchmark)를 생성하는 벤치마크 생성 엔진(benchmark generation engine)입니다.

**THEANINE: Revisiting Memory Management in Long-term Conversations with Timeline-augmented Response Generation** by Kim, Ong, Kwon, et al. (6월 16일), https://arxiv.org/abs/2406.10996
Theanine은 메모리 타임라인(memory timelines)(과거 사건의 발전과 인과 관계를 보여주는 일련의 기억)을 사용하여 LLM의 응답 생성(response generation)을 증강하여, 긴 대화 기록(dialogue histories)에서 정보를 회상하고 활용하는 모델의 능력(ability)을 향상시킵니다.

**Regularizing Hidden States Enables Learning Generalizable Reward Model for LLMs** by Yang, Ding, Lin, et al. (6월 14일) https://arxiv.org/abs/2406.10216
이 연구는 기본 모델의 언어 모델 헤드(language model head)를 유지하고 텍스트 생성 손실(text-generation losses)을 통합하여 은닉 상태(hidden states)를 정규화(regularizing)함으로써 RLHF에서 보상 모델 일반화(reward model generalization)를 향상시키는 방법을 제안합니다. 동시에 보상 헤드(reward head)를 학습하여 분포 외 작업 성능(out-of-distribution task performance)을 개선하고 보상 과최적화(reward over-optimization)를 완화합니다.

**Be like a Goldfish, Don't Memorize! Mitigating Memorization in Generative LLMs** by Hans, Wen, Jain, et al. (6월 14일) , https://arxiv.org/abs/2406.10209
"금붕어 손실(goldfish loss)" 기술은 훈련 중 손실 계산(loss computation)에서 토큰(token)의 서브셋(subset)을 무작위로 제외하여 LLM의 모델 기억(model memorization)을 줄이고, 모델이 훈련 데이터(training data)에서 완전한 문자 그대로의 시퀀스(verbatim sequences)를 학습하는 것을 방지합니다.

**Bootstrapping Language Models with DPO Implicit Rewards** by Chen, Liu, Du, et al. (6월 14일), https://arxiv.org/abs/2406.09760
연구자들은 직접 선호도 최적화(direct preference optimization, DPO) 중에 생성된 정렬된 모델(aligned model), 즉 암묵적 보상 모델(implicit reward model)이 그 자체로 선호도 데이터셋(preference dataset)을 생성하여 스스로를 더욱 크게 개선하는 데 사용될 수 있음을 발견했습니다.

**FouRA: Fourier Low Rank Adaptation** by Borse, Kadambi, Pandey, et al. (6월 13일), https://arxiv.org/abs/2406.08798
이 연구는 푸리에 도메인(Fourier domain)에서 작동하고 적응형 랭크 선택(adaptive rank selection)을 사용하는 새로운 저랭크 적응(low-rank adaptation, LoRA) 방법인 FouRA를 소개합니다. 이는 LoRA 미세 조정(fine-tuned)된 텍스트-이미지 확산 모델(text-to-image diffusion models)에서 데이터 복사(data copying) 및 분포 붕괴(distribution collapse) 문제를 해결하면서 이미지 품질(image quality)과 일반화(generalization)를 향상시킵니다.

**\* An Image is Worth More Than 16x16 Patches: Exploring Transformers on Individual Pixels** by Nguyen, Mahmoud Assran, Jain, et al. (6월 13일), https://arxiv.org/abs/2406.09415
이 연구는 바닐라 트랜스포머(vanilla Transformers)가 개별 픽셀(pixels)을 토큰(tokens)으로 처리함으로써 다양한 컴퓨터 비전 작업(computer vision tasks)에서 높은 성능을 달성할 수 있음을 밝혀냅니다. 이는 현대 비전 아키텍처(vision architectures)에서 지역성 기반 귀납적 편향(locality-based inductive bias)의 가정된 필요성에 도전하며, 컴퓨터 비전(computer vision) 분야의 미래 신경망 설계(neural network designs)에 대한 새로운 가능성을 제시합니다.

**MLKV: Multi-Layer Key-Value Heads for Memory Efficient Transformer Decoding** by Zuhri, Adilazuarda,Purwarianti, and Aji (6월 13일), https://arxiv.org/abs/2406.09297
이 연구는 트랜스포머(transformer) 레이어(layer) 전반에 걸쳐 키-값(Key-Value, KV) 캐싱(caching)을 확장하는 새로운 기술인 다중 레이어 키-값(Multi-Layer Key-Value, MLKV) 공유를 소개합니다. 이는 다중 쿼리 어텐션(Multi-Query Attention, MQA) 및 그룹 쿼리 어텐션(Grouped-Query Attention, GQA)과 같은 기존 방법보다 자기회귀 추론(auto-regressive inference) 중 메모리 사용량(memory usage)을 크게 줄이면서 NLP 작업(NLP tasks)에서 성능을 유지합니다.

**Transformers Meet Neural Algorithmic Reasoners** by Bounsi, Ibarz, Dudzik, et al. (6월 13일), https://arxiv.org/abs/2406.09308
TransNAR은 트랜스포머(Transformers)와 그래프 신경망 기반 신경 알고리즘 추론기(graph neural network-based neural algorithmic reasoners, NARs)를 결합한 하이브리드 아키텍처(hybrid architecture)로, 트랜스포머(Transformer)가 NAR의 강력한 계산 능력(computational capabilities)을 활용하면서도 강력한 자연어 이해(natural language understanding)를 유지함으로써 알고리즘 추론 작업(algorithmic reasoning tasks)에서 향상된 성능을 가능하게 합니다.

**Discovering Preference Optimization Algorithms with and for Large Language Models** by Lu, Holt, Fanconi, et al. (6월 12일), https://arxiv.org/abs/2406.08414
제안된 발견된 선호도 최적화(Discovered Preference Optimization) 방법은 LLM을 사용하여 LLM 출력(LLM outputs)을 개선하기 위한 새로운 선호도 최적화 알고리즘(preference optimization algorithms)을 자동으로 발견하고 구현합니다.

**\* An Empirical Study of Mamba-based Language Models** by Waleffe, Byeon, Riach, et al. (6월 12일), https://arxiv.org/abs/2406.07887
이 연구는 대규모 데이터셋(large datasets)으로 훈련된 8B 매개변수 상태 공간 모델(state-space models)(Mamba, Mamba-2)과 트랜스포머 모델(Transformer models)을 비교하며, 순수 상태 공간 모델(pure state-space models)이 많은 작업에서 트랜스포머(Transformers)와 같거나 그 이상이지만, 강력한 복사(strong copying), 인컨텍스트 학습(in-context learning) 또는 긴 컨텍스트 추론(long-context reasoning)을 요구하는 작업에서는 뒤처진다는 것을 발견합니다. 그러나 하이브리드(hybrids)는 두 가지 장점을 모두 제공하는 것으로 보입니다. 이는 트랜스포머 아키텍처의 대안을 모색하는 중요한 연구 방향을 제시합니다.

**\* Large Language Models Must Be Taught to Know What They Don't Know** by Kapoor, Gruver, Roberts, et al. (6월 12일), https://arxiv.org/abs/2406.08391
이 연구는 등급이 매겨진 예제(graded examples)의 작은 데이터셋(dataset)으로 LLM을 미세 조정(finetuning)하는 것이 프롬프트(prompting)만 사용하는 것보다 더 신뢰할 수 있는 불확실성 추정치(uncertainty estimates)를 생성할 수 있음을 보여주며, 결과 모델(resulting models)은 자신과 다른 모델(models)에 대한 불확실성(uncertainty)을 추정할 수 있습니다. 이는 LLM의 신뢰성을 높이고, 안전한 AI 시스템을 구축하는 데 필수적인 요소입니다.

**Large Language Model Unlearning via Embedding-Corrupted Prompts** by Liu, Flannigan, and Liu (6월 12일), https://arxiv.org/abs/2406.07933
이 연구는 임베딩 손상 프롬프트(embedding-corrupted prompts)를 소개합니다. 이는 LLM에서 선택적 지식 망각(selective knowledge unlearning)을 위한 방법으로, 프롬프트 분류(prompt classification) 및 임베딩 손상(embedding corruption)을 사용하여 광범위한 모델 크기(model sizes)에서 최소한의 부작용으로 목표화된 망각(targeted forgetting)을 달성합니다.

**What If We Recaption Billions of Web Images with LLaMA-3?** by Li, Tu, Hui, et al. (6월 12일) https://arxiv.org/abs/2406.08478
이 연구는 미세 조정(finetuned)된 Llama 3 기반 LLaVA-1.5 다중 모달 LLM을 사용하여 DataComp-1B 데이터셋(dataset)에서 13억 개의 이미지(image)를 재캡션(recaption)하는 것이 다양한 작업에서 비전-언어 모델(vision-language models)의 성능을 크게 향상시킨다는 것을 보여줍니다.

**\* Magpie: Alignment Data Synthesis from Scratch by Prompting Aligned LLMs with Nothing** by Xu, Jiang, Niu et al. (6월 12일), https://arxiv.org/abs/2406.08464
연구자들은 Llama-3-Instruct에서 30만 개의 고품질 명령어-응답 쌍(instruction-response pairs)을 생성하는 합성 명령어 데이터 생성(synthetic instruction data generation) 방법을 제안합니다. 이 데이터는 실제 정렬 단계(alignment step) 없이 정렬된 LLM의 성능에 필적하는 지도 명령어 미세 조정(supervised instruction fine-tuning)에 사용될 수 있습니다. 이 논문은 본고의 첫 번째 섹션에서 심층적으로 다룬 중요한 연구로, 합성 데이터의 잠재력을 명확히 보여줍니다.

**\* Samba: Simple Hybrid State Space Models for Efficient Unlimited Context Language Modeling** (6월 11일), https://arxiv.org/abs/2406.07522
Samba는 선택적 상태 공간 모델(selective state space models)(Mamba를 생각해보세요)과 슬라이딩 윈도우 어텐션(sliding window attention)을 결합한 하이브리드 모델(hybrid model)로, 3.8B 매개변수(parameters)까지 효율적으로 확장됩니다. 이는 SSM과 트랜스포머의 장점을 결합하여 효율성과 성능을 동시에 추구하는 새로운 아키텍처 방향을 제시합니다.

**\* Never Miss A Beat: An Efficient Recipe for Context Window Extension of Large Language Models with Consistent "Middle" Enhancement** (6월 11일) by Wu, Zhao, and Zheng, https://arxiv.org/abs/2406.07138
CREAM은 위치 인코딩(positional encodings)을 보간하고 절단된 가우시안(truncated Gaussian)을 사용하여 중간 컨텍스트 정보(middle-context information)를 우선시함으로써 LLM의 컨텍스트 길이(context length)를 확장하는 훈련 효율적인 방법입니다. 긴 컨텍스트 처리는 LLM의 핵심 능력 중 하나이며, CREAM은 이를 효율적으로 달성하는 방법을 제시합니다.

**Simple and Effective Masked Diffusion Language Models** by Sahoo, Arriola, Schiff, et al. (6월 11일), https://arxiv.org/abs/2406.07524
이 연구는 마스크된 이산 확산 모델(masked discrete diffusion models)이 효과적인 레시피(recipe)와 단순화된 목표(objective)로 훈련될 때, 언어 모델링(language modeling)에서 자기회귀 방법(autoregressive methods)과의 성능 격차를 크게 좁힐 수 있음을 보여줍니다.

**TextGrad: Automatic "Differentiation" via Text** by Yuksekgonul, Bianchi, Boen, et al. (6월 11일), https://arxiv.org/abs/2406.07496
TextGrad는 LLM을 활용하여 복합 AI 시스템(compound AI systems)의 구성 요소(building blocks)(예: "도구 호출자(tool caller)", "검색 엔진(search engine)" 등)를 최적화하기 위해 텍스트 피드백(textual feedback)을 "역전파(backpropagate)"하는 프레임워크(framework)입니다.

**An Image is Worth 32 Tokens for Reconstruction and Generation** by Yu, Weber, Deng, et al. (6월 11일), https://arxiv.org/abs/2406.07550
저자들은 이미지 생성(image generation)을 위한 트랜스포머 기반 1차원 토크나이저(transformer-based 1-dimensional tokenizer)를 제안하며, 이는 256x256x3 이미지를 단 32개의 이산 토큰(discrete tokens)으로 줄입니다.

**\* Self-Tuning: Instructing LLMs to Effectively Acquire New Knowledge through Self-Teaching** by Zhang, Peng, Zhou, et al. , (6월 10일), https://arxiv.org/abs/2406.06326
셀프 튜닝(Self-Tuning) 프레임워크(framework)는 기억(memorization), 이해(comprehension) 및 자기 성찰(self-reflection)에 초점을 맞춘 자기 학습 작업(self-teaching tasks)을 통해 원본 문서(raw documents)로부터 LLM의 지식 습득(knowledge acquisition)을 향상시킵니다. 이는 LLM이 스스로 학습하고 개선하는 자율 학습(autonomous learning)의 중요한 단계입니다.

**Turbo Sparse: Achieving LLM SOTA Performance with Minimal Activated Parameters** by Song, Xie, Zhang, et al. (6월 10일), https://arxiv.org/abs/2406.05955
이 논문은 LLM의 활성화 희소성(activation sparsity)을 개선하기 위해 dReLU 활성화 함수(activation function)와 최적화된 훈련 데이터 혼합(training data mixture)을 제안합니다.

**Husky: A Unified, Open-Source Language Agent for Multi-Step Reasoning** by Kim, Paranjape, Khot, and Hajishirzi (6월 10일), https://arxiv.org/abs/2406.06469
Husky는 전문가 모델(expert models)과 함께 행동을 생성하고 실행하는 것을 반복함으로써 숫자, 표 형식(tabular) 및 지식 기반 추론(knowledge-based reasoning)을 포함하는 다양한 작업을 처리하기 위해 통합된 행동 공간(unified action space)에서 추론(reason)하는 방법을 학습하는 오픈 소스(open-source) 언어 에이전트(language agent)입니다.

**Margin-aware Preference Optimization for Aligning Diffusion Models Without Reference** by Hong, Paul, Lee, et al. (6월 10일), https://arxiv.org/abs/2406.06424
RLHF 및 DPO와 같은 전통적인 정렬 기술(alignment techniques)의 한계를 해결하기 위해, 저자들은 텍스트-이미지 확산 모델(text-to-image diffusion models)을 위한 마진 인식 선호도 최적화(Margin-Aware Preference Optimization, MaPO)를 제안합니다. 이는 참조 모델(reference model)을 사용하지 않고 선호되는 이미지 세트(preferred image sets)와 비선호되는 이미지 세트(dispreferred image sets) 사이의 가능성 마진(likelihood margin)을 최대화합니다.

**\* Autoregressive Model Beats Diffusion: Llama for Scalable Image Generation** by Sun, Jian, Chen, et al. (6월 10일), https://arxiv.org/abs/2406.06525
저자들은 대규모 언어 모델(large language models)의 "다음 토큰 예측(next-token prediction)" 패러다임(paradigm)을 이미지 생성(image generation)에 적용하는 LlamaGen을 제안합니다. 이는 LLM이 언어뿐만 아니라 이미지 생성에서도 강력한 잠재력을 가지고 있음을 보여줍니다.

**Creativity Has Left the Chat: The Price of Debiasing Language Models** by Mohammidi (6월 8일), https://arxiv.org/abs/2406.05587
이 연구는 RLHF와 같은 정렬 기술(alignment techniques)이 LLM의 편향(biases)을 완화하지만, 모델의 창의적 능력(creative capabilities)을 감소시켜 구문적(syntactic) 및 의미론적 다양성(semantic diversity)에 영향을 미칠 수 있음을 밝혀냅니다. 이는 창의적 출력(creative output)을 요구하는 작업에 중요합니다. LLM의 윤리적 사용과 창의성 사이의 균형점을 찾는 것이 중요함을 시사합니다.

**3D-GRAND: A Million-Scale Dataset for 3D-LLMs with Better Grounding and Less Hallucination** by Yang, Chen, Madaan, et al. (6월 7일), https://arxiv.org/abs/2406.05132
이 연구는 40,087개의 가정 장면과 620만 개의 장면-언어 명령어(scene-language instructions)로 구성된 데이터셋(dataset)인 3D-GRAND를 소개하며, 명령어 튜닝(instruction tuning)과 3D-POPE 벤치마크(benchmark)를 활용하여 3D-LLM의 그라운딩 능력(grounding capabilities)을 향상시키고 환각(hallucinations)을 줄입니다.

**BERTs are Generative In-Context Learners** by Samuel (6월 7일), https://arxiv.org/abs/2406.04823
이 논문은 DeBERTa와 같은 마스크된 언어 모델(masked language models)이 인과적 어텐션 마스크(causal attention mask)의 구조와 유사한 마스크 토큰(mask tokens)으로 입력 토큰(input tokens)의 시퀀스(sequence)를 재구성하는 간단한 추론 기술(inference technique)을 사용하여 인컨텍스트 학습(in-context learning)을 수행할 수 있음을 보여줍니다.

**June 7, Mixture-of-Agents Enhances Large Language Model Capabilities**, https://arxiv.org/abs/2406.04692

**WildBench: Benchmarking LLMs with Challenging Tasks from Real Users in the Wild** by Lin, Deng, Chandu, et al. (6월 7일), https://arxiv.org/abs/2406.04770
저자들은 실제 사용자 쿼리(user queries)를 사용하여 LLM을 벤치마킹(benchmarking)하기 위한 자동화된 평가 프레임워크(evaluation framework)를 소개하며, 1,024개의 작업과 두 가지 고급 지표(advanced metrics)인 WB-Reward 및 WB-Score를 특징으로 합니다. 이들은 작업별 체크리스트(task-specific checklists)와 구조화된 설명(structured explanations)을 사용하여 신뢰할 수 있고 해석 가능한 자동 판단(automatic judgments)을 제공합니다.

**CRAG -- Comprehensive RAG Benchmark** by Yang, Sun, Xin, et al. (6월 7일), https://arxiv.org/abs/2406.04744
이 연구는 웹 및 지식 그래프 검색(Knowledge Graph searches)을 시뮬레이션(simulate)하는 모의 API(mock APIs)를 포함하는 4,409개의 질문-답변 쌍으로 구성된 사실 질문 답변 데이터셋(factual question answering dataset)을 소개하며, 다양하고 동적인 실제 QA 작업(real-world QA tasks)을 반영하도록 설계되었습니다.

**Boosting Large-scale Parallel Training Efficiency with C4: A Communication-Driven Approach** by Dong, Luo, Zhang, et al. (6월 7일), https://arxiv.org/abs/2406.04594
이 연구는 LLM의 병렬 훈련(parallel training)을 위한 통신 중심 솔루션(communication-driven solution)인 C4를 소개합니다. 이는 하드웨어 오류(hardware faults)를 신속하게 식별하고 격리하며, 네트워크 혼잡(network congestion)을 줄이기 위해 트래픽 계획(traffic planning)을 최적화하여 오류로 인한 오버헤드(error-induced overhead)를 최대 30% 줄이고 런타임 성능(runtime performance)을 최대 15% 향상시킬 수 있습니다.

**Step-aware Preference Optimization: Aligning Preference with Denoising Performance at Each Step** by Liang, Yuan, Gu, et al. (6월 6일), https://arxiv.org/abs/2406.04314
이 연구는 텍스트-이미지 확산 모델(text-to-image diffusion models)에서 각 단계의 노이즈 제거 성능(denoising performance)을 독립적으로 평가하고 조정하는 후처리 훈련(post-training) 접근 방식인 단계 인식 선호도 최적화(Step-aware Preference Optimization)를 소개합니다. 이는 이미지 정렬(image alignment) 및 미학(aesthetics)에서 Diffusion-DPO보다 우수하며 20배 빠른 훈련 효율성(training efficiency)을 제공합니다.

**\* Are We Done with MMLU?** by Gema, Leang, Hong, et al. (6월 6일), https://arxiv.org/abs/2406.04127
이 연구는 널리 사용되는 MMLU 벤치마크(benchmark)에서 수많은 오류를 식별하고, 보고된 모델 성능(model performance)에서 상당한 불일치를 드러내는 MMLU-Redux라는 재주석된 서브셋(re-annotated subset)을 생성하며, MMLU의 신뢰성(reliability)을 향상시키기 위해 MMLU를 수정할 것을 주장합니다. 이는 LLM 평가의 견고성에 대한 중요한 질문을 던집니다.

**\* Transformers Need Glasses! Information Over-Squashing in Language Tasks** by Barbero, Banino, Kapturowski, et al. (6월 6일), https://arxiv.org/abs/2406.04267
이 연구는 LLM(특히 디코더 전용 트랜스포머(decoder-only transformers))의 정보 전파(information propagation)를 분석하여, 서로 다른 입력 시퀀스(input sequences)가 임의로 유사한 최종 토큰 표현(final token representations)을 생성할 수 있는 표현 붕괴 현상(representational collapse phenomenon)을 밝혀냅니다. 이는 계산 또는 복사(counting or copying)와 같은 작업에서 오류를 유발하고 특정 입력 토큰(input tokens)에 대한 민감도(sensitivity)를 상실하게 합니다. 이는 트랜스포머의 근본적인 한계를 지적하며, 미래 아키텍처 설계에 중요한 고려 사항을 제공합니다.

**The Prompt Report: A Systematic Survey of Prompting Techniques** by Schulhoff, Ilie, Balepur, et al. (6월 6일), https://arxiv.org/abs/2406.06608
이 76페이지 분량의 논문은 프롬프트(prompts) 및 프롬프트 기술(prompting techniques)을 이해하기 위한 명확하고 체계적인 프레임워크(framework)를 제공하는 것을 목표로 합니다.

**Buffer of Thoughts: Thought-Augmented Reasoning with Large Language Models** by Yang, Yu, Zhang, et al. (6월 6일), https://arxiv.org/abs/2406.04271
이 Buffer of Thoughts 접근 방식은 다양한 도메인(domains)에 걸쳐 추론(reasoning)을 위해 일반적인 문제 해결 청사진(problem-solving blueprints)인 사고 템플릿(thought-templates)을 검색하고 인스턴스화(instantiating)함으로써 LLM을 개선합니다.

**Block Transformer: Global-to-Local Language Modeling for Fast Inference** (6월 4일) by Ho, Bae, Kim, et al. , https://arxiv.org/abs/2406.02657
제안된 블록 트랜스포머(Block Transformer)는 비용이 많이 드는 전역 어텐션(global attention)을 고정 크기 토큰 블록(fixed-size token blocks)의 하위 레이어(lower layers)로 격리하고 상위 레이어(upper layers)에서 빠른 지역 어텐션(local attention)을 적용하여 추론 처리량(inference throughput)을 10-20배 향상시킵니다.

**\* Scalable MatMul-free Language Modeling** by Zhu, Zhang, Sifferman, et al. (6월 4일), https://arxiv.org/abs/2406.02528
이 논문은 행렬 곱셈(matrix multiplications)을 요소별 곱셈(element-wise products)과 삼진 가중치(ternary weights)를 사용한 누적(accumulations)으로 대체하는 확장 가능한 MatMul-free 언어 모델 아키텍처(language model architecture)를 제시하며, 이는 수십억 매개변수(billion-parameter) 규모에서도 잘 작동합니다. 이는 LLM의 계산 효율성을 극대화하는 혁신적인 접근법입니다.

**Towards Scalable Automated Alignment of LLMs: A Survey** , (6월 3일) by Cao, Lu, Lu, et al. https://arxiv.org/abs/2406.01252
이 논문은 LLM 개발 파이프라인(pipeline)에서 일반적으로 명령어 미세 조정(instruction finetuning) 단계를 따르는 LLM을 위한 최근 및 새로 부상하는 자동화된 정렬 방법(automated alignment methods)을 검토합니다.

**The Geometry of Categorical and Hierarchical Concepts in Large Language Models** by by Park, Choe, Jiang, and Veitch (6월 3일), https://arxiv.org/abs/2406.01506
Gemma LLM을 사용하여, 이 논문은 선형 표현 가설(linear representation hypothesis)을 확장하여 범주형 개념(categorical concepts)은 단체(simplices)이고, 계층적 관계(hierarchical relations)는 직교하며, 복잡한 개념(complex concepts)은 다면체(polytopes)임을 보여주며, 957개의 WordNet 개념(concepts)으로 검증되었습니다.

**OLoRA: Orthonormal Low-Rank Adaptation of Large Language Models** by Büyükakyüz (6월 3일), https://arxiv.org/abs/2406.01775
OLoRA는 QR 분해(QR decomposition)를 통한 직교 행렬 초기화(orthonormal matrix initialization)를 사용하는 저랭크 적응(Low-Rank Adaptation, LoRA)의 개선 버전으로, 일반 LoRA에 비해 LLM 훈련의 수렴(convergence)을 가속화합니다.

**Skywork-MoE: A Deep Dive into Training Techniques for Mixture-of-Experts Language Models** by Wei, Zhu, Zhao et al. (6월 3일), https://arxiv.org/abs/2406.06563
기존 13B 매개변수 밀집(dense)(비전문가 혼합) 모델에서 146B 매개변수 전문가 혼합(mixture-of-experts) LLM을 개발하는 데 사용된 접근 방식과 방법 중 일부를 설명하는 보고서입니다.

**Show, Don't Tell: Aligning Language Models with Demonstrated Feedback** by Shaikh, Lam, Hejna, et al. (6월 2일), https://arxiv.org/abs/2406.00888
제안된 방법은 모방 학습(imitation learning)을 활용하여 10개 미만의 데모(demonstrations)를 피드백(feedback)으로 사용하여 LLM 출력을 특정 사용자 행동(user behaviors)에 정렬합니다.

6월의 연구 동향을 종합해 볼 때, LLM 개발은 몇 가지 핵심 방향으로 수렴하고 있음을 알 수 있습니다. 첫째, **데이터의 질과 효율성**에 대한 강조가 커지고 있습니다. Magpie와 같은 합성 데이터 생성 기술은 고품질의 학습 데이터를 저비용으로 확보하는 새로운 가능성을 열었으며, FineWeb 같은 대규모 큐레이션 데이터셋은 모델의 기반을 더욱 튼튼히 하고 있습니다. 둘째, **모델 아키텍처의 혁신과 효율성** 추구가 지속되고 있습니다. Gemma 2의 슬라이딩 윈도우 어텐션, 그룹 쿼리 어텐션, 그리고 MoE 모델에 대한 심층 연구는 계산 자원 효율성을 높이면서도 성능을 유지하거나 향상시키는 방법을 모색합니다. 셋째, **LLM의 신뢰성과 제어 가능성**에 대한 연구가 활발합니다. 불확실성 추정, 명령어 길이 제약 준수, 그리고 편향 완화와 창의성 사이의 균형에 대한 탐구는 LLM이 더욱 책임감 있고 유용한 도구가 되기 위한 필수적인 과정입니다. 마지막으로, **멀티모달리티(Multimodality)와 에이전트(Agent) 능력**의 확장은 LLM이 단순한 텍스트 생성기를 넘어 복잡한 현실 세계 문제를 해결하는 지능형 시스템으로 진화하고 있음을 보여줍니다. 이러한 연구 동향은 앞으로 LLM이 더욱 강력하고 실용적인 인공지능 기술로 자리매김할 것임을 시사합니다.

이 매거진은 직접적인 보상을 제공하지 않는 개인적인 열정 프로젝트입니다. 그러나 저를 지원하고 싶으신 분들은 제 책 중 한 권을 구매해 주시면 감사하겠습니다. 이 책들이 통찰력 있고 유익하다고 생각하시면, 친구와 동료들에게 자유롭게 추천해 주십시오. (Amazon에 책 리뷰를 통해 다른 사람들과 피드백을 공유하는 것도 큰 도움이 됩니다!)

Build A Large Language Model (From Scratch), Machine Learning Q And AI, and Machine Learning with PyTorch and Scikit-Learn

여러분들의 지속적인 관심과 격려는 저에게 큰 힘이 됩니다. 감사합니다!