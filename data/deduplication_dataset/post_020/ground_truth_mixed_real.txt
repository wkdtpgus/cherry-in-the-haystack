지난 몇 달간 AI 분야에서는 끊임없는 혁신이 있었습니다. 애플은 온디바이스 LLM(on-device LLM) 통합을 발표했고, 엔비디아는 대규모 네모트론(Nemotron) 모델을 공개했으며, FlashAttention-3가 발표되었고, 구글의 Gemma 2가 출시되는 등 다양한 소식이 있었습니다. 이러한 소식들은 이미 여러 뉴스 매체를 통해 접하셨을 것입니다. 따라서 이 글에서는 LLM 훈련을 위한 근본적인 기술인 인스트럭션 파인튜닝(instruction finetuning)에 초점을 맞춘 최근 연구 동향에 대해 다루고자 합니다.

이 글에서 다룰 내용은 다음과 같습니다:
*   인스트럭션 파인튜닝(instruction finetuning)을 위한 데이터를 생성하는 새롭고 비용 효율적인 방법
*   처음부터 시작하는 인스트럭션 파인튜닝(instruction finetuning)의 실용적 구현
*   인스트럭션 데이터(instruction data)를 사용한 LLM 사전 훈련(pretraining) 방법론
*   Gemma 2 모델의 주요 기술적 특징과 현재 위치
*   최근 몇 달간 발표된 다른 흥미로운 연구 논문 개요

즐거운 독서 되시길 바랍니다!

## 1. 처음부터 정렬 데이터(Alignment Data) 생성하기

"The Magpie: Alignment Data Synthesis from Scratch by Prompting Aligned LLMs with Nothing" 논문은 LLM 인스트럭션 파인튜닝(instruction finetuning)을 위한 고품질 데이터셋을 생성하는 흥미로운 해킹(hack) 방법을 공유합니다. 이 방법은 최신 연구 동향을 반영하면서도, 여전히 매우 유용하고 실용적인 활용 사례로 평가받고 있습니다.

### 1.1 아무것도 없는 상태에서 인스트럭션 데이터셋(Instruction Dataset) 생성하기

이 인스트럭션 데이터 생성(instruction-data-generating) 방법이 다른 방법들과 구별되는 점은 완전히 자동화될 수 있으며, 초기 질문이나 인스트럭션이 필요 없다는 것입니다. 논문 제목이 시사하듯이, 이 방법은 "아무것도 없는(Nothing)" 상태에서 인스트럭션 데이터셋(instruction dataset)을 생성할 수 있게 합니다. 필요한 유일한 것은 로컬에서 실행되는 Llama 3 8B 모델입니다.

아래 그림은 이 방법이 어떻게 작동하는지 요약합니다.

인스트럭션 파인튜닝(instruction finetuning)을 위한 합성 데이터셋을 생성하는 Magpie 방법론의 주석이 달린 그림.
이 그림은 Magpie 논문의 그림을 기반으로 합니다: https://arxiv.org/abs/2406.08464

기본적으로, 위 그림에서 볼 수 있듯이, 우리는 Llama 3 8B Instruct 모델에 사전 쿼리 템플릿(pre-query template)으로 프롬프트(prompt)를 주기만 하면, 모델이 우리를 위한 인스트럭션(instruction)을 생성할 것입니다. 그런 다음, 그 인스트럭션(instruction)을 다시 LLM에 입력하면, 모델이 응답을 생성합니다. 이 절차를 수천 번 반복하면, 인스트럭션 파인튜닝(instruction finetuning)을 위한 데이터셋을 얻을 수 있습니다. (선택적으로, LLM을 사용하여 인스트럭션-응답 쌍(instruction-response pairs)의 품질을 필터링할 수 있습니다.)

### 1.2 데이터셋 품질

흥미로운 점은, 생성된 인스트럭션 데이터셋을 사용하여 저자들이 인스트럭션 파인튜닝(instruction finetuning)만으로 Llama 3 8B 기본 모델을 파인튜닝(finetuning)하는 것이 (RLHF 및 DPO를 통한 선호도 파인튜닝(preference finetuning) 없이) Meta AI의 오리지널 Llama 2 8B Instruct 모델을 능가한다는 것을 발견했다는 것입니다. 이는 아래 그림에 나와 있습니다.

Magpie가 생성한 인스트럭션 데이터셋으로 파인튜닝(finetuned)된 Llama 3 8B 기본 모델이 오리지널 Llama 3 8B Instruct 모델을 능가합니다.
Magpie 논문의 주석이 달린 그림을 기반으로 합니다: https://arxiv.org/abs/2406.08464

위 그림에 나타난 Magpie 결과는 단 30만 개의 샘플(samples)로 달성되었습니다. 이에 비해, 오리지널 Llama 3 Instruct 모델은 1억 개의 샘플(samples)로 파인튜닝(finetuned)되고 정렬(aligned)되었습니다!

### 1.3 데이터셋 생성 로컬에서 실행하기

처음에는 회의적이었지만, 직접 구현해 보았습니다. 정말 작동합니다! 여기에서 Ollama를 사용한 제 재구현(reimplementation)을 찾을 수 있으며, 이는 MacBook Air에서도 로컬로 잘 실행됩니다.

로컬에서 실행되는 Magpie 방법의 재구현(reimplementation) 코드 스크린샷.
코드는 여기에서 확인할 수 있습니다.

### 1.4 추가 세부 정보

저자들은 두 가지 데이터셋 세트를 만들었습니다: Llama 3 70B Instruct 모델을 사용한 "Pro" 버전과 Llama 3 8B Instruct 모델을 사용한 "Air" 버전입니다.

이전 그림에서 보았듯이, Magpie-Pro가 생성한 데이터셋은 Llama 3 8B 기본 모델을 인스트럭션 파인튜닝(instruction-finetune)하는 데 사용될 때 Magpie-Air 데이터셋에 비해 약간 더 강력한 모델을 만듭니다.

아래 그림은 LLM을 통해 평가된 데이터셋 품질 및 난이도에 대한 추가 비교를 보여줍니다.

Magpie 논문의 주석이 달린 플롯(plots)으로, Air 및 Pro 데이터셋의 상대적인 데이터셋 품질과 난이도를 보여줍니다.

위 그림에서 볼 수 있듯이, Air 및 Pro 데이터셋의 품질은 대략 동등합니다. 또한, Alpaca 데이터셋이 이들과 어떻게 비교되는지 보는 것도 흥미로웠을 것입니다. (Magpie 데이터가 Alpaca보다 훨씬 고품질이라는 가정이 있지만, 참조 지점(reference point)이 있다면 흥미로울 것입니다.)

더 나아가, 이 논문은 이 데이터셋의 폭 또는 다양성(breadth or diversity)이 Alpaca, Evol Instruct, UltraChat와 같은 다른 인기 있는 인스트럭션 파인튜닝(instruction finetuning) 데이터셋보다 훨씬 크다는 분석을 포함합니다.

또한, 다른 인스트럭션 파인튜닝(instruction finetuning) 데이터셋으로 훈련된 모델과 비교했을 때, Magpie-Pro로 파인튜닝(finetuned)된 모델도 매우 유리하게 비교됩니다.

### 1.5 결론

전반적으로, Magpie는 한편으로는 그 효과성에서 매혹적이고, 다른 한편으로는 많은 실용적인 유용성을 가진 흥미로운 활용 사례라고 생각합니다. 저는 앞으로 범용 인스트럭션 데이터셋(general-purpose instruction datasets)을 구축하는 데 있어 흥미롭고, 간단하며, 비용 효율적인 후보로 확실히 고려할 것입니다. 이후 Magpie와 유사한 접근 방식들이 다양한 오픈 소스 프로젝트에서 활용되며, 합성 데이터 생성의 중요성을 다시 한번 입증했습니다.

## 2. 처음부터 인스트럭션 파인튜닝(Instruction Finetuning)

LLM 인스트럭션 파인튜닝(instruction finetuning)의 심층적인 이해를 돕고자, 제 책의 7장이 Manning 웹사이트를 통해 공개되었음을 알려드립니다.

이것은 책에서 가장 긴 장이며, 인스트럭션 파인튜닝(instruction finetuning) 파이프라인(pipeline)을 처음부터 구현하는 접근 방식을 취합니다. 여기에는 입력 형식 지정(input formatting)부터 사용자 정의 콜레이트 함수(custom collate function)를 사용한 배치 처리(batching), 패딩 토큰(padding tokens) 마스킹(masking), 훈련 루프(training loop) 자체, 그리고 사용자 정의 테스트 세트(custom test set)에서 파인튜닝(finetuned)된 LLM의 응답 품질 채점까지 모든 것이 포함됩니다. (연습 문제에는 프롬프트 스타일(prompt styles) 변경, 인스트럭션 마스킹(instruction masking), LoRA 추가가 포함됩니다.)

즐거운 코딩 되시길 바랍니다!

제 책 "Build a Large Language Model From Scratch"의 7장 개요.
보충 코드 자료는 GitHub에서 여기에서 확인할 수 있습니다.

이 장을 통해 독자 여러분이 LLM 파인튜닝의 복잡한 과정을 직접 구현하며 깊이 있는 지식을 습득하시기를 바랍니다.

## 3. 인스트럭션 사전 훈련(Instruction Pretraining) LLM

"Instruction Pre-Training: Language Models are Supervised Multitask Learners" (https://arxiv.org/abs/2406.14491) 논문에서 연구자들은 원본 텍스트(raw text) 대신 합성 인스트럭션-응답 쌍(synthetic instruction-response pairs)을 포함함으로써 LLM 사전 훈련(pretraining)을 더 효율적으로 만들 수 있는지 조사합니다. (여기서 "원본 텍스트(raw text)"는 특정 형식으로 재처리되지 않은 책, 웹사이트, 논문 등의 텍스트를 의미합니다.) 이 논문에서 제시된 인스트럭션 사전 훈련 개념은 이후 다양한 모델 학습 전략에 영감을 주었습니다.

일반 사전 훈련(regular pretraining)(상단)과 제안된 인스트럭션 사전 훈련(instruction pretraining) 접근 방식(하단)의 비교 (https://arxiv.org/abs/2406.14491의 주석이 달린 그림을 통해)

구체적으로, 연구자들은 이 작업을 위해 특별히 파인튜닝(finetuned)된 LLM인 "인스트럭션 합성기(instruction synthesizer)"를 통해 원본 훈련 코퍼스(corpus) 자체에서 인스트럭션-응답 데이터(instruction-response data)를 생성하는 실험을 진행합니다. (이것이 원본 텍스트(raw text)를 인스트럭션 데이터(instruction data)로 형식화하는 것을 제안하는 첫 번째 논문은 아닙니다. 떠오르는 또 다른 작업은 "Genie: Achieving Human Parity in Content-Grounded Datasets Generation" (https://arxiv.org/abs/2401.14367)입니다. 또한 몇 달 전 사전 훈련(pretraining) 중에 인스트럭션 데이터(instruction data)를 사용하는 또 다른 논문이나 블로그 게시물을 본 기억이 있습니다. 이 방법을 동료들과 논의했지만, 아쉽게도 참조를 찾을 수 없었습니다. 그럼에도 불구하고, 여기서 논의된 논문은 로컬에서 실행되는 공개적으로 사용 가능한 LLM을 기반으로 하며 사전 훈련(pretraining)과 연속 사전 훈련(continual pretraining)을 모두 다루기 때문에 특히 흥미롭습니다.)

### 3.1 인스트럭션 합성기(Instruction Synthesizer)

사전 훈련(pretraining) 및 연속 사전 훈련(continual pretraining) 결과에 대해 자세히 알아보기 전에, 이 방법의 핵심 구성 요소인 인스트럭션 합성기(instruction synthesizer)에 대해 이야기해 봅시다.

이것은 공개적으로 사용 가능한 Mistral 7B v0.1 LLM(작년에 제가 여기에서 썼던 내용: https://magazine.sebastianraschka.com/i/138555764/mistral-b)으로, 원본 텍스트(raw text)에서 인스트럭션-응답 쌍(instruction-response pairs)을 생성하도록 파인튜닝(finetuned)되었습니다. 이 합성기(synthesizer)를 파인튜닝(finetune)하기 위해 연구자들은 질문과 답변이 연결된 위키피디아(Wikipedia)의 구절로 구성된 HotpotQA (https://arxiv.org/abs/1809.09600)와 같은 데이터셋을 사용합니다. 이를 위해 저자들은 상식 추론(commonsense reasoning), 감성 분석(sentiment analysis), 수학 문제(math problems) 등 다양한 작업이 다루어지도록 합니다.

인스트럭션 합성기(instruction synthesizer)의 입력 및 출력 데이터 (https://arxiv.org/abs/2406.14491의 주석이 달린 그림을 통해)

이 인스트럭션 합성기(instruction synthesizer)가 개발되면 (즉, 파인튜닝(finetuned)되면), 대상 LLM을 사전 훈련(pretraining)하기 위한 입력 데이터(input data)를 생성하는 데 사용될 수 있습니다. 인스트럭션 합성기(instruction synthesizer)에 대한 마지막 주목할 만한 세부 사항은 아래 그림과 같이 여러 원본 텍스트(T n )와 인스트럭션-응답 쌍(I n ⊕ R n )이 퓨샷 예제(few-shot examples)로 연결된다는 것입니다.

인스트럭션 합성기(instruction synthesizer)를 파인튜닝(finetuning)하고 사용하는 인스트럭션 데이터(instruction-data) 형식 (https://arxiv.org/abs/2406.14491의 주석이 달린 그림을 통해)

### 3.2 인스트럭션 데이터(Instruction Data)를 사용한 사전 훈련(Pretraining)

이제 인스트럭션-응답 쌍(instruction-response pairs)을 생성하는 방법에 대해 논의했으니, 흥미로운 부분인 이 증강된 데이터셋(augmented dataset)에서 모델이 얼마나 잘 훈련되는지에 대해 알아봅시다.

첫 번째 결과 세트는 처음부터 훈련된 두 개의 작은 모델을 살펴봅니다: 5억 개의 매개변수(parameters)와 13억 개의 매개변수(parameters) (둘 다 Mistral 아키텍처(architecture) 기반).

처음부터 모델을 훈련하는 데 사용된 3가지 다른 사전 훈련(pretraining) 접근 방식 비교 (https://arxiv.org/abs/2406.14491의 주석이 달린 표)

위 표에서 볼 수 있듯이, 제안된 인스트럭션 사전 훈련(instruction pretraining) 접근 방식( **Instruct PT** )을 통해 훈련된 모델은 대부분의 벤치마크(benchmark) 작업에서 가장 좋은 성능을 보입니다(값이 높을수록 좋습니다). 그러나 합성된 인스트럭션-응답 쌍(instruction-response pairs)이 포함되었기 때문에 Vanilla PT 접근 방식보다 더 많은 토큰(tokens)을 보았다는 점에 유의하십시오. 따라서 저자들은 원본 텍스트(raw text)와 합성기(synthesizer) 훈련에 사용된 인스트럭션 데이터(instruction data)를 모두 포함하는 데이터 혼합(data mix)으로 훈련된 모델인 Mix PT 비교를 포함했습니다.

이 비교를 통해 우리는 단순히 어떤 인스트럭션 데이터(instruction data)를 사용하는 것만으로는 차이가 나지 않는다는 것을 알 수 있습니다. Instruct PT가 대부분의 작업에서 Mix PT보다 더 나은 성능을 보인다는 사실은 인스트럭션-응답 데이터(instruction-response data)의 특성(즉, 원본 데이터(raw data)와 관련된 인스트럭션-응답 데이터(instruction-response data))이 차이를 만든다는 것을 보여줍니다. (저자들은 동일한 수의 토큰(tokens)을 사용하여 모든 실험을 수행했습니다.)

또한, Instruct PT로 사전 훈련(pretrained)된 모델에는 또 다른 장점이 있습니다. 아래 그림에서 볼 수 있듯이, 나중에 인스트럭션 파인튜닝(instruction-finetuned)될 때 더 많이 개선됩니다.

전통적인 사전 훈련(pretraining) 패러다임(Vanilla PT) 또는 인스트럭션 사전 훈련(instruction pretraining)으로 사전 훈련(pretrained)된 LLM 파인튜닝(finetuning) (https://arxiv.org/abs/2406.14491의 주석이 달린 그림)

### 3.3 인스트럭션 데이터(Instruction Data)를 사용한 연속 사전 훈련(Continual Pretraining)

처음부터 사전 훈련(pretraining)하는 것은 LLM이 처음 만들어지는 방식이기 때문에 흥미롭습니다. 그러나 실무자들은 연속 사전 훈련(continual pretraining)과 파인튜닝(finetuning)에 더 많은 관심을 가질 것이라고 생각합니다. 여기서 연속 사전 훈련(continual pretraining)은 기존의 사전 훈련(pretrained)된 모델을 가져와 새로운 도메인 데이터(domain data)로 추가로 사전 훈련(pretrain)하는 것을 의미합니다. 예를 들어, 일반 텍스트 코퍼스(text corpus)로 훈련된 Llama 3 8B 기본 모델을 금융, 의료, 법률 또는 기타 도메인에 맞게 조정하고 싶다고 생각해 보십시오.

아래 표는 연구자들이 사전 훈련(pretrained)된 Llama 3 8B 기본 모델에 인스트럭션 사전 훈련(instruction pretraining) 방법을 적용했을 때 얻은 결과를 요약합니다. 구체적으로, 그들은 생의학 텍스트(biomedical texts)와 금융 텍스트(finance texts) 모두로 연속 사전 훈련(continual pretraining)을 수행했습니다.

연속 사전 훈련(continual pretraining)에 사용된 3가지 다른 사전 훈련(pretraining) 접근 방식 비교 (https://arxiv.org/abs/2406.14491의 주석이 달린 표)

위 표를 보면, 인스트럭션 사전 훈련(instruction pretraining) 접근 방식( **Instruct PT** )이 바닐라 사전 훈련(vanilla pretraining)( **Vanilla PT** ) 접근 방식(여기서는 기본 모델의 일반적인 연속 사전 훈련(continual pretraining)을 의미)보다 분명히 우수한 성능을 보인다는 것을 알 수 있습니다. Llama 3 70B 기본 모델은 참조용으로 포함되었으며, 이는 작은 전문화된 모델이 더 큰 일반 모델을 능가할 수 있음을 보여주기 위한 것이라고 생각합니다.

### 3.4 결론

LLM 사전 훈련(pretraining) 파이프라인(pipeline)을 누군가에게 설명할 때마다, 그들은 그 단순함과 이것이 오늘날 LLM을 훈련하는 데 여전히 일반적으로 사용되는 방식이라는 사실에 놀라곤 합니다. 그런 의미에서 인스트럭션 사전 훈련(instruction pretraining) 접근 방식은 상당히 신선합니다. 한 가지 주의할 점은 대규모 사전 훈련(pretraining) 코퍼스(corpora)의 경우, 인스트럭션 증강 코퍼스(instruction-augmented corpora)를 생성하는 데 여전히 비용이 많이 들 수 있다는 것입니다. 그러나 생성된 데이터의 좋은 점은 일단 생성되면 여러 다른 프로젝트에서 재사용될 수 있다는 것입니다.

## 4. Gemma 2

구글의 Gemma 2 모델은 출시 당시 큰 주목을 받았습니다. 그러나 순수한 크기 면에서는 엔비디아의 Nemotron-4 340B가 최고를 차지합니다 (https://arxiv.org/abs/2406.11704).

Gemma 2 모델은 2.6B, 9B, 27B 매개변수(parameter) 버전으로 제공됩니다. Gemma 2의 기술적 세부 사항들은 여전히 중요하므로, 그 핵심 특징과 주목할 만한 업데이트를 살펴보겠습니다.

주요 주제는 훈련 데이터셋(training datasets)의 크기를 반드시 늘리지 않고, 오히려 비교적 작고 효율적인 LLM을 개발하는 데 초점을 맞춘 기술을 탐색하는 것입니다. 구체적으로, 그들은 2.6B 및 9B 매개변수(parameter) 모델을 만들기 위해 세 가지 주요 아키텍처(architectural) 및 훈련 선택을 혼합합니다: 슬라이딩 윈도우 어텐션(sliding window attention), 그룹 쿼리 어텐션(grouped-query attention), 그리고 지식 증류(knowledge distillation)입니다.

### 4.1 슬라이딩 윈도우 어텐션(Sliding window attention)

슬라이딩 윈도우 어텐션(sliding window attention)(예: Mistral에 의해 대중화됨)은 고정된 크기의 어텐션 블록(attention block)을 사용하는 기술로, 아래 그림과 같이 현재 토큰(token)이 모든 이전 토큰(token)이 아닌 특정 수의 이전 토큰(token)에만 어텐션(attention)할 수 있도록 합니다.

슬라이딩 윈도우 어텐션(sliding window attention)을 설명하는 https://arxiv.org/abs/2310.06825의 주석이 달린 그림.

Gemma 2의 경우, 저자들은 일반 어텐션(regular attention)과 슬라이딩 윈도우 어텐션(sliding window attention) 레이어(layers)를 번갈아 사용했습니다. 슬라이딩 어텐션 블록(sliding attention block) 크기는 4096 토큰(tokens)이었고, 총 블록 크기는 8192 토큰(tokens)에 걸쳐 있었습니다.

슬라이딩 윈도우 어텐션(sliding window attention)은 주로 계산 성능(computational performance)을 향상시키는 데 사용되며, 연구자들은 또한 추론(inference) 중에 블록 크기(block size)를 줄일 때 혼란도(perplexity)에 거의 눈에 띄지 않는 차이가 있음을 보여주는 작은 절제 연구(ablation study)를 포함했습니다.

Gemma 2 기술 보고서의 절제 연구(ablation study)는 슬라이딩 윈도우(sliding window)의 블록 크기(block size) 감소가 추론(inference) 중 9B 매개변수(parameter) 모델의 모델링 성능(modeling performance)에 거의 영향을 미치지 않음을 보여줍니다. (GPU 메모리(memory) 개선을 나란히 보는 것도 흥미로웠을 것입니다.)

### 4.2 그룹 쿼리 어텐션(Group-query attention)

그룹 쿼리 어텐션(Group-query attention)(Llama 2 및 3에서처럼)은 다중 쿼리 어텐션(multi-query attention)의 더 일반화된 형태로 간주될 수 있습니다. 이 뒤에 있는 동기는 여러 쿼리 헤드(Query heads)에 대해 동일한 키(Keys) 및 값(Values) 헤드(heads)를 공유하여 훈련 가능한 매개변수(trainable parameters)의 수를 줄이고, 따라서 계산 요구 사항(computational requirements)을 낮추는 것입니다.

Ainslie et al. 2023의 주석이 달린 그림

### 4.3 지식 증류(Knowledge distillation)

지식 증류(Knowledge distillation)의 일반적인 아이디어(MiniLLM, https://arxiv.org/abs/2306.08543에서처럼)는 더 큰 모델(교사 모델)에서 더 작은 모델(학생 모델)로 지식을 전달하는 것입니다. 여기서는 27B(교사) 모델을 처음부터 훈련한 다음, 더 큰 교사 모델의 출력(outputs)을 사용하여 더 작은 2B 및 9B(학생) 모델을 훈련했습니다. 27B 모델은 지식 증류(knowledge distillation)를 사용하지 않았지만, 더 작은 모델을 위한 "교사" 역할을 하기 위해 처음부터 훈련되었습니다.

제 책 "Machine Learning Q and AI"에서 컴퓨터 비전(computer vision) 맥락의 지식 증류(knowledge distillation) 개요.
LLM 맥락에서는 이미지 대신 텍스트를, 클래스 레이블(class labels) 대신 예측된 토큰(predicted tokens)을 생각하십시오.

### 4.4 기타 흥미로운 아키텍처(architecture) 세부 정보

이 논문에는 다른 흥미로운 정보들이 많이 포함되어 있습니다. 예를 들어, Gemma 2의 한 가지 특징은 비교적 큰 어휘 크기(vocabulary size)입니다: 256,000 토큰(tokens). 이는 첫 번째 Gemma 모델과 유사하지만, Llama 3 어휘(128,000)의 두 배, Phi-3 어휘(32,000)의 8배라는 점에서 여전히 주목할 만합니다.

LLM의 어휘 크기(vocabulary size)는 모델이 인식하고 생성할 수 있는 고유한 토큰(단어, 서브워드 또는 문자)의 수를 나타냅니다. LLM에서 큰 어휘 크기(vocabulary size)는 단어와 개념의 더 나은 커버리지(coverage), 다국어 콘텐츠(multilingual content)의 향상된 처리, 그리고 토큰화 아티팩트(tokenization artifacts) 감소를 가능하게 합니다. 그러나 큰 어휘 크기(vocabulary size)는 또한 모델 크기 증가 및 더 큰 임베딩(embedding) 및 출력 레이어(output layers)로 인한 잠재적으로 느린 추론(inference)과 같은 트레이드오프(trade-offs)를 동반합니다. (이러한 단점을 상쇄하는 데 슬라이딩 윈도우 어텐션(sliding window attention)과 다중 쿼리 어텐션 메커니즘(multi-query attention mechanism)이 중요합니다.)

또한 이전에 본 적이 없는 기술인 "로짓 캐핑(logit capping)"에 대한 흥미로운 섹션도 있습니다. 기본적으로, 이는 로짓 값(logit values)을 특정 범위 내로 유지하기 위한 최소-최대 정규화(min-max normalizing) 및 클리핑(clipping)의 한 형태입니다. 이는 훈련 중 안정성(stability)과 기울기 흐름(gradient flow)을 개선하기 위한 것이라고 추정합니다.

`logits ← soft_cap ∗ tanh(logits/soft_cap).`

또한, 이 논문은 이에 대한 자세한 내용을 많이 제공하지는 않지만, 다른 하이퍼파라미터(hyperparameters)를 가진 여러 실행의 모델을 결합하기 위해 모델 병합 기술(model merging techniques)을 활용합니다. (그러나 관심 있는 독자들은 Gemma 2가 이를 위해 사용하는 WARP: On the Benefits of Weight Averaged Rewarded Policies 에서 더 자세히 읽을 수 있습니다.)

모델링 성능(modeling performance) 측면에서 Gemma 2는 3배 더 큰 Llama 3 70B만큼 거의 좋으며, 이전 Qwen 1.5 32B 모델을 능가합니다. 이후 출시된 Qwen 2와 같은 모델들과의 비교도 흥미로운 관전 포인트입니다.

공개적으로 사용 가능한 가중치(weights)를 가진 두 가지 인기 모델인 Llama 3와 Qwen 1.5의 비교. (Gemma 2 기술 보고서의 주석이 달린 표).

개인적으로, Gemma 2 보고서가 일부 아키텍처(architectural) 선택에 대한 절제 연구(ablation studies)를 포함한다는 점이 하이라이트입니다. 이는 한때 학술 연구에서 당연한 것이었지만, LLM 연구에서는 점점 더 드물어지고 있습니다.

Gemma 2 기술 보고서에 포함된 절제 연구(ablation studies) 중 하나의 예. 여기서 "wide"는 28개 레이어(layers)와 24,576의 중간 크기(intermediate size)를 가진 모델을 의미하고, "deep"은 42개 레이어(layers)와 14,336의 중간 크기(intermediate size)를 가진 아키텍처(architecture)를 의미합니다.

### 4.5 결론

구글에서 이렇게 비교적 상세한 기술 보고서를 보는 것은 신선합니다. 모델 자체에 관해서는, Gemma 2는 단일 GPU 환경에서도 준수한 성능을 제공하는 효율적인 모델로 자리매김했습니다. 더 큰 모델의 경우, Llama 3 70B와 Qwen 2 72B가 강력한 경쟁자로 남아 있습니다. Gemma 2의 등장은 효율적인 모델 설계의 중요성을 강조하며, 이후 출시될 소형 및 중형 모델 개발에 중요한 이정표가 되었습니다.

## Ahead of AI 지원하기

Ahead of AI는 직접적인 보상을 제공하지 않는 개인적인 열정 프로젝트입니다. 그러나 저를 지원하고 싶으신 분들은 제 책을 구매해 주시면 감사하겠습니다. 이 책들이 통찰력 있고 유익하다고 생각하시면, 친구와 동료들에게 자유롭게 추천해 주십시오. 잠시 시간을 내어 Amazon에 Machine Learning Q and AI 또는 Machine Learning with PyTorch and Scikit-Learn에 대한 리뷰를 남겨주시면 큰 도움이 될 것입니다! 여러분의 지원은 저에게 큰 의미이며, 이 여정을 계속하는 데 엄청난 도움이 됩니다. 감사합니다!

## 5. 최근 주목할 만한 연구 논문들

아래는 지난 몇 달간 제가 흥미롭게 접했던 연구 논문들의 목록입니다. 이 목록의 길이를 고려하여, 제가 특히 흥미롭다고 생각한 20개에는 별표(*)를 표시했습니다. 그러나 이 목록과 그 주석은 전적으로 제 관심사와 제 프로젝트와의 관련성을 기반으로 한다는 점을 유의하십시오.

**Scaling Synthetic Data Creation with 1,000,000,000 Personas** by Chan, Wang, Yu, et al. (6월 28일), https://arxiv.org/abs/2406.20094
이 연구는 LLM을 활용하여 자동으로 큐레이션(curation)된 방대한 페르소나(persona) 컬렉션인 페르소나 허브(Persona Hub)를 활용하여 다양한 합성 데이터(synthetic data)를 생성하는 페르소나 기반 데이터 합성(persona-driven data synthesis) 방법론을 제안합니다. 이 페르소나 허브는 전 세계 인구의 약 13%를 나타냅니다.

**LLM Critics Help Catch LLM Bugs** by McAleese, Pokorny, Ceron Uribe, et al. (6월 28일), https://arxiv.org/abs/2407.00215
이 연구는 RLHF를 사용하여 모델이 생성한 코드(code)를 평가하는 데 인간을 돕는 "비평가(critic)" 모델을 개발하고, 코드(code) 오류에 대한 자연어 피드백(natural language feedback)을 작성하도록 LLM을 훈련하며, 다양한 작업에서 버그(bug)를 찾아내는 데 있어 그 효과를 입증합니다.

**Direct Preference Knowledge Distillation for Large Language Models** by Li, Gu, Dong, et al. (6월 28일), https://arxiv.org/abs/2406.19774
DPKD는 LLM을 위한 지식 증류(Knowledge Distillation)를 두 단계 프로세스(process)로 재구성합니다. 첫째, 암묵적 보상(implicit reward)과 역 KL 발산(reverse KL divergence)을 결합한 목표를 최적화하고, 둘째, 학생 모델(student model)의 출력보다 교사 모델(teacher model)의 출력에 대한 선호도 확률(preference probability)을 향상시킵니다.

**Changing Answer Order Can Decrease MMLU Accuracy** by Gupta, Pantoja, Ross, et al. (6월 27일), https://arxiv.org/abs/2406.19470
이 연구는 LLM을 위한 MMLU 벤치마크(benchmark)에서 정확도 측정의 견고성(robustness)을 조사하며, 답변 레이블(label) 내용을 섞는 것이 모델(model) 전반에 걸쳐 정확도(accuracy) 감소를 초래하고, 민감도(sensitivity)가 다양하다는 것을 밝혀냅니다.

**From Artificial Needles to Real Haystacks: Improving Retrieval Capabilities in LLMs by Finetuning on Synthetic Data** by Xiong, Papageorgiou, Lee, and Papailiopoulos (6월 27일), https://arxiv.org/abs/2406.19292
이 연구는 LLM의 긴 컨텍스트(long-context) 정보 검색 및 추론 능력(reasoning capabilities)을 향상시키기 위해 숫자 키-값 검색 작업(numerical key-value retrieval tasks)의 합성 데이터셋(synthetic dataset)을 사용하는 파인튜닝(finetuning) 접근 방식을 제안합니다.

**Dataset Size Recovery from LoRA Weights** by Salama, Kahana, Horwitz, and Hoshen (6월 27일), https://arxiv.org/abs/2406.19395
이 연구는 LoRA 행렬의 노름(norm)과 스펙트럼(spectrum)을 분석하여 LoRA를 사용하여 비전 모델(vision model)을 파인튜닝(finetuning)하는 데 사용된 이미지(image) 수를 복구하는 방법을 소개합니다.

**Step-DPO: Step-wise Preference Optimization for Long-chain Reasoning of LLMs** by Azerbayev, Shao, Lin, et al. (6월 26일), https://arxiv.org/abs/2406.18629
이 논문은 LLM의 수학 문제 해결에서 개별 추론 단계(reasoning steps)를 최적화하는 방법인 Step-DPO를 소개하며, 이를 위해 맞춤형 10K 단계별 선호도 쌍 데이터셋(step-wise preference pair dataset)을 사용합니다.

**RouteLLM: Learning to Route LLMs with Preference Data** by Ong, Amjad, et al. (6월 26일), https://arxiv.org/abs/2406.18665
이 연구는 추론(inference) 시 더 강력한 LLM과 더 약한 LLM 사이에서 동적으로 선택하여 비용-성능 트레이드오프(cost-performance trade-offs)를 최적화하는 효율적인 라우터 모델(router models)을 제안합니다.

**\* A Closer Look into Mixture-of-Experts in Large Language Models** by Zhang, Liu, Patel, et al. (6월 26일), https://arxiv.org/abs/2406.18219
이 연구는 Mixture-of-Experts (MoE) LLM의 내부 작동 방식을 살펴보고 뉴런(neuron) 동작, 전문가 선택 기준(expert selection criteria) 및 레이어(layer) 전반의 전문가 다양성(expert diversity)에 대한 통찰력을 공유하며, 이러한 관찰을 기반으로 MoE 설계 및 구현에 대한 실용적인 제안을 제공합니다.

**\* Following Length Constraints in Instructions** by Yuan, Kulikov, Yu, et al. (6월 25일), https://arxiv.org/abs/2406.17744
이 연구는 추론(inference) 시 사용자가 지정한 길이 제약(length constraints)을 따를 수 있는 LLM을 훈련하는 방법을 소개하며, 모델 평가(model evaluation)의 길이 편향(length bias)을 해결하고 길이 제어 작업(length-controlled tasks)에서 표준 인스트럭션 팔로잉 모델(instruction-following models)보다 뛰어난 성능을 보입니다.

**LongIns: A Challenging Long-context Instruction-based Exam for LLMs** by Shaham, Bai, An, et al. (6월 25일), https://arxiv.org/abs/2406.17588
LongIns는 LLM의 긴 컨텍스트(long-context) 능력(capabilities)을 평가하기 위한 새로운 벤치마크(benchmark)로, 검색 및 추론 능력(reasoning abilities)을 평가하기 위해 세 가지 설정(setting)을 사용합니다.

**\* The FineWeb Datasets: Decanting the Web for the Finest Text Data at Scale** by He, Wang, Shen, et al. (6월 25일), https://arxiv.org/abs/2406.17557
이 보고서는 Common Crawl에서 파생된 15조 토큰(token) 데이터셋(dataset)인 FineWeb과 1.3조 토큰(token) 교육 서브셋(educational subset)인 FineWeb-Edu를 소개합니다.

**Adam-mini: Use Fewer Learning Rates To Gain More** by Zhang, Chen, Li, et al . (6월 24일), https://arxiv.org/abs/2406.16793
Adam-mini는 학습률(learning rate) 자원을 전략적으로 줄이고, 헤시안 구조(Hessian structure)를 기반으로 매개변수(parameter)를 분할하며, 매개변수 블록(parameter blocks)에 최적화된 단일 학습률(learning rate)을 할당하여 AdamW와 유사하거나 더 나은 성능을 달성하면서 메모리(memory)를 45-50% 적게 사용하는 제안된 최적화기(optimizer)입니다.

**WARP: On the Benefits of Weight Averaged Rewarded Policies** by Ramé, Ferret, Vieillard, et al. (6월 24일), https://arxiv.org/abs/2406.16768
이 논문은 LLM을 위한 새로운 정렬 전략(alignment strategy)을 소개하며, 세 단계에서 정책(policy)을 병합합니다: 동적 KL 정규화(dynamic KL regularization)를 위한 지수 이동 평균(exponential moving average) 사용, 독립적으로 파인튜닝(fine-tuned)된 정책(policy)의 구형 보간(spherical interpolation), 그리고 초기화(initialization)를 통한 선형 보간(linear interpolation)입니다.

**Sparser is Faster and Less is More: Efficient Sparse Attention for Long-Range Transformers** by Lou, Jia, Zheng, and Tu (6월 24일), https://arxiv.org/abs/2406.16747
저자들은 자기회귀 트랜스포머(autoregressive Transformers)를 위한 새로운 희소 어텐션 메커니즘(sparse attention mechanism)을 제안하며, 스코어링 네트워크(scoring network)와 미분 가능한 top-k 마스크 연산자(differentiable top-k mask operator)를 사용하여 쿼리(query)당 일정한 수의 KV 쌍을 선택함으로써 선형 시간 복잡도(linear time complexity)와 일정한 메모리 사용량(constant memory footprint)을 달성합니다.

**Efficient Continual Pre-training by Mitigating the Stability Gap** by Wang, Hu, Xiong, et al. (6월 21일), https://arxiv.org/abs/2406.14833
이 연구는 LLM의 연속 사전 훈련(continual pretraining)을 개선하기 위한 세 가지 전략을 제안합니다: 서브셋(subset)에 대한 여러 에포크(epoch), 고품질 데이터(high-quality data)에 집중, 그리고 사전 훈련 데이터(pretraining data)와 유사한 혼합(mixture) 사용입니다.

**MoA: Mixture of Sparse Attention for Automatic Large Language Model Compression** by Fu, Huang, Ning, et al. (6월 21일), https://arxiv.org/abs/2406.14909
Mixture of Attention (MoA)은 LLM에서 다양한 모델 구성 요소(model components) 및 입력 길이(input lengths)에 대한 희소 어텐션 패턴(sparse attention patterns)을 자동으로 최적화하여, 균일 희소 어텐션(uniform sparse attention) 접근 방식보다 컨텍스트 길이(context length), 정확도(accuracy) 및 효율성(efficiency)을 향상시킵니다.

**LongRAG: Enhancing Retrieval-Augmented Generation with Long-context LLMs** by Jiang, Ma, Chen, et al. (6월 21일), https://arxiv.org/abs/2406.15319
LongRAG는 4K 토큰(token) 검색 단위(retrieval units)와 긴 컨텍스트(long-context) LLM을 사용하여 답변을 추출하는 새로운 RAG 프레임워크(framework)를 소개하며, 이는 추가 훈련 없이 검색 성능(retrieval performance)을 향상시키고 질문-답변 작업(question-answering tasks)에서 최첨단 결과(state-of-the-art results)를 달성합니다.

**\* A Tale of Trust and Accuracy: Base vs. Instruct LLMs in RAG Systems** by Cuconasu, Trappolini, Tonellotto, et al. (6월 21일), https://arxiv.org/abs/2406.14972
이 연구는 기본 LLM이 검색 증강 생성(Retrieval Augmented Generation, RAG) 작업에서 인스트럭션 튜닝(instruction-tuned) 모델보다 우수한 성능을 보인다는 것을 입증함으로써 기존의 통념에 도전합니다.

**Can LLMs Learn by Teaching? A Preliminary Study** by Ning, Wang, Li, Lin, et al. (6월 20일), https://arxiv.org/abs/2406.14629
저자들은 LLM에서 "가르치면서 배우기(Learning by Teaching)"를 구현하기 위한 세 가지 방법을 개발하고 테스트합니다. 이는 학생 피드백(student feedback) 관찰, 피드백(feedback)으로부터 학습, 반복 학습(iterative learning)과 같이 다양한 수준에서 인간의 교육 프로세스(teaching processes)를 모방하여 추가적인 인간 생성 데이터(human-produced data)나 더 강력한 모델(model)에 의존하지 않고 모델 성능(model performance)을 향상시킵니다.

**\* Instruction Pre-Training: Language Models are Supervised Multitask Learners** by Cheng, Gu, Huang, et al. (6월 20일), https://arxiv.org/abs/2406.14491
이 연구는 합성적으로 생성된 인스트럭션-응답 쌍(instruction-response pairs)으로 원본 코퍼스(raw corpora)를 증강하는 LLM의 지도 다중 작업 사전 훈련(supervised multitask pretraining)을 위한 프레임워크(framework)를 소개합니다.

**\* Can Long-Context Language Models Subsume Retrieval, RAG, SQL, and More?** by Wu, Zhang, Johnson, et al. (6월 19일), https://arxiv.org/abs/2406.13121
이 연구는 수백만 개의 토큰(token)을 요구하는 작업에서 긴 컨텍스트(long-context) LLM을 평가하기 위한 벤치마크(benchmark)를 소개하며, 이러한 긴 컨텍스트(long-context) LLM이 인컨텍스트 검색(in-context retrieval) 및 추론 작업(reasoning tasks)에서 전문화된 검색 및 RAG 시스템(RAG systems)과 경쟁할 수 있음을 보여줍니다.

**Judging the Judges: Evaluating Alignment and Vulnerabilities in LLMs-as-Judges** by Ye, Turpin, Li, He, et al. (6월 18일), https://arxiv.org/abs/2406.12624
이 논문은 TriviaQA를 벤치마크(benchmark)로 사용하여 LLM-as-a-judge 패러다임(paradigm)을 평가하며, 9개의 심사 모델(judge models)과 9개의 시험 응시 모델(exam taker models)을 인간 주석(human annotations)과 비교합니다. 그 결과, 높은 인간 정렬(human alignment)을 가진 모델이 반드시 시험 응시 모델(exam taker models)을 순위 매기는 데 가장 적합하지는 않다는 것을 밝혀냅니다.

**From RAGs to Rich Parameters: Probing How Language Models Utilize External Knowledge Over Parametric Information for Factual Queries** by Wadhwa, Seetharaman, Aggarwal, et al. (6월 18일), https://arxiv.org/abs/2406.12824
저자들은 LLM에서 검색 증강 생성(Retrieval Augmented Generation, RAG)의 메커니즘(mechanics)을 조사하여, 모델이 질문에 답할 때 매개변수 기억(parametric memory)보다는 검색된 컨텍스트 정보(retrieved context information)에 주로 의존하며, 다양한 모델 계열(model families)에서 지름길 행동(shortcut behavior)을 보인다는 것을 밝혀냅니다.

**Self-MoE: Towards Compositional Large Language Models with Self-Specialized Experts** by Kang, Karlinsky, and Luo, et al. (6월 17일), https://arxiv.org/abs/2406.12034
이 논문은 모놀리식(monolithic) LLM을 MiXSE(MiXture of Self-specialized Experts)라는 모듈식 시스템(modular system)으로 변환하는 방법을 소개하며, 자체 생성된 합성 데이터(self-generated synthetic data)를 사용하여 공유 기본 LLM 및 자체 최적화된 라우팅(routing)을 갖춘 전문화된 전문가 모듈(specialized expert modules)을 생성합니다.

**Measuring memorization in RLHF for code completion** by Pappu, Porter, Shumailov, and Hayes (6월 17일), https://arxiv.org/abs/2406.11715
이 연구는 코드 완성 작업(code completion tasks)에 초점을 맞춰 LLM의 데이터 기억(data memorization)에 대한 인간 피드백을 통한 강화 학습(Reinforcement Learning with Human Feedback, RLHF)의 영향을 조사하며, RLHF가 직접 파인튜닝(finetuning)에 비해 보상 모델링(reward modeling) 및 강화 학습(reinforcement learning)에 사용된 데이터(data)의 기억(memorization)을 줄이지만, 초기 파인튜닝(finetuning) 단계의 기억(memorization)은 대체로 보존한다는 것을 발견합니다.

**HARE: HumAn pRiors, a key to small language model Efficiency** by Zhang, Jin, Ge, et al. (6월 17일), https://arxiv.org/abs/2406.11410
이 연구는 벤치마크(benchmark) 데이터 유출을 피하면서 의미론적 다양성(semantic diversity)과 데이터 품질 일관성(data quality consistency)에 초점을 맞춘 소형 언어 모델(Small Language Models, SLMs)의 데이터 구성에서 인간 사전 지식(human priors)을 활용하는 원칙을 제안합니다.

**Iterative Length-Regularized Direct Preference Optimization: A Case Study on Improving 7B Language Models to GPT-4 Level** by Kim, Lee, Park, et al. (6월 17일), https://arxiv.org/abs/2406.11817
이 연구는 반복적인 길이 정규화 직접 선호도 최적화(iterative length-regularized Direct Preference Optimization, iLR-DPO)를 소개하며, 이는 응답의 장황함(response verbosity)을 제어하면서 인간의 선호도(human preferences)에 대한 LLM 정렬(alignment)을 개선하는 방법입니다.

**Unveiling Encoder-Free Vision-Language Models** by Choi, Yoon, Lee, et al . (6월 17일), https://arxiv.org/abs/2406.11832
이 연구는 통합된 디코더(decoder)에서 시각 및 텍스트 입력(visual and textual inputs)을 직접 처리하는 인코더 없는 비전-언어 모델(encoder-free vision-language model, VLM)을 제시합니다.

**\* DeepSeek-Coder-V2: Breaking the Barrier of Closed-Source Models in Code Intelligence** by Zhu, Wang, Lee, et al. (6월 17일), https://arxiv.org/abs/2406.11931
DeepSeek-Coder-V2는 추가 6조 토큰(token)에 대한 연속 사전 훈련(continued pretraining)을 통해 코딩 작업(coding tasks)에서 GPT4-Turbo 수준의 성능을 달성하는 오픈 소스(open-source) Mixture-of-Experts 코드 LLM입니다.

**Tokenization Falling Short: The Curse of Tokenization** by Nguyen, Kim, Patel, et al. (6월 17일), https://arxiv.org/abs/2406.11687
이 연구는 복잡한 문제 해결(complex problem solving), 토큰 구조 탐색(token structure probing), 그리고 타이포그래피(typographical) 변화에 대한 복원력(resilience)에서 LLM의 성능을 조사함으로써 LLM의 "토큰화의 저주(curse of tokenization)"를 탐구합니다. 그 결과, 모델 크기(model size)를 확장하는 것이 도움이 되지만, LLM은 토큰화로 인한 편향(tokenization-induced biases)에 여전히 취약하다는 것을 밝혀냅니다.

**DataComp-LM: In Search of the Next Generation of Training Sets for Language Models** by Li, Fang, Smyrnis, et al. (6월 17일), https://arxiv.org/abs/2406.11794
저자들은 언어 모델 훈련에서 데이터셋 큐레이션(dataset curation) 전략을 실험하기 위한 표준화된 테스트베드(testbed)를 제공하며, 여기에는 240조 토큰(token) 코퍼스(corpus), 사전 훈련 레시피(pretraining recipes), 그리고 53개의 다운스트림 평가(downstream evaluations)가 포함됩니다.

**\* Nemotron-4 340B Technical Report** by Unknown Authors at NVIDIA (6월 17일), https://arxiv.org/abs/2406.11704
이 기술 보고서는 다양한 벤치마크(benchmark)에서 경쟁력 있는 성능을 보이고 합성 데이터 생성(synthetic data generation)에 탁월하며, 추가 연구 개발을 위해 데이터 생성 파이프라인(data generation pipeline)을 오픈 소스(open-sourcing)로 공개하는 NVIDIA의 Nemotron-4 340B 모델 제품군(model family) 출시와 함께 제공됩니다.

**mDPO: Conditional Preference Optimization for Multimodal Large Language Models** by Wang, Zhou, Huang, et al. (6월 17일), https://arxiv.org/abs/2406.11839
mDPO는 언어 선호도(language preferences)와 함께 이미지 선호도(image preference)를 최적화하고 선택된 응답에 대한 가능성 감소(likelihood decrease)를 방지하기 위해 보상 앵커(reward anchor)를 도입함으로써 다중 모달 DPO(multimodal DPO)의 무조건적 선호도 문제(unconditional preference problem)를 해결합니다.

**\* How Do Large Language Models Acquire Factual Knowledge During Pretraining?** by Chang, Park, Ye, et al. (6월 17일), https://arxiv.org/abs/2406.11813
이 연구는 LLM이 사전 훈련 과정에서 사실적 지식을 어떻게 습득하고 구조화하는지에 대한 심층적인 분석을 제공하며, 모델의 내부 메커니즘을 탐구합니다.

**Task Me Anything** by Zhang, Huang, Ma, et al. (6월 17일), https://arxiv.org/abs/2406.11775
Task-Me-Anything은 방대한 이미지(image) 및 비디오(video) 분류 체계(taxonomy)에서 작업 인스턴스(task instances)를 프로그래밍 방식으로 생성하여 다중 모달 언어 모델(multimodal language models)을 위한 맞춤형 벤치마크(benchmark)를 생성하는 벤치마크 생성 엔진(benchmark generation engine)입니다.

**THEANINE: Revisiting Memory Management in Long-term Conversations with Timeline-augmented Response Generation** by Kim, Ong, Kwon, et al. (6월 16일), https://arxiv.org/abs/2406.10996
Theanine은 메모리 타임라인(memory timelines)(과거 사건의 발전과 인과 관계를 보여주는 일련의 기억)을 사용하여 LLM의 응답 생성(response generation)을 증강하여, 긴 대화 기록(dialogue histories)에서 정보를 회상하고 활용하는 모델의 능력(ability)을 향상시킵니다.

**Regularizing Hidden States Enables Learning Generalizable Reward Model for LLMs** by Yang, Ding, Lin, et al. (6월 14일) https://arxiv.org/abs/2406.10216
이 연구는 기본 모델의 언어 모델 헤드(language model head)를 유지하고 텍스트 생성 손실(text-generation losses)을 통합하여 은닉 상태(hidden states)를 정규화(regularizing)함으로써 RLHF에서 보상 모델 일반화(reward model generalization)를 향상시키는 방법을 제안합니다. 동시에 보상 헤드(reward head)를 학습하여 분포 외 작업 성능(out-of-distribution task performance)을 개선하고 보상 과최적화(reward over-optimization)를 완화합니다.

**Be like a Goldfish, Don't Memorize! Mitigating Memorization in Generative LLMs** by Hans, Wen, Jain, et al. (6월 14일) , https://arxiv.org/abs/2406.10209
"금붕어 손실(goldfish loss)" 기술은 훈련 중 손실 계산(loss computation)에서 토큰(token)의 서브셋(subset)을 무작위로 제외하여 LLM의 모델 기억(model memorization)을 줄이고, 모델이 훈련 데이터(training data)에서 완전한 문자 그대로의 시퀀스(verbatim sequences)를 학습하는 것을 방지합니다.

**Bootstrapping Language Models with DPO Implicit Rewards** by Chen, Liu, Du, et al. (6월 14일), https://arxiv.org/abs/2406.09760
연구자들은 직접 선호도 최적화(direct preference optimization, DPO) 중에 생성된 정렬된 모델(aligned model), 즉 암묵적 보상 모델(implicit reward model)이 그 자체로 선호도 데이터셋(preference dataset)을 생성하여 스스로를 더욱 크게 개선하는 데 사용될 수 있음을 발견했습니다.

**FouRA: Fourier Low Rank Adaptation** by Borse, Kadambi, Pandey, et al. (6월 13일), https://arxiv.org/abs/2406.08798
이 연구는 푸리에 도메인(Fourier domain)에서 작동하고 적응형 랭크 선택(adaptive rank selection)을 사용하는 새로운 저랭크 적응(low-rank adaptation, LoRA) 방법인 FouRA를 소개합니다. 이는 LoRA 파인튜닝(fine-tuned)된 텍스트-이미지 확산 모델(text-to-image diffusion models)에서 데이터 복사(data copying) 및 분포 붕괴(distribution collapse) 문제를 해결하면서 이미지 품질(image quality)과 일반화(generalization)를 향상시킵니다.

**\* An Image is Worth More Than 16x16 Patches: Exploring Transformers on Individual Pixels** by Nguyen, Mahmoud Assran, Jain, et al. (6월 13일), https://arxiv.org/abs/2406.09415
이 연구는 바닐라 트랜스포머(vanilla Transformers)가 개별 픽셀(pixels)을 토큰(tokens)으로 처리함으로써 다양한 컴퓨터 비전 작업(computer vision tasks)에서 높은 성능을 달성할 수 있음을 밝혀냅니다. 이는 현대 비전 아키텍처(vision architectures)에서 지역성 기반 귀납적 편향(locality-based inductive bias)의 가정된 필요성에 도전하며, 컴퓨터 비전(computer vision) 분야의 미래 신경망 설계(neural network designs)에 대한 새로운 가능성을 제시합니다.

**MLKV: Multi-Layer Key-Value Heads for Memory Efficient Transformer Decoding** by Zuhri, Adilazuarda,Purwarianti, and Aji (6월 13일), https://arxiv.org/abs/2406.09297
이 연구는 트랜스포머(transformer) 레이어(layer) 전반에 걸쳐 키-값(Key-Value, KV) 캐싱(caching)을 확장하는 새로운 기술인 다중 레이어 키-값(Multi-Layer Key-Value, MLKV) 공유를 소개합니다. 이는 다중 쿼리 어텐션(Multi-Query Attention, MQA) 및 그룹 쿼리 어텐션(Grouped-Query Attention, GQA)과 같은 기존 방법보다 자기회귀 추론(auto-regressive inference) 중 메모리 사용량(memory usage)을 크게 줄이면서 NLP 작업(NLP tasks)에서 성능을 유지합니다.

**Transformers Meet Neural Algorithmic Reasoners** by Bounsi, Ibarz, Dudzik, et al. (6월 13일), https://arxiv.org/abs/2406.09308
TransNAR은 트랜스포머(Transformers)와 그래프 신경망 기반 신경 알고리즘 추론기(graph neural network-based neural algorithmic reasoners, NARs)를 결합한 하이브리드 아키텍처(hybrid architecture)로, 트랜스포머(Transformer)가 NAR의 강력한 계산 능력(computational capabilities)을 활용하면서도 강력한 자연어 이해(natural language understanding)를 유지함으로써 알고리즘 추론 작업(algorithmic reasoning tasks)에서 향상된 성능을 가능하게 합니다.

**Discovering Preference Optimization Algorithms with and for Large Language Models** by Lu, Holt, Fanconi, et al. (6월 12일), https://arxiv.org/abs/2406.08414
제안된 발견된 선호도 최적화(Discovered Preference Optimization) 방법은 LLM을 사용하여 LLM 출력(LLM outputs)을 개선하기 위한 새로운 선호도 최적화 알고리즘(preference optimization algorithms)을 자동으로 발견하고 구현합니다.

**\* An Empirical Study of Mamba-based Language Models** by Waleffe, Byeon, Riach, et al. (6월 12일), https://arxiv.org/abs/2406.07887
이 연구는 대규모 데이터셋(large datasets)으로 훈련된 8B 매개변수 상태 공간 모델(state-space models)(Mamba, Mamba-2)과 트랜스포머 모델(Transformer models)을 비교하며, 순수 상태 공간 모델(pure state-space models)이 많은 작업에서 트랜스포머(Transformers)와 같거나 그 이상이지만, 강력한 복사(strong copying), 인컨텍스트 학습(in-context learning) 또는 긴 컨텍스트 추론(long-context reasoning)을 요구하는 작업에서는 뒤처진다는 것을 발견합니다. 그러나 하이브리드(hybrids)는 두 가지 장점을 모두 제공하는 것으로 보입니다.

**\* Large Language Models Must Be Taught to Know What They Don't Know** by Kapoor, Gruver, Roberts, et al. (6월 12일), https://arxiv.org/abs/2406.08391
이 연구는 등급이 매겨진 예제(graded examples)의 작은 데이터셋(dataset)으로 LLM을 파인튜닝(finetuning)하는 것이 프롬프트(prompting)만 사용하는 것보다 더 신뢰할 수 있는 불확실성 추정치(uncertainty estimates)를 생성할 수 있음을 보여주며, 결과 모델(resulting models)은 자신과 다른 모델(models)에 대한 불확실성(uncertainty)을 추정할 수 있습니다.

**Large Language Model Unlearning via Embedding-Corrupted Prompts** by Liu, Flannigan, and Liu (6월 12일), https://arxiv.org/abs/2406.07933
이 연구는 임베딩 손상 프롬프트(embedding-corrupted prompts)를 소개합니다. 이는 LLM에서 선택적 지식 망각(selective knowledge unlearning)을 위한 방법으로, 프롬프트 분류(prompt classification) 및 임베딩 손상(embedding corruption)을 사용하여 광범위한 모델 크기(model sizes)에서 최소한의 부작용으로 목표화된 망각(targeted forgetting)을 달성합니다.

**What If We Recaption Billions of Web Images with LLaMA-3?** by Li, Tu, Hui, et al. (6월 12일) https://arxiv.org/abs/2406.08478
이 연구는 파인튜닝(finetuned)된 Llama 3 기반 LLaVA-1.5 다중 모달 LLM을 사용하여 DataComp-1B 데이터셋(dataset)에서 13억 개의 이미지(image)를 재캡션(recaption)하는 것이 다양한 작업에서 비전-언어 모델(vision-language models)의 성능을 크게 향상시킨다는 것을 보여줍니다.

**\* Magpie: Alignment Data Synthesis from Scratch by Prompting Aligned LLMs with Nothing** by Xu, Jiang, Niu et al. (6월 12일), https://arxiv.org/abs/2406.08464
연구자들은 Llama-3-Instruct에서 30만 개의 고품질 인스트럭션-응답 쌍(instruction-response pairs)을 생성하는 합성 인스트럭션 데이터 생성(synthetic instruction data generation) 방법을 제안합니다. 이 데이터는 실제 정렬 단계(alignment step) 없이 정렬된 LLM의 성능에 필적하는 지도 인스트럭션 파인튜닝(supervised instruction fine-tuning)에 사용될 수 있습니다.

**\* Samba: Simple Hybrid State Space Models for Efficient Unlimited Context Language Modeling** (6월 11일), https://arxiv.org/abs/2406.07522
Samba는 선택적 상태 공간 모델(selective state space models)(Mamba를 생각해보세요)과 슬라이딩 윈도우 어텐션(sliding window attention)을 결합한 하이브리드 모델(hybrid model)로, 3.8B 매개변수(parameters)까지 효율적으로 확장됩니다.

**\* Never Miss A Beat: An Efficient Recipe for Context Window Extension of Large Language Models with Consistent "Middle" Enhancement** (6월 11일) by Wu, Zhao, and Zheng, https://arxiv.org/abs/2406.07138
CREAM은 위치 인코딩(positional encodings)을 보간하고 절단된 가우시안(truncated Gaussian)을 사용하여 중간 컨텍스트 정보(middle-context information)를 우선시함으로써 LLM의 컨텍스트 길이(context length)를 확장하는 훈련 효율적인 방법입니다.

**Simple and Effective Masked Diffusion Language Models** by Sahoo, Arriola, Schiff, et al. (6월 11일), https://arxiv.org/abs/2406.07524
이 연구는 마스크된 이산 확산 모델(masked discrete diffusion models)이 효과적인 레시피(recipe)와 단순화된 목표(objective)로 훈련될 때, 언어 모델링(language modeling)에서 자기회귀 방법(autoregressive methods)과의 성능 격차를 크게 좁힐 수 있음을 보여줍니다.

**TextGrad: Automatic "Differentiation" via Text** by Yuksekgonul, Bianchi, Boen, et al. (6월 11일), https://arxiv.org/abs/2406.07496
TextGrad는 LLM을 활용하여 복합 AI 시스템(compound AI systems)의 구성 요소(building blocks)(예: "도구 호출자(tool caller)", "검색 엔진(search engine)" 등)를 최적화하기 위해 텍스트 피드백(textual feedback)을 "역전파(backpropagate)"하는 프레임워크(framework)입니다.

**An Image is Worth 32 Tokens for Reconstruction and Generation** by Yu, Weber, Deng, et al. (6월 11일), https://arxiv.org/abs/2406.07550
저자들은 이미지 생성(image generation)을 위한 트랜스포머 기반 1차원 토크나이저(transformer-based 1-dimensional tokenizer)를 제안하며, 이는 256x256x3 이미지를 단 32개의 이산 토큰(discrete tokens)으로 줄입니다.

**\* Self-Tuning: Instructing LLMs to Effectively Acquire New Knowledge through Self-Teaching** by Zhang, Peng, Zhou, et al. , (6월 10일), https://arxiv.org/abs/2406.06326
셀프 튜닝(Self-Tuning) 프레임워크(framework)는 기억(memorization), 이해(comprehension) 및 자기 성찰(self-reflection)에 초점을 맞춘 자기 학습 작업(self-teaching tasks)을 통해 원본 문서(raw documents)로부터 LLM의 지식 습득(knowledge acquisition)을 향상시킵니다.

**Turbo Sparse: Achieving LLM SOTA Performance with Minimal Activated Parameters** by Song, Xie, Zhang, et al. (6월 10일), https://arxiv.org/abs/2406.05955
이 논문은 LLM의 활성화 희소성(activation sparsity)을 개선하기 위해 dReLU 활성화 함수(activation function)와 최적화된 훈련 데이터 혼합(training data mixture)을 제안합니다.

**Husky: A Unified, Open-Source Language Agent for Multi-Step Reasoning** by Kim, Paranjape, Khot, and Hajishirzi (6월 10일), https://arxiv.org/abs/2406.06469
Husky는 전문가 모델(expert models)과 함께 행동을 생성하고 실행하는 것을 반복함으로써 숫자, 표 형식(tabular) 및 지식 기반 추론(knowledge-based reasoning)을 포함하는 다양한 작업을 처리하기 위해 통합된 행동 공간(unified action space)에서 추론(reason)하는 방법을 학습하는 오픈 소스(open-source) 언어 에이전트(language agent)입니다.

**Margin-aware Preference Optimization for Aligning Diffusion Models Without Reference** by Hong, Paul, Lee, et al. (6월 10일), https://arxiv.org/abs/2406.06424
RLHF 및 DPO와 같은 전통적인 정렬 기술(alignment techniques)의 한계를 해결하기 위해, 저자들은 텍스트-이미지 확산 모델(text-to-image diffusion models)을 위한 마진 인식 선호도 최적화(Margin-Aware Preference Optimization, MaPO)를 제안합니다. 이는 참조 모델(reference model)을 사용하지 않고 선호되는 이미지 세트(preferred image sets)와 비선호되는 이미지 세트(dispreferred image sets) 사이의 가능성 마진(likelihood margin)을 최대화합니다.

**\* Autoregressive Model Beats Diffusion: Llama for Scalable Image Generation** by Sun, Jian, Chen, et al. (6월 10일), https://arxiv.org/abs/2406.06525
저자들은 대규모 언어 모델(large language models)의 "다음 토큰 예측(next-token prediction)" 패러다임(paradigm)을 이미지 생성(image generation)에 적용하는 LlamaGen을 제안합니다.

**Creativity Has Left the Chat: The Price of Debiasing Language Models** by Mohammidi (6월 8일), https://arxiv.org/abs/2406.05587
이 연구는 RLHF와 같은 정렬 기술(alignment techniques)이 LLM의 편향(biases)을 완화하지만, 모델의 창의적 능력(creative capabilities)을 감소시켜 구문적(syntactic) 및 의미론적 다양성(semantic diversity)에 영향을 미칠 수 있음을 밝혀냅니다. 이는 창의적 출력(creative output)을 요구하는 작업에 중요합니다.

**3D-GRAND: A Million-Scale Dataset for 3D-LLMs with Better Grounding and Less Hallucination** by Yang, Chen, Madaan, et al. (6월 7일), https://arxiv.org/abs/2406.05132
이 연구는 40,087개의 가정 장면과 620만 개의 장면-언어 인스트럭션(scene-language instructions)으로 구성된 데이터셋(dataset)인 3D-GRAND를 소개하며, 인스트럭션 튜닝(instruction tuning)과 3D-POPE 벤치마크(benchmark)를 활용하여 3D-LLM의 그라운딩 능력(grounding capabilities)을 향상시키고 환각(hallucinations)을 줄입니다.

**BERTs are Generative In-Context Learners** by Samuel (6월 7일), https://arxiv.org/abs/2406.04823
이 논문은 DeBERTa와 같은 마스크된 언어 모델(masked language models)이 인과적 어텐션 마스크(causal attention mask)의 구조와 유사한 마스크 토큰(mask tokens)으로 입력 토큰(input tokens)의 시퀀스(sequence)를 재구성하는 간단한 추론 기술(inference technique)을 사용하여 인컨텍스트 학습(in-context learning)을 수행할 수 있음을 보여줍니다.

**June 7, Mixture-of-Agents Enhances Large Language Model Capabilities**, https://arxiv.org/abs/2406.04692

**WildBench: Benchmarking LLMs with Challenging Tasks from Real Users in the Wild** by Lin, Deng, Chandu, et al. (6월 7일), https://arxiv.org/abs/2406.04770
저자들은 실제 사용자 쿼리(user queries)를 사용하여 LLM을 벤치마킹(benchmarking)하기 위한 자동화된 평가 프레임워크(evaluation framework)를 소개하며, 1,024개의 작업과 두 가지 고급 지표(advanced metrics)인 WB-Reward 및 WB-Score를 특징으로 합니다. 이들은 작업별 체크리스트(task-specific checklists)와 구조화된 설명(structured explanations)을 사용하여 신뢰할 수 있고 해석 가능한 자동 판단(automatic judgments)을 제공합니다.

**CRAG -- Comprehensive RAG Benchmark** by Yang, Sun, Xin, et al. (6월 7일), https://arxiv.org/abs/2406.04744
이 연구는 웹 및 지식 그래프 검색(Knowledge Graph searches)을 시뮬레이션(simulate)하는 모의 API(mock APIs)를 포함하는 4,409개의 질문-답변 쌍으로 구성된 사실 질문 답변 데이터셋(factual question answering dataset)을 소개하며, 다양하고 동적인 실제 QA 작업(real-world QA tasks)을 반영하도록 설계되었습니다.

**Boosting Large-scale Parallel Training Efficiency with C4: A Communication-Driven Approach** by Dong, Luo, Zhang, et al. (6월 7일), https://arxiv.org/abs/2406.04594
이 연구는 LLM의 병렬 훈련(parallel training)을 위한 통신 중심 솔루션(communication-driven solution)인 C4를 소개합니다. 이는 하드웨어 오류(hardware faults)를 신속하게 식별하고 격리하며, 네트워크 혼잡(network congestion)을 줄이기 위해 트래픽 계획(traffic planning)을 최적화하여 오류로 인한 오버헤드(error-induced overhead)를 최대 30% 줄이고 런타임 성능(runtime performance)을 최대 15% 향상시킬 수 있습니다.

**Step-aware Preference Optimization: Aligning Preference with Denoising Performance at Each Step** by Liang, Yuan, Gu, et al. (6월 6일), https://arxiv.org/abs/2406.04314
이 연구는 텍스트-이미지 확산 모델(text-to-image diffusion models)에서 각 단계의 노이즈 제거 성능(denoising performance)을 독립적으로 평가하고 조정하는 후처리 훈련(post-training) 접근 방식인 단계 인식 선호도 최적화(Step-aware Preference Optimization)를 소개합니다. 이는 이미지 정렬(image alignment) 및 미학(aesthetics)에서 Diffusion-DPO보다 우수하며 20배 빠른 훈련 효율성(training efficiency)을 제공합니다.

**\* Are We Done with MMLU?** by Gema, Leang, Hong, et al. (6월 6일), https://arxiv.org/abs/2406.04127
이 연구는 널리 사용되는 MMLU 벤치마크(benchmark)에서 수많은 오류를 식별하고, 보고된 모델 성능(model performance)에서 상당한 불일치를 드러내는 MMLU-Redux라는 재주석된 서브셋(re-annotated subset)을 생성하며, MMLU의 신뢰성(reliability)을 향상시키기 위해 MMLU를 수정할 것을 주장합니다.

**\* Transformers Need Glasses! Information Over-Squashing in Language Tasks** by Barbero, Banino, Kapturowski, et al. (6월 6일), https://arxiv.org/abs/2406.04267
이 연구는 LLM(특히 디코더 전용 트랜스포머(decoder-only transformers))의 정보 전파(information propagation)를 분석하여, 서로 다른 입력 시퀀스(input sequences)가 임의로 유사한 최종 토큰 표현(final token representations)을 생성할 수 있는 표현 붕괴 현상(representational collapse phenomenon)을 밝혀냅니다. 이는 계산 또는 복사(counting or copying)와 같은 작업에서 오류를 유발하고 특정 입력 토큰(input tokens)에 대한 민감도(sensitivity)를 상실하게 합니다.

**The Prompt Report: A Systematic Survey of Prompting Techniques** by Schulhoff, Ilie, Balepur, et al. (6월 6일), https://arxiv.org/abs/2406.06608
이 76페이지 분량의 논문은 프롬프트(prompts) 및 프롬프트 기술(prompting techniques)을 이해하기 위한 명확하고 체계적인 프레임워크(framework)를 제공하는 것을 목표로 합니다.

**Buffer of Thoughts: Thought-Augmented Reasoning with Large Language Models** by Yang, Yu, Zhang, et al. (6월 6일), https://arxiv.org/abs/2406.04271
이 Buffer of Thoughts 접근 방식은 다양한 도메인(domains)에 걸쳐 추론(reasoning)을 위해 일반적인 문제 해결 청사진(problem-solving blueprints)인 사고 템플릿(thought-templates)을 검색하고 인스턴스화(instantiating)함으로써 LLM을 개선합니다.

**Block Transformer: Global-to-Local Language Modeling for Fast Inference** (6월 4일) by Ho, Bae, Kim, et al. , https://arxiv.org/abs/2406.02657
제안된 블록 트랜스포머(Block Transformer)는 비용이 많이 드는 전역 어텐션(global attention)을 고정 크기 토큰 블록(fixed-size token blocks)의 하위 레이어(lower layers)로 격리하고 상위 레이어(upper layers)에서 빠른 지역 어텐션(local attention)을 적용하여 추론 처리량(inference throughput)을 10-20배 향상시킵니다.

**\* Scalable MatMul-free Language Modeling** by Zhu, Zhang, Sifferman, et al. (6월 4일), https://arxiv.org/abs/2406.02528
이 논문은 행렬 곱셈(matrix multiplications)을 요소별 곱셈(element-wise products)과 삼진 가중치(ternary weights)를 사용한 누적(accumulations)으로 대체하는 확장 가능한 MatMul-free 언어 모델 아키텍처(language model architecture)를 제시하며, 이는 수십억 매개변수(billion-parameter) 규모에서도 잘 작동합니다.

**Towards Scalable Automated Alignment of LLMs: A Survey** , (6월 3일) by Cao, Lu, Lu, et al. https://arxiv.org/abs/2406.01252
이 논문은 LLM 개발 파이프라인(pipeline)에서 일반적으로 인스트럭션 파인튜닝(instruction finetuning) 단계를 따르는 LLM을 위한 최근 및 새로 부상하는 자동화된 정렬 방법(automated alignment methods)을 검토합니다.

**The Geometry of Categorical and Hierarchical Concepts in Large Language Models** by by Park, Choe, Jiang, and Veitch (6월 3일), https://arxiv.org/abs/2406.01506
Gemma LLM을 사용하여, 이 논문은 선형 표현 가설(linear representation hypothesis)을 확장하여 범주형 개념(categorical concepts)은 단체(simplices)이고, 계층적 관계(hierarchical relations)는 직교하며, 복잡한 개념(complex concepts)은 다면체(polytopes)임을 보여주며, 957개의 WordNet 개념(concepts)으로 검증되었습니다.

**OLoRA: Orthonormal Low-Rank Adaptation of Large Language Models** by Büyükakyüz (6월 3일), https://arxiv.org/abs/2406.01775
OLoRA는 QR 분해(QR decomposition)를 통한 직교 행렬 초기화(orthonormal matrix initialization)를 사용하는 저랭크 적응(Low-Rank Adaptation, LoRA)의 개선 버전으로, 일반 LoRA에 비해 LLM 훈련의 수렴(convergence)을 가속화합니다.

**Skywork-MoE: A Deep Dive into Training Techniques for Mixture-of-Experts Language Models** by Wei, Zhu, Zhao et al. (6월 3일), https://arxiv.org/abs/2406.06563
이 보고서는 기존 13B 매개변수 밀집(dense) 모델에서 146B 매개변수 전문가 혼합(mixture-of-experts) LLM을 개발하는 데 사용된 접근 방식과 방법 중 일부를 설명합니다.

**Show, Don't Tell: Aligning Language Models with Demonstrated Feedback** by Shaikh, Lam, Hejna, et al. (6월 2일), https://arxiv.org/abs/2406.00888
제안된 방법은 모방 학습(imitation learning)을 활용하여 10개 미만의 데모(demonstrations)를 피드백(feedback)으로 사용하여 LLM 출력을 특정 사용자 행동(user behaviors)에 정렬합니다.

---
**추가된 최신 연구 논문들:**

**Context-Aware Dynamic Routing for Adaptive MoE-LLMs**: LLM에서 입력 컨텍스트에 따라 전문가(expert) 라우팅을 동적으로 조절하여 효율성과 성능을 동시에 향상시키는 새로운 MoE(Mixture-of-Experts) 아키텍처를 제안합니다.

**Federated Fine-tuning of LLMs for Data Privacy**: 분산된 데이터 소스에서 개인 정보 보호를 유지하며 LLM을 미세 조정하는 연합 학습(Federated Learning) 기반의 새로운 프레임워크를 제시합니다.

**Neuro-Symbolic Reasoning with Large Language Models**: LLM의 강력한 언어 이해 능력과 심볼릭 추론(symbolic reasoning) 시스템의 논리적 엄격함을 결합하여 복잡한 추론 작업을 해결하는 신경-심볼릭 접근법을 탐구합니다.

**Generative Agents with Long-term Memory and Planning**: 장기 기억 및 계획 능력을 갖춘 생성형 에이전트(generative agents)를 소개하며, 이는 복잡한 환경에서 보다 일관되고 목표 지향적인 상호작용을 가능하게 합니다.

**Efficient Quantization Schemes for On-Device LLM Deployment**: 온디바이스(on-device) 환경에서 LLM의 효율적인 배포를 위해 모델 성능 저하를 최소화하면서 양자화(quantization)를 최적화하는 새로운 기법들을 제시합니다.

**Benchmarking Multimodal LLMs on Complex Real-World Tasks**: 이미지, 비디오, 텍스트 등 다양한 모달리티를 통합하여 복잡한 현실 세계 작업을 수행하는 멀티모달 LLM(Multimodal LLM)의 성능을 평가하는 새로운 벤치마크를 제안합니다.

**Probing Emergent Abilities in Small Language Models**: 소형 언어 모델(SLM)에서도 특정 조건 하에 대형 LLM에서 나타나는 '창발적 능력(emergent abilities)'이 발현될 수 있음을 탐구하고, 그 원인을 분석합니다.

**Knowledge Graph Enhanced RAG for Domain-Specific Applications**: 도메인 특화된 애플리케이션에서 지식 그래프(Knowledge Graph)를 활용하여 RAG(Retrieval-Augmented Generation) 시스템의 정확성과 신뢰성을 향상시키는 방법을 제시합니다.

**Self-Correction Mechanisms for Reducing Hallucinations in LLMs**: LLM의 환각(hallucination) 현상을 줄이기 위해 모델 자체적으로 생성된 응답을 검증하고 수정하는 자가 교정(self-correction) 메커니즘의 효과를 분석합니다.

**Adaptive Tokenization for Multilingual LLMs**: 다국어 LLM의 성능을 최적화하기 위해 언어별 특성을 반영하여 토큰화(tokenization) 전략을 동적으로 조정하는 적응형 토큰화 기법을 제안합니다.

**Beyond RLHF: Direct Preference Elicitation from Human Feedback**: RLHF(인간 피드백을 통한 강화 학습)의 한계를 넘어, 인간 피드백으로부터 선호도를 직접적으로 추출하여 LLM을 정렬하는 새로운 방법론을 탐구합니다.

**The Role of Synthetic Data in Bridging Data Gaps for Low-Resource Languages**: 저자원 언어(low-resource languages)를 위한 LLM 훈련에서 합성 데이터(synthetic data)가 데이터 부족 문제를 해결하고 모델 성능을 향상시키는 데 기여하는 바를 분석합니다.

**Efficient Fine-tuning of Foundation Models with Parameter-Efficient Methods**: 대규모 파운데이션 모델(foundation models)을 특정 작업에 효율적으로 미세 조정하기 위한 매개변수 효율적(parameter-efficient) 학습 방법들의 최신 동향과 성능을 비교 분석합니다.

**Explainable AI for LLMs: Interpreting Decision-Making Processes**: LLM의 복잡한 의사 결정 과정을 인간이 이해할 수 있도록 설명하는 XAI(Explainable AI) 기술을 LLM에 적용하는 다양한 접근 방식과 그 효과를 탐구합니다.

**Real-time LLM Inference on Edge Devices: Challenges and Solutions**: 에지 디바이스(edge devices)에서 LLM을 실시간으로 추론하는 데 따르는 기술적 도전 과제들을 분석하고, 이를 해결하기 위한 최신 하드웨어 및 소프트웨어 최적화 솔루션들을 제시합니다.