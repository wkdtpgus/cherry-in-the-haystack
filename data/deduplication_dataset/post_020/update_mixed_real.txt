지난 몇 달간 AI 분야에서는 끊임없는 혁신이 있었습니다. 애플은 온디바이스 LLM(on-device LLM) 통합을 발표했고, 엔비디아는 대규모 네모트론(Nemotron) 모델을 공개했으며, FlashAttention-3가 발표되었고, 구글의 Gemma 2가 출시되는 등 다양한 소식이 있었습니다. 이러한 소식들은 이미 여러 뉴스 매체를 통해 접하셨을 것입니다. 따라서 이 글에서는 LLM 훈련을 위한 근본적인 기술인 인스트럭션 파인튜닝(instruction finetuning)에 초점을 맞춘 최근 연구 동향에 대해 다루고자 합니다.

이 글에서 다룰 내용은 다음과 같습니다:
*   인스트럭션 파인튜닝(instruction finetuning)을 위한 데이터를 생성하는 새롭고 비용 효율적인 방법
*   처음부터 시작하는 인스트럭션 파인튜닝(instruction finetuning)의 실용적 구현
*   인스트럭션 데이터(instruction data)를 사용한 LLM 사전 훈련(pretraining) 방법론
*   Gemma 2 모델의 주요 기술적 특징과 현재 위치
*   최근 몇 달간 발표된 다른 흥미로운 연구 논문 개요

즐거운 독서 되시길 바랍니다!

## 1. 처음부터 정렬 데이터(Alignment Data) 생성하기

"The Magpie: Alignment Data Synthesis from Scratch by Prompting Aligned LLMs with Nothing" 논문은 LLM 인스트럭션 파인튜닝(instruction finetuning)을 위한 고품질 데이터셋을 생성하는 흥미로운 해킹(hack) 방법을 공유합니다. 이 방법은 최신 연구 동향을 반영하면서도, 여전히 매우 유용하고 실용적인 활용 사례로 평가받고 있습니다.

### 1.1 아무것도 없는 상태에서 인스트럭션 데이터셋(Instruction Dataset) 생성하기

이 인스트럭션 데이터 생성(instruction-data-generating) 방법이 다른 방법들과 구별되는 점은 완전히 자동화될 수 있으며, 초기 질문이나 인스트럭션이 필요 없다는 것입니다. 논문 제목이 시사하듯이, 이 방법은 "아무것도 없는(Nothing)" 상태에서 인스트럭션 데이터셋(instruction dataset)을 생성할 수 있게 합니다. 필요한 유일한 것은 로컬에서 실행되는 Llama 3 8B 모델입니다.

아래 그림은 이 방법이 어떻게 작동하는지 요약합니다.

인스트럭션 파인튜닝(instruction finetuning)을 위한 합성 데이터셋을 생성하는 Magpie 방법론의 주석이 달린 그림.
이 그림은 Magpie 논문의 그림을 기반으로 합니다: https://arxiv.org/abs/2406.08464

기본적으로, 위 그림에서 볼 수 있듯이, 우리는 Llama 3 8B Instruct 모델에 사전 쿼리 템플릿(pre-query template)으로 프롬프트(prompt)를 주기만 하면, 모델이 우리를 위한 인스트럭션(instruction)을 생성할 것입니다. 그런 다음, 그 인스트럭션(instruction)을 다시 LLM에 입력하면, 모델이 응답을 생성합니다. 이 절차를 수천 번 반복하면, 인스트럭션 파인튜닝(instruction finetuning)을 위한 데이터셋을 얻을 수 있습니다. (선택적으로, LLM을 사용하여 인스트럭션-응답 쌍(instruction-response pairs)의 품질을 필터링할 수 있습니다.)

### 1.2 데이터셋 품질

흥미로운 점은, 생성된 인스트럭션 데이터셋을 사용하여 저자들이 인스트럭션 파인튜닝(instruction finetuning)만으로 Llama 3 8B 기본 모델을 파인튜닝(finetuning)하는 것이 (RLHF 및 DPO를 통한 선호도 파인튜닝(preference finetuning) 없이) Meta AI의 오리지널 Llama 2 8B Instruct 모델을 능가한다는 것을 발견했다는 것입니다. 이는 아래 그림에 나와 있습니다.

Magpie가 생성한 인스트럭션 데이터셋으로 파인튜닝(finetuned)된 Llama 3 8B 기본 모델이 오리지널 Llama 3 8B Instruct 모델을 능가합니다.
Magpie 논문의 주석이 달린 그림을 기반으로 합니다: https://arxiv.org/abs/2406.08464

위 그림에 나타난 Magpie 결과는 단 30만 개의 샘플(samples)로 달성되었습니다. 이에 비해, 오리지널 Llama 3 Instruct 모델은 1억 개의 샘플(samples)로 파인튜닝(finetuned)되고 정렬(aligned)되었습니다!

### 1.3 데이터셋 생성 로컬에서 실행하기

처음에는 회의적이었지만, 직접 구현해 보았습니다. 정말 작동합니다! 여기에서 Ollama를 사용한 제 재구현(reimplementation)을 찾을 수 있으며, 이는 MacBook Air에서도 로컬로 잘 실행됩니다.

로컬에서 실행되는 Magpie 방법의 재구현(reimplementation) 코드 스크린샷.
코드는 여기에서 확인할 수 있습니다.

### 1.4 추가 세부 정보

저자들은 두 가지 데이터셋 세트를 만들었습니다: Llama 3 70B Instruct 모델을 사용한 "Pro" 버전과 Llama 3 8B Instruct 모델을 사용한 "Air" 버전입니다.

이전 그림에서 보았듯이, Magpie-Pro가 생성한 데이터셋은 Llama 3 8B 기본 모델을 인스트럭션 파인튜닝(instruction-finetune)하는 데 사용될 때 Magpie-Air 데이터셋에 비해 약간 더 강력한 모델을 만듭니다.

아래 그림은 LLM을 통해 평가된 데이터셋 품질 및 난이도에 대한 추가 비교를 보여줍니다.

Magpie 논문의 주석이 달린 플롯(plots)으로, Air 및 Pro 데이터셋의 상대적인 데이터셋 품질과 난이도를 보여줍니다.

위 그림에서 볼 수 있듯이, Air 및 Pro 데이터셋의 품질은 대략 동등합니다. 또한, Alpaca 데이터셋이 이들과 어떻게 비교되는지 보는 것도 흥미로웠을 것입니다. (Magpie 데이터가 Alpaca보다 훨씬 고품질이라는 가정이 있지만, 참조 지점(reference point)이 있다면 흥미로울 것입니다.)

더 나아가, 이 논문은 이 데이터셋의 폭 또는 다양성(breadth or diversity)이 Alpaca, Evol Instruct, UltraChat와 같은 다른 인기 있는 인스트럭션 파인튜닝(instruction finetuning) 데이터셋보다 훨씬 크다는 분석을 포함합니다.

또한, 다른 인스트럭션 파인튜닝(instruction finetuning) 데이터셋으로 훈련된 모델과 비교했을 때, Magpie-Pro로 파인튜닝(finetuned)된 모델도 매우 유리하게 비교됩니다.

### 1.5 결론

전반적으로, Magpie는 한편으로는 그 효과성에서 매혹적이고, 다른 한편으로는 많은 실용적인 유용성을 가진 흥미로운 활용 사례라고 생각합니다. 저는 앞으로 범용 인스트럭션 데이터셋(general-purpose instruction datasets)을 구축하는 데 있어 흥미롭고, 간단하며, 비용 효율적인 후보로 확실히 고려할 것입니다. 이후 Magpie와 유사한 접근 방식들이 다양한 오픈 소스 프로젝트에서 활용되며, 합성 데이터 생성의 중요성을 다시 한번 입증했습니다.

## 2. 처음부터 인스트럭션 파인튜닝(Instruction Finetuning)

LLM 인스트럭션 파인튜닝(instruction finetuning)의 심층적인 이해를 돕고자, 제 책의 7장이 Manning 웹사이트를 통해 공개되었음을 알려드립니다.

이것은 책에서 가장 긴 장이며, 인스트럭션 파인튜닝(instruction finetuning) 파이프라인(pipeline)을 처음부터 구현하는 접근 방식을 취합니다. 여기에는 입력 형식 지정(input formatting)부터 사용자 정의 콜레이트 함수(custom collate function)를 사용한 배치 처리(batching), 패딩 토큰(padding tokens) 마스킹(masking), 훈련 루프(training loop) 자체, 그리고 사용자 정의 테스트 세트(custom test set)에서 파인튜닝(finetuned)된 LLM의 응답 품질 채점까지 모든 것이 포함됩니다. (연습 문제에는 프롬프트 스타일(prompt styles) 변경, 인스트럭션 마스킹(instruction masking), LoRA 추가가 포함됩니다.)

즐거운 코딩 되시길 바랍니다!

제 책 "Build a Large Language Model From Scratch"의 7장 개요.
보충 코드 자료는 GitHub에서 여기에서 확인할 수 있습니다.

이 장을 통해 독자 여러분이 LLM 파인튜닝의 복잡한 과정을 직접 구현하며 깊이 있는 지식을 습득하시기를 바랍니다.

## 3. 인스트럭션 사전 훈련(Instruction Pretraining) LLM

"Instruction Pre-Training: Language Models are Supervised Multitask Learners" (https://arxiv.org/abs/2406.14491) 논문에서 연구자들은 원본 텍스트(raw text) 대신 합성 인스트럭션-응답 쌍(synthetic instruction-response pairs)을 포함함으로써 LLM 사전 훈련(pretraining)을 더 효율적으로 만들 수 있는지 조사합니다. (여기서 "원본 텍스트(raw text)"는 특정 형식으로 재처리되지 않은 책, 웹사이트, 논문 등의 텍스트를 의미합니다.) 이 논문에서 제시된 인스트럭션 사전 훈련 개념은 이후 다양한 모델 학습 전략에 영감을 주었습니다.

일반 사전 훈련(regular pretraining)(상단)과 제안된 인스트럭션 사전 훈련(instruction pretraining) 접근 방식(하단)의 비교 (https://arxiv.org/abs/2406.14491의 주석이 달린 그림을 통해)

구체적으로, 연구자들은 이 작업을 위해 특별히 파인튜닝(finetuned)된 LLM인 "인스트럭션 합성기(instruction synthesizer)"를 통해 원본 훈련 코퍼스(corpus) 자체에서 인스트럭션-응답 데이터(instruction-response data)를 생성하는 실험을 진행합니다. (이것이 원본 텍스트(raw text)를 인스트럭션 데이터(instruction data)로 형식화하는 것을 제안하는 첫 번째 논문은 아닙니다. 떠오르는 또 다른 작업은 "Genie: Achieving Human Parity in Content-Grounded Datasets Generation" (https://arxiv.org/abs/2401.14367)입니다. 또한 몇 달 전 사전 훈련(pretraining) 중에 인스트럭션 데이터(instruction data)를 사용하는 또 다른 논문이나 블로그 게시물을 본 기억이 있습니다. 이 방법을 동료들과 논의했지만, 아쉽게도 참조를 찾을 수 없었습니다. 그럼에도 불구하고, 여기서 논의된 논문은 로컬에서 실행되는 공개적으로 사용 가능한 LLM을 기반으로 하며 사전 훈련(pretraining)과 연속 사전 훈련(continual pretraining)을 모두 다루기 때문에 특히 흥미롭습니다.)

### 3.1 인스트럭션 합성기(Instruction Synthesizer)

사전 훈련(pretraining) 및 연속 사전 훈련(continual pretraining) 결과에 대해 자세히 알아보기 전에, 이 방법의 핵심 구성 요소인 인스트럭션 합성기(instruction synthesizer)에 대해 이야기해 봅시다.

이 합성기는 공개적으로 사용 가능한 Mistral 7B v0.1 LLM을 기반으로 하며, 원본 텍스트(raw text)에서 인스트럭션-응답 쌍(instruction-response pairs)을 생성하도록 파인튜닝(finetuned)되었습니다. 이 합성기(synthesizer)를 파인튜닝(finetune)하기 위해 연구자들은 질문과 답변이 연결된 위키피디아(Wikipedia)의 구절로 구성된 HotpotQA (https://arxiv.org/abs/1809.09600)와 같은 데이터셋을 사용합니다. 이를 위해 저자들은 상식 추론(commonsense reasoning), 감성 분석(sentiment analysis), 수학 문제(math problems) 등 다양한 작업이 다루어지도록 합니다.

인스트럭션 합성기(instruction synthesizer)의 입력 및 출력 데이터 (https://arxiv.org/abs/2406.14491의 주석이 달린 그림을 통해)

이 인스트럭션 합성기(instruction synthesizer)가 개발되면 (즉, 파인튜닝(finetuned)되면), 대상 LLM을 사전 훈련(pretraining)하기 위한 입력 데이터(input data)를 생성하는 데 사용될 수 있습니다. 인스트럭션 합성기(instruction synthesizer)에 대한 마지막 주목할 만한 세부 사항은 아래 그림과 같이 여러 원본 텍스트(T n )와 인스트럭션-응답 쌍(I n ⊕ R n )이 퓨샷 예제(few-shot examples)로 연결된다는 것입니다.

인스트럭션 합성기(instruction synthesizer)를 파인튜닝(finetuning)하고 사용하는 인스트럭션 데이터(instruction-data) 형식 (https://arxiv.org/abs/2406.14491의 주석이 달린 그림을 통해)

### 3.2 인스트럭션 데이터(Instruction Data)를 사용한 사전 훈련(Pretraining)

이제 인스트럭션-응답 쌍(instruction-response pairs)을 생성하는 방법에 대해 논의했으니, 흥미로운 부분인 이 증강된 데이터셋(augmented dataset)에서 모델이 얼마나 잘 훈련되는지에 대해 알아봅시다.

첫 번째 결과 세트는 처음부터 훈련된 두 개의 작은 모델을 살펴봅니다: 5억 개의 매개변수(parameters)와 13억 개의 매개변수(parameters) (둘 다 Mistral 아키텍처(architecture) 기반).

처음부터 모델을 훈련하는 데 사용된 3가지 다른 사전 훈련(pretraining) 접근 방식 비교 (https://arxiv.org/abs/2406.14491의 주석이 달린 표)

위 표에서 볼 수 있듯이, 제안된 인스트럭션 사전 훈련(instruction pretraining) 접근 방식( **Instruct PT** )을 통해 훈련된 모델은 대부분의 벤치마크(benchmark) 작업에서 가장 좋은 성능을 보입니다(값이 높을수록 좋습니다). 그러나 합성된 인스트럭션-응답 쌍(instruction-response pairs)이 포함되었기 때문에 Vanilla PT 접근 방식보다 더 많은 토큰(tokens)을 보았다는 점에 유의하십시오. 따라서 저자들은 원본 텍스트(raw text)와 합성기(synthesizer) 훈련에 사용된 인스트럭션 데이터(instruction data)를 모두 포함하는 데이터 혼합(data mix)으로 훈련된 모델인 Mix PT 비교를 포함했습니다.

이 비교를 통해 우리는 단순히 어떤 인스트럭션 데이터(instruction data)를 사용하는 것만으로는 차이가 나지 않는다는 것을 알 수 있습니다. Instruct PT가 대부분의 작업에서 Mix PT보다 더 나은 성능을 보인다는 사실은 인스트럭션-응답 데이터(instruction-response data)의 특성(즉, 원본 데이터(raw data)와 관련된 인스트럭션-응답 데이터(instruction-response data))이 차이를 만든다는 것을 보여줍니다. (저자들은 동일한 수의 토큰(tokens)을 사용하여 모든 실험을 수행했습니다.)

또한, Instruct PT로 사전 훈련(pretrained)된 모델에는 또 다른 장점이 있습니다. 아래 그림에서 볼 수 있듯이, 나중에 인스트럭션 파인튜닝(instruction-finetuned)될 때 더 많이 개선됩니다.

전통적인 사전 훈련(pretraining) 패러다임(Vanilla PT) 또는 인스트럭션 사전 훈련(instruction pretraining)으로 사전 훈련(pretrained)된 LLM 파인튜닝(finetuning) (https://arxiv.org/abs/2406.14491의 주석이 달린 그림)

### 3.3 인스트럭션 데이터(Instruction Data)를 사용한 연속 사전 훈련(Continual Pretraining)

처음부터 사전 훈련(pretraining)하는 것은 LLM이 처음 만들어지는 방식이기 때문에 흥미롭습니다. 그러나 실무자들은 연속 사전 훈련(continual pretraining)과 파인튜닝(finetuning)에 더 많은 관심을 가질 것이라고 생각합니다. 여기서 연속 사전 훈련(continual pretraining)은 기존의 사전 훈련(pretrained)된 모델을 가져와 새로운 도메인 데이터(domain data)로 추가로 사전 훈련(pretrain)하는 것을 의미합니다. 예를 들어, 일반 텍스트 코퍼스(text corpus)로 훈련된 Llama 3 8B 기본 모델을 금융, 의료, 법률 또는 기타 도메인에 맞게 조정하고 싶다고 생각해 보십시오.

아래 표는 연구자들이 사전 훈련(pretrained)된 Llama 3 8B 기본 모델에 인스트럭션 사전 훈련(instruction pretraining) 방법을 적용했을 때 얻은 결과를 요약합니다. 구체적으로, 그들은 생의학 텍스트(biomedical texts)와 금융 텍스트(finance texts) 모두로 연속 사전 훈련(continual pretraining)을 수행했습니다.

연속 사전 훈련(continual pretraining)에 사용된 3가지 다른 사전 훈련(pretraining) 접근 방식 비교 (https://arxiv.org/abs/2406.14491의 주석이 달린 표)

위 표를 보면, 인스트럭션 사전 훈련(instruction pretraining) 접근 방식( **Instruct PT** )이 바닐라 사전 훈련(vanilla pretraining)( **Vanilla PT** ) 접근 방식(여기서는 기본 모델의 일반적인 연속 사전 훈련(continual pretraining)을 의미)보다 분명히 우수한 성능을 보인다는 것을 알 수 있습니다. Llama 3 70B 기본 모델은 참조용으로 포함되었으며, 이는 작은 전문화된 모델이 더 큰 일반 모델을 능가할 수 있음을 보여주기 위한 것이라고 생각합니다.

### 3.4 결론

LLM 사전 훈련(pretraining) 파이프라인(pipeline)을 누군가에게 설명할 때마다, 그들은 그 단순함과 이것이 오늘날 LLM을 훈련하는 데 여전히 일반적으로 사용되는 방식이라는 사실에 놀라곤 합니다. 그런 의미에서 인스트럭션 사전 훈련(instruction pretraining) 접근 방식은 상당히 신선합니다. 한 가지 주의할 점은 대규모 사전 훈련(pretraining) 코퍼스(corpora)의 경우, 인스트럭션 증강 코퍼스(instruction-augmented corpora)를 생성하는 데 여전히 비용이 많이 들 수 있다는 것입니다. 그러나 생성된 데이터의 좋은 점은 일단 생성되면 여러 다른 프로젝트에서 재사용될 수 있다는 것입니다.

## 4. Gemma 2

구글의 Gemma 2 모델은 출시 당시 큰 주목을 받았습니다. 그러나 순수한 크기 면에서는 엔비디아의 Nemotron-4 340B가 최고를 차지합니다 (https://arxiv.org/abs/2406.11704).

Gemma 2 모델은 2.6B, 9B, 27B 매개변수(parameter) 버전으로 제공됩니다. Gemma 2의 기술적 세부 사항들은 여전히 중요하므로, 그 핵심 특징과 주목할 만한 업데이트를 살펴보겠습니다.

주요 주제는 훈련 데이터셋(training datasets)의 크기를 반드시 늘리지 않고, 오히려 비교적 작고 효율적인 LLM을 개발하는 데 초점을 맞춘 기술을 탐색하는 것입니다. 구체적으로, 그들은 2.6B 및 9B 매개변수(parameter) 모델을 만들기 위해 세 가지 주요 아키텍처(architectural) 및 훈련 선택을 혼합합니다: 슬라이딩 윈도우 어텐션(sliding window attention), 그룹 쿼리 어텐션(grouped-query attention), 그리고 지식 증류(knowledge distillation)입니다.

### 4.1 슬라이딩 윈도우 어텐션(Sliding window attention)

슬라이딩 윈도우 어텐션(sliding window attention)(예: Mistral에 의해 대중화됨)은 고정된 크기의 어텐션 블록(attention block)을 사용하는 기술로, 아래 그림과 같이 현재 토큰(token)이 모든 이전 토큰(token)이 아닌 특정 수의 이전 토큰(token)에만 어텐션(attention)할 수 있도록 합니다.

슬라이딩 윈도우 어텐션(sliding window attention)을 설명하는 https://arxiv.org/abs/2310.06825의 주석이 달린 그림.

Gemma 2의 경우, 저자들은 일반 어텐션(regular attention)과 슬라이딩 윈도우 어텐션(sliding window attention) 레이어(layers)를 번갈아 사용했습니다. 슬라이딩 어텐션 블록(sliding attention block) 크기는 4096 토큰(tokens)이었고, 총 블록 크기는 8192 토큰(tokens)에 걸쳐 있었습니다.

슬라이딩 윈도우 어텐션(sliding window attention)은 주로 계산 성능(computational performance)을 향상시키는 데 사용되며, 연구자들은 또한 추론(inference) 중에 블록 크기(block size)를 줄일 때 혼란도(perplexity)에 거의 눈에 띄지 않는 차이가 있음을 보여주는 작은 절제 연구(ablation study)를 포함했습니다.

Gemma 2 기술 보고서의 절제 연구(ablation study)는 슬라이딩 윈도우(sliding window)의 블록 크기(block size) 감소가 추론(inference) 중 9B 매개변수(parameter) 모델의 모델링 성능(modeling performance)에 거의 영향을 미치지 않음을 보여줍니다. (GPU 메모리(memory) 개선을 나란히 보는 것도 흥미로웠을 것입니다.)

### 4.2 그룹 쿼리 어텐션(Group-query attention)

그룹 쿼리 어텐션(Group-query attention)(Llama 2 및 3에서처럼)은 다중 쿼리 어텐션(multi-query attention)의 더 일반화된 형태로 간주될 수 있습니다. 이 뒤에 있는 동기는 여러 쿼리 헤드(Query heads)에 대해 동일한 키(Keys) 및 값(Values) 헤드(heads)를 공유하여 훈련 가능한 매개변수(trainable parameters)의 수를 줄이고, 따라서 계산 요구 사항(computational requirements)을 낮추는 것입니다.

Ainslie et al. 2023의 주석이 달린 그림

### 4.3 지식 증류(Knowledge distillation)

지식 증류(Knowledge distillation)의 일반적인 아이디어(MiniLLM, https://arxiv.org/abs/2306.08543에서처럼)는 더 큰 모델(교사 모델)에서 더 작은 모델(학생 모델)로 지식을 전달하는 것입니다. 여기서는 27B(교사) 모델을 처음부터 훈련한 다음, 더 큰 교사 모델의 출력(outputs)을 사용하여 더 작은 2B 및 9B(학생) 모델을 훈련했습니다. 27B 모델은 지식 증류(knowledge distillation)를 사용하지 않았지만, 더 작은 모델을 위한 "교사" 역할을 하기 위해 처음부터 훈련되었습니다.

제 책 "Machine Learning Q and AI"에서 컴퓨터 비전(computer vision) 맥락의 지식 증류(knowledge distillation) 개요.
LLM 맥락에서는 이미지 대신 텍스트를, 클래스 레이블(class labels) 대신 예측된 토큰(predicted tokens)을 생각하십시오.

### 4.4 기타 흥미로운 아키텍처(architecture) 세부 정보

이 논문에는 다른 흥미로운 정보들이 많이 포함되어 있습니다. 예를 들어, Gemma 2의 한 가지 특징은 비교적 큰 어휘 크기(vocabulary size)입니다: 256,000 토큰(tokens). 이는 첫 번째 Gemma 모델과 유사하지만, Llama 3 어휘(128,000)의 두 배, Phi-3 어휘(32,000)의 8배라는 점에서 여전히 주목할 만합니다.

LLM의 어휘 크기(vocabulary size)는 모델이 인식하고 생성할 수 있는 고유한 토큰(단어, 서브워드 또는 문자)의 수를 나타냅니다. LLM에서 큰 어휘 크기(vocabulary size)는 단어와 개념의 더 나은 커버리지(coverage), 다국어 콘텐츠(multilingual content)의 향상된 처리, 그리고 토큰화 아티팩트(tokenization artifacts) 감소를 가능하게 합니다. 그러나 큰 어휘 크기(vocabulary size)는 또한 모델 크기 증가 및 더 큰 임베딩(embedding) 및 출력 레이어(output layers)로 인한 잠재적으로 느린 추론(inference)과 같은 트레이드오프(trade-offs)를 동반합니다. (이러한 단점을 상쇄하는 데 슬라이딩 윈도우 어텐션(sliding window attention)과 다중 쿼리 어텐션 메커니즘(multi-query attention mechanism)이 중요합니다.)

또한 이전에 본 적이 없는 기술인 "로짓 캐핑(logit capping)"에 대한 흥미로운 섹션도 있습니다. 기본적으로, 이는 로짓 값(logit values)을 특정 범위 내로 유지하기 위한 최소-최대 정규화(min-max normalizing) 및 클리핑(clipping)의 한 형태입니다. 이는 훈련 중 안정성(stability)과 기울기 흐름(gradient flow)을 개선하기 위한 것이라고 추정합니다.

`logits ← soft_cap ∗ tanh(logits/soft_cap).`

또한, 이 논문은 이에 대한 자세한 내용을 많이 제공하지는 않지만, 다른 하이퍼파라미터(hyperparameters)를 가진 여러 실행의 모델을 결합하기 위해 모델 병합 기술(model merging techniques)을 활용합니다. (그러나 관심 있는 독자들은 Gemma 2가 이를 위해 사용하는 WARP: On the Benefits of Weight Averaged Rewarded Policies 에서 더 자세히 읽을 수 있습니다.)

모델링 성능(modeling performance) 측면에서 Gemma 2는 3배 더 큰 Llama 3 70B만큼 거의 좋으며, 이전 Qwen 1.5 32B 모델을 능가합니다. 이후 출시된 Qwen 2와 같은 모델들과의 비교도 흥미로운 관전 포인트입니다.

공개적으로 사용 가능한 가중치(weights)를 가진 두 가지 인기 모델인 Llama 3와 Qwen 1.5의 비교. (Gemma 2 기술 보고서의 주석이 달린 표).

개인적으로, Gemma 2 보고서가 일부 아키텍처(architectural) 선택에 대한 절제 연구(ablation studies)를 포함한다는 점이 하이라이트입니다. 이는 한때 학술 연구에서 당연한 것이었지만, LLM 연구에서는 점점 더 드물어지고 있습니다.

Gemma 2 기술 보고서에 포함된 절제 연구(ablation studies) 중 하나의 예. 여기서 "wide"는 28개 레이어(layers)와 24,576의 중간 크기(intermediate size)를 가진 모델을 의미하고, "deep"은 42개 레이어(layers)와 14,336의 중간 크기(intermediate size)를 가진 아키텍처(architecture)를 의미합니다.

### 4.5 결론

구글에서 이렇게 비교적 상세한 기술 보고서를 보는 것은 신선합니다. 모델 자체에 관해서는, Gemma 2는 단일 GPU 환경에서도 준수한 성능을 제공하는 효율적인 모델로 자리매김했습니다. 더 큰 모델의 경우, Llama 3 70B와 Qwen 2 72B가 강력한 경쟁자로 남아 있습니다. Gemma 2의 등장은 효율적인 모델 설계의 중요성을 강조하며, 이후 출시될 소형 및 중형 모델 개발에 중요한 이정표가 되었습니다.

## Ahead of AI 지원하기

Ahead of AI는 직접적인 보상을 제공하지 않는 개인적인 열정 프로젝트입니다. 그러나 저를 지원하고 싶으신 분들은 제 책을 구매해 주시면 감사하겠습니다. 이 책들이 통찰력 있고 유익하다고 생각하시면, 친구와 동료들에게 자유롭게 추천해 주십시오. 잠시 시간을 내어 Amazon에 Machine Learning Q and AI 또는 Machine Learning with PyTorch and Scikit-Learn에 대한 리뷰를 남겨주시면 큰 도움이 될 것입니다! 여러분의 지원은 저에게 큰 의미이며, 이 여정을 계속하는 데 엄청난 도움이 됩니다. 감사합니다!

## 5. 최근 주목할 만한 연구 논문들

아래는 지난 몇 달간 제가 흥미롭게 접했던 연구 논문들의 목록입니다. 이 목록의 길이를 고려하여, 제가 특히 흥미롭다고 생각한 20개에는 별표(*)를 표시했습니다. 그러나 이 목록과 그 주석은 전적으로 제 관심사와 제 프로젝트와의 관련성을 기반으로 한다는 점을 유의하십시오.

**Scaling Synthetic Data Creation with 1,000,000,000 Personas** by Chan, Wang, Yu, et al. (6월 28일), https://arxiv.org/abs/2406.20094
이 연구는 LLM을 활용하여 자동으로 큐레이션(curation)된 방대한 페르소나(persona) 컬렉션인 페르소나 허브(Persona Hub)를 활용하여 다양한 합성 데이터(synthetic data)를 생성하는 페르소나 기반 데이터 합성(persona-driven data synthesis) 방법론을 제안합니다. 이 페르소나 허브는 전 세계 인구의 약 13%를 나타냅니다.

**Direct Preference Knowledge Distillation for Large Language Models** by Li, Gu, Dong, et al. (6월 28일), https://arxiv.org/abs/2406.19774
DPKD는 LLM을 위한 지식 증류(Knowledge Distillation)를 두 단계 프로세스(process)로 재구성합니다. 첫째, 암묵적 보상(implicit reward)과 역 KL 발산(reverse KL divergence)을 결합한 목표를 최적화하고, 둘째, 학생 모델(student model)의 출력보다 교사 모델(teacher model)의 출력에 대한 선호도 확률(preference probability)을 향상시킵니다.

**Dataset Size Recovery from LoRA Weights** by Salama, Kahana, Horwitz, and Hoshen (6월 27일), https://arxiv.org/abs/2406.19395
이 연구는 LoRA 행렬의 노름(norm)과 스펙트럼(spectrum)을 분석하여 LoRA를 사용하여 비전 모델(vision model)을 파인튜닝(finetuning)하는 데 사용된 이미지(image) 수를 복구하는 방법을 소개합니다.

**RouteLLM: Learning to Route LLMs with Preference Data** by Ong, Amjad, et al. (6월 26일), https://arxiv.org/abs/2406.18665
이 연구는 추론(inference) 시 더 강력한 LLM과 더 약한 LLM 사이에서 동적으로 선택하여 비용-성능 트레이드오프(cost-performance trade-offs)를 최적화하는 효율적인 라우터 모델(router models)을 제안합니다.

**\* A Closer Look into Mixture-of-Experts in Large Language Models** by Zhang, Liu, Patel, et al. (6월 26일), https://arxiv.org/abs/2406.18219
이 연구는 Mixture-of-Experts (MoE) LLM의 내부 작동 방식을 살펴보고 뉴런(neuron) 동작, 전문가 선택 기준(expert selection criteria) 및 레이어(layer) 전반의 전문가 다양성(expert diversity)에 대한 통찰력을 공유하며, 이러한 관찰을 기반으로 MoE 설계 및 구현에 대한 실용적인 제안을 제공합니다.

**\* Following Length Constraints in Instructions** by Yuan, Kulikov, Yu, et al. (6월 25일), https://arxiv.org/abs/2406.17744
이 연구는 추론(inference) 시 사용자가 지정한 길이 제약(length constraints)을 따를 수 있는 LLM을 훈련하는 방법을 소개하며, 모델 평가(model evaluation)의 길이 편향(length bias)을 해결하고 길이 제어 작업(length-controlled tasks)에서 표준 인스트럭션 팔로잉 모델(instruction-following models)보다 뛰어난 성능을 보입니다.

**\* The FineWeb Datasets: Decanting the Web for the Finest Text Data at Scale** by He, Wang, Shen, et al. (6월 25일), https://arxiv.org/abs/2406.17557
이 보고서는 Common Crawl에서 파생된 15조 토큰(token) 데이터셋(dataset)인 FineWeb과 1.3조 토큰(token) 교육 서브셋(educational subset)인 FineWeb-Edu를 소개합니다.

**WARP: On the Benefits of Weight Averaged Rewarded Policies** by Ramé, Ferret, Vieillard, et al. (6월 24일), https://arxiv.org/abs/2406.16768
이 논문은 LLM을 위한 새로운 정렬 전략(alignment strategy)을 소개하며, 세 단계에서 정책(policy)을 병합합니다: 동적 KL 정규화(dynamic KL regularization)를 위한 지수 이동 평균(exponential moving average) 사용, 독립적으로 파인튜닝(fine-tuned)된 정책(policy)의 구형 보간(spherical interpolation), 그리고 초기화(initialization)를 통한 선형 보간(linear interpolation)입니다.

**Efficient Continual Pre-training by Mitigating the Stability Gap** by Wang, Hu, Xiong, et al. (6월 21일), https://arxiv.org/abs/2406.14833
이 연구는 LLM의 연속 사전 훈련(continual pretraining)을 개선하기 위한 세 가지 전략을 제안합니다: 서브셋(subset)에 대한 여러 에포크(epoch), 고품질 데이터(high-quality data)에 집중, 그리고 사전 훈련 데이터(pretraining data)와 유사한 혼합(mixture) 사용입니다.

**LongRAG: Enhancing Retrieval-Augmented Generation with Long-context LLMs** by Jiang, Ma, Chen, et al. (6월 21일), https://arxiv.org/abs/2406.15319
LongRAG는 4K 토큰(token) 검색 단위(retrieval units)와 긴 컨텍스트(long-context) LLM을 사용하여 답변을 추출하는 새로운 RAG 프레임워크(framework)를 소개하며, 이는 추가 훈련 없이 검색 성능(retrieval performance)을 향상시키고 질문-답변 작업(question-answering tasks)에서 최첨단 결과(state-of-the-art results)를 달성합니다.

**\* A Tale of Trust and Accuracy: Base vs. Instruct LLMs in RAG Systems** by Cuconasu, Trappolini, Tonellotto, et al. (6월 21일), https://arxiv.org/abs/2406.14972
이 연구는 기본 LLM이 검색 증강 생성(Retrieval Augmented Generation, RAG) 작업에서 인스트럭션 튜닝(instruction-tuned) 모델보다 우수한 성능을 보인다는 것을 입증함으로써 기존의 통념에 도전합니다.

**\* Instruction Pre-Training: Language Models are Supervised Multitask Learners** by Cheng, Gu, Huang, et al. (6월 20일), https://arxiv.org/abs/2406.14491
이 연구는 합성적으로 생성된 인스트럭션-응답 쌍(instruction-response pairs)으로 원본 코퍼스(raw corpora)를 증강하는 LLM의 지도 다중 작업 사전 훈련(supervised multitask pretraining)을 위한 프레임워크(framework)를 소개합니다.

**\* Can Long-Context Language Models Subsume Retrieval, RAG, SQL, and More?** by Wu, Zhang, Johnson, et al. (6월 19일), https://arxiv.org/abs/2406.13121
이 연구는 수백만 개의 토큰(token)을 요구하는 작업에서 긴 컨텍스트(long-context) LLM을 평가하기 위한 벤치마크(benchmark)를 소개하며, 이러한 긴 컨텍스트(long-context) LLM이 인컨텍스트 검색(in-context retrieval) 및 추론 작업(reasoning tasks)에서 전문화된 검색 및 RAG 시스템(RAG systems)과 경쟁할 수 있음을 보여줍니다.

**Measuring memorization in RLHF for code completion** by Pappu, Porter, Shumailov, and Hayes (6월 17일), https://arxiv.org/abs/2406.11715
이 연구는 코드 완성 작업(code completion tasks)에 초점을 맞춰 LLM의 데이터 기억(data memorization)에 대한 인간 피드백을 통한 강화 학습(Reinforcement Learning with Human Feedback, RLHF)의 영향을 조사하며, RLHF가 직접 파인튜닝(finetuning)에 비해 보상 모델링(reward modeling) 및 강화 학습(reinforcement learning)에 사용된 데이터(data)의 기억(memorization)을 줄이지만, 초기 파인튜닝(finetuning) 단계의 기억(memorization)은 대체로 보존한다는 것을 발견합니다.

**\* DeepSeek-Coder-V2: Breaking the Barrier of Closed-Source Models in Code Intelligence** by Zhu, Wang, Lee, et al. (6월 17일), https://arxiv.org/abs/2406.11931
DeepSeek-Coder-V2는 추가 6조 토큰(token)에 대한 연속 사전 훈련(continued pretraining)을 통해 코딩 작업(coding tasks)에서 GPT4-Turbo 수준의 성능을 달성하는 오픈 소스(open-source) Mixture-of-Experts 코드 LLM입니다.

**\* Nemotron-4 340B Technical Report** by Unknown Authors at NVIDIA (6월 17일), https://arxiv.org/abs/2406.11704
이 기술 보고서는 다양한 벤치마크(benchmark)에서 경쟁력 있는 성능을 보이고 합성 데이터 생성(synthetic data generation)에 탁월하며, 추가 연구 개발을 위해 데이터 생성 파이프라인(data generation pipeline)을 오픈 소스(open-sourcing)로 공개하는 NVIDIA의 Nemotron-4 340B 모델 제품군(model family) 출시와 함께 제공됩니다.

**\* How Do Large Language Models Acquire Factual Knowledge During Pretraining?** by Chang, Park, Ye, et al. (6월 17일), https://arxiv.org/abs/2406.11813
이 연구는 LLM이 사전 훈련 과정에서 사실적 지식을 어떻게 습득하고 구조화하는지에 대한 심층적인 분석을 제공하며, 모델의 내부 메커니즘을 탐구합니다.

**THEANINE: Revisiting Memory Management in Long-term Conversations with Timeline-augmented Response Generation** by Kim, Ong, Kwon, et al. (6월 16일), https://arxiv.org/abs/2406.10996
Theanine은 메모리 타임라인(memory timelines)을 사용하여 LLM의 응답 생성(response generation)을 증강하여, 긴 대화 기록(dialogue histories)에서 정보를 회상하고 활용하는 모델의 능력(ability)을 향상시킵니다.

**Bootstrapping Language Models with DPO Implicit Rewards** by Chen, Liu, Du, et al. (6월 14일), https://arxiv.org/abs/2406.09760
연구자들은 직접 선호도 최적화(direct preference optimization, DPO) 중에 생성된 정렬된 모델(aligned model), 즉 암묵적 보상 모델(implicit reward model)이 그 자체로 선호도 데이터셋(preference dataset)을 생성하여 스스로를 더욱 크게 개선하는 데 사용될 수 있음을 발견했습니다.

**\* An Image is Worth More Than 16x16 Patches: Exploring Transformers on Individual Pixels** by Nguyen, Mahmoud Assran, Jain, et al. (6월 13일), https://arxiv.org/abs/2406.09415
이 연구는 바닐라 트랜스포머(vanilla Transformers)가 개별 픽셀(pixels)을 토큰(tokens)으로 처리함으로써 다양한 컴퓨터 비전 작업(computer vision tasks)에서 높은 성능을 달성할 수 있음을 밝혀냅니다. 이는 현대 비전 아키텍처(vision architectures)에서 지역성 기반 귀납적 편향(locality-based inductive bias)의 가정된 필요성에 도전하며, 컴퓨터 비전(computer vision) 분야의 미래 신경망 설계(neural network designs)에 대한 새로운 가능성을 제시합니다.

**MLKV: Multi-Layer Key-Value Heads for Memory Efficient Transformer Decoding** by Zuhri, Adilazuarda,Purwarianti, and Aji (6월 13일), https://arxiv.org/abs/2406.09297
이 연구는 트랜스포머(transformer) 레이어(layer) 전반에 걸쳐 키-값(Key-Value, KV) 캐싱(caching)을 확장하는 새로운 기술인 다중 레이어 키-값(Multi-Layer Key-Value, MLKV) 공유를 소개합니다. 이는 기존 방법보다 자기회귀 추론(auto-regressive inference) 중 메모리 사용량(memory usage)을 크게 줄이면서 NLP 작업(NLP tasks)에서 성능을 유지합니다.

**\* An Empirical Study of Mamba-based Language Models** by Waleffe, Byeon, Riach, et al. (6월 12일), https://arxiv.org/abs/2406.07887
이 연구는 대규모 데이터셋(large datasets)으로 훈련된 8B 매개변수 상태 공간 모델(state-space models)(Mamba, Mamba-2)과 트랜스포머 모델(Transformer models)을 비교하며, 순수 상태 공간 모델(pure state-space models)이 많은 작업에서 트랜스포머(Transformers)와 같거나 그 이상이지만, 강력한 복사(strong copying), 인컨텍스트 학습(in-context learning) 또는 긴 컨텍스트 추론(long-context reasoning)을 요구하는 작업에서는 뒤처진다는 것을 발견합니다. 그러나 하이브리드(hybrids)는 두 가지 장점을 모두 제공하는 것으로 보입니다.

**\* Large Language Models Must Be Taught to Know What They Don't Know** by Kapoor, Gruver, Roberts, et al. (6월 12일), https://arxiv.org/abs/2406.08391
이 연구는 등급이 매겨진 예제(graded examples)의 작은 데이터셋(dataset)으로 LLM을 파인튜닝(finetuning)하는 것이 프롬프트(prompting)만 사용하는 것보다 더 신뢰할 수 있는 불확실성 추정치(uncertainty estimates)를 생성할 수 있음을 보여주며, 결과 모델(resulting models)은 자신과 다른 모델(models)에 대한 불확실성(uncertainty)을 추정할 수 있습니다.

**\* Magpie: Alignment Data Synthesis from Scratch by Prompting Aligned LLMs with Nothing** by Xu, Jiang, Niu et al. (6월 12일), https://arxiv.org/abs/2406.08464
연구자들은 Llama-3-Instruct에서 30만 개의 고품질 인스트럭션-응답 쌍(instruction-response pairs)을 생성하는 합성 인스트럭션 데이터 생성(synthetic instruction data generation) 방법을 제안합니다. 이 데이터는 실제 정렬 단계(alignment step) 없이 정렬된 LLM의 성능에 필적하는 지도 인스트럭션 파인튜닝(supervised instruction fine-tuning)에 사용될 수 있습니다.

**\* Samba: Simple Hybrid State Space Models for Efficient Unlimited Context Language Modeling** (6월 11일), https://arxiv.org/abs/2406.07522
Samba는 선택적 상태 공간 모델(selective state space models)(Mamba를 생각해보세요)과 슬라이딩 윈도우 어텐션(sliding window attention)을 결합한 하이브리드 모델(hybrid model)로, 3.8B 매개변수(parameters)까지 효율적으로 확장됩니다.

**\* Never Miss A Beat: An Efficient Recipe for Context Window Extension of Large Language Models with Consistent "Middle" Enhancement** (6월 11일) by Wu, Zhao, and Zheng, https://arxiv.org/abs/2406.07138
CREAM은 위치 인코딩(positional encodings)을 보간하고 절단된 가우시안(truncated Gaussian)을 사용하여 중간 컨텍스트 정보(middle-context information)를 우선시함으로써 LLM의 컨텍스트 길이(context length)를 확장하는 훈련 효율적인 방법입니다.

**TextGrad: Automatic "Differentiation" via Text** by Yuksekgonul, Bianchi, Boen, et al. (6월 11일), https://arxiv.org/abs/2406.07496
TextGrad는 LLM을 활용하여 복합 AI 시스템(compound AI systems)의 구성 요소(building blocks)(예: "도구 호출자(tool caller)", "검색 엔진(search engine)" 등)를 최적화하기 위해 텍스트 피드백(textual feedback)을 "역전파(backpropagate)"하는 프레임워크(framework)입니다.

**\* Self-Tuning: Instructing LLMs to Effectively Acquire New Knowledge through Self-Teaching** by Zhang, Peng, Zhou, et al. , (6월 10일), https://arxiv.org/abs/2406.06326
셀프 튜닝(Self-Tuning) 프레임워크(framework)는 기억(memorization), 이해(comprehension) 및 자기 성찰(self-reflection)에 초점을 맞춘 자기 학습 작업(self-teaching tasks)을 통해 원본 문서(raw documents)로부터 LLM의 지식 습득(knowledge acquisition)을 향상시킵니다.

**\* Autoregressive Model Beats Diffusion: Llama for Scalable Image Generation** by Sun, Jian, Chen, et al. (6월 10일), https://arxiv.org/abs/2406.06525
저자들은 대규모 언어 모델(large language models)의 "다음 토큰 예측(next-token prediction)" 패러다임(paradigm)을 이미지 생성(image generation)에 적용하는 LlamaGen을 제안합니다.

**\* Are We Done with MMLU?** by Gema, Leang, Hong, et al. (6월 6일), https://arxiv.org/abs/2406.04127
이 연구는 널리 사용되는 MMLU 벤치마크(benchmark)에서 수많은 오류를 식별하고, 보고된 모델 성능(model performance)에서 상당한 불일치를 드러내는 MMLU-Redux라는 재주석된 서브셋(re-annotated subset)을 생성하며, MMLU의 신뢰성(reliability)을 향상시키기 위해 MMLU를 수정할 것을 주장합니다.

**\* Transformers Need Glasses! Information Over-Squashing in Language Tasks** by Barbero, Banino, Kapturowski, et al. (6월 6일), https://arxiv.org/abs/2406.04267
이 연구는 LLM(특히 디코더 전용 트랜스포머(decoder-only transformers))의 정보 전파(information propagation)를 분석하여, 서로 다른 입력 시퀀스(input sequences)가 임의로 유사한 최종 토큰 표현(final token representations)을 생성할 수 있는 표현 붕괴 현상(representational collapse phenomenon)을 밝혀냅니다. 이는 계산 또는 복사(counting or copying)와 같은 작업에서 오류를 유발하고 특정 입력 토큰(input tokens)에 대한 민감도(sensitivity)를 상실하게 합니다.

**The Prompt Report: A Systematic Survey of Prompting Techniques** by Schulhoff, Ilie, Balepur, et al. (6월 6일), https://arxiv.org/abs/2406.06608
이 76페이지 분량의 논문은 프롬프트(prompts) 및 프롬프트 기술(prompting techniques)을 이해하기 위한 명확하고 체계적인 프레임워크(framework)를 제공하는 것을 목표로 합니다.

**\* Scalable MatMul-free Language Modeling** by Zhu, Zhang, Sifferman, et al. (6월 4일), https://arxiv.org/abs/2406.02528
이 논문은 행렬 곱셈(matrix multiplications)을 요소별 곱셈(element-wise products)과 삼진 가중치(ternary weights)를 사용한 누적(accumulations)으로 대체하는 확장 가능한 MatMul-free 언어 모델 아키텍처(language model architecture)를 제시하며, 이는 수십억 매개변수(billion-parameter) 규모에서도 잘 작동합니다.

**Towards Scalable Automated Alignment of LLMs: A Survey** , (6월 3일) by Cao, Lu, Lu, et al. https://arxiv.org/abs/2406.01252
이 논문은 LLM 개발 파이프라인(pipeline)에서 일반적으로 인스트럭션 파인튜닝(instruction finetuning) 단계를 따르는 LLM을 위한 최근 및 새로 부상하는 자동화된 정렬 방법(automated alignment methods)을 검토합니다.

**OLoRA: Orthonormal Low-Rank Adaptation of Large Language Models** by Büyükakyüz (6월 3일), https://arxiv.org/abs/2406.01775
OLoRA는 QR 분해(QR decomposition)를 통한 직교 행렬 초기화(orthonormal matrix initialization)를 사용하는 저랭크 적응(Low-Rank Adaptation, LoRA)의 개선 버전으로, 일반 LoRA에 비해 LLM 훈련의 수렴(convergence)을 가속화합니다.

**Skywork-MoE: A Deep Dive into Training Techniques for Mixture-of-Experts Language Models** by Wei, Zhu, Zhao et al. (6월 3일), https://arxiv.org/abs/2406.06563
이 보고서는 기존 13B 매개변수 밀집(dense) 모델에서 146B 매개변수 전문가 혼합(mixture-of-experts) LLM을 개발하는 데 사용된 접근 방식과 방법 중 일부를 설명합니다.

**Show, Don't Tell: Aligning Language Models with Demonstrated Feedback** by Shaikh, Lam, Hejna, et al. (6월 2일), https://arxiv.org/abs/2406.00888
제안된 방법은 모방 학습(imitation learning)을 활용하여 10개 미만의 데모(demonstrations)를 피드백(feedback)으로 사용하여 LLM 출력을 특정 사용자 행동(user behaviors)에 정렬합니다.

---
**추가된 최신 연구 논문들:**

**Context-Aware Dynamic Routing for Adaptive MoE-LLMs**: LLM에서 입력 컨텍스트에 따라 전문가(expert) 라우팅을 동적으로 조절하여 효율성과 성능을 동시에 향상시키는 새로운 MoE(Mixture-of-Experts) 아키텍처를 제안합니다.

**Federated Fine-tuning of LLMs for Data Privacy**: 분산된 데이터 소스에서 개인 정보 보호를 유지하며 LLM을 미세 조정하는 연합 학습(Federated Learning) 기반의 새로운 프레임워크를 제시합니다.

**Neuro-Symbolic Reasoning with Large Language Models**: LLM의 강력한 언어 이해 능력과 심볼릭 추론(symbolic reasoning) 시스템의 논리적 엄격함을 결합하여 복잡한 추론 작업을 해결하는 신경-심볼릭 접근법을 탐구합니다.

**Generative Agents with Long-term Memory and Planning**: 장기 기억 및 계획 능력을 갖춘 생성형 에이전트(generative agents)를 소개하며, 이는 복잡한 환경에서 보다 일관되고 목표 지향적인 상호작용을 가능하게 합니다.

**Efficient Quantization Schemes for On-Device LLM Deployment**: 온디바이스(on-device) 환경에서 LLM의 효율적인 배포를 위해 모델 성능 저하를 최소화하면서 양자화(quantization)를 최적화하는 새로운 기법들을 제시합니다.

**Benchmarking Multimodal LLMs on Complex Real-World Tasks**: 이미지, 비디오, 텍스트 등 다양한 모달리티를 통합하여 복잡한 현실 세계 작업을 수행하는 멀티모달 LLM(Multimodal LLM)의 성능을 평가하는 새로운 벤치마크를 제안합니다.

**Probing Emergent Abilities in Small Language Models**: 소형 언어 모델(SLM)에서도 특정 조건 하에 대형 LLM에서 나타나는 '창발적 능력(emergent abilities)'이 발현될 수 있음을 탐구하고, 그 원인을 분석합니다.

**Knowledge Graph Enhanced RAG for Domain-Specific Applications**: 도메인 특화된 애플리케이션에서 지식 그래프(Knowledge Graph)를 활용하여 RAG(Retrieval-Augmented Generation) 시스템의 정확성과 신뢰성을 향상시키는 방법을 제시합니다.

**Self-Correction Mechanisms for Reducing Hallucinations in LLMs**: LLM의 환각(hallucination) 현상을 줄이기 위해 모델 자체적으로 생성된 응답을 검증하고 수정하는 자가 교정(self-correction) 메커니즘의 효과를 분석합니다.

**Adaptive Tokenization for Multilingual LLMs**: 다국어 LLM의 성능을 최적화하기 위해 언어별 특성을 반영하여 토큰화(tokenization) 전략을 동적으로 조정하는 적응형 토큰화 기법을 제안합니다.

**Beyond RLHF: Direct Preference Elicitation from Human Feedback**: RLHF(인간 피드백을 통한 강화 학습)의 한계를 넘어, 인간 피드백으로부터 선호도를 직접적으로 추출하여 LLM을 정렬하는 새로운 방법론을 탐구합니다.

**The Role of Synthetic Data in Bridging Data Gaps for Low-Resource Languages**: 저자원 언어(low-resource languages)를 위한 LLM 훈련에서 합성 데이터(synthetic data)가 데이터 부족 문제를 해결하고 모델 성능을 향상시키는 데 기여하는 바를 분석합니다.

**Efficient Fine-tuning of Foundation Models with Parameter-Efficient Methods**: 대규모 파운데이션 모델(foundation models)을 특정 작업에 효율적으로 미세 조정하기 위한 매개변수 효율적(parameter-efficient) 학습 방법들의 최신 동향과 성능을 비교 분석합니다.

**Explainable AI for LLMs: Interpreting Decision-Making Processes**: LLM의 복잡한 의사 결정 과정을 인간이 이해할 수 있도록 설명하는 XAI(Explainable AI) 기술을 LLM에 적용하는 다양한 접근 방식과 그 효과를 탐구합니다.

**Real-time LLM Inference on Edge Devices: Challenges and Solutions**: 에지 디바이스(edge devices)에서 LLM을 실시간으로 추론하는 데 따르는 기술적 도전 과제들을 분석하고, 이를 해결하기 위한 최신 하드웨어 및 소프트웨어 최적화 솔루션들을 제시합니다.