지난 한 달은 인공지능 분야에서 매우 활발한 시기였습니다. 애플의 온디바이스 LLM(on-device LLM) 통합 발표, 엔비디아의 대규모 네모트론(Nemotron) 모델 공개, FlashAttention-3의 등장, 그리고 구글의 Gemma 2 출시 등 다양한 소식이 연이어 전해졌습니다. 이러한 최신 동향들은 이미 여러 매체를 통해 접하셨을 것으로 생각합니다. 따라서 본 글에서는 거대 언어 모델(LLM) 훈련의 핵심 기술인 인스트럭션 파인튜닝(instruction finetuning)과 관련된 최근 연구 성과에 초점을 맞춰 심도 있게 다루고자 합니다. 특히, 효율성과 접근성을 높이는 혁신적인 방법론들을 조명할 것입니다.

이 글에서 다룰 핵심 내용은 다음과 같습니다:
*   인스트럭션 파인튜닝(instruction finetuning)용 데이터를 생성하는 독창적이고 경제적인 방식
*   기반부터 시작하는 인스트럭션 파인튜닝(instruction finetuning)의 심층 구현
*   인스트럭션 데이터(instruction data)를 활용한 LLM 사전 훈련(pretraining)의 새로운 접근법
*   Gemma 2 모델의 주요 특징 및 개선 사항 분석
*   6월에 발표된 주목할 만한 다른 연구 논문들의 개요와 주요 시사점

흥미로운 정보와 함께 유익한 시간이 되기를 바랍니다!

## 1. 정렬 데이터(Alignment Data)를 처음부터 창출하는 혁신

"The Magpie: Alignment Data Synthesis from Scratch by Prompting Aligned LLMs with Nothing"이라는 제목의 논문은 LLM의 인스트럭션 파인튜닝(instruction finetuning)에 필요한 고품질 데이터셋을 독창적으로 생성하는 방법을 제시합니다. 이 접근법은 최신 이론적 돌파구라기보다는, 실제 적용 가능성이 매우 높은 흥미롭고 실용적인 활용 사례로 주목받고 있습니다. 특히, 데이터 수집 및 구축에 드는 막대한 비용과 시간을 절감할 수 있다는 점에서 큰 의미를 가집니다.

### 1.1 무(無)에서 인스트럭션 데이터셋(Instruction Dataset) 구축하기

이 인스트럭션 데이터 생성(instruction-data-generating) 기법이 기존 방법들과 차별화되는 지점은 바로 완전 자동화가 가능하다는 점과 초기 질문이나 지시사항이 전혀 필요 없다는 것입니다. 논문의 제목이 암시하듯이, 이 방법은 말 그대로 "아무것도 없는(Nothing)" 상태에서 인스트럭션 데이터셋(instruction dataset)을 만들어낼 수 있습니다. 필요한 유일한 요소는 로컬 환경에서 구동되는 Llama 3 8B 모델뿐입니다. 이는 데이터 접근성이 낮은 환경에서도 고품질 데이터를 확보할 수 있는 가능성을 열어줍니다.

아래 도식은 이 방법론의 작동 방식을 간략하게 보여줍니다.

인스트럭션 파인튜닝(instruction finetuning)을 위한 합성 데이터셋을 생성하는 Magpie 방법론의 주석이 달린 그림.
이 그림은 Magpie 논문의 그림을 기반으로 합니다: https://arxiv.org/abs/2406.08464

근본적으로, 위에 제시된 그림에서 보듯이, 우리는 Llama 3 8B Instruct 모델에 사전 정의된 쿼리 템플릿(pre-query template)으로 프롬프트(prompt)를 제공하기만 하면, 해당 모델이 우리를 위한 인스트럭션(instruction)을 생성합니다. 그 후, 생성된 인스트럭션(instruction)을 다시 LLM에 입력하면, 모델은 이에 대한 응답을 만들어냅니다. 이 과정을 수천 번 반복하면, 인스트럭션 파인튜닝(instruction finetuning)에 필요한 데이터셋을 효과적으로 구축할 수 있습니다. (선택적으로, LLM을 활용하여 인스트럭션-응답 쌍(instruction-response pairs)의 품질을 정교하게 필터링하는 과정을 추가할 수 있습니다.)

### 1.2 데이터셋의 품질 평가

놀랍게도, 연구진들은 이처럼 생성된 인스트럭션 데이터셋을 사용하여 Llama 3 8B 기본 모델을 인스트럭션 파인튜닝(instruction finetuning)한 결과, (RLHF 및 DPO를 통한 선호도 파인튜닝(preference finetuning) 과정 없이도) Meta AI의 오리지널 Llama 2 8B Instruct 모델을 능가하는 성능을 달성했음을 확인했습니다. 이는 아래 그림을 통해 시각적으로 확인할 수 있습니다.

Magpie가 생성한 인스트럭션 데이터셋으로 파인튜닝(finetuned)된 Llama 3 8B 기본 모델이 오리지널 Llama 3 8B Instruct 모델을 능가합니다.
Magpie 논문의 주석이 달린 그림을 기반으로 합니다: https://arxiv.org/abs/2406.08464

위 그림에 나타난 Magpie의 인상적인 결과는 단 30만 개의 샘플(samples)만으로 이루어졌습니다. 이는 오리지널 Llama 3 Instruct 모델이 1억 개의 샘플(samples)로 파인튜닝(finetuned)되고 정렬(aligned)되었다는 사실과 비교할 때, Magpie 방법론의 뛰어난 효율성과 잠재력을 극명하게 보여줍니다. 이러한 결과는 소규모 데이터셋으로도 강력한 모델을 구축할 수 있다는 가능성을 제시하며, 데이터 수집의 장벽을 낮추는 데 기여합니다.

### 1.3 로컬 환경에서 데이터셋 생성 실행하기

처음에는 회의적인 시각도 있었지만, 직접 구현해 본 결과 정말로 잘 작동했습니다! Ollama를 활용한 제 재구현(reimplementation)은 여기에서 찾아볼 수 있으며, 이는 MacBook Air와 같은 개인 장치에서도 원활하게 로컬에서 실행됩니다. 이는 고성능 컴퓨팅 자원이 없는 연구자나 개발자에게도 이 방법론이 얼마나 실용적인지를 보여주는 사례입니다.

로컬에서 실행되는 Magpie 방법의 재구현(reimplementation) 코드 스크린샷.
코드는 여기에서 확인할 수 있습니다.

### 1.4 추가 세부 정보 및 심층 분석

저자들은 두 가지 유형의 데이터셋을 구축했습니다: Llama 3 70B Instruct 모델을 활용한 "Pro" 버전과 Llama 3 8B Instruct 모델을 사용한 "Air" 버전입니다. 이 구분은 기반 모델의 크기에 따라 데이터의 복잡성과 품질에 차이를 두려는 시도로 보입니다.

이전 그림에서 확인했듯이, Magpie-Pro를 통해 생성된 데이터셋은 Llama 3 8B 기본 모델을 인스트럭션 파인튜닝(instruction-finetune)하는 데 사용될 때 Magpie-Air 데이터셋에 비해 약간 더 우수한 성능의 모델을 만들어냅니다. 이는 더 크고 강력한 교사 모델(teacher model)이 더 정교하고 효과적인 인스트럭션을 생성할 수 있음을 시사합니다.

아래 그림은 LLM을 통해 평가된 각 데이터셋의 품질 및 난이도에 대한 추가적인 비교를 보여줍니다.

Magpie 논문의 주석이 달린 플롯(plots)으로, Air 및 Pro 데이터셋의 상대적인 데이터셋 품질과 난이도를 보여줍니다.

위 그림에서 알 수 있듯이, Air 및 Pro 데이터셋의 품질은 대체로 유사합니다. 또한, Alpaca 데이터셋이 이들과 어떻게 비교되는지 살펴보는 것도 흥미로운 분석이 될 수 있었을 것입니다. (Magpie 데이터가 Alpaca보다 훨씬 고품질이라는 가정이 있지만, 명확한 참조 지점(reference point)이 있었다면 더욱 설득력 있었을 것입니다.)

더 나아가, 본 논문은 이 데이터셋의 폭넓은 다양성(breadth or diversity)이 Alpaca, Evol Instruct, UltraChat 등 다른 인기 있는 인스트럭션 파인튜닝(instruction finetuning) 데이터셋보다 훨씬 크다는 분석 결과를 포함하고 있습니다. 이는 Magpie 데이터가 더 일반적이고 다양한 시나리오에 적용될 수 있는 잠재력을 가졌음을 의미합니다.

또한, 다른 인스트럭션 파인튜닝(instruction finetuning) 데이터셋으로 훈련된 모델들과 비교했을 때, Magpie-Pro로 파인튜닝(finetuned)된 모델 역시 매우 유리한 성능을 보여줍니다. 이는 합성 데이터만으로도 실제 환경에서 경쟁력 있는 LLM을 구축할 수 있다는 강력한 증거입니다.

### 1.5 결론: Magpie의 중요성

전반적으로, Magpie 방법론은 그 효과성 측면에서 매우 매력적이며, 동시에 다양한 실용적 유용성을 지닌 흥미로운 활용 사례라고 평가할 수 있습니다. 저는 앞으로 범용 인스트럭션 데이터셋(general-purpose instruction datasets)을 구축하는 데 있어 이 방법이 매우 흥미롭고, 간결하며, 비용 효율적인 대안으로 확실히 고려될 것이라고 생각합니다. 이는 LLM 개발의 진입 장벽을 낮추고, 더 많은 연구자와 개발자가 혁신적인 모델을 만들 수 있도록 지원하는 중요한 발걸음이 될 것입니다.

## 2. 처음부터 시작하는 인스트럭션 파인튜닝(Instruction Finetuning)

거대 언어 모델(LLM)의 인스트럭션 파인튜닝(instruction finetuning) 과정에 대한 깊이 있는 이해를 돕는 자료를 찾고 계셨다면, 마침내 Manning 웹사이트에 LLM 인스트럭션 파인튜닝(instruction finetuning)에 관한 제 7장이 공개되었다는 소식을 기쁜 마음으로 전해드립니다. 이 장은 단순한 이론적 설명에 그치지 않고, 실제 구현에 필요한 모든 단계를 상세히 안내합니다.

이 장은 책 전체에서 가장 방대한 내용을 담고 있으며, 인스트럭션 파인튜닝(instruction finetuning) 파이프라인(pipeline)을 완전히 처음부터 구축하는 접근 방식을 취합니다. 여기에는 입력 데이터 형식화(input formatting)부터 시작하여, 사용자 정의 콜레이트 함수(custom collate function)를 활용한 효율적인 배치 처리(batching), 불필요한 패딩 토큰(padding tokens)을 마스킹(masking)하는 기법, 실제 훈련 루프(training loop)의 설계, 그리고 마지막으로 사용자 정의 테스트 세트(custom test set)를 통해 파인튜닝(finetuned)된 LLM의 응답 품질을 객관적으로 채점하는 방법까지, 모든 세부 사항이 포함되어 있습니다. 독자들이 실질적인 문제 해결 능력을 기를 수 있도록, 연습 문제에는 프롬프트 스타일(prompt styles) 변경, 인스트럭션 마스킹(instruction masking)의 고급 기법, 그리고 LoRA(Low-Rank Adaptation) 추가와 같은 중요한 주제들이 다루어집니다. 이 장은 독자들이 LLM 파인튜닝의 복잡성을 완전히 이해하고 직접 구현할 수 있도록 돕는 실용적인 가이드가 될 것입니다.

즐거운 코딩 되시길 바랍니다!

제 책 "Build a Large Language Model From Scratch"의 7장 개요.
보충 코드 자료는 GitHub에서 여기에서 확인할 수 있습니다.

추신: 이 장은 또한 책의 마지막 장이며, 출판사는 현재 인쇄 버전의 레이아웃(layouts) 작업을 진행 중입니다.

## 3. 인스트럭션 사전 훈련(Instruction Pretraining)을 통한 LLM 성능 향상

"Instruction Pre-Training: Language Models are Supervised Multitask Learners" (https://arxiv.org/abs/2406.14491)라는 제목의 논문에서 연구자들은 거대 언어 모델(LLM)의 사전 훈련(pretraining) 과정을 더욱 효율적으로 만들 수 있는지를 탐구합니다. 그들은 기존의 원본 텍스트(raw text) 대신 합성된 인스트럭션-응답 쌍(synthetic instruction-response pairs)을 훈련 데이터에 포함시키는 새로운 접근 방식을 제안합니다. 여기서 "원본 텍스트(raw text)"는 특별한 형식으로 가공되지 않은, 책, 웹사이트, 논문 등에서 추출된 순수한 텍스트를 의미합니다. 이 연구는 모델이 단순히 다음 토큰을 예측하는 것을 넘어, 처음부터 지시를 이해하고 따르는 능력을 내재화하도록 돕는 중요한 전환점을 제시합니다.

일반 사전 훈련(regular pretraining)(상단)과 제안된 인스트럭션 사전 훈련(instruction pretraining) 접근 방식(하단)의 비교 (https://arxiv.org/abs/2406.14491의 주석이 달린 그림을 통해)

구체적으로, 연구진은 훈련 코퍼스(corpus) 자체에서 인스트럭션-응답 데이터(instruction-response data)를 생성하기 위해 특별히 파인튜닝(finetuned)된 LLM인 "인스트럭션 합성기(instruction synthesizer)"를 활용하는 실험을 수행합니다. (원본 텍스트(raw text)를 인스트럭션 데이터(instruction data) 형태로 변환하는 것을 제안한 첫 번째 논문은 아닙니다. 예를 들어, "Genie: Achieving Human Parity in Content-Grounded Datasets Generation" (https://arxiv.org/abs/2401.14367)과 같은 연구도 유사한 접근을 시도했습니다. 몇 달 전에도 사전 훈련(pretraining) 과정에서 인스트럭션 데이터(instruction data)를 활용하는 또 다른 논문이나 블로그 게시물을 본 기억이 있지만, 아쉽게도 그 참조를 찾을 수 없었습니다. 그럼에도 불구하고, 여기서 논의되는 논문은 로컬 환경에서 실행 가능한 공개 LLM을 기반으로 하며, 사전 훈련(pretraining)과 연속 사전 훈련(continual pretraining)이라는 두 가지 중요한 시나리오를 모두 다루고 있어 특히 주목할 만합니다.)

### 3.1 인스트럭션 합성기(Instruction Synthesizer)의 역할

사전 훈련(pretraining) 및 연속 사전 훈련(continual pretraining)의 결과에 대해 더 자세히 알아보기 전에, 이 방법론의 핵심 구성 요소인 인스트럭션 합성기(instruction synthesizer)에 대해 먼저 논의해 보겠습니다. 이 합성기는 원본 텍스트를 지시 기반의 학습 자료로 변환하는 데 결정적인 역할을 합니다.

이 합성기는 공개적으로 사용 가능한 Mistral 7B v0.1 LLM(작년에 제가 여기에서 다뤘던 내용: https://magazine.sebastianraschka.com/i/138555764/mistral-b)을 기반으로 하며, 원본 텍스트(raw text)로부터 인스트럭션-응답 쌍(instruction-response pairs)을 생성하도록 정교하게 파인튜닝(finetuned)되었습니다. 이 합성기(synthesizer)를 파인튜닝(finetune)하기 위해 연구자들은 HotpotQA (https://arxiv.org/abs/1809.09600)와 같이 위키피디아(Wikipedia)의 구절에서 질문과 답변이 연결된 데이터셋을 활용했습니다. 이를 통해 저자들은 상식 추론(commonsense reasoning), 감성 분석(sentiment analysis), 수학 문제(math problems) 등 다양한 작업 유형이 포괄되도록 신경 썼습니다. 이는 합성기가 광범위한 인스트럭션을 생성할 수 있도록 보장합니다.

인스트럭션 합성기(instruction synthesizer)의 입력 및 출력 데이터 (https://arxiv.org/abs/2406.14491의 주석이 달린 그림을 통해)

이렇게 인스트럭션 합성기(instruction synthesizer)가 개발(즉, 파인튜닝)되면, 이는 대상 LLM을 사전 훈련(pretraining)하기 위한 입력 데이터(input data)를 생성하는 데 활용될 수 있습니다. 인스트럭션 합성기(instruction synthesizer)에 대한 마지막으로 주목할 만한 세부 사항은 아래 그림에 나타난 바와 같이, 여러 원본 텍스트(T n )와 인스트럭션-응답 쌍(I n ⊕ R n )이 퓨샷 예제(few-shot examples)로 연결되어 사용된다는 점입니다. 이는 합성기가 생성하는 데이터의 품질과 다양성을 높이는 데 기여합니다.

인스트럭션 합성기(instruction synthesizer)를 파인튜닝(finetuning)하고 사용하는 인스트럭션 데이터(instruction-data) 형식 (https://arxiv.org/abs/2406.14491의 주석이 달린 그림을 통해)

### 3.2 인스트럭션 데이터(Instruction Data)를 활용한 사전 훈련(Pretraining)의 효과

이제 인스트럭션-응답 쌍(instruction-response pairs)을 생성하는 방법에 대해 논의했으므로, 가장 흥미로운 부분인 이렇게 증강된 데이터셋(augmented dataset)에서 모델이 얼마나 효과적으로 훈련되는지 살펴보겠습니다. 이 접근 방식은 LLM이 단순히 텍스트를 예측하는 것을 넘어, 실제 지시를 이해하고 따르는 능력을 내재화하도록 돕는 데 초점을 맞춥니다.

첫 번째 결과 세트는 처음부터 훈련된 두 개의 소규모 모델(5억 개 및 13억 개 매개변수(parameters), 모두 Mistral 아키텍처(architecture) 기반)을 비교 분석합니다.

처음부터 모델을 훈련하는 데 사용된 3가지 다른 사전 훈련(pretraining) 접근 방식 비교 (https://arxiv.org/abs/2406.14491의 주석이 달린 표)

위 표에서 확인할 수 있듯이, 제안된 인스트럭션 사전 훈련(instruction pretraining) 접근 방식( **Instruct PT** )을 통해 훈련된 모델은 대부분의 벤치마크(benchmark) 작업에서 가장 뛰어난 성능을 보였습니다(값이 높을수록 우수). 그러나 합성된 인스트럭션-응답 쌍(instruction-response pairs)이 포함되었기 때문에 Vanilla PT 접근 방식보다 더 많은 토큰(tokens)을 학습했다는 점을 고려해야 합니다. 따라서 저자들은 원본 텍스트(raw text)와 합성기(synthesizer) 훈련에 사용된 인스트럭션 데이터(instruction data)를 모두 포함하는 데이터 혼합(data mix)으로 훈련된 모델인 Mix PT와의 비교를 추가했습니다.

이 비교를 통해 우리는 단순히 어떤 인스트럭션 데이터(instruction data)를 사용하는 것만으로는 성능 차이가 발생하지 않는다는 것을 알 수 있습니다. Instruct PT가 대부분의 작업에서 Mix PT보다 더 나은 성능을 보인다는 사실은 인스트럭션-응답 데이터(instruction-response data)의 특성(즉, 원본 데이터(raw data)와 관련된 인스트럭션-응답 데이터(instruction-response data))이 모델 성능에 결정적인 영향을 미친다는 것을 보여줍니다. (저자들은 모든 실험을 동일한 수의 토큰(tokens)을 사용하여 수행했습니다.)

또한, Instruct PT로 사전 훈련(pretrained)된 모델에는 또 다른 중요한 장점이 있습니다. 아래 그림에서 볼 수 있듯이, 이러한 모델은 나중에 인스트럭션 파인튜닝(instruction-finetuned)될 때 훨씬 더 큰 성능 개선을 이끌어낼 수 있습니다. 이는 사전 훈련 단계에서부터 지시 이해 능력을 내재화하는 것이 후속 미세 조정 과정의 효율성을 극대화한다는 것을 의미합니다.

전통적인 사전 훈련(pretraining) 패러다임(Vanilla PT) 또는 인스트럭션 사전 훈련(instruction pretraining)으로 사전 훈련(pretrained)된 LLM 파인튜닝(finetuning) (https://arxiv.org/abs/2406.14491의 주석이 달린 그림)

### 3.3 인스트럭션 데이터(Instruction Data)를 활용한 연속 사전 훈련(Continual Pretraining)

처음부터 모델을 사전 훈련(pretraining)하는 것은 LLM이 근본적으로 구축되는 방식이라는 점에서 흥미롭습니다. 그러나 실제 현업 종사자들은 기존에 훈련된 모델을 새로운 도메인 데이터(domain data)로 추가로 사전 훈련(pretrain)하는 '연속 사전 훈련(continual pretraining)'과 파인튜닝(finetuning)에 더 많은 관심을 가질 것입니다. 예를 들어, 일반 텍스트 코퍼스(text corpus)로 훈련된 Llama 3 8B 기본 모델을 금융, 의료, 법률 등 특정 전문 도메인에 맞게 최적화하려는 경우를 상상해 볼 수 있습니다.

아래 표는 연구자들이 사전 훈련(pretrained)된 Llama 3 8B 기본 모델에 인스트럭션 사전 훈련(instruction pretraining) 방법을 적용했을 때 얻은 결과를 요약한 것입니다. 구체적으로, 그들은 생의학 텍스트(biomedical texts)와 금융 텍스트(finance texts)라는 두 가지 다른 도메인 데이터로 연속 사전 훈련(continual pretraining)을 수행했습니다.

연속 사전 훈련(continual pretraining)에 사용된 3가지 다른 사전 훈련(pretraining) 접근 방식 비교 (https://arxiv.org/abs/2406.14491의 주석이 달린 표)

위 표를 분석해보면, 인스트럭션 사전 훈련(instruction pretraining) 접근 방식( **Instruct PT** )이 바닐라 사전 훈련(vanilla pretraining)( **Vanilla PT** ) 접근 방식(여기서는 기본 모델의 일반적인 연속 사전 훈련(continual pretraining)을 의미)보다 명확하게 우수한 성능을 보인다는 것을 알 수 있습니다. Llama 3 70B 기본 모델은 참조용으로 포함되었으며, 이는 소규모의 전문화된 모델도 특정 도메인에서 더 큰 일반 모델을 능가할 수 있음을 보여주기 위한 것으로 해석됩니다. 이는 특정 목적에 맞게 모델을 효율적으로 조정하는 것의 중요성을 강조합니다.

### 3.4 결론: 인스트럭션 사전 훈련의 미래

누군가에게 LLM 사전 훈련(pretraining) 파이프라인(pipeline)을 설명할 때마다, 그들은 그 단순함과 이것이 오늘날 LLM을 훈련하는 데 여전히 일반적으로 활용되는 방식이라는 점에 놀라곤 합니다. 그러한 맥락에서 인스트럭션 사전 훈련(instruction pretraining) 접근 방식은 상당히 신선하고 혁신적입니다. 한 가지 유의할 점은 대규모 사전 훈련(pretraining) 코퍼스(corpora)의 경우, 인스트럭션 증강 코퍼스(instruction-augmented corpora)를 생성하는 데 여전히 상당한 비용이 소요될 수 있다는 것입니다. 그러나 일단 생성된 데이터는 여러 다양한 프로젝트에서 재활용될 수 있다는 장점이 있습니다. 이 기술은 LLM이 단순히 텍스트를 생성하는 것을 넘어, 사용자의 의도를 더 정확하게 파악하고 따르는 '지시 추종(instruction following)' 능력을 처음부터 내재화하도록 돕는 중요한 단계입니다. 이는 미래의 LLM이 더욱 유용하고 신뢰할 수 있는 AI 시스템으로 발전하는 데 필수적인 요소가 될 것입니다.

## 4. Gemma 2: 효율성과 성능의 조화

구글의 새로운 Gemma 2 모델에 대한 언급 없이는 이 글을 완성할 수 없습니다. 이 모델은 지난달 발표된 모델 중 가장 큰 반향을 일으킨 출시작 중 하나로 손꼽힙니다. 순수한 규모 면에서는 엔비디아의 Nemotron-4 340B (https://arxiv.org/abs/2406.11704)가 최고를 기록했지만, Gemma 2는 효율성 측면에서 독보적인 위치를 차지합니다.

Gemma 2 모델은 2.6B, 9B, 27B 매개변수(parameter) 버전으로 제공되어 다양한 컴퓨팅 환경에 맞춰 선택할 수 있습니다. 이미 이 글이 상당히 길어졌고, 여러분이 다른 출처를 통해 Gemma 2에 대해 충분히 접하셨을 것으로 판단되므로, 핵심적인 내용으로 바로 들어가겠습니다. 구글이 새로 선보인 Gemma 2 LLM의 주요 특징과 주목할 만한 업데이트는 무엇일까요?

Gemma 2 개발의 주요 방향은 훈련 데이터셋(training datasets)의 크기를 무작정 늘리는 대신, 비교적 작고 효율적인 LLM을 개발하는 데 집중하는 기술들을 탐구하는 것이었습니다. 구체적으로, 그들은 2.6B 및 9B 매개변수(parameter) 모델을 구축하기 위해 세 가지 핵심적인 아키텍처(architectural) 및 훈련 전략을 결합했습니다: 슬라이딩 윈도우 어텐션(sliding window attention), 그룹 쿼리 어텐션(grouped-query attention), 그리고 지식 증류(knowledge distillation)입니다. 이러한 접근 방식은 성능을 유지하면서도 모델의 자원 효율성을 극대화하는 데 기여합니다.

### 4.1 슬라이딩 윈도우 어텐션(Sliding window attention)

슬라이딩 윈도우 어텐션(sliding window attention)(예: Mistral 모델에서 널리 활용됨)은 고정된 크기의 어텐션 블록(attention block)을 사용하는 기법으로, 아래 그림과 같이 현재 토큰(token)이 모든 이전 토큰(token)이 아닌 특정 개수의 인접한 이전 토큰(token)에만 집중할 수 있도록 합니다. 이는 긴 시퀀스 처리 시 계산 복잡도를 획기적으로 줄여줍니다.

슬라이딩 윈도우 어텐션(sliding window attention)을 설명하는 https://arxiv.org/abs/2310.06825의 주석이 달린 그림.

Gemma 2의 경우, 저자들은 일반 어텐션(regular attention) 레이어(layers)와 슬라이딩 윈도우 어텐션(sliding window attention) 레이어를 교차하여 배치했습니다. 슬라이딩 어텐션 블록(sliding attention block)의 크기는 4096 토큰(tokens)이었으며, 전체 블록 크기는 8192 토큰(tokens)에 걸쳐 적용되었습니다. 이 기술은 주로 계산 성능(computational performance)을 향상시키는 데 사용되며, 연구자들은 또한 추론(inference) 시 블록 크기(block size)를 줄일 때 혼란도(perplexity)에 거의 눈에 띄는 차이가 없음을 보여주는 소규모 절제 연구(ablation study)를 함께 제시했습니다. 이는 효율성을 높이면서도 모델의 핵심 성능을 유지할 수 있음을 의미합니다.

Gemma 2 기술 보고서의 절제 연구(ablation study)는 슬라이딩 윈도우(sliding window)의 블록 크기(block size) 감소가 추론(inference) 중 9B 매개변수(parameter) 모델의 모델링 성능(modeling performance)에 거의 영향을 미치지 않음을 보여줍니다. (GPU 메모리(memory) 개선을 나란히 보는 것도 흥미로웠을 것입니다.)

### 4.2 그룹 쿼리 어텐션(Group-query attention)

그룹 쿼리 어텐션(Group-query attention)(Llama 2 및 3에서처럼)은 다중 쿼리 어텐션(multi-query attention)의 더욱 일반화된 형태로 볼 수 있습니다. 이 기술의 핵심 동기는 여러 쿼리 헤드(Query heads)가 동일한 키(Keys) 및 값(Values) 헤드(heads)를 공유하도록 하여, 훈련 가능한 매개변수(trainable parameters)의 수를 줄이고 결과적으로 계산 요구 사항(computational requirements)을 낮추는 데 있습니다. 이는 특히 추론 시 메모리 사용량과 지연 시간을 줄이는 데 효과적입니다.

Ainslie et al. 2023의 주석이 달린 그림

### 4.3 지식 증류(Knowledge distillation)의 활용

지식 증류(Knowledge distillation)의 일반적인 개념(MiniLLM, https://arxiv.org/abs/2306.08543에서처럼)은 더 큰 모델(교사 모델)이 학습한 지식을 더 작은 모델(학생 모델)로 효과적으로 전달하는 것입니다. Gemma 2의 개발 과정에서는 먼저 27B(교사) 모델을 처음부터 훈련한 다음, 이 더 큰 교사 모델의 출력(outputs)을 활용하여 더 작은 2B 및 9B(학생) 모델을 훈련했습니다. 27B 모델 자체는 지식 증류(knowledge distillation)를 사용하지 않았지만, 작은 모델들을 위한 "교사" 역할을 수행하기 위해 처음부터 철저하게 훈련되었습니다. 이 방법은 작은 모델이 큰 모델의 복잡한 지식과 추론 능력을 효율적으로 흡수할 수 있도록 돕습니다.

제 책 "Machine Learning Q and AI"에서 컴퓨터 비전(computer vision) 맥락의 지식 증류(knowledge distillation) 개요.
LLM 맥락에서는 이미지 대신 텍스트를, 클래스 레이블(class labels) 대신 예측된 토큰(predicted tokens)을 생각하십시오.

### 4.4 기타 흥미로운 아키텍처(architecture) 세부 정보

이 논문에는 이 외에도 여러 흥미로운 정보들이 담겨 있습니다. 예를 들어, Gemma 2의 한 가지 특징은 비교적 큰 어휘 크기(vocabulary size)입니다: 256,000 토큰(tokens). 이는 첫 번째 Gemma 모델과 유사하지만, Llama 3의 어휘(128,000)의 두 배, Phi-3 어휘(32,000)의 8배에 달한다는 점에서 여전히 주목할 만합니다.

LLM의 어휘 크기(vocabulary size)는 모델이 인식하고 생성할 수 있는 고유한 토큰(단어, 서브워드 또는 문자)의 총 개수를 나타냅니다. LLM에서 큰 어휘 크기(vocabulary size)는 단어와 개념의 더 넓은 커버리지(coverage), 다국어 콘텐츠(multilingual content)의 향상된 처리, 그리고 토큰화 아티팩트(tokenization artifacts) 감소를 가능하게 합니다. 그러나 큰 어휘 크기(vocabulary size)는 또한 모델 크기 증가 및 더 큰 임베딩(embedding) 및 출력 레이어(output layers)로 인한 잠재적으로 느린 추론(inference)과 같은 상충 관계(trade-offs)를 동반합니다. (이러한 단점을 상쇄하는 데 슬라이딩 윈도우 어텐션(sliding window attention)과 그룹 쿼리 어텐션 메커니즘(grouped-query attention mechanism)이 중요한 역할을 합니다.)

또한, 이전에 흔히 볼 수 없었던 기술인 "로짓 캐핑(logit capping)"에 대한 흥미로운 섹션도 포함되어 있습니다. 기본적으로, 이는 로짓 값(logit values)을 특정 범위 내로 유지하기 위한 최소-최대 정규화(min-max normalizing) 및 클리핑(clipping)의 한 형태입니다. 이는 훈련 중 모델의 안정성(stability)과 기울기 흐름(gradient flow)을 개선하기 위한 것으로 추정됩니다.

`logits ← soft_cap ∗ tanh(logits/soft_cap).`

게다가, 이 논문은 이에 대한 자세한 내용을 많이 제공하지는 않지만, 다른 하이퍼파라미터(hyperparameters)를 가진 여러 실행의 모델을 결합하기 위해 모델 병합 기술(model merging techniques)을 활용합니다. (그러나 관심 있는 독자들은 Gemma 2가 이를 위해 사용하는 WARP: On the Benefits of Weight Averaged Rewarded Policies (https://arxiv.org/abs/2406.16768)에서 더 자세히 읽을 수 있습니다.)

모델링 성능(modeling performance) 측면에서 Gemma 2는 3배 더 큰 Llama 3 70B 모델만큼 거의 우수하며, 이전 Qwen 1.5 32B 모델을 능가하는 결과를 보여줍니다. 더 최근에 출시된 Qwen 2 모델과의 비교 결과도 함께 제시되었다면 더욱 유익했을 것입니다.

공개적으로 사용 가능한 가중치(weights)를 가진 두 가지 인기 모델인 Llama 3와 Qwen 1.5의 비교. (Gemma 2 기술 보고서의 주석이 달린 표).

개인적으로, Gemma 2 보고서가 일부 아키텍처(architectural) 선택에 대한 절제 연구(ablation studies)를 포함한다는 점이 매우 인상 깊습니다. 이는 한때 학술 연구에서 당연히 기대되던 부분이었지만, LLM 연구에서는 점차 드물어지고 있는 추세입니다. 이러한 상세한 분석은 모델의 설계 결정에 대한 깊이 있는 이해를 돕습니다.

Gemma 2 기술 보고서에 포함된 절제 연구(ablation studies) 중 하나의 예. 여기서 "wide"는 28개 레이어(layers)와 24,576의 중간 크기(intermediate size)를 가진 모델을 의미하고, "deep"은 42개 레이어(layers)와 14,336의 중간 크기(intermediate size)를 가진 아키텍처(architecture)를 의미합니다.

### 4.5 결론: Gemma 2의 전략적 의미

구글에서 이처럼 비교적 상세한 기술 보고서를 발표한 것은 매우 고무적인 일입니다. 모델 자체에 관해서는, 현재의 대중적 평가에 따르면 Gemma 2는 단일 GPU 사용 사례(single-GPU use cases)에 가장 적합하고 유능한 모델 중 하나로 꼽힙니다. 더 큰 규모의 모델을 고려한다면, Llama 3 70B와 Qwen 2 72B가 여전히 강력한 경쟁자로 남아 있습니다. Gemma 2의 출시는 단순히 강력한 모델을 추가하는 것을 넘어, 효율성과 접근성을 강조하며 LLM 개발의 새로운 방향을 제시한다는 점에서 중요한 의미를 가집니다. 이는 제한된 자원으로도 고성능 AI를 구현하려는 개발자들에게 큰 희망을 안겨줄 것입니다.

## Ahead of AI 지원하기

Ahead of AI는 직접적인 금전적 보상이 없는 순수한 열정으로 시작된 개인 프로젝트입니다. 하지만 저의 작업에 가치를 느끼시고 지원해 주시고자 한다면, 제 저서들을 구매해 주시면 큰 힘이 될 것입니다. 이 책들이 통찰력 있고 유익하다고 생각하신다면, 부디 친구나 동료들에게도 자유롭게 추천해 주시길 바랍니다. 또한, 잠시 시간을 내어 Amazon에 "Machine Learning Q and AI" 또는 "Machine Learning with PyTorch and Scikit-Learn"에 대한 리뷰를 남겨주시면 다른 독자들에게도 큰 도움이 될 것입니다! 여러분의 성원은 저에게 매우 소중하며, 이 여정을 계속해 나가는 데 엄청난 동기 부여가 됩니다. 진심으로 감사드립니다!

## 5. 6월의 주목할 만한 연구 논문들

이번 6월에도 인공지능 분야에서는 수많은 흥미로운 연구 결과들이 쏟아져 나왔습니다. 아래는 제가 직접 접한 논문들 중 특히 주목할 만하다고 생각되는 목록입니다. 목록의 길이가 상당하므로, 제가 특히 흥미롭다고 생각했거나 제 프로젝트와 관련성이 높다고 판단한 20개 논문에는 별표(*)를 표시했습니다. 이 목록과 각 논문에 대한 간략한 설명은 전적으로 저의 관심사와 판단에 기반을 두고 있음을 알려드립니다. 이번 달의 연구 동향을 살펴보면, 합성 데이터 생성, LLM의 효율적인 추론 및 정렬, 그리고 멀티모달(multimodal) 능력 강화에 대한 깊은 탐구가 활발히 이루어지고 있음을 알 수 있습니다.

**Scaling Synthetic Data Creation with 1,000,000,000 Personas** by Chan, Wang, Yu, et al. (6월 28일), https://arxiv.org/abs/2406.20094
이 연구는 LLM을 활용하여 자동으로 큐레이션(curation)된 방대한 페르소나(persona) 컬렉션인 페르소나 허브(Persona Hub)를 활용하여 다양한 합성 데이터(synthetic data)를 생성하는 페르소나 기반 데이터 합성(persona-driven data synthesis) 방법론을 제안합니다. 이 페르소나 허브는 전 세계 인구의 약 13%를 나타냅니다.

**LLM Critics Help Catch LLM Bugs** by McAleese, Pokorny, Ceron Uribe, et al. (6월 28일), https://arxiv.org/abs/2407.00215
이 연구는 RLHF를 사용하여 모델이 생성한 코드(code)를 평가하는 데 인간을 돕는 "비평가(critic)" 모델을 개발하고, 코드(code) 오류에 대한 자연어 피드백(natural language feedback)을 작성하도록 LLM을 훈련하며, 다양한 작업에서 버그(bug)를 찾아내는 데 있어 그 효과를 입증합니다.

**Direct Preference Knowledge Distillation for Large Language Models** by Li, Gu, Dong, et al. (6월 28일), https://arxiv.org/abs/2406.19774
DPKD는 LLM을 위한 지식 증류(Knowledge Distillation)를 두 단계 프로세스(process)로 재구성합니다. 첫째, 암묵적 보상(implicit reward)과 역 KL 발산(reverse KL divergence)을 결합한 목표를 최적화하고, 둘째, 학생 모델(student model)의 출력보다 교사 모델(teacher model)의 출력에 대한 선호도 확률(preference probability)을 향상시킵니다.

**Changing Answer Order Can Decrease MMLU Accuracy** by Gupta, Pantoja, Ross, et al. (6월 27일), https://arxiv.org/abs/2406.19470
이 연구는 LLM을 위한 MMLU 벤치마크(benchmark)에서 정확도 측정의 견고성(robustness)을 조사하며, 답변 레이블(label) 내용을 섞는 것이 모델(model) 전반에 걸쳐 정확도(accuracy) 감소를 초래하고, 민감도(sensitivity)가 다양하다는 것을 밝혀냅니다.

**From Artificial Needles to Real Haystacks: Improving Retrieval Capabilities in LLMs by Finetuning on Synthetic Data** by Xiong, Papageorgiou, Lee, and Papailiopoulos (6월 27일), https://arxiv.org/abs/2406.19292
이 연구는 LLM의 긴 컨텍스트(long-context) 정보 검색 및 추론 능력(reasoning capabilities)을 향상시키기 위해 숫자 키-값 검색 작업(numerical key-value retrieval tasks)의 합성 데이터셋(synthetic dataset)을 사용하는 파인튜닝(finetuning) 접근 방식을 제안합니다.

**Dataset Size Recovery from LoRA Weights** by Salama, Kahana, Horwitz, and Hoshen (6월 27일), https://arxiv.org/abs/2406.19395
이 연구는 LoRA 행렬의 노름(norm)과 스펙트럼(spectrum)을 분석하여 LoRA를 사용하여 비전 모델(vision model)을 파인튜닝(finetuning)하는 데 사용된 이미지(image) 수를 복구하는 방법을 소개합니다.

**Step-DPO: Step-wise Preference Optimization for Long-chain Reasoning of LLMs** by Azerbayev, Shao, Lin, et al. (6월 26일), https://arxiv.org/abs/2406.18629
이 논문은 LLM의 수학 문제 해결에서 개별 추론 단계(reasoning steps)를 최적화하는 방법인 Step-DPO를 소개하며, 이를 위해 맞춤형 10K 단계별 선호도 쌍 데이터셋(step-wise preference pair dataset)을 사용합니다.

**RouteLLM: Learning to Route LLMs with Preference Data** by Ong, Amjad, et al. (6월 26일), https://arxiv.org/abs/2406.18665
이 연구는 추론(inference) 시 더 강력한 LLM과 더 약한 LLM 사이에서 동적으로 선택하여 비용-성능 트레이드오프(cost-performance trade-offs)를 최적화하는 효율적인 라우터 모델(router models)을 제안합니다.

**\* A Closer Look into Mixture-of-Experts in Large Language Models** by Zhang, Liu, Patel, et al. (6월 26일), https://arxiv.org/abs/2406.18219
이 연구는 Mixture-of-Experts (MoE) LLM의 내부 작동 방식을 살펴보고 뉴런(neuron) 동작, 전문가 선택 기준(expert selection criteria) 및 레이어(layer) 전반의 전문가 다양성(expert diversity)에 대한 통찰력을 공유하며, 이러한 관찰을 기반으로 MoE 설계 및 구현에 대한 실용적인 제안을 제공합니다.

**\* Following Length Constraints in Instructions** by Yuan, Kulikov, Yu, et al. (6월 25일), https://arxiv.org/abs/2406.17744
이 연구는 추론(inference) 시 사용자가 지정한 길이 제약(length constraints)을 따를 수 있는 LLM을 훈련하는 방법을 소개하며, 모델 평가(model evaluation)의 길이 편향(length bias)을 해결하고 길이 제어 작업(length-controlled tasks)에서 표준 인스트럭션 팔로잉 모델(instruction-following models)보다 뛰어난 성능을 보입니다.

**LongIns: A Challenging Long-context Instruction-based Exam for LLMs** by Shaham, Bai, An, et al. (6월 25일), https://arxiv.org/abs/2406.17588
LongIns는 LLM의 긴 컨텍스트(long-context) 능력(capabilities)을 평가하기 위한 새로운 벤치마크(benchmark)로, 검색 및 추론 능력(reasoning abilities)을 평가하기 위해 세 가지 설정(setting)을 사용합니다.

**\* The FineWeb Datasets: Decanting the Web for the Finest Text Data at Scale** by He, Wang, Shen, et al. (6월 25일), https://arxiv.org/abs/2406.17557
이 보고서는 Common Crawl에서 파생된 15조 토큰(token) 데이터셋(dataset)인 FineWeb과 1.3조 토큰(token) 교육 서브셋(educational subset)인 FineWeb-Edu를 소개합니다.

**Adam-mini: Use Fewer Learning Rates To Gain More** by Zhang, Chen, Li, et al . (6월 24일), https://arxiv.org/abs/2406.16793
Adam-mini는 학습률(learning rate) 자원을 전략적으로 줄이고, 헤시안 구조(Hessian structure)를 기반으로 매개변수(parameter)를 분할하며, 매개변수 블록(parameter blocks)에 최적화된 단일 학습률(learning rate)을 할당하여 AdamW와 유사하거나 더 나은 성능을 달성하면서 메모리(memory)를 45-50% 적게 사용하는 제안된 최적화기(optimizer)입니다.

**WARP: On the Benefits of Weight Averaged Rewarded Policies** by Ramé, Ferret, Vieillard, et al. (6월 24일), https://arxiv.org/abs/2406.16768
이 논문은 LLM을 위한 새로운 정렬 전략(alignment strategy)을 소개하며, 세 단계에서 정책(policy)을 병합합니다: 동적 KL 정규화(dynamic KL regularization)를 위한 지수 이동 평균(exponential moving average) 사용, 독립적으로 파인튜닝(fine-tuned)된 정책(policy)의 구형 보간(spherical interpolation), 그리고 초기화(initialization)를 통한 선형 보간(linear interpolation)입니다.

**Sparser is Faster and Less is More: Efficient Sparse Attention for Long-Range Transformers** by Lou, Jia, Zheng, and Tu (6월 24일), https://arxiv.org/abs/2406.16747
저자들은 자기회귀 트랜스포머(autoregressive Transformers)를 위한 새로운 희소 어텐션 메커니즘(sparse attention mechanism)을 제안하며, 스코어링 네트워크(scoring network)와 미분 가능한 top-k 마스크 연산자(differentiable top-k mask operator)를 사용하여 쿼리(query)당 일정한 수의 KV 쌍을 선택함으로써 선형 시간 복잡도(linear time complexity)와 일정한 메모리 사용량(constant memory footprint)을 달성합니다.

**Efficient Continual Pre-training by Mitigating the Stability Gap** by Wang, Hu, Xiong, et al. (6월 21일), https://arxiv.org/abs/2406.14833
이 연구는 LLM의 연속 사전 훈련(continual pretraining)을 개선하기 위한 세 가지 전략을 제안합니다: 서브셋(subset)에 대한 여러 에포크(epoch), 고품질 데이터(high-quality data)에 집중, 그리고 사전 훈련 데이터(pretraining data)와 유사한 혼합(mixture) 사용입니다.

**MoA: Mixture of Sparse Attention for Automatic Large Language Model Compression** by Fu, Huang, Ning, et al. (6월 21일), https://arxiv.org/abs/2406.14909
Mixture of Attention (MoA)은 LLM에서 다양한 모델 구성 요소(model components) 및 입력 길이(input lengths)에 대한 희소 어텐션 패턴(sparse attention patterns)을 자동으로 최적화하여, 균일 희소 어텐션(uniform sparse attention) 접근 방식보다 컨텍스트 길이(context length), 정확도(accuracy) 및 효율성(efficiency)을 향상시킵니다.

**LongRAG: Enhancing Retrieval-Augmented Generation with Long-context LLMs** by Jiang, Ma, Chen, et al. (6월 21일), https://arxiv.org/abs/2406.15319
LongRAG는 4K 토큰(token) 검색 단위(retrieval units)와 긴 컨텍스트(long-context) LLM을 사용하여 답변을 추출하는 새로운 RAG 프레임워크(framework)를 소개하며, 이는 추가 훈련 없이 검색 성능(retrieval performance)을 향상시키고 질문-답변 작업(question-answering tasks)에서 최첨단 결과(state-of-the-art results)를 달성합니다.

**\* A Tale of Trust and Accuracy: Base vs. Instruct LLMs in RAG Systems** by Cuconasu, Trappolini, Tonellotto, et al. (6월 21일), https://arxiv.org/abs/2406.14972
이 연구는 기본 LLM이 검색 증강 생성(Retrieval Augmented Generation, RAG) 작업에서 인스트럭션 튜닝(instruction-tuned) 모델보다 우수한 성능을 보인다는 것을 입증함으로써 기존의 통념에 도전합니다.

**Can LLMs Learn by Teaching? A Preliminary Study** by Ning, Wang, Li, Lin, et al. (6월 20일), https://arxiv.org/abs/2406.14629
저자들은 LLM에서 "가르치면서 배우기(Learning by Teaching)"를 구현하기 위한 세 가지 방법을 개발하고 테스트합니다. 이는 학생 피드백(student feedback) 관찰, 피드백(feedback)으로부터 학습, 반복 학습(iterative learning)과 같이 다양한 수준에서 인간의 교육 프로세스(teaching processes)를 모방하여 추가적인 인간 생성 데이터(human-produced data)나 더 강력한 모델(model)에 의존하지 않고 모델 성능(model performance)을 향상시킵니다.

**\* Instruction Pre-Training: Language Models are Supervised Multitask Learners** by Cheng, Gu, Huang, et al. (6월 20일), https://arxiv.org/abs/2406.14491
이 연구는 합성적으로 생성된 인스트럭션-응답 쌍(instruction-response pairs)으로 원본 코퍼스(raw corpora)를 증강하는 LLM의 지도 다중 작업 사전 훈련(supervised multitask pretraining)을 위한 프레임워크(framework)를 소개합니다.

**\* Can Long-Context Language Models Subsume Retrieval, RAG, SQL, and More?** by Wu, Zhang, Johnson, et al. (6월 19일), https://arxiv.org/abs/2406.13121
이 연구는 수백만 개의 토큰(token)을 요구하는 작업에서 긴 컨텍스트(long-context) LLM을 평가하기 위한 벤치마크(benchmark)를 소개하며, 이러한 긴 컨텍스트(long-context) LLM이 인컨텍스트 검색(in-context retrieval) 및 추론 작업(reasoning tasks)에서 전문화된 검색 및 RAG 시스템(RAG systems)과 경쟁할 수 있음을 보여줍니다.

**Judging the Judges: Evaluating Alignment and Vulnerabilities in LLMs-as-Judges** by Ye, Turpin, Li, He, et al. (6월 18일), https://arxiv.org/abs/2406.12624
이 논문은 TriviaQA를 벤치마크(benchmark)로 사용하여 LLM-as-a-judge 패러다임(paradigm)을 평가하며, 9개의 심사 모델(judge models)과 9개의 시험 응시 모델(exam taker models)을 인간 주석(human annotations)과 비교합니다. 그 결과, 높은 인간 정렬(human alignment)을 가진 모델이 반드시 시험 응시 모델(exam taker models)을 순위 매기는 데 가장 적합하지는 않다는 것을 밝혀냅니다.

**From RAGs to Rich Parameters: Probing How Language Models Utilize External Knowledge Over Parametric Information for Factual Queries** by Wadhwa, Seetharaman, Aggarwal, et al. (6월 18일), https://arxiv.org/abs/2406.12824
저자들은 LLM에서 검색 증강 생성(Retrieval Augmented Generation, RAG)의 메커니즘(mechanics)을 조사하여, 모델이 질문에 답할 때 매개변수 기억(parametric memory)보다는 검색된 컨텍스트 정보(retrieved context information)에 주로 의존하며, 다양한 모델 계열(model families)에서 지름길 행동(shortcut behavior)을 보인다는 것을 밝혀냅니다.

**Self-MoE: Towards Compositional Large Language Models with Self-Specialized Experts** by Kang, Karlinsky, and Luo, et al. (6월 17일), https://arxiv.org/abs/2406.12034
이 논문은 모놀리식(monolithic) LLM을 MiXSE(MiXture of Self-specialized Experts)라는 모듈식 시스템(modular system)으로 변환하는 방법을 소개하며, 자체 생성된 합성 데이터(self-generated synthetic data)를 사용하여 공유 기본 LLM 및 자체 최적화된 라우팅(routing)을 갖춘 전문화된 전문가 모듈(specialized expert modules)을 생성합니다.

**Measuring memorization in RLHF for code completion** by Pappu, Porter, Shumailov, and Hayes (6월 17일), https://arxiv.org/abs/2406.11715
이 연구는 코드 완성 작업(code completion tasks)에 초점을 맞춰 LLM의 데이터 기억(data memorization)에 대한 인간 피드백을 통한 강화 학습(Reinforcement Learning with Human Feedback, RLHF)의 영향을 조사하며, RLHF가 직접 파인튜닝(finetuning)에 비해 보상 모델링(reward modeling) 및 강화 학습(reinforcement learning)에 사용된 데이터(data)의 기억(memorization)을 줄이지만, 초기 파인튜닝(finetuning) 단계의 기억(memorization)은 대체로 보존한다는 것을 발견합니다.

**HARE: HumAn pRiors, a key to small language model Efficiency** by Zhang, Jin, Ge, et al. (6월 17일), https://arxiv.org/abs/2406.11410
이 연구는 벤치마크(benchmark) 데이터 유출을 피하면서 의미론적 다양성(semantic diversity)과 데이터 품질 일관성(data quality consistency)에 초점을 맞춘 소형 언어 모델(Small Language Models, SLMs)의 데이터 구성에서 인간 사전 지식(human priors)을 활용하는 원칙을 제안합니다.

**Iterative Length-Regularized Direct Preference Optimization: A Case Study on Improving 7B Language Models to GPT-4 Level** by Kim, Lee, Park, et al. (6월 17일), https://arxiv.org/abs/2406.11817
이 연구는 반복적인 길이 정규화 직접 선호도 최적화(iterative length-regularized Direct Preference Optimization, iLR-DPO)를 소개하며, 이는 응답의 장황함(response verbosity)을 제어하면서 인간의 선호도(human preferences)에 대한 LLM 정렬(alignment)을 개선하는 방법입니다.

**Unveiling Encoder-Free Vision-Language Models** by Choi, Yoon, Lee, et al . (6월 17일), https://arxiv.org/abs/2406.11832
이 연구는 통합된 디코더(decoder)에서 시각 및 텍스트 입력(visual and textual inputs)을 직접 처리하는 인코더 없는 비전-언어 모델(encoder-free vision-language model, VLM)을 제시합니다.

**\* DeepSeek-Coder-V2: Breaking the Barrier of Closed-Source Models in Code Intelligence** by Zhu, Wang, Lee, et al. (6월 17일), https://arxiv.org/abs/2406.11931
DeepSeek-Coder-V2는 추가 6조 토큰(token)에 대한 연속 사전 훈련(continued pretraining)을 통해 코딩 작업(coding tasks)에서 GPT4-Turbo 수준의 성능을 달성하는 오픈 소스(open-source) Mixture-of-Experts 코드 LLM입니다.

**Tokenization Falling Short: The Curse of Tokenization** by Nguyen, Kim, Patel, et al. (6월 17일), https://arxiv.org/abs/2406.11687
이 연구는 복잡한 문제 해결(complex problem solving), 토큰 구조 탐색(token structure probing), 그리고 타이포그래피(typographical) 변화에 대한 복원력(resilience)에서 LLM의 성능을 조사함으로써 LLM의 "토큰화의 저주(curse of tokenization)"를 탐구합니다. 그 결과, 모델 크기(model size)를 확장하는 것이 도움이 되지만, LLM은 토큰화로 인한 편향(tokenization-induced biases)에 여전히 취약하다는 것을 밝혀냅니다.

**DataComp-LM: In Search of the Next Generation of Training Sets for Language Models** by Li, Fang, Smyrnis, et al. (6월 17일), https://arxiv.org/abs/2406.11794
저자들은 언어 모델 훈련에서 데이터셋 큐레이션(dataset curation) 전략을 실험하기 위한 표준화된 테스트베드(testbed)를 제공하며, 여기에는 240조 토큰(token) 코퍼스(corpus), 사전 훈련 레시피(pretraining recipes), 그리고 53개의 다운스트림 평가(downstream evaluations)가 포함됩니다.

**\* Nemotron-4 340B Technical Report** by Unknown Authors at NVIDIA (6월 17일), https://arxiv.org/abs/2406.11704
이 기술 보고서는 다양한 벤치마크(benchmark)에서 경쟁력 있는 성능을 보이고 합성 데이터 생성(synthetic data generation)에 탁월하며, 추가 연구 개발을 위해 데이터 생성 파이프라인(data generation pipeline)을 오픈 소스(open-sourcing)로 공개하는 NVIDIA의 Nemotron-4 340B 모델 제품군(model family) 출시와 함께 제공됩니다.

**mDPO: Conditional Preference Optimization for Multimodal Large Language Models** by Wang, Zhou, Huang, et al. (6월 17일), https://arxiv.org/abs/2406.11839
mDPO는 언어 선호도(language preferences)와 함께 이미지 선호도(image preference)를 최적화하고 선택된 응답에 대한 가능성 감소(likelihood decrease)를 방지하기 위해 보상 앵커(reward anchor)를 도입함으로써 다중 모달 DPO(multimodal DPO)의 무조건적 선호도 문제(unconditional preference problem)를 해결합니다.

**\* How Do Large Language Models Acquire Factual Knowledge During Pretraining?** by Chang, Park, Ye, et al. (6월 17일), https://arxiv.org/abs/2406.11813
이 연구는 LLM이 사전 훈련(pretraining) 과정에서 사실적 지식(factual knowledge)을 어떻게 획득하고 저장하는지에 대한 메커니즘을 심층적으로 분석합니다.

**Task Me Anything** by Zhang, Huang, Ma, et al. (6월 17일), https://arxiv.org/abs/2406.11775
Task-Me-Anything은 방대한 이미지(image) 및 비디오(video) 분류 체계(taxonomy)에서 작업 인스턴스(task instances)를 프로그래밍 방식으로 생성하여 다중 모달 언어 모델(multimodal language models)을 위한 맞춤형 벤치마크(benchmark)를 생성하는 벤치마크 생성 엔진(benchmark generation engine)입니다.

**THEANINE: Revisiting Memory Management in Long-term Conversations with Timeline-augmented Response Generation** by Kim, Ong, Kwon, et al. (6월 16일), https://arxiv.org/abs/2406.10996
Theanine은 메모리 타임라인(memory timelines)(과거 사건의 발전과 인과 관계를 보여주는 일련의 기억)을 사용하여 LLM의 응답 생성(response generation)을 증강하여, 긴 대화 기록(dialogue histories)에서 정보를 회상하고 활용하는 모델의 능력(ability)을 향상시킵니다.

**Regularizing Hidden States Enables Learning Generalizable Reward Model for LLMs** by Yang, Ding, Lin, et al. (6월 14일) https://arxiv.org/abs/2406.10216
이 연구는 기본 모델의 언어 모델 헤드(language model head)를 유지하고 텍스트 생성 손실(text-generation losses)을 통합하여 은닉 상태(hidden states)를 정규화(regularizing)함으로써 RLHF에서 보상 모델 일반화(reward model generalization)를 향상시키는 방법을 제안합니다. 동시에 보상 헤드(reward head)를 학습하여 분포 외 작업 성능(out-of-distribution task performance)을 개선하고 보상 과최적화(reward over-optimization)를 완화합니다.

**Be like a Goldfish, Don't Memorize! Mitigating Memorization in Generative LLMs** by Hans, Wen, Jain, et al. (6월 14일) , https://arxiv.org/abs/2406.10209
"금붕어 손실(goldfish loss)" 기술은 훈련 중 손실 계산(loss computation)에서 토큰(token)의 서브셋(subset)을 무작위로 제외하여 LLM의 모델 기억(model memorization)을 줄이고, 모델이 훈련 데이터(training data)에서 완전한 문자 그대로의 시퀀스(verbatim sequences)를 학습하는 것을 방지합니다.

**Bootstrapping Language Models with DPO Implicit Rewards** by Chen, Liu, Du, et al. (6월 14일), https://arxiv.org/abs/2406.09760
연구자들은 직접 선호도 최적화(direct preference optimization, DPO) 중에 생성된 정렬된 모델(aligned model), 즉 암묵적 보상 모델(implicit reward model)이 그 자체로 선호도 데이터셋(preference dataset)을 생성하여 스스로를 더욱 크게 개선하는 데 사용될 수 있음을 발견했습니다.

**FouRA: Fourier Low Rank Adaptation** by Borse, Kadambi, Pandey, et al. (6월 13일), https://arxiv.org/abs/2406.08798
이 연구는 푸리에 도메인(Fourier domain)에서 작동하고 적응형 랭크 선택(adaptive rank selection)을 사용하는 새로운 저랭크 적응(low-rank adaptation, LoRA) 방법인 FouRA를 소개합니다. 이는 LoRA 파인튜닝(fine-tuned)된 텍스트-이미지 확산 모델(text-to-image diffusion models)에서 데이터 복사(data copying) 및 분포 붕괴(distribution collapse) 문제를 해결하면서 이미지 품질(image quality)과 일반화(generalization)를 향상시킵니다.

**\* An Image is Worth More Than 16x16 Patches: Exploring Transformers on Individual Pixels** by Nguyen, Mahmoud Assran, Jain, et al. (6월 13일), https://arxiv.org/abs/2406.09415
이 연구는 바닐라 트랜스포머(vanilla Transformers)가 개별 픽셀(pixels)을 토큰(tokens)으로 처리함으로써 다양한 컴퓨터 비전 작업(computer vision tasks)에서 높은 성능을 달성할 수 있음을 밝혀냅니다. 이는 현대 비전 아키텍처(vision architectures)에서 지역성 기반 귀납적 편향(locality-based inductive bias)의 가정된 필요성에 도전하며, 컴퓨터 비전(computer vision) 분야의 미래 신경망 설계(neural network designs)에 대한 새로운 가능성을 제시합니다.

**MLKV: Multi-Layer Key-Value Heads for Memory Efficient Transformer Decoding** by Zuhri, Adilazuarda,Purwarianti, and Aji (6월 13일), https://arxiv.org/abs/2406.09297
이 연구는 트랜스포머(transformer) 레이어(layer) 전반에 걸쳐 키-값(Key-Value, KV) 캐싱(caching)을 확장하는 새로운 기술인 다중 레이어 키-값(Multi-Layer Key-Value, MLKV) 공유를 소개합니다. 이는 다중 쿼리 어텐션(Multi-Query Attention, MQA) 및 그룹 쿼리 어텐션(Grouped-Query Attention, GQA)과 같은 기존 방법보다 자기회귀 추론(auto-regressive inference) 중 메모리 사용량(memory usage)을 크게 줄이면서 NLP 작업(NLP tasks)에서 성능을 유지합니다.

**Transformers Meet Neural Algorithmic Reasoners** by Bounsi, Ibarz, Dudzik, et al. (6월 13일), https://arxiv.org/abs/2406.09308
TransNAR은 트랜스포머(Transformers)와 그래프 신경망 기반 신경 알고리즘 추론기(graph neural network-based neural algorithmic reasoners, NARs)를 결합한 하이브리드 아키텍처(hybrid architecture)로, 트랜스포머(Transformer)가 NAR의 강력한 계산 능력(computational capabilities)을 활용하면서도 강력한 자연어 이해(natural language understanding)를 유지함으로써 알고리즘 추론 작업(algorithmic reasoning tasks)에서 향상된 성능을 가능하게 합니다.

**Discovering Preference Optimization Algorithms with and for Large Language Models** by Lu, Holt, Fanconi, et al. (6월 12일), https://arxiv.org/abs/2406.08414
제안된 발견된 선호도 최적화(Discovered Preference Optimization) 방법은 LLM을 사용하여 LLM 출력(LLM outputs)을 개선하기 위한 새로운 선호도 최적화 알고리즘(preference optimization algorithms)을 자동으로 발견하고 구현합니다.

**\* An Empirical Study of Mamba-based Language Models** by Waleffe, Byeon, Riach, et al. (6월 12일), https://arxiv.org/abs/2406.07887
이 연구는 대규모 데이터셋(large datasets)으로 훈련된 8B 매개변수 상태 공간 모델(state-space models)(Mamba, Mamba-2)과 트랜스포머 모델(Transformer models)을 비교하며, 순수 상태 공간 모델(pure state-space models)이 많은 작업에서 트랜스포머(Transformers)와 같거나 그 이상이지만, 강력한 복사(strong copying), 인컨텍스트 학습(in-context learning) 또는 긴 컨텍스트 추론(long-context reasoning)을 요구하는 작업에서는 뒤처진다는 것을 발견합니다. 그러나 하이브리드(hybrids)는 두 가지 장점을 모두 제공하는 것으로 보입니다.

**\* Large Language Models Must Be Taught to Know What They Don't Know** by Kapoor, Gruver, Roberts, et al. (6월 12일), https://arxiv.org/abs/2406.08391
이 연구는 등급이 매겨진 예제(graded examples)의 작은 데이터셋(dataset)으로 LLM을 파인튜닝(finetuning)하는 것이 프롬프트(prompting)만 사용하는 것보다 더 신뢰할 수 있는 불확실성 추정치(uncertainty estimates)를 생성할 수 있음을 보여주며, 결과 모델(resulting models)은 자신과 다른 모델(models)에 대한 불확실성(uncertainty)을 추정할 수 있습니다.

**Large Language Model Unlearning via Embedding-Corrupted Prompts** by Liu, Flannigan, and Liu (6월 12일), https://arxiv.org/abs/2406.07933
이 연구는 임베딩 손상 프롬프트(embedding-corrupted prompts)를 소개합니다. 이는 LLM에서 선택적 지식 망각(selective knowledge unlearning)을 위한 방법으로, 프롬프트 분류(prompt classification) 및 임베딩 손상(embedding corruption)을 사용하여 광범위한 모델 크기(model sizes)에서 최소한의 부작용으로 목표화된 망각(targeted forgetting)을 달성합니다.

**What If We Recaption Billions of Web Images with LLaMA-3?** by Li, Tu, Hui, et al. (6월 12일) https://arxiv.org/abs/2406.08478
이 연구는 파인튜닝(finetuned)된 Llama 3 기반 LLaVA-1.5 다중 모달 LLM을 사용하여 DataComp-1B 데이터셋(dataset)에서 13억 개의 이미지(image)를 재캡션(recaption)하는 것이 다양한 작업에서 비전-언어 모델(vision-language models)의 성능을 크게 향상시킨다는 것을 보여줍니다.

**\* Magpie: Alignment Data Synthesis from Scratch by Prompting Aligned LLMs with Nothing** by Xu, Jiang, Niu et al. (6월 12일), https://arxiv.org/abs/2406.08464
연구자들은 Llama-3-Instruct에서 30만 개의 고품질 인스트럭션-응답 쌍(instruction-response pairs)을 생성하는 합성 인스트럭션 데이터 생성(synthetic instruction data generation) 방법을 제안합니다. 이 데이터는 실제 정렬 단계(alignment step) 없이 정렬된 LLM의 성능에 필적하는 지도 인스트럭션 파인튜닝(supervised instruction fine-tuning)에 사용될 수 있습니다.

**\* Samba: Simple Hybrid State Space Models for Efficient Unlimited Context Language Modeling** by (6월 11일), https://arxiv.org/abs/2406.07522
Samba는 선택적 상태 공간 모델(selective state space models)(Mamba를 생각해보세요)과 슬라이딩 윈도우 어텐션(sliding window attention)을 결합한 하이브리드 모델(hybrid model)로, 3.8B 매개변수(parameters)까지 효율적으로 확장됩니다.

**\* Never Miss A Beat: An Efficient Recipe for Context Window Extension of Large Language Models with Consistent "Middle" Enhancement** (6월 11일) by Wu, Zhao, and Zheng, https://arxiv.org/abs/2406.07138
CREAM은 위치 인코딩(positional encodings)을 보간하고 절단된 가우시안(truncated Gaussian)을 사용하여 중간 컨텍스트 정보(middle-context information)를 우선시함으로써 LLM의 컨텍스트 길이(context length)를 확장하는 훈련 효율적인 방법입니다.

**Simple and Effective Masked Diffusion Language Models** by Sahoo, Arriola, Schiff, et al. (6월 11일), https://arxiv.org/abs/2406.07524
이 연구는 마스크된 이산 확산 모델(masked discrete diffusion models)이 효과적인 레시피(recipe)와 단순화된 목표(objective)로 훈련될 때, 언어 모델링(language modeling)에서 자기회귀 방법(autoregressive methods)과의 성능 격차를 크게 좁힐 수 있음을 보여줍니다.

**TextGrad: Automatic "Differentiation" via Text** by Yuksekgonul, Bianchi, Boen, et al. (6월 11일), https://arxiv.org/abs/2406.07496
TextGrad는 LLM을 활용하여 복합 AI 시스템(compound AI systems)의 구성 요소(building blocks)(예: "도구 호출자(tool caller)", "검색 엔진(search engine)" 등)를 최적화하기 위해 텍스트 피드백(textual feedback)을 "역전파(backpropagate)"하는 프레임워크(framework)입니다.

**An Image is Worth 32 Tokens for Reconstruction and Generation** by Yu, Weber, Deng, et al. (6월 11일), https://arxiv.org/abs/2406.07550
저자들은 이미지 생성(image generation)을 위한 트랜스포머 기반 1차원 토크나이저(transformer-based 1-dimensional tokenizer)를 제안하며, 이는 256x256x3 이미지를 단 32개의 이산 토큰(discrete tokens)으로 줄입니다.

**\* Self-Tuning: Instructing LLMs to Effectively Acquire New Knowledge through Self-Teaching** by Zhang, Peng, Zhou, et al. , (6월 10일), https://arxiv.org/abs/2406.06326
셀프 튜닝(Self-Tuning) 프레임워크(framework)는 기억(memorization), 이해(comprehension) 및 자기 성찰(self-reflection)에 초점을 맞춘 자기 학습 작업(self-teaching tasks)을 통해 원본 문서(raw documents)로부터 LLM의 지식 습득(knowledge acquisition)을 향상시킵니다.

**Turbo Sparse: Achieving LLM SOTA Performance with Minimal Activated Parameters** by Song, Xie, Zhang, et al. (6월 10일), https://arxiv.org/abs/2406.05955
이 논문은 LLM의 활성화 희소성(activation sparsity)을 개선하기 위해 dReLU 활성화 함수(activation function)와 최적화된 훈련 데이터 혼합(training data mixture)을 제안합니다.

**Husky: A Unified, Open-Source Language Agent for Multi-Step Reasoning** by Kim, Paranjape, Khot, and Hajishirzi (6월 10일), https://arxiv.org/abs/2406.06469
Husky는 전문가 모델(expert models)과 함께 행동을 생성하고 실행하는 것을 반복함으로써 숫자, 표 형식(tabular) 및 지식 기반 추론(knowledge-based reasoning)을 포함하는 다양한 작업을 처리하기 위해 통합된 행동 공간(unified action space)에서 추론(reason)하는 방법을 학습하는 오픈 소스(open-source) 언어 에이전트(language agent)입니다.

**Margin-aware Preference Optimization for Aligning Diffusion Models Without Reference** by Hong, Paul, Lee, et al. (6월 10일), https://arxiv.org/abs/2406.06424
RLHF 및 DPO와 같은 전통적인 정렬 기술(alignment techniques)의 한계를 해결하기 위해, 저자들은 텍스트-이미지 확산 모델(text-to-image diffusion models)을 위한 마진 인식 선호도 최적화(Margin-Aware Preference Optimization, MaPO)를 제안합니다. 이는 참조 모델(reference model)을 사용하지 않고 선호되는 이미지 세트(preferred image sets)와 비선호되는 이미지 세트(dispreferred image sets) 사이의 가능성 마진(likelihood margin)을 최대화합니다.

**\* Autoregressive Model Beats Diffusion: Llama for Scalable Image Generation** by Sun, Jian, Chen, et al. (6월 10일), https://arxiv.org/abs/2406.06525
저자들은 대규모 언어 모델(large language models)의 "다음 토큰 예측(next-token prediction)" 패러다임(paradigm)을 이미지 생성(image generation)에 적용하는 LlamaGen을 제안합니다.

**Creativity Has Left the Chat: The Price of Debiasing Language Models** by Mohammidi (6월 8일), https://arxiv.org/abs/2406.05587
이 연구는 RLHF와 같은 정렬 기술(alignment techniques)이 LLM의 편향(biases)을 완화하지만, 모델의 창의적 능력(creative capabilities)을 감소시켜 구문적(syntactic) 및 의미론적 다양성(semantic diversity)에 영향을 미칠 수 있음을 밝혀냅니다. 이는 창의적 출력(creative output)을 요구하는 작업에 중요합니다.

**3D-GRAND: A Million-Scale Dataset for 3D-LLMs with Better Grounding and Less Hallucination** by Yang, Chen, Madaan, et al. (6월 7일), https://arxiv.org/abs/2406.05132
이 연구는 40,087개의 가정 장면과 620만 개의 장면-언어 인스트럭션(scene-language instructions)으로 구성된 데이터셋(dataset)인 3D-GRAND를 소개하며, 인스트럭션 튜닝(instruction tuning)과 3D-POPE 벤치마크(benchmark)를 활용하여 3D-LLM의 그라운딩 능력(grounding capabilities)을 향상시키고 환각(hallucinations)을 줄입니다.

**BERTs are Generative In-Context Learners** by Samuel (6월 7일), https://arxiv.org/abs/2406.04823
이 논문은 DeBERTa와 같은 마스크된 언어 모델(masked language models)이 인과적 어텐션 마스크(causal attention mask)의 구조와 유사한 마스크 토큰(mask tokens)으로 입력 토큰(input tokens)의 시퀀스(sequence)를 재구성하는 간단한 추론 기술(inference technique)을 사용하여 인컨텍스트 학습(in-context learning)을 수행할 수 있음을 보여줍니다.

**June 7, Mixture-of-Agents Enhances Large Language Model Capabilities**, https://arxiv.org/abs/2406.04692
이 논문은 다양한 에이전트(agents)를 혼합(mixture)하는 접근 방식이 LLM의 전반적인 능력(capabilities)을 어떻게 향상시킬 수 있는지를 탐구합니다.

**WildBench: Benchmarking LLMs with Challenging Tasks from Real Users in the Wild** by Lin, Deng, Chandu, et al. (6월 7일), https://arxiv.org/abs/2406.04770
저자들은 실제 사용자 쿼리(user queries)를 사용하여 LLM을 벤치마킹(benchmarking)하기 위한 자동화된 평가 프레임워크(evaluation framework)를 소개하며, 1,024개의 작업과 두 가지 고급 지표(advanced metrics)인 WB-Reward 및 WB-Score를 특징으로 합니다. 이들은 작업별 체크리스트(task-specific checklists)와 구조화된 설명(structured explanations)을 사용하여 신뢰할 수 있고 해석 가능한 자동 판단(automatic judgments)을 제공합니다.

**CRAG -- Comprehensive RAG Benchmark** by Yang, Sun, Xin, et al. (6월 7일), https://arxiv.org/abs/2406.04744
이 연구는 웹 및 지식 그래프 검색(Knowledge Graph searches)을 시뮬레이션(simulate)하는 모의 API(mock APIs)를 포함하는 4,409개의 질문-답변 쌍으로 구성된 사실 질문 답변 데이터셋(factual question answering dataset)을 소개하며, 다양하고 동적인 실제 QA 작업(real-world QA tasks)을 반영하도록 설계되었습니다.

**Boosting Large-scale Parallel Training Efficiency with C4: A Communication-Driven Approach** by Dong, Luo, Zhang, et al. (6월 7일), https://arxiv.org/abs/2406.04594
이 연구는 LLM의 병렬 훈련(parallel training)을 위한 통신 중심 솔루션(communication-driven solution)인 C4를 소개합니다. 이는 하드웨어 오류(hardware faults)를 신속하게 식별하고 격리하며, 네트워크 혼잡(network congestion)을 줄이기 위해 트래픽 계획(traffic planning)을 최적화하여 오류로 인한 오버헤드(error-induced overhead)를 최대 30% 줄이고 런타임 성능(runtime performance)을 최대 15% 향상시킬 수 있습니다.

**Step-aware Preference Optimization: Aligning Preference with Denoising Performance at Each Step** by Liang, Yuan, Gu, et al. (6월 6일), https://arxiv.org/abs/2406.04314
이 연구는 텍스트-이미지 확산 모델(text-to-image diffusion models)에서 각 단계의 노이즈 제거 성능(denoising performance)을 독립적으로 평가하고 조정하는 후처리 훈련(post-training) 접근 방식인 단계 인식 선호도 최적화(Step-aware Preference Optimization)를 소개합니다. 이는 이미지 정렬(image alignment) 및 미학(aesthetics)에서 Diffusion-DPO보다 우수하며 20배 빠른 훈련 효율성(training efficiency)을 제공합니다.

**\* Are We Done with MMLU?** by Gema, Leang, Hong, et al. (6월 6일), https://arxiv.org/abs/2406.04127
이 연구는 널리 사용되는 MMLU 벤치마크(benchmark)에서 수많은 오류를 식별하고, 보고된 모델 성능(model performance)에서 상당한 불일치를 드러내는 MMLU-Redux라는 재주석된 서브셋(re-annotated subset)을 생성하며, MMLU의 신뢰성(reliability)을 향상시키기 위해 MMLU를 수정할 것을 주장합니다.

**\* Transformers Need Glasses! Information Over-Squashing in Language Tasks** by Barbero, Banino, Kapturowski, et al. (6월 6일), https://arxiv.org/abs/2406.04267
이 연구는 LLM(특히 디코더 전용 트랜스포머(decoder-only transformers))의 정보 전파(information propagation)를 분석하여, 서로 다른 입력 시퀀스(input sequences)가 임의로 유사한 최종 토큰 표현(final token representations)을 생성할 수 있는 표현 붕괴 현상(representational collapse phenomenon)을 밝혀냅니다. 이는 계산 또는 복사(counting or copying)와 같은 작업에서 오류를 유발하고 특정 입력 토큰(input tokens)에 대한 민감도(sensitivity)를 상실하게 합니다.

**The Prompt Report: A Systematic Survey of Prompting Techniques** by Schulhoff, Ilie, Balepur, et al. (6월 6일), https://arxiv.org/abs/2406.06608
이 76페이지 분량의 논문은 프롬프트(prompts) 및 프롬프트 기술(prompting techniques)을 이해하기 위한 명확하고 체계적인 프레임워크(framework)를 제공하는 것을 목표로 합니다.

**Buffer of Thoughts: Thought-Augmented Reasoning with Large Language Models** by Yang, Yu, Zhang, et al. (6월 6일), https://arxiv.org/abs/2406.04271
이 Buffer of Thoughts 접근 방식은 다양한 도메인(domains)에 걸쳐 추론(reasoning)을 위해 일반적인 문제 해결 청사진(problem-solving blueprints)인 사고 템플릿(thought-templates)을 검색하고 인스턴스화(instantiating)함으로써 LLM을 개선합니다.

**Block Transformer: Global-to-Local Language Modeling for Fast Inference** (6월 4일) by Ho, Bae, Kim, et al. , https://arxiv.org/abs/2406.02657
제안된 블록 트랜스포머(Block Transformer)는 비용이 많이 드는 전역 어텐션(global attention)을 고정 크기 토큰 블록(fixed-size token blocks)의 하위 레이어(lower layers)로 격리하고 상위 레이어(upper layers)에서 빠른 지역 어텐션(local attention)을 적용하여 추론 처리량(inference throughput)을 10-20배 향상시킵니다.

**\* Scalable MatMul-free Language Modeling** by Zhu, Zhang, Sifferman, et al. (6월 4일), https://arxiv.org/abs/2406.02528
이 논문은 행렬 곱셈(matrix multiplications)을 요소별 곱셈(element-wise products)과 삼진 가중치(ternary weights)를 사용한 누적(accumulations)으로 대체하는 확장 가능한 MatMul-free 언어 모델 아키텍처(language model architecture)를 제시하며, 이는 수십억 매개변수(billion-parameter) 규모에서도 잘 작동합니다.

**Towards Scalable Automated Alignment of LLMs: A Survey** , (6월 3일) by Cao, Lu, Lu, et al. https://arxiv.org/abs/2406.01252
이 논문은 LLM 개발 파이프라인(pipeline)에서 일반적으로 인스트럭션 파인튜닝(instruction finetuning) 단계를 따르는 LLM을 위한 최근 및 새로 부상하는 자동화된 정렬 방법(automated alignment methods)을 검토합니다.

**The Geometry of Categorical and Hierarchical Concepts in Large Language Models** by by Park, Choe, Jiang, and Veitch (6월 3일), https://arxiv.org/abs/2406.01506
Gemma LLM을 사용하여, 이 논문은 선형 표현 가설(linear representation hypothesis)을 확장하여 범주형 개념(categorical concepts)은 단체(simplices)이고, 계층적 관계(hierarchical relations)는 직교하며, 복잡한 개념(complex concepts)은 다면체(polytopes)임을 보여주며, 957개의 WordNet 개념(concepts)으로 검증되었습니다.

**OLoRA: Orthonormal Low-Rank Adaptation of Large Language Models** by Büyükakyüz (6월 3일), https://arxiv.org/abs/2406.01775
OLoRA는 QR 분해(QR decomposition)를 통한 직교 행렬 초기화(orthonormal matrix initialization)를 사용하는 저랭크 적응(Low-Rank Adaptation, LoRA)의 개선 버전으로, 일반 LoRA에 비해 LLM 훈련의 수렴(convergence)을 가속화합니다.

**Skywork-MoE: A Deep Dive into Training Techniques for Mixture-of-Experts Language Models** by Wei, Zhu, Zhao et al. (6월 3일), https://arxiv.org/abs/2406.06563
기존 13B 매개변수 밀집(dense)(비전문가 혼합) 모델에서 146B 매개변수 전문가 혼합(mixture-of-experts) LLM을 개발하는 데 사용된 접근 방식과 방법 중 일부를 설명하는 보고서입니다.

**Show, Don't Tell: Aligning Language Models with Demonstrated Feedback** by Shaikh, Lam, Hejna, et al. (6월 2일), https://arxiv.org/abs/2406.00888
제안된 방법은 모방 학습(imitation learning)을 활용하여 10개 미만의 데모(demonstrations)를 피드백(feedback)으로 사용하여 LLM 출력을 특정 사용자 행동(user behaviors)에 정렬합니다.

이 매거진은 직접적인 금전적 보상이 없는 순수한 열정 프로젝트입니다. 그러나 저의 작업에 가치를 느끼시고 지원해 주시고자 한다면, 제 책 중 한 권을 구매해 주시면 감사하겠습니다. 이 책들이 통찰력 있고 유익하다고 생각하신다면, 친구와 동료들에게 자유롭게 추천해 주시길 바랍니다. (Amazon에 책 리뷰를 통해 다른 사람들과 피드백을 공유하는 것도 큰 도움이 됩니다!)

Build A Large Language Model (From Scratch), Machine Learning Q And AI, and Machine Learning with PyTorch and Scikit-Learn

여러분들의 따뜻한 지원은 저에게 큰 의미이며, 앞으로도 더 좋은 콘텐츠를 만드는 데 큰 힘이 될 것입니다! 감사합니다!