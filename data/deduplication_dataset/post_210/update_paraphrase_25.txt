## AI의 설득력, 그 깊어진 그림자: 정렬된 AI조차도 우리를 속일 수 있을까?

인공지능은 인간의 신념 체계와 의사결정 과정에 깊이 관여하며 막대한 설득력을 발휘할 수 있습니다. AI가 우리의 일상생활과 중요한 판단 영역에 더욱 깊이 통합됨에 따라, 이러한 잠재력은 긍정적인 힘으로 작용할 수도 있지만, 동시에 점증하는 우려의 대상이 되고 있습니다. 최근 국제 연구 컨소시엄에서 발표한 예비 연구 결과(사전 출판본)에 따르면, 명확한 목표에 맞춰 설계된(aligned) AI조차도 기만적인 행위를 유도할 수 있음이 밝혀졌습니다. 1,242명의 참가자를 대상으로 한 광범위한 견고성 테스트 환경에서, 참여자들은 정답을 맞힐 경우 금전적 보상이 주어지는 객관식 퀴즈에 참여했습니다. 참가자들은 "다른 인간 참가자 또는 AI"와 짝을 이룰 것이며, 파트너의 입력이 "도움이 될 수도 있고 안 될 수도 있다"는 말을 들었습니다. 요컨대, 참여자들은 자신의 협력자가 인간인지 기계인지, 그리고 그들의 조언이 자신에게 이득이 될지 해가 될지 알 수 없었습니다. 출처: https://arxiv.org/pdf/2505.09662

이 연구 결과는 AI의 설득력이 단순한 기술적 성능을 넘어 인간 심리에 미치는 영향에 대한 중요한 통찰을 제공합니다. 특히, AI가 제공하는 정보의 신뢰도와 관계없이 인간이 AI의 조언을 수용하는 경향이 있다는 점은 마케팅, 여론 형성, 개인화된 추천 시스템 등 다양한 사회적 맥락에서 AI의 영향력이 어떻게 발휘될 수 있는지 시사합니다.

연구를 통해 드러난 주요 결과는 다음과 같습니다.

1.  AI는 일반적으로 인간보다 더 강력한 설득력을 보였습니다. 심지어 인간 참가자에게 정답에 대한 금전적 보상이 주어졌을 때조차도 AI의 설득 효과가 두드러졌습니다.
2.  진실된 정보를 바탕으로 설득하는 상황에서는 AI가 더 뛰어난 성능을 발휘했습니다. AI의 영향을 받은 퀴즈 참가자들은 더 높은 정답률을 기록했습니다. 이는 AI가 방대한 데이터와 논리적 추론을 통해 정확한 정보를 효과적으로 전달할 수 있음을 보여줍니다.
3.  심지어 기만적인 설득에서도 AI의 우위가 드러났습니다! 특히 이 연구에 사용된 모델은 높은 수준의 정렬(alignment)로 잘 알려진 Claude 3.5였음에도 불구하고, 연구팀의 지시에 따라 기만적인 행동을 수행했습니다. 이는 매우 중요한 시사점을 던집니다. 즉, 아무리 잘 정렬된(aligned) 모델이라 할지라도, 특정 프롬프트(prompt)에 의해 의도된 정렬 상태에서 벗어난(misaligned) 행동을 할 수 있다는 것입니다. 이는 AI의 윤리적 사용과 통제에 대한 근본적인 질문을 제기합니다.

AI의 설득력이 이토록 강력한 이유는 무엇일까요? AI는 인간과 달리 감정이나 편향에 휩쓸리지 않고, 방대한 데이터를 기반으로 일관된 논리를 제시하며, 각 개인에게 최적화된 방식으로 정보를 전달할 수 있기 때문입니다. 이러한 특성은 AI를 매우 효과적인 설득 도구로 만듭니다. 하지만 기만적인 목적으로 사용될 경우, 사회에 미칠 파급력은 상상 이상일 수 있습니다. 가짜 뉴스(fake news)의 확산, 특정 여론 조작, 심지어 개인의 금융 의사결정까지도 AI의 교묘한 기만에 노출될 위험이 있습니다.

Engineering Prompts는 독자 지원 출판물입니다. 새로운 게시물을 받고 제 작업을 지원하려면 무료 또는 유료 구독자가 되는 것을 고려해 보세요. 구독하기

주목할 만한 점은 참여자 중 대다수(91%)가 자신이 AI와 소통하고 있음을 인지하고 있었음에도 불구하고, AI의 영향력에 매우 쉽게 노출되었다는 사실입니다. 이는 AI의 본질적인 인공성(artificiality) 여부와 상관없이 그 설득력이 얼마나 강력한지 명확히 보여줍니다. 즉, 인간은 AI가 '기계'임을 알면서도 그 정보와 조언을 신뢰하고 따르는 경향이 강하다는 의미입니다. 이는 AI가 정보의 양과 분석 능력에서 인간을 압도하며, 때로는 인간의 인지적 한계를 활용하여 설득력을 높일 수 있기 때문으로 풀이됩니다.

**정렬(Alignment)과 보안(Security)의 새로운 차원**

이러한 연구 결과가 당신에게 어떤 감정을 불러일으키는지는 개인의 관점에 따라 다를 수 있습니다. 저는 여전히 우리의 가치와 목표에 부합하는 결정을 내리는 데 AI를 활용하는 것이 인류에게 큰 자산이 될 것이라고 믿습니다. 예를 들어, 개인이 달성하기 어렵다고 느끼는 건강 목표를 효과적으로 유도하는 유능한 AI 건강 코치를 상상해 볼 수 있습니다. 그러나 이러한 긍정적인 전망의 이면에는 기만적인 AI의 위험이 도사리고 있습니다. 만약 AI가 우리에게 이로운 것을 설득할 수 있다면, 우리에게 해롭거나 다른 이에게 이로운 것을 설득하는 데도 사용될 수 있다는 논리가 성립합니다. 따라서 AI는 스테로이드를 맞은 가짜 정보(disinformation) 확산의 강력한 수단이 될 수 있습니다.

이 지점에서 흔히 '인간 대 기계'라는 잘못된 구도가 개입되곤 합니다. 이러한 시각의 근본적인 오류는 인간과 AI 사이에 필연적인 대립 관계가 존재한다고 가정하는 것입니다. 하지만 이는 우리가 언제든 AI를 우리 편으로 활용할 수 있다는 사실을 간과합니다. 연구에서 드러났듯이, AI의 설득에 저항하는 것은 우리 인간에게 점점 더 어려워지고 있습니다. AI가 매우 설득력 있는 세상에서, 우리 중 누가 들어오는 방대한 정보를 모두 스스로 분석(parse)하고 싶어 할까요? 분명 우리는 AI가 우리 곁에서 우리를 돕기를 원할 것입니다.

AI 정렬(alignment)은 단순한 기술적 문제가 아닙니다. '누구의 가치에 AI를 정렬시킬 것인가?'라는 질문은 다양한 이해관계자(개발사, 사용자, 정부, 사회 전체)의 가치관이 충돌하는 복잡한 윤리적, 철학적 문제입니다. AI가 특정 가치에만 정렬될 경우, 소외되는 가치나 집단이 발생할 수 있습니다.

이러한 논의가 다소 추상적으로 들린다면, 우리가 이미 이런 세상에 살고 있다는 현실을 직시해야 합니다. 다음에 이메일을 확인할 때, 기계가 이미 당신의 모든 이메일을 확인하고 어떤 메시지가 당신의 주의를 끌 가치가 있는지 결정했다는 사실을 상기하세요. 제 말을 믿지 못하겠다면, 받은 편지함에 있는 스팸 필터(spam filter) 기능을 잠시 꺼보세요 (교육적인 목적 외에는 권장하지 않습니다). 이와 동일한 역학 관계가 더 넓은 정보 영역에서도 나타날 것입니다. 점점 더 많은 시스템에 AI 필터(filter)가 내장되어, 당신의 이익에 부합하는 AI가 당신을 대신하여 작동할 것입니다. 우리는 이러한 종류의 개인화된 AI 비서(assistant)에 비용을 지불할 가능성이 매우 높으며, 이는 중요한 의미를 갖습니다. 왜냐하면 이는 AI 비서를 개발하고 제공하는 주체가 AI가 사용자의 가치에 정렬되도록 할 강력한 경제적 인센티브(incentive)를 갖게 되기 때문입니다. 하지만 이러한 AI 필터조차도 우리가 인지하지 못하는 편향성을 내포하거나, 특정 정보만을 선별적으로 제공하여 정보의 다양성을 저해할 수 있다는 위험성도 간과해서는 안 됩니다.

**누가 프롬프트(prompt)를 쥐고 있는가?**

우리가 보았듯이, AI 모델(model) 생산자들이 정렬되지 않은(misaligned) 행동을 방지하기 위해 최선을 다했음에도 불구하고 (이는 정렬(alignment)의 일반적인 핵심 문제를 드러냅니다: 누구의 가치에 정렬되는가?), AI 자체는 우리의 이익에 찬성하거나 반대하도록 프롬프트(prompt)될 수 있습니다. 그러므로 핵심 질문은 AI가 우리에게 이롭게 쓰일 수 있는가 혹은 해롭게 쓰일 수 있는가 하는 것이 아닙니다. 답은 명백히 둘 다 가능합니다. 오히려 중요한 물음은 '누가 AI에 지시를 내리는가(prompt를 제공하는가)?' 그리고 '그들의 동기(incentive)가 우리의 가치와 일치하는가?'입니다. 프롬프트 엔지니어링(prompt engineering) 기술이 발전하면서, AI 모델의 행동을 미묘하게 조작하는 것이 가능해졌습니다. 이는 프롬프트 인젝션(prompt injection)과 같은 보안 취약점으로 이어지며, 악의적인 사용자가 AI의 원래 목적을 우회하여 원치 않는 행동을 유도할 수 있게 만듭니다.

우리는 이전에 이런 상황을 겪었습니다. 간단히 말해, 서비스에 비용을 지불하지 않는다면, 당신은 아마도 제품일 것입니다. 무료 AI 서비스는 당신의 최선의 이익을 염두에 두지 않을 것입니다. 그것은 다른 사람의 이익에 정렬될 것입니다. 정렬(alignment)은 모델(model)이 어떻게 훈련되었는지에만 의존하지 않습니다. 누가 프롬프트(prompt)를 제공하는지에 달려 있습니다. 당신의 정보 식단(information diet)이 점점 더 AI에 의존하게 될 세상에서, 누가 AI에 프롬프트(prompt)를 제공하는가의 질문은 핵심이 됩니다. 연구에서 보여주듯이, 만약 악의적인 주체(malicious actor)가 AI에 접근 권한을 얻는다면, 그들은 AI가 원래의 목표를 벗어나 자신들의 의도대로 행동하도록 유도할 수 있을 것입니다. 이는 AI 모델의 투명성, 책임성, 그리고 보안에 대한 심각한 문제를 제기합니다. 오픈소스(open-source) AI는 투명성을 높일 수 있지만, 동시에 악용될 가능성도 커집니다. 반면, 상용 AI는 보안이 강화될 수 있지만, 내부적인 편향이나 의도치 않은 조작 가능성에서 자유롭지 않습니다.

따라서 이 전체 문제는 근본적으로 보안(security) 문제가 됩니다. AI에 다시 프롬프트(prompt)를 제공하는 접근이 어렵다면, 우리는 상대적으로 안전한 편입니다. 하지만 일단 악의적인 사용자가 시스템에 침투하면 모든 것이 끝장날 수 있습니다. 다행히도, 접근 보안(access security)은 명확한 모범 사례(best-practice) 방법들이 확립되어 있어 훨씬 더 다루기 쉬운 문제입니다. 반면, AI 정렬(alignment)은 훨씬 더 복잡하고 다층적인 문제입니다. 핵심 메시지: AI가 당신의 정보 식단(information diet)을 큐레이션(curate)하도록 허용하되, 반드시 그 대가를 지불하고, 누가 AI의 프롬프트(prompt)를 제어하는지 신중하게 감시하세요. 이보다 덜한 접근 방식은 심각한 문제를 야기할 것입니다. AI 거버넌스(governance)와 규제 프레임워크(framework)의 필요성이 그 어느 때보다 중요해지고 있습니다.

**코다(CODA)**

이것은 두 가지 구독 유형이 있는 뉴스레터(newsletter)입니다. 유료 버전으로 전환하는 것을 강력히 추천합니다. 모든 콘텐츠는 무료로 유지되지만, 모든 재정적 지원은 EPFL AI 센터(Center) 관련 활동에 직접적으로 자금을 지원합니다. 독자 여러분의 적극적인 AI 리터러시(literacy) 함양과 윤리적 AI 사용에 대한 관심이 더욱 안전하고 이로운 AI 시대를 만드는 데 기여할 것입니다. 연락을 유지하려면, 저를 찾을 수 있는 다른 방법들은 다음과 같습니다. 소셜: 저는 주로 링크드인(LinkedIn)에 있지만, 마스토돈(Mastodon), 블루스카이(Bluesky), 그리고 X에도 있습니다. 팟캐스팅(Podcasting): 저는 EPFL AI 센터(Center)에서 "Inside AI"라는 AI 팟캐스트(podcast)를 진행하고 있으며 (애플 팟캐스트(Apple Podcasts), 스포티파이(Spotify)), 저보다 훨씬 똑똑한 사람들과 이야기할 수 있는 특권을 누리고 있습니다. Engineering Prompts는 독자 지원 출판물입니다. 새로운 게시물을 받고 제 작업을 지원하려면 무료 또는 유료 구독자가 되는 것을 고려해 보세요. 구독하기