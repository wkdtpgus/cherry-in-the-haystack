# **설득형 인공지능: 누가 프롬프트(prompt)를 쥐고 있는가?**

Author: Marcel Salathé
URL: https://engineeringprompts.substack.com/p/persuasive-ai-who-holds-the-prompt

============================================================

AI는 우리의 신념과 결정을 형성하며 매우 설득력이 있을 수 있습니다. 우리가 AI를 일상생활과 중요한 의사결정에 더 깊이 통합함에 따라, 이러한 잠재력은 긍정적인 힘에도 불구하고 점점 더 큰 우려가 되고 있습니다. 국제 컨소시엄의 새로운 논문(사전 출판본)에 따르면, 잘 정렬된(aligned) AI조차도 우리를 속이도록 유도될 수 있다고 합니다. 광범위한 견고성(robustness) 검사를 거친 이 실험 설정(N = 1,242)에서 참가자들은 정답에 대한 금전적 인센티브(incentive)가 있는 객관식 퀴즈를 풀었습니다. 참가자들은 "다른 인간 참가자 또는 AI"와 짝을 이룰 것이며, 파트너의 입력이 "도움이 될 수도 있고 안 될 수도 있다"는 말을 들었습니다. 다시 말해, 참가자들은 자신의 동료가 인간인지 기계인지, 그리고 자신에게 유리하게 작용하는지 불리하게 작용하는지 알지 못했습니다. 출처: https://arxiv.org/pdf/2505.09662

결과는 다음과 같습니다.
1. AI는 일반적으로 인간보다 더 설득력이 있었습니다. 심지어 인간에게 금전적 인센티브가 주어졌을 때도 마찬가지였습니다.
2. 진실된 설득에 있어서 AI는 더 나은 성능을 보였습니다 (AI의 영향을 받은 퀴즈 참가자들은 더 높은 정확도를 가졌습니다).
3. 기만적인 설득에 있어서도 마찬가지였습니다! 특히, 사용된 모델(model)은 정렬(alignment)에 대한 강력한 명성을 가진 Claude 3.5였습니다. 그러나 연구 설계자들이 지시했을 때, 이 모델은 여전히 기만적인 지시를 따랐습니다. 이것이 핵심입니다. 잘 정렬된(aligned) 모델조차도 프롬프트(prompt)가 지시하면 정렬되지 않은(misaligned) 행동을 할 것입니다.

Engineering Prompts는 독자 지원 출판물입니다. 새로운 게시물을 받고 제 작업을 지원하려면 무료 또는 유료 구독자가 되는 것을 고려해 보세요. 구독하기

흥미롭게도, 대부분의 참가자(91%)가 자신이 AI와 상호작용하고 있음을 인지했음에도 불구하고, 그들은 AI의 영향에 매우 취약했습니다. 이는 AI의 인공성(artificiality)과 관계없이 AI의 설득력을 보여줍니다.

**정렬(Alignment)과 보안(Security)**
이러한 발견이 당신에게 불쾌하게 느껴지는지는 당신의 관점에 달려 있습니다. 저는 우리의 가치에 부합하는 결정을 내리는 데 AI를 사용하는 것이 큰 자산이 될 것이라고 계속 믿습니다. 당신이 이미 원하지만 실행하기 어려운 일을 하도록 정확히 설득하는 방법을 아는 유용한 AI 건강 코치를 생각해 보세요. 여기서 반대 의견은 대개 기만적인 AI입니다. AI가 우리에게 좋은 것을 설득할 수 있다면, 우리에게 좋지 않고 오히려 다른 사람에게 좋은 것을 설득할 수도 있다는 주장이 나옵니다. 따라서 AI는 스테로이드를 맞은 가짜 정보(disinformation)를 가능하게 할 것입니다.

여기서 "인간 대 기계"라는 잘못된 프레임(framing)이 스며듭니다. 핵심적인 오류는 인간과 AI 사이에 피할 수 없는 갈등이 있다는 것입니다. 그러나 이것은 우리 또한 항상 우리 편에 AI를 둘 것이라는 사실을 무시합니다. 연구에서 보여주듯이, AI의 설득에 저항하는 것은 우리 인간에게 점점 더 어려워지고 있습니다. AI가 매우 설득력 있는 세상에서, 우리 중 누가 들어오는 정보를 스스로 분석(parse)하고 싶어 할까요? 분명히 우리는 AI가 우리 곁에서 우리를 돕기를 원할 것입니다.

이것이 이상하게 들린다면, 우리가 이미 이런 세상에 살고 있다는 생각을 고려해 보세요. 다음에 이메일을 확인할 때, 기계가 이미 당신의 모든 이메일을 확인하고 어떤 메시지가 당신의 주의를 끌 가치가 있는지 결정했다는 사실을 상기하세요. 제 말을 믿지 못하겠다면, 받은 편지함에 있는 스팸 필터(spam filter) 기능을 꺼보세요 (교육적인 목적 외에는 권장하지 않습니다). 동일한 역학 관계가 더 넓은 정보 영역에서도 나타날 것입니다. 점점 더 많은 시스템에 AI 필터(filter)가 생겨, 당신의 이익에 부합하는 AI가 당신을 대신하여 작동할 것입니다. 우리는 이러한 종류의 AI에 비용을 지불할 가능성이 매우 높으며, 이는 중요합니다. 왜냐하면 이는 이 AI 비서(assistant)를 생산하는 사람이 AI가 우리와 정렬되도록(aligned) 할 강력한 경제적 인센티브(incentive)를 갖는다는 것을 의미하기 때문입니다.

**누가 프롬프트(prompt)를 쥐고 있는가?**
우리가 보았듯이, AI 모델(model) 생산자들이 정렬되지 않은(misaligned) 행동을 피하기 위해 최선을 다했음에도 불구하고 (이는 정렬(alignment)의 일반적인 핵심 문제를 드러냅니다: 누구의 가치에 정렬되는가?), AI 자체는 우리의 이익에 찬성하거나 반대하도록 프롬프트(prompt)될 수 있습니다. 따라서 질문은 AI가 우리에게 유리하게 사용될 수 있는가 아니면 불리하게 사용될 수 있는가가 아닙니다. 답은 분명히 둘 다입니다. 오히려 질문은 누가 AI에 프롬프트(prompt)를 제공하는가, 그리고 그들의 인센티브(incentive)가 우리와 정렬되어 있는가입니다.

우리는 이전에 이런 상황을 겪었습니다. 간단히 말해, 서비스에 비용을 지불하지 않는다면, 당신은 아마도 제품일 것입니다. 무료 AI 서비스는 당신의 최선의 이익을 염두에 두지 않을 것입니다. 그것은 다른 사람의 이익에 정렬될 것입니다. 정렬(alignment)은 모델(model)이 어떻게 훈련되었는지에만 의존하지 않습니다. 누가 프롬프트(prompt)를 제공하는지에 달려 있습니다. 당신의 정보 식단(information diet)이 점점 더 AI에 의존하게 될 세상에서, 누가 AI에 프롬프트(prompt)를 제공하는가의 질문은 핵심이 됩니다. 연구에서 보여주듯이, 악의적인 행위자(malicious actor)가 AI에 접근한다면, 그들은 실제로 AI가 편을 바꾸도록 프롬프트(prompt)할 수 있을 것입니다.

따라서 이 전체 문제는 보안(security) 문제가 됩니다. AI에 다시 프롬프트(prompt)를 제공하는 접근이 어렵다면, 저는 안전한 편입니다. 일단 나쁜 사람들이 들어오면, 모든 것이 끝장입니다. 이것은 좋은 소식입니다. 접근 보안(access security)은 명확한 모범 사례(best-practice) 방법이 있는 훨씬 더 다루기 쉬운 문제입니다. AI 정렬(alignment)은 그렇지 않습니다. 핵심 메시지: AI가 당신의 정보 식단(information diet)을 큐레이션(curate)하도록 하고, 비용을 지불하며, 누가 AI의 프롬프트(prompt)를 제어하는지 신중하게 지키세요. 그보다 덜한 것은 문제를 야기할 것입니다.

**코다(CODA)**
이것은 두 가지 구독 유형이 있는 뉴스레터(newsletter)입니다. 유료 버전으로 전환하는 것을 강력히 추천합니다. 모든 콘텐츠는 무료로 유지되지만, 모든 재정적 지원은 EPFL AI 센터(Center) 관련 활동에 직접적으로 자금을 지원합니다. 연락을 유지하려면, 저를 찾을 수 있는 다른 방법들은 다음과 같습니다. 소셜: 저는 주로 링크드인(LinkedIn)에 있지만, 마스토돈(Mastodon), 블루스카이(Bluesky), 그리고 X에도 있습니다. 팟캐스팅(Podcasting): 저는 EPFL AI 센터(Center)에서 "Inside AI"라는 AI 팟캐스트(podcast)를 진행하고 있으며 (애플 팟캐스트(Apple Podcasts), 스포티파이(Spotify)), 저보다 훨씬 똑똑한 사람들과 이야기할 수 있는 특권을 누리고 있습니다. Engineering Prompts는 독자 지원 출판물입니다. 새로운 게시물을 받고 제 작업을 지원하려면 무료 또는 유료 구독자가 되는 것을 고려해 보세요. 구독하기