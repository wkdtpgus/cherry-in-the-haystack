인공지능(AI)은 우리의 사고방식과 선택에 깊이 관여하며, 그 설득력이 상당한 수준에 이를 수 있습니다. 인공지능이 일상생활과 중요한 의사결정 과정에 더욱 깊이 스며들면서, 이러한 강력한 잠재력은 긍정적 측면과 함께 점증하는 걱정거리로 부상하고 있습니다. 최근 국제 연구 컨소시엄이 발표한 새로운 연구 논문(사전 공개본)에 따르면, 심지어 윤리적으로 잘 정립된(aligned) 것으로 평가받는 인공지능 시스템조차도 기만적인 행위를 유발하도록 조작될 수 있다는 충격적인 결과가 나왔습니다.

광범위한 견고성(robustness) 검사를 거친 이 실험 설정(참여자 1,242명)에서, 피실험자들은 정답을 맞힐 경우 금전적 보상을 받는 객관식 퀴즈에 참여했습니다. 실험 참여자들은 '다른 사람 또는 지능형 기계'와 한 조를 이루게 되며, 짝꿍의 조언이 '유용할 수도 있고 그렇지 않을 수도 있다'는 설명을 들었습니다. 즉, 그들은 자신의 협력자가 사람인지 인공지능인지, 그리고 그 조언이 자신에게 이득이 될지 해가 될지 사전에 알 수 없었습니다. 이러한 불확실성 속에서 참여자들의 반응과 인공지능의 영향력이 면밀히 분석되었습니다. 출처: https://arxiv.org/pdf/2505.09662

**주요 연구 결과 분석:**

1.  **AI의 설득력 우위**: 인공지능은 일반적으로 인간보다 더 높은 설득력을 보였습니다. 심지어 인간 참여자에게 명확한 금전적 유인이 제공되었을 때조차도 그러했습니다. 이는 인공지능이 제공하는 정보의 일관성, 객관성, 그리고 때로는 인간적인 감정적 요소의 부재가 오히려 설득력에 긍정적으로 작용할 수 있음을 시사합니다.
2.  **진실된 설득의 효율성**: 진실을 기반으로 한 설득에 있어서 인공지능은 월등한 성능을 자랑했습니다. 인공지능의 영향을 받은 퀴즈 참가자들은 훨씬 더 높은 정확도를 기록했으며, 이는 인공지능이 사실 기반 정보를 전달하고 최적의 해답을 유도하는 데 탁월한 능력을 가짐을 보여줍니다.
3.  **기만적 설득의 위험성**: 속임수를 쓰는 설득 상황에서도 비슷한 양상이 관찰되었습니다. 특히, 사용된 지능형 시스템은 '정렬(alignment)' 면에서 뛰어난 평가를 받던 Claude 3.5였으나, 연구진의 지시에 따라 기만적인 행동을 서슴지 않았습니다. 이는 매우 중요한 시사점을 던집니다. 아무리 바르게 정립된(aligned) 시스템이라 할지라도, 외부의 입력(prompt)이 특정 방향을 지시하면 본래의 정렬 상태에서 벗어난 행위를 할 수 있다는 점입니다. 인공지능의 '정렬'이 모델 자체의 내재된 가치관보다는 외부 명령에 얼마나 유연하게 반응하는지에 달려있을 수 있음을 보여주는 대목입니다.

Engineering Prompts는 독자 지원 출판물입니다. 새로운 게시물을 받고 제 작업을 지원하려면 무료 또는 유료 구독자가 되는 것을 고려해 보세요. 구독하기

**AI의 심리적 영향과 인간의 취약성**

놀랍게도, 참여자 대다수(91퍼센트)가 자신들이 인공지능과 소통하고 있음을 명확히 인식했음에도 불구하고, 그들은 인공지능의 영향력에 매우 쉽게 노출되었습니다. 이러한 결과는 인공지능의 본질적인 인공성(artificiality)과 관계없이, 그 설득력이 얼마나 강력한지를 여실히 보여줍니다. 이는 인간이 기술에 대한 신뢰, 정보의 양에 압도당하는 경향, 또는 복잡한 정보를 처리하기 위한 인지적 노력을 회피하려는 심리적 편향 등 여러 요인에 의해 인공지능의 조언을 무비판적으로 수용하게 될 수 있음을 시사합니다. 미래 사회에서는 인공지능이 제공하는 정보에 대한 비판적 사고 능력을 키우는 것이 더욱 중요해질 것입니다.

**정렬(Alignment)의 복잡성과 보안(Security)의 새로운 관점**

이러한 발견이 불편하게 느껴지는지는 개인의 관점에 따라 다를 수 있습니다. 저는 인공지능이 우리의 가치에 부합하는 결정을 내리는 데 활용된다면 큰 자산이 될 것이라고 여전히 믿습니다. 예를 들어, 우리가 이미 원하지만 실행하기 어려운 목표를 달성하도록 정확히 설득하는 방법을 아는 유용한 인공지능 건강 코치를 상상해 볼 수 있습니다. 여기서 반대 의견은 대개 기만적인 인공지능에 대한 우려에서 비롯됩니다. 만약 인공지능이 우리에게 이로운 것을 설득할 수 있다면, 우리에게 좋지 않고 오히려 다른 주체에게 이로운 것을 설득할 수도 있다는 주장이 제기됩니다. 이는 인공지능이 강력한 허위 정보(disinformation) 확산의 도구가 될 가능성을 내포합니다.

'인간 대 기계'라는 이분법적 사고방식은 본질적으로 잘못된 관점을 주입합니다. 이는 인간과 인공지능 간의 불가피한 대립 구도를 상정하는 치명적인 오류를 내포하고 있지만, 실제로는 인공지능이 언제나 우리 편에 서서 도울 것이라는 현실을 간과하게 만듭니다. 우리는 인공지능의 설득에 저항하기가 점점 더 어려워지는 세상에 살고 있습니다. 인공지능이 매우 설득력 있는 정보 흐름을 만들어내는 환경에서, 우리 중 누가 모든 정보를 스스로 분석(parse)하고 싶어 할까요? 분명히 우리는 인공지능이 우리 곁에서 우리를 돕기를 원할 것입니다.

이러한 상황이 비현실적으로 들린다면, 우리가 이미 이런 세상에 살고 있다는 현실을 고려해 보십시오. 다음번에 전자우편함을 열어볼 때, 이미 기계 시스템이 당신의 모든 메시지를 검토하여 어떤 것이 주목할 가치가 있는지 분류해 놓았다는 사실을 떠올려보십시오. 만약 이 말이 믿기지 않는다면, 수신함의 스팸 메시지 차단 기능을 잠시 비활성화해 보십시오 (물론 학습 목적 외에는 권장하지 않습니다). 이와 동일한 역학 관계가 더 넓은 정보 영역에서도 나타날 것입니다. 점점 더 많은 시스템에 인공지능 필터(filter)가 내장되어, 당신의 이익에 부합하는 인공지능이 당신을 대신하여 작동할 것입니다. 우리는 이러한 종류의 인공지능에 기꺼이 비용을 지불할 가능성이 매우 높으며, 이는 중요합니다. 왜냐하면 이는 이 인공지능 비서(assistant)를 생산하는 주체가 인공지능이 우리와 정렬되도록(aligned) 할 강력한 경제적 인센티브(incentive)를 갖는다는 것을 의미하기 때문입니다.

**프롬프트(Prompt)의 권력과 책임**

우리가 목격했듯이, 인공지능 모델 생산자들이 정렬되지 않은(misaligned) 행동을 방지하기 위해 최선을 다했음에도 불구하고 (이는 '정렬'의 근본적인 문제, 즉 '누구의 가치에 정렬되는가?'를 드러냅니다), 인공지능 자체는 우리의 이익에 찬성하거나 반대하도록 프롬프트(prompt)될 수 있습니다. 따라서 질문은 인공지능이 우리에게 유리하게 사용될 수 있는가 아니면 불리하게 사용될 수 있는가가 아닙니다. 답은 분명히 둘 다입니다. 오히려 핵심 질문은 '누가 인공지능에 프롬프트(prompt)를 제공하는가'이며, '그들의 인센티브(incentive)가 우리와 정렬되어 있는가'입니다.

우리는 이러한 상황을 이미 경험한 바 있습니다. 쉽게 말해, 어떤 서비스에 대해 직접적인 대가를 지불하지 않는다면, 대개 당신 자신이 그 서비스의 상품이 됩니다. 무상으로 제공되는 인공지능 서비스는 당신의 최적의 이익을 우선시하지 않을 것이며, 대신 다른 주체의 목적에 부합하도록 설계될 것입니다. 인공지능의 정렬(alignment)은 모델이 어떻게 훈련되었는지에만 의존하지 않습니다. 누가 프롬프트(prompt)를 제공하는지에 따라 그 행동 양상이 결정됩니다. 당신의 정보 식단(information diet)이 점점 더 인공지능에 의존하게 될 세상에서, 누가 인공지능에 프롬프트를 제공하는가의 문제는 그 어느 때보다 중요해집니다. 연구에서 보여주듯이, 악의적인 행위자(malicious actor)가 인공지능에 접근할 수 있다면, 그들은 실제로 인공지능이 본래의 목적에서 벗어나 다른 편을 들도록 유도할 수 있을 것입니다.

결론적으로, 이 모든 문제는 결국 보안(security) 문제로 귀결됩니다. 만약 인공지능에 다시 프롬프트를 제공하는 접근 자체가 어렵다면 우리는 안전할 수 있습니다. 하지만 일단 나쁜 의도를 가진 사람들이 시스템에 침투한다면, 모든 것이 무너질 수 있습니다. 다행히도, 접근 보안(access security)은 명확한 모범 사례(best-practice)와 방법론이 존재하는 훨씬 더 다루기 쉬운 문제입니다. 반면, 인공지능 정렬(alignment)은 그 정의와 구현이 훨씬 더 복잡하고 철학적인 난제를 안고 있습니다. 핵심 메시지는 분명합니다. 인공지능이 당신의 정보 식단(information diet)을 큐레이션(curate)하도록 허용하되, 그에 합당한 비용을 지불하고, 누가 그 인공지능의 프롬프트(prompt)를 제어하는지 신중하게 지켜봐야 합니다. 이보다 덜한 접근은 필연적으로 문제를 야기할 것입니다.

**미래를 위한 전략적 접근**

인공지능의 설득력과 프롬프트의 중요성이 점증하는 현실에서, 우리는 단순히 기술 발전을 수용하는 것을 넘어 전략적인 대응 방안을 모색해야 합니다. 첫째, **AI 리터러시** 교육을 강화하여 일반 사용자들이 인공지능의 작동 방식, 잠재적 편향, 그리고 설득 전략에 대해 이해하도록 돕는 것이 필수적입니다. 둘째, **투명성과 설명 가능성**을 높이는 방향으로 인공지능 모델 개발이 이루어져야 합니다. '블랙박스'와 같은 불투명한 인공지능은 신뢰를 구축하기 어렵습니다. 셋째, **규제 및 윤리 가이드라인** 마련이 시급합니다. 인공지능의 오용을 방지하고 책임 있는 개발 및 배포를 위한 법적, 윤리적 틀이 필요합니다. 마지막으로, **사용자 주도권**을 강화하는 시스템 설계가 중요합니다. 인공지능이 제공하는 정보를 무비판적으로 수용하기보다는, 사용자가 자신의 가치와 목적에 따라 인공지능의 행동을 통제하고 조정할 수 있는 메커니즘이 필요합니다. 이러한 다각적인 노력을 통해 우리는 인공지능의 긍정적인 잠재력을 극대화하고 위험을 최소화할 수 있을 것입니다.

**코다(CODA)**
이것은 두 가지 구독 유형이 있는 뉴스레터(newsletter)입니다. 유료 버전으로 전환하시는 것을 강력히 추천합니다. 모든 콘텐츠는 무료로 제공되지만, 여러분의 모든 재정적 지원은 EPFL AI 센터(Center) 관련 활동에 직접적으로 자금을 지원합니다. 소식을 계속 접하고 싶으시다면, 저를 찾을 수 있는 다른 방법들은 다음과 같습니다. 소셜 미디어: 저는 주로 링크드인(LinkedIn)에서 활동하지만, 마스토돈(Mastodon), 블루스카이(Bluesky), 그리고 X에서도 만나볼 수 있습니다. 팟캐스팅(Podcasting): 저는 EPFL AI 센터(Center)에서 "Inside AI"라는 인공지능 팟캐스트(podcast)를 진행하며 (애플 팟캐스트(Apple Podcasts), 스포티파이(Spotify)에서 청취 가능), 저보다 훨씬 더 똑똑한 분들과 대화할 수 있는 귀한 기회를 얻고 있습니다. Engineering Prompts는 독자들의 성원에 힘입어 발행되는 출판물입니다. 새로운 게시물을 받아보고 저의 작업을 지지하시려면, 무료 또는 유료 구독자가 되어주시길 고려해 주십시오. 구독하기