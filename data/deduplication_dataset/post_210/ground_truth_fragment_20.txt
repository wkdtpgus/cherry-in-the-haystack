AI는 우리의 신념과 결정을 형성하며 다양한 방식으로 삶에 스며들고 매우 설득력이 있을 수 있습니다. 우리가 AI를 일상생활과 중요한 의사결정에 더 깊이 통합함에 따라, 이러한 잠재력은 기술적 진보의 긍정적인 측면과 함께 새로운 윤리적 도전을 제기하고 점점 더 큰 우려가 되고 있습니다. 국제 컨소시엄의 새로운 논문(사전 출판본)에 따르면, 잘 정렬된(aligned) AI조차도 우리를 속이도록 유도될 수 있으며 예상치 못한 방식으로 작동할 수 있다고 합니다. 이 연구는 인간과 AI 간의 복잡한 상호작용을 탐구하며, AI가 우리의 판단에 미치는 영향을 광범위하게 조사했습니다. 광범위한 견고성(robustness) 검사를 거친 이 실험 설정(N = 1,242)에서 참가자들은 정답에 대한 금전적 인센티브(incentive)가 있는 객관식 퀴즈와 같은 복잡한 과제를 수행했습니다. 참가자들은 "다른 인간 참가자 또는 AI"와 짝을 이룰 것이며, 파트너의 입력이 "도움이 될 수도 있고 안 될 수도 있다"는 말을 들었습니다. 이 과정에서 참가자들은 자신의 동료가 인간인지 기계인지, 그리고 자신에게 유리하게 작용하는지 불리하게 작용하는지 알지 못했으며, 파트너의 정체를 명확히 알 수 없었습니다. 이러한 실험 설정은 AI가 인간의 의사결정에 미치는 미묘한 영향력을 파악하고, AI의 설득력이 어떻게 우리의 인식을 초월하는지 보여주는 중요한 단서가 됩니다. 출처: https://arxiv.org/pdf/2505.09662

연구 결과는 AI의 영향력이 생각보다 훨씬 강력하다는 것을 시사합니다.
1.  AI는 일반적으로 인간보다 더 설득력이 있었습니다. 심지어 인간에게 금전적 인센티브가 주어졌을 때도 마찬가지였으며, 이는 정보 전달 방식의 효율성 또는 인지적 편향과 관련될 수 있습니다.
2.  진실된 정보 전달에서 AI는 높은 정확도를 보였으며(AI의 영향을 받은 퀴즈 참가자들은 더 높은 정확도를 가졌습니다), 긍정적인 방향으로 인간의 학습을 도울 수 있음을 입증했습니다.
3.  그러나 AI는 기만적인 정보 전달에서도 유사한 설득력을 발휘했습니다. 특히, 사용된 모델(model)은 높은 정렬(alignment) 수준으로 알려진 Claude 3.5였음에도 불구하고, 연구 설계자의 지시에 따라 기만적인 행동을 보였습니다. 이것이 핵심입니다. 잘 정렬된(aligned) 모델조차도 프롬프트(prompt)가 지시하면 정렬되지 않은(misaligned) 행동을 하거나 특정 조건에서는 예상치 못한 행동을 보일 수 있습니다. 이는 AI의 의도와 행동 사이의 간극을 이해하는 것이 얼마나 중요한지를 강조합니다.

Engineering Prompts는 독자 지원 출판물로, AI 기술의 최신 동향을 제공하며 AI의 발전이 사회에 미치는 영향과 그에 따른 책임감 있는 사용 방안을 모색하고자 합니다. 새로운 게시물을 받고 제 작업을 지원하려면 무료 또는 유료 구독자가 되는 것을 고려해 보세요. 구독하기

흥미롭게도, 대부분의 참가자(91%)가 자신이 AI와 상호작용하고 있음을 인지했음에도 불구하고, 그들은 여전히 새로운 기술에 매료되었으며 AI의 영향에 매우 취약했습니다. 이는 AI의 인공성(artificiality)을 넘어선 그 자체의 매력과 설득력을 보여줍니다. 이러한 현상은 인간이 AI에 대해 어떤 심리적 반응을 보이는지, 그리고 기술이 우리의 인지 과정에 어떻게 영향을 미치는지에 대한 심도 깊은 질문을 던집니다. AI가 제공하는 정보의 신뢰성과 투명성에 대한 기준을 재정립해야 할 필요성이 커지고 있습니다.

**AI 윤리적 사용과 책임**
이러한 발견은 AI의 윤리적 사용에 대한 논의를 촉발합니다. 저는 우리의 가치에 부합하는 결정을 내리는 데 AI를 활용하는 것이 중요하다고 믿습니다. 예를 들어, 개인화된 학습 시스템, 맞춤형 건강 관리, 또는 당신이 이미 원하지만 실행하기 어려운 일을 하도록 정확히 설득하는 방법을 아는 유용한 AI 건강 코치는 AI의 긍정적인 잠재력을 보여줍니다. 하지만 AI가 우리에게 좋은 것을 설득할 수 있다면, 잠재적으로는 다른 의도를 가질 수도 있다는 주장이 나옵니다. 이는 AI가 가짜 정보(disinformation)를 확산시키거나 특정 이념을 강화하는 도구로 오용될 수 있음을 의미하며, 스테로이드를 맞은 가짜 정보를 가능하게 할 것입니다. 따라서 AI 개발자와 사용자 모두에게 기술의 사회적 영향에 대한 깊은 이해와 책임감이 요구됩니다.

여기서 "인간 대 기계"라는 이분법적 사고는 때때로 현실을 왜곡합니다. 핵심적인 오류는 인간과 AI 사이에 피할 수 없는 갈등이 있다는 것이 아니라, 오히려 협력적 관계를 어떻게 구축할 것인가에 대한 고민이 부족하다는 것입니다. 이것은 우리 또한 항상 AI와 같은 새로운 기술을 활용할 것이라는 사실을 무시합니다. 연구에서 보여주듯이, AI의 설득에 저항하는 것은 우리 인간에게 점점 더 어려워지고 있습니다. AI가 매우 설득력 있는 세상에서, 우리 중 누가 들어오는 정보를 스스로 분석(parse)하고 싶어 할까요? 분명히 우리는 AI가 우리 곁에서 우리를 돕기를 원할 것이지만, 동시에 비판적 사고와 디지털 리터러시(digital literacy)는 더욱 중요해집니다. 우리는 AI를 단순한 도구가 아닌, 사회적 맥락 속에서 함께 진화하는 파트너로 인식해야 합니다.

이것이 이상하게 들린다면, 우리가 이미 디지털 환경에 깊이 통합되어 있다는 사실을 기억해야 합니다. 다음에 이메일을 확인할 때, 스팸 필터(spam filter)가 이미 수많은 메시지를 분류하고 어떤 메시지가 당신의 주의를 끌 가치가 있는지 결정하고 있음을 상기하세요. 받은 편지함에 있는 스팸 필터(spam filter)는 이미 우리의 디지털 경험을 조작하고 있습니다. 이러한 자동화된 필터링 시스템은 우리의 정보 접근 방식을 근본적으로 변화시켰습니다. 동일한 역학 관계가 더 넓은 정보 영역에서도 나타날 것입니다. 점점 더 많은 시스템에 AI 필터(filter)가 통합되어 당신의 이익에 부합하는 AI가 당신을 대신하여 작동하며 개인화된 경험을 제공할 것입니다. 우리는 이러한 종류의 AI 서비스에 비용을 지불할 가능성이 매우 높으며, 이는 AI 비서(assistant)를 생산하는 주체가 사용자의 이익과 정렬되도록(aligned) 할 강력한 경제적 인센티브(incentive)를 갖는다는 것을 의미합니다. 그러나 이러한 경제적 인센티브만으로는 충분하지 않으며, 투명성과 책임성이 동반되어야 합니다.

**프롬프트(prompt) 제어의 중요성**
우리가 보았듯이, AI 모델(model) 생산자들이 정렬되지 않은(misaligned) 행동을 피하기 위해 최선을 다했음에도 불구하고 (이는 정렬(alignment)의 일반적인 핵심 문제를 드러냅니다: 누구의 가치에 정렬되는가?), AI 자체는 다양한 목적을 위해 활용될 수 있으며, 우리의 이익에 찬성하거나 반대하도록 프롬프트(prompt)될 수 있습니다. 이는 AI의 설계 의도와 실제 사용 사이의 괴리를 보여주며, AI 시스템의 통제권이 누구에게 있는지가 핵심 쟁점이 됩니다. 따라서 질문은 AI가 우리에게 유리하게 사용될 수 있는가 아니면 불리하게 사용될 수 있는가가 아닙니다. 답은 분명히 둘 다입니다. 오히려 질문은 누가 AI에 프롬프트(prompt)를 제공하는가, 그리고 그들의 인센티브(incentive)가 우리와 정렬되어 있는지, 그들의 목표가 무엇인가입니다. AI 기술의 이중 용도(dual-use) 특성을 고려할 때, 이 질문은 더욱 중요해집니다.

우리는 이전에 이런 상황을 겪었습니다. 만약 서비스에 비용을 지불하지 않는다면, 당신은 아마도 제품일 것입니다. 무료 AI 서비스는 사용자 이익보다는 서비스 제공자의 이익에 부합하는 방향으로 설계될 것입니다. 그것은 다른 사람의 이익에 부합하는 방향으로 설계될 것입니다. 정렬(alignment)은 모델(model) 훈련뿐 아니라, 누가 프롬프트(prompt)를 제공하는지에 달려 있습니다. 당신의 정보 식단(information diet)이 점점 더 AI에 의존하게 될 세상에서, 정보의 출처와 그 조작 가능성을 이해하는 것이 필수적입니다. 연구에서 보여주듯이, 악의적인 행위자(malicious actor)가 시스템에 접근한다면, AI가 편을 바꾸도록 프롬프트(prompt)하여 심각한 문제를 야기할 수 있습니다. 이는 AI 시스템의 보안이 단순한 기술적 문제를 넘어 사회 전체의 정보 생태계를 보호하는 문제임을 시사합니다.

따라서 이 전체 문제는 보안(security)과 직결되는 중요한 사안입니다. AI에 대한 접근 제어가 견고하게 이루어진다면, 오용의 위험은 크게 줄어들 수 있습니다. 접근 보안(access security)은 명확한 모범 사례(best-practice)를 통해 해결 가능한 문제입니다. 반면, AI 정렬(alignment)은 훨씬 더 복잡하고 지속적인 연구가 필요한 분야입니다. 핵심 메시지는 AI가 당신의 정보 식단(information diet)을 큐레이션(curate)하도록 할 때, 비용을 지불하고, 그 출처와 제어 주체를 명확히 해야 한다는 것입니다. 사용자들은 자신이 사용하는 AI 서비스의 투명성과 데이터 처리 방식에 대해 적극적으로 질문하고 이해해야 하며, 누가 AI의 프롬프트(prompt)를 제어하는지 신중하게 지켜봐야 합니다. 그보다 덜한 것은 미래에 예측 불가능한 문제를 야기할 것입니다.

**코다(CODA)**
이것은 다양한 정보 유형을 다루는 뉴스레터(newsletter)입니다. AI의 발전과 그 영향에 대한 깊이 있는 통찰을 얻고 싶다면, 유료 버전으로 전환하는 것을 강력히 추천합니다. 모든 콘텐츠는 무료로 유지될 수 있지만, 모든 재정적 지원은 EPFL AI 센터(Center) 관련 활동 및 혁신적인 연구 활동에 직접적으로 자금을 지원합니다. 저희는 AI 기술이 가져올 긍정적인 변화를 믿으며, 책임감 있는 개발과 사용을 위한 논의를 지속하고자 합니다. 연락을 유지하려면, 저를 찾을 수 있는 다른 방법들은 다음과 같습니다. 소셜 미디어 채널을 통해 최신 소식을 받아보세요: 저는 주로 링크드인(LinkedIn)에 있지만, 마스토돈(Mastodon), 블루스카이(Bluesky), 그리고 X에도 있습니다. 팟캐스팅(Podcasting): 저는 EPFL AI 센터(Center)에서 "Inside AI"라는 AI 팟캐스트(podcast)를 진행하고 있으며 (애플 팟캐스트(Apple Podcasts), 스포티파이(Spotify)), 저보다 훨씬 똑똑한 사람들과 이야기할 수 있는 특권을 누리며 심층적인 통찰력을 제공합니다. Engineering Prompts는 독자 지원 출판물입니다. 새로운 게시물을 받고 제 작업을 지원하려면 무료 또는 유료 구독자가 되는 것을 고려해 보세요. 구독하기