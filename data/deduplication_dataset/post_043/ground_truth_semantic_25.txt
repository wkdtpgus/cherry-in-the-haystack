(출처: [2, 5, 7, 9, 10]) 거대 언어 모델(LLM)의 조정(alignment) 과정은 인류의 기대에 부합하는 결과물(output)을 생성하도록 유도하는 핵심 절차입니다. 이러한 조정 기법의 고전적인 방식으로는 지도형 미세 조정(supervised finetuning)과 근접 정책 최적화(PPO)에 기반한 인간 피드백 강화 학습(RLHF)이 있습니다. 이 기법들은 대체로 효과적이지만, PPO 기반 RLHF는 여러 복잡성 때문에 실행하기 까다로운 온라인 강화 학습(RL) 훈련 알고리즘으로 평가됩니다. PPO는 훈련 과정에서 현재 LLM으로부터 직접 샘플(이를 "온-정책(on-policy) 샘플"이라 칭함)을 만들어내기 위해 추론(inference) 작업을 활발히 수행합니다. 이처럼 온-정책 데이터(on-policy data)를 실시간으로 생성하는 특성 때문에 PPO는 온라인 알고리즘으로 분류됩니다. 온라인 RL 훈련은 효율적인 관리가 어렵고(특히 동기식 훈련 환경에서), 종종 안정성 문제에 직면합니다. 또한, PPO는 훈련 시 LLM의 여러 버전을 동시에 유지해야 하므로 상당한 메모리 오버헤드(memory overhead)와 높은 하드웨어 사양을 요구합니다. 성공적인 훈련을 위해서는 PPO의 방대한 훈련 설정과 설계 결정을 세심하게 관리해야 합니다 [21].

우리는 (i) 부담이 적은 온라인 RL 기법을 활용하거나, (ii) 오프라인 알고리즘(offline algorithms)을 개발하거나, 심지어 (iii) 조정 과정에서 RL 요소를 완전히 배제함으로써 온라인 RL의 복잡성을 회피할 수 있습니다. 그러나 온라인 RL은 매우 뛰어난 성능을 보여주며, 더 간소화된 조정 알고리즘들은 일반적으로 성능 저하를 수반하는 경향이 있습니다. "일부 연구 결과는 온라인 RL이 우수한 미세 조정 성과를 달성하는 데 결정적이라고 시사하는 반면, 다른 연구는 (오프라인) 대조 학습(contrastive learning)이나 순수 지도 방식만으로도 충분하다고 판단합니다." - 출처: [5]

본 개요에서는 LLM 조정을 위한 온라인, PPO 기반 인간 피드백 강화 학습의 대안들을 심층적으로 탐구할 것입니다. 특히, 우리의 주요 관심사는 온-정책 샘플링(on-policy sampling)을 수행하는 온라인 알고리즘과 고정된 데이터셋(fixed dataset)으로 LLM을 훈련하는 오프라인 알고리즘 간의 성과 차이를 분석하는 데 있습니다. 이 분야의 연구 논문들을 검토함으로써 다음과 같은 핵심 질문에 대한 해답을 모색할 것입니다: 고품질 LLM 조정에 강화 학습이 필수적인가? 조정 과정에서 온-정책 훈련 데이터(on-policy training data) 샘플링이 중요한가?

앞으로 살펴보겠지만, 온-정책 샘플링은 명백한 성능 우위를 제공하여 온라인 및 오프라인 조정 알고리즘 사이에 간극을 만듭니다. 그럼에도 불구하고, 오프라인(또는 RL-이용하지 않는) 접근 방식도 이러한 온라인-오프라인 격차에도 불구하고 여전히 유효할 수 있습니다. 특히, 오프라인 알고리즘에 온-정책 데이터를 통합하여 강화하면, 완전히 온라인 RL 방식에 비해 효율적이고 구현이 용이한 준-온라인 알고리즘(semi-online algorithms)을 구축할 수 있습니다.

AI 연구의 최신 동향을 파악하기 위해 Deep (Learning) Focus를 구독하는 50,000명의 다른 전문가들과 함께하세요. 구독하기

### LLM을 위한 조정 기법

먼저, LLM 훈련에서 조정의 역할에 대해 간략히 논의하고, 현재 활용되는 다양한 온라인 및 오프라인 조정 알고리즘의 변형들을 설명하겠습니다.

오늘날의 LLM은 위에 제시된 그림처럼 여러 단계에 걸쳐 학습됩니다. LLM의 주요 훈련 절차는 다음과 같습니다:

*   **사전 훈련(Pretraining)**은 방대한 인터넷 규모의 텍스트 자료를 바탕으로 다음 토큰 예측(next token prediction) 목표(training objective)를 사용하여 LLM을 처음부터 학습시키는 대규모 과정입니다. 더 자세한 내용은 여기를 참조하십시오.
*   **지도 미세 조정(Supervised finetuning, SFT)** 또는 **명령 미세 조정(instruction finetuning, IFT)**은 (지도 방식의) 다음 토큰 예측 목표를 활용하여, LLM이 모방하도록 학습할 수 있는 소량의 고품질 결과물(completion) 집합에 대해 모델을 훈련합니다. 더 자세한 내용은 여기를 참조하십시오.
*   **인간 피드백 기반 강화 학습(Reinforcement learning from Human Feedback, RLHF)** 또는 **선호 미세 조정(Preference finetuning, PreFT)**은 강화 학습(RL) 원리를 적용하여 인간의 선호 데이터를 기반으로 LLM을 학습시킵니다. 더 자세한 내용은 여기를 참조하십시오.
*   **검증 가능한 보상 기반 강화 학습(Reinforcement Learning with Verifiable Rewards, RLVR)** 또는 **강화 미세 조정(reinforcement finetuning, RFT)**은 규칙 기반 또는 확정적 검증기(deterministic verifiers)에서 파생된 보상을 사용하여 RL로 LLM을 훈련합니다.

위에 설명된 훈련 전략들은 개별적인 단계로 분류될 수 있습니다. 아래 그림을 참조하십시오. 사전 훈련(및 중간 훈련(midtraining)) 과정은 LLM의 핵심 지식 기반을 구축하는 데 초점을 맞추는 반면, 조정은 LLM에 인간 선호도를 극대화하기 위한 적절한 형식과 스타일을 습득하게 합니다. 추론 훈련(Reasoning training)은 검증 가능한 과업에서 성능을 추가적으로 향상시키는 마지막 단계입니다.

<center>훈련 단계를 별개의 단계로 그룹화</center>

이 개요는 LLM 조정과 그를 위해 제안된 수많은 알고리즘(SFT 및 다양한 형태의 RL 기반 및 RL-이용하지 않는 RLHF 포함)에 중점을 둡니다. 특히, RLHF 훈련 과정에서 더 단순한 오프라인 조정 알고리즘을 사용하는 것과 대비하여 온라인 RL의 역할과 필요성에 초점을 맞출 것입니다. 이 섹션에서는 온라인 및 오프라인 알고리즘을 포함하여, 조정 알고리즘의 다양한 선택지를 설명하며 이 논의를 시작할 것입니다.

#### 지도 미세 조정(SFT)

<center>다음 토큰 예측 훈련 목표</center>

가장 단순한 LLM 조정 전략 중 하나는 사전 훈련 단계에서 사용된 것과 동일한 다음 토큰 예측 훈련 목표를 차용하는 지도 미세 조정(SFT)입니다. 우리는 LLM이 모든 이전 토큰을 맥락(context)으로 활용하여 일련의 토큰(sequence)에서 다음 토큰을 예측하도록 훈련합니다(위에 시각화됨). 이것은 대량의 비정형 텍스트 데이터에 효율적으로 적용될 수 있는 자기 지도 학습 목표(self-supervised training objective)입니다. 다음 토큰 예측 훈련 목표의 기본적인 구현 예시는 참고용으로 아래에 제공됩니다.

```python
import torch
import torch.nn.functional as F

# token_indices: (batch_size, seq_length)
logits = LLM(token_indices) # (batch_size, seq_length, vocab_size)

# shift to predict next token at each position
logits = logits[:, :-1, :] # (batch_size, seq_length - 1, vocab_size)
targets = token_indices[:, 1:] # (batch_size, seq_length - 1)

# resize tensors for cross-entropy loss
logits = logits.reshape(-1, logits.size(-1))
targets = targets.reshape(-1)

# compute cross-entropy loss
loss = F.cross_entropy(logits, targets)
```

사전 훈련 시에는 이 훈련 목표가 인터넷에서 수집된 방대한 텍스트 코퍼스(corpus)에 적용됩니다. 이와 대조적으로, SFT는 LLM을 조정하기 위해 더 적은 양의 고품질 프롬프트-응답 쌍(prompt-response pairs)으로 구성된 데이터셋을 선별(curating)하는 데 집중합니다. 예를 들어, LIMA는 단 1K개의 예시로 구성된 데이터셋을 활용하여 LLM을 조정하는 데 성공한 인기 있는 논문입니다. 최신 LLM들은 SFT 데이터셋에 더 많은 샘플을 사용하며, 예를 들어 Tulu-3은 대략 100만 개의 SFT 예시로 훈련됩니다. 요약하자면, SFT는 선호되는 응답의 구체적인 시연(demonstrations)을 통해 모델을 학습시킴으로써 LLM을 조정합니다. 대부분의 경우, SFT에서 완성 전용 손실(completion-only loss)을 적용하면 더 나은 성능을 달성할 수 있습니다. 이는 교차 엔트로피 손실(cross-entropy loss)이 모든 프롬프트 토큰에 대해 마스킹(masked)되고 응답 또는 완성 내의 토큰에만 적용된다는 것을 의미합니다.1 SFT에 대한 보다 상세한 설명은 아래 링크된 제 이전 개요를 참조하십시오.

<center>
<img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F6263e804-9457-4185-8737-142277d1217e_1000x500.png" alt="Understanding and Using Supervised Fine-Tuning (SFT) for Language Models">
</center>

<center>
<p>LLM에 가장 널리 사용되는 조정 기법 중 하나는 지도 미세 조정(SFT)으로, 표준 언어 모델링 목표(language modeling objective)를 활용하여 고품질 시연으로 구성된 선별된 데이터셋에 대해 모델을 훈련합니다. SFT는 사용이 간편하고 경제적이며, LLM을 조정하는 데 효과적인 도구입니다.</p>
<p>전체 이야기 읽기</p>
</center>

**거부 샘플링(Rejection sampling)**은 SFT의 온라인 변형으로, 매우 효과적이고 사용하기 용이합니다. SFT의 표준 공식은 오프라인 방식입니다. 즉, 고정된 프롬프트-응답 쌍 데이터셋을 가지고 모델을 훈련합니다. 거부 샘플링은 다음 단계를 통해 이 설정을 변경합니다.

*   프롬프트 데이터셋을 준비합니다.
*   현재 LLM을 이용하여 각 프롬프트에 대한 결과물을 생성합니다.
*   보상 모델(reward model) 또는 LLM 심사관(LLM judge)을 활용하여 이 모든 결과물을 평가합니다.
*   가장 높은 점수를 받은 프롬프트-결과물 쌍을 선정(또는 필터링)합니다.2
*   선정된 상위 예시들을 바탕으로 SFT를 수행합니다.

거부 샘플링 과정은 아래에 도식화되어 있습니다. 이 접근 방식은 SFT와 유사한 방식으로 LLM을 훈련하지만, 차이점은 데이터에 있습니다. 우리는 LLM 자체를 활용하여 준-온라인(semi-online) 방식으로 SFT 훈련 데이터를 샘플링합니다. 보상 모델은 우리가 최고 품질의 결과물에 대해 훈련하고 있는지 확인하는 데 사용됩니다.

<center>
<img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F6710408d-8356-4318-8798-2305597950c4_1000x500.png" alt="Rejection sampling process">
</center>
<center>(출처: RLHF book, 라이선스)</center>

우리는 일반적으로 거부 샘플링을 반복적으로 수행합니다. 예를 들어, Llama-2의 조정 과정은 RL 기반 RLHF를 진행하기 전에 네 번의 거부 샘플링 주기를 활용합니다. 위 논의에서 우리는 거부 샘플링을 SFT의 변형으로 설명했는데, 이는 두 기법이 동일한 훈련 목표를 공유하기 때문입니다. 그러나 거부 샘플링은 실제로는 선호 튜닝(preference tuning) 기술이며, SFT의 대안이라기보다는 RLHF의 더 간소화된 대안으로 가장 자주 사용됩니다. 실제로 거부 샘플링은 SFT 대신, SFT 이후에 적용되는 경우가 많습니다.

<center>
<img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F6263e804-9457-4185-8737-142277d1217e_1000x500.png" alt="SFT variants">
</center>
<center>(출처: [13]) SFT 변형.</center>

거부 샘플링(Best-of-N 샘플링이라고도 불림) 외에도, 여러 온라인 또는 반복적인 SFT 변형들이 제안되었습니다. 이 개요에서 다룰 주목할 만한 예시들은 다음과 같습니다.

*   **인간 피드백 기반 지도 반복 학습(Supervised Iterative Learning from Human Feedback, SuperHF)** [13]은 모델에서 온-정책 출력(on-policy outputs) 배치를 샘플링하고, 보상 모델로 이 출력을 필터링하며, KL 발산(KL divergence) 제약 조건(constraint) 하에 지도 목표(supervised objective)를 사용하여 모델을 최적화하는 온라인 학습 기법입니다. 위 내용을 참조하십시오.
*   **강화된 자기 훈련(Reinforced Self-Training, ReST)** [14]은 위에 설명된 거부 샘플링 공식을 활용합니다. 이 공식에서는 LLM으로부터 온-정책 데이터를 반복적으로 샘플링하고, 각 샘플을 보상 모델로 평가하며, 가장 우수한 샘플에 대해 훈련을 진행합니다.
*   **보상 가중 회귀(Reward-Weighted Regression, RWR)** [15] 역시 LLM을 사용하여 보상 모델로 평가되는 온-정책 샘플을 생성합니다. 그러나 이 점수는 필터링 대신, 훈련 손실(training loss)에서 각 샘플에 가중치를 부여하는 데 사용됩니다.
*   **보상 순위 미세 조정(Reward Ranked Finetuning, RAFT)** [16]는 다시 LLM으로부터 온라인 결과물을 샘플링하고, 보상 모델의 점수를 활용하여 SFT에 사용할 결과물을 필터링하는 표준 거부 샘플링 설정을 채택합니다.

#### 강화 학습(RL) 훈련

<center>
<img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F6263e804-9457-4185-8737-142277d1217e_1000x500.png" alt="Reinforcement Learning (RL) Training">
</center>
<center>(출처: [16])</center>

LLM을 훈련하는 데 일반적으로 사용되는 두 가지 유형의 강화 학습(RL) 훈련이 있습니다(위에 제시됨).

*   **인간 피드백 기반 강화 학습(Reinforcement Learning from Human Feedback, RLHF)**은 인간 선호 보상 모델(human preference reward model)에서 파생된 보상을 사용하여 RL 방식으로 LLM을 훈련합니다.
*   **검증 가능한 보상 기반 강화 학습(Reinforcement Learning with Verifiable Rewards, RLVR)**은 규칙 기반 또는 확정적 검증기(deterministic verifiers)에서 파생된 보상을 사용하여 RL 방식으로 LLM을 훈련합니다.

이러한 RL 훈련 기술들은 주로 훈련을 위한 보상을 도출하는 방식에서 차이를 보이지만, 알고리즘의 다른 세부 사항들은 대체로 유사합니다. 아래에 도식화된 바와 같이, 두 방식 모두 일련의 프롬프트에 대한 결과물을 생성하고, 이 결과물에 대한 보상을 산출하며, 이 보상을 활용하여 정책 업데이트(policy update) 또는 RL 최적화기(optimizer)를 통한 LLM 매개변수(parameters) 업데이트를 유도합니다.

<center>
<img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F6263e804-9457-4185-8737-142277d1217e_1000x500.png" alt="Visual walkthrough of RL training for LLMs">
</center>
<center>LLM을 위한 RL 훈련 시각적 안내</center>

LLM을 RL로 최적화할 때, 우리는 아래에 제시된 목표를 달성하고자 합니다. 이 목표는 LLM 결과물에서 얻은 보상을 극대화하는 동시에, 참조 모델(reference model)에 대한 모델의 KL 발산(KL divergence)을 최소화하는 것입니다. 참조 모델은 일반적으로 RL 훈련 시작 시점의 LLM 체크포인트(checkpoint)입니다. 간단히 말해, 이는 새로운 모델이 원래(참조) 모델과 크게 다르지 않으면서도 보상을 최대한으로 높이기를 바란다는 의미입니다.

<center>
<img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F6263e804-9457-4185-8737-142277d1217e_1000x500.png" alt="RL training objective">
</center>
<center>RL 훈련 목표</center>

**온-정책 샘플링(On-policy sampling).** 위에 제시된 바와 같이, RL로 LLM을 훈련할 때 온-정책 샘플링을 수행합니다. "온-정책" 샘플링이란 핵심 RL 훈련 루프(training loop) 내에서 LLM을 훈련하는 데 사용되는 결과물이 LLM 자체에 의해 실시간으로 생성된다는 것을 의미합니다. 즉, 결과물은 다른 모델에 의해 생성되거나 오프라인의, 사전에 계산된 데이터셋에 저장되지 않습니다. LLM의 맥락에서 온-정책 샘플링을 활용하는 훈련 알고리즘은 일반적으로 "온라인" 훈련 알고리즘이라고 지칭됩니다. 온-정책 샘플링은 RL 훈련의 맥락에서만 사용되는 것이 아닙니다. 예를 들어, 우리는 이전 섹션에서 SFT의 여러 온라인 변형에 대해 알아보았습니다.

**RLHF에 대해 더 자세히.** 이 개요는 LLM 조정에 초점을 맞추고 있으므로, 주로 RLHF 방식의 훈련을 다룰 것입니다. LLM 조정에 대한 초기 접근 방식은 SFT와 RLHF를 결합한 3단계 기법(아래에 제시됨)을 활용했습니다.

<center>
<img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F6263e804-9457-4185-8737-142277d1217e_1000x500.png" alt="RLHF three-stage technique">
</center>
<center>(출처: [7])</center>

RLHF에서 우리는 각 선호 쌍(preference pair)이 다음을 포함하는 선호 쌍 데이터셋을 수집하는 것으로 시작합니다.

*   프롬프트.
*   선택된(또는 승리한) 결과물.
*   거부된(또는 패배한) 결과물.

그 후, 선호 데이터셋에 대해 보상 모델을 훈련하고 위에 설명된 RL 훈련 루프를 통해 LLM을 최적화합니다. 이 선호 데이터셋의 결과물은 다양한 출처에서 나올 수 있습니다. 예를 들어, 참조 모델, 이전 모델 체크포인트, 또는 완전히 다른 모델에서 나올 수도 있습니다. 선호 주석(preference annotation) 또는 쌍에서 선택된 결과물 및 거부된 결과물의 선택은 일반적으로 인간 주석자(human annotator) 또는 LLM 심사관(즉, AI 피드백)에 의해 제공됩니다. 특히, 선호 데이터와 보상 모델은 RL 훈련 시작 시점에 고정됩니다. 이를 좀 더 공식적으로 표현하자면, LLM은 오프라인 모델 기반 RL(offline model-based RL)의 변형으로 훈련됩니다.

**RL 최적화기(RL optimizers).** 위 RL 훈련 설명에서 빠진 한 가지 세부 사항은 정책 업데이트(policy update)를 어떻게 계산하는가입니다. 여기서는 이 질문에 간략하게 답하겠지만, 관심 있는 독자는 전체 세부 사항을 위해 이 심층 개요를 참조해야 합니다. 일반적으로 정책 경사(policy gradient) 기반 RL 최적화기(예: REINFORCE, PPO, 또는 GRPO)가 사용됩니다. PPO 기반 RLHF는 과거에 사실상의 선택이었지만, PPO는 LLM으로 가치 함수(value function)를 추정하기 때문에 계산 비용이 많이 듭니다. 실제로 PPO 기반 RLHF는 훈련 중에 LLM의 네 가지 다른 복사본(즉, 정책, 참조 정책, 가치 모델, 보상 모델)을 저장합니다. 오버헤드를 줄이기 위해 REINFORCE는 훈련 내내 모델이 받은 보상의 평균으로 가치 함수를 근사하여 정책 경사의 몬테카를로 추정(Monte Carlo estimate)을 도출합니다(즉, LLM 대신). 유사하게, GRPO는 동일한 프롬프트에 대한 여러 결과물의 보상 평균으로 가치 함수를 근사합니다. 이를 그룹(group)이라고 합니다. GRPO는 RLVR에 가장 일반적인 RL 최적화기이므로 보상 모델 없이도 일반적으로 사용됩니다. 이 경우 RL 훈련을 위해 LLM의 두 가지 복사본(정책 및 참조 정책)만 저장합니다. 그러나 보상 모델의 부재는 RLVR의 부산물입니다(즉, GRPO는 보상 모델 유무에 관계없이 사용할 수 있습니다).

#### 직접 조정 기법(Direct Alignment Techniques)

<center>
<img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F6263e804-9457-4185-8737-142277d1217e_1000x500.png" alt="Direct Alignment Techniques">
</center>
<center>(출처: [18])</center>

온라인 RL 훈련이 매우 비싸기 때문에, 연구자들은 직접 선호 최적화(direct preference optimization, DPO) [18]와 같은 오프라인 조정 기법도 제안했습니다. PPO 기반 RLHF와 비교하여 DPO는 명시적인 보상 모델(explicit reward model) 훈련을 피하고, 대신 LLM 자체에서 암묵적으로 보상 신호(reward signal)를 도출합니다. 이 암묵적 보상을 사용하여 LLM은 아래에 제시된 대조 학습 목표(contrastive learning objective)로 훈련되며, 이는 표준 경사 하강법(gradient descent)으로 최적화될 수 있습니다(즉, RL 훈련 없이).

<center>
<img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F6263e804-9457-4185-8737-142277d1217e_1000x500.png" alt="DPO training loss">
</center>
<center>(출처: [18]) DPO 훈련 손실</center>

직관적으로, 이 대조 손실(contrastive loss)은 선호 데이터셋에서 선택된 응답과 거부된 응답 간의 확률 마진(probability margin)을 증가시킵니다. LLM은 고정된 선호 데이터셋(RLHF에서 보상 모델을 훈련하는 데 사용되는 것과 동일한 데이터)으로 훈련됩니다. 이러한 이유로 DPO는 오프라인(즉, 훈련 데이터가 고정되어 있고 온-정책 샘플링이 없음) 직접 조정 알고리즘(direct alignment algorithm)으로 특징지어집니다. RL 기반 조정 알고리즘과 비교하여 DPO는 훨씬 적은 계산 오버헤드를 필요로 하고, 튜닝하기 쉬우며, 여전히 좋은 성능을 보이는 경향이 있습니다. 자세한 내용은 아래를 참조하십시오.

<center>
<img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F6263e804-9457-4185-8737-142277d1217e_1000x500.png" alt="Direct Preference Optimization (DPO)">
</center>

<center>
<p>RLHF와 같은 조정 기술은 LLM 품질을 크게 향상시켰지만, 계산 비용이 많이 들고 사용하기 어렵습니다. 이 개요는 경사 하강법으로 최적화할 수 있는 목표를 사용하여 LLM을 조정함으로써 이러한 복잡성을 피하는 DPO라는 더 간단한 LLM 조정 접근 방식을 다룹니다.</p>
<p>전체 이야기 읽기</p>
</center>

**DPO의 변형.** DPO는 PPO 기반 RLHF에 비해 사용하기 훨씬 간단했기 때문에 이 기술은 LLM 연구에서 빠르게 인기를 얻었습니다. 그 결과, 항등 선호 최적화(Identity Preference Optimization, IPO) [8], 카네만-트버스키 최적화(Kahneman-Tversky Optimization, KTO) [19], 또는 대조 선호 최적화(Contrastive Preference Optimization, CPO) [20]와 같은 DPO의 많은 변형이 제안되었습니다. 이러한 기술 중 다수는 DPO에 약간의 수정을 가하여 성능을 약간 향상시키지만, 대조 목표를 사용한 직접 조정이라는 핵심 아이디어는 유사합니다. 그러나 이러한 기술 중 일부는 DPO와 의미론적으로 다릅니다. 예를 들어, KTO는 선호 쌍(preference pair)이 아닌 이진(좋음 또는 나쁨) 등급이 있는 단일 완성에 적용될 수 있는 DPO 스타일 손실을 공식화합니다.

**온라인 또는 반복 DPO.** 표준 공식에서 DPO는 완전히 오프라인 조정 알고리즘입니다. DPO 훈련 내내 선호 데이터셋은 고정되어 있지만, 훈련 과정에 온-정책 샘플을 도입함으로써 온라인(또는 준-온라인) DPO 변형을 만들 수 있습니다. 아래에 묘사된 바와 같이, 이 아이디어의 한 예는 자기 보상 언어 모델(self-rewarding language models) [10]입니다. 이 프레임워크에서 우리는 다음과 같이 DPO 훈련을 위한 새로운 데이터를 주기적으로 샘플링합니다.

*   일련의 프롬프트로 시작합니다.
*   현재 LLM으로 이 프롬프트에 대한 여러 결과물을 샘플링합니다.
*   이 결과물을 순위 매겨(예: LLM 심사관 또는 보상 모델 사용) 선호 데이터셋을 생성합니다.
*   위에 설명된 DPO를 사용하여 이 데이터에 대해 LLM을 훈련합니다.
*   1단계로 돌아가 여러 라운드 동안 반복합니다.

이 과정에서 우리는 DPO로 모델을 반복적으로 훈련하지만, 훈련 데이터는 현재 정책에서 주기적으로 다시 샘플링됩니다. 이는 준-온라인 훈련 설정(semi-online training setup)입니다. 현재 정책에서 결과물을 더 자주 샘플링함으로써 이 접근 방식을 더 온-정책으로 만들 수 있습니다. 사실, 모든 훈련 데이터 배치에 대해 온-정책 결과물을 샘플링함으로써 완전한 온라인 DPO 변형을 만들 수도 있습니다!

<center>
<img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F6263e804-9457-4185-8737-142277d1217e_1000x500.png" alt="Self-rewarding language models">
</center>
<center>(출처: [10])</center>

### 온라인-오프라인 성능 간극

PPO 기반 RLHF는 한동안 LLM 조정의 표준적인 선택이었지만, 이 방식은 비용이 많이 들고 복잡하며, 최고 수준의 LLM 연구 기관 밖에서는 재현하기 어렵습니다. 그 결과, 연구자들은 오프라인 및 RL-이용하지 않는 훈련 전략에 기반한 다양한 더 간단한 조정 알고리즘들을 개발했습니다. 이 섹션에서는 다음 질문에 답하는 것을 목표로 합니다: 오프라인 조정 기술을 사용하는 것이 성능 저하를 수반하는가? 이를 해결하기 위해 오프라인 훈련, 온-정책 샘플 사용, 대조 훈련 목표 및 LLM 성능에 미치는 기타 요인의 영향을 연구하는 다양한 논문들을 검토할 것입니다.

#### LLM 조정에서 DPO가 PPO보다 우월한가? 포괄적 연구 [6]

"실험 결과는 PPO가 모든 측면에서 다른 조정 방식들을 능가할 수 있음을 입증합니다... 특히 가장 어려운 코드 경쟁 과업에서 PPO는 최첨단 성과를 달성합니다." - 출처: [6]

[6] 연구에서는 PPO 기반 RLHF와 (오프라인) DPO를 비교하기 위해 이론적 분석, 합성 실험(synthetic experiments), 그리고 실제 LLM 훈련을 포함한 여러 가지 접근 방식을 활용합니다. 이 연구의 목적은 LLM 조정에서 DPO의 한계를 찾아내고 설명하는 것입니다. 먼저, 저자들은 DPO와 PPO 기반 RLHF 사이에 성능 차이가 존재함을 확인합니다. 이어서, 그들은 이러한 경향의 주요 원인(DPO의 성능은 기본 선호 데이터셋에 분포 외(out-of-distribution) 예시가 존재함에 따라 크게 영향을 받음)을 밝히는 분석을 제시합니다.

**보상 해킹(Reward hacking).** PPO 기반 RLHF로 LLM을 훈련할 때, 우리는 프롬프트 데이터셋에 포함된 질의에 대한 결과물을 온라인 방식으로 생성하고 보상 모델로 평가합니다. 보상 모델은 고정된(그리고 편향된) 선호 데이터셋에 대해 훈련된 LLM이라는 점을 감안할 때, 이 모델은 실제의, 진실값(ground-truth) 보상에 대한 불완전한 대리자(proxy) 역할을 합니다. 즉, 제공하는 점수에서 오류를 범할 수 있습니다! 더 나아가, PPO에 의해 훈련되는 LLM은 인간의 선호 기대치를 실제로 충족시키지 못하면서도, 보상 모델이 제공하는 보상을 잘못 최대화하는 방법을 찾아 이러한 오류를 악용하는 방식을 학습할 수도 있습니다. 일반적으로 "보상 해킹"이라고 불리는 이 현상은 강화 학습 문헌에서 오랜 연구 역사를 가지고 있습니다. 그러나 [6]에서는 DPO와 같은 RL-이용하지 않는, 오프라인 조정 알고리즘을 사용할 때도 유사한 문제가 발생할 수 있음을 보여줍니다. 특히, 저자들은 아래에 인용된 진술을 하는데, 이는 다음을 알려줍니다.

*   PPO가 찾아낸 모든 해법은 DPO의 훈련 목표 또한 최소화합니다(즉, PPO 해법 집합은 DPO 해법 집합의 부분집합입니다).
*   PPO는 잘못된(또는 보상 해킹된) 해법을 찾아낼 수 있습니다.
*   따라서 동일한 잘못된 해법은 DPO를 통해서도 발견될 수 있습니다.

<center>
<img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F6263e804-9457-4185-8737-142277d1217e_1000x500.png" alt="PPO vs DPO solutions">
</center>

<center>
<p>지상 진실 보상 r과 선호 데이터셋 D가 주어졌을 때, Π_PPO를 D에 대해 보상 모델 R_Φ를 훈련하고 PPO를 실행하여 유도된 정책 클래스라고 하자. Π_DPO를 DPO를 실행하여 유도된 정책 클래스라고 하자. 우리는 다음 결론을 얻는다:</p>
<p>Π_PPO는 Π_DPO의 적절한 부분 집합이다.</p>
<p>- 출처: [6]</p>
</center>

명시적인 보상 모델을 활용하지 않기 때문에, DPO는 PPO와 동일한 방식으로 보상 해킹될 수는 없습니다. 그러나 DPO는 다른 방식으로 분포 외 데이터와 유사한 문제에 직면합니다. 구체적으로, DPO는 아래에 설명된 바와 같이 보이지 않는(또는 분포 외) 결과물에 대한 편향(bias)을 학습합니다.

<center>
<p>"DPO는 보이지 않는 응답을 선호하는 편향된 분포를 개발할 수 있으며, 이는 학습된 정책의 품질에 직접적인 영향을 미칩니다... DPO는 분포 외 응답을 선호하는 편향된 정책을 생성하기 쉽고, 예측할 수 없는 행동으로 이어집니다." - 출처: [6]</p>
</center>

이 편향은 DPO에서 활용되는 참조 모델과 선호 데이터셋 내에서 결과물을 생성하는 데 사용되는 모델 사이에 현저한 분포 변화(distribution shift)가 있을 때 가장 두드러집니다. 이상적으로는 이러한 결과물들이 DPO에서 사용되는 참조 모델로 생성되어야 합니다. PPO와 같은 온라인 알고리즘은 훈련 과정에서 온-정책 결과물을 생성하는 반면, DPO와 같은 오프라인 알고리즘은 고정된 선호 데이터셋에 대해 훈련되며, 여기서 결과물은 임의의 LLM에서 비롯될 수 있습니다.

**합성 예시(Synthetic example).** 분포 외 데이터에 대한 DPO의 문제를 검증하기 위해 [6] 연구팀은 간단한 합성 훈련 예시를 구성합니다. 이 환경에서 정책은 원-핫 벡터(one-hot vector)를 입력(즉, 프롬프트)으로 받아들이고, 8차원 범주형 분포(categorical distribution)를 출력으로 생성하는 기본적인 다층 퍼셉트론(multi-layer perceptron)입니다.3 우리는 아래 그림에 묘사된 바와 같이 최적 정책(optimal policy)이 대각선 형태라고 가정합니다.

<center>
<img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F6263e804-9457-4185-8737-142277d1217e_1000x500.png" alt="Optimal policy is diagonal">
</center>
<center>(출처: [6])</center>

이 장난감 설정(toy setup)을 사용하여 훈련 데이터에서 특정 선호 쌍을 의도적으로 생략하는 합성 선호 데이터셋을 생성할 수 있으며, 이를 통해 분포 외 데이터 처리에서 DPO와 PPO의 동작을 모두 테스트할 수 있습니다. 위에 제시된 바와 같이, PPO는 이 커버리지(coverage) 문제를 올바르게 처리하고 최적 정책을 복구합니다. 이와 대조적으로, DPO는 분포 외 데이터에 높은 확률을 할당하도록 잘못 학습하며, 이는 [6]의 주장을(적어도 작은 규모에서는) 검증합니다. 즉, DPO는 선호 데이터셋의 분포 외 데이터에 대해 잘못된 편향을 개발합니다.

**실제 실험.** 이 합성 테스트에 이어, SafeRLHF 데이터셋을 활용하여 다양한 Llama-2 파생 LLM을 대상으로 대규모 선호 튜닝 실험이 수행됩니다. 실험은 Alpaca 데이터셋에 대해 훈련된 SFT 모델을 시작점으로 삼아, SFT 모델과 선호 데이터 사이에 분포 변화를 의도적으로 만듭니다. Alpaca 데이터셋의 결과물은 SafeRLHF의 결과물과 현저히 다릅니다.

<center>
<img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F6263e804-9457-4185-8737-142277d1217e_1000x500.png" alt="Alpaca SFT model performance">
</center>
<center>(출처: [6])</center>

위에 제시된 바와 같이, Alpaca SFT 모델을 DPO의 시작점으로 직접 활용하는 것은 저조한 성능을 보입니다. 그러나 DPO 훈련을 수행하기 전에 SafeRLHF 데이터셋의 선호 결과물에 대해 Alpaca SFT 모델을 먼저 미세 조정하면 성능이 크게 향상됩니다. 이러한 결과는 참조 모델과 DPO의 선호 데이터 사이의 분포 변화가 실제 조정 시나리오에서 LLM 성능에 실제로 해롭다는 것을 시사합니다. 특히, DPO 이전에 선호 데이터셋의 선호 결과물에 대해 추가 SFT를 실행하는 접근 방식은 원래 DPO 논문 [1]에서도 권장되었습니다!

<center>
<p>"우리는 SFT (Safe)로 새로운 응답을 생성하고 학습된 보상 모델을 선호 레이블링(preference labeling)에 사용합니다. 이 과정을 반복하고 마지막 반복에서 최신 DPO 모델을 참조 모델로 반복적으로 설정합니다." - 출처: [6]</p>
</center>

[6] 연구에서는 반복 DPO를 통해 분포 외 데이터를 회피하는 새로운 접근 방식도 제안됩니다. 우리는 여러 라운드의 DPO를 실행할 수 있으며, 각 라운드에서 현재 참조 정책을 활용하여 새로운 결과물을 생성하고, 보상 모델에 의해 자동 평가되어 선호 데이터셋을 구축합니다. 각 라운드가 완료된 후 현재 정책이 새로운 참조 정책이 되며, 이 과정을 반복하여 참조 정책과 선호 데이터셋 사이에 분포 변화가 없도록 합니다. 이 접근 방식을 사용하면 PPO로 얻은 것과 유사한 안전성(그러나 유용성은 아님) 등급을 가진 모델을 훈련할 수 있으며, 따라서 온라인 및 오프라인 조정 알고리즘 간의 성능 간극을 좁힐 수 있습니다。

#### LLM의 선호 미세 조정은 최적 이하의 온-정책 데이터를 활용해야 한다 [7]

[7]의 저자들은 LLM을 위한 거의 모든 가능한 조정 전략들을 다루는 포괄적인 연구를 수행함으로써, 성공적인 조정 알고리즘을 구성하는 두 가지 핵심 특성을 밝혀냅니다.

*   온-정책 샘플링의 활용.
*   부정적인 응답의 확률을 줄이는 "음의 경사(negative gradient)"의 존재; 즉, 긍정적인 응답의 확률만 높이는 대신.

예를 들어, SFT는 고품질 결과물 집합에 대해 최대 우도(maximum likelihood) 목표만을 사용하여 LLM을 순수하게 훈련하는 반면, DPO는 (i) 선택된 응답의 확률을 높이고 (ii) 거부된 응답의 확률을 낮추는 대조 목표를 활용합니다. 그러나 이 두 전략 모두 훈련 데이터가 고정되어 있습니다. 즉, 온-정책 샘플링을 수행하지 않습니다. 우리는 PPO와 같은 온라인 RL 알고리즘을 사용하거나, 현재 정책에서 새로운 데이터를 주기적으로 샘플링하는 반복 DPO 전략을 채택함으로써 이러한 문제를 해결할 수 있습니다.

<center>
<p>"우리의 주요 발견은 일반적으로 온-정책 샘플링을 사용하거나 특정 응답의 우도(likelihood)를 낮추려는 접근 방식이 오프라인 및 최대 우도 목표보다 성능이 우수하다는 것입니다." - 출처: [7]</p>
</center>

[7] 연구에서는 또한 온-정책 샘플링과 음의 경사가 가장 유용하게 작용하는 경우가 어려운 조정 사례, 즉 높은 보상을 받는 응답이 참조 정책 내에서 발생할 가능성이 낮은 경우임을 발견합니다. 이러한 경우, 조정 과정은 낮은 보상 응답에서 높은 보상 응답으로 확률 질량(probability mass)을 "이동"시킴으로써 LLM을 훈련해야 합니다. 오프라인 및 순수 지도 조정 방법은 이러한 복잡한 시나리오에서 특히 저조한 성능을 보입니다.

**조정 알고리즘.** [5]의 저자들은 온-정책 샘플링, 음의 경사, 그리고 샘플 재사용(sample reuse, 즉 동일한 데이터에 대해 여러 경사 업데이트를 수행하는 것) 사용을 기반으로 광범위한 잠재적 조정 알고리즘(아래에 제시됨)을 특성화하는 것으로 논의를 시작합니다. 샘플 재사용의 구체적인 예로, PPO는 각 훈련 데이터 배치에 대해 2~4개의 순차적 경사 업데이트를 실행하는 반면, GRPO4 및 REINFORCE는 일반적으로 이러한 샘플 재사용을 피합니다 [9].

<center>
<img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F6263e804-9457-4185-8737-142277d1217e_1000x500.png" alt="Alignment algorithms">
</center>
<center>(출처: [7])</center>

모든 SFT 및 거부 샘플링 변형은 RL 기반 및 직접 조정 방법에서 발견되는 음의 경사를 가지고 있지 않습니다. 이 방법에서는 거부된(직접 조정의 경우) 또는 낮은 보상을 받는(RL의 경우) 응답의 확률을 명시적으로 감소시킵니다. 마지막으로, 온-정책 샘플링은 훈련 설정에 따라 대부분의 기술에서 사용될 수도 있고 사용되지 않을 수도 있습니다。 DPO 또는 IPO와 같은 직접 조정 방법은 온-정책 샘플링 없이 고정된 선호 데이터셋에 대해 대조 훈련을 실행하지만, 현재 정책에서 새로운 훈련 데이터를 주기적으로 샘플링함으로써 오프라인 알고리즘의 온라인 버전을 만들 수 있습니다. 그러나 PPO 및 REINFORCE와 같은 일부 알고리즘은 자연스럽게 온-정책 샘플링을 기반으로 합니다.

**통합 조정 알고리즘(Unified alignment algorithm).** 가능한 조정 알고리즘의 범위를 포착하기 위해 [7]의 저자들은 아래에 제시된 프레임워크를 만듭니다. 이 프레임워크는 기본 조정 알고리즘 내의 다양한 설정을 체계적으로 연구할 수 있도록 합니다. 예를 들어, 1단계와 2단계는 다음 중 하나로 수행될 수 있습니다.

*   온-정책 데이터 수집(즉, 현재 정책에서 응답을 생성하고 보상 모델로 자동으로 평가).
*   온-정책 샘플링 없이 오프라인 선호 데이터를 직접 사용(예: 표준 DPO에서와 같이).

더 나아가, 총 샘플 수 B를 변경하거나 일련의 샘플에 대해 수행되는 총 경사 단계 T를 변경하여 온-정책 샘플링의 정도를 다양하게 할 수 있습니다. 특히, T를 증가시키면 샘플 재사용이 도입되는 반면, B를 증가시켜도 그렇지 않으므로 온-정책 샘플 재사용의 영향을 분리할 수 있습니다.

<center>
<img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F6263e804-9457-4185-8737-142277d1217e_1000x500.png" alt="Unified alignment algorithm framework">
</center>
<center>(출처: [7])</center>

특히, 이 통합 알고리즘은 최대 우도 조정 알고리즘을 포착하지 않지만, 이 알고리즘들은 [7]에서도 여전히 고려됩니다.

**훈련 설정.** 이러한 다양한 조정 알고리즘의 속성은 다음을 포함한 여러 실험 설정을 사용하여 분석됩니다.

*   소규모(교훈적) 밴딧 문제(bandit problems).
*   합성 LLM 문제.
*   전체 규모 LLM 조정.

합성 조정 시나리오에서는 LLM 응답의 길이를 기반으로 수작업으로 만든 보상을 사용합니다. 구체적으로, 두 가지 보상 설정이 고려됩니다. 응답 길이 최소화와 평균 응답 길이 일치입니다. 아래를 참조하세요. 이러한 보상 시나리오는 높은 보상 응답이 참조 정책의 가능한 완성 영역 내외에 모두 있는 경우를 테스트합니다.5

<center>
<img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F6263e804-9457-4185-8737-142277d1217e_1000x500.png" alt="Reward settings">
</center>
<center>(출처: [7])</center>

교훈적 밴딧 문제(didactic bandit problems)도 보상 함수의 최적값을 변경하는 여러 보상 설정을 테스트합니다. 보상 설정을 변경함으로써, 우리는 각 알고리즘이 원래 참조 정책에서 확률이 낮더라도 높은 보상 응답에 확률을 할당하는 능력을 테스트합니다. 위를 참조하세요.

<center>
<p>"보상 함수 R1의 최적값은 참조 정책의 낮은 우도 영역에 위치하는 반면, R2의 최적값은 참조 정책의 모드(mode)와 대략적으로 일치합니다. 우리는 온-정책 샘플링이 보상 함수 R1을 최적화하는 데 중요할 것이며, 오프라인 또는 최대 우도 방법은 R2의 최적화에 충분할 수 있다고 가정합니다." - [7]의 밴딧 문제 설명</p>
</center>

전체 규모 조정 시나리오는 AlpacaFarm, UltraChat 및 UltraFeedback의 공개 선호 데이터를 사용하여 Pythia-1.4B 및 Mistral-7B와 같은 소규모 LLM을 조정합니다. 이 훈련 설정은 더 표준적인 LLM 조정 시나리오이며, 모델은 황금 인간 선호 보상 모델(golden human preference reward model)을 사용하여 평가됩니다.

**온-정책 샘플링의 역할.** [7]의 실험에서 우리는 온-정책 데이터를 더 자주, 더 작은 배치로 샘플링하는 것(가능한 가장 엄격한 온-정책 설정)이 최고의 성능으로 이어진다는 것을 알 수 있습니다. 온-정책 샘플링의 영향은 높은 보상 응답이 참조 정책의 가능한 영역 내에 이미 있지 않은 복잡한 조정 시나리오에서 가장 두드러집니다.

<center>
<p>"[우리는] 더 작지만 자주 샘플링되는 배치로 온-정책 샘플링이 더 나은 성능을 가져온다는 강력하고 명확한 경향을 관찰합니다... 오프-정책(off-policy) 업데이트가 많을수록 R2의 성능 저하는 상당히 완만합니다. 이는 보상 함수의 피크가 참조 정책의 가능성 있는 영역에 있을 때 더 높은 정도의 오프-정책 업데이트가 허용된다는 것을 나타냅니다." - 출처: [7]</p>
</center>

높은 보상을 받는 응답이 참조 정책 내에서 이미 가능성이 있는 더 간단한 조정 사례에서는 모델이 오프라인 훈련 알고리즘 사용을 더 잘 견딜 수 있습니다. 이 현상은 합성 및 교훈적 문제 설정 모두에서 확인됩니다. 또한, 전체 규모 LLM 조정 실험에서도 동일한 경향을 관찰할 수 있습니다. 여기서 가장 높은 보상은 훈련 과정을 더 온-정책으로 만들기 위해 배치 크기 B를 줄이는 것에서 나옵니다. 아래를 참조하세요.

<center>
<img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F6263e804-9457-4185-8737-142277d1217e_1000x500.png" alt="Batch size B vs performance">
</center>
<center>(출처: [7])</center>

**음의 경사.** 온-정책 샘플링과 유사하게, 음의 경사 사용은 조정에 도움이 되는 것으로 밝혀졌습니다. 음의 경사를 사용하는 알고리즘은 그렇지 않은 알고리즘에 비해 성능이 눈에 띄게 향상되며, 특히 원래 참조 정책에 의해 낮은 확률이 할당된 응답의 확률을 증가시켜야 하는 어려운 조정 사례에서 더욱 그렇습니다. 아래(상단 그림)에 제시된 바와 같이, 음의 경사를 사용하는 알고리즘은 훈련 중에 선택된 응답과 거부된 응답 간의 확률 마진을 증가시킵니다. 음의 경사가 없는 알고리즘에서는 이러한 경향이 관찰되지 않습니다.

<center>
<img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F6263e804-9457-4185-8737-142277d1217e_1000x500.png" alt="Negative gradient impact">
</center>
<center>(출처: [7])</center>

그러나 흥미롭게도 위(하단 그래프)에서 마진이 증가함에도 불구하고 훈련 중에 선택된 응답과 거부된 응답의 절대 확률이 실제로 감소하는 것을 볼 수 있습니다. 이와 동일한 경향은 다른 논문 [8]에서도 관찰되었습니다.

<center>
<img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F6263e804-9457-4185-8737-142277d1217e_1000x500.png" alt="Absolute probability of chosen and rejected responses">
</center>
<center>(출처: [7])</center>

온-정책 샘플링과 음의 경사는 함께 사용될 때 복합적인 이점을 제공합니다. 예를 들어, 온-정책 IPO 및 DPO는 교훈적 밴딧 및 합성 LLM 실험 모두에서 오프라인 변형에 비해 더 빠른 수렴과 더 나은 성능을 보입니다. 위를 참조하세요. 전체 규모 LLM 실험에서 대조 조정 알고리즘의 온라인 버전은 일부 경우 PPO보다 낮은 계산 비용과 실제 훈련 시간에도 불구하고 PPO보다 성능이 우수합니다.

**샘플 재사용은 해로운가?** T 값을 상당히 증가시키면 훈련 과정에 오프-정책 데이터(off-policy data)가 도입되어 성능이 사소하게 저하될 것입니다. 그러나 적당한 T 설정은 모델이 성능의 큰 저하 없이 오프-정책 업데이트를 훈련 과정에 통합할 수 있도록 합니다. 예를 들어, PPO를 사용한 합성 LLM 설정에서는 T를 1에서 8로 증가시켜도 성능 저하가 눈에 띄지 않습니다. 아래를 참조하세요.

<center>
<img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F6263e804-9457-4185-8737-142277d1217e_1000x500.png" alt="Sample reuse impact">
</center>
<center>(출처: [7])</center>

거부 샘플링(위 그림에서는 Best-of-N이라고 함)과 같은 최대 우도 훈련 목표는 샘플 재사용에 더 민감하지만6, 적당한 T 설정으로도 좋은 결과를 얻을 수 있습니다. 간단히 말해, 이러한 결과는 샘플 재사용으로 인한 오프-정책 업데이트가 LLM의 성능을 해치지 않는다는 것을 보여줍니다.

<center>
<img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F6263e804-9457-4185-8737-142277d1217e_1000x500.png" alt="Key takeaways from alignment experiments">
</center>
<center>(출처: [7])</center>

[7]의 조정 실험에서 얻은 핵심 요점은 위 그림에 묘사되어 있으며 다음과 같이 요약할 수 있습니다.

*   온-정책 샘플링은 고품질 조정에 매우 중요하며, 특히 최적 보상을 가진 응답이 참조 정책에서 가능성이 낮을 경우 더욱 그렇습니다.
*   적당한 양의 샘플 재사용은 조정 품질의 눈에 띄는 저하 없이 오프-정책 업데이트를 도입할 수 있습니다.
*   음의 경사 사용은 더 빠른 수렴으로 이어지며 온-정책 샘플링에 보완적인 이점을 제공합니다.
*   보상 피크가 참조 정책에서 이미 가능성이 있는 간단한 조정 사례의 경우, 온-정책 샘플링이나 음의 경사를 사용하지 않는 완전히 오프라인 또는 지도 방법도 여전히 잘 수행될 수 있습니다.

이러한 각 핵심 사항은 [7]에 제시된 실제 조정 요점에도 포착되어 있으며, 쉽게 참조할 수 있도록 아래에 복사되었습니다.

<center>
<img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F6263e804-9457-4185-8737-142277d1217e_1000x500.png" alt="Practical alignment takeaways">
</center>
<center>(출처: [7])</center>

#### DPO와 PPO 분리: 선호 피드백 학습을 위한 최적 관행 식별 [2]

<center>
<img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F6263e804-9457-4185-8737-142277d1217e_1000x500.png" alt="DPO and PPO comparison">
</center>
<center>(출처: [2])</center>

[2]에서 저자들은 중규모 LLM을 조정하기 위해 온라인 및 오프라인 RL 알고리즘(특히 PPO 기반 RLHF 및 DPO) 간의 경험적 비교를 수행합니다. 이 분석은 다음을 변경하여 여러 도메인에 걸쳐 광범위한 벤치마크에서 단일 LLM의 성능을 최대화하려고 합니다.

*   사용되는 선호 데이터의 유형, 출처 또는 규모.
*   훈련 알고리즘의 스타일(즉, 오프라인 또는 온라인).

또한, PPO 기반 RLHF의 성능을 향상시키기 위해 여러 하이퍼파라미터(hyperparameter) 설정 및 훈련 설정이 고려되어 온라인 RL로 결과를 최대화하기 위한 유용한 직관을 제공합니다. 이 분석을 통해 우리는 다음을 알 수 있습니다.

*   선호 데이터의 선택이 LLM 품질에 가장 큰 영향을 미칩니다. 데이터 품질과 구성이 조정 성공의 핵심 결정 요인입니다.
*   온라인 RL 알고리즘은 DPO와 같은 오프라인 알고리즘보다 지속적으로 성능이 우수합니다.

<center>
<img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F6263e804-9457-4185-8737-142277d1217e_1000x500.png" alt="Experimental setup for PPO and DPO">
</center>
<center>(출처: [2])</center>

[2]의 실험 설정은 PPO 기반 RLHF와 DPO 모두에 대한 표준 접근 방식을 채택합니다. 위를 참조하세요. 모든 실험은 Tulu-2-13B [3]을 DPO와 PPO 모두의 시작 모델로 사용합니다. 선호 튜닝 후, 모델은 다음 도메인에서 성능을 측정하는 광범위한 벤치마크에 대해 평가됩니다.

*   사실성(Factuality, 예: MMLU)
*   추론(Reasoning, 예: GSM8K)
*   진실성(Truthfulness, 예: TruthfulQA)
*   코딩(Coding, 예: HumanEval+)
*   안전성(Safety, 예: ToxiGen)
*   명령 따르기(Instruction following, 예: IFEval)

이러한 다양한 벤치마크를 통해 우리는 개별 도메인에서의 모델 성능과 도메인 전반의 일반적인 성능을 관찰할 수 있습니다.

**데이터 선택.** LLM 조정을 위한 합성 선호도(synthetic preferences)를 활용하는 최근 연구 [4]를 기반으로, 우리는 세 가지 출처에서 선호 데이터를 도출할 수 있습니다.

*   인간 선호도.
*   웹 스크래핑(Web scraping)7.
*   합성 선호도.

흥미롭게도 [2]에서는 합성 선호 데이터셋(특히 UltraFeedback 데이터셋)이 인간 주석 선호 데이터(human-annotated preference data)와 비교해도 최고의 결과를 낸다는 것을 알 수 있습니다. 더 나아가, [2]의 저자들은 선호 데이터를 선별하는 데 있어 다음의 중요한 고려 사항을 특별히 언급합니다.

*   선호도 품질(즉, 선호 쌍 내에서 선택된 결과물 또는 거부된 결과물의 선택)은 결과물 자체의 품질보다 실제로 더 중요합니다.
*   측면별 선호 피드백(per-aspect preference feedback)을 수집하면 명확한 성능 이점을 얻을 수 있습니다. 집계된 측면별 선호(aggregated, per-aspect preferences)로 훈련된 모델은 표준 선호 데이터 양의 15배로 훈련된 모델보다 성능이 우수합니다.
*   [1]에서 고려된 데이터로, 선호 튜닝은 채팅 기능과 출력 스타일을 개선하는 데 가장 큰 영향을 미치지만, 모델은 새로운 사실이나 정보를 학습하는 것 같지는 않습니다.

측면별 선호 피드백은 인간이나 모델에게 데이터의 각 측면(예: 유용성 및 무해성)을 독립적으로 평가하도록 요청한 다음, 이러한 측면별 점수를 집계하여 최종 집계 선호 점수를 도출함으로써 수집됩니다. 주석자에게 단일 전체 선호 점수를 요청하는 것과 비교하여, 이러한 접근 방식은 선호 피드백의 품질을 향상시키고, 이는 다시 선호 튜닝 후 결과 모델의 품질을 향상시키는 것으로 밝혀졌습니다. [2]의 저자들은 훈련 후 품질에 영향을 미치는 다양한 요인을 고려하지만, 선호 데이터의 출처와 품질이 가장 중요한 영향을 미치는 것으로 밝혀졌습니다.

<center>
<p>"PPO는 수학에서 최대 2.5%, 일반 도메인에서 1.2%까지 DPO를 능가합니다. 고품질 선호 데이터는 명령 따르기 및 진실성에서 최대 8%의 개선을 가져옵니다." - 출처: [2]</p>
</center>

**PPO 대 DPO.** 온라인 또는 오프라인 접근 방식으로 훈련된 모델을 직접 비교할 때, [2]에서는 온라인 훈련 알고리즘이 명확한 우위를 가진다는 것을 알 수 있습니다. 실제로, 모든 데이터셋에서 PPO 기반 RLHF로 훈련된 거의 모든 모델이 동일한 설정으로 DPO로 훈련된 모델보다