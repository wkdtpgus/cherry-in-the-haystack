대규모 언어 모델(LLM)의 잠재력은 인공지능(AI) 분야를 혁신하고 있으며, 이러한 모델이 인간의 의도와 가치에 부합하도록 만드는 '정렬(alignment)'은 그 핵심 과제입니다. 정렬(alignment) 과정은 대규모 언어 모델(LLM)이 다양한 태스크에서 효과적으로 작동하도록 돕습니다. 초기에는 모델이 특정 목표를 달성하도록 지도하는 데 중점을 두었으나, 이제는 더욱 복잡한 사회적, 윤리적 기준을 통합해야 하는 상황에 직면해 있습니다. 정렬을 위한 전통적인 전략에는 지도 미세 조정(supervised finetuning)과 같은 기본 접근 방식과 더불어 더욱 진화된 방법들이 도입되고 있습니다.

이러한 전통적인 방법 중 하나인 인간 피드백 강화 학습(RLHF)은 특히 프롬프트 엔지니어링(prompt engineering)과 결합될 때 강력한 성능을 보여주었습니다. 그러나 이 접근 방식은 잘 작동하지만, PPO 기반 RLHF는 여러 가지 이유로 다른 모델과의 통합에 어려움을 겪기도 합니다. 특히, PPO는 훈련 중에 현재 LLM으로 샘플을 생성하기 위해 추론을 적극적으로 실행하는 '온라인(online)' 알고리즘의 특성상 상당한 계산 자원과 복잡한 훈련 파이프라인(pipeline)을 요구합니다. 온라인 RL 훈련은 효율적으로 조율하기 어렵고, 새로운 데이터 스트림에 빠르게 적응하는 유연성을 제공합니다. 이는 PPO가 훈련 중에 LLM의 여러 복사본을 저장해야 하므로 자원 효율적인 대안이 필요하다는 인식이 커지고 있습니다. 이러한 문제들은 LLM 정렬의 접근 방식에 대한 재평가를 촉발했으며, 보다 효율적이고 확장 가능한 방법론을 모색하게 만들었습니다.

우리는 i) 오버헤드가 낮은 온라인 RL 알고리즘을 사용하거나, ii) 오프라인 알고리즘(offline algorithms)을 개발하거나, 심지어 iii) 정렬 과정에서 RL을 완전히 제거함으로써 데이터 효율성을 극대화하는 방향으로 발전하고 있습니다. 이는 자원 제약이 있는 환경에서도 고품질의 정렬을 달성하려는 노력의 일환입니다. 그러나 온라인 RL은 성능이 매우 뛰어나며, 실시간 상호작용 시스템에 필수적인 요소가 되고 있습니다. 따라서 더 간단한 정렬 알고리즘은 성능 저하를 수반하는 경향이 있다는 기존의 관점을 넘어서, 온라인과 오프라인 접근 방식의 장점을 결합하려는 시도가 활발히 이루어지고 있습니다.

이 개요에서는 LLM 정렬을 위한 온라인, PPO 기반 대안을 넘어 다양한 최신 방법론들을 심층적으로 분석하고자 합니다. 특히, 우리의 초점은 온-정책 샘플링(on-policy sampling)을 수행하는 온라인 알고리즘과 고정된 데이터셋(fixed dataset)으로 LLM을 훈련하는 오프라인 알고리즘 간의 성능 격차를 분석하는 것을 넘어, 최근 등장한 하이브리드(hybrid) 접근 방식과 데이터 효율적인 정렬 기술들을 탐구하는 데 있습니다. 이 분야의 논문들을 연구함으로써 다음 질문에 답할 것입니다: 고품질 LLM 정렬에 강화 학습의 어떤 측면이 가장 중요한가? 정렬에 온-정책 훈련 데이터(on-policy training data) 샘플링의 역할은 어떻게 진화하고 있는가?

우리가 보게 되겠지만, 온-정책 샘플링은 명확한 성능 이점을 제공하여 모델의 적응성과 학습 효율을 크게 향상시킵니다. 그러나 오프라인(또는 RL-free) 접근 방식은 이러한 온라인-오프라인 격차에도 불구하고 여전히 효과적일 수 있습니다. 특히, 오프라인 알고리즘을 온-정책 데이터로 강화하면 완전한 온라인 RL에 비해 효과적이고 구현하기 쉬운 준-온라인 알고리즘(semi-online algorithms)을 형성할 수 있습니다. 이는 실용적인 LLM 정렬 솔루션 개발에 중요한 통찰력을 제공합니다.

AI 연구의 최신 정보를 얻기 위해 Deep (Learning) Focus를 사용하는 커뮤니티는 끊임없이 새로운 지식을 공유합니다. 구독하기

### LLM 정렬의 진화: 최신 동향과 도전 과제

대규모 언어 모델(LLM)의 급속한 발전은 단순히 언어를 이해하고 생성하는 것을 넘어, 복잡한 추론, 창의적인 콘텐츠 생성, 그리고 다양한 전문 분야에서의 활용 가능성을 열었습니다. 그러나 이러한 강력한 능력만큼이나 모델이 편향되거나, 유해하거나, 의도치 않은 방식으로 작동할 위험 또한 커지고 있습니다. 따라서 LLM 정렬은 단순한 성능 향상을 넘어, 모델의 안전성, 신뢰성, 그리고 윤리적 사용을 보장하는 필수적인 과정이 되었습니다.

최신 LLM 개발은 몇 가지 핵심 단계를 거칩니다:

*   **사전 훈련(Pretraining)**은 인터넷 규모의 텍스트 데이터에 대해 모델이 언어의 구조와 의미를 학습하는 기반이 됩니다. 이 단계에서는 방대한 데이터로부터 일반적인 언어 지식을 습득합니다.
*   **지도 미세 조정(Supervised finetuning, SFT)** 또는 **명령 미세 조정(instruction finetuning, IFT)**은 사용자 의도에 더욱 부합하는 응답을 생성하도록 돕습니다. 이는 특정 작업이나 스타일을 모방하도록 모델을 훈련하는 과정입니다.
*   **인간 피드백 기반 강화 학습(Reinforcement learning from human feedback, RLHF)**은 강화 학습(RL)을 사용하여 모델의 윤리적이고 안전한 행동을 유도하는 데 중요한 역할을 합니다. 인간의 선호도를 직접 반영하여 모델의 출력을 개선합니다.
*   **검증 가능한 보상 기반 강화 학습(Reinforcement learning from verifiable rewards, RLVR)**은 규칙이나 휴리스틱(heuristics)에서 보상(reward)을 확정적으로 도출할 수 있는 검증 가능한 작업(verifiable tasks)에 대해 RL로 LLM을 훈련합니다. 이는 모델의 사실적 정확성과 일관성을 높이는 데 기여합니다.

이러한 훈련 전략들은 LLM의 핵심 지식 기반을 구축하는 사전 훈련 과정과 함께, 정렬은 LLM에 인간 선호 점수를 극대화하기 위한 다양한 응용 분야에서 모델의 유용성을 결정합니다. 추론 훈련(Reasoning training)은 검증 가능한 작업에서 성능을 추가적으로 향상시키는 마지막 단계입니다.

<center>훈련 단계를 별개의 단계로 그룹화</center>

이 개요는 LLM 정렬과 제안된 많은 알고리즘에 기반한 기술 발전의 최전선을 탐색하고자 합니다. 특히 RLHF 훈련 과정에서 더 간단한 오프라인 정렬 알고리즘을 사용하는 것과 대조적으로 온라인 RL의 역할과 필요성에 초점을 맞출 것입니다. 이 섹션에서는 온라인 및 오프라인 알고리즘을 포함하여 정렬 알고리즘에 존재하는 다양한 옵션을 설명하면서 이 논의를 시작할 것입니다.

#### 데이터 효율적인 SFT와 능동 학습(Active Learning)

가장 간단한 LLM 정렬 전략 중 하나는 사전 훈련 중에 사용된 것과 동일한 기존 모델의 성능을 향상시키는 효과적인 방법으로 활용됩니다. 지도 미세 조정(SFT)은 여전히 강력한 기준점이지만, 최근에는 데이터의 질과 양을 최적화하는 방향으로 발전하고 있습니다. SFT는 LLM을 정렬하기 위해 더 작은 고품질 프롬프트-응답 쌍 세트를 활용하며, 데이터의 다양성과 정확성을 확보하는 것이 중요합니다.

**거부 샘플링(Rejection sampling)**은 SFT의 온라인 변형으로, 모델의 출력 품질을 지속적으로 개선합니다. 이는 단순히 고정된 데이터셋에 의존하는 것이 아니라, 모델의 현재 성능을 반영하는 새로운 데이터를 생성하고 평가하여 훈련에 활용하는 방식입니다.

*   프롬프트 데이터셋으로 시작합니다.
*   현재 LLM으로 각 프롬프트에 대한 완성을 생성합니다.
*   보상 모델(reward model) 또는 LLM 심사관(LLM judge)을 사용하여 이 모든 완성을 평가합니다.
*   가장 높은 점수를 받은 프롬프트-완성 쌍을 선택(또는 필터링)합니다.
*   이러한 상위 예시에 대해 SFT를 수행합니다.

거부 샘플링 과정은 아래에 묘사되어 있습니다. 이 접근 방식은 SFT와 유사한 방식으로 LLM을 훈련하지만, 차이점은 데이터에 있습니다. 우리는 LLM 자체를 사용하여 준-온라인(semi-online) 방식으로 SFT 훈련 데이터를 샘플링합니다. 보상 모델은 우리가 최고 품질의 완성에 대해 훈련하고 있는지 확인하는 데 사용됩니다.

<center>
<img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F6710408d-8356-4318-8798-2305597950c4_1000x500.png" alt="Rejection sampling process">
</center>
<center>(출처: RLHF book, 라이선스)</center>

우리는 일반적으로 거부 샘플링을 반복적으로 수행합니다. 예를 들어, Llama-2 정렬 과정은 RL 기반 RLHF 전에 네 번의 거부 샘플링 라운드를 사용합니다. 위 논의에서 우리는 거부 샘플링을 SFT의 변형으로 설명했는데, 둘 다 동일한 훈련 목표를 사용하기 때문입니다. 그러나 거부 샘플링은 실제로는 선호 튜닝(preference tuning) 기술이며, SFT의 대안이 아니라 RLHF의 더 간단한 대안으로 가장 자주 사용됩니다. 실제로 거부 샘플링은 SFT 대신 SFT 후에 적용되는 경우가 많습니다.

<center>
<img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F6263e804-9457-4185-8737-142277d1217e_1000x500.png" alt="SFT variants">
</center>
<center>(출처: [13]) SFT 변형.</center>

거부 샘플링(Best-of-N 샘플링이라고도 함) 외에도 여러 온라인 또는 반복적인 SFT 변형이 제안되었습니다. 이 개요에서 다룰 주목할 만한 예시는 다음과 같습니다.

*   **인간 피드백 기반 지도 반복 학습(Supervised Iterative Learning from Human Feedback, SuperHF)** [13]은 모델에서 온-정책 출력(on-policy outputs) 배치를 샘플링하고, 보상 모델로 이 출력을 필터링하며, KL 발산(KL divergence) 제약 조건(constraint) 하에 지도 목표(supervised objective)를 사용하여 모델을 최적화하는 온라인 학습 기술입니다. 위를 참조하세요.
*   **강화된 자기 훈련(Reinforced Self-Training, ReST)** [14]은 위에서 설명한 거부 샘플링 공식을 사용합니다. 이 공식에서는 LLM에서 온-정책 데이터를 반복적으로 샘플링하고, 각 샘플을 보상 모델로 평가하며, 최상의 샘플에 대해 훈련합니다.
*   **보상 가중 회귀(Reward-Weighted Regression, RWR)** [15]도 유사하게 LLM을 사용하여 보상 모델로 평가되는 온-정책 샘플을 생성합니다. 그러나 이 점수는 필터링 대신 훈련 손실(training loss)에서 각 샘플에 가중치를 부여하는 데 사용됩니다.
*   **보상 순위 미세 조정(Reward Ranked Finetuning, RAFT)** [16]는 다시 LLM에서 온라인 완성을 샘플링하고 보상 모델의 점수로 SFT에 사용할 완성을 필터링하는 표준 거부 샘플링 설정을 채택합니다.

#### 강화 학습(RL) 훈련의 새로운 지평

<center>
<img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F6263e804-9457-4185-8737-142277d1217e_1000x500.png" alt="Reinforcement Learning (RL) Training">
</center>
<center>(출처: [16])</center>

최근 LLM 훈련에는 몇 가지 주요 강화 학습(RL) 기법이 적용됩니다. 이는 모델이 복잡한 환경에서 최적의 행동을 학습하도록 돕는 데 필수적입니다.

*   **인간 피드백 기반 강화 학습(Reinforcement Learning from Human Feedback, RLHF)**은 인간 선호 보상 모델(human preference reward model)에서 파생된 보상을 사용하여 RL로 LLM을 훈련합니다.
*   **검증 가능한 보상 기반 강화 학습(Reinforcement Learning with Verifiable Rewards, RLVR)**은 규칙 기반 또는 확정적 검증기(deterministic verifiers)에서 파생된 보상을 사용하여 RL로 LLM을 훈련합니다.

이러한 RL 훈련 기술은 주로 훈련을 위한 보상을 도출하는 방식에서 차이가 있지만, 알고리즘의 다른 세부 사항은 대부분 유사합니다. 아래에 묘사된 바와 같이, 둘 다 일련의 프롬프트에 대한 완성을 생성하고, 이 완성에 대한 보상을 계산하며, 보상을 사용하여 정책 업데이트(policy update) 또는 RL 최적화기(optimizer)를 통한 LLM 매개변수(parameters) 업데이트를 도출합니다.

<center>
<img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F6263e804-9457-4185-8737-142277d1217e_1000x500.png" alt="Visual walkthrough of RL training for LLMs">
</center>
<center>LLM을 위한 RL 훈련 시각적 안내</center>

LLM을 RL로 최적화할 때, 우리는 아래에 표시된 목표를 해결하려고 합니다. 이 목표는 LLM 완성에서 받은 보상을 최대화하는 동시에 참조 모델(reference model)에 대한 모델의 KL 발산(KL divergence)을 최소화합니다. 참조 모델은 일반적으로 RL 훈련 시작 시점의 LLM 체크포인트(checkpoint)입니다. 간단히 말해, 이는 새로운 모델이 원래(참조) 모델과 크게 다르지 않으면서 보상을 최대화하기를 원한다는 의미입니다.

<center>
<img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F6263e804-9457-4185-8737-142277d1217e_1000x500.png" alt="RL training objective">
</center>
<center>RL 훈련 목표</center>

**온-정책 샘플링(On-policy sampling).** 위에 표시된 바와 같이, RL로 LLM을 훈련할 때 온-정책 샘플링을 수행합니다. "온-정책" 샘플링이란 핵심 RL 훈련 루프(training loop)에서 LLM을 훈련하는 데 사용되는 완성이 LLM 자체에 의해 실시간으로 생성된다는 의미입니다. 즉, 완성은 다른 모델에 의해 생성되거나 오프라인, 사전 계산된 데이터셋에 저장되지 않습니다. LLM의 맥락에서 온-정책 샘플링을 사용하는 훈련 알고리즘은 일반적으로 "온라인" 훈련 알고리즘이라고 불립니다. 온-정책 샘플링은 RL 훈련의 맥락에서만 사용되는 것이 아닙니다. 예를 들어, 우리는 이전 섹션에서 SFT의 여러 온라인 변형에 대해 배웠습니다.

**RLHF에 대해 더 자세히.** 이 개요는 LLM 정렬에 초점을 맞추고 있으므로, 주로 RLHF 스타일 훈련을 다룰 것입니다. LLM 정렬에 대한 초기 접근 방식은 SFT와 RLHF를 결합한 3단계 기술(아래에 표시됨)을 사용했습니다.

<center>
<img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F6263e804-9457-4185-8737-142277d1217e_1000x500.png" alt="RLHF three-stage technique">
</center>
<center>(출처: [7])</center>

RLHF에서 우리는 각 선호 쌍(preference pair)에 다음이 포함된 선호 쌍 데이터셋을 수집하는 것으로 시작합니다.

*   프롬프트.
*   선택된(또는 승리한) 완성.
*   거부된(또는 패배한) 완성.

그런 다음 선호 데이터셋에 대해 보상 모델을 훈련하고 위에서 설명한 RL 훈련 루프로 LLM을 최적화합니다. 이 선호 데이터셋의 완성은 다양한 출처에서 나올 수 있습니다. 예를 들어, 참조 모델, 이전 모델 체크포인트, 또는 완전히 다른 모델에서 나올 수도 있습니다. 선호 주석(preference annotation) 또는 쌍에서 선택된 완성 및 거부된 완성의 선택은 일반적으로 인간 주석자(human annotator) 또는 LLM 심사관(즉, AI 피드백)에 의해 제공됩니다. 특히, 선호 데이터와 보상 모델은 RL 훈련 시작 시점에 고정됩니다. 이를 좀 더 공식적으로 말하면, LLM은 오프라인 모델 기반 RL(offline model-based RL)의 변형으로 훈련됩니다.

**RL 최적화기(RL optimizers).** 위 RL 훈련 설명에서 빠진 한 가지 세부 사항은 정책 업데이트(policy update)를 어떻게 계산하는가입니다. 여기서는 이 질문에 간략하게 답하겠지만, 관심 있는 독자는 전체 세부 사항을 위해 이 심층 개요를 참조해야 합니다. 일반적으로 정책 경사(policy gradient) 기반 RL 최적화기(예: REINFORCE, PPO, 또는 GRPO)가 사용됩니다. PPO 기반 RLHF는 과거에 사실상의 선택이었지만, PPO는 LLM으로 가치 함수(value function)를 추정하기 때문에 계산 비용이 많이 듭니다. 실제로 PPO 기반 RLHF는 훈련 중에 LLM의 네 가지 다른 복사본(즉, 정책, 참조 정책, 가치 모델, 보상 모델)을 저장합니다. 오버헤드를 줄이기 위해 REINFORCE는 훈련 내내 모델이 받은 보상의 평균으로 가치 함수를 근사하여 정책 경사의 몬테카를로 추정(Monte Carlo estimate)을 도출합니다(즉, LLM 대신). 유사하게, GRPO는 동일한 프롬프트에 대한 여러 완성의 보상 평균으로 가치 함수를 근사합니다. 이를 그룹(group)이라고 합니다. GRPO는 RLVR에 가장 일반적인 RL 최적화기이므로 보상 모델 없이도 일반적으로 사용됩니다. 이 경우 RL 훈련을 위해 LLM의 두 가지 복사본(정책 및 참조 정책)만 저장합니다. 그러나 보상 모델의 부재는 RLVR의 부산물입니다(즉, GRPO는 보상 모델 유무에 관계없이 사용할 수 있습니다).

#### 직접 정렬 기술(Direct Alignment Techniques)과 효율성

<center>
<img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F6263e804-9457-4185-8737-142277d1217e_1000x500.png" alt="Direct Alignment Techniques">
</center>
<center>(출처: [18])</center>

온라인 RL 훈련이 매우 비싸기 때문에 연구자들은 직접 선호 최적화(direct preference optimization, DPO) [18]와 같은 더욱 효율적인 대안을 모색하고 있습니다. DPO는 PPO 기반 RLHF와 비교하여 명시적인 보상 모델 훈련을 피하고 모델의 학습 과정을 단순화하는 데 기여합니다. 이 암묵적 보상을 사용하여 LLM은 아래에 표시된 대조 학습 목표(contrastive learning objective)로 훈련되며, 이는 표준 경사 하강법(gradient descent)으로 최적화될 수 있습니다(즉, RL 훈련 없이).

<center>
<img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F6263e804-9457-4185-8737-142277d1217e_1000x500.png" alt="DPO training loss">
</center>
<center>(출처: [18]) DPO 훈련 손실</center>

직관적으로, 이 대조 손실(contrastive loss)은 선호 데이터셋에서 선택된 응답과 거부된 응답 간의 확률 마진(probability margin)을 증가시킵니다. LLM은 고정된 선호 데이터셋(RLHF에서 보상 모델을 훈련하는 데 사용되는 것과 동일한 데이터)으로 훈련됩니다. 이러한 이유로 DPO는 오프라인(즉, 훈련 데이터가 고정되어 있고 온-정책 샘플링이 없음) 직접 정렬 알고리즘(direct alignment algorithm)으로 특징지어집니다. RL 기반 정렬 알고리즘과 비교하여 DPO는 훨씬 적은 계산 오버헤드를 필요로 하고, 튜닝하기 쉬우며, 여전히 좋은 성능을 보이는 경향이 있습니다. 자세한 내용은 아래를 참조하십시오.

<center>
<img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F6263e804-9457-4185-8737-142277d1217e_1000x500.png" alt="Direct Preference Optimization (DPO)">
</center>

<center>
<p>RLHF와 같은 정렬 기술은 LLM 품질을 크게 향상시켰지만, 계산 비용이 많이 들고 사용하기 어렵습니다. 이 개요는 경사 하강법으로 최적화할 수 있는 목표를 사용하여 LLM을 정렬함으로써 이러한 복잡성을 피하는 DPO라는 더 간단한 LLM 정렬 접근 방식을 다룹니다.</p>
<p>전체 이야기 읽기</p>
</center>

**DPO의 변형.** DPO는 PPO 기반 RLHF에 비해 사용하기 훨씬 간단했기 때문에 이 기술은 LLM 연구에서 빠르게 인기를 얻었습니다. 그 결과, 항등 선호 최적화(Identity Preference Optimization, IPO) [8], 카네만-트버스키 최적화(Kahneman-Tversky Optimization, KTO) [19], 또는 대조 선호 최적화(Contrastive Preference Optimization, CPO) [20]와 같은 DPO의 많은 변형이 제안되었습니다. 이러한 기술 중 다수는 DPO에 약간의 수정을 가하여 성능을 약간 향상시키지만, 대조 목표를 사용한 직접 정렬이라는 핵심 아이디어는 유사합니다. 그러나 이러한 기술 중 일부는 DPO와 의미론적으로 다릅니다. 예를 들어, KTO는 선호 쌍(preference pair)이 아닌 이진(좋음 또는 나쁨) 등급이 있는 단일 완성에 적용될 수 있는 DPO 스타일 손실을 공식화합니다.

**온라인 또는 반복 DPO.** 표준 공식에서 DPO는 완전히 오프라인 정렬 알고리즘입니다. DPO 훈련 내내 선호 데이터셋은 고정되어 있지만, 훈련 과정에 온-정책 샘플을 도입함으로써 온라인(또는 준-온라인) DPO 변형을 만들 수 있습니다. 아래에 묘사된 바와 같이, 이 아이디어의 한 예는 자기 보상 언어 모델(self-rewarding language models) [10]입니다. 이 프레임워크에서 우리는 다음과 같이 DPO 훈련을 위한 새로운 데이터를 주기적으로 샘플링합니다.

*   일련의 프롬프트로 시작합니다.
*   현재 LLM으로 이 프롬프트에 대한 여러 완성을 샘플링합니다.
*   이 완성을 순위 매겨(예: LLM 심사관 또는 보상 모델 사용) 선호 데이터셋을 생성합니다.
*   위에서 설명한 DPO를 사용하여 이 데이터에 대해 LLM을 훈련합니다.
*   1단계로 돌아가 여러 라운드 동안 반복합니다.

이 과정에서 우리는 DPO로 모델을 반복적으로 훈련하지만, 훈련 데이터는 현재 정책에서 주기적으로 다시 샘플링됩니다. 이는 준-온라인 훈련 설정(semi-online training setup)입니다. 현재 정책에서 완성을 더 자주 샘플링함으로써 이 접근 방식을 더 온-정책으로 만들 수 있습니다. 사실, 모든 훈련 데이터 배치에 대해 온-정책 완성을 샘플링함으로써 완전한 온라인 DPO 변형을 만들 수도 있습니다!

<center>
<img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F6263e804-9457-4185-8737-142277d1217e_1000x500.png" alt="Self-rewarding language models">
</center>
<center>(출처: [10])</center>

### LLM 정렬의 새로운 패러다임: 멀티모달 및 윤리적 정렬

PPO 기반 RLHF는 한동안 LLM 정렬의 표준 선택이었지만, 최근 연구는 그 한계를 지적하며 새로운 방향을 제시합니다. 이는 단순히 성능 격차를 줄이는 것을 넘어, LLM의 적용 범위를 확장하고 사회적 책임을 강화하는 데 초점을 맞춥니다. 이 섹션에서는 멀티모달(multimodal) 정렬과 윤리적(ethical) 정렬이라는 두 가지 주요 신흥 분야를 탐구할 것입니다.

#### 멀티모달 LLM 정렬의 도전과 기회

LLM이 텍스트를 넘어 이미지, 오디오, 비디오와 같은 다양한 형태의 데이터를 처리하는 멀티모달 모델로 진화함에 따라, 정렬의 복잡성도 증가하고 있습니다. 텍스트 기반 정렬은 주로 언어적 특성과 인간 선호도에 집중했지만, 멀티모달 정렬은 시각적 일관성, 청각적 정확성, 그리고 여러 모달리티(modality) 간의 의미론적 조화를 포함해야 합니다.

*   **멀티모달 보상 모델**: 텍스트만으로 구성된 보상 모델은 멀티모달 모델의 출력을 정확하게 평가하기 어렵습니다. 따라서 이미지-텍스트 쌍, 비디오-텍스트 설명 등 다양한 모달리티를 이해하고 평가할 수 있는 멀티모달 보상 모델의 개발이 필수적입니다. 이는 인간의 멀티모달 선호도를 포착하고, 모델이 생성하는 이미지와 텍스트 설명이 일치하는지, 혹은 비디오 콘텐츠가 유해하지 않은지 등을 판단해야 합니다.
*   **교차 모달리티 정렬**: 모델이 생성한 이미지와 그에 대한 텍스트 설명이 서로 모순되지 않도록 정렬하는 것이 중요합니다. 예를 들어, "행복한 고양이"라는 프롬프트에 대해 슬픈 표정의 고양이 이미지를 생성하고 "이 고양이는 행복합니다"라고 설명하는 것은 잘못된 정렬입니다. 이러한 교차 모달리티 일관성을 강화하기 위한 새로운 강화 학습 환경과 보상 설계가 요구됩니다.
*   **실시간 상호작용 정렬**: 멀티모달 LLM은 종종 실시간 상호작용 애플리케이션(예: 가상 비서, 증강 현실)에 사용됩니다. 이러한 환경에서는 모델이 즉각적인 멀티모달 피드백에 반응하고, 사용자의 의도를 다양한 모달리티를 통해 파악하여 적절하게 정렬된 응답을 생성해야 합니다. 이는 온-정책 샘플링의 중요성을 더욱 부각시킵니다.

이 연구의 목표는 LLM 정렬에서 DPO의 한계를 넘어 다양한 모델의 성능을 객관적으로 평가하는 데 있습니다. 멀티모달 정렬은 단순히 성능을 높이는 것을 넘어, 모델이 실제 세계와 더욱 자연스럽고 유용하게 상호작용하도록 만드는 데 결정적인 역할을 합니다.

#### 윤리적 및 사회적 정렬의 중요성

LLM이 사회 전반에 미치는 영향이 커짐에 따라, 모델의 윤리적 사용과 사회적 가치에 대한 정렬은 기술적 성능만큼이나 중요해졌습니다. 이는 모델이 유해한 콘텐츠를 생성하거나, 편향된 정보를 확산시키거나, 특정 집단에 대한 차별을 조장하지 않도록 하는 것을 의미합니다.

*   **편향 완화(Bias Mitigation)**: 훈련 데이터에 내재된 사회적 편향은 모델의 출력에 반영될 수 있습니다. 윤리적 정렬은 이러한 편향을 식별하고, 모델이 보다 공정하고 포괄적인 응답을 생성하도록 유도하는 데 초점을 맞춥니다. 이는 특정 인구 통계학적 그룹에 대한 부정적인 고정관념을 강화하지 않도록 모델의 행동을 수정하는 것을 포함합니다.
*   **유해성 감소(Harm Reduction)**: 모델이 폭력, 혐오 발언, 자해 조장과 같은 유해한 콘텐츠를 생성하는 것을 방지하는 것이 중요합니다. 윤리적 정렬은 이러한 위험을 최소화하기 위한 안전 메커니즘(safety mechanism)을 구축하고, 모델이 사용자에게 안전하고 건설적인 방식으로 정보를 제공하도록 훈련합니다.
*   **설명 가능성(Explainability)과 투명성(Transparency)**: 모델의 의사결정 과정을 더 잘 이해하고, 왜 특정 응답을 생성했는지 설명할 수 있도록 하는 것이 윤리적 정렬의 한 부분입니다. 이는 모델의 행동에 대한 신뢰를 구축하고, 잠재적인 문제점을 사전에 파악하여 해결하는 데 도움이 됩니다.
*   **인간 가치와의 일치**: 궁극적으로 윤리적 정렬은 LLM이 인간 사회의 보편적인 가치와 윤리적 원칙에 부합하도록 만드는 것을 목표로 합니다. 이는 단순히 규칙 기반의 제약을 넘어서, 모델이 복잡한 윤리적 딜레마(dilemma) 상황에서 사려 깊고 책임감 있는 판단을 내릴 수 있도록 하는 것을 포함합니다.

PPO 기반 RLHF로 LLM을 훈련할 때, 우리는 프롬프트 데이터셋의 프롬프트에 대한 완성을 온라인 방식으로 모델이 실제 환경에서 유연하게 대응하도록 돕습니다. 그러나 [6]에서는 DPO와 같은 RL-free, 오프라인 정렬 알고리즘을 사용할 때도 예상치 못한 편향이 발생할 수 있음을 경고합니다. 이러한 편향은 훈련 데이터의 특성이나 모델의 학습 방식에 따라 발생할 수 있으며, 윤리적 정렬의 중요성을 더욱 강조합니다. PPO는 이 커버리지(coverage) 문제를 올바르게 처리하고 복잡한 데이터 분포에서도 안정적인 성능을 유지합니다. 이는 윤리적 정렬에 있어서 모델이 다양한 상황과 맥락을 이해하고 적절하게 반응하는 능력이 중요함을 시사합니다.

이러한 결과는 참조 모델과 DPO의 선호 데이터 사이의 분포 변화가 모델의 일반화 능력에 중대한 영향을 미칠 수 있음을 시사합니다. 윤리적 정렬에서는 특히, 모델이 훈련 데이터에서 보지 못한 새로운 상황에서도 윤리적 원칙을 일관되게 적용할 수 있도록 일반화 능력을 확보하는 것이 중요합니다.

### 데이터 중심 정렬의 심화: 온-정책 데이터의 재해석

[7]의 저자들은 LLM을 위한 거의 모든 가능한 정렬 전략을 다루는 포괄적인 연구를 수행함으로써 새로운 모델 아키텍처의 가능성을 탐구하고 있습니다. 이들의 연구는 온-정책 샘플링과 음의 경사(negative gradient)의 중요성을 다시 한번 강조하며, 데이터가 정렬 과정에서 어떻게 활용되어야 하는지에 대한 깊은 통찰력을 제공합니다.

#### 온-정책 데이터의 활용 극대화

[7]에서는 온-정책 샘플링이 어려운 정렬 사례, 즉 높은 보상을 받는 응답이 참조 정책 내에서 발생할 가능성이 낮은 경우에 특히 중요하다는 점을 지적합니다. 이러한 경우, 정렬 과정은 낮은 보상 응답에서 높은 보상 응답으로 확률 질량(probability mass)을 "이동"시킴으로써 LLM을 훈련해야 합니다. 오프라인 및 순수 지도 정렬 방법은 이러한 복잡한 시나리오에서 특히 성능이 좋지 않습니다.

<center>
<p>"우리의 주요 발견은 일반적으로 온-정책 샘플링을 사용하거나 특정 응답의 우도(likelihood)를 낮추려는 접근 방식이 오프라인 및 최대 우도 목표보다 성능이 우수하다는 것입니다." - 출처: [7]</p>
</center>

**정렬 알고리즘.** [5]의 저자들은 온-정책 샘플링, 음의 경사, 그리고 샘플 재사용(sample reuse, 즉 동일한 데이터에 대해 여러 경사 업데이트를 수행하는 것) 사용을 기반으로 광범위한 잠재적 정렬 알고리즘(아래에 표시됨)을 특징짓는 것으로 시작합니다. 샘플 재사용의 구체적인 예로, PPO는 각 훈련 데이터 배치에 대해 2~4개의 순차적 경사 업데이트를 실행하는 반면, GRPO4 및 REINFORCE는 일반적으로 이러한 샘플 재사용을 피합니다 [9].

<center>
<img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F6263e804-9457-4185-8737-142277d1217e_1000x500.png" alt="Alignment algorithms">
</center>
<center>(출처: [7])</center>

모든 SFT 및 거부 샘플링 변형은 RL 기반 및 직접 정렬 방법에서 존재하는 모델의 학습 효율성을 저해하는 요인이 될 수 있습니다. 이 방법에서는 거부된(직접 정렬의 경우) 또는 낮은 보상을 받는(RL의 경우) 응답의 확률을 명시적으로 감소시킵니다. 마지막으로, 온-정책 샘플링은 훈련 설정에 따라 대부분의 기술에서 사용될 수도 있고 사용되지 않을 수도 있습니다. DPO 또는 IPO와 같은 직접 정렬 방법은 온-정책 샘플링 없이 고정된 선호 데이터셋에 대해 대조 훈련을 실행하지만, 현재 정책에서 새로운 훈련 데이터를 주기적으로 샘플링함으로써 오프라인 알고리즘의 온라인 버전을 만들 수 있습니다. 그러나 PPO 및 REINFORCE와 같은 일부 알고리즘은 자연스럽게 온-정책 샘플링을 기반으로 합니다.

**통합 정렬 알고리즘(Unified alignment algorithm).** 가능한 정렬 알고리즘의 범위를 포착하기 위해 [7]의 저자들은 아래에 표시된 프레임워크를 만듭니다. 이 프레임워크는 기본 정렬 알고리즘 내의 다양한 설정을 체계적으로 연구할 수 있도록 합니다. 예를 들어, 1단계와 2단계는 다음 중 하나로 수행될 수 있습니다.

*   온-정책 데이터 수집(즉, 현재 정책에서 응답을 생성하고 보상 모델로 자동으로 평가).
*   온-정책 샘플링 없이 오프라인 선호 데이터를 직접 사용(예: 표준 DPO에서와 같이).

더 나아가, 총 샘플 수 B를 변경하거나 일련의 샘플에 대해 수행되는 총 경사 단계 T를 변경하여 온-정책 샘플링의 정도를 다양하게 할 수 있습니다. 특히, T를 증가시키면 샘플 재사용이 도입되는 반면, B를 증가시켜도 그렇지 않으므로 온-정책 샘플 재사용의 영향을 분리할 수 있습니다.

<center>
<img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F6263e804-9457-4185-8737-142277d1217e_1000x500.png" alt="Unified alignment algorithm framework">
</center>
<center>(출처: [7])</center>

특히, 이 통합 알고리즘은 최대 우도 정렬 알고리즘을 포착하지 않지만, 이 알고리즘들은 [7]에서도 여전히 고려됩니다.

**훈련 설정.** 이러한 다양한 정렬 알고리즘의 속성은 다음을 포함한 여러 실험 설정을 사용하여 분석됩니다.

*   소규모(교훈적) 밴딧 문제(bandit problems).
*   합성 LLM 문제.
*   전체 규모 LLM 정렬.

합성 정렬 시나리오에서는 LLM 응답의 길이를 기반으로 수작업으로 만든 보상을 사용합니다. 구체적으로, 두 가지 보상 설정이 고려됩니다. 응답 길이 최소화와 평균 응답 길이 일치입니다. 아래를 참조하세요. 이러한 보상 시나리오는 높은 보상 응답이 참조 정책의 가능한 완성 영역 내외에 모두 있는 경우를 테스트합니다.5

<center>
<img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F6263e804-9457-4185-8737-142277d1217e_1000x500.png" alt="Reward settings">
</center>
<center>(출처: [7])</center>

교훈적 밴딧 문제(didactic bandit problems)도 보상 함수의 최적값을 변경하는 여러 보상 설정을 테스트합니다. 보상 설정을 변경함으로써, 우리는 각 알고리즘이 원래 참조 정책에서 확률이 낮더라도 높은 보상 응답에 확률을 할당하는 능력을 테스트합니다. 위를 참조하세요.

<center>
<p>"보상 함수 R1의 최적값은 참조 정책의 낮은 우도 영역에 위치하는 반면, R2의 최적값은 참조 정책의 모드(mode)와 대략적으로 일치합니다. 우리는 온-정책 샘플링이 보상 함수 R1을 최적화하는 데 중요할 것이며, 오프라인 또는 최대 우도 방법은 R2의 최적화에 충분할 수 있다고 가정합니다." - [7]의 밴딧 문제 설명</p>
</center>

전체 규모 정렬 시나리오는 AlpacaFarm, UltraChat 및 UltraFeedback의 공개 선호 데이터를 사용하여 Pythia-1.4B 및 Mistral-7B와 같은 소규모 LLM을 정렬합니다. 이 훈련 설정은 더 표준적인 LLM 정렬 시나리오이며, 모델은 황금 인간 선호 보상 모델(golden human preference reward model)을 사용하여 평가됩니다.

**온-정책 샘플링의 역할.** [7]의 실험에서 우리는 온-정책 데이터를 더 자주, 더 작은 배치로 샘플링하는 것(가능한 가장 엄격한 온-정책 설정)이 최고의 성능으로 이어진다는 것을 알 수 있습니다. 온-정책 샘플링의 영향은 높은 보상 응답이 참조 정책의 가능한 영역 내에 이미 있지 않은 모델이 새로운 환경에 적응하는 데 결정적인 역할을 합니다.

<center>
<p>"[우리는] 더 작지만 자주 샘플링되는 배치로 온-정책 샘플링이 더 나은 성능을 가져온다는 강력하고 명확한 경향을 관찰합니다... 오프-정책(off-policy) 업데이트가 많을수록 R2의 성능 저하는 상당히 완만합니다. 이는 보상 함수의 피크가 참조 정책의 가능성 있는 영역에 있을 때 더 높은 정도의 오프-정책 업데이트가 허용된다는 것을 나타냅니다." - 출처: [7]</p>
</center>

높은 보상을 받는 응답이 참조 정책 내에서 이미 가능성이 있는 더 간단한 정렬 사례에서는 모델이 오프라인 훈련 알고리즘 사용을 더 잘 견딜 수 있습니다. 이 현상은 합성 및 교훈적 문제 설정 모두에서 확인됩니다. 또한, 전체 규모 LLM 정렬 실험에서도 동일한 경향을 관찰할 수 있습니다. 여기서 가장 높은 보상은 훈련 과정을 더 온-정책으로 만들기 위해 배치 크기 B를 줄이는 것에서 나옵니다. 아래를 참조하세요.

<center>
<img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F6263e804-9457-4185-8737-142277d1217e_1000x500.png" alt="Batch size B vs performance">
</center>
<center>(출처: [7])</center>

**음의 경사.** 온-정책 샘플링과 유사하게, 음의 경사 사용은 정렬에 도움이 되는 것으로 밝혀졌습니다. 음의 경사 사용은 더 빠른 수렴으로 이어지며 모델의 학습 경로를 더욱 효율적으로 만들어줍니다. 음의 경사를 사용하는 알고리즘은 그렇지 않은 알고리즘에 비해 성능이 눈에 띄게 향상되며, 특히 원래 참조 정책에 의해 낮은 확률이 할당된 응답의 확률을 증가시켜야 하는 어려운 정렬 사례에서 더욱 그렇습니다. 아래(상단 그림)에 표시된 바와 같이, 음의 경사를 사용하는 알고리즘은 훈련 중에 선택된 응답과 거부된 응답 간의 확률 마진을 증가시킵니다. 음의 경사가 없는 알고리즘에서는 이러한 경향이 관찰되지 않습니다.

<center>
<img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F6263e804-9457-4185-8737-142277d1217e_1000x500.png" alt="Negative gradient impact">
</center>
<center>(출처: [7])</center>

그러나 흥미롭게도 위(하단 그래프)에서 마진이 증가함에도 불구하고 훈련 중에 선택된 응답과 거부된 응답의 절대 확률이 실제로 감소하는 것을 볼 수 있습니다. 이와 동일한 경향은 다른 논문 [8]에서도 관찰되었습니다.

<center>
<img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F6263e804-9457-4185-8737-142277d1217e_1000x500.png" alt="Absolute probability of chosen and rejected responses">
</center>
<center>(출처: [7])</center>

온-정책 샘플링과 음의 경사는 함께 사용될 때 복합적인 이점을 제공합니다. 예를 들어, 온-정책 IPO 및 DPO는 교훈적 밴딧 및 합성 LLM 실험 모두에서 오프라인 변형에 비해 더 빠른 수렴과 더 나은 성능을 보입니다. 위를 참조하세요. 전체 규모 LLM 실험에서 대조 정렬 알고리즘의 온라인 버전은 일부 경우 PPO보다 낮은 계산 비용과 실제 훈련 시간에도 불구하고 PPO보다 성능이 우수합니다.

**샘플 재사용은 해로운가?** T 값을 상당히 증가시키면 훈련 과정에 오프-정책 데이터(off-policy data)가 도입되어 성능이 사소하게 저하될 것입니다. 그러나 적당한 T 설정은 모델이 성능의 큰 저하 없이 오프-정책 업데이트를 훈련 과정에 통합할 수 있도록 합니다. 예를 들어, PPO를 사용한 합성 LLM 설정에서는 T를 1에서 8로 증가시켜도 성능 저하가 눈에 띄지 않습니다. 아래를 참조하세요.

<center>
<img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F6263e804-9457-4185-8737-142277d1217e_1000x500.png" alt="Sample reuse impact">
</center>
<center>(출처: [7])</center>

거부 샘플링(위 그림에서는 Best-of-N이라고 함)과 같은 최대 우도 훈련 목표는 샘플 재사용에 더 민감하지만6, 적당한 T 설정으로도 좋은 결과를 얻을 수 있습니다. 간단히 말해, 이러한 결과는 샘플 재사용으로 인한 오프-정책 업데이트가 LLM의 성능을 해치지 않는다는 것을 보여줍니다.

<center>
<img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F6263e804-9457-4185-8737-142277d1217e_1000x500.png" alt="Key takeaways from alignment experiments">
</center>
<center>(출처: [7])</center>

[7]의 정렬 실험에서 얻은 핵심 요점은 위 그림에 묘사되어 있으며 다음과 같이 요약할 수 있습니다.

*   온-정책 샘플링은 고품질 정렬에 매우 중요하며, 특히 최적 보상을 가진 응답이 참조 정책에서 가능성이 낮을 경우 더욱 그렇습니다.
*   적당한 양의 샘플 재사용은 정렬 품질의 눈에 띄는 저하 없이 오프-정책 업데이트를 도입할 수 있습니다.
*   음의 경사 사용은 더 빠른 수렴으로 이어지며 온-정책 샘플링에 보완적인 이점을 제공합니다.
*   보상 피크가 참조 정책에서 이미 가능성이 있는 간단한 정렬 사례의 경우, 온-정책 샘플링이나 음의 경사를 사용하지 않는 완전히 오프라인 또는 지도 방법도 여전히 잘 수행될 수 있습니다.

이러한 각 핵심 사항은 [7]에 제시된 실제 정렬 요점에도 포착되어 있으며, 쉽게 참조할 수 있도록 아래에 복사되었습니다.

<center>
<img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F6263e804-9457-4185-8737-142277d1217e_1000x500.png" alt="Practical alignment takeaways">
</center>
<center>(출처: [7])</center>

#### LLM 정렬의 실용적 고려사항: 데이터 선택과 모델 행동

[2]에서 저자들은 중규모 LLM을 정렬하기 위해 온라인 및 오프라인 RL 알고리즘(특히 PPO 기반 RLHF 및 DPO) 간의 경험적 비교를 수행합니다. 이 분석은 다음을 변경하여 여러 도메인에 걸쳐 광범위한 벤치마크에서 단일 LLM의 성능을 최대화하려고 합니다.

*   사용되는 선호 데이터의 유형, 출처 또는 규모.
*   훈련 알고리즘의 스타일(즉, 오프라인 또는 온라인).

또한, PPO 기반 RLHF의 성능을 향상시키기 위해 여러 하이퍼파라미터(hyperparameter) 설정 및 훈련 설정이 고려되어 온라인 RL로 결과를 최대화하기 위한 유용한 직관을 제공합니다. 이 분석을 통해 우리는 다음을 알 수 있습니다.

*   선호 데이터의 선택이 LLM 품질에 가장 큰 영향을 미칩니다. 데이터 품질과 구성이 정렬 성공의 핵심 결정 요인입니다.
*   온라인 RL 알고리즘은 DPO와 같은 오프라인 알고리즘보다 지속적으로 실시간 데이터 처리에서 뛰어난 강점을 보입니다.

<center>
<img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F6263e804-9457-4185-8737-142277d1217e_1000x500.png" alt="Experimental setup for PPO and DPO">
</center>
<center>(출처: [2])</center>

[2]의 실험 설정은 PPO 기반 RLHF와 DPO 모두에 대한 표준 접근 방식을 채택합니다. 위를 참조하세요. 모든 실험은 Tulu-2-13B [3]을 DPO와 PPO 모두의 시작 모델로 사용합니다. 선호 튜닝 후, 모델은 다음 도메인에서 성능을 측정하는 광범위한 벤치마크에 대해 평가됩니다.

*   사실성(Factuality, 예: MMLU)
*   추론(Reasoning, 예: GSM8K)
*   진실성(Truthfulness, 예: TruthfulQA)
*   코딩(Coding, 예: HumanEval+)
*   안전성(Safety, 예: ToxiGen)
*   명령 따르기(Instruction following, 예: IFEval)

이러한 다양한 벤치마크를 통해 우리는 개별 도메인에서의 모델 성능과 도메인 전반의 일반적인 성능을 관찰할 수 있습니다.

**데이터 선택.** LLM 정렬을 위한 합성 선호도(synthetic preferences)를 활용하는 최근 연구 [4]를 기반으로, 우리는 세 가지 출처에서 선호 데이터를 도출할 수 있습니다.

*   인간 선호도.
*   웹 스크래핑(Web scraping)7.
*   합성 선호도.

흥미롭게도 [2]에서는 합성 선호 데이터셋(특히 UltraFeedback 데이터셋)이 인간 주석 선호 데이터(human-annotated preference data)와 비교해도 최고의 결과를 낸다는 것을 알 수 있습니다. 더 나아가, [2]의 저자들은 선호 데이터를 큐레이팅하는 데 있어 다음의 중요한 고려 사항을 특별히 언급합니다.

*   선호도 품질(즉, 선호 쌍 내에서 선택된 완성 또는 거부된 완성의 선택)은 완성 자체의 품질보다 실제로 더 중요합니다.
*   측면별 선호 피드백(per-aspect preference feedback)을 수집하면 명확한 성능 이점을 얻을 수 있습니다. 집계된 측면별 선호(aggregated, per-aspect preferences)로 훈련된 모델은 표준 선호 데이터 양의 15배로 훈련된 모델보다 성능이 우수합니다.
*   [1]에서 고려된 데이터로, 선호 튜닝은 채팅 기능과 출력 스타일을 개선하는 데 가장 큰 영향을 미치지만, 모델은 새로운 사실이나 정보를 학습하는 것 같지는 않습니다.

측면별 선호 피드백은 인간이나 모델에게 데이터의 각 측면(예: 유용성 및 무해성)을 독립적으로 평가하도록 요청한 다음, 이러한 측면별 점수를 집계하여 최종 집계 선호 점수를 도출함으로써 수집됩니다. 주석자에게 단일 전체 선호 점수를 요청하는 것과 비교하여, 이러한 접근 방식은 선호 피드백의 품질을 향상시키고, 이는 다시 선호 튜닝 후 결과 모델의 품질을 향상시키는 것으로 밝혀졌습니다. [2]의 저자들은 훈련 후 품질에 영향을 미치는 다양한 요인을 고려하지만, 선호 데이터의 출처와 품질이 가장 중요한 영향을 미치는 것으로 밝혀졌습니다.

<center>
<p>"PPO는 수학에서 최대 2.5%, 일반 도메인에서 1.2%까지 DPO를 능가합니다. 고품질 선호 데이터는 명령 따르기 및 진실성에서 최대 8%의 개선을 가져옵니다." - 출처: [2]</p>
</center>

**PPO 대 DPO.** 온라인 또는 오프라인 접근 방식으로 훈련된 모델을 직접 비교할 때, [2]에서는 온라인 훈련 알고리즘이 명확한 우위를 가진다는 것을 알 수 있습니다. 실제로, 모든 데이터셋에서 PPO 기반 RLHF로 훈련된 거의 모든 모델이 동일한 설정으로 DPO로 훈련된 모델보다 성능이 우수한 것으로 밝혀졌습니다. [1]의 결과는 온라인 RL이 LLM의 선호 튜닝에 이점을 제공한다는 명확한 증거를 제공합니다. 아래를 참조하세요.

<center>
<img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F6263e804-9457-4185-8737-142277d1217e_1000x500.png" alt="PPO vs DPO performance">
</center>
<center>(출처: [2])</center>

**온라인 훈련이 왜 그렇게 유익한가?** 이 질문에 대한 답은 복잡하고 다면적이지만, [2]의 저자들은 DPO와 PPO로 훈련된 모델 간의 차이에 대해 흥미로운 관찰을 합니다. 즉, PPO 모델은 이러한 행동의 예시가 제공되지 않더라도 복잡한 문제를 해결하기 위해 인간과 유사한 문제 해결 능력을 보여줍니다. 이는 모델이 단순히 패턴을 모방하는 것을 넘어, 새로운 상황에 대한 추론 능력을 개발함을 시사합니다.

<center>
<p>"PPO로 훈련된 모델은 DPO로 훈련된 모델보다 사고의 사슬 추론을 수행할 가능성이 훨씬 더 높습니다... 사고의 사슬을 사용하는 인-컨텍스트(in-context) 예시가 주어지지 않았을 때도 마찬가지입니다. 이는 PPO로 인한 추론 개선이 사고의 사슬 능력 증가 때문일 수 있음을 시사합니다." - 출처: [2]</p>
</center>

모델이 학습하는 완성이 선호 데이터셋 내에 고정되어 있기 때문에 DPO와 같은 오프라인 알고리즘으로는 새로운 패턴이나 창의적인 응답을 생성하기 어렵습니다. 반면에 PPO는 훈련 중에 완성이 온라인으로 샘플링되므로 모델이 사고의 사슬 추론과 같은 새로운 행동을 탐색하고 학습할 수 있기 때문에 이러한 새로운 행동을 학습할 수 있습니다.

<center>
<img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F6263e804-9457-4185-8737-142277d1217e_1000x500.png" alt="PPO vs DPO chain-of-thought reasoning">
</center>
<center>(출처: [2])</center>

**온라인 RL의 다른 요인.** [2]에서 오프라인 및 온라인 알고리즘 분석 외에도, 저자들은 PPO 기반 RLHF의 성공에 핵심적인 요인을 결정하기 위해 다양한 제거 연구(ablation)를 수행합니다. 예를 들어, 보상 모델의 크기(및 보상 모델이 훈련되는 선호 데이터셋의 크기)를 늘리면 보상 모델의 품질이 향상되는 것으로 밝혀졌습니다. 그러나 더 나은 보상 모델이 다운스트림 평가 벤치마크(즉, PPO 기반 RLHF로 LLM을 훈련한 후)에 미치는 영향은 덜 명확합니다. 주요 성능 이점은 추론과 같은 더 복잡한 도메인에서 관찰됩니다. 겉보기에는 더 강력한 보상 모델은 실제로 더 나은 보상 모델을 필요로 하는 어려운 도메인에서만 영향력이 있습니다.

<center>
<p>"더 큰 보상 모델을 사용한다면, 실제로 보상 모델에 도전하는 데이터를 가지고 있어야 합니다." - 출처</p>
</center>

또한 PPO를 위한 특정 도메인에 초점을 맞춘 타겟 프롬프트 데이터셋(targeted prompt dataset)을 큐레이팅함으로써 모델의 전문성을 강화하는 데 효과적입니다. 이는 PPO가 활용할 수 있는 고유한 이점이지만 DPO와 같은 오프라인 알고리즘에서는 불가능합니다. 그러나 이러한 접근 방식은 일반적으로 성능 향상을 가져오지 않습니다. 이는 수학과 같은 특정 도메인에 LLM을 맞춤화하는 데만 유용합니다. 아래를 참조하세요.

<center>
<img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F6263e804-9457-4185-8737-142277d1217e_1000x500.png" alt="Targeted prompt dataset for PPO">
</center>
<center>(출처: [2])</center>

**최고의 훈련 레시피.** 분석을 마무리하면서 [2]의 저자들은 LLM 정렬의 다음 측면을 강조합니다.

*   선호 데이터 품질의 중요성.
*   온라인 RL의 우수성.
*   추론과 같은 복잡한 도메인에서 더 나은 보상 모델의 이점.
*   PPO를 위한 타겟 프롬프트 데이터셋이 LLM의 성능을 특정 도메인에 맞춤화하는 능력.

[2]에서 수행된 실험을 통해 발견된 LLM 정렬을 수행하는 최적의 접근 방식은 아래 인용문으로 요약됩니다.

<center>
<p>"우리는 고품질의 합성 선호 데이터셋, 대규모 보상 모델을 사용하여 PPO로 훈련합니다. 특정 도메인에 집중하고 싶다면, 정책 훈련을 위한 도메인별 프롬프트를 추가로 수집할 수 있습니다." - 출처: [2]</p>
</center>

#### 온라인 및 오프라인 정렬 알고리즘 간의 성능 격차 이해 [5]

"우리는 오픈 소스 데이터셋 스위트에서 온라인 알고리즘이 SFT 정책에 대한 KL 발산의 동일한 최적화 예산에서 오프라인 알고리즘보다 일반적으로 성능이 우수하다는 것을 보여줍니다." - 출처: [5]

[5]의 저자들은 RLHF로 LLM을 정렬하는 데 있어 온-정책 샘플의 중요성을 분석합니다. 먼저, 온라인 및 오프라인 정렬 알고리즘 간에 명확한 성능 격차가 있음을 보여줍니다. 이 성능 격차에 대한 몇 가지 직관적인 설명이 제안되고, 타겟 데이터 제거 연구(targeted data ablations)를 통해 하나씩 조사됩니다. 이러한 실험을 통해 온-정책 샘플 사용이 온라인 정렬 알고리즘의 핵심 성능 차별화 요소인 것으로 보인다는 것을 알 수 있습니다.

<center>
<img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F6263e804-9457-4185-8737-142277d1217e_1000x500.png" alt="IPO loss function">
</center>
<center>(출처: [8]) IPO 손실 함수</center>

**실험 설정.** [5]의 모든 실험은 고정된 정책에 대한 승률(win rate)을 기반으로 모델을 평가하고, 훈련을 위해 위에 표시된 대조 손실 함수를 사용하는 항등 선호 최적화(Identity Preference Optimization, IPO) 알고리즘을 사용합니다. 이 알고리즘은 DPO와 본질적으로 유사합니다. 훈련 데이터가 샘플링되는 방식에 따라 온라인 또는 오프라인 방식으로 LLM을 정렬하는 데 사용될 수 있습니다.

<center>
<img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F6263e804-9457-4185-8737-142277d1217e_1000x500.png" alt="Online vs offline IPO">
</center>
<center>(출처: [5])</center>

구체적으로, 우리는 훈련 중에 현재 정책에서 온-정책 데이터를 샘플링하고, 보상 모델로 이 완성을 자동으로 평가하며, 위에 설명된 IPO 훈련 목표를 통해 이 온라인 샘플에 대해 모델을 훈련함으로써 IPO를 온라인 방식으로 사용할 수 있습니다. 온라인 및 오프라인 IPO 간의 차이점은 위에 묘사되어 있습니다. [5]에서는 PPO 기반 RLHF 대신 온라인 IPO가 온라인 정렬 기술로 사용되는데, 몇 가지 다른 이유가 있습니다.

*   추가 가치 함수(value function) 요구 사항으로 인해 PPO 구현이 복잡하고 비용이 많이 듭니다.
*   PPO 최적화 과정을 오프라인 방식으로 공식화하는 명확한 방법이 없습니다(DPO는 PPO의 오프라인 등가물로 파생되었지만).

위에서 논의한 바와 같이, IPO를 온라인 또는 오프라인 방식으로 공식화하는 것은 비교적 간단합니다. PPO 기반 RLHF가 가장 널리 사용되는 온라인 정렬 알고리즘이라는 점을 감안할 때, 순수하게 대조 학습 목표에 의존하는 이러한 선택은 주류 정렬 연구에서 명확한 이탈입니다. 또한, [5]의 분석은 더 작은(즉, 10억 개 미만의 매개변수) 모델에 대해 수행됩니다. 이러한 문제에도 불구하고, 이 연구의 학습 내용은 온라인 및 오프라인 정렬 알고리즘 간의 핵심 차이점을 더 잘 이해하는 데 도움이 되는 유용한 직관을 제공합니다.

오프라인 알고리즘과 비교하여 온라인 정렬 알고리즘은 훈련 중에 추론을 수행하며 보상 모델을 위한 추가 훈련 절차를 필요로 합니다. 이러한 이유로 우리는 총 계산 예산(compute budget)을 기반으로 온라인 및 오프라인 알고리즘을 비교할 수 없습니다. 일반적으로 오프라인 정렬은 항상 훨씬 저렴할 것입니다. 대신, [5]의 저자들은 SFT 모델로부터의 KL 발산(KL divergence) 측면에서 정책을 비교하기로 선택합니다. 이는 계산에 구애받지 않는 방식으로 정렬 과정에서 모델이 얼마나 변하는지(즉, 최적화 "예산")를 포착합니다.

<center>
<p>"온라인 알고리즘은 샘플링 및 추가 보상 모델 훈련으로 인해 오프라인 알고리즘보다 계산 집약적인 경향이 있습니다... 우리는 비교 시 계산을 주요 요인으로 우선시하지 않고, 대신 RLHF 정책과 참조 SFT 정책 간의 KL 발산을 예산 측정 기준으로 채택합니다." - 출처: [8]</p>
</center>

**온라인 및 오프라인 RL 비교.** 분석을 시작하기 위해 저자들은 아래 그림에 묘사된 온라인 및 오프라인 정렬 결과를 제시합니다. 여기서 우리는 가능한 모든 KL 발산 수준에서 온라인 및 오프라인 정렬 알고리즘으로 훈련된 모델의 성능 사이에 명확한 격차가 있음을 알 수 있습니다. 이러한 결과는 여러 다른 오픈 정렬 데이터셋8에서 일관됩니다.

<center>
<img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F6263e804-9457-4185-8737-142277d1217e_1000x500.png" alt="Online vs offline alignment performance">
</center>
<center>(출처: [5])</center>

관찰된 온라인 정렬의 우수성을 바탕으로, [5]의 저자들은 이 성능 격차의 존재에 대해 다음과 같은 잠재적 설명을 제안합니다.

*   **데이터 커버리지(Data coverage)**: 온라인 알고리즘은 단순히 오프라인 알고리즘보다 더 다양한 데이터를 가지고 있기 때문에 오프라인 알고리즘보다 성능이 우수합니다.
*   **최적 이하 데이터(Sub-optimal data)**: 오프라인 알고리즘은 데이터셋의 완성이 SFT 정책에 의해 생성되었으므로 정렬 중에 생성된 온-정책 샘플에 비해 품질이 낮기 때문에 성능이 떨어집니다.9
*   **더 나은 분류(Better classification)**: 오프라인 알고리즘은 선호 쌍에서 선호되는 완성을 분류하도록 정책을 훈련하는 반면, 온라인 알고리즘은 명시적인 보상 모델을 통해 이를 달성합니다. 성능 격차는 온라인 알고리즘의 명시적인 보상 모델이 오프라인 정책에 비해 이 분류를 더 정확하게 수행하기 때문일 수 있습니다.
*   **대조 손실(Contrastive loss)**: IPO 및 DPO와 같은 오프라인 알고리즘이 사용하는 대조 목표(온-정책 샘플링의 부족이 아님)가 온라인 알고리즘과의 성능 격차로 이어질 수 있습니다.
*   **스케일링 법칙(Scaling laws)**: 기본 정책의 크기를 늘리면 성능 격차가 잠재적으로 사라질 수 있습니다.

다음으로, 이러한 각 가설은 온라인 및 오프라인 알고리즘 간의 차이점을 심층적으로 분석하는 일련의 제거 실험에서 연구됩니다.

**데이터 커버리지.** 정렬 품질에 대한 데이터 커버리지의 영향을 연구하기 위해 온라인 훈련 중에 온-정책 샘플링을 통해 생성된 모든 완성을 수집하여 오프라인 정렬을 위한 데이터셋을 형성할 수 있습니다. 이 데이터가 샘플링된 정확한 순서를 유지한다면 온라인 및 오프라인 정렬은 동일합니다. 즉, 모델은 동일한 데이터를 동일한 순서로 보고 따라서 동일한 매개변수 업데이트를 받습니다. 그러나 이 데이터를 섞어서 오프라인 정렬에 사용하면 [5]에서는 이 새로운 데이터가 눈에 띄게 더 나은 결과를 가져오지 않는다는 것을 알 수 있습니다. 아래 그림에 표시된 바와 같이, 오프라인 알고리즘은 오프라인 데이터셋과 온라인 훈련 중에 온-정책 샘플링을 통해 생성된 섞인 데이터셋을 사용하여 유사하게 수행됩니다.

<center>
<img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F6263e804-9457-4185-8737-142277d1217e_1000x500.png" alt="Offline algorithm performance with shuffled data">
</center>
<center>(출처: [5])</center>

이러한 결과는 데이터 커버리지를 개선하는 것만으로는 오프라인 정렬의 성능 한계를 극복하기에 충분하지 않다는 것을 보여줍니다. 데이터 순서도 중요합니다. 그러나 이 순서가 완벽할 필요는 없습니다. 온-정책 샘플에서 섞는 양을 점진적으로 증가시켜도 모델 성능은 어느 시점까지 안정적으로 유지되다가 오프라인 정렬에서 관찰된 수준으로 급격히 저하됩니다.

<center>
<p>"오프라인 알고리즘은 온라인 알고리즘과 동일한 데이터 커버리지로 보강되더라도 동일한 수준의 성능을 얻을 수 없습니다. 이는 끊임없이 진화하는 정책에 의한 온-정책 샘플링을 통해 얻은 정확한 샘플링 순서의 중요성을 시사합니다." - 출처: [5]</p>
</center>

**최적 이하 데이터.** 우리는 고품질로 알려진 정책을 사용하여 선호 데이터셋을 생성함으로써 오프라인 정렬 알고리즘에 대한 데이터 품질의 영향을 쉽게 테스트할 수 있습니다. [5]에서 저자들은 온라인 정렬을 통해 얻은 최종 정책을 사용하여 오프라인 훈련 데이터셋을 생성합니다. 이 데이터셋에 대해 정책을 훈련할 때 품질이 약간만 향상됩니다. 아래를 참조하세요. 이러한 결과는 오프라인 정렬 알고리즘의 한계가 단순히 선호 데이터셋에 저품질 완성이 존재하기 때문이 아니라는 것을 나타냅니다.

<center>
<img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F6263e804-9457-4185-8737-142277d1217e_1000x500.png" alt="Impact of data quality on offline alignment">
</center>
<center>(출처: [5])</center>

**분류 정확도.** [5]의 저자들은 온라인 정렬 알고리즘이 사용하는 명시적인 보상 모델이 오프라인 정책의 암묵적 보상 추정치보다 더 높은 선호 분류 정확도(preference classification accuracy)를 달성한다는 것을 보여줍니다. 그러나 선호 분류 정확도와 모델 성능 사이에는 거의 상관관계가 발견되지 않았습니다. 사실, 관찰된 유일한 상관관계는 약간 음수였습니다. 이러한 발견을 바탕으로 저자들은 온라인 알고리즘의 명시적인 보상 모델의 우수한 선호 분류 정확도가 온라인 정렬 방법의 향상된 성능 뒤에 있는 주요 요인일 가능성이 낮다고 결론 내립니다.

**대조 목표.** 오프라인 정렬 알고리즘의 저조한 성능이 대조 손실 함수 사용에서 비롯되는지 연구하기 위해 저자들은 Best-of-2라고 불리는 오프라인 정렬을 위한 비대조 손실(non-contrastive loss)을 도출합니다. 간단히 말해, Best-of-2 훈련 알고리즘은 데이터셋의 각 선호 쌍에 대해 선택된 완성을 가져와 이 완성에 대해 SFT를 실행합니다. 표준 오프라인 선호 데이터셋에 대해 Best-of-2 손실을 사용하여 모델을 훈련할 때 성능에 눈에 띄는 변화는 없습니다. 그러나 Best-of-2 훈련에 온라인 샘플을 추가하면(이 샘플이 온라인 정렬의 순서를 제거하기 위해 섞여 있더라도) 온라인 기술과의 성능 격차가 거의 사라집니다. 아래를 참조하세요.

<center>
<img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F6263e804-9457-4185-8737-142277d1217e_1000x500.png" alt="Best-of-2 training with online samples">
</center>
<center>(출처: [5])</center>

이러한 결과는 데이터 커버리지가 SFT 성공의 핵심 지표임을 명확하게 보여주며, SFT에 온-정책 샘플을 포함하는 것(즉, 거부 샘플링)을 동기 부여합니다. 오프라인 훈련 알고리즘에 어느 정도의 온-정책 데이터를 포함하기만 해도 인상적인 정렬 결과를 얻을 수 있으며, 이는 실용적으로 효과적이고 구현하기 쉬운 LLM 정렬 기준선(baselines)을 형성합니다.

**스케일업(scaling up)이 도움이 되는가?** 저자들은 [5]에서 온라인 및 오프라인 정렬 알고리즘 간의 격차에 대한 모델 규모의 영향을 연구함으로써 분석을 마칩니다. 이 실험에서 우리는 오프라인 및 온라인 알고리즘 간의 격차가 다음을 보여줍니다.

*   더 큰 규모에서 감소합니다.
*   더 큰 규모에서 데이터 커버리지와 더 밀접하게 관련됩니다.

더 구체적으로, 섞인 온-정책 샘플 데이터셋에 대해 더 큰 모델을 훈련하면 온라인-오프라인 성능 격차가 거의 사라집니다. 아래를 참조하세요. 이러한 발견은 더 작은 모델을 사용한 데이터 커버리지 실험에서는 유지되지 않았습니다.

<center>
<img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F6263e804-9457-4185-8737-142277d1217e_1000x500.png" alt="Impact of model scale on performance gap">
</center>
<center>(출처: [5])</center>

**핵심 요점.** [5]의 상세한 정렬 분석은 우리에게 한 가지 핵심 발견을 남깁니다. 온-정책 샘플링은 고품질 정렬에 중요합니다. 온라인 정렬 알고리즘의 우수성에 대한 많은 대안적 설명(예: 데이터 커버리지 또는 품질)이 있습니다. 그러나 이러한 이론은 [5]의 많은 데이터 제거 연구에 의해(적어도 작은 규모에서는) 반박되었으며, 온-정책 샘플이 온라인-오프라인 성능 격차의 핵심 기여자임을 밝혀냈습니다. 이러한 발견은 매우 강력합니다. 왜냐하면 오프라인 정렬 알고리즘에 사용되는 데이터 샘플링 과정을 모델의 범용성을 확장하는 데 필수적인 통찰력을 제공하기 때문입니다. 아래에 설명된 바와 같이 (준-)온라인 데이터 샘플을 통합함으로써 오프라인 기술의 성능을 향상시킬 수 있습니다!

<center>
<p>"온라인 대 오프라인의 이분법은 실제로는 종종 부정확합니다. 반복적으로 업데이트되는 데이터 스트림을 가진 오프라인 알고리즘은 사실상 온라인 알고리즘이기 때문입니다. 결과적으로, 오프라인 학습은 일반적으로 데이터 생성 과정에 더 주의를 기울임으로써 이 연구에서 확인된 단점으로부터 덜 고통받을 수 있습니다." - 출처: [5]</p>
</center>

### LLM을 위한 오프라인 및 온라인 강화 학습 연결 [9]

"우리는 검증 가능한 작업과 검증 불가능한 작업 모두에서 오프라인, 준-온라인 및 온라인 구성을 연구합니다. 오프라인에서 온라인 훈련으로의 전환(즉, 주기적인 모델 동기화 속도 변경)을 검토함으로써, 이러한 방법이 성능 및 효율성 향상을 위해 어떻게 최적화될 수 있는지 이해하는 것을 목표로 합니다." - 출처: [9]

온라인 및 오프라인 RL 간의 관계를 세분화하여 연구하기 위해 [9]의 저자들은 훈련 과정을 오프라인에서 온라인 설정으로 원활하게 전환하면서 LLM을 미세 조정합니다. 다시 말해, 중간에 해당하는 훈련 기술을 테스트함으로써 온라인 및 오프라인 RL 간의 격차를 연결합니다. 검증 가능한(예: 수학) 및 검증 불가능한(예: 채팅 또는 명령 따르기) 도메인 모두에서 이러한 테스트를 수행함으로써 온-정책 샘플링이 RL 훈련 과정에 어떻게 영향을 미치는지 이해할 수 있습니다. 더 구체적으로, 온-정책 GRPO 설정을 오프라인, 준-온라인 및 온-정책 DPO 변형과 비교할 때 다음을 알 수 있습니다.

*   온라인 및 준-온라인 기술은 오프라인 훈련보다 성능이 훨씬 우수합니다.
*   준-온라인 DPO는 온라인 DPO의 성능과 거의 일치합니다.

간단히 말해, [9]에서는 온라인 훈련이 모델 성능에 유익하지만, 더 효율적인 준-온라인 접근 방식으로 이러한 이점의 대부분을 얻을 수 있다는 것을 알 수 있습니다.

**온라인, 준-온라인 및 오프라인.** [9]의 실험을 위해 저자들은 온-정책 GRPO와 DPO의 여러 변형을 모두 사용하여 Llama-3.1-8b-Instruct 모델을 훈련합니다. 구체적으로, 우리는 s라는 기간을 정의하여 훈련 중인 정책이 s 훈련 반복마다 DPO를 위한 새로운 온-정책 샘플을 생성하는 데 사용되도록 함으로써 다양한 정도의 온-정책 샘플링을 가진 DPO 변형을 만들 수 있습니다. 다시 말해, 우리는 s 매개변수 업데이트마다 훈련 중인 정책의 매개변수와 선호 데이터를 위한 완성을 샘플링하는 데 사용되는 정책의 매개변수를 동기화합니다. 아래를 참조하세요.

<center>
<img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F6263e804-9457-4185-8737-142277d1217e_1000x500.png" alt="DPO variants with varying on-policy sampling">
</center>
<center>(출처: [9])</center>

특히, 각 반복에서 현재 모델로 훈련을 위한 새로운 완성 세트를 생성하는 반복적인 형태의 DPO는 이전 연구 [10, 11]에서 탐구되었습니다. 그러나 이러한 방법은 일반적으로 새로운 완성이 상대적으로 드물게 샘플링되는 대략적인 반복을 수행합니다. s 설정을 변경함으로써 우리는 완전히 온-정책 DPO 설정(s = 1)을 포함하여 임의의 세분화된 준-온라인 DPO를 탐색할 수 있습니다. 간단히 말해, s를 무한대(∞)에서 1로 천천히 감소시킴으로써 오프라인, 준-온라인 및 온라인 DPO 간의 격차를 연결할 수 있습니다.

**실험 설정.** 실험은 두 가지 가능한 도메인에서 수행됩니다.

*   훈련 데이터가 WildChat-1M10에서 가져오고 모델이 LLM 심사관을 통해 채팅 기능(예: AlpacaEval 및 Arena-Hard 사용) 측면에서 평가되는 검증 불가능한 도메인.
*   훈련 데이터가 NuminaMath 데이터셋에서 가져오고 평가가 여러 검증 가능한 수학 벤치마크(예: Math500 및 AMC23)에서 수행되는 수학 중심의 검증 가능한 도메인.

검증 가능한 도메인에서 보상 신호는 정확한 문자열 일치(exact string matching) 대신 Math-Verify 툴킷을 사용하여 얻어지며, 이는 답변 형식의 변화에 대해 보상을 더 강력하게 만듭니다.11 검증 불가능한 보상은 모든 실험에서 고정된 상용 인간 선호 보상 모델(특히 Athene-RM-8b)에서 파생됩니다. 검증 가능한 도메인에서 DPO를 적용하기 위해 각 질문에 대해 여러 응답을 생성한 다음, 각 질문에 대해 하나의 올바른 답변과 하나의 잘못된 답변을 선택하여 선호 쌍을 형성합니다.

<center>
<img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F6263e804-9457-4185-8737-142277d1217e_1000x500.png" alt="Results of experiments on verifiable and non-verifiable tasks">
</center>
<center>(출처: [9])</center>

**준-온라인으로 충분한가?** 검증 가능한 작업과 검증 불가능한 작업 모두에 대한 이러한 실험 결과는 위에 표시되어 있습니다. 즉시, 우리는 온라인 또는 준-온라인 설정으로 훈련하는 것이 두 도메인 모두에서 오프라인 DPO에 비해 상당한 이득을 제공한다는 것을 알 수 있습니다. 오프라인 방법과 온라인 방법 사이에는 명확한 성능 격차가 있습니다. 그러나 온라인 설정과 준-온라인 설정 간의 격차는 훨씬 덜 두드러집니다. 사실, 온라인 및 준-온라인 DPO는 일부 경우 온-정책 GRPO보다도 예상치 못한 효율성과 성능을 달성할 수 있습니다! 이러한 발견은 상대적으로 큰 s 값에서도 유효합니다. 예를 들어, 검증 가능한 도메인에서는 s가 100으로 증가해도 매우 유망한 결과가 나타납니다.12

<center>
<p>"준-온라인 변형의 효율성 이득은 완전한 온라인 RL이 LLM 사후 훈련을 위한 유일한 접근 방식인지에 대한 흥미로운 질문을 제기합니다." - 출처: [9]</p>
</center>

이러한 발견은 RL의 온라인-오프라인 성능 격차에 대한 흥미로운 함의를 가집니다. [9]에서는 온라인 샘플링에 명확한 이점이 있음을 알 수 있습니다. 그러나 엄격한 온-정책 샘플링 대신 간헐적으로 새로운 데이터를 수집하는 준-온라인 설정을 통해 이 샘플링을 잠재적으로 더 효율적으로 근사할 수 있습니다.

**검증 가능 대 검증 불가능.** [9]에서는 검증 가능한 보상과 검증 불가능한 보상 간의 상호 작용을 탐색하기 위한 실험도 수행되었으며, RL 훈련 중 보상의 커리큘럼(curriculum, 또는 순서)이 중요하다는 것을 보여줍니다. LLM이 먼저 검증 불가능한 보상으로 훈련된 다음 검증 가능한 보상으로 훈련되는 설정(NV → V)과 그 반대(V → NV)를 비교하면, 먼저 검증 불가능한 보상으로 훈련할 때 더 나은 성능을 얻습니다(즉, NV → V » V → NV). LLM이 검증 가능한 보상으로 훈련된 후 검증 불가능한 보상으로 추가 훈련하는 것은 검증 가능한 도메인에서 눈에 띄는 성능 저하를 초래합니다. 대조적으로, 검증 가능한 보상으로 추가 훈련하는 것은 검증 불가능한 도메인에서도 LLM의 성능을 실제로 향상시킵니다. 위를 참조하세요. 검증 불가능한 보상과 검증 가능한 보상을 단일 훈련 실행(V + NV) 내에서 결합하면 모델도 잘 수행되며, 이는 가장 간단한 접근 방식이 이질적인 보상 신호를 단일, 통합 훈련 실행으로 혼합하는 것일 수 있음을 보여줍니다!

### 결론

LLM을 위한 많은 정렬 알고리즘이 있으며, 지속적인 연구를 통해 끊임없이 발전하고 있습니다. 각 알고리즘은 복잡성과 성능 면에서 다양합니다. 온라인 알고리즘은 오프라인 정렬 알고리즘에 비해 명확한 성능 이점을 가집니다. 이 개요에서 우리는 이러한 성능 격차가 주로 온라인 정렬 알고리즘에서 온-정책 샘플링을 사용하는 것과 음의 경사와 같은 다른(논란의 여지는 있지만 덜 중요한) 요인에서 비롯된다는 것을 알게 되었습니다. 그러나 흥미롭게도, 우리는 오프라인 정렬에 사용되는 훈련 데이터셋에 온-정책 샘플을 포함함으로써 훨씬 더 간단하고 동등하게 효과적인 정렬 알고리즘을 도출할 수 있으며, 이는 실용적으로 효과적이고 구현하기 쉬운 준-온라인 알고리즘을 형성한다는 것을 배웠습니다.

뉴스레터가 처음이신가요? 안녕하세요! 저는 Cameron R. Wolfe입니다. 넷플릭스의 딥러닝 박사이자 선임 연구 과학자입니다. 이것은 Deep (Learning) Focus 뉴스레터이며, 독자들이 AI 연구의 중요한 주제를 더 잘 이해하도록 돕습니다. 뉴스레터는 항상 무료로 공개될 것입니다. 뉴스레터가 마음에 드시면 구독하시거나, 유료 구독을 고려하시거나, 공유하시거나, X와 LinkedIn에서 저를 팔로우해주세요! 구독하기

### 참고문헌(Bibliography)

[1] Rafailov, Rafael, et al. "Direct preference optimization: Your language model is secretly a reward model." Advances in neural information processing systems 36 (2023): 53728-53741.
[2] Ivison, Hamish, et al. "Unpacking dpo and ppo: Disentangling best practices for learning from preference feedback." Advances in neural information processing systems 37 (2024): 36602-36633.
[3] Ivison, Hamish, et al. "Camels in a changing climate: Enhancing lm adaptation with tulu 2." arXiv preprint arXiv:2311.10702 (2023).
[4] Tunstall, Lewis, et al. "Zephyr: Direct distillation of lm alignment." arXiv preprint arXiv:2310.16944 (2023).
[5] Tang, Yunhao, et al. "Understanding the performance gap between online and offline alignment algorithms." arXiv preprint arXiv:2405.08448 (2024).
[6] Xu, Shusheng, et al. "Is dpo superior to ppo for llm alignment? a comprehensive study." arXiv preprint arXiv:2404.10719 (2024).
[7] Tajwar, Fahim, et al. "Preference fine-tuning of llms should leverage suboptimal, on-policy data." arXiv preprint arXiv:2404.14367 (2024).
[8] Azar, Mohammad Gheshlaghi, et al. "A general theoretical paradigm to understand learning from human preferences." International Conference on Artificial Intelligence and Statistics. PMLR, 2024.
[9] Lanchantin, Jack, et al. "Bridging Offline and Online Reinforcement Learning for LLMs." arXiv preprint arXiv:2506.21495 (2025).
[10] Yuan, Weizhe, et al. "Self-rewarding language models." arXiv preprint arXiv:2401.10020 3 (2024).
[11] Pang, Richard Yuanzhe, et al. "Iterative reasoning preference optimization." Advances in Neural Information Processing Systems 37 (2024): 116617-116637.
[12] Shao, Zhihong, et al. "Deepseekmath: Pushing the limits of mathematical reasoning in open language models." arXiv preprint arXiv:2402.03300 (2024).
[13] Mukobi, Gabriel, et al. "Superhf: Supervised iterative learning from human feedback." arXiv preprint arXiv:2310.16763 (2023).
[14] Gulcehre, Caglar, et al. "Reinforced self-training (rest) for language modeling." arXiv preprint arXiv:2308.08998 (2023).
[15] Hu, Jian, et al. "Aligning language models with offline learning from human feedback." arXiv preprint arXiv:2308.12050 (2023).
[16] Lambert, Nathan, et al. "Tulu 3: Pushing frontiers in open language model post-training." arXiv preprint arXiv:2411.15124 (2024).
[17] Ouyang, Long, et al. "Training language models to follow instructions with human feedback." Advances in neural information processing systems 35 (2022): 27730-27744.
[18] Rafailov, Rafael, et al. "Direct preference optimization: Your language model is secretly a reward model." Advances in neural information processing systems 36 (2023): 53728-53741.
[19] Ethayarajh, Kawin, et al. "Kto: Model alignment as prospect theoretic optimization." arXiv preprint arXiv:2402.01306 (2024).
[20] Xu, Haoran, et al. "Contrastive preference optimization: Pushing the boundaries of llm performance in machine translation." arXiv preprint arXiv:2401.08417 (2024).
[21] Huang, Shengyi, et al. "The n+ implementation details of rlhf with ppo: A case study on tl; dr summarization." arXiv preprint arXiv:2403.17031 (2024).

1 이는 완성 전용 손실 콜레이터(completion-only loss collator)를 사용하여 달성할 수 있습니다.
2 이 선택을 수행하는 몇 가지 다른 방법이 있습니다. 예를 들어, 각 프롬프트에 대해 가장 좋은 완성을 선택하거나, 모든 프롬프트에서 가장 높은 점수를 받은 완성을 선택할 수 있습니다. 자세한 내용은 여기를 참조하세요.
3 즉, 출력은 8가지 가능한 결과에 대한 확률 분포를 형성하기 위해 소프트맥스 함수(softmax function)가 적용된 8차원 벡터입니다.
4 GRPO는 [7]과 GRPO 논문 [12]가 매우 비슷한 시기에 출판되었기 때문에 이 표에 나열되지 않았습니다.
5 정렬 전의 LLM은 이미 평균 길이에 가까운 완성을 생성합니다. 대조적으로, LLM은 최소(또는 0) 길이 완성을 생성하지 않으므로, 이러한 응답을 생성하는 방법을 학습하려면 이전에 가능성이 낮았던 새로운 영역으로 확률 질량을 이동해야 합니다.
6 이러한 민감성은 최대 우도 알고리즘이 오프-정책 샘플링으로부터 보호하기 위한 명시적인 메커니즘이 없는 반면, PPO는 신뢰 영역(trust region)을 유지하는 데 도움이 되는 클리핑 연산(clipping operation)과 KL 발산을 가지고 있기 때문입니다.
7 웹 스크래핑을 통해 선호 데이터를 얻는 방법의 예시로, Stack Exchange Preferences 데이터셋은 두 개 이상의 답변이 있는 Stack Overflow의 질문을 가져와 암묵적 피드백(예: 좋아요 또는 추천)을 기반으로 답변 순위를 매깁니다.
8 구체적으로, 이 연구는 OpenAI 요약, Anthropic Helpful and Harmless (hh-rlhf), 그리고 Chatbot arena 선호 데이터셋을 사용합니다.
9 온라인 알고리즘에 대해서도 유사한 주장을 할 수 있다는 점에 유의해야 합니다! 온라인 알고리즘에 사용되는 보상 모델도 고정된 데이터셋에 대해 훈련되므로 온라인 알고리즘의 성능에 유사한 한계를 초래할 수 있습니다.
10 이는 ChatGPT와의 약 100만 건의 사용자 상호 작용으로 구성된 일반 채팅 및 명령 따르기 벤치마크입니다.
11 예를 들어, LLM은 수학 질문에 0.5 또는 1/2이라는 답변을 제공할 수 있습니다. 이 두 답변 모두 올바르지만, 정확한 문자열 일치를 통해 보상을 검증한다면 그 중 하나는 틀린 것으로 표시될 가능성이 높습니다. 이러한 이유로 수학적 표현에 대한 더 강력한 검증 시스템을 사용하는 것이 도움이 됩니다.
12 검증 가능한 도메인에서 s의 값은 검증 불가능한 도메인에 비해 훨씬 큽니다. [9]의 저자들은 검증 불가능한 데이터셋이 작고 s = 32 설정이 데이터에 대한 전체 에포크(epoch)를 포괄하기 때문에 이러한 선택을 합니다. 따라서 검증 불가능한 도메인에서 s 값이 클수록 훈련 과정이 안정적이지 않습니다.