AI의 모든 획기적인 발전은 핵심적인 스케일링 통찰력에 의해 주도되어 왔습니다. 과거 무어의 법칙(Moore's Law)이 황의 법칙(Huang's Law, 실리콘 기반 컴퓨팅 성능 발전)으로 대체되었고, Kaplan 등의 데이터 스케일링 연구는 Hoffman 등의 후속 연구(데이터 효율성)로 이어졌습니다. 또한, AlexNet의 등장은 딥러닝과 머신러닝을 위한 GPU 활용 혁명(사전 훈련)의 서막을 열었습니다. 2024년 o1 모델의 성공적인 출시와 함께 DeepSeek, Anthropic, GDM 등 주요 AI 연구 기관들이 연이어 테스트 시간 컴퓨팅(test time compute) 확장 패러다임의 중요성을 입증하면서, 우리는 이제 이 시대에 본격적으로 진입했습니다. 이 패러다임은 2022년 친칠라 스케일링 법칙(Chinchilla scaling law) 이후 LLM 연구에서 가장 중요한 전환점이자 핵심 동력으로 평가받고 있습니다. "이것은 2022년의 오리지널 친칠라 스케일링 법칙(Chinchilla scaling law) 이후 LLM 연구에서 가장 중요한 수치일 수 있습니다." — 짐 팬(Jim Fan). 추론(reasoning) 분야에서 세계적인 권위를 가진 노암 브라운(Noam Brown)은 o1 출시 영상과 시스템 카드에서도 공동 연구자로 이름을 올렸으며, TED 강연, OpenAI 영상, 그리고 다수의 유명 AI 팟캐스트를 통해 그의 통찰력을 공유해왔습니다. 그가 AI 엔지니어 청중을 위해 심도 깊은 이야기를 나누고자 저희 팟캐스트에 귀한 시간을 내어주신 것에 깊이 감사드립니다. 이제 그의 뛰어난 업적에 대한 찬사는 잠시 멈추고, 오늘 팟캐스트의 주요 내용을 요약한 저희의 노트를 살펴보겠습니다.

### 추론의 본질과 발전

**추론은 창발적(emergent)입니다**: 비추론 모델과 추론 모델을 구분하는 "생각에 관한 생각(Thinking Fast and Slow)"의 시스템 1(System 1)과 시스템 2(System 2) 모델 개념은 이제 널리 이해되고 있습니다. 또한, 단 1 단위의 테스트 시간 컴퓨팅이 1000배에서 10,000배에 달하는 모델 규모 확장과 동일한 효과를 낼 수 있다는 비대칭성 역시 잘 알려져 있습니다. 그러나 **덜** 인지되는 사실은, 이러한 추론 능력이 GPT-4 이후에야 비로소 실질적으로 발현될 수 있었다는 점입니다. 짐 팬이 강조했듯이, **시스템 1과 시스템 2, 두 가지 모두의 확장이 필수적입니다**.

노암은 "모델, 즉 사전 훈련된 모델이 이러한 추가적인 사고 과정(추론)을 통해 진정한 이점을 얻기 위해서는 특정 수준의 기본적인 역량을 갖추어야 한다"고 언급했습니다. 그는 "이것이 바로 추론 패러다임이 특정 시점에 나타난 이유"라고 설명하며, "더 일찍 나타날 수도 있었겠지만, GPT-2와 같은 초기 모델에 추론 패러다임을 적용했다면 거의 아무런 성과도 얻지 못했을 것"이라고 덧붙였습니다. 비둘기에게 체스를 가르치려 해도 천 년을 생각한다고 해서 체스 실력이 향상되지 않는 것처럼, AI 모델 역시 시스템 2 추론의 효과를 보기 위해서는 시스템 1 차원에서 일정 수준의 지적 능력이 선행되어야 한다는 비유를 들었습니다.

GPT-3 이후, 2021년에 일리야(Ilya Sutskever)가 테스트 시간 컴퓨팅 연구를 위해 'GPT-Zero'라는 코드명의 프로젝트를 진행했다는 소문은 널리 알려지지 않았지만, 팟캐스트에서 밝혀진 놀라운 사실은 일리야가 노암에게 추론 기반 LLM의 실현 가능성이 생각보다 가깝다는 확신을 주었다는 것입니다. 노암은 원래 추론 패러다임을 알아내는 데 오랜 시간이 걸릴 것이라고 예상했으나, 일리야는 "어쩌면 그렇게 어렵지 않을 수도 있다"는 견해를 피력했습니다.

이러한 맥락에서, (제거되지 않은) 가설은 GPT-3에서 곧바로 o1으로의 도약은 불가능했으며, GPT-4와 4o가 먼저 기준선 역할을 수행해야 했다는 것입니다.

**추론은 AI 얼라인먼트(alignment)에 기여합니다**. 안전성(Safety), 조종 가능성(steerability), 그리고 얼라인먼트는 AI 커뮤니티의 중요한 화두이며, 추론은 이 분야에 놀랍게도 긍정적인 영향을 미칩니다. 노암은 Cicero의 출시 사례를 들며, "Cicero는 특정 행동에 조건화되어 매우 제어 가능한 시스템이었고, 이는 모델이 명확하게 해석 가능한 행동을 추구하도록 하는 높은 조종 가능성을 부여했다"고 설명했습니다. 이러한 시스템은 단순한 언어 모델이 제멋대로 작동하는 것이 아니라, 인간과의 상호작용 방식을 조종하는 정교한 추론 시스템을 내포하고 있습니다. 많은 연구자들이 이러한 접근 방식이 AI 시스템의 안전성을 확보하는 효과적인 방법이 될 수 있다고 평가했습니다.

**추론은 검증 가능한 보상(verifiable rewards)을 넘어 일반화됩니다**. 검증 가능한 보상으로부터의 강화 학습(RLVR)에 대한 흔한 비판 중 하나는 수학 및 코딩과 같은 명확한 영역에서만 모델을 개선한다는 것입니다. 이에 대해 노암은 "이러한 인식이 흔하다는 것에 놀랐다"고 반박했습니다. 그는 Deep Research와 같은 프로젝트가 성공에 대한 명확한 지표가 없는 영역에서도 뛰어난 성능을 보이며, 이는 추론 모델이 쉽게 검증 가능한 보상이 없는 복잡한 작업에서도 성공할 수 있다는 강력한 존재 증명(existence proof)이라고 강조했습니다.

**시각적 추론(Visual Reasoning)에는 여전히 한계가 있습니다**. O3 모델이 마스터 레벨의 Geoguessr 플레이어를 능가하는 성과에 대한 기대감이 높았지만, 노암은 이러한 시각적 추론에도 한계가 있음을 지적했습니다. 그는 "시스템 2 추론의 혜택을 크게 받지 못하는 질문들도 있다"며, GeoGuessr는 혜택을 받는 좋은 예시인 반면, 정보 검색(information retrieval)과 같이 단순히 '알거나 모르는' 유형의 질문에는 시스템 2의 역할이 제한적이라고 설명했습니다. 웹 접근 없이 특정 인물의 생년월일을 알아내려 할 때, 아무리 오래 생각해도 정보를 알지 못하면 정확한 답을 얻기 어려운 것과 같은 이치입니다.

**추론은 OpenAI 내부에서 과소평가되었지만 데이터 장벽(Data Wall)에 직면하며 그 중요성이 부각되었습니다**. 초기에는 그 "추가적인 패러다임"이 무엇인지에 대한 논쟁이 많았습니다. 많은 연구자들이 추론과 강화 학습(RL)에 주목했으나, 이는 주로 데이터 효율성에 관한 것이었습니다. 당시에는 컴퓨팅 자원은 풍부하지만 데이터가 더 큰 제약이라는 인식이 지배적이었고, 컴퓨팅 한계에 도달하기 전에 데이터 장벽에 부딪힐 것이라는 우려가 있었습니다. 이러한 맥락에서, 추론 모델은 단순히 데이터 효율성을 높일 뿐만 아니라, 컴퓨팅 자원을 대규모로 확장하는 것과 동등한 효과를 낼 수 있다는 점이 밝혀졌습니다. 노암은 o1 발표 전 OpenAI를 떠나 경쟁사로 이직한 한 연구원의 일화를 소개하며, 초기에는 추론 모델의 중요성을 간과했지만, o1 출시 후 업계의 폭발적인 반응을 보고서야 그 가치를 깨달았다고 언급했습니다. 돌이켜보면 명백해 보이는 발전도 당시에는 그 진정한 의미를 파악하기 어려웠던 것입니다.

**추론 + Windsurf = AGI를 경험하다.**
Q: Windsurf를 깊이 사용해보신 입장에서, 프로 팁이 있다면 무엇인가요?
A: 제가 놀란 점 중 하나는 O3의 존재를 모르는 사람들이 생각보다 많다는 것입니다. 저는 매일 O3를 사용하며, **사실상 저에게는 구글 검색을 완전히 대체했습니다**. 항상 O3를 활용하고 있으며, 코딩 작업을 할 때도 **주로 추론 모델을 사용합니다**. 아직 추론 모델을 경험해보지 못한 분들께 적극 추천합니다. 솔직히, 이 모델을 사용하는 사람들은 정말 만족해합니다. 물론 GPT-4.0과 ChatGPT의 기본 기능을 사용하는 사람이 훨씬 많겠지만, 추론 모델을 시도해보면 그 능력에 놀랄 것이라고 확신합니다.

### 테스트 시간 컴퓨팅, 다음 단계의 도전과 기회

테스트 시간 컴퓨팅의 확장은 AI 모델의 능력을 비약적으로 향상시키고 있지만, 동시에 새로운 도전 과제들을 제시하고 있습니다. 2025년 현재, 우리는 모델들이 3분에서 3시간, 나아가 3일, 3주 동안 사고하도록 이끌고 있습니다. 이러한 장기적 사고의 스케일링에는 두 가지 주요 우려가 존재합니다. 첫째, 모델이 더 오랫동안 사고하고 테스트 시간 컴퓨팅을 확장하는 데 드는 비용이 기하급수적으로 증가한다는 점입니다. 이는 궁극적으로 지출 가능한 금액에 한계를 설정하며, 잠재적인 상한선으로 작용합니다. 물론, 모델들이 사고하는 방식이 점차 효율적으로 개선되고 있어 동일한 테스트 시간 컴퓨팅으로 더 많은 작업을 수행할 수 있다는 점은 간과할 수 없는 중요한 진전입니다. 이는 단순히 모델의 사고 시간을 늘리는 것을 넘어, 사고의 질적 향상도 동반하고 있음을 의미합니다.

두 번째 우려는 모델의 사고 시간이 길어질수록 실제 경과 시간(wall-clock time)에 의한 병목 현상이 발생한다는 점입니다. 모델이 즉각적으로 응답할 때는 실험을 반복하는 것이 매우 쉽지만, 응답에 3시간이 걸린다면 훨씬 어려워집니다. 만약 모델이 3주 동안 생각해야 한다면 어떨까요? 평가를 수행하고 그 결과를 바탕으로 다음 단계를 반복하는 데 최소 3주가 소요될 것입니다. 물론 실험의 일부는 병렬화할 수 있지만, 많은 경우 하나의 실험을 완료하고 결과를 확인해야만 다음 실험을 계획할 수 있습니다. 이러한 시간적 제약은 장기적인 AI 개발 타임라인에 가장 강력한 영향을 미 미칠 수 있습니다. 특히 신약 개발과 같이 결과 확인에 오랜 시간이 필요한 도메인에서는 이러한 병목 현상이 더욱 심각하게 작용할 수 있습니다. 예를 들어, 새로운 약물이 인간 수명 연장에 효과가 있고 부작용이 없는지 확인하는 데는 막대한 시간이 필요합니다.

일각에서는 장기적 강화 학습(long-horizon RL)에 필요한 데이터 확보가 예상보다 훨씬 더 어려운 과제라는 의견도 있습니다. 그럼에도 불구하고, 테스트 시간 스케일링 시대는 Orion 실행이 SG1이 온라인되기 전까지 컴퓨팅 자원을 최대한 활용한 것으로 추정되는 바로 지금, 그 어느 때보다 적절한 시기에 도래했습니다. 2025년 현재, 이러한 비용과 시간의 제약을 극복하기 위한 새로운 기술, 예를 들어 더욱 최적화된 추론 아키텍처나, 효율적인 병렬 처리 기술, 그리고 인간의 피드백을 통해 모델의 사고 과정을 가속화하는 방법론 등이 활발히 연구되고 있습니다. 이러한 노력은 테스트 시간 컴퓨팅의 잠재력을 최대한 발휘하는 데 필수적입니다.

### 다중 에이전트 시스템의 비전

최근 인공지능 분야에서는 다중 에이전트 시스템에 대한 논의가 뜨겁습니다. Cognition이 "다중 에이전트를 만들지 말라"고 주장하는 반면, Anthropic은 "다중 에이전트 구축 방법"에 대한 연구를 공유했습니다. 이 논쟁에 대해 수많은 의견이 오갔지만, 노암 브라운은 수년간 다중 에이전트 강화 학습(Multi-Agent RL) 분야에서 연구를 수행해왔으며, OpenAI에서 다중 에이전트 팀을 이끌고 있습니다. 비록 이 팀의 이름이 연구 범위의 일부분만을 나타내지만, 그들의 비전은 훨씬 더 광범위합니다.

노암은 "팀 이름이 사실 잘못된 명칭"이라고 설명하며, 다중 에이전트 연구 외에도 테스트 시간 컴퓨팅을 대규모로 확장하는 데 주력하고 있다고 밝혔습니다. 현재 모델들이 15분 정도 사고하는 데 그치지만, 목표는 이를 수 시간, 수 일, 심지어 그 이상으로 확장하여 엄청나게 어려운 문제들을 해결하는 것입니다. 다중 에이전트 연구는 또 다른 중요한 방향이며, 여기에는 협력적 측면과 경쟁적 측면 모두에 대한 관심이 있습니다. 노암은 AI 분야에서 종종 인간의 지능 범위가 매우 좁고 AI가 곧 이를 따라잡고 넘어설 것이라는 인식이 있지만, 실제 인간 지능의 범위는 훨씬 넓다고 주장합니다. 원시 시대의 인간과 현대 인류를 비교하며, 수천 년에 걸쳐 수십억 명의 인간이 협력하고 경쟁하며 문명을 건설해온 결과물이 오늘날의 기술이라는 점을 지적했습니다. 그는 오늘날의 AI를 "AI의 원시인"에 비유하며, 수십억 개의 AI가 오랜 시간 동안 협력하고 경쟁하며 "AI 문명"을 건설한다면, 그들이 생산하고 해결할 수 있는 것들은 현재 AI의 능력을 훨씬 뛰어넘을 것이라는 가설을 제시했습니다. 2025년 현재, OpenAI의 다중 에이전트 팀은 이러한 비전을 현실화하기 위해 인간의 피드백을 활용한 학습, 에이전트 간의 효율적인 지식 공유 메커니즘, 그리고 복잡한 상호작용 속에서 장기적인 목표를 달성하는 방법을 모색하는 등 다양한 연구 방향을 탐구하고 있습니다.

### 비터 레슨과 월드 모델의 재해석

**다중 에이전트 분야의 비터 레슨**: 노암은 다중 에이전트 연구에 대한 접근 방식이 역사적으로 그리고 현재 다른 연구기관들에서 이루어지는 방식과 매우 다르다고 강조합니다. 그는 "그동안 많은 접근 방식이 매우 휴리스틱(heuristic)했으며, 스케일링과 연구에 대한 비터 레슨(Bitter Lesson) 접근 방식을 제대로 따르지 않았다"고 비판했습니다. '비터 레슨'은 AI 연구에서 특정 도메인 지식이나 복잡한 휴리스틱에 의존하기보다, 일반적인 학습 방법과 대규모 컴퓨팅 파워를 활용하는 것이 궁극적으로 더 효과적이라는 원칙을 의미합니다.

**비터 레슨 대 월드 모델(World Models) & 얀 르쿤(Yann LeCun)**: 노암은 모델이 커질수록 암묵적으로 월드 모델을 개발하며, 이 월드 모델은 스케일에 비례하여 개선된다는 점이 "꽤 명백하다"고 말했습니다. 따라서 그는 월드 모델을 명시적으로 모델링할 필요는 없다고 주장합니다. 다중 에이전트 AI 커뮤니티에서는 오랫동안, 그리고 현재까지도 다른 에이전트(사람들)를 명시적으로 모델링해야 하는지, 아니면 환경의 일부로서 암묵적으로 모델링될 수 있는지에 대한 논쟁이 이어지고 있습니다. 노암은 오랫동안 명시적 모델링의 필요성을 주장했으나, 시간이 지나면서 생각이 바뀌었다고 고백했습니다. 이제 그는 모델이 충분히 똑똑해지면 마음 이론(theory of mind)과 같은 개념을 자연스럽게 개발하며, 자신들이 행동하고 동기를 가진 에이전트임을 암묵적으로 이해하게 된다고 봅니다. 이러한 능력은 스케일과 전반적인 유능한 행동을 통해 자연스럽게 발달한다는 것이 그의 현재 관점입니다.

### 개방성, 다중 에이전트, 그리고 자가 대국(Self-Play)의 한계

OpenAI는 '약한 것에서 강한 것으로(Weak to Strong)' 문제에 대해 다루었으며, GDM의 개방성(Open-Endedness) 책임자인 팀 록타쉘(Tim Rocktaschel)은 싱가포르 ICLR에서 큰 호평을 받은 기조연설을 통해 다중 에이전트 스케일링이 인간 능력을 뛰어넘는 지점(비터 레슨의 궁극적인 한계)에 대한 질문을 던졌습니다.

"Q: 가장 일관된 연구 결과 중 하나는 항상 AI가 인간의 훈련과 지도보다는 경쟁적인 자가 대국을 통해 더 잘 개선된다는 것입니다. 알파제로(AlphaZero)와 R1 제로에서도 이러한 경향을 볼 수 있습니다. 이것이 다중 에이전트에서도 유효할 것이라고 보시나요? 즉, 인간보다 더 나은 개선을 위해 자가 대국이 효과적일까요?

A: 이것은 매우 중요한 질문이며, 더 자세히 논의할 가치가 있습니다. 오늘날 많은 사람들이 자가 대국을 초지능을 향한 다음 단계이자 최종 단계로 보고 있다고 생각합니다. 알파고(AlphaGo)와 알파제로(AlphaZero)의 사례를 보면, 우리는 매우 유사한 추세를 따르고 있는 것처럼 보입니다. 알파고의 첫 단계는 대규모 사전 훈련(pre-training)이었습니다. 이 경우 인간의 바둑 기보를 사용했습니다. LLM의 경우, 방대한 인터넷 데이터를 사전 훈련에 활용하죠. 이를 통해 강력한 모델을 얻지만, 초인적인 모델은 아닙니다. 알파고 패러다임의 다음 단계는 대규모 테스트 시간 컴퓨팅 또는 대규모 추론 컴퓨팅(inference compute)이었습니다. 이 단계에서는 MCTS(Monte Carlo Tree Search)를 사용했죠. 현재 우리는 이러한 대규모 추론 컴퓨팅을 수행하는 추론 모델들을 가지고 있으며, 이는 모델의 능력을 크게 향상시킵니다. 마지막으로, 알파고와 알파제로에는 자가 대국(self-play)이 있습니다. 모델이 자신과 대결하고, 게임을 통해 학습하며, 점점 더 발전하여 인간 수준을 훨씬 뛰어넘는 능력을 갖추게 됩니다. 현재 바둑 정책은 인간이 이해할 수 없을 정도로 강력하며, 체스도 마찬가지입니다. 하지만 언어 모델에서는 아직 이러한 자가 대국 단계가 없습니다. 그래서 우리는 'AI 모델들이 서로 상호작용하고 학습하면 초지능에 도달할 것이다'라고 생각하고 싶은 강한 유혹을 느낍니다.

그러나 문제는 바둑이 2인 제로섬 게임(two-player zero-sum game)이라는 점입니다. 2인 제로섬 게임은 자가 대국을 통해 최소최대 균형(minimax equilibrium)에 수렴한다는 매우 유용한 특성을 가집니다. 체스, 바둑, 심지어 2인 포커도 모두 2인 제로섬 게임입니다. 일반적으로 우리가 원하는 것은 '미니맥스 균형'이라고 불리는 GTO(Game Theory Optimal) 정책입니다. 이 정책은 어떤 상대에게도 기대값에서 지지 않을 것을 보장합니다. 체스와 바둑에서는 이것이 명확하게 원하는 바입니다. 흥미롭게도 포커에서는 그렇게 명확하지 않습니다. 2인 제로섬 포커에서 GTO 미니맥스 정책을 플레이하면 어떤 상대에게도 지지 않을 것을 보장하지만, 약한 플레이어로부터는 착취적 정책(exploitative policy)을 플레이했을 때만큼 많은 돈을 벌지 못할 것입니다. 따라서 '무엇을 원하는가?'라는 질문이 생깁니다. 가능한 한 많은 돈을 벌고 싶은가, 아니면 어떤 인간에게도 지지 않을 것을 보장하고 싶은가? 게임 AI 개발자들은 미니맥스 정책을 선택하며, 자가 대국은 정확히 이 정책으로 수렴합니다. AI들이 서로 대결하고 실수로부터 배우면, 시간이 지남에 따라 이 미니맥스 정책으로 수렴하는 것이 보장됩니다.

하지만 2인 제로섬 게임의 범위를 벗어나면, 이러한 정책은 더 이상 유용하지 않습니다. 너무 방어적인 정책만을 고수할 필요가 없으며, 수학과 같은 분야에서 동일한 종류의 자가 대국을 시도하면 매우 이상한 행동을 초래할 수 있습니다. 예를 들어, 수학에서 자가 대국을 한다는 것은 무엇을 의미할까요? 한 모델이 매우 어려운 질문을 던지고 다른 모델이 이를 해결하게 하는 방식은 2인 제로섬 게임과 유사합니다. 그러나 문제는 흥미롭지 않은 매우 어려운 질문을 던질 수 있다는 것입니다. 예를 들어, 30자리 곱셈을 요구하는 것은 AI 모델에게 매우 어려운 문제이지만, 이것이 우리가 원하는 방향으로 진전을 이루는 것일까요? 그렇지 않습니다. 따라서 2인 제로섬 게임 밖에서의 자가 대국은 훨씬 더 어렵고 미묘한 문제가 됩니다. 팀 록타쉘 또한 그의 강연에서 비슷한 점을 언급하며, 2인 제로섬 게임 밖에서 자가 대국에 대해 논의할 때 최적화 대상을 결정하는 데 많은 도전 과제가 있다고 지적했습니다. 이는 알파고 비유가 한계를 드러내는 지점입니다."

### 게임 연구와 AI의 미래

포커에서 모든 상대를 이기고, LLM을 활용하여 세계 디플로머시(Diplomacy)에서 상위 10%에 들며, 개인적으로 세계 디플로머시 챔피언십에서 우승하는 등, 게임은 노암 브라운의 사고와 경력에서 매우 중요한 부분을 차지합니다. 하지만 그가 다루는 게임은 단순히 오락을 넘어섭니다.

노암은 "불완전 정보 게임(imperfect information games)을 위한 AI에 대한 방대한 지식을 가지고 있으며, 이는 오랫동안 자신의 연구 분야였다"고 언급했습니다. 그는 노리밋 텍사스 홀덤(No Limit Texas Hold'em)을 위한 초인적인 포커 AI를 개발한 경험을 공유하며, 이 게임의 숨겨진 정보(두 장의 손패)가 실제로는 꽤 제한적이라는 점을 지적했습니다. 헤즈업(heads up) 플레이 시 가능한 상태의 수는 1,326개이며, 이는 다른 플레이어의 수에 따라 곱해지더라도 엄청난 숫자는 아닙니다. 포커 AI는 이러한 가능한 상태들을 모두 열거하고 각 상태에 확률을 할당한 후 신경망에 입력하여 행동을 결정하는 방식으로 작동합니다. 그러나 숨겨진 가능성의 수가 극도로 커지면 이러한 접근 방식은 한계에 부딪힙니다. 예를 들어, 네 장의 숨겨진 카드를 가진 오마하 포커(Omaha poker)나 40개의 말이 있는 스트라테고(Stratego)와 같은 게임에서는 기존 포커 AI 접근 방식이 무너지고 새로운 방법론이 필요합니다.

노암은 이러한 기술들을 스트라테고나 매직 더 개더링(Magic the Gathering)과 같은 게임에 확장하여 적용할 수 있겠지만, 여전히 한계가 있을 것이며 언어 모델로 초인적인 코드포스(Codeforces) 실력을 얻게 해주지는 않을 것이라고 강조합니다. 대신 그는 "매우 일반적인 추론 기술에 집중하는 것이 더 가치 있다"고 주장합니다. 이러한 추론 기술이 개선되면, 언젠가는 매직 더 개더링을 초인적인 수준으로 플레이하는 모델이 '아웃 오브 박스(out of the box)'로 등장할 것이며, 이것이 더 중요하고 인상적인 연구 방향이라고 믿습니다. 2025년 현재, 게임은 단순한 연구 플랫폼을 넘어 AI 모델 자체의 창의성과 전략적 사고를 검증하는 장으로 진화하고 있습니다. 예를 들어, LLM이 새로운 게임 규칙을 설계하거나, 복잡한 게임 환경에서 에이전트의 행동을 시뮬레이션하는 연구가 활발히 진행되고 있으며, 이는 AI의 일반 지능(general intelligence) 발전에 기여할 잠재력을 가지고 있습니다.

**사이드 노트**: 팟캐스트에서 노암이 제안한 LLM이 디플로머시를 자가 대국하는 챌린지는 실제로 AIE 월드 페어(AIE World’s Fair)에서 즉시 구현되었습니다.

팟캐스트에 노암을 모시게 된 것은 저희에게 큰 영광이었습니다. 이 전체 내용을 통해 더 많은 흥미로운 이야기, 정보, 그리고 조언을 얻으셨기를 바랍니다! 혹시 저희가 놓친 중요한 내용이 있다면 언제든 알려주십시오.

### 타임스탬프 (2025년 업데이트 버전)

00:00 소개 – 디플로머시, Cicero & 세계 챔피언십
02:00 리버스 켄타우로스: AI가 노암의 인간 플레이를 어떻게 향상시켰나 (최신 인사이트)
05:00 채팅에서의 튜링 테스트 실패: 환각 & 조종 가능성 (LLM 발전과 함께 재평가)
07:30 추론 모델 & 빠른 사고 vs. 느린 사고 패러다임 (현실적 적용과 한계)
11:00 시각적 과제에서의 시스템 1 vs. 시스템 2 (GeoGuessr, 틱택토)
14:00 검증 불가능한 도메인에 대한 Deep Research의 존재 증명 (최근 성공 사례)
17:30 AI 에이전트의 하네스, 도구 사용, 그리고 취약성 (2025년 현황)
21:00 스캐폴드와 라우터에 대한 과도한 의존에 반대하는 논거 (단일 모델 지향점)
24:00 강화 미세 조정과 장기적 모델 적응성 (RFT의 현재 가치)
28:00 일리야의 추론에 대한 베팅과 O-시리즈의 돌파구 (역사적 관점과 중요성)
34:00 노암의 개발 스택: Codex, Windsurf & AGI의 순간들 (개인적 경험과 추천)
38:00 더 나은 AI 개발자 만들기: 메모리, 재사용, 그리고 PR 리뷰 (AI 개발 사이클 개선)
41:00 다중 에이전트 지능과 "AI 문명" 가설 (OpenAI 팀의 비전)
44:30 스케일링을 통한 암묵적 월드 모델과 마음 이론 (진화하는 AI 인지)
48:00 바둑과 체스를 넘어서면 자가 대국이 무너지는 이유 (복잡한 게임의 도전)
54:00 모호한 과제를 위한 더 나은 벤치마크 설계 (미래 평가 기준)
57:30 테스트 시간 컴퓨팅의 실제 한계: 비용 vs. 시간 (2025년의 현실적 제약)
1:00:30 인간과 LLM 간의 데이터 효율성 격차 (미해결 연구 과제)
1:03:00 훈련 파이프라인: 사전 훈련, 중간 훈련, 사후 훈련 (모델 개발 단계의 진화)
1:05:00 연구 시험장으로서의 게임: 포커, MTG, 스트라테고 (게임 AI 연구의 심화)
1:10:00 맺음말 – 5년 후의 전망과 열린 연구 방향 (미래 AI 로드맵)