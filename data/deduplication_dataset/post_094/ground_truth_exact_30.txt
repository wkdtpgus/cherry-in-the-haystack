AI의 모든 획기적인 발전은 핵심적인 스케일링 통찰력에 의해 주도되었습니다. 무어의 법칙은 황의 법칙(실리콘)에 자리를 내주었고, Kaplan 등의 연구는 Hoffman 등의 연구(데이터 1)로 이어졌으며, AlexNet은 딥러닝과 머신러닝을 위한 GPU 혁명(사전 훈련)을 촉발했습니다. o1 출시에서 드러났듯이, 그리고 곧이어 DeepSeek, Anthropic, GDM이 뒤따르면서, 우리는 이제 테스트 시간 컴퓨팅(test time compute)을 확장하는 시대에 확고히 들어섰습니다. "이것은 2022년의 오리지널 친칠라 스케일링 법칙(Chinchilla scaling law) 이후 LLM 연구에서 가장 중요한 수치일 수 있습니다." — 짐 팬(Jim Fan) 노암 브라운(Noam Brown)은 추론(reasoning) 분야에서 세계 최고의 연구자 중 한 명이며(o1 출시 영상과 o1 시스템 카드에 공동 크레딧을 올렸습니다), 여러분은 그를 TED 무대, OpenAI 영상, 그리고 세계 최고의 AI 팟캐스트들에서 들어보셨을 것입니다. 그래서 그가 시간을 내어 저희 팟캐스트에 참여해 AI 엔지니어 청중을 위해 더 깊이 있는 이야기를 나눠준 것을 영광으로 생각합니다. 그에 대한 칭찬은 이쯤하고 오늘의 팟캐스트 하이라이트로 바로 들어가겠습니다. 아래는 저희의 노트입니다:

### 추론 패러다임의 부상과 진화

AI 분야는 끊임없이 변화하며 새로운 스케일링 법칙과 패러다임을 발견하고 있습니다. 최근 가장 주목받는 흐름 중 하나는 바로 '추론(reasoning)'의 중요성입니다. 단순한 패턴 인식이나 데이터 처리 능력을 넘어, 모델이 복잡한 문제를 해결하기 위해 '생각하는' 과정 자체가 핵심적인 가치로 부상하고 있습니다.

**추론은 창발적(emergent)입니다**: 비추론 모델 대 추론 모델에 대한 "생각에 관한 생각(Thinking Fast and Slow)"의 시스템 1(System 1) 대 시스템 2(System 2) 모델, 그리고 1 단위의 테스트 시간 컴퓨팅이 1000~10,000배 더 큰 규모에 해당한다는 비대칭성은 이제 잘 알려져 있습니다. 하지만 **덜** 알려진 사실은 이것이 GPT-4 이후에야 가능했다는 점입니다. 짐 팬이 말했듯이, **둘 다 확장해야 합니다**.

“제가 생각하기에 과소평가된 한 가지는, 모델, 즉 사전 훈련된 모델이 이러한 추가적인 사고로부터 진정으로 이익을 얻기 위해서는 특정 수준의 능력이 필요하다는 것입니다. 이것이 바로 추론 패러다임이 그 시점에 등장한 이유이기도 합니다. 더 일찍 일어날 수도 있었겠지만, GPT-2 위에 추론 패러다임을 적용하려고 했다면 거의 아무것도 얻지 못했을 거라고 생각합니다… 비둘기에게 체스를 두는 것에 대해 정말 열심히 생각하라고 하면, 그리 멀리 가지 못할 겁니다. 천 년을 생각한다고 해도 체스를 더 잘 두게 될 수는 없을 겁니다. 그래서 아마도 동물과 인간에게도 시스템 2로부터 이익을 얻기 위해서는 시스템 1 측면에서 일정 수준의 지적 능력이 필요한 것 같습니다.”

이러한 통찰은 단순히 모델 크기를 키우는 것만으로는 한계가 있다는 인식에서 비롯됩니다. 초기 LLM들은 방대한 데이터 학습을 통해 놀라운 언어 이해 및 생성 능력을 보여주었지만, 복잡한 논리적 추론이나 다단계 문제 해결에는 여전히 약점을 보였습니다. 이는 마치 인간이 직관적이고 빠른 사고(시스템 1)와 깊고 분석적인 사고(시스템 2)를 모두 사용하는 것과 유사합니다. AI 모델 역시 시스템 1의 기본 역량(사전 훈련된 지식)이 충분히 발달해야 시스템 2의 추론 능력이 효과적으로 발현될 수 있다는 점이 강조됩니다. 초기 모델에서는 연쇄적 사고(Chain-of-Thought) 같은 간단한 추론 프롬프트조차 큰 효과를 보지 못했지만, GPT-3.5와 GPT-4 같은 고성능 모델에서 그 위력이 입증된 것이 대표적인 예입니다.

GPT-3 이후, 2021년에 일리야(Ilya)가 테스트 시간 컴퓨팅을 탐색하기 위해 GPT-Zero라는 코드명의 프로젝트를 진행했다는 것은 잘 알려져 있지도 않고 (확인되지도 않았습니다). 팟캐스트에서 놀라웠던 점 중 하나는, 사실 일리야가 노암에게 그가 생각했던 것보다 추론 LLM이 더 가까운 현실이라고 설득했다는 것입니다. 그 반대가 아니고요.

“…만약 우리가 이 모델들을 훈련시키는 데 1000조 달러가 있다면, 아마도 그렇게 하겠지만, 추론 패러다임이 없다면 초지능에 도달하기 전에 경제적으로 실현 가능한 한계에 부딪힐 겁니다. 그리고 저는 추론 패러다임을 알아내는 데 오랜 시간이 걸릴 것이라고 잘못 확신하고 있었습니다. 왜냐하면 이것은 큰 미해결 연구 문제와 같았기 때문입니다. 일리야는 제게 동의하며 이 추가적인 패러다임이 필요하다고 말했지만, 그의 생각은, **어쩌면 그렇게 어렵지 않을 수도 있다**는 것이었습니다.”

(제거되지 않은) 가설은 우리가 GPT-3에서 o1으로 바로 갈 수 없었고, 기준선으로 GPT-4와 4o가 먼저 필요했다는 것입니다.

이러한 '추론 패러다임'의 발견은 AI 개발의 중요한 전환점이 되었습니다. 특히 데이터 학습의 한계, 즉 '데이터 장벽(Data Wall)'에 대한 우려가 커지면서, AI 모델이 주어진 데이터를 단순히 암기하거나 패턴을 찾는 것을 넘어, 능동적으로 지식을 탐색하고 문제를 해결하는 방식의 필요성이 더욱 부각되었습니다. 이는 무한정 데이터를 수집하고 컴퓨팅 파워를 확장하는 것만으로는 더 이상 진정한 지능에 도달하기 어렵다는 인식과도 맞닿아 있습니다. 추론은 모델이 제한된 데이터와 컴퓨팅 자원 내에서 더 효율적으로 학습하고, 일반화하며, 복잡한 상황에 대처할 수 있도록 돕는 핵심적인 열쇠가 됩니다.

**추론은 OpenAI의 비신봉자들에게 과소평가되었지만 데이터 장벽(Data Wall)에 부딪히면서 탄생했습니다**.

“우선, 그 추가적인 패러다임이 무엇인지에 대해 많은 논쟁이 있었습니다. 많은 연구자들이 추론과 RL을 보았지만, 그것은 테스트 시간 컴퓨팅을 확장하는 것에 대한 것이 아니었습니다. 그것은 데이터 효율성(data efficiency)에 더 가까웠습니다. 왜냐하면, 아시다시피, 우리는 엄청난 양의 컴퓨팅을 가지고 있지만 실제로는 데이터에 더 제한을 받는다는 느낌이 있었기 때문입니다. 그래서 데이터 장벽이 있고, 우리는 컴퓨팅의 한계에 도달하기 전에 그것에 부딪힐 것입니다. 그렇다면 어떻게 이 알고리즘들을 더 데이터 효율적으로 만들 수 있을까요? 그것들은 더 데이터 효율적이지만, 제 생각에는 그것들이 또한 컴퓨팅을 엄청나게 확장하는 것과 동등하다고 생각합니다… 그리고 제가 기억하기에 흥미로웠던 것은, 우리가 추론 패러다임을 발견한 후, 하지만 o1을 발표하기 전에 OpenAI를 떠나 경쟁 연구소로 간 사람과 이야기한 것입니다. 우리가 발표한 후에 그를 만났는데, 그는 당시에는 그 스트로베리 모델(strawberry models)들이 그렇게 대단한 것이라고 생각하지 않았다고 말했습니다. 우리가 실제보다 더 과장하고 있다고 생각했다는 겁니다. 그리고 우리가 o1을 발표하고 그가 이 경쟁 연구소의 동료들의 반응을 보았을 때, 모두가 '이건 대단한 일이다'라고 하는 것을 보고… 그들은 전체 연구 의제를 이것에 집중하도록 전환했습니다… 이 중 많은 것이 돌이켜보면 명백해 보이지만, 당시에는 실제로는 그렇게 명백하지 않았고, 어떤 것을 있는 그대로 인식하기가 꽤 어려울 수 있습니다.”

### 추론과 AI 안전성 및 일반화

**추론은 얼라인먼트(alignment)에 도움이 됩니다**. 안전성(Safety), 조종 가능성(steerability), 그리고 얼라인먼트는 AI 커뮤니티의 특정 부분에서 매우 뜨거운 주제이며, 놀랍게도 추론이 도움이 됩니다:

“저희가 Cicero를 출시한 후, 많은 AI 안전 커뮤니티가 그 연구와 작동 방식에 대해 정말 만족했습니다. 왜냐하면 그것은 매우 제어 가능한 시스템이었기 때문입니다. 저희는 Cicero를 특정 구체적인 행동에 조건화했고, 그것은 '좋아, 이것은 우리가 매우 명확하게 해석할 수 있는 행동을 추구할 것이다'라고 말할 수 있는 많은 조종 가능성을 주었습니다. 그리고 그것은 매우 명확하게 정의되어 있습니다. 그냥 언어 모델이 마음대로 돌아다니며 하고 싶은 대로 하는 것이 아닙니다. 아니요, 실제로는 꽤 조종 가능합니다. 그리고 언어 모델이 인간과 상호 작용하는 방식을 조종하는 이 모든 추론 시스템이 있습니다. 실제로 많은 연구자들이 제게 연락해서 '이것이 이 시스템들로 안전을 달성할 수 있는 잠재적으로 정말 좋은 방법이라고 생각한다'고 말했습니다.”

추론 모델은 단순히 답을 내놓는 것을 넘어, 그 답에 도달하는 과정을 설명할 수 있기 때문에 투명성과 해석 가능성(interpretability) 측면에서 큰 강점을 가집니다. 이는 AI 시스템의 '블랙박스' 문제를 해결하고, 개발자와 사용자 모두가 모델의 행동을 이해하고 예측할 수 있게 하여 안전성 확보에 기여합니다. 예를 들어, 의료 진단 AI가 특정 질병 진단을 내릴 때, 단순히 '질병 X'라고 말하는 것이 아니라, 어떤 증상과 검사 결과들을 바탕으로 어떤 추론 과정을 거쳐 그 결론에 도달했는지 설명할 수 있다면, 의료진은 그 진단을 더 신뢰하고 필요시 개입할 수 있습니다. 이러한 조종 가능성은 AI의 책임감 있는 개발에 필수적인 요소입니다.

**추론은 검증 가능한 보상(verifiable rewards)을 넘어 일반화됩니다**. RLVR(검증 가능한 보상으로부터의 강화 학습)에 대한 한 가지 비판은 수학과 코딩 영역에서만 모델을 개선한다는 것입니다. 노암은 이렇게 답합니다:

“이것이 그렇게 흔한 인식이라는 점에 놀랐습니다. 왜냐하면 저희는 Deep Research를 출시했고 사람들은 그것을 사용해 볼 수 있습니다. 사람들은 그것을 사용하고, 매우 인기가 있습니다. 그리고 그것은 성공에 대한 쉽게 검증 가능한 지표가 없는 영역임이 매우 분명합니다… 그럼에도 불구하고 이 모델들은 이 영역에서 매우 잘하고 있습니다. 그래서 저는 이것이 이 모델들이 쉽게 검증 가능한 보상이 없는 작업에서도 성공할 수 있다는 존재 증명(existence proof)이라고 생각합니다.”

전통적인 강화 학습(Reinforcement Learning, RL)은 보상 함수가 명확하고 객관적으로 측정 가능한 환경, 예를 들어 게임이나 로봇 제어에서 탁월한 성능을 보였습니다. 그러나 현실 세계의 많은 문제들은 성공을 정의하거나 보상을 측정하기가 모호하거나 주관적입니다. Deep Research의 예시처럼, 복잡한 연구 보고서 작성과 같은 작업은 단순히 정답을 맞히는 것을 넘어, 보고서의 깊이, 독창성, 설득력 등 다면적인 품질을 요구합니다. 추론 모델은 이러한 '검증 불가능한' 영역에서도 인간과 유사한 판단력과 창의성을 발휘하여 높은 수준의 결과물을 생산할 수 있음을 보여주었습니다. 이는 AI의 적용 범위를 과학 연구, 예술 창작, 전략 기획 등 인간의 고유 영역으로 확장시키는 중요한 시사점을 제공합니다.

### 테스트 시간 컴퓨팅의 도전과 미래

**테스트 시간 컴퓨팅은 스케일링에 어려움을 겪을 것입니다 2**

“우리는 모델들이 3분 대신 3시간, 그리고 3일, 3주 동안 생각하게 만들 것입니다. 두 가지 우려가 있습니다: 하나는 모델들이 그렇게 오랫동안 생각하게 하거나 테스트 시간 컴퓨팅을 확장하는 것이 훨씬 더 비싸진다는 것입니다. 테스트 시간 컴퓨팅을 확장함에 따라, 테스트 시간 컴퓨팅에 더 많은 비용을 지출하게 되고, 이는 지출할 수 있는 금액에 한계가 있다는 것을 의미합니다. 그것이 하나의 잠재적인 천장입니다. 이 모델들이 생각하는 방식에서 더 효율적이 되고 있어서, 같은 양의 테스트 시간 컴퓨팅으로 더 많은 것을 할 수 있다는 점을 말해야겠습니다. 그리고 저는 이것이 매우 과소평가된 점이라고 생각합니다. 우리가 단지 이 모델들을 더 오래 생각하게 만드는 것만이 아니라는 점입니다. 두 번째 요점은, 이 모델들이 더 오래 생각하게 함에 따라, 실제 시간(wall-clock time)에 의해 병목 현상이 발생한다는 것입니다. 이 모델들이 즉시 응답할 때는 실험을 반복하기가 정말 쉽습니다. 그들이 응답하는 데 3시간이 걸릴 때는 훨씬 더 어렵습니다. 그리고 3주가 걸리면 어떻게 될까요? 그 평가를 하고 그것을 반복하는 데 최소 3주가 걸립니다. 그리고 이 중 많은 부분에서 실험을 어느 정도 병렬화할 수 있지만, 많은 경우 실험을 실행하고 완료한 다음 결과를 봐야 다음 실험 세트를 결정할 수 있습니다. 저는 이것이 사실 긴 타임라인에 대한 가장 강력한 근거라고 생각합니다. 모델들이 해야 할 일이 너무 많기 때문입니다. 그리고 저는 그것이 도메인에 따라 다르다고 생각합니다. 그래서 신약 개발은 이것이 실제 병목이 될 수 있는 한 도메인이라고 생각합니다. 만약 어떤 것이 인간의 수명을 연장하는지 보고 싶다면, 당신이 개발한 이 새로운 약이 실제로 인간의 수명을 연장하고 그 과정에서 끔찍한 부작용이 없는지 알아내는 데 오랜 시간이 걸릴 것입니다.”

테스트 시간 컴퓨팅(Test Time Compute)은 모델이 추론 과정을 수행하는 데 필요한 계산 자원을 의미하며, 이는 모델의 '생각하는 시간'과 직결됩니다. 이 컴퓨팅을 확장하는 것은 AI 모델의 능력을 비약적으로 향상시키지만, 동시에 막대한 비용과 시간 제약이라는 현실적인 문제에 직면합니다. 모델이 몇 시간, 며칠 심지어 몇 주 동안 '생각'해야 하는 작업의 경우, 개발 과정에서 실험을 반복하고 결과를 평가하는 데 엄청난 시간이 소요됩니다. 이는 연구 주기를 늦추고 혁신 속도를 저해할 수 있습니다.

이러한 병목 현상을 해결하기 위한 노력은 크게 두 가지 방향으로 진행되고 있습니다. 첫째, 모델 자체의 효율성을 높여 동일한 컴퓨팅 자원으로 더 깊고 복잡한 추론을 수행할 수 있도록 하는 것입니다. 이는 알고리즘 개선, 모델 구조 최적화, 혹은 추론 과정의 내부 메커니즘을 더 스마트하게 만드는 것을 포함합니다. 둘째, 분산 컴퓨팅 기술을 활용하여 테스트 시간 컴퓨팅을 병렬화하고, 비동기적인 평가 시스템을 구축하는 것입니다. 예를 들어, 신약 개발과 같이 장기적인 실험이 필요한 분야에서는 AI가 가설을 세우고 시뮬레이션을 통해 초기 검증을 수행한 후, 가장 유망한 후보군만을 실제 실험으로 넘기는 방식이 효율성을 극대화할 수 있습니다. 이는 AI가 '생각'하는 시간을 인간의 '실험' 시간과 병렬화하여 전체 연구 개발 기간을 단축하는 데 기여합니다.

다른 평론가들도 장기적 RL을 위한 데이터가 사람들이 생각하는 것보다 더 멀리 있다고 생각했습니다. 그럼에도 불구하고, 테스트 시간 스케일링의 시대는 Orion 실행이 12월에 SG1이 온라인 상태가 될 때까지 컴퓨팅을 최대한 활용한 것으로 추정되는 바로 지금, 더 좋거나 더 이른 시기에 올 수 없었습니다.

### 다중 에이전트 시스템의 잠재력과 과제

최근 다중 에이전트에 대한 많은 논쟁이 있었습니다. Cognition은 **다중 에이전트를 만들지 말라**고 말하고 Anthropic은 **다중 에이전트를 만드는 방법**을 말합니다. 이 논쟁에 대해 **많은**, **많은**, **많은** 의견이 있었지만, 노암은 수년간 다중 에이전트 RL을 해왔고 OpenAI에서 다중 에이전트 팀을 발표했습니다… 비록 그것이 몇 가지 가능한 연구 방향 중 가장 두드러진 것일 뿐이지만요…

“저는 팀 [이름]이 여러 면에서 사실 잘못된 이름이라고 생각합니다. 왜냐하면 우리는 다중 에이전트 이상의 것을 연구하고 있기 때문입니다. 다중 에이전트는 우리가 연구하는 것들 중 하나입니다. 우리가 연구하는 다른 것들 중 일부는 테스트 시간 컴퓨팅을 엄청나게 확장할 수 있게 하는 것입니다. 그래서, 아시다시피, 우리는 지금 이 모델들이 15분 동안 생각하게 합니다. 어떻게 하면 그들이 몇 시간, 며칠, 심지어 더 오래 생각하게 할 수 있을까요? 그리고 엄청나게 어려운 문제들을 해결할 수 있게 할까요? 그래서 그것이 우리가 추구하는 한 방향입니다. 다중 에이전트는 또 다른 방향입니다. 그리고 여기서, 저는 몇 가지 다른 동기가 있다고 생각합니다. 우리는 다중 에이전트의 협력적 측면과 경쟁적 측면 모두에 관심이 있습니다. 제가 그것을 묘사하는 방식은, AI 서클에서 사람들은 종종 인간이 매우 좁은 지능의 밴드를 차지하고 있다고 말합니다. 그리고 AI는 그냥 빠르게 따라잡고, 그리고 이 지능의 밴드를 넘어설 것이라고요. 그리고 저는 사실 인간 지능의 밴드가 그렇게 좁다고 생각하지 않습니다. 저는 그것이 실제로는 꽤 넓다고 생각합니다. 왜냐하면, 아시다시피, 원시인 시대의 해부학적으로 동일한 인간들을 비교해 보면, 그들은 오늘날 우리가 지능이라고 생각하는 것의 관점에서 그리 멀리 가지 못했습니다, 그렇죠? 그들은 달에 사람을 보내지도 않고, 반도체나 원자로나 그런 것들을 만들지도 않습니다. 그리고 우리는 오늘날 그것들을 가지고 있습니다, 비록 우리 인간은 그렇지 않지만요. 그래서, 차이점은 무엇일까요? 글쎄요, 제 생각에 차이점은 수천 년 동안, 많은 인간들, 수십억의 인간들이 서로 협력하고 경쟁하며, 시간이 지남에 따라 문명을 건설했다는 것입니다. 우리가 보고 있는 기술은 이 문명의 산물입니다. 그리고 저는 비슷하게, 오늘날 우리가 가진 AI는 일종의 AI의 원시인과 같다고 생각합니다. 그리고 만약 당신이 그들을 수십억의 AI와 오랜 기간 동안 협력하고 경쟁하게 하고, 본질적으로 문명을 건설하게 할 수 있다면, 그들이 생산하고 답할 수 있는 것들은 오늘날 우리가 가진 AI로는 가능한 것보다 훨씬 더 뛰어날 것입니다.”

다중 에이전트 시스템(Multi-Agent Systems)은 단일 AI 모델의 한계를 넘어, 여러 AI 에이전트가 상호작용하며 복잡한 문제를 해결하는 접근 방식입니다. 이는 인간 사회가 수많은 개인이 협력하고 경쟁하며 문명을 발전시킨 방식과 유사하게, AI들이 서로 배우고 진화하여 개별 AI의 능력을 훨씬 뛰어넘는 집단 지능을 창출할 잠재력을 가집니다. 최근 다중 에이전트 연구는 단순히 게임 환경에서의 승리를 넘어, 실제 세계의 복잡한 시나리오에 적용될 수 있는 방향으로 진화하고 있습니다. 예를 들어, 복잡한 공급망 관리에서 여러 에이전트가 생산, 물류, 수요 예측을 담당하며 최적의 흐름을 찾아내거나, 기후 변화 모델링에서 각 에이전트가 특정 지역의 기상 데이터와 생태계 변화를 분석하고 공유하여 전 지구적 예측 정확도를 높이는 시나리오를 상상할 수 있습니다.

### 비터 레슨 vs 월드 모델(World Models) & 얀 르쿤(Yann LeCun)

**다중 에이전트에서의 비터 레슨**: “…우리가 다중 에이전트에 접근하는 방식의 세부 사항과 우리가 실제로 그것을 진행하는 방식은, 역사적으로 행해져 왔고 오늘날 다른 곳에서 행해지고 있는 방식과는 매우 다르다고 생각합니다. 저는 오랫동안 다중 에이전트 분야에 있었습니다… 저는 취해진 많은 접근 방식이 매우 휴리스틱(heuristic)했고, 스케일링과 연구에 대한 비터 레슨 접근 방식을 실제로 따르지 않았다고 생각합니다.”

**비터 레슨 vs 월드 모델(World Models) & 얀 르쿤(Yann LeCun)**: “…이 모델들이 커질수록 월드 모델을 가지게 되고, 그 월드 모델이 스케일에 따라 더 좋아진다는 것은 꽤 명백하다고 생각합니다. 그래서, 그들은 암묵적으로 월드 모델을 개발하고 있으며, 저는 그것을 명시적으로 모델링할 필요가 있다고 생각하지 않습니다… 다중 에이전트 AI 커뮤니티에서는 오랫동안 다른 에이전트, 즉 다른 사람들을 명시적으로 모델링해야 하는지, 아니면 환경의 일부로서 암묵적으로 모델링될 수 있는지에 대한 긴 논쟁이 있었습니다. 오랫동안 저는, '물론 이 다른 에이전트들을 명시적으로 모델링해야 한다'는 관점을 가졌습니다. 왜냐하면 그들은 환경과 다르게 행동하기 때문입니다. 그들은 행동을 취하고, 예측할 수 없으며, 행위성(agency)을 가지고 있습니다. 하지만 저는 시간이 지남에 따라 생각이 바뀌어, 사실 이 모델들이 충분히 똑똑해지면, 그들은 마음 이론(theory of mind) 같은 것을 개발한다고 생각하게 되었습니다. 그들은 자신들이… 행동을 취하고 동기를 가질 수 있는 에이전트라는 이해를 발전시킵니다. 그리고 이 모델들은 스케일과 더 유능한 행동 전반에 걸쳐 암묵적으로 그것을 개발합니다. 그래서, 요즘 제가 취하는 관점은 그것입니다.”

### 자가 대국(Self-Play)의 한계와 새로운 탐색

OpenAI는 **약한 것에서 강한 것으로(Weak to Strong)** 문제에 대해 썼고, GDM의 개방성 책임자인 팀 록타쉘(Tim Rocktaschel)은 싱가포르 ICLR에서 매우 호평받은 기조연설을 했습니다(전체 영상은 여기). 이는 다중 에이전트가 인간의 능력을 넘어서는 스케일링(비터 레슨의 궁극적인 제한 요소) 사이의 관계에 대한 질문을 불러일으켰습니다:

“Q: 가장 일관된 발견 중 하나는 항상 AI가 인간이 훈련하고 안내하는 것보다 경쟁적으로 자가 대국을 하고 개선하는 것이 더 낫다는 것입니다. 그리고 당신은 알파제로(AlphaZero)와 R1 제로에서 그것을 발견합니다. 이것이 다중 에이전트에서도, 즉 인간보다 더 나은 개선을 위해 자가 대국을 하는 것이 유효할 것이라고 생각하십니까?

A: 네, 이것은 훌륭한 질문입니다. 그리고… 이것은 좀 더 확장할 가치가 있다고 생각합니다. 그래서, 오늘날 많은 사람들이 자가 대국을 다음 단계이자 아마도 초지능에 필요한 마지막 단계로 보고 있다고 생각합니다. 그리고 알파고(AlphaGo)와 알파제로 같은 것을 보면, 우리는 매우 비슷한 추세를 따르고 있는 것 같습니다, 그렇죠? 알파고의 첫 단계는 대규모 사전 훈련(pre-training)을 하는 것이었습니다. 그 경우에는 인간의 바둑 게임에 대한 것이었죠. LLM의 경우, 수많은 인터넷 데이터에 대한 사전 훈련입니다. 그리고 그것은 강력한 모델을 만들어주지만, 초인적인 모델을 만들어주지는 않습니다. 그리고 알파고 패러다임의 다음 단계는 대규모 테스트 시간 컴퓨팅 또는 대규모 추론 컴퓨팅(inference compute)을 하는 것입니다. 그 경우에는 MCTS를 사용했죠. 그리고 이제 우리는 또한 이런 대규모 추론 컴퓨팅을 하는 추론 모델들을 가지고 있습니다. 그리고 다시, 그것은 능력을 엄청나게 향상시킵니다. 마지막으로, 알파고와 알파제로에서는 자가 대국이 있습니다. 모델이 자신과 대결하고, 그 게임들로부터 배우고, 점점 더 나아져서, 인간 수준의 성능에서 인간의 능력을 훨씬 뛰어넘는 수준으로 갑니다. 이 바둑 정책들은 이제 너무 강력해서 그냥 이해할 수 없습니다. 그들이 하는 일은 인간에게 이해할 수 없습니다. 체스도 마찬가지입니다. 그리고 우리는 지금 언어 모델에서는 그것을 가지고 있지 않습니다. 그래서 그것을 보고 '아, 우리는 이제 이 AI 모델들이 서로 상호작용하고 그들로부터 배우게 하기만 하면, 그들은 그냥 초지능에 도달할 것이다'라고 말하고 싶은 유혹이 정말 큽니다. … 도전 과제는 바둑이 2인 제로섬 게임(two-player zero-sum game)이라는 것입니다. 그리고 2인 제로섬 게임은 자가 대국을 할 때 최소최대 균형(minimax equilibrium)에 수렴한다는 매우 좋은 속성을 가지고 있습니다. 그리고 한 걸음 물러서서 말하자면, 2인 제로섬 게임, 즉 체스, 바둑, 심지어 2인 포커, 모두 2인 제로섬입니다. 음, 그건 사실이 아닙니다. 당신이 일반적으로 원하는 것은 최소최대 균형이라고 불리는 것입니다. 이것이 그 GTO 정책, 즉 당신이 플레이하는 이 정책으로, 어떤 상대에게도 기대값에서 지지 않을 것을 보장합니다. 저는 체스와 바둑에서는 그것이 꽤 명백하게 당신이 원하는 것이라고 생각합니다. 흥미롭게도, 포커를 보면, 그것은 그렇게 명백하지 않습니다. 2인 제로섬 버전의 포커에서는 GTO 최소최대 정책을 플레이할 수 있고, 그것은 지구상의 어떤 상대에게도 지지 않을 것을 보장합니다. 하지만, 다시 말하지만, 약한 플레이어로부터는 착취적 정책(exploitative policy)을 플레이했을 때만큼 많은 돈을 벌지는 못할 것입니다. 그래서, '당신은 무엇을 원하는가?'라는 질문이 있습니다. 가능한 한 많은 돈을 벌고 싶습니까, 아니면 살아있는 어떤 인간에게도 지지 않을 것을 보장하고 싶습니까? 모든 봇들이 결정한 것은, 음, 이 게임들의 모든 AI 개발자들이 결정한 것은, '음, 우리는 최소최대 정책을 선택할 것이다'입니다. 그리고 편리하게도, 그것이 바로 자가 대국이 수렴하는 것입니다. 이 AI들이 서로 대결하고, 그들의 실수로부터 배우게 하면, 그들은 시간이 지남에 따라 이 최소최대 정책으로 수렴합니다, 보장됩니다. 하지만 일단 2인 제로섬 게임을 벗어나면, 그것은 더 이상 유용한 정책이 아닙니다. 당신은 그냥 이런 매우 방어적인 정책을 가지고 싶지 않을 것이고, 수학 같은 것에서 같은 종류의 자가 대국을 시작하면 정말 이상한 행동을 하게 될 것입니다.”

자가 대국은 알파고(AlphaGo)와 알파제로(AlphaZero)에서 초인적인 성능을 달성하는 데 결정적인 역할을 했습니다. AI가 인간의 데이터 없이 스스로 게임을 플레이하며 학습하고 개선하는 이 방식은 많은 연구자들에게 초지능(superintelligence)으로 가는 마지막 단계처럼 보였습니다. 그러나 이러한 성공은 주로 체스나 바둑과 같은 '2인 제로섬 게임(two-player zero-sum game)'의 특성에 기반합니다. 이러한 게임에서는 한 플레이어의 이득이 다른 플레이어의 손실로 이어지며, 자가 대국을 통해 '최소최대 균형(minimax equilibrium)'이라는 최적의 전략을 찾아 수렴할 수 있습니다.

하지만 현실 세계의 많은 문제나 언어 모델이 다루는 작업은 2인 제로섬 게임이 아닙니다. 협력, 부분적 정보, 주관적인 성공 기준 등이 복합적으로 작용하는 경우가 많습니다. 예를 들어, 수학 문제 해결에서 '자가 대국'을 적용한다고 할 때, 한 모델이 어려운 문제를 내고 다른 모델이 푸는 방식은 쉽게 흥미롭지 않거나 비생산적인 문제(예: 30자리 숫자 곱셈)로 이어질 수 있습니다. 이러한 환경에서는 단순히 '이기기' 위한 전략보다는 '가치 있는 지식 생성'이나 '효율적인 협력'을 위한 새로운 목적 함수(objective function)와 평가 기준이 필요합니다.

최근 연구는 이러한 한계를 극복하기 위해 다양한 방향을 모색하고 있습니다. 첫째, '개방성(Open-Endedness)'을 추구하는 환경에서 AI가 스스로 목표를 설정하고, 새로운 문제를 발견하며, 그 과정에서 지식을 확장하는 자가 대국 방식을 탐색합니다. 둘째, '다중 목표 최적화(Multi-Objective Optimization)'나 '사회적 학습(Social Learning)' 개념을 도입하여, AI 에이전트들이 경쟁뿐만 아니라 협력을 통해 집단적인 성능을 향상시키고, 더 복잡하고 현실적인 시나리오에 적응할 수 있도록 합니다. 셋째, 인간의 피드백을 활용한 강화 학습(Reinforcement Learning from Human Feedback, RLHF)을 넘어서, AI가 스스로 인간의 가치와 선호를 추론하고 내재화하는 '가치 학습(Value Learning)' 연구도 활발합니다. 이러한 새로운 접근 방식은 자가 대국 패러다임을 2인 제로섬 게임의 한계를 넘어, 복잡하고 다면적인 현실 세계 문제에 적용될 수 있는 길을 열어줄 것으로 기대됩니다.

### AI 개발자의 역할과 진화하는 도구

**추론 + Windsurf = AGI를 느끼다.**

Q: Windsurf에 몰두해 보셨으니, 프로 팁이 있나요?
A: 제가 놀란 것 중 하나는 얼마나 많은 사람들이 O3가 존재한다는 것조차 모른다는 것입니다. 저는 매일 사용하고 있습니다. **기본적으로 저에게는 구글 검색을 대체했습니다**. 그냥 항상 사용합니다. 그리고 코딩 같은 것에도, **저는 그냥 추론 모델을 사용하는 경향이 있습니다**. 제 제안은, 만약 사람들이 아직 추론 모델을 사용해보지 않았다면, 솔직히, 사람들은 그것들을 좋아합니다. 사용하는 사람들은 그것들을 좋아합니다. 물론, 훨씬 더 많은 사람들이 GPT-4.0과 ChatGPT의 기본 설정을 사용하고 그런 종류의 것들을 사용합니다. 저는 추론 모델을 시도해 볼 가치가 있다고 생각합니다. 사람들은 그것들이 할 수 있는 것에 놀랄 것입니다.”

노암 브라운이 Windsurf와 CodeX를 일상적으로 사용하여 구글 검색을 대체하고 코딩 작업을 수행한다고 언급한 것은, AI 도구가 개발자의 작업 흐름을 근본적으로 변화시키고 있음을 보여줍니다. 이러한 'AGI를 느끼는 순간'은 처음에는 마법처럼 느껴지지만, 기술의 빠른 발전 속도 때문에 곧 익숙해지고 새로운 개선점을 찾게 됩니다.

AI 개발 도구는 단순히 코드 자동 완성이나 버그 수정 기능을 넘어, 복잡한 문제 해결 과정을 AI에 위임하는 방식으로 진화하고 있습니다. 예를 들어, CodeX는 개발자가 추상적인 작업을 지시하면, AI가 스스로 코드를 작성하고 테스트하며, 심지어 풀 리퀘스트(pull request)까지 생성합니다. 그러나 이러한 도구들도 여전히 한계를 가지고 있습니다. AI는 '천재지만 직장에서 첫날인 사람'과 같아서, 새로운 맥락이나 미묘한 요구사항에 대해서는 여전히 인간의 지시와 피드백이 필요합니다.

미래의 AI 개발은 AI 도구의 '경험'과 '재사용성'을 높이는 방향으로 나아갈 것입니다. AI가 이전에 해결했던 문제나 프로젝트의 맥락을 기억하고 활용하며, 특정 팀의 코딩 스타일이나 디자인 원칙을 학습하여 더욱 맞춤화된 결과물을 제공하는 것이 중요해집니다. 이는 개발자가 AI에게 단순한 지시를 내리는 것을 넘어, AI와 함께 복잡한 문제를 공동으로 설계하고 해결하는 'AI 페어 프로그래밍(AI Pair Programming)' 시대를 의미합니다. 또한, AI가 풀 리퀘스트 리뷰, 문서화, 테스트 케이스 생성 등 개발 생명주기(Software Development Lifecycle, SDLC)의 다양한 단계에 깊이 관여하여, 인간 개발자가 더 창의적이고 전략적인 작업에 집중할 수 있도록 돕는 방향으로 발전할 것입니다. 궁극적으로, AI는 개발자의 생산성을 극대화하고, 소프트웨어 개발의 속도와 품질을 동시에 향상시키는 핵심적인 파트너가 될 것입니다.

### 시각적 추론의 확장과 로보틱스의 미래

**시각적 추론(Visual Reasoning)에는 한계가 있습니다**. O3가 마스터 레벨의 Geoguessr 플레이어를 이기는 것에 대해 많은 흥분이 있었습니다. 하지만 한계도 있습니다:

“그것은 당신이 묻는 질문의 종류에 정확히 달려 있습니다. 시스템 2의 혜택을 별로 받지 못하는 질문들도 있다고 생각합니다. GeoGuessr는 확실히 혜택을 받는 경우 중 하나입니다… 제가 보통 지적하는 것은 정보 검색(information retrieval)입니다. 누군가 당신에게 '이 사람이 언제 태어났나요?'라고 묻고 웹에 접근할 수 없다면, 당신은 그것을 알거나 모릅니다. 그리고 앉아서 오랫동안 생각해 볼 수는 있습니다. 아마도 교육받은 추측을 할 수는 있겠죠… 하지만 실제로 그것을 알지 못하는 한 날짜를 알아낼 수는 없을 겁니다.”

O3 모델이 GeoGuessr와 같은 시각적 추론 과제에서 뛰어난 성능을 보이는 것은, LLM 기반의 추론 능력이 텍스트 영역을 넘어 다른 양식(modality)으로 확장될 수 있음을 보여줍니다. 시각적 추론은 단순히 이미지 인식이나 객체 감지를 넘어, 시각 정보에서 복잡한 공간적 관계, 시간적 변화, 그리고 잠재적인 의도를 추론하는 것을 포함합니다. 예를 들어, 자율 주행 차량은 도로 표지판, 다른 차량의 움직임, 보행자의 행동 등을 종합적으로 '추론'하여 안전한 주행 경로를 결정해야 합니다. 의료 영상 분석에서는 AI가 환자의 MRI나 CT 스캔을 통해 질병의 진행 상황이나 미세한 이상 징후를 '추론'하여 진단을 돕습니다.

그러나 시각적 추론에도 '시스템 2' 사고가 필요한 영역과 그렇지 않은 영역이 존재합니다. 단순한 정보 검색(예: 이미지 속 특정 객체 식별)은 시스템 1에 가깝지만, 복잡한 전략 게임(예: 스타크래프트)이나 미묘한 사회적 상황(예: 표정에서 감정 추론)에서는 시스템 2와 같은 깊이 있는 추론이 필수적입니다.

로보틱스 분야는 이러한 추론 능력의 궁극적인 시험대입니다. 물리적 세계와 상호작용하는 로봇은 예측 불가능한 환경에서 실시간으로 추론하고 행동을 결정해야 합니다. 초기 OpenAI가 펜 돌리기 로봇 팔과 같은 하드웨어 프로젝트에 투자했던 것처럼, AI 연구의 궁극적인 목표 중 하나는 지능을 물리적 형태로 구현하는 것입니다. 그러나 하드웨어의 느린 연구 주기와 높은 비용은 로보틱스 분야의 발전을 더디게 하는 주요 요인입니다.

휴머노이드 로봇에 대한 논쟁 역시 흥미롭습니다. 인간 형태의 로봇은 인간 중심의 환경에 적합하다는 장점이 있지만, 동시에 인간의 인지적 편향(familiarity bias)이나 '불쾌한 골짜기(uncanny valley)' 효과를 유발할 수 있습니다. 비휴머노이드 로봇(예: 드론, 산업용 로봇 팔)은 특정 작업에 최적화된 형태로 훨씬 더 실용적인 가치를 제공하기도 합니다. 미래에는 특정 목적에 맞는 다양한 형태의 로봇들이 각자의 추론 능력을 활용하여 인간의 삶을 보조하고 확장할 것입니다. 중요한 것은 형태 그 자체가 아니라, 로봇이 얼마나 효율적으로 환경을 이해하고, 의미 있는 추론을 수행하며, 안전하고 유능하게 행동할 수 있는가에 있습니다.

### 게임에 대하여

포커에서 모두를 이기는 것부터, LLM으로 세계 디플로머시(Diplomacy) 상위 10%에 드는 것, 그리고 개인적으로 세계 디플로머시 챔피언십에서 우승하는 것까지, 게임은 노암의 생각과 경력에서 큰 부분을 차지합니다. 하지만 아무 게임이나 다루는 것은 아닙니다…

“…저는 불완전 정보 게임(imperfect information games)을 위한 AI에 대한 엄청난 지식을 가지고 있습니다. 이것이 오랫동안 제 연구 분야였기 때문입니다. 그리고 저는 이 모든 것을 알고 있지만, 자주 이야기할 기회는 없습니다. 우리는 노리밋 텍사스 홀덤(No Limit Texas Hold'em)을 위한 초인적인 포커 AI를 만들었습니다. 그것에 대한 흥미로운 점 중 하나는 숨겨진 정보의 양이 실제로는 꽤 제한적이라는 것입니다. 텍사스 홀덤을 할 때 두 장의 숨겨진 카드를 가지고 있기 때문입니다. 그래서 당신이 있을 수 있는 가능한 상태의 수는 적어도 헤즈업(heads up)으로 플레이할 때 1,326개입니다. 그리고 아시다시피, 그것은 테이블에 있는 다른 플레이어의 수만큼 곱해지지만, 여전히 엄청난 수는 아닙니다… 문제는 숨겨진 가능성의 수, 즉 당신이 있을 수 있는 가능한 상태의 수가 확장됨에 따라 그 접근 방식이 무너진다는 것입니다. 그리고 숨겨진 상태의 수가 극도로 커질 때 무엇을 해야 하는지에 대한 매우 흥미로운 미해결 질문이 여전히 남아 있습니다. 아시다시피, 만약 당신이 네 장의 숨겨진 카드를 가진 오마하 포커(Omaha poker)로 간다면, 할 수 있는 것들이 있습니다. 그것은 상태의 수를 줄이기 위해 할 수 있는 일종의 휴리스틱입니다만, 실제로는 여전히 매우 어려운 문제입니다. 그리고 만약 당신이 40개의 말을 가진 스트라테고(Stratego) 같은 게임으로 간다면, 거의 40 팩토리얼(40!)에 가까운 다른 상태에 있을 수 있습니다. 그러면 우리가 포커에 사용했던 모든 기존 접근 방식이 무너지고 다른 접근 방식이 필요합니다…. 만약 당신이 그 기술들을 확장한다면, 아마도 스트라테고나 매직 더 개더링(Magic the Gathering) 같은 것들에도 작동하게 할 수 있겠지만, 그것들은 여전히 제한적일 것입니다. 그것들은 언어 모델로 초인적인 코드포스(encode forces)를 얻게 해주지는 않을 것입니다. 그래서 저는 매우 일반적인 추론 기술에 집중하는 것이 더 가치 있다고 생각합니다. 그리고 언젠가 우리가 그것들을 개선함에 따라, 어느 날 갑자기 초인적인 수준으로 매직 더 개더링을 플레이하는 모델을 갖게 될 것이라고 생각합니다. 그리고 저는 그것이 더 중요하고 더 인상적인 연구 방향이라고 생각합니다.”

**사이드 노트**: 팟캐스트에서 노암이 요청한 LLM이 디플로머시를 자가 대국하는 챌린지는 바로 그 후에 AIE 월드 페어(AIE World’s Fair)에서 출시되었습니다 :)

팟캐스트에 노암을 모시게 되어 큰 영광이었고, 여러분이 전체 내용을 들으시면서 더 많은 여담, 정보, 조언을 얻으시길 바랍니다! 저희가 놓친 큰 내용이 있다면 알려주세요.

### AI 연구의 방향과 미래 전망

AI 연구는 이제 단순히 '더 큰 모델'을 만드는 것을 넘어, '더 똑똑하게 생각하는 모델'을 만드는 방향으로 나아가고 있습니다. 사전 훈련(pre-training)으로 모델의 기본 지능을 구축하고, 중간 훈련(mid-training)을 통해 특정 도메인이나 작업에 대한 전문성을 부여하며, 사후 훈련(post-training)으로 사용자 상호작용에 최적화하는 다단계 파이프라인이 중요해지고 있습니다.

이러한 변화는 AI 모델을 '원시적인 지식 덩어리'에서 '정제되고 유능한 전문가'로 진화시키는 과정입니다. 중간 훈련은 모델이 특정 산업 분야(예: 법률, 금융, 의학)의 방대한 전문 지식을 깊이 있게 학습하고, 해당 분야의 복잡한 추론 패턴을 내재화하는 데 활용될 수 있습니다. 이는 특정 산업의 요구사항에 맞춰 AI 모델을 맞춤화하고, 범용 AI가 아닌 '도메인 특화 AI'의 성능을 극대화하는 핵심 단계가 될 것입니다.

향후 5~10년 동안 AI는 더욱 강력한 추론 능력과 다중 에이전트 협력을 통해 현재 상상하기 어려운 문제들을 해결할 것입니다. 신약 개발, 기후 변화 모델링, 복잡한 사회 문제 해결 등 인류가 직면한 난제들에 AI가 핵심적인 역할을 할 가능성이 높습니다. 그러나 이러한 발전은 AI의 통제 가능성, 안전성, 그리고 윤리적 활용에 대한 깊은 고민을 동반해야 합니다. AI가 인류에게 긍정적인 영향을 미치도록 방향을 설정하는 것은 단순히 기술적 과제를 넘어, 사회 전체의 지혜와 노력이 필요한 공동의 목표가 될 것입니다. 이러한 미래를 만들어나가기 위해 AI 연구자들은 계속해서 열린 마음으로 탐구하고, 다양한 분야의 전문가들과 협력하며, 예측 불가능한 도전을 헤쳐나가야 할 것입니다.