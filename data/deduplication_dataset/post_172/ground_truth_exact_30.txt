안녕하세요, LLM 소식과 함께 Jay가 다시 찾아왔습니다! 최근 저는 여러 편의 비디오를 제작했으며, Deeplearning AI에서 LLM을 활용한 의미 검색(semantic search) 과정(course)을 만들기 위해 몇몇 ML 영웅들과 협력했습니다. 다음은 그 내용들과 곧 출간될 저희 책에 대한 업데이트입니다. "Language Models and Machine Learning"을 읽어주셔서 감사합니다! 새로운 게시물을 무료로 받아보고 제 작업을 지원하려면 구독하세요. 구독하기

**LLM 토크나이저의 중요성과 발전**
대규모 언어 모델(LLM)의 핵심 구성 요소 중 하나인 토크나이저(tokenizer)는 텍스트 데이터를 모델이 이해할 수 있는 작은 단위(토큰)로 분할하는 역할을 합니다. 인터넷 규모의 방대한 텍스트 데이터를 처리함에도 불구하고, LLM은 우리가 단어를 인식하는 방식과는 다르게 텍스트를 처리합니다. 토크나이저는 텍스트를 받아들여 언어 모델이 실제로 작동하는 다른 형식으로 번역합니다. 토크나이저의 선택은 모델의 성능, 효율성, 그리고 다국어 처리 능력에 지대한 영향을 미칩니다. Byte-Pair Encoding (BPE), WordPiece, SentencePiece와 같은 다양한 토크나이징 전략은 각각의 장단점을 가지며, 특정 언어나 데이터 유형에 최적화되어 발전해왔습니다. 예를 들어, SentencePiece는 공백을 포함한 모든 문자를 토큰화하여 새로운 언어에 대한 처리 능력을 향상시키며, 이는 다국어 모델에서 특히 중요합니다.

**비디오: ChatGPT는 (대부분의 인터넷을 읽었음에도 불구하고) 단 한 단어도 본 적이 없습니다. LLM 토크나이저(tokenizer)를 만나보세요**
인터넷 규모의 텍스트 데이터를 처리함에도 불구하고, 대규모 언어 모델(large language models)은 우리가 보는 방식대로 단어를 보지 않습니다. 네, 텍스트를 소비하지만, 토크나이저(tokenizer)라고 불리는 또 다른 소프트웨어가 실제로 텍스트를 받아들여 언어 모델이 실제로 작동하는 다른 형식으로 번역합니다. 이 비디오에서 Jay는 언어 모델 토크나이저(tokenizer)를 검토하여 그것들이 어떻게 작동하는지 알려드립니다.

**비디오: LLM 토크나이저(tokenizer)는 서로 어떻게 다를까요? GPT4 vs. FlanT5 vs. Starcoder vs. BERT 외 다수**
토크나이저(tokenizer)가 무엇을 하는지 이해하는 가장 좋은 방법 중 하나는 다른 토크나이저들의 동작을 비교하는 것입니다. 이 비디오에서 Jay는 신중하게 작성된 텍스트(영어, 코드, 들여쓰기, 숫자, 이모지 및 기타 언어를 포함)를 가져와 다양한 훈련된 토크나이저(tokenizer)를 통해 전달하여, 그들이 인코딩(encoding)에 성공하고 실패하는 부분, 그리고 다양한 토크나이저(tokenizer)의 설계 선택과 그것들이 각 모델에 대해 무엇을 말하는지 보여줍니다.

**과정(Course): Cohere와 함께하는 새로운 과정: 의미 검색(Semantic Search)을 활용한 대규모 언어 모델(Large Language Models)**
이 짧은 과정(course)에서 저의 영웅들인 Luis Serrano, Meor Amer, Andrew Ng와 협력하게 되어 정말 놀라웠습니다. 여기에서 등록하세요: https://bit.ly/3OLOEzo

기대할 수 있는 내용은 다음과 같습니다:
*   **LLM 기초 이해**: 대규모 언어 모델(large language models)이 어떻게 작동하는지에 대한 이해를 심화하여, 더욱 능숙한 AI 개발자가 될 수 있도록 준비합니다.
*   **키워드 검색(Keyword Search) 강화**: 기존 프레임워크를 전면 개편하지 않고도 키워드 또는 벡터 검색(vector search) 시스템의 품질을 향상시키는 도구인 ReRank를 통합하는 방법을 배웁니다.
*   **밀집 검색(Dense Retrieval) 활용**: 임베딩(embeddings)과 대규모 언어 모델(large language models)을 사용하여 검색 애플리케이션의 Q&A 기능을 향상시키는 방법을 알아봅니다.
*   **평가 및 구현**: 검색 모델을 평가하고 프로젝트에 이러한 기술을 효율적으로 구현하는 방법에 대한 통찰력을 얻습니다.
*   **실제 적용**: 위키피디아 데이터셋(dataset)을 사용하여 검색(retrieval) 및 최근접 이웃(nearest neighbors)과 같은 프로세스를 최적화하는 방법을 이해하고, 대규모 데이터셋(dataset)으로 실질적인 경험을 제공합니다.

이 과정(course)을 마치면, 대규모 언어 모델(LLMs)이 어떻게 작동하는지에 대한 기본 원리를 더 깊이 이해하게 되어, AI 개발자로서의 기술을 향상시킬 수 있을 것입니다.

**의미 검색과 RAG: 실제 적용 사례**
의미 검색(Semantic Search)은 단순한 키워드 매칭을 넘어 사용자 질의의 의도를 파악하여 더욱 관련성 높은 결과를 제공하는 기술입니다. 이 과정은 특히 Retrieval Augmented Generation (RAG) 아키텍처와 결합될 때 그 진가를 발휘합니다. RAG는 LLM이 최신 정보나 도메인 특정 지식에 접근하여 답변을 생성하도록 돕는 강력한 프레임워크로, 환각(hallucination)을 줄이고 답변의 정확성을 높이는 데 필수적입니다. 기업의 내부 지식 베이스, 고객 서비스 챗봇, 그리고 법률 및 의료 분야에서의 정보 검색 시스템에 의미 검색과 RAG를 적용함으로써, 우리는 더욱 정확하고 신뢰할 수 있는 AI 기반 솔루션을 구축할 수 있습니다.

**책 업데이트**
저희는 "Hands-On Large Language Models" 집필에 매진하고 있습니다. 현재 O'Reilly 플랫폼에서 얼리 릴리즈(Early Release) 버전으로 5개 챕터(150페이지)를 이용할 수 있습니다:
1.  텍스트 분류(Categorizing Text)
2.  의미 검색(Semantic Search)
3.  텍스트 클러스터링(Text Clustering) 및 토픽 모델링(Topic Modeling)
4.  멀티모달 대규모 언어 모델(Multimodal Large Language Models)
5.  토큰(Tokens) 및 토큰 임베딩(Token Embeddings)

30일 무료 체험으로 책의 얼리 릴리즈(Early Release) 버전에 접속하세요: https://learning.oreilly.com/get-learning/?code=HOLLM23

**Hands-On LLM 심화 학습 가이드**
이 책은 LLM의 핵심 개념부터 실제 적용까지 포괄적으로 다룹니다. 특히 멀티모달 대규모 언어 모델(Multimodal Large Language Models) 챕터는 텍스트를 넘어 이미지, 오디오 등 다양한 형태의 데이터를 이해하고 생성하는 LLM의 최신 트렌드를 탐구합니다. 이는 LLM이 단순한 텍스트 처리기를 넘어 범용 AI로 진화하는 과정을 보여줍니다. 또한, 토큰(Tokens) 및 토큰 임베딩(Token Embeddings) 챕터는 모든 LLM의 근간이 되는 중요한 개념을 심도 있게 다루어, 모델의 작동 방식을 근본적으로 이해하는 데 도움을 줍니다. 이러한 기초 지식은 LoRA(Low-Rank Adaptation)와 같은 효율적인 미세 조정(fine-tuning) 기법을 이해하고 활용하는 데 필수적입니다. LoRA는 대규모 모델을 전체적으로 재훈련하지 않고도 특정 작업에 맞춰 모델의 성능을 최적화할 수 있게 해주어, 컴퓨팅 자원과 시간을 크게 절약할 수 있습니다.

**다음 책 내용: 다시 보는 일러스트레이티드 트랜스포머(The Illustrated Transformer Revisited)**
Maarten(저의 공동 저자)과 저는 현재 얼리 릴리즈(Early Release)에는 포함되지 않았지만 향후 몇 주 내에 공개될 여러 챕터를 검토 중입니다. 제가 방금 마친 챕터의 제목은 "트랜스포머 LLM(Transformer LLMs) 내부 들여다보기"입니다. 이는 기본적으로 지난 5년간 트랜스포머 아키텍처(Transformer Architecture)의 주요 업데이트를 반영하여 "일러스트레이티드 트랜스포머(The Illustrated Transformer)"를 다시 살펴보는 것입니다. 하지만 텍스트 생성 LLM(한 번에 하나의 토큰을 생성하는 자기회귀 모델(autoregressive models))에 초점을 맞추고 있습니다. 해당 챕터에는 39개의 새로운 그림이 포함되어 있으며, 제가 아는 한 가장 명확한 방식으로 자기 어텐션(self-attention)을 설명한다고 생각합니다.

다음은 몇 가지 티저 시각 자료입니다:
*   자기 어텐션(self-attention)의 두 가지 주요 단계
*   멀티 헤드 자기 어텐션(multi-head self-attention)의 쿼리(queries), 키(keys), 값(values)
*   더 효율적인 멀티 쿼리 어텐션(multi-query attention) — 헤드(heads)는 개별 쿼리(queries)를 가지지만 키(keys)와 값(values)을 공유합니다. (논문: Fast Transformer Decoding: One Write-Head is All You Need)
*   트랜스포머 어댑터(Transformer adapters)는 효율적인 미세 조정(fine-tuning)을 위한 한 가지 접근 방식입니다.
*   저랭크 적응(Low-Rank adaptation), 즉 LoRA는 대규모 가중치 행렬(weight matrices)을 더 작고 낮은 랭크(rank)의 행렬로 줄이는 효율적인 미세 조정(fine-tuning)의 또 다른 방법으로, 종종 유사한 성능을 유지하면서 크기와 필요한 저장 공간/메모리/연산량을 압축할 수 있습니다. 이는 언어 모델이 "매우 낮은 내재적 차원(intrinsic dimension)을 가지고 있기" 때문에 작동합니다. 따라서 175B 모델의 효율적인 버전은 예를 들어 랭크(rank) = 8로 많은 작업을 수행할 수 있습니다. 이는 행렬의 크기와 해당 매개변수(parameters)를 미세 조정(fine-tune)하는 데 필요한 시간을 크게 줄여줍니다.

**트랜스포머 아키텍처의 최신 혁신**
트랜스포머 아키텍처는 LLM 혁명의 핵심이었으며, 지난 몇 년간 지속적인 발전을 거듭해왔습니다. 멀티 헤드 자기 어텐션(multi-head self-attention)은 모델이 다양한 정보 표현에 동시에 집중할 수 있게 함으로써 성능을 향상시켰습니다. 더 효율적인 멀티 쿼리 어텐션(multi-query attention)은 추론 속도를 크게 높여 대규모 모델의 실용성을 더했습니다. 또한, FlashAttention과 같은 혁신적인 어텐션 메커니즘은 메모리 사용량과 계산 복잡성을 줄여 더 긴 시퀀스를 처리할 수 있게 하였고, 이는 LLM의 컨텍스트 창을 확장하는 데 결정적인 역할을 했습니다. 이러한 아키텍처 개선은 LLM이 더욱 복잡한 작업을 수행하고 더 큰 규모로 확장될 수 있도록 뒷받침합니다.

**비디오: KeyLLM 소개 - Mistral 7B 및 KeyBERT를 활용한 키워드 추출(Keyword Extraction)**
저의 공동 저자인 Maarten은 훌륭한 LLM 소프트웨어와 이를 설명하는 비디오를 제작해왔습니다. 이 비디오에서 저는 대규모 언어 모델(Large Language Models)을 사용하여 키워드(keywords)를 추출하기 위한 KeyBERT의 확장 기능인 KeyLLM을 소개하게 되어 자랑스럽습니다! 저희는 놀라운 Mistral 7B LLM을 사용하고 여러 사용 사례(use cases)를 살펴볼 것입니다.

**KeyLLM 및 고급 키워드 추출 기술**
KeyLLM은 최신 LLM을 활용하여 문서에서 핵심 키워드를 효과적으로 추출하는 고급 도구입니다. 기존의 통계 기반 키워드 추출 방식과 달리, KeyLLM은 LLM의 심층적인 언어 이해 능력을 활용하여 문맥적으로 더욱 정확하고 의미 있는 키워드를 식별할 수 있습니다. 이는 콘텐츠 태깅, 자동 요약, 정보 검색 시스템의 색인 생성, 그리고 시장 트렌드 분석 등 다양한 분야에서 혁신적인 변화를 가져올 수 있습니다. Mistral 7B와 같은 강력한 오픈 소스 LLM의 등장은 이러한 고급 키워드 추출 기술을 더 많은 개발자와 기업이 활용할 수 있도록 만들고 있습니다.

… 이번 업데이트는 여기까지입니다. 아직 말씀드릴 수 없는 더 많은 흥미로운 소식들이 있으니 계속 지켜봐 주세요! LLM 기술은 매일같이 빠르게 발전하고 있으며, 저희는 이 여정에서 여러분과 함께 최신 지식과 통찰력을 공유하게 되어 기쁩니다. "Language Models and Machine Learning"을 읽어주셔서 감사합니다! 새로운 게시물을 무료로 받아보고 제 작업을 지원하려면 구독하세요. 구독하기