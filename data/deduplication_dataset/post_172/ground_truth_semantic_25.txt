안녕하세요, LLM 기술 동향 소식과 함께 Jay가 다시 인사드립니다! 최근 저는 여러 영상 콘텐츠를 제작했으며, Deeplearning AI의 의미론적 탐색(semantic search) 과정(course) 개발을 위해 몇몇 뛰어난 머신러닝 전문가들과 협업하는 기회를 가졌습니다. 아래에서는 그 결과물들과 곧 출간될 저희 저서에 대한 최신 정보를 전해드립니다. "Language Models and Machine Learning"에 보내주신 성원에 감사드립니다! 새로운 소식을 무료로 받아보고 제 활동을 지원하시려면 구독을 신청해 주세요. 구독하기

**영상: ChatGPT는 (대부분의 웹 데이터를 학습했음에도 불구하고) 실제 단어를 인지한 적이 없습니다. LLM 토큰화기(tokenizer)의 세계로 초대합니다.**
거대한 인터넷 규모의 문자열 데이터를 다루더라도, 대형 언어 모델(large language models)은 사람이 글자를 이해하는 것과 동일한 방식으로 단어를 해석하지 않습니다. 그렇습니다, 이들은 텍스트를 입력으로 받지만, 토큰화기(tokenizer)라는 별도의 소프트웨어 모듈이 원본 텍스트를 인공지능 모델이 실제 연산을 수행할 수 있는 다른 형식으로 변환하는 역할을 담당합니다. 본 영상에서 Jay는 언어 모델의 토큰화기(tokenizer)를 심층 분석하며 그 작동 원리를 명확하게 설명해 드립니다.

새로운 관점: 토큰화 과정은 단순히 단어를 쪼개는 것을 넘어섭니다. 현대 LLM에서 사용되는 서브워드 토큰화(subword tokenization) 기법(예: BPE, WordPiece, SentencePiece)은 어휘 크기를 최적화하고, 모델이 학습하지 못한 단어(out-of-vocabulary, OOV) 문제를 효과적으로 처리하며, 언어의 복잡한 형태론적 특성을 반영하는 데 필수적입니다. 이러한 토큰 분할 방식은 모델의 효율성과 성능에 직접적인 영향을 미칩니다.

**영상: LLM 토큰화기(tokenizer)는 어떻게 서로 다른 방식으로 작동할까요? GPT4, FlanT5, Starcoder, BERT 등을 비교 분석합니다.**
토큰화기(tokenizer)가 어떤 역할을 하는지 파악하는 가장 좋은 방법 중 하나는 여러 유형의 토큰화기가 데이터를 처리하는 방식을 대조해보는 것입니다. 본 영상에서 Jay는 신중하게 구성된 텍스트 샘플(영문, 프로그래밍 코드, 들여쓰기 구조, 숫자, 이모지 및 기타 언어 요소 포함)을 가져와 다양한 사전 학습된 토큰화기(tokenizer)에 적용합니다. 이를 통해 각 토큰화기가 인코딩(encoding) 과정에서 성공하거나 실패하는 지점들을 보여주고, 각 토큰화기 설계가 해당 모델의 특성과 성능에 어떤 의미를 부여하는지 상세히 분석합니다.

새로운 관점: 각 모델에 사용되는 토큰화기(tokenizer)의 차이는 주로 학습 데이터의 특성, 어휘 구축 알고리즘, 그리고 모델의 최종 목표에 따라 발생합니다. 예를 들어, 코드에 특화된 Starcoder의 토큰화기는 프로그래밍 언어의 문법과 구조를 더 잘 이해하도록 설계되었으며, 다국어 모델의 토큰화기는 여러 언어의 효율적인 표현을 위해 최적화됩니다. 이러한 설계 선택은 모델이 특정 유형의 텍스트를 얼마나 효과적으로 처리할 수 있는지 결정하는 핵심 요소가 됩니다.

**강좌(Course): Cohere와 함께하는 신규 강좌: 의미론적 검색(Semantic Search)을 활용한 대규모 언어 모델(Large Language Models)**
이 단기 집중 과정(course)을 저의 존경하는 동료들인 Luis Serrano, Meor Amer, 그리고 Andrew Ng 교수님과 함께 개발하게 되어 정말 영광스럽습니다. 지금 바로 등록하세요: https://bit.ly/3OLOEzo

이 강좌에서 여러분이 얻을 수 있는 내용은 다음과 같습니다:
*   **LLM의 핵심 원리 이해**: 거대 언어 모델(large language models)의 작동 방식을 깊이 있게 탐구하여, 보다 능숙한 AI 개발자로 성장하기 위한 토대를 마련합니다.
*   **키워드 검색(Keyword Search) 역량 증진**: 기존 시스템을 전면적으로 재구축하지 않고도 키워드 또는 벡터 기반 검색(vector search) 체계의 품질을 개선하는 도구인 ReRank를 통합하는 실질적인 방법을 습득합니다.
*   **밀집 검색(Dense Retrieval) 기술 활용**: 임베딩(embeddings)과 대규모 언어 모델(large language models)을 사용하여 검색 애플리케이션의 질문 답변(Q&A) 기능을 고도화하는 방법을 배웁니다.
*   **평가 및 실질적 구현**: 검색 모델의 성능을 측정하고, 이러한 기술들을 프로젝트에 효과적으로 통합하는 방법에 대한 깊이 있는 통찰력을 얻습니다.
*   **현실 세계 적용**: 위키피디아 데이터셋(dataset)을 활용하여 정보 검색(retrieval) 및 최근접 이웃(nearest neighbors) 탐색과 같은 과정을 최적화하는 방안을 이해하고, 대규모 데이터셋(dataset)을 다루는 실제 경험을 제공합니다.

새로운 관점: 의미론적 검색은 단순한 단어 매칭을 넘어 사용자 질의의 의도를 파악하여 더욱 관련성 높은 결과를 제공하는 현대 정보 검색의 핵심 기술입니다. 이 강좌는 전자상거래 추천 시스템, 고객 서비스 챗봇, 지식 관리 시스템 등 다양한 분야에서 의미론적 검색을 효과적으로 구축하고 최적화하는 데 필요한 실용적인 지식과 기술을 제공합니다. 강좌를 수료하시면, 대규모 언어 모델(LLMs)의 근본적인 작동 원리에 대한 이해를 더욱 심화하고, AI 개발자로서의 역량을 한층 더 강화할 수 있을 것입니다.

**도서 업데이트 소식**
저희는 "Hands-On Large Language Models" 집필에 전념하고 있습니다. 현재 O'Reilly 플랫폼에서 얼리 릴리즈(Early Release) 형태로 5개의 장(총 150페이지 분량)이 공개되어 있습니다:
1.  텍스트 분류(Categorizing Text)
2.  의미 검색(Semantic Search)
3.  텍스트 클러스터링(Text Clustering) 및 주제 모델링(Topic Modeling)
4.  멀티모달 대규모 언어 모델(Multimodal Large Language Models)
5.  토큰(Tokens) 및 토큰 임베딩(Token Embeddings)

새로운 관점: 이 책은 LLM을 활용한 실제 애플리케이션 개발에 초점을 맞추고 있으며, 각 장은 특정 문제 해결을 위한 실용적인 가이드를 제공합니다. 예를 들어, 텍스트 분류는 스팸 필터링이나 감성 분석에 필수적이며, 멀티모달 LLM은 텍스트 외에 이미지나 음성 데이터를 함께 처리하는 미래 지향적인 애플리케이션의 기반이 됩니다. 이러한 장들은 독자들이 LLM의 이론을 넘어 실제 프로젝트에 적용할 수 있는 능력을 키우는 데 기여할 것입니다.

30일 무료 체험을 통해 도서의 얼리 릴리즈(Early Release) 버전에 접속하세요: https://learning.oreilly.com/get-learning/?code=HOLLM23

**다음 도서 내용: 다시 조명하는 일러스트레이티드 트랜스포머(The Illustrated Transformer Revisited)**
저의 공동 저자인 Maarten과 저는 현재 얼리 릴리즈(Early Release)에는 포함되지 않았지만, 앞으로 몇 주 내에 공개될 여러 장들을 검토하고 있습니다. 제가 최근 작업을 마친 장의 제목은 "트랜스포머 LLM(Transformer LLMs) 심층 탐구"입니다. 이는 기본적으로 지난 5년간 트랜스포머 아키텍처(Transformer Architecture)의 주요 발전 사항을 반영하여 "일러스트레이티드 트랜스포머(The Illustrated Transformer)"를 재해석한 것입니다. 특히, 텍스트 생성 LLM(단일 토큰을 순차적으로 생성하는 자기회귀 모델(autoregressive models))에 초점을 맞추고 있습니다. 해당 장에는 39개의 새로운 도해가 수록되어 있으며, 제가 아는 한 가장 명료한 방식으로 자기 어텐션(self-attention) 메커니즘을 설명한다고 자부합니다.

새로운 관점: 트랜스포머 아키텍처는 LLM 혁명의 핵심 동력이었지만, 그 내부 작동 방식은 여전히 복잡하게 느껴질 수 있습니다. 이 장에서는 기본적인 인코더-디코더 구조를 넘어, 위치 임베딩(positional embeddings)의 중요성, 피드포워드 네트워크(feed-forward networks)의 역할, 그리고 레이어 정규화(layer normalization)가 모델 안정성에 기여하는 방식 등 트랜스포머의 각 구성 요소를 상세히 해부합니다. 또한, 최근 연구 동향인 효율적인 어텐션 메커니즘(예: FlashAttention)이나 MoE(Mixture of Experts)와 같은 스케일링 기술에 대한 간략한 소개를 통해 독자들이 최신 LLM 기술의 흐름을 이해할 수 있도록 돕습니다.

다음은 몇 가지 미리보기 시각 자료입니다:
*   자기 어텐션(self-attention)의 두 가지 핵심 단계
*   멀티 헤드 자기 어텐션(multi-head self-attention)에서의 쿼리(queries), 키(keys), 값(values) 역할
*   더 효율적인 멀티 쿼리 어텐션(multi-query attention) — 각 헤드(heads)는 개별적인 쿼리(queries)를 사용하지만, 키(keys)와 값(values)은 공유합니다. (논문: Fast Transformer Decoding: One Write-Head is All You Need)
*   트랜스포머 어댑터(Transformer adapters)는 효율적인 미세 조정(fine-tuning)을 위한 한 가지 접근 방식입니다.
*   저랭크 적응(Low-Rank adaptation), 즉 LoRA는 대규모 가중치 행렬(weight matrices)을 더 작고 낮은 랭크(rank)의 행렬로 축소하여 효율적으로 미세 조정(fine-tuning)하는 또 다른 기법으로, 종종 유사한 성능을 유지하면서 모델의 크기와 필요한 저장 공간/메모리/연산량을 압축할 수 있습니다. 이는 언어 모델이 "매우 낮은 내재적 차원(intrinsic dimension)을 가지고 있기" 때문에 가능합니다. 따라서 175B 규모의 모델에 대한 효율적인 버전은 예를 들어 랭크(rank) = 8만으로도 많은 작업을 수행할 수 있습니다. 이는 행렬의 크기와 해당 매개변수(parameters)를 미세 조정(fine-tune)하는 데 필요한 시간을 현저히 줄여줍니다.

새로운 관점: LoRA의 핵심 아이디어는 대규모 신경망의 가중치 업데이트가 실제로는 훨씬 낮은 차원의 공간에서 이루어진다는 관찰에서 출발합니다. 즉, 모델의 복잡한 학습 과정이 본질적으로 적은 수의 독립적인 방향으로만 의미 있는 변화를 만들어낸다는 것입니다. LoRA는 이러한 저차원 특성을 활용하여, 전체 가중치 행렬을 직접 조정하는 대신 작은 보조 행렬(adapter matrices)만을 학습시켜 효율성을 극대화합니다. 이는 특히 제한된 컴퓨팅 자원을 가진 환경에서 대규모 모델을 특정 작업에 맞게 조정할 때 매우 유용합니다.

**영상: KeyLLM 소개 - Mistral 7B 및 KeyBERT를 활용한 핵심어 추출(Keyword Extraction)**
저의 공동 저자인 Maarten은 뛰어난 LLM 소프트웨어와 이를 설명하는 영상들을 꾸준히 제작해왔습니다. 이 영상에서 저는 대규모 언어 모델(Large Language Models)을 활용하여 핵심어(keywords)를 추출하기 위한 KeyBERT의 확장판인 KeyLLM을 소개하게 되어 매우 기쁩니다! 저희는 놀라운 Mistral 7B LLM을 사용하고 여러 활용 사례(use cases)를 심층적으로 탐구할 것입니다.

새로운 관점: 전통적인 키워드 추출 방식은 주로 통계적 빈도나 문법적 패턴에 의존하여 문맥적 의미를 충분히 반영하지 못하는 한계가 있었습니다. KeyLLM은 LLM의 강력한 문맥 이해 능력을 활용하여 단순히 자주 등장하는 단어가 아닌, 문서의 핵심 주제와 밀접하게 관련된 구문을 정확하게 식별합니다. 이는 콘텐츠 요약, 문서 분류, 정보 검색 시스템의 정교화 등 다양한 분야에서 혁신적인 가능성을 제공하며, 특히 Mistral 7B와 같은 효율적인 오픈소스 LLM과의 결합은 접근성을 크게 향상시킵니다.

… 이번 업데이트는 여기까지입니다. 아직 공개할 수 없는 더 많은 흥미로운 소식들이 준비되어 있으니 계속해서 저희 소식에 주목해 주세요! "Language Models and Machine Learning"에 보내주신 관심에 다시 한번 감사드립니다! 새로운 게시물을 무료로 받아보고 제 작업을 지원하시려면 구독을 신청해 주세요. 구독하기