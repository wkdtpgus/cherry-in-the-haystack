대규모 언어 모델(large language models)의 가장 매력적인 활용 사례 중 일부는 항공권 예매나 소프트웨어 결함(software defects)을 찾아내 고치는 것과 같이 현실 세계에서 직접적인 행동을 수반합니다. 이러한 임무를 수행하는 인공지능 시스템을 우리는 '지능형 자율 시스템' 또는 '에이전트(agents)'라고 부릅니다. 이들은 대규모 언어 모델을 웹 검색(information retrieval)이나 코드 터미널(command-line interfaces)과 같은 외부 도구와 결합하여 활용합니다. 궁극적인 목표는 시리(Siri)나 알렉사(Alexa)와 같은 비서(advanced conversational UIs)를 구축하여, 복잡한 작업을 능숙하게 처리하고, 사용자의 요청을 정확하게 파악하며, 신뢰성 있게 실행하도록 만드는 것입니다. 하지만 현재는 이러한 이상적인 모습과는 거리가 멀고, 관련 연구 분야 자체도 매우 초기 단계에 있습니다. 에이전트(agents) 개발을 촉진하고 그 효용성을 측정하기 위해 연구자들은 표준 평가 데이터셋(benchmark datasets)을 구축했습니다. 그럼에도 불구하고, 언어 모델 평가(LLM assessment)가 본래 난해한 영역인 것처럼, 에이전트 평가(agent evaluation)는 현행 벤치마크(benchmarks)와 평가 방식에 영향을 미치는 수많은 추가적인 난관(pitfalls)을 내포하고 있음이 드러났습니다. 이러한 상황은 실제로는 무용지물임에도 불구하고 평가 기준에서만 우수한 성능을 보이는 에이전트(agents)의 개발을 부추깁니다. 저희는 에이전트(agents) 평가의 핵심 과제를 명확히 하고 이를 극복할 방안을 제시하는 새로운 논문을 발표했습니다. 해당 논문은 여기에서 확인하실 수 있습니다. 이 연구는 프린스턴 대학교(Princeton University)의 Sayash Kapoor, Benedikt Ströbl, Zachary S. Siegel, Nitya Nadgir, Arvind Narayanan이 공동으로 수행했습니다. 본 게시물에서는 AI 에이전트(AI agents)의 개념을 정의하고, AI 에이전트 연구의 미래에 대한 저희의 조심스러운 낙관론, AI 에이전트가 과장(hype)인지 실체(substance)인지에 대한 통찰, 그리고 논문의 주요 내용을 간략하게 설명합니다.

**'에이전트(agent)'라는 말, 그 의미는 무엇이며 단순히 일시적인 유행어(buzzword)에 불과할까요?**

'에이전트(agent)'라는 용어는 인공지능 연구자들 사이에서 명확한 공식 정의 없이 폭넓게 사용되어 왔습니다. 이로 인해 때로는 마케팅 용어로 잘못 활용되기도 했으며, 그 사용에 대한 반발심을 불러일으키기도 했습니다. 그러나 이 용어가 완전히 무의미한 것은 아닙니다. 많은 학자들이 언어 모델 기반 시스템(language-model-driven systems)의 맥락에서 자율적인 개체(autonomous entity)를 구성하는 요소에 대한 공동체의 직관적 이해를 체계화하려는 노력을 기울여왔습니다 [1, 2, 3, 4, 5]. 이는 이분법적인 개념이라기보다는 '에이전트적(agentic)'이라는 표현처럼, 연속적인 스펙트럼(spectrum)으로 이해될 수 있습니다. 위에 언급된 인공지능 에이전트(AI agents)에 대한 다섯 가지 최근 정의는 각각 차이가 있지만, 공통적으로 강한 유사성을 지니고 있습니다. 저희는 새로운 정의를 제안하는 대신, 기존 개념들을 바탕으로 인공지능 시스템이 더욱 '에이전트적(agentic)'으로 간주되도록 하는 세 가지 속성 집합(clusters of properties)을 식별했습니다.

*   **환경적 복잡성과 목적 추구 (Environmental complexity and goal pursuit)**. 운영 환경이 복잡할수록, 즉 다양한 작업 범위, 다수의 이해관계자(stakeholders), 장기적인 시간 범위(operational horizons), 그리고 예측 불가능한 변화를 포함할수록 해당 환경에서 작동하는 인공지능 시스템은 더 에이전트적(agentic)입니다. 또한, 목표 달성 방법에 대한 명시적인 지침 없이 복잡한 목적을 추구하는 시스템도 더욱 에이전트적(agentic)인 특성을 보입니다.
*   **상호작용 방식 및 자율성 (Interaction modality and autonomy level)**. 자연어(natural language)로 지시를 받고 사용자를 대신하여 자율적으로 행동할 수 있는 인공지능 시스템은 더 에이전트적(agentic)입니다. 특히 사용자 감독(human oversight)이 덜 필요한 시스템일수록 더욱 에이전트적(agentic)입니다. 예를 들어, 대화형 인공지능(conversational AI platforms)은 물리적인 세계에서 직접적인 행동을 취할 수는 없지만, 플러그인(plugins)을 통합하면(예: ChatGPT용 Zapier) 사용자를 대신하여 특정 작업을 수행할 수 있습니다.
*   **내부 구조 및 작동 원리 (Internal architecture and operational principles)**. 웹 정보 검색(web information retrieval)이나 명령줄 인터페이스(command-line interfaces)와 같은 외부 유틸리티를 활용하거나, 전략적 계획(strategic planning)(예: 이전 결과에 대한 성찰, 목표를 하위 목표(subgoals)로 분해하는 것)을 사용하는 시스템은 더 에이전트적(agentic)입니다. 정적인 프로그램(static program)이 언어 모델을 호출하는 방식이 아니라, 언어 모델 자체가 제어 흐름(sequence of operations)을 주도하는 시스템은 더욱 에이전트적(agentic)입니다.

**에이전트(agents)는 실제로 효용성이 있을까요?**

챗GPT(ChatGPT)의 코드 인터프리터(code interpreter)/데이터 분석 모드(data analysis mode)와 같은 특정 에이전트 기능은 유용함이 입증되었지만, 지금까지 출시된 더욱 야심 찬 에이전트 기반 제품들은 아직 광범위한 성공을 거두지 못했습니다. AI 에이전트(AI agents) 개념에 기반한 두 가지 주요 제품 출시작은 래빗 R1(Rabbit R1)과 휴메인 AI 핀(Humane AI Pin)이었습니다. 이 장치들은 스마트폰 의존도를 없애거나 줄이겠다고 약속했지만, 결국 느리고 신뢰할 수 없음이 드러났습니다. "AI 소프트웨어 엔지니어(AI software developer)"인 데빈(Devin)은 몇 달 전 큰 기대를 모으며 발표되었지만, 비디오 리뷰(video demonstrations)에서 혹평을 받았고 여전히 대기 목록(limited access phase) 모드로 남아있습니다. 인공지능 에이전트(AI agents)가 실제 상업적 응용 분야에서 상당한 가치를 제공하기까지는 아직 갈 길이 멀다는 것이 분명합니다.

**그렇다면 인공지능 에이전트(AI agents)는 모두 과장(hype)에 불과할까요?**

아직 단정하기에는 이릅니다. 우리는 위에 언급된 자율 시스템(autonomous systems)이 광범위하게 채택될 만큼 충분히 잘 작동하기 전에 해결해야 할 근본적인 연구 과제(research hurdles)가 있다고 생각합니다. 따라서 인공지능 에이전트(AI agents)에 대한 지속적인 연구는 상당한 가치를 지닙니다. 가장 중요한 과제 중 하나는 신뢰성(dependability)입니다. 대규모 언어 모델(large language models)은 디지털 비서(digital assistant)가 처리하기를 원하는 많은 작업을 수행할 만큼 이미 충분한 역량(capabilities)을 가지고 있지만, 견고한 제품 배포(robust product deployment)에 필요한 수준의 일관성(consistency)은 아직 부족합니다. 예를 들어, 언어 모델과 수십 번의 상호작용을 요구하는 자동화된 여행 예약 시스템(automated travel booking system)을 상상해 보십시오. 만약 각 상호작용이 독립적으로 단 2%의 오류 확률을 가진다면, 누적된 시스템은 그 신뢰성 부족으로 인해 완전히 비실용적이 될 것입니다. 이는 우리가 목격한 일부 제품 실패를 부분적으로 설명합니다. 결과적으로, 기초 언어 모델 자체의 개선 없이도 신뢰성 연구의 발전은 수많은 새로운 응용 분야를 개척할 수 있습니다. 스케일링(scaling)의 한계에 도달한다면, 자율 에이전트(autonomous agents)는 인공지능 발전의 가장 논리적인 다음 단계입니다. 그러나 현재로서는 평가 방법론(evaluation methodologies)이 충분히 엄격하지 못하여, 초기 기계 학습 연구 시대에 표준화된 관행이 정립되기 전과 마찬가지로, 연구 자체도 과장된 주장과 과도한 낙관론(excessive optimism)에 일조하고 있습니다. 바로 이것이 저희 논문의 핵심 주제입니다.

**논문의 기여(Contributions of the paper)**

저희 논문의 핵심 질문은 다음과 같습니다. 인공지능 커뮤니티(AI community)는 단순한 합성 벤치마크(synthetic benchmarks)에서만 우수한 것이 아니라 실제 세계에서 유용한 인공지능 에이전트(AI agents)의 개발을 촉진하기 위해 어떤 변화를 적용해야 할까요? 저희 연구는 다섯 가지 주요 권고 사항을 제시합니다.

1.  **비용 제어 평가 도입 (Integrate cost-controlled evaluations)**. 대부분의 인공지능 에이전트(AI agents)의 기반이 되는 대규모 언어 모델(large language models)은 본질적으로 확률적(probabilistic)입니다. 이는 기본 모델(underlying model)을 단순히 여러 번 호출하는 것만으로도 성능 지표(performance metrics)를 인위적으로 높일 수 있음을 의미합니다. 저희는 휴먼이벌(HumanEval)과 같은 벤치마크에서 이러한 간단한 기술이 훨씬 적은 계산 비용으로 복잡한 에이전트 아키텍처(agent architectures)를 능가할 수 있음을 보여줍니다. 저희는 모든 에이전트 성능 평가가 비용 고려 사항을 포함해야 한다고 주장합니다. (이 발견은 저희가 처음 제시했으며, 이 발표 후 두 달 이내에 파레토 곡선(Pareto curves)을 이용한 비용 및 정확성(cost and accuracy)의 동시 최적화(joint optimization)가 에이전트 평가에서 점차 보편화되었습니다.)
2.  **정확성 및 자원 소모의 동시 최적화 (Jointly optimize accuracy and resource consumption)**. 추론 비용(inference expenditure)에 대한 정확성(accuracy)을 나타내는 파레토 곡선(Pareto curve)을 통해 평가 결과(assessment outcomes)를 시각화하는 것은 에이전트 설계(agent design)를 위한 새로운 길을 열어줍니다. 즉, 두 측정 기준을 동시에 최적화하는 것입니다. 저희는 DSPy 프레임워크(framework)를 수정하여 HotPotQA 데이터셋에서 정확성을 유지하면서 운영 비용(operational costs)을 절감할 수 있는 방법을 보여줍니다.
3.  **모델 성능 평가와 실제 적용 평가의 분리 (Differentiate between model performance assessment and downstream application evaluation)**. 노벨QA(NovelQA)에 대한 사례 연구(case study)를 통해, 모델 평가(model assessment)를 위해 의도된 벤치마크가 실제 적용 사례(downstream use cases)에 사용될 때 어떻게 오해를 불러일으킬 수 있는지 밝힙니다. 저희는 실제 적용 평가가 모델 매개변수 수(number of model parameters)와 같은 비용 대리 지표(proxy metrics for cost) 대신 실제 재정적 지출(actual financial outlays)을 정량화해야 한다고 주장합니다.
4.  **에이전트 벤치마크 내 편법 방지 (Prevent shortcuts in agent benchmarks)**. 저희는 에이전트 벤치마크가 다양한 형태의 과적합(overfitting)에 취약함을 입증합니다. 저희는 에이전트의 일반성(generality)을 네 가지 명확한 수준으로 분류하고, 원하는 일반화 수준에 따라 다른 유형의 홀드아웃 샘플(hold-out samples)이 필수적이라고 주장합니다. 적절한 홀드아웃이 없으면 에이전트 개발자(agent developers)는 의도치 않게 최적화되지 않은, 편법적인 해결책을 채택할 수 있습니다. 이 현상은 웹아레나(WebArena) 벤치마크를 사용한 사례 연구를 통해 예시됩니다.
5.  **에이전트 벤치마크의 표준화 및 재현성 강화 (Enhance standardization and reproducibility of agent benchmarks)**. 저희 연구는 웹아레나(WebArena) 및 휴먼이벌(HumanEval) 평가의 재현성(reproducibility)에서 만연한 결함(pervasive deficiencies)을 발견했습니다. 이러한 부정확성(inaccuracies)은 성능 추정치(performance estimates)를 부풀리고 에이전트 역량(agent capabilities)에 대한 과도하게 낙관적인 인식(overly optimistic perception)으로 이어집니다.
6.  **장기적이고 동적인 환경에서의 평가 (Evaluate in long-term, dynamic environments)**. 현재의 많은 벤치마크는 단기적이고 정적인 작업을 중심으로 설계되어 있습니다. 그러나 실제 에이전트의 가치는 변화하는 환경에 적응하고, 장기적인 목표를 추구하며, 예상치 못한 상황에 대처하는 능력에서 나옵니다. 우리는 시뮬레이션 기반 테스트나 지속적인 평가 프레임워크를 통해 이러한 동적인 환경에서의 에이전트 성능을 측정해야 한다고 제안합니다.

**결론: 조심스러운 낙관론의 근거(reasons for cautious optimism)**

인공지능 에이전트(AI agents) 평가는 초기 단계의 분야이며, 확립된 모범 사례(best practices)의 부재는 진정한 발전과 단순한 과장(sensationalism)을 구별하기 어렵게 만듭니다. 우리는 자율 에이전트(autonomous agents)가 기존 모델(conventional models)과 충분히 다르기 때문에 벤치마킹 방법론(benchmarking methodologies)을 재고해야 한다고 믿습니다. 저희 논문은 에이전트 평가를 위한 원칙적인 프레임워크(principled framework)를 향한 첫걸음을 제시합니다. 우리는 이러한 조치들이 인공지능 에이전트 평가의 엄격함(rigor)을 강화하고 미래 발전을 위한 확고한 토대(firm foundation)를 제공하기를 기대합니다.

저희 연구의 또 다른 갈래는 의학이나 사회 과학과 같은 과학 분야에서 기계 학습(ML-driven) 기반 연구의 재현성 위기(replication crisis)를 탐구합니다. 어떤 면에서는 저희의 현재 논문도 유사한 주제를 공유합니다. 저희의 전망은 기계 학습 기반 과학 분야에서 상황이 개선되기 전에 더 악화될 수 있다는 것입니다. 그러나 인공지능 에이전트 연구(AI agent research)에 관해서는, 관행이 빠르게 진화할 것이라고 조심스럽게 낙관합니다.

한 가지 요인은 발표된 논문과 함께 코드(code)와 데이터(data)를 공개적으로 공유하는 문화가 더욱 강력해져서 오류를 더 신속하게 식별할 수 있다는 점입니다. (이러한 문화적 변화는 지난 5년간의 집단적 노력 덕분에 이루어졌습니다.) 두 번째 이유는 오해의 소지가 있는 평가(misleading evaluations)를 기반으로 구축된 제품이 결국 실패할 때, 과도하게 열정적인 연구(overzealous research)가 빠르게 현실 점검(reality check)을 받기 때문입니다. 이 분야는 향후 몇 년 동안 학술 연구와 상업적 제품 출시(commercial product releases) 모두에서 흥미로운 관찰 영역이 될 것입니다.

1.  전통적인 인공지능에서 에이전트(agents)는 환경을 인지하고 그에 따라 행동하는 개체로 정의되지만, 대규모 언어 모델(LLM) 시대에는 이러한 정의가 덜 유용합니다. 그 정의에 따르면 온도 조절 장치(thermostat)조차도 에이전트(agent)로 분류될 수 있습니다. LLM 기반 에이전트는 단순히 반응하는 것을 넘어, 복잡한 계획과 추론을 통해 자율적으로 목표를 달성하려는 경향이 있습니다.