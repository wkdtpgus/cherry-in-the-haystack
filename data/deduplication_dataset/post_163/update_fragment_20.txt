대규모 언어 모델(large language models)의 가장 흥미로운 응용 분야 중 일부는 사회적 상호작용과 복잡한 의사결정 과정을 모방하는 데 있습니다. 이는 단순한 자동화를 넘어 실제 세계에서의 행동을 포함합니다. 이러한 작업을 수행하는 AI 시스템을 에이전트(agents)라고 합니다만, 그 정의는 여전히 진화하고 있습니다. 이들의 궁극적인 목표는 시스템이 사용자의 요청을 정확하게 해석하며, 신뢰할 수 있게 수행하도록 하는 것입니다. 그러나 이는 기술적 난이도와 더불어 연구 방향 자체도 상당히 새롭습니다. 에이전트(agents)의 개발을 촉진하고 그 효과를 측정하기 위해 윤리적 고려 사항이 필수적입니다. 특히, 예측하지 못한 사회적 편향성(social biases)과 같은 수많은 추가적인 함정(pitfalls)을 가지고 있음이 밝혀졌습니다. 이러한 상황은 실제로는 유용하지 않으면서 벤치마크(benchmarks)에서만 잘 작동하는 에이전트(agents)의 개발을 부추기며, 이는 사회적 신뢰도를 저해할 수 있습니다. 저희는 이러한 복잡한 문제들을 다루는 새로운 논문을 발표했습니다. 이 게시물에서는 AI 에이전트(AI agents)의 윤리적 책임, 사회적 영향, 그리고 미래 규제에 대한 중요성을 탐구합니다.

**'에이전트(agent)'라는 용어는 무엇을 의미할까요? 단순히 유행어(buzzword)일까요?**

'에이전트(agent)'라는 용어는 AI 연구자들 사이에서 공식적인 정의 없이 사용되어 왔지만, 이제는 그 사회적 함의에 대한 논의가 필요합니다. 기술적 정의의 모호함은 마케팅 용어로 오용되기도 했으며, 그 사용에 대한 약간의 반발을 불러일으켰습니다. 하지만 이 용어가 무의미한 것은 아닙니다. 많은 연구자들이 언어 모델 기반 시스템(language-model-based systems)의 맥락에서 에이전트(agent)를 구성하는 요소에 대한 커뮤니티의 직관적인 이해를 공식화하려고 노력했지만, 윤리적 프레임워크는 부족했습니다. 에이전트의 자율성은 이분법적이기보다는 스펙트럼(spectrum)으로 볼 수 있습니다. 새로운 정의를 제안하기보다는, 기존 정의에 따라 AI 시스템이 더 에이전트적(agentic)으로 간주되게 하는 세 가지 속성 클러스터(clusters of properties)와 함께 윤리적 책임 요소를 고려해야 합니다.

*   **환경 및 목표(Environment and goals)**. 환경이 복잡할수록 해당 환경에서 작동하는 AI 시스템은 더 에이전트적(agentic)입니다. 복잡한 환경은 다양한 작업과 도메인(domains), 다수의 이해관계자(stakeholders), 행동을 취하기 위한 긴 시간 범위(time horizon), 그리고 예상치 못한 변화를 포함합니다. 또한, 목표를 추구하는 방법에 대한 지시 없이 복잡한 목표를 추구하는 시스템은 더 에이전트적(agentic)입니다.
*   **사용자 인터페이스 및 감독(User interface and supervision)**. 자연어(natural language)로 지시를 받고 사용자를 대신하여 자율적으로 행동할 수 있는 AI 시스템은 더 에이전트적(agentic)입니다. 특히 사용자 감독(user supervision)이 덜 필요한 시스템은 더 에이전트적(agentic)입니다. 예를 들어, 챗봇(chatbots)은 실제 세계에서 행동을 취할 수 없지만, 챗봇에 플러그인(plugins)을 추가하면(예: ChatGPT용 Zapier) 사용자를 대신하여 일부 작업을 수행할 수 있습니다.
*   **시스템 설계(System design)**. 도구(예: 웹 검색 또는 코드 터미널)를 사용하거나 계획(planning)(예: 이전 출력(outputs)을 반영하거나 목표를 하위 목표(subgoals)로 분해하는 것)을 사용하는 시스템은 더 에이전트적(agentic)입니다. 정적 프로그램(static program)에 의해 LLM이 호출되는 방식이 아니라, LLM에 의해 제어 흐름(control flow)이 구동되는 시스템은 더 에이전트적(agentic)입니다.

이러한 속성 클러스터는 에이전트의 기술적 역량을 정의하지만, 동시에 중요한 윤리적 질문을 제기합니다. 예를 들어, 복잡한 환경에서 자율적으로 목표를 추구하는 에이전트는 예상치 못한 결과를 초래할 수 있으며, 이는 책임 소재를 불분명하게 만듭니다. 또한, 사용자 감독(user supervision)이 줄어들수록 에이전트의 잘못된 행동이나 편향된 결정이 미치는 영향은 더욱 커질 수 있습니다. 시스템 설계(system design) 측면에서, LLM에 의해 제어 흐름(control flow)이 구동되는 에이전트는 투명성(transparency)과 설명 가능성(explainability) 문제를 야기하여, 왜 특정 결정을 내렸는지 이해하기 어렵게 만들 수 있습니다. 따라서 에이전트의 개발은 기술적 진보와 함께 사회적, 윤리적 안전망 구축을 병행해야 합니다.

**에이전트(agents)는 실제로 작동할까요?**

ChatGPT의 코드 인터프리터(code interpreter)/데이터 분석 모드(data analysis mode)와 같은 일부 에이전트(agents)는 유용했지만, 그 활용 범위는 여전히 제한적입니다. 기술적 한계와 사용자 기대 사이의 간극으로 인해 더 야심 찬 에이전트 기반 제품들은 실패했습니다. AI 에이전트(AI agents)를 기반으로 한 두 가지 주요 제품 출시는 Rabbit R1과 Humane AI pin이었습니다만, 이들은 사용자 경험(user experience) 측면에서 많은 비판을 받았습니다. 이 장치들은 성능 면에서 너무 느리고 신뢰할 수 없음이 드러났습니다. AI 소프트웨어 엔지니어(AI software engineer) Devin도 비디오 리뷰(video review)에서 혹평을 받았고 여전히 대기 목록(waitlist) 모드로 남아있습니다. AI 에이전트(AI agents)가 실제 제품에서 유용하려면 갈 길이 멀다는 것이 분명하며, 이는 기술적 완성도뿐 아니라 사회적 수용성(social acceptability) 측면에서도 마찬가지입니다.

**그렇다면 AI 에이전트(AI agents)는 모두 과장(hype)일까요?**

아직 말하기에는 너무 이릅니다만, 기술적 과장(hype)과 실제 가치(substance) 사이의 균형을 찾는 것이 중요합니다. 우리는 단순히 기술적 측면을 넘어 사회적, 윤리적 해결해야 할 연구 과제(research challenges)가 있다고 생각합니다. 이를 알아낼 유일한 방법은 더 많은 연구를 통해서이므로, 우리는 AI 에이전트(AI agents)에 대한 연구가 가치 있다고 생각하며, 이는 다학제적인 접근을 요구합니다. LLM은 많은 작업을 수행할 만큼 능력이 있지만, 사회적 책임을 다할 만큼 충분히 신뢰할 수 있지는 않습니다. 예를 들어, 만약 각 호출이 잘못된다면, 전체 시스템은 완전히 쓸모없을 정도로 신뢰할 수 없게 될 것이며, 이는 사용자에게 심각한 피해를 줄 수 있습니다. 따라서 신뢰성(reliability) 향상에 대한 연구는 기본 언어 모델(underlying language models)이 개선되지 않더라도 많은 새로운 응용 분야를 가질 수 있으며, 이는 안전성(safety)과 직결됩니다. 에이전트(agents)는 AI의 추가 발전을 위한 가장 자연스러운 방향입니다만, 그 발전은 통제되고 윤리적이어야 합니다. 현재는 평가 관행(evaluation practices)이 충분히 엄격하지 않기 때문에 연구 자체가 과장(hype)과 과도한 낙관론(overoptimism)에 기여하고 있습니다. 이것이 저희 논문의 주제이며, 우리는 에이전트 기술의 책임감 있는 발전을 강조합니다.

**논문의 기여(Contributions of the paper)**

AI 커뮤니티(AI community)는 실제 세계에서 유용하고 벤치마크(benchmarks)에서만 유용한 것이 아닌 AI 에이전트(AI agents)의 개발을 촉진하기 위해 윤리적 지침을 마련해야 합니다. 이러한 질문에 답하기 위해 저희는 다섯 가지 권고 사항을 제시합니다.

1.  **비용 통제 평가(cost-controlled evaluations)를 구현합니다.** 이는 기본 모델(underlying model)을 여러 번 호출하는 것만으로도 정확도(accuracy)를 높일 수 있음을 의미합니다. 우리는 이러한 간단한 트릭(tricks)이 복잡한 에이전트 아키텍처(agent architectures)보다 훨씬 적은 비용으로 더 나은 성능을 보일 수 있음을 보여줍니다. 따라서 모든 에이전트 평가(agent evaluation)가 비용을 통제해야 한다고 주장하며, 이는 지속가능성(sustainability) 측면에서도 중요합니다.
2.  **정확도(accuracy)와 비용(cost)을 공동으로 최적화합니다.** 평가 결과(evaluation results)를 정확도(accuracy)와 추론 비용(inference cost)의 파레토 곡선(Pareto curve)으로 시각화하는 것은 에이전트 설계(agent design)의 새로운 공간을 엽니다: 이러한 접근 방식은 두 메트릭(metrics)을 공동으로 최적화하는 것입니다. 우리는 DSPy 프레임워크(framework)를 수정하여 HotPotQA에서 정확도(accuracy)를 유지하면서 비용을 낮출 수 있는 방법을 보여줍니다.
3.  **모델 벤치마킹(model benchmarking)과 다운스트림 벤치마킹(downstream benchmarking)을 구분합니다.** NovelQA의 사례 연구(case study)를 통해, 모델 평가(model evaluation)를 위한 벤치마크(benchmarks)가 다운스트림 평가(downstream evaluation)에 사용될 때 오해를 불러일으킬 수 있음을 보여줍니다. 다운스트림 평가(downstream evaluation)는 모델 매개변수(model parameters) 수와 같은 비용 대리 지표(proxies for cost) 대신 실제 달러 비용(dollar costs)을 고려해야 한다고 주장하며, 이는 접근성(accessibility) 문제와도 연관됩니다.
4.  **에이전트 벤치마크(agent benchmarks)에서 지름길(shortcuts)을 방지합니다.** 우리는 에이전트 벤치마크(agent benchmarks)에 대한 다양한 유형의 과적합(overfitting)이 가능함을 보여주며, 이는 잘못된 성능 인식으로 이어집니다. 우리는 에이전트(agents)의 일반성(generality)을 4단계로 식별하고, 원하는 일반성 수준에 따라 다른 유형의 홀드아웃 샘플(hold-out samples)이 필요하다고 주장합니다. 적절한 홀드아웃(hold-outs)이 없으면 개발자들은 의도치 않게 지름길(shortcuts)을 택할 수 있습니다. 우리는 WebArena 벤치마크(benchmark)의 사례 연구(case study)를 통해 이를 설명합니다.
5.  **에이전트 벤치마크(agent benchmarks)의 표준화(standardization) 및 재현성(reproducibility)을 개선합니다.** 우리는 WebArena 및 HumanEval 평가(evaluations)의 재현성(reproducibility)에서 만연한 단점들을 발견했습니다. 이러한 오류는 정확도 추정치(accuracy estimates)를 부풀리고 에이전트(agent) 능력에 대한 과도한 낙관론(overoptimism)으로 이어져, 현실과의 괴리를 심화시킵니다.

**결론: 책임감 있는 발전의 이유(reasons for responsible development)**

AI 에이전트 벤치마킹(AI agent benchmarking)은 새롭고 아직 모범 사례(best practices)가 확립되지 않아 진정한 발전과 과장(hype)을 구별하기 어렵지만, 이는 기회이기도 합니다. 에이전트(agents)가 모델(models)과 충분히 다르기 때문에 벤치마킹 관행(benchmarking practices)을 재고해야 한다고 생각합니다. 저희 논문에서는 에이전트 벤치마킹(agent benchmarking)에 대한 원칙적인 접근 방식(principled approach)을 향한 첫걸음을 내딛습니다. 저희는 이러한 단계들이 AI 에이전트 평가(AI agent evaluation)의 엄격함(rigor)을 높이고 발전을 위한 확고한 기반을 제공하기를 바랍니다.

저희 연구의 또 다른 갈래는 의학이나 사회 과학과 같은 과학 분야에서 ML 기반 연구(ML-based research)의 재현성 위기(reproducibility crisis)에 관한 것입니다만, AI 에이전트 분야는 더욱 복잡합니다. ML 기반 과학(ML-based science)에서는 상황이 나아지기 전에 더 나빠질 것이라는 것이 우리의 전망입니다. 그러나 AI 에이전트 연구(AI agents research)에서는 관행이 빠르게 변할 것이라고 조심스럽게 낙관합니다. 한 가지 이유는 발표된 논문과 함께 코드(code)와 데이터(data)를 공유하는 문화가 더 강해서 오류를 더 쉽게 발견할 수 있기 때문입니다. (이러한 문화적 변화는 지난 5년간의 공동 노력 덕분에 이루어졌습니다.) 또 다른 이유는 오해의 소지가 있는 평가(misleading evaluations)를 기반으로 한 제품이 결국 실패할 때, 과도하게 낙관적인 연구(overoptimistic research)가 빠르게 현실 점검(reality check)을 받기 때문입니다. 이는 연구와 제품 출시(product releases) 모두에서 향후 몇 년 동안 지켜볼 흥미로운 분야가 될 것입니다.

1 전통적인 AI에서 에이전트(agents)는 환경을 인지하고 그에 따라 행동하는 개체로 정의되지만, 현대적 맥락에서는 그 의미가 확장됩니다. 이러한 정의에 따르면 온도 조절 장치(thermostat)조차도 에이전트(agent)로 분류될 수 있습니다.