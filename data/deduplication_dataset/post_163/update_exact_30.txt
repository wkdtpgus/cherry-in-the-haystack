대규모 언어 모델(large language models)의 가장 흥미로운 응용 분야 중 일부는 항공권 예약이나 소프트웨어 버그(software bugs)를 찾아 수정하는 것과 같은 실제 세계에서의 행동을 포함합니다. 이러한 작업을 수행하는 AI 시스템을 에이전트(agents)라고 합니다. 이들은 웹 검색(web search) 및 코드 터미널(code terminals)과 같은 도구를 사용하기 위해 LLM을 다른 소프트웨어와 결합하여 사용합니다. 이 분야의 궁극적인 목표는 Siri나 Alexa와 같은 비서(assistants)를 구축하고, 이들이 실제로 작동하여 복잡한 작업을 처리하고, 사용자의 요청을 정확하게 해석하며, 신뢰할 수 있게 수행하도록 하는 것입니다. 저희는 에이전트(agents) 평가의 과제를 식별하고 이를 해결할 방법을 제안하는 새로운 논문을 발표했습니다. 논문은 여기에서 읽을 수 있습니다. 저자는 프린스턴 대학교(Princeton University)의 Sayash Kapoor, Benedikt Ströbl, Zachary S. Siegel, Nitya Nadgir, Arvind Narayanan입니다.

AI 에이전트 연구는 최근 몇 년간 대규모 언어 모델의 발전과 함께 폭발적인 성장을 이루었습니다. 이 기술은 단순히 텍스트를 생성하는 것을 넘어, 실제 환경과 상호작용하며 목표를 달성하는 능력을 지향합니다. 하지만 이러한 높은 잠재력만큼이나, 에이전트의 개발과 평가는 복잡하고 다층적인 도전 과제를 안고 있습니다. 특히, 실질적인 유용성과 벤치마크에서의 성능 간의 괴리가 커지면서, 우리는 에이전트 기술의 본질적인 한계와 발전 방향에 대해 깊이 고민해야 할 시점에 이르렀습니다. 이 게시물에서는 AI 에이전트의 정의를 다시 한번 살펴보고, 현재 직면한 기술적 한계와 이를 극복하기 위한 연구 방향, 그리고 저희 논문에서 제안하는 평가 방법론에 대해 자세히 논의하고자 합니다. 궁극적으로, AI 에이전트가 단순한 '과장(hype)'을 넘어 '실체(substance)'가 되기 위한 길을 모색합니다.

**'에이전트(agent)'라는 용어는 무엇을 의미할까요? 단순히 유행어(buzzword)일까요?**

'에이전트(agent)'라는 용어는 AI 연구자들 사이에서 공식적인 정의 없이 사용되어 왔습니다. 1 이로 인해 마케팅 용어로 오용되기도 했으며, 그 사용에 대한 약간의 반발을 불러일으켰습니다. 하지만 이 용어가 무의미한 것은 아닙니다. 많은 연구자들이 언어 모델 기반 시스템(language-model-based systems)의 맥락에서 에이전트(agent)를 구성하는 요소에 대한 커뮤니티의 직관적인 이해를 공식화하려고 노력했습니다 [1, 2, 3, 4, 5]. 이분법적이기보다는 '에이전트적(agentic)'이라는 용어로 표현되기도 하는 스펙트럼(spectrum)으로 볼 수 있습니다. 위에 인용된 AI 에이전트(AI agents)에 대한 다섯 가지 최근 정의는 모두 다르지만 서로 강한 유사성을 가지고 있습니다. 새로운 정의를 제안하기보다는, 기존 정의에 따라 AI 시스템이 더 에이전트적(agentic)으로 간주되게 하는 세 가지 속성 클러스터(clusters of properties)를 식별했습니다.

*   **시스템 설계(System design)**. 도구(예: 웹 검색 또는 코드 터미널)를 사용하거나 계획(planning)(예: 이전 출력(outputs)을 반영하거나 목표를 하위 목표(subgoals)로 분해하는 것)을 사용하는 시스템은 더 에이전트적(agentic)입니다. 정적 프로그램(static program)에 의해 LLM이 호출되는 방식이 아니라, LLM에 의해 제어 흐름(control flow)이 구동되는 시스템은 더 에이전트적(agentic)입니다.

이러한 '에이전트적(agentic)' 속성들은 AI 시스템이 단순히 지시를 따르는 것을 넘어, 복잡한 환경에서 자율적으로 추론하고 행동하는 능력을 얼마나 잘 갖추고 있는지를 나타냅니다. 예를 들어, **환경 및 목표(Environment and goals)** 측면에서는 에이전트가 얼마나 다양한 변수와 불확실성을 가진 환경에서 복잡하고 추상적인 목표를 효과적으로 추구할 수 있는지가 중요합니다. 단순히 고정된 규칙에 따라 움직이는 시스템보다는, 예상치 못한 상황에 유연하게 대처하고 장기적인 목표를 위해 스스로 하위 목표를 설정하는 시스템이 더 에이전트적입니다. 또한, **사용자 인터페이스 및 감독(User interface and supervision)** 측면에서는 자연어로 주어진 지시를 이해하고, 최소한의 사용자 개입만으로도 복잡한 작업을 수행할 수 있는 자율성이 핵심입니다. 사용자가 매 단계마다 지시를 내려야 하는 시스템보다는, 높은 수준의 지시만으로도 여러 단계를 스스로 계획하고 실행하며, 필요할 때만 사용자에게 피드백을 요청하는 시스템이 훨씬 더 '에이전트적'이라고 할 수 있습니다. 이러한 속성들은 에이전트의 자율성과 지능을 평가하는 중요한 기준이 됩니다.

**에이전트(agents)는 실제로 작동할까요?**

최근 몇 년간 AI 에이전트 분야는 많은 기대를 모았으나, 실제 제품 출시에서는 여러 한계를 드러냈습니다. 앞서 언급된 Rabbit R1과 Humane AI pin은 사용자들의 높은 기대를 충족시키지 못했습니다. 이들 장치는 스마트폰을 대체할 만한 혁신적인 AI 비서를 표방했지만, 느린 응답 속도, 잦은 오류, 제한적인 기능으로 인해 실용성이 떨어진다는 평가를 받았습니다. 또한, "AI 소프트웨어 엔지니어"로 기대를 모았던 Devin 역시 초기 데모 영상과 달리 실제 사용 환경에서는 버그 수정이나 복잡한 코드 작성에 어려움을 겪으며 혹평을 받았습니다. 이러한 실패 사례들은 AI 에이전트가 단순히 '언어 이해'를 넘어 '실제 행동'으로 이어지는 과정에서 얼마나 많은 기술적 난관에 부딪히는지 여실히 보여줍니다.

주요 실패 원인으로는 다음과 같은 기술적 문제들을 들 수 있습니다. 첫째, **다단계 추론의 취약성**입니다. 에이전트는 하나의 목표를 달성하기 위해 여러 단계의 의사결정과 행동을 연결해야 하는데, 각 단계에서 발생하는 LLM의 작은 오류(환각, 부정확한 도구 사용 등)가 누적되어 최종적으로는 큰 실패로 이어질 수 있습니다. 둘째, **실세계의 복잡성 처리 능력 부족**입니다. 실제 환경은 예측 불가능하고 모호하며, 에이전트가 접하는 정보는 불완전할 수 있습니다. 에이전트는 이러한 불확실성 속에서 견고하게 작동해야 하지만, 현재 LLM 기반 에이전트는 미묘한 상황 변화나 예외 처리에 취약합니다. 셋째, **비용과 지연 시간(latency) 문제**입니다. 복잡한 작업을 수행하는 에이전트는 수십 번의 LLM 호출과 외부 도구 사용을 필요로 하는데, 이는 상당한 컴퓨팅 자원과 시간을 소모합니다. 사용자들은 즉각적인 반응을 기대하지만, 현재 에이전트의 처리 속도는 이를 따라가지 못하는 경우가 많습니다.

그럼에도 불구하고, 특정 분야에서는 에이전트적 접근 방식이 성공적으로 적용되고 있습니다. 예를 들어, ChatGPT의 코드 인터프리터(code interpreter)나 데이터 분석 모드(data analysis mode)는 제한된 환경 내에서 강력한 도구 사용 능력을 보여주며 유용성을 입증했습니다. 이는 잘 정의된 환경과 명확한 목표를 가진 작업에서는 에이전트가 뛰어난 성능을 발휘할 수 있음을 시사합니다. 또한, GitHub Copilot과 같은 고급 코딩 보조 도구들은 개발자의 작업 흐름에 통합되어, 코드 생성, 버그 제안, 리팩토링 등 특정 에이전트적 기능을 제공하며 생산성 향상에 기여하고 있습니다. 이러한 성공 사례들은 에이전트 기술이 아직 초기 단계에 있지만, 적절한 제약 조건과 설계가 뒷받침된다면 충분히 실용적인 가치를 제공할 수 있음을 보여줍니다.

**그렇다면 AI 에이전트(AI agents)는 모두 과장(hype)일까요?**

아직 말하기에는 너무 이릅니다. 현재의 에이전트 기술이 직면한 과제들은 분명하지만, 이는 기술 발전의 자연스러운 과정입니다. 에이전트 연구는 AI의 다음 단계를 이끌 중요한 분야이며, 현재의 한계점들은 오히려 더 깊이 있는 연구와 혁신적인 해결책을 모색할 기회를 제공합니다. 특히, 에이전트의 신뢰성(reliability)을 향상시키는 것은 핵심적인 연구 과제입니다. 현재 LLM은 많은 작업을 수행할 능력이 있지만, 성공적인 제품이 될 만큼 충분히 신뢰할 수 있지는 않습니다. 예를 들어, 항공권 예약 에이전트가 수십 번의 API 호출을 해야 한다면, 각 호출에서 2%의 오류 확률만 있어도 전체 시스템의 신뢰성은 급격히 떨어져 실용성이 매우 낮아집니다. 따라서 신뢰성 향상에 대한 연구는 기본 언어 모델이 개선되지 않더라도 많은 새로운 응용 분야를 창출할 수 있습니다.

신뢰성 외에도, 에이전트의 **견고성(robustness)**과 **안전성(safety)** 확보는 매우 중요합니다. 에이전트가 다양한 입력과 예상치 못한 상황에서도 안정적으로 작동하고, 의도치 않은 해로운 행동을 하지 않도록 보장해야 합니다. 이를 위해 **자율적인 자기 수정(self-correction) 메커니즘**, **외부 지식 기반(external knowledge bases)과의 통합**, 그리고 **인간 개입(human-in-the-loop)을 통한 감독 및 피드백 시스템** 등이 활발히 연구되고 있습니다. 또한, 에이전트의 의사결정 과정을 **설명 가능하도록(explainable)** 만들고, 특정 중요한 단계에 대해서는 **형식 검증(formal verification)**과 같은 엄격한 접근 방식을 적용하여 안전성을 확보하려는 노력도 이어지고 있습니다. 이러한 연구들은 에이전트가 단순히 벤치마크에서 높은 점수를 얻는 것을 넘어, 실제 세계에서 책임감 있고 유용하게 작동할 수 있도록 만드는 데 필수적입니다.

**논문의 기여(Contributions of the paper)**

AI 커뮤니티(AI community)는 실제 세계에서 유용하고 벤치마크(benchmarks)에서만 유용한 것이 아닌 AI 에이전트(AI agents)의 개발을 촉진하기 위해 어떤 변화를 구현해야 할까요? 이것이 논문의 핵심 질문입니다. 저희는 다섯 가지 권고 사항을 제시합니다.

1.  **비용 통제 평가(cost-controlled evaluations)를 구현합니다.** 대부분의 AI 에이전트(AI agents)의 기반이 되는 언어 모델(language models)은 확률적(stochastic)입니다. 이는 단순히 기본 모델(underlying model)을 여러 번 호출하는 것만으로도 정확도(accuracy)를 높일 수 있음을 의미합니다. 우리는 이러한 간단한 트릭(tricks)이 HumanEval 벤치마크(benchmark)에서 복잡한 에이전트 아키텍처(agent architectures)보다 훨씬 적은 비용으로 더 나은 성능을 보일 수 있음을 보여줍니다. 우리는 모든 에이전트 평가(agent evaluation)가 비용을 통제해야 한다고 주장합니다. (우리는 이 발견을 여기에서 처음 발표했습니다. 이 게시물을 발표한 지 두 달 만에 파레토 곡선(Pareto curves)과 비용 및 정확도(cost and accuracy)의 공동 최적화(joint optimization)가 에이전트 평가(agent evaluations)에서 점점 더 보편화되었습니다.)

이 외에도 저희 논문은 에이전트 평가의 엄격성을 높이기 위한 다음과 같은 중요한 권고 사항들을 제시합니다.

2.  **정확도(accuracy)와 비용(cost)을 공동으로 최적화합니다.** 에이전트의 성능은 단순히 정확도만으로 측정될 수 없습니다. 실제 환경에서는 LLM 호출 비용, API 사용료, 처리 시간 등 다양한 비용이 발생합니다. 파레토 곡선(Pareto curve)을 통해 정확도와 비용 사이의 상충 관계(trade-off)를 시각화하고, 이 두 가지 메트릭을 동시에 최적화하는 에이전트 설계 방법을 모색해야 합니다. 이는 개발자들이 한정된 자원 내에서 최적의 성능을 달성할 수 있도록 돕습니다.

3.  **모델 벤치마킹(model benchmarking)과 다운스트림 벤치마킹(downstream benchmarking)을 구분합니다.** LLM 자체의 성능을 평가하는 벤치마크와 실제 애플리케이션에서 에이전트의 성능을 평가하는 벤치마크는 명확히 구분되어야 합니다. 모델의 파라미터 수와 같은 내부 지표보다는, 에이전트가 실제 작업을 수행하는 데 드는 실제 달러 비용(dollar costs)과 같은 외부적이고 실질적인 비용을 고려한 다운스트림 평가가 중요합니다. NovelQA 사례 연구에서 볼 수 있듯이, 모델 평가용 벤치마크를 에이전트의 다운스트림 평가에 사용하는 것은 오해를 불러일으킬 수 있습니다.

4.  **에이전트 벤치마크(agent benchmarks)에서 지름길(shortcuts)을 방지합니다.** 벤치마크에 과적합(overfitting)되는 현상은 에이전트 연구의 신뢰성을 저해할 수 있습니다. 에이전트 개발자가 특정 벤치마크의 약점을 이용하여 실제 능력 없이 높은 점수를 얻는 '지름길'을 택하지 않도록, 다양한 유형의 홀드아웃 샘플(hold-out samples)을 사용하여 에이전트의 일반성(generality)을 철저히 검증해야 합니다. WebArena 벤치마크 사례 연구는 이러한 과적합의 위험성을 명확히 보여줍니다.

5.  **에이전트 벤치마크(agent benchmarks)의 표준화(standardization) 및 재현성(reproducibility)을 개선합니다.** 에이전트 평가 결과의 신뢰성을 높이기 위해서는 평가 환경, 방법론, 메트릭에 대한 명확한 표준화가 필요합니다. 또한, 연구자들이 서로의 결과를 쉽게 재현하고 검증할 수 있도록 코드와 데이터 공개를 의무화하는 문화가 정착되어야 합니다. WebArena 및 HumanEval 평가에서 발견된 재현성 문제는 정확도 추정치를 부풀리고 에이전트 능력에 대한 과도한 낙관론으로 이어질 수 있음을 경고합니다.

**결론: 조심스러운 낙관론의 이유(reasons for cautious optimism)**

AI 에이전트 벤치마킹은 새롭고 아직 모범 사례(best practices)가 확립되지 않아 진정한 발전과 과장(hype)을 구별하기 어렵습니다. 우리는 에이전트(agents)가 모델(models)과 충분히 다르기 때문에 벤치마킹 관행(benchmarking practices)을 재고해야 한다고 생각합니다. 저희 논문에서는 에이전트 벤치마킹(agent benchmarking)에 대한 원칙적인 접근 방식(principled approach)을 향한 첫걸음을 내딛습니다. 우리는 이러한 단계들이 AI 에이전트 평가(AI agent evaluation)의 엄격함(rigor)을 높이고 발전을 위한 확고한 기반을 제공하기를 바랍니다.

저희 연구의 또 다른 갈래는 의학이나 사회 과학과 같은 과학 분야에서 ML 기반 연구(ML-based research)의 재현성 위기(reproducibility crisis)에 관한 것입니다. 어떤 면에서는 저희의 현재 논문도 유사합니다. ML 기반 과학(ML-based science)에서는 상황이 나아지기 전에 더 나빠질 것이라는 것이 우리의 전망입니다. 그러나 AI 에이전트 연구(AI agents research)에서는 관행이 빠르게 변할 것이라고 조심스럽게 낙관합니다. 한 가지 이유는 발표된 논문과 함께 코드(code)와 데이터(data)를 공유하는 문화가 더 강해서 오류를 더 쉽게 발견할 수 있기 때문입니다. (이러한 문화적 변화는 지난 5년간의 공동 노력 덕분에 이루어졌습니다.) 또 다른 이유는 오해의 소지가 있는 평가(misleading evaluations)를 기반으로 한 제품이 결국 실패할 때, 과도하게 낙관적인 연구(overoptimistic research)가 빠르게 현실 점검(reality check)을 받기 때문입니다. 이는 연구와 제품 출시(product releases) 모두에서 향후 몇 년 동안 지켜볼 흥미로운 분야가 될 것입니다.

결론적으로, AI 에이전트 기술은 엄청난 잠재력을 가지고 있지만, 그 발전을 위해서는 현재의 벤치마크 및 평가 방법론을 근본적으로 재고해야 합니다. 단순한 성능 지표를 넘어, 실제 환경에서의 견고성, 신뢰성, 안전성을 종합적으로 평가하고, 비용 효율성을 고려하는 다각적인 접근 방식이 필요합니다. 이러한 노력은 에이전트가 단순히 '과장'으로 끝나지 않고, 인류에게 실질적인 가치를 제공하는 '실체'로 자리매김하는 데 결정적인 역할을 할 것입니다. 앞으로의 연구는 이러한 원칙적인 평가 프레임워크를 기반으로, 윤리적 고려사항(예: 자율적인 행동의 책임 소재, 편향성)까지 포함하여 더욱 심층적으로 발전해야 할 것입니다.

1 전통적인 AI에서 에이전트(agents)는 환경을 인지하고 그에 따라 행동하는 개체로 정의되지만, LLM 시대에는 그 정의가 덜 유용합니다. 그 정의에 따르면 온도 조절 장치(thermostat)조차도 에이전트(agent)로 분류될 수 있습니다.