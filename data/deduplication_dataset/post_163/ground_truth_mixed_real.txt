대규모 언어 모델(large language models, LLM)의 가장 흥미로운 응용 분야 중 일부는 항공권 예약이나 소프트웨어 버그(software bugs)를 찾아 수정하는 것과 같이 실제 세계에서의 행동을 포함합니다. 이러한 작업을 수행하는 AI 시스템을 에이전트(agents)라고 합니다. 이들은 웹 검색(web search) 및 코드 터미널(code terminals)과 같은 다양한 도구를 사용하기 위해 LLM을 다른 소프트웨어와 결합하여 활용합니다. 이 분야의 궁극적인 목표는 Siri나 Alexa와 같은 비서(assistants)를 구축하고, 이들이 실제로 작동하여 복잡한 작업을 처리하고, 사용자의 요청을 정확하게 해석하며, 신뢰할 수 있게 수행하도록 하는 것입니다. 하지만 에이전트 기술은 여전히 초기 단계에 있으며, 연구 방향이 빠르게 진화하고 있습니다. 에이전트(agents)의 개발을 촉진하고 그 효과를 객관적으로 측정하기 위해 연구자들은 벤치마크 데이터셋(benchmark datasets)을 구축해왔습니다. 그러나 LLM 평가(evaluation)는 지뢰밭과 같으며, 에이전트 평가(agent evaluation)는 오늘날의 벤치마크(benchmarks)와 평가 관행에 영향을 미치는 수많은 추가적인 함정(pitfalls)을 가지고 있음이 밝혀졌습니다. 이러한 상황은 실제로는 유용하지 않으면서 벤치마크(benchmarks)에서만 잘 작동하는 에이전트(agents)의 개발을 부추깁니다.

저희 연구팀은 최근 에이전트(agents) 평가의 여러 과제를 심층적으로 분석하고, 이를 해결할 수 있는 효과적인 방법론을 제안하는 새로운 논문을 발표했습니다. 논문은 여기에서 읽을 수 있습니다. 저자는 프린스턴 대학교(Princeton University)의 Sayash Kapoor, Benedikt Ströbl, Zachary S. Siegel, Nitya Nadgir, Arvind Narayanan입니다. 이 게시물에서는 AI 에이전트(AI agents)의 정의, AI 에이전트 연구의 미래에 대해 우리가 조심스럽게 낙관하는 이유, AI 에이전트가 과장(hype)인지 실체(substance)인지에 대한 생각, 그리고 논문에 대한 간략한 개요를 제공합니다. 빠르게 발전하는 AI 에이전트 분야에서 더욱 견고한 평가 기준을 마련하는 것이 중요해지고 있습니다.

**'에이전트(agent)'라는 용어는 무엇을 의미할까요? 단순히 유행어(buzzword)일까요?**

AI 연구 커뮤니티 내에서 '에이전트(agent)'라는 용어는 아직 통일된 공식적 정의 없이 다양하게 활용되어 왔습니다. 1 이로 인해 마케팅 용어로 오용되기도 했으며, 그 사용에 대한 약간의 반발을 불러일으켰습니다. 하지만 이 용어가 무의미한 것은 아닙니다. 많은 연구자들이 언어 모델 기반 시스템(language-model-based systems)의 맥락에서 에이전트(agent)를 구성하는 요소에 대한 커뮤니티의 직관적인 이해를 공식화하려고 노력했습니다 [1, 2, 3, 4, 5]. 이러한 노력 덕분에 '에이전트적(agentic)'이라는 용어로 표현되기도 하는 스펙트럼(spectrum)으로 이 개념을 이해하는 것이 보편화되고 있습니다. 위에 인용된 AI 에이전트(AI agents)에 대한 다섯 가지 최근 정의는 모두 다르지만 서로 강한 유사성을 가지고 있습니다. 새로운 정의를 제안하기보다는, 기존 정의에 따라 AI 시스템이 더 에이전트적(agentic)으로 간주되게 하는 세 가지 속성 클러스터(clusters of properties)를 식별했습니다.

*   **환경 및 목표(Environment and goals)**. 환경이 복잡할수록 해당 환경에서 작동하는 AI 시스템은 더 에이전트적(agentic)입니다. 복잡한 환경은 다양한 작업과 도메인(domains), 다수의 이해관계자(stakeholders), 행동을 취하기 위한 긴 시간 범위(time horizon), 그리고 예상치 못한 변화를 포함합니다. 또한, 목표를 추구하는 방법에 대한 지시 없이 복잡한 목표를 추구하는 시스템은 더 에이전트적(agentic)입니다.
*   **사용자 인터페이스 및 감독(User interface and supervision)**. 자연어(natural language)로 지시를 받고 사용자를 대신하여 자율적으로 행동할 수 있는 AI 시스템은 더 에이전트적(agentic)입니다. 특히 사용자 감독(user supervision)이 덜 필요한 시스템은 더 에이전트적(agentic)입니다. 예를 들어, 챗봇(chatbots)은 실제 세계에서 행동을 취할 수 없지만, 챗봇에 플러그인(plugins)을 추가하면(예: ChatGPT용 Zapier) 사용자를 대신하여 일부 작업을 수행할 수 있습니다.
*   **시스템 설계(System design)**. 도구(예: 웹 검색 또는 코드 터미널)를 사용하거나 계획(planning)(예: 이전 출력(outputs)을 반영하거나 목표를 하위 목표(subgoals)로 분해하는 것)을 사용하는 시스템은 더 에이전트적(agentic)입니다. 정적 프로그램(static program)에 의해 LLM이 호출되는 방식이 아니라, LLM에 의해 제어 흐름(control flow)이 구동되는 시스템은 더 에이전트적(agentic)입니다.

**에이전트(agents)는 실제로 작동할까요?**

ChatGPT의 코드 인터프리터(code interpreter)/데이터 분석 모드(data analysis mode)와 같은 일부 에이전트(agents)는 유용성을 입증했지만, 지금까지 출시된 더 야심 찬 에이전트 기반 제품들은 종종 기대에 미치지 못했습니다. AI 에이전트(AI agents)를 기반으로 한 두 가지 주요 제품 출시는 Rabbit R1과 Humane AI pin이었습니다. 이 장치들은 전화 의존도를 없애거나 줄이겠다고 약속했지만, 너무 느리고 신뢰할 수 없음이 드러나 사용자들의 실망을 안겼습니다. "AI 소프트웨어 엔지니어(AI software engineer)"인 Devin은 4개월 전 큰 기대를 모으며 발표되었지만, 비디오 리뷰(video review)에서 혹평을 받았고 여전히 대기 목록(waitlist) 모드로 남아있습니다. 이러한 사례들은 에이전트 기술이 아직 갈 길이 멀다는 것을 명확히 보여줍니다. 초기 실패에도 불구하고, 이러한 시도들은 에이전트 개발자들에게 중요한 교훈을 제공하며 기술 발전의 촉매제가 되고 있습니다. AI 에이전트(AI agents)가 실제 제품에서 유용하려면 갈 길이 멀다는 것이 분명합니다.

**그렇다면 AI 에이전트(AI agents)는 모두 과장(hype)일까요?**

아직 말하기에는 너무 이릅니다. 우리는 위와 같은 에이전트(agents)가 널리 채택될 만큼 충분히 잘 작동하기를 기대하기 전에 해결해야 할 연구 과제(research challenges)가 있다고 생각합니다. 이를 알아낼 유일한 방법은 더 많은 연구를 통해서이므로, 우리는 AI 에이전트(AI agents)에 대한 연구가 가치 있다고 생각합니다. 한 가지 주요 연구 과제는 신뢰성(reliability)입니다. LLM은 사람들이 비서(assistant)가 처리하기를 원하는 많은 작업을 수행할 만큼 이미 충분히 능력이 있지만, 성공적인 제품이 될 만큼 충분히 신뢰할 수 있지는 않습니다. 예를 들어, 항공권 예약 에이전트가 수많은 LLM 호출을 필요로 한다고 가정해 봅시다. 각 호출에서 2%의 오류 확률이 독립적으로 발생한다면, 전체 시스템의 신뢰성은 급격히 떨어져 실제 사용이 어려워질 수 있습니다. 이는 초기 에이전트 제품의 실패 원인 중 하나로 작용했습니다. 따라서 신뢰성(reliability) 향상에 대한 연구는 기본 언어 모델(underlying language models)이 개선되지 않더라도 많은 새로운 응용 분야를 가질 수 있습니다. 그리고 스케일링(scaling)이 한계에 도달한다면, 에이전트(agents)는 AI의 추가 발전을 위한 가장 자연스러운 방향입니다. 그러나 현재는 공통 작업 방법(common task method)이 자리 잡기 전 기계 학습(machine learning) 연구의 초기와 마찬가지로, 평가 관행(evaluation practices)이 충분히 엄격하지 않기 때문에 연구 자체가 과장(hype)과 과도한 낙관론(overoptimism)에 기여하고 있습니다. 이것이 저희 논문의 주제입니다. 최근에는 에이전트의 자율성(autonomy)과 안정성(stability)을 높이기 위한 자기 수정(self-correction), 다중 에이전트 협업(multi-agent collaboration) 등 다양한 연구가 활발히 진행되고 있습니다.

**논문의 기여(Contributions of the paper)**

AI 커뮤니티(AI community)는 실제 세계에서 유용하고 벤치마크(benchmarks)에서만 유용한 것이 아닌 AI 에이전트(AI agents)의 개발을 촉진하기 위해 어떤 변화를 구현해야 할까요? 이것이 논문의 핵심 질문입니다. 저희는 다섯 가지 권고 사항을 제시합니다.

1.  **비용 통제 평가(cost-controlled evaluations)를 구현합니다.** 대부분의 AI 에이전트(AI agents)의 기반이 되는 언어 모델(language models)은 확률적(stochastic)입니다. 이는 단순히 기본 모델(underlying model)을 여러 번 호출하는 것만으로도 정확도(accuracy)를 높일 수 있음을 의미합니다. 우리는 이러한 간단한 트릭(tricks)이 HumanEval 벤치마크(benchmark)에서 복잡한 에이전트 아키텍처(agent architectures)보다 훨씬 적은 비용으로 더 나은 성능을 보일 수 있음을 보여줍니다. 우리는 모든 에이전트 평가(agent evaluation)가 비용을 통제해야 한다고 주장합니다. (우리는 이 발견을 여기에서 처음 발표했습니다. 이 게시물을 발표한 지 두 달 만에 파레토 곡선(Pareto curves)과 비용 및 정확도(cost and accuracy)의 공동 최적화(joint optimization)가 에이전트 평가(agent evaluations)에서 점점 더 보편화되었습니다.)
2.  **정확도(accuracy)와 비용(cost)을 공동으로 최적화합니다.** 평가 결과(evaluation results)를 정확도(accuracy)와 추론 비용(inference cost)의 파레토 곡선(Pareto curve)으로 시각화하는 것은 에이전트 설계(agent design)의 새로운 공간을 엽니다: 두 메트릭(metrics)을 공동으로 최적화하는 것입니다. 우리는 DSPy 프레임워크(framework)를 수정하여 HotPotQA에서 정확도(accuracy)를 유지하면서 비용을 낮출 수 있는 방법을 보여줍니다. 이러한 접근 방식은 효율적인 에이전트 개발의 핵심 요소로 자리 잡고 있습니다.
3.  **모델 벤치마킹(model benchmarking)과 다운스트림 벤치마킹(downstream benchmarking)을 구분합니다.** NovelQA의 사례 연구(case study)를 통해, 모델 평가(model evaluation)를 위한 벤치마크(benchmarks)가 다운스트림 평가(downstream evaluation)에 사용될 때 어떻게 오해를 불러일으킬 수 있는지 보여줍니다. 우리는 다운스트림 평가(downstream evaluation)가 모델 매개변수(model parameters) 수와 같은 비용 대리 지표(proxies for cost) 대신 실제 달러 비용(dollar costs)을 고려해야 한다고 주장합니다.
4.  **에이전트 벤치마크(agent benchmarks)에서 지름길(shortcuts)을 방지합니다.** 우리는 에이전트 벤치마크(agent benchmarks)에 대한 다양한 유형의 과적합(overfitting)이 가능함을 보여줍니다. 우리는 에이전트(agents)의 일반성(generality)을 4단계로 식별하고, 원하는 일반성 수준에 따라 다른 유형의 홀드아웃 샘플(hold-out samples)이 필요하다고 주장합니다. 적절한 홀드아웃(hold-outs)이 없으면 에이전트 개발자(agent developers)는 의도치 않게 지름길(shortcuts)을 택할 수 있습니다. 우리는 WebArena 벤치마크(benchmark)의 사례 연구(case study)를 통해 이를 설명합니다. 이러한 지름길 방지 노력은 에이전트의 실제 적용 가능성을 높이는 데 필수적입니다.
5.  **에이전트 벤치마크(agent benchmarks)의 표준화(standardization) 및 재현성(reproducibility)을 개선합니다.** 우리는 WebArena 및 HumanEval 평가(evaluations)의 재현성(reproducibility)에서 만연한 단점들을 발견했습니다. 이러한 오류는 정확도 추정치(accuracy estimates)를 부풀리고 에이전트(agent) 능력에 대한 과도한 낙관론(overoptimism)으로 이어집니다. 투명하고 재현 가능한 평가 환경 구축은 신뢰할 수 있는 에이전트 연구의 근간입니다.

**결론: 조심스러운 낙관론의 이유(reasons for cautious optimism)**

AI 에이전트 벤치마킹(AI agent benchmarking)은 새롭고 아직 모범 사례(best practices)가 확립되지 않아 진정한 발전과 과장(hype)을 구별하기 어렵습니다. 우리는 에이전트(agents)가 모델(models)과 충분히 다르기 때문에 벤치마킹 관행(benchmarking practices)을 재고해야 한다고 생각합니다. 저희 논문에서는 에이전트 벤치마킹(agent benchmarking)에 대한 원칙적인 접근 방식(principled approach)을 향한 첫걸음을 내딛습니다. 우리는 이러한 단계들이 AI 에이전트 평가(AI agent evaluation)의 엄격함(rigor)을 높이고 발전을 위한 확고한 기반을 제공하기를 바랍니다.

저희 연구의 또 다른 갈래는 의학이나 사회 과학과 같은 과학 분야에서 ML 기반 연구(ML-based research)의 재현성 위기(reproducibility crisis)에 관한 것입니다. 어떤 면에서는 저희의 현재 논문도 유사합니다. ML 기반 과학(ML-based science)에서는 상황이 개선되기 전에 오히려 악화될 수 있다는 것이 우리의 예측입니다. 그러나 AI 에이전트 연구(AI agents research)에서는 평가 관행이 더욱 빠르게 발전할 것이라는 조심스러운 낙관론을 가지고 있습니다. 한 가지 이유는 발표된 논문과 함께 코드(code)와 데이터(data)를 공유하는 문화가 더욱 강력해져서 오류를 더 쉽게 발견할 수 있기 때문입니다. (이러한 문화적 변화는 지난 5년간의 공동 노력 덕분에 이루어졌으며, 최근에는 오픈 소스(open source) 생태계의 성장이 이를 더욱 가속화하고 있습니다.) 또 다른 이유는 오해의 소지가 있는 평가(misleading evaluations)를 기반으로 한 제품이 결국 실패할 때, 과도하게 낙관적인 연구(overoptimistic research)가 빠르게 현실 점검(reality check)을 받기 때문입니다. 이러한 현실적인 피드백 루프는 연구의 방향을 더욱 실용적으로 이끌어 나갈 것입니다. 이는 연구와 제품 출시(product releases) 모두에서 향후 몇 년 동안 지켜볼 흥미로운 분야가 될 것이며, 에이전트 분야가 점차 성숙해지고 있음을 시사합니다.

1 전통적인 AI에서 에이전트(agents)는 환경을 인지하고 그에 따라 행동하는 개체로 정의되지만, LLM 시대에는 그 정의가 덜 유용합니다. 그 정의에 따르면 온도 조절 장치(thermostat)조차도 에이전트(agent)로 분류될 수 있습니다.