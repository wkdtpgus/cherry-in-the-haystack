대규모 언어 모델(large language models)의 가장 매력적인 적용 사례 중 일부는 항공권 예매나 소프트웨어 내 버그(software bugs) 탐지 및 수정과 같이 현실 세계에서 구체적인 행동을 수행하는 것을 포함합니다. 이러한 유형의 작업을 처리하는 AI 시스템을 에이전트(agents)라고 부릅니다. 이들은 웹 검색(web search)이나 코드 터미널(code terminals) 등 다양한 도구를 활용하기 위해 LLM을 다른 소프트웨어 구성 요소와 결합하여 작동합니다. 궁극적인 목표는 Siri나 Alexa와 같은 지능형 비서(assistants)를 개발하여, 복잡한 업무를 효과적으로 처리하고, 사용자의 요청을 정확하게 해석하며, 믿을 수 있는 방식으로 임무를 수행하도록 만드는 것입니다. 그러나 이러한 이상적인 비전은 아직 현실과 거리가 멀며, 관련 연구 분야 자체도 비교적 최근에 부상했습니다. 에이전트(agents)의 개발을 촉진하고 그 성능을 객관적으로 측정하기 위해 연구자들은 벤치마크 데이터셋(benchmark datasets)을 구축해왔습니다. 하지만 앞서 언급했듯이, LLM의 평가(evaluation)는 복잡한 문제들을 내포하고 있으며, 에이전트 평가(agent evaluation)는 현재의 벤치마크(benchmarks)와 평가 방식에 영향을 미치는 수많은 추가적인 함정(pitfalls)을 지니고 있음이 밝혀졌습니다. 이러한 환경은 실제로는 유용성이 떨어지지만 벤치마크(benchmarks)에서만 높은 점수를 받는 에이전트(agents)의 개발을 부추길 수 있습니다. 이에 저희 연구팀은 에이전트(agents) 평가의 주요 과제들을 규명하고 이를 해결하기 위한 방안을 제시하는 새로운 논문을 발표했습니다. 해당 논문은 여기서 확인하실 수 있습니다. 저자로는 프린스턴 대학교(Princeton University)의 Sayash Kapoor, Benedikt Ströbl, Zachary S. Siegel, Nitya Nadgir, Arvind Narayanan이 참여했습니다. 본 게시물에서는 AI 에이전트(AI agents)의 개념, AI 에이전트 연구의 미래에 대한 저희의 조심스러운 낙관론, AI 에이전트가 단지 과장(hype)인지 아니면 실체(substance)를 지니는지에 대한 고찰, 그리고 논문의 주요 내용을 간략하게 소개합니다.

**'에이전트(agent)'라는 용어는 무엇을 의미할까요? 단순히 유행어(buzzword)일까요?**

‘에이전트(agent)’라는 용어는 AI 연구자들 사이에서 명확한 공식 정의 없이 사용되어 왔습니다. 1 이로 인해 종종 마케팅 용어로 오용되기도 했고, 그 사용에 대한 일각의 비판을 야기했습니다. 하지만 이 용어가 전적으로 무의미한 것은 아닙니다. 많은 연구자들이 언어 모델 기반 시스템(language-model-based systems)의 맥락에서 에이전트(agent)를 구성하는 핵심 요소에 대한 학계의 직관적인 이해를 공식화하려는 노력을 기울여 왔습니다 [1, 2, 3, 4, 5]. 이는 단순히 이분법적인 개념이라기보다는 '에이전트적(agentic)'이라는 표현으로 나타낼 수 있는 스펙트럼(spectrum)으로 이해될 수 있습니다. 위에 언급된 AI 에이전트(AI agents)에 대한 다섯 가지 최신 정의들은 서로 다르면서도 강한 공통점을 가지고 있습니다. 저희는 새로운 정의를 제시하기보다는, 기존 정의들을 바탕으로 AI 시스템이 더욱 에이전트적(agentic)으로 간주될 수 있도록 하는 세 가지 주요 속성군(clusters of properties)을 식별했습니다.

*   **환경과 목표(Environment and goals)**. 환경이 복잡할수록 그러한 환경에서 기능하는 AI 시스템은 더 에이전트적(agentic)입니다. 복잡한 환경은 다양한 과업과 도메인(domains), 다수의 이해관계자(stakeholders), 행동을 위한 긴 시간적 제약(time horizon), 그리고 예측 불가능한 변화를 수반합니다. 또한, 목표를 달성하는 구체적인 방법에 대한 지시 없이 복잡한 목표를 추구하는 시스템은 더 에이전트적(agentic)입니다.
*   **사용자 인터페이스 및 감독(User interface and supervision)**. 자연어(natural language)로 지시를 수용하고 사용자를 대신하여 자율적으로 행동할 수 있는 AI 시스템은 더 에이전트적(agentic)입니다. 특히 사용자 개입(user supervision)이 적게 요구되는 시스템일수록 더욱 에이전트적(agentic)입니다. 예를 들어, 챗봇(chatbots)은 물리적 세계에서 행동을 취할 수 없지만, 챗봇에 플러그인(plugins)을 추가하면(예: ChatGPT용 Zapier) 사용자를 대신하여 특정 작업을 수행할 수 있습니다.
*   **시스템 설계(System design)**. 도구(예: 웹 검색 또는 코드 터미널)를 활용하거나 계획(planning)(예: 이전 결과(outputs)를 성찰하거나 목표를 하위 목표(subgoals)로 분할하는 것) 기능을 사용하는 시스템은 더 에이전트적(agentic)입니다. LLM이 정적인 프로그램(static program)에 의해 호출되는 방식이 아니라, LLM 자체에 의해 제어 흐름(control flow)이 주도되는 시스템은 더 에이전트적(agentic)입니다.

이 세 가지 속성은 에이전트(agents)가 단순한 도구 집합을 넘어, 환경과 상호작용하며 자율적으로 목표를 추구하는 지능형 실체로 진화하는 과정을 설명합니다. 이들은 인간의 개입을 최소화하면서 복잡한 문제를 해결하고, 예상치 못한 상황에 유연하게 대처할 수 있는 능력을 지향합니다.

**에이전트(agents)는 실제로 작동할까요?**

ChatGPT의 코드 인터프리터(code interpreter)/데이터 분석 모드(data analysis mode)와 같은 일부 에이전트(agents)는 분명 유용성을 입증했지만, 현재까지는 더 야심찬 에이전트 기반 상업 제품들이 기대에 미치지 못했습니다. 대표적인 사례로 Rabbit R1과 Humane AI pin이 있었습니다. 이 장치들은 스마트폰 의존도를 줄이거나 없애겠다고 선언했지만, 실제로는 너무 느리고 신뢰성이 부족하다는 평가를 받았습니다. "AI 소프트웨어 엔지니어(AI software engineer)"를 표방한 Devin 역시 4개월 전 큰 기대 속에 발표되었으나, 비디오 리뷰(video review)에서 혹평을 받았고 여전히 대기 목록(waitlist) 모드에 머물러 있습니다. 이러한 사례들은 AI 에이전트(AI agents)가 실제 제품으로서 유용하게 기능하기 위해서는 아직 많은 발전이 필요하다는 점을 명확히 보여줍니다.

이러한 상업적 실패는 에이전트 기술의 근본적인 한계라기보다는, 지나치게 광범위하고 일반적인 AI를 지향하는 초기 시도의 어려움을 반영합니다. 범용 인공지능(General AI)을 목표로 하는 에이전트는 복잡한 현실 세계의 모든 변수를 처리해야 하므로 개발이 매우 까다롭습니다. 반면, 특정 도메인에 특화된 에이전트들은 이미 상당한 진전을 보이고 있습니다. 예를 들어, 특정 산업의 데이터 분석 파이프라인을 자동화하거나, 기업 내부 시스템과 연동하여 고객 문의를 처리하는 정교한 챗봇, 또는 개발 환경(IDE) 내에서 코드 리팩토링이나 디버깅을 돕는 지능형 도구들은 제한된 범위 내에서 높은 효율성을 발휘하며 실질적인 가치를 제공하고 있습니다. 이는 에이전트 기술이 특정 문제 해결에 집중할 때 성공 가능성이 훨씬 높음을 시사합니다.

**그렇다면 AI 에이전트(AI agents)는 모두 과장(hype)일까요?**

아직 단정하기는 이릅니다. 저희는 위에서 언급된 에이전트(agents)들이 대중적으로 널리 사용될 만큼 충분히 잘 작동하기를 기대하기 전에 해결해야 할 중요한 연구 과제(research challenges)들이 있다고 생각합니다. 이러한 과제들을 규명하고 해결하는 유일한 방법은 지속적인 연구를 통해서이므로, AI 에이전트(AI agents)에 대한 연구 노력은 여전히 가치 있다고 봅니다. 한 가지 핵심적인 연구 과제는 신뢰성(reliability)입니다. LLM은 사람들이 비서(assistant)가 처리하기를 원하는 많은 작업을 수행할 수 있을 만큼 충분한 능력을 갖추고 있지만, 상업적으로 성공적인 제품이 될 만큼 충분히 신뢰할 수 있지는 않습니다. 그 이유를 이해하기 위해, 수십 번의 LLM 호출(calls)을 요구하는 항공권 예약 에이전트(flight-booking agent)를 예로 들어 봅시다. 만약 각 호출이 독립적으로 2%의 확률로 오류를 발생시킨다면, 전체 시스템은 너무나도 신뢰할 수 없어서 완전히 무용지물이 될 것입니다 (이는 우리가 목격한 일부 제품 실패를 부분적으로 설명합니다). 따라서 신뢰성(reliability) 향상에 대한 연구는 기반 언어 모델(underlying language models)이 개선되지 않더라도 수많은 새로운 응용 분야를 창출할 수 있습니다. 그리고 스케일링(scaling)이 한계에 도달한다면, 에이전트(agents)는 AI의 추가 발전을 위한 가장 자연스러운 방향이 될 것입니다.

그러나 현재 상황은 공통적인 작업 방법론(common task method)이 정립되기 전의 초기 기계 학습(machine learning) 연구 시기와 유사하게, 평가 관행(evaluation practices)이 충분히 엄격하지 않아 연구 자체도 과장(hype)과 과도한 낙관론(overoptimism)에 기여하고 있습니다. 이는 단순한 기술적 문제를 넘어, 윤리적 고려 사항, 안전성, 투명성 및 설명 가능성(explainability)과 같은 복합적인 문제들을 야기합니다. 에이전트가 자율적으로 행동할 때, 잠재적인 편향을 어떻게 완화하고, 의도치 않은 해로운 결과를 어떻게 방지할 것인가? 또한, 사용자는 에이전트가 특정 결정을 내린 이유를 명확히 이해할 필요가 있으며, 이는 많은 LLM의 '블랙박스(black box)' 특성으로 인해 어려운 과제입니다. 강력한 인간-에이전트 상호작용 프레임워크의 개발도 필수적입니다. 에이전트는 단순히 사용자를 위해 행동하는 것을 넘어, 사용자와 함께 협력하며 원활한 감독, 개입 및 학습을 가능하게 해야 합니다. 이러한 복잡한 사회적, 기술적 통합 문제들을 간과하고 단순한 성능 지표에만 초점을 맞추는 현재의 과장된 추세는 장기적인 발전에 걸림돌이 될 수 있습니다. 이것이 바로 저희 논문의 핵심 주제입니다.

**논문의 기여(Contributions of the paper)**

AI 커뮤니티(AI community)는 실제 세계에서 유용하고 벤치마크(benchmarks)에서만 유용한 것이 아닌 AI 에이전트(AI agents)의 개발을 촉진하기 위해 어떤 변화를 구현해야 할까요? 이것이 논문의 핵심 질문입니다. 저희는 다섯 가지 권고 사항을 제시합니다.

1.  **비용 통제 평가(cost-controlled evaluations)를 구현합니다.** 대부분의 AI 에이전트(AI agents)의 기반이 되는 언어 모델(language models)은 확률적(stochastic)입니다. 이는 단순히 기반 모델(underlying model)을 여러 번 호출하는 것만으로도 정확도(accuracy)를 높일 수 있음을 의미합니다. 우리는 이러한 간단한 방법(tricks)이 HumanEval 벤치마크(benchmark)에서 복잡한 에이전트 아키텍처(agent architectures)보다 훨씬 적은 비용으로 더 나은 성능을 보일 수 있음을 보여줍니다. 저희는 모든 에이전트 평가(agent evaluation)가 비용을 통제해야 한다고 주장합니다. 예를 들어, 특정 에이전트가 100번의 LLM 호출로 90% 정확도를 달성하고 다른 에이전트가 10번의 호출로 85% 정확도를 달성했다면, 단순히 정확도만으로 우열을 가릴 수 없으며, 효율성을 함께 고려해야 합니다. (우리는 이 발견을 여기에서 처음 발표했습니다. 이 게시물을 발표한 지 두 달 만에 파레토 곡선(Pareto curves)과 비용 및 정확도(cost and accuracy)의 공동 최적화(joint optimization)가 에이전트 평가(agent evaluations)에서 점점 더 보편화되었습니다.)
2.  **정확도(accuracy)와 비용(cost)을 공동으로 최적화합니다.** 평가 결과(evaluation results)를 정확도(accuracy)와 추론 비용(inference cost)의 파레토 곡선(Pareto curve)으로 시각화하는 것은 에이전트 설계(agent design)의 새로운 가능성을 열어줍니다: 두 가지 지표(metrics)를 동시에 최적화하는 것입니다. 이는 제한된 예산으로 최대의 효과를 내야 하는 실제 서비스 환경에서 매우 중요합니다. 우리는 DSPy 프레임워크(framework)를 수정하여 HotPotQA에서 정확도(accuracy)를 유지하면서 비용을 낮출 수 있는 방법을 보여줍니다.
3.  **모델 벤치마킹(model benchmarking)과 다운스트림 벤치마킹(downstream benchmarking)을 구분합니다.** NovelQA의 사례 연구(case study)를 통해, 모델 평가(model evaluation)를 위한 벤치마크(benchmarks)가 실제 적용 단계인 다운스트림 평가(downstream evaluation)에 사용될 때 어떻게 오해를 불러일으킬 수 있는지 보여줍니다. 예를 들어, 특정 모델이 일반적인 언어 이해 벤치마크에서 높은 점수를 받았더라도, 의료 진단과 같은 특정 다운스트림 작업에서는 실제 환자 데이터와 임상적 맥락을 고려하지 못해 성능이 저조할 수 있습니다. 우리는 다운스트림 평가(downstream evaluation)가 모델 매개변수(model parameters) 수와 같은 비용 대리 지표(proxies for cost) 대신 실제 달러 비용(dollar costs)을 고려해야 한다고 주장합니다.
4.  **에이전트 벤치마크(agent benchmarks)에서 지름길(shortcuts)을 방지합니다.** 우리는 에이전트 벤치마크(agent benchmarks)에 대한 다양한 유형의 과적합(overfitting)이 가능함을 보여줍니다. 벤치마크 데이터셋의 특정 패턴에만 과도하게 최적화되어 실제 환경에서는 제대로 작동하지 않는 경우가 발생할 수 있습니다. 우리는 에이전트(agents)의 일반성(generality)을 4단계로 식별하고, 원하는 일반성 수준에 따라 다른 유형의 홀드아웃 샘플(hold-out samples)이 필요하다고 주장합니다. 적절한 홀드아웃(hold-outs)이 없으면 에이전트 개발자(agent developers)는 의도치 않게 지름길(shortcuts)을 택할 수 있습니다. 우리는 WebArena 벤치마크(benchmark)의 사례 연구(case study)를 통해 이를 설명합니다.
5.  **에이전트 벤치마크(agent benchmarks)의 표준화(standardization) 및 재현성(reproducibility)을 개선합니다.** 우리는 WebArena 및 HumanEval 평가(evaluations)의 재현성(reproducibility)에서 만연한 단점들을 발견했습니다. 이러한 오류는 정확도 추정치(accuracy estimates)를 부풀리고 에이전트(agent) 능력에 대한 과도한 낙관론(overoptimism)으로 이어집니다. 이는 AI 연구의 신뢰성을 저해하고, 학계와 산업계 모두에서 잘못된 의사결정을 초래할 수 있습니다. 따라서 엄격한 프로토콜과 명확한 문서화를 통해 평가 과정을 표준화하고 재현성을 확보하는 것이 필수적입니다.

**결론: 조심스러운 낙관론의 이유(reasons for cautious optimism)**

AI 에이전트 벤치마킹(AI agent benchmarking)은 비교적 새로운 분야이며, 아직 최적의 관행(best practices)이 확립되지 않아 진정한 발전과 단순히 과장(hype)된 기대를 구별하기가 어렵습니다. 저희는 에이전트(agents)가 기존 모델(models)과는 충분히 다른 특성을 지니므로, 벤치마킹 방법론(benchmarking practices)을 재고해야 한다고 믿습니다. 저희 논문은 에이전트 벤치마킹(agent benchmarking)에 대한 원칙적인 접근 방식(principled approach)을 향한 중요한 첫걸음을 제시합니다. 저희는 이러한 노력들이 AI 에이전트 평가(AI agent evaluation)의 엄격함(rigor)을 높이고, 향후 발전을 위한 견고한 토대를 제공하기를 기대합니다.

저희 연구의 또 다른 갈래는 의학이나 사회 과학과 같은 과학 분야에서 기계 학습 기반 연구(ML-based research)가 겪는 재현성 위기(reproducibility crisis)에 관한 것입니다. 어떤 면에서는 저희의 현재 논문도 이와 유사한 맥락에 있습니다. ML 기반 과학(ML-based science)에서는 상황이 나아지기 전에 더 나빠질 것이라는 것이 저희의 전망입니다. 그러나 AI 에이전트 연구(AI agents research)에서는 이러한 평가 관행이 빠르게 개선될 것이라고 조심스럽게 낙관합니다. 한 가지 이유는 연구 논문 발표 시 코드(code)와 데이터(data)를 함께 공유하는 문화가 더욱 강력해져서 오류를 더 쉽게 발견하고 수정할 수 있기 때문입니다. (이러한 문화적 변화는 지난 5년간의 공동 노력 덕분에 이루어졌습니다.) 또 다른 이유는 오해의 소지가 있는 평가(misleading evaluations)를 기반으로 한 제품들이 결국 시장에서 실패할 때, 과도하게 낙관적인 연구(overoptimistic research)가 빠르게 현실 점검(reality check)을 받기 때문입니다. 이러한 현실적 피드백은 연구 방향을 올바르게 이끄는 중요한 역할을 합니다. 이 분야는 연구와 제품 출시(product releases) 모두에서 향후 몇 년 동안 매우 흥미로운 발전이 기대되는 영역이 될 것입니다. 궁극적으로 이러한 평가의 엄격함과 투명성이 확보될 때, AI 에이전트는 진정으로 사회에 긍정적인 영향을 미치는 강력한 도구로 자리매김할 수 있을 것입니다.

1 전통적인 AI에서 에이전트(agents)는 환경을 인지하고 그에 따라 행동하는 개체로 정의되지만, LLM 시대에는 그 정의가 덜 유용합니다. 그 정의에 따르면 온도 조절 장치(thermostat)조차도 에이전트(agent)로 분류될 수 있습니다.