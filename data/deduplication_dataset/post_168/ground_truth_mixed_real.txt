지금 무료로 등록하세요: https://bit.ly/4aRnn7Z 깃허브 저장소(Github Repo): https://github.com/HandsOnLLM/Hands-On-Large-Language-Models

저희는 "트랜스포머 LLM 작동 방식(How Transformer LLMs Work)" 강좌의 2025년 업데이트 버전을 제공하게 되어 매우 기쁩니다. 이 무료 강좌는 약 90분 분량의 비디오, 실습 코드, 그리고 현대 트랜스포머 아키텍처, 토크나이저, 임베딩, 전문가 혼합 모델(MoE)을 설명하는 선명한 시각 자료 및 애니메이션으로 구성됩니다.

Language Models & Co.를 읽어주셔서 감사합니다! 새로운 게시물을 무료로 받아보고 제 작업을 지원하려면 구독하세요. 구독하기

마르텐 그루텐도르스트(Maarten Grootendorst)와 저는 수년에 걸쳐 이 강좌를 위한 풍부한 시각적 언어와 자료들을 개발해왔습니다. 저희의 작업은 Cohere, C4AI, 그리고 오픈 소스 및 오픈 사이언스 ML 커뮤니티의 뛰어난 동료들로부터 영감을 받았습니다. 특히 전설적인 앤드류 응(Andrew Ng)과 DeepLearning.ai 팀과의 협력은 애니메이션과 간결한 설명으로 강좌를 한 단계 더 발전시키는 계기가 되었습니다. 이는 기술 학습자들이 머신러닝(ML) 논문을 접하고 아키텍처 설명을 이해할 수 있도록 돕기 위함입니다.

2025년 현재, LLM 기술은 전례 없는 속도로 발전하고 있습니다. 이러한 변화 속에서도 변치 않는 핵심 트랜스포머 원리를 이해하는 것은 필수적입니다.

이 강좌에서 여러분은 LLM을 구동하는 트랜스포머 네트워크 아키텍처(transformer network architecture)가 어떻게 작동하는지 배우게 될 것입니다. LLM이 텍스트를 처리하는 방식에 대한 깊은 직관을 기르고, 트랜스포머 아키텍처의 핵심 구성 요소를 설명하는 최신 코드 예제와 함께 직접 실습하게 될 것입니다.

이 강좌에서 다루는 주요 주제는 다음과 같습니다.
*   Bag-of-Words 모델부터 Word2Vec 임베딩을 거쳐 단어의 문맥적 의미를 정교하게 포착하는 트랜스포머 아키텍처에 이르기까지, 언어 표현 방식의 진화 과정.
*   LLM 입력이 언어 모델(language model)로 전송되기 전에 단어나 조각을 나타내는 토큰(token)으로 분해되는 방식.
*   토큰화 및 임베딩, 여러 트랜스포머 블록의 스택, 그리고 언어 모델 헤드로 구성된 트랜스포머의 세 가지 주요 단계에 대한 심층적인 분석.
*   관련성 점수를 계산하는 어텐션(attention)을 포함하며, 그 뒤에는 훈련 중에 학습된 저장된 정보를 통합하는 피드포워드 레이어(feedforward layer)가 이어지는 트랜스포머 블록의 세부 사항.
*   트랜스포머의 효율성을 높이는 캐시된 계산 기법과, 원본 논문 이후 트랜스포머 블록이 어떻게 발전하여 오늘날에도 널리 활용되는지.
*   Hugging Face 트랜스포머 라이브러리(Hugging Face transformer library)를 활용하여 최신 LLM 모델들의 실제 구현 방식을 심층적으로 탐색합니다.
*   또한, 최근 주목받는 LLM의 효율적인 배포 및 운영을 위한 양자화(quantization) 및 추론 최적화 기법에 대한 개요도 포함합니다.

이 강좌를 마치면 LLM이 언어를 처리하는 방식에 대해 깊이 이해하게 될 것이며, 모델을 설명하는 논문을 읽고 이러한 아키텍처를 설명하는 데 사용되는 세부 사항을 이해할 수 있을 것입니다. 이러한 직관은 LLM 애플리케이션(application) 구축 접근 방식을 혁신하고 실제 문제 해결에 기여하는 데 도움이 될 것입니다. 급변하는 LLM 생태계에서 탄탄한 기초 지식은 새로운 기술을 빠르게 습득하고 적용하는 데 필수적입니다. 본 강좌는 여러분의 LLM 여정에 든든한 나침반이 될 것입니다.

즐겁게 수강하시길 바랍니다! Language Models & Co.를 읽어주셔서 감사합니다! 새로운 게시물을 무료로 받아보고 제 작업을 지원하려면 구독하세요. 구독하기