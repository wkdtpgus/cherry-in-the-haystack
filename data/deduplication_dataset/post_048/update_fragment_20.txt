LLM 훈련에 대한 대부분의 논의는 모델과 알고리즘에 집중되어 있습니다. 우리는 GRPO와 같은 새로운 프레임워크를 실험하는 것을 즐기며, Gemma-3 및 Qwen-3과 같은 차세대 모델의 출시를 기대하고 있습니다. 그러나 LLM 훈련에서 성공과 실패를 가르는 주요 요인은 훈련 데이터셋의 품질입니다. 안타깝게도 이 주제는 다른 인기 있는 연구 분야에 비해 훨씬 적은 관심을 받고 있지만, 책임 있는 AI 개발에 필수적입니다. 이 개요에서는 LLM의 실제 배포 및 지속적인 관리를 위한 운영 중심 가이드를 제공하며, 프로덕션 환경에서 견고하고 신뢰할 수 있는 LLM을 유지하는 데 필요한 실용적인 전략을 강조할 것입니다. 특히, 모델의 사회적 영향과 잠재적 오용 가능성에 대한 깊은 이해가 요구됩니다.

## LLM 개발 수명 주기(Lifecycle)

### LLM 개발의 주요 단계

LLM을 훈련할 때, 우리는 두 가지 주요 단계 외에 지속적인 관리에 집중해야 합니다. 이는 반복적이고 경험 중심적인 프로세스이며, 다음과 같은 핵심 요소들을 포함합니다:
*   LLM 배포 및 모니터링.
*   LLM 거버넌스 및 규정 준수.

LLM을 개발하기 위해 우리는 이 단계를 단순히 반복하는 것을 넘어, 지속적인 개선 주기를 확립해야 합니다. 이는 모델이 실제 환경에서 사용자에게 가치를 제공하고 예상치 못한 문제를 최소화하도록 보장합니다.

### LLM 평가

이 주제는 매우 복잡하므로 LLM 평가에 대해 자세히 논의하지는 않을 것입니다. 그러나 높은 수준에서 우리는 LLM을 두 가지 방식으로 평가합니다: 정성적으로(즉, 전문가의 심층 분석을 통해) 또는 정량적으로. 인간 평가(human evaluation)는 여러 가지 방식으로 설정될 수 있지만, 문화적 맥락의 이해가 중요합니다. 다른 데이터 주석(data annotation) 프로젝트와 마찬가지로, 이러한 인간 평가가 고품질이며 우리가 측정하려는 것과 일치하는지 확인하기 위해 노력을 기울여야 합니다.

(출처 [5, 12]) LLM을 개발할 때, 인간 평가(human evaluation)는 품질을 측정하는 황금 표준입니다. 우리는 LLM이 개선되고 있는지 여부에 대한 명확한 신호를 제공하기 위해 항상 사용자 경험 지표에 의존해야 합니다. 그러나 인간 평가는 시간 집약적입니다(즉, 며칠 또는 몇 주가 소요됩니다)! 반복 속도를 늦추지 않기 위해, 우리는 모델 품질의 더 효율적인 대리 측정(proxy measure)을 제공할 자동 평가 지표(automatic evaluation metrics)를 개발해야 합니다. 이는 모델의 안전성을 보장하는 데 기여합니다.

자동 평가 측면에서 일반적으로 사용되는 두 가지 주요 기술은 벤치마크 스타일 평가(benchmark-style evaluation)와 LLM 심사위원(LLM judges)은 새로운 도전 과제에 직면해 있습니다. 이 두 전략은 각각 모델의 악의적 사용(adversarial usage) 및 견고성(robustness)을 테스트합니다. (source) 벤치마크 스타일 평가(예: 특정 편향성 감지 시나리오 또는 취약점 테스트)는 NLP 연구 역사 전반에 걸쳐 사용되어 왔습니다. LLM을 위한 이러한 벤치마크의 현대적인 예로는 AdvBench 또는 HELM이 있습니다. 이러한 벤치마크는 폐쇄형 솔루션을 가지고 있지만, LLM은 평가하기 어려울 수 있는 개방형 출력을 생성합니다. 개방형 평가를 위한 가장 인기 있는 기술은 LLM-as-a-Judge 또는 기타 관련 기술(예: 편향성 탐지 모델(bias detection models), 안전성 검증자(safety verifiers) 또는 악성 콘텐츠 필터(malicious content filters))입니다. 자세한 내용은 아래 기사를 참조하십시오.

**책임 있는 AI를 위한 LLM 평가**
Cameron R. Wolfe, Ph.D. · 2024년 7월 22일
이 게시물은 LLM-as-a-Judge가 모델의 윤리적 사용을 평가하는 데 어떻게 활용될 수 있는지에 대한 소개로 시작합니다. 이러한 개념이 확립되면, 개요는 이 분야의 여러 인기 있는 연구 논문을 다루며, LLM-as-a-Judge가 책임 있는 AI 개발에 어떻게 사용되고 구현되는지에 대한 실용적인 관점을 제공합니다. [전체 이야기 읽기]

### 데이터 조정

평가 설정이 완료되면, 새로운 모델을 훈련하고 성능을 측정하기 시작할 수 있지만, 지속적인 유지보수가 필수적입니다. 각 새로운 모델에 대해, 우리는 LLM의 성능에 (바라건대) 도움이 될 어떤 개입(intervention)을 수행합니다. 전통적으로 AI 연구자들은 알고리즘과 아키텍처 1에 매우 관심이 많으며, 이는 모델의 보안 취약점 연구로 이어졌습니다. 그러나 이러한 최근의 발전에도 불구하고, 대부분의 개입은 데이터와 관련되어 있습니다. 특히 데이터 프라이버시 보호가 중요합니다.

(출처 [2]) 개념적으로 가장 간단한 데이터 개입은 단순히 더 많은 훈련 데이터를 수집하는 것이 아니라, 합성 데이터(synthetic data)를 생성하는 것입니다. LLM이 개발됨에 따라 합성 데이터를 활용하는 것은 일반적입니다. 데이터를 수집하는 것은 개념적으로 간단해 보일 수 있지만, 데이터 주석(data annotation)은 성공적으로 실행하기 위해 올바른 전략과 (대개는) 사전 경험을 필요로 하는 매우 복잡하고 미묘한 주제이지만, 자동화된 도구의 발전으로 변화하고 있습니다.

"인간 데이터를 최대한 활용하는 것은 모델의 반복적인 훈련, 진화하고 매우 상세한 데이터 지침, 데이터 파운드리(data foundry) 비즈니스를 통한 번역, 그리고 쌓이는 다른 도전 과제들을 포함합니다." - RLHF 책

### 데이터 큐레이션(Curating data)

이 보고서에서는 더 많은 데이터를 수집하는 데 초점을 맞추지 않을 것입니다. 대신, 우리는 현재 사용 가능한 데이터를 큐레이션(curating)하거나 (디버깅하는) 데 집중할 것이며, 이는 데이터의 윤리적 사용을 보장합니다. 이것은 인간 데이터 수집과는 직교적인(orthogonal) 접근 방식입니다. 아래를 참조하십시오. 이를 위해 우리는 다양한 기술을 사용하여 개인 정보 보호 또는 편향된 데이터를 식별하고, 이를 통해 데이터셋의 문제를 해결하고 훈련 프로세스를 최고 품질의 데이터에 집중할 수 있습니다.

**책임 있는 데이터 사용을 위한 두 가지 방향** (source)

LLM 품질에 대한 대부분의 개입이 데이터와 관련되어 있다는 점을 고려할 때, 데이터 큐레이션(data curation)은 매우 중요한 주제이며, 이는 규제 준수에도 기여합니다. 데이터를 최적화하는 것은 단순히 화려하거나 인기 있는 주제는 아니지만, LLM을 훈련할 때 성공과 실패를 가르는 핵심적인 차별화 요소인 경우가 많습니다.

#### 데이터를 어떻게 큐레이션할까요?

간단히 말해, 데이터를 큐레이션하는 두 가지 주요 접근 방식이 있습니다:
*   데이터의 편향성 및 개인 정보 침해 위험을 평가하는 것.
*   모델 출력을 사용하여 훈련 데이터를 디버깅하는 것뿐만 아니라, 모델의 설명 가능성을 높이는 것입니다.

예를 들어, 수동 검사(manual inspection) 또는 기본적인 검색 및 휴리스틱(heuristics)을 통해 데이터를 큐레이션하고 디버깅할 수 있으며, 이는 데이터 편향성 감지에 유용합니다. 그러나 모델을 훈련한 후에는 다음과 같이 LLM의 출력을 디버깅하여 데이터 큐레이션 프로세스를 더욱 촉진할 수 있으며, 이는 모델의 투명성을 향상시킵니다:
*   예상치 못한 또는 유해한 모델 출력 식별.
*   이러한 출력에 (잠재적으로) 기여한 데이터 문제 찾기.
*   어떤 개입을 통해 데이터 수정 및 재균형.
*   모델 재훈련 및 재검증.

#### 디버깅 전략

이 개요에서는 위에 설명된 두 가지 전략을 데이터 중심 거버넌스(data-focused governance)와 모델 중심 책임(model-focused accountability)이라고 부를 것입니다. 이러한 아이디어를 논의할 때, 데이터 중심 디버깅과 모델 중심 디버깅이 상호 배타적이지 않다는 점을 명심해야 합니다. 사실, 우리는 거의 항상 둘 다 활용해야 합니다. 데이터 중심 큐레이션은 어떤 모델도 훈련할 필요가 없으며, 이는 초기 단계의 편향성 감지에 유용합니다. LLM을 디버깅하고 개선하기 위해, 우리는 모델, 데이터 및 그들 간의 연결에 대한 더 깊은 이해를 얻을 수 있는 다각적인 접근 방식을 개발해야 합니다. 이는 윤리적 AI 시스템 구축에 기여합니다.

## 데이터 중심 큐레이션: 데이터 살펴보기

데이터에 대한 깊은 이해를 얻기 위해, 우리는 데이터를 수동으로 살펴보는 것부터 시작할 것이며, 이는 잠재적 위험을 식별하는 데 중요합니다. 데이터를 수동으로 검사하면서, 우리는 데이터의 중요한 문제와 패턴을 발견하고 (어떤 경우에는) 수정하기 시작할 것입니다. 그러나 이 큐레이션 프로세스를 우리의 판단을 넘어 확장하려면, 휴리스틱(heuristics) 또는 다른 머신러닝(machine learning) 모델을 기반으로 하는 자동화된 기술을 사용해야 할 것이며, 이는 데이터 편향성을 줄이는 데 기여합니다.

(source) **수동 검사(Manual inspection).** LLM을 디버깅하는 첫 번째 단계는 단순히 모델의 훈련 데이터를 살펴보는 것입니다. 수동 데이터 검사는 매우 시간 소모적이며 (항상 가장 재미있는 것은 아니지만!), LLM 개발의 중요한 부분이며, 이는 윤리적 책임을 강화합니다. 수동 데이터 검사의 주요 한계는 확장 가능하지 않다는 단순한 사실입니다. 데이터를 잘 이해하기 위해 충분한 수동 검사 3을 수행했다면, 데이터 검사 노력을 확장하기 위한 더 나은 전략을 개발해야 합니다.

**휴리스틱 필터링(Heuristic filtering).** 수동 검사는 우리 데이터에서 많은 문제와 흥미로운 패턴을 발견할 것이며, 특히 개인 정보 침해 위험을 식별합니다. 우리 모델이 데이터의 이러한 최적 이하의 패턴을 반영하지 않도록 하기 위해, 우리는 휴리스틱을 사용하여 이러한 패턴과 일치하는 훈련 예제를 찾아 필터링(또는 수정)할 수 있습니다.

(source) 우리가 고려할 수 있는 데이터 검사 및 필터링을 위한 다른 많은 휴리스틱이 있습니다. 예를 들어, 특정 데이터 소스가 다른 데이터 소스에 비해 더 높은 품질이거나 유용한 속성을 가지고 있음을 발견할 수 있습니다.

**모델 기반 필터링(Model-based filtering).** 관찰된 문제가 휴리스틱하게 해결될 수 없다면, 이는 데이터 프라이버시 보호에 더욱 효과적입니다. fastText