**I. AI 산업은 풍전등화와 같다**

2025년 8월, 제가 작성했던 인기 있는 AI 거품 관련 글에서 다음 문구를 인용했습니다. 많은 이들이 거품이 허무맹랑한 곳에서 발생하며, 그 속에는 거짓과 미완성 제품(vaporware) 외에는 아무런 본질적 가치가 없다고 생각합니다. 마치 언제 터질지 모르는 폭탄처럼 말이죠. 그러나 현실은 다릅니다. 거품은 단지 실제 가치의 건강하지 못한 과도한 확장에 불과합니다. OpenAI의 샘 올트먼(Sam Altman) CEO가 더 버지(The Verge)에 밝힌 대로, 모든 것에는 '진실의 핵심(kernel of truth)'이 존재합니다. 저는 이 진실의 핵심이 과도한 약속과 투자에 가려질 때 발생하는 일반적인 상황을 계속해서 설명해 왔습니다. 하지만 이번에는 이야기가 다릅니다. 거품은 그 진실의 핵심에서 얼마나 멀리까지 팽창할 수 있는지에 따라 정의되며, 핵심의 취약성으로 정의되는 것이 아닙니다.

지난 2025년 9월에 발표된 또 다른 글에서는 AI 거품을 철도 붐과 견주어 보았습니다. 철도 붐이 일기 전부터 철도가 사회에 미치는 긍정적인 영향은 명백했습니다. 저는 당시 이렇게 기술했습니다. 1800년대 중반의 철도 열풍, 1700년대 후반의 운하 열풍, 그리고 1600년대 중반의 튤립 열풍처럼 과도한 거품 현상을 이해하는 것은 그리 어렵지 않습니다. (이러한 투기는 주로 규모에 초점을 맞춥니다: '이 정도는 필요 없을지 몰라도, 누군가는 이득을 볼 것이다'). 반면, 투기적 거품(betting bubble)을 이해하는 것은 더욱 복잡합니다. (이 투기는 실현 가능성에 기반합니다: '이것이 성공할 수도 있고 아닐 수도 있으니, 한번 해보자'). AI는 이 두 가지 유형의 거품 특성을 모두 지니고 있습니다.

이것은 실로 좋지 않은 소식입니다! 하지만 앞선 두 편의 글에서 많은 내용을 다루었음에도 불구하고, 저는 핵심적인 질문에 대한 답을 제시하지 못했습니다. AI의 '진실의 핵심' 중 어떤 부분이 AI를 도박으로 만드는가? 저는 거대 언어 모델(LLM)의 본질적인 신뢰성 부족(이는 분명한 사실입니다)과 더불어, 이러한 도박이 지닌 정치적, 사회문화적, 지정학적, 경제적 측면들을 폭넓게 설명했습니다. 그러나 우리는 너무나도 당연하게 받아들이는 진실의 핵심에 존재하는 명백한 균열처럼, 제 주장을 뒷받침할 구체적인 기술적 사례를 적극적으로 탐구하지는 않았습니다. AI 분야는 역사적으로 상징주의(symbolism)와 연결주의(connectionism)라는 두 가지 주요 패러다임 사이를 오가며 발전해왔습니다. 각 패러다임이 번성하고 쇠퇴하는 과정에서, '진실의 핵심'은 항상 존재했지만, 그 해석과 확장 방식에 따라 거품이 형성되곤 했습니다. 현재의 LLM 중심 AI 역시 이러한 역사적 패턴의 연장선상에 있으며, 그 '진실의 핵심'이 과연 얼마나 견고한지에 대한 의문은 여전히 남아 있습니다.

그러던 중, 10월에 출간한 “신이 되고 싶었던 뉴런(The Neuron That Wanted to Be God)”을 통해 저는 잠시 AI 역사가가 되어 80년 묵은 비밀을 밝혀냈습니다. (이 분야의 역사에서 단 한 가지만 기억해야 한다면 바로 이것입니다.) 현대 AI의 근간은 인간 지능의 기본 구성 요소인 뉴런(neuron)을 지나치게 단순화한 모델 위에 구축되어 있습니다. 물론 여러분은 그 글 또한 좋아해 주셨지만, 아마도 너무 추상적이어서 설득력이 부족했을 것입니다. ChatGPT 내부의 수많은 추상화 계층 아래에 있는 인공 뉴런(artificial neuron)이 여러분의 뇌를 구성하는 생물학적 뉴런(biological neuron)과 전혀 유사하지 않다면, 과연 그것이 그렇게 중요한 문제일까요? 이에 대한 유일하게 합리적인 답변은 '모른다'입니다. **불확실한 대안만으로 진실의 핵심에 균열을 낼 수는 없습니다!** 이처럼 근본적인 가정에 대한 의문은 AI의 기초를 다시 한번 돌아보게 만듭니다. 우리는 과연 올바른 방향으로 나아가고 있는가, 아니면 과거의 실수를 반복하고 있는가? 이 질문에 대한 답은 단순히 기술적인 문제를 넘어, AI가 사회에 미칠 장기적인 영향과도 직결됩니다.

오늘 제가 제시할 내용은 시의성 있는 주제에 대한 심층적인 분석입니다. 이는 지난 세기의 기록을 파고들 필요 없이, 1) AI 산업의 기술적 근간—즉, LLM 확장이 무한한 발전을 위한 필수 전제라는 믿음—에 의문을 제기하고, 2) 재정적 근간—거대하고 고비용의 LLM을 훈련하고 서비스할 데이터센터 구축에 1조 달러를 투자하는 것이 불필요할 수 있다는 생각—에 회의를 가질 충분한 근거를 제시합니다. 저는 실리콘밸리가 현상 유지에 지나치게 집착하고 안주해왔다고 생각합니다. 저를 포함한 많은 이들은 LLM이 취약성과 신뢰성 문제를 드러내면서도 막대한 비용을 요구하지 않는 AI 혁신을 찾아야 할 강력한 동기가 있다고 믿습니다. 오늘 저는 그 진실의 핵심에 균열을 내고자 합니다(스포일러가 있지만, 이야기에 반전이 있으니 끝까지 읽어주시기 바랍니다). 이러한 탐구는 현재 AI 생태계가 안고 있는 구조적인 문제점을 직시하고, 보다 지속 가능하며 효율적인 대안을 모색하는 데 중요한 역할을 할 것입니다.

이러한 여정을 위해 우리는 시간과 공간을 초월해야 합니다. 2025년 7월의 싱가포르로, 중국 최고 명문 공과대학인 칭화대학교(Tsinghua University) 출신들이 세운, 거의 알려지지 않은 한 AI 연구소의 본사로 돌아가야 합니다. 이 대학은 서구의 모든 대학을 합친 것보다도 빠른 속도로 AI 인재를 배출하고 있습니다. 우리의 이야기는 사피엔트 인텔리전스(Sapient Intelligence)에서 시작되며, 이는 기발한 발상에서 비롯됩니다. 뇌에서 영감을 받은 모델, 전체 웹을 사전 훈련하는 LLM에 대한 도전, 그리고 세계 반대편에서 아무것도 모른 채 잠들어 있는 훨씬 부유한 AI 연구소들을 상대로 한 명확한 승리가 그것입니다. 이 이야기는 계층적 추론 모델(Hierarchical Reasoning Model)에서 출발합니다. 이 연구소는 기존의 거대하고 비효율적인 LLM 패러다임에 대한 근본적인 의문을 제기하며, 새로운 방향을 제시할 가능성을 보여주었습니다.

**II. 돌파구: 계층적 추론 모델(Hierarchical Reasoning Model)**

2025년 7월, 왕 관(Guan Wang) 연구팀은 arXiv에 한 연구 논문을 게재했으며, 이는 즉시 AI 커뮤니티의 이목을 집중시킨 트위터 스레드와 함께 발표되었습니다. "뇌의 계층적 처리(hierarchical processing) 방식에서 영감을 받은 HRM은 단 1천 개의 샘플만으로, 사전 훈련(pre-training)이나 CoT(chain of thought) 과정 없이 ARC-AGI 및 전문가 난이도 스도쿠와 같은 복잡한 문제에서 탁월한 추론 능력을 선보입니다! 신경과학을 통해 다음 AI 혁신을 주도하세요." 이 발표는 기존 LLM 연구의 한계에 대한 새로운 시각을 제공하며, AI 분야에 신선한 충격을 주었습니다.

몇 가지 점은 즉시 우리의 관심을 끕니다. 생체 모방(bioinspiration)이라는 개념? 단 1,000개의 예시로? 사전 훈련 없이? CoT 없이? 심지어 ARC-AGI에서 기존 추론 LLM을 능가한다니? 이 모든 수사적 질문들은 잠재적으로 패러다임을 완전히 전환시킬 수 있는 요소들입니다. AI 연구소들은 어떻게 이런 중요한 발전을 간과할 수 있었을까요? 저는 '이는 엄청난 사건이거나, 아니면 실망스러운 농담일 것이다'라고 생각했습니다. 하지만 제가 이 글을 쓰고 있고 여러분이 웃지 않는다는 사실을 보면, 우리는 이미 그 답을 알고 있는 듯합니다. 이러한 주장은 특히 오늘날의 AI 개발이 막대한 자원과 데이터에 의존하는 현실 속에서 더욱 두드러집니다. 과연 효율적인 소규모 모델이 거대 모델의 성능을 넘어설 수 있을까요?

내용이 다소 전문적일 수 있지만, 최대한 명확하게 설명하고자 합니다. 왕 연구팀이 계층적 추론 모델(HRM)을 개발하게 된 주요 동기는 거대 언어 모델(LLM)이 추론을 위해 사고의 사슬(CoT)에 의존한다는 점 때문입니다. 이는 높은 비용, 막대한 데이터 요구량, 그리고 긴 지연 시간이라는 단점을 수반합니다. CoT는 현재 상업적으로 활용되는 거의 모든 LLM의 핵심 기반입니다. CoT 없이는 ChatGPT가 복잡한 수학이나 코딩 문제를 해결하도록 할 수 없습니다. 대부분의 경우, 기업들은 모델의 CoT 과정을 숨기지만, DeepSeek-R1과 같은 모델은 CoT를 가시적으로 유지하여 특별히 주목받았습니다(가시적인 CoT는 예상치 못한 결과를 초래하기도 했습니다). 여러분은 R1이 답변을 내기 전에 문제 해결 과정을 스스로에게 말하고, 탐색하고, 되짚어보고, 심지어 곁가지로 빠지는 등 인간과 유사한 방식으로 작동하는 것을 보았을 것입니다. 왕 연구팀은 CoT를 비효율적인 접근 방식으로 간주합니다. 그들은 서구 AI 연구자들이 뇌에서 영감을 얻으려는 시도를 하지 않았기 때문에 CoT가 여전히 존재한다고 주장합니다. 이와 대조적으로 HRM은 "인간 뇌의 계층적이고 다중 시간 규모 처리(hierarchical and multi-timescale processing) 방식에서 영감을 받았습니다." LLM의 근간을 이루는 중간 계층을 재검토하여 인간 뇌와 유사하게 만들려는 시도는 저 혼자만의 관심사가 아닌 듯합니다! 이는 오래된 순환 신경망(RNN)과 컨볼루션 신경망(CNN) (많은 분들에게는 생소할 수 있는 개념입니다)에서 어느 정도 찾아볼 수 있습니다. 그러나 모든 현대 LLM의 기반인 유명한 트랜스포머(Transformer)(Vaswani et al., 2017)는 순환과 컨볼루션 모두를 배제하고 어텐션 메커니즘(attention mechanism)을 선호했습니다. 그래서 논문 제목이 "어텐션만 있으면 된다(Attention is all you need)"인 것이죠. 흥미롭게도 어텐션 메커니즘 자체는 뇌에서 느슨하게 영감을 받았지만, 현대 LLM이 우리의 인지 엔진(cognitive engine)과 유사한 정도는 거기까지입니다(즉, 유사성이 거의 없습니다). CoT의 이러한 한계는 LLM이 진정한 지능보다는 패턴 매칭에 가깝다는 비판을 불러일으킵니다. 인간은 문제를 해결할 때 단순히 순차적인 사고 과정을 거치지 않고, 병렬적으로 다양한 정보를 탐색하고, 직관적으로 판단하며, 때로는 불완전한 정보 속에서도 결론을 도출합니다. HRM은 이러한 인간의 인지 과정을 모방하려는 시도로, 기존 LLM의 접근 방식과는 근본적인 차이를 보입니다.

앞서 언급된 주목할 만한 사항들을 좀 더 면밀히 검토해 봅시다. 단 1,000개의 예시로 구성된 데이터셋은 수십조 개의 토큰(단어)으로 훈련되는 LLM의 규모와 비교하면 사실상 미미한 수준입니다. 물론, 이는 직접적인 비교(apples-to-apples comparison)는 아닙니다. 왕 연구팀이 HRM을 언어 관련 작업에 대해 테스트하지 않았기 때문입니다. 그들은 스도쿠와 같은 퍼즐과 제가 특히 흥미를 느끼는 ARC-AGI 벤치마크에 초점을 맞췄습니다. (이것이 HRM이 언어를 이해하지 못하므로 무관하다는 뜻도 아니며, ARC-AGI에서 LLM을 능가했으니 언어를 배우면 분명 LLM을 이길 것이라는 의미도 아닙니다. 우리는 이에 대해 어떠한 단정적인 주장도 할 수 없으므로, 비교는 최대한 신중하게 접근해야 합니다.) 하지만 이처럼 극소량의 데이터만으로도 복잡한 추론 문제를 해결할 수 있다는 점은 기존 LLM의 '데이터 스케일링 법칙'에 대한 근본적인 의문을 제기합니다. 과연 지능은 방대한 데이터의 양에 비례하는가, 아니면 효율적인 학습 메커니즘과 아키텍처에서 비롯되는가? HRM은 후자의 가능성을 강력하게 시사합니다.

결과적으로 HRM은 놀라운 훈련 데이터 효율성을 보이며, 사전 훈련(pre-training)이라는 기존 패러다임 또한 거부합니다. 여러분은 스케일링 법칙(scaling laws)과 관련하여 사전 훈련에 대해 들어본 적이 있을 것입니다. AI 연구소들은 컴퓨팅 자원과 데이터셋 규모를 확장하여 사전 훈련(즉, 인터넷의 방대한 데이터를 학습)을 통해 AI 모델을 개선합니다. 그러나 2024년 말, 스케일링 법칙이 수확 체감(diminishing returns) 현상을 보이기 시작하면서, 기업들은 후처리 훈련(post-training, 추론 데이터에 대한 강화 학습)과 테스트 시점 연산(test-time compute, 모델이 응답하기 전에 '생각'하도록 허용하는 방식)으로 전략을 전환했습니다. 오늘날 OpenAI, Google, Meta 등은 후처리 훈련 확장에 주력하고 있으며, 사전 훈련 확장은 (완전히는 아니지만) 뒷전으로 미뤄두었습니다. 하지만 그들 중 어느 누구도 단 한 순간도 사전 훈련을 완전히 없애는 것을 고려하지 않았습니다. 이것이 바로 HRM을 기존과는 다른 패러다임(paradigm)으로 만듭니다. 이것을 HRM이 기존 패러다임을 개선하는 패러다임 전환(paradigm shift)이라고 오해해서는 안 됩니다. HRM이 이론적으로나 근본적인 측면에서 반드시 우월하다고 단정할 수는 없습니다! 저는 DeepSeek-R1 Zero의 맥락에서 AI가 인터넷을 학습하지 않고도 학습할 수 있는 가능성에 대해 이전 글에서 다룬 바 있습니다. 제 요점은 알파고 제로(AlphaGo Zero)가 인간의 지도 없이, 인간 바둑 기보를 보지 않고 순수하게 자기 학습(self-play)을 통해 바둑을 익혀 인간 기보로 훈련된 알파고를 즉시 능가했던 것처럼, 언어와 추론 영역에서도 유사한 일이 일어날 수 있다는 것입니다. 저는 이 아이디어를 매우 긍정적으로 봅니다. 왜냐하면 AI 선구자인 리처드 서튼(Richard Sutton)이 말했듯이, 인간은 "기본적인 것을 하는 방법"에 대한 모든 서적을 읽어서 배우는 것이 아니기 때문입니다. 우리는 실험하고 시행착오를 겪으며 (상당히 유용한 유전적 소질의 도움을 받습니다). 어쨌든, 성급하게 결론을 내리고 싶지는 않습니다. 이는 매력적인 가능성이지만, 인터넷 전체를 미리 학습하지 않고 언어 숙달을 달성하는 것이 가능한지는 아직 아무도 알 수 없습니다. 사전 훈련의 부재는 모델의 편향성(bias) 문제를 해결하고, 특정 도메인에 특화된 AI를 개발하는 데 더 큰 유연성을 제공할 수 있습니다. 이는 AI의 범용성을 추구하면서도 동시에 특정 목적에 최적화된 모델을 만들 수 있는 새로운 길을 열어줄 수 있습니다.

연구 결과로 깊이 들어가기 전에, HRM에 대한 또 다른 중요한 사실은 (곧 자세히 다룰 내용입니다!) 이 모델이 "단지" 2,700만 개의 매개변수만을 가진 극도로 작은 모델이라는 점입니다. 예를 들어, GPT-4가 1조 8천억 개의 매개변수를 가졌다고 추정되는 점을 고려하면, '단지'라는 표현에 인용 부호가 필요한지 의문입니다. 이는 180만 백만 개의 매개변수, 즉 HRM 크기의 10만 배에 달하는 수치입니다. 저는 앞서 동등한 비교가 아니므로 직접적인 비교를 피하고 싶다고 언급했지만, 이는 LLM과 AI 구조의 근간에서 그들을 대체하려는 다른 모든 신경망(neural network) 간의 근본적인 차이를 보여줍니다. 이 매개변수 규모의 차이는 단순히 숫자의 문제가 아니라, AI 모델의 개발, 훈련, 배포 및 유지보수 비용에 막대한 영향을 미칩니다. 작은 모델은 더 적은 컴퓨팅 자원, 더 적은 에너지 소비, 더 빠른 개발 주기를 의미하며, 이는 AI 접근성을 획기적으로 향상시킬 수 있습니다.

**[차트 삽입: HRM은 ARC-AGI, 스도쿠-익스트림(Sudoku-Extreme), 미로-하드(Maze-Hard)에서 최첨단 CoT 모델을 능가합니다. (출처)]**

단 1,000개의 예시로 구성된 공식 데이터셋으로 처음부터 훈련된 HRM은 2,700만 개의 매개변수와 30x30 그리드 컨텍스트(900 토큰)라는 작은 규모에도 불구하고 40.3%의 성능을 달성했습니다. 이는 o3-mini-high(34.5%) 및 Claude 3.7 8K 컨텍스트(21.2%)와 같이 훨씬 더 큰 매개변수와 컨텍스트 길이를 가진 선도적인 CoT 기반 모델들을 상당히 앞서는 수치입니다. 그래프의 왼쪽 두 막대는 ARC-AGI 1과 2의 결과입니다. HRM은 DeepSeek-R1, Claude Sonnet 3.7, o3-mini-high를 모두 능가합니다. 이 모든 모델은 해당 도전에 앞서 방대한 인류 데이터를 학습했습니다. 어떻게 이런 결과가 가능할까요? HRM이 언어를 학습하지 않았기 때문에 광범위한 의미에서 직접적인 비교는 아니지만, ARC-AGI 1과 2의 이러한 결과는 강력한 모델이라면 쉽게 해결해야 할 문제라는 점에서 공정한 경쟁입니다! 그럼에도 불구하고, 뇌에서 영감을 받고, 사전 훈련도, CoT도 없이, 그리고 매우 작은 규모의 HRM이 승리했습니다. 왕 연구팀이 이러한 예상치 못한 성공에 대해 어떤 설명을 제시하는지 살펴보겠습니다. 이 결과는 AI 커뮤니티에 '지능의 본질은 무엇인가?'라는 근본적인 질문을 던집니다. 단순히 많은 데이터를 외우고 복잡한 패턴을 인식하는 것을 넘어, 진정한 추론 능력은 어디에서 오는가? HRM의 성공은 효율적인 아키텍처와 학습 메커니즘이 데이터 양만큼이나 중요하다는 것을 강력하게 시사합니다.

**III. 맞대결: HRM 대 LLM**

저자들은 LLM의 아키텍처(architecture)에 대해 명확한 비판을 제기하며, 이는 HRM과의 첫 번째 대립 지점입니다. "거대 언어 모델의 놀라운 성과에도 불구하고, 그 핵심 아키텍처는 역설적으로 얕은 특성을 지닙니다." 여기서 '역설적으로 얕다'는 것은 트랜스포머(Transformer)가 겉으로는 복잡해 보이지만, 그 계산 능력의 이론적 한계가 "복잡한 알고리즘적 추론"을 수행하는 데 제약이 된다는 의미입니다. 그들은 이러한 특성 때문에 강력한 LLM조차 ARC-AGI와 같은 퍼즐을 해결하지 못한다고 주장합니다. (이 부분에서 전문 용어가 많아 모든 세부 사항을 이해할 필요는 없습니다. 명확성을 위해 대부분 생략했지만, 필요하다면 언제든 원 논문과 관련 자료를 찾아볼 수 있습니다!) 이러한 비판은 LLM의 성공이 특정 유형의 문제 해결에 국한될 수 있음을 시사하며, 진정한 일반 인공지능(AGI)으로 가는 길에 구조적인 한계가 존재할 수 있다는 우려를 더합니다.

HRM은 느리고 고수준의 계획 모듈(module)과 빠르고 저수준의 실행 모듈이라는 두 개의 결합된 구성 요소를 활용하여 트랜스포머의 이러한 한계를 극복하는 것으로 보입니다. 그러나 AI 연구소들은 같은 압력이나 우선순위를 따르지 않습니다. 그들은 AI 모델이 상업적으로 실행 가능해야 하며, 그 후에야 탁월한 추론 능력을 갖춰야 한다고 믿습니다. 이것이 바로 현실적인 접근 방식입니다. 아무리 훌륭한 극초음속 비행기 설계도가 있더라도, 속도가 절반인 비행기보다 100배나 비싸다면, 대부분은 후자를 선택할 것입니다. 이러한 실용주의적 접근은 단기적인 성과와 시장 점유율 확보에는 유리할 수 있지만, 장기적인 관점에서 AI의 근본적인 발전을 저해할 수 있다는 비판을 받습니다. 상업적 성공이라는 목표가 기술적 진보의 방향을 왜곡할 수도 있다는 것이죠.

따라서 생체 모방(bioinspiration)을 통해 아키텍처를 불필요하게 복잡하게 만드는 대신(복잡성 때문에 상업적으로 실현 불가능합니다. 진화의 걸작을 재설계하는 것은 불가능합니다), 그들은 다른 방식으로 LLM의 추론 능력을 향상시킵니다. 바로 사고의 사슬(chain of thought)을 이용하는 것입니다. LLM이 내부적으로 사고하는 대신, 인간 언어(명시적인 단어)를 사용하여 문제 해결의 중간 단계를 "소리 내어 생각"하도록 허용합니다. 이를 통해 그들은 병목 현상을 '트랜스포머는 추론할 수 없으며 이는 문제다'에서 '트랜스포머는 CoT 패치(patch)를 활용하여 추론할 수 있으며 이는 문제다'로 전환합니다. 여전히 단점이 있지만, 적어도 상업적으로 실현 가능한 단점입니다! 이 '패치'는 LLM이 지닌 본질적인 한계를 우회하는 임시방편적인 해결책으로 볼 수 있습니다. 마치 자동차가 고장 났을 때 임시로 테이프를 붙여 운행하는 것과 같죠. 당장은 작동하지만, 근본적인 문제는 해결되지 않은 채 남아있습니다.

하지만 그들은 아직 만족할 수 없습니다. LLM의 추론 문제는 해결되었지만, 이제는 그 '패치' 자체를 다뤄야 합니다. 요약하자면, LLM은 과학적 정교함보다는 공학적 간결성을 우선시하는 타협의 결과물입니다. AI 기업들이 내부적으로 추론 능력이 부족한 한계를 우회해야 했기 때문에 LLM은 "덜 세련된" 모습을 띠지만, 그 대신 병렬화 및 확장이 용이해져, 여러분과 같은 까다로운 사용자들도 ChatGPT를 활용할 수 있게 되었습니다. 이러한 트레이드오프는 기술 발전의 한 단면을 보여줍니다. 즉, 이론적으로 완벽한 솔루션이 항상 현실 세계에서 가장 좋은 선택은 아니라는 것입니다. 그러나 이러한 선택이 장기적인 관점에서 최적인지는 여전히 논란의 여지가 있습니다.

반면 HRM은 자연계에서 추론을 본질적인 기능으로 수행하는 유일한 존재인 인간의 뇌를 모방함으로써 이러한 '패치'를 우회합니다. HRM은 추론 과정을 내면화하여, '사고의 공간'과 유사한 '잠재 공간(latent space)'에서 진행되도록 합니다. LLM이 사고 과정을 명시적으로 기록해야 하는 것과 달리, HRM은 그러한 과정이 필요 없습니다. 잠재 공간에서의 추론은 훨씬 더 유연하고 효율적인 정보 처리 방식을 가능하게 합니다. 이는 인간이 의식적으로 모든 사고 과정을 언어로 표현하지 않듯이, AI도 내부적으로 더 추상적이고 통합적인 방식으로 추론할 수 있음을 의미합니다.

이 분야의 저명한 인물들도 이 점에 대해 의견을 피력했습니다. 대표적으로 메타 FAIR(Meta FAIR)의 수석 AI 과학자인 얀 르쿤(Yann LeCun)은 연속적인 잠재 공간(continuous latent space)에서의 추론이 순차적이고 이산적인 단어 공간(sequential, discrete space of words)에서의 추론보다 훨씬 더 풍부한 연산 능력과 넓은 대역폭(bandwidth)을 제공할 때, AI 시스템이 인간처럼 추론하도록 강요하는 것은 비합리적이라고 주장합니다. 그가 언급했듯이, "연속적인 임베딩 공간(continuous embedding space)에서의 추론이 이산적인 토큰 공간(discrete token space)에서의 추론보다 훨씬 강력하다는 것은 직관적으로 명백합니다." 왕 연구팀이 인용한 메타 FAIR의 연구 논문(Hao et al., 2024)과 르쿤이 자신의 트윗에서 언급한 또 다른 FAIR 논문(Zhu et al., 2025)이 이를 뒷받침합니다. 르쿤의 이러한 관점은 LLM의 현재 구조적 한계를 명확히 지적하며, AI 연구가 나아가야 할 방향에 대한 중요한 통찰을 제공합니다. 잠재 공간에서의 효율적인 추론은 AI가 보다 인간적인 방식으로 세계를 이해하고 문제를 해결할 수 있는 기반이 될 수 있습니다.

저는 LLM과 CoT 패러다임에 대한 이러한 매우 직관적인 비판에 전적으로 공감합니다. LLM이 사고 과정을 명시적으로 기록하는 방식만을 고수해야 한다는 주장은, 사고 수준에서 추론을 수행하면서 동시에 상업적으로 실현 가능한 더 나은 아키텍처를 발견할 수 없다는 안일한 변명에 불과합니다. 그럼에도 불구하고 단어 수준의 추론은 유용합니다. 부분적으로 인간은 사고를 정리하기 위해 글을 쓰기도 합니다. 그러나 우리는 이러한 메커니즘에만 국한되지 않으며, 이것이 주된 사고 방식도 아님이 분명합니다. 언어가 의사소통보다는 사고를 위한 도구라는 일반적인 인식과 달리, 최근 연구에서는 언어가 실제로는 "사고보다는 의사소통을 위한 도구"임을 시사합니다. 이러한 관점에서 LLM의 CoT는 인간의 사고 과정을 모방하는 데 있어 본질적인 한계를 가집니다. 인간은 언어 없이도 복잡한 추론을 수행하며, 언어는 주로 그 결과를 전달하고 공유하는 매개체 역할을 합니다.

그렇다면 HRM이 내부적으로 사고하고 뇌와 유사하다는 주장은 실제로 어떤 의미일까요? 구체적으로 어떻게 추론이 이루어질까요? ARC-AGI 팀이 명명한 "사고 폭발(thinking bursts)"이라는 과정을 통해 추론합니다. HRM은 특정 작업(예: ARC-AGI 1 퍼즐)에 대한 해결책을 제시한 다음, 내부적으로 반복하여 이를 개선하며, 각 단계에서 해결책을 더 다듬을지 아니면 중단할지 결정합니다. 느린 계획자(slow planner)와 빠른 실행자(fast executor)가 협력하여 새로운 답변을 도출하는 과정은 바로 이러한 반복적인 개선 과정(iterative refinement process) 내에서 이루어집니다. (이는 테스트 시점 훈련(test-time training)의 한 예시로, HRM은 추론/테스트 시점에 문제 해결 방법을 학습하여 훈련 단계에서 연산 비용을 절감합니다.) 이 방식은 인간이 문제를 풀 때 다양한 시도를 하고, 오류를 수정하며, 점진적으로 해결책을 찾아가는 과정과 매우 흡사합니다. 이는 LLM이 한 번에 최종 답변을 생성하는 방식과는 대조적이며, 보다 유연하고 강건한 추론 능력을 제공할 수 있습니다.

좋습니다, 하지만 솔직히 말해서, LLM이 유행하고 있고 ChatGPT, Gemini, Claude, Grok, Llama, DeepSeek 등 세계에서 가장 강력한 AI 시스템의 기반이 되는 상황에서 누가 HRM에 신경 쓰겠습니까? 음, 여기 제 불완전한 목록이 있습니다. HRM은 다음 사람들에게 좋은 소식입니다.

*   그동안 몇 가지 알고리즘적 돌파구가 없다면 LLM 확장이 작동할 수 없는 무차별 대입 방식(brute-force approach)이라고 생각하는 사람들.
*   LLM이 "소리 내어" 추론하도록 강요하는 것을 싫어하고 대신 "잠재/토큰 공간" 추론을 선호하는 사람들. 모든 인간의 글쓰기는 생각하는 것이지만, 모든 인간의 생각이 글쓰기는 아닙니다.
*   AI 연구소들이 악명 높게 신뢰할 수 없는 연산 집약적이고 데이터 집약적인 LLM을 훈련하기 위해 모든 사람의 데이터를 훔치고 있다는 사실에 분노하는 사람들.
*   AI 시스템이 현재보다 생물학과 인간 뇌에서 더 많은 영감을 얻어야 한다고 생각하는 사람들.
*   근본적인 것을 재검토하기 전에 데이터센터 인프라에 수조 달러를 지출하는 것은 말이 안 된다고 생각하여 금융 AI 거품이 있다고 생각하는 사람들.
*   인간에게는 쉽지만 최고의 AI 모델들을 끊임없이 좌절시키는 몇 안 되는 AI 평가 중 하나인 ARC-AGI 벤치마크를 좋아하는 사람들.
*   AI 개발의 환경적 지속 가능성에 대해 우려하는 사람들. LLM의 막대한 에너지 소비는 전 세계적인 문제로 대두되고 있으며, 소규모 모델의 효율성은 이러한 문제에 대한 실질적인 대안이 될 수 있습니다.
*   AI 기술의 민주화와 접근성 확대를 바라는 사람들. 소규모 모델은 대기업이 아닌 개인 연구자나 소규모 팀도 혁신적인 AI를 개발할 수 있는 기회를 제공합니다.

**IV. HRM 성능의 숨겨진 동인**

하지만 운명의 아이러니처럼—반전이 있다고 경고했었죠!—HRM이 논문(혹은 이 섹션)에서 제시하는 만큼 대단하지 않다는 사실이 드러났습니다. 첫째, 저자들은 테스트 데이터로 훈련했다는 비난(데이터 유출)에 직면했지만, 다행히도 이는 사실이 아님으로 밝혀졌습니다(논문과 HRM은 여전히 유효합니다). 따라서 이것이 문제의 원인은 아닙니다(이에 대해 들어봤지만 해결 과정을 몰랐던 분들을 위해). 둘째, 저자들이 공개적으로 인정한 여러 단점들이 존재합니다. 확장성은 언제나 어려운 문제이며 HRM으로는 불가능할 수도 있고, 훈련과 추론이 결합되어 있어 비용이 많이 듭니다. HRM은 "범용" 모델이 아닙니다(즉, 이전에 접한 작업만 해결할 수 있습니다). 이러한 한계는 HRM 접근 방식에 치명적일 수 있습니다. 마지막으로, 저자들이 명백히 간과한 또 다른 치명적인 결함이 있는데, 제 생각에는 그것이 결정적인 부분입니다. 즉, HRM의 성공 요인이 '생체 모방'이라는 주장과는 전혀 다른 곳에 있었다는 점입니다. 이는 과학 연구에서 가설과 실제 결과가 다를 때 발생하는 흥미로운 사례를 보여줍니다.

HRM의 핵심 주장—즉, 뇌의 계층적이고 다중 시간 규모 처리 방식에서 영감을 받았기 때문에 사전 훈련이나 CoT 없이도 LLM보다 잠재 공간(latent space) 대 토큰 공간(token space)에서 추론하고 더 큰 "계산 깊이"를 달성할 수 있다는 것—은 대부분 사실과 무관합니다! 저자들은 생체 모방 등에 대한 수많은 잘못된 결론 아래에 의도치 않게 진정한 핵심을 숨겨버렸습니다. 이 연구의 실제 혁신이자 ARC-AGI 성능의 주요 원동력은 ARC-AGI 팀이 언급했듯이 "예상치 못한 출처"에서 비롯됩니다. 지금부터 어떤 일이 벌어지고 있는지 살펴보겠습니다. 이러한 발견은 AI 연구자들이 특정 가설에 얽매이지 않고, 실제 데이터를 통해 얻은 통찰을 바탕으로 연구 방향을 유연하게 조정해야 함을 강조합니다. 때로는 가장 기대하지 않았던 곳에서 진정한 돌파구가 나타나기도 합니다.

우선, 제가 말씀드렸듯이, HRM의 ARC-AGI 성능과 논문의 정당성은 확인되었습니다. ARC 프라이즈(ARC Prize) 공동 설립자이자 ARC-AGI 개발자인 프랑수아 숄레(François Chollet)는 트위터에서 이렇게 말했습니다.

ARC-AGI-1: 32% - 최첨단은 아니지만, 이렇게 작은 모델치고는 인상적입니다.
ARC-AGI-2: 2% - 0%를 초과하는 점수는 어느 정도 신호를 보여주지만, 우리는 이것을 ARC-AGI-2에 대한 실질적인 진전으로 간주하지 않습니다.

**[차트 삽입: 작업당 비용 대비 HRM 성능을 나타내는 ARC-AGI-1 리더보드. (출처)]**

ARC-AGI 팀은 외부 루프(outer loop), 즉 왕 연구팀이 "순환 연결성(Recurrent connectivity)"이라 칭하는 요소가 HRM이 ARC-AGI에서 LLM을 능가하는 진정한 원인임을 밝혀냈습니다. 그들은 HRM 구성 요소 중 불필요하거나 심지어 방해가 되는 요소가 있는지 확인하기 위해 제거 실험(ablation experiment)(매우 중요합니다!)을 수행했으며, 그 결과 다음과 같은 사실을 발견했습니다. HRM 모델 아키텍처 자체(논문의 핵심으로 제시된 부분)는 중요한 요소가 아니며, [오히려] 논문에서 거의 언급되지 않았던 외부 개선 루프(outer refinement loop)가 성능의 주요 동력이었습니다. 이는 성공을 이끈 요소가 인간 뇌와는 전혀 무관하다는 것을 의미합니다. 그들은 또한 "'계층적(hierarchical)' 아키텍처는 유사한 크기의 트랜스포머(Transformer)와 비교했을 때 성능에 미미한 영향만을 미쳤다"고 덧붙였습니다. 이런! 결국 트랜스포머 기반 LLM이 다시 한번 우위를 점합니다! 이 결과는 생체 모방의 한계를 명확히 보여줍니다. 뇌에서 영감을 얻는 것은 좋은 출발점일 수 있지만, 맹목적인 모방이 항상 최적의 결과를 가져오는 것은 아니라는 점을 시사합니다. 중요한 것은 어떤 원리가 작동하는지를 정확히 파악하고, 이를 효율적으로 구현하는 것입니다.

**[차트 삽입: "다양한 훈련 및 추론 개선 루프(refinement loop) 수에 따른 Pass@2 성능. 데이터를 개선하여 반복하는 것이 강력한 영향을 미치며, 1(개선 없음)에서 2(1회 개선)로의 도약이 보여주듯이." (출처)]**

외부 루프는 "모델 출력을 다시 자신에게 피드백하여, 모델이 예측을 반복적으로 개선할 수 있도록 합니다." 따라서 성능 향상을 제공하는 것은 결합된 모듈(느린 계획자 + 빠른 실행자)의 실제 작업 해결이나 아키텍처의 세부 사항이 아니라, 해결책이 모델에 다시 피드백된다는 사실입니다. (위 차트에서 볼 수 있듯이, 루프를 수행하는 횟수는 성능에 큰 영향을 미칩니다.) 이 '피드백 루프'는 인간의 학습 과정과도 유사합니다. 우리는 어떤 문제를 해결할 때 단순히 한 번에 답을 내는 것이 아니라, 초기 시도에서 피드백을 얻고, 이를 바탕으로 다음 시도를 개선해 나갑니다. 이러한 반복적인 개선 과정이 지능적인 문제 해결에 필수적이라는 점을 AI 모델이 증명한 셈입니다.

ARC-AGI 팀은 이러한 간소화된 HRM을 유니버설 트랜스포머(Universal Transformer)와 비교합니다. 유니버설 트랜스포머는 구글(Google)과 딥마인드(DeepMind) 연구진이 개발한 오리지널 트랜스포머의 한 변형(DehGhani et al., 2018)으로, 순환 루프(recurrent loop)를 활용하여 일반화(generalization) 능력을 향상시킵니다. 현대 LLM이 유니버설 트랜스포머를 기반으로 하지 않는 이유를 궁금해하실 수 있는데 (실제로 그렇지 않습니다!), 이는 순환성(recurrence)이 트랜스포머를 현대 하드웨어에서 훈련 가능하게 하는 병렬성(parallelism)을 저해하기 때문입니다. 모든 순환 단계는 이전 단계의 완료를 기다려야 하는데, 이는 일괄 처리 및 동시 행렬 연산을 위해 설계된 GPU/TPU 아키텍처와 근본적으로 상충됩니다. 이와 대조적으로, 표준 트랜스포머는 —다른 측면에서는 비효율적이라는 지적을 받지만— 오늘날의 연산 스택(compute stack)에 완벽하게 들어맞습니다. 수만 개의 가속기(accelerator)에 걸쳐 병렬화, 파이프라인화, 확장이 용이합니다. 따라서 AI 연구소들은 사실상 또 다른 절충안을 수용한 것입니다. 즉, 확장되지 않는 알고리즘적 효율성(유니버설 트랜스포머)보다 연산 규모에 따라 확장되는 알고리즘적 비효율성(바닐라 트랜스포머)을 선호한 것입니다. HRM이 언어 도메인에서 작동하더라도 AI 연구소들은 동일한 절충안을 택할 것입니다. 이러한 선택은 단기적인 상업적 이득과 확장성을 우선시하는 현 AI 산업의 현실을 반영합니다. 그러나 이러한 절충안이 장기적으로 AI 발전의 발목을 잡을 수 있다는 우려 또한 커지고 있습니다. 진정한 지능을 향한 길은 단순히 계산 능력을 확장하는 것만으로는 충분하지 않을 수 있습니다.

결론적으로 HRM은 유망한 개념이었습니다 (비록 ChatGPT, Gemini, Claude 등을 구동하는 LLM에 비해 이론과 실제 모두에서 훨씬 덜 성숙했지만). 그러나 저자들은 진정한 돌파구가 무엇인지 정확히 파악하지 못했습니다. 사피엔트 팀이 중단했던 지점에서 바통을 이어받은 것은 삼성 SAIL(Samsung SAIL)의 연구원인 알렉시아 졸리쾨르-마르티노(Alexia Jolicoeur-Martineau)였습니다. 싱가포르를 떠나 우리는 캐나다 몬트리올로 향합니다. 알렉시아는 그들의 접근 방식을 개선하고, 철저히 테스트하며, 약점을 파악하고, 불필요한 요소를 제거하고, 개선 루프를 분리한 후, 10월에 자신의 연구 결과를 "적을수록 많다: 소형 네트워크를 이용한 재귀적 추론(Less is More: Recursive Reasoning with Tiny Networks)"이라는 논문으로 발표했습니다. 이 논문은 HRM의 숨겨진 잠재력을 극대화하고, 더욱 간결하면서도 강력한 모델을 제시하며 AI 연구에 새로운 이정표를 세웠습니다.

**V. 루프 개선: 소형 재귀 모델(TRM)**

알렉시아의 핵심 통찰은 HRM이 ARC-AGI 및 기타 퍼즐에서 LLM을 능가할 수 있음에도 불구하고, 훨씬 작은 모델(2,700만 매개변수 대 수천억 개)이고 사전 훈련 없이 소규모 데이터셋(약 1,000개 예시)으로 훈련되었기 때문에 "최적의 상태가 아닐 수 있다"는 것이었습니다. 이는 실제로 ARC-AGI 팀이 독립적으로 밝혀낸 사실과 일치합니다 (알렉시아는 그들의 발견에서 영감을 받아 TRM을 개발했습니다). 그녀는 HRM이 가진 성공의 본질을 꿰뚫어보고, 불필요한 복잡성을 걷어내는 데 집중했습니다.

다음은 TRM 연구 논문의 주요 성과입니다. 우리는 소형 재귀 모델(Tiny Recursive Model, TRM)을 제안합니다. 이는 HRM보다 훨씬 뛰어난 일반화(generalization) 능력을 보여주는 훨씬 더 간결한 재귀적 추론(recursive reasoning) 방법론이며, 단 2개 계층으로 구성된 하나의 소형 네트워크만을 활용합니다. 불과 700만 개의 매개변수로, TRM은 ARC-AGI1에서 45%, ARC-AGI-2에서 8%의 테스트 정확도를 달성합니다. 이는 전체 매개변수의 0.01% 미만을 사용하면서도 대부분의 LLM(예: Deepseek R1, o3-mini, Gemini 2.5 Pro)을 능가하는 수치입니다. 이 결과는 AI 모델의 크기가 반드시 성능과 직결되는 것은 아니며, 효율적인 아키텍처와 학습 메커니즘이 더 중요하다는 강력한 증거를 제시합니다.

알렉시아의 접근 방식은 다음과 같습니다. 좋아 보여도 과도한 부분을 잘라내고(생체 모방 계층적 아키텍처, 즉 느린 계획자와 빠른 실행자가 사라졌고, 여기서는 무관하지만 알렉시아가 능숙하게 제거한 다른 기술적 세부 사항들), 약점을 개선하고(재귀적 프로세스를 단순화하여 TRM을 더 일반화 가능하게 만들고), 다른 사람들이 쉽게 기반을 구축하고 재현할 수 있는 방식으로 패키징하는 것입니다(700만 매개변수 대 GPT-4의 1조 8천억 매개변수를 비교해 보십시오). 이러한 '적을수록 많다(less is more)'는 원칙은 소프트웨어 공학의 오랜 지혜이기도 합니다. 불필요한 복잡성은 유지보수를 어렵게 하고, 확장성을 저해하며, 오류 발생 가능성을 높입니다. TRM은 AI 분야에서도 이러한 원칙이 유효함을 보여줍니다.

TRM의 근본적인 개념은 HRM과 매우 유사하지만, 훨씬 더 간결합니다. TRM은 불필요한 복잡한 장치 없이, 이전의 추론과 답변을 반복적으로 검토하고 개선함으로써 주어진 문제에 대한 해답을 탐색합니다. 다음은 이에 대한 고수준의 설명과 그림입니다.

**[그림 삽입: 소형 재귀 모델(Tiny Recursion Model) (출처)]**

각 단계마다 TRM은 질문-답변(x, y) 쌍과 현재 추론(z, 잠재)을 활용하여 다음 추론 단계를 갱신합니다(본질적으로 모델은 개선을 위해 이전에 추론하고 제시했던 답변을 기억해야 합니다). 이것이 바로 알렉시아가 "재귀적 추론(recursive reasoning)"이라고 명명한 방식입니다. (이 과정에서는 답변이 직접적으로 갱신되지 않는다는 점에 주목하십시오!) 이 메커니즘은 인간이 문제를 풀 때, 중간 단계를 검토하고, 필요한 경우 다시 생각하며, 최종 해결책에 도달하는 과정을 정확하게 모방합니다. 이는 단순히 한 번의 계산으로 결과를 도출하는 것이 아니라, 내부적인 성찰과 조정을 통해 지능을 발휘하는 방식입니다.

이는 인간의 사고방식과 유사하여 매우 직관적입니다. 수학 문제의 답변을 개선하려면, 어떤 답변이 틀렸는지, 그리고 어떤 추론 과정이 잘못된 답변으로 이어졌는지 파악해야 합니다. 이는 매우 간단한 원리입니다. 따라서 TRM은 추론을 여러 차례 업데이트한 후, 최종 추론 z를 이전 답변 y와 결합하여 새로운 답변을 제안합니다. 이 두 가지 요소가 전체 재귀 루프를 구성합니다: 1) 정해진 횟수만큼 재귀적 추론을 수행하고, 2) 답변을 갱신하여 제시합니다. 이러한 반복적인 자기 수정 능력은 모델이 새로운 상황이나 복잡한 문제에 직면했을 때, 유연하게 대처하고 학습할 수 있는 기반이 됩니다.

중요한 점은 HRM과 마찬가지로 TRM도 CoT를 사용하지 않는다는 것입니다. 추론은 LLM처럼 토큰 공간(기록된 형태)이 아닌 잠재 공간(내부적으로)에서 진행됩니다. CoT를 회피하려는 명백한 동기—즉, 소리 내어 생각하는 방식은 추론을 언어로 처리 가능한 영역으로 제한하며 비효율적이다—외에도, 알렉시아는 왕 연구팀과 마찬가지로 CoT가 계산적으로 매우 비싸고 고품질 데이터가 필수적이라고 지적합니다. (AI 연구소들이 코딩, 수학과 같은 난해한 문제에 대한 질문-답변 쌍 형태의 "추론 데이터"를 확보하기 위해 외부 계약자에게 얼마나 막대한 비용을 지불하는지 상상하기 어려울 것입니다. AI 분야의 거물인 안드레이 카르파티(Andrej Karpathy)는 이를 "끔찍한" 접근 방식이며, 모델들이 단지 "빨대로 감독의 지식을 빨아들이는 것"에 불과하다고 비판했습니다!) 잠재 공간에서의 추론은 외부적인 표현 없이 내부적으로 정보를 처리하고 통합하는 능력을 의미하며, 이는 진정한 인지 능력에 더 가깝다고 볼 수 있습니다.

그러나 TRM이 단순성과 일반화 가능성 측면에서 HRM보다 우월할지라도, ARC-AGI에서의 성공이 생성형 AI가 유용하게 활용될 수 있는 언어 및 코딩과 같은 다른 영역으로 확장될 수 있을지는 아직 불확실합니다 (TRM은 현재 생성형 모델은 아니지만, 향후 그렇게 발전될 가능성은 있습니다). 어떤 경우든, 이처럼 작고 간결한 모델이 ARC-AGI에서 달성한 점수(아래 참조)는 제게는 깊은 사색을 불러일으키고, AI 연구소들에게는 불안감을 안겨줍니다. 이는 마치 다윗이 골리앗을 이긴 것처럼, 거대한 자원과 인력을 가진 LLM 개발사들에게는 위협적인 메시지를 던지는 것입니다.

**[차트 삽입: "ARC-AGI 벤치마크 테스트 정확도(2회 시도)" (출처)]**

알렉시아는 아마도 가장 핵심적인 기술적 질문을 던집니다. 왜 훨씬 더 큰 언어 모델을 사용하는 것보다 깊은 재귀적 접근 방식이 더 효과적으로 작동하는가? 그녀의 주요 가설은 과적합(overfitting)입니다 (이는 모델이 훈련 데이터의 분포를 너무 과도하게 학습하여, 학습된 분포를 벗어나는 문제에 대한 일반화 능력을 저해하는 과도한 매개변수 수를 의미합니다). 그러나 논문에서도 언급했듯이, "이러한 설명을 뒷받침할 이론적 근거는 없습니다." 결국 AI 분야에서는 직관이 중요한 역할을 합니다! 과적합 문제는 LLM의 고질적인 약점 중 하나로 지적되어 왔습니다. 모델의 크기가 커질수록 더 많은 데이터를 암기할 수 있게 되지만, 이는 새로운 문제에 대한 유연한 적용 능력을 저해할 수 있습니다. TRM은 이러한 딜레마에 대한 해결책을 제시하는 셈입니다.

7월에 HRM을 둘러쌌던 초기 논란에 더하지 않기 위해, 저는 ARC-AGI 팀이 자체 반비공개 ARC-AGI 평가 테스트에서 알렉시아의 TRM 결과에 대한 확증을 줄 때까지 기다렸습니다. 그들은 방금 그렇게 했습니다. TRM이 모든 상업용 LLM을 능가하는 것은 아닙니다. 때로는 비용 효율성 측면에서도 그러합니다 (예: Grok 4 Thinking은 작업당 비용이 유사하며 ARC-AGI 2에서 16%를 달성하고, Claude Sonnet 4.5는 약 10배 저렴하면서도 14%를 달성합니다. 이는 LLM의 막대한 훈련 비용은 제외한 수치입니다). 하지만 TRM은 AI 커뮤니티가 주목해야 할 진정한 혁신입니다. 그 잠재력은 단순히 점수표의 숫자를 넘어섭니다. 이는 AI 개발의 방향성과 가치에 대한 근본적인 질문을 던지며, 더 효율적이고 지속 가능한 미래를 향한 가능성을 열어줍니다.

**[차트 삽입: ARC-AGI 1 리더보드 (출처)]**
**[차트 삽입: ARC-AGI 2 리더보드(척도가 35%까지임을 주목하라; LLM, TRM 또는 그 외 어떤 모델도 이를 해결하지 못했다!) (출처)]**

**VI. 실리콘밸리는 개방적이어야 할 때 집착하고 있다**

이 탐색을 마무리하기 전에, HRM과 TRM의 핵심적인 차이점으로 다시 돌아가고자 합니다. 그 안에 중요하지만 미묘한 교훈이 있다고 생각하기 때문입니다. 서두에 언급했던 “신이 되고 싶었던 뉴런(The neuron that wanted to be God)”을 읽으셨다면, 제가 결정화된 지능보다는 유동적인 지능(숄레(Chollet)의 용어를 빌리자면)을 구현하는 데 장애가 될 수 있는 오래된 가정과 단순화를 재검토하는 것을 선호한다는 점을 아실 것입니다. 그러나 저는 '뇌는 이렇게 작동한다'는 식의 생체 모방(bioinspiration)이 타당한 근거 없이 AI 아키텍처에 강제되어서는 안 된다고 봅니다. HRM은 모델이 현실에 맞춰 조정되기보다는, 현실을 미리 정해진 모델에 억지로 끼워 맞추려 한 사례입니다. 이러한 접근 방식은 과학적 탐구 정신에 위배될 뿐만 아니라, 실제 문제 해결 능력을 저해할 수 있습니다.

알렉시아는 이러한 이유로 HRM이 왜 말이 안 되는지 다음과 같이 썼습니다.

HRM의 저자들은 인공 신경망과는 거리가 먼 생물학적 주장을 바탕으로 서로 다른 계층에서 작동하는 두 개의 잠재 변수와 두 개의 네트워크를 정당화합니다. 그들은 심지어 HRM을 실제 쥐 뇌 실험에 맞추려고 시도합니다. 흥미롭기는 하지만, 이러한 종류의 설명은 HRM이 왜 그렇게 설계되었는지 파악하기를 엄청나게 어렵게 만듭니다. 논문에 제거 테이블(ablation table)이 부족하고, 생물학적 주장과 (완벽하게 적용되지 않는) 고정점 정리(fixed-point theorem)에 대한 과도한 의존을 고려할 때, HRM의 어떤 부분이 무엇을, 왜 돕는지 파악하기 어렵습니다.

그러나 그녀는 저의 더 넓은 관점에 직접적으로 동의합니다. 즉, 뇌를 단순히 복제하는 것(이는 거의 무의미합니다!)이 아니라, 뉴런부터 네트워크, 모델, 시스템, 그리고 제품에 이르는 전체 스펙트럼에서 표준 LLM과 수천억 달러에 달하는 막대한 투자(이는 실리콘밸리의 거대 기업만이 감당할 수 있는 규모입니다)에 대한 대안적 가능성들을 배제하지 않는 것입니다. 중요한 것은 단순히 생물학적 구조를 모방하는 것이 아니라, 생물학적 지능의 핵심 원리를 이해하고, 이를 AI 시스템에 효율적으로 적용하는 것입니다. 이러한 접근 방식은 AI 연구의 지평을 넓히고, 독점적인 기술 개발에서 벗어나 보다 개방적이고 협력적인 생태계를 조성하는 데 기여할 수 있습니다.

신경-상징적 접근 방식(Neuro-symbolic approach)은 당연히 저평가되는 경향이 있습니다. 과거에 상징주의에 대한 지나친 낙관이 두 차례의 AI 겨울과 이 분야의 장기적인 재정적 침체로 이어졌기 때문입니다. 그러나 기술자들이 추상화와 메타 사고에 능숙하다는 점을 고려할 때, 일반적인 과신보다는 특히 상징적 AI에 대한 두려움과 거부감이 크다는 것은 아이러니합니다. 현재의 LLM은 주로 연결주의(connectionism)에 기반하고 있지만, 진정한 일반 인공지능(AGI)을 위해서는 상징적 추론 능력과의 통합이 필수적이라는 주장이 점차 힘을 얻고 있습니다. TRM과 같은 모델의 성공은 이러한 하이브리드 접근 방식의 잠재력을 다시 한번 상기시켜 줍니다.

저는 수년 동안 ARC-AGI를 주시해왔습니다. LLM이 여전히 ARC-AGI를 이기지 못하는 이유가 이것 때문이라고 직감했기 때문입니다. AI 연구소와 주주들의 막대한 자금 덕분에 존재할 수 있는 이 산업은 성숙하기 위해 매우 미묘한 절충안을 받아들여야 했습니다. 위에서 썼듯이, 그들은 비실현 가능성의 위험에 대처하기 위해 비효율성과 고비용을 교환하고 있습니다. AI 연구소들이 LLM을 작동시킬 충분한 연산 능력과 데이터를 가지고 있기 때문에 지금은 작동할 수도 있습니다. 하지만 에너지 사용량과 필요한 투자 측면에서 이 분야가 몇 가지 선을 넘었다고 생각하지 않는다면—혹은 자신의 급여/정체성이 그것에 달려 있다면—눈이 멀었거나, 적어도 조심하고 의심해야 합니다. 이러한 과도한 투자와 자원 소모는 단순히 경제적인 문제를 넘어, 환경적 지속 가능성과 AI 기술의 사회적 형평성 문제까지 야기합니다. 실리콘밸리의 거대 기술 기업들이 주도하는 현재의 AI 개발 방식은 특정 소수에게만 혜택이 집중되고, 나머지 사회는 그 비용을 부담하는 불균형을 초래할 수 있습니다.

TRM은 기본적으로, 처음부터 실현 가능하도록 노력하지 않으면 불가능한 일은 없다는 것을 보여주는 증거이기 때문에 하나의 돌파구입니다. 그리고 우리는 확신에 의해서라기보다는 습관 때문에 (즉, LLM을) 당연하게 받아들입니다. 수많은 연구 방향이 실패해서가 아니라, 관심 부족, 자금 부족, 그리고 다른 곳으로 향하는 압력 때문에 포기되었습니다. 알렉시아는 트위터에 이와 유사한 내용을 게시했습니다. 그녀의 주요 기여는 기술적인 것이지만, 이러한 사회경제적 발언은 제 생각에 그만큼 중요합니다. 그녀의 발언은 AI 연구 커뮤니티 전체에 대한 경고이자, 새로운 가능성을 향한 촉구입니다. 현재의 AI 패러다임이 만들어낸 '익숙함의 감옥'에서 벗어나, 다양한 접근 방식과 소규모의 효율적인 솔루션들이 정당하게 평가받을 수 있는 환경을 조성해야 합니다.

어려운 작업을 해결하기 위해 어떤 대기업이 수백만 달러를 들여 훈련한 거대한 기반 모델에 의존해야 한다는 생각은 함정입니다. 현재, 새로운 방향을 고안하고 확장하기보다는 LLM을 활용하는 데 너무 많은 초점이 맞춰져 있습니다. 실제로 때로는 "적을수록 많다(less is more)"는 것이며, 우리는 "많을수록 너무 많다(more is too much)"는 지점에 도달하고 있을지도 모릅니다.

**VII. AI 산업에 대한 7가지 거대한 함의(TRM이 작동한다면)**

이 글은 이미 충분히 길지만, 이 모든 것이 무엇을 의미하는지 한눈에 보지 않고서는 HRM과 TRM 등을 읽는 것은 5,500개의 무작위 단어를 읽는 것만큼이나 실용적으로 유용합니다. 그래서 여기, AI 산업에 대한 TRM의 함의와 영향을 명확히 하기 위한 1,000개의 무작위가 아닌 단어가 있습니다. TRM의 성공이 단지 하나의 학술적 성과를 넘어, AI 산업 전반에 걸쳐 혁명적인 변화를 가져올 수 있는 잠재력을 지니고 있다는 점을 강조하고자 합니다. 만약 TRM과 같은 소형 재귀 모델이 언어 및 다른 복잡한 도메인으로 확장될 수 있다면, 다음과 같은 거대한 함의를 가지게 될 것입니다.

1.  **탈중앙화 및 민주화된 AI 개발 (Decentralized & Democratized AI Development)**
    TRM과 같이 적은 수의 매개변수, 소량의 훈련 데이터, 그리고 상대적으로 적은 컴퓨팅 자원을 필요로 하는 모델은 AI 개발의 장벽을 크게 낮출 수 있습니다. 현재 LLM 개발은 막대한 자금과 인프라를 가진 소수의 거대 기술 기업에 의해 독점되고 있습니다. 그러나 TRM은 개인 연구자, 스타트업, 심지어 일반 사용자도 혁신적인 AI 모델을 개발하고 배포할 수 있는 기회를 제공합니다. 이는 AI 기술의 접근성을 높이고, 다양하고 창의적인 아이디어가 발현될 수 있는 토대를 마련하여 AI 생태계를 더욱 풍요롭게 만들 것입니다. 더 이상 AI 혁신이 특정 기업의 연구실에만 갇혀 있지 않게 되는 것이죠.

2.  **지능형 에이전트의 새로운 패러다임 (New Paradigm for Intelligent Agents)**
    CoT 없이 잠재 공간에서 내부적으로 추론하는 TRM의 능력은 자율 에이전트(autonomous agents) 개발에 혁명적인 영향을 미 미칠 수 있습니다. 현재의 LLM 기반 에이전트는 '생각의 사슬'을 통해 외부적으로 추론 과정을 표현해야 하므로, 실시간 의사결정이나 복잡한 환경에서의 유연한 대응에 한계가 있습니다. 반면 TRM은 인간처럼 '내면적으로' 사고하고 반복적으로 개선하는 능력을 통해, 더욱 효율적이고 민첩하며 견고한 지능형 에이전트의 등장을 가능하게 할 것입니다. 이는 로봇 공학, 자율 주행, 복잡한 시스템 제어 등 다양한 분야에서 혁신을 촉진할 것입니다.

3.  **지속 가능한 AI (Sustainable AI)**
    LLM의 훈련과 운영은 막대한 에너지 소비와 탄소 배출을 야기하며, 이는 AI 기술의 환경적 지속 가능성에 대한 심각한 우려를 낳고 있습니다. TRM과 같은 소형 모델은 이러한 문제를 해결할 실질적인 대안을 제시합니다. 훨씬 적은 전력으로 훈련되고 운영될 수 있는 TRM은 AI 기술이 환경에 미치는 부담을 최소화하면서도 강력한 성능을 제공할 수 있음을 보여줍니다. 이는 AI 산업이 장기적으로 성장하기 위해 반드시 해결해야 할 과제이며, TRM은 그 해답의 중요한 부분이 될 수 있습니다.

4.  **AI 모델 평가의 재정립 (Re-evaluation of AI Model Evaluation)**
    TRM의 성공은 ARC-AGI와 같은 인간과 유사한 추론 능력을 평가하는 벤치마크의 중요성을 재조명합니다. 기존의 벤치마크들이 주로 언어 이해, 생성, 혹은 특정 작업의 정확도에 초점을 맞추었다면, ARC-AGI는 모델의 일반적인 지능과 추론 능력을 측정합니다. TRM이 ARC-AGI에서 LLM을 능가하는 성과를 보이면서, AI 모델의 '진정한 지능'을 평가하는 기준이 단순히 대규모 데이터 학습 능력을 넘어, 효율적인 추론 메커니즘과 일반화 능력에 있음을 입증했습니다. 이는 AI 연구 커뮤니티가 벤치마크 선택과 모델 평가 방식에 대한 심도 깊은 논의를 시작하게 만들 것입니다.

5.  **하드웨어 혁신의 촉진 (Catalyst for Hardware Innovation)**
    현재 AI 하드웨어는 주로 LLM의 병렬 연산에 최적화되어 있습니다. 그러나 TRM과 같은 재귀적 추론 모델이 부상하면서, 기존의 병렬화 효율성 외에 '순환 연산'이나 '피드백 루프'를 효율적으로 처리할 수 있는 새로운 하드웨어 아키텍처에 대한 수요가 증가할 것입니다. 이는 AI 칩 설계자들에게 새로운 도전 과제이자 기회를 제공하며, AI 하드웨어 생태계의 다양성과 혁신을 촉진할 수 있습니다. 특정 모델에 종속되지 않는 범용적인 AI 하드웨어 개발의 중요성이 더욱 커질 것입니다.

6.  **AI 연구 방향의 다양화 (Diversification of AI Research Directions)**
    지난 몇 년간 AI 연구는 LLM 중심의 '스케일링 법칙'에 지나치게 집중되어 왔습니다. TRM의 성공은 이러한 단일 패러다임에서 벗어나, 생체 모방, 신경-상징적 접근, 소규모 효율 모델 등 다양한 연구 경로의 중요성을 다시 한번 강조합니다. 이는 AI 연구 커뮤니티가 보다 개방적이고 탐구적인 자세로 미지의 영역을 탐험하도록 장려할 것입니다. 특정 기술에 대한 과도한 집착을 경계하고, 여러 가지 가능성을 열어두는 것이 장기적인 AI 발전에 필수적임을 일깨워줍니다.

7.  **"진실의 핵심" 재조명 (Re-examination of the "Kernel of Truth")**
    TRM은 AI의 '진실의 핵심'이 단순히 거대한 데이터셋과 복잡한 신경망에 있는 것이 아니라, 효율적인 학습 및 추론 메커니즘에 있을 수 있다는 가능성을 제시합니다. 이는 AI가 무엇이며, 어떻게 작동해야 하는지에 대한 근본적인 질문을 다시 던지게 만듭니다. 과연 지능은 양적인 확장으로 얻어지는 것인가, 아니면 질적인 메커니즘의 정교함에서 오는 것인가? TRM은 후자에 대한 강력한 증거를 제공하며, AI 연구자들이 지능의 본질에 대한 보다 심오한 이해를 추구하도록 영감을 줄 것입니다. 이는 AI의 미래가 단순히 '더 크게, 더 많이'가 아닌 '더 똑똑하게, 더 효율적으로' 나아갈 수 있음을 시사합니다.