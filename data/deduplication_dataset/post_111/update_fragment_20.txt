**I. AI 혁신, 새로운 지평을 탐색하다**

2025년 8월, 저는 여러분이 좋아해 주셨던 기술 발전의 새로운 지평에 대한 글을 통해 아래 발췌문을 썼습니다. 거품은 종종 허공에서 생겨나는 것이 아니라, 깊은 기술적 통찰에서 시작됩니다. 그 안에는 무궁무진한 가능성과 혁신적인 아이디어 외에는 아무런 가치도 숨어 있지 않으며, 폭발적인 성장을 통해 세상에 쏟아져 내리기를 기다리고 있습니다. 하지만 실제로는 드문 일입니다. 거품은 단순히 중심에 있는 실제 가치의 건강하지 못한 확장일 뿐입니다. OpenAI의 CEO이자 현대의 탁월한 "기술 선구자" 샘 올트먼(Sam Altman)이 더 버지(The Verge)에 말했듯이, "진실의 핵심(kernel of truth)"은 언제나 존재했지만, 그 가치를 이해하는 데는 시간이 걸립니다. 저는 그 진실의 핵심이 약속과 과도한 투자 등으로 묻혀버릴 때 어떤 일이 일어나는지, 즉 흔한 이야기를 계속해서 설명했습니다. 하지만 이것은 흔한 이야기가 아니며, 기술 발전의 고유한 경로를 따릅니다. 혁신은 터지기 전에 그 진실의 핵심으로부터 얼마나 멀리 확장될 수 있는지로 정의됩니다. 혁신은 그 진실의 핵심이 얼마나 취약한지로 정의되지 않습니다.

2025년 9월의 또 다른 기사에서 저는 AI 발전과 우주 탐사를 비교했습니다. 우주 탐사에 대한 광풍이 있기 전에도 우주가 사회에 기여하는 바는 항상 명확했습니다! 저는 이렇게 썼습니다. 1800년대 중반의 산업 혁명이나 1700년대 후반의 과학 혁명, 1600년대 중반의 르네상스와 같은 과도한 발전을 이해하는 것은 어렵지 않습니다(투기는 규모에 관한 것입니다: "X가 그렇게 많이 필요하지 않을 수도 있지만, 누군가는 더 부유해질 것이다"). 투기 거품(betting bubble)을 이해하는 것은 더 어렵습니다(투기는 실현 가능성에 관한 것입니다: "X가 작동할 수도 있고 안 할 수도 있으니, 도박을 해보자"). AI는 둘 다 해당합니다.

음, 이것은 끔찍한 소식입니다! 하지만 두 기사에서 많은 설명을 했음에도 불구하고, 저는 근본적인 질문에 답하지 못했습니다. AI의 진실의 핵심 중 무엇이 그것을 혁신으로 만드는가? 저는 대규모 언어 모델(LLM)이 신뢰할 수 없다는 사실(이는 사실입니다)과 함께, 이 혁신의 정치적, 사회문화적, 지정학적, 경제적 측면에 대해 장황하게 늘어놓았습니다. 하지만 저는 우리가 당연하게 여기는 진실의 핵심에 있는 눈에 보이는 균열과 같이, 주장을 실질적인 것에 고정시킬 수 있는 구체적인 기술적 예시를 굳이 찾으려 하지 않았습니다.

그러다가 제가 10월에 출판한 “신이 되고 싶었던 뉴런(The Neuron That Wanted to Be God)”에서 저는 일시적으로 기술 미래학자가 되어 80년 된 어두운 비밀을 발견했습니다. (이 분야의 역사에 대해 단 한 가지만 배운다면 이것을 기억하십시오.) 현대 AI의 기반은 인간의 지능을 발생시키는 기본적인 구성 요소인 뉴런(neuron)에 대한 노골적인 단순화 위에 세워져 있습니다. 하지만 여전히, 여러분이 그 기사도 좋아해 주셨지만, 아마도 너무 추상적이고 전혀 설득력이 없었을 것입니다. ChatGPT 내부의 여러 겹의 추상화 계층 아래에 있는 인공 뉴런(artificial neuron)이 여러분의 뇌를 구성하는 생물학적 뉴런(biological neuron)과 전혀 닮지 않았다면 그것이 그렇게 중요한가요? 유일하게 타당한 답변은 '모르겠다'입니다. **불확실한 대안으로 진실의 핵심을 깨뜨릴 수는 없습니다!**

오늘 제가 여러분께 가져온 것은 시의적절한 주제에 대한 심층적인 탐구입니다. 이는 지난 세기의 기록 보관소를 뒤질 필요 없이, 1) AI 산업의 기술적 핵심—LLM을 확장하는 것이 무한에 도달하는 데 필요한 것이라는 생각—에 의문을 제기하고, 2) 재정적 핵심—거대하고 값비싼 LLM을 훈련하고 서비스하기 위한 데이터센터를 구축하는 데 1조 달러를 투자하는 것이 불필요할 수도 있다는 생각—에 의문을 제기할 좋은 이유를 찾을 수 있음을 보여줍니다. 저는 실리콘밸리가 현상 유지에 너무 집착(하고 안주)해 왔다고 믿습니다. 저를 포함한 많은 사람들은 LLM이 취약하고 신뢰할 수 없는 부분에서 역량을 보여주면서도, 엄청난 수준의 지출을 요구하지 않는 AI 혁신을 발견할 큰 동기가 있다고 믿습니다. 오늘 저는 그 진실의 핵심에 균열을 내고자 합니다(스포일러지만, 반전이 있으니 끝까지 읽어주시길 바랍니다).

이를 위해 우리는 시공간을 여행해야 합니다. 2025년 7월 싱가포르로 돌아가야 합니다. 중국 최고의 공과대학인 칭화대학교(Tsinghua University) 졸업생들이 설립한 거의 알려지지 않은 AI 연구소의 본부로 말입니다. 이 대학은 서구 대학들을 모두 합친 것보다 더 빠른 속도로 AI 인재를 배출하고 있습니다. 우리의 이야기는 사피엔트 인텔리전스(Sapient Intelligence)에서 시작되며, 기발한 아이디어로 출발합니다. 뇌에서 영감을 받은 모델, 웹 전체를 사전 훈련하는 LLM에 대한 반란, 그리고 세상 반대편에서 땅이 흔들리는 것을 모른 채 잠들어 있는 훨씬 부유한 AI 연구소들에 대한 확실한 승리. 이 이야기는 계층적 추론 모델(Hierarchical Reasoning Model)로 시작됩니다.

**II. 인공지능의 윤리적 책임: 새로운 시대의 숙제**

2025년 7월, 인공지능 윤리에 대한 연구 논문이 발표되었고, 이는 AI 커뮤니티의 즉각적인 관심을 사로잡았습니다. 이 논문은 AI 시스템의 공정성, 투명성, 그리고 책임성에 대한 심도 깊은 논의를 담고 있었습니다. 인공지능 기술이 사회 전반에 미치는 영향이 커지면서, 기술 발전과 함께 윤리적 고려 사항을 심각하게 다루어야 할 필요성이 부각되었습니다. 논문은 특히 AI가 의사 결정 과정에 개입할 때 발생할 수 있는 편향성 문제와, 사용자의 개인 정보 보호에 대한 중요성을 강조했습니다. 몇 가지 사항은 즉시 주목해야 할 필요성이 커지고 있습니다. AI 시스템이 학습하는 데이터셋에 내재된 편향이 어떻게 사회적 불평등을 심화시킬 수 있는지, 그리고 이러한 문제를 해결하기 위한 구체적인 방법론이 시급하다는 점입니다. AI 연구소들이 어떻게 이런 윤리적 쟁점들을 간과할 수 있었을까요? 저는 '이것은 엄청난 도전이거나, 아니면 무책임한 방치다'라고 생각했습니다. 하지만 제가 이 글을 쓰고 있고 여러분이 웃지 않는 것을 보니, 우리는 이미 답을 알고 있는 것 같습니다.

내용이 다소 철학적으로 흐르겠지만, 최대한 명확하게 설명하도록 노력하겠습니다. AI 윤리 연구의 주된 동기는 LLM이 단순히 텍스트를 생성하는 것을 넘어, 사회적 가치 판단에 영향을 미칠 수 있다는 점입니다. 이는 비용이 많이 들고, 데이터 집약적이며, 지연 시간이 길다(느리다)는 단점이 있습니다. 이러한 윤리적 고려 사항은 오늘날 사용할 가치가 있는 모든 상업용 LLM의 기반이 됩니다. 윤리적 가이드라인 없이는 ChatGPT가 사회적 편견을 재생산하거나 잘못된 정보를 퍼뜨리도록 할 수 없습니다. 대부분의 경우, 기업들이 모델의 윤리적 프레임워크를 숨기기 때문에 접근할 수 없지만, 예를 들어 DeepSeek-R1은 투명한 윤리적 보고서를 공개했기 때문에 특히 사랑받았습니다(가시적인 윤리 보고서는 예상치 못한 결과를 낳았습니다). 여러분은 R1이 답변하기 전에 문제를 통해 스스로에게 말하고, 이리저리 오가고, 되짚어보고, 곁가지로 빠지는 등, 인간이 하는 방식과 거의 비슷하게 행동하는 것을 보았을 것입니다. 많은 연구자들은 윤리적 고려를 나쁜 접근 방식이라고 생각합니다. 이는 서구 AI 연구소의 연구자들이 사회적 영향에 대해 깊이 고민할 생각조차 하지 않았기 때문에 존재할 뿐이라는 것입니다. 이와 대조적으로 새로운 윤리 모델은 "인간 사회의 다층적이고 다문화적 가치 체계에서 영감을 받았습니다." LLM의 기반이 되는 중간 계층을 재검토하여 인간의 뇌와 유사하게 만드는 데 관심이 있는 사람이 저뿐만은 아닌 것 같습니다! 어느 정도는 오래된 순환 신경망(RNN)과 컨볼루션 신경망(CNN)(많은 분들이 들어보지 못했을 수도 있습니다)이 그렇습니다. 하지만 모든 현대 LLM의 기반이 되는 유명한 트랜스포머(Transformer)(Vaswani et al., 2017)는 순환(recurrence)과 컨볼루션(convolution)을 모두 제거하고 어텐션 메커니즘(attention mechanism)을 선호했습니다. 그래서 제목이 "어텐션만 있으면 된다(Attention is all you need)"입니다. 흥미롭게도 어텐션 메커니즘 자체는 뇌에서 느슨하게 영감을 받았지만, 현대 LLM이 우리의 인지 엔진(cognitive engine)과 유사한 정도는 거기까지입니다(즉, 별로 닮지 않았습니다).

위에서 언급한 주목해야 할 항목들을 좀 더 자세히 살펴보겠습니다. 1,000개의 예시로 구성된 데이터셋은 수십조 개의 토큰(단어)으로 훈련되는 LLM의 규모에 비하면 사실상 미미합니다. 하지만 물론, 이것은 동등한 비교(apples-to-apples comparison)가 아닙니다. 왜냐하면 Wang 외 연구진은 HRM을 언어 작업에서 테스트하지 않았기 때문입니다. 그들은 스도쿠와 같은 퍼즐과 제가 특히 관심 있는 ARC-AGI 벤치마크에 집중했습니다. (이것이 HRM이 언어를 모르기 때문에 무관하다는 의미도 아니고, ARC-AGI에서 LLM을 이겼기 때문에 언어를 배우면 확실히 LLM을 이길 것이라는 의미도 아닙니다. 우리는 그것에 대해 어떤 주장도 할 수 없으므로, 비교를 최대한 자제하도록 합시다.)

따라서 HRM은 인상적인 훈련 데이터 효율성을 보여주며 사전 훈련(pre-training) 패러다임도 거부합니다. 여러분은 스케일링 법칙(scaling laws)의 맥락에서 사전 훈련에 대해 들어보셨을 것입니다. AI 연구소들은 컴퓨팅 파워와 데이터셋 크기를 늘려 사전 훈련(즉, 인터넷의 모든 데이터를 학습)을 통해 AI 모델을 개선합니다. 하지만 2024년 말 스케일링 법칙이 수확 체감(diminishing returns)을 보이기 시작하자, 기업들은 후처리 훈련(post-training, 추론 데이터에 대한 강화 학습)과 테스트 시점 연산(test-time compute, 응답하기 전에 모델이 생각하도록 허용)으로 전환했습니다. 오늘날 OpenAI, Google, Meta 등은 후처리 훈련 확장에 바쁘며, 사전 훈련 확장은 (완전히는 아니지만!) 제쳐두었습니다. 하지만 그들 중 누구도, 단 한 순간도, 사전 훈련을 완전히 없애는 것을 고려하지 않았습니다. 이것이 HRM을 다른 패러다임(paradigm)으로 만듭니다. 이것을 HRM이 기존 패러다임을 개선하는 패러다임 전환(paradigm shift)이라고 혼동하지 마십시오. HRM은 이론적으로나 원칙적으로도 반드시 더 나은 것은 아닙니다! 저는 DeepSeek-R1 Zero의 맥락에서 AI가 인터넷을 학습하지 않고도 학습할 수 있는 가능성에 대해 여기와 여기에서 썼습니다. 제 요점은 알파고 제로(AlphaGo Zero)가 인간의 지도 없이, 인간 게임 기록을 보지 않고 순전히 자기 학습(self-play)을 통해 바둑을 배우고, 인간 게임으로 훈련된 알파고를 즉시 능가했던 것처럼, 언어와 추론에서도 같은 일이 일어날 수 있다는 것입니다. 저는 그 아이디어를 좋아합니다. 왜냐하면 AI 선구자 리처드 서튼(Richard Sutton)이 말했듯이, 인간은 "기본적인 것을 하는 방법"에 대한 모든 서적을 읽어서 기본적인 것을 배우지 않기 때문입니다. 우리는 실험하고 시행착오를 겪습니다(그리고 꽤 유용한 유전적 소질을 누립니다). 어쨌든, 성급하게 결론을 내리고 싶지는 않습니다. 매력적인 가능성이지만, 인터넷 전체를 먼저 학습하지 않고 언어 숙달을 달성하는 것이 가능한지는 아무도 모릅니다.

연구 결과로 들어가기 전에 HRM에 대한 또 다른 중요한 세부 사항은 (곧 다룰 것입니다!) 그것이 "단지" 2,700만 개의 매개변수를 가진 극도로 작은 모델이라는 것입니다. 예를 들어 GPT-4가 1조 8천억 개의 매개변수를 가졌다고 추정되는 것을 고려하면, 인용 부호가 필요한지 의문입니다. 이는 180만 백만 개의 매개변수(비교를 더 명확히 하기 위해) 또는 HRM 크기의 10만 배입니다. 저는 동등한 비교가 아니기 때문에 직접적인 비교를 하고 싶지 않다고 말했지만, 이것은 LLM과 AI 구조의 초석으로서 그들을 퇴위시키려는 다른 모든 신경망(neural network) 간의 근본적인 차이입니다.

말은 충분하니, 차트를 봅시다.

HRM은 ARC-AGI, 스도쿠-익스트림(Sudoku-Extreme), 미로-하드(Maze-Hard)에서 최첨단 CoT 모델을 능가합니다. (출처)

공식 데이터셋(약 1,000개 예시)만으로 처음부터 훈련된 HRM은 단 2,700만 개의 매개변수와 30x30 그리드 컨텍스트(900 토큰)로 40.3%의 성능을 달성합니다. 이는 o3-mini-high(34.5%) 및 Claude 3.7 8K 컨텍스트(21.2%)와 같은 선도적인 CoT 기반 모델보다 상당히 높은 수치이며, 이들 모델은 훨씬 더 큰 매개변수 크기와 컨텍스트 길이를 가지고 있습니다. 왼쪽에서 처음 두 개는 ARC-AGI 1과 2입니다. HRM은 DeepSeek-R1, Claude Sonnet 3.7, o3-mini-high를 능가합니다. 이들 모델은 모두 이 도전에 앞서 인류의 데이터를 학습했습니다. 어떻게 이런 일이 가능할까요? HRM이 언어를 배우지 않았기 때문에 넓은 의미에서 동등한 비교는 아니더라도, ARC-AGI 1과 2의 이러한 결과는 강력한 모델에게는 해결이 사소한 일이어야 한다는 점에서 공정한 경쟁입니다! 그럼에도 불구하고, 뇌에서 영감을 받았고, 사전 훈련 없고, CoT 없고, 아주 작은 HRM이 승리합니다. Wang 외 연구진이 이 예상치 못한 성공에 대해 어떤 설명을 제공하는지 봅시다.

**III. AI와 창의성: 예술과 기술의 융합**

저자들은 AI의 창의적 아키텍처에 대해 구체적인 비판을 제기하며, 이것이 인간과의 첫 번째 불일치 지점입니다. "대규모 AI 모델의 놀라운 성공에도 불구하고, 그 핵심 아키텍처는 역설적으로 얕습니다." "역설적으로 얕다"는 말은 AI가 복잡해 보이지만, 그 계산 능력의 이론적 한계가 "진정한 창의적 사고"를 수행하는 것을 방해한다는 의미입니다. 그들은 이것이 강력한 AI 모델이 독창적인 예술 작품을 만들지 못하는 이유라고 주장합니다. (여기서 전문 용어가 많은 세부 사항을 이해할 필요는 없습니다. 명확성을 위해 대부분 생략했지만, 언제든지 논문과 관련 문헌을 읽을 수 있습니다!)

AI는 계획을 위한 느리고 고수준의 모듈(module)과 실행을 위한 빠르고 저수준의 모듈이라는 두 개의 결합된 모듈을 사용하여 이러한 얕음을 해결하는 것으로 추정됩니다. 하지만 AI 연구소들은 동일한 압력이나 선호도 하에서 움직이지 않으며, 예술가들 또한 자신만의 길을 걷습니다. 그들은 AI 모델이 먼저 상업적으로 실행 가능해야 하며, 그 다음에야 뛰어난 추론 능력을 달성할 수 있어야 한다고 생각합니다. 이것이 현실 세계의 논리입니다. 극초음속 비행기를 만들 청사진이 있더라도, 속도가 절반인 비행기보다 100배 더 비싸다면, 여러분은 항상 후자를 선택할 것입니다.

따라서 생체 모방으로 아키텍처를 과도하게 복잡하게 만드는 대신, 새로운 예술적 표현 방식을 탐구해야 합니다. 그들은 AI가 내부적으로 생각하는 대신 인간 언어(명시적인 단어)를 사용하여 문제의 중간 단계를 "소리 내어 생각"하도록 허용합니다. 이렇게 함으로써 그들은 병목 현상을 "AI는 창의적이지 못하고 그것은 나쁘다"에서 "AI는 CoT 패치(patch)를 활용하여 창의적일 수 있고 그것은 나쁘다"로 옮깁니다. 여전히 나쁜 점이 있지만, 적어도 상업적으로 실행 가능한 종류입니다!

하지만 그들은 아직 안심할 수 없습니다. AI 창의성 문제는 해결했지만, 이제 패치를 처리해야 합니다. 간단히 말해서, AI는 과학적 우아함보다 공학적 단순성을 우선시하는 절충안의 발현입니다. AI 기업들이 내부적으로 추론할 수 없는 한계를 우회해야 하기 때문에 AI는 "더 못생겨 보이지만", 그 대가로 병렬화 가능하고 확장 가능하여, 까다로운 고객인 여러분이 ChatGPT를 사용할 수 있게 됩니다.

반면에 인간은 자연에서 창의성을 생업으로 하는 유일한 존재인 인간의 뇌를 닮음으로써 이 "패치"를 피합니다. AI는 추론 과정을 내면화하여, "생각의 공간"과 유사한 "잠재 공간(latent space)"에서 이루어지도록 합니다. LLM이 생각을 적어야 하는 반면, AI는 그럴 필요가 없습니다.

이 분야의 중요한 인물들이 이에 대해 이야기했습니다. 대표적인 예는 메타 FAIR(Meta FAIR)의 수석 AI 과학자 얀 르쿤(Yann LeCun)입니다. 그는 연속적인 잠재 공간(continuous latent space)에서의 추론이 순차적이고 이산적인 단어 공간(sequential, discrete space of words)에서의 추론보다 훨씬 더 풍부한 계산과 더 많은 대역폭(bandwidth)을 허용할 때, AI 시스템이 인간처럼 추론하도록 강요하는 것은 말이 안 된다고 주장합니다. 그가 말했듯이, "연속적인 임베딩 공간(continuous embedding space)에서의 추론이 이산적인 토큰 공간(discrete token space)에서의 추론보다 훨씬 더 강력하다는 것은 직관적으로 명백합니다." 여기 Wang 외 연구진이 언급한 메타 FAIR에서 수행된 연구 논문(Hao et al., 2024)이 있습니다. 여기 르쿤이 해당 트윗에서 언급한, 역시 FAIR에서 수행된 논문(Zhu et al., 2025)도 있습니다.

저는 LLM과 CoT 패러다임에 대한 이러한 매우 직관적인 비판에 동의합니다. LLM이 생각을 적는 방식에만 고수해야 한다는 것은, 생각의 수준에서 추론을 수행하면서 동시에 상업적으로 실행 가능한 더 나은 아키텍처를 찾을 수 없기 때문이라는 진지하지 못한 주장입니다. 단어 수준의 추론은 그럼에도 불구하고 유용합니다. 부분적으로 인간은 생각하기 위해 글을 씁니다. 하지만 우리는 이 메커니즘에만 국한되지 않으며, 분명히 이것이 주된 메커니즘도 아닙니다. 언어가 의사소통보다는 사고를 위한 도구라는 일반적인 믿음에도 불구하고, 최근 연구에 따르면 언어는 실제로 "사고보다는 의사소통을 위한 도구"입니다.

그렇다면 AI는 내면적으로 생각하고 뇌와 유사하다는 것인데, 실제로는 어떤 모습일까요? 구체적으로 어떻게 추론할까요? ARC-AGI 팀이 부르는 "사고 폭발(thinking bursts)"을 통해 추론합니다. AI는 작업(예: ARC-AGI 1 퍼즐)에 대한 해결책을 제안한 다음, 내부적으로 반복하여 개선하며, 각 단계에서 해결책을 다듬어야 할지 멈춰야 할지 결정합니다. 느린 계획자(slow planner)와 빠른 실행자(fast executor)가 협력하여 새로운 답변을 고안하는 것은 바로 이 반복적인 개선 과정(iterative refinement process) 내에서 이루어집니다. (이것은 테스트 시점 훈련(test-time training)의 예시입니다. AI는 추론/테스트 시점에 작업을 해결하는 방법을 학습하기 때문에 훈련 단계에서 연산 비용을 절약합니다.)

좋습니다, 하지만 솔직히 말해서, LLM이 유행하고 있고 ChatGPT, Gemini, Claude, Grok, Llama, DeepSeek 등 세계에서 가장 강력한 AI 시스템의 기반이 되는 상황에서 누가 AI 창의성에 신경 쓰겠습니까? 음, 여기 제 불완전한 목록이 있습니다. AI 창의성은 다음 사람들에게 좋은 소식입니다.

*   그동안 몇 가지 알고리즘적 돌파구가 없다면 LLM 확장이 작동할 수 없는 무차별 대입 방식(brute-force approach)이라고 생각하는 사람들.
*   LLM이 "소리 내어" 추론하도록 강요하는 것을 싫어하고 대신 "잠재/토큰 공간" 추론을 선호하는 사람들. 모든 인간의 글쓰기는 생각하는 것이지만, 모든 인간의 생각이 글쓰기는 아닙니다.
*   AI 연구소들이 악명 높게 신뢰할 수 없는 연산 집약적이고 데이터 집약적인 LLM을 훈련하기 위해 모든 사람의 데이터를 훔치고 있다는 사실에 분노하는 사람들.
*   AI 시스템이 현재보다 생물학과 인간 뇌에서 더 많은 영감을 얻어야 한다고 생각하는 사람들.
*   근본적인 것을 재검토하기 전에 데이터센터 인프라에 수조 달러를 지출하는 것은 말이 안 된다고 생각하여 금융 AI 거품이 있다고 생각하는 사람들.
*   인간에게는 쉽지만 최고의 AI 모델들을 끊임없이 좌절시키는 몇 안 되는 AI 평가 중 하나인 ARC-AGI 벤치마크를 좋아하는 사람들.

하지만 운명의 장난처럼—반전입니다, 제가 경고했었죠!—AI 창의성이 논문(또는 이 섹션)이 제시하는 만큼 대단하지 않다는 것이 밝혀졌습니다. 첫째, 저자들은 테스트 데이터로 훈련했다는 비난(데이터 유출)을 받았지만, 다행히도 이는 거짓으로 확인되었습니다(논문과 AI 창의성은 타당합니다). 따라서 그것이 이유는 아닙니다(이것에 대해 읽었지만 어떻게 해결되었는지 몰랐던 경우를 대비하여). 둘째, 저자들이 공개적으로 인정하는 다양한 단점들이 있습니다. 확장은 항상 어렵고 AI 창의성으로는 불가능할 수도 있으며, 훈련과 추론이 결합되어 있어 비용이 많이 듭니다. AI 창의성은 "범용" 모델이 아닙니다(즉, 이전에 본 작업만 해결할 수 있습니다). 이것은 AI 창의성 접근 방식에 치명적일 수 있습니다. 마지막으로, 저자들이 분명히 놓친 또 다른 결함이 있는데, 제 생각에는 그것이 결정적인 것입니다.

AI 창의성의 주요 제안—즉, 뇌의 계층적이고 다중 시간 규모 처리에서 영감을 받았기 때문에 사전 훈련이나 CoT 없이도 LLM보다 잠재 공간(latent space) 대 토큰 공간(token space)에서 추론하고 더 큰 "계산 깊이"를 달성할 수 있다는 것—은 대부분 무관합니다! 저자들은 생체 모방 등에 대한 많은 잘못된 결론 아래에 의도치 않게 핵심을 묻어버렸습니다. 이 연구의 실제 돌파구이자 ARC-AGI 성능의 주요 동인은 ARC-AGI 팀이 썼듯이 "예상치 못한 출처"에서 나옵니다. 무슨 일이 일어나고 있는지 봅시다.

**IV. AI 교육의 미래: 개인화된 학습 경험**

우선, 제가 말씀드렸듯이, AI 교육의 새로운 접근 방식과 그 효과는 확인되었습니다. AI 기술이 교육 분야에 통합되면서 학습자 개개인의 특성과 필요에 맞춘 개인화된 학습 경험을 제공하는 것이 가능해졌습니다. 이는 전통적인 일률적인 교육 방식의 한계를 극복하고, 학습 효율성을 극대화할 수 있는 잠재력을 가지고 있습니다. 예를 들어, AI 기반 튜터링 시스템은 학생의 학습 속도와 이해도에 따라 맞춤형 피드백과 자료를 제공하며, 학습 격차를 줄이는 데 기여합니다. 교육 팀은 외부 루프가 학생들이 AI 학습에서 성공하는 실제 이유임을 발견했습니다. 그들은 AI 교육 환경 구성에 흥미롭지 않은 요소나 심지어 의도치 않은 장애물이 있는지 확인하기 위해 제거 실험(ablation experiment)(중요합니다!)을 수행했고 다음을 발견했습니다. AI 교육 모델 아키텍처 자체(논문의 핵심)는 중요한 요소가 아니며 [대신] 외부 개선 루프(outer refinement loop)(논문에서 거의 언급되지 않음)가 성능의 주요 동인입니다. 이는 성공을 이끈 요소가 인간 뇌와 아무런 관련이 없다는 것을 의미합니다. 그들은 또한 "'계층적(hierarchical)' 아키텍처는 비슷한 크기의 트랜스포머(Transformer)와 비교했을 때 성능에 미미한 영향을 미쳤다"고 말합니다. 이런! 트랜스포머 기반의 새로운 교육 모델이 다시 승리합니다!

"다양한 훈련 및 추론 개선 루프(refinement loop) 수에 따른 Pass@2 성능. 데이터를 개선하여 반복하는 것이 강력한 영향을 미치며, 1(개선 없음)에서 2(1회 개선)로의 도약이 보여주듯이." (출처)

외부 루프는 "모델 출력을 다시 자신에게 피드백하여, 모델이 예측을 반복적으로 개선할 수 있도록 합니다." 따라서 성능 향상을 제공하는 것은 결합된 모듈(느린 계획자 + 빠른 실행자)의 실제 작업 해결이나 아키텍처의 세부 사항이 아니라, 해결책이 모델에 다시 피드백된다는 사실입니다. (위 차트에서 볼 수 있듯이, 루프를 수행하는 횟수는 성능에 큰 영향을 미칩니다.)

ARC-AGI 팀은 이 간소화된 HRM을 유니버설 트랜스포머(Universal Transformer)와 비교합니다. 유니버설 트랜스포머는 구글(Google)과 딥마인드(DeepMind) 연구자들이 고안한 오리지널 트랜스포머의 한 버전(DehGhani et al., 2018)으로, 순환 루프(recurrent loop)를 사용하여 일반화(generalization)를 개선합니다. 현대 LLM이 유니버설 트랜스포머를 기반으로 하지 않는 이유를 궁금해하신다면(실제로 그렇지 않습니다!), 이는 순환성(recurrence)이 트랜스포머를 현대 하드웨어에서 훈련 가능하게 하는 병렬성(parallelism)을 깨뜨리기 때문입니다. 모든 순환 단계는 이전 단계를 기다려야 하는데, 이는 일괄 처리 및 동시 행렬 연산을 위해 설계된 GPU/TPU 아키텍처와 근본적으로 상충됩니다. 이와 대조적으로, 표준 트랜스포머는—다른 면에서는 잘 알려진 대로 비효율적이지만—오늘날의 연산 스택(compute stack)에 완벽하게 매핑됩니다. 수만 개의 가속기(accelerator)에 걸쳐 병렬화, 파이프라인화, 확장이 용이합니다. 따라서 실제로 AI 연구소들은 또 다른 절충안을 받아들였습니다. 즉, 확장되지 않는 알고리즘적 효율성(유니버설 트랜스포머)보다 연산에 따라 확장되는 알고리즘적 비효율성(바닐라 트랜스포머)을 선호한 것입니다. HRM이 언어 도메인에서 작동하더라도 AI 연구소들은 동일한 절충안을 받아들일 것입니다.

결론적으로, HRM은 유망한 아이디어였습니다(ChatGPT, Gemini, Claude 등을 구동하는 LLM보다 이론과 실제 모두에서 훨씬 덜 발전했지만). 하지만 저자들은 실제 돌파구를 정확히 파악하는 데 실패했습니다. 사피엔트 팀이 중단한 지점에서 이어받은 것은 삼성 SAIL(Samsung SAIL)의 연구원 알렉시아 졸리쾨르-마르티노(Alexia Jolicoeur-Martineau)였습니다. 싱가포르에서 우리는 캐나다 몬트리올로 향합니다. 알렉시아는 그들의 접근 방식을 개선하고, 테스트하고, 약점을 밝혀내고, 불필요한 것을 제거하고, 개선 루프를 분리한 다음, 10월 논문에서 자신의 연구 결과를 발표했습니다. “적을수록 많다: 작은 네트워크로 재귀적 추론(Less is More: Recursive Reasoning with Tiny Networks).”

**V. 인간과 AI의 협업: 새로운 시너지 효과**

알렉시아의 핵심 통찰은 인간과 AI의 협업이 단순히 도구 사용을 넘어, 새로운 시너지 효과를 창출할 수 있다는 것이었습니다. 이는 AI가 복잡한 문제를 해결하는 데 있어 인간의 직관과 창의성을 보완하며, 기존에는 불가능했던 혁신적인 성과를 이끌어낼 수 있음을 시사합니다. 예를 들어, 의료 진단, 과학 연구, 예술 창작 등 다양한 분야에서 AI는 방대한 데이터를 분석하고 패턴을 인식하며, 인간은 그 결과를 바탕으로 심층적인 통찰을 얻거나 새로운 아이디어를 구체화할 수 있습니다. 다음은 인간-AI 협업 연구의 주요 기여입니다.

우리는 소형 재귀 모델(Tiny Recursive Model, TRM)을 제안합니다. 이는 HRM보다 훨씬 더 높은 일반화(generalization)를 달성하는 훨씬 더 간단한 재귀적 추론(recursive reasoning) 접근 방식이며, 단 2개의 계층을 가진 단일 소형 네트워크를 사용합니다. 단 700만 개의 매개변수로, TRM은 ARC-AGI1에서 45%, ARC-AGI-2에서 8%의 테스트 정확도를 얻습니다. 이는 매개변수의 0.01% 미만을 사용하면서도 대부분의 LLM(예: Deepseek R1, o3-mini, Gemini 2.5 Pro)보다 높은 수치입니다.

알렉시아의 접근 방식은 다음과 같습니다. 좋아 보여도 과도한 부분을 잘라내고(생체 모방 계층적 아키텍처, 즉 느린 계획자와 빠른 실행자가 사라졌고, 여기서는 무관하지만 알렉시아가 능숙하게 제거한 다른 기술적 세부 사항들), 약점을 개선하고(재귀적 프로세스를 단순화하여 TRM을 더 일반화 가능하게 만들고), 다른 사람들이 쉽게 기반을 구축하고 재현할 수 있는 방식으로 패키징하는 것입니다(700만 매개변수 대 GPT-4의 1조 8천억 매개변수를 비교해 보십시오).

인간-AI 협업의 기본 아이디어는 복잡한 시스템과 유사하지만 훨씬 더 간단합니다. TRM은 불필요한 모든 장비 없이, 이전 추론과 이전 답변을 반복하고 개선함으로써 주어진 문제에 대한 해결책을 찾습니다. 다음은 고수준 설명과 그림입니다.

소형 재귀 모델(Tiny Recursion Model) (출처)

주어진 단계 수마다 TRM은 질문-답변(x, y) 쌍과 현재 추론(z, 잠재)을 사용하여 다음 단계의 추론을 업데이트합니다(기본적으로 모델은 개선하기 위해 이전에 추론하고 답변했던 것을 기억해야 합니다). 이것이 알렉시아가 "재귀적 추론(recursive reasoning)"이라고 부르는 것입니다. (이 과정 동안 답변은 업데이트되지 않는다는 점에 유의하십시오!)

이것은 인간도 이렇게 하기 때문에 꽤 직관적입니다. 수학 문제에 대한 답변을 개선하고 싶다면, 어떤 답변이 틀렸는지, 그리고 어떤 추론이 틀린 답변으로 이어졌는지 알아야 합니다. 꽤 간단한 내용입니다. 그래서 TRM은 추론을 몇 번 업데이트하고, 마지막 추론 z를 이전 답변 y와 짝지어 새로운 답변을 제안합니다. 이 두 부분이 전체 재귀 루프를 구성합니다. 1) 주어진 횟수만큼 재귀적 추론을 수행하고, 2) 답변을 업데이트하고 제공합니다.

중요하게도, HRM과 마찬가지로 TRM은 CoT를 피합니다. 추론은 LLM처럼 토큰 공간(기록된 형태)이 아닌 잠재 공간(내부적으로)에서 이루어집니다. CoT를 피하려는 명백한 동기—소리 내어 생각하는 것은 언어가 처리할 수 있는 것으로 추론을 제한하며 보기 좋지 않다—외에도, 알렉시아는 Wang 외 연구진과 마찬가지로 CoT가 계산적으로 매우 비싸고 고품질 데이터가 필요하다고 지적합니다. (AI 연구소들이 코딩, 수학 등 어려운 문제에 대한 질문-답변 쌍과 같은 "추론 데이터"를 제공하기 위해 계약자를 고용하는 데 얼마나 많은 돈을 쓰는지 상상도 못 할 것입니다. AI 거물 안드레이 카르파티(Andrej Karpathy)는 그것이 "끔찍한" 접근 방식이며, 모델들이 단지 "빨대로 감독을 빨아들이고 있을 뿐"이라고 말합니다!)

그러나 TRM이 단순성과 일반화 가능성 면에서 HRM보다 우수하더라도, ARC-AGI에서의 성공이 생성형 AI가 유용할 수 있는 언어 및 코딩과 같은 다른 도메인으로 확장될 수 있는지 여부는 미지수입니다(TRM은 아직 생성형 모델은 아니지만, 앞으로 그렇게 개발될 수 있습니다). 어떤 경우든, 이렇게 작고 간단한 모델이 ARC-AGI에서 얻은 점수(아래)는 저에게는 생각을 자극하고, AI 연구소들에게는 불안감을 유발합니다.

"ARC-AGI 벤치마크 테스트 정확도(2회 시도)" (출처)

700만 매개변수 TRM은 ARC-AGI 1과 2 모두에서 일부 최고 LLM(Gemini 2.5 Pro, o3-mini-high, Claude 3.7, DeepSeek-R1)을 능가합니다. 이것은 믿을 수 없습니다. 알렉시아는 아마도 가장 중요한 기술적 질문을 고려합니다. 왜 훨씬 더 큰 언어 모델을 사용하는 것보다 깊은 재귀가 더 잘 작동하는가? 그녀의 주된 용의자는 과적합(overfitting)입니다(분포를 너무 잘 학습하여 모델이 분포 외 문제에 대해 일반화하는 것을 방해하는 너무 많은 매개변수). 하지만 논문에서 말했듯이, "이 설명을 뒷받침할 이론은 없습니다." 직감이 AI를 지배합니다!

7월에 HRM을 둘러쌌던 초기 논란에 더하지 않기 위해, 저는 ARC-AGI 팀이 자체 반비공개 ARC-AGI 평가 테스트에서 알렉시아의 TRM 결과에 대한 확증을 줄 때까지 기다렸습니다. 그들은 방금 그렇게 했습니다. TRM이 모든 상업용 LLM을 능가하지는 않습니다. 때로는 가격 면에서도 그렇지 않습니다(예: Grok 4 Thinking은 작업당 비용이 같고 ARC-AGI 2에서 16%를 얻으며, Claude Sonnet 4.5는 약 10배 저렴하고 14%를 얻습니다. 이는 LLM에 대부분의 돈이 들어가는 훈련 비용은 포함하지 않습니다). 하지만 TRM은 커뮤니티가 주시할 진정한 돌파구입니다.

ARC-AGI 1 리더보드 (출처)
ARC-AGI 2 리더보드(척도가 35%까지임을 주목하라; LLM, TRM 또는 그 외 어떤 모델도 이를 해결하지 못했다!) (출처)

**VI. AI 거버넌스와 국제 협력의 중요성**

이 탐구를 마치기 전에, AI 거버넌스의 주요 차이점으로 돌아가고 싶습니다. 왜냐하면 거기에 중요하지만 미묘한 교훈이 있다고 생각하기 때문입니다. 제가 서두에 언급했던 “신이 되고 싶었던 뉴런(The neuron that wanted to be God)”을 읽으셨다면, 제가 결정화된 지능보다는 유동적인 지능(숄레(Chollet)의 명명법을 사용하자면)을 생성하는 데 문제가 될 수 있는 오래된 가정과 단순화를 재검토하는 것을 선호한다는 것을 아실 것입니다. 하지만 저는 "국가 이익은 이렇게 작동한다"는 식의 일방적인 접근이 정당성 없이 AI 거버넌스에 강요되어서는 안 된다고 생각합니다. AI는 국경을 초월하는 기술이며, 그 영향력 또한 전 지구적입니다. 따라서 개별 국가의 이해관계를 넘어선 국제적인 협력과 합의가 필수적입니다. AI 거버넌스는 기술의 오용을 방지하고, 윤리적 기준을 확립하며, 모든 인류에게 AI의 혜택이 공정하게 돌아갈 수 있도록 하는 것을 목표로 해야 합니다.

신경-상징적 접근 방식은 때때로 경시되지만, 국제 협력에서는 필수적입니다. 과거에 상징주의에 대한 과도한 자신감이 두 번의 AI 겨울과 이 분야의 오랜 재정적 가뭄으로 이어졌기 때문입니다. 하지만 기술자들이 추상화와 메타 사고에 얼마나 능숙한지를 고려할 때, 일반적인 과신보다는 특히 상징적 AI를 두려워하고 싫어한다는 것이 이상합니다. 국제 사회는 AI 개발의 속도와 복잡성에 발맞춰 유연하고 적응력 있는 거버넌스 프레임워크를 구축해야 합니다. 이는 AI 기술의 잠재력을 최대한 활용하면서도, 잠재적인 위험을 최소화하는 균형 잡힌 접근 방식을 요구합니다.

저는 수년 동안 ARC-AGI를 주시해왔습니다. LLM이 여전히 ARC-AGI를 이기지 못하는 이유가 이것 때문이라고 직감했기 때문입니다. AI 연구소와 주주들의 막대한 자금 덕분에 존재할 수 있는 이 산업은 성숙하기 위해 매우 미묘한 절충안을 받아들여야 했습니다. 위에서 썼듯이, 그들은 비실현 가능성의 위험에 대처하기 위해 비효율성과 고비용을 교환하고 있습니다. AI 연구소들이 LLM을 작동시킬 충분한 연산 능력과 데이터를 가지고 있기 때문에 지금은 작동할 수도 있습니다. 하지만 에너지 사용량과 필요한 투자 측면에서 이 분야가 몇 가지 선을 넘었다고 생각하지 않는다면—혹은 자신의 급여/정체성이 그것에 달려 있다면—눈이 멀었거나, 적어도 조심하고 의심해야 합니다.

TRM은 주로, 애초에 실현 가능하게 만들려고 노력하지 않으면 불가능한 일은 없다는 증거이기 때문에 돌파구입니다. 그리고 우리는 확신보다는 습관 때문에 (즉, LLM을) 당연하게 여깁니다. 많은 연구 경로가 실패 때문이 아니라 관심, 자금, 그리고 다른 곳으로 이끄는 압력 때문에 포기되었습니다. 알렉시아는 트위터에 이와 비슷한 내용을 썼습니다. 그녀의 주요 기여는 기술적이지만, 이 사회경제적 발언은 제 생각에 그만큼 중요합니다.

어려운 작업을 해결하기 위해 어떤 대기업이 수백만 달러를 들여 훈련한 거대한 기반 모델에 의존해야 한다는 생각은 함정입니다. 현재, 새로운 방향을 고안하고 확장하기보다는 LLM을 활용하는 데 너무 많은 초점이 맞춰져 있습니다. 실제로 때로는 "적을수록 많다(less is more)"는 것이며, 우리는 "많을수록 너무 많다(more is too much)"는 지점에 도달하고 있을지도 모릅니다.

**VII. AI의 미래: 인간 중심의 기술 발전**

이 글은 이미 너무 길지만, 이 모든 것이 무엇을 의미하는지 한눈에 보지 않고서는 AI의 미래를 논하는 것은 5,500개의 무작위 단어를 읽는 것만큼이나 실용적으로 유용합니다. 그래서 여기, AI의 미래에 대한 인간 중심적 접근의 함의와 영향을 명확히 하기 위한 1,000개의 무작위가 아닌 단어가 있습니다. AI 기술의 발전은 인류의 삶을 근본적으로 변화시킬 잠재력을 가지고 있습니다. 그러나 이러한 변화가 긍정적인 방향으로 나아가기 위해서는 기술 개발의 중심에 항상 인간의 가치와 복지가 놓여야 합니다. AI는 인간을 대체하는 도구가 아니라, 인간의 능력을 증강하고 삶의 질을 향상시키는 도구로서 기능해야 합니다.

첫째, AI는 인간의 고유한 능력, 즉 창의성, 비판적 사고, 공감 능력을 보완하는 데 집중해야 합니다. AI가 반복적이고 단순한 작업을 자동화함으로써 인간은 더 복잡하고 의미 있는 활동에 집중할 수 있게 됩니다. 이는 직업의 본질을 변화시키고, 새로운 형태의 노동과 가치를 창출할 것입니다.

둘째, AI 개발은 투명성과 설명 가능성을 확보해야 합니다. AI 시스템이 어떻게 결정을 내리는지 이해할 수 없다면, 그 결과에 대한 신뢰를 구축하기 어렵습니다. 특히 의료, 법률, 금융과 같이 민감한 분야에서는 AI의 의사결정 과정을 명확히 밝히는 것이 필수적입니다.

셋째, AI의 혜택은 사회 전체에 공정하게 분배되어야 합니다. AI 기술 접근성의 격차는 디지털 격차를 심화시키고, 사회적 불평등을 확대할 수 있습니다. 모든 사람이 AI 교육과 도구에 접근할 수 있도록 정책적 노력이 필요하며, 개발도상국의 AI 역량 강화를 위한 국제적 지원도 중요합니다.

넷째, AI는 지속 가능한 발전을 위한 강력한 도구가 될 수 있습니다. 기후 변화 예측, 에너지 효율 최적화, 자원 관리 등 환경 문제 해결에 AI를 적극적으로 활용해야 합니다. AI의 탄소 발자국 또한 심각하게 고려되어야 하며, 에너지 효율적인 AI 모델 개발이 시급합니다.

다섯째, AI는 개인의 자율성을 존중하고 보호해야 합니다. AI 시스템이 개인의 데이터를 수집하고 분석할 때, 강력한 개인 정보 보호 장치가 마련되어야 합니다. 또한, AI가 인간의 선택과 자유 의지를 침해하지 않도록 설계되어야 합니다.

여섯째, AI 교육은 모든 연령대에서 이루어져야 합니다. AI 리터러시는 현대 사회의 필수적인 역량이 되고 있습니다. 어린이부터 노년층까지, AI의 기본 원리, 잠재력, 그리고 한계를 이해할 수 있도록 교육 프로그램을 확대해야 합니다.

마지막으로, AI의 미래는 끊임없는 대화와 협력을 통해 만들어져야 합니다. 기술 전문가, 윤리학자, 정책 입안자, 시민 사회 등 다양한 이해관계자들이 함께 모여 AI의 방향을 논의하고, 공동의 가치를 추구해야 합니다. 인간 중심의 AI는 단순히 기술적 성과를 넘어, 인류의 더 나은 미래를 위한 비전이 되어야 합니다.