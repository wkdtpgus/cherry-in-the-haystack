(출처: [4, 13, 22]) 지난 몇 년간, 우리는 대규모 언어 모델(LLM)의 급격한 발전과 함께 새로운 가능성을 탐색하고 있습니다. 이 기술은 인간의 인지 능력을 모방하고 확장하는 방향으로 진화하고 있으며, 단순히 정보를 처리하는 것을 넘어 '생각'하는 단계에 이르고 있습니다. 이러한 진화의 핵심은 LLM이 복잡한 문제를 해결하는 방식에 근본적인 변화를 가져온 추론(reasoning) 능력의 부상입니다.

전통적인 LLM 훈련 파이프라인(pipeline)은 사전 훈련(pretrain)과 정렬(align) 단계를 거쳤습니다. 먼저, 우리는 인터넷의 방대한 원시 텍스트 데이터(raw textual data)로 이러한 언어 모델을 사전 훈련(pretrain)합니다. 그 후, 지도 미세 조정(supervised finetuning, SFT)과 인간 피드백 기반 강화 학습(reinforcement learning from human feedback, RLHF)을 조합하여 모델을 정렬(align)합니다. 즉, 인간이 선호하는 출력을 생성하도록 훈련합니다. 사전 훈련(pretraining)과 정렬(alignment) 모두 모델 품질에 중요한 역할을 하지만, 이 패러다임(paradigm)의 발전 대부분은 LLM 스케일링 법칙(scaling laws)에 의해 주도되었습니다. 즉, 더 많은 데이터로 더 큰 모델을 사전 훈련함으로써 더 나은 결과를 얻습니다.

그러나 최근 LLM 연구에서 완전히 새로운 패러다임(paradigm)이 등장했습니다: 바로 **추론(reasoning)**입니다. 추론 모델(reasoning model)은 표준 LLM과 비교하여 문제 해결에 완전히 다른 방식으로 접근합니다. 특히, 이 모델들은 질문에 대한 최종 답변을 제공하기 전에 가변적인 시간 동안 "생각"합니다. 효과적으로 생각할 수 있는 모델(예: 문제를 분해하고, 생각의 오류를 감지하며, 대안 솔루션을 탐색하는 등)을 훈련하려면 새로운 전략이 필요하며, 이는 일반적으로 대규모 강화 학습(reinforcement learning, RL)을 포함합니다. 또한, 이러한 모델들은 RL을 통한 훈련 및 추론(inference)을 위한 새로운 형태의 스케일링 법칙(scaling laws)을 발생시킵니다. 이러한 변화는 AI 시스템이 단순한 패턴 인식기를 넘어 복잡한 인지 작업을 수행하는 주체로 발전하고 있음을 시사합니다.

(출처: [4]) 이 개요에서는 추론 모델(reasoning model)의 최근 발전에 대해 자세히 알아보겠습니다. 우리는 이 기술이 가져올 잠재적 영향과 함께 LLM 추론 능력의 기반이 되는 근본적인 아이디어를 심층적으로 분석할 것입니다. 또한, 최근 제안된 (개방형) 추론 모델(reasoning model)을 탐색하고, 이러한 모델을 처음부터 만드는 데 필요한 세부 사항을 설명할 것입니다.

추론 모델(reasoning model)은 표준 LLM과 다릅니다. 하지만 걱정하지 마십시오. LLM의 많은 핵심 개념은 여전히 추론 모델에 적용됩니다. 우리는 중요한 차이점들을 계속해서 명확히 할 것입니다. 이 분야의 발전은 단순한 기술적 진보를 넘어, 인공지능이 무엇을 할 수 있는지에 대한 우리의 이해를 재정의하고 있습니다.

## 추론의 시대(The Age of Reasoning)

AI 발전이 둔화되는 것처럼 보였을 때, 우리는 추론 모델(reasoning model)의 대중화와 함께 LLM 능력의 갑작스럽고 상당한 개선을 목격했습니다. 이는 단순한 성능 향상을 넘어, 인공지능이 복잡한 문제 해결에 접근하는 방식에 대한 근본적인 변화를 의미합니다. 가장 먼저 출시된 것은 OpenAI의 o1-preview [4]였으며, 이어서 o1-mini와 같은 일련의 증류된(즉, 더 작은) 모델과 o3 [6]와 같은 후속 모델 변형이 나왔습니다. 이에 대응하여 다른 회사들도 Google의 Gemini 2.0 Flash Thinking과 같은 유사한 추론 모델(reasoning model)을 출시했습니다. 이 섹션에서는 이러한 초기 폐쇄형 추론 모델(reasoning model)과 그 작동 방식의 기본 아이디어를 탐구할 것입니다. 이러한 경쟁적인 개발은 AI 연구의 새로운 황금기를 열고 있으며, 다양한 접근 방식과 혁신적인 아이디어를 촉진하고 있습니다.

## 초기 추론 모델(Initial Reasoning Models): o1 및 o1-mini

> “우리는 응답하기 전에 더 많은 시간을 생각하는 데 할애하도록 설계된 새로운 AI 모델 시리즈를 개발했습니다.” - [4]에서

OpenAI의 o1-preview [4, 5] 출시는 두 가지를 매우 명확하게 했습니다:
*   추론 모델(reasoning model)은 **수학 및 코딩 작업과 같은 검증 가능한 작업(verifiable tasks)을 매우 정확하게 해결할 수 있습니다.** 이는 AI가 단순히 데이터를 암기하는 것을 넘어, 규칙 기반의 논리적 사고를 수행할 수 있음을 보여줍니다.
*   이러한 문제를 해결하기 위해 추론 모델이 취하는 접근 방식은 전통적인 LLM의 접근 방식과 매우 다릅니다. 이는 AI 시스템 설계에 대한 새로운 사고방식을 요구합니다.

**긴 CoT(Long CoT).** 추론 모델(reasoning model)과 표준 LLM의 주요 차이점은 질문에 답하기 전에 "생각"할 수 있는 능력입니다. 추론 모델의 생각은 단순히 **긴 사고의 연쇄(long chains of thought)**입니다. 줄여서 **긴 CoT(long CoT)**라고도 하며, 때로는 **추론 흔적(reasoning trace)** 또는 **궤적(trajectory)**이라고도 불리며, LLM에 의해 출력됩니다. 이 긴 CoT는 단순한 텍스트 시퀀스(sequence)를 넘어 복잡한 사고 과정을 시뮬레이션합니다. 그러나 이러한 추론 궤적(reasoning trajectory)은 일반적인 텍스트 생성(text generation)보다 검색 알고리즘(search algorithm)에 더 가까운 매우 흥미로운 속성을 보여줍니다. 예를 들어, 모델은 다음과 같이 작동합니다:
*   복잡한 문제의 각 부분을 심사숙고합니다.
*   복잡한 문제를 더 작고 해결 가능한 부분으로 분해합니다.
*   자체 (부분적) 솔루션을 비판하고 오류를 찾습니다.
*   많은 대안 솔루션을 탐색합니다.

이러한 추론 궤적(reasoning trajectory)의 구체적인 예시는 [이 블로그 게시물](https://openai.com/index/learning-to-reason-with-llms/)을 참조하십시오.

특히, OpenAI의 추론 모델(reasoning model)이 사용하는 긴 CoT(long CoT)는 "내부적"입니다. 즉, 모델과 상호 작용할 때 사용자에게 숨겨져 있습니다. 대신, 사용자는 모델이 작성한 긴 CoT의 요약을 봅니다. 이는 사용자의 인지 부하를 줄이면서도 모델의 복잡한 추론 능력을 간접적으로 활용하는 방식입니다.

추론 모델(reasoning model)의 긴 CoT 출력은 LLM의 추론 시간 컴퓨팅(inference-time compute)을 제어하는 쉬운 방법을 제공합니다. 문제 해결에 더 많은 컴퓨팅 자원을 사용하고 싶다면, 단순히 더 긴 CoT를 생성하면 됩니다. 마찬가지로, 덜 복잡한 문제는 더 짧은 CoT로 해결할 수 있으므로 추론 시간(inference time)에 컴퓨팅 자원을 절약할 수 있습니다. 이러한 동적 자원 할당은 효율적인 AI 시스템 운영에 필수적입니다.

**추론 능력(Reasoning capabilities).** 초기 추론 모델(reasoning model)은 특정 영역에서 LLM의 추론 능력을 획기적으로 확장했습니다. 예를 들어, o1-preview는 GPT-4o를 만장일치로 능가하며, 대부분의 복잡한 추론 작업(reasoning tasks)에서 인간 전문가의 성능에 필적합니다. 이러한 결과를 달성하기 위해 o1-preview는 최대 추론 시간 컴퓨팅(maximal inference-time compute) 2를 사용하여 평가되며, i) 단일 출력 샘플(solid bar) 또는 ii) 64개의 병렬 출력 샘플(parallel output samples) 중 다수결 투표(majority vote, shaded bar)를 사용합니다.

추론 작업(reasoning tasks)에서의 o1 모델 대 GPT-4o (출처: [5])

o1-preview를 넘어, OpenAI의 o1(프리뷰 몇 달 후 출시된 o1의 정식 버전)은 미국 수학 올림피아드 예선 시험(AIME 2024)에서 상위 500명 학생 안에 들었으며, Codeforces에서 경쟁력 있는 인간 프로그래머 중 상위 11%에 랭크되었습니다. 참고로, GPT-4o는 AIME 문제의 12%만 해결했지만, o1은 추론 설정(inference settings)에 따라 문제의 74%에서 93%까지 해결합니다. o1과 GPT-4o의 성능에 대한 더 자세한 비교는 아래 그림을 참조하십시오.

GPT-4o 대비 o1의 개선 (출처: [5])

마찬가지로, o1-mini(o1의 더 저렴하고 빠른 버전)는 전체 o1 모델에 비해 80% 비용 절감에도 불구하고 인상적인 추론 능력(reasoning capabilities)을 가지고 있습니다. 이 모델은 o1에 비해 세계 지식(world knowledge)이 제한적임에도 불구하고 코딩 작업에 특히 능숙하며, 효율성을 고려할 때 매우 우수한 성능을 보입니다. 이는 효율성과 성능 간의 균형을 찾는 중요한 진전입니다.

## 최첨단 추론 모델(State-of-the-Art Reasoning Models): o3 및 o3-mini

ARC-AGI에서 OpenAI o3의 성능 (출처)

o1 모델의 발표 및 출시 직후, OpenAI는 o1 계보의 최신 모델인 o3를 발표했습니다. 이 모델은 처음에는 발표만 되었고(출시되지 않음), OpenAI가 측정한 몇 가지 주목할 만한 벤치마크(benchmark)에서 모델의 성능을 볼 수 있었지만, 실제로 모델을 사용할 수는 없었습니다. OpenAI가 발표한 지표들은 매우 인상적이었습니다. 사실, o3의 성능은 많은 사람들에게 상당히 충격적이었습니다. o3의 가장 주목할 만한 성과는 다음과 같습니다:
*   ARC-AGI 벤치마크(benchmark)에서 87.5%의 점수 달성. ARC-AGI는 5년 동안 깨지지 않았던 AGI를 향한 "북극성(North Star)"이며, GPT-4o는 여기서 5%의 정확도를 달성했습니다. o3는 ARC-AGI에서 85%의 인간 수준 성능을 초과한 최초의 모델입니다. 이는 인공 일반 지능(AGI)의 가능성을 엿보게 하는 중요한 이정표입니다.
*   SWE-Bench Verified에서 71.7%의 정확도와 Codeforces에서 2727의 Elo 점수를 기록하여, o3를 전 세계 상위 200명의 경쟁 프로그래머 중 하나로 만들었습니다.
*   EpochAI의 FrontierMath 벤치마크(benchmark)에서 25.2%의 정확도를 달성하여, 이전 최고 정확도인 2.0% 4를 개선했습니다.

그러나 대중은 이러한 결과 중 어떤 것도 검증하기 위해 o3 모델에 접근할 수 없었습니다. 이 글을 쓰는 시점에는 전체 o3 모델이 아직 출시되지 않았지만, OpenAI는 최근 모델의 더 작은 버전인 o3-mini [6]를 출시했습니다. 폐쇄형 모델의 이러한 경향은 혁신과 투명성 사이의 균형에 대한 논쟁을 불러일으키고 있습니다.

> “추론 노력(reasoning effort)을 줄이면 더 빠른 응답과 응답에서 추론에 사용되는 토큰(token) 수를 줄일 수 있습니다.” - [6]에서

OpenAI의 다른 추론 모델(reasoning model)과 비교하여 o3-mini는 비용 효율적이며 프로덕션(production)에 더 적합합니다. 예를 들어, 이 모델은 함수 호출(function calling), 웹 검색(web search) 및 구조화된 출력(structured outputs) 5과 같은 기능을 지원합니다. o3-mini는 또한 문제를 해결할 때 수행하는 추론의 양에 대해 낮음(low), 중간(medium), 높음(high) 노력(effort)을 포함한 여러 설정을 가지고 있습니다. 이 설정은 API 요청에서 직접 지정할 수 있으며, 추론 노력(reasoning effort) 수준에 따라 모델은 매우 인상적인 성능을 보입니다(많은 경우 o1과 동등합니다). 아래를 참조하십시오.

o3-mini 성능 분석 (출처: [6])

대부분의 경우, 낮은 추론 노력(low reasoning effort)의 o3-mini는 o1-mini의 성능과 일치하며, 높은 추론 노력(high reasoning effort)의 o3-mini는 OpenAI가 출시한 다른 모든 추론 모델(reasoning model)(전체 o1 모델 포함)의 성능을 능가합니다. o3-mini는 또한 더 나은 세계 지식(world knowledge)(즉, 향상된 사실성)을 가지고 있으며, 눈에 띄게 더 효율적이고, 이전 추론 모델(reasoning model)에 비해 인간 선호도 연구에서 더 높은 점수를 받았습니다. 아래를 참조하십시오. 특히, [6]의 저자들은 내부 A/B 테스트(A/B tests)에서 "o3-mini는 o1-mini보다 24% 더 빠르게 응답했으며, 평균 응답 시간은 10.16초에 비해 7.7초였습니다."라고 언급합니다. o3-mini는 OpenAI의 o1 스타일 추론 모델(reasoning model) 중 (현재까지) 가장 효율적인 모델입니다.

STEM / 비STEM 프롬프트(prompt)에서 o3-mini 대 o1-mini의 승률 (출처: [6])

**다른 모델 제공업체.** OpenAI의 o1 스타일 모델 출시는 다른 모델 제공업체들에 의해 빠르게 이어졌습니다. 예를 들어, Google은 최근 실험적인 Gemini-2.0 Flash Thinking을 출시했는데, 이는 Gemini 모델의 특징인 긴 컨텍스트(long context)(1M 토큰(token) 컨텍스트 창(context window))를 유지하며, 주요 검증 가능한 작업(verifiable tasks)(예: AIME 및 GPQA)에서 상당한 지표를 달성합니다. 그러나 이 모델은 여전히 o1 및 o3-mini의 성능에 뒤처집니다. (출처)

아주 최근에는 Grok-3의 추론 베타(reasoning beta)가 발표되었는데, 이는 매우 설득력이 있습니다. 아래에서 볼 수 있듯이, Grok-3 추론 모델(reasoning model)은 높은 추론 노력(reasoning efforts)을 가진 o3-mini의 성능을 능가하며, 몇몇 경우에는 전체 o3 모델에 거의 필적합니다. 예를 들어, AIME'24에서 96%의 정확도를 기록했는데, o3의 97% 정확도와 비교됩니다. 대규모 새 컴퓨팅 클러스터(compute cluster)를 사용하여 훈련된 Grok-3는 인상적입니다(특히 xAI의 젊음을 고려할 때). 이 글을 쓰는 시점에서 Grok-3의 추론 베타(reasoning beta)는 OpenAI의 추론 모델(reasoning model)에 가장 가까운 경쟁자입니다. (X의 Grok-3 발표 영상에서) 이러한 경쟁은 인공지능 연구의 발전을 가속화하고 있으며, 궁극적으로는 더 강력하고 효율적인 AI 시스템을 사용자에게 제공할 것입니다.

## 추론 모델을 위한 벤치마크(Benchmarks for Reasoning Models)

> “최근의 선도적인 모델들은 MATH와 GSM8K에서 너무 잘 수행하여, 이러한 벤치마크(benchmark)는 더 이상 모델을 구별하는 데 효과적이지 않습니다.” - [5]에서

추론 모델(reasoning model)이 어떻게 작동하는지 더 자세히 알아보기 전에, 그 성능을 더 깊이 살펴보겠습니다. 이 모델들의 능력을 진정으로 이해하려면, 단순히 지표(metrics)를 보는 것 이상을 해야 합니다. 이 모델들이 해결하는 문제의 구체적인 예시를 검토해야 합니다. 예를 들어, 아래에 표시된 GSM8K를 고려해 보십시오. 이는 초등학교 수준의 수학 벤치마크(benchmark)입니다. 이 질문들은 사소해 보일 수 있지만, LLM은 몇 년 동안 이 벤치마크(benchmark)를 정확하게 해결하는 데 어려움을 겪었습니다.

GSM8K의 예시 질문 (출처)

추론 모델(reasoning model)의 등장으로 이 벤치마크(benchmark)는 완전히 포화 상태가 되었습니다. 우리는 더 이상 이를 사용하여 최고의 추론 모델을 의미 있게 평가할 수 없습니다. 대신, 우리는 LLM으로 훨씬 더 어려운 문제를 해결하기 시작했습니다. 이는 AI 벤치마크 개발자들이 끊임없이 새로운 도전 과제를 찾아야 하는 "벤치마크 쳇바퀴(benchmark treadmill)" 현상을 보여줍니다.

AIME 2024의 예시 문제 (출처)

예를 들어, 위에 표시된 AIME 2024의 15번 문제를 고려해 보십시오. 이 문제는 상당히 복잡하며 GSM8K에서 발견되는 산술 추론(arithmetic reasoning) 질문을 넘어섭니다. 이 문제를 해결할 수 있는 방법은 (최소한) 6가지가 있으며, 이 모든 방법은 고급 수학 기술(예: 미분(derivatives), 정수론(number theory) 또는 라그랑주 승수(Lagrange multipliers))에 대한 지식을 필요로 합니다. 이러한 다각적인 접근 방식은 AI가 특정 도메인 지식을 통합하고 다양한 전략을 탐색하는 능력을 요구합니다.

또한, 추론 모델(reasoning model)이 해결하는 복잡한 벤치마크(benchmark)는 수학을 넘어섭니다! 예를 들어, GPQA [7]는 생물학, 물리학, 화학 등 여러 과학 분야의 수백 가지 객관식 질문을 포함합니다. 이 모든 질문은 해당 분야 전문가가 작성했으며, 매우 어렵고 "구글 검색으로 해결 불가능(Google-proof)"하다는 것이 검증되었습니다. 이는 비전문가들이 충분한 시간과 무제한 인터넷 접근이 주어져도 이 문제를 해결하는 데 어려움을 겪는다는 의미입니다.

> “우리는 질문이 고품질이며 극도로 어렵다는 것을 보장합니다. 해당 분야에서 박사 학위를 소유하거나 추구하는 전문가들은 65%의 정확도를 달성하는 반면, 고도로 숙련된 비전문가 검증자들은 웹에 무제한으로 접근하여 평균 30분 이상을 소비했음에도 불구하고 34%의 정확도만 달성합니다.” - [7]에서

ARC-AGI 벤치마크(benchmark)(“AGI를 향한 실질적인 디딤돌(material stepping stone)”로 묘사됨)는 LLM이 입력-출력 그리드(grid) 간의 패턴을 학습하고 이 학습된 패턴을 최종 출력 예시에 완벽하게 복제해야 하는 다양한 그리드 기반 퍼즐(puzzle)을 포함합니다. 아래를 참조하십시오. 대부분의 LLM은 이러한 퍼즐을 해결하는 데 어려움을 겪지만(예: GPT-4o는 5%의 정확도만 달성), 추론 모델(reasoning model)은 이 벤치마크(benchmark)에서 컴퓨팅 예산(compute budget)에 따라 30-90%의 정확도를 보이며 상당히 잘 수행합니다. 적어도, 이것들은 추론 LLM이 해결하기 시작하는 (사소하지 않은) 다른 수준의 문제입니다. 이러한 벤치마크들은 AI가 진정한 일반 지능에 도달하기 위해 넘어야 할 중요한 장애물을 제시합니다.

이러한 벤치마크(benchmark)의 어려움에도 불구하고, 현대 추론 모델(reasoning model)은 놀라울 정도로 유능한 것으로 밝혀졌습니다. OpenAI의 o3 모델은 AIME 2024에서 거의 97%의 점수를 달성했다고 보고되었습니다. 이 질문들 중 일부를 수동으로 검토한 후, 우리는 이 결과의 중요성을 진정으로 이해할 수 있습니다. 이는 AI가 이제는 인간의 사고 과정을 깊이 모방하고 심지어 특정 영역에서는 능가할 수 있음을 보여줍니다.

## 추론 모델의 기본(Fundamentals of Reasoning Models)

> “우리는 o1의 성능이 더 많은 강화 학습(reinforcement learning)(훈련 시간 컴퓨팅(train-time compute))과 더 많은 사고 시간(test-time compute)을 할애할수록 지속적으로 향상된다는 것을 발견했습니다.” - [1]에서

위에 제시된 추론 모델(reasoning model)들은 분명히 인상적이지만, 모두 폐쇄형 모델입니다. 따라서 우리는 그것들이 실제로 어떻게 작동하는지에 대한 정보가 없습니다. 우리가 받은 유일한 정보는 위 인용문과 아래에 표시된 그래프입니다. 이러한 "블랙박스" 모델은 성능은 뛰어나지만, 내부 작동 방식에 대한 이해를 어렵게 하여 투명성과 신뢰성 문제를 야기합니다.

(출처: [5])

그러나 이 제한된 정보로부터 우리는 몇 가지 유용한 결론을 도출할 수 있습니다. 주로 추론 모델(reasoning model)을 스케일링(scaling)하는 데 관련된 두 가지 핵심 구성 요소가 있습니다:
*   RL을 통한 더 많은 훈련. 이는 AI가 복잡한 환경에서 시행착오를 통해 학습하는 능력을 강조합니다.
*   더 많은 추론 시간 컴퓨팅(inference-time compute)(즉, 추론 시간 스케일링(inference-time scaling)). 이는 AI 시스템이 문제 해결에 필요한 자원을 동적으로 할당하는 능력을 의미합니다.

OpenAI는 추론 모델(reasoning model)의 이 두 가지 구성 요소를 스케일링(scaling)하는 접근 방식에 대한 많은 세부 사항을 공개하지 않지만, 이 주제에 대해 발표된 많은 연구가 여전히 존재합니다. 더 많은 맥락을 제공하기 위해, OpenAI가 공유한 세부 사항과 함께 이 연구 중 일부를 간략히 살펴보고, 추론 모델(reasoning model)이 훈련되고 사용되는 방식의 기반이 되는 몇 가지 핵심 개념을 설명해 보겠습니다.

### 검증 가능한 보상(Verifiable Rewards)을 사용한 강화 학습(Reinforcement Learning)

o1 스타일 모델에 대해 즉시 주목해야 할 한 가지 세부 사항은, 이 모델들이 본질적으로 검증 가능한 문제(verifiable problems)에 주로 사용되고 평가된다는 것입니다. 예를 들어, 수학 및 코딩입니다. 그런데 이 맥락에서 "검증 가능(verifiable)"하다는 것은 정확히 무엇을 의미할까요? 첫째, 우리는 i) 문제에 대한 정답(ground truth answer) 또는 ii) 정확성을 검증하는 데 사용할 수 있는 규칙 기반 기술(rules-based technique)에 접근할 수 있다고 가정합니다. 이는 AI 학습 과정에 객관적이고 측정 가능한 피드백을 제공하는 데 중요합니다.

**정확한 문자열 일치(exact string match)를 통한 수학 문제 검증**

예를 들어, 대부분의 수학 문제에 대한 정답(ground truth final answer)을 정의할 수 있습니다. 이는 GSM8K에서 `#### <answer>` 구문으로 수행됩니다. 그런 다음, LLM의 출력에서 최종 답변을 추출하고 기본적인 문자열 일치(string match)를 사용하여 이 답변을 정답과 비교할 수 있습니다. 위를 참조하십시오. 마찬가지로, 코딩 질문에 대해 테스트 케이스(test cases)가 준비되어 있다면, LLM이 생성한 코드를 실행하고 제공된 솔루션이 모든 테스트 케이스를 만족하는지 확인할 수 있습니다.

> “검증 가능한 보상(Verifiable Rewards)을 사용한 강화 학습(Reinforcement Learning with Verifiable Rewards, RLVR)은 LM 추론을 부트스트랩(bootstrapping)하기 위한 기존 접근 방식의 단순화된 형태이거나, 실행 피드백(execution feedback)을 사용하는 RL의 더 간단한 형태로 볼 수 있습니다. 여기서 우리는 단순히 답변 일치 또는 제약 조건 검증을 이진 신호(binary signal)로 사용하여 모델을 훈련합니다.” - [13]에서

어떤 도메인(domain)이 "검증 가능(verifiable)"하다고 말하는 것은 이 도메인의 문제에 대한 임의의 솔루션을 자동으로 검증할 수 있다는 의미가 아닙니다. 오히려, 우리는 검증을 위해 종종 정답(ground truth answers)(일반적으로 인간으로부터 얻은)에 접근해야 할 것입니다. 그러나 정답(ground truth) 대신 간단한 규칙을 사용하여 검증할 수 있는 몇 가지 동작이 있습니다. 예를 들어, 하드 코딩된 규칙(hard-coded rules) 세트를 사용하여 간단한 검사를 수행함으로써 추론 모델(reasoning model)이 올바른 출력 형식(output format)을 가지고 있는지, 특정 지침을 따르는지, 또는 특정 길이의 출력(예: o3-mini에서 사용되는 낮음, 중간 또는 높음 추론 노력(reasoning effort))을 생성하는지 확인할 수 있습니다.

**검증 복잡성(Verification complexities).** LLM의 출력을 검증하는 것은 우리가 해결하는 문제에 따라 상당히 복잡해질 수 있습니다. 수학 문제의 경우에도 LLM의 답변과 정답(ground truth) 간의 일치를 검증하는 것은 어렵습니다. 예를 들어, 솔루션이 다른 형태나 형식으로 제시되어 오탐(false negative) 검증으로 이어질 수 있습니다. 이러한 경우, 단순한 문자열 일치(string matching)만으로는 충분하지 않을 수 있습니다! 대신, LLM에게 두 솔루션이 일치하는지 여부를 알려달라고 프롬프트(prompt)할 수 있으며, 이는 잘못된 검증을 크게 줄이는 것으로 밝혀졌습니다 [14]. 코드의 경우, 검증을 구현하는 것도 어렵습니다. 이는 훈련 설정(training setup) 내에서 테스트 케이스(test cases)를 매우 효율적으로 실행하고 검증할 수 있는 데이터 파이프라인(data pipeline)을 구축해야 합니다.

> “우리는 DeepSeek-R1-Zero 개발에 신경 보상 모델(neural reward model)을 적용하지 않습니다. 왜냐하면 대규모 RL 프로세스에서 신경 보상 모델이 보상 해킹(reward hacking)에 시달릴 수 있고, 보상 모델을 재훈련하는 데 추가 훈련 자원이 필요하며 전체 훈련 파이프라인(training pipeline)을 복잡하게 만들기 때문입니다.” - [1]에서

**신경 검증(Neural verification).** 위에 설명된 검증 가능한 문제 외에도, 우리는 더 약한 형태의 검증을 고려할 수 있습니다. 예를 들어, 창의적 글쓰기는 검증하기 어려운 작업입니다. 그러나 우리는 다음과 같이 할 수 있습니다:
*   신경 보상 모델(neural reward model) 또는 검증자(verifier)를 훈련합니다.
*   이 모델로 LLM의 출력을 평가합니다.
*   예측된 점수를 보상 또는 검증 신호로 사용합니다.

이러한 설정은 인간 피드백 기반 강화 학습(reinforcement learning from human feedback, RLHF)과 매우 유사합니다. 이 경우, 우리는 모델 응답의 정확성 또는 품질 6을 기반으로 이진 검증(binary verification)을 수행하도록 보상 모델(reward model)을 훈련하고 있습니다. 그러나 신경 검증자(neural verifier)를 사용하는 것은 특히 대규모 RL을 수행할 때 **보상 해킹(reward hacking)**의 위험을 수반합니다. 모델은 더 오래 훈련되고 보상 환경(reward landscape)을 훨씬 더 많이 탐색하므로 보상 해킹(reward hacking)의 위험이 증가합니다. 결과적으로, 많은 최근 추론 모델(reasoning model)은 이 접근 방식을 피했습니다. 이는 AI 시스템의 의도치 않은 행동을 방지하기 위한 중요한 설계 고려 사항입니다.

**검증 가능한 보상으로부터 학습.** 이제 우리는 검증을 이해했지만, 검증을 사용하여 LLM을 훈련하는 방법은 무엇일까요? 여기의 아이디어는 간단합니다. 우리는 검증 결과를 RL을 통한 훈련을 위한 보상 신호(reward signal)로 직접 사용합니다. 아래를 참조하십시오. 이 아이디어를 구현하는 많은 다른 방법이 있지만(예: 프로세스 보상(process rewards) 또는 순수 RL(pure RL)), 이들은 검증 가능한 보상으로부터 학습하기 위해 RL을 사용한다는 공통된 주제를 공유합니다. 이것이 모든 현대 추론 모델(reasoning model)이 기반을 두는 근본적인 개념입니다.

(출처: [13])

RL과 함께 검증 가능한 보상으로부터 학습하는 데 사용할 수 있는 방법에 대한 완전한 설명은 아래 Sasha Rush의 놀라운 비디오를 확인하십시오.

### 추론 시간 전략(Inference-Time Strategies): 사고의 연쇄(Chain of Thought) 및 디코딩(Decoding)

언어 모델이 추론 시간(inference time)에 소비하는 컴퓨팅 양을 늘릴 수 있는 두 가지 기본적인 방법 7이 있습니다:
*   더 많은 토큰(token) 생성(즉, 더 긴 출력 시퀀스(output sequence)). 이는 모델이 더 깊이 탐색하고 상세한 설명을 제공할 수 있도록 합니다.
*   여러 출력 생성. 이는 다양한 관점을 탐색하고 최적의 솔루션을 찾기 위한 앙상블(ensemble) 접근 방식을 가능하게 합니다.

이 섹션에서는 이러한 기술을 더 자세히 살펴보고, 사고의 연쇄(chains of thought)와 다양한 디코딩 전략(decoding strategies)(예: 병렬 디코딩(parallel decoding) 대 순차 디코딩(sequential decoding))을 통해 LLM에서 실제로 어떻게 구현되는지 탐구할 것입니다.

(출처: [8])

**사고의 연쇄(Chain of thought).** 우리는 이미 추론 모델(reasoning model)이 긴 CoT(long CoT)를 추론을 위한 매개체로 사용한다는 것을 알고 있습니다. [8]에서 제안된 사고의 연쇄(chain of thought)는 가장 간단한 수준에서 LLM이 자체 출력에 대해 제공하는 설명일 뿐입니다. 대부분의 경우, 이러한 설명은 LLM이 복잡한 문제를 해결하기 위한 전략을 구성하는 데 활용됩니다.

추론 모델(reasoning model)이 사용하는 긴 CoT(long CoT)는 표준 CoT와는 많이 다릅니다. 표준 CoT는 간결하고 인간이 읽기 쉽습니다. 긴 CoT는 수천 개의 토큰(token) 길이 8입니다. 해석 가능성(interpretability) 목적으로 사용될 수 있지만, 긴 CoT는 인간이 읽기 쉽게 최적화되어 있지 않습니다. 오히려, 이는 문제 해결에 상세하게 접근하고 다양한 복잡한 추론 행동(reasoning behaviors)(예: 백트래킹(backtracking) 및 자기 개선(self-refinement))을 포함하는 광범위한 추론 흔적(reasoning trace)입니다. 이는 AI가 단순히 답을 내놓는 것이 아니라, 그 답에 도달하는 과정을 명시적으로 보여주는 중요한 단계입니다.

> “우리는 사용자에게 원시 사고의 연쇄(raw chains of thought)를 보여주지 않기로 결정했습니다… 우리는 모델에게 답변에서 사고의 연쇄로부터 유용한 아이디어를 재현하도록 가르침으로써 [이 결정]을 부분적으로 보완하려고 노력합니다. o1 모델 시리즈의 경우, 모델이 생성한 사고의 연쇄 요약을 보여줍니다.” - [5]에서

또한, 추론 모델(reasoning model)은 CoT를 모델의 최종 출력과 논리적으로 분리합니다. 예를 들어, OpenAI는 긴 CoT를 사용자에게 직접 노출하는 것을 피하고, 대신 LLM이 생성한 긴 CoT의 요약을 제공하여 추론 모델(reasoning model)의 최종 답변을 보완합니다. 이러한 논리적 분리는 CoT의 길이 때문에 근본적으로 필요합니다. 대부분의 사용자는 최종 답변만 읽을 것입니다. 전체 추론 흔적(reasoning trace)을 읽는 것은 엄청나게 시간이 많이 걸릴 것입니다.

(출처: [15])

**병렬 디코딩(Parallel decoding).** LLM의 최종 출력 정확도를 향상시키기 위해 병렬 디코딩 기술(parallel decoding techniques)을 사용할 수도 있습니다. 위를 참조하십시오. 여기의 아이디어는 간단합니다. LLM으로 단일 출력을 생성하는 대신, 여러 출력을 생성하고 이들을 집계하여 단일의 최종 답변을 형성합니다. 이 집계는 여러 가지 방법으로 수행될 수 있습니다. 예를 들어, 다수결 투표(majority vote) 또는 합의(consensus) 사용, 가중 투표(weighted voting) 사용, 신경 보상 모델(neural reward model) 또는 검증자(verifier)로 최상의 출력(들) 식별(즉, Best-of-N 또는 거부 샘플링(rejection sampling)으로도 알려짐), 또는 기타 도메인(domain)별 알고리즘(algorithm) 사용 등이 있습니다. 이러한 접근 방식의 주요 이점은 단순성과 효과성입니다. 병렬 디코딩(parallel decoding)을 확장하는 것은 쉽습니다. 단순히 더 많은 출력을 생성하고, 검증하고, 집계하면 됩니다. 이는 성능에 의미 있는 향상을 가져옵니다 [9, 10, 11]. 병렬 디코딩 기술(parallel decoding techniques)은 o1 스타일 모델에서 분명히 사용됩니다. 그들의 블로그 게시물(blog posts)에 제공된 그래프(아래 표시)의 세부 사항을 살펴보십시오!

그러나 병렬 디코딩 기술(parallel decoding techniques)만으로는 최근 출시된 추론 모델(reasoning model)이 보이는 더 복잡한 추론 행동(reasoning behaviors) 중 일부를 설명할 수 없습니다. 이는 AI가 단순한 병렬 처리를 넘어선, 더 정교한 인지 메커니즘을 사용하고 있음을 시사합니다.

(출처: [5])

덧붙여 말하자면, 우리는 거부 샘플링(rejection sampling) 아이디어를 훈련에도 적용할 수 있습니다(즉, 훈련 시간 대 테스트 시간 거부 샘플링). 이를 위해 우리는 다음과 같이 합니다:
*   여러 출력 또는 궤적(trajectory)을 샘플링(sample)합니다.
*   보상 모델(reward model)(또는 다른 점수 매기기 메커니즘(scoring mechanism))을 사용하여 최상의 출력을 선택합니다.
*   이 출력으로 훈련합니다.

이 접근 방식은 실제로 흔히 사용됩니다. 예를 들어, LLaMA 모델은 RLHF 적용 전에 후처리 훈련(post training) 과정에서 여러 차례의 훈련 시간 거부 샘플링(training-time rejection sampling)을 수행합니다. 거부 샘플링(rejection sampling)은 실제로 매우 효과적이며, PPO 기반 RLHF에 비해 구현 및 확장이 더 쉽습니다.

> “우리는 덜 안정적이고 확장하기 어려운 경향이 있는 더 복잡한 강화 학습(reinforcement learning) 알고리즘(algorithm) 대신, 지도 미세 조정(supervised finetuning, SFT), 거부 샘플링(rejection sampling, RS) 및 직접 선호도 최적화(direct preference optimization, DPO)를 기반으로 하는 비교적 간단한 후처리 훈련 절차(post-training procedure)를 채택합니다.” - [12]에서

**자기 개선(Self-refinement).** 병렬 디코딩(parallel decoding) 외에도, 디코딩(decoding)을 위한 비판 또는 자기 개선 전략(self-refinement strategies)을 고려할 수 있습니다. 먼저, LLM은 초기 응답을 생성합니다. 그런 다음, LLM 또는 외부 소스(external source)로부터 피드백(feedback)이 응답에 제공되며, LLM은 이 피드백을 기반으로 응답을 수정할 수 있습니다. 이 주기는 임의의 횟수만큼 반복될 수 있습니다. 아래 그림을 참조하십시오. 이는 인간의 학습 과정과 유사하게, AI가 자신의 오류를 인식하고 수정하는 능력을 개발하고 있음을 보여줍니다.

(출처: [15])

개선을 위한 여러 가지 다른 접근 방식이 존재하지만, 크게 두 가지 그룹으로 분류할 수 있습니다:
*   **외재적(Extrinsic)**: 피드백(feedback)이 외부 검증자(verifier) 또는 모듈(module)로부터 옵니다.
*   **내재적(Intrinsic)**: LLM이 자체 생성에 대한 피드백을 제공합니다.

개선의 결과와 실제 효과는 다소 엇갈립니다. 검증자(verifier) [16] 또는 코드 인터프리터(code interpreter) [17]와 같은 외재적 피드백(extrinsic feedback)을 사용하여 LLM의 출력을 개선한 성공적인 사례가 많이 있습니다. 내재적 개선(intrinsic refinement)이 효과적인지 여부는 LLM이 제공하는 피드백(feedback)의 품질에 크게 좌우됩니다. 내재적 개선(intrinsic refinement)은 간단한 작업에 잘 작동할 수 있습니다 [18]. 그러나 이 접근 방식은 더 복잡한 작업(예: 수학)으로 일반화하는 데 어려움을 겪습니다 [19]. 이는 AI의 자기 성찰 능력이 아직 초기 단계에 있음을 의미합니다.

> “LLM이 비교적 정확한 자기 검토를 보상으로 제공할 때, 컨텍스트 내에서(in-context) 응답을 개선할 수 있습니다.” - [18]에서

## 오픈 추론(Open Reasoning): DeepSeek-R1 및 그 이상

지금까지 우리는 LLM 내에 추론 능력(reasoning capabilities)을 부여할 수 있게 하는 기본 개념에 대해 배웠습니다. 그러나 우리가 배운 모든 모델은 폐쇄형입니다. 우리는 이 모델들이 정확히 어떻게 만들어졌는지 알 방법이 없습니다. 다행히도, 최근 몇 가지 개방형 추론 모델(open reasoning models)이 출시되었습니다. 이는 AI 연구의 투명성과 재현성을 높이는 데 중요한 기여를 합니다. 이 섹션에서 다룰 이 모델들 중 가장 주목할 만한 것은 DeepSeek-R1 [1]입니다. OpenAI의 o1 성능에 필적할 뿐만 아니라, 이 모델은 복제에 충분한 세부 정보를 제공하는 완전한 기술 보고서(technical report)를 제공하여, 강력한 추론 모델(reasoning model)을 만드는 데 필요한 과정을 완전히 명확히 합니다.

(출처: [1])

DeepSeek-R1의 핵심 아이디어는 우리가 지금까지 배운 것과 잘 일치합니다. 이 모델은 검증 가능한 작업(verifiable tasks)에 대해 RL로 훈련되며, 여기서 긴 CoT(long CoT)를 활용하여 복잡한 추론 문제(reasoning problems)를 해결하는 방법을 학습합니다. 흥미롭게도, RL 훈련 과정이 모델의 강력한 추론 능력(reasoning capabilities)에 핵심적인 기여자입니다. 이 모델의 여러 버전(DeepSeek-R1-Zero 및 DeepSeek-R1)이 출시되었으며, 이들은 유사한 추론 능력(reasoning capabilities)을 가지고 있습니다. 우리가 보게 될 것처럼, 이 모델들 중 첫 번째는 어떤 지도 훈련(supervised training)도 완전히 포기하며, 복잡한 추론 능력(reasoning capabilities)이 대규모 RL 훈련으로부터 자연스럽게 나타난다는 것을 보여줍니다. 이는 AI가 최소한의 인간 감독으로도 고차원적인 인지 능력을 습득할 수 있음을 시사합니다.

> “DeepSeek-R1-Zero는 사전 단계로서 지도 미세 조정(supervised fine-tuning, SFT) 없이 대규모 강화 학습(reinforcement learning, RL)을 통해 훈련된 모델로, 놀라운 추론 능력(reasoning capabilities)을 보여줍니다. RL을 통해 DeepSeek-R1-Zero는 수많은 강력하고 흥미로운 추론 행동(reasoning behaviors)을 자연스럽게 나타냅니다.” - [1]에서

**DeepSeek-v3.** DeepSeek-R1-Zero와 DeepSeek-R1의 생성은 모두 DeepSeek-v3 [2]라는 강력한 기본 모델(base model)로 시작합니다. 오픈 웨이트(open weights)와 상세한 기술 보고서(technical report) [2]를 가지고 있을 뿐만 아니라, 이 모델은 이전 오픈 LLM의 성능을 능가하며 폐쇄형 모델의 품질과도 일치합니다. 아래를 참조하십시오.

(출처: [2])

DeepSeek-v3는 6,710억 개의 매개변수(parameter)를 가진 전문가 혼합(Mixture-of-Experts, MoE) 모델입니다. MoE에 익숙하지 않다면, 아래 게시물을 확인하십시오. 이 게시물은 개념을 설명하고 DeepSeek-v3를 포함한 몇 가지 실제 예시를 제공합니다. MoE는 모델의 효율성과 확장성을 높이는 중요한 아키텍처 혁신으로 평가받고 있습니다.

**전문가 혼합(Mixture-of-Experts, MoE) LLM**
Cameron R. Wolfe, Ph.D. · 1월 27일
[전체 이야기 읽기](https://www.crwolfe.com/p/mixture-of-experts-moe-llms)

추론(inference) 및 훈련 효율성을 개선하기 위해 DeepSeek-v3는 다음과 같은 설계 선택을 합니다(자세한 내용은 [여기](https://www.crwolfe.com/p/mixture-of-experts-moe-llms) 참조):
*   다중 헤드 잠재 어텐션(Multi-Headed Latent Attention, MLA)을 사용합니다.
*   최적화된 MoE 구조(예: 세분화된(fine-grained) 및 공유된(shared) 전문가(experts))를 채택합니다.
*   사전 훈련(pretraining) 중에 다중 토큰 예측 목표(multi-token prediction objective)를 사용합니다.
*   MoE 모델 훈련에 일반적으로 사용되는 부하 분산 손실(load balancing losses)을 포기합니다.
*   [2]에서 제안된 새로운 양자화 훈련 전략(quantized training strategy)을 채택하여 훈련 전반에 걸쳐 정밀도를 FP8로 감소시킵니다.

이러한 이유로 DeepSeek-v3의 훈련은 다른 모델에 비해 매우 경제적입니다. 이 모델은 성능과 효율성 모두에서 인상적입니다. 이 모델의 여러 이전 버전이 출시되었으며, 이는 DeepSeek-v3가 내린 일부 설계 결정에 영감을 주었습니다. 예를 들어, DeepSeek-v2 및 DeepSeek-v2.5 9를 참조하십시오.

### DeepSeek-R1-Zero

> “우리는 어떤 지도 데이터(supervised data) 없이도 LLM이 추론 능력(reasoning capabilities)을 개발할 수 있는 잠재력을 탐구하며, 순수한 강화 학습(reinforcement learning) 과정을 통한 자기 진화(self-evolution)에 초점을 맞춥니다.” - [1]에서

DeepSeek이 제안한 첫 번째 추론 모델(reasoning model)은 DeepSeek-R1-Zero였습니다. 이 모델은 대규모 RL을 통해 순수하게 추론하도록 모델을 가르치는 흥미로운 훈련 전략을 채택합니다. SFT(지도 미세 조정) 없이 말입니다. 이 모델은 RL을 통해 긴 CoT(long CoT)를 활용하여 복잡한 추론 문제(reasoning problems)를 해결하는 방법을 자연스럽게 탐색하고 학습합니다. DeepSeek-R1-Zero는 지도 훈련(supervised training) 없이도 추론 능력(reasoning capabilities)이 개발될 수 있음을 보여준 최초의 공개 연구 노력입니다. 이는 AI가 외부의 명시적인 지시 없이도 내재적인 학습 메커니즘을 통해 복잡한 기술을 습득할 수 있음을 증명합니다.

(출처: [22])

**GRPO를 사용한 RL.** DeepSeek-R1-Zero의 훈련은 DeepSeek-v3 [2] 기본 모델(base model)로 시작합니다. 우리는 이 기본 모델(base model)을 RL을 통해 직접 미세 조정(finetune)합니다. 특히, [1]의 저자들은 위에 그림으로 묘사된 그룹 상대 정책 최적화(Group Relative Policy Optimization, GRPO) [3]를 그들의 RL 알고리즘(algorithm)으로 선택합니다. LLM 훈련을 위한 RL 알고리즘(algorithm) 선택은 개방적이고 활발한 연구 주제입니다. 전통적으로 연구자들은 LLM 훈련에 PPO를 사용했지만, 최근에는 REINFORCE 또는 GRPO와 같은 더 간단한 RL 알고리즘(algorithm)을 LLM 훈련에 채택하는 경향이 있습니다. [1]에서 GRPO 선택에 대해 제공된 주요 이유는 다음과 같습니다:
*   RL 훈련 비용 절감.
*   정책 모델(policy model)(즉, LLM 자체)과 (일반적으로) 동일한 크기인 비판 모델(critic model)의 제거.

**보상 정의(Defining rewards).** LLM을 사용한 RL에 대한 대부분의 전통적인 연구와 달리, DeepSeek-R1-Zero를 훈련하는 데 신경 보상 모델(neural reward models)(즉, 선호도 데이터(preference data)로 훈련된 LLM 기반 보상 모델)은 사용되지 않습니다. 오히려, 저자들은 i) 보상 해킹(reward hacking)을 피하고, ii) 컴퓨팅 비용 10을 절약하며, iii) 구현이 더 간단한 규칙 기반 보상 시스템(rules-based reward system)을 사용합니다. 특히 두 가지 유형의 보상이 사용됩니다:
*   **정확도 보상(Accuracy reward)**: 모델의 응답이 올바른지 평가합니다.
*   **형식 보상(Format reward)**: 모델 출력에 원하는 형식을 강제합니다.

DeepSeek-R1-Zero는 수학 및 코딩 문제와 같은 자동으로 검증 가능한 작업(automatically verifiable tasks)에만 훈련됩니다. 결정론적 결과(deterministic results)를 가진 수학 문제의 경우, 모델은 지정된 형식으로 답변을 제공할 수 있으며, 이를 통해 기본적인 문자열 일치(string matching)를 통해 검증할 수 있습니다. 마찬가지로, 코딩 문제는 LLM이 생성한 코드를 미리 정의된 테스트 케이스(test cases)에 대해 샌드박스(sandbox)에서 실행함으로써 검증할 수 있습니다.

> “신경 보상 모델(neural reward model)은 대규모 강화 학습(reinforcement learning) 과정에서 보상 해킹(reward hacking)에 시달릴 수 있으며, 보상 모델을 재훈련하는 데 추가 훈련 자원이 필요하고 전체 훈련 파이프라인(training pipeline)을 복잡하게 만듭니다.” - [1]에서

위에서 언급했듯이, 형식 보상(format reward)은 모델이 올바른 형식 또는 템플릿(template)을 사용하는 출력을 생성할 때 긍정적인 훈련 신호(training signal)를 제공합니다. [1]에서 사용된 형식은 단순히 모델의 긴 CoT(long CoT)(즉, 사고/추론 과정)를 두 개의 특수 토큰(special tokens)인 `<think>`와 `</think>` 사이에 배치합니다. 그런 다음 모델은 추론 과정이 완료된 후 `<answer>`와 `</answer>` 태그(tags) 사이에 별도로 답변을 생성합니다. 아래 그림을 참조하십시오.

(출처: [1])

**RL을 통한 학습.** SFT를 사용하지 않았음에도 불구하고, DeepSeek-R1-Zero는 RL 훈련 과정 전반에 걸쳐 추론 능력(reasoning capabilities)에서 명확한 진전을 보입니다. 훈련이 진행됨에 따라 AIME 2024에서 모델의 성능이 아래에 그래프로 표시됩니다. 여기서 모델의 성능은 점진적으로 향상되어 결국 o1-preview 11와 동등한 수준에 도달합니다. 훈련 완료 후, DeepSeek-R1-Zero는 AIME 2024에서 초기 성능 15.6%에서 71.0%로 향상되었으며, 16표의 다수결 투표(majority voting)를 사용할 경우 86.7%에 달합니다! 이러한 결과는 폐쇄형 추론 모델(reasoning model)에서 볼 수 있는 성능 추세와 일치합니다. DeepSeek-R1-Zero는 RL 훈련 후 인상적인 성능을 달성하며, 병렬 디코딩 전략(parallel decoding strategies)을 통해 성능을 더욱 향상시킬 수 있습니다.

(출처: [1])

DeepSeek-R1-Zero와 o1 모델 간의 전체 성능 비교는 아래 표에 제공됩니다. DeepSeek-R1은 대부분의 경우 o1-mini의 성능과 일치하거나 능가하며, 여러 작업에서 o1-preview와 유사한 성능을 보입니다. 그러나 OpenAI의 추론 모델(reasoning model)은 코딩 도메인(coding domain)에서 훨씬 더 나은 성능을 보입니다. DeepSeek-R1-Zero는 분명히 덜 강력한 코딩 모델입니다. 곧 보게 되겠지만, 이 문제는 DeepSeek-R1(후속 모델)에서 해결됩니다.

(출처: [1])

**여기서 무슨 일이 일어나고 있을까요?** 분명히, DeepSeek-R1-Zero는 [1]에 설명된 RL 훈련 과정에서 인상적인 추론 능력(reasoning capabilities)을 얻습니다. 그러나 모델 학습 과정의 역학(dynamics) 또한 상당히 관찰 가능합니다! SFT 스타일 훈련을 수행하지 않기 때문에, 우리는 RL 훈련 과정 전반에 걸쳐 모델의 추론 전략(reasoning strategy) 진행 상황을 면밀히 모니터링할 수 있습니다. 아래에 표시된 바와 같이, DeepSeek-R1-Zero는 훈련이 진행됨에 따라 더 많은 "사고 시간"(또는 점진적으로 더 긴 사고의 연쇄(chains of thought)를 생성)을 활용하여 추론 과정(reasoning process)을 개선하는 방법을 학습합니다. 모델은 더 어려운 문제를 해결하기 위해 더 많은 테스트 시간 컴퓨팅(test-time compute)을 활용하는 방법을 자연스럽게 학습합니다! 이는 AI의 자기 조직화(self-organization) 및 적응 능력에 대한 중요한 통찰력을 제공합니다.

(출처: [1])

[1]의 저자들은 또한 RL 훈련 중에 자연스럽게 나타나는 몇 가지 흥미로운 경향을 관찰합니다. 예를 들어, 모델은 추론 과정(reasoning process)의 이전 구성 요소를 다시 검토하고 평가함으로써 자체 솔루션을 반성하는 능력을 개발합니다. 마찬가지로, 모델은 문제 해결 과정에서 대안 솔루션(alternative solutions)이나 접근 방식을 명시적으로 테스트하고 탐색하기 시작합니다. 이 행동은 명시적으로 프로그래밍된 것이 아닙니다. RL 훈련 중에 자연스럽게 발생합니다! 이는 AI의 창발적 행동(emergent behavior)에 대한 강력한 증거입니다.

> “DeepSeek-R1-Zero의 자기 진화(self-evolution)는 RL이 모델이 자율적으로 추론 능력(reasoning capabilities)을 향상시키도록 어떻게 이끌 수 있는지 보여주는 매혹적인 시연입니다.” - [1]에서

가장 기본적인 수준에서, [1]에서 구축된 RL 환경(RL environment)은 모델이 검증에 의해 결정된 올바른 최종 솔루션에 도달하기 위한 다양한 전략을 탐색하도록 허용합니다. 탐색 중에 우리는 모델에게 다음과 같은 것에 대해 보상합니다:
*   올바른 추론 템플릿(reasoning template) 또는 구조 사용.
*   올바른 최종 솔루션 생성.

이러한 보상만으로도 모델은 복잡한 추론 문제(reasoning problems)를 해결하는 방법을 학습합니다. 우리는 모델에게 문제를 분해하고, 솔루션을 검색하고, 백트래킹(backtracking)을 수행하거나, 자체 사고 과정을 평가하는 방법을 명시적으로 가르칠 필요가 없습니다. 대신, 우리는 훈련 과정 동안 모델에 올바른 인센티브(incentives)(또는 보상)를 제공합니다. 그러면 LLM은 RL 기반 "자기 진화(self-evolution)" 과정을 통해 문제 해결에 필요한 행동을 자율적으로 학습할 수 있습니다. 이는 지능의 본질에 대한 깊은 질문을 던집니다.

### DeepSeek-R1

DeepSeek-R1-Zero는 LLM이 SFT 없이 순수 RL로부터 인상적인 추론 능력(reasoning capabilities)을 개발할 수 있음을 보여주지만, 이 모델에는 몇 가지 사소한 버그(bug)가 있습니다. 예를 들어, 가독성(readability)이 좋지 않고 12 언어를 잘못 혼합합니다. 간단히 말해, DeepSeek-R1-Zero는 추론에 매우 능숙하지만, 잘 정렬된(well-aligned) LLM의 바람직한 속성 중 일부가 부족합니다. 해결책으로, [1]의 저자들은 몇 가지 "콜드 스타트(cold start)" SFT 데이터(data)를 다른 트릭(tricks)과 함께 훈련에 통합하는 새로운 다단계 훈련 과정(multi-stage training process)을 제안합니다. 이 훈련 파이프라인(training pipeline)은 정렬(aligned)되어 있고 복잡한 추론(reasoning)이 가능한 LLM인 DeepSeek-R1을 만드는 데 사용됩니다. 이는 순수한 능력과 실제 적용 가능성 사이의 간극을 메우는 중요한 단계입니다.

DeepSeek-R1-Zero와 마찬가지로, 우리는 DeepSeek-v3를 기본 모델(base model)로 시작합니다. 그런 다음 DeepSeek-R1은 두 번의 SFT 단계(phases)와 두 번의 RL 단계(phases)를 포함한 네 단계의 훈련을 거칩니다. SFT 단계(phases)의 목적은 각 RL 단계(phases) 동안 탐색을 위한 더 나은 시작점(starting point)을 제공하는 것입니다. 이 훈련 파이프라인(training pipeline)은 [1]의 주요 기여 중 하나입니다. 이는 추론 스타일 훈련(reasoning-style training)과 LLM을 위한 표준 후처리 훈련(post training) 레시피(recipe)를 결합하는 효과적인 방법을 제공합니다. DeepSeek-R1에 사용된 훈련 레시피(recipe)의 각 단계를 더 자세히 살펴보겠습니다.

> “기본 모델(base model)로부터 RL 훈련의 초기 불안정한 콜드 스타트(cold start) 단계를 방지하기 위해, DeepSeek-R1의 경우 우리는 소량의 긴 CoT 데이터(data)를 구축하고 수집하여 모델을 초기 RL 액터(actor)로 미세 조정(fine-tune)합니다.” - [1]에서

**1단계: 콜드 스타트(Cold Start)(또는 추론 지향 SFT).** RL 훈련에 앞서, R1은 [1]에서 "콜드 스타트(cold start)" 데이터(data)라고 불리는 소량의 긴 CoT 예시 데이터셋(dataset)에 대해 SFT를 통해 훈련됩니다. 이 콜드 스타트(cold start) 데이터(data)를 수집하는 데 사용할 수 있는 몇 가지 다른 접근 방식이 있습니다:
*   모델(예: DeepSeek-v3)에게 소수점 예시(few-shot examples)를 사용하거나, 모델에게 상세한 답변과 함께 반성 및 검증을 생성하도록 지시하여 긴 CoT 데이터(data)를 생성하도록 프롬프트(prompt)합니다.
*   R1-Zero 모델을 사용하여 많은 수의 긴 CoT 출력을 생성한 다음, 인간에게 후처리(post-process)하고 모델의 최상의 출력을 선택하도록 요청합니다.

[1]의 저자들은 이러한 접근 방식을 결합하여 "수천 개의 콜드 스타트(cold-start) 데이터(data)"를 수집했으며, 이를 통해 DeepSeek-v3는 SFT를 통해 직접 미세 조정(finetune)됩니다. 긴 CoT 데이터(data)를 사용하기 때문에, 이는 추론 지향 미세 조정 과정(reasoning-oriented finetuning process)입니다. 이 콜드 스타트(cold start) 데이터(data)로부터 모델은 추론 문제(reasoning problems) 해결을 위한 실행 가능한 (초기) 템플릿(template)을 학습합니다. 추론 지향 SFT에 사용된 데이터(data)는 DeepSeek-R1의 훈련 과정에 인간 사전 지식(human prior)을 도입합니다. 우리는 이 단계에서 모델이 학습하는 데이터(data)의 스타일과 패턴을 명시적으로 선택할 수 있습니다. 예를 들어, [1]의 저자들은 이 데이터(data)를 각 긴 CoT(long CoT)의 요약을 포함하도록 구성하여, 모델이 최종 답변을 제공하기 전에 전체 추론 과정(reasoning process)을 요약하도록 가르친다고 언급합니다. 이 데이터(data)는 RL 훈련 과정의 씨앗(seed) 역할을 합니다. 모델은 SFT 훈련 데이터(data)의 스타일과 일치시키면서 자기 탐색(self-exploration)을 시작합니다. 이는 초기 안내를 통해 학습 효율을 높이는 중요한 전략입니다.

**2단계: 추론 지향 RL.** SFT 후, 우리는 R1-Zero가 제안한 대규모 RL 훈련 과정(large-scale RL training process)을 반복하여 기본 모델(underlying model)이 추론 집약적 작업(reasoning-intensive tasks)을 처리하는 능력을 향상시킵니다. DeepSeek-R1에 대한 유일한 변경 사항은 언어 일관성 보상(language consistency reward)의 추가입니다. 이는 모델 출력 중 원하는 대상 언어로 작성된 부분으로 계산됩니다. 이 언어 일관성 보상(language consistency reward)은 [1]에서 모델의 추론 능력(reasoning capabilities)을 약간 저하시키는 것으로 밝혀졌습니다. 그러나 언어 일관성(language consistency)은 결과 모델의 인간 선호도(human preferences)와의 전반적인 정렬(alignment)을 향상시킵니다. 모델의 출력이 더 유창하고 가독성(readable)이 높아집니다. 이는 기능적 성능과 사용자 경험 사이의 균형을 맞추는 중요한 과정입니다.

**3단계: 거부 샘플링(Rejection sampling).** 추론 지향 RL의 수렴 후, 우리는 결과 모델을 사용하여 크고 다양한 SFT 데이터셋(dataset)을 수집합니다. 그러나 초기 콜드 스타트(cold start) SFT 단계와 달리, 우리는 단순히 추론 지향 데이터(reasoning-oriented data) 이상을 수집합니다. 즉, 모델이 더 넓은 범위의 문제와 도메인(domain)에서 학습할 수 있도록 추론 데이터(reasoning data)를 일반 목적 데이터(general purpose data)로 증강합니다. 더 많은 추론 데이터(reasoning data)를 수집하기 위해 [1]의 저자들은 다음과 같이 합니다:
*   다양한 추론 기반 프롬프트(reasoning-based prompts)를 큐레이션(curate)합니다.
*   2단계 모델을 사용하여 후보 궤적(candidate trajectories) 13을 생성합니다.
*   거부 샘플링(rejection sampling)을 수행합니다. 즉, 각 궤적(trajectory)의 품질과 정확성을 기반으로 상위 궤적을 필터링하고 선택합니다.

이것은 이 게시물에서 이전에 배웠던 것과 동일한 훈련 시간 거부 샘플링(training-time rejection sampling) 과정입니다! 흥미롭게도, 이 단계에서는 검증을 위해 규칙 기반 기술(rules-based techniques) 이상에 의존합니다. 우리는 또한 DeepSeek-v3를 생성 보상 모델(generative reward model) 또는 약한 검증자(weak verifier)로 사용하여 검증 불가능한 도메인(non-verifiable domains)의 추가 데이터(data)를 통합합니다. 휴리스틱 필터링(heuristic filtering)(예: 언어 혼합 또는 긴 단락이 있는 출력 제거)을 적용한 후, 우리는 최종적으로 60만 개의 추론 궤적(reasoning trajectories) 세트에 도달합니다.

> “우리는 DeepSeek-V3의 SFT 데이터셋(dataset) 일부를 재사용합니다. 특정 비추론 작업(non-reasoning tasks)의 경우, 프롬프트(prompt)를 통해 질문에 답하기 전에 DeepSeek-V3를 호출하여 잠재적인 사고의 연쇄(chain-of-thought)를 생성합니다.” - [1]에서

이 단계의 SFT 데이터셋(dataset)에는 상당한 비율의 비추론 데이터(non-reasoning data)(예: 글쓰기 또는 번역 예시)가 포함됩니다. 우리는 이 데이터(data)를 DeepSeek-v3에 사용된 동일한 후처리 훈련 데이터셋(post training dataset)에서 가져옵니다. 그러나 데이터(data)는 DeepSeek-v3에게 복잡한 쿼리(queries)의 출력을 설명하기 위해 긴 CoT(long CoT)를 생성하도록 요청함으로써 증강됩니다. 반면, 더 간단한 쿼리에는 CoT가 제공되지 않습니다. 총 20만 개의 비추론 예시가 수집되어, 80만 개의 예시로 구성된 SFT 데이터셋(dataset)을 형성합니다.

**4단계: 일반 목적 RLHF.** DeepSeek-R1의 최종 훈련 단계는 모델의 추론 능력(reasoning abilities)을 계속 연마하면서 인간 선호도(human preferences)에 모델을 정렬(align)합니다. 이전 단계와 마찬가지로, 우리는 추론 기반 데이터(reasoning-based data)와 일반 목적 데이터(general purpose data)의 조합으로 모델을 훈련합니다. 특히, 우리는 각 데이터(data) 유형에 대해 다른 보상 조합을 사용하여 RL로 모델을 훈련합니다:
*   추론 기반 문제(reasoning-based problems)에 대한 규칙 기반 보상(rules-based rewards)(R1-Zero와 동일).
*   일반 목적 데이터(general purpose data)에 대한 신경 보상 모델(neural reward models)(표준 RLHF와 마찬가지로 인간 선호도 쌍(human preference pairs)으로 훈련됨).

DeepSeek-R1은 일반 목적 데이터(general purpose data)에서 더 유용하고 무해하도록 정렬(aligned)됩니다. 이들은 LLM 연구에서 사용되는 두 가지 매우 일반적인 정렬 기준(alignment criteria)입니다. 이 각 기준은 인간 선호도(human preferences)의 (지도) 데이터셋(dataset)으로 훈련된 별도의 신경 보상 모델(neural reward model)로 모델링됩니다. 유용성 보상(Helpfulness rewards)은 모델의 최종 답변(즉, 긴 CoT(long CoT) 제외)에 대해서만 측정되는 반면, 무해성 보상(harmless rewards)은 모델의 전체 출력 궤적(output trajectory) 14를 고려합니다. 규칙과 선호도 기반 보상(preference-based rewards)을 결합함으로써, DeepSeek-R1은 강력한 추론 성능(reasoning performance)을 유지하면서 인간 선호도(human preferences)에 정렬(aligned)될 수 있습니다. 이는 AI의 윤리적이고 사회적으로 책임 있는 발전을 위한 핵심 요소입니다.

(출처: [1])

**성능은 어떻습니까?** 위에 표시된 바와 같이, R1은 대부분의 추론 작업(reasoning tasks)에서 o1의 성능과 일치하거나 능가합니다. R1-Zero와 달리, R1은 또한 상당히 강력한 코딩 능력(coding abilities)을 가지고 있습니다. 일반 목적 작업(general purpose tasks)에서 R1은 하이브리드 훈련 파이프라인(hybrid training pipeline)의 결과로 계속해서 좋은 성능을 보입니다. 일반적으로 R1은 OpenAI의 o1과 동등한 수준으로 보이는 매우 유능한 모델이며, 전통적인 작업과 추론 지향 작업(reasoning-oriented tasks)을 포함한 다양한 작업을 높은 정확도로 해결할 수 있습니다. 이 모델(및 다른 추론 모델(reasoning models))에 대한 한 가지 흥미로운 관찰은 표준 LLM과 비교하여 지시 따르기 벤치마크(instruction following benchmarks)(예: IF-Eval)에서 성능이 좋지 않다는 것입니다. 현재 추론 모델(reasoning models)은 지시를 따르는 데 표준 LLM보다 못한 것으로 보입니다. 미래에는 이 추세가 역전될 가능성이 높다고 개인적으로 믿습니다. 이론적으로, 추론 모델(reasoning models)은 사고 과정(thought process)을 활용하여 인간 사용자가 제공한 프롬프트(prompt)를 더 잘 해석하고 준수할 수 있어야 합니다. 예를 들어, 심의적 정렬(deliberative alignment)은 다소 유사한 접근 방식을 따릅니다.

**SFT가 필요한가?** R1-Zero는 SFT 없이 강력한 추론 모델(reasoning models)을 훈련할 수 있는 능력을 강조하는 반면, 전체 R1 모델은 더 강력한 최종 모델을 얻기 위해 여러 SFT 단계(phases)를 사용합니다. 그래서 우리는 궁금해지기 시작할 수 있습니다: SFT를 사용해야 할까요, 말아야 할까요? 추론 모델(reasoning models)에 SFT가 필수적일까요?

표준 LLM의 경우, SFT는 RLHF를 위한 고품질 시작점(starting point)을 제공합니다. 기본 모델(base model)에 RLHF를 직접 적용한다면, 학습 과정은 훨씬 덜 효율적일 것입니다. SFT를 위한 데이터(data)는 합성적으로 생성되거나 인간에 의해 수동으로 생성됩니다. 일반적으로 SFT를 위한 데이터(data) 수집은 비용이 많이 듭니다(시간과 돈 모두). 우리는 LLM을 위해 처음부터 좋은 응답을 수동으로 작성해야 합니다! 추론 모델(reasoning models)의 경우 긴 CoT(long CoT) 때문에 이러한 SFT 데이터(data)를 수집하는 것이 더 어렵습니다. 인간에게 긴 CoT 데이터(data)를 수동으로 생성하도록 요청하는 것은 시간이 많이 걸리고 비용이 많이 들 것입니다! 우리의 유일한 선택은 이 데이터(data)를 합성적으로 생성하는 것이지만:
*   모델로 이러한 특정 스타일의 출력을 생성하는 것은 여전히 어려울 수 있습니다.
*   이러한 긴 출력을 올바르게 검증하는 것은 어렵습니다.

추론 모델(reasoning models)을 위한 SFT 데이터(data) 수집의 추가적인 복잡성을 고려할 때, [1]의 저자들은 먼저 SFT를 완전히 피하려고 시도합니다! 이러한 실험을 통해, 우리는 그러한 추론 능력(reasoning abilities)이 순수 RL로부터 자연스럽게 나타난다는 것을 알 수 있습니다. 이것은 놀라운 발견입니다! 그러나 결과 모델에는 몇 가지 단점(예: 언어 혼합)이 있습니다. RL 이전에 일부 SFT로 훈련할 때(즉, "콜드 스타트(cold start)"), 우리는 RL에 더 나은 사전 지식(prior)을 제공하며, 이는 i) RL 훈련의 초기 단계(phases) 동안 불안정성을 제거하고, ii) 훈련 속도를 높이며, iii) 모델 품질을 향상시킵니다. 따라서 SFT가 완전히 필수적인 것은 아니지만, 데이터(data)가 있다면 여전히 실용적으로 유용합니다!

## 증류된 모델(Distilled Models)

지식 증류(knowledge distillation) 과정의 그림 (출처)

DeepSeek-R1 외에도, [1]의 저자들은 R1에서 증류된(distilled) 일련의 밀집 모델(dense models)을 출시합니다. 증류 과정(distillation process)은 더 작고 효율적인 모델의 추론 능력(reasoning capabilities)을 크게 향상시키는 것으로 밝혀졌습니다. 전체 DeepSeek-R1 모델은 대규모이므로(즉, 6,710억 개의 매개변수(parameter)를 가진 전문가 혼합(Mixture-of-Experts) 모델), 이러한 증류된 모델(distilled models)은 실용적으로 유용합니다. R1과 비교할 만하지만 비용에 더 민감하고 사용하기 쉽습니다. 또한, 이러한 증류된 모델(distilled models)의 출시는 폐쇄형 추론 모델(closed reasoning models)(예: o1-mini 및 o3-mini)의 최근 추세와 일치합니다.

(출처: [1])

**R1 증류.** 이러한 모델을 만들기 위해, 우리는 두 가지 기본 모델(base models) 15(Qwen-2.5 [20] 및 LLaMA-3 [21])의 여러 크기로 시작합니다. 그런 다음 DeepSeek-R1의 훈련 파이프라인(training pipeline) 3단계에서 큐레이션(curated)된 80만 개의 지도 훈련 예시(supervised training examples)에 대해 SFT를 통해 기본 모델(base models)을 훈련합니다. 그게 전부입니다! 이것은 간단한 지식 증류 파이프라인(knowledge distillation pipeline)이지만, 결과는 인상적입니다. 위에 표시된 바와 같이, 증류된(distilled) Qwen2.5-14B 모델은 R1 출시 이전에 최고의 오픈 추론 모델(open reasoning model)이었던 QwQ-32B-Preview를 능가합니다. 또한, 가장 작은 증류된 모델(distilled models)조차 추론(reasoning)에 최적화되지 않은 표준 폐쇄형 LLM(예: GPT-4o)을 능가하며, 320억 및 700억 매개변수(parameter) 증류된 모델은 대부분의 벤치마크(benchmark)에서 o1-mini의 성능을 초과합니다. 이는 대규모 모델의 지능을 소규모 모델로 효과적으로 이전하는 증류 기술의 잠재력을 보여줍니다.

> “더 강력한 모델을 더 작은 모델로 증류하는 것은 훌륭한 결과를 낳는 반면, 대규모 RL에 의존하는 더 작은 모델은 엄청난 컴퓨팅 파워(computational power)를 필요로 하며 증류의 성능을 달성하지 못할 수도 있습니다.” - [1]에서

**증류(Distillation) 대 RL.** 위 논의에서 증류가 효과적임을 알 수 있지만, DeepSeek-R1이 사용하는 대규모 RL 훈련 과정(large-scale RL training process)을 이러한 더 작은 모델에 직접 적용함으로써 더 나은 결과를 얻을 수 있을지 궁금할 수 있습니다. 흥미롭게도, [1]의 저자들은 R1에서 Qwen2.5-32B 기본 모델(base model)을 증류하는 것(위에 설명된 증류 접근 방식 사용)이 이 모델을 대규모 RL을 통해 직접 훈련하는 것보다 더 나은 성능을 보인다는 것을 관찰합니다. 아래를 참조하십시오.

(출처: [1])

다시 말해, 대규모 모델이 발견한 추론 패턴(reasoning patterns)은 이러한 더 작고 밀집된 모델의 추론 능력(reasoning capabilities)을 향상시키는 데 중요합니다. 그러나 [1]의 저자들은 다음과 같은 추가적인 점들을 지적합니다:
*   증류된 모델(distilled models)의 성능은 RL을 통한 추가 훈련을 통해 더욱 향상될 수 있습니다.
*   "지능의 경계를 넘어서는 발전"(즉, DeepSeek-R1과 같은 모델의 성능을 능가하는 새로운 추론 모델(reasoning models) 생성)은 여전히 강력한 기본 모델(base models)과 대규모 RL 훈련을 필요로 할 것입니다.

**다른 증류된 추론 모델(distilled reasoning models).** 증류를 통해 고품질 추론 모델(reasoning models)을 훈련하는 단순성을 고려할 때, R1 제안 이후 연구 커뮤니티(research community)에서 다양한 추론 모델이 출시되었습니다. 가장 주목할 만한 출시작 중 일부는 다음과 같습니다:
*   Sky-T1 및 Sky-T1-Flash
*   Bespoke Stratos
*   LIMO S1
*   RedStar

더 많은 모델들이 출시되었습니다! 현재 추론 모델(reasoning model) 출시 속도는 LLM 연구의 포스트-LLaMA 시대(post-LLaMA era)를 연상시킵니다. 강력한 오픈 기본 모델(open base model)(즉, LLaMA) 출시 이후, 이 모델을 기반으로 한 다양한 모델 변형(model variants)(예: Alpaca, Vicuna, Koala 등 다수)이 출시되는 것을 보았습니다. 이제 우리는 강력한 오픈 추론 모델(open reasoning model)에 접근할 수 있으며, 매우 유사한 추세가 나타나고 있습니다! 이 분야의 연구는 매우 흥미롭고 별도의 게시물로 다룰 가치가 있습니다. 계속 지켜봐 주십시오!

## 주요 신흥 트렌드(Key Emerging Trends)

우리는 이제 o1 또는 o3와 같은 폐쇄형 모델(closed models)로 시작하여 DeepSeek-R1에서 이러한 모델의 완전히 설명된 복제(replication)로 끝나는 다양한 추론 모델(reasoning models)에 대해 배웠습니다. 이 연구에 대해 배우면서, 몇 가지 공통적인 추세가 나타나기 시작합니다. 아래에 설명된 이러한 추세는 추론 모델(reasoning models) 연구와 표준 LLM 연구 간의 몇 가지 중요한 차이점을 만듭니다.

**긴 CoT(Long CoT)(및 추론 시간 스케일링(inference-time scaling)).** 추론 모델(reasoning models)과 표준 LLM의 주요 차이점은 출력 구조(output structure)입니다. 단순히 최종 답변(선택적인 간결한 설명 포함)을 직접 생성하는 대신, 추론 모델(reasoning models)은 추론 과정(reasoning process)을 매우 상세하게 설명하는 긴 CoT(long CoT)를 생성합니다. 이 긴 CoT(long CoT)는 가변적인 길이를 가질 수 있어 추론 시간(inference time)에 제어 가능한 컴퓨팅 비용(compute costs)을 가능하게 합니다: **더 긴 CoT = 더 많은 토큰(token) = 더 많은 컴퓨팅 자원**. 이러한 방식으로, 추론 시간(inference time)에 더 많은 컴퓨팅 자원을 사용하는 것(더 긴 CoT를 생성함으로써)은 사용자가 모델의 추론 능력(reasoning capabilities)을 동적으로 향상시킬 수 있는 도구가 되었습니다. 이는 자원 효율적인 AI 시스템을 설계하는 데 중요한 패러다임 변화를 의미합니다.

**RL을 통한 자기 진화(Self-evolution).** 분명히, LLM이 긴 CoT(long CoT) 내에서 복잡한 추론 전략(reasoning strategies)을 실행하는 능력은 새롭고 흥미롭습니다. 최근 연구를 통해, 이러한 특별한 능력 개발의 핵심 기여자는 대규모 RL 훈련(large-scale RL training)이라는 것을 알 수 있습니다. [1]에서 우리는 모델이 올바르게 인센티브(incentivized)를 받는다면(일반적으로 결정론적이고 신뢰할 수 있는 규칙 기반 보상(rules-based rewards)을 통해), 그러한 추론 능력(reasoning capabilities)이 RL 중에 자연스럽게 나타난다는 것을 봅니다. 또한, RL을 통한 훈련에 더 많은 컴퓨팅 자원을 사용하여 모델의 추론 능력(reasoning capabilities)을 더욱 향상시킬 수 있습니다. 이것은 우리가 활용할 수 있는 또 다른 스케일링 법칙(scaling law)입니다!

**적은 감독.** 추론 모델(reasoning models)의 인간 감독(human supervision)에 대한 의존도는 표준 LLM에 비해 덜 두드러집니다. 특히, RL 훈련 중 보상(rewards)은 인간 선호도(human preferences)에 의존하는 대신 주로 규칙 기반 시스템(rules-based systems)에서 파생됩니다. 물론, 추론 모델(reasoning models)은 여전히 인간 감독(human supervision)에 의존하는 여러 영역을 가지고 있습니다. 예를 들어, 기본 모델(base model)은 인간이 큐레이션(curated)한 데이터(data)로 훈련되고 검증은 인간이 제공한 정답 레이블(ground truth labels)에 의존합니다. 그러나 R1(특히 R1-Zero)과 같은 추론 모델(reasoning models)은 추론 능력(reasoning capabilities)이 자율적으로 개발될 수 있음을 보여주기 위해 여전히 큰 노력을 기울이고 있습니다. 이는 궁극적으로 AI가 스스로 학습하고 발전하는 방향으로 나아가고 있음을 시사합니다.

**증류(Distillation)는 효과적입니다.** 이제 우리는 크고 강력한 추론 모델(reasoning models)에 접근할 수 있으므로, 간단한 전략을 사용하여 이러한 모델의 능력을 더 작고 밀집된 모델(dense models)로 증류할 수 있습니다! 이 발견은 이 분야의 연구 폭발로 이어졌으며, 가까운 미래에 훨씬 더 효율적이고 증류된 추론 모델(distilled reasoning models)이 많이 출시될 것으로 예상됩니다. 이 분야의 한 가지 핵심 질문은 더 작은 모델이 일반화될지, 아니면 그들의 "교사" 모델의 폭넓은 능력을 완전히 따라잡는 데 어려움을 겪을지 여부입니다.

> “DeepSeek-R1을 평가할 때, 우리는 이 모델이 프롬프트(prompt)에 민감하다는 것을 관찰했습니다. 소수점 프롬프트(few-shot prompting)는 지속적으로 성능을 저하시킵니다.” - [1]에서

**해결해야 할 새로운 문제.** 무엇보다도, 추론 모델(reasoning models)의 등장은 우리가 해결해야 할 다양하고 새로운 (그리고 흥미로운!) 질문들을 제기했습니다:
*   긴 CoT(long CoT)에 대한 안전 훈련(safety training)을 어떻게 처리해야 할까요? 이는 AI의 책임 있는 개발 및 배포에 필수적입니다.
*   일반/추론 능력(reasoning capabilities) 사이의 최적의 균형은 무엇일까요? 특정 작업에 특화된 능력과 범용 지능 사이의 균형을 찾는 것이 중요합니다.
*   추론 모델(reasoning models) 훈련에서 SFT의 최적 역할은 무엇일까요? 지도 학습과 강화 학습의 시너지를 극대화하는 방법을 탐색해야 합니다.
*   긴 CoT(long CoT)에서 "과도한 사고(overthinking)"를 어떻게 최소화할까요? 효율성과 정확성 사이의 균형을 유지하는 것이 중요합니다.
*   추론 모델(reasoning models)의 효율적인 호스팅(hosting)을 어떻게 처리해야 할까요? 실용적인 배포를 위한 인프라 및 비용 문제가 대두됩니다.

이 게시물 시작 부분에서 언급했듯이, 추론 모델(reasoning models)은 기존 프레임워크(framework)를 재고하게 만들 진정으로 새로운 유형의 LLM입니다. 수년 동안 사용되어 온 확고한 기술(예: 소수점 프롬프트(few-shot prompting))은 이러한 새로운 모델에는 구식이 되고 있습니다. LLM 연구 분야는 다시 한번 스스로를 재창조하고 있습니다. 이는 인공지능이 끊임없이 진화하고 있으며, 우리의 기대치를 지속적으로 뛰어넘고 있음을 보여줍니다. 추론의 시대는 AI가 단순히 도구가 아니라, 복잡한 세상에서 사고하고 학습하는 동반자가 될 수 있는 미래를 약속합니다.

뉴스레터가 처음이신가요? 안녕하세요! 저는 **Cameron R. Wolfe**입니다. 넷플릭스(Netflix)의 딥러닝(Deep Learning) 박사이자 머신러닝 과학자(Machine Learning Scientist)입니다. 이것은 독자들이 AI 연구의 중요한 주제를 더 잘 이해하도록 돕는 딥(러닝) 포커스 뉴스레터(Deep (Learning) Focus newsletter)입니다. 뉴스레터가 마음에 드셨다면, 구독하시고, 공유하시거나, X와 링크드인(LinkedIn)에서 저를 팔로우(follow)해 주세요!

[구독](https://www.crwolfe.com/subscribe)

## 참고문헌(Bibliography)

[1] Guo, Daya, et al. "Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning." arXiv preprint arXiv:2501.12948 (2025).
[2] Liu, Aixin, et al. "Deepseek-v3 technical report." arXiv preprint arXiv:2412.19437 (2024).
[3] Shao, Zhihong, et al. "Deepseekmath: Pushing the limits of mathematical reasoning in open language models." arXiv preprint arXiv:2402.03300 (2024).
[4] OpenAI. “Introducing OpenAI o1-preview” https://openai.com/index/introducing-openai-o1-preview/ (2024).
[5] OpenAI. “Learning to Reason with LLMs” https://openai.com/index/learning-to-reason-with-llms/ (2024).
[6] OpenAI. “OpenAI o3-mini” https://openai.com/index/openai-o3-mini/ (2025).
[7] Rein, David, et al. "Gpqa: A graduate-level google-proof q&a benchmark." arXiv preprint arXiv:2311.12022 (2023).
[8] Wei, Jason, et al. "Chain-of-thought prompting elicits reasoning in large language models." Advances in neural information processing systems 35 (2022): 24824-24837.
[9] Zelikman, Eric, et al. "Star: Bootstrapping reasoning with reasoning." Advances in Neural Information Processing Systems 35 (2022): 15476-15488.
[10] Gulcehre, Caglar, et al. "Reinforced self-training (rest) for language modeling." arXiv preprint arXiv:2308.08998 (2023).
[11] Nakano, Reiichiro, et al. "Webgpt: Browser-assisted question-answering with human feedback." arXiv preprint arXiv:2112.09332 (2021).
[12] Dubey, Abhimanyu, et al. "The llama 3 herd of models." arXiv preprint arXiv:2407.21783 (2024).
[13] Lambert, Nathan, et al. "Tulu 3: Pushing frontiers in open language model post-training." arXiv preprint arXiv:2411.15124 (2024).
[14] Bespoke Labs. “Bespoke-Stratos: The unreasonable effectiveness of reasoning distillation” https://www.bespokelabs.ai/blog/bespoke-stratos-the-unreasonable-effectiveness-of-reasoning-distillation (2025).
[15] Welleck, Sean, et al. "From decoding to meta-generation: Inference-time algorithms for large language models." arXiv preprint arXiv:2406.16838 (2024).
[16] Aggarwal, Pranjal, Bryan Parno, and Sean Welleck. "AlphaVerus: Bootstrapping formally verified code generation through self-improving translation and treefinement." arXiv preprint arXiv:2412.06176 (2024).
[17] Chen, Xinyun, et al. "Teaching large language models to self-debug." arXiv preprint arXiv:2304.05128 (2023).
[18] Wang, Yifei, et al. "A Theoretical Understanding of Self-Correction through In-context Alignment." arXiv preprint arXiv:2405.18634 (2024).
[19] Huang, Jie, et al. "Large language models cannot self-correct reasoning yet." arXiv preprint arXiv:2310.01798 (2023).
[20] Yang, An, et al. "Qwen2. 5 technical report." arXiv preprint arXiv:2412.15115 (2024).
[21] Dubey, Abhimanyu, et al. "The llama 3 herd of models." arXiv preprint arXiv:2407.21783 (2024).
[22] Shao, Zhihong, et al. "Deepseekmath: Pushing the limits of mathematical reasoning in open language models." arXiv preprint arXiv:2402.03300 (2024).

1 예를 들어, o1-preview는 파일을 업로드하는 기능이 없었고, 다른 데이터 양식(modalities of data)(예: 이미지)을 이해할 수 없었으며, 웹 검색 기능도 없었습니다.
2 OpenAI가 o1 스타일 모델이 사용하는 추론 시간 컴퓨팅(inference-time compute) 양을 어떻게 제어하는지에 대한 세부 사항은 명확하지 않지만, 그들의 블로그 게시물(blog post)에 따르면 이 모델들은 추론 시간(inference time)에 사용할 수 있는 컴퓨팅 양에 대해 여러 "설정"을 가지고 있는 것으로 보입니다. 이러한 설정은 모델의 긴 CoT(long CoT) 길이에 관련될 가능성이 높으므로, 높은 추론 시간 컴퓨팅(inference-time compute) 설정은 단순히 매우 긴 사고의 연쇄(chains of thought)를 생성할 것입니다.
3 기술적으로, o3가 85% 이상의 정확도를 달성할 때 최대 계산 예산(computational budget)을 초과했기 때문에 이 벤치마크(benchmark)는 여전히 깨지지 않은 상태입니다.
4 이 벤치마크(benchmark)는 테렌스 타오(Terence Tao)에 의해 AI 시스템이 "적어도 몇 년 동안" 해결하지 못할 가능성이 높다고 묘사되었습니다. OpenAI와 이 벤치마크(benchmark)를 만든 조직(EpochAI) 간의 이해 상충(conflict of interest)으로 인해 이 벤치마크(benchmark)에서 OpenAI의 성능에 대한 최근 의문이 제기되었습니다.
5 특히, o3-mini는 o1과 달리 비전 지원(vision support)이 없습니다.
6 대조적으로, RLHF는 일반적으로 순위 손실(ranking loss)을 통해 다양한 종류의 인간 선호도(human preferences)에 대해 보상 모델(reward model)을 훈련합니다.
7 이 두 가지 기술 외에도, 우리는 어떤 종류의 검색(search)(예: 몬테카를로 트리 검색(monte carlo tree search))을 수행할 수도 있습니다. 예시는 [여기](https://www.crwolfe.com/p/inference-time-algorithms-for-llms)를 참조하십시오. 그러나 우리는 검색 기반 방법(search-based methods)을 추론 시간(inference time)에 더 많은 토큰(token)을 생성하는 것으로 분류할 수도 있습니다.
8 긴 CoT(long CoT)의 길이는 모델 설정(예: OpenAI는 "추론 노력(reasoning effort)"에 대한 여러 설정을 제공) 또는 문제 난이도에 따라 달라질 수 있습니다.
9 DeepSeek-v1 모델도 있지만, 이 모델은 밀집형(dense)(즉, MoE가 아님)이며 DeepSeek-R1에 사용되는 모델 계열과는 많이 다릅니다.
10 컴퓨팅 절약은 어떤 보상 모델(reward models)도 훈련하거나 (추론을 실행할) 필요가 없다는 사실에서 비롯됩니다.
11 OpenAI의 o1 모델 전체 목록은 [여기](https://openai.com/index/learning-to-reason-with-llms/)를 참조하십시오. 명확히 하자면, [1]에서 언급된 o1-0912 모델은 o1-preview 모델과 동일합니다.
12 예를 들어, 이 모델은 답변 내에 마크다운 형식(markdown formatting) 및 강조 표시(highlighting)가 부족한데, 이는 현대 LLM의 일반적인 기능입니다.
13 [1]에서 저자들은 DeepSeek-R1 모델 변형(model variants)이 생성한 긴 CoT(long CoT) 출력을 "궤적(trajectories)"이라고 지칭합니다.
14 특히, 이는 OpenAI가 채택한 (원래) 접근 방식과 직접적으로 대조됩니다. o1 스타일 모델은 최종 사용자에게 긴 CoT(long CoT)를 숨기며, 이러한 추론 흔적(reasoning traces)은 어떤 안전 훈련(safety training)도 거치지 않습니다. 이 전략의 근거는 모델이 궤적(trajectory)에서 더 투명해지도록 하여 해석 가능성(interpretability)을 향상시키는 것입니다.
15 사용된 정확한 모델은 Qwen2.5-Math-1.5B, Qwen2.5-Math-7B, Qwen2.5-14B, Qwen2.5-32B, Llama-3.1-8B 및 Llama-3.3-70B-Instruct입니다. 특히, 우리는 항상 기본 모델(base model)로 시작하지 않습니다. 이 모델들 중 다수는 후처리 훈련(post training)을 거쳤습니다!