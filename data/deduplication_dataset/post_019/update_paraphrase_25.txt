거대 언어 모델(LLM) 분야는 초기 GPT 모델의 등장 이래로 현재의 고도화된 오픈 소스(open-source) LLM에 이르기까지 상당한 진보를 거듭해 왔습니다. 초창기에는 LLM 훈련 방식이 주로 사전 훈련(pre-training) 단계에만 초점을 맞추었으나, 점차적으로 사전 훈련과 더불어 사후 훈련(post-training)까지 아우르는 형태로 발전했습니다. 사후 훈련 과정은 보통 ChatGPT가 널리 알린 지도 학습 기반의 명령어 미세 조정(supervised instruction fine-tuning)과 모델 정렬(alignment) 작업을 포함합니다. ChatGPT의 최초 공개 이후, LLM 훈련 기법들은 지속적으로 발전해 왔습니다. 본 게시물에서는 특히 최근 수개월간 나타난 사전 훈련 및 사후 훈련 기법의 가장 최신 동향과 발전에 대해 심층적으로 분석하고자 합니다. 이러한 훈련 기법의 진화는 단순히 모델의 규모를 키우는 것을 넘어, 데이터의 질적 향상, 효율적인 자원 활용, 그리고 인간의 의도에 부합하는 응답 생성이라는 복합적인 목표를 달성하기 위한 끊임없는 연구의 결과입니다. 특히, 대중에게 공개된 LLM들의 경우, 기술 보고서에 담긴 훈련 파이프라인의 세부 사항은 해당 모델의 성능과 신뢰성을 이해하는 데 결정적인 역할을 합니다.

본 글은 새로운 사전 훈련 및 사후 훈련 기법에 중점을 둔 LLM 개발 및 훈련 파이프라인의 전반적인 개요를 제공합니다. 매월 수백 편에 달하는 LLM 관련 연구 논문들이 혁신적인 기술과 접근법을 쏟아내고 있습니다. 하지만 실질적으로 어떤 방식이 효과적인지 파악하는 가장 확실한 방법은, 최신 최고 성능 모델(state-of-the-art model)들의 사전 훈련 및 사후 훈련 파이프라인을 면밀히 검토하는 것입니다. 다행스럽게도 최근 몇 달 사이, 네 가지 주요 신규 LLM이 상당히 구체적인 기술 보고서(technical report)와 함께 공개되었습니다. 이러한 기술 보고서는 단순한 성과 지표 나열을 넘어, 모델 설계 철학, 데이터 처리 전략, 그리고 훈련 과정에서 발생한 도전과 해결책에 대한 귀중한 통찰을 제공합니다. 이는 학계와 산업계 모두에서 LLM 연구의 '블랙박스'를 해소하고, 재현 가능한 연구 환경을 조성하는 데 필수적인 요소로 작용합니다. 이 글에서는 아래에 나열된 모델들의 사전 훈련 및 사후 훈련 파이프라인에 집중하여 분석할 예정입니다:

*   Alibaba의 Qwen 2
*   Apple Intelligence Foundation Language Models
*   Google의 Gemma 2
*   Meta AI의 Llama 3.1

이 모델들은 arXiv.org에 게재된 각 기술 논문의 발행 시점을 기준으로 배열되었으며, 흥미롭게도 이는 알파벳 순서와도 부합합니다. 본 글은 필자가 여가 시간과 주말을 활용하여 공들여 준비한 결과물입니다. 이 글이 유익하다고 판단하시어 필자의 노력을 지지하고 싶으시다면, 필자의 저서를 구매하시거나 동료들에게 권유해 주시면 감사하겠습니다. 아마존에 남겨주시는 서평 또한 큰 힘이 될 것입니다! 이러한 정보 공유는 LLM 생태계의 건강한 성장을 촉진하고, 개발자들이 서로의 경험을 바탕으로 더 나은 모델을 구축할 수 있도록 돕습니다. 필자는 이러한 지식 공유의 가치를 믿으며, 독자 여러분의 관심과 지지가 더 많은 양질의 콘텐츠를 생산하는 원동력이 됩니다.

*   Build a Large Language Model (from Scratch)
*   Machine Learning Q and AI
*   Machine Learning with PyTorch and Scikit-Learn

'Build a Large Language Model (from Scratch)'는 파이토치(PyTorch)를 활용하여 LLM을 바닥부터 직접 구현하는 과정에 초점을 맞춘 심도 있는 서적으로, 사전 훈련부터 사후 훈련에 이르는 전 과정을 상세히 다룹니다. 이는 LLM의 본질을 깊이 있게 이해하는 데 가장 효과적인 방법 중 하나라고 평가할 수 있습니다. 'Machine Learning Q and AI'는 머신러닝의 기초 개념에 이미 익숙한 독자들에게 적합한 훌륭한 자료입니다. 이 책은 심층 신경망(deep neural network), 비전 트랜스포머(vision transformer), 다중 GPU 훈련 패러다임(multi-GPU training paradigm), LLM과 같은 중급 및 고급 주제들을 폭넓게 다룹니다. 'Machine Learning with PyTorch and Scikit-Learn'은 기계 학습, 딥러닝, 인공지능(AI) 분야를 아우르는 종합적인 안내서로서, 이론적 배경과 실제 구현 코드의 균형 잡힌 조합을 제공합니다. 이 분야에 처음 발을 들이는 모든 이들에게 더할 나위 없이 좋은 출발점이 될 것입니다. 각 서적은 필자의 오랜 연구와 실무 경험을 바탕으로, 독자들이 이론적 지식뿐만 아니라 실질적인 문제 해결 능력을 함양할 수 있도록 구성되었습니다. 특히, LLM의 급속한 발전 속도를 고려할 때, 기초부터 심화까지 체계적으로 학습하는 것은 변화하는 기술 환경에 적응하고 혁신적인 아이디어를 구현하는 데 필수적입니다.

---

1.  **Alibaba의 Qwen 2**

다른 주요 거대 언어 모델들과 견줄 만한 매우 강력한 역량을 지닌 LLM 모델 시리즈인 Qwen 2부터 논의를 시작합니다. 그러나 어떠한 연유에서인지 이 모델은 Meta AI, Microsoft, Google 등에서 공개한 오픈 소스(open-source) 모델들에 비해 대중적 인지도가 상대적으로 낮은 편입니다. 이러한 인지도 차이에도 불구하고, Qwen 2는 그 기술적 성취와 혁신적인 접근 방식으로 인해 LLM 연구 커뮤니티 내에서 상당한 주목을 받고 있습니다. 특히 비영어권 시장에서의 강력한 입지는 이 모델의 잠재력을 더욱 부각시킵니다.

1.1 **Qwen 2 개요**

Qwen 2의 기술 보고서(Technical Report)에 제시된 사전 훈련 및 사후 훈련 기법들을 상세히 검토하기에 앞서, 몇 가지 핵심적인 사양들을 간략하게 정리해 보겠습니다. Qwen 2 모델은 총 5가지 형태로 제공됩니다. 구체적으로는 0.5억, 1.5억, 7억, 720억 개의 매개변수(parameter)를 갖는 4종류의 일반(밀집형) LLM이 존재합니다. 이와 더불어, 570억 개의 매개변수를 지닌 전문가 혼합(Mixture-of-Experts) 모델도 있으며, 이 모델에서는 140억 개의 매개변수가 동시에 활성화됩니다. (이번 글에서는 아키텍처의 구체적인 내용이 핵심 주제가 아니므로, 전문가 혼합(Mixture-of-Experts) 모델에 대해 깊이 있게 다루지는 않을 것입니다. 그러나 간략히 설명하자면, 이 모델은 Mistral AI의 Mixtral과 유사한 구조를 가지면서도, 더 많은 활성화된 전문가(expert)를 포함하고 있습니다. 보다 심층적인 개요는 필자의 "Model Merging, Mixtures of Experts, and Towards Smaller LLMs" 글에 있는 Mixtral 아키텍처 부분을 참조하시기 바랍니다.) Qwen 2의 다양한 모델 크기는 개발자가 특정 애플리케이션의 요구사항과 컴퓨팅 자원에 맞춰 최적의 모델을 선택할 수 있도록 유연성을 제공합니다. 특히, 전문가 혼합(MoE) 모델의 도입은 효율적인 추론(inference)과 뛰어난 성능을 동시에 추구하는 최근 LLM 트렌드를 반영합니다. MoE 아키텍처는 특정 작업에 특화된 전문가들을 활용하여 모델의 전체 매개변수 수는 크지만, 실제 활성화되는 매개변수 수를 제한함으로써 연산 효율성을 높이는 장점이 있습니다.

Qwen 2 LLM이 가진 탁월한 특성 중 하나는 30개 이상의 언어를 아우르는 우수한 다국어 처리 능력(multilingual capability)입니다. 이 모델은 또한 놀라울 정도로 방대한 151,642개의 토큰 어휘(token vocabulary)를 보유하고 있습니다 (참고로, Llama 2는 32k 어휘를, Llama 3.1은 128k 토큰 어휘를 사용합니다). 일반적으로 어휘의 규모가 두 배 증가하면 입력에 필요한 토큰 수가 절반으로 줄어들어, 동일한 길이의 입력에 더 많은 정보를 담을 수 있게 됩니다. 이는 특히 표준 영어 어휘를 넘어서는 다국어 데이터나 코딩 관련 텍스트를 처리할 때 매우 유용합니다. 이러한 대규모 어휘는 토크나이저(tokenizer)가 희귀 단어나 전문 용어를 더 효율적으로 인코딩할 수 있게 하여, 결과적으로 모델의 표현력을 향상시키고 토큰화 과정에서 정보 손실을 최소화합니다. 특히, 아시아 언어와 같은 복잡한 문자 체계를 가진 언어의 경우, 풍부한 어휘는 언어적 뉘앙스를 포착하고 번역 및 다국어 이해 능력을 크게 향상시키는 데 기여합니다.

다음은 향후 논의할 다른 LLM들과 Qwen 2의 MMLU 벤치마크(benchmark) 점수를 간략히 비교한 것입니다. (MMLU는 객관식 형태의 벤치마크(multiple-choice benchmark)라는 점에서 분명한 한계가 존재하지만, 여전히 LLM의 전반적인 성능을 평가하고 보고하는 데 가장 널리 사용되는 방식 중 하나입니다.) MMLU(Massive Multitask Language Understanding)는 다양한 학문 분야의 지식과 추론 능력을 측정하기 위해 고안된 테스트로, 모델의 일반적인 지식 수준을 가늠하는 데 유용합니다. 그러나 실제 응용 시나리오에서는 창의적 글쓰기, 복잡한 문제 해결, 또는 특정 도메인에 대한 깊이 있는 이해와 같은 다양한 능력이 요구되므로, MMLU 점수만을 맹신하기보다는 다각적인 평가가 필요합니다.

[최신 오픈 소스(open-source) 모델의 MMLU 벤치마크 점수 – 높을수록 우수함. 이 그래프의 수치는 각 모델의 공식 연구 논문에서 인용되었습니다. MMLU 개념이 생소하신 분들을 위해, 필자의 최근 강연 46분 05초 부분에서 간략한 설명을 들으실 수 있습니다.]

1.2 **Qwen 2 사전 학습(Pre-training)**

Qwen 2 개발팀은 15억, 70억, 720억 매개변수(parameter) 모델들을 총 7조 개의 훈련 토큰(training token)을 사용하여 학습시켰으며, 이는 상당히 적절한 규모로 판단됩니다. 이를 다른 모델과 비교해 보면, Llama 2 모델은 2조 개의 토큰으로, Llama 3.1 모델은 15조 개의 토큰으로 훈련되었습니다. 주목할 만한 점은 5억 매개변수 모델의 경우 12조 개의 토큰으로 훈련되었다는 것입니다. 하지만 연구진은 이 과정에서 추가적인 성능 향상을 관찰하지 못했고, 늘어난 연산 비용이 그 가치를 정당화하지 못한다고 판단하여 다른 모델들에는 12조 개의 더 큰 토큰 데이터셋을 적용하지 않았습니다. 이러한 결정은 LLM 훈련에 있어서 데이터셋의 양적 확대가 반드시 성능 향상으로 이어지는 것은 아니며, 특정 시점 이후에는 수확 체감의 법칙이 적용될 수 있음을 시사합니다. 즉, 무조건 많은 데이터를 사용하는 것보다는 데이터의 품질, 다양성, 그리고 모델 아키텍처와의 조화가 더욱 중요할 수 있습니다. 이는 LLM 훈련의 비용 효율성을 고려할 때 매우 중요한 통찰을 제공합니다.

핵심적인 중점 영역 중 하나는 저품질 데이터를 걸러내기 위한 데이터 필터링 파이프라인(data filtering pipeline)을 고도화하고, 데이터의 다양성을 증진시키기 위한 데이터 혼합(data mixing) 전략을 강화하는 것이었습니다. 이 주제는 이후 다른 모델들을 분석할 때 다시 논의될 것입니다. 흥미로운 점은, 그들이 추가적인 사전 훈련 데이터를 생성하기 위해 Qwen 모델(구체적인 내용은 명시되지 않았지만, 이전 세대 Qwen 모델을 의미하는 것으로 추정됩니다)을 활용했다는 것입니다. 또한, 사전 훈련 과정에는 "인컨텍스트 학습(in-context learning) 및 명령어 추종 능력(instruction-following ability)을 개선하기 위한 다중 작업 명령어 데이터(multi-task instruction data)"가 포함되었습니다. 데이터 필터링과 혼합 전략은 LLM의 최종 성능에 지대한 영향을 미칩니다. 특히, 웹에서 수집된 방대한 데이터에는 편향되거나 유해한 정보, 혹은 단순히 품질이 낮은 내용이 많기 때문에, 이를 정교하게 정제하는 과정이 필수적입니다. 데이터 다양성 확보는 모델이 다양한 도메인과 스타일의 텍스트를 이해하고 생성할 수 있도록 돕습니다. 합성 데이터의 활용은 고품질의 특정 데이터가 부족할 때 유용한 대안이 될 수 있으며, 모델 스스로 학습 데이터를 생성하는 '자기 개선(self-improvement)'의 가능성을 열어줍니다.

더불어, 그들은 일반적인 사전 훈련에 이어서 장문맥 훈련(long-context training)이라는 두 단계에 걸쳐 훈련을 진행했습니다. 이 후반 단계에서는 "고품질의 긴 데이터"를 활용하여, 사전 훈련의 최종 부분에서 문맥 길이(context length)를 기존 4,096개 토큰에서 32,768개 토큰으로 대폭 확장했습니다. 이러한 다단계 훈련 접근 방식은 LLM이 단기적인 패턴 인식 능력과 더불어 장기적인 의존성(long-range dependencies)을 효과적으로 포착하고 이해하도록 설계되었습니다. 특히, 문맥 길이 확장은 복잡한 문서 분석, 긴 대화 요약, 또는 장문의 코드 이해와 같은 고난이도 작업에서 모델의 성능을 결정하는 핵심 요소입니다. '고품질의 긴 데이터'는 이러한 장문맥 능력을 효과적으로 학습시키는 데 필수적이며, 단순히 긴 텍스트를 사용하는 것을 넘어 내용의 일관성과 정보 밀도가 높은 데이터를 의미합니다.

[Qwen 2 사전 훈련(pre-training)을 위한 기술적 요약. "연속 사전 훈련(Continued pre-training)"은 연구진이 일반 사전 훈련으로 시작한 후 장문맥 연속 사전 훈련을 이어가는 2단계 사전 훈련 방식을 의미합니다. 안타깝게도 기술 보고서의 또 다른 한계점은 데이터셋에 대한 구체적인 정보가 충분치 않다는 점입니다. 따라서 본 글에서 특정 부분이 아주 상세하게 다뤄지지 못했다면, 이는 공개된 정보의 제약 때문임을 양해 바랍니다.]

데이터셋의 투명성은 LLM 연구의 중요한 과제 중 하나입니다. 많은 상용 모델의 경우, 학습 데이터셋 구성에 대한 자세한 정보가 영업 비밀로 간주되어 공개되지 않는 경향이 있습니다. 이는 모델의 편향성 분석, 재현성 검증, 그리고 학술적 발전을 저해할 수 있습니다. Qwen 2의 경우에도 데이터셋 구성에 대한 추가적인 정보 공개는 모델의 강점과 약점을 더 명확히 이해하는 데 큰 도움이 될 것입니다.

1.3 **Qwen 2 사후 학습(Post-training)**

Qwen 2 개발팀은 널리 사용되는 2단계 사후 훈련(post-training) 접근법을 채택했으며, 이는 500,000개의 예시 데이터를 활용하여 2 에포크(epoch) 동안 수행된 지도 명령어 미세 조정(SFT)으로 시작되었습니다. 이 초기 단계의 목적은 특정 시나리오에서 모델이 생성하는 응답의 정확성과 적절성을 향상시키는 데 있었습니다. 지도 명령어 미세 조정(SFT)은 모델이 특정 지시사항을 이해하고 그에 따라 적절한 출력을 생성하도록 학습시키는 데 필수적인 과정입니다. 초기 사전 훈련된 모델은 방대한 텍스트를 이해할 수 있지만, 사용자의 의도를 정확히 파악하고 따르는 데는 한계가 있습니다. SFT는 이러한 '의도 정렬(intent alignment)'을 가능하게 하며, 고품질의 명령어-응답 쌍 데이터셋이 이 과정의 성공을 좌우합니다.

[일반적인 LLM 개발 과정.]

SFT를 마친 후, 그들은 직접 선호 최적화(DPO) 기법을 활용하여 LLM을 인간의 선호도에 맞춰 정렬(align)시켰습니다. (흥미롭게도, 그들의 보고서에서는 이를 인간 피드백 기반 강화 학습(RLHF)의 일종으로 지칭합니다.) 필자가 몇 주 전 작성한 "LLM 사전 훈련 및 보상 모델 평가 팁(Tips for LLM Pretraining and Evaluating Reward Models)"이라는 글에서 언급했듯이, SFT와 DPO를 결합한 접근 방식은 PPO(Proximal Policy Optimization)를 사용하는 RLHF와 같은 다른 방법론들에 비해 구현의 용이성 덕분에 현재 가장 각광받는 선호 조정 전략으로 자리매김하고 있습니다. (DPO의 작동 원리를 더 깊이 이해하고 싶으시다면, 필자가 최근 이곳에서 DPO를 처음부터 구현한 내용을 참고할 수 있습니다.) DPO는 강화 학습(RL)의 복잡한 절차 없이도 인간의 선호 데이터를 직접 활용하여 모델을 조정하는 효율적인 방법으로 각광받고 있습니다. PPO 기반 RLHF가 보상 모델 학습, 정책 모델 학습 등 여러 단계를 거치는 반면, DPO는 선호 쌍 데이터를 직접 손실 함수에 반영하여 학습을 간소화합니다. 이러한 간결함은 훈련 과정을 안정화하고 필요한 컴퓨팅 자원을 절감하는 데 기여하며, 이는 특히 소규모 연구팀이나 자원 제약이 있는 환경에서 큰 장점으로 작용합니다.

모델 정렬(alignment) 과정 자체도 두 단계로 나뉘어 수행되었습니다. 첫째, 기존 데이터셋에 DPO를 적용하는 오프라인 단계였습니다. 둘째, 보상 모델(reward model)을 활용하여 선호 쌍을 구성하는 온라인 단계였습니다. 이 단계에서는 모델이 훈련 중에 다양한 응답을 생성하고, 보상 모델이 "실시간"(즉, 훈련이 진행되는 동안)으로 최적화에 가장 적합한 선호 응답을 선별합니다. 이러한 방식은 종종 "거부 샘플링(rejection sampling)"으로도 알려져 있습니다. 거부 샘플링은 모델이 생성할 수 있는 다양한 응답 중에서 가장 품질이 높거나 인간의 선호에 부합하는 응답을 선별하여 학습에 활용하는 기법입니다. 이는 모델이 단순히 그럴듯한 응답을 생성하는 것을 넘어, 실제로 유용하고 바람직한 응답을 생성하도록 유도하는 데 효과적입니다. 보상 모델은 이 과정에서 핵심적인 역할을 수행하며, 인간의 피드백을 통해 학습되어 어떤 응답이 더 좋은지 평가하는 기준이 됩니다. 온라인 방식의 거부 샘플링은 모델이 훈련 과정에서 지속적으로 개선될 수 있는 동적인 피드백 루프를 제공합니다.

데이터셋을 구축하는 과정에서 그들은 기존 코퍼스(corpus)에 인간 라벨링(human labeling)을 추가하여 SFT를 위한 목표 응답을 명확히 하고, DPO에 필수적인 선호 및 거부 응답을 구별했습니다. 연구진은 또한 인공적으로 주석이 부여된 데이터(artificially annotated data)를 합성하는 작업도 수행했습니다. 나아가, Qwen 개발팀은 LLM 자체를 활용하여 "고품질 문학 데이터(literary data)"에 특화된 명령어-응답 쌍(instruction-response pair)을 만들어, 훈련에 사용될 고품질 Q&A 쌍(Q&A pair)을 구축했습니다. 인간의 전문적인 판단이 담긴 라벨링은 모델이 미묘한 뉘앙스를 이해하고 복잡한 지시를 따르는 데 결정적인 역할을 합니다. 그러나 인간 라벨링은 시간과 비용이 많이 소요된다는 단점이 있습니다. 이러한 한계를 극복하기 위해 인공적으로 주석이 달린 데이터를 합성하거나, 기존 LLM을 사용하여 새로운 명령어-응답 쌍을 생성하는 방식이 점점 더 중요해지고 있습니다. 특히, '고품질 문학 데이터'와 같은 특정 도메인에 특화된 Q&A 쌍을 생성하는 것은 모델이 해당 분야에서 심층적인 이해와 창의적인 응답 능력을 발휘하도록 돕습니다. 이는 모델의 전문성을 강화하고 특정 산업 분야에서의 활용 가능성을 높이는 전략입니다.

[Qwen 2 사후 훈련(post-training)을 위한 기술적 요약.]

Qwen 2의 사후 훈련 파이프라인은 SFT와 DPO, 그리고 거부 샘플링을 결합함으로써 모델의 성능과 사용자 만족도를 동시에 최적화하려는 노력을 보여줍니다. 이러한 다각적인 접근 방식은 단순한 지식 전달을 넘어, 사용자의 의도를 정확히 파악하고 윤리적이며 유용한 응답을 생성하는 '정렬된(aligned)' LLM을 만드는 데 필수적입니다.

1.4 **결론**

Qwen 2는 상대적으로 뛰어난 역량을 가진 모델이며, 이전 세대 Qwen의 특성을 계승하고 있습니다. 2023년 12월 NeurIPS LLM 효율성 챌린지(challenge)에 참여했을 당시, 우승을 차지한 대부분의 접근법들이 Qwen 모델을 활용했던 것이 기억에 남습니다. Qwen 2의 훈련 파이프라인(training pipeline)에서 특히 주목할 만한 점은 사전 훈련(pre-training)과 사후 훈련(post-training) 양쪽에 걸쳐 합성 데이터(synthetic data)가 적극적으로 활용되었다는 사실입니다. 또한, 무조건적으로 많은 데이터를 확보하는 것보다는 데이터셋의 정교한 필터링에 집중하는 경향이 LLM 훈련의 중요한 흐름 중 하나로 부상하고 있습니다. 이 맥락에서 "양이 많을수록 좋다"는 말은 특정 품질 기준을 충족하는 경우에만 유효하다고 할 수 있습니다. Qwen 2가 보여주는 합성 데이터의 성공적인 활용은 고품질의 학습 데이터를 구축하는 새로운 패러다임을 제시합니다. 전통적으로 데이터 수집에 많은 자원이 투입되었으나, 합성 데이터는 비용 효율적으로 데이터를 확장하고 특정 시나리오에 맞는 맞춤형 데이터를 생성할 수 있는 잠재력을 가집니다. 이는 특히 희귀하거나 민감한 데이터가 필요한 분야에서 LLM의 활용 가능성을 넓히는 중요한 진전으로 평가됩니다. 데이터 필터링의 강조는 LLM의 성능이 단순히 데이터의 양뿐만 아니라 그 질에 의해 크게 좌우된다는 인식을 반영하며, '쓰레기를 넣으면 쓰레기가 나온다(Garbage In, Garbage Out)'는 원칙이 LLM 훈련에서도 여전히 유효함을 보여줍니다.

**직접 선호 최적화(Direct Preference Optimization)를 통한 LLM 정렬: 처음부터 배우기**

직접 선호 최적화(DPO)는 LLM을 사용자 선호도에 더욱 가깝게 정렬(align)시키는 핵심적인 방법론 중 하나로 부상했으며, 본 글에서도 여러 차례 언급되었습니다. DPO의 작동 방식에 대해 더 깊이 배우고 싶으시다면, 필자가 이곳에서 DPO를 처음부터 직접 코딩한 내용을 참고하시기 바랍니다: [LLM 정렬을 위한 직접 선호 최적화(DPO) (처음부터)](https://www.aheadofai.com/p/direct-preference-optimization-dpo-for-llm-alignment-from-scratch).

[LLM 정렬을 위한 DPO 개요]

'Ahead of AI'는 독자 여러분의 지원으로 운영되는 출판물입니다. 새로운 게시물을 받아보시고 필자의 작업을 지지하고자 하신다면, 무료 또는 유료 구독자가 되어주시기를 고려해 주십시오. [구독](https://www.aheadofai.com/subscribe)

---

2.  **Apple의 Apple Intelligence Foundation Language Models (AFM)**

arXiv.org를 통해 애플(Apple)의 모델 훈련 과정을 상세히 다룬 또 하나의 기술 논문(technical paper)을 접하게 되어 매우 반가웠습니다. 이는 예상 밖의 일이었으나, 분명히 긍정적인 놀라움으로 다가왔습니다! 애플과 같은 거대 기술 기업이 LLM 개발에 대한 내부 방법론을 공개하는 것은 흔치 않은 일이며, 이는 해당 분야 연구자들에게 귀중한 통찰력을 제공합니다. 특히 'Apple Intelligence'라는 새로운 비전과 함께 공개된 이 모델들은 온디바이스(on-device) AI의 미래를 엿볼 수 있게 해준다는 점에서 그 의미가 더욱 큽니다.

2.1 **AFM 개요**

[Apple Intelligence Foundation Language Models 논문](https://arxiv.org/abs/2407.03052)에서 연구팀은 애플 기기의 "Apple Intelligence" 생태계 내에서 활용될 목적으로 개발된 두 가지 핵심 모델의 발전 과정을 설명합니다. 글의 간결성을 위해 본 섹션 전반에 걸쳐 이 모델들은 "Apple Foundation Models"의 약어인 AFM으로 통칭될 것입니다. 구체적으로, 해당 논문은 AFM의 두 가지 변형을 소개하는데, 하나는 휴대폰, 태블릿, 노트북 등 모바일 기기에 탑재될 30억 매개변수(parameter) 온디바이스(on-device) 모델이며, 다른 하나는 크기가 명시되지 않은 더욱 강력한 서버 기반 모델입니다. 이 모델들은 채팅, 수학, 코딩 작업에 특화되어 개발되었으나, 논문에서는 코딩 관련 훈련 및 기능에 대한 자세한 내용은 다루지 않습니다. Qwen 2와 마찬가지로 AFM은 밀집형 LLM이며, 전문가 혼합(mixture-of-experts) 방식은 채택하지 않았습니다. 온디바이스 LLM의 중요성은 개인 정보 보호, 낮은 지연 시간(latency), 그리고 오프라인 기능 제공 능력에서 비롯됩니다. 특히 애플의 경우, 사용자 데이터 보호에 대한 강한 철학을 고려할 때, 기기 내에서 직접 AI 처리가 이루어지는 온디바이스 모델의 개발은 필연적인 선택으로 보입니다. 비록 MoE와 같은 최신 아키텍처 혁신을 직접적으로 도입하지는 않았지만, 밀집형 모델의 최적화에 집중함으로써 안정적이고 효율적인 성능을 확보하려 한 것으로 해석됩니다. 이는 특정 하드웨어 환경에 최적화된 AI 솔루션을 제공하려는 애플의 전략과도 일치합니다.

2.2 **AFM 사전 학습(Pre-training)**

연구진에게 두 가지 중요한 칭찬을 하고 싶습니다. 첫째, 그들은 공개적으로 접근 가능한 데이터와 출판사로부터 정식 라이선스(license)를 획득한 데이터를 활용하는 것 외에도, 웹사이트의 robots.txt 규약을 존중하며 무분별한 크롤링(crawling)을 자제했습니다. 둘째, 벤치마크 데이터(benchmark data)에 대한 오염 제거(decontamination) 작업을 성실히 수행했다고 명시했습니다. Qwen 2 논문의 핵심 시사점 중 하나를 다시 한번 강조하자면, 애플 연구진 또한 데이터의 양보다는 질이 훨씬 더 중요하다고 역설했습니다. (디바이스 모델의 어휘 크기는 49k 토큰, 서버 모델은 100k 토큰으로, 150k 토큰 어휘를 사용한 Qwen 2 모델보다 어휘 규모가 상당히 작았습니다.) robots.txt 존중과 데이터 오염 제거는 LLM 개발의 윤리적이고 방법론적인 측면에서 매우 중요한 모범 사례입니다. 무단 크롤링은 저작권 문제와 웹 생태계에 부담을 줄 수 있으며, 벤치마크 데이터 오염은 모델의 실제 성능을 과대평가하게 만들어 연구의 신뢰성을 훼손합니다. 애플의 이러한 접근 방식은 데이터의 출처와 처리 과정에 대한 투명성과 책임감을 보여주며, 이는 신뢰할 수 있는 AI 시스템을 구축하는 데 필수적인 요소입니다. 어휘 크기가 Qwen 2보다 작음에도 불구하고 성능을 확보하려 한 점은 효율적인 토큰화 전략과 고품질 데이터의 집중적인 활용을 시사합니다.

흥미롭게도, 사전 훈련은 두 단계가 아닌 세 단계로 진행되었습니다!

*   핵심(일반) 사전 훈련(pre-training)
*   웹 크롤링(web-crawl) 데이터(저품질)의 중요도를 낮추고, 수학 및 코드 데이터의 중요도를 높인 연속 사전 훈련(continued pre-training)
*   더 긴 시퀀스 데이터와 합성 데이터(synthetic data)를 활용한 문맥 확장(context-lengthening)

[AFM 모델이 거친 3단계 사전 훈련(pre-training) 과정에 대한 개요. 이제 이 세 단계를 좀 더 자세히 들여다보겠습니다.]

다단계 사전 훈련은 모델이 점진적으로 복잡한 능력을 습득하도록 설계된 정교한 전략입니다. 초기 단계에서는 광범위한 언어 이해 능력을 구축하고, 중간 단계에서는 특정 도메인(예: 수학, 코드)에 대한 전문성을 강화하며, 최종 단계에서는 장문맥 이해 능력을 극한으로 끌어올립니다. 이러한 계층적 접근 방식은 각 단계에서 최적화된 데이터와 훈련 기법을 적용함으로써, 모델이 전반적으로 균형 잡힌 고성능을 달성하도록 돕습니다. 특히, 저품질 웹 데이터의 가중치를 낮추고 특정 고품질 데이터의 가중치를 높이는 것은 데이터 믹싱 전략의 중요성을 보여주는 대표적인 예입니다.

2.2.1 **사전 학습(Pre-training) I: 핵심 사전 학습(Core Pre-training)**

핵심 사전 훈련은 애플의 사전 훈련 파이프라인에서 가장 첫 번째 단계를 구성합니다. 이 단계는 일반적인 사전 훈련과 유사한 방식으로 진행되며, AFM-서버 모델은 6.3조 개의 토큰, 4096의 배치 크기(batch size), 그리고 4096 토큰의 시퀀스 길이(sequence length)를 사용하여 훈련되었습니다. 이는 7조 개의 토큰으로 훈련된 Qwen 2 모델의 규모와 상당히 흡사합니다. 이 초기 단계의 목표는 모델이 대규모 텍스트 데이터에서 광범위한 언어 패턴, 문법, 상식 등을 학습하여 견고한 언어 기반을 마련하는 것입니다. 배치 크기, 시퀀스 길이, 총 토큰 수와 같은 매개변수들은 모델의 훈련 안정성과 효율성에 직접적인 영향을 미칩니다. 특히, 대규모 배치 크기는 훈련 속도를 높이는 데 도움이 되지만, 때로는 최적화 과정에서 수렴(convergence) 문제를 야기할 수도 있어 신중한 조정이 필요합니다.

그러나 AFM 온디바이스(on-device) 모델의 사례는 더욱 주목할 만한데, 이 모델은 더 큰 64억 매개변수(parameter) 모델로부터 증류 및 가지치기(distilled and pruned)를 통해 파생되었습니다(이전 문단에서 언급된 AFM-서버 모델과 유사하게 처음부터 훈련됨). 증류 과정에 대한 상세한 설명은 "목표 레이블(target label)을 실제 레이블과 교사 모델(teacher model)의 최상위 1개 예측(교사 레이블에 0.9의 가중치를 부여)의 볼록 조합(convex combination)으로 대체하여 증류 손실(distillation loss)이 적용된다"는 내용 외에는 많지 않습니다. 지식 증류(knowledge distillation)는 LLM 사전 훈련에서 점차 보편화되고 유용성이 증대되고 있는 추세라고 생각합니다(Gemma-2 또한 이 기법을 활용합니다). 언젠가 이 주제에 대해 더 심층적으로 다룰 계획입니다. 현재로서는 이 과정이 대략적으로 어떻게 작동하는지에 대한 간략한 개요를 제공합니다. 지식 증류는 대규모의 복잡한 모델(교사 모델)의 지식을 작고 효율적인 모델(학생 모델)로 전이시키는 기술입니다. 이는 온디바이스 AI와 같이 자원 제약이 있는 환경에서 고성능 모델을 배포하기 위한 핵심 전략입니다. 가지치기(pruning)와 함께 사용될 때, 증류는 모델의 크기를 크게 줄이면서도 성능 저하를 최소화하는 데 기여합니다. 특히, 교사 모델의 소프트 레이블(soft label)을 활용하는 것은 학생 모델이 단순히 정답을 맞추는 것을 넘어, 교사 모델이 학습한 복잡한 패턴과 불확실성까지 모방하도록 돕습니다. 이는 모델의 일반화 능력을 향상시키는 데 중요한 역할을 합니다.

[작은 모델(본 사례에서는 AFM-디바이스 3B 모델)이 원래의 훈련 토큰과 더 큰 교사 모델(여기서는 6.4B 모델)의 출력 데이터를 사용하여 훈련되는 지식 증류(knowledge distillation)에 대한 개요. 그림 a)에 나타난 교차 엔트로피 손실(cross entropy loss)은 LLM 사전 훈련에 흔히 사용되는 훈련 손실 함수입니다. 일반 사전 훈련 단계의 구현에 대한 더 자세한 내용은 필자의 저서 "[Build a Large Language Model from Scratch](https://www.amazon.com/Build-Large-Language-Model-Scratch/dp/1032601700)" 5장을 참조하시기 바랍니다.]

교차 엔트로피 손실은 모델의 예측 분포와 실제 정답 분포 간의 차이를 측정하여 모델이 정답을 얼마나 잘 예측하는지 평가하는 데 사용됩니다. 지식 증류에서는 이 손실 함수를 확장하여 교사 모델의 예측 분포도 함께 고려함으로써, 학생 모델이 교사 모델의 '사고방식'을 모방하도록 유도합니다. 이를 통해 학생 모델은 단순히 정답을 맞추는 것을 넘어, 교사 모델의 복잡한 추론 과정까지 학습하여 성능을 극대화할 수 있습니다.

위에 기술된 지식 증류(knowledge distillation) 방식은 여전히 원본 데이터셋을 활용한 훈련을 포함합니다. 하지만 데이터셋의 훈련 토큰 외에도, 훈련될 모델(학생 모델(student model)이라 불림)은 더 큰 규모의 (교사) 모델로부터 추가적인 정보를 받게 되는데, 이는 지식 증류 없이 훈련하는 것보다 훨씬 풍부한 학습 신호를 제공합니다. 이 방식의 단점은 다음과 같습니다: 첫째, 더 큰 교사 모델을 사전에 훈련시켜야 하며, 둘째, 모든 훈련 토큰에 대해 더 큰 교사 모델을 사용하여 예측값을 계산해야 합니다. 이러한 예측값은 미리 계산해 둘 수도 있지만(상당한 저장 공간 요구), 훈련 과정 중에 실시간으로 계산할 경우 훈련 속도 저하를 초래할 수 있습니다. 지식 증류의 이러한 장점과 단점은 모델 개발 시 신중한 균형점을 찾도록 요구합니다. 특히, 교사 모델의 예측값을 미리 계산하는 '오프라인 증류'와 훈련 중에 계산하는 '온라인 증류' 사이의 선택은 사용 가능한 컴퓨팅 자원과 훈련 시간 목표에 따라 달라집니다. 대규모 LLM의 경우, 교사 모델의 예측값을 저장하는 데 필요한 저장 공간과 이를 계산하는 데 필요한 시간이 상당하기 때문에, 이러한 오버헤드를 관리하는 것이 중요한 기술적 과제가 됩니다. 그럼에도 불구하고, 작은 모델로 큰 모델의 성능을 모방할 수 있다는 점은 온디바이스 AI의 실현 가능성을 크게 높여줍니다.

2.2.2 **사전 학습(Pre-training) II: 연속 사전 학습(Continued Pre-training)**

연속 사전 훈련 단계는 1조 개의 토큰으로 구성된 데이터셋(핵심 사전 훈련 세트보다 5배 적은 규모)에서 문맥 길이(context length)를 4,096개에서 8,192개 토큰으로 확장하는 소규모 문맥 확장(context lengthening) 과정을 포함합니다. 하지만 이 단계의 주요 핵심은 수학 및 코드 관련 데이터에 중점을 둔 고품질 데이터 혼합(data mix)으로 훈련을 진행하는 것이었습니다. 흥미롭게도, 연구진은 이러한 특정 맥락에서는 증류 손실(distillation loss)이 효과적이지 않다는 사실을 발견했습니다. 특정 도메인에 대한 전문성 강화는 LLM이 범용적인 언어 이해를 넘어 특정 분야에서 고도로 정확하고 유용한 응답을 생성하도록 만드는 데 필수적입니다. 수학 및 코드 데이터의 가중치를 높인 것은 애플이 'Apple Intelligence'를 통해 제공하려는 기능, 즉 복잡한 계산이나 프로그래밍 작업을 지원하는 능력과 직접적으로 연결됩니다. 증류 손실이 유익하지 않았다는 발견은, 지식 증류가 모든 훈련 시나리오에 만능 해결책이 아니며, 특정 목표와 데이터 특성에 따라 최적의 훈련 기법이 달라질 수 있음을 시사합니다. 이는 LLM 훈련의 복잡성과 미묘한 차이를 보여주는 중요한 예시입니다.

2.2.3 **사전 학습(Pre-training) III: 문맥 확장(Context Lengthening)**

세 번째 사전 훈련 단계는 1,000억 개의 토큰(이는 두 번째 단계에서 활용된 토큰의 10%에 불과합니다)만을 포함하지만, 문맥 길이(context length)를 32,768개 토큰으로 더욱 크게 확장하는 데 중점을 둡니다. 이를 실현하기 위해 연구진은 합성 장문맥 Q&A 데이터(synthetic long-context Q&A data)를 사용하여 데이터셋을 보강했습니다. 장문맥 이해 능력은 LLM의 핵심적인 경쟁력 중 하나로, 긴 문서나 대화에서 중요한 정보를 놓치지 않고 맥락을 유지하는 데 필수적입니다. 합성 장문맥 Q&A 데이터의 사용은 실제 고품질 장문맥 데이터의 부족 문제를 해결하고, 모델이 특정 길이의 텍스트에서 질문에 답하는 능력을 체계적으로 학습하도록 돕습니다. 이는 모델이 단순히 긴 텍스트를 처리하는 것을 넘어, 그 안에서 복잡한 정보 추출 및 추론을 수행할 수 있도록 훈련하는 정교한 방법론입니다. 이러한 전략은 특히 법률 문서 분석, 의료 기록 요약, 장문의 기술 보고서 이해와 같은 응용 분야에서 모델의 가치를 크게 높일 수 있습니다.

[AFM 사전 훈련(pre-training)을 위한 기술적 요약.]

AFM의 다단계 사전 훈련 과정은 애플이 온디바이스 AI의 성능과 효율성을 동시에 추구하는 방식을 명확히 보여줍니다. 각 단계별로 최적화된 데이터와 훈련 목표를 설정함으로써, 애플은 자사 기기에 최적화된 LLM을 구축하려는 전략적 접근을 구사하고 있습니다. 이는 범용적인 성능을 추구하는 다른 모델들과는 차별화된, 특정 생태계에 깊이 통합된 AI 솔루션의 가능성을 제시합니다.

2.3 **AFM 사후 학습(Post-training)**

애플은 사전 훈련(pre-training) 단계와 유사하게 사후 훈련(post-training) 과정에서도 포괄적인 접근법을 채택한 것으로 관찰됩니다. 그들은 인간이 직접 주석을 단 데이터와 합성 데이터(synthetic data)를 모두 활용하면서, 데이터의 양보다는 질이 더욱 중요하다는 점을 역설했습니다. 흥미로운 점은, 미리 정해진 데이터 비율에 얽매이지 않고 다양한 실험을 통해 데이터 혼합(data mixture)을 정교하게 조정하여 최적의 균형점을 찾아냈다는 것입니다. 데이터 혼합의 미세 조정은 LLM의 성능을 최적화하는 데 있어 예술과 과학의 영역을 넘나드는 과정입니다. 각 데이터 소스가 모델에 미치는 영향을 면밀히 분석하고, 수많은 반복 실험을 통해 가장 효과적인 조합을 찾아내는 것은 상당한 시간과 자원을 요구합니다. 애플의 이러한 노력은 모델이 다양한 사용자 시나리오에서 일관되고 고품질의 응답을 생성하도록 보장하기 위한 깊이 있는 이해와 헌신을 보여줍니다. 특히, 합성 데이터의 활용은 인간 라벨링의 한계를 보완하고, 특정 사용 사례에 대한 모델의 응답을 정교하게 다듬는 데 기여합니다.

사후 훈련 단계는 지도 명령어 미세 조정(supervised instruction fine-tuning)과 여러 번의 인간 피드백 기반 강화 학습(RLHF)으로 이루어진 2단계 절차를 포함했습니다. 이 과정에서 특히 눈여겨볼 점은 애플이 RLHF 단계에 두 가지 혁신적인 알고리즘을 적용했다는 것입니다:

*   교사 위원회(Teacher Committee)를 활용한 거부 샘플링 미세 조정(iTeC)
*   미러 디센트 정책 최적화(Mirror Descent Policy Optimization)를 적용한 RLHF

본 글의 분량을 고려하여 이 방법론들의 기술적 세부 사항을 심층적으로 다루지는 않겠지만, 간략한 개요는 다음과 같습니다: RLHF는 모델을 인간의 가치와 선호도에 정렬시키는 데 필수적인 기술이지만, 그 구현은 매우 복잡하고 까다롭습니다. 애플이 새로운 알고리즘을 도입했다는 것은 기존 RLHF 기법의 한계를 극복하고 더욱 효율적이고 안정적인 정렬을 달성하려는 시도로 해석됩니다. 이러한 혁신은 LLM이 단순히 지시를 따르는 것을 넘어, 인간의 미묘한 의도와 윤리적 기준까지 이해하고 반영하는 데 중요한 역할을 합니다. 특히, 온디바이스 환경에서 이러한 정렬을 달성하는 것은 사용자 경험에 직접적인 영향을 미치므로, 새로운 알고리즘 개발의 필요성이 더욱 컸을 것입니다.

iTeC 알고리즘은 거부 샘플링(rejection sampling)과 다양한 선호 조정 기법(preference tuning technique)들을 통합합니다. 구체적으로는 지도 미세 조정(SFT), 직접 선호 최적화(DPO), 인간 선호 최적화(IPO), 그리고 온라인 강화 학습(online RL)이 포함됩니다. 애플은 단일 알고리즘에만 의존하지 않고, 각 접근 방식을 개별적으로 활용하여 모델을 훈련했습니다. 이후 이 모델들은 응답을 생성했고, 인간 작업자가 선호 레이블(preference label)을 부여하여 이 응답들을 평가했습니다. 이렇게 수집된 선호 데이터는 RLHF 프레임워크 내에서 보상 모델(reward model)을 반복적으로 훈련시키는 데 사용되었습니다. 거부 샘플링(rejection sampling) 단계에서는 여러 모델로 구성된 위원회(committee of models)가 다양한 응답을 생성했으며, 보상 모델이 그중 가장 우수한 응답을 선별했습니다. 이러한 위원회 기반 접근법은 상당히 복잡하지만, 관련 모델들의 비교적 작은 규모(대략 30억 매개변수)를 고려할 때 충분히 현실적으로 구현 가능했을 것으로 보입니다. Llama 3.1의 700억 또는 4050억 매개변수 모델과 같이 훨씬 더 큰 모델들로 이러한 위원회를 구성하는 것은 분명히 더 많은 난관에 부딪힐 것입니다. 교사 위원회(Teacher Committee) 방식은 단일 모델의 한계를 넘어 여러 모델의 장점을 결합하여 더욱 견고하고 다양한 응답을 생성하려는 시도입니다. 각 모델이 특정 영역에서 강점을 가질 수 있으므로, 이들을 통합하여 더 나은 '초인적인(superhuman)' 응답을 생성하고 이를 통해 보상 모델을 강화할 수 있습니다. 이는 RLHF의 '탐색(exploration)' 문제를 해결하고, 모델이 더 넓은 범위의 행동 공간을 탐색하도록 유도하는 데 도움이 됩니다. 하지만 이러한 복잡한 시스템을 대규모 모델에 적용하는 것은 계산 비용과 관리의 어려움 때문에 상당한 기술적 도전을 수반합니다.

두 번째 알고리즘인 미러 디센트(Mirror Descent)를 활용한 RLHF는 일반적으로 사용되는 PPO(근접 정책 최적화(Proximal Policy Optimization))보다 더 뛰어난 성능을 입증했기 때문에 채택되었습니다. 미러 디센트는 최적화 이론에서 유래한 방법론으로, 특히 제약 조건이 있는 최적화 문제에 강점을 가집니다. RLHF 맥락에서 미러 디센트가 PPO보다 효과적이라는 발견은, 모델의 행동 분포를 인간 선호도에 더 정교하게 정렬시키는 데 새로운 가능성을 제시합니다. PPO가 정책 업데이트의 안정성을 강조하는 반면, 미러 디센트는 특정 기하학적 구조를 활용하여 더 효율적인 정책 탐색과 수렴을 가능하게 할 수 있습니다. 이러한 알고리즘적 혁신은 LLM의 정렬 과정을 더욱 정교하게 만들고, 미세 조정된 성능을 달성하는 데 기여합니다.

[AFM 사후 훈련(post-training)을 위한 기술적 요약.]

애플의 사후 훈련 접근 방식은 다수의 정렬 기법을 통합하고 새로운 알고리즘을 탐구함으로써, 온디바이스 LLM이라는 특수한 환경에서 최적의 사용자 경험을 제공하려는 그들의 노력을 명확히 보여줍니다. 이는 단순한 성능 지표를 넘어, 사용자의 개인 정보 보호와 기기 내에서의 원활한 작동이라는 복합적인 요구사항을 충족시키기 위한 전략적 선택으로 해석될 수 있습니다.

2.4 **결론**

애플의 사전 훈련(pre-training) 및 사후 훈련(post-training) 접근법은 상당히 포괄적인 특성을 보이며, 이는 아마도 모델이 수백만, 심지어 수십억 대의 기기에 배포된다는 점을 고려할 때 매우 높은 위험 부담 때문일 것입니다. 하지만 이 모델들의 상대적으로 작은 규모를 감안하면, 30억 매개변수 모델이 가장 작은 Llama 3.1 모델 크기의 절반에도 미치지 못한다는 점을 고려할 때, 이처럼 광범위한 기술 적용이 현실적으로 가능해집니다. 핵심적인 특징 중 하나는 RLHF와 DPO 중에서 단순히 하나를 선택한 것이 아니라는 점입니다. 대신, 그들은 위원회(committee) 형태를 통해 다양한 선호 조정(preference-tuning) 알고리즘을 활용했습니다. 또한, 사전 훈련 과정의 일부로 Q&A 데이터를 명시적으로 사용했다는 점도 흥미로운데, 이는 필자가 이전 글인 "[명령어 사전 학습 LLM(Instruction Pretraining LLMs)](https://www.aheadofai.com/p/instruction-pretraining-llms)"에서 다루었던 내용과 일맥상통합니다. 전반적으로, 매우 새롭고 만족스러운 기술 보고서(technical report)였습니다. 애플의 사례는 LLM 개발에 있어 '모델 크기'와 '기술적 복잡성' 간의 상충 관계를 효과적으로 관리하는 방법을 보여줍니다. 온디바이스 환경이라는 제약 속에서 최고의 성능을 이끌어내기 위해, 그들은 데이터 필터링, 지식 증류, 다단계 훈련, 그리고 다중 정렬 알고리즘과 같은 정교한 기술 스택을 활용했습니다. 이는 단순히 대규모 모델을 구축하는 것을 넘어, 특정 목표와 환경에 최적화된 LLM 솔루션을 설계하는 데 있어 중요한 시사점을 제공합니다. 특히, Q&A 데이터를 사전 훈련에 통합하는 것은 모델이 초기 단계부터 질문 이해 및 답변 생성 능력을 내재화하도록 하여, 사후 훈련 단계의 효율성을 높이는 전략적 이점을 가집니다.

---

3.  **Google의 Gemma 2**

구글(Google)의 Gemma 모델은 최근 "[Gemma 2: Improving Open Language Models at a Practical Size](https://arxiv.org/abs/2407.13038)" 논문을 통해 상세히 소개되었습니다. 이어지는 개요 부분에서는 몇 가지 핵심적인 사실들을 간략히 정리한 후, Gemma 2의 사전 훈련(pre-training) 및 사후 훈련(post-training) 과정에 대해 논의할 예정입니다. Gemma 시리즈는 '책임감 있는 AI'를 표방하며 구글이 공개한 오픈 모델로서, 특히 접근성과 효율성에 중점을 두고 개발되었습니다. Gemma 2의 출시는 구글이 오픈 소스 LLM 생태계에 기여하려는 의지를 재확인시켜주며, 실용적인 규모에서 최첨단 성능을 달성하려는 노력을 엿볼 수 있게 합니다.

3.1 **Gemma 2 개요**

Gemma 2 모델은 20억, 90억, 270억 개의 매개변수(parameter)를 가진 세 가지 규모로 제공됩니다. 이 모델의 핵심 목표는 훈련 데이터셋의 규모를 필연적으로 확장하지 않으면서도, 상대적으로 작고 효율적인 LLM을 개발하는 기술을 깊이 탐구하는 데 있습니다. 특히 Gemma 2는 256k 토큰이라는 매우 큰 어휘 크기를 자랑합니다. 비교하자면, Llama 2는 32k 토큰 어휘를, Llama 3는 128k 토큰 어휘를 사용합니다. 또한 Gemma 2는 미스트랄(Mistral)의 초기 모델들과 유사하게 슬라이딩 윈도우 어텐션(sliding window attention) 메커니즘을 활용하여 메모리 사용량을 절감하는 것으로 파악됩니다. Gemma 2 아키텍처에 대한 더 상세한 내용은 필자의 이전 글 [Gemma 2 섹션](https://www.aheadofai.com/p/model-merging-mixtures-of-experts-and-towards-smaller-llms#%C2%A7gemma-2)을 참고하시기 바랍니다. Gemma 2의 설계 철학은 '실용적인 규모(practical size)'에서 최적의 성능을 끌어내는 데 초점을 맞추고 있습니다. 이는 대규모 모델만이 최첨단 성능을 달성할 수 있다는 기존의 통념에 도전하며, 자원 효율적인 LLM 개발의 중요성을 강조합니다. 대규모 어휘는 다양한 언어와 전문 용어를 효과적으로 처리할 수 있게 하여, 모델의 다목적성을 높입니다. 슬라이딩 윈도우 어텐션은 긴 문맥을 처리할 때 발생하는 계산 복잡성을 줄여, 더 긴 시퀀스 길이를 효율적으로 지원할 수 있게 하는 중요한 아키텍처적 개선입니다. 이러한 설계 선택은 Gemma 2가 다양한 하드웨어 환경에서 유연하게 배포될 수 있는 가능성을 열어줍니다.

3.2 **Gemma 2 사전 학습(Pre-training)**

Gemma 연구진은 작은 규모의 모델들조차도 종종 충분히 훈련되지 않은(undertrained) 상태로 남아있다고 주장합니다. 그러나 그들은 단순히 훈련 데이터셋의 규모를 확대하는 대신, 데이터 품질 유지에 중점을 두며 애플(Apple)의 접근법과 유사하게 지식 증류(knowledge distillation)와 같은 대안적인 기법을 통해 성능 향상을 이루어냈습니다. 270억 매개변수의 Gemma 2 모델은 처음부터 훈련되었지만, 더 작은 규모의 모델들은 앞서 언급된 애플의 방식과 유사하게 지식 증류(knowledge distillation)를 활용하여 훈련되었습니다. 구체적으로, 270억 모델은 13조 개의 토큰으로, 90억 모델은 8조 개의 토큰으로, 20억 모델은 2조 개의 토큰으로 훈련되었습니다. 또한, 애플의 접근법과 마찬가지로 Gemma 팀 역시 성능 개선을 위해 데이터 혼합(data mixture)을 최적화하는 데 주력했습니다. '불충분하게 훈련된' 모델이라는 개념은, 모델의 잠재력을 최대한 발휘하기 위해 단순히 더 많은 데이터를 주입하는 것을 넘어, 데이터의 효율적인 활용과 훈련 전략의 정교화가 필요하다는 점을 강조합니다. 지식 증류는 이러한 맥락에서 매우 강력한 도구로, 이미 학습된 대규모 모델의 '지혜'를 작은 모델에 전수하여 효율성을 극대화합니다. 이는 자원 제약이 있는 환경에서 고성능 AI를 구현하려는 구글의 전략과 부합합니다. 데이터 혼합의 최적화는 모델이 다양한 유형의 정보에서 견고한 학습을 할 수 있도록 보장하며, 이는 모델의 일반화 능력과 실용적 가치를 높이는 데 결정적입니다.

[Gemma 2 사전 훈련(pre-training)을 위한 기술적 요약.]

Gemma 2의 사전 훈련 접근 방식은 데이터의 '양'보다는 '질'과 '활용 효율성'에 초점을 맞춤으로써, 제한된 자원으로도 경쟁력 있는 LLM을 구축할 수 있음을 보여줍니다. 특히, 지식 증류를 통한 소규모 모델의 성능 강화는 오픈 소스 커뮤니티와 중소기업들에게 고성능 LLM을 개발하고 배포할 수 있는 현실적인 대안을 제시한다는 점에서 큰 의미를 가집니다.

3.3 **Gemma 2 사후 학습(Post-training)**

Gemma 모델의 사후 훈련(post-training) 과정에는 일반적인 지도 미세 조정(SFT)과 인간 피드백 기반 강화 학습(RLHF) 단계가 포함되었습니다. 명령어 데이터는 인간이 만든 내용과 합성적으로 생성된 내용이 혼합된 영어 전용 프롬프트 쌍(English-only prompt pair)을 활용했습니다. 특히 주목할 만한 점은, 응답이 주로 교사 모델(teacher model)에 의해 생성되었으며, SFT 단계에서도 지식 증류(knowledge distillation) 기법이 적용되었다는 것입니다. SFT 단계에서 지식 증류를 적용한 것은 매우 흥미로운 전략입니다. 이는 학생 모델이 단순히 명령어에 따르는 것을 넘어, 교사 모델이 생성한 고품질의 미세 조정된 응답 패턴과 스타일까지 학습하도록 유도합니다. 즉, 사후 훈련 단계에서도 교사 모델의 '전문성'을 학생 모델에 효과적으로 전수하여, 모델의 응답 품질과 일관성을 한층 더 높이려는 시도입니다. 합성 데이터의 활용은 이러한 SFT 데이터셋을 확장하고 다양성을 확보하는 데 중요한 역할을 합니다.

SFT 이후 그들의 RLHF 접근법에서 흥미로운 부분은, RLHF에 사용되는 보상 모델(reward model)이 정책(목표) 모델보다 10배나 더 크다는 점입니다. Gemma가 채택한 RLHF 알고리즘은 대체로 표준적인 형태를 띠지만, 한 가지 독특한 특징이 있습니다: 그들은 WARM(가중치 평균 보상 모델)(weight-averaged reward models)의 후속 개념인 WARP라는 기법을 통해 정책 모델들을 평균화합니다. 필자는 이전에 "Model Merging, Mixtures of Experts, and Towards Smaller LLMs"라는 글에서 이 방법론에 대해 상세히 다룬 바 있습니다. 보상 모델이 정책 모델보다 훨씬 크다는 점은, 보상 모델이 인간의 선호도를 더욱 정교하고 미묘하게 포착하여 정확하게 평가할 수 있도록 설계되었음을 의미합니다. 이는 RLHF 과정에서 정책 모델이 학습해야 할 '목표 신호'의 품질을 극대화하려는 전략입니다. WARP와 같은 모델 평균화 기법은 훈련 과정의 안정성을 높이고, 특정 훈련 단계에서 발생할 수 있는 과적합(overfitting) 문제를 완화하며, 최종 모델의 일반화 성능을 향상시키는 데 기여합니다. 이는 여러 훈련 스냅샷(snapshot)의 지식을 통합하여 더욱 견고한 모델을 구축하는 효과적인 방법입니다.

[Gemma 2 사후 훈련(post-training)을 위한 기술적 요약.]

Gemma 2의 사후 훈련 전략은 지식 증류를 SFT에 통합하고, 대규모 보상 모델과 WARP 같은 혁신적인 평균화 기법을 활용함으로써, 효율성과 성능을 동시에 추구합니다. 이는 구글이 LLM의 '실용적 배포'에 얼마나 중점을 두고 있는지를 명확히 보여주며, 복잡한 RLHF 과정을 안정적이고 효과적으로 구현하려는 노력을 반영합니다.

3.4 **결론**

Gemma 개발팀은 애플(Apple)과 유사하게 사전 훈련(pre-training)과 사후 훈련(post-training) 전반에 걸쳐 지식 증류(knowledge distillation) 기법에 깊이 집중하는 모습을 보입니다. 흥미롭게도, 그들은 다단계 사전 훈련 접근법을 사용하지 않았거나, 적어도 기술 논문에서 그 내용을 상세히 언급하지는 않았습니다. 지식 증류에 대한 이러한 집중은 구글이 제한된 컴퓨팅 자원 내에서 고성능 LLM을 구현하는 데 있어 이 기술의 잠재력을 높이 평가하고 있음을 시사합니다. 이는 특히 오픈 모델로서 더 많은 개발자들이 접근하고 활용할 수 있도록 모델의 효율성을 극대화하려는 전략과 일치합니다. 다단계 사전 훈련의 부재 또는 미언급은 그들이 초기 단계에서부터 데이터 품질과 증류를 통해 충분한 성능을 확보하려 했거나, 혹은 다른 형태의 '암묵적인' 다단계 전략을 사용했을 가능성도 배제할 수 없습니다.

다가오는 파이토치 컨퍼런스(conference)에서 기조 강연(keynote talk)을 맡게 되어 매우 기쁩니다. 필자에게는 첫 파이토치 컨퍼런스 참여이며, 커뮤니티 구성원들과 만나 최신 인공지능(AI) 및 LLM 개발 동향에 대해 논의할 수 있기를 기대하고 있습니다! 이러한 학술 및 산업 컨퍼런스는 LLM 분야의 최신 연구와 실용적 적용 사례를 공유하고, 전 세계 연구자 및 개발자들이 협력할 수 있는 중요한 장을 제공합니다. 필자는 이러한 교류를 통해 얻은 통찰을 바탕으로, 앞으로도 독자들에게 더욱 심도 있고 유익한 정보를 전달하기 위해 노력할 것입니다.

---

4.  **Meta AI의 Llama 3.1**

메타(Meta)의 라마(Llama) LLM의 새로운 버전 출시는 언제나 큰 화제를 불러일으킵니다. 이번에는 총 92페이지에 달하는 방대한 기술 보고서(technical report)인 "[The Llama 3 Herd of Models](https://arxiv.org/abs/2407.13038)"와 함께 공개되었습니다. 마지막으로, 이 섹션에서는 지난달에 발표된 네 번째 주요 대규모 모델 논문을 심층적으로 분석해보고자 합니다. 라마 시리즈는 오픈 소스 LLM의 발전과 대중화에 지대한 영향을 미쳐왔습니다. 메타가 이러한 상세한 기술 보고서를 공개하는 것은 연구 커뮤니티에 대한 기여이자, 투명성을 통해 LLM 기술의 발전을 가속화하려는 의지를 보여줍니다. 라마 3.1은 이전 버전의 성공을 기반으로, 성능과 접근성 모두를 향상시키려는 메타의 지속적인 노력을 반영합니다.

4.1 **Llama 3.1 개요**

메타는 거대한 4050억 매개변수(parameter) 모델을 공개함과 동시에, 기존의 80억 및 700억 매개변수 모델들을 업데이트하여 MMLU 성능을 소폭 개선했습니다. 4050억 매개변수 모델의 등장은 LLM 규모의 한계를 다시 한번 확장하며, 더욱 복잡하고 미묘한 언어 현상을 포착할 수 있는 잠재력을 시사합니다. 동시에 기존 모델들의 성능을 지속적으로 개선하는 것은, 다양한 컴퓨팅 환경과 사용 사례에 맞는 최적의 솔루션을 제공하려는 메타의 전략을 보여줍니다. MMLU 성능의 개선은 모델의 일반적인 지식과 추론 능력이 꾸준히 향상되고 있음을 나타냅니다.

[다양한 모델의 MMLU 벤치마크(benchmark) 성능.]

라마 3는 다른 최신 LLM들과 마찬가지로 그룹 쿼리 어텐션(group query attention)을 활용하지만, 놀랍게도 메타 AI는 슬라이딩 윈도우 어텐션(sliding window attention)이나 전문가 혼합(Mixture-of-Experts) 방식은 채택하지 않았습니다. 이는 라마 3.1이 상당히 전통적인 아키텍처를 유지하며, 구조적 혁신보다는 사전 훈련(pre-training) 및 사후 훈련(post-training) 방법론에 명확히 중점을 두었음을 의미합니다. 이전 라마 버전들과 마찬가지로 모델의 가중치(weight)는 공개적으로 접근 가능합니다. 또한, 메타는 라마 3 라이선스(license)를 갱신하여 이제 다른 모델들의 성능을 향상시키기 위한 합성 데이터(synthetic data) 생성이나 지식 증류(knowledge distillation) 목적으로 라마 3를 활용하는 것이 마침내 허용되었음을 밝혔습니다. 그룹 쿼리 어텐션(GQA)은 멀티-헤드 어텐션(multi-head attention)의 효율성을 개선한 방식으로, 특히 대규모 모델에서 추론 속도를 향상시키는 데 기여합니다. 슬라이딩 윈도우 어텐션이나 MoE를 사용하지 않은 결정은 메타가 안정적이고 잘 이해된 아키텍처를 기반으로, 데이터와 훈련 과정의 최적화를 통해 성능을 극대화하려는 전략을 선택했음을 보여줍니다. 이는 '아키텍처 혁신'보다는 '훈련 파이프라인의 정교함'이 LLM 성능 향상에 더 중요할 수 있다는 관점을 제시합니다. 라마 3 라이선스 정책의 변화는 오픈 소스 LLM 생태계에 큰 영향을 미칠 것입니다. 이는 라마 3를 기반으로 한 파생 모델 개발과 연구를 촉진하여, LLM 기술의 전반적인 발전을 가속화하는 중요한 전환점이 될 것입니다.

4.2 **Llama 3.1 사전 학습(Pre-training)**

라마 3는 라마 2의 1.8조 개 토큰보다 현저히 증가한 15.6조 개의 토큰 데이터셋을 사용하여 훈련되었습니다. 연구진은 라마 3가 최소 8개 언어를 지원한다고 명시했습니다(Qwen 2는 20개 언어를 처리할 수 있습니다). 라마 3의 주목할 만한 특징 중 하나는 OpenAI의 tiktoken 토크나이저(tokenizer)를 기반으로 개발된 128,000개의 어휘 규모입니다. (토크나이저의 성능에 대해 궁금하신 분들을 위해, 필자가 이곳에서 [간단한 벤치마크(benchmark) 비교](https://www.aheadofai.com/p/tokenizer-benchmark-gpt-4-llama-3-qwen-2)를 수행한 바 있습니다.) 15.6조 개에 달하는 방대한 훈련 데이터셋은 라마 3가 전례 없는 규모의 언어 패턴과 정보를 흡수했음을 의미합니다. 이는 모델의 일반화 능력, 지식 보유량, 그리고 다양한 작업에서의 견고성을 크게 향상시킵니다. Qwen 2에 비해 지원 언어 수가 적지만, 핵심 언어에 대한 깊이 있는 학습에 집중했을 가능성을 시사합니다. tiktoken 토크나이저의 채택은 효율적인 텍스트 인코딩과 디코딩을 가능하게 하여, 모델의 입력 및 출력 처리 성능을 최적화하는 데 기여합니다. 토크나이저의 선택은 LLM의 최종 성능과 효율성에 예상보다 큰 영향을 미칩니다.

사전 훈련(pre-training) 데이터의 품질을 관리하는 측면에서, 라마 3는 메타 AI의 fastText 및 RoBERTa 기반 분류기(classifier)와 같은 고속 분류기(fast classifier)를 활용하여 휴리스틱 기반 필터링(heuristic-based filtering)과 모델 기반 품질 필터링(model-based quality filtering)을 병행합니다. 이러한 분류기들은 훈련 과정에서 사용되는 데이터 혼합(data mix)의 문맥 범주(context category)를 식별하는 데도 유용하게 활용됩니다. 데이터 필터링은 LLM의 편향성을 줄이고, 훈련 데이터의 효율성을 높이는 데 핵심적인 과정입니다. 휴리스틱 기반 필터링은 미리 정의된 규칙을 사용하여 특정 유형의 데이터를 걸러내고, 모델 기반 품질 필터링은 별도의 분류 모델을 훈련하여 데이터의 품질을 평가하고 점수를 매깁니다. 이 두 가지 방식의 결합은 데이터 정제 과정을 더욱 정교하게 만들며, 고품질의 훈련 데이터셋을 구축하는 데 필수적입니다. 문맥 범주를 결정하는 것은 모델이 다양한 도메인에 걸쳐 균형 잡힌 학습을 할 수 있도록 데이터 혼합을 동적으로 조정하는 데 중요한 정보를 제공합니다.

라마 3의 사전 훈련은 세 가지 단계로 구분됩니다. 첫 번째 단계는 8k 문맥 창(context window)을 가진 15.6조 개의 토큰을 사용하여 표준적인 초기 사전 훈련을 수행합니다. 두 번째 단계는 사전 훈련을 지속하면서도 문맥 길이(context length)를 128k로 확장합니다. 마지막 단계는 모델의 성능을 추가적으로 개선하는 어닐링(annealing) 과정을 포함합니다. 이제 아래에서 각 단계를 더 자세히 들여다보겠습니다. 다단계 사전 훈련은 LLM 개발의 표준적인 접근 방식으로 자리 잡고 있으며, 각 단계는 특정 학습 목표를 가집니다. 초기 단계에서는 광범위한 언어 이해 능력을 구축하고, 이후 단계에서는 문맥 길이 확장과 같은 특정 기능적 개선에 집중합니다. 어닐링 단계는 모델이 특정 성능 목표에 도달하도록 미세 조정하는 최종 단계로, 마치 금속을 가열하여 더 강하게 만드는 과정과 유사하게 모델의 '강도'를 높이는 역할을 합니다. 이러한 체계적인 접근 방식은 모델이 복잡한 지식과 능력을 점진적으로 습득하도록 돕습니다.

4.2.1 **사전 학습(Pre-training) I: 표준(초기) 사전 학습(Standard (Initial) Pre-training)**

훈련 설정에서 그들은 4백만 개의 토큰으로 구성된 배치(batch)로 시작했으며, 각 배치 내에는 4096의 시퀀스 길이(sequence length)가 적용되었습니다. 이는 4백만이라는 숫자가 가장 가까운 정수로 반올림되었다고 가정할 경우, 대략 1024 토큰의 배치 크기(batch size)를 의미합니다. 초기 2억 5천 2백만 개의 토큰을 처리한 후, 그들은 시퀀스 길이를 8192로 두 배 확장했습니다. 훈련 과정이 더욱 진행되어 2.87조 개의 토큰을 처리하고 나서는, 배치 크기를 다시 두 배로 늘렸습니다. 더불어, 연구진은 훈련 전반에 걸쳐 데이터 혼합(data mix)을 고정적으로 유지하지 않았습니다. 대신, 모델의 학습 효율성과 성능을 최적화하기 위해 훈련 과정에서 사용되는 데이터 혼합을 동적으로 조정했습니다. 데이터 처리에 대한 이러한 동적 접근 방식(dynamic approach)은 모델이 여러 유형의 데이터에 걸쳐 일반화(generalize)하는 능력을 향상시키는 데 기여했을 것입니다. 훈련 과정에서 시퀀스 길이와 배치 크기를 동적으로 조정하는 것은 컴퓨팅 자원 활용을 최적화하고, 모델이 점진적으로 더 복잡한 패턴을 학습하도록 유도하는 고급 전략입니다. 초기에는 작은 시퀀스 길이로 빠르게 기본 패턴을 학습하고, 이후 긴 시퀀스로 확장하여 장기적인 의존성을 포착합니다. 데이터 혼합을 동적으로 조정하는 것은 모델이 특정 시점에 가장 필요한 데이터를 학습하도록 유도함으로써, 훈련 효율성을 극대화합니다. 이는 모델이 다양한 도메인과 작업에 걸쳐 견고한 성능을 발휘하도록 돕는 중요한 기술적 진보입니다.

4.2.2 **사전 학습(Pre-training) II: 문맥 확장(Context Lengthening)을 위한 연속 사전 학습(Continued Pre-training)**

문맥 창(context window)을 한 번에 확장한 다른 모델들과는 다르게, 라마 3.1의 문맥 확장(context lengthening)은 더욱 점진적인 접근법을 취했습니다. 이 단계에서 연구진은 8,000개 토큰에서 128,000개 토큰에 이르기까지 총 6개의 개별 단계를 거쳐 문맥 길이(context length)를 점진적으로 늘렸습니다. 이러한 단계별 증분(stepwise increment) 방식은 모델이 더 확장된 문맥에 더욱 부드럽게 적응할 수 있도록 기여했을 것입니다. 이 과정에 활용된 훈련 세트(training set)는 전체 데이터셋 규모의 약 5%에 해당하는 8,000억 개의 토큰을 포함했습니다. 점진적인 문맥 길이 확장은 모델이 갑작스러운 변화에 적응하는 데 겪을 수 있는 어려움을 줄이고, 안정적인 훈련을 가능하게 합니다. 이는 모델이 짧은 문맥에서 학습한 지식을 기반으로 점차 긴 문맥의 복잡성을 이해하도록 돕는 교육학적 접근과 유사합니다. 또한, 전체 데이터셋의 일부만을 사용하여 문맥 확장에 집중하는 것은, 해당 단계의 목표를 명확히 하고 훈련 자원을 효율적으로 사용하는 전략입니다. 장문맥 이해 능력은 LLM이 복잡한 질의응답, 문서 요약, 그리고 긴 대화 관리와 같은 실제 응용 시나리오에서 필수적인 성능을 발휘하도록 합니다.

4.2.3 **사전 학습(Pre-training) III: 고품질 데이터에 대한 어닐링(Annealing)**

세 번째 사전 훈련 단계에서는 연구진이 소량이지만 고품질의 혼합 데이터로 모델을 훈련시켰는데, 이는 벤치마크 데이터셋(benchmark dataset)에서의 성능을 향상시키는 데 효과적임을 확인했습니다. 예를 들어, GSM8K 및 MATH 훈련 세트(training set)에 대한 어닐링(annealing)은 해당 GSM8K 및 MATH 검증 세트(validation set)에서 현저한 성능 개선을 가져왔습니다. 논문의 3.1.3 섹션에서 연구진은 어닐링(annealing) 데이터셋의 규모가 400억 개의 토큰(전체 데이터셋 크기의 0.02%)이라고 밝혔습니다. 반면, 3.4.3 섹션에서는 실제 어닐링 작업이 4천만 개의 토큰(어닐링 데이터의 0.1%)에서만 수행되었다고 명시하고 있습니다. 어닐링은 훈련 과정의 마지막 단계에서 모델의 성능을 특정 목표에 맞춰 미세 조정하는 전략입니다. 특히, 수학 문제 해결 능력과 같은 특정 벤치마크에서 모델의 성능을 끌어올리는 데 효과적입니다. 매우 작은 규모의 고품질 데이터를 사용하여 이러한 최종 조정을 수행하는 것은, 모델이 이미 학습한 광범위한 지식을 바탕으로 특정 능력에 대한 '예리함'을 더하는 과정으로 볼 수 있습니다. 이는 LLM이 범용적인 능력을 갖추는 것을 넘어, 특정 전문 분야에서 뛰어난 역량을 발휘하도록 만드는 데 중요한 역할을 합니다.

[Llama 3.1 사전 훈련(pre-training)을 위한 기술적 요약.]

라마 3.1의 3단계 사전 훈련 전략은 대규모 데이터 학습, 점진적 문맥 확장, 그리고 목표 지향적 어닐링의 조합을 통해 모델의 전반적인 성능과 특정 능력 모두를 최적화하려는 정교한 접근 방식을 보여줍니다. 이는 LLM 개발이 단순히 데이터의 양을 늘리는 것을 넘어, 훈련 파이프라인의 각 단계에서 전략적인 결정을 내리는 복잡한 과정임을 시사합니다.

4.3 **Llama 3.1 사후 학습(Post-training)**

메타 AI 팀은 사후 훈련(post-training) 과정에서 지도 미세 조정(SFT), 거부 샘플링(rejection sampling), 그리고 직접 선호 최적화(DPO)를 포함하는 상대적으로 간결한 방법론을 적용했습니다. 그들은 PPO를 활용한 RLHF와 같은 강화 학습 알고리즘이 이러한 기법들에 비해 안정성이 떨어지고 확장하기 더 어렵다는 것을 확인했습니다. SFT 및 DPO 단계가 인간이 생성한 데이터와 합성 데이터(synthetic data)를 모두 통합하여 여러 차례에 걸쳐 반복적으로 진행되었다는 점은 주목할 만합니다. 더 자세한 내용을 설명하기에 앞서, 그들의 작업 흐름은 아래 그림에 나타나 있습니다. 라마 3.1 팀의 이러한 결정은 LLM 정렬에 있어 '복잡성'과 '실용성' 사이의 균형점을 찾는 중요한 시사점을 제공합니다. PPO 기반 RLHF는 이론적으로 강력하지만, 구현의 복잡성, 훈련의 불안정성, 그리고 대규모 모델에 대한 확장성 문제로 인해 실제 적용이 쉽지 않은 경우가 많습니다. 반면, SFT와 DPO는 상대적으로 간단하면서도 효과적인 정렬 결과를 제공할 수 있습니다. 특히, 여러 라운드에 걸쳐 데이터와 훈련을 반복하는 것은, 모델이 점진적으로 인간의 선호도와 더욱 긴밀하게 정렬되도록 만드는 정교한 전략입니다.

[라마 3.1 논문에 제시된 사후 훈련 절차를 설명하는 주석 달린 그림.]

이 그림은 라마 3.1이 사후 훈련 과정에서 얼마나 체계적이고 반복적인 접근 방식을 취했는지를 시각적으로 보여줍니다. 각 단계가 서로 어떻게 연결되고 피드백 루프를 형성하는지 이해하는 것은 모델의 최종 성능이 어떻게 달성되었는지 파악하는 데 중요합니다.

DPO를 활용했음에도 불구하고, 그들은 RLHF에서와 마찬가지로 보상 모델(reward model)을 개발했다는 점에 주목해야 합니다. 초기에는 사전 훈련 단계의 체크포인트(checkpoint)를 사용하여 인간이 직접 주석을 단 데이터로 보상 모델을 훈련시켰습니다. 이 보상 모델은 이후 거부 샘플링(rejection sampling) 과정에 활용되어 추가 훈련을 위한 적합한 프롬프트를 선별하는 데 기여했습니다. 각 훈련 라운드에서 그들은 보상 모델뿐만 아니라 SFT 및 DPO 모델에도 모델 평균화 기법(model averaging technique)을 적용했습니다. 이 평균화는 최근 모델과 이전 모델의 매개변수(parameter)를 통합하여 시간이 지남에 따라 성능을 안정화하고 동시에 향상시키는 것을 목표로 했습니다. 모델 평균화의 기술적 세부 사항에 관심 있는 독자들을 위해, 필자의 이전 글 "[Model Merging, Mixtures of Experts, and Towards Smaller LLMs](https://www.aheadofai.com/p/model-merging-mixtures-of-experts-and-towards-smaller-llms)"의 "모델 병합 및 가중치 평균 이해(Understanding Model Merging and Weight Averaging)" 섹션에서 이 주제를 다룬 바 있습니다. 보상 모델은 DPO와 같은 선호 학습 기법에서 인간의 피드백을 수치화하는 핵심적인 역할을 합니다. DPO를 사용하더라도 보상 모델을 개발하는 것은 거부 샘플링을 통해 고품질의 훈련 데이터를 생성하거나, 모델의 정렬 수준을 평가하는 데 필수적입니다. 모델 평균화는 훈련 과정의 불확실성을 줄이고, 최종 모델이 특정 지점에 과적합되는 것을 방지하며, 일반화 성능을 향상시키는 효과적인 방법입니다. 여러 훈련 단계의 지식을 통합함으로써, 모델은 더욱 견고하고 안정적인 성능을 발휘할 수 있습니다. 이는 LLM의 장기적인 안정성과 신뢰성을 확보하는 데 중요한 전략입니다.

요약하자면, 라마 3.1의 사후 훈련은 본질적으로 상대적으로 표준적인 SFT와 DPO 단계를 포함합니다. 하지만 이 단계는 여러 차례의 라운드를 통해 반복적으로 수행됩니다. 그리고 그들은 거부 샘플링(rejection sampling)을 위해 보상 모델(reward model)을 추가했는데, 이는 Qwen 2 및 AFM에서 사용된 방식과 유사합니다. 또한, Gemma처럼 모델 평균화(model averaging) 기법을 사용했지만, 이는 보상 모델뿐만 아니라 관련 모든 모델에 적용되었습니다. 라마 3.1의 사후 훈련 전략은 '단순함 속의 정교함'을 추구합니다. 기본적인 SFT와 DPO를 반복적으로 적용하고, 보상 모델 기반의 거부 샘플링을 통해 데이터 품질을 향상시키며, 모델 평균화를 통해 안정성을 확보하는 방식은 매우 실용적이고 효과적입니다. 이는 복잡한 강화 학습 기법이 항상 최적의 해답이 아닐 수 있으며, 잘 설계된 데이터 파이프라인과 안정적인 최적화 기법의 조합이 강력한 LLM을 만드는 데 충분하다는 것을 시사합니다.

[라마 3.1 사후 훈련(post-training)을 위한 기술적 요약.]

이 요약은 라마 3.1이 사후 훈련에서 다각적인 접근 방식을 채택했음을 보여줍니다. 각 요소가 상호 보완적으로 작용하여 모델의 정렬을 강화하고, 사용자 만족도를 높이는 데 기여합니다.

4.4 **결론**

라마 3 모델은 이전 라마 2 모델과 상당히 표준적이고 유사한 특성을 공유하지만, 몇 가지 주목할 만한 접근법을 포함하고 있습니다. 특히, 15조 개 토큰으로 이루어진 방대한 훈련 세트(training set)는 라마 3를 다른 모델들과 차별화하는 요소입니다. 흥미롭게도 애플의 AFM 모델처럼 라마 3도 3단계 사전 훈련(pre-training) 과정을 구현했습니다. 다른 최신 거대 언어 모델들과는 달리, 라마 3는 지식 증류(knowledge distillation) 기술을 사용하지 않고, 대신 보다 간결한 모델 개발 경로를 택했습니다. 사후 훈련(post-training)의 경우, 모델은 다른 모델들에서 인기를 얻었던 더 복잡한 강화 학습 전략 대신 직접 선호 최적화(DPO)를 활용했습니다. 전반적으로, 이러한 선택들은 더 단순하면서도 검증된 방법들을 통해 LLM 성능을 개선하는 데 중점을 둔다는 점에서 매우 흥미롭습니다. 라마 3의 전략은 '규모와 단순성'의 조화를 추구합니다. 방대한 데이터셋을 통해 강력한 기본 능력을 구축하고, 검증된 훈련 및 정렬 기법을 활용하여 안정적이고 효율적인 성능을 확보하는 것입니다. 지식 증류를 사용하지 않고도 최상위 성능을 달성한 것은, 모델의 본질적인 능력이 데이터의 양과 질, 그리고 훈련 과정의 최적화에서 비롯될 수 있음을 보여줍니다. DPO와 같은 간결한 정렬 기법의 선택은 모델 개발 및 배포의 복잡성을 줄이고, 더 넓은 범위의 사용자들이 LLM 기술에 접근할 수 있도록 하는 데 기여합니다. 이는 오픈 소스 LLM의 가치와 잠재력을 극대화하려는 메타의 비전과도 일치합니다.

---

5.  **주요 시사점**

본 글에서 심층적으로 분석한 알리바바(Alibaba)의 Qwen 2, 애플(Apple)의 파운데이션 모델(AFM), 구글(Google)의 Gemma 2, 그리고 메타(Meta)의 Llama 3 이 네 가지 모델로부터 우리는 어떤 중요한 통찰을 얻을 수 있을까요?

이 네 가지 모델은 모두 사전 훈련(pre-training) 및 사후 훈련(post-training) 과정에서 각기 다른 접근법을 채택하고 있습니다. 물론 일부 방법론은 서로 겹치는 부분이 존재하지만, 완벽하게 동일한 훈련 파이프라인(training pipeline)을 가진 모델은 없었습니다. 이러한 다양성은 LLM 개발에 있어 '만능 해결책(one-size-fits-all)'은 존재하지 않으며, 각 모델의 목표, 자원, 그리고 철학에 따라 최적의 경로가 달라질 수 있음을 명확히 보여줍니다. 각 기업은 자신들의 강점과 시장 전략에 맞춰 독자적인 훈련 파이프라인을 구축하고 있음을 알 수 있습니다.

사전 훈련의 경우, 모든 모델이 다단계 사전 훈련 파이프라인을 활용한다는 공통된 특징이 관찰됩니다. 이는 일반적인 핵심 사전 훈련 이후, 문맥 확장(context lengthening) 단계가 이어지고, 때로는 고품질 데이터에 대한 어닐링(annealing) 단계가 추가되는 형태를 보입니다. 다음 그림은 사전 훈련에 적용된 다양한 방법론들을 한눈에 요약해 보여줍니다. 다단계 사전 훈련은 이제 LLM 개발의 표준적인 접근 방식으로 자리 잡았음을 시사합니다. 이는 모델이 기초적인 언어 이해를 넘어 특정 기능(예: 장문맥 이해, 특정 도메인 전문성)을 점진적으로 강화하도록 설계된 정교한 전략입니다. 각 단계에서 데이터의 종류, 훈련 목표, 그리고 최적화 기법을 달리함으로써, 모델은 더욱 효율적이고 견고하게 학습될 수 있습니다.

[사전 훈련(pre-training)에 사용된 기술 개요]

이 개요는 각 모델이 사전 훈련 과정에서 어떤 기술적 선택을 했는지를 비교 분석하는 데 유용합니다. 특히, 데이터셋 규모, 다단계 훈련 구조, 그리고 특정 기법(예: 지식 증류)의 채택 여부가 모델의 최종 성능과 특성에 어떻게 영향을 미치는지 이해할 수 있습니다.

사후 훈련(post-training) 분야에서도 정확히 동일한 파이프라인은 발견되지 않았습니다. 거부 샘플링(rejection sampling)은 이제 사후 훈련 과정에서 보편적인 필수 요소로 자리 잡은 것으로 보입니다. 하지만 직접 선호 최적화(DPO) 또는 인간 피드백 기반 강화 학습(RLHF)에 관해서는 아직 명확한 합의나 우열(말장난 아님)이 정해지지 않은 상황입니다. DPO와 RLHF 사이의 지속적인 논쟁은 LLM 정렬의 복잡성과 중요성을 반영합니다. DPO는 구현의 용이성과 안정성 면에서 강점을 가지는 반면, RLHF는 이론적으로 더 강력한 정렬을 제공할 수 있습니다. 각 모델 개발팀은 자신들의 자원, 목표, 그리고 기술적 역량에 맞춰 이 두 가지 주요 정렬 패러다임 중 하나를 선택하거나, 혹은 애플처럼 여러 기법을 결합하는 하이브리드 접근 방식을 취하고 있습니다. 이는 LLM 정렬 분야가 여전히 활발한 연구와 혁신이 이루어지고 있는 영역임을 보여줍니다.

[사후 훈련(post-training)에 사용된 기술 개요]

이 개요는 모델들이 사후 훈련을 통해 어떻게 인간의 의도에 정렬되고 윤리적인 응답을 생성하도록 학습되었는지를 비교하는 데 도움을 줍니다. SFT, DPO, RLHF, 그리고 합성 데이터 활용과 같은 요소들이 모델의 '성격'과 '행동'을 형성하는 데 결정적인 역할을 합니다.

결론적으로, 고성능 LLM을 개발하는 데 있어 유일한 정답은 존재하지 않으며, 다양한 경로와 방법론이 효과적일 수 있습니다. 마지막으로, 이 네 가지 모델은 전반적으로 유사한 수준의 성능을 보여줍니다. 유감스럽게도, 이 모델들 중 일부는 LMSYS 및 AlpacaEval 리더보드(leaderboard)에 포함되어 있지 않아, MMLU와 같은 객관식 벤치마크(multiple-choice benchmark) 점수를 제외하고는 아직 직접적인 성능 비교가 어려운 실정입니다. LLM의 성능 평가 지표는 여전히 진화 중이며, 객관식 벤치마크만으로는 모델의 복잡한 능력을 완전히 포착하기 어렵습니다. 실제 사용자 시나리오에서의 평가, 인간 평가(human evaluation), 그리고 다양한 작업에 대한 미세 조정된 벤치마크의 중요성이 점차 강조되고 있습니다. 이러한 평가 시스템의 발전은 LLM 개발의 방향을 제시하고, 모델 간의 진정한 경쟁력을 판단하는 데 필수적인 기반을 제공할 것입니다.

**'Ahead of AI' 지지하기**

'Ahead of AI'는 독자 여러분의 지원으로 운영되는 출판물입니다. 새로운 게시물을 받아보시고 필자의 작업을 지지하고자 하신다면, 무료 또는 유료 구독자가 되어주시기를 고려해 주십시오. [구독](https://www.aheadofai.com/subscribe)

이 잡지는 필자의 개인적인 열정이 담긴 프로젝트입니다. 필자의 작업을 지지하고 싶으신 독자분들께서는 필자의 저서 "[Build a Large Language Model (From Scratch)](https://www.amazon.com/Build-Large-Language-Model-Scratch/dp/1032601700)"를 구매해 주시기를 정중히 요청드립니다. (이 책은 LLM의 작동 원리를 다른 곳에서는 찾아보기 힘든 수준의 세부 사항으로 설명하고 있으므로, 독자 여러분께서 분명 많은 것을 얻어가실 것이라고 확신합니다.)

[지금 아마존에서 'Build a Large Language Model (From Scratch)'를 만나보세요.](https://www.amazon.com/Build-Large-Language-Model-Scratch/dp/1032601700)

책을 읽으신 후 잠시 시간을 할애하여 [짧은 서평을 남겨주시면](https://www.amazon.com/Build-Large-Language-Model-Scratch/dp/1032601700#customerReviews) 필자에게 정말 큰 격려가 될 것입니다! 독자들의 피드백은 저자들에게 매우 소중합니다. 또한, 최근에는 이 잡지를 직접적으로 지원하기 위한 유료 구독 옵션을 Substack을 통해 활성화했습니다.

'Ahead of AI'는 독자 여러분의 지원으로 운영되는 출판물입니다. 새로운 게시물을 받아보시고 필자의 작업을 지지하고자 하신다면, 무료 또는 유료 구독자가 되어주시기를 고려해 주십시오. [구독](https://www.aheadofai.com/subscribe) 이러한 지원은 필자가 LLM 분야의 최신 연구와 실용적 적용에 대한 깊이 있는 분석을 지속적으로 제공하는 데 필수적인 역할을 합니다. 기술의 빠른 변화 속에서 정확하고 신뢰할 수 있는 정보를 제공하는 것은 쉬운 일이 아니지만, 독자 여러분의 격려는 이러한 노력을 계속하게 만드는 가장 큰 동기 부여가 됩니다. 함께 LLM의 미래를 탐구하고 지식을 공유하는 여정에 동참해 주시길 바랍니다.