대규모 언어 모델(LLM) 시대의 지속 가능한 혁신과 책임 있는 개발 및 최신 학습 파이프라인 분석

대규모 언어 모델(LLM)의 개발은 초기 GPT 모델부터 오늘날의 정교한 오픈 웨이트(open-weight) LLM에 이르기까지 인공지능 분야에 혁신적인 변화를 가져왔으며, 이는 단순한 기술 발전을 넘어 사회 전반에 걸쳐 새로운 가능성을 열어주고 있습니다. 초기에 LLM 학습 과정은 사전 학습(pre-training)에만 집중했지만, 이후 사전 학습과 사후 학습(post-training)을 모두 포함하도록 확장되었으며, 이제는 다양한 도메인과 윤리적 고려 사항까지 포함합니다. 사후 학습은 일반적으로 ChatGPT에 의해 대중화된 지도 학습 기반 명령어 미세 조정(supervised instruction fine-tuning)과 정렬(alignment)을 포함합니다. ChatGPT가 처음 출시된 이후 학습 방법론과 사용자 경험 및 상호작용 방식은 진화해 왔습니다. 이 글에서는 특히 최근 몇 달 동안 이루어진 사전 학습 및 사후 학습 방법론의 최신 발전과 더불어 LLM의 사회적 영향 및 책임 있는 개발에 대한 논의를 검토합니다. 매달 수백 편의 LLM 논문이 새로운 기술과 접근 방식을 제안하고 있지만, 그 이면에는 지속 가능성과 윤리적 문제가 함께 제기되고 있습니다. 인공지능 기술이 빠르게 발전함에 따라, 단순히 모델의 성능을 향상시키는 것을 넘어, 사회적 책임과 장기적인 지속 가능성을 고려하는 것이 중요해졌습니다. 실제로 무엇이 잘 작동하는지 확인하는 가장 좋은 방법 중 하나는 최신 최첨단 모델(state-of-the-art model)의 사전 학습 및 사후 학습 파이프라인을 살펴보는 것입니다. 다행히 지난 몇 달 동안 4개의 주요 신규 LLM이 비교적 상세한 기술 보고서(technical report)와 함께 출시되었습니다.

LLM의 발전은 정보 접근성 향상, 자동화된 작업 처리, 새로운 창의적 도구 제공 등 긍정적인 영향을 미치고 있습니다. 그러나 동시에 편향된 정보 생성, 오용 가능성, 에너지 소비 증가와 같은 심각한 문제점들도 수면 위로 떠오르고 있습니다. 이러한 문제들은 LLM 개발자들이 기술 혁신과 더불어 윤리적, 환경적 책임감을 가져야 함을 시사합니다. 우리는 기술이 사회에 미치는 영향을 깊이 이해하고, 인간 중심의 가치를 반영하는 방향으로 LLM을 발전시켜야 합니다.

이 글에서는 다음 모델들의 사전 학습 및 사후 학습 파이프라인에 중점을 둡니다:

*   Alibaba의 Qwen 2
*   Apple Intelligence Foundation Language Models
*   Google의 Gemma 2
*   Meta AI의 Llama 3.1

이 모델들은 arXiv.org에 게시된 각 기술 논문의 발행 날짜 순으로 제시되었으며, 이는 또한 알파벳 순서와도 일치합니다.

---

1.  **Alibaba의 Qwen 2**

다른 주요 LLM과 경쟁할 수 있는 매우 강력한 LLM 모델 제품군인 Qwen 2부터 시작하겠습니다. 하지만 어떤 이유에서인지 Meta AI, Microsoft, Google의 오픈 웨이트(open-weight) 모델보다 인기가 덜합니다.

1.1 **Qwen 2 개요**

Qwen 2 기술 보고서(Technical Report)에서 논의된 사전 학습 및 사후 학습 방법을 살펴보기 전에 몇 가지 핵심 사양을 간략하게 요약해 보겠습니다. Qwen 2 모델은 5가지 종류로 제공됩니다. 0.5억, 1.5억, 7억, 720억 매개변수(parameter) 크기의 4가지 일반(밀집) LLM이 있습니다. 또한, 570억 매개변수를 가진 전문가 혼합(Mixture-of-Experts) 모델이 있으며, 이 모델에서는 140억 매개변수가 동시에 활성화됩니다. (이번에는 아키텍처 세부 사항이 주요 초점이 아니므로 전문가 혼합(Mixture-of-Experts) 모델에 대해 너무 깊이 다루지는 않겠습니다. 하지만 간단히 말해, 이는 Mistral AI의 Mixtral과 유사하지만, 더 많은 활성 전문가(expert)를 가지고 있습니다. 높은 수준의 개요는 제 "Model Merging, Mixtures of Experts, and Towards Smaller LLMs" 글의 Mixtral 아키텍처 섹션을 참조하십시오.)

Qwen 2 LLM의 뛰어난 특징 중 하나는 30개 언어에 걸친 뛰어난 다국어 기능(multilingual capability)입니다. 또한 놀랍도록 큰 151,642개의 토큰 어휘(token vocabulary)를 가지고 있습니다(참고로 Llama 2는 32k 어휘를 사용하고, Llama 3.1은 128k 토큰 어휘를 사용합니다). 경험상 어휘 크기를 2배 늘리면 입력 토큰 수가 2배 줄어들어 동일한 입력에 더 많은 텍스트를 담을 수 있습니다. 또한 표준 영어 어휘 외의 단어를 다루는 다국어 데이터 및 코딩에 특히 도움이 됩니다.

아래는 나중에 다룰 다른 LLM과의 간략한 MMLU 벤치마크(benchmark) 비교입니다. (MMLU는 객관식 벤치마크(multiple-choice benchmark)이므로 한계가 있지만, 여전히 LLM 성능을 보고하는 가장 인기 있는 방법 중 하나입니다.)

최신 오픈 웨이트(open-weight) 모델의 MMLU 벤치마크 점수(높을수록 좋음). 이 그래프의 점수는 각 모델의 공식 연구 논문에서 수집했습니다. (MMLU에 대해 처음이시라면, 최근 강연 46분 05초에서 간략하게 설명했습니다.)

1.2 **Qwen 2 사전 학습(Pre-training)**

Qwen 2 팀은 15억, 70억, 720억 매개변수(parameter) 모델을 7조 개의 학습 토큰(training token)으로 학습시켰는데, 이는 합리적인 규모입니다. 비교하자면, Llama 2 모델은 2조 개의 토큰으로, Llama 3.1 모델은 15조 개의 토큰으로 학습되었습니다. 흥미롭게도 5억 매개변수 모델은 12조 개의 토큰으로 학습되었습니다. 그러나 연구원들은 학습 중에 어떠한 개선도 관찰하지 못했고 추가적인 계산 비용이 정당화되지 않았기 때문에 다른 모델들을 더 큰 12조 개의 토큰 데이터셋으로 학습시키지 않았습니다.

주요 초점 영역 중 하나는 저품질 데이터를 제거하기 위한 데이터 필터링 파이프라인(data filtering pipeline)을 개선하고 데이터 다양성을 높이기 위한 데이터 혼합(data mixing)을 강화하는 것이었습니다. 이는 나중에 다른 모델을 검토할 때 다시 다룰 주제입니다. 흥미롭게도 그들은 추가 사전 학습 데이터를 합성하기 위해 Qwen 모델(자세한 내용은 명시하지 않았지만, 이전 세대 Qwen 모델을 의미한다고 가정합니다)도 사용했습니다. 그리고 사전 학습에는 "인컨텍스트 학습(in-context learning) 및 명령어 추종 능력(instruction-following ability)을 향상시키기 위한 다중 작업 명령어 데이터(multi-task instruction data)"가 포함되었습니다.

또한, 그들은 일반 사전 학습에 이어 장문맥 학습(long-context training)이라는 두 단계로 학습을 수행했습니다. 후자는 "고품질의 긴 데이터"를 사용하여 사전 학습의 마지막 단계에서 문맥 길이(context length)를 4,096개에서 32,768개 토큰으로 늘렸습니다.

Qwen 2 사전 학습(pre-training)을 위한 기술 요약. "연속 사전 학습(Continued pre-training)"은 연구원들이 일반 사전 학습으로 시작하여 장문맥 연속 사전 학습을 이어서 수행한 2단계 사전 학습을 의미합니다. (안타깝게도 기술 보고서의 또 다른 특징은 데이터셋에 대한 세부 정보가 부족하다는 것입니다. 따라서 제 글이 매우 상세하지 않다면, 이는 공개적으로 이용 가능한 정보의 부족 때문입니다.)

1.3 **Qwen 2 사후 학습(Post-training)**

Qwen 2 팀은 인기 있는 2단계 사후 학습(post-training) 방법론을 사용했는데, 이는 500,000개 예시에 대해 2 에포크(epoch) 동안 적용된 지도 명령어 미세 조정(SFT)으로 시작했습니다. 이 단계는 미리 정해진 시나리오에서 모델의 응답 정확도를 개선하는 것을 목표로 했습니다.

일반적인 LLM 개발 흐름. SFT 후, 그들은 직접 선호 최적화(DPO)를 사용하여 LLM을 인간의 선호도에 정렬(align)시켰습니다. (흥미롭게도 그들의 용어로는 인간 피드백 기반 강화 학습(RLHF)이라고 불립니다.) 몇 주 전 "LLM 사전 학습 및 보상 모델 평가 팁(Tips for LLM Pretraining and Evaluating Reward Models)" 글에서 논의했듯이, SFT+DPO 접근 방식은 PPO를 사용한 RLHF와 같은 다른 방법들에 비해 사용 편의성 때문에 현재 가장 인기 있는 선호 조정 전략인 것 같습니다. (DPO가 어떻게 작동하는지 배우고 싶다면, 제가 최근에 여기에서 처음부터 구현했습니다.)

정렬(alignment) 단계 자체도 2단계로 진행되었습니다. 첫째, 기존 데이터셋에 DPO를 사용했습니다(오프라인 단계). 둘째, 보상 모델(reward model)을 사용하여 선호 쌍을 형성했습니다(온라인). 여기서 모델은 학습 중에 여러 응답을 생성하고, 보상 모델은 "실시간"(즉, 학습 중)으로 최적화 단계에 가장 선호되는 응답을 선택합니다. 이는 종종 "거부 샘플링(rejection sampling)"이라고도 불립니다.

데이터셋 구축을 위해 그들은 기존 코퍼스(corpus)에 인간 라벨링(human labeling)을 보완하여 SFT를 위한 목표 응답을 결정하고 DPO에 필수적인 선호 및 거부 응답을 식별했습니다. 연구원들은 또한 인공적으로 주석이 달린 데이터(artificially annotated data)를 합성했습니다. 더욱이, 팀은 LLM을 사용하여 "고품질 문학 데이터(literary data)"에 특별히 맞춰진 명령어-응답 쌍(instruction-response pair)을 생성하여 학습을 위한 고품질 Q&A 쌍(Q&A pair)을 만들었습니다.

Qwen 2 사후 학습(post-training)을 위한 기술 요약.

1.4 **결론**

Qwen 2는 비교적 유능한 모델이며, 이전 세대 Qwen과 유사합니다. 2023년 12월 NeurIPS LLM 효율성 챌린지(challenge)에 참석했을 때, 우승한 접근 방식의 대부분이 Qwen 모델을 포함하고 있었던 것이 기억납니다. Qwen 2의 학습 파이프라인(training pipeline)과 관련하여 눈에 띄는 점은 사전 학습(pre-training)과 사후 학습(post-training) 모두에 합성 데이터(synthetic data)가 사용되었다는 것입니다. 또한, (가능한 한 많은 데이터를 수집하기보다는) 데이터셋 필터링에 중점을 두는 것이 LLM 학습의 주목할 만한 추세 중 하나입니다. 여기서 저는 "더 많은 것이 더 좋다"고 말하겠지만, 이는 특정 품질 기준을 충족할 때만 해당됩니다.

---

2.  **Apple의 Apple Intelligence Foundation Language Models (AFM)**

arXiv.org에서 Apple의 모델 학습을 설명하는 또 다른 기술 논문(technical paper)을 보게 되어 정말 기뻤습니다. 예상치 못했지만 분명히 긍정적인 놀라움이었습니다!

2.1 **AFM 개요**

[Apple Intelligence Foundation Language Models 논문](https://arxiv.org/abs/2407.03052)에서 연구팀은 Apple 기기의 "Apple Intelligence" 맥락에서 사용하도록 설계된 두 가지 주요 모델의 개발을 설명합니다. 간결성을 위해 이 섹션 전체에서 이 모델들은 "Apple Foundation Models"의 약어인 AFM으로 지칭될 것입니다. 구체적으로, 이 논문은 AFM의 두 가지 버전을 설명합니다: 휴대폰, 태블릿 또는 노트북에 배포하기 위한 30억 매개변수(parameter) 온디바이스(on-device) 모델과 크기가 명시되지 않은 더 강력한 서버 모델입니다. 이 모델들은 채팅, 수학 및 코딩 작업을 위해 개발되었지만, 논문에서는 코딩 관련 학습 및 기능에 대해서는 논의하지 않습니다. Qwen 2와 마찬가지로 AFM은 밀집 LLM이며 전문가 혼합(mixture-of-experts) 접근 방식을 사용하지 않습니다.

2.2 **AFM 사전 학습(Pre-training)**

연구원들에게 두 가지 큰 칭찬을 하고 싶습니다. 첫째, 공개적으로 사용 가능한 데이터와 출판사로부터 라이선스(license)를 받은 데이터를 사용하는 것 외에도, 그들은 웹사이트의 robots.txt 파일을 존중하고 크롤링(crawling)을 자제했습니다. 둘째, 벤치마크 데이터(benchmark data)로 오염 제거(decontamination)를 수행했다고 언급했습니다. Qwen 2 논문의 주요 시사점 중 하나를 강조하기 위해, 연구원들은 양보다 질이 훨씬 더 중요하다고 언급했습니다. (디바이스 모델의 어휘 크기는 49k 토큰, 서버 모델은 100k 토큰으로, 150k 토큰 어휘를 사용한 Qwen 2 모델보다 어휘 크기가 현저히 작았습니다.)

흥미롭게도 사전 학습은 2단계가 아닌 3단계로 진행되었습니다!

*   핵심(일반) 사전 학습(pre-training)
*   웹 크롤링(web-crawl) (저품질) 데이터의 가중치를 낮추고, 수학 및 코드 데이터의 가중치를 높인 연속 사전 학습(continued pre-training)
*   더 긴 시퀀스 데이터와 합성 데이터(synthetic data)를 사용한 문맥 확장(context-lengthening)

이 3단계를 좀 더 자세히 살펴보겠습니다.

2.2.1 **사전 학습(Pre-training) I: 핵심 사전 학습(Core Pre-training)**

핵심 사전 학습은 Apple의 사전 학습 파이프라인에서 첫 번째 사전 학습 단계를 설명합니다. 이는 일반적인 사전 학습과 유사하며, AFM-서버 모델은 6.3조 개의 토큰, 4096의 배치 크기(batch size), 4096 토큰의 시퀀스 길이(sequence length)로 학습되었습니다. 이는 7조 개의 토큰으로 학습된 Qwen 2 모델과 매우 유사합니다.

그러나 AFM 온디바이스(on-device) 모델의 경우 더 흥미로운데, 이는 더 큰 64억 매개변수(parameter) 모델에서 증류 및 가지치기(distilled and pruned)되었습니다(이전 단락에서 설명된 AFM-서버 모델처럼 처음부터 학습됨). 증류 과정에 대한 자세한 내용은 "목표 레이블(target label)을 실제 레이블과 교사 모델(teacher model)의 상위 1개 예측(교사 레이블에 0.9 가중치 할당)의 볼록 조합(convex combination)으로 대체하여 증류 손실(distillation loss)이 사용된다"는 것 외에는 많지 않습니다. 지식 증류(knowledge distillation)가 LLM 사전 학습에 점점 더 널리 퍼지고 유용해지고 있다고 생각합니다(Gemma-2도 이를 사용합니다). 언젠가 더 자세히 다룰 계획입니다. 지금은 이 과정이 높은 수준에서 어떻게 작동하는지에 대한 간략한 개요입니다.

작은 모델(여기서는 AFM-디바이스 3B 모델)이 원래 학습 토큰과 더 큰 교사 모델(여기서는 6.4B 모델)의 출력으로 학습되는 지식 증류(knowledge distillation) 개요. a)의 교차 엔트로피 손실(cross entropy loss)은 LLM 사전 학습에 사용되는 일반적인 학습 손실입니다(일반 사전 학습 단계가 어떻게 구현되는지에 대한 자세한 내용은 제 "[Build a Large Language Model from Scratch](https://www.amazon.com/Build-Large-Language-Model-Scratch/dp/1032601700)" 책의 5장을 참조하십시오).

위에 설명된 지식 증류(knowledge distillation)는 여전히 원래 데이터셋으로 학습하는 것을 포함합니다. 그러나 데이터셋의 학습 토큰 외에도 학습될 모델(학생 모델(student model)이라고 함)은 더 큰 (교사) 모델로부터 정보를 받는데, 이는 지식 증류 없이 학습하는 것보다 더 풍부한 신호를 제공합니다. 단점은 다음과 같습니다: 1) 더 큰 교사 모델을 먼저 학습시켜야 하고, 2) 더 큰 교사 모델을 사용하여 모든 학습 토큰에 대한 예측을 계산해야 합니다. 이러한 예측은 미리 계산할 수 있거나(상당한 저장 공간 필요) 학습 중에 계산할 수 있습니다(학습 프로세스 속도 저하 가능).

2.2.2 **사전 학습(Pre-training) II: 연속 사전 학습(Continued Pre-training)**

연속 사전 학습 단계는 1조 개의 토큰으로 구성된 데이터셋(핵심 사전 학습 세트는 5배 더 컸음)에서 문맥 길이(context length)를 4,096개에서 8,192개 토큰으로 늘리는 작은 문맥 확장(context lengthening) 단계를 포함합니다. 그러나 주요 초점은 수학 및 코드에 중점을 둔 고품질 데이터 혼합(data mix)으로 학습하는 것입니다. 흥미롭게도 연구원들은 이 맥락에서 증류 손실(distillation loss)이 유익하지 않다는 것을 발견했습니다.

2.2.3 **사전 학습(Pre-training) III: 문맥 확장(Context Lengthening)**

세 번째 사전 학습 단계는 1,000억 개의 토큰(두 번째 단계에서 사용된 토큰의 10%)만 포함하지만, 문맥 길이(context length)를 32,768개 토큰으로 더 크게 확장하는 것을 나타냅니다. 이를 달성하기 위해 연구원들은 합성 장문맥 Q&A 데이터(synthetic long-context Q&A data)로 데이터셋을 증강했습니다.

AFM 사전 학습(pre-training)을 위한 기술 요약.

2.3 **AFM 사후 학습(Post-training)**

Apple은 사전 학습(pre-training)과 마찬가지로 사후 학습(post-training) 과정에도 유사하게 포괄적인 접근 방식을 취한 것으로 보입니다. 그들은 인간이 주석을 단 데이터와 합성 데이터(synthetic data)를 모두 활용했으며, 데이터 품질이 양보다 우선시된다는 점을 강조했습니다. 흥미롭게도 그들은 미리 정해진 데이터 비율에 의존하지 않고, 여러 실험을 통해 데이터 혼합(data mixture)을 미세 조정하여 최적의 균형을 달성했습니다.

사후 학습 단계는 지도 명령어 미세 조정(supervised instruction fine-tuning)과 여러 차례의 인간 피드백 기반 강화 학습(RLHF)으로 구성된 2단계 프로세스를 포함했습니다. 이 과정에서 특히 주목할 만한 점은 Apple이 RLHF 단계에 두 가지 새로운 알고리즘을 도입했다는 것입니다:

*   교사 위원회(Teacher Committee)를 이용한 거부 샘플링 미세 조정(iTeC)
*   미러 디센트 정책 최적화(Mirror Descent Policy Optimization)를 이용한 RLHF

이 글의 길이를 고려하여 이 방법들의 기술적 세부 사항은 다루지 않겠지만, 간략한 개요는 다음과 같습니다:

iTeC 알고리즘은 거부 샘플링(rejection sampling)과 여러 선호 조정 기법(preference tuning technique)을 결합합니다. 구체적으로 SFT, DPO, IPO, 그리고 온라인 강화 학습(online RL)입니다. Apple은 단일 알고리즘에 의존하기보다는 각 접근 방식을 독립적으로 사용하여 모델을 학습시켰습니다. 그런 다음 이 모델들은 응답을 생성했고, 인간이 선호 레이블(preference label)을 제공하여 이를 평가했습니다. 이 선호 데이터는 RLHF 프레임워크에서 보상 모델(reward model)을 반복적으로 학습시키는 데 사용되었습니다. 거부 샘플링(rejection sampling) 단계에서는 모델 위원회(committee of models)가 여러 응답을 생성했고, 보상 모델이 가장 좋은 응답을 선택했습니다. 이 위원회 기반 접근 방식은 상당히 복잡하지만, 관련된 모델의 비교적 작은 크기(약 30억 매개변수)를 고려할 때 비교적 실현 가능해야 합니다. Llama 3.1의 70B 또는 405B 매개변수 모델과 같이 훨씬 더 큰 모델로 이러한 위원회를 구현하는 것은 확실히 더 어려울 것입니다.

두 번째 알고리즘인 미러 디센트(Mirror Descent)를 이용한 RLHF는 일반적으로 사용되는 PPO(근접 정책 최적화(Proximal Policy Optimization))보다 더 효과적임이 입증되었기 때문에 선택되었습니다.

AFM 사후 학습(post-training)을 위한 기술 요약.

2.4 **결론**

Apple의 사전 학습(pre-training) 및 사후 학습(post-training) 접근 방식은 비교적 포괄적이며, 이는 아마도 위험 부담이 매우 높기 때문일 것입니다(모델이 수백만, 심지어 수십억 대의 기기에 배포됨). 그러나 이 모델들의 작은 특성을 고려할 때, 3B 모델은 가장 작은 Llama 3.1 모델 크기의 절반에도 미치지 못하므로, 광범위한 기술 또한 실현 가능해집니다. 주요 특징 중 하나는 RLHF와 DPO 사이의 단순한 선택이 아니라는 것입니다. 대신, 그들은 위원회(committee) 형태로 여러 선호 조정(preference-tuning) 알고리즘을 사용했습니다. 또한 사전 학습의 일부로 Q&A 데이터를 명시적으로 사용했다는 점도 흥미롭습니다. 이는 제가 이전 글인 "[명령어 사전 학습 LLM(Instruction Pretraining LLMs)](https://www.aheadofai.com/p/instruction-pretraining-llms)"에서 논의했던 내용입니다. 전반적으로, 신선하고 즐거운 기술 보고서(technical report)입니다.

---

3.  **Google의 Gemma 2**

Google의 Gemma 모델은 최근 "[Gemma 2: Improving Open Language Models at a Practical Size](https://arxiv.org/abs/2407.13038)"에서 설명되었습니다. 다음 개요 섹션에서 몇 가지 주요 사실을 간략하게 설명한 후 사전 학습(pre-training) 및 사후 학습(post-training) 프로세스에 대해 논의하겠습니다.

3.1 **Gemma 2 개요**

Gemma 2 모델은 20억, 90억, 270억 매개변수(parameter)의 세 가지 크기로 제공됩니다. 주요 초점은 학습 데이터셋의 크기를 반드시 늘릴 필요는 없지만, 비교적 작고 효율적인 LLM을 개발하는 기술을 탐구하는 데 있습니다. 특히 Gemma 2는 256k 토큰이라는 상당한 어휘 크기를 특징으로 합니다. 비교를 위해 Llama 2는 32k 토큰 어휘를 사용하고, Llama 3는 128k 토큰 어휘를 사용합니다. 또한 Gemma 2는 Mistral의 초기 모델과 유사하게 슬라이딩 윈도우 어텐션(sliding window attention)을 사용하여 메모리 비용을 줄이는 것으로 보입니다. Gemma 2 아키텍처에 대한 자세한 내용은 제 이전 글의 [Gemma 2 섹션](https://www.aheadofai.com/p/model-merging-mixtures-of-experts-and-towards-smaller-llms#%C2%A7gemma-2)을 참조하십시오. Gemma 2 모델은 다양한 크기로 제공되지만, 모든 모델은 공정성과 투명성을 중요한 가치로 삼고 있습니다. 이는 모델의 신뢰성과 사회적 수용성을 결정하는 핵심 요소입니다.

3.2 **Gemma 2 사전 학습(Pre-training)**

Gemma 연구원들은 작은 모델조차도 종종 불충분하게 학습된(undertrained) 상태라고 주장합니다. 그러나 단순히 학습 데이터셋의 크기를 늘리는 대신, 그들은 품질 유지에 중점을 두고 Apple의 접근 방식과 유사하게 지식 증류(knowledge distillation)와 같은 대체 방법을 통해 개선을 달성합니다. 27B Gemma 2 모델은 처음부터 학습되었지만, 더 작은 모델들은 이전에 설명된 Apple의 접근 방식과 유사하게 지식 증류(knowledge distillation)를 사용하여 학습되었습니다. 27B 모델은 13조 개의 토큰으로, 9B 모델은 8조 개의 토큰으로, 2B 모델은 2조 개의 토큰으로 학습되었습니다. 또한, Apple의 접근 방식과 유사하게 Gemma 팀은 성능 향상을 위해 데이터 혼합(data mixture)을 최적화했습니다.

Gemma 2 사전 학습(pre-training)을 위한 기술 요약.

3.3 **Gemma 2 사후 학습(Post-training)**

Gemma 모델의 사후 학습(post-training) 과정은 일반적인 지도 미세 조정(SFT) 및 인간 피드백 기반 강화 학습(RLHF) 단계를 포함했습니다. 명령어 데이터는 인간이 생성한 콘텐츠와 합성 생성 콘텐츠가 혼합된 영어 전용 프롬프트 쌍(English-only prompt pair)을 사용했습니다. 특히 흥미롭게도, 응답은 주로 교사 모델(teacher model)에 의해 생성되었으며, SFT 단계에서도 지식 증류(knowledge distillation)가 적용되었습니다.

SFT 이후 그들의 RLHF 접근 방식의 흥미로운 점은 RLHF에 사용되는 보상 모델(reward model)이 정책(목표) 모델보다 10배 더 크다는 것입니다. Gemma가 사용하는 RLHF 알고리즘은 상당히 표준적이지만, 독특한 특징이 있습니다: 그들은 WARM(가중치 평균 보상 모델)(weight-averaged reward models)의 후속작인 WARP라는 방법을 통해 정책 모델들을 평균화합니다. 저는 이전에 "Model Merging, Mixtures of Experts, and Towards Smaller LLMs"라는 제 글에서 이 방법을 자세히 논의했습니다.

Gemma 2 사후 학습(post-training)을 위한 기술 요약.

3.4 **결론**

Gemma 팀은 Apple과 유사하게 사전 학습(pre-training)과 사후 학습(post-training) 모두에서 지식 증류(knowledge distillation)에 정말 집중하는 것으로 보입니다. 흥미롭게도 그들은 다단계 사전 학습 접근 방식을 사용하지 않았거나, 적어도 논문에서 자세히 설명하지는 않았습니다.

다가오는 PyTorch 컨퍼런스(conference)에서 기조 강연(keynote talk)을 하게 되어 기쁩니다. 첫 PyTorch 컨퍼런스가 될 것이며, 커뮤니티를 만나 최신 AI 및 LLM 개발에 대해 이야기할 수 있기를 기대합니다!

---

4.  **Meta AI의 Llama 3.1**

Meta의 Llama LLM의 새로운 출시는 항상 큰 이슈입니다. 이번에는 92페이지 분량의 기술 보고서(technical report)인 "[The Llama 3 Herd of Models](https://arxiv.org/abs/2407.13038)"와 함께 출시되었습니다. 마지막으로, 이 섹션에서는 지난달에 발표된 네 번째 대규모 모델 논문을 살펴보겠습니다.

4.1 **Llama 3.1 개요**

Meta는 거대한 4050억 매개변수(parameter) 모델을 출시하는 것과 함께, 이전의 80억 및 700억 매개변수 모델을 업데이트하여 MMLU 성능을 약간 향상시켰습니다.

다양한 모델의 MMLU 벤치마크(benchmark) 성능. Llama 3는 다른 최신 LLM과 마찬가지로 그룹 쿼리 어텐션(group query attention)을 사용하지만, 놀랍게도 Meta AI는 슬라이딩 윈도우 어텐션(sliding window attention)과 전문가 혼합(Mixture-of-Experts) 접근 방식에 대해서는 "아니오"라고 말했습니다. 즉, Llama 3.1은 매우 전통적으로 보이며, 아키텍처 혁신보다는 사전 학습(pre-training) 및 사후 학습(post-training)에 분명히 초점을 맞추었습니다. 이전 Llama 출시와 마찬가지로 가중치(weight)는 공개적으로 사용 가능합니다. 또한 Meta는 Llama 3 라이선스(license)를 업데이트하여 이제 다른 모델을 개선하기 위한 합성 데이터(synthetic data) 생성 또는 지식 증류(knowledge distillation)에 Llama 3를 사용하는 것이 마침내 가능(허용)해졌다고 밝혔습니다.

4.2 **Llama 3.1 사전 학습(Pre-training)**

Llama 3는 Llama 2의 1.8조 개 토큰에서 크게 증가한 15.6조 개의 토큰 데이터셋으로 학습되었습니다. 연구원들은 Llama 3가 최소 8개 언어를 지원한다고 말합니다(Qwen 2는 20개 언어를 처리할 수 있습니다). Llama 3의 흥미로운 측면은 OpenAI의 tiktoken 토크나이저(tokenizer)를 사용하여 개발된 128,000개의 어휘 크기입니다. (토크나이저 성능에 관심 있는 분들을 위해 여기에서 [간단한 벤치마크(benchmark) 비교](https://www.aheadofai.com/p/tokenizer-benchmark-gpt-4-llama-3-qwen-2)를 했습니다.)

사전 학습(pre-training) 데이터 품질 관리 측면에서 Llama 3는 Meta AI의 fastText 및 RoBERTa 기반 분류기(classifier)와 같은 고속 분류기(fast classifier)를 활용하여 휴리스틱 기반 필터링(heuristic-based filtering)과 모델 기반 품질 필터링(model-based quality filtering)을 함께 사용합니다. 이러한 분류기는 학습 중에 사용되는 데이터 혼합(data mix)의 문맥 범주(context category)를 결정하는 데도 도움이 됩니다.

Llama 3의 사전 학습은 세 단계로 나뉩니다. 첫 번째 단계는 8k 문맥 창(context window)을 가진 15.6조 개의 토큰을 사용하여 표준 초기 사전 학습을 포함합니다. 두 번째 단계는 사전 학습을 계속하지만 문맥 길이(context length)를 128k로 확장합니다. 최종 단계는 모델 성능을 더욱 향상시키는 어닐링(annealing)을 포함합니다. 아래에서 이러한 단계를 더 자세히 살펴보겠습니다.

4.2.1 **사전 학습(Pre-training) I: 표준(초기) 사전 학습(Standard (Initial) Pre-training)**

학습 설정에서 그들은 4백만 개의 토큰으로 구성된 배치(batch)로 시작했으며, 각 배치에는 4096의 시퀀스 길이(sequence length)가 있었습니다. 이는 4백만이라는 숫자가 가장 가까운 자릿수로 반올림되었다고 가정할 때 약 1024 토큰의 배치 크기(batch size)를 의미합니다. 처음 2억 5천 2백만 개의 토큰을 처리한 후, 그들은 시퀀스 길이를 8192로 두 배 늘렸습니다. 학습 과정이 더 진행되어 2.87조 개의 토큰을 처리한 후, 그들은 배치 크기를 다시 두 배 늘렸습니다. 또한, 연구원들은 학습 내내 데이터 혼합(data mix)을 일정하게 유지하지 않았습니다. 대신, 모델 학습 및 성능을 최적화하기 위해 학습 과정에서 사용되는 데이터 혼합을 조정했습니다. 데이터 처리에 대한 이러한 동적 접근 방식(dynamic approach)은 모델이 다양한 유형의 데이터에 걸쳐 일반화(generalize)하는 능력을 향상시키는 데 도움이 되었을 것입니다.

4.2.2 **사전 학습(Pre-training) II: 문맥 확장(Context Lengthening)을 위한 연속 사전 학습(Continued Pre-training)**

문맥 창(context window)을 한 번에 늘린 다른 모델들과 비교하여, Llama 3.1의 문맥 확장(context lengthening)은 더 점진적인 접근 방식이었습니다. 여기서 연구원들은 8,000개에서 128,000개 토큰까지 6개의 개별 단계를 통해 문맥 길이(context length)를 늘렸습니다. 이러한 단계별 증가(stepwise increment)는 모델이 더 큰 문맥에 더 원활하게 적응할 수 있도록 했을 것입니다. 이 과정에 사용된 학습 세트(training set)는 전체 데이터셋 크기의 약 5%에 해당하는 8,000억 개의 토큰을 포함했습니다.

4.2.3 **사전 학습(Pre-training) III: 고품질 데이터에 대한 어닐링(Annealing)**

세 번째 사전 학습 단계에서는 연구원들이 작지만 고품질의 혼합 데이터로 모델을 학습시켰는데, 이는 벤치마크 데이터셋(benchmark dataset)의 성능을 향상시키는 데 도움이 된다는 것을 발견했습니다. 예를 들어, GSM8K 및 MATH 학습 세트(training set)에 대한 어닐링(annealing)은 해당 GSM8K 및 MATH 검증 세트(validation set)에서 상당한 성능 향상을 제공했습니다. 논문의 3.1.3 섹션에서 연구원들은 어닐링(annealing) 데이터셋 크기가 400억 개의 토큰(전체 데이터셋 크기의 0.02%)이라고 밝혔습니다. 이 40B 어닐링 데이터셋은 데이터 품질을 평가하는 데 사용되었습니다. 3.4.3 섹션에서는 실제 어닐링이 4천만 개의 토큰(어닐링 데이터의 0.1%)에서만 수행되었다고 명시합니다.

Llama 3.1 사전 학습(pre-training)을 위한 기술 요약.

4.3 **Llama 3.1 사후 학습(Post-training)**

Meta AI 팀은 사후 학습(post-training) 과정에서 지도 미세 조정(SFT), 거부 샘플링(rejection sampling), 직접 선호 최적화(DPO)를 포함하는 비교적 간단한 방법을 사용했습니다. 그들은 PPO를 사용한 RLHF와 같은 강화 학습 알고리즘이 이러한 기술에 비해 안정성이 떨어지고 확장하기 더 어렵다는 것을 관찰했습니다. SFT 및 DPO 단계가 인간이 생성한 데이터와 합성 데이터(synthetic data)를 모두 통합하여 여러 라운드에 걸쳐 반복적으로 수행되었다는 점은 주목할 가치가 있습니다. 추가 세부 사항을 설명하기 전에, 그들의 워크플로우는 아래 그림에 설명되어 있습니다.

DPO를 사용했음에도 불구하고, RLHF에서 하는 것처럼 보상 모델(reward model)도 개발했다는 점에 유의하십시오. 처음에는 사전 학습 단계의 체크포인트(checkpoint)를 사용하여 인간이 주석을 단 데이터로 보상 모델을 학습시켰습니다. 이 보상 모델은 그 후 거부 샘플링(rejection sampling) 과정에 사용되어 추가 학습을 위한 적절한 프롬프트를 선택하는 데 도움이 되었습니다. 각 학습 라운드에서 그들은 보상 모델뿐만 아니라 SFT 및 DPO 모델에도 모델 평균화 기법(model averaging technique)을 적용했습니다. 이 평균화는 최근 모델과 이전 모델의 매개변수(parameter)를 병합하여 시간이 지남에 따라 성능을 안정화(및 향상)시키는 것을 포함했습니다. 모델 평균화의 기술적 세부 사항에 관심 있는 분들을 위해, 저는 이전 글 "[Model Merging, Mixtures of Experts, and Towards Smaller LLMs](https://www.aheadofai.com/p/model-merging-mixtures-of-experts-and-towards-smaller-llms)"의 "모델 병합 및 가중치 평균 이해(Understanding Model Merging and Weight Averaging)" 섹션에서 이 주제를 논의했습니다.

요약하자면, 핵심적으로는 비교적 표준적인 SFT + DPO 단계입니다. 그러나 이 단계는 여러 라운드에 걸쳐 반복됩니다. 그런 다음, 그들은 거부 샘플링(rejection sampling)을 위해 보상 모델(Qwen 2 및 AFM과 유사)을 추가했습니다. 그들은 또한 Gemma처럼 모델 평균화(model averaging)를 사용했지만, 이는 보상 모델뿐만 아니라 관련된 모든 모델에 적용되었습니다.

Llama 3.1 사후 학습(post-training)을 위한 기술 요약.

4.4 **결론**

Meta의 Llama LLM의 새로운 출시는 항상 큰 이슈이지만, Llama 3 모델은 이전 Llama 2 모델과 상당히 표준적이고 유사하면서도 몇 가지 흥미로운 접근 방식을 가지고 있습니다. 특히, 15조 개의 토큰으로 구성된 대규모 학습 세트(training set)는 Llama 3를 다른 모델들과 구별합니다. 흥미롭게도 Apple의 AFM 모델처럼 Llama 3도 3단계 사전 학습(pre-training) 프로세스를 구현했습니다. Llama 3는 방대한 데이터셋으로 학습되었지만, 데이터의 양을 넘어 인간의 가치를 반영하는 데 중점을 둡니다. 다른 최신 대규모 언어 모델과 달리 Llama 3는 지식 증류(knowledge distillation) 기술을 사용하지 않고, 대신 더 간단한 모델 개발 경로를 선택했습니다. 사후 학습(post-training)의 경우, 모델은 다른 모델에서 인기를 끌었던 더 복잡한 강화 학습 전략 대신 직접 선호 최적화(DPO)를 활용했습니다. 사전 학습 및 사후 학습에 분명히 초점을 맞추었으며, 이는 인간의 피드백을 통합하는 방향으로 나아가고 있습니다. 전반적으로, 이러한 선택은 더 간단하지만 입증된 방법을 통해 LLM 성능을 개선하는 데 중점을 둔다는 점에서 흥미롭습니다.

---

5.  **처음부터 직접 선호 최적화(Direct Preference Optimization)로 LLM 정렬**

직접 선호 최적화(DPO)는 LLM을 사용자 선호도에 더 가깝게 정렬(align)시키는 주요 방법 중 하나가 되었으며, 이 글에서 많이 접하게 될 내용입니다. DPO가 어떻게 작동하는지 배우고 싶다면, 제가 여기에서 처음부터 코딩했습니다: [LLM 정렬을 위한 직접 선호 최적화(DPO) (처음부터)](https://www.aheadofai.com/p/direct-preference-optimization-dpo-for-llm-alignment-from-scratch).

---

6.  **LLM의 사회적 영향과 윤리적 과제**

인공지능, 특히 대규모 언어 모델(LLM)의 급속한 발전은 우리 사회에 광범위한 영향을 미치고 있습니다. 이러한 모델들은 효율성을 증대시키고 새로운 서비스를 창출하는 잠재력을 가지고 있지만, 동시에 여러 윤리적 과제를 야기합니다. 최신 최첨단 모델(state-of-the-art model)의 개발은 인간 사회에 깊은 영향을 미치고 있습니다. 이는 정보의 확산 방식, 의사 결정 과정, 심지어 인간의 창의성에까지 영향을 미칠 수 있습니다.

6.1 **편향성과 공정성**

LLM은 학습 데이터에 내재된 편향을 그대로 반영하거나 증폭시킬 수 있습니다. 이는 특정 인종, 성별, 문화에 대한 고정관념을 강화하거나 차별적인 결과를 초래할 수 있습니다. 예를 들어, 채용 도구에 LLM을 적용할 경우, 과거의 편향된 채용 데이터를 학습한 모델은 특정 집단에 불리한 결정을 내릴 수 있습니다. 이러한 편향은 사회적 불평등을 심화시킬 위험이 있습니다. 따라서, 학습 데이터의 다양성을 확보하고 편향을 식별하며 완화하는 기술적, 사회적 노력이 필수적입니다. 최근에는 비교적 상세한 기술 보고서(technical report)와 함께 윤리 가이드라인이 발표되고 있습니다.

6.2 **오정보 및 허위 정보의 확산**

LLM은 설득력 있는 텍스트를 생성하는 능력이 뛰어나기 때문에, 오정보(misinformation)나 허위 정보(disinformation)를 대규모로 확산시키는 데 악용될 수 있습니다. 딥페이크(deepfake) 기술과 결합될 경우, 가짜 뉴스나 사기 행위의 위험은 더욱 커집니다. 이는 대중의 신뢰를 저해하고 사회적 혼란을 야기할 수 있습니다. LLM 개발자들은 모델이 생성하는 정보의 신뢰성을 높이고, 허위 정보 탐지 및 방지 메커니즘을 강화해야 할 책임이 있습니다.

6.3 **개인 정보 보호 및 보안**

LLM은 방대한 양의 데이터를 학습하며, 이 과정에서 개인 정보가 포함될 수 있습니다. 모델이 학습 데이터에 있는 개인 정보를 기억하고 재현할 가능성(memorization)은 심각한 개인 정보 유출 문제로 이어질 수 있습니다. 또한, LLM에 대한 악의적인 공격(adversarial attack)은 모델의 보안을 위협하고 민감한 정보를 추출하거나 조작할 수 있습니다. 강력한 데이터 거버넌스(data governance)와 보안 프로토콜은 LLM 개발 및 배포의 핵심 요소가 되어야 합니다.

6.4 **노동 시장의 변화**

LLM의 자동화 능력은 특정 직업군의 업무를 대체하거나 변화시킬 수 있습니다. 이는 생산성 향상으로 이어질 수 있지만, 동시에 대규모 실업과 같은 사회경제적 파장을 일으킬 수 있습니다. 정부, 기업, 교육 기관은 이러한 변화에 대비하여 새로운 일자리 창출, 재교육 프로그램 개발, 사회 안전망 강화 등의 노력을 기울여야 합니다.

---

7.  **지속 가능한 LLM 개발을 위한 노력**

대규모 언어 모델(LLM)의 급격한 성장은 그 성능만큼이나 막대한 에너지 소비와 환경적 영향을 수반합니다. 이러한 문제를 해결하기 위해 지속 가능한 LLM 개발을 위한 노력이 전 세계적으로 중요하게 대두되고 있습니다.

7.1 **에너지 효율적인 아키텍처 및 학습 방법**

LLM 학습에는 수천 개의 GPU가 수주에서 수개월 동안 가동되어야 하며, 이는 엄청난 전력을 소모합니다. 이러한 에너지 소비는 탄소 발자국(carbon footprint)을 증가시키고 기후 변화에 기여합니다. 개발자들은 에너지 효율적인 아키텍처 설계와 학습 방법론을 모색하고 있습니다. 예를 들어, 모델의 크기를 줄이거나, 희소성(sparsity)을 활용하여 불필요한 계산을 줄이는 방법이 연구되고 있습니다.

*   **모델 경량화:** 매개변수(parameter) 수를 줄이거나 모델 압축(model compression) 기술(가지치기(pruning), 양자화(quantization) 등)을 사용하여 모델의 크기를 최적화합니다. 이는 모델의 배포 및 추론 단계에서의 에너지 소비를 줄이는 데 크게 기여합니다.
*   **하드웨어 최적화:** GPU 외에 전력 효율적인 맞춤형 AI 칩(예: TPU)을 개발하거나, 분산 학습 시스템의 효율성을 극대화하여 전체적인 에너지 소비를 줄입니다.
*   **Mixture-of-Experts (MoE) 모델:** 전통적인 밀집 LLM이며 전문가 혼합(Mixture-of-Experts) 접근 방식을 통해 효율성을 추구하고 있습니다. 이는 전체 모델 중 일부 전문가(expert)만 활성화하여 계산 비용을 절감하는 방식으로, 특히 대규모 모델에서 효과적입니다.

7.2 **그린 AI(Green AI) 이니셔티브**

그린 AI는 인공지능 기술의 환경적 영향을 최소화하려는 노력을 총칭합니다. 이는 에너지 소비를 줄이는 기술적 접근뿐만 아니라, AI 시스템의 전체 수명 주기(개발, 학습, 배포, 사용, 폐기)에 걸쳐 환경 친화적인 관행을 적용하는 것을 포함합니다.

*   **재생 가능 에너지 사용:** AI 데이터 센터의 전력원을 재생 가능 에너지(태양광, 풍력 등)로 전환하여 탄소 배출량을 줄입니다.
*   **환경 영향 평가:** LLM 개발 프로젝트의 시작 단계부터 예상되는 에너지 소비량과 탄소 배출량을 예측하고, 이를 줄이기 위한 구체적인 계획을 수립합니다.
*   **오픈 소스 및 투명성:** 연구 결과와 데이터를 공유하여 중복 연구를 줄이고, 효율적인 학습 방법을 확산시켜 전반적인 에너지 소비를 줄이는 데 기여합니다.

7.3 **데이터 효율성 및 재활용**

학습 데이터의 양을 무작정 늘리는 것이 아니라, 고품질의 데이터를 효율적으로 선별하고 재사용하는 것도 지속 가능성에 기여합니다. 데이터 필터링(data filtering)과 데이터 증강(data augmentation) 기술을 사용하여 필요한 데이터의 양을 최적화하고, 데이터 수집 및 처리 과정에서 발생하는 에너지 소비를 줄입니다.

---

8.  **데이터 거버넌스와 공정성**

대규모 언어 모델(LLM)의 성능은 학습 데이터의 품질과 양에 크게 의존하지만, 데이터 거버넌스(data governance)와 공정성 문제는 단순히 기술적인 측면을 넘어 사회적, 윤리적 중요성을 가집니다. 이는 모델의 신뢰성과 사회적 수용성을 결정하는 핵심 요소입니다.

8.1 **데이터 선별 및 정제**

LLM 학습에 사용되는 데이터는 인터넷에서 수집된 방대한 텍스트로 구성됩니다. 이 데이터에는 편향, 오류, 유해한 콘텐츠가 포함될 수 있습니다. Gemma 연구원들은 작은 모델조차도 종종 불충분하게 학습된(undertrained) 상태라고 주장하지만, 데이터의 질적 개선에 더욱 집중하고 있습니다. 따라서 데이터의 선별과 정제 과정은 모델의 공정성과 안전성을 확보하는 데 결정적인 역할을 합니다.

*   **편향 탐지 및 완화:** 특정 집단에 대한 편향된 표현이나 고정관념을 포함하는 데이터를 식별하고, 이를 제거하거나 중립적인 데이터로 대체하는 기술을 개발합니다.
*   **유해 콘텐츠 필터링:** 혐오 발언, 폭력적 내용, 성차별적 표현 등 유해한 콘텐츠를 학습 데이터에서 효과적으로 제거하여 모델이 이러한 내용을 생성하지 않도록 방지합니다.
*   **데이터 다양성 확보:** 다양한 문화, 언어, 배경을 대표하는 데이터를 균형 있게 포함하여 모델의 포괄성을 높이고, 소수 집단에 대한 편향을 줄입니다.

8.2 **데이터 출처 투명성 및 라이선스**

LLM 학습에 사용되는 데이터의 출처와 라이선스(license)에 대한 투명성은 매우 중요합니다. 많은 데이터가 웹 크롤링(web crawling)을 통해 수집되는데, 이 과정에서 저작권 침해나 개인 정보 보호 문제가 발생할 수 있습니다. 데이터 출처를 명확히 하고, 적절한 라이선스를 준수하며, 필요에 따라 데이터 소유자에게 보상을 제공하는 정책이 필요합니다. 이는 인공지능 윤리를 확립하고, 데이터 생태계의 지속 가능한 발전을 도모하는 데 기여합니다.

8.3 **지식 증류(Knowledge Distillation)를 통한 공정성 개선**

모델의 크기를 줄이면서도 성능을 유지하는 지식 증류(knowledge distillation) 기술은 효율성뿐만 아니라 공정성 개선에도 활용될 수 있습니다. 교사 모델(teacher model)이 학습한 복잡한 패턴을 학생 모델(student model)에게 전달하는 과정에서, 편향된 정보를 걸러내거나 특정 윤리적 지침을 반영하도록 증류 과정을 설계할 수 있습니다. 이를 통해 지식 증류(knowledge distillation)와 같은 대체 방법을 통해 모델의 공정성을 개선합니다.

8.4 **데이터 거버넌스 프레임워크 구축**

효과적인 데이터 거버넌스는 LLM 개발의 모든 단계에서 데이터를 관리하고 통제하는 체계적인 접근 방식입니다. 이는 데이터 수집, 저장, 처리, 사용 및 폐기 전반에 걸쳐 윤리적 원칙과 법적 규제를 준수하도록 보장합니다. 명확한 정책, 책임 할당, 정기적인 감사 및 평가를 통해 LLM이 사회적으로 책임감 있는 방식으로 개발되고 사용되도록 지원해야 합니다.

---

9.  **LLM의 미래와 인간 중심 AI**

대규모 언어 모델(LLM)의 미래는 단순히 기술적 성능의 향상을 넘어, 인간과의 협력, 윤리적 가치 통합, 그리고 사회적 책임이라는 더 넓은 관점에서 논의되어야 합니다. Meta의 Llama LLM의 새로운 출시는 항상 큰 이슈이지만, 그 미래는 인간 중심의 접근 방식에 달려 있습니다. 기술이 인간의 삶을 풍요롭게 하고 사회에 긍정적인 영향을 미치기 위해서는 인간 중심 AI(Human-Centered AI) 원칙이 LLM 개발의 핵심이 되어야 합니다.

9.1 **인간-AI 협업의 강화**

LLM은 인간의 도구로서 가장 큰 가치를 발휘할 때 진정한 잠재력을 발휘할 수 있습니다. Llama 3는 방대한 데이터셋으로 학습되었지만, 데이터의 양을 넘어 인간의 가치를 반영하는 데 중점을 둡니다. 이는 LLM이 인간의 창의성, 비판적 사고, 공감 능력을 보완하고 확장하는 역할을 해야 함을 의미합니다. 예를 들어, LLM은 아이디어 구상, 초고 작성, 정보 요약 등 반복적이고 시간이 많이 소요되는 작업을 자동화하여 인간이 더 복잡하고 창의적인 문제 해결에 집중할 수 있도록 돕습니다. 인간과 AI가 상호 보완적인 관계를 맺고 협력하는 시스템(Human-in-the-Loop)을 구축하는 것이 중요합니다.

9.2 **설명 가능 인공지능(Explainable AI, XAI)**

LLM은 종종 '블랙박스(black box)' 모델로 불리며, 그 내부 작동 방식과 의사 결정 과정을 이해하기 어렵다는 비판을 받습니다. 그러나 모델의 투명성과 설명 가능성(interpretability)은 신뢰를 구축하고, 편향을 탐지하며, 책임 소재를 명확히 하는 데 필수적입니다. LLM의 미래는 단순히 답변을 생성하는 것을 넘어, 그 답변이 도출된 근거를 제시하고, 사용자가 모델의 작동 방식을 이해하고 신뢰할 수 있도록 돕는 설명 가능 인공지능(XAI) 기술의 발전에 달려 있습니다. 이는 사전 학습(pre-training) 및 사후 학습(post-training)에 분명히 초점을 맞추었으며, 인간의 피드백을 통합하는 방향으로 나아가고 있습니다.

9.3 **윤리적 가치와 문화적 맥락의 통합**

LLM이 전 세계적으로 사용됨에 따라, 다양한 문화적 맥락과 윤리적 가치를 이해하고 존중하는 것이 중요합니다. 모델이 특정 문화권의 가치관이나 도덕적 기준에만 치우치지 않도록, 다문화적 관점과 윤리적 원칙을 학습 데이터와 모델 설계에 통합해야 합니다. 이는 단순히 유해한 콘텐츠를 필터링하는 것을 넘어, 모델이 다양한 사용자들의 요구와 기대에 부응하며 긍정적인 사회적 상호작용을 촉진하도록 돕습니다.

9.4 **장기적인 사회적 영향 연구 및 정책 개발**

LLM의 발전은 사회의 근본적인 구조를 변화시킬 잠재력을 가지고 있습니다. 이러한 변화에 선제적으로 대응하기 위해, LLM이 장기적으로 사회, 경제, 문화에 미칠 영향을 예측하고 연구하는 것이 필요합니다. 또한, 기술 혁신을 저해하지 않으면서도 공공의 이익을 보호하고 윤리적 사용을 장려하는 정책 및 규제 프레임워크를 개발해야 합니다. 이는 정부, 학계, 산업계, 시민 사회의 협력을 통해 이루어져야 합니다.

---

10. **주요 시사점 및 발전 방향**

이 글에서 논의된 Alibaba의 Qwen 2, Apple의 파운데이션 모델(AFM), Google의 Gemma 2, Meta의 Llama 3.1 이 네 가지 모델의 다양한 사례를 통해 우리는 LLM의 발전이 단순한 기술 경쟁을 넘어선다는 것을 배울 수 있습니다. 이제 LLM은 기술적 성능뿐만 아니라 사회적 책임, 윤리적 고려, 그리고 지속 가능성에 대한 심도 깊은 논의를 요구하고 있습니다. 네 가지 모델 모두 사전 학습(pre-training) 및 사후 학습(post-training)에 다소 다른 접근 방식을 취하지만, 공통적으로 책임감 있는 AI 개발에 대한 중요성을 인식하고 있습니다. 물론 방법론이 겹치기는 하지만, 완전히 동일한 학습 파이프라인(training pipeline)은 없습니다.

사전 학습의 경우, 모든 방법이 다단계 사전 학습 파이프라인을 사용한다는 공통된 특징이 있는 것으로 보이지만, 데이터의 윤리적 수집과 처리 과정이 더욱 강조되어야 합니다. 여기서 일반적인 핵심 사전 학습에 이어 문맥 확장(context lengthening)과 때로는 고품질 어닐링(annealing) 단계가 뒤따릅니다. 이는 데이터의 양적 증가를 넘어 질적 개선과 편향 완화에 중점을 두는 방향으로 진화하고 있습니다. 또한, 에너지 효율적인 아키텍처와 학습 방법론에 대한 연구는 지속 가능한 LLM 개발의 핵심 요소가 될 것입니다.

사후 학습(post-training)에 있어서도 파이프라인 중 정확히 동일한 것은 없었지만, 사용자 피드백과 가치 정렬의 중요성은 일관되게 나타납니다. 거부 샘플링(rejection sampling)은 이제 사후 학습 과정에서 흔한 필수 요소가 된 것 같습니다. 그러나 DPO 또는 RLHF에 관해서는 아직 합의나 선호도(말장난 아님)가 없습니다. 인간의 선호도를 반영하고, 모델의 안전성과 공정성을 확보하기 위한 다양한 정렬(alignment) 기법이 계속해서 발전할 것입니다. 특히, 설명 가능 인공지능(XAI)과 인간-AI 협업 시스템의 도입은 LLM의 사회적 수용성을 높이는 데 기여할 것입니다.

따라서, 고성능 LLM을 개발하는 데는 단 하나의 정답이 아니라 여러 가지 경로가 있습니다. 마지막으로, 네 가지 모델은 비슷한 수준의 성능을 보입니다. 안타깝게도 이 모델들 중 일부는 LMSYS 및 AlpacaEval 리더보드(leaderboard)에 포함되지 않아, MMLU와 같은 객관식 벤치마크(multiple-choice benchmark) 점수를 제외하고는 아직 직접적인 비교가 어렵습니다. 이제는 기술적 우수성과 함께 윤리적 리더십을 발휘하여 인공지능이 인류에게 진정으로 유익한 방향으로 발전할 수 있도록 노력해야 할 때입니다. LLM의 미래는 기술 자체의 발전뿐만 아니라, 우리가 이 기술을 어떻게 이해하고 활용하며, 어떤 가치를 부여하는지에 달려 있습니다.

---

**Ahead of AI 지원**

이 잡지는 개인적인 열정 프로젝트입니다. 저를 지원하고 싶으신 분들은 제 "[Build a Large Language Model (From Scratch)](https://www.amazon.com/Build-Large-Language-Model-Scratch/dp/1032601700)" 책을 구매하여 동료들에게 추천해 주시기를 고려해 주십시오. (이 책은 LLM이 어떻게 작동하는지 다른 곳에서는 찾을 수 없는 수준의 세부 사항으로 설명하므로, 이 책에서 많은 것을 얻으실 것이라고 확신합니다.) Amazon에 남겨주시는 리뷰도 큰 도움이 될 것입니다!

*   Build a Large Language Model (from Scratch)
*   Machine Learning Q and AI
*   Machine Learning with PyTorch and Scikit-Learn

Build a Large Language Model (from Scratch)는 PyTorch로 LLM을 처음부터 코딩하는 데 전념하는 매우 집중적인 책으로, 사전 학습부터 사후 학습까지 모든 것을 다룹니다. 이는 LLM을 진정으로 이해하는 가장 좋은 방법이라고 할 수 있습니다. Machine Learning Q and AI는 기본에 이미 익숙한 분들을 위한 훌륭한 책입니다. 심층 신경망(deep neural network), 비전 트랜스포머(vision transformer), 다중 GPU 학습 패러다임(multi-GPU training paradigm), LLM 등 중간 및 고급 개념을 다룹니다. Machine Learning with PyTorch and Scikit-Learn은 기계 학습, 딥러닝, AI에 대한 포괄적인 가이드로, 이론과 실제 코드의 균형 잡힌 조합을 제공합니다. 이 분야에 처음 입문하는 모든 사람에게 이상적인 시작점입니다.

또한, AI의 책임 있는 개발과 지속 가능한 미래를 위한 논의에 적극적으로 참여해 주시기를 고려해 주십시오. (이러한 노력은 LLM이 어떻게 작동하는지 다른 곳에서는 찾을 수 없는 수준의 세부 사항으로 설명하는 것만큼이나 중요합니다.) AI의 책임 있는 개발은 지금 바로 우리의 참여를 필요로 합니다. 이러한 중요한 논의에 잠시 시간을 내주실 수 있다면, 여러분의 의견과 제안은 큰 도움이 될 것입니다!

Ahead of AI는 독자 지원 출판물입니다. 새로운 게시물을 받고 제 작업을 지원하려면 무료 또는 유료 구독자가 되는 것을 고려해 주십시오. [구독](https://www.aheadofai.com/subscribe)