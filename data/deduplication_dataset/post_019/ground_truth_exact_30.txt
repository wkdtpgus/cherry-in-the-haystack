대규모 언어 모델(LLM)의 개발은 초기 GPT 모델부터 오늘날의 정교한 오픈 웨이트(open-weight) LLM에 이르기까지 많은 발전을 이루었습니다. 초기에 LLM 학습 과정은 사전 학습(pre-training)에만 집중했지만, 이후 사전 학습과 사후 학습(post-training)을 모두 포함하도록 확장되었습니다. 사후 학습은 일반적으로 ChatGPT에 의해 대중화된 지도 학습 기반 명령어 미세 조정(supervised instruction fine-tuning)과 정렬(alignment)을 포함합니다. ChatGPT가 처음 출시된 이후 학습 방법론은 진화해 왔습니다. 이 글에서는 특히 최근 몇 달 동안 이루어진 사전 학습 및 사후 학습 방법론의 최신 발전에 대해 검토합니다. 이 글에서 다루는 새로운 사전 학습 및 사후 학습 방법론에 초점을 맞춘 LLM 개발 및 학습 파이프라인 개요 매달 수백 편의 LLM 논문이 새로운 기술과 접근 방식을 제안하고 있습니다. 그러나 실제로 무엇이 잘 작동하는지 확인하는 가장 좋은 방법 중 하나는 최신 최첨단 모델(state-of-the-art model)의 사전 학습 및 사후 학습 파이프라인을 살펴보는 것입니다. 다행히 지난 몇 달 동안 4개의 주요 신규 LLM이 비교적 상세한 기술 보고서(technical report)와 함께 출시되었습니다. 빠르게 변화하는 LLM 연구 환경 속에서, 새로운 아키텍처와 학습 전략이 끊임없이 등장하고 있습니다. 이 글은 단순히 새로운 모델을 나열하는 것을 넘어, 각 모델이 어떤 독특한 방식으로 사전 학습 및 사후 학습 문제를 해결했는지 심층적으로 분석합니다. 특히 데이터 품질의 중요성, 다단계 학습 접근 방식, 그리고 다양한 정렬(alignment) 기법의 진화를 조명합니다. 이 글에서는 다음 모델들의 사전 학습 및 사후 학습 파이프라인에 중점을 둡니다:

*   Alibaba의 Qwen 2
*   Apple Intelligence Foundation Language Models
*   Google의 Gemma 2
*   Meta AI의 Llama 3.1

이 모델들은 arXiv.org에 게시된 각 기술 논문의 발행 날짜 순으로 제시되었으며, 이는 또한 알파벳 순서와도 일치합니다. 이 글은 제가 여가 시간과 주말을 이용하여 만든 열정적인 프로젝트입니다. 이 글이 유용하다고 생각하시고 제 작업을 지원하고 싶으시다면, 제 책을 구매하여 동료들에게 추천해 주시기를 고려해 주십시오. Amazon에 남겨주시는 리뷰도 큰 도움이 될 것입니다!

*   Build a Large Language Model (from Scratch)
*   Machine Learning Q and AI
*   Machine Learning with PyTorch and Scikit-Learn

Build a Large Language Model (from Scratch)는 PyTorch로 LLM을 처음부터 코딩하는 데 전념하는 매우 집중적인 책으로, 사전 학습부터 사후 학습까지 모든 것을 다룹니다. 이는 LLM을 진정으로 이해하는 가장 좋은 방법이라고 할 수 있습니다. Machine Learning Q and AI는 기본에 이미 익숙한 분들을 위한 훌륭한 책입니다. 심층 신경망(deep neural network), 비전 트랜스포머(vision transformer), 다중 GPU 학습 패러다임(multi-GPU training paradigm), LLM 등 중간 및 고급 개념을 다룹니다. Machine Learning with PyTorch and Scikit-Learn은 기계 학습, 딥러닝, AI에 대한 포괄적인 가이드로, 이론과 실제 코드의 균형 잡힌 조합을 제공합니다. 이 분야에 처음 입문하는 모든 사람에게 이상적인 시작점입니다.

---

1.  **Alibaba의 Qwen 2**

다른 주요 LLM과 경쟁할 수 있는 매우 강력한 LLM 모델 제품군인 Qwen 2부터 시작하겠습니다. 하지만 어떤 이유에서인지 Meta AI, Microsoft, Google의 오픈 웨이트(open-weight) 모델보다 인기가 덜합니다. 이는 주로 서구권 중심의 LLM 생태계와 접근성, 그리고 커뮤니티 지원의 차이에서 비롯될 수 있습니다. 그럼에도 불구하고 Qwen 2는 아시아 시장 및 다국어 지원 측면에서 독보적인 강점을 보여줍니다.

1.1 **Qwen 2 개요**

Qwen 2 기술 보고서(Technical Report)에서 논의된 사전 학습 및 사후 학습 방법을 살펴보기 전에 몇 가지 핵심 사양을 간략하게 요약해 보겠습니다. Qwen 2 모델은 5가지 종류로 제공됩니다. 0.5억, 1.5억, 7억, 720억 매개변수(parameter) 크기의 4가지 일반(밀집) LLM이 있습니다. 또한, 570억 매개변수를 가진 전문가 혼합(Mixture-of-Experts) 모델이 있으며, 이 모델에서는 140억 매개변수가 동시에 활성화됩니다. (이번에는 아키텍처 세부 사항이 주요 초점이 아니므로 전문가 혼합(Mixture-of-Experts) 모델에 대해 너무 깊이 다루지는 않겠습니다. 하지만 간단히 말해, 이는 Mistral AI의 Mixtral과 유사하지만, 더 많은 활성 전문가(expert)를 가지고 있습니다. 높은 수준의 개요는 제 "Model Merging, Mixtures of Experts, and Towards Smaller LLMs" 글의 Mixtral 아키텍처 섹션을 참조하십시오.)

Qwen 2 LLM의 뛰어난 특징 중 하나는 30개 언어에 걸친 뛰어난 다국어 기능(multilingual capability)입니다. 또한 놀랍도록 큰 151,642개의 토큰 어휘(token vocabulary)를 가지고 있습니다(참고로 Llama 2는 32k 어휘를 사용하고, Llama 3.1은 128k 토큰 어휘를 사용합니다). 경험상 어휘 크기를 2배 늘리면 입력 토큰 수가 2배 줄어들어 동일한 입력에 더 많은 텍스트를 담을 수 있습니다. 이는 특히 다국어 데이터 처리 및 코딩 관련 작업에서 효율성을 극대화합니다. 또한 표준 영어 어휘 외의 단어를 다루는 다국어 데이터 및 코딩에 특히 도움이 됩니다. 이러한 대규모 어휘는 토크나이저(tokenizer)의 효율성을 높여 문맥 창(context window) 활용도를 최적화하는 데 기여합니다.

아래는 나중에 다룰 다른 LLM과의 간략한 MMLU 벤치마크(benchmark) 비교입니다. (MMLU는 객관식 벤치마크(multiple-choice benchmark)이므로 한계가 있지만, 여전히 LLM 성능을 보고하는 가장 인기 있는 방법 중 하나입니다.)

최신 오픈 웨이트(open-weight) 모델의 MMLU 벤치마크 점수(높을수록 좋음). 이 그래프의 점수는 각 모델의 공식 연구 논문에서 수집했습니다. (MMLU에 대해 처음이시라면, 최근 강연 46분 05초에서 간략하게 설명했습니다.)

1.2 **Qwen 2 사전 학습(Pre-training)**

Qwen 2 팀은 15억, 70억, 720억 매개변수(parameter) 모델을 7조 개의 학습 토큰(training token)으로 학습시켰는데, 이는 합리적인 규모입니다. 비교하자면, Llama 2 모델은 2조 개의 토큰으로, Llama 3.1 모델은 15조 개의 토큰으로 학습되었습니다. 흥미롭게도 5억 매개변수 모델은 12조 개의 토큰으로 학습되었습니다. 그러나 연구원들은 학습 중에 어떠한 개선도 관찰하지 못했고 추가적인 계산 비용이 정당화되지 않았기 때문에 다른 모델들을 더 큰 12조 개의 토큰 데이터셋으로 학습시키지 않았습니다. 이는 단순한 데이터량 증가가 성능 향상으로 직결되지 않음을 시사하며, 데이터 품질과 효율적인 학습 전략의 중요성을 강조합니다.

주요 초점 영역 중 하나는 저품질 데이터를 제거하기 위한 데이터 필터링 파이프라인(data filtering pipeline)을 개선하고 데이터 다양성을 높이기 위한 데이터 혼합(data mixing)을 강화하는 것이었습니다. 이는 나중에 다른 모델을 검토할 때 다시 다룰 주제입니다. 특히 Qwen 2는 휴리스틱 기반 필터링과 함께 학습된 분류기(classifier)를 활용하여 데이터의 품질을 엄격하게 관리했습니다. 흥미롭게도 그들은 추가 사전 학습 데이터를 합성하기 위해 Qwen 모델(자세한 내용은 명시하지 않았지만, 이전 세대 Qwen 모델을 의미한다고 가정합니다)도 사용했습니다. 이러한 합성 데이터(synthetic data)는 모델이 특정 도메인이나 작업에 대한 이해를 심화하는 데 도움을 주며, 특히 희귀하거나 고품질의 데이터가 부족한 경우 유용하게 활용됩니다. 그리고 사전 학습에는 "인컨텍스트 학습(in-context learning) 및 명령어 추종 능력(instruction-following ability)을 향상시키기 위한 다중 작업 명령어 데이터(multi-task instruction data)"가 포함되었습니다. 이는 모델이 복잡한 지시를 이해하고 따르는 능력을 초기 학습 단계부터 강화하려는 시도입니다.

또한, 그들은 일반 사전 학습에 이어 장문맥 학습(long-context training)이라는 두 단계로 학습을 수행했습니다. 후자는 "고품질의 긴 데이터"를 사용하여 사전 학습의 마지막 단계에서 문맥 길이(context length)를 4,096개에서 32,768개 토큰으로 늘렸습니다. 이러한 점진적인 문맥 확장(context lengthening)은 모델이 장거리 의존성(long-range dependencies)을 효과적으로 학습할 수 있도록 돕습니다.

Qwen 2 사전 학습(pre-training)을 위한 기술 요약. "연속 사전 학습(Continued pre-training)"은 연구원들이 일반 사전 학습으로 시작하여 장문맥 연속 사전 학습을 이어서 수행한 2단계 사전 학습을 의미합니다. (안타깝게도 기술 보고서의 또 다른 특징은 데이터셋에 대한 세부 정보가 부족하다는 것입니다. 따라서 제 글이 매우 상세하지 않다면, 이는 공개적으로 이용 가능한 정보의 부족 때문입니다.)

1.3 **Qwen 2 사후 학습(Post-training)**

Qwen 2 팀은 인기 있는 2단계 사후 학습(post-training) 방법론을 사용했는데, 이는 500,000개 예시에 대해 2 에포크(epoch) 동안 적용된 지도 명령어 미세 조정(SFT)으로 시작했습니다. 이 단계는 미리 정해진 시나리오에서 모델의 응답 정확도를 개선하는 것을 목표로 했습니다. SFT 단계에서는 고품질의 명령어-응답 쌍을 사용하여 모델이 특정 지시를 정확히 따르도록 학습시킵니다.

일반적인 LLM 개발 흐름. SFT 후, 그들은 직접 선호 최적화(DPO)를 사용하여 LLM을 인간의 선호도에 정렬(align)시켰습니다. (흥미롭게도 그들의 용어로는 인간 피드백 기반 강화 학습(RLHF)이라고 불립니다.) 몇 주 전 "LLM 사전 학습 및 보상 모델 평가 팁(Tips for LLM Pretraining and Evaluating Reward Models)" 글에서 논의했듯이, SFT+DPO 접근 방식은 PPO를 사용한 RLHF와 같은 다른 방법들에 비해 사용 편의성 때문에 현재 가장 인기 있는 선호 조정 전략인 것 같습니다. (DPO가 어떻게 작동하는지 배우고 싶다면, 제가 최근에 여기에서 처음부터 구현했습니다.) DPO는 PPO와 같은 복잡한 강화 학습 파이프라인 없이도 보상 모델을 직접적으로 정책 모델에 통합하여 학습시킬 수 있다는 장점이 있습니다.

정렬(alignment) 단계 자체도 2단계로 진행되었습니다. 첫째, 기존 데이터셋에 DPO를 사용했습니다(오프라인 단계). 둘째, 보상 모델(reward model)을 사용하여 선호 쌍을 형성했습니다(온라인). 여기서 모델은 학습 중에 여러 응답을 생성하고, 보상 모델은 "실시간"(즉, 학습 중)으로 최적화 단계에 가장 선호되는 응답을 선택합니다. 이는 종종 "거부 샘플링(rejection sampling)"이라고도 불립니다. 이 온라인 단계는 모델이 생성한 다양한 응답 중에서 최적의 응답을 선택하여 학습 효율을 높이는 데 기여합니다.

데이터셋 구축을 위해 그들은 기존 코퍼스(corpus)에 인간 라벨링(human labeling)을 보완하여 SFT를 위한 목표 응답을 결정하고 DPO에 필수적인 선호 및 거부 응답을 식별했습니다. 연구원들은 또한 인공적으로 주석이 달린 데이터(artificially annotated data)를 합성했습니다. 더욱이, 팀은 LLM을 사용하여 "고품질 문학 데이터(literary data)"에 특별히 맞춰진 명령어-응답 쌍(instruction-response pair)을 생성하여 학습을 위한 고품질 Q&A 쌍(Q&A pair)을 만들었습니다. 이러한 합성 데이터 생성은 특정 도메인에 대한 모델의 전문성을 높이는 데 효과적인 전략입니다.

Qwen 2 사후 학습(post-training)을 위한 기술 요약.

1.4 **결론**

Qwen 2는 비교적 유능한 모델이며, 이전 세대 Qwen과 유사합니다. 2023년 12월 NeurIPS LLM 효율성 챌린지(challenge)에 참석했을 때, 우승한 접근 방식의 대부분이 Qwen 모델을 포함하고 있었던 것이 기억납니다. Qwen 2의 학습 파이프라인(training pipeline)과 관련하여 눈에 띄는 점은 사전 학습(pre-training)과 사후 학습(post-training) 모두에 합성 데이터(synthetic data)가 사용되었다는 것입니다. 이는 고품질의 다양한 데이터를 확보하기 위한 전략적 선택으로, 특히 특정 언어나 문화권에 특화된 데이터가 부족할 때 효과적입니다. 또한, (가능한 한 많은 데이터를 수집하기보다는) 데이터셋 필터링에 중점을 두는 것이 LLM 학습의 주목할 만한 추세 중 하나입니다. 여기서 저는 "더 많은 것이 더 좋다"고 말하겠지만, 이는 특정 품질 기준을 충족할 때만 해당됩니다. Qwen 2의 강력한 다국어 기능은 글로벌 LLM 시장에서 중요한 경쟁 우위로 작용하며, 향후 더 다양한 언어와 문화권에 특화된 LLM 개발의 방향을 제시합니다.

**처음부터 직접 선호 최적화(Direct Preference Optimization)로 LLM 정렬**

직접 선호 최적화(DPO)는 LLM을 사용자 선호도에 더 가깝게 정렬(align)시키는 주요 방법 중 하나가 되었으며, 이 글에서 많이 접하게 될 내용입니다. DPO가 어떻게 작동하는지 배우고 싶다면, 제가 여기에서 처음부터 코딩했습니다: [LLM 정렬을 위한 직접 선호 최적화(DPO) (처음부터)](https://www.aheadofai.com/p/direct-preference-optimization-dpo-for-llm-alignment-from-scratch).

LLM 정렬을 위한 DPO 개요

Ahead of AI는 독자 지원 출판물입니다. 새로운 게시물을 받고 제 작업을 지원하려면 무료 또는 유료 구독자가 되는 것을 고려해 주십시오. [구독](https://www.aheadofai.com/subscribe)

---

2.  **Apple의 Apple Intelligence Foundation Language Models (AFM)**

arXiv.org에서 Apple의 모델 학습을 설명하는 또 다른 기술 논문(technical paper)을 보게 되어 정말 기뻤습니다. 예상치 못했지만 분명히 긍정적인 놀라움이었습니다! Apple은 그동안 자사의 AI 기술을 내부적으로 개발하고 활용해왔지만, 최근 오픈 소스 LLM 생태계에 참여하려는 움직임을 보이고 있습니다.

2.1 **AFM 개요**

[Apple Intelligence Foundation Language Models 논문](https://arxiv.org/abs/2407.03052)에서 연구팀은 Apple 기기의 "Apple Intelligence" 맥락에서 사용하도록 설계된 두 가지 주요 모델의 개발을 설명합니다. 간결성을 위해 이 섹션 전체에서 이 모델들은 "Apple Foundation Models"의 약어인 AFM으로 지칭될 것입니다. 구체적으로, 이 논문은 AFM의 두 가지 버전을 설명합니다: 휴대폰, 태블릿 또는 노트북에 배포하기 위한 30억 매개변수(parameter) 온디바이스(on-device) 모델과 크기가 명시되지 않은 더 강력한 서버 모델입니다. 이 모델들은 채팅, 수학 및 코딩 작업을 위해 개발되었지만, 논문에서는 코딩 관련 학습 및 기능에 대해서는 논의하지 않습니다. Qwen 2와 마찬가지로 AFM은 밀집 LLM이며 전문가 혼합(mixture-of-experts) 접근 방식을 사용하지 않습니다. 온디바이스 모델의 존재는 Apple이 개인 정보 보호와 낮은 지연 시간(latency)을 중요하게 생각하며, 클라우드 의존도를 줄이려는 전략을 가지고 있음을 보여줍니다.

2.2 **AFM 사전 학습(Pre-training)**

연구원들에게 두 가지 큰 칭찬을 하고 싶습니다. 첫째, 공개적으로 사용 가능한 데이터와 출판사로부터 라이선스(license)를 받은 데이터를 사용하는 것 외에도, 그들은 웹사이트의 robots.txt 파일을 존중하고 크롤링(crawling)을 자제했습니다. 이는 데이터 수집의 윤리적 측면을 고려한 모범적인 사례로 평가됩니다. 둘째, 벤치마크 데이터(benchmark data)로 오염 제거(decontamination)를 수행했다고 언급했습니다. 이는 모델이 벤치마크 데이터에 과적합(overfit)되는 것을 방지하고 실제 성능을 정확하게 측정하기 위한 필수적인 과정입니다. Qwen 2 논문의 주요 시사점 중 하나를 강조하기 위해, 연구원들은 양보다 질이 훨씬 더 중요하다고 언급했습니다. (디바이스 모델의 어휘 크기는 49k 토큰, 서버 모델은 100k 토큰으로, 150k 토큰 어휘를 사용한 Qwen 2 모델보다 어휘 크기가 현저히 작았습니다.)

흥미롭게도 사전 학습은 2단계가 아닌 3단계로 진행되었습니다! 이러한 다단계 접근 방식은 모델이 각 단계에서 특정 학습 목표를 달성하도록 설계된 일종의 커리큘럼 학습(curriculum learning)으로 볼 수 있습니다.

*   핵심(일반) 사전 학습(pre-training)
*   웹 크롤링(web-crawl) (저품질) 데이터의 가중치를 낮추고, 수학 및 코드 데이터의 가중치를 높인 연속 사전 학습(continued pre-training)
*   더 긴 시퀀스 데이터와 합성 데이터(synthetic data)를 사용한 문맥 확장(context-lengthening)

AFM 모델이 거친 3단계 사전 학습(pre-training) 과정 개요. 이 3단계를 좀 더 자세히 살펴보겠습니다.

2.2.1 **사전 학습(Pre-training) I: 핵심 사전 학습(Core Pre-training)**

핵심 사전 학습은 Apple의 사전 학습 파이프라인에서 첫 번째 사전 학습 단계를 설명합니다. 이는 일반적인 사전 학습과 유사하며, AFM-서버 모델은 6.3조 개의 토큰, 4096의 배치 크기(batch size), 4096 토큰의 시퀀스 길이(sequence length)로 학습되었습니다. 이는 7조 개의 토큰으로 학습된 Qwen 2 모델과 매우 유사합니다.

그러나 AFM 온디바이스(on-device) 모델의 경우 더 흥미로운데, 이는 더 큰 64억 매개변수(parameter) 모델에서 증류 및 가지치기(distilled and pruned)되었습니다(이전 단락에서 설명된 AFM-서버 모델처럼 처음부터 학습됨). 증류 과정에 대한 자세한 내용은 "목표 레이블(target label)을 실제 레이블과 교사 모델(teacher model)의 상위 1개 예측(교사 레이블에 0.9 가중치 할당)의 볼록 조합(convex combination)으로 대체하여 증류 손실(distillation loss)이 사용된다"는 것 외에는 많지 않습니다. 지식 증류(knowledge distillation)가 LLM 사전 학습에 점점 더 널리 퍼지고 유용해지고 있다고 생각합니다(Gemma-2도 이를 사용합니다). 언젠가 더 자세히 다룰 계획입니다. 지금은 이 과정이 높은 수준에서 어떻게 작동하는지에 대한 간략한 개요입니다.

작은 모델(여기서는 AFM-디바이스 3B 모델)이 원래 학습 토큰과 더 큰 교사 모델(여기서는 6.4B 모델)의 출력으로 학습되는 지식 증류(knowledge distillation) 개요. a)의 교차 엔트로피 손실(cross entropy loss)은 LLM 사전 학습에 사용되는 일반적인 학습 손실입니다(일반 사전 학습 단계가 어떻게 구현되는지에 대한 자세한 내용은 제 "[Build a Large Language Model from Scratch](https://www.amazon.com/Build-Large-Language-Model-Scratch/dp/1032601700)" 책의 5장을 참조하십시오).

위에 설명된 지식 증류(knowledge distillation)는 여전히 원래 데이터셋으로 학습하는 것을 포함합니다. 그러나 데이터셋의 학습 토큰 외에도 학습될 모델(학생 모델(student model)이라고 함)은 더 큰 (교사) 모델로부터 정보를 받는데, 이는 지식 증류 없이 학습하는 것보다 더 풍부한 신호를 제공합니다. 단점은 다음과 같습니다: 1) 더 큰 교사 모델을 먼저 학습시켜야 하고, 2) 더 큰 교사 모델을 사용하여 모든 학습 토큰에 대한 예측을 계산해야 합니다. 이러한 예측은 미리 계산할 수 있거나(상당한 저장 공간 필요) 학습 중에 계산할 수 있습니다(학습 프로세스 속도 저하 가능). 하지만 온디바이스 모델의 경우, 이러한 효율성 증대는 매우 중요합니다.

2.2.2 **사전 학습(Pre-training) II: 연속 사전 학습(Continued Pre-training)**

연속 사전 학습 단계는 1조 개의 토큰으로 구성된 데이터셋(핵심 사전 학습 세트는 5배 더 컸음)에서 문맥 길이(context length)를 4,096개에서 8,192개 토큰으로 늘리는 작은 문맥 확장(context lengthening) 단계를 포함합니다. 그러나 주요 초점은 수학 및 코드에 중점을 둔 고품질 데이터 혼합(data mix)으로 학습하는 것입니다. 이는 모델이 특정 전문 분야에서 더 뛰어난 성능을 발휘하도록 유도하는 전략입니다. 흥미롭게도 연구원들은 이 맥락에서 증류 손실(distillation loss)이 유익하지 않다는 것을 발견했습니다. 이는 일반적인 언어 이해와 달리, 특정 전문 지식의 학습에는 직접적인 데이터가 더 효과적일 수 있음을 시사합니다.

2.2.3 **사전 학습(Pre-training) III: 문맥 확장(Context Lengthening)**

세 번째 사전 학습 단계는 1,000억 개의 토큰(두 번째 단계에서 사용된 토큰의 10%)만 포함하지만, 문맥 길이(context length)를 32,768개 토큰으로 더 크게 확장하는 것을 나타냅니다. 이를 달성하기 위해 연구원들은 합성 장문맥 Q&A 데이터(synthetic long-context Q&A data)로 데이터셋을 증강했습니다. 이 단계는 모델이 매우 긴 문맥을 이해하고 추론하는 능력을 극대화하는 데 중점을 둡니다.

AFM 사전 학습(pre-training)을 위한 기술 요약.

2.3 **AFM 사후 학습(Post-training)**

Apple은 사전 학습(pre-training)과 마찬가지로 사후 학습(post-training) 과정에도 유사하게 포괄적인 접근 방식을 취한 것으로 보입니다. 그들은 인간이 주석을 단 데이터와 합성 데이터(synthetic data)를 모두 활용했으며, 데이터 품질이 양보다 우선시된다는 점을 강조했습니다. 흥미롭게도 그들은 미리 정해진 데이터 비율에 의존하지 않고, 여러 실험을 통해 데이터 혼합(data mixture)을 미세 조정하여 최적의 균형을 달성했습니다. 이는 실용적인 LLM 개발에서 데이터 혼합의 중요성을 다시 한번 보여줍니다.

사후 학습 단계는 지도 명령어 미세 조정(supervised instruction fine-tuning)과 여러 차례의 인간 피드백 기반 강화 학습(RLHF)으로 구성된 2단계 프로세스를 포함했습니다. 이 과정에서 특히 주목할 만한 점은 Apple이 RLHF 단계에 두 가지 새로운 알고리즘을 도입했다는 것입니다:

*   교사 위원회(Teacher Committee)를 이용한 거부 샘플링 미세 조정(iTeC)
*   미러 디센트 정책 최적화(Mirror Descent Policy Optimization)를 이용한 RLHF

이 글의 길이를 고려하여 이 방법들의 기술적 세부 사항은 다루지 않겠지만, 간략한 개요는 다음과 같습니다:

iTeC 알고리즘은 거부 샘플링(rejection sampling)과 여러 선호 조정 기법(preference tuning technique)을 결합합니다. 구체적으로 SFT, DPO, IPO, 그리고 온라인 강화 학습(online RL)입니다. Apple은 단일 알고리즘에 의존하기보다는 각 접근 방식을 독립적으로 사용하여 모델을 학습시켰습니다. 그런 다음 이 모델들은 응답을 생성했고, 인간이 선호 레이블(preference label)을 제공하여 이를 평가했습니다. 이 선호 데이터는 RLHF 프레임워크에서 보상 모델(reward model)을 반복적으로 학습시키는 데 사용되었습니다. 거부 샘플링(rejection sampling) 단계에서는 모델 위원회(committee of models)가 여러 응답을 생성했고, 보상 모델이 가장 좋은 응답을 선택했습니다. 이 위원회 기반 접근 방식은 상당히 복잡하지만, 관련된 모델의 비교적 작은 크기(약 30억 매개변수)를 고려할 때 비교적 실현 가능해야 합니다. Llama 3.1의 70B 또는 405B 매개변수 모델과 같이 훨씬 더 큰 모델로 이러한 위원회를 구현하는 것은 확실히 더 어려울 것입니다. 이처럼 여러 모델의 앙상블(ensemble)을 통해 얻은 응답을 평가하는 것은 단일 모델의 한계를 보완하고 더 견고한 정렬을 가능하게 합니다.

두 번째 알고리즘인 미러 디센트(Mirror Descent)를 이용한 RLHF는 일반적으로 사용되는 PPO(근접 정책 최적화(Proximal Policy Optimization))보다 더 효과적임이 입증되었기 때문에 선택되었습니다. 미러 디센트는 특히 복잡한 최적화 문제에서 수렴 안정성과 효율성을 향상시키는 것으로 알려져 있습니다.

AFM 사후 학습(post-training)을 위한 기술 요약.

2.4 **결론**

Apple의 사전 학습(pre-training) 및 사후 학습(post-training) 접근 방식은 비교적 포괄적이며, 이는 아마도 위험 부담이 매우 높기 때문일 것입니다(모델이 수백만, 심지어 수십억 대의 기기에 배포됨). 그러나 이 모델들의 작은 특성을 고려할 때, 3B 모델은 가장 작은 Llama 3.1 모델 크기의 절반에도 미치지 못하므로, 광범위한 기술 또한 실현 가능해집니다. 주요 특징 중 하나는 RLHF와 DPO 사이의 단순한 선택이 아니라는 것입니다. 대신, 그들은 위원회(committee) 형태로 여러 선호 조정(preference-tuning) 알고리즘을 사용했습니다. 이는 다양한 정렬 기법의 장점을 통합하고 단점을 상쇄하려는 시도로 볼 수 있습니다. 또한 사전 학습의 일부로 Q&A 데이터를 명시적으로 사용했다는 점도 흥미롭습니다. 이는 제가 이전 글인 "[명령어 사전 학습 LLM(Instruction Pretraining LLMs)](https://www.aheadofai.com/p/instruction-pretraining-llms)"에서 논의했던 내용입니다. 전반적으로, 신선하고 즐거운 기술 보고서(technical report)입니다. Apple의 이러한 접근 방식은 온디바이스 AI의 미래를 위한 청사진을 제공하며, 효율성과 성능을 동시에 추구하는 방법을 보여줍니다.

---

3.  **Google의 Gemma 2**

Google의 Gemma 모델은 최근 "[Gemma 2: Improving Open Language Models at a Practical Size](https://arxiv.org/abs/2407.13038)"에서 설명되었습니다. 다음 개요 섹션에서 몇 가지 주요 사실을 간략하게 설명한 후 사전 학습(pre-training) 및 사후 학습(post-training) 프로세스에 대해 논의하겠습니다. Gemma 2는 Google의 핵심 기술을 활용하여 오픈 소스 커뮤니티에 기여하려는 Google의 노력을 보여주는 중요한 모델입니다.

3.1 **Gemma 2 개요**

Gemma 2 모델은 20억, 90억, 270억 매개변수(parameter)의 세 가지 크기로 제공됩니다. 주요 초점은 학습 데이터셋의 크기를 반드시 늘릴 필요는 없지만, 비교적 작고 효율적인 LLM을 개발하는 기술을 탐구하는 데 있습니다. 이는 "작은 것이 아름답다"는 LLM 개발의 새로운 패러다임을 반영합니다. 특히 Gemma 2는 256k 토큰이라는 상당한 어휘 크기를 특징으로 합니다. 비교를 위해 Llama 2는 32k 토큰 어휘를 사용하고, Llama 3는 128k 토큰 어휘를 사용합니다. 이러한 대규모 어휘는 다양한 언어와 전문 용어를 효율적으로 처리할 수 있게 하여, 토크나이징 과정에서 정보 손실을 최소화합니다. 또한 Gemma 2는 Mistral의 초기 모델과 유사하게 슬라이딩 윈도우 어텐션(sliding window attention)을 사용하여 메모리 비용을 줄이는 것으로 보입니다. 이는 장문맥 처리 시 계산 효율성을 크게 향상시키는 기술입니다. Gemma 2 아키텍처에 대한 자세한 내용은 제 이전 글의 [Gemma 2 섹션](https://www.aheadofai.com/p/model-merging-mixtures-of-experts-and-towards-smaller-llms#%C2%A7gemma-2)을 참조하십시오.

3.2 **Gemma 2 사전 학습(Pre-training)**

Gemma 연구원들은 작은 모델조차도 종종 불충분하게 학습된(undertrained) 상태라고 주장합니다. 이는 스케일링 법칙(scaling laws)에 따르면, 모델 크기에 비해 학습 토큰의 양이 부족할 때 발생할 수 있습니다. 그러나 단순히 학습 데이터셋의 크기를 늘리는 대신, 그들은 품질 유지에 중점을 두고 Apple의 접근 방식과 유사하게 지식 증류(knowledge distillation)와 같은 대체 방법을 통해 개선을 달성합니다. 27B Gemma 2 모델은 처음부터 학습되었지만, 더 작은 모델들은 이전에 설명된 Apple의 접근 방식과 유사하게 지식 증류(knowledge distillation)를 사용하여 학습되었습니다. 지식 증류는 더 큰 교사 모델의 "지식"을 작은 학생 모델에 전달하여, 작은 모델이 제한된 데이터로도 높은 성능을 달성할 수 있도록 합니다. 27B 모델은 13조 개의 토큰으로, 9B 모델은 8조 개의 토큰으로, 2B 모델은 2조 개의 토큰으로 학습되었습니다. 또한, Apple의 접근 방식과 유사하게 Gemma 팀은 성능 향상을 위해 데이터 혼합(data mixture)을 최적화했습니다. 이는 특정 작업에 더 적합한 데이터를 더 많이 포함시키거나, 데이터의 다양성을 높이는 방식으로 이루어졌습니다.

Gemma 2 사전 학습(pre-training)을 위한 기술 요약.

3.3 **Gemma 2 사후 학습(Post-training)**

Gemma 모델의 사후 학습(post-training) 과정은 일반적인 지도 미세 조정(SFT) 및 인간 피드백 기반 강화 학습(RLHF) 단계를 포함했습니다. 명령어 데이터는 인간이 생성한 콘텐츠와 합성 생성 콘텐츠가 혼합된 영어 전용 프롬프트 쌍(English-only prompt pair)을 사용했습니다. 특히 흥미롭게도, 응답은 주로 교사 모델(teacher model)에 의해 생성되었으며, SFT 단계에서도 지식 증류(knowledge distillation)가 적용되었습니다. 이는 SFT 단계에서도 교사 모델의 전문성을 활용하여 학생 모델의 초기 정렬을 강화하려는 전략입니다.

SFT 이후 그들의 RLHF 접근 방식의 흥미로운 점은 RLHF에 사용되는 보상 모델(reward model)이 정책(목표) 모델보다 10배 더 크다는 것입니다. 이는 보상 모델의 정확성과 견고성이 RLHF의 성공에 얼마나 중요한지를 보여줍니다. Gemma가 사용하는 RLHF 알고리즘은 상당히 표준적이지만, 독특한 특징이 있습니다: 그들은 WARM(가중치 평균 보상 모델)(weight-averaged reward models)의 후속작인 WARP라는 방법을 통해 정책 모델들을 평균화합니다. 저는 이전에 "Model Merging, Mixtures of Experts, and Towards Smaller LLMs"라는 제 글에서 이 방법을 자세히 논의했습니다. WARP는 학습 과정에서 여러 체크포인트(checkpoint)의 정책 모델을 평균화하여 더 안정적이고 성능이 좋은 최종 모델을 얻는 데 기여합니다.

Gemma 2 사후 학습(post-training)을 위한 기술 요약.

3.4 **결론**

Gemma 팀은 Apple과 유사하게 사전 학습(pre-training)과 사후 학습(post-training) 모두에서 지식 증류(knowledge distillation)에 정말 집중하는 것으로 보입니다. 이는 효율적이고 성능 좋은 소형 모델을 개발하기 위한 핵심 전략으로 자리 잡고 있습니다. 흥미롭게도 그들은 다단계 사전 학습 접근 방식을 사용하지 않았거나, 적어도 논문에서 자세히 설명하지는 않았습니다. 이는 단일 단계의 효율적인 학습과 지식 증류가 충분히 강력한 결과를 가져올 수 있음을 시사할 수 있습니다. Google의 Gemma 2는 제한된 자원으로도 고성능 LLM을 구축할 수 있다는 가능성을 보여주며, 특히 온디바이스 또는 엣지(edge) 환경에서의 LLM 배포에 중요한 시사점을 제공합니다.

다가오는 PyTorch 컨퍼런스(conference)에서 기조 강연(keynote talk)을 하게 되어 기쁩니다. 첫 PyTorch 컨퍼런스가 될 것이며, 커뮤니티를 만나 최신 AI 및 LLM 개발에 대해 이야기할 수 있기를 기대합니다!

---

4.  **Meta AI의 Llama 3.1**

Meta의 Llama LLM의 새로운 출시는 항상 큰 이슈입니다. 이번에는 92페이지 분량의 기술 보고서(technical report)인 "[The Llama 3 Herd of Models](https://arxiv.org/abs/2407.13038)"와 함께 출시되었습니다. 마지막으로, 이 섹션에서는 지난달에 발표된 네 번째 대규모 모델 논문을 살펴보겠습니다. Llama 시리즈는 오픈 소스 LLM의 표준을 제시하며, 연구 및 상업적 활용에 지대한 영향을 미치고 있습니다.

4.1 **Llama 3.1 개요**

Meta는 거대한 4050억 매개변수(parameter) 모델을 출시하는 것과 함께, 이전의 80억 및 700억 매개변수 모델을 업데이트하여 MMLU 성능을 약간 향상시켰습니다.

다양한 모델의 MMLU 벤치마크(benchmark) 성능. Llama 3는 다른 최신 LLM과 마찬가지로 그룹 쿼리 어텐션(group query attention)을 사용하지만, 놀랍게도 Meta AI는 슬라이딩 윈도우 어텐션(sliding window attention)과 전문가 혼합(Mixture-of-Experts) 접근 방식에 대해서는 "아니오"라고 말했습니다. 즉, Llama 3.1은 매우 전통적으로 보이며, 아키텍처 혁신보다는 사전 학습(pre-training) 및 사후 학습(post-training)에 분명히 초점을 맞추었습니다. 이는 기존의 검증된 아키텍처를 기반으로 학습 데이터와 정렬 기법을 최적화하는 것이 강력한 LLM을 만드는 효과적인 방법임을 보여줍니다. 이전 Llama 출시와 마찬가지로 가중치(weight)는 공개적으로 사용 가능합니다. 또한 Meta는 Llama 3 라이선스(license)를 업데이트하여 이제 다른 모델을 개선하기 위한 합성 데이터(synthetic data) 생성 또는 지식 증류(knowledge distillation)에 Llama 3를 사용하는 것이 마침내 가능(허용)해졌다고 밝혔습니다. 이는 오픈 소스 커뮤니티의 혁신을 더욱 가속화할 것으로 기대됩니다.

4.2 **Llama 3.1 사전 학습(Pre-training)**

Llama 3는 Llama 2의 1.8조 개 토큰에서 크게 증가한 15.6조 개의 토큰 데이터셋으로 학습되었습니다. 연구원들은 Llama 3가 최소 8개 언어를 지원한다고 말합니다(Qwen 2는 20개 언어를 처리할 수 있습니다). Llama 3의 흥미로운 측면은 OpenAI의 tiktoken 토크나이저(tokenizer)를 사용하여 개발된 128,000개의 어휘 크기입니다. (토크나이저 성능에 관심 있는 분들을 위해 여기에서 [간단한 벤치마크(benchmark) 비교](https://www.aheadofai.com/p/tokenizer-benchmark-gpt-4-llama-3-qwen-2)를 했습니다.) 이 대규모 어휘는 다국어 및 다양한 도메인의 텍스트를 효율적으로 인코딩하는 데 필수적입니다.

사전 학습(pre-training) 데이터 품질 관리 측면에서 Llama 3는 Meta AI의 fastText 및 RoBERTa 기반 분류기(classifier)와 같은 고속 분류기(fast classifier)를 활용하여 휴리스틱 기반 필터링(heuristic-based filtering)과 모델 기반 품질 필터링(model-based quality filtering)을 함께 사용합니다. 이러한 분류기는 학습 중에 사용되는 데이터 혼합(data mix)의 문맥 범주(context category)를 결정하는 데도 도움이 됩니다. 이는 데이터의 양뿐만 아니라 질과 다양성까지 고려하는 정교한 데이터 준비 과정을 보여줍니다.

Llama 3의 사전 학습은 세 단계로 나뉩니다. 첫 번째 단계는 8k 문맥 창(context window)을 가진 15.6조 개의 토큰을 사용하여 표준 초기 사전 학습을 포함합니다. 두 번째 단계는 사전 학습을 계속하지만 문맥 길이(context length)를 128k로 확장합니다. 최종 단계는 모델 성능을 더욱 향상시키는 어닐링(annealing)을 포함합니다. 아래에서 이러한 단계를 더 자세히 살펴보겠습니다.

4.2.1 **사전 학습(Pre-training) I: 표준(초기) 사전 학습(Standard (Initial) Pre-training)**

학습 설정에서 그들은 4백만 개의 토큰으로 구성된 배치(batch)로 시작했으며, 각 배치에는 4096의 시퀀스 길이(sequence length)가 있었습니다. 이는 4백만이라는 숫자가 가장 가까운 자릿수로 반올림되었다고 가정할 때 약 1024 토큰의 배치 크기(batch size)를 의미합니다. 처음 2억 5천 2백만 개의 토큰을 처리한 후, 그들은 시퀀스 길이를 8192로 두 배 늘렸습니다. 학습 과정이 더 진행되어 2.87조 개의 토큰을 처리한 후, 그들은 배치 크기를 다시 두 배 늘렸습니다. 또한, 연구원들은 학습 내내 데이터 혼합(data mix)을 일정하게 유지하지 않았습니다. 대신, 모델 학습 및 성능을 최적화하기 위해 학습 과정에서 사용되는 데이터 혼합을 조정했습니다. 데이터 처리의 이러한 동적 접근 방식(dynamic approach)은 모델이 다양한 유형의 데이터에 걸쳐 일반화(generalize)하는 능력을 향상시키는 데 도움이 되었을 것입니다. 특정 학습 단계에서 모델이 특정 종류의 데이터에 더 집중하도록 유도함으로써, 학습 효율성을 극대화할 수 있습니다.

4.2.2 **사전 학습(Pre-training) II: 문맥 확장(Context Lengthening)을 위한 연속 사전 학습(Continued Pre-training)**

문맥 창(context window)을 한 번에 늘린 다른 모델들과 비교하여, Llama 3.1의 문맥 확장(context lengthening)은 더 점진적인 접근 방식이었습니다. 여기서 연구원들은 8,000개에서 128,000개 토큰까지 6개의 개별 단계를 통해 문맥 길이(context length)를 늘렸습니다. 이러한 단계별 증가(stepwise increment)는 모델이 더 큰 문맥에 더 원활하게 적응할 수 있도록 했을 것입니다. 이는 급격한 문맥 길이 변화가 가져올 수 있는 학습 불안정성을 줄이는 데 효과적입니다. 이 과정에 사용된 학습 세트(training set)는 전체 데이터셋 크기의 약 5%에 해당하는 8,000억 개의 토큰을 포함했습니다.

4.2.3 **사전 학습(Pre-training) III: 고품질 데이터에 대한 어닐링(Annealing)**

세 번째 사전 학습 단계에서는 연구원들이 작지만 고품질의 혼합 데이터로 모델을 학습시켰는데, 이는 벤치마크 데이터셋(benchmark dataset)의 성능을 향상시키는 데 도움이 된다는 것을 발견했습니다. 예를 들어, GSM8K 및 MATH 학습 세트(training set)에 대한 어닐링(annealing)은 해당 GSM8K 및 MATH 검증 세트(validation set)에서 상당한 성능 향상을 제공했습니다. 논문의 3.1.3 섹션에서 연구원들은 어닐링(annealing) 데이터셋 크기가 400억 개의 토큰(전체 데이터셋 크기의 0.02%)이라고 밝혔습니다. 이 40B 어닐링 데이터셋은 데이터 품질을 평가하는 데 사용되었습니다. 3.4.3 섹션에서는 실제 어닐링이 4천만 개의 토큰(어닐링 데이터의 0.1%)에서만 수행되었다고 명시합니다. 이처럼 마지막 단계에서 특정 고품질 데이터에 대한 집중적인 학습은 모델의 특정 능력, 예를 들어 수학적 추론이나 코딩 능력과 같은 것을 정교하게 다듬는 데 매우 효과적입니다.

Llama 3.1 사전 학습(pre-training)을 위한 기술 요약.

4.3 **Llama 3.1 사후 학습(Post-training)**

Meta AI 팀은 사후 학습(post-training) 과정에서 지도 미세 조정(SFT), 거부 샘플링(rejection sampling), 직접 선호 최적화(DPO)를 포함하는 비교적 간단한 방법을 사용했습니다. 그들은 PPO를 사용한 RLHF와 같은 강화 학습 알고리즘이 이러한 기술에 비해 안정성이 떨어지고 확장하기 더 어렵다는 것을 관찰했습니다. SFT 및 DPO 단계가 인간이 생성한 데이터와 합성 데이터(synthetic data)를 모두 통합하여 여러 라운드에 걸쳐 반복적으로 수행되었다는 점은 주목할 가치가 있습니다. 추가 세부 사항을 설명하기 전에, 그들의 워크플로우는 아래 그림에 설명되어 있습니다. Llama 3.1은 복잡한 강화 학습보다는 DPO의 간결함과 효율성을 선호하는 최신 트렌드를 따릅니다.

Llama 3.1 논문에서 사후 학습 절차를 설명하는 주석이 달린 그림

DPO를 사용했음에도 불구하고, RLHF에서 하는 것처럼 보상 모델(reward model)도 개발했다는 점에 유의하십시오. 처음에는 사전 학습 단계의 체크포인트(checkpoint)를 사용하여 인간이 주석을 단 데이터로 보상 모델을 학습시켰습니다. 이 보상 모델은 그 후 거부 샘플링(rejection sampling) 과정에 사용되어 추가 학습을 위한 적절한 프롬프트를 선택하는 데 도움이 되었습니다. 각 학습 라운드에서 그들은 보상 모델뿐만 아니라 SFT 및 DPO 모델에도 모델 평균화 기법(model averaging technique)을 적용했습니다. 이 평균화는 최근 모델과 이전 모델의 매개변수(parameter)를 병합하여 시간이 지남에 따라 성능을 안정화(및 향상)시키는 것을 포함했습니다. 모델 평균화의 기술적 세부 사항에 관심 있는 분들을 위해, 저는 이전 글 "[Model Merging, Mixtures of Experts, and Towards Smaller LLMs](https://www.aheadofai.com/p/model-merging-mixtures-of-experts-and-towards-smaller-llms)"의 "모델 병합 및 가중치 평균 이해(Understanding Model Merging and Weight Averaging)" 섹션에서 이 주제를 논의했습니다.

요약하자면, 핵심적으로는 비교적 표준적인 SFT + DPO 단계입니다. 그러나 이 단계는 여러 라운드에 걸쳐 반복됩니다. 그런 다음, 그들은 거부 샘플링(rejection sampling)을 위해 보상 모델(Qwen 2 및 AFM과 유사)을 추가했습니다. 그들은 또한 Gemma처럼 모델 평균화(model averaging)를 사용했지만, 이는 보상 모델뿐만 아니라 관련된 모든 모델에 적용되었습니다. 이러한 반복적인 접근 방식과 모델 평균화는 성능의 점진적인 향상과 안정성을 보장합니다.

Llama 3.1 사후 학습(post-training)을 위한 기술 요약.

4.4 **결론**

Llama 3 모델은 이전 Llama 2 모델과 상당히 표준적이고 유사하지만, 몇 가지 흥미로운 접근 방식을 가지고 있습니다. 특히, 15조 개의 토큰으로 구성된 대규모 학습 세트(training set)는 Llama 3를 다른 모델들과 구별합니다. 이는 데이터 스케일링의 중요성을 다시 한번 확인시켜 줍니다. 흥미롭게도 Apple의 AFM 모델처럼 Llama 3도 3단계 사전 학습(pre-training) 프로세스를 구현했습니다. 다른 최신 대규모 언어 모델과 달리 Llama 3는 지식 증류(knowledge distillation) 기술을 사용하지 않고, 대신 더 간단한 모델 개발 경로를 선택했습니다. 이는 충분한 양과 품질의 데이터가 있다면, 복잡한 증류 과정 없이도 강력한 모델을 만들 수 있음을 시사합니다. 사후 학습(post-training)의 경우, 모델은 다른 모델에서 인기를 끌었던 더 복잡한 강화 학습 전략 대신 직접 선호 최적화(DPO)를 활용했습니다. 전반적으로, 이러한 선택은 더 간단하지만 입증된 방법을 통해 LLM 성능을 개선하는 데 중점을 둔다는 점에서 흥미롭습니다. Llama 3.1은 오픈 소스 LLM의 지속적인 발전을 이끌며, 실용적인 성능과 접근성을 동시에 제공하는 모범 사례로 평가됩니다.

---

5.  **주요 시사점**

이 글에서 논의된 Alibaba의 Qwen 2, Apple의 파운데이션 모델(AFM), Google의 Gemma 2, Meta의 Llama 3 이 네 가지 모델에서 무엇을 배울 수 있을까요?

네 가지 모델 모두 사전 학습(pre-training) 및 사후 학습(post-training)에 다소 다른 접근 방식을 취합니다. 물론 방법론이 겹치기는 하지만, 완전히 동일한 학습 파이프라인(training pipeline)은 없습니다. 이는 LLM 개발이 여전히 활발한 연구 분야이며, 단일한 "정답"보다는 다양한 최적화 경로가 존재함을 보여줍니다.

사전 학습의 경우, 모든 방법이 다단계 사전 학습 파이프라인을 사용한다는 공통된 특징이 있는 것으로 보입니다. 여기서 일반적인 핵심 사전 학습에 이어 문맥 확장(context lengthening)과 때로는 고품질 어닐링(annealing) 단계가 뒤따릅니다. 이는 모델이 일반적인 언어 이해에서 시작하여 특정 능력과 긴 문맥 처리 능력을 점진적으로 습득하도록 유도하는 효과적인 방법입니다. 아래 그림은 사전 학습에 사용된 다양한 방법을 다시 한눈에 보여줍니다.

사전 학습(pre-training)에 사용된 기술 개요

사후 학습(post-training)에 있어서도 파이프라인 중 정확히 동일한 것은 없었습니다. 거부 샘플링(rejection sampling)은 이제 사후 학습 과정에서 흔한 필수 요소가 된 것 같습니다. 그러나 DPO 또는 RLHF에 관해서는 아직 합의나 선호도(말장난 아님)가 없습니다. DPO는 구현의 용이성 때문에 인기를 얻고 있지만, PPO와 같은 RLHF 방법론은 여전히 강력한 대안으로 남아있으며 특정 시나리오에서는 더 나은 성능을 제공할 수 있습니다.

사후 학습(post-training)에 사용된 기술 개요

따라서, 고성능 LLM을 개발하는 데는 단 하나의 정답이 아니라 여러 가지 경로가 있습니다. 마지막으로, 네 가지 모델은 비슷한 수준의 성능을 보입니다. 안타깝게도 이 모델들 중 일부는 LMSYS 및 AlpacaEval 리더보드(leaderboard)에 포함되지 않아, MMLU와 같은 객관식 벤치마크(multiple-choice benchmark) 점수를 제외하고는 아직 직접적인 비교가 어렵습니다. 향후에는 다양한 벤치마크와 인간 평가(human evaluation)를 통해 모델의 실제 활용성을 더 정확하게 측정할 필요가 있습니다.

**새로운 동향과 미래 전망**

최근 LLM 개발에서는 데이터 품질과 효율적인 학습 전략 외에도 몇 가지 중요한 동향이 나타나고 있습니다.

*   **멀티모달리티(Multimodality)의 부상**: 텍스트 외에 이미지, 오디오, 비디오 등 다양한 형태의 데이터를 이해하고 생성하는 멀티모달 LLM이 빠르게 발전하고 있습니다. 이는 LLM의 응용 범위를 크게 확장할 것입니다.
*   **에이전트 능력(Agentic Capabilities) 강화**: LLM이 단순한 텍스트 생성기를 넘어, 복잡한 작업을 계획하고 실행하며 외부 도구와 상호작용하는 에이전트로서의 능력을 갖추는 방향으로 발전하고 있습니다. 이는 LLM이 실제 세상의 문제를 해결하는 데 더 큰 역할을 할 수 있도록 합니다.
*   **소형 LLM의 최적화**: 온디바이스(on-device) 또는 엣지 컴퓨팅(edge computing) 환경을 위한 소형 LLM의 효율성 최적화는 계속해서 중요한 연구 분야입니다. 지식 증류, 모델 가지치기(pruning), 양자화(quantization) 등의 기술이 더욱 발전할 것입니다.

이러한 동향들은 LLM이 단순히 언어를 이해하고 생성하는 것을 넘어, 우리의 일상과 산업 전반에 걸쳐 더욱 깊이 통합될 것임을 시사합니다. 앞으로의 LLM 연구는 기술적 혁신뿐만 아니라 윤리적 고려사항과 사회적 영향까지 아우르는 포괄적인 접근이 필요할 것입니다.

**Ahead of AI 지원**

이 잡지는 개인적인 열정 프로젝트입니다. 저를 지원하고 싶으신 분들은 제 "[Build a Large Language Model (From Scratch)](https://www.amazon.com/Build-Large-Language-Model-Scratch/dp/1032601700)" 책을 구매해 주시기를 고려해 주십시오. (이 책은 LLM이 어떻게 작동하는지 다른 곳에서는 찾을 수 없는 수준의 세부 사항으로 설명하므로, 이 책에서 많은 것을 얻으실 것이라고 확신합니다.)

[Build a Large Language Model (From Scratch)는 지금 Amazon에서 구매 가능합니다.](https://www.amazon.com/Build-Large-Language-Model-Scratch/dp/1032601700)

책을 읽으시고 잠시 시간을 내주실 수 있다면, [짧은 리뷰를 남겨주시면](https://www.amazon.com/Build-Large-Language-Model-Scratch/dp/1032601700#customerReviews) 정말 감사하겠습니다. 이는 저희 저자들에게 큰 도움이 됩니다! 또는, 최근에 이 잡지를 직접 지원하기 위해 Substack에서 유료 구독 옵션을 활성화했습니다.

Ahead of AI는 독자 지원 출판물입니다. 새로운 게시물을 받고 제 작업을 지원하려면 무료 또는 유료 구독자가 되는 것을 고려해 주십시오. [구독](https://www.aheadofai.com/subscribe)