거대 언어 모델(LLM)의 발전 경로는 최초의 GPT 계열 모델들에서부터 현재의 고도화된 공개 가중치(open-weight) LLM에 이르기까지 상당한 진보를 거듭해왔습니다. 초기에는 모델의 선행 학습(pre-training) 단계에만 초점이 맞춰졌으나, 점차 선행 학습과 후속 학습(post-training) 단계를 모두 아우르는 형태로 그 범위가 넓어졌습니다. 후속 학습은 주로 지도 방식의 지시문 미세 조정(supervised instruction fine-tuning)과 더불어 인간 의도와의 부합화(alignment) 과정을 수반하며, 이는 ChatGPT의 등장으로 대중화되었습니다. ChatGPT가 처음 공개된 이후 모델 훈련 방법론은 꾸준히 진화해왔습니다. 본 포스팅에서는 특히 최근 몇 달간 이루어진 선행 학습 및 후속 학습 방법론의 최신 동향을 면밀히 분석하고자 합니다.

이 글에서 다루는 새로운 선행 학습 및 후속 학습 기법에 초점을 맞춘 LLM 개발 및 훈련 파이프라인 개요. 매달 수백 편의 LLM 관련 연구 논문들이 새로운 기술과 접근법을 제시하지만, 가장 효과적인 접근 방식을 파악하는 한 가지 방법은 최근 등장한 최고 성능 모델(state-of-the-art model)들의 선행 학습 및 후속 학습 과정을 면밀히 검토하는 것입니다. 다행히도 지난 몇 달 동안 네 가지 주요 신규 LLM이 비교적 상세한 기술 보고서(technical report)와 함께 공개되었습니다. 본 글에서는 다음 모델들의 선행 학습 및 후속 학습 파이프라인에 집중합니다:

*   Alibaba의 Qwen 2
*   Apple Intelligence Foundation Language Models
*   Google의 Gemma 2
*   Meta AI의 Llama 3.1

이 모델들은 arXiv.org에 게시된 각 기술 논문의 발행 시점 순서대로 제시되었으며, 이는 공교롭게도 알파벳 순서와도 일치합니다. 이 글은 제가 개인적인 여가 시간과 주말을 활용하여 공들여 만든 프로젝트입니다. 만약 이 글이 유익하다고 생각하시고 저의 노력을 지지하고 싶으시다면, 저의 저서를 구매하시거나 동료들에게 추천해 주시기를 정중히 요청드립니다. Amazon에 남겨주시는 서평 또한 큰 힘이 될 것입니다!

*   Build a Large Language Model (from Scratch)
*   Machine Learning Q and AI
*   Machine Learning with PyTorch and Scikit-Learn

『Build a Large Language Model (from Scratch)』는 파이토치(PyTorch)를 사용하여 LLM을 밑바닥부터 직접 구현하는 과정에 집중적으로 초점을 맞춘 서적으로, 선행 학습부터 후속 학습까지 모든 단계를 상세히 다룹니다. 이는 LLM의 작동 원리를 깊이 있게 이해하는 데 가장 효과적인 방법 중 하나입니다. 『Machine Learning Q and AI』는 이미 기본적인 개념에 익숙한 독자들을 위한 훌륭한 참고 자료입니다. 이 책은 심층 신경망(deep neural network), 비전 트랜스포머(vision transformer), 다중 GPU 훈련 방식(multi-GPU training paradigm), 그리고 LLM과 같은 중급 및 고급 주제들을 포괄합니다. 『Machine Learning with PyTorch and Scikit-Learn』은 기계 학습, 딥러닝, 그리고 인공지능 분야 전반에 걸친 포괄적인 안내서로, 이론과 실제 코드의 균형 잡힌 조합을 제공합니다. 이 분야에 처음 발을 들이는 모든 이들에게 이상적인 출발점을 제시합니다.

---

1.  **Alibaba의 Qwen 2**

다른 주요 거대 언어 모델들과 경쟁할 수 있는 매우 강력한 성능을 지닌 Qwen 2 제품군부터 분석을 시작합니다. 흥미롭게도 이 모델은 메타 AI, 마이크로소프트, 구글의 공개 가중치(open-weight) 모델들에 비해 상대적으로 덜 알려져 있습니다.

1.1 **Qwen 2 개요**

Qwen 2의 기술 보고서(Technical Report)에서 다루는 선행 학습 및 후속 학습 기법들을 살펴보기 전에, 몇 가지 핵심 사양을 간략히 요약해 보겠습니다. Qwen 2 계열 모델은 다섯 가지 형태로 출시되었습니다. 5천만, 1억 5천만, 7억, 720억 개의 파라미터(parameter)를 갖는 네 종류의 일반형(dense) 거대 언어 모델이 포함됩니다. 이와 함께 570억 개의 파라미터를 가진 전문가 혼합(Mixture-of-Experts) 모델도 존재하며, 해당 모델에서는 140억 개의 파라미터가 동시에 동작합니다. (이번 글에서는 아키텍처 세부 사항이 주된 관심사가 아니므로, 전문가 혼합(Mixture-of-Experts) 모델에 대해 깊이 있게 다루지는 않겠습니다. 다만 간단히 설명하자면, 이는 Mistral AI의 Mixtral과 유사하지만, 더 많은 활성 전문가(expert)를 포함하고 있습니다. 높은 수준의 개요는 제 "Model Merging, Mixtures of Experts, and Towards Smaller LLMs" 글의 Mixtral 아키텍처 섹션을 참조하십시오.)

Qwen 2 LLM의 두드러진 특징 중 하나는 30개 이상의 언어를 아우르는 탁월한 다중 언어 처리 능력(multilingual capability)을 자랑한다는 것입니다. 또한, 총 151,642개의 토큰 집합(token vocabulary)을 보유하고 있어 놀라울 정도로 규모가 큽니다(참고로 Llama 2는 32k 어휘를, Llama 3.1은 128k 토큰 어휘를 사용합니다). 경험적으로, 어휘 집합의 규모가 두 배 증가하면 입력 토큰의 개수가 절반으로 줄어들어, 동일한 입력 공간에 더 많은 텍스트를 담을 수 있게 됩니다. 이는 특히 표준 영어 어휘 외의 단어를 처리해야 하는 다국어 데이터나 코딩 작업에 큰 이점을 제공합니다.

아래는 이후에 다룰 다른 LLM들과의 간략한 MMLU 벤치마크(benchmark) 비교입니다. (MMLU는 객관식 평가(multiple-choice benchmark)이므로 한계가 있지만, 여전히 LLM 성능을 측정하는 가장 널리 사용되는 방법 중 하나입니다.)

최신 공개 가중치(open-weight) 모델의 MMLU 벤치마크 점수(높을수록 우수). 이 그래프의 수치는 각 모델의 공식 연구 논문에서 인용되었습니다. (MMLU에 대해 처음이시라면, 최근 강연 46분 05초에서 간략하게 설명했습니다.)

1.2 **Qwen 2 선행 학습(Pre-training)**

Qwen 2 개발팀은 1억 5천만, 7억, 720억 파라미터 모델들을 7조 개의 훈련 토큰(training token)으로 훈련되었으며, 이는 적절한 수준의 규모입니다. 비교하자면, Llama 2 모델은 2조 개의 토큰으로, Llama 3.1 모델은 15조 개의 토큰으로 훈련되었습니다. 흥미롭게도 5억 파라미터 모델은 12조 개의 토큰으로 훈련되었습니다. 그러나 연구원들은 훈련 과정에서 어떠한 성능 개선도 확인하지 못했고, 추가적인 컴퓨팅 자원 투입이 정당화되지 않았기 때문에 다른 모델들을 더 큰 12조 개의 토큰 데이터셋으로 훈련시키지 않았습니다.

주요 초점은 낮은 품질의 정보를 걸러내기 위한 데이터 정제 과정(data filtering pipeline)을 고도화하고, 자료의 다채로움을 증진시키기 위한 데이터 조합(data mixing)을 강화하는 데 있었습니다. 이 주제는 이후 다른 모델들을 검토할 때 다시 논의될 것입니다. 놀랍게도 그들은 추가적인 선행 학습 데이터를 생성하기 위해 Qwen 모델(자세한 내용은 명시되지 않았지만, 이전 세대 Qwen 모델을 의미한다고 가정합니다)도 활용했습니다. 또한 선행 학습에는 "인컨텍스트 학습(in-context learning) 및 지시문 추종 능력(instruction-following ability)을 향상시키기 위한 다중 작업 지시문 데이터(multi-task instruction data)"가 포함되었습니다.

더 나아가, 그들은 일반 선행 학습에 이어 장문맥 훈련(long-context training)이라는 두 단계로 훈련을 진행했습니다. 후자는 "고품질의 긴 데이터"를 사용하여 선행 학습의 마지막 단계에서 문맥 길이(context length)를 4,096개에서 32,768개 토큰으로 확장했습니다.

Qwen 2 선행 학습(pre-training)을 위한 기술 요약. "연속 선행 학습(Continued pre-training)"은 연구원들이 일반 선행 학습으로 시작하여 장문맥 연속 선행 학습을 이어서 수행한 2단계 선행 학습을 의미합니다. (안타깝게도 기술 보고서의 또 다른 특징은 데이터셋에 대한 세부 정보가 부족하다는 것입니다. 따라서 제 글이 매우 상세하지 않다면, 이는 공개적으로 이용 가능한 정보의 부족 때문입니다.)

1.3 **Qwen 2 후속 학습(Post-training)**

Qwen 2 개발진은 널리 사용되는 두 단계 후속 학습(post-training) 접근 방식을 채택했으며, 이는 50만 개의 사례에 대해 두 번의 에포크(epoch) 동안 수행된 지도 기반 지시문 미세 조정(SFT)으로 시작되었습니다. 이 단계는 미리 정의된 시나리오에서 모델의 응답 정확도를 개선하는 것을 목표로 했습니다.

일반적인 LLM 개발 흐름. SFT 이후, 연구팀은 직접 선호 최적화(DPO) 기법을 활용하여 거대 언어 모델을 인간의 선호 경향에 맞게 조정(align)했습니다. (흥미롭게도 그들의 용어로는 인간 피드백 기반 강화 학습(RLHF)이라고 불립니다.) 몇 주 전 "LLM 선행 학습 및 보상 모델 평가 팁(Tips for LLM Pretraining and Evaluating Reward Models)" 글에서 논의했듯이, SFT+DPO 접근 방식은 PPO를 사용한 RLHF와 같은 다른 방법들에 비해 사용 편의성 때문에 현재 가장 인기 있는 선호 조정 전략인 것 같습니다. (DPO가 어떻게 작동하는지 배우고 싶다면, 제가 최근에 여기에서 처음부터 구현했습니다.)

정렬(alignment) 단계 자체도 두 단계로 진행되었습니다. 첫째, 기존 데이터셋에 DPO를 적용했습니다(오프라인 단계). 둘째, 보상 모델(reward model)을 사용하여 선호 쌍을 형성했습니다(온라인). 여기서 모델은 훈련 중에 여러 응답을 생성하고, 보상 모델은 "실시간"(즉, 훈련 중)으로 최적화 단계에 가장 선호되는 응답을 선택합니다. 이는 종종 "거부 샘플링(rejection sampling)"이라고도 불립니다.

데이터셋 구축을 위해 그들은 기존 코퍼스(corpus)에 수동 레이블링(human labeling) 작업을 보완하여 SFT에 필요한 목표 응답을 정하고, DPO에 필수적인 선호 및 비선호 응답을 구분했습니다. 연구원들은 또한 인공적으로 주석이 달린 데이터(artificially annotated data)를 합성했습니다. 더욱이, 개발팀은 LLM을 활용하여 "고품질 문학 데이터(literary data)"에 특별히 맞춰진 지시문-응답 쌍(instruction-response pair)을 생성함으로써, 훈련에 필요한 고품질 Q&A 쌍(Q&A pair)을 구축했습니다.

Qwen 2 후속 학습(post-training)을 위한 기술 요약.

1.4 **결론**

Qwen 2는 이전 세대 Qwen과 유사하게 비교적 유능한 모델입니다. 2023년 12월 NeurIPS LLM 효율성 챌린지(challenge)에 참석했을 때, 우승한 접근 방식의 대부분이 Qwen 모델을 포함하고 있었던 것이 기억납니다. Qwen 2의 훈련 파이프라인(training pipeline)과 관련하여 눈에 띄는 점은 선행 학습(pre-training)과 후속 학습(post-training) 모두에 합성 데이터(synthetic data)가 활용되었다는 것입니다. 또한, (가능한 한 많은 데이터를 수집하기보다는) 데이터셋 정제에 중점을 두는 것이 LLM 훈련의 주목할 만한 추세 중 하나입니다. 여기서 저는 "더 많은 것이 더 좋다"고 말하겠지만, 이는 특정 품질 기준을 충족할 때만 해당됩니다.

**직접 선호 최적화(Direct Preference Optimization)를 통한 LLM 정렬: 심층 분석**

직접 선호 최적화(DPO)는 거대 언어 모델을 사용자의 선호도에 보다 밀접하게 부합하도록 정렬(align)하는 핵심 방법론 중 하나로 자리매김했으며, 본 글에서도 이 기법을 자주 접하게 될 것입니다. DPO의 작동 원리를 깊이 있게 이해하고 싶으시다면, 제가 다음 링크에서 처음부터 직접 코드를 구현한 내용을 참조하실 수 있습니다: [LLM 정렬을 위한 직접 선호 최적화(DPO) (처음부터)](https://www.aheadofai.com/p/direct-preference-optimization-dpo-for-llm-alignment-from-scratch).

LLM 정렬을 위한 DPO 개요. DPO는 복잡한 강화 학습(RL) 파이프라인 없이도 인간의 선호도를 모델에 효과적으로 주입할 수 있다는 점에서 큰 이점을 가집니다. 이는 모델의 응답이 사용자의 기대치와 더욱 일치하도록 유도하며, 특히 대화형 AI 시스템에서 사용자 경험을 향상시키는 데 필수적인 역할을 합니다.

Ahead of AI는 독자 지원 출판물입니다. 새로운 게시물을 받고 제 작업을 지원하려면 무료 또는 유료 구독자가 되는 것을 고려해 주십시오. [구독](https://www.aheadofai.com/subscribe)

---

2.  **Apple의 Apple Intelligence Foundation Language Models (AFM)**

arXiv.org에서 애플의 모델 훈련 과정을 설명하는 또 다른 기술 논문(technical paper)을 접하게 되어 무척 기뻤습니다. 이는 예상치 못한 소식이었지만, 분명히 반가운 놀라움이었습니다!

2.1 **AFM 개요**

[Apple Intelligence Foundation Language Models 논문](https://arxiv.org/abs/2407.03052)에서 연구팀은 애플 기기 생태계 내 "Apple Intelligence" 환경에서 활용될 목적으로 개발된 두 가지 핵심 모델의 발전 과정을 서술합니다. 간결성을 위해 이 섹션 전체에서 이 모델들은 "Apple Foundation Models"의 약어인 AFM으로 지칭될 것입니다. 구체적으로, 이 논문은 AFM의 두 가지 버전을 설명합니다: 휴대폰, 태블릿, 혹은 휴대용 컴퓨터에 탑재될 30억 개의 파라미터를 지닌 장치 내(on-device) 모델과, 그 크기가 구체적으로 명시되지 않은 보다 강력한 서버용 모델을 포함합니다. 이 모델들은 채팅, 수학 및 코딩 작업을 위해 개발되었지만, 논문에서는 코딩 관련 훈련 및 기능에 대해서는 논의하지 않습니다. Qwen 2와 마찬가지로 AFM은 밀집 LLM이며 전문가 혼합(mixture-of-experts) 접근 방식을 사용하지 않습니다. 이는 모델의 단순성과 직접적인 성능 최적화에 중점을 두었음을 시사합니다.

2.2 **AFM 선행 학습(Pre-training)**

연구원들에게 두 가지 큰 칭찬을 하고 싶습니다. 첫째, 공개적으로 활용 가능한 데이터와 출판사로부터 라이선스(license)를 받은 자료를 사용하는 것 외에도, 웹사이트의 robots.txt 규약을 준수하며 데이터 수집(crawling)을 자제했습니다. 이는 데이터 윤리 측면에서 모범적인 사례로 평가됩니다. 둘째, 벤치마크 평가 자료(benchmark data)로부터 데이터 오염(decontamination) 작업을 실행했다고 밝혔습니다. Qwen 2 논문의 주요 시사점 중 하나를 강조하기 위해, 연구원들은 데이터의 양보다 질이 훨씬 더 중요하다고 언급했습니다. (장치 모델의 어휘 집합 크기는 49k 토큰, 서버 모델은 100k 토큰으로, 150k 토큰 어휘를 사용한 Qwen 2 모델보다 어휘 집합 크기가 현저히 작았습니다. 이는 모델의 효율성과 특정 도메인에 대한 최적화를 반영할 수 있습니다.)

흥미롭게도 선행 학습은 2단계가 아닌 3단계로 진행되었습니다! 이는 모델의 복잡성과 정교함을 보여주는 대목입니다.

*   핵심(일반) 선행 학습(pre-training)
*   웹 크롤링(web-crawl) (저품질) 데이터의 가중치를 낮추고, 수학 및 코드 데이터의 가중치를 높인 연속 선행 학습(continued pre-training)
*   더 긴 시퀀스 데이터와 합성 데이터(synthetic data)를 사용한 문맥 확장(context-lengthening)

AFM 모델이 거친 3단계 선행 학습(pre-training) 과정 개요. 이 3단계를 좀 더 자세히 살펴보겠습니다.

2.2.1 **선행 학습(Pre-training) I: 핵심 선행 학습(Core Pre-training)**

핵심 선행 학습은 애플의 선행 학습 파이프라인에서 첫 번째 단계를 의미합니다. 이는 일반적인 선행 학습과 유사하며, AFM-서버 모델은 6.3조 개의 토큰, 4096의 배치 크기(batch size), 4096 토큰의 시퀀스 길이(sequence length)로 훈련되었습니다. 이는 7조 개의 토큰으로 훈련된 Qwen 2 모델과 매우 유사한 규모입니다.

그러나 AFM 장치 내(on-device) 모델의 경우는 더욱 흥미로운데, 이는 더 큰 64억 파라미터 모델에서 증류 및 가지치기(distilled and pruned)되었습니다(이전 단락에서 설명된 AFM-서버 모델처럼 처음부터 훈련됨). 증류 과정에 대한 자세한 내용은 "목표 레이블(target label)을 실제 레이블과 교사 모델(teacher model)의 상위 1개 예측(교사 레이블에 0.9 가중치 할당)의 볼록 조합(convex combination)으로 대체하여 증류 손실(distillation loss)이 사용된다"는 것 외에는 많지 않습니다. 지식 전수(knowledge distillation)가 LLM 선행 학습에 점점 더 널리 퍼지고 유용해지고 있다고 생각합니다(Gemma-2도 이를 사용합니다). 언젠가 더 자세히 다룰 계획입니다. 지금은 이 과정이 높은 수준에서 어떻게 작동하는지에 대한 간략한 개요입니다.

여기서는 AFM-디바이스 30억 파라미터 모델과 같은 소규모 모델이 기존 훈련 토큰 및 더 큰 교사 모델(여기서는 64억 파라미터 모델)의 산출물을 통해 훈련되는 지식 전수(knowledge distillation) 방식에 대한 개요입니다. a)의 교차 엔트로피 손실(cross entropy loss)은 LLM 선행 학습에 사용되는 일반적인 훈련 손실입니다(일반 선행 학습 단계가 어떻게 구현되는지에 대한 자세한 내용은 제 "[Build a Large Language Model from Scratch](https://www.amazon.com/Build-Large-Language-Model-Scratch/dp/1032601700)" 책의 5장을 참조하십시오).

위에 설명된 지식 전수(knowledge distillation)는 여전히 원래 데이터셋으로 훈련하는 것을 포함합니다. 그러나 데이터셋의 훈련 토큰 외에도 훈련될 모델(학생 모델(student model)이라고 함)은 더 큰 (교사) 모델로부터 정보를 받는데, 이는 지식 전수 없이 훈련하는 것보다 더 풍부한 신호를 제공합니다. 단점은 다음과 같습니다: 1) 더 큰 교사 모델을 먼저 훈련시켜야 하고, 2) 더 큰 교사 모델을 사용하여 모든 훈련 토큰에 대한 예측을 계산해야 합니다. 이러한 예측은 미리 계산할 수 있거나(상당한 저장 공간 필요) 훈련 중에 계산할 수 있습니다(훈련 프로세스 속도 저하 가능).

2.2.2 **선행 학습(Pre-training) II: 연속 선행 학습(Continued Pre-training)**

연속 선행 학습 단계는 1조 개의 토큰으로 구성된 데이터셋(핵심 선행 학습 세트는 5배 더 컸음)에서 문맥 길이(context length)를 4,096개에서 8,192개 토큰으로 늘리는 작은 문맥 확장(context lengthening) 단계를 포함합니다. 그러나 주요 초점은 수학 및 코드에 중점을 둔 고품질 데이터 조합(data mix)으로 훈련하는 것입니다. 흥미롭게도 연구원들은 이 맥락에서 증류 손실(distillation loss)이 유익하지 않다는 것을 발견했습니다. 이는 특정 데이터 유형과 훈련 단계에서는 지식 전수 방식이 항상 최적의 결과를 내지 못할 수 있음을 시사합니다.

2.2.3 **선행 학습(Pre-training) III: 문맥 확장(Context Lengthening)**

세 번째 선행 학습 단계는 1,000억 개의 토큰(두 번째 단계에서 사용된 토큰의 10%)만 포함하지만, 문맥 길이(context length)를 32,768개 토큰으로 더 크게 확장하는 것을 나타냅니다. 이를 달성하기 위해 연구원들은 합성 장문맥 Q&A 데이터(synthetic long-context Q&A data)로 데이터셋을 증강했습니다. 이 단계는 모델이 복잡하고 긴 질의응답 시나리오를 효과적으로 처리할 수 있도록 하는 데 중요합니다.

AFM 선행 학습(pre-training)을 위한 기술 요약.

2.3 **AFM 후속 학습(Post-training)**

애플은 선행 학습(pre-training)과 마찬가지로 후속 학습(post-training) 과정에도 유사하게 포괄적인 접근 방식을 취한 것으로 보입니다. 그들은 인간이 주석을 단 데이터와 합성 데이터(synthetic data)를 모두 활용했으며, 데이터 품질이 양보다 우선시된다는 점을 강조했습니다. 흥미롭게도 그들은 미리 정해진 데이터 비율에 의존하지 않고, 여러 실험을 통해 데이터 조합(data mixture)을 미세 조정하여 최적의 균형을 달성했습니다. 이는 데이터 엔지니어링의 중요성을 보여줍니다.

후속 학습 단계는 지도 지시문 미세 조정(supervised instruction fine-tuning)과 여러 차례의 인간 피드백 기반 강화 학습(RLHF)으로 구성된 2단계 프로세스를 포함했습니다. 이 과정에서 특히 주목할 만한 점은 애플이 RLHF 단계에 두 가지 새로운 알고리즘을 도입했다는 것입니다:

*   교사 위원회(Teacher Committee)를 이용한 거부 샘플링 미세 조정(iTeC)
*   미러 디센트 정책 최적화(Mirror Descent Policy Optimization)를 이용한 RLHF

이 글의 길이를 고려하여 이 방법들의 기술적 세부 사항은 다루지 않겠지만, 간략한 개요는 다음과 같습니다:

iTeC 알고리즘은 거부 샘플링(rejection sampling)과 여러 선호 조정 기법(preference tuning technique)을 결합합니다. 구체적으로 SFT, DPO, IPO, 그리고 온라인 강화 학습(online RL)입니다. 애플은 단일 알고리즘에 의존하기보다는 각 접근 방식을 독립적으로 사용하여 모델을 훈련시켰습니다. 그런 다음 이 모델들은 응답을 생성했고, 인간이 선호 레이블(preference label)을 제공하여 이를 평가했습니다. 이 선호 데이터는 RLHF 프레임워크에서 보상 모델(reward model)을 반복적으로 훈련시키는 데 사용되었습니다. 거부 샘플링(rejection sampling) 단계에서는 모델 위원회(committee of models)가 여러 응답을 생성했고, 보상 모델이 가장 좋은 응답을 선택했습니다. 이 위원회 기반 접근 방식은 상당히 복잡하지만, 관련된 모델의 비교적 작은 크기(약 30억 파라미터)를 고려할 때 비교적 실현 가능해야 합니다. Llama 3.1의 70B 또는 405B 파라미터 모델과 같이 훨씬 더 큰 모델로 이러한 위원회를 구현하는 것은 확실히 더 어려울 것입니다.

두 번째 알고리즘인 미러 디센트(Mirror Descent)를 이용한 RLHF는 일반적으로 사용되는 PPO(근접 정책 최적화(Proximal Policy Optimization))보다 더 효과적임이 입증되었기 때문에 선택되었습니다. 이 방법은 정책 업데이트의 안정성을 높이고 수렴 속도를 개선하는 데 기여할 수 있습니다.

AFM 후속 학습(post-training)을 위한 기술 요약.

2.4 **결론**

애플의 선행 학습(pre-training) 및 후속 학습(post-training) 접근 방식은 비교적 포괄적이며, 이는 아마도 위험 부담이 매우 높기 때문일 것입니다(모델이 수백만, 심지어 수십억 대의 기기에 배포됨). 그러나 이 모델들의 작은 특성을 고려할 때, 30억 파라미터 모델은 가장 작은 Llama 3.1 모델 크기의 절반에도 미치지 못하므로, 광범위한 기술 또한 실현 가능해집니다. 주요 특징 중 하나는 RLHF와 DPO 사이의 단순한 선택이 아니라는 것입니다. 대신, 그들은 위원회(committee) 형태로 여러 선호 조정(preference-tuning) 알고리즘을 사용했습니다. 또한 선행 학습의 일부로 Q&A 데이터를 명시적으로 사용했다는 점도 흥미롭습니다. 이는 제가 이전 글인 "[명령어 선행 학습 LLM(Instruction Pretraining LLMs)](https://www.aheadofai.com/p/instruction-pretraining-llms)"에서 논의했던 내용입니다. 전반적으로, 신선하고 즐거운 기술 보고서(technical report)입니다. 특히 장치 내 모델의 경우, 개인 정보 보호 및 지연 시간 측면에서 상당한 이점을 제공하며, 이는 애플의 핵심 가치와도 부합합니다.

---

3.  **Google의 Gemma 2**

구글의 Gemma 모델은 최근 "[Gemma 2: Improving Open Language Models at a Practical Size](https://arxiv.org/abs/2407.13038)"에서 설명되었습니다. 다음 개요 섹션에서 몇 가지 주요 사실을 간략하게 설명한 후 선행 학습(pre-training) 및 후속 학습(post-training) 프로세스에 대해 논의하겠습니다.

3.1 **Gemma 2 개요**

Gemma 2 계열 모델은 20억, 90억, 그리고 270억 개의 파라미터(parameter)를 가지는 세 가지 규모로 제공됩니다. 주요 초점은 훈련 데이터셋의 크기를 반드시 늘릴 필요는 없지만, 비교적 작고 효율적인 LLM을 개발하는 기술을 탐구하는 데 있습니다. 특히 Gemma 2는 256,000개의 토큰을 포함하는 방대한 어휘 집합을 특징으로 합니다. 비교를 위해 Llama 2는 32k 토큰 어휘를 사용하고, Llama 3는 128k 토큰 어휘를 사용합니다. 또한 Gemma 2는 Mistral의 초기 모델과 유사하게 슬라이딩 윈도우 어텐션(sliding window attention) 기법을 활용하여 메모리 자원 소모를 감소시킨 것으로 보입니다. 이는 긴 문맥을 효율적으로 처리하면서도 하드웨어 요구 사항을 낮추는 데 기여합니다. Gemma 2 아키텍처에 대한 자세한 내용은 제 이전 글의 [Gemma 2 섹션](https://www.aheadofai.com/p/model-merging-mixtures-of-experts-and-towards-smaller-llms#%C2%A7gemma-2)을 참조하십시오.

3.2 **Gemma 2 선행 학습(Pre-training)**

Gemma 연구원들은 소규모 모델들조차도 대개는 충분히 훈련되지 않은(undertrained) 상태라고 주장합니다. 그러나 단순히 훈련 데이터셋의 크기를 늘리는 대신, 그들은 데이터 품질 유지에 중점을 두고 애플의 접근 방식과 유사하게 지식 전수(knowledge distillation)와 같은 대체 방법을 통해 개선을 달성합니다. 270억 파라미터 Gemma 2 모델은 처음부터 훈련되었지만, 더 작은 모델들은 이전에 설명된 애플의 접근 방식과 유사하게 지식 전수(knowledge distillation)를 사용하여 훈련되었습니다. 270억 파라미터 모델은 13조 개의 토큰으로, 90억 파라미터 모델은 8조 개의 토큰으로, 20억 파라미터 모델은 2조 개의 토큰으로 훈련되었습니다. 또한, 애플의 접근 방식과 유사하게 Gemma 팀은 성능 향상을 위해 데이터 조합(data mixture)을 최적화했습니다. 이는 소규모 모델에서도 대규모 모델에 필적하는 성능을 달성하기 위한 구글의 전략을 보여줍니다.

Gemma 2 선행 학습(pre-training)을 위한 기술 요약.

3.3 **Gemma 2 후속 학습(Post-training)**

Gemma 모델의 후속 학습(post-training) 과정은 일반적인 지도 미세 조정(SFT) 및 인간 피드백 기반 강화 학습(RLHF) 단계를 포함했습니다. 지시문 데이터는 인간이 생성한 콘텐츠와 합성 생성 콘텐츠가 혼합된 영어 전용 프롬프트 쌍(English-only prompt pair)을 사용했습니다. 특히 흥미롭게도, 응답은 주로 교사 모델(teacher model)에 의해 생성되었으며, SFT 단계에서도 지식 전수(knowledge distillation)가 적용되었습니다. 이는 SFT 과정 자체의 효율성을 높이고, 소규모 모델이 더 큰 모델의 지식을 흡수하도록 돕는 혁신적인 방법입니다.

SFT 이후 그들의 RLHF 접근 방식의 흥미로운 점은 RLHF에 사용되는 보상 모델(reward model)이 정책(목표) 모델보다 10배 더 크다는 것입니다. Gemma가 사용하는 RLHF 알고리즘은 상당히 표준적이지만, 독특한 특징이 있습니다: 그들은 WARM(가중치 평균 보상 모델)(weight-averaged reward models)의 후속작인 WARP라는 방법을 통해 정책 모델들을 평균화합니다. 저는 이전에 "Model Merging, Mixtures of Experts, and Towards Smaller LLMs"라는 제 글에서 이 방법을 자세히 논의했습니다. 이 기술은 모델의 안정성과 일반화 성능을 향상시키는 데 기여할 수 있습니다.

Gemma 2 후속 학습(post-training)을 위한 기술 요약.

3.4 **결론**

Gemma 팀은 애플과 유사하게 선행 학습(pre-training)과 후속 학습(post-training) 모두에서 지식 전수(knowledge distillation)에 정말 집중하는 것으로 보입니다. 이는 효율성과 실용적인 규모의 모델 개발에 대한 구글의 강력한 의지를 반영합니다. 흥미롭게도 그들은 다단계 선행 학습 접근 방식을 사용하지 않았거나, 적어도 논문에서 자세히 설명하지는 않았습니다. 이는 단일 단계의 최적화된 훈련 파이프라인만으로도 충분한 성능을 달성할 수 있음을 시사할 수 있습니다.

다가오는 PyTorch 컨퍼런스(conference)에서 기조 강연(keynote talk)을 하게 되어 기쁩니다. 첫 PyTorch 컨퍼런스가 될 것이며, 커뮤니티를 만나 최신 AI 및 LLM 개발에 대해 이야기할 수 있기를 기대합니다!

---

4.  **Meta AI의 Llama 3.1**

메타의 Llama LLM의 새로운 출시는 항상 큰 이슈입니다. 이번에는 92페이지 분량의 기술 보고서(technical report)인 "[The Llama 3 Herd of Models](https://arxiv.org/abs/2407.13038)"와 함께 출시되었습니다. 마지막으로, 이 섹션에서는 지난달에 발표된 네 번째 대규모 모델 논문을 살펴보겠습니다.

4.1 **Llama 3.1 개요**

메타는 막대한 4천50억 개의 파라미터를 가진 모델을 공개함과 더불어, 기존의 80억 및 700억 파라미터 모델들을 개선하여 MMLU 평가 점수를 소폭 상승시켰습니다.

다양한 모델의 MMLU 벤치마크(benchmark) 성능. Llama 3는 다른 최신 LLM과 마찬가지로 그룹 쿼리 어텐션(group query attention)을 채택했지만, 의외로 메타 AI는 슬라이딩 윈도우 어텐션(sliding window attention)이나 전문가 혼합(Mixture-of-Experts) 방식에는 의존하지 않았습니다. 즉, Llama 3.1은 매우 전통적인 아키텍처를 유지하며, 아키텍처 혁신보다는 선행 학습(pre-training) 및 후속 학습(post-training)에 분명히 초점을 맞추었습니다. 이전 Llama 출시와 마찬가지로 가중치(weight)는 공개적으로 사용 가능합니다. 또한 메타는 Llama 3 라이선스(license)를 업데이트하여 이제 다른 모델을 개선하기 위한 합성 데이터(synthetic data) 생성 또는 지식 전수(knowledge distillation)에 Llama 3를 사용하는 것이 마침내 가능(허용)해졌다고 밝혔습니다. 이는 오픈 소스 생태계에 긍정적인 영향을 미칠 것으로 예상됩니다.

4.2 **Llama 3.1 선행 학습(Pre-training)**

Llama 3는 Llama 2의 1.8조 개 토큰에서 크게 증가한 총 15조 6천억 개의 토큰으로 구성된 데이터셋을 통해 훈련되었습니다. 연구원들은 Llama 3가 최소 8개 언어를 지원한다고 말합니다(Qwen 2는 20개 언어를 처리할 수 있습니다). Llama 3의 흥미로운 측면은 OpenAI의 tiktoken 토크나이저(tokenizer)를 사용하여 개발된 12만 8천 개의 어휘 집합을 사용한다는 것입니다. (토크나이저 성능에 관심 있는 분들을 위해 여기에서 [간단한 벤치마크(benchmark) 비교](https://www.aheadofai.com/p/tokenizer-benchmark-gpt-4-llama-3-qwen-2)를 했습니다.)

선행 학습(pre-training) 데이터 품질 관리 측면에서 Llama 3는 메타 AI의 fastText 및 RoBERTa 기반 분류기(classifier)와 같은 고속 분류기(fast classifier)를 활용하여 경험적 필터링(heuristic-based filtering)과 모델 기반 품질 필터링(model-based quality filtering)을 함께 사용합니다. 이러한 분류기는 훈련 중에 사용되는 데이터 조합(data mix)의 문맥 범주(context category)를 결정하는 데도 도움이 됩니다. 이는 데이터의 품질과 다양성을 동시에 관리하는 정교한 접근 방식입니다.

Llama 3의 선행 학습은 세 단계로 나뉩니다. 첫 번째 단계는 8k 문맥 창(context window)을 가진 15.6조 개의 토큰을 사용하여 표준 초기 선행 학습을 포함합니다. 두 번째 단계는 선행 학습을 계속하지만 문맥 길이(context length)를 128k로 확장합니다. 최종 단계는 모델 성능을 더욱 향상시키는 어닐링(annealing)을 포함합니다. 아래에서 이러한 단계를 더 자세히 살펴보겠습니다.

4.2.1 **선행 학습(Pre-training) I: 표준(초기) 선행 학습(Standard (Initial) Pre-training)**

훈련 설정에서 그들은 4백만 개의 토큰으로 구성된 배치(batch)로 시작했으며, 각 배치에는 4096의 시퀀스 길이(sequence length)가 있었습니다. 이는 4백만이라는 숫자가 가장 가까운 자릿수로 반올림되었다고 가정할 때 약 1024 토큰의 배치 크기(batch size)를 의미합니다. 처음 2억 5천 2백만 개의 토큰을 처리한 후, 그들은 시퀀스 길이를 8192로 두 배 늘렸습니다. 훈련 과정이 더 진행되어 2.87조 개의 토큰을 처리한 후, 그들은 배치 크기를 다시 두 배 늘렸습니다. 또한, 연구원들은 훈련 내내 데이터 조합(data mix)을 일정하게 유지하지 않았습니다. 대신, 모델 훈련 및 성능을 최적화하기 위해 훈련 과정에서 사용되는 데이터 조합을 동적으로 조정했습니다. 데이터 처리에 대한 이러한 동적 접근 방식(dynamic approach)은 모델이 다양한 유형의 데이터에 걸쳐 일반화(generalize)하는 능력을 향상시키는 데 도움이 되었을 것입니다.

4.2.2 **선행 학습(Pre-training) II: 문맥 확장(Context Lengthening)을 위한 연속 선행 학습(Continued Pre-training)**

문맥 창(context window)을 한 번에 늘린 다른 모델들과 비교하여, Llama 3.1의 문맥 확장(context lengthening)은 더 점진적인 접근 방식이었습니다. 여기서 연구원들은 8,000개에서 128,000개 토큰까지 6개의 개별 단계를 통해 문맥 길이(context length)를 늘렸습니다. 이러한 단계별 증가(stepwise increment)는 모델이 더 큰 문맥에 더 원활하게 적응할 수 있도록 했을 것입니다. 이 과정에 사용된 훈련 세트(training set)는 전체 데이터셋 크기의 약 5%에 해당하는 8,000억 개의 토큰을 포함했습니다. 이처럼 점진적인 확장은 모델이 급격한 변화 없이 새로운 문맥 길이에 적응하고 안정적인 성능을 유지하도록 돕습니다.

4.2.3 **선행 학습(Pre-training) III: 고품질 데이터에 대한 어닐링(Annealing)**

세 번째 선행 학습 단계에서는 연구원들이 작지만 고품질의 혼합 데이터로 모델을 훈련시켰는데, 이는 벤치마크 데이터셋(benchmark dataset)의 성능을 향상시키는 데 도움이 된다는 것을 발견했습니다. 예를 들어, GSM8K 및 MATH 훈련 세트(training set)에 대한 어닐링(annealing)은 해당 GSM8K 및 MATH 검증 세트(validation set)에서 상당한 성능 향상을 제공했습니다. 논문의 3.1.3 섹션에서 연구원들은 어닐링(annealing) 데이터셋 크기가 400억 개의 토큰(전체 데이터셋 크기의 0.02%)이라고 밝혔습니다. 이 40B 어닐링 데이터셋은 데이터 품질을 평가하는 데 사용되었습니다. 3.4.3 섹션에서는 실제 어닐링이 4천만 개의 토큰(어닐링 데이터의 0.1%)에서만 수행되었다고 명시합니다. 이는 소량의 고품질 데이터만으로도 특정 벤치마크 성능을 크게 개선할 수 있음을 보여줍니다.

Llama 3.1 선행 학습(pre-training)을 위한 기술 요약.

4.3 **Llama 3.1 후속 학습(Post-training)**

메타 AI 팀은 후속 학습(post-training) 과정에서 지도 미세 조정(SFT), 거부 샘플링(rejection sampling), 직접 선호 최적화(DPO)를 포함하는 비교적 간단한 방법을 사용했습니다. 그들은 PPO를 사용한 RLHF와 같은 강화 학습 알고리즘이 이러한 기술에 비해 안정성이 떨어지고 확장하기 더 어렵다는 것을 관찰했습니다. SFT 및 DPO 단계가 인간이 생성한 데이터와 합성 데이터(synthetic data)를 모두 통합하여 여러 라운드에 걸쳐 반복적으로 수행되었다는 점은 주목할 가치가 있습니다. 추가 세부 사항을 설명하기 전에, 그들의 워크플로우는 아래 그림에 설명되어 있습니다.

Llama 3.1 논문에서 후속 학습 절차를 설명하는 주석이 달린 그림.

DPO를 사용했음에도 불구하고, RLHF에서 하는 것처럼 보상 모델(reward model)도 개발했다는 점에 유의하십시오. 처음에는 선행 학습 단계의 체크포인트(checkpoint)를 사용하여 인간이 주석을 단 데이터로 보상 모델을 훈련시켰습니다. 이 보상 모델은 그 후 거부 샘플링(rejection sampling) 과정에 사용되어 추가 훈련을 위한 적절한 프롬프트를 선택하는 데 도움이 되었습니다. 각 훈련 라운드에서 그들은 보상 모델뿐만 아니라 SFT 및 DPO 모델에도 모델 평균화 기법(model averaging technique)을 적용했습니다. 이 평균화는 최근 모델과 이전 모델의 파라미터(parameter)를 병합하여 시간이 지남에 따라 성능을 안정화(및 향상)시키는 것을 포함했습니다. 모델 평균화의 기술적 세부 사항에 관심 있는 분들을 위해, 저는 이전 글 "[Model Merging, Mixtures of Experts, and Towards Smaller LLMs](https://www.aheadofai.com/p/model-merging-mixtures-of-experts-and-towards-smaller-llms)"의 "모델 병합 및 가중치 평균 이해(Understanding Model Merging and Weight Averaging)" 섹션에서 이 주제를 논의했습니다.

요약하자면, 핵심적으로는 비교적 표준적인 SFT + DPO 단계입니다. 그러나 이 단계는 여러 라운드에 걸쳐 반복됩니다. 그런 다음, 그들은 거부 샘플링(rejection sampling)을 위해 보상 모델(Qwen 2 및 AFM과 유사)을 추가했습니다. 그들은 또한 Gemma처럼 모델 평균화(model averaging)를 사용했지만, 이는 보상 모델뿐만 아니라 관련된 모든 모델에 적용되었습니다.

Llama 3.1 후속 학습(post-training)을 위한 기술 요약.

4.4 **결론**

Llama 3 모델은 이전 Llama 2 모델과 상당히 표준적이고 유사하지만, 몇 가지 흥미로운 접근 방식을 가지고 있습니다. 특히, 15조 개의 토큰으로 구성된 대규모 훈련 세트(training set)는 Llama 3를 다른 모델들과 구별합니다. 흥미롭게도 애플의 AFM 모델처럼 Llama 3도 3단계 선행 학습(pre-training) 프로세스를 구현했습니다. 다른 최신 대규모 언어 모델과 달리 Llama 3는 지식 전수(knowledge distillation) 기술을 사용하지 않고, 대신 더 간단한 모델 개발 경로를 선택했습니다. 후속 학습(post-training)의 경우, 모델은 다른 모델에서 인기를 끌었던 더 복잡한 강화 학습 전략 대신 직접 선호 최적화(DPO)를 활용했습니다. 전반적으로, 이러한 선택은 더 간단하지만 입증된 방법을 통해 LLM 성능을 개선하는 데 중점을 둔다는 점에서 흥미롭습니다. 이는 복잡한 최신 기법만이 항상 최적의 해답은 아니라는 점을 시사하며, 견고하고 안정적인 파이프라인의 중요성을 강조합니다.

---

5.  **주요 시사점**

본 글에서 논의된 알리바바의 Qwen 2, 애플의 파운데이션 모델(AFM), 구글의 Gemma 2, 메타의 Llama 3 이 네 가지 모델에서 우리는 어떤 교훈을 얻을 수 있을까요?

네 가지 모델 모두 선행 학습(pre-training) 및 후속 학습(post-training)에 다소 다른 접근 방식을 취합니다. 물론 방법론이 겹치기는 하지만, 완전히 동일한 훈련 파이프라인(training pipeline)은 없습니다. 이는 고성능 LLM 개발에 단 하나의 정답이 존재하지 않음을 명확히 보여줍니다.

선행 학습 단계에서는 모든 기법이 다단계 파이프라인을 활용하는 공통점을 보였습니다. 이는 일반적인 핵심 선행 학습 이후 문맥 길이 확장(context lengthening)과 때로는 고품질 데이터에 대한 어닐링(annealing) 단계가 연이어 진행되는 형태입니다. 이러한 다단계 접근 방식은 모델이 다양한 유형의 지식을 점진적으로 습득하고, 특정 능력(예: 장문맥 이해)을 강화하는 데 효과적임을 시사합니다. 아래 그림은 선행 학습에 사용된 다양한 방법을 다시 한눈에 보여줍니다.

선행 학습(pre-training)에 사용된 기술 개요. 각 모델은 고유한 방식으로 다단계 훈련을 구성했지만, 핵심적인 목표는 유사했습니다.

후속 학습(post-training)에 있어서도 파이프라인 중 정확히 동일한 것은 없었습니다. 거부 샘플링(rejection sampling)은 이제 후속 학습 과정에서 필수적인 요소로 자리 잡은 것으로 보입니다. 그러나 DPO 또는 RLHF에 관해서는 아직 합의나 선호도(말장난 아님)가 없습니다. 이는 각 방법론의 장단점과 구현 복잡성, 그리고 특정 모델 및 데이터셋에 대한 적합성 때문일 것입니다.

후속 학습(post-training)에 사용된 기술 개요. DPO와 RLHF 사이의 선택은 여전히 활발한 연구 주제이며, 각 방법론의 변형들이 계속해서 등장하고 있습니다.

따라서, 고성능 LLM을 개발하는 데는 단 하나의 정답이 아니라 여러 가지 경로가 있습니다. 마지막으로, 네 가지 모델은 비슷한 수준의 성능을 보입니다. 안타깝게도 이 모델들 중 일부는 LMSYS 및 AlpacaEval 리더보드(leaderboard)에 포함되지 않아, MMLU와 같은 객관식 벤치마크(multiple-choice benchmark) 점수를 제외하고는 아직 직접적인 비교가 어렵습니다. 이는 LLM 평가 방법론의 표준화와 투명성 확보가 여전히 중요한 과제임을 보여줍니다.

이러한 분석을 통해 우리는 LLM 개발이 단순한 모델 스케일링을 넘어, 데이터 품질 관리, 다단계 훈련 전략, 그리고 인간의 선호도를 반영하기 위한 정교한 후속 학습 기법들이 복합적으로 작용하는 '데이터 중심 AI'의 시대로 접어들었음을 확인할 수 있습니다. 미래에는 이러한 파이프라인들이 더욱 자동화되고 효율화될 것이며, 모델 간의 상호작용을 통해 합성 데이터가 생성되고 지식이 전수되는 '모델 생태계'가 더욱 강화될 것으로 예상됩니다.

**Ahead of AI 지원**

이 잡지는 개인적인 열정 프로젝트입니다. 저를 지원하고 싶으신 분들은 제 "[Build a Large Language Model (From Scratch)](https://www.amazon.com/Build-Large-Language-Model-Scratch/dp/1032601700)" 책을 구매해 주시기를 고려해 주십시오. (이 책은 LLM이 어떻게 작동하는지 다른 곳에서는 찾을 수 없는 수준의 세부 사항으로 설명하므로, 이 책에서 많은 것을 얻으실 것이라고 확신합니다.)

[Build a Large Language Model (From Scratch)는 지금 Amazon에서 구매 가능합니다.](https://www.amazon.com/Build-Large-Language-Model-Scratch/dp/1032601700)

책을 읽으시고 잠시 시간을 내주실 수 있다면, [짧은 리뷰를 남겨주시면](https://www.amazon.com/Build-Large-Language-Model-Scratch/dp/1032601700#customerReviews) 정말 감사하겠습니다. 이는 저희 저자들에게 큰 도움이 됩니다! 또는, 최근에 이 잡지를 직접 지원하기 위해 Substack에서 유료 구독 옵션을 활성화했습니다.

Ahead of AI는 독자 지원 출판물입니다. 새로운 게시물을 받고 제 작업을 지원하려면 무료 또는 유료 구독자가 되는 것을 고려해 주십시오. [구독](https://www.aheadofai.com/subscribe)