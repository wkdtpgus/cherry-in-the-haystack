본문에서는 거대 언어 모델(LLM)의 사유 역량(reasoning capabilities)을 증진시키는 네 가지 핵심 방안을 다룹니다. 이 글이 귀중한 통찰력을 제공하고, 이 주제를 둘러싼 빠르게 발전하는 문헌과 과대광고 속에서 길을 찾는 데 도움이 되기를 바랍니다.

2024년 들어 LLM 영역은 특화 추세가 두드러졌습니다. 사전 훈련(pre-training)과 미세 조정(fine-tuning) 과정을 넘어, 검색 증강 생성(RAG)부터 프로그래밍 지원 도구(code assistants)에 이르기까지, 맞춤형 애플리케이션(applications)들이 출현하는 양상을 보였습니다. 저는 2025년에 이러한 경향이 더욱 가속화되어 도메인(domain) 및 애플리케이션(application)별 최적화(optimization), 즉 '전문화'에 더 큰 비중을 둘 것으로 예상합니다.

개발 단계의 1부부터 3부는 LLM의 일반적인 진화 과정입니다. 4단계는 특정 사용 사례(use cases)에 맞춰 LLM을 전문화하는 것을 의미합니다. 추론 모델(reasoning models)의 개발은 이러한 전문화 중 하나입니다. 이는 퍼즐, 고급 수학, 코딩 과제(coding challenges)와 같이 중간 단계(intermediate steps)를 통해 가장 잘 해결되는 난해한 과업에서 LLM이 뛰어난 성능을 발휘하도록 최적화하는 것을 의미합니다. 하지만 이러한 전문화가 다른 LLM 애플리케이션(applications)을 대체하는 것은 아닙니다. LLM을 추론 모델로 전환하는 것은 특정 한계점도 수반하기 때문이며, 이에 대해서는 추후 논의할 것입니다.

아래에서 다룰 내용을 간략히 살펴보자면, 이 글에서는 다음을 다룰 것입니다:

*   "추론 모델(reasoning model)"의 본질적 정의 해설
*   추론 모델의 강점과 약점 분석
*   DeepSeek R1의 개발 접근법 요약
*   추론 모델을 구축하고 개선하는 네 가지 주요 접근 방식 설명
*   DeepSeek V3 및 R1 출시 이후 LLM 생태계에 대한 견해 공유
*   제한된 예산으로 추론 모델을 개발하기 위한 조언 제시

올해 인공지능(AI)이 빠르게 발전하는 가운데 이 글이 유용하게 활용되기를 바랍니다!

### "추론 모델(reasoning model)"은 어떻게 정의할까요?

인공지능(AI) (혹은 통상적으로 기계 학습(machine learning)) 분야에서 종사한다면, 모호하고 열띤 논쟁이 벌어지는 정의에 익숙할 것입니다. "추론 모델(reasoning models)"이라는 용어도 예외는 아닙니다. 결국 누군가가 논문에서 공식적으로 정의하겠지만, 다음 논문에서 재정의되는 식의 과정이 반복될 것입니다.

본문에서는 '추론(reasoning)'을 다수의 중간 과정(intermediate steps)을 수반하는 복합적이고 연속적인 답변 도출(multi-step generation)을 요구하는 질의에 대응하는 행위로 간주합니다. 가령, '프랑스의 수도는 어디인가요?'와 같은 단순 사실 확인 질문은 추론 과정을 내포하지 않습니다. 이와 달리, '기차가 시속 60마일로 3시간을 달리면 얼마나 이동할까요?' 같은 질문은 기본적인 추론 능력(reasoning)을 요구합니다. 이는 해답에 이르기 전 거리, 속도, 시간 간의 연관성을 파악해야 하기 때문입니다.

일반적인 LLM은 짧은 응답만 제공할 수 있지만(왼쪽에 표시된 것처럼), 추론 모델은 일반적으로 사고 과정의 일부를 드러내는 중간 단계(intermediate steps)를 포함합니다. (추론 작업을 위해 특별히 개발되지 않은 많은 LLM도 응답에 중간 추론 단계(intermediate reasoning steps)를 제시할 수 있다는 점에 유의하십시오. 대부분의 최신 LLM은 기본적인 추론이 가능하며 "기차가 시속 60마일로 3시간 동안 이동한다면, 얼마나 멀리 갈까요?"와 같은 질문에 답할 수 있습니다. 따라서 오늘날 우리가 추론 모델(reasoning models)을 언급할 때, 일반적으로 퍼즐, 수수께끼, 수학적 증명과 같은 더 복잡한 추론 작업에 탁월한 LLM을 의미합니다.)

또한, 오늘날 추론 모델(reasoning models)로 불리는 대부분의 LLM은 응답의 일부로서 '사고(thought)' 또는 '고려(thinking)' 과정을 함께 제시합니다. LLM이 실제로 어떻게 '사고'하는지는 별개의 논의입니다.

추론 모델의 중간 단계(intermediate steps)는 두 가지 방식으로 나타날 수 있습니다. 첫째, 이전 그림에 표시된 것처럼 응답에 명시적으로 포함될 수 있습니다. 둘째, OpenAI의 o1과 같은 일부 추론 LLM은 사용자에게 표시되지 않는 중간 단계(intermediate steps)로 여러 번의 반복을 실행합니다.

"추론(reasoning)"은 두 가지 다른 수준에서 사용됩니다: 1) 입력을 처리하고 여러 중간 단계(intermediate steps)를 통해 생성하는 것, 그리고 2) 사용자에게 제공되는 응답의 일부로 일종의 추론을 제공하는 것.

### 추론 모델은 언제 활용해야 할까요?

이제 추론 모델(reasoning models)을 정의했으니, 더 흥미로운 부분인 추론 작업을 위한 LLM을 구축하고 개선하는 방법으로 넘어갈 수 있습니다. 하지만 기술적인 세부 사항에 들어가기 전에, 추론 모델이 실제로 언제 필요한지 고려하는 것이 중요합니다.

**추론 모델은 언제 필요할까요?**

추론 모델은 퍼즐 해독, 고난도 수학 과제, 복잡한 코딩 프로젝트(coding tasks) 등 난해한 업무를 효과적으로 처리하도록 고안되었습니다. 하지만 요약, 번역 또는 지식 기반 질문 답변과 같은 단순한 작업에는 불필요합니다. 실상, 모든 상황에 추론 모델을 적용하는 것은 비효율적이며 고비용을 초래할 수 있습니다. 예를 들어, 추론 모델은 대체로 운용 비용이 높고, 장황한 응답을 내놓으며, 때로는 '과도한 숙고(overthinking)'로 인해 오류를 범할 가능성도 있습니다. 여기에도 간단한 규칙이 적용됩니다: 작업에 적합한 도구(또는 LLM 유형)를 사용하십시오.

추론 모델의 주요 강점과 약점.

### DeepSeek 훈련 파이프라인(training pipeline)에 대한 간략한 개요

이어지는 부분에서 추론 모델을 구축하고 향상에 사용되는 네 가지 주요 방안을 논하기에 앞서, DeepSeek R1 기술 보고서에 명시된 DeepSeek R1의 개발 과정을 간략히 소개하고자 합니다. 해당 보고서는 흥미로운 사례 분석이자 추론 LLM 개발의 중요한 지침이 됩니다.

DeepSeek은 단일 R1 추론 모델을 출시한 것이 아니라 DeepSeek-R1-Zero, DeepSeek-R1, DeepSeek-R1-Distill이라는 세 가지 개별 변형 모델을 공개했습니다. 기술 보고서의 설명을 바탕으로, 저는 이 모델들의 개발 과정을 아래 다이어그램(diagram)에 요약했습니다.

**DeepSeek R1 기술 보고서에서 논의된 DeepSeek의 세 가지 다른 추론 모델 개발 과정.**

다음으로, 위 다이어그램에 표시된 과정을 간략히 살펴보겠습니다. 더 자세한 내용은 다음 섹션에서 다룰 것이며, 거기서 추론 모델을 구축하고 개선하는 네 가지 주요 접근 방식을 논의할 것입니다.

**(1) DeepSeek-R1-Zero:** 이 모델은 2024년 12월에 공개된 6710억 개의 매개변수(parameters)를 가진 사전 훈련(pre-trained) DeepSeek-V3 기반 모델(base model)에서 출발합니다. 연구진은 두 가지 보상(rewards) 유형을 활용하여 강화 학습(RL) 방식으로 이를 훈련했습니다. 이러한 방식은 통상적인 인간 피드백 기반 강화 학습(RLHF)의 구성 요소인 지도식 미세 조정(SFT) 단계가 생략되었기에 '콜드 스타트(cold start)' 훈련으로 명명됩니다.

**(2) DeepSeek-R1:** 이 모델은 DeepSeek의 주력 추론 모델로, DeepSeek-R1-Zero를 기반으로 구축되었습니다. 연구팀은 추가 지도식 미세 조정(SFT) 단계와 추가 강화 학습(RL)을 통해 이를 더욱 정교화하여 '콜드 스타트(cold-started)' R1-Zero 모델의 성능을 향상시켰습니다.

**(3) DeepSeek-R1-Distill\*:** 이전 단계에서 생성된 지도식 미세 조정(SFT) 데이터(data)를 사용하여 DeepSeek 팀은 Qwen 및 Llama 모델을 미세 조정(fine-tuned)하여 사유 능력을 증진시켰습니다. 전통적인 의미의 지식 증류(distillation)는 아니지만, 이 과정은 더 큰 DeepSeek-R1 671B 모델의 결과물(outputs)을 사용하여 더 작은 모델(Llama 8B 및 70B, Qwen 1.5B–30B)을 훈련시키는 것을 포함했습니다.

### 추론 모델을 구축하고 개선하는 4가지 주요 방법

본 장에서는 거대 언어 모델(LLM)의 사유 역량을 증진시키고, DeepSeek-R1, OpenAI의 o1 및 o3와 같은 전문화된 추론 모델을 개발하는 데 현재 활용되는 핵심 기법들을 서술할 것입니다.

**참고:** o1과 o3의 정확한 작동 방식은 OpenAI 외부에서는 알려져 있지 않습니다. 하지만 추론(inference) 및 학습(training) 기술의 조합을 활용하는 것으로 알려져 있습니다.

#### 1) 추론 시점 확장(Inference-time scaling)

거대 언어 모델(LLM)의 사유 역량(혹은 전반적인 성능)을 개선하는 한 가지 방안은 추론 시점 확장(inference-time scaling)입니다. 이 용어는 다의적일 수 있으나, 여기서는 추론(inference) 과정에서 연산 자원(computational resources)을 증대시켜 결과물의 질을 높이는 것을 지칭합니다. 대략적인 비유는 인간이 복잡한 문제를 더 오래 숙고할 시간을 주면 더 나은 반응을 보이는 경향이 있다는 것입니다. 마찬가지로, 응답을 생성하는 동안 LLM이 더 많이 '사고'하도록 장려하는 기술을 적용할 수 있습니다. (하지만 LLM이 실제로 '사고'하는지는 다른 논의입니다.)

추론 시점 확장(inference-time scaling)에 대한 한 가지 간단한 접근 방식은 영리한 프롬프트 엔지니어링(prompt engineering)입니다. 대표적인 사례는 사고의 사슬(CoT) 프롬프트(prompting) 기법으로, '단계적으로 사고하라(think step by step)'와 같은 지시문이 입력 질의(input prompt)에 포함됩니다. 이는 모델이 최종 응답으로 직행하는 대신, 중간 추론 단계(intermediate reasoning steps)를 생성하도록 유도하며, 이는 대개(항상 그렇지는 않지만) 난해한 문제에서 더 정교한 결과 도출로 이어질 수 있습니다. (이 전략을 "프랑스의 수도는 어디인가요?"와 같은 간단한 지식 기반 질문에 사용하는 것은 의미가 없다는 점에 유의하십시오. 이는 주어진 입력 질의(input query)에 추론 모델이 적합한지 여부를 파악하는 좋은 경험칙이기도 합니다.)

**2022년 논문 "Large Language Models are Zero-Shot Reasoners"에서 발췌한 고전적인 CoT 프롬프트(CoT prompting) 예시 (https://arxiv.org/abs/2205.11916).**

앞서 언급한 사고의 사슬(CoT) 접근 방식은 더 많은 출력 토큰(output tokens)을 생성함으로써 추론(inference) 비용을 증가시키기 때문에 추론 시점 확장(inference-time scaling)으로 볼 수 있습니다.

추론 시점 확장(inference-time scaling)의 또 다른 접근 방식은 투표 및 탐색 전략(voting and search strategies)을 사용하는 것입니다. 한 가지 간단한 예는 다수결 투표(majority voting)로, LLM이 여러 응답을 생성하게 하고 다수결 투표를 통해 올바른 응답을 선택합니다. 마찬가지로, 빔 탐색(beam search) 및 기타 탐색 알고리즘(search algorithms)을 사용하여 더 나은 응답을 생성할 수 있습니다. 이러한 다양한 전략에 대한 자세한 내용은 제가 이전 "2024년 주목할 만한 AI 연구 논문 (2부)" 기사에서 설명했던 "Scaling LLM Test-Time Compute Optimally can be More Effective than Scaling Model Parameters" 논문을 강력히 추천합니다.

**다양한 탐색 기반 방법은 최적의 답변을 선택하기 위해 프로세스 보상 기반 모델(process-reward-based model)에 의존합니다.**

**LLM Test-Time Compute 논문에서 발췌한 주석이 달린 그림, https://arxiv.org/abs/2408.03314**

DeepSeek R1 기술 보고서는 일반적인 추론 시점 확장(inference-time scaling) 방법(예: 과정 보상 모델 기반 및 몬테카를로 트리 탐색 기반 접근 방식)을 '성공적이지 못한 시도(unsuccessful attempts)'로 분류합니다. 이는 DeepSeek이 R1 모델의 긴 응답 생성 경향을 넘어 이러한 기술을 명시적으로 사용하지 않았음을 시사하며, 이는 V3 기반 모델(base model)에 비해 암묵적인 형태의 추론 시점 확장(inference-time scaling) 역할을 합니다. 하지만 명시적인 추론 시점 확장(inference-time scaling)은 LLM 자체 내에서보다는 응용 프로그램 계층(application layer)에서 구현되는 경우가 많으므로, DeepSeek은 여전히 앱 내에서 이러한 기술을 적용할 수 있습니다.

저는 OpenAI의 o1 및 o3 모델이 추론 시점 확장(inference-time scaling)을 사용한다고 생각하며, 이는 GPT-4o와 같은 모델에 비해 상대적으로 비싼 이유를 설명할 수 있습니다. 추론 시점 확장(inference-time scaling) 외에도 o1과 o3는 DeepSeek R1에 사용된 것과 유사한 강화 학습(RL) 파이프라인(pipelines)을 사용하여 훈련되었을 가능성이 높습니다. 강화 학습(reinforcement learning)에 대한 자세한 내용은 아래 다음 두 섹션에서 다루겠습니다.

#### 2) 순수 강화 학습(Pure reinforcement learning, RL)

DeepSeek R1 연구 보고서에서 제가 특히 주목했던 지점 중 하나는, 순수한 강화 학습(RL)만으로도 추론 능력이 발현될 수 있다는 그들의 발견이었습니다. 이것이 무엇을 의미하는지 더 자세히 살펴보겠습니다.

앞서 설명했듯이, DeepSeek은 세 가지 유형의 R1 모델을 개발했습니다. 첫 번째인 DeepSeek-R1-Zero는 2024년 12월에 공개된 표준 사전 훈련(pre-trained) 거대 언어 모델(LLM)인 DeepSeek-V3 기반 모델(base model)을 기반으로 구축되었습니다. 강화 학습(RL) 전에 지도식 미세 조정(SFT)이 적용되는 일반적인 강화 학습(RL) 파이프라인(pipelines)과 달리, DeepSeek-R1-Zero는 아래 다이어그램에 강조된 것처럼 초기 지도식 미세 조정(SFT) 단계 없이 오직 강화 학습(reinforcement learning)으로만 훈련되었습니다.

**DeepSeek-R1-Zero 모델의 개발 과정.**

그럼에도 불구하고, 이 강화 학습(RL) 과정은 일반적으로 LLM의 선호도 조정(preference-tune)에 적용되는 인간 피드백 기반 강화 학습(RLHF) 접근 방식과 유사합니다. (저는 제 기사 "LLM 학습: RLHF와 그 대안"에서 RLHF를 더 자세히 다루었습니다.) 하지만 위에서 언급했듯이, DeepSeek-R1-Zero의 주요 차이점은 명령어 조정(instruction tuning)을 위한 지도식 미세 조정(SFT) 단계를 건너뛰었다는 것입니다. 이것이 그들이 이를 '순수(pure)' 강화 학습(RL)이라고 부르는 이유입니다. (하지만 LLM 맥락에서의 강화 학습(RL)은 전통적인 강화 학습(RL)과는 상당히 다르며, 이는 나중에 다룰 주제입니다.)

보상(rewards) 체계로는, 사람의 선호도를 기반으로 훈련된 보상 모델(reward model) 대신, 정밀도 보상(accuracy reward)과 양식 보상(format reward)이라는 두 가지 종류를 채택했습니다. 정밀도 보상(accuracy reward)은 LeetCode 컴파일러(compiler)를 통해 코딩 해답의 유효성을 검증하고, 결정론적 체계(deterministic system)를 활용하여 수학적 응답을 평가합니다. 양식 보상(format reward)은 LLM 평가자(judge)를 통해 응답이 `<think>` 태그(tags) 내에 추론 단계(reasoning steps)를 포함하는 것과 같은 기대되는 양식을 준수하는지 확인합니다.

놀랍게도, 이 접근 방식은 LLM이 기본적인 추론 기술을 개발하기에 충분했습니다. 연구원들은 아래 그림에 표시된 것처럼, 모델이 명시적으로 훈련되지 않았음에도 불구하고 응답의 일부로 추론 흔적(reasoning traces)을 생성하기 시작하는 '아하!(Aha!)' 순간을 관찰했습니다.

**DeepSeek R1 기술 보고서에서 발췌한 "아하(Aha)" 순간의 출현을 보여주는 그림 (https://arxiv.org/abs/2501.12948).**

R1-Zero는 최고 성능의 추론 모델은 아니지만, 위 그림에 표시된 것처럼 중간 '사고' 단계(intermediate "thinking" steps)를 생성함으로써 추론 능력을 보여줍니다. 이는 순수 강화 학습(RL)을 사용하여 추론 모델을 개발하는 것이 가능함을 확인시켜주며, DeepSeek 팀이 이 접근 방식을 처음으로 시연(또는 최소한 발표)했습니다.

Ahead of AI는 독자 지원 출판물입니다. 새로운 게시물을 받고 제 작업을 지원하려면 무료 또는 유료 구독자가 되는 것을 고려해 보세요.
[Subscribe]

#### 3) 지도식 미세 조정(Supervised finetuning, SFT) 및 강화 학습(reinforcement learning, RL)

다음으로, 추론 모델 구축을 위한 청사진 역할을 하는 DeepSeek의 주력 추론 모델인 DeepSeek-R1의 개발을 살펴보겠습니다. 이 모델은 추가적인 지도식 미세 조정(SFT)과 강화 학습(RL)을 결합하여 사유 성능을 증진시키며, DeepSeek-R1-Zero의 한계를 뛰어넘습니다. 표준 인간 피드백 기반 강화 학습(RLHF) 파이프라인(pipeline)에서 볼 수 있듯이, 강화 학습(RL) 전에 지도식 미세 조정(SFT) 단계를 포함하는 것이 실제로 일반적이라는 점에 유의하십시오. OpenAI의 o1도 유사한 접근 방식을 사용하여 개발되었을 가능성이 높습니다.

**DeepSeek-R1 모델의 개발 과정.**

상기 도표에 나타난 바와 같이, DeepSeek 연구진은 DeepSeek-R1-Zero를 활용하여 '콜드 스타트(cold-start)' 지도식 미세 조정(SFT) 데이터를 만들어냈습니다. '콜드 스타트(cold start)'라는 명칭은 이 데이터가 DeepSeek-R1-Zero에 의해 생산되었으며, DeepSeek-R1-Zero 자체는 어떠한 지도식 미세 조정(SFT) 데이터로도 훈련되지 않았다는 사실을 뜻합니다. 이 콜드 스타트 지도식 미세 조정(SFT) 데이터(data)를 사용하여 DeepSeek은 명령어 미세 조정(instruction fine-tuning)을 통해 모델을 훈련시킨 다음, 또 다른 강화 학습(RL) 단계를 거쳤습니다. 이 강화 학습(RL) 단계는 DeepSeek-R1-Zero의 강화 학습(RL) 과정에서 사용된 것과 동일한 정밀도 및 양식 보상(rewards)을 유지했습니다. 하지만 그들은 언어 혼합(language mixing)을 방지하기 위해 일관성 보상(consistency reward)을 추가했습니다. 언어 혼합은 모델이 응답 내에서 여러 언어 사이를 전환할 때 발생합니다.

강화 학습(RL) 단계 다음에는 또 다른 지도식 미세 조정(SFT) 데이터(data) 수집 라운드가 이어졌습니다. 이 단계에서는 가장 최근의 모델 체크포인트(checkpoint)가 60만 개의 사고의 사슬(CoT) 지도식 미세 조정(SFT) 예시를 생성하는 데 사용되었고, 추가로 20만 개의 지식 기반 지도식 미세 조정(SFT) 예시는 DeepSeek-V3 기반 모델(base model)을 사용하여 생성되었습니다. 이 60만 + 20만 개의 지도식 미세 조정(SFT) 샘플(samples)은 최종 강화 학습(RL) 라운드를 진행하기 전에 DeepSeek-V3 기반 모델(base model)의 명령어 미세 조정(instruction-finetuning)에 사용되었습니다. 이 단계에서는 수학 및 코딩 질문에 대한 정밀도 보상(accuracy rewards)에 규칙 기반 방법(rule-based methods)을 다시 사용했으며, 다른 질문 유형에는 인간 선호도 레이블(human preference labels)이 사용되었습니다. 종합적으로 볼 때, 이는 지도식 미세 조정(SFT) 데이터(data)에 (더 많은) 사고의 사슬(CoT) 예시가 포함되어 있다는 점을 제외하면 일반적인 인간 피드백 기반 강화 학습(RLHF)과 매우 유사합니다. 그리고 강화 학습(RL)은 인간 선호도 기반 보상(rewards) 외에 검증 가능한 보상(verifiable rewards)을 가집니다.

최종 모델인 DeepSeek-R1은 아래 표에 표시된 것처럼 추가 지도식 미세 조정(SFT) 및 강화 학습(RL) 단계 덕분에 DeepSeek-R1-Zero보다 눈에 띄는 성능 향상을 보입니다.

**OpenAI O1 및 DeepSeek R1 모델의 벤치마크(benchmark) 비교.**

**DeepSeek-R1 기술 보고서에서 발췌한 주석이 달린 그림 (https://arxiv.org/abs/2501.12948).**

#### 4) 순수 지도식 미세 조정(Pure supervised finetuning, SFT) 및 지식 증류(distillation)

지금까지 추론 모델을 구축하고 개선하는 세 가지 주요 접근 방식을 다루었습니다:

1.  **추론 시점 확장(inference-time scaling)**: 기본 모델을 훈련시키거나 수정하지 않고 사유 능력을 향상시키는 기술.
2.  **DeepSeek-R1-Zero에서와 같은 순수 강화 학습(RL)**: 지도식 미세 조정(supervised fine-tuning) 없이도 추론이 학습된 행동으로 나타날 수 있음을 보여주었습니다.
3.  **지도식 미세 조정(SFT) + 강화 학습(RL)**: DeepSeek의 주력 추론 모델인 DeepSeek-R1로 이어졌습니다.

그렇다면 남은 것은 무엇일까요? 모델 '지식 증류(distillation)'입니다. 놀랍게도 DeepSeek은 그들이 지식 증류(distillation)라고 부르는 과정을 통해 훈련된 더 작은 모델들도 출시했습니다. 하지만 LLM 맥락에서 지식 증류(distillation)는 딥러닝(deep learning)에서 사용되는 고전적인 지식 증류(knowledge distillation) 접근 방식을 반드시 따르지는 않습니다. 전통적으로 지식 증류(knowledge distillation)에서는(제 책 "Machine Learning Q and AI" 6장에서 간략히 설명했듯이) 더 작은 학생 모델(student model)이 더 큰 교사 모델(teacher model)의 로짓(logits)과 목표 데이터셋(target dataset) 모두에서 훈련됩니다. 그 대신, 본 맥락에서의 지식 증류(distillation)는 대형 거대 언어 모델(LLM)이 생성한 지도식 미세 조정(SFT) 데이터셋을 활용하여 Llama 8B 및 70B, Qwen 2.5 모델(0.5B ~ 32B)과 같은 소형 LLM을 명령어 미세 조정(instruction fine-tuning)하는 과정을 뜻합니다. 구체적으로, 이러한 대형 LLM은 DeepSeek-V3와 DeepSeek-R1의 중간 체크포인트(checkpoint)입니다. 사실, 이 지식 증류(distillation) 과정에 사용된 지도식 미세 조정(SFT) 데이터(data)는 이전 섹션에서 설명한 DeepSeek-R1을 훈련시키는 데 사용된 것과 동일한 데이터셋(dataset)입니다.

**DeepSeek-R1-Distill 모델의 개발 과정.**

그들이 이러한 지식 증류 모델(distilled models)을 만들어낸 까닭은 무엇일까요? 저는 두 가지 핵심적인 배경이 있다고 봅니다:

1.  **소규모 모델의 높은 효율성.** 이는 운영 비용을 절감할 뿐만 아니라, 저성능 하드웨어(hardware)에서도 구동 가능하여 저를 포함한 수많은 연구자 및 실험가들에게 특히 매력적입니다.
2.  **순수 지도식 미세 조정(SFT)의 실증 사례.** 이 지식 증류 모델들은 강화 학습(RL) 없이 오직 지도식 미세 조정(SFT)만으로 모델이 얼마나 진보할 수 있는지를 보여주는 흥미로운 기준점(benchmark) 역할을 합니다.

아래 표는 이 지식 증류 모델(distilled models)의 성능을 다른 인기 모델뿐만 아니라 DeepSeek-R1-Zero 및 DeepSeek-R1과 비교합니다.

**지식 증류 모델(distilled models)과 비증류 모델(non-distilled models)의 벤치마크(benchmark) 비교.**

**DeepSeek-R1 기술 보고서에서 발췌한 주석이 달린 그림 (https://arxiv.org/abs/2501.12948).**

보시다시피, 지식 증류 모델(distilled models)은 DeepSeek-R1보다 눈에 띄게 약하지만, DeepSeek-R1-Zero에 비해서는 훨씬 작음에도 불구하고 놀랍도록 강력합니다. 또한 이 모델들이 o1 mini와 비교하여 얼마나 잘 작동하는지 주목하는 것도 흥미롭습니다(저는 o1-mini 자체가 o1의 유사한 지식 증류 버전일 수 있다고 생각합니다).

이 섹션을 결론으로 마무리하기 전에, 언급할 가치가 있는 또 다른 흥미로운 비교가 있습니다. DeepSeek 팀은 DeepSeek-R1-Zero에서 나타난 발현적 추론 행동(emergent reasoning behavior)이 더 작은 모델에서도 나타날 수 있는지 테스트했습니다. 이를 조사하기 위해, 그들은 DeepSeek-R1-Zero의 동일한 순수 강화 학습(RL) 접근 방식을 Qwen-32B에 직접 적용했습니다. 이 실험 결과는 아래 표에 요약되어 있으며, QwQ-32B-Preview는 Qwen 팀이 개발한 Qwen 2.5 32B 기반의 참조 추론 모델(reference reasoning model) 역할을 합니다(훈련 세부 사항은 공개되지 않은 것으로 생각됩니다). 이 비교는 순수 강화 학습(RL)만으로 DeepSeek-R1-Zero보다 훨씬 작은 모델에서 추론 능력을 유도할 수 있는지에 대한 추가적인 통찰력을 제공합니다.

**더 작은 32B 모델에 대한 지식 증류(distillation) 및 강화 학습(RL) 벤치마크(benchmark) 비교.**

**DeepSeek-R1 기술 보고서에서 발췌한 주석이 달린 그림 (https://arxiv.org/abs/2501.12948).**

흥미롭게도, 결과는 지식 증류(distillation)가 더 작은 모델에 대해 순수 강화 학습(RL)보다 훨씬 더 효과적임을 시사합니다. 이는 강화 학습(RL)만으로는 이 규모의 모델에서 강력한 추론 능력을 유도하기에 충분하지 않을 수 있으며, 고품질 추론 데이터(data)에 대한 지도식 미세 조정(SFT)이 작은 모델로 작업할 때 더 효과적인 전략이 될 수 있다는 생각과 일치합니다.

완전성을 위해, 표에 추가적인 비교를 포함했으면 유용했을 것입니다:

1.  DeepSeek-R1이 개발된 방식과 유사하게 지도식 미세 조정(SFT) + 강화 학습(RL)로 훈련된 Qwen-32B. 이는 강화 학습(RL)이 지도식 미세 조정(SFT)과 결합될 때 순수 강화 학습(RL) 및 순수 지도식 미세 조정(SFT)과 비교하여 얼마나 많은 개선이 이루어질 수 있는지 판단하는 데 도움이 될 것입니다.
2.  지식 증류 모델(distilled models)이 생성된 방식과 유사하게 순수 지도식 미세 조정(SFT)으로 훈련된 DeepSeek-V3. 이는 강화 학습(RL) + 지도식 미세 조정(SFT)이 순수 지도식 미세 조정(SFT)보다 얼마나 효과적인지 직접 비교할 수 있게 해줄 것입니다.

Ahead of AI는 독자 지원 출판물입니다. 새로운 게시물을 받고 제 작업을 지원하려면 무료 또는 유료 구독자가 되는 것을 고려해 보세요.
[Subscribe]

### 결론

이 섹션에서는 추론 모델을 구축하고 개선하기 위한 네 가지 다른 전략을 살펴보았습니다:

1.  **추론 시점 확장(inference-time scaling)**은 별도의 훈련 과정 없이도 적용 가능하나, 추론 비용을 상승시켜 사용자 수나 질의(query)량이 늘어날수록 대규모 시스템 구축(deployment)의 경제성을 저해합니다. 그럼에도 불구하고, 이미 우수한 모델의 성능을 한층 더 끌어올리는 데는 자연스러운 선택지입니다. 저는 o1이 추론 시점 확장(inference-time scaling)을 활용한다고 강력히 의심하며, 이는 DeepSeek-R1에 비해 토큰(token)당 비용이 더 비싼 이유를 설명하는 데 도움이 됩니다.
2.  **순수 강화 학습(RL)**은 사유 능력이 예상치 못하게 발현되는 현상(emergent behavior)에 대한 심층적 이해를 제공하여 연구적 관점에서 매력적입니다. 그러나 실제 모델 개발에서는 강화 학습(RL)과 지도식 미세 조정(SFT)의 결합이 더욱 강력한 추론 모델을 구현하므로, 이 방식이 더 선호됩니다. 저는 o1도 강화 학습(RL) + 지도식 미세 조정(SFT)을 사용하여 훈련되었다고 강력히 의심합니다. 더 정확히 말하면, o1은 DeepSeek-R1보다 약하고 작은 기반 모델(base model)에서 시작하지만, 강화 학습(RL) + 지도식 미세 조정(SFT)과 추론 시점 확장(inference-time scaling)으로 이를 보완한다고 생각합니다.
3.  위에서 언급했듯이, **강화 학습(RL) + 지도식 미세 조정(SFT)**는 고성능 추론 모델을 구축하기 위한 핵심 접근 방식입니다. DeepSeek-R1은 이를 수행하는 방법을 보여주는 좋은 청사진입니다.
4.  **지식 증류(distillation)**는 특히 더 작고 효율적인 모델을 만드는 데 매력적인 접근 방식입니다. 하지만 지식 증류(distillation)는 혁신을 주도하거나 차세대 추론 모델을 생산하지 못한다는 한계가 있습니다. 예를 들어, 지식 증류(distillation)는 항상 지도식 미세 조정(SFT) 데이터(data)를 생성하기 위해 기존의 더 강력한 모델에 의존합니다.

다음으로 제가 기대하는 흥미로운 측면 중 하나는 강화 학습(RL) + 지도식 미세 조정(SFT)(접근 방식 3)을 추론 시점 확장(inference-time scaling, 접근 방식 1)과 결합하는 것입니다. 이것이 OpenAI o1이 하고 있는 일일 가능성이 높습니다. 단, DeepSeek-R1보다 약한 기반 모델(base model)을 기반으로 할 가능성이 있으며, 이는 DeepSeek-R1이 추론 시간(inference time)에 상대적으로 저렴하면서도 왜 그렇게 좋은 성능을 보이는지 설명합니다.

### DeepSeek R1에 대한 견해

최근 몇 주 동안 많은 사람들이 DeepSeek-R1 모델에 대한 제 생각을 물어왔습니다. 요컨대, 저는 이 모델들이 매우 뛰어난 업적이라고 평가합니다. 연구 개발자(research engineer)의 입장에서, 그들의 접근 방식(methodology)에 대한 깊이 있는 이해를 제공하여 제가 배울 수 있었던 상세한 기술 문헌(technical report)을 특히 높이 삽니다. 가장 흥미로운 점 중 하나는 순수 강화 학습(RL)으로부터 추론 능력이 어떻게 발현되었는지입니다. 그리고 DeepSeek이 Meta의 Llama 모델보다도 제한이 적은 허용적인 오픈 소스(open-source) MIT 라이선스(license) 하에 모델을 공개(open-sourced)했다는 점은 인상적입니다.

**o1과 비교하면 어떤가요?**

DeepSeek-R1이 o1보다 더 나은가요? 저는 대략 비슷한 수준이라고 말하고 싶습니다. 그러나 주목할 만한 점은 DeepSeek-R1이 추론 과정(inference time)에서 더 높은 효율성을 보인다는 것입니다. 이는 DeepSeek이 훈련 절차(training process)에 더 많은 자원을 투입했을 가능성을 시사하며, OpenAI는 o1에 대해 추론 시점 확장(inference-time scaling)에 더 크게 의존했을 수 있음을 의미합니다.

그렇긴 하지만, OpenAI가 o1에 대해 많은 것을 공개하지 않았기 때문에 o1과 DeepSeek-R1을 직접 비교하기는 어렵습니다. 예를 들어, 우리는 다음을 알지 못합니다:

*   o1도 전문가 혼합(MoE) 모델인가요?
*   o1은 얼마나 큰가요?
*   o1이 최소한의 강화 학습(RL) + 지도식 미세 조정(SFT)과 광범위한 추론 시점 확장(inference-time scaling)만 적용된 GPT-4o의 약간 개선된 버전일 수 있을까요?

이러한 세부 사항을 알지 못하면, 직접적인 비교는 여전히 동떨어진 비교(apples-to-oranges comparison)로 남습니다.

### DeepSeek-R1 훈련 비용

또 다른 논의점은 DeepSeek-R1 개발 비용이었습니다. 일부에서는 약 6백만 달러의 훈련 비용을 거론했으나, 이는 DeepSeek-V3(지난 12월에 공개된 기반 모델(base model))와 DeepSeek-R1을 혼동했을 공산이 큽니다. 6백만 달러라는 추산치는 GPU 시간당 2달러와 2024년 12월에 처음 언급되었던 DeepSeek-V3의 최종 훈련 과정에 소요된 GPU 시간량을 전제로 한 것입니다. 하지만 DeepSeek 팀은 R1의 정확한 GPU 시간이나 개발 비용을 공개한 적이 없으므로, 모든 비용 추정치는 순전히 추측에 불과합니다.

어느 쪽이든, 궁극적으로 DeepSeek-R1은 공개 가중치(open-weight) 추론 모델의 주요 이정표이며, 추론 시간(inference time)에서의 효율성은 OpenAI의 o1에 대한 흥미로운 대안이 됩니다.

### 제한된 예산으로 추론 모델 개발하기

DeepSeek-V3와 같은 공개 가중치(open-weight) 기반 모델(base model)을 사용하더라도, DeepSeek-R1급의 추론 모델을 개발하기 위해서는 수십만에서 수백만 달러에 이르는 자금이 필요할 공산이 큽니다. 이는 한정된 예산으로 작업하는 연구원이나 개발자에게 좌절감을 안겨줄 수 있습니다.

**희소식: 지식 증류(distillation)는 많은 것을 할 수 있습니다.**

다행히도 모델 지식 증류(model distillation)는 더 비용 효율적인 대안을 제공합니다. DeepSeek 팀은 DeepSeek-R1보다 훨씬 작음에도 불구하고 놀랍도록 강력한 추론 성능을 달성하는 R1 지식 증류 모델(distilled models)로 이를 입증했습니다. 하지만 이 접근 방식조차 완전히 저렴하지는 않습니다. 그들의 지식 증류(distillation) 과정은 80만 개의 지도식 미세 조정(SFT) 샘플(samples)을 사용했으며, 이는 상당한 연산(compute)을 필요로 합니다.

흥미로운 사실은, DeepSeek-R1이 공개되기 며칠 앞서 저는 Sky-T1에 관한 기사를 접했습니다. 이는 소규모 연구진이 불과 1만 7천 개의 지도식 미세 조정(SFT) 샘플만을 활용하여 공개 가중치(open-weight) 320억 매개변수 모델을 훈련시킨 인상적인 프로젝트였습니다. 총 소요 비용은? 단 450달러로, 대다수 AI 학술대회(conferences) 참가비보다 저렴합니다. 이 예시는 대규모 훈련(training)은 여전히 고가이지만, 더 작고 목표 지향적인 미세 조정(fine-tuning) 노력은 비용의 일부만으로도 인상적인 결과를 낼 수 있음을 강조합니다.

**"Sky-T1: Train your own O1 preview model within $450" 기사에서 발췌한 그림, https://novasky-ai.github.io/posts/sky-t1/**

그들의 벤치마크(benchmarks)에 따르면, Sky-T1은 o1과 거의 동등한 성능을 보이며, 낮은 훈련 비용을 고려할 때 인상적입니다.

**예산 내 순수 강화 학습(RL): TinyZero**

Sky-T1이 모델 지식 증류(model distillation)에 중점을 두는 동안, 저는 '순수 강화 학습(RL)' 분야에서도 흥미로운 작업을 접했습니다. 주목할 만한 예시 중 하나는 DeepSeek-R1-Zero 접근 방식을 재현한 30억 매개변수(parameter) 모델인 TinyZero입니다(참고: 훈련 비용은 30달러 미만입니다). 놀랍게도, 단 30억 매개변수(parameters)임에도 불구하고 TinyZero는 일부 발현적 자기 검증 능력(emergent self-verification abilities)을 보여주며, 이는 작은 모델에서도 순수 강화 학습(RL)을 통해 추론이 나타날 수 있다는 생각을 뒷받침합니다. TinyZero 저장소(repository)는 연구 보고서가 아직 진행 중이라고 언급하며, 저는 추가 세부 사항을 계속 주시할 것입니다.

**TinyZero 저장소(https://github.com/Jiayi-Pan/TinyZero)에서 발췌한 모델이 자기 검증(self-verification)이 가능함을 보여주는 그림. (비교를 위해 기반 모델(base model)의 응답을 보는 것도 흥미로웠을 것입니다.)**

위에서 언급된 두 프로젝트는 제한된 예산으로도 추론 모델에 대한 흥미로운 작업이 가능하다는 것을 보여줍니다. 두 접근 방식 모두 DeepSeek-R1의 방법을 재현하지만, 하나는 순수 강화 학습(RL)(TinyZero)에, 다른 하나는 순수 지도식 미세 조정(SFT)(Sky-T1)에 중점을 둡니다. 이러한 아이디어를 어떻게 더 확장할 수 있을지 탐구하는 것은 매우 흥미로울 것입니다.

**전통적인 지도식 미세 조정(SFT)을 넘어: 여정 학습(Journey Learning)**

작년에 제가 접한 특히 흥미로운 접근 방식 중 하나는 "O1 Replication Journey: A Strategic Progress Report – Part 1" 논문에 설명되어 있습니다. 제목과는 달리, 이 논문은 실제로 o1을 재현하지 않습니다. 대신, 지식 증류(distillation)(순수 지도식 미세 조정(SFT)) 과정을 개선하는 다른 방법을 소개합니다. 이 논문의 핵심 아이디어는 '지름길 학습(shortcut learning)'의 대안으로서 '여정 학습(journey learning)'입니다. 지름길 학습(shortcut learning)은 명령어 미세 조정(instruction fine-tuning)에서 모델이 올바른 해결 경로만을 사용하여 학습되는 전통적인 접근 방식을 의미합니다. 반면에 여정 학습(journey learning)은 잘못된 해결 경로도 포함하여 모델이 실수로부터 학습할 수 있도록 합니다. 이 접근 방식은 TinyZero의 순수 강화 학습(RL) 훈련에서 관찰된 자기 검증 능력(self-verification abilities)과 어느 정도 관련이 있지만, 지도식 미세 조정(SFT)을 통해서만 모델을 개선하는 데 중점을 둡니다. 모델을 잘못된 추론 경로와 그 수정 사항에 노출시킴으로써, 여정 학습(journey learning)은 자기 수정 능력(self-correction abilities)을 강화하여 추론 모델을 이 방식으로 더 신뢰할 수 있게 만들 수 있습니다.

**전통적인 지름길 학습(shortcut learning)과 달리 여정 학습(journey learning)은 지도식 미세 조정(SFT) 데이터(data)에 잘못된 해결 경로를 포함합니다.**

**O1 Replication Journey: A Strategic Progress Report – Part 1에서 발췌한 주석이 달린 그림 (https://arxiv.org/abs/2410.18982)**

이는 특히 강화 학습(RL) 기반 접근 방식이 연산적으로 비현실적일 수 있는 저예산 추론 모델 개발에 있어 미래 작업의 흥미로운 방향이 될 수 있습니다. 어쨌든, 추론 모델 분야에서는 현재 많은 흥미로운 작업이 진행 중이며, 앞으로 몇 달 안에 훨씬 더 많은 흥미로운 작업을 보게 될 것이라고 확신합니다!

이 잡지는 개인적인 열정 프로젝트입니다. 저를 지원하고 싶으신 분들은 제 책 "Build a Large Language Model (From Scratch)"을 구매해 주시면 감사하겠습니다. (이 책은 다른 곳에서는 찾아볼 수 없는 수준의 세부 사항으로 LLM이 어떻게 작동하는지 설명하므로, 이 책에서 많은 것을 얻으실 것이라고 확신합니다.)

**"Build a Large Language Model (From Scratch)" 아마존(Amazon)에서 지금 구매 가능**

책을 읽으셨고 잠시 시간을 내주실 수 있다면, 짧은 서평을 남겨주시면 정말 감사하겠습니다. 이는 저희 저자들에게 큰 도움이 됩니다! 여러분의 지원은 큰 의미가 있습니다! 감사합니다!