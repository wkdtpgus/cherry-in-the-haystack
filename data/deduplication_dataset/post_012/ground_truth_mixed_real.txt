이 글에서는 추론 모델(reasoning models)을 구축하는 네 가지 주요 접근 방식, 즉 LLM(대규모 언어 모델)에 추론 능력을 강화하는 방법을 설명합니다. 이 글이 귀중한 통찰력을 제공하고, 이 주제를 둘러싼 빠르게 발전하는 문헌과 과대광고 속에서 길을 찾는 데 도움이 되기를 바랍니다.

최근 몇 년간, LLM 분야에서는 전문화가 급격히 진행되었습니다. 특히 2024년에는 사전 학습(pre-training)과 미세 조정(fine-tuning)을 넘어 RAG(검색 증강 생성)나 코드 어시스턴트(code assistants)와 같은 특화된 애플리케이션(applications)이 활발히 등장했습니다. 이제 2025년을 넘어 2026년을 바라보는 시점에서, 이러한 전문화 추세는 더욱 가속화되어 특정 도메인(domain)과 애플리케이션(application)에 최적화된 모델 개발에 전례 없는 관심이 집중되고 있습니다. 이는 단순히 성능 향상을 넘어, 실제 산업 현장에서의 LLM 활용도를 극대화하려는 노력의 일환으로 해석됩니다.

LLM 개발은 보통 1단계부터 3단계까지의 일반적인 절차를 따릅니다. 이후 4단계에서는 특정 사용 사례(use cases)에 맞춰 LLM을 전문화하게 됩니다. 이러한 전문화의 한 예가 바로 추론 모델(reasoning models)의 개발입니다. 추론 모델은 퍼즐 풀이, 고난도 수학 문제, 코딩 챌린지(coding challenges)와 같이 중간 단계(intermediate steps)를 거쳐야 해결되는 복잡한 작업에서 LLM이 뛰어난 성능을 발휘하도록 정교하게 다듬는 것을 의미합니다. 하지만 이러한 전문화가 다른 LLM 애플리케이션(applications)을 대체하는 것은 아닙니다. LLM을 추론 모델로 전환하는 과정에는 특정 단점도 따르며, 이에 대해서는 이 글에서 자세히 다룰 예정입니다.

아래에서 다룰 내용을 간략히 살펴보자면, 이 글에서는 다음을 다룰 것입니다:

*   "추론 모델(reasoning model)"의 정의와 의미 설명
*   추론 모델의 장점과 단점에 대한 심층 논의
*   DeepSeek R1의 방법론 개요 분석
*   추론 모델을 구축하고 개선하는 네 가지 주요 접근 방식 설명
*   DeepSeek V3 및 R1 출시 이후 LLM 환경의 변화와 그에 대한 생각 공유
*   제한된 예산으로 추론 모델을 개발하기 위한 실용적인 팁 제공
*   최신 연구 동향과 앞으로의 발전 방향 제시

올해 AI가 빠르게 발전하는 가운데 이 글이 유용하게 활용되기를 바랍니다!

### "추론 모델(reasoning model)"은 어떻게 정의할까요?

AI(또는 일반적으로 머신러닝(machine learning)) 분야에서 일한다면, 모호하고 열띤 논쟁이 벌어지는 정의에 익숙할 것입니다. "추론 모델(reasoning models)"이라는 용어도 예외는 아닙니다. 결국 누군가가 논문에서 공식적으로 정의하겠지만, 다음 논문에서 재정의되는 식의 과정이 반복될 것입니다.

이 글에서 저는 "추론(reasoning)"을 중간 단계(intermediate steps)를 포함하는 복잡하고 다단계적인 생성(multi-step generation)을 필요로 하는 질문에 답하는 과정으로 정의합니다. 예를 들어, "프랑스의 수도는 어디인가요?"와 같은 사실 기반 질문 답변은 추론을 포함하지 않습니다. 반대로, "기차가 시속 60마일로 3시간 동안 이동한다면, 얼마나 멀리 갈까요?"와 같은 질문은 간단한 추론을 필요로 합니다. 예를 들어, 답에 도달하기 전에 거리, 속도, 시간 사이의 관계를 인식해야 합니다.

일반적인 LLM은 짧은 답변만 제공할 수 있지만, 추론 모델은 일반적으로 사고 과정의 일부를 드러내는 중간 단계(intermediate steps)를 포함합니다. (추론 작업을 위해 특별히 개발되지 않은 많은 LLM도 답변에 중간 추론 단계(intermediate reasoning steps)를 제공할 수 있다는 점에 유의하십시오. 대부분의 최신 LLM은 기본적인 추론이 가능하며 "기차가 시속 60마일로 3시간 동안 이동한다면, 얼마나 멀리 갈까요?"와 같은 질문에 답할 수 있습니다. 따라서 오늘날 우리가 추론 모델(reasoning models)을 언급할 때, 일반적으로 퍼즐, 수수께끼, 수학적 증명과 같은 더 복잡한 추론 작업에 탁월한 LLM을 의미합니다.)

또한, 오늘날 추론 모델(reasoning models)로 불리는 대부분의 LLM은 답변의 일부로 "사고(thought)" 또는 "생각(thinking)" 과정을 포함합니다. LLM이 실제로 어떻게 "생각"하는지는 별개의 논의입니다.

추론 모델의 중간 단계(intermediate steps)는 두 가지 방식으로 나타날 수 있습니다. 첫째, 이전 그림에 표시된 것처럼 응답에 명시적으로 포함될 수 있습니다. 둘째, OpenAI의 o1과 같은 일부 추론 LLM은 사용자에게 표시되지 않는 중간 단계(intermediate steps)로 여러 번의 반복을 실행합니다.

"추론(reasoning)"은 두 가지 다른 수준에서 사용됩니다: 1) 입력을 처리하고 여러 중간 단계(intermediate steps)를 통해 생성하는 것, 그리고 2) 사용자에게 제공되는 응답의 일부로 일종의 추론을 제공하는 것.

### 추론 모델은 언제 사용해야 할까요?

이제 추론 모델(reasoning models)을 정의했으니, 더 흥미로운 부분인 추론 작업을 위한 LLM을 구축하고 개선하는 방법으로 넘어갈 수 있습니다. 하지만 기술적인 세부 사항에 들어가기 전에, 추론 모델이 실제로 언제 필요한지 고려하는 것이 중요합니다.

**추론 모델은 언제 필요할까요?**

추론 모델은 퍼즐 풀기, 고급 수학 문제, 어려운 코딩 작업과 같은 복잡한 작업에 능숙하도록 설계되었습니다. 그러나 요약, 번역 또는 지식 기반 질문 답변과 같은 간단한 작업에는 필요하지 않습니다. 사실, 모든 것에 추론 모델을 사용하는 것은 비효율적이고 비용이 많이 들 수 있습니다. 예를 들어, 추론 모델은 일반적으로 사용 비용이 더 비싸고, 더 장황하며, 때로는 "과도한 생각(overthinking)"으로 인해 오류가 발생하기 쉽습니다. 여기에도 간단한 규칙이 적용됩니다: 작업에 적합한 도구(또는 LLM 유형)를 사용하십시오.

추론 모델의 주요 강점과 한계는 아래 그림에 요약되어 있습니다.

**추론 모델의 주요 강점과 약점.**

### DeepSeek 학습 파이프라인(training pipeline)에 대한 간략한 살펴보기

다음 섹션에서 추론 모델을 구축하고 개선하는 네 가지 주요 접근 방식을 논의하기 전에, DeepSeek R1 기술 보고서에 설명된 DeepSeek R1 파이프라인(pipeline)을 간략하게 설명하고자 합니다. 이 보고서는 흥미로운 사례 연구이자 추론 LLM 개발을 위한 청사진 역할을 합니다.

DeepSeek은 단일 R1 추론 모델을 출시한 것이 아니라 DeepSeek-R1-Zero, DeepSeek-R1, DeepSeek-R1-Distill이라는 세 가지 개별 변형 모델을 소개했습니다. 기술 보고서의 설명을 바탕으로, 저는 이 모델들의 개발 과정을 아래 다이어그램(diagram)에 요약했습니다.

**DeepSeek R1 기술 보고서에서 논의된 DeepSeek의 세 가지 다른 추론 모델 개발 과정.**

다음으로, 위 다이어그램에 표시된 과정을 간략히 살펴보겠습니다. 더 자세한 내용은 다음 섹션에서 다룰 것이며, 거기서 추론 모델을 구축하고 개선하는 네 가지 주요 접근 방식을 논의할 것입니다.

**(1) DeepSeek-R1-Zero:** 이 모델은 2024년 12월에 출시된 671B 사전 학습(pre-trained) DeepSeek-V3 기본 모델(base model)을 기반으로 합니다. 연구팀은 두 가지 유형의 보상(rewards)을 사용하여 RL(강화 학습)로 이를 학습시켰습니다. 이 접근 방식은 일반적으로 RLHF(인간 피드백 기반 강화 학습)의 일부인 SFT(지도 미세 조정) 단계를 포함하지 않았기 때문에 "콜드 스타트(cold start)" 학습이라고 불립니다.

**(2) DeepSeek-R1:** DeepSeek의 주력 추론 모델인 이 모델은 DeepSeek-R1-Zero를 기반으로 구축되었습니다. 개발팀은 추가적인 SFT 단계와 심층적인 RL 학습을 통해 이 모델을 더욱 정제하여, "콜드 스타트(cold-started)" 방식으로 학습된 R1-Zero 모델의 성능을 크게 향상시켰습니다.

**(3) DeepSeek-R1-Distill\*:** 이전 단계에서 생성된 SFT 데이터(data)를 활용하여, DeepSeek 팀은 Qwen 및 Llama 모델을 미세 조정(fine-tuned)함으로써 이들의 추론 능력을 강화했습니다. 전통적인 의미의 지식 증류(distillation)와는 다소 차이가 있지만, 이 과정은 대규모 DeepSeek-R1 671B 모델의 출력(outputs)을 사용하여 더 작은 모델(Llama 8B 및 70B, Qwen 1.5B–30B)을 학습시키는 것을 포함했습니다.

### 추론 모델을 구축하고 개선하는 4가지 주요 방법

이 섹션에서는 LLM의 추론 능력을 향상시키고 DeepSeek-R1, OpenAI의 o1 및 o3와 같은 전문화된 추론 모델을 구축하는 데 현재 사용되는 주요 기술을 설명할 것입니다.

**참고:** o1과 o3의 정확한 작동 방식은 OpenAI 외부에서는 알려져 있지 않습니다. 하지만 추론(inference) 및 학습(training) 기술의 조합을 활용하는 것으로 알려져 있습니다.

#### 1) 추론 시간 스케일링(Inference-time scaling)

LLM의 추론 능력을 개선하는 한 가지 방법은 추론 시간 스케일링(inference-time scaling)입니다. 이 용어는 여러 의미를 가질 수 있지만, 여기서는 추론(inference) 과정에서 계산 자원(computational resources)을 더 많이 투입하여 모델의 출력 품질을 향상시키는 전략을 의미합니다. 대략적인 비유는 인간이 복잡한 문제에 대해 더 오래 숙고할 시간을 가질 때 더 나은 해결책을 내놓는 경향이 있다는 것입니다. 이와 유사하게, LLM이 답변을 생성하는 동안 더 깊이 "생각"하도록 유도하는 기술들을 적용할 수 있습니다. (물론 LLM이 실제로 "생각"하는지는 별개의 논의입니다.)

추론 시간 스케일링(inference-time scaling)에 대한 한 가지 간단한 접근 방식은 영리한 프롬프트 엔지니어링(prompt engineering)입니다. 고전적인 예는 CoT(사고의 사슬) 프롬프팅(chain-of-thought prompting)으로, "단계별로 생각하세요(think step by step)"와 같은 문구가 입력 프롬프트(input prompt)에 포함됩니다. 이는 모델이 최종 답변으로 바로 넘어가지 않고 중간 추론 단계(intermediate reasoning steps)를 생성하도록 장려하며, 이는 종종(항상 그런 것은 아니지만) 더 복잡한 문제에서 더 정확한 결과를 가져올 수 있습니다. (이 전략을 "프랑스의 수도는 어디인가요?"와 같은 간단한 지식 기반 질문에 사용하는 것은 의미가 없다는 점에 유의하십시오. 이는 주어진 입력 쿼리(input query)에 추론 모델이 적합한지 여부를 파악하는 좋은 경험칙이기도 합니다.)

**2022년 논문 "Large Language Models are Zero-Shot Reasoners"에서 발췌한 고전적인 CoT 프롬프팅(CoT prompting) 예시 (https://arxiv.org/abs/2205.11916).**

앞서 언급한 CoT 접근 방식은 더 많은 출력 토큰(output tokens)을 생성함으로써 추론(inference) 비용을 증가시키기 때문에 추론 시간 스케일링(inference-time scaling)으로 볼 수 있습니다.

추론 시간 스케일링(inference-time scaling)의 또 다른 접근 방식은 투표 및 검색 전략(voting and search strategies)을 사용하는 것입니다. 한 가지 간단한 예는 다수결 투표(majority voting)로, LLM이 여러 답변을 생성하게 하고 다수결 투표를 통해 올바른 답변을 선택합니다. 마찬가지로, 빔 탐색(beam search) 및 기타 검색 알고리즘(search algorithms)을 사용하여 더 나은 응답을 생성할 수 있습니다. 이러한 다양한 전략에 대한 자세한 내용은 제가 이전 "2024년 주목할 만한 AI 연구 논문 (2부)" 기사에서 설명했던 "Scaling LLM Test-Time Compute Optimally can be More Effective than Scaling Model Parameters" 논문을 강력히 추천합니다.

**다양한 검색 기반 방법은 최적의 답변을 선택하기 위해 프로세스 보상 기반 모델(process-reward-based model)에 의존합니다.**

**LLM Test-Time Compute 논문에서 발췌한 주석이 달린 그림, https://arxiv.org/abs/2408.03314**

DeepSeek R1 기술 보고서는 일반적인 추론 시간 스케일링(inference-time scaling) 방법(예: 프로세스 보상 모델 기반 및 몬테카를로 트리 탐색 기반 접근 방식)을 "실패한 시도(unsuccessful attempts)"로 분류합니다. 이는 DeepSeek이 R1 모델의 긴 응답 생성 경향을 넘어 이러한 기술을 명시적으로 사용하지 않았음을 시사하며, 이는 V3 기본 모델(base model)에 비해 암묵적인 형태의 추론 시간 스케일링(inference-time scaling) 역할을 합니다. 하지만 명시적인 추론 시간 스케일링(inference-time scaling)은 LLM 자체 내에서보다는 애플리케이션 계층(application layer)에서 구현되는 경우가 많으므로, DeepSeek은 여전히 앱 내에서 이러한 기술을 적용할 수 있습니다. 최신 LLM 개발 동향을 보면, 복잡한 추론 작업을 위해 내부적으로 여러 번의 추론 단계(multi-step reasoning)를 거치거나, 외부 도구 사용(tool use)과 결합하여 추론 능력을 강화하는 방식이 점차 보편화되고 있습니다.

저는 OpenAI의 o1 및 o3 모델이 추론 시간 스케일링(inference-time scaling)을 적극적으로 사용한다고 생각하며, 이는 GPT-4o와 같은 모델에 비해 토큰(token)당 비용이 상대적으로 비싼 이유를 설명할 수 있습니다. 추론 시간 스케일링(inference-time scaling) 외에도 o1과 o3는 DeepSeek R1에 사용된 것과 유사한 RL 파이프라인(pipelines)을 사용하여 학습되었을 가능성이 높습니다. 강화 학습(reinforcement learning)에 대한 자세한 내용은 아래 다음 두 섹션에서 다루겠습니다.

#### 2) 순수 강화 학습(Pure reinforcement learning, RL)

DeepSeek R1 논문에서 제가 개인적으로 주목한 점 중 하나는 순수 RL(강화 학습)로부터 추론이 행동으로 나타난다는 그들의 발견입니다. 이것이 무엇을 의미하는지 더 자세히 살펴보겠습니다.

앞서 설명했듯이, DeepSeek은 세 가지 유형의 R1 모델을 개발했습니다. 첫 번째인 DeepSeek-R1-Zero는 2024년 12월에 출시된 표준 사전 학습(pre-trained) LLM인 DeepSeek-V3 기본 모델(base model)을 기반으로 구축되었습니다. RL(강화 학습) 전에 SFT(지도 미세 조정)가 적용되는 일반적인 RL 파이프라인(pipelines)과 달리, DeepSeek-R1-Zero는 아래 다이어그램에 강조된 것처럼 초기 SFT 단계 없이 오직 강화 학습(reinforcement learning)으로만 학습되었습니다.

**DeepSeek-R1-Zero 모델의 개발 과정.**

그럼에도 불구하고, 이 RL 과정은 일반적으로 LLM의 선호도 조정(preference-tune)에 적용되는 RLHF(인간 피드백 기반 강화 학습) 접근 방식과 유사합니다. (저는 제 기사 "LLM 학습: RLHF와 그 대안"에서 RLHF를 더 자세히 다루었습니다.) 하지만 위에서 언급했듯이, DeepSeek-R1-Zero의 주요 차이점은 명령어 조정(instruction tuning)을 위한 SFT(지도 미세 조정) 단계를 건너뛰었다는 것입니다. 이것이 그들이 이를 "순수(pure)" RL이라고 부르는 이유입니다. (하지만 LLM 맥락에서의 RL은 전통적인 RL과는 상당히 다르며, 이는 나중에 다룰 주제입니다.)

보상(rewards)의 경우, 인간의 선호도에 따라 학습된 보상 모델(reward model)을 사용하는 대신, 정확도 보상(accuracy reward)과 형식 보상(format reward)이라는 두 가지 유형의 보상을 사용했습니다. 정확도 보상(accuracy reward)은 LeetCode 컴파일러(compiler)를 사용하여 코딩 답변을 검증하고, 확정적 시스템(deterministic system)을 사용하여 수학적 응답을 평가합니다. 형식 보상(format reward)은 LLM 심사관(judge)에 의존하여 응답이 `<think>` 태그(tags) 안에 추론 단계(reasoning steps)를 배치하는 것과 같은 예상 형식을 따르도록 합니다.

놀랍게도, 이 접근 방식은 LLM이 기본적인 추론 기술을 개발하기에 충분했습니다. 연구원들은 아래 그림에 표시된 것처럼, 모델이 명시적으로 훈련되지 않았음에도 불구하고 응답의 일부로 추론 흔적(reasoning traces)을 생성하기 시작하는 "아하!(Aha!)" 순간을 관찰했습니다.

**DeepSeek R1 기술 보고서에서 발췌한 "아하(Aha)" 순간의 출현을 보여주는 그림 (https://arxiv.org/abs/2501.12948).**

R1-Zero는 최고 성능의 추론 모델은 아니지만, 위 그림에 표시된 것처럼 중간 "생각" 단계(intermediate "thinking" steps)를 생성함으로써 추론 능력을 보여줍니다. 이는 순수 RL(강화 학습)을 사용하여 추론 모델을 개발하는 것이 가능함을 확인시켜주며, DeepSeek 팀이 이 접근 방식을 처음으로 시연(또는 최소한 발표)했습니다.

Ahead of AI는 독자 지원 출판물입니다. 새로운 게시물을 받고 제 작업을 지원하려면 무료 또는 유료 구독자가 되는 것을 고려해 보세요.
[Subscribe]

#### 3) 지도 미세 조정(Supervised finetuning, SFT) 및 강화 학습(reinforcement learning, RL)

다음으로, 추론 모델 구축을 위한 청사진 역할을 하는 DeepSeek의 주력 추론 모델인 DeepSeek-R1의 개발을 살펴보겠습니다. 이 모델은 추가 SFT(지도 미세 조정) 및 RL(강화 학습)을 통합하여 추론 성능을 향상시킴으로써 DeepSeek-R1-Zero를 개선합니다. 표준 RLHF(인간 피드백 기반 강화 학습) 파이프라인(pipeline)에서 볼 수 있듯이, RL 전에 SFT 단계를 포함하는 것이 실제로 일반적이라는 점에 유의하십시오. OpenAI의 o1도 유사한 접근 방식을 사용하여 개발되었을 가능성이 높습니다.

**DeepSeek-R1 모델의 개발 과정.**

위 다이어그램에 표시된 것처럼, DeepSeek 팀은 DeepSeek-R1-Zero를 사용하여 그들이 "콜드 스타트(cold-start)" SFT 데이터(data)라고 부르는 것을 생성했습니다. "콜드 스타트(cold start)"라는 용어는 이 데이터가 DeepSeek-R1-Zero에 의해 생성되었으며, DeepSeek-R1-Zero 자체는 어떤 SFT(지도 미세 조정) 데이터로도 학습되지 않았다는 사실을 의미합니다. 이 콜드 스타트 SFT 데이터(data)를 사용하여 DeepSeek은 명령어 미세 조정(instruction fine-tuning)을 통해 모델을 학습시킨 다음, 또 다른 RL(강화 학습) 단계를 거쳤습니다. 이 RL 단계는 DeepSeek-R1-Zero의 RL 과정에서 사용된 것과 동일한 정확도 및 형식 보상(rewards)을 유지했습니다. 하지만 그들은 언어 혼합(language mixing)을 방지하기 위해 일관성 보상(consistency reward)을 추가했습니다. 언어 혼합은 모델이 응답 내에서 여러 언어 사이를 전환할 때 발생합니다.

RL 단계 다음에는 또 다른 SFT 데이터(data) 수집 라운드가 이어졌습니다. 이 단계에서는 가장 최근의 모델 체크포인트(checkpoint)가 60만 개의 CoT(사고의 사슬) SFT 예시를 생성하는 데 사용되었고, 추가로 20만 개의 지식 기반 SFT 예시는 DeepSeek-V3 기본 모델(base model)을 사용하여 생성되었습니다. 이 60만 + 20만 개의 SFT 샘플(samples)은 최종 RL(강화 학습) 라운드를 진행하기 전에 DeepSeek-V3 기본 모델(base model)의 명령어 미세 조정(instruction-finetuning)에 사용되었습니다. 이 단계에서는 수학 및 코딩 질문에 대한 정확도 보상(accuracy rewards)에 규칙 기반 방법(rule-based methods)을 다시 사용했으며, 다른 질문 유형에는 인간 선호도 레이블(human preference labels)이 사용되었습니다. 종합적으로 볼 때, 이는 SFT 데이터(data)에 (더 많은) CoT 예시가 포함되어 있다는 점을 제외하면 일반적인 RLHF(인간 피드백 기반 강화 학습)와 매우 유사합니다. 그리고 RL은 인간 선호도 기반 보상(rewards) 외에 검증 가능한 보상(verifiable rewards)을 가집니다.

최종 모델인 DeepSeek-R1은 아래 표에 표시된 것처럼 추가 SFT(지도 미세 조정) 및 RL(강화 학습) 단계 덕분에 DeepSeek-R1-Zero보다 눈에 띄는 성능 향상을 보입니다.

**OpenAI O1 및 DeepSeek R1 모델의 벤치마크(benchmark) 비교.**

**DeepSeek-R1 기술 보고서에서 발췌한 주석이 달린 그림 (https://arxiv.org/abs/2501.12948).**

#### 4) 순수 지도 미세 조정(Pure supervised finetuning, SFT) 및 증류(distillation)

지금까지 추론 모델을 구축하고 개선하는 세 가지 주요 접근 방식을 다루었습니다:

1.  **추론 시간 스케일링(inference-time scaling)**: 기본 모델을 학습시키거나 수정하지 않고 추론 능력을 향상시키는 기술.
2.  **DeepSeek-R1-Zero에서와 같은 순수 RL(강화 학습)**: 지도 미세 조정(supervised fine-tuning) 없이도 추론이 학습된 행동으로 나타날 수 있음을 보여주었습니다.
3.  **SFT(지도 미세 조정) + RL(강화 학습)**: DeepSeek의 주력 추론 모델인 DeepSeek-R1로 이어졌습니다.

그렇다면 남은 것은 무엇일까요? 모델 "증류(distillation)"입니다. 놀랍게도 DeepSeek은 그들이 증류(distillation)라고 부르는 과정을 통해 학습된 더 작은 모델들도 출시했습니다. 하지만 LLM 맥락에서 증류(distillation)는 딥러닝(deep learning)에서 사용되는 고전적인 지식 증류(knowledge distillation) 접근 방식을 반드시 따르지는 않습니다. 전통적으로 지식 증류(knowledge distillation)에서는(제 책 "Machine Learning Q and AI" 6장에서 간략히 설명했듯이) 더 작은 학생 모델(student model)이 더 큰 교사 모델(teacher model)의 로짓(logits)과 목표 데이터셋(target dataset) 모두에서 학습됩니다. 대신, 여기에서 증류(distillation)는 더 큰 LLM이 생성한 SFT 데이터셋(dataset)을 사용하여 Llama 8B 및 70B, Qwen 2.5 모델(0.5B ~ 32B)과 같은 더 작은 LLM을 명령어 미세 조정(instruction fine-tuning)하는 것을 의미합니다. 구체적으로, 이러한 더 큰 LLM은 DeepSeek-V3와 DeepSeek-R1의 중간 체크포인트(checkpoint)입니다. 사실, 이 증류(distillation) 과정에 사용된 SFT 데이터(data)는 이전 섹션에서 설명한 DeepSeek-R1을 학습시키는 데 사용된 것과 동일한 데이터셋(dataset)입니다.

이 과정을 명확히 하기 위해, 아래 다이어그램(diagram)에서 증류(distillation) 부분을 강조했습니다.

**DeepSeek-R1-Distill 모델의 개발 과정.**

그들은 왜 이러한 증류 모델(distilled models)을 개발했을까요? 제 생각에는 두 가지 주요 이유가 있습니다:

1.  **더 작은 모델은 더 효율적입니다.** 이는 실행 비용이 저렴할 뿐만 아니라, 저사양 하드웨어(hardware)에서도 실행될 수 있어 저와 같은 많은 연구자와 실험가들에게 특히 흥미롭습니다.
2.  **순수 SFT(지도 미세 조정)의 사례 연구.** 이 증류 모델(distilled models)은 강화 학습(reinforcement learning) 없이 순수 SFT(지도 미세 조정)가 모델을 얼마나 발전시킬 수 있는지 보여주는 흥미로운 벤치마크(benchmark) 역할을 합니다.

아래 표는 이 증류 모델(distilled models)의 성능을 다른 인기 모델뿐만 아니라 DeepSeek-R1-Zero 및 DeepSeek-R1과 비교합니다.

**증류 모델(distilled models)과 비증류 모델(non-distilled models)의 벤치마크(benchmark) 비교.**

**DeepSeek-R1 기술 보고서에서 발췌한 주석이 달린 그림 (https://arxiv.org/abs/2501.12948).**

보시다시피, 증류 모델(distilled models)은 DeepSeek-R1보다 눈에 띄게 약하지만, DeepSeek-R1-Zero에 비해서는 훨씬 작음에도 불구하고 놀랍도록 강력합니다. 또한 이 모델들이 o1 mini와 비교하여 얼마나 잘 작동하는지 주목하는 것도 흥미롭습니다(저는 o1-mini 자체가 o1의 유사한 증류 버전일 수 있다고 생각합니다).

이 섹션을 결론으로 마무리하기 전에, 언급할 가치가 있는 또 다른 흥미로운 비교가 있습니다. DeepSeek 팀은 DeepSeek-R1-Zero에서 나타난 발현적 추론 행동(emergent reasoning behavior)이 더 작은 모델에서도 나타날 수 있는지 테스트했습니다. 이를 조사하기 위해, 그들은 DeepSeek-R1-Zero의 동일한 순수 RL(강화 학습) 접근 방식을 Qwen-32B에 직접 적용했습니다. 이 실험 결과는 아래 표에 요약되어 있으며, QwQ-32B-Preview는 Qwen 팀이 개발한 Qwen 2.5 32B 기반의 참조 추론 모델(reference reasoning model) 역할을 합니다(학습 세부 사항은 공개되지 않은 것으로 생각됩니다). 이 비교는 순수 RL(강화 학습)만으로 DeepSeek-R1-Zero보다 훨씬 작은 모델에서 추론 능력을 유도할 수 있는지에 대한 추가적인 통찰력을 제공합니다.

**더 작은 32B 모델에 대한 증류(distillation) 및 RL(강화 학습) 벤치마크(benchmark) 비교.**

**DeepSeek-R1 기술 보고서에서 발췌한 주석이 달린 그림 (https://arxiv.org/abs/2501.12948).**

흥미롭게도, 결과는 증류(distillation)가 더 작은 모델에 대해 순수 RL(강화 학습)보다 훨씬 더 효과적임을 시사합니다. 이는 RL(강화 학습)만으로는 이 규모의 모델에서 강력한 추론 능력을 유도하기에 충분하지 않을 수 있으며, 고품질 추론 데이터(data)에 대한 SFT(지도 미세 조정)가 작은 모델로 작업할 때 더 효과적인 전략이 될 수 있다는 생각과 일치합니다.

완전성을 위해, 표에 추가적인 비교를 포함했으면 유용했을 것입니다:

1.  DeepSeek-R1이 개발된 방식과 유사하게 SFT(지도 미세 조정) + RL(강화 학습)로 학습된 Qwen-32B. 이는 RL이 SFT와 결합될 때 순수 RL 및 순수 SFT와 비교하여 얼마나 많은 개선이 이루어질 수 있는지 판단하는 데 도움이 될 것입니다.
2.  증류 모델(distilled models)이 생성된 방식과 유사하게 순수 SFT(지도 미세 조정)로 학습된 DeepSeek-V3. 이는 RL + SFT가 순수 SFT보다 얼마나 효과적인지 직접 비교할 수 있게 해줄 것입니다.

Ahead of AI는 독자 지원 출판물입니다. 새로운 게시물을 받고 제 작업을 지원하려면 무료 또는 유료 구독자가 되는 것을 고려해 보세요.
[Subscribe]

### 결론

이 섹션에서는 추론 모델을 구축하고 개선하기 위한 네 가지 다른 전략을 살펴보았습니다:

1.  **추론 시간 스케일링(inference-time scaling)**은 추가 학습이 필요 없지만 추론 비용을 증가시켜 사용자 수 또는 쿼리(query) 볼륨이 증가함에 따라 대규모 배포(deployment)를 더 비싸게 만듭니다. 그럼에도 불구하고, 이미 강력한 모델의 성능을 향상시키는 데는 당연한 선택입니다. 저는 o1이 추론 시간 스케일링(inference-time scaling)을 활용한다고 강력히 의심하며, 이는 DeepSeek-R1에 비해 토큰(token)당 비용이 더 비싼 이유를 설명하는 데 도움이 됩니다.
2.  **순수 RL(강화 학습)**은 추론이 발현적 행동(emergent behavior)으로 나타나는 것에 대한 통찰력을 제공하기 때문에 연구 목적으로 흥미롭습니다. 하지만 실제 모델 개발에서는 RL + SFT가 더 강력한 추론 모델로 이어지므로 선호되는 접근 방식입니다. 저는 o1도 RL + SFT를 사용하여 학습되었다고 강력히 의심합니다. 더 정확히 말하면, o1은 DeepSeek-R1보다 약하고 작은 기본 모델(base model)에서 시작하지만, RL + SFT와 추론 시간 스케일링(inference-time scaling)으로 이를 보완한다고 생각합니다.
3.  위에서 언급했듯이, **RL + SFT**는 고성능 추론 모델을 구축하기 위한 핵심 접근 방식입니다. DeepSeek-R1은 이를 수행하는 방법을 보여주는 좋은 청사진입니다.
4.  **증류(distillation)**는 특히 더 작고 효율적인 모델을 만드는 데 매력적인 접근 방식입니다. 하지만 증류(distillation)는 혁신을 주도하거나 차세대 추론 모델을 생산하지 못한다는 한계가 있습니다. 예를 들어, 증류(distillation)는 항상 SFT(지도 미세 조정) 데이터(data)를 생성하기 위해 기존의 더 강력한 모델에 의존합니다.

다음으로 제가 기대하는 흥미로운 측면 중 하나는 RL + SFT(접근 방식 3)를 추론 시간 스케일링(inference-time scaling, 접근 방식 1)과 결합하는 것입니다. 이것이 OpenAI o1이 하고 있는 일일 가능성이 높습니다. 단, DeepSeek-R1보다 약한 기본 모델(base model)을 기반으로 할 가능성이 있으며, 이는 DeepSeek-R1이 추론 시간(inference time)에 상대적으로 저렴하면서도 왜 그렇게 좋은 성능을 보이는지 설명합니다.

### DeepSeek R1에 대한 생각

최근 몇 주 동안 많은 사람들이 DeepSeek-R1 모델에 대한 제 생각을 물어왔습니다. 간단히 말해, 저는 그것들이 대단한 성과라고 생각합니다. 연구 엔지니어(research engineer)로서, 저는 그들의 방법론(methodology)에 대한 통찰력을 제공하여 제가 배울 수 있는 상세한 기술 보고서(technical report)를 특히 높이 평가합니다. 가장 흥미로운 점 중 하나는 순수 RL(강화 학습)로부터 추론이 행동으로 어떻게 나타났는지입니다. 그리고 DeepSeek이 Meta의 Llama 모델보다도 제한이 적은 허용적인 오픈 소스(open-source) MIT 라이선스(license) 하에 모델을 오픈 소스화(open-sourced)했다는 점은 인상적입니다.

**o1과 비교하면 어떤가요?**

DeepSeek-R1이 o1보다 더 나은가요? 저는 대략 비슷한 수준이라고 말하고 싶습니다. 하지만 눈에 띄는 점은 DeepSeek-R1이 추론 시간(inference time)에 더 효율적이라는 것입니다. 이는 DeepSeek이 학습 과정(training process)에 더 많은 투자를 했을 가능성이 높고, OpenAI는 o1에 대해 추론 시간 스케일링(inference-time scaling)에 더 의존했을 수 있음을 시사합니다.

그렇긴 하지만, OpenAI가 o1에 대해 많은 것을 공개하지 않았기 때문에 o1과 DeepSeek-R1을 직접 비교하기는 어렵습니다. 예를 들어, 우리는 다음을 알지 못합니다:

*   o1도 MoE(전문가 혼합)인가요?
*   o1은 얼마나 큰가요?
*   o1이 최소한의 RL + SFT와 광범위한 추론 시간 스케일링(inference-time scaling)만 적용된 GPT-4o의 약간 개선된 버전일 수 있을까요?

이러한 세부 사항을 알지 못하면, 직접적인 비교는 여전히 동떨어진 비교(apples-to-oranges comparison)로 남습니다.

### DeepSeek-R1 학습 비용

또 다른 논의점은 DeepSeek-R1 개발 비용이었습니다. 일부는 약 600만 달러의 학습 비용을 언급했지만, 그들은 DeepSeek-V3(작년 12월에 출시된 기본 모델(base model))와 DeepSeek-R1을 혼동했을 가능성이 높습니다. 600만 달러 추정치는 GPU 시간당 2달러와 2024년 12월에 처음 논의되었던 DeepSeek-V3의 최종 학습 실행에 필요한 GPU 시간 수를 가정한 것입니다. 하지만 DeepSeek 팀은 R1의 정확한 GPU 시간이나 개발 비용을 공개한 적이 없으므로, 모든 비용 추정치는 순전히 추측에 불과합니다.

어느 쪽이든, 궁극적으로 DeepSeek-R1은 오픈 웨이트(open-weight) 추론 모델의 주요 이정표이며, 추론 시간(inference time)에서의 효율성은 OpenAI의 o1에 대한 흥미로운 대안이 됩니다.

### 제한된 예산으로 추론 모델 개발하기

DeepSeek-V3와 같은 오픈 웨이트(open-weight) 기본 모델(base model)로 시작하더라도 DeepSeek-R1 수준의 추론 모델을 개발하려면 수십만에서 수백만 달러가 필요할 가능성이 높습니다. 이는 제한된 예산으로 일하는 연구원이나 엔지니어에게는 낙담스러울 수 있습니다.

**좋은 소식: 증류(distillation)는 많은 것을 할 수 있습니다.**

다행히도 모델 증류(model distillation)는 더 비용 효율적인 대안을 제공합니다. DeepSeek 팀은 DeepSeek-R1보다 훨씬 작음에도 불구하고 놀랍도록 강력한 추론 성능을 달성하는 R1 증류 모델(distilled models)로 이를 입증했습니다. 하지만 이 접근 방식조차 완전히 저렴하지는 않습니다. 그들의 증류(distillation) 과정은 80만 개의 SFT 샘플(samples)을 사용했으며, 이는 상당한 계산(compute)을 필요로 합니다.

흥미롭게도, DeepSeek-R1이 출시되기 며칠 전, 저는 Sky-T1에 대한 기사를 접했습니다. 이는 소규모 팀이 단 1만 7천 개의 SFT 샘플(samples)만을 사용하여 오픈 웨이트(open-weight) 32B 모델을 학습시킨 매력적인 프로젝트였습니다. 총 비용은? 단 450달러로, 대부분의 AI 컨퍼런스(conferences) 등록비보다 적습니다. 이러한 사례는 대규모 학습(training)은 여전히 상당한 비용을 요구하지만, 더욱 작고 특정 목표에 집중하는 미세 조정(fine-tuning) 노력은 훨씬 적은 비용으로도 인상적인 결과를 만들어낼 수 있음을 명확히 보여줍니다.

**"Sky-T1: Train your own O1 preview model within $450" 기사에서 발췌한 그림, https://novasky-ai.github.io/posts/sky-t1/**

그들의 벤치마크(benchmarks)에 따르면, Sky-T1은 o1과 거의 동등한 성능을 보이며, 낮은 학습 비용을 고려할 때 인상적입니다.

**예산 내 순수 RL(강화 학습): TinyZero**

Sky-T1이 모델 증류(model distillation)에 중점을 두는 동안, 저는 "순수 RL(강화 학습)" 분야에서도 흥미로운 작업을 접했습니다. 주목할 만한 예시 중 하나는 DeepSeek-R1-Zero 접근 방식을 재현한 3B 파라미터(parameter) 모델인 TinyZero입니다(참고: 학습 비용은 30달러 미만입니다). 놀랍게도, 단 3B 파라미터(parameters)임에도 불구하고 TinyZero는 일부 발현적 자기 검증 능력(emergent self-verification abilities)을 보여주며, 이는 작은 모델에서도 순수 RL(강화 학습)을 통해 추론이 나타날 수 있다는 생각을 뒷받침합니다. TinyZero 저장소(repository)는 연구 보고서가 아직 진행 중이라고 언급하며, 저는 추가 세부 사항을 계속 주시할 것입니다.

**TinyZero 저장소(https://github.com/Jiayi-Pan/TinyZero)에서 발췌한 모델이 자기 검증(self-verification)이 가능함을 보여주는 그림. (비교를 위해 기본 모델(base model)의 응답을 보는 것도 흥미로웠을 것입니다.)**

위에서 언급된 두 프로젝트는 제한된 예산으로도 추론 모델에 대한 흥미로운 작업이 가능하다는 것을 보여줍니다. 두 접근 방식 모두 DeepSeek-R1의 방법을 재현하지만, 하나는 순수 RL(강화 학습)(TinyZero)에, 다른 하나는 순수 SFT(지도 미세 조정)(Sky-T1)에 중점을 둡니다. 이러한 아이디어를 어떻게 더 확장할 수 있을지 탐구하는 것은 매우 흥미로울 것입니다.

**전통적인 SFT(지도 미세 조정)를 넘어: 여정 학습(Journey Learning)**

작년에 제가 접한 특히 흥미로운 접근 방식 중 하나는 "O1 Replication Journey: A Strategic Progress Report – Part 1" 논문에 설명되어 있습니다. 제목과는 달리, 이 논문은 실제로 o1을 재현하지 않습니다. 대신, 증류(distillation)(순수 SFT) 과정을 개선하는 다른 방법을 소개합니다. 이 논문의 핵심 아이디어는 "지름길 학습(shortcut learning)"의 대안으로서 "여정 학습(journey learning)"입니다. 지름길 학습(shortcut learning)은 명령어 미세 조정(instruction fine-tuning)에서 모델이 올바른 해결 경로만을 사용하여 학습되는 전통적인 접근 방식을 의미합니다. 반면에 여정 학습(journey learning)은 잘못된 해결 경로도 포함하여 모델이 실수로부터 학습할 수 있도록 합니다. 이 접근 방식은 TinyZero의 순수 RL(강화 학습) 학습에서 관찰된 자기 검증 능력(self-verification abilities)과 어느 정도 관련이 있지만, SFT(지도 미세 조정)를 통해서만 모델을 개선하는 데 중점을 둡니다. 모델을 잘못된 추론 경로와 그 수정 사항에 노출시킴으로써, 여정 학습(journey learning)은 자기 수정 능력(self-correction abilities)을 강화하여 추론 모델을 이 방식으로 더 신뢰할 수 있게 만들 수 있습니다.

**전통적인 지름길 학습(shortcut learning)과 달리 여정 학습(journey learning)은 SFT 데이터(data)에 잘못된 해결 경로를 포함합니다.**

**O1 Replication Journey: A Strategic Progress Report – Part 1에서 발췌한 주석이 달린 그림 (https://arxiv.org/abs/2410.18982)**

이는 특히 RL 기반 접근 방식이 계산적으로 비실용적일 수 있는 저예산 추론 모델 개발에 있어 미래 작업의 흥미로운 방향이 될 수 있습니다. 최근에는 오류 분석 및 수정(error analysis and correction)을 학습 데이터에 통합하는 다양한 접근 방식이 탐구되고 있으며, 이는 모델의 견고성과 추론 신뢰도를 높이는 중요한 열쇠로 작용하고 있습니다. 어쨌든, 추론 모델 분야에서는 현재 많은 흥미로운 작업이 진행 중이며, 앞으로 몇 달 안에 훨씬 더 많은 흥미로운 작업을 보게 될 것이라고 확신합니다!

이 잡지는 개인적인 열정 프로젝트입니다. 저를 지원하고 싶으신 분들은 제 책 "Build a Large Language Model (From Scratch)"을 구매해 주시면 감사하겠습니다. (이 책은 다른 곳에서는 찾아볼 수 없는 수준의 세부 사항으로 LLM이 어떻게 작동하는지 설명하므로, 이 책에서 많은 것을 얻으실 것이라고 확신합니다.)

**"Build a Large Language Model (From Scratch)" 아마존(Amazon)에서 지금 구매 가능**

책을 읽으셨고 잠시 시간을 내주실 수 있다면, 짧은 서평을 남겨주시면 정말 감사하겠습니다. 이는 저희 저자들에게 큰 도움이 됩니다! 여러분의 지원은 큰 의미가 있습니다! 감사합니다!