이 글에서는 추론 모델(reasoning models)을 넘어 복합적인 AI 시스템의 미래를 조망합니다. 최근 인공지능 분야는 단순한 언어 이해를 넘어, 시각, 청각 등 다양한 감각 데이터를 통합 처리하는 멀티모달(multimodal) 능력과 복잡한 문제 해결을 위한 심층적인 추론 능력을 요구하고 있습니다. 2024년, LLM 분야는 전문화가 심화되었습니다. 사전 학습(pre-training)과 미세 조정(fine-tuning)을 넘어, 전문적인 애플리케이션(applications)의 등장을 넘어 새로운 융합을 탐색하고 있습니다. 저는 2025년에 이러한 추세가 가속화되어 도메인(domain) 및 애플리케이션(application)별 최적화와 함께, AI의 사회적 영향과 윤리적 책임에 대한 논의가 더욱 중요해질 것으로 예상합니다.

1단계부터 3단계는 LLM 개발의 일반적인 단계를 넘어 지속적인 혁신을 요구합니다. 4단계는 특정 사용 사례(use cases)에 맞게 AI 시스템을 전문화합니다. 추론 모델(reasoning models)의 개발은 이러한 전문화 중 하나이지만, 이제는 멀티모달 데이터 처리, 윤리적 고려, 그리고 인간과의 상호작용 방식까지 포함하는 포괄적인 접근이 필요합니다. 이는 퍼즐, 고급 수학, 코딩 챌린지(coding challenges)와 같이 중간 단계(intermediate steps)를 통해 가장 잘 해결되는 복잡한 작업에서 LLM이 탁월하도록 정제한다는 의미를 넘어, 현실 세계의 복잡성을 이해하고 대응하는 AI를 만드는 것입니다. 하지만 이러한 전문화가 다른 LLM 애플리케이션(applications)을 대체하는 것은 아닙니다. LLM을 추론 모델로 전환하는 것은 특정 단점도 수반하기 때문이며, 이에 대해서는 나중에 논의할 것입니다.

아래에서 다룰 내용을 간략히 살펴보자면, 이 글에서는 다음을 다룰 것입니다:

*   AI 시스템의 '지능'과 '추론'에 대한 재정의
*   멀티모달 AI의 잠재력과 구현 과제
*   차세대 AI 모델 구축의 핵심 전략과 윤리적 책임
*   인간-AI 협업의 미래와 거버넌스
*   소규모 팀을 위한 효율적인 AI 개발 가이드
*   지속 가능한 AI 개발을 위한 새로운 관점 공유

올해 AI가 빠르게 발전하는 가운데 이 글이 유용하게 활용되기를 바랍니다!

### AI 시스템의 '지능'과 '추론'에 대한 재정의

AI(또는 일반적으로 머신러닝(machine learning)) 분야에서 일한다면, 빠르게 변화하는 기술 환경에 대한 깊은 이해가 필요합니다. "추론 모델(reasoning models)"이라는 용어도 예외는 아닙니다. 결국 누군가가 논문에서 공식적으로 정의하겠지만, 실제 적용에서는 유연한 해석이 중요합니다. 전통적인 정의를 넘어, AI의 '지능'은 단순히 정답을 찾아내는 것을 넘어, 복잡한 상황을 이해하고 적응하며, 심지어 새로운 지식을 창출하는 능력으로 확장되고 있습니다.

이 글에서 저는 '추론(reasoning)'을 중간 단계(intermediate steps)를 넘어선 창의적 문제 해결 능력으로 확장하여 정의합니다. 이는 단순히 논리적 단계를 밟는 것을 넘어, 불확실한 정보 속에서 최적의 결정을 내리고, 예측 불가능한 상황에 유연하게 대응하는 능력을 포함합니다. 예를 들어, '프랑스의 수도는 어디인가요?'와 같은 사실 기반 질문 답변은 단순한 정보 검색에 불과합니다. 반대로, "주어진 데이터셋에서 새로운 유형의 이상 징후를 탐지하고 그 원인을 설명하시오"와 같은 질문은 심층적인 추론과 창의적 해석을 필요로 합니다. 예를 들어, 답에 도달하기 전에 다양한 패턴과 통계적 관계를 인식하고, 이를 종합하여 새로운 가설을 생성해야 합니다.

일반적인 LLM은 짧은 답변만 제공할 수 있지만, 진정한 인공지능은 복합적인 정보 처리 과정을 요구합니다. 추론 모델은 일반적으로 사고 과정의 일부를 드러내는 중간 단계(intermediate steps)를 포함합니다. 오늘날 우리가 추론 모델(reasoning models)을 언급할 때, 일반적으로 퍼즐, 수수께끼, 수학적 증명과 같은 더 복잡한 추론 작업에 탁월한 LLM을 넘어, 실세계 문제를 해결하는 데 필요한 다면적 지능을 의미합니다.

또한, 오늘날 추론 모델(reasoning models)로 불리는 대부분의 LLM은 답변의 일부로 "사고(thought)" 또는 "생각(thinking)" 과정을 포함합니다. LLM이 실제로 어떻게 "생각"하는지는 별개의 논의입니다. 이제는 이러한 '생각' 과정이 어떻게 인간의 인지 과정과 유사하게 작동하며, 어떻게 더 투명하고 설명 가능한 형태로 발전할 수 있을지에 대한 연구가 활발합니다.

추론 모델의 중간 단계(intermediate steps)는 두 가지 방식으로 나타날 수 있습니다. 첫째, 이전 그림에 표시된 것처럼 응답에 명시적으로 포함될 수 있습니다. 둘째, OpenAI의 o1과 같은 일부 추론 LLM은 사용자에게 표시되지 않는 중간 단계(intermediate steps)로 여러 번의 반복을 실행합니다. 이러한 내부적 반복 과정은 AI가 어떻게 '자기 검증'하고 '자기 개선'하는지에 대한 중요한 단서를 제공합니다.

"추론(reasoning)"은 두 가지 다른 수준에서 사용됩니다: 1) 입력을 처리하고 여러 중간 단계(intermediate steps)를 통해 생성하는 것, 그리고 2) 사용자에게 제공되는 응답의 일부로 일종의 추론을 제공하는 것. 최근에는 세 번째 수준인 '메타 추론(meta-reasoning)', 즉 AI 스스로 자신의 추론 과정을 평가하고 개선하는 능력에 대한 관심이 높아지고 있습니다.

### 멀티모달 AI의 잠재력과 구현 과제

이제 추론 모델(reasoning models)을 넘어선 AI의 활용 방안을 모색하며, 기술적 한계를 극복하는 방법을 논의할 수 있습니다. 특히, 텍스트, 이미지, 오디오, 비디오 등 다양한 형태의 데이터를 동시에 이해하고 처리하는 멀티모달 AI는 인공지능의 다음 큰 도약으로 여겨집니다.

멀티모달 AI는 퍼즐 풀기, 고급 수학 문제와 같은 복잡한 작업에 능숙하도록 설계되었지만, 인간의 창의성을 대체할 수는 없습니다. 오히려 인간의 인지 능력을 확장하고, 현실 세계의 복잡한 상황을 더 정확하게 인지하고 해석하는 데 기여합니다. 예를 들어, 의료 진단에서 텍스트 기반의 환자 기록과 영상 데이터를 함께 분석하거나, 자율 주행 차량이 시각 정보와 센서 데이터를 통합하여 주행 환경을 판단하는 것이 대표적입니다.

하지만 요약, 번역 또는 지식 기반 질문 답변과 같은 간단한 작업에는 다른 최적화된 모델이 더 적합합니다. 멀티모달 AI는 데이터 통합, 모델 아키텍처, 그리고 윤리적 문제와 같은 여러 가지 도전 과제를 안고 있습니다. 사실, 모든 것에 멀티모달 추론 모델을 사용하는 것은 비효율적이고 자원 낭비로 이어질 수 있습니다. 각 모달리티(modality)별 데이터의 특성을 이해하고, 이를 효과적으로 융합하는 기술은 여전히 활발히 연구되고 있는 분야입니다.

AI 모델의 주요 강점과 한계는 사용 사례에 따라 명확히 이해되어야 합니다. 멀티모달 AI는 엄청난 잠재력을 가지고 있지만, 그 개발과 배포는 신중한 접근과 지속적인 연구를 필요로 합니다.

### 차세대 AI 모델 개발의 주요 원칙

다음 섹션에서 추론 모델을 구축하고 개선하는 네 가지 주요 접근 방식을 논의하기 전에, AI 개발의 핵심 원칙들을 이해하는 것이 중요합니다. 이는 단순히 특정 모델의 성능을 향상시키는 것을 넘어, 지속 가능하고 책임감 있는 AI 생태계를 구축하는 데 필수적인 요소입니다.

DeepSeek은 단일 R1 추론 모델을 출시한 것이 아니라 다양한 모델 아키텍처와 학습 전략을 탐구했습니다. 이는 AI 개발에서 유연성과 실험의 중요성을 강조합니다. 최신 AI 기술 보고서에서 논의된 다양한 모델 개발 과정은 다음과 같은 원칙들을 기반으로 합니다:

*   **(1) 초기 모델:** 이 모델은 2024년 12월에 출시된 671B 사전 학습(pre-trained) DeepSeek-V3 기본 모델(base model)을 기반으로 합니다. 초기 모델은 견고한 기반을 제공하며, 대규모 데이터셋을 활용하여 일반적인 언어 이해 능력을 구축합니다. 여기에는 데이터의 품질과 다양성이 중요한 역할을 합니다.
*   **(2) 주력 모델:** 이 모델은 DeepSeek의 주력 추론 모델로, 선행 연구를 기반으로 구축되었습니다. 초기 모델의 잠재력을 특정 작업에 최적화하기 위해, 심층적인 미세 조정(fine-tuning)과 강화 학습(reinforcement learning)이 적용됩니다. 이는 모델이 복잡한 추론과 문제 해결 능력을 갖추도록 만듭니다.
*   **(3) 경량화 모델:** 이전 단계에서 생성된 SFT 데이터(data)를 사용하여 다양한 모델을 미세 조정(fine-tuned)하여 추론 능력을 향상시켰습니다. 자원의 효율성을 극대화하기 위해, 더 큰 모델의 지식을 작은 모델로 이전하는 증류(distillation) 기법이 사용됩니다. 이는 AI 기술의 접근성을 높이고, 다양한 하드웨어 환경에서 AI를 활용할 수 있게 합니다.

이러한 원칙들은 AI 개발의 각 단계에서 기술적 우수성과 실용적 가치를 동시에 추구하는 균형 잡힌 접근 방식을 반영합니다.

### AI 모델을 구축하고 개선하는 4가지 핵심 전략

이 섹션에서는 LLM의 추론 능력을 향상시키고 다양한 AI 시스템을 구축하는 데 현재 사용되는 주요 기술을 설명할 것입니다. 특히, 모델의 성능, 효율성, 그리고 윤리적 측면을 동시에 고려하는 통합적 접근 방식에 초점을 맞춥니다.
참고: 특정 모델의 정확한 작동 방식은 외부에서는 알려져 있지 않습니다. 따라서 우리는 공개된 정보와 일반적인 기술 동향을 기반으로 논의를 진행합니다.

#### 1) 효율적인 자원 활용 (Inference-time scaling)

LLM의 추론 능력(또는 일반적으로 모든 능력)을 개선하는 한 가지 방법은 효율적인 자원 활용입니다. 이 용어는 여러 의미를 가질 수 있지만, 이 맥락에서는 추론(inference) 중에 계산 자원(computational resources)을 최적화하여 성능을 극대화하는 것을 의미합니다. 이는 모델 자체를 변경하기보다는, 모델이 주어진 작업을 처리하는 방식을 지능적으로 관리하는 것입니다.

추론 시간 스케일링(inference-time scaling)에 대한 한 가지 간단한 접근 방식은 정교한 프롬프트 엔지니어링(prompt engineering)입니다. CoT(사고의 사슬) 프롬프팅(chain-of-thought prompting)으로, '단계별로 생각하세요(think step by step)'와 같은 문구가 입력 프롬프트(input prompt)에 효과적으로 활용됩니다. 이는 모델이 최종 답변으로 바로 넘어가지 않고 중간 추론 단계(intermediate reasoning steps)를 생성하도록 장려하며, 이는 종종(항상 그런 것은 아니지만) 더 복잡한 문제에서 더 정확한 결과를 가져올 수 있습니다. (이 전략을 "프랑스의 수도는 어디인가요?"와 같은 간단한 지식 기반 질문에 사용하는 것은 의미가 없다는 점에 유의하십시오. 이는 주어진 입력 쿼리(input query)에 AI 모델이 적합한지 여부를 파악하는 좋은 경험칙이기도 합니다.) 이 전략은 AI가 단순히 답변을 제공하는 것을 넘어, 그 답변에 도달하는 과정을 투명하게 보여주는 데도 기여합니다.

**2022년 논문 "Large Language Models are Zero-Shot Reasoners"에서 발췌한 고전적인 CoT 프롬프팅(CoT prompting) 예시 (https://arxiv.org/abs/2205.11916).**

앞서 언급한 CoT 접근 방식은 더 많은 출력 토큰(output tokens)을 생성함으로써 추론(inference) 비용을 증가시키기 때문에 추론 시간 스케일링(inference-time scaling)으로 볼 수 있습니다.

추론 시간 스케일링(inference-time scaling)의 또 다른 접근 방식은 투표 및 검색 전략(voting and search strategies)을 사용하는 것입니다. 한 가지 간단한 예는 다수결 투표(majority voting)로, LLM이 여러 답변을 생성하게 하고 다수결 투표를 통해 올바른 답변을 선택합니다. 마찬가지로, 빔 탐색(beam search) 및 기타 검색 알고리즘(search algorithms)을 사용하여 더 나은 응답을 생성할 수 있습니다. 이러한 다양한 전략에 대한 자세한 내용은 제가 이전 "2024년 주목할 만한 AI 연구 논문 (2부)" 기사에서 설명했던 "Scaling LLM Test-Time Compute Optimally can be More Effective than Scaling Model Parameters" 논문을 강력히 추천합니다. 이러한 방법들은 모델의 내재된 능력을 최대한 활용하면서도, 추가적인 학습 없이 성능을 향상시킬 수 있는 실용적인 방법입니다.

**다양한 검색 기반 방법은 최적의 답변을 선택하기 위해 프로세스 보상 기반 모델(process-reward-based model)에 의존합니다.**

**LLM Test-Time Compute 논문에서 발췌한 주석이 달린 그림, https://arxiv.org/abs/2408.03314**

DeepSeek R1 기술 보고서는 일반적인 추론 시간 스케일링(inference-time scaling) 방법(예: 프로세스 보상 모델 기반 및 몬테카를로 트리 탐색 기반 접근 방식)을 "실패한 시도(unsuccessful attempts)"로 분류합니다. 이는 DeepSeek이 R1 모델의 긴 응답 생성 경향을 넘어 이러한 기술을 명시적으로 사용하지 않았음을 시사하며, 이는 V3 기본 모델(base model)에 비해 암묵적인 형태의 추론 시간 스케일링(inference-time scaling) 역할을 합니다. 하지만 명시적인 추론 시간 스케일링(inference-time scaling)은 LLM 자체 내에서보다는 애플리케이션 계층(application layer)에서 구현되는 경우가 많으므로, DeepSeek은 여전히 앱 내에서 이러한 기술을 적용할 수 있습니다.

저는 OpenAI의 o1 및 o3 모델이 추론 시간 스케일링(inference-time scaling)을 사용한다고 생각하며, 이는 GPT-4o와 같은 모델에 비해 상대적으로 비싼 이유를 설명할 수 있습니다. 추론 시간 스케일링(inference-time scaling) 외에도 o1과 o3는 DeepSeek R1에 사용된 것과 유사한 RL 파이프라인(pipelines)을 사용하여 학습되었을 가능성이 높습니다. 강화 학습(reinforcement learning)에 대한 자세한 내용은 아래 다음 두 섹션에서 다루겠습니다.

#### 2) 자율적 학습과 행동 (Pure reinforcement learning, RL)

최근 연구에서 제가 개인적으로 주목한 점 중 하나는 순수 RL(강화 학습)로부터 자율적인 행동이 나타난다는 발견입니다. 이는 모델이 외부의 명시적인 지시 없이도 환경과의 상호작용을 통해 스스로 학습하고 최적의 행동 전략을 찾아내는 능력을 의미합니다.

앞서 설명했듯이, DeepSeek은 세 가지 유형의 R1 모델을 개발했습니다. 첫 번째인 DeepSeek-R1-Zero는 2024년 12월에 출시된 표준 사전 학습(pre-trained) LLM인 DeepSeek-V3 기본 모델(base model)을 기반으로 구축되었습니다. RL(강화 학습) 전에 SFT(지도 미세 조정)가 적용되는 일반적인 RL 파이프라인(pipelines)과 달리, DeepSeek-R1-Zero는 아래 다이어그램에 강조된 것처럼 초기 SFT 단계 없이 오직 강화 학습(reinforcement learning)으로만 학습되었습니다.

**DeepSeek-R1-Zero 모델의 개발 과정.**

그럼에도 불구하고, 이 RL 과정은 일반적으로 LLM의 선호도 조정(preference-tune)에 적용되는 RLHF(인간 피드백 기반 강화 학습) 접근 방식과 여러 면에서 유사합니다. (저는 제 기사 "LLM 학습: RLHF와 그 대안"에서 RLHF를 더 자세히 다루었습니다.) 하지만 초기 모델의 주요 차이점은 명령어 조정(instruction tuning)을 위한 SFT(지도 미세 조정) 단계를 건너뛰었다는 것입니다. 이것이 그들이 이를 "순수(pure)" RL이라고 부르는 이유입니다. (하지만 LLM 맥락에서의 RL은 전통적인 RL과는 상당히 다르며, 이는 나중에 다룰 주제입니다.)

보상(rewards)의 경우, 인간의 선호도에 따라 학습된 보상 모델(reward model)을 사용하는 대신, 정확도 보상(accuracy reward)과 형식 보상(format reward)이라는 두 가지 유형의 보상을 사용했습니다. 정확도 보상(accuracy reward)은 LeetCode 컴파일러(compiler)를 사용하여 코딩 답변을 검증하고, 확정적 시스템(deterministic system)을 사용하여 수학적 응답을 평가합니다. 형식 보상(format reward)은 LLM 심사관(judge)에 의존하여 응답이 `<think>` 태그(tags) 안에 추론 단계(reasoning steps)를 배치하는 것과 같은 예상 형식을 따르도록 합니다. 이러한 보상 체계는 모델이 단순히 정답을 맞추는 것을 넘어, 일관된 형식과 구조를 유지하며 응답을 생성하도록 유도합니다.

놀랍게도, 이 접근 방식은 LLM이 기본적인 학습 기술을 개발하기에 충분했습니다. 연구원들은 아래 그림에 표시된 것처럼, 모델이 명시적으로 훈련되지 않았음에도 불구하고 응답의 일부로 추론 흔적(reasoning traces)을 생성하기 시작하는 "아하!(Aha!)" 순간을 관찰했습니다. 이는 AI가 예상치 못한 방식으로 지능적인 행동을 발현할 수 있음을 보여주는 중요한 사례입니다.

**DeepSeek R1 기술 보고서에서 발췌한 "아하(Aha)" 순간의 출현을 보여주는 그림 (https://arxiv.org/abs/2501.12948).**

R1-Zero는 최고 성능의 추론 모델은 아니지만, 위 그림에 표시된 것처럼 중간 "생각" 단계(intermediate "thinking" steps)를 생성함으로써 추론 능력을 보여줍니다. 이는 순수 RL(강화 학습)을 사용하여 추론 모델을 개발하는 것이 가능함을 확인시켜주며, DeepSeek 팀이 이 접근 방식을 처음으로 시연(또는 최소한 발표)했습니다.

Ahead of AI는 독자 지원 출판물입니다. 새로운 게시물을 받고 제 작업을 지원하려면 무료 또는 유료 구독자가 되는 것을 고려해 보세요.
[Subscribe]

#### 3) 통합 학습 전략: SFT(지도 미세 조정) 및 RL(강화 학습)

다음으로, 추론 모델 구축을 위한 청사진 역할을 하는 차세대 AI 모델의 개발을 살펴보겠습니다. 이 모델은 추가 SFT(지도 미세 조정) 및 RL(강화 학습)을 통합하여 전반적인 성능을 향상시킵니다. 표준 RLHF(인간 피드백 기반 강화 학습) 파이프라인(pipeline)에서 볼 수 있듯이, RL 전에 SFT 단계를 포함하는 것이 실제로 일반적이라는 점에 유의하십시오. OpenAI의 o1도 유사한 접근 방식을 사용하여 개발되었을 가능성이 높습니다.

**차세대 AI 모델의 개발 과정.**

위 다이어그램에 표시된 것처럼, 개발 팀은 초기 모델을 사용하여 그들이 '콜드 스타트(cold-start)' SFT 데이터(data)라고 부르는 것을 생성했습니다. '콜드 스타트(cold start)'라는 용어는 이 데이터가 DeepSeek-R1-Zero에 의해 생성되었으며, DeepSeek-R1-Zero 자체는 어떤 SFT(지도 미세 조정) 데이터로도 학습되지 않았다는 사실을 의미합니다. 이 콜드 스타트 SFT 데이터(data)를 사용하여 모델은 명령어 미세 조정(instruction fine-tuning)을 통해 학습시킨 다음, 또 다른 RL(강화 학습) 단계를 거쳤습니다. 이 RL 단계는 DeepSeek-R1-Zero의 RL 과정에서 사용된 것과 동일한 정확도 및 형식 보상(rewards)을 유지했습니다. 하지만 그들은 언어 혼합(language mixing)을 방지하기 위해 일관성 보상(consistency reward)을 추가했습니다. 언어 혼합은 모델이 응답 내에서 여러 언어 사이를 전환할 때 발생합니다.

RL 단계 다음에는 또 다른 SFT 데이터(data) 수집 라운드가 이어졌습니다. 이 단계에서는 가장 최근의 모델 체크포인트(checkpoint)가 60만 개의 CoT(사고의 사슬) SFT 예시를 생성하는 데 사용되었고, 추가로 20만 개의 지식 기반 SFT 예시는 DeepSeek-V3 기본 모델(base model)을 사용하여 생성되었습니다. 이 60만 + 20만 개의 SFT 샘플(samples)은 최종 RL(강화 학습) 라운드를 진행하기 전에 DeepSeek-V3 기본 모델(base model)의 명령어 미세 조정(instruction-finetuning)에 사용되었습니다. 이 단계에서는 수학 및 코딩 질문에 대한 정확도 보상(accuracy rewards)에 규칙 기반 방법(rule-based methods)을 다시 사용했으며, 다른 질문 유형에는 인간 선호도 레이블(human preference labels)이 사용되었습니다. 종합적으로 볼 때, 이는 SFT 데이터(data)에 (더 많은) CoT 예시가 포함되어 있다는 점을 제외하면 일반적인 RLHF(인간 피드백 기반 강화 학습)와 매우 유사합니다. 그리고 RL은 인간 선호도 기반 보상(rewards) 외에 검증 가능한 보상(verifiable rewards)을 가집니다.

최종 모델인 DeepSeek-R1은 아래 표에 표시된 것처럼 추가 SFT(지도 미세 조정) 및 RL(강화 학습) 단계 덕분에 DeepSeek-R1-Zero보다 눈에 띄는 성능 향상을 보입니다. 이러한 통합 학습 전략은 모델이 다양한 유형의 데이터를 효과적으로 학습하고, 복잡한 추론 능력을 개발하며, 동시에 인간의 선호도와 윤리적 기준을 반영하도록 돕습니다.

**OpenAI O1 및 DeepSeek R1 모델의 벤치마크(benchmark) 비교.**

**DeepSeek-R1 기술 보고서에서 발췌한 주석이 달린 그림 (https://arxiv.org/abs/2501.12948).**

#### 4) 모델 경량화 및 지식 증류 (Pure supervised finetuning, SFT)

지금까지 AI 모델을 구축하고 개선하는 세 가지 주요 접근 방식을 다루었습니다:

1.  **효율적인 자원 활용**: 기본 모델을 학습시키거나 수정하지 않고 추론 능력을 향상시키는 기술.
2.  **자율적 학습과 행동**: 지도 미세 조정(supervised fine-tuning) 없이도 추론이 학습된 행동으로 나타날 수 있음을 보여주었습니다.
3.  **통합 학습 전략**: 고성능 AI 모델로 이어지는 SFT(지도 미세 조정) + RL(강화 학습)의 결합.

그렇다면 남은 것은 무엇일까요? 모델 '증류(distillation)'는 핵심 전략 중 하나입니다. 놀랍게도 DeepSeek은 그들이 증류(distillation)라고 부르는 과정을 통해 학습된 더 작은 모델들도 출시했습니다. 하지만 LLM 맥락에서 증류(distillation)는 딥러닝(deep learning)에서 사용되는 고전적인 지식 증류(knowledge distillation) 접근 방식을 반드시 따르지는 않습니다. 전통적으로 지식 증류(knowledge distillation)에서는(제 책 "Machine Learning Q and AI" 6장에서 간략히 설명했듯이) 더 작은 학생 모델(student model)이 더 큰 교사 모델(teacher model)의 로짓(logits)과 목표 데이터셋(target dataset) 모두에서 학습됩니다. 대신, 여기에서 증류(distillation)는 더 큰 LLM이 생성한 SFT 데이터셋(dataset)을 사용하여 Llama 8B 및 70B, Qwen 2.5 모델(0.5B ~ 32B)과 같은 더 작은 LLM을 명령어 미세 조정(instruction fine-tuning)하는 것을 의미합니다. 구체적으로, 이러한 더 큰 LLM은 DeepSeek-V3와 DeepSeek-R1의 중간 체크포인트(checkpoint)입니다. 사실, 이 증류(distillation) 과정에 사용된 SFT 데이터(data)는 이전 섹션에서 설명한 DeepSeek-R1을 학습시키는 데 사용된 것과 동일한 데이터셋(dataset)입니다.

그들은 왜 이러한 경량화 모델(distilled models)을 개발했을까요? 제 생각에는 두 가지 주요 이유가 있습니다:

1.  **더 작은 모델은 더 효율적이며, 자원 활용에 이점이 있습니다.** 이는 실행 비용이 저렴할 뿐만 아니라, 저사양 하드웨어(hardware)에서도 실행될 수 있어 저와 같은 많은 연구자와 실험가들에게 특히 흥미롭습니다.
2.  **순수 SFT(지도 미세 조정)의 사례 연구.** 이 증류 모델(distilled models)은 강화 학습(reinforcement learning) 없이 순수 SFT(지도 미세 조정)가 모델을 얼마나 발전시킬 수 있는지 보여주는 흥미로운 사례 연구 역할을 합니다.

아래 표는 이 증류 모델(distilled models)의 성능을 다른 인기 모델뿐만 아니라 DeepSeek-R1-Zero 및 DeepSeek-R1과 비교합니다.

**증류 모델(distilled models)과 비증류 모델(non-distilled models)의 벤치마크(benchmark) 비교.**

**DeepSeek-R1 기술 보고서에서 발췌한 주석이 달린 그림 (https://arxiv.org/abs/2501.12948).**

보시다시피, 증류 모델(distilled models)은 DeepSeek-R1보다 눈에 띄게 약하지만, DeepSeek-R1-Zero에 비해서는 훨씬 작음에도 불구하고 놀랍도록 강력합니다. 또한 이 모델들이 o1 mini와 비교하여 얼마나 잘 작동하는지 주목하는 것도 흥미롭습니다(저는 o1-mini 자체가 o1의 유사한 증류 버전일 수 있다고 생각합니다).

이 섹션을 결론으로 마무리하기 전에, 언급할 가치가 있는 또 다른 흥미로운 비교가 있습니다. DeepSeek 팀은 DeepSeek-R1-Zero에서 나타난 발현적 추론 행동(emergent reasoning behavior)이 더 작은 모델에서도 나타날 수 있는지 테스트했습니다. 이를 조사하기 위해, 그들은 DeepSeek-R1-Zero의 동일한 순수 RL(강화 학습) 접근 방식을 Qwen-32B에 직접 적용했습니다. 이 실험 결과는 아래 표에 요약되어 있으며, QwQ-32B-Preview는 Qwen 팀이 개발한 Qwen 2.5 32B 기반의 참조 추론 모델(reference reasoning model) 역할을 합니다(학습 세부 사항은 공개되지 않은 것으로 생각됩니다). 이 비교는 순수 RL(강화 학습)만으로 DeepSeek-R1-Zero보다 훨씬 작은 모델에서 추론 능력을 유도할 수 있는지에 대한 추가적인 통찰력을 제공합니다.

**더 작은 32B 모델에 대한 증류(distillation) 및 RL(강화 학습) 벤치마크(benchmark) 비교.**

**DeepSeek-R1 기술 보고서에서 발췌한 주석이 달린 그림 (https://arxiv.org/abs/2501.12948).**

흥미롭게도, 결과는 증류(distillation)가 더 작은 모델에 대해 순수 RL(강화 학습)보다 훨씬 더 효과적임을 시사합니다. 이는 RL(강화 학습)만으로는 이 규모의 모델에서 강력한 추론 능력을 유도하기에 충분하지 않을 수 있으며, 고품질 추론 데이터(data)에 대한 SFT(지도 미세 조정)가 작은 모델로 작업할 때 더 효과적인 전략이 될 수 있다는 생각과 일치합니다.

완전성을 위해, 표에 추가적인 비교를 포함했으면 유용했을 것입니다:

1.  DeepSeek-R1이 개발된 방식과 유사하게 SFT(지도 미세 조정) + RL(강화 학습)로 학습된 Qwen-32B. 이는 RL이 SFT와 결합될 때 순수 RL 및 순수 SFT와 비교하여 얼마나 많은 개선이 이루어질 수 있는지 판단하는 데 도움이 될 것입니다.
2.  증류 모델(distilled models)이 생성된 방식과 유사하게 순수 SFT(지도 미세 조정)로 학습된 DeepSeek-V3. 이는 RL + SFT가 순수 SFT보다 얼마나 효과적인지 직접 비교할 수 있게 해줄 것입니다.

Ahead of AI는 독자 지원 출판물입니다. 새로운 게시물을 받고 제 작업을 지원하려면 무료 또는 유료 구독자가 되는 것을 고려해 보세요.
[Subscribe]

### 지속 가능한 AI 개발을 위한 새로운 관점

최근 AI의 급속한 발전은 단순히 성능 향상을 넘어, AI가 사회에 미치는 영향과 자원 소비에 대한 심도 깊은 논의를 촉발하고 있습니다. AI 모델의 크기가 기하급수적으로 커지면서, 학습 및 추론에 필요한 에너지와 컴퓨팅 자원은 환경적, 경제적 부담으로 작용하고 있습니다. 따라서 '지속 가능한 AI' 개발은 이제 선택이 아닌 필수가 되었습니다.

이러한 관점에서, '그린 AI(Green AI)'는 AI 모델의 에너지 효율성을 높이고 탄소 발자국을 줄이는 것을 목표로 합니다. 이는 모델의 아키텍처를 최적화하고, 효율적인 학습 알고리즘을 개발하며, 증류(distillation)와 같은 경량화 기법을 적극적으로 활용함으로써 달성될 수 있습니다. 또한, 학습 데이터의 양과 품질을 최적화하여 불필요한 컴퓨팅 자원 사용을 줄이는 것도 중요합니다.

AI 개발은 기술적 혁신과 더불어 환경적, 사회적 책임을 동시에 고려해야 합니다. 이는 AI 기술이 장기적으로 인류 사회에 긍정적인 영향을 미치기 위한 필수적인 전제 조건입니다.

### 결론

이 섹션에서는 AI 모델을 구축하고 개선하기 위한 네 가지 다른 전략을 살펴보았습니다:

1.  **효율적인 자원 활용(inference-time scaling)**은 추가 학습이 필요 없지만 추론 비용을 증가시켜 사용자 수 또는 쿼리(query) 볼륨이 증가함에 따라 운영 비용을 높일 수 있습니다. 그럼에도 불구하고, 이미 강력한 모델의 성능을 향상시키는 데는 당연한 선택입니다. 저는 o1이 추론 시간 스케일링(inference-time scaling)을 활용한다고 강력히 의심하며, 이는 DeepSeek-R1에 비해 토큰(token)당 비용이 더 비싼 이유를 설명하는 데 도움이 됩니다.
2.  **순수 RL(강화 학습)**은 학습이 발현적 행동(emergent behavior)으로 나타나는 것에 대한 통찰력을 제공하기 때문에 연구 목적으로 흥미롭습니다. 하지만 실제 모델 개발에서는 RL + SFT가 더 강력한 AI 모델로 이어지므로 선호되는 접근 방식입니다. 저는 o1도 RL + SFT를 사용하여 학습되었다고 강력히 의심합니다. 더 정확히 말하면, o1은 DeepSeek-R1보다 약하고 작은 기본 모델(base model)에서 시작하지만, RL + SFT와 추론 시간 스케일링(inference-time scaling)으로 이를 보완한다고 생각합니다.
3.  위에서 언급했듯이, **RL + SFT**는 고성능 AI 모델을 구축하기 위한 핵심 접근 방식입니다. DeepSeek-R1은 이를 수행하는 방법을 보여주는 좋은 청사진입니다.
4.  **증류(distillation)**는 특히 더 작고 효율적인 모델을 만드는 데 있어 매우 효과적인 접근 방식입니다. 하지만 증류(distillation)는 혁신을 주도하거나 차세대 추론 모델을 생산하지 못한다는 한계가 있습니다. 예를 들어, 증류(distillation)는 항상 SFT(지도 미세 조정) 데이터(data)를 생성하기 위해 기존의 더 강력한 모델에 의존합니다.

다음으로 제가 기대하는 흥미로운 측면 중 하나는 RL + SFT(접근 방식 3)를 추론 시간 스케일링(inference-time scaling, 접근 방식 1)과 결합하는 시너지 효과입니다. 이것이 OpenAI o1이 하고 있는 일일 가능성이 높습니다. 단, DeepSeek-R1보다 약한 기본 모델(base model)을 기반으로 할 가능성이 있으며, 이는 DeepSeek-R1이 추론 시간(inference time)에 상대적으로 저렴하면서도 왜 그렇게 좋은 성능을 보이는지 설명합니다.

### AI 모델 개발의 시사점

최근 몇 주 동안 많은 사람들이 AI 모델 개발의 방향성에 대한 제 생각을 물어왔습니다. 간단히 말해, 저는 그것들이 대단한 성과이자 중요한 이정표라고 생각합니다. 연구 엔지니어(research engineer)로서, 저는 그들의 방법론(methodology)에 대한 통찰력을 제공하여 제가 배울 수 있는 상세한 기술 보고서(technical report)를 특히 중요하게 생각합니다. 가장 흥미로운 점 중 하나는 순수 RL(강화 학습)로부터 복잡한 행동이 어떻게 나타났는지입니다. 그리고 DeepSeek이 Meta의 Llama 모델보다도 제한이 적은 허용적인 오픈 소스(open-source) MIT 라이선스(license) 하에 모델을 오픈 소스화(open-sourced)했다는 점은 인상적입니다. 이는 AI 연구의 투명성과 접근성 증진에 크게 기여합니다.

**다른 최신 모델과 비교하면 어떤가요?**

최신 AI 모델들이 이전 세대보다 더 나은가요? 저는 대략 비슷한 수준이거나 특정 분야에서 우위를 보인다고 말하고 싶습니다. 하지만 눈에 띄는 점은 특정 모델이 추론 시간(inference time)에 더 효율적이라는 것입니다. 이는 개발 과정에서 학습 효율성에 더 많은 투자를 했거나, 추론 단계에서 최적화에 더 집중했음을 시사합니다.

그렇긴 하지만, 특정 모델들이 많은 것을 공개하지 않았기 때문에 직접적인 비교는 어렵습니다. 예를 들어, 우리는 다음을 알지 못합니다:

*   특정 모델도 MoE(전문가 혼합)인가요?
*   특정 모델은 얼마나 큰가요?
*   특정 모델이 최소한의 RL + SFT와 광범위한 추론 시간 스케일링(inference-time scaling)만 적용된 GPT-4o의 약간 개선된 버전일 수 있을까요?

이러한 세부 사항을 알지 못하면, 직접적인 비교는 여전히 동떨어진 비교(apples-to-oranges comparison)로 남습니다. 따라서 모델 선택 시에는 공개된 벤치마크(benchmark)와 함께 실제 적용 시나리오에서의 성능 및 비용 효율성을 종합적으로 고려해야 합니다.

### AI 모델 개발 비용과 접근성

또 다른 논의점은 최신 AI 모델 개발 비용이었습니다. 일부는 약 600만 달러의 학습 비용을 언급했지만, 그들은 초기 모델과 최종 모델을 혼동했을 가능성이 높습니다. 600만 달러 추정치는 GPU 시간당 2달러와 초기 모델의 최종 학습 실행에 필요한 GPU 시간 수를 가정한 것입니다. 하지만 DeepSeek 팀은 R1의 정확한 GPU 시간이나 개발 비용을 공개한 적이 없으므로, 모든 비용 추정치는 순전히 추측에 불과합니다.

어느 쪽이든, 궁극적으로 최신 AI 모델들은 오픈 웨이트(open-weight) 추론 모델의 주요 이정표이며, 추론 시간(inference time)에서의 효율성은 중요한 고려 사항이 됩니다. 이는 소규모 연구팀이나 스타트업(startup)이 AI 개발에 참여할 수 있는 기회를 모색하는 데 중요한 단서를 제공합니다.

### 제한된 예산으로 AI 모델 개발하기

오픈 웨이트(open-weight) 기본 모델(base model)로 시작하더라도 최상위 수준의 AI 모델을 개발하려면 수십만에서 수백만 달러가 필요할 가능성이 높습니다. 이는 제한된 예산으로 일하는 연구원이나 엔지니어에게는 낙담스러울 수 있습니다. 하지만 혁신은 반드시 막대한 자원을 필요로 하는 것은 아닙니다.

**좋은 소식: 증류(distillation)는 많은 것을 할 수 있습니다.**

다행히도 모델 증류(model distillation)는 더 비용 효율적인 대안을 제공하며, 소규모 팀에게 특히 유용합니다. DeepSeek 팀은 DeepSeek-R1보다 훨씬 작음에도 불구하고 놀랍도록 강력한 추론 성능을 달성하는 R1 증류 모델(distilled models)로 이를 입증했습니다. 하지만 이 접근 방식조차 완전히 저렴하지는 않습니다. 그들의 증류(distillation) 과정은 80만 개의 SFT 샘플(samples)을 사용했으며, 이는 상당한 계산(compute)을 필요로 합니다.

흥미롭게도, DeepSeek-R1이 출시되기 며칠 전, 저는 Sky-T1에 대한 흥미로운 기사를 접했습니다. 이는 소규모 팀이 단 1만 7천 개의 SFT 샘플(samples)만을 사용하여 오픈 웨이트(open-weight) 32B 모델을 학습시킨 매력적인 프로젝트였습니다. 총 비용은? 단 450달러로, 대부분의 AI 컨퍼런스(conferences) 등록비보다 훨씬 저렴합니다. 이 예시는 대규모 학습(training)은 여전히 비싸지만, 더 작고 목표 지향적인 미세 조정(fine-tuning) 노력은 비용의 일부만으로도 인상적인 결과를 낼 수 있음을 강조합니다.

**"Sky-T1: Train your own O1 preview model within $450" 기사에서 발췌한 그림, https://novasky-ai.github.io/posts/sky-t1/**

그들의 벤치마크(benchmarks)에 따르면, Sky-T1은 o1과 거의 동등한 성능을 보이며, 낮은 학습 비용을 고려할 때 인상적입니다.

**예산 내 순수 RL(강화 학습): TinyZero**

Sky-T1이 모델 증류(model distillation)에 중점을 두는 동안, 저는 '순수 RL(강화 학습)' 분야에서도 흥미로운 발전을 접했습니다. 주목할 만한 예시 중 하나는 DeepSeek-R1-Zero 접근 방식을 재현한 3B 파라미터(parameter) 모델인 TinyZero입니다(참고: 학습 비용은 30달러 미만입니다). 놀랍게도, 단 3B 파라미터(parameters)임에도 불구하고 TinyZero는 일부 발현적 자기 검증 능력(emergent self-verification abilities)을 보여주며, 이는 작은 모델에서도 순수 RL(강화 학습)을 통해 추론이 나타날 수 있다는 생각을 뒷받침합니다. TinyZero 저장소(repository)는 연구 보고서가 아직 진행 중이라고 언급하며, 저는 추가 세부 사항을 계속 주시할 것입니다.

**TinyZero 저장소(https://github.com/Jiayi-Pan/TinyZero)에서 발췌한 모델이 자기 검증(self-verification)이 가능함을 보여주는 그림. (비교를 위해 기본 모델(base model)의 응답을 보는 것도 흥미로웠을 것입니다.)**

위에서 언급된 두 프로젝트는 제한된 예산으로도 AI 모델에 대한 흥미로운 작업이 가능하다는 것을 보여줍니다. 두 접근 방식 모두 DeepSeek-R1의 방법을 재현하지만, 하나는 순수 RL(강화 학습)(TinyZero)에, 다른 하나는 순수 SFT(지도 미세 조정)(Sky-T1)에 중점을 둡니다. 이러한 아이디어를 어떻게 더 확장할 수 있을지 탐구하는 것은 매우 흥미로울 것입니다.

**전통적인 SFT(지도 미세 조정)를 넘어: 여정 학습(Journey Learning)의 탐구**

작년에 제가 접한 특히 흥미로운 접근 방식 중 하나는 'O1 Replication Journey: A Strategic Progress Report – Part 1' 논문에 설명되어 있습니다. 제목과는 달리, 이 논문은 실제로 o1을 재현하지 않습니다. 대신, 증류(distillation)(순수 SFT) 과정을 개선하는 다른 방법을 소개합니다. 이 논문의 핵심 아이디어는 '지름길 학습(shortcut learning)'의 대안으로서 '여정 학습(journey learning)'입니다. 지름길 학습(shortcut learning)은 명령어 미세 조정(instruction fine-tuning)에서 모델이 올바른 해결 경로만을 사용하여 학습되는 전통적인 접근 방식을 의미합니다. 반면에 여정 학습(journey learning)은 잘못된 해결 경로도 포함하여 모델이 실수로부터 학습할 수 있도록 합니다. 이 접근 방식은 TinyZero의 순수 RL(강화 학습) 학습에서 관찰된 자기 검증 능력(self-verification abilities)과 어느 정도 관련이 있지만, SFT(지도 미세 조정)를 통해서만 모델을 개선하는 데 중점을 둡니다. 모델을 잘못된 추론 경로와 그 수정 사항에 노출시킴으로써, 여정 학습(journey learning)은 자기 수정 능력(self-correction abilities)을 강화하여 추론 모델을 이 방식으로 더 신뢰할 수 있게 만들 수 있습니다.

**전통적인 지름길 학습(shortcut learning)과 달리 여정 학습(journey learning)은 SFT 데이터(data)에 다양한 해결 경로를 포함합니다.**

**O1 Replication Journey: A Strategic Progress Report – Part 1에서 발췌한 주석이 달린 그림 (https://arxiv.org/abs/2410.18982)**

이는 특히 RL 기반 접근 방식이 계산적으로 비실용적일 수 있는 저예산 AI 모델 개발에 있어 미래 작업의 흥미로운 방향이 될 수 있습니다. 어쨌든, AI 모델 개발 분야에서는 현재 많은 흥미로운 작업이 진행 중이며, 앞으로 몇 달 안에 훨씬 더 많은 흥미로운 작업을 보게 될 것이라고 확신합니다!

이 잡지는 개인적인 열정 프로젝트입니다. 저를 지원하고 싶으신 분들은 제 책 "Build a Large Language Model (From Scratch)"을 구매해 주시면 감사하겠습니다. (이 책은 다른 곳에서는 찾아볼 수 없는 수준의 세부 사항으로 LLM이 어떻게 작동하는지 설명하므로, 이 책에서 많은 것을 얻으실 것이라고 확신합니다.)

**"Build a Large Language Model (From Scratch)" 아마존(Amazon)에서 지금 구매 가능**

책을 읽으셨고 잠시 시간을 내주실 수 있다면, 짧은 서평을 남겨주시면 정말 감사하겠습니다. 이는 저희 저자들에게 큰 도움이 됩니다! 여러분의 지원은 큰 의미가 있습니다! 감사합니다!