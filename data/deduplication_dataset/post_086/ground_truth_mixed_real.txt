AI 엔지니어 가을 서밋(AI Engineer Fall Summit)이 11월 20-21일 뉴욕(NYC)에서 다시 개최됩니다! 최신 AI 기술과 동향을 만나보세요. 2023년 12월, 우리는 AI 분야의 주요 도전 과제인 'AI의 네 가지 전쟁(The Four Wars of AI)'과 그 중 핵심이었던 'RAG/Ops 전쟁(RAG/Ops War)'을 심도 있게 다뤘습니다. 수많은 투자가 벡터 데이터베이스(vector databases) 시장에 집중되고, 기술 하이프 사이클(hype cycle)의 여러 단계를 거친 후, 크로마(Chroma)의 창립자이자 CEO인 제프 휴버(Jeff Huber)는 최근 "RAG는 죽었다"는 과감한 견해를 제시했습니다. 이는 단순한 초기 RAG 구현이 더 이상 복잡한 AI 애플리케이션의 요구사항을 충족하지 못하며, '컨텍스트 엔지니어링(Context Engineering)'이라는 보다 정교한 접근 방식이 필요하다는 의미입니다.

컨텍스트 길이(context lengths)가 급증하고, AI 워크로드(AI workloads)가 단순한 정보 제공 챗봇(chatbots)을 넘어 자율적인 에이전트(IMPACTful agents)로 진화함에 따라, 랜스 마틴(Lance Martin)과 덱스 호시(Dex Horthy)와 같은 업계 리더(thoughtleaders)들의 연구는 컨텍스트 관리의 중요성을 재조명하고 있습니다. 특히, 컨텍스트 엔지니어링(context engineering) 분야의 최신 동향에 대한 랜스 마틴의 블로그 게시물은 이 변화의 흐름을 잘 보여줍니다: https://rlancemartin.github.io/2025/06/23/context_engineering/

크로마(Chroma)는 '컨텍스트 로트(Context Rot)' 및 '생성형 벤치마킹(Generative Benchmarking)'과 같은 주요 보고서를 통해 컨텍스트 엔지니어링(context engineering)이라는 새로운 분야에서 선도적인 연구를 수행해 왔습니다. 우리는 이 분야에서 검색(retrieval)의 현재와 미래, AI 시스템의 메모리(memory) 관리, 그리고 검색 성능 벤치마킹(retrieval benchmarking)의 중요성에 대해 깊이 논의했습니다. 이는 단순히 정보를 검색하는 것을 넘어, 검색된 정보를 LLM이 가장 효과적으로 활용할 수 있도록 최적화하는 전반적인 과정에 대한 탐구입니다.

**5가지 검색 팁**
*   "RAG"를 출시하지 마세요. 검색(retrieval)을 출시하세요.
*   기본 요소(primitives)의 이름을 지정하세요 (밀집(dense), 어휘(lexical), 필터(filters), 재순위화(re-rank), 어셈블리(assembly), 평가 루프(eval loop)).
*   하이브리드 검색(hybrid retrieval)을 통해 초기 단계에서 우위를 점하십시오. 200-300개의 후보는 LLM이 처리하기에 적절한 양입니다.
*   컨텍스트(context)를 조립하기 전에 항상 재순위화(re-rank)하세요.
*   '컨텍스트 로트(context rot)' 현상을 인지하고, 과도하게 긴 컨텍스트(contexts) 대신 간결하고 구조화된 정보를 제공하는 것이 중요합니다.
*   어느 날 저녁 피자를 사고 작은 골드 세트(gold set)를 만드는 데 투자하세요. 그것을 CI(Continuous Integration) 및 대시보드(dashboards)에 연결하세요.

**[수집(Ingest)]**
├─ 파싱(Parse) + 청킹(chunk) (도메인 인식(domain-aware): 헤딩(headings), 코드 블록(code blocks), 테이블(tables))
├─ 보강(Enrich): 제목(titles), 앵커(anchors), 심볼(symbols), 메타데이터(metadata)
├─ 선택 사항: LLM "청크 요약(chunk summaries)" (코드/API용 자연어 주석(NL glosses))
├─ 임베딩(Embeddings) (밀집(dense)) + 선택적 희소 신호(sparse signals)
└─ DB에 쓰기(text, vectors, metadata)

**[쿼리(Query)]**
├─ 1단계 하이브리드(First-stage hybrid): 벡터(vector) + 어휘/정규식(lexical/regex) + 메타데이터 필터(metadata filters)
├─ 후보 풀(Candidate pool): ~100–300
├─ 재순위화(Re-rank) (LLM 또는 크로스 인코더(cross-encoder)) → 상위 ~20–40
└─ 컨텍스트 어셈블리(Context assembly):
    - 지침/시스템 프롬프트(system prompt) 우선
    - 중복 제거/유사 중복 병합
    - 소스 다양화
    - 토큰(tokens)에 대한 하드캡(hard cap)

**[외부 루프(Outer loop)]**
├─ 캐시/비용 가드레일(guardrails)
├─ 작은 골드 세트(gold sets)에 대한 생성형 벤치마킹(Generative benchmarking)
├─ 오류 분석(Error analysis) → 재청킹(re-chunk)/필터 재조정/재순위화 프롬프트(re-rank prompt)
└─ 메모리/압축(compaction): 상호작용 트레이스(interaction traces) 요약 → 검색 가능한 사실

크로마(Chroma) 사무실에 위치한 저희 팟캐스트 스튜디오(podcast studio)에서, 건물주를 게스트로 모시고 흥미로운 대화를 나눌 수 있어 매우 기뻤습니다! 이 에피소드를 즐겁게 청취해 주시길 바랍니다!

**쇼 노트(Show Notes)**
*   크로마(Chroma)
*   제프 휴버(Jeff Huber)
*   AI는 새로운 컴퓨터
*   컨텍스트 로트(Context Rot) 기술 보고서(Technical Report)
*   생성형 벤치마킹(Generative Benchmarking) 기술 보고서(Technical Report)
*   REALM (검색 증강 언어 모델(Retrieval-Augmented Language Model)) 논문
*   RETRO (검색 강화 트랜스포머(Retrieval-Enhanced Transformer)) 논문
*   SF 시스템 그룹(SF Systems Group)
*   보이저(Voyager) 논문
*   고급 컨텍스트 엔지니어링 전략 (Advanced Context Engineering Strategies)

**타임스탬프(Timestamps)**
*   [00:00:00] 소개: 팟캐스트의 서막, AI 엔지니어링의 최신 동향과 컨텍스트 엔지니어링의 부상에 대한 논의 시작.
*   [00:00:48] 왜 크로마(Chroma)를 구축했는가: 데모와 실제 프로덕션 환경 사이의 격차를 해소하고, AI 개발자들을 위한 신뢰할 수 있는 검색 인프라를 제공하려는 크로마의 비전.
*   [00:02:55] 정보 검색(Information Retrieval) 대 검색(Search): '현대적인 AI 검색 인프라'의 의미와 기존 검색 시스템과의 차이점, 특히 AI 워크로드와 LLM 소비자의 역할에 대한 제프의 통찰.
*   [00:04:29] 경쟁적인 AI 시장에서 집중 유지하기: 과열된 벡터DB 시장 속에서 크로마가 어떻게 핵심 가치와 비전에 집중하며 품질을 최우선으로 했는지에 대한 제프의 철학.
*   [00:08:08] 크로마 클라우드(Chroma Cloud) 구축: 단일 노드 크로마의 성공 이후, 개발자 경험을 유지하면서 확장 가능한 클라우드 서비스를 구축하기 위한 도전과 '제로 구성(zero config)' 원칙.
*   [00:12:15] 컨텍스트 엔지니어링(Context Engineering)과 RAG의 문제점: 'RAG는 죽었다'는 제프의 주장이 의미하는 바와, 컨텍스트 엔지니어링이 AI 애플리케이션 개발에서 왜 핵심적인 새로운 추상화인지에 대한 설명.
*   [00:16:11] 컨텍스트 로트(Context Rot): LLM의 컨텍스트 길이 증가에도 불구하고 성능 저하가 발생하는 '컨텍스트 로트' 현상에 대한 연구와 그 중요성.
*   [00:21:49] 컨텍스트 품질(Context Quality) 우선순위 지정: 모델 제공업체들이 컨텍스트 품질을 벤치마킹하고 개선해야 하는 이유와 개발자들이 이를 요구해야 하는 필요성.
*   [00:27:02] 코드 인덱싱(Code Indexing) 및 검색 전략(Retrieval Strategies): 코드 검색을 위한 인덱싱, 임베딩, 정규식(regex)의 활용법과 크로마의 코드 포킹(forking) 기능이 제공하는 이점.
*   [00:32:04] 코드 청크 재작성(Chunk Rewriting) 및 쿼리 최적화(Query Optimization): 수집 단계에서 청크 재작성과 메타데이터 보강을 통해 쿼리 성능을 최적화하는 방법, 그리고 '골든 데이터 세트'의 중요성.
*   [00:34:07] 트랜스포머(Transformer) 아키텍처(Architecture) 진화 및 검색 시스템(Retrieval Systems): 인코더-디코더 분리를 통한 현대 검색 시스템의 아키텍처적 진화와 미래의 '잠재 공간(latent space)' 기반 검색 및 지속적인 검색(continuous retrieval)에 대한 전망.
*   [00:38:06] 컨텍스트 엔지니어링(Context Engineering)의 이점으로서의 메모리(Memory): AI 시스템에서 '메모리'의 역할과 그것이 컨텍스트 엔지니어링의 한 형태로 어떻게 구현될 수 있는지에 대한 논의.
*   [00:40:13] AI 메모리(Memory) 구조화 및 오프라인 압축(Offline Compaction): AI 메모리 관리에서 압축(compaction)과 오프라인 처리의 중요성, 그리고 LLM을 활용한 쿼리 생성 기술 보고서에 대한 설명.
*   [00:45:46] 이전 스타트업(Startups)의 교훈과 목적 있는 구축: 제프의 이전 스타트업 경험에서 얻은 교훈, 즉 자신이 진정으로 사랑하는 일과 함께 일하고 싶은 사람들과 함께 일하는 것의 중요성.
*   [00:47:32] 실리콘 밸리(Silicon Valley)의 종교와 가치: 실리콘 밸리의 'AGI 종교'와 같은 현대적 신념에 대한 제프의 관점, 그리고 장기적인 비전과 가치 중심의 구축의 중요성.
*   [00:50:18] 기업 문화, 디자인, 브랜드 일관성: 크로마의 모든 측면에서 일관된 품질과 의도적인 디자인이 어떻게 기업 문화와 브랜드 가치를 형성하는지에 대한 통찰.
*   [00:52:36] 크로마(Chroma) 채용: 디자이너(Designers), 연구원(Researchers), 엔지니어(Engineers): 크로마가 현재 찾고 있는 인재상, 특히 개발자 도구, 저수준 분산 시스템, 연구 분야의 전문가들에 대한 구체적인 설명.

**대본**
**알레시오(Alessio) [00:00:04]:** 안녕하세요, 여러분. 레이턴트 스페이스(Latent Space) 팟캐스트(podcast)의 새로운 에피소드에 오신 것을 환영합니다. 저는 데시벨(Decibel)의 파트너(partner)이자 CTO인 알레시오(Alessio)이며, 스몰AI(SmolAI)의 창립자 스윅스(Swyx)와 함께 진행합니다.
**스윅스(Swyx) [00:00:11]:** 안녕하세요. 오늘 게스트인 제프(Jeff)는 이미 몇 달 동안 저희를 크로마(Chroma) 사무실로 환영해 주셨으니, '환영한다'는 말이 다소 어색하게 들리네요. 제프, 초대해 주셔서 감사합니다. 오늘 함께하게 되어 기쁩니다. 제프는 크로마(Chroma)의 창립자이자 CEO입니다. 저는 오랫동안 크로마를 지켜봐 왔으며, 특히 초기에는 오픈 소스 벡터 데이터베이스(open source vector database)로 시작하여 '보이저(Voyager)' 논문과 같은 여러 중요한 프로젝트에서 활용된 것을 기억합니다. 오늘날 크로마를 어떻게 정의하시겠습니까?

**왜 크로마(Chroma)를 구축했는가**
**제프(Jeff) [00:00:48]:** 좋은 질문입니다. 우리는 항상 청중의 관점에서 메시지를 전달하고자 합니다. 크로마(Chroma)를 설립한 계기는 수년간 응용 기계 학습(applied machine learning) 분야에서 일하면서 데모(demos)는 쉽게 만들 수 있었지만, 프로덕션(production) 환경에서 신뢰할 수 있는 시스템(system)을 구축하는 것이 매우 어렵다는 것을 깨달았기 때문입니다. 데모와 실제 운영 환경 사이의 간극은 엔지니어링(engineering)이라기보다 마치 연금술처럼 느껴졌습니다. "이게 당신의 데이터 시스템(data system)입니까?"라는 질문에 "네"라고 답하는 XKCD 밈(memes)처럼, "그것이 좋은지 어떻게 압니까? 아니면 어떻게 더 좋게 만듭니까?"라는 질문에 "그냥 냄비를 휘젓고 더 나아지는지 봅니다"라고 말하는 상황은 근본적으로 잘못된 접근 방식이었습니다.

이러한 고민은 2021년과 2022년 당시의 주된 화두였습니다. 이 시기에 우리는 잠재 공간(Latent Space)이 단순한 팟캐스트(podcast)를 넘어, AI 모델(models)이 데이터를 이해하는 방식과 인간이 이를 해석할 수 있는 중요한 도구로서 매우 저평가되어 있다는 가설을 세웠습니다. 이 가설은 우리의 출발점이 되었고, 우리가 나아가고자 하는 방향을 제시했습니다. 우리의 목표는 개발자(developers)가 AI 기반의 프로덕션(production) 애플리케이션(applications)을 더 쉽게 구축할 수 있도록 지원하는 것입니다. 데모 단계에서 실제 운영까지의 과정을 연금술이 아닌, 예측 가능하고 신뢰할 수 있는 엔지니어링(engineering) 프로세스로 만들려면 어떻게 해야 할까요?

데이터베이스(database) 구축은 결코 부수적인 과제(side quest)가 아니었습니다. 오히려 우리의 '메인 퀘스트(main quest)'의 핵심 부분이었습니다. 이 과정에서 우리는 검색(search)이 AI 애플리케이션(applications) 개발의 핵심 워크로드(workload)임을 깨달았습니다. 이는 유일한 워크로드(workload)는 아니지만, 그 중요성은 매우 큽니다. 한 가지를 세계 최고 수준으로 달성하기 전에는 다른 것에 집중할 수 없다는 신념 아래, 우리는 검색 기술에 광적으로 집중해 왔습니다. 이것이 지난 몇 년간 크로마(Chroma)가 걸어온 길입니다. 요약하자면, 오늘날 크로마(Chroma)는 AI 애플리케이션(applications)을 위한 강력한 검색 엔진(retrieval engine)을 제공하며, 우리는 AI를 위한 최첨단 검색 인프라(search infrastructure)를 연구하고 있습니다.

**정보 검색(Information Retrieval) 대 검색(Search)**
**스윅스(Swyx) [00:02:55]:** 이 부분에 대해 좀 더 깊이 들어가 보죠. 정보 검색(information retrieval)과 일반적인 검색(search)은 같은 개념인가요? 아니면 미묘한 차이가 있다고 보십니까? 용어의 정의를 명확히 하고 싶습니다.
**제프(Jeff) [00:03:04]:** AI를 위한 현대적인 검색 인프라(search infrastructure)에 대해 설명하겠습니다. 여기서 '현대적인'이라는 의미는 '전통적인' 방식과 대조되며, 주로 최신 분산 시스템(distributed systems) 아키텍처를 의미합니다. 지난 5년에서 10년간 발전한 분산 시스템 구축의 핵심 기본 요소(primitives)들이 여기에 포함됩니다. 이는 기존 기술에서는 찾아보기 힘든 특징들입니다. 구체적으로, 읽기와 쓰기의 분리, 스토리지(storage)와 컴퓨트(compute)의 분리, 그리고 크로마(Chroma)가 러스트(Rust)로 개발되어 완전한 멀티 테넌트(multi-tenant) 환경을 지원한다는 점입니다. 우리는 객체 스토리지(object storage)를 크로마(Chroma)의 핵심 데이터 계층으로 활용하며, 크로마 클라우드(Chroma Cloud)에서도 분산 아키텍처를 채택하고 있습니다. 이것이 바로 '현대적인' 측면입니다.

'AI를 위한'이라는 부분은 네 가지 중요한 의미를 가집니다. 첫째, 검색에 활용되는 도구와 기술이 기존 검색 시스템과 다릅니다. 둘째, 처리해야 할 워크로드(workload)도 고전적인 시스템과 차이가 있습니다. 셋째, 검색 시스템을 다루는 개발자(developer)의 역할도 변화했습니다. 넷째, 검색 결과(search results)를 최종적으로 소비하는 주체가 고전적인 검색 시스템(search systems)과는 다릅니다. 과거에는 사람이 검색의 마지막 단계를 직접 수행했습니다. 즉, 여러 링크를 클릭하고, 내용을 요약하며, 관련성을 판단하는 모든 과정을 사람이 담당했습니다. 그러나 이제는 언어 모델(language model)이 이 역할을 수행합니다. 인간은 제한된 수의 링크만 처리할 수 있지만, 언어 모델은 훨씬 더 방대한 양의 정보를 소화하고 처리할 수 있습니다. 이러한 변화는 시스템(system) 설계 방식에 결정적인 영향을 미칩니다.

**경쟁적인 AI 시장에서 집중 유지하기**
**알레시오(Alessio) [00:04:29]:** 2023년은 벡터DB(VectorDB) 시장이 가장 뜨거웠던 시기 중 하나였습니다. 파인콘(Pinecone)이 1억 달러를 유치하고, 다양한 경쟁사들이 등장했죠. 이러한 상황 속에서 어떻게 외부의 압력이나 유행에 휩쓸리지 않고, 크로마(Chroma)만의 핵심 가치에 집중할 수 있었습니까? 크로마 클라우드(Chroma Cloud) 출시에도 신중하게 접근하여, 섣부른 출시로 인해 발생할 수 있는 문제를 피했습니다. AI 분야에서 인내심을 유지하는 것에 대해 조언해 주실 수 있나요? 창업자로서 자신만의 비전(vision)을 확고히 하고, 외부의 소음 속에서도 그 비전(vision)을 굳건히 지켜나가는 비결은 무엇입니까?
**제프(Jeff) [00:05:03]:** 스타트업(startup)을 세우는 데는 여러 가지 접근 방식이 있습니다. 한 가지는 고객의 요구에 따라 빠르게 반복하는 린 스타트업(lean startup) 방식입니다. 하지만 이 방식은 때때로 본질적인 가치보다 즉각적인 반응에 치중하게 만들 수 있습니다. 반면, 또 다른 방식은 확고한 비전(vision)과 차별화된 관점을 가지고 목표에 광적으로 집중하는 것입니다. 우리는 항상 후자의 방식을 추구해 왔습니다.

물론, 크로마(Chroma)의 단일 노드(single node) 버전이 성공적으로 운영되고 있었기에, 호스팅 서비스(hosted service)를 빠르게 출시할 유혹도 있었습니다. 그러나 우리는 크로마(Chroma)의 핵심 가치가 뛰어난 개발자 경험(developer experience)과 장인정신에 있다고 믿었습니다. 단순히 서비스형 단일 노드 제품을 제공하는 것만으로는 우리가 추구하는 개발자 경험의 기준을 충족시킬 수 없다고 판단했습니다. 결국, 우리는 "아니, 우리는 옳다고 생각하는 것을 구축할 것이다"라는 어려운 결정을 내렸습니다. 이 길은 쉽지 않았고 오랜 시간이 걸렸지만, 현재 수십만 명의 개발자(developers)가 크로마(Chroma)를 사용하며 만족하고 있다는 사실에 큰 자부심을 느낍니다.

**알레시오(Alessio) [00:06:38]:** 팀을 구성할 때, 이러한 비전을 어떻게 전달하셨습니까? 외부에서 'PG 벡터(PG vector)'나 다른 대안을 사용해도 된다고 생각할 때, 어떻게 비전을 명확히 하고 같은 목표를 가진 사람들을 모을 수 있었습니까? 초기 채용 과정에서 얻은 중요한 교훈이 있다면 무엇입니까?
**제프(Jeff) [00:07:07]:** 콘웨이의 법칙(Conway's law)의 상위 버전은 "당신은 당신의 조직도를 출시한다"는 것입니다. 이는 곧 회사 문화를 출시하는 것과 같습니다. 우리는 항상 팀원들에게 큰 가치를 두었습니다. 회사의 미래 성장은 전적으로 팀원들의 역량에 달려 있다고 믿기 때문입니다. 그래서 우리는 채용에 매우 신중하고 까다롭게 접근했습니다. 이전 스타트업(startups) 경험을 통해 "함께 일하는 것을 좋아하는 사람들과 일하고 싶다"는 확고한 신념을 갖게 되었습니다. 개발자(developers)들에게 최고의 장인정신과 품질을 제공하기 위해, 우리는 독립적으로 뛰어난 성과를 낼 수 있는 팀을 구축하는 데 집중했습니다.

**크로마 클라우드(Chroma Cloud) 구축**
**스윅스(Swyx) [00:08:08]:** 이제 크로마(Chroma)의 핵심적인 부분에 집중해 보죠. 저는 항상 중요한 수치(headline numbers)를 먼저 소개하고 싶습니다. 현재 파이파이(PyPI)에서 월간 500만 다운로드(downloads) 이상, 깃허브(GitHub) 스타(stars) 21,000개 이상을 기록하고 있습니다. 이 외에 크로마에 대해 사람들이 알아야 할 주요 사실이나 강점이 있다면 무엇일까요?
**제프(Jeff) [00:08:33]:** 네. 음, 네, 깃허브(GitHub) 스타(stars) 20,000개, 월간 500만 이상의 다운로드(downloads)를 기록하고 있습니다. 최근 확인한 총 다운로드(downloads) 수는 6천만에서 7천만을 넘어섰습니다. 수년 동안 크로마(Chroma)는 광범위하게, 그리고 랭체인(LangChain)과 많은 인덱스(index)와 같은 커뮤니티(communities) 내에서도 가장 많이 사용되는 프로젝트(project)였습니다.
**스윅스(Swyx) [00:08:51]:** 단일 노드(single node) 크로마(Chroma)가 품질을 상징한다고 하셨는데, 크로마 클라우드(Chroma Cloud)는 어떤 점에서 차별화됩니까? GA(General Availability)를 통해 출시된 크로마 클라우드에 대해 사람들이 알아야 할 점은 무엇이며, 이 서비스를 어떻게 개발하셨습니까? 특히, 스토리지(storage)와 컴퓨트(compute)의 분리는 무엇을 의미하나요?
**제프(Jeff) [00:09:13]:** 전적으로 동의합니다. 크로마(Chroma)는 직관적인 개발자 경험(developer experience)으로 잘 알려져 있습니다. `pip install ChromaDB` 명령 하나로 인메모리(in memory) 데이터베이스를 즉시 사용할 수 있도록 한 것은 우리의 초기 목표였습니다.
**스윅스(Swyx) [00:09:25]:** 처음일 수도 있습니다.
**제프(Jeff) [00:09:26]:** `pip install` 가능한 데이터베이스(database)는 처음일 것입니다.
**스윅스(Swyx) [00:09:28]:** 모든 SQLite 래퍼(wrapper)는 기술적으로 `pip install` 가능합니다. SQLite는 오늘날까지도 `pip install` 가능하지 않다고 생각합니다. 당신은 아마 이것에 대해 더 깊이 파고들고 지식을 가지고 있을 것입니다. 저는 그냥 추측하는 것입니다.
**제프(Jeff) [00:09:40]:** 이는 새로운 사용자(users)에게 매우 원활한 온보딩(onboarding) 경험을 제공했습니다. 우리는 아두이노(Arduinos)나 PowerPC 아키텍처(architectures)와 같은 다양한 환경에서도 크로마(Chroma)가 작동하도록 최적화했습니다. 이처럼 어디에서든 쉽게 사용할 수 있는 것이 크로마(Chroma) 단일 노드(single node)의 강점이었습니다. 우리는 이러한 개발자 경험(developer experience)을 클라우드 환경에서도 재현하고자 했습니다. `pip install ChromaDB`처럼 복잡한 추상화(abstractions)나 API(API)를 학습할 필요 없이, 클라우드(cloud)에서도 즉시 사용할 수 있는 경험을 제공하는 것이 목표였습니다.

즉, 사용자가 노드(nodes) 수, 크기, 샤딩(sharding), 백업(backup) 또는 데이터 계층화(data tiering) 전략에 대해 고민할 필요가 없는 제품이 필요했습니다. '제로 구성(zero config)'을 지향하며, 사용자가 조정할 복잡한 설정이 없어야 했습니다. 크로마 클라우드(Chroma Cloud)는 트래픽(traffic)이나 데이터 스케일(data scale) 변화에 관계없이 항상 빠르고, 비용 효율적이며, 최신 상태를 유지해야 했습니다. 이러한 기준이 우리의 개발을 이끌었습니다. 또한, 사용량 기반 청구(usage-based billing)는 매우 중요한 원칙이었습니다. 우리는 사용자가 실제로 사용한 컴퓨트(compute) 자원에 대해서만 요금을 부과하며, 이는 모든 서버리스 데이터베이스(serverless databases)가 달성하기 어려운 공정한 방식입니다.

**스윅스(Swyx) [00:11:19]:** 사실상 서버리스 컴퓨트 플랫폼(serverless compute platform)도 구축하고 있는 셈이네요.
**제프(Jeff) [00:11:23]:** 네, 그래야 합니다. 정확히 그렇습니다. 그것이 크로마 분산(Chroma Distributed)의 설계를 동기 부여했습니다. 크로마 분산(Chroma Distributed)도 동일한 모노레포(monorepo)의 일부이며, 아파치 2(Apache 2) 오픈 소스(open source)입니다. 그리고 제어 및 데이터 플레인(data plane)은 모두 아파치 2(Apache 2) 오픈 소스(open source)입니다. 그리고 크로마 클라우드(Chroma Cloud)는 크로마 분산(Chroma Distributed)을 사용하여 서비스를 실행합니다. 이 서비스는 가입하고, 데이터베이스(database)를 생성하고, 30초 이내에 데이터를 로드(load)할 수 있습니다. 그리고 촬영 시점에는 사람들이 5달러의 무료 크레딧(credits)을 받는데, 이는 10만 개의 문서(documents)를 로드(load)하고 10만 번 쿼리(query)하는 데 충분한 양입니다. 이는 분명히 많은 시간이 걸립니다. 많은 사용 사례에서 실제로 몇 년 동안 무료로 사용할 수도 있을 것입니다. 괜찮습니다. 그리고 그곳에 도달하기 위해 우리는 모든 어려운 작업을 수행해야 했습니다.
**스윅스(Swyx) [00:12:03]:** 모든 블로그(blog)는 기본적으로 의미론적 인덱싱(semantic indexing)을 가져야 한다고 생각합니다. 제프, 당신의 개인 블로그도 크로마(Chroma)에 호스팅(host)하고 있죠.
**제프(Jeff) [00:12:10]:** 네. 세상의 방대한 정보를 체계적으로 정리하는 과제는 여전히 미완성입니다.

**컨텍스트 엔지니어링(Context Engineering)과 RAG의 문제점**
**스윅스(Swyx) [00:12:15]:** 네. 컨텍스트 엔지니어링(Context Engineering)과 RAG의 문제점.
**알레시오(Alessio) [00:12:15]:** 몇 달 전, 4월에 컨텍스트 엔지니어링(context engineering)에 대한 트윗(tweet)을 올리셨죠. 많은 사람들의 이목을 집중시켰습니다.
**제프(Jeff) [00:12:24]:** 새로운 기술 시장이 형성될 때, 핵심적인 것은 바로 추상화(abstractions)와 우리가 사고하는 데 사용하는 기본 요소(primitives)입니다. AI 분야는 과도한 홍보 속에서 수많은 기본 요소와 추상화가 무분별하게 제시되어, 많은 개발자(developers)들이 무엇이 중요하고 어디에 집중해야 할지 혼란스러워했습니다. 예를 들어, 'RAG'라는 용어 자체가 그렇습니다. 우리는 RAG라는 용어를 사용하지 않습니다. 저는 RAG라는 용어를 싫어합니다.
**스윅스(Swyx) [00:13:08]:** 네, 당신의 영향 때문에 RAG 트랙을 부분적으로 없앴습니다.
**제프(Jeff) [00:13:10]:** 감사합니다. 감사합니다. 'RAG'는 검색(retrieval)과 생성(generation)이라는 세 가지 개념이 모호하게 묶여 있어 혼란을 야기합니다. 게다가 RAG는 이제 단순히 '단일 밀집 벡터 검색(single dense vector search)을 사용하는 것'으로 잘못 인식되는 경향이 있습니다. 제가 이 용어 대신 '컨텍스트 엔지니어링(context engineering)'에 대해 강조하는 이유는, 이는 AI 엔지니어링(AI engineering)의 중요한 하위 분야이기 때문입니다. 컨텍스트 엔지니어링(context engineering)은 주어진 LLM 생성 단계에서 컨텍스트 창(context window)에 무엇이 있어야 하는지를 알아내는 작업입니다. 여기에는 컨텍스트 창(context window)의 내용을 설정하는 내부 루프(inner loop)와, 시간이 지남에 따라 관련성 높은 정보로 컨텍스트 창을 최적화하는 외부 루프(outer loop)가 모두 포함됩니다.

우리는 최근 '컨텍스트 로트(Context Rot)'에 대한 기술 보고서(technical report)를 발표했습니다. 이 보고서는 LLM의 성능이 사용되는 토큰(tokens)의 양에 따라 저하될 수 있음을 상세히 보여줍니다. 즉, 더 많은 토큰을 사용할수록 모델(model)의 집중력과 추론 능력이 떨어질 수 있습니다. 이러한 '컨텍스트 로트(context rot)' 현상은 컨텍스트 엔지니어링(context engineering)의 필요성을 명확히 보여줍니다. 컨텍스트 엔지니어링이 중요한 이유는, 이 분야가 개발자 직업의 위상을 높이고 역할을 명확히 하는 데 기여하기 때문입니다. 오늘날 성공적인 AI 스타트업(startups)의 핵심 역량 중 하나는 바로 뛰어난 컨텍스트 엔지니어링(context engineering) 능력입니다.

**스윅스(Swyx) [00:14:45]:** 특히 제가 읽은 많은 글들은 에이전트(agents) 대 비에이전트(non-agent)에 초점을 맞추는 것 같습니다. 컨텍스트 엔지니어링(context engineering)은 에이전트(agents)에 더 관련이 있습니까? 아니면 컨텍스트 엔지니어링(context engineering)을 일반적으로 보고 있습니까?
**제프(Jeff) [00:15:00]:** 아니요. 에이전트(agent) 학습과 같은 흥미로운 에이전트(agent) 함의가 있지만, 저는 에이전트(agents)와 비에이전트(non-agent)를 엄격히 구분하지 않습니다. 에이전트가 상호작용을 통해 학습하는 것은 정적인 지식 기반 코퍼스(corpuses)를 사용하는 것과는 다소 거리가 있을 수 있지만, 문서와 대화하는 경우에도 상호작용을 통해 개선될 여지가 많다고 봅니다. 사실, '에이전트(agent)'라는 용어 자체의 정의가 아직 모호하다고 생각합니다. 불분명한 추상화(abstractions)와 단어는 중요하며, 많은 정의가 있지만, 저는 에이전트라는 용어가 사람들의 희망과 두려움을 담는 수단일 뿐이라고 생각합니다. 확실히 그렇습니다.

**스윅스(Swyx) [00:15:42]:** 음, 아마도 우리는 컨텍스트 엔지니어링(context engineering)에 대해 더 간결하고 정확하게 설명하려고 노력할 것입니다. 그래서 그것이 실제로 의미가 있고, 사람들이 실제로 그것을 사용하여 무언가를 할 수 있도록 말입니다. 컨텍스트 엔지니어링(context engineering) 또는 컨텍스트 로트(context rot)에 대해 제가 확실히 언급하고 싶은 한 가지는, 모든 최첨단 모델(frontier model)이 이제 100만 토큰(tokens)에 걸쳐 완전히 녹색의 완벽한 활용도 차트와 함께 출시되는 "건초 더미 속 바늘(needle in a haystack)"에 대한 많은 마케팅(marketing)이 있었다는 것입니다. 그런 종류의 마케팅(marketing)에 대해 여러분은 어떻게 생각하십니까?

**컨텍스트 로트(Context Rot)**
**제프(Jeff) [00:16:11]:** 아마도 조금 뒤로 돌아가서 설명하겠습니다. 이 연구는 에이전트(agent) 학습에 대한 관심에서 시작되었습니다. 우리는 에이전트(agents)가 과거의 성공과 실패 경험에 접근하여 성능을 향상시킬 수 있는지 탐구했습니다. 스위트벤치(SweetBench)와 같은 데이터 세트(data sets)를 분석하면서, 다중 턴 상호작용에서 전체 대화 창(conversation window)이 주어질 때 토큰(tokens) 수가 급증하고, 중요한 지침들이 무시되는 패턴(patterns)을 발견했습니다. 이는 분명한 문제였고, '컨텍스트 로트(context rot)' 기술 보고서(technical report)에 대한 연구 커뮤니티(community)의 반응은 "우리도 알고 있었다"는 식이었습니다. 이는 빌더(builders)들에게 AI 모델(model)의 실제 한계를 알리는 데 중요한 역할을 했습니다.

저는 연구소(labs)들을 비난할 의도는 없습니다. 모델(models) 개발은 매우 경쟁적인 분야이며, 각 연구소는 자신들이 가장 잘하는 벤치마크(benchmarks)에 맞춰 모델을 훈련하고 이를 마케팅(marketing)에 활용합니다. 대부분의 경우, 제품의 모든 장점과 단점을 솔직하게 공개할 동기는 부족합니다. 이러한 이유로 '건초 더미 속 바늘(needle in a haystack)'과 같은 과장된 마케팅(marketing)이 등장하며, 컨텍스트 창(context window)을 자유롭게 사용할 수 있다는 오해를 불러일으켰습니다. 언젠가 그것이 사실이 되기를 바라지만, 지금은 그렇지 않습니다.

**스윅스(Swyx) [00:17:43]:** 유튜브 비디오(YouTube video)에서 컨텍스트 로트(context rot) 보고서의 핵심인 그림 1 차트를 보여드리겠습니다. 이 차트에 따르면, 곡선 아래 면적(area under curve) 기준으로 소네트 4(Sonnet 4)가 가장 우수한 성능을 보입니다. 반면 GPC 4.1과 제미니 플래시(Gemini Flash)는 컨텍스트 길이(context length)가 길어질수록 성능이 급격히 저하되는 모습을 확인할 수 있습니다.
**제프(Jeff) [00:18:03]:** 저는 별다른 논평이 없습니다. 그것이 이 특정 작업에서 우리가 발견한 것입니다. 물론, 이것이 실제 개발자(developers)들의 경험에 어떻게 반영되는지는 또 다른 문제입니다. 클로드(Claude)에 대한 개발자들의 선호도와 이러한 결과는 어느 정도 상관관계가 있을 것으로 보입니다. 만약 이 결과가 사실이라면, 왜 그런지에 대한 중요한 설명이 여기에 있다고 생각합니다. 명확한 기준선과 지침을 따르는 것이 중요합니다.
**스윅스(Swyx) [00:18:27]:** 여기서는 완전히 답변되지 않았지만, 저는 추론 모델(reasoning models)이 컨텍스트 활용(context utilization)에 더 능숙하다는 이론도 가지고 있습니다. 이는 일반적인 자기회귀 모델(autoregressive models)이 단순히 왼쪽에서 오른쪽으로 진행하는 것과 달리, 추론 모델은 이전에 놓쳤을 수 있는 연결을 다시 찾아낼 수 있기 때문입니다. 하지만 오늘 발표된 논문은 그 반대를 보여주었다고 합니다.
**제프(Jeff) [00:18:49]:** 나중에 보내드리겠습니다.
**스윅스(Swyx) [00:18:50]:** 흥미롭네요.
**알레시오(Alessio) [00:18:52]:** 매일 새로운 논문이 쏟아져 나옵니다. 가장 인상 깊었던 점은 당신이 단순히 문제를 지적하고 "이것은 제대로 작동하지 않습니다"라고 말했을 뿐, 특정 제품을 팔려고 하지 않았다는 것입니다. 당신이 해결하고 싶은 문제와, 문제를 강조하기 위해 수행하는 연구, 그리고 다른 사람들이 참여하기를 바라는 연구에 대해 어떻게 생각하십니까? 당신이 이야기하는 모든 것이 기본적으로 크로마(Chroma) 로드맵(roadmap)에 있습니까? 아니면 그냥 사람들에게 "이것은 나쁩니다. 우회하세요. 하지만 우리에게 고쳐달라고 하지 마세요"라고 조언하는 것입니까?
**제프(Jeff) [00:19:20]:** 방금 전에 말했던 것으로 돌아가서, 크로마(Chroma)의 광범위한 임무는 애플리케이션(applications) 구축 과정을 연금술보다 엔지니어링(engineering)에 가깝게 만드는 것입니다. 그래서 그것은 꽤 광범위한 영역이지만, 우리는 작은 팀이고 너무 많은 것에 집중할 수 없습니다. 우리는 당분간 한 가지에 매우 집중하기로 선택했습니다. 그래서 저는 우리가 스스로 이 문제를 결정적으로 해결할 수 있다고 생각할 만큼 오만하지 않습니다. 매우 역동적이고 거대한 신흥 산업에서는 커뮤니티(community)가 필요하고, 모두 함께 일하는 사람들의 물결이 필요하다고 생각합니다. 우리는 의도적으로 이 연구에 상업적 동기가 없다는 것을 매우 명확히 하고 싶었습니다. 우리는 어떤 해결책도 제시하지 않습니다. 우리는 사람들에게 크로마(Chroma)를 사용하라고 말하지 않습니다. 그냥 여기 문제가 있습니다.
**스윅스(Swyx) [00:20:02]:** 암시되어 있습니다.
**제프(Jeff) [00:20:05]:** 물론, 긍정적인 암시가 있을 수 있다는 점에 대해 부정하지 않습니다. 하지만 여전히 중요한 것은 속도와 비용에 관계없이 해결해야 할 과제가 많다는 것입니다. 흥미로운 점은, 대규모 연구소(labs)들이 개발자(developers) 지원에 대한 동기가 점점 줄어든다는 것입니다. 주요 LLM 제공업체들의 시장은 주로 소비자(consumer)에 집중되어 있어, 개발자들을 돕는 것은 부차적인 관심사가 됩니다. 따라서 개발자들이 AI로 무언가를 구축하는 방법을 배우도록 돕는 데 적극적인 주체가 부족한 상황입니다. SaaS 회사(SaaS company)나 소비자 회사(consumer company)가 AI 네이티브(native)로 구축한다면, 그들의 핵심 기술을 외부에 공개하여 마케팅(marketing)할 이유가 없습니다. 이러한 상황에서 개발자들이 AI를 효과적으로 활용하도록 돕는 것은 분명히 우리가 기여할 수 있는 중요한 영역이라고 생각했습니다.

**스윅스(Swyx) [00:21:02]:** 소비자(consumer) 문제에 대해 반박하자면, 연구소(labs)라고 말씀하셨는데, 오픈AI(OpenAI)가 ChatGPT에 메모리(memory)를 구축하고 말 그대로 모든 사람에게 제공하는 것에 대해 생각하지 않으십니까? 제 생각에는 너무 노골적이라고 생각하지만, 그들은 메모리 활용(memory utilization)을 좋게 만드는 데 정말 신경 쓸 것입니다. 컨텍스트 활용(context utilization), 컨텍스트 엔지니어링(context engineering)은 그들에게도 중요하다고 생각합니다. 비록 그들이 소비자(consumer)만을 위해 구축하고 개발자(developers)에게는 신경 쓰지 않더라도 말입니다.
**제프(Jeff) [00:21:25]:** 네. 오늘날 얼마나 좋은지는 분명히 중요한 질문이지만, 그 질문은 건너뛰겠습니다. 설령 그렇다 하더라도, 그들이 실제로 그 연구 결과를 발표할까요? 아니요. 정확히 그렇습니다. 그것은 알파(alpha) 단계입니다. 왜 당신의 비밀을 누설하겠습니까? 네. 그래서 실제로 사업을 하는 회사는 거의 없다고 생각합니다. 그들은 개발자(developers)에게 AI로 유용한 것을 구축하는 방법을 가르치려고 노력하는 데 인센티브(incentive)를 가지고 있고 정말 신경 쓰는 위치에 있다고 생각합니다. 그래서 우리는 그런 인센티브(incentive)를 가지고 있다고 생각합니다.

**컨텍스트 품질(Context Quality) 우선순위 지정**
**알레시오(Alessio) [00:21:49]:** 하지만 이것이 다음 건초 더미 속 바늘(needle in a haystack)이 될 정도로 성장하여 모델(model) 제공업체들이 실제로 그것을 잘하도록 강제할 수 있다고 생각하십니까?
**제프(Jeff) [00:21:57]:** 누구에게든 무엇이든 강요할 방법은 없습니다. 그래서 우리는 이것을 준비할 때 그것에 대해 생각했습니다. "아마도 이것을 공식 벤치마크(benchmark)로 공식화하여 매우 쉽게 만들 수 있을 것이다"라고요. 우리는 모든 코드를 오픈 소스(open source)로 공개했습니다. 그래서 당신이 이것을 보고 있고 대규모 모델(model) 회사에서 왔다면, 당신은 이것을 할 수 있습니다. 아직 출시하지 않은 새 모델(model)을 가져와서 이 수치들을 실행할 수 있습니다. 그리고 저는 500만 토큰(tokens) 모델(model)보다 6만 컨텍스트(context), 토큰(token) 컨텍스트 창(context window)을 가지고 완벽하게 주의를 기울이고 완벽하게 추론할 수 있는 모델(model)을 선호할 것입니다. 개발자(developer)로서 전자가 후자보다 훨씬 더 가치 있습니다. 모델(model) 제공업체들이 이것을 중요하게 생각하고, 그것을 중심으로 훈련하고, 진행 상황을 평가하고, 개발자(developers)에게 소통하기를 진심으로 바랍니다. 그렇게 된다면 훌륭할 것입니다.
**알레시오(Alessio) [00:22:42]:** 이것이 더 나은 교훈이 될 것이라고 보십니까? 당신이 "모델(models)이 이 방법을 학습하지 않거나, 그 방법을 공개하지 않을 것"이라고 말하는 것은, 모델(model) API(API)를 통해서는 불가능하지만, ChatGPT와 같은 서비스에서는 구현될 수 있다는 의미인가요?
**제프(Jeff) [00:23:04]:** 무엇이 더 나은 교훈이 될지 아닐지 예측하는 것은 매우 위험합니다. 저는 추측하지 않겠습니다. 바라건대 AI 엔지니어(engineers)는 아닐 것입니다. 네. 바라건대 모든 인류는 아닐 것입니다. 모르겠습니다.
**스윅스(Swyx) [00:23:14]:** 저에게도 컨텍스트 엔지니어링(context engineering)을 중심으로 흥미로운 분야가 발전하고 있습니다. 랭체인(Langchain)의 랜스 마틴(Lance Martin)은 모든 다른 분리에 대한 정말 좋은 블로그 게시물을 작성했습니다. 그리고 뉴욕(New York)에서 당신은 첫 번째 밋업(meetup)을 주최했습니다. 샌프란시스코(San Francisco)에서도 하나를 할 예정입니다. 하지만 저는 그냥 궁금합니다. 현장에서 무엇을 보고 있습니까? 누가 흥미로운 작업을 하고 있습니까? 주요 논쟁은 무엇입니까? 그런 것들 말입니다.
**제프(Jeff) [00:23:37]:** 이것은 여전히… 많은 사람들이 아무것도 하지 않고 있습니다. 많은 사람들이 여전히 모든 것을 컨텍스트 창(context window)에 넣고 있습니다. 그것은 매우 인기가 많습니다. 그리고 컨텍스트 캐싱(context caching)을 사용하고 있으며, 그것은 분명히 도움이 되지만, 비용과 속도 문제가 있습니다. 하지만 컨텍스트 문제에는 전혀 도움이 되지 않습니다. 그래서 네, 아직 많은 모범 사례(best practices)가 마련되어 있다고 생각하지 않습니다. 몇 가지를 강조하겠습니다. 근본적인 문제는 아주 간단합니다. N개의 후보 청크(chunks)가 있고, Y개의 사용 가능한 공간이 있습니다. 그리고 1만 개 또는 10만 개 또는 100만 개의 후보 청크(chunks) 중에서 이 정확한 단계에 필요한 20개를 선별하고 줄이는 과정을 수행해야 합니다. 이 최적화 문제(optimization problem)는 많은 애플리케이션(applications)과 산업에서 새로운 문제가 아닙니다. 고전적인 문제입니다. 그리고 물론, 사람들이 그 문제를 해결하기 위해 사용하는 도구는 여전히 매우 초기 단계라고 생각합니다. 몇 가지 주목할 만한 패턴(patterns)이 있습니다. 많은 개발자들이 1단계 검색(first stage retrieval)을 통해 대규모 데이터 축소를 수행합니다. 이는 벡터 검색(vector search), 전체 텍스트 검색(full text search), 메타데이터 필터링(metadata filtering) 등을 활용하여 수만 개의 후보를 300개 정도로 줄이는 방식입니다. LLM에 모든 것을 제공할 필요 없이, 이 300개의 후보 중에서 LLM을 재순위화기(re-ranker)로 사용하여 최종 30개 정도로 압축하는 방식이 부상하고 있습니다. 놀랍게도 이 방법은 많은 사람들이 생각하는 것보다 훨씬 비용 효율적입니다. 실제로 LLM을 재순위화기(re-ranker)로 활용하는 경우가 많으며, 이는 전용 재순위화기(re-ranker) 모델(models)보다 더 유연한 접근 방식입니다. LLM의 속도와 비용 효율성이 획기적으로 개선된다면, LLM 기반의 재순위화(re-ranking)가 지배적인 패러다임(paradigm)이 될 것입니다. 오늘날 병렬 LLM 호출에는 지연 시간(tail latency) 문제가 있지만, 이러한 제약도 점차 해소될 것입니다. 이러한 패턴(patterns)은 최근 몇 달간 주목받기 시작했으며, 미래의 검색 시스템(retrieval systems)을 이끌어갈 중요한 방향이라고 생각합니다.

**코드 인덱싱(Code Indexing) 및 검색 전략(Retrieval Strategies)**
**스윅스(Swyx) [00:27:02]:** 코드 인덱싱(code indexing)에 대해 잠시 이야기해 보죠. 우리가 논의한 모든 컨텍스트(contexts) 관리 기법은 코드에도 적용될 수 있습니다. 코드는 분명히 독특한 형태의 컨텍스트(context)이자 인덱싱(indexing) 대상 코퍼스(corpus)입니다. 이전 에피소드에서 클라우드 코드 담당자들이 코드베이스(code base)를 직접 임베딩(embed)하거나 인덱싱(index)하기보다는, 도구를 활용하여 코드 검색(code search)을 수행한다고 언급한 바 있습니다. 저는 이것이 미래의 컨텍스트 검색(context retrieval) 패러다임(paradigm)이 되어야 하는지에 대해 종종 고민합니다. 에이전트(agent)를 구축할 때, 재귀적 재순위화기(re-rankers)나 요약기(summarizers)와 같은 도구를 가진 다른 에이전트를 호출하는 방식이 효과적일까요, 아니면 이 모든 것을 단일 에이전트에 통합해야 할까요? '에이전트'의 정의가 아직 불분명하지만, 이에 대한 의견이 궁금합니다.
**제프(Jeff) [00:27:47]:** 알겠습니다. 그것을 분해해 보겠습니다. 인덱싱(indexing)은 정의상 트레이드오프(trade-off)입니다. 데이터를 인덱싱(indexing)할 때, 쓰기 시간 성능을 희생하여 쿼리(query) 시간 성능을 향상시키는 것입니다. 데이터 세트(data sets)가 커질수록 이러한 이점은 더욱 두드러집니다. 예를 들어, 작은 코드베이스(code bases)에서는 단순한 '그레핑(grepping)'으로 충분할 수 있지만, 방대한 오픈 소스(open source) 종속성을 검색할 때는 인덱싱이 필수적입니다. 임베딩(embeddings)은 의미론적 유사성(semantic similarity)을 통해 정보를 압축하는 강력한 도구이며, 코드 임베딩(embeddings)은 아직 초기 단계지만 잠재력이 크다고 생각합니다. 반면, 정규식(regex)은 분명히 엄청나게 가치 있는 도구이며, 우리는 실제로 크로마(Chroma) 내부에서 단일 노드(single node)와 분산 환경 모두에서 정규식 검색(regex search)을 기본적으로 지원합니다. 그래서 크로마(Chroma) 내부에서 정규식 검색(regex search)을 할 수 있습니다. 왜냐하면 우리는 그것을 코드 검색(code search)을 위한 매우 강력한 도구로 보았기 때문입니다. 훌륭합니다. 그리고 우리는 대규모 데이터 볼륨(data volumes)에서 정규식 검색(regex search)을 빠르게 하기 위해 인덱스(indexes)를 구축합니다. 당신이 언급한 코딩 사용 사례에서 말입니다.

크로마(Chroma)에 추가한 또 다른 기능은 포킹(forking) 기능입니다. 이 기능을 통해 기존 인덱스(index)의 복사본을 100밀리초(milliseconds) 미만에 저렴한 비용으로 생성할 수 있습니다. 이는 변경된 파일에 대한 차이점(diff)을 새로운 인덱스(index)에 빠르게 적용할 수 있게 하여, 데이터(data) 코퍼스(corpus)의 빠른 재인덱싱(re-indexing)을 가능하게 합니다. 결과적으로 각 커밋(commit)별 인덱스를 관리하고, 다른 브랜치(branches)나 릴리스 태그(release tags)를 쉽게 검색할 수 있게 됩니다. 즉, 버전 관리되는 모든 데이터(data) 코퍼스(corpus)를 쉽고 효율적으로 검색할 수 있습니다. 이것이 제가 정규식(regex)과 인덱싱(indexing) 및 임베딩(embeddings)에 대해 생각하는 방식입니다. 이 분야는 계속해서 발전하고 있으며, '정답'을 주장하는 이들의 말은 경계해야 한다고 생각합니다.
**제프(Jeff) [00:30:02]:** 코드 임베딩(embeddings)이 저평가되어 있다고 말씀드렸는데, 그 이유는 대부분의 사람들이 일반적인 임베딩 모델(embedding models)을 코드에 바로 적용하려 하기 때문입니다. 이는 일부 사용 사례에서는 효과적이지만, 모든 상황에서 최적의 성능을 보장하지는 않습니다. 핵심은 '신호(signal)를 찾는 것'이며, 텍스트 검색(text search)은 쿼리(query) 작성자가 데이터(data)를 명확히 알 때 매우 효과적입니다. 예를 들어, 구글 드라이브(Google Drive)에서 '자본금 테이블(cap table)' 스프레드시트를 찾을 때, 파일명을 아는 경우 전체 텍스트 검색(full text search)이 완벽합니다. 하지만 파일명을 모른다면 "모든 투자자 목록이 있는 스프레드시트"라고 검색할 것이고, 이때 임베딩 공간(embedding space)의 의미론적 유사성(semantic similarity)이 빛을 발합니다. 결국, 이는 상황과 쿼리(query) 작성자의 데이터 전문성에 따라 적절한 도구 조합을 선택해야 한다는 의미입니다. 코드 검색(code search)의 경우, 여전히 정규식(Regex)이 쿼리(queries)의 상당 부분을 만족시키지만, 임베딩(embeddings)을 함께 사용하면 추가적인 개선 효과를 얻을 수 있습니다. 최고 수준의 소프트웨어(software)를 구축하려는 기업에게는, 80%의 쉬운 부분을 넘어 나머지 20%를 최적화하는 것이 중요하며, 작은 개선점 하나하나가 사용자(users)에게 더 나은 가치를 제공하는 핵심이 됩니다.

**코드 청크 재작성(Chunk Rewriting) 및 쿼리 최적화(Query Optimization)**
**알레시오(Alessio) [00:32:04]:** 개발자 경험(developer experience)과 에이전트 경험(agent experience) 사이의 균형에 대해 어떻게 생각하십니까? 코드를 임베딩(embed)하기 위해 더 쉽게 재포맷(reformat)하고 재작성한 다음 모델(models)을 훈련하는 방식이 효과적일까요? 이 스펙트럼(spectrum)에서 당신의 입장은 어디에 있습니까?
**제프(Jeff) [00:32:19]:** 음, 일부 사용 사례에서 잘 작동하는 것을 본 도구 중 하나는 코드를 임베딩(embed)하는 대신, 먼저 LLM이 이 코드가 무엇을 하는지에 대한 자연어 설명(natural language description)을 생성하도록 하는 것입니다. 그리고 자연어 설명만 임베딩(embed)하거나, 그것과 코드를 함께 임베딩(embed)하거나, 별도로 임베딩(embed)하여 별도의 벡터 검색(vector search) 인덱스(indexes)에 넣는 것입니다. 청크 재작성(chunk rewriting)은 그것이 무엇인지에 대한 광범위한 범주입니다. 다시 말하지만, 여기서 아이디어는 인덱싱(indexing)과 관련이 있다는 것입니다. 즉, 쓰기 또는 수집 파이프라인(pipeline)에 넣을 수 있는 구조화된 정보(structured information)는 가능한 한 많이 넣어야 합니다. 추출할 수 있는 모든 메타데이터(metadata)는 수집 시점에 하세요. 할 수 있는 모든 청크 재작성(chunk rewriting)은 수집 시점에 하세요. 수집 측에서 가능한 한 많은 신호(signals)를 추출하고 미리 구워내려고 노력하는 데 정말 투자한다면, 다운스트림 쿼리(query) 작업이 훨씬 쉬워진다고 생각합니다.

하지만 또한, 우리가 여기에 있으니 말할 가치가 있습니다. 사람들은 작동하기를 원하는 쿼리(queries)와 반환해야 하는 청크(chunks)의 작은 골든 데이터 세트(golden data sets)를 만들어야 합니다. 그리고 나서 무엇이 중요한지 정량적으로 평가할 수 있습니다. 아마도 당신의 애플리케이션(application)에 많은 화려한 것들을 할 필요가 없을 수도 있습니다. 사용 사례에 따라 정규식(regex)만 사용하거나 벡터 검색(vector search)만 사용하는 것이 당신에게 필요한 전부일 수도 있습니다. 다시 말하지만, 답을 안다고 주장하는 사람은 누구든, 가장 먼저 물어봐야 할 것은 "당신의 데이터(data)를 보여줘"입니다. 그리고 만약 그들이 데이터(data)가 없다면, 당신은 이미 답을 가지고 있는 것입니다.
**스윅스(Swyx) [00:33:47]:** 당신이 컨퍼런스(conference)에서 했던 강연에 대해 칭찬하겠습니다. "데이터(data)를 보는 방법"입니다. 네. 데이터(data)를 보는 것은 중요합니다. 골든 데이터 세트(golden data sets)를 가지는 것은 모두 좋은 관행(practices)입니다. 누군가가 작은 팸플릿(pamphlet)에 넣어 "AI 엔지니어링(engineering)의 10계명"이라고 부르면 좋을 것 같습니다.
**제프(Jeff) [00:34:07]:** 이제 메모리(memory)로 넘어갈 예정이지만, 거기서 마무리하고 싶습니다.
**스윅스(Swyx) [00:34:09]:** 당신이 항상 열변을 토하고 싶어 하는 다른 주제에 대해 여지를 남겨두고 싶습니다. 네, 그것은 위험한 질문입니다.

**트랜스포머(Transformer) 아키텍처(Architecture) 진화 및 검색 시스템(Retrieval Systems)**
**스윅스(Swyx) [00:34:18]:** 제가 궁금한 점이 있습니다. 원래 트랜스포머(transformer)는 인코더-디코더(encoder-decoder) 아키텍처(architecture)였지만, GPT는 대부분의 트랜스포머(transformers)를 디코더(decoder) 전용으로 변환했습니다. 동시에 우리는 모든 임베딩 모델(embedding models)을 인코더(encoder) 전용 모델(models)로 활용하고 있습니다. 이는 마치 트랜스포머(transformer)를 분리하여, 먼저 인코더(encoder) 전용 모델(model)로 데이터를 인코딩(encoding)한 후 크로마(Chroma)와 같은 벡터 데이터베이스(vector database)에 저장하고, 이후 LLM으로 디코딩(decoding)하는 방식과 같습니다. 저는 이것이 전체 아키텍처(architecture)에 대한 흥미로운 메타 학습(meta-learning)이자, 모델(model)에서 시스템(system)으로의 진화라고 생각합니다. 이에 대한 당신의 생각은 어떻습니까?
**제프(Jeff) [00:35:20]:** 오늘날 우리가 작업을 수행하는 방식은 5년에서 10년 후에는 매우 조잡하고 원시적으로 느껴질 것이라는 직관이 있습니다. 왜 우리는 다시 자연어로 돌아가야 할까요? 임베딩(embeddings)을 모델(models)의 잠재 공간(latent space)에 직접 전달하여 활용하면 안 될까요? 미래의 검색 시스템(retrieval systems)에 대해 몇 가지 예측할 수 있는 것이 있습니다. 첫째, 시스템은 잠재 공간(latent space) 내에서 작동하며 자연어 변환을 거치지 않을 것입니다. 둘째, 오랫동안 우리는 생성(generation)당 한 번의 검색(retrieval)을 수행했습니다. 하지만 앞으로는 필요할 때마다 지속적으로 검색(retrieve)하는 방식이 보편화될 것입니다. 최근에 RAG R1이라는 논문에서 DCR1이 검색(retrieve)하는 방법을 학습하는 사례를 보았습니다. 이는 내부적인 사고의 연쇄 과정에서 컴퓨트(compute)를 집중적으로 활용하여 검색(search)을 수행하는 방식입니다.
**스윅스(Swyx) [00:36:22]:** 검색 증강 언어 모델(retrieval augmented language models)도 있습니다.
**제프(Jeff) [00:36:24]:** 이것은 더 오래된 논문이라고 생각합니다. 네. 네. REALM과 RETRO 등 많은 역사가 있습니다.
**스윅스(Swyx) [00:36:31]:** 어쩐지 그렇게 인기가 많지는 않습니다.
**제프(Jeff) [00:36:32]:** 왜 그런지 모르겠습니다. 어쩐지 그렇게 인기가 많지는 않습니다. 음, 많은 것들이 검색기(retriever)나 언어 모델(language model)이 고정되어야 하는 문제가 있습니다. 그리고 코퍼스(corpus)는 변경될 수 없는데, 대부분의 개발자(developers)는 개발자 경험(developer experience)을 다루고 싶어 하지 않습니다.
**스윅스(Swyx) [00:36:45]:** 이득이 그렇게 높다면 우리는 할 것이라고 말하고 싶습니다. 아니면 연구소(labs)가 당신이 그렇게 하는 것을 원하지 않습니다. 모르겠습니다.
**제프(Jeff) [00:36:54]:** 연구소(labs)는 엄청난 영향력을 가지고 있습니다. 엄청난 영향력을 가지고 있습니다. 또한 그것을 함으로써 점수를 얻지 못한다고 생각합니다. 그냥, 아무도 신경 쓰지 않습니다. 지위 게임은 당신의 문제를 해결하는 것에 대해 보상하지 않습니다. 그래서 네. 그래서 광범위하게. 지속적인 검색(retrieval)이 현장에 나오는 것을 보는 것은 흥미로울 것입니다. 첫째, 둘째, 임베딩 공간(embedding space)에 머무르는 것은 매우 흥미로울 것입니다. 그리고 네, GPU와 GPU 메모리(memory)에 정보를 페이징(paging)하는 방식에 대해서도 흥미로운 점이 있습니다. 훨씬 더 효율적으로 할 수 있다고 생각합니다. 음, 이것은 5년 또는 10년 후의 미래에 대해 우리가 생각하는 것이지만, 네, 우리가 되돌아볼 때, 오늘날 우리가 일을 하는 방식은 우스꽝스럽게 조잡했다고 생각합니다.
**스윅스(Swyx) [00:37:34]:** 아마도 아닐 수도 있습니다. 우리는 IMO, 언어만으로 문제를 해결하고 있습니다. 네, 훌륭합니다. 저는 여전히 그것의 함의를 연구하고 있습니다. 그것은 여전히 엄청난 성과이지만, 제가 생각했던 방식과는 매우 다릅니다.
**알레시오(Alessio) [00:37:47]:** 메모리(memory)가 컨텍스트 엔지니어링(context engineering)의 이점이라고 말씀하셨습니다. 당신은 "메모리(memory)를 너무 복잡하게 만들지 마세요"라는 무작위 트윗(Twitter)을 올렸습니다. 메모리(memory)에 대해 어떻게 생각하십니까? 그리고 우리가 연결하지 못했을 수도 있는 컨텍스트 엔지니어링(context engineering)의 다른 이점은 무엇입니까?

**컨텍스트 엔지니어링(Context Engineering)의 이점으로서의 메모리(Memory)**
**제프(Jeff) [00:38:06]:** 메모리(memory)는 좋은 용어라고 생각합니다. 많은 사람들에게 매우 읽기 쉽습니다. 다시 말하지만, 이것은 LLM의 의인화를 계속하는 것입니다. 우리는 우리 자신이 어떻게 기억을 사용하는지 이해합니다. 우리는 작업을 수행하는 방법을 배우기 위해 기억을 사용하는 데 매우 능숙합니다. 그리고 그 학습은 새로운 환경에 유연하게 적용됩니다. 그리고 AI를 가져와서 AI 옆에 앉아 10분 또는 몇 시간 동안 지시하는 아이디어는, 당신이 원하는 것을 말하면 AI가 그것을 수행하고, 당신은 "다음번에는 이렇게 해"라고 말하는 것입니다. 인간에게 하는 것과 똑같이 말입니다. 그 10분 또는 몇 시간 후에 AI는 이제 인간이 할 수 있는 것과 같은 신뢰성 수준으로 그것을 할 수 있습니다. 이것은 엄청나게 매력적이고 흥미로운 비전(vision)입니다. 저는 그것이 일어날 것이라고 생각합니다. 그리고 메모리(memory)는 모든 사람이 이해할 수 있는 용어라고 생각합니다. 우리 모두는 우리 엄마들도 이해합니다. 그리고 메모리(memory)의 이점도 매우 매력적입니다. 하지만 메모리(memory)의 내부 작동 방식은 무엇입니까? 그것은 여전히 컨텍스트 엔지니어링(context engineering)일 뿐이라고 생각합니다. 즉, 컨텍스트 창(context window)에 올바른 정보를 넣는 방법의 영역입니다. 그래서 네, 저는 메모리(memory)를 이점으로 생각하고, 컨텍스트 엔지니어링(context engineering)은 그 이점을 제공하는 도구라고 생각합니다. 그리고 다른 것들도 있을 수 있습니다. 즉, 메모리(memory)의 어떤 버전에서는 "아, 당신은 실제로 데이터(data)를 통해 모델(model)을 개선하기 위해 RL을 사용하고 있다"와 같을 수도 있습니다. 그래서 저는 컨텍스트(context)만 변경하는 것이 작업을 훌륭하게 수행하는 유일한 도구라고 주장하는 것은 아닙니다. 하지만 그것은 매우 중요한 부분이라고 생각합니다.
**알레시오(Alessio) [00:39:43]:** 이 대화를 바탕으로 암묵적인 선호는 무엇인가와 같은 메모리(memory)를 합성하는 것과, 이 프롬프트(prompt)를 바탕으로 어떤 메모리(memory)를 넣어야 하는가라는 다른 측면 사이에 큰 차이가 있다고 생각하십니까?
**제프(Jeff) [00:39:58]:** 저는 그것들이 모두 동일한 데이터(data)에 의해 공급될 것이라고 생각합니다. 더 나은 검색(retrieve) 방법을 알려주는 동일한 피드백 신호(feedback signals)가 무엇을 더 잘 기억해야 하는지도 알려줄 것입니다. 그래서 저는 그것들이 실제로 다른 문제라고 생각하지 않습니다. 저는 그것들이 동일한 문제라고 생각합니다.

**AI 메모리(Memory) 구조화 및 오프라인 압축(Offline Compaction)**
**스윅스(Swyx) [00:40:13]:** 저에게는 좀 더 씨름하고 있는 것은 메모리(memory)의 구조가 무엇인가 하는 것입니다. 그것은 의미가 있습니다. 그래서 장기 기억, 단기 기억과 같은 모든 비유가 분명히 있습니다. 수면과 관련된 무언가를 만들려고 노력해 봅시다. 저는 LLM이 잠자는 동안 어떤 종류의 배치 수집 주기, 아마도 가비지 컬렉션(garbage collection) 주기가 있어야 한다고 생각합니다. 하지만 무엇이 의미가 있는지 모르겠습니다. 우리는 인간이 어떻게 작동한다고 생각하는지에 기반하여 모든 비유를 만들고 있지만, AI는 같은 방식으로 작동하지 않을 수도 있습니다. 네. 당신이 본 것 중에 작동하는 것이 있는지 궁금합니다.
**제프(Jeff) [00:40:48]:** 네, 저는 항상, 다시 말하지만, 이 대화의 일관된 흐름으로서, 새로운 개념과 새로운 약어를 만들고, 갑자기 "여기 10가지 유형의 메모리(memory)가 있습니다"와 같은 정보 차트가 있을 때 약간 불안해집니다. 그리고 당신은 "왜?"라고 생각합니다. 이것들은 실제로 자세히 보면 모두 같은 것입니다. 달라야 할까요? 당신은 사람들의 마음을 날려버려야 합니다. 아니요, 그렇지 않다고 생각합니다. 모르겠습니다. 슬롯 머신(slot machine)과 슬롯 머신(slot machine)을 저항해야 합니다.
**제프(Jeff) [00:41:16]:** 압축(compaction)은 항상 유용한 개념이었습니다. 심지어 데이터베이스(databases)에서도요. 컴퓨터의 데이터베이스(databases)에서 우리는 모두 1998년에 윈도우(Windows) 머신에서 디스크 조각 모음(Defrag)을 실행했던 것을 기억합니다. 그래서 네, 다시 말하지만. 우리 중 일부는 그렇게 할 만큼 나이가 많지 않습니다. 저는 그렇습니다. 이 테이블에는 없습니다. 그리고 네. 분명히, 분명히 오프라인 처리(offline processing)는 도움이 되며, 이 경우에도 도움이 된다고 생각합니다. 그리고 이전에 이야기했듯이, 인덱싱(indexing)의 목표는 무엇입니까? 인덱싱(indexing)의 목표는 쓰기 시간 성능을 쿼리(query) 시간 성능과 교환하는 것입니다. 압축(compaction)은 쓰기 시간 성능과 같은 도구 상자의 또 다른 도구입니다. 데이터를 재인덱싱(re-indexing)하는 것입니다.
**스윅스(Swyx) [00:41:52]:** 인덱싱(indexing)이 아니라 실제로 인덱싱(indexing)입니다.
**제프(Jeff) [00:41:55]:** 재인덱싱(re-indexing