**AI 엔지니어 가을 서밋: 컨텍스트 엔지니어링의 미래와 Chroma의 역할**

오는 11월 20일부터 21일까지 뉴욕(NYC)에서 AI 엔지니어 가을 서밋(AI Engineer Fall Summit)이 개최됩니다! 지금 바로 등록하세요. 지난 2023년 12월, 저희는 'AI의 4대 전쟁(The Four Wars of AI)'과 'RAG/Ops 전쟁(RAG/Ops War)'을 심층적으로 다뤘습니다. 그 이후로 벡터 데이터베이스(vector databases)에 막대한 투자가 이어지고 기술적 기대감의 등락을 경험한 끝에, 오늘 크로마(Chroma)의 창립자 제프 휴버(Jeff Huber)가 저희와 함께 "RAG는 이미 끝났다"는 파격적인 견해를 제시합니다. AI 기술의 발전 속도가 가속화되면서, LLM(대규모 언어 모델)의 컨텍스트 길이(context lengths)는 지속적으로 확장되고 있으며, AI 워크로드(AI workloads)는 단순한 챗봇(chatbots)을 넘어 더욱 복잡하고 영향력 있는 에이전트(IMPACTful agents)로 진화하고 있습니다. 이러한 변화의 흐름 속에서 랜스 마틴(Lance Martin)과 덱스 호시(Dex Horthy) 같은 선도적인 사상가들(thoughtleaders)의 새로운 연구는 그동안 저평가되었던 컨텍스트 박스(context box)의 중요성을 재조명하며 실질적인 혁신을 이끌고 있습니다.

[https://rlancemartin.github.io/2025/06/23/context_engineering/](https://rlancemartin.github.io/2025/06/23/context_engineering/)

크로마(Chroma)는 컨텍스트 로트(Context Rot) 및 생성형 벤치마킹(Generative Benchmarking) 보고서 발간을 포함하여, 새로운 컨텍스트 엔지니어링(context engineering) 분야에서 가장 주목할 만한 연구를 선도해 왔습니다. 본 서밋에서는 검색(retrieval)의 현주소, AI 메모리(memory)의 역할, 그리고 검색 벤치마킹(retrieval benchmarking)의 중요성 등 다양한 주제를 심도 있게 다룰 예정입니다.

**효율적인 검색을 위한 5가지 전략적 조언**

*   RAG 시스템 대신, 핵심적인 검색(retrieval) 기능에 집중하여 배포하십시오.
*   검색의 기초 구성 요소들(primitives)을 명확히 정의하세요 (예: 고밀도(dense) 임베딩, 어휘 기반(lexical) 검색, 필터링(filters), 재정렬(re-rank), 최종 구성(assembly), 그리고 평가 주기(eval loop)).
*   하이브리드 리콜(hybrid recall)을 통해 초기 단계에서 우위를 점하십시오 (200-300개의 후보는 LLM이 처리하기에 적합합니다).
*   최종 컨텍스트(context)를 구성하기 전에 항상 재정렬(re-rank) 과정을 거치세요.
*   컨텍스트 로트(context rot) 현상을 인지하고, 최대 길이의 창(maximal windows)보다 간결하고 구조화된 컨텍스트(contexts)를 우선하십시오.
*   정기적으로 리소스(resouces)를 투자하여 소규모의 '골드 세트(gold set)'를 구축하고, 이를 CI(Continuous Integration) 및 대시보드(dashboards)와 연동하여 지속적인 품질 관리를 수행하십시오.

**고도화된 컨텍스트 파이프라인 설계**

AI 애플리케이션의 성능은 컨텍스트 파이프라인의 견고함에 달려 있습니다. 다음은 각 단계별로 고려해야 할 핵심 요소들입니다.

**[수집(Ingest)]**
├─ **파싱(Parse) + 청킹(chunk) (도메인 인식(domain-aware): 헤딩(headings), 코드 블록(code blocks), 테이블(tables))**: 단순한 텍스트 분할을 넘어, 문서의 논리적 구조(예: 헤딩, 코드 블록, 표)를 이해하고 보존하는 도메인 특화 청킹(domain-specific chunking)이 필수적입니다. 이는 검색의 정확도를 크게 향상시킵니다.
├─ **보강(Enrich): 제목(titles), 앵커(anchors), 심볼(symbols), 메타데이터(metadata)**: 원본 콘텐츠에서 추출 가능한 모든 메타데이터(metadata)를 추가하여 검색 가능성을 높입니다. 이는 쿼리 시 필터링(filtering) 및 재순위화(re-ranking)의 중요한 기반이 됩니다.
├─ **선택 사항: LLM "청크 요약(chunk summaries)" (코드/API용 자연어 주석(NL glosses))**: LLM을 활용하여 각 청크의 핵심 내용을 요약하거나, 코드/API에 대한 자연어 설명(natural language glosses)을 생성함으로써 정보 밀도를 높이고 다양한 쿼리에 대응할 수 있습니다.
├─ **임베딩(Embeddings) (밀집(dense)) + 선택적 희소 신호(sparse signals)**: 텍스트를 의미론적 벡터(semantic vectors)로 변환하는 밀집 임베딩(dense embeddings)은 필수적입니다. 여기에 키워드(keywords)나 정규식(regex) 같은 희소 신호(sparse signals)를 결합하여 하이브리드 검색의 효율성을 극대화합니다.
└─ **DB에 쓰기(text, vectors, metadata)**: 원본 텍스트, 생성된 벡터, 그리고 모든 메타데이터를 효율적으로 저장하여 빠른 검색과 관리가 가능하도록 합니다.

**[쿼리(Query)]**
├─ **1단계 하이브리드(First-stage hybrid): 벡터(vector) + 어휘/정규식(lexical/regex) + 메타데이터 필터(metadata filters)**: 초기 검색 단계에서는 벡터 유사도, 키워드 매칭(lexical search), 정규식(regex) 패턴, 그리고 메타데이터 필터(metadata filters)를 복합적으로 사용하여 광범위한 후보군을 빠르게 식별합니다.
├─ **후보 풀(Candidate pool): ~100–300**: 1단계 검색에서 도출된 후보 문서의 수는 LLM이 효율적으로 처리할 수 있는 범위 내에서 관리되어야 합니다.
├─ **재순위화(Re-rank) (LLM 또는 크로스 인코더(cross-encoder)) → 상위 ~20–40**: LLM 또는 특화된 크로스 인코더(cross-encoder)를 활용하여 후보군 내의 문서들을 쿼리 관련성에 따라 다시 정렬합니다. 이는 최종 LLM에 전달될 컨텍스트의 품질을 결정하는 중요한 단계입니다.
└─ **컨텍스트 어셈블리(Context assembly)**:
    -   **지침/시스템 프롬프트(system prompt) 우선**: LLM에 전달되는 컨텍스트는 항상 시스템 프롬프트(system prompt)의 지침을 최우선으로 고려하여 구성되어야 합니다.
    -   **중복 제거/유사 중복 병합**: 유사하거나 중복되는 정보를 제거하거나 병합하여 컨텍스트의 효율성을 높입니다.
    -   **소스 다양화**: 단일 소스에 의존하기보다 다양한 출처의 정보를 포함하여 LLM의 편향을 줄이고 답변의 견고성을 높입니다.
    -   **토큰(tokens)에 대한 하드캡(hard cap)**: LLM의 컨텍스트 창(context window) 한계를 초과하지 않도록 토큰(tokens) 수에 대한 엄격한 상한선(hard cap)을 설정합니다.

**[외부 루프(Outer loop)]**
├─ **캐시/비용 가드레일(guardrails)**: 반복되는 쿼리에 대한 응답 시간을 단축하고 비용을 절감하기 위한 캐싱 전략을 구현합니다.
├─ **작은 골드 세트(gold sets)에 대한 생성형 벤치마킹(Generative benchmarking)**: 소규모의 고품질 '골드 세트(gold sets)'를 활용하여 검색 시스템의 성능을 정량적으로 평가하고 지속적으로 개선합니다.
├─ **오류 분석(Error analysis) → 재청킹(re-chunk)/필터 재조정/재순위화 프롬프트(re-rank prompt)**: 시스템의 오류를 분석하여 청킹 전략, 필터 설정, 재순위화 프롬프트(re-rank prompt) 등을 반복적으로 조정하여 최적의 성능을 달성합니다.
└─ **메모리/압축(compaction): 상호작용 트레이스(interaction traces) 요약 → 검색 가능한 사실**: 사용자 상호작용 기록(interaction traces)을 요약하고 압축하여 장기 메모리로 활용될 수 있는 검색 가능한 사실(retrievable facts)을 생성합니다.

저희 팟캐스트 스튜디오(podcast studio)는 크로마(Chroma) 사무실에 있어서, 드디어 건물주를 게스트로 모시게 되어 기뻤습니다! 즐겁게 들어주세요!

**Chroma의 탄생 배경과 철학**

크로마(Chroma)의 탄생 배경은 응용 기계 학습(applied machine learning) 분야에서 오랜 경험을 쌓으면서 겪었던 난관 때문입니다. 즉, 시연(demos)은 비교적 쉽게 구현할 수 있었으나, 실제 운영 환경(production)에서 안정적인 시스템(system)을 구축하는 것은 극도로 복잡했습니다. 이러한 시연과 실제 서비스 간의 간극은 공학적인 문제라기보다는 미지의 영역, 즉 연금술에 가깝게 느껴졌습니다. 2021년과 2022년 당시, 우리는 이러한 비효율적인 상황을 목격하며 "데모와 프로덕션 사이의 간극을 어떻게 엔지니어링의 영역으로 가져올 수 있을까?"라는 질문을 던졌습니다.

Chroma는 개발자들이 AI 애플리케이션을 손쉽게 구축하고, 데모 단계에서 프로덕션 단계로 나아가는 과정을 연금술이 아닌 견고한 엔지니어링처럼 느끼게 돕는 것을 목표로 합니다. 벡터 데이터베이스를 만드는 것은 단순한 부가 작업(side quest)이 아니라, 이 메인 퀘스트(main quest)의 필수적인 부분입니다. 우리는 AI 애플리케이션의 핵심 워크로드(workload)가 '검색(search)'이라는 것을 깨달았고, 이에 대한 세계 최고 수준의 솔루션을 제공하는 데 집중했습니다. 오늘날 Chroma는 AI를 위한 현대적인 검색 인프라(search infrastructure)를 구축하며, AI 애플리케이션을 위한 강력한 검색 엔진(retrieval engine)을 제공하고 있습니다.

**정보 검색(IR)과 AI 시대의 검색(Search)**

AI 시대를 위한 현대적인 검색 인프라(search infrastructure)는 기존의 검색 시스템과는 확연히 다릅니다. '현대적'이라는 의미는 지난 5~10년간 발전한 분산 시스템(distributed systems) 기술을 기반으로 한다는 뜻입니다. Chroma는 러스트(Rust)로 구현되었으며, 읽기와 쓰기 분리(read/write separation), 스토리지(storage)와 컴퓨트(compute) 분리, 완전한 멀티 테넌트(multi-tenant) 아키텍처를 특징으로 합니다. 객체 스토리지(object storage)를 핵심 데이터 계층으로 활용하며, Chroma Cloud는 분산 환경에서 운영됩니다.

'AI를 위한' 검색은 네 가지 측면에서 중요합니다. 첫째, 검색에 사용되는 도구와 기술(technology)이 기존 시스템과 다릅니다. 둘째, 처리해야 할 워크로드(workload)의 성격이 다릅니다. 셋째, 이 시스템을 사용하는 개발자(developer)의 기대치와 요구사항이 다릅니다. 넷째, 그리고 가장 중요한 것은, 검색 결과를 최종적으로 '소비하는 주체'가 달라졌다는 점입니다. 전통적인 검색에서는 사람이 검색 결과를 해석하고 클릭하며 정보를 요약했습니다. 하지만 이제는 LLM이 이 역할을 수행합니다. 인간은 10개의 파란색 링크만 소화할 수 있지만, LLM은 훨씬 더 방대한 양의 정보를 처리하고 통합할 수 있습니다. 이러한 변화는 시스템 설계의 모든 측면에 영향을 미칩니다.

**경쟁 속 집중과 Chroma Cloud의 가치**

2023년은 벡터 데이터베이스(VectorDB) 분야가 뜨겁게 달아올랐던 한 해였습니다. 수많은 투자와 경쟁 속에서 Chroma는 자신만의 비전을 고수하며 핵심 가치에 집중했습니다. 제프 휴버는 흔히 '린 스타트업(lean startup)' 방식이 시장의 신호(signal)에 따라 빠르게 움직이지만, 때로는 표면적인 요구에만 응답할 수 있다고 지적합니다. Chroma는 확고한 비전과 역발상적인 관점을 가지고, '최고의 개발자 경험(developer experience)'을 제공하는 것에 집중했습니다.

Chroma Cloud의 개발은 이러한 철학의 정점입니다. `pip install ChromaDB`처럼 간단하게 시작할 수 있었던 단일 노드(single node) 버전의 편리함을 클라우드 환경에서도 구현하고자 했습니다. 이는 단순히 호스팅(hosted service)을 제공하는 것을 넘어, '제로 구성(zero config)', '항상 빠르고 비용 효율적이며 최신 상태 유지'라는 원칙을 지키는 것을 의미했습니다. 개발자가 노드(nodes) 수, 샤딩(sharding) 전략, 백업(backup) 등에 대해 고민할 필요 없이, 오직 애플리케이션 개발에만 집중할 수 있도록 하는 것이 목표였습니다. Chroma Distributed는 동일한 모노레포(monorepo)에 포함된 오픈 소스(Apache 2) 프로젝트로, Chroma Cloud는 이를 기반으로 운영됩니다. 사용자들은 30초 이내에 데이터베이스를 생성하고 데이터를 로드할 수 있으며, 사용량 기반 청구(usage-based billing)를 통해 효율적인 비용 관리가 가능합니다. 이는 사실상 서버리스 컴퓨트 플랫폼(serverless compute platform)을 구축하는 것과 같으며, AI 개발의 진입 장벽을 낮추는 데 기여합니다.

**'RAG는 죽었다'는 선언과 컨텍스트 엔지니어링의 부상**

새로운 시장이 형성될 때, 핵심적인 것은 추상화(abstractions)와 그에 대한 추론에 활용되는 기본 구성 요소들(primitives)의 명확성입니다. 그러나 AI 분야에서는 과도한 홍보로 인해 수많은 기본 요소들과 추상화 개념들이 무분별하게 제시되었고, 이는 많은 개발자들이 이 기술의 본질, 구성 방법, 해결 가능한 문제, 중요성, 그리고 시간 투자 우선순위에 대해 심도 있게 성찰하는 것을 방해했습니다. 대표적인 예가 'RAG'라는 용어입니다. 저희는 이 용어 자체를 사용하지 않으며, 개인적으로도 선호하지 않습니다.

제프 휴버는 'RAG'라는 용어가 검색(retrieval)과 생성(generation)이라는 세 가지 개념을 모호하게 묶어 혼란을 초래했다고 비판합니다. 대신 그는 '컨텍스트 엔지니어링(Context Engineering)'이라는 용어를 제안합니다. 컨텍스트 엔지니어링은 "주어진 LLM 생성 단계에서 컨텍스트 창(context window)에 무엇이 있어야 하는지"를 결정하는 작업입니다. 여기에는 컨텍스트 창을 채우는 내부 루프(inner loop)와 시간이 지남에 따라 관련 정보로 컨텍스트를 개선하는 외부 루프(outer loop)가 포함됩니다.

Chroma의 '컨텍스트 로트(Context Rot)' 기술 보고서는 LLM의 성능이 단순히 토큰(tokens) 수에 비례하지 않음을 명확히 보여줍니다. 토큰 수가 증가할수록 모델의 주의력과 추론 능력이 저하될 수 있다는 점을 강조하며, 이는 정교한 컨텍스트 엔지니어링의 필요성을 역설합니다. '컨텍스트 엔지니어링'은 AI 개발자의 직업적 위상을 높이고, 이 분야의 핵심 과제를 명확히 제시하는 중요한 개념입니다.

**코드 인덱싱 및 검색 전략의 재해석**

코드 인덱싱(code indexing)은 인덱싱(indexing) 자체의 본질적인 트레이드오프(trade-off)를 이해하는 것에서 시작합니다. 데이터(data)를 인덱싱하면 쓰기 성능을 희생하는 대신 쿼리(query) 성능이 비약적으로 향상됩니다. Chroma는 정규식(regex) 검색을 기본적으로 지원하며, 대규모 코드베이스(codebases)에서도 빠른 검색을 가능하게 하는 인덱스(indexes)를 구축합니다. 또한, 기존 인덱스(index)를 포크(fork)하여 단 몇 밀리초(milliseconds) 만에 복사본을 만들고, 변경된 파일에 대한 차이점(diff)을 새로운 인덱스(index)에 적용할 수 있는 기능을 제공합니다. 이는 각 커밋(commit)별로 인덱스를 생성하고, 다양한 브랜치(branches)나 릴리스 태그(release tags)에 대한 검색을 효율적으로 수행할 수 있게 합니다.

코드 임베딩(code embeddings)은 아직 초기 단계이지만 잠재력이 매우 큽니다. 일반적인 임베딩 모델(embedding models)이 모든 코드 검색 시나리오에 적합하지 않을 수 있으며, 도메인 특화된 임베딩이 필요할 수 있습니다. 중요한 것은 '신호(signal)'를 찾는 것입니다. 쿼리를 작성하는 사람의 전문 지식에 따라 가장 적합한 도구 조합이 달라집니다. 어떤 경우에는 정규식(Regex)만으로도 충분하지만, 더 복잡하거나 의미론적인 검색을 위해서는 임베딩(embeddings)이 필수적입니다. AI 시대의 코드 검색은 단순히 키워드를 찾는 것을 넘어, 코드의 의도와 기능을 이해하는 방향으로 진화하고 있으며, 이는 개발자 경험(developer experience)을 혁신할 것입니다.

**데이터 중심 접근 방식의 중요성**

효율적인 컨텍스트 엔지니어링을 위해서는 데이터 중심(data-centric) 접근 방식이 필수적입니다. 제프 휴버는 수집(ingestion) 파이프라인에서 가능한 한 많은 구조화된 정보와 메타데이터(metadata)를 추출하고, 청크 재작성(chunk rewriting)과 같은 전처리 작업을 수행하는 것이 다운스트림 쿼리(query) 작업을 훨씬 용이하게 한다고 강조합니다.

가장 중요한 실천 중 하나는 '골든 데이터 세트(golden data set)'의 구축입니다. 이는 특정 쿼리에 대해 반환되어야 하는 정확한 청크(chunks) 목록을 포함하는 소규모의 고품질 데이터 세트입니다. 이 골든 세트를 통해 검색 전략의 효과를 정량적으로 평가하고, 임베딩 모델(embedding model) 변경이나 재순위화(re-ranking) 기법 도입이 실제 성능에 미치는 영향을 측정할 수 있습니다. 개발자들은 종종 쿼리는 있지만, 그에 상응하는 '정답 청크'를 보유하지 못하는 경우가 많습니다. Chroma는 LLM을 활용하여 청크(chunks)로부터 고품질의 쿼리(queries)를 자동으로 생성하는 기술 보고서(technical report)를 발표했으며, 이는 벤치마킹을 위한 QA(Question-Answering) 쌍 생성에 혁혁한 공을 세웁니다. 소규모의 고품질 레이블 데이터 세트(label data set)는 대규모 데이터 세트만큼이나 큰 가치를 지니며, AI 시스템의 부트스트랩(bootstrap) 및 지속적인 개선에 핵심적인 역할을 합니다.

**트랜스포머 아키텍처의 진화와 검색 시스템의 미래**

트랜스포머(Transformer) 아키텍처는 인코더-디코더(encoder-decoder) 구조에서 시작하여, GPT와 같은 디코더 전용(decoder-only) 모델, 그리고 임베딩 모델(embedding models)과 같은 인코더 전용(encoder-only) 모델로 분화되었습니다. 이는 트랜스포머를 사실상 '분리'하여, 인코더로 모든 것을 임베딩(embedding)하고 Chroma와 같은 벡터 데이터베이스에 저장한 후, LLM으로 디코딩(decoding)하는 메타 학습(meta-learning) 아키텍처로 진화했음을 의미합니다.

미래의 검색 시스템은 몇 가지 흥미로운 방향으로 발전할 것입니다. 첫째, 검색은 '잠재 공간(latent space)' 내에 머무르며 자연어(natural language)로 다시 전환되지 않을 것입니다. 둘째, 현재의 '생성(generation)당 한 번의 검색(retrieval)' 방식에서 벗어나, 필요할 때마다 지속적으로 검색(continuous retrieval)을 수행하는 형태로 진화할 것입니다. 이는 LLM이 내부적인 사고 과정에서 능동적으로 정보를 탐색하고 통합하는 능력을 갖게 됨을 의미합니다. 이러한 변화는 AI 시스템이 더욱 효율적이고 지능적으로 컨텍스트를 활용할 수 있게 할 것입니다.

**AI 메모리 구조화와 오프라인 압축**

'메모리(memory)'는 AI 시스템의 핵심적인 이점 중 하나이며, 인간에게 익숙한 개념이라 직관적으로 이해하기 쉽습니다. 우리는 기억을 활용하여 학습하고 새로운 환경에 적응합니다. AI 메모리 역시 LLM이 과거의 상호작용과 학습을 바탕으로 새로운 작업을 더 잘 수행할 수 있도록 돕는 역할을 합니다. 그러나 이러한 메모리의 내부 작동 방식은 결국 '컨텍스트 엔지니어링(context engineering)'의 영역에 속합니다. 즉, 컨텍스트 창(context window)에 가장 적절한 정보를 효과적으로 배치하는 방법을 찾는 것입니다.

AI 시스템의 메모리 관리는 '압축(compaction)' 개념과 밀접하게 연결됩니다. 이는 데이터베이스(databases)의 디스크 조각 모음(Defrag)과 유사하게, 오래되거나 중복된 정보를 효율적으로 정리하고 요약하는 과정입니다. 상호작용 트레이스(interaction traces)를 분석하고 이를 검색 가능한 사실(retrievable facts)로 압축함으로써, AI 시스템은 장기적인 효율성과 관련성을 유지할 수 있습니다. 이러한 오프라인 처리(offline processing)와 '능동적 메모리 관리(active memory management)'는 AI 시스템이 지속적으로 자체 개선되고, 어떤 정보를 기억하고, 잊고, 요약할지 지능적으로 결정하는 데 필수적입니다. 불필요한 새로운 용어를 남발하기보다, 기존의 견고한 엔지니어링 원칙과 데이터 관리 기법을 AI 메모리 시스템에 적용하는 것이 중요합니다.

**Chroma의 가치관과 미래 인재상**

제프 휴버는 이전 스타트업(startups)에서의 경험을 통해 얻은 교훈을 공유하며, "인생은 짧으므로 정말 좋아하는 일을, 좋아하는 사람들과 함께, 그리고 봉사하고 싶은 고객을 위해 해야 한다"는 철학을 강조합니다. 이는 단지 금전적인 성공을 넘어, 의미 있는 영향력(impact)을 창출하는 데 중요한 북극성(North Star)이 됩니다. 영향력은 훌륭한 것을 발명하는 것을 넘어, 가능한 한 많은 사람이 그 발명품을 사용하게 만드는 데서 비롯됩니다.

Chroma는 기업 문화, 디자인, 브랜드 일관성에 깊은 가치를 둡니다. "한 가지를 하는 방식이 모든 것을 하는 방식이다"라는 패트릭 콜리슨(Patrick Collison)의 명언처럼, Chroma는 사무실 환경부터 웹사이트, 사용자 API(API), 그리고 채용 과정에 이르기까지 모든 접점에서 의도적이고 사려 깊은 경험을 제공하고자 노력합니다. 이는 리더(leader)가 회사의 '취향 큐레이터(curator)'가 되어 품질과 일관성을 유지하기 위해 끊임없이 노력해야 함을 의미합니다.

Chroma는 현재 뛰어난 인재들을 적극적으로 채용하고 있습니다. 개발자 도구(developer tools) 분야에서 혁신적인 제품을 만들고 싶은 유능한 제품 디자이너(product designer), 컨텍스트 엔지니어링 연구를 확장할 연구원(researchers), 그리고 저수준 분산 시스템(distributed systems)에 열정을 가진 엔지니어(engineers)를 찾고 있습니다. 특히 분산 시스템 엔지니어는 러스트(Rust), 결정론적 시뮬레이션 테스트(deterministic simulation testing), Raft, Paxos, TLA Plus와 같은 기술에 깊은 이해와 관심을 가지고, 복잡한 인프라 문제를 해결하여 애플리케이션 개발자들이 핵심 가치 창출에 집중할 수 있도록 돕는 역할을 수행합니다. AI 기술의 발전은 이러한 도구들을 활용하여 소규모 팀도 거대한 영향력을 만들 수 있게 하며, Chroma는 이러한 시대적 흐름 속에서 최고의 인재들과 함께 AI 인프라의 미래를 만들어나가고자 합니다.

**쇼 노트(Show Notes)**
크로마(Chroma)
제프 휴버(Jeff Huber)
AI는 새로운 컴퓨터
컨텍스트 로트(Context Rot) 기술 보고서(Technical Report)
생성형 벤치마킹(Generative Benchmarking) 기술 보고서(Technical Report)
REALM (검색 증강 언어 모델(Retrieval-Augmented Language Model)) 논문
RETRO (검색 강화 트랜스포머(Retrieval-Enhanced Transformer)) 논문
SF 시스템 그룹(SF Systems Group)
보이저(Voyager) 논문

**타임스탬프(Timestamps)**
[00:00:00] 소개
[00:00:48] 왜 크로마(Chroma)를 구축했는가
[00:02:55] 정보 검색(Information Retrieval) 대 검색(Search)
[00:04:29] 경쟁적인 AI 시장에서 집중 유지하기
[00:08:08] 크로마 클라우드(Chroma Cloud) 구축
[00:12:15] 컨텍스트 엔지니어링(Context Engineering)과 RAG의 문제점
[00:16:11] 컨텍스트 로트(Context Rot)
[00:21:49] 컨텍스트 품질(Context Quality) 우선순위 지정
[00:27:02] 코드 인덱싱(Code Indexing) 및 검색 전략(Retrieval Strategies)
[00:32:04] 코드 청크 재작성(Chunk Rewriting) 및 쿼리 최적화(Query Optimization)
[00:34:07] 트랜스포머(Transformer) 아키텍처(Architecture) 진화 및 검색 시스템(Retrieval Systems)
[00:38:06] 컨텍스트 엔지니어링(Context Engineering)의 이점으로서의 메모리(Memory)
[00:40:13] AI 메모리(Memory) 구조화 및 오프라인 압축(Offline Compaction)
[00:45:46] 이전 스타트업(Startups)의 교훈과 목적 있는 구축
[00:47:32] 실리콘 밸리(Silicon Valley)의 종교와 가치
[00:50:18] 기업 문화, 디자인, 브랜드 일관성
[00:52:36] 크로마(Chroma) 채용: 디자이너(Designers), 연구원(Researchers), 엔지니어(Engineers)