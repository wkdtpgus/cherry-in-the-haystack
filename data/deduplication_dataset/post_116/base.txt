# **신이 되고 싶어 했던 뉴런(neuron)**

Author: Alberto Romero
URL: https://www.thealgorithmicbridge.com/p/the-neuron-that-wanted-to-be-god

============================================================

다음 텍스트를 한국어로 번역합니다.

**번역 지침**:
1.  전문적이고 자연스러운 한국어를 사용합니다.
2.  기술 용어는 괄호 안에 영어를 병기합니다 (예: "어텐션 메커니즘(attention mechanism)").
3.  원문의 의미와 뉘앙스를 정확하게 보존합니다.
4.  마크다운(markdown) 서식이 있는 경우 유지합니다.
5.  코드 블록이나 수학 공식은 번역하지 않고 원문 그대로 유지합니다.
6.  **중요**: 텍스트 전체를 끝까지 번역합니다. 중간에 끊지 않습니다.
7.  **오직 번역만 합니다**: 원문에 없는 어떠한 주석, 설명 또는 추가 콘텐츠도 추가하지 않습니다.
8.  **누락 없음**: 원문의 어떤 부분도 건너뛰거나, 요약하거나, 생략하지 않습니다.

[문맥: 기사 내용]

**원문**:
I. When scientists Warren S. McCulloch and Walter Pitts invented the first mathematical model of the neuron in 1943, they couldn’t have imagined that eighty years later, humanity would be pouring hundreds of billions of dollars into their idea. But here we are, and they are not, so they cannot tell us just how patently absurd the current artificial intelligence frenzy has become. If you take a look at the modern pipeline of how AI models are created—from the first line of code some unknown developer writes to the moment Sam Altman takes the stage and claims “With this new model, I did feel the AGI”—there are various points at which one can seriously argue that “ok, this should have been done some other way.” (Maybe it’s not something to be proud of that the next generation of AI models needs a quadrillion tokens of data —that is, 1,000,000,000,000,000 as many—to increment their performance half a percentage point in some unknown benchmark test.) However, it’s genuinely surprising that you’d have to go all the way back to the inception of the field to find the first controversial decision that never received sufficient care or consideration—not even after more powerful computers and more money and more data tokens were devoted to the cause. You could wonder why AI labs are trying to scale large language models (LLMs) to the size of the human brain (in the ballpark of a quadrillion synapses), or why they think that combining deep learning (DL) and reinforcement learning (RL) is sufficient (that’s the current pre-training + post-training strategy) or even whether artificial neural networks (ANNs) is truly the only paradigm they need, and still you wouldn’t be pointing to the original sin of the field: accepting the simplicity of McCulloch and Pitts neuron model as a faithful representation of its biological counterpart, which has proven to be orders of magnitude more complex functionally and structurally. AI people, for all the love they profess for brain-inspired metaphors, never revisited that one. It might be too late to backtrack now, but it’s never too late to call out the naivety and folly of those who have humanity’s fate in their hands. This might sound dramatic, but that’s because you haven’t read this other article of mine: “ They Are Sacrificing the Economy on the Altar of a Machine God .” We need to understand how we got here from McCulloch and Pitts’ work, and the first step is to clarify some common misconceptions. Many of you have come to think of ChatGPT or generative AI as a default label to encapsulate the entirety of AI. I don’t blame you; the industry has made a great effort to push this synecdoche (the part is the whole): if the most popular sub-sub-sub-field can gather so much investment and interest, why not let this mischaracterization leak into history books and the public opinion uncorrected? It’s a marketing bargain. But, if we were to conceive a taxonomy of the AI field—such a daunting task that not even Wikipedia has managed to do well—we should reject this fallacy and instead draw a Venn diagram like this (experts will forgive any inaccuracies): A toy taxonomy of AI down to ChatGPT. If I were epistemically rigorous, I’d have to define classification criteria first and then account for all the exceptions. But neither have I time nor you the attention span for that. Ok, so that was easier than I thought: the AI field is basically a bunch of nested circles, not even a Venn diagram in the strict sense because none of them intersect with one another. So we have that artificial intelligence contains machine learning, contains artificial neural networks, contains deep learning, contains generative AI, contains large language models, contains ChatGPT. Or, in mathematical notation (quote-ready): AI ⊃ ML ⊃ ANNs ⊃ DL ⊃ GenAI ⊃ LLMs ⊃ ChatGPT. (Show this to your friends, and they will think you’re either 1) an intellectual genius or 2) an insufferable nerd.) There’s something else you need to know that most people don’t, which is fundamental to grasping just how deep the rabbit hole goes: Whereas the smaller three subfields—generative AI, LLMs, and transformer-based chatbots, for which ChatGPT serves as a stand-in—were conceived in the last decade (if I were to put an inception date for each, I’d say GenAI corresponds to the “ Generative Adversarial Networks ” paper in 2014, LLMs to the “ Attention is all you need ” paper in 2017, and modern chatbots, of course, with ChatGPT’s launch in 2022) the other four fields are much older . AI was established as a field of research in the summer of 1956 . ML and DL were introduced in 1959 and 1986 , respectively. And artificial neural networks—a term borrowed from the, at the time, immature field of neuroscience—appeared the earliest, in 1943, with the pioneering work of Warren McCulloch and Walter Pitts . Their work is now considered the first ever on artificial intelligence, and that’s where our story begins: to heal one’s traumas, one needs to go search for the origins. (AI pioneer Jürgen Schmidhuber would contest every one of these dates , but since I’m no historian, a peer-reviewed genealogy is unnecessary; we only need a shorthand timeline of turning points.) After all this boring background, I might continue by asking a rhetorical question: What if I told you that a biological neuron is better represented by a whole artificial neural network than by a single artificial neuron? Or, to be less nerdy: Why a field that’s concerned with creating intelligent agents that can reason insists on being as stubborn as a rock? Or, to be clearer (last question, I swear): Why has no one revisited the first assumption ever made in AI despite having been proven, time and again, that it’s a dangerous simplification of reality?

II. It was in 1943, during the early days of computer science, that the first neural network was created. Alan Turing, one of the fathers of computer science, had published his foundational paper on computability and the Turing machine just seven years earlier, and it would still take him another seven years to publish his famous paper on the Imitation Game (known as the “Turing Test” which, despite what Gary Marcus says , has clearly been beaten . By 1943, neuroscientists had been modeling biological neurons for decades (at least since the late 19th century, through the work of Santiago Ramón y Cajal and Camillo Golgi), but McCulloch and Pitts’ work was the first to describe them in terms of propositional logic; in other words, as a system that could, in principle, be implemented by a computer . Those of you familiar with the history of AI will recognize this picture: MCP neuron Simple enough. This is the McCulloch and Pitts neuron model (MCP). It’s an 80-year-old fossil. And still, it is the same model that’s taught in every modern introduction course on AI. The reason is not so much that MCP is an accurate model that has required no refinement over the years, but that deep learning hasn’t changed a bit at the elemental level since 1943. (Scientists love to repeat that “ all models are wrong, but some are useful ”; the AI field has stretched the implication that “wrong models can work well” to a point that borders on irresponsible, as we will soon see.) The MCP neuron was intended to represent a simplified neurophysiological version of biological neurons: A series of inputs goes into the neuron, which processes them and then either generates an output or doesn’t. It had a threshold activation function: if the sum of the inputs is negative, the output is 0; otherwise, it’s 1. Current neural networks (like the transformer architecture underlying ChatGPT) have weighted inputs and more complex nonlinear activation functions (no need to understand what any of this means) to allow for meaningful learning and more precise input-output mapping. But they’re just a slightly improved version of the MCP model. The basis is the same; some inputs are transformed into an output. Whether the neuron learns well or not is irrelevant to the fact that these models don’t resemble a real, biological neuron. Current ANN neuron model That’s why you will hear, not totally inaccurately, that “AI is just linear algebra.” At this level, it’s true. The problem is over-extending this assertion: Complexities emerge with scale (” more is different ” and such), which is why I dislike that kind of criticism ; in the sense it’s true, it’s trivial, but in the sense it might be interesting, it’s wrong. It’s like saying that Anna Karenina is a book like any other ; true, but meaningless. Insofar as the analysis stays at the neuron level, it’s fair to be dismissive of the architecture, but scale creates epistemic voids that no one can bridge. That’s why you’ll also hear, also not totally inaccurately, that “ we don’t know how AI works ,” because modern AI is not designed, coded, or built but rather nurtured and grown, like flowers in a garden or trees in the forest. But let’s get back to MCP. The main simplification—and the one that most hurts the fidelity of the model—is that each neuron is collapsed into a single point in space. This is sufficient to simulate the behavior of some simpler neurons, but the biophysical nature of other, more complex neurons is too nuanced and intricate. In animals, electricity flows through the dendrites, the soma, and to the axons through space and time. Not all dendrites work the same way. Not all inputs participate in generating the outputs (voltage decreases along the dendrites). Dendritic tree morphology, synaptic patterns, and different types of receptors all influence neuronal behavior. And many more elemental mechanisms and processes form the bases that eventually give rise to our intelligence. None of those characteristics is described in the MCP or the neuronal models currently in use. (There have been attempts to modernize AI from the ground up, like neuromorphic research and updated elemental neuronal models—“ Expressive Leaky Memory neurons ,” “ Multi-compartment spiking neurons ,” or “ Dendritic spiking neurons ”—but although those efforts exist, they gather no funding or interest beyond some obscure corners in academia.) (You should know that all these weird-sounding neuron names are also models . Again, we must invoke the “all models are wrong, some are useful” aphorism. The goal of science is not to reveal the True Nature of the world, which is most likely beyond our grasp, but to reduce the discrepancy between our models and that unfathomable reality while keeping them useful; that’s where science and engineering clash. Get too close and you will have a slightly wrong, but unaffordable or unfeasible model. Stay too far and you will have an excessively wrong model. In case you’re curious, the classic gold standard model in neuroscience for mathematically expressing neuronal behavior is the Hodgkin–Huxley model from 1952 , which describes the neuron with four coupled differential equations describing ion channel dynamics (Na⁺, K⁺, leak currents—whatever that means, right?) and is the basis for nearly all later biophysical models that, funny enough, require computers for simulation and whatnot.) Notably, by the time the MCP neuron was ideated in 1943, neuroscience already knew neurons had features that made them non-reducible to a space-point neuron. McCulloch and Pitts simplified those annoying complexities in order to make a logic-based model (a great starting point!). But in the process, they set the groundwork of a whole field that never deigned to look again at its own premises and compare them with relevant discoveries in the cognitive sciences. If you ask why it didn’t happen with AI, even though science ”progresses one funeral at a time”—to use the popular paraphrase of Planck’s principle —meaning that dead ideas (actually, dead people who once held those ideas) are the norm and a requirement for progress, then you’d be asking the right question. I will answer, not to spoil later sections, with a parallelism: It takes a kind of idealistic bravery or foolhardy courage to topple a brittle foundation in academia; it takes a will of steel or the power of an emperor to topple a brittle foundation in industry. Neuroscience, in contrast to AI’s alchemical essence ( which is fine ), is an established science; being commercially irrelevant, neuroscientists are compelled to follow the scientific method, for they can’t follow profits. But the most important contrast is that neuroscience has developed drastically in the last 80 years; it was more mature back in the 50s, and it’s much more mature today. Our understanding of biological neurons has come to a point where AI researchers can’t keep calling artificial neurons “neurons” at all. And indeed, they don’t. Everyone working in AI takes for granted that ChatGPT has a distant inspiration in the human brain, but you don’t hear them mention the words “neuron” or “unit” anymore, like they used to in pre-ChatGPT times. What began as a useful simplification by the field turned into an intentional omission by the industry. Insofar as LLMs work and ChatGPT is popular, and both provide a return on the investment, it doesn’t matter to them. I will argue, contrary to what intuition suggests, that it actually matters and that the situation we’re in—you know, the 1,000,000,000,000 dollars devoted to building AI infrastructure in the form of GPUs designed in Taiwan and datacenters erected in Northern Virginia, and the 1,000,000,000,000,000 data tokens needed to advance the state of the art ever so slightly—is a direct consequence of not having backtracked when the sunk cost was still reasonable. But before going into that, let’s strengthen my case by taking a look at some of the most relevant recent neuroscience research on the topic. It will illustrate, I hope convincingly, just how far AI has diverged from the cognitive sciences and, thus, from the human brain, the only instance of intelligence that we know of.

A blog about AI that’s actually about people
Subscribe

III. (Keep in mind that you don’t need to follow any technical or jargon-y details here. The important part is that you get a feeling of the sharp contrast between what neuroscience knows about neurons, neural networks, the brain, and intelligence, and what AI knows about them.) Let’s take a quick look at dendrites, the biological neuron’s signal receptor. In the early 1980s, Christof Koch and others found that dendritic morphology and synaptic patterns could influence how neurons internally processed the inputs. For a long time, scientists thought dendrites behaved uniformly and passively added the inputs coming from other neurons, but Koch’s findings revealed that form and function are much more complex than textbooks suggest. In 1996, scientists from Columbia University and Bell Labs investigated the role of individual dendrites and discovered that they act as processing units themselves: Dendrites have their own threshold to generate spikes (called dendritic spikes), which is different than the threshold of the whole neuron. That is, neurons aren’t simple “logic gates,” as the MCP model suggests. Dendrites seem to be capable of acting as logic gates themselves. A biological neuron is therefore a processing system that, in turn, is composed of independent processing systems: processors within processors. To represent this in artificial neural networks, connections between neurons would need to have distinct morphologies affecting their role in neural outputs (it doesn’t happen). Then, those connections would act internally as processing systems (it doesn’t happen): Each input connection that arrives at the neuron would generate (or not) a spike that would change the overall output of the neuron (it doesn’t happen). This complexity mismatch implies that a biological neuron is better understood as a layered network , in which the layers (dendrites) function as nonlinear intermediate input-output mappings. The resulting intermediate outputs are then summed accordingly to the morphology of the “connection tree” to produce the final output. In 2020, only five years ago, Albert Gidon and colleagues at the Humboldt University of Berlin published a groundbreaking paper in Science, building on these striking discoveries. They found a new input-output feature in pyramidal human neurons that wasn’t described by current models. Dendrites from these neurons produce a type of spike in which intensity is highest for stimuli at the threshold level and lowest when the incoming electrical current is increased. This discovery proved that some dendrites can act as XOR logical gates; the output is true if and only if one of the inputs is true. In 1969, Marvin Minsky and Seymour A. Papert , AI pioneers, proved that a single-layer perceptron (an early type of artificial neural network) couldn’t do this type of computation. Gidon’s research proves that a single biological dendrite can . An element inside a biological neuron can conduct complex computations that an entire neural network can’t. Of course, more complex ones could (we’re talking here about dozens of parameters, whereas GPT-4o had hundreds of billions), but this still signals a two-order-of-magnitude computational gap between the basic elements of human brains and AI systems. If a dendrite can do the work of a basic artificial neural network, then how much more complex are biological neurons compared to artificial neurons? Or, a better question, how much more powerful are they? In 2021, a year after Gidon’s research, David Beniaguev and colleagues published a paper in Neuron that proved what’s been suggested for decades: An artificial neuron can’t accurately represent a biological neuron at all. To prove this, they used modern machine learning techniques to simulate the input-output behavior of a pyramidal human neuron. They wanted to test two things: Whether an artificial neural network can precisely predict neuronal outputs when trained on real input-output pairs, and how big the artificial neural network needs to be to capture the whole complexity of a biological neuron with sufficient accuracy. They found that, at the very least, a 5-layer 128-unit temporal convolutional network (TCN) is needed to simulate the input-output patterns of a pyramidal neuron at the millisecond resolution (single spike precision). They modified depth and width and found that the best performance was achieved by an 8-layer 256-unit TCN. To make a gross comparison: This means that a single biological neuron needs between 640 and 2048 artificial neurons to be simulated adequately; that’s three orders of magnitude . Notice, because I want to be as rigorous as possible, that this doesn’t necessarily imply that a biological neuron has this much more computational power or complexity, but it’s a clear sign that both types of neurons are further apart in structure, function, and behavior than previously thought. It is for this reason, and others I won’t mention here, that it is a miracle that artificial neural networks, and the entire modern edifice of AI based on deep learning and LLMs, work at all. The “ unreasonable effectiveness of deep learning ,” they call it. The problem with something that is unreasonably effective is that it might break at an equally unreasonable, or rather unanticipated , moment. Beniaguev’s team was able to pin down the exact mechanisms by which the biological neuron was so difficult to simulate: dendritic morphology and the presence of a specific type of synapse receptor called NMDA. These are structural and functional aspects of biological neurons well-known in neuroscience for a long time, but have been completely ignored in modern AI and ANNs, except for a handful of studies that seldom leave the classroom. One last example I will share is this paper published in Nature Communications in January 2025 by Panayiota Poirazi’s team, who built an artificial neural network with dendritic morphology and connectivity and showed that it presents superior efficiency, resilience, and precision than standard ones. It’s clever and brave in the well-intended, naive way that characterizes academics: they keep trying to bridge the gap between AI and neuroscience without realizing that the bridge was dismantled by the people they’re trying to appeal to. This paper, as interesting as it is, attracted a striking 23 citations and none of the funding. Some questions arise from these insights: Why hasn’t the AI community tried to remodel its foundations to better adapt to the reality they’re trying to simulate? Is AI destined to fail in its quest to achieve artificial general intelligence (AGI) until those foundations are overthrown and rebuilt from the ground up? What would be the consequences of changing AI at such an elemental level? Or, conversely, what would be the consequences of not changing anything at all ?

IV. Let’s take the standard position in AI: It may be the case that AI systems like ChatGPT can be scaled and improved iteratively—that’s what Google DeepMind, OpenAI, Anthropic, and Meta do—until they eventually reach AGI or human-level performance without any intentional restructuring of the basic assumptions that preceded the current work in generative AI and LLMs (I say intentional because it’s always possible to correct superficially while moving forward, just like a dumb, non-teleological evolutionary system would). Perhaps the apparent differences between artificial and biological neural structures are, at worst, reconcilable with enough resources (e.g., compute and data—you know, all those quadrillion GPUs and tokens), and at best, irrelevant. Neuroscience is concerned with intelligence, the brain, and the mind. Neuroscientists are naturally concerned with looking inwards, to the only instance of intelligence we know of: us. In contrast, AI researchers are concerned with replicating intelligence using artificial means . They care about designing and building intelligent agents that perceive the world and act on it accordingly (I take this as the ultimate goal of the field, even though I know there are intermediate goals that have nothing to do with neuroscience, like Vibes and Sora and the AI slop that’s having a feast on our biological neurons, hopefully to replicate them someday). If you look dispassionately at this discrepancy, you will realize that both fields are doing the sensible thing. Neuroscience cares about the premise of intelligence, whereas AI, about its outputs. If we can find a different substrate (e.g., silicon vs carbon), architecture (e.g., LLMs vs cortices), or a set of trade-offs that better meet our circumstances, then we needn’t model an intelligent agent after the human brain. I’d even go further: it’s not possible! Because computers and AI systems already display a series of abilities that humans can’t replicate, like near-instant computation, arbitrarily high precision, eidetic memory, etc. In short: humans are one instance in the space of possible intelligences, but in principle, no physical law prevents the existence of others, and, all things considered, I see no reason to prefer ours… Even people who’ve grown wary of scale maximalists and LLM maximalists (those are the labels for people who think we already have all the elements we need) but can’t be accused of hating AI in any way, like scientists Francois Chollet, Andrej Karpathy, and Richard Sutton, to name a few well-known figures in the field, generally agree with this point: There’s no need for full biological fidelity if you can achieve functional competence some other way. Here are two tweets by Chollet on his stance on LLMs (2024) and the pursuit of AGI (2025): LLMs (and deep learning in general) are useful, and will keep getting more useful over time. They also won’t morph into intelligent agents, because they simply don’t implement the mechanism of intelligence. The point of our work isn’t to build an artificial human. The universe is full of questions far more interesting than our own reflection. The point is to create a new kind of mind to help us explore & understand the universe better than we can ourselves. LLMs lack key mechanisms for intelligence, but that doesn’t mean that they need to resemble an “artificial human” to be intelligent! Here’s Karpathy , in a comment on Sutton’s interview in the Dwarkesh podcast: . . . today’s frontier LLM research is not about building animals. It is about summoning ghosts. You can think of ghosts as a fundamentally different kind of point in the space of possible intelligences. They are muddled by humanity. Thoroughly engineered by it. They are these imperfect replicas, a kind of statistical distillation of humanity’s documents with some sprinkle on top. They are not platonically bitter lesson pilled, but they are perhaps “practically” bitter lesson pilled, at least compared to a lot of what came before. It seems possibly to me that over time, we can further finetune our ghosts more and more in the direction of animals; That it’s not so much a fundamental incompatibility but a matter of initialization in the intelligence space. But it’s also quite possible that they diverge even further and end up permanently different, un-animal-like, but still incredibly helpful and properly world-altering. And here’s Sutton on the podcast about how kids don’t do imitation but mostly experience things as proactive agents living in the world, whereas LLMs mostly imitate human-made training data: When I see kids, I see kids just trying things and waving their hands around and moving their eyes around. There’s no imitation for how they move their eyes around or even the sounds they make. They may want to create the same sounds, but the actions, the thing that the infant actually does, there’s no targets for that. There are no examples for that. . . . The large language models are learning from training data. It’s not learning from experience. It’s learning from something that will never be available during its normal life. There’s never any training data that says you should do this action in normal life. They’re talking about the same thing in slightly different ways: What we have in the AI field (LLMs) is not enough, and although bioinspiration is worth taking into account, we may end up with something that resembles little “an artificial human” and is more “un-animal-like.” It makes no sense to reject a priori any insights that the cognitive sciences could provide (that’s Karpathy and Sutton’s main argument), but we also don’t need to stick to what we already know just because it’s what we already know (that’s Chollet’s point). To answer the question with which I ended the previous section: Maybe there’s no need to revisit the original neuron model from 80 years ago. Maybe we keep going just like we have for the last 10 years, and nothing catastrophic happens. We mustn’t make this mistake: AI’s goal isn’t and has never been replicating human intelligence at every level. If a shallow abstraction achieves the goal, why complicate it with dendritic spikes, voltage gradients, and weird morphology? Is complexity a prerequisite for intelligence? That makes no sense; evolution makes things convoluted precisely because it carries no teleological impulse whatsoever, proceeding instead under the modulation of survival pressures and other external forces. Those research studies I referenced are concerned with how much more complex human neurons are than we thought, but that’s not the right lens to look at the problem if you’re an AI researcher! Is neuron-level fidelity essential for cognition? I will answer by validating the opposite hypothesis: two instantiations of intelligence of drastically distinct complexity, form, size, substrate, and phenomenology could, in principle, result in comparable intelligence; the map needn’t match the territory to produce a usable compass. These are totally reasonable counterpoints to my starting arguments. After all, McCulloch and Pitts’ “good enough” 80-year-old simplistic neural model is the foundational basis of systems smart enough to beat humans at chess and coding , and math , and in an increasingly larger number of problems in diverse fields. Benchmarks that help us evaluate AI models’ performance are saturating because we can’t come up with tasks that are hard enough. However, there’s an important limitation with standard evaluations of AI (I include here those trying to capture the chaotic nature of the real world, like METR’s analysis of long tasks and OpenAI’s GDPval ): AI tests are designed so that none of the stuff that we’re uncertain about is actually measured. To be precise, the only relevant benchmark is competence in the real world , and so far, every single benchmark fails to capture the complexity of reality and every single AI model fails at this when tested “in the wild” in a way that human toddlers wouldn’t (you can see it with benchmarks that require on-the-fly solving skills like ARC-AGI or in large-scale projects that are factored in in productivity and economic analyses but not tested for in laboratory evaluations). There’s a huge gap between prediction-competence and environment-competence ( Chollet discusses crystallized vs. fluid intelligence, a psychological concept that highlights the distinction between possessing extensive skills and being adept at efficiently acquiring new ones ). So maybe, although functional performance is the goal, it turns out that biological fidelity is the requirement to a much deeper degree than we’d like to admit; after all, it’s the only reference we have of world-competence. It’s fine to formulate the hypothesis like: “Ok, maybe this plane can fly without wings,” even if any onlooker familiar with those little animals called birds thinks it’s nonsense. That hypothesis is the equivalent of saying, “Ok, maybe this AI can be intelligent without all those oddly shaped gyri and sulci .” But, if after years of attempts and amounts of funding that would dwarf the cost of directly breeding a giant species of bird that could carry us around, you’re still unable to make the damn thing fly, maybe it’s time to reconsider your premises. Maybe adding wings to the plane is not an unreasonable consideration. But I played a trick on you just now, because I used an analogy to draw the parallelism that I know is false. One could also propose this other hypothesis: “Ok, maybe this plane can fly without flapping its wings.” Oh, wow, that’s almost the same thing and yet so different—because it’s true! Planes don’t need to flap their wings! Having limited knowledge about what works and what doesn’t is the root cause of the problem. How much should we take from biology? We don’t know because, turns out, our metaphorical AI plane kinda flies and kinda doesn’t. And we have no idea why both things are true because the pattern it follows is, itself, completely unfamiliar to us. So it’s pretty clear at this point that the space of possible intelligences is larger than nature allows for (I mean it in the biological sense; in the physical sense no one, not even AI systems, are allowed to disobey, e.g., the speed of light and the second law of thermodynamics are inviolable), and yet we may not want to drift away from the carved path too much. Better safe than sorry, especially if the apology will be accompanied by a trillion dollars in losses. We don’t know what elements are always needed and which are evolution’s specific trade-offs that we, intelligent designers, don’t need to incur (e.g., planes don’t flap their wings because engines create the required propelling force, but still are subjected to the physical constraints imposed by aerodynamics, so the wings stay). That’s why playing conservatively in the creation game is the best approach: evolution’s design space might contain structural priors that encode principles of intelligence that we take as incidental quirks; you don’t play with the unknown unknowns. We’re not as free of compromise solutions as we like to think, and we don’t know how ours—to some degree similar and to some degree distinct from biology’s—affect our arbitrary mapping from design space to solution space. The map needn’t match the territory to produce a usable compass, but the more faithful the representation, the harder it is to get lost. Or, in other words, we don’t need to copy nature, but we’d better respect its boundaries (those we deem sensible and those we don’t yet understand). Richard Sutton is best known for his 2019 Bitter Lesson essay , which says, simplifying, that in the long term it’s preferable to let computers take the burden of finding better AI systems instead of trying ourselves by leveraging our limited knowledge. And that’s fine, probably true. The mistake is having called that a bitter lesson. It’s bittersweet at most. You know what the true bitter lesson is? That evolution, through non-teleological trial and error and random mutations, managed to scale efficiently a more complex elemental building block into a greater, smarter, more robust, and more versatile system: it expertly carved a seemingly convoluted path from the biological neuron to human intelligence. Sutton’s lesson is powerful because it doesn’t attend to the complexity of the elements to be scaled, just to the fact that they should be efficiently scalable. Adding compute works better than tweaking weak heuristics and leveraging incomplete knowledge, but nothing beats—and I remark nothing —starting from the correct building blocks. In a way, this lesson reminds me of the main takeaway from Seeing Like a State : if you simplify the world too much, you will make it legible, but you will also destroy the very elements that keep it alive. By trying to perfectly control and monitor the artificial brain that you’re making through the simplest neuronal model you could find, you have ensured, almost as an axiomatic truth, that it will not be smart in the ways you want. Despite this bitter lesson that both field and industry have ignored for so long, you will keep adding layers to your LLM, so I make this bet: you will run out of layers to add (computing power, training data, etc.) much sooner than you will beat the unnecessary hurdles you incurred by not having revisited the foundations of your field. That I would call bitter.

V. I’ve mentioned several times throughout this long piece something that deserves a standalone section, the kind of trade-off that we suffer in every aspect of life, but biology never faced and will never face: the motivations of capital. It’s extremely expensive (if possible at all) to model a neuron in its full complexity, so they chose the simplest model they had (that was affordably scalable) and rolled with that. In a way, evolution’s trade-off between brain size and brain weight is not that different from AI labs’ trade-off between model size and model cost, except for one thing: whereas evolution has no goals and thus no interest in short-term wins (whatever that means in a genetic context), investors have a laser-sharp focus on the prize they’re chasing. So it makes sense to not overcomplicate things early on, but it’s put us in a tricky situation: We saved a lot of money by using an extremely simplified model of the neuron, but maybe we’re paying for it extra now that it’s proven harder to emulate human intelligence in real-world settings. The hundreds of billions of dollars that unprofitable AI companies are pouring into datacenters to train more and more LLMs are not a testament to their grit, ambition, and vision but to their haste . (I won’t entertain here the possibility that they don’t care about any of this, and they’re actually pursuing more prosaic purposes like absolute surveillance ; that topic requires a standalone article.) Had they devoted more resources and time in the beginning to finding a better foundation, we wouldn’t be wasting so much energy and money in training LLMs on the entire corpus of human data when newborns need an infinitesimal fraction of that to grow into much smarter creatures. But of course, had AI pioneers done that, no rational investor would have given them a single dollar. If ChatGPT exists, it’s because we pay for it; if ChatGPT exists, it’s because there was no one reckless enough to aim higher; if ChatGPT exists, it’s because the socioeconomic dynamics that govern its existence don’t allow for something better. And so we find ourselves, like the very systems we try to transform into truly intelligent entities, trapped in a local minima. (Isn’t it ironic that our economic optimizer mirrors the gradient descent that powers AI systems— or perhaps it is a pattern …) Capital allocation agents do this all the time (if you want to surrender the last ounce of faith in humanity you might still be holding onto at this point, you can read Scott Alexander’s “ Meditations on Moloch ” to finally see the entire rabbit hole at once). Think about the Industrial Revolution, when we decided that automation was worth polluting the world. (I will also not turn this into a climate change debate.) Back then, we didn’t have a real-world model on which to base our progress. How could we have known that burning dirty coal would be bad for the lungs of our little green planet, and instead should have invested in nuclear, solar, and wind power early on? Besides, we shouldn’t forget that innovation has other limitations; it can only push frontiers into the adjacent possible thing, not further. But the AI field is special because it has access to the best inspiration it could ever need (I mean us, humans, although I’d understand if you considered this a controversial affirmation), and yet capital allocation agents are still making the same mistake : they allow optimization and financial pressures to steer us away from the safer, more sensible path. Unfortunately, science follows what’s fundable, measurable, and publishable, and that’s a hard trade-off to navigate because from the short-termist eye view of a capital allocator, what looks like the better approach may prove to be, for reasons beyond its gaze, the most expensive one; pushing forward those trade-offs it doesn’t want to take until it’s forced to take them—the very essence of capitalistic dynamics—is what brough us from there to here. What do I mean by “from there to here”? If you’ve been paying attention, I’ve poured 7,000 words and then some into drawing an association between the 1943 pioneering work of McCulloch and Pitts and the 2025 financial catastrophe that’s looming because AI companies have gathered so much opportunity and so much sway that they can do whatever they want, which is not what they should. And they happen to want to spend hundreds of billions of dollars in building AI infrastructure that only needs to be so expensive because what appeared, 80 years ago, like a simplified model of the computational unit—and thus a welcome news in the eyes of capital—turned out to be an expensive mistake when the industry realized that it needed all the data humans have ever produced and the electricity bill of a mid-sized country to train and serve one of those LLMs that’s smaller and dumber than a human brain, which would consume, other things being equal, the energy of a home bulb. To quote another of LLMs’ critics and computer science giant, Grady Booch, responding (in a now deleted tweet) to OpenAI CEO Sam Altman about the $7 trillion he said he’d need to “reshape the global semiconductor industry” : “If you need $7 trillion to build the chips and the energy demand equivalent of the consumption of the United Kingdom, then—with a high level of confidence—I can assure you that you have the wrong architecture.” The rewards of practical, empirical success outweigh the epistemic cost of theoretical impurity until they don’t. When they say that the invisible hand of the markets is an existing example of docile superintelligence, you can use this example as a counterargument. And, if you’re still convinced that we should build an actual godlike superintelligence, whatever that is, I hope that I have convinced you that we may want to rethink the foundations we’re standing on a little bit more than we’d otherwise do. I don’t think it’s a reasonable goal for the McCulloch and Pitts neuron to become God. Contrary to what it might seem from this essay, I don’t resent the choices the AI field made. It’s fine that they didn’t pay attention to biology’s trade-offs, because the human world has its own. But wouldn’t it be darkly ironic if, in ignoring nature’s warnings—she’s wiser than we’ll ever be—we ended up digging our grave? It’s not clear whether one can draw a reasonably precise trajectory between these two realities—that McCulloch and Pitts’ neural model was too simple, and that we’re now facing a financial disaster because AI won’t deliver—but it is striking that, despite all the years that have passed, it’s still trivial to write a story like this and argue, with some firmness that the shadow of that first metaphor we failed to do justice to is still chasing us. Karpathy is right when he says that LLMs are ghosts we have summoned here from another dimension. For your farthest mistakes are the ones that haunt you the longest.

A blog about AI that’s actually about people
Subscribe

---

**번역**:

I. 1943년 과학자 워렌 S. 맥컬록(Warren S. McCulloch)과 월터 피츠(Walter Pitts)가 최초의 뉴런 수학적 모델을 발명했을 때, 그들은 80년 후 인류가 자신들의 아이디어에 수천억 달러를 쏟아부을 것이라고는 상상조차 하지 못했을 것입니다. 하지만 우리는 지금 여기에 있고 그들은 없으므로, 현재의 인공지능 열풍이 얼마나 명백히 터무니없게 되었는지 그들은 우리에게 말해줄 수 없습니다. AI 모델이 생성되는 현대적인 파이프라인을 살펴보면—어떤 무명의 개발자가 작성한 첫 번째 코드 줄부터 샘 알트만(Sam Altman)이 무대에 올라 "이 새로운 모델로 저는 AGI(인공 일반 지능)를 느꼈습니다"라고 주장하는 순간까지—"음, 이건 다른 방식으로 했어야 했어"라고 진지하게 주장할 수 있는 여러 지점들이 있습니다. (아마도 다음 세대 AI 모델이 알려지지 않은 벤치마크(benchmark) 테스트에서 성능을 0.5% 포인트 높이기 위해 1000조 개의 데이터 토큰(tokens)—즉, 1,000,000,000,000,000개—를 필요로 한다는 것은 자랑할 만한 일이 아닐 것입니다.) 그러나 이 분야의 시작으로 거슬러 올라가야만 충분한 관심이나 고려를 받지 못했던 최초의 논란이 되는 결정을 찾을 수 있다는 것은 진정으로 놀라운 일입니다. 심지어 더 강력한 컴퓨터와 더 많은 돈, 더 많은 데이터 토큰이 이 목적에 투입된 후에도 말이죠. AI 연구소들이 왜 대규모 언어 모델(LLMs)을 인간 두뇌 크기(대략 1000조 개의 시냅스(synapses) 범위)로 확장하려고 하는지, 또는 왜 딥러닝(DL)과 강화 학습(RL)을 결합하는 것이 충분하다고 생각하는지(이것이 현재의 사전 학습(pre-training) + 사후 학습(post-training) 전략입니다), 심지어 인공 신경망(ANNs)이 진정으로 그들이 필요로 하는 유일한 패러다임(paradigm)인지 의아해할 수도 있겠지만, 그래도 이 분야의 원죄를 지적하지는 못할 것입니다: 맥컬록과 피츠의 뉴런 모델의 단순성을 생물학적 대응물의 충실한 표현으로 받아들인 것 말입니다. 이는 기능적으로나 구조적으로나 훨씬 더 복잡하다는 것이 입증되었습니다. AI를 연구하는 사람들은 뇌에서 영감을 받은 은유에 대한 모든 애정에도 불구하고, 그 한 가지를 결코 다시 검토하지 않았습니다. 이제 와서 되돌리기는 너무 늦었을지 모르지만, 인류의 운명을 손에 쥐고 있는 사람들의 순진함과 어리석음을 지적하기에 너무 늦은 때는 없습니다. 이것이 극적으로 들릴 수도 있지만, 그것은 당신이 저의 다른 기사 "그들은 기계 신의 제단에 경제를 희생하고 있다"를 읽지 않았기 때문입니다. 우리는 맥컬록과 피츠의 연구에서부터 어떻게 여기까지 왔는지 이해해야 하며, 첫 번째 단계는 몇 가지 흔한 오해를 명확히 하는 것입니다. 많은 분들이 ChatGPT나 생성형 AI(generative AI)를 AI 전체를 아우르는 기본 레이블(label)로 생각하게 되었습니다. 저는 당신을 비난하지 않습니다. 업계는 이 제유법(synecdoche, 부분이 전체를 나타냄)을 밀어붙이기 위해 많은 노력을 기울였습니다. 가장 인기 있는 하위-하위-하위 분야가 그렇게 많은 투자와 관심을 모을 수 있다면, 이 잘못된 특성 부여가 역사책과 대중의 의견에 수정되지 않은 채 스며들도록 내버려 두지 않을 이유가 무엇이겠습니까? 이것은 마케팅 거래입니다. 그러나 우리가 AI 분야의 분류 체계(taxonomy)를 구상한다면—위키피디아(Wikipedia)조차 잘 해내지 못한 벅찬 과제입니다—이러한 오류를 거부하고 대신 다음과 같은 벤 다이어그램(Venn diagram)을 그려야 합니다(전문가들은 어떠한 부정확성도 용서할 것입니다): ChatGPT까지의 AI 분류 체계(간략화된). 만약 제가 인식론적으로 엄격했다면, 먼저 분류 기준을 정의하고 모든 예외를 설명해야 했을 것입니다. 하지만 저에게는 그럴 시간이 없고 당신에게는 그럴 집중력이 없습니다. 좋습니다, 생각보다 쉬웠네요: AI 분야는 기본적으로 중첩된 원들의 묶음이며, 엄밀한 의미의 벤 다이어그램조차 아닙니다. 왜냐하면 그 어떤 것도 서로 교차하지 않기 때문입니다. 그래서 우리는 인공지능(artificial intelligence)이 기계 학습(machine learning)을 포함하고, 기계 학습이 인공 신경망(artificial neural networks)을 포함하고, 인공 신경망이 딥러닝(deep learning)을 포함하고, 딥러닝이 생성형 AI를 포함하고, 생성형 AI가 대규모 언어 모델(large language models)을 포함하고, 대규모 언어 모델이 ChatGPT를 포함한다는 것을 알 수 있습니다. 또는 수학적 표기법으로 (인용 준비 완료): AI ⊃ ML ⊃ ANNs ⊃ DL ⊃ GenAI ⊃ LLMs ⊃ ChatGPT. (이것을 친구들에게 보여주면, 그들은 당신이 1) 지적 천재이거나 2) 참을 수 없는 괴짜라고 생각할 것입니다.) 대부분의 사람들이 모르는 또 다른 것이 있는데, 이는 토끼굴이 얼마나 깊은지 파악하는 데 근본적입니다: ChatGPT가 대표 역할을 하는 더 작은 세 가지 하위 분야—생성형 AI, LLM, 그리고 트랜스포머(transformer) 기반 챗봇(chatbot)—는 지난 10년 동안 구상되었지만 (각각의 시작일을 정한다면, GenAI는 2014년 "생성적 적대 신경망(Generative Adversarial Networks)" 논문에, LLM은 2017년 "어텐션 이즈 올 유 니드(Attention is all you need)" 논문에, 그리고 현대 챗봇은 물론 2022년 ChatGPT 출시에 해당한다고 말할 수 있습니다), 다른 네 분야는 훨씬 더 오래되었습니다. AI는 1956년 여름에 연구 분야로 확립되었습니다. ML과 DL은 각각 1959년과 1986년에 도입되었습니다. 그리고 인공 신경망(artificial neural networks)—당시 미성숙했던 신경과학 분야에서 차용한 용어—은 워렌 맥컬록과 월터 피츠의 선구적인 연구와 함께 1943년에 가장 먼저 등장했습니다. 그들의 연구는 현재 인공지능에 대한 최초의 연구로 간주되며, 우리의 이야기는 여기서 시작됩니다: 트라우마를 치유하려면, 그 기원을 찾아야 합니다. (AI 선구자 위르겐 슈미트후버(Jürgen Schmidhuber)는 이 모든 날짜에 이의를 제기할 것이지만, 저는 역사가가 아니므로, 동료 심사를 거친 계보(genealogy)는 불필요합니다; 우리는 전환점의 약식 타임라인(timeline)만 필요합니다.) 이 모든 지루한 배경 설명 후에, 저는 수사적 질문을 던지며 계속할 수 있을 것입니다: 생물학적 뉴런(biological neuron)이 단일 인공 뉴런(artificial neuron)보다 전체 인공 신경망(artificial neural network)으로 더 잘 표현된다고 제가 말한다면 어떨까요? 아니면, 덜 괴짜스럽게 말하자면: 추론할 수 있는 지능형 에이전트(agent)를 만드는 데 관심을 두는 분야가 왜 바위처럼 고집스럽게 행동하는 것을 고집할까요? 아니면, 더 명확하게 말하자면 (마지막 질문입니다, 맹세합니다): AI에서 처음으로 만들어진 가정이 현실의 위험한 단순화라는 것이 거듭해서 입증되었음에도 불구하고, 왜 아무도 그 가정을 다시 검토하지 않았을까요?

II. 최초의 신경망이 만들어진 것은 컴퓨터 과학의 초기였던 1943년이었습니다. 컴퓨터 과학의 아버지 중 한 명인 앨런 튜링(Alan Turing)은 불과 7년 전에 계산 가능성(computability)과 튜링 기계(Turing machine)에 대한 기초 논문을 발표했으며, 모방 게임(Imitation Game, 게리 마커스(Gary Marcus)의 말과는 달리 명백히 통과된 "튜링 테스트(Turing Test)"로 알려짐)에 대한 그의 유명한 논문을 발표하는 데는 여전히 7년이 더 걸릴 것이었습니다. 1943년까지 신경과학자들은 수십 년 동안(적어도 19세기 후반부터 산티아고 라몬 이 카할(Santiago Ramón y Cajal)과 카밀로 골지(Camillo Golgi)의 연구를 통해) 생물학적 뉴런을 모델링해왔지만, 맥컬록과 피츠의 연구는 명제 논리(propositional logic)의 관점에서 뉴런을 설명한 최초의 연구였습니다. 즉, 원칙적으로 컴퓨터로 구현될 수 있는 시스템으로 말입니다. AI의 역사에 익숙한 분들은 이 그림을 알아볼 것입니다: MCP 뉴런. 충분히 간단합니다. 이것이 맥컬록과 피츠 뉴런 모델(MCP)입니다. 80년 된 화석이죠. 그리고 여전히, 이것은 모든 현대 AI 입문 과정에서 가르쳐지는 동일한 모델입니다. 그 이유는 MCP가 수년간 정교화가 필요 없었던 정확한 모델이라기보다는, 딥러닝이 1943년 이후 근본적인 수준에서 전혀 변하지 않았기 때문입니다. (과학자들은 "모든 모델은 틀렸지만, 일부는 유용하다"는 말을 반복하기를 좋아합니다. AI 분야는 "틀린 모델도 잘 작동할 수 있다"는 함의를 무책임할 정도로 확장했습니다. 곧 보게 될 것입니다.) MCP 뉴런은 생물학적 뉴런의 단순화된 신경생리학적 버전을 나타내기 위한 것이었습니다: 일련의 입력이 뉴런으로 들어가고, 뉴런은 이를 처리한 다음 출력을 생성하거나 생성하지 않습니다. 여기에는 임계 활성화 함수(threshold activation function)가 있었습니다: 입력의 합이 음수이면 출력은 0이고, 그렇지 않으면 1입니다. 현재 신경망(ChatGPT의 기반이 되는 트랜스포머(transformer) 아키텍처(architecture)와 같은)은 의미 있는 학습과 더 정밀한 입출력 매핑(mapping)을 가능하게 하기 위해 가중치 입력(weighted inputs)과 더 복잡한 비선형 활성화 함수(nonlinear activation functions)를 가지고 있습니다(이것이 무엇을 의미하는지 이해할 필요는 없습니다). 하지만 그것들은 MCP 모델의 약간 개선된 버전일 뿐입니다. 기본은 동일합니다. 일부 입력이 출력으로 변환됩니다. 뉴런이 잘 학습하는지 여부는 이 모델들이 실제 생물학적 뉴런과 닮지 않았다는 사실과 무관합니다. 현재 ANN 뉴런 모델. 그래서 당신은 "AI는 단지 선형 대수학(linear algebra)일 뿐이다"라는 말을 완전히 틀리지 않게 들을 것입니다. 이 수준에서는 사실입니다. 문제는 이 주장을 과도하게 확장하는 것입니다: 규모가 커지면서 복잡성이 나타납니다("더 많은 것은 다르다" 등), 이것이 제가 그런 종류의 비판을 싫어하는 이유입니다. 사실인 의미에서는 사소하지만, 흥미로울 수 있는 의미에서는 틀렸습니다. 마치 안나 카레니나(Anna Karenina)가 다른 책과 다를 바 없는 책이라고 말하는 것과 같습니다. 사실이지만 무의미합니다. 분석이 뉴런 수준에 머무는 한, 아키텍처를 무시하는 것은 정당하지만, 규모는 아무도 메울 수 없는 인식론적 공백을 만듭니다. 그래서 당신은 "우리는 AI가 어떻게 작동하는지 모른다"는 말을 완전히 틀리지 않게 들을 것입니다. 왜냐하면 현대 AI는 설계되거나 코딩되거나 구축되는 것이 아니라, 정원의 꽃이나 숲의 나무처럼 양육되고 성장하기 때문입니다. 하지만 MCP로 돌아가 봅시다. 주요 단순화—그리고 모델의 충실도를 가장 해치는 부분—는 각 뉴런이 공간의 한 점으로 축소된다는 것입니다. 이것은 일부 더 간단한 뉴런의 행동을 시뮬레이션하기에 충분하지만, 다른 더 복잡한 뉴런의 생물물리학적 본질은 너무 미묘하고 복잡합니다. 동물에서는 전기가 수상돌기(dendrites), 세포체(soma), 그리고 축삭(axons)을 통해 시공간적으로 흐릅니다. 모든 수상돌기가 같은 방식으로 작동하는 것은 아닙니다. 모든 입력이 출력을 생성하는 데 참여하는 것은 아닙니다(수상돌기를 따라 전압이 감소합니다). 수상돌기 나무 형태(dendritic tree morphology), 시냅스 패턴(synaptic patterns), 그리고 다양한 유형의 수용체(receptors)가 모두 뉴런 행동에 영향을 미칩니다. 그리고 훨씬 더 많은 기본적인 메커니즘(mechanisms)과 과정이 궁극적으로 우리의 지능을 발생시키는 기반을 형성합니다. 이러한 특성 중 어느 것도 MCP나 현재 사용되는 뉴런 모델에 설명되어 있지 않습니다. (신경형태학(neuromorphic) 연구 및 업데이트된 기본 뉴런 모델—"표현형 누출 기억 뉴런(Expressive Leaky Memory neurons)", "다중 구획 스파이크 뉴런(Multi-compartment spiking neurons)", 또는 "수상돌기 스파이크 뉴런(Dendritic spiking neurons)"—과 같이 AI를 근본적으로 현대화하려는 시도가 있었지만, 그러한 노력은 존재함에도 불구하고 학계의 일부 알려지지 않은 구석을 제외하고는 자금이나 관심을 모으지 못합니다.) (이 이상하게 들리는 뉴런 이름들도 모두 모델이라는 것을 알아야 합니다. 다시 한번, 우리는 "모든 모델은 틀렸지만, 일부는 유용하다"는 경구를 떠올려야 합니다. 과학의 목표는 우리가 파악하기 어려울 가능성이 높은 세상의 진정한 본질을 밝히는 것이 아니라, 모델과 헤아릴 수 없는 현실 사이의 불일치를 줄이면서도 유용하게 유지하는 것입니다. 바로 이 지점에서 과학과 공학이 충돌합니다. 너무 가까이 다가가면 약간 틀렸지만, 감당할 수 없거나 실행 불가능한 모델을 갖게 될 것입니다. 너무 멀리 떨어져 있으면 지나치게 틀린 모델을 갖게 될 것입니다. 궁금하시다면, 신경과학에서 뉴런 행동을 수학적으로 표현하는 고전적인 황금 표준 모델은 1952년의 호지킨-헉슬리 모델(Hodgkin–Huxley model)인데, 이는 이온 채널 역학(Na⁺, K⁺, 누설 전류—이게 무슨 뜻이든 간에, 그렇죠?)을 설명하는 네 개의 연립 미분 방정식으로 뉴런을 설명하며, 나중에 나온 거의 모든 생물물리학적 모델의 기반이 됩니다. 재미있게도 이 모델들은 시뮬레이션 등을 위해 컴퓨터를 필요로 합니다.) 특히, 1943년에 MCP 뉴런이 구상될 당시 신경과학은 이미 뉴런이 공간-점 뉴런(space-point neuron)으로 환원될 수 없게 만드는 특징을 가지고 있다는 것을 알고 있었습니다. 맥컬록과 피츠는 논리 기반 모델을 만들기 위해 그러한 성가신 복잡성을 단순화했습니다(훌륭한 출발점이었죠!). 그러나 그 과정에서 그들은 자신의 전제를 다시 들여다보고 인지 과학(cognitive sciences)의 관련 발견과 비교하려고 하지 않은 전체 분야의 기반을 마련했습니다. 과학이 "한 번에 한 번의 장례식을 통해 발전한다"—플랑크(Planck) 원리의 대중적인 의역을 사용하자면, 죽은 아이디어(실제로는 한때 그 아이디어를 가졌던 죽은 사람들)가 정상적이며 진보의 필수 조건이라는 의미입니다—고 하더라도, 왜 AI에서는 그런 일이 일어나지 않았는지 묻는다면, 당신은 올바른 질문을 하는 것입니다. 저는 나중 섹션의 내용을 미리 말하지 않고, 유사성으로 답하겠습니다: 학계에서 취약한 기반을 무너뜨리는 데는 이상주의적인 용기나 무모한 용기가 필요합니다. 산업에서 취약한 기반을 무너뜨리는 데는 강철 같은 의지나 황제의 권력이 필요합니다. AI의 연금술적 본질(이는 괜찮습니다)과는 대조적으로, 신경과학은 확립된 과학입니다. 상업적으로 무관하기 때문에 신경과학자들은 이윤을 쫓을 수 없으므로 과학적 방법을 따를 수밖에 없습니다. 그러나 가장 중요한 대조는 신경과학이 지난 80년 동안 급격히 발전했다는 것입니다. 50년대에는 더 성숙했고, 오늘날에는 훨씬 더 성숙합니다. 생물학적 뉴런에 대한 우리의 이해는 AI 연구자들이 인공 뉴런을 더 이상 "뉴런"이라고 부를 수 없는 지경에 이르렀습니다. 그리고 실제로 그들은 그렇게 하지 않습니다. AI 분야에서 일하는 모든 사람들은 ChatGPT가 인간 두뇌에서 멀리 떨어진 영감을 받았다는 것을 당연하게 여기지만, ChatGPT 이전 시대처럼 더 이상 "뉴런"이나 "단위(unit)"라는 단어를 언급하는 것을 듣지 못합니다. 이 분야에서 유용한 단순화였던 것이 산업계에서는 의도적인 누락으로 변했습니다. LLM이 작동하고 ChatGPT가 인기가 있으며, 둘 다 투자 수익을 제공하는 한, 그들에게는 중요하지 않습니다. 저는 직관이 시사하는 바와는 달리, 그것이 실제로 중요하며, 우리가 처한 상황—아시다시피, 대만에 설계된 GPU(그래픽 처리 장치)와 북부 버지니아에 건설된 데이터센터(datacenter) 형태의 AI 인프라(infrastructure) 구축에 투입된 1조 달러, 그리고 최첨단 기술을 아주 조금이라도 발전시키는 데 필요한 1000조 개의 데이터 토큰—이 매몰 비용(sunk cost)이 여전히 합리적이었을 때 되돌아가지 않은 직접적인 결과라고 주장할 것입니다. 하지만 그 전에, 이 주제에 대한 최근의 가장 관련성 높은 신경과학 연구를 살펴봄으로써 제 주장을 강화해 봅시다. 저는 AI가 인지 과학, 그리고 우리가 아는 유일한 지능의 사례인 인간 두뇌로부터 얼마나 멀리 벗어났는지 설득력 있게 보여줄 수 있기를 바랍니다.

사실은 사람에 대한 AI 블로그
구독

III. (여기서 어떤 기술적이거나 전문 용어적인 세부 사항을 따를 필요는 없다는 점을 명심하십시오. 중요한 부분은 신경과학이 뉴런, 신경망, 뇌, 지능에 대해 아는 것과 AI가 그것들에 대해 아는 것 사이의 극명한 대조를 느끼는 것입니다.) 생물학적 뉴런의 신호 수용체(signal receptor)인 수상돌기(dendrites)를 잠시 살펴보겠습니다. 1980년대 초, 크리스토프 코흐(Christof Koch)와 다른 연구자들은 수상돌기 형태(dendritic morphology)와 시냅스 패턴(synaptic patterns)이 뉴런이 입력을 내부적으로 처리하는 방식에 영향을 미칠 수 있음을 발견했습니다. 오랫동안 과학자들은 수상돌기가 균일하게 행동하고 다른 뉴런에서 오는 입력을 수동적으로 추가한다고 생각했지만, 코흐의 발견은 형태와 기능이 교과서에서 제시하는 것보다 훨씬 더 복잡하다는 것을 밝혀냈습니다. 1996년, 컬럼비아 대학교(Columbia University)와 벨 연구소(Bell Labs)의 과학자들은 개별 수상돌기의 역할을 조사하여 수상돌기 자체가 처리 단위(processing units)로 작동한다는 것을 발견했습니다: 수상돌기는 스파이크(spikes)를 생성하기 위한 자체 임계값(threshold, 수상돌기 스파이크(dendritic spikes)라고 불림)을 가지고 있으며, 이는 전체 뉴런의 임계값과는 다릅니다. 즉, 뉴런은 MCP 모델이 제시하는 것처럼 단순한 "논리 게이트(logic gates)"가 아닙니다. 수상돌기는 그 자체로 논리 게이트 역할을 할 수 있는 것으로 보입니다. 따라서 생물학적 뉴런은 독립적인 처리 시스템으로 구성된 처리 시스템입니다: 프로세서(processor) 내의 프로세서. 인공 신경망에서 이를 표현하려면, 뉴런 간의 연결은 신경 출력에서의 역할에 영향을 미치는 독특한 형태를 가져야 할 것입니다(실제로 그렇지 않습니다). 그런 다음, 그러한 연결은 내부적으로 처리 시스템으로 작동할 것입니다(실제로 그렇지 않습니다): 뉴런에 도달하는 각 입력 연결은 뉴런의 전체 출력을 변경하는 스파이크를 생성하거나(또는 생성하지 않거나) 할 것입니다(실제로 그렇지 않습니다). 이러한 복잡성 불일치는 생물학적 뉴런이 계층형 네트워크(layered network)로 더 잘 이해되어야 함을 의미하며, 여기서 계층(수상돌기)은 비선형 중간 입출력 매핑(nonlinear intermediate input-output mappings)으로 기능합니다. 결과로 나오는 중간 출력은 "연결 트리(connection tree)"의 형태에 따라 합산되어 최종 출력을 생성합니다. 불과 5년 전인 2020년, 알베르트 기돈(Albert Gidon)과 베를린 훔볼트 대학교(Humboldt University of Berlin) 동료들은 이러한 놀라운 발견을 바탕으로 사이언스(Science)지에 획기적인 논문을 발표했습니다. 그들은 현재 모델로는 설명되지 않은 피라미드형 인간 뉴런(pyramidal human neurons)의 새로운 입출력 특징을 발견했습니다. 이 뉴런의 수상돌기는 임계 수준(threshold level)의 자극에 대해 강도가 가장 높고, 들어오는 전류가 증가할 때 가장 낮은 유형의 스파이크를 생성합니다. 이 발견은 일부 수상돌기가 XOR 논리 게이트(XOR logical gates) 역할을 할 수 있음을 증명했습니다. 즉, 입력 중 하나만 참일 경우에만 출력이 참입니다. 1969년, AI 선구자인 마빈 민스키(Marvin Minsky)와 시모어 A. 파퍼트(Seymour A. Papert)는 단일 계층 퍼셉트론(single-layer perceptron, 초기 유형의 인공 신경망)이 이러한 유형의 계산을 수행할 수 없음을 증명했습니다. 기돈의 연구는 단일 생물학적 수상돌기가 이를 수행할 수 있음을 증명합니다. 생물학적 뉴런 내부의 요소가 전체 신경망이 할 수 없는 복잡한 계산을 수행할 수 있습니다. 물론, 더 복잡한 것들은 할 수 있었겠지만(여기서는 수십 개의 매개변수(parameters)에 대해 이야기하고 있으며, GPT-4o는 수천억 개를 가졌습니다), 이것은 여전히 인간 두뇌의 기본 요소와 AI 시스템 간의 두 자릿수 규모의 계산 격차를 나타냅니다. 수상돌기가 기본적인 인공 신경망의 작업을 수행할 수 있다면, 생물학적 뉴런은 인공 뉴런에 비해 얼마나 더 복잡한가요? 아니면, 더 나은 질문은, 얼마나 더 강력한가요? 2021년, 기돈의 연구 1년 후, 데이비드 베니아게프(David Beniaguev)와 동료들은 뉴런(Neuron)지에 수십 년 동안 제기되어 온 것을 증명하는 논문을 발표했습니다: 인공 뉴런은 생물학적 뉴런을 전혀 정확하게 표현할 수 없습니다. 이를 증명하기 위해 그들은 피라미드형 인간 뉴런의 입출력 행동을 시뮬레이션하기 위해 현대 기계 학습(machine learning) 기술을 사용했습니다. 그들은 두 가지를 테스트하고 싶었습니다: 실제 입출력 쌍으로 훈련되었을 때 인공 신경망이 뉴런 출력을 정확하게 예측할 수 있는지 여부, 그리고 생물학적 뉴런의 전체 복잡성을 충분한 정확도로 포착하기 위해 인공 신경망이 얼마나 커야 하는지. 그들은 최소한 5계층 128단위 시공간 컨볼루션 네트워크(temporal convolutional network, TCN)가 피라미드형 뉴런의 입출력 패턴을 밀리초(millisecond) 해상도(단일 스파이크 정밀도)로 시뮬레이션하는 데 필요하다는 것을 발견했습니다. 그들은 깊이와 너비를 수정하여 8계층 256단위 TCN이 최고의 성능을 달성한다는 것을 발견했습니다. 대략적인 비교를 하자면: 이는 단일 생물학적 뉴런이 적절하게 시뮬레이션되기 위해 640개에서 2048개의 인공 뉴런을 필요로 한다는 의미입니다. 이는 세 자릿수 규모입니다. 제가 가능한 한 엄격하게 말하고 싶기 때문에, 이것이 생물학적 뉴런이 이만큼 더 많은 계산 능력이나 복잡성을 가지고 있다는 것을 반드시 의미하지는 않지만, 두 가지 유형의 뉴런이 구조, 기능, 행동 면에서 이전에 생각했던 것보다 훨씬 더 다르다는 명확한 신호라는 점을 주목하십시오. 이러한 이유와 제가 여기서 언급하지 않을 다른 이유들 때문에, 인공 신경망, 그리고 딥러닝과 LLM에 기반한 현대 AI의 전체 구조가 전혀 작동한다는 것이 기적입니다. 그들은 이것을 "딥러닝의 비합리적인 효과(unreasonable effectiveness of deep learning)"라고 부릅니다. 비합리적으로 효과적인 것의 문제는 그것이 똑같이 비합리적이거나, 오히려 예상치 못한 순간에 고장날 수 있다는 것입니다. 베니아게프 팀은 생물학적 뉴런을 시뮬레이션하기가 왜 그렇게 어려운지 정확한 메커니즘을 밝혀낼 수 있었습니다: 수상돌기 형태(dendritic morphology)와 NMDA라고 불리는 특정 유형의 시냅스 수용체(synapse receptor)의 존재였습니다. 이것들은 신경과학에서 오랫동안 잘 알려져 있었지만, 교실을 거의 벗어나지 않는 소수의 연구를 제외하고는 현대 AI와 ANN에서는 완전히 무시되어 온 생물학적 뉴런의 구조적, 기능적 측면입니다. 제가 공유할 마지막 예시는 파나요타 포이라지(Panayiota Poirazi) 팀이 2025년 1월 네이처 커뮤니케이션즈(Nature Communications)에 발표한 논문입니다. 이 팀은 수상돌기 형태와 연결성을 가진 인공 신경망을 구축하여 표준 신경망보다 우수한 효율성, 회복력, 정밀도를 보임을 보여주었습니다. 이것은 학자들을 특징짓는 선의적이고 순진한 방식으로 영리하고 용감합니다: 그들은 AI와 신경과학 사이의 간극을 메우려 노력하지만, 그들이 호소하려는 사람들에 의해 다리가 해체되었다는 것을 깨닫지 못합니다. 이 논문은 흥미로움에도 불구하고, 놀랍게도 23회 인용되었지만, 자금은 전혀 받지 못했습니다. 이러한 통찰력에서 몇 가지 질문이 발생합니다: 왜 AI 커뮤니티는 시뮬레이션하려는 현실에 더 잘 적응하기 위해 그 기반을 재구성하려 하지 않았는가? AI는 그 기반이 전복되고 처음부터 재건될 때까지 AGI(인공 일반 지능)를 달성하려는 탐구에서 실패할 운명인가? 그렇게 근본적인 수준에서 AI를 변경하는 것의 결과는 무엇인가? 아니면, 반대로, 아무것도 변경하지 않는 것의 결과는 무엇인가?

IV. AI의 표준적인 입장을 취해 봅시다: ChatGPT와 같은 AI 시스템은 생성형 AI와 LLM의 현재 작업에 선행하는 기본 가정에 대한 의도적인 재구조화 없이 궁극적으로 AGI 또는 인간 수준의 성능에 도달할 때까지 반복적으로 확장되고 개선될 수 있습니다—이것이 구글 딥마인드(Google DeepMind), 오픈AI(OpenAI), 앤트로픽(Anthropic), 메타(Meta)가 하는 일입니다 (제가 의도적이라고 말하는 이유는, 멍청하고 목적론적이지 않은 진화 시스템이 그러하듯이, 앞으로 나아가면서 표면적으로는 항상 수정할 수 있기 때문입니다). 아마도 인공 신경 구조와 생물학적 신경 구조 사이의 명백한 차이는 최악의 경우 충분한 자원(예: 컴퓨팅(compute) 및 데이터—아시다시피, 그 수많은 1000조 개의 GPU와 토큰)으로 조화될 수 있으며, 최선의 경우 무관할 것입니다. 신경과학은 지능, 뇌, 그리고 마음에 관심을 둡니다. 신경과학자들은 우리가 아는 유일한 지능의 사례인 우리 자신을 내면적으로 들여다보는 데 자연스럽게 관심을 둡니다. 대조적으로, AI 연구자들은 인공적인 수단을 사용하여 지능을 복제하는 데 관심을 둡니다. 그들은 세상을 인지하고 그에 따라 행동하는 지능형 에이전트(agent)를 설계하고 구축하는 데 관심을 둡니다 (저는 이것을 이 분야의 궁극적인 목표로 간주합니다. 비록 바이브(Vibes)와 소라(Sora)와 같이 신경과학과는 아무 관련이 없는 중간 목표들, 그리고 언젠가 우리의 생물학적 뉴런을 복제하기를 바라며 우리의 생물학적 뉴런을 먹어치우는 AI 슬롭(AI slop)과 같은 것들이 있다는 것을 알지만 말입니다). 이 불일치를 냉정하게 본다면, 두 분야 모두 합리적인 일을 하고 있다는 것을 깨달을 것입니다. 신경과학은 지능의 전제에 관심을 두는 반면, AI는 그 결과물에 관심을 둡니다. 만약 우리가 다른 기판(substrate, 예: 실리콘(silicon) 대 탄소(carbon)), 아키텍처(architecture, 예: LLM 대 피질(cortices)), 또는 우리의 상황에 더 잘 맞는 일련의 절충안을 찾을 수 있다면, 우리는 인간 두뇌를 본떠 지능형 에이전트를 모델링할 필요는 없습니다. 저는 더 나아가서, 그것은 불가능하다고 말하고 싶습니다! 왜냐하면 컴퓨터와 AI 시스템은 이미 거의 즉각적인 계산, 임의로 높은 정밀도, 사진 같은 기억력(eidetic memory) 등 인간이 복제할 수 없는 일련의 능력을 보여주기 때문입니다. 요컨대: 인간은 가능한 지능의 공간에서 하나의 사례일 뿐이지만, 원칙적으로 어떤 물리 법칙도 다른 지능의 존재를 막지 않으며, 모든 것을 고려할 때, 우리의 것을 선호할 이유가 없습니다… 규모 극대주의자(scale maximalists)와 LLM 극대주의자(LLM maximalists, 우리가 필요한 모든 요소를 이미 가지고 있다고 생각하는 사람들을 지칭하는 용어)에 대해 경계심을 갖게 되었지만, 프랑수아 숄레(Francois Chollet), 안드레이 카르파티(Andrej Karpathy), 리처드 서튼(Richard Sutton)과 같이 이 분야에서 잘 알려진 몇몇 인물들처럼 어떤 식으로든 AI를 싫어한다고 비난받을 수 없는 사람들조차 일반적으로 이 점에 동의합니다: 다른 방식으로 기능적 역량(functional competence)을 달성할 수 있다면 완전한 생물학적 충실도(biological fidelity)는 필요 없습니다. 다음은 숄레가 LLM(2024)과 AGI 추구(2025)에 대한 그의 입장을 담은 두 개의 트윗입니다: LLM(및 일반적인 딥러닝)은 유용하며, 시간이 지남에 따라 더욱 유용해질 것이다. 또한 그것들은 지능형 에이전트로 변모하지 않을 것이다. 왜냐하면 그들은 단순히 지능의 메커니즘을 구현하지 않기 때문이다. 우리 작업의 요점은 인공 인간을 만드는 것이 아니다. 우주는 우리 자신의 반영보다 훨씬 더 흥미로운 질문들로 가득하다. 요점은 우리가 스스로 할 수 있는 것보다 우주를 더 잘 탐험하고 이해하는 데 도움이 될 새로운 종류의 마음을 만드는 것이다. LLM은 지능을 위한 핵심 메커니즘이 부족하지만, 그렇다고 해서 지능적이기 위해 "인공 인간"과 닮아야 한다는 의미는 아니다! 드와르케시(Dwarkesh) 팟캐스트(podcast)에서 서튼의 인터뷰에 대한 카르파티의 코멘트입니다: "...오늘날의 최첨단 LLM 연구는 동물을 만드는 것이 아니다. 유령을 소환하는 것이다. 유령은 가능한 지능의 공간에서 근본적으로 다른 종류의 지점이라고 생각할 수 있다. 그것들은 인류에 의해 혼란스러워졌다. 인류에 의해 철저히 공학적으로 만들어졌다. 그것들은 불완전한 복제품이며, 인류의 문서를 통계적으로 정제한 것에 약간의 양념을 더한 것이다. 그것들은 플라톤적으로 "쓴 교훈(bitter lesson)"을 완전히 받아들인 것은 아니지만, 적어도 이전에 나온 많은 것들과 비교할 때 "실질적으로" 쓴 교훈을 받아들인 것일 수 있다. 시간이 지남에 따라 우리는 유령을 점점 더 동물에 가깝게 미세 조정(finetune)할 수 있을 것 같다. 그것은 근본적인 비호환성이라기보다는 지능 공간에서의 초기화(initialization) 문제이다. 하지만 그것들이 심지어 더 멀리 발산하여 영구적으로 다르고, 동물 같지 않지만, 여전히 엄청나게 유용하고 세상을 제대로 변화시키는 존재가 될 가능성도 상당히 높다." 그리고 팟캐스트에서 서튼은 아이들이 모방을 하는 것이 아니라 세상에 사는 능동적인 주체(proactive agents)로서 대부분의 것을 경험하는 반면, LLM은 주로 인간이 만든 훈련 데이터(training data)를 모방한다고 말했습니다: "아이들을 보면, 그저 이것저것 시도하고 손을 흔들고 눈을 움직이는 것을 봅니다. 그들이 눈을 움직이는 방식이나 심지어 내는 소리에 대한 모방은 없습니다. 같은 소리를 내고 싶을 수도 있지만, 아기가 실제로 하는 행동에는 목표가 없습니다. 그것에 대한 예시도 없습니다. ...대규모 언어 모델은 훈련 데이터로부터 학습합니다. 경험으로부터 학습하는 것이 아닙니다. 정상적인 삶 동안 결코 이용할 수 없을 것에서 학습합니다. 정상적인 삶에서 이 행동을 해야 한다고 말하는 훈련 데이터는 결코 없습니다." 그들은 약간 다른 방식으로 같은 것을 이야기하고 있습니다: AI 분야(LLM)에서 우리가 가진 것은 충분하지 않으며, 생체 영감(bioinspiration)은 고려할 가치가 있지만, 우리는 "인공 인간"과는 거의 닮지 않고 "동물 같지 않은" 것에 더 가까운 것으로 귀결될 수 있습니다. 인지 과학이 제공할 수 있는 어떤 통찰력도 선험적으로(a priori) 거부하는 것은 말이 안 되지만(이것이 카르파티와 서튼의 주요 주장입니다), 단지 우리가 이미 알고 있다는 이유만으로 우리가 이미 알고 있는 것에 집착할 필요는 없습니다(이것이 숄레의 요점입니다). 이전 섹션에서 제가 끝맺었던 질문에 답하자면: 아마도 80년 전의 원래 뉴런 모델을 다시 검토할 필요는 없을 수도 있습니다. 아마도 우리는 지난 10년 동안 해왔던 것처럼 계속 나아가고, 아무런 재앙도 일어나지 않을 수도 있습니다. 우리는 이 실수를 저질러서는 안 됩니다: AI의 목표는 모든 수준에서 인간 지능을 복제하는 것이 아니며, 결코 아니었습니다. 얕은 추상화(abstraction)로 목표를 달성할 수 있다면, 수상돌기 스파이크, 전압 기울기(voltage gradients), 이상한 형태(morphology)로 복잡하게 만들 필요가 있는가? 복잡성이 지능의 전제 조건인가? 말이 안 됩니다. 진화는 어떤 목적론적 충동도 가지고 있지 않기 때문에 정확히 복잡하게 만들며, 대신 생존 압력과 다른 외부 힘의 조절 하에 진행됩니다. 제가 언급한 연구들은 인간 뉴런이 우리가 생각했던 것보다 얼마나 더 복잡한지에 관심을 두지만, AI 연구자라면 문제를 바라보는 올바른 렌즈가 아닙니다! 뉴런 수준의 충실도가 인지(cognition)에 필수적인가? 저는 반대 가설을 검증함으로써 답하겠습니다: 극적으로 다른 복잡성, 형태, 크기, 기판, 현상학을 가진 두 가지 지능의 구현은 원칙적으로 유사한 지능을 초래할 수 있습니다. 지도가 영토와 일치할 필요는 없지만, 유용한 나침반을 만들 수 있습니다. 이것들은 제 초기 주장에 대한 완전히 합리적인 반론입니다. 결국, 맥컬록과 피츠의 "충분히 좋은" 80년 된 단순한 신경 모델은 체스, 코딩, 수학, 그리고 다양한 분야에서 점점 더 많은 문제에서 인간을 이길 만큼 똑똑한 시스템의 기초가 됩니다. AI 모델의 성능을 평가하는 데 도움이 되는 벤치마크는 우리가 충분히 어려운 작업을 생각해낼 수 없기 때문에 포화 상태에 이르고 있습니다. 그러나 AI의 표준 평가에는 중요한 한계가 있습니다 (여기에는 METR의 장기 작업 분석과 OpenAI의 GDPval처럼 현실 세계의 혼돈스러운 본질을 포착하려는 시도도 포함됩니다): AI 테스트는 우리가 불확실한 것들이 실제로 측정되지 않도록 설계됩니다. 정확히 말하자면, 유일하게 관련 있는 벤치마크는 실제 세계에서의 역량(competence)이며, 지금까지 모든 벤치마크는 현실의 복잡성을 포착하는 데 실패하고, 모든 AI 모델은 인간 유아가 실패하지 않을 방식으로 "야생에서" 테스트될 때 실패합니다 (ARC-AGI와 같이 즉석에서 해결하는 기술을 요구하는 벤치마크나 생산성 및 경제 분석에 고려되지만 실험실 평가에서는 테스트되지 않는 대규모 프로젝트에서 이를 볼 수 있습니다). 예측 역량(prediction-competence)과 환경 역량(environment-competence) 사이에는 거대한 격차가 있습니다 (숄레는 결정화된 지능(crystallized intelligence)과 유동 지능(fluid intelligence)에 대해 논의합니다. 이는 광범위한 기술을 소유하는 것과 새로운 기술을 효율적으로 습득하는 데 능숙한 것 사이의 구별을 강조하는 심리학적 개념입니다). 그래서 아마도, 기능적 성능이 목표이지만, 생물학적 충실도가 우리가 인정하고 싶어 하는 것보다 훨씬 더 깊은 수준의 요구 사항임이 밝혀집니다. 결국, 그것은 우리가 가진 세계 역량의 유일한 참고 자료입니다. "좋아, 어쩌면 이 비행기는 날개 없이 날 수 있을지도 몰라"와 같은 가설을 세우는 것은 괜찮습니다. 비행기라는 작은 동물을 아는 어떤 구경꾼이 보기에는 말도 안 되는 소리일지라도 말입니다. 그 가설은 "좋아, 어쩌면 이 AI는 그 모든 이상하게 생긴 뇌회(gyri)와 뇌구(sulci) 없이도 지능적일 수 있을지도 몰라"라고 말하는 것과 같습니다. 하지만 수년간의 시도와 우리를 태우고 다닐 수 있는 거대한 새 종을 직접 사육하는 비용을 왜소하게 만들 정도의 자금 투입에도 불구하고, 여전히 그 빌어먹을 것을 날게 할 수 없다면, 아마도 전제를 재고할 때일 것입니다. 아마도 비행기에 날개를 추가하는 것은 불합리한 고려 사항이 아닐 것입니다. 하지만 저는 방금 당신에게 속임수를 썼습니다. 왜냐하면 저는 거짓임을 아는 유사성을 그리기 위해 비유를 사용했기 때문입니다. 다른 가설을 제안할 수도 있습니다: "좋아, 어쩌면 이 비행기는 날개를 퍼덕이지 않고도 날 수 있을지도 몰라." 오, 와우, 거의 같은 것이지만 너무나 다릅니다—왜냐하면 사실이기 때문입니다! 비행기는 날개를 퍼덕일 필요가 없습니다! 무엇이 작동하고 무엇이 작동하지 않는지에 대한 제한된 지식이 문제의 근본 원인입니다. 생물학에서 얼마나 많은 것을 취해야 하는가? 우리는 모릅니다. 왜냐하면, 알고 보니, 우리의 은유적인 AI 비행기는 어느 정도 날고 어느 정도 날지 않습니다. 그리고 우리는 두 가지 모두 사실인 이유를 전혀 모릅니다. 왜냐하면 그것이 따르는 패턴 자체가 우리에게 완전히 낯설기 때문입니다. 그래서 이 시점에서 가능한 지능의 공간은 자연이 허용하는 것보다 더 넓다는 것이 꽤 명확합니다 (생물학적 의미에서 그렇습니다; 물리적 의미에서는 AI 시스템조차 빛의 속도와 열역학 제2법칙을 위반할 수 없습니다). 하지만 우리는 새겨진 길에서 너무 멀리 벗어나고 싶지 않을 수도 있습니다. 후회하는 것보다 안전한 것이 낫습니다. 특히 사과가 수조 달러의 손실을 동반할 것이라면 말입니다. 우리는 어떤 요소가 항상 필요하고, 어떤 요소가 우리 지능형 설계자들이 감수할 필요가 없는 진화의 특정 절충안인지 모릅니다 (예: 비행기는 엔진이 필요한 추진력을 생성하기 때문에 날개를 퍼덕이지 않지만, 여전히 공기역학이 부과하는 물리적 제약에 종속되므로 날개는 유지됩니다). 이것이 창조 게임에서 보수적으로 접근하는 것이 최선의 방법인 이유입니다: 진화의 설계 공간은 우리가 우발적인 기벽으로 여기는 지능의 원리를 인코딩하는 구조적 사전 정보(structural priors)를 포함할 수 있습니다. 알 수 없는 미지의 것들과 놀지 마십시오. 우리는 우리가 생각하는 만큼 타협적인 해결책에서 자유롭지 않으며, 우리 것—어느 정도는 생물학적인 것과 유사하고 어느 정도는 다른—이 설계 공간에서 해결 공간으로의 임의적인 매핑(mapping)에 어떻게 영향을 미치는지 모릅니다. 지도가 영토와 일치할 필요는 없지만, 유용한 나침반을 만들 수 있습니다. 그러나 표현이 더 충실할수록 길을 잃기 어렵습니다. 다시 말해, 우리는 자연을 복사할 필요는 없지만, 그 경계(우리가 합리적이라고 생각하는 것과 아직 이해하지 못하는 것)를 존중하는 것이 좋습니다. 리처드 서튼(Richard Sutton)은 2019년 "쓴 교훈(Bitter Lesson)" 에세이로 가장 잘 알려져 있는데, 간단히 말해, 장기적으로는 우리의 제한된 지식을 활용하여 더 나은 AI 시스템을 찾으려고 스스로 노력하는 대신 컴퓨터가 그 부담을 지도록 하는 것이 더 낫다는 내용입니다. 그리고 그것은 괜찮습니다. 아마도 사실일 것입니다. 그것을 쓴 교훈이라고 부른 것이 실수입니다. 기껏해야 씁쓸합니다. 진정한 쓴 교훈이 무엇인지 아십니까? 진화는 목적론적이지 않은 시행착오와 무작위 돌연변이를 통해 더 복잡한 기본 구성 요소를 더 크고, 더 똑똑하고, 더 강력하며, 더 다재다능한 시스템으로 효율적으로 확장하는 데 성공했습니다: 그것은 생물학적 뉴런에서 인간 지능에 이르는 겉보기에 복잡한 경로를 능숙하게 개척했습니다. 서튼의 교훈은 확장될 요소의 복잡성에는 신경 쓰지 않고, 단지 효율적으로 확장 가능해야 한다는 사실에만 주목하기 때문에 강력합니다. 컴퓨팅 자원을 추가하는 것이 약한 휴리스틱(heuristics)을 조정하고 불완전한 지식을 활용하는 것보다 더 잘 작동하지만, 올바른 구성 요소에서 시작하는 것보다 더 좋은 것은 없습니다—그리고 저는 아무것도 없다고 강조합니다. 어떤 면에서, 이 교훈은 "국가처럼 보기(Seeing Like a State)"의 주요 교훈을 상기시킵니다: 세상을 너무 단순화하면 읽기 쉽게 만들겠지만, 동시에 세상을 살아있게 하는 바로 그 요소들을 파괴할 것입니다. 찾을 수 있는 가장 단순한 뉴런 모델을 통해 만들고 있는 인공 뇌를 완벽하게 제어하고 모니터링하려고 시도함으로써, 당신은 거의 공리적인 진실처럼, 그것이 당신이 원하는 방식으로 똑똑하지 않을 것임을 보장했습니다. 분야와 산업 모두 오랫동안 무시해온 이 쓴 교훈에도 불구하고, 당신은 LLM에 계속 계층을 추가할 것입니다. 그래서 저는 이렇게 내기합니다: 당신은 분야의 기반을 다시 검토하지 않아 발생한 불필요한 장애물을 극복하기보다 훨씬 더 빨리 추가할 계층(컴퓨팅 능력, 훈련 데이터 등)이 바닥날 것입니다. 그것을 나는 쓰다고 부를 것입니다.

V. 이 긴 글을 통해 제가 여러 번 언급했지만, 독립적인 섹션을 할애할 가치가 있는 것이 있습니다. 바로 우리가 삶의 모든 측면에서 겪지만, 생물학은 결코 직면하지 않았고 앞으로도 직면하지 않을 종류의 절충안: 자본의 동기입니다. 뉴런을 완전한 복잡성으로 모델링하는 것은 극도로 비싸기 때문에(가능하다면), 그들은 그들이 가진 가장 단순한 모델(저렴하게 확장 가능한)을 선택하고 그것으로 진행했습니다. 어떤 면에서, 진화의 뇌 크기와 뇌 무게 사이의 절충안은 AI 연구소의 모델 크기와 모델 비용 사이의 절충안과 크게 다르지 않습니다. 한 가지를 제외하고는: 진화는 목표가 없으므로 단기적인 승리(유전적 맥락에서 무엇을 의미하든)에 관심이 없지만, 투자자들은 그들이 쫓는 상에 레이저처럼 집중합니다. 그래서 초기에 일을 너무 복잡하게 만들지 않는 것이 합리적이지만, 이것은 우리를 곤란한 상황에 빠뜨렸습니다: 우리는 뉴런의 극도로 단순화된 모델을 사용하여 많은 돈을 절약했지만, 실제 환경에서 인간 지능을 모방하는 것이 더 어렵다는 것이 입증된 지금 우리는 그 대가를 추가로 지불하고 있을지도 모릅니다. 수익성이 없는 AI 회사들이 점점 더 많은 LLM을 훈련하기 위해 데이터센터(datacenter)에 쏟아붓는 수천억 달러는 그들의 투지, 야망, 비전의 증거가 아니라 그들의 성급함의 증거입니다. (그들이 이 모든 것에 신경 쓰지 않고, 실제로는 절대 감시와 같은 더 평범한 목적을 추구하고 있을 가능성은 여기서는 다루지 않겠습니다. 그 주제는 독립적인 기사를 필요로 합니다.) 만약 그들이 처음에 더 나은 기반을 찾는 데 더 많은 자원과 시간을 할애했다면, 신생아가 훨씬 더 똑똑한 존재로 성장하는 데 필요한 데이터의 극히 일부만으로도 충분할 때, 인간 데이터의 전체 코퍼스(corpus)로 LLM을 훈련하는 데 그렇게 많은 에너지와 돈을 낭비하지 않았을 것입니다. 하지만 물론, AI 선구자들이 그렇게 했다면, 어떤 합리적인 투자자도 그들에게 단 한 푼도 주지 않았을 것입니다. ChatGPT가 존재한다면, 그것은 우리가 비용을 지불하기 때문입니다. ChatGPT가 존재한다면, 그것은 더 높은 목표를 세울 만큼 무모한 사람이 없었기 때문입니다. ChatGPT가 존재한다면, 그것은 그 존재를 지배하는 사회경제적 역학이 더 나은 것을 허용하지 않기 때문입니다. 그래서 우리는 진정으로 지능적인 존재로 변모시키려는 바로 그 시스템처럼, 지역 최저점(local minima)에 갇혀 있음을 발견합니다. (우리의 경제 최적화 도구(optimizer)가 AI 시스템을 구동하는 경사 하강법(gradient descent)을 반영한다는 것이 아이러니하지 않습니까—아니면 어쩌면 그것이 패턴일 수도 있습니다…) 자본 배분 에이전트(capital allocation agents)는 항상 이런 일을 합니다 (이 시점에서 당신이 여전히 붙잡고 있을지도 모르는 인류에 대한 마지막 한 조각의 믿음을 포기하고 싶다면, 스콧 알렉산더(Scott Alexander)의 "몰록에 대한 명상(Meditations on Moloch)"을 읽고 마침내 전체 토끼굴을 한 번에 볼 수 있습니다). 산업 혁명(Industrial Revolution)을 생각해 보십시오. 우리가 자동화가 세상을 오염시킬 가치가 있다고 결정했을 때 말입니다. (저는 이것을 기후 변화 논쟁으로 바꾸지 않겠습니다.) 그때는 우리의 발전을 기반으로 할 실제 세계 모델이 없었습니다. 더러운 석탄을 태우는 것이 우리의 작은 푸른 행성의 폐에 해로울 것이며, 대신 일찍이 원자력, 태양광, 풍력에 투자했어야 했다는 것을 어떻게 알 수 있었겠습니까? 게다가, 혁신에는 다른 한계가 있다는 것을 잊어서는 안 됩니다. 그것은 인접한 가능한 것(adjacent possible thing)으로만 경계를 밀어붙일 수 있을 뿐, 더 멀리는 아닙니다. 하지만 AI 분야는 필요한 최고의 영감(즉, 우리 인간)에 접근할 수 있기 때문에 특별합니다 (이것을 논란의 여지가 있는 주장으로 간주하더라도 이해합니다). 그런데도 자본 배분 에이전트들은 여전히 같은 실수를 저지르고 있습니다: 그들은 최적화(optimization)와 재정적 압력이 우리를 더 안전하고 합리적인 길에서 벗어나게 합니다. 불행히도, 과학은 자금을 지원받을 수 있고, 측정 가능하며, 출판 가능한 것을 따릅니다. 그리고 그것은 헤쳐나가기 어려운 절충안입니다. 왜냐하면 자본 배분자의 단기적인 관점에서 볼 때, 더 나은 접근 방식으로 보이는 것이 그 시야를 넘어선 이유로 인해 가장 비싼 것으로 판명될 수 있기 때문입니다. 원치 않는 절충안을 강요받을 때까지 미루는 것—자본주의 역학의 본질—이 우리를 거기서 여기까지 이끌었습니다. "거기서 여기까지"라는 말은 무슨 뜻일까요? 만약 당신이 주의 깊게 읽었다면, 저는 맥컬록과 피츠의 1943년 선구적인 연구와 AI 기업들이 너무 많은 기회와 영향력을 모아 원하는 대로 할 수 있게 되어(그들이 그래야 하는 방식이 아님) 다가오는 2025년 금융 재앙 사이의 연관성을 그리는 데 7,000단어 이상을 쏟아부었습니다. 그리고 그들은 AI 인프라 구축에 수천억 달러를 지출하고 싶어 하는데, 이는 80년 전에는 계산 단위의 단순화된 모델처럼 보였던 것—따라서 자본의 눈에는 환영할 만한 소식이었던 것—이, 인간 두뇌보다 작고 멍청한 LLM 중 하나를 훈련하고 서비스하기 위해 인간이 생산한 모든 데이터와 중간 규모 국가의 전기 요금이 필요하다는 것을 업계가 깨달았을 때 값비싼 실수로 판명되었기 때문에 그렇게 비쌀 수밖에 없습니다. 이 LLM은 다른 조건이 동일하다면 가정용 전구 하나의 에너지를 소비할 것입니다. LLM 비평가이자 컴퓨터 과학의 거장인 그레이디 부치(Grady Booch)가 오픈AI CEO 샘 알트만(Sam Altman)이 "글로벌 반도체 산업을 재편"하는 데 필요하다고 말한 7조 달러에 대해 (현재는 삭제된 트윗에서) 응답한 내용을 인용하자면: "칩을 만들고 영국 전체의 에너지 소비량에 해당하는 에너지를 충당하는 데 7조 달러가 필요하다면, 저는 높은 확신을 가지고 말씀드릴 수 있습니다. 당신은 잘못된 아키텍처를 가지고 있습니다." 실용적이고 경험적인 성공의 보상은 이론적 불순물의 인식론적 비용보다 큽니다—그렇지 않을 때까지는 말입니다. 시장의 보이지 않는 손(invisible hand)이 온순한 초지능(superintelligence)의 현존하는 예시라고 말할 때, 당신은 이 예시를 반론으로 사용할 수 있습니다. 그리고 만약 당신이 여전히 우리가 실제 신과 같은 초지능을 구축해야 한다고 확신한다면, 그것이 무엇이든 간에, 저는 우리가 서 있는 기반을 평소보다 조금 더 재고해야 할 수도 있다는 것을 당신에게 설득했기를 바랍니다. 저는 맥컬록과 피츠 뉴런이 신이 되는 것이 합리적인 목표라고 생각하지 않습니다. 이 에세이에서 보이는 것과는 달리, 저는 AI 분야가 내린 선택에 대해 분개하지 않습니다. 그들이 생물학의 절충안에 주의를 기울이지 않은 것은 괜찮습니다. 왜냐하면 인간 세계는 그 자체의 절충안을 가지고 있기 때문입니다. 하지만 자연의 경고—자연은 우리보다 훨씬 현명합니다—를 무시하고 결국 우리 자신의 무덤을 파게 된다면 암울하게 아이러니하지 않겠습니까? 맥컬록과 피츠의 신경 모델이 너무 단순했고, AI가 성과를 내지 못해 우리가 지금 금융 재앙에 직면하고 있다는 두 현실 사이에 합리적으로 정확한 궤적을 그릴 수 있는지는 명확하지 않습니다. 하지만 수년이 흘렀음에도 불구하고, 이런 이야기를 쓰고 우리가 제대로 다루지 못한 그 첫 번째 은유의 그림자가 여전히 우리를 쫓고 있다고 어느 정도 확고하게 주장하는 것이 여전히 사소하다는 것은 놀랍습니다. 카르파티가 LLM은 우리가 다른 차원에서 소환한 유령이라고 말할 때 그는 옳습니다. 가장 멀리 떨어진 실수가 가장 오랫동안 당신을 괴롭히는 법이니까요.

사실은 사람에 대한 AI 블로그
구독