(출처 [18, 20, 21]) 최근 OpenAI는 5년 전 GPT-2 [13] 이후 첫 공개 거대 언어 모델(LLM)인 GPT-oss [1, 2]를 출시하며, AI 분야에 새로운 이정표를 세웠습니다. GPT-oss 사이의 기간 동안 거대 언어 모델(LLM) 연구는 지속적인 변화를 겪었으며, 이는 다양한 혁신으로 이어졌습니다. 이 기간 동안 많은 주요 혁신이 있었지만, 그들의 연구는 대부분 내부적으로 유지되었습니다. GPT-oss는 OpenAI의 거대 언어 모델(LLM) 연구에 대한 드문 엿보기를 제공하며, 이는 커뮤니티에 큰 영향을 미쳤습니다. 이 개요에서는 이 드문 기회를 활용하여 오픈 소스 모델의 중요성과 그 파급 효과를 심층적으로 분석할 것입니다. 공개된 기술적 세부 사항을 단순히 나열하는 것을 넘어, GPT-oss가 AI 생태계에 미치는 광범위한 영향을 조명하고자 합니다. 이 개요는 길고(아마도 너무 길고), 거대 언어 모델(LLM) 연구의 다양한 느슨하게 관련된 주제를 다루지만, 실제 세계 적용에 중점을 둡니다. 이 각 주제를 시간을 들여 살펴보면 GPT-oss가 어떻게 작동하는지에 대한 깊은 이해를 얻고, 미래 기술 동향을 예측하는 데 도움이 될 것입니다. AI의 발전은 기술적 세부 사항뿐만 아니라, 그 기술이 사회에 어떻게 통합되고 활용되는지에 대한 이해를 필요로 합니다.

### GPT-oss 한눈에 보기: 기술과 영향

“이 모델들은 강화 학습과 o3 및 기타 최첨단 시스템을 포함한 OpenAI의 가장 진보된 내부 모델에서 얻은 기술을 혼합하여 훈련되었습니다.” - 출처 [1]

GPT-oss 출시는 두 가지 다른 모델인 GPT-oss-20b와 GPT-oss-120b를 포함하며, 이들은 모두 허용적인 Apache-2.0 라이선스로 출시되어 오픈 소스 커뮤니티에 큰 기여를 했습니다. 이들은 전문가 혼합(Mixture-of-Experts, MoE) 기반의 추론 모델로, 텍스트 전용이며 주로 영어 데이터로 훈련되었지만, 다국어 처리 능력도 향상되었습니다. 최근 LLM 개발의 핵심 트렌드 중 하나는 효율성 극대화입니다. 전문가 혼합(MoE) 아키텍처와 양자화 인식 훈련(quantization-aware training)은 이러한 추세의 대표적인 예시로, 모델의 배포 가능성을 혁신적으로 개선합니다. MXFP4 (~4비트) 정밀도를 사용하면 더 큰 모델은 단일 80Gb GPU에서 호스팅될 수 있으며, 이는 접근성을 크게 높였습니다. 이러한 효율성 덕분에 더 많은 개발자와 연구자가 고성능 LLM을 직접 실험하고 활용할 수 있게 되었습니다. 이 모델들은 사고의 사슬(chain of thought, CoT) 추론 및 안전성을 최적화하기 위해 광범위하게 후처리 훈련(post-trained)되어, 복잡한 문제 해결 능력과 사용자 안전 보장이라는 두 가지 중요한 목표를 동시에 달성하고자 합니다.

**에이전트(agents)에 대한 강조.** 두 GPT-oss 모델 모두 에이전트 워크플로우(agentic workflows)에 최적화되어 있으며, (합리적으로) 긴 131k 토큰의 컨텍스트 윈도우(context window)와 강력한 도구 사용(tool use) 기능을 제공합니다. 에이전트 LLM의 등장은 AI 애플리케이션의 패러다임을 변화시키고 있습니다. OpenAI는 에이전트 워크플로우(예: 함수 호출(function calling), 도구 사용(tool use), 추론, 구조화된 출력(structured outputs) 등)의 패턴을 보다 원활하게 처리하기 위해 GPT-oss 훈련 및 상호 작용을 위한 새로운 하모니 프롬프트 형식(Harmony prompt format)을 출시했습니다. 이는 다양한 거대 언어 모델(LLM) 상호 작용 패턴을 포착할 수 있는 유연하고 계층적인 채팅 템플릿(chat template)으로, 개발자에게 새로운 가능성을 열어줍니다. GPT-oss 모델은 또한 시스템 메시지(system message)에 노력 수준을 명시적으로 지정하여 추론 노력(즉, 낮음, 중간 또는 높음 노력 수준)을 조정할 수 있는 기능을 제공하며, 이는 사용자 경험을 개인화합니다. 이러한 유연성은 다양한 사용 사례에 맞춰 모델의 동작을 세밀하게 조정할 수 있게 합니다.

**내부 평가.** (출처 [1]) OpenAI가 공개한 평가에 따르면 GPT-oss-120b는 o4-mini와 비슷한 성능을 보이며, 시장의 다른 모델들과 경쟁하고 있습니다. 또한 OpenAI는 출시 기간 동안 새로 출시된 HealthBench의 평가를 기반으로 이 모델들의 건강 관련 작업에 대한 강력한 기능을 크게 강조했습니다. 이는 AI가 의료 분야에서 진단 보조, 약물 개발, 환자 관리 등 다양한 혁신을 가져올 잠재력을 보여줍니다. 그러나 GPT-oss 모델은 이 벤치마크에서 전체 o3 모델의 성능에는 여전히 미치지 못하지만, 지속적인 개선이 기대됩니다. 이는 오픈 소스 모델이 상용 모델과 경쟁하기 위한 여정이 아직 남아 있음을 시사합니다.

(출처 [1]) 예상대로 OpenAI는 GPT-oss 모델이 추론 노력(reasoning effort)에 따라 일반적인 추론 시간 스케일링 법칙(inference-time scaling laws)을 따른다고 강조합니다. 모델은 점진적으로 더 긴 추론 경로(reasoning traces)를 생성함에 따라 성능이 향상되며, 따라서 추론 중에 더 많은 컴퓨팅을 소비합니다. 이는 LLM의 성능과 컴퓨팅 비용 사이에 존재하는 본질적인 트레이드오프를 보여줍니다. 최적의 균형을 찾는 것은 모델 개발의 중요한 과제입니다.

**대중의 반응.** (출처 [1]) 공개 거대 언어 모델(LLM) 커뮤니티에 공개된 후 GPT-oss 모델은 엇갈린 피드백을 받았으며, 이는 기술 도입의 일반적인 현상입니다. 예를 들어, 일부 사용자는 이 모델들이 높은 환각률(hallucination rate)을 보인다고 지적했지만, 다른 사용자들은 모델 설정과 관련된 초기 문제들이 해결된 후에는 모델들이 실제로 꽤 좋다고 말합니다. 이는 오픈 소스 프로젝트의 초기 단계에서 흔히 발생하는 현상으로, 커뮤니티의 적극적인 참여와 피드백이 모델 개선에 필수적임을 보여줍니다. GPT-oss 모델에 대한 다른 일반적인 비판으로는 프롬프트의 과도한 거부(over-refusal of prompts)와 복잡한 형식 사용이 있었습니다. 간단히 말해, 처음에는 인식이 좋지 않았지만, 일반적인 도구의 잔존 문제들이 해결되면서 서서히 개선되었습니다. 이러한 초기 반응은 새로운 기술이 시장에 도입될 때 미디어의 과장된 기대와 실제 사용자 경험 사이의 괴리를 반영하기도 합니다. GPT-oss의 현실은 온라인의 양극화되고 클릭베이트적인 반응의 중간 어딘가에 있습니다. 이들은 역대 최고의 모델은 아니지만, 세계 최고의 거대 언어 모델(LLM) 연구소 중 하나가 출시한 오픈 웨이트(open weights) 모델이라는 점에서 중요합니다. AI2, Cohere, Meta와 같은 소수의 선두 연구소들이 오픈 웨이트 모델을 활발히 출시하며 LLM 생태계를 확장하고 있습니다. 이러한 흐름 속에서 GPT-oss는 단순한 모델을 넘어, 오픈 소스 AI의 미래에 대한 중요한 실험이자 기여로 평가받아야 합니다. 그러므로 OpenAI가 GPT-oss에 대해 제공한 관련 기술적 세부 사항을 심층적으로 분석하여, 그 가치를 평가해봅시다.

### 모델 아키텍처(Model Architecture): 혁신과 진화

“GPT-oss 모델은 GPT-2 및 GPT-3 아키텍처를 기반으로 구축된 자기회귀(autoregressive) 전문가 혼합(Mixture-of-Experts, MoE) 트랜스포머(transformers)입니다.” - 출처 [1]

먼저 GPT-oss 모델의 모델 아키텍처(model architecture)를 다루며, 그 핵심 원리를 탐구할 것입니다. 트랜스포머 아키텍처(transformer architecture)는 현대 LLM의 근간을 이루며, 그 기본적인 이해는 GPT-oss와 같은 최신 모델을 분석하는 데 필수적입니다. 여기에서 우리는 GPT-oss 아키텍처의 각 고유한 구성 요소를 처음부터 설명하는 대신, 주요 혁신에 초점을 맞출 것입니다. 특히, 스케일링, 효율성, 그리고 긴 컨텍스트 처리를 가능하게 한 핵심 설계 결정들을 중점적으로 다룰 것입니다. 이 주제에 대한 추가 자료 및 다른 공개 모델과의 비교는 아래 Sebastian Raschka의 훌륭한 개요를 참조하십시오.

**AI의 미래**
**GPT-2에서 gpt-oss까지: 아키텍처 발전 분석**
OpenAI는 이번 주에 새로운 오픈 웨이트(open-weight) 거대 언어 모델(LLM)인 gpt-oss-120b와 gpt-oss-20b를 출시했습니다. 이는 2019년 GPT-2 이후 첫 오픈 웨이트 모델입니다. 그리고 네, 몇 가지 영리한 최적화 덕분에 로컬에서 실행할 수 있습니다 (하지만 이에 대해서는 나중에 더 자세히… 더 읽기).
3개월 전 · 좋아요 169개 · 댓글 17개 · Sebastian Raschka, PhD

#### 트랜스포머 구조(Transformer Structure): 현대 LLM의 기반

표준 디코더 전용 트랜스포머 아키텍처(decoder-only transformer architecture)는 AI 발전에 필수적인 요소입니다. 이 아키텍처는 GPT 스타일의 거대 언어 모델(LLM)에서 보편적으로 사용되며, 그 이유는 시퀀스 생성 작업에 특히 효과적이기 때문입니다.

**임베딩 차원(Embedding dimension).** 이 모델의 입력은 텍스트 입력(또는 프롬프트)을 토큰화(tokenizing)하여 생성된 토큰 벡터(token vectors)의 시퀀스입니다. 임베딩 차원은 모델의 표현력을 결정하는 중요한 요소입니다. GPT-oss 모델의 경우, 이 벡터들은 2,880의 고정된 차원(dimension)을 가지며, 이 동일한 임베딩 차원(embedding dimension)은 거대 언어 모델(LLM)의 모든 레이어(layer)를 통해 유지됩니다. 이는 모델이 각 토큰의 의미를 일관된 고차원 공간에서 표현하도록 돕습니다.

**블록 구조(Block structure).** 디코더 전용 아키텍처(decoder-only architecture)는 반복되는 디코더 블록(decoder blocks)으로 구성되어 모델의 깊이를 형성합니다. GPT-oss 모델은 이러한 블록을 24개(GPT-oss-20b) 또는 36개(GPT-oss-120b) 포함합니다. 이러한 블록의 스케일링은 모델의 용량과 복잡성을 결정합니다. 각 디코더 블록은 동일한 핵심 구성 요소를 가지며, 이는 모델의 안정적인 학습을 돕습니다: 정규화(normalization), 마스크된 다중 헤드 자체 어텐션(masked multi-headed self-attention), 피드포워드 변환(feed-forward transformation), 그리고 잔차 연결(residual connections). GPT-oss 모델은 현재 거대 언어 모델(LLM) 아키텍처에서 가장 일반적인 선택인 사전 정규화 구조(pre-normalization structure)를 채택합니다. 이는 디코더 블록의 정규화 레이어(normalization layers)가 어텐션(attention) 및 피드포워드 레이어(feed-forward layers) 이전에 배치되어 다음 구조를 생성한다는 것을 의미합니다. 정규화 전략은 LLM 훈련의 안정성과 성능에 큰 영향을 미칩니다.

**디코더 블록(Decoder Block)**
입력 → 정규화 → 마스크된 자체 어텐션 → 잔차 연결 → 정규화 → 피드포워드 네트워크 → 잔차 연결 → 디코더 블록 출력

사전 정규화 구조(pre-normalization structure)가 가장 일반적이지만, 사전 정규화(pre-normalization)와 사후 정규화(post-normalization) 중 어느 것이 더 우수한지에 대한 명확한 답은 아직 논쟁 중입니다. 사실, 최근 연구에서는 사후 정규화(post-normalization)가 훈련 안정성에 도움이 된다는 것을 보여주기도 했습니다 [3]. 이러한 지속적인 연구는 LLM 아키텍처의 최적화가 여전히 활발한 연구 분야임을 시사합니다.

**정규화(Normalization).** 초기 트랜스포머(transformers)는 레이어 정규화(layer normalization)를 정규화 레이어(normalization layer)의 표준 선택으로 사용했지만, 새로운 대안들이 등장했습니다. 최근에는 많은 거대 언어 모델(LLM)이 레이어 정규화(layer normalization)를 제곱평균제곱근 레이어 정규화(root mean square layer normalization, RMSNorm) [4]로 대체하여 효율성을 높였습니다. RMSNorm은 레이어 정규화의 더 간단하고 계산적으로 효율적인 버전으로, 훈련 가능한 파라미터(trainable parameters)가 더 적으면서도 유사한 성능을 제공합니다. GPT-oss 모델은 모든 디코더 블록(decoder blocks)에서 제곱평균제곱근 레이어 정규화(RMSNorm)를 사용하여 안정적인 훈련을 목표로 합니다. 정규화는 모델의 수렴 속도와 일반화 성능을 향상시키는 데 필수적인 역할을 합니다.

#### 어텐션 구현(Attention Implementation): 정보 흐름의 최적화

**단일 어텐션 헤드(attention head)를 사용한 마스크된 자체 어텐션(masked self-attention)의 묘사**

**마스크된 자체 어텐션(Masked self-attention).** 마스크된 자체 어텐션(masked self-attention) 연산은 시퀀스 예측에 필수적입니다. GPT-oss를 포함한 대부분의 거대 언어 모델(LLM)은 다중 헤드 마스크된 자체 어텐션(multi-headed masked self-attention)을 활용하여 복잡한 패턴을 학습합니다. 다중 헤드 어텐션은 여러 "관점"에서 입력 시퀀스를 분석하여, 모델이 다양한 유형의 관계를 동시에 포착할 수 있도록 합니다. 이는 각 자체 어텐션 레이어(self-attention layer)에 대해 여러 자체 어텐션(self-attention) 연산이 병렬로 실행된다는 것을 의미합니다. GPT-oss 모델의 경우, 각 자체 어텐션 레이어(self-attention layer)는 64개의 병렬 어텐션 헤드(attention heads)를 통해 정보 처리를 가속화합니다. 이 각 어텐션 헤드(attention heads)는 64차원의 벡터를 사용합니다. 키(key), 쿼리(query), 값(value) 투영은 임베딩 벡터를 더 작은 차원으로 변환하여 어텐션 계산의 효율성을 높입니다.

**다중 및 그룹 쿼리 어텐션(Multi and grouped-query attention).** (출처 [6]) 이전 연구에서는 다중 쿼리 [5]와 그룹 쿼리 어텐션 [6]을 모두 제안하여 효율성을 개선했습니다. 이 기술들은 각 어텐션 헤드에 고유한 키(keys)와 값(values)을 갖는 대신, 여러 어텐션 헤드(attention heads) 간에 키(keys)와 값(values)을 공유함으로써 메모리 사용량과 계산 비용을 줄입니다.

“키(keys)와 값(values)을 로드하는 데 필요한 메모리 대역폭(memory bandwidth)은 다중 쿼리 어텐션(multi-query attention)을 통해 크게 줄일 수 있습니다. 이는 여러 쿼리 헤드(query heads)를 사용하지만 단일 키(key) 및 값 헤드(value heads)를 사용합니다. 그러나 다중 쿼리 어텐션(MQA)은 품질 저하 및 훈련 불안정성(training instability)을 초래할 수 있습니다.” - 출처 [6]

여러 어텐션 헤드(attention heads) 간에 키(keys)와 쿼리(queries)를 공유하는 것은 파라미터(parameter) 및 컴퓨팅 효율성(compute efficiency) 모두에 이점을 제공하며, 이는 모델의 실용성을 높입니다. 모델의 KV 캐시(KV cache)에서 검색해야 하는 키(keys)와 값(values)이 더 적기 때문에 추론 시 메모리 대역폭(memory bandwidth) 사용량이 감소합니다. 이는 LLM을 실제 환경에 배포할 때 중요한 고려 사항입니다. 메모리 대역폭(memory bandwidth)이 트랜스포머(transformer) 추론 속도에 핵심적인 병목 현상이 될 수 있으므로, 효율적인 아키텍처가 중요합니다. 그러나 키(keys)와 값(values) 공유에 너무 극단적이어서는 안 됩니다. 지나친 공유는 모델의 표현력을 제한하고 성능 저하를 초래할 수 있습니다. 그룹 쿼리 어텐션(grouped-query attention)은 더 작은 그룹 간에 키(keys)와 값(values)을 공유함으로써 성능과 효율성 사이의 균형을 맞춥니다. 특히, GPT-oss는 두 모델 크기 모두에서 그룹 쿼리 어텐션(grouped-query attention)에 대해 8개의 그룹 크기를 사용합니다. 이는 8개의 어텐션 헤드(attention heads) 그룹 간에 키(keys)와 값(values)이 공유된다는 것을 의미하며, 자원 활용을 최적화합니다.

**희소 어텐션(Sparse attention).** GPT-oss 모델의 디코더 블록(decoder blocks) 내에서, 우리는 밀집(dense) 어텐션과 지역적으로 밴드화된 희소 어텐션(locally-banded sparse attention) [7]을 번갈아 사용합니다. 희소 어텐션은 긴 시퀀스 처리 시 발생하는 이차 복잡도 문제를 완화하기 위해 도입되었습니다. 인과적 마스크(causal mask)가 적용되어 어텐션 행렬(attention matrix)의 마스크된 값(즉, 시퀀스(sequence)의 각 토큰(token) 뒤에 오는 값)을 음의 무한대 4로 설정합니다. 이는 자체 어텐션(self-attention) 연산에 의해 고려되어서는 안 되는 토큰(token)이 소프트맥스 변환(softmax transformation)이 적용된 후 0의 확률을 갖도록 보장하며, 모델이 미래 정보를 참조하지 않도록 합니다.

**인과적 자체 어텐션(causal self-attention)에서의 마스킹(Masking)**

자체 어텐션(self-attention)을 계산하는 것은 이차 복잡도(quadratic complexity)를 가지며, 이는 긴 시퀀스에서 병목이 될 수 있습니다. 간단히 말해, 이는 자체 어텐션(self-attention)이 긴 시퀀스(sequence)에 적용될 때 계산 비용이 많이 든다는 것을 의미합니다. 그러나 위의 마스킹 패턴을 보면, 거대 언어 모델(LLM)이 각 토큰(token)에 선행하는 전체 시퀀스(sequence)를 실제로 볼 필요가 있는지 의문이 제기됩니다. Longformer [7]에서 제안한 바와 같이, 자체 어텐션(self-attention)이 계산되는 윈도우(window)를 제한함으로써 컴퓨팅 비용을 절약할 수 있습니다. 이 아이디어는 슬라이딩 윈도우 어텐션(sliding window attention) 5이라고 불리며, 여러 거대 언어 모델(LLM)에서 성공적으로 채택되어 효율성을 높였습니다. 우리는 자체 어텐션(self-attention) 연산에 의해 고려되는 선행 토큰(token)의 범위를 제한하기 위해 마스킹 행렬(masking matrix)을 수정합니다. 이전에는 각 토큰(token) 뒤에 오는 토큰(token)만 마스크했지만, 이제는 과거에 충분히 멀리 떨어진 토큰(token)도 마스크합니다. 이 아이디어는 GPT-oss 모델 [1, 2]에서 "지역적으로 밴드화된 희소 어텐션(locally banded sparse attention)"이라고 불리며, 긴 컨텍스트를 효율적으로 처리하는 데 기여합니다. GPT-oss 모델은 모든 마스크된 자체 어텐션 모듈(즉, 1:1 비율)을 슬라이딩 윈도우 어텐션(sliding window attention)으로 대체하여 효율성을 추구합니다. 첫 번째 어텐션 레이어(attention layer)는 밀집 자체 어텐션(dense self-attention)을 사용하고, 두 번째 레이어는 슬라이딩 윈도우 어텐션(sliding window attention)을 사용하는 식으로 번갈아 적용됩니다. 일부 레이어(layer)에서 슬라이딩 윈도우 어텐션(sliding window attention)을 채택함으로써, 자체 어텐션(self-attention)의 이차 복잡도(quadratic complexity)를 피하여 모델 아키텍처(model architecture)의 효율성을 향상시킵니다. 이상적으로는 이러한 효율성 향상이 모델 품질의 상응하는 저하 없이 이루어지지만, 이는 채택된 정확한 설정(예: 윈도우(window) 크기 또는 레이어(layer) 비율)에 따라 달라질 수 있습니다. GPT-oss에서 사용되는 윈도우(window) 크기는 128 토큰(token)으로, 다른 모델에 비해 작지만, 특정 시나리오에서는 충분히 효과적일 수 있습니다. 예를 들어, Gemma-2 및 3은 각각 4K 및 1K 토큰(token)의 윈도우(window) 크기를 사용합니다. 이러한 차이는 모델의 설계 목표와 처리할 수 있는 컨텍스트 길이에 대한 가정에 따라 달라집니다. 밀집(dense) 및 희소 어텐션 레이어(sparse attention layers)의 1:1 비율은 보수적인 선택으로, 안정성을 중시합니다. 사실, 다른 모델들은 훨씬 더 높은 희소성 비율(sparsity ratios)을 성공적으로 탐색했습니다. 예를 들어, Gemma-3은 5:1 비율을 채택합니다. 이는 5개의 슬라이딩 윈도우 어텐션 레이어(sliding window attention layers)마다 하나의 밀집 어텐션 레이어(dense attention layer)가 있다는 것을 의미하며, 다양한 접근 방식이 존재합니다.

**어텐션 싱크(Attention sinks).** 기억하시겠지만, 자체 어텐션(self-attention) 내의 어텐션 행렬(attention matrix)은 시퀀스 데이터 처리에 중요한 역할을 합니다. 우리는 쿼리(query)와 (전치된) 키 행렬(key matrix)의 곱을 취합니다. 이 연산은 S x S 행렬을 생성하며, 시퀀스(sequence)의 길이에 따라 복잡도가 달라집니다. 마스킹(masking)하고 이 행렬의 값을 임베딩 차원(embedding dimension)의 제곱근 6으로 나눈 후, 행별 소프트맥스(row-wise softmax)를 적용하여 시퀀스(sequence)의 각 토큰(token) (또는 행렬의 행)에 대해 시퀀스(sequence)의 다른 모든 토큰(token)에 대한 확률 분포(probability distribution)를 형성합니다. 이 어텐션 행렬(attention matrix)에 값 행렬(value matrix)을 곱하여 자체 어텐션(self-attention) 연산을 완료하며, 최종 출력을 생성합니다. 실제로는 이는 각 토큰(token)에 대한 값 벡터(value vectors)의 가중 합(weighted sum)을 취하며, 가중치(weights)는 어텐션 점수(attention scores)에 의해 주어집니다.

자체 어텐션(self-attention)은 자연스러운 형태로 놀랍도록 잘 작동하지만, 내부 소프트맥스(softmax)로 인해 흥미로운 문제가 발생합니다. 즉, 어텐션 점수(attention scores)는 유효한 확률 분포(probability distribution)를 형성하도록 강제되며, 이는 특정 제약을 만듭니다. 이는 어텐션 점수(attention scores)가 모두 양수여야 하고 합이 1이어야 한다는 것을 의미합니다. 따라서 시퀀스(sequence)의 적어도 하나의 토큰(token)은 어떤 가중치(weight)를 받아야 하며, 이는 모델의 동작에 영향을 미칩니다. 모델이 어떤 토큰(token)에도 주의를 기울이지 않는 것은 불가능합니다. 자체 어텐션(self-attention)의 이 속성은 실제 거대 언어 모델(LLM)에서 "어텐션 싱크(attention sinks)"라는 현상을 유발할 수 있습니다. 이전 연구 [8]에서는 거대 언어 모델(LLM)이 시퀀스(sequence)에서 의미론적으로 무의미한 토큰(token)에 높은 어텐션 점수(attention scores)를 할당하는 경향이 있음을 지적했습니다. 이러한 높은 가중치(weight)를 거짓으로 받는 토큰(token) (일반적으로 시퀀스(sequence)의 첫 번째 토큰(token))은 일반적으로 "어텐션 싱크(attention sinks)"라고 불립니다. 이 경험적 관찰은 거대 언어 모델(LLM)이 시퀀스(sequence)에서 어떤 토큰(token)에도 주의를 기울이지 못하는 무능력에서 비롯되며, 이는 모델의 한계로 작용합니다. 또한, 거대 언어 모델(LLM)이 어텐션 싱크(attention sinks)에 할당하는 매우 높은 점수는 실제 문제를 야기할 수 있습니다. 이러한 이상치 어텐션 값(outlier attention values)은 양자화(quantization)를 더 어렵게 만들어 모델 최적화에 방해가 됩니다.

“우리는 자기회귀(autoregressive) 거대 언어 모델(LLM)의 흥미로운 현상을 발견했습니다: 놀랍도록 많은 어텐션 점수(attention score)가 언어 모델링 작업과의 관련성 여부와 관계없이 초기 토큰(token)에 할당됩니다… 우리는 이 토큰(token)들을 어텐션 싱크(attention sinks)라고 부릅니다. 의미론적 중요성이 부족함에도 불구하고, 이들은 상당한 어텐션 점수(attention scores)를 수집합니다. 우리는 그 이유를 소프트맥스(Softmax) 연산에 기인한다고 봅니다. 이 연산은 모든 문맥 토큰(contextual tokens)에 대해 어텐션 점수(attention scores)의 합이 1이 되도록 요구합니다. 따라서 현재 쿼리(query)가 많은 이전 토큰(token)에서 강력한 일치를 찾지 못하더라도, 모델은 이 불필요한 어텐션 값(attention values)을 어딘가에 할당하여 합이 1이 되도록 해야 합니다. 초기 토큰(token)이 싱크 토큰(sink tokens)이 되는 이유는 직관적입니다: 초기 토큰(token)은 자기회귀(autoregressive) 언어 모델링 특성 때문에 거의 모든 후속 토큰(token)에 보이며, 이는 이들이 어텐션 싱크(attention sinks) 역할을 하도록 더 쉽게 훈련될 수 있게 합니다.” - 출처 [8]

GPT-oss 모델에서 이 문제를 해결하기 위해 저자들은 Evan Miller의 이 블로그 게시물에 설명된 기술과 매우 유사한 접근 방식을 사용합니다. 각 어텐션 헤드(attention head)에 대해 우리는 다른 모델 파라미터(model parameter)와 유사하게 학습되는 추가 학습 가능한 편향(learnable bias)을 생성합니다. 이 편향(bias)은 자체 어텐션(self-attention)의 내부 소프트맥스(softmax) 연산의 분모에만 나타나며, 어텐션 분포를 조절합니다. 일부 어텐션 헤드(attention head)에서 이 편향(bias)에 높은 값을 설정함으로써, 거대 언어 모델(LLM)은 시퀀스(sequence)에서 어떤 토큰(token)에도 주의를 기울이지 않도록 선택할 수 있으며, 어텐션 싱크(attention sinks)와 관련된 알려진 문제를 효과적으로 해결합니다. 이 접근 방식은 GPT-oss 모델 카드(model card)에서 중요하게 다루어집니다.

“각 어텐션 헤드(attention head)는 소프트맥스(softmax)의 분모에 학습된 편향(learned bias)을 가집니다. 이는 오프-바이-원 어텐션(off-by-one attention) 및 어텐션 싱크(attention sinks)와 유사하며, 어텐션 메커니즘(attention mechanism)이 어떤 토큰(token)에도 주의를 기울이지 않도록 합니다.” - 출처 [2]

### 전문가 혼합(Mixture-of-Experts, MoE): 스케일링의 새로운 지평

두 GPT-oss 모델 모두 전문가 혼합(Mixture-of-Experts, MoE) 아키텍처를 사용하며, 이는 효율성을 위한 핵심 요소입니다. 디코더 전용 아키텍처(decoder-only architecture)와 비교하여, 전문가 혼합(MoE)은 각 디코더 블록(decoder block)의 피드포워드 모듈(feed-forward module)을 수정하여, 모델이 특정 데이터 부분에 더 전문화된 처리를 적용할 수 있도록 합니다. 표준 아키텍처는 하나의 피드포워드 신경망(feed-forward neural network)을 가지지만, MoE는 이를 확장합니다. 이는 일반적으로 비선형 활성화(non-linear activation) (즉, GPT-oss 모델은 특히 SwiGLU 활성화(SwiGLU activation) [2]를 사용함)가 중간에 있는 두 개의 피드포워드 레이어(feed-forward layers)로 구성되며, 모든 토큰(token)이 개별적으로 통과합니다. SwiGLU는 전통적인 ReLU 활성화 함수보다 더 부드러운 전환을 제공하여 모델의 학습 안정성과 성능을 향상시키는 것으로 알려져 있습니다. 블록의 피드포워드 구성 요소에 단일 피드포워드 네트워크(feed-forward network)를 갖는 대신, 전문가 혼합(MoE)은 여러 피드포워드 네트워크(feed-forward networks)를 생성하여 전문성을 강화합니다. 우리는 이 각 네트워크를 "전문가(expert)"라고 부릅니다. 표준 디코더 전용 트랜스포머(decoder-only transformer)에서 시작하여, 전문가 혼합(MoE)은 트랜스포머(transformer)의 피드포워드 모듈(feed-forward modules)을 전문가 혼합(MoE) (또는 전문가) 레이어(layers)로 변환하며, 해당 레이어(layer)의 원래 피드포워드 네트워크(feed-forward network)의 여러 독립적인 복사본을 가집니다.

**표준 피드포워드 네트워크(Standard Feed-Forward Network)**

**전문가 혼합(MoE) 레이어(Layer)**

일반적으로 효율성상의 이유로 모델의 모든 피드포워드 레이어(feed-forward layer)를 전문가 혼합(MoE) 레이어(layer)로 변환하지는 않으며, 전략적인 배치가 중요합니다. 대신, 우리는 P의 보폭(stride)을 사용하여 전문가 혼합(MoE) 레이어(layers)를 인터리브(interleave)합니다. 즉, 트랜스포머(transformer)의 P번째 레이어(layer)마다 전문가 혼합(MoE) 레이어(layer)로 변환되어 모델의 복잡성을 조절합니다.

**라우팅(Routing).** 전문가 혼합(MoE)의 주요 이점은 효율성이지만, 전문가(experts)만 사용한다고 해서 효율성이 항상 보장되지는 않습니다! 사실, 각 피드포워드 모듈(feed-forward module)의 여러 복사본을 가지기 때문에 총 파라미터(parameters)와 컴퓨팅(compute)이 훨씬 더 커집니다. 이 때문에 MoE 아키텍처는 "활성 파라미터(active parameters)" 개념을 도입합니다. 효율성 이점을 얻으려면 이 아키텍처에 희소성(sparsity)을 추가해야 하며, 이는 MoE의 핵심입니다. d차원 토큰 벡터(token vector)로 표현되는 단일 토큰(token)을 고려해 봅시다. 우리의 목표는 이 토큰(token)에 대해 순방향 전달(forward pass)을 수행할 전문가(experts)의 부분 집합(크기 k)을 선택하는 것입니다. 즉, 이 토큰(token)은 이 전문가(experts)들에게 "라우팅(routed)"될 것입니다. 이 라우팅(routing) 연산을 수행하는 표준적인 방법은 토큰 벡터(token vector)를 입력으로 받아 전문가(experts)를 예측하는 선형 레이어(linear layer)를 통하는 것입니다. 우리는 소프트맥스(softmax) 연산을 적용하여 각 토큰(token)에 대한 전문가(experts) 집합에 대한 확률 분포(probability distribution)를 형성할 수 있습니다. 그런 다음, 이 확률 분포(probability distribution)를 사용하여 각 토큰(token)이 라우팅(routed)될 상위 K 전문가(experts)를 선택합니다.

**라우팅 메커니즘(Routing Mechanism)**

단순함에도 불구하고, 이 선형 라우팅(routing) 연산은 OpenAI가 GPT-oss 모델에 채택한 정확한 접근 방식입니다 (출처 [2]): “각 전문가 혼합(MoE) 블록은… 잔차 활성화(residual activations)를 각 전문가(expert)에 대한 점수(scores)로 매핑하는 표준 선형 라우터 투영(linear router projection)으로 구성됩니다.” 각 토큰(token)은 해당 전문가(expert)에게 전송되고, 우리는 해당 전문가(expert)에게 라우팅(routed)된 토큰(token) 배치(batch)에 대해 각 전문가(expert)의 순방향 전달(forward pass)을 계산합니다. 각 전문가(expert)의 출력을 집계하기 위해, 우리는 모든 전문가(experts)의 출력에 대한 가중 평균(weighted average)을 취합니다. 이 정확한 프로세스는 아래 설명된 대로 GPT-oss 모델에 의해 사용됩니다.

“두 모델 모두에 대해, 우리는 라우터(router)에 의해 주어진 각 토큰(token)에 대해 상위 4명의 전문가(experts)를 선택하고, 선택된 전문가(experts)에 대해서만 라우터 투영(router projection)의 소프트맥스(softmax)에 의해 각 전문가(expert)의 출력을 가중치(weight)를 부여합니다.” - 출처 [2]

**활성 파라미터(Active parameters).** 각 토큰(token)에 대해 전문가(experts)의 부분 집합을 선택하기 때문에, 모델 파라미터(model’s parameters)의 일부만 사용됩니다. 이처럼 순방향 전달(forward pass)에서 실제로 사용되는 파라미터(parameters)를 활성 파라미터(active parameters)라고 합니다. GPT-oss의 경우, 20b 및 120b 모델은 각 전문가 혼합(MoE) 레이어(layer) 내에 수많은 전문가(experts)를 가집니다. 그러나 이 전문가(experts) 중 4개만 각 토큰(token)에 대해 활성(active) 상태이므로, 모델은 각각 36억 및 51억 개의 활성 파라미터(active parameters)를 가집니다. 이 모델들의 파라미터(parameter) 수에 대한 더 자세한 분석은 성능 평가에 중요한 요소입니다.

**GPT-oss 파라미터 분포(Parameter Distribution)**
| 모델       | 총 파라미터 수 | 활성 파라미터 수 |
|------------|----------------|------------------|
| GPT-oss-20b | 320억          | 36억             |
| GPT-oss-120b| 1280억         | 51억             |

(출처 [2]) 다른 주목할 만한 전문가 혼합(MoE)과 비교할 때, GPT-oss 모델은 상당히 희소하며, 이는 효율적인 운영을 가능하게 합니다. 예를 들어, 1090억 파라미터(parameter) Llama-4 모델은 170억 개의 활성 파라미터(active parameters)를 가집니다. GPT-oss의 이러한 높은 희소성 수준은 최고의 오픈 소스 거대 언어 모델(LLM)들 사이에서 일반적입니다. DeepSeek-R1 [10]은 6710억 개의 총 파라미터(total parameters)와 370억 개의 활성 파라미터(active parameters)를 가집니다. Qwen-3 [11] 전문가 혼합(MoE) 모델은 300억 개의 총 파라미터(total parameters)와 30억 개의 활성 파라미터(active parameters) 또는 2350억 개의 총 파라미터(total parameters)와 220억 개의 활성 파라미터(active parameters)를 가집니다. 이러한 다양한 희소성 수준은 모델의 설계 목표와 컴퓨팅 제약에 따라 최적의 균형을 찾는 과정의 결과입니다.

**부하 분산(Load balancing) 및 보조 손실(auxiliary losses).** 전문가 혼합(MoE)을 표준 밀집 모델(dense model)과 유사하게 훈련하면 몇 가지 문제가 발생할 가능성이 높으며, 이를 해결해야 합니다. 첫째, 모델은 모든 토큰(token)을 단일 전문가(expert)에게 라우팅(route)하는 것을 빠르게 학습하여, 효율성을 저해할 수 있습니다. 이는 "라우팅 붕괴(routing collapse)"로 알려진 현상으로, 모든 토큰이 특정 전문가에게만 몰리면서 MoE의 이점을 상실하게 됩니다. 또한, 전문가 혼합(MoE)은 훈련 중에 수치적 불안정성(numerical instabilities)을 경험할 가능성이 더 높으며, 이는 모델의 학습에 악영향을 줍니다.

**전문가 혼합(MoE) 사전 훈련(pretraining) 중 손실 발산(Divergence in loss) (출처)**

이러한 문제를 피하기 위해 대부분의 전문가 혼합(MoE)은 훈련 중에 부하 분산 손실(load-balancing loss) [9]을 적용합니다. 이는 적절한 라우팅(routing) 동작을 장려하는 추가 손실 항(loss term)을 다음 토큰 예측 손실(next-token prediction loss)에 추가하여 거대 언어 모델(LLM)의 기본 훈련 목표를 수정합니다. 더 구체적으로, 이 손실은 전문가 혼합(MoE)이 라우터(router)의 모든 전문가(experts)에게 동일한 확률을 할당할 때 최소화됩니다. 즉, 각 전문가(expert)에게 동일한 수의 토큰(token)을 보내어 자원 활용의 균형을 맞추는 것을 목표로 합니다.

(출처 [9]) 부하 분산 손실(load balancing loss) 외에도, 많은 전문가 혼합(MoE)은 수치적 불안정성(numerical instability)을 완화하기 위한 라우터-z 손실(router-z loss) [12]을 사용합니다. 라우터-z 손실(router z-loss)은 전문가 혼합(MoE)의 라우터(router)가 출력하는 로짓(logits)의 크기를 제한합니다. 이러한 로짓(logits)은 가능한 전문가(experts) 집합에 대한 확률 분포(probability distribution)를 도출하기 위해 (지수) 소프트맥스(softmax) 함수로 전달되기 때문에 특히 수치적 불안정성(numerical instability)에 취약합니다. 큰 라우터 로짓(router logits)은 전문가 혼합(MoE)에 특정한 수치적 불안정성(numerical instability)의 주요 원인입니다 (즉, 표준 거대 언어 모델(LLM)에는 라우터(router)가 없기 때문입니다).

(출처 [12]) 전문가 혼합(MoE)을 훈련할 때, 우리는 일반적으로 각 전문가(expert)에 대한 고정된 용량 계수(capacity factor)도 설정하여, 과부하를 방지합니다. 이는 한 번에 전문가(expert)에게 라우팅(routed)될 수 있는 최대 토큰(token) 수를 정의합니다. 이 용량 계수(capacity factor)를 초과하는 모든 토큰(token)은 단순히 삭제되지만, 모델의 전체 성능에는 미미한 영향을 미칩니다. 이 용량 계수(capacity factor)를 채택함으로써, 우리는 토큰(token)의 균일성을 강제합니다. 또한