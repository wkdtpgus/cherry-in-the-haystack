(출처 [18, 20, 21]) 최근 OpenAI는 5년 전 GPT-2 [13] 이후 처음으로 일반에 공개하는 대규모 언어 모델(LLM)인 GPT-oss [1, 2]를 선보였습니다. GPT-2 출시 이후부터 GPT-oss가 등장하기까지의 기간 동안, 거대 언어 모델 연구 분야는 끊임없이 변화하고 발전해 왔습니다. 이 시기 동안 OpenAI는 거대 언어 모델 연구의 여러 중요한 혁신을 주도했지만, 그들의 연구 결과는 대부분 내부적으로만 활용되었습니다. 따라서 GPT-oss의 공개는 OpenAI의 거대 언어 모델 연구 현황을 엿볼 수 있는 드문 기회를 제공합니다. 본 개요에서는 이 특별한 기회를 활용하여 다음과 같은 내용을 상세히 다룰 것입니다. OpenAI가 공개한 보고서에서 GPT-oss에 대해 언급된 모든 기술적 상세 사항들을 심도 있게 분석하고, 이러한 각 세부 기술이 어떻게 작동하는지 기초부터 설명할 것입니다. 이 개요는 다소 방대할 수 있지만, 거대 언어 모델 연구의 여러 관련 주제들을 깊이 있게 탐구함으로써 독자 여러분은 GPT-oss의 작동 원리에 대한 깊이 있는 이해를 얻고, 나아가 OpenAI의 거대 언어 모델 연구 역량에 대한 명확한 통찰을 얻을 수 있을 것입니다. 딥 (러닝) 포커스(Deep (Learning) Focus)를 통해 인공지능(AI) 연구의 최신 동향을 파악하는 50,000명의 전문가들과 함께하세요. 지금 구독하세요!

### GPT-oss 개요

“이 모델들은 OpenAI의 가장 진보된 내부 모델인 o3 및 기타 최첨단 시스템에서 얻은 강화 학습 및 기술을 통합하여 훈련되었습니다.” - 출처 [1]

GPT-oss 공개에는 두 가지 상이한 모델, 즉 GPT-oss-20b와 GPT-oss-120b가 포함되어 있으며, 이들은 모두 관대한 Apache-2.0 라이선스 아래 배포되었습니다. 이 모델들은 전문가 혼합(Mixture-of-Experts, MoE) 아키텍처에 기반한 추론 모델로, 주로 텍스트 데이터와 영어 데이터로 학습되었습니다. 전문가 혼합(MoE) 구조와 양자화 인식 훈련(quantization-aware training) 기법의 도입 덕분에, 이 모델들은 계산 자원 및 메모리 사용 측면에서 매우 효율적입니다. 각 모델인 20b와 120b는 각각 50억 개와 35억 개의 활성 파라미터(active parameters)를 보유하고 있습니다. MXFP4 (~4비트) 정밀도를 활용하면, 더 큰 모델도 단일 80Gb GPU에서 구동될 수 있으며, GPT-oss-20b는 약 16Gb의 메모리만으로도 충분히 호스팅 가능합니다. 이 모델들은 사고의 사슬(chain of thought, CoT) 추론 능력과 안전성을 극대화하기 위해 광범위한 후처리 훈련(post-trained) 과정을 거쳤습니다.

**에이전트(agent) 기능의 강화.** 두 GPT-oss 모델은 모두 에이전트 기반 작업 흐름(agentic workflows)에 최적화되어 있으며, 이는 (상대적으로) 긴 131k 토큰의 컨텍스트 윈도우(context window)와 강력한 도구 사용(tool use), 추론, 그리고 지시 따르기(instruction-following) 능력을 통해 구현됩니다. 함수 호출(function calling), 도구 활용, 복잡한 추론, 구조화된 출력(structured outputs) 등 에이전트 작업 흐름의 다양한 패턴을 더욱 원활하게 처리하기 위해, OpenAI는 GPT-oss의 훈련 및 상호 작용에 사용되는 새로운 '하모니 프롬프트 형식(Harmony prompt format)'을 도입했습니다. 이는 유연하면서도 계층적인 채팅 템플릿(chat template)으로, 다양한 거대 언어 모델 상호 작용 시나리오를 효과적으로 담아낼 수 있습니다. 또한, GPT-oss 모델은 시스템 메시지(system message)에 추론 노력 수준(예: 낮음, 중간, 높음)을 명시적으로 지정함으로써, 모델의 추론 강도를 조절할 수 있는 기능을 제공합니다.

(출처 [1]) **내부 성능 평가.** OpenAI가 공개한 평가 결과에 따르면, GPT-oss-120b는 o4-mini와 유사한 성능을 보이며, GPT-oss-20b는 o3-mini와 견줄만한 성능을 나타냅니다. (자세한 내용은 위 참조). 특히 OpenAI는 출시 기간 동안 새로 공개된 HealthBench 벤치마크를 기반으로, 이 모델들의 건강 관련 작업 처리 능력이 매우 뛰어나다는 점을 강조했습니다. (관련 내용은 아래 참조). 그러나 GPT-oss 모델은 여전히 완전한 o3 모델의 전체적인 성능에는 미치지 못하는 것으로 평가됩니다.

(출처 [1]) 예상했던 대로, OpenAI는 GPT-oss 모델이 추론 노력(reasoning effort)에 따라 일반적인 추론 시간 스케일링 법칙(inference-time scaling laws)을 따른다는 점을 강조합니다. 이는 모델이 점진적으로 더 긴 추론 경로(reasoning traces)를 생성할수록 성능이 향상되며, 이 과정에서 더 많은 연산 자원을 소비하게 됨을 의미합니다. (자세한 내용은 아래 참조). 이러한 특성은 모델의 유연성을 높이는 동시에, 사용자가 필요한 성능 수준에 맞춰 컴퓨팅 자원을 조절할 수 있도록 합니다.

(출처 [1]) **대중의 반응.** GPT-oss 모델이 공개 대규모 언어 모델 커뮤니티에 공개된 후, 이에 대한 반응은 엇갈렸습니다. 예를 들어, 일부 사용자들은 초기 모델에서 높은 환각률(hallucination rate)을 지적했지만, 다른 사용자들은 모델 설정과 관련된 초기 문제들이 해결된 이후에는 모델이 실제로 상당히 우수하다고 평가했습니다. GPT-oss 모델에 대한 다른 일반적인 비판으로는 프롬프트의 과도한 거부(over-refusal of prompts), 모델 양자화(quantization) 설정을 제대로 하는 것의 어려움, 그리고 하모니 프롬프트 형식(Harmony prompt format)이 지나치게 복잡하거나 사용하기 어렵다는 점 등이 있었습니다. 간단히 말해, 초기의 부정적인 인식은 ollama, llama.cpp, unsloth와 같은 대중적인 도구들의 잔존 문제들이 해결되면서 점차 개선되었습니다. GPT-oss의 실제 평가는 온라인상의 양극화되고 자극적인 반응의 중간 어딘가에 있습니다. 이 모델들이 (분명히) 역대 최고의 모델은 아니지만, 세계 최고 수준의 거대 언어 모델 연구소 중 한 곳에서 공개한 오픈 웨이트(open weights) 모델이라는 점은 매우 중요합니다. AI2, Cohere, Meta와 같은 일부를 제외하고는, 대다수의 미국 상위 거대 언어 모델 연구소들이 활발하게 오픈 웨이트 모델을 출시하지 않는다는 점을 고려할 때, 이 모델들을 시도해보고 그 작동 방식을 깊이 이해하지 못한다면 아쉬운 일일 것입니다. 그러므로 이제 OpenAI가 GPT-oss에 대해 제공한 관련 기술적 세부 사항들을 면밀히 검토해 보겠습니다.

### 모델 아키텍처(Model Architecture)

“GPT-oss 모델은 GPT-2 및 GPT-3 아키텍처를 기반으로 구축된 자기회귀(autoregressive) 전문가 혼합(Mixture-of-Experts, MoE) 트랜스포머(transformers)입니다.” - 출처 [1]

먼저 GPT-oss 모델의 전반적인 아키텍처를 살펴보겠습니다. 이 논의는 트랜스포머 아키텍처(transformer architecture) 2에 대한 기본적인 이해를 전제로 합니다. 여기서는 GPT-oss 아키텍처를 구성하는 각 고유한 요소들을 처음부터 상세히 설명할 것입니다. 이 주제에 대한 추가 자료 및 다른 공개 모델과의 비교는 Sebastian Raschka의 뛰어난 개요를 참고하시기 바랍니다.

**AI의 미래**
**GPT-2에서 gpt-oss까지: 아키텍처 발전 분석**
OpenAI는 이번 주에 새로운 오픈 웨이트(open-weight) 거대 언어 모델(LLM)인 gpt-oss-120b와 gpt-oss-20b를 출시했습니다. 이는 2019년 GPT-2 이후 첫 오픈 웨이트 모델입니다. 그리고 네, 몇 가지 영리한 최적화 덕분에 로컬에서 실행할 수 있습니다 (하지만 이에 대해서는 나중에 더 자세히… 더 읽기).
3개월 전 · 좋아요 169개 · 댓글 17개 · Sebastian Raschka, PhD

#### 트랜스포머 구조(Transformer Structure)

**디코더 전용 트랜스포머 아키텍처(Decoder-only transformer architecture)**
위 그림은 표준적인 디코더 전용 트랜스포머 아키텍처를 보여줍니다. 이 구조는 현대적인 GPT 스타일의 대규모 언어 모델에서 거의 보편적으로 채택되고 있습니다. 생성형 인공지능 모델의 핵심인 다음 토큰 예측(next token prediction)에 특화되어 있기 때문입니다.

**임베딩 차원(Embedding dimension).** 모델의 입력은 텍스트(프롬프트)를 토큰화(tokenizing)하고 임베딩(embedding)하여 생성된 토큰 벡터(token vectors)의 시퀀스입니다. GPT-oss 모델의 경우, 이 벡터들은 2,880의 고정된 차원(dimension)을 가지며, 이 동일한 임베딩 차원(embedding dimension)은 거대 언어 모델의 모든 레이어(layer)를 통해 일관되게 유지됩니다. 이는 모델의 정보 처리 용량을 결정하는 중요한 요소입니다.

**블록 구조(Block structure).** 디코더 전용 아키텍처는 반복되는 디코더 블록(decoder blocks)으로 구성됩니다. GPT-oss-20b 모델은 24개의 블록을, GPT-oss-120b 모델은 36개의 블록을 포함합니다. 각 디코더 블록은 핵심 구성 요소를 공유합니다: 정규화(normalization), 마스크된 다중 헤드 자체 어텐션(masked multi-headed self-attention), 피드포워드 변환(feed-forward transformation), 그리고 잔차 연결(residual connections). GPT-oss 모델은 현재 거대 언어 모델 아키텍처에서 널리 사용되는 사전 정규화 구조(pre-normalization structure) 3를 채택합니다. 이는 디코더 블록의 정규화 레이어(normalization layers)가 어텐션(attention) 및 피드포워드 레이어(feed-forward layers) 이전에 배치되어, 다음과 같은 흐름을 형성합니다:

**디코더 블록(Decoder Block)**