안녕하세요, AI 기술에 관심 있는 모든 분! 이번 LLM Watch 업데이트에서는 최신 인공지능 연구 동향을 심층적으로 다룹니다. 대규모 언어 모델(LLM)을 위한 강화 학습(reinforcement learning)의 진화, 신경망(neural) AI와 심볼릭(symbolic) AI의 융합 시도, 그리고 LLM 기반 에이전트(agent)의 효율적인 개발 전략까지, 흥미로운 주제들이 가득합니다. 급변하는 AI 분야의 최신 정보를 계속 받아보시려면 구독 버튼을 눌러주세요.

가장 효율적으로 AI 전문가로 성장하는 길은 무엇일까요? 바로 직접 구현해보는 것입니다! 이론에만 머무르지 않고, 실제 프로젝트를 통해 탄탄한 실력을 쌓는 것이 중요합니다. Towards AI에서 제공하는 산업 중심의 '초급부터 고급 LLM 개발자까지(약 90개 강의)' 코스를 통해 이러한 실습 경험을 쌓을 수 있습니다. 실제 문제 해결에 대한 깊은 갈증을 느꼈던 전직 연구원들과 개발자들이 직접 커리큘럼을 설계했으며, 이는 여러분이 실무에서 바로 적용할 수 있는 지식과 기술을 습득하도록 돕습니다.

*   **실전 애플리케이션(application) 구축**: RAG(검색 증강 생성), 미세 조정(fine-tuning), 에이전트(agent) 개발 등 실제 서비스에 필요한 핵심 기술을 학습합니다.
*   **멘토링**: 디스코드(Discord) 채널을 통한 전문가 지원으로 학습 과정의 궁금증을 즉시 해결하고 개인 맞춤형 피드백을 받을 수 있습니다.
*   **선수 과목**: 기본 파이썬(Python) 지식만 있다면 누구나 시작할 수 있습니다.
*   **결과**: 수료 후 인증된 제품을 직접 출시하며 포트폴리오를 강화합니다.
*   **가치 보장**: 30일 환불 보장으로 안심하고 학습에 전념할 수 있습니다.

**전문성 향상**
**전문가 조언**: 이 코스와 LLM Watch는 회사 학습 및 개발 예산으로 지원받을 수 있는 좋은 기회가 될 수 있습니다.

### LLM을 위한 강화 학습(RL)의 새로운 지평

**RL 컴퓨팅(compute)의 확장성 예측( [paper](https://arxiv.org/abs/2405.14590) )**: 메타(Meta)와 여러 연구 기관의 협력 연구는 대규모 언어 모델(LLM)에 적용되는 강화 학습(RL) 미세 조정(fine-tuning)의 확장성을 40만 GPU-시간이라는 방대한 데이터를 활용하여 최초로 심층 분석했습니다. 이 연구는 컴퓨팅 자원이 증가함에 따라 성능이 어떻게 변화하는지 시그모이드(sigmoidal) 곡선 형태로 나타난다는 것을 밝혀냈습니다. 또한, 손실 집계(loss aggregation)나 정규화(normalization)와 같은 다양한 설계 요소들이 주로 컴퓨팅 효율성에 영향을 미치며, 최종 성능에는 큰 영향을 주지 않는다는 중요한 사실을 발견했습니다. 이러한 통찰력을 바탕으로, 연구팀은 소규모 테스트의 학습 곡선을 바탕으로 최종적인 성능을 예측할 수 있는 표준화된 방법론인 "**ScaleRL**"을 제안합니다. ScaleRL을 활용하여 10만 GPU-시간 규모의 실행 결과를 성공적으로 예측했으며, 이는 강화 학습(RL) 훈련의 예측 가능성을 사전 훈련(pre-training) 수준으로 끌어올리는 데 크게 기여합니다. 이는 연구자들이 자원 배분을 더욱 효율적으로 계획하고, 대규모 실험 전에 성공 가능성을 미리 가늠할 수 있도록 돕는 중요한 발전입니다.

**장기 추론을 위한 “마르코프적 사고(Markovian Thinker)”( [paper](https://arxiv.org/abs/2405.14590) )**: LLM 에이전트(agent)의 긴 연쇄적 사고(chain-of-thought) 추론은 일반적으로 프롬프트(prompt) 길이가 점진적으로 증가하여 강화 학습(RL) 정책에 막대한 비용을 초래합니다. Aghajohari 외 연구진은 '**마르코프적 사고(Markovian Thinking)**' 개념을 도입하여 이 문제를 해결합니다. 이들은 강화 학습(RL)의 "사고 환경(thinking environment)"을 재설계하여 모델이 언제나 **일정한 크기의 상태(state)**만을 인지하도록 환경을 재구성합니다. Delethink라는 이 설정에서, 복잡한 추론 과정은 여러 세그먼트(segment)로 나뉘게 됩니다. 모델은 각 청크(chunk)의 끝에서 핵심적인 상태 요약(state summary)을 생성한 후, 이전 컨텍스트(context)를 초기화하고 새로 생성된 요약으로 다시 추론을 시작합니다. 이 혁신적인 접근 방식 덕분에 선형 시간(linear-time) 복잡성과 상수 메모리(constant-memory) 사용량으로 추론을 확장할 수 있습니다. 예를 들어, Delethink로 훈련된 15억(1.5B) 모델은 8천 토큰(8k-token) 청크(chunk)를 활용하여 2만 4천 토큰(24k tokens)에 달하는 추론을 수행할 수 있었으며, 이는 전체 2만 4천 토큰(24k-token) 컨텍스트(context)로 훈련된 기준선(baseline) 모델과 동등하거나 그 이상의 성능을 보였습니다. 더욱 놀라운 점은, 기준선 모델이 정체되는 9만 6천 토큰(96k tokens)과 같은 더 긴 추론 작업에서도 지속적으로 성능이 향상되었다는 것입니다. 컴퓨팅 비용 또한 기준선(27 H100-개월) 대비 약 4분의 1 수준(7 H100-개월)으로 추정됩니다. 이는 모델 자체의 개선뿐만 아니라, 환경을 재설계하는 것이 효율적이고 확장 가능한 장기 추론을 가능하게 하는 핵심 요소임을 시사합니다. 이러한 접근 방식은 LLM이 방대한 정보를 처리하고 장기적인 계획을 세울 때 발생하는 메모리 및 계산량 문제를 효과적으로 해결할 수 있는 새로운 길을 열었습니다.

### 강화 학습(RL) 미세 조정(Fine-Tuning)이 추론 능력을 해제하다

**강화 학습(RL)이 LLM 추론에 도움이 되는 이유( [paper](https://arxiv.org/abs/2405.14590) )**: Tsilivis 외 연구진은 사전 훈련(pre-training) 이후 강화 학습(RL) 미세 조정(fine-tuning)이 LLM의 추론 능력을 어떻게 향상시키는지에 대한 근본적인 질문을 탐구합니다. 그들은 사전 학습 이후 강화 학습(RL)을 적용하는 것이, 단순히 다음 토큰(next-token)을 예측하는 방식보다 훨씬 적은 정보로도 복잡한 연쇄적 사고(chain-of-thought)를 요구하는 문제를 해결할 수 있음을 입증하는 이론적 틀을 제시합니다. 본질적으로 강화 학습(RL)은 다단계 추론 체인(reasoning chain)의 최종 결과에 보상을 부여함으로써, 모델이 더 긴 테스트 시간 컴퓨팅(test-time computation), 즉 더 복잡하고 긴 답변을 생성하는 과정을 통해 작업을 학습하도록 유도합니다. 이는 모델이 단기적인 다음 토큰 예측에만 집중하는 것이 아니라, 전체적인 목표 달성을 위한 장기적인 전략을 스스로 학습하게 만듭니다. 예를 들어, 대부분 짧은 예시로 구성된 장난감 패리티(parity) 작업에서, 강화 학습(RL)으로 미세 조정된 트랜스포머(transformer)는 긴 추론 과정을 통해 올바르게 일반화(generalize)하는 반면, 순수하게 다음 토큰(next-token) 예측으로만 훈련된 모델은 훨씬 더 많은 데이터가 필요했습니다. 연구팀은 실제 모델에서도 이러한 효과를 확인했습니다. 전문가 혼합(mixture-of-experts) 수학 문제에 LLaMA 모델을 강화 학습(RL) 미세 조정(fine-tuning)한 결과, 더 긴 추론 단계를 활용하여 더 나은 일반화(generalization) 성능을 달성했습니다. 이는 강화 학습(RL)이 LLM의 '새로운 능력(emergent abilities)'을 이끌어내는 데 중요한 역할을 하며, 단순히 언어를 유창하게 구사하는 것을 넘어 복잡한 문제 해결 능력을 부여할 수 있음을 보여줍니다.

**“기반(Base)” 모델 vs “사고(Thinking)” 모델 - 방법을 아는 것 vs 시기를 아는 것( [paper](https://arxiv.org/abs/2405.14590) )**: 연쇄적 사고(chain-of-thought)를 수행하는 "사고(thinking)" 모델이 본질적으로 더 뛰어난 능력을 가졌을까요, 아니면 단순히 기반 모델(base model)의 잠재된 기술을 더 잘 활용하는 것일까요? Venhoff 외 연구진은 주로 후자임을 밝혀냈습니다. 놀랍게도, 이 모든 성과는 **모델의 가중치(weight)를 전혀 갱신하지 않고** 달성되었습니다. 고정된 기반 모델(base model)의 활성화(activation)에 약 12%의 위치에서 작은 조향 벡터(steering vector)를 주입하는 것만으로도, 모델의 잠재된 추론 능력(reasoning abilities)을 촉발할 수 있었으며, 완전한 연쇄적 사고(chain-of-thought) 훈련 모델의 성능 격차 중 약 91%를 회복할 수 있었습니다. 이는 대규모로 사전 훈련(pre-trained)된 LLM이 이미 추론하는 "**방법**"을 내재하고 있으며, 특정 목적의 미세 조정(fine-tuning)은 실제로 이러한 기술을 "**언제**" 배포할지 학습시키는 역할을 한다는 것을 시사합니다. 다시 말해, 사전 훈련(pre-training)은 모델에 추론 메커니즘(reasoning mechanisms)을 부여하고, 사후 훈련("사고(thinking)" 모델 훈련)은 주로 적절한 시기에 이러한 메커니즘을 효율적으로 사용하는 방법을 가르칩니다. 이러한 통찰력은 고급 추론 LLM에 대한 우리의 관점을 재구성합니다. 이는 모델의 내부 작동 방식을 이해하고, 미세 조정 없이도 강력한 추론 능력을 이끌어낼 수 있는 새로운 가능성을 제시합니다. 예를 들어, 특정 작업을 위해 모델 전체를 재학습시키는 대신, 소량의 제어 신호만으로도 원하는 행동을 유도할 수 있다면, 훨씬 효율적인 모델 개발 및 배포가 가능해질 것입니다.

### LLM 에이전트(Agent) 및 도구 사용을 위한 새로운 패러다임

**순수 모방 대신 초기 경험( [paper](https://arxiv.org/abs/2405.14590) )**: 메타(Meta)의 "초기 경험을 통한 에이전트 학습(Agent Learning via Early Experience)" 연구는 고정된 모방 학습(imitation learning)과 완전한 자율 강화 학습(reinforcement learning) 사이의 균형 잡힌 접근 방식을 제시합니다. 기존의 모방 학습은 전문가가 미리 정의한 좁은 범위의 데모(demos)에만 의존하기 때문에, 환경의 다양성에 대한 노출이 부족하여 새로운 상황에 대한 일반화(generalization) 능력이 떨어지는 한계가 있었습니다. 이를 극복하기 위해 연구팀은 에이전트(agent)가 어떠한 외부 보상 없이 스스로 환경과 상호작용하며 데이터를 축적하도록 합니다. 이를 "**초기 경험(early experience)**"이라고 부릅니다. 이 초기 경험 데이터는 두 가지 주요 방식으로 활용됩니다. (1) **암묵적 세계 모델(Implicit world models)**: 에이전트(agent)는 수집된 상태(state) 전환을 통해 환경의 역학을 이해하고 정책(policy)을 학습합니다. (2) **자기 성찰(Self-reflection)**: 에이전트(agent)는 최적이 아닌 행동(suboptimal actions)으로부터 스스로 배우고 추론 능력을 개선합니다. 이 전략은 8가지 다양한 환경에서 성능과 도메인 외 일반화(out-of-domain generalization) 능력을 향상시키며, 모방(imitation) 기반 에이전트와 완전 자율 에이전트 사이의 간극을 효과적으로 메워줍니다. 특히, 최종 보상이 사용 가능할 때조차 초기 경험(early experience)은 후속 강화 학습(RL)을 더욱 효과적으로 만드는 강력한 기반을 제공하여, 에이전트가 복잡한 환경에서 보다 견고하고 효율적으로 학습할 수 있도록 돕습니다.

**에이전트 강화 학습(Agentic RL)을 위한 모범 사례( [paper](https://arxiv.org/abs/2405.14590) )**: Yu 외 연구진의 "에이전트 추론에서의 강화 학습(RL) 이해(Demystifying RL in Agentic Reasoning)" 논문은 도구 사용 LLM 에이전트(agent)를 훈련할 때 강화 학습(RL)을 최대한 활용하는 체계적인 방법을 평가합니다. 그들은 몇 가지 간단하지만 매우 강력한 관행을 제시합니다. (i) 파편화된 코드 조각(snippet) 대신, 실제 환경에서 발생하는 종단 간(end-to-end) 도구 활용 과정을 담은 완전한 궤적(trajectory) 데이터를 활용해야 합니다. 이는 지도 미세 조정(supervised fine-tuning)을 위한 훨씬 견고한 초기화(initialization)를 제공하며, 다양하고 모델 인지적인 경험 데이터(experience data)를 보장합니다. 높은 다양성을 가진 데이터셋(dataset)은 에이전트(agent)의 과적합(overfitting)을 방지하고 새로운 탐색(exploration)을 유지하는 데 필수적입니다. (ii) 탐색 친화적인 기술을 적극적으로 사용해야 합니다. 예를 들어, 더 높은 클리핑 임계값(clipping thresholds)을 허용하고, 더 긴 시간 범위(longer horizons)에 걸쳐 보상을 설계하며, 충분한 정책 엔트로피(policy entropy)를 유지하여 에이전트(agent)가 새로운 해결책을 시도하도록 장려함으로써 강화 학습(RL) 훈련의 효율성을 크게 높일 수 있습니다. (iii) 매우 빈번하거나 지나치게 장황한 자기 대화(self-dialogues)보다는, 더 적고 목표 지향적인 도구 호출을 사용하는 신중한 추론 전략(deliberative reasoning strategy)을 선호해야 합니다. 즉, 에이전트(agent)에게 행동하기 전에 조금 더 깊이 생각하도록 가르치는 것이 도구 활용 효율성과 최종 정확도를 향상시키는 핵심입니다. 이러한 팁을 적용하여 연구팀은 더 작은 LLM으로도 어려운 벤치마크(benchmark)에서 최첨단 결과(state-of-the-art results)를 달성했으며, 심지어 40억(4B) 모델이 에이전트 추론 작업에서 320억(32B) 모델을 능가하도록 만들었습니다. 또한, 다른 연구자들이 이러한 성과를 재현할 수 있도록 고품질의 지도 및 강화 학습(RL) 에이전트 궤적(trajectory) 데이터셋(dataset)을 공개했습니다.

**토큰 사전 확률(Token Priors)을 통한 훈련 없는 강화 학습(RL)( [paper](https://arxiv.org/abs/2405.14590) )**: 강화 학습(reinforcement learning)은 일반적으로 모델 가중치(model weights)를 업데이트(update)하는 과정을 수반하지만, Cai 외 연구진은 미세 조정(fine-tuning) 없이도 정책 개선(policy improvements)을 달성할 수 있음을 보여줍니다. 그들이 제안하는 "**훈련 없는 그룹 RPO(Training-Free Group RPO)**" 방식은 기존의 LLM을 변경하지 않고 고정된 상태로 유지하면서, 대신 축적된 경험을 바탕으로 출력 토큰(token)의 분포를 실시간으로 조절합니다. 배포(deployment) 중에 에이전트(agent)는 여러 롤아웃(rollout)을 생성합니다. 각 롤아웃(rollout) 그룹 내에서, 이 방법은 토큰(token)에 대한 "그룹 상대 의미론적 이점(group relative semantic advantage)"(어떤 토큰이 더 나은 결과로 이어졌는지 평가하는 방식)을 계산하고, 이를 토큰 수준의 사전 확률(token-level prior)로 증류하여 모델의 다음 결정을 편향되게 만듭니다. 이 과정을 몇 번의 에포크(epoch) 동안 반복함으로써(참조용으로 소수의 실제 예시만 사용), LLM은 경사 업데이트(gradient updates) 없이도 출력을 조종하는 "경험적 지식(experiential knowledge)"을 습득하게 됩니다. 수학 추론 및 웹 검색과 같은 작업에서 이 경량 루프(lightweight loop)를 최첨단 에이전트(DeepSeek)에 추가하자 도메인 외 성능(out-of-domain performance)이 크게 향상되었습니다. 실제로, 단 몇십 개의 예시만으로도 훈련 없는 접근 방식은 전통적인 강화 학습(RL) 훈련 단계를 피하므로, 비용과 시간의 극히 일부만으로 완전히 미세 조정된 더 작은 LLM을 능가하는 놀라운 결과를 보여주었습니다. 이는 LLM을 실제 환경에 더욱 빠르고 유연하게 적용할 수 있는 길을 열어줍니다.

### 텐서 로직(Tensor Logic): 신경망(Neural) 및 심볼릭(Symbolic) AI 통합

페드로 도밍고스(Pedro Domingos) 교수는 신경망(neural) 기반의 방식과 심볼릭(symbolic) 기반의 방식을 근본적으로 결합하는 "AI의 공통 언어"를 목표로 하는 혁신적인 프로그래밍 언어 "**텐서 로직(Tensor Logic)**"( [paper](https://arxiv.org/abs/2405.14590) )을 발표했습니다. 이러한 시도의 배경에는 기존 AI 개발 도구들의 한계에 대한 인식이 자리 잡고 있습니다. 파이토치(PyTorch)나 텐서플로우(TensorFlow)와 같은 현대적인 프레임워크(framework)는 GPU를 활용한 자동 미분(auto-differentiation) 기능을 제공하지만, 파이썬(Python)이라는 언어에 종속되어 있어 논리적 추론이나 지식 표현에 대한 내장 지원이 부족합니다. 반대로 프롤로그(Prolog)나 리스프(Lisp)와 같은 고전적인 AI 언어들은 기호 처리와 논리적 추론에 뛰어나지만, 대규모 데이터로부터 학습하거나 확장하는 데는 한계가 있습니다. 텐서 로직(Tensor Logic)은 텐서 방정식(tensor equation)을 단일한 **중심 요소(core construct)**로 설정하여 이 난제를 해결합니다. 도밍고스 교수는 논리적 추론 규칙(logical inference rules)이 수학적으로 텐서 인덱스 합산(tensor index summation)(아인슈타인 합산(Einstein summation))과 유사하다는 점에 착안했습니다. AI의 다른 모든 요소들이 이 형태로 환원될 수 있다고 본 것입니다. 이 논문은 트랜스포머(transformer)와 신경망(neural nets)부터 형식 논리 증명(formal logic proofs), 커널 메서드(kernel methods), 그래픽 모델(graphical models)에 이르기까지, 주요 인공지능 패러다임(paradigm)들이 텐서 로직(Tensor Logic)의 통합 프레임워크(unified framework) 내에서 어떻게 우아하게 구현될 수 있는지를 보여줍니다. 결정적으로, 이는 신경망(neural)의 확장성과 학습 능력, 그리고 심볼릭 추론(symbolic reasoning)의 엄격함을 결합하는 "**임베딩 공간(embedding space)에서의 건전한 추론(sound reasoning)**"과 같은 새로운 가능성을 열어줍니다. 예를 들어, 텐서 로직(Tensor Logic)은 모델이 추론 과정을 명확하게 설명할 수 있도록 돕는 설명 가능한 AI(explainable AI) 시스템을 구축하거나, 복잡한 규칙 기반 시스템과 데이터 기반 학습 시스템을 유연하게 결합하는 하이브리드(hybrid) AI 시스템 개발에 활용될 수 있습니다. 따라서 텐서 로직(Tensor Logic)은 앞으로 AI의 기반이 되어 기호의 투명성과 텐서(tensor)의 힘을 제공함으로써, 오랫동안 지속되어 온 신경-심볼릭(neuro-symbolic) 분야의 분열을 해소하고 AI의 더 넓은 채택을 촉진할 잠재력을 가집니다.

구독하기 ❤️
저희 LLM Watch를 함께해주셔서 진심으로 감사합니다! 이 기사가 유익하셨다면, '좋아요'를 눌러주시고 동료들과 공유해주세요. 여러분의 소중한 의견을 댓글로 남겨주세요. 새로운 소식들을 놓치지 않고 받아보시고, 저희 콘텐츠 제작에 힘을 실어주시려면 구독을 부탁드립니다. AI 기술의 최전선에서 여러분과 함께하겠습니다!

구독하기