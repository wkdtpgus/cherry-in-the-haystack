환영합니다, 독자 여러분! 이번 주 LLM Watch에서는 거대 언어 모델의 강화 학습(reinforcement learning) 기법 심화 및 확장, 신경망(neural) AI와 기호 논리(symbolic) AI의 융합, 그리고 LLM 기반 행위자(agent)를 위한 새로운 전략 및 접근 방식 등 다채로운 주제들을 다룹니다. 최신 소식을 놓치지 않도록 구독하는 것을 잊지 마세요.

### AI 엔지니어링 역량 강화 가이드
인공지능 분야의 숙련된 엔지니어로 성장하는 가장 효과적인 길은 무엇일까요? 이론 학습에만 머무르지 않고, 실제 시스템을 직접 구축하고 개선하는 과정에 몰입하는 것입니다. 이는 단순한 지식 습득을 넘어 문제 해결 능력과 실용적인 구현 기술을 체득하는 데 필수적입니다. 빠르게 변화하는 AI 생태계에서 지속적인 학습과 실전 경험은 그 어떤 자격증보다 값진 자산이 됩니다. 최신 연구 동향을 이해하고, 이를 실제 서비스에 적용해보는 과정을 통해 여러분은 진정한 전문가로 거듭날 수 있습니다.

가장 빠르게 AI 엔지니어가 되는 방법은? 직접 만들어보는 것입니다! Towards AI의 산업 중심 코스인 '초급부터 고급 LLM 개발자까지(약 90개 강의)'를 통해 실습 경험을 쌓으세요. 실제 세상에 영향을 미치기 위해 좌절했던 전직 박사들과 개발자들이 만들었습니다.
*   **프로덕션(production) 수준의 앱 구축**: RAG, 미세 조정(fine-tuning), 에이전트(agent)
*   **지도**: 디스코드(Discord)를 통한 강사 지원
*   **선수 과목**: 기본 파이썬(Python)
*   **결과**: 인증된 제품 출시
*   **가치 보장**: 30일 환불 보장

**기술 향상**
**전문가 팁**: 이 코스와 LLM Watch 모두 회사 학습 및 개발 예산에 해당될 수 있습니다.

### 최신 AI 연구 동향 심층 분석
이번 주에는 언어 모델 기술의 지평을 넓히는 다양한 연구 성과들을 집중적으로 살펴보겠습니다. 특히, 강화 학습(RL)의 확장 가능성과 그 실제 적용 사례, 신경망(neural network)과 기호적(symbolic) 추론의 통합을 시도하는 혁신적인 접근법, 그리고 LLM 기반 행위자(agent)의 성능을 비약적으로 향상시킬 새로운 전략들에 주목할 필요가 있습니다.

### LLM을 위한 강화 학습(RL) 확장

**RL 연산의 예측 가능한 성장([paper](https://arxiv.org/abs/2405.14590))**: 메타(Meta) 연구진과 협력자들의 보고서는 거대 언어 모델(LLM)의 강화 학습(RL) 미세 조정(fine-tuning)이 어떻게 확장되는지에 대한 최초의 대규모 분석(총 40만 GPU-시간)을 제시합니다. 연구진은 S자형 연산-효율 곡선에 데이터를 대입하여, 여러 가지 설계 요소(예: 손실 합산 방식, 정규화 기법)가 주로 계산 자원 사용 효율에 영향을 줄 뿐, 최종적인 모델 성과에는 큰 차이를 만들지 않는다는 점을 밝혀냈습니다. 이러한 발견을 토대로, 그들은 적은 규모의 시뮬레이션 학습 곡선을 외부로 확장하여 최종 성과를 예측하는 최적의 방법론인 “**ScaleRL**”을 제시합니다. ScaleRL을 활용하면 10만 GPU-시간에 달하는 실행 결과를 성공적으로 예측할 수 있었으며, 이는 강화 학습(RL) 훈련의 예측 가능성을 사전 학습(pre-training) 수준으로 끌어올리는 중요한 진전입니다. 이러한 예측 가능성은 연구자들이 자원을 보다 효율적으로 배분하고, 실험 설계의 신뢰도를 높이는 데 결정적인 역할을 합니다.

**장기 추론을 위한 “마르코프적 사고”([paper](https://arxiv.org/abs/2405.14590))**: 장문 사고 과정(chain-of-thought) 추론 방식은 대개 언어 모델 행위자들이 점진적으로 길어지는 질의에 의존하게 만듭니다. 이는 사고의 길이가 늘어날수록 강화 학습(RL) 정책에 막대한 이차 비용을 부과하는 결과를 초래합니다. Aghajohari 연구팀은 **마르코프적 사고(Markovian Thinking)**라는 개념을 도입하여, 강화 학습의 '사고 환경'을 재설계함으로써 모형이 언제나 **일정한 크기의 상황 정보**만을 인지하도록 만들었습니다. 그들의 Delethink 환경에서 추론 과정은 여러 세그먼트(segment)로 나뉘며, 모형은 각 조각의 마지막에 간결한 상태 요약(state summary)을 생성한 후, 컨텍스트를 초기화하고 해당 요약으로 새로운 추론을 시작합니다. 이 방식은 선형 시간(linear-time) 복잡도와 상수 메모리(constant-memory) 사용을 가능하게 하여 확장성을 획기적으로 개선합니다. 15억(1.5B) 매개변수 모델이 Delethink로 훈련되었을 때, 8천 토큰(8k-token) 청크(chunk)를 사용하여 총 2만 4천 토큰(24k tokens)을 추론할 수 있었으며, 이는 전체 2만 4천 토큰 컨텍스트로 훈련된 기준선 모델과 동등하거나 더 나은 성능을 보였습니다. 더 나아가, 9만 6천 토큰(96k tokens)과 같은 훨씬 긴 추론에서도 지속적인 개선을 보여주었지만, 기준선 모델은 성능 정체를 겪었습니다. 계산 비용 또한 기준선 모델의 27 H100-개월에 비해 약 7 H100-개월로 4분의 1 수준에 불과했습니다. 이처럼 모델 자체의 개선뿐만 아니라, 환경의 재설계를 통해 효율적이고 확장 가능한 장기 추론이 실현될 수 있음을 시사합니다.

### 강화 학습(RL) 미세 조정(Fine-Tuning)이 추론 능력을 해제하다

**강화 학습(RL)이 LLM 추론에 기여하는 방식([paper](https://arxiv.org/abs/2405.14590))**: Tsilivis 연구팀은 다음 토큰(next-token) 사전 학습(pre-training) 이후 강화 학습(RL) 미세 조정(fine-tuning)이 추론 능력을 어떻게 향상시키는지에 대한 근본적인 질문을 탐구합니다. 그들은 사전 학습 이후 강화 학습(RL)을 적용하면, 다음 토큰 예측만으로는 훨씬 더 많은 데이터가 필요한 긴 사고 과정(chain-of-thought)을 요구하는 작업을, 훨씬 적은 데이터로 해결할 수 있음을 보여주는 이론적 틀(framework)을 제시합니다. 핵심적으로, 강화 학습은 여러 단계로 이루어진 추론 연쇄의 최종 산출물에 대한 보상을 통해, 모형이 더 확장된 실행 시간 계산(즉, 장문의 응답)을 활용하여 과업을 습득하게끔 유도합니다. 예를 들어, 주로 짧은 예시로 구성된 간단한 패리티(parity) 작업에서, 강화 학습으로 미세 조정된 트랜스포머(transformer)는 긴 추론 과정을 사용하여 올바르게 일반화(generalize)하는 반면, 순수하게 다음 토큰 예측으로 훈련된 모델은 기하급수적으로 더 많은 데이터가 필요했습니다. 그들은 또한 실제 모델에서도 이러한 효과를 확인했습니다. 전문가 혼합(mixture-of-experts) 수학 문제에 LLaMA 모델을 강화 학습(RL) 미세 조정하면, 더 긴 추론 단계를 활용하여 더 나은 일반화 성능을 얻을 수 있었습니다. 이는 강화 학습이 복잡한 추론 문제 해결에 있어 모델의 잠재력을 해방시키는 강력한 도구임을 입증합니다.

**기반 모델 vs. 사고 모델 – '어떻게'와 '언제'의 차이([paper](https://arxiv.org/abs/2405.14590))**: 사고 과정(chain-of-thought)을 수행하는 '사고 모델'이 근본적으로 더 유능한 것일까요, 아니면 단순히 기반 모델(base model)의 역량을 더 효과적으로 활용하는 것일까요? Venhoff 연구팀은 후자에 가깝다는 결론을 내렸습니다. 고정된 기반 모델의 활성화(activation) 상태에서 약 12% 지점에 작은 조향 벡터(steering vector)를 삽입하는 것만으로도, 그들은 모델 내에 잠재되어 있던 추론 능력을 촉발시킬 수 있었고, 완전한 사고 과정 훈련 모델과의 성능 격차를 약 91%까지 줄일 수 있었습니다. 이 모든 성과는 **가중치 업데이트(weight updates) 없이** 달성되었습니다. 이는 방대한 사전 학습을 거친 거대 언어 모형이 이미 추론하는 '**방식**'을 숙지하고 있으며, 특정 목적의 미세 조정은 실질적으로 이러한 능력을 '**어떤 시점**'에 발휘해야 할지 교육하는 역할에 불과하다는 점을 암시합니다. 다시 말해, 사전 학습은 모델에게 추론 메커니즘을 부여하고, 사후 학습(즉, '사고 모델' 훈련)은 주로 적절한 시기에 이러한 메커니즘을 효율적으로 사용하는 방법을 가르칩니다. 이러한 통찰력은 고급 추론 LLM을 바라보는 우리의 관점을 재구성합니다. 추론 능력의 대부분은 처음부터 기반 모델 내에 잠재되어 있었던 것입니다. 이는 효율적인 미세 조정 전략 개발에 중요한 시사점을 제공하며, 모델의 특정 능력을 활성화하는 데 필요한 계산 자원을 크게 줄일 수 있음을 보여줍니다.

### LLM 행위자(Agent) 및 도구 활용의 진화된 접근법

**모방 학습을 넘어선 초기 경험의 가치([paper](https://arxiv.org/abs/2405.14590))**: 메타(Meta)의 "초기 경험을 통한 행위자 학습(Agent Learning via Early Experience)" 연구는 정적 모방 학습(static imitation learning)과 완전한 강화 학습(full reinforcement learning) 사이의 새로운 중간 지점을 제안합니다. 환경 다양성을 충분히 반영하지 못하는 제한적인 전문가 데모(expert demos)에만 의존하는 대신, 이들은 행위자(agent)가 보상 없이 자체적으로 상호작용 데이터(interaction data)를 수집하도록 합니다. 이를 "**초기 경험(early experience)**"이라고 명명합니다. 이 데이터를 활용하는 두 가지 주요 방법이 탐구됩니다. 첫째, **내재적 세계 모형(Implicit world models)** 구축입니다. 행위자가 모은 상태 정보를 바탕으로 주변 환경의 작동 원리를 파악하고, 그에 맞춰 행동 방침을 수립합니다. 둘째, **자기 반성(Self-reflection)**입니다. 행위자가 비최적의 행동으로 인한 오류를 통해 배우고, 이를 바탕으로 추론 역량을 증진시킵니다. 8가지 환경에 걸쳐 이러한 전략은 성능과 도메인 외 일반화(out-of-domain generalization) 능력을 향상시키며, 모방 학습과 완전 자율 행위자(fully autonomous agents) 사이의 간극을 효과적으로 메워줍니다. 특히, 최종 보상을 사용할 수 있는 상황에서도 초기 경험은 후속 강화 학습(RL)을 더욱 효과적으로 만드는 강력한 기반을 제공하여, 초기 탐색 비용을 줄이고 학습 안정성을 높이는 데 기여합니다.

**행위자 강화 학습(Agentic RL) 최적화 전략([paper](https://arxiv.org/abs/2405.14590))**: "행위자 추론에서의 강화 학습(RL) 이해(Demystifying RL in Agentic Reasoning)"에서 Yu 연구팀은 도구 활용 LLM 행위자(agent) 훈련 시 강화 학습(RL)을 최대한 활용하는 방법을 체계적으로 평가합니다. 그들은 몇 가지 간단하지만 강력한 실천 원칙을 제시합니다. 첫째, 단편적인 스니펫(snippet) 대신 실제의 종단 간(end-to-end) 도구 사용 궤적(trajectory)을 활용해야 합니다. 이는 지도 미세 조정(supervised fine-tuning)을 위한 훨씬 강력한 초기화(initialization)를 제공하고, 다양하며 모델 인지적인 경험 데이터(experience data)를 보장합니다. 높은 다양성을 가진 데이터셋(dataset)은 행위자의 과적합(overfitting)을 방지하고 탐색(exploration)을 유지하는 데 도움이 됩니다. 둘째, 탐색 친화적인 기술을 사용합니다. 예를 들어, 더 높은 클리핑 임계값(clipping thresholds)을 허용하고, 더 긴 시간 범위(longer horizons)에 걸쳐 보상을 형성하며, 충분한 정책 엔트로피(policy entropy)를 유지하여 행위자가 새로운 해결책을 시도하도록 장려함으로써 강화 학습(RL) 훈련 효율성을 크게 높입니다. 셋째, 과도하게 잦은 호출이나 불필요하게 긴 내부 대화보다는, 더 적지만 목적 지향적인 도구 사용을 지향하는 **심사숙고형 추론 전략(deliberative reasoning strategy)**을 권장합니다. 즉, 행위자가 행동에 나서기 전에 충분히 숙고하도록 훈련하는 것이 도구 활용의 효율성을 높이고 궁극적인 정밀도를 향상시키는 결과로 이어집니다. 이러한 조언들을 적용하여, 그들은 더 작은 LLM으로도 어려운 벤치마크(benchmark)에서 최첨단 결과(state-of-the-art results)를 달성했으며, 심지어 40억(4B) 모델이 행위자 추론 작업에서 320억(32B) 모델을 능가하는 성과를 보여주었습니다. 또한, 다른 연구자들이 이러한 성과를 재현하는 데 도움이 되도록 고품질의 지도 및 강화 학습(RL) 행위자 궤적(trajectory) 데이터셋(dataset)을 공개했습니다.

**토큰 사전 확률을 활용한 무훈련 강화 학습([paper](https://arxiv.org/abs/2405.14590))**: 강화 학습(reinforcement learning)은 일반적으로 모델 가중치(model weights)를 갱신(update)하는 것을 의미하지만, Cai 연구팀은 미세 조정(finetune) 없이도 정책 개선(policy improvements)을 얻을 수 있음을 입증합니다. 그들의 '**무훈련 그룹 RPO(Training-Free Group RPO)**' 접근 방식은 기본 언어 모델을 변경 불가능한 상태로 간주하며, 그 대신 축적된 경험을 활용하여 출력 토큰의 확률 분포를 실시간으로 조율합니다. 배포(deployment) 과정에서 행위자는 여러 롤아웃(rollout)을 생성합니다. 각 롤아웃 그룹 내에서 이 방법은 토큰에 대한 "그룹 상대 의미론적 이점(group relative semantic advantage)"(특정 토큰이 더 나은 결과로 이어지는 경향)을 계산하고, 이를 토큰 수준의 사전 확률(token-level prior)로 정제하여 모델의 다음 결정을 편향되게 만듭니다. 이 과정을 몇 번의 에포크(epoch) 동안 반복함으로써(소수의 실제 예시만 참조), LLM은 경사 갱신(gradient updates) 없이도 출력을 조종하는 "경험적 지식(experiential knowledge)"을 습득합니다. 수학 추론 및 웹 검색과 같은 작업에서 이 경량 루프(lightweight loop)를 최첨단 행위자(DeepSeek)에 추가했을 때 도메인 외 성능(out-of-domain performance)이 크게 향상되었습니다. 실제로, 단 수십 개의 예시만으로도 무훈련 접근 방식은 전통적인 강화 학습(RL) 훈련 단계를 회피함으로써, 비용과 시간의 극히 일부만으로 완전히 미세 조정된 더 작은 LLM을 능가하는 결과를 보여주었습니다. 이는 실시간 적응 및 계산 효율성 측면에서 큰 잠재력을 지닌 혁신적인 방법론입니다.

### 텐서 논리(Tensor Logic): 신경망(Neural)과 기호적(Symbolic) AI의 융합
페드로 도밍고스(Pedro Domingos)는 **텐서 논리(Tensor Logic)**([paper](https://arxiv.org/abs/2405.14590))를 제안합니다. 이는 신경망(neural)과 기호적(symbolic) 접근 방식을 근본적으로 융합하는 "AI의 언어"로 설계된 야심찬 새 프로그래밍 언어입니다. 이러한 제안의 배경에는 현존하는 개발 도구들의 한계가 있습니다. 파이토치(PyTorch)나 텐서플로우(TensorFlow) 같은 구조들은 GPU 가속 자동 미분 기능을 제공하지만, 논리적 추론이나 지식 표현에 대한 내재적 지원이 부족한 파이썬(Python) 언어에 의존하고 있습니다. 반면, 프롤로그(Prolog)나 리스프(Lisp)와 같은 고전적인 AI 언어들은 기호와 논리를 능숙하게 다루지만, 데이터로부터 확장하거나 학습하는 능력은 제한적입니다. 텐서 논리(Tensor Logic)는 텐서 방정식(tensor equation)을 하나의 **핵심 구성 요소(core construct)**로 설정함으로써 이 문제를 해결합니다. 도밍고스는 논리적 추론 규칙(logical inference rules)이 수학적으로 텐서 인덱스 합산(tensor index summation), 즉 아인슈타인 합산(Einstein summation)과 유사하다는 점을 발견했습니다. AI의 다른 모든 요소는 이 형태로 환원될 수 있다는 것이 그의 주장입니다.

이 논문은 트랜스포머(transformer)와 신경망(neural nets)부터 형식 논리 증명(formal logic proofs), 커널 메서드(kernel methods), 그래픽 모델(graphical models)에 이르기까지 주요 AI 패러다임(paradigm)이 텐서 논리(Tensor Logic)의 통합 프레임워크(unified framework)에서 어떻게 우아하게 구현될 수 있는지를 상세히 보여줍니다. 결정적으로, 이는 신경망(neural)의 확장성 및 학습 가능성과 기호적 추론(symbolic reasoning)의 엄격함을 결합하는 "**임베딩 공간(embedding space)에서의 건전한 추론(sound reasoning)**"과 같은 새로운 가능성을 열어줍니다. 이러한 통합은 단순히 두 패러다임을 나란히 놓는 것을 넘어, 서로의 강점을 유기적으로 결합하여 새로운 형태의 인공지능을 창조할 잠재력을 가집니다. 따라서 텐서 논리(Tensor Logic)는 미래 AI 개발의 기반이 되어, 기호의 투명성과 텐서(tensor)의 강력한 연산 능력을 동시에 제공함으로써, 오랫동안 지속되어 온 신경-기호(neuro-symbolic) 분야의 분열을 해소하고 AI의 더 넓은 채택을 촉진할 수 있을 것입니다.

구독하기 ❤️
이 글이 유익하셨다면, '좋아요'를 누르고 동료들과 공유해 주십시오.
의견을 남겨주세요
LLM Watch를 읽어주셔서 감사합니다! 새로운 소식을 무료로 받아보고 저의 작업을 지지해 주시려면 구독해 주십시오.
구독하기