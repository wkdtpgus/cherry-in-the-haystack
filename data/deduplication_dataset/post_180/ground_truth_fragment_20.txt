환영합니다, 독자 여러분! 이번 주 LLM Watch에서는 빠르게 변화하는 AI 생태계의 최신 동향을 깊이 있게 분석합니다. 특히 언어 모델을 위한 강화 학습(reinforcement learning) 확장, 신경망(neural) AI와 심볼릭(symbolic) AI의 통합, LLM 기반 에이전트(agent)를 위한 새로운 전략 등 다양한 내용을 다룹니다. AI 엔지니어가 되는 방법은 윤리적 고려 사항을 깊이 이해하는 것이며, 산업 중심 코스인 '초급부터 고급 LLM 개발자까지'는 윤리적 AI 개발의 중요성을 강조합니다. 실제 세상에 영향을 미치기 위해 AI는 책임감 있게 개발되어야 합니다. 다시는 중요한 업데이트를 놓치지 않도록 구독하는 것을 잊지 마세요.

가장 빠르게 AI 엔지니어가 되는 방법은? 직접 만들어보는 것입니다! Towards AI의 산업 중심 코스인 '초급부터 고급 LLM 개발자까지(약 90개 강의)'를 통해 실습 경험을 쌓으세요. 실제 세상에 긍정적인 영향을 미치기 위해 윤리적 AI 개발의 중요성을 강조하며, 좌절했던 전직 박사들과 개발자들이 만들었습니다.

*   **프로덕션(production) 수준의 앱 구축**: RAG, 미세 조정(fine-tuning), 에이전트(agent)
*   **지도**: 디스코드(Discord)를 통한 강사 지원
*   **선수 과목**: 기본 파이썬(Python)
*   **결과**: 인증된 제품 출시
*   **가치 보장**: 30일 환불 보장

**기술 향상**
**전문가 팁**: 이 코스와 LLM Watch 모두 회사 학습 및 개발 예산에 포함될 수 있는 중요한 정보원입니다.

### LLM을 위한 강화 학습(RL) 확장

**RL 컴퓨팅(compute)의 예측 가능한 확장( [paper](https://arxiv.org/abs/2405.14590) )**: 메타(Meta)와 협력자들의 연구는 LLM을 위한 강화 학습(RL) 미세 조정(fine-tuning)이 어떻게 확장되는지에 대한 최초의 대규모 분석(40만 GPU-시간!)을 제공합니다. 그들은 시그모이드(sigmoidal) 컴퓨팅-성능 곡선에 맞춰보고, 많은 설계 선택(손실 집계(loss aggregation), 정규화(normalization) 등)이 주로 컴퓨팅 효율성에 영향을 미치지만 최종 성능에는 영향을 미치지 않는다는 것을 발견했습니다. 이러한 통찰력을 바탕으로, 그들은 소규모 실행 학습 곡선을 외삽(extrapolate)하여 최종 성능을 예측하는 모범 사례 레시피인 “**ScaleRL**”을 소개합니다. ScaleRL을 사용하여 그들은 10만 GPU-시간 실행의 결과를 성공적으로 예측했으며, 이는 강화 학습(RL) 훈련을 사전 훈련(pre-training)의 예측 가능성에 더 가깝게 만듭니다.

**장기 추론을 위한 “마르코프 사상가(Markovian Thinker)”( [paper](https://arxiv.org/abs/2405.14590) )**: 긴 체인 오브 스루트(chain-of-thought) 추론은 일반적으로 LLM 에이전트(agent)가 계속 증가하는 프롬프트(prompt)에 의존하도록 강제합니다(생각이 길어질수록 강화 학습(RL) 정책이 이차 비용을 지불하게 됨). Aghajohari 등은 **마르코프 사상(Markovian Thinking)**을 제안하며, 강화 학습(RL) “사고 환경(thinking environment)”을 재구성하여 모델이 항상 **고정된 크기의 상태(state)**만 보도록 합니다. 그들의 Delethink 설정에서 추론은 세그먼트(segment)로 분할됩니다. 모델은 각 청크(chunk)의 끝에서 간략한 상태 요약(state summary)을 생성한 다음, 컨텍스트(context)를 재설정하고 해당 요약으로 새로 시작합니다. 이는 선형 시간(linear-time), 상수 메모리(constant-memory) 확장을 가능하게 합니다. Delethink로 훈련된 15억(1.5B) 모델은 8천 토큰(8k-token) 청크(chunk)로 2만 4천 토큰(24k tokens)을 추론할 수 있으며, 전체 2만 4천 토큰(24k-token) 컨텍스트(context)로 훈련된 기준선(baseline)과 같거나 능가합니다. 더욱 인상적인 것은, 더 긴 추론(9만 6천 토큰(96k tokens))에서도 계속 개선되는 반면, 기준선은 정체됩니다. 컴퓨팅 비용은 대략 4분의 1 수준입니다(기준선의 27 H100-개월 대비 7 H100-개월 추정). 따라서 모델뿐만 아니라 환경을 재설계하는 것이 효율적이고 확장 가능한 장기 추론을 가능하게 합니다.

### 강화 학습(RL) 미세 조정(Fine-Tuning)이 추론 능력을 해제하다

**강화 학습(RL)이 LLM 추론에 도움이 되는 이유( [paper](https://arxiv.org/abs/2405.14590) )**: Tsilivis 등은 다음 토큰(next-token) 사전 훈련(pre-training) 후 강화 학습(RL) 미세 조정(fine-tuning)이 추론을 어떻게 향상시키는지에 대한 질문을 다룹니다. 그들은 사전 훈련 후 강화 학습(RL)을 적용하면 모델이 다음 토큰 예측(next-token prediction)만으로는 훨씬 더 적은 데이터로 긴 체인 오브 스루트(chain-of-thought)를 요구하는 작업을 해결할 수 있음을 보여주는 이론적 프레임워크(framework)를 소개합니다. 본질적으로 강화 학습(RL)은 다단계 추론 체인(reasoning chain)의 최종 결과에 보상을 제공하여 모델이 더 긴 테스트 시간 컴퓨팅(test-time computation)(더 긴 답변)을 활용하여 작업을 학습하도록 합니다. 예를 들어, 대부분 짧은 예시로 구성된 장난감 패리티(parity) 작업에서 강화 학습(RL)으로 미세 조정된 트랜스포머(transformer)는 긴 체인을 사용하여 올바르게 일반화(generalize)하는 반면, 순수하게 다음 토큰(next-token)으로 훈련된 모델은 기하급수적으로 더 많은 데이터를 필요로 합니다. 그들은 또한 실제 모델에 대한 효과를 확인했습니다. 전문가 혼합(mixture-of-experts) 수학 문제에 LLaMA를 강화 학습(RL) 미세 조정(fine-tuning)하면 더 긴 추론 단계(reasoning steps)를 활용하여 더 나은 일반화(generalization)를 얻을 수 있습니다.

**“기반(Base)” 모델 vs “사고(Thinking)” 모델 - 방법을 아는 것 vs 시기를 아는 것( [paper](https://arxiv.org/abs/2405.14590) )**: 체인 오브 스루트(chain-of-thought) “사고(thinking)” 모델이 근본적으로 더 유능한가, 아니면 단순히 기반 모델(base model) 기술을 더 잘 활용하는가? Venhoff 등은 대체로 후자임을 발견했습니다. 고정된 기반 모델(base model)의 활성화(activation)에 약 12%의 위치에서 작은 조향 벡터(steering vector)를 주입함으로써, 그들은 모델의 잠재된 추론 능력(reasoning abilities)을 촉발하고 완전한 체인 오브 스루트(chain-of-thought) 훈련 모델까지의 성능 격차의 약 91%를 회복할 수 있었습니다. 이 모든 것은 **가중치 업데이트(weight updates) 없이** 이루어졌습니다. 이는 대규모 사전 훈련(pre-trained) LLM이 이미 추론 “**방법**”을 알고 있으며, 특화된 미세 조정(finetuning)이 실제로 하는 일은 이러한 기술을 **언제** 배포할지 가르치는 것임을 시사합니다. 다시 말해, 사전 훈련(pre-training)은 모델에 추론 메커니즘(reasoning mechanisms)을 부여하고, 사후 훈련(“사고(thinking)” 모델 훈련)은 주로 적절한 시기에 이러한 메커니즘을 효율적으로 사용하는 방법을 가르칩니다. 이러한 통찰력은 고급 추론 LLM을 보는 우리의 관점을 재구성합니다. 추론 능력의 대부분은 처음부터 기반 모델(base model)에 잠재되어 있었습니다.

### LLM 에이전트(Agent) 및 도구 사용을 위한 새로운 패러다임

**순수 모방 대신 초기 경험( [paper](https://arxiv.org/abs/2405.14590) )**: 메타(Meta)의 * “초기 경험을 통한 에이전트 학습(Agent Learning via Early Experience)”은 정적 모방 학습(static imitation learning)과 완전한 강화 학습(full reinforcement learning) 사이의 중간 지점을 제안합니다. 환경 다양성을 노출하지 않는 좁은 전문가 데모(expert demos)에만 의존하는 대신, 그들은 에이전트(agent)가 보상 없이 자체 상호작용 데이터(interaction data)를 수집하도록 합니다. 이를 “**초기 경험(early experience)**”이라고 부릅니다. 이 데이터의 두 가지 활용법이 탐구됩니다. (1) **암묵적 세계 모델(Implicit world models)**: 에이전트(agent)가 수집된 상태(state)를 통해 환경이 어떻게 작동하는지에 따라 정책(policy)을 학습하고, (2) **자기 성찰(Self-reflection)**: 에이전트(agent)가 실수(최적이 아닌 행동)로부터 학습하여 추론을 개선합니다. 8가지 환경에서 이러한 전략은 성능과 도메인 외 일반화(out-of-domain generalization)를 향상시키며, 모방(imitation)과 완전 자율 에이전트(fully autonomous agents) 사이의 간극을 효과적으로 메웁니다. 특히, 최종 보상이 사용 가능할 때에도 초기 경험(early experience)은 후속 강화 학습(RL)을 더욱 효과적으로 만드는 강력한 기반을 제공합니다.

**에이전트 강화 학습(Agentic RL)을 위한 모범 사례( [paper](https://arxiv.org/abs/2405.14590) )**: “에이전트 추론에서의 강화 학습(RL) 이해(Demystifying RL in Agentic Reasoning)”에서 Yu 등은 도구 사용 LLM 에이전트(agent)를 훈련할 때 강화 학습(RL)을 최대한 활용하는 방법을 체계적으로 평가합니다. 그들은 몇 가지 간단하지만 강력한 관행을 식별합니다. (i) 조각조각 이어진 스니펫(snippet) 대신 실제의, 종단 간(end-to-end) 도구 사용 궤적(trajectory)을 사용합니다. 이는 지도 미세 조정(supervised fine-tuning)을 위한 훨씬 강력한 초기화(initialization)를 제공하고 다양하며 모델 인지적인 경험 데이터(experience data)를 보장합니다. 높은 다양성을 가진 데이터셋(dataset)은 에이전트(agent)의 과적합(overfitting)을 방지하고 탐색(exploration)을 유지하는 데 도움이 됩니다. (ii) 탐색 친화적인 기술을 사용합니다. 예를 들어, 더 높은 클리핑 임계값(clipping thresholds)을 허용하고, 더 긴 시간 범위(longer horizons)에 걸쳐 보상을 형성하며, 충분한 정책 엔트로피(policy entropy)를 유지하여 에이전트(agent)가 새로운 해결책을 시도하도록 장려함으로써 강화 학습(RL) 훈련 효율성을 크게 높입니다. (iii) 매우 빈번한 호출이나 지나치게 장황한 자기 대화(self-dialogues)보다 더 적고 더 목표 지향적인 도구 호출을 사용하는 신중한 추론 전략(deliberative reasoning strategy)을 선호합니다. 다시 말해, 에이전트(agent)에게 행동하기 전에 조금 더 생각하도록 가르치는 것이 더 나은 도구 효율성과 최종 정확도로 이어집니다. 이러한 팁을 사용하여 그들은 더 작은 LLM으로도 어려운 벤치마크(benchmark)에서 최첨단 결과(state-of-the-art results)를 달성했으며, 심지어 40억(4B) 모델이 에이전트 추론 작업에서 320억(32B) 모델을 능가하도록 만들었습니다. 그들은 또한 다른 사람들이 이러한 성과를 재현하는 데 도움이 되도록 고품질의 지도 및 강화 학습(RL) 에이전트 궤적(trajectory) 데이터셋(dataset)을 공개합니다.

**토큰 사전 확률(Token Priors)을 통한 훈련 없는 강화 학습(RL)( [paper](https://arxiv.org/abs/2405.14590) )**: 강화 학습(reinforcement learning)은 일반적으로 모델 가중치(model weights)를 업데이트(update)하는 것을 의미하지만, Cai 등은 미세 조정(finetune) 없이도 정책 개선(policy improvements)을 얻을 수 있음을 보여줍니다. 그들의 “**훈련 없는 그룹 RPO(Training-Free Group RPO)**” 방법은 기반 LLM을 고정된 것으로 취급하고, 대신 경험을 사용하여 출력 토큰 분포(output token distribution)를 즉석에서 조정합니다. 배포 중에 에이전트(agent)는 여러 롤아웃(rollout)을 생성합니다. 각 롤아웃(rollout) 그룹 내에서 이 방법은 토큰에 대한 “그룹 상대 의미론적 이점(group relative semantic advantage)”(어떤 토큰이 더 나은 결과로 이어졌는지 생각해보세요)을 계산하고, 이를 토큰 수준의 사전 확률(token-level prior)로 증류하여 모델의 다음 결정을 편향되게 만듭니다. 이 과정을 몇 번의 에포크(epoch) 동안 반복함으로써(참조용으로 소수의 실제 예시만 사용), LLM은 경사 업데이트(gradient updates) 없이도 출력을 조종하는 “경험적 지식(experiential knowledge)”을 습득합니다. 수학 추론 및 웹 검색과 같은 작업에서 이 경량 루프(lightweight loop)를 최첨단 에이전트(DeepSeek)에 추가하면 도메인 외 성능(out-of-domain performance)이 크게 향상되었습니다. 실제로, 단 몇십 개의 예시만으로도 훈련 없는 접근 방식은 전통적인 강화 학습(RL) 훈련 단계를 피하므로 비용과 시간의 극히 일부만으로 완전히 미세 조정된 더 작은 LLM을 능가했습니다.

### LLM의 윤리적 사용과 책임 있는 개발

대규모 언어 모델(LLM)은 사회 전반에 걸쳐 그 영향력을 확장하고 있으며, 이에 따라 윤리적 사용과 책임 있는 개발의 중요성이 더욱 부각되고 있습니다. 강력한 AI 기술이 가져올 긍정적인 변화만큼이나, 잠재적인 위험 요소에 대한 깊이 있는 이해와 대비가 필수적입니다.

첫째, **편향(bias) 문제**는 LLM 개발의 핵심 과제 중 하나입니다. 모델은 훈련 데이터에 내재된 사회적, 문화적 편향을 학습하고 이를 출력에 반영할 수 있습니다. 예를 들어, 복잡한 작업에서 미세 조정된 모델은 뛰어난 일반화(generalize) 능력을 보여주지만, 편향된 데이터로 훈련될 경우 예측할 수 없는 결과를 초래할 수 있습니다. 공정성(fairness)을 확보하기 위한 데이터셋(dataset)의 신중한 큐레이션(curation)과 편향 완화(bias mitigation) 기술 개발이 시급합니다. 강화 학습(RL)은 다양한 AI 시스템에 긍정적인 영향을 미치지만, 윤리적 가이드라인 없이는 오용될 위험이 있습니다.

둘째, **투명성(transparency)과 설명 가능성(explainability)**은 LLM의 신뢰성을 높이는 데 결정적입니다. 현재 대부분의 LLM은 '블랙박스'처럼 작동하여, 특정 결과가 도출된 과정을 이해하기 어렵습니다. Tsilivis 등은 대규모 언어 모델의 성능 향상에 대한 중요한 질문을 다루며, 모델의 의사결정 과정을 추적하고 설명할 수 있는 기술, 즉 XAI(Explainable AI) 연구가 활발히 진행되고 있습니다. 본질적으로 강화 학습(RL)은 복잡한 의사결정 과정에서 최적의 결과를 얻도록 모델을 이끌어주지만, 그 과정 자체의 투명성은 여전히 과제로 남아 있습니다.

셋째, **안전(safety) 및 보안(security)** 측면에서도 고려해야 할 점이 많습니다. LLM은 유해하거나 오해의 소지가 있는 콘텐츠를 생성하거나, 악의적인 목적으로 사용될 수 있습니다. 혐오 발언, 허위 정보, 사이버 공격 코드 생성 등 다양한 형태의 오용 가능성에 대비하여 안전 필터링(safety filtering) 메커니즘을 강화하고, 모델의 취약점을 보완하는 보안 기술을 개발해야 합니다. “기반(Base)” 모델과 “최적화(Optimized)” 모델 간의 중요한 차이는 성능을 결정하지만, 안전성 측면에서는 동일한 수준의 주의가 필요합니다.

결론적으로, LLM의 발전은 인류에게 엄청난 기회를 제공하지만, 동시에 중대한 윤리적, 사회적 책임을 요구합니다. 이러한 통찰력은 고급 AI 모델을 보는 우리의 관점을 재구성하며, 추론 능력의 대부분은 처음부터 기반 모델(base model)에 내재되어 있었음을 상기시킵니다. 기술 개발자, 정책 입안자, 그리고 일반 사용자 모두가 협력하여 AI의 긍정적인 잠재력을 극대화하고 부정적인 영향을 최소화하는 책임 있는 AI 생태계를 구축해야 합니다.

### 멀티모달(Multimodal) LLM의 진화와 새로운 응용

텍스트 기반의 한계를 넘어, 다양한 양식(modality)의 정보를 이해하고 생성하는 **멀티모달(Multimodal) LLM**은 AI 연구의 최전선에 있습니다. 이는 인간이 세상을 인지하는 방식에 더 가깝게 AI를 발전시키려는 노력의 일환입니다. 컴퓨팅(compute)의 예측 가능한 확장은 멀티모달 LLM의 미래를 밝힙니다.

**비전-언어 모델(Vision-Language Models, VLM)**은 이미지와 텍스트를 동시에 처리하여, 이미지 캡셔닝(image captioning), 시각적 질문 답변(Visual Question Answering, VQA), 이미지 생성 등 다양한 작업을 수행합니다. 메타(Meta)와 협력자들의 연구는 멀티모달 데이터의 대규모 분석을 제공하며, 비전-언어 모델의 가능성을 확장하고 있습니다. 예를 들어, 사용자가 이미지를 보여주며 "이 이미지에 대해 설명해줘"라고 질문하면, 모델은 이미지의 내용을 분석하여 자연어로 답변을 생성할 수 있습니다. 이는 시각 장애인을 위한 정보 접근성 개선이나, 복잡한 시각 자료의 자동 분석에 활용될 수 있습니다. 그들은 많은 설계 선택이 주로 컴퓨팅 효율성에 영향을 미치지만, 멀티모달 모델의 최적화에도 중요합니다.

나아가, **오디오-언어 모델**은 음성 데이터와 텍스트를 통합하여 음성 인식(speech recognition), 음성 합성(speech synthesis), 화자 인식(speaker recognition) 및 감성 분석(sentiment analysis)과 같은 기능을 제공합니다. 이 기술은 고객 서비스 자동화, 언어 학습 도구, 그리고 개인 비서 기능 향상에 크게 기여할 수 있습니다. 이러한 통찰력을 바탕으로, 그들은 멀티모달 모델의 최종 성능을 예측하는 새로운 방법을 제시합니다.

이러한 멀티모달 모델의 핵심은 단순히 여러 양식을 개별적으로 처리하는 것을 넘어, **통합된 이해**를 통해 각 양식 간의 복잡한 관계를 파악하고 심층적인 추론을 수행하는 능력에 있습니다. 예를 들어, 특정 상황을 담은 비디오와 오디오, 그리고 관련 텍스트 설명을 동시에 분석하여 상황을 종합적으로 판단하는 것입니다. 순수 모방 대신 초기 경험은 학습 효율성을 높이며, 이는 멀티모달 환경에서도 적용될 수 있습니다. 메타(Meta)의 최근 연구는 정적 모방 학습(static imitation learning)과 완전한 강화 학습(full reinforcement learning) 사이의 새로운 접근 방식을 제안하며, 멀티모달 에이전트 학습에 영감을 줍니다.

멀티모달 LLM은 **로봇 공학(robotics)** 분야에서 로봇이 주변 환경을 더 잘 이해하고 상호작용하는 데 필수적인 역할을 합니다. 또한, **의료 진단(medical diagnosis)** 분야에서는 의료 영상(image)과 환자 기록(text), 음성 데이터(audio)를 결합하여 보다 정확한 진단을 돕고, **교육 콘텐츠 생성**에서는 시각 자료와 설명을 결합하여 학습 효과를 극대화할 수 있습니다. 다양한 환경에서 이러한 전략은 성능과 일반화(generalization)를 향상시키며, 기존 방법론 사이의 간극을 효과적으로 메웁니다. 특히, 최종 보상이 사용 가능할 때에도 초기 경험(early experience)은 후속 학습을 더욱 효과적으로 만드는 강력한 기반을 제공합니다. 멀티모달 AI의 발전은 우리가 상상하는 것 이상의 새로운 응용 분야를 지속적으로 창출할 것입니다.

### 소규모 LLM 최적화 및 배포 전략

대규모 언어 모델(LLM)의 뛰어난 성능에도 불구하고, 막대한 컴퓨팅 자원과 메모리 요구사항은 광범위한 배포에 큰 장벽이 됩니다. 이러한 한계를 극복하고 LLM의 접근성을 높이기 위해 **소규모 LLM 최적화 및 배포 전략**이 중요하게 부상하고 있습니다. 장기 추론을 위한 효율적인 접근 방식은 소규모 LLM에 특히 중요합니다.

**모델 경량화 기술**은 LLM을 더 작고 효율적으로 만드는 핵심 방법입니다.
*   **양자화(Quantization)**: 모델 가중치의 정밀도를 32비트 부동소수점(float32)에서 16비트(float16) 또는 8비트(int8), 심지어 4비트(int4) 정수로 낮추는 기술입니다. 이는 모델의 크기를 크게 줄이고 추론 속도를 향상시키면서도 성능 손실을 최소화합니다. 그들은 10만 GPU-시간 실행의 결과를 성공적으로 예측했으며, 이는 소규모 LLM의 배포 가능성을 높입니다.
*   **지식 증류(Knowledge Distillation)**: 대규모 '교사(teacher)' 모델의 지식을 훨씬 작은 '학생(student)' 모델에 전이하는 방법입니다. 학생 모델은 교사 모델의 예측 결과나 내부 표현을 모방하도록 학습되어, 작은 크기에서도 유사한 성능을 낼 수 있습니다. 긴 체인 오브 스루트(chain-of-thought) 추론은 일반적으로 복잡한 문제 해결에 활용되지만, 소규모 모델에서는 최적화가 필수적입니다. Aghajohari 등은 모델이 항상 고정된 크기의 상태(state)만 효율적으로 처리하도록 제안했습니다.
*   **가지치기(Pruning)**: 모델 내에서 중요도가 낮은 가중치나 연결을 제거하여 모델의 희소성(sparsity)을 높이는 기술입니다. 이는 모델 크기를 줄이고 계산량을 감소시키는 효과가 있습니다. 그들의 설정에서 추론은 세그먼트(segment)로 분할되어 소규모 모델의 효율성을 높입니다. 모델은 각 청크(chunk)의 끝에서 간략한 상태 요약(state summary)을 생성한 다음, 소규모 모델의 메모리 사용량을 최적화합니다.

이러한 최적화 기술은 **온디바이스(on-device) LLM**의 실현 가능성을 열어줍니다. 스마트폰, 태블릿, 엣지(edge) 기기 등 제한된 자원을 가진 장치에서도 LLM을 직접 실행할 수 있게 되어, 클라우드 연결 없이도 AI 기능을 사용할 수 있습니다. 이는 데이터 프라이버시(data privacy)를 강화하고, 네트워크 지연 시간(latency)을 줄이며, 클라우드 컴퓨팅 비용을 절감하는 이점을 제공합니다. 토큰 사전 확률(Token Priors)을 통한 훈련 없는 학습은 효율적인 AI 개발을 가능하게 합니다. 강화 학습(reinforcement learning)은 일반적으로 모델 가중치(model weights)를 업데이트(update)하는 것을 의미하지만, Cai 등은 미세 조정(finetune) 없이도 성능 개선(improvements)을 얻을 수 있음을 보여줍니다.

효율적인 배포 전략은 LLM을 더 많은 사용자에게 더 낮은 비용으로 제공하는 데 필수적입니다. 에이전트 강화 학습(Agentic RL)을 위한 모범 사례는 최적의 성능을 보장하며, 이는 소규모 모델에도 적용될 수 있습니다. 그들의 “**훈련 없는 그룹 RPO(Training-Free Group RPO)**” 방법은 기반 모델을 고정된 것으로 취급하고, 대신 경험을 사용하여 출력 분포(output distribution)를 즉석에서 조정합니다. 배포 중에 에이전트(agent)는 여러 시나리오(rollout)를 생성합니다. 각 시나리오(rollout) 그룹 내에서 이 방법은 토큰에 대한 의미론적 이점(semantic advantage)을 계산하고, 이를 토큰 수준의 사전 확률(token-level prior)로 증류하여 모델의 다음 결정을 편향되게 만듭니다. 이 과정을 몇 번의 반복(epoch) 동안 반복함으로써, 모델은 경사 업데이트(gradient updates) 없이도 출력을 조종하는 “경험적 지식(experiential knowledge)”을 습득합니다. 수학 추론 및 웹 검색과 같은 작업에서 이 경량 루프(lightweight loop)를 추가하면 도메인 외 성능(out-of-domain performance)이 크게 향상되었습니다. 실제로, 단 몇십 개의 예시만으로도 훈련 없는 접근 방식은 전통적인 학습 단계를 피하므로 비용과 시간의 극히 일부만으로 더 작은 모델을 능가했습니다. 궁극적으로, 이러한 노력은 LLM이 단순히 거대 기술 기업의 전유물이 아닌, 모든 사람이 접근하고 활용할 수 있는 강력한 도구가 되도록 하는 데 기여할 것입니다.

### 텐서 로직(Tensor Logic): 신경망(Neural) 및 심볼릭(Symbolic) AI 통합

페드로 도밍고스(Pedro Domingos)는 **텐서 로직(Tensor Logic)**( [paper](https://arxiv.org/abs/2405.14590) )을 제안합니다. 이는 신경망(neural) 및 심볼릭(symbolic) 접근 방식을 기본적으로 융합하는 “AI의 언어”로 설계된 야심찬 새 프로그래밍 언어입니다. 그 동기는 현재 도구가 부족하다는 것입니다. 파이토치(PyTorch)/텐서플로우(TF)와 같은 프레임워크(framework)는 GPU에서 자동 미분(auto-differentiation)을 제공하지만 파이썬(Python)에 부착되어 있습니다(파이썬은 논리적 추론이나 지식에 대한 내장 지원이 없음). 반면 고전적인 AI 언어(프롤로그(Prolog), 리스프(Lisp))는 기호와 논리를 처리하지만 데이터로부터 확장하거나 학습할 수 없습니다. 텐서 로직(Tensor Logic)은 텐서 방정식(tensor equation)을 하나의 **핵심 구성 요소(core construct)**로 만듦으로써 이 문제를 해결합니다. 도밍고스는 논리적 추론 규칙(logical inference rules)이 수학적으로 텐서 인덱스 합산(tensor index summation)(아인슈타인 합산(Einstein summation))과 유사하다고 관찰했기 때문입니다. AI의 다른 모든 것은 그 형태로 환원될 수 있습니다. 이 논문은 트랜스포머(transformer)와 신경망(neural nets)부터 형식 논리 증명(formal logic proofs), 커널 메서드(kernel methods), 그래픽 모델(graphical models)에 이르기까지 주요 패러다임(paradigm)이 텐서 로직(Tensor Logic)의 통합 프레임워크(unified framework)에서 어떻게 우아하게 구현될 수 있는지를 보여줍니다. 결정적으로, 이는 신경망(neural)의 확장성/학습 가능성과 심볼릭 추론(symbolic reasoning)의 엄격함을 결합하는 “**임베딩 공간(embedding space)에서의 건전한 추론(sound reasoning)**”과 같은 새로운 가능성을 열어줍니다. 따라서 텐서 로직(Tensor Logic)은 앞으로 AI의 기반이 되어 기호의 투명성과 텐서(tensor)의 힘을 제공함으로써, 오랫동안 지속되어 온 신경-심볼릭(neuro-symbolic) 분열을 해소하여 AI의 더 넓은 채택을 촉진할 잠재력을 가집니다.

### 결론 및 전망

LLM의 윤리적 사용, 멀티모달 기능의 확장, 그리고 효율적인 배포 전략은 AI 기술이 사회에 더 깊이 통합되고 긍정적인 영향을 미치기 위한 필수적인 요소들입니다. 앞으로 LLM은 더욱 안전하고, 다재다능하며, 접근하기 쉬운 형태로 발전할 것입니다.

구독하기 ❤️
이 기사가 마음에 드셨다면, 좋아요를 누르고 동료들과 공유해주세요!
댓글을 남겨주세요.
LLM Watch를 읽어주셔서 감사합니다! 새로운 게시물을 무료로 받고 제 작업을 지원하려면 지금 구독하기.