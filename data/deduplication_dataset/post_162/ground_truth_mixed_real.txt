연구자들 간의 합의가 부족하다는 점을 고려할 때, 정부는 인공지능(AI)으로 인한 실존적 위험(existential risk, x-risk)의 위협을 얼마나 심각하게 받아들여야 할까요? 한편으로, 실존적 위험(x-risk)은 필연적으로 다소 추측적입니다. 구체적인 증거가 나올 때쯤이면 너무 늦을 수도 있기 때문입니다. 다른 한편으로, 정부는 우선순위를 정해야 합니다. 결국, 외계인 침공으로 인한 실존적 위험(x-risk)에 대해 지나치게 걱정하지는 않으니까요. 이 글은 AI 실존적 위험(AI x-risk)에 대해 우려하는 정책 입안자들을 위한 증거 기반 접근 방식을 제시하는 일련의 에세이 중 업데이트된 버전입니다. 이 접근 방식은 "알려지지 않은 미지의 것(unknown unknowns)"이 존재함을 인정하면서도 현실에 기반을 두고 있습니다. 이 업데이트된 에세이에서는 한 가지 유형의 증거인 확률 추정치를 살펴봅니다. AI 안전 커뮤니티는 의사 결정 및 정책 수립에 정보를 제공하기 위해 AI로 인한 인류 멸종 확률(주어진 기간 내)을 예측하는 데 크게 의존합니다. 예를 들어, 수십 년 내 10%라는 추정치는 이 문제가 사회의 최우선 과제가 되기에 충분히 높은 수치일 것입니다. 우리의 핵심 주장은 AI 실존적 위험(AI x-risk) 예측이 정책에 활용하기에는 너무 신뢰할 수 없으며, 사실상 매우 오해의 소지가 있다는 것입니다. 이 시리즈의 다음 에세이를 받아보려면 구독하세요. 구독

### 장막 뒤를 들여다보다: 예측의 본질과 정책의 책임

만약 우리 둘이 앞으로 10년 안에 외계인이 지구에 착륙할 확률이 80%라고 예측한다면, 당신은 이 가능성을 심각하게 받아들일까요? 물론 아닐 것입니다. 당신은 우리의 증거를 요구할 것입니다. 이것이 아무리 명백해 보일지라도, AI 실존적 위험(AI x-risk) 논쟁에서는 확률 자체가 권위를 가지지 않는다는 사실이 잊혀진 것 같습니다. 확률은 일반적으로 어떤 근거 있는 방법에서 도출되므로, 우리는 정량화된 위험 추정치를 질적 추정치보다 더 타당하다고 보는 강한 인지 편향을 가지고 있습니다. 그러나 확률이 단순한 추측에 불과할 수도 있습니다. 이 에세이 전체(그리고 더 넓게는 AI 실존적 위험(AI x-risk) 논쟁에서) 이 점을 명심하십시오. 최근 몇 년간 AI 기술의 급격한 발전과 함께, 이러한 예측에 대한 대중의 관심과 정책적 논의는 더욱 증폭되었습니다.

켄터키 더비(Kentucky Derby)의 배당률을 예측한다면, 우리는 당신에게 이유를 설명할 필요가 없습니다. 당신은 그것을 받아들이거나 말거나 할 수 있습니다. 그러나 정책 입안자가 예측가가 제시한 확률에 근거하여 조치를 취한다면, 그들은 대중에게 그 확률을 설명할 수 있어야 합니다(그리고 그 설명은 다시 예측가로부터 나와야 합니다). 정당성은 정부의 정당성과 권력 행사에 필수적입니다. 자유 민주주의의 핵심 원칙은 국가가 합리적인 사람들이 거부할 수 있는 논란의 여지가 있는 신념에 기반하여 사람들의 자유를 제한해서는 안 된다는 것입니다. 설명은 고려 중인 정책이 비용이 많이 들 때 특히 중요하며, 그 비용이 이해관계자들 사이에 불균등하게 분배될 때는 더욱 그렇습니다. 좋은 예는 AI 모델의 공개 출시를 제한하는 것입니다. 정부는 공개 모델로부터 이익을 얻을 사람들과 기업들에게 추측적인 미래 위험 때문에 이러한 희생을 해야 한다고 설득할 수 있을까요? 2024년 이후, AI 모델의 접근성과 배포에 대한 규제 논의는 더욱 활발해졌으며, 이 질문의 중요성은 더욱 커졌습니다.

이 에세이의 주요 목표는 정책 논쟁에서 인용된 특정 실존적 위험(x-risk) 확률 추정치에 대한 정당성이 있는지 분석하는 것입니다. 우리는 AI 실존적 위험(AI x-risk) 예측을 학술 활동으로 보는 것에 반대하지 않으며, 예측은 기업 및 기타 사적 의사 결정자들에게 유용할 수 있습니다. 우리는 단지 공공 정책의 맥락에서 그 사용에 의문을 제기할 뿐입니다.

예측가가 회의론자를 설득할 수 있는 알려진 방법은 기본적으로 세 가지뿐입니다: 귀납적(inductive), 연역적(deductive), 그리고 주관적 확률 추정(subjective probability estimation)입니다. 다음 섹션에서 각각을 살펴보겠습니다. 세 가지 모두 양 당사자가 세상에 대한 몇 가지 기본적인 가정(그 자체로 증명될 수 없는)에 동의해야 합니다. 세 가지 접근 방식은 확률 추정치가 그 가정 집합에서 도출되는 경험적 및 논리적 방식에서 차이가 있습니다.

### 참조 집단(reference class)의 부족으로 인해 귀납적 확률 추정(inductive probability estimation)은 신뢰할 수 없습니다

대부분의 위험 추정치는 귀납적(inductive)입니다. 즉, 과거 관찰에 기반합니다. 예를 들어, 보험사는 유사한 운전자의 과거 사고 데이터에 기반하여 개인의 자동차 사고 위험을 예측합니다. 확률 추정에 사용되는 관찰 집합을 참조 집단(reference class)이라고 합니다. 자동차 보험에 적합한 참조 집단(reference class)은 같은 도시에 사는 운전자 집합일 수 있습니다. 분석가가 개인에 대한 더 많은 정보(예: 나이 또는 운전하는 자동차 유형)를 가지고 있다면, 참조 집단(reference class)은 더욱 세분될 수 있습니다.

AI로 인한 실존적 위험(existential risk)의 경우, 다른 어떤 사건과도 같지 않으므로 참조 집단(reference class)이 없습니다. 분명히 말하자면, 이것은 종류의 문제가 아니라 정도의 문제입니다. 사용할 명확한 "올바른" 참조 집단(reference class)은 결코 없으며, 실제 참조 집단(reference class)의 선택은 분석가의 직관에 달려 있습니다. 예측의 정확성은 예측되는 사건을 생성하는 과정과 참조 집단(reference class)의 사건을 생성한 과정 간의 유사성 정도에 따라 달라지며, 이는 스펙트럼으로 볼 수 있습니다.

동전 던지기와 같은 물리적 시스템의 결과를 예측하는 데는 과거 경험이 매우 신뢰할 수 있는 지침이 됩니다. 다음으로, 자동차 사고의 경우, 위험 추정치는 사용된 과거 데이터셋에 따라 예를 들어 20% 정도 달라질 수 있습니다. 이는 보험 회사에 충분합니다. 스펙트럼을 더 따라가면 지정학적 사건이 있는데, 여기서는 참조 집단(reference class)의 선택이 훨씬 더 모호해집니다. 예측 전문가 필립 테틀록(Philip Tetlock)은 다음과 같이 설명합니다: "그렉시트(Grexit)는 2015년 현재 유로존을 탈퇴한 국가가 없었기 때문에 독특해 보였을 수 있지만, 협상 실패와 같은 광범위한 비교 집단이나, 국가가 국제 협정에서 탈퇴하는 것과 같은 더 좁은 집단, 또는 더 좁게는 강제 통화 전환의 또 다른 사례로 볼 수도 있습니다." 그는 소련 붕괴나 아랍의 봄과 같은 겉보기에 블랙 스완(Black Swan) 사건조차도 참조 집단(reference class)의 구성원으로 모델링될 수 있으며, 귀납적 추론(inductive reasoning)이 이러한 종류의 사건에도 유용하다는 생각을 옹호합니다. 테틀록의 스펙트럼에서 이러한 사건들은 독특함의 "정점"을 나타냅니다. 지정학적 사건에 관해서는 그것이 사실일 수 있습니다. 그러나 그러한 사건들조차도 AI로 인한 멸종보다는 훨씬 덜 독특합니다.

AI 실존적 위험(AI x-risk)에 대한 참조 집단(reference class)을 찾으려는 시도를 보십시오: 동물 멸종(인류 멸종의 참조 집단(reference class)으로), 산업 혁명과 같은 과거의 전 세계적 변화(AI로 인한 사회경제적 변화의 참조 집단(reference class)으로), 또는 대규모 사망을 초래한 사고(전 세계적 재앙을 초래한 사고의 참조 집단(reference class)으로). 현실을 직시합시다. 그 어떤 것도 초지능 AI(superintelligent AI) 개발 가능성이나 그러한 AI에 대한 통제력 상실 가능성에 대해 우리에게 아무것도 알려주지 않습니다. 이는 AI 실존적 위험(AI x-risk) 예측의 불확실성의 핵심 원천입니다. 요약하자면, AI로 인한 인류 멸종은 과거에 일어났던 어떤 일과도 너무나 동떨어진 결과이므로, 귀납적 방법(inductive methods)을 사용하여 그 확률을 "예측"할 수 없습니다. 물론, 과거의 기술적 돌파구와 재앙적인 사건들로부터 질적 통찰력을 얻을 수는 있지만, AI 위험은 정책 수립의 정당성에 필요한 종류의 정량적 추정치 정당성이 부족할 정도로 충분히 다릅니다. 최근에는 AI가 가져올 수 있는 사회적 변화를 과거의 기술 혁명과 비교하려는 시도가 있었으나, AI의 자기 개선(self-improvement) 가능성 등 근본적인 차이점으로 인해 이러한 비교는 여전히 한계를 가집니다.

### 이론의 부족으로 인해 연역적 확률 추정(deductive probability estimation)은 신뢰할 수 없습니다

코난 도일(Conan Doyle)의 『여섯 개의 나폴레옹 흉상(The Adventure of the Six Napoleons)』에서 — 스포일러 경고! — 셜록 홈즈(Sherlock Holmes)는 잠복 수사에 착수하기 전에 용의자를 잡을 확률이 정확히 3분의 2라고 발표합니다. 이것은 당혹스럽게 들립니다. 인간 행동과 관련된 어떤 것이 어떻게 수학적으로 정확한 확률로 귀속될 수 있을까요?

알고 보니 홈즈는 용의자의 겉보기에 불규칙한 관찰된 행동을 야기한 근본적인 일련의 사건들을 연역적으로 추론했습니다. 용의자는 런던과 그 주변의 다른 사람들이 소유한 여섯 개의 나폴레옹 흉상 중 하나 안에 숨겨져 있다고 알려진 보석을 체계적으로 찾고 있었습니다. 세부 사항은 그리 중요하지 않지만, 핵심은 용의자도 형사도 여섯 개의 흉상 중 어느 것에 보석이 있는지 모른다는 것과 용의자의 행동에 대한 다른 모든 것이 완전히 예측 가능하다고 (가정된다는) 것입니다. 따라서 정확하게 정량화 가능한 불확실성이 발생합니다.

요점은 우리가 의존할 수 있는 세상의 모델을 가지고 있다면, 과거 관찰에 의존하지 않고도 논리적 연역(logical deduction)을 통해 위험을 추정할 수 있다는 것입니다. 물론, 허구적인 시나리오 밖에서는 세상이 그렇게 깔끔하지 않습니다. 특히 먼 미래를 예측하고자 할 때는 더욱 그렇습니다.

실존적 위험(x-risk)에 관해서는 연역적 모델(deductive models)이 없다는 일반적인 규칙에 대한 흥미로운 예외가 있습니다. 바로 소행성 충돌입니다. 귀납적(inductive) 및 연역적 위험 추정(deductive risk estimation)의 조합은 우리가 실존적 위험(x-risk)의 확률을 추정할 수 있게 해주는데, 이는 우리가 순전히 물리적인 시스템에 대해 이야기하고 있기 때문입니다. 이 방법이 어떻게 작동하는지 잠시 살펴보겠습니다. 왜냐하면 이 방법이 다른 유형의 실존적 위험(x-risk)에 일반화될 수 없다는 것을 인식하는 것이 중요하기 때문입니다. 핵심은 소행성의 크기(더 정확하게는 충돌 에너지)와 충돌 빈도 간의 관계를 모델링할 수 있다는 것입니다. 수천 개의 작은 충돌을 관찰했기 때문에, 우리는 직접 관찰된 적이 없는 큰 충돌의 빈도를 추론하기 위해 외삽할 수 있습니다. 우리는 또한 전 세계적인 재앙을 초래할 임계값을 추정할 수 있습니다.

1 그림: 작은 소행성 충돌 데이터(왼쪽 그림)는 멸종 수준의 충돌(오른쪽)로 외삽될 수 있습니다.

AI의 경우, 미지의 요소는 물리적 시스템보다는 기술 발전 및 거버넌스(governance)와 관련되어 있으므로, 이를 수학적으로 모델링하는 방법이 명확하지 않습니다. 그럼에도 불구하고 사람들은 시도해 왔습니다. 예를 들어, 가상의 일반 인공지능(AGI)의 계산 요구 사항을 예측하기 위해, 여러 연구에서는 AI 시스템이 인간 두뇌와 거의 동일한 양의 계산을 필요로 할 것이라고 가정하고, 나아가 인간 두뇌에 필요한 계산량에 대한 가정을 합니다. 이러한 가정들은 소행성 모델링에 관련된 가정들보다 훨씬 더 미약하며, 이 중 어느 것도 통제 상실 문제를 다루지 않습니다. 최근에는 AI 모델의 규모와 능력 간의 관계를 연역적으로 추론하려는 시도가 있었으나, 이는 여전히 AI의 복잡성과 예측 불가능성을 완전히 포착하지 못합니다.

### 주관적 확률(subjective probabilities)은 숫자로 위장한 감정입니다

참조 집단(reference class)이나 근거 있는 이론이 없다면, 예측은 필연적으로 "주관적 확률(subjective probabilities)", 즉 예측가의 판단에 기반한 추측이 됩니다. 놀랍게도, 이들은 자릿수(orders of magnitude)만큼 차이가 납니다. 주관적 확률 추정(subjective probability estimation)은 확률 추정치에 대한 귀납적(inductive) 또는 연역적(deductive) 근거를 가질 필요성을 회피하지 못합니다. 단지 예측가가 자신의 추정치를 설명할 필요성을 피할 뿐입니다. 설명은 인간이 귀납적, 연역적 또는 그 조합이든 직관적인 추론을 설명하는 능력이 제한적이기 때문에 어려울 수 있습니다. 본질적으로, 이는 예측가에게 다음과 같이 말할 수 있게 합니다: "내 방법을 보여주지는 않았지만, 내 실적 때문에 이 추정치를 신뢰할 수 있습니다" (다음 섹션에서 AI 실존적 위험(AI x-risk) 예측의 경우에도 이것이 왜 무너지는지 설명합니다). 그러나 궁극적으로 귀납적 또는 연역적 근거가 없다면, 예측가들이 할 수 있는 일은 숫자를 만들어내는 것뿐이며, 그 만들어진 숫자들은 제각각입니다.

2022년 말 예측 연구소(Forecasting Research Institute)에서 실시한 실존적 위험 설득 토너먼트(Existential Risk Persuasion Tournament, XPT)를 고려해 봅시다. 우리는 이것이 현재까지 수행된 실존적 위험(x-risk) 예측 연습 중 가장 정교하고 잘 실행된 것이라고 생각합니다. 여기에는 AI 전문가와 예측 전문가("슈퍼 예측가(superforecasters)" 그림 참조)를 포함한 다양한 예측가 그룹이 참여했습니다. AI 전문가의 경우, 2100년까지 AI 멸종 위험에 대한 추정치의 상위 75%는 12%, 중앙값은 3%, 하위 25%는 0.25%였습니다. 예측 전문가의 경우, 상위 75%조차도 1%에 불과했고, 중앙값은 겨우 0.38%였으며, 하위 25%는 그래프에서 시각적으로 0과 구별할 수 없었습니다. 다시 말해, AI 전문가의 75% 예측과 슈퍼 예측가(superforecaster)의 25% 예측은 최소 100배 이상 차이가 납니다. 이 모든 추정치는 해당 주제에 대한 깊은 전문 지식을 가지고 있으며 몇 달 동안 서로를 설득하려 노력한 토너먼트에 참여한 사람들로부터 나온 것입니다!

여기서 예측 범위가 충분히 극단적이지 않다면, 이 전체 연습이 한 시점에 한 그룹에 의해 수행되었다는 점을 명심하십시오. 토너먼트가 오늘 반복되거나 질문이 다르게 구성되었다면 다른 숫자를 얻을 수도 있습니다. 가장 중요한 것은 예측가들이 제공한 근거를 살펴보는 것인데, 이는 보고서에 상세히 설명되어 있습니다. 그들은 특히 강력한 AI 개발 시 나쁜 결과의 가능성에 대해 생각할 때 정량적 모델을 사용하지 않습니다. 대부분의 경우, 예측가들은 초지능 AI(superintelligent AI)에 대해 논의할 때 일반 사람들이 하는 것과 같은 종류의 추측에 참여하고 있습니다. 아마도 AI는 시스템 운영자에 대한 초인적인 설득을 통해 중요한 시스템을 장악할 것입니다. 아마도 AI는 컴퓨터가 더 빨리 작동하는 데 도움이 되므로 지구 온도를 낮추려고 시도하다가 실수로 인류를 전멸시킬 것입니다. 아니면 AI는 지구 대신 우주에서 자원을 찾을 것이므로 우리는 그렇게 걱정할 필요가 없을 것입니다. 그러한 추측에 잘못된 것은 없습니다. 그러나 AI 실존적 위험(AI x-risk)에 관해서는 예측가들이 당신이나 우리 또는 다른 누구의 직감보다 그들의 직감을 더 신뢰할 수 있게 만드는 특별한 지식, 증거 또는 모델에 의존하고 있지 않다는 것을 분명히 해야 합니다.

"슈퍼 예측(superforecasting)"이라는 용어는 필립 테틀록(Philip Tetlock)의 20년간의 예측 연구에서 비롯되었습니다(그는 또한 XPT의 주최자 중 한 명이었습니다). 슈퍼 예측가(superforecasters)는 다양한 정보를 통합하고 심리적 편향을 최소화하는 등 예측을 개선하는 방법에 대해 훈련받는 경향이 있습니다. 이러한 방법은 지정학(geopolitics)과 같은 영역에서 효과적인 것으로 나타났습니다. 그러나 활용할 유용한 증거가 많지 않다면 아무리 훈련해도 좋은 예측으로 이어지지 않을 것입니다. 예측가들이 신뢰할 수 있는 정량적 모델을 가지고 있다고 하더라도(그들은 그렇지 않습니다), 그들은 "알려지지 않은 미지의 것(unknown unknowns)", 즉 모델 자체가 틀릴 가능성을 고려해야 합니다. 저명한 실존적 위험(x-risk) 철학자 닉 보스트롬(Nick Bostrom)이 설명하듯이: "우리의 1차 위험 평가의 불확실성과 오류 가능성은 우리가 모든 것을 고려한 확률 할당에 포함시켜야 하는 요소입니다. 이 요소는 종종 낮은 확률, 높은 결과의 위험 — 특히 잘 이해되지 않는 자연 현상, 복잡한 사회 역학, 또는 새로운 기술과 관련된 위험, 또는 다른 이유로 평가하기 어려운 위험 — 에서 지배적입니다."

이것은 합리적인 관점이며, AI 실존적 위험(AI x-risk) 예측가들은 위험 평가의 불확실성에 대해 많이 걱정합니다. 그러나 이 원칙을 따르는 사람들에게는 예측이 모델의 결과라기보다는 추측이 될 수밖에 없다는 것이 한 가지 결과입니다. 결국, 모델 자체가 틀릴 확률이나 모델이 틀렸을 경우의 위험을 추정하는 데 어떤 모델도 사용될 수 없기 때문입니다. 최근의 AI 모델 능력 급증은 이러한 불확실성을 더욱 가중시키고 있으며, 예측의 신뢰성에 대한 의문을 심화시키고 있습니다.

### 독특하거나 희귀한 사건의 경우 예측 능력은 측정할 수 없습니다

요약하자면, 주관적인 AI 위험 예측은 자릿수(orders of magnitude)만큼 차이가 납니다. 그러나 예측가들의 실적을 측정할 수 있다면, 이를 사용하여 어떤 예측가를 신뢰해야 할지 알아낼 수 있을 것입니다. 위험 추정치를 정당화하는 이전 두 가지 접근 방식(귀납적 및 연역적)과 달리, 예측가는 자신의 추정치를 설명할 필요가 없지만, 대신 과거에 다른 결과를 예측하는 데 입증된 능력에 기반하여 정당화합니다.

이것은 지정학적 사건 영역에서 매우 귀중한 것으로 입증되었으며, 예측 커뮤니티는 능력 측정에 많은 노력을 기울입니다. 보정(calibration), 브라이어 점수(Brier score), 로그 점수(logarithmic score), 또는 예측 경쟁 웹사이트 메타큘러스(Metaculus)에서 사용되는 피어 점수(Peer score)와 같이 예측 능력을 평가하는 많은 방법이 존재합니다. 그러나 어떤 방법을 사용하든, 실존적 위험(existential risk)에 관해서는 주관적 확률(subjective probabilities)에 대한 예측 능력을 평가하는 데 많은 장벽이 있습니다: 참조 집단(reference class)의 부족, 낮은 기본 발생률(base rate), 그리고 긴 시간 지평(time horizon)입니다. 각각을 차례로 살펴보겠습니다.

참조 집단(reference class) 문제가 예측가를 괴롭히는 것처럼, 평가자에게도 영향을 미칩니다. 외계인 착륙 예시로 돌아가 봅시다. 선거 예측에서 매우 정확하다고 입증된 예측가를 생각해 봅시다. 이 예측가가 아무런 증거 없이 1년 안에 외계인이 지구에 착륙할 것이라고 발표한다고 가정해 봅시다. 예측가의 입증된 능력에도 불구하고, 이것은 외계인 착륙에 대한 우리의 믿음을 업데이트하게 만들지 않을 것입니다. 왜냐하면 그것은 선거 예측과 너무나 다르며, 우리는 예측가의 능력이 일반화될 것이라고 기대하지 않기 때문입니다. 마찬가지로, AI 실존적 위험(AI x-risk)은 예측된 과거의 어떤 사건과도 너무나 달라서 AI 실존적 위험(AI x-risk)을 추정하는 예측가의 능력에 대한 증거가 없습니다.

참조 집단(reference class) 문제를 어떻게든 해결하더라도 다른 문제들이 남아 있습니다. 특히, 멸종 위험이 "꼬리 위험(tail risks)", 즉 희귀한 사건으로 인해 발생하는 위험이라는 사실입니다. 예측가 A가 AI 실존적 위험(AI x-risk)의 확률이 1%라고 말하고, 예측가 B가 100만분의 1이라고 말한다고 가정해 봅시다. 어떤 예측을 더 신뢰해야 할까요? 우리는 그들의 실적을 살펴볼 수 있습니다. 예측가 A(AI 실존적 위험(AI x-risk)에 1%의 확률을 할당한)가 더 나은 실적을 가지고 있다고 가정해 봅시다. 그렇다고 해서 우리가 A의 예측을 더 신뢰해야 한다는 의미는 아닙니다. 왜냐하면 능력 평가는 꼬리 위험(tail risks)의 과대평가에 둔감하기 때문입니다. 다시 말해, A가 발생할 상당한 확률을 가진 일상적인 사건에 관해서는 B보다 약간 더 잘 보정(calibrated)되어 있기 때문에 전반적으로 더 높은 점수를 받을 수 있지만, 드물게 발생하는 꼬리 위험(tail risks)(예를 들어, 100만분의 1의 확률을 가진)을 자릿수(orders of magnitude)만큼 과대평가하는 경향이 있을 수 있습니다. 어떤 점수 규칙도 이러한 유형의 오보정(miscalibration)을 적절하게 처벌하지 않습니다.

이것이 왜 사실인지 보여주는 사고 실험이 있습니다. 두 예측가 F와 G가 두 가지 다른 사건 집합을 예측하고, 두 집합의 사건에 대한 "진정한" 확률이 0과 1 사이에 균일하게 분포되어 있다고 가정해 봅시다. 우리는 매우 낙관적으로, F와 G 모두가 예측하는 모든 사건 e에 대한 진정한 확률 P[e]를 알고 있다고 가정합니다. F는 항상 P[e]를 출력하지만, G는 약간 보수적이어서 1% 미만의 값을 예측하지 않습니다. 즉, G는 P[e] >= 1%이면 P[e]를 출력하고, 그렇지 않으면 1%를 출력합니다. 구성상 F가 더 나은 예측가입니다. 그러나 이것이 그들의 실적에서 명백할까요? 다시 말해, F가 G보다 더 높은 점수를 받을 확률이 95%가 되려면 각각의 예측을 얼마나 많이 평가해야 할까요? 로그 점수 규칙(logarithmic scoring rule)을 사용하면 약 1억 개 정도가 됩니다. 브라이어 점수(Brier score)를 사용하면 약 1조 개 정도가 됩니다.

2 우리는 여기서의 가정에 대해 논쟁할 수 있지만, 요점은 예측가가 꼬리 위험(tail risks)을 체계적으로 과대평가한다면, 그것은 단순히 경험적으로 감지할 수 없다는 것입니다. 이러한 문제점은 AI x-risk와 같은 극단적인 사건에 대한 예측의 유효성에 심각한 의문을 제기합니다.

실존적 위험(x-risk) 예측 능력 평가의 마지막 장벽은 장기 예측은 평가하는 데 너무 오랜 시간이 걸린다는 것입니다(그리고 멸종 예측은 물론 평가 자체가 불가능합니다). 이것은 잠재적으로 극복될 수 있습니다. 연구자들은 상호 점수 매기기(reciprocal scoring)라는 방법을 개발했습니다. 이는 예측가들이 서로의 예측을 얼마나 잘 예측하는지에 따라 보상을 받는 방식이며, 코비드-19 정책의 효과를 예측하는 것과 같은 일부 실제 환경에서 이를 검증했습니다. 이러한 환경에서 상호 점수 매기기(reciprocal scoring)는 전통적인 점수 매기기 방법만큼 좋은 예측을 산출했습니다. 좋습니다. 그러나 상호 점수 매기기(reciprocal scoring)는 참조 집단(reference class) 문제나 꼬리 위험(tail risk) 문제를 우회하는 방법은 아닙니다.

지금까지의 우리 주장을 요약하면, 세 가지 예측 방법 중 어느 것도 AI 실존적 위험(AI x-risk)에 대한 신뢰할 수 있는 추정치를 산출할 수 없는 이유를 보여줍니다.

### 위험 추정치가 체계적으로 부풀려질 수 있는 여러 가지 이유

요약하자면, 귀납적(inductive) 및 연역적(deductive) 방법은 작동하지 않고, 주관적 예측은 제각각이며, 어떤 예측이 더 신뢰할 수 있는지 알 방법이 없습니다. 그래서 정책에 잠재적으로 정보를 제공할 수 있는 더 신뢰할 수 있는 추정치를 도출하기 위해, 일부 연구자들은 여러 예측가의 예측을 결합하는 예측 집계 방법(forecast aggregation methods)으로 눈을 돌렸습니다. 주목할 만한 노력은 AI 발전 설문조사(AI Impacts Survey on Progress in AI)이지만, 이는 무응답 편향(non-response bias)을 포함한 심각한 방법론적 한계로 비판받아 왔습니다. 더 중요한 것은, 집계가 예측 정확도를 왜 개선해야 하는지 불분명하다는 것입니다. 결국, 대부분의 예측가들은 동일한 편향을 공유할 수 있으며(다시 말하지만, 그들 중 누구도 신뢰할 수 있는 예측의 근거를 가지고 있지 않습니다).

예측가들이 AI 실존적 위험(AI x-risk)을 체계적으로 과대평가할 수 있는 여러 가지 이유가 있습니다. 3 첫 번째는 선택 편향(selection bias)입니다. AI 연구자들을 예로 들어 봅시다. AI가 세상을 바꿀 수 있다는 믿음은 AI 연구자가 되는 주요 동기 중 하나입니다. 그리고 일단 이 커뮤니티에 들어서면, 그 메시지가 끊임없이 강화되는 환경에 놓이게 됩니다. 그리고 이 기술이 엄청나게 강력하다고 믿는다면, 그 세상 변화 효과가 긍정적이기보다는 부정적일 심각한 가능성이 있다고 생각하는 것은 완벽하게 합리적입니다. 그리고 다소 고립된 AI 안전 하위 커뮤니티에서는 에코 챔버(echo chamber)가 귀청이 터질 듯할 수 있습니다. 높은 p(doom)(AI 파멸 확률에 대한 자신의 추정치)를 가지고 있다고 주장하는 것은 자신의 정체성과 대의에 대한 헌신을 알리는 방법이 된 것 같습니다.

예측 전문가의 경우 약간 다른 선택 편향(selection bias)이 작용합니다. 예측 커뮤니티는 효과적 이타주의(effective altruism) 및 실존적 위험(existential risk), 특히 AI 위험에 대한 우려와 강한 중첩을 이룹니다. 이것이 개별 예측가들이 편향되어 있다는 의미는 아닙니다. 그러나 높은 p(doom)을 가지고 있으면 예측 활동에 더 참여하고 싶어질 수 있습니다. 따라서 커뮤니티 전체는 실존적 위험(x-risk)에 대한 우려를 가진 사람들에게 편향되어 있을 가능성이 높습니다.

예측가들은 증거에 반응하여 자신의 신념을 업데이트하는 데 능숙하지만, 문제는 예를 들어 소행성 충돌 위험과 달리 AI 실존적 위험(AI x-risk)에 관해서는 어떤 식으로든 자신의 신념을 바꿀 수 있는 증거가 거의 없다는 것입니다. 따라서 우리는 예측이 사람들이 커뮤니티에 들어올 때 가지고 있던 사전 신념(priors)에 강하게 영향을 받는다고 의심합니다. XPT 보고서는 "가장 적극적인 참가자들 사이에서도, 그리고 다른 사람들을 설득하기 위한 금전적 인센티브에도 불구하고, XPT 동안 마음이 바뀐 사람은 거의 없었다"고 언급합니다. 후속 연구에서 그들은 많은 의견 불일치가 AI를 넘어선 근본적인 세계관 차이 때문이라는 것을 발견했습니다.

다시 강조하자면, 편향에 대한 우리의 주장은 AI 실존적 위험(AI x-risk)에만 해당됩니다. 만약 체계적으로 편향된(예를 들어, 현직자에게 편향된) 선거 예측가 커뮤니티가 있었다면, 몇 번의 선거 후 예측과 현실을 비교할 때 이것은 명백해질 것입니다. 그러나 AI 실존적 위험(AI x-risk)의 경우, 이전 섹션에서 보여주었듯이, 능력 평가는 꼬리 위험(tail risks)의 과대평가에 둔감합니다. 흥미롭게도, 능력 평가는 꼬리 위험(tail risks)의 과소평가에 극도로 민감합니다. 실제로 발생하는 희귀한 사건에 대해 0의 확률을 할당하면, 로그 점수 규칙(logarithmic scoring rule)에 따라 무한한 페널티를 받게 되며, 다른 사건을 얼마나 잘 예측했는지와 관계없이 결코 회복할 수 없습니다. 이것은 로그 점수(logarithmic score)의 주요 이점 중 하나로 간주되며, 메타큘러스(Metaculus)에서 채택되는 이유입니다.

이제 정확한 추정치를 가지고 있지 않은 예측가를 생각해 봅시다. 그리고 AI 실존적 위험(AI x-risk)처럼 불확실성의 축이 많은 것에 대해 정확한 추정치를 가진 예측가는 분명히 없을 것입니다. 비대칭적인 페널티를 고려할 때, 합리적인 행동은 추정 범위의 상한선을 선택하는 것입니다. 4 어쨌든, 예측가들이 추정치가 매우 불확실할 때 실제로 무엇을 보고하는지는 명확하지 않습니다. 아마도 그들은 점수 함수의 인센티브에 반응하지 않을 것입니다. 결국, 장기 예측은 곧 해결되지 않을 것입니다. 그리고 XPT의 경우, 인센티브는 실제로 긴 시간 지평(time horizons) 문제를 우회하기 위해 서로의 예측을 예측하는 것이었음을 상기하십시오. 상호 점수 매기기(reciprocal scoring) 논문은 이것이 예측가들이 진정성 있고 노력이 많이 들어간 추정치를 제출하도록 유도할 것이라고 주장하며, 이 주장에 대한 다양한 반론을 고려합니다. 그들의 방법론 방어는 두 가지 핵심 가정에 기반합니다: 예측가들이 더 많은 노력을 기울임으로써 진정한 추정치에 더 가까워질 수 있다는 것과, 다른 예측가들이 무엇을 할지 예측할 더 나은 방법이 없다는 것입니다.

이러한 가정이 충족되지 않으면 어떻게 될까요? 이 게시물 전체에서 주장했듯이, AI 실존적 위험(AI x-risk)의 경우, 증거가 예측가들의 사전 신념(prior beliefs)을 바꿀 것이라고 기대해서는 안 되므로 첫 번째 가정은 의심스럽습니다. 그리고 XPT의 한 번의 반복이 끝난 지금, 그 토너먼트에서 발표된 중앙값 추정치는 강력한 앵커(게임 이론의 "초점(focal point)") 역할을 합니다. 미래에 상호 점수 매기기(reciprocal scoring) 인센티브를 가진 예측가들이 기존 중앙값 예측을 시작점으로 사용하여, 지난 토너먼트 이후에 얻을 수 있는 새로운 정보를 반영하기 위해 사소한 조정만 할 가능성이 있습니다. 기존 추정치가 미래 추정치의 앵커 역할을 함에 따라 추정 범위가 좁아질 수 있습니다. 이 모든 것은 돌려서 말하는 것입니다: 활용할 실제 증거가 적을수록 집단 사고(groupthink)의 위험이 커집니다. AI 커뮤니티 내에서 특정 관점이 고착화되는 현상은 2024년 이후 더욱 두드러졌으며, 이는 예측의 다양성과 독립성을 저해할 수 있습니다.

### 파스칼의 내기(Pascal’s wager)를 경계하라: 효용 극대화(utility maximization)의 위험

참고로, AI로 인한 멸종 위험과 멸종에 이르지 않는 재앙적 위험에 대한 XPT의 중앙값 추정치는 다음과 같습니다:

다시 말하지만, 우리는 이 숫자들을 너무 심각하게 받아들여서는 안 된다는 견해입니다. 이들은 참가자들의 다양한 샘플이 다른 어떤 것보다 AI에 대해 얼마나 걱정하는지를 반영합니다. 이전과 마찬가지로, 예측 전문가(superforecasters)와 AI 전문가의 추정치는 자릿수(order of magnitude) 이상 차이가 납니다. 우리가 이러한 추정치를 어느 정도 신뢰한다면, AI 전문가의 추정치보다는 예측 전문가의 추정치여야 합니다. 과거 연구에서 얻은 중요한 통찰 중 하나는, 다양한 정보를 통합하고 심리적 편향을 최소화하는 훈련을 받은 예측 전문가보다 특정 분야 전문가(domain experts)가 더 나쁜 성과를 보인다는 것입니다. 그럼에도 불구하고, 위에서 말했듯이, 그들의 예측조차도 엄청난 과대평가일 수 있으며, 우리는 확실히 알 수 없습니다.

그래서 뭐가 문제일까요? 정책 입안자들이 특정 기간 동안의 위험이 0.01%가 아니라 1%라고 믿는다면 어떨까요? 어느 경우든 꽤 낮아 보입니다! 그것은 그들이 그 확률을 가지고 무엇을 하는지에 달려 있습니다. 대부분의 경우, 이러한 추정치는 단순히 일부 전문가 그룹이 위험이 중요하다고 생각한다는 사실을 알리는 방법일 뿐입니다. 그것이 전부라면, 그렇게 되십시오. 그러나 다른 사람들이 같은 숫자를 극도로 다르게 해석한다는 점을 고려할 때, 이러한 정량화에 대한 정교한 노력이 이러한 신호 전달 목적에 도움이 되는지조차 명확하지 않습니다. 예를 들어, 연방거래위원회(Federal Trade Commission) 위원장 리나 칸(Lina Khan)은 자신의 p(doom)이 15%에 불과했기 때문에 이 문제에 대한 자신의 견해가 기술 낙관적(techno-optimistic)이라고 말했는데, 이는 전문가들을 당혹스럽게 했습니다. (참고로, 그 숫자는 우리가 기술 낙관론자라고 부르기에 편안하게 생각하는 것보다 약 천 배 더 높습니다.)

의사 결정에서 매우 작거나 매우 큰 숫자를 정확하게 정신적으로 처리하고, 단순히 "무의미하게 작은"과 같은 범주로 묶지 않으려면 많은 정량적 훈련이 필요합니다. 대부분의 사람들은 이런 식으로 훈련받지 않습니다. 요컨대, 전문가들의 모호한 직관과 두려움이 의사-정확한 숫자(pseudo-precise numbers)로 번역된 다음, 정책 입안자들에 의해 다시 모호한 직관과 두려움으로 번역되는 것처럼 보입니다. 정량화의 허세를 그만둡시다! AI 안전 센터(Center for AI Safety)의 AI 위험 성명(Statement on AI Risk)은 이 점에서 놀랍도록 솔직했습니다(물론, 우리는 그 내용에 강력히 동의하지 않습니다).

의사 결정에서 확률을 사용하는 원칙적이고 정량적인 방법은 비용-편익 분석(cost-benefit analysis)을 통한 효용 극대화(utility maximization)입니다. 아이디어는 간단합니다. 어떤 결과가 주관적인 가치, 즉 효용(utility) U(양수 또는 음수일 수 있음)를 가지고 있고, 예를 들어 발생 확률이 10%라면, 우리는 그것이 확실히 발생하고 0.1 * U의 가치를 가진다고 가정하고 행동할 수 있습니다. 그런 다음 우리가 사용할 수 있는 각 옵션에 대한 비용과 편익을 합산하고, 비용에서 편익을 뺀 값(즉, "기대 효용(expected utility)")을 최대화하는 옵션을 선택할 수 있습니다.

여기서부터 문제가 정말 심각해집니다. 첫째, 어떤 사람들은 멸종이 헤아릴 수 없을 정도로 큰 부정적인 가치를 가진다고 생각할 수 있습니다. 왜냐하면 그것은 미래에 태어날 수 있는 모든 인간 생명, 물리적이든 시뮬레이션된 것이든, 존재를 배제하기 때문입니다. 논리적인 결론은 실존적 위험(x-risk)이 항상 모든 사람의 최우선 과제가 되어야 한다는 것입니다! 이는 파스칼의 내기(Pascal’s wager)를 연상시킵니다. 신이 존재할 가능성이 극히 작더라도, 불신앙의 대가는 무한하며(영원한 행복과 대비되는 지옥에서의 영원), 따라서 기대 효용(expected utility)도 무한하다는 주장입니다. 다행히 정책 입안자들은 무한을 포함하는 의사 결정 프레임워크에 너무 많은 신뢰를 주지 않습니다. 그러나 이 아이디어는 AI 안전 커뮤니티를 강력하게 사로잡았고, 일부 사람들에게 AI 실존적 위험(AI x-risk)이 사회의 최우선 과제가 되어야 한다는 확신을 심어줍니다.

재앙적이지만 실존적이지 않은 위험으로 스스로를 제한하더라도, 우리는 수십억 명의 생명이 위태로운 상황에 대해 이야기하고 있으므로, 심지어 1%의 위험에 대한 기대 비용(expected cost)도 너무 높아서 정책적 함의가 극단적입니다. 정부는 AI 실존적 위험(AI x-risk) 완화에 대한 지출을 자릿수(orders of magnitude)만큼 늘리고 AI 개발 중단과 같은 가혹한 조치를 고려해야 합니다. 이것이 이러한 추정치들이 어떤 방법론으로도 뒷받침되지 않는다는 것을 이해하는 것이 매우 중요한 이유입니다. 이렇게 적은 증거에 기반하여 세상을 바꾸는 정책 결정을 내리는 것은 엄청나게 현명하지 못한 일일 것입니다. 2024년 이후, AI 개발 속도 조절 및 일시 중단을 주장하는 목소리가 커졌는데, 이러한 주장의 근거가 되는 예측은 여전히 견고한 증거 기반을 가지고 있지 못합니다.

당신은 우리의 책에 대한 블로그인 AI Snake Oil을 읽고 있습니다. 새 게시물을 받으려면 구독하세요. 구독

### 이정표 예측은 결과 모호성(outcome ambiguity)으로 인해 어려움을 겪습니다

AI 정책에서 예측의 역할이 있을까요? 우리는 그렇다고 생각합니다. 단지 실존적 위험(existential risk)을 예측하는 것이 아닐 뿐입니다. 특정 능력 벤치마크(capability benchmarks) 또는 경제적 영향에 대한 성과와 같은 AI 이정표를 예측하는 것이 더 달성 가능하고 의미 있습니다. 예측가가 다양한 AI 이정표가 언제 도달할지 예측하는 데 능숙함을 입증했다면, 이는 그들이 미래에도 잘할 것이라는 증거를 제공합니다. 우리는 더 이상 독특하거나 희귀한 사건에 대해 이야기하는 것이 아닙니다. 그리고 살인 로봇을 막는 것보다 잠재적인 경제적 혼란에 대비하는 것과 같은 낮은 이해관계의 정책 개입을 고려할 때, 예측이 모든 합리적인 사람의 만족을 위해 정당화되어야 하는 중요성은 덜합니다.

예측 커뮤니티는 이정표 예측에 많은 노력을 기울입니다. 메타큘러스(Metaculus)에서 "2040년 이전에 인간-기계 지능 동등성(Human-machine intelligence parity)이 달성될 것인가?"라는 질문에 대해 1,300명 이상의 예측가들이 96%의 집계 예측을 내놓았습니다. 놀랍습니다! 우리가 이 예측에 동의한다면, 우리는 일반 인공지능(AGI)으로의 안전한 전환을 관리하는 것이 전 세계적인 우선순위가 되어야 한다는 입장을 지지할 것입니다. 왜 우리는 그렇지 않을까요?

답은 세부 사항에 있습니다. 일반 인공지능(AGI)과 같은 모호한 개념의 정의에 대한 합의가 없습니다. 정의를 고정하더라도, 그것이 달성되었는지 여부를 결정하는 것은 어렵거나 불가능할 수 있습니다. 효과적인 예측을 위해서는 모호한 결과(ambiguous outcomes)를 피하는 것이 극히 중요합니다. 예측 커뮤니티가 이 문제를 해결하는 방법은 시험 성적과 같은 비교적 좁은 기술 측면에서 정의하는 것입니다. 메타큘러스(Metaculus)의 지능 동등성 질문은 수학, 물리학, 컴퓨터 과학 분야의 대학원 시험 성적 측면에서 정의됩니다. 이 정의에 기반하여, 우리는 96%의 예측에 동의합니다. 그러나 우리는 그 정의가 너무 희석되어 정책에 큰 의미가 없다고 생각합니다. 실존적 위험(existential risk)은 잊어버리십시오. 이전에 썼듯이, 시험에서의 AI 성능은 구성 타당도(construct validity)가 너무 낮아서 AI가 노동자를 대체할지조차 예측할 수 없습니다. 다른 벤치마크(benchmarks)도 크게 다르지 않습니다. 요컨대, 벤치마크와 실제 영향 사이의 큰 격차 때문에 AI 능력 타임라인을 예측하는 것은 까다롭습니다.

다행히도, 실제 중요한 작업을 반영하는 더 나은 벤치마크(benchmarks)가 개발되고 있습니다. 벤치마크(benchmarks) 외에도, 비용이 더 많이 들더라도 자연주의적 평가(naturalistic evaluation)가 필요합니다. 자연주의적 평가(naturalistic evaluation)의 한 유형은 사람들이 AI 지원을 받아 자신의 업무를 어떻게 다르게 수행하는지 측정하는 것입니다. 노동 시장 변화나 군대의 AI 관련 지출과 같은 경제적, 사회적, 정치적 영향을 직접 예측하는 것이 더 유용할 수 있지만, 명확하게 정의하고 측정하기는 더 어렵습니다. 최근에는 특정 산업 분야에서의 AI 도입률, 생산성 향상 지표, 혹은 AI로 인한 새로운 직업 창출 및 소멸률 등을 예측하려는 시도가 늘어나고 있습니다.

### 결론

정책에서 확률의 오용을 피할 책임은 정책 입안자에게 있습니다. 우리는 정책 입안자들이 오도되는 것을 "보호"하기 위해 예측가들에게 예측 발표를 중단하라고 요구하는 것이 아닙니다. 그렇지만, 우리는 예측이 사용된 과정과 고려된 증거에 대한 명확한 설명과 함께 제공되어야 한다고 생각합니다. 이것은 정책 입안자들이 제시된 정당성이 그들이 편안하게 받아들일 수 있는 기준을 충족하는지 여부에 대해 정보에 입각한 결정을 내릴 수 있도록 할 것입니다. XPT는 투명성의 좋은 예이며, 이 논문도 마찬가지입니다(비록 실존적 위험(x-risk)에 관한 것은 아니지만). 반면에, 단순히 많은 연구자들을 설문조사하고 집계된 숫자를 제시하는 것은 오해의 소지가 있으며 정책 입안자들은 이를 무시해야 합니다.

그렇다면 정부는 AI 실존적 위험(AI x-risk)에 대해 무엇을 해야 할까요? 우리의 견해는 아무것도 하지 말아야 한다는 것이 아닙니다. 그러나 그들은 실존적 위험(x-risk)을 시급하고 심각하게 본다면 설득력 있게 보일 수 있는 종류의 정책, 특히 AI 개발 제한과 같은 정책을 거부해야 합니다. 이 시리즈의 향후 에세이에서 주장하겠지만, 그러한 정책은 불필요할 뿐만 아니라 실존적 위험(x-risk)을 증가시킬 가능성이 높습니다. 대신, 정부는 AI 위험에 대한 다양한 가능한 추정치와 양립할 수 있으며, 위험이 미미하더라도 전반적으로 도움이 되는 정책을 채택해야 합니다. 예를 들어, 강력한 AI 윤리 규범 확립, AI 연구의 안전성 및 책임성 강화, 국제 협력을 통한 AI 거버넌스 프레임워크 구축 등이 있습니다. 정부는 또한 새로운 증거에 더 잘 반응하도록 정책 결정 과정을 변경해야 합니다. 이 모든 것에 대한 자세한 내용은 곧 알려드리겠습니다.

### 추가 자료

XPT 보고서의 제목은 『실존적 위험 예측: 장기 예측 토너먼트의 증거(Forecasting Existential Risks: Evidence from a Long-Run Forecasting Tournament)』입니다. 테틀록(Tetlock)과 가드너(Gardner)의 책 『슈퍼 예측(Superforecasting)』은 테틀록, 바바라 멜러스(Barbara Mellers) 등의 연구를 요약합니다. 스콧 알렉산더(Scott Alexander)는 독특한 사건에 확률을 부여하는 것이 원칙적으로 잘못되었다는 주장을 반박합니다(우리는 대체로 동의합니다). 우리의 주장은 그가 다루는 입장과 두 가지 주요 방식으로 다릅니다. 우리는 일반적인 예측에 대해 이야기하는 것이 아니라, 정책 수립에 대한 적용에 대해서만 이야기합니다. 그리고 우리는 원칙적으로 반대하지 않습니다. 우리의 주장은 경험적입니다. AI 실존적 위험(AI x-risk) 예측은 극도로 신뢰할 수 없으며 정당성이 부족합니다. 이론적으로 이것은 미래에 바뀔 수 있지만, 우리는 크게 기대하지 않습니다. 프리드먼(Friedman)과 주크하우저(Zuckhauser)의 논문은 확률이 전부가 아닌 이유를 설명합니다. 동일한 위험 추정치를 가진 두 예측은 정책 입안자들에게 매우 다른 함의를 가질 수 있습니다. 우리의 견해로는, AI 실존적 위험(AI x-risk) 예측은 신뢰의 세 가지 차원 중 두 가지에서 좋지 않은 성과를 보입니다: 증거에 대한 견고한 기반과 합리적인 의견의 좁은 범위. 샤제다 아메드(Shazeda Ahmed)가 이끄는 프린스턴 CITP 동료들의 논문은 "지식 생산 및 커뮤니티 구축의 응집력 있고 상호 연결된 사회 구조"로 구성된 AI 안전의 인식론적 문화(epistemic culture)를 설명합니다. 이는 동료 심사(peer review)와 같은 관행을 중심으로 하는 더 넓은 과학 커뮤니티와 달리, 예측과 같은 관행이 AI 안전 커뮤니티가 신념을 형성하는 방식의 기둥이 된 이유를 이해하는 데 도움이 됩니다. 물론, 우리는 어떤 견해가 과학적 정통성에 부합하지 않는다는 이유만으로 거부해서는 안 됩니다. 그러나 동시에, 우리는 스스로를 AI 안전 커뮤니티라고 칭하는 집단의 AI 안전에 대한 견해에 어떤 존중도 표해서는 안 됩니다. AI 안전 커뮤니티의 일반적인 구성원이 AI 안전에 대해 특정 입장을 가지고 있다는 것을 이해하는 것이 중요합니다. 이 입장은 매우 논쟁의 여지가 많고, 우리의 견해로는 다소 과장된 것입니다. 우리는 증거 기반 AI 안전에 대해 광범위하게 글을 써왔습니다. 우리의 가장 잘 알려진 작업에는 에세이 『AI 안전은 모델 속성이 아니다(AI safety is not a model property)』와 대규모 협력의 결과물인 『오픈 파운데이션 모델의 사회적 영향에 대하여(On the Societal Impact of Open Foundation Models)』라는 제목의 논문이 포함됩니다. 2024년 이후, AI 거버넌스 및 규제에 대한 국제적 논의가 활발해지면서, 다양한 전문가 그룹의 목소리가 더욱 중요해지고 있습니다.

**감사의 말씀.** 초고에 대한 피드백을 주신 벤자민 에델만(Benjamin Edelman), 에즈라 카거(Ezra Karger), 맷 살가닉(Matt Salganik), 올리 스테픈슨(Ollie Stephenson)께 감사드립니다. 이 에세이 시리즈는 세스 라자르(Seth Lazar)와 호주 국립대학교 MINT 연구소 구성원들, 프린스턴 대학교 예측의 한계(Limits to Prediction) 과정 학생들, 샤제다 아메드(Shazeda Ahmed), 재커리 시겔(Zachary Siegel)을 포함한 많은 분들의 피드백을 통해 완성될 예정인 논문을 기반으로 합니다.

1 구체적으로, 핵무기 실험 데이터를 사용하고 몇 가지 물리학 기반 가정을 통해, 장기간 하늘을 어둡게 하고 전 세계 농업의 붕괴를 초래할 만큼 충분한 먼지를 일으키는 데 필요한 것이 무엇인지 계산할 수 있습니다.
2 보수적인 예측가 G가 예측에 대해 ϵ(우리 예에서는 0.01)의 하한선을 가지고 있다고 가정해 봅시다. 두 예측가 간의 차이가 관련 있는 사건은 ϵ 비율에 불과합니다. 심지어 진정한 확률이 ϵ 미만인 사건 집합으로 제한하더라도, 일부 계산은 F의 예상 로그 점수가 G의 예상 로그 점수보다 미미한 양(O(ϵ))만큼 더 좋고, 브라이어 점수(Brier score)의 경우 O(ϵ^2)만큼 더 좋다는 것을 보여줍니다. 따라서 모든 사건을 볼 때, 예상 점수의 차이는 각각 O(ϵ^2) 및 O(ϵ^3)입니다. 한편, 그들의 점수 분산은 두 점수 함수 모두 약 0.1로 나옵니다. 두 예측가의 점수 평균이 다르다고 자신 있게 주장하려면, 평균 점수 차이의 표준 편차(sqrt(0.1 / N))가 예상 차이보다 훨씬 작을 만큼 사건의 수 N이 충분히 커야 합니다. 따라서 로그 점수(log score)의 경우 N ~ O(1/ϵ^4)이고, 브라이어 점수(Brier score)의 경우 N ~ O(1/ϵ^6)가 필요합니다.
3 물론, 예측가들이 위험을 과소평가할 수 있는 이유도 있습니다. 우리는 그러한 이유들이 덜 설득력 있다고 생각하지만, 더 중요한 것은, 이 논의의 범위를 벗어난다고 생각합니다. 이유는 간단합니다. 정부가 자유를 제한하는 정책을 시행하도록 옹호하는 것은 높은 실존적 위험(x-risk) 추정치에 의해 정당화되고 있습니다. 따라서 증거의 부담은 비대칭적입니다. 그러한 정책을 요구하는 사람들은 상향 편향(upward biases)에 대한 우려에 대응하는 것을 포함하여 자신의 확률 추정치를 정당화해야 합니다. 만약 어떤 이상한 미래 세계에서 일부 사람들이 낮은 실존적 위험(x-risk) 추정치에 기반하여 제한적인 정책을 옹호한다면, 잠재적인 과소평가에 대해 동일한 조사를 하는 것이 중요해질 것입니다.
4 예측 이론에서 로그 점수(logarithmic score) 및 기타 소위 적절한 점수 규칙(proper scoring rules)은 예측가의 "진정한 믿음(true belief)"을 이끌어내는 것이 수학적으로 보장됩니다. 그러나 적절한 점수 규칙(proper scoring rule)의 정의는 진정한 믿음이 단일 값이라고 가정하는 반면, 실제로는 넓은 범위인 경향이 있습니다. 이러한 시나리오에서 예측가는 자신의 분포의 평균을 보고하도록 유도되지만, 우리가 직관적으로 "진정한 믿음(true belief)"이라고 의미하는 것은 중앙값에 더 잘 부합할 수 있습니다.