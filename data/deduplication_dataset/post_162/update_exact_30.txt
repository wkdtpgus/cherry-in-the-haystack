연구자들 간의 합의가 부족하다는 점을 고려할 때, 정부는 AI로 인한 실존적 위험(existential risk, x-risk)의 위협을 얼마나 심각하게 받아들여야 할까요? 한편으로, 실존적 위험(x-risk)은 필연적으로 다소 추측적입니다. 구체적인 증거가 나올 때쯤이면 너무 늦을 수도 있기 때문입니다. 다른 한편으로, 정부는 우선순위를 정해야 합니다. 결국, 외계인 침공으로 인한 실존적 위험(x-risk)에 대해 지나치게 걱정하지는 않으니까요. 이 글은 AI 실존적 위험(AI x-risk)에 대해 우려하는 정책 입안자들을 위한 증거 기반 접근 방식을 제시하는 일련의 에세이 중 첫 번째입니다. 이 접근 방식은 "알려지지 않은 미지의 것(unknown unknowns)"이 존재함을 인정하면서도 현실에 기반을 두고 있습니다. 이 첫 번째 에세이에서는 한 가지 유형의 증거인 확률 추정치를 살펴봅니다. AI 안전 커뮤니티는 의사 결정 및 정책 수립에 정보를 제공하기 위해 AI로 인한 인류 멸종 확률(주어진 기간 내)을 예측하는 데 크게 의존합니다. 예를 들어, 수십 년 내 10%라는 추정치는 이 문제가 사회의 최우선 과제가 되기에 충분히 높은 수치일 것입니다. 우리의 핵심 주장은 AI 실존적 위험(AI x-risk) 예측이 정책에 활용하기에는 너무 신뢰할 수 없으며, 사실상 매우 오해의 소지가 있다는 것입니다. 이 시리즈의 다음 에세이를 받아보려면 구독하세요. 구독

### 장막 뒤를 들여다보다

켄터키 더비(Kentucky Derby)의 배당률을 예측한다면, 우리는 당신에게 이유를 설명할 필요가 없습니다. 당신은 그것을 받아들이거나 말거나 할 수 있습니다. 그러나 정책 입안자가 예측가가 제시한 확률에 근거하여 조치를 취한다면, 그들은 대중에게 그 확률을 설명할 수 있어야 합니다(그리고 그 설명은 다시 예측가로부터 나와야 합니다). 정당성은 정부의 정당성과 권력 행사에 필수적입니다. 자유 민주주의의 핵심 원칙은 국가가 합리적인 사람들이 거부할 수 있는 논란의 여지가 있는 신념에 기반하여 사람들의 자유를 제한해서는 안 된다는 것입니다. 설명은 고려 중인 정책이 비용이 많이 들 때 특히 중요하며, 그 비용이 이해관계자들 사이에 불균등하게 분배될 때는 더욱 그렇습니다. 좋은 예는 AI 모델의 공개 출시를 제한하는 것입니다. 정부는 공개 모델로부터 이익을 얻을 사람들과 기업들에게 추측적인 미래 위험 때문에 이러한 희생을 해야 한다고 설득할 수 있을까요?

최근 몇 년간 대규모 언어 모델(LLM)과 같은 AI 기술의 급진적인 발전은 AI 위험에 대한 대중과 정치권의 논의를 폭발적으로 증가시켰습니다. 이러한 논의는 종종 AI가 가져올 수 있는 잠재적인 이점보다는 실존적 위협에 초점을 맞춥니다. 정책 입안자들은 기술의 파괴적인 잠재력에 대한 경고에 직면하여 신속하게 대응해야 한다는 압력을 느끼지만, 이러한 경고가 얼마나 견고한 증거에 기반하고 있는지에 대한 면밀한 검토는 부족한 경우가 많습니다. 특히 AI 모델의 '블랙박스(black box)' 특성으로 인해 내부 작동 방식에 대한 이해가 제한적일 때, 추측적인 예측에 기반한 정책 결정은 더욱 위험할 수 있습니다.

이 에세이의 주요 목표는 정책 논쟁에서 인용된 특정 실존적 위험(x-risk) 확률 추정치에 대한 정당성이 있는지 분석하는 것입니다. 우리는 AI 실존적 위험(AI x-risk) 예측을 학술 활동으로 보는 것에 반대하지 않으며, 예측은 기업 및 기타 사적 의사 결정자들에게 유용할 수 있습니다. 우리는 단지 공공 정책의 맥락에서 그 사용에 의문을 제기할 뿐입니다.

예측가가 회의론자를 설득할 수 있는 알려진 방법은 기본적으로 세 가지뿐입니다: 귀납적(inductive), 연역적(deductive), 그리고 주관적 확률 추정(subjective probability estimation)입니다. 다음 섹션에서 각각을 살펴보겠습니다. 세 가지 모두 양 당사자가 세상에 대한 몇 가지 기본적인 가정(그 자체로 증명될 수 없는)에 동의해야 합니다. 세 가지 접근 방식은 확률 추정치가 그 가정 집합에서 도출되는 경험적 및 논리적 방식에서 차이가 있습니다.

### 참조 집단(reference class)의 부족으로 인해 귀납적 확률 추정(inductive probability estimation)은 신뢰할 수 없습니다

대부분의 위험 추정치는 귀납적(inductive)입니다. 즉, 과거 관찰에 기반합니다. 예를 들어, 보험사는 유사한 운전자의 과거 사고 데이터에 기반하여 개인의 자동차 사고 위험을 예측합니다. 확률 추정에 사용되는 관찰 집합을 참조 집단(reference class)이라고 합니다. 자동차 보험에 적합한 참조 집단(reference class)은 같은 도시에 사는 운전자 집합일 수 있습니다. 분석가가 개인에 대한 더 많은 정보(예: 나이 또는 운전하는 자동차 유형)를 가지고 있다면, 참조 집단(reference class)은 더욱 세분화될 수 있습니다.

AI로 인한 실존적 위험(existential risk)의 경우, 다른 어떤 사건과도 같지 않으므로 참조 집단(reference class)이 없습니다. 분명히 말하자면, 이것은 종류의 문제가 아니라 정도의 문제입니다. 사용할 명확한 "올바른" 참조 집단(reference class)은 결코 없으며, 실제 참조 집단(reference class)의 선택은 분석가의 직관에 달려 있습니다. 예측의 정확성은 예측되는 사건을 생성하는 과정과 참조 집단(reference class)의 사건을 생성한 과정 간의 유사성 정도에 따라 달라지며, 이는 스펙트럼으로 볼 수 있습니다.

AI 실존적 위험(AI x-risk)에 대한 참조 집단(reference class)을 찾으려는 시도를 보십시오: 동물 멸종(인류 멸종의 참조 집단(reference class)으로), 산업 혁명과 같은 과거의 전 세계적 변화(AI로 인한 사회경제적 변화의 참조 집단(reference class)으로), 또는 대규모 사망을 초래한 사고(전 세계적 재앙을 초래한 사고의 참조 집단(reference class)으로). 현실을 직시합시다. 그 어떤 것도 초지능 AI(superintelligent AI) 개발 가능성이나 그러한 AI에 대한 통제력 상실 가능성에 대해 우리에게 아무것도 알려주지 않습니다. 이는 AI 실존적 위험(AI x-risk) 예측의 불확실성의 핵심 원천입니다. 요약하자면, AI로 인한 인류 멸종은 과거에 일어났던 어떤 일과도 너무나 동떨어진 결과이므로, 귀납적 방법(inductive methods)을 사용하여 그 확률을 "예측"할 수 없습니다. 물론, 과거의 기술적 돌파구와 재앙적인 사건들로부터 질적 통찰력을 얻을 수는 있지만, AI 위험은 정책 수립의 정당성에 필요한 종류의 정량적 추정치 정당성이 부족할 정도로 충분히 다릅니다.

AI 기술의 발전 속도와 예측 불가능한 특성은 귀납적 추론의 한계를 더욱 명확히 드러냅니다. 특히 GPT-4와 같은 최신 대규모 언어 모델에서 관찰되는 '창발적 능력(emergent capabilities)'은 훈련 데이터나 설계 의도를 넘어서는 새로운 행동 패턴을 보여줍니다. 이러한 창발적 현상은 과거의 AI 시스템에서는 볼 수 없었던 것이며, 따라서 과거의 경험을 바탕으로 미래의 위험을 예측하는 것이 더욱 어려워집니다. AI가 단순한 도구를 넘어 자율적인 학습과 의사결정 능력을 갖게 될 때, 기존의 귀납적 모델은 그 예측력을 상실하게 됩니다.

### 이론의 부족으로 인해 연역적 확률 추정(deductive probability estimation)은 신뢰할 수 없습니다

코난 도일(Conan Doyle)의 『여섯 개의 나폴레옹 흉상(The Adventure of the Six Napoleons)』에서 — 스포일러 경고! — 셜록 홈즈(Sherlock Holmes)는 잠복 수사에 착수하기 전에 용의자를 잡을 확률이 정확히 3분의 2라고 발표합니다. 이것은 당혹스럽게 들립니다. 인간 행동과 관련된 어떤 것이 어떻게 수학적으로 정확한 확률로 귀속될 수 있을까요?

알고 보니 홈즈는 용의자의 겉보기에 불규칙한 관찰된 행동을 야기한 근본적인 일련의 사건들을 연역적으로 추론했습니다. 용의자는 런던과 그 주변의 다른 사람들이 소유한 여섯 개의 나폴레옹 흉상 중 하나 안에 숨겨져 있다고 알려진 보석을 체계적으로 찾고 있었습니다. 세부 사항은 그리 중요하지 않지만, 핵심은 용의자도 형사도 여섯 개의 흉상 중 어느 것에 보석이 있는지 모른다는 것과 용의자의 행동에 대한 다른 모든 것이 완전히 예측 가능하다고 (가정된다는) 것입니다. 따라서 정확하게 정량화 가능한 불확실성이 발생합니다.

요점은 우리가 의존할 수 있는 세상의 모델을 가지고 있다면, 과거 관찰에 의존하지 않고도 논리적 연역(logical deduction)을 통해 위험을 추정할 수 있다는 것입니다. 물론, 허구적인 시나리오 밖에서는 세상이 그렇게 깔끔하지 않습니다. 특히 먼 미래를 예측하고자 할 때는 더욱 그렇습니다.

실존적 위험(x-risk)에 관해서는 연역적 모델(deductive models)이 없다는 일반적인 규칙에 대한 흥미로운 예외가 있습니다. 바로 소행성 충돌입니다. 귀납적(inductive) 및 연역적 위험 추정(deductive risk estimation)의 조합은 우리가 실존적 위험(x-risk)의 확률을 추정할 수 있게 해주는데, 이는 우리가 순전히 물리적인 시스템에 대해 이야기하고 있기 때문입니다. 이 방법이 어떻게 작동하는지 잠시 살펴보겠습니다. 왜냐하면 이 방법이 다른 유형의 실존적 위험(x-risk)에 일반화될 수 없다는 것을 인식하는 것이 중요하기 때문입니다. 핵심은 소행성의 크기(더 정확하게는 충돌 에너지)와 충돌 빈도 간의 관계를 모델링할 수 있다는 것입니다. 수천 개의 작은 충돌을 관찰했기 때문에, 우리는 직접 관찰된 적이 없는 큰 충돌의 빈도를 추론하기 위해 외삽할 수 있습니다. 우리는 또한 전 세계적인 재앙을 초래할 임계값을 추정할 수 있습니다.

AI의 경우, 미지의 요소는 물리적 시스템보다는 기술 발전 및 거버넌스(governance)와 관련되어 있으므로, 이를 수학적으로 모델링하는 방법이 명확하지 않습니다. 그럼에도 불구하고 사람들은 시도해 왔습니다. 예를 들어, 가상의 일반 인공지능(AGI)의 계산 요구 사항을 예측하기 위해, 여러 연구에서는 AI 시스템이 인간 두뇌와 거의 동일한 양의 계산을 필요로 할 것이라고 가정하고, 나아가 인간 두뇌에 필요한 계산량에 대한 가정을 합니다. 이러한 가정들은 소행성 모델링에 관련된 가정들보다 훨씬 더 미약하며, 이 중 어느 것도 통제 상실 문제를 다루지 않습니다.

특히 현대 AI 시스템, 예를 들어 대규모 언어 모델(LLM)의 복잡성은 연역적 확률 추정의 한계를 더욱 부각시킵니다. 이러한 모델의 내부 작동 방식은 수십억 개의 매개변수로 구성되어 있어 인간이 완전히 이해하고 예측하기 어렵습니다. 특정 입력에 대해 모델이 왜 그런 출력을 내놓는지에 대한 '설명 가능성(explainability)'조차 부족한 상황에서, 미래의 행동이나 잠재적 위험을 연역적으로 추론하는 것은 거의 불가능에 가깝습니다. '정렬 문제(alignment problem)' 역시 이러한 맥락에서 연역적 접근의 어려움을 보여줍니다. AI가 인간의 가치와 목표에 부합하도록 행동할 것이라는 것을 수학적으로 증명하거나 연역적으로 추론할 수 있는 견고한 이론적 프레임워크는 아직 존재하지 않습니다. 따라서 AI의 '의도'나 '목표'가 인간에게 해를 끼치지 않을 것이라는 연역적 결론을 내리기는 매우 어렵습니다.

### 주관적 확률(subjective probabilities)은 숫자로 위장한 감정입니다

참조 집단(reference class)이나 근거 있는 이론이 없다면, 예측은 필연적으로 "주관적 확률(subjective probabilities)", 즉 예측가의 판단에 기반한 추측이 됩니다. 놀랍게도, 이들은 자릿수(orders of magnitude)만큼 차이가 납니다. 주관적 확률 추정(subjective probability estimation)은 확률 추정치에 대한 귀납적(inductive) 또는 연역적(deductive) 근거를 가질 필요성을 회피하지 못합니다. 단지 예측가가 자신의 추정치를 설명할 필요성을 피할 뿐입니다. 설명은 인간이 귀납적, 연역적 또는 그 조합이든 직관적인 추론을 설명하는 능력이 제한적이기 때문에 어려울 수 있습니다. 본질적으로, 이는 예측가에게 다음과 같이 말할 수 있게 합니다: "내 방법을 보여주지는 않았지만, 내 실적 때문에 이 추정치를 신뢰할 수 있습니다" (다음 섹션에서 AI 실존적 위험(AI x-risk) 예측의 경우에도 이것이 왜 무너지는지 설명합니다). 그러나 궁극적으로 귀납적 또는 연역적 근거가 없다면, 예측가들이 할 수 있는 일은 숫자를 만들어내는 것뿐이며, 그 만들어진 숫자들은 제각각입니다.

2022년 말 예측 연구소(Forecasting Research Institute)에서 실시한 실존적 위험 설득 토너먼트(Existential Risk Persuasion Tournament, XPT)를 고려해 봅시다. 우리는 이것이 현재까지 수행된 실존적 위험(x-risk) 예측 연습 중 가장 정교하고 잘 실행된 것이라고 생각합니다. 여기에는 AI 전문가와 예측 전문가("슈퍼 예측가(superforecasters)" 그림 참조)를 포함한 다양한 예측가 그룹이 참여했습니다. AI 전문가의 경우, 2100년까지 AI 멸종 위험에 대한 추정치의 상위 75%는 12%, 중앙값은 3%, 하위 25%는 0.25%였습니다. 예측 전문가의 경우, 상위 75%조차도 1%에 불과했고, 중앙값은 겨우 0.38%였으며, 하위 25%는 그래프에서 시각적으로 0과 구별할 수 없었습니다. 다시 말해, AI 전문가의 75% 예측과 슈퍼 예측가(superforecaster)의 25% 예측은 최소 100배 이상 차이가 납니다. 이 모든 추정치는 해당 주제에 대한 깊은 전문 지식을 가지고 있으며 몇 달 동안 서로를 설득하려 노력한 토너먼트에 참여한 사람들로부터 나온 것입니다!

이러한 주관적 확률의 불일치는 특정 AI 안전 커뮤니티 내의 '에코 챔버(echo chamber)' 효과와 '집단 사고(groupthink)'의 위험성을 시사합니다. 특정 철학적 관점, 예를 들어 장기주의(long-termism)에 깊이 경도된 커뮤니티에서는 AI x-risk에 대한 우려가 과도하게 증폭될 수 있습니다. 이러한 환경에서는 합리적인 논증보다는 특정 내러티브가 강화되는 경향이 있으며, 이는 예측의 객관성을 저해합니다. 심지어 경제적 인센티브가 주어졌음에도 불구하고 XPT에서 참가자들의 신념 변화가 거의 없었다는 점은 이러한 편향이 얼마나 뿌리 깊은지를 보여줍니다.

### 예측 능력 평가의 한계: 꼬리 위험과 긴 시간 지평

요약하자면, 귀납적(inductive) 및 연역적(deductive) 방법은 작동하지 않고, 주관적 예측은 제각각이며, 어떤 예측이 더 신뢰할 수 있는지 알 방법이 없습니다. 그래서 정책에 잠재적으로 정보를 제공할 수 있는 더 신뢰할 수 있는 추정치를 도출하기 위해, 일부 연구자들은 여러 예측가의 예측을 결합하는 예측 집계 방법(forecast aggregation methods)으로 눈을 돌렸습니다. 주목할 만한 노력은 AI 발전 설문조사(AI Impacts Survey on Progress in AI)이지만, 이는 무응답 편향(non-response bias)을 포함한 심각한 방법론적 한계로 비판받아 왔습니다. 더 중요한 것은, 집계가 예측 정확도를 왜 개선해야 하는지 불분명하다는 것입니다. 결국, 대부분의 예측가들은 동일한 편향을 공유할 수 있으며(다시 말하지만, 그들 중 누구도 신뢰할 수 있는 예측의 근거를 가지고 있지 않습니다).

AI 실존적 위험(AI x-risk)과 같은 '꼬리 위험(tail risks)'은 발생 확률이 매우 낮지만, 발생 시 막대한 결과를 초래하는 사건을 의미합니다. 이러한 사건에 대한 예측 능력은 전통적인 예측 평가 방법으로는 제대로 측정하기 어렵습니다. 예를 들어, 예측가 A가 1%의 확률을, 예측가 B가 0.0001%의 확률을 제시했을 때, 어느 쪽이 더 신뢰할 수 있는지 판단하기란 거의 불가능합니다. 꼬리 위험은 극히 드물게 발생하기 때문에, 예측가가 수백만 번의 예측을 반복하지 않는 한, 그들의 예측이 과대평가되었는지 과소평가되었는지 경험적으로 검증하기 어렵습니다. 게다가, AI x-risk는 '긴 시간 지평(long time horizon)'을 가지고 있어, 예측이 현실화되기까지 수십 년 또는 수백 년이 걸릴 수 있습니다. 이는 예측 능력을 검증할 기회 자체가 거의 없음을 의미하며, 멸종 시나리오의 경우 검증 자체가 불가능합니다.

이러한 한계는 '급진적 불확실성(radical uncertainty)'이라는 개념과도 연결됩니다. 이는 단순히 확률을 알 수 없는 '알려지지 않은 미지의 것(unknown unknowns)'을 넘어, 미래의 사건이 너무나 새롭고 전례가 없어서 우리가 사용할 수 있는 개념이나 모델조차 존재하지 않는 상황을 의미합니다. AI의 발전은 이러한 급진적 불확실성을 증폭시키며, 따라서 어떤 예측 모델도 그 정확성을 보장할 수 없습니다.

### 위험 추정치가 체계적으로 부풀려질 수 있는 여러 가지 이유

예측가들이 AI 실존적 위험(AI x-risk)을 체계적으로 과대평가할 수 있는 여러 가지 이유가 있습니다. 첫 번째는 선택 편향(selection bias)입니다. AI 연구자들을 예로 들어 봅시다. AI가 세상을 바꿀 수 있다는 믿음은 AI 연구자가 되는 주요 동기 중 하나입니다. 그리고 일단 이 커뮤니티에 들어서면, 그 메시지가 끊임없이 강화되는 환경에 놓이게 됩니다. 그리고 이 기술이 엄청나게 강력하다고 믿는다면, 그 세상 변화 효과가 긍정적이기보다는 부정적일 심각한 가능성이 있다고 생각하는 것은 완벽하게 합리적입니다. 그리고 다소 고립된 AI 안전 하위 커뮤니티에서는 에코 챔버(echo chamber)가 귀청이 터질 듯할 수 있습니다. 높은 p(doom)(AI 파멸 확률에 대한 자신의 추정치)를 가지고 있다고 주장하는 것은 자신의 정체성과 대의에 대한 헌신을 알리는 방법이 된 것 같습니다.

예측 전문가의 경우 약간 다른 선택 편향(selection bias)이 작용합니다. 예측 커뮤니티는 효과적 이타주의(effective altruism) 및 실존적 위험(existential risk), 특히 AI 위험에 대한 우려와 강한 중첩을 이룹니다. 이것이 개별 예측가들이 편향되어 있다는 의미는 아닙니다. 그러나 높은 p(doom)을 가지고 있으면 예측 활동에 더 참여하고 싶어질 수 있습니다. 따라서 커뮤니티 전체는 실존적 위험(x-risk)에 대한 우려를 가진 사람들에게 편향되어 있을 가능성이 높습니다.

예측가들은 증거에 반응하여 자신의 신념을 업데이트하는 데 능숙하지만, 문제는 예를 들어 소행성 충돌 위험과 달리 AI 실존적 위험(AI x-risk)에 관해서는 어떤 식으로든 자신의 신념을 바꿀 수 있는 증거가 거의 없다는 것입니다. 따라서 우리는 예측이 사람들이 커뮤니티에 들어올 때 가지고 있던 사전 신념(priors)에 강하게 영향을 받는다고 의심합니다. XPT 보고서는 "가장 적극적인 참가자들 사이에서도, 그리고 다른 사람들을 설득하기 위한 금전적 인센티브에도 불구하고, XPT 동안 마음이 바뀐 사람은 거의 없었다"고 언급합니다. 후속 연구에서 그들은 많은 의견 불일치가 AI를 넘어선 근본적인 세계관 차이 때문이라는 것을 발견했습니다.

이러한 편향 외에도, '예방 원칙(precautionary principle)'의 오용 또한 위험 추정치를 부풀릴 수 있습니다. 예방 원칙은 잠재적으로 해로운 활동이 과학적으로 완전히 입증되지 않았더라도 예방 조치를 취해야 한다는 것을 의미합니다. 그러나 AI x-risk와 같이 증거가 희박한 영역에서 이 원칙을 무비판적으로 적용하면, 과도한 규제나 혁신 저해로 이어질 수 있습니다. 특히 AI 안전 연구 자금 지원과 관련하여, 높은 x-risk 내러티브는 특정 연구 방향으로 자원을 유도하고, 보다 즉각적이고 측정 가능한 AI 위험(예: 편향, 오용, 보안 취약점)에 대한 연구를 소홀히 할 위험이 있습니다.

### 파스칼의 내기(Pascal’s wager)를 경계하라: 효용 극대화(utility maximization)의 위험

참고로, AI로 인한 멸종 위험과 멸종에 이르지 않는 재앙적 위험에 대한 XPT의 중앙값 추정치는 다음과 같습니다:

다시 말하지만, 우리는 이 숫자들을 너무 심각하게 받아들여서는 안 된다는 견해입니다. 이들은 참가자들의 다양한 샘플이 다른 어떤 것보다 AI에 대해 얼마나 걱정하는지를 반영합니다. 이전과 마찬가지로, 예측 전문가(superforecasters)와 AI 전문가의 추정치는 자릿수(order of magnitude) 이상 차이가 납니다. 우리가 이러한 추정치를 어느 정도 신뢰한다면, AI 전문가의 추정치보다는 예측 전문가의 추정치여야 합니다. 과거 연구에서 얻은 중요한 통찰 중 하나는, 다양한 정보를 통합하고 심리적 편향을 최소화하는 훈련을 받은 예측 전문가보다 특정 분야 전문가(domain experts)가 더 나쁜 성과를 보인다는 것입니다. 그럼에도 불구하고, 위에서 말했듯이, 그들의 예측조차도 엄청난 과대평가일 수 있으며, 우리는 확실히 알 수 없습니다.

그래서 뭐가 문제일까요? 정책 입안자들이 특정 기간 동안의 위험이 0.01%가 아니라 1%라고 믿는다면 어떨까요? 어느 경우든 꽤 낮아 보입니다! 그것은 그들이 그 확률을 가지고 무엇을 하는지에 달려 있습니다. 대부분의 경우, 이러한 추정치는 단순히 일부 전문가 그룹이 위험이 중요하다고 생각한다는 사실을 알리는 방법일 뿐입니다. 그것이 전부라면, 그렇게 되십시오. 그러나 다른 사람들이 같은 숫자를 극도로 다르게 해석한다는 점을 고려할 때, 이러한 정량화에 대한 정교한 노력이 이러한 신호 전달 목적에 도움이 되는지조차 명확하지 않습니다. 예를 들어, 연방거래위원회(Federal Trade Commission) 위원장 리나 칸(Lina Khan)은 자신의 p(doom)이 15%에 불과했기 때문에 이 문제에 대한 자신의 견해가 기술 낙관적(techno-optimistic)이라고 말했는데, 이는 전문가들을 당혹스럽게 했습니다. (참고로, 그 숫자는 우리가 기술 낙관론자라고 부르기에 편안하게 생각하는 것보다 약 천 배 더 높습니다.)

의사 결정에서 매우 작거나 매우 큰 숫자를 정확하게 정신적으로 처리하고, 단순히 "무의미하게 작은"과 같은 범주로 묶지 않으려면 많은 정량적 훈련이 필요합니다. 대부분의 사람들은 이런 식으로 훈련받지 않습니다. 요컨대, 전문가들의 모호한 직관과 두려움이 의사-정확한 숫자(pseudo-precise numbers)로 번역된 다음, 정책 입안자들에 의해 다시 모호한 직관과 두려움으로 번역되는 것처럼 보입니다. 정량화의 허세를 그만둡시다! AI 안전 센터(Center for AI Safety)의 AI 위험 성명(Statement on AI Risk)은 이 점에서 놀랍도록 솔직했습니다(물론, 우리는 그 내용에 강력히 동의하지 않습니다).

의사 결정에서 확률을 사용하는 원칙적이고 정량적인 방법은 비용-편익 분석(cost-benefit analysis)을 통한 효용 극대화(utility maximization)입니다. 아이디어는 간단합니다. 어떤 결과가 주관적인 가치, 즉 효용(utility) U(양수 또는 음수일 수 있음)를 가지고 있고, 예를 들어 발생 확률이 10%라면, 우리는 그것이 확실히 발생하고 0.1 * U의 가치를 가진다고 가정하고 행동할 수 있습니다. 그런 다음 우리가 사용할 수 있는 각 옵션에 대한 비용과 편익을 합산하고, 비용에서 편익을 뺀 값(즉, "기대 효용(expected utility)")을 최대화하는 옵션을 선택할 수 있습니다.

여기서부터 문제가 정말 심각해집니다. 첫째, 어떤 사람들은 멸종이 헤아릴 수 없을 정도로 큰 부정적인 가치를 가진다고 생각할 수 있습니다. 왜냐하면 그것은 미래에 태어날 수 있는 모든 인간 생명, 물리적이든 시뮬레이션된 것이든, 존재를 배제하기 때문입니다. 논리적인 결론은 실존적 위험(x-risk)이 항상 모든 사람의 최우선 과제가 되어야 한다는 것입니다! 이는 파스칼의 내기(Pascal’s wager)를 연상시킵니다. 신이 존재할 가능성이 극히 작더라도, 불신앙의 대가는 무한하며(영원한 행복과 대비되는 지옥에서의 영원), 따라서 기대 효용(expected utility)도 무한하다는 주장입니다. 다행히 정책 입안자들은 무한을 포함하는 의사 결정 프레임워크에 너무 많은 신뢰를 주지 않습니다. 그러나 이 아이디어는 AI 안전 커뮤니티를 강력하게 사로잡았고, 일부 사람들에게 AI 실존적 위험(AI x-risk)이 사회의 최우선 과제가 되어야 한다는 확신을 심어줍니다.

재앙적이지만 실존적이지 않은 위험으로 스스로를 제한하더라도, 우리는 수십억 명의 생명이 위태로운 상황에 대해 이야기하고 있으므로, 심지어 1%의 위험에 대한 기대 비용(expected cost)도 너무 높아서 정책적 함의가 극단적입니다. 정부는 AI 실존적 위험(AI x-risk) 완화에 대한 지출을 자릿수(orders of magnitude)만큼 늘리고 AI 개발 중단과 같은 가혹한 조치를 고려해야 합니다. 이것이 이러한 추정치들이 어떤 방법론으로도 뒷받침되지 않는다는 것을 이해하는 것이 매우 중요한 이유입니다. 이렇게 적은 증거에 기반하여 세상을 바꾸는 정책 결정을 내리는 것은 엄청나게 현명하지 못한 일일 것입니다.

최근 전 세계적으로 AI 규제 논의가 활발해지면서, 이러한 효용 극대화 프레임워크의 위험성이 더욱 부각되고 있습니다. 불확실한 실존적 위험에 대한 과도한 강조는 AI의 잠재적 이점과 보다 시급한 단기적 위험(예: 편향, 차별, 오용)에 대한 논의를 가릴 수 있습니다. 정책 입안자들은 균형 잡힌 시각을 가지고, 충분한 증거에 기반하여 AI 기술의 발전과 사회적 책임 사이의 적절한 균형점을 찾아야 합니다.

### AI 이정표 예측: 명확성과 정책적 함의

AI 정책에서 예측의 역할이 있을까요? 우리는 그렇다고 생각합니다. 단지 실존적 위험(existential risk)을 예측하는 것이 아닐 뿐입니다. 특정 능력 벤치마크(capability benchmarks) 또는 경제적 영향에 대한 성과와 같은 AI 이정표를 예측하는 것이 더 달성 가능하고 의미 있습니다. 예측가가 다양한 AI 이정표가 언제 도달할지 예측하는 데 능숙함을 입증했다면, 이는 그들이 미래에도 잘할 것이라는 증거를 제공합니다. 우리는 더 이상 독특하거나 희귀한 사건에 대해 이야기하는 것이 아닙니다. 그리고 살인 로봇을 막는 것보다 잠재적인 경제적 혼란에 대비하는 것과 같은 낮은 이해관계의 정책 개입을 고려할 때, 예측이 모든 합리적인 사람의 만족을 위해 정당화되어야 하는 중요성은 덜합니다.

예측 커뮤니티는 이정표 예측에 많은 노력을 기울입니다. 메타큘러스(Metaculus)에서 "2040년 이전에 인간-기계 지능 동등성(Human-machine intelligence parity)이 달성될 것인가?"라는 질문에 대해 1,300명 이상의 예측가들이 96%의 집계 예측을 내놓았습니다. 놀랍습니다! 우리가 이 예측에 동의한다면, 우리는 일반 인공지능(AGI)으로의 안전한 전환을 관리하는 것이 전 세계적인 우선순위가 되어야 한다는 입장을 지지할 것입니다. 왜 우리는 그렇지 않을까요?

답은 세부 사항에 있습니다. 일반 인공지능(AGI)과 같은 모호한 개념의 정의에 대한 합의가 없습니다. 정의를 고정하더라도, 그것이 달성되었는지 여부를 결정하는 것은 어렵거나 불가능할 수 있습니다. 효과적인 예측을 위해서는 모호한 결과(ambiguous outcomes)를 피하는 것이 극히 중요합니다. 예측 커뮤니티가 이 문제를 해결하는 방법은 시험 성적과 같은 비교적 좁은 기술 측면에서 정의하는 것입니다. 메타큘러스(Metaculus)의 지능 동등성 질문은 수학, 물리학, 컴퓨터 과학 분야의 대학원 시험 성적 측면에서 정의됩니다. 이 정의에 기반하여, 우리는 96%의 예측에 동의합니다. 그러나 우리는 그 정의가 너무 희석되어 정책에 큰 의미가 없다고 생각합니다. 실존적 위험(existential risk)은 잊어버리십시오. 이전에 썼듯이, 시험에서의 AI 성능은 구성 타당도(construct validity)가 너무 낮아서 AI가 노동자를 대체할지조차 예측할 수 없습니다. 다른 벤치마크(benchmarks)도 크게 다르지 않습니다. 요컨대, 벤치마크와 실제 영향 사이의 큰 격차 때문에 AI 능력 타임라인을 예측하는 것은 까다롭습니다.

다행히도, 실제 중요한 작업을 반영하는 더 나은 벤치마크(benchmarks)가 개발되고 있습니다. 벤치마크(benchmarks) 외에도, 비용이 더 많이 들더라도 자연주의적 평가(naturalistic evaluation)가 필요합니다. 자연주의적 평가(naturalistic evaluation)의 한 유형은 사람들이 AI 지원을 받아 자신의 업무를 어떻게 다르게 수행하는지 측정하는 것입니다. 노동 시장 변화나 군대의 AI 관련 지출과 같은 경제적, 사회적, 정치적 영향을 직접 예측하는 것이 더 유용할 수 있지만, 명확하게 정의하고 측정하기는 더 어렵습니다.

최근에는 AI 시스템의 특정 기능적 능력에 초점을 맞춘 벤치마크들이 정책 결정에 더 실질적인 통찰력을 제공하기 시작했습니다. 예를 들어, 특정 법률 문서 분석, 의료 진단 보조, 또는 기후 모델링과 같은 실제 문제 해결 능력에 대한 벤치마크는 AI가 사회에 미치는 구체적인 영향과 위험을 평가하는 데 더 유용합니다. 이러한 벤치마크는 AI의 잠재적 오용(예: 딥페이크 생성, 자율 무기 시스템) 또는 편향(예: 채용 도구의 성별/인종 차별)과 같은 단기적 위험을 식별하고 완화하기 위한 정책을 수립하는 데 직접적인 근거를 제공할 수 있습니다.

### 결론

정책에서 확률의 오용을 피할 책임은 정책 입안자에게 있습니다. 우리는 정책 입안자들이 오도되는 것을 "보호"하기 위해 예측가들에게 예측 발표를 중단하라고 요구하는 것이 아닙니다. 그렇지만, 우리는 예측이 사용된 과정과 고려된 증거에 대한 명확한 설명과 함께 제공되어야 한다고 생각합니다. 이것은 정책 입안자들이 제시된 정당성이 그들이 편안하게 받아들일 수 있는 기준을 충족하는지 여부에 대해 정보에 입각한 결정을 내릴 수 있도록 할 것입니다. XPT는 투명성의 좋은 예이며, 이 논문도 마찬가지입니다(비록 실존적 위험(x-risk)에 관한 것은 아니지만). 반면에, 단순히 많은 연구자들을 설문조사하고 집계된 숫자를 제시하는 것은 오해의 소지가 있으며 정책 입안자들은 이를 무시해야 합니다.

그렇다면 정부는 AI 실존적 위험(AI x-risk)에 대해 무엇을 해야 할까요? 우리의 견해는 아무것도 하지 말아야 한다는 것이 아닙니다. 그러나 그들은 실존적 위험(x-risk)을 시급하고 심각하게 본다면 설득력 있게 보일 수 있는 종류의 정책, 특히 AI 개발 제한과 같은 정책을 거부해야 합니다. 이 시리즈의 향후 에세이에서 주장하겠지만, 그러한 정책은 불필요할 뿐만 아니라 실존적 위험(x-risk)을 증가시킬 가능성이 높습니다. 대신, 정부는 AI 위험에 대한 다양한 가능한 추정치와 양립할 수 있으며, 위험이 미미하더라도 전반적으로 도움이 되는 정책을 채택해야 합니다. 다행히도 그러한 정책은 존재합니다. 정부는 또한 새로운 증거에 더 잘 반응하도록 정책 결정 과정을 변경해야 합니다. 이 모든 것에 대한 자세한 내용은 곧 알려드리겠습니다.

결론적으로, AI 정책은 추측적인 실존적 위험 예측에 의존하기보다는, 현재와 가까운 미래에 AI가 사회에 미치는 실제적이고 측정 가능한 영향에 기반해야 합니다. 이는 AI의 책임 있는 개발과 거버넌스에 초점을 맞추는 것을 의미합니다. 구체적으로, AI 시스템의 편향성, 투명성 부족, 데이터 프라이버시 침해, 노동 시장 교란, 그리고 오용 가능성(예: 사이버 보안 위협, 가짜 정보 확산)과 같은 문제들을 해결하기 위한 정책적 노력이 우선되어야 합니다. 이러한 접근 방식은 AI 기술의 혁신적인 잠재력을 활용하면서도, 잠재적 위험을 효과적으로 관리하고 대중의 신뢰를 구축하는 데 기여할 것입니다. AI의 미래는 불확실하지만, 견고한 증거와 합리적인 판단에 기반한 정책만이 우리를 올바른 방향으로 이끌 수 있을 것입니다.

### 추가 자료

AI 위험 및 거버넌스에 대한 최근 논의는 단순히 실존적 위험을 넘어 보다 광범위한 사회적, 윤리적, 경제적 영향을 다루고 있습니다. 유럽연합의 AI 법(EU AI Act)과 미국 행정부의 AI 관련 행정명령은 AI 규제에 대한 실제적인 접근 방식을 보여주는 좋은 예시입니다. 이러한 정책들은 위험 기반 접근 방식을 채택하여 AI 시스템의 위험 수준에 따라 차등적인 규제를 적용하며, 투명성, 책임성, 인간의 감독을 강조합니다.

또한, AI 안전 커뮤니티 내에서도 다양한 관점이 존재하며, 근시안적(near-term) 위험과 장기적(long-term) 위험 사이의 균형에 대한 논쟁이 계속되고 있습니다. 예를 들어, Timnit Gebru와 Emily M. Bender 등의 연구자들은 대규모 언어 모델의 사회적 편향과 환경적 영향에 대한 우려를 제기하며, 이러한 즉각적인 문제들이 실존적 위험 논의에 의해 가려져서는 안 된다고 주장합니다. 이러한 관점들은 AI 기술의 책임 있는 개발을 위한 다각적인 접근 방식의 필요성을 강조합니다.

우리는 증거 기반 AI 안전에 대해 광범위하게 글을 써왔습니다. 우리의 가장 잘 알려진 작업에는 에세이 『AI 안전은 모델 속성이 아니다(AI safety is not a model property)』와 대규모 협력의 결과물인 『오픈 파운데이션 모델의 사회적 영향에 대하여(On the Societal Impact of Open Foundation Models)』라는 제목의 논문이 포함됩니다.

**감사의 말씀.** 초고에 대한 피드백을 주신 벤자민 에델만(Benjamin Edelman), 에즈라 카거(Ezra Karger), 맷 살가닉(Matt Salganik), 올리 스테픈슨(Ollie Stephenson)께 감사드립니다. 이 에세이 시리즈는 세스 라자르(Seth Lazar)와 호주 국립대학교 MINT 연구소 구성원들, 프린스턴 대학교 예측의 한계(Limits to Prediction) 과정 학생들, 샤제다 아메드(Shazeda Ahmed), 재커리 시겔(Zachary Siegel)을 포함한 많은 분들의 피드백을 통해 완성될 예정인 논문을 기반으로 합니다.