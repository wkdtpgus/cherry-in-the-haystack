인공지능(AI) 관련 분야의 전문가들 사이에서 의견 일치가 부족한 상황에서, 정부는 AI가 초래할 수 있는 실존적 위협(existential risk, x-risk)을 어느 정도의 심각성으로 다루어야 할까요? 한편으로, 이러한 실존적 위협은 본질적으로 다분히 가설적입니다. 이는 명확한 증거가 나타날 시점에는 이미 대처하기 어려울 수 있다는 우려 때문입니다. 반대로, 정부는 정책적 우선순위를 설정해야만 합니다. 예를 들어, 외계 문명의 침공과 같은 실존적 위험에 대해 과도하게 염려하지 않는 것처럼 말입니다. 본 글은 AI 실존적 위험에 대해 고심하는 정책 결정자들을 위해 증거에 기반한 접근법을 제시하는 연속 에세이 중 첫 편입니다. 이 접근법은 '알려지지 않은 미지(unknown unknowns)'의 존재를 인정하면서도 현실적인 관점을 유지합니다. 첫 번째 에세이에서는 증거의 한 형태인 확률적 예측에 초점을 맞춥니다. AI 안전 분야는 특정 기간 내에 AI로 인한 인류 멸종 가능성(주어진 기간 내)을 예측함으로써 의사 결정 및 정책 개발에 중요한 정보를 제공하고자 합니다. 가령, 향후 수십 년 내 10%의 확률이라는 예측은 이 사안을 사회적 최우선 과제로 삼기에 충분히 높은 수치로 간주될 수 있습니다. 그러나 우리의 핵심 주장은 AI 실존적 위험에 대한 예측이 정책 결정에 활용하기에는 지나치게 불확실하며, 실제로는 상당한 오해를 불러일으킬 수 있다는 점입니다. 본 시리즈의 다음 글을 받아보시려면 구독해주십시오.

### 장막 뒤를 들여다보다

만약 우리가 향후 10년 이내에 외계 생명체가 지구에 도달할 가능성이 80%라고 전망한다면, 과연 당신은 이러한 예측을 진지하게 고려할까요? 분명히 그렇지 않을 것입니다. 당신은 그 주장에 대한 근거를 요구할 테지요. 이처럼 자명해 보이는 사실임에도 불구하고, AI 실존적 위험 논의에서는 수치화된 확률 그 자체가 지니는 권위가 종종 과대평가되는 경향이 있습니다. 보통 확률은 특정 방법론에 기반하여 산출되기 때문에, 우리는 정량화된 위험 평가를 질적 평가보다 더 신뢰하는 강력한 인지적 편향을 지니고 있습니다. 하지만 때로는 그 확률이 단순한 억측에 불과할 수도 있습니다. 본 에세이 전반에 걸쳐 (그리고 AI 실존적 위험 논의의 더 넓은 맥락에서) 이 점을 유념해 주십시오.

경마와 같은 사적인 예측의 경우, 예측가는 자신의 전망에 대한 상세한 설명을 제공할 의무가 없습니다. 듣는 사람은 그 정보를 선택적으로 수용할 수 있습니다. 그러나 정책 결정자가 특정 확률 예측에 기반하여 공공 정책을 수립하려 한다면, 그들은 시민들에게 그 결정의 근거를 명확히 설명할 책임이 있습니다. 이러한 정당성은 정부의 권한 행사와 합법성에 필수적인 요소입니다. 특히, 자유 민주주의 사회에서는 국가가 논란의 여지가 있는, 합리적인 시민들이 동의하지 않을 수 있는 신념을 바탕으로 개인의 자유를 제한해서는 안 된다는 기본 원칙이 있습니다. 이러한 설명의 필요성은 정책이 높은 비용을 수반하거나, 그 비용이 특정 집단에 불균형하게 전가될 때 더욱 커집니다. 예를 들어, AI 모델의 공개 배포를 제한하는 정책을 고려해 봅시다. 정부는 이러한 제한으로 인해 이득을 얻을 수 있는 개인과 기업들에게 단지 추측에 불과한 미래의 위험 때문에 이러한 희생을 감수해야 한다고 설득할 수 있을까요?

이러한 윤리적 난제는 잠재적인 대재앙적 위험과 기술 진보의 즉각적인 이점 및 개인의 자유 사이의 근본적인 긴장 관계를 부각합니다. 정책 입안자들은 행동해야 한다는 막대한 압력에 직면하지만, 고도로 추측적인 'x-risk'에 대한 구체적인 증거의 부재는 딜레마를 야기합니다. 사회는 가상의 미래 위험을 현재의 혁신과 경제 성장에 앞세워야 할까요? 이 질문은 AI 모델을 넘어 다른 신흥 기술에도 적용되며, 종종 예방 원칙(precautionary principle)이 혁신 동력(innovation imperative)과 충돌합니다. 더욱이, AI 개발의 전 세계적인 특성을 고려할 때, 한 국가의 일방적인 규제는 단순히 개발을 다른 곳으로 옮겨 의도된 안전 이점을 훼손하면서 기술 리더십을 양보할 수 있습니다. 따라서 이러한 추측적 위험에 기반한 모든 정책은 설명 가능해야 할 뿐만 아니라, 더 넓은 사회적 및 국제적 함의에 비추어 신중하게 고려되어야 합니다.

본 글의 핵심 목적은 정책 논의 과정에서 제시되는 특정 실존적 위험 확률 추정치가 과연 타당한 근거를 갖는지 탐구하는 것입니다. 우리는 AI 실존적 위험 예측 자체를 학문적 탐구 영역으로 보거나, 기업 및 여타 사적 주체의 의사 결정에 유용한 도구로 활용되는 것에 반대하지 않습니다. 다만, 공공 정책의 영역에서 이러한 예측이 사용되는 방식에 대해 의문을 제기하고자 합니다.

회의적인 입장의 사람들을 설득하기 위해 예측가들이 활용할 수 있는 방법은 본질적으로 세 가지로 압축됩니다: 귀납적 추론(inductive reasoning), 연역적 추론(deductive reasoning), 그리고 주관적 확률 평가(subjective probability assessment)입니다. 이어지는 섹션들에서 각 방법에 대해 상세히 다루겠습니다. 이 세 가지 접근법 모두, 양측이 세상에 대한 몇 가지 기본적인 전제(그 자체로는 증명될 수 없는)에 합의해야 합니다. 각 방식은 이러한 전제로부터 확률 추정치를 도출하는 경험적 및 논리적 절차에서 차이를 보입니다.

### 참조 집단(reference class)의 부족으로 인해 귀납적 확률 추정(inductive probability estimation)은 신뢰할 수 없습니다

대부분의 위험 평가는 귀납적 방식에 의존합니다. 이는 과거의 관찰 데이터에 근거하여 미래를 추론하는 것을 의미합니다. 예를 들어, 보험 회사들은 유사한 운전 경력을 가진 사람들의 과거 사고 기록을 활용하여 특정 개인의 자동차 사고 발생 위험을 산정합니다. 이때 확률을 추정하는 데 사용되는 관찰 데이터의 집합을 '참조 집단(reference class)'이라고 합니다. 자동차 보험의 경우, 동일한 지역에 거주하는 운전자들의 그룹이 적절한 참조 집단이 될 수 있습니다. 만약 분석가가 운전자의 연령이나 차량 종류와 같은 추가 정보를 보유하고 있다면, 참조 집단은 더 구체적으로 분류될 수 있습니다.

그러나 인공지능으로 인한 실존적 위험은 그 유례를 찾기 어렵기 때문에, 적합한 참조 집단이 존재하지 않습니다. 정확히 말하자면, 이는 본질적인 차이라기보다는 정도의 문제입니다. 완벽하게 '올바른' 참조 집단은 사실상 없으며, 어떤 집단을 선택할지는 분석가의 주관적 판단에 좌우됩니다. 예측의 정확도는 예측 대상 사건의 발생 과정과 참조 집단 내 사건들의 발생 과정 간의 유사성 정도에 따라 달라지며, 이는 연속적인 스펙트럼으로 이해될 수 있습니다.

이처럼 적절한 참조 집단의 부재는 방법론적으로 심각한 장애물입니다. 과거 데이터가 견고한 기반을 제공하는 기존 위험과는 달리, AI 실존적 위험은 근본적으로 결함이 있는 유추에 의존하는 경우가 많습니다. 예를 들어, 초지능 AI의 등장을 인쇄술의 발명이나 산업 혁명에 비유하는 것은 사회 변혁에 대한 질적 논의에는 유용할 수 있지만, 멸종 확률에 대한 정량적 데이터 포인트를 제공하지 못합니다. 지능, 자기 개선, 그리고 고급 AI 시스템의 잠재적인 목표 불일치(goal misalignment)의 독특한 특성은 과거 인간 주도의 기술 변화나 자연재해와 직접적인 유사점을 찾기 어렵게 만듭니다. 따라서 AI 실존적 위험에 귀납적 추론을 적용하려는 모든 시도는 본질적으로 추측적이며, 이러한 규모와 참신성을 가진 사건에 대한 '표본 크기'는 사실상 0입니다.

동전 던지기처럼 물리적 특성이 명확한 시스템의 결과를 예측할 때는 과거의 경험이 매우 신뢰할 만한 기준이 됩니다. 나아가 자동차 사고의 경우, 사용되는 과거 데이터셋에 따라 위험 추정치가 약 20% 정도 차이를 보일 수 있으나, 이는 보험사에게는 충분히 수용 가능한 범위입니다. 이 스펙트럼을 더 넓혀보면 지정학적 사건이 나타나는데, 여기서는 참조 집단의 선정이 훨씬 더 모호해집니다. 예측 전문가 필립 테틀록(Philip Tetlock)은 이렇게 설명합니다: "2015년 당시 유로존을 탈퇴한 국가가 없었기에 '그렉시트(Grexit)'는 매우 독특해 보였을 수 있습니다. 하지만 이를 협상 실패라는 더 넓은 범주의 비교 집단이나, 국가가 국제 협약에서 이탈하는 사례, 혹은 강제적인 통화 전환의 한 형태로 볼 수도 있습니다." 그는 소련 해체나 아랍의 봄과 같이 겉보기에는 예측 불가능한 '블랙 스완(Black Swan)' 사건들조차도 참조 집단의 일원으로 모델링될 수 있으며, 귀납적 추론이 이러한 유형의 사건에도 유용하다는 입장을 지지합니다. 테틀록이 제시하는 스펙트럼에서 이러한 사건들은 독특함의 '정점'을 대표합니다. 지정학적 사건에 대해서는 이 주장이 타당할 수 있습니다. 그러나 그러한 사건들조차 AI로 인한 인류 멸종보다는 훨씬 덜 독특한 성격을 지닙니다.

AI 실존적 위험에 대한 적절한 참조 집단을 찾으려는 시도를 살펴보십시오. 예를 들어, 동물의 멸종을 인류 멸종의 사례로 보거나, 산업 혁명과 같은 과거의 전 지구적 변혁을 AI로 인한 사회경제적 변화의 유사 사례로 들거나, 혹은 대규모 인명 피해를 야기한 사고를 전 세계적 재앙의 범주에 포함시키는 경우를 들 수 있습니다. 솔직히 말해, 이러한 어떠한 사례도 초지능 AI의 개발 가능성이나 그러한 시스템에 대한 통제 상실 가능성에 대해 우리에게 유의미한 정보를 제공하지 못합니다. 바로 이 점이 AI 실존적 위험 예측에 내재된 불확실성의 핵심 근원입니다. 결론적으로, AI로 인한 인류 멸종은 과거에 발생했던 그 어떤 사건과도 너무나 이질적인 결과이므로, 귀납적 방법론을 활용하여 그 확률을 "예측"하는 것은 불가능합니다. 물론, 과거의 기술적 진보와 재앙적 사건들로부터 질적인 통찰을 얻는 것은 가능하지만, AI 위험은 정책 수립의 정당성을 뒷받침하는 데 필요한 수준의 정량적 추정치 타당성을 확보하기에는 충분히 다른 특성을 지니고 있습니다.

### 이론의 부족으로 인해 연역적 확률 추정(deductive probability estimation)은 신뢰할 수 없습니다

아서 코난 도일(Arthur Conan Doyle)의 작품 『여섯 개의 나폴레옹 흉상(The Adventure of the Six Napoleons)』에서 — 스포일러 주의! — 탐정 셜록 홈즈(Sherlock Holmes)는 잠입 수사를 시작하기 전, 용의자를 체포할 확률이 정확히 3분의 2라고 선언합니다. 이는 다소 의아하게 들립니다. 인간의 행위와 관련된 사안이 어떻게 이처럼 수학적으로 정밀한 확률로 표현될 수 있을까요?

밝혀진 바에 따르면, 홈즈는 용의자의 겉으로는 무작위적인 행동 뒤에 숨겨진 근본적인 사건의 흐름을 연역적으로 추론했습니다. 용의자는 런던 일대의 사람들이 소유한 여섯 개의 나폴레옹 흉상 중 한 곳에 숨겨진 보석을 체계적으로 찾아다니고 있었습니다. 세부적인 내용은 부차적이며, 핵심은 용의자 자신도, 그리고 형사도 여섯 개의 흉상 중 어느 곳에 보석이 있는지는 알지 못했지만, 용의자의 다른 모든 행동은 완전히 예측 가능하다고 가정되었다는 점입니다. 이로 인해 정확하게 수량화 가능한 불확실성이 발생한 것입니다.

셜록 홈즈의 예시는 연역적 확률에 대한 이상적인 조건을 보여줍니다: 알려진 변수와 예측 가능한 행동을 가진 폐쇄 시스템입니다. 그러나 고급 AI의 개발과 영향은 이러한 시스템과는 거리가 니다. 특히 인간 수준의 지능에 도달하거나 이를 초과하는 AI 시스템은 복잡하고, 적응적이며, 불투명합니다. 이들의 내부 작동 방식은 종종 '블랙 박스(black box)'와 같아, 연역적 추론에 필요한 결정론적 인과 사슬을 확립하기 어렵습니다. 더욱이, AI가 작동하는 환경, 즉 인간 사회, 경제, 지정학은 본질적으로 개방적이고 예측 불가능합니다. AI 실존적 위험에 대한 연역적 모델을 만들려는 시도는 종종 이러한 복잡성을 비현실적일 정도로 단순화합니다. 예를 들어, 완벽하게 합리적인 AI나 AI 발전에 대한 완벽하게 예측 가능한 인간의 반응을 가정하는 것은 복잡계의 특징인 수많은 비예측적 속성과 예상치 못한 상호작용을 간과합니다. 초지능 AI의 행동과 사회적 상호작용에 대한 견고하고 보편적으로 받아들여지는 이론적 모델의 이러한 근본적인 부재는 순전히 연역적인 확률 추정을 신뢰할 수 없게 만듭니다.

핵심은 신뢰할 수 있는 세계 모델이 존재한다면, 우리는 과거의 관찰에 의존하지 않고도 논리적 연역을 통해 위험을 평가할 수 있다는 것입니다. 물론, 허구적인 상황을 벗어나 현실 세계에서는 모든 것이 그렇게 명확하지 않습니다. 특히 장기적인 미래를 예측하려 할 때는 더욱 그러합니다.

실존적 위험과 관련하여 연역적 모델의 부재라는 일반적인 원칙에는 흥미로운 예외가 있는데, 바로 소행성 충돌입니다. 이 경우 귀납적 추론과 연역적 위험 평가가 결합되어 실존적 위험의 확률을 추정할 수 있게 해줍니다. 이는 우리가 순전히 물리적인 시스템을 다루고 있기 때문에 가능합니다. 이 방법이 어떻게 작동하는지 간략히 살펴보는 것이 중요합니다. 왜냐하면 이 방식이 다른 유형의 실존적 위험에는 일반화될 수 없음을 인지해야 하기 때문입니다. 핵심은 소행성의 크기(더 정확히는 충돌 에너지)와 충돌 빈도 사이의 관계를 모델링할 수 있다는 점입니다. 수많은 작은 충돌 사례를 관찰했으므로, 우리는 직접 목격되지 않은 대규모 충돌의 빈도를 추론하기 위해 외삽할 수 있습니다. 나아가 전 지구적 재앙을 초래할 임계값 또한 추정할 수 있습니다.

1 그림: 작은 소행성 충돌 데이터(왼쪽 그림)는 멸종 수준의 충돌(오른쪽)로 외삽될 수 있습니다.

AI의 맥락에서는 미지수가 물리적 시스템보다는 기술적 진보와 거버넌스(governance) 측면에 집중되어 있어, 이를 수학적으로 모델링하는 명확한 방법론이 부재합니다. 그럼에도 불구하고 시도는 계속되어 왔습니다. 예를 들어, 가상의 일반 인공지능(AGI)이 필요로 할 계산량을 예측하기 위해, 일부 연구들은 AI 시스템이 인간 두뇌와 거의 동등한 수준의 연산 능력을 요구할 것이라고 가정하며, 더 나아가 인간 두뇌의 연산량에 대한 추정치를 사용합니다. 이러한 가정들은 소행성 모델링에 사용되는 가정들에 비해 훨씬 더 취약하며, 그 어떤 것도 통제 상실이라는 핵심적인 문제를 제대로 다루지 못합니다.

AI 실존적 위험을 연역적으로 모델링하는 어려움은 고급 AI 개발에 내재된 '알려지지 않은 미지'에 의해 더욱 가중됩니다. 우리는 단순히 알려진 시스템이 새로운 조건에서 어떻게 작동할지를 예측하는 것이 아니라, 아직 존재하지 않는 시스템의 존재와 능력을 추측하고 있습니다. '초지능(superintelligence)'이라는 개념 자체는 측정 가능한 매개변수를 가진 잘 정의된 과학적 용어가 아니라, 오히려 추측적인 구성물입니다. 따라서 이처럼 불분명한 기반 위에 구축된 모든 연역적 모델은 매우 불확실하고 잠재적으로 오해의 소지가 있는 결과를 낳을 수밖에 없습니다. 근본적인 법칙이 연역의 안정적인 기반을 제공하는 물리학과는 달리, AI는 심지어 '법칙'(예: 스케일링 법칙)조차도 공리적 진리가 아닌 경험적 관찰인 빠르게 진화하는 분야입니다. 이는 AI 실존적 위험에 대한 견고한 연역적 모델을 구축하는 것을 불가능하지는 않더라도 엄청나게 어려운 과제로 만듭니다.

### 주관적 확률(subjective probabilities)은 숫자로 위장한 감정입니다

적절한 참조 집단이나 확고한 이론적 기반이 부재할 경우, 예측은 필연적으로 '주관적 확률(subjective probabilities)', 즉 예측가의 개인적인 판단에 의존하는 추측이 됩니다. 놀랍게도 이러한 추정치들은 자릿수(orders of magnitude) 단위로 큰 차이를 보입니다. 주관적 확률 평가는 확률 추정치에 대한 귀납적 또는 연역적 근거를 제시해야 할 필요성을 회피하지 못합니다. 단지 예측가가 자신의 추정치를 명확히 설명해야 하는 부담을 덜어줄 뿐입니다. 인간이 직관적 추론(귀납적이든 연역적이든, 혹은 그 조합이든)의 과정을 설명하는 능력에 한계가 있기 때문에 이러한 설명은 어려울 수 있습니다. 본질적으로, 이는 예측가로 하여금 "내 방법론을 상세히 밝히지는 않았지만, 나의 과거 실적을 고려할 때 이 추정치를 신뢰할 수 있습니다"라고 주장할 수 있게 합니다 (다음 섹션에서는 AI 실존적 위험 예측의 경우 이러한 주장이 왜 설득력을 잃는지 설명할 것입니다). 그러나 궁극적으로 귀납적 또는 연역적 근거가 없다면, 예측가들이 할 수 있는 일은 숫자를 임의로 만들어내는 것뿐이며, 그 결과로 생성된 숫자들은 저마다 다르게 나타납니다.

2022년 말 예측 연구소(Forecasting Research Institute)가 주최한 실존적 위험 설득 토너먼트(Existential Risk Persuasion Tournament, XPT)의 사례를 살펴보겠습니다. 우리는 이 대회가 현재까지 진행된 실존적 위험 예측 훈련 중 가장 정교하고 체계적으로 수행된 것이라고 판단합니다. 이 행사에는 AI 전문가와 예측 전문가(이른바 '슈퍼 예측가(superforecasters)' 참조)를 포함한 다수의 예측가 집단이 참여했습니다. AI 전문가들의 경우, 2100년까지 AI로 인한 멸종 위험 추정치에서 상위 75%는 12%, 중앙값은 3%, 하위 25%는 0.25%를 기록했습니다. 반면 예측 전문가들의 경우, 상위 75%조차 1%에 불과했고, 중앙값은 단 0.38%였으며, 하위 25%는 그래프 상에서 0과 사실상 구분하기 어려웠습니다. 즉, AI 전문가 집단의 75% 추정치와 슈퍼 예측가 집단의 25% 추정치 사이에는 최소 100배 이상의 현격한 차이가 존재했습니다. 이 모든 수치는 해당 분야에 대한 깊은 전문성을 갖추고 수개월에 걸쳐 서로를 설득하려 노력한 참가자들로부터 도출된 것입니다!

전문가들 사이에서도 주관적 확률이 광범위하게 diverge하는 현상은 AI 실존적 위험을 둘러싼 심오한 불확실성을 강조합니다. 이는 단순히 약간의 의견 불일치 문제가 아니라, 세계관과 가정의 근본적인 차이 문제입니다. 이는 신생 분야나 진정으로 새로운 현상을 다룰 때 드물지 않게 발생합니다. 그러나 정책 결정에 있어 이러한 전문가 의견의 분산은 문제가 됩니다. 정부는 일반적으로 상당한 개입을 정당화하기 위해 합의 또는 적어도 좁은 범위의 그럴듯한 결과를 추구합니다. '전문가'들조차 여러 자릿수 범위 내에서 합의하지 못할 때, 이는 근본적인 모델, 데이터 또는 심지어 개념적 프레임워크가 공공 정책 적용에 충분히 견고하지 않다는 신호입니다. 이러한 상황은 고도로 이질적이고 추측적인 수치에 기반한 급진적인 조치를 서두르기보다는 겸손하고 신중한 정책 접근 방식을 요구합니다.

만약 이 예측 범위가 충분히 극단적이지 않다고 느껴진다면, 이 모든 연습이 특정 시점에 단일 그룹에 의해 진행되었다는 점을 상기해야 합니다. 만약 이 토너먼트가 오늘 다시 개최되거나 질문의 구성이 달라졌다면, 우리는 또 다른 수치를 얻을 수도 있었을 것입니다. 가장 중요한 것은 예측가들이 제시한 근거를 검토하는 것으로, 이는 보고서에 상세히 기술되어 있습니다. 그들은 특히 강력한 AI 개발 시 발생할 수 있는 부정적인 결과의 가능성을 논할 때 정량적 모델을 사용하지 않습니다. 대부분의 경우, 예측가들은 초지능 AI에 대해 논의할 때 일반 대중이 하는 것과 유사한 종류의 추측에 참여하고 있습니다. 예를 들어, AI가 초인적인 설득 능력을 통해 중요한 시스템을 장악할 수도 있고, 혹은 컴퓨터 작동 효율을 높이려 지구 온도를 낮추려다가 실수로 인류를 멸망시킬 수도 있다는 식입니다. 아니면 AI가 지구 대신 우주에서 자원을 탐색할 것이므로 우리가 크게 걱정할 필요가 없을 것이라는 전망도 있습니다. 이러한 추측 자체에 문제가 있는 것은 아닙니다. 그러나 AI 실존적 위험과 관련하여 예측가들이 여러분이나 우리, 또는 다른 누구의 직감보다 그들의 직감을 더 신뢰할 수 있게 만드는 특별한 지식, 증거 또는 모델에 기반하고 있지 않다는 사실은 분명히 인지해야 합니다.

'슈퍼 예측(superforecasting)'이라는 용어는 필립 테틀록(Philip Tetlock)의 20년간에 걸친 예측 연구에서 유래했습니다 (그는 XPT의 공동 주최자 중 한 명이기도 했습니다). 슈퍼 예측가들은 다양한 정보를 통합하고 심리적 편향을 최소화하는 등 예측 정확도를 높이는 훈련을 받습니다. 이러한 기법은 지정학(geopolitics)과 같은 분야에서 그 효과가 입증되었습니다. 하지만 활용할 만한 유용한 증거가 부족하다면, 아무리 잘 훈련된 예측가라도 좋은 예측을 내놓기 어렵습니다. 예측가들이 설령 신뢰할 만한 정량적 모델을 가지고 있다고 하더라도 (실제로는 그렇지 않습니다), 그들은 '알려지지 않은 미지의 것(unknown unknowns)', 즉 모델 자체의 오류 가능성까지 고려해야 합니다. 저명한 실존적 위험 철학자 닉 보스트롬(Nick Bostrom)은 다음과 같이 설명합니다: "우리의 초기 위험 평가에 내재된 불확실성과 오류 가능성은 모든 요소를 고려한 확률 배정에 반드시 포함되어야 할 요소입니다. 이 요소는 종종 발생 확률은 낮지만 결과가 심각한 위험, 특히 잘 이해되지 않는 자연 현상, 복잡한 사회 역학, 또는 새로운 기술과 관련된 위험, 또는 다른 이유로 평가하기 어려운 위험에서 지배적인 역할을 합니다."

'알려지지 않은 미지'의 개념은 AI 실존적 위험에서 특히 두드러지는데, 미래 고급 AI의 본질 자체가 여전히 대부분 이론적이기 때문입니다. 우리는 단순히 알려진 시스템이 새로운 조건에서 어떻게 작동할지를 예측하는 것이 아니라, 아직 존재하지 않는 시스템의 존재와 능력을 추측하고 있습니다. 이는 복잡한 지정학적 사건을 위해 정제된 예측 방법론조차도 부적합하게 만듭니다. 핵심 과제는 단순히 이질적인 데이터를 통합하는 것이 아니라, 근본적인 인식론적 한계와 씨름하는 것입니다: 전례 없는, 그리고 근본적인 메커니즘이 아직 발명되고 있는 것을 어떻게 신뢰할 수 있게 예측할 수 있을까요? 이는 정확한 확률적 예측보다는 견고성과 적응성을 우선시하는 위험 관리 전략의 필요성을 시사하며, 우리의 모델이 필연적으로 불완전할 것임을 인정합니다.

이는 합리적인 관점이며, 실제로 AI 실존적 위험을 예측하는 이들은 위험 평가의 불확실성에 대해 깊이 우려합니다. 그러나 이러한 원칙을 따르는 사람들에게는 예측이 모델의 산출물이라기보다는 본질적으로 추측에 불과할 수밖에 없다는 결론이 도출됩니다. 결국, 모델 자체의 오류 가능성이나 모델이 틀렸을 때의 위험을 추정하기 위해 어떠한 모델도 사용될 수 없기 때문입니다.

### 독특하거나 희귀한 사건의 경우 예측 능력은 측정할 수 없습니다

요약하자면, AI 위험에 대한 주관적인 예측들은 그 규모에서 자릿수 단위로 큰 편차를 보입니다. 하지만 만약 예측가들의 과거 실적을 평가할 수 있다면, 우리는 어떤 예측가를 더 신뢰해야 할지 판단할 수 있을 것입니다. 위험 추정치를 정당화하는 이전의 두 가지 접근 방식(귀납적 및 연역적)과는 달리, 예측가는 자신의 추정치를 직접 설명할 필요 없이, 대신 과거에 다른 결과들을 예측하는 데 입증된 능력을 근거로 정당성을 주장합니다.

이러한 방식은 지정학적 사건 영역에서 매우 유용함이 입증되었으며, 예측 커뮤니티는 예측 능력 측정에 상당한 노력을 기울입니다. 보정(calibration), 브라이어 점수(Brier score), 로그 점수(logarithmic score), 그리고 예측 경쟁 플랫폼인 메타큘러스(Metaculus)에서 활용되는 피어 점수(Peer score)와 같이 예측 능력을 평가하는 다양한 방법론이 존재합니다. 그러나 어떤 평가 방법을 적용하더라도, 실존적 위험과 관련된 주관적 확률 예측 능력을 평가하는 데는 여러 가지 난관이 따릅니다. 주요 장애물로는 참조 집단의 부재, 낮은 기본 발생률(base rate), 그리고 매우 긴 시간 지평(time horizon)이 있습니다. 이 세 가지 요소를 순서대로 살펴보겠습니다.

참조 집단 문제(reference class problem)가 예측가들에게 난제로 작용하는 것처럼, 평가자들에게도 동일한 영향을 미칩니다. 다시 외계인 착륙의 예를 들어봅시다. 선거 예측에서 매우 높은 정확도를 보인 예측가가 있다고 가정해 봅시다. 이 예측가가 아무런 근거 없이 1년 안에 외계인이 지구에 착륙할 것이라고 주장한다면 어떨까요? 그 예측가의 검증된 능력에도 불구하고, 우리는 외계인 착륙에 대한 우리의 믿음을 수정하지 않을 것입니다. 이는 외계인 착륙이 선거 예측과는 매우 다른 성격의 사건이며, 예측가의 능력이 이러한 이질적인 영역까지 일반화될 것이라고 기대하지 않기 때문입니다. 마찬가지로, AI 실존적 위험은 과거에 예측되었던 그 어떤 사건과도 너무나 이질적이기에, AI 실존적 위험을 추정하는 예측가의 능력을 입증할 증거가 부재합니다.

'기본 발생률(base rate)' 문제는 또 다른 심각한 장애물입니다. AI 실존적 위험이 정의상 극히 드문 사건(멸종)을 의미하기 때문에, 확률을 약간이라도 과대평가하면 정책적 함의가 불균형적으로 커질 수 있습니다. 희귀 질병을 연구하는 의학 분야에서는 정확한 기본 발생률을 확립하기 위해 광범위한 데이터 수집과 종단 연구가 필요합니다. AI 실존적 위험에 대해서는 그러한 데이터가 존재하지 않습니다. 이는 주어진 예측(낮은 예측이라 할지라도)이 진정으로 그럴듯한 것인지, 아니면 단순히 예측가의 높은 영향력과 낮은 확률 사건을 인지하는 경향에 대한 내재된 편향을 반영하는 것인지를 식별하는 것을 거의 불가능하게 만듭니다. 더욱이, '긴 시간 지평'은 예측가 보정(forecaster calibration)을 위한 피드백 루프가 사실상 존재하지 않는다는 것을 의미합니다. 우리는 예측이 정확했는지 확인하기 위해 수세기를 기다릴 수 없으므로, x-risk에 대한 전통적인 예측 능력 평가 방법은 무의미합니다.

참조 집단 문제를 설령 해결한다 하더라도, 다른 문제들이 여전히 남아 있습니다. 특히, 멸종 위험이 '꼬리 위험(tail risks)', 즉 극히 드물게 발생하는 사건과 연관되어 있다는 사실입니다. 예측가 A가 AI 실존적 위험의 확률을 1%로, 예측가 B가 100만분의 1로 제시한다고 가정해 봅시다. 우리는 어떤 예측을 더 신뢰해야 할까요? 이들의 과거 실적을 검토할 수 있습니다. 만약 AI 실존적 위험에 1%의 확률을 부여한 예측가 A가 더 나은 실적을 가졌다고 하더라도, 이것이 우리가 A의 예측을 더 신뢰해야 함을 의미하지는 않습니다. 그 이유는 예측 능력 평가가 꼬리 위험의 과대평가에 대해 둔감하기 때문입니다. 다시 말해, A는 발생 확률이 높은 일반적인 사건에 대해서는 B보다 약간 더 잘 보정(calibrated)되어 있어 전반적으로 더 높은 점수를 받을 수 있지만, 100만분의 1과 같이 드물게 발생하는 꼬리 위험을 자릿수 단위로 과대평가하는 경향이 있을 수 있습니다. 어떠한 점수 규칙도 이러한 종류의 오보정(miscalibration)을 적절히 제재하지 못합니다.

이러한 현상이 발생하는 이유를 설명하는 사고 실험이 있습니다. 두 예측가 F와 G가 각각 다른 사건 집합을 예측하며, 두 집합 내 사건들의 '실제' 확률이 0과 1 사이에 균등하게 분포되어 있다고 가정해 봅시다. 우리는 매우 낙관적으로, F와 G가 예측하는 모든 사건 e에 대한 실제 확률 P[e]를 알고 있다고 전제합니다. F는 항상 P[e]를 그대로 예측하는 반면, G는 다소 보수적인 경향이 있어 1% 미만의 값은 예측하지 않습니다. 즉, G는 P[e]가 1% 이상일 때는 P[e]를 예측하고, 그렇지 않을 때는 1%를 예측합니다. 본질적으로 F가 더 우수한 예측가입니다. 하지만 이러한 우수성이 그들의 실적에서 명확하게 드러날까요? 다시 말해, F가 G보다 더 높은 점수를 받을 확률이 95%에 도달하려면 각각의 예측을 얼마나 많이 평가해야 할까요? 로그 점수 규칙(logarithmic scoring rule)을 적용하면 약 1억 개의 평가가 필요하며, 브