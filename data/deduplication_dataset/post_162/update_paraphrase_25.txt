인공지능(AI) 관련 분야의 전문가들 사이에서 의견 일치가 부족한 상황에서, 정부는 AI가 초래할 수 있는 실존적 위협(existential risk, x-risk)을 어느 정도의 심각성으로 다루어야 할까요? 한편으로, 이러한 실존적 위협은 본질적으로 다분히 가설적입니다. 이는 명확한 증거가 나타날 시점에는 이미 대처하기 어려울 수 있다는 우려 때문입니다. 반대로, 정부는 정책적 우선순위를 설정해야만 합니다. 예를 들어, 외계 문명의 침공과 같은 실존적 위험에 대해 과도하게 염려하지 않는 것처럼 말입니다. 본 글은 AI 실존적 위험에 대해 고심하는 정책 결정자들을 위해 증거에 기반한 접근법을 제시하는 연속 에세이 중 첫 편입니다. 이 접근법은 '알려지지 않은 미지(unknown unknowns)'의 존재를 인정하면서도 현실적인 관점을 유지합니다. 첫 번째 에세이에서는 증거의 한 형태인 확률적 예측에 초점을 맞춥니다. AI 안전 분야는 특정 기간 내에 AI로 인한 인류 멸종 가능성(주어진 기간 내)을 예측함으로써 의사 결정 및 정책 개발에 중요한 정보를 제공하고자 합니다. 가령, 향후 수십 년 내 10%의 확률이라는 예측은 이 사안을 사회적 최우선 과제로 삼기에 충분히 높은 수치로 간주될 수 있습니다. 그러나 우리의 핵심 주장은 AI 실존적 위험에 대한 예측이 정책 결정에 활용하기에는 지나치게 불확실하며, 실제로는 상당한 오해를 불러일으킬 수 있다는 점입니다. 본 시리즈의 다음 글을 받아보시려면 구독해주십시오.

### 장막 뒤를 들여다보다

만약 우리가 향후 10년 이내에 외계 생명체가 지구에 도달할 가능성이 80%라고 전망한다면, 과연 당신은 이러한 예측을 진지하게 고려할까요? 분명히 그렇지 않을 것입니다. 당신은 그 주장에 대한 근거를 요구할 테지요. 이처럼 자명해 보이는 사실임에도 불구하고, AI 실존적 위험 논의에서는 수치화된 확률 그 자체가 지니는 권위가 종종 과대평가되는 경향이 있습니다. 보통 확률은 특정 방법론에 기반하여 산출되기 때문에, 우리는 정량화된 위험 평가를 질적 평가보다 더 신뢰하는 강력한 인지적 편향을 지니고 있습니다. 하지만 때로는 그 확률이 단순한 억측에 불과할 수도 있습니다. 본 에세이 전반에 걸쳐 (그리고 AI 실존적 위험 논의의 더 넓은 맥락에서) 이 점을 유념해 주십시오.

경마와 같은 사적인 예측의 경우, 예측가는 자신의 전망에 대한 상세한 설명을 제공할 의무가 없습니다. 듣는 사람은 그 정보를 선택적으로 수용할 수 있습니다. 그러나 정책 결정자가 특정 확률 예측에 기반하여 공공 정책을 수립하려 한다면, 그들은 시민들에게 그 결정의 근거를 명확히 설명할 책임이 있습니다. 이러한 정당성은 정부의 권한 행사와 합법성에 필수적인 요소입니다. 특히, 자유 민주주의 사회에서는 국가가 논란의 여지가 있는, 합리적인 시민들이 동의하지 않을 수 있는 신념을 바탕으로 개인의 자유를 제한해서는 안 된다는 기본 원칙이 있습니다. 이러한 설명의 필요성은 정책이 높은 비용을 수반하거나, 그 비용이 특정 집단에 불균형하게 전가될 때 더욱 커집니다. 예를 들어, AI 모델의 공개 배포를 제한하는 정책을 고려해 봅시다. 정부는 이러한 제한으로 인해 이득을 얻을 수 있는 개인과 기업들에게 단지 추측에 불과한 미래의 위험 때문에 이러한 희생을 감수해야 한다고 설득할 수 있을까요?

이러한 윤리적 난제는 잠재적인 대재앙적 위험과 기술 진보의 즉각적인 이점 및 개인의 자유 사이의 근본적인 긴장 관계를 부각합니다. 정책 입안자들은 행동해야 한다는 막대한 압력에 직면하지만, 고도로 추측적인 'x-risk'에 대한 구체적인 증거의 부재는 딜레마를 야기합니다. 사회는 가상의 미래 위험을 현재의 혁신과 경제 성장에 앞세워야 할까요? 이 질문은 AI 모델을 넘어 다른 신흥 기술에도 적용되며, 종종 예방 원칙(precautionary principle)이 혁신 동력(innovation imperative)과 충돌합니다. 더욱이, AI 개발의 전 세계적인 특성을 고려할 때, 한 국가의 일방적인 규제는 단순히 개발을 다른 곳으로 옮겨 의도된 안전 이점을 훼손하면서 기술 리더십을 양보할 수 있습니다. 따라서 이러한 추측적 위험에 기반한 모든 정책은 설명 가능해야 할 뿐만 아니라, 더 넓은 사회적 및 국제적 함의에 비추어 신중하게 고려되어야 합니다.

본 글의 핵심 목적은 정책 논의 과정에서 제시되는 특정 실존적 위험 확률 추정치가 과연 타당한 근거를 갖는지 탐구하는 것입니다. 우리는 AI 실존적 위험 예측 자체를 학문적 탐구 영역으로 보거나, 기업 및 여타 사적 주체의 의사 결정에 유용한 도구로 활용되는 것에 반대하지 않습니다. 다만, 공공 정책의 영역에서 이러한 예측이 사용되는 방식에 대해 의문을 제기하고자 합니다.

회의적인 입장의 사람들을 설득하기 위해 예측가들이 활용할 수 있는 방법은 본질적으로 세 가지로 압축됩니다: 귀납적 추론(inductive reasoning), 연역적 추론(deductive reasoning), 그리고 주관적 확률 평가(subjective probability assessment)입니다. 이어지는 섹션들에서 각 방법에 대해 상세히 다루겠습니다. 이 세 가지 접근법 모두, 양측이 세상에 대한 몇 가지 기본적인 전제(그 자체로는 증명될 수 없는)에 합의해야 합니다. 각 방식은 이러한 전제로부터 확률 추정치를 도출하는 경험적 및 논리적 절차에서 차이를 보입니다.

### 참조 집단(reference class)의 부족으로 인해 귀납적 확률 추정(inductive probability estimation)은 신뢰할 수 없습니다

대부분의 위험 평가는 귀납적 방식에 의존합니다. 이는 과거의 관찰 데이터에 근거하여 미래를 추론하는 것을 의미합니다. 예를 들어, 보험 회사들은 유사한 운전 경력을 가진 사람들의 과거 사고 기록을 활용하여 특정 개인의 자동차 사고 발생 위험을 산정합니다. 이때 확률을 추정하는 데 사용되는 관찰 데이터의 집합을 '참조 집단(reference class)'이라고 합니다. 자동차 보험의 경우, 동일한 지역에 거주하는 운전자들의 그룹이 적절한 참조 집단이 될 수 있습니다. 만약 분석가가 운전자의 연령이나 차량 종류와 같은 추가 정보를 보유하고 있다면, 참조 집단은 더 구체적으로 분류될 수 있습니다.

그러나 인공지능으로 인한 실존적 위험은 그 유례를 찾기 어렵기 때문에, 적합한 참조 집단이 존재하지 않습니다. 정확히 말하자면, 이는 본질적인 차이라기보다는 정도의 문제입니다. 완벽하게 '올바른' 참조 집단은 사실상 없으며, 어떤 집단을 선택할지는 분석가의 주관적 판단에 좌우됩니다. 예측의 정확도는 예측 대상 사건의 발생 과정과 참조 집단 내 사건들의 발생 과정 간의 유사성 정도에 따라 달라지며, 이는 연속적인 스펙트럼으로 이해될 수 있습니다.

이처럼 적절한 참조 집단의 부재는 방법론적으로 심각한 장애물입니다. 과거 데이터가 견고한 기반을 제공하는 기존 위험과는 달리, AI 실존적 위험은 근본적으로 결함이 있는 유추에 의존하는 경우가 많습니다. 예를 들어, 초지능 AI의 등장을 인쇄술의 발명이나 산업 혁명에 비유하는 것은 사회 변혁에 대한 질적 논의에는 유용할 수 있지만, 멸종 확률에 대한 정량적 데이터 포인트를 제공하지 못합니다. 지능, 자기 개선, 그리고 고급 AI 시스템의 잠재적인 목표 불일치(goal misalignment)의 독특한 특성은 과거 인간 주도의 기술 변화나 자연재해와 직접적인 유사점을 찾기 어렵게 만듭니다. 따라서 AI 실존적 위험에 귀납적 추론을 적용하려는 모든 시도는 본질적으로 추측적이며, 이러한 규모와 참신성을 가진 사건에 대한 '표본 크기'는 사실상 0입니다.

동전 던지기처럼 물리적 특성이 명확한 시스템의 결과를 예측할 때는 과거의 경험이 매우 신뢰할 만한 기준이 됩니다. 나아가 자동차 사고의 경우, 사용되는 과거 데이터셋에 따라 위험 추정치가 약 20% 정도 차이를 보일 수 있으나, 이는 보험사에게는 충분히 수용 가능한 범위입니다. 이 스펙트럼을 더 넓혀보면 지정학적 사건이 나타나는데, 여기서는 참조 집단의 선정이 훨씬 더 모호해집니다. 예측 전문가 필립 테틀록(Philip Tetlock)은 이렇게 설명합니다: "2015년 당시 유로존을 탈퇴한 국가가 없었기에 '그렉시트(Grexit)'는 매우 독특해 보였을 수 있습니다. 하지만 이를 협상 실패라는 더 넓은 범주의 비교 집단이나, 국가가 국제 협약에서 이탈하는 사례, 혹은 강제적인 통화 전환의 한 형태로 볼 수도 있습니다." 그는 소련 해체나 아랍의 봄과 같이 겉보기에는 예측 불가능한 '블랙 스완(Black Swan)' 사건들조차도 참조 집단의 일원으로 모델링될 수 있으며, 귀납적 추론이 이러한 유형의 사건에도 유용하다는 입장을 지지합니다. 테틀록이 제시하는 스펙트럼에서 이러한 사건들은 독특함의 '정점'을 대표합니다. 지정학적 사건에 대해서는 이 주장이 타당할 수 있습니다. 그러나 그러한 사건들조차 AI로 인한 인류 멸종보다는 훨씬 덜 독특한 성격을 지닙니다.

AI 실존적 위험에 대한 적절한 참조 집단을 찾으려는 시도를 살펴보십시오. 예를 들어, 동물의 멸종을 인류 멸종의 사례로 보거나, 산업 혁명과 같은 과거의 전 지구적 변혁을 AI로 인한 사회경제적 변화의 유사 사례로 들거나, 혹은 대규모 인명 피해를 야기한 사고를 전 세계적 재앙의 범주에 포함시키는 경우를 들 수 있습니다. 솔직히 말해, 이러한 어떠한 사례도 초지능 AI의 개발 가능성이나 그러한 시스템에 대한 통제 상실 가능성에 대해 우리에게 유의미한 정보를 제공하지 못합니다. 바로 이 점이 AI 실존적 위험 예측에 내재된 불확실성의 핵심 근원입니다. 결론적으로, AI로 인한 인류 멸종은 과거에 발생했던 그 어떤 사건과도 너무나 이질적인 결과이므로, 귀납적 방법론을 활용하여 그 확률을 "예측"하는 것은 불가능합니다. 물론, 과거의 기술적 진보와 재앙적 사건들로부터 질적인 통찰을 얻는 것은 가능하지만, AI 위험은 정책 수립의 정당성을 뒷받침하는 데 필요한 수준의 정량적 추정치 타당성을 확보하기에는 충분히 다른 특성을 지니고 있습니다.

### 이론의 부족으로 인해 연역적 확률 추정(deductive probability estimation)은 신뢰할 수 없습니다

아서 코난 도일(Arthur Conan Doyle)의 작품 『여섯 개의 나폴레옹 흉상(The Adventure of the Six Napoleons)』에서 — 스포일러 주의! — 탐정 셜록 홈즈(Sherlock Holmes)는 잠입 수사를 시작하기 전, 용의자를 체포할 확률이 정확히 3분의 2라고 선언합니다. 이는 다소 의아하게 들립니다. 인간의 행위와 관련된 사안이 어떻게 이처럼 수학적으로 정밀한 확률로 표현될 수 있을까요?

밝혀진 바에 따르면, 홈즈는 용의자의 겉으로는 무작위적인 행동 뒤에 숨겨진 근본적인 사건의 흐름을 연역적으로 추론했습니다. 용의자는 런던 일대의 사람들이 소유한 여섯 개의 나폴레옹 흉상 중 한 곳에 숨겨진 보석을 체계적으로 찾아다니고 있었습니다. 세부적인 내용은 부차적이며, 핵심은 용의자 자신도, 그리고 형사도 여섯 개의 흉상 중 어느 곳에 보석이 있는지는 알지 못했지만, 용의자의 다른 모든 행동은 완전히 예측 가능하다고 가정되었다는 점입니다. 이로 인해 정확하게 수량화 가능한 불확실성이 발생한 것입니다.

셜록 홈즈의 예시는 연역적 확률에 대한 이상적인 조건을 보여줍니다: 알려진 변수와 예측 가능한 행동을 가진 폐쇄 시스템입니다. 그러나 고급 AI의 개발과 영향은 이러한 시스템과는 거리가 니다. 특히 인간 수준의 지능에 도달하거나 이를 초과하는 AI 시스템은 복잡하고, 적응적이며, 불투명합니다. 이들의 내부 작동 방식은 종종 '블랙 박스(black box)'와 같아, 연역적 추론에 필요한 결정론적 인과 사슬을 확립하기 어렵습니다. 더욱이, AI가 작동하는 환경, 즉 인간 사회, 경제, 지정학은 본질적으로 개방적이고 예측 불가능합니다. AI 실존적 위험에 대한 연역적 모델을 만들려는 시도는 종종 이러한 복잡성을 비현실적일 정도로 단순화합니다. 예를 들어, 완벽하게 합리적인 AI나 AI 발전에 대한 완벽하게 예측 가능한 인간의 반응을 가정하는 것은 복잡계의 특징인 수많은 비예측적 속성과 예상치 못한 상호작용을 간과합니다. 초지능 AI의 행동과 사회적 상호작용에 대한 견고하고 보편적으로 받아들여지는 이론적 모델의 이러한 근본적인 부재는 순전히 연역적인 확률 추정을 신뢰할 수 없게 만듭니다.

핵심은 신뢰할 수 있는 세계 모델이 존재한다면, 우리는 과거의 관찰에 의존하지 않고도 논리적 연역을 통해 위험을 평가할 수 있다는 것입니다. 물론, 허구적인 상황을 벗어나 현실 세계에서는 모든 것이 그렇게 명확하지 않습니다. 특히 장기적인 미래를 예측하려 할 때는 더욱 그러합니다.

실존적 위험과 관련하여 연역적 모델의 부재라는 일반적인 원칙에는 흥미로운 예외가 있는데, 바로 소행성 충돌입니다. 이 경우 귀납적 추론과 연역적 위험 평가가 결합되어 실존적 위험의 확률을 추정할 수 있게 해줍니다. 이는 우리가 순전히 물리적인 시스템을 다루고 있기 때문에 가능합니다. 이 방법이 어떻게 작동하는지 간략히 살펴보는 것이 중요합니다. 왜냐하면 이 방식이 다른 유형의 실존적 위험에는 일반화될 수 없음을 인지해야 하기 때문입니다. 핵심은 소행성의 크기(더 정확히는 충돌 에너지)와 충돌 빈도 사이의 관계를 모델링할 수 있다는 점입니다. 수많은 작은 충돌 사례를 관찰했으므로, 우리는 직접 목격되지 않은 대규모 충돌의 빈도를 추론하기 위해 외삽할 수 있습니다. 나아가 전 지구적 재앙을 초래할 임계값 또한 추정할 수 있습니다.

1 그림: 작은 소행성 충돌 데이터(왼쪽 그림)는 멸종 수준의 충돌(오른쪽)로 외삽될 수 있습니다.

AI의 맥락에서는 미지수가 물리적 시스템보다는 기술적 진보와 거버넌스(governance) 측면에 집중되어 있어, 이를 수학적으로 모델링하는 명확한 방법론이 부재합니다. 그럼에도 불구하고 시도는 계속되어 왔습니다. 예를 들어, 가상의 일반 인공지능(AGI)이 필요로 할 계산량을 예측하기 위해, 일부 연구들은 AI 시스템이 인간 두뇌와 거의 동등한 수준의 연산 능력을 요구할 것이라고 가정하며, 더 나아가 인간 두뇌의 연산량에 대한 추정치를 사용합니다. 이러한 가정들은 소행성 모델링에 사용되는 가정들에 비해 훨씬 더 취약하며, 그 어떤 것도 통제 상실이라는 핵심적인 문제를 제대로 다루지 못합니다.

AI 실존적 위험을 연역적으로 모델링하는 어려움은 고급 AI 개발에 내재된 '알려지지 않은 미지'에 의해 더욱 가중됩니다. 우리는 단순히 알려진 시스템이 새로운 조건에서 어떻게 작동할지를 예측하는 것이 아니라, 아직 존재하지 않는 시스템의 존재와 능력을 추측하고 있습니다. '초지능(superintelligence)'이라는 개념 자체는 측정 가능한 매개변수를 가진 잘 정의된 과학적 용어가 아니라, 오히려 추측적인 구성물입니다. 따라서 이처럼 불분명한 기반 위에 구축된 모든 연역적 모델은 매우 불확실하고 잠재적으로 오해의 소지가 있는 결과를 낳을 수밖에 없습니다. 근본적인 법칙이 연역의 안정적인 기반을 제공하는 물리학과는 달리, AI는 심지어 '법칙'(예: 스케일링 법칙)조차도 공리적 진리가 아닌 경험적 관찰인 빠르게 진화하는 분야입니다. 이는 AI 실존적 위험에 대한 견고한 연역적 모델을 구축하는 것을 불가능하지는 않더라도 엄청나게 어려운 과제로 만듭니다.

### 주관적 확률(subjective probabilities)은 숫자로 위장한 감정입니다

적절한 참조 집단이나 확고한 이론적 기반이 부재할 경우, 예측은 필연적으로 '주관적 확률(subjective probabilities)', 즉 예측가의 개인적인 판단에 의존하는 추측이 됩니다. 놀랍게도 이러한 추정치들은 자릿수(orders of magnitude) 단위로 큰 차이를 보입니다. 주관적 확률 평가는 확률 추정치에 대한 귀납적 또는 연역적 근거를 제시해야 할 필요성을 회피하지 못합니다. 단지 예측가가 자신의 추정치를 명확히 설명해야 하는 부담을 덜어줄 뿐입니다. 인간이 직관적 추론(귀납적이든 연역적이든, 혹은 그 조합이든)의 과정을 설명하는 능력에 한계가 있기 때문에 이러한 설명은 어려울 수 있습니다. 본질적으로, 이는 예측가로 하여금 "내 방법론을 상세히 밝히지는 않았지만, 나의 과거 실적을 고려할 때 이 추정치를 신뢰할 수 있습니다"라고 주장할 수 있게 합니다 (다음 섹션에서는 AI 실존적 위험 예측의 경우 이러한 주장이 왜 설득력을 잃는지 설명할 것입니다). 그러나 궁극적으로 귀납적 또는 연역적 근거가 없다면, 예측가들이 할 수 있는 일은 숫자를 임의로 만들어내는 것뿐이며, 그 결과로 생성된 숫자들은 저마다 다르게 나타납니다.

2022년 말 예측 연구소(Forecasting Research Institute)가 주최한 실존적 위험 설득 토너먼트(Existential Risk Persuasion Tournament, XPT)의 사례를 살펴보겠습니다. 우리는 이 대회가 현재까지 진행된 실존적 위험 예측 훈련 중 가장 정교하고 체계적으로 수행된 것이라고 판단합니다. 이 행사에는 AI 전문가와 예측 전문가(이른바 '슈퍼 예측가(superforecasters)' 참조)를 포함한 다수의 예측가 집단이 참여했습니다. AI 전문가들의 경우, 2100년까지 AI로 인한 멸종 위험 추정치에서 상위 75%는 12%, 중앙값은 3%, 하위 25%는 0.25%를 기록했습니다. 반면 예측 전문가들의 경우, 상위 75%조차 1%에 불과했고, 중앙값은 단 0.38%였으며, 하위 25%는 그래프 상에서 0과 사실상 구분하기 어려웠습니다. 즉, AI 전문가 집단의 75% 추정치와 슈퍼 예측가 집단의 25% 추정치 사이에는 최소 100배 이상의 현격한 차이가 존재했습니다. 이 모든 수치는 해당 분야에 대한 깊은 전문성을 갖추고 수개월에 걸쳐 서로를 설득하려 노력한 참가자들로부터 도출된 것입니다!

전문가들 사이에서도 주관적 확률이 광범위하게 diverge하는 현상은 AI 실존적 위험을 둘러싼 심오한 불확실성을 강조합니다. 이는 단순히 약간의 의견 불일치 문제가 아니라, 세계관과 가정의 근본적인 차이 문제입니다. 이는 신생 분야나 진정으로 새로운 현상을 다룰 때 드물지 않게 발생합니다. 그러나 정책 결정에 있어 이러한 전문가 의견의 분산은 문제가 됩니다. 정부는 일반적으로 상당한 개입을 정당화하기 위해 합의 또는 적어도 좁은 범위의 그럴듯한 결과를 추구합니다. '전문가'들조차 여러 자릿수 범위 내에서 합의하지 못할 때, 이는 근본적인 모델, 데이터 또는 심지어 개념적 프레임워크가 공공 정책 적용에 충분히 견고하지 않다는 신호입니다. 이러한 상황은 고도로 이질적이고 추측적인 수치에 기반한 급진적인 조치를 서두르기보다는 겸손하고 신중한 정책 접근 방식을 요구합니다.

만약 이 예측 범위가 충분히 극단적이지 않다고 느껴진다면, 이 모든 연습이 특정 시점에 단일 그룹에 의해 진행되었다는 점을 상기해야 합니다. 만약 이 토너먼트가 오늘 다시 개최되거나 질문의 구성이 달라졌다면, 우리는 또 다른 수치를 얻을 수도 있었을 것입니다. 가장 중요한 것은 예측가들이 제시한 근거를 검토하는 것으로, 이는 보고서에 상세히 기술되어 있습니다. 그들은 특히 강력한 AI 개발 시 발생할 수 있는 부정적인 결과의 가능성을 논할 때 정량적 모델을 사용하지 않습니다. 대부분의 경우, 예측가들은 초지능 AI에 대해 논의할 때 일반 대중이 하는 것과 유사한 종류의 추측에 참여하고 있습니다. 예를 들어, AI가 초인적인 설득 능력을 통해 중요한 시스템을 장악할 수도 있고, 혹은 컴퓨터 작동 효율을 높이려 지구 온도를 낮추려다가 실수로 인류를 멸망시킬 수도 있다는 식입니다. 아니면 AI가 지구 대신 우주에서 자원을 탐색할 것이므로 우리가 크게 걱정할 필요가 없을 것이라는 전망도 있습니다. 이러한 추측 자체에 문제가 있는 것은 아닙니다. 그러나 AI 실존적 위험과 관련하여 예측가들이 여러분이나 우리, 또는 다른 누구의 직감보다 그들의 직감을 더 신뢰할 수 있게 만드는 특별한 지식, 증거 또는 모델에 기반하고 있지 않다는 사실은 분명히 인지해야 합니다.

'슈퍼 예측(superforecasting)'이라는 용어는 필립 테틀록(Philip Tetlock)의 20년간에 걸친 예측 연구에서 유래했습니다 (그는 XPT의 공동 주최자 중 한 명이기도 했습니다). 슈퍼 예측가들은 다양한 정보를 통합하고 심리적 편향을 최소화하는 등 예측 정확도를 높이는 훈련을 받습니다. 이러한 기법은 지정학(geopolitics)과 같은 분야에서 그 효과가 입증되었습니다. 하지만 활용할 만한 유용한 증거가 부족하다면, 아무리 잘 훈련된 예측가라도 좋은 예측을 내놓기 어렵습니다. 예측가들이 설령 신뢰할 만한 정량적 모델을 가지고 있다고 하더라도 (실제로는 그렇지 않습니다), 그들은 '알려지지 않은 미지의 것(unknown unknowns)', 즉 모델 자체의 오류 가능성까지 고려해야 합니다. 저명한 실존적 위험 철학자 닉 보스트롬(Nick Bostrom)은 다음과 같이 설명합니다: "우리의 초기 위험 평가에 내재된 불확실성과 오류 가능성은 모든 요소를 고려한 확률 배정에 반드시 포함되어야 할 요소입니다. 이 요소는 종종 발생 확률은 낮지만 결과가 심각한 위험, 특히 잘 이해되지 않는 자연 현상, 복잡한 사회 역학, 또는 새로운 기술과 관련된 위험, 혹은 다른 이유로 평가하기 어려운 위험에서 지배적인 역할을 합니다."

'알려지지 않은 미지'의 개념은 AI 실존적 위험에서 특히 두드러지는데, 미래 고급 AI의 본질 자체가 여전히 대부분 이론적이기 때문입니다. 우리는 단순히 알려진 시스템이 새로운 조건에서 어떻게 작동할지를 예측하는 것이 아니라, 아직 존재하지 않는 시스템의 존재와 능력에 대해 추측하고 있습니다. 이는 복잡한 지정학적 사건을 위해 정제된 예측 방법론조차도 부적합하게 만듭니다. 핵심 과제는 단순히 이질적인 데이터를 통합하는 것이 아니라, 근본적인 인식론적 한계와 씨름하는 것입니다: 전례 없는, 그리고 근본적인 메커니즘이 아직 발명되고 있는 것을 어떻게 신뢰할 수 있게 예측할 수 있을까요? 이는 정확한 확률적 예측보다는 견고성과 적응성을 우선시하는 위험 관리 전략의 필요성을 시사하며, 우리의 모델이 필연적으로 불완전할 것임을 인정합니다.

이는 합리적인 관점이며, 실제로 AI 실존적 위험을 예측하는 이들은 위험 평가의 불확실성에 대해 깊이 우려합니다. 그러나 이러한 원칙을 따르는 사람들에게는 예측이 모델의 산출물이라기보다는 본질적으로 추측에 불과할 수밖에 없다는 결론이 도출됩니다. 결국, 모델 자체의 오류 가능성이나 모델이 틀렸을 때의 위험을 추정하기 위해 어떠한 모델도 사용될 수 없기 때문입니다.

### 독특하거나 희귀한 사건의 경우 예측 능력은 측정할 수 없습니다

요약하자면, AI 위험에 대한 주관적인 예측들은 그 규모에서 자릿수 단위로 큰 편차를 보입니다. 하지만 만약 예측가들의 과거 실적을 평가할 수 있다면, 우리는 어떤 예측가를 더 신뢰해야 할지 판단할 수 있을 것입니다. 위험 추정치를 정당화하는 이전의 두 가지 접근 방식(귀납적 및 연역적)과는 달리, 예측가는 자신의 추정치를 직접 설명할 필요 없이, 대신 과거에 다른 결과들을 예측하는 데 입증된 능력을 근거로 정당성을 주장합니다.

이러한 방식은 지정학적 사건 영역에서 매우 유용함이 입증되었으며, 예측 커뮤니티는 예측 능력 측정에 상당한 노력을 기울입니다. 보정(calibration), 브라이어 점수(Brier score), 로그 점수(logarithmic score), 그리고 예측 경쟁 플랫폼인 메타큘러스(Metaculus)에서 활용되는 피어 점수(Peer score)와 같이 예측 능력을 평가하는 다양한 방법론이 존재합니다. 그러나 어떤 평가 방법을 적용하더라도, 실존적 위험과 관련된 주관적 확률 예측 능력을 평가하는 데는 여러 가지 난관이 따릅니다. 주요 장애물로는 참조 집단의 부재, 낮은 기본 발생률(base rate), 그리고 매우 긴 시간 지평(time horizon)이 있습니다. 이 세 가지 요소를 순서대로 살펴보겠습니다.

참조 집단 문제(reference class problem)가 예측가들에게 난제로 작용하는 것처럼, 평가자들에게도 동일한 영향을 미칩니다. 다시 외계인 착륙의 예를 들어봅시다. 선거 예측에서 매우 높은 정확도를 보인 예측가가 있다고 가정해 봅시다. 이 예측가가 아무런 근거 없이 1년 안에 외계인이 지구에 착륙할 것이라고 주장한다면 어떨까요? 그 예측가의 검증된 능력에도 불구하고, 우리는 외계인 착륙에 대한 우리의 믿음을 수정하지 않을 것입니다. 이는 외계인 착륙이 선거 예측과는 매우 다른 성격의 사건이며, 예측가의 능력이 이러한 이질적인 영역까지 일반화될 것이라고 기대하지 않기 때문입니다. 마찬가지로, AI 실존적 위험은 과거에 예측되었던 그 어떤 사건과도 너무나 이질적이기에, AI 실존적 위험을 추정하는 예측가의 능력을 입증할 증거가 부재합니다.

'기본 발생률(base rate)' 문제는 또 다른 심각한 장애물입니다. AI 실존적 위험이 정의상 극히 드문 사건(멸종)을 의미하기 때문에, 확률을 약간이라도 과대평가하면 정책적 함의가 불균형적으로 커질 수 있습니다. 희귀 질병을 연구하는 의학 분야에서는 정확한 기본 발생률을 확립하기 위해 광범위한 데이터 수집과 종단 연구가 필요합니다. AI 실존적 위험에 대해서는 그러한 데이터가 존재하지 않습니다. 이는 주어진 예측(낮은 예측이라 할지라도)이 진정으로 그럴듯한 것인지, 아니면 단순히 예측가의 높은 영향력과 낮은 확률 사건을 인지하는 경향에 대한 내재된 편향을 반영하는 것인지를 식별하는 것을 거의 불가능하게 만듭니다. 더욱이, '긴 시간 지평'은 예측가 보정(forecaster calibration)을 위한 피드백 루프가 사실상 존재하지 않는다는 것을 의미합니다. 우리는 예측이 정확했는지 확인하기 위해 수세기를 기다릴 수 없으므로, x-risk에 대한 전통적인 예측 능력 평가 방법은 무의미합니다.

참조 집단 문제를 설령 해결한다 하더라도, 다른 문제들이 여전히 남아 있습니다. 특히, 멸종 위험이 '꼬리 위험(tail risks)', 즉 극히 드물게 발생하는 사건과 연관되어 있다는 사실입니다. 예측가 A가 AI 실존적 위험의 확률을 1%로, 예측가 B가 100만분의 1로 제시한다고 가정해 봅시다. 우리는 어떤 예측을 더 신뢰해야 할까요? 이들의 과거 실적을 검토할 수 있습니다. 만약 AI 실존적 위험에 1%의 확률을 부여한 예측가 A가 더 나은 실적을 가졌다고 하더라도, 이것이 우리가 A의 예측을 더 신뢰해야 함을 의미하지는 않습니다. 그 이유는 예측 능력 평가가 꼬리 위험의 과대평가에 대해 둔감하기 때문입니다. 다시 말해, A는 발생 확률이 높은 일반적인 사건에 대해서는 B보다 약간 더 잘 보정(calibrated)되어 있어 전반적으로 더 높은 점수를 받을 수 있지만, 100만분의 1과 같이 드물게 발생하는 꼬리 위험을 자릿수 단위로 과대평가하는 경향이 있을 수 있습니다. 어떠한 점수 규칙도 이러한 종류의 오보정(miscalibration)을 적절히 제재하지 못합니다.

이러한 현상이 발생하는 이유를 설명하는 사고 실험이 있습니다. 두 예측가 F와 G가 각각 다른 사건 집합을 예측하며, 두 집합 내 사건들의 '실제' 확률이 0과 1 사이에 균등하게 분포되어 있다고 가정해 봅시다. 우리는 매우 낙관적으로, F와 G가 예측하는 모든 사건 e에 대한 실제 확률 P[e]를 알고 있다고 전제합니다. F는 항상 P[e]를 그대로 예측하는 반면, G는 다소 보수적인 경향이 있어 1% 미만의 값은 예측하지 않습니다. 즉, G는 P[e]가 1% 이상일 때는 P[e]를 예측하고, 그렇지 않을 때는 1%를 예측합니다. 본질적으로 F가 더 우수한 예측가입니다. 하지만 이러한 우수성이 그들의 실적에서 명확하게 드러날까요? 다시 말해, F가 G보다 더 높은 점수를 받을 확률이 95%에 도달하려면 각각의 예측을 얼마나 많이 평가해야 할까요? 로그 점수 규칙(logarithmic scoring rule)을 적용하면 약 1억 개의 평가가 필요하며, 브라이어 점수(Brier score)를 사용하면 약 1조 개의 평가가 필요합니다.
2 우리는 여기서의 가정에 대해 논쟁할 수 있지만, 핵심은 예측가들이 꼬리 위험을 체계적으로 과대평가하더라도, 이는 단순히 경험적으로 감지하기 어렵다는 점입니다.

실존적 위험 예측 능력 평가의 마지막 난관은 장기 예측의 경우 평가에 너무 오랜 시간이 소요된다는 점입니다 (그리고 멸종 예측은 말할 것도 없이 그 자체로 평가 불가능합니다). 이러한 문제는 잠재적으로 해결될 수 있습니다. 연구자들은 '상호 점수 매기기(reciprocal scoring)'라는 방법론을 개발했습니다. 이는 예측가들이 서로의 예측을 얼마나 정확하게 예측하는지에 따라 보상을 받는 방식이며, 코로나19 정책의 효과 예측과 같은 실제 상황에서 그 유효성이 검증되었습니다. 이러한 환경에서 상호 점수 매기기는 전통적인 점수 매기기 방식만큼 우수한 예측 결과를 도출했습니다. 물론 좋습니다. 그러나 상호 점수 매기기 역시 참조 집단 문제나 꼬리 위험 문제를 근본적으로 회피하는 방법은 아닙니다.

지금까지의 논의를 종합해 보면, 세 가지 주요 예측 방법론 중 그 어떤 것도 AI 실존적 위험에 대한 신뢰할 만한 추정치를 제공하기 어려운 이유를 명확히 보여줍니다.

### 위험 추정치가 체계적으로 부풀려질 수 있는 여러 가지 이유

결론적으로, 귀납적 및 연역적 방법론은 AI 실존적 위험 예측에 효과적이지 않으며, 주관적 예측들은 서로 크게 상이하여 어떤 예측이 더 신뢰할 만한지 판단하기 어렵습니다. 이러한 상황에서 정책 수립에 유용한, 보다 신뢰할 수 있는 추정치를 얻기 위해 일부 연구자들은 여러 예측가의 예측을 종합하는 예측 집계 방법론에 주목했습니다. 대표적인 노력 중 하나는 AI 발전 설문조사(AI Impacts Survey on Progress in AI)였으나, 이는 무응답 편향(non-response bias)을 포함한 심각한 방법론적 한계로 인해 비판을 받았습니다. 더욱 중요한 점은, 단순한 집계가 예측 정확도를 반드시 향상시켜야 하는 이유가 불분명하다는 것입니다. 결국, 대부분의 예측가들은 유사한 편향을 공유할 수 있으며 (다시 강조하지만, 그들 중 누구도 확고한 예측 근거를 가지고 있지 않기 때문에).

'집단 사고(groupthink)' 문제는 여기에서 특히 중요합니다. 커뮤니티가 유사한 사전 신념(prior beliefs)을 공유하고 외부의 객관적인 피드백 메커니즘이 부족할 때, 단순한 집계조차도 공유된 편향을 수정하기보다는 증폭시킬 수 있습니다. 이는 일반적으로 집단 지성(collective intelligence)의 가치를 부정하는 것이 아니라, 근본적인 정보가 추측적이고 경험적 기반이 부족할 때 그 한계를 강조하는 것입니다. 그러한 맥락에서 합의를 추구하는 것은 예측력이나 추정치의 정확도를 실제로 높이지 않으면서 견고성의 환상을 만들 수 있습니다. 이러한 의사-합의(pseudo-consensus)는 정책 입안자들에 의해 강력한 증거로 오인되어, 약하게 근거한 전제에 기반한 과도한 자신감 있는 의사 결정으로 이어질 수 있습니다.

예측가들이 AI 실존적 위험을 체계적으로 과대평가할 수 있는 몇 가지 요인이 존재합니다. 3 첫째는 '선택 편향(selection bias)'입니다. AI 연구자들의 사례를 들어보면, AI가 세상을 근본적으로 변화시킬 것이라는 믿음은 AI 연구 분야에 뛰어드는 중요한 동기 중 하나입니다. 그리고 일단 이 분야에 진입하면, 이러한 메시지가 지속적으로 강화되는 환경에 노출됩니다. 기술이 막강한 영향력을 지닌다고 믿는다면, 그 변화의 효과가 긍정적이기보다는 부정적일 가능성을 심각하게 고려하는 것은 지극히 합리적입니다. 또한, 다소 폐쇄적인 AI 안전 하위 커뮤니티 내에서는 '에코 챔버(echo chamber)' 현상이 심화될 수 있습니다. 높은 p(doom)(AI로 인한 파멸 확률에 대한 개인의 추정치)를 주장하는 것이 자신의 정체성과 특정 대의에 대한 헌신을 표명하는 방식으로 작용하는 경향이 있는 듯합니다.

예측 전문가들의 경우에는 다소 다른 형태의 선택 편향이 작용합니다. 예측 커뮤니티는 효과적 이타주의(effective altruism) 및 실존적 위험, 특히 AI 위험에 대한 우려와 밀접하게 연관되어 있습니다. 이는 개별 예측가들이 편향되어 있다는 것을 의미하지는 않습니다. 그러나 높은 p(doom)을 가진 사람들은 예측 활동에 더욱 적극적으로 참여하려는 경향을 보일 수 있습니다. 결과적으로, 커뮤니티 전체는 실존적 위험에 대해 우려하는 사람들 쪽으로 편향될 가능성이 높습니다.

선택 편향 외에도, 다른 심리적 요인들이 x-risk에 대한 과장된 인식을 초래할 수 있습니다. 예를 들어, 생생하거나 쉽게 기억되는 사례들이 판단에 영향을 미치는 '가용성 휴리스틱(availability heuristic)'은 특히 대중문화와 일부 학술적 논의에서 디스토피아적 서사가 만연하다는 점을 고려할 때, 대재앙적 AI 시나리오의 확률을 과대평가하게 만들 수 있습니다. 또한, 개인이 자신의 기존 신념을 확인하는 정보를 찾고 해석하는 '확증 편향(confirmation bias)'은 특정 커뮤니티 내에서 높은 p(doom) 추정치를 더욱 공고히 할 수 있습니다. 더욱이, 일부 맥락에서는 가치가 있는 '예방 원칙(precautionary principle)'조차도 때로는 기회비용이나 의도치 않은 부정적 결과를 충분히 고려하지 않고 원격한 가능성에 기반한 극단적인 개입을 정당화하는 데 오용될 수 있습니다. 이러한 인지 편향은 AI x-risk 예측에만 국한되는 것은 아니지만, 전례 없고 감정적으로 고조된 사건을 다룰 때 특히 강력하게 작용합니다.

예측가들은 증거에 따라 자신의 신념을 수정하는 데 능숙하지만, 문제는 소행성 충돌 위험과 달리 AI 실존적 위험과 관련해서는 자신의 신념을 변화시킬 만한 증거가 거의 없다는 것입니다. 따라서 우리는 예측이 사람들이 특정 커뮤니티에 합류할 때 지니고 있던 사전 신념(prior beliefs)에 강하게 영향을 받는다고 추정합니다. XPT 보고서는 "가장 적극적인 참가자들 사이에서도, 심지어 서로를 설득하기 위한 금전적 유인책이 있었음에도 불구하고, XPT 기간 동안 자신의 견해를 바꾼 사람은 거의 없었다"고 명시하고 있습니다. 후속 연구에서는 많은 의견 불일치가 AI를 넘어선 근본적인 세계관의 차이에서 비롯되었음을 밝혀냈습니다.

다시 한번 강조하건대, 편향에 대한 우리의 주장은 오직 AI 실존적 위험에 국한됩니다. 만약 체계적으로 편향된 (예를 들어, 현직자에게 유리한) 선거 예측가 집단이 존재했다면, 몇 차례의 선거가 진행된 후 예측과 실제 결과를 비교할 때 이러한 편향은 명확하게 드러날 것입니다. 그러나 AI 실존적 위험의 경우, 이전 섹션에서 설명했듯이, 예측 능력 평가는 꼬리 위험의 과대평가에 대해 둔감합니다. 흥미롭게도, 능력 평가는 꼬리 위험의 과소평가에 대해서는 극도로 민감하게 반응합니다. 실제로 발생한 희귀한 사건에 0의 확률을 할당하면, 로그 점수 규칙(logarithmic scoring rule)에 따라 무한한 페널티를 받게 되며, 다른 사건들을 얼마나 잘 예측했는지와 무관하게 결코 만회할 수 없습니다. 이는 로그 점수의 주요 장점 중 하나로 여겨지며, 메타큘러스(Metaculus)와 같은 플랫폼에서 이를 채택하는 이유이기도 합니다.

이제 정확한 추정치를 보유하고 있지 않은 예측가를 상정해 봅시다. AI 실존적 위험과 같이 불확실성이 높은 사안에 대해 정확한 추정치를 가진 예측가는 분명히 없을 것입니다. 비대칭적인 페널티를 감안할 때, 합리적인 행동은 추정 범위의 상한선을 선택하는 것입니다. 4 어쨌든, 예측가들이 추정치가 매우 불확실할 때 실제로 어떤 숫자를 보고하는지는 명확하지 않습니다. 아마도 그들은 점수 함수의 인센티브에 직접적으로 반응하지 않을 것입니다. 결국, 장기 예측은 단기간 내에 그 결과가 판명되지 않을 테니까요. 또한 XPT의 경우, 인센티브가 긴 시간 지평 문제를 우회하기 위해 서로의 예측을 예측하는 방식이었다는 점을 기억해야 합니다. 상호 점수 매기기(reciprocal scoring)에 관한 논문은 이러한 방식이 예측가들로 하여금 진정성 있고 노력이 투입된 추정치를 제출하도록 유도할 것이라고 주장하며, 이 주장에 대한 여러 반론을 검토합니다. 그들의 방법론적 방어는 두 가지 핵심 가정에 기반합니다: 예측가들이 더 많은 노력을 기울임으로써 실제 추정치에 더 근접할 수 있다는 점과, 다른 예측가들이 어떤 행동을 할지 더 나은 방식으로 예측할 수 없다는 점입니다.

만약 이러한 가정이 충족되지 않는다면 어떻게 될까요? 본 게시물 전반에서 주장했듯이, AI 실존적 위험의 경우 증거가 예측가들의 사전 신념(prior beliefs)을 변화시킬 것이라고 기대하기 어렵기 때문에 첫 번째 가정은 의심스럽습니다. 그리고 XPT의 단 한 차례 진행이 완료된 지금, 해당 토너먼트에서 발표된 중앙값 추정치는 강력한 기준점(게임 이론의 '초점(focal point)') 역할을 합니다. 미래에 상호 점수 매기기 인센티브를 부여받는 예측가들은 기존의 중앙값 예측을 출발점으로 삼아, 지난 토너먼트 이후에 얻을 수 있는 새로운 정보들을 반영하기 위해 미미한 조정만을 할 가능성이 있습니다. 기존 추정치가 미래 예측의 기준점 역할을 하면서 추정 범위가 점차 좁아질 수 있습니다. 이 모든 것은 한마디로 요약됩니다: 활용할 실제 증거가 적을수록 '집단 사고(groupthink)'의 위험이 증대됩니다.

### 파스칼의 내기(Pascal’s wager)를 경계하라: 효용 극대화(utility maximization)의 위험

참고로, AI로 인한 멸종 위험과 멸종에 이르지 않는 재앙적 위험에 대한 XPT의 중앙값 추정치는 다음과 같습니다:

| 예측 대상                   | AI 전문가 중앙값 | 슈퍼 예측가 중앙값 |
| :-------------------------- | :--------------- | :----------------- |
| AI로 인한 멸종 위험 (2100년) | 3%               | 0.38%              |
| 재앙적 위험 (2100년)        | 12%              | 1%                 |

다시 한번 강조하지만, 우리는 이러한 수치들을 지나치게 심각하게 받아들여서는 안 된다는 입장입니다. 이 수치들은 다양한 참가자 집단이 다른 어떤 요소보다 AI에 대해 얼마나 우려하는지를 반영합니다. 이전과 마찬가지로, 예측 전문가(superforecasters)와 AI 전문가의 추정치는 자릿수 단위로 큰 차이를 보입니다. 만약 우리가 이러한 추정치들을 어느 정도 신뢰해야 한다면, AI 전문가의 추정치보다는 예측 전문가의 추정치를 고려해야 할 것입니다. 과거 연구에서 얻은 중요한 통찰 중 하나는, 다양한 정보를 통합하고 심리적 편향을 최소화하도록 훈련받은 예측 전문가들이 특정 분야 전문가(domain experts)보다 더 나은 성과를 보인다는 점입니다. 그럼에도 불구하고, 앞서 언급했듯이, 예측 전문가들의 예측조차도 상당한 과대평가일 수 있으며, 우리는 이를 확실히 단정할 수 없습니다.

AI 실존적 위험에 객관적인 확률을 부여하는 내재된 어려움, 심리적 편향, 그리고 꼬리 위험에 대한 전통적인 보정 방법의 비효율성을 종합하면, 이러한 수치들을 정책에 사용하는 것에 심각한 의문이 제기됩니다. 전문가들조차 몇 자릿수 범위 내에서 의견이 나뉘고, 그들의 추정치가 경험적 증거보다는 사전 신념에 크게 영향을 받을 때, 정책 결정에 단일한 '중앙값 추정치'에 의존하는 것은 매우 문제가 됩니다. 이는 과도하게 신중하여 혁신을 저해하거나, 반대로 자원 분배를 왜곡하여 다른 더 가시적인 위험을 과소평가하는 정책을 초래할 위험이 있습니다. 정책 입안자들에게 더 신중한 접근 방식은 AI 실존적 위험의 정확한 확률과 무관하게 유익한, 견고하고 '후회 없는(no-regrets)' 정책에 초점을 맞추는 것일 수 있습니다. 이는 고도로 추측적인 예측에 기반한 특정 수치 목표를 쫓기보다는 더 나은 접근 방식입니다.

그렇다면 무엇이 문제일까요? 정책 결정자들이 특정 기간 동안의 위험이 0.01%가 아닌 1%라고 믿는다면 어떨까요? 양쪽 모두 상당히 낮은 수치로 보일 수 있습니다! 문제는 그들이 그 확률을 어떻게 활용하느냐에 달려 있습니다. 대부분의 경우, 이러한 추정치는 단순히 특정 전문가 집단이 해당 위험을 중요하게 여긴다는 사실을 알리는 수단일 뿐입니다. 만약 그것이 전부라면, 그대로 두어도 무방할 것입니다. 그러나 같은 숫자를 사람들이 극도로 다르게 해석한다는 점을 고려할 때, 이러한 정교한 정량화 노력이 과연 신호 전달 목적에 부합하는지조차 불분명합니다. 예를 들어, 연방거래위원회(Federal Trade Commission) 위원장 리나 칸(Lina Khan)은 자신의 p(doom)이 15%에 불과했기 때문에 이 문제에 대한 자신의 견해가 '기술 낙관적(techno-optimistic)'이라고 발언하여 전문가들을 당혹스럽게 했습니다. (덧붙여, 그 수치는 우리가 기술 낙관론자라고 편안하게 부를 수 있는 수준보다 약 천 배나 더 높은 것입니다.)

의사 결정 과정에서 극히 작거나 매우 큰 수치들을 정확하게 인지적으로 처리하고, 단순히 '무의미하게 작음'과 같은 범주로 묶지 않기 위해서는 상당한 수준의 정량적 훈련이 요구됩니다. 대다수의 사람들은 이러한 방식으로 훈련받지 않습니다. 결국, 전문가들의 모호한 직관과 우려가 '의사-정확한 숫자(pseudo-precise numbers)'로 전환된 후, 다시 정책 결정자들에 의해 모호한 직관과 우려로 재해석되는 양상으로 보입니다. 이제 정량화의 겉치레를 멈춰야 합니다! AI 안전 센터(Center for AI Safety)의 AI 위험 성명(Statement on AI Risk)은 이 점에 있어 놀랍도록 솔직했습니다 (물론, 우리는 그 내용에 전적으로 동의하지는 않습니다).

의사 결정 시 확률을 활용하는 원칙적이고 정량적인 접근 방식은 비용-편익 분석(cost-benefit analysis)을 통해 효용을 극대화하는 것입니다. 기본적인 개념은 다음과 같습니다. 어떤 결과가 주관적인 가치, 즉 효용(utility) U(이는 양수 또는 음수일 수 있습니다)를 지니고 있고, 예를 들어 발생 확률이 10%라고 가정할 때, 우리는 그 결과가 확실히 발생하며 0.1 * U의 가치를 갖는다고 간주하고 행동할 수 있습니다. 이후, 우리가 선택할 수 있는 각 대안에 대한 비용과 편익을 모두 합산하여, 비용을 제외한 편익 값(즉, '기대 효용(expected utility)')을 최대화하는 선택지를 고를 수 있습니다.

이 지점에서부터 문제가 심각해집니다. 첫째, 일부는 인류 멸종이 헤아릴 수 없는 거대한 부정적 가치를 지닌다고 여길 수 있습니다. 이는 미래에 존재할 수 있는 모든 인간 생명, 즉 물리적이든 시뮬레이션된 형태이든, 그 존재 가능성을 완전히 소멸시키기 때문입니다. 이러한 관점의 논리적 귀결은 실존적 위험이 언제나 모든 개인의 최우선 과제가 되어야 한다는 것입니다! 이는 '파스칼의 내기(Pascal’s wager)'를 떠올리게 합니다. 즉, 신의 존재 가능성이 극히 낮더라도, 신앙을 거부했을 때의 대가는 무한하며 (영원한 행복과 대조되는 지옥에서의 영원한 고통), 따라서 기대 효용(expected utility) 또한 무한하다는 주장입니다. 다행히 정책 결정자들은 무한의 개념을 포함하는 의사 결정 프레임워크에 지나친 신뢰를 부여하지 않습니다. 그러나 이러한 발상은 AI 안전 커뮤니티 내에서 강력한 영향력을 행사하며, 일부 사람들에게 AI 실존적 위험이 사회의 최우선 과제가 되어야 한다는 확신을 심어주고 있습니다.

설령 스스로를 재앙적이지만 실존적이지 않은 위험으로 한정한다 해도, 우리는 수십억 명의 생명이 위협받는 상황에 대해 논하고 있기에, 단 1%의 위험에 대한 기대 비용(expected cost)조차도 엄청나게 높아 정책적 함의가 극단적입니다. 정부는 AI 실존적 위험 완화에 대한 지출을 자릿수 단위로 대폭 증액하고, AI 개발 중단과 같은 강경한 조치를 검토해야 할 수도 있습니다. 바로 이러한 이유 때문에, 이러한 추정치들이 어떠한 견고한 방법론으로도 뒷받침되지 않는다는 사실을 이해하는 것이 매우 중요합니다. 이처럼 미약한 증거에 기반하여 세상을 뒤흔들 수 있는 정책 결정을 내리는 것은 지극히 현명하지 못한 행동일 것입니다.

### 이정표 예측은 결과 모호성(outcome ambiguity)으로 인해 어려움을 겪습니다

AI 정책 분야에서 예측이 역할을 할 수 있을까요? 우리는 그렇다고 생각합니다. 다만, 그것이 실존적 위험을 예측하는 것만을 의미하지는 않습니다. 특정 능력 벤치마크(capability benchmarks)나 경제적 파급 효과와 같은 AI 이정표를 예측하는 것이 더 실현 가능하고 유의미합니다. 예측가들이 다양한 AI 이정표가 언제 도달할지 예측하는 데 능숙함을 입증한다면, 이는 그들이 미래에도 좋은 성과를 낼 것이라는 증거를 제공합니다. 이때 우리는 더 이상 독특하거나 희귀한 사건에 대해 논하는 것이 아닙니다. 그리고 살인 로봇을 막는 것보다 잠재적인 경제적 혼란에 대비하는 것과 같이 이해관계가 낮은 정책 개입을 고려할 때는, 예측이 모든 합리적인 사람들을 만족시킬 만큼 정당화되어야 하는 중요성이 덜합니다.

예측 커뮤니티는 이정표 예측에 상당한 노력을 기울이고 있습니다. 메타큘러스(Metaculus)에서는 '2040년 이전에 인간-기계 지능 동등성(Human-machine intelligence parity)이 달성될 것인가?'라는 질문에 대해 1,300명 이상의 예측가들이 96%라는 집계 예측을 제시했습니다. 이는 놀라운 결과입니다! 만약 우리가 이 예측에 동의한다면, 일반 인공지능(AGI)으로의 안전한 전환을 관리하는 것이 전 세계적인 최우선 과제가 되어야 한다는 입장을 지지할 것입니다. 그렇다면 우리가 그렇게 하지 않는 이유는 무엇일까요?

그 답은 세부 사항에 있습니다. 일반 인공지능(AGI)과 같은 모호한 개념의 정의에 대한 합의가 부재합니다. 설령 정의를 고정한다 해도, 그것이 실제로 달성되었는지 여부를 판단하는 것은 어렵거나 불가능할 수 있습니다. 효과적인 예측을 위해서는 모호한 결과(ambiguous outcomes)를 피하는 것이 극히 중요합니다. 예측 커뮤니티가 이 문제를 해결하는 방식은 시험 성적과 같은 상대적으로 협소한 기술적 측면에서 정의를 내리는 것입니다. 메타큘러스(Metaculus)의 지능 동등성 질문은 수학, 물리학, 컴퓨터 과학 분야의 대학원 시험 성적을 기준으로 정의됩니다. 이 정의에 기반한다면, 우리는 96%의 예측에 동의합니다. 그러나 우리는 그 정의가 너무 협소하여 정책적 의미가 크게 없다고 판단합니다. 실존적 위험은 차치하고라도, 이전에 언급했듯이, 시험에서의 AI 성능은 구성 타당도(construct validity)가 너무 낮아 AI가 노동 시장에 미칠 영향조차 예측하기 어렵습니다. 다른 벤치마크들도 크게 다르지 않습니다. 요컨대, 벤치마크와 실제 영향 사이의 상당한 간극 때문에 AI 능력 발전의 시기를 예측하는 것은 매우 까다로운 일입니다.

'결과 모호성(outcome ambiguity)'의 문제는 기술적 능력과 실제 세계 영향 사이의 중요한 차이를 강조합니다. AI 시스템이 특정 학업 벤치마크에서 뛰어날 수 있지만, 이것이 실존적 피해를 일으키거나 심지어 광범위한 경제적 혼란을 야기할 수 있는 초지능 에이전트로 자동적으로 전환되는 것을 의미하지는 않습니다. 좁은 지능에서 일반 지능으로, 그리고 일반 지능에서 제한 없는 힘과 잠재적으로 잘못 정렬된 목표를 가진 에이전트로의 도약은 엄청나고 제대로 이해되지 않고 있습니다. 따라서 벤치마크 예측은 기술 발전 추적에 유용할 수 있지만, 사회적 또는 실존적 위험 예측과 혼동되어서는 안 됩니다. 정책 입안자들은 순전히 기술적 벤치마크에서 직접적인 실제 세계 결과를 외삽하는 예측에 대해 경계해야 하며, 이 둘을 연결하는 견고한 다리 없이 이루어지는 예측은 피해야 합니다. 대신, 원시적인 계산 능력이나 시험 점수뿐만 아니라 사회 기술 시스템, 윤리적 함의, 인간-AI 상호작용 역학을 고려하는 보다 총체적인 평가 프레임워크 개발에 초점을 맞춰야 합니다.

다행히도, 실제 중요한 작업을 반영하는 더 나은 벤치마크(benchmarks)가 개발되고 있습니다. 벤치마크(benchmarks) 외에도, 비용이 더 많이 들더라도 자연주의적 평가(naturalistic evaluation)가 필요합니다. 자연주의적 평가(naturalistic evaluation)의 한 유형은 사람들이 AI 지원을 받아 자신의 업무를 어떻게 다르게 수행하는지 측정하는 것입니다. 노동 시장 변화나 군대의 AI 관련 지출과 같은 경제적, 사회적, 정치적 영향을 직접 예측하는 것이 더 유용할 수 있지만, 명확하게 정의하고 측정하기는 더 어렵습니다.

### 결론

정책 결정 과정에서 확률의 오용을 방지할 책임은 정책 입안자들에게 있습니다. 우리는 정책 입안자들이 잘못된 정보에 현혹되는 것을 '보호'하기 위해 예측가들에게 예측 발표를 중단하라고 요구하는 것이 아닙니다. 그럼에도 불구하고, 우리는 예측이 그 산출 과정과 고려된 증거에 대한 명확한 설명과 함께 제시되어야 한다고 주장합니다. 이러한 투명성은 정책 입안자들이 제시된 정당성이 자신들이 수용할 수 있는 기준을 충족하는지 여부에 대해 정보에 기반한 결정을 내릴 수 있도록 도울 것입니다. XPT는 투명성의 훌륭한 사례이며, 본 논문도 마찬가지입니다 (비록 실존적 위험에 관한 것은 아니지만). 반대로, 단순히 다수의 연구자들을 대상으로 설문조사를 실시하고 집계된 수치만을 제시하는 방식은 오해를 불러일으킬 수 있으므로, 정책 입안자들은 이를 간과해야 합니다.

그렇다면 정부는 AI 실존적 위험에 대해 어떤 조치를 취해야 할까요? 우리의 견해는 아무것도 하지 말아야 한다는 것이 아닙니다. 다만, 정부는 실존적 위험을 시급하고 중대한 사안으로 인식하더라도, AI 개발 제한과 같이 겉으로는 설득력 있어 보이는 특정 유형의 정책들은 지양해야 합니다. 본 시리즈의 다음 에세이들에서 주장하겠지만, 그러한 정책들은 불필요할 뿐만 아니라 오히려 실존적 위험을 증대시킬 가능성이 높습니다. 대신, 정부는 AI 위험에 대한 다양한 추정치와 조화를 이루면서, 심지어 위험이 미미하더라도 전반적으로 유익한 정책들을 채택해야 합니다. 다행히도 그러한 정책들은 분명히 존재합니다. 정부는 또한 새로운 증거에 더욱 민감하게 반응하도록 정책 결정 과정을 개선해야 합니다. 이 모든 내용에 대한 자세한 정보는 조만간 공개될 예정입니다.

이러한 '후회 없는(no-regrets)' 정책에는 AI 시스템의 견고성, 해석 가능성, 그리고 검증 가능한 정렬(alignment)에 초점을 맞춘 AI 안전 연구에 투자하는 것이 포함됩니다. 이는 x-risk가 현실화될지 여부와 관계없이 AI 시스템을 개선하는 영역입니다. 또한 AI 거버넌스 표준에 대한 국제 협력을 촉진하고, 책임 있는 AI 개발 관행을 장려하며, 기술 변화에 대한 사회적 회복력을 구축하는 것도 포함됩니다. 나아가 정부는 AI에 대한 대중 교육을 우선시하여 시민들이 AI의 기회와 더 가시적인 위험(예: 개인 정보 보호, 편향, 일자리 대체) 모두에 대해 잘 알 수 있도록 해야 합니다. 추측적인 x-risk 확률에서 벗어나 실행 가능하고 광범위하게 유익한 조치에 초점을 맞춤으로써, 정책 입안자들은 AI의 미래를 탐색하기 위한 보다 회복력 있고 적응력 있는 프레임워크를 구축하고, 실용적이고 증거에 기반한 방식으로 위험을 완화하면서 혁신을 촉진할 수 있습니다.

### 추가 자료

XPT 보고서의 제목은 『Forecasting Existential Risks: Evidence from a Long-Run Forecasting Tournament』입니다. 테틀록(Tetlock)과 가드너(Gardner)의 책 『Superforecasting』은 테틀록, 바바라 멜러스(Barbara Mellers) 등의 연구를 요약합니다. 스콧 알렉산더(Scott Alexander)는 독특한 사건에 확률을 부여하는 것이 원칙적으로 잘못되었다는 주장을 반박합니다(우리는 대체로 동의합니다). 우리의 주장은 그가 다루는 입장과 두 가지 주요 방식으로 다릅니다. 우리는 일반적인 예측에 대해 이야기하는 것이 아니라, 정책 수립에 대한 적용에 대해서만 이야기합니다. 그리고 우리는 원칙적으로 반대하지 않습니다. 우리의 주장은 경험적입니다. AI 실존적 위험(AI x-risk) 예측은 극도로 신뢰할 수 없으며 정당성이 부족합니다. 이론적으로 이것은 미래에 바뀔 수 있지만, 우리는 크게 기대하지 않습니다. 프리드먼(Friedman)과 주크하우저(Zuckhauser)의 논문은 확률이 전부가 아닌 이유를 설명합니다. 동일한 위험 추정치를 가진 두 예측은 정책 입안자들에게 매우 다른 함의를 가질 수 있습니다. 우리의 견해로는, AI 실존적 위험(AI x-risk) 예측은 신뢰의 세 가지 차원 중 두 가지에서 좋지 않은 성과를 보입니다: 증거에 대한 견고한 기반과 합리적인 의견의 좁은 범위. 샤제다 아메드(Shazeda Ahmed)가 이끄는 프린스턴 CITP 동료들의 논문은 "지식 생산 및 커뮤니티 구축의 응집력 있고 상호 연결된 사회 구조"로 구성된 AI 안전의 인식론적 문화(epistemic culture)를 설명합니다. 이는 동료 심사(peer review)와 같은 관행을 중심으로 하는 더 넓은 과학 커뮤니티와 달리, 예측과 같은 관행이 AI 안전 커뮤니티가 신념을 형성하는 방식의 기둥이 된 이유를 이해하는 데 도움이 됩니다. 물론, 우리는 어떤 견해가 과학적 정통성에 부합하지 않는다는 이유만으로 거부해서는 안 됩니다. 그러나 동시에, 우리는 스스로를 AI 안전 커뮤니티라고 칭하는 집단의 AI 안전에 대한 견해에 어떤 존중도 표해서는 안 됩니다. AI 안전 커뮤니티의 일반적인 구성원이 AI 안전에 대해 특정 입장을 가지고 있다는 것을 이해하는 것이 중요합니다. 이 입장은 매우 논쟁의 여지가 많고, 우리의 견해로는 다소 과장된 것입니다. 우리는 증거 기반 AI 안전에 대해 광범위하게 글을 써왔습니다. 우리의 가장 잘 알려진 작업에는 에세이 『AI safety is not a model property』와 대규모 협력의 결과물인 『On the Societal Impact of Open Foundation Models』라는 제목의 논문이 포함됩니다.

**감사의 말씀.** 초고에 대한 피드백을 주신 벤자민 에델만(Benjamin Edelman), 에즈라 카거(Ezra Karger), 맷 살가닉(Matt Salganik), 올리 스테픈슨(Ollie Stephenson)께 감사드립니다. 이 에세이 시리즈는 세스 라자르(Seth Lazar)와 호주 국립대학교 MINT 연구소 구성원들, 프린스턴 대학교 예측의 한계(Limits to Prediction) 과정 학생들, 샤제다 아메드(Shazeda Ahmed), 재커리 시겔(Zachary Siegel)을 포함한 많은 분들의 피드백을 통해 완성될 예정인 논문을 기반으로 합니다.

1 구체적으로, 핵무기 실험 데이터를 사용하고 몇 가지 물리학 기반 가정을 통해, 장기간 하늘을 어둡게 하고 전 세계 농업의 붕괴를 초래할 만큼 충분한 먼지를 일으키는 데 필요한 것이 무엇인지 계산할 수 있습니다.
2 보수적인 예측가 G가 예측에 대해 ϵ(우리 예에서는 0.01)의 하한선을 가지고 있다고 가정해 봅시다. 두 예측가 간의 차이가 관련 있는 사건은 ϵ 비율에 불과합니다. 심지어 진정한 확률이 ϵ 미만인 사건 집합으로 제한하더라도, 일부 계산은 F의 예상 로그 점수(log score)가 G의 예상 로그 점수보다 미미한 양(O(ϵ))만큼 더 좋고, 브라이어 점수(Brier score)의 경우 O(ϵ^2)만큼 더 좋다는 것을 보여줍니다. 따라서 모든 사건을 볼 때, 예상 점수의 차이는 각각 O(ϵ^2) 및 O(ϵ^3)입니다. 한편, 그들의 점수 분산은 두 점수 함수 모두 약 0.1로 나옵니다. 두 예측가의 점수 평균이 다르다고 자신 있게 주장하려면, 평균 점수 차이의 표준 편차(sqrt(0.1 / N))가 예상 차이보다 훨씬 작을 만큼 사건의 수 N이 충분히 커야 합니다. 따라서 로그 점수(log score)의 경우 N ~ O(1/ϵ^4)이고, 브라이어 점수(Brier score)의 경우 N ~ O(1/ϵ^6)가 필요합니다.
3 물론, 예측가들이 위험을 과소평가할 수 있는 이유도 있습니다. 우리는 그러한 이유들이 덜 설득력 있다고 생각하지만, 더 중요한 것은, 이 논의의 범위를 벗어난다고 생각합니다. 이유는 간단합니다. 정부가 자유를 제한하는 정책을 시행하도록 옹호하는 것은 높은 실존적 위험(x-risk) 추정치에 의해 정당화되고 있습니다. 따라서 증거의 부담은 비대칭적입니다. 그러한 정책을 요구하는 사람들은 상향 편향(upward biases)에 대한 우려에 대응하는 것을 포함하여 자신의 확률 추정치를 정당화해야 합니다. 만약 어떤 이상한 미래 세계에서 일부 사람들이 낮은 실존적 위험(x-risk) 추정치에 기반하여 제한적인 정책을 옹호한다면, 잠재적인 과소평가에 대해 동일한 조사를 하는 것이 중요해질 것입니다.
4 예측 이론에서 로그 점수(logarithmic score) 및 기타 소위 적절한 점수 규칙(proper scoring rules)은 예측가의 "진정한 믿음(true belief)"을 이끌어내는 것이 수학적으로 보장됩니다. 그러나 적절한 점수 규칙(proper scoring rule)의 정의는 진정한 믿음이 단일 값이라고 가정하는 반면, 실제로는 넓은 범위인 경향이 있습니다. 이러한 시나리오에서 예측가는 자신의 분포의 평균을 보고하도록 유도되지만, 우리가 직관적으로 "진정한 믿음(true belief)"이라고 의미하는 것은 중앙값에 더 잘 부합할 수 있습니다.