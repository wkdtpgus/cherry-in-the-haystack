대규모 언어 모델(LLM) 분야의 연구는 최근 몇 년간 경이로운 속도로 진보해 왔습니다. 그러나 대다수 LLM의 근간이 되는 아키텍처인 디코더 전용 트랜스포머(decoder-only transformer)는 이처럼 급변하는 발전 속에서도 그 구조적 핵심은 비교적 변함없이 유지되어 왔습니다. 최근 들어서는 최첨단 연구소들이 새로운 아키텍처, 즉 MoE(Mixture-of-Experts)를 적극적으로 도입하기 시작했습니다. 예를 들어, GPT-4가 MoE 기반으로 알려져 있으며, 최근에 발표되어 큰 주목을 받는 DeepSeek-v3 및 R1 모델 또한 마찬가지입니다. 다음 발췌 내용을 통해 그 추세를 엿볼 수 있습니다.

“오픈 소스 모델의 역량을 더욱 확장하기 위해, 우리는 모델을 확장하고 6,710억 개의 매개변수를 가지며 각 토큰에 대해 370억 개의 매개변수가 활성화되는 대규모 MoE(Mixture-of-Experts) 모델인 DeepSeek-V3를 소개합니다.” - [8]에서 발췌

MoE 기반 LLM은 방대한 모델의 학습 및 운용 효율성을 크게 높일 수 있는 잠재력 덕분에 인기를 얻고 있는, 기존 디코더 전용 트랜스포머의 변형된 형태를 활용합니다. 이러한 MoE 기반 LLM은 전체 매개변수(parameter) 수에서 압도적인 규모를 자랑합니다. 하지만 실제 모델이 출력을 계산할 때는 이 매개변수 중 극히 일부만이 (추론(inference) 과정에서 동적으로 선택되어) 사용됩니다. MoE의 이러한 희소성(sparsity)은 엄청나게 크고 강력한 LLM을 구축하는 데 필요한 비용을 획기적으로 절감해 줍니다. 수많은 최첨단 LLM이 MoE 기반 아키텍처를 채택하기 시작하는 현 시점에서, MoE에 대한 심도 깊은 이해를 갖추는 것은 매우 중요합니다. 본 게시물에서는 PyTorch 환경에서 nanoMoE라고 명명된 중규모 MoE 모델을 처음부터 구현(및 사전 훈련)하는 과정을 통해 이 목표를 달성하고자 합니다. nanoMoE와 관련된 모든 코드는 아래 저장소에서 확인하실 수 있으며, 이는 Andrej Karpathy의 nanoGPT 라이브러리를 MoE 사전 훈련을 지원하도록 확장한 포크(fork) 버전입니다. 먼저 nanoMoE의 작동 원리를 이해하기 위한 필수적인 배경 지식부터 설명하겠습니다. 그 다음, nanoMoE의 각 구성 요소를 밑바닥부터 구축하고, 최종적으로 모델의 성공적인 사전 훈련 실행으로 마무리할 것입니다.

nanoMoE 저장소

**디코더 전용 트랜스포머의 핵심 원리**

**디코더 전용 트랜스포머: 생성형 LLM의 핵심 동력**
Cameron R. Wolfe, Ph.D. · 2024년 3월 4일
전체 기사 읽기

MoE 기반 LLM의 작동 방식을 이해하려면, 대부분의 거대 언어 모델이 기반으로 하는 표준 구조인 **디코더 전용 트랜스포머(decoder-only transformer) 아키텍처**에 대한 명확한 이해가 선행되어야 합니다. 이 아키텍처는 GPT 시리즈를 통해 널리 알려진 인코더-디코더 트랜스포머(encoder-decoder transformer) 아키텍처 [1]를 단순화하고 발전시킨 형태입니다. 이전 게시물에서 이 구조를 면밀히 분석했지만(위 참조), 해당 지식이 본 게시물의 나머지 부분을 이해하는 데 필수적이므로 여기서 다시 한번 다루겠습니다. 아키텍처를 설명하는 동안, 우리는 디코더 전용 트랜스포머의 최소 기능 구현체인 Andrej Karpathy의 nanoGPT를 참고 자료로 활용할 것입니다.

([1]에서 발췌) 원본 트랜스포머 아키텍처. 원래 [1]에서 기계 번역(machine translation)과 같은 시퀀스-투-시퀀스(sequence-to-sequence) 작업 해결을 위해 제안된 트랜스포머는 인코더(encoder)와 디코더(decoder) 모듈을 모두 포함합니다. 위 그림을 참조하십시오. 여기서는 전체(인코더-디코더) 트랜스포머에 초점을 맞추지 않을 것입니다. 그러나 이 아키텍처에 대한 상세한(그리고 널리 인용되는) 개요는 여기에서 찾을 수 있습니다. 현대 LLM에 더 보편적으로 사용되는 디코더 전용 트랜스포머는 그 이름이 시사하듯이, 이 아키텍처에서 인코더 부분을 단순히 제거하고 디코더 2만을 활용합니다. 실질적으로 이는 디코더 전용 트랜스포머 아키텍처의 모든 계층이 다음 핵심 요소를 포함한다는 것을 의미합니다.

*   마스크드 셀프 어텐션(masked self-attention) 계층.
*   피드포워드(feed-forward) 계층.

완전한 디코더 전용 트랜스포머 아키텍처를 구성하기 위해, 우리는 구조는 동일하지만 독립적인 가중치(weight)를 가지는 이 계층 L개를 서로 위에 쌓아 올립니다. 이러한 구조의 개념도는 아래 그림에 제시되어 있습니다.

**디코더 전용 트랜스포머 아키텍처**

이제 더 깊이 있는 이해를 돕기 위해 아키텍처의 각 구성 요소를 개별적으로 논의해 보겠습니다. 모델의 입력 구조부터 시작하여 각 계층의 핵심 요소(즉, 셀프 어텐션(self-attention) 및 피드포워드 계층)와 이들이 어떻게 결합되어 전체 모델 아키텍처를 형성하는지 다룰 것입니다.

**텍스트에서 토큰으로의 전환**

우리 대부분이 이미 알고 있듯이, LLM의 입력은 단순히 텍스트 시퀀스(즉, 프롬프트(prompt))입니다. 그러나 위 그림에서 보이는 입력은 텍스트 시퀀스가 아닙니다! 오히려 모델의 입력은 토큰 벡터(token vector)들의 배열입니다. 텍스트를 모델에 입력으로 제공한다면, 원시 텍스트 입력으로부터 이러한 벡터들을 어떻게 생성할 수 있을까요?

**원시 텍스트를 토큰 시퀀스로 변환하는 과정**

**토큰화(Tokenization).** LLM 입력을 구성하는 첫 단계는 원시 텍스트 입력(문자들의 시퀀스)을 개별 토큰(token)들로 분리하는 것입니다. 토큰화라고 불리는 이 과정은 모델의 **토크나이저(tokenizer)**에 의해 처리됩니다. 다양한 종류의 토크나이저가 존재하지만, BPE(Byte-Pair Encoding) 토크나이저 [2]가 가장 널리 사용됩니다. 더 자세한 내용은 여기를 참조하십시오. 이 토크나이저들은 원시 텍스트 시퀀스를 받아들여, 위 그림에 나타난 바와 같이 이 텍스트를 개별 토큰 시퀀스로 분해합니다.

This file contains bidirectional Unicode text that may be interpreted or compiled differently than what appears below. To review, open the file in an editor that reveals hidden Unicode characters. Learn more about bidirectional Unicode characters Show hidden characters
```python
import torch
from transformers import AutoTokenizer

# load the llama-3.2 tokenizer
tokenizer = AutoTokenizer.from_pretrained('meta-llama/Llama-3.1-8B')

# raw text
text = "This raw text will be tokenized"

# create tokens using tokenizer
tokens = tokenizer.tokenize(text)
token_ids = tokenizer.convert_tokens_to_ids(tokens)
# token_ids = tokenizer.encode(text) # directly create token ids

# view the results
print("Original Text:", text)
print("Tokens:", tokens)
print("Token IDs:", token_ids)

# create token embedding layer
VOCABULARY_SIZE: int = 128000
EMBEDDING_DIM: int = 768
token_embedding_layer = torch.nn.Embedding(
    num_embeddings=VOCABULARY_SIZE,
    embedding_dim=EMBEDDING_DIM,
)

# get token embeddings (IDs must be passed as a tensor, not a list)
token_emb = token_embedding_layer(torch.tensor(token_ids))
print(f'Token Embeddings Shape: {token_emb.shape}')
```
view raw tokenizer_example.py hosted with ❤ by GitHub

LLM 훈련 및 상호 작용을 위한 패키지(예: HuggingFace 또는 torchtune)는 토크나이저와 편리하게 상호 작용할 수 있는 인터페이스를 제공합니다. 또한 OpenAI는 GPT 토크나이저와 연동하기 위한 tiktoken 패키지를 공개했습니다. 위 코드 스니펫은 텍스트 시퀀스를 다음과 같이 토큰화하는 과정을 보여줍니다.

**원시 텍스트**: This raw text will be tokenized
**토큰화된 텍스트**: ['This', 'Ġraw', 'Ġtext', 'Ġwill', 'Ġbe', 'Ġtoken', 'ized']

여기서 'Ġ' 문자는 해당 토큰이 공백 바로 뒤에 위치함을 나타냅니다. 이러한 특수 문자는 사용되는 토크나이저의 종류에 따라 달라질 수 있습니다. 예를 들어, 많은 토크나이저들은 단어의 연속성을 표현하기 위해 대신 '#' 문자를 사용하며, 이는 위 시퀀스의 마지막 두 토큰에 대해 ['token', '#ized']와 같은 결과를 생성할 것입니다.

**어휘(Vocabulary).** 각 LLM은 특정 토크나이저로 훈련되지만, 하나의 토크나이저가 여러 다른 LLM에 활용될 수도 있습니다. 특정 토크나이저가 생성할 수 있는 토큰들의 집합은 고정되어 있습니다. 따라서 LLM은 자신이 이해하고 훈련된 고정된 토큰 집합(즉, 토크나이저가 생성하는 토큰들)을 가지며, 이를 바탕으로 언어를 처리합니다. 이 고정된 토큰 집합은 통상적으로 LLM의 "어휘(vocabulary)"라고 불립니다. 아래 그림을 참조하십시오. 어휘의 크기는 모델마다 상이하며 여러 요인에 따라 달라지지만(예: 다국어 모델은 더 큰 어휘를 갖는 경향이 있음), 최근 LLM의 경우 총 64K에서 256K 토큰의 어휘 크기가 비교적 일반적입니다.

**LLM을 위한 토큰 어휘(및 벡터)**

**토큰 ID 및 임베딩(Embeddings).** LLM 어휘 내의 각 토큰은 고유한 정수 식별자(ID)와 연결됩니다. 예를 들어, 이전 코드는 텍스트를 토큰화할 때 다음과 같은 ID 시퀀스를 생성합니다: `[2028, 7257, 1495, 690, 387, 4037, 1534]`. 이 각각의 ID는 **임베딩 레이어(embedding layer)**에서 **토큰 임베딩(token embedding)**으로 알려진 벡터와 연결됩니다. 임베딩 레이어는 기본적으로 수많은 벡터 임베딩 행을 저장하는 거대한 행렬입니다. 특정 토큰에 대한 임베딩을 검색하려면, 임베딩 레이어에서 해당 토큰 ID에 해당하는 행을 찾아내기만 하면 됩니다. 위 그림을 참조하십시오.

**토큰 임베딩(또는 벡터)의 입력 행렬**

이제 토큰 임베딩의 목록을 확보했습니다. 이 임베딩들을 행렬 형태로 쌓아 트랜스포머 아키텍처에 실제로 입력되는 형태를 만들 수 있습니다. 위 그림을 참조하십시오. PyTorch에서는 이러한 행렬 생성이 이전 코드에 나타난 바와 같이 토크나이저와 임베딩 레이어에 의해 자동으로 처리됩니다. 토큰 임베딩 행렬의 크기는 `[C, d]`이며, 여기서 `C`는 입력 시퀀스의 토큰 수이고 `d`는 LLM이 채택한 토큰 임베딩의 차원(dimension)입니다. 일반적으로 단일 입력 시퀀스 대신 `B`개의 입력 시퀀스 배치를 다루므로, `[B, C, d]` 크기의 입력 행렬을 형성하게 됩니다. 차원 `d`는 트랜스포머 내부의 모든 계층 또는 활성화(activation)의 크기에 영향을 미치므로, `d`는 중요한 하이퍼파라미터(hyperparameter) 선택이 됩니다. 이 행렬을 트랜스포머에 입력으로 전달하기 전에, 우리는 또한 입력 3의 각 토큰에 위치 임베딩(positional embedding)을 추가하여 각 토큰의 시퀀스 내 상대적 위치 정보를 트랜스포머에 제공합니다.

**(마스크드 및 멀티 헤드) 셀프 어텐션의 원리**

이제 토큰 임베딩 행렬이라는 입력이 디코더 전용 트랜스포머에 전달되어 처리가 시작될 준비가 되었습니다. 앞서 설명했듯이, 트랜스포머는 셀프 어텐션(self-attention)과 피드포워드 변환(feed-forward transformation)을 포함하는 반복적인 블록으로 구성되며, 각 블록 뒤에는 정규화(normalization) 연산이 뒤따릅니다. 먼저 셀프 어텐션의 작동 방식을 살펴보겠습니다.

([1]에서 발췌) 셀프 어텐션이란 무엇인가? 간단히 말해, 셀프 어텐션은 시퀀스 내 각 토큰의 표현을 시퀀스 내 다른 토큰과의 관계에 기반하여 변형시킵니다. 직관적으로, 셀프 어텐션은 각 토큰의 표현을 해당 토큰과 가장 관련이 있는 시퀀스 내 다른 토큰들(자기 자신 포함)을 바탕으로 구축합니다. 다시 말해, 시퀀스 내 토큰의 의미를 이해하려고 할 때 어떤 토큰에 "주목해야" 하는지 학습하는 메커니즘입니다. 예를 들어, 위 그림에서 `making`이라는 단어의 표현이 `more`와 `difficult`라는 단어에 의해 크게 영향을 받는 것을 볼 수 있습니다. 이 단어들은 문장의 전체적인 의미를 전달하는 데 중요한 역할을 합니다.

“어텐션 함수(attention function)는 쿼리(query)와 키-값(key-value) 쌍 집합을 출력으로 매핑하며, 여기서 쿼리, 키, 값, 출력은 모두 벡터입니다. 출력은 값의 가중 합으로 계산되며, 각 값에 할당된 가중치는 쿼리와 해당 키의 호환성 함수(compatibility function)에 의해 계산됩니다.” - [1]에서 발췌

**스케일드 닷 프로덕트 어텐션(Scaled Dot Product Attention).** `[C, d]` 크기의 입력 토큰 행렬이 주어졌을 때(즉, 단순화를 위해 배치 대신 단일 입력 시퀀스를 처리한다고 가정), 우리는 세 개의 개별 선형 투영(linear projection)을 사용하여 입력을 변환함으로써 세 개의 독립적인 (변형된) 토큰 벡터 집합을 형성합니다. 이러한 투영은 각각 키(key), 쿼리(query) 및 값(value) 투영이라고 불립니다. 아래 그림을 참조하십시오.

**키, 쿼리 및 값 벡터 생성**

이러한 명명법은 다소 무작위적으로 보일 수 있지만, 정보 검색(information retrieval) 분야의 초기 연구에서 그 뿌리를 찾을 수 있습니다. 각 투영 이름에 대한 직관적인 설명은 다음과 같습니다.

*   **쿼리(query)**는 정보를 탐색하고 조회하는 데 사용되는 요소입니다. 이는 현재 시퀀스에서 다른 관련 토큰을 찾으려는 토큰을 나타냅니다.
*   **키(key)**는 시퀀스 내 다른 각각의 토큰을 나타내며, 쿼리를 시퀀스 내의 다른 관련 토큰과 매칭시키는 색인(index) 역할을 합니다.
*   **값(value)**은 쿼리가 키와 일치할 때 실제로 검색되는 정보입니다. 값은 셀프 어텐션에서 각 토큰의 최종 출력을 계산하는 데 활용됩니다.

**어텐션 점수(attention score) 계산.** 입력을 투영한 후, 우리는 입력 시퀀스 내 각 토큰 쌍 `[i, j]`에 대해 어텐션 점수 `a[i, j]`를 산출합니다. 직관적으로, `[0, 1]` 범위에 있는 이 어텐션 점수는 특정 토큰이 시퀀스 내 다른 토큰에 얼마나 "주목해야" 하는지를 포착합니다. 어텐션 점수가 높을수록 토큰 쌍이 서로 매우 관련성이 깊다는 것을 의미합니다. 위에서 암시했듯이, 어텐션 점수는 키와 쿼리 벡터를 사용하여 생성됩니다. 우리는 토큰 `i`의 쿼리 벡터와 토큰 `j`의 키 벡터의 내적(dot product)을 계산하여 `a[i, j]`를 도출합니다. 이 과정의 묘사는 아래 그림을 참조하십시오.

**토큰 쌍에 대한 어텐션 점수 계산**

시퀀스 내 모든 쌍별 어텐션 점수를 효율적으로 계산하는 방법은 다음과 같습니다.

*   쿼리 및 키 벡터를 두 개의 행렬로 쌓습니다.
*   쿼리 행렬에 전치된 키 행렬을 곱합니다.

이 연산은 전체 시퀀스에 대한 모든 쌍별 어텐션 점수를 포함하는 `[C, C]` 크기의 행렬(어텐션 행렬(attention matrix)이라고 함)을 형성합니다. 여기에서, 우리는 어텐션 행렬의 각 값을 `d`의 제곱근으로 나눕니다(이는 훈련 안정성을 향상시키는 것으로 밝혀진 접근 방식 [1]입니다) — 그리고 어텐션 행렬의 각 행에 소프트맥스(softmax) 연산을 적용합니다. 아래 그림을 참조하십시오. 소프트맥스가 적용된 후, 어텐션 행렬의 각 행은 유효한 확률 분포를 형성합니다. 각 행은 합이 1인 양수 값을 포함합니다. 어텐션 행렬의 `i`번째 행은 `i`번째 토큰과 시퀀스 내 다른 각 토큰 사이의 확률적 관계를 저장합니다.

**셀프 어텐션을 위한 어텐션 점수 및 출력 계산**

**출력 계산.** 어텐션 점수를 확보하면, 셀프 어텐션의 최종 출력을 도출하는 것은 간단합니다. 각 토큰의 출력은 단순히 값 벡터들의 가중 조합이며, 여기서 가중치는 어텐션 점수에 의해 결정됩니다. 이 출력을 계산하기 위해, 위에서 보여준 대로 어텐션 행렬에 값 행렬을 곱하기만 하면 됩니다. 특히, 셀프 어텐션은 입력의 차원을 보존합니다. 입력 내 각 토큰 벡터에 대해 변환된 `d`차원 출력 벡터가 생성됩니다.

**마스크드 셀프 어텐션(Masked self-attention).** 지금까지 우리가 학습한 공식은 기본적인 (또는 양방향) 셀프 어텐션에 대한 것입니다. 그러나 앞서 언급했듯이, 디코더 전용 트랜스포머는 마스크드 셀프 어텐션(masked self-attention)을 사용하며, 이는 시퀀스 내 각 토큰 뒤에 오는 토큰을 "마스킹(masking)"하여 기본적인 어텐션 패턴을 수정합니다. 즉, 각 토큰은 자신보다 앞에 오는 토큰만을 고려할 수 있습니다. 뒤따르는 토큰들은 정보 처리에서 배제됩니다.

**마스크드 어텐션 점수 계산**

`[“LLM”, “#s”, “are”, “cool”, “.”]`라는 토큰 시퀀스를 고려하고 토큰 `“are”`에 대한 마스크드 어텐션 점수를 계산해 봅시다. 지금까지 우리는 셀프 어텐션이 “are”와 시퀀스 내 다른 모든 토큰 사이에 어텐션 점수를 계산한다는 것을 배웠습니다. 그러나 마스크드 셀프 어텐션을 사용하면 “LLM”, “#s”, “are”에 대해서만 어텐션 점수를 계산합니다. 마스크드 셀프 어텐션은 모델이 시퀀스에서 미래의 정보를 "미리 엿보는" 것을 엄격히 금지합니다! 실제로, 이는 해당 토큰에 대한 모든 어텐션 점수를 단순히 음의 무한대(negative infinity)로 설정함으로써 달성되며, 소프트맥스 적용 후 마스크된 토큰에 대한 쌍별 확률이 0이 됩니다.

([1]에서 발췌) **어텐션 헤드(Attention heads).** 지금까지 설명된 어텐션 연산은 소프트맥스를 활용하여 시퀀스 전체에 걸쳐 계산된 어텐션 점수를 정규화합니다. 이 접근 방식은 유효한 확률 분포를 형성하지만, 시퀀스 내 여러 위치에 동시에 초점을 맞추는 셀프 어텐션의 역량을 제한하기도 합니다. 확률 분포는 하나(또는 소수의) 단어에 의해 쉽게 지배될 수 있습니다. 이 문제를 해결하기 위해, 우리는 일반적으로 여러 개의 "헤드(head)"에 걸쳐 어텐션을 병렬로 계산합니다. 위 그림을 참조하십시오. 각 헤드 내부에서 마스크드 어텐션 연산은 동일하게 수행됩니다. 그러나 우리는 다음 두 가지 조치를 취합니다.

*   각 어텐션 헤드마다 독립적인 키, 쿼리 및 값 투영을 사용합니다.
*   계산 비용을 절감하기 위해 키, 쿼리 및 값 벡터의 차원을 축소합니다(즉, 선형 투영을 조정하여 수행 가능). 보다 구체적으로, 우리는 각 어텐션 헤드의 벡터 차원을 `d`에서 `d // H`로 변경할 것입니다. 여기서 `H`는 어텐션 헤드의 수이며, 이는 멀티 헤드 셀프 어텐션(multi-headed self-attention)의 전체 계산 비용을 (상대적으로) 일정하게 유지하기 위함입니다.

**여러 어텐션 헤드의 출력 결합**

이제 우리는 셀프 어텐션을 병렬로 계산하는 여러 어텐션 헤드를 갖게 되었습니다. 그러나 여전히 셀프 어텐션 모듈의 여러 헤드로부터 단일 출력 표현을 생성해야 합니다. 각 어텐션 헤드의 출력을 결합하는 데는 연결(concatenation), 평균화(averaging), 투영(projecting) 등 여러 방법이 있습니다. 하지만 멀티 헤드 셀프 어텐션의 기본적인 구현은 다음 과정을 따릅니다(위에 묘사됨).

*   각 헤드의 출력을 연결합니다.
*   연결된 출력을 선형적으로 투영합니다.

각 어텐션 헤드가 `d // H` 차원의 토큰 벡터를 출력하므로, 모든 어텐션 헤드의 연결된 출력은 `d` 차원을 가집니다. 따라서 멀티 헤드 셀프 어텐션 연산은 여전히 입력의 원래 크기를 보존합니다.

This file contains bidirectional Unicode text that may be interpreted or compiled differently than what appears below. To review, open the file in an editor that reveals hidden Unicode characters. Learn more about bidirectional Unicode characters Show hidden characters
```python
"""
Source: https://github.com/karpathy/nanoGPT/blob/master/model.py
"""
import math
import torch
from torch import nn
import torch.nn.functional as F

class CausalSelfAttention(nn.Module):

    def __init__(self, d, H, T, bias=False, dropout=0.2,):
        """
        Arguments:
            d: size of embedding dimension
            H: number of attention heads
            T: maximum length of input sequences (in tokens)
            bias: whether or not to use bias in linear layers
            dropout: probability of dropout
        """
        super().__init__()
        assert d % H == 0
        # key, query, value projections for all heads, but in a batch
        # output is 3X the dimension because it includes key, query and value
        self.c_attn = nn.Linear(d, 3 * d, bias=bias)
        # projection of concatenated attention head outputs
        self.c_proj = nn.Linear(d, d, bias=bias)
        # dropout modules
        self.attn_dropout = nn.Dropout(dropout)
        self.resid_dropout = nn.Dropout(dropout)
        self.H = H
        self.d = d
        # causal mask to ensure that attention is only applied to
        # the left in the input sequence
        self.register_buffer("mask", torch.tril(torch.ones(T, T)).view(1, 1, T, T))

    def forward(self, x):
        B, T, _ = x.size() # batch size, sequence length, embedding dimensionality

        # compute query, key, and value vectors for all heads in batch
        # split the output into separate query, key, and value tensors
        q, k, v = self.c_attn(x).split(self.d, dim=2) # [B, T, d]

        # reshape tensor into sequences of smaller token vectors for each head
        k = k.view(B, T, self.H, self.d // self.H).transpose(1, 2) # [B, H, T, d // H]
        q = q.view(B, T, self.H, self.d // self.H).transpose(1, 2)
        v = v.view(B, T, self.H, self.d // self.H).transpose(1, 2)

        # compute the attention matrix, perform masking, and apply dropout
        att = (q @ k.transpose(-2, -1)) * (1.0 / math.sqrt(k.size(-1))) # [B, H, T, T]
        att = att.masked_fill(self.mask[:,:,:T,:T] == 0, float('-inf'))
        att = F.softmax(att, dim=-1)
        att = self.attn_dropout(att)

        # compute output vectors for each token
        y = att @ v # [B, H, T, d // H]

        # concatenate outputs from each attention head and linearly project
        y = y.transpose(1, 2).contiguous().view(B, T, self.d)
        y = self.resid_dropout(self.c_proj(y))
        return y
```
view raw causal_self_attention.py hosted with ❤ by GitHub

**전체 구현 상세.** 마스크드 멀티 헤드 셀프 어텐션의 완전한 구현은 위에 제시되어 있습니다. 여기서는 `[C, d]` 크기의 단일 입력 시퀀스를 넘어 `[B, C, d]` 크기의 입력 배치를 처리합니다. 위 코드는 지금까지 설명한 각 구성 요소를 효과적으로 구현합니다.

*   **52-59행**: 각 어텐션 헤드에 대한 키, 쿼리 및 값 투영을 계산하고(단일 선형 투영을 활용하여) 필요에 따라 분할 및 재구성합니다.
*   **62-65행**: 어텐션 점수를 계산하고, 해당 점수에 마스킹을 적용한 다음, 결과 4에 소프트맥스 변환을 적용합니다.
*   **68행**: 어텐션 행렬과 값 행렬의 곱셈을 통해 최종 출력 벡터를 산출합니다.
*   **71-72행**: 각 어텐션 헤드의 출력을 연결하고 선형 투영을 거쳐 최종 출력을 형성합니다.

PyTorch에서 다소 복잡한 행렬 조작 및 연산을 사용하지만, 이 구현은 마스크드 셀프 어텐션에 대한 우리의 설명과 정확히 부합합니다!

**피드포워드 변환의 이해**

**포인트와이즈 피드포워드 변환**

마스크드 셀프 어텐션 외에도, 트랜스포머의 각 블록은 포인트와이즈 5 피드포워드 변환을 포함합니다. 위 그림을 참조하십시오. 이 변환은 시퀀스 내 각 토큰 벡터를 동일한 피드포워드 신경망(feed-forward neural network)을 통해 개별적으로 통과시킵니다. 일반적으로 이는 은닉층(hidden layer)에 비선형 활성화 함수(non-linear activation function)(예: ReLU, GeLU 또는 SwiGLU [3])를 가진 2계층 네트워크입니다. 대부분의 경우, 은닉층의 차원은 우리 토큰 임베딩의 원래 차원보다 훨씬 큽니다(예: 4배). PyTorch에서 피드포워드 신경망을 구현하는 것은 `Linear` 모듈을 사용하면 간단하게 달성할 수 있습니다. 예시는 아래 코드를 참조하십시오.

This file contains bidirectional Unicode text that may be interpreted or compiled differently than what appears below. To review, open the file in an editor that reveals hidden Unicode characters. Learn more about bidirectional Unicode characters Show hidden characters
```python
"""
Source: https://github.com/karpathy/nanoGPT/blob/master/model.py
"""
from torch import nn

class MLP(nn.Module):
    def __init__(self, d, bias=False, dropout=0.2):
        """
        Arguments:
            d: size of embedding dimension
            bias: whether or not to use bias in linear layers
            dropout: probability of dropout
        """
        super().__init__()
        self.c_fc    = nn.Linear(d, 4 * d, bias=bias)
        self.gelu    = nn.GELU()
        self.c_proj  = nn.Linear(4 * d, d, bias=bias)
        self.dropout = nn.Dropout(dropout)

    def forward(self, x):
        x = self.c_fc(x)
        x = self.gelu(x)
        x = self.c_proj(x)
        x = self.dropout(x)
        return x
```
view raw transformer_ffnn.py hosted with ❤ by GitHub

**디코더 전용 트랜스포머 블록의 구성**

**디코더 전용 트랜스포머 블록**

디코더 전용 트랜스포머 블록을 구성하기 위해, 우리는 지금까지 살펴보았던 두 핵심 구성 요소(마스크드 셀프 어텐션과 피드포워드 변환)를 활용하며, 이 구성 요소들 사이에 정규화 연산과 잔차 연결(residual connection)을 배치합니다. 완전한 디코더 전용 트랜스포머 블록 6의 개념도는 위에 제시되어 있습니다. 잔차 연결 [4]은 신경망 계층의 입력을 해당 계층의 출력에 단순히 더한 다음, 이 합쳐진 표현을 다음 계층으로 전달하는 방식입니다. 이는 입력의 추가 없이 계층의 출력만을 다음 계층으로 전달하는 전통적인 방식과는 대조됩니다.

**일반 신경망 레이어의 잔차 연결**

잔차 연결은 딥러닝(deep learning) 분야에서 광범위하게 사용되며, 모든 종류의 신경망 계층 7에 적용될 수 있습니다. 잔차 연결을 도입하는 것은 기울기 소실/폭발(vanishing / exploding gradients) 문제를 완화하고, 역전파(backpropagation) 과정에서 기울기가 네트워크를 통해 자유롭게 흐르도록 하는 "지름길"을 제공하여 훈련의 안정성을 전반적으로 향상시킵니다. 더 자세한 내용은 여기를 참조하십시오.

**어파인 변환을 활용한 레이어 정규화**

신경망 계층의 입력(또는 출력)을 정규화하는 것 또한 훈련 안정성에 긍정적인 영향을 미칠 수 있습니다. 다양한 종류의 정규화 기법이 존재하지만, 트랜스포머/LLM에서 가장 일반적으로 채택되는 정규화 변형은 레이어 정규화(layer normalization)입니다. 위 그림을 참조하십시오. 여기에서 정규화 연산은 두 가지 핵심 구성 요소를 가집니다.

*   실질적인 정규화 수행.
*   (학습 가능한) 어파인 변환(affine transformation) 적용.

다시 말해, 우리는 단순히 정규화된 출력을 사용하는 대신, 정규화된 값에 가중치를 곱하고 편향(bias)을 추가합니다. 이 가중치와 편향은 모두 다른 네트워크 매개변수와 함께 훈련될 수 있는 학습 가능한 파라미터입니다. 레이어 정규화는 PyTorch에 이미 구현되어 있으며 사용하기 매우 간편합니다. 여기를 참조하십시오.

This file contains bidirectional Unicode text that may be interpreted or compiled differently than what appears below. To review, open the file in an editor that reveals hidden Unicode characters. Learn more about bidirectional Unicode characters Show hidden characters
```python
"""
Source: https://github.com/karpathy/nanoGPT/blob/master/model.py
"""
from torch import nn

class Block(nn.Module):
    def __init__(self, d, H, T, bias=False, dropout=0.2,):
        """
        Arguments:
            d: size of embedding dimension
            H: number of attention heads
            T: maximum length of input sequences (in tokens)
            bias: whether or not to use bias in linear layers
            dropout: probability of dropout
        """
        super().__init__()
        self.ln_1 = nn.LayerNorm(d)
        self.attn = CausalSelfAttention(d, H, T, bias, dropout)
        self.ln_2 = nn.LayerNorm(d)
        self.ffnn = MLP(d, bias, dropout)

    def forward(self, x):
        x = x + self.attn(self.ln_1(x))
        x = x + self.ffnn(self.ln_2(x))
        return x
```
view raw decoder_only_block.py hosted with ❤ by GitHub

**블록 구현 상세.** 디코더 전용 트랜스포머 블록의 구현은 위에 제시되어 있습니다. 여기서는 앞서 정의한 어텐션 및 피드포워드 변환 구현을 활용합니다. 이미 정의된 모듈을 재사용함으로써, 디코더 전용 트랜스포머 블록의 구현은 실제로 매우 간결해집니다!

**디코더 전용 트랜스포머 아키텍처의 완성**

디코더 전용 트랜스포머의 입력 방식과 블록 구조를 파악했다면, 나머지 아키텍처는 놀라울 정도로 직관적입니다. 동일한 블록을 `L`번 반복하여 쌓기만 하면 됩니다! 각 블록에서 모델 입력의 크기 `[B, C, d]`가 유지되므로, `L`번째 디코더 전용 트랜스포머 블록의 출력 또한 이 크기의 텐서(tensor)가 됩니다. 아래 그림을 참조하십시오.

**LLM으로 다음 토큰 예측**

(GPT 스타일의) 디코더 전용 트랜스포머 아키텍처의 완전한 구현은 아래에 제시되어 있습니다. 여기에서 아키텍처는 두 개의 임베딩 계층(즉, 토큰 및 위치 임베딩용), 모든 `L`개의 트랜스포머 블록, 그리고 출력 토큰 임베딩을 입력으로 받아 다음 토큰 예측을 수행하기 위한 최종 분류 모듈(레이어 정규화 및 선형 계층 포함)을 포함합니다. 모델은 크기 `[B, C]`의 입력 토큰 ID 집합을 이러한 각 구성 요소를 통해 처리하여 출력 토큰 ID 집합을 생성함으로써 작동합니다.

This file contains bidirectional Unicode text that may be interpreted or compiled differently than what appears below. To review, open the file in an editor that reveals hidden Unicode characters. Learn more about bidirectional Unicode characters Show hidden characters
```python
"""
Source: https://github.com/karpathy/nanoGPT/blob/master/model.py
"""
import torch
from torch import nn
import torch.nn.functional as F

class GPT(nn.Module):

    def __init__(self, d, H, C, V, layers, bias=False, dropout=0.2,):
        """
        Arguments:
            d: size of embedding dimension
            H: number of attention heads
            C: maximum length of input sequences (in tokens)
            V: size of the token vocabulary
            layers: number of decoder-only blocks
            bias: whether or not to use bias in linear layers
            dropout: probability of dropout
        """
        super().__init__()
        self.transformer = nn.ModuleDict(dict(
            wte = nn.Embedding(V, d), # token embeddings
            wpe = nn.Embedding(C, d), # position embeddings
            drop = nn.Dropout(dropout),
            blocks = nn.ModuleList([Block(d, H, C, bias, dropout) for _ in range(layers)]),
            ln_f = nn.LayerNorm(d),
            head = nn.Linear(d, V, bias=bias),
        ))

    def forward(self, idx, targets=None):
        # idx is a [B, C] matrix of token indices
        # targets is a [B, C] matrix of target (next) token indices
        device = idx.device
        _, C = idx.size() # [B, C]
        pos = torch.arange(0, C, dtype=torch.long, device=device) # generate token and position embeddings

        tok_emb = self.transformer.wte(idx) # [B, C, d]
        pos_emb = self.transformer.wpe(pos) # [C, d]
        x = self.transformer.drop(tok_emb + pos_emb)

        # pass through all decoder-only blocks
        for block in self.transformer.blocks:
            x = block(x)
        x = self.transformer.ln_f(x) # final layer norm

        if targets is not None:
            # compute the loss if we are given targets
            logits = self.transformer.head(x)
            loss = F.cross_entropy(logits.view(-1, logits.size(-1)), targets.view(-1), ignore_index=-1,)
        else:
            # only look at last token if performing inference
            logits = self.transformer.head(x[:, [-1], :])
            loss = None
        return logits, loss
```
view raw gpt.py hosted with ❤ by GitHub

**출력 생성(디코딩) 메커니즘.** LLM은 **다음 토큰 예측**이라는 특정 목표를 위해 훈련됩니다. 다시 말해, 이 모델들은 주어진 토큰 시퀀스를 바탕으로 다음에 올 토큰을 매우 정확하게 예측하는 데 특화되어 있습니다. 우리가 앞서 살펴보았듯이, 모델의 출력은 각 입력 토큰에 대응하는 출력 토큰 벡터들의 목록입니다. 따라서 우리는 이러한 입력 토큰 중 어느 것에 대해서든 다음 토큰을 다음과 같은 과정을 통해 예측할 수 있습니다.

*   특정 토큰에 대한 출력 임베딩을 추출합니다.
*   이 임베딩을 선형 계층을 통해 전달합니다. 여기서 출력 차원은 모델 어휘의 크기와 동일합니다.
*   모델 출력의 argmax를 취하여 가장 확률이 높은 토큰 ID를 얻습니다.

텍스트 시퀀스를 연속적으로 생성하기 위해, 우리는 이 과정을 반복적으로 수행합니다. 텍스트 프롬프트를 입력으로 받아들이고, 이를 디코더 전용 트랜스포머 전체를 통과시킨 후, 출력 시퀀스에서 마지막 토큰 벡터를 가져와 다음 토큰을 예측합니다. 이 예측된 다음 토큰을 원래 입력 시퀀스에 추가하고 이 과정을 다시 반복합니다. 이 자기회귀적 디코딩(autoregressive decoding) 과정은 모든 LLM이 출력을 생성하는 데 사용하는 핵심 메커니즘입니다. 아래 그림을 참조하십시오.

**다음 토큰 예측을 통한 자기회귀적 출력 생성**

**왜 디코더 구조인가?** 이 아키텍처를 이해했으니, 우리는 자연스럽게 궁금해할 수 있습니다. 왜 LLM은 트랜스포머의 디코더 구성 요소만을 활용할까요? 트랜스포머의 인코더와 디코더 사이의 주요 차이점은 사용되는 어텐션의 종류에 있습니다. 인코더는 양방향 셀프 어텐션(bidirectional self-attention)을 사용합니다. 이는 주어진 토큰의 표현을 만들 때 해당 토큰 이전과 이후의 모든 토큰이 셀프 어텐션 메커니즘에 의해 고려된다는 것을 의미합니다. 이와 대조적으로, 디코더는 마스크드 셀프 어텐션(masked self-attention)을 사용하며, 이는 토큰이 시퀀스에서 자신을 따르는 (미래) 토큰에 주의를 기울이는 것을 방지합니다.

**다음 토큰 예측을 위한 인과적 마스크**

마스크드 셀프 어텐션의 활용 덕분에, 디코더는 다음 토큰 예측 작업에 매우 효과적입니다. 만약 각 토큰이 자신의 표현을 구축할 때 시퀀스에서 미래의 정보를 미리 볼 수 있다면, 모델은 단순히 "속임수"를 통해(즉, 시퀀스에서 다음 토큰을 직접 복사하여) 다음 토큰을 예측하는 것을 학습할 수도 있습니다. 위 그림을 참조하십시오. 마스크드 셀프 어텐션은 모델이 자신보다 앞에 오는 토큰 정보만을 사용하여 다음 토큰을 예측하기 위한 일반화 가능한 패턴을 학습하도록 강제하며, 이는 디코더를 LLM에 완벽하게 적합한 구조로 만듭니다.

**MoE(Mixture-of-Experts) 모델의 생성 원리**

“딥러닝에서 모델은 일반적으로 모든 입력에 대해 동일한 매개변수를 재사용합니다. MoE(Mixture of Experts) 모델은 이를 거부하고 대신 들어오는 각 예제에 대해 다른 매개변수를 선택합니다. 그 결과는 엄청난 수의 매개변수를 가지지만 일정한 계산 비용을 갖는 희소하게 활성화된 모델입니다.” - [6]에서 발췌

이제 디코더 전용 트랜스포머에 대한 심층적인 이해를 얻었으므로, 다음 단계는 MoE(Mixture-of-Experts) 모델을 구축하는 것입니다. MoE 기반 LLM은 기본적인 디코더 전용 트랜스포머 아키텍처의 틀을 유지하지만, 몇 가지 미묘하면서도 중요한 방식으로 이 아키텍처를 변형합니다. 이러한 핵심 아이디어에 대한 더 깊이 있는 내용은 아래 게시물들을 참조하십시오.

**MoE(Mixture-of-Experts): 조건부 계산의 탄생과 부상**
Cameron R. Wolfe, Ph.D. · 2024년 3월 18일
전체 기사 읽기

**MoE(Mixture-of-Experts) LLM**
Cameron R. Wolfe, Ph.D. · 1월 27일
전체 기사 읽기

모델 아키텍처를 MoE 형태로 변환하는 것이 본질적으로 복잡하지는 않지만, 모델이 효과적으로 작동하려면 올바르게 구현되어야 하는 많은 세부 사항들이 존재합니다. 또한, 이러한 모델을 적절하게 훈련시키려면 추가적인 주의와 깊은 이해가 필요합니다. 일반적으로 MoE 모델은 표준 LLM에 비해 훈련 과정이 더 까다롭습니다.

**전문가 계층(Expert Layers)의 도입**

표준 디코더 전용 트랜스포머와 비교했을 때, MoE 모델이 적용하는 주요 수정 사항은 트랜스포머 블록의 피드포워드 구성 요소 내에 있습니다. 일반적으로 이 블록은 모든 토큰 벡터에 포인트와이즈 방식으로 적용되는 하나의 피드포워드 네트워크를 가집니다. 그러나 단일 피드포워드 네트워크를 사용하는 대신, MoE는 여러 개의 피드포워드 네트워크를 생성하며, 각 네트워크는 자체적인 독립적인 가중치를 가집니다. 우리는 이러한 각 네트워크를 "전문가(expert)"라고 부르며, 여러 전문가를 포함하는 피드포워드 계층을 "전문가 계층(expert layer)"이라고 지칭합니다. 한 계층에 `N`개의 전문가가 있다면, 우리는 `i`번째 전문가를 `E_i` 표기법을 사용하여 참조할 수 있습니다. 아래 그림을 참조하십시오.

**PyTorch 구현 상세.** PyTorch에서 전문가 계층을 구현하는 것은 생각보다 복잡하지 않습니다. 아래에 제시된 바와 같이, 우리는 이전과 동일한 구현 방식을 사용하지만, 하나의 피드포워드 네트워크 대신 여러 개의 피드포워드 네트워크를 생성합니다. 이 구현의 주요 복잡성은 PyTorch의 표준 `Linear` 계층을 직접 사용하지 않는다는 점입니다. 대신, 우리는 모든 전문가의 가중치를 여러 `Parameter` 객체로 감싸서(wrap) 배치 행렬 곱셈(batch matrix multiplication) 연산자를 사용하여 모든 전문가의 출력을 배치 단위로 효율적으로 계산할 수 있도록 합니다. 이 구현 방식은 각 전문가의 출력을 계산하기 위해 개별적으로 루프를 돌 필요를 없애주어 효율성을 획기적으로 향상시킵니다.

This file contains bidirectional Unicode text that may be interpreted or compiled differently than what appears below. To review, open the file in an editor that reveals hidden Unicode characters. Learn more about bidirectional Unicode characters Show hidden characters
```python
"""
Based upon ColossalAI OpenMoE: https://github.com/hpcaitech/ColossalAI/blob/main/colossalai/moe/experts.py
"""
import torch
from torch import nn

class MLPExperts(nn.Module):
    def __init__(self, d, n_exp=8, bias=False, dropout=0.2,):
        """
        Arguments:
            d: size of embedding dimension
            n_exp: the number of experts to create in the expert layer
            bias: whether or not to use bias in linear layers
            dropout: probability of dropout
        """
        super().__init__()
        self.bias = bias
        self.c_fc = nn.Parameter(torch.empty(n_exp, d, 4 * d))
        self.c_proj = nn.Parameter(torch.empty(n_exp, 4 * d, d))
        self.fc_bias = nn.Parameter(torch.empty(n_exp, 1, 4 * d)) if self.bias else None
        self.proj_bias = nn.Parameter(torch.empty(n_exp, 1, d)) if self.bias else None
        self.gelu = nn.GELU()
        self.dropout = nn.Dropout(dropout)

    def forward(self, x):
        x = torch.bmm(x, self.c_fc)
        if self.bias:
            x += self.fc_bias
        x = self.gelu(x)
        x = torch.bmm(x, self.c_proj)
        if self.bias:
            x += self.proj_bias
        x = self.dropout(x)
        return x
```
view raw expert_layer.py hosted with ❤ by GitHub

**MoE 생성 원리.** MoE 기반 디코더 전용 트랜스포머를 구축하려면, 트랜스포머의 피드포워드 계층을 MoE(또는 전문가) 계층으로 단순히 전환합니다. MoE 계층 내의 각 전문가는 해당 계층의 원래 피드포워드 네트워크와 동일한 아키텍처를 가집니다. 즉, 우리는 전문가 계층 내에 원래 피드포워드 네트워크의 여러 독립적인 복사본을 포함시킵니다. 아래 그림을 참조하십시오.

**디코더 전용 트랜스포머에 전문가 추가 ([1]에서 발췌)**

그러나 트랜스포머의 모든 피드포워드 계층에 전문가를 사용할 필요는 없습니다. 대부분의 MoE 기반 LLM은 `P`의 스트라이드(stride)를 활용합니다. 이는 `P`번째 계층마다 전문가 계층으로 변환되고, 나머지 계층들은 기존 피드포워드 네트워크를 그대로 유지한다는 것을 의미합니다.

“ST-MoE 모델은 32개의 전문가를 가지며, 전문가 레이어 빈도는 1/4입니다(매 네 번째 FFN 레이어가 MoE 레이어로 대체됩니다).” - [24]에서 발췌

이 아이디어의 상위 수준 구현은 아래에 제시된 의사 코드(pseudocode)에 나타나 있습니다. 이러한 "인터리브된(interleaved)" MoE 계층은 MoE 내의 총 전문가 수를 제어하며, 이는 모델의 성능과 계산 효율성 사이의 균형을 조절하는 데 유용한 메커니즘입니다.

```python
transformer_blocks = []
for i in range(num_blocks):
    use_moe = (i % P) == 0 # when use_moe = False, this is regular transformer block
                           # when use_moe = True, this is an expert layer
    transformer_blocks.append(Block(use_moe=use_moe))
```

**토큰을 전문가에게 라우팅하는 과정**

MoE 기반 아키텍처의 핵심 이점은 효율성이지만, 단순히 전문가를 사용하는 것만으로는 효율성이 저절로 향상되지 않습니다! 사실, 모델의 각 계층에 더 많은 전문가를 추가하는 것은 모델의 총 매개변수 수와 요구되는 계산량을 크게 증가시킬 뿐입니다. 효율성을 진정으로 높이려면, 우리는 각 계층 내에서 전문가의 부분 집합만을 희소하게 선택하고 활용해야 합니다. 전문가를 희소하게 활용함으로써, 훈련 및 추론의 계산 비용을 크게 늘리지 않으면서도 훨씬 더 큰 모델의 이점을 누릴 수 있습니다.

“MoE 아키텍처를 사용하면 밀집 모델(dense model)이 일반적으로 달성하는 것보다 모델 품질과 추론 효율성 사이에서 더 나은 절충점을 얻을 수 있습니다.” - 출처

**전문가 선택 메커니즘.** `d`차원 토큰 벡터로 표현되는 단일 토큰을 가정해 봅시다. 우리의 목표는 이 토큰을 처리할 전문가의 부분 집합(크기 `k`)을 선별하는 것입니다. MoE 관련 문헌에서는 일반적으로 토큰이 이러한 전문가에게 "라우팅(routed)"될 것이라고 표현합니다. 이 라우팅 연산을 계산하고 최적화할 알고리즘이 필요합니다.

**단일 토큰을 위한 라우팅 메커니즘**

가장 기본적인 라우팅 알고리즘은 토큰 벡터에 선형 변환을 적용하여 `N` 크기(즉, 전체 전문가 수)의 벡터를 생성하는 것입니다. 그런 다음 소프트맥스 함수를 적용하여 우리 토큰에 대한 전문가 집합에 대한 확률 분포를 형성할 수 있습니다. 위 그림을 참조하십시오. 우리는 이 분포를 활용하여 분포에서 상위 `K`개의 전문가를 선택함으로써 우리 토큰이 어떤 전문가에게 라우팅되어야 할지 결정할 수 있습니다. 이때 상위 `K`개의 값(즉, "전문가 확률") 또한 중요한 정보를 제공합니다.

**간단한 라우터 구현 상세.** 위에서 설명했듯이, 이 라우팅 메커니즘은 실제로 매우 간결합니다. 본질적으로는 단지 선형 계층일 뿐입니다! 이 소프트맥스 라우터의 구현은 아래에 나와 있으며, 우리 라우터의 출력은 다음 두 가지를 포함합니다.

*   입력의 각 토큰에 대한 상위 `K`개 전문가 인덱스 집합.
*   선택된 전문가와 관련된 상위 `K`개 전문가 확률(즉, 상위 `K`개 인덱스 각각에 대한 확률 값).

이러한 단순함에도 불구하고, 이 라우팅 메커니즘은 효과적이며 그 목적을 훌륭히 수행합니다. 대부분의 현대 MoE 모델은 소프트맥스를 사용하는 이와 유사한 선형 라우팅 방식을 채택합니다.

This file contains bidirectional Unicode text that may be interpreted or compiled differently than what appears below. To review, open the file in an editor that reveals hidden Unicode characters. Learn more about bidirectional Unicode characters Show hidden characters
```python
import torch
from torch import nn
from torch.nn import functional as F

class BasicSoftmaxRouter(nn.Module):
    def __init__(self, d, n_exp=8, top_k=2, use_noisy_top_k=True,):
        """
        Arguments:
            d: size of embedding dimension
            n_exp: the number of experts to create in the expert layer
            top_k: the number of active experts for each token
            use_noisy_top_k: whether to add noise when computing expert output
        """
        super().__init__()
        # router settings
        self.top_k = top_k
        assert self.top_k >= 1 and self.top_k <= n_exp
        self.use_noisy_top_k = use_noisy_top_k

        # linear projection for (noisy) softmax routing
        # no bias used, see page 4 eq (4) in https://arxiv.org/abs/1701.06538
        self.w_g = nn.Linear(d, n_exp, bias=False)
        self.w_noise = nn.Linear(d, n_exp, bias=False) if self.use_noisy_top_k else None

    def forward(self, x):
        # eq (4) in https://arxiv.org/abs/1701.06538
        logits = self.w_g(x) # [B, C, d] -> [B, C, n_exp]
        if self.use_noisy_top_k:
            # (optionally) add noise into the router
            noise = F.softplus(self.w_noise(x))
            noise *= torch.randn_like(noise)
            logits += noise

        top_k_logits, top_k_indices = logits.topk(self.top_k, dim=-1) # [B, C, k]
        return top_k_logits, top_k_indices
```
view raw basic_softmax_router.py hosted with ❤ by GitHub

선택적으로, 우리는 라우팅 메커니즘에 의도적인 노이즈를 추가할 수 있습니다. 이는 [8]에서 제안된 접근 방식으로, 신경망에 MoE를 적용한 초기 연구 중 하나입니다. 라우팅 메커니즘의 출력에 이 소량의 (학습 가능한) 노이즈를 추가함으로써(더 자세한 내용은 아래 그림 참조), MoE의 훈련 과정을 정규화(regularize)하는 데 기여할 수 있습니다.

**상위 k 소프트맥스 라우팅에 노이즈 추가 ([7]에서 발췌)**

**활성 매개변수(Active parameters)의 개념.** MoE 계층 내에서 각 토큰을 처리하기 위해 전문가의 부분 집합만을 선택하기 때문에, MoE 문헌에는 "활성(active)" 매개변수라는 중요한 개념이 등장합니다. 간단히 말해, 특정 토큰을 처리할 때 MoE 모델의 전체 매개변수 중에서 극히 작은 부분(각 MoE 계층에서 선택된 전문가에 의해 결정됨)만이 실제로 활성화됩니다. MoE에 의해 수행되는 총 계산량은 전체 매개변수 수보다는 활성 매개변수 수에 비례하여 결정됩니다.

**전문가 용량(Expert Capacity)의 중요성**

“하드웨어 활용도를 높이기 위해, 희소 모델의 대부분 구현은 각 전문가에 대해 정적 배치 크기를 가집니다. 전문가 용량(expert capacity)은 각 전문가에게 라우팅될 수 있는 토큰의 수를 의미합니다. 이 용량을 초과하면 오버플로우된 토큰은 잔차 연결을 통해 다음 레이어로 전달됩니다.” - [5]에서 발췌

전문가 계층에서 수행되는 계산은 그 특성상 동적입니다. 우리는 라우터의 출력에 기반하여 각 전문가가 처리할 토큰을 선택하며, 이는 MoE에 입력으로 제공되는 토큰 시퀀스에 따라 유동적으로 달라집니다. 각 전문가 입력의 이러한 동적 특성은 전문가 계층의 구현을 다소 복잡하게 만들 수 있습니다. 각 전문가의 입력이 다르고 예측 불가능한 크기를 가질 것이라는 사실을 어떻게 효율적으로 처리할 수 있을까요?

**전문가 용량 계산**

**전문가 용량.** MoE의 대부분의 실제 구현은 이 문제를 해결하기 위해 각 전문가에 대해 고정된 배치 크기를 활용합니다. 이는 하드웨어 활용도를 크게 향상시키는 데 유용한 기법입니다. 각 전문가는 "전문가 용량"이라고 불리는 동일한 정적 배치 크기를 사용합니다. 위에 정의된 전문가 용량은 각 배치에서 단일 전문가에게 전송될 수 있는 최대 토큰 수를 명시합니다. 전문가 용량은 용량 계수(capacity factor) 설정을 통해 제어됩니다. 용량 계수가 1이라는 것은 토큰이 전문가들에게 균일하게 라우팅된다는 것을 의미하며, 용량 계수를 1보다 크게 설정하면 전문가 간의 불균형한 토큰 라우팅 상황을 처리하기 위한 추가적인 버퍼를 제공합니다. 물론 이는 더 높은 메모리 사용량과 다소 낮은 효율성이라는 대가를 수반합니다.

([6]에서 발췌) 전문가에게 라우팅된 토큰 수가 전문가 용량을 초과하는 경우, 우리는 해당 계산을 수행하지 않고 해당 표현이 트랜스포머의 잔차 연결을 통해 다음 계층으로 직접 흐르도록 함으로써 이러한 추가 토큰들을 "드롭(drop)"합니다. 위 그림을 참조하십시오. MoE는 비교적 낮은 용량 계수 8에서도 잘 작동하지만, 너무 많은 토큰이 드롭되는 것은 피해야 합니다. 용량 계수는 훈련 및 평가 중에도 다르게 설정될 수 있습니다. 예를 들어, ST-MoE [5]는 훈련 중에는 1.25, 평가 중에는 2.0의 용량 계수를 사용합니다.

**PyTorch 구현 상세.** 이제 전문가 용량과 전문가 계층 내 라우팅의 세부 사항을 명확히 이해했으므로, 완전한 기능을 하는 라우터를 구현해야 합니다. 이 라우터는 이전 구현(즉, 소프트맥스가 있는 선형 계층)과 동일한 핵심 논리를 공유하지만, 각 전문가에 대한 고정 크기 입력 텐서를 생성함으로써 이 구현을 더욱 발전시킬 것입니다. 아래 그림을 참조하십시오. 이것이 완전한 기능을 하는 구현이라는 점을 고려할 때, 아래 라우터 코드는 이전보다 더 복잡합니다. 그러나 우리는 이 구현을 다음 핵심 구성 요소로 요약할 수 있습니다.

*   **41-47행**: (노이즈가 포함된) 선형 라우터의 출력을 계산합니다.
*   **49-52행**: 상위 `K`개 전문가와 그와 관련된 확률을 산출합니다.
*   **55-58행**: 전문가 용량을 계산합니다.
*   **60-88행**: 정교한 PyTorch 인덱싱 및 텐서 조작을 사용하여 전문가 입력 9의 배치를 구성하는 과정을 처리합니다.
*   **90-93행**: 전문가 입력의 최종 배치를 형성합니다.

This file contains bidirectional Unicode text that may be interpreted or compiled differently than what appears below. To review, open the file in an editor that reveals hidden Unicode characters. Learn more about bidirectional Unicode characters Show hidden characters
```python
import math
import torch
from torch import nn
from torch.nn import functional as F

class Router(nn.Module):
    def __init__(self, d, n_exp=8, top_k=2, use_noisy_top_k=True, capacity_factor=1.25,):
        """
        Arguments:
            d: size of embedding dimension
            n_exp: the number of experts to create in the expert layer
            top_k: the number of active experts for each token
            use_noisy_top_k: whether to add noise when computing expert output
            capacity_factor: used to compute expert capacity
        """
        super().__init__()
        self.d = d
        self.n_exp = n_exp
        self.top_k = top_k
        assert self.top_k >= 1 and self.top_k <= n_exp
        self.use_noisy_top_k = use_noisy_top_k
        self.capacity_factor = capacity_factor

        self.w_g = nn.Linear(d, n_exp, bias=False)
        self.w_noise = nn.Linear(d, n_exp, bias=False) if self.use_noisy_top_k else None

    def forward(self, x):
        # get the total number of tokens in the batch
        B, C, _ = x.size()
        num_tokens = B * C

        # eq (4) in https://arxiv.org/abs/1701.06538
        logits = self.w_g(x) # [B, C, d] -> [B, C, n_exp]
        if self.use_noisy_top_k:
            # (optionally) add noise into the router
            noise = F.softplus(self.w_noise(x))
            noise *= torch.randn_like(noise)
            logits += noise

        # top-K expert selection, compute probabilities over active experts
        top_k_logits, top_k_indices = logits.topk(self.top_k, dim=-1) # [B, C, K]
        router_probs = torch.full_like(logits, float('-inf')) # [B, C, n_exp]
        router_probs.scatter_(-1, top_k_indices, top_k_logits)
        router_probs = F.softmax(router_probs, dim=-1)

        # compute the expert capacity
        exp_capacity = math.floor(self.top_k * self.capacity_factor * num_tokens / self.n_exp)
        exp_capacity += exp_capacity % 2 # make sure expert capacity is an even integer
        exp_capacity = int(exp_capacity)

        # make a multi-hot mask of chosen experts
        # values are 0 if expert not chosen, 1 if expert chosen
        exp_mask = F.one_hot(top_k_indices, num_classes=self.n_exp) # [B, C, K, n_exp]
        exp_mask = exp_mask.view(num_tokens, self.top_k, self.n_exp) # [B * C, K, n_exp]
        exp_mask = exp_mask.permute(1, 0, 2) # [K, B * C, n_exp]

        # compute index for each token in expert batch
        # NOTE: cumsum counts top-1 first, top-2 second, etc.
        # to prioritize top experts when dropping tokens
        exp_rank = exp_mask.reshape(self.top_k * num_tokens, self.n_exp) # [K * B * C, n_exp]
        exp_rank = torch.cumsum(exp_rank, dim=0) - 1 # cumsum of expert selections [K * B * C, n_exp]
        exp_rank = exp_rank.reshape(self.top_k, num_tokens, self.n_exp) # [K, B * C, n_exp]

        # mask entries beyond expert capacity and compute used capacity
        exp_mask *= torch.lt(exp_rank, exp_capacity) # [K, B * C, n_exp]

        # matrix storing token position in batch of corresponding expert
        exp_rank = torch.sum(exp_mask * exp_rank, dim=-1) # [K, B * C]

        # mask probabilities to only include selected experts
        router_probs = router_probs.view(num_tokens, self.n_exp)[None, :] # [1, B * C, n_exp]
        exp_weights = exp_mask * router_probs # [K, B * C, n_exp]

        # position of each token within the capacity of the selected expert
        exp_rank_sc = F.one_hot(exp_rank, num_classes=exp_capacity) # [K, B * C, exp_capacity]

        # weight of selected expert for each token at position the capacity of that expert
        exp_weights = torch.sum(exp_weights.unsqueeze(3) * exp_rank_sc.unsqueeze(2), dim=0) # [B * C, n_exp, exp_capacity]
        exp_mask = exp_weights.bool() # binary mask of selected experts for each token

        # reshape tokens into batches for each expert, return both weights and batches
        # [n_exp, exp_capacity, B * C] * [B * C, d] -> [n_exp, exp_capacity, n_embd]
        x = x.view(num_tokens, self.d)
        expert_batches = exp_mask.permute(1, 2, 0).type_as(x) @ x
        return exp_weights, exp_mask, expert_batches
```
view raw full_softmax_router.py hosted with ❤ by GitHub

**로드 밸런싱(Load Balancing) 및 보조 손실(Auxiliary Losses)의 필요성**

“게이팅 네트워크(gating network)는 항상 동일한 소수의 전문가에 대해 큰 가중치를 생성하는 상태로 수렴하는 경향이 있습니다. 이러한 불균형은 선호되는 전문가가 더 빠르게 훈련되고 따라서 게이팅 네트워크에 의해 더욱 많이 선택되기 때문에 자기 강화됩니다.” - [7]에서 발췌

지금까지 우리가 설계한 라우팅 시스템은 각 계층에서 전문가의 균형 잡힌 활용을 명시적으로 장려하지 않습니다. 결과적으로, 모델은 전체 전문가 풀을 충분히 활용하는 대신, 모든 토큰에 대해 동일한 소수의 전문가만을 반복적으로 선택하는 상태로 수렴할 가능성이 높습니다. 위 인용문에서 설명된 이 현상은 일반적으로 "라우팅 붕괴(routing collapse)"라고 불리며, MoE 훈련의 주요 과제 중 하나입니다.

([6]에서 발췌) **로드 밸런싱 손실.** 훈련 과정 동안 전문가들이 균형 있게 선택되도록 유도하기 위해, 우리는 모델이 전문가들을 균등하게 활용하는 것에 보상하는 추가 구성 요소를 훈련 손실에 간단히 추가할 수 있습니다. 보다 구체적으로, 우리는 위에 제시된 보조 손실 항을 생성합니다. 이는 전문가 중요도(즉, 각 전문가에 할당된 확률)와 로드 밸런싱(즉, 각 전문가에게 전송된 토큰 수)을 모두 측정합니다. 이러한 접근 방식은 [2]에서 제안되었으며, 저자들은 두 가지 양을 고려하는 손실을 만듭니다.

*   각 전문가 10에게 할당된 라우터 확률의 비율.
*   각 전문가에게 발송된 토큰의 비율.

이 두 양을 각각의 `N`차원 벡터에 저장하면, 이 두 벡터의 내적을 취하여 단일 손실 항을 생성할 수 있습니다. 이 손실은 전문가가 균일한 확률과 로드 밸런싱을 받을 때 최소화됩니다. PyTorch에서 이 로드 밸런싱 손실의 구현은 아래에 제공됩니다. 이 구현은 다음 핵심 구성 요소를 가집니다.

*   **9-17행**: 로드 밸런싱 손실 계산에 사용되는 모든 상수와 입력 텐서를 정의합니다.
*   **19-24행**: 각 전문가에게 전송된 토큰의 비율 또는 분수를 계산합니다.
*   **26-27행**: 각 전문가에게 할당된 확률의 분수를 계산합니다.
*   **29-31행**: 각 전문가 11에 대한 토큰 비율과 확률 사이의 (스케일링된) 내적을 취합니다.

This file contains bidirectional Unicode text that may be interpreted or compiled differently than what appears below. To review, open the file in an editor that reveals hidden Unicode characters. Learn more about bidirectional Unicode characters Show hidden characters
```python
"""
Computes Switch Transformer auxiliary loss (https://arxiv.org/abs/2101.03961)
See equations (4)-(6) on page 7
"""
import torch
import torch.nn.functional as F

# constants
B = 16 # batch size
C = 256 # sequence length
n_exp = 8 # number of experts
K = 2 # number of active expert

# define tensors needed to compute load balancing loss
indices = torch.randint(1, n_exp + 1, (B, C, K)) # top-K indices ([B, C, K])
expert_probs = F.softmax(torch.rand(B, C, n_exp), dim=2) # expert probabilities ([B, C, n_exp])

# equation (5): compute ratio of tokens allocated to each expert
# total number of tokens is defined as total tokens in batch * K
with torch.no_grad():
    one_hot_indices = F.one_hot(indices, num_classes=n_exp) # [B, C, K, n_exp]
    one_hot_indices = torch.sum(one_hot_indices.float(), dim=2) # [B, C, n_exp] (sum over K dimension)
    tokens_per_expert = torch.mean(one_hot_indices.float(), dim=(0, 1))

# equation (6): compute ratio of router probability allocated to each expert
prob_per_expert = torch.mean(expert_probs.float(), dim=(0, 1))

# equation (4): take a scaled dot product between prob / token allocation vectors
# multiply the result by the number of experts
load_balance_loss = n_exp * torch.sum(prob_per_expert * tokens_per_expert)
```
view raw load_balancing_loss.py hosted with ❤ by GitHub

**라우터 z-손실(Router z-loss).** 로드 밸런싱 손실을 보완하기 위해, [3]의 저자들은 **라우터 z-손실**이라고 불리는 또 다른 보조 손실 항을 제안합니다. 라우터 z-손실은 라우팅 메커니즘에 의해 예측되는 **로짓(logits)**의 크기를 제한하는 역할을 합니다(여기서 로짓은 확률이 아니라 소프트맥스가 적용되기 전의 원시 값입니다). 공식은 아래 그림을 참조하십시오. 라우터가 (지수 함수를 포함하는) 소프트맥스 함수를 사용한다는 사실 때문에 이러한 로짓이 너무 커지는 것을 원하지 않습니다. 그러나 이러한 로짓은 훈련 중에 매우 커질 수 있으며, 이는 훈련 과정을 불안정하게 만드는 반올림 오류(round-off errors)로 이어질 수 있습니다. 심지어 전체(float32) 정밀도를 사용할 때도 마찬가지입니다. 라우터 z-손실은 MoE가 이러한 로짓의 크기를 작게 유지하도록 장려하고, 결과적으로 이러한 반올림 오류를 피하도록 돕습니다.

“라우터는 float32 정밀도로 전문가에 대한 확률 분포를 계산합니다. 그러나 가장 큰 규모에서는 이것이 신뢰할 수 있는 훈련을 제공하기에 불충분하다는 것을 발견했습니다.” - [3]에서 발췌

라우터 z-손실의 구현은 아래에 제시되어 있으며, 세 가지 핵심 단계를 포함합니다.

*   **8-14행**: 라우터 z-손실 계산에 필요한 입력 텐서(즉, 라우팅 메커니즘의 로짓)를 생성합니다.
*   **21행**: 라우터 로짓의 제곱된 logsumexp를 취합니다. 이는 지수, 합, 로그 연산을 순서대로 적용하는 수치적으로 안정적인 약식 표현입니다.
*   **24행**: 위 연산의 결과를 모든 토큰에 대해 합산하고 총 토큰 수로 나눕니다(즉, 평균을 취합니다).

This file contains bidirectional Unicode text that may be interpreted or compiled differently than what appears below. To review, open the file in an editor that reveals hidden Unicode characters. Learn more about bidirectional Unicode characters Show hidden characters
```python
"""
Computes ST-MoE router z loss (https://arxiv.org/abs/2202.08906)
See equation (5) on page 7
"""
import torch

# constants
B = 16 # batch size
C = 256 # sequence length
n_exp = 8 # number of experts

# create input tensor for router z-loss
router_logits = torch.rand(B, C, n_exp) # [B, C, n_exp]

# exponentiate logits, sum logits of each expert, take log, and square
# code below is equivalent to the following:
# z_loss = torch.exp(router_logits)
# z_loss = torch.sum(z_loss, dim=-1)
# z_loss = torch.log(z_loss) ** 2.0
router_z_loss = torch.logsumexp(router_logits, dim=-1) ** 2.0 # [B, C]

# sum over all tokens and divide by total number of tokens
router_z_loss = torch.mean(router_z_loss)
```
view raw router_z_loss.py hosted with ❤ by GitHub

**보조 손실 결합 전략.** 여러 보조 손실이 존재한다는 점을 고려할 때, 우리는 실제로 어떤 것을 사용해야 할지 의문을 가질 수 있습니다. 정답은: **모든 손실을 함께 활용하는 것입니다!** 우리는 훈련 중에 이러한 각 손실을 표준 언어 모델링 손실에 추가할 수 있습니다. 각 보조 손실에는 곱해지는 스케일링 계수(scaling factor)가 있으며, 그런 다음 모든 (스케일링된) 손실을 합산하여 최종 손실 함수를 구성합니다. 아래 그림을 참조하십시오. 로드 밸런싱 및 라우터 z-손실에 대한 일반적인 기본 스케일링 계수는 각각 `0.001`과 `0.01`입니다.

**최신 연구 동향.** 우리가 곧 확인하겠지만, 이 섹션에서 배운 보조 손실들은 매우 효과적으로 작동합니다. 그러나 최근 연구 [8]는 스케일링 계수가 어떻게 설정되는지에 따라 이러한 보조 손실이 특정 상황에서 훈련 안정성을 위해 모델 성능을 희생시킬 수도 있음을 보여주었습니다. 따라서 MoE 훈련을 위한 최적의 접근 방식과 전략은 여전히 (매우) 활발하게 연구되고 있는 분야입니다.

**DeepSeek-v3 [8]의 보조 손실 없는 로드 밸런싱 기법**

예를 들어, 최근에 제안된 DeepSeek-v3 [8] 모델(DeepSeek-R1 추론 모델을 구축하는 데 사용된 기반 모델)은 보조 손실을 사용하지 않는 로드 밸런싱 전략을 채택합니다. 이는 상위 `K`개 전문가를 선택할 때 라우터 출력에 동적 편향(dynamic bias)을 단순히 추가하는 방식입니다. 위 그림을 참조하십시오. 이 편향은 충분히 선택되지 않은 전문가에 대해서는 증가하고, 너무 많이 선택된 전문가에 대해서는 감소하여, 활용도가 낮은 전문가가 선택될 가능성을 높입니다. 이 동적 편향은 모델 성능 저하 없이 로드 밸런싱을 개선하는 것으로 밝혀졌습니다. 그러나 로드 밸런싱 손실은 [8]에서도 여전히 사용됩니다(단지 더 작은 스케일링 계수와 함께).

“우리는 각 훈련 단계의 전체 배치에 대한 전문가 로드를 계속 모니터링합니다. 각 단계가 끝날 때, 해당 전문가가 과부하되면 편향 항을 𝛾만큼 감소시키고, 과소 부하되면 𝛾만큼 증가시킵니다. 여기서 𝛾는 편향 업데이트 속도라고 불리는 하이퍼파라미터입니다.” - [8]에서 발췌

**디코더 전용 MoE의 구현 상세**

**MoE 기반 디코더 전용 트랜스포머 아키텍처**

이제 전문가 계층의 모든 주요 구성 요소를 이해했으니, 이러한 개념들을 결합하여 완전한 MoE 기반 디코더 전용 아키텍처를 만들어 봅시다. 이 모델 내의 MoE 블록(위에 표시됨)은 다음을 포함할 것입니다.

*   일반적인 (마스크드) 셀프 어텐션 계층.
*   모델의 `P`번째 계층마다 일반 피드포워드 계층 대신 전문가 계층.

이 블록 구조는 표준 디코더 전용 트랜스포머와 유사하지만, 모델 계층의 일부에서 피드포워드 계층을 전문가 계층(MoE 블록을 형성함)으로 대체한다는 점에서 차이가 있습니다. 먼저, 전문가 계층의 최종 출력이 어떻게 계산되는지에 대한 몇 가지 남은 세부 사항을 다루겠습니다. 그런 다음, MoE 기반 디코더 전용 트랜스포머의 전체 구현을 제시할 것입니다.

**전문가 계층 출력 계산.** 라우팅 메커니즘을 사용하여 주어진 토큰에 대한 활성 전문가 집합을 결정했다면, 이 전문가 계층의 최종 출력을 다음과 같은 방식으로 계산할 수 있습니다.

*   토큰을 활성 전문가들에게 보냅니다.
*   이 토큰에 대한 각 활성 전문가의 출력을 계산합니다.
*   각 토큰에 대한 전문가 출력의 가중 평균을 취합니다. 여기서 가중치는 라우터에 의해 각 활성 전문가에게 할당된 확률입니다.

이 과정은 위 그림에서 단일 토큰에 대해 시각적으로 묘사되어 있습니다. MoE에 대한 최근 연구는 또한 모든 토큰에 대해 항상 활성화되는 "공유(shared)" 전문가의 개념을 도입했습니다. 공유 전문가는 라우팅 논리를 약간 수정하지만, 위에 설명된 동일한 핵심 아이디어는 여전히 유효합니다. 이 주제에 대한 더 자세한 내용은 여기를 참조하십시오. 완전한 전문가 계층의 구현은 아래에 제시되어 있으며, 여기서 이러한 아이디어가 PyTorch에 어떻게 적용되었는지 확인할 수 있습니다. 49행에서 우리는 라우터로부터 각 전문가에 대한 데이터 배치와 각 토큰에 대한 관련 전문가 확률을 얻습니다. 그런 다음 이 배치들을 전문가 피드포워드 네트워크(52행)를 통해 전달하여 각 전문가의 출력을 얻습니다. 마지막으로, 54-58행에서 각 전문가의 출력에 관련 확률을 곱하여 전문가 계층의 최종 출력을 형성합니다.

This file contains bidirectional Unicode text that may be interpreted or compiled differently than what appears below. To review, open the file in an editor that reveals hidden Unicode characters. Learn more about bidirectional Unicode characters Show hidden characters
```python
"""
Based upon ColossalAI OpenMoE
"""
from torch import nn

class MOELayer(nn.Module):
    def __init__(self, d, n_exp=8, top_k=2, use_noisy_top_k=True, capacity_factor=1.25, bias=False, dropout=0.2,):
        """
        Arguments:
            d: size of embedding dimension
            n_exp: the number of experts to create in the expert layer
            top_k: the number of active experts for each token
            use_noisy_top_k: whether to add noise when computing expert output
            capacity_factor: used to compute expert capacity
            bias: whether or not to use bias in linear layers
            dropout: probability of dropout
        """
        super().__init__()
        self.router = Router( # (noisy) top k router
            d=d,
            n_exp=n_exp,
            top_k=top_k,
            use_noisy_top_k=use_noisy_top_k,
            capacity_factor=capacity_factor,
        )
        self.experts = MLPExperts( # group of MLPs (experts)
            d=d,
            n_exp=n_exp,
            bias=bias,
            dropout=dropout,
        )

    def forward(self, x : torch.Tensor):
        B, C, d = x.size() # track original shape of input
        num_tokens = (B * C) # pass each token through the router

        exp_weight, exp_mask, exp_batches = self.router(x)

        # compute expert output
        exp_out = self.experts(exp_batches) # [n_exp, exp_capacity, d]

        # aggregate expert outputs based on router weights
        # eq (2) on page 4 of ST-MoE (https://arxiv.org/abs/2202.08906)
        exp_weight = exp_weight.view(num_tokens, -1) # [B * C, n_exp * exp_capacity]
        exp_out = exp_out.view(-1, d) # [n_exp * exp_capacity, d]
        output = exp_weight @ exp_out # [B * C, d]

        # resize output before return
        return output.view(B, C, d)
```
view raw expert_layer.py hosted with ❤ by GitHub

**PyTorch에서 MoE 블록 구현.** 이제 우리는 디코더 전용 트랜스포머 블록을 수정하여 일반적인 피드포워드 계층 대신 선택적으로 전문가 계층을 사용할 수 있습니다. 이는 아래 코드에서 달성되며, 여기서 우리는 `MLP` 모듈을 새로운 `MoELayer`로 드롭인 대체(drop-in replacement)하여 `MoEBlock`을 형성합니다.

This file contains bidirectional Unicode text that may be interpreted or compiled differently than what appears below. To review, open the file in an editor that reveals hidden Unicode characters. Learn more about bidirectional Unicode characters Show hidden characters
```python
from torch import nn

class MoEBlock(nn.Module):
    def __init__(self, d, H, C, n_exp, top_k, use_noisy_top_k=True, capacity_factor=1.25, bias=False, dropout=0.2,):
        """
        Arguments:
            d: size of embedding dimension
            H: number of attention heads
            C: maximum length of input sequences (in tokens)
            n_exp: the number of experts to create in the expert layer
            top_k: the number of active experts for each token
            use_noisy_top_k: whether to add noise when computing expert output
            capacity_factor: used to compute expert capacity
            bias: whether or not to use bias in linear layers
            dropout: probability of dropout
        """
        super().__init__()
        self.ln_1 = nn.LayerNorm(d)
        self.attn = CausalSelfAttention(d, H, C, bias, dropout)
        self.ln_2 = nn.LayerNorm(d)
        self.mlp = MOELayer(d, n_exp, top_k, use_noisy_top_k, capacity_factor, bias, dropout,)

    def forward(self, x):
        x = x + self.attn(self.ln_1(x))
        x = x + self.mlp(self.ln_2(x))
        return x
```
view raw moe_block.py hosted with ❤ by GitHub

여기에서, 우리 MoE 아키텍처의 최종 구현은 이전의 디코더 전용 트랜스포머(GPT) 구현과 구조적으로 거의 동일합니다. 유일한 변경 사항은 `P`번째 `Block`을 `MoEBlock`으로 대체한다는 것입니다. 인터리브된 MoE 블록의 추가를 제외하고는 코드가 이전에 정의된 GPT 모델과 동일하므로, 여기서는 이 구현을 명시적으로 다시 작성하는 것을 피할 것입니다.

**nanoMoE를 처음부터 사전 훈련하기: 실제 적용**

이제 MoE가 어떻게 작동하는지에 대한 이론적 이해를 갖추었으니, 이 아키텍처를 사용하여 LLM을 처음부터 사전 훈련해 봅시다. MoE 기반 LLM의 완전한 구현은 아래 저장소에 있습니다. nanoMoE라고 불리는 이 구현은 Andrej Karpathy의 nanoGPT 저장소를 기반으로 합니다. 그러나 원래 GPT 아키텍처는 MoE 기반 디코더 전용 트랜스포머 아키텍처를 사용하도록 신중하게 수정되었습니다.

nanoMoE 저장소

nanoMoE 저장소는 이 게시물에서 지금까지 살펴보았던 모든 MoE 구성 요소에 대한 코드를 재사용합니다. 이 구현의 핵심 구성 요소는 다음과 같습니다.

*   **모델 구현**: MoE 모델을 구성하는 기능이 추가된 GPT 모델 정의를 참조하십시오. [링크]
*   **훈련**: 모든 훈련 코드는 단일 파일에 있으며, 원래 nanoGPT 코드에서 의미 있게 수정되지 않았습니다. [링크]
*   **데이터셋**: nanoMoE는 OpenWebText 데이터셋의 250억 토큰 부분 집합 12에서 사전 훈련됩니다(nanoGPT와 동일하지만 토큰 수가 적음). [링크]
*   **구성**: nanoMoE를 사전 훈련하는 데 사용된 최종 훈련 구성은 다음 섹션에서 설명할 것이며, 여기에서 찾을 수 있습니다.

이 섹션에서는 nanoMoE를 성공적으로 사전 훈련하기 위해 발견된 모범 사례를 추가로 설명하고, 사전 훈련 결과를 검토하며, 이 중간 크기 MoE 모델에 대해 발견된 최적의 사전 훈련 설정을 상세히 설명할 것입니다.

**MoE 훈련을 위한 최적의 모범 사례**

“MoE의 여러 주목할 만한 성공에도 불구하고, 복잡성, 통신 비용 및 훈련 불안정성으로 인해 광범위한 채택이 방해받아 왔습니다.” - [6]에서 발췌

MoE는 오래전에 제안된 개념이지만, LLM 연구 분야에서 그 인기는 최근에야 폭발적으로 증가했습니다. 수년 동안 MoE 채택의 주요 걸림돌은 구현 및 사용의 어려움이었습니다. 밀집 모델(dense model)에 비해 MoE는 구조적으로 더 복잡하고 일반적으로 훈련 과정에서 불안정성에 취약하다는 특징을 가집니다.

**MoE가 불안정한 근본적인 이유.** 우리가 살펴보았듯이, MoE 기반 LLM은 디코더 전용 트랜스포머 아키텍처에 미미한 수정만을 가합니다. 이를 염두에 두고, 우리는 궁금해할 수 있습니다. MoE 아키텍처의 정확히 어떤 부분이 훈련 중 어려움을 유발할까요? 왜 MoE의 훈련은 표준 LLM에 비해 덜 안정적일까요?

**nanoMoE 사전 훈련 중 발산 현상**

MoE를 훈련할 때 발생하는 두 가지 주요 문제는 다음과 같습니다.

*   **라우팅 붕괴(Routing collapse)**: 모델이 전체 전문가 풀을 활용하지 않고 동일한 소수의 전문가만을 반복적으로 선택하여 활용하는 상태로 수렴하는 경향이 있습니다.
*   **수치 불안정성(Numerical instability)**: MoE는 특히 라우터 부분에서 반올림 오류(round-off errors)를 경험할 수 있습니다(즉, 소프트맥스에서 지수 함수를 사용하기 때문) 13.

이러한 문제들은 훈련 불안정성으로 직결되며, 이는 모델의 손실이 훈련 과정 중에 단순히 발산(diverge)할 수 있음을 의미합니다. nanoMoE 훈련의 구체적인 예시는 위 그림을 참조하십시오. 이런 일이 발생하면, 훈련 과정을 중단하고 이전에 저장된 체크포인트(checkpoint)에서 다시 시작해야 하는데, 이는 시간이 많이 소요되고 비효율적입니다(즉, 많은 유휴 GPU 시간 발생!). 이상적으로는 이러한 불안정성을 회피하는 안정적인 훈련 과정을 목표로 합니다. 따라서 MoE 훈련 안정성을 개선하기 위한 핵심 모범 사례들을 다루겠습니다.

**보조 손실의 활용.** 이전에 논의했듯이, MoE를 훈련할 때 여러 보조 손실 중에서 하나를 선택할 필요는 없습니다. 대신, 다양한 보조 손실들을 단일 손실 함수로 효과적으로 결합할 수 있습니다. nanoMoE의 경우, 우리는 훈련 중에 표준 보조 로드 밸런싱 손실과 라우터 z-손실을 모두 사용합니다. 올바른 보조 손실의 조합은 전문가의 균일한 활용을 가능하게 하고 훈련 중 라우팅 붕괴를 방지함으로써 전반적인 훈련 안정성을 크게 향상시킵니다.

**훈련 정밀도(Training precision) 최적화.** LLM을 훈련할 때, 일반적으로 혼합 정밀도 훈련(mixed precision training)을 사용하는 것이 효율적입니다. 이는 모델의 일부 구성 요소를 전체 float32 정밀도 대신 더 낮은 float16 또는 bfloat16 정밀도 형식으로 실행하도록 변환하는 기술입니다. 이 기능은 PyTorch에서 자동 혼합 정밀도(AMP) 모듈을 통해 자동으로 지원되며, 모델 성능 저하 없이 훈련 비용을 크게 절감할 수 있습니다. 다시 말해, 이는 최소한의 코드 변경으로 쉽게 활성화할 수 있는 "무료" 사전 훈련 속도 향상 기법입니다.

“BF16 기준선과 비교할 때, 우리의 FP8 훈련 모델의 상대 손실 오류는 일관되게 0.25% 미만을 유지하며, 이는 훈련 무작위성의 허용 가능한 범위 내에 있습니다.” - [8]에서 발췌

혼합 정밀도는 이미 오랫동안 사용되어 왔지만, 연구자들은 최근 LLM 훈련 정밀도를 16비트보다 더 낮은 수준으로 낮추는 방법을 적극적으로 탐구하고 있습니다. 예를 들어, DeepSeek-v3 [8]는 8비트 정밀도를 사용하여 훈련됩니다. 그러나 훈련 정밀도가 감소할수록 동일한 수준의 모델 품질을 유지하는 것이 더욱 어려워집니다. FP8 정밀도로 대규모 LLM 훈련을 성공적으로 구현하려면 새롭고 복잡한 양자화(quantization) 기술이 필요합니다. 그렇지 않으면, 그렇게 낮은 정밀도로 LLM을 훈련하는 것이 모델의 성능에 부정적인 영향을 미칠 수 있습니다.

```python
with torch.amp.autocast(device_type='cuda', enabled=False):
    # AMP is disabled for code in this block!
    <router code goes here>
```

**이것이 MoE와 어떤 관련이 있을까요?** 앞서 언급했듯이, MoE 내부의 라우팅 메커니즘은 본질적으로 수치 불안정성에 취약합니다. 라우터의 출력을 더 낮은 정밀도로 계산하면 이 문제가 더욱 악화될 수 있습니다! 이 문제는 [6]에 명시적으로 기술되어 있으며, 저자들은 낮은 정밀도 훈련이 라우터에서 상당한 반올림 오류로 이어진다는 것을 발견했습니다. 이 문제를 해결하기 위해, 우리는 AMP로 훈련할 때도 라우터를 전체(float32) 정밀도로 실행해야 합니다. 이는 MoE의 라우팅 메커니즘에서 AMP를 단순히 비활성화함으로써 달성할 수 있습니다. 위 코드 스니펫을 참조하십시오.

**가중치 초기화(Weight initialization) 전략.** 전통적으로 대규모 신경망의 안정적인 훈련을 위한 가장 중요한 요소 중 하나는 올바른 가중치 초기화 전략을 채택하는 것이었습니다. 예를 들어, Glorot 또는 He 초기화가 있습니다. 이러한 기술들은 배치 정규화(batch normalization)와 같은 전략과 함께, 이전에는 훈련이 매우 어려웠던 놀라울 정도로 깊은 신경망을 훈련할 수 있는 능력을 열어주었습니다. LLM의 경우, 우리는 일반적으로 이러한 동일한 가중치 초기화 전략을 따릅니다. 그러나 [6]의 저자들은 MoE를 위해 특별히 설계된 약간 수정된 가중치 초기화 방식을 채택할 것을 권장합니다.

```python
# linear layers have flipped dimensions ([out_dim, in_dim]) in torch
w_fan_in = module.weight.shape[-1]
w_std = (scale / w_fan_in) ** 0.5
torch.nn.init.trunc_normal_(
    module.weight,
    mean=0.0,
    std=w_std,
    a=-2*w_std,
    b=2*w_std,
)
```

이 가중치 초기화 전략은 평균이 0(µ = 0)이고 표준 편차가 σ = SQRT(s/n)로 주어지는 절단 정규 분포(truncated normal distribution)에서 가중치를 샘플링합니다. 여기서 `s`는 스케일 하이퍼파라미터이고 `n`은 초기화되는 계층의 입력 크기(즉, fan-in 전략)입니다. [6]의 저자들은 또한 "모델 품질을 개선하고 훈련 불안정성 가능성을 줄이기 위해" `s = 0.1`의 감소된 스케일 하이퍼파라미터를 사용할 것을 권장합니다. PyTorch에서 이 수정된 가중치 초기화 전략의 구현은 위에 제시되어 있습니다.

**MoE 미세 조정(finetuning)의 과제.** 이 개요에서는 nanoMoE 사전 훈련에만 집중할 것입니다. 그러나 MoE는 표준 밀집 모델에 비해 미세 조정하기가 더 까다로울 수 있다는 점을 인지해야 합니다. 특히, MoE는 매우 많은 매개변수를 가지고 있다는 특성 때문에 과적합(overfitting)에 취약합니다. 이러한 대규모 모델은 방대한 데이터셋에 대한 사전 훈련에는 탁월하지만, 소량의 데이터로 미세 조정할 경우 쉽게 과적합될 수 있습니다. 우리는 이 문제를 인식하고 MoE를 미세 조정할 때 과적합을 방지하기 위한 최선의 노력을 기울여야 합니다(예: 더 높은 드롭아웃(dropout) 비율 적용). nanoMoE의 미세 조정 및 과적합 방지에 대한 심층적인 탐구는 향후 연구 과제로 남겨둡니다.

**nanoMoE 사전 훈련 실험 및 결과**

이제 MoE를 안정적으로 훈련하는 데 활용할 수 있는 다양한 기법들을 이해했으니, nanoMoE를 처음부터 사전 훈련하여 실제 환경에서의 효과를 시험해 봅시다. 이 명령들을 직접 테스트하려면 하나 이상의 GPU에 접근할 수 있어야 합니다. 여기에 제시된 실험을 위해, 저는 개인 워크스테이션에서 두 개의 RTX 3090 GPU를 사용했습니다. 이들은 일반적인 소비자용 GPU입니다. 메모리가 그리 많지 않습니다(각 24GB). 따라서 사전 훈련 설정은 모든 것이 GPU 메모리에 적재되고 일주일 이내에 전체 훈련이 완료될 수 있도록 적절히 축소되었습니다.

**일반적인 사전 훈련 설정.** 사전 훈련에 사용된 최종 구성은 여기 에서 확인할 수 있으며, 다음과 같은 주요 설정을 가집니다.

*   **모델 아키텍처**: 6개 계층(또는 블록), 셀프 어텐션 계층당 6개 어텐션 헤드, `d = 368`(임베딩 차원), `N = 8` (총 전문가 수), `K = 2` (활성 전문가 수), `P = 2` (격층마다 MoE 블록 사용).
*   **전문가 용량**: 훈련 시 1.25, 평가 시 2.0의 용량 계수.
*   **보조 손실**: 로드 밸런싱 보조 손실(스케일링 계수 `0.01`)과 라우터 z-손실(스케일링 계수 `0.001`)을 모두 사용합니다.
*   **정밀도**: 훈련에는 자동 혼합 정밀도(bfloat16)를 사용하지만, 라우터는 항상 전체(float32) 정밀도를 사용하도록 강제합니다.
*   **학습률(Learning rate)**: 표준 LLM 학습률 전략을 채택합니다. 훈련 시작 시 `6e-5`에서 `6e-4`로 선형 웜업(linear warmup)한 후, `6e-5`로 코사인 감쇠(cosine decay)합니다.
*   **가중치 초기화**: MoE 훈련 안정성을 개선하기 위해 [6]에서 제안된 수정된 가중치 초기화 방식을 사용합니다.

**사전 훈련 데이터셋.** nanoGPT와 유사하게, nanoMoE 사전 훈련을 위해 OpenWebText 데이터셋을 활용합니다. 사전 훈련 프로세스는 약 250억 개의 총 토큰으로 축소되었습니다. 이는 nanoGPT 사전 훈련에 사용된 토큰 수의 약 10%에 해당합니다. 이 더 작은 데이터셋 덕분에 두 개의 3090 GPU에서 약 5일 만에 사전 훈련을 완료할 수 있었습니다. 그러나 더 나은 GPU 설정(예: 8×A100 GPU)을 확보하고 `max_iters = 600,000` (50,000 대신)으로 설정함으로써 이를 전체 규모의 사전 훈련 실행으로 쉽게 확장할 수 있습니다.

**안정성 실험 결과.** nanoMoE의 훈련 안정성에 대한 다양한 설정의 영향을 면밀히 테스트하기 위해, 우리는 다섯 가지 다른 실험을 수행했습니다. 먼저, 보조 손실이나 모범 사례를 전혀 사용하지 않는 기준선 nanoMoE 모델을 사전 훈련했습니다. 예상대로, 이는 좋지 않은 로드 밸런싱과 심각한 불안정성으로 이어졌습니다. 그런 다음, 사전 훈련 안정성에 미치는 개별적인 영향을 관찰하기 위해 여러 개선 사항을 하나씩 활성화했습니다.

*   보조 로드 밸런싱 손실 추가.
*   라우터 z-손실 추가.
*   라우터의 전체 정밀도 사용.
*   개선된 가중치 초기화 방식 적용.

이 다섯 가지 실험 결과는 아래 그림에 명확하게 제시되어 있습니다. 보시다시피, 사전 훈련 과정에 적용된 각 개선 사항은 훈련 안정성에 점진적인 향상을 가져왔습니다. 특히, 사전 훈련의 발산 현상은 훈련 과정에서 점점 더 늦게 나타났습니다. 그리고 모든 개선 사항을 함께 활성화했을 때, 모델은 실제로 아무런 문제 없이 전체 훈련 과정을 성공적으로 완료했습니다! 이는 여기서 논의된 아이디어들이 nanoMoE의 훈련 안정성에 실질적이고 긍정적인 영향을 미친다는 것을 분명히 입증합니다.

**nanoMoE를 위한 다양한 안정성 기술 테스트**

관심 있는 분들은 직접 이 아이디어들을 시도해 보시길 적극 권장합니다! 아래에 표시된 명령을 사용하여 훈련 구성을 조정하고 사전 훈련 프로세스를 실행하기만 하면 됩니다. 이 명령은 하나 이상의 GPU를 사용할 수 있는 단일 노드에서 사전 훈련을 실행한다고 가정합니다.

```bash
torchrun --standalone --nproc_per_node=<number of GPUs> train.py <path to config; e.g., config/train_nano_moe.py>
```

**MoE(Mixture-of-Experts)에 대한 추가 학습 및 미래 전망**

이 개요를 통해 우리는 표준 디코더 전용 트랜스포머 아키텍처에서 출발하여 이를 MoE 아키텍처를 활용하도록 수정함으로써 MoE(Mixture-of-Experts) 기반 LLM이 어떻게 작동하는지에 대한 심층적인 이해를 얻었습니다. 이어서, OpenWebText 데이터셋에서 nanoMoE라고 불리는 중간 크기의 MoE 기반 LLM을 처음부터 사전 훈련함으로써 이러한 이론적 아이디어들을 실제적으로 적용해 보았습니다. MoE는 일반적으로 표준 LLM보다 훈련하기 더 어렵다고 여겨지지만, 우리의 실험을 통해 보조 손실, 혼합 정밀도 활용, 더 나은 가중치 초기화 등과 같은 기법들이 MoE를 성공적으로 (즉, 어떠한 불안정성도 없이) 훈련하는 데 어떻게 적용될 수 있는지 확인할 수 있었습니다! nanoMoE는 훌륭한 학습 도구이지만, MoE의 대부분의 실제 구현은 이보다 훨씬 더 복잡할 것입니다. MoE가 LLM 연구 및 실제 응용에서 어떻게 사용되는지 더 깊이 배우려면, 효율적인 훈련 및 추론을 위한 프로덕션급 MoE 프레임워크(예: OpenMoE [9] 또는 Megablocks [10])와 Mixtral, DeepSeek-v3 또는 DBRX와 같은 최신 MoE 기반 모델들의 연구 결과를 살펴보는 것이 필수적입니다. MoE는 LLM의 확장성과 효율성을 혁신적으로 개선할 잠재력을 가지고 있으며, 앞으로 더욱 다양한 응용 분야에서 그 중요성이 커질 것으로 예상됩니다.

뉴스레터가 처음이신가요? 안녕하세요! 저는 Cameron R. Wolfe입니다. 넷플릭스(Netflix)의 딥러닝(Deep Learning) 박사이자 연구 과학자입니다. 이것은 독자들이 AI 연구의 중요한 주제를 이해하도록 돕는 Deep (Learning) Focus 뉴스레터입니다. 뉴스레터가 마음에 드신다면, 구독하거나 공유하거나 X 및 LinkedIn에서 저를 팔로우해주세요!

구독

**참고문헌**

[1] Vaswani, Ashish, et al. "Attention is all you need." Advances in neural information processing systems 30 (2017).
[2] Sennrich, Rico, Barry Haddow, and Alexandra Birch. "Neural machine translation of rare words with subword units." arXiv preprint arXiv:1508.07909 (2015).
[3] Shazeer, Noam. "Glu variants improve transformer." arXiv preprint arXiv:2002.05202 (2020).
[4] He, Kaiming, et al. "Deep residual learning for image recognition." Proceedings of the IEEE conference on computer vision and pattern recognition. 2016.
[5] Zoph, Barret, et al. "St-moe: Designing stable and transferable sparse expert models." arXiv preprint arXiv:2202.08906 (2022).
[6] Fedus, William, Barret Zoph, and Noam Shazeer. "Switch transformers: Scaling to trillion parameter models with simple and efficient sparsity." Journal of Machine Learning Research 23.120 (2022): 1-39.
[7] Shazeer, Noam, et al. "Outrageously large neural networks: The sparsely-gated mixture-of-experts layer." arXiv preprint arXiv:1701.06538 (2017).
[8] Liu, Aixin, et al. "Deepseek-v3 technical report." arXiv preprint arXiv:2412.19437 (2024).
[9] Xue, Fuzhao, et al. "Openmoe: An early effort on open mixture-of-experts language models." arXiv preprint arXiv:2402.01739 (2024).
[10] Gale, Trevor, et al. "Megablocks: Efficient sparse training with mixture-of-experts." Proceedings of Machine Learning and Systems 5 (2023): 288-304.

1 이 아키텍처는 그 자체로 "새로운" 것은 아닙니다. 아주 오래전부터 존재해 왔습니다. 하지만 대규모 LLM 애플리케이션에서의 채택은 비교적 최근의 일입니다.
2 완전한 인코더-디코더 모델의 디코더에서 사용되는 교차 어텐션(cross-attention) 레이어를 제거하기 때문에 디코더는 약간 다릅니다.
3 트랜스포머를 위한 기본적인 위치 인코딩(또는 임베딩)에 대한 설명은 여기에서 찾을 수 있습니다. 그러나 대부분의 현대 LLM은 [1]의 이 기본적인 위치 인코딩 방식 대신 회전 위치 임베딩(RoPE, rotary positional embeddings)을 사용합니다.
4 여기 우리의 구현은 또한 어텐션 드롭아웃(attention dropout)을 수행합니다. 이는 정규화(regularization) 목적으로 훈련 중에 특정 어텐션 점수를 무작위로 드롭하는 것입니다.
5 "포인트와이즈(pointwise)"라는 단어는 시퀀스의 모든 토큰 벡터에 동일한 연산이 적용됨을 나타냅니다. 이 경우, 우리는 시퀀스의 모든 토큰 벡터를 동일한 가중치를 가진 동일한 피드포워드 신경망을 통해 개별적으로 전달합니다.
6 우리는 각 레이어의 입력에 정규화가 적용되는 사전 정규화(pre-normalization) 구조를 사용합니다. 원래 트랜스포머 [1]는 사후 정규화(post-normalization) 구조를 사용했지만, 이후 분석에서 사전 정규화가 더 유리하다는 것이 밝혀졌습니다.
7 신경망 레이어 내에서 잔차 연결을 적용하려면 해당 레이어의 입력 및 출력 차원이 동일해야 합니다. 차원이 동일하지 않은 경우에도 입력을 선형적으로 투영하여 잔차 연결을 적용할 수 있습니다.
8 용량 계수 튜닝에 대한 자세한 내용과 실험은 [5] 및 [6]을 참조하십시오.
9 여기서 세부 사항은 그리 중요하지 않습니다. 이는 라우터의 연산을 벡터화하기 위해 도입된 구현 복잡성일 뿐입니다. 그러나 이는 이해에 관심 있는 사람들에게 PyTorch에서 훌륭한 코딩 연습이 될 것입니다!
10 이 양은 우리의 라우팅 알고리즘에 의해 예측되며, 따라서 미분 가능합니다. 따라서 각 전문가에게 전송된 토큰의 비율 자체가 미분 가능한 양이 아니더라도 손실 함수 전체는 미분 가능합니다.
11 우리는 또한 연산 결과에 N(총 전문가 수)을 곱합니다. 이는 N 값이 증가함에 따라 손실이 일정하게 유지되도록 보장합니다.
12 이 토큰 수는 2개의 RTX 3090 GPU 설정에서 전체 사전 훈련 실행이 약 5일 만에 완료될 수 있도록 선택되었습니다.
13 소프트맥스 변환은 꽤 흔한 연산이지만, 표준 디코더 전용 트랜스포머는 아키텍처 내 어디에도 이러한 지수 함수를 가지고 있지 않다는 점에 유의해야 합니다!