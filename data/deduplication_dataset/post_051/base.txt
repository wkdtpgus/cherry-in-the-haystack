# **nanoMoE: 전문가 혼합(Mixture-of-Experts) 대규모 언어 모델(LLMs)을 PyTorch로 바닥부터 구현하기**

Author: Cameron Wolfe
URL: https://cameronrwolfe.substack.com/p/nano-moe

============================================================

대규모 언어 모델(LLM) 연구는 지난 몇 년간 놀라운 속도로 발전해 왔습니다. 그러나 대부분의 LLM이 기반으로 하는 아키텍처인 디코더 전용 트랜스포머(decoder-only transformer)는 이 분야의 혼란스럽고 빠른 발전에도 불구하고 고정된 상태를 유지해 왔습니다. 최근에는 최고 연구실에서 새로운 아키텍처인 MoE(Mixture-of-Experts)가 채택되는 것을 보기 시작했습니다. 예를 들어, GPT-4는 MoE 기반으로 알려져 있으며, 최근에 제안되어 큰 인기를 얻고 있는 DeepSeek-v3 및 R1 모델도 마찬가지입니다. 아래를 참조하십시오.

“오픈 소스 모델 역량의 한계를 더욱 확장하기 위해, 우리는 모델을 확장하고 6,710억 개의 매개변수를 가지며 각 토큰에 대해 370억 개의 매개변수가 활성화되는 대규모 MoE(Mixture-of-Experts) 모델인 DeepSeek-V3를 소개합니다.” - [8]에서 발췌

MoE 기반 LLM은 대규모 모델의 훈련 및 사용을 더욱 효율적으로 만들 수 있는 능력 덕분에 인기를 얻은 디코더 전용 트랜스포머의 수정된 버전을 사용합니다. MoE 기반 LLM은 총 매개변수(parameter) 수 면에서 매우 거대합니다. 그러나 모델의 출력을 계산할 때는 이러한 매개변수 중 일부만(추론(inference) 중에 동적으로 선택됨) 사용됩니다. MoE의 희소성(sparsity)은 매우 크고 강력한 LLM의 비용을 극적으로 줄여줍니다. 많은 최첨단 LLM이 MoE 기반 아키텍처를 사용하기 시작하고 있다는 점을 고려할 때, MoE에 대한 심층적인 이해를 개발하는 것이 중요합니다. 이 게시물에서는 PyTorch에서 nanoMoE라고 불리는 중간 크기의 MoE 모델을 처음부터 구축(및 사전 훈련)함으로써 이 방향으로 나아갈 것입니다. nanoMoE의 모든 코드는 아래 저장소에서 사용할 수 있으며, 이는 MoE 사전 훈련을 지원하도록 확장된 Andrej Karpathy의 nanoGPT 라이브러리의 포크(fork)입니다. nanoMoE가 어떻게 작동하는지 이해하기 위해 필요한 배경 정보를 설명하는 것으로 시작하겠습니다. 그런 다음 nanoMoE의 각 구성 요소를 처음부터 구축하여 궁극적으로 모델의 (성공적인) 사전 훈련 실행으로 마무리할 것입니다.

nanoMoE 저장소

**디코더 전용 트랜스포머의 기본**

**디코더 전용 트랜스포머: 생성형 LLM의 핵심 동력**
Cameron R. Wolfe, Ph.D. · 2024년 3월 4일
전체 기사 읽기

MoE 기반 LLM을 이해하려면 먼저 대부분의 LLM이 기반으로 하는 표준 아키텍처인 **디코더 전용 트랜스포머(decoder-only transformer) 아키텍처**를 이해해야 합니다. 이 아키텍처는 GPT에 의해 대중화된 인코더-디코더 트랜스포머(encoder-decoder transformer) 아키텍처 [1]의 수정된 버전입니다. 이전 게시물에서 이 아키텍처를 깊이 연구했지만(위 참조), 이 지식이 이 게시물의 나머지 부분에 필수적이므로 여기에서 다시 다룰 것입니다. 아키텍처를 설명하는 동안, 우리는 디코더 전용 트랜스포머의 최소한의 기능적 구현인 Andrej Karpathy의 nanoGPT를 참고 자료로 활용할 것입니다.

([1]에서 발췌) 원본 아키텍처. 원래 [1]에서 기계 번역(machine translation) 작업을 해결하기 위해 제안된 트랜스포머는 인코더(encoder)와 디코더(decoder) 모듈을 모두 가지고 있습니다. 위를 참조하십시오. 여기서는 전체(인코더-디코더) 트랜스포머에 초점을 맞추지 않을 것입니다. 그러나 이 아키텍처에 대한 상세한(그리고 널리 인용되는) 개요는 여기에서 찾을 수 있습니다. 현대 LLM에 더 일반적으로 사용되는 디코더 전용 트랜스포머는 이름에서 알 수 있듯이 이 아키텍처에서 인코더를 단순히 제거하고 디코더 2만 사용합니다. 실제적으로, 이는 디코더 전용 트랜스포머 아키텍처의 모든 레이어가 다음을 포함한다는 것을 의미합니다.

*   마스크드 셀프 어텐션(masked self-attention) 레이어.
*   피드포워드(feed-forward) 레이어.

완전한 디코더 전용 트랜스포머 아키텍처를 형성하기 위해, 우리는 구조는 동일하지만 독립적인 가중치(weight)를 가지는 이 레이어 L개를 서로 위에 쌓습니다. 이 구조의 묘사는 아래 그림에 제공됩니다.

**디코더 전용 트랜스포머 아키텍처**

이제 더 나은 이해를 위해 아키텍처의 각 구성 요소를 개별적으로 논의해 보겠습니다. 모델의 입력 구조부터 시작하여 각 레이어의 구성 요소(즉, 셀프 어텐션(self-attention) 및 피드포워드 레이어)와 이들이 어떻게 결합되어 전체 모델 아키텍처를 형성하는지 다룰 것입니다.

**텍스트에서 토큰으로**

우리 대부분이 아마도 알고 있듯이, LLM의 입력은 단순히 텍스트 시퀀스(즉, 프롬프트(prompt))입니다. 그러나 위 그림에서 보는 입력은 텍스트 시퀀스가 아닙니다! 오히려 모델의 입력은 토큰 벡터(token vector)의 목록입니다. 텍스트를 모델에 입력으로 전달한다면, 텍스트 입력에서 이러한 벡터를 어떻게 생성할까요?

**원시 텍스트를 토큰 시퀀스로 변환**

**토큰화(Tokenization).** LLM의 입력을 구성하는 첫 번째 단계는 원시 텍스트 입력(문자 시퀀스)을 개별 토큰(token)으로 분해하는 것입니다. 토큰화라고 불리는 이 과정은 모델의 **토크나이저(tokenizer)**에 의해 처리됩니다. 다양한 종류의 토크나이저가 있지만, BPE(Byte-Pair Encoding) 토크나이저 [2]가 가장 일반적입니다. 자세한 내용은 여기를 참조하십시오. 이 토크나이저들은 원시 텍스트 시퀀스를 입력으로 받아 위 그림에 표시된 대로 이 텍스트를 개별 토큰 시퀀스로 분해합니다.

This file contains bidirectional Unicode text that may be interpreted or compiled differently than what appears below. To review, open the file in an editor that reveals hidden Unicode characters. Learn more about bidirectional Unicode characters Show hidden characters
```python
import torch
from transformers import AutoTokenizer

# load the llama-3.2 tokenizer
tokenizer = AutoTokenizer.from_pretrained('meta-llama/Llama-3.1-8B')

# raw text
text = "This raw text will be tokenized"

# create tokens using tokenizer
tokens = tokenizer.tokenize(text)
token_ids = tokenizer.convert_tokens_to_ids(tokens)
# token_ids = tokenizer.encode(text) # directly create token ids

# view the results
print("Original Text:", text)
print("Tokens:", tokens)
print("Token IDs:", token_ids)

# create token embedding layer
VOCABULARY_SIZE: int = 128000
EMBEDDING_DIM: int = 768
token_embedding_layer = torch.nn.Embedding(
    num_embeddings=VOCABULARY_SIZE,
    embedding_dim=EMBEDDING_DIM,
)

# get token embeddings (IDs must be passed as a tensor, not a list)
token_emb = token_embedding_layer(torch.tensor(token_ids))
print(f'Token Embeddings Shape: {token_emb.shape}')
```
view raw tokenizer_example.py hosted with ❤ by GitHub

LLM 훈련 및 상호 작용을 위한 패키지(예: HuggingFace 또는 torchtune)는 토크나이저와 상호 작용하기 위한 인터페이스를 제공합니다. 또한 OpenAI는 GPT 토크나이저와 상호 작용하기 위한 tiktoken 패키지를 출시했습니다. 위 코드 스니펫은 텍스트 시퀀스를 다음과 같이 토큰화합니다.

**원시 텍스트**: This raw text will be tokenized
**토큰화된 텍스트**: ['This', 'Ġraw', 'Ġtext', 'Ġwill', 'Ġbe', 'Ġtoken', 'ized']

여기서 Ġ 문자는 토큰이 공백 바로 뒤에 온다는 것을 나타냅니다. 이러한 특수 문자는 토크나이저에 따라 다릅니다. 예를 들어, 많은 토크나이저는 대신 # 문자를 사용하여 단어의 연속을 나타내며, 이는 위 시퀀스의 마지막 두 토큰에 대해 ['token', '#ized']를 생성할 것입니다.

**어휘(Vocabulary).** 각 LLM은 특정 토크나이저로 훈련되지만, 단일 토크나이저가 여러 다른 LLM에 사용될 수도 있습니다. 주어진 토크나이저가 생성할 수 있는 토큰 집합도 고정되어 있습니다. 따라서 LLM은 이해하는 고정된 토큰 집합(즉, 토크나이저가 생성하는 토큰)을 가지며, 이를 기반으로 훈련됩니다. 이 고정된 토큰 집합은 통상적으로 LLM의 "어휘(vocabulary)"라고 불립니다. 아래를 참조하십시오. 어휘 크기는 모델마다 다르며 여러 요인에 따라 달라지지만(예: 다국어 모델은 더 큰 어휘를 가지는 경향이 있음), 최근 LLM의 경우 총 64K에서 256K 토큰의 어휘 크기가 비교적 일반적입니다.

**LLM을 위한 토큰 어휘(및 벡터)**

**토큰 ID 및 임베딩(Embeddings).** LLM 어휘의 각 토큰은 고유한 정수 ID와 연결됩니다. 예를 들어, 이전 코드는 텍스트를 토큰화할 때 다음과 같은 ID 시퀀스를 생성합니다: `[2028, 7257, 1495, 690, 387, 4037, 1534]`. 이 각 ID는 **임베딩 레이어(embedding layer)**에서 **토큰 임베딩(token embedding)**으로 알려진 벡터와 연결됩니다. 임베딩 레이어는 단순히 많은 벡터 임베딩 행을 저장하는 큰 행렬입니다. 토큰에 대한 임베딩을 검색하려면, 임베딩 레이어에서 해당 토큰 ID로 주어진 해당 행을 찾기만 하면 됩니다. 위를 참조하십시오.

**토큰 임베딩(또는 벡터)의 입력 행렬**

이제 토큰 임베딩 목록을 가지고 있습니다. 이 임베딩들을 행렬로 쌓아 트랜스포머 아키텍처에 의해 입력되는 실제 입력을 형성할 수 있습니다. 위를 참조하십시오. PyTorch에서는 이 행렬의 생성이 이전 코드에 표시된 대로 토크나이저와 임베딩 레이어에 의해 자동으로 처리됩니다. 토큰 임베딩 행렬의 크기는 `[C, d]`이며, 여기서 `C`는 입력의 토큰 수이고 `d`는 LLM이 채택한 토큰 임베딩의 차원(dimension)입니다. 일반적으로 단일 입력 시퀀스 대신 `B`개의 입력 시퀀스 배치를 가지며, `[B, C, d]` 크기의 입력 행렬을 형성합니다. 차원 `d`는 트랜스포머 내의 모든 레이어 또는 활성화(activation)의 크기에 영향을 미치므로, `d`는 중요한 하이퍼파라미터(hyperparameter) 선택이 됩니다. 이 행렬을 트랜스포머에 입력으로 전달하기 전에, 우리는 또한 입력 3의 각 토큰에 위치 임베딩(positional embedding)을 추가하여 각 토큰의 시퀀스 내 위치를 트랜스포머에 전달합니다.

**(마스크드 및 멀티 헤드) 셀프 어텐션**

이제 토큰 임베딩 행렬인 입력을 디코더 전용 트랜스포머에 전달하여 처리를 시작할 준비가 되었습니다. 이전에 설명했듯이, 트랜스포머는 셀프 어텐션(self-attention)과 피드포워드 변환(feed-forward transformation)을 포함하는 반복되는 블록으로 구성되며, 각 블록 뒤에는 정규화(normalization) 연산이 따릅니다. 먼저 셀프 어텐션을 살펴보겠습니다.

([1]에서 발췌) 셀프 어텐션이란 무엇인가? 간단히 말해, 셀프 어텐션은 시퀀스 내 각 토큰의 표현을 시퀀스 내 다른 토큰과의 관계를 기반으로 변환합니다. 직관적으로, 셀프 어텐션은 각 토큰의 표현을 해당 토큰과 가장 관련이 있는 시퀀스 내 다른 토큰(자기 자신 포함)을 기반으로 합니다. 다시 말해, 시퀀스 내 토큰의 의미를 이해하려고 할 때 어떤 토큰에 "주의를 기울여야" 하는지 학습합니다. 예를 들어, 위에서 `making`이라는 단어의 표현이 `more`와 `difficult`라는 단어에 의해 크게 영향을 받는 것을 볼 수 있습니다. 이 단어들은 문장의 전체적인 의미를 전달하는 데 도움이 됩니다.

“어텐션 함수(attention function)는 쿼리(query)와 키-값(key-value) 쌍 집합을 출력으로 매핑하며, 여기서 쿼리, 키, 값, 출력은 모두 벡터입니다. 출력은 값의 가중 합으로 계산되며, 각 값에 할당된 가중치는 쿼리와 해당 키의 호환성 함수(compatibility function)에 의해 계산됩니다.” - [1]에서 발췌

**스케일드 닷 프로덕트 어텐션(Scaled Dot Product Attention).** 크기 `[C, d]`의 입력 토큰 행렬이 주어졌을 때(즉, 단순화를 위해 배치 대신 단일 입력 시퀀스를 처리한다고 가정), 우리는 세 개의 별도 선형 투영(linear projection)을 사용하여 입력을 투영함으로써 세 개의 별도 (변환된) 토큰 벡터 집합을 형성합니다. 이러한 투영은 키(key), 쿼리(query) 및 값(value) 투영이라고 불립니다. 아래를 참조하십시오.

**키, 쿼리 및 값 벡터 생성**

이 명명 규칙은 무작위로 보일 수 있지만, 정보 검색(information retrieval) 분야의 이전 연구에서 유래했습니다. 각 투영 이름에 대한 직관적인 추론은 다음과 같습니다.

*   **쿼리(query)**는 정보를 검색하는 데 사용하는 것입니다. 이는 시퀀스에서 다른 관련 토큰을 찾고자 하는 현재 토큰을 나타냅니다.
*   **키(key)**는 시퀀스 내의 다른 각 토큰을 나타내며, 쿼리를 시퀀스 내의 다른 관련 토큰과 일치시키는 인덱스 역할을 합니다.
*   **값(value)**은 쿼리가 키와 일치할 때 검색되는 실제 정보입니다. 값은 셀프 어텐션에서 각 토큰의 출력을 계산하는 데 사용됩니다.

**어텐션 점수(attention score) 계산.** 입력을 투영한 후, 우리는 입력 시퀀스 내 각 토큰 쌍 `[i, j]`에 대해 어텐션 점수 `a[i, j]`를 계산합니다. 직관적으로, `[0, 1]` 범위에 있는 이 어텐션 점수는 주어진 토큰이 시퀀스 내 다른 토큰에 얼마나 "주의를 기울여야" 하는지를 포착합니다. 어텐션 점수가 높을수록 토큰 쌍이 서로 매우 관련성이 높다는 것을 나타냅니다. 위에서 암시했듯이, 어텐션 점수는 키와 쿼리 벡터를 사용하여 생성됩니다. 우리는 토큰 `i`의 쿼리 벡터와 토큰 `j`의 키 벡터의 내적(dot product)을 취하여 `a[i, j]`를 계산합니다. 이 과정의 묘사는 아래를 참조하십시오.

**토큰 쌍에 대한 어텐션 점수 계산**

시퀀스 내 모든 쌍별 어텐션 점수를 효율적으로 계산하는 방법은 다음과 같습니다.

*   쿼리 및 키 벡터를 두 개의 행렬로 쌓습니다.
*   쿼리 행렬에 전치된 키 행렬을 곱합니다.

이 연산은 전체 시퀀스에 대한 모든 쌍별 어텐션 점수를 포함하는 `[C, C]` 크기의 행렬(어텐션 행렬(attention matrix)이라고 함)을 형성합니다. 여기에서, 우리는 어텐션 행렬의 각 값을 `d`의 제곱근으로 나눕니다(이는 훈련 안정성을 향상시키는 것으로 밝혀진 접근 방식 [1]입니다) — 그리고 어텐션 행렬의 각 행에 소프트맥스(softmax) 연산을 적용합니다. 아래를 참조하십시오. 소프트맥스가 적용된 후, 어텐션 행렬의 각 행은 유효한 확률 분포를 형성합니다. 각 행은 합이 1인 양수 값을 포함합니다. 어텐션 행렬의 `i`번째 행은 `i`번째 토큰과 시퀀스 내 다른 각 토큰 사이의 확률을 저장합니다.

**셀프 어텐션을 위한 어텐션 점수 및 출력 계산**

**출력 계산.** 어텐션 점수를 얻으면 셀프 어텐션의 출력을 도출하는 것은 쉽습니다. 각 토큰의 출력은 단순히 값 벡터의 가중 조합이며, 가중치는 어텐션 점수에 의해 주어집니다. 이 출력을 계산하기 위해, 위에서 보여준 대로 어텐션 행렬에 값 행렬을 곱하기만 하면 됩니다. 특히, 셀프 어텐션은 입력의 크기를 보존합니다. 입력 내 각 토큰 벡터에 대해 변환된 `d`차원 출력 벡터가 생성됩니다.

**마스크드 셀프 어텐션(Masked self-attention).** 지금까지 우리가 배운 공식은 바닐라(또는 양방향) 셀프 어텐션에 대한 것입니다. 그러나 이전에 언급했듯이, 디코더 전용 트랜스포머는 마스크드 셀프 어텐션(masked self-attention)을 사용하며, 이는 시퀀스 내 각 토큰 뒤에 오는 토큰을 "마스킹(masking)"하여 기본 어텐션 패턴을 수정합니다. 각 토큰은 자신보다 앞에 오는 토큰만 고려할 수 있습니다. 뒤따르는 토큰은 마스킹됩니다.

**마스크드 어텐션 점수 계산**

토큰 시퀀스 `[“LLM”, “#s”, “are”, “cool”, “.”]`를 고려하고 토큰 `“are”`에 대한 마스크드 어텐션 점수를 계산해 봅시다. 지금까지 우리는 셀프 어텐션이 “are”와 시퀀스 내 다른 모든 토큰 사이에 어텐션 점수를 계산한다는 것을 배웠습니다. 그러나 마스크드 셀프 어텐션을 사용하면 “LLM”, “#s”, “are”에 대해서만 어텐션 점수를 계산합니다. 마스크드 셀프 어텐션은 시퀀스에서 앞을 내다보는 것을 금지합니다! 실제적으로, 이는 이러한 토큰에 대한 모든 어텐션 점수를 단순히 음의 무한대(negative infinity)로 설정함으로써 달성되며, 소프트맥스 적용 후 마스크된 토큰에 대해 쌍별 확률이 0이 됩니다.

([1]에서 발췌) **어텐션 헤드(Attention heads).** 지금까지 설명한 어텐션 연산은 소프트맥스를 사용하여 시퀀스 전체에 걸쳐 계산된 어텐션 점수를 정규화합니다. 이 접근 방식은 유효한 확률 분포를 형성하지만, 시퀀스 내 여러 위치에 초점을 맞추는 셀프 어텐션의 능력을 제한하기도 합니다. 확률 분포는 하나(또는 소수의) 단어에 의해 쉽게 지배될 수 있습니다. 이 문제를 해결하기 위해, 우리는 일반적으로 여러 "헤드(head)"에 걸쳐 어텐션을 병렬로 계산합니다. 위를 참조하십시오. 각 헤드 내에서 마스크드 어텐션 연산은 동일합니다. 그러나 우리는 다음을 수행합니다.

*   각 어텐션 헤드에 대해 별도의 키, 쿼리 및 값 투영을 사용합니다.
*   계산 비용을 줄이기 위해 키, 쿼리 및 값 벡터의 차원을 줄입니다(즉, 선형 투영을 수정하여 수행할 수 있습니다). 더 구체적으로, 우리는 각 어텐션 헤드의 벡터 차원을 `d`에서 `d // H`로 변경할 것입니다. 여기서 `H`는 어텐션 헤드의 수이며, 이는 멀티 헤드 셀프 어텐션(multi-headed self-attention)의 계산 비용을 (상대적으로) 고정된 상태로 유지하기 위함입니다.

**여러 어텐션 헤드의 출력 결합**

이제 우리는 셀프 어텐션을 병렬로 계산하는 여러 어텐션 헤드를 가지고 있습니다. 그러나 우리는 여전히 셀프 어텐션 모듈의 여러 헤드로부터 단일 출력 표현을 생성해야 합니다. 각 어텐션 헤드의 출력을 결합하는 데는 여러 옵션이 있습니다. 예를 들어, 연결(concatenation), 평균화(averaging), 투영(projecting) 등이 있습니다. 그러나 멀티 헤드 셀프 어텐션의 바닐라 구현은 다음을 수행합니다(위에 묘사됨).

*   각 헤드의 출력을 연결합니다.
*   연결된 출력을 선형적으로 투영합니다.

각 어텐션 헤드가 `d // H` 차원의 토큰 벡터를 출력하므로, 모든 어텐션 헤드의 연결된 출력은 `d` 차원을 가집니다. 따라서 멀티 헤드 셀프 어텐션 연산은 여전히 입력의 원래 크기를 보존합니다.

This file contains bidirectional Unicode text that may be interpreted or compiled differently than what appears below. To review, open the file in an editor that reveals hidden Unicode characters. Learn more about bidirectional Unicode characters Show hidden characters
```python
"""
Source: https://github.com/karpathy/nanoGPT/blob/master/model.py
"""
import math
import torch
from torch import nn
import torch.nn.functional as F

class CausalSelfAttention(nn.Module):

    def __init__(self, d, H, T, bias=False, dropout=0.2,):
        """
        Arguments:
            d: size of embedding dimension
            H: number of attention heads
            T: maximum length of input sequences (in tokens)
            bias: whether or not to use bias in linear layers
            dropout: probability of dropout
        """
        super().__init__()
        assert d % H == 0
        # key, query, value projections for all heads, but in a batch
        # output is 3X the dimension because it includes key, query and value
        self.c_attn = nn.Linear(d, 3 * d, bias=bias)
        # projection of concatenated attention head outputs
        self.c_proj = nn.Linear(d, d, bias=bias)
        # dropout modules
        self.attn_dropout = nn.Dropout(dropout)
        self.resid_dropout = nn.Dropout(dropout)
        self.H = H
        self.d = d
        # causal mask to ensure that attention is only applied to
        # the left in the input sequence
        self.register_buffer("mask", torch.tril(torch.ones(T, T)).view(1, 1, T, T))

    def forward(self, x):
        B, T, _ = x.size() # batch size, sequence length, embedding dimensionality

        # compute query, key, and value vectors for all heads in batch
        # split the output into separate query, key, and value tensors
        q, k, v = self.c_attn(x).split(self.d, dim=2) # [B, T, d]

        # reshape tensor into sequences of smaller token vectors for each head
        k = k.view(B, T, self.H, self.d // self.H).transpose(1, 2) # [B, H, T, d // H]
        q = q.view(B, T, self.H, self.d // self.H).transpose(1, 2)
        v = v.view(B, T, self.H, self.d // self.H).transpose(1, 2)

        # compute the attention matrix, perform masking, and apply dropout
        att = (q @ k.transpose(-2, -1)) * (1.0 / math.sqrt(k.size(-1))) # [B, H, T, T]
        att = att.masked_fill(self.mask[:,:,:T,:T] == 0, float('-inf'))
        att = F.softmax(att, dim=-1)
        att = self.attn_dropout(att)

        # compute output vectors for each token
        y = att @ v # [B, H, T, d // H]

        # concatenate outputs from each attention head and linearly project
        y = y.transpose(1, 2).contiguous().view(B, T, self.d)
        y = self.resid_dropout(self.c_proj(y))
        return y
```
view raw causal_self_attention.py hosted with ❤ by GitHub

**전체 구현.** 마스크드 멀티 헤드 셀프 어텐션의 전체 구현은 위에 제공됩니다. 여기서는 `[C, d]` 크기의 단일 입력 시퀀스를 넘어 `[B, C, d]` 크기의 입력 배치를 처리합니다. 위 코드는 지금까지 설명한 각 구성 요소를 구현합니다.

*   **52-59행**: 각 어텐션 헤드에 대한 키, 쿼리 및 값 투영을 계산하고(단일 선형 투영 사용) 필요에 따라 분할/재구성합니다.
*   **62-65행**: 어텐션 점수를 계산하고, 어텐션 점수를 마스킹한 다음, 결과 4에 소프트맥스 변환을 적용합니다.
*   **68행**: 어텐션 행렬과 값 행렬의 곱을 취하여 출력 벡터를 계산합니다.
*   **71-72행**: 각 어텐션 헤드의 출력을 연결하고 선형 투영하여 최종 출력을 형성합니다.

PyTorch에서 일부 정교한 행렬 조작 및 연산을 사용하지만, 이 구현은 마스크드 셀프 어텐션에 대한 우리의 설명과 정확히 일치합니다!

**피드포워드 변환**

**포인트와이즈 피드포워드 변환**

마스크드 셀프 어텐션 외에도, 트랜스포머의 각 블록은 포인트와이즈 5 피드포워드 변환을 포함합니다. 위를 참조하십시오. 이 변환은 시퀀스 내 각 토큰 벡터를 동일한 피드포워드 신경망(feed-forward neural network)을 통해 통과시킵니다. 일반적으로 이는 은닉층(hidden layer)에 비선형 활성화 함수(non-linear activation function)(예: ReLU, GeLU 또는 SwiGLU [3])를 가진 2계층 네트워크입니다. 대부분의 경우, 은닉층의 차원은 우리 토큰 임베딩의 원래 차원보다 큽니다(예: 4배). PyTorch에서 피드포워드 신경망을 구현하는 것은 `Linear` 모듈을 사용하면 쉽게 달성할 수 있습니다. 예시는 아래를 참조하십시오.

This file contains bidirectional Unicode text that may be interpreted or compiled differently than what appears below. To review, open the file in an editor that reveals hidden Unicode characters. Learn more about bidirectional Unicode characters Show hidden characters
```python
"""
Source: https://github.com/karpathy/nanoGPT/blob/master/model.py
"""
from torch import nn

class MLP(nn.Module):
    def __init__(self, d, bias=False, dropout=0.2):
        """
        Arguments:
            d: size of embedding dimension
            bias: whether or not to use bias in linear layers
            dropout: probability of dropout
        """
        super().__init__()
        self.c_fc    = nn.Linear(d, 4 * d, bias=bias)
        self.gelu    = nn.GELU()
        self.c_proj  = nn.Linear(4 * d, d, bias=bias)
        self.dropout = nn.Dropout(dropout)

    def forward(self, x):
        x = self.c_fc(x)
        x = self.gelu(x)
        x = self.c_proj(x)
        x = self.dropout(x)
        return x
```
view raw transformer_ffnn.py hosted with ❤ by GitHub

**디코더 전용 트랜스포머 블록**

**디코더 전용 트랜스포머 블록**

디코더 전용 트랜스포머 블록을 구성하기 위해, 우리는 지금까지 보았던 두 구성 요소(마스크드 셀프 어텐션과 피드포워드 변환)를 사용하며, 구성 요소 사이에 정규화 연산과 잔차 연결(residual connection)을 배치합니다. 완전한 디코더 전용 트랜스포머 블록 6의 묘사는 위에 표시되어 있습니다. 잔차 연결 [4]은 신경망 레이어의 입력을 해당 레이어의 출력에 단순히 추가한 다음 이 표현을 다음 레이어로 전달합니다. 이는 입력 추가 없이 레이어의 출력만을 다음 레이어로 전달하는 것과는 대조적입니다.

**일반 신경망 레이어의 잔차 연결**

잔차 연결은 딥러닝(deep learning) 내에서 널리 사용되며 모든 종류의 신경망 레이어 7에 적용될 수 있습니다. 잔차 연결을 추가하는 것은 기울기 소실/폭발(vanishing / exploding gradients) 문제를 피하고, 역전파(backpropagation) 동안 기울기가 네트워크를 통해 자유롭게 흐르도록 하는 "지름길"을 제공하여 훈련의 안정성을 일반적으로 향상시킵니다. 자세한 내용은 여기를 참조하십시오.

**어파인 변환을 사용한 레이어 정규화**

신경망 레이어의 입력(또는 출력)을 정규화하는 것도 훈련 안정성에 도움이 될 수 있습니다. 많은 종류의 정규화가 존재하지만, 트랜스포머/LLM에 가장 일반적으로 사용되는 정규화 변형은 레이어 정규화(layer normalization)입니다. 위를 참조하십시오. 여기에서 정규화 연산은 두 가지 구성 요소를 가집니다.

*   정규화 수행.
*   (학습 가능한) 어파인 변환(affine transformation) 적용.

다시 말해, 우리는 정규화된 출력을 직접 사용하는 대신 정규화된 값에 가중치를 곱하고 편향(bias)을 추가합니다. 가중치와 편향은 모두 다른 네트워크 매개변수와 함께 훈련될 수 있는 학습 가능한 매개변수입니다. 레이어 정규화는 PyTorch에 구현되어 있으며 사용하기 쉽습니다. 여기를 참조하십시오.

This file contains bidirectional Unicode text that may be interpreted or compiled differently than what appears below. To review, open the file in an editor that reveals hidden Unicode characters. Learn more about bidirectional Unicode characters Show hidden characters
```python
"""
Source: https://github.com/karpathy/nanoGPT/blob/master/model.py
"""
from torch import nn

class Block(nn.Module):
    def __init__(self, d, H, T, bias=False, dropout=0.2,):
        """
        Arguments:
            d: size of embedding dimension
            H: number of attention heads
            T: maximum length of input sequences (in tokens)
            bias: whether or not to use bias in linear layers
            dropout: probability of dropout
        """
        super().__init__()
        self.ln_1 = nn.LayerNorm(d)
        self.attn = CausalSelfAttention(d, H, T, bias, dropout)
        self.ln_2 = nn.LayerNorm(d)
        self.ffnn = MLP(d, bias, dropout)

    def forward(self, x):
        x = x + self.attn(self.ln_1(x))
        x = x + self.ffnn(self.ln_2(x))
        return x
```
view raw decoder_only_block.py hosted with ❤ by GitHub

**블록 구현.** 디코더 전용 트랜스포머 블록 구현은 위에 제공됩니다. 여기서는 이전의 어텐션 및 피드포워드 변환 구현을 사용합니다. 이미 정의한 모듈을 사용함으로써, 디코더 전용 트랜스포머 블록 구현은 실제로 매우 간단해집니다!

**디코더 전용 트랜스포머 아키텍처**

**디코더 전용 트랜스포머 아키텍처**

디코더 전용 트랜스포머의 입력 및 블록 구조를 파악하면, 나머지 아키텍처는 매우 간단합니다. 동일한 블록을 `L`번 반복하기만 하면 됩니다! 각 블록에 대해 모델 입력의 크기 `[B, C, d]`가 유지되므로, `L`번째 디코더 전용 트랜스포머 블록의 출력도 이 크기의 텐서(tensor)입니다. 아래를 참조하십시오.

**LLM으로 다음 토큰 예측**

(GPT 스타일의) 디코더 전용 트랜스포머 아키텍처의 전체 구현은 아래에 제공됩니다. 여기에서 아키텍처는 두 개의 임베딩 레이어(즉, 토큰 및 위치용), 모든 `L`개의 트랜스포머 블록, 그리고 출력 토큰 임베딩을 입력으로 받아 다음 토큰 예측을 수행하기 위한 최종 분류 모듈(레이어 정규화 및 선형 레이어 포함)을 포함합니다. 모델은 크기 `[B, C]`의 입력 토큰 ID 집합인 입력을 이러한 각 구성 요소를 통해 전달하여 출력 토큰 ID 집합을 생성함으로써 작동합니다.

This file contains bidirectional Unicode text that may be interpreted or compiled differently than what appears below. To review, open the file in an editor that reveals hidden Unicode characters. Learn more about bidirectional Unicode characters Show hidden characters
```python
"""
Source: https://github.com/karpathy/nanoGPT/blob/master/model.py
"""
import torch
from torch import nn
import torch.nn.functional as F

class GPT(nn.Module):

    def __init__(self, d, H, C, V, layers, bias=False, dropout=0.2,):
        """
        Arguments:
            d: size of embedding dimension
            H: number of attention heads
            C: maximum length of input sequences (in tokens)
            V: size of the token vocabulary
            layers: number of decoder-only blocks
            bias: whether or not to use bias in linear layers
            dropout: probability of dropout
        """
        super().__init__()
        self.transformer = nn.ModuleDict(dict(
            wte = nn.Embedding(V, d), # token embeddings
            wpe = nn.Embedding(C, d), # position embeddings
            drop = nn.Dropout(dropout),
            blocks = nn.ModuleList([Block(d, H, C, bias, dropout) for _ in range(layers)]),
            ln_f = nn.LayerNorm(d),
            head = nn.Linear(d, V, bias=bias),
        ))

    def forward(self, idx, targets=None):
        # idx is a [B, C] matrix of token indices
        # targets is a [B, C] matrix of target (next) token indices
        device = idx.device
        _, C = idx.size() # [B, C]
        pos = torch.arange(0, C, dtype=torch.long, device=device) # generate token and position embeddings

        tok_emb = self.transformer.wte(idx) # [B, C, d]
        pos_emb = self.transformer.wpe(pos) # [C, d]
        x = self.transformer.drop(tok_emb + pos_emb)

        # pass through all decoder-only blocks
        for block in self.transformer.blocks:
            x = block(x)
        x = self.transformer.ln_f(x) # final layer norm

        if targets is not None:
            # compute the loss if we are given targets
            logits = self.transformer.head(x)
            loss = F.cross_entropy(logits.view(-1, logits.size(-1)), targets.view(-1), ignore_index=-1,)
        else:
            # only look at last token if performing inference
            logits = self.transformer.head(x[:, [-1], :])
            loss = None
        return logits, loss
```
view raw gpt.py hosted with ❤ by GitHub

**출력 생성(디코딩).** LLM은 **다음 토큰 예측**을 수행하도록 특별히 훈련됩니다. 다시 말해, 이 모델들은 토큰 목록이 입력으로 주어졌을 때 다음 토큰을 예측하는 전문가입니다. 우리가 배웠듯이, 모델의 출력은 각 입력 토큰에 해당하는 출력 토큰 벡터의 목록입니다. 따라서 우리는 다음을 통해 이러한 입력 토큰 중 어느 것에 대해서든 다음 토큰을 예측할 수 있습니다.

*   특정 토큰에 대한 출력 임베딩을 가져옵니다.
*   이 임베딩을 선형 레이어를 통해 전달합니다. 여기서 출력 크기는 모델 어휘의 차원입니다.
*   모델 출력의 argmax를 취하여 최대 토큰 ID를 얻습니다.

텍스트 시퀀스를 생성하기 위해, 우리는 이 과정을 계속 반복합니다. 텍스트 프롬프트를 입력으로 받아들이고, 모든 것을 디코더 전용 트랜스포머를 통해 전달하고, 출력 시퀀스에서 마지막 토큰 벡터를 가져와 다음 토큰을 예측하고, 이 다음 토큰을 입력 시퀀스에 추가하고 반복합니다. 이 자기회귀적 디코딩(autoregressive decoding) 과정은 모든 LLM이 출력을 생성하는 데 사용합니다. 아래를 참조하십시오.

**다음 토큰 예측을 통한 자기회귀적 출력 생성**

**왜 디코더인가?** 이 아키텍처를 이해했으니, 우리는 궁금해할 수 있습니다. 왜 LLM은 트랜스포머의 디코더 구성 요소만 사용할까요? 트랜스포머의 인코더와 디코더 사이의 핵심 차이점은 사용되는 어텐션의 유형입니다. 인코더는 양방향 셀프 어텐션(bidirectional self-attention)을 사용합니다. 이는 주어진 토큰 이전과 이후의 토큰을 포함하여 시퀀스 내의 모든 토큰이 셀프 어텐션 메커니즘에 의해 고려된다는 것을 의미합니다. 대조적으로, 디코더는 마스크드 셀프 어텐션(masked self-attention)을 사용하며, 이는 토큰이 시퀀스에서 자신을 따르는 토큰에 주의를 기울이는 것을 방지합니다.

**다음 토큰 예측을 위한 인과적 마스크**

마스크드 셀프 어텐션의 사용 덕분에, 디코더는 다음 토큰 예측에 잘 작동합니다. 만약 각 토큰이 자신의 표현을 만들 때 시퀀스에서 앞을 내다볼 수 있다면, 모델은 속임수(즉, 시퀀스에서 다음 토큰을 직접 복사)를 통해 다음 토큰을 예측하는 것을 단순히 학습할 수 있습니다. 위를 참조하십시오. 마스크드 셀프 어텐션은 모델이 자신보다 앞에 오는 토큰으로부터 다음 토큰을 예측하기 위한 일반화 가능한 패턴을 학습하도록 강제하며, 이는 디코더를 LLM에 완벽하게 만듭니다.

**MoE(Mixture-of-Experts) 모델 생성**

“딥러닝에서 모델은 일반적으로 모든 입력에 대해 동일한 매개변수를 재사용합니다. MoE(Mixture of Experts) 모델은 이를 거부하고 대신 들어오는 각 예제에 대해 다른 매개변수를 선택합니다. 그 결과는 엄청난 수의 매개변수를 가지지만 일정한 계산 비용을 갖는 희소하게 활성화된 모델입니다.” - [6]에서 발췌

이제 디코더 전용 트랜스포머에 대한 심층적인 이해를 얻었으므로, MoE(Mixture-of-Experts) 모델을 생성해야 합니다. MoE 기반 LLM은 동일한 디코더 전용 트랜스포머 아키텍처를 유지하지만, 몇 가지 미묘한 방식으로 이 아키텍처를 수정합니다. 이러한 아이디어에 대한 심층적인 내용은 아래 게시물을 참조하십시오.

**MoE(Mixture-of-Experts): 조건부 계산의 탄생과 부상**
Cameron R. Wolfe, Ph.D. · 2024년 3월 18일
전체 기사 읽기

**MoE(Mixture-of-Experts) LLM**
Cameron R. Wolfe, Ph.D. · 1월 27일
전체 기사 읽기

모델 아키텍처를 MoE로 변환하는 것은 그리 어렵지 않지만, 모델이 잘 작동하려면 올바르게 구현되어야 하는 많은 작은 세부 사항들이 있습니다. 또한, 이러한 모델을 적절하게 훈련하려면 추가적인 주의와 이해가 필요합니다. MoE 모델은 표준 LLM보다 훈련하기가 더 어렵습니다.

**전문가 레이어(Expert Layers)**

표준 디코더 전용 트랜스포머와 비교할 때, MoE 모델이 수행하는 주요 수정은 트랜스포머 블록의 피드포워드 구성 요소 내에 있습니다. 일반적으로 이 블록은 모든 토큰 벡터에 포인트와이즈 방식으로 적용되는 하나의 피드포워드 네트워크를 가집니다. 단일 피드포워드 네트워크를 가지는 대신, MoE는 여러 개의 피드포워드 네트워크를 생성하며, 각 네트워크는 자체적인 독립적인 가중치를 가집니다. 우리는 이러한 각 네트워크를 "전문가(expert)"라고 부르며, 여러 전문가를 가진 피드포워드 레이어를 "전문가 레이어(expert layer)"라고 부릅니다. 한 레이어에 `N`개의 전문가가 있다면, 우리는 `i`번째 전문가를 `E_i` 표기법을 사용하여 참조할 수 있습니다. 아래를 참조하십시오.

**PyTorch 구현.** PyTorch에서 전문가 레이어를 구현하는 것은 그리 복잡하지 않습니다. 아래에 표시된 대로, 우리는 이전과 동일한 구현을 사용하지만, 하나의 피드포워드 네트워크 대신 여러 개의 피드포워드 네트워크를 생성합니다. 이 구현의 주요 복잡성은 PyTorch에서 표준 `Linear` 레이어를 사용하지 않는다는 것입니다. 대신, 우리는 모든 전문가의 가중치를 여러 `Parameter` 객체로 래핑(wrap)하여 배치 행렬 곱셈(batch matrix multiplication) 연산자를 사용하여 모든 전문가의 출력을 배치로 계산할 수 있습니다. 이 구현은 각 전문가의 출력을 계산하기 위해 루프를 돌 필요가 없게 하여 효율성을 극적으로 향상시킵니다.

This file contains bidirectional Unicode text that may be interpreted or compiled differently than what appears below. To review, open the file in an editor that reveals hidden Unicode characters. Learn more about bidirectional Unicode characters Show hidden characters
```python
"""
Based upon ColossalAI OpenMoE: https://github.com/hpcaitech/ColossalAI/blob/main/colossalai/moe/experts.py
"""
import torch
from torch import nn

class MLPExperts(nn.Module):
    def __init__(self, d, n_exp=8, bias=False, dropout=0.2,):
        """
        Arguments:
            d: size of embedding dimension
            n_exp: the number of experts to create in the expert layer
            bias: whether or not to use bias in linear layers
            dropout: probability of dropout
        """
        super().__init__()
        self.bias = bias
        self.c_fc = nn.Parameter(torch.empty(n_exp, d, 4 * d))
        self.c_proj = nn.Parameter(torch.empty(n_exp, 4 * d, d))
        self.fc_bias = nn.Parameter(torch.empty(n_exp, 1, 4 * d)) if self.bias else None
        self.proj_bias = nn.Parameter(torch.empty(n_exp, 1, d)) if self.bias else None
        self.gelu = nn.GELU()
        self.dropout = nn.Dropout(dropout)

    def forward(self, x):
        x = torch.bmm(x, self.c_fc)
        if self.bias:
            x += self.fc_bias
        x = self.gelu(x)
        x = torch.bmm(x, self.c_proj)
        if self.bias:
            x += self.proj_bias
        x = self.dropout(x)
        return x
```
view raw expert_layer.py hosted with ❤ by GitHub

**MoE 생성.** MoE 기반 디코더 전용 트랜스포머를 생성하려면, 트랜스포머의 피드포워드 레이어를 MoE(또는 전문가) 레이어로 간단히 변환합니다. MoE 레이어 내의 각 전문가는 해당 레이어의 원래 피드포워드 네트워크와 동일한 아키텍처를 가집니다. 우리는 전문가 레이어 내에 원래 피드포워드 네트워크의 여러 독립적인 복사본을 가집니다. 아래를 참조하십시오.

**디코더 전용 트랜스포머에 전문가 추가 ([1]에서 발췌)**

그러나 트랜스포머의 모든 피드포워드 레이어에 전문가를 사용할 필요는 없습니다. 대부분의 MoE 기반 LLM은 `P`의 스트라이드(stride)를 사용합니다. 이는 `P`번째 레이어마다 전문가 레이어로 변환되고 다른 레이어는 그대로 유지된다는 것을 의미합니다.

“ST-MoE 모델은 32개의 전문가를 가지며, 전문가 레이어 빈도는 1/4입니다(매 네 번째 FFN 레이어가 MoE 레이어로 대체됩니다).” - [24]에서 발췌

이 아이디어의 상위 수준 구현은 아래에 표시된 의사 코드(pseudocode)에 제공됩니다. 이러한 "인터리브된(interleaved)" MoE 레이어는 MoE 내의 총 전문가 수를 제어하며, 이는 성능과 효율성의 균형을 맞추는 데 유용한 메커니즘입니다.

```python
transformer_blocks = []
for i in range(num_blocks):
    use_moe = (i % P) == 0 # when use_moe = False, this is regular transformer block
                           # when use_moe = True, this is an expert layer
    transformer_blocks.append(Block(use_moe=use_moe))
```

**토큰을 전문가에게 라우팅**

MoE 기반 아키텍처의 주요 이점은 효율성이지만, 전문가만 사용하는 것만으로는 효율성이 향상되지 않습니다! 사실, 모델의 각 레이어에 더 많은 전문가를 추가하는 것은 모델의 총 매개변수 수와 필요한 계산량을 크게 증가시킵니다. 효율성을 향상시키기 위해, 우리는 각 레이어 내에서 전문가의 부분 집합만을 희소하게 선택하고 사용해야 합니다. 전문가를 희소하게 활용함으로써, 훈련 및 추론의 계산 비용을 크게 증가시키지 않고 훨씬 더 큰 모델의 이점을 얻을 수 있습니다.

“MoE 아키텍처를 사용하면 밀집 모델(dense model)이 일반적으로 달성하는 것보다 모델 품질과 추론 효율성 사이에서 더 나은 절충점을 얻을 수 있습니다.” - 출처

**전문가 선택.** `d`차원 토큰 벡터로 표현되는 단일 토큰을 고려해 봅시다. 우리의 목표는 이 토큰을 처리할 전문가의 부분 집합(크기 `k`)을 선택하는 것입니다. MoE 문헌에서는 일반적으로 토큰이 이러한 전문가에게 "라우팅(routed)"될 것이라고 말합니다. 이 라우팅 연산을 계산하고 최적화할 알고리즘이 필요합니다.

**단일 토큰을 위한 라우팅 메커니즘**

가장 간단한 라우팅 알고리즘은 토큰 벡터에 선형 변환을 적용하여 `N` 크기(즉, 전문가 수)의 벡터를 형성하는 것입니다. 그런 다음 소프트맥스 함수를 적용하여 우리 토큰에 대한 전문가 집합에 대한 확률 분포를 형성할 수 있습니다. 위를 참조하십시오. 우리는 이 분포를 사용하여 분포에서 상위 `K`개의 전문가를 선택함으로써 우리 토큰이 라우팅되어야 할 전문가를 선택할 수 있습니다. 상위 `K` 값(즉, "전문가 확률")도 중요합니다.

**간단한 라우터 구현.** 위에서 설명했듯이, 이 라우팅 메커니즘은 실제로 매우 간단합니다. 단지 선형 레이어일 뿐입니다! 이 소프트맥스 라우터의 구현은 아래에 나와 있으며, 우리 라우터의 출력은 다음과 같습니다.

*   입력의 각 토큰에 대한 상위 `K`개 전문가 인덱스 집합.
*   선택된 전문가와 관련된 상위 `K`개 전문가 확률(즉, 상위 `K`개 인덱스 각각에 대한 확률 값).

단순함에도 불구하고, 이 라우팅 메커니즘은 효과적이며 그 목적을 잘 수행합니다. 대부분의 현대 MoE는 소프트맥스를 사용하는 유사한 선형 라우팅 방식을 채택합니다.

This file contains bidirectional Unicode text that may be interpreted or compiled differently than what appears below. To review, open the file in an editor that reveals hidden Unicode characters. Learn more about bidirectional Unicode characters Show hidden characters
```python
import torch
from torch import nn
from torch.nn import functional as F

class BasicSoftmaxRouter(nn.Module):
    def __init__(self, d, n_exp=8, top_k=2, use_noisy_top_k=True,):
        """
        Arguments:
            d: size of embedding dimension
            n_exp: the number of experts to create in the expert layer
            top_k: the number of active experts for each token
            use_noisy_top_k: whether to add noise when computing expert output
        """
        super().__init__()
        # router settings
        self.top_k = top_k
        assert self.top_k >= 1 and self.top_k <= n_exp
        self.use_noisy_top_k = use_noisy_top_k

        # linear projection for (noisy) softmax routing
        # no bias used, see page 4 eq (4) in https://arxiv.org/abs/1701.06538
        self.w_g = nn.Linear(d, n_exp, bias=False)
        self.w_noise = nn.Linear(d, n_exp, bias=False) if self.use_noisy_top_k else None

    def forward(self, x):
        # eq (4) in https://arxiv.org/abs/1701.06538
        logits = self.w_g(x) # [B, C, d] -> [B, C, n_exp]
        if self.use_noisy_top_k:
            # (optionally) add noise into the router
            noise = F.softplus(self.w_noise(x))
            noise *= torch.randn_like(noise)
            logits += noise

        top_k_logits, top_k_indices = logits.topk(self.top_k, dim=-1) # [B, C, k]
        return top_k_logits, top_k_indices
```
view raw basic_softmax_router.py hosted with ❤ by GitHub

선택적으로, 우리는 라우팅 메커니즘에 노이즈를 추가할 수 있습니다. 이는 [8]에서 제안된 접근 방식으로, 신경망에 MoE를 적용한 초기 연구 중 하나입니다. 라우팅 메커니즘의 출력에 이 소량의 (학습 가능한) 노이즈를 추가함으로써(자세한 내용은 아래 참조), MoE의 훈련 과정을 정규화(regularize)하는 데 도움을 줄 수 있습니다.

**상위 k 소프트맥스 라우팅에 노이즈 추가 ([7]에서 발췌)**

**활성 매개변수(Active parameters).** MoE 레이어 내에서 각 토큰을 처리하기 위해 전문가의 부분 집합만 선택하기 때문에, MoE 문헌에는 "활성(active)" 매개변수라는 개념이 있습니다. 간단히 말해, 주어진 토큰을 처리할 때 MoE 모델의 전체 매개변수 중 작은 부분(각 MoE 레이어에서 선택된 전문가에 의해 주어짐)만이 활성화됩니다. MoE에 의해 수행되는 총 계산량은 총 매개변수 수보다는 활성 매개변수 수에 비례합니다.

**전문가 용량(Expert Capacity)**

“하드웨어 활용도를 높이기 위해, 희소 모델의 대부분 구현은 각 전문가에 대해 정적 배치 크기를 가집니다. 전문가 용량(expert capacity)은 각 전문가에게 라우팅될 수 있는 토큰의 수를 의미합니다. 이 용량을 초과하면 오버플로우된 토큰은 잔차 연결을 통해 다음 레이어로 전달됩니다.” - [5]에서 발췌

전문가 레이어에서 수행되는 계산은 동적입니다. 우리는 라우터의 출력에 기반하여 각 전문가가 계산할 토큰을 선택하며, 이는 MoE에 입력으로 제공되는 토큰 시퀀스에 따라 달라집니다. 각 전문가 입력의 동적 특성은 전문가 레이어의 구현을 다소 복잡하게 만들 수 있습니다. 각 전문가의 입력이 다르고 예측 불가능한 크기를 가질 것이라는 사실을 어떻게 처리할 수 있을까요?

**전문가 용량 계산**

**전문가 용량.** MoE의 대부분의 실제 구현은 각 전문가에 대해 고정된 배치 크기를 사용하여 이 문제를 피합니다. 이는 하드웨어 활용도를 향상시키는 데 유용한 트릭입니다. 각 전문가는 "전문가 용량"이라고 불리는 동일한 정적 배치 크기를 사용합니다. 위에 정의된 전문가 용량은 각 배치에서 단일 전문가에게 보낼 수 있는 최대 토큰 수를 지시합니다. 전문가 용량은 용량 계수(capacity factor) 설정을 통해 제어됩니다. 용량 계수가 1이라는 것은 토큰이 균일하게 라우팅된다는 것을 의미하며, 용량 계수를 1보다 크게 설정하면 전문가 간의 불균형한 토큰 라우팅을 처리하기 위한 추가 버퍼를 제공합니다. 이는 더 높은 메모리 사용량과 낮은 효율성이라는 대가를 치릅니다.

([6]에서 발췌) 전문가에게 라우팅된 토큰 수가 전문가 용량을 초과하면, 우리는 계산을 수행하지 않고 해당 표현이 트랜스포머의 잔차 연결을 통해 다음 레이어로 직접 흐르도록 함으로써 이러한 추가 토큰을 "드롭(drop)"합니다. 위를 참조하십시오. MoE는 비교적 낮은 용량 계수 8에서도 잘 작동하지만, 너무 많은 토큰이 드롭되는 것을 피해야 합니다. 용량 계수는 훈련 및 평가 중에도 다를 수 있습니다. 예를 들어, ST-MoE [5]는 훈련 중에는 1.25, 평가 중에는 2.0의 용량 계수를 사용합니다.

**PyTorch 구현.** 이제 전문가 용량과 전문가 레이어 내 라우팅의 세부 사항을 이해했으므로, 완전한 기능을 하는 라우터를 구현해야 합니다. 이 라우터는 이전 구현(즉, 소프트맥스가 있는 선형 레이어)과 동일한 논리를 공유하지만, 각 전문가에 대한 고정 크기 입력 텐서를 생성함으로써 이 구현을 넘어설 것입니다. 아래를 참조하십시오. 이것이 완전한 기능을 하는 구현이라는 점을 고려할 때, 아래 라우터는 이전보다 더 복잡합니다. 그러나 우리는 이 구현을 다음 구성 요소로 요약할 수 있습니다.

*   **41-47행**: (노이즈가 있는) 선형 라우터의 출력을 계산합니다.
*   **49-52행**: 상위 `K`개 전문가와 그와 관련된 확률을 계산합니다.
*   **55-58행**: 전문가 용량을 계산합니다.
*   **60-88행**: 정교한 PyTorch 인덱싱 및 텐서 조작을 사용하여 전문가 입력 9의 배치를 구성하는 것을 처리합니다.
*   **90-93행**: 전문가 입력의 최종 배치를 구성합니다.

This file contains bidirectional Unicode text that may be interpreted or compiled differently than what appears below. To review, open the file in an editor that reveals hidden Unicode characters. Learn more about bidirectional Unicode characters Show hidden characters
```python
import math
import torch
from torch import nn
from torch.nn import functional as F

class Router(nn.Module):
    def __init__(self, d, n_exp=8, top_k=2, use_noisy_top_k=True, capacity_factor=1.25,):
        """
        Arguments:
            d: size of embedding dimension
            n_exp: the number of experts to create in the expert layer
            top_k: the number of active experts for each token
            use_noisy_top_k: whether to add noise when computing expert output
            capacity_factor: used to compute expert capacity
        """
        super().__init__()
        self.d = d
        self.n_exp = n_exp
        self.top_k = top_k
        assert self.top_k >= 1 and self.top_k <= n_exp
        self.use_noisy_top_k = use_noisy_top_k
        self.capacity_factor = capacity_factor

        self.w_g = nn.Linear(d, n_exp, bias=False)
        self.w_noise = nn.Linear(d, n_exp, bias=False) if self.use_noisy_top_k else None

    def forward(self, x):
        # get the total number of tokens in the batch
        B, C, _ = x.size()
        num_tokens = B * C

        # eq (4) in https://arxiv.org/abs/1701.06538
        logits = self.w_g(x) # [B, C, d] -> [B, C, n_exp]
        if self.use_noisy_top_k:
            # (optionally) add noise into the router
            noise = F.softplus(self.w_noise(x))
            noise *= torch.randn_like(noise)
            logits += noise

        # top-K expert selection, compute probabilities over active experts
        top_k_logits, top_k_indices = logits.topk(self.top_k, dim=-1) # [B, C, K]
        router_probs = torch.full_like(logits, float('-inf')) # [B, C, n_exp]
        router_probs.scatter_(-1, top_k_indices, top_k_logits)
        router_probs = F.softmax(router_probs, dim=-1)

        # compute the expert capacity
        exp_capacity = math.floor(self.top_k * self.capacity_factor * num_tokens / self.n_exp)
        exp_capacity += exp_capacity % 2 # make sure expert capacity is an even integer
        exp_capacity = int(exp_capacity)

        # make a multi-hot mask of chosen experts
        # values are 0 if expert not chosen, 1 if expert chosen
        exp_mask = F.one_hot(top_k_indices, num_classes=self.n_exp) # [B, C, K, n_exp]
        exp_mask = exp_mask.view(num_tokens, self.top_k, self.n_exp) # [B * C, K, n_exp]
        exp_mask = exp_mask.permute(1, 0, 2) # [K, B * C, n_exp]

        # compute index for each token in expert batch
        # NOTE: cumsum counts top-1 first, top-2 second, etc.
        # to prioritize top experts when dropping tokens
        exp_rank = exp_mask.reshape(self.top_k * num_tokens, self.n_exp) # [K * B * C, n_exp]
        exp_rank = torch.cumsum(exp_rank, dim=0) - 1 # cumsum of expert selections [K * B * C, n_exp]
        exp_rank = exp_rank.reshape(self.top_k, num_tokens, self.n_exp) # [K, B * C, n_exp]

        # mask entries beyond expert capacity and compute used capacity
        exp_mask *= torch.lt(exp_rank, exp_capacity) # [K, B * C, n_exp]

        # matrix storing token position in batch of corresponding expert
        exp_rank = torch.sum(exp_mask * exp_rank, dim=-1) # [K, B * C]

        # mask probabilities to only include selected experts
        router_probs = router_probs.view(num_tokens, self.n_exp)[None, :] # [1, B * C, n_exp]
        exp_weights = exp_mask * router_probs # [K, B * C, n_exp]

        # position of each token within the capacity of the selected expert
        exp_rank_sc = F.one_hot(exp_rank, num_classes=exp_capacity) # [K, B * C, exp_capacity]

        # weight of selected expert for each token at position the capacity of that expert
        exp_weights = torch.sum(exp_weights.unsqueeze(3) * exp_rank_sc.unsqueeze(2), dim=0) # [B * C, n_exp, exp_capacity]
        exp_mask = exp_weights.bool() # binary mask of selected experts for each token

        # reshape tokens into batches for each expert, return both weights and batches
        # [n_exp, exp_capacity, B * C] * [B * C, d] -> [n_exp, exp_capacity, n_embd]
        x = x.view(num_tokens, self.d)
        expert_batches = exp_mask.permute(1, 2, 0).type_as(x) @ x
        return exp_weights, exp_mask, expert_batches
```
view raw full_softmax_router.py hosted with ❤ by GitHub

**로드 밸런싱(Load Balancing) 및 보조 손실(Auxiliary Losses)**

“게이팅 네트워크(gating network)는 항상 동일한 소수의 전문가에 대해 큰 가중치를 생성하는 상태로 수렴하는 경향이 있습니다. 이러한 불균형은 선호되는 전문가가 더 빠르게 훈련되고 따라서 게이팅 네트워크에 의해 더욱 많이 선택되기 때문에 자기 강화됩니다.” - [7]에서 발췌

지금까지 우리가 고안한 라우팅 시스템은 각 레이어에서 전문가의 균형 잡힌 선택을 명시적으로 장려하지 않습니다. 결과적으로, 모델은 전문가를 완전히 활용하는 대신 모든 토큰에 대해 동일한 소수의 전문가를 반복적으로 선택하는 상태로 수렴할 것입니다. 위 인용문에서 설명된 이 현상은 일반적으로 "라우팅 붕괴(routing collapse)"라고 불립니다.

([6]에서 발췌) **로드 밸런싱 손실.** 훈련 중에 전문가의 균형 잡힌 선택을 장려하기 위해, 우리는 모델이 전문가를 균일하게 활용하는 것에 보상하는 추가 구성 요소를 훈련 손실에 단순히 추가할 수 있습니다. 더 구체적으로, 우리는 위에 표시된 보조 손실 항을 생성합니다. 이는 전문가 중요도(즉, 각 전문가에 할당된 확률)와 로드 밸런싱(즉, 각 전문가에게 전송된 토큰 수)을 측정합니다. 이러한 접근 방식은 [2]에서 제안되었으며, 저자들은 두 가지 양을 고려하는 손실을 생성합니다.

*   각 전문가 10에 할당된 라우터 확률의 비율.
*   각 전문가에게 발송된 토큰의 비율.

이 두 양을 각각의 `N`차원 벡터에 저장하면, 이 두 벡터의 내적을 취하여 단일 손실 항을 생성할 수 있습니다. 이 손실은 전문가가 균일한 확률과 로드 밸런싱을 받을 때 최소화됩니다. PyTorch에서 이 로드 밸런싱 손실의 구현은 아래에 제공됩니다. 이 구현은 다음 핵심 구성 요소를 가집니다.

*   **9-17행**: 로드 밸런싱 손실 계산에 사용되는 모든 상수와 입력 텐서를 정의합니다.
*   **19-24행**: 각 전문가에게 전송된 토큰의 비율 또는 분수를 계산합니다.
*   **26-27행**: 각 전문가에게 할당된 확률의 분수를 계산합니다.
*   **29-31행**: 각 전문가 11에 대한 토큰 비율과 확률 사이의 (스케일링된) 내적을 취합니다.

This file contains bidirectional Unicode text that may be interpreted or compiled differently than what appears below. To review, open the file in an editor that reveals hidden Unicode characters. Learn more about bidirectional Unicode characters Show hidden characters
```python
"""
Computes Switch Transformer auxiliary loss (https://arxiv.org/abs/2101.03961)
See equations (4)-(6) on page 7
"""
import torch
import torch.nn.functional as F

# constants
B = 16 # batch size
C = 256 # sequence length
n_exp = 8 # number of experts
K = 2 # number of active expert

# define tensors needed to compute load balancing loss
indices = torch.randint(1, n_exp + 1, (B, C, K)) # top-K indices ([B, C, K])
expert_probs = F.softmax(torch.rand(B, C, n_exp), dim=2) # expert probabilities ([B, C, n_exp])

# equation (5): compute ratio of tokens allocated to each expert
# total number of tokens is defined as total tokens in batch * K
with torch.no_grad():
    one_hot_indices = F.one_hot(indices, num_classes=n_exp) # [B, C, K, n_exp]
    one_hot_indices = torch.sum(one_hot_indices.float(), dim=2) # [B, C, n_exp] (sum over K dimension)
    tokens_per_expert = torch.mean(one_hot_indices.float(), dim=(0, 1))

# equation (6): compute ratio of router probability allocated to each expert
prob_per_expert = torch.mean(expert_probs.float(), dim=(0, 1))

# equation (4): take a scaled dot product between prob / token allocation vectors
# multiply the result by the number of experts
load_balance_loss = n_exp * torch.sum(prob_per_expert * tokens_per_expert)
```
view raw load_balancing_loss.py hosted with ❤ by GitHub

**라우터 z-손실(Router z-loss).** 로드 밸런싱 손실을 보완하기 위해, [3]의 저자들은 **라우터 z-손실**이라고 불리는 추가 보조 손실 항을 제안합니다. 라우터 z-손실은 라우팅 메커니즘에 의해 예측되는 **로짓(logits)**의 크기를 제한합니다(확률이 아닌, 소프트맥스가 적용되기 전의 값입니다). 공식은 아래를 참조하십시오. 라우터가 (지수) 소프트맥스 함수를 포함하고 있다는 사실 때문에 이러한 로짓이 너무 커지는 것을 원치 않습니다. 그러나 이러한 로짓은 훈련 중에 매우 커질 수 있으며, 이는 훈련 과정을 불안정하게 만드는 반올림 오류(round-off errors)로 이어질 수 있습니다. 심지어 전체(float32) 정밀도를 사용할 때도 마찬가지입니다. 라우터 z-손실은 MoE가 이러한 로짓을 작게 유지하도록 장려하고, 결과적으로 이러한 반올림 오류를 피하도록 합니다.

“라우터는 float32 정밀도로 전문가에 대한 확률 분포를 계산합니다. 그러나 가장 큰 규모에서는 이것이 신뢰할 수 있는 훈련을 제공하기에 불충분하다는 것을 발견했습니다.” - [3]에서 발췌

라우터 z-손실의 구현은 아래에 제공되며, 세 가지 핵심 단계를 포함합니다.

*   **8-14행**: 라우터 z-손실 계산에 필요한 입력 텐서(즉, 라우팅 메커니즘의 로짓)를 생성합니다.
*   **21행**: 라우터 로짓의 제곱된 logsumexp를 취합니다. 이는 지수, 합, 로그 연산을 순서대로 적용하는 수치적으로 안정적인 약식 표현입니다.
*   **24행**: 위 연산의 결과를 모든 토큰에 대해 합산하고 총 토큰 수로 나눕니다(즉, 평균을 취합니다).

This file contains bidirectional Unicode text that may be interpreted or compiled differently than what appears below. To review, open the file in an editor that reveals hidden Unicode characters. Learn more about bidirectional Unicode characters Show hidden characters
```python
"""
Computes ST-MoE router z loss (https://arxiv.org/abs/2202.08906)
See equation (5) on page 7
"""
import torch

# constants
B = 16 # batch size
C = 256 # sequence length
n_exp = 8 # number of experts

# create input tensor for router z-loss
router_logits = torch.rand(B, C, n_exp) # [B, C, n_exp]

# exponentiate logits, sum logits of each expert, take log, and square
# code below is equivalent to the following:
# z_loss = torch.exp(router_logits)
# z_loss = torch.sum(z_loss, dim=-1)
# z_loss = torch.log(z_loss) ** 2.0
router_z_loss = torch.logsumexp(router_logits, dim=-1) ** 2.0 # [B, C]

# sum over all tokens and divide by total number of tokens
router_z_loss = torch.mean(router_z_loss)
```
view raw router_z_loss.py hosted with ❤ by GitHub

**보조 손실 결합.** 여러 보조 손실이 존재한다는 점을 고려할 때, 우리는 실제로 어떤 것을 사용해야 할지 궁금할 수 있습니다. 대답은: **모두 다입니다!** 우리는 훈련 중에 이러한 각 손실을 표준 언어 모델링 손실에 추가할 수 있습니다. 각 보조 손실에는 곱해지는 스케일링 계수(scaling factor)가 있으며, 그런 다음 모든 (스케일링된) 손실을 함께 합산합니다. 아래를 참조하십시오. 로드 밸런싱 및 라우터 z-손실에 대한 기본 스케일링 계수는 각각 `0.001`과 `0.01`입니다.

**현재 연구.** 우리가 보게 되겠지만, 이 섹션에서 배운 보조 손실들은 꽤 잘 작동합니다. 그러나 최근 연구 [8]는 스케일링 계수가 어떻게 설정되는지에 따라 이러한 보조 손실이 경우에 따라 훈련 안정성을 위해 모델 성능을 희생시킬 수 있음을 보여주었습니다. 따라서 MoE 훈련을 위한 최적의 과정과 전략은 여전히 (매우) 활발한 연구 분야입니다.

**DeepSeek-v3 [8]의 보조 손실 없는 로드 밸런싱**

예를 들어, 최근에 제안된 DeepSeek-v3 [8] 모델(DeepSeek-R1 추론 모델을 만드는 데 사용된 기본 모델)은 보조 손실 없는 로드 밸런싱 전략을 사용합니다. 이는 상위 `K`개 전문가를 선택할 때 라우터 출력에 동적 편향(dynamic bias)을 단순히 추가하는 것입니다. 위를 참조하십시오. 이 편향은 충분히 선택되지 않은 전문가에 대해서는 증가하고 너무 많이 선택된 전문가에 대해서는 감소하여, 활용도가 낮은 전문가가 선택될 가능성을 높입니다. 이 동적 편향은 모델 성능을 희생시키지 않고 로드 밸런싱을 개선하는 것으로 밝혀졌습니다. 그러나 로드 밸런싱 손실은 [8]에서도 여전히 사용됩니다(단지 더 작은 스케일링 계수와 함께).

“우리는 각 훈련 단계의 전체 배치에 대한 전문가 로드를 계속 모니터링합니다. 각 단계가 끝날 때, 해당 전문가가 과부하되면 편향 항을 𝛾만큼 감소시키고, 과소 부하되면 𝛾만큼 증가시킵니다. 여기서 𝛾는 편향 업데이트 속도라고 불리는 하이퍼파라미터입니다.” - [8]에서 발췌

**디코더 전용 MoE 구현**

**MoE 기반 디코더 전용 트랜스포머 아키텍처**

이제 전문가 레이어의 모든 주요 구성 요소를 이해했으므로, 이러한 개념들을 결합하여 완전한 MoE 기반 디코더 전용 아키텍처를 만들어 봅시다. 이 모델 내의 MoE 블록(위에 표시됨)은 다음을 포함할 것입니다.

*   일반 (마스크드) 셀프 어텐션 레이어
*   모델의 `P`번째 레이어마다 일반 피드포워드 레이어 대신 전문가 레이어.

이 블록 구조는 표준 디코더 전용 트랜스포머와 유사하지만, 모델 레이어의 일부에서 피드포워드 레이어를 전문가 레이어(MoE 블록을 형성함)로 대체합니다. 먼저, 전문가 레이어의 최종 출력이 어떻게 계산되는지에 대한 몇 가지 남은 세부 사항을 다루겠습니다. 그런 다음, MoE 기반 디코더 전용 트랜스포머의 전체 구현을 제시할 것입니다.

**전문가 레이어 출력 계산.** 라우팅 메커니즘을 사용하여 주어진 토큰에 대한 활성 전문가 집합을 결정했다면, 이 전문가 레이어의 최종 출력을 다음과 같이 계산할 수 있습니다.

*   토큰을 활성 전문가에게 보냅니다.
*   이 토큰에 대한 활성 전문가의 출력을 계산합니다.
*   각 토큰에 대한 전문가 출력의 가중 평균을 취합니다. 여기서 가중치는 라우터에 의해 각 활성 전문가에게 할당된 확률입니다.

이 과정은 위 그림에서 단일 토큰에 대해 묘사되어 있습니다. MoE에 대한 최근 연구는 또한 모든 토큰에 대해 항상 활성화되는 "공유(shared)" 전문가의 아이디어를 도입했습니다. 공유 전문가는 라우팅 논리를 약간 수정하지만, 위에 설명된 동일한 핵심 아이디어는 여전히 적용됩니다. 이 주제에 대한 자세한 내용은 여기를 참조하십시오. 완전한 전문가 레이어의 구현은 아래에 제공되며, 여기서 이러한 아이디어가 PyTorch에 적용된 것을 볼 수 있습니다. 49행에서 우리는 라우터로부터 각 전문가에 대한 데이터 배치와 각 토큰에 대한 관련 전문가 확률을 얻습니다. 그런 다음 이 배치들을 전문가 피드포워드 네트워크(52행)를 통해 전달하여 각 전문가의 출력을 얻습니다. 마지막으로, 54-58행에서 각 전문가의 출력에 관련 확률을 곱하여 전문가 레이어의 최종 출력을 형성합니다.

This file contains bidirectional Unicode text that may be interpreted or compiled differently than what appears below. To review, open the file in an editor that reveals hidden Unicode characters. Learn more about bidirectional Unicode characters Show hidden characters
```python
"""
Based upon ColossalAI OpenMoE
"""
from torch import nn

class MOELayer(nn.Module):
    def __init__(self, d, n_exp=8, top_k=2, use_noisy_top_k=True, capacity_factor=1.25, bias=False, dropout=0.2,):
        """
        Arguments:
            d: size of embedding dimension
            n_exp: the number of experts to create in the expert layer
            top_k: the number of active experts for each token
            use_noisy_top_k: whether to add noise when computing expert output
            capacity_factor: used to compute expert capacity
            bias: whether or not to use bias in linear layers
            dropout: probability of dropout
        """
        super().__init__()
        self.router = Router( # (noisy) top k router
            d=d,
            n_exp=n_exp,
            top_k=top_k,
            use_noisy_top_k=use_noisy_top_k,
            capacity_factor=capacity_factor,
        )
        self.experts = MLPExperts( # group of MLPs (experts)
            d=d,
            n_exp=n_exp,
            bias=bias,
            dropout=dropout,
        )

    def forward(self, x : torch.Tensor):
        B, C, d = x.size() # track original shape of input
        num_tokens = (B * C) # pass each token through the router

        exp_weight, exp_mask, exp_batches = self.router(x)

        # compute expert output
        exp_out = self.experts(exp_batches) # [n_exp, exp_capacity, d]

        # aggregate expert outputs based on router weights
        # eq (2) on page 4 of ST-MoE (https://arxiv.org/abs/2202.08906)
        exp_weight = exp_weight.view(num_tokens, -1) # [B * C, n_exp * exp_capacity]
        exp_out = exp_out.view(-1, d) # [n_exp * exp_capacity, d]
        output = exp_weight @ exp_out # [B * C, d]

        # resize output before return
        return output.view(B, C, d)
```
view raw expert_layer.py hosted with ❤ by GitHub

**PyTorch의 MoE.** 이제 우리는 디코더 전용 트랜스포머 블록을 수정하여 일반적인 피드포워드 레이어 대신 선택적으로 전문가 레이어를 사용할 수 있습니다. 이는 아래 코드에서 달성됩니다. 여기서 우리는 `MLP` 모듈을 새로운 `MoELayer`로 드롭인 대체(drop-in replacement)하여 `MoEBlock`을 형성합니다.

This file contains bidirectional Unicode text that may be interpreted or compiled differently than what appears below. To review, open the file in an editor that reveals hidden Unicode characters. Learn more about bidirectional Unicode characters Show hidden characters
```python
from torch import nn

class MoEBlock(nn.Module):
    def __init__(self, d, H, C, n_exp, top_k, use_noisy_top_k=True, capacity_factor=1.25, bias=False, dropout=0.2,):
        """
        Arguments:
            d: size of embedding dimension
            H: number of attention heads
            C: maximum length of input sequences (in tokens)
            n_exp: the number of experts to create in the expert layer
            top_k: the number of active experts for each token
            use_noisy_top_k: whether to add noise when computing expert output
            capacity_factor: used to compute expert capacity
            bias: whether or not to use bias in linear layers
            dropout: probability of dropout
        """
        super().__init__()
        self.ln_1 = nn.LayerNorm(d)
        self.attn = CausalSelfAttention(d, H, C, bias, dropout)
        self.ln_2 = nn.LayerNorm(d)
        self.mlp = MOELayer(d, n_exp, top_k, use_noisy_top_k, capacity_factor, bias, dropout,)

    def forward(self, x):
        x = x + self.attn(self.ln_1(x))
        x = x + self.mlp(self.ln_2(x))
        return x
```
view raw moe_block.py hosted with ❤ by GitHub

여기에서, 우리 MoE 아키텍처의 최종 구현은 이전의 디코더 전용 트랜스포머(GPT) 구현과 정확히 일치합니다. 유일한 변경 사항은 `P`번째 `Block`을 `MoEBlock`으로 대체한다는 것입니다. 인터리브된 MoE 블록의 추가를 제외하고는 코드가 이전에 정의된 GPT 모델과 동일하므로, 여기서는 이 구현을 명시적으로 작성하는 것을 피할 것입니다.

**nanoMoE를 처음부터 사전 훈련하기**

이제 MoE가 어떻게 작동하는지 이해했으니, 이 아키텍처를 사용하여 LLM을 처음부터 사전 훈련해 봅시다. MoE 기반 LLM의 전체 구현은 아래 저장소에 있습니다. nanoMoE라고 불리는 이 구현은 Andrej Karpathy의 nanoGPT 저장소를 기반으로 합니다. 그러나 원래 GPT 아키텍처는 MoE 기반 디코더 전용 트랜스포머 아키텍처를 사용하도록 수정되었습니다.

nanoMoE 저장소

nanoMoE 저장소는 이 게시물에서 지금까지 보았던 모든 MoE 구성 요소에 대한 코드를 재사용합니다. 이 구현의 핵심 구성 요소는 다음과 같습니다.

*   **모델 구현**: MoE 모델을 구성하는 기능이 추가된 GPT 모델 정의를 참조하십시오. [링크]
*   **훈련**: 모든 훈련 코드는 단일 파일에 있으며, 원래 nanoGPT 코드에서 의미 있게 수정되지 않았습니다. [링크]
*   **데이터셋**: nanoMoE는 OpenWebText 데이터셋의 250억 토큰 부분 집합 12에서 사전 훈련됩니다(nanoGPT와 동일하지만 토큰 수가 적음). [링크]
*   **구성**: nanoMoE를 사전 훈련하는 데 사용된 최종 훈련 구성은 다음 섹션에서 설명할 것이며, 여기에서 찾을 수 있습니다.

이 섹션에서는 nanoMoE를 성공적으로 사전 훈련하기 위해 발견된 모범 사례를 추가로 설명하고, 사전 훈련 결과를 검토하며, 이 중간 크기 MoE 모델에 대해 발견된 최적의 사전 훈련 설정을 설명할 것입니다.

**MoE 훈련을 위한 모범 사례**

“MoE의 여러 주목할 만한 성공에도 불구하고, 복잡성, 통신 비용 및 훈련 불안정성으로 인해 광범위한 채택이 방해받아 왔습니다.” - [6]에서 발췌

MoE는 오래전에 제안되었지만, LLM 연구에서 그 인기는 최근에야 급격히 증가했습니다. 수년 동안 MoE 채택의 주요 장애물은 사용의 어려움이었습니다. 밀집 모델에 비해 MoE는 더 복잡하고 일반적으로 훈련 중에 불안정성에 취약합니다.

**왜 MoE는 불안정한가?** 우리가 보았듯이, MoE 기반 LLM은 디코더 전용 트랜스포머 아키텍처에 약간의 수정만 가합니다. 이를 염두에 두고, 우리는 궁금해할 수 있습니다. MoE 아키텍처의 정확히 무엇이 훈련 중 어려움을 유발할까요? 왜 MoE의 훈련은 표준 LLM에 비해 덜 안정적일까요?

**nanoMoE 사전 훈련 중 발산**

MoE를 훈련할 때 발생하는 두 가지 주요 문제는 다음과 같습니다.

*   **라우팅 붕괴(Routing collapse)**: 모델이 동일한 전문가를 반복해서 활용하는 상태로 수렴합니다.
*   **수치 불안정성(Numerical instability)**: MoE는 특히 라우터에서 반올림 오류(round-off errors)를 경험할 수 있습니다(즉, 소프트맥스에서 지수 함수를 사용하기 때문) 13.

이러한 문제들은 훈련 불안정성으로 이어지며, 이는 모델의 손실이 훈련 과정 중에 단순히 발산할 수 있음을 의미합니다. nanoMoE 훈련의 구체적인 예시는 위를 참조하십시오. 이런 일이 발생하면, 훈련 과정을 중단하고 저장된 체크포인트(checkpoint)에서 다시 시작해야 하는데, 이는 시간이 많이 걸리고 비효율적입니다(즉, 많은 유휴 GPU 시간!). 이상적으로는 이러한 불안정성을 피하는 안정적인 훈련 과정을 원합니다. 따라서 MoE 훈련 안정성을 개선하기 위한 모범 사례를 다루겠습니다.

**보조 손실.** 이전에 논의했듯이, MoE를 훈련할 때 보조 손실 중에서 선택할 필요는 없습니다. 대신, 여러 보조 손실을 단일 손실 함수로 결합할 수 있습니다. nanoMoE의 경우, 우리는 훈련 중에 표준 보조 로드 밸런싱 손실과 라우터 z-손실을 모두 사용합니다. 올바른 보조 손실을 사용하면 전문가의 균일한 사용을 가능하게 하고 훈련 중 라우팅 붕괴를 피함으로써 훈련 안정성을 향상시킵니다.

**훈련 정밀도(Training precision).** LLM을 훈련할 때, 일반적으로 혼합 정밀도 훈련(mixed precision training)을 사용하는 것이 합리적입니다. 이는 모델의 일부 구성 요소를 전체 float32 정밀도 대신 더 낮은 float16 또는 bfloat16 정밀도 형식으로 실행하도록 변환합니다. 이 기능은 PyTorch에서 자동 혼합 정밀도(AMP) 모듈을 통해 자동으로 지원되며, 모델 성능을 저하시키지 않고 훈련 비용을 크게 줄일 수 있습니다. 다시 말해, 이는 최소한의 코드 변경으로 쉽게 활성화할 수 있는 "무료" 사전 훈련 속도 향상입니다.

“BF16 기준선과 비교할 때, 우리의 FP8 훈련 모델의 상대 손실 오류는 일관되게 0.25% 미만을 유지하며, 이는 훈련 무작위성의 허용 가능한 범위 내에 있습니다.” - [8]에서 발췌

혼합 정밀도는 한동안 사용되어 왔지만, 연구자들은 최근 LLM 훈련 정밀도를 16비트보다 더 낮추는 방법을 탐구했습니다. 예를 들어, DeepSeek-v3 [8]는 8비트 정밀도를 사용하여 훈련됩니다. 그러나 훈련 정밀도가 감소함에 따라 동일한 수준의 모델 품질을 유지하는 것이 더 어려워집니다. FP8 정밀도로 대규모 LLM 훈련을 구현하려면 새롭고 복잡한 양자화(quantization) 기술이 필요합니다. 그렇지 않으면, 그렇게 낮은 정밀도로 LLM을 훈련하는 것이 모델의 성능에 부정적인 영향을 미칠 수 있습니다.

```python
with torch.amp.autocast(device_type='cuda', enabled=False):
    # AMP is disabled for code in this block!
    <router code goes here>
```

**이것이 MoE와 왜 관련이 있을까요?** 이전에 언급했듯이, MoE 내의 라우팅 메커니즘은 수치 불안정성에 취약합니다. 라우터의 출력을 더 낮은 정밀도로 계산하면 이 문제가 더욱 악화됩니다! 이 문제는 [6]에 명시적으로 설명되어 있으며, 저자들은 낮은 정밀도 훈련이 라우터에서 큰 반올림 오류로 이어진다는 것을 발견했습니다. 이 문제를 해결하기 위해, 우리는 AMP로 훈련할 때도 라우터를 전체(float32) 정밀도로 실행해야 합니다. 이는 MoE의 라우팅 메커니즘에서 AMP를 단순히 비활성화함으로써 달성할 수 있습니다. 위를 참조하십시오.

**가중치 초기화(Weight initialization).** 전통적으로 대규모 신경망의 안정적인 훈련을 위한 가장 큰 요인 중 하나는 올바른 가중치 초기화 전략을 사용하는 것이었습니다. 예를 들어, Glorot 또는 He 초기화가 있습니다. 이러한 기술들은 배치 정규화(batch normalization)와 같은 전략과 함께, 이전에는 매우 어려웠던 놀랍도록 깊은 신경망을 훈련할 수 있는 능력을 열어주었습니다. LLM의 경우, 우리는 일반적으로 이러한 동일한 가중치 초기화 전략을 채택합니다. 그러나 [6]의 저자들은 MoE를 위해 특별히 설계된 약간 수정된 가중치 초기화 방식을 채택할 것을 권장합니다.

```python
# linear layers have flipped dimensions ([out_dim, in_dim]) in torch
w_fan_in = module.weight.shape[-1]
w_std = (scale / w_fan_in) ** 0.5
torch.nn.init.trunc_normal_(
    module.weight,
    mean=0.0,
    std=w_std,
    a=-2*w_std,
    b=2*w_std,
)
```

이 가중치 초기화 전략은 평균이 0(µ = 0)이고 표준 편차가 σ = SQRT(s/n)로 주어지는 절단 정규 분포(truncated normal distribution)에서 가중치를 샘플링합니다. 여기서 `s`는 스케일 하이퍼파라미터이고 `n`은 초기화되는 레이어의 입력 크기(즉, fan-in 전략)입니다. [6]의 저자들은 또한 "품질을 개선하고 불안정한 훈련 가능성을 줄이기 위해" `s = 0.1`의 감소된 스케일 하이퍼파라미터를 사용할 것을 권장합니다. PyTorch에서 이 수정된 가중치 초기화 전략의 구현은 위에 제공됩니다.

**MoE 미세 조정(finetuning).** 이 개요에서는 nanoMoE 사전 훈련에만 초점을 맞출 것입니다. 그러나 MoE는 표준 밀집 모델에 비해 미세 조정하기가 더 어려울 수 있다는 점을 알아야 합니다. 특히, MoE는 매개변수가 너무 많다는 사실 때문에 과적합(overfitting)에 취약합니다. 이러한 대규모 모델은 방대한 데이터셋에 대한 사전 훈련에는 훌륭하지만, 소량의 데이터로 미세 조정할 경우 과적합될 수 있습니다. 우리는 이 문제를 인지하고 MoE를 미세 조정할 때 과적합을 방지하기 위해 최선을 다해야 합니다(예: 더 높은 드롭아웃(dropout) 비율을 통해). nanoMoE 미세 조정 및 과적합 방지에 대한 탐구는 향후 작업으로 남겨둡니다.

**nanoMoE 사전 훈련 실험**

이제 MoE를 안정적으로 훈련하는 데 사용할 수 있는 다양한 트릭을 이해했으니, nanoMoE를 처음부터 사전 훈련하여 실제 환경에서 테스트해 봅시다. 이 명령들을 직접 테스트하려면 하나 이상의 GPU에 접근할 수 있어야 합니다. 여기에 제시된 실험을 위해, 저는 개인 워크스테이션에서 두 개의 RTX 3090 GPU를 사용했습니다. 이들은 상용 GPU입니다. 메모리가 많지 않습니다(24GB에 불과). 사전 훈련 설정은 그에 따라 축소되어 모든 것이 GPU 메모리에 맞고 일주일 이내에 완전히 실행될 수 있도록 했습니다.

**일반 사전 훈련 설정.** 사전 훈련에 사용된 최종 구성은 여기 에 있으며 다음 설정을 가집니다.

*   **모델 아키텍처**: 6개 레이어(또는 블록), 셀프 어텐션 레이어당 6개 어텐션 헤드, `d = 368`, `N = 8` (총 전문가 수), `K = 2` (활성 전문가 수), `P = 2` (격층마다 MoE 블록 사용).
*   **전문가 용량**: 훈련 시 1.25, 평가 시 2.0의 용량 계수.
*   **보조 손실**: 로드 밸런싱 보조 손실(스케일링 계수 `0.01`)과 라우터 z-손실(스케일링 계수 `0.001`)을 모두 사용합니다.
*   **정밀도**: 훈련에는 자동 혼합 정밀도(bfloat16)를 사용하지만, 라우터는 항상 전체(float32) 정밀도를 사용합니다.
*   **학습률(Learning rate)**: 표준 LLM 학습률 전략을 채택합니다. 훈련 시작 시 `6e-5`에서 `6e-4`로 선형 웜업(linear warmup)한 후, `6e-5`로 코사인 감쇠(cosine decay)합니다.
*   **가중치 초기화**: MoE 훈련 안정성을 개선하기 위해 [6]에서 제안된 가중치 초기화 방식을 사용합니다.

**사전 훈련 데이터셋.** nanoGPT와 유사하게, nanoMoE 사전 훈련을 위해 OpenWebText 데이터셋을 사용합니다. 사전 훈련 프로세스는 약 250억 개의 총 토큰으로 축소되었습니다. 이는 nanoGPT 사전 훈련에 사용된 토큰의 약 10%에 해당합니다. 이 더 작은 데이터셋 덕분에 두 개의 3090 GPU에서 약 5일 만에 사전 훈련을 완료할 수 있습니다. 그러나 더 나은 GPU 설정(예: 8×A100 GPU)을 확보하고 `max_iters = 600,000` (50,000 대신)으로 설정함으로써 이를 전체 사전 훈련 실행으로 쉽게 확장할 수 있습니다.

**안정성 실험.** nanoMoE의 훈련 안정성에 대한 다양한 설정의 영향을 테스트하기 위해, 우리는 다섯 가지 다른 실험을 수행합니다. 먼저, 보조 손실이나 모범 사례를 사용하지 않는 기준선 nanoMoE 모델을 사전 훈련합니다. 이는 좋지 않은 로드 밸런싱과 불안정성으로 이어집니다. 그런 다음, 사전 훈련 안정성에 미치는 영향을 관찰하기 위해 여러 개선 사항을 하나씩 활성화합니다.

*   보조 로드 밸런싱 손실.
*   라우터 z-손실.
*   라우터의 전체 정밀도.
*   개선된 가중치 초기화 방식.

이 다섯 가지 실험 결과는 아래 그림에 나와 있습니다. 보시다시피, 사전 훈련 과정의 각 개선은 훈련 안정성에 약간의 향상을 가져옵니다. 사전 훈련의 발산은 훈련 과정에서 조금 더 늦게 나타납니다. 모든 개선 사항을 함께 활성화하면, 모델은 실제로 아무 문제 없이 전체 훈련 과정을 완료합니다! 여기서 논의된 아이디어들이 nanoMoE의 훈련 안정성에 실질적으로 영향을 미친다는 것을 분명히 알 수 있습니다.

**nanoMoE를 위한 다양한 안정성 기술 테스트**

관심 있는 분들은 직접 이 아이디어들을 시도해 보시길 권합니다! 아래에 표시된 명령을 사용하여 훈련 구성을 조정하고 사전 훈련 프로세스를 실행하기만 하면 됩니다. 이 명령은 하나 이상의 GPU를 사용할 수 있는 단일 노드에서 사전 훈련을 실행한다고 가정합니다.

```bash
torchrun --standalone --nproc_per_node=<number of GPUs> train.py <path to config; e.g., config/train_nano_moe.py>
```

**MoE(Mixture-of-Experts)에 대한 추가 학습**

이 개요에서 우리는 표준 디코더 전용 트랜스포머 아키텍처에서 시작하여 이를 MoE 아키텍처를 사용하도록 수정함으로써 MoE(Mixture-of-Experts) 기반 LLM이 어떻게 작동하는지에 대한 심층적인 이해를 얻었습니다. 그런 다음, OpenWebText 데이터셋에서 nanoMoE라고 불리는 중간 크기의 MoE 기반 LLM을 처음부터 사전 훈련함으로써 이러한 아이디어를 적용했습니다. MoE는 표준 LLM보다 훈련하기 더 어려운 것으로 간주되지만, 우리의 실험에서 보조 손실, 혼합 정밀도, 더 나은 가중치 초기화 등과 같은 아이디어가 MoE를 성공적으로 훈련하는 데(즉, 어떤 불안정성도 없이) 어떻게 적용될 수 있는지 보았습니다! nanoMoE는 훌륭한 학습 도구이지만, MoE의 대부분의 실제 구현은 이보다 더 복잡할 것입니다. MoE가 LLM 연구에서 실제로 어떻게 사용되는지 배우려면, 효율적인 훈련 및 추론을 위한 프로덕션급 MoE 프레임워크(예: OpenMoE [9] 또는 Megablocks [10])와 MoE 주제에 대한 최근 출판물(예: Mixtral, DeepSeek-v3 또는 DBRX)을 살펴보아야 합니다.

뉴스레터가 처음이신가요? 안녕하세요! 저는 Cameron R. Wolfe입니다. 넷플릭스(Netflix)의 딥러닝(Deep Learning) 박사이자 연구 과학자입니다. 이것은 독자들이 AI 연구의 중요한 주제를 이해하도록 돕는 Deep (Learning) Focus 뉴스레터입니다. 뉴스레터가 마음에 드신다면, 구독하거나 공유하거나 X 및 LinkedIn에서 저를 팔로우해주세요!

구독

**참고문헌**

[1] Vaswani, Ashish, et al. "Attention is all you need." Advances in neural information processing systems 30 (2017).
[2] Sennrich, Rico, Barry Haddow, and Alexandra Birch. "Neural machine translation of rare words with subword units." arXiv preprint arXiv:1508.07909 (2015).
[3] Shazeer, Noam. "Glu variants improve transformer." arXiv preprint arXiv:2002.05202 (2020).
[4] He, Kaiming, et al. "Deep residual learning for image recognition." Proceedings of the IEEE conference on computer vision and pattern recognition. 2016.
[5] Zoph, Barret, et al. "St-moe: Designing stable and transferable sparse expert models." arXiv preprint arXiv:2202.08906 (2022).
[6] Fedus, William, Barret Zoph, and Noam Shazeer. "Switch transformers: Scaling to trillion parameter models with simple and efficient sparsity." Journal of Machine Learning Research 23.120 (2022): 1-39.
[7] Shazeer, Noam, et al. "Outrageously large neural networks: The sparsely-gated mixture-of-experts layer." arXiv preprint arXiv:1701.06538 (2017).
[8] Liu, Aixin, et al. "Deepseek-v3 technical report." arXiv preprint arXiv:2412.19437 (2024).
[9] Xue, Fuzhao, et al. "Openmoe: An early effort on open mixture-of-experts language models." arXiv preprint arXiv:2402.01739 (2024).
[10] Gale, Trevor, et al. "Megablocks: Efficient sparse training with mixture-of-experts." Proceedings of Machine Learning and Systems 5 (2023): 288-304.

1 이 아키텍처는 그 자체로 "새로운" 것은 아닙니다. 아주 오래전부터 존재해 왔습니다. 하지만 대규모 LLM 애플리케이션에서의 채택은 비교적 최근의 일입니다.
2 완전한 인코더-디코더 모델의 디코더에서 사용되는 교차 어텐션(cross-attention) 레이어를 제거하기 때문에 디코더는 약간 다릅니다.
3 트랜스포머를 위한 기본적인 위치 인코딩(또는 임베딩)에 대한 설명은 여기에서 찾을 수 있습니다. 그러나 대부분의 현대 LLM은 [1]의 이 기본적인 위치 인코딩 방식 대신 회전 위치 임베딩(RoPE, rotary positional embeddings)을 사용합니다.
4 여기 우리의 구현은 또한 어텐션 드롭아웃(attention dropout)을 수행합니다. 이는 정규화(regularization) 목적으로 훈련 중에 특정 어텐션 점수를 무작위로 드롭하는 것입니다.
5 "포인트와이즈(pointwise)"라는 단어는 시퀀스의 모든 토큰 벡터에 동일한 연산이 적용됨을 나타냅니다. 이 경우, 우리는 시퀀스의 모든 토큰 벡터를 동일한 가중치를 가진 동일한 피드포워드 신경망을 통해 개별적으로 전달합니다.
6 우리는 각 레이어의 입력에 정규화가 적용되는 사전 정규화(pre-normalization) 구조를 사용합니다. 원래 트랜스포머 [1]는 사후 정규화(post-normalization) 구조를 사용했지만, 이후 분석에서 사전 정규화가 더 유리하다는 것이 밝혀졌습니다.
7 신경망 레이어 내에서 잔차 연결을 적용하려면 해당 레이어의 입력 및 출력 차원이 동일해야 합니다. 차원이 동일하지 않은 경우에도 입력을 선형적으로 투영하여 잔차 연결을 적용할 수 있습니다.
8 용량 계수 튜닝에 대한 자세한 내용과 실험은 [5] 및 [6]을 참조하십시오.
9 여기서 세부 사항은 그리 중요하지 않습니다. 이는 라우터의 연산을 벡터화하기 위해 도입된 구현 복잡성일 뿐입니다. 그러나 이는 이해에 관심 있는 사람들에게 PyTorch에서 훌륭한 코딩 연습이 될 것입니다!
10 이 양은 우리의 라우팅 알고리즘에 의해 예측되며, 따라서 미분 가능합니다. 따라서 각 전문가에게 전송된 토큰의 비율 자체가 미분 가능한 양이 아니더라도 손실 함수 전체는 미분 가능합니다.
11 우리는 또한 연산 결과에 N(총 전문가 수)을 곱합니다. 이는 N 값이 증가함에 따라 손실이 일정하게 유지되도록 보장합니다.
12 이 토큰 수는 2개의 RTX 3090 GPU 설정에서 전체 사전 훈련 실행이 약 5일 만에 완료될 수 있도록 선택되었습니다.
13 소프트맥스 변환은 꽤 흔한 연산이지만, 표준 디코더 전용 트랜스포머는 아키텍처 내 어디에도 이러한 지수 함수를 가지고 있지 않다는 점에 유의해야 합니다!