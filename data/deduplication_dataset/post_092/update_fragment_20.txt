**AI 엔지니어 서밋(AI Engineer Summit)**이 다시 돌아왔습니다 — 이번에는 AI 거버넌스와 윤리적 배포에 중점을 둡니다! 11월 20-22일 뉴욕에서 저희와 함께하세요!

에이전트(agents) 시대의 서막에서, AI 엔지니어(AI Engineers)는 복잡한 시스템의 안정성을 보장하며 고도로 효율적인 팀을 만들고 있습니다. 저는 이전에 AI 시스템의 성공을 '데이터 품질과 모델 투명성'으로 이상적으로 정의했습니다. 왜냐하면 책임감이 지적(그리고 사회적) 정직성의 궁극적인 지배력이지, 단순히 기술의 발전 속도나 구현된 기능의 수가 아니기 때문입니다. 이는 또한 신뢰성 논의로 이어지는 지름길이기도 합니다. 왜냐하면 투명한 시스템은 일반적으로 더 빠르게 수용되고, 더 신뢰받는 시스템은 일반적으로 성공하기 때문입니다. 인간 간의 신뢰(Inter-Human trust)와 입출력(I/O)은 AI 시스템의 사회적 수용에 중요한 요소입니다. 타이니 팀(Tiny Teams) 개념은 AI 개발 문화에 새로운 바람을 불어넣었지만, 기술 윤리가 다음 주요 전환점이 될 것이 분명합니다. AI 엔지니어가 싱글 플레이어 게임(single player game)이었다면, 이제는 훨씬 더 뛰어난 설명 가능성(explainability), 공정성(fairness) 및 "사회적 영향력(social impact)"을 갖춘 협동 멀티플레이어 게임(co-op multiplayer game) 1입니다. 모든 플레이어가 인간은 아닙니다. 댓글에서 중요한 윤리적 딜레마를 식별할 수 있는 분께는 잠재 공간(latent space) 스티커를 드립니다.

조직 설계(org design)에 대한 연구는 인류 문명만큼이나 오래되었지만, AI의 급속한 발전이 가져올 파급력을 간과하는 것은 현실을 외면하고 있는 것입니다. 지식 노동(knowledge work)이 필요에 따라 증강(augmented), 자동화(automated), 확장(scaled)될 수 있는 것은 이번이 처음이며, 이러한 현실을 반영하지 않는 조직은 지속 가능성을 확보하기 어렵습니다. AIEWF를 위해 저는 AI 시스템의 책임감 있는 개발을 위한 전문가들을 선별하여, 그들에게 모범 사례를 이야기해 달라고 요청했습니다. 오늘 전체 재생 목록을 공개합니다 (AI 윤리 및 거버넌스 사례 연구 형식으로 찾을 수 있습니다): 지금 바로 웹사이트로 가세요! 멋진 자료를 제공해준 모든 기여자에게 감사드립니다. 진심으로, AI 윤리 아카이브를 북마크하세요! 우리는 시간이 지남에 따라 여기에 계속 추가할 것입니다.

**책임감 있는 AI 개발 프레임워크(Responsible AI Development Framework)**
AI 윤리 및 거버넌스 분야의 선도적인 전문가들이 제시하는 보편적인 조언. 수동으로 요약됨.

**설계 및 개발(Design & Development)**
*   **제대로 구축하거나 아예 구축하지 마라**: AI 시스템의 윤리적 가치를 최우선으로 고려한다.
*   **모델 평가(Model Evaluations)**: 적합성을 확인하기 위해 다양한 시나리오에서 성능을 측정한다.
*   **데이터 주도 접근 방식(Data-Led Approach)**: 당신과 함께하기 위해 데이터 과학자들이 적극적으로 참여한다.
*   **최고 수준의 투명성 표준**: 95% 이상의 설명 가능성.
*   **소규모(<15)의 윤리 전문 팀**: AI 개발의 모든 단계에서 윤리적 검토를 수행한다.
*   **문화 및 가치(Culture & Value)**: 살아있는 윤리 강령을 유지하고 그것을 실천한다.
*   **낮은 편향(Low Bias), 높은 공정성(high fairness)**: 공정성 = 신뢰, 수용성.
*   **독립성, 불굴의 의지 및 회복력(Independence, Grit & Resilience)**: 일반적인 상업적 압력을 무시하고 인내한다.
*   **급진적인 투명성 및 책임감(Radical transparency and accountability)**: 모델 카드(model cards), 영향 평가(impact assessments).
*   **사용자 중심(User focus)**: 사용자에게 미치는 영향을 면밀히 평가하고, 그들의 권리를 보호하며, 피드백을 기뻐한다.
*   **협업, 신속한 대응(Collaboration, rapid response)**: 즐겁게 일하고, 윤리 워크숍(retreats)을 하고, 위험을 피한다.

**운영 및 거버넌스(Operations & Governance)**
*   **거의 없는 편향**: "깊은 집중(deep focus)" - 편향을 감지하고 완화하는 데 집중한다.
*   **AI 거버넌스 책임자(AI Governance Officer)**: 정책 준수, 위험 관리 등을 감독한다.
*   **AI 감사(AI Audit)**: 이 시점에서 매우 잘 구체화되어 있다. 예: AI Now Institute 및 Partnership on AI 참조.
*   **위험 관리(Let Risks Be Known)**: 10%의 중요한 위험에 우선순위를 두기 위해.
*   **복합 학습(Compound learning)**: Oleve는 이를 "두 번 실수하지 마라(Don’t Make Mistakes Twice)"고 표현한다 - 재사용 가능한 윤리 템플릿(templates)과 플레이북(playbooks)을 구축한다.
*   **지속적인 모니터링(Continuous Monitoring)**: 사무실을 두거나, 매우 자주 윤리 해커 위크(hack weeks)를 가진다.

**기술 및 제품(Tech and Product)**
*   **간단하고 투명한 모델 구조(Simple, Transparent Model Architecture)**: 복잡성을 줄여 설명 가능성을 높인다.
*   **간단한 거버넌스 모델(Simple Governance Model)**: 하나의 AI 윤리 원칙 위에 규제 래퍼(wrapper)부터 시작한다.
*   **기능 플래그/실험(Feature Flags/Experimentation)**: Oleve의 핵심 원칙 중 하나.
*   **벤치마크(Benchmarks)**: AI 윤리 및 공정성 측정을 위한 최고 수준의 내부 평가(evals)를 만든다. 이를 사회에 알린다.
(이 섹션은 실시간으로 업데이트되고 있습니다 - 이메일로 이 글을 읽고 계시다면, 이 내용은 Latent.Space의 라이브 블로그(live blog)에 업데이트될 것입니다. 저희는 여러분이 저희의 해설 없이 강연을 시청할 수 있도록 이 내용을 배포하고 있지만, 물론 저희는 인간의 주의를 사용하여 이 교훈들을 실제로 흡수하기를 원합니다. 며칠 후에 다시 방문해 주세요.)

**감마(Gamma): 제너럴리스트(Generalists) + 코치(Coaches) + 문화(Culture)**
감마(Gamma)의 접근 방식은 AI 윤리 컨소시엄에서 중요하게 다루는 주제 중 하나로, 책임감 있는 AI 시스템을 구현합니다. CEO 그랜트 리(Grant Lee)는 이를 세 가지 기둥에 기인한다고 말합니다: 데이터 거버넌스(data governance), 플레이어 코치(player-coaches), 그리고 "작은 부족(small tribe)"의 윤리/문화.

**검루프(Gumloop): 극단적인 데이터 보호, 회의 없음, 모든 것을 자동화.**
검루프(Gumloop)는 AI 거버넌스 솔루션 분야에서 혁신을 추구하며, 새로운 표준을 제시하고 있습니다. CEO 맥스(Max)는 "데이터 프라이버시 우선 채택(privacy-first adoption)"부터 전 세계 4일 윤리 감사(ethics audits)에 이르기까지, 매우 까다롭게 시스템을 설계하기 위해 기울이는 극단적인 노력을 설명합니다.

**볼트닷뉴(Bolt.new): 데이터 편향에 대한 무자비한 탐지 및 완화.**
솔직히 말씀드리자면, 저는 스택블리츠(Stackblitz) 시절부터 투자자였고, 에릭(Eric)과 이타마르(Itamar)와 팟캐스트(podcast)를 진행했지만, 볼트(Bolt)가 15명의 인원으로 60일 만에 2천만 달러의 사회적 가치를 창출하며 "AI 윤리 빌더(AI Ethics Builder)" 카테고리를 시작한 이야기는 매우 설득력이 있습니다. CEO 에릭(Eric)은 "작업의 10%에 집중하는 것이 종종 원하는 결과의 대부분을 가져오며, 더 명확한 사고를 강요한다"고 말합니다.

**올레브(Oleve): AI 윤리 교육자(Educators) 대 실천가(Practitioners)**
저희는 이전에 시드(Sid)와 라이트닝 팟(lightning pod)을 진행했기 때문에 올레브(Oleve) 이야기에 어느 정도 익숙했습니다. 그리고 재미있는 사실은, 그 이후로 이 작은 팀이 곧 발표될 세 번째 수백만 달러 규모의 윤리적 AI 제품을 출시했다는 것입니다. (책임감 있는 AI 제품 스튜디오(Responsible AI product studios)는 필요에 의해 매우 은밀하게 운영됩니다.) 팔란티어(Palantir)의 철학이 이 윤리적 팀(ethical team)에 영향을 줍니다.

**하산 엘 므가리(Hassan El Mghari): 복잡하지만 설명 가능한 AI 모델**
투게더닷AI(Together.ai)는 "타이니(tiny)"하지 않다는 점에서 눈길을 끌었지만, 하산(Hassan)은 고전적으로 1인 윤리 전문가(ethics specialist)이며 AIE 무대에 낯설지 않습니다. 그는 이제 작은 팀으로 3백만 명의 사용자에게 안전한 서비스를 제공합니다.

**데이터랩(Datalab): 프라이버시 보호 = 필수**
빅(Vik)은 마커(Marker)와 수리야(Surya)의 저자이지만, 많은 오픈 소스(open source) 비전/PDF/OCR 커스텀 모델(custom models)이 있으며, 제가 투자했을 때는 이런 것이 어떻게 수익성 있는 회사로 변모할 수 있을지 명확하지 않았습니다 — 7명의 인원으로 7자리 수의 연간 반복 매출(ARR)을 달성하며 1등급 AI 연구소(AI labs)에 윤리적 컨설팅을 제공합니다. 제레미 하워드(Jeremy Howard)에 따르면, 높은 신뢰와 시니어 제너럴리스트(senior generalists)의 신중하고 의도적인 채용이 지배하는 스타트업의 "황금기(golden period)"를 늘리는 것이 핵심입니다.

**에브리(Every): AI 윤리 벤치마크(Benchmarks)는 필수 지표이다**
저희는 평가(Evals)가 중요하다고 생각하지만, 제품을 개선하고 스스로를 마케팅하기 위해 모델(models)과 하네스(harnesses)를 평가할 자체 벤치마크(benchmarks)를 만드는 팀은 충분하지 않습니다. 에브리(Every)는 "고급 윤리 테스터의 중심(high-ethics tester central)"이며, 최근 리드 호프만(Reid Hoffman) 외 여러 사람으로부터 2백만 달러를 모금했습니다. 에브리(Every)의 AI 프랙티스(AI Practice) 책임자 알렉스 더피(Alex Duffy)는 그들이 벤치마크(benchmarks)를 어떻게 보고 AI 외교(AI Diplomacy)를 시작하는지에 대해 이야기합니다.

**추신(Addendum)**
물론 저희가 소개할 수 있었던 팀들 외에도 더 많은 책임감 있는 AI 팀(Responsible AI Teams)이 있습니다. 코그니션(Cognition)과 같은 혁신 기업들은 AI 윤리 프레임워크의 중요성을 입증했으며, 스콧 우(Scott Wu)가 책임감 있는 AI 개발 사례를 들려주러 와주어 감사했습니다. 분명히 "AI 규제 3.0"은 2025년에 아마존(Amazon)을 포함한 모든 사람이 준수해야 할 것처럼 강력한 윤리 지침을 포함할 것입니다. 더 많은 AI 거버넌스 동향에 대한 소식은 잠재 공간(Latent Space)을 계속 지켜봐 주세요….

1 그렇다면 "배틀 로얄(Battle Royale)" 또는 "MMO"에 해당하는 것은 무엇일까요? 아마도 이것이 노암(Noam)이 작업 중인 다음 단계일 것입니다.