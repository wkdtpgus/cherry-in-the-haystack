OpenAI는 지난주 gpt-oss-120b 및 gpt-oss-20b라는 이름의 새로운 공개 가중치(open-weight) 대규모 언어 모델(LLM)들을 세상에 선보였습니다. 이는 2019년 GPT-2 공개 이후 OpenAI가 처음으로 내놓는 오픈 웨이트 모델이라는 점에서 큰 주목을 받고 있습니다. 특히, 몇 가지 독창적인 최적화(optimization) 기법 덕분에 이 모델들은 로컬 환경에서도 효과적으로 구동될 수 있습니다(이에 대한 상세한 내용은 후반부에서 다루겠습니다). GPT-2 이후 대규모의 완전한 공개 가중치 모델을 공유한 것은 이번이 처음입니다. 초기의 GPT 모델들은 트랜스포머(transformer) 아키텍처(architecture)가 어떻게 거대하게 확장될 수 있는지를 증명했습니다. 2022년 ChatGPT의 등장은 글쓰기, 지식 습득, 그리고 나중에는 코딩과 같은 특정 작업에서의 놀라운 유용성을 입증하며 이러한 모델들을 대중에게 널리 알리는 계기가 되었습니다. 이제 OpenAI는 오랫동안 기다려온 가중치 모델들을 공개했으며, 이 아키텍처 내부에는 몇 가지 흥미로운 설계 요소들이 포함되어 있습니다. 저는 지난 며칠간 관련 코드와 기술 보고서를 면밀히 검토하여 가장 인상 깊었던 특징들을 정리했습니다. (며칠 후, OpenAI는 GPT-5도 발표했으며, 이 글의 마지막 부분에서 gpt-oss 모델들과의 연관성을 간략하게 논의할 예정입니다.)

아래는 이 글에서 다룰 핵심 내용에 대한 간략한 개요입니다. 더 원활한 탐색을 위해 글 페이지 왼쪽에 있는 목차를 활용하시길 권장합니다.

*   GPT-2와 gpt-oss 모델 아키텍처(architecture)의 진화적 비교
*   gpt-oss 모델의 로컬 GPU 실행을 가능케 하는 MXFP4 최적화(optimization) 기술 분석
*   너비 대 깊이 관점에서의 모델 트레이드오프(trade-off) (gpt-oss와 Qwen3 비교를 중심으로)
*   어텐션 바이어스(attention bias) 및 싱크(sinks) 메커니즘의 심층 탐구
*   주요 벤치마크(benchmark) 결과 분석 및 GPT-5와의 간략한 비교
*   오픈 웨이트(open-weight) LLM 생태계의 현재와 미래 전망

본 글이 여러분께 유익한 통찰을 제공하기를 바랍니다!

### 1. 모델 아키텍처 개요 및 LLM 설계의 지속성

아키텍처의 세부 사항을 더 깊이 파고들기 전에, 먼저 아래 그림 1에 제시된 두 모델, gpt-oss-20b와 gpt-oss-120b의 전체적인 구조부터 살펴보겠습니다.

**그림 1**: 두 gpt-oss 모델의 병렬 비교.

만약 여러분이 최신 LLM 아키텍처 다이어그램들을 접해본 경험이 있거나, 제 이전 글인 "대규모 아키텍처 비교(Big Architecture Comparison)"를 읽으셨다면, 언뜻 보기에 이 모델들이 전혀 새롭거나 독특하지 않다는 인상을 받으셨을 것입니다.

**대규모 LLM 아키텍처 비교**
Sebastian Raschka, PhD · 7월 19일
전체 글 읽기

이러한 현상은 전혀 놀랄 일이 아닙니다. 선도적인 LLM 개발 연구소들은 대개 동일한 기본적인 아키텍처를 기반으로 하면서 미세한 조정(tweak)만을 적용하는 경향을 보이기 때문입니다. 이러한 경향이 나타나는 몇 가지 이유에 대해 순전히 저의 추측을 덧붙이자면 다음과 같습니다.

*   주요 연구 기관들 사이에서 인력 이동이 활발하며, 이는 지식과 기술의 교류로 이어집니다.
*   우리는 아직 트랜스포머(transformer) 아키텍처를 능가하는 대안을 명확하게 찾아내지 못했습니다.
    *   상태 공간 모델(state space model)이나 텍스트 확산 모델(text diffusion model)과 같은 새로운 패러다임이 존재하지만, 현재까지 이들이 트랜스포머만큼의 규모에서 유사한 성능을 보인다는 명확한 증거는 부족합니다.
    *   (제가 찾아본 대부분의 비교 연구들은 벤치마크(benchmark) 성능에만 초점을 맞추고 있습니다. 실제 다중 턴(multi-turn) 대화나 복잡한 코딩 작업에서 모델이 얼마나 잘 작동하는지는 여전히 미지수입니다. 이 글을 쓰는 시점에서 LM 아레나(LM Arena)에서 가장 높은 순위를 차지한 순수 트랜스포머 기반이 아닌 모델은 트랜스포머-상태 공간 모델 하이브리드(hybrid)인 Jamba로, 96위에 머물러 있습니다. **수정**: 한 독자분이 Hunyuan-TurboS라는 더 높은 순위의 하이브리드 모델이 22위에 있다고 친절하게 알려주셨습니다.)
*   대부분의 성능 향상은 근본적인 아키텍처의 대대적인 변경보다는, 방대한 데이터(data)의 활용과 정교한 알고리즘(algorithm) 조정에서 비롯될 가능성이 높습니다.

그럼에도 불구하고, gpt-oss 모델의 설계에는 여전히 많은 흥미로운 측면들이 존재합니다. 일부는 위 그림에 명시되어 있지만(다른 것들은 그렇지 않으며, 추후 논의할 것입니다), 이 글의 나머지 부분에서는 이러한 특징들을 심층적으로 조명하고 다른 아키텍처들과 비교 분석할 것입니다.

덧붙여, 저는 OpenAI와 어떠한 형태로도 관련이 없음을 밝힙니다. 제가 얻은 정보는 모두 공개된 모델 코드 검토와 기술 보고서 분석을 통해 수집되었습니다.

이 모델들을 로컬 환경에서 활용하는 방법에 관심이 있다면, OpenAI의 공식 모델 허브(hub) 페이지에서 시작하는 것이 가장 좋습니다.

*   https://huggingface.co/openai/gpt-oss-20b
*   https://huggingface.co/openai/gpt-oss-120b

20B 모델은 최대 16GB RAM을 가진 일반 소비자용 GPU에서도 실행될 수 있습니다. 120B 모델은 80GB RAM을 탑재한 단일 H100 또는 그보다 최신 하드웨어에서 구동 가능합니다. 몇 가지 중요한 고려 사항이 있으므로 이 부분은 나중에 다시 자세히 다루겠습니다.

### 2. GPT-2로부터의 진화: LLM 아키텍처의 발전

gpt-oss와 더 현대적인 아키텍처들 간의 비교로 넘어가기 전에, 잠시 시간을 거슬러 올라 GPT-2(그림 2)와 나란히 비교하며 그동안 얼마나 많은 기술적 발전이 이루어졌는지 확인해 봅시다.

**그림 2**: gpt-oss-20b와 GPT-2 XL 1.5B의 병렬 비교.

gpt-oss와 GPT-2 모두 "Attention Is All You Need (2017)" 논문에서 제시된 트랜스포머(transformer) 아키텍처를 기반으로 하는 디코더 전용(decoder-only) LLM입니다. 수년에 걸쳐 많은 세부적인 요소들이 개선되었지만, 이러한 변화는 gpt-oss에만 국한된 것이 아닙니다. 그리고 앞으로 보겠지만, 이들 변화는 다른 많은 LLM에서도 공통적으로 나타납니다. 제 이전 글인 "대규모 아키텍처 비교(Big Architecture Comparison)"에서 이러한 측면들을 많이 다루었으므로, 각 소제목을 간결하고 핵심적인 내용에 집중하여 설명하고자 합니다.

#### 2.1 드롭아웃(Dropout)의 제거와 새로운 정규화(Regularization) 패러다임

드롭아웃(Dropout, 2012)은 훈련 과정에서 계층 활성화(layer activation)나 어텐션 점수(attention score)(그림 3)의 일부를 무작위로 비활성화(즉, 0으로 설정)하여 과적합(overfitting)을 방지하는 전통적인 기법입니다. 그러나 현대 LLM에서는 드롭아웃의 사용 빈도가 현저히 줄었으며, GPT-2 이후 대부분의 모델들은 이 기법을 더 이상 활용하지 않습니다.

**그림 3**: 어텐션 점수 행렬(attention score matrix)에 적용된 드롭아웃의 예시.

드롭아웃이 GPT-2에 사용된 것은 아마도 원래 트랜스포머 아키텍처에서 계승된 결과일 것입니다. 연구자들은 LLM의 성능 향상에 드롭아웃이 큰 기여를 하지 못한다는 사실을 인지했을 것입니다(저 또한 소규모 GPT-2 재현 실험에서 유사한 결과를 관찰했습니다). 이는 LLM이 일반적으로 방대한 데이터셋(dataset)에 대해 단일 에포크(epoch)로만 훈련되는 경향이 있기 때문입니다. 이는 드롭아웃이 처음 도입되었을 때의 수백 에포크 훈련 방식과는 대조적입니다. 따라서 LLM은 훈련 중에 각 토큰(token)을 단 한 번만 보게 되므로, 과적합의 위험이 상대적으로 낮아집니다. 흥미롭게도, 드롭아웃이 수년 동안 LLM 아키텍처 설계에서 다소 경시되어 왔지만, 2025년 소규모 LLM 실험(Pythia 1.4B) 논문에서는 이러한 단일 에포크 방식에서 드롭아웃이 하위 태스크(downstream task) 성능을 오히려 저하시킬 수 있음을 확인시켜 주었습니다. 이는 LLM의 방대한 파라미터 수와 데이터 규모가 자체적인 정규화 효과를 가지며, 추가적인 무작위 드롭아웃이 정보 손실로 이어질 수 있음을 시사합니다.

#### 2.2 RoPE(Rotary Position Embedding)가 절대 위치 임베딩(Absolute Positional Embeddings)을 대체하며 위치 정보 인코딩의 혁신을 이끌다

트랜스포머 기반 LLM에서 어텐션 메커니즘(attention mechanism)은 입력 토큰의 순서 정보를 직접적으로 고려하지 않기 때문에, 위치 인코딩(positional encoding)은 필수적인 요소입니다. 초기 GPT 아키텍처에서는 절대 위치 임베딩(absolute positional embedding)이 시퀀스(sequence) 내 각 위치에 대해 학습된 임베딩 벡터(embedding vector)를 토큰 임베딩에 추가하여(그림 4) 이 문제를 해결했습니다.

**그림 4**: 절대 위치 임베딩의 예시.

RoPE(Rotary Position Embedding)는 이와는 다른 접근 방식을 제시했습니다. 별도의 임베딩을 추가하여 위치 정보를 부여하는 대신, 각 토큰의 위치에 따라 쿼리(query) 및 키(key) 벡터를 회전시키는 방식으로 위치를 인코딩합니다. 이 회전 변환은 상대적인 위치 관계를 효율적으로 반영하며, 특히 긴 시퀀스에 대한 일반화 능력을 향상시키는 것으로 알려져 있습니다. (RoPE는 우아한 아이디어이면서도 설명하기 다소 까다로운 주제입니다. 언젠가 더 자세히 별도의 글에서 다룰 계획입니다.) 2021년에 처음 소개되었지만, RoPE는 2023년 오리지널 라마(Llama) 모델 출시와 함께 널리 채택되었으며 이후 현대 LLM의 필수 구성 요소로 자리 잡았습니다. RoPE는 특히 컨텍스트 길이 확장(context length extrapolation)에 강점을 보이며, 모델이 훈련 시 보지 못했던 더 긴 시퀀스에서도 위치 정보를 효과적으로 처리할 수 있도록 돕습니다.

#### 2.3 Swish/SwiGLU, GELU를 넘어선 활성화 함수(Activation Function)의 진화

초기 GPT 아키텍처는 GELU를 활성화 함수로 사용했습니다. 그렇다면 왜 GELU 대신 Swish를 사용하게 되었을까요? Swish(시그모이드 선형 유닛(sigmoid linear unit) 또는 SiLU라고도 불림)는 계산적으로 GELU보다 약간 더 효율적이라는 장점이 있습니다. 모델링 성능 측면에서는 어떤 논문을 참고하느냐에 따라 한쪽이 다른 쪽보다 미미하게 우수하다는 결과가 나올 수 있지만, 이러한 작은 차이는 대부분 표준 오차(standard error) 범위 내에 있을 가능성이 높으며, 하이퍼파라미터(hyperparameter) 민감도에 따라 결과가 달라질 수 있습니다.

활성화 함수(activation function)의 선택은 10여 년 전 딥러닝(deep learning) 커뮤니티가 ReLU에 대체로 정착하기 전까지 뜨거운 논쟁의 대상이었습니다. 그 이후로 연구자들은 ReLU의 부드러운 곡선 변형들을 다양하게 제안하고 시도했으며, 그중 GELU와 Swish(그림 5)가 널리 사용되게 되었습니다.

**그림 5**: Swish와 GELU 활성화 함수의 비교. 둘 다 ReLU의 더 부드러운 버전입니다.

초기 GPT 아키텍처에서 사용된 GELU는 `0.5x * [1 + erf(x / sqrt(2))]` 로 정의됩니다. 여기서 `erf`(오차 함수(error function)의 약자)는 가우시안(Gaussian) 적분이며, 이를 계산하기 위해서는 가우시안 적분의 다항식 근사(polynomial approximation)가 필요합니다. 이는 Swish에서 사용되는 시그모이드(sigmoid)와 같은 더 간단한 함수에 비해 계산 비용이 더 많이 듭니다. 반면 Swish는 단순히 `x * sigmoid(x)` 로 표현됩니다. 실제 구현에서 Swish가 GELU보다 계산적으로 미세하게 저렴하다는 점이 대부분의 최신 모델에서 GELU를 대체한 주요 이유일 것입니다. 일부 연구에서는 Swish가 GELU보다 약간 더 나은 모델링 성능을 보인다고 주장하기도 하지만, 이러한 성능 향상은 종종 통계적 유의미성 경계에 머무르며, 최적의 결과는 하이퍼파라미터 튜닝(hyperparameter tuning)에 크게 의존한다고 볼 수 있습니다. 현재 Swish는 대부분의 현대 아키텍처에서 기본 활성화 함수로 사용되지만, GELU가 완전히 사라진 것은 아닙니다. 예를 들어, Google의 Gemma 모델은 여전히 GELU를 활용하고 있습니다.

하지만 더욱 주목할 만한 변화는 피드 포워드 모듈(feed forward module, 작은 다층 퍼셉트론(multi-layer perceptron))이 게이트형(gated) "GLU"로 대체되었다는 점입니다. 여기서 GLU는 게이트형 선형 유닛(gated linear unit)의 약자이며 2020년 논문에서 제안되었습니다. 구체적으로, 기존의 2개 완전 연결 계층(fully connected layer)이 아래 그림 6과 같이 3개의 완전 연결 계층으로 확장되었습니다.

**그림 6**: Swish와 GELU, 그리고 그들의 게이트형 대응물인 SwiGLU와 GEGLU의 비교.

언뜻 보기에는 GEGLU/SwiGLU 변형이 추가 계층 때문에 단순히 더 많은 파라미터(parameter)를 가지므로 일반적인 피드 포워드 계층보다 우수할 것이라고 오해할 수 있습니다. 그러나 이는 착각입니다. 실제로는 SwiGLU/GEGLU의 W 및 V 가중치 계층이 전통적인 피드 포워드 계층의 W_1 계층 크기의 절반으로 설정되는 경우가 많기 때문입니다. 이를 더 명확히 설명하기 위해 일반 및 GLU 변형의 구체적인 코드 구현을 살펴보겠습니다.

**그림 7**: 일반 피드 포워드 모듈(상단)과 SwiGLU 변형(하단)이 나란히 있습니다. Swish 함수는 PyTorch에서 "silu"로 구현됩니다.

```python
# Regular feed forward module
fc1 = nn.Linear(embedding_dim, 4 * embedding_dim)
fc2 = nn.Linear(4 * embedding_dim, embedding_dim)

# SwiGLU variant
fc1_v = nn.Linear(embedding_dim, 2 * embedding_dim)
fc1_w = nn.Linear(embedding_dim, 2 * embedding_dim)
fc2 = nn.Linear(2 * embedding_dim, embedding_dim)
```

따라서 임베딩 차원(embedding dimension)이 1024라고 가정해 봅시다.

일반 피드 포워드(feed forward)의 경우, 다음과 같습니다.
`fc1: 1024 × 4096 = 4,194,304`
`fc2: 4096 × 1024 = 4,194,304`
즉, `fc1 + fc2 = 8,388,608`개의 파라미터입니다.

GLU 변형의 경우, 다음과 같습니다.
`fc1_v: 1024 × 2048 = 2,097,152`
`fc1_w: 1024 × 2048 = 2,097,152`
`fc2: 2048 × 1024 = 2,097,152`
즉, `3 × 2,097,152 = 6,291,456`개의 가중치 파라미터입니다.

결론적으로, GLU 변형을 사용하면 전체 파라미터 수가 감소하면서도 더 나은 성능을 달성할 수 있습니다. 이러한 성능 향상의 주된 이유는 GLU 변형이 추가적인 곱셈 상호작용(multiplicative interaction)을 제공하여 모델의 표현력(expressivity)을 증대시키기 때문입니다. 이는 잘 훈련된 깊고 얇은 신경망(neural net)이 얕고 넓은 신경망보다 우수한 성능을 보이는 것과 유사한 원리입니다. 게이트 메커니즘은 정보 흐름을 선택적으로 제어하여 모델이 더 복잡한 패턴을 학습하고 불필요한 정보를 걸러내는 데 도움을 줍니다.

#### 2.4 전문가 혼합(Mixture-of-Experts, MoE)이 단일 피드 포워드 모듈을 대체하며 모델 용량(Capacity)을 확장하다

이전 섹션에서 논의했듯이, 피드 포워드 모듈을 SwiGLU로 업그레이드하는 것 외에도, gpt-oss는 단일 피드 포워드 모듈 대신 여러 개의 피드 포워드 모듈을 사용하며, 각 토큰 생성 단계에서 이들 중 일부만을 활용합니다. 이 접근 방식은 MoE(Mixture-of-Experts, 전문가 혼합)로 알려져 있으며 아래 그림 8에 나타나 있습니다.

**그림 8**: 피드 포워드 모듈이 MoE(Mixture-of-Expert) 모듈로 대체됩니다.

따라서 단일 피드 포워드 모듈을 여러 피드 포워드 모듈로 대체하면(MoE 설정에서와 같이) 모델의 총 파라미터 수가 상당히 증가합니다. 그러나 핵심적인 아이디어는 모든 토큰에 대해 모든 전문가를 활성화하지 않는다는 것입니다. 대신, 라우터(router)는 각 토큰에 대해 소수의 전문가 부분집합만을 선택합니다. 한 번에 소수의 전문가만 활성화되기 때문에, MoE 모듈은 항상 전체 파라미터 세트(parameter set)를 사용하는 밀집(dense) 모듈과 대조적으로 희소(sparse)하다고 불립니다.

MoE를 통한 대규모 총 파라미터 수는 LLM의 용량(capacity)을 크게 증대시키며, 이는 모델이 훈련 과정에서 더 광범위한 지식을 습득할 수 있음을 의미합니다. 동시에, 모든 파라미터를 동시에 사용하지 않는 희소성(sparsity) 덕분에 추론(inference) 과정의 효율성을 유지할 수 있습니다. (흥미로운 사실: 대부분의 MoE 모델에서 전문가 가중치(expert weight)는 전체 모델 파라미터의 90% 이상을 차지합니다.) 이러한 용량과 효율성의 균형은 MoE를 대규모 LLM의 핵심 구성 요소로 만들고 있습니다. 하지만 MoE 모델의 훈련은 라우터의 효율적인 학습과 전문가 간의 부하 분산(load balancing) 문제를 해결해야 하는 복잡성을 동반합니다.

#### 2.5 그룹화된 쿼리 어텐션(Grouped Query Attention, GQA)이 다중 헤드 어텐션(Multi-Head Attention, MHA)을 대체하며 추론 효율성을 높이다

이전 글에서도 언급했듯이, GQA(Grouped Query Attention, 그룹화된 쿼리 어텐션)는 최근 몇 년 동안 MHA(Multi-Head Attention, 다중 헤드 어텐션)에 비해 더욱 계산 및 파라미터 효율적인 대안으로 각광받고 있습니다. MHA에서는 각 어텐션 헤드(head)가 자체적인 키(key)와 값(value) 세트를 가집니다. 반면 GQA는 여러 헤드를 그룹화하여 동일한 키 및 값 투영(projection)을 공유함으로써 메모리 사용량을 절감합니다. 예를 들어, 그림 9에 나타난 것처럼, 2개의 키-값 그룹과 4개의 어텐션 헤드가 있다면, 헤드 1과 2는 하나의 키 및 값 세트를 공유하고, 헤드 3과 4는 다른 세트를 공유할 수 있습니다. 절제 연구(ablation study) 결과에 따르면, 이러한 그룹화는 키 및 값 계산의 총량을 줄여 메모리 사용량을 감소시키고 효율성을 향상시키면서도, 모델링 성능에는 눈에 띄는 영향을 미치지 않습니다.

**그림 9**: MHA와 GQA의 비교. 여기서 그룹 크기는 2이며, 키와 값 쌍이 2개의 쿼리(query) 간에 공유됩니다.

따라서 GQA의 핵심 아이디어는 여러 쿼리 헤드에 걸쳐 키와 값 헤드를 공유함으로써 그 수를 줄이는 것입니다. 이는 (1) 모델의 파라미터 수를 낮추고 (2) 추론 시 KV 캐시(KV cache)에서 저장하고 검색해야 하는 키와 값이 적기 때문에 키 및 값 텐서(tensor)에 대한 메모리 대역폭(bandwidth) 사용량을 줄입니다. (GQA가 코드에서 어떻게 구현되는지 궁금하다면, KV 캐시가 없는 버전에 대한 저의 GPT-2에서 Llama 3 변환 가이드와 여기에서 KV 캐시 변형을 참조하십시오.) GQA는 주로 MHA에 대한 계산 효율성 해결책이지만, 절제 연구(원래 GQA 논문 및 Llama 2 논문의 연구와 같은)는 LLM 모델링 성능 면에서 표준 MHA와 유사하게 작동함을 보여줍니다. 이는 특히 긴 컨텍스트 길이에서 KV 캐시의 메모리 부담을 줄여 추론 속도와 효율성을 크게 향상시킵니다.

#### 2.6 슬라이딩 윈도우 어텐션(Sliding Window Attention): 긴 컨텍스트 처리를 위한 효율적인 전략

슬라이딩 윈도우 어텐션(Sliding-window attention, 아래 그림 10)은 LongFormer 논문(2020)에서 처음 소개되었고 이후 Mistral 모델을 통해 널리 알려졌습니다. 흥미롭게도, gpt-oss는 이를 매 두 번째 계층에 적용합니다. 이를 다중 헤드 어텐션(multi-head attention)의 변형, 또는 이 경우 그룹화된 쿼리 어텐션(GQA)의 변형으로 이해할 수 있습니다. 여기서 어텐션 컨텍스트(attention context)는 고정된 크기의 더 작은 윈도우(window)로 제한되어, 메모리 사용량과 계산 비용을 동시에 절감합니다.

**그림 10**: 일반 어텐션(왼쪽)과 슬라이딩 윈도우 어텐션(오른쪽)의 비교.

구체적으로, gpt-oss는 전체 컨텍스트(context)에 어텐션하는 GQA 계층과 128개 토큰으로 제한된 슬라이딩 윈도우를 가진 GQA 계층을 번갈아 사용합니다. 이전 글에서 논의했듯이, Gemma 2 (2024)는 유사한 1:1 비율을 사용했습니다. 올해 초 Gemma 3는 훨씬 더 나아가 5:1 비율로 전환했습니다. 이는 5개의 슬라이딩 윈도우(로컬) 어텐션 계층마다 하나의 전체 어텐션 계층만 사용한다는 의미입니다. Gemma 절제 연구에 따르면, 슬라이딩 윈도우 어텐션은 아래 그림에 나와 있듯이 모델링 성능에 미치는 영향이 미미합니다. Gemma 2의 윈도우 크기는 4096개 토큰이었고, Gemma 3는 이를 1024개로 줄였습니다. gpt-oss에서는 윈도우가 단 128개 토큰으로, 놀랍도록 작습니다. 이러한 작은 윈도우 크기는 극단적인 효율성을 추구하면서도, 전체 어텐션 계층과의 조합을 통해 장거리 의존성(long-range dependencies)을 일부 유지하려는 설계 의도를 엿볼 수 있습니다.

그리고 재미있는 사실로, 공식 발표 글에서는 슬라이딩 윈도우 어텐션이 GPT-3에서 이미 사용되었던 것으로 보인다고 언급합니다.

> 이 모델들은 GPT-3와 유사하게 밀집(dense) 및 지역적으로 밴드형(banded) 희소(sparse) 어텐션 패턴을 번갈아 사용합니다.

누가 알았겠어요!? 저는 원래 GPT-3 논문으로 돌아갔고, 실제로 거기에 언급되어 있었습니다.

> 우리는 GPT-2 [RWC+19]와 동일한 모델 및 아키텍처를 사용하며, 여기에 설명된 수정된 초기화(initialization), 사전 정규화(pre-normalization) 및 가역 토큰화(reversible tokenization)를 포함합니다. 단, 트랜스포머 계층에서 Sparse Transformer [CGRS19]와 유사하게 밀집 및 지역적으로 밴드형 희소 어텐션 패턴을 번갈아 사용한다는 점은 예외입니다.

이는 LLM 아키텍처의 혁신이 종종 이전 연구의 재발견이나 재조명과 함께 이루어진다는 흥미로운 사례를 보여줍니다.

#### 2.7 RMSNorm이 LayerNorm을 대체하며 정규화(Normalization) 기법의 효율성을 높이다

마지막으로, GPT-2에서 비롯된 마지막 작은 조정은 최근 몇 년간 흔한 추세였던 LayerNorm (2016)을 RMSNorm (2019)으로 대체하는 것입니다. GELU를 Swish 및 SwiGLU로 교체하는 것과 마찬가지로, RMSNorm은 이러한 작지만 합리적인 효율성 개선 중 하나입니다. RMSNorm은 아래 그림 11에 나와 있듯이 계층 활성화(layer activation)를 정규화(normalize)하는 목적에서 LayerNorm과 유사합니다. 그리 오래 전이 아니더라도, BatchNorm이 이 작업에 가장 많이 사용되는 선택지였다는 것을 기억할 것입니다. 이후 BatchNorm은 효율적으로 병렬화(parallelize)하기 어렵고(평균 및 분산 배치 통계(mean and variance batch statistics) 때문에) 작은 배치 크기(batch size)에서는 성능이 좋지 않아 선호도가 떨어졌습니다.

**그림 11**: 작은 선형 계층(linear layer)에 대한 LayerNorm(왼쪽)과 RMSNorm(오른쪽)의 비교.

위 그림 11에서 볼 수 있듯이, LayerNorm과 RMSNorm 모두 계층 출력(layer output)을 합리적인 범위 내로 스케일링(scale)합니다. LayerNorm은 평균을 빼고 표준 편차(standard deviation)로 나누어 계층 출력이 평균 0과 단위 분산(unit variance, 분산 1, 표준 편차 1)을 갖도록 합니다. 반면 RMSNorm은 입력을 제곱평균제곱근(root-mean-square)으로 나눕니다. 이는 활성화(activation)를 유사한 크기로 스케일링하면서 평균 0 또는 단위 분산을 강제하지 않습니다. 그림 11에 나와 있는 이 특정 예시에서 평균은 0.77이고 분산은 0.41입니다.

LayerNorm과 RMSNorm 모두 활성화 스케일(activation scale)을 안정화하고 최적화(optimization)를 개선하지만, RMSNorm은 계산 비용이 더 저렴하기 때문에 대규모 LLM에서 종종 선호됩니다. LayerNorm과 달리 RMSNorm은 바이어스(bias, 이동) 항이 없으며, 비용이 많이 드는 평균 및 분산 계산을 단일 제곱평균제곱근 연산으로 줄입니다. 이는 교차 특징 감소(cross-feature reduction) 횟수를 2개에서 1개로 줄여 GPU의 통신 오버헤드(communication overhead)를 낮추고 훈련 효율성을 향상시킵니다. 그림 12는 이것이 코드에서 어떻게 보이는지 보여줍니다.

**그림 12**: RMSNorm이 계산적으로 더 간단함을 보여주는 LayerNorm 및 RMSNorm의 코드 구현.

```python
# LayerNorm
mean = x.mean(dim=-1, keepdim=True)
var = x.var(dim=-1, keepdim=True, unbiased=False)
x_norm = (x - mean) / torch.sqrt(var + eps)

# RMSNorm
rms = torch.sqrt(torch.mean(x**2, dim=-1, keepdim=True) + eps)
x_norm = x / rms
```

RMSNorm은 단순한 계산 구조에도 불구하고 LayerNorm과 유사한 안정화 효과를 제공하며, 특히 대규모 모델 훈련에서 중요한 자원 절약 효과를 가져옵니다.

#### 2.8 GPT-2의 유산과 LLM 학습의 기초

저는 여전히 GPT-2가 LLM에 대해 배우기 시작하는 이들에게 훌륭한 입문용 아키텍처라고 생각합니다. 이 모델은 복잡한 최적화 기법의 여러 계층에 갇히지 않아 이해하기에 충분히 단순하면서도, 현대 트랜스포머 모델이 어떻게 작동하는지에 대한 견고한 통찰력을 제공할 만큼 충분히 복잡합니다. GPT-2로 학습을 시작함으로써, 새로운 아키텍처에서 발견되는 추가 기능과 조정에 압도되지 않고 기본적인 원리(어텐션 메커니즘, 위치 임베딩, 정규화(normalization), 전반적인 훈련 파이프라인(training pipeline))에 집중할 수 있습니다. 실제로, 저는 새로운 변경 사항을 학습하기 전에 GPT-2에 대해 먼저 배우고 심지어 직접 구현해 보는 것이 시간을 투자할 가치가 있다고 생각합니다. 그렇게 하면 그러한 변경 사항들을 더 쉽게 이해할 수 있을 뿐만 아니라, 그들이 해결하려는 근본적인 한계나 문제가 무엇인지 더 깊이 이해하게 되어 해당 기술들을 더욱 높이 평가하게 될 것입니다.

예를 들어, 저는 저의 GPT-2 코드를 기반으로 최근 Qwen3 아키텍처를 처음부터 구현했으며, 이는 gpt-oss와 매우 유사한 구조를 가집니다. 이는 다음 주제로 이어집니다: gpt-oss를 더 최신 아키텍처와 비교하기.

Ahead of AI는 독자 지원 출판물입니다. 새로운 게시물을 받고 저의 작업을 지원하려면 무료 또는 유료 구독자가 되는 것을 고려해 보세요.
구독하기

### 3. gpt-oss를 최신 아키텍처(Qwen3)와 비교: 설계 철학의 차이점

이제 GPT-2에서 gpt-oss로의 진화 과정을 살펴보았으니, 다음 단계로 나아가 gpt-oss를 2025년 5월에 3개월 먼저 출시된 더 최신 아키텍처인 Qwen3와 비교해 볼 수 있습니다. 제가 여기서 Qwen3를 선택한 이유는 이 글을 쓰는 시점에서 Qwen3가 최고의 오픈 웨이트 모델 중 하나이기 때문입니다. 또한, Qwen3 MoE 모델 중 하나는 훈련 가능한 파라미터(trainable parameter) 측면에서 상대적으로 유사한 전체 크기를 가지고 있어 gpt-oss와 거의 직접적으로 비교할 수 있습니다. 아래 그림 13은 gpt-oss-20b와 유사한 크기의 Qwen3 모델을 비교합니다.

**그림 13**: 유사한 크기의 gpt-oss 및 Qwen3 모델의 병렬 비교.

보시다시피, gpt-oss 20B와 Qwen3 30B-A3B는 아키텍처 구성 요소에서 매우 유사합니다. 여기서 주요 차이점은 차원(dimension) 외에도 gpt-oss는 섹션 1.6에서 이전에 논의했듯이 슬라이딩 윈도우 어텐션(sliding window attention)을 사용하지만(이 그림에는 표시되지 않음), Qwen3는 그렇지 않다는 것입니다. 다음 소제목들에서 주목할 만한 세부 사항들을 하나씩 살펴보겠습니다.

#### 3.1 너비 대 깊이: LLM 스케일링의 복잡한 균형

두 모델을 자세히 살펴보면, Qwen3는 24개가 아닌 48개의 트랜스포머 블록(transformer block)을 가진 훨씬 더 깊은 아키텍처임을 알 수 있습니다(그림 14). 이러한 깊이 증가는 모델의 계층적 추상화 능력을 향상시킬 수 있습니다.

**그림 14**: Qwen3는 gpt-oss-20b보다 두 배 많은 트랜스포머 블록을 가지고 있습니다.

반면에 gpt-oss는 훨씬 더 넓은 아키텍처를 채택하고 있습니다.

*   2048이 아닌 2880의 임베딩 차원(embedding dimension)
*   또한 768이 아닌 2880의 중간 전문가(피드 포워드) 투영 차원(projection dimension)

gpt-oss가 두 배 많은 어텐션 헤드(attention head)를 사용한다는 점도 주목할 만하지만, 이것이 모델의 너비를 직접적으로 증가시키지는 않습니다. 너비는 주로 임베딩 차원에 의해 결정됩니다.

고정된 파라미터 수에서 한 접근 방식이 다른 접근 방식보다 이점을 제공할까요? 일반적으로, 더 깊은 모델은 더 많은 유연성을 가지며 복잡한 데이터 패턴을 학습하는 데 잠재적으로 유리하지만, 폭발 및 소실 기울기(exploding and vanishing gradient)와 같은 불안정성 문제 때문에 훈련하기가 더 어려울 수 있습니다(RMSNorm과 숏컷 연결(shortcut connection)이 이를 완화하는 것을 목표로 합니다). 반대로, 더 넓은 아키텍처는 더 높은 메모리 비용이 들지만, 더 나은 병렬화(parallelization) 덕분에 추론 시 더 빠르다는(더 높은 초당 토큰 처리량(tokens/second throughput)을 가짐) 장점이 있습니다.

모델링 성능에 관해서는, 불행히도 제가 아는 한 좋은 동등 비교(파라미터 수와 데이터셋이 일정하게 유지되는)는 거의 없습니다. 다만 Gemma 2 논문(표 9)의 절제 연구에서 90억 파라미터 아키텍처의 경우 더 넓은 설정이 더 깊은 설정보다 약간 더 낫다는 것을 발견했습니다. 4개의 벤치마크에서 더 넓은 모델은 평균 52.0점을 달성했고, 더 깊은 모델은 평균 50.8점을 달성했습니다. 이는 특정 작업이나 모델 크기에서는 너비가 깊이보다 미세한 우위를 점할 수 있음을 시사합니다. 그러나 이러한 차이는 모델의 특정 설계 목표와 훈련 데이터에 따라 달라질 수 있으며, 최적의 너비-깊이 비율은 여전히 활발히 연구되는 분야입니다.

#### 3.2 적은 수의 대규모 전문가 대 많은 수의 소규모 전문가: MoE 설계의 미묘한 차이

위 그림 14에서 볼 수 있듯이, gpt-oss가 놀랍도록 적은 수의 전문가(128개 대신 32개)를 가지고 있으며, 토큰당 8개가 아닌 4개의 활성 전문가만 사용한다는 점도 주목할 만합니다. 하지만 각 전문가는 Qwen3의 전문가보다 훨씬 더 큽니다. 이는 최근 추세와 발전이 더 많고 작은 모델이 유리하다는 점을 시사하기 때문에 흥미롭습니다. 이 변화는 총 파라미터 크기가 일정할 때, DeepSeekMoE 논문의 아래 그림 15에 잘 설명되어 있습니다.

**그림 15**: "DeepSeekMoE: Mixture-of-Experts 언어 모델에서 궁극적인 전문가 전문화를 향하여" 논문의 주석이 달린 그림, https://arxiv.org/abs/2401.06066

특히, DeepSeek의 모델과 달리 gpt-oss와 Qwen3 모두 공유 전문가(shared expert)를 사용하지 않습니다. 공정하게 말하면, gpt-oss의 적은 수의 전문가는 20B 크기의 부작용일 수 있습니다. 아래 120B 모드를 보면, 아래 그림 16에 나와 있듯이 다른 모든 것을 고정한 채 전문가 수(및 트랜스포머 블록)를 실제로 늘렸습니다.

**그림 16**: 두 gpt-oss 아키텍처의 병렬 비교. 더 큰 120B 모델은 트랜스포머 블록 수와 전문가 수만 스케일링(scaling)합니다.

20B와 120B 모델이 매우 유사하다는 지루한 설명은 아마도 120B 모델이 주요 초점이었기 때문일 것입니다. 그리고 더 작은 모델을 만드는 가장 쉬운 방법은 모델을 약간 더 짧게(더 적은 트랜스포머 블록) 만들고 전문가 수를 줄이는 것이었습니다. 대부분의 파라미터가 그곳에 있기 때문입니다. 하지만 그들이 120B 모델을 훈련하기 시작한 다음, 일부 트랜스포머 블록과 전문가를 잘라내어 계속적인 사전 훈련(pre-training)을 했는지(무작위 가중치에서 시작하는 대신) 추측해 볼 수도 있습니다. 어떤 경우든, 이 두 가지(트랜스포머 블록과 전문가 수)만 스케일링하는 것은 매우 이례적이기 때문입니다. 예를 들어, 여러 크기의 Qwen3 MoE 모델(아래 그림 17)을 보면, 훨씬 더 많은 측면에서 서로 더 비례적으로 스케일링되었습니다.

**그림 17**: 다양한 Qwen3 모델의 아키텍처 차이.

이러한 MoE 설계의 선택은 모델의 훈련 안정성, 추론 속도, 그리고 특정 작업에 대한 전문화 정도에 영향을 미칩니다. 많은 수의 작은 전문가를 사용하는 것이 개별 전문가의 전문화를 촉진하고 라우팅 유연성을 높일 수 있지만, 라우팅 오버헤드와 부하 불균형 문제를 야기할 수 있습니다. 반면, 적은 수의 큰 전문가를 사용하는 것은 각 전문가가 더 광범위한 지식을 처리하게 하여 라우팅을 단순화할 수 있지만, 전문화 정도가 낮아질 수 있습니다. gpt-oss의 선택은 아마도 특정 하드웨어 환경에서의 효율성과 성능 균형을 고려한 결과일 것입니다.

#### 3.3 어텐션 바이어스(Attention Bias) 및 어텐션 싱크(Attention Sinks): 미묘하지만 중요한 최적화

gpt-oss와 Qwen3 모두 그룹화된 쿼리 어텐션(grouped query attention)을 사용합니다. 주요 차이점은 gpt-oss가 이전에 언급했듯이 각 두 번째 계층에서 슬라이딩 윈도우 어텐션(sliding window attention)을 통해 컨텍스트 크기(context size)를 제한한다는 것입니다. 하지만 제 눈길을 끈 한 가지 흥미로운 세부 사항이 있습니다. 아래 그림에 나와 있듯이, gpt-oss는 어텐션 가중치(attention weight)에 바이어스 유닛(bias unit)을 사용하는 것으로 보입니다.

**그림 18**: gpt-oss 모델은 어텐션 계층에 바이어스 유닛을 사용합니다. 여기에서 코드 예시를 참조하십시오.

저는 GPT-2 시절 이후로 이러한 바이어스 유닛이 사용되는 것을 본 적이 없으며, 일반적으로 불필요하다고 여겨집니다. 실제로, 저는 이것이 적어도 키 변환(key transformation, k_proj)에 대해서는 수학적으로 사실임을 보여주는 최근 논문을 찾았습니다. 더욱이, 경험적 결과는 바이어스 유닛의 유무에 따른 차이가 거의 없음을 보여줍니다(아래 그림 19 참조). 이는 바이어스 유닛이 이론적으로 불필요하거나 그 영향이 미미할 수 있음을 시사합니다.

**그림 19**: https://arxiv.org/pdf/2302.08626에서 가져온 표로, 바이어스 유닛의 유무에 따라 모델을 처음부터 훈련했을 때의 평균 테스트 손실(test loss)을 보여줍니다.

여러분이 눈치챘을 또 다른 세부 사항은 그림 18의 코드 스크린샷에 있는 싱크(sinks)의 정의입니다. 일반적인 모델에서 어텐션 싱크(attention sink)는 어텐션을 안정화하기 위해 시퀀스 시작 부분에 배치되는 특별한 "항상 어텐션되는" 토큰으로, 긴 컨텍스트 시나리오에서 특히 유용합니다. 즉, 컨텍스트가 매우 길어지더라도 시작 부분의 이 특별한 어텐션되는 토큰은 여전히 어텐션되며, 전체 시퀀스에 대한 일반적으로 유용한 정보를 저장하는 방법을 학습할 수 있습니다. (원래 "Efficient Streaming Language Models with Attention Sinks" 논문에서 제안된 것으로 생각합니다.) gpt-oss 구현에서 어텐션 싱크는 입력 시퀀스(input sequence)의 실제 토큰이 아닙니다. 대신, 이들은 어텐션 점수(attention score)에 추가되는 학습된 헤드별 바이어스 로짓(per-head bias logit)입니다(그림 20). 목표는 위에서 언급한 어텐션 싱크와 동일하지만, 토큰화된 입력(tokenized input)을 수정하지 않는다는 점에서 차이가 있습니다. 이러한 방식은 입력 시퀀스에 인위적인 토큰을 추가하지 않고도 유사한 효과를 얻을 수 있어 더 유연한 구현이 가능합니다.

**그림 20**: gpt-oss에서 어텐션 싱크의 사용; 여기 Hugging Face 코드를 기반으로 합니다.

#### 3.4 라이선스: 오픈 웨이트(Open-weight)와 오픈 소스(Open-source)의 경계

마지막으로, Qwen3와 유사하게 gpt-oss 모델은 Apache 2.0 오픈 소스 라이선스(open-source license)를 따르며, 이는 매우 긍정적인 소식입니다(제가 개인적으로 제 오픈 소스 프로젝트에 선호하는 라이선스와 동일합니다). 이는 모델이 다른 모델로 증류(distill)되거나 상업용 제품에 제한 없이 사용될 수 있음을 의미합니다. 이러한 개방성은 LLM 생태계의 혁신을 가속화하고 다양한 응용 분야에서 활용될 수 있는 잠재력을 제공합니다.

오픈 웨이트(open-weight) 대 오픈 소스(open-source) LLM. 이 구분은 수년 동안 논의되어 왔지만, 이번 출시와 그 결과물에 대한 혼란을 피하기 위해 명확히 할 가치가 있습니다. 일부 모델 개발자들은 모델 가중치(model weight)와 추론 코드(inference code)만 공개하는 반면(예: Llama, Gemma, gpt-oss), 다른 개발자들(예: OLMo)은 훈련 코드(training code), 데이터셋, 가중치를 포함한 모든 것을 진정한 오픈 소스로 공개합니다. 이러한 엄격한 정의에 따르면, gpt-oss는 가중치와 추론 코드를 포함하지만 훈련 코드나 데이터셋은 포함하지 않으므로 오픈 웨이트 모델입니다(Qwen3와 마찬가지로). 하지만 이 용어는 업계 전반에 걸쳐 일관성 없이 사용되고 있습니다. 저는 "gpt-oss"의 "oss"가 오픈 소스 소프트웨어(open source software)를 의미한다고 가정했지만, OpenAI 자체가 공식 발표 글에서 gpt-oss를 오픈 웨이트 모델로 명확하게 설명한 것에 긍정적으로 놀랐습니다. 이러한 명확한 용어 사용은 커뮤니티의 혼란을 줄이고, 모델의 활용 범위와 개발자들의 기대치를 명확히 하는 데 기여합니다.

### 4. 기타 흥미로운 정보: LLM의 숨겨진 측면들

이전 섹션에서는 GPT-2 이후 아키텍처가 어떻게 발전했는지 설명하고 Qwen3(및 대부분의 다른 최신 모델)와의 유사점을 논의했지만, 아직 언급하지 않은 몇 가지 추가적이지만 주목할 만한 세부 사항들이 있습니다. 이들은 이전 섹션에 깔끔하게 들어맞지 않았지만 여전히 언급할 가치가 있는 점들입니다.

#### 4.1 훈련 개요: 미스터리 속의 LLM 학습 과정

불행히도, gpt-oss의 훈련 세트(training set) 크기와 알고리즘에 대한 구체적인 정보는 충분하지 않습니다. 아래에 모델 카드 보고서(1)와 발표 게시물(2)에서 가장 흥미로운 조각들을 추가했습니다.

*   gpt-oss 모델은 우리의 가장 진보된 사전 훈련(pre-training) 및 사후 훈련(post-training) 기술을 사용하여 훈련되었습니다 [...] (1)
*   [...] 완료하는 데 210만 H100-시간이 필요했으며, gpt-oss-20b는 거의 10배 적게 필요했습니다. (1)
*   [...] 지도 미세 조정(supervised fine-tuning) 단계와 고성능 RL(강화 학습) 단계(high-compute RL stage)를 포함합니다 [...] (2)
*   우리는 주로 영어 텍스트 전용 데이터셋에서 모델을 훈련했으며, STEM, 코딩 및 일반 지식에 중점을 두었습니다. (2)

따라서 gpt-oss 모델은 추론 모델(reasoning model)이라는 것을 알 수 있습니다. 210만 H100 GPU 시간의 훈련 계산량은 약 5.6배 더 큰 DeepSeek V3 모델이 훈련된 278.8만 H800 GPU 시간과 거의 비슷합니다. 불행히도 Qwen3의 훈련 시간에 대한 정보는 아직 없습니다. 흥미롭게도, gpt-oss 훈련 시간 추정치는 지시 따르기(instruction following)를 위한 지도 학습(supervised learning)과 추론을 위한 강화 학습(reinforcement learning)을 모두 포함하는 반면, DeepSeek V3는 DeepSeek R1이 별도로 훈련된 사전 훈련된 기본 모델(pre-trained base model)일 뿐입니다. 이는 gpt-oss가 단순히 언어 모델을 넘어 복잡한 지시를 이해하고 추론하는 능력에 초점을 맞추었음을 시사합니다. MoE 모델의 훈련은 특히 라우터의 효율적인 학습과 전문가 간의 부하 분산을 포함하여 추가적인 복잡성을 수반합니다. OpenAI가 이러한 과제를 어떻게 해결했는지에 대한 더 많은 세부 정보가 공개된다면 LLM 훈련 커뮤니티에 큰 도움이 될 것입니다.

#### 4.2 추론 노력(Inference Effort): 사용자 맞춤형 AI 응답 제어

이전 섹션에서 언급했듯이, gpt-oss 모델은 추론 모델입니다. 하지만 특히 흥미로운 점은 사용자가 추론 시간 스케일링(inference time scaling)을 통해 추론의 정도를 쉽게 제어할 수 있도록 훈련되었다는 것입니다. 구체적으로, gpt-oss 모델은 시스템 프롬프트(system prompt)의 일부로 "추론 노력: 낮음/중간/높음" 지시를 받을 수 있으며, 이는 그림 21에 나와 있듯이 응답 길이와 정확도에 직접적인 영향을 미칩니다.

**그림 21**: 다양한 추론 노력 하에서의 gpt-oss 모델의 응답 길이 및 품질 (모델 카드에서 가져온 주석이 달린 그림)

이러한 조정 가능성은 비용, 계산량, 정확도의 균형을 맞출 수 있게 해주므로 매우 유용합니다. 예를 들어, 간단한 지식 질문에 답하거나 작은 오타를 수정하는 것과 같이 작업이 간단하다면, 확장된 추론을 건너뛸 수 있습니다. 이는 불필요하게 긴 응답과 장황한 추론 흔적을 피하면서 시간과 자원을 절약합니다. 이는 마치 인간이 간단한 질문에는 즉각적으로 답하고, 복잡한 문제에는 더 많은 사고 과정을 거치는 것과 유사한 방식으로 AI의 응답을 제어할 수 있게 합니다.

Qwen3나 OLMo와 달리 OpenAI가 강화 학습 기반 추론 훈련 전에 기본 모델을 공개하지 않은 것은 다소 아쉽습니다. 기본 모델은 추론 방법론을 연구하는 연구자들에게 특히 귀중한 출발점입니다(제가 현재 Qwen3 Base로 작업하는 것을 좋아하는 이유 중 하나입니다). 제 생각에 OpenAI의 결정은 연구적 고려보다는 산업 및 생산 사용 사례에 의해 더 많이 좌우되었을 것입니다.

원래 Qwen3 모델에도 사고(추론) 모드를 활성화/비활성화하는 토글(toggle)이 있습니다(토크나이저(tokenizer)의 `enable_thinking=True/False` 설정으로, 추론 동작을 비활성화하기 위해 단순히 `<think></think>` 태그를 추가합니다). 하지만 Qwen3 팀은 지난 몇 주 동안 모델을 업데이트하여 하이브리드 모델에서 전용 Instruct/Thinking/Coder 변형으로 전환했습니다. 그 이유는 하이브리드 모드가 개별 모델에 비해 성능이 낮았기 때문입니다.

> 커뮤니티와 논의하고 이 문제를 숙고한 결과, 우리는 하이브리드 사고 모드를 포기하기로 결정했습니다. 이제 최상의 품질을 달성하기 위해 Instruct 및 Thinking 모델을 별도로 훈련할 것입니다.
> 출처

이는 LLM의 설계가 여전히 실험과 반복의 과정에 있으며, 단일 모델이 모든 시나리오에서 최적의 성능을 제공하기 어렵다는 점을 보여줍니다.

#### 4.3 MXFP4 최적화: 로컬 LLM의 새로운 지평을 열다

한 가지 흥미로운 놀라움은 OpenAI가 MoE 전문가를 위한 MXFP4 양자화(quantization) 방식을 사용하여 gpt-oss 모델을 출시했다는 것입니다. 양자화 형식은 주로 모바일 또는 임베디드 AI와 관련된 틈새 주제였지만, 더 큰 모델로의 전환과 함께 그 중요성이 증대되었습니다. 이 경우, MXFP4 최적화는 모델이 단일 GPU 장치에서 효율적으로 실행될 수 있도록 합니다.

실제 적용은 다음과 같습니다.

*   대규모 모델(120B)은 단일 80GB H100 또는 더 새로운 GPU에 들어맞습니다. 이는 일반 소비자 하드웨어는 아니지만, 여러 H100 머신을 사용하는 것보다 단일 H100 머신을 임대하는 것이 훨씬 저렴합니다. 게다가, 모델을 여러 GPU에 분산시키고 그에 따른 통신 오버헤드(communication overhead)를 걱정할 필요가 없습니다. AMD MI300X 카드도 첫날부터 지원된다는 점이 정말 좋습니다!
*   더 작은 20B 모델은 심지어 16GB VRAM에도 들어맞습니다. 단, MXFP4를 지원하려면 RTX 50 시리즈 GPU 또는 더 새로운 것이어야 합니다. (수정: RTX 4090과 같은 구형 카드에 대한 지원이 최근 패치(patch)를 통해 추가되었습니다.)
*   모델은 구형 하드웨어에서도 실행될 수 있지만, MXFP4 지원 없이 더 많은 RAM을 소비할 것입니다. MXFP4 최적화 없이 bfloat16 형식의 모델은 gpt-oss-20b의 경우 약 48GB, gpt-oss-120b의 경우 240GB를 소비할 것입니다.

그건 그렇고, 저는 ollama를 사용하여 제 Mac Mini에서 gpt-oss-20b 모델을 편안하게 실행할 수 있습니다. 약 13.5GB의 메모리를 사용하는데, 이는 정말 합리적입니다. MXFP4와 같은 고급 양자화 기술은 고성능 LLM을 더 많은 사용자가 접근할 수 있도록 하는 핵심적인 요소이며, 로컬 AI의 미래를 밝히는 중요한 진전입니다. 이는 개발자와 연구자들이 값비싼 클라우드 자원에 의존하지 않고도 최신 LLM을 실험하고 배포할 수 있는 기회를 제공합니다.

#### 4.4 벤치마크(Benchmark): 성능 평가의 복잡성과 실제 사용성

이 모델들은 독립적인 벤치마크를 수행하기에는 아직 너무 새로운 상태입니다. LM 아레나 리더보드(leaderboard)를 확인해 보니, gpt-oss는 아직 목록에 없습니다. 따라서 LM 아레나 사용자들에 따르면, Qwen3-Instruct가 현재까지 최고의 오픈 웨이트 모델로 남아 있습니다(그림 22).

**그림 22**: LM 아레나 리더보드의 현재 모습 (2025년 8월 8일 기준)

gpt-oss 발표 게시물에서 제공된 추론 벤치마크를 보면, gpt-oss 모델이 OpenAI의 독점 모델뿐만 아니라 Qwen3와도 동등한 수준임을 알 수 있습니다(그림 23).

**그림 23**: 주요 벤치마크 차트는 공식 gpt-oss 발표 게시물에서 가져왔습니다. "도구 없음(no tools)" gpt-oss-120b 데이터는 공식 모델 카드 논문에서 가져왔고, Qwen3 수치는 공식 Qwen3 저장소(repository)에서 가져왔습니다.

하지만 gpt-oss-120b가 Qwen3 A235B-A22B-Thinking-2507 모델의 거의 절반 크기이며 단일 GPU에서 실행될 수 있다는 사실로 인해 이 점은 주의해야 합니다. 이는 상당한 효율성 개선을 의미합니다.

하지만 벤치마크 성능이 항상 실제 사용성을 정확하게 반영하는 것은 아닙니다. 지난 며칠간의 제한적인 사용에서 저는 gpt-oss가 상당히 유능하다는 것을 발견했습니다. 그렇지만, 다른 사람들이 관찰했듯이, 환각(hallucinate) 현상이 비교적 높은 경향이 있는 것으로 보입니다(모델 카드에도 언급된 점입니다). 이는 수학, 퍼즐, 코드와 같은 추론 작업에 대한 집중적인 훈련 초점에서 비롯될 수 있으며, 이는 일부 "일반 지식 망각"으로 이어졌을 수 있습니다.

그럼에도 불구하고, gpt-oss는 도구 사용을 염두에 두고 설계되었기 때문에 이 한계는 시간이 지남에 따라 덜 중요해질 수 있습니다. 오픈 소스 LLM에서의 도구 통합은 아직 초기 단계이지만, 성숙해짐에 따라 사실 기반 또는 지식 기반 질문에 답할 때 모델이 외부 소스(검색 엔진과 같은)를 참조하도록 점점 더 많이 허용할 것으로 예상합니다. 그렇게 된다면, 암기보다는 추론 능력(reasoning capacity)을 우선시하는 것이 합리적일 수 있습니다. 이는 학교에서의 인간 학습(또는 일반적으로 삶)과 매우 유사합니다. 문제 해결 능력이 사실을 암기하는 것보다 더 중요한 경우가 많기 때문입니다. 따라서 벤치마크 점수 외에 모델의 실제 적용 가능성과 생태계 통합 능력을 함께 평가하는 것이 중요합니다.

### 5. gpt-oss와 GPT-5: 오픈 웨이트와 독점 모델의 공존

OpenAI는 매우 바쁜 한 주를 보냈고, gpt-oss 출시 직후 오랫동안 기다려온 GPT-5 모델을 공개했습니다. GPT-5 출시는 흥미로웠습니다. 그리고 여기서 한 가지 말해야 할 것이 있다면, 벤치마크 성능(그림 24) 면에서 그들의 오픈 소스 모델이 최고의 제품 제공 모델과 비교하여 얼마나 좋은지에 정말 놀랐다는 것입니다.

**그림 24**: 주요 벤치마크 차트는 공식 GPT-5 발표 게시물에서 가져왔습니다. gpt-oss 데이터는 공식 모델 카드 논문 및 발표 게시물에서 가져왔고, Qwen3 수치는 공식 Qwen3-Coder 저장소에서 가져왔습니다.

전반적으로, 일부 사람들이 이번 출시를 과대광고라고 불렀음에도 불구하고, 최고의 독점 모델에 크게 뒤지지 않는 정말 강력한 새로운 오픈 웨이트 모델 세트를 갖게 되어 기쁩니다. 물론, 벤치마크는 종종 실제 사용을 정확하게 반영하지 않으며, 제한된 사용량만으로는 아직 판단하기 이릅니다. 하지만 저는 오픈 웨이트 및 로컬(또는 개인 호스팅) 모델로 작업하는 것을 좋아하는 사람들에게는 좋은 시기라고 생각합니다. OpenAI가 gpt-oss를 통해 오픈 웨이트 모델 시장에 다시 진입한 것은 LLM 생태계 전반에 긍정적인 영향을 미칠 것으로 예상됩니다. 이는 다른 기업들이 더욱 강력하고 효율적인 오픈 웨이트 모델을 공개하도록 자극할 것이며, 궁극적으로는 개발자와 사용자 모두에게 더 넓은 선택의 폭과 혁신의 기회를 제공할 것입니다. 독점 모델과 오픈 웨이트 모델이 서로 경쟁하고 보완하며 발전하는 시대가 도래하고 있습니다.

이 잡지는 개인적인 열정 프로젝트이며, 여러분의 지원은 이를 유지하는 데 도움이 됩니다. 기여하고 싶으시다면 몇 가지 좋은 방법이 있습니다.

*   **제 책을 구입하세요**. "대규모 언어 모델 구축하기 (처음부터)"는 토크나이저(tokenizer)부터 훈련까지 LLM을 단계별로 구축하는 과정을 안내합니다.
*   **비디오 코스를 확인하세요**. 이제 Manning에서 제공하는 책 기반의 17시간 분량 비디오 코스가 있습니다. 이 코스는 책을 섹션별로 면밀히 따르며, 단독으로 또는 코딩 실습 자료로 모두 잘 작동합니다. 이 비디오 코스는 광고가 없으며(YouTube 버전과 달리) 더 깔끔하고 체계적인 형식을 가지고 있습니다. 또한 Abhinav Kimothi가 제작한 5시간 분량의 추가 선수 학습 비디오 자료도 포함되어 있습니다.
*   **구독하세요**. 유료 구독은 저의 글쓰기를 지속 가능하게 하고 추가 콘텐츠에 대한 접근을 제공합니다.

읽어주셔서 감사하며, 독립적인 연구를 지원해 주셔서 감사합니다!

Ahead of AI는 독자 지원 출판물입니다. 새로운 게시물을 받고 저의 작업을 지원하려면 무료 또는 유료 구독자가 되는 것을 고려해 보세요.
구독하기