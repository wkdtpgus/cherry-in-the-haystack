OpenAI는 금주에 gpt-oss-120b 및 gpt-oss-20b라는 신규 공개 가중치 대규모 언어 모델(LLM)을 세상에 내놓았습니다. 이는 2019년 GPT-2 발표 이래 처음으로 공개하는 공개 가중치 모델입니다. 몇몇 기발한 최적화(optimization) 덕분에 이 모델들은 개인 기기에서도 구동될 수 있습니다(자세한 내용은 추후 설명하겠습니다). OpenAI가 GPT-2 이후 대규모의 완전 공개 가중치 모델을 공유한 것은 이번이 선례 없는 일입니다. 초기 GPT 계열 모델들은 트랜스포머(transformer) 구조가 어떻게 규모에 따라 성능이 향상될 수 있는지를 명확히 보여주었습니다. 2022년 ChatGPT의 등장은 글쓰기, 지식 습득, 그리고 나중에는 코딩 작업에 대한 이 모델들의 구체적인 효용성을 증명하며 이 기술들을 대중화시켰습니다. 이제 그들은 오랫동안 기다려온 가중치 모델을 공유했으며, 그 내부 구조에는 몇 가지 흥미로운 설계 특징들이 담겨 있습니다. 저는 지난 며칠간 관련 코드와 기술 문서를 탐독하며 가장 주목할 만한 세부 사항들을 정리해보았습니다. (며칠 후, OpenAI는 GPT-5도 발표했는데, 이 글의 마지막 부분에서 gpt-oss 모델과 관련하여 간략하게 논의할 예정입니다.)

아래는 이 글에서 다룰 내용에 대한 간략한 개요입니다. 원활한 탐색을 위해 글 페이지 왼쪽에 있는 목차를 활용하시길 권장합니다.

*   GPT-2와 gpt-oss 모델의 구조적 대비
*   단일 GPU 환경에서 gpt-oss 모델을 운용하기 위한 MXFP4 효율 증진 기법
*   모델의 '폭'과 '깊이' 사이의 상충 관계(trade-off) 분석 (gpt-oss와 Qwen3 비교)
*   어텐션 바이어스(attention bias) 및 어텐션 싱크(sinks)의 역할
*   성능 측정 지표(benchmark) 및 GPT-5와의 비교 검토

이 정보가 여러분께 유익하기를 바랍니다!

### 1. 모델 구조 개괄

모델의 설계에 대해 더 심도 있게 논하기 전에, 아래 그림 1에 제시된 두 모델, gpt-oss-20b와 gpt-oss-120b의 일반적인 윤곽부터 살펴보겠습니다.

**그림 1**: 두 gpt-oss 모델의 병렬 비교.

만약 여러분이 최신 LLM 구조 다이어그램을 접한 경험이 있거나, 저의 이전 "대규모 아키텍처 비교(Big Architecture Comparison)" 글을 읽어보셨다면, 이 모델들이 첫눈에 특별히 새롭거나 이질적인 요소를 포함하고 있지 않다는 것을 알아차릴 것입니다.

**대규모 LLM 아키텍처 비교**
Sebastian Raschka, PhD · 7월 19일
전체 글 읽기

이는 사실 놀라운 일이 아닙니다. 주요 LLM 개발사들이 공통의 핵심 구조를 기반으로 미세한 조정(tweak)만을 가하는 경향이 있기 때문입니다. 이는 순전히 저의 추측이지만, 몇 가지 이유가 있다고 생각합니다. 첫째, 이들 연구 기관 간에 인력의 상당한 이동이 발생하고 있습니다. 이는 특정 설계 방식이나 혁신이 여러 조직으로 전파되는 자연스러운 경로를 제공합니다. 둘째, 우리는 아직 트랜스포머(transformer) 구조를 능가하는 대안을 발견하지 못했습니다. 상태 공간 모델(state space model)이나 텍스트 확산 모델(text diffusion model)과 같은 새로운 접근 방식들이 제안되고 있지만, 제가 아는 한 이 정도 규모에서 트랜스포머만큼의 성능을 입증한 사례는 아직 없습니다. (대부분의 비교 연구는 벤치마크(benchmark) 성능에만 초점을 맞추고 있으며, 모델이 실제 다중 턴(multi-turn) 글쓰기나 코딩 작업에서 얼마나 효과적인지는 아직 불분명합니다. 이 글을 쓰는 시점에서 LM 아레나(LM Arena)에서 가장 높은 순위의 비(非)트랜스포머 기반 모델은 트랜스포머-상태 공간 모델의 혼합형(hybrid)인 Jamba로, 96위입니다. **수정**: 어떤 분이 더 높은 순위의 하이브리드 모델인 Hunyuan-TurboS가 22위에 있다고 친절하게 알려주셨습니다.) 셋째, 대부분의 성능 향상은 근본적인 구조 변경보다는 데이터 처리 방식과 알고리즘(algorithm)의 미세 조정에서 비롯될 가능성이 높습니다.

그럼에도 불구하고, 이들의 설계 선택에는 여전히 많은 흥미로운 측면이 있습니다. 일부는 위 그림에 나타나 있지만(다른 것들은 그렇지 않으며, 나중에 논의할 것입니다), 이 글의 남은 부분에서는 이러한 특징들을 부각시키고 다른 구조들과 하나씩 대조해 볼 것입니다.

또한, 저는 OpenAI와 어떠한 형태로든 연관되어 있지 않다는 점을 명확히 밝힙니다. 저의 정보는 공개된 모델 코드 검토와 공식 기술 보고서 분석에서 얻은 것입니다.

이 모델들을 개인 환경에서 활용하는 방법을 배우고 싶다면, OpenAI의 공식 모델 허브(hub) 페이지부터 시작하는 것이 가장 좋습니다.

*   https://huggingface.co/openai/gpt-oss-20b
*   https://huggingface.co/openai/gpt-oss-120b

20B 모델은 최대 16GB 램을 갖춘 일반 사용자용 GPU에서도 실행될 수 있습니다. 120B 모델은 80GB 램을 지닌 단일 H100 또는 더 신형 하드웨어에서 운용 가능합니다. 몇 가지 중요한 유의사항이 있으므로 이 내용은 나중에 다시 다루겠습니다.

### 2. GPT-2로부터의 진화

gpt-oss와 더 최신 구조 간의 비교로 넘어가기 전에, 시간 여행을 통해 GPT-2(그림 2)를 나란히 살펴보며 그동안 얼마나 많은 진보가 이루어졌는지 확인해 봅시다.

**그림 2**: gpt-oss-20b와 GPT-2 XL 1.5B의 병렬 비교.

gpt-oss와 GPT-2는 모두 "Attention Is All You Need (2017)" 논문에서 제시된 트랜스포머(transformer) 구조를 기반으로 한 디코더 전용(decoder-only) LLM입니다. 수년에 걸쳐 많은 세부 사항들이 발전했습니다. 하지만 이러한 변화는 gpt-oss에만 국한된 것이 아닙니다. 그리고 나중에 보겠지만, 이들은 다른 많은 LLM에서도 공통적으로 나타납니다. 이전 "대규모 아키텍처 비교(Big Architecture Comparison)" 글에서 이러한 측면들을 많이 다루었으므로, 각 소제목을 간결하고 핵심적으로 유지하고자 노력할 것입니다.

#### 2.1 드롭아웃(Dropout)의 배제

신경망 훈련 시 과도한 데이터 적응을 막기 위해 계층의 활성화 값이나 어텐션 평가점(그림 3) 중 일부를 임의로 비활성화(즉, 0으로 설정)하는 드롭아웃(2012)은 과거부터 사용되던 일반적인 기법입니다. 그러나 현대 LLM에서는 드롭아웃의 사용 빈도가 현저히 낮아졌으며, GPT-2 이후 대부분의 모델들은 이를 제거했습니다(말장난 의도 없음).

**그림 3**: 어텐션 평가점 행렬(attention score matrix)에 드롭아웃이 적용된 예시.

저는 드롭아웃이 원래 트랜스포머 구조에서 계승되었기 때문에 GPT-2에서 사용되었다고 추정합니다. 연구자들은 드롭아웃이 LLM 성능을 실질적으로 향상시키지 못한다는 것을 깨달았을 것입니다(저 또한 소규모 GPT-2 재현 실험에서 동일한 현상을 관찰했습니다). 이는 LLM이 일반적으로 방대한 양의 데이터셋(dataset)에 대해 단일 에포크(epoch) 방식으로만 학습되기 때문일 가능성이 높습니다. 이는 드롭아웃이 처음 도입되었던 수백 에포크 훈련 방식과는 대조적입니다. 따라서 LLM은 훈련 과정에서 각 토큰(token)을 단 한 번만 접하기 때문에 지나친 학습(overfitting)의 위험이 거의 없습니다. 흥미롭게도, 드롭아웃이 수년 동안 LLM 구조 설계에서 다소 경시되어 왔지만, 2025년 소규모 LLM 실험(Pythia 1.4B) 논문에서는 이러한 단일 에포크 방식에서 드롭아웃이 하위 태스크(downstream task) 성능을 저하시킨다는 것을 확인했습니다. 이러한 결과는 드롭아웃의 유용성이 훈련 방식 및 모델 규모에 따라 크게 달라질 수 있음을 시사합니다.

#### 2.2 RoPE가 절대 위치 임베딩(Absolute Positional Embeddings)을 대체하다

트랜스포머 기반 LLM에서는 어텐션 메커니즘(attention mechanism)의 특성상 위치 인코딩(positional encoding)이 필수적입니다. 기본적으로 어텐션은 입력 토큰들을 순서가 없는 것처럼 처리하기 때문입니다. 원래 GPT 구조에서는 절대 위치 임베딩(absolute positional embedding)이 시퀀스(sequence)의 각 위치에 대해 학습된 임베딩 벡터(embedding vector)를 추가하여 이 문제를 해결했으며(그림 4), 이는 토큰 임베딩에 더해집니다.

**그림 4**: 절대 위치 임베딩의 예시.

회전 위치 임베딩(RoPE: Rotary Position Embedding)은 다른 접근 방식을 제시했습니다. 별도의 임베딩으로 위치 정보를 추가하는 대신, 각 토큰의 위치에 따라 쿼리(query) 및 키(key) 벡터를 회전시키는 방식으로 위치를 인코딩합니다. RoPE는 개념적으로 매우 우아한 아이디어이지만 설명하기 다소 까다로운 측면이 있습니다. 언젠가 이 주제에 대해 더 상세히 다룰 계획입니다. 2021년에 처음 소개되었으나, RoPE는 2023년 오리지널 라마(Llama) 모델 출시와 함께 광범위하게 수용되었으며 이후 현대 LLM의 필수 구성 요소로 자리 잡았습니다. 이는 위치 정보의 인코딩 방식이 모델의 성능과 효율성에 얼마나 중요한 영향을 미 미치는지를 보여주는 좋은 예시입니다.

#### 2.3 Swish/SwiGLU가 GELU를 대신하다

초기 GPT 구조는 가우시안 오차 선형 유닛(GELU)을 활용했습니다. 그렇다면 왜 이제 GELU 대신 Swish를 사용할까요? Swish (시그모이드 선형 유닛(sigmoid linear unit) 또는 SiLU로도 불림)는 연산 부담이 적다고 간주되며, 저는 이것이 주된 이유라고 생각합니다. 어떤 논문을 참조하느냐에 따라 모델링 성능 면에서 한쪽이 다른 쪽보다 약간 더 우수하다고 주장될 수 있겠지만, 제 견해로는 이러한 미미한 차이는 아마도 표준 오차(standard error) 범위 내에 있을 것이며, 하이퍼파라미터(hyperparameter) 민감도에 따라 결과가 달라질 것입니다.

활성화 함수(activation function)는 10여 년 전 딥러닝(deep learning) 커뮤니티가 ReLU에 대체로 안착하기 전까지 뜨거운 논쟁의 주제였습니다. 그 이후로 연구자들은 더 부드러운 곡선을 가진 많은 ReLU 유사 변형들을 제안하고 실험했으며, GELU와 Swish(그림 5)가 주요 선택지로 자리 잡았습니다.

**그림 5**: Swish와 GELU 활성화 함수의 비교. 둘 다 ReLU의 더 부드러운 버전입니다.

초기 GPT 구조는 `0.5x * [1 + erf(x / sqrt(2))]` 로 정의되는 GELU를 사용했습니다. 여기서 `erf`(오차 함수(error function)의 약자)는 가우시안(Gaussian)의 적분이며, 가우시안 적분의 다항식 근사(polynomial approximation)를 사용하여 계산됩니다. 이는 Swish에서 사용되는 시그모이드(sigmoid)와 같은 더 간단한 함수보다 연산 비용이 더 많이 듭니다. Swish는 단순히 `x * sigmoid(x)` 입니다. 실제로는 Swish가 GELU보다 계산적으로 약간 더 저렴하며, 이것이 대부분의 최신 모델에서 GELU를 대체한 주된 이유일 것입니다. 어떤 논문을 보느냐에 따라 모델링 성능 면에서 하나가 다소 더 나을 수 있습니다. 하지만 이러한 이득은 종종 표준 오차 범위 내에 있으며, 승자는 하이퍼파라미터 튜닝(hyperparameter tuning)에 크게 좌우될 것이라고 말하고 싶습니다. Swish는 오늘날 대부분의 구조에서 사용됩니다. 하지만 GELU가 완전히 잊혀진 것은 아닙니다. 예를 들어, Google의 Gemma 모델은 여전히 GELU를 사용합니다.

그러나 더 주목할 만한 점은 피드 포워드 모듈(feed forward module, 작은 다층 퍼셉트론(multi-layer perceptron))이 게이트형(gated) "GLU"로 대체되었다는 것입니다. 여기서 GLU는 게이트형 선형 유닛(gated linear unit)의 약자이며 2020년 논문에서 제안되었습니다. 구체적으로, 2개의 완전 연결 계층(fully connected layer)이 아래 그림 6과 같이 사용되는 3개의 완전 연결 계층으로 교체됩니다. 이는 정보 흐름을 더 세밀하게 제어하고 모델의 표현력을 증대시키는 역할을 합니다.

**그림 6**: Swish와 GELU, 그리고 그들의 게이트형 대응물인 SwiGLU와 GEGLU의 비교.

언뜻 보기에는 GEGLU/SwiGLU 변형이 추가 계층으로 인해 단순히 더 많은 파라미터(parameter)를 가지고 있기 때문에 일반적인 피드 포워드 계층보다 더 나을 수 있다고 보일 수 있습니다. 하지만 이것은 착각입니다. 실제로는 SwiGLU/GEGLU의 W 및 V 가중치 계층이 전통적인 피드 포워드 계층의 W_1 계층 크기의 절반으로 선택되는 경우가 많기 때문입니다. 이를 더 잘 설명하기 위해 일반 및 GLU 변형의 구체적인 코드 구현을 살펴보겠습니다.

**그림 7**: 일반 피드 포워드 모듈(상단)과 SwiGLU 변형(하단)이 나란히 있습니다. Swish 함수는 PyTorch에서 "silu"로 구현됩니다.

```python
# Regular feed forward module
fc1 = nn.Linear(embedding_dim, 4 * embedding_dim)
fc2 = nn.Linear(4 * embedding_dim, embedding_dim)

# SwiGLU variant
fc1_v = nn.Linear(embedding_dim, 2 * embedding_dim)
fc1_w = nn.Linear(embedding_dim, 2 * embedding_dim)
fc2 = nn.Linear(2 * embedding_dim, embedding_dim)
```

따라서 임베딩 차원(embedding dimension)이 1024라고 가정해 봅시다.

일반 피드 포워드(feed forward)의 경우, 다음과 같습니다.
`fc1: 1024 × 4096 = 4,194,304`
`fc2: 4096 × 1024 = 4,194,304`
즉, `fc1 + fc2 = 8,388,608`개의 파라미터입니다.

GLU 변형의 경우, 다음과 같습니다.
`fc1_v: 1024 × 2048 = 2,097,152`
`fc1_w: 1024 × 2048 = 2,097,152`
`fc2: 2048 × 1024 = 2,097,152`
즉, `3 × 2,097,152 = 6,291,456`개의 가중치 파라미터입니다.

따라서 전반적으로 GLU 변형을 사용하면 파라미터 수가 줄어들고 성능도 더 좋습니다. 이러한 더 나은 성능의 이유는 GLU 변형이 추가적인 곱셈 상호작용(multiplicative interaction)을 제공하여 표현력(expressivity)을 향상시키기 때문입니다(이는 잘 훈련되었을 때 깊고 얇은 신경망(neural net)이 얕고 넓은 신경망보다 더 나은 성능을 보이는 것과 같은 이유입니다).

#### 2.4 전문가 혼합(Mixture-of-Experts)이 단일 피드 포워드 모듈을 대신하다

이전 섹션에서 논의했듯이, 피드 포워드 모듈을 SwiGLU로 업그레이드하는 것 외에도, gpt-oss는 단일 피드 포워드 모듈을 여러 피드 포워드 모듈로 교체하며, 각 토큰 생성 단계에서 부분집합만 사용합니다. 이 접근 방식은 MoE(Mixture-of-Experts, 전문가 혼합)로 알려져 있으며 아래 그림 8에 나와 있습니다.

**그림 8**: 피드 포워드 모듈이 MoE(Mixture-of-Expert) 모듈로 대체됩니다.

따라서 단일 피드 포워드 모듈을 여러 피드 포워드 모듈로 대체하면(MoE 설정에서와 같이) 모델의 총 파라미터 수가 상당히 늘어납니다. 하지만 핵심 비결은 모든 토큰에 대해 모든 전문가를 사용("활성화")하지 않는다는 것입니다. 대신, 라우터(router)는 토큰당 소수의 전문가 부분집합만을 선택합니다. 한 번에 소수의 전문가만 활성화되기 때문에, MoE 모듈은 항상 전체 파라미터 세트(parameter set)를 사용하는 밀집(dense) 모듈과 대조적으로 희소(sparse)하다고 불립니다. 이러한 희소성은 추론(inference) 효율성을 보존하면서도 모델의 용량(capacity)을 크게 확장할 수 있게 합니다. 이는 훈련 중에 모델이 더 많은 지식을 습득할 수 있음을 의미합니다. (재미있는 사실: 대부분의 MoE 모델에서 전문가 가중치(expert weight)는 전체 모델 파라미터의 90% 이상을 차지합니다.)

#### 2.5 그룹화된 쿼리 어텐션(Grouped Query Attention)이 다중 헤드 어텐션(Multi-Head Attention)을 대신하다

이전 글에서 언급했듯이, GQA(Grouped Query Attention, 그룹화된 쿼리 어텐션)는 최근 몇 년 동안 MHA(Multi-Head Attention, 다중 헤드 어텐션)에 비해 더 연산 및 파라미터 효율적인 대안으로 부상했습니다. MHA에서는 각 헤드(head)가 자체적인 키(key)와 값(value) 세트를 가집니다. GQA는 여러 헤드를 그룹화하여 동일한 키 및 값 투영(projection)을 공유함으로써 메모리 사용량을 줄입니다. 예를 들어, 그림 9에 나와 있듯이, 2개의 키-값 그룹과 4개의 어텐션 헤드가 있다면, 헤드 1과 2는 하나의 키 및 값 세트를 공유하고, 헤드 3과 4는 다른 세트를 공유할 수 있습니다. 절제 연구(ablation study)에 따르면, 이러한 그룹화는 키 및 값 계산의 총량을 감소시켜 메모리 사용량을 줄이고 효율성을 향상시키면서 모델링 성능에 눈에 띄는 영향을 미치지 않습니다.

**그림 9**: MHA와 GQA의 비교. 여기서 그룹 크기는 2이며, 키와 값 쌍이 2개의 쿼리(query) 간에 공유됩니다.

따라서 GQA의 핵심 아이디어는 여러 쿼리 헤드에 걸쳐 키와 값 헤드를 공유함으로써 그 수를 줄이는 것입니다. 이는 (1) 모델의 파라미터 수를 낮추고 (2) 추론 시 KV 캐시(KV cache)에서 저장하고 검색해야 하는 키와 값이 적기 때문에 키 및 값 텐서(tensor)에 대한 메모리 대역폭(bandwidth) 사용량을 줄입니다. (GQA가 코드에서 어떻게 보이는지 궁금하다면, KV 캐시가 없는 버전에 대한 저의 GPT-2에서 Llama 3 변환 가이드와 여기에서 KV 캐시 변형을 참조하십시오.) GQA는 주로 MHA에 대한 연산 효율성 해결책이지만, 절제 연구(원래 GQA 논문 및 Llama 2 논문의 연구와 같은)는 LLM 모델링 성능 면에서 표준 MHA와 유사하게 작동함을 보여줍니다.

#### 2.6 슬라이딩 윈도우 어텐션(Sliding Window Attention)

슬라이딩 윈도우 어텐션(Sliding-window attention, 아래 그림 10)은 LongFormer 논문(2020)에서 처음 소개되었고 나중에 Mistral에 의해 대중화되었습니다. 흥미롭게도, gpt-oss는 이를 매 두 번째 계층에 적용합니다. 이를 다중 헤드 어텐션(multi-head attention)의 변형, 또는 이 경우 그룹화된 쿼리 어텐션(GQA)의 변형으로 생각할 수 있습니다. 여기서 어텐션 컨텍스트(attention context)는 더 작은 윈도우(window)로 제한되어 메모리 사용량과 연산 비용을 모두 줄입니다.

**그림 10**: 일반 어텐션(왼쪽)과 슬라이딩 윈도우 어텐션(오른쪽)의 비교.

구체적으로, gpt-oss는 전체 컨텍스트(context)에 어텐션하는 GQA 계층과 128개 토큰으로 제한된 슬라이딩 윈도우를 가진 GQA 계층을 번갈아 사용합니다. 이전 글에서 논의했듯이, Gemma 2 (2024)는 유사한 1:1 비율을 사용했습니다. 올해 초 Gemma 3는 훨씬 더 나아가 5:1 비율로 전환했습니다. 이는 5개의 슬라이딩 윈도우(로컬) 어텐션 계층마다 하나의 전체 어텐션 계층만 사용한다는 의미입니다. Gemma 절제 연구에 따르면, 슬라이딩 윈도우 어텐션은 아래 그림에 나와 있듯이 모델링 성능에 미치는 영향이 미미합니다. Gemma 2의 윈도우 크기는 4096개 토큰이었고, Gemma 3는 이를 1024개로 줄였습니다. gpt-oss에서는 윈도우가 단 128개 토큰으로, 놀랍도록 작습니다.

그리고 재미있는 사실로, 공식 발표 글에서는 슬라이딩 윈도우 어텐션이 GPT-3에서 이미 사용되었던 것으로 보인다고 언급합니다.

> 이 모델들은 GPT-3와 유사하게 밀집(dense) 및 지역적으로 밴드형(banded) 희소(sparse) 어텐션 패턴을 번갈아 사용합니다.

누가 알았겠어요!? 저는 원래 GPT-3 논문으로 돌아갔고, 실제로 거기에 언급되어 있었습니다.

> 우리는 GPT-2 [RWC+19]와 동일한 모델 및 아키텍처를 사용하며, 여기에 설명된 수정된 초기화(initialization), 사전 정규화(pre-normalization) 및 가역 토큰화(reversible tokenization)를 포함합니다. 단, 트랜스포머 계층에서 Sparse Transformer [CGRS19]와 유사하게 밀집 및 지역적으로 밴드형 희소 어텐션 패턴을 번갈아 사용한다는 점은 예외입니다.

#### 2.7 RMSNorm이 LayerNorm을 대신하다

마지막으로, GPT-2에서 비롯된 마지막 작은 조정은 최근 몇 년간 흔한 추세였던 LayerNorm (2016)을 RMSNorm (2019)으로 대체하는 것입니다. GELU를 Swish 및 SwiGLU로 교체하는 것과 마찬가지로, RMSNorm은 이러한 작지만 합리적인 효율성 개선 중 하나입니다. RMSNorm은 아래 그림 11에 나와 있듯이 계층 활성화(layer activation)를 정규화(normalize)하는 목적에서 LayerNorm과 유사합니다. 그리 오래 전이 아니더라도, BatchNorm이 이 작업에 가장 많이 사용되는 선택지였다는 것을 기억할 것입니다. 이후 BatchNorm은 효율적으로 병렬화(parallelize)하기 어렵고(평균 및 분산 배치 통계(mean and variance batch statistics) 때문에) 작은 배치 크기(batch size)에서는 성능이 좋지 않아 선호도가 떨어졌습니다.

**그림 11**: 작은 선형 계층(linear layer)에 대한 LayerNorm(왼쪽)과 RMSNorm(오른쪽)의 비교.

위 그림 11에서 볼 수 있듯이, LayerNorm과 RMSNorm 모두 계층 출력(layer output)을 합리적인 범위 내로 스케일링(scale)합니다. LayerNorm은 평균을 빼고 표준 편차(standard deviation)로 나누어 계층 출력이 평균 0과 단위 분산(unit variance, 분산 1, 표준 편차 1)을 갖도록 합니다. RMSNorm은 입력을 제곱평균제곱근(root-mean-square)으로 나눕니다. 이는 활성화(activation)를 유사한 크기로 스케일링하면서 평균 0 또는 단위 분산을 강제하지 않습니다. 그림 11에 나와 있는 이 특정 예시에서 평균은 0.77이고 분산은 0.41입니다.

LayerNorm과 RMSNorm 모두 활성화 스케일(activation scale)을 안정화하고 최적화(optimization)를 개선하지만, RMSNorm은 연산 부담이 적기 때문에 대규모 LLM에서 종종 선호됩니다. LayerNorm과 달리 RMSNorm은 바이어스(bias, 이동) 항이 없으며, 비용이 많이 드는 평균 및 분산 계산을 단일 제곱평균제곱근 연산으로 줄입니다. 이는 교차 특징 감소(cross-feature reduction) 횟수를 2개에서 1개로 줄여 GPU의 통신 오버헤드(communication overhead)를 낮추고 훈련 효율성을 향상시킵니다. 그림 12는 이것이 코드에서 어떻게 보이는지 보여줍니다.

**그림 12**: RMSNorm이 계산적으로 더 간단함을 보여주는 LayerNorm 및 RMSNorm의 코드 구현.

```python
# LayerNorm
mean = x.mean(dim=-1, keepdim=True)
var = x.var(dim=-1, keepdim=True, unbiased=False)
x_norm = (x - mean) / torch.sqrt(var + eps)

# RMSNorm
rms = torch.sqrt(torch.mean(x**2, dim=-1, keepdim=True) + eps)
x_norm = x / rms
```

#### 2.8 GPT-2의 유산

저는 여전히 GPT-2가 LLM에 대해 배울 때 훌륭한 초보자용 아키텍처라고 생각합니다. 최적화 트릭의 여러 계층에 갇히지 않고 이해하기에 충분히 간단하지만, 현대 트랜스포머 모델이 어떻게 작동하는지에 대한 확실한 이해를 제공할 만큼 여전히 복잡합니다. GPT-2로 시작함으로써, 새로운 아키텍처에서 발견되는 추가 기능과 조정에 압도되지 않고 기본(어텐션 메커니즘, 위치 임베딩, 정규화(normalization), 전반적인 훈련 파이프라인(training pipeline))에 집중할 수 있습니다. 사실, 저는 새로운 변경 사항을 추가하기 전에 GPT-2에 대해 먼저 배우고 심지어 구현하는 것이 시간을 들일 가치가 있다고 생각합니다. 그렇게 하면 그러한 변경 사항을 더 쉽게 이해할 수 있을 뿐만 아니라, 그들이 해결하려는 한계나 문제가 무엇인지 더 잘 이해하게 되므로 더 높이 평가하게 될 것입니다.

예를 들어, 저의 GPT-2 코드를 시작으로 최근 Qwen3 아키텍처를 처음부터 구현했는데, 이는 gpt-oss와 매우 유사합니다. 이는 다음 주제로 이어집니다: gpt-oss를 더 최신 구조와 비교하기.

Ahead of AI는 독자 지원 출판물입니다. 새로운 게시물을 받고 저의 작업을 지원하려면 무료 또는 유료 구독자가 되는 것을 고려해 보세요.
구독하기

### 3. gpt-oss를 최신 구조(Qwen3)와 비교하다

이제 GPT-2에서 gpt-oss로의 진화를 살펴보았으니, 다음 단계로 나아가 gpt-oss를 2025년 5월에 3개월 먼저 출시된 더 최신 아키텍처인 Qwen3와 비교할 수 있습니다. 제가 여기서 Qwen3를 선택하는 이유는 이 글을 쓰는 시점에서 Qwen3가 최고의 오픈 웨이트 모델 중 하나이기 때문입니다. 또한, Qwen3 MoE 모델 중 하나는 훈련 가능한 파라미터(trainable parameter) 측면에서 상대적으로 유사한 전체 크기를 가지고 있어 gpt-oss와 거의 직접적으로 비교할 수 있습니다. 아래 그림 13은 gpt-oss-20b와 유사한 크기의 Qwen3 모델을 비교합니다.

**그림 13**: 유사한 크기의 gpt-oss 및 Qwen3 모델의 병렬 비교.

보시다시피, gpt-oss 20B와 Qwen3 30B-A3B는 아키텍처 구성 요소에서 매우 유사합니다. 여기서 주요 차이점은 차원(dimension) 외에도 gpt-oss는 섹션 1.6에서 이전에 논의했듯이 슬라이딩 윈도우 어텐션(sliding window attention)을 사용하지만(이 그림에는 표시되지 않음), Qwen3는 그렇지 않다는 것입니다. 다음 소제목들에서 주목할 만한 세부 사항들을 하나씩 살펴보겠습니다.

#### 3.1 모델의 폭과 깊이

두 모델을 자세히 살펴보면, Qwen3는 24개가 아닌 48개의 트랜스포머 블록(transformer block)을 가진 훨씬 더 깊은 구조임을 알 수 있습니다(그림 14). 이러한 깊이는 모델이 더 복잡한 특징 계층을 학습하고 추상적인 관계를 파악하는 데 유리할 수 있습니다.

**그림 14**: Qwen3는 gpt-oss-20b보다 두 배 많은 트랜스포머 블록을 가지고 있습니다.

반면에 gpt-oss는 훨씬 더 넓은 구조를 채택했습니다. 이는 주로 다음과 같은 특징에서 드러납니다.

*   2048이 아닌 2880의 임베딩 차원(embedding dimension)
*   또한 768이 아닌 2880의 중간 전문가(피드 포워드) 투영 차원(projection dimension)

gpt-oss가 두 배 많은 어텐션 헤드(attention head)를 사용한다는 점도 주목할 만하지만, 이것이 모델의 너비를 직접적으로 증가시키지는 않습니다. 너비는 임베딩 차원에 의해 결정됩니다.

고정된 파라미터 수에서 한 접근 방식이 다른 접근 방식보다 이점을 제공할까요? 일반적으로, 더 깊은 모델은 더 많은 유연성을 가지지만, 폭발 및 소실 기울기(exploding and vanishing gradient)로 인한 불안정성 문제 때문에 훈련하기 더 어려울 수 있습니다(RMSNorm과 숏컷 연결(shortcut connection)이 이를 완화하는 것을 목표로 합니다). 더 넓은 구조는 더 높은 메모리 비용으로 더 나은 병렬화(parallelization) 덕분에 추론 시 더 빠르다는(더 높은 초당 토큰 처리량(tokens/second throughput)을 가짐) 장점이 있습니다.

모델링 성능에 관해서는, 불행히도 제가 아는 한 좋은 동등 비교(parameter size와 데이터셋이 일정하게 유지되는)는 없습니다. 다만 Gemma 2 논문(표 9)의 절제 연구에서 90억 파라미터 아키텍처의 경우 더 넓은 설정이 더 깊은 설정보다 약간 더 낫다는 것을 발견했습니다. 4개의 벤치마크에서 더 넓은 모델은 평균 52.0점을 달성했고, 더 깊은 모델은 평균 50.8점을 달성했습니다. 이는 특정 조건 하에서는 너비가 깊이보다 미세하게 유리할 수 있음을 시사합니다.

#### 3.2 소수의 대규모 전문가 vs. 다수의 소규모 전문가

위 그림 14에서 볼 수 있듯이, gpt-oss가 놀랍도록 적은 수의 전문가(128개 대신 32개)를 가지고 있으며, 토큰당 8개가 아닌 4개의 활성 전문가만 사용한다는 점도 주목할 만합니다. 하지만 각 전문가는 Qwen3의 전문가보다 훨씬 더 큽니다. 이는 최근 추세와 발전이 더 많고 작은 모델이 유리하다는 점을 시사하기 때문에 흥미롭습니다. 이 변화는 총 파라미터 크기가 일정할 때, DeepSeekMoE 논문의 아래 그림 15에 잘 설명되어 있습니다.

**그림 15**: "DeepSeekMoE: Mixture-of-Experts 언어 모델에서 궁극적인 전문가 전문화를 향하여" 논문의 주석이 달린 그림, https://arxiv.org/abs/2401.06066

특히, DeepSeek의 모델과 달리 gpt-oss와 Qwen3 모두 공유 전문가(shared expert)를 사용하지 않습니다. 공정하게 말하면, gpt-oss의 적은 수의 전문가는 20B 크기의 부작용일 수 있습니다. 아래 120B 모드를 보면, 아래 그림 16에 나와 있듯이 다른 모든 것을 고정한 채 전문가 수(및 트랜스포머 블록)를 실제로 늘렸습니다.

**그림 16**: 두 gpt-oss 아키텍처의 병렬 비교. 더 큰 120B 모델은 트랜스포머 블록 수와 전문가 수만 스케일링(scaling)합니다.

20B와 120B 모델이 매우 유사하다는 지루한 설명은 아마도 120B 모델이 주요 초점이었기 때문일 것입니다. 그리고 더 작은 모델을 만드는 가장 쉬운 방법은 모델을 약간 더 짧게(더 적은 트랜스포머 블록) 만들고 전문가 수를 줄이는 것이었습니다. 대부분의 파라미터가 그곳에 있기 때문입니다. 하지만 그들이 120B 모델을 훈련하기 시작한 다음, 일부 트랜스포머 블록과 전문가를 잘라내어 계속적인 사전 훈련(pre-training)을 했는지(무작위 가중치에서 시작하는 대신) 추측해 볼 수도 있습니다. 어떤 경우든, 이 두 가지(트랜스포머 블록과 전문가 수)만 스케일링하는 것은 매우 이례적이기 때문입니다. 예를 들어, 여러 크기의 Qwen3 MoE 모델(아래 그림 17)을 보면, 훨씬 더 많은 측면에서 서로 더 비례적으로 스케일링되었습니다.

**그림 17**: 다양한 Qwen3 모델의 아키텍처 차이.

#### 3.3 어텐션 바이어스(Attention Bias) 및 어텐션 싱크(Attention Sinks)

gpt-oss와 Qwen3 모두 그룹화된 쿼리 어텐션(grouped query attention)을 사용합니다. 주요 차이점은 gpt-oss가 이전에 언급했듯이 각 두 번째 계층에서 슬라이딩 윈도우 어텐션(sliding window attention)을 통해 컨텍스트 크기(context size)를 제한한다는 것입니다. 하지만 제 눈길을 끈 한 가지 흥미로운 세부 사항이 있습니다. 아래 그림에 나와 있듯이, gpt-oss는 어텐션 가중치(attention weight)에 바이어스 유닛(bias unit)을 사용하는 것으로 보입니다.

**그림 18**: gpt-oss 모델은 어텐션 계층에 바이어스 유닛을 사용합니다. 여기에서 코드 예시를 참조하십시오.

저는 GPT-2 시절 이후로 이러한 바이어스 유닛이 사용되는 것을 본 적이 없으며, 일반적으로 불필요하다고 여겨집니다. 실제로, 저는 이것이 적어도 키 변환(key transformation, k_proj)에 대해서는 수학적으로 사실임을 보여주는 최근 논문을 찾았습니다. 더욱이, 경험적 결과는 바이어스 유닛의 유무에 따른 차이가 거의 없음을 보여줍니다(아래 그림 19 참조).

**그림 19**: https://arxiv.org/pdf/2302.08626에서 가져온 표로, 바이어스 유닛의 유무에 따라 모델을 처음부터 훈련했을 때의 평균 테스트 손실(test loss)을 보여줍니다.

여러분들이 눈치챘을 또 다른 세부 사항은 그림 18의 코드 스크린샷에 있는 싱크(sinks)의 정의입니다. 일반적인 모델에서 어텐션 싱크(attention sink)는 어텐션을 안정화하기 위해 시퀀스 시작 부분에 배치되는 특별한 "항상 어텐션되는" 토큰으로, 긴 컨텍스트 시나리오에서 특히 유용합니다. 즉, 컨텍스트가 매우 길어지더라도 시작 부분의 이 특별한 어텐션되는 토큰은 여전히 어텐션되며, 전체 시퀀스에 대한 일반적으로 유용한 정보를 저장하는 방법을 학습할 수 있습니다. (원래 "Efficient Streaming Language Models with Attention Sinks" 논문에서 제안된 것으로 생각합니다.) gpt-oss 구현에서 어텐션 싱크는 입력 시퀀스(input sequence)의 실제 토큰이 아닙니다. 대신, 이들은 어텐션 점수(attention score)에 추가되는 학습된 헤드별 바이어스 로짓(per-head bias logit)입니다(그림 20). 목표는 위에서 언급한 어텐션 싱크와 동일하지만, 토큰화된 입력(tokenized input)을 수정하지 않습니다.

**그림 20**: gpt-oss에서 어텐션 싱크의 사용; 여기 Hugging Face 코드를 기반으로 합니다.

#### 3.4 라이선스

마지막으로, Qwen3와 유사하게 gpt-oss 모델은 Apache 2.0 오픈 소스 라이선스(open-source license)를 따르며, 이는 훌륭합니다(제가 제 오픈 소스 프로젝트에 선호하는 라이선스와 동일합니다). 이는 모델이 다른 모델로 증류(distill)되거나 상업용 제품에 제한 없이 사용될 수 있음을 의미합니다.

오픈 웨이트(open-weight) 대 오픈 소스(open-source) LLM. 이 구분은 수년 동안 논의되어 왔지만, 이번 출시와 그 결과물에 대한 혼란을 피하기 위해 명확히 할 가치가 있습니다. 일부 모델 개발자들은 모델 가중치(model weight)와 추론 코드(inference code)만 공개하는 반면(예: Llama, Gemma, gpt-oss), 다른 개발자들(예: OLMo)은 훈련 코드(training code), 데이터셋, 가중치를 포함한 모든 것을 진정한 오픈 소스로 공개합니다. 이러한 엄격한 정의에 따르면, gpt-oss는 가중치와 추론 코드를 포함하지만 훈련 코드나 데이터셋은 포함하지 않으므로 오픈 웨이트 모델입니다(Qwen3와 마찬가지로). 하지만 이 용어는 업계 전반에 걸쳐 일관성 없이 사용됩니다. 저는 "gpt-oss"의 "oss"가 오픈 소스 소프트웨어(open source software)를 의미한다고 가정하지만, OpenAI 자체가 공식 발표 글에서 gpt-oss를 오픈 웨이트 모델로 명확하게 설명한 것에 긍정적으로 놀랐습니다.

### 4. 기타 흥미로운 정보

이전 섹션에서는 GPT-2 이후 구조가 어떻게 발전했는지 설명하고 Qwen3(및 대부분의 다른 최신 모델)와의 유사점을 논의했지만, 아직 언급하지 않은 몇 가지 추가적이지만 주목할 만한 세부 사항들이 있습니다. 이들은 이전 섹션에 깔끔하게 들어맞지 않았지만 여전히 언급할 가치가 있는 점들입니다.

#### 4.1 훈련 개요

불행히도, 훈련 세트(training set) 크기와 알고리즘에 대한 정보는 많지 않습니다. 아래에 모델 카드 보고서(1)와 발표 게시물(2)에서 가장 흥미로운 조각들을 추가했습니다.

*   gpt-oss 모델은 우리의 가장 진보된 사전 훈련(pre-training) 및 사후 훈련(post-training) 기술을 사용하여 훈련되었습니다 [...] (1)
*   [...] 완료하는 데 210만 H100-시간이 필요했으며, gpt-oss-20b는 거의 10배 적게 필요했습니다. (1)
*   [...] 지도 미세 조정(supervised fine-tuning) 단계와 고성능 RL(강화 학습) 단계(high-compute RL stage)를 포함합니다 [...] (2)
*   우리는 주로 영어 텍스트 전용 데이터셋에서 모델을 훈련했으며, STEM, 코딩 및 일반 지식에 중점을 두었습니다. (2)

따라서 gpt-oss 모델은 추론 모델(reasoning model)이라는 것을 알 수 있습니다. 210만 H100 GPU 시간의 훈련 계산량은 약 5.6배 더 큰 DeepSeek V3 모델이 훈련된 278.8만 H800 GPU 시간과 거의 비슷합니다. 불행히도 Qwen3의 훈련 시간에 대한 정보는 아직 없습니다. 흥미롭게도, GPT-oss 훈련 시간 추정치는 지시 따르기(instruction following)를 위한 지도 학습(supervised learning)과 추론을 위한 강화 학습(reinforcement learning)을 모두 포함하는 반면, DeepSeek V3는 DeepSeek R1이 별도로 훈련된 사전 훈련된 기본 모델(pre-trained base model)일 뿐입니다. 이는 모델의 최종 활용 목적에 따라 훈련 방식이 어떻게 달라지는지를 명확히 보여줍니다.

#### 4.2 추론 노력

이전 섹션에서 언급했듯이, gpt-oss 모델은 추론 모델입니다. 하지만 특히 흥미로운 점은 사용자가 추론 시간 스케일링(inference time scaling)을 통해 추론의 정도를 쉽게 제어할 수 있도록 훈련되었다는 것입니다. 구체적으로, gpt-oss 모델은 시스템 프롬프트(system prompt)의 일부로 "추론 노력: 낮음/중간/높음" 지시를 받을 수 있으며, 이는 그림 21에 나와 있듯이 응답 길이와 정확도에 직접적인 영향을 미칩니다.

**그림 21**: 다양한 추론 노력 하에서의 gpt-oss 모델의 응답 길이 및 품질 (모델 카드에서 가져온 주석이 달린 그림)

이러한 조정 가능성은 비용, 계산량, 정확도의 균형을 맞출 수 있게 해주므로 유용합니다. 예를 들어, 간단한 지식 질문에 답하거나 작은 오타를 수정하는 것과 같이 작업이 간단하다면, 확장된 추론을 건너뛸 수 있습니다. 이는 불필요하게 긴 응답과 장황한 추론 흔적을 피하면서 시간과 자원을 절약합니다.

Qwen3나 OLMo와 달리 OpenAI가 강화 학습 기반 추론 훈련 전에 기본 모델을 공개하지 않은 것은 다소 아쉽습니다. 기본 모델은 추론 방법론을 연구하는 연구자들에게 특히 귀중한 출발점입니다(제가 현재 Qwen3 Base로 작업하는 것을 좋아하는 이유 중 하나입니다). 제 생각에 OpenAI의 결정은 연구적 고려보다는 산업 및 생산 사용 사례에 의해 더 많이 좌우되었을 것입니다.

원래 Qwen3 모델에도 사고(추론) 모드를 활성화/비활성화하는 토글(toggle)이 있습니다(토크나이저(tokenizer)의 `enable_thinking=True/False` 설정으로, 추론 동작을 비활성화하기 위해 단순히 `<think></think>` 태그를 추가합니다). 하지만 Qwen3 팀은 지난 몇 주 동안 모델을 업데이트하여 하이브리드 모델에서 전용 Instruct/Thinking/Coder 변형으로 전환했습니다. 그 이유는 하이브리드 모드가 개별 모델에 비해 성능이 낮았기 때문입니다.

> 커뮤니티와 논의하고 이 문제를 숙고한 결과, 우리는 하이브리드 사고 모드를 포기하기로 결정했습니다. 이제 최상의 품질을 달성하기 위해 Instruct 및 Thinking 모델을 별도로 훈련할 것입니다.
> 출처

#### 4.3 MXFP4 최적화: 작지만 중요한 세부 사항

한 가지 흥미로운 놀라움은 OpenAI가 MoE 전문가를 위한 MXFP4 양자화(quantization) 방식을 사용하여 gpt-oss 모델을 출시했다는 것입니다. 양자화 형식은 주로 모바일 또는 임베디드 AI와 관련된 틈새 주제였지만, 더 큰 모델로의 전환과 함께 변화했습니다. 이 경우, MXFP4 최적화는 모델이 단일 GPU 장치에서 실행될 수 있도록 합니다.

실제로는 다음과 같습니다.

*   대규모 모델(120B)은 단일 80GB H100 또는 더 새로운 GPU에 들어맞습니다. 소비자 하드웨어는 아니지만, 멀티 H100 머신보다 1-H100 머신을 빌리는 것이 훨씬 저렴합니다. 게다가, 모델을 여러 GPU에 분산시키고 통신 오버헤드(communication overhead)를 추가하는 것에 대해 걱정할 필요가 없습니다. AMD MI300X 카드도 첫날부터 지원된다는 점이 정말 좋습니다!
*   더 작은 20B 모델은 심지어 16GB VRAM에 들어맞습니다. 단, MXFP4를 지원하려면 RTX 50 시리즈 GPU 또는 더 새로운 것이어야 합니다. (수정: RTX 4090과 같은 구형 카드에 대한 지원이 최근 패치(patch)를 통해 추가되었습니다.)
*   모델은 구형 하드웨어에서도 실행되지만 MXFP4 지원 없이 더 많은 RAM을 소비할 것입니다. MXFP4 최적화 없이 bfloat16 형식의 모델은 gpt-oss-20b의 경우 약 48GB, gpt-oss-120b의 경우 240GB를 소비할 것입니다.

그건 그렇고, 저는 ollama를 사용하여 제 Mac Mini에서 gpt-oss-20b 모델을 편안하게 실행할 수 있습니다. 약 13.5GB의 메모리를 사용하는데, 이는 정말 합리적입니다.

#### 4.4 벤치마크

이 모델들은 독립적인 벤치마크를 하기에는 아직 너무 새롭습니다. LM 아레나 리더보드(leaderboard)를 확인해 보니, gpt-oss는 아직 목록에 없습니다. 따라서 LM 아레나 사용자들에 따르면, Qwen3-Instruct가 현재까지 최고의 오픈 웨이트 모델로 남아 있습니다(그림 22).

**그림 22**: LM 아레나 리더보드의 현재 모습 (2025년 8월 8일 기준)

gpt-oss 발표 게시물에서 제공된 추론 벤치마크를 보면, gpt-oss 모델이 OpenAI의 독점 모델뿐만 아니라 Qwen3와도 동등한 수준임을 알 수 있습니다(그림 23).

**그림 23**: 주요 벤치마크 차트는 공식 gpt-oss 발표 게시물에서 가져왔습니다. "도구 없음(no tools)" gpt-oss-120b 데이터는 공식 모델 카드 논문에서 가져왔고, Qwen3 수치는 공식 Qwen3 저장소(repository)에서 가져왔습니다.

하지만 gpt-oss-120b가 Qwen3 A235B-A22B-Thinking-2507 모델의 거의 절반 크기이며 단일 GPU에서 실행될 수 있다는 사실로 인해 이 점은 주의해야 합니다.

하지만 벤치마크 성능이 항상 실제 사용성을 반영하는 것은 아닙니다. 지난 며칠간의 제한적인 사용에서 저는 gpt-oss가 상당히 유능하다는 것을 발견했습니다. 그렇지만, 다른 사람들이 관찰했듯이, 환각(hallucinate) 현상이 비교적 높은 경향이 있는 것으로 보입니다(모델 카드에도 언급된 점입니다). 이는 수학, 퍼즐, 코드와 같은 추론 작업에 대한 집중적인 훈련 초점에서 비롯될 수 있으며, 이는 일부 "일반 지식 망각"으로 이어졌을 수 있습니다.

그럼에도 불구하고, gpt-oss는 도구 사용을 염두에 두고 설계되었기 때문에 이 한계는 시간이 지남에 따라 덜 중요해질 수 있습니다. 오픈 소스 LLM에서의 도구 통합은 아직 초기 단계이지만, 성숙해짐에 따라 사실 기반 또는 지식 기반 질문에 답할 때 모델이 외부 소스(검색 엔진과 같은)를 참조하도록 점점 더 많이 허용할 것으로 예상합니다. 그렇게 된다면, 암기보다는 추론 능력(reasoning capacity)을 우선시하는 것이 합리적일 수 있습니다. 이는 학교에서의 인간 학습(또는 일반적으로 삶)과 매우 유사합니다. 문제 해결 능력이 사실을 암기하는 것보다 더 중요한 경우가 많기 때문입니다.

### 5. gpt-oss와 GPT-5

OpenAI는 바쁜 한 주를 보냈고, gpt-oss 출시 직후 오랫동안 기다려온 GPT-5 모델을 공개했습니다. GPT-5 출시는 흥미로웠습니다. 그리고 여기서 한 가지 말해야 할 것이 있다면, 벤치마크 성능(그림 24) 면에서 그들의 오픈 소스 모델이 최고의 제품 제공 모델과 비교하여 얼마나 좋은지에 정말 놀랐다는 것입니다.

**그림 24**: 주요 벤치마크 차트는 공식 GPT-5 발표 게시물에서 가져왔습니다. gpt-oss 데이터는 공식 모델 카드 논문 및 발표 게시물에서 가져왔고, Qwen3 수치는 공식 Qwen3-Coder 저장소에서 가져왔습니다.

전반적으로, 일부 사람들이 이번 출시를 과대광고라고 불렀음에도 불구하고, 최고의 독점 모델에 크게 뒤지지 않는 정말 강력한 새로운 오픈 웨이트 모델 세트를 갖게 되어 기쁩니다. 물론, 벤치마크는 종종 실제 사용을 정확하게 반영하지 않으며, 제한된 사용량만으로는 아직 판단하기 이릅니다. 하지만 저는 오픈 웨이트 및 로컬(또는 개인 호스팅) 모델로 작업하는 것을 좋아하는 사람들에게는 좋은 시기라고 생각합니다.

이 잡지는 개인적인 열정 프로젝트이며, 여러분의 지원은 이를 유지하는 데 도움이 됩니다. 기여하고 싶으시다면 몇 가지 좋은 방법이 있습니다.

*   **제 책을 구입하세요**. "대규모 언어 모델 구축하기 (처음부터)"는 토크나이저(tokenizer)부터 훈련까지 LLM을 단계별로 구축하는 과정을 안내합니다.
*   **비디오 코스를 확인하세요**. 이제 Manning에서 제공하는 책 기반의 17시간 분량 비디오 코스가 있습니다. 이 코스는 책을 섹션별로 면밀히 따르며, 단독으로 또는 코딩 실습 자료로 모두 잘 작동합니다. 이 비디오 코스는 광고가 없으며(YouTube 버전과 달리) 더 깔끔하고 체계적인 형식을 가지고 있습니다. 또한 Abhinav Kimothi가 제작한 5시간 분량의 추가 선수 학습 비디오 자료도 포함되어 있습니다.
*   **구독하세요**. 유료 구독은 저의 글쓰기를 지속 가능하게 하고 추가 콘텐츠에 대한 접근을 제공합니다.

읽어주셔서 감사하며, 독립적인 연구를 지원해 주셔서 감사합니다!

Ahead of AI는 독자 지원 출판물입니다. 새로운 게시물을 받고 저의 작업을 지원하려면 무료 또는 유료 구독자가 되는 것을 고려해 보세요.
구독하기