OpenAI는 이번 주에 새로운 오픈 웨이트(open-weight) LLM인 gpt-oss-120b와 gpt-oss-20b를 출시했습니다. 이는 2019년 GPT-2 이후 처음으로 공개하는 오픈 웨이트 모델입니다. 이 모델들은 몇 가지 영리한 최적화 덕분에 로컬 환경에서도 실행 가능하며, 이는 사용자들에게 큰 이점을 제공합니다. 지난해 ChatGPT의 등장은 글쓰기, 지식 습득, 그리고 코딩 작업에 대한 대규모 언어 모델의 실용성을 대중에게 각인시켰습니다. 이제 우리는 그 이후로 1년이 지난 시점에서, OpenAI가 다시 한번 오픈 웨이트 모델을 공유하며 보여준 발전에 주목할 필요가 있습니다. 초기 GPT 모델들은 트랜스포머(transformer) 아키텍처(architecture)가 어떻게 확장되는지를 보여주었습니다. 이제 그들은 오랫동안 기다려온 가중치 모델을 공유했으며, 이 아키텍처에는 몇 가지 흥미로운 세부 사항이 있습니다. 저는 지난 며칠 동안 코드와 기술 보고서를 읽으며 가장 흥미로운 세부 사항들을 요약했습니다. (작년 후반, OpenAI는 GPT-5도 발표했는데, 이는 이 글의 마지막 부분에서 gpt-oss 모델과 관련하여 간략히 논의할 예정입니다.)

아래는 이 글에서 다룰 내용에 대한 간략한 미리 보기입니다. 더 쉬운 탐색을 위해 글 페이지 왼쪽에 있는 목차를 사용하는 것을 권장합니다.

*   GPT-2와의 모델 아키텍처(architecture) 비교
*   gpt-oss 모델을 단일 GPU에 맞추기 위한 MXFP4 최적화(optimization)
*   너비 대 깊이 트레이드오프(trade-off) (gpt-oss 대 Qwen3)
*   어텐션 바이어스(attention bias) 및 싱크(sinks)
*   벤치마크(benchmark) 및 GPT-5와의 비교

유익한 정보가 되기를 바랍니다!

### 1. 모델 아키텍처 개요

아키텍처를 더 자세히 논의하기 전에, 아래 그림 1에 나와 있는 두 모델, gpt-oss-20b와 gpt-oss-120b의 개요부터 시작하겠습니다.

**그림 1**: 두 gpt-oss 모델의 나란히 비교.

만약 이전에 최신 LLM 아키텍처 다이어그램을 접했거나, 저의 이전 "대규모 아키텍처 비교(Big Architecture Comparison)" 글을 읽었다면, 이 아키텍처가 첫눈에 특별히 새롭거나 이례적이지 않다는 것을 인지할 수 있을 것입니다.

**대규모 LLM 아키텍처 비교**
Sebastian Raschka, PhD · 7월 19일
전체 글 읽기

이는 놀라운 일이 아닙니다. 선도적인 LLM 개발자들이 동일한 기본 아키텍처를 사용하고 작은 조정(tweak)을 적용하는 경향이 있기 때문입니다. 이것은 순전히 저의 추측이지만, 저는 그 이유가 다음과 같다고 생각합니다.

*   이 연구소들 간에 직원들의 상당한 이동이 있습니다.
*   여전히 트랜스포머 아키텍처를 능가하는 대안을 찾지 못했습니다.
    *   상태 공간 모델(state space model) 및 텍스트 확산 모델(text diffusion model)과 같은 혁신적인 접근 방식이 계속 연구되고 있지만, 대규모 다중 턴(multi-turn) 글쓰기 및 코딩 작업에서 트랜스포머만큼 검증된 성능을 보여준 사례는 아직 드뭅니다. (최근 연구에서는 하이브리드 모델의 가능성이 지속적으로 제시되고 있으며, 2025년 중반 기준으로 LM 아레나 상위권에 여러 하이브리드 모델이 포진해 있습니다.)
*   대부분의 성능 향상은 주요 아키텍처 변경보다는 데이터(data)와 알고리즘(algorithm) 조정에서 비롯될 가능성이 높습니다.

그렇다고는 하지만, 그들의 설계 선택에는 여전히 많은 흥미로운 측면이 있습니다. 일부는 위 그림에 나와 있지만(다른 것들은 그렇지 않지만, 나중에 논의할 것입니다). 이 글의 나머지 부분에서는 이러한 특징들을 강조하고 다른 아키텍처들과 하나씩 비교할 것입니다.

또한 저는 OpenAI와 어떤 식으로든 관련이 없다는 점을 말씀드립니다. 저의 정보는 공개된 모델 코드 검토와 기술 보고서 읽기에서 비롯됩니다.

이 모델들을 로컬에서 사용하는 방법을 배우고 싶다면, OpenAI의 공식 모델 허브(hub) 페이지에서 시작하는 것이 가장 좋습니다.

*   https://huggingface.co/openai/gpt-oss-20b
*   https://huggingface.co/openai/gpt-oss-120b

20B 모델은 최신 소비자용 GPU(예: 24GB VRAM을 가진 RTX 40 시리즈 이상)에서 MXFP4 양자화(quantization) 기술을 활용하여 넉넉하게 실행될 수 있습니다. 120B 모델은 여전히 80GB RAM을 가진 단일 H100 또는 동급의 최신 하드웨어가 필요하지만, 클라우드 환경에서는 접근성이 더욱 향상되었습니다. 몇 가지 중요한 주의사항이 있으므로 이 내용은 나중에 다시 다루겠습니다.

### 2. GPT-2에서 시작하여

gpt-oss와 더 최신 아키텍처 간의 비교로 넘어가기 전에, 타임머신을 타고 GPT-2(그림 2)를 나란히 살펴보며 얼마나 많은 발전이 있었는지 확인해 봅시다.

**그림 2**: gpt-oss-20b와 GPT-2 XL 1.5B의 나란히 비교.

gpt-oss와 GPT-2는 모두 "Attention Is All You Need (2017)" 논문에서 제시된 트랜스포머 아키텍처를 기반으로 구축된 디코더 전용(decoder-only) LLM입니다. 수년에 걸쳐 많은 세부 사항이 진화했지만, 이러한 변화는 gpt-oss에만 국한된 것이 아니며, 다른 많은 LLM에서도 공통적으로 나타납니다. 이전 "대규모 아키텍처 비교(Big Architecture Comparison)" 글에서 이러한 측면들을 많이 다루었으므로, 각 소제목을 간결하고 집중적으로 유지하려고 노력할 것입니다.

#### 2.1 드롭아웃(Dropout) 제거

드롭아웃(Dropout, 2012)은 훈련 중에 계층 활성화(layer activation) 또는 어텐션 점수(attention score)(그림 3)의 일부를 무작위로 "드롭아웃(dropping out)"(즉, 0으로 설정)하여 과적합(overfitting)을 방지하는 전통적인 기술입니다. 하지만 현대 LLM에서는 드롭아웃의 사용이 현저히 줄었으며, GPT-2 이후 대부분의 모델에서 사실상 제거되었습니다. 이는 LLM이 일반적으로 방대한 데이터셋(dataset)에 대해 단일 에포크(epoch)로만 훈련되기 때문일 가능성이 높습니다. 훈련 데이터의 규모가 워낙 크기 때문에, 각 토큰이 훈련 중에 한 번만 노출되더라도 과적합의 위험이 상대적으로 낮아집니다. 최근 연구에서도 대규모 모델의 경우 드롭아웃이 오히려 하위 태스크(downstream task) 성능을 저해할 수 있다는 분석이 지속적으로 나옵니다.

**그림 3**: 어텐션 점수 행렬(attention score matrix)에 적용된 드롭아웃의 예시.

#### 2.2 RoPE가 절대 위치 임베딩(Absolute Positional Embeddings)을 대체

트랜스포머 기반 LLM에서는 어텐션 메커니즘(attention mechanism) 때문에 위치 인코딩(positional encoding)이 필요합니다. 기본적으로 어텐션은 입력 토큰을 순서가 없는 것처럼 처리합니다. 원래 GPT 아키텍처에서는 절대 위치 임베딩(absolute positional embedding)이 시퀀스(sequence)의 각 위치에 대해 학습된 임베딩 벡터(embedding vector)를 추가하여 이 문제를 해결했으며(그림 4), 이는 토큰 임베딩에 더해집니다.

**그림 4**: 절대 위치 임베딩의 예시.

RoPE(Rotary Position Embedding)는 다른 접근 방식을 도입하여, 별도의 임베딩을 추가하는 대신 각 토큰의 위치에 따라 쿼리(query) 및 키(key) 벡터를 회전시키는 방식으로 위치 정보를 인코딩합니다. (RoPE는 우아한 아이디어이지만 설명하기 다소 까다로운 주제이기도 합니다. 언젠가 더 자세히 별도로 다룰 계획입니다.) 2021년 처음 소개된 이후, RoPE는 2023년 오리지널 라마(Llama) 모델 출시와 함께 널리 채택되었고, 이제는 현대 LLM의 필수적인 구성 요소로 자리 잡았습니다. 이 외에도 ALiBi(Attention with Linear Biases)나 T5의 상대적 위치 인코딩과 같은 다양한 방법론이 연구되고 있으나, RoPE는 특히 효율성과 성능 면에서 강점을 보이며 광범위하게 사용되고 있습니다.

#### 2.3 Swish/SwiGLU가 GELU를 대체

초기 GPT 아키텍처는 GELU를 사용했습니다. 오늘날에는 Swish(시그모이드 선형 유닛(sigmoid linear unit) 또는 SiLU라고도 함)가 GELU의 대안으로 널리 쓰이며, 계산적으로 약간 더 효율적이라는 장점이 있습니다. 어떤 논문을 보느냐에 따라 모델링 성능 면에서 하나가 다른 것보다 약간 더 낫다는 것을 알 수 있을 것입니다. 제 생각에는 이러한 작은 차이들은 아마도 표준 오차(standard error) 범위 내에 있을 것이며, 하이퍼파라미터(hyperparameter) 민감도에 따라 결과가 달라질 것입니다.

활성화 함수(activation function)는 10여 년 전 딥러닝(deep learning) 커뮤니티가 ReLU에 대체로 정착하기 전까지 뜨거운 논쟁의 주제였습니다. 그 이후로 연구자들은 더 부드러운 곡선을 가진 많은 ReLU 유사 변형들을 제안하고 시도했으며, GELU와 Swish(그림 5)가 정착했습니다.

**그림 5**: Swish와 GELU 활성화 함수의 비교. 둘 다 ReLU의 더 부드러운 버전입니다.

초기 GPT 아키텍처는 GELU를 사용했는데, 이는 `0.5x * [1 + erf(x / sqrt(2))]` 로 정의됩니다. 여기서 `erf`(오차 함수(error function)의 약자)는 가우시안(Gaussian)의 적분이며, 가우시안 적분의 다항식 근사(polynomial approximation)를 사용하여 계산됩니다. 이는 Swish에서 사용되는 시그모이드(sigmoid)와 같은 더 간단한 함수보다 계산 비용이 더 많이 듭니다. Swish는 단순히 `x * sigmoid(x)` 입니다. 실제로 Swish는 GELU보다 계산 비용이 약간 더 낮으며, 이것이 대부분의 최신 모델에서 GELU를 대체한 주된 이유일 것입니다. 어떤 논문을 참고하느냐에 따라 모델링 성능에서 미세한 차이가 있을 수 있지만, 이러한 성능 향상은 대체로 표준 오차 범위 내에 있으며, 하이퍼파라미터 튜닝에 따라 결과가 달라질 수 있습니다. Swish는 오늘날 대부분의 아키텍처에서 사용됩니다. 하지만 GELU가 완전히 잊혀진 것은 아닙니다. 예를 들어, Google의 Gemma 모델은 여전히 GELU를 사용합니다.

그러나 더 주목할 만한 점은 피드 포워드 모듈(feed forward module, 작은 다층 퍼셉트론(multi-layer perceptron))이 게이트형(gated) "GLU"로 대체되었다는 것입니다. 여기서 GLU는 게이트형 선형 유닛(gated linear unit)의 약자이며 2020년 논문에서 제안되었습니다. 구체적으로, 2개의 완전 연결 계층(fully connected layer)이 아래 그림 6과 같이 사용되는 3개의 완전 연결 계층으로 대체됩니다.

**그림 6**: Swish와 GELU, 그리고 그들의 게이트형 대응물인 SwiGLU와 GEGLU의 비교.

언뜻 보기에는 GEGLU/SwiGLU 변형이 추가 계층으로 인해 단순히 더 많은 파라미터(parameter)를 가지고 있기 때문에 일반적인 피드 포워드 계층보다 더 나을 수 있다고 보일 수 있습니다. 하지만 이것은 착각입니다. 실제로는 SwiGLU/GEGLU의 W 및 V 가중치 계층이 전통적인 피드 포워드 계층의 W_1 계층 크기의 절반으로 선택되는 경우가 많기 때문입니다. 이를 더 잘 설명하기 위해 일반 및 GLU 변형의 구체적인 코드 구현을 살펴보겠습니다.

**그림 7**: 일반 피드 포워드 모듈(상단)과 SwiGLU 변형(하단)이 나란히 있습니다. Swish 함수는 PyTorch에서 "silu"로 구현됩니다.

```python
# Regular feed forward module
fc1 = nn.Linear(embedding_dim, 4 * embedding_dim)
fc2 = nn.Linear(4 * embedding_dim, embedding_dim)

# SwiGLU variant
fc1_v = nn.Linear(embedding_dim, 2 * embedding_dim)
fc1_w = nn.Linear(embedding_dim, 2 * embedding_dim)
fc2 = nn.Linear(2 * embedding_dim, embedding_dim)
```

따라서 임베딩 차원(embedding dimension)이 1024라고 가정해 봅시다. 일반 피드 포워드(feed forward)의 경우, 다음과 같습니다: `fc1: 1024 × 4096 = 4,194,304`, `fc2: 4096 × 1024 = 4,194,304`, 즉 `fc1 + fc2 = 8,388,608`개의 파라미터입니다. GLU 변형의 경우, 다음과 같습니다: `fc1_v: 1024 × 2048 = 2,097,152`, `fc1_w: 1024 × 2048 = 2,097,152`, `fc2: 2048 × 1024 = 2,097,152`, 즉 `3 × 2,097,152 = 6,291,456`개의 가중치 파라미터입니다.

따라서 전반적으로 GLU 변형을 사용하면 파라미터 수가 줄어들고 성능도 더 좋습니다. 이러한 더 나은 성능의 이유는 GLU 변형이 추가적인 곱셈 상호작용(multiplicative interaction)을 제공하여 표현력(expressivity)을 향상시키기 때문입니다(이는 잘 훈련되었을 때 깊고 얇은 신경망(neural net)이 얕고 넓은 신경망보다 더 나은 성능을 보이는 것과 같은 이유입니다).

#### 2.4 전문가 혼합(Mixture-of-Experts)이 단일 피드 포워드 모듈을 대체

이전 섹션에서 논의했듯이, 피드 포워드 모듈을 SwiGLU로 업그레이드하는 것 외에도, gpt-oss는 단일 피드 포워드 모듈을 여러 피드 포워드 모듈로 대체하며, 각 토큰 생성 단계에서 부분집합만 사용합니다. 이 접근 방식은 MoE(Mixture-of-Experts, 전문가 혼합)로 알려져 있으며 아래 그림 8에 나와 있습니다.

**그림 8**: 피드 포워드 모듈이 MoE(Mixture-of-Expert) 모듈로 대체됩니다.

따라서 단일 피드 포워드 모듈을 다수의 모듈로 대체하는 MoE 설정에서는 모델의 총 파라미터 수가 상당히 증가합니다. 그러나 핵심은 모든 토큰에 대해 모든 전문가를 활성화하지 않는다는 점입니다. 대신, 라우터(router)는 토큰당 소수의 전문가 부분집합만을 선택합니다. 한 번에 소수의 전문가만 활성화되기 때문에, MoE 모듈은 항상 전체 파라미터 세트(parameter set)를 사용하는 밀집(dense) 모듈과 대조적으로 희소(sparse)하다고 불립니다. 하지만 MoE를 통한 많은 총 파라미터 수는 LLM의 용량(capacity)을 증가시키며, 이는 훈련 중에 더 많은 지식을 습득할 수 있음을 의미합니다. 희소성(sparsity)은 모든 파라미터를 동시에 사용하지 않으므로 추론(inference) 효율성을 높여줍니다. (재미있는 사실: 대부분의 MoE 모델에서 전문가 가중치(expert weight)는 전체 모델 파라미터의 90% 이상을 차지하며, 2025년 현재 MoE는 대규모 모델의 표준 아키텍처 중 하나로 확고히 자리매김했습니다.)

#### 2.5 그룹화된 쿼리 어텐션(Grouped Query Attention)이 다중 헤드 어텐션(Multi-Head Attention)을 대체

이전 글에서 언급했듯이, GQA(Grouped Query Attention, 그룹화된 쿼리 어텐션)는 최근 몇 년 동안 MHA(Multi-Head Attention, 다중 헤드 어텐션)에 비해 더 계산 및 파라미터 효율적인 대안으로 부상했습니다. MHA에서는 각 헤드(head)가 고유한 키(key)와 값(value) 세트를 가지지만, GQA는 여러 헤드를 그룹화하여 동일한 키 및 값 투영(projection)을 공유함으로써 메모리 사용량을 줄입니다. 예를 들어, 그림 9에 나와 있듯이, 2개의 키-값 그룹과 4개의 어텐션 헤드가 있다면, 헤드 1과 2는 하나의 키 및 값 세트를 공유하고, 헤드 3과 4는 다른 세트를 공유할 수 있습니다. 절제 연구(ablation study)에 따르면, 이러한 그룹화는 키 및 값 계산의 총량을 감소시켜 메모리 사용량을 줄이고 효율성을 향상시키면서 모델링 성능에 눈에 띄는 영향을 미치지 않습니다.

**그림 9**: MHA와 GQA의 비교. 여기서 그룹 크기는 2이며, 키와 값 쌍이 2개의 쿼리(query) 간에 공유됩니다.

따라서 GQA의 핵심 아이디어는 여러 쿼리 헤드에 걸쳐 키와 값 헤드를 공유함으로써 그 수를 줄이는 것입니다. 이는 (1) 모델의 파라미터 수를 낮추고 (2) 추론 시 KV 캐시(KV cache)에서 저장하고 검색해야 하는 키와 값이 적어 키 및 값 텐서(tensor)에 대한 메모리 대역폭(bandwidth) 사용량을 줄입니다. (GQA가 코드에서 어떻게 보이는지 궁금하다면, KV 캐시가 없는 버전에 대한 저의 GPT-2에서 Llama 3 변환 가이드와 여기에서 KV 캐시 변형을 참조하십시오.) GQA는 주로 MHA에 대한 계산 효율성 해결책이지만, 절제 연구(원래 GQA 논문 및 Llama 2 논문의 연구와 같은)는 LLM 모델링 성능 면에서 표준 MHA와 유사하게 작동함을 보여줍니다. 2025년 현재, GQA는 대부분의 최신 LLM에서 사실상 표준으로 채택되고 있습니다.

#### 2.6 슬라이딩 윈도우 어텐션(Sliding Window Attention)

슬라이딩 윈도우 어텐션(Sliding-window attention, 아래 그림 10)은 LongFormer 논문(2020)에서 처음 소개되었고 나중에 Mistral에 의해 대중화되었습니다. 흥미롭게도, gpt-oss는 이를 매 두 번째 계층에 적용합니다. 이를 다중 헤드 어텐션(multi-head attention)의 변형, 또는 이 경우 그룹화된 쿼리 어텐션(GQA)의 변형으로 생각할 수 있습니다. 여기서 어텐션 컨텍스트(attention context)는 더 작은 윈도우(window)로 제한되어 메모리 사용량과 계산 비용을 모두 줄이는 효과가 있습니다.

**그림 10**: 일반 어텐션(왼쪽)과 슬라이딩 윈도우 어텐션(오른쪽)의 비교.

구체적으로, gpt-oss는 전체 컨텍스트(context)에 어텐션하는 GQA 계층과 128개 토큰으로 제한된 슬라이딩 윈도우를 가진 GQA 계층을 번갈아 사용합니다. 이전 글에서 논의했듯이, 2024년 출시된 Gemma 2는 유사한 1:1 비율을 사용했습니다. 올해 초(2025년) Gemma 3는 이 전략을 더욱 발전시켜 5:1 비율, 즉 5개의 슬라이딩 윈도우(로컬) 어텐션 계층마다 하나의 전체 어텐션 계층만 사용하는 방식을 채택했습니다. Gemma 절제 연구에 따르면, 이러한 슬라이딩 윈도우 어텐션은 모델링 성능에 미치는 영향이 미미하면서도 효율성을 크게 개선합니다. Gemma 2의 윈도우 크기는 4096개 토큰이었고, Gemma 3는 이를 1024개로 줄였지만, gpt-oss에서는 윈도우가 단 128개 토큰으로, 이는 상당히 작은 편에 속합니다.

그리고 재미있는 사실로, 공식 발표 글에서는 슬라이딩 윈도우 어텐션이 GPT-3에서 이미 사용되었던 것으로 보인다고 언급합니다.

> 이 모델들은 GPT-3와 유사하게 밀집(dense) 및 지역적으로 밴드형(banded) 희소(sparse) 어텐션 패턴을 번갈아 사용합니다.

누가 알았겠어요!? 저는 원래 GPT-3 논문으로 돌아갔고, 실제로 거기에 언급되어 있었습니다.

> 우리는 GPT-2 [RWC+19]와 동일한 모델 및 아키텍처를 사용하며, 여기에 설명된 수정된 초기화(initialization), 사전 정규화(pre-normalization) 및 가역 토큰화(reversible tokenization)를 포함합니다. 단, 트랜스포머 계층에서 Sparse Transformer [CGRS19]와 유사하게 밀집 및 지역적으로 밴드형 희소 어텐션 패턴을 번갈아 사용한다는 점은 예외입니다.

#### 2.7 RMSNorm이 LayerNorm을 대체

마지막으로, GPT-2에서 비롯된 마지막 작은 조정은 최근 몇 년간 흔한 추세였던 LayerNorm (2016)을 RMSNorm (2019)으로 대체하는 것입니다. GELU를 Swish 및 SwiGLU로 교체하는 것과 마찬가지로, RMSNorm은 이러한 작지만 합리적인 효율성 개선 중 하나입니다. RMSNorm은 아래 그림 11에 나타나듯이 계층 활성화(layer activation)를 정규화(normalize)하는 목적에서 LayerNorm과 유사한 역할을 합니다. 그리 오래 전이 아니더라도, BatchNorm이 이 작업에 가장 많이 사용되는 선택지였다는 것을 기억할 것입니다. 이후 BatchNorm은 효율적으로 병렬화(parallelize)하기 어렵고(평균 및 분산 배치 통계(mean and variance batch statistics) 때문에) 작은 배치 크기(batch size)에서는 성능이 좋지 않아 선호도가 떨어졌습니다.

**그림 11**: 작은 선형 계층(linear layer)에 대한 LayerNorm(왼쪽)과 RMSNorm(오른쪽)의 비교.

위 그림 11에서 볼 수 있듯이, LayerNorm과 RMSNorm 모두 계층 출력(layer output)을 합리적인 범위 내로 스케일링(scale)합니다. LayerNorm은 평균을 빼고 표준 편차(standard deviation)로 나누어 계층 출력이 평균 0과 단위 분산(unit variance, 분산 1, 표준 편차 1)을 갖도록 합니다. RMSNorm은 입력을 제곱평균제곱근(root-mean-square)으로 나눕니다. 이는 활성화(activation)를 유사한 크기로 스케일링하면서 평균 0 또는 단위 분산을 강제하지 않습니다. 그림 11에 나와 있는 이 특정 예시에서 평균은 0.77이고 분산은 0.41입니다.

LayerNorm과 RMSNorm 모두 활성화 스케일을 안정화하고 최적화를 개선하지만, RMSNorm은 계산 비용이 더 저렴하다는 점에서 대규모 LLM에서 선호되는 경향이 있습니다. LayerNorm과 달리 RMSNorm은 바이어스(bias, 이동) 항이 없으며, 비용이 많이 드는 평균 및 분산 계산을 단일 제곱평균제곱근 연산으로 줄입니다. 이는 교차 특징 감소(cross-feature reduction) 횟수를 2개에서 1개로 줄여 GPU의 통신 오버헤드(communication overhead)를 낮추고 훈련 효율성을 향상시킵니다. 그림 12는 이것이 코드에서 어떻게 보이는지 보여줍니다.

**그림 12**: RMSNorm이 계산적으로 더 간단함을 보여주는 LayerNorm 및 RMSNorm의 코드 구현.

```python
# LayerNorm
mean = x.mean(dim=-1, keepdim=True)
var = x.var(dim=-1, keepdim=True, unbiased=False)
x_norm = (x - mean) / torch.sqrt(var + eps)

# RMSNorm
rms = torch.sqrt(torch.mean(x**2, dim=-1, keepdim=True) + eps)
x_norm = x / rms
```

최근 연구에서는 더욱 다양한 정규화 기법들이 탐구되고 있지만, RMSNorm은 그 효율성과 안정성 덕분에 여전히 많은 최신 LLM에서 핵심적인 역할을 하고 있습니다.

#### 2.8 GPT-2의 유산

저는 여전히 GPT-2가 LLM에 대해 배울 때 훌륭한 초보자용 아키텍처라고 생각합니다. 최적화 트릭의 여러 계층에 갇히지 않고 이해하기에 충분히 간단하지만, 현대 트랜스포머 모델이 어떻게 작동하는지에 대한 확실한 이해를 제공할 만큼 여전히 복잡합니다. GPT-2로 시작하면, 새로운 아키텍처에 추가된 기능과 조정에 압도되지 않고 어텐션 메커니즘, 위치 임베딩, 정규화, 전반적인 훈련 파이프라인과 같은 기본적인 원리에 집중할 수 있습니다. 실제로, 수많은 새로운 모델이 쏟아져 나오는 현재 시점에도 GPT-2 아키텍처를 먼저 이해하고 구현하는 것은 시간 투자 가치가 충분합니다. 이는 단순히 새로운 변경 사항을 쉽게 이해하는 것을 넘어, 그들이 해결하려는 한계나 문제가 무엇인지 더 깊이 이해하고 그 가치를 높이 평가하게 할 것입니다.

예를 들어, 저의 GPT-2 코드를 시작으로 최근 Qwen3 아키텍처를 처음부터 구현했는데, 이는 gpt-oss와 매우 유사합니다. 이는 다음 주제로 이어집니다: gpt-oss를 더 최신 아키텍처와 비교하기.

Ahead of AI는 독자 지원 출판물입니다. 새로운 게시물을 받고 저의 작업을 지원하려면 무료 또는 유료 구독자가 되는 것을 고려해 보세요.
구독하기

### 3. gpt-oss를 최신 아키텍처(Qwen3)와 비교

이제 GPT-2에서 GPT OSS로의 진화를 살펴보았으니, 다음 단계로 나아가 GPT OSS를 작년 5월에 출시되어 현재까지도 강력한 성능을 보이는 Qwen3와 비교해 볼 차례입니다. 제가 여기서 Qwen3를 선택하는 이유는 이 글을 쓰는 시점에서 Qwen3가 최고의 오픈 웨이트 모델 중 하나이기 때문입니다. 또한, Qwen3 MoE 모델 중 하나는 훈련 가능한 파라미터(trainable parameter) 측면에서 상대적으로 유사한 전체 크기를 가지고 있어 GPT OSS와 거의 직접적으로 비교할 수 있습니다. 아래 그림 13은 gpt-oss-20b와 유사한 크기의 Qwen3 모델을 비교합니다.

**그림 13**: 유사한 크기의 gpt-oss 및 Qwen3 모델의 나란히 비교.

보시다시피, gpt-oss 20B와 Qwen3 30B-A3B는 아키텍처 구성 요소에서 매우 유사합니다. 여기서 주요 차이점은 차원(dimension) 외에도 gpt-oss는 섹션 1.6에서 이전에 논의했듯이 슬라이딩 윈도우 어텐션(sliding window attention)을 사용하지만(이 그림에는 표시되지 않음), Qwen3는 그렇지 않다는 것입니다. 다음 소제목들에서 주목할 만한 세부 사항들을 하나씩 살펴보겠습니다.

#### 3.1 너비 대 깊이

두 모델을 자세히 살펴보면, Qwen3는 24개가 아닌 48개의 트랜스포머 블록(transformer block)을 가진 훨씬 더 깊은 아키텍처임을 알 수 있습니다(그림 14).

**그림 14**: Qwen3는 gpt-oss-20b보다 두 배 많은 트랜스포머 블록을 가지고 있습니다.

반면에 gpt-oss는 훨씬 더 넓은 아키텍처입니다.

*   2048이 아닌 2880의 임베딩 차원(embedding dimension)
*   또한 768이 아닌 2880의 중간 전문가(피드 포워드) 투영 차원(projection dimension)

gpt-oss가 두 배 많은 어텐션 헤드를 사용한다는 점도 주목할 만하지만, 모델의 너비는 주로 임베딩 차원에 의해 결정됩니다. 고정된 파라미터 수에서 한 접근 방식이 다른 접근 방식보다 이점을 제공할까요? 일반적으로, 더 깊은 모델은 더 많은 유연성을 가지지만, 폭발 및 소실 기울기(exploding and vanishing gradient)로 인한 불안정성 문제 때문에 훈련하기 더 어려울 수 있습니다(RMSNorm과 숏컷 연결(shortcut connection)이 이를 완화하는 것을 목표로 합니다). 더 넓은 아키텍처는 더 높은 메모리 비용이 들 수 있지만, 더 나은 병렬화(parallelization) 덕분에 추론 시 더 빠른 처리량(tokens/second throughput)을 가질 수 있다는 장점이 있습니다. 모델링 성능에 관해서는, 여전히 좋은 동등 비교(parameter size와 데이터셋이 일정하게 유지되는) 연구는 부족하지만, Gemma 2 논문(표 9)의 절제 연구에서 90억 파라미터 아키텍처의 경우 더 넓은 설정이 더 깊은 설정보다 약간 더 낫다는 것을 발견했습니다. 2025년 현재에도 이러한 너비 대 깊이 트레이드오프는 여전히 활발한 연구 주제이며, 최적의 균형점은 특정 작업과 하드웨어 제약에 따라 달라질 수 있습니다.

#### 3.2 적은 수의 대규모 전문가 대 많은 수의 소규모 전문가

위 그림 14에서 볼 수 있듯이, gpt-oss가 놀랍도록 적은 수의 전문가(128개 대신 32개)를 가지고 있으며, 토큰당 8개가 아닌 4개의 활성 전문가만 사용한다는 점도 주목할 만합니다. 하지만 각 전문가는 Qwen3의 전문가보다 훨씬 더 큽니다. 이는 최근 연구와 동향이 더 많고 작은 전문가를 선호한다는 점을 시사하기 때문에 흥미로운 설계 선택입니다. 이러한 변화는 총 파라미터 크기가 일정할 때 DeepSeekMoE 논문의 아래 그림 15에 잘 설명되어 있습니다.

**그림 15**: "DeepSeekMoE: Mixture-of-Experts 언어 모델에서 궁극적인 전문가 전문화를 향하여" 논문의 주석이 달린 그림, https://arxiv.org/abs/2401.06066

특히, DeepSeek의 모델과 달리 gpt-oss와 Qwen3 모두 공유 전문가(shared expert)를 사용하지 않습니다. 공정하게 말하면, gpt-oss의 적은 수의 전문가는 20B 크기의 부작용일 수 있습니다. 아래 120B 모드를 보면, 아래 그림 16에 나와 있듯이 다른 모든 것을 고정한 채 전문가 수(및 트랜스포머 블록)를 실제로 늘렸습니다.

**그림 16**: 두 gpt-oss 아키텍처의 나란히 비교. 더 큰 120B 모델은 트랜스포머 블록 수와 전문가 수만 스케일링(scaling)합니다.

20B와 120B 모델이 매우 유사하다는 설명은 아마도 120B 모델이 주요 초점이었기 때문일 것입니다. 더 작은 모델을 만드는 가장 쉬운 방법은 모델을 약간 더 짧게(더 적은 트랜스포머 블록) 만들고 전문가 수를 줄이는 것이었습니다. 대부분의 파라미터가 그곳에 있기 때문입니다. 하지만 그들이 120B 모델을 훈련하기 시작한 다음, 일부 트랜스포머 블록과 전문가를 잘라내어 계속적인 사전 훈련(pre-training)을 했는지(무작위 가중치에서 시작하는 대신) 추측해 볼 수도 있습니다. 어떤 경우든, 이 두 가지(트랜스포머 블록과 전문가 수)만 스케일링하는 것은 매우 이례적이기 때문입니다. 예를 들어, 여러 크기의 Qwen3 MoE 모델(아래 그림 17)을 보면, 훨씬 더 많은 측면에서 서로 더 비례적으로 스케일링되었습니다. 2025년 현재, MoE 모델의 최적 스케일링 전략에 대한 연구는 여전히 활발하며, 전문가의 크기, 수, 활성화 방식 등 다양한 요소가 모델 성능에 미치는 영향이 탐구되고 있습니다.

**그림 17**: 다양한 Qwen3 모델의 아키텍처 차이.

#### 3.3 어텐션 바이어스(Attention Bias) 및 어텐션 싱크(Attention Sinks)

gpt-oss와 Qwen3 모두 그룹화된 쿼리 어텐션(grouped query attention)을 사용합니다. 주요 차이점은 gpt-oss가 이전에 언급했듯이 각 두 번째 계층에서 슬라이딩 윈도우 어텐션(sliding window attention)을 통해 컨텍스트 크기(context size)를 제한한다는 것입니다. 하지만 제 눈길을 끈 한 가지 흥미로운 세부 사항은, 아래 그림에 나타나듯이 gpt-oss가 어텐션 가중치(attention weight)에 바이어스 유닛(bias unit)을 사용하는 것으로 보인다는 점입니다.

**그림 18**: gpt-oss 모델은 어텐션 계층에 바이어스 유닛을 사용합니다. 여기에서 코드 예시를 참조하십시오.

저는 GPT-2 시절 이후로 이러한 바이어스 유닛이 사용되는 것을 본 적이 없으며, 일반적으로 불필요하다고 여겨집니다. 실제로, 저는 이것이 적어도 키 변환(key transformation, k_proj)에 대해서는 수학적으로 사실임을 보여주는 최근 논문을 찾았습니다. 더욱이, 경험적 결과는 바이어스 유닛의 유무에 따른 차이가 거의 없음을 보여줍니다(아래 그림 19 참조).

**그림 19**: https://arxiv.org/pdf/2302.08626에서 가져온 표로, 바이어스 유닛의 유무에 따라 모델을 처음부터 훈련했을 때의 평균 테스트 손실(test loss)을 보여줍니다.

여러분이 눈치챘을 또 다른 세부 사항은 그림 18의 코드 스크린샷에 있는 싱크(sinks)의 정의입니다. 일반적인 모델에서 어텐션 싱크(attention sink)는 어텐션을 안정화하기 위해 시퀀스 시작 부분에 배치되는 특별한 "항상 어텐션되는" 토큰으로, 긴 컨텍스트 시나리오에서 특히 유용합니다. 즉, 컨텍스트가 매우 길어지더라도 시작 부분의 이 특별한 어텐션되는 토큰은 여전히 어텐션되며, 전체 시퀀스에 대한 일반적으로 유용한 정보를 저장하는 방법을 학습할 수 있습니다. (이는 원래 "Efficient Streaming Language Models with Attention Sinks" 논문에서 제안된 것으로 알려져 있습니다.) gpt-oss 구현에서 어텐션 싱크는 입력 시퀀스(input sequence)의 실제 토큰은 아닙니다. 대신, 이들은 어텐션 점수(attention score)에 추가되는 학습된 헤드별 바이어스 로짓(per-head bias logit)입니다(그림 20). 목표는 위에서 언급한 어텐션 싱크와 동일하지만, 토큰화된 입력(tokenized input)을 수정하지 않는다는 점에서 차이가 있습니다. 2025년 현재, 어텐션 싱크는 긴 컨텍스트 처리를 위한 효과적인 기술로 계속해서 발전하고 있으며, 다양한 변형이 제안되고 있습니다.

**그림 20**: gpt-oss에서 어텐션 싱크의 사용; 여기 Hugging Face 코드를 기반으로 합니다.

#### 3.4 라이선스

마지막으로, Qwen3와 유사하게 gpt-oss 모델은 Apache 2.0 오픈 소스 라이선스(open-source license)를 따르며, 이는 훌륭합니다(제가 제 오픈 소스 프로젝트에 선호하는 라이선스와 동일합니다). 이는 모델이 다른 모델로 증류(distill)되거나 상업용 제품에 제한 없이 사용될 수 있음을 의미합니다.

오픈 웨이트(open-weight) 대 오픈 소스(open-source) LLM의 구분은 수년 동안 논의되어 왔지만, 이번 출시와 그 결과물에 대한 혼란을 피하기 위해 다시 한번 명확히 할 가치가 있습니다. 일부 모델 개발자들은 모델 가중치(model weight)와 추론 코드(inference code)만 공개하는 반면(예: Llama, Gemma, gpt-oss), 다른 개발자들(예: OLMo)은 훈련 코드(training code), 데이터셋, 가중치를 포함한 모든 것을 진정한 오픈 소스로 공개합니다. 이러한 엄격한 정의에 따르면, gpt-oss는 가중치와 추론 코드를 포함하지만 훈련 코드나 데이터셋은 포함하지 않으므로 오픈 웨이트 모델입니다(Qwen3와 마찬가지로). 하지만 이 용어는 업계 전반에 걸쳐 일관성 없이 사용되며, '오픈'의 의미에 대한 논의는 2025년 현재에도 여전히 뜨겁습니다. 저는 "gpt-oss"의 "oss"가 오픈 소스 소프트웨어(open source software)를 의미한다고 가정하지만, OpenAI 자체가 공식 발표 글에서 gpt-oss를 오픈 웨이트 모델로 명확하게 설명한 것에 긍정적으로 놀랐습니다. 이러한 투명성은 업계의 혼란을 줄이는 데 기여할 수 있습니다.

### 4. 기타 흥미로운 정보

이전 섹션에서는 GPT-2 이후 아키텍처가 어떻게 발전했는지 설명하고 Qwen3(및 대부분의 다른 최신 모델)와의 유사점을 논의했지만, 아직 언급하지 않은 몇 가지 추가적이지만 주목할 만한 세부 사항들이 있습니다. 이들은 이전 섹션에 깔끔하게 들어맞지 않았지만 여전히 언급할 가치가 있는 점들입니다.

#### 4.1 훈련 개요

불행히도, 훈련 세트(training set) 크기와 알고리즘에 대한 정보는 많지 않습니다. 아래에 모델 카드 보고서(1)와 발표 게시물(2)에서 가장 흥미로운 조각들을 추가했습니다.

*   gpt-oss 모델은 우리의 가장 진보된 사전 훈련(pre-training) 및 사후 훈련(post-training) 기술을 사용하여 훈련되었습니다 [...] (1)
*   [...] 완료하는 데 210만 H100-시간이 필요했으며, gpt-oss-20b는 거의 10배 적게 필요했습니다. (1)
*   [...] 지도 미세 조정(supervised fine-tuning) 단계와 고성능 RL(강화 학습) 단계(high-compute RL stage)를 포함합니다 [...] (2)
*   우리는 주로 영어 텍스트 전용 데이터셋에서 모델을 훈련했으며, STEM, 코딩 및 일반 지식에 중점을 두었습니다. (2)

따라서 gpt-oss 모델이 추론(reasoning) 능력에 중점을 둔 모델이라는 것을 알 수 있습니다. 210만 H100 GPU 시간의 훈련 계산량은 DeepSeek V3 모델(약 5.6배 더 큼)이 훈련된 278.8만 H800 GPU 시간과 비교했을 때 상당한 규모임을 시사합니다. 불행히도 Qwen3의 훈련 시간에 대한 정보는 아직 없습니다. 흥미롭게도, GPT-oss 훈련 시간 추정치는 지시 따르기(instruction following)를 위한 지도 학습(supervised learning)과 추론을 위한 강화 학습(reinforcement learning)을 모두 포함하는 반면, DeepSeek V3는 DeepSeek R1이 별도로 훈련된 사전 훈련된 기본 모델(pre-trained base model)일 뿐입니다. LLM 훈련 데이터셋의 투명성과 재현성은 2025년 현재에도 여전히 중요한 이슈로 남아있으며, 더 많은 연구소들이 훈련 방법론과 데이터셋에 대한 상세한 정보를 공개하기를 기대합니다.

#### 4.2 추론 노력

이전 섹션에서 언급했듯이, gpt-oss 모델은 추론 모델입니다. 하지만 특히 흥미로운 점은 사용자가 추론 시간 스케일링(inference time scaling)을 통해 추론의 정도를 쉽게 제어할 수 있도록 훈련되었다는 것입니다. 구체적으로, gpt-oss 모델은 시스템 프롬프트(system prompt)의 일부로 "추론 노력: 낮음/중간/높음" 지시를 받을 수 있으며, 이는 그림 21에 나타나듯이 응답 길이와 정확도에 직접적인 영향을 미칩니다.

**그림 21**: 다양한 추론 노력 하에서의 gpt-oss 모델의 응답 길이 및 품질 (모델 카드에서 가져온 주석이 달린 그림)

이러한 조정 가능성은 비용, 계산량, 정확도의 균형을 맞출 수 있게 해주므로 유용합니다. 예를 들어, 간단한 지식 질문에 답하거나 작은 오타를 수정하는 것과 같이 작업이 간단하다면, 확장된 추론을 건너뛸 수 있습니다. 이는 불필요하게 긴 응답과 장황한 추론 흔적을 피하면서 시간과 자원을 절약합니다. Qwen3나 OLMo와 달리 OpenAI가 강화 학습 기반 추론 훈련 전에 기본 모델을 공개하지 않은 것은 다소 아쉽습니다. 기본 모델은 추론 방법론을 연구하는 연구자들에게 특히 귀중한 출발점입니다(제가 현재 Qwen3 Base로 작업하는 것을 좋아하는 이유 중 하나입니다). 제 생각에 OpenAI의 결정은 연구적 고려보다는 산업 및 생산 사용 사례에 의해 더 많이 좌우되었을 것입니다. 2025년 현재, LLM의 추론 제어 및 동적 추론(dynamic inference)은 활발한 연구 분야이며, gpt-oss의 접근 방식은 사용자에게 유연성을 제공하는 좋은 예시입니다.

원래 Qwen3 모델에도 사고(추론) 모드를 활성화/비활성화하는 토글(toggle)이 있습니다(토크나이저(tokenizer)의 `enable_thinking=True/False` 설정으로, 추론 동작을 비활성화하기 위해 단순히 `<think></think>` 태그를 추가합니다). 하지만 Qwen3 팀은 지난 몇 주 동안 모델을 업데이트하여 하이브리드 모델에서 전용 Instruct/Thinking/Coder 변형으로 전환했습니다. 그 이유는 하이브리드 모드가 개별 모델에 비해 성능이 낮았기 때문입니다.

> 커뮤니티와 논의하고 이 문제를 숙고한 결과, 우리는 하이브리드 사고 모드를 포기하기로 결정했습니다. 이제 최상의 품질을 달성하기 위해 Instruct 및 Thinking 모델을 별도로 훈련할 것입니다.
> 출처

#### 4.3 MXFP4 최적화: 작지만 중요한 세부 사항

한 가지 흥미로운 놀라움은 OpenAI가 MoE 전문가를 위한 MXFP4 양자화(quantization) 방식을 사용하여 gpt-oss 모델을 출시했다는 것입니다. 양자화 형식은 주로 모바일 또는 임베디드 AI와 관련된 틈새 주제였지만, 더 큰 모델로의 전환과 함께 변화했습니다. 이 경우, MXFP4 최적화는 모델이 단일 GPU 장치에서 실행될 수 있도록 합니다.

실제로는 다음과 같습니다.

*   대규모 모델(120B)은 단일 80GB H100 또는 더 새로운 GPU에 들어맞습니다. 소비자 하드웨어는 아니지만, 멀티 H100 머신보다 1-H100 머신을 빌리는 것이 훨씬 저렴합니다. 게다가, 모델을 여러 GPU에 분산시키고 통신 오버헤드(communication overhead)를 추가하는 것에 대해 걱정할 필요가 없습니다. AMD MI300X 카드도 첫날부터 지원된다는 점이 정말 좋습니다! 2025년 현재, 클라우드 제공업체들은 이러한 고사양 단일 GPU 인스턴스를 더욱 다양하게 제공하고 있습니다.
*   더 작은 20B 모델은 최신 소비자용 GPU의 16GB VRAM에도 충분히 들어맞습니다. MXFP4를 지원하려면 RTX 50 시리즈 GPU 또는 더 새로운 것이어야 했으나, 최근 패치(patch)를 통해 RTX 4090과 같은 구형 카드에서도 지원이 확대되었습니다. 이는 더 많은 사용자들이 로컬에서 모델을 활용할 수 있게 된 긍정적인 발전입니다.
*   모델은 구형 하드웨어에서도 실행되지만 MXFP4 지원 없이 더 많은 RAM을 소비할 것입니다. MXFP4 최적화 없이 bfloat16 형식의 모델은 gpt-oss-20b의 경우 약 48GB, gpt-oss-120b의 경우 240GB를 소비할 것입니다.

그건 그렇고, 저는 ollama를 사용하여 제 Mac Mini에서 gpt-oss-20b 모델을 편안하게 실행할 수 있습니다. 약 13.5GB의 메모리를 사용하는데, 이는 정말 합리적입니다. MXFP4와 같은 고급 양자화 기술은 2025년 LLM 배포의 핵심 요소가 되었으며, 온디바이스(on-device) AI 및 엣지 컴퓨팅(edge computing) 환경에서 대규모 모델을 실행하는 데 필수적인 역할을 하고 있습니다.

#### 4.4 벤치마크

이 모델들은 출시된 지 1년이 지났으므로, 이제는 다양한 독립적인 벤치마크 결과를 확인할 수 있습니다. LM 아레나 리더보드(leaderboard)를 확인해 보니, gpt-oss는 현재 상위권에 여러 버전이 등재되어 있으며, Qwen3-Instruct와 같은 강력한 경쟁자들과 함께 최고의 오픈 웨이트 모델 중 하나로 평가받고 있습니다(그림 22).

**그림 22**: LM 아레나 리더보드의 현재 모습 (2025년 8월 8일 기준)

gpt-oss 발표 게시물에서 제공된 초기 추론 벤치마크를 보면, gpt-oss 모델이 OpenAI의 독점 모델뿐만 아니라 Qwen3와도 대등한 수준임을 보여주었습니다(그림 23).

**그림 23**: 주요 벤치마크 차트는 공식 gpt-oss 발표 게시물에서 가져왔습니다. "도구 없음(no tools)" gpt-oss-120b 데이터는 공식 모델 카드 논문에서 가져왔고, Qwen3 수치는 공식 Qwen3 저장소(repository)에서 가져왔습니다.

하지만 gpt-oss-120b가 Qwen3 A235B-A22B-Thinking-2507 모델의 거의 절반 크기이며 단일 GPU에서 실행될 수 있다는 사실로 인해 이 점은 주의해야 합니다.

하지만 벤치마크 성능이 항상 실제 사용성을 완벽하게 반영하는 것은 아닙니다. 지난 1년간의 사용 경험에 따르면, gpt-oss는 상당히 유능하지만, 일부 사용자들은 환각(hallucinate) 현상이 비교적 높은 경향이 있다고 보고했습니다(이는 모델 카드에도 언급된 점입니다). 이는 수학, 퍼즐, 코드와 같은 추론 작업에 대한 집중적인 훈련 초점에서 비롯될 수 있으며, 일부 "일반 지식 망각"으로 이어졌을 수 있습니다. 2025년 현재, 환각 현상 감소를 위한 다양한 기술(예: RAG(Retrieval-Augmented Generation) 통합)이 LLM 개발의 주요 초점 중 하나입니다.

그럼에도 불구하고, gpt-oss는 도구 사용을 염두에 두고 설계되었기 때문에 이 한계는 시간이 지남에 따라 덜 중요해질 수 있습니다. 오픈 소스 LLM에서의 도구 통합은 아직 초기 단계이지만, 성숙해짐에 따라 사실 기반 또는 지식 기반 질문에 답할 때 모델이 외부 소스(검색 엔진과 같은)를 참조하도록 점점 더 많이 허용할 것으로 예상합니다. 그렇게 된다면, 암기보다는 추론 능력(reasoning capacity)을 우선시하는 것이 합리적일 수 있습니다. 이는 학교에서의 인간 학습(또는 일반적으로 삶)과 매우 유사합니다. 문제 해결 능력이 사실을 암기하는 것보다 더 중요한 경우가 많기 때문입니다.

### 5. gpt-oss와 GPT-5

OpenAI는 작년에 gpt-oss 출시 직후 오랫동안 기다려온 GPT-5 모델을 공개하며 바쁜 한 해를 보냈습니다. GPT-5 출시는 흥미로웠습니다. 그리고 여기서 한 가지 말해야 할 것이 있다면, 벤치마크 성능(그림 24) 면에서 그들의 오픈 소스 모델이 최고의 제품 제공 모델과 비교하여 얼마나 좋은지에 정말 놀랐다는 것입니다.

**그림 24**: 주요 벤치마크 차트는 공식 GPT-5 발표 게시물에서 가져왔습니다. gpt-oss 데이터는 공식 모델 카드 논문 및 발표 게시물에서 가져왔고, Qwen3 수치는 공식 Qwen3-Coder 저장소에서 가져왔습니다.

전반적으로, 일부 사람들이 이번 출시를 과대광고라고 불렀음에도 불구하고, 최고의 독점 모델에 크게 뒤지지 않는 정말 강력한 새로운 오픈 웨이트 모델 세트를 갖게 되어 기쁩니다. 물론, 벤치마크는 종종 실제 사용을 정확하게 반영하지 않으며, 지난 1년간의 제한된 사용량만으로는 아직 완벽하게 판단하기는 이릅니다. 하지만 2025년 현재, 오픈 웨이트 모델 생태계는 이전 어느 때보다 활발하며, 로컬(또는 개인 호스팅) 모델로 작업하는 것을 좋아하는 사람들에게는 정말 좋은 시기라고 생각합니다. 오픈 소스 커뮤니티의 지속적인 발전은 독점 모델과의 격차를 빠르게 줄여나가고 있습니다.

이 잡지는 개인적인 열정 프로젝트이며, 여러분의 지원은 이를 유지하는 데 도움이 됩니다. 기여하고 싶으시다면 몇 가지 좋은 방법이 있습니다.

*   **제 책을 구입하세요**. "대규모 언어 모델 구축하기 (처음부터)"는 토크나이저(tokenizer)부터 훈련까지 LLM을 단계별로 구축하는 과정을 안내합니다.
*   **비디오 코스를 확인하세요**. 이제 Manning에서 제공하는 책 기반의 17시간 분량 비디오 코스가 있습니다. 이 코스는 책을 섹션별로 면밀히 따르며, 단독으로 또는 코딩 실습 자료로 모두 잘 작동합니다. 이 비디오 코스는 광고가 없으며(YouTube 버전과 달리) 더 깔끔하고 체계적인 형식을 가지고 있습니다. 또한 Abhinav Kimothi가 제작한 5시간 분량의 추가 선수 학습 비디오 자료도 포함되어 있습니다.
*   **구독하세요**. 유료 구독은 저의 글쓰기를 지속 가능하게 하고 추가 콘텐츠에 대한 접근을 제공합니다.

읽어주셔서 감사하며, 독립적인 연구를 지원해 주셔서 감사합니다!

Ahead of AI는 독자 지원 출판물입니다. 새로운 게시물을 받고 저의 작업을 지원하려면 무료 또는 유료 구독자가 되는 것을 고려해 보세요.
구독하기