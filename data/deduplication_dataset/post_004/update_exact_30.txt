OpenAI는 이번 주에 새로운 오픈 웨이트(open-weight) LLM인 gpt-oss-120b와 gpt-oss-20b를 출시했습니다. 이는 2019년 GPT-2 이후 처음으로 공개하는 오픈 웨이트 모델입니다. 그리고 몇 가지 영리한 최적화(optimization) 덕분에 이 모델들은 로컬(local)에서 실행될 수 있습니다(자세한 내용은 나중에 다루겠습니다). OpenAI가 대규모의 완전한 오픈 웨이트 모델을 공유한 것은 GPT-2 이후 처음입니다. 초기 GPT 모델들은 트랜스포머(transformer) 아키텍처(architecture)가 어떻게 확장되는지를 보여주었습니다. 2022년 ChatGPT 출시는 글쓰기 및 지식(그리고 나중에는 코딩) 작업에 대한 구체적인 유용성을 입증함으로써 이러한 모델들을 주류로 만들었습니다. 이제 그들은 오랫동안 기다려온 가중치 모델을 공유했으며, 이 아키텍처에는 몇 가지 흥미로운 세부 사항이 있습니다. 저는 지난 며칠 동안 코드와 기술 보고서를 읽으며 가장 흥미로운 세부 사항들을 요약했습니다. (며칠 후, OpenAI는 GPT-5도 발표했는데, 이 글의 마지막 부분에서 gpt-oss 모델과 관련하여 간략하게 논의할 예정입니다.)

대규모 언어 모델(LLM)의 발전은 놀라웠지만, 대부분의 최첨단 모델은 독점적이고 폐쇄적인 생태계 내에 머물러 있었습니다. 따라서 OpenAI가 이러한 오픈 웨이트 모델을 공개한 것은 연구 커뮤니티와 개발자들에게 큰 의미를 가집니다. 이는 투명성을 높이고, 모델의 내부 작동 방식을 더 깊이 이해할 수 있게 하며, 새로운 응용 프로그램과 혁신을 촉진할 수 있는 중요한 계기가 됩니다. 특히, 로컬 환경에서 대규모 모델을 실행할 수 있게 되었다는 점은 개인 정보 보호와 접근성 측면에서 새로운 가능성을 열어줍니다.

아래는 이 글에서 다룰 내용에 대한 간략한 미리 보기입니다. 더 쉬운 탐색을 위해 글 페이지 왼쪽에 있는 목차를 사용하는 것을 권장합니다.

*   GPT-2 아키텍처에서 gpt-oss로의 진화
*   gpt-oss 모델에 적용된 주요 최적화 기법 분석 (MXFP4 양자화 포함)
*   최신 LLM 아키텍처 트렌드 비교 (너비 대 깊이, 전문가 혼합)
*   어텐션 메커니즘의 미묘한 차이 (어텐션 바이어스 및 싱크)
*   모델 성능 벤치마크 및 실제 사용성 평가
*   gpt-oss와 GPT-5의 관계 및 미래 전망

유익한 정보가 되기를 바랍니다!

### 1. 모델 아키텍처 개요

gpt-oss 모델은 현대 LLM 아키텍처의 일반적인 특징을 따르면서도 몇 가지 독특한 최적화와 설계를 포함하고 있습니다. 아래 그림 1은 gpt-oss-20b와 gpt-oss-120b 두 모델의 전체적인 구조를 보여줍니다.

**그림 1**: 두 gpt-oss 모델의 나란히 비교.

최근의 LLM 아키텍처 다이어그램을 자주 접했거나, 저의 이전 "대규모 아키텍처 비교(Big Architecture Comparison)" 글을 읽어보셨다면, gpt-oss의 기본 구조가 낯설지 않을 것입니다. 이는 업계의 선도적인 연구소들이 트랜스포머(transformer) 기반의 디코더 전용(decoder-only) 아키텍처를 기본 틀로 삼고, 그 위에 미세한 조정(tweak)을 가하는 경향이 있기 때문입니다.

**대규모 LLM 아키텍처 비교**
Sebastian Raschka, PhD · 7월 19일
전체 글 읽기

이러한 현상은 여러 가지 이유로 설명될 수 있습니다. 첫째, 핵심 연구원들이 다양한 연구소 간에 이동하면서 공통된 설계 철학과 노하우가 공유되는 경향이 있습니다. 둘째, 트랜스포머 아키텍처는 여전히 텍스트 생성 및 이해 분야에서 가장 강력하고 범용적인 프레임워크로 자리 잡고 있습니다. 상태 공간 모델(state space model, 예를 들어 Mamba)이나 텍스트 확산 모델(text diffusion model)과 같은 대안적인 접근 방식들이 탐구되고 있지만, 현재로서는 대규모에서 트랜스포머만큼의 성능을 입증한 사례는 드뭅니다. 대부분의 비교 연구는 특정 벤치마크 점수에 초점을 맞추는 경향이 있으며, 실제 다중 턴 대화나 복잡한 코딩 작업에서의 유용성은 여전히 더 많은 검증이 필요합니다. 셋째, 모델 성능 향상의 상당 부분은 아키텍처의 근본적인 변경보다는, 훈련 데이터의 품질과 양, 그리고 최적화 알고리즘의 개선에서 비롯되는 경우가 많습니다.

그럼에도 불구하고, gpt-oss의 설계에는 여전히 주목할 만한 흥미로운 측면들이 존재합니다. 일부는 위 그림에서 명확하게 드러나지만, 다른 중요한 세부 사항들은 더 깊이 탐구해야 합니다. 이 글의 나머지 부분에서는 이러한 특징들을 하나씩 자세히 살펴보고, 다른 최신 LLM 아키텍처들과 비교 분석할 것입니다.

참고로, 저는 OpenAI와 직접적인 관계가 없으며, 이 글의 모든 정보는 공개된 모델 코드 검토와 기술 보고서 분석을 기반으로 합니다.

이 모델들을 로컬에서 직접 사용해보고 싶다면, OpenAI의 공식 모델 허브(hub) 페이지를 방문하는 것이 가장 좋습니다.

*   https://huggingface.co/openai/gpt-oss-20b
*   https://huggingface.co/openai/gpt-oss-120b

20B 모델은 최대 16GB RAM을 가진 일반 소비자용 GPU에서도 실행될 수 있으며, 120B 모델은 80GB RAM을 가진 단일 H100 또는 그보다 새로운 하드웨어에서 실행 가능합니다. 몇 가지 중요한 고려사항이 있으므로, 이 부분은 나중에 MXFP4 최적화 섹션에서 더 자세히 다루겠습니다.

### 2. GPT-2에서 시작하여 gpt-oss로의 진화

gpt-oss와 더 최신 아키텍처 간의 비교로 넘어가기 전에, 타임머신을 타고 GPT-2(그림 2)를 나란히 살펴보며 얼마나 많은 발전이 있었는지 확인해 봅시다.

**그림 2**: gpt-oss-20b와 GPT-2 XL 1.5B의 나란히 비교.

gpt-oss와 GPT-2는 모두 "Attention Is All You Need (2017)" 논문에서 소개된 트랜스포머(transformer) 아키텍처를 기반으로 구축된 디코더 전용(decoder-only) LLM입니다. 수년에 걸쳐 많은 세부 사항들이 발전했습니다. 하지만 이러한 변화는 gpt-oss에만 국한된 것이 아닙니다. 그리고 나중에 보겠지만, 이들은 다른 많은 LLM에서도 나타납니다. 이전 "대규모 아키텍처 비교(Big Architecture Comparison)" 글에서 이러한 측면들을 많이 다루었으므로, 각 소제목을 간결하고 집중적으로 유지하려고 노력할 것입니다.

#### 2.1 드롭아웃(Dropout) 제거 및 정규화의 변화

드롭아웃(Dropout, 2012)은 딥러닝 모델의 과적합(overfitting)을 방지하기 위해 훈련 중에 계층 활성화(layer activation)나 어텐션 점수(attention score)의 일부를 무작위로 0으로 설정하는 전통적인 기법입니다. 그러나 현대 대규모 언어 모델에서는 드롭아웃의 사용이 현저히 줄어들었으며, GPT-2 이후 대부분의 모델들은 이를 제거하는 추세입니다.

**그림 3**: 어텐션 점수 행렬(attention score matrix)에 적용된 드롭아웃의 예시.

LLM이 드롭아웃을 사용하지 않게 된 주된 이유는 훈련 방식의 변화에 있습니다. LLM은 일반적으로 방대한 데이터셋(dataset)에 대해 단일 에포크(epoch)로만 훈련되기 때문에, 각 토큰(token)을 한 번만 보게 됩니다. 이는 드롭아웃이 처음 도입되었을 때의 수백 에포크 훈련 방식과는 대조적이며, 과적합의 위험이 상대적으로 낮아집니다. 또한, 드롭아웃이 하위 태스크(downstream task) 성능을 저하시킬 수 있다는 연구 결과도 있습니다. 대신, LLM에서는 레이어 정규화(Layer Normalization)나 RMSNorm과 같은 다른 형태의 정규화 기법이 모델의 안정성과 성능 향상에 더 효과적인 것으로 입증되었습니다. 이러한 변화는 모델의 규모가 커지고 훈련 데이터가 방대해짐에 따라 정규화 전략이 어떻게 진화했는지를 보여줍니다.

#### 2.2 RoPE가 절대 위치 임베딩(Absolute Positional Embeddings)을 대체

트랜스포머 기반 LLM에서는 어텐션 메커니즘(attention mechanism) 때문에 위치 인코딩(positional encoding)이 필요합니다. 기본적으로 어텐션은 입력 토큰을 순서가 없는 것처럼 처리합니다. 원래 GPT 아키텍처에서는 절대 위치 임베딩(absolute positional embedding)이 시퀀스(sequence)의 각 위치에 대해 학습된 임베딩 벡터(embedding vector)를 추가하여 이 문제를 해결했으며(그림 4), 이는 토큰 임베딩에 더해집니다.

**그림 4**: 절대 위치 임베딩의 예시.

RoPE(Rotary Position Embedding)는 다른 접근 방식을 도입했습니다. 별도의 임베딩으로 위치 정보를 추가하는 대신, 각 토큰의 위치에 따라 쿼리(query) 및 키(key) 벡터를 회전시키는 방식으로 위치를 인코딩합니다. (RoPE는 우아한 아이디어이지만 설명하기 다소 까다로운 주제이기도 합니다. 언젠가 더 자세히 별도로 다룰 계획입니다.) 2021년에 처음 소개되었지만, RoPE는 2023년 오리지널 라마(Llama) 모델 출시와 함께 널리 채택되었으며 이후 현대 LLM의 필수 요소가 되었습니다. RoPE는 특히 긴 시퀀스에 대한 모델의 일반화 능력을 향상시키는 것으로 알려져 있으며, 외삽(extrapolation) 성능 면에서 절대 위치 임베딩보다 우수한 결과를 보여줍니다. 이는 RoPE가 상대적인 위치 정보에 더 중점을 두기 때문에, 훈련 시 보지 못했던 더 긴 시퀀스에서도 효과적으로 작동할 수 있음을 의미합니다.

#### 2.3 Swish/SwiGLU가 GELU를 대체

활성화 함수(activation function)는 신경망의 비선형성을 부여하여 복잡한 패턴을 학습할 수 있게 하는 핵심 요소입니다. 10여 년 전 딥러닝(deep learning) 커뮤니티가 ReLU에 대체로 정착하기 전까지는 활성화 함수에 대한 논쟁이 뜨거웠습니다. 그 이후로 연구자들은 ReLU의 성능을 유지하거나 개선하면서도 더 부드러운 곡선을 가진 다양한 변형들을 제안했으며, 그중 GELU와 Swish(그림 5)가 현대 LLM에서 널리 사용되고 있습니다.

**그림 5**: Swish와 GELU 활성화 함수의 비교. 둘 다 ReLU의 더 부드러운 버전입니다.

초기 GPT 아키텍처는 GELU를 사용했는데, 이는 `0.5x * [1 + erf(x / sqrt(2))]` 로 정의됩니다. 여기서 `erf`(오차 함수(error function)의 약자)는 가우시안(Gaussian)의 적분이며, 가우시안 적분의 다항식 근사(polynomial approximation)를 사용하여 계산됩니다. 이는 Swish에서 사용되는 시그모이드(sigmoid)와 같은 더 간단한 함수보다 계산 비용이 더 많이 듭니다. Swish는 단순히 `x * sigmoid(x)` 입니다. 실제로는 Swish가 GELU보다 계산적으로 약간 더 저렴하며, 이것이 대부분의 최신 모델에서 GELU를 대체한 주된 이유일 것입니다. 어떤 논문을 보느냐에 따라 모델링 성능 면에서 하나가 다소 더 나을 수 있습니다. 하지만 이러한 이득은 종종 표준 오차 범위 내에 있으며, 승자는 하이퍼파라미터 튜닝(hyperparameter tuning)에 크게 좌우될 것이라고 말하고 싶습니다. Swish는 오늘날 대부분의 아키텍처에서 사용됩니다. 하지만 GELU가 완전히 잊혀진 것은 아닙니다. 예를 들어, Google의 Gemma 모델은 여전히 GELU를 사용합니다.

그러나 더 주목할 만한 점은 피드 포워드 모듈(feed forward module, 작은 다층 퍼셉트론(multi-layer perceptron))이 게이트형(gated) "GLU"로 대체되었다는 것입니다. 여기서 GLU는 게이트형 선형 유닛(gated linear unit)의 약자이며 2020년 논문에서 제안되었습니다. 구체적으로, 2개의 완전 연결 계층(fully connected layer)이 아래 그림 6과 같이 사용되는 3개의 완전 연결 계층으로 대체됩니다.

**그림 6**: Swish와 GELU, 그리고 그들의 게이트형 대응물인 SwiGLU와 GEGLU의 비교.

언뜻 보기에는 GEGLU/SwiGLU 변형이 추가 계층으로 인해 단순히 더 많은 파라미터(parameter)를 가지고 있기 때문에 일반적인 피드 포워드 계층보다 더 나을 수 있다고 보일 수 있습니다. 하지만 이것은 착각입니다. 실제로는 SwiGLU/GEGLU의 W 및 V 가중치 계층이 전통적인 피드 포워드 계층의 W_1 계층 크기의 절반으로 선택되는 경우가 많기 때문입니다. 이를 더 잘 설명하기 위해 일반 및 GLU 변형의 구체적인 코드 구현을 살펴보겠습니다.

**그림 7**: 일반 피드 포워드 모듈(상단)과 SwiGLU 변형(하단)이 나란히 있습니다. Swish 함수는 PyTorch에서 "silu"로 구현됩니다.

```python
# Regular feed forward module
fc1 = nn.Linear(embedding_dim, 4 * embedding_dim)
fc2 = nn.Linear(4 * embedding_dim, embedding_dim)

# SwiGLU variant
fc1_v = nn.Linear(embedding_dim, 2 * embedding_dim)
fc1_w = nn.Linear(embedding_dim, 2 * embedding_dim)
fc2 = nn.Linear(2 * embedding_dim, embedding_dim)
```

따라서 임베딩 차원(embedding dimension)이 1024라고 가정해 봅시다.

일반 피드 포워드(feed forward)의 경우, 다음과 같습니다:
`fc1: 1024 × 4096 = 4,194,304`
`fc2: 4096 × 1024 = 4,194,304`
즉, `fc1 + fc2 = 8,388,608`개의 파라미터입니다.

GLU 변형의 경우, 다음과 같습니다:
`fc1_v: 1024 × 2048 = 2,097,152`
`fc1_w: 1024 × 2048 = 2,097,152`
`fc2: 2048 × 1024 = 2,097,152`
즉, `3 × 2,097,152 = 6,291,456`개의 가중치 파라미터입니다.

따라서 전반적으로 GLU 변형을 사용하면 파라미터 수가 줄어들고 성능도 더 좋습니다. 이러한 더 나은 성능의 이유는 GLU 변형이 추가적인 곱셈 상호작용(multiplicative interaction)을 제공하여 표현력(expressivity)을 향상시키기 때문입니다(이는 잘 훈련되었을 때 깊고 얇은 신경망(neural net)이 얕고 넓은 신경망보다 더 나은 성능을 보이는 것과 같은 이유입니다).

#### 2.4 전문가 혼합(Mixture-of-Experts)이 단일 피드 포워드 모듈을 대체

이전 섹션에서 논의했듯이, 피드 포워드 모듈을 SwiGLU로 업그레이드하는 것 외에도, gpt-oss는 단일 피드 포워드 모듈을 여러 피드 포워드 모듈로 대체하며, 각 토큰 생성 단계에서 부분집합만 사용합니다. 이 접근 방식은 MoE(Mixture-of-Experts, 전문가 혼합)로 알려져 있으며 아래 그림 8에 나와 있습니다.

**그림 8**: 피드 포워드 모듈이 MoE(Mixture-of-Expert) 모듈로 대체됩니다.

따라서 단일 피드 포워드 모듈을 여러 피드 포워드 모듈로 대체하면(MoE 설정에서와 같이) 모델의 총 파라미터 수가 상당히 증가합니다. 하지만 핵심 비결은 모든 토큰에 대해 모든 전문가를 사용("활성화")하지 않는다는 것입니다. 대신, 라우터(router)는 토큰당 소수의 전문가 부분집합만을 선택합니다. 한 번에 소수의 전문가만 활성화되기 때문에, MoE 모듈은 항상 전체 파라미터 세트(parameter set)를 사용하는 밀집(dense) 모듈과 대조적으로 희소(sparse)하다고 불립니다.

MoE 아키텍처는 모델의 용량(capacity)을 크게 확장하면서도 추론(inference) 비용을 효과적으로 관리할 수 있게 해줍니다. 즉, 더 많은 지식을 습득할 수 있는 잠재력을 가지면서도, 모든 파라미터를 동시에 활성화할 필요가 없어 효율적인 추론이 가능합니다. 이 희소성(sparsity)은 MoE 모델의 핵심 장점 중 하나이며, 특히 대규모 모델에서 컴퓨팅 자원을 절약하는 데 기여합니다. 최근에는 라우터의 성능을 개선하고, 전문가 간의 부하 균형을 최적화하며, 전문가의 전문화(specialization)를 유도하는 다양한 연구들이 활발히 진행되고 있습니다. 이는 MoE 모델이 단순한 파라미터 확장 이상의 복잡한 최적화 영역임을 시사합니다. (재미있는 사실: 대부분의 MoE 모델에서 전문가 가중치(expert weight)는 전체 모델 파라미터의 90% 이상을 차지합니다.)

#### 2.5 그룹화된 쿼리 어텐션(Grouped Query Attention)이 다중 헤드 어텐션(Multi-Head Attention)을 대체

이전 글에서 언급했듯이, GQA(Grouped Query Attention, 그룹화된 쿼리 어텐션)는 최근 몇 년 동안 MHA(Multi-Head Attention, 다중 헤드 어텐션)에 비해 더 계산 및 파라미터 효율적인 대안으로 부상했습니다. MHA에서는 각 헤드(head)가 자체적인 키(key)와 값(value) 세트를 가집니다. GQA는 여러 헤드를 그룹화하여 동일한 키 및 값 투영(projection)을 공유함으로써 메모리 사용량을 줄입니다. 예를 들어, 그림 9에 나와 있듯이, 2개의 키-값 그룹과 4개의 어텐션 헤드가 있다면, 헤드 1과 2는 하나의 키 및 값 세트를 공유하고, 헤드 3과 4는 다른 세트를 공유할 수 있습니다. 절제 연구(ablation study)에 따르면, 이러한 그룹화는 키 및 값 계산의 총량을 감소시켜 메모리 사용량을 줄이고 효율성을 향상시키면서 모델링 성능에 눈에 띄는 영향을 미치지 않습니다.

**그림 9**: MHA와 GQA의 비교. 여기서 그룹 크기는 2이며, 키와 값 쌍이 2개의 쿼리(query) 간에 공유됩니다.

GQA의 핵심 아이디어는 여러 쿼리 헤드에 걸쳐 키와 값 헤드를 공유함으로써 그 수를 줄이는 것입니다. 이는 (1) 모델의 파라미터 수를 낮추고 (2) 추론 시 KV 캐시(KV cache)에서 저장하고 검색해야 하는 키와 값이 적기 때문에 키 및 값 텐서(tensor)에 대한 메모리 대역폭(bandwidth) 사용량을 줄입니다. (GQA가 코드에서 어떻게 보이는지 궁금하다면, KV 캐시가 없는 버전에 대한 저의 GPT-2에서 Llama 3 변환 가이드와 여기에서 KV 캐시 변형을 참조하십시오.) GQA는 주로 MHA에 대한 계산 효율성 해결책이지만, 절제 연구(원래 GQA 논문 및 Llama 2 논문의 연구와 같은)는 LLM 모델링 성능 면에서 표준 MHA와 유사하게 작동함을 보여줍니다. 또한 GQA의 변형인 MQA(Multi-Query Attention)는 모든 쿼리 헤드가 하나의 키 및 값 헤드를 공유하여 메모리 효율성을 극대화하지만, 때때로 성능 저하가 발생할 수 있습니다. GQA는 MHA와 MQA 사이의 균형점을 찾아, 효율성과 성능을 모두 잡으려는 시도라고 볼 수 있습니다.

#### 2.6 슬라이딩 윈도우 어텐션(Sliding Window Attention)

슬라이딩 윈도우 어텐션(Sliding-window attention, 아래 그림 10)은 LongFormer 논문(2020)에서 처음 소개되었고 나중에 Mistral에 의해 대중화되었습니다. 흥미롭게도, gpt-oss는 이를 매 두 번째 계층에 적용합니다. 이를 다중 헤드 어텐션(multi-head attention)의 변형, 또는 이 경우 그룹화된 쿼리 어텐션(GQA)의 변형으로 생각할 수 있습니다. 여기서 어텐션 컨텍스트(attention context)는 더 작은 윈도우(window)로 제한되어 메모리 사용량과 계산 비용을 모두 줄입니다.

**그림 10**: 일반 어텐션(왼쪽)과 슬라이딩 윈도우 어텐션(오른쪽)의 비교.

구체적으로, gpt-oss는 전체 컨텍스트(context)에 어텐션하는 GQA 계층과 128개 토큰으로 제한된 슬라이딩 윈도우를 가진 GQA 계층을 번갈아 사용합니다. 이전 글에서 논의했듯이, Gemma 2 (2024)는 유사한 1:1 비율을 사용했습니다. 올해 초 Gemma 3는 훨씬 더 나아가 5:1 비율로 전환했습니다. 이는 5개의 슬라이딩 윈도우(로컬) 어텐션 계층마다 하나의 전체 어텐션 계층만 사용한다는 의미입니다. Gemma 절제 연구에 따르면, 슬라이딩 윈도우 어텐션은 아래 그림에 나와 있듯이 모델링 성능에 미치는 영향이 미미합니다. Gemma 2의 윈도우 크기는 4096개 토큰이었고, Gemma 3는 이를 1024개로 줄였습니다. gpt-oss에서는 윈도우가 단 128개 토큰으로, 놀랍도록 작습니다.

이처럼 슬라이딩 윈도우 어텐션은 긴 컨텍스트를 효율적으로 처리하기 위한 중요한 전략으로 자리 잡았습니다. 특히, 어텐션 메커니즘의 2차적인 계산 복잡성(O(N^2))을 선형 복잡성(O(N))으로 줄여주어, 훨씬 더 긴 입력 시퀀스를 처리할 수 있게 합니다. gpt-oss의 128개 토큰이라는 작은 윈도우 크기는 모델이 주로 지역적인 정보에 집중하면서도, 간헐적으로 전체 어텐션 계층을 통해 전역적인 컨텍스트를 통합하는 방식으로 작동함을 시사합니다. 이는 지역적 정보와 전역적 정보 간의 균형을 찾는 효율적인 방법으로, 특히 장문 이해나 코드 생성과 같은 작업에서 유용할 수 있습니다.

그리고 재미있는 사실로, 공식 발표 글에서는 슬라이딩 윈도우 어텐션이 GPT-3에서 이미 사용되었던 것으로 보인다고 언급합니다.

> 이 모델들은 GPT-3와 유사하게 밀집(dense) 및 지역적으로 밴드형(banded) 희소(sparse) 어텐션 패턴을 번갈아 사용합니다.

누가 알았겠어요!? 저는 원래 GPT-3 논문으로 돌아갔고, 실제로 거기에 언급되어 있었습니다.

> 우리는 GPT-2 [RWC+19]와 동일한 모델 및 아키텍처를 사용하며, 여기에 설명된 수정된 초기화(initialization), 사전 정규화(pre-normalization) 및 가역 토큰화(reversible tokenization)를 포함합니다. 단, 트랜스포머 계층에서 Sparse Transformer [CGRS19]와 유사하게 밀집 및 지역적으로 밴드형 희소 어텐션 패턴을 번갈아 사용한다는 점은 예외입니다.

#### 2.7 RMSNorm이 LayerNorm을 대체

마지막으로, GPT-2에서 비롯된 마지막 작은 조정은 최근 몇 년간 흔한 추세였던 LayerNorm (2016)을 RMSNorm (2019)으로 대체하는 것입니다. GELU를 Swish 및 SwiGLU로 교체하는 것과 마찬가지로, RMSNorm은 이러한 작지만 합리적인 효율성 개선 중 하나입니다. RMSNorm은 아래 그림 11에 나와 있듯이 계층 활성화(layer activation)를 정규화(normalize)하는 목적에서 LayerNorm과 유사합니다. 그리 오래 전이 아니더라도, BatchNorm이 이 작업에 가장 많이 사용되는 선택지였다는 것을 기억할 것입니다. 이후 BatchNorm은 효율적으로 병렬화(parallelize)하기 어렵고(평균 및 분산 배치 통계(mean and variance batch statistics) 때문에) 작은 배치 크기(batch size)에서는 성능이 좋지 않아 선호도가 떨어졌습니다.

**그림 11**: 작은 선형 계층(linear layer)에 대한 LayerNorm(왼쪽)과 RMSNorm(오른쪽)의 비교.

위 그림 11에서 볼 수 있듯이, LayerNorm과 RMSNorm 모두 계층 출력(layer output)을 합리적인 범위 내로 스케일링(scale)합니다. LayerNorm은 평균을 빼고 표준 편차(standard deviation)로 나누어 계층 출력이 평균 0과 단위 분산(unit variance, 분산 1, 표준 편차 1)을 갖도록 합니다. RMSNorm은 입력을 제곱평균제곱근(root-mean-square)으로 나눕니다. 이는 활성화(activation)를 유사한 크기로 스케일링하면서 평균 0 또는 단위 분산을 강제하지 않습니다. 그림 11에 나와 있는 이 특정 예시에서 평균은 0.77이고 분산은 0.41입니다.

LayerNorm과 RMSNorm 모두 활성화 스케일(activation scale)을 안정화하고 최적화(optimization)를 개선하지만, RMSNorm은 계산 비용이 더 저렴하기 때문에 대규모 LLM에서 종종 선호됩니다. LayerNorm과 달리 RMSNorm은 바이어스(bias, 이동) 항이 없으며, 비용이 많이 드는 평균 및 분산 계산을 단일 제곱평균제곱근 연산으로 줄입니다. 이는 교차 특징 감소(cross-feature reduction) 횟수를 2개에서 1개로 줄여 GPU의 통신 오버헤드(communication overhead)를 낮추고 훈련 효율성을 향상시킵니다. 그림 12는 이것이 코드에서 어떻게 보이는지 보여줍니다.

**그림 12**: RMSNorm이 계산적으로 더 간단함을 보여주는 LayerNorm 및 RMSNorm의 코드 구현.

```python
# LayerNorm
mean = x.mean(dim=-1, keepdim=True)
var = x.var(dim=-1, keepdim=True, unbiased=False)
x_norm = (x - mean) / torch.sqrt(var + eps)

# RMSNorm
rms = torch.sqrt(torch.mean(x**2, dim=-1, keepdim=True) + eps)
x_norm = x / rms
```

#### 2.8 GPT-2의 유산

저는 여전히 GPT-2가 LLM에 대해 배울 때 훌륭한 초보자용 아키텍처라고 생각합니다. 최적화 트릭의 여러 계층에 갇히지 않고 이해하기에 충분히 간단하지만, 현대 트랜스포머 모델이 어떻게 작동하는지에 대한 확실한 이해를 제공할 만큼 여전히 복잡합니다. GPT-2로 시작함으로써, 새로운 아키텍처에서 발견되는 추가 기능과 조정에 압도되지 않고 기본(어텐션 메커니즘, 위치 임베딩, 정규화(normalization), 전반적인 훈련 파이프라인(training pipeline))에 집중할 수 있습니다. 사실, 저는 새로운 변경 사항을 추가하기 전에 GPT-2에 대해 먼저 배우고 심지어 구현하는 것이 시간을 들일 가치가 있다고 생각합니다. 그렇게 하면 그러한 변경 사항을 더 쉽게 이해할 수 있을 뿐만 아니라, 그들이 해결하려는 한계나 문제가 무엇인지 더 잘 이해하게 되므로 더 높이 평가하게 될 것입니다.

예를 들어, 저의 GPT-2 코드를 시작으로 최근 Qwen3 아키텍처를 처음부터 구현했는데, 이는 gpt-oss와 매우 유사합니다. 이는 다음 주제로 이어집니다: gpt-oss를 더 최신 아키텍처와 비교하기.

GPT-2의 단순성은 오늘날의 복잡한 LLM 아키텍처를 이해하기 위한 훌륭한 토대를 제공합니다. 이는 기본적인 트랜스포머 블록의 작동 방식, 어텐션 메커니즘의 역할, 그리고 위치 정보 인코딩의 중요성을 명확하게 보여줍니다. 이 기본 지식을 바탕으로 MoE, GQA, 슬라이딩 윈도우 어텐션과 같은 고급 기술들이 왜 도입되었고, 어떤 문제를 해결하는지 더 쉽게 파악할 수 있습니다. 실제로 많은 최신 LLM 프레임워크나 라이브러리들도 GPT-2의 구조를 기반으로 확장되거나 변형된 형태를 취하고 있어, GPT-2에 대한 이해는 현대 LLM 개발의 필수적인 부분이라고 할 수 있습니다.

Ahead of AI는 독자 지원 출판물입니다. 새로운 게시물을 받고 저의 작업을 지원하려면 무료 또는 유료 구독자가 되는 것을 고려해 보세요.
구독하기

### 3. gpt-oss를 최신 아키텍처(Qwen3)와 비교

이제 GPT-2에서 GPT OSS로의 진화를 살펴보았으니, 다음 단계로 나아가 GPT OSS를 2025년 5월에 3개월 먼저 출시된 더 최신 아키텍처인 Qwen3와 비교할 수 있습니다. 제가 여기서 Qwen3를 선택하는 이유는 이 글을 쓰는 시점에서 Qwen3가 최고의 오픈 웨이트 모델 중 하나이기 때문입니다. 또한, Qwen3 MoE 모델 중 하나는 훈련 가능한 파라미터(trainable parameter) 측면에서 상대적으로 유사한 전체 크기를 가지고 있어 GPT OSS와 거의 직접적으로 비교할 수 있습니다. 아래 그림 13은 gpt-oss-20b와 유사한 크기의 Qwen3 모델을 비교합니다.

**그림 13**: 유사한 크기의 gpt-oss 및 Qwen3 모델의 나란히 비교.

보시다시피, gpt-oss 20B와 Qwen3 30B-A3B는 아키텍처 구성 요소에서 매우 유사합니다. 여기서 주요 차이점은 차원(dimension) 외에도 gpt-oss는 섹션 1.6에서 이전에 논의했듯이 슬라이딩 윈도우 어텐션(sliding window attention)을 사용하지만(이 그림에는 표시되지 않음), Qwen3는 그렇지 않다는 것입니다. 다음 소제목들에서 주목할 만한 세부 사항들을 하나씩 살펴보겠습니다.

#### 3.1 너비 대 깊이: 모델 스케일링의 전략적 선택

두 모델을 자세히 살펴보면, Qwen3는 24개가 아닌 48개의 트랜스포머 블록(transformer block)을 가진 훨씬 더 깊은 아키텍처임을 알 수 있습니다(그림 14). 이는 모델이 더 많은 계층을 통해 데이터를 처리하고 추상적인 특징을 학습할 수 있음을 의미합니다.

**그림 14**: Qwen3는 gpt-oss-20b보다 두 배 많은 트랜스포머 블록을 가지고 있습니다.

반면에 gpt-oss는 훨씬 더 넓은 아키텍처입니다.

*   2048이 아닌 2880의 임베딩 차원(embedding dimension)
*   또한 768이 아닌 2880의 중간 전문가(피드 포워드) 투영 차원(projection dimension)

gpt-oss가 두 배 많은 어텐션 헤드(attention head)를 사용한다는 점도 주목할 만하지만, 이것이 모델의 너비를 직접적으로 증가시키지는 않습니다. 너비는 임베딩 차원에 의해 결정됩니다.

고정된 파라미터 수에서 한 접근 방식이 다른 접근 방식보다 이점을 제공할까요? 일반적으로, 더 깊은 모델은 더 많은 유연성을 가지지만, 폭발 및 소실 기울기(exploding and vanishing gradient)로 인한 불안정성 문제 때문에 훈련하기 더 어려울 수 있습니다(RMSNorm과 숏컷 연결(shortcut connection)이 이를 완화하는 것을 목표로 합니다). 더 넓은 아키텍처는 더 높은 메모리 비용으로 더 나은 병렬화(parallelization) 덕분에 추론 시 더 빠르다는(더 높은 초당 토큰 처리량(tokens/second throughput)을 가짐) 장점이 있습니다.

모델링 성능에 관해서는, 불행히도 제가 아는 한 좋은 동등 비교(parameter size와 데이터셋이 일정하게 유지되는)는 없습니다. 다만 Gemma 2 논문(표 9)의 절제 연구에서 90억 파라미터 아키텍처의 경우 더 넓은 설정이 더 깊은 설정보다 약간 더 낫다는 것을 발견했습니다. 4개의 벤치마크에서 더 넓은 모델은 평균 52.0점을 달성했고, 더 깊은 모델은 평균 50.8점을 달성했습니다. 최근 연구들은 모델의 깊이와 너비를 어떻게 균형 있게 스케일링할지에 대한 다양한 접근 방식을 제시하고 있습니다. 예를 들어, 일부 연구는 초기 계층을 더 넓게, 후기 계층을 더 깊게 구성하는 하이브리드 접근 방식이 효율적일 수 있다고 제안하기도 합니다. 이는 특정 작업이나 컴퓨팅 환경에 따라 최적의 아키텍처 스케일링 전략이 달라질 수 있음을 시사합니다.

#### 3.2 적은 수의 대규모 전문가 대 많은 수의 소규모 전문가

위 그림 14에서 볼 수 있듯이, gpt-oss가 놀랍도록 적은 수의 전문가(128개 대신 32개)를 가지고 있으며, 토큰당 8개가 아닌 4개의 활성 전문가만 사용한다는 점도 주목할 만합니다. 하지만 각 전문가는 Qwen3의 전문가보다 훨씬 더 큽니다. 이는 최근 추세와 발전이 더 많고 작은 모델이 유리하다는 점을 시사하기 때문에 흥미롭습니다. 이 변화는 총 파라미터 크기가 일정할 때, DeepSeekMoE 논문의 아래 그림 15에 잘 설명되어 있습니다.

**그림 15**: "DeepSeekMoE: Mixture-of-Experts 언어 모델에서 궁극적인 전문가 전문화를 향하여" 논문의 주석이 달린 그림, https://arxiv.org/abs/2401.06066

DeepSeekMoE 연구는 전문가의 수와 크기가 모델의 성능에 미치는 영향을 심층적으로 분석했습니다. 이들은 전문가의 수가 많을수록 각 전문가가 더 전문화될 수 있고, 이는 모델의 전반적인 지식 습득 능력을 향상시킨다는 것을 보여주었습니다. 또한, 공유 전문가(shared expert)의 도입은 모델의 안정성을 높이고 일부 태스크에서 성능을 개선할 수 있음을 시사했습니다. 특히, DeepSeekMoE는 극도로 많은 수의 전문가(1000개 이상)를 사용하면서도 각 전문가의 크기를 줄여 효율적인 훈련과 추론을 달성했습니다. 이러한 관점에서 gpt-oss의 상대적으로 적은 전문가 수는 다소 보수적인 접근 방식으로 보일 수 있습니다.

특히, DeepSeek의 모델과 달리 gpt-oss와 Qwen3 모두 공유 전문가(shared expert)를 사용하지 않습니다. 공정하게 말하면, gpt-oss의 적은 수의 전문가는 20B 크기의 부작용일 수 있습니다. 아래 120B 모드를 보면, 아래 그림 16에 나와 있듯이 다른 모든 것을 고정한 채 전문가 수(및 트랜스포머 블록)를 실제로 늘렸습니다.

**그림 16**: 두 gpt-oss 아키텍처의 나란히 비교. 더 큰 120B 모델은 트랜스포머 블록 수와 전문가 수만 스케일링(scaling)합니다.

20B와 120B 모델이 매우 유사하다는 지루한 설명은 아마도 120B 모델이 주요 초점이었기 때문일 것입니다. 그리고 더 작은 모델을 만드는 가장 쉬운 방법은 모델을 약간 더 짧게(더 적은 트랜스포머 블록) 만들고 전문가 수를 줄이는 것이었습니다. 대부분의 파라미터가 그곳에 있기 때문입니다. 하지만 그들이 120B 모델을 훈련하기 시작한 다음, 일부 트랜스포머 블록과 전문가를 잘라내어 계속적인 사전 훈련(pre-training)을 했는지(무작위 가중치에서 시작하는 대신) 추측해 볼 수도 있습니다. 어떤 경우든, 이 두 가지(트랜스포머 블록과 전문가 수)만 스케일링하는 것은 매우 이례적이기 때문입니다. 예를 들어, 여러 크기의 Qwen3 MoE 모델(아래 그림 17)을 보면, 훨씬 더 많은 측면에서 서로 더 비례적으로 스케일링되었습니다.

**그림 17**: 다양한 Qwen3 모델의 아키텍처 차이.

#### 3.3 어텐션 바이어스(Attention Bias) 및 어텐션 싱크(Attention Sinks)

gpt-oss와 Qwen3 모두 그룹화된 쿼리 어텐션(grouped query attention)을 사용합니다. 주요 차이점은 gpt-oss가 이전에 언급했듯이 각 두 번째 계층에서 슬라이딩 윈도우 어텐션(sliding window attention)을 통해 컨텍스트 크기(context size)를 제한한다는 것입니다. 하지만 제 눈길을 끈 한 가지 흥미로운 세부 사항이 있습니다. 아래 그림에 나와 있듯이, gpt-oss는 어텐션 가중치(attention weight)에 바이어스 유닛(bias unit)을 사용하는 것으로 보입니다.

**그림 18**: gpt-oss 모델은 어텐션 계층에 바이어스 유닛을 사용합니다. 여기에서 코드 예시를 참조하십시오.

저는 GPT-2 시절 이후로 이러한 바이어스 유닛이 사용되는 것을 본 적이 없으며, 일반적으로 불필요하다고 여겨집니다. 실제로, 저는 이것이 적어도 키 변환(key transformation, k_proj)에 대해서는 수학적으로 사실임을 보여주는 최근 논문을 찾았습니다. 더욱이, 경험적 결과는 바이어스 유닛의 유무에 따른 차이가 거의 없음을 보여줍니다(아래 그림 19 참조).

**그림 19**: https://arxiv.org/pdf/2302.08626에서 가져온 표로, 바이어스 유닛의 유무에 따라 모델을 처음부터 훈련했을 때의 평균 테스트 손실(test loss)을 보여줍니다.

여러분이 눈치챘을 또 다른 세부 사항은 그림 18의 코드 스크린샷에 있는 싱크(sinks)의 정의입니다. 일반적인 모델에서 어텐션 싱크(attention sink)는 어텐션을 안정화하기 위해 시퀀스 시작 부분에 배치되는 특별한 "항상 어텐션되는" 토큰으로, 긴 컨텍스트 시나리오에서 특히 유용합니다. 즉, 컨텍스트가 매우 길어지더라도 시작 부분의 이 특별한 어텐션되는 토큰은 여전히 어텐션되며, 전체 시퀀스에 대한 일반적으로 유용한 정보를 저장하는 방법을 학습할 수 있습니다. (원래 "Efficient Streaming Language Models with Attention Sinks" 논문에서 제안된 것으로 생각합니다.) gpt-oss 구현에서 어텐션 싱크는 입력 시퀀스(input sequence)의 실제 토큰이 아닙니다. 대신, 이들은 어텐션 점수(attention score)에 추가되는 학습된 헤드별 바이어스 로짓(per-head bias logit)입니다(그림 20). 목표는 위에서 언급한 어텐션 싱크와 동일하지만, 토큰화된 입력(tokenized input)을 수정하지 않습니다.

**그림 20**: gpt-oss에서 어텐션 싱크의 사용; 여기 Hugging Face 코드를 기반으로 합니다.

이러한 어텐션 싱크의 구현 방식은 기존의 컨텍스트 창(context window) 제한을 넘어 모델이 장기적인 의존성(long-term dependencies)을 더 잘 유지하도록 돕는 정교한 방법입니다. 학습된 바이어스 로짓을 사용하는 것은 모델이 특정 위치나 정보에 대한 영구적인 "기억"을 내재화하도록 하여, 긴 시퀀스에서도 일관된 성능을 유지할 수 있게 합니다. 이는 스트리밍 환경이나 지속적으로 확장되는 대화에서 모델의 견고성을 높이는 데 기여합니다.

#### 3.4 라이선스

마지막으로, Qwen3와 유사하게 gpt-oss 모델은 Apache 2.0 오픈 소스 라이선스(open-source license)를 따르며, 이는 훌륭합니다(제가 제 오픈 소스 프로젝트에 선호하는 라이선스와 동일합니다). 이는 모델이 다른 모델로 증류(distill)되거나 상업용 제품에 제한 없이 사용될 수 있음을 의미합니다.

오픈 웨이트(open-weight) 대 오픈 소스(open-source) LLM. 이 구분은 수년 동안 논의되어 왔지만, 이번 출시와 그 결과물에 대한 혼란을 피하기 위해 명확히 할 가치가 있습니다. 일부 모델 개발자들은 모델 가중치(model weight)와 추론 코드(inference code)만 공개하는 반면(예: Llama, Gemma, gpt-oss), 다른 개발자들(예: OLMo)은 훈련 코드(training code), 데이터셋, 가중치를 포함한 모든 것을 진정한 오픈 소스로 공개합니다. 이러한 엄격한 정의에 따르면, gpt-oss는 가중치와 추론 코드를 포함하지만 훈련 코드나 데이터셋은 포함하지 않으므로 오픈 웨이트 모델입니다(Qwen3와 마찬가지로). 하지만 이 용어는 업계 전반에 걸쳐 일관성 없이 사용됩니다. 저는 "gpt-oss"의 "oss"가 오픈 소스 소프트웨어(open source software)를 의미한다고 가정하지만, OpenAI 자체가 공식 발표 글에서 gpt-oss를 오픈 웨이트 모델로 명확하게 설명한 것에 긍정적으로 놀랐습니다.

Apache 2.0 라이선스는 상업적 사용을 포함한 광범위한 활용을 허용하므로, 개발자와 기업 모두에게 매우 매력적입니다. 이는 gpt-oss 모델이 단순히 연구용 도구를 넘어, 다양한 실제 응용 프로그램에 통합될 수 있는 강력한 기반을 제공합니다. 이러한 개방성은 오픈 소스 생태계의 성장을 촉진하고, LLM 기술의 민주화를 가속화하는 데 중요한 역할을 합니다.

### 4. 기타 흥미로운 정보

이전 섹션에서는 GPT-2 이후 아키텍처가 어떻게 발전했는지 설명하고 Qwen3(및 대부분의 다른 최신 모델)와의 유사점을 논의했지만, 아직 언급하지 않은 몇 가지 추가적이지만 주목할 만한 세부 사항들이 있습니다. 이들은 이전 섹션에 깔끔하게 들어맞지 않았지만 여전히 언급할 가치가 있는 점들입니다.

#### 4.1 훈련 개요 및 데이터셋 전략

불행히도, 훈련 세트(training set) 크기와 알고리즘에 대한 정보는 많지 않습니다. 아래에 모델 카드 보고서(1)와 발표 게시물(2)에서 가장 흥미로운 조각들을 추가했습니다.

*   gpt-oss 모델은 우리의 가장 진보된 사전 훈련(pre-training) 및 사후 훈련(post-training) 기술을 사용하여 훈련되었습니다 [...] (1)
*   [...] 완료하는 데 210만 H100-시간이 필요했으며, gpt-oss-20b는 거의 10배 적게 필요했습니다. (1)
*   [...] 지도 미세 조정(supervised fine-tuning) 단계와 고성능 RL(강화 학습) 단계(high-compute RL stage)를 포함합니다 [...] (2)
*   우리는 주로 영어 텍스트 전용 데이터셋에서 모델을 훈련했으며, STEM, 코딩 및 일반 지식에 중점을 두었습니다. (2)

따라서 gpt-oss 모델은 추론 모델(reasoning model)이라는 것을 알 수 있습니다. 210만 H100 GPU 시간의 훈련 계산량은 약 5.6배 더 큰 DeepSeek V3 모델이 훈련된 278.8만 H800 GPU 시간과 거의 비슷합니다. 불행히도 Qwen3의 훈련 시간에 대한 정보는 아직 없습니다. 흥미롭게도, GPT-oss 훈련 시간 추정치는 지시 따르기(instruction following)를 위한 지도 학습(supervised learning)과 추론을 위한 강화 학습(reinforcement learning)을 모두 포함하는 반면, DeepSeek V3는 DeepSeek R1이 별도로 훈련된 사전 훈련된 기본 모델(pre-trained base model)일 뿐입니다.

gpt-oss의 훈련 전략에서 특히 주목할 만한 점은 "STEM, 코딩 및 일반 지식"에 중점을 둔 데이터셋 구성입니다. 이는 모델이 단순히 언어 유창성을 넘어, 복잡한 문제 해결 능력과 특정 도메인 지식을 효과적으로 학습하도록 설계되었음을 의미합니다. 최근 LLM 훈련의 추세는 단순히 데이터의 양을 늘리는 것을 넘어, 데이터의 품질과 다양성, 그리고 특정 작업에 최적화된 데이터셋 큐레이션(curation)에 더 많은 초점을 맞추고 있습니다. 예를 들어, 수학적 추론이나 코드 생성과 같은 특정 능력을 강화하기 위해 합성 데이터(synthetic data)를 활용하거나, 특정 유형의 데이터를 반복적으로 훈련하는 "데이터 큐레이션" 기법이 중요하게 다뤄지고 있습니다. 이러한 접근 방식은 모델이 특정 강점을 가지도록 유도하고, 일반적인 상식이나 창의적 글쓰기 능력과 함께 전문적인 역량을 겸비하게 만듭니다.

#### 4.2 추론 노력: 유연한 성능 제어

이전 섹션에서 언급했듯이, gpt-oss 모델은 추론 모델입니다. 하지만 특히 흥미로운 점은 사용자가 추론 시간 스케일링(inference time scaling)을 통해 추론의 정도를 쉽게 제어할 수 있도록 훈련되었다는 것입니다. 구체적으로, gpt-oss 모델은 시스템 프롬프트(system prompt)의 일부로 "추론 노력: 낮음/중간/높음" 지시를 받을 수 있으며, 이는 그림 21에 나와 있듯이 응답 길이와 정확도에 직접적인 영향을 미칩니다.

**그림 21**: 다양한 추론 노력 하에서의 gpt-oss 모델의 응답 길이 및 품질 (모델 카드에서 가져온 주석이 달린 그림)

이러한 조정 가능성은 비용, 계산량, 정확도의 균형을 맞출 수 있게 해주므로 유용합니다. 예를 들어, 간단한 지식 질문에 답하거나 작은 오타를 수정하는 것과 같이 작업이 간단하다면, 확장된 추론을 건너뛸 수 있습니다. 이는 불필요하게 긴 응답과 장황한 추론 흔적을 피하면서 시간과 자원을 절약합니다.

Qwen3나 OLMo와 달리 OpenAI가 강화 학습 기반 추론 훈련 전에 기본 모델을 공개하지 않은 것은 다소 아쉽습니다. 기본 모델은 추론 방법론을 연구하는 연구자들에게 특히 귀중한 출발점입니다(제가 현재 Qwen3 Base로 작업하는 것을 좋아하는 이유 중 하나입니다). 제 생각에 OpenAI의 결정은 연구적 고려보다는 산업 및 생산 사용 사례에 의해 더 많이 좌우되었을 것입니다.

원래 Qwen3 모델에도 사고(추론) 모드를 활성화/비활성화하는 토글(toggle)이 있습니다(토크나이저(tokenizer)의 `enable_thinking=True/False` 설정으로, 추론 동작을 비활성화하기 위해 단순히 `<think></think>` 태그를 추가합니다). 하지만 Qwen3 팀은 지난 몇 주 동안 모델을 업데이트하여 하이브리드 모델에서 전용 Instruct/Thinking/Coder 변형으로 전환했습니다. 그 이유는 하이브리드 모드가 개별 모델에 비해 성능이 낮았기 때문입니다.

> 커뮤니티와 논의하고 이 문제를 숙고한 결과, 우리는 하이브리드 사고 모드를 포기하기로 결정했습니다. 이제 최상의 품질을 달성하기 위해 Instruct 및 Thinking 모델을 별도로 훈련할 것입니다.
> 출처

gpt-oss의 "추론 노력" 제어 기능은 사용자에게 모델의 행동을 미세 조정할 수 있는 강력한 도구를 제공합니다. 이는 특히 실시간 응용 프로그램이나 자원 제약이 있는 환경에서 모델을 효율적으로 활용하는 데 필수적입니다. 예를 들어, 고객 서비스 챗봇에서는 빠른 응답이 중요하므로 "낮음" 노력 설정을 사용할 수 있고, 복잡한 기술 문서 작성에는 "높음" 노력 설정을 통해 더 깊이 있는 분석과 정확한 정보를 얻을 수 있습니다. 이러한 유연성은 LLM이 다양한 산업 분야에서 더욱 광범위하게 채택될 수 있도록 하는 중요한 요소입니다.

#### 4.3 MXFP4 최적화: 로컬 배포의 새로운 지평

한 가지 흥미로운 놀라움은 OpenAI가 MoE 전문가를 위한 MXFP4 양자화(quantization) 방식을 사용하여 gpt-oss 모델을 출시했다는 것입니다. 양자화 형식은 주로 모바일 또는 임베디드 AI와 관련된 틈새 주제였지만, 더 큰 모델로의 전환과 함께 변화했습니다. 이 경우, MXFP4 최적화는 모델이 단일 GPU 장치에서 실행될 수 있도록 합니다.

실제로는 다음과 같습니다:

*   대규모 모델(120B)은 단일 80GB H100 또는 더 새로운 GPU에 들어맞습니다. 소비자 하드웨어는 아니지만, 멀티 H100 머신보다 1-H100 머신을 빌리는 것이 훨씬 저렴합니다. 게다가, 모델을 여러 GPU에 분산시키고 통신 오버헤드(communication overhead)를 추가하는 것에 대해 걱정할 필요가 없습니다. AMD MI300X 카드도 첫날부터 지원된다는 점이 정말 좋습니다!
*   더 작은 20B 모델은 심지어 16GB VRAM에 들어맞습니다. 단, MXFP4를 지원하려면 RTX 50 시리즈 GPU 또는 더 새로운 것이어야 합니다. (수정: RTX 4090과 같은 구형 카드에 대한 지원이 최근 패치(patch)를 통해 추가되었습니다.)
*   모델은 구형 하드웨어에서도 실행되지만 MXFP4 지원 없이 더 많은 RAM을 소비할 것입니다. MXFP4 최적화 없이 bfloat16 형식의 모델은 gpt-oss-20b의 경우 약 48GB, gpt-oss-120b의 경우 240GB를 소비할 것입니다.

그건 그렇고, 저는 ollama를 사용하여 제 Mac Mini에서 gpt-oss-20b 모델을 편안하게 실행할 수 있습니다. 약 13.5GB의 메모리를 사용하는데, 이는 정말 합리적입니다.

MXFP4 양자화는 단순히 메모리 효율성을 높이는 것을 넘어, LLM의 접근성을 혁신적으로 개선합니다. 이전에는 수십, 수백 GB의 VRAM을 요구하는 대규모 모델을 로컬에서 실행하는 것은 극히 제한적인 사용자들만이 가능했습니다. 하지만 MXFP4와 같은 고급 양자화 기술 덕분에, 이제는 일반 소비자용 GPU나 심지어 통합 GPU를 갖춘 장치에서도 상당한 규모의 LLM을 구동할 수 있게 되었습니다. 이는 개인 정보 보호가 중요한 응용 프로그램, 오프라인 환경에서의 사용, 그리고 클라우드 비용 절감 측면에서 엄청난 잠재력을 가집니다. 또한, 이는 오픈 소스 커뮤니티가 이러한 모델을 더 쉽게 실험하고 개선할 수 있는 기반을 마련해 줍니다.

#### 4.4 벤치마크: 숫자 너머의 실제 사용성

이 모델들은 독립적인 벤치마크를 하기에는 아직 너무 새롭습니다. LM 아레나 리더보드(leaderboard)를 확인해 보니, gpt-oss는 아직 목록에 없습니다. 따라서 LM 아레나 사용자들에 따르면, Qwen3-Instruct가 현재까지 최고의 오픈 웨이트 모델로 남아 있습니다(그림 22).

**그림 22**: LM 아레나 리더보드의 현재 모습 (2025년 8월 8일 기준)

gpt-oss 발표 게시물에서 제공된 추론 벤치마크를 보면, gpt-oss 모델이 OpenAI의 독점 모델뿐만 아니라 Qwen3와도 동등한 수준임을 알 수 있습니다(그림 23).

**그림 23**: 주요 벤치마크 차트는 공식 gpt-oss 발표 게시물에서 가져왔습니다. "도구 없음(no tools)" gpt-oss-120b 데이터는 공식 모델 카드 논문에서 가져왔고, Qwen3 수치는 공식 Qwen3 저장소(repository)에서 가져왔습니다.

하지만 gpt-oss-120b가 Qwen3 A235B-A22B-Thinking-2507 모델의 거의 절반 크기이며 단일 GPU에서 실행될 수 있다는 사실로 인해 이 점은 주의해야 합니다.

하지만 벤치마크 성능이 항상 실제 사용성을 반영하는 것은 아닙니다. 지난 며칠간의 제한적인 사용에서 저는 gpt-oss가 상당히 유능하다는 것을 발견했습니다. 그렇지만, 다른 사람들이 관찰했듯이, 환각(hallucinate) 현상이 비교적 높은 경향이 있는 것으로 보입니다(모델 카드에도 언급된 점입니다). 이는 수학, 퍼즐, 코드와 같은 추론 작업에 대한 집중적인 훈련 초점에서 비롯될 수 있으며, 이는 일부 "일반 지식 망각"으로 이어졌을 수 있습니다.

그럼에도 불구하고, gpt-oss는 도구 사용을 염두에 두고 설계되었기 때문에 이 한계는 시간이 지남에 따라 덜 중요해질 수 있습니다. 오픈 소스 LLM에서의 도구 통합은 아직 초기 단계이지만, 성숙해짐에 따라 사실 기반 또는 지식 기반 질문에 답할 때 모델이 외부 소스(검색 엔진과 같은)를 참조하도록 점점 더 많이 허용할 것으로 예상합니다. 그렇게 된다면, 암기보다는 추론 능력(reasoning capacity)을 우선시하는 것이 합리적일 수 있습니다. 이는 학교에서의 인간 학습(또는 일반적으로 삶)과 매우 유사합니다. 문제 해결 능력이 사실을 암기하는 것보다 더 중요한 경우가 많기 때문입니다.

벤치마크는 모델의 특정 능력에 대한 객관적인 측정을 제공하지만, LLM의 복잡성과 다용성을 완전히 포착하지 못할 때가 많습니다. 실제 환경에서는 모델의 유창성, 일관성, 유용성, 그리고 사용자 프롬프트에 대한 적응성 등이 벤치마크 점수만큼이나 중요하게 작용합니다. 특히, gpt-oss와 같은 추론 중심 모델은 도구 연동(tool-use)을 통해 외부 지식을 활용할 때 그 진정한 가치를 발휘할 수 있습니다. 이는 LLM이 단순히 정보를 생성하는 것을 넘어, 문제 해결을 위한 지능형 에이전트(intelligent agent)로 진화하는 방향을 제시합니다.

### 5. gpt-oss와 GPT-5: 오픈 소스 전략의 의미

OpenAI는 바쁜 한 주를 보냈고, gpt-oss 출시 직후 오랫동안 기다려온 GPT-5 모델을 공개했습니다. GPT-5 출시는 흥미로웠습니다. 그리고 여기서 한 가지 말해야 할 것이 있다면, 벤치마크 성능(그림 24) 면에서 그들의 오픈 소스 모델이 최고의 제품 제공 모델과 비교하여 얼마나 좋은지에 정말 놀랐다는 것입니다.

**그림 24**: 주요 벤치마크 차트는 공식 GPT-5 발표 게시물에서 가져왔습니다. gpt-oss 데이터는 공식 모델 카드 논문 및 발표 게시물에서 가져왔고, Qwen3 수치는 공식 Qwen3-Coder 저장소에서 가져왔습니다.

전반적으로, 일부 사람들이 이번 출시를 과대광고라고 불렀음에도 불구하고, 최고의 독점 모델에 크게 뒤지지 않는 정말 강력한 새로운 오픈 웨이트 모델 세트를 갖게 되어 기쁩니다. 물론, 벤치마크는 종종 실제 사용을 정확하게 반영하지 않으며, 제한된 사용량만으로는 아직 판단하기 이릅니다. 하지만 저는 오픈 웨이트 및 로컬(또는 개인 호스팅) 모델로 작업하는 것을 좋아하는 사람들에게는 좋은 시기라고 생각합니다.

OpenAI가 gpt-oss와 GPT-5를 거의 동시에 발표한 것은 흥미로운 전략적 움직임으로 해석될 수 있습니다. 이는 OpenAI가 최첨단 독점 기술을 계속 발전시키는 동시에, 오픈 웨이트 모델을 통해 연구 커뮤니티 및 개발자 생태계와의 접점을 넓히려는 의지를 보여줍니다. 이러한 이중 전략은 AI 기술의 발전 속도를 가속화하고, 더 많은 사용자들이 LLM 기술에 접근하고 기여할 수 있는 기회를 제공할 것입니다. gpt-oss와 같은 모델은 GPT-5와 같은 최상위 모델의 연구 개발에 간접적으로 기여할 수도 있으며, 반대로 GPT-5의 혁신은 미래의 오픈 웨이트 모델에 영감을 줄 수 있습니다. 이는 AI의 미래가 폐쇄적인 소수의 기업에 의해서만 결정되는 것이 아니라, 광범위한 협력과 개방적인 접근 방식을 통해 형성될 수 있음을 시사하는 긍정적인 신호입니다.

이 잡지는 개인적인 열정 프로젝트이며, 여러분의 지원은 이를 유지하는 데 도움이 됩니다. 기여하고 싶으시다면 몇 가지 좋은 방법이 있습니다.

*   **저의 책을 구입하세요**. "대규모 언어 모델 구축하기 (처음부터)"는 토크나이저(tokenizer)부터 훈련까지 LLM을 단계별로 구축하는 과정을 안내합니다.
*   **비디오 코스를 확인하세요**. 이제 Manning에서 제공하는 책 기반의 17시간 분량 비디오 코스가 있습니다. 이 코스는 책을 섹션별로 면밀히 따르며, 단독으로 또는 코딩 실습 자료로 모두 잘 작동합니다. 이 비디오 코스는 광고가 없으며(YouTube 버전과 달리) 더 깔끔하고 체계적인 형식을 가지고 있습니다. 또한 Abhinav Kimothi가 제작한 5시간 분량의 추가 선수 학습 비디오 자료도 포함되어 있습니다.
*   **구독하세요**. 유료 구독은 저의 글쓰기를 지속 가능하게 하고 추가 콘텐츠에 대한 접근을 제공합니다.

읽어주셔서 감사하며, 독립적인 연구를 지원해 주셔서 감사합니다!

Ahead of AI는 독자 지원 출판물입니다. 새로운 게시물을 받고 저의 작업을 지원하려면 무료 또는 유료 구독자가 되는 것을 고려해 보세요.
구독하기