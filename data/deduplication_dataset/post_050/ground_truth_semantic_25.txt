텍스트 기반의 거대 언어 모델(LLM)이 널리 사용되면서, 학계에서는 이 유능한 모델들을 시각, 청각 등 다양한 정보 유형을 처리할 수 있도록 발전시키는 방안에 주목했습니다. 이러한 다중 모달(multi-modal) LLM에 대한 탐구는 여러 측면에서 상당한 잠재력을 지닙니다. 이는 모델의 지능을 한층 더 끌어올리고, 기존에 활용되지 않던 새로운 학습 데이터원을 발굴하며, LLM이 해결할 수 있는 문제의 지평을 넓힙니다. 최근에는 글자 외에도 그림이나 동영상을 이해할 수 있는 비전 기반 LLM, 즉 vLLM이 특히 각광받고 있습니다. 예를 들어, 최신 OpenAI 모델 대다수는 시각적 입력 처리를 지원하며, Meta는 LLaMA-3의 시각적 확장판인 LLaMA-3.2 Vision을 선보였습니다. 본 글에서는 vLLM의 기본적인 작동 원리부터 시작하여, 궁극적으로 LLaMA-3.2 Vision이 실제 환경에서 어떻게 구현되는지 깊이 있게 분석함으로써 vLLM에 대한 이해를 돕고자 합니다. 놀랍게도 vLLM은 그 인상적인 능력에도 불구하고, 핵심적으로는 순수 텍스트 기반 LLM과 크게 다르지 않다는 사실을 깨닫게 될 것입니다.

### vLLM의 구성 요소(The Building Blocks of vLLMs)

vLLM을 온전히 파악하려면 근본적인 개념부터 접근해야 합니다. 이 섹션에서는 교차 어텐션(cross-attention)이나 이미지 및 동영상 처리 인코더(encoder)와 같은, 이러한 모델을 구성하는 데 필요한 핵심 사상들을 다룰 것입니다. 우리는 트랜스포머(transformer) 아키텍처에 대한 전반적인 이해 등 텍스트 기반 LLM의 기초 지식이 있다고 (대부분) 가정합니다. 하지만 이 개념에 익숙하지 않은 독자들을 위해 아래 링크에서 더 자세한 정보를 찾아볼 수 있습니다.

[Decoder-Only Transformers: The Workhorse of Generative LLMs](https://cameronrwolfe.substack.com/p/decoder-only-transformers-the-workhorse)
Cameron R. Wolfe, Ph.D. · 2024년 3월 4일
전체 스토리 읽기

### 교차 어텐션(Cross-Attention)과 트랜스포머(Transformers) 구조 (참고: [8])

트랜스포머 구조 [8]는 언어 모델링 분야에서 광범위하게 활용되는 핵심 아키텍처입니다. 초기 트랜스포머 구조는 인코더부와 디코더부로 나뉘며, 각 부분은 반복되는 단위로 구성됩니다. 이 단위들에는 시퀀스 내의 다른 요소들과의 관계를 통해 개별 토큰 벡터를 재구성하는 자가-어텐션(self-attention) 메커니즘과, 각 토큰 벡터를 독립적으로 변형하는 전방향 신경망(feed-forward network)이 포함됩니다. 트랜스포머는 이러한 구성 요소들을 통해 입력 데이터의 장거리 의존성을 효과적으로 포착할 수 있게 됩니다.

### 디코더-온리 트랜스포머(Decoder-only transformer) 아키텍처

디코더-온리 트랜스포머는 GPT 계열의 생성형 LLM에서 가장 흔히 사용되는 트랜스포머 구조의 변형입니다. 대부분의 vLLM 또한 디코더-온리 아키텍처를 채택하지만, 시각 정보 처리를 위해 추가적인 모듈이 구조에 포함됩니다. 간단히 말해, 이 아키텍처는 인코더 구성 요소가 없는 트랜스포머와 동일하며, 바로 이 때문에 "디코더-온리"라는 명칭이 붙었습니다.

원래 트랜스포머 디코더는 각 블록에 마스크된 자가-어텐션과 전방향 변환뿐만 아니라, 추가적인 교차 어텐션 모듈을 포함합니다. 자가-어텐션은 단일 시퀀스 내의 토큰들 간에 어텐션을 계산하는 반면, 교차 어텐션은 두 개의 토큰 시퀀스(인코더의 토큰과 디코더의 토큰)를 고려하여 이들 간의 어텐션을 산출합니다. 이를 통해 디코더는 출력을 생성할 때 인코더가 만들어낸 표현을 참고할 수 있게 됩니다! 먼저 자가-어텐션을 이해한 후, 교차 어텐션에 대해 다룰 것입니다.

### 자가-어텐션(Self-attention) 간략 설명

자가-어텐션 메커니즘의 입력은 토큰 벡터들의 나열입니다. 자가-어텐션은 시퀀스 내 다른 모든 토큰들을 고려하여 각 토큰에 대한 결과 표현을 구성합니다. 이를 위해 자가-어텐션 과정은 각 입력 토큰 벡터로부터 세 가지 종류의 선형 변환 결과물인 질의(query), 키(key), 값(value)을 만들어냅니다. 이 질의와 키를 활용하여 해당 시퀀스 내 모든 요소 쌍 간의 관련성 점수를 산출합니다. 이 관련성 점수는 각 토큰이 시퀀스 내 다른 모든 토큰에 대해 얼마나 중요한지, 또는 특정 토큰이 다른 토큰에 얼마나 "집중해야 하는지"를 나타냅니다. 이 관련성 점수에 값을 곱함으로써 최종 결과물을 얻을 수 있습니다.

자가-어텐션의 기본적인 구현은 아래에 제시됩니다 2.

This file contains bidirectional Unicode text that may be interpreted or compiled differently than what appears below.
To review, open the file in an editor that reveals hidden Unicode characters.
Learn more about bidirectional Unicode characters
Show hidden characters
```python
import math
import torch
from torch import nn
import torch.nn.functional as F

class SelfAttention(nn.Module):
    def __init__(self, d):
        """
        Arguments:
            d: size of embedding dimension
        """
        super().__init__()
        self.d = d
        # key, query, value projections for all heads, but in a batch
        # output is 3X the dimension because it includes key, query and value
        self.c_attn = nn.Linear(d, 3 * d, bias=False)

    def forward(self, x):
        # compute query, key, and value vectors in batch
        # split the output into separate query, key, and value tensors
        q, k, v = self.c_attn(x).split(self.d, dim=2)

        # compute the attention matrix and apply dropout
        att = (q @ k.transpose(-2, -1)) * (1.0 / math.sqrt(k.size(-1)))
        att = F.softmax(att, dim=-1)

        # compute output vectors for each token
        y = att @ v
        return y
```
[view raw](https://gist.githubusercontent.com/cameronrwolfe/209c13d789069d5119932d1633534a74/raw/26270451000639f72782991e48810c926a762031/bidir_self_attn.py)
hosted with ❤ by GitHub

### 교차 어텐션(Cross-attention)의 작동 방식

교차 어텐션의 개략적인 표현은 아래에 제시되어 있습니다. 보시다시피, 이 모듈은 자가-어텐션과 크게 다르지 않습니다. 여기서 핵심적인 차이점은 질의(query), 키(key), 값(value) 행렬을 계산하는 데 사용되는 초기 선형 투영(linear projection)에 있습니다. 단일 토큰 벡터 시퀀스를 선형 투영하여 이 세 가지 행렬을 모두 계산하는 대신, 두 개의 다른 벡터 시퀀스를 선형 투영합니다. 아래를 참조하십시오. 질의 행렬은 첫 번째 시퀀스를 선형 투영하여 생성되는 반면, 키와 값 행렬은 모두 두 번째 시퀀스를 선형 투영하여 생성됩니다. 결과적으로, 우리의 어텐션 행렬은 첫 번째 시퀀스와 두 번째 시퀀스의 토큰 간의 모든 쌍별 어텐션 점수를 포함합니다. 시퀀스의 길이는 같을 필요가 없으며, 출력의 길이는 첫 번째 시퀀스의 길이와 일치합니다.

This file contains bidirectional Unicode text that may be interpreted or compiled differently than what appears below.
To review, open the file in an editor that reveals hidden Unicode characters.
Learn more about bidirectional Unicode characters
Show hidden characters
```python
import math
import torch
from torch import nn
import torch.nn.functional as F

class CrossAttention(nn.Module):
    def __init__(self, d):
        """
        Arguments:
            d: size of embedding dimension
        """
        super().__init__()
        self.d = d
        # linear projection for producing query matrix
        self.w_q = nn.Linear(d, d, bias=False)
        # linear projection for producing key / value matrices
        self.w_kv = nn.Linear(d, 2 * d, bias=False)

    def forward(self, x_1, x_2):
        # compute query, key, and value matrices
        q = self.w_q(x_1)
        k, v = self.w_kv(x_2).split(self.d, dim=2)

        # compute the attention matrix and apply dropout
        att = (q @ k.transpose(-2, -1)) * (1.0 / math.sqrt(k.size(-1)))
        att = F.softmax(att, dim=-1)

        # compute output vectors for each token in x_1
        y = att @ v
        return y
```
[view raw](https://gist.githubusercontent.com/cameronrwolfe/209c13d789069d5119932d1633534a74/raw/26270451000639f72782991e48810c926a762031/cross_attention.py)
hosted with ❤ by GitHub

교차 어텐션의 구현이 위에 제공됩니다. 이 구현에서 설명된 바와 같이, 우리는 더 이상 단일 시퀀스 내의 토큰 간에 어텐션 점수를 계산하지 않습니다. 대신, 시퀀스 간 어텐션 점수(inter-sequence attention scores)를 계산하여 두 입력 시퀀스의 융합된 표현(fused representation)을 형성합니다.

### 이미지 인코더(image encoder) 특징을 LLM에 교차 어텐션으로 통합하기

vLLM에 적용. 이 개요의 이 시점에서 교차 어텐션에 대한 우리의 설명이 무작위로 보일 수 있습니다. 그러나 우리가 보게 되겠지만, 교차 어텐션은 다중 모달(multi-modal) LLM 연구에서 끊임없이 사용됩니다. 우리는 교차 어텐션을 사용하여 비전 모델(vision model)이 생성한 이미지 표현을 텍스트 기반 LLM에 융합할 수 있습니다. 위를 참조하십시오. 다시 말해, LLM이 출력을 생성할 때 시각적 정보(visual information)를 통합하여 모델이 텍스트 외에도 이미지(또는 다른 양식의 데이터)를 입력으로 받아들이고 해석할 수 있도록 합니다!

### 비전 트랜스포머(Vision Transformers, ViT) [3]

"우리는 가능한 한 최소한의 수정으로 표준 트랜스포머(Transformer)를 이미지에 직접 적용합니다. 이를 위해 이미지를 패치(patch)로 분할하고 이 패치들의 선형 임베딩(linear embedding) 시퀀스를 트랜스포머의 입력으로 제공합니다." - [3]에서 발췌

트랜스포머 (및 BERT, GPT와 같은 많은 변형)가 처음에는 자연어 처리 애플리케이션을 위해 제안되었지만, 이 영향력 있는 모델 아키텍처는 이후 컴퓨터 비전 도메인의 애플리케이션으로 확장되었습니다. 비전 트랜스포머 [3] (줄여서 ViT)는 오늘날 가장 일반적으로 사용되는 아키텍처 중 하나입니다. 아래 그림에서 보듯이, 이 아키텍처는 인코더-온리 (BERT 스타일) 트랜스포머 아키텍처와 매우 유사하게 보입니다. 우리는 단순히 벡터 시퀀스를 입력으로 받아들이고 i) 양방향 자가-어텐션(bidirectional self-attention)과 ii) 전방향 변환(feed-forward transformation)을 모두 포함하는 트랜스포머 블록 시퀀스를 적용합니다.

**표준 비전 트랜스포머(ViT) 아키텍처**

### 입력 이미지 처리

비전 트랜스포머의 입력은 이미지입니다. 그러나 이 이미지를 트랜스포머의 입력으로 전달하려면 이미지를 텍스트 토큰 벡터 시퀀스와 유사한 벡터 목록으로 변환해야 합니다. ViT의 경우, 이미지를 작은 조각(patch)들로 나눈 다음 각 조각을 벡터 형태로 평탄화(flattening)하여 이를 수행합니다. 이 벡터들은 트랜스포머가 요구하는 크기와 다를 수 있으므로, 단순한 선형 투영(linear projection)을 통해 올바른 차원(dimension)으로 맞춰줍니다. (출처: [3])

일반 트랜스포머와 유사하게, 각 패치에 대한 벡터에 위치 임베딩(positional embedding)을 추가합니다. 여기서 위치 임베딩은 이미지 내 각 패치의 2D 위치를 포착합니다. 이 트랜스포머 아키텍처의 출력은 입력과 동일한 크기의 각 패치에 대한 벡터 시퀀스입니다. 이미지 분류(image classification)와 같은 작업을 해결하기 위해 위 그림에서 보듯이 이 모델의 끝에 추가 분류 모듈(예: 선형 레이어)을 추가할 수 있습니다.

### 왜 인코더(encoder)인가?

대부분의 LLM에서 사용되는 디코더-온리 트랜스포머 아키텍처 대신, ViT에는 인코더-온리 트랜스포머 아키텍처를 사용합니다. 그 이유는 ViT가 생성형(generative) 모델이 아니기 때문입니다. LLM의 경우, 다음 토큰 예측(next token prediction)을 통해 모델을 훈련하여 텍스트 시퀀스를 생성합니다. 결과적으로, 모델이 시퀀스에서 미래 토큰을 미리 볼 수 없도록 각 트랜스포머 레이어에서 마스크된 자가-어텐션(masked self-attention)을 사용해야 합니다. 그렇지 않으면 모델이 다음 토큰을 예측할 때 "속임수"를 쓸 수 있을 것입니다! 대조적으로, ViT는 이미지의 좋은 표현을 형성하기 위해 전체 이미지 패치 시퀀스를 볼 수 있어야 합니다. 이 입력 시퀀스에서 다음 패치를 예측할 필요가 없습니다! (출처: [3])

### ViT 훈련 및 성능 최적화

[3]의 오리지널 ViT 모델은 BERT와 동일한 아키텍처를 공유합니다. 위에서 보듯이, 다양한 크기의 ViT가 훈련되며, 그 중 가장 큰 것은 ViT-H (또는 ViT-Huge)입니다. 이 모델은 이 개요의 뒷부분에서 다시 보게 될 것입니다. 모든 ViT 모델은 다양한 크기의 데이터셋에서 지도 이미지 분류(supervised image classification)를 사용하여 훈련됩니다. ViT가 작거나 중간 크기의 데이터셋(예: ImageNet)에서 훈련될 때, 유사한 크기의 ResNet 3과 비슷하거나 약간 더 나쁜 성능을 보입니다. 그러나 ViT는 훨씬 더 큰 데이터셋(예: JFT-300M)에서 사전 훈련(pretrained)되고 이후 다운스트림 작업(downstream task)에서 미세 조정(finetuned)될 때 빛을 발하기 시작합니다. 이는 비전 트랜스포머가 대규모 데이터에서 훈련될 때 그 진가를 발휘하며, 일반화 능력이 크게 향상됨을 의미합니다. (출처: [3])

### 대조 언어-이미지 사전 훈련(Contrastive Language-Image Pre-Training, CLIP) [4]

표준 ViT는 대규모 지도 이미지 분류 예제 데이터셋에서 훈련됩니다. 이러한 모델은 방대한 양의 주석이 달린(일반적으로 사람에 의해) 데이터에서 사전 훈련될 때 가장 잘 작동하지만, 이러한 데이터는 얻기 어렵고 비용이 많이 듭니다. [4]에서 저자들은 온라인에서 더 쉽게 구할 수 있는 이미지-캡션(image-caption) 쌍을 사용하여 강력한 이미지 표현 모델을 훈련하는 대안적인 접근 방식을 탐구합니다. 이 접근 방식을 대조 언어-이미지 사전 훈련(Contrastive Language-Image Pre-Training, CLIP)이라고 합니다. (출처: [4])

### CLIP 아키텍처

CLIP 모델은 이미지 인코더(image encoder)와 텍스트 인코더(text encoder)라는 두 개의 독립적인 구성 요소로 이루어져 있습니다. 이미지-텍스트 쌍이 입력되면, 이들은 각 인코더에 개별적으로 전달되어 관련된 벡터 표현(vector representation)을 얻습니다. 이미지 인코더는 표준 ViT 모델 [3]인 반면, 텍스트 인코더는 디코더-온리 트랜스포머(즉, 일반적인 GPT 스타일 LLM)입니다. CLIP의 텍스트 인코더는 텍스트를 생성하는 데 사용되지 않지만(적어도 [4]에서는), 저자들은 향후 CLIP을 생성형 애플리케이션으로 확장하는 것을 용이하게 하기 위해 디코더-온리 아키텍처를 채택했습니다. CLIP 아키텍처의 개요는 위에 제공됩니다.

"어떤 캡션이 어떤 이미지와 어울리는지 예측하는 간단한 사전 훈련 작업은 인터넷에서 수집된 4억 개의 (이미지, 텍스트) 쌍 데이터셋에서 이미지 표현을 처음부터 학습하는 효율적이고 확장 가능한 방법입니다." - [4]에서 발췌

### 대조 학습(Contrastive learning)

위에서 설명한 CLIP 모델을 훈련하는 데는 여러 가지 접근 방식이 있을 수 있습니다. 예를 들어, 캡션의 단어를 기반으로 이미지를 분류하거나 [5] 아키텍처의 LLM 구성 요소를 사용하여 이미지를 기반으로 캡션을 생성할 수 있습니다 [6]. 그러나 이러한 목표는 이전 연구에서 성능이 좋지 않거나 모델이 느리게 학습하게 하는 것으로 밝혀졌습니다. [4]의 핵심 기여는 대조 학습(contrastive learning)의 아이디어를 기반으로 한 간단하고 효율적인 훈련 목표를 사용하여 이미지-텍스트 쌍으로부터 학습하는 아이디어입니다.

**CLIP 훈련 목표의 개략적인 묘사**

좀 더 자세히 설명하자면, CLIP은 학습 배치 내의 여러 후보 설명문 중에서 주어진 이미지에 맞는 정확한 설명문을 찾아내는 간단한 목표를 통해 학습됩니다. 이 목표는 다음과 같이 실현됩니다:
*   이미지와 텍스트 설명문 집합을 각 전용 인코더(예: 이미지용 ViT, 텍스트용 LLM)에 통과시켜 각 임베딩을 얻습니다.
*   실제 이미지-설명문 쌍에서 추출된 임베딩 사이의 코사인 유사도(cosine similarity)를 극대화합니다.
*   그 외 모든 이미지-설명문 조합의 코사인 유사도를 최소화합니다.

이 목표는 다중 클래스 N-쌍(multi-class N-pair) (또는 InfoNCE) 손실 [7]이라고 불리며, 대조 학습(contrastive learning) 및 메트릭 학습(metric learning) 문헌에서 일반적으로 사용됩니다.

**제로샷 분류(Zero-shot classification) (왼쪽 [4]) 및 CLIP 훈련 효율성 (오른쪽 [4])**

### CLIP 활용 및 중요성

CLIP 모델은 이미지 인코더와 텍스트 인코더 모두로 훈련되지만, 이 개요에서 보게 될 대부분의 작업은 CLIP의 이미지 인코더만 사용합니다. CLIP의 핵심 기여는 모델 아키텍처 자체가 아니라 혁신적인 훈련 목표에 있습니다. 이미지 인코더와 텍스트 인코더를 모두 사용하여 위에서 설명한 대조 목표를 통해 이미지 인코더를 훈련할 수 있으며, 이는 매우 효율적이고(위 참조) 대량의 지도 데이터(supervised data)에 의존하지 않습니다. CLIP 모델 아키텍처는 전체적으로 유용할 수 있습니다. 예를 들어, 위에서 보듯이 제로샷 이미지 분류(zero-shot image classification)를 수행하는 데 사용할 수 있습니다. 그러나 고품질 이미지 인코더를 얻기 위한 목적으로만 CLIP 모델을 훈련할 수도 있습니다! 이는 CLIP이 다양한 시각-언어 작업을 위한 강력한 기반 모델 역할을 할 수 있음을 보여줍니다.

### 이미지에서 비디오로: 동적 정보 처리

LLM으로 이미지를 처리하려면, 단순히 이 이미지를 이미지 인코더(예: CLIP)에 전달하여 이 이미지를 나타내는 벡터 또는 임베딩(embedding) 세트를 생성할 수 있습니다. 그런 다음 LLM은 이러한 임베딩을 추가 입력으로 사용할 수 있습니다(이에 대한 자세한 내용은 이 개요의 뒷부분에서 다룰 것입니다).

그러나 이미지 대신 비디오에 액세스할 수 있다면 어떨까요? 흥미롭게도, LLM으로 비디오 입력을 처리하는 것은 이미지 입력을 처리하는 것과 크게 다르지 않습니다. 우리는 이 비디오를 이미지와 유사하게 벡터 세트로 변환하기 위한 전략만 필요합니다!

### 비디오의 본질과 데이터 구조

가장 근본적인 수준에서 비디오는 일반적으로 "프레임(frames)"이라 불리는 연속적인 이미지들의 집합입니다. 대개 이미지는 RGB 형식으로 저장됩니다. 예를 들어, 아래 그림의 이미지는 세 가지 색상 채널(빨강, 파랑, 초록)과 5x5의 높이 및 너비를 가집니다. 이 이미지의 크기는 3 (색상 채널) × 5 (높이) × 5 (너비)입니다. 또한 여러 이미지를 미니 배치(mini-batch)로 쌓아 batch × 3 × 5 × 5 크기의 텐서(tensor)를 형성할 수 있습니다.

**이미지와 비디오의 데이터 구조 비교**

비디오의 구조도 크게 다르지 않습니다. 비디오는 단순히 정렬된 프레임의 모음입니다. 올바른 시간 순서로 볼 때, 이 프레임들은 시간의 흐름에 따른 장면의 움직임을 드러내어 비디오를 형성합니다. 이미지와 유사하게, 이 프레임들 각각은 일반적으로 RGB 형식으로 표현되며, 비디오의 모든 프레임은 동일한 공간 해상도(spatial resolution)를 가집니다. 예를 들어, 위 그림의 비디오는 세 개의 프레임을 가지며, 각 프레임은 세 가지 색상 채널과 높이 및 너비가 5로, 3 (프레임) × 3 (색상 채널) × 5 (높이) × 5 (너비) 크기의 텐서를 형성합니다. 우리는 또한 비디오의 미니 배치를 생성할 수 있지만, 각 비디오가 동일한 수의 프레임을 가지도록 해야 합니다. 이는 일반적으로 비디오에서 고정 길이의 "클립(clips)"을 추출하여 수행됩니다(예: 64프레임).

### 비디오 프레임 서브샘플링(Sub-sampling frames)의 효율성

프레임 속도(Frame rate). 비디오는 일반적으로 초당 고정된 수의 프레임(FPS)으로 녹화됩니다. 예를 들어, 24 FPS는 일반적인 프레임 속도이며, 이는 비디오의 각 초에 24개의 프레임이 포함됨을 의미합니다. 영화를 보거나 비디오 게임을 할 때, 세밀한 프레임 속도는 중요합니다. 비디오 프레임 사이에 시각적으로 인지할 수 있는 간격이 없기를 원합니다.

그러나 신경망(neural network)은 이 수준의 세밀도로 비디오를 처리할 필요가 없습니다. 위에서 보듯이, 비디오 내의 프레임을 서브샘플링(sub-sampling)하여 계산 비용을 절약할 수 있습니다. 예를 들어, 24 FPS 비디오의 8번째 프레임마다 샘플링하여 3 FPS를 시뮬레이션할 수 있습니다. 이는 정보 손실을 최소화하면서도 모델의 처리 부하를 크게 줄일 수 있는 효과적인 방법입니다.

### 비디오 인코딩과 Perceiver Resampler의 역할

비디오 프레임을 서브샘플링한 후에는 비디오를 단순히 이미지 세트로 간주할 수 있습니다! 일반적으로, 각 비디오 프레임을 CLIP과 같은 이미지 인코더를 통해 독립적으로 전달하여 각 비디오 프레임을 나타내는 해당 벡터 세트를 얻습니다. 그런 다음 LLM은 이러한 비디오 프레임에 대한 벡터를 이미지와 유사하게 추가 입력으로 받아들일 수 있습니다.

그러나 여기에는 여전히 문제가 있습니다. 비디오에 대해 생성되는 벡터의 수가 많고 비디오 길이가 임의적일 수 있기 때문에 때로는 예측할 수 없습니다. 비디오의 프레임 표현을 단일의 고정 크기 벡터 세트로 집계(aggregate)할 추가 모듈이 필요합니다! (출처: [9])

이것이 Perceiver [9]와 Perceiver Resampler [10]가 유용한 이유입니다. Perceiver (위에 표시됨)는 비디오 프레임에서 생성된 가변 크기의 대규모 벡터 세트와 같은 고차원 입력을 받아들이고 이 입력을 기반으로 고정 크기 표현을 출력할 수 있는 어텐션 기반 신경망 아키텍처입니다. 간단히 말해, 이는 모든 비디오 벡터를 Perceiver에 전달하면 고정 크기 벡터 세트를 반환한다는 의미입니다. 그런 다음 이미지처럼 이 추가 입력을 LLM에 쉽게 통합할 수 있습니다! (출처: [10])

Perceiver는 원래 Flamingo [10]에 의해 다중 모달 LLM에 적용되었으며, Flamingo는 Perceiver Resampler를 제안했습니다. 위를 참조하십시오. Flamingo는 1 FPS(즉, 비디오의 매 초마다 단일 프레임)로 비디오를 샘플링합니다. 비디오의 각 서브샘플링된 프레임은 이미지 인코더 4를 통해 독립적으로 전달되어 해당 이미지 임베딩을 생성합니다. 그러나 이러한 이미지 임베딩을 텍스트 기반 LLM에 전달하기 전에, 비디오에 대한 고정된(64개) 시각 토큰 벡터를 생성하는 Perceiver 아키텍처를 통과시킵니다. 그런 다음 이 벡터들을 이전에 설명한 대로 교차 어텐션을 사용하여 LLM에 통합합니다.

### vLLM 아키텍처 및 훈련 전략(vLLM Architectures and Training Strategies)

이제 vLLM과 관련된 대부분의 배경 개념을 이해했습니다. 다음으로, 이러한 개념을 사용하여 vLLM에 대한 이해를 처음부터 구축할 것입니다. 이 섹션에서는 vLLM을 생성하는 데 일반적으로 사용되는 아키텍처와 훈련 전략에 중점을 둘 것입니다. 이 논의는 당분간 개념적인 수준으로 유지하고, 다음 섹션에서 이러한 아이디어를 실제 vLLM 구현에 적용할 것입니다.

### vLLM 아키텍처 변형(vLLM Architecture Variants)

vLLM의 아키텍처는 항상 두 가지 주요 구성 요소, 즉 LLM 백본(backbone)과 비전 인코더(vision encoder)를 가집니다. LLM 백본은 표준 디코더-온리 트랜스포머이며, 비전 인코더는 일반적으로 CLIP/ViT 모델입니다(비디오 기반 입력을 처리하려는 경우 선택적으로 Perceiver Resampler 포함).

이러한 구성 요소를 함께 융합하는 두 가지 일반적인 vLLM 아키텍처 변형이 있습니다: **통합 임베딩(unified embedding)** 및 **교차 모달리티 어텐션(cross-modality attention) 아키텍처**입니다. 우리는 Sebastian Raschka가 vLLM에 대한 훌륭한 개요에서 제안한 이러한 아키텍처의 명명 체계를 사용합니다. 이제 이러한 아키텍처가 어떻게 작동하는지 알아보겠습니다.

### 원시 텍스트에서 토큰 벡터 생성

토큰 벡터(Token vectors). LLM 백본은 원시 텍스트를 입력으로 받지만, 이 텍스트는 먼저 이산 토큰(discrete token) 세트로 토큰화(tokenized)되고 임베딩 레이어(embedding layer)에서 각 토큰에 해당하는 임베딩을 검색하여 토큰 벡터로 변환됩니다. 위를 참조하십시오. 이 토큰 벡터 세트는 디코더-온리 트랜스포머 아키텍처에 직접 입력으로 전달될 수 있습니다.

이미지(또는 비디오)의 경우에도 유사하게, 이미지 또는 비디오를 비전 인코더를 통해 전달하여 비전 인코더에서 토큰 벡터 세트를 생성하며, 이는 시각 토큰 벡터(visual token vector) 세트를 출력으로 반환합니다. 아래를 참조하십시오.

**비전 인코더로 이미지 토큰 벡터 생성**

### 통합 임베딩(Unified embedding)

이제 텍스트 및 이미지(또는 비디오) 토큰 벡터 세트를 입력으로 가집니다. 첫 번째 보편적인 vLLM 구조는 다음 단계를 따릅니다:
*   텍스트와 시각 정보에서 파생된 두 종류의 벡터를 하나로 결합하여 단일 토큰 벡터 열을 구성합니다.
*   이렇게 병합된 벡터를 디코더-온리 트랜스포머 구조에 직접 입력으로 제공합니다.

통합 임베딩 아키텍처라고 불리는 이 아키텍처는 아래 그림에 묘사되어 있습니다. 특히, 시각 토큰 벡터의 크기가 텍스트 토큰 벡터의 크기와 일치하지 않을 수 있습니다. 따라서 우리는 일반적으로 비전 인코더의 토큰 벡터를 연결하기 전에 올바른 차원으로 선형 투영합니다.

**통합 임베딩 아키텍처(The Unified Embedding Architecture)**

통합 임베딩 아키텍처는 개념적으로 단순하지만, LLM에 전달되는 입력의 길이를 증가시켜 훈련 및 추론(inference) 중 계산 비용이 크게 증가할 수 있습니다. 이러한 시각 토큰은 강력한 LLM 백본의 모든 레이어를 통과합니다! 다행히도, 약간 다른 종류의 vLLM 아키텍처를 사용하여 이 문제를 해결할 수 있습니다.

### 교차 모달리티 어텐션(Cross-modality attention)

텍스트 및 비전 토큰 벡터를 연결하는 대신, 텍스트 토큰 벡터를 LLM에 입력으로 전달할 수 있습니다. 비전 정보를 통합하기 위해, 텍스트와 비전 토큰 벡터 간에 교차 어텐션을 수행하는 추가 교차 어텐션 모듈을 LLM의 특정 레이어(일반적으로 두 번째 또는 네 번째 레이어마다)에 추가할 수 있습니다. 이 아키텍처 변형은 일반적으로 교차 모달리티 어텐션 아키텍처라고 불립니다. 묘사는 아래를 참조하십시오.

특히, 이 아키텍처는 원래 트랜스포머 디코더와 매우 유사하게 보입니다. 우리는 트랜스포머 인코더 대신 이미지 인코더와 교차 어텐션을 수행할 뿐입니다!

**교차 모달리티 어텐션 아키텍처(The Cross-Modality Attention Architecture)**

이 구조의 이점은 언어 모델에 주입되는 입력 시퀀스의 길이가 늘어나지 않는다는 점입니다. 그보다는, 훨씬 더 효율적인 교차 어텐션 메커니즘을 활용하여 시각적 정보를 언어 모델에 통합합니다. 또한, 교차 모달리티 어텐션 아키텍처는 시각 및 텍스트 정보를 융합하기 위해 LLM의 기존 레이어에 의존하는 대신, 모델 아키텍처에 새로운 레이어를 추가합니다. 이러한 이유로, 우리는 훈련 중에 LLM 백본을 고정된 상태로 유지하고 추가된 레이어만 훈련할 수 있으며, 이는 텍스트 전용 작업에서 LLM의 성능이 전혀 변경되지 않도록 보장합니다.

### vLLM은 어떻게 훈련할까요?

이 개요에서는 시각적 입력을 받아들일 수 있는 LLM만 고려할 것입니다. 이러한 모델은 여전히 텍스트만 출력으로 생성합니다. 따라서 다른 LLM과 유사하게 다음 토큰 예측(next-token prediction)을 사용하여 이러한 모델을 훈련할 수 있습니다. 통합 임베딩 아키텍처의 경우에도, 우리는 주로 텍스트 토큰을 예측하여 모델을 훈련합니다. 일반적으로 시각 토큰을 예측하려고 시도하지 않습니다(즉, 다음 이미지 예측을 수행하지 않습니다).

"Gemini 모델의 시각적 인코딩은 Flamingo, CoCa 및 PaLI에 대한 우리의 기초 작업에서 영감을 받았으며, 모델이 처음부터 다중 모달이며 이산 이미지 토큰을 사용하여 이미지를 기본적으로 출력할 수 있다는 중요한 차이점이 있습니다." - [11]에서 발췌

그러나 훈련 목표를 넘어 vLLM을 훈련하기 위해 따를 수 있는 몇 가지 전략이 있습니다. 예를 들어, 우리는 네이티브 다중 모달 훈련(native multi-modal training)을 수행할 수 있습니다. 이는 아키텍처의 모든 구성 요소를 처음부터 초기화하고 다중 모달 데이터(즉, 텍스트, 이미지, 비디오 등)를 사용하여 모델을 처음부터 훈련하는 것을 의미합니다. 예를 들어, 이 접근 방식은 Gemini [11]를 훈련하는 데 사용됩니다.

그러나 실제로는 네이티브 다중 모달리티(multi-modality)는 복잡하고 어렵습니다. 이러한 접근 방식을 사용할 때 발생할 수 있는 많은 문제가 있습니다:
*   대량의 쌍을 이룬 이미지-텍스트 데이터에 접근하기 어렵습니다.
*   사전 훈련 규모에서 시각 데이터의 효율적인 토큰화가 어렵습니다.
*   양식 간 불균형이 발생할 수 있습니다. 예를 들어, 텍스트가 다음 토큰 예측에 충분한 정보를 제공하기 때문에 모델이 이미지를 무시하는 법을 배울 수 있습니다.

이러한 이유로 vLLM은 구성적 접근 방식(compositional approach)을 사용하여 더 자주 훈련됩니다. 구체적으로, 이는 LLM 백본과 비전 인코더를 독립적으로 사전 훈련하는 것으로 시작한다는 의미입니다. 그런 다음 텍스트 및 비전 모델을 단일 vLLM으로 결합하는 추가 훈련 단계(이를 융합 단계(fusion stage)라고 부를 것입니다)를 가집니다.

이 접근 방식은 다음과 같은 여러 가지 이점을 가집니다:
*   텍스트 및 이미지 모델 개발을 병렬화할 수 있습니다.
*   매우 강력하고 발전된 기존 텍스트 기반 LLM을 vLLM 훈련의 시작점으로 사용할 수 있습니다.
*   텍스트 전용, 비전 전용, 쌍을 이룬 텍스트-비전 데이터를 훈련에 사용할 수 있으므로 훨씬 더 많은 양의 데이터를 사용할 수 있습니다.

융합 단계 동안 전체 vLLM 아키텍처를 훈련할 수도 있고 훈련하지 않을 수도 있습니다. 예를 들어, 교차 모달리티 어텐션 아키텍처를 사용할 때, 융합 동안 LLM 백본을 고정(freeze)하고 교차 어텐션 및 비전 인코더 레이어만 훈련할 수 있습니다. 이러한 접근 방식은 기존 텍스트 기반 LLM으로 시작하여 기본 LLM 백본을 수정하지 않고 해당 vLLM을 생성할 수 있게 해주기 때문에 문헌에서 흔히 사용됩니다. 우리가 보게 되겠지만, 이것이 LLaMA-3.2 Vision 모델을 훈련하는 데 사용된 정확한 접근 방식이었습니다.

### LLaMA-3.2 Vision: 강력하고 개방적인 vLLM (LLaMA-3.2 Vision: Powerful, Open vLLMs)

이제 vLLM의 기본 개념을 이해했으니, 실제 사례 연구를 살펴보겠습니다. LLaMA-3 [1] LLM은 원래 텍스트 전용이었지만, 이후 이미지(및 비디오) 입력을 처리하도록 확장되었습니다. 이 모델들은 또한 (대부분) 오픈 소스 5이므로, i) 해당 기술 보고서에 제공된 세부 정보를 연구하고 ii) 코드를 살펴보면서 깊이 이해할 수 있습니다. 이 섹션에서는 LLaMA-3 LLM 제품군이 해당 vLLM 제품군을 생성하기 위해 어떻게 확장되었는지 자세히 연구할 것입니다.

### LLaMA-3를 이미지 및 비디오로 확장하기 [1]

[1]에서 제안된 LLaMA-3는 가장 인기 있고 강력한 오픈 소스 LLM 제품군 중 하나입니다. LLaMA-3 모델은 모두 밀집형(dense)입니다. 즉, MoE 아키텍처 6를 사용하지 않으며, 8B, 70B, 405B의 세 가지 크기로 제공됩니다. 이 모델들은 이전 LLaMA-2 모델보다 한 자릿수 더 개선되었습니다. 30배 더 큰 컨텍스트 창(context window) (128k 대 4k)을 가지며, 30배 (15.6T 토큰 대 1.8T 토큰) 더 큰 데이터셋을 사용하고, 50배 많은 컴퓨팅 양을 사용하여 훈련됩니다.

"우리는 Llama 3가 수많은 작업에서 GPT-4와 같은 선도적인 언어 모델과 비교할 만한 품질을 제공한다는 것을 발견했습니다... 이 논문은 또한 구성적 접근 방식을 통해 이미지, 비디오 및 음성 기능을 Llama 3에 통합한 실험 결과를 제시합니다." - [2]에서 발췌

초기 LLaMA-3 모델은 텍스트만 입력으로 받습니다. 그러나 저자들은 [1]에서 비전(즉, 이미지 및 비디오) 및 음성 기능(speech features)을 모두 통합하는 실험을 포함합니다. 이 섹션에서는 LLaMA-3가 시각적 입력에 대해 어떻게 훈련되는지 알아볼 것입니다.

### 구성적 vLLM(Compositional vLLMs)

LLaMA-3는 다중 모달(multi-modal) 모델을 생성하기 위해 구성적 접근 방식(compositional approach)을 따릅니다. 우리는 먼저 비전 인코더(vision encoder)와 텍스트 전용 LLM을 독립적으로 사전 훈련합니다. 여기서 텍스트 전용 LLM은 텍스트 기반 LLaMA-3 모델이며, 비전 인코더는 사전 훈련된 CLIP 모델입니다.

교차 모달리티 어텐션(cross-modality attention) 아키텍처를 채택하여, 이 두 모델 사이에 교차 어텐션 레이어(cross-attention layer)를 삽입하고 이 추가 레이어를 훈련하는 데 집중합니다. 편의상 이 교차 어텐션 레이어를 "이미지 어댑터(image adapter)"라고 부를 것입니다. 이를 통해 LLM은 출력을 생성할 때 추가 시각적 특징(visual features)을 통합하도록 학습됩니다.

LLaMA-3의 비전 인코더는 ViT [3] 아키텍처, 특히 6억 3천만 개의 매개변수(parameter)를 가진 ViT-H 7 모델을 기반으로 하며, 25억 개의 이미지-텍스트 쌍에 대한 대조 목표(contrastive objective)를 통해 사전 훈련됩니다. 다시 말해, 이 모델은 CLIP [4] 아키텍처의 이미지 인코더 구성 요소와 거의 동일합니다! 우리는 이 모델을 통해 이미지를 전달하고 해당 임베딩(embedding)을 추출하여 시각적 특징을 생성합니다. 아래를 참조하십시오.

**여러 ViT 레이어에서 임베딩 연결**

특히, 우리는 이전 연구를 통해 대조(CLIP 스타일) 목표로 훈련된 이미지 인코더가 의미 정보(semantic information)를 포착하지만, 이미지의 세밀한 지각적 세부 정보(fine-grained perceptual details)를 포착하는 데 실패한다는 것을 알고 있습니다. 이러한 이유로, 이러한 시각적 특징에 의존하는 LLM은 이미지 내에서 정확한 위치 파악(exact localization)을 요구하는 질문에 답하는 데 실패할 수 있습니다. GPT-4V의 예시를 아래에서 참조하십시오.

위에서 보듯이, 이 문제는 LLaMA-3에서 비전 인코더의 여러 다른 레이어 8에서 시각적 특징을 추출하고 이를 함께 연결하여 해결됩니다. 이는 초기 레이어가 저수준 특징(예: 가장자리, 질감)을, 후기 레이어가 고수준 의미 특징을 포착하는 경향이 있다는 점을 활용하여, 모델이 이미지의 다양한 추상화 수준에서 정보를 얻을 수 있도록 합니다.

LLaMA-3는 또한 이미지 인코더 이후와 LLM과의 융합 이전에 여러 추가 자가-어텐션 레이어를 추가합니다. 최종 이미지 인코더는 총 8억 5천만 개의 매개변수를 가집니다. 이 인코더는 입력 이미지의 각 패치(patch)에 대해 7680차원 임베딩을 생성하며, 각 이미지는 총 16 × 16 = 256개의 패치를 가집니다.

"우리는 이미지 인코더가 생성한 시각 토큰 표현과 언어 모델이 생성한 토큰 표현 사이에 교차 어텐션 레이어를 도입합니다." - [1]에서 발췌

이미지 어댑터(Image adapter). 이미지 인코더의 특징을 LLaMA-3에 통합하기 위해 교차 어텐션 기반 이미지 어댑터를 사용합니다. 더 구체적으로, LLM의 텍스트 토큰과 이미지 인코더의 이미지 임베딩 간에 어텐션을 계산하는 교차 어텐션 레이어가 LLM의 매 네 번째 트랜스포머 블록(transformer block)에 추가됩니다. 이러한 교차 어텐션 레이어는 모델의 크기를 크게 증가시킵니다. 예를 들어, LLaMA-3-405B는 이미지 어댑터와 함께 약 5천억 개의 매개변수를 가집니다. 그러나 이미지 어댑터는 LLM이 텍스트를 생성할 때 이미지 인코더의 정보를 토큰 표현에 통합할 수 있도록 합니다.

### 비디오 어댑터(Video adapter)

이미지 외에도, [1]의 저자들은 LLaMA-3를 비디오 입력을 지원하도록 확장합니다. 비디오가 단순히 이미지(또는 프레임)의 시퀀스라는 점을 감안할 때, 기존 아키텍처를 크게 수정할 필요는 없습니다. 모델은 64개의 프레임을 입력으로 받으며, 각 프레임은 기존 이미지 인코더를 통해 전달됩니다. 아래를 참조하십시오. 프레임 간의 시간적 관계를 포착하기 위해, 32개의 연속 프레임의 표현을 하나로 집계하는 Perceiver Resampler를 사용합니다. 마지막으로, 추가 비디오 교차 어텐션 레이어가 LLM에 추가됩니다. (출처: [1])

비디오 및 이미지 구성 요소를 모두 포함하는 다중 모달 LLaMA-3의 전체 아키텍처는 위에 표시되어 있습니다. 여기서 이미지 및 비디오 입력은 모두 먼저 이미지 인코더를 통해 처리된 다음, 교차 어텐션 레이어를 통해 LLM에 통합되는 것을 볼 수 있습니다. 비디오의 경우, 비디오 프레임 간의 순차적 관계를 포착하기 위해 추가 집계 모듈인 Perceiver Resampler를 추가합니다.

### 사전 훈련 데이터셋(Pretraining dataset) 구성

이미지 인코더와 교차 어텐션 레이어는 모두 대규모 이미지-텍스트 쌍 데이터셋에서 훈련됩니다. 이 데이터셋은 i) 비영어 캡션 제거, ii) 중복 제거, iii) 저품질 데이터 제거, iv) 다양성 극대화(즉, n-gram TF-IDF 점수 기반)를 위해 필터링됩니다. 비디오 어댑터 훈련을 위한 비디오-텍스트 쌍을 수집하기 위해 매우 유사한 프로세스가 따릅니다. 이러한 정교한 데이터 필터링 과정은 모델의 학습 효율성을 높이고 편향을 줄이며, 최종적으로는 더 견고한 다중 모달 이해 능력을 갖추도록 돕습니다.

LLaMA-3의 문서 이해 능력(document understanding capabilities)을 향상시키기 위해, [1]의 저자들은 또한 각 텍스트 캡션의 끝에 OCR 출력을 연결하고, 관련 텍스트와 함께 이미지로 표현된 많은 수의 문서를 수집합니다. 이는 모델이 단순히 이미지 내용을 파악하는 것을 넘어, 문서의 시각적 레이아웃과 텍스트 정보를 동시에 해석하는 능력을 기르도록 유도합니다.

LLaMA-3의 다른 주목할 만한 다중 모달 훈련 데이터 소스는 다음과 같습니다:
*   **시각적 접지(Visual grounding)**: 텍스트의 명사구(noun phrase)는 이미지에 오버레이되거나 텍스트에 (정규화된) 좌표를 통해 지정된 이미지 내의 바운딩 박스(bounding box) / 마스크(mask)에 연결됩니다. 이를 통해 모델은 텍스트에서 언급된 객체를 이미지 내에서 정확히 식별하는 능력을 학습합니다.
*   **스크린샷 파싱(Screenshot parsing)**: HTML 코드에서 스크린샷이 렌더링되고, 모델은 스크린샷에서 오버레이된 바운딩 박스로 표시된 요소를 생성한 코드를 예측하도록 요청받습니다. 이는 웹 페이지 구조와 시각적 요소 간의 관계를 이해하는 데 중요합니다.
*   **질문-답변 쌍(Question-answer pairs)**: 여러 소스에서 얻은 대량의 QA 데이터. 이 데이터는 모델이 주어진 이미지에 대해 질문을 이해하고 정확한 답변을 생성하는 능력을 향상시킵니다.
*   **합성 캡션(Synthetic captions)**: LLaMA-3 초기 버전이 생성한 합성 캡션이 있는 이미지. [1]의 저자들은 합성 캡션이 원래 사람이 작성한 캡션보다 더 포괄적인 경향이 있음을 관찰합니다. 이는 데이터 증강 효과를 가져와 모델의 일반화 성능을 높일 수 있습니다.
*   **합성 구조화 이미지(Synthetic structured images)**: 차트, 테이블, 순서도, 수학 방정식 등과 구조화된 표현(예: 마크다운 또는 LaTeX)이 함께 제공됩니다. 이러한 데이터는 모델이 복잡한 시각적 정보를 구조화된 텍스트로 변환하는 능력을 개발하는 데 필수적입니다.

### 이미지 어댑터 훈련(Image adapter training)

이미지 어댑터를 훈련하기 전에, 이미지 인코더는 위에서 설명한 데이터셋의 이미지-텍스트 쌍에 대해 여러 에포크(epoch) 동안 사전 훈련됩니다. 어댑터를 훈련할 때, 이미지 인코더의 가중치(weight)는 고정되지 않고 계속 업데이트됩니다. 그러나 이 훈련 과정 동안 LLM 가중치는 고정됩니다. 결과적으로, 다중 모달 LLaMA-3 모델의 LLM 백본은 텍스트 전용 LLaMA-3와 동일하며, 텍스트 전용 작업에서 동등한 성능을 보장합니다.

이미지 어댑터는 두 단계로 훈련되며, 두 단계 모두 텍스트 캡션에 적용된 표준 언어 모델링 목표를 사용합니다. 첫 번째 단계에서는 모든 이미지가 낮은 해상도로 크기 조정되어 훈련 프로세스를 가능한 한 효율적으로 만듭니다. 이 초기 훈련 단계 다음에는 더 짧은 두 번째 단계가 이어지는데, 이 단계에서는 이미지 해상도를 높이고 최고 품질의 데이터를 강조하는 원본 데이터셋의 더 작은(샘플링된) 버전을 사용합니다. 두 훈련 단계가 모두 완료되면, 완전히 훈련된 이미지 인코더와 어댑터로 시작하여 유사한 프로세스를 사용하여 비디오-텍스트 데이터셋에 대해 비디오 어댑터를 훈련합니다.

"사전 훈련 후, 우리는 고도로 선별된 다중 모달 대화 데이터에 대해 모델을 미세 조정하여 채팅 기능을 활성화합니다. 우리는 또한 인간 평가 성능을 향상시키기 위해 직접 선호 최적화(DPO)를 구현하고, 다중 모달 추론 능력을 개선하기 위해 거부 샘플링(rejection sampling)을 사용합니다." - [1]에서 발췌

### 후처리 훈련(Post training)

텍스트 기반 LLaMA-3 모델과 유사하게, 다중 모달 모델은 모델을 인간의 선호도에 맞추고, 지시를 따르는 방법을 가르치고, 대화형 입력을 처리하는 능력을 향상시키는 등 전체 후처리 훈련 절차를 거칩니다. 텍스트 전용 LLaMA-3 모델과 유사하게, 다중 모달 모델은 지도 미세 조정(SFT), 거부 샘플링(RS), 직접 선호 최적화(DPO)의 조합을 여러 번 순차적으로(즉, "라운드"로) 적용하여 후처리 훈련됩니다. 이 과정은 아래에 묘사되어 있으며, LLaMA-3의 후처리 훈련에 대한 전체 개요는 여기에서 찾을 수 있습니다.

(출처: [1])

이미지 인코더와 어댑터를 훈련할 때와는 달리, 후처리 훈련 중에는 LLM에 기본 LLaMA-3 모델의 가중치를 사용하지 않습니다. 대신, 이 기본 모델의 가중치를 이미 광범위한 후처리 훈련을 거친 LLaMA-3-Instruct 모델의 가중치로 대체합니다. 이는 모델이 이미 인간의 지시를 잘 따르고 대화 능력을 갖춘 상태에서 다중 모달 기능을 추가함으로써, 더욱 효과적인 상호작용 능력을 확보하기 위함입니다.

후처리 훈련을 위한 데이터셋은 다양한 소스에서 수집됩니다:
*   템플릿을 통해 또는 LLM으로 다시 작성하여 대화 형식으로 변환된 학술 데이터셋.
*   i) 시드 이미지 또는 비디오를 제공하고 사람에게 관련 대화를 작성하도록 요청하거나 ii) 사람에게 모델 출력을 비교하여 선호 쌍을 형성하도록 요청하여 수집된 인간 주석 데이터셋.
*   이미지 또는 비디오의 텍스트 표현(즉, 캡션)을 LLM에 제공하고 모델에게 관련 질문-답변 쌍을 생성하도록 프롬프트하여 수집된 합성 데이터셋.
*   오류를 생성하기 위해 LLM에 의해 미묘하게(그러나 의미 있게) 교란된 기존 모델 출력으로, 선호 쌍을 형성합니다.

후처리 훈련된 모델의 성능을 최적화하기 위해 몇 가지 독특한 전략이 채택됩니다. 예를 들어, 저자들은 후처리 훈련의 각 단계에서 여러 모델을(다른 하이퍼파라미터로) 훈련하고, 이 모델들의 가중치를 평균하여 최종 모델을 얻습니다. 이 모델 병합 접근 방식은 하이퍼파라미터 그리드 탐색(hyperparameter grid search)을 통해 얻은 최고의 모델보다 뛰어난 성능을 보입니다.

### LLaMA-3.2: 중간 크기 비전 LLM (LLaMA-3.2: Medium-Sized Vision LLMs) [2]

(출처: [2])

다중 모달 LLaMA-3 모델에 대한 예비 실험은 [1]에 제공되었지만, 이 모델들은 LLaMA-3.2 [2]가 출시될 때까지 공식적으로 공개되지 않았습니다. 11B 및 90B 매개변수 LLaMA-3.2 Vision 모델 9는 이미지를 입력으로 지원하고 이미지 캡션(image captioning) 및 문서 이해(document understanding)와 같은 시각 이해 작업에서 강력한 기능을 가진 최초의 LLaMA 모델이었습니다. [1]에서 탐색된 다른 양식(예: 음성 및 비디오)은 LLaMA-3.2에 포함되지 않았으며, 가장 큰 (405B 매개변수) LLaMA-3.1 모델의 다중 모달 버전은 출시되지 않았습니다. 이는 대규모 모델의 다중 모달 확장이 여전히 연구 과제이거나, 특정 규모에서 최적의 성능과 효율성을 우선시했음을 시사합니다.

### LLaMA-3.2 Vision 아키텍처

LLaMA-3.2 Vision 모델에 대해 [2]에 설명된 아키텍처는 [1]에 설명된 예비 모델의 아키텍처와 완벽하게 일치합니다. 이 모델들은 다음으로 구성됩니다:
*   사전 훈련된 LLM 백본(backbone).
*   사전 훈련된 비전 인코더(vision encoder).
*   LLM과 비전 인코더 사이의 여러 교차 어텐션(cross-attention) 레이어.

LLaMA-3.2의 LLM 백본은 단순히 텍스트 전용 LLaMA-3.1-8B 및 LLaMA-3.1-70B 모델입니다. 비전 LLM은 이미지-텍스트 쌍에 대해 여러 단계로 훈련되지만, 훈련 중에는 LLM 백본이 업데이트되지 않습니다. 우리는 이미지 인코더와 어댑터 레이어만 업데이트합니다. 결과적으로, 텍스트 전용 작업에서 LLaMA-3.2 Vision 모델의 성능은 LLaMA-3.1에 비해 그대로 유지됩니다.

### 훈련 단계(Stages of training)

앞서 언급했듯이, LLaMA-3.2 Vision 모델은 여러 단계로 훈련됩니다. 먼저, LLM 백본과 이미지 인코더를 서로 독립적으로 사전 훈련해야 합니다. 그런 다음 두 모델 사이에 교차 어텐션 레이어를 추가하여 이 모델들을 통합하고, 대규모(그리고 노이즈가 많은) 이미지-텍스트 쌍 데이터셋에 대해 결합된 비전 모델을 사전 훈련합니다. 마지막으로, 더 높은 품질의 향상된 데이터로 구성된 중간 크기 데이터셋에 대해 모델을 추가로 훈련하고 후처리 훈련을 수행합니다. 비전 모델의 후처리 훈련 전략에는 여러 라운드의 SFT, 거부 샘플링(rejection sampling) 및 DPO가 포함됩니다(즉, LLaMA-3.1과 동일). (출처: [2])

### 모델 평가(Model evaluation) 및 활용 분야

텍스트 기반 작업의 경우, LLaMA-3.2 모델의 성능은 LLaMA-3.1과 동일합니다. LLM 백본은 다중 모달 사전 훈련 프로세스에 의해 변경되지 않습니다. 그러나 저자들은 [2]에서 LLaMA-3.2 Vision 모델을 광범위한 시각 이해 작업에 걸쳐 평가하기도 했습니다. 위를 참조하십시오. 가장 주목할 만한 점은 이 모델들이 문서, 차트 또는 다이어그램과 관련된 작업에서 강력한 성능을 보인다는 것입니다. 이러한 능력은 모델이 많은 수의 문서-텍스트 쌍과 차트 및 테이블의 합성 이미지에 대해 훈련되었다는 점을 고려할 때 놀라운 일이 아닙니다. 다른 시각 이해 작업에서도 LLaMA-3.2는 계속해서 좋은 성능을 보이며 여러 선도적인 파운데이션 모델(foundation model)과 경쟁력이 있습니다. 이는 LLaMA-3.2 Vision이 단순한 이미지 캡셔닝을 넘어, 복잡한 시각적 정보를 해석하고 추론하는 데 탁월함을 보여줍니다. 특히, 차트나 다이어그램을 분석하여 통찰력을 제공하거나, 복잡한 문서를 요약하고 질의응답하는 등의 비즈니스 및 연구 분야에서 큰 잠재력을 가집니다.

### LLaMA-3.2 Vision 구현(LLaMA-3.2 Vision Implementation)

이제 LLaMA-3.2 Vision 모델에 대해 배웠으므로, 그 구현을 더 자세히 살펴보겠습니다. 이를 위해 torchtune에서 코드를 연구할 것입니다. 단순화를 위해 구현의 일부 세부 사항은 생략하고 핵심 모델링 구성 요소를 설명하는 의사 코드(pseudocode)를 제시할 것입니다. 그러나 관심 있는 분들은 torchtune에서 전체 코드를 언제든지 읽어볼 수 있습니다!

### 최상위 구조(Top-level structure)

LLaMA-3.2 Vision 아키텍처를 인스턴스화(instantiating)하는 기본 함수를 살펴보면, 모델이 예상대로 이미지 인코더(image encoder)와 LLM 백본(위에서는 비전 디코더(vision decoder)라고 불림)이라는 두 가지 주요 구성 요소로 구성되어 있음을 알 수 있습니다. 이 두 모델은 FusionModel에서 결합됩니다.

위에서 보듯이, 우리는 이 FusionModel의 훈련 가능한 구성 요소(trainable components)를 토글(toggle)할 수 있습니다. 이는 각 모델 구성 요소를 훈련 가능하게 할지 여부를 설정하고 비전 인코더의 출력을 일반적인 방식으로 비전 디코더에 전달합니다.

```python
# compute the output of the vision encoder
encoder_embed = None
if encoder_input is not None:
    encoder_embed = self.encoder(**encoder_input)

# pass the vision encoder output to the vision decoder
output = self.decoder(
    tokens=tokens,
    mask=mask,
    encoder_input=encoder_embed,
    encoder_mask=encoder_mask,
    input_pos=input_pos,
)
```

특히, FusionModel의 입출력 구조는 PyTorch 10의 표준 트랜스포머 디코더(transformer decoder)와 동일합니다. 이 두 가지 유형의 모델은 상호 교환적으로 사용될 수 있습니다. 위 코드에서 보듯이, 우리는 선택된 텍스트 토큰에서 이미지 토큰을 마스킹(mask)할 수 있는 인코더 마스크(encoder mask)를 제공할 수도 있습니다.

"DeepFusion은 사전 훈련된 인코더가 사전 훈련된 디코더(LLM)와 결합되는 융합 모델 아키텍처의 한 유형입니다... 이 모듈은 인코더와 디코더가 어떻게 융합되는지에 대한 가정을 하지 않습니다. 단순히 인코더 임베딩을 디코더에 전달하고 디코더가 모든 융합을 처리하도록 합니다." - 출처

LLaMA-3.2 Vision에서 사용되는 비전 인코더는 표준 CLIP 기반 비전 인코더입니다. 이 인코더는 입력 이미지를 CLIP을 통해 전달하여 이미지 임베딩 세트를 검색합니다. 여기서 우리는 CLIP의 출력을 비전 디코더에 직접 전달하지 않습니다. CLIP과 비전 디코더 사이에 추가적인 VisionProjectionHead 모듈이 있습니다. 구현은 아래에 제공됩니다.

이 모듈은 CLIP 임베딩을 비전 디코더에 흡수되기 전에 여러 추가 자가-어텐션 레이어를 통해 전달합니다. 또한, 투영 헤드(projection head)는 최종 레이어의 출력만 취하는 대신 CLIP 모델의 여러 숨겨진 레이어(hidden layer)에서 특징을 추출하여 지각 정보(perceptual information)가 손실되지 않도록 합니다. 이 모든 임베딩은 함께 연결되고 선형 투영되어 비전 디코더에서 사용되는 텍스트 토큰 벡터의 크기와 일치하도록 합니다.

LLaMA-3.2 Vision의 비전 디코더는 표준 텍스트 기반 LLM과 거의 동일합니다. 위를 참조하십시오. 우리는 단순히 이 아키텍처를 수정하여 디코더의 일부 레이어에 교차 어텐션 레이어를 추가합니다. 이를 위해 FusionLayer가 사용되며, 이는 교차 어텐션 레이어와 디코더 블록의 매개변수를 분리하여 유지합니다. 이러한 방식으로, 우리는 각 구성 요소를 훈련할지 여부를 토글할 수 있습니다. 예를 들어, LLAMA-3.2는 다중 모달 훈련 과정 내내 교차 어텐션 레이어를 훈련하고 LLM 백본을 고정된 상태로 유지합니다.

### 결론(Closing Remarks)

이 개요에서 얻어야 할 주요 시사점은 vLLM이 표준 텍스트 기반 LLM과 크게 다르지 않다는 사실입니다. 우리는 단순히 이 모델에 추가 이미지 인코더와 두 모델을 함께 융합하기 위한 몇 가지 추가 레이어를 추가합니다. 이미지 인코더와 텍스트 기반 LLM 간의 융합은 통합 임베딩 아키텍처 또는 교차 모달리티 어텐션을 통해 달성될 수 있습니다. 여기서부터 우리는 이 결합된 모델을 이미지-텍스트 쌍에 대해 (여러 단계로) 훈련하여 강력한 vLLM을 형성할 수 있습니다. vLLM의 많은 변형이 존재하지만, 그 뒤에 있는 근본적인 아이디어는 정말로 간단합니다!

vLLM의 발전은 단순히 텍스트와 이미지를 이해하는 것을 넘어, 미래의 AI 시스템이 현실 세계의 복잡한 정보를 더욱 풍부하게 인지하고 상호작용하는 기반을 마련하고 있습니다. 의료 영상 진단, 자율 주행, 창의적인 콘텐츠 생성 등 다양한 분야에서 vLLM의 혁신적인 활용이 기대됩니다. 앞으로 연구자들은 이러한 다중 모달 모델의 효율성을 높이고, 편향을 줄이며, 실제 배포 시 발생할 수 있는 윤리적 문제들을 해결하는 데 집중할 것입니다.

뉴스레터가 처음이신가요? 안녕하세요! 저는 Cameron R. Wolfe입니다. 넷플릭스(Netflix)의 딥러닝(Deep Learning) 박사이자 머신러닝 과학자(Machine Learning Scientist)입니다. 이것은 독자들이 AI 연구의 중요한 주제를 더 잘 이해하도록 돕는 Deep (Learning) Focus 뉴스레터입니다. 뉴스레터가 마음에 드신다면 구독, 공유 또는 X 및 LinkedIn에서 저를 팔로우해주세요!

[구독](https://cameronrwolfe.substack.com/subscribe)

### 참고문헌(Bibliography)

[1] Grattafiori, Aaron, et al. "The llama 3 herd of models." arXiv preprint arXiv:2407.21783 (2024).
[2] Meta LLaMA Team. “Llama 3.2: Revolutionizing edge AI and vision with open, customizable models” https://ai.meta.com/blog/llama-3-2-connect-2024-vision-edge-mobile-devices/ (2024).
[3] Dosovitskiy, Alexey, et al. "An image is worth 16x16 words: Transformers for image recognition at scale." arXiv preprint arXiv:2010.11929 (2020).
[4] Radford, Alec, et al. "Learning transferable visual models from natural language supervision." International conference on machine learning. PmLR, 2021.
[5] Joulin, Armand, et al. "Learning visual features from large weakly supervised data." Computer Vision–ECCV 2016: 14th European Conference, Amsterdam, The Netherlands, October 11–14, 2016, Proceedings, Part VII 14. Springer International Publishing, 2016.
[6] Desai, Karan, and Justin Johnson. "Virtex: Learning visual representations from textual annotations." Proceedings of the IEEE/CVF conference on computer vision and pattern recognition. 2021.
[7] Sohn, Kihyuk. “Improved deep metric learning with multi-class n-pair loss objective.” Advances in neural information processing systems 29 (2016).
[8] Vaswani, Ashish, et al. "Attention is all you need." Advances in neural information processing systems 30 (2017).
[9] Jaegle, Andrew, et al. "Perceiver: General perception with iterative attention." International conference on machine learning. PMLR, 2021.
[10] Alayrac, Jean-Baptiste, et al. "Flamingo: a visual language model for few-shot learning." Advances in neural information processing systems 35 (2022): 23716-23736.
[11] Team, Gemini, et al. "Gemini: A family of highly capable multimodal models, 2024." arXiv preprint arXiv:2312.11805 (2024).

---
1.  분명히, 디코더-온리 트랜스포머는 인코더 구성 요소가 없으므로 교차 어텐션 모듈은 이 아키텍처에서 단순히 제거됩니다.
2.  어텐션 점수를 계산할 때, 모든 어텐션 점수를 자가-어텐션에 사용되는 벡터의 크기인 d의 제곱근으로 나눕니다. 이를 스케일드 닷 프로덕트 어텐션(scaled dot product attention)이라고 하며, 이 나눗셈을 수행하면 훈련 안정성을 향상시키는 데 도움이 됩니다.
3.  ViT가 제안되기 전에는 컴퓨터 비전 작업에 가장 일반적으로 사용되는 아키텍처는 합성곱 신경망(CNN) 또는 특히 ResNet이었습니다.
4.  구체적으로, Flamingo는 ResNet 아키텍처를 사용하여 이미지 임베딩을 생성하지만, CLIP(LLM에 더 일반적으로 사용되는 비전 인코더)을 사용할 수도 있습니다.
5.  오픈 소스의 실제 정의와 존재하는 다양한 종류의 "오픈" LLM에 대한 더 깊은 개요는 이 글을 참조하십시오.
6.  [1]에서 저자들은 단순성을 극대화하려는 설계 원칙 때문에 MoE 아키텍처 사용을 피한다고 언급합니다. MoE는 더 복잡하고 훈련하기 어렵습니다.
7.  여기서 "H"는 "Huge"를 의미합니다. 이것은 [3]에서 탐색된 총 매개변수 측면에서 가장 큰 ViT 아키텍처입니다.
8.  이 전략의 동기는 ViT의 다른 레이어가 다른 종류의 정보를 포착한다는 것입니다. 예를 들어, 모델의 초기 레이어는 낮은 수준의 공간적 세부 정보를 포착할 가능성이 높고, 후기 레이어는 의미 정보를 포착합니다. 이 논문을 참조하십시오.
9.  1B 및 3B 매개변수를 가진 경량 모델도 LLaMA-3.2의 일부로 출시되었지만, 이 모델들은 텍스트 입력만 지원합니다.
10. 트랜스포머 디코더는 기본적으로 인코더에서 토큰 벡터 시퀀스를 입력으로 제공하는 옵션을 가집니다. 표준 트랜스포머에서는 인코더가 전체 인코더-디코더 아키텍처의 텍스트 인코더입니다. LLaMA-3.2 Vision의 경우, 인코더는 비전 인코더입니다!