**2년 전, 저는 십대 딸과 함께 비행기에 앉아 와이파이가 작동하지 않는 동안 새로운 AI 이미지 생성기를 가지고 놀고 있었습니다.**

약 2년 전, 저는 청소년기 딸과 함께 항공기에 탑승한 채, 인터넷 연결이 두절된 상황에서 당시 막 부상하던 인공지능 기반의 시각 예술 도구들을 탐색하고 있었습니다. 제 아이가 가장 선호하는 동물은 수달이었고, 통신망이 복구되자마자 저는 자연스럽게 "와이파이를 사용하는 비행기 위의 수달(otter on a plane using wifi)"이라는 문구를 입력했습니다. 이 생성물은 곧 대중의 관심을 끌었고, 이후 "와이파이를 사용하는 비행기 위의 수달"은 AI의 시각적 창조 능력 발전을 가늠하는 저의 개인적인 지표 중 하나로 자리매김했습니다. 챗GPT(ChatGPT)와 확산 모델(diffusion models)의 시대가 도래하기 전인 2021년, 가장 대중적인 AI 이미지 합성 도구였던 VQGAN + CLIP으로 해당 문구를 시각화했을 때의 결과는 다소 조악했습니다. 이처럼 평범하지 않은 입력 문구에서 시작된 것이 인공지능 기술 진보의 예기치 않은 척도가 된 것입니다. 이러한 수달 이미지를 수년간 추적하면서 저는 지난 몇 년간 인공지능 분야에서 세 가지 핵심적인 변화를 목격했습니다. 즉, 다양한 유형의 AI 유틸리티 확산, 기술 발전의 가속화, 그리고 로컬(local) 및 개방형 모델(open models)의 중요성 증대입니다.

초기 단계의 AI 아트워크는 종종 기괴하거나 추상적인 형태로 나타나곤 했습니다. 당시 생성된 수달 이미지들은 형태가 불분명하고 색상이 뒤섞여, 마치 물감이 녹아내린 듯한 인상을 주었죠. 이러한 결과는 당시 AI가 시각적 개념을 얼마나 피상적으로 이해하고 있는지를 여실히 보여주었습니다. 하지만 이 경험은 저에게 한 가지 중요한 통찰을 주었습니다. 즉, 특정하고 다소 비현실적인 시나리오를 제시하는 프롬프트(prompt)가 AI의 상상력과 구현 능력을 시험하는 데 매우 효과적이라는 점이었습니다. 2021년은 흔히 'AI의 봄'이라 불리는 시기의 초입이었고, 대중은 이러한 초기 AI 예술 작품들에 대해 호기심과 함께 약간의 당혹감을 느끼곤 했습니다. 이 시기부터 AI는 단순한 도구를 넘어 창의적 파트너로서의 가능성을 보여주기 시작했습니다.

**확산 모델(Diffusion models)**

제가 처음 만들어본 수달 이미지들은 확산(diffusion)이라는 과정을 통해 생성된 것이었습니다. 인공지능의 최근 발전사 대부분 동안, 이미지 제작은 확산이라는 생성적 접근 방식을 활용해왔으며, 이는 대규모 언어 모델(Large Language Models, LLM)과는 근본적으로 다른 작동 원리를 가집니다. LLM이 텍스트를 한 단어씩 순차적으로 생성하며 항상 전진하는 방식이라면, 확산 모델은 무작위적인 노이즈(static)에서 출발하여 수많은 반복 단계를 거쳐 전체 이미지를 동시에 변형합니다. 이는 마치 이야기를 한 문장씩 써내려가는 것과, 거친 대리석 덩어리에서 점진적으로 조각상을 깎아내는 것의 차이와 같습니다. 이미지의 각 부분은 선형적으로 구축되는 것이 아니라, 전체적으로 동시에 정제됩니다. 언어 모델이 "다음에 무엇이 올까?"를 예측하는 대신, 확산 모델은 "이 노이즈가 궁극적으로 무엇을 나타내야 하는가?"를 추론하며, 반복적인 정교화 과정을 통해 무작위성을 일관성 있는 시각적 결과물로 전환합니다.

이러한 반복적 정제 과정은 확산 모델이 단순히 픽셀을 추가하는 것을 넘어, 잠재 공간(latent space)이라는 추상적인 개념 영역에서 이미지의 본질적인 구조와 특징을 학습하고 조작하기 때문에 강력합니다. 마치 조각가가 대리석 내부의 형태를 상상하며 깎아내듯이, 확산 모델은 노이즈 속에서 이미지의 의미론적 내용을 찾아내고 이를 점진적으로 구체화합니다. 이러한 과정은 마르코프 연쇄(Markov chains)와 같은 확률론적 개념에 기반하며, 각 단계에서 노이즈를 제거하며 이미지의 품질을 향상시키는 노이즈 제거 오토인코더(denoising autoencoders)와 유사한 메커니즘을 사용합니다. 하지만 이러한 복잡한 계산 과정은 상당한 컴퓨팅 자원을 요구하며, 특히 고해상도 이미지를 생성할 때 더욱 그러합니다.

시중에는 다양한 확산 모델이 존재하지만, 저는 다른 많은 AI 도구들보다 더 오랜 역사를 가진 미드저니(Midjourney)를 주로 사용해왔습니다. 미드저니를 활용하면 "와이파이를 사용하는 비행기 위의 수달"이라는 간단한 명령을 통해 확산 모델들이 시간의 흐름에 따라 어떻게 진화했는지 명확히 관찰할 수 있습니다. (이 글에 제시된 모든 이미지와 영상의 경우, 저는 처음에 생성된 네 개의 이미지 중 가장 우수한 것을 선택했습니다.) 2022년 초의 흐릿하고 녹아내린 듯한 털 덩어리에서, 그해 말에는 (여전히 기형적인 손가락과 이상한 키보드를 가진) 보다 명확한 형태의 수달로 발전했습니다. 2023년에는 사진 같은(photorealistic) 수달을 얻었지만, 여전히 어색한 키보드와 비행기 창문 같은 사소한 결함이 남아있었습니다. 2024년에는 조명과 구도가 한층 개선되었고, 2025년에는 탁월한 시각적 사실성(photorealism)을 달성하게 됩니다.

초기 미드저니 이미지들에서는 종종 '아티팩트(artifacts)'라고 불리는 왜곡된 시각적 요소들이 눈에 띄었습니다. 예를 들어, 수달의 몸이 부분적으로 녹아내린 듯 보이거나, 팔다리가 비정상적으로 길거나 짧게 표현되기도 했습니다. 특히 손가락이나 텍스트와 같이 미세하고 구조적인 요소는 AI 이미지 생성에서 가장 어려운 과제 중 하나로 남아있었습니다. 이러한 초기 한계에도 불구하고, 확산 모델을 진정으로 매력적으로 만드는 것은 단지 사실적인 이미지를 생성하는 능력의 향상을 넘어, 다양한 예술적 양식으로 이미지를 창조할 수 있다는 점입니다. 이는 AI 이미지 생성 기술이 왜 그토록 논란의 여지가 많은지를 단적으로 보여줍니다. 수많은 AI 모델들이 저작권이 있는 저작물을 포함한 방대한 웹 데이터를 기반으로 훈련되었기 때문에, 살아있는 예술가들의 동의나 정당한 보상 없이 그들의 고유한 스타일을 복제할 수 있기 때문입니다.

하지만 이러한 능력이 역사적인 예술가나 고전적인 스타일에 적용될 때는 흥미로운 방식으로 작동합니다. 다음은 바이에른 태피스트리(Bayeux Tapestry)의 양식, 에곤 실레(Egon Schiele)의 표현주의적 화풍, 거리 예술 그래피티(street art graffiti)의 자유분방함, 그리고 일본 우키요에(Ukiyo-e) 판화의 독특한 미학적 특징을 담은 "와이파이를 사용하는 비행기 위의 수달" 이미지입니다. (사용자가 미술사에 대한 지식이 풍부할수록 이 이미지 생성기들의 잠재력을 더욱 깊이 있게 활용할 수 있습니다.) 스테이블 디퓨전(Stable Diffusion)이나 DALL-E와 같은 다른 인기 있는 확산 모델들도 유사한 스타일 변환 기능을 제공하지만, 미드저니는 특히 예술적이고 미학적인 결과물에 강점을 보입니다. 이러한 스타일 전이 기술은 종종 '스타일 임베딩(style embedding)'이나 LoRA(Low-Rank Adaptation)와 같은 기술을 활용하여 특정 예술가의 특징을 학습하고 이를 새로운 이미지에 적용하는 방식으로 구현됩니다. 현재 AI 예술과 저작권 침해에 대한 법적 소송이 전 세계적으로 진행 중이며, 이는 기술 발전과 윤리적, 법적 프레임워크 간의 복잡한 간극을 보여주고 있습니다.

확산 모델은 단순히 기존 양식을 모방하는 데 그치지 않습니다. 미드저니는 각 창작자가 자신만의 선호하는 양식으로 이미지를 제작하도록 모델을 훈련시킨 후, 이러한 고유한 "스타일 코드(style codes)"를 다른 이들과 공유할 수 있도록 지원합니다. 제가 프롬프트 끝에 이러한 스타일 코드를 추가하면, 사이버펑크(cyberpunk) 수달부터 만화적인 수달에 이르기까지 매우 상이한 결과물을 얻을 수 있습니다. 이는 LoRA 어댑터(LoRA adapters)와 같은 미세 조정(fine-tuning) 기법을 통해 구현되며, 사용자가 적은 데이터로도 특정 스타일이나 객체를 모델에 주입할 수 있게 합니다.

마지막으로 확산 방식으로 생성된 이미지를 하나 더 보여드리려 하는데, 이는 앞선 결과물들과 근본적으로 다릅니다. 이 이미지는 제가 플럭스(Flux)를 사용하여 제 개인 컴퓨터에서 직접 제작한 것입니다. 미드저니나 챗GPT(ChatGPT)와 같이 기업 데이터 센터에서 운영되는 독점적인 인공지능 모델들과 달리, 오픈 웨이트 모델(open weights models)은 누구나 자유롭게 내려받고, 수정하고, 자신의 환경에서 실행할 수 있습니다. 이 고품질 이미지는 거대 기술 기업의 서버가 아닌, 제 PC에 장착된 그래픽 카드(graphics card)를 통해 생성되었습니다(이미지 생성에 활용된 ComfyUI 인터페이스도 함께 보실 수 있습니다). 이러한 결과물은 최고 수준의 비공개 소스 모델(closed-source models)이 보여주는 품질에 놀랍도록 근접해 있습니다.

오픈 웨이트 모델의 등장은 AI의 민주화를 의미합니다. 이는 더 많은 사람들이 고가의 클라우드 서비스 없이도 강력한 AI 기술에 접근하고, 심지어 이를 자신만의 목적에 맞게 커스터마이징할 수 있게 함으로써, AI 개발 및 활용의 진입 장벽을 낮춥니다. 물론 이러한 모델들을 로컬에서 실행하려면 일정 수준 이상의 GPU VRAM과 컴퓨팅 파워가 필요하지만, 이는 개인이 소유할 수 있는 하드웨어의 범주 안에 있습니다. 이러한 개방성은 AI 기술의 검열 저항성(censorship resistance)을 높이고, 특정 기업의 통제를 벗어나 다양한 혁신을 촉진하는 중요한 역할을 합니다. 스테이블 디퓨전(Stable Diffusion)과 같은 다른 오픈 소스 대안들도 미드저니와 경쟁하며 빠르게 발전하고 있습니다.

오픈 소스든 독점 소스든, 확산 모델들은 다소 무작위적인 결과물을 생성하는 경향이 있으며, 단 하나의 고품질 이미지를 얻기 위해 여러 번의 시도가 필요한 경우가 많습니다. 이는 노이즈 제거 과정의 확률적 특성에서 기인하며, 매번 약간 다른 초기 노이즈 패턴이 다른 결과물로 이어질 수 있기 때문입니다. 최신 확산 모델들(예: 구글의 이마젠 4(Imagen 4)와 같은)은 이러한 문제를 개선하여 더 나은 성능을 보여주지만, 만족스러운 결과물을 얻기 위해서는 여전히 상당한 운과 반복적인 실험이 요구됩니다.

이러한 무작위성을 완화하기 위한 전략으로는 특정 시드 값(seed value)을 사용하여 동일한 초기 노이즈 패턴을 재현하거나, 프롬프트 엔지니어링(prompt engineering)을 통해 더욱 상세하고 명확한 지시를 제공하는 방법이 있습니다. 또한, '네거티브 프롬프트(negative prompts)'를 사용하여 원치 않는 요소들을 명시적으로 제외함으로써 결과물의 품질을 향상시킬 수도 있습니다. 확산 모델은 사용자가 원하는 이미지를 얻기 위해 상당한 '프롬프트 엔지니어링' 기술을 요구하는 반면, 다음에 설명할 멀티모달 모델은 보다 직관적인 방식으로 제어할 수 있습니다.

**멀티모달 이미지 생성(Multimodal Image Generation)**

대규모 언어 모델(Large Language Models) 시대의 대부분 동안, 챗GPT(ChatGPT)와 같은 LLM이 이미지를 생성할 때, 실제로는 내부적으로 확산 모델 중 하나를 호출하여 이미지를 제작하고 그 결과물을 사용자에게 보여주었습니다. 이 모든 과정이 간접적으로(LLM이 확산 모델에 프롬프트를 전달하고 확산 모델이 이미지를 생성하는 방식) 이루어졌기 때문에, 이미지를 만드는 과정은 전용 이미지 생성기를 사용하는 것보다 훨씬 더 예측 불가능하게 느껴졌습니다. 이러한 패러다임은 지난 몇 달간 OpenAI와 구글(Google)이 진정한 멀티모달 이미지 생성(multimodal image generation) 능력을 선보이면서 변화하기 시작했습니다. 노이즈를 이미지로 변환하는 확산 모델과 달리, 멀티모달 생성은 대규모 언어 모델이 단어를 하나씩 추가하는 방식과 유사하게, 작은 색상 패치(patch)를 하나씩 쌓아 올리며 이미지를 직접 생성할 수 있도록 합니다. 이 방식은 인공지능이 생성하는 이미지에 대해 훨씬 더 깊이 있고 세밀한 제어력(deep control)을 제공합니다. 다음은 제가 처음 시도한, 복잡한 지시가 담긴 프롬프트 "와이파이를 사용하는 비행기 위의 수달, 노트북 화면에는 와이파이를 사용하는 비행기 위의 수달 이미지를 생성하는 이미지 생성 소프트웨어가 있다"의 결과물입니다.

이전에는 LLM이 이미지 생성 API를 '호출'하는 방식이었다면, 이제는 모델 자체의 통합된 아키텍처(unified transformer architecture) 내에서 시각적 토큰(visual tokens)을 직접 생성하는 방식으로 바뀌었다는 점에서 큰 차이가 있습니다. 이는 텍스트와 이미지가 공유된 잠재 공간(shared latent space)에서 처리되어, 언어 모델이 시각적 요소를 텍스트처럼 유연하게 다룰 수 있게 합니다. DALL-E 3나 구글의 이마젠 2(Imagen 2) 및 제미니(Gemini) 모델들이 이러한 멀티모달 기능을 선도하고 있습니다. 이러한 심층적인 제어 능력은 특정 제품의 목업(mockup)을 만들거나, 정교한 장면 구성을 요구하는 광고 콘텐츠, 혹은 세부적인 스토리텔링이 필요한 예술 작품 등 다양한 상업적 및 창의적 분야에서 매우 중요합니다.

하지만 이제 고백할 것이 있습니다. 제 딸이 가장 좋아하는 동물은 단순한 수달이 아니라 해달이며, 지금까지의 모든 이미지들은 훨씬 더 흔한 민물 수달이었습니다. 마침내 멀티모달 생성 기술을 통해 저는 아버지로서의 체면을 세울 수 있었습니다. 멀티모달 모델은 "대신 해달로 만들어주고, 모히칸(mohawk) 스타일을 적용하며, 레이저 게이밍 노트북(Razer gaming laptop)을 사용하게 해달라"는 식으로 구체적인 변경 및 미세 조정을 가능하게 하기 때문입니다.

이러한 정밀한 제어는 AI가 이제 단순히 '그려내는' 것을 넘어, 사용자의 의도를 '이해하고' 이를 시각적으로 구현하는 단계에 도달했음을 시사합니다. 이는 특히 상업 예술, 제품 디자인, 그리고 특정 내러티브를 담은 콘텐츠 제작에 혁명적인 영향을 미칠 것입니다. 사용자는 더 이상 수많은 프롬프트 반복을 통해 원하는 이미지를 찾아 헤맬 필요 없이, 마치 유능한 디자이너에게 지시를 내리듯이 직관적으로 소통할 수 있게 됩니다. 저는 시각적 임팩트가 중요하거나 무작위적인 이미지 탐색에 시간을 투자할 용의가 있을 때는 여전히 미드저니와 이마젠을 사용하지만, 특정하고 정확한 그림을 원할 때는 이제 항상 멀티모달 이미지 생성기를 찾게 됩니다. 저는 이러한 멀티모달 모델들이 점점 더 보편화될 것이라고 생각합니다. 아직 오픈 웨이트 멀티모달 이미지 생성기는 없지만, 조만간 상황이 바뀔 가능성이 높습니다. 오픈 소스 멀티모달 모델 개발은 방대한 데이터셋과 엄청난 컴퓨팅 파워를 요구하기 때문에 기술적 난이도가 높지만, 오픈 소스 커뮤니티의 빠른 발전 속도를 고려할 때 이는 시간 문제일 뿐입니다.

**이미지를 위한 코드(Code)와 "불꽃(Sparks)" 사용**

멀티모달 생성은 인공지능이 이미지를 정교하게 제어할 수 있음을 보여줍니다. 그러나 여기서 더 깊은 질문이 제기됩니다. 과연 AI가 자신이 무엇을 만들고 있는지 실제로 이해하는 것일까요, 아니면 단순히 훈련 데이터(training data)에서 학습된 패턴을 재조합하는 것일까요? 진정한 공간 추론(spatial reasoning) 능력을 시험하기 위해, 우리는 AI에게 코드(code)를 사용하여 그림을 그리도록 강제할 수 있습니다. 여기에는 시각적 피드백도 없고, 의지할 수 있는 사전 학습된 이미지 패턴(pre-trained image patterns)도 없습니다. 이는 마치 수학적 지시(mathematical instructions)만을 사용하여 눈을 가린 채 그림을 그리라고 요청하는 것과 유사합니다.

특히 그림을 그리는 데 사용하기 어려운 코드 유형 중 하나는 학술 논문에서 과학 다이어그램(scientific diagrams)을 생성하는 데 주로 사용되는 수학적 언어인 TikZ입니다. 이 언어는 일반적인 그림 그리기에 너무 부적합하여, TikZ라는 이름 자체가 재귀적인 독일어 문구인 "TikZ ist kein Zeichenprogramm"("TikZ는 그림 그리기 프로그램이 아니다")에서 유래했습니다. 이러한 특성 때문에 TikZ를 그림 그리기에 사용하는 훈련 데이터는 거의 존재하지 않습니다. 이는 AI가 훈련 과정에서 코드를 단순히 '기억'할 수 없음을 의미하며, 스스로 창조해내야 한다는 것을 뜻합니다. 이 언어를 사용하여 순수한 수학적 원리로 이미지를 생성하는 것은 매우 어려운 작업입니다.

AI의 '패턴 매칭'과 '이해'에 대한 논쟁은 인공지능 연구의 핵심적인 철학적 문제입니다. AI가 단순히 데이터에서 통계적 상관관계를 찾아내는 것인지, 아니면 인간처럼 개념을 추론하고 새로운 지식을 구성하는 능력을 가지는지에 대한 질문이죠. TikZ는 이러한 질문에 답하기 위한 훌륭한 시험대입니다. 이는 벡터 그래픽(vector graphics) 기반으로 정확한 좌표와 논리적 구조를 요구하기 때문에, AI가 시각적 요소를 추상적인 개념으로 다루고 이를 구성하는 능력을 보여줄 수 있습니다. 실제로, 지금은 구형이 된 GPT-4가 그린 유니콘의 TikZ 그림은 매우 영향력 있는 논문("Sparks of AGI: Early experiments with GPT-4")에서 LLM이 인공 일반 지능(Artificial General Intelligence, AGI)의 "불꽃(spark)"을 지니고 있을 가능성을 보여주는 신호로 간주되었습니다. 만약 그렇지 않다면, 어떻게 그렇게 창의적이고 복잡한 코드를 스스로 만들어낼 수 있었을까요? 참고용으로 그 유니콘의 모습은 다음과 같습니다. 저는 구형 GPT-4에게 와이파이를 사용하는 비행기 위의 수달을 그리게 하는 데는 운이 좀 덜했습니다. 이 사례는 AI가 단순히 학습된 데이터를 반복하는 것을 넘어, 새로운 규칙과 논리를 조합하여 독창적인 결과물을 만들어내는 '구성성(compositionality)'의 가능성을 보여주었습니다.

하지만 제미니 2.5 프로(Gemini 2.5 Pro)와 같은 더 최신 모델에게 TikZ로 수달을 그려달라고 요청하면 어떻게 될까요? 완벽하지는 않지만(그리고 제미니는 "비행기 위(on a plane)"라는 문구를 문자 그대로 받아들여 수달을 날개 위에 앉혔습니다), 만약 분홍색 유니콘이 "불꽃(spark)"을 보여주었다면, 이 결과는 분명 더 큰 도약(larger leap)을 의미합니다. 이러한 AI의 '문자 그대로의 해석' 문제는 프롬프트 엔지니어링의 중요한 영역 중 하나이며, 사용자는 모호성을 줄이기 위해 더욱 명확하고 구체적인 지시를 내려야 합니다.

최신 LLM들이 코드를 생성할 때 보이는 '생각의 사슬(chain of thought)' 능력은 이러한 공간 추론 능력 향상에 기여합니다. 모델은 단순히 최종 코드를 출력하는 것이 아니라, 문제를 작은 단계로 나누고 각 단계를 논리적으로 해결해나가는 과정을 보여줄 수 있습니다. 이는 모델이 복잡한 지시를 더 잘 이해하고 계획을 수립하는 데 도움이 됩니다. 그리고 오픈 웨이트 모델(open weights models)도 이 분야에서 빠르게 따라잡고 있지만, 일반적으로 최첨단 기술에 몇 달 뒤처져 있습니다. 아마도 현재 사용 가능한 최고의 오픈 웨이트 모델인 딥시크 r1(DeepSeek r1)의 새 버전은 제미니와 같은 클로즈드 소스 모델만큼 좋지는 않지만, 지속적인 개선이 예상됩니다. 오픈 소스 커뮤니티의 활발한 기여와 협업은 이러한 모델들의 발전 속도를 가속화하는 중요한 동력입니다.

이러한 그림 자체는 모델이 아무것도 없는 상태에서 공간 관계를 추론(reasoning about spatial relationships from scratch)한다는 사실만큼 중요하지 않습니다. 이것이 바로 '불꽃(Sparks)' 논문의 저자들이 이러한 시스템이 훈련 데이터에서 단순한 패턴 매칭(pattern-matching)을 넘어 실제 이해(actual understanding)에 가까운 무언가를 개발하고 있다고 제안한 이유입니다. AI가 추상적인 개념을 이해하고 이를 물리적 공간에 투영하는 능력을 보여준다면, 이는 미래의 AI가 단순한 도구를 넘어 진정한 지능적 에이전트로서의 역할을 수행할 수 있음을 시사합니다.

**비디오(Video)**

정지 이미지가 인상적인 발전을 보여주었다면, 비디오 생성(video generation)은 인공지능 기술이 얼마나 빠르게 가속화(accelerating)되고 있는지를 더욱 극명하게 드러냅니다. 비디오 생성은 단순히 여러 장의 이미지를 이어 붙이는 것을 넘어, 시간적 일관성(temporal consistency), 움직임의 역학(motion dynamics), 그리고 객체 간의 상호작용을 이해하고 구현해야 하므로 이미지 생성보다 훨씬 더 복잡한 기술적 난이도를 가집니다. 이는 엄청난 컴퓨팅 자원을 요구하며, 고품질 비디오를 생성하는 데는 많은 양의 GPU 메모리와 처리 능력이 필요합니다.

이것은 2024년 7월 현재 사용 가능한 최고의 비디오 생성기로 평가받는 런웨이 젠-3 알파(Runway Gen-3 alpha)가 생성한 "컴퓨터에서 와이파이를 사용하는 비행기 위의 수달" 영상입니다. 그리고 이것은 불과 1년도 채 지나지 않은 2025년에 구글(Google)의 비오 3(Veo 3)에서 동일한 프롬프트인 "컴퓨터에서 와이파이를 사용하는 비행기 위의 수달"로 만든 것입니다. 네, 이 비디오에 삽입된 소리(sound) 또한 100% 인공지능에 의해 생성된 것입니다. AI가 텍스트를 음성으로 변환하는 것을 넘어, 영상에 맞는 효과음이나 배경 음악까지 자체적으로 생성하는 단계에 이르렀습니다.

그리고 이러한 주제를 이어가자면, 이제 제 집 컴퓨터에서 실행할 수 있는 오픈 웨이트 AI 모델(open weights AI models) 또한 비디오 생성 분야에 등장하고 있습니다. 이 모델들은 최첨단 기술에 비해 다소 뒤처져 있지만, 놀라운 속도로 그 격차를 좁히고 있습니다. 다음은 텐센트(Tencent)의 훈위안비디오(HunyuanVideo)로 동일한 프롬프트에 대해 얻은 결과입니다. 네, 아직은 다소 조악하지만, 이 영상은 대규모 데이터 센터가 아닌 제 개인 컴퓨터에서 만들어진 것입니다. AI 비디오 생성 기술의 발전은 영화 제작, 마케팅, 그리고 개인 콘텐츠 제작 등 다양한 분야에 혁명적인 변화를 가져올 잠재력을 가지고 있습니다. 이미지, 비디오, 오디오 생성 기술이 궁극적으로 하나의 통합된 모델로 수렴될 것이라는 예측도 나오고 있습니다.

**이 모든 것이 의미하는 것**

수달의 진화 과정은 몇 가지 중요한 시사점을 지닌 두 가지 핵심적인 추세(crucial trends)를 명확히 보여줍니다. 첫째, 이미지 생성부터 비디오 제작, 그리고 LLM 기반의 코드 생성(LLM code generation)에 이르기까지, 인공지능의 광범위한 기능(AI capabilities) 전반에서 급속한 발전이 지속되고 있다는 점입니다. 둘째, 오픈 웨이트 모델(open weights models)은 일반적으로 독점 모델(proprietary models)만큼의 성능을 즉각적으로 보여주지는 못하지만, 최첨단 기술에 불과 몇 달 뒤처져 있을 뿐이며 그 격차를 빠르게 좁히고 있다는 사실입니다.

이러한 두 가지 추세(trends)를 종합적으로 고려해 볼 때, 우리는 인공지능이 생성한 이미지와 비디오가 대부분의 사람들을 속일 만큼 충분히 사실적일 뿐만 아니라, 이러한 기능이 매우 광범위하게 사용 가능하며, 특히 오픈 모델(open models)의 확산 덕분에 규제하거나 통제하기가 극도로 어려울 것이라는 방향으로 나아가고 있음이 분명해집니다. 저는 우리가 즐기는 엔터테인먼트 콘텐츠부터 온라인상의 정보에 대한 신뢰에 이르기까지, 사회 전반의 광범위한 영역에 영향을 미칠, 현실과 인공지능 생성 이미지 및 비디오를 구별하기 불가능한 세상에 대비해야 한다고 생각합니다.

이러한 미래는 단순히 기술적 진보의 문제가 아니라, 윤리적, 사회적, 심지어 존재론적 질문들을 던집니다. 딥페이크(deepfakes)와 같은 악용 사례는 이미 현실이 되고 있으며, 가짜 정보(misinformation)의 확산은 민주주의 사회에 심각한 위협이 될 수 있습니다. 이러한 문제에 대응하기 위해 워터마킹(watermarking), 출처 추적(provenance tracking), 그리고 AI 생성 콘텐츠 탐지 도구(AI detection tools)와 같은 기술적 해법들이 모색되고 있지만, 오픈 소스 모델의 특성상 그 효과는 제한적일 수 있습니다. 또한, 이러한 기술의 발전은 창조 산업에 종사하는 예술가와 디자이너들에게 경제적 영향을 미칠 것이며, 정책 입안자와 기술 기업들은 이러한 변화에 대한 책임감 있는 대응 방안을 마련해야 합니다.

제가 구글 비오 3(Veo 3)에 간단한 텍스트 프롬프트(text prompts)를 사용하여 만든 이 마지막 비디오에서 보듯이, 그러한 미래는 결코 멀리 있지 않습니다. 영상을 시청하신 후(프롬프트 "뮤지컬 캣츠(Cats) 같지만 수달 버전"의 결과물에 대해 미리 사과드립니다), 2022년의 첫 미드저니 이미지를 다시 한번 살펴보시기 바랍니다. 텍스트 프롬프트가 추상적인 털 덩어리(abstracts masses of fur)를 생성하던 시점부터, 소리가 포함된 사실적인 비디오(realistic videos with sound)를 생성하는 현재에 이르기까지 채 3년도 걸리지 않았습니다. 이는 인간의 인지 능력과 현실에 대한 인식이 기술의 발전 속도를 따라잡지 못할 수도 있다는 깊은 성찰을 요구합니다.