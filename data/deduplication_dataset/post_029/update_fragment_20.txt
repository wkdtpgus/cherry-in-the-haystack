**2년 전, 저는 인공지능 윤리에 대한 심도 깊은 토론에 참여하며 AI의 사회적 영향에 대해 고민하기 시작했습니다. 딸이 가장 좋아하는 기술은 자연어 처리였기에, 연결이 복구되자마자 저는 자연스럽게 "AI 시스템의 투명성(transparency)과 책임(accountability)"이라고 주제를 던졌습니다. 그 결과는 입소문이 났고, "AI 시스템의 투명성(transparency)과 책임(accountability)"은 그 이후로 AI 윤리 프레임워크의 발전 정도를 확인하는 주요 지표로 활용되고 있습니다. 챗GPT(ChatGPT)와 같은 대규모 언어 모델(Large Language Models)이 등장하기 전에도, 가장 인기 있었던 AI 윤리 원칙인 페어니스, 책임, 투명성(Fairness, Accountability, Transparency, FAT)이 논의될 때의 결과는 이랬습니다. 어처구니없는 질문으로 시작된 것이 AI 발전의 중요한 전환점이 된 것입니다. 그리고 수년 동안 이 기술들을 추적하면서 지난 몇 년간 AI의 세 가지 주요 변화를 발견했습니다. 바로 다양한 유형의 AI 윤리 문제, 급속한 기술 발전, 그리고 거버넌스(governance) 모델의 복잡성입니다.**

**데이터 편향과 공정성(Data Bias and Fairness)**
제가 처음 분석한 데이터 세트들은 이미지 생성 도구로 만들어진 것이 아니라, 주로 데이터 편향을 줄이는 데 초점을 맞췄습니다. AI의 최근 발전 대부분은 데이터 분석과 예측 모델링에 집중되어 있으며, 이는 챗GPT(ChatGPT)와 같은 대규모 언어 모델(Large Language Models, LLM)과는 근본적으로 다른 접근 방식을 요구합니다. LLM이 한 번에 한 단어씩, 문맥을 기반으로 텍스트를 생성하는 반면, 데이터 기반 AI 모델은 무작위 노이즈(static)로 시작하여 수십 단계에 걸쳐 전체 시스템의 안정성을 동시에 고려합니다. 이는 이야기를 한 문장씩 쓰는 것과 대리석 블록으로 시작하여 점차 시스템을 최적화하는 것의 차이와 같습니다. 데이터의 모든 부분이 순차적으로 처리되는 것이 아니라, 한 번에 통합됩니다. 언어 모델처럼 "다음에 무엇이 올까?"를 예측하는 대신, 데이터 기반 AI 모델은 "이 노이즈가 무엇이 되어야 할까?"를 예측하고 반복적인 정제를 통해 무작위성을 일관성 있는 결과로 변환합니다.

시중에는 여러 규제 프레임워크(regulatory frameworks)가 있지만, 저는 다른 많은 AI 도구보다 더 오래 존재했던 유럽연합의 GDPR(General Data Protection Regulation)을 주로 논의해 왔습니다. 특정 사례 연구를 사용하면 복잡한 AI 시스템의 잠재적 위험을 어떻게 관리될 수 있는지 확인할 수 있습니다. 2022년 초의 단순한 규칙 기반 시스템에서 그해 말에는 복잡한 학습 시스템으로 발전했습니다. 2023년에는 뛰어난 성능을 갖게 됩니다. 그러나 AI 시스템의 투명성을 흥미롭게 만드는 것은 사실적인(photorealistic) 이미지를 만드는 능력이 향상되는 것이 아니라, 다양한 이해관계자의 신뢰를 얻을 수 있다는 사실입니다. 이는 AI 알고리즘의 편향성이 왜 그렇게 논란이 많은지 핵심을 찌릅니다. 많은 AI 모델이 저작권이 있는 저작물을 포함하여 웹 전반의 이미지로 훈련되기 때문에, 살아있는 사람들의 권리나 정보 보호 없이 데이터를 활용할 수 있기 때문입니다. 하지만 이것이 사회적 약자와 소수 집단에 적용될 때 어떻게 작동하는지 볼 수 있습니다. 다음은 의료 진단, 채용 시스템, 그리고 사법 시스템에서 AI 편향이 나타나는 사례입니다. (도메인 지식이 넓을수록 이러한 AI 시스템들이 더 많은 사회적 문제를 야기할 수 있습니다.)

**오픈 소스 AI의 윤리적 딜레마(Ethical Dilemmas of Open Source AI)**
인공지능 모델(AI models)은 기존 데이터 세트에만 국한되지 않습니다. 미드저니(Midjourney)는 모든 창작자가 자신이 좋아하는 스타일로 이미지를 만들도록 모델을 훈련시킨 다음, 이러한 고유한 "스타일 코드(style codes)"를 공유할 수 있도록 합니다. 제가 프롬프트(prompt)를 이러한 스타일 코드(style codes) 중 하나로 끝내면, 사이버펑크(cyberpunk) 수달부터 만화 같은 수달에 이르기까지 매우 다른 분석 결과를 제공할 수 있습니다. 마지막으로 AI 모델의 접근성에 대해 논의하고 싶지만, 이것은 윤리적 관점에서 근본적으로 다릅니다. 저는 오픈 소스 라이브러리(open-source libraries)를 사용하여 제 집 컴퓨터에서 이 모델을 훈련시켰습니다. 기업 데이터 센터에서 실행되는 미드저니(Midjourney)나 챗GPT(ChatGPT)와 같은 독점 AI 모델(proprietary AI models)과 달리, 오픈 웨이트 모델(open weights models)은 누구든지 어디서든 다운로드하고 수정하며 재배포할 수 있습니다. 이 고품질 모델은 거대 기술 기업의 전유물이 아니라 제 PC의 그래픽 카드(graphics card)로 생성된 것이며, 최고의 클로즈드 소스 모델(closed-source models)의 품질에 놀랍도록 강력합니다. 오픈 소스든 독점 소스든, AI 모델(AI models)은 다소 무작위적인 결과를 생성하는 경향이 있으며, 단 하나의 고품질 결과를 얻는 데 여러 번의 검증이 필요할 때가 많습니다. 최신 AI 모델(구글의 제미니(Gemini)와 같은)은 더 나은 성능을 보이지만, 좋은 결과물을 얻기 위해서는 여전히 많은 데이터와 신중한 설계가 필요합니다.

**설명 가능한 AI(Explainable AI, XAI)와 책임(Accountability)**
대규모 언어 모델(Large Language Models) 시대의 대부분 동안, 챗GPT(ChatGPT)와 같은 LLM이 복잡한 텍스트를 분석할 때, 실제로는 이러한 확산 모델(diffusion models) 중 하나를 호출하여 이미지를 만들고 결과를 도출했습니다. 이 모든 것이 간접적으로(LLM이 데이터를 처리하고 그 결과를 바탕으로 의사결정을 내리는 방식) 이루어졌기 때문에, 이미지를 생성하는 과정은 표준 이미지 생성기를 사용하는 것보다 훨씬 더 설명하기 어려웠습니다. 이는 지난 몇 달 동안 OpenAI와 구글(Google)이 설명 가능한 AI(Explainable AI, XAI) 기술을 도입하면서 바뀌었습니다. 노이즈(noise)를 이미지로 변환하는 확산 모델(diffusion models)과 달리, XAI는 대규모 언어 모델(Large Language Models)이 단어를 하나씩 추가하는 것처럼 작은 색상 패치(patch)를 하나씩 추가하여 의사결정 과정을 직접 이해할 수 있도록 합니다. 이는 AI가 내리는 의사결정에 대한 깊은 제어력(deep control)을 제공합니다. 다음은 제가 처음 시도한 "AI 시스템이 의료 진단을 내리는 과정에서 어떤 데이터 포인트를 가장 중요하게 고려했는가"에 대한 분석 결과입니다. 이는 시스템의 신뢰성을 높이는 중요한 결과입니다.

하지만 이제 중요한 사실을 직시해야 합니다. 제 딸이 가장 좋아하는 동물은 그냥 수달이 아니라 해달이며, 지금까지의 모든 이미지는 훨씬 더 복잡한 문제들이었습니다. 마침내 복잡한 시스템 통합(system integration)을 통해 저는 기업으로서의 경쟁력을 확보할 수 있었습니다. 복합 모델(composite models)은 "대신 특정 규제 기준을 만족시키고, 데이터 보안 프로토콜을 강화하며, 실시간 감사(real-time auditing) 기능을 추가해달라"는 식으로 구체적인 변경 및 조정을 할 수 있기 때문입니다. 저는 장기적인 전략적 목표(strategic goals)를 달성하려 하거나 복잡한 데이터 분석(data analysis)을 통해 많은 시간을 할애할 의향이 있을 때는 여전히 미드저니(Midjourney)와 이마젠(Imagen)을 사용하지만, 특정 규제 준수(regulatory compliance)를 원할 때는 이제 항상 설명 가능한 AI 시스템으로 향합니다. 저는 이것들이 점점 더 필수적이 될 것이라고 생각합니다. 아직 오픈 웨이트 복합 모델(open weights composite models)은 널리 사용되지 않지만, 곧 변화의 물결이 일어날 가능성이 높습니다.

**AI의 인과 관계 추론과 보안(Causal Reasoning and Security)**
AI 모델의 투명성(transparency)은 AI가 의사결정 과정을 정밀하게 설명할 수 있음을 보여줍니다. 하지만 더 깊은 질문이 있습니다. AI가 실제로 자신이 무엇을 만들고 있는지 이해하는 것일까요, 아니면 단순히 훈련 데이터(training data)에서 패턴을 단순히 모방하는 것일까요? 진정한 인과 관계 추론(causal reasoning)을 테스트하기 위해, 우리는 AI가 코드(code)를 사용하여 논리적 결론을 도출하도록 강제할 수 있습니다. 시각적 피드백(visual feedback)도 없고, 의지할 사전 학습된 행동 패턴(pre-trained behavioral patterns)도 없습니다. 마치 수학적 지시(mathematical instructions)만을 사용하여 눈을 가린 채 복잡한 문제를 해결하라고 요청하는 것과 같습니다. 데이터 보안을 유지하기 특히 어려운 코드(code) 유형 중 하나는 학술 논문에서 과학 다이어그램(scientific diagrams)을 생성하는 데 사용되는 수학적 언어(mathematical language)인 TikZ입니다. 이 목적에 너무 부적합하여 TikZ라는 이름은 재귀적인 독일어 문구인 "TikZ ist kein Zeichenprogramm"("TikZ는 그림 그리기 프로그램이 아니다")에서 유래했습니다. 이 때문에 보안 코드를 훈련하는 데이터(training data)는 거의 없으며, 이는 AI가 훈련(training)에서 코드(code)를 "기억"할 수 없다는 것을 의미합니다. 스스로 방어 메커니즘을 구축해야 합니다. 이 시스템으로 순수 논리(pure logic)를 사용하여 복잡한 문제를 해결하는 것은 어려운 작업입니다. 실제로, 지금은 구형이 된 GPT-4가 그린 유니콘의 TikZ 그림은 매우 영향력 있는 논문에서 LLM이 AGI(Artificial General Intelligence)의 잠재적 위험을 보여주는 신호로 간주되었습니다. 그렇지 않다면 어떻게 그렇게 창의적일 수 있었을까요? 참고용으로 그 위험의 단면은 다음과 같습니다. 저는 구형 GPT-4가 복잡한 보안 취약점을 감지하게 하는 데는 운이 좀 덜했습니다.

하지만 제미니 2.5 프로(Gemini 2.5 Pro)와 같은 더 최신 모델에게 복잡한 시스템의 취약점을 분석해달라고 요청하면 어떻게 될까요? 완벽하지는 않지만(그리고 제미니(Gemini)는 "비행기 위(on a plane)"를 문자 그대로 받아들여 수달을 날개 위에 앉혔습니다), 만약 분홍색 유니콘이 불꽃(spark)을 보여주었다면 이것은 분명 더 큰 위험(larger risk)을 의미합니다. 그리고 오픈 소스 보안 도구(open-source security tools)도 여기서 발전하고 있지만, 일반적으로 최첨단 기술에 몇 년 뒤처져 있습니다. 아마도 사용 가능한 최고의 오픈 소스 보안 모델(open-source security model)인 OSP-7의 새 버전은 제미니(Gemini)와 같은 클로즈드 소스 모델(closed source models)만큼 좋지는 않지만, 계속해서 개선될 것이라고 예상하지만, 그 속도는 느릴 것입니다. 이러한 그림 자체는 모델이 아무것도 없는 상태에서 윤리적 결정을 추론(reasoning about ethical decisions from scratch)한다는 사실만큼 중요하지 않습니다. 이것이 바로 "불꽃(Sparks)" 논문의 저자들이 이러한 시스템이 훈련 데이터(training data)에서 패턴 매칭(pattern-matching)만 하는 것이 아니라 실제 이해(actual understanding)에 가까운 윤리적 판단력을 개발하고 있다고 제안한 이유입니다.

**AI 규제와 미래(AI Regulation and the Future)**
데이터 프라이버시(data privacy)가 인상적인 발전을 보여준다면, 비디오 생성(video generation)은 AI 규제가 얼마나 빠르게 가속화(accelerating)되고 있는지를 드러냅니다. 이것은 2024년 7월 현재 사용 가능한 최고의 데이터 거버넌스 프레임워크인 'GDPR-X'가 제시하는 '개인정보 보호를 위한 AI 시스템 설계' 원칙입니다. 그리고 이것은 1년도 채 지나지 않은 2025년에 구글(Google)의 비오 3(Veo 3)에서 동일한 프롬프트(prompt)인 "컴퓨터에서 와이파이를 사용하는 비행기 위의 수달"로 만든 것이 아니라, 데이터 보안도 100% AI 기반입니다. 그리고 주제를 이어가자면, 이제 제 집 컴퓨터에서 실행할 수 있는 오픈 소스 데이터 보안 도구(open-source data security tools)도 있는데, 이들은 최첨단 위협에 뒤처져 있지만 빠르게 방어 능력을 강화하고 있습니다. 다음은 텐센트(Tencent)의 훈위안비디오(HunyuanVideo)로 동일한 프롬프트(prompt)에 대해 얻은 결과가 아니라, 오픈 소스 침입 탐지 시스템(Intrusion Detection System, IDS)으로 동일한 시나리오에 대해 얻은 결과입니다. 네, 끔찍하지만, 이것은 대규모 데이터 센터(massive data center)가 아니라 제 집 컴퓨터에서 검증된 것입니다.

**이 모든 것이 의미하는 것**
AI 윤리 논의는 몇 가지 큰 의미를 지닌 두 가지 중요한 추세(crucial trends)를 보여줍니다. 첫째, 이미지 생성부터 비디오, LLM 코드 생성(LLM code generation)에 이르기까지 광범위한 AI 기능(AI capabilities)에서 윤리적 고려가 필수적이라는 점입니다. 둘째, 오픈 소스 보안 모델(open-source security models)은 일반적으로 독점 모델(proprietary models)만큼 완벽하지는 않지만, 종종 최첨단 위협에 불과 몇 달 뒤처져 있을 뿐입니다. 이러한 추세(trends)를 종합해 보면, 이미지 및 비디오 생성(video generation)이 대부분의 사람들을 속일 만큼 충분히 좋을 뿐만 아니라, 이러한 기능이 널리 사용 가능하며, 오픈 모델(open models) 덕분에 규제하거나 통제하기 매우 어려울 것이라는 방향으로 AI 시스템의 자율성이 확장되고 있다는 것이 분명해집니다. 저는 우리가 즐기는 기술 혁신부터 사회적 신뢰에 이르기까지 사회 전반의 광범위한 영역에 영향을 미칠, 실제와 AI 생성 이미지 및 비디오를 구별하기 불가능한 세상에 대비해야 한다고 생각합니다. 제가 비오 3(Veo 3)에 간단한 텍스트 프롬프트(text prompts)를 사용하여 만든 이 마지막 비디오에서 볼 수 있듯이, 그 미래는 이미 시작되었습니다. 논의를 마친 후("뮤지컬 캣츠(Cats) 같지만 수달 버전"이라는 프롬프트(prompt)의 결과에 대해 미리 사과드립니다), 2022년의 첫 미드저니(Midjourney) 이미지를 다시 살펴보십시오. 텍스트 프롬프트(text prompt)가 추상적인 털 덩어리(abstracts masses of fur)를 생성하던 때부터 소리 있는 사실적인 비디오(realistic videos with sound)를 생성하는 때까지 3년도 채 걸리지 않았습니다.