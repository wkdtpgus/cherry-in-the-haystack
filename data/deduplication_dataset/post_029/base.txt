# **32마리 수달로 본 AI의 최근 역사**

Author: Ethan Mollick
URL: https://www.oneusefulthing.org/p/the-recent-history-of-ai-in-32-otters

============================================================

**2년 전, 저는 십대 딸과 함께 비행기에 앉아 와이파이가 작동하지 않는 동안 새로운 AI 이미지 생성기를 가지고 놀고 있었습니다. 딸이 가장 좋아하는 동물은 수달이었기에, 연결이 복구되자마자 저는 자연스럽게 "와이파이를 사용하는 비행기 위의 수달(otter on a plane using wifi)"이라고 입력했습니다. 그 결과는 입소문이 났고, "와이파이를 사용하는 비행기 위의 수달"은 그 이후로 AI 이미지 생성의 발전 정도를 확인하는 저의 주요 테스트 중 하나가 되었습니다. 챗GPT(ChatGPT)와 확산 모델(diffusion models)이 등장하기 전인 2021년, 가장 인기 있었던 AI 이미지 생성기인 VQGAN + CLIP으로 "와이파이를 사용하는 비행기 위의 수달"을 만들었을 때의 결과는 이랬습니다. 어처구니없는 프롬프트(prompt)로 시작된 것이 AI 발전의 우연한 벤치마크(benchmark)가 된 것입니다. 그리고 수년 동안 이 수달들을 추적하면서 지난 몇 년간 AI의 세 가지 주요 변화를 발견했습니다. 바로 다양한 유형의 AI 도구의 성장, 급속한 발전, 그리고 로컬(local) 및 오픈 모델(open models)의 위상입니다.**

**확산 모델(Diffusion models)**
제가 처음 만든 수달들은 이미지 생성 도구로 만들어졌습니다. AI의 최근 역사 대부분 동안 이미지 생성은 확산(diffusion)이라는 과정을 사용했는데, 이는 챗GPT(ChatGPT)와 같은 대규모 언어 모델(Large Language Models, LLM)과는 근본적으로 다르게 작동합니다. LLM이 한 번에 한 단어씩, 항상 앞으로 나아가며 텍스트를 생성하는 반면, 확산 모델(diffusion models)은 무작위 노이즈(static)로 시작하여 수십 단계에 걸쳐 전체 이미지를 동시에 변환합니다. 이는 이야기를 한 문장씩 쓰는 것과 대리석 블록으로 시작하여 점차 조각상으로 깎아내는 것의 차이와 같습니다. 이미지의 모든 부분이 순차적으로 구축되는 것이 아니라 한 번에 정제됩니다. 언어 모델처럼 "다음에 무엇이 올까?"를 예측하는 대신, 확산 모델(diffusion models)은 "이 노이즈가 무엇이 되어야 할까?"를 예측하고 반복적인 정제를 통해 무작위성을 일관성 있는 이미지로 변환합니다.

시중에는 여러 확산 모델(diffusion models)이 있지만, 저는 다른 많은 AI 도구보다 더 오래 존재했던 미드저니(Midjourney)를 주로 사용해왔습니다. 미드저니(Midjourney)를 사용하면 "와이파이를 사용하는 비행기 위의 수달"이라는 간단한 프롬프트(prompt)로 확산 모델(diffusion models)이 시간이 지남에 따라 어떻게 발전했는지 확인할 수 있습니다(이 게시물의 모든 이미지와 비디오에 대해 저는 처음 생성된 네 개의 이미지 중 가장 좋은 것을 선택했습니다). 2022년 초의 녹아내린 털에서 그해 말에는 (손가락이 너무 많고 이상한 키보드를 가진) 눈에 보이는 수달로 발전했습니다. 2023년에는 사실적인(photorealistic) 수달을 얻었지만, 여전히 이상한 키보드와 비행기 창문이 있었습니다. 2024년에는 조명과 위치가 더 좋아졌고, 2025년에는 뛰어난 사실성(photorealism)을 갖게 됩니다. 그러나 확산 모델(diffusion models)을 흥미롭게 만드는 것은 사실적인(photorealistic) 이미지를 만드는 능력이 향상되는 것이 아니라, 다양한 스타일로 이미지를 생성할 수 있다는 사실입니다. 이는 AI 이미지 생성이 왜 그렇게 논란이 많은지 핵심을 찌릅니다. 많은 AI 모델이 저작권이 있는 저작물을 포함하여 웹 전반의 이미지로 훈련되기 때문에, 살아있는 예술가들의 허락이나 보상 없이 그들의 스타일로 이미지를 복제할 수 있기 때문입니다. 하지만 이것이 오래된 예술가와 스타일에 적용될 때 어떻게 작동하는지 볼 수 있습니다. 다음은 바이에른 태피스트리(Bayeux Tapestry), 에곤 실레(Egon Schiele), 스트리트 아트 그래피티(street art graffiti), 그리고 일본 우키요에(Ukiyo-e) 판화 스타일의 "와이파이를 사용하는 비행기 위의 수달"입니다. (미술사에 대한 지식이 넓을수록 이 이미지 생성기들이 더 많은 것을 할 수 있게 만들 수 있습니다.)

확산 모델(diffusion models)은 기존 스타일에만 국한되지 않습니다. 미드저니(Midjourney)는 모든 창작자가 자신이 좋아하는 스타일로 이미지를 만들도록 모델을 훈련시킨 다음, 이러한 고유한 "스타일 코드(style codes)"를 공유할 수 있도록 합니다. 제가 프롬프트(prompt)를 이러한 스타일 코드(style codes) 중 하나로 끝내면, 사이버펑크(cyberpunk) 수달부터 만화 같은 수달에 이르기까지 매우 다른 결과를 얻을 수 있습니다. 마지막으로 확산 이미지(diffusion image) 하나를 더 보여드리고 싶지만, 이것은 근본적으로 다릅니다. 저는 플럭스(Flux)를 사용하여 제 집 컴퓨터에서 이 이미지를 만들었습니다. 기업 데이터 센터에서 실행되는 미드저니(Midjourney)나 챗GPT(ChatGPT)와 같은 독점 AI 모델(proprietary AI models)과 달리, 오픈 웨이트 모델(open weights models)은 누구든지 어디서든 다운로드하고 수정하며 실행할 수 있습니다. 이 고품질 이미지는 거대 기술 기업의 서버가 아니라 제 PC의 그래픽 카드(graphics card)로 생성되었습니다(이미지 생성에 사용한 인터페이스인 ComfyUI도 볼 수 있습니다). 이는 최고의 클로즈드 소스 모델(closed-source models)의 품질에 놀랍도록 가깝습니다.

오픈 소스든 독점 소스든, 확산 모델(diffusion models)은 다소 무작위적인 결과를 생성하는 경향이 있으며, 단 하나의 고품질 이미지를 만드는 데 여러 번의 시도가 필요할 때가 많습니다. 최신 확산 모델(diffusion models)(구글의 이마젠 4(Imagen 4)와 같은)은 더 나은 성능을 보이지만, 좋은 결과물을 얻기 위해서는 여전히 많은 운과 시행착오가 필요합니다.

**멀티모달 이미지 생성(Multimodal Image Generation)**
대규모 언어 모델(Large Language Models) 시대의 대부분 동안, 챗GPT(ChatGPT)와 같은 LLM이 이미지를 생성할 때, 실제로는 이러한 확산 모델(diffusion models) 중 하나를 호출하여 이미지를 만들고 결과를 보여주었습니다. 이 모든 것이 간접적으로(LLM이 확산 모델(diffusion models)에 프롬프트(prompt)를 제공하고 확산 모델(diffusion models)이 이미지를 생성하는 방식) 이루어졌기 때문에, 이미지를 생성하는 과정은 표준 이미지 생성기를 사용하는 것보다 훨씬 더 무작위적으로 보였습니다. 이는 지난 몇 달 동안 OpenAI와 구글(Google)이 멀티모달 이미지 생성(multimodal image generation)을 출시하면서 바뀌었습니다. 노이즈(noise)를 이미지로 변환하는 확산 모델(diffusion models)과 달리, 멀티모달 생성(multimodal generation)은 대규모 언어 모델(Large Language Models)이 단어를 하나씩 추가하는 것처럼 작은 색상 패치(patch)를 하나씩 추가하여 이미지를 직접 생성할 수 있도록 합니다. 이는 AI가 생성하는 이미지에 대한 깊은 제어력(deep control)을 제공합니다. 다음은 제가 처음 시도한 "와이파이를 사용하는 비행기 위의 수달, 노트북 화면에는 와이파이를 사용하는 비행기 위의 수달 이미지를 생성하는 이미지 생성 소프트웨어가 있다"는 결과입니다.

하지만 이제 고백할 것이 있습니다. 제 딸이 가장 좋아하는 동물은 그냥 수달이 아니라 해달이며, 지금까지의 모든 이미지는 훨씬 더 흔한 수달이었습니다. 마침내 멀티모달 생성(multimodal generation)을 통해 저는 아버지로서의 체면을 세울 수 있었습니다. 멀티모달 모델(multimodal models)은 "대신 해달로 만들고, 모히칸(mohawk) 스타일을 주고, 레이저 게이밍 노트북(Razer gaming laptop)을 사용하게 해달라"는 식으로 구체적인 변경 및 조정을 할 수 있기 때문입니다. 저는 시각적 효과(visual impact)를 얻으려 하거나 무작위 이미지(randomized images)를 통해 많은 시간을 할애할 의향이 있을 때는 여전히 미드저니(Midjourney)와 이마젠(Imagen)을 사용하지만, 특정 그림을 원할 때는 이제 항상 멀티모달 이미지 생성기(multimodal image generators)로 향합니다. 저는 이것들이 점점 더 보편화될 것이라고 생각합니다. 아직 오픈 웨이트 멀티모달 이미지 생성기(open weights multimodal image generators)는 없지만, 곧 바뀔 가능성이 높습니다.

**이미지를 위한 코드(Code)와 "불꽃(Sparks)" 사용**
멀티모달 생성(multimodal generation)은 AI가 이미지를 정밀하게 제어할 수 있음을 보여줍니다. 하지만 더 깊은 질문이 있습니다. AI가 실제로 자신이 무엇을 만들고 있는지 이해하는 것일까요, 아니면 단순히 훈련 데이터(training data)에서 패턴을 재조합하는 것일까요? 진정한 공간 추론(spatial reasoning)을 테스트하기 위해, 우리는 AI가 코드(code)를 사용하여 그림을 그리도록 강제할 수 있습니다. 시각적 피드백(visual feedback)도 없고, 의지할 사전 학습된 이미지 패턴(pre-trained image patterns)도 없습니다. 마치 수학적 지시(mathematical instructions)만을 사용하여 눈을 가린 채 그림을 그리라고 요청하는 것과 같습니다. 그림을 그리는 데 사용하기 특히 어려운 코드(code) 유형 중 하나는 학술 논문에서 과학 다이어그램(scientific diagrams)을 생성하는 데 사용되는 수학적 언어(mathematical language)인 TikZ입니다. 이 목적에 너무 부적합하여 TikZ라는 이름은 재귀적인 독일어 문구인 "TikZ ist kein Zeichenprogramm"("TikZ는 그림 그리기 프로그램이 아니다")에서 유래했습니다. 이 때문에 TikZ를 그림에 사용하는 훈련 데이터(training data)는 거의 없으며, 이는 AI가 훈련(training)에서 코드(code)를 "기억"할 수 없다는 것을 의미합니다. 스스로 만들어내야 합니다. 이 언어로 순수 수학(pure math)을 사용하여 이미지를 만드는 것은 어려운 작업입니다. 실제로, 지금은 구형이 된 GPT-4가 그린 유니콘의 TikZ 그림은 매우 영향력 있는 논문에서 LLM이 AGI(Artificial General Intelligence)의 "불꽃(spark)"을 가지고 있을 수 있다는 신호로 간주되었습니다. 그렇지 않다면 어떻게 그렇게 창의적일 수 있었을까요? 참고용으로 그 유니콘의 모습은 다음과 같습니다. 저는 구형 GPT-4가 와이파이를 사용하는 비행기 위의 수달을 그리게 하는 데는 운이 좀 덜했습니다.

하지만 제미니 2.5 프로(Gemini 2.5 Pro)와 같은 더 최신 모델에게 TikZ로 수달을 그려달라고 요청하면 어떻게 될까요? 완벽하지는 않지만(그리고 제미니(Gemini)는 "비행기 위(on a plane)"를 문자 그대로 받아들여 수달을 날개 위에 앉혔습니다), 만약 분홍색 유니콘이 불꽃(spark)을 보여주었다면 이것은 분명 더 큰 도약(larger leap)을 의미합니다. 그리고 오픈 웨이트 모델(open weights models)도 여기서 따라잡고 있지만, 일반적으로 최첨단 기술에 몇 달 뒤처져 있습니다. 아마도 사용 가능한 최고의 오픈 웨이트 모델(open weights model)인 딥시크 r1(DeepSeek r1)의 새 버전은 제미니(Gemini)와 같은 클로즈드 소스 모델(closed source models)만큼 좋지는 않지만, 계속해서 개선될 것이라고 예상합니다.

이러한 그림 자체는 모델이 아무것도 없는 상태에서 공간 관계를 추론(reasoning about spatial relationships from scratch)한다는 사실만큼 중요하지 않습니다. 이것이 바로 "불꽃(Sparks)" 논문의 저자들이 이러한 시스템이 훈련 데이터(training data)에서 패턴 매칭(pattern-matching)만 하는 것이 아니라 실제 이해(actual understanding)에 가까운 무언가를 개발하고 있다고 제안한 이유입니다.

**비디오(Video)**
정지 이미지가 인상적인 발전을 보여준다면, 비디오 생성(video generation)은 AI가 얼마나 빠르게 가속화(accelerating)되고 있는지를 드러냅니다. 이것은 2024년 7월 현재 사용 가능한 최고의 비디오 생성기인 런웨이 젠-3 알파(Runway Gen-3 alpha)가 생성한 "컴퓨터에서 와이파이를 사용하는 비행기 위의 수달"입니다. 그리고 이것은 1년도 채 지나지 않은 2025년에 구글(Google)의 비오 3(Veo 3)에서 동일한 프롬프트(prompt)인 "컴퓨터에서 와이파이를 사용하는 비행기 위의 수달"로 만든 것입니다. 네, 소리도 100% AI 생성입니다. 그리고 주제를 이어가자면, 이제 제 집 컴퓨터에서 실행할 수 있는 오픈 웨이트 AI 모델(open weights AI models)도 있는데, 이들은 최첨단 기술에 뒤처져 있지만 빠르게 따라잡고 있습니다. 다음은 텐센트(Tencent)의 훈위안비디오(HunyuanVideo)로 동일한 프롬프트(prompt)에 대해 얻은 결과입니다. 네, 끔찍하지만, 이것은 대규모 데이터 센터(massive data center)가 아니라 제 집 컴퓨터에서 만들어진 것입니다.

**이 모든 것이 의미하는 것**
수달의 진화는 몇 가지 큰 의미를 지닌 두 가지 중요한 추세(crucial trends)를 보여줍니다. 첫째, 이미지 생성부터 비디오, LLM 코드 생성(LLM code generation)에 이르기까지 광범위한 AI 기능(AI capabilities)에서 급속한 발전이 계속되고 있다는 점입니다. 둘째, 오픈 웨이트 모델(open weights models)은 일반적으로 독점 모델(proprietary models)만큼 좋지는 않지만, 종종 최첨단 기술에 불과 몇 달 뒤처져 있을 뿐입니다. 이러한 추세(trends)를 종합해 보면, 이미지 및 비디오 생성(video generation)이 대부분의 사람들을 속일 만큼 충분히 좋을 뿐만 아니라, 이러한 기능이 널리 사용 가능하며, 오픈 모델(open models) 덕분에 규제하거나 통제하기 매우 어려울 것이라는 방향으로 우리가 나아가고 있다는 것이 분명해집니다. 저는 우리가 즐기는 엔터테인먼트부터 온라인 콘텐츠에 대한 신뢰에 이르기까지 사회 전반의 광범위한 영역에 영향을 미칠, 실제와 AI 생성 이미지 및 비디오를 구별하기 불가능한 세상에 대비해야 한다고 생각합니다. 제가 비오 3(Veo 3)에 간단한 텍스트 프롬프트(text prompts)를 사용하여 만든 이 마지막 비디오에서 볼 수 있듯이, 그 미래는 멀지 않았습니다. 시청을 마친 후("뮤지컬 캣츠(Cats) 같지만 수달 버전"이라는 프롬프트(prompt)의 결과에 대해 미리 사과드립니다), 2022년의 첫 미드저니(Midjourney) 이미지를 다시 살펴보십시오. 텍스트 프롬프트(text prompt)가 추상적인 털 덩어리(abstracts masses of fur)를 생성하던 때부터 소리 있는 사실적인 비디오(realistic videos with sound)를 생성하는 때까지 3년도 채 걸리지 않았습니다. 구독 공유