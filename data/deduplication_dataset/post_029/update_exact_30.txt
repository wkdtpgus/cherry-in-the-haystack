**AI 이미지 생성의 진화: 수달 벤치마크를 통해 본 지난 3년간의 변화와 미래**

**2년 전, 저는 십대 딸과 함께 비행기에 앉아 와이파이가 작동하지 않는 동안 새로운 AI 이미지 생성기를 가지고 놀고 있었습니다. 딸이 가장 좋아하는 동물은 수달이었기에, 연결이 복구되자마자 저는 자연스럽게 "와이파이를 사용하는 비행기 위의 수달(otter on a plane using wifi)"이라고 입력했습니다. 그 결과는 입소문이 났고, "와이파이를 사용하는 비행기 위의 수달"은 그 이후로 AI 이미지 생성의 발전 정도를 확인하는 저의 주요 테스트 중 하나가 되었습니다. 챗GPT(ChatGPT)와 확산 모델(diffusion models)이 등장하기 전인 2021년, 가장 인기 있었던 AI 이미지 생성기인 VQGAN + CLIP으로 "와이파이를 사용하는 비행기 위의 수달"을 만들었을 때의 결과는 이랬습니다. 어처구니없는 프롬프트(prompt)로 시작된 것이 AI 발전의 우연한 벤치마크(benchmark)가 된 것입니다. 그리고 수년 동안 이 수달들을 추적하면서 지난 몇 년간 AI의 세 가지 주요 변화를 발견했습니다. 바로 다양한 유형의 AI 도구의 성장, 급속한 발전, 그리고 로컬(local) 및 오픈 모델(open models)의 위상입니다.**

**AI 도구의 다양화와 발전 가속화**

지난 몇 년간 AI 도구의 성장은 단순히 이미지 생성기에만 국한되지 않았습니다. 텍스트를 기반으로 코드를 생성하는 코딩 도우미(coding assistants), 음성을 텍스트로 변환하고 번역하는 음성 AI(speech AI), 그리고 복잡한 데이터를 분석하여 통찰력을 제공하는 분석 도구에 이르기까지, AI는 거의 모든 산업 분야에 침투했습니다. 특히 이미지 생성 분야에서는 VQGAN + CLIP에서 시작하여 DALL-E, Stable Diffusion, 그리고 Midjourney와 같은 강력한 모델들이 연이어 등장하며 접근성과 품질 모두에서 비약적인 발전을 이루었습니다. 이러한 발전은 단순히 기술적인 진보를 넘어, 창작의 문턱을 낮추고 개인 창작자들이 이전에는 상상하기 어려웠던 수준의 콘텐츠를 생산할 수 있도록 지원하고 있습니다.

오픈 모델(open models)의 위상 또한 크게 변화했습니다. 처음에는 독점 모델(proprietary models)에 비해 품질이나 기능 면에서 뒤처지는 경향이 있었지만, 이제는 Stable Diffusion XL(SDXL)과 같은 오픈 소스 모델들이 최첨첨 기술에 매우 근접한 성능을 보여주며 커뮤니티 주도의 혁신을 이끌고 있습니다. 개발자들은 이러한 오픈 모델을 기반으로 특정 목적에 맞게 모델을 미세 조정(fine-tuning)하거나, LoRA(Low-Rank Adaptation)와 같은 기술을 활용하여 특정 스타일이나 개념을 학습시키는 등 무한한 가능성을 탐색하고 있습니다. 이는 AI 기술의 민주화를 가속화하며, 누구나 강력한 AI 도구를 활용할 수 있는 환경을 조성하고 있습니다.

**확산 모델(Diffusion models)**
**제가 처음 만든 수달들은 이미지 생성 도구로 만들어졌습니다. AI의 최근 역사 대부분 동안 이미지 생성은 확산(diffusion)이라는 과정을 사용했는데, 이는 챗GPT(ChatGPT)와 같은 대규모 언어 모델(Large Language Models, LLM)과는 근본적으로 다르게 작동합니다. LLM이 한 번에 한 단어씩, 항상 앞으로 나아가며 텍스트를 생성하는 반면, 확산 모델(diffusion models)은 무작위 노이즈(static)로 시작하여 수십 단계에 걸쳐 전체 이미지를 동시에 변환합니다. 이는 이야기를 한 문장씩 쓰는 것과 대리석 블록으로 시작하여 점차 조각상으로 깎아내는 것의 차이와 같습니다. 이미지의 모든 부분이 순차적으로 구축되는 것이 아니라 한 번에 정제됩니다. 언어 모델처럼 "다음에 무엇이 올까?"를 예측하는 대신, 확산 모델(diffusion models)은 "이 노이즈가 무엇이 되어야 할까?"를 예측하고 반복적인 정제를 통해 무작위성을 일관성 있는 이미지로 변환합니다.**

**확산 모델의 발전과 창의적 활용**

미드저니(Midjourney)의 사례는 확산 모델(diffusion models)이 얼마나 빠르게 진화했는지를 잘 보여줍니다. 초기 버전에서 보여준 다소 추상적이거나 기괴한 이미지는 불과 몇 년 만에 실제 사진과 구별하기 어려운 수준의 사실적인(photorealistic) 결과물로 발전했습니다. 미드저니 v6나 v7과 같은 최신 버전은 텍스트 프롬프트(text prompt)에 대한 이해도가 훨씬 높아져, 사용자가 원하는 세부적인 묘사나 구도, 조명 효과까지 정확하게 반영할 수 있게 되었습니다. 예를 들어, 단순히 "수달"이 아니라 "따뜻한 저녁 햇살 아래 평화롭게 노트북을 사용하는 수달, 팝아트 스타일"과 같이 구체적인 지시를 내릴수록 더욱 정교하고 예술적인 이미지를 얻을 수 있습니다.

확산 모델을 효과적으로 사용하기 위한 '프롬프트 엔지니어링(prompt engineering)'은 이제 하나의 예술이자 과학으로 자리 잡았습니다. 단순히 키워드를 나열하는 것을 넘어, 이미지의 구성 요소(주제, 배경, 스타일, 조명, 카메라 앵글 등)를 명확히 정의하고, 원치 않는 요소를 제거하기 위한 네거티브 프롬프트(negative prompts)를 활용하는 것이 중요합니다. 예를 들어, "화려한 색감의 미래 도시 풍경, 네온사인, 비 내리는 밤, 8K, 언리얼 엔진 5(Unreal Engine 5) --ar 16:9 --style raw"와 같이 구체적인 매개변수(parameters)를 추가하여 원하는 결과에 근접할 수 있습니다.

예술 스타일 복제 논란은 여전히 뜨거운 감자입니다. 기존 예술가들의 작품으로 훈련된 모델이 그들의 스타일을 모방하는 것에 대한 저작권 및 윤리적 문제는 계속해서 논의되고 있습니다. 그러나 이러한 기술은 한편으로는 사라져가는 고대 예술 양식을 재해석하거나, 특정 시대의 시각적 특징을 현대적인 맥락에서 재창조하는 데에도 활용될 수 있습니다. 예를 들어, "르네상스 시대의 유화 스타일로 그린 우주 비행사"와 같은 프롬프트는 과거와 현재, 그리고 미래를 융합하는 새로운 창작의 가능성을 열어줍니다.

오픈 소스 확산 모델(open source diffusion models)의 발전 또한 주목할 만합니다. Stable Diffusion XL(SDXL)은 고품질 이미지를 생성할 수 있는 가장 강력한 오픈 모델 중 하나로, ComfyUI나 Automatic1111과 같은 사용자 친화적인 인터페이스와 결합되어 전문가뿐만 아니라 일반 사용자도 쉽게 접근할 수 있게 되었습니다. 특히 LoRA(Low-Rank Adaptation)와 같은 기술은 사용자가 특정 캐릭터, 스타일, 또는 오브젝트를 소량의 데이터로 모델에 학습시켜 자신만의 맞춤형 모델을 만들 수 있도록 합니다. 이는 AI 이미지 생성의 개인화와 커스터마이징(customizing) 시대를 열었으며, 특정 브랜드나 개인의 고유한 시각적 아이덴티티(visual identity)를 AI로 구현하는 데 활용됩니다. 이제는 클라우드 기반의 독점 모델뿐만 아니라, 로컬 환경에서도 상당한 수준의 이미지 생성이 가능해지면서 개인 정보 보호나 데이터 주권(data sovereignty) 측면에서도 새로운 가능성을 제시하고 있습니다.

**멀티모달 이미지 생성(Multimodal Image Generation)**
**대규모 언어 모델(Large Language Models) 시대의 대부분 동안, 챗GPT(ChatGPT)와 같은 LLM이 이미지를 생성할 때, 실제로는 이러한 확산 모델(diffusion models) 중 하나를 호출하여 이미지를 만들고 결과를 보여주었습니다. 이 모든 것이 간접적으로(LLM이 확산 모델(diffusion models)에 프롬프트(prompt)를 제공하고 확산 모델(diffusion models)이 이미지를 생성하는 방식) 이루어졌기 때문에, 이미지를 생성하는 과정은 표준 이미지 생성기를 사용하는 것보다 훨씬 더 무작위적으로 보였습니다. 이는 지난 몇 달 동안 OpenAI와 구글(Google)이 멀티모달 이미지 생성(multimodal image generation)을 출시하면서 바뀌었습니다. 노이즈(noise)를 이미지로 변환하는 확산 모델(diffusion models)과 달리, 멀티모달 생성(multimodal generation)은 대규모 언어 모델(Large Language Models)이 단어를 하나씩 추가하는 것처럼 작은 색상 패치(patch)를 하나씩 추가하여 이미지를 직접 생성할 수 있도록 합니다. 이는 AI가 생성하는 이미지에 대한 깊은 제어력(deep control)을 제공합니다. 다음은 제가 처음 시도한 "와이파이를 사용하는 비행기 위의 수달, 노트북 화면에는 와이파이를 사용하는 비행기 위의 수달 이미지를 생성하는 이미지 생성 소프트웨어가 있다"는 결과입니다.**

**멀티모달 제어의 심화와 새로운 응용**

멀티모달 이미지 생성(multimodal image generation)은 단순한 이미지 생성을 넘어, AI가 이미지의 내용을 '이해'하고 '조작'하는 능력을 보여줍니다. 해달과 수달의 구별처럼 미묘한 차이를 인식하고 반영하는 것은 물론, 이미지 내의 특정 객체를 선택하여 스타일을 변경하거나, 배경을 완전히 교체하거나, 심지어 객체의 자세나 표정을 수정하는 등 정교한 편집이 가능해졌습니다. 이는 GPT-4o나 Claude 3 Vision과 같은 최신 멀티모달 모델들이 텍스트뿐만 아니라 이미지, 비디오, 오디오 등 다양한 형태의 정보를 동시에 처리하고 추론하는 능력을 갖추었기 때문입니다.

이러한 멀티모달 기능은 단순히 이미지를 생성하는 것을 넘어, 기존 이미지를 분석하고 개선하는 데에도 활용됩니다. 예를 들어, "이 사진 속 인물의 표정을 더 행복하게 바꿔줘" 또는 "이 제품 사진의 배경을 숲으로 바꿔주고, 조명을 부드럽게 해줘"와 같은 자연어 명령만으로 복잡한 이미지 편집 작업을 수행할 수 있습니다. 이는 그래픽 디자이너나 콘텐츠 제작자에게 혁신적인 워크플로우(workflow)를 제공하며, 아이디어를 시각화하는 과정을 획기적으로 단축시킵니다.

아직 오픈 웨이트 멀티모달 이미지 생성기(open weights multimodal image generators)는 독점 모델만큼 정교하지는 않지만, LLaVA(Large Language and Vision Assistant)와 같은 프로젝트들이 빠르게 발전하고 있습니다. 이들은 시각적 질문 응답(Visual Question Answering, VQA)이나 이미지 캡셔닝(image captioning)과 같은 작업에서 뛰어난 성능을 보이며, 오픈 소스 커뮤니티에서도 멀티모달 이해 및 생성 능력을 구현하기 위한 노력이 활발히 진행 중입니다. 가까운 미래에는 로컬 환경에서도 고도로 제어 가능한 멀티모달 이미지 생성 및 편집이 가능해질 것으로 예상됩니다.

**AI의 공간 추론 능력과 '불꽃'의 재해석**

AI가 단순히 훈련 데이터(training data)의 패턴을 재조합하는 것을 넘어, 진정한 의미의 이해와 추론 능력을 가지고 있는지에 대한 질문은 AI 연구의 핵심입니다. '불꽃(Sparks)' 논문에서 언급된 TikZ 코드 생성은 AI가 시각적 피드백 없이 순수 논리적 지시만으로 공간 관계를 구축하는 능력을 보여주는 중요한 사례였습니다. 이는 AI가 단순한 이미지 픽셀 패턴을 암기하는 것이 아니라, 도형, 선, 좌표와 같은 추상적인 개념을 이해하고 이를 바탕으로 새로운 시각적 구조를 만들어낼 수 있음을 시사합니다.

최근에는 이러한 능력이 더욱 발전하여, 텍스트 프롬프트만으로 3D 모델이나 복잡한 CAD 도면을 생성하는 AI 모델들이 등장하고 있습니다. 예를 들어, "강아지 모양의 로봇이 뛰어가는 모습의 3D 모델" 또는 "내부 구조가 보이는 자동차 엔진의 단면도"와 같은 명령으로 실제 사용 가능한 3D 에셋(asset)을 만들어내는 것은 AI의 공간 추론 능력이 비약적으로 발전했음을 보여줍니다. 이는 건축, 제품 디자인, 게임 개발 등 다양한 분야에서 혁신적인 변화를 가져올 잠재력을 가지고 있습니다. 이러한 발전은 AI가 단순히 '무엇을 그려야 하는지'를 아는 것을 넘어, '어떻게 구성되어야 하는지'를 이해하고 있음을 의미하며, 이는 AGI(Artificial General Intelligence)를 향한 중요한 단계로 해석될 수 있습니다. 오픈 소스 커뮤니티에서도 Blender나 Unity와 같은 3D 소프트웨어와 연동되는 AI 플러그인(plugins)이 개발되어, 텍스트 기반 3D 모델링의 문턱을 낮추고 있습니다.

**비디오(Video)**
정지 이미지가 인상적인 발전을 보여준다면, 비디오 생성(video generation)은 AI가 얼마나 빠르게 가속화(accelerating)되고 있는지를 드러냅니다. 이것은 2024년 7월 현재 사용 가능한 최고의 비디오 생성기인 런웨이 젠-3 알파(Runway Gen-3 alpha)가 생성한 "컴퓨터에서 와이파이를 사용하는 비행기 위의 수달"입니다. 그리고 이것은 1년도 채 지나지 않은 2025년에 구글(Google)의 비오 3(Veo 3)에서 동일한 프롬프트(prompt)인 "컴퓨터에서 와이파이를 사용하는 비행기 위의 수달"로 만든 것입니다. 네, 소리도 100% AI 생성입니다. 그리고 주제를 이어가자면, 이제 제 집 컴퓨터에서 실행할 수 있는 오픈 웨이트 AI 모델(open weights AI models)도 있는데, 이들은 최첨단 기술에 뒤처져 있지만 빠르게 따라잡고 있습니다. 다음은 텐센트(Tencent)의 훈위안비디오(HunyuanVideo)로 동일한 프롬프트(prompt)에 대해 얻은 결과입니다. 네, 끔찍하지만, 이것은 대규모 데이터 센터(massive data center)가 아니라 제 집 컴퓨터에서 만들어진 것입니다.

**비디오 생성의 혁명적 진보**

정지 이미지의 발전이 놀라웠다면, 비디오 생성(video generation)은 그야말로 혁명적인 속도로 발전하고 있습니다. 런웨이 젠-3 알파(Runway Gen-3 alpha)와 구글의 비오 3(Veo 3)가 보여준 "와이파이를 사용하는 비행기 위의 수달" 비디오는 단순한 움직임을 넘어, 일관된 스토리텔링과 감정 표현까지 가능한 수준에 도달했습니다. 특히 OpenAI의 Sora, Luma AI의 Dream Machine, 그리고 중국의 Kling과 같은 최신 모델들은 단 몇 줄의 텍스트 프롬프트만으로 수십 초 길이의 고품질 비디오 클립을 생성할 수 있으며, 이는 마치 실제 촬영된 영상과 구별하기 어려울 정도의 사실성을 자랑합니다.

이러한 비디오 생성 AI는 단순히 장면을 만드는 것을 넘어, 카메라 움직임, 조명, 캐릭터의 표정 변화까지 세밀하게 제어할 수 있습니다. 예를 들어, "석양 아래 해변을 걷는 두 연인, 드라마틱한 카메라 워크, 감성적인 배경 음악"과 같은 프롬프트로 완벽한 분위기의 비디오를 만들 수 있습니다. 이는 영화 제작, 광고, 소셜 미디어 콘텐츠 제작 등 광범위한 분야에서 콘텐츠 생산 방식을 근본적으로 변화시킬 잠재력을 가지고 있습니다.

오픈 웨이트 모델(open weights models) 또한 비디오 생성 분야에서 빠르게 추격하고 있습니다. 텐센트(Tencent)의 훈위안비디오(HunyuanVideo)와 같은 초기 모델은 아직 독점 모델에 비해 품질이 떨어지지만, Stable Video Diffusion(SVD)과 같은 오픈 소스 모델들은 이미 상당한 수준의 비디오를 생성할 수 있습니다. 커뮤니티의 활발한 참여와 지속적인 연구 개발 덕분에, 로컬 PC에서도 전문가 수준의 비디오를 생성할 수 있는 시대가 머지않아 도래할 것입니다. 이는 개인 창작자들이 고가의 장비나 복잡한 소프트웨어 없이도 자신만의 스토리를 시각적으로 구현할 수 있는 기회를 제공할 것입니다.

**이 모든 것이 의미하는 것**
**수달의 진화는 몇 가지 큰 의미를 지닌 두 가지 중요한 추세(crucial trends)를 보여줍니다. 첫째, 이미지 생성부터 비디오, LLM 코드 생성(LLM code generation)에 이르기까지 광범위한 AI 기능(AI capabilities)에서 급속한 발전이 계속되고 있다는 점입니다. 둘째, 오픈 웨이트 모델(open weights models)은 일반적으로 독점 모델(proprietary models)만큼 좋지는 않지만, 종종 최첨단 기술에 불과 몇 달 뒤처져 있을 뿐입니다. 이러한 추세(trends)를 종합해 보면, 이미지 및 비디오 생성(video generation)이 대부분의 사람들을 속일 만큼 충분히 좋을 뿐만 아니라, 이러한 기능이 널리 사용 가능하며, 오픈 모델(open models) 덕분에 규제하거나 통제하기 매우 어려울 것이라는 방향으로 우리가 나아가고 있다는 것이 분명해집니다. 저는 우리가 즐기는 엔터테인먼트부터 온라인 콘텐츠에 대한 신뢰에 이르기까지 사회 전반의 광범위한 영역에 영향을 미칠, 실제와 AI 생성 이미지 및 비디오를 구별하기 불가능한 세상에 대비해야 한다고 생각합니다. 제가 비오 3(Veo 3)에 간단한 텍스트 프롬프트(text prompts)를 사용하여 만든 이 마지막 비디오에서 볼 수 있듯이, 그 미래는 멀지 않았습니다. 시청을 마친 후("뮤지컬 캣츠(Cats) 같지만 수달 버전"이라는 프롬프트(prompt)의 결과에 대해 미리 사과드립니다), 2022년의 첫 미드저니(Midjourney) 이미지를 다시 살펴보십시오. 텍스트 프롬프트(text prompt)가 추상적인 털 덩어리(abstracts masses of fur)를 생성하던 때부터 소리 있는 사실적인 비디오(realistic videos with sound)를 생성하는 때까지 3년도 채 걸리지 않았습니다. 구독 공유**