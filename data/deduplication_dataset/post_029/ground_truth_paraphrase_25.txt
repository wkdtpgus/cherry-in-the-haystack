2년 전, 저는 제 십대 딸과 함께 비행기 좌석에 앉아 있었고, 기내 와이파이가 작동하지 않는 상황에서 새로운 AI 이미지 생성 도구들을 탐색하고 있었습니다. 딸이 가장 좋아하는 동물은 수달이었기에, 인터넷 연결이 복구되자마자 저는 당연하게도 "와이파이를 사용하는 비행기 위의 수달(otter on a plane using wifi)"이라는 문구를 입력했습니다. 이 결과는 빠르게 온라인에서 화제가 되었고, 이후 이 프롬프트는 AI 이미지 생성 기술의 발전 수준을 평가하는 저만의 핵심적인 기준점 중 하나가 되었습니다. 챗GPT와 확산 모델이 널리 알려지기 전인 2021년, 당시 가장 인기 있었던 AI 이미지 생성기였던 VQGAN + CLIP을 활용하여 이 터무니없는 요청을 시도했을 때의 결과는 놀라웠습니다. 이렇게 시작된 것이 AI 기술 진보를 측정하는 예상치 못한 척도가 된 것입니다. 그리고 지난 몇 년간 이 수달들의 변화를 지켜보면서, 저는 AI 분야에서 세 가지 중요한 흐름을 포착할 수 있었습니다. 바로 다양한 종류의 AI 도구들의 등장과 확장, 기술의 비약적인 발전 속도, 그리고 로컬 환경 및 오픈 소스 모델들의 부상입니다.

이번 글에서는 이 수달 벤치마크를 통해 AI가 텍스트를 이미지와 비디오로 변환하는 방식에 있어 얼마나 혁신적인 변화를 겪었는지, 그리고 이러한 변화가 우리 사회와 기술 생태계에 어떤 의미를 가지는지 심층적으로 탐구하고자 합니다. 특히, 단순한 시각적 재현을 넘어 AI가 개념을 이해하고 추론하는 능력의 진화에 초점을 맞출 것입니다.

**확산 모델(Diffusion models)**
제가 처음 생성한 수달 이미지는 확산 기반의 이미지 생성 도구를 통해 만들어졌습니다. AI의 최근 발전사에서 이미지 생성 기술은 대부분 '확산'이라는 과정을 활용해 왔는데, 이는 챗GPT 같은 대규모 언어 모델(LLM)과는 작동 방식에서 근본적인 차이를 보입니다. LLM이 텍스트를 한 번에 한 단어씩, 순차적으로 생성하며 앞으로 나아가는 것과 달리, 확산 모델은 무작위적인 노이즈 상태에서 시작하여 수십 단계에 걸쳐 전체 이미지를 점진적으로 정교하게 다듬어 나갑니다. 이는 마치 이야기를 한 문장씩 써 내려가는 방식과, 거친 대리석 덩어리에서 시작하여 점차 섬세한 조각상으로 깎아내는 과정의 비유와 같습니다. 이미지의 각 부분이 순서대로 구축되는 것이 아니라, 전체가 동시에 세련되어 가는 것입니다. 언어 모델이 "다음 단어는 무엇일까?"를 예측하는 것과 달리, 확산 모델은 "이 노이즈가 어떤 형태가 되어야 할까?"를 반복적으로 추론하며 무작위성을 점차 일관성 있는 시각적 결과물로 변환합니다.

확산 모델의 핵심은 '노이즈 제거(denoising)' 과정에 있습니다. 모델은 훈련 데이터를 통해 노이즈가 추가된 이미지를 원래 이미지로 되돌리는 방법을 학습합니다. 이 과정을 수천, 수만 번 반복하면서, 어떤 노이즈 패턴이 어떤 이미지 특징에 해당하는지 깊이 이해하게 됩니다. 따라서 새로운 이미지를 생성할 때는 단순히 무작위 노이즈에 이 학습된 노이즈 제거 과정을 역으로 적용하여 점진적으로 이미지를 형성해 나가는 것입니다. 이 방식은 놀랍도록 유연하여 다양한 스타일과 복잡한 장면을 생성할 수 있게 해줍니다.

시중에는 여러 확산 모델이 존재하지만, 저는 다른 많은 AI 도구보다 오랜 역사를 가진 미드저니(Midjourney)를 주로 사용하여왔습니다. 미드저니를 활용하면 "와이파이를 사용하는 비행기 위의 수달"이라는 간단한 프롬프트가 확산 모델 기술의 발전을 시간 흐름에 따라 어떻게 보여주는지 명확히 확인할 수 있습니다. (이 글에 포함된 모든 이미지와 비디오는 처음 생성된 네 가지 결과물 중 가장 우수한 것을 선별한 것입니다.) 2022년 초의 녹아내린 듯한 모호한 털 덩어리에서, 그해 말에는 (여전히 손가락이 과도하고 이상한 키보드를 가진) 형태를 알아볼 수 있는 수달로 발전했습니다. 2023년에는 사진처럼 사실적인 수달을 얻었지만, 여전히 기이한 키보드와 비행기 창문이 나타났습니다. 2024년에는 조명과 구도가 한층 개선되었고, 2025년에는 탁월한 사실성(photorealism)을 갖추게 됩니다.
그러나 확산 모델을 단순히 사실적인 이미지를 만드는 능력의 향상으로만 볼 수는 없습니다. 이 기술의 진정한 매력은 다양한 예술적 스타일로 이미지를 창조할 수 있다는 사실에 있습니다. 이는 AI 이미지 생성 기술이 왜 그토록 논란의 중심에 서게 되었는지에 대한 핵심적인 이유를 제공합니다. 많은 AI 모델이 웹상의 방대한 이미지 데이터(저작권이 있는 저작물 포함)로 훈련되기 때문에, 살아있는 예술가들의 허락이나 적절한 보상 없이 그들의 독특한 스타일을 모방하여 이미지를 재생산할 수 있기 때문입니다. 하지만 이러한 방식이 과거의 예술가나 고전적인 스타일에 적용될 때 어떻게 구현되는지 살펴보는 것은 흥미롭습니다. 다음은 바이에른 태피스트리(Bayeux Tapestry), 에곤 실레(Egon Schiele), 스트리트 아트 그래피티(street art graffiti), 그리고 일본 우키요에(Ukiyo-e) 판화 스타일로 재해석된 "와이파이를 사용하는 비행기 위의 수달"입니다. (미술사에 대한 지식이 풍부할수록 이러한 이미지 생성기들의 잠재력을 더욱 극대화할 수 있습니다.)

확산 모델은 기존의 예술 스타일에만 국한되지 않습니다. 미드저니는 모든 창작자가 자신만의 독특한 스타일로 모델을 훈련시킨 다음, 이러한 고유한 "스타일 코드(style codes)"를 다른 사람들과 공유할 수 있도록 지원합니다. 제가 프롬프트 끝에 이러한 스타일 코드 중 하나를 추가하면, 사이버펑크 스타일의 수달부터 만화 같은 수달에 이르기까지 매우 다채로운 결과물을 얻을 수 있습니다.
이제 마지막으로 또 하나의 확산 이미지(diffusion image)를 보여드리고 싶지만, 이 이미지는 앞선 것들과 근본적으로 다른 의미를 가집니다. 저는 플럭스(Flux)를 사용하여 제 개인 컴퓨터에서 이 이미지를 생성했습니다. 미드저니나 챗GPT와 같이 기업의 대규모 데이터 센터에서 운영되는 독점 AI 모델(proprietary AI models)과 달리, 오픈 웨이트 모델(open weights models)은 누구나 자유롭게 다운로드하고, 수정하며, 자신의 장치에서 실행할 수 있습니다. 이 고품질 이미지는 거대 기술 기업의 서버가 아닌, 제 PC의 그래픽 카드(graphics card)를 통해 만들어졌습니다 (이미지 생성에 사용된 인터페이스인 ComfyUI도 함께 볼 수 있습니다). 이는 최고 수준의 클로즈드 소스 모델(closed-source models)이 제공하는 품질에 놀라울 정도로 근접하고 있습니다.

오픈 소스든 독점 소스든, 확산 모델은 다소 무작위적인 결과물을 생성하는 경향이 있으며, 원하는 단 하나의 고품질 이미지를 얻기 위해서는 여러 번의 시도가 필요할 때가 많습니다. 구글의 이마젠 4(Imagen 4)와 같은 최신 확산 모델은 성능이 더욱 향상되었지만, 여전히 만족스러운 결과물을 얻기 위해서는 상당한 운과 반복적인 시행착오가 수반됩니다.

**멀티모달 이미지 생성(Multimodal Image Generation)**
대규모 언어 모델(Large Language Models)의 시대 대부분 동안, 챗GPT와 같은 LLM이 이미지를 생성해야 할 때, 실제로는 내부적으로 확산 모델 중 하나를 호출하여 이미지를 만든 다음 그 결과를 사용자에게 보여주는 방식이었습니다. 이 모든 과정이 간접적으로(LLM이 확산 모델에 프롬프트를 전달하고 확산 모델이 이미지를 생성하는 방식) 이루어졌기 때문에, 이미지를 생성하는 과정은 일반적인 이미지 생성기를 사용하는 것보다 훨씬 더 예측 불가능하게 느껴졌습니다. 이러한 상황은 지난 몇 달간 OpenAI와 구글이 멀티모달 이미지 생성(multimodal image generation) 기술을 선보이면서 크게 달라졌습니다. 노이즈를 이미지로 변환하는 확산 모델과 달리, 멀티모달 생성은 대규모 언어 모델이 단어를 하나씩 추가하듯이 작은 색상 패치(patch)를 순차적으로 추가하여 이미지를 직접적으로 생성할 수 있도록 합니다. 이는 AI가 생성하는 이미지에 대해 훨씬 더 정교하고 깊은 제어력을 제공합니다. 다음은 제가 처음 시도했던 "와이파이를 사용하는 비행기 위의 수달, 노트북 화면에는 와이파이를 사용하는 비행기 위의 수달 이미지를 생성하는 이미지 생성 소프트웨어가 있다"는 프롬프트에 대한 결과입니다.

멀티모달 모델은 텍스트와 이미지라는 서로 다른 형태의 데이터를 동시에 이해하고 처리하도록 설계된 통합된 신경망 아키텍처를 사용합니다. 이는 LLM이 이미지의 시각적 요소와 그 의미를 더 깊이 연결하고, 사용자의 복잡한 지시를 훨씬 더 정확하게 해석하여 시각적 결과물로 구현할 수 있게 합니다. 단일 모델 내에서 텍스트 프롬프트를 직접 시각적 정보로 변환함으로써, 중간 단계의 프롬프트 엔지니어링이나 간접적인 호출 없이도 사용자가 원하는 바를 더욱 섬세하게 반영할 수 있습니다.

하지만 이제 고백할 것이 있습니다. 제 딸이 가장 좋아하는 동물은 그냥 수달이 아니라 해달이며, 지금까지의 모든 이미지는 사실 흔히 볼 수 있는 강수달이었습니다. 마침내 멀티모달 생성 기술을 통해 저는 아버지로서의 체면을 세울 수 있었습니다. 멀티모달 모델은 "대신 해달로 만들고, 모히칸(mohawk) 스타일을 주고, 레이저 게이밍 노트북(Razer gaming laptop)을 사용하게 해달라"는 식으로 매우 구체적인 변경 및 조정을 할 수 있기 때문입니다. 저는 시각적 임팩트(visual impact)를 추구하거나 무작위적인 이미지(randomized images)를 통해 탐색하는 데 많은 시간을 할애할 의향이 있을 때는 여전히 미드저니(Midjourney)와 이마젠(Imagen) 같은 확산 모델을 사용하지만, 특정하고 정확한 그림을 원할 때는 이제 항상 멀티모달 이미지 생성기(multimodal image generators)를 찾게 됩니다. 이러한 모델들이 점점 더 보편화될 것이라고 확신합니다. 아직 오픈 웨이트 멀티모달 이미지 생성기는 많지 않지만, 이 상황은 조만간 바뀔 가능성이 높습니다. 오픈 소스 커뮤니티의 활발한 연구와 개발 덕분에, 우리는 머지않아 개인 장치에서도 강력한 멀티모달 기능을 활용할 수 있을 것입니다.

**이미지를 위한 코드(Code)와 "불꽃(Sparks)" 사용**
멀티모달 생성 기술은 AI가 이미지를 정교하게 제어할 수 있음을 입증합니다. 그러나 더 근본적인 질문이 남아 있습니다. AI가 실제로 자신이 무엇을 만들고 있는지 이해하는 것일까요, 아니면 단순히 방대한 훈련 데이터(training data)에서 패턴을 재조합하는 것일까요? 진정한 공간 추론(spatial reasoning) 능력을 시험하기 위해, 우리는 AI에게 코드를 사용하여 그림을 그리도록 강제할 수 있습니다. 이 방식에서는 시각적 피드백(visual feedback)이 전혀 없으며, 의지할 만한 사전 학습된 이미지 패턴(pre-trained image patterns)도 존재하지 않습니다. 마치 수학적 지시(mathematical instructions)만을 사용하여 눈을 가린 채 그림을 그리라고 요청하는 것과 같습니다. 그림을 그리는 데 사용하기 특히 까다로운 코드 유형 중 하나는 학술 논문에서 과학 다이어그램(scientific diagrams)을 생성하는 데 주로 사용되는 수학적 언어인 TikZ입니다. 이 언어는 그림 그리기 목적에 너무 부적합하여, TikZ라는 이름 자체가 재귀적인 독일어 문구인 "TikZ ist kein Zeichenprogramm"("TikZ는 그림 그리기 프로그램이 아니다")에서 유래했을 정도입니다. 이러한 특성 때문에 TikZ를 그림 생성에 사용한 훈련 데이터는 매우 드물며, 이는 AI가 훈련 과정에서 이 코드를 "기억"하여 재현할 수 없다는 것을 의미합니다. AI는 스스로 새로운 그림 코드를 생성해야 합니다. 이 언어로 순수 수학적 명령만을 사용하여 이미지를 만드는 것은 인간에게도 매우 어려운 작업입니다. 실제로, 지금은 구형이 된 GPT-4가 그린 유니콘의 TikZ 그림은 매우 영향력 있는 논문에서 LLM이 AGI(Artificial General Intelligence)의 "불꽃(spark)"을 가지고 있을 수 있다는 신호로 간주되었습니다. 그렇지 않다면 어떻게 그렇게 창의적인 결과물을 만들 수 있었을까요? 참고용으로 그 유니콘의 모습은 다음과 같습니다. 저는 구형 GPT-4에게 "와이파이를 사용하는 비행기 위의 수달"을 그리게 하는 데는 그 유니콘만큼의 운이 따르지는 않았습니다.

TikZ와 같은 선언적(declarative) 언어로 그림을 그리는 것은 AI에게 시각적 표현을 위한 추상적이고 논리적인 구조를 이해해야 함을 의미합니다. 이는 단순히 픽셀 데이터를 조작하는 것을 넘어, 기하학적 형태, 상대적 위치, 그리고 복잡한 수학적 관계를 추론하는 능력을 요구합니다. 이러한 능력은 AI가 실제 세계의 물리적 법칙이나 3D 공간을 이해하는 데 중요한 기초가 될 수 있습니다.

하지만 제미니 2.5 프로(Gemini 2.5 Pro)와 같은 더 최신 모델에게 TikZ로 해달을 그려달라고 요청하면 어떻게 될까요? 결과는 완벽하지는 않지만 (그리고 제미니는 "비행기 위(on a plane)"라는 말을 문자 그대로 받아들여 해달을 날개 위에 앉혔습니다), 만약 분홍색 유니콘이 "불꽃(spark)"을 보여주었다면, 이 해달 그림은 분명 더 큰 도약(larger leap)을 의미합니다. 이는 AI가 복잡한 지시를 해석하고, 추상적인 개념을 구체적인 시각적 형태로 변환하는 능력이 지속적으로 향상되고 있음을 시사합니다. 그리고 오픈 웨이트 모델(open weights models)도 여기서 빠르게 추격하고 있지만, 일반적으로 최첨단 기술에 몇 달 정도 뒤처져 있습니다. 현재 사용 가능한 최고의 오픈 웨이트 모델 중 하나인 딥시크 r1(DeepSeek r1)의 새 버전은 제미니와 같은 클로즈드 소스 모델만큼 정교하지는 않지만, 앞으로 계속해서 개선될 것이라고 예상합니다. 이러한 오픈 소스 모델의 발전은 연구와 혁신을 가속화하며, AI 기술의 민주화를 더욱 촉진할 것입니다.

이러한 그림 자체는 모델이 아무것도 없는 상태에서 공간적 관계를 추론(reasoning about spatial relationships from scratch)한다는 사실만큼 중요하지 않습니다. 이것이 바로 "불꽃" 논문의 저자들이 이러한 시스템이 훈련 데이터에서 단순한 패턴 매칭(pattern-matching)을 넘어, 실제 이해(actual understanding)에 가까운 무언가를 개발하고 있다고 주장한 이유입니다. 이러한 발전은 AI가 단순한 도구를 넘어, 새로운 지식을 생성하고 복잡한 문제를 해결하는 데 기여할 수 있는 잠재력을 보여줍니다.

**비디오(Video)**
정지 이미지가 인상적인 발전을 보여주었다면, 비디오 생성(video generation)은 AI 기술 가속화(accelerating)의 가장 극적인 증거입니다. 다음은 2024년 7월 현재 시장에 나와 있는 최고의 비디오 생성기 중 하나인 런웨이 젠-3 알파(Runway Gen-3 alpha)가 만든 "컴퓨터에서 와이파이를 사용하는 비행기 위의 해달"입니다. 그리고 이는 1년도 채 지나지 않은 2025년에 구글(Google)의 비오 3(Veo 3)에서 동일한 프롬프트인 "컴퓨터에서 와이파이를 사용하는 비행기 위의 해달"로 생성된 비디오입니다. 네, 비디오에 포함된 소리까지도 100% AI가 생성한 것입니다. 그리고 주제를 확장하자면, 이제 제 개인 컴퓨터에서도 실행할 수 있는 오픈 웨이트 AI 비디오 모델(open weights AI models)들도 등장하고 있습니다. 이들은 아직 최첨단 기술에 비해 다소 뒤처져 있지만, 그 격차를 빠르게 좁히고 있습니다. 다음은 텐센트(Tencent)의 훈위안비디오(HunyuanVideo)로 동일한 프롬프트에 대해 얻은 결과입니다. 네, 아직은 다소 미흡하지만, 이 비디오는 대규모 데이터 센터가 아닌, 제 집 컴퓨터에서 만들어졌다는 점을 기억해야 합니다.

비디오 생성은 이미지 생성보다 훨씬 더 복잡한 기술적 과제를 안고 있습니다. 단순히 개별 프레임을 사실적으로 만드는 것을 넘어, 시간적 일관성(temporal consistency), 객체의 움직임 추적, 3D 공간 이해, 그리고 일관된 스토리텔링까지 요구되기 때문입니다. 초기 비디오 모델들은 종종 프레임 간의 불일치나 객체의 깜빡임 현상(flickering)을 보였지만, 최근에는 Diffusion Transformer (DiT) 아키텍처와 같은 혁신적인 접근 방식들이 이러한 문제들을 해결하며 놀라운 발전을 이루고 있습니다. 특히 Sora와 같은 모델들은 몇 분 길이의 고품질 비디오를 생성할 수 있는 잠재력을 보여주며, 영화 제작, 광고, 게임 개발 등 다양한 분야에 혁명적인 변화를 예고하고 있습니다. 오픈 소스 비디오 모델의 등장은 이러한 기술을 더 많은 개발자와 창작자들이 접근하고 실험할 수 있게 하여, 이 분야의 혁신 속도를 더욱 가속화할 것입니다.

**이 모든 것이 의미하는 것**
해달의 진화 과정은 몇 가지 중요한 시사점을 지닌 두 가지 핵심적인 흐름을 명확히 보여줍니다. 첫째, 이미지 생성부터 비디오, LLM 기반 코드 생성에 이르기까지 AI의 광범위한 기능(AI capabilities) 영역에서 믿을 수 없을 만큼 빠른 속도의 발전이 지속되고 있다는 점입니다. 둘째, 오픈 웨이트 모델(open weights models)은 일반적으로 독점 모델(proprietary models)만큼 완벽하지는 않지만, 종종 최첨단 기술에 불과 몇 달 뒤처져 있을 뿐이라는 사실입니다.

이 두 가지 흐름에 더해, AI 기술이 전문가의 영역을 넘어 일반 사용자들도 쉽게 접근하고 활용할 수 있는 형태로 진화하고 있다는 세 번째 중요한 추세도 간과할 수 없습니다. 복잡한 코딩이나 고급 지식 없이도 직관적인 인터페이스를 통해 누구나 고품질의 콘텐츠를 생성할 수 있게 되면서, 창작의 민주화가 가속화되고 있습니다.
이러한 추세들을 종합해 볼 때, 이미지 및 비디오 생성 기술이 대부분의 사람들을 충분히 속일 만큼 정교해질 뿐만 아니라, 이러한 기능이 매우 광범위하게 사용 가능하며, 오픈 모델 덕분에 그 확산을 규제하거나 통제하기가 극히 어려울 것이라는 방향으로 우리가 나아가고 있다는 것이 분명해집니다. 우리는 즐기는 엔터테인먼트 콘텐츠부터 온라인 정보에 대한 신뢰에 이르기까지 사회 전반의 광범위한 영역에 심각한 영향을 미칠, 실제와 AI 생성 이미지 및 비디오를 구분하기 불가능한 세상에 대비해야 한다고 생각합니다. 이러한 미래는 단순히 기술적 경이로움을 넘어, 정보의 진위 여부, 저작권 문제, 그리고 인간의 창의성 역할에 대한 근본적인 질문을 던질 것입니다.

제가 비오 3(Veo 3)에 간단한 텍스트 프롬프트(text prompts)만을 사용하여 만든 이 마지막 비디오에서 볼 수 있듯이, 그 미래는 결코 멀리 있지 않습니다. 이 비디오를 시청하신 후 ("뮤지컬 캣츠(Cats) 같지만 해달 버전"이라는 프롬프트의 결과물에 대해 미리 사과드립니다), 2022년의 첫 미드저니(Midjourney) 이미지를 다시 한번 살펴보십시오. 텍스트 프롬프트가 추상적인 털 덩어리(abstracts masses of fur)를 생성하던 때부터, 소리까지 포함된 사실적인 비디오(realistic videos with sound)를 생성하는 때까지 불과 3년도 채 걸리지 않았습니다. 이 경이로운 발전 속도는 AI가 다음 3년 동안 또 어떤 놀라운 변화를 가져올지 상상조차 어렵게 만듭니다. 우리는 기술의 진보를 관찰하는 것을 넘어, 그 영향에 대해 깊이 숙고하고 현명하게 대응해야 할 시점에 와 있습니다.