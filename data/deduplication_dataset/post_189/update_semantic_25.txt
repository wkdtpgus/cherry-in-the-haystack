LLM 인사이트 독자 여러분께. 금번 호에서는 거대 언어 모델(LLM) 분야의 혁신적인 진보와 더불어 인공지능, 컴퓨터 비전, 과학 탐구 등 다채로운 영역에서 이 기술이 발휘하는 활용성에 대해 심층적으로 분석합니다. 특히, 눈여겨볼 만한 핵심 연구 성과는 다음과 같습니다:

*   **자율적 알고리즘 발명:** LLM을 활용하여 전례 없는 데이터 효율성으로 신규 알고리즘을 자동 생성하는 개방형 진화 시스템(ShinkaEvolve)에 대한 소식입니다.
*   **내재적 추론 능력 강화:** 원시 학습 자료에서 스스로 추론 방식을 터득하여 표준 평가 지표에서 상당한 성능 개선(최대 8점)을 보인 자율 학습 강화 기법(RLPT)의 새로운 접근법을 소개합니다.
*   **범용 시각 지능의 도래:** 생성형 비디오 모델이 객체 분리부터 물리적 상호작용 이해까지 62가지 다양한 시각 작업을 사전 학습 없이 처리할 수 있음을 입증하며, 광범위한 시각 모델의 잠재력을 보여줍니다.
*   **사고 과정 명시를 통한 대화 능력 향상:** LLM이 최종 답변 도출 전 '생각의 흐름'을 명확히 하도록 유도하는 학습 방식(RLMT)이 불과 7천여 개의 훈련 예시만으로도 최고 수준의 대화 성능을 달성한 사례를 다룹니다.
*   **연속적 사고 경로 훈련:** 강화 학습을 통해 추론을 위한 연속형 '유연한' 토큰을 학습시키는 독창적인 기법이 기존 모델의 역량을 유지하면서도 수학 문제 해결에서 불연속적인 사고 흐름 방식보다 우수한 결과를 보였습니다.
*   이 외에도 풍부한 내용이 준비되어 있습니다!

먼저 아래에 정리된 핵심 개념들을 살펴보시거나, 즉시 본 연구 논문 분석 부분으로 넘어가실 수 있습니다.

저희 LLM 인사이트 구독자 여러분들을 텍사스 오스틴에서 개최되는 제6회 MLOps 월드 | GenAI 글로벌 서밋에 특별히 모십니다. 본 행사는 OpenAI, HuggingFace 등 업계 선두 주자들이 참여하며, 60개 이상의 심층 세션으로 구성되어 있습니다. 구독자께서는 온라인으로 비용 부담 없이 참여하실 수 있습니다. 만약 오스틴 현지에서 진행되는 실전 워크숍, 실제 적용 사례 발표, 그리고 네트워킹 행사에 직접 참여를 원하신다면, 다음 코드를 활용하여 등록 시 150달러 상당의 혜택을 누리실 수 있습니다.

**특별 할인 코드 제공**

**핵심 용어 해설**

**사전 훈련 데이터 기반 강화 학습 (Reinforcement Learning on Pre-Training Data, RLPT):** 이 훈련 방식은 거대 언어 모델이 기존의 사전 학습 코퍼스(corpus)를 상호작용적인 학습 환경으로 간주하도록 합니다. 모델은 다음 단어 예측 과정에서 다양한 '경로'를 탐색하고, 실제 텍스트의 연속성과 일치하는 정도에 따라 보상을 받음으로써 학습을 진행합니다. 이로써 인간의 개입(예: 주석 작업) 없이도 모델 스스로 추론 역량을 키울 수 있게 됩니다. 이는 데이터의 자율적 활용을 극대화하는 혁신적인 접근법입니다.

**검증 가능 보상 기반 강화 학습 (Reinforcement Learning with Verifiable Rewards, RLVR):** 수학 문제 풀이나 코드 생성과 같이 결과의 정확성을 기계적으로 확인 가능한 특정 분야에서, 프로그램적으로 정의된 보상 체계를 활용하는 강화 학습 기법입니다. 이는 명확한 정답이 존재하는 영역에서의 추론 능력 향상에 매우 효과적이지만, 창의적 글쓰기나 복잡한 의사결정 등 정답이 모호한 개방형 과제에는 적용하기 어려운 한계가 있습니다.

**모델 보상 기반 사고 강화 학습 (RL with Model-rewarded Thinking, RLMT):** 이 새로운 강화 학습 파이프라인은 모델이 최종 답변을 도출하기 전에 심층적인 '사고의 흐름'(chain-of-thought)을 명시적으로 생성하도록 유도합니다. 독립적인 보상 모델이 이 추론 과정을 평가하며, 이를 통해 기본 모델은 단순히 정답을 맞히는 것을 넘어, 보다 숙고된 과정을 통해 응답을 생성하도록 최적화됩니다. 이는 일반적인 대화 능력에서 기존 RLHF 대비 현저한 개선을 가져옵니다.

**사고 증강 사전 학습 (Thinking Augmented Pre-Training, TPT):** LLM의 학습 효율을 높이기 위해, '사고 경로'(단계별 추론 과정)를 텍스트 훈련 데이터에 통합하는 방법론입니다. 자동 생성된 설명이나 추론 과정을 원본 텍스트와 함께 학습시킴으로써, TPT는 각 토큰이 지닌 정보 가치를 증대시키고, 모델이 적은 양의 예시만으로도 복잡한 개념을 효과적으로 습득하도록 지원합니다. 이는 데이터 희소성 문제를 해결하는 데 기여합니다.

**연속적 사고의 사슬 ('소프트' 토큰) (Continuous Chain-of-Thought, “Soft” tokens):** LLM의 내부 추론 과정에 불연속적인 언어 토큰 대신 연속적인 임베딩(embeddings)을 활용하는 기법입니다. 이러한 '소프트 토큰'은 여러 추론 경로를 동시에 표현할 수 있는 잠재력을 지니며, 최근 연구에서는 강화 학습을 통해 LLM이 이러한 연속형 사고 흐름 토큰을 스스로 학습하여 기존 이산적 방식보다 더 풍부하고 다양한 추론 능력을 발휘할 수 있음을 입증했습니다. 이는 모델의 내재적 사고 방식에 새로운 지평을 엽니다.

---

**ShinkaEvolve: 개방적이고 데이터 효율적인 프로그램 진화 연구**

관련 연구: ShinkaEvolve ( [논문](https://arxiv.org/abs/2405.08643) / [코드](https://github.com/ShinkaEvolve/ShinkaEvolve) )

**해결하려는 당면 과제는?**
현존하는 대규모 언어 모델(LLM) 기반의 코드 생성 및 최적화 기법들은 상당한 **데이터 소모성**을 보입니다. 만족스러운 해결책을 찾기 위해 수천 번의 반복적인 시도가 필요한 경우가 허다합니다. 또한, 대다수가 비공개 형태로 운영되어 일반 연구자들의 접근성과 활용도를 저해합니다. 결과적으로, LLM을 활용한 혁신적인 발견은 막대한 비용과 기술적 장벽으로 인해 소수의 기관만이 누릴 수 있는 특권으로 남아 있었습니다.

**어떻게 접근하는가?**
ShinkaEvolve는 LLM을 프로그램 코드를 점진적으로 개선하는 '변형 연산자'로 활용하는 개방형 프레임워크입니다. 이 시스템은 필요한 탐색 횟수를 획기적으로 줄이기 위해 세 가지 핵심적인 혁신을 통합합니다. 첫째, 새로운 아이디어 탐색과 기존의 성공적인 솔루션 활용 간의 균형을 지능적으로 조절하는 '균형 잡힌 부모 선택 전략'을 채택합니다. 둘째, 임베딩 유사도 분석과 LLM의 독창성 평가를 통해 중복되거나 비창의적인 변형을 걸러내는 '참신성 기반 거부 샘플링' 기법을 사용합니다. 셋째, 매 세대마다 앙상블 내에서 가장 유망한 LLM 변형을 동적으로 선택하는 '밴딧 알고리즘'을 적용합니다. 이러한 요소들은 ShinkaEvolve가 가장 효과적인 프로그램 수정에만 컴퓨팅 자원을 집중하도록 하여, 기존 '무작위 탐색' 방식의 비효율성을 극복합니다. 이는 단순히 코드를 생성하는 것을 넘어, 마치 생명체가 진화하듯 스스로 더 나은 프로그램을 찾아나가는 인공지능 진화론적 접근 방식이라 할 수 있습니다.

**주목할 만한 성과는 무엇인가?**
ShinkaEvolve는 프로그램 진화의 효율성을 비약적으로 향상시킴으로써, 이전에는 사실상 불가능했던 광범위한 문제들에서 성공을 거두었습니다. 이 프레임워크는 단 150번의 시도만으로 고전적인 26개 원형 배치 문제에 대한 새로운 최첨단 해법을 찾아냈는데, 이는 기존 방식 대비 **'효율성 면에서 압도적인 진보'**를 의미합니다. 또한, 수학 경시대회(AIME 벤치마크)에서는 75세대 만에 강력한 인간 설계 기준선을 능가하는 고성능 다중 에이전트 전략을 발전시켰습니다. 경쟁 프로그래밍 분야에서 AtCoder 대회 에이전트에 적용된 ShinkaEvolve의 개선 사항은 매우 유의미하여, 특정 문제에서는 진화된 해법이 대회에서 **2위를 차지할 수 있었을 것**입니다. 더 나아가, ShinkaEvolve는 대규모 전문가 혼합(MoE) LLM을 훈련하는 데 있어 더욱 우수한 방식을 발견했습니다. 새로운 부하 분산 손실 함수를 통해 DeepMind의 'Global LBL' 기준선보다 **1.73% 더 높은 작업 점수와 5.8% 더 적은 낭비 용량**을 달성했습니다. 이러한 결과들은 광범위한 **'개방형 탐색 및 발견이 이제 합리적인 비용으로 가능해졌음'**을 명확히 보여주며, 과학자와 엔지니어에게 새로운 해결책을 자율적으로 탐색할 수 있는 AI 기반 '공동 연구자'를 제공합니다.

**향후 전망은?**
본 연구는 알고리즘 설계부터 최적화 문제 해결에 이르기까지, 과학 및 공학 분야의 난제들이 LLM 기반 진화적 탐색을 통해 효율적으로 해결될 수 있음을 시사합니다. ShinkaEvolve를 오픈소스화하고 진화 과정을 시각적으로 확인할 수 있는 웹 UI까지 제공함으로써, 연구진은 이 혁신적인 접근법을 대중화하고자 합니다. 향후 연구는 이 방법을 회로 설계, 새로운 과학 공식 발견, 하이퍼파라미터 최적화 등 새로운 영역에 적용하고, 발전하는 LLM 기술과의 통합을 모색할 것입니다. 장기적으로 ShinkaEvolve와 같은 기술은 자동화된 **'연구 조수'** 역할을 수행하며, 인간이 간과할 수 있는 아이디어와 개선 사항을 빠르게 반복 탐색하는 동시에, 기존의 무차별 대입 방식보다 훨씬 적은 시도로 효율적인 탐색을 가능하게 할 것입니다. 이는 연구 개발 패러다임 자체를 변화시킬 잠재력을 지닙니다.

---

**비디오 모델: 사전 학습 없이도 배우고 추론하는 지능**

관련 연구: Veo 3 비디오 모델 ( [논문](https://arxiv.org/abs/2405.08643) / [프로젝트](https://deepmind.google/discover/blog/veo-3-video-model-is-a-zero-shot-learner-and-reasoner/) )

**이 연구가 주목하는 핵심 과제는?**
대규모 언어 모델(LLM)이 방대한 데이터와 규모를 통해 이전에는 볼 수 없었던 제로샷(zero-shot) 문제 해결 능력을 선보이며 AI 분야에 혁명을 가져왔습니다. 이러한 맥락에서, 과연 비디오 생성 모델도 시각 영역에서 이와 같은 범용적인 문제 해결 능력을 갖출 수 있을지에 대한 의문이 제기되었습니다. 기존의 비디오 모델들은 주로 특정 분야의 좁은 작업이나 제한된 벤치마크에서 평가되어 왔기에, 인간과 유사한 폭넓은 시각적 추론 역량을 보유하고 있는지에 대한 명확한 답은 부재했습니다.

**어떻게 문제를 해결했는가?**
연구팀은 최첨단 생성형 비디오 모델인 Veo 3를 활용하여, 고전적인 컴퓨터 비전 과제부터 물리적 추론, 심지어 도구 활용 시뮬레이션에 이르기까지 62가지의 다양한 작업에 걸쳐 체계적인 성능 검증을 수행했습니다. 이 과정에서 핵심은 **최소한의 프롬프트 기반 접근 방식**을 사용했다는 점입니다. 모델에는 초기 비디오 프레임이나 이미지가 제공되고, "경계를 찾아라" 또는 "이 미로를 풀어라"와 같은 텍스트 지시가 주어지면, 모델은 8초 분량의 비디오를 '답변'으로 생성합니다. 이 모든 평가는 미세 조정(fine-tuning)이나 특정 작업에 대한 추가 훈련 없이 이루어진 진정한 제로샷 방식입니다. 비디오 모델 자체의 추론 능력을 순수하게 평가하기 위해, 연구진은 단일 이미지 정보만을 가진 독립형 LLM(Google Gemini 2.5)이 해당 작업을 해결할 수 없도록 설계하여, 모든 성공이 비디오 생성 과정에서 비롯된 것임을 확인했습니다. 이는 비디오 모델이 생성하는 프레임을 통해 시각적으로 단계별 '사고'를 수행하고, 그 결과가 올바른 해결책을 제시하는지를 검증하는 방식입니다.

**주요 발견은 무엇인가?**
Veo 3는 특정 작업에 대한 최적화 없이도 놀랍도록 광범위한 새로운 역량을 입증했습니다. 이 모델은 객체를 **분할**하고, **가장자리를 감지**하며, **이미지 편집**(예: 불필요한 객체 제거)을 수행하고, **물리적 특성**(예: 움직임을 통해 객체의 질량 추론)을 유추하며, **객체의 기능적 용도(affordances)**를 파악하고, 심지어 **도구 사용을 시뮬레이션**하는 것까지 제로샷으로 가능하게 했습니다. 이러한 지각 및 조작 능력은 더욱 고차원적인 **시각적 추론**을 가능하게 합니다. 예를 들어, Veo 3는 스스로 생성하는 비디오 내에서 해결 경로를 내부적으로 '시각화'함으로써 미로 풀이나 대칭 퍼즐과 같은 복잡한 문제를 해결합니다. 정량적으로, 테스트된 62가지 다채로운 작업에서 Veo 3는 에지 감지(92%), 이미지 노이즈 제거(100%)와 같은 저수준 비전 작업뿐만 아니라 물리적 추론과 같은 인지적 작업에서도 높은 성공률을 기록했습니다. Veo 3는 이전 버전(Veo 2)에 비해 모든 작업에서 명확한 개선을 보여, 모델 규모 및 버전 개선과 함께 이러한 기능이 확장됨을 시사합니다. 이 모든 결과는 충분한 규모와 훈련이 이루어진 비디오 모델이 LLM과 유사한 발전 궤적을 따르고 있으며, 훈련 범위 밖의 수많은 작업을 처리하도록 프롬프트될 수 있는 범용 **'비전 파운데이션 모델'**로 진화하고 있음을 강력히 시사합니다.

**다음 단계는 무엇인가?**
본 연구는 미래 AI가 **통합된 멀티모달 파운데이션 모델**에 기반을 둘 것임을 강력히 시사합니다. 마치 하나의 LLM이 다양한 언어 과제를 해결하듯, 하나의 비디오 모델도 수많은 시각 과제를 처리할 수 있게 될 것입니다. 다음 핵심 과제는 LLM 발전을 이끌었던 프롬프트 엔지니어링 및 표준화된 평가 방법론과 유사하게, 이러한 비디오 모델을 위한 프롬프트 기술과 벤치마크를 고도화하는 것입니다. 연구자들은 또한 텍스트 기반 계획과 비디오 생성의 결합과 같이, 명시적인 추론 단계를 도입하는 것이 복잡한 작업에서 성능을 더욱 향상시킬 수 있는지 탐구할 것입니다. 응용 측면에서, 강력한 제로샷 비디오 추론기는 혁신적인 변화를 가져올 수 있습니다. 예를 들어, 행동에 앞서 결과를 시각적으로 예측하는 로봇 보조원이나, 실시간으로 물리 실험을 시뮬레이션하는 과학 모델을 상상해볼 수 있습니다. 궁극적으로 언어 및 비디오 모델 기능의 융합은 **범용 AI 시스템**이라는 더 큰 흐름을 가속화하며, 이러한 능력이 어떻게, 그리고 왜 나타나는지(예: 훈련 데이터나 아키텍처의 어떤 요소가 도구 사용 시뮬레이션으로 이어지는가?)를 이해하는 것은 근본적인 연구 질문으로 남습니다.

---

**사전 학습 데이터 기반 강화 학습 (RLPT): 텍스트 데이터의 잠재력 해방**

관련 연구: RLPT ( [논문](https://arxiv.org/abs/2405.08643) )

**어떤 한계를 극복하는가?**
대규모 언어 모델(LLM)의 성능을 향상시키기 위해 더 많은 텍스트 데이터를 주입하는 방식은 점차 한계에 부딪히고 있습니다. 컴퓨팅 자원은 비교적 쉽게 확장할 수 있지만, 양질의 텍스트 데이터는 본질적으로 유한합니다. 더욱이, 단순히 다음 토큰을 예측하는 표준 훈련 방식만으로는 모델이 복잡한 상호 의존성을 통해 추론하는 능력을 효과적으로 학습하기 어렵습니다. 이는 모델이 기존 데이터 분포를 넘어 새로운 지식을 탐색하도록 유도하지 않기 때문입니다. 기존의 RLHF(인간 피드백 기반 강화 학습)와 같은 방법은 일부 개선을 가져오지만, 막대한 비용과 시간 소모적인 인간 주석 작업이 필수적입니다. 결론적으로, 우리는 대규모의 인간 개입 없이도 LLM이 이미 보유한 데이터로부터 더 깊이 학습하고, 특히 추론 능력을 스스로 발전시킬 수 있는 대안적인 접근 방식이 절실히 필요했습니다.

**어떻게 접근하는가?**
RLPT는 LLM의 초기 사전 학습 코퍼스(corpus)를 강화 학습(RL)을 위한 역동적인 훈련 환경으로 재정의합니다. 이는 '다음 세그먼트(segment) 예측' 작업을 순차적인 의사결정 문제로 간주함으로써 가능합니다. 구체적으로, 모델은 주어진 텍스트 맥락을 분석한 후 다음 텍스트 덩어리를 생성합니다. 이때, 별도의 생성형 보상 모델(Generative Reward Model) 또는 암묵적인 정답 신호가 해당 생성이 코퍼스 내의 실제 연속과 얼마나 잘 부합하는지에 따라 보상을 제공합니다. 이 과정을 통해 LLM은 실제 텍스트를 최적 행동의 예시로 삼아 다양한 텍스트 연속체를 탐색하고, 인간의 레이블 없이도 즉각적인 피드백을 얻습니다. 중요한 점은 이러한 보상이 사전 학습 데이터 자체에서 직접 파생되므로, 수동으로 생성된 보상이나 인간 선호도 모델에 대한 의존성을 완전히 배제한다는 것입니다. 실제로 RLPT는 모델이 레이블 없는 데이터 환경에서 자율적으로 추론 능력을 연마할 수 있도록 합니다. 모델은 더 높은 보상(텍스트와의 전반적인 일관성 증진)을 가져오는 장기적인 경로를 탐색하며, 이는 단순히 즉각적인 다음 토큰 예측을 넘어설 수 있게 합니다. 이러한 훈련 중 탐색 과정은 수십억 개의 토큰 규모로 신중하게 확장되어, 모델의 정책이 광범위한 텍스트 도메인에서 더욱 풍부한 추론 전략을 발견하도록 유도합니다.

**주요 발견은 무엇인가?**
40억 매개변수 기본 LLM(Qwen3-4B)에 RLPT를 적용한 결과, 여러 난이도 높은 벤치마크에서 성능이 괄목할 만하게 향상되었습니다. 예를 들어, MMLU(지식 평가 시험)에서는 모델 점수가 **3.0점**, MMLU-Pro(고급 버전)에서는 **5.1점** 상승했습니다. 특히 수학 및 논리 중심 작업에서는 훨씬 더 큰 개선을 보였습니다. QA 벤치마크(GPQA-Diamond)에서 **8.1점**, AIME24(수학 경시대회 문제)에서 **6.6점**이 향상되었습니다. 이는 강력한 기본 모델에 대한 절대적인 성능 개선이며, 어떠한 인간 레이블링 데이터나 작업별 미세 조정(finetuning) 없이 달성되었다는 점에서 그 의미가 큽니다. 또한, 스케일링 연구에 따르면 RLPT에 더 많은 컴퓨팅 자원(더 많은 훈련 단계)을 투입할수록 모델 성능은 지속적으로 향상되어, 더 큰 예산으로 더 큰 성과를 얻을 수 있음을 시사합니다. 연구진은 RLPT로 훈련된 모델이 더욱 강력한 **일반화 가능한 추론 능력**을 보여준다고 강조합니다. 이 모델은 복잡한 프롬프트(prompt)를 처리하는 능력을 확장하며, 기존 검증 기반 강화 학습(RLVR)과 결합될 때 성능을 추가적으로 향상시킵니다. 요약하자면, RLPT는 우리가 이미 보유한 텍스트 데이터로부터 훨씬 더 많은 정보를 추출하여 데이터 부족이라는 장벽을 허물고, 수동적인 사전 학습 데이터를 능동적인 학습 경험으로 효과적으로 전환하는 매우 유망한 경로를 제시합니다.

**다음 단계는 무엇인가?**
RLPT의 자기 지도형 보상 체계의 성공은 향후 **하이브리드 훈련 체제**의 발전을 위한 중요한 토대를 마련합니다. 미래의 LLM은 인간의 개입 없이도 텍스트, 게임, 또는 시뮬레이션 환경에서 수동적인 정보 흡수와 능동적인 탐색 학습을 번갈아 수행할 수 있게 될 것입니다. 즉각적인 다음 연구 방향 중 하나는 RLPT를 더 큰 규모의 모델(예: 340억, 700억 매개변수)과 코드 또는 멀티모달 데이터와 같은 더 다양한 도메인에 적용하여 추론 능력 향상 여부를 확인하는 것입니다. 보상 모델링(reward modeling) 또한 개선의 여지가 있습니다. 현재의 '다음 세그먼트 예측' 보상에 텍스트에서 자동으로 파생될 수 있는 논리적 일관성이나 사실적 정확성 지표를 통합한다면, 모델은 스스로 일관성과 진실성을 검증하며 학습할 수 있을 것입니다. 더 넓은 관점에서 RLPT는 단순히 모방을 넘어 '미리 생각하는'(토큰 계획) 것을 학습하는 언어 모델(LM)의 광범위한 추세의 일부입니다. 따라서 이를 '사고의 나무(tree-of-thought)'나 훈련 중 도구 사용과 같은 기술과 결합하는 연구가 기대됩니다. 이 모든 발전은 인터넷 텍스트를 단순히 흡수하는 것을 넘어, 문제를 해결하여 자료를 더 깊이 이해하는 학생처럼, 텍스트를 적극적으로 연습하고 일반화하는 LLM의 시대를 열어갈 것입니다.

---

**유연한 토큰, 견고한 추론: 연속적 사고의 사슬**

관련 연구: 연속적 사고의 사슬 (Continuous CoT) ( [논문](https://arxiv.org/abs/2405.08643) )

**어떤 문제를 해결하는가?**
LLM(대규모 언어 모델)이 단계별 추론 과정을 명시적으로 생성하도록 유도하는 사고의 사슬(chain-of-thought, CoT) 프롬프트 방식은 성능 향상에 기여했지만, 이는 여전히 이산적인 자연어 토큰에 의존합니다. 이러한 이산적 표현은 복잡한 추론을 위한 가장 효율적인 내부 표현이 아닐 수 있습니다. 반면, 연속 토큰(continuous tokens)은 본질적으로 이산적인 어휘의 제약 없이 벡터(vector) 형태로 존재하며, 이론적으로는 훨씬 더 풍부한 표현력을 가지고 동시에 여러 아이디어를 인코딩(encode)할 수 있습니다. 연속적인 공간에서 '사고'하는 모델은 하나의 추론 경로에 얽매이지 않고, 다수의 경로를 병렬적으로(중첩(superposition) 형태로) 탐색할 수 있는 잠재력을 지닙니다. 그러나 LLM이 추론 과정에서 이러한 연속적인 비언어적 토큰을 효과적으로 사용하도록 훈련하는 것은 매우 어려운 과제였습니다. 과거 시도는 주로 추론 시점에 연속 토큰을 주입하거나(훈련 없이), 이미 알려진 인간 작성 추론 사슬로부터 지식을 증류(distilling)하는 방식이었는데, 이는 번거롭고 짧은 사슬에만 국한되는 한계를 보였습니다. 모델이 유용한 연속적 사고의 사슬(CoT)을 처음부터 스스로 학습하는 확장 가능한 방법은 아직 제시되지 않았습니다.

**어떻게 접근하는가?**
본 연구는 어떠한 인간의 정답 추론(ground-truth human rationales)에도 의존하지 않고, 강화 학습(reinforcement learning)을 통해 연속적 사고의 사슬(CoT)을 훈련하는 최초의 성공적인 방법론을 제안합니다. 핵심 아이디어는 모델이 프롬프트와 최종 답변 사이에 '소프트(soft)' 토큰(연속 임베딩(continuous embeddings))을 생성하도록 하고, 보상 신호(reward signal)를 활용하여 이 토큰의 사용을 최적화하는 것입니다. 구체적으로, 연구팀은 탐색(exploration)을 장려하기 위해 입력 임베딩에 미량의 노이즈(noise)를 추가한 다음, 정책 경사 강화 학습(policy-gradient RL)을 적용하여 최종 답변이 정확할 경우 모델에 보상을 제공합니다. 본질적으로 모델은 더 나은 문제 해결 결과로 이어지는 자신만의 내부 언어(연속 토큰과 그들이 나타내는 의미)를 발명하도록 유도됩니다. 이산적인 사슬에 대한 어떠한 지도 학습(supervised training)도 배제함으로써, 이러한 소프트 토큰의 잠재력을 제한할 수 있는 인간의 편향을 제거합니다. 특히, 이 접근 방식은 계산 오버헤드(computational overhead)를 최소화하여, 훈련 과정에서 모델이 수백 개의 연속 토큰을 추론 단계에 활용할 수 있도록 합니다. 이는 기존의 증류(distillation) 방식이 허용했던 것보다 훨씬 더 큰 '사고 용량'을 모델에 부여하는 것입니다.

**주요 발견은 무엇인가?**
수학 추론 벤치마크에서 이 연속적 사고의 사슬(CoT) 기법으로 훈련된 LLM은 기존의 이산적 사고의 사슬을 사용하는 모델과 동등하거나 더 뛰어난 성능을 달성했습니다. 예를 들어, GSM8K 수학 문제 해결에서 연속적 CoT를 사용하는 Llama-7B 모델은 단일 최적 답변(pass@1)을 기준으로 했을 때 표준(이산적) CoT 모델과 유사한 정확도를 보였습니다. 그러나 여러 답변을 샘플링(pass@32)하도록 허용했을 때, **연속적 CoT 모델은 이산적 CoT 모델을 능가했습니다**. 이는 연속 토큰이 올바른 답변으로 이어지는 더욱 다양하고 풍부한 추론 경로를 탐색할 수 있음을 의미합니다. 이는 연속 토큰의 가장 큰 장점 중 하나를 보여줍니다. 즉, 더 폭넓고 다양한 해결책을 포착할 수 있으며, 여러 번의 시도를 통해 그 가치를 극대화할 수 있다는 것입니다. 흥미롭게도, 연구진은 최적의 전략이 하이브리드(hybrid) 방식이라는 것을 발견했습니다. 즉, 연속 토큰으로 훈련하되, **추론 시에는 이산 토큰을 사용하는 것입니다.** 다시 말해, 훈련 과정에서는 모델이 벡터 형태로 사고하여 이점을 얻지만, 실제 배포 시에는 필요에 따라 일반 텍스트 형태의 근거를 출력할 수 있습니다. 이러한 훈련은 모델의 잠재적 추론 능력을 여전히 향상시켰습니다. 더욱이, 연속적 CoT 훈련은 모델의 다른 능력에 대한 간섭을 최소화했습니다. 이 모델은 이산적 CoT로 훈련된 모델보다 관련 없는 작업에서 정확도를 더 잘 유지했으며, 이는 이 접근 방식이 추론 데이터에 대한 과적합(overfitting)을 피하는 '더 부드러운' 방법임을 시사합니다. 종합적으로 볼 때, 이는 LLM이 실제 문제 해결에 유의미한 이득을 가져오는, 인간이 직접 해독하기 어려운 자체적인 사고 벡터를 개발할 수 있다는 강력한 개념 증명입니다.

**다음 단계는 무엇인가?**
LLM이 벡터 형태로 사고하도록 훈련하는 것은 수많은 새로운 연구 방향을 제시합니다. 당장 제기되는 질문 중 하나는 이러한 학습된 연속 토큰들을 어떻게 해석하거나 시각화할 것인가입니다. 과연 그것들이 인간의 개념과 유사한 의미를 담고 있는가, 아니면 완전히 이질적이지만 효과적인 어떤 형태인가? 또한, 연속적 CoT를 멀티모달 추론(multimodal reasoning)으로 확장할 가능성도 큽니다(예를 들어, 추론 과정에서 '소프트 시각 토큰'을 통해 이미지를 내부적으로 표현하는 LLM을 상상해볼 수 있습니다). 여기서 강화 학습의 성공은 논리적 일관성 검사나 사실 확인과 같은 다른 보상 신호를 활용하여 연속적인 사고를 정교하게 조형함으로써, 더욱 신뢰할 수 있는 추론을 생성하는 데 영감을 줄 수 있습니다. 실제로 우리는 모델이 연속 공간에서 고강도 추론을 수행한 후, 그 결과를 인간이 이해하기 쉬운 간결한 설명으로 요약하는 하이브리드 시스템을 기대할 수 있습니다. '소프트' 모델의 최종 답변이 표준 형식으로 실행될 수 있다는 점은 실제 적용의 용이성을 의미합니다. 예를 들어, 수학 튜터 LLM은 연속적 CoT를 조용히 사용하여 어려운 증명을 해결한 다음, 깔끔한 자연어로 학생에게 답을 제시할 수 있습니다. 전반적으로 이 연구는 LLM에서 더욱 효율적이고 다양한 추론을 위한 기반을 마련하며, 기존 사고의 사슬(chain-of-thought)이 지녔던 이산 토큰 기반 사고의 한계를 잠재적으로 극복할 수 있는 길을 열어줍니다.

---

**TPT: 사고 증강 사전 학습으로 데이터 효율성 극대화**

관련 연구: TPT ( [논문](https://arxiv.org/abs/2405.08643) )

**어떤 한계를 극복하는가?**
대규모 언어 모델(LLM) 훈련에 필요한 고품질 데이터는 늘 제한적이며, 언어 내부에 내재된 복잡한 패턴 중 일부는 단순히 다음 단어를 예측하는 방식만으로는 모델이 효과적으로 학습하기 어렵습니다. 종종 문장이나 진술의 기저에 깔린 논리적 근거나 연결 고리는 텍스트에 명시적으로 드러나기보다는 암묵적으로 가정되어 있습니다. 이러한 특성 때문에 수학 증명의 단계나 코드의 숨겨진 논리적 관계와 같은 특정 '고가치 토큰'은 모델이 제대로 학습하기 매우 어렵습니다. 이는 결국 데이터 비효율성으로 이어집니다. 수십억 개의 단어를 학습하더라도, 모델은 중간 사고 과정을 전혀 접하지 못하기 때문에 다단계 추론이나 심층적인 이해가 필요한 과제에서 여전히 난항을 겪을 수 있습니다. 따라서 이 연구의 핵심 과제는 **숨겨진 추론 과정을 명시적으로 드러냄으로써 데이터의 활용도를 극대화하는 방법**을 모색하는 것입니다.

**어떻게 접근하는가?**
TPT(사고 증강 사전 학습)는 '사고 경로(thinking trajectories)'를 사전 학습 코퍼스(corpus)에 증강함으로써 이러한 문제를 해결합니다. 이는 본질적으로 단계별 추론 과정이나 상세한 설명 콘텐츠를 생성하여 원본 텍스트와 함께 삽입하는 방식입니다. 예를 들어, 원본 텍스트가 "학생이 문제를 풀고 42라는 답을 얻었다"고 되어 있다면, 사고 경로에는 학생이 문제 해결을 위해 거친 구체적인 단계들이 포함될 수 있습니다. 이러한 경로들은 강력한 LLM이나 휴리스틱(heuristics)을 활용하여 광범위한 작업 및 도메인에 걸쳐 자동으로 생성될 가능성이 높으며, 훈련을 위해 원본 데이터와 밀접하게 결합됩니다. 이로써 TPT는 유효 데이터의 양을 증가시킬 뿐만 아니라(새로운 토큰이 추가되므로), 결정적으로 복잡한 개념의 근본적인 추론 과정을 세분화하여 모델이 학습하기 쉽게 만듭니다. 이 방법은 '범용적' 특성을 지니며, 제한된 데이터로 처음부터 사전 학습하는 경우, 이미 방대한 코퍼스를 증강하는 경우, 심지어 오픈소스 모델을 중간 단계에서 훈련하여 추가적인 개선을 꾀하는 경우 등 다양한 훈련 환경에 적용될 수 있습니다. 각 시나리오에서 명시적인 추론 사슬의 존재는 모델이 동일한 양의 원본 텍스트를 통해 더욱 효과적으로 일반화(generalize)하도록 돕습니다.

**주요 발견은 무엇인가?**
TPT는 모델 크기와 훈련 설정 전반에 걸쳐 상당한 성능 향상을 가져왔으며, 이는 데이터 효율성(data efficiency) 측면에서 지대한 성공을 의미합니다. 특히 연구진은 TPT가 사전 학습(pre-training)의 데이터 효율성을 **무려 3배 향상시킨다**고 보고합니다. 이는 실질적으로 TPT 증강을 통해 1000억 개의 토큰으로 훈련된 LLM이 3000억 개의 표준 데이터 토큰으로 훈련된 모델과 유사하거나 더 우수한 성능을 달성할 수 있음을 의미합니다. 30억 매개변수 모델의 경우, 훈련 중에 사고 경로(thinking trajectories)를 통합하는 것만으로도 여러 난이도 높은 추론 벤치마크에서 **10% 이상의 개선**을 보였습니다. 더 큰 모델과 다양한 아키텍처(디코더 전용(decoder-only) 및 기타 모델 모두 테스트)에서도 동일한 이점을 얻었으며, 이는 TPT 방법론의 견고성을 시사합니다. 중요하게도, 이러한 이득은 특정 틈새 작업에만 국한되지 않습니다. 논문은 일반 NLP 벤치마크에서 '다양한 모델 크기와 아키텍처 전반에 걸쳐' 개선이 있었다고 언급합니다. 이는 이 방법이 특정 문제에 과적합(overfitting)되지 않고, 광범위한 이해 능력이나 기술을 모델에 주입한다는 것을 의미합니다. 추론을 명시적으로 포함함으로써 모델은 단계별 논리, 수학 단어 문제, 복잡한 QA 등 추론을 요구하는 작업에서 더 나은 성능을 보이며, 표준 언어 작업의 성능을 저해하지도 않습니다. 본질적으로 TPT는 **토큰당 더 많은 사고가 단순히 더 많은 토큰을 사용하는 것만큼 좋거나 그 이상으로 효과적임**을 입증하며, 이는 효율적인 모델 훈련을 위한 매우 중요한 결과입니다.

**다음 단계는 무엇인가?**
TPT의 접근 방식은 LLM 훈련을 더욱 의도적이고 구조화하는 최근의 추세와 일치합니다. 미래 연구는 사고 경로(thinking trajectories)의 생성을 더욱 자동화하는 방안을 탐구할 수 있습니다. 예를 들어, 하나의 LLM이 추론을 생성하고, 다른 LLM이 이를 검증하거나 개선한 후 훈련에 사용하는 방식입니다. 또한, 이 방법을 다른 양식(modality)으로 확장할 가능성도 있습니다. 이미지 캡션(caption)에 시각적 추론 사슬을 추가하거나, 코드에 프로그램 논리 사슬을 추가하여 학습 효과를 유사하게 증진시키는 식입니다. 즉각적인 실용적 측면에서, 모델을 훈련하는 기업들은 TPT를 채택하여 더 적은 데이터로 높은 성능을 달성하거나(혹은 동일한 데이터로 더 나은 결과를 얻을 수 있으며), 이는 경제적으로 매우 매력적인 이점입니다. 또한 TPT를 RLPT(위 논문 #4)와 결합하는 시너지 효과도 기대됩니다. 먼저 추론으로 데이터를 증강(TPT)한 다음, 모델이 강화 학습(RL)을 통해 해당 데이터를 탐색하도록 하는 것입니다. 이는 자체 개선형 AI를 위한 매우 강력한 조합이 될 수 있습니다. 궁극적으로 TPT는 데이터의 양보다는 질에 주목하도록 우리를 이끌고 있습니다. 텍스트 내의 '숨겨진' 정보에 집중하고 이를 명시적으로 드러냄으로써, 우리는 훨씬 더 많은 데이터 없이도 LLM 능력의 새로운 지평을 열 수 있을 것입니다.

---

**SimpleFold: 단백질 접힘, 복잡성 속의 단순함을 찾다**

관련 연구: SimpleFold ( [논문](https://arxiv.org/abs/2405.08643) / [코드](https://github.com/google-deepmind/simplefold) )

**이 연구가 던지는 질문은?**
알파폴드(AlphaFold)와 같은 단백질 접힘 분야의 혁신적인 발전은 단백질 고유의 기하학적 특성을 포착하기 위해 고도로 복잡하게 설계된 모델 아키텍처에 크게 의존해 왔습니다. 예를 들어, 삼각형 어텐션 모듈(triangle attention modules), 쌍별 거리 행렬(pairwise distance matrices), 그리고 여러 특수 손실 함수 등이 그 복잡성을 더합니다. 이러한 전문화된 설계 방식은 놀라운 성공을 거두었지만, 계산 비용이 막대하며 자연어 처리(NLP)나 컴퓨터 비전 분야에서 사용되는 '표준' 아키텍처와는 상당히 이질적입니다. 이는 중요한 의문을 제기합니다. 과연 단백질 접힘 문제 해결에 이 모든 도메인별 복잡성이 필수적인가, 아니면 훨씬 더 단순하고 범용적인 모델로도 유사한 수준의 정확도를 달성할 수 있을까? 즉, 단백질 접힘이라는 현상이 현재의 모델들이 암시하는 것보다 본질적으로 더 단순한 원리에 기반하고 있는 것은 아닐까?

**어떻게 문제를 해결하는가?**
SimpleFold는 단백질 접힘 모델의 근본적인 단순화를 목표로 하는 대담한 시도입니다. 이 모델은 특별한 단백질 전용 블록을 배제하고, 범용적인 트랜스포머(Transformer) 아키텍처를 채택합니다. 기존의 복잡한 기법들(삼각형 업데이트, 아미노산의 독립적인 2D 쌍 표현 등) 대신, 일부 적응형 게이팅 레이어(adaptive gating layers)로 강화된 표준 자기 어텐션 레이어(self-attention layers)에 의존하며, 단백질 구조 데이터를 통해 종단 간(end-to-end) 방식으로 훈련됩니다. 핵심 통찰은 단백질 접힘 과정을 생성 모델링 문제(generative modeling problem)로 바라보는 것입니다. SimpleFold는 확산 모델(diffusion models)이나 정규화 흐름(normalizing flows)과 유사한 **흐름 일치 목표(flow-matching objective)**를 사용하여 훈련되며, 이를 통해 무작위적인 초기 구조를 올바른 접힌 구조로 점진적으로 정제하도록 유도합니다. 연구진은 올바른 구조 예측을 장려하기 위한 최소한의 추가 손실 항(loss term)을 포함했습니다(따라서 순수하게 일반적이지는 않지만, 거의 그렇습니다). 이후 이 모델은 30억 개의 매개변수로 확장되었고, 약 9백만 개의 단백질 구조 데이터(대규모 증류/예측 세트 및 실험적으로 해명된 구조 포함)로 훈련되었습니다. 본질적으로 SimpleFold는 단백질 좌표를 일반 데이터 시퀀스처럼 취급하며, 단백질 생물 물리학적 지식을 명시적으로 인코딩하지 않고도 트랜스포머가 이를 올바른 형태로 '흐르게' 하는 방법을 학습하도록 합니다.

**주요 발견은 무엇인가?**
SimpleFold-3B는 표준 단백질 접힘 벤치마크에서 최첨단 전문 모델들과 경쟁할 만한 성능을 보여주었습니다. 이 모델은 3D 구조 예측에서 매우 경쟁력 있는 정확도를 달성하며, 범용 트랜스포머(vanilla Transformer)가 단백질 접힘에 필요한 복잡한 의존성을 실제로 학습할 수 있음을 증명했습니다. 더욱이 SimpleFold는 결정론적 모델(deterministic models)이 종종 어려움을 겪는 영역에서 강점을 보입니다. 생성형(generative) 모델의 특성상, 자연스럽게 **다양한 확률적 구조의 앙상블(ensemble)**을 생성할 수 있습니다. 본 논문은 이러한 앙상블 예측에서 강력한 성능을 강조합니다. 여러 접힘 형태를 샘플링하고 대체 가능한 구조(alternative conformations)를 포착할 수 있다는 점은 단일 답변만을 제공하는 알파폴드와 같은 모델이 일반적으로 어려워하는 부분입니다. 또 다른 실용적인 이점은 효율성입니다. SimpleFold는 더 단순한 아키텍처(architecture) 덕분에 배포가 용이하며, 특수 연산 없이 표준 하드웨어에서 더 빠르게 실행됩니다. SimpleFold의 성공은 단백질 접힘에 고도로 도메인별 설계가 필수적이라는 기존의 개념에 효과적으로 이의를 제기합니다. 이는 과학 분야에서 더욱 범용적인 AI 구성 요소를 활용할 수 있는 길을 열어줍니다. 요컨대, 본 논문은 단백질 접힘의 상당 부분이 **일반적인 시퀀스 모델(generic sequence model)**에 의해 학습될 수 있음을 보여주며, 이는 놀랍고 고무적인 발견입니다.

**다음 단계는 무엇인가?**
SimpleFold의 접근법은 과학 문제에 대한 모델 설계 방식을 재평가하는 중요한 계기가 될 수 있습니다. 만약 범용 트랜스포머(Transformer)가 단백질 구조 예측에 효과적이라면, 분자 특성 예측, DNA 접힘 등 다른 과학적 과제들도 올바른 훈련 방법론을 통해 더 단순한 아키텍처로 전환될 가능성이 있습니다. 향후 연구는 SimpleFold를 약물 결합 예측(drug binding prediction)과 같은 다운스트림 작업(downstream tasks)과 통합하여, 생성 앙상블(generative ensemble) 기능을 통해 여러 단백질 형태(conformations)를 탐색하는 데 활용할 수 있을 것입니다. 흐름 일치 목표(flow-matching objective)의 사용은 확산 모델(diffusion models)과의 연관성도 시사합니다. 접힘 과정을 시계열(time series)로 시뮬레이션하여 정확도를 더욱 높이거나 단백질의 동역학(dynamics)을 포착하는 확산 기반 접힘 모델을 상상해볼 수 있습니다. 또한 SimpleFold는 표준 AI 모델에 더 가깝기 때문에 **전이 학습(transfer learning)**의 이점을 얻을 수 있습니다. 예를 들어, 언어 모델의 가중치로 초기화하거나 그 반대로 하여 일부 교차 도메인 지식(cross-domain knowledge)을 주입할 수 있습니다(일부 언어적 특징이 단백질 서열 분석에 도움이 될 수 있다는 초기 가설도 있습니다). 가장 중요한 교훈은 **단순함이 때로는 복잡함과 동일한 목표를 달성할 수 있다**는 점입니다. 이는 수작업으로 설계된 복잡한 네트워크가 지배적인 도메인에서 연구자들이 더 '미니멀리스트(minimalist)' 기준선(baselines)을 시도하도록 이끌 수 있는 귀중한 통찰입니다. 이러한 추세가 지속된다면, 과학(단백질 접힘, 화학)과 일반 AI 모두에서 동일한 핵심 모델 유형이 발전을 견인하며, 주로 아키텍처보다는 훈련 데이터의 차이에서 비롯되는 수렴 현상을 목격하게 될 것입니다.

---

**LLMs4All: 학제 간 대규모 언어 모델 활용의 총체적 조망**

관련 연구: LLMs4All 서베이 (Survey) ( [논문](https://arxiv.org/abs/2405.08643) )

**이 연구가 해결하려는 당면 과제는?**
대규모 언어 모델(LLM)의 영향력은 컴퓨터 과학 영역을 넘어 역사학에서 생물학에 이르기까지 모든 학문 분야로 급속히 확장되고 있습니다. 그러나 이처럼 다양한 학문 분야에서 LLM을 효과적으로 활용하는 방식에 대한 정보는 파편화되어 있습니다. 예를 들어, 법학이나 화학을 연구하는 학자들은 자신의 전문 분야에 특화된 최신 LLM 기술 동향에 대해 충분한 정보를 얻기 어려울 수 있습니다. 본 논문은 학문 연구의 전 스펙트럼에 걸쳐 최첨단 LLM 응용 사례, 활용 기회, 그리고 당면 과제들을 한데 모아 종합적으로 분석하는 포괄적인 조사 보고서의 필요성에 응답합니다.

**어떻게 접근하는가?**
LLMs4All은 학문의 세 가지 광범위한 영역에서 LLM이 각 분야에 어떻게 적용되고 있는지를 상세히 설명하는 광범위한 검토 및 안내서 역할을 수행합니다. 저자들은 학문 분야를 (1) 예술, 인문학 및 법학(역사학, 철학, 정치학, 건축학, 법학 등), (2) 경제 및 경영(재무, 마케팅, 경영 등), (3) 과학 및 공학(수학, 물리학, 생물학, 화학, 지구과학, 컴퓨터 과학 등)으로 분류합니다. 각 영역에 대해 이 서베이(survey)는 연구 및 실제 적용 사례에서 LLM의 현재 활용 방안을 설명합니다. 예를 들어, 역사 텍스트 분석 지원, 법률 문서 요약 지원, 과학 연구에서 가설 생성 등이 해당 도메인의 최첨단 모델 또는 시스템 예시와 함께 제시됩니다. 또한 텍스트 생성, 추론, 코딩, 다국어 이해와 같은 LLM 기능이 분야별 특정한 요구 사항을 충족하기 위해 어떻게 맞춤화되거나 미세 조정(fine-tuned)되는지 논의합니다. 응용 분야 외에도, 이 검토는 각 분야의 주요 한계와 도전 과제(의학 분야의 데이터 프라이버시, 역사학의 사실 정확성, 법률 분야의 윤리적 문제(예: 편향 또는 공정성) 등)뿐만 아니라, LLM 통합을 위한 미해결 연구 질문 및 미래 방향을 다룹니다. 결과적으로 LLMs4All은 AI의 최전선과 도메인 전문가 사이를 잇는 다리 역할을 하며, 'LLM이 X 분야에 어떤 기여를 할 수 있는가'에 대한 핵심 정보를 한곳에 집약합니다.

**주요 발견은 무엇인가?**
이 조사 보고서의 핵심적인 기여는 질적인 종합 분석에 있지만, 동시에 여러 중요한 통찰과 관찰을 제공합니다. 가장 포괄적인 발견 중 하나는 사실상 모든 학문 분야가 LLM의 영향권에서 벗어날 수 없다는 점입니다. GPT-4를 활용한 법률 계약 초안 작성부터 화학 실험 설계를 위한 생성 모델 사용에 이르기까지, 학계는 LLM 기반 혁신의 물결을 적극적으로 경험하고 있습니다. 그러나 이 검토는 각 분야에서의 LLM 활용 성숙도에 차이가 있음을 지적합니다. 일부 분야(컴퓨터 과학, 법학 등)는 이미 수많은 LLM 응용 프로그램을 개발했지만, 다른 분야(예: 철학, 예술)는 여전히 초기 활용 사례를 탐색하는 단계에 머물러 있습니다. 또한, LLM이 그럴듯하게 들리지만 부정확한 정보(환각(hallucinations))를 생성하여 의학이나 금융과 같은 민감한 분야에서 비전문가 사용자들을 오도할 수 있다는 우려와 같이, 여러 학문 분야에서 공통적으로 나타나는 과제들도 확인되었습니다. 본 논문은 **학제 간 협력의 중요성**을 강조합니다. 예를 들어, LLM을 특정 도메인의 지식 기반 또는 전문 모델과 결합할 때 더 나은 결과를 얻을 수 있음이 드러났습니다(LLM과 화학 규칙을 함께 활용하는 과학적 발견 도구 사례에서처럼). 긍정적인 발견으로는 LLM이 연구 분야에서 **지식의 민주화 동력**으로 작용한다는 점입니다. 이는 고급 기술 훈련이 없는 개인도 자신의 전문 분야 문제에 AI를 활용할 수 있도록 지원합니다(예: 역사학자가 GPT를 사용하여 고대 문헌을 번역하고 요약하는 경우). 이 서베이(survey)는 또한 책임감 있는 LLM 사용(예: AI 활용 시 학술 저작물에서의 명확한 공개 정책)에 대한 공동체의 고민과 함께 나타나는 모범 사례 및 윤리적 지침을 수집합니다. 종합적으로 LLMs4All은 각 분야의 연구자들이 현재 상황을 이해하고 자신의 연구에 LLM을 어떻게 적용할 수 있는지 파악하기 위한 명확한 로드맵을 제공합니다.

**왜 중요한가?**
생성형 AI가 데이터 분석만큼이나 학문 연구의 근본적인 요소로 자리매김함에 따라, 각 학문 분야에서 그 역할에 대한 명확한 이해를 갖는 것이 필수적입니다. 이 서베이(survey)는 교육자와 정책 입안자에게도 중요한 지침이 될 것입니다. 예를 들어, 대학 학과들은 해당 분야와 관련된 AI 리터러시(literacy)를 교육과정에 포함시킬 때 이 보고서를 참고할 수 있습니다. 한계점과 미래 방향을 명확히 문서화함으로써, 본 논문은 AI 연구자들에게 과학 Q&A의 사실성 향상이나 LLM을 법적 추론과 일치시키는 것과 같은 중요한 미해결 연구 문제들을 제시합니다. 한 가지 유력한 결과는 이 서베이(survey)가 **학제 간 협력(cross-disciplinary collaborations)을 적극적으로 촉진할 것**이라는 점입니다. 화학 분야의 LLM 활용 사례를 접한 생물학자가 AI 전문가와 협력하여 유사한 기술을 생물학에 적용하는 시나리오를 상상할 수 있습니다. 또한, 이 논문은 과장된 기대감에도 불구하고 현재 LLM이 전문 분야에서 여전히 심각한 단점을 가지고 있음을 강조하며, 이는 현실적인 기대치를 설정하고 신뢰성 향상을 위한 추가 연구를 장려하는 데 기여합니다. 요약하자면, LLMs4All은 AI와 다른 모든 학문 분야의 교차점에서 일어나고 있는 변화를 기록하고, 지식이 고립되지 않고 광범위하게 공유되도록 보장하며, AI가 진정으로 모든 분야의 도구가 되는 다음 연구 단계를 이끄는 데 도움이 되기 때문에 매우 중요합니다.

---

**사고하는 LLM, 더 나은 대화형 AI를 만들다**

관련 연구: RLMT ( [논문](https://arxiv.org/abs/2405.08643) / [코드](https://github.com/google-deepmind/rlmt) )

**어떤 한계를 극복하는가?**
인간 피드백 기반 강화 학습(RLHF)은 대화형 LLM의 유용성과 안전성을 개선하는 표준적인 미세 조정(finetuning) 기법으로 자리 잡았습니다. 그러나 RLHF는 모델이 생성하는 최종 답변만을 최적화할 뿐, 그 답변에 이르는 내부 추론 과정은 직접적으로 개선하지 못합니다. 비록 모델이 내부적으로 '사고의 흐름'(chain-of-thought)을 생성할 수 있다 하더라도, 보상 모델(reward model)은 오직 최종 응답만을 평가합니다. 이는 그럴듯하게 들리지만 깊이 있는 추론이 결여된 답변으로 이어질 수 있습니다. 또 다른 접근법인 검증 가능 보상 기반 강화 학습(RLVR)은 모델이 수학 증명이나 코드 테스트와 같이 검증 가능한 작업을 출력하도록 유도하고 정확성에 따라 보상을 제공합니다. RLVR은 더 나은 추론을 가능하게 하지만, 객관적인 검증이 가능한 특정 도메인에서만 작동한다는 한계가 있습니다. 따라서 해결되지 않은 핵심 과제는 다음과 같습니다. RLHF의 범용성을 유지하면서도, 모델이 실제로 문제를 깊이 있게 사고하도록 유도할 수 있는 방법은 무엇일까요?

**어떻게 접근하는가?**
이러한 질문에 대한 해답은 RLMT(모델 보상 기반 사고 강화 학습, Reinforcement Learning with Model-rewarded Thinking)라는 새로운 훈련 패러다임(paradigm)에 있습니다. RLMT에서는 훈련 과정 중 모델이 최종 답변을 제시하기 전에 길고 상세한 사고의 사슬(chain-of-thought, CoT)을 생성하도록 요구됩니다. 이 CoT는 상세한 개요, 단계별 추론 과정, 또는 중간 '생각'의 형태를 띨 수 있습니다. 보상 모델(reward model)(RLHF에서 사용되는 것과 동일하며 인간 선호 데이터로 사전 훈련됨)은 최종 답변뿐만 아니라 CoT와 답변의 전체 조합을 평가합니다. 본질적으로 모델은 좋은 답변으로 이어지는 유용한 추론 과정을 생성한 것에 대해 보상을 받게 됩니다. 연구팀은 에세이 작성, 식사 계획, 복잡한 질문 답변과 같은 다양한 실제 개방형 프롬프트(prompt)를 활용하고, 정책 경사 방법(policy gradient methods)(PPO, DPO 등)을 적용하여 LLM의 정책이 더 나은 '사고+답변' 쌍을 출력하도록 최적화합니다. 이 접근 방식의 견고성을 보장하기 위해, 두 가지 기본 모델(Llama-3.1 8B 및 Qwen-7B)에 대해 40가지의 개별 훈련 실행을 다양한 설정에서 수행했습니다. 특히, 지도 미세 조정(supervised fine-tuning) 없이 기본 모델에 RLMT를 직접 적용하여 일반적인 지시 조정(instruction-tuning) 단계를 건너뛸 수 있는지 확인하는 'R1-Zero' 훈련 방식도 탐구했습니다.

**주요 발견은 무엇인가?**
RLMT로 훈련된 모델은 광범위한 평가에서 표준 RLHF로 훈련된 모델보다 지속적으로 우수한 성능을 입증했습니다. 예를 들어, AlpacaEval2, WildBench, Arena Hard 등 세 가지 개방형 채팅 벤치마크에서 RLMT 모델은 동등한 RLHF 모델 대비 **3점에서 7점 더 높은 점수**를 기록했으며, 이는 품질 면에서 상당한 진전을 의미합니다. 또한 창의적 글쓰기 작업 및 지식 퀴즈에서도 **1점에서 3점 향상**되는 등 일반적인 능력 개선도 관찰되었습니다. 아마도 가장 인상적인 결과는, 최적의 RLMT로 미세 조정된 80억 매개변수 모델이 해당 채팅 벤치마크 및 창의적 작업에서 GPT-4(오픈 변형)의 성능을 실제로 능가했으며, 심지어 한 벤치마크에서는 Claude 2의 수준에 근접했다는 점입니다. 이는 모델이 GPT-4보다 훨씬 작다는 점을 고려할 때 매우 놀라운 성과입니다. 또 다른 주목할 만한 발견은 다음과 같습니다. 단 **7,000개의 RLMT 프롬프트(어떠한 지도 미세 조정도 없음)로 훈련된 80억 매개변수 Llama 모델이 2,500만 개의 예시로 지시 조정된 공식 Llama-3.1-8B를 능가했습니다**. 이는 '사고하고 답변하기' 최적화를 통해 신중하게 선별된 소수의 시나리오가 대규모의 전통적인 훈련 방식보다 더 효과적일 수 있음을 보여주며, RLMT의 강력한 효율성을 입증합니다. 질적으로, 연구진은 RLMT 모델이 더 구조화되고 사려 깊은 응답(예: 목록 작성, 명시적 추론, 대안 고려)을 생성하며, 주제를 벗어나는 것과 같은 실패 모드가 더 적다는 것을 관찰했습니다. 이러한 결과는 사고 과정에 보상을 부여하는 것이 최종 답변에만 보상을 주는 것보다 측정 가능하게 더 나은 채팅 성능으로 이어진다는 것을 강력히 시사합니다.

**다음 단계는 무엇인가?**
이 연구는 대화형 에이전트(conversational agents) 훈련 방식에 패러다임(paradigm)의 전환을 가져올 잠재력을 지닙니다. 사고의 사슬(chain-of-thought)을 단순히 선택적인 부산물로 취급하기보다는, LLM이 자신의 추론 과정을 명시적으로 표현하도록 훈련하는 것이 표준적인 접근 방식이 될 수 있습니다. 미래 연구 방향에는 RLMT를 **휴먼 인 더 루프(human-in-the-loop)** 시스템과 결합하는 것이 포함됩니다. 예를 들어, 최종 답변뿐만 아니라 중간 단계의 '생각'에 대해서도 인간 피드백을 받아 추론 품질에 대한 보상 모델(reward model)을 더욱 정교하게 다듬는 것입니다(기존 선호 모델의 한계를 넘어). 또한, RLMT를 더 큰 모델(본 논문에서는 80억 매개변수 모델을 다루었지만, 340억 또는 700억 매개변수 모델에 적용하면 일부 영역에서 훨씬 더 큰 비공개 모델들을 능가할 수 있는 강력한 모델을 얻을 수 있습니다). 또 다른 중요한 고려 사항은 실제 배포 환경입니다. RLMT 모델은 더 많은 설명을 제공함으로써 **해석 가능성(interpretability)**을 높일 수 있으며, 이는 안전성 측면에서 큰 이점입니다. 그러나 이는 프롬프트되지 않아도 자신의 '생각'을 드러낼 수 있음을 의미하며, 이는 필요에 따라 조정될 수 있습니다. 마지막으로, 이 연구는 RLMT가 왜 그렇게 효과적인지에 대한 심층적인 이해를 요구합니다. 보상 모델이 인간 선호도와 더 잘 일치하는 사고의 사슬(CoT)의 특정 구조를 간접적으로 선호하는 것일까요, 아니면 더 긴 맥락(context)을 생성하는 행위 자체가 모델이 실수를 방지하는 데 도움이 되는 것일까요? 이러한 질문에 답하는 것은 훈련 과정을 더욱 개선하는 데 기여할 것입니다. 종합적으로 볼 때, **사고하는 언어 모델은 진정으로 더 나은 대화 능력을 제공하며**, 이러한 기술의 발전으로 다음 세대 AI 비서들이 추론 과정에서 훨씬 더 명시적이고 투명해질 것으로 기대할 수 있습니다.

---

**SciReasoner: 과학적 추론을 위한 학제 간 기반 모델 구축**

관련 연구: SciReasoner ( [논문](https://arxiv.org/abs/2405.08643) / [코드](https://github.com/google-deepmind/scireasoner) )

**이 연구가 해결하려는 당면 과제는?**
현재까지 과학 분야에 특화된 대규모 언어 모델(LLM)들은 대부분 특정 도메인(예: 화학 반응 예측 또는 수학 정리 증명)에 맞춰 미세 조정(fine-tuned)된 전문가 모델이었습니다. 그러나 실제 과학 연구는 종종 여러 학문 분야와 다양한 데이터 형식(예를 들어, 생물학적 발견을 화학 이론과 연결하고, 수학 방정식과 자연어 텍스트를 동시에 활용하는 상황)에 걸쳐 복합적으로 진행됩니다. 따라서 자연어 질문을 이해할 뿐만 아니라 공식, 생체 서열(DNA, 단백질 시퀀스), 속성 테이블 등 다채로운 형태의 정보를 통합적으로 처리할 수 있는, 과학적 추론을 위한 범용 파운데이션 모델(foundation model)의 필요성이 증대되고 있습니다. 요컨대, 이 연구의 목표는 특정 분야에만 국한되지 않고, 과학 분야에서 사용되는 다양한 표현 방식과 함께 폭넓고 학제 간적인 추론 능력을 갖춘 인공지능 과학자를 구현하는 것입니다.

**어떻게 접근하는가?**
SciReasoner는 다양한 과학적 표현과 언어를 효과적으로 정렬하기 위한 광범위한 다단계 훈련 과정을 통해 개발되었습니다. 첫째, 이 모델은 수많은 분야의 과학 텍스트뿐만 아니라, 순수하게 상징적인 시퀀스(symbolic sequences)와 혼합 시퀀스-텍스트 데이터(mixed sequence-text data)를 포함하는 2060억 토큰(token) 규모의 방대한 코퍼스(corpus)로 사전 학습(pre-trained)됩니다. 이는 아미노산 서열, 화학 SMILES 문자열, 수학 방정식과 같은 데이터들이 설명 텍스트와 함께 학습되었음을 의미합니다. 이 대규모 사전 학습 이후, 연구진은 **4천만 개에 달하는 과학 관련 지시(instruction)**에 대해 지도 미세 조정(supervised fine-tuning)을 수행하여, 모델이 **103가지에 이르는 다양한 과학 작업을 지원**할 수 있도록 했습니다. 이러한 작업들은 (i) 텍스트와 과학 형식 간 번역(예: "이 분자 구조를 말로 설명해라" 및 그 역), (ii) 텍스트 또는 시각 자료에서 지식 추출, (iii) 속성 예측(주어진 화합물의 녹는점 예측 등), (iv) 속성 분류(예: 데이터에서 별을 적색 왜성으로 분류할지 여부), (v) 시퀀스 생성 또는 설계(특정 속성을 가진 DNA 서열 제안 등)와 같은 범주로 나뉩니다. 지도 조정 후, 모델에 과학 문제에 대한 장문 사고의 사슬(long-form chain-of-thought) 추론을 특별히 가르치기 위해 '어닐링된 콜드 스타트(annealed cold-start)' 부트스트래핑(bootstrapping) 기법을 적용합니다. 이는 모델이 복잡한 질문에 대한 단계별 해결책을 생성하도록 프롬프트(prompting)하고, 이를 추가 훈련 데이터로 활용하는 과정을 포함할 가능성이 높습니다(점진적으로 복잡성을 증가시키므로 '어닐링된'). 마지막으로, 과학적 추론을 위한 맞춤형 보상 형성(reward shaping)과 함께 강화 학습(reinforcement learning)을 사용합니다. 이 최종 단계는 모델에 중간 단계(단위 일관성, 방정식 정확성, 논리적 일관성 등)에 대한 피드백을 제공하여, 의도적이고 엄격한 추론 습관을 확고히 심어주는 역할을 합니다. 모든 훈련 아티팩트(모델 가중치, 지시 데이터, 평가 코드)는 공개적으로 배포되어 SciReasoner를 커뮤니티의 중요한 자원(resource)으로 만듭니다.

**주요 발견은 무엇인가?**
SciReasoner는 이전에는 별도의 도구 앙상블(ensemble)이 필요했던 작업을 단일 모델로 처리할 수 있는 능력을 보여주었습니다. 전문 모델이나 기존 기준선(baselines)과 비교했을 때, SciReasoner는 더 넓은 지시 범위, 향상된 교차 도메인 일반화(cross-domain generalization) 능력, 그리고 출력의 더 높은 충실도(fidelity)를 입증했습니다. 예를 들어, 텍스트로 설명된 화학 문제를 받아 방정식과 함께 단계별 해결책을 출력하거나, 유전체 시퀀스(genomic sequence)를 가능성 있는 기능 설명으로 번역하는 등 언어와 형식 데이터를 연결하는 작업을 놀라운 정확도로 수행할 수 있습니다. 본 논문은 여러 학문 분야를 함께 훈련하는 것이 실제로 **전이 학습(transfer learning)을 향상시켰음**을 명확히 보여줍니다. 즉, 한 도메인의 작업을 해결하는 과정에서 학습된 지식이 다른 도메인에서의 성능 향상으로 이어졌는데, 이는 모델이 일반적인 과학적 추론 전략을 습득했기 때문으로 해석됩니다. 이러한 지식의 교차 수분(cross-pollination)은 모델의 신뢰성을 더욱 강화했습니다. 예를 들어, 물리학 방정식에서 학습한 엄격함은 회계 계산과 같은 분야에서 오류를 방지하는 데 기여했습니다. 평가 결과, SciReasoner는 특정 단일 분야의 전문 모델은 아니지만, 많은 벤치마크에서 도메인별 모델과 동등하거나 더 우수한 성능을 보였습니다. 특히 생물학과 화학을 모두 포함하는 질문과 같이, 지식 혼합을 요구하는 과제에서는 명확한 이점을 가졌습니다. 본질적으로 SciReasoner는 **하나의 모델이 동시에 유능한 물리학자, 화학자, 생물학자 등이 될 수 있으며, 이러한 통합이 오히려 각 개별 역량을 더욱 강화한다**는 기반을 마련합니다. 이는 과학 전반에 걸쳐 추론할 수 있는 AI를 향한 중요한 발걸음입니다. 모델과 데이터의 오픈소스화(open-sourcing) 또한 주요 성과이며, 이는 과학 QA, 가설 생성 등을 위한 추가 미세 조정(finetune) 또는 벤치마크 개발을 위한 강력한 기반을 커뮤니티에 제공함으로써 과학 AI 연구를 가속화할 것입니다.

**다음 단계는 무엇인가?**
SciReasoner는 수많은 새로운 연구 및 응용 가능성을 열어줍니다. 가까운 미래에 연구자들은 이 모델을 기반으로 전문화된 에이전트(agents)를 개발할 수 있을 것입니다. 예를 들어, SciReasoner를 사용하여 실험을 설계하고, 이를 가상 또는 실제 실험실 시뮬레이션(simulation)에서 실행하는 로봇 과학자를 상상해볼 수 있습니다. 대규모 다중 형식 사전 학습(multi-format pre-training) 및 신중한 단계별 정렬(staged alignment)과 같은 훈련 기술은 경제학(텍스트와 스프레드시트 및 공식을 혼합) 또는 사회 과학(텍스트와 통계 데이터를 혼합)과 같이 다중 표현 추론을 요구하는 다른 도메인에도 적용될 수 있습니다. 또 다른 유력한 방향은 모델의 스케일링(scaling)입니다. SciReasoner-8B는 이미 인상적이지만, 유사하게 훈련된 700억 매개변수 모델은 많은 분야에서 인간 전문가 수준에 근접할 수 있을 것입니다. 평가에 대한 추가 연구도 필요합니다. 모델이 103가지 작업을 다루지만, 각 작업에서 추론 품질과 사실적 정확성을 어떻게 철저히 검증할 것인가에 대한 질문은 새로운 학제 간 벤치마크(interdisciplinary benchmarks)의 개발로 이어질 수 있습니다. 마지막으로, SciReasoner의 출시는 개방형 과학 AI 문화를 조성하는 데 기여합니다. 더 많은 연구자들이 이를 활용하고 개선함에 따라, 우리는 모든 과학 교과서를 섭렵한 강력하면서도 폭넓은 지식을 갖춘 동료처럼, 어떤 연구자라도 자신의 작업을 향상시키는 데 사용할 수 있는 **'AI 과학 보조원'**으로 이어지는 선순환을 목격하게 될 것입니다. 장기적인 비전은 학문 분야 간 통찰력을 교차 수분(cross-pollinate)할 수 있는 AI(예: 물리학 원리를 사용하여 생물학 문제를 해결)를 구현하는 것이며, SciReasoner는 AI 분야에서 광범위한 사고를 위한 포괄적인 훈련의 가치를 보여주는 그 방향의 기초적인 단계를 제공합니다.

---

❤️ 본 글이 유익하셨다면 '추천'을 눌러주시고 동료들과 공유해 주시면 감사하겠습니다.
여러분의 소중한 의견을 댓글로 남겨주세요.
LLM 인사이트를 읽어주셔서 진심으로 감사드립니다!
저희의 최신 발행물을 무료로 받아보고 저희의 노력을 응원하시려면 지금 바로 구독해 주세요.
구독 신청