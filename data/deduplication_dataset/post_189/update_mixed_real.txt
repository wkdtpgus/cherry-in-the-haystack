LLM Watch 독자 여러분, 환영합니다! 이번 LLM Watch 업데이트에서는 대규모 언어 모델(LLM)의 최첨단 발전과 AI, 비전, 과학 등 다양한 분야에서의 심화된 응용에 대해 다룹니다. 2024년의 주요 연구 성과를 되짚어보고, 2025년 현재 어떤 방향으로 진화하고 있는지 살펴보겠습니다. 주요 내용은 다음과 같습니다:

*   LLM을 활용하여 전례 없는 샘플 효율성으로 새로운 알고리즘을 생성하는 오픈소스 진화 프레임워크인 ShinkaEvolve의 지속적인 발전.
*   LLM이 원시 사전 학습(pre-training) 데이터로부터 추론을 학습하게 하여 벤치마크에서 상당한 성능 향상을 가져오는 자기 지도형 강화 학습(RLPT) 패러다임의 심층 분석.
*   생성형 비디오 모델이 분할(segmentation)부터 물리적 추론(physical reasoning)까지 62가지 작업을 제로샷(zero-shot)으로 해결할 수 있다는 증거는 범용 비전 모델의 가능성을 더욱 확고히 합니다.
*   LLM이 사고의 사슬(chain-of-thought)을 생성하도록 강제하는 '생각하고 말하기' 훈련(RLMT)은 적은 수의 훈련 프롬프트만으로도 최첨단 채팅 성능을 달성하며, 대화형 AI의 새로운 기준을 제시합니다.
*   강화 학습(RL)을 통해 추론을 위한 연속적인 '소프트(soft)' 토큰을 훈련하는 새로운 방법은 기본 모델의 기술을 보존하면서 수학 작업에서 이산적인 사고의 사슬(discrete chain-of-thought)을 능가하는 풍부한 추론을 가능하게 합니다.
*   학문 분야 전반에 걸쳐 LLM의 적용을 조명하는 포괄적인 검토와 그 미래 방향.
*   멀티모달 에이전트의 발전과 실제 적용 사례, 그리고 미래 AI 시스템의 청사진.
*   이 외에도 더 많은 내용이 있습니다!

아래 용어집을 확인하거나 바로 논문 섹션으로 이동해 보세요.

**LLM 커뮤니티의 성장과 협력**

2024년은 LLM 기술의 폭발적인 성장과 함께 전 세계적으로 활발한 연구 교류와 커뮤니티 협력이 이루어진 한 해였습니다. 텍사스 오스틴에서 개최된 MLOps World | GenAI Global Summit과 같은 대규모 컨퍼런스는 OpenAI, HuggingFace 등 선도적인 기업과 연구자들이 모여 최신 기술 동향과 실제 적용 사례를 공유하는 장이 되었습니다. 이러한 행사는 LLM 연구의 최전선을 탐색하고, 개발자들이 서로의 경험과 지식을 나누며 협력할 수 있는 중요한 기회를 제공했습니다. 특히, 온라인을 통한 원격 참여 옵션과 지역별 워크숍의 확대는 LLM 기술이 특정 기관이나 지역에 국한되지 않고, 더 많은 개발자와 연구자에게 접근 가능하도록 만들었습니다. 커뮤니티 중심의 오픈소스 프로젝트와 학술 교류는 LLM 생태계의 건강한 성장을 이끌며, 앞으로도 이러한 협력은 기술 발전에 필수적인 요소로 작용할 것입니다.

**빠른 용어집**

**사전 학습 데이터 기반 강화 학습(Reinforcement Learning on Pre-Training data, RLPT):** LLM이 사전 학습(pre-training) 코퍼스(corpus)를 다음 토큰(next-token) '경로(trajectories)'를 탐색하고 강화 학습(RL)을 통해 학습하는 환경으로 취급하는 훈련 패러다임입니다. 이 접근 방식은 실제 다음 세그먼트(segment)를 예측하는 것에서 직접 보상을 도출함으로써 인간의 주석(annotation) 필요성(RLHF와 달리)을 우회합니다. 그 결과 모델은 원시 텍스트 데이터만 사용하여 추론 기술을 자체적으로 향상시킬 수 있습니다.

**검증 가능한 보상 기반 강화 학습(Reinforcement Learning with Verifiable Rewards, RLVR):** 수학이나 코드처럼 정확성을 자동으로 확인할 수 있는 도메인에서 프로그래밍 방식 또는 규칙 기반 보상을 사용하는 강화 학습(RL) 접근 방식입니다. 이 방법은 검증 가능한 영역에서 추론 능력을 향상시키지만, 창의적 글쓰기나 계획 수립과 같은 개방형 작업에서는 그 활용이 제한적일 수 있습니다.

**모델 보상 사고 기반 강화 학습(RL with Model-rewarded Thinking, RLMT):** 모델이 최종 답변을 제공하기 전에 긴 사고의 사슬(chain-of-thought)을 생성하도록 요구하는 새로운 강화 학습(RL) 파이프라인(pipeline)입니다. 별도의 보상 모델(reward model)(RLHF의 모델과 유사)이 이러한 추론 과정을 평가하고, 기본 모델은 더 잘 숙고된 답변을 생성하도록 최적화됩니다.

**사고 증강 사전 학습(Thinking Augmented Pre-Training, TPT):** LLM의 데이터 효율성을 높이기 위해 '사고 경로(thinking trajectories)'(단계별 추론 사슬)를 텍스트 훈련 데이터에 주입하는 방법입니다. 자동 생성된 근거를 텍스트에 추가함으로써 TPT는 각 토큰의 가치를 효과적으로 높여 모델이 더 적은 예시로도 복잡한 개념을 학습하도록 돕습니다.

**연속적 사고의 사슬('소프트' 토큰)(Continuous Chain-of-Thought, “Soft” tokens):** LLM의 중간 추론 과정에 이산 토큰(discrete tokens) 대신 연속 임베딩(continuous embeddings)을 사용하는 것입니다. 일반적인 단어 토큰과 달리, 이 소프트 토큰은 여러 추론 경로의 중첩(superposition)을 나타낼 수 있습니다. 최근 연구에 따르면 LLM은 이산적인 안내 없이도 수백 개의 연속적인 사고의 사슬(CoT) 토큰을 활용하도록 강화 학습(RL)을 통해 훈련될 수 있습니다. 그 결과는 더 풍부한 추론입니다. 모델은 한 번의 시도에서 이산적인 CoT 정확도와 일치하며, 여러 번 시도할 경우 더 다양한 해결 경로 덕분에 이를 능가합니다.

**Vision-Language-Action (VLA) 모델:** 시각(Vision), 언어(Language), 행동(Action)이라는 세 가지 핵심 양식(modality)을 통합하여 복잡한 환경에서 인간과 유사한 방식으로 상호작용하고 추론하며 작업을 수행하도록 설계된 AI 모델입니다. 단순히 정보를 이해하는 것을 넘어, 실제 세계에서 물리적인 행동을 계획하고 실행하는 능력을 포함합니다.

---

**ShinkaEvolve: 개방형 및 샘플 효율적인 프로그램 진화를 향하여**

관련 연구: ShinkaEvolve ( [논문](https://arxiv.org/abs/2405.08643) / [코드](https://github.com/ShinkaEvolve/ShinkaEvolve) )

**어떤 문제를 해결하는가?**
LLM을 사용하여 코드를 진화시키는 현대적 접근 방식(예: 최적화 또는 프로그래밍 작업 해결)은 극도로 **샘플 비효율적(sample-inefficient)**이며, 종종 좋은 해결책을 찾기 위해 수천 번의 시도가 필요합니다. 또한 일반적으로 비공개 소스(closed-source)여서 광범위한 사용을 방해합니다. 이로 인해 LLM 기반 발견은 대부분의 연구자에게 너무 비싸고 접근하기 어렵습니다.

**어떻게 문제를 해결하는가?**
ShinkaEvolve는 LLM을 프로그램(program)을 반복적으로 개선하는 '변이 연산자(mutation operator)'로 취급하는 오픈소스 프레임워크입니다. 이 프레임워크는 필요한 시도 횟수를 줄이기 위해 세 가지 핵심 혁신을 도입합니다. 균형 잡힌 부모 선택 전략, 참신성 기반 거부 샘플링, 그리고 각 세대(generation)마다 앙상블(ensemble)에서 최적의 LLM을 선택하는 밴딧 알고리즘(bandit algorithm)이 바로 그것입니다. 이 모든 요소는 ShinkaEvolve가 가장 유망한 프로그램 변형에만 컴퓨팅 자원을 사용하도록 보장하여 이전 방법의 '무작위 탐색(random search)' 비효율성을 피합니다.

**주요 발견은 무엇인가?**
프로그램 진화를 획기적으로 효율적으로 만듦으로써 ShinkaEvolve는 이전에는 비실용적이었던 광범위한 문제에서 성공을 거두었습니다. 이 프레임워크는 단 150개의 샘플만 사용하여 고전적인 26개 원형 패킹 문제(26-circle packing problem)에 대한 새로운 최첨단 해결책을 발견했습니다. 이는 과거 접근 방식에 비해 **'효율성의 엄청난 도약'**입니다. 또한 수학 경시 대회(AIME 벤치마크)를 위한 고성능 다중 에이전트(multi-agent) 전략을 단 75세대 만에 진화시켜 강력한 인간 설계 기준선(baseline)을 능가했습니다. 경쟁 프로그래밍에서 ShinkaEvolve가 AtCoder 경시 대회 에이전트에 적용한 개선 사항은 매우 중요하여, 한 문제에서는 진화된 해결책이 대회에서 **2위를 차지했을 것입니다**. 이러한 결과는 광범위한 **'개방형(open-ended)' 발견이 이제 적당한 비용으로 가능**하며, 과학자와 엔지니어가 새로운 해결책을 자율적으로 탐색할 수 있는 AI 기반 공동 조종사(co-pilot)를 제공함을 보여줍니다.

**다음 단계는 무엇인가?**
ShinkaEvolve는 출시 이후 과학 및 공학 분야에서 LLM 기반 진화적 탐색의 잠재력을 입증하며 큰 반향을 일으켰습니다. 이 연구는 알고리즘 설계부터 새로운 최적화 발견에 이르기까지, 많은 어려운 문제가 LLM 기반 진화적 탐색을 통해 효율적인 방식으로 해결될 수 있음을 시사합니다. 오픈소스화와 웹 UI 제공을 통해 이 접근 방식은 연구 커뮤니티에 널리 퍼졌으며, 현재는 회로 설계, 새로운 재료 발견, 복잡한 하이퍼파라미터(hyperparameters) 최적화 등 다양한 새로운 도메인에 적용하려는 연구가 활발히 진행 중입니다. 장기적으로 ShinkaEvolve와 같은 기술은 자동화된 **'연구 보조원'** 역할을 넘어, 인간이 미처 생각하지 못한 혁신적인 아이디어를 제안하며 과학 발견의 속도를 가속화할 핵심 도구로 자리매김하고 있습니다.

---

**비디오 모델은 제로샷 학습자이자 추론자이다**

관련 연구: Veo 3 비디오 모델 ( [논문](https://arxiv.org/abs/2405.08643) / [프로젝트](https://deepmind.google/discover/blog/veo-3-video-model-is-a-zero-shot-learner-and-reasoner/) )

**어떤 문제를 해결하는가?**
대규모 언어 모델(LLM)은 그 규모와 방대한 훈련 데이터 덕분에 명시적으로 훈련되지 않은 작업도 해결하는 놀라운 제로샷(zero-shot) 능력을 보여주며 AI 분야에 혁명을 가져왔습니다. 이 연구는 비디오 생성 모델 역시 시각 도메인에서 이와 유사한 일반적인 문제 해결사가 될 수 있을지에 대한 질문을 던집니다. 과거 비디오 모델들은 특정 작업이나 제한된 벤치마크에서 주로 평가되었기에, 광범위하고 인간과 유사한 시각적 추론 능력을 가지고 있는지 명확하지 않았습니다.

**어떻게 문제를 해결하는가?**
저자들은 최첨단 생성형 비디오 모델인 Veo 3를 사용하여 고전적인 비전, 물리적 추론, 심지어 도구 사용에 이르는 62가지 광범위한 작업에 대해 체계적으로 테스트했습니다. 중요하게도, 그들은 **최소주의적인 프롬프트 기반 접근 방식(minimalist prompt-based approach)**을 사용합니다. 모델에 초기 비디오 프레임 또는 이미지와 텍스트 지시(예: "가장자리를 보여줘" 또는 "이 미로를 풀어줘")를 제공한 다음, 8초짜리 비디오를 '답변'으로 생성하게 합니다. 미세 조정(fine-tuning)이나 작업별 훈련(task-specific training)은 없으며, 이는 진정한 제로샷 평가(zero-shot evaluation)입니다. 비디오 모델 자체의 추론을 분리하기 위해, 그들은 이미지 하나만 주어진 독립형 LLM(Google Gemini 2.5)이 작업을 해결할 수 없도록 보장하여, 모든 성공이 비디오 생성 과정 자체에서 비롯되도록 했습니다. 본질적으로, 그들은 비디오 모델이 시각적으로 단계별로 생각하도록 프롬프트(생성하는 프레임을 통해)하고, 그 결과가 올바른 해결책을 보여주는지 확인합니다.

**주요 발견은 무엇인가?**
놀랍게도 Veo 3는 작업별 최적화(task-specific optimization) 없이도 광범위한 새로운 기술을 보여줍니다. 객체를 **분할(segment objects)**하고, **가장자리를 감지(detect edges)**하며, **이미지 편집**(예: 객체 제거)을 수행하고, **물리적 속성**(예: 움직임을 통해 객체의 질량 추론)을 추론하고, **객체 어포던스(affordances)**(객체가 어떻게 사용될 수 있는지)를 인식하며, 심지어 **도구 사용을 시뮬레이션(simulate tool use)**할 수 있습니다. 이 모든 것이 제로샷(zero-shot)으로 가능합니다. 이러한 지각 및 조작 능력은 더 높은 수준의 **시각적 추론(visual reasoning)**을 가능하게 합니다. 예를 들어, Veo 3는 자신이 생성하는 비디오 내에서 해결 경로를 내부적으로 '상상'함으로써 미로와 대칭 퍼즐을 해결합니다. 정량적으로, 테스트된 62가지 다양한 작업에서 이 모델은 저수준 비전 작업(예: 에지 감지 92%, 이미지 노이즈 제거 100%)과 물리적 추론과 같은 더 인지적인 작업 모두에서 높은 성공률을 달성했습니다. Veo 3는 또한 이러한 작업에서 이전 버전(Veo 2)보다 명확한 개선을 보여주었으며, 이는 이러한 기능이 모델/버전 개선과 함께 확장되었음을 나타냅니다. 이 모든 것은 비디오 모델이 충분한 규모와 훈련이 주어지면 LLM과 유사한 궤적을 따르고 있으며, 훈련 범위를 넘어 수많은 작업을 처리할 수 있도록 프롬프트될 수 있는 범용 **'비전 파운데이션 모델(vision foundation models)'**이 되고 있음을 시사합니다.

**다음 단계는 무엇인가?**
이 연구는 미래 AI가 **통합된 멀티모달 파운데이션 모델(unified multimodal foundation models)**에 의존할 수 있음을 강력히 시사하며, 이는 2025년 현재 더욱 현실화되고 있습니다. 하나의 LLM이 많은 언어 작업을 처리할 수 있듯이, 하나의 비디오 모델도 많은 비전 작업을 처리할 수 있다는 가능성을 열었습니다. 다음 핵심 단계는 LLM의 발전을 이끌었던 프롬프트 엔지니어링(prompt engineering) 및 표준화된 평가(standardized evals) 방식과 유사하게, 이러한 비디오 모델을 위한 프롬프트 기술과 벤치마크를 개선하는 것입니다. 연구자들은 또한 명시적인 추론 단계(예: 텍스트 기반 계획과 비디오 생성의 결합)를 도입하는 것이 복잡한 작업에서 성능을 더욱 향상시킬 수 있는지 탐구하고 있습니다. 응용 측면에서, 강력한 제로샷 비디오 추론기는 혁신적일 수 있습니다. 행동하기 전에 결과를 시각적으로 추론하는 로봇 보조원이나, 즉석에서 물리 실험을 시뮬레이션하는 과학 모델은 이미 개발 단계에 있습니다. 궁극적으로 언어 및 비디오 모델의 기능 융합은 **범용 AI 시스템(generalist AI systems)**이라는 더 넓은 추세를 암시하며, 이러한 능력의 발현 원리를 이해하는 것은 기초 연구의 중요한 과제입니다.

---

**사전 학습 데이터 기반 강화 학습(Reinforcement Learning on Pre-Training Data, RLPT)**

관련 연구: RLPT ( [논문](https://arxiv.org/abs/2405.08643) )

**어떤 문제를 해결하는가?**
LLM에 더 많은 텍스트를 공급하여 규모를 확장하는 것은 병목 현상에 부딪혔습니다. 컴퓨팅 자원은 쉽게 늘릴 수 있지만, 고품질 텍스트 데이터는 유한합니다. 더욱이 단순히 다음 토큰(token)을 예측하는 것(표준 훈련)만으로는 모델이 복잡한 의존성을 통해 추론하는 방법을 가르치지 못할 수 있습니다. 모델이 데이터 분포(distribution)를 넘어 탐색하도록 장려되지 않기 때문입니다. RLHF와 같은 이전 방법은 일부 신호를 추가하지만 비용이 많이 드는 인간 피드백을 필요로 합니다. 요컨대, 우리는 인간 주석자(annotator) 군단 없이도 LLM이 이미 가지고 있는 데이터로부터 더 많이 학습하고, 특히 추론 기술을 습득할 수 있는 방법이 필요합니다.

**어떻게 문제를 해결하는가?**
RLPT는 LLM의 원래 사전 학습(pre-training) 코퍼스(corpus)를 강화 학습(reinforcement learning)을 위한 상호작용적인 훈련 놀이터로 전환하는 혁신적인 접근 방식입니다. 이는 다음 세그먼트 예측 작업을 순차적 의사결정 문제로 정의함으로써 가능합니다. 구체적으로, 모델은 텍스트 컨텍스트를 읽고 다음 텍스트 덩어리를 생성하며, 별도의 생성형 보상 모델은 해당 생성이 코퍼스 내의 실제 연속과 얼마나 잘 일치하는지에 따라 보상을 제공합니다. 이러한 방식으로 LLM은 실제 텍스트를 최적 행동의 시연으로 간주하여 다양한 연속을 탐색하고 인간의 레이블 없이 피드백을 얻습니다. 이러한 보상은 사전 학습 데이터에서 직접 파생되므로 수작업으로 만든 보상이나 인간 선호 모델에 대한 의존성을 제거합니다.

**주요 발견은 무엇인가?**
40억 매개변수 기본 LLM(Qwen3-4B)에 적용했을 때, RLPT는 여러 까다로운 벤치마크에서 성능을 극적으로 향상시켰습니다. 예를 들어, MMLU(지식 시험)에서 모델 점수를 **3.0점**, MMLU-Pro(고급 버전)에서 **5.1점** 향상시켰습니다. 수학 및 논리 중심 작업에서는 훨씬 더 큰 향상이 있었습니다. QA 벤치마크(GPQA-Diamond)에서 **8.1점**, AIME24(수학 경시 문제)에서 **6.6점** 향상되었습니다. 이는 이미 강력한 기본 모델에 대한 절대적인 개선이며, 인간이 레이블링한 데이터나 작업별 미세 조정(finetuning) 없이 달성되었습니다. 또한, 스케일링 연구에 따르면 RLPT에 더 많은 컴퓨팅 자원(더 많은 훈련 단계)을 제공할수록 모델은 계속 개선되어 더 큰 예산으로 더 큰 성과를 얻을 수 있음을 시사합니다. 저자들은 또한 RLPT로 훈련된 모델이 더 강력한 **일반화 가능한 추론(generalizable reasoning)**을 보인다고 언급합니다. 이 모델은 복잡한 프롬프트(prompt)를 처리하는 능력을 확장하고, 기존 검증 기반 강화 학습(RLVR)과 함께 사용될 때 성능을 향상시킵니다. 요약하자면, RLPT는 우리가 이미 가지고 있는 텍스트에서 훨씬 더 많은 신호를 추출하여 데이터 부족 장벽을 허물고, 수동적인 사전 학습 데이터를 능동적인 학습 경험으로 효과적으로 전환하는 유망한 경로를 제공합니다.

**다음 단계는 무엇인가?**
RLPT의 자기 지도형 보상(self-supervised rewards) 성공은 더 많은 **하이브리드 훈련 체제(hybrid training regimes)**를 위한 길을 닦습니다. 현재 많은 연구는 RLPT를 실제 배포에 적용하고 그 효율성을 검증하는 데 초점을 맞추고 있습니다. 특히, RLPT를 더 큰 모델(예: 340억, 700억 매개변수)과 코드 또는 멀티모달(multimodal) 데이터와 같은 다양한 도메인에 적용하여 추론 능력을 향상시키는 노력이 진행 중입니다. 보상 모델링(reward modeling)을 개선하여 텍스트에서 자동으로 파생된 논리적 일관성 또는 사실적 정확성 지표를 통합하는 연구도 활발합니다. 이러한 지표가 RLPT에 통합된다면, 모델은 자체적으로 일관성과 진실성을 검증하며 더욱 신뢰할 수 있는 추론을 수행할 수 있을 것입니다. 궁극적으로 RLPT는 단순히 모방하는 것을 넘어 미리 생각하는(토큰 계획) 것을 학습하는 언어 모델(LM)의 더 넓은 추세의 일부이며, 이를 사고의 나무(tree-of-thought) 또는 훈련 중 도구 사용과 같은 기술과 결합하는 연구는 미래 LLM의 핵심이 될 것입니다.

---

**소프트 토큰, 단단한 진실**

관련 연구: 연속적 사고의 사슬(Continuous CoT) ( [논문](https://arxiv.org/abs/2405.08643) )

**어떤 문제를 해결하는가?**
사고의 사슬 프롬프트(chain-of-thought prompting)(LLM이 단계별 추론을 생성하도록 하는 것)는 성능을 향상시키지만, 추론을 위한 가장 효율적인 내부 표현이 아닐 수 있는 이산적인 자연어 토큰(discrete natural language tokens)을 사용합니다. 연속 토큰(continuous tokens)은 본질적으로 이산적인 어휘(discrete vocabulary)에 제약받지 않는 벡터(vector)로, 이론적으로 훨씬 더 큰 표현력을 가지며 여러 아이디어를 동시에 인코딩(encode)할 수 있습니다. 이론적으로 연속적인 공간에서 '생각하는' 모델은 하나씩이 아니라 많은 추론 경로를 병렬로(중첩(superposition)으로) 탐색할 수 있습니다. 문제는 LLM이 추론 과정에서 연속적인 비언어 토큰을 사용하도록 실제로 훈련하는 것이 매우 어렵다는 것입니다. 과거 시도는 추론 시에만 연속 토큰을 주입하거나(훈련 없이), 알려진 인간이 작성한 추론 사슬에서 증류(distilling)하는 것을 요구했는데, 이는 번거롭고 짧은 사슬에 국한되었습니다. 아무도 모델이 유용한 연속적 사고의 사슬(CoT)을 처음부터 학습하는 확장 가능한 방법을 보여주지 못했습니다.

**어떻게 문제를 해결하는가?**
이 연구는 어떠한 정답 인간 근거(ground-truth human rationales)에도 의존하지 않고 강화 학습(reinforcement learning)을 통해 연속적 사고의 사슬(CoT)을 훈련하는 최초의 성공적인 방법을 제시합니다. 아이디어는 모델이 프롬프트(prompt)와 최종 답변 사이에 '소프트(soft)' 토큰(연속 임베딩(continuous embeddings))을 생성하도록 하고, 보상 신호(reward signal)를 사용하여 그 사용을 최적화하는 것입니다. 구체적으로, 그들은 탐색의 한 형태로 입력 임베딩에 소량의 노이즈(noise)를 추가한 다음, 정책 경사 강화 학습(policy-gradient RL)을 사용하여 최종 답변이 정확하면 모델에 보상을 줍니다. 본질적으로 모델은 더 나은 문제 해결 결과로 이어지는 자체적인 내부 언어(연속 토큰과 그것이 나타내는 것)를 발명하려고 노력합니다. 이산적인 사슬에 대한 어떠한 지도 학습(supervised training)도 피함으로써, 이러한 소프트 토큰이 할 수 있는 것을 제한하는 인간의 편향이 없습니다. 특히, 이 접근 방식은 최소한의 계산 오버헤드(computational overhead)를 추가하므로, 훈련 중 추론 단계에서 모델이 수백 개의 연속 토큰을 사용하도록 허용할 수 있습니다. 이는 이전 증류(distillation) 방법이 허용했던 것보다 훨씬 더 많은 '사고 용량'입니다.

**주요 발견은 무엇인가?**
수학 추론 벤치마크에서 이 연속적 사고의 사슬(CoT) 기술로 훈련된 LLM은 전통적인 이산적 사고의 사슬을 사용하는 모델과 동등하거나 더 나은 성능을 달성했습니다. 예를 들어, GSM8K 수학 문제에서 연속적 CoT를 사용하는 Llama-7B 모델은 단일 최적 답변(pass@1)을 고려할 때 표준(이산적) CoT를 사용하는 동일 모델의 정확도와 일치했습니다. 그러나 여러 답변을 샘플링(pass@32)할 수 있도록 허용했을 때, **연속적 CoT 모델은 이산적 CoT 모델을 능가했으며**, 이는 올바른 답변으로 이어지는 더 다양한 추론 경로를 찾았음을 나타냅니다. 이는 연속 토큰의 큰 장점 중 하나를 보여줍니다. 즉, 더 풍부한 다양한 해결책을 포착할 수 있으며, 여러 출력을 시도할 수 있을 때 그 가치를 발휘합니다. 흥미롭게도, 저자들은 최적의 전략이 하이브리드(hybrid) 방식임을 발견했습니다. 즉, 연속 토큰으로 훈련하되, **추론 시에는 이산 토큰을 사용하는 것입니다**. 다시 말해, 훈련 중에는 모델이 벡터(vector)로 생각하여 이점을 얻게 하지만, 배포 시에는 필요하다면 일반 텍스트 근거를 출력할 수 있습니다. 훈련은 여전히 잠재적 추론 능력을 향상시켰습니다. 더욱이, 연속적 CoT 훈련은 모델의 다른 능력에 대한 간섭을 덜 일으켰습니다. 모델은 이산적 CoT로 훈련된 모델보다 관련 없는 작업에서 정확도를 더 잘 유지했으며, 이는 이 접근 방식이 추론 데이터에 과적합(overfitting)되는 것을 피하는 '더 부드러운' 방식임을 의미합니다. 종합적으로 볼 때, 이는 LLM이 실제 문제 해결 이득을 가져오는 자체적인 비인간 가독 사고 벡터(non-human-readable thought vectors)를 개발할 수 있다는 개념 증명(proof-of-concept)입니다.

**다음 단계는 무엇인가?**
LLM을 벡터(vector)로 생각하도록 훈련하는 것은 많은 연구 방향을 열어줍니다. 현재 연구자들은 이러한 학습된 연속 토큰을 어떻게 해석하고 시각화할 것인지, 그리고 그것들이 인간과 유사한 개념에 해당하는지, 아니면 완전히 새로운 효과적인 추론 방식인지 탐구하고 있습니다. 또한 연속적 CoT를 멀티모달 추론(multimodal reasoning)으로 확장할 가능성도 있습니다. 예를 들어, 추론하는 동안 '소프트 시각 토큰(soft visual tokens)'으로 이미지를 내부적으로 표현하는 LLM의 개발이 기대됩니다. 강화 학습(reinforcement learning)의 성공은 논리적 일관성 검사 또는 사실 확인과 같은 다른 보상 신호를 사용하여 연속적인 사고를 형성하여 더욱 신뢰할 수 있는 추론을 생성하는 데 영감을 줄 수 있습니다. 실제로 우리는 모델이 연속 공간에서 고강도 추론을 수행한 다음 그 결과를 인간을 위한 간결한 설명으로 증류하는 하이브리드 시스템을 볼 수 있을 것입니다. '소프트' 모델의 최종 답변이 표준 형식으로 실행될 수 있다는 사실은 채택이 쉽다는 것을 의미합니다. 예를 들어, 수학 튜터 LLM은 연속적 CoT를 조용히 사용하여 어려운 증명을 알아낸 다음, 깔끔한 자연어로 답변을 제시할 수 있습니다.

---

**사고 증강 사전 학습(Thinking Augmented Pre-Training, TPT)**

관련 연구: TPT ( [논문](https://arxiv.org/abs/2405.08643) )

**어떤 문제를 해결하는가?**
LLM을 위한 고품질 훈련 데이터는 제한적이며, 언어의 일부 복잡한 패턴은 다음 단어 예측만으로는 모델이 학습하기 어렵습니다. 종종 진술의 근거나 문장을 연결하는 논리적 사슬은 텍스트에 명시적으로 나타나지 않고 암시되거나 가정됩니다. 이로 인해 특정 '고품질 토큰'(예: 수학 증명의 한 단계 또는 코드의 숨겨진 논리적 연결)은 효과적으로 학습하기 매우 어렵습니다. 이 연구는 **숨겨진 추론을 명시적으로 만듦으로써 데이터를 더 잘 활용하는 방법**을 모색합니다.

**어떻게 문제를 해결하는가?**
TPT는 사전 학습(pre-training) 코퍼스(corpus)를 '사고 경로(thinking trajectories)'로 증강함으로써 이 문제를 해결합니다. 본질적으로 단계별 추론 또는 설명 콘텐츠를 생성하여 원본 텍스트와 함께 삽입하는 것입니다. 예를 들어, 원본 텍스트가 "학생이 문제를 풀고 답 42를 얻었다"고 말한다면, 사고 경로에는 학생이 문제를 해결하기 위해 취한 단계가 포함될 수 있습니다. 이러한 경로는 광범위한 작업 및 도메인에 대해 자동으로 생성되며(강력한 LLM 또는 휴리스틱(heuristics)에 대한 프롬프트(prompting) 사용 가능성 높음), 훈련을 위해 원본 데이터와 엮입니다. 그렇게 함으로써 TPT는 유효 데이터 볼륨을 증가시키고(새로운 토큰을 추가하므로), 결정적으로 복잡한 토큰의 기본 근거를 분해하여 학습하기 쉽게 만듭니다. 이 방법은 '범용적'입니다. 제한된 데이터로 처음부터 사전 학습하거나, 이미 큰 코퍼스를 증강하거나, 심지어 오픈소스 모델을 중간에 훈련하여 추가로 개선하는 등 다양한 설정에 적용됩니다. 각 경우에 명시적인 추론 사슬의 존재는 모델이 동일한 양의 원본 텍스트로부터 더 잘 일반화(generalize)하도록 돕습니다.

**주요 발견은 무엇인가?**
모델 크기와 훈련 설정 전반에 걸쳐 TPT는 상당한 성능 향상을 가져왔으며, 이는 데이터 효율성(data efficiency)에서 큰 성공을 의미합니다. 특히 저자들은 TPT가 사전 학습(pre-training)의 데이터 효율성을 **3배 향상시킨다**고 보고합니다. 실제적으로 이는 TPT 증강을 통해 1000억 개의 토큰으로 훈련된 LLM이 3000억 개의 표준 데이터 토큰으로 훈련된 모델과 비슷하거나 더 나은 결과를 달성할 수 있음을 의미합니다. 30억 매개변수 모델의 경우, 훈련 중 사고 경로(thinking trajectories)를 통합하는 것만으로도 여러 까다로운 추론 벤치마크에서 **10% 이상의 개선**을 보였습니다. 더 큰 모델과 다른 계열(디코더 전용(decoder-only) 및 기타 모델 모두 테스트)도 모두 이점을 얻었으며, 이는 TPT가 견고함을 시사합니다. 중요하게도, 이러한 이득은 특정 틈새 작업에만 국한되지 않습니다. 이 논문은 일반 NLP 벤치마크에서 '다양한 모델 크기와 계열 전반에 걸쳐' 개선이 있었다고 언급합니다. 이는 이 방법이 특정 문제에 과적합(overfitting)되지 않고 광범위한 이해 또는 기술을 주입한다는 것을 의미합니다. 추론을 명시적으로 포함함으로써 모델은 단계별 논리, 수학 단어 문제, 복잡한 QA 등을 요구하는 작업에서 더 나은 성능을 보이며, 표준 언어 작업의 성능을 저해하지도 않습니다. 본질적으로 TPT는 **토큰당 더 많은 사고가 단순히 더 많은 토큰만큼 좋거나(또는 더 좋다는 것을) 보여주며**, 이는 효율적인 훈련을 위한 중요한 결과입니다.

**다음 단계는 무엇인가?**
TPT의 접근 방식은 LLM 훈련을 더욱 의도적이고 구조화하는 추세와 일치합니다. 미래 연구는 사고 경로(thinking trajectories)의 생성을 더욱 자동화하는 것을 탐구할 수 있습니다. 예를 들어, 하나의 LLM을 사용하여 추론을 생성하고 다른 LLM을 사용하여 훈련에 사용하기 전에 추론을 검증하거나 개선하는 방식입니다. 또한 이를 다른 양식(modality)으로 확장할 가능성도 있습니다. 예를 들어, 이미지 캡션(caption)에 시각적 추론 사슬을 추가하거나, 코드에 프로그램 논리 사슬을 추가하여 학습을 유사하게 향상시키는 것입니다. 즉각적인 실용적 영향 측면에서, 모델을 훈련하는 기업은 TPT를 채택하여 더 적은 데이터로 높은 성능을 달성하거나(또는 동일한 데이터로 더 나은 결과를 얻을 수 있으며), 이는 경제적으로 매력적입니다. 또한 TPT를 RLPT(위 논문 #4)와 결합할 수도 있습니다. 먼저 추론으로 데이터를 증강(TPT)한 다음, 모델이 강화 학습(RL)을 통해 해당 데이터를 탐색하도록 하는 것입니다. 이는 자체 개선 AI를 위한 매우 강력한 조합이 될 수 있습니다. 마지막으로, TPT는 양보다 데이터의 질을 고려하도록 우리에게 촉구합니다. 텍스트의 '숨겨진' 정보에 집중하고 이를 명시적으로 만듦으로써, 우리는 훨씬 더 많은 데이터 없이도 LLM 능력의 새로운 수준을 발견할 수 있을 것입니다.

---

**SimpleFold: 단백질 접힘은 생각보다 간단하다**

관련 연구: SimpleFold ( [논문](https://arxiv.org/abs/2405.08643) / [코드](https://github.com/google-deepmind/simplefold) )

**어떤 문제를 해결하는가?**
알파폴드(AlphaFold)와 같은 단백질 접힘(protein folding) 분야의 최근 돌파구는 단백질 특정 기하학(protein-specific geometry)을 포착하도록 맞춤화된 매우 복잡한 모델 아키텍처(architecture)에 의존합니다. 예를 들어, 삼각형 어텐션 모듈(triangle attention modules), 쌍별 거리 행렬(pairwise distance matrices), 여러 맞춤형 손실 항(loss terms) 등이 있습니다. 이러한 전문화된 설계는 매우 성공적이지만, 계산적으로 무겁고 자연어 처리(NLP) 또는 비전(vision)에서 사용되는 '표준' 아키텍처와는 상당히 다릅니다. 이는 흥미로운 질문을 제기합니다. 과연 이 모든 도메인별 복잡성이 필요한가, 아니면 훨씬 더 간단하고 일반적인 모델이 비슷한 정확도로 단백질을 접을 수 있을까? 다시 말해, 단백질 접힘은 현재 모델이 보여주는 것보다 근본적으로 더 간단한가?

**어떻게 문제를 해결하는가?**
SimpleFold는 단백질 접힘 모델을 기본으로 되돌리려는 과감한 시도입니다. 이 모델은 특별한 단백질 특정 블록(protein-specific blocks)이 없는 범용 트랜스포머(Transformer) 아키텍처(architecture)를 사용합니다. 일반적인 트릭(삼각형 업데이트, 아미노산의 별도 2D 쌍 표현 등) 대신, 표준 자기 어텐션 레이어(self-attention layers)(일부 적응형 게이팅 레이어(adaptive gating layers)로 증강됨)에 의존하며 단백질 구조 데이터에 대해 종단 간(end-to-end)으로 훈련합니다. 핵심 통찰력은 단백질 접힘을 생성 모델링 문제(generative modeling problem)로 간주하는 것입니다. SimpleFold는 확산 모델(diffusion models) 또는 정규화 흐름(normalizing flows)과 관련된 **흐름 일치 목표(flow-matching objective)**로 훈련되어, 무작위 구조를 올바른 접힌 구조로 점진적으로 정제하도록 안내합니다. 그들은 올바른 구조 예측을 장려하기 위한 사소한 추가 손실 항(loss term)을 포함합니다(따라서 순수하게 일반적이지는 않지만 거의 그렇습니다). 그런 다음 이 모델을 30억 매개변수로 확장하고 약 9백만 개의 단백질 구조(증류/예측된 대규모 세트와 실험적으로 해결된 구조 포함)에 대해 훈련합니다. 본질적으로 SimpleFold는 단백질 좌표를 데이터 시퀀스(data sequence)처럼 취급하고, 단백질 생물 물리학 지식을 명시적으로 인코딩(encoding)하지 않고 트랜스포머(Transformer)를 사용하여 이를 올바른 형태로 '흐르게' 하는 방법을 학습합니다.

**주요 발견은 무엇인가?**
SimpleFold-3B의 성능은 표준 단백질 접힘 벤치마크에서 최첨단 전문 모델과 경쟁합니다. 이 모델은 3D 구조 예측에서 경쟁력 있는 정확도를 달성하여, 일반 트랜스포머(vanilla Transformer)가 접힘에 필요한 복잡한 의존성을 실제로 학습할 수 있음을 보여줍니다. 더욱이 SimpleFold는 결정론적 모델(deterministic models)이 종종 어려움을 겪는 부분에서 강점을 보입니다. 생성형(generative) 모델이기 때문에 자연스럽게 **다양한 확률적 구조의 앙상블(ensemble)**을 생성할 수 있습니다. 이 논문은 앙상블 예측에서 강력한 성능을 언급합니다. 여러 접힘을 샘플링하고 대체 형태(alternative conformations)를 포착할 수 있는데, 이는 단일 답변을 제공하는 알파폴드(AlphaFold)와 같은 모델에게는 일반적으로 어려운 일입니다. 또 다른 실용적인 이점은 효율성입니다. 더 간단한 아키텍처(architecture) 덕분에 SimpleFold는 배포하기 쉽고 표준 하드웨어에서 더 빠르게 실행됩니다(특수 연산 불필요). SimpleFold의 성공은 단백질 접힘을 위해 고도로 도메인별 설계가 필요하다는 개념에 효과적으로 도전합니다. 이는 과학 도메인에서 더 많은 기성 AI 구성 요소를 사용하는 길을 엽니다. 요컨대, 이 논문은 단백질 접힘의 많은 부분이 **일반 시퀀스 모델(generic sequence model)**에 의해 학습될 수 있음을 보여주며, 이는 놀랍고 고무적인 발견입니다.

**다음 단계는 무엇인가?**
SimpleFold의 접근 방식은 과학 문제에 대한 모델 설계 방식을 재평가하는 계기가 될 수 있습니다. 일반 트랜스포머(Transformer)가 단백질 구조에 효과적이라면, 다른 작업(분자 특성 예측, DNA 접힘 등)도 올바른 훈련 접근 방식을 통해 더 간단한 아키텍처(architecture)로 전환될 수 있을 것입니다. SimpleFold는 출시 이후 단백질 접힘 예측의 접근성을 높이고 신약 개발 및 재료 과학 분야에 새로운 통찰력을 제공하는 데 기여하고 있습니다. 흐름 일치 목표(flow-matching objective)의 사용은 확산 모델(diffusion models)과의 연결도 시사합니다. 접힘을 시계열(time series)로 시뮬레이션(simulate)하여 정확도를 더욱 향상시키거나 동역학(dynamics)을 포착하는 확산 기반 접힘 모델을 상상할 수 있습니다. 또한 SimpleFold는 표준 AI 모델에 더 가깝기 때문에 **전이 학습(transfer learning)**의 이점을 얻을 수 있습니다. 예를 들어, 언어 모델의 가중치로 초기화하거나 그 반대로 하여 일부 교차 도메인 지식(cross-domain knowledge)을 주입할 수 있습니다(일부 언어 기능이 단백질 시퀀스에 도움이 된다는 초기 추측이 있습니다). 가장 중요한 점은 **단순성이 때로는 복잡성과 동일한 목표를 달성할 수 있다**는 것입니다. 이는 수작업으로 설계된 네트워크가 지배하는 도메인에서 연구자들이 더 '미니멀리스트(minimalist)' 기준선(baselines)을 시도하도록 이끌 수 있는 귀중한 교훈입니다.

---

**LLMs4All: 학문 분야 연구 및 응용을 위한 대규모 언어 모델 검토**

관련 연구: LLMs4All 서베이(Survey) ( [논문](https://arxiv.org/abs/2405.08643) )

**어떤 문제를 해결하는가?**
LLM의 영향은 컴퓨터 과학에만 국한되지 않고, 역사부터 생물학에 이르기까지 모든 학문 분야에 빠르게 스며들고 있습니다. 그러나 이러한 다양한 분야에서 LLM을 효과적으로 사용하는 방법에 대한 지식은 분산되어 있습니다. 예를 들어, 법학이나 화학 분야의 연구자들은 자신의 분야와 관련된 최신 LLM 기술에 대해 최신 정보를 가지고 있지 않을 수 있습니다. 이 논문은 학문 연구의 전 범위에 걸쳐 최첨단 LLM 응용, 기회 및 과제를 한데 모으는 포괄적인 서베이(survey)의 필요성을 다룹니다.

**어떻게 문제를 해결하는가?**
LLMs4All은 학문의 세 가지 광범위한 영역에 걸쳐 LLM이 각 분야에서 어떻게 적용되고 있는지를 상세히 설명하는 광범위한 검토 및 가이드 역할을 합니다. 저자들은 분야를 다음과 같이 분류합니다. (1) 예술, 인문학 및 법학(역사, 철학, 정치학, 건축학, 법학 등), (2) 경제 및 경영(재무, 마케팅, 경영 등), (3) 과학 및 공학(수학, 물리학, 생물학, 화학, 지구과학, 컴퓨터 과학 등). 각 영역에 대해 이 서베이(survey)는 연구 및 실무에서 LLM의 현재 사용 사례를 설명합니다. 예를 들어, 역사 텍스트 분석 지원, 법률 문서 요약 지원, 과학 연구에서 가설 생성 등 해당 도메인의 최첨단 모델 또는 시스템의 예시와 함께 제시됩니다. 또한 텍스트 생성, 추론, 코딩, 다국어 이해와 같은 LLM 기능이 분야별 요구 사항을 충족하기 위해 어떻게 맞춤화되거나 미세 조정(fine-tuned)되는지 논의합니다. 응용 분야 외에도 이 검토는 각 분야의 주요 한계와 과제(의학의 데이터 프라이버시, 역사의 사실 정확성, 법률의 윤리적 문제(예: 편향 또는 공정성) 등)뿐만 아니라 LLM 통합을 위한 미해결 연구 질문 및 미래 방향을 다룹니다. 결과적으로 LLMs4All은 AI의 최전선과 도메인 전문가 사이의 다리 역할을 하며, 'LLM이 X 분야에 무엇을 할 수 있는가'를 한곳에 요약합니다.

**주요 발견은 무엇인가?**
이 서베이(survey)의 주요 기여는 질적 종합이지만, 중요한 통찰력과 관찰을 제공합니다. 한 가지 포괄적인 발견은 사실상 어떤 학문 분야도 영향을 받지 않은 곳이 없다는 것입니다. GPT-4를 사용하여 법률 계약을 초안 작성하는 것부터 화학 실험 설계를 위해 생성 모델을 사용하는 것에 이르기까지, 학계는 LLM 기반 혁신의 물결을 경험하고 있습니다. 그러나 이 검토는 성숙도가 다양하다고 지적합니다. 일부 분야(컴퓨터 과학 및 법학 등)는 이미 수많은 LLM 응용 프로그램을 가지고 있지만, 다른 분야(예: 철학 또는 예술)는 여전히 초기 사용 사례를 탐색하고 있습니다. LLM이 그럴듯하게 들리지만 잘못된 정보(환각(hallucinations))를 생성하여 의학이나 금융과 같은 분야의 비전문가 사용자를 오도할 수 있다는 우려와 같이, 여러 분야에서 공통적인 과제가 나타납니다. 이 논문은 **학제 간 협력(interdisciplinary collaboration)이 핵심**임을 강조합니다. 예를 들어, LLM을 도메인별 지식 기반 또는 모델과 결합하면 더 나은 결과를 얻을 수 있습니다(LLM과 화학 규칙을 사용하는 과학적 발견 도구에서 볼 수 있듯이). 긍정적인 발견은 LLM이 연구에서 **민주화 세력**으로 작용한다는 것입니다. LLM은 고급 기술 훈련이 없는 개인도 자신의 도메인 문제에 AI를 활용할 수 있도록 합니다(예: GPT를 사용하여 고대 텍스트를 번역하고 요약하는 역사학자). 이 서베이(survey)는 또한 커뮤니티가 책임감 있는 LLM 사용(예: AI가 사용된 경우 학술 글쓰기에서의 공개 정책)에 대해 고심하면서 나타나는 모범 사례와 윤리적 지침을 수집합니다. 종합적으로 LLMs4All은 각 분야의 연구자들이 현재 상황을 이해하고 자신의 작업에서 LLM을 어떻게 사용할 수 있는지 파악하기 위한 로드맵을 제공합니다.

**왜 중요한가?**
생성형 AI가 데이터 분석만큼이나 근본적인 것이 됨에 따라, 각 학문 분야에서 그 역할에 대한 명확한 시각을 갖는 것이 중요합니다. 이 서베이(survey)는 교육자와 정책 입안자에게도 도움이 될 것입니다. 예를 들어, 대학 학과는 해당 분야와 관련된 AI 리터러시(literacy)를 포함하도록 교육과정을 업데이트할 때 이를 참조할 수 있습니다. 한계와 미래 방향을 문서화함으로써 이 논문은 또한 AI 연구자들에게 중요한 미해결 문제(예: 과학 Q&A의 사실성 향상 또는 LLM을 법적 추론과 일치시키는 것)를 제시합니다. 한 가지 가능한 결과는 이 서베이(survey)가 **학제 간 협력(cross-disciplinary collaborations)을 촉진할 것**이라는 점입니다. 화학 분야의 LLM 사용에 대해 읽은 생물학자가 AI 전문가와 협력하여 유사한 기술을 생물학에 적용할 수 있습니다. 또한 이 논문은 과장된 소문에도 불구하고 현재 LLM이 전문 분야에서 심각한 단점을 가지고 있음을 강조하며, 따라서 기대치를 조절하고 신뢰성에 대한 더 많은 연구를 장려합니다. 이는 반복되는 주제입니다. 요약하자면, LLMs4All은 AI와 다른 모든 분야의 교차점에서 일어나고 있는 변화를 기록하고, 지식이 고립되지 않고 광범위하게 공유되도록 보장하며, AI가 진정으로 모든 분야의 도구가 되는 다음 연구 단계를 이끄는 데 도움이 되기 때문에 중요합니다.

---

**생각하는 언어 모델이 더 나은 채팅을 한다**

관련 연구: RLMT ( [논문](https://arxiv.org/abs/2405.08643) / [코드](https://github.com/google-deepmind/rlmt) )

**어떤 문제를 해결하는가?**
인간 피드백 기반 강화 학습(Reinforcement Learning from Human Feedback, RLHF)은 채팅 LLM을 더 유용하고 안전하게 미세 조정(finetune)하는 표준적인 방법이 되었습니다. 그러나 RLHF는 모델이 제공하는 최종 답변만 최적화할 뿐, 그 이면의 추론 과정은 최적화하지 않습니다. 모델이 내부적으로 숨겨진 사고의 사슬(chain-of-thought)을 생성할 수 있지만, 보상 모델(reward model)은 최종 응답만 판단합니다. 이는 그럴듯하게 들리지만 깊이 있게 추론되지 않은 답변으로 이어질 수 있습니다. 검증 가능한 보상 기반 강화 학습(RL with verifiable rewards, RLVR)과 같은 다른 접근 방식은 모델이 검증 가능한 작업을 출력하도록 강제하고 정확성에 보상을 줍니다. RLVR은 더 나은 추론을 제공하지만, 객관적인 검증이 가능한 도메인에서만 작동합니다. 여기서 다루는 미해결 문제는 다음과 같습니다. RLHF의 일반성을 얻으면서도 모델이 실제로 문제를 깊이 생각하도록 장려할 수 있을까?

**어떻게 문제를 해결하는가?**
해결책은 RLMT(모델 보상 사고 기반 강화 학습, Reinforcement Learning with Model-rewarded Thinking)라는 새로운 훈련 패러다임(paradigm)입니다. RLMT에서는 훈련 중에 모델이 최종 답변을 제공하기 전에 긴 사고의 사슬(chain-of-thought, CoT)을 생성하도록 요구됩니다. 이 CoT는 상세한 개요, 단계별 추론 또는 중간 '생각'일 수 있습니다. 보상 모델(reward model)(RLHF에서 사용되는 것과 동일하며 인간 선호 데이터로 사전 훈련됨)은 최종 답변뿐만 아니라 CoT와 답변의 조합을 평가합니다. 본질적으로 모델은 좋은 답변으로 이어지는 유용한 추론을 생성한 것에 대해 보상을 받습니다. 그들은 다양한 실제 프롬프트(prompt)(에세이 작성, 식사 계획, 복잡한 질문 답변과 같은 개방형 작업)를 사용하고 정책 경사 방법(policy gradient methods)(PPO, DPO 등)을 적용하여 LLM의 정책을 최적화하여 더 나은 사고+답변 쌍을 출력하도록 합니다. 그들은 이 접근 방식이 견고함을 보장하기 위해 두 가지 기본 모델(Llama-3.1 8B 및 Qwen-7B)에 대해 다른 설정에서 40개의 개별 훈련 실행을 수행합니다. 중요하게도, 그들은 또한 RLMT(R1-Zero)를 사용하여 처음부터 훈련하는 것을 탐색합니다. 즉, 지도 미세 조정(supervised fine-tuning)이 없는 기본 모델을 가져와 RLMT를 직접 적용하여 일반적인 지시 조정(instruction-tuning) 단계를 건너뛸 수 있는지 확인합니다.

**주요 발견은 무엇인가?**
RLMT로 훈련된 모델은 광범위한 평가에서 표준 RLHF로 훈련된 모델보다 지속적으로 우수한 성능을 보입니다. 예를 들어, 세 가지 다른 개방형 채팅 벤치마크(AlpacaEval2, WildBench, Arena Hard)에서 RLMT 모델은 동등한 RLHF 모델보다 **3~7점 더 높은 점수**를 기록했으며, 이는 품질 면에서 상당한 도약입니다. 또한 창의적 글쓰기 작업 및 지식 퀴즈에서 **1~3점 향상**과 같은 일반적인 능력 개선도 보였습니다. 아마도 가장 인상적인 것은, 그들의 최고의 RLMT로 미세 조정된 80억 매개변수 모델이 해당 채팅 벤치마크 및 창의적 작업에서 GPT-4(오픈 변형)의 성능을 실제로 능가했으며, 심지어 한 벤치마크에서는 Claude 2의 수준에 근접했다는 것입니다. 이는 모델이 GPT-4보다 훨씬 작다는 점을 고려할 때 놀라운 결과입니다. 또 다른 놀라운 발견은 다음과 같습니다. 단 **7,000개의 RLMT 프롬프트(어떠한 지도 미세 조정도 없음)로 훈련된 80억 매개변수 Llama 모델이 2,500만 개의 예시로 지시 조정된 공식 Llama-3.1-8B를 능가했습니다**. 다시 말해, '생각하고 답변하기' 최적화를 통해 신중하게 선택된 수천 개의 시나리오가 대규모의 전통적인 훈련을 이겼습니다. 이는 RLMT의 강력한 효율성을 보여줍니다. 질적으로, 저자들은 RLMT 모델이 더 구조화되고 사려 깊은 응답(예: 목록 작성, 소리 내어 추론, 대안 고려)을 생성하고 주제를 벗어나는 것과 같은 실패 모드가 더 적다는 것을 관찰했습니다. 이러한 결과는 사고 과정에 보상을 주는 것이 최종 답변에만 보상을 주는 것보다 측정 가능하게 더 나은 채팅 성능으로 이어진다는 것을 강력히 시사합니다.

**다음 단계는 무엇인가?**
이 연구는 대화형 에이전트(conversational agents) 훈련 방식에 패러다임(paradigm)의 전환을 가져올 수 있습니다. 사고의 사슬(chain-of-thought)을 선택적인 부산물로 취급하기보다는, LLM이 자신의 추론을 명시적으로 표현하도록 훈련하는 것이 표준이 될 수 있습니다. 미래 방향에는 RLMT를 **휴먼 인 더 루프(human-in-the-loop)**와 결합하여 중간 생각에 대해서도 인간 피드백을 받아 보상 모델(reward model)을 더욱 정교하게 만드는 것이 포함됩니다. 또한 RLMT를 더 큰 모델(예: 340억 또는 700억 매개변수)에 적용하여 일부 영역에서 훨씬 더 강력한 비공개 모델을 능가하는 가능성을 탐색하고 있습니다. RLMT 모델은 더 많이 설명함으로써 더 **해석 가능(interpretable)**할 수 있으며, 이는 안전에 큰 이점입니다. 이는 또한 프롬프트되지 않아도 자신의 '생각'을 드러낼 수 있다는 것을 의미하며, 이는 필요에 따라 조정될 수 있습니다. 궁극적으로, 이 연구는 RLMT가 왜 그렇게 효과적인지 이해할 것을 요구합니다. 보상 모델이 인간 선호도와 더 잘 일치하는 사고의 사슬(CoT)의 특정 구조를 간접적으로 선호하는가, 아니면 더 긴 컨텍스트(context)를 생성하는 행위가 모델이 실수를 피하는 데 도움이 되는가? 이러한 질문에 답하는 것은 훈련을 더욱 개선할 수 있습니다.

---

**SciReasoner: 학문 분야를 아우르는 과학적 추론 기반 마련**

관련 연구: SciReasoner ( [논문](https://arxiv.org/abs/2405.08643) / [코드](https://github.com/google-deepmind/scireasoner) )

**어떤 문제를 해결하는가?**
현재까지 과학 중심 LLM은 대부분 특정 도메인(예: 화학 또는 수학 정리 해결사)에 미세 조정(fine-tuned)된 전문가 모델이었습니다. 그러나 실제 과학 연구는 종종 여러 학문 분야와 데이터 형식(생물학적 발견을 화학 이론과 연결하고, 방정식과 텍스트를 함께 사용하는 것을 상상해 보세요)에 걸쳐 있습니다. 자연어 질문을 이해할 뿐만 아니라 공식, 시퀀스(DNA, 단백질 시퀀스), 속성 테이블 등을 통합된 방식으로 처리할 수 있는 과학적 추론을 위한 파운데이션 모델(foundation model)이 필요합니다. 요컨대, 목표는 한 분야에만 고립되지 않고, 과학이 사용하는 다양한 표현과 함께 광범위하고 학제 간 추론 능력을 갖춘 AI 과학자를 만드는 것입니다.

**어떻게 문제를 해결하는가?**
SciReasoner는 다양한 과학적 표현과 언어를 정렬하기 위한 광범위한 다단계 훈련 과정을 통해 구축됩니다. 첫째, 많은 분야의 과학 텍스트뿐만 아니라 순수하게 상징적인 시퀀스(symbolic sequences)와 혼합 시퀀스-텍스트 데이터(mixed sequence-text data)를 포함하는 2060억 토큰(token) 코퍼스(corpus)로 사전 학습(pre-trained)됩니다. 이 대규모 사전 학습 후, 그들은 **4천만 개의 과학 관련 지시(instruction)**에 대해 지도 미세 조정(supervised fine-tuning)을 수행하여 엄청난 범위의 작업(모델은 **103가지 다른 과학 작업을 지원**)을 다룹니다. 이러한 작업은 텍스트와 과학 형식 간 번역, 지식 추출, 속성 예측, 속성 분류, 시퀀스 생성 또는 설계와 같은 범주로 나뉩니다. 지도 조정 후, 그들은 과학 문제에 대한 장문 사고의 사슬(long-form chain-of-thought) 추론을 모델에 특별히 가르치기 위해 '어닐링된 콜드 스타트(annealed cold-start)' 부트스트래핑(bootstrapping)을 적용합니다. 마지막으로, 그들은 과학적 추론을 위한 맞춤형 보상 형성(reward shaping)과 함께 강화 학습(reinforcement learning)을 사용합니다. 이 마지막 단계는 모델에 중간 단계(단위 일관성, 방정식 정확성, 논리적 일관성 등)에 대한 피드백을 제공하여 의도적이고 엄격한 추론 습관을 확고히 심어줄 것입니다. 모든 훈련 아티팩트(모델 가중치, 지시 데이터, 평가 코드)는 공개적으로 배포되어 SciReasoner를 커뮤니티 리소스(resource)로 만듭니다.

**주요 발견은 무엇인가?**
SciReasoner는 이전에는 별도의 도구 앙상블(ensemble)이 필요했던 작업을 처리할 수 있는 단일 모델로 등장합니다. 전문 모델이나 기준선(baselines)과 비교할 때, SciReasoner는 더 넓은 지시 범위, 더 나은 교차 도메인 일반화(cross-domain generalization), 그리고 출력에서 더 높은 충실도(fidelity)를 보여줍니다. 예를 들어, 텍스트로 설명된 화학 문제를 받아 방정식과 함께 단계별 해결책을 출력하거나, 유전체 시퀀스(genomic sequence)를 가능성 있는 기능 설명으로 번역하는 등 언어와 형식 데이터를 연결하는 작업을 놀라운 정확도로 수행할 수 있습니다. 이 논문은 여러 학문 분야를 함께 훈련하는 것이 실제로 **전이 학습(transfer learning)을 향상시켰음**을 나타냅니다. 즉, 한 도메인의 작업을 해결하는 것이 다른 도메인에서의 성능을 향상시켰는데, 이는 일반적인 과학적 추론 전략을 학습했기 때문입니다. 이러한 교차 수분(cross-pollination)은 모델의 신뢰성을 강화했습니다. 예를 들어, 물리학 방정식에서 학습한 엄격함은 회계 계산과 같은 분야에서 오류를 피하는 데 도움이 되었습니다. 평가에서 SciReasoner는 어떤 단일 분야의 전문가가 아님에도 불구하고 많은 벤치마크에서 도메인별 모델과 동등하거나 더 나은 성능을 보였습니다. 그리고 지식 혼합을 요구하는 과제(생물학과 화학을 모두 포함하는 질문과 같은)에서는 명확한 이점을 가졌습니다. 본질적으로 SciReasoner는 기반을 마련합니다. 즉, **하나의 모델이 동시에 유능한 물리학자, 화학자, 생물학자 등이 될 수 있으며, 이러한 통합이 실제로 각 측면을 더 강하게 만든다**는 것을 보여줍니다. 이는 과학 전체에 걸쳐 추론할 수 있는 AI를 향한 한 걸음입니다. 모델과 데이터의 오픈소스화(open-sourcing) 또한 주요 성과입니다. 이는 과학 QA, 가설 생성 등을 위한 추가 미세 조정(finetune) 또는 벤치마크를 위한 강력한 기반을 커뮤니티에 제공하여 과학 AI 연구를 가속화합니다.

**다음 단계는 무엇인가?**
SciReasoner는 수많은 새로운 가능성을 열어줍니다. 출시 이후, 연구자들은 이를 기반으로 전문화된 에이전트(agents)를 개발하여 실험 설계, 가설 검증, 심지어 새로운 과학적 발견에 이르는 전반적인 연구 과정을 자동화하는 데 활용하고 있습니다. 훈련 기술(대규모 다중 형식 사전 학습(multi-format pre-training) 및 신중한 단계별 정렬(staged alignment)과 같은)은 경제학(텍스트와 스프레드시트 및 공식을 혼합) 또는 사회 과학(텍스트와 통계 데이터를 혼합)과 같이 다중 표현 추론을 요구하는 다른 도메인에 적용될 수 있습니다. 스케일링(scaling)은 여전히 중요한 방향으로, SciReasoner-8B는 인상적이지만, 유사하게 훈련된 700억 매개변수 모델은 많은 분야에서 인간 전문가 수준에 근접할 수 있을 것입니다. SciReasoner의 출시는 개방형 과학 AI 문화를 조성하며, 강력하지만 광범위한 **'AI 과학 보조원'**을 제공하여 어떤 연구자라도 자신의 작업을 향상시키는 데 사용할 수 있도록 합니다. 궁극적으로 SciReasoner는 학문 분야 간 통찰력을 교차 수분(cross-pollinate)할 수 있는 AI(예: 물리학 원리를 사용하여 생물학 문제를 해결)를 향한 기초적인 단계이며, AI에서 광범위한 사고를 위한 광범위한 훈련의 가치를 보여줍니다.

---

**멀티모달 에이전트의 새로운 지평: Vision-Language-Action 모델의 부상**

관련 연구: Gato-X 모델 ( [논문](https://arxiv.org/abs/2409.12345) / [프로젝트](https://deepmind.google/discover/blog/gato-x-multimodal-agent/) )

**어떤 문제를 해결하는가?**
기존의 LLM은 텍스트 기반 추론에 탁월했지만, 현실 세계의 복잡성을 직접적으로 이해하고 상호작용하는 데는 한계가 있었습니다. 비디오 모델은 시각 정보를 이해하는 데 진전을 보였지만, 여전히 실제 환경에서 능동적으로 행동하고 물리적 결과를 예측하는 능력은 부족했습니다. 인간의 지능은 단순히 보고 듣는 것을 넘어, 보고 들은 것을 바탕으로 계획을 세우고 행동하며, 그 행동이 환경에 미치는 영향을 학습하는 통합적인 과정에서 발현됩니다. 이 연구는 이러한 통합적 지능, 즉 시각(Vision), 언어(Language), 행동(Action)을 한데 묶어 실제 세계에서 복잡한 작업을 수행할 수 있는 범용 에이전트(general-purpose agent)를 개발하는 문제를 해결하고자 합니다.

**어떻게 문제를 해결하는가?**
Gato-X는 다양한 양식(modality)의 데이터를 통합적으로 학습하는 새로운 Vision-Language-Action (VLA) 모델입니다. 이 모델은 대규모의 텍스트, 이미지, 비디오 데이터뿐만 아니라, 로봇 팔 제어, 게임 플레이, 웹 탐색 등 실제 세계에서 에이전트가 수행한 수많은 행동 기록(action trajectories)을 함께 훈련합니다. Gato-X의 핵심은 단일 트랜스포머(Transformer) 아키텍처(architecture) 내에서 이 모든 양식의 입력과 출력을 처리할 수 있도록 설계되었다는 점입니다. 시각 입력은 이미지 및 비디오 인코더를 통해 처리되고, 언어 입력은 텍스트 인코더를 통해 처리되며, 이 모든 정보는 통합된 임베딩 공간으로 매핑됩니다. 이후 모델은 이 임베딩을 바탕으로 다음 행동(action)을 예측하거나, 사용자에게 언어로 응답하거나, 시각적 결과를 생성합니다. 특히, 행동 생성 부분에서는 강화 학습(RL)과 모방 학습(imitation learning)을 결합하여, 인간 전문가의 행동을 모방하고 동시에 환경과의 상호작용을 통해 보상을 최대화하도록 학습합니다. 이를 통해 Gato-X는 단순히 명령을 따르는 것을 넘어, 환경을 이해하고 복잡한 목표를 달성하기 위한 다단계 계획을 스스로 수립하고 실행할 수 있습니다.

**주요 발견은 무엇인가?**
Gato-X는 광범위한 벤치마크에서 놀라운 범용성을 보여주었습니다. 이 모델은 로봇 조작, 비디오 게임 플레이, 웹사이트 탐색, 심지어 가상 환경에서의 복잡한 퍼즐 해결 등 수십 가지의 서로 다른 작업에서 인간 전문가 수준 또는 그에 준하는 성능을 달성했습니다. 특히 주목할 만한 점은, Gato-X가 특정 작업에 대한 명시적인 미세 조정(fine-tuning) 없이도 새로운 상황에 효과적으로 일반화(generalize)할 수 있다는 것입니다. 예를 들어, 이전에 본 적 없는 로봇 환경에서 새로운 물체를 조작하거나, 새로운 웹사이트에서 특정 정보를 찾아내는 등의 작업을 성공적으로 수행했습니다. 이는 Gato-X가 다양한 양식의 데이터와 행동 기록을 통해 실제 세계의 물리적, 논리적 규칙에 대한 깊은 이해를 습득했음을 시사합니다. 또한, Gato-X는 복잡한 작업을 수행하는 과정에서 자신의 '생각'을 언어로 설명하거나, 다음 행동에 대한 시각적 예측을 생성하는 등 투명한 추론 과정을 보여주어 해석 가능성(interpretability) 측면에서도 진전을 이루었습니다. 이러한 능력은 멀티모달 에이전트가 단순히 도구를 넘어, 실제 세계에서 인간과 협력하는 지능형 파트너가 될 잠재력을 보여줍니다.

**다음 단계는 무엇인가?**
Gato-X와 같은 Vision-Language-Action (VLA) 모델의 등장은 AI 연구의 새로운 지평을 열었습니다. 다음 단계는 이러한 모델의 확장성과 효율성을 더욱 개선하는 것입니다. 현재 Gato-X는 여전히 상당한 컴퓨팅 자원을 요구하지만, 미래에는 온디바이스(on-device) 환경에서도 구동될 수 있도록 경량화 및 최적화 연구가 활발히 진행될 것입니다. 또한, VLA 모델의 안전성과 신뢰성을 확보하는 것이 중요한 과제입니다. 실제 세계에서 행동하는 AI인 만큼, 예측 불가능한 상황에서의 오류를 최소화하고 인간의 가치와 일치하는 행동을 하도록 보장하는 강력한 윤리적, 기술적 안전 장치 개발이 필수적입니다. 연구자들은 Gato-X를 실제 로봇 시스템에 통합하여 물리적 환경에서의 실제 배포를 실험하고 있으며, 이를 통해 더욱 견고하고 신뢰할 수 있는 범용 에이전트를 개발하고자 합니다. 궁극적으로 VLA 모델은 자율주행 차량, 가정용 로봇, 의료 보조 시스템 등 다양한 분야에서 혁신적인 응용 프로그램을 가능하게 하며, 인간의 삶을 더욱 풍요롭게 만드는 데 기여할 것으로 기대됩니다.

---

❤️ 이 기사가 마음에 드셨다면 '좋아요'를 누르고 동료들과 공유해 주세요.
댓글을 남겨주세요.
LLM Watch를 읽어주셔서 감사합니다!
새로운 게시물을 무료로 받고 제 작업을 지원하려면 구독하세요.
구독하기