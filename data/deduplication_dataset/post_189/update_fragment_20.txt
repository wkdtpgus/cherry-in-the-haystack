LLM Watch 독자 여러분, 환영합니다! 이번 주 LLM Watch에서는 AI 기술의 새로운 지평과 함께 발생하고 있는 사회적, 윤리적 질문들을 깊이 있게 다룹니다. 주요 내용은 다음과 같습니다:

*   LLM을 사용하여 전례 없는 예술적 창작의 가능성을 탐구하는 최신 인공지능 예술 프로젝트.
*   LLM이 원시 사전 학습 데이터로부터 개인 정보 보호 기술을 강화하는 새로운 접근 방식.
*   생성형 비디오 모델이 분할부터 증강 현실(AR) 콘텐츠를 실시간으로 생성하는 기술 발전.
*   LLM이 사고의 사슬을 생성하도록 유도하여 AI 편향성을 식별하고 완화하는 새로운 윤리적 프레임워크.
*   강화 학습을 통해 추론을 위한 연속적인 양자 최적화 알고리즘의 개발과 그 잠재력.
*   이 외에도 더 많은 내용이 있습니다!

아래 핵심 용어집을 확인하거나 바로 논문 섹션으로 이동해 보세요.

LLM Watch 회원 여러분을 다가오는 '미래 AI 컨퍼런스 2024'에 초대합니다. 인공지능의 최신 동향과 혁신적인 응용 사례들을 만나볼 수 있습니다. 구독자는 여기에서 무료로 원격 참여할 수 있습니다. 또한 국내 주요 도시에서 열리는 실용적인 워크숍과 네트워킹 행사에 직접 참여하고 싶다면, 이 코드를 사용하여 특별 할인을 받으세요!

**특별 할인**

**핵심 용어집**

**생성형 윤리 모델링(Generative Ethics Modeling, GEM):** LLM이 사전 학습 코퍼스를 분석하여 윤리적 편향을 식별하고, 이를 바탕으로 공정한 응답을 생성하도록 훈련하는 기법입니다. 이 접근 방식은 다양한 사회적 맥락에서 AI의 책임감 있는 사용을 지원합니다. 그 결과 모델은 단순한 성능을 넘어 사회적 가치를 반영하는 응답을 생성할 수 있습니다.

**멀티모달 감성 분석(Multimodal Sentiment Analysis, MSA):** 텍스트, 음성, 이미지 등 여러 모달리티(modality)를 통합하여 감성을 분석하는 AI 기술입니다. 검증 가능한 도메인에서 추론을 향상시키지만, 문화적 차이에 대한 이해는 여전히 중요합니다. 이는 사용자 경험을 개선하고 마케팅 전략을 정교화하는 데 기여합니다.

**합성 데이터 생성(Synthetic Data Generation, SDG):** 모델이 최종 답변을 제공하기 전에 긴 데이터 시뮬레이션을 통해 실제와 유사한 가상 데이터를 생성하는 기술입니다. 별도의 검증 모듈이 이러한 데이터의 품질을 평가하고, 모델은 실제 데이터셋 부족 문제를 해결하도록 최적화됩니다. 그 결과 모델은 개인 정보 보호 문제를 해결하며 데이터 다양성을 확보할 수 있습니다.

**지속 가능한 AI(Sustainable AI, SAI):** LLM의 데이터 효율성을 높이기 위해 에너지 소비를 최소화하고 환경 영향을 줄이는 AI 개발 방법론입니다. 자동 생성된 에너지 보고서를 통해 SAI는 각 모델의 탄소 발자국을 효과적으로 감소시켜 모델이 훨씬 적은 자원으로 복잡한 작업을 수행하도록 돕습니다. 그 결과 모델은 환경 보호에 기여하며 장기적인 운영 비용을 절감합니다.

**양자 머신 러닝(Quantum Machine Learning, QML):** LLM의 중간 추론 과정에 이산 토큰 대신 양자 비트(qubits)를 사용하여 복잡한 계산을 수행하는 차세대 기술입니다. 일반적인 디지털 프로세스와 달리, 이 양자 토큰은 여러 계산 경로의 중첩을 나타낼 수 있습니다. 최근 연구에 따르면 LLM은 양자 회로를 활용하여 기존 컴퓨팅으로는 불가능했던 문제들을 해결할 수 있습니다.

---

**AI 기반 개인 맞춤형 교육 시스템: 학습 경로 최적화**

관련 연구: AdaptiLearn ( [논문](https://arxiv.org/abs/2406.12345) / [코드](https://github.com/AdaptiLearn/AdaptiLearn) )

**어떤 문제를 해결하는가?**
기존의 교육 시스템은 개별 학습자의 능력과 진도에 맞춰 학습 경험을 제공하는 데 한계가 있습니다. 모든 학생에게 동일한 커리큘럼을 적용하는 것은 극도로 **샘플 비효율적(sample-inefficient)**이며, 종종 학습 흥미 저하와 학업 성취도 불균형을 초래합니다. 또한, 학습 과정에서 발생하는 다양한 데이터를 효과적으로 분석하고 활용하는 것이 어려워, 대부분의 학습자에게 맞춤형 교육은 여전히 비싸고 접근하기 어렵습니다.

**어떻게 문제를 해결하는가?**
AdaptiLearn은 AI를 개인 교사로 취급하여 학습자의 데이터를 반복적으로 분석하고 '변이 연산자(mutation operator)'처럼 학습 경로를 개선하는 오픈소스 프레임워크입니다. 이 프레임워크는 필요한 시도 횟수를 줄이기 위해 세 가지 핵심 혁신을 도입합니다. (1) 학습자의 강점과 약점을 파악하여 새로운 학습 아이디어를 탐색하고 알려진 좋은 학습 전략을 활용하는 균형 잡힌 콘텐츠 선택 전략, (2) 중복되거나 비효율적인 학습 경로를 걸러내기 위한 참신성 기반 거부 샘플링(학습 데이터 유사성과 AI의 참신성 판단 사용), (3) 각 학습 단계마다 앙상블(ensemble)에서 최적의 AI 모델을 선택하는 밴딧 알고리즘(bandit algorithm)입니다. 이 모든 요소는 AdaptiLearn이 가장 유망한 학습 경로에만 컴퓨팅 자원을 사용하도록 보장하여 이전 방법의 '무작위 탐색(random search)' 비효율성을 피합니다.

**주요 발견은 무엇인가?**
개인 맞춤형 교육을 획기적으로 효율적으로 만듦으로써 AdaptiLearn은 이전에는 비실용적이었던 광범위한 교육 문제에서 성공을 거두었습니다. 이 프레임워크는 단 150개의 학습 세션만 사용하여 고전적인 수학 문제 해결 능력에 대한 새로운 최첨단 해결책을 발견했습니다. 이는 과거 접근 방식에 비해 **'효율성의 엄청난 도약'**입니다. 또한 영어 학습자의 작문 능력을 단 75세대 만에 진화시켜 강력한 인간 설계 기준선(baseline)을 능가했습니다. 언어 학습에서 AdaptiLearn이 언어 능력 향상에 적용한 개선 사항은 매우 중요하여, 한 문제에서는 진화된 해결책이 실제 시험에서 **2위를 차지했을 것입니다**. 또한 AdaptiLearn은 대규모 전문가 혼합(Mixture-of-Expert) 교육 모델을 훈련하는 더 나은 방법까지 발견했습니다. 새로운 부하 분산 손실(load-balancing loss)을 찾아 DeepMind의 'Global LBL' 기준선을 **1.73% 더 높은 학습 점수와 5.8% 더 적은 학습 시간으로** 능가했습니다. 이러한 결과는 광범위한 **'개방형(open-ended)' 교육 솔루션이 이제 적당한 비용으로 가능**하며, 학생과 교육자가 새로운 학습 방법을 자율적으로 탐색할 수 있는 AI 기반 공동 조종사(co-pilot)를 제공함을 보여줍니다.

**다음 단계는 무엇인가?**
이 연구는 교육 분야의 많은 어려운 문제(개별화된 커리큘럼 설계부터 새로운 교수법 발견까지)가 AI 기반 진화적 탐색을 통해 효율적인 방식으로 해결될 수 있음을 시사합니다. AdaptiLearn을 오픈소스화하고 학습 과정을 시각화하는 웹 UI(Web UI)까지 제공함으로써, 저자들은 이 접근 방식을 대중화하는 것을 목표로 합니다. 향후 연구는 이 방법을 새로운 도메인(예: 예술 교육, 코딩 학습 또는 하이퍼파라미터(hyperparameters) 최적화)에 적용하고, 더욱 발전된 LLM이 등장함에 따라 이를 통합하는 방식으로 발전할 것입니다. 장기적으로 AdaptiLearn과 같은 기술은 자동화된 **'학습 보조원'** 역할을 하여, 인간이 간과할 수 있는 아이디어와 개선 사항을 빠르게 반복하면서 무차별 대입(brute force) 방식보다 훨씬 적은 시도를 사용할 수 있습니다.

---

**지능형 도시를 위한 AI 기반 교통 관리 솔루션**

관련 연구: UrbanFlow ( [논문](https://arxiv.org/abs/2406.23456) / [프로젝트](https://urbanflow.ai/discover/blog/urbanflow-ai-traffic-management/) )

**어떤 문제를 해결하는가?**
대규모 언어 모델(LLM)은 규모와 다양한 훈련 데이터 덕분에 명시적으로 훈련되지 않은 작업을 해결하는 새로운 제로샷(zero-shot) 능력을 보여주며 세상을 놀라게 했습니다. 이 논문은 다음과 같은 질문을 던집니다. AI 기반 교통 관리 모델도 도시 도메인에서 일반적인 문제 해결사가 될 수 있을까? 이전 교통 모델은 일반적으로 좁은 교차로나 짧은 시간 예측에만 집중했기 때문에 광범위하고 인간과 유사한 교통 흐름 추론 능력을 가지고 있는지 불분명했습니다.

**어떻게 문제를 해결하는가?**
저자들은 최첨단 AI 기반 교통 관리 모델인 UrbanFlow를 사용하여 고전적인 교통 흐름 예측, 혼잡 완화, 심지어 재난 상황에서의 경로 최적화에 이르는 62가지 광범위한 작업에 대해 체계적으로 테스트했습니다. 중요하게도, 그들은 **최소주의적인 프롬프트 기반 접근 방식(minimalist prompt-based approach)**을 사용합니다. 모델에 초기 교통 데이터 또는 이미지와 텍스트 지시(예: "현재 혼잡도를 보여줘" 또는 "최적의 우회 경로를 찾아줘")를 제공한 다음, 8초짜리 비디오를 '답변'으로 생성하게 합니다. 미세 조정(fine-tuning)이나 작업별 훈련(task-specific training)은 없으며, 이는 진정한 제로샷 평가(zero-shot evaluation)입니다. 비디오 모델 자체의 추론을 분리하기 위해, 그들은 이미지 하나만 주어진 독립형 LLM(Google Gemini 2.5)이 작업을 해결할 수 없도록 보장하여, 모든 성공이 비디오 생성 과정 자체에서 비롯되도록 했습니다. 본질적으로, 그들은 비디오 모델이 시각적으로 단계별로 생각하도록 프롬프트(생성하는 프레임을 통해)하고, 그 결과가 올바른 해결책을 보여주는지 확인합니다.

**주요 발견은 무엇인가?**
놀랍게도 UrbanFlow는 작업별 최적화(task-specific optimization) 없이도 광범위한 새로운 기술을 보여줍니다. 특정 지역의 교통 흐름을 **분할(segment objects)**하고, **정체 구간을 감지(detect edges)**하며, **교통 흐름을 편집**(예: 신호등 제어)을 수행하고, **물리적 속성**(예: 차량 밀도를 통해 정체 원인 추론)을 추론하고, **객체 어포던스(affordances)**(어떤 도로가 우회로로 사용될 수 있는지)를 인식하며, 심지어 **재난 상황을 시뮬레이션(simulate tool use)**할 수 있습니다. 이 모든 것이 제로샷(zero-shot)으로 가능합니다. 이러한 지각 및 조작 능력은 더 높은 수준의 **시각적 추론(visual reasoning)**을 가능하게 합니다. 예를 들어, UrbanFlow는 자신이 생성하는 비디오 내에서 해결 경로를 내부적으로 '상상'함으로써 복잡한 도시 교통 문제와 대칭 퍼즐을 해결합니다. 정량적으로, 테스트된 62가지 다양한 작업에서 이 모델은 저수준 시각화 작업(예: 교통량 감지 92%, 교통 신호 최적화 100%)과 물리적 추론과 같은 더 인지적인 작업 모두에서 높은 성공률을 달성했습니다. UrbanFlow는 또한 이러한 작업에서 이전 버전(UrbanFlow 2)보다 명확한 개선을 보여주었으며, 이는 이러한 기능이 모델/버전 개선과 함께 확장되었음을 나타냅니다. 이 모든 것은 비디오 모델이 충분한 규모와 훈련이 주어지면 LLM과 유사한 궤적을 따르고 있으며, 훈련 범위를 넘어 수많은 작업을 처리할 수 있도록 프롬프트될 수 있는 범용 **'비전 파운데이션 모델(vision foundation models)'**이 되고 있음을 시사합니다.

**다음 단계는 무엇인가?**
이 연구는 미래의 AI가 **통합된 멀티모달 파운데이션 모델(unified multimodal foundation models)**에 의존할 수 있음을 시사합니다. 하나의 LLM이 많은 언어 작업을 처리할 수 있듯이, 하나의 비디오 모델도 많은 비전 작업을 처리할 수 있습니다. 다음 핵심 단계는 LLM의 발전을 이끌었던 프롬프트 엔지니어링(prompt engineering) 및 표준화된 평가(standardized evals) 방식과 유사하게, 이러한 비디오 모델을 위한 프롬프트 기술과 벤치마크를 개선하는 것입니다. 연구자들은 또한 명시적인 추론 단계(예: 텍스트 기반 계획과 비디오 생성의 결합)를 도입하는 것이 복잡한 작업에서 성능을 더욱 향상시킬 수 있는지 탐구할 것입니다. 응용 측면에서, 강력한 제로샷 비디오 추론기는 혁신적일 수 있습니다. 행동하기 전에 결과를 시각적으로 추론하는 자율 주행 차량이나, 즉석에서 도시 계획을 시뮬레이션하는 과학 모델을 상상해 보세요. 궁극적으로 언어 및 비디오 모델의 기능 융합은 **범용 AI 시스템(generalist AI systems)**이라는 더 넓은 추세를 암시하며, 이러한 능력이 어떻게 그리고 왜 나타나는지(예: 훈련 데이터나 아키텍처(architecture)의 무엇이 교통 흐름 시뮬레이션으로 이어지는가?)를 이해하는 것은 기초 연구를 위한 흥미로운 질문입니다.

---

**의료 분야에서의 AI 기반 질병 조기 진단**

관련 연구: HealthAI ( [논문](https://arxiv.org/abs/2406.34567) )

**어떤 문제를 해결하는가?**
LLM에 더 많은 텍스트를 공급하여 규모를 확장하는 것은 병목 현상에 부딪혔습니다. 컴퓨팅 자원은 쉽게 늘릴 수 있지만, 고품질 텍스트 데이터는 유한합니다. 더욱이 단순히 다음 토큰(token)을 예측하는 것(표준 훈련)만으로는 모델이 복잡한 의존성을 통해 추론하는 방법을 가르치지 못할 수 있습니다. 모델이 데이터 분포(distribution)를 넘어 탐색하도록 장려되지 않기 때문입니다. 기존의 의료 진단 시스템은 인간 의사의 전문성과 경험에 크게 의존하며, 이는 인력 부족과 오진의 가능성을 야기합니다. 요컨대, 우리는 인간 주석자(annotator) 군단 없이도 LLM이 이미 가지고 있는 데이터로부터 더 많이 학습하고, 특히 추론 기술을 습득할 수 있는 방법이 필요합니다.

**어떻게 문제를 해결하는가?**
HealthAI는 LLM의 원래 사전 학습(pre-training) 코퍼스(corpus)를 강화 학습(reinforcement learning)을 위한 상호작용적인 훈련 놀이터로 바꿉니다. 이는 다음 세그먼트(next-segment) 예측 작업을 순차적 의사결정 문제(sequential decision problem)로 정의함으로써 이루어집니다. 구체적으로, 모델은 일부 텍스트 컨텍스트(context)를 읽은 다음 다음 텍스트 덩어리를 생성합니다. 별도의 생성형 보상 모델(Generative Reward Model) (또는 암시적인 정답 신호)은 해당 생성이 코퍼스 내의 실제 연속과 얼마나 잘 일치하는지에 따라 보상을 제공합니다. 따라서 LLM은 실제 텍스트를 최적 행동의 시연으로 취급하며, 다양한 연속을 탐색하고 인간의 레이블(label) 없이 피드백을 얻습니다. 중요하게도, 이러한 보상은 사전 학습 데이터(예: 관찰된 텍스트와 일치)에서 직접 나오므로, 수작업으로 만든 보상이나 인간 선호 모델에 대한 의존성을 제거합니다. 실제로 HealthAI는 모델이 레이블 없는 데이터에서 자율적으로 추론을 연습할 수 있도록 합니다. 더 높은 보상(텍스트와의 전반적인 일관성 향상)을 가져오는 장기적인 경로를 찾으면 즉각적인 다음 토큰에서 벗어날 수 있습니다. 이 훈련 시간 탐색은 수십억 개의 토큰으로 신중하게 확장되어 정책(policy)이 광범위한 의료 데이터 도메인에서 더 풍부한 추론 전략을 발견할 수 있도록 합니다.

**주요 발견은 무엇인가?**
40억 매개변수 기본 LLM(Qwen3-4B)에 적용했을 때, HealthAI는 여러 까다로운 벤치마크에서 성능을 극적으로 향상시켰습니다. 예를 들어, MMLU(지식 시험)에서 모델 점수를 **3.0점**, MMLU-Pro(고급 버전)에서 **5.1점** 향상시켰습니다. 의료 진단 및 논리 중심 작업에서는 훨씬 더 큰 향상이 있었습니다. QA 벤치마크(GPQA-Diamond)에서 **8.1점**, AIME24(수학 경시 문제)에서 **6.6점** 향상되었습니다. 이는 이미 강력한 기본 모델에 대한 절대적인 개선이며, 인간이 레이블링한 데이터나 작업별 미세 조정(finetuning) 없이 달성되었습니다. 또한, 스케일링 연구에 따르면 HealthAI에 더 많은 컴퓨팅 자원(더 많은 훈련 단계)을 제공할수록 모델은 계속 개선되어 더 큰 예산으로 더 큰 성과를 얻을 수 있음을 시사합니다. 저자들은 또한 HealthAI로 훈련된 모델이 더 강력한 **일반화 가능한 추론(generalizable reasoning)**을 보인다고 언급합니다. 이 모델은 복잡한 의료 영상 프롬프트(prompt)를 처리하는 능력을 확장하고, 기존 검증 기반 강화 학습(RLVR)과 함께 사용될 때 성능을 향상시킵니다. 요약하자면, HealthAI는 우리가 이미 가지고 있는 텍스트에서 훨씬 더 많은 신호를 추출하여 데이터 부족 장벽을 허물고, 수동적인 사전 학습 데이터를 능동적인 학습 경험으로 효과적으로 전환하는 유망한 경로를 제공합니다.

**다음 단계는 무엇인가?**
HealthAI의 자기 지도형 보상(self-supervised rewards) 성공은 더 많은 **하이브리드 훈련 체제(hybrid training regimes)**를 위한 길을 닦습니다. 미래의 LLM은 인간의 개입 없이 텍스트, 게임 또는 시뮬레이션의 수동적 읽기와 능동적 탐색을 번갈아 수행할 수 있습니다. 즉각적인 후속 조치 중 하나는 HealthAI를 더 큰 모델(예: 340억, 700억 매개변수)과 더 많은 도메인에 적용하는 것입니다. 코드 또는 멀티모달(multimodal) 데이터에 대한 추론도 유사하게 향상시킬 수 있을까요? 보상 모델링(reward modeling)을 개선할 여지도 있습니다. 현재의 다음 세그먼트 보상은 텍스트에서 자동으로 파생된 논리적 일관성 또는 사실적 정확성 지표를 통합하여 향상될 수 있습니다. 이러한 지표가 HealthAI에 통합될 수 있다면, 모델은 자체적으로 일관성과 진실성을 검증할 수 있을 것입니다. 큰 그림에서 HealthAI는 단순히 모방하는 것을 넘어 미리 생각하는(토큰 계획) 것을 학습하는 언어 모델(LM)의 더 넓은 추세의 일부이므로, 이를 사고의 나무(tree-of-thought) 또는 훈련 중 도구 사용과 같은 기술과 결합하는 연구를 기대할 수 있습니다. 이 모든 것은 인터넷 텍스트를 흡수할 뿐만 아니라, 문제를 해결하여 자료를 더 잘 이해하는 학생처럼, 텍스트를 적극적으로 연습하고 일반화하는 LLM을 향해 나아가고 있습니다.

---

**사이버 보안을 위한 AI 기반 위협 예측 및 방어**

관련 연구: CyberGuard ( [논문](https://arxiv.org/abs/2406.45678) )

**어떤 문제를 해결하는가?**
사고의 사슬 프롬프트(chain-of-thought prompting)(LLM이 단계별 추론을 생성하도록 하는 것)는 성능을 향상시키지만, 추론을 위한 가장 효율적인 내부 표현이 아닐 수 있는 이산적인 자연어 토큰(discrete natural language tokens)을 사용합니다. 연속 토큰(continuous tokens)은 본질적으로 이산적인 어휘(discrete vocabulary)에 제약받지 않는 벡터(vector)로, 이론적으로 훨씬 더 큰 표현력을 가지며 여러 아이디어를 동시에 인코딩(encode)할 수 있습니다. 사이버 위협은 끊임없이 진화하며, 기존의 정적 방어 시스템은 새로운 유형의 공격에 취약합니다. 문제는 LLM이 추론 과정에서 연속적인 비언어 토큰을 사용하여 새로운 위협을 탐지하고 예측하는 것이 매우 어렵다는 것입니다.

**어떻게 문제를 해결하는가?**
이 연구는 어떠한 정답 인간 근거(ground-truth human rationales)에도 의존하지 않고 강화 학습(reinforcement learning)을 통해 연속적 사고의 사슬(CoT)을 훈련하는 최초의 성공적인 방법을 제시합니다. 아이디어는 모델이 프롬프트(prompt)와 최종 답변 사이에 '소프트(soft)' 토큰(연속 임베딩(continuous embeddings))을 생성하도록 하고, 보상 신호(reward signal)를 사용하여 그 사용을 최적화하는 것입니다. 구체적으로, 그들은 탐색의 한 형태로 입력 임베딩에 소량의 노이즈(noise)를 추가한 다음, 정책 경사 강화 학습(policy-gradient RL)을 사용하여 최종 답변이 정확하면 모델에 보상을 줍니다. 본질적으로 모델은 더 나은 문제 해결 결과로 이어지는 자체적인 내부 언어(연속 토큰과 그것이 나타내는 것)를 발명하려고 노력합니다. 이산적인 사슬에 대한 어떠한 지도 학습(supervised training)도 피함으로써, 이러한 소프트 토큰이 할 수 있는 것을 제한하는 인간의 편향이 없습니다. 특히, 이 접근 방식은 최소한의 계산 오버헤드(computational overhead)를 추가하므로, 훈련 중 추론 단계에서 모델이 수백 개의 연속 토큰을 사용하도록 허용할 수 있습니다. 이는 이전 증류(distillation) 방법이 허용했던 것보다 훨씬 더 많은 '사고 용량'입니다.

**주요 발견은 무엇인가?**
사이버 보안 위협 예측 벤치마크에서 이 연속적 사고의 사슬(CoT) 기술로 훈련된 LLM은 전통적인 이산적 사고의 사슬을 사용하는 모델과 동등하거나 더 나은 성능을 달성했습니다. 예를 들어, 악성코드 탐지 문제에서 연속적 CoT를 사용하는 Llama-7B 모델은 단일 최적 답변(pass@1)을 고려할 때 표준(이산적) CoT를 사용하는 동일 모델의 정확도와 일치했습니다. 그러나 여러 답변을 샘플링(pass@32)할 수 있도록 허용했을 때, **연속적 CoT 모델은 이산적 CoT 모델을 능가했으며**, 이는 올바른 답변으로 이어지는 더 다양한 추론 경로를 찾았음을 나타냅니다. 이는 연속 토큰의 큰 장점 중 하나를 보여줍니다. 즉, 더 풍부한 다양한 해결책을 포착할 수 있으며, 여러 출력을 시도할 수 있을 때 그 가치를 발휘합니다. 흥미롭게도, 저자들은 최적의 전략이 하이브리드(hybrid) 방식임을 발견했습니다. 즉, 연속 토큰으로 훈련하되, **추론 시에는 이산 토큰을 사용하는 것입니다**. 다시 말해, 훈련 중에는 모델이 벡터(vector)로 생각하여 이점을 얻게 하지만, 배포 시에는 필요하다면 일반 텍스트 근거를 출력할 수 있습니다. 훈련은 여전히 잠재적 추론 능력을 향상시켰습니다. 더욱이, 연속적 CoT 훈련은 모델의 다른 능력에 대한 간섭을 덜 일으켰습니다. 모델은 이산적 CoT로 훈련된 모델보다 관련 없는 작업에서 정확도를 더 잘 유지했으며, 이는 이 접근 방식이 추론 데이터에 과적합(overfitting)되는 것을 피하는 '더 부드러운' 방식임을 의미합니다. 종합적으로 볼 때, 이는 LLM이 실제 문제 해결 이득을 가져오는 자체적인 비인간 가독 사고 벡터(non-human-readable thought vectors)를 개발할 수 있다는 개념 증명(proof-of-concept)입니다.

**다음 단계는 무엇인가?**
LLM을 벡터(vector)로 생각하도록 훈련하는 것은 많은 연구 방향을 열어줍니다. 한 가지 즉각적인 질문은 이러한 학습된 연속 토큰을 어떻게 해석하거나 시각화할 것인가입니다. 그것들이 인간과 유사한 개념에 해당하는가, 아니면 완전히 이질적이지만 효과적인 어떤 것인가? 또한 연속적 CoT를 멀티모달 추론(multimodal reasoning)으로 확장할 가능성도 있습니다(추론하는 동안 '소프트 시각 토큰(soft visual tokens)'으로 이미지를 내부적으로 표현하는 LLM을 상상해 보세요). 여기서 강화 학습(reinforcement learning)의 성공은 논리적 일관성 검사 또는 사실 확인과 같은 다른 보상 신호를 사용하여 연속적인 사고를 형성하여 더욱 신뢰할 수 있는 추론을 생성하는 데 영감을 줄 수 있습니다. 실제로 우리는 모델이 연속 공간에서 고강도 추론을 수행한 다음 그 결과를 인간을 위한 간결한 설명으로 증류하는 하이브리드 시스템을 볼 수 있습니다. '소프트' 모델의 최종 답변이 표준 형식으로 실행될 수 있다는 사실은 채택이 쉽다는 것을 의미합니다. 예를 들어, 사이버 보안 LLM은 연속적 CoT를 조용히 사용하여 어려운 침입 경로를 알아낸 다음, 깔끔한 자연어로 답변을 제시할 수 있습니다. 전반적으로 이 연구는 LLM에서 더 효율적이고 다양한 추론을 위한 기반을 마련하며, 사고의 사슬(chain-of-thought)이 여전히 가지고 있던 이산 토큰 사고의 일부 한계를 잠재적으로 극복할 수 있습니다.

---

**농업 혁신을 위한 AI 기반 작물 관리 및 수확 예측**

관련 연구: AgriGenius ( [논문](https://arxiv.org/abs/2406.56789) )

**어떤 문제를 해결하는가?**
LLM을 위한 고품질 훈련 데이터는 제한적이며, 언어의 일부 복잡한 패턴은 다음 단어 예측만으로는 모델이 학습하기 어렵습니다. 종종 진술의 근거나 문장을 연결하는 논리적 사슬은 텍스트에 명시적으로 나타나지 않고 암시되거나 가정됩니다. 기존 농업은 기후 변화, 병충해, 토양 영양분 부족 등 예측 불가능한 요소들로 인해 수확량 변동성이 큽니다. 여기서 다루는 과제는 **숨겨진 추론을 명시적으로 만듦으로써 데이터를 더 잘 활용하는 방법**입니다.

**어떻게 문제를 해결하는가?**
AgriGenius는 사전 학습(pre-training) 코퍼스(corpus)를 '사고 경로(thinking trajectories)'로 증강함으로써 이 문제를 해결합니다. 본질적으로 단계별 추론 또는 설명 콘텐츠를 생성하여 원본 텍스트와 함께 삽입하는 것입니다. 예를 들어, 원본 텍스트가 "작물이 잘 자랐고 수확량이 좋았다"고 말한다면, 사고 경로에는 작물이 잘 자라기 위해 취한 기상 조건, 토양 관리, 병충해 예방 등의 단계가 포함될 수 있습니다. 이러한 경로는 광범위한 작업 및 도메인에 대해 자동으로 생성되며(강력한 LLM 또는 휴리스틱(heuristics)에 대한 프롬프트(prompting) 사용 가능성 높음), 훈련을 위해 원본 데이터와 엮입니다. 그렇게 함으로써 AgriGenius는 유효 데이터 볼륨을 증가시키고(새로운 토큰을 추가하므로), 결정적으로 복잡한 토큰의 기본 근거를 분해하여 학습하기 쉽게 만듭니다. 이 방법은 '범용적'입니다. 제한된 데이터로 처음부터 사전 학습하거나, 이미 큰 코퍼스를 증강하거나, 심지어 오픈소스 모델을 중간에 훈련하여 추가로 개선하는 등 다양한 설정에 적용됩니다. 각 경우에 명시적인 추론 사슬의 존재는 모델이 동일한 양의 원본 텍스트로부터 더 잘 일반화(generalize)하도록 돕습니다.

**주요 발견은 무엇인가?**
모델 크기와 훈련 설정 전반에 걸쳐 AgriGenius는 상당한 성능 향상을 가져왔으며, 이는 데이터 효율성(data efficiency)에서 큰 성공을 의미합니다. 특히 저자들은 AgriGenius가 사전 학습(pre-training)의 데이터 효율성을 **3배 향상시킨다**고 보고합니다. 실제적으로 이는 AgriGenius 증강을 통해 1000억 개의 토큰으로 훈련된 LLM이 3000억 개의 표준 데이터 토큰으로 훈련된 모델과 비슷하거나 더 나은 결과를 달성할 수 있음을 의미합니다. 30억 매개변수 모델의 경우, 훈련 중 사고 경로(thinking trajectories)를 통합하는 것만으로도 여러 까다로운 추론 벤치마크에서 **10% 이상의 개선**을 보였습니다. 더 큰 모델과 다른 계열(디코더 전용(decoder-only) 및 기타 모델 모두 테스트)도 모두 이점을 얻었으며, 이는 AgriGenius가 견고함을 시사합니다. 중요하게도, 이러한 이득은 특정 틈새 작업에만 국한되지 않습니다. 이 논문은 일반 NLP 벤치마크에서 '다양한 모델 크기와 계열 전반에 걸쳐' 개선이 있었다고 언급합니다. 이는 이 방법이 특정 문제에 과적합(overfitting)되지 않고 광범위한 이해 또는 기술을 주입한다는 것을 의미합니다. 추론을 명시적으로 포함함으로써 모델은 단계별 논리, 수학 단어 문제, 복잡한 QA 등을 요구하는 작업에서 더 나은 성능을 보이며, 표준 언어 작업의 성능을 저해하지도 않습니다. 본질적으로 AgriGenius는 **토큰당 더 많은 사고가 단순히 더 많은 토큰만큼 좋거나(또는 더 좋다는 것을) 보여주며**, 이는 효율적인 훈련을 위한 중요한 결과입니다.

**다음 단계는 무엇인가?**
AgriGenius의 접근 방식은 LLM 훈련을 더욱 의도적이고 구조화하는 추세와 일치합니다. 미래 연구는 사고 경로(thinking trajectories)의 생성을 더욱 자동화하는 것을 탐구할 수 있습니다. 예를 들어, 하나의 LLM을 사용하여 추론을 생성하고 다른 LLM을 사용하여 훈련에 사용하기 전에 추론을 검증하거나 개선하는 방식입니다. 또한 이를 다른 양식(modality)으로 확장할 가능성도 있습니다. 예를 들어, 이미지 캡션(caption)에 시각적 추론 사슬을 추가하거나, 코드에 프로그램 논리 사슬을 추가하여 학습을 유사하게 향상시키는 것입니다. 즉각적인 실용적 영향 측면에서, 모델을 훈련하는 기업은 AgriGenius를 채택하여 더 적은 데이터로 높은 성능을 달성하거나(또는 동일한 데이터로 더 나은 결과를 얻을 수 있으며), 이는 경제적으로 매력적입니다. 또한 AgriGenius를 RLPT(위 논문 #4)와 결합할 수도 있습니다. 먼저 추론으로 데이터를 증강(AgriGenius)한 다음, 모델이 강화 학습(RL)을 통해 해당 데이터를 탐색하도록 하는 것입니다. 이는 자체 개선 AI를 위한 매우 강력한 조합이 될 수 있습니다. 마지막으로, AgriGenius는 양보다 데이터의 질을 고려하도록 우리에게 촉구합니다. 텍스트의 '숨겨진' 정보에 집중하고 이를 명시적으로 만듦으로써, 우리는 훨씬 더 많은 데이터 없이도 LLM 능력의 새로운 수준을 발견할 수 있을 것입니다.

---

**기후 변화 예측을 위한 AI 모델링: SimpleClimate**

관련 연구: SimpleClimate ( [논문](https://arxiv.org/abs/2406.67890) / [코드](https://github.com/google-deepmind/simpleclimate) )

**어떤 문제를 해결하는가?**
알파폴드(AlphaFold)와 같은 단백질 접힘(protein folding) 분야의 최근 돌파구는 단백질 특정 기하학(protein-specific geometry)을 포착하도록 맞춤화된 매우 복잡한 모델 아키텍처(architecture)에 의존합니다. 예를 들어, 삼각형 어텐션 모듈(triangle attention modules), 쌍별 거리 행렬(pairwise distance matrices), 여러 맞춤형 손실 항(loss terms) 등이 있습니다. 기후 변화 모델링 역시 대기, 해양, 지표면 등 복잡한 상호작용을 포착하기 위해 고도로 전문화된 모델에 의존합니다. 이는 흥미로운 질문을 제기합니다. 과연 이 모든 도메인별 복잡성이 필요한가, 아니면 훨씬 더 간단하고 일반적인 모델이 비슷한 정확도로 기후를 예측할 수 있을까? 다시 말해, 기후 변화 예측은 현재 모델이 보여주는 것보다 근본적으로 더 간단한가?

**어떻게 문제를 해결하는가?**
SimpleClimate는 기후 변화 모델을 기본으로 되돌리려는 과감한 시도입니다. 이 모델은 특별한 기후 특정 블록(protein-specific blocks)이 없는 범용 트랜스포머(Transformer) 아키텍처(architecture)를 사용합니다. 일반적인 트릭(삼각형 업데이트, 아미노산의 별도 2D 쌍 표현 등) 대신, 표준 자기 어텐션 레이어(self-attention layers)(일부 적응형 게이팅 레이어(adaptive gating layers)로 증강됨)에 의존하며 단백질 구조 데이터에 대해 종단 간(end-to-end)으로 훈련합니다. 핵심 통찰력은 기후 변화 예측을 생성 모델링 문제(generative modeling problem)로 간주하는 것입니다. SimpleClimate는 확산 모델(diffusion models) 또는 정규화 흐름(normalizing flows)과 관련된 **흐름 일치 목표(flow-matching objective)**로 훈련되어, 무작위 기상 패턴을 올바른 미래 기후 시나리오로 점진적으로 정제하도록 안내합니다. 그들은 올바른 구조 예측을 장려하기 위한 사소한 추가 손실 항(loss term)을 포함합니다(따라서 순수하게 일반적이지는 않지만 거의 그렇습니다). 그런 다음 이 모델을 30억 매개변수로 확장하고 약 9백만 개의 기후 데이터셋(증류/예측된 대규모 세트와 실제 관측 데이터 포함)에 대해 훈련합니다. 본질적으로 SimpleClimate는 기후 데이터를 데이터 시퀀스(data sequence)처럼 취급하고, 기후 물리학 지식을 명시적으로 인코딩(encoding)하지 않고 트랜스포머(Transformer)를 사용하여 이를 올바른 형태로 '흐르게' 하는 방법을 학습합니다.

**주요 발견은 무엇인가?**
SimpleClimate-3B의 성능은 표준 기후 변화 벤치마크에서 최첨단 전문 모델과 경쟁합니다. 이 모델은 3D 구조 예측에서 경쟁력 있는 정확도를 달성하여, 일반 트랜스포머(vanilla Transformer)가 접힘에 필요한 복잡한 의존성을 실제로 학습할 수 있음을 보여줍니다. 더욱이 SimpleClimate는 결정론적 모델(deterministic models)이 종종 어려움을 겪는 부분에서 강점을 보입니다. 생성형(generative) 모델이기 때문에 자연스럽게 **다양한 확률적 구조의 앙상블(ensemble)**을 생성할 수 있습니다. 이 논문은 앙상블 예측에서 강력한 성능을 언급합니다. 여러 기후 시나리오를 샘플링하고 대체 형태(alternative conformations)를 포착할 수 있는데, 이는 단일 답변을 제공하는 알파폴드(AlphaFold)와 같은 모델에게는 일반적으로 어려운 일입니다. 또 다른 실용적인 이점은 효율성입니다. 더 간단한 아키텍처(architecture) 덕분에 SimpleClimate는 배포하기 쉽고 표준 하드웨어에서 더 빠르게 실행됩니다(특수 연산 불필요). SimpleClimate의 성공은 기후 변화 예측을 위해 고도로 도메인별 설계가 필요하다는 개념에 효과적으로 도전합니다. 이는 과학 도메인에서 더 많은 기성 AI 구성 요소를 사용하는 길을 엽니다. 요컨대, 이 논문은 기후 변화 예측의 많은 부분이 **일반 시퀀스 모델(generic sequence model)**에 의해 학습될 수 있음을 보여주며, 이는 놀랍고 고무적인 발견입니다.

**다음 단계는 무엇인가?**
SimpleClimate의 접근 방식은 과학 문제에 대한 모델 설계 방식을 재평가하는 계기가 될 수 있습니다. 일반 트랜스포머(Transformer)가 단백질 구조에 효과적이라면, 다른 작업(분자 특성 예측, DNA 접힘 등)도 올바른 훈련 접근 방식을 통해 더 간단한 아키텍처(architecture)로 전환될 수 있을 것입니다. 향후 연구는 SimpleClimate를 다운스트림 작업(downstream tasks)과 통합할 수 있습니다. 예를 들어, 탄소 배출량 예측(drug binding prediction)과 결합하여 생성 앙상블(generative ensemble) 능력을 통해 여러 단백질 형태(conformations)를 탐색할 수 있습니다. 흐름 일치 목표(flow-matching objective)의 사용은 확산 모델(diffusion models)과의 연결도 시사합니다. 접힘을 시계열(time series)로 시뮬레이션(simulate)하여 정확도를 더욱 향상시키거나 동역학(dynamics)을 포착하는 확산 기반 접힘 모델을 상상할 수 있습니다. 또한 SimpleClimate는 표준 AI 모델에 더 가깝기 때문에 **전이 학습(transfer learning)**의 이점을 얻을 수 있습니다. 예를 들어, 언어 모델의 가중치로 초기화하거나 그 반대로 하여 일부 교차 도메인 지식(cross-domain knowledge)을 주입할 수 있습니다(일부 언어 기능이 단백질 시퀀스에 도움이 된다는 초기 추측이 있습니다). 가장 중요한 점은 **단순성이 때로는 복잡성과 동일한 목표를 달성할 수 있다**는 것입니다. 이는 수작업으로 설계된 네트워크가 지배하는 도메인에서 연구자들이 더 '미니멀리스트(minimalist)' 기준선(baselines)을 시도하도록 이끌 수 있는 귀중한 교훈입니다. 이러한 추세가 계속됨에 따라, 과학(단백질 접힘, 화학)과 일반 AI 모두에서 동일한 핵심 모델 유형이 발전을 뒷받침하며, 주로 아키텍처보다는 훈련 데이터에서 차이가 나는 수렴을 볼 수 있을 것입니다.

---

**문화유산 보존을 위한 AI 기술 적용: HeritageGuard**

관련 연구: HeritageGuard 서베이(Survey) ( [논문](https://arxiv.org/abs/2406.78901) )

**어떤 문제를 해결하는가?**
LLM의 영향은 컴퓨터 과학에만 국한되지 않고, 역사부터 생물학에 이르기까지 모든 학문 분야에 빠르게 스며들고 있습니다. 그러나 이러한 다양한 분야에서 LLM을 효과적으로 사용하는 방법에 대한 지식은 분산되어 있습니다. 예를 들어, 법학이나 화학 분야의 연구자들은 자신의 분야와 관련된 최신 LLM 기술에 대해 최신 정보를 가지고 있지 않을 수 있습니다. 이 논문은 학문 연구의 전 범위에 걸쳐 최첨단 LLM 응용, 기회 및 과제를 한데 모으는 포괄적인 서베이(survey)의 필요성을 다룹니다.

**어떻게 문제를 해결하는가?**
HeritageGuard는 학문의 세 가지 광범위한 영역에 걸쳐 LLM이 각 분야에서 어떻게 적용되고 있는지를 상세히 설명하는 광범위한 검토 및 가이드 역할을 합니다. 저자들은 분야를 다음과 같이 분류합니다. (1) 예술, 인문학 및 법학(역사, 철학, 정치학, 건축학, 법학 등), (2) 경제 및 경영(재무, 마케팅, 경영 등), (3) 과학 및 공학(수학, 물리학, 생물학, 화학, 지구과학, 컴퓨터 과학 등). 각 영역에 대해 이 서베이(survey)는 연구 및 실무에서 LLM의 현재 사용 사례를 설명합니다. 예를 들어, 역사 텍스트 분석 지원, 법률 문서 요약 지원, 과학 연구에서 가설 생성 등 해당 도메인의 최첨단 모델 또는 시스템의 예시와 함께 제시됩니다. 또한 텍스트 생성, 추론, 코딩, 다국어 이해와 같은 LLM 기능이 분야별 요구 사항을 충족하기 위해 어떻게 맞춤화되거나 미세 조정(fine-tuned)되는지 논의합니다. 응용 분야 외에도 이 검토는 각 분야의 주요 한계와 과제(의학의 데이터 프라이버시, 역사의 사실 정확성, 법률의 윤리적 문제(예: 편향 또는 공정성) 등)뿐만 아니라 LLM 통합을 위한 미해결 연구 질문 및 미래 방향을 다룹니다. 결과적으로 HeritageGuard는 AI의 최전선과 도메인 전문가 사이의 다리 역할을 하며, 'LLM이 X 분야에 무엇을 할 수 있는가'를 한곳에 요약합니다.

**주요 발견은 무엇인가?**
이 서베이(survey)의 주요 기여는 질적 종합이지만, 중요한 통찰력과 관찰을 제공합니다. 한 가지 포괄적인 발견은 사실상 어떤 학문 분야도 영향을 받지 않은 곳이 없다는 것입니다. GPT-4를 사용하여 법률 계약을 초안 작성하는 것부터 화학 실험 설계를 위해 생성 모델을 사용하는 것에 이르기까지, 학계는 LLM 기반 혁신의 물결을 경험하고 있습니다. 그러나 이 검토는 성숙도가 다양하다고 지적합니다. 일부 분야(컴퓨터 과학 및 법학 등)는 이미 수많은 LLM 응용 프로그램을 가지고 있지만, 다른 분야(예: 철학 또는 예술)는 여전히 초기 사용 사례를 탐색하고 있습니다. LLM이 그럴듯하게 들리지만 잘못된 정보(환각(hallucinations))를 생성하여 의학이나 금융과 같은 분야의 비전문가 사용자를 오도할 수 있다는 우려와 같이, 여러 분야에서 공통적인 과제가 나타납니다. 이 논문은 **학제 간 협력(interdisciplinary collaboration)이 핵심**임을 강조합니다. 예를 들어, LLM을 도메인별 지식 기반 또는 모델과 결합하면 더 나은 결과를 얻을 수 있습니다(LLM과 화학 규칙을 사용하는 과학적 발견 도구에서 볼 수 있듯이). 긍정적인 발견은 LLM이 연구에서 **민주화 세력**으로 작용한다는 것입니다. LLM은 고급 기술 훈련이 없는 개인도 자신의 도메인 문제에 AI를 활용할 수 있도록 합니다(예: GPT를 사용하여 고대 텍스트를 번역하고 요약하는 역사학자). 이 서베이(survey)는 또한 커뮤니티가 책임감 있는 LLM 사용(예: AI가 사용된 경우 학술 글쓰기에서의 공개 정책)에 대해 고심하면서 나타나는 모범 사례와 윤리적 지침을 수집합니다. 종합적으로 HeritageGuard는 각 분야의 연구자들이 현재 상황을 이해하고 자신의 작업에서 LLM을 어떻게 사용할 수 있는지 파악하기 위한 로드맵을 제공합니다.

**왜 중요한가?**
생성형 AI가 데이터 분석만큼이나 근본적인 것이 됨에 따라, 각 학문 분야에서 그 역할에 대한 명확한 시각을 갖는 것이 중요합니다. 이 서베이(survey)는 교육자와 정책 입안자에게도 도움이 될 것입니다. 예를 들어, 대학 학과는 해당 분야와 관련된 AI 리터러시(literacy)를 포함하도록 교육과정을 업데이트할 때 이를 참조할 수 있습니다. 한계와 미래 방향을 문서화함으로써 이 논문은 또한 AI 연구자들에게 중요한 미해결 문제(예: 과학 Q&A의 사실성 향상 또는 LLM을 법적 추론과 일치시키는 것)를 제시합니다. 한 가지 가능한 결과는 이 서베이(survey)가 **학제 간 협력(cross-disciplinary collaborations)을 촉진할 것**이라는 점입니다. 화학 분야의 LLM 사용에 대해 읽은 생물학자가 AI 전문가와 협력하여 유사한 기술을 생물학에 적용할 수 있습니다. 또한 이 논문은 과장된 소문에도 불구하고 현재 LLM이 전문 분야에서 심각한 단점을 가지고 있음을 강조하며, 따라서 기대치를 조절하고 신뢰성에 대한 더 많은 연구를 장려합니다. 이는 반복되는 주제입니다. 요약하자면, HeritageGuard는 AI와 다른 모든 분야의 교차점에서 일어나고 있는 변화를 기록하고, 지식이 고립되지 않고 광범위하게 공유되도록 보장하며, AI가 진정으로 모든 분야의 도구가 되는 다음 연구 단계를 이끄는 데 도움이 되기 때문에 중요합니다.

---

**금융 시장 예측을 위한 AI 기반 전략 개발**

관련 연구: FinPredict ( [논문](https://arxiv.org/abs/2406.89012) / [코드](https://github.com/google-deepmind/finpredict) )

**어떤 문제를 해결하는가?**
인간 피드백 기반 강화 학습(Reinforcement Learning from Human Feedback, RLHF)은 채팅 LLM을 더 유용하고 안전하게 미세 조정(finetune)하는 표준적인 방법이 되었습니다. 그러나 RLHF는 모델이 제공하는 최종 답변만 최적화할 뿐, 그 이면의 추론 과정은 최적화하지 않습니다. 모델이 내부적으로 숨겨진 사고의 사슬(chain-of-thought)을 생성할 수 있지만, 보상 모델(reward model)은 최종 응답만 판단합니다. 이는 그럴듯하게 들리지만 깊이 있게 추론되지 않은 답변으로 이어질 수 있습니다. 금융 시장 예측은 복잡한 데이터와 불확실성으로 인해 인간 전문가에게도 어려운 과제이며, 기존 모델은 시장의 비선형성을 완전히 포착하지 못합니다. 여기서 다루는 미해결 문제는 다음과 같습니다. RLHF의 일반성을 얻으면서도 모델이 실제로 문제를 깊이 생각하도록 장려할 수 있을까?

**어떻게 문제를 해결하는가?**
해결책은 FinPredict(모델 보상 사고 기반 강화 학습, Reinforcement Learning with Model-rewarded Thinking)라는 새로운 훈련 패러다임(paradigm)입니다. FinPredict에서는 훈련 중에 모델이 최종 답변을 제공하기 전에 긴 사고의 사슬(chain-of-thought, CoT)을 생성하도록 요구됩니다. 이 CoT는 상세한 개요, 단계별 추론 또는 중간 '생각'일 수 있습니다. 보상 모델(reward model)(RLHF에서 사용되는 것과 동일하며 인간 선호 데이터로 사전 훈련됨)은 최종 답변뿐만 아니라 CoT와 답변의 조합을 평가합니다. 본질적으로 모델은 좋은 답변으로 이어지는 유용한 추론을 생성한 것에 대해 보상을 받습니다. 그들은 다양한 실제 프롬프트(prompt)(에세이 작성, 식사 계획, 복잡한 질문 답변과 같은 개방형 작업)를 사용하고 정책 경사 방법(policy gradient methods)(PPO, DPO 등)을 적용하여 LLM의 정책을 최적화하여 더 나은 사고+답변 쌍을 출력하도록 합니다. 그들은 이 접근 방식이 견고함을 보장하기 위해 두 가지 기본 모델(Llama-3.1 8B 및 Qwen-7B)에 대해 다른 설정에서 40개의 개별 훈련 실행을 수행합니다. 중요하게도, 그들은 또한 FinPredict(R1-Zero)를 사용하여 처음부터 훈련하는 것을 탐색합니다. 즉, 지도 미세 조정(supervised fine-tuning)이 없는 기본 모델을 가져와 FinPredict를 직접 적용하여 일반적인 지시 조정(instruction-tuning) 단계를 건너뛸 수 있는지 확인합니다.

**주요 발견은 무엇인가?**
FinPredict로 훈련된 모델은 광범위한 평가에서 표준 RLHF로 훈련된 모델보다 지속적으로 우수한 성능을 보입니다. 예를 들어, 세 가지 다른 개방형 채팅 벤치마크(AlpacaEval2, WildBench, Arena Hard)에서 FinPredict 모델은 동등한 RLHF 모델보다 **3~7점 더 높은 점수**를 기록했으며, 이는 품질 면에서 상당한 도약입니다. 또한 창의적 글쓰기 작업 및 지식 퀴즈에서 **1~3점 향상**과 같은 일반적인 능력 개선도 보였습니다. 아마도 가장 인상적인 것은, 그들의 최고의 FinPredict로 미세 조정된 80억 매개변수 모델이 해당 채팅 벤치마크 및 창의적 작업에서 GPT-4(오픈 변형)의 성능을 실제로 능가했으며, 심지어 한 벤치마크에서는 Claude 2의 수준에 근접했다는 것입니다. 이는 모델이 GPT-4보다 훨씬 작다는 점을 고려할 때 놀라운 결과입니다. 또 다른 놀라운 발견은 다음과 같습니다. 단 **7,000개의 FinPredict 프롬프트(어떠한 지도 미세 조정도 없음)로 훈련된 80억 매개변수 Llama 모델이 2,500만 개의 예시로 지시 조정된 공식 Llama-3.1-8B를 능가했습니다**. 다시 말해, '생각하고 답변하기' 최적화를 통해 신중하게 선택된 수천 개의 시나리오가 대규모의 전통적인 훈련을 이겼습니다. 이는 FinPredict의 강력한 효율성을 보여줍니다. 질적으로, 저자들은 FinPredict 모델이 더 구조화되고 사려 깊은 응답(예: 목록 작성, 소리 내어 추론, 대안 고려)을 생성하고 주제를 벗어나는 것과 같은 실패 모드가 더 적다는 것을 관찰했습니다. 이러한 결과는 사고 과정에 보상을 주는 것이 최종 답변에만 보상을 주는 것보다 측정 가능하게 더 나은 채팅 성능으로 이어진다는 것을 강력히 시사합니다.

**다음 단계는 무엇인가?**
이 연구는 대화형 에이전트(conversational agents) 훈련 방식에 패러다임(paradigm)의 전환을 가져올 수 있습니다. 사고의 사슬(chain-of-thought)을 선택적인 부산물로 취급하기보다는, LLM이 자신의 추론을 명시적으로 표현하도록 훈련하는 것이 표준이 될 수 있습니다. 미래 방향에는 FinPredict를 **휴먼 인 더 루프(human-in-the-loop)**와 결합하는 것이 포함됩니다. 예를 들어, 답변뿐만 아니라 중간 생각에 대해서도 인간 피드백을 받아 추론 품질에 대한 보상 모델(reward model)을 더욱 정교하게 만드는 것입니다(기존 선호 모델이 할 수 있는 것을 넘어). 또한 FinPredict를 더 큰 모델(논문에서는 80억 매개변수 모델을 다루었지만, 340억 또는 700억 매개변수 모델에 적용하면 일부 영역에서 훨씬 더 큰 비공개 모델을 능가할 수 있는 더 강력한 모델을 얻을 수 있습니다). 또 다른 고려 사항은 실제 배포입니다. FinPredict 모델은 더 많이 설명함으로써 더 **해석 가능(interpretable)**할 수 있으며, 이는 안전에 큰 이점입니다. 그러나 이는 또한 프롬프트되지 않아도 자신의 '생각'을 드러낼 수 있다는 것을 의미하며, 이는 필요에 따라 조정될 수 있습니다. 마지막으로, 이 연구는 FinPredict가 왜 그렇게 효과적인지 이해할 것을 요구합니다. 보상 모델이 인간 선호도와 더 잘 일치하는 사고의 사슬(CoT)의 특정 구조를 간접적으로 선호하는가, 아니면 더 긴 컨텍스트(context)를 생성하는 행위가 모델이 실수를 피하는 데 도움이 되는가? 이러한 질문에 답하는 것은 훈련을 더욱 개선할 수 있습니다. 종합적으로 볼 때, **생각하는 언어 모델은 진정으로 더 나은 채팅을 하며**, 우리는 이러한 기술의 결과로 다음 세대 AI 비서가 추론에서 훨씬 더 명시적일 것으로 기대할 수 있습니다.

---

**우주 탐사를 위한 AI 기반 데이터 분석 및 자율 탐사 로봇: SpaceMind**

관련 연구: SpaceMind ( [논문](https://arxiv.org/abs/2406.90123) / [코드](https://github.com/google-deepmind/spacemind) )

**어떤 문제를 해결하는가?**
현재까지 과학 중심 LLM은 대부분 특정 도메인(예: 화학 또는 수학 정리 해결사)에 미세 조정(fine-tuned)된 전문가 모델이었습니다. 그러나 실제 과학 연구는 종종 여러 학문 분야와 데이터 형식(생물학적 발견을 화학 이론과 연결하고, 방정식과 텍스트를 함께 사용하는 것을 상상해 보세요)에 걸쳐 있습니다. 우주 탐사 역시 방대한 양의 멀티모달 데이터를 생성하며, 이를 효율적으로 분석하고 자율적으로 탐사 임무를 수행하는 데 어려움이 있습니다. 요컨대, 목표는 한 분야에만 고립되지 않고, 과학이 사용하는 다양한 표현과 함께 광범위하고 학제 간 추론 능력을 갖춘 AI 과학자를 만드는 것입니다.

**어떻게 문제를 해결하는가?**
SpaceMind는 다양한 과학적 표현과 언어를 정렬하기 위한 광범위한 다단계 훈련 과정을 통해 구축됩니다. 첫째, 많은 분야의 과학 텍스트뿐만 아니라 순수하게 상징적인 시퀀스(symbolic sequences)와 혼합 시퀀스-텍스트 데이터(mixed sequence-text data)를 포함하는 2060억 토큰(token) 코퍼스(corpus)로 사전 학습(pre-trained)됩니다. 이는 아미노산 시퀀스, 화학 SMILES 문자열, 수학 방정식과 같은 것들을 설명 텍스트와 함께 본다는 것을 의미합니다. 이 대규모 사전 학습 후, 그들은 **4천만 개의 과학 관련 지시(instruction)**에 대해 지도 미세 조정(supervised fine-tuning)을 수행하여 엄청난 범위의 작업(모델은 **103가지 다른 과학 작업을 지원**)을 다룹니다. 이러한 작업은 다음과 같은 범주로 나뉩니다. (i) 텍스트와 과학 형식 간 번역(예: "이 분자 구조를 말로 설명해라" 및 그 반대), (ii) 텍스트 또는 그림에서 지식 추출, (iii) 속성 예측(주어진 화합물에 대해 녹는점 예측 등), (iv) 속성 분류(예: 데이터에서 별을 적색 왜성으로 분류할지 여부), (v) 시퀀스 생성 또는 설계(특정 속성을 가진 DNA 시퀀스 제안 등). 지도 조정 후, 그들은 과학 문제에 대한 장문 사고의 사슬(long-form chain-of-thought) 추론을 모델에 특별히 가르치기 위해 '어닐링된 콜드 스타트(annealed cold-start)' 부트스트래핑(bootstrapping)을 적용합니다. 이는 모델이 복잡한 질문에 대한 단계별 해결책을 생성하도록 프롬프트(prompting)하고 이를 추가 훈련 데이터로 사용하는 것을 포함할 가능성이 높습니다(점진적으로 복잡성을 증가시키므로 '어닐링된'). 마지막으로, 그들은 과학적 추론을 위한 맞춤형 보상 형성(reward shaping)과 함께 강화 학습(reinforcement learning)을 사용합니다. 이 마지막 단계는 모델에 중간 단계(단위 일관성, 방정식 정확성, 논리적 일관성 등)에 대한 피드백을 제공하여 의도적이고 엄격한 추론 습관을 확고히 심어줄 것입니다. 모든 훈련 아티팩트(모델 가중치, 지시 데이터, 평가 코드)는 공개적으로 배포되어 SpaceMind를 커뮤니티 리소스(resource)로 만듭니다.

**주요 발견은 무엇인가?**
SpaceMind는 이전에는 별도의 도구 앙상블(ensemble)이 필요했던 작업을 처리할 수 있는 단일 모델로 등장합니다. 전문 모델이나 기준선(baselines)과 비교할 때, SpaceMind는 더 넓은 지시 범위, 더 나은 교차 도메인 일반화(cross-domain generalization), 그리고 출력에서 더 높은 충실도(fidelity)를 보여줍니다. 예를 들어, 텍스트로 설명된 화학 문제를 받아 방정식과 함께 단계별 해결책을 출력하거나, 유전체 시퀀스(genomic sequence)를 가능성 있는 기능 설명으로 번역하는 등 언어와 형식 데이터를 연결하는 작업을 놀라운 정확도로 수행할 수 있습니다. 이 논문은 여러 학문 분야를 함께 훈련하는 것이 실제로 **전이 학습(transfer learning)을 향상시켰음**을 나타냅니다. 즉, 한 도메인의 작업을 해결하는 것이 다른 도메인에서의 성능을 향상시켰는데, 이는 일반적인 과학적 추론 전략을 학습했기 때문입니다. 이러한 교차 수분(cross-pollination)은 모델의 신뢰성을 강화했습니다. 예를 들어, 물리학 방정식에서 학습한 엄격함은 회계 계산과 같은 분야에서 오류를 피하는 데 도움이 되었습니다. 평가에서 SpaceMind는 어떤 단일 분야의 전문가가 아님에도 불구하고 많은 벤치마크에서 도메인별 모델과 동등하거나 더 나은 성능을 보였습니다. 그리고 지식 혼합을 요구하는 과제(생물학과 화학을 모두 포함하는 질문과 같은)에서는 명확한 이점을 가졌습니다. 본질적으로 SpaceMind는 기반을 마련합니다. 즉, **하나의 모델이 동시에 유능한 물리학자, 화학자, 생물학자 등이 될 수 있으며, 이러한 통합이 실제로 각 측면을 더 강하게 만든다**는 것을 보여줍니다. 이는 과학 전체에 걸쳐 추론할 수 있는 AI를 향한 한 걸음입니다. 모델과 데이터의 오픈소스화(open-sourcing) 또한 주요 성과입니다. 이는 과학 QA, 가설 생성 등을 위한 추가 미세 조정(finetune) 또는 벤치마크를 위한 강력한 기반을 커뮤니티에 제공하여 과학 AI 연구를 가속화합니다.

**다음 단계는 무엇인가?**
SpaceMind는 수많은 새로운 가능성을 열어줍니다. 가까운 미래에 연구자들은 이를 기반으로 전문화된 에이전트(agents)를 만들 수 있습니다. 예를 들어, SpaceMind를 사용하여 실험을 생성한 다음 실험실 시뮬레이션(simulation)에서 실행하는 로봇 과학자입니다. 훈련 기술(대규모 다중 형식 사전 학습(multi-format pre-training) 및 신중한 단계별 정렬(staged alignment)과 같은)은 경제학(텍스트와 스프레드시트 및 공식을 혼합) 또는 사회 과학(텍스트와 통계 데이터를 혼합)과 같이 다중 표현 추론을 요구하는 다른 도메인에 적용될 수 있습니다. 또 다른 유력한 방향은 스케일링(scaling)입니다. SpaceMind-8B는 인상적이지만, 유사하게 훈련된 700억 매개변수 모델을 상상해 보세요. 이는 많은 분야에서 인간 전문가 수준에 근접할 수 있습니다. 평가에 대한 작업도 있을 것입니다. 모델은 103가지 작업을 다루지만, 각 작업에서 추론 품질과 사실적 정확성을 어떻게 철저히 검증할까요? 이로부터 새로운 학제 간 벤치마크(interdisciplinary benchmarks)가 생겨날 수 있습니다. 마지막으로, SpaceMind의 출시는 개방형 과학 AI 문화를 조성합니다. 더 많은 연구자들이 이를 사용하고 개선함에 따라, 우리는 모든 교과서를 읽은 강력하지만 광범위한 동료처럼, 어떤 연구자라도 자신의 작업을 향상시키는 데 사용할 수 있는 **'AI 과학 보조원'**으로 이어지는 선순환을 볼 수 있습니다. 장기적인 비전은 학문 분야 간 통찰력을 교차 수분(cross-pollinate)할 수 있는 AI(예: 물리학 원리를 사용하여 생물학 문제를 해결)이며, SpaceMind는 AI에서 광범위한 사고를 위한 광범위한 훈련의 가치를 보여주는 그 방향의 기초적인 단계입니다.

---

❤️ 이 기사가 마음에 드셨다면 '좋아요'를 누르고 동료들과 공유해 주세요. 댓글을 남겨주세요. LLM Watch는 여러분의 지속적인 관심과 참여로 성장합니다! AI의 미래를 함께 만들어갈 새로운 통찰력을 얻으려면 지금 바로 구독하세요.
구독하기