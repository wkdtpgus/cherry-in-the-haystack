LLM Watch 독자 여러분, 환영합니다! 이번 주 LLM Watch에서는 대규모 언어 모델(LLM)의 최첨단 발전과 AI, 비전, 과학 등 다양한 분야에서의 응용에 대해 깊이 있게 다룹니다. 주요 내용은 다음과 같습니다:

*   LLM 기반 진화 프레임워크인 ShinkaEvolve가 어떻게 효율적으로 새로운 알고리즘을 발견하는지.
*   원시 사전 학습(pre-training) 데이터에서 추론 능력을 스스로 학습하는 RLPT(자기 지도형 강화 학습)의 혁신.
*   생성형 비디오 모델이 시각적 추론과 도구 사용 시뮬레이션까지 제로샷(zero-shot)으로 수행하는 능력.
*   LLM이 사고의 사슬(chain-of-thought)을 명시적으로 생성하도록 훈련하는 RLMT(모델 보상 사고 기반 강화 학습)가 채팅 성능을 비약적으로 향상시키는 방법.
*   강화 학습(RL)을 통해 연속적인 '소프트(soft)' 토큰으로 추론을 훈련하여 이산적인 사고의 사슬(discrete chain-of-thought)을 뛰어넘는 새로운 접근 방식.
*   단백질 접힘과 같은 복잡한 과학 문제를 일반적인 트랜스포머(Transformer) 아키텍처(architecture)로 해결하는 SimpleFold의 단순성.
*   학문 분야 전반에 걸쳐 LLM의 응용과 과제를 종합적으로 분석한 LLMs4All 서베이(survey).
*   다양한 과학적 표현과 언어를 통합하여 학제 간 추론을 가능하게 하는 SciReasoner의 등장.

아래 용어집을 확인하거나 바로 논문 섹션으로 이동해 보세요.

LLM Watch는 AI 커뮤니티의 활발한 참여를 장려합니다. 최신 연구 동향에 대한 논의에 참여하고, 다가오는 온라인 웨비나 및 기술 워크숍에 함께하세요. 구독자 여러분께는 특별한 기회와 할인 혜택이 제공됩니다. AI 기술의 미래를 함께 만들어갈 여러분의 목소리를 기다립니다!

**빠른 용어집**

**사전 학습 데이터 기반 강화 학습(Reinforcement Learning on Pre-Training data, RLPT):** LLM이 사전 학습(pre-training) 코퍼스(corpus)를 다음 토큰(next-token) '경로(trajectories)'를 탐색하고 강화 학습(RL)을 통해 학습하는 환경으로 취급하는 훈련 패러다임입니다. 이 접근 방식은 실제 다음 세그먼트(segment)를 예측하는 것에서 직접 보상을 도출함으로써 인간의 주석(annotation) 필요성(RLHF와 달리)을 우회합니다. 그 결과 모델은 원시 텍스트 데이터만 사용하여 추론 기술을 자체적으로 향상시킬 수 있습니다.

**검증 가능한 보상 기반 강화 학습(Reinforcement Learning with Verifiable Rewards, RLVR):** 수학이나 코드와 같이 결과의 정확성을 자동으로 확인할 수 있는 특정 도메인에서 프로그래밍 방식의 보상 메커니즘을 활용하는 강화 학습(RL) 기법입니다. 이 방법은 검증 가능한 작업에서 모델의 추론 능력을 효과적으로 향상시키지만, 창의적 글쓰기나 계획 수립과 같이 객관적인 정답이 없는 개방형 작업에서는 그 적용 범위와 이점이 제한적입니다.

**모델 보상 사고 기반 강화 학습(RL with Model-rewarded Thinking, RLMT):** 모델이 최종 답변을 제공하기 전에 긴 사고의 사슬(chain-of-thought)을 생성하도록 요구하는 새로운 강화 학습(RL) 파이프라인(pipeline)입니다. 별도의 보상 모델(reward model)(RLHF의 모델과 유사)이 이러한 추론 과정을 평가하고, 기본 모델은 더 잘 숙고된 답변을 생성하도록 최적화됩니다. 이 '생각하고 응답하기' 전략은 여러 벤치마크에서 큰 성능 향상을 보여주듯이, 표준 RLHF보다 더 강력한 일반 채팅 능력을 이끌어냅니다.

**사고 증강 사전 학습(Thinking Augmented Pre-Training, TPT):** LLM의 훈련 데이터 효율성을 극대화하기 위해 기존 텍스트 데이터에 '사고 경로(thinking trajectories)' 또는 단계별 추론 과정을 인위적으로 주입하는 방법입니다. 이 접근 방식은 모델이 단순히 다음 토큰을 예측하는 것을 넘어, 숨겨진 논리적 연결과 복잡한 개념의 근거를 명시적으로 학습하도록 돕습니다. 결과적으로 모델은 훨씬 적은 양의 데이터로도 고급 추론 능력을 습득할 수 있게 됩니다.

**연속적 사고의 사슬('소프트' 토큰)(Continuous Chain-of-Thought, “Soft” tokens):** LLM의 중간 추론 과정에 이산 토큰(discrete tokens) 대신 연속 임베딩(continuous embeddings)을 사용하는 것입니다. 일반적인 단어 토큰과 달리, 이 소프트 토큰은 여러 추론 경로의 중첩(superposition)을 나타낼 수 있습니다. 최근 연구에 따르면 LLM은 이산적인 안내 없이도 수백 개의 연속적인 사고의 사슬(CoT) 토큰을 활용하도록 강화 학습(RL)을 통해 훈련될 수 있습니다. 그 결과는 더 풍부한 추론입니다. 모델은 한 번의 시도에서 이산적인 CoT 정확도와 일치하며, 여러 번 시도할 경우 더 다양한 해결 경로 덕분에 이를 능가합니다.

---

**ShinkaEvolve: 개방형 및 샘플 효율적인 프로그램 진화를 향하여**

관련 연구: ShinkaEvolve ( [논문](https://arxiv.org/abs/2405.08643) / [코드](https://github.com/ShinkaEvolve/ShinkaEvolve) )

**어떤 문제를 해결하는가?**
최근 대규모 언어 모델(LLM)을 활용하여 코드를 생성하고 개선하는 접근 방식은 이론적으로 매우 강력하지만, 실제 적용에서는 심각한 샘플 비효율성(sample-inefficiency)이라는 문제에 직면합니다. 즉, 유용한 프로그램이나 최적화된 해결책을 찾기 위해 수천, 수만 번의 비효율적인 시도를 반복해야 하는 경우가 많습니다. 이는 컴퓨팅 자원 측면에서 매우 비용이 많이 들고, 특히 비공개 소스(closed-source) 모델에 의존할 경우 접근성이 떨어져 소규모 연구팀이나 개인 개발자에게는 사실상 불가능한 기술이 됩니다. 이러한 한계는 LLM 기반의 창의적 발견과 혁신이 특정 대기업에만 국한되는 결과를 초래합니다.

**어떻게 문제를 해결하는가?**
ShinkaEvolve는 LLM을 활용하여 프로그램을 반복적으로 개선하는 과정에서 발생하는 비효율성을 극복하기 위해 설계된 오픈소스 프레임워크입니다. 이 시스템은 LLM을 단순한 코드 생성기가 아닌 '변이 연산자(mutation operator)'로 간주하여, 세 가지 핵심적인 혁신을 통해 필요한 시도 횟수를 획기적으로 줄입니다. 첫째, 새로운 아이디어를 탐색하는 '탐사(exploration)'와 기존의 성공적인 해결책을 활용하는 '활용(exploitation)' 사이의 균형을 정교하게 조절하는 '균형 잡힌 부모 선택 전략'을 사용합니다. 둘째, 중복되거나 창의적이지 않은 변이(mutation)를 효과적으로 제거하기 위해 '참신성 기반 거부 샘플링(novelty-based rejection sampling)'을 도입합니다. 이는 임베딩(embedding) 유사성과 LLM 자체의 참신성 판단을 결합하여 불필요한 계산을 줄입니다. 셋째, 각 진화 세대(generation)마다 앙상블(ensemble) 내에서 가장 유망한 LLM을 동적으로 선택하는 밴딧 알고리즘(bandit algorithm)을 적용합니다. 이러한 요소들은 ShinkaEvolve가 컴퓨팅 자원을 가장 잠재력 있는 프로그램 변형에 집중하게 함으로써, 이전 방법들의 '무작위 탐색(random search)'과 같은 비효율성을 효과적으로 회피합니다.

**주요 발견은 무엇인가?**
ShinkaEvolve는 프로그램 진화를 전례 없이 효율적으로 만듦으로써, 이전에는 비현실적이었던 다양한 문제 영역에서 놀라운 성공을 거두었습니다. 이 프레임워크는 단 150개의 샘플만으로 고전적인 '26개 원형 패킹 문제(26-circle packing problem)'에 대한 새로운 최첨단 해결책을 발견했습니다. 이는 기존 접근 방식에 비해 **'효율성의 엄청난 도약'**으로 평가됩니다. 또한, 수학 경시 대회(AIME 벤치마크)에서 강력한 인간 설계 기준선(baseline)을 능가하는 고성능 다중 에이전트(multi-agent) 전략을 단 75세대 만에 진화시켰습니다. 경쟁 프로그래밍 분야에서는 AtCoder 경시 대회 에이전트에 적용된 ShinkaEvolve의 개선 사항이 매우 중요하여, 특정 문제에서는 진화된 해결책이 대회에서 **2위를 차지했을 것입니다**. 더 나아가, ShinkaEvolve는 대규모 전문가 혼합(Mixture-of-Expert) LLM을 훈련하는 더 나은 방법까지 찾아냈습니다. 새로운 부하 분산 손실(load-balancing loss)을 발견하여 DeepMind의 'Global LBL' 기준선을 **1.73% 더 높은 작업 점수와 5.8% 더 적은 낭비 용량으로** 능가했습니다. 이러한 결과들은 광범위한 **'개방형(open-ended)' 발견이 이제 합리적인 비용으로 가능**하며, 과학자와 엔지니어가 새로운 해결책을 자율적으로 탐색할 수 있는 AI 기반 공동 조종사(co-pilot)를 제공함을 명확히 보여줍니다.

**다음 단계는 무엇인가?**
이 연구는 과학 및 공학 분야의 수많은 난제들, 예를 들어 새로운 알고리즘 설계나 최적화 문제 발견 등이 LLM 기반의 진화적 탐색을 통해 효율적으로 해결될 수 있음을 강력히 시사합니다. ShinkaEvolve를 오픈소스화하고 진화 과정을 시각적으로 보여주는 웹 UI(Web UI)까지 제공함으로써, 연구진은 이 강력한 접근 방식을 더 많은 사람들에게 보급하고자 합니다. 향후 연구는 이 방법을 회로 설계, 과학 공식 도출, 하이퍼파라미터(hyperparameters) 최적화와 같은 새로운 도메인에 적용하고, 더욱 발전된 LLM이 등장함에 따라 이를 통합하는 방향으로 발전할 것입니다. 장기적으로 볼 때, ShinkaEvolve와 같은 기술은 자동화된 **'연구 보조원'** 역할을 수행하며, 인간이 간과할 수 있는 아이디어와 개선 사항들을 빠르게 반복하고 탐색하는 동시에, 무차별 대입(brute force) 방식보다 훨씬 적은 시도를 통해 효율적인 발견을 가능하게 할 것입니다.

---

**비디오 모델은 제로샷 학습자이자 추론자이다**

관련 연구: Veo 3 비디오 모델 ( [논문](https://arxiv.org/abs/2405.08643) / [프로젝트](https://deepmind.google/discover/blog/veo-3-video-model-is-a-zero-shot-learner-and-reasoner/) )

**어떤 문제를 해결하는가?**
대규모 언어 모델(LLM)은 방대한 규모와 다양한 훈련 데이터 덕분에 명시적으로 학습되지 않은 작업을 해결하는 놀라운 제로샷(zero-shot) 능력을 선보이며 AI 분야에 혁명을 일으켰습니다. 이러한 성공은 자연스럽게 다음과 같은 질문으로 이어집니다. 과연 비디오 생성 모델도 시각 도메인에서 이와 같은 범용적인 문제 해결사가 될 수 있을까? 이전 비디오 모델들은 주로 특정 작업이나 제한된 벤치마크에서만 평가되었기 때문에, 인간과 유사한 광범위한 시각적 추론 능력을 갖추고 있는지 여부는 불분명했습니다. 즉, 비디오 모델이 단순히 패턴을 모방하는 것을 넘어, 시각적 세계를 이해하고 추론할 수 있는 잠재력을 가지고 있는지에 대한 근본적인 의문이 존재했습니다.

**어떻게 문제를 해결하는가?**
이 연구에서는 최첨단 생성형 비디오 모델인 Veo 3를 사용하여 광범위한 시각적 추론 능력을 체계적으로 평가했습니다. 저자들은 고전적인 비전 과제부터 복잡한 물리적 추론, 심지어 도구 사용 시뮬레이션에 이르기까지 62가지의 다양한 작업을 선정하여 테스트를 진행했습니다. 여기서 핵심은 **최소주의적인 프롬프트 기반 접근 방식(minimalist prompt-based approach)**을 사용했다는 점입니다. 모델에는 초기 비디오 프레임 또는 이미지와 함께 "가장자리를 보여줘" 또는 "이 미로를 풀어줘"와 같은 텍스트 지시가 주어지고, 모델은 이에 대한 '답변'으로 8초 분량의 비디오를 생성합니다. 중요한 점은 어떠한 미세 조정(fine-tuning)이나 작업별 훈련(task-specific training)도 없었다는 것으로, 이는 진정한 제로샷 평가(zero-shot evaluation)를 의미합니다. 비디오 모델 자체의 추론 능력을 순수하게 분리하기 위해, 연구진은 이미지 하나만 주어진 독립형 LLM(Google Gemini 2.5)이 해당 작업을 해결할 수 없음을 확인했습니다. 이는 모든 성공적인 결과가 비디오 생성 과정 자체에서 비롯되었음을 입증합니다. 본질적으로, 그들은 비디오 모델이 생성하는 프레임을 통해 시각적으로 단계별로 '생각하도록' 프롬프트하고, 그 결과가 올바른 해결책을 시각적으로 제시하는지 확인하는 방식으로 접근했습니다.

**주요 발견은 무엇인가?**
놀랍게도 Veo 3는 작업별 최적화(task-specific optimization) 없이도 광범위한 새로운 기술을 보여줍니다. 객체를 **분할(segment objects)**하고, **가장자리를 감지(detect edges)**하며, **이미지 편집**(예: 객체 제거)을 수행하고, **물리적 속성**(예: 움직임을 통해 객체의 질량 추론)을 추론하고, **객체 어포던스(affordances)**(객체가 어떻게 사용될 수 있는지)를 인식하며, 심지어 **도구 사용을 시뮬레이션(simulate tool use)**할 수 있습니다. 이 모든 것이 제로샷(zero-shot)으로 가능합니다. 이러한 지각 및 조작 능력은 더 높은 수준의 **시각적 추론(visual reasoning)**을 가능하게 합니다. 예를 들어, Veo 3는 자신이 생성하는 비디오 내에서 해결 경로를 내부적으로 '상상'함으로써 미로와 대칭 퍼즐을 해결합니다. 정량적으로, 테스트된 62가지 다양한 작업에서 이 모델은 저수준 비전 작업(예: 에지 감지 92%, 이미지 노이즈 제거 100%)과 물리적 추론과 같은 더 인지적인 작업 모두에서 높은 성공률을 달성했습니다. Veo 3는 또한 이러한 작업에서 이전 버전(Veo 2)보다 명확한 개선을 보여주었으며, 이는 이러한 기능이 모델/버전 개선과 함께 확장되었음을 나타냅니다. 이 모든 것은 비디오 모델이 충분한 규모와 훈련이 주어지면 LLM과 유사한 궤적을 따르고 있으며, 훈련 범위를 넘어 수많은 작업을 처리할 수 있도록 프롬프트될 수 있는 범용 **'비전 파운데이션 모델(vision foundation models)'**이 되고 있음을 시사합니다.

**다음 단계는 무엇인가?**
이 연구는 미래의 AI가 **통합된 멀티모달 파운데이션 모델(unified multimodal foundation models)**에 의존할 수 있음을 강력히 시사합니다. 하나의 LLM이 다양한 언어 작업을 처리할 수 있듯이, 하나의 비디오 모델도 수많은 비전 관련 작업을 처리할 수 있게 될 것입니다. 다음 핵심 단계는 LLM의 발전을 이끌었던 프롬프트 엔지니어링(prompt engineering) 및 표준화된 평가(standardized evals) 방식과 유사하게, 이러한 비디오 모델을 위한 프롬프트 기술과 벤치마크를 정교하게 개선하는 것입니다. 연구자들은 또한 명시적인 추론 단계, 예를 들어 텍스트 기반의 계획과 비디오 생성을 결합하는 방식이 복잡한 작업에서 성능을 더욱 향상시킬 수 있는지 탐구할 것입니다. 응용 측면에서, 이처럼 강력한 제로샷 비디오 추론기는 혁신적인 파급력을 가질 수 있습니다. 행동하기 전에 결과를 시각적으로 추론하는 로봇 보조원이나, 즉석에서 물리 실험을 시뮬레이션하는 과학 모델을 상상해 볼 수 있습니다. 궁극적으로 언어 및 비디오 모델의 기능 융합은 **범용 AI 시스템(generalist AI systems)**이라는 더 넓은 추세를 암시하며, 이러한 능력이 어떻게 그리고 왜 나타나는지, 즉 훈련 데이터나 아키텍처(architecture)의 어떤 요소가 도구 사용 시뮬레이션으로 이어지는지 이해하는 것은 기초 연구를 위한 흥미로운 질문들을 던집니다.

---

**사전 학습 데이터 기반 강화 학습(Reinforcement Learning on Pre-Training Data, RLPT)**

관련 연구: RLPT ( [논문](https://arxiv.org/abs/2405.08643) )

**어떤 문제를 해결하는가?**
LLM에 더 많은 텍스트를 공급하여 규모를 확장하는 것은 병목 현상에 부딪혔습니다. 컴퓨팅 자원은 쉽게 늘릴 수 있지만, 고품질 텍스트 데이터는 유한합니다. 더욱이 단순히 다음 토큰(token)을 예측하는 것(표준 훈련)만으로는 모델이 복잡한 의존성을 통해 추론하는 방법을 가르치지 못할 수 있습니다. 모델이 데이터 분포(distribution)를 넘어 탐색하도록 장려되지 않기 때문입니다. RLHF와 같은 이전 방법은 일부 신호를 추가하지만 비용이 많이 드는 인간 피드백을 필요로 합니다. 요컨대, 우리는 인간 주석자(annotator) 군단 없이도 LLM이 이미 가지고 있는 데이터로부터 더 많이 학습하고, 특히 추론 기술을 습득할 수 있는 방법이 필요합니다.

**어떻게 문제를 해결하는가?**
RLPT는 LLM의 기존 사전 학습(pre-training) 코퍼스(corpus)를 강화 학습(reinforcement learning)을 위한 동적이고 상호작용적인 훈련 환경으로 재정의합니다. 이 접근 방식은 '다음 세그먼트(next-segment) 예측'이라는 전통적인 작업을 순차적 의사결정 문제(sequential decision problem)로 전환합니다. 구체적으로, 모델은 주어진 텍스트 컨텍스트(context)를 기반으로 다음 텍스트 덩어리를 생성하고, 별도의 생성형 보상 모델(Generative Reward Model) 또는 암시적인 정답 신호가 해당 생성이 코퍼스 내의 실제 연속과 얼마나 잘 일치하는지에 따라 보상을 제공합니다. 이 과정에서 LLM은 실제 텍스트를 최적의 행동 시연으로 간주하여, 다양한 텍스트 연속을 탐색하고 인간의 수동적인 레이블(label) 없이도 피드백을 얻습니다. 이러한 보상은 사전 학습 데이터에서 직접 파생되므로, 수작업으로 제작된 보상이나 인간 선호도 모델에 대한 의존성을 제거합니다. 결과적으로 RLPT는 모델이 레이블 없는 데이터에서 자율적으로 추론 능력을 연습하고 향상시킬 수 있도록 합니다. 모델은 즉각적인 다음 토큰 예측을 넘어, 더 높은 보상(즉, 텍스트와의 전반적인 일관성 향상)으로 이어지는 장기적인 경로를 탐색하도록 학습됩니다. 이 훈련 시간 탐색은 수십억 개의 토큰 규모로 신중하게 확장되어, 정책(policy)이 광범위한 텍스트 도메인에서 더 풍부하고 복잡한 추론 전략을 발견하도록 돕습니다.

**주요 발견은 무엇인가?**
40억 매개변수 기본 LLM(Qwen3-4B)에 RLPT를 적용한 결과, 여러 까다로운 벤치마크에서 성능이 극적으로 향상되었습니다. 예를 들어, MMLU(지식 시험)에서 모델 점수가 **3.0점**, MMLU-Pro(고급 버전)에서는 **5.1점** 향상되었습니다. 수학 및 논리 중심 작업에서는 훨씬 더 큰 향상이 관찰되었습니다. QA 벤치마크(GPQA-Diamond)에서 **8.1점**, AIME24(수학 경시 문제)에서 **6.6점**이라는 인상적인 개선을 보였습니다. 이는 이미 강력한 기본 모델에 대한 절대적인 성능 향상이며, 인간이 레이블링한 데이터나 작업별 미세 조정(finetuning) 없이 달성되었다는 점에서 주목할 만합니다. 또한, 스케일링 연구에 따르면 RLPT에 더 많은 컴퓨팅 자원(더 많은 훈련 단계)을 할당할수록 모델은 지속적으로 개선되어, 더 큰 예산으로 더 큰 성과를 얻을 수 있음을 시사합니다. 저자들은 RLPT로 훈련된 모델이 더 강력한 **일반화 가능한 추론(generalizable reasoning)** 능력을 보인다고 강조합니다. 이 모델은 복잡한 프롬프트(prompt)를 처리하는 능력을 확장하고, 기존 검증 기반 강화 학습(RLVR)과 함께 사용될 때 성능을 더욱 향상시키는 시너지를 보여주었습니다. 요약하자면, RLPT는 우리가 이미 가지고 있는 텍스트에서 훨씬 더 많은 추론 신호를 추출하여 데이터 부족이라는 한계를 허물고, 수동적인 사전 학습 데이터를 능동적인 학습 경험으로 효과적으로 전환하는 유망한 경로를 제공합니다.

**다음 단계는 무엇인가?**
RLPT의 자기 지도형 보상(self-supervised rewards) 성공은 앞으로 더 많은 **하이브리드 훈련 체제(hybrid training regimes)**를 위한 길을 열어줍니다. 미래의 LLM은 인간의 직접적인 개입 없이 텍스트, 게임 환경 또는 시뮬레이션에서의 수동적인 정보 습득과 능동적인 탐색을 번갈아 수행하며 학습할 수 있을 것입니다. 즉각적인 후속 연구 중 하나는 RLPT를 더 큰 모델(예: 340억, 700억 매개변수)과 더 다양한 도메인에 적용하는 것입니다. 코드 생성이나 멀티모달(multimodal) 데이터에 대한 추론도 이와 유사하게 향상될 수 있을지 탐구될 것입니다. 보상 모델링(reward modeling)을 개선할 여지도 있습니다. 현재의 다음 세그먼트 보상은 텍스트에서 자동으로 파생된 논리적 일관성 또는 사실적 정확성 지표를 통합하여 더욱 정교해질 수 있습니다. 이러한 지표가 RLPT에 통합될 수 있다면, 모델은 자체적으로 일관성과 진실성을 검증하며 학습하는 능력을 갖추게 될 것입니다. 큰 그림에서 RLPT는 단순히 모방하는 것을 넘어 미리 생각하는(토큰 계획) 것을 학습하는 언어 모델(LM)의 더 넓은 추세의 일부이므로, 이를 사고의 나무(tree-of-thought) 또는 훈련 중 도구 사용과 같은 기술과 결합하는 연구를 기대할 수 있습니다. 이 모든 것은 인터넷 텍스트를 흡수하는 것을 넘어, 문제를 해결하여 자료를 더 잘 이해하는 학생처럼, 텍스트를 적극적으로 연습하고 일반화하는 LLM을 향해 나아가고 있습니다.

---

**소프트 토큰, 단단한 진실**

관련 연구: 연속적 사고의 사슬(Continuous CoT) ( [논문](https://arxiv.org/abs/2405.08643) )

**어떤 문제를 해결하는가?**
사고의 사슬(chain-of-thought, CoT) 프롬프트(prompting)는 LLM이 단계별 추론 과정을 생성하도록 유도하여 성능을 향상시키는 효과적인 방법입니다. 그러나 CoT는 본질적으로 이산적인 자연어 토큰(discrete natural language tokens)을 사용하는데, 이는 추론을 위한 가장 효율적인 내부 표현이 아닐 수 있습니다. 반면 연속 토큰(continuous tokens)은 이산적인 어휘(discrete vocabulary)의 제약에서 벗어나, 이론적으로 훨씬 더 큰 표현력을 가지며 여러 아이디어를 동시에 인코딩(encode)할 수 있습니다. 이론적으로 연속적인 공간에서 '생각하는' 모델은 한 번에 하나의 추론 경로가 아닌, 여러 추론 경로를 병렬로(중첩(superposition) 형태로) 탐색할 수 있는 잠재력을 가집니다. 문제는 LLM이 추론 과정에서 연속적인 비언어 토큰을 효과적으로 사용하도록 훈련하는 것이 매우 어렵다는 점입니다. 과거 시도들은 추론 시에만 연속 토큰을 주입하거나(훈련 없이), 인간이 작성한 알려진 추론 사슬에서 지식 증류(distilling)를 요구했는데, 이는 번거롭고 짧은 사슬에만 국한되는 한계가 있었습니다. 지금까지 모델이 유용한 연속적 사고의 사슬(CoT)을 처음부터 확장 가능한 방식으로 학습하는 방법을 성공적으로 보여준 연구는 없었습니다.

**어떻게 문제를 해결하는가?**
이 연구는 어떠한 정답 인간 근거(ground-truth human rationales)에도 의존하지 않고 강화 학습(reinforcement learning)을 통해 연속적 사고의 사슬(CoT)을 훈련하는 최초의 성공적인 방법을 제시합니다. 아이디어는 모델이 프롬프트(prompt)와 최종 답변 사이에 '소프트(soft)' 토큰(연속 임베딩(continuous embeddings))을 생성하도록 하고, 보상 신호(reward signal)를 사용하여 그 사용을 최적화하는 것입니다. 구체적으로, 그들은 탐색의 한 형태로 입력 임베딩에 소량의 노이즈(noise)를 추가한 다음, 정책 경사 강화 학습(policy-gradient RL)을 사용하여 최종 답변이 정확하면 모델에 보상을 줍니다. 본질적으로 모델은 더 나은 문제 해결 결과로 이어지는 자체적인 내부 언어(연속 토큰과 그것이 나타내는 것)를 발명하려고 노력합니다. 이산적인 사슬에 대한 어떠한 지도 학습(supervised training)도 피함으로써, 이러한 소프트 토큰이 할 수 있는 것을 제한하는 인간의 편향이 없습니다. 특히, 이 접근 방식은 최소한의 계산 오버헤드(computational overhead)를 추가하므로, 훈련 중 추론 단계에서 모델이 수백 개의 연속 토큰을 사용하도록 허용할 수 있습니다. 이는 이전 증류(distillation) 방법이 허용했던 것보다 훨씬 더 많은 '사고 용량'입니다.

**주요 발견은 무엇인가?**
수학 추론 벤치마크에서 이 연속적 사고의 사슬(CoT) 기술로 훈련된 LLM은 전통적인 이산적 사고의 사슬을 사용하는 모델과 동등하거나 더 나은 성능을 달성했습니다. 예를 들어, GSM8K 수학 문제에서 연속적 CoT를 사용하는 Llama-7B 모델은 단일 최적 답변(pass@1)을 고려할 때 표준(이산적) CoT를 사용하는 동일 모델의 정확도와 일치했습니다. 그러나 여러 답변을 샘플링(pass@32)할 수 있도록 허용했을 때, **연속적 CoT 모델은 이산적 CoT 모델을 능가했으며**, 이는 올바른 답변으로 이어지는 더 다양한 추론 경로를 찾았음을 나타냅니다. 이는 연속 토큰의 큰 장점 중 하나를 보여줍니다. 즉, 더 풍부한 다양한 해결책을 포착할 수 있으며, 여러 출력을 시도할 수 있을 때 그 가치를 발휘합니다. 흥미롭게도, 저자들은 최적의 전략이 하이브리드(hybrid) 방식임을 발견했습니다. 즉, 연속 토큰으로 훈련하되, **추론 시에는 이산 토큰을 사용하는 것입니다**. 다시 말해, 훈련 중에는 모델이 벡터(vector)로 생각하여 이점을 얻게 하지만, 배포 시에는 필요하다면 일반 텍스트 근거를 출력할 수 있습니다. 훈련은 여전히 잠재적 추론 능력을 향상시켰습니다. 더욱이, 연속적 CoT 훈련은 모델의 다른 능력에 대한 간섭을 덜 일으켰습니다. 모델은 이산적 CoT로 훈련된 모델보다 관련 없는 작업에서 정확도를 더 잘 유지했으며, 이는 이 접근 방식이 추론 데이터에 과적합(overfitting)되는 것을 피하는 '더 부드러운' 방식임을 의미합니다. 종합적으로 볼 때, 이는 LLM이 실제 문제 해결 이득을 가져오는 자체적인 비인간 가독 사고 벡터(non-human-readable thought vectors)를 개발할 수 있다는 개념 증명(proof-of-concept)입니다.

**다음 단계는 무엇인가?**
LLM을 벡터(vector)로 생각하도록 훈련하는 것은 많은 흥미로운 연구 방향을 제시합니다. 한 가지 즉각적인 질문은 이러한 학습된 연속 토큰을 어떻게 해석하거나 시각화할 것인가입니다. 그것들이 인간이 이해할 수 있는 개념에 직접적으로 대응하는가, 아니면 완전히 이질적이지만 효과적인 새로운 형태의 추론인가? 또한 연속적 CoT를 멀티모달 추론(multimodal reasoning)으로 확장할 가능성도 있습니다. 예를 들어, 추론 과정에서 '소프트 시각 토큰(soft visual tokens)'으로 이미지를 내부적으로 표현하는 LLM을 상상해 볼 수 있습니다. 여기서 강화 학습(reinforcement learning)의 성공은 논리적 일관성 검사 또는 사실 확인과 같은 다른 보상 신호를 사용하여 연속적인 사고를 형성함으로써 더욱 신뢰할 수 있는 추론을 생성하는 데 영감을 줄 수 있습니다. 실제로 우리는 모델이 연속 공간에서 고강도 추론을 수행한 다음, 그 결과를 인간을 위한 간결한 설명으로 증류하는 하이브리드 시스템을 보게 될 것입니다. '소프트' 모델의 최종 답변이 표준적인 텍스트 형식으로 실행될 수 있다는 사실은 이러한 기술의 채택을 용이하게 합니다. 예를 들어, 수학 튜터 LLM은 연속적 CoT를 조용히 사용하여 어려운 증명을 알아낸 다음, 이를 깔끔한 자연어로 답변으로 제시할 수 있습니다. 전반적으로 이 연구는 LLM에서 더 효율적이고 다양한 추론을 위한 기반을 마련하며, 기존의 사고의 사슬(chain-of-thought)이 가지고 있던 이산 토큰 기반 추론의 일부 한계를 잠재적으로 극복할 수 있는 길을 제시합니다.

---

**사고 증강 사전 학습(Thinking Augmented Pre-Training, TPT)**

관련 연구: TPT ( [논문](https://arxiv.org/abs/2405.08643) )

**어떤 문제를 해결하는가?**
대규모 언어 모델(LLM)을 위한 고품질 훈련 데이터는 그 양이 제한적이며, 언어에 내재된 일부 복잡한 패턴은 단순히 다음 단어 예측이라는 표준적인 훈련 방식만으로는 모델이 효과적으로 학습하기 어렵습니다. 종종 진술의 근거나 문장 간의 논리적 연결은 텍스트에 명시적으로 나타나기보다는 암묵적으로 가정되거나 숨겨져 있습니다. 이러한 '고품질 토큰', 예를 들어 수학 증명의 중요한 단계나 코드의 숨겨진 논리적 흐름과 같은 정보는 모델이 학습하기 매우 어려운 대상이 됩니다. 그 결과는 데이터 비효율성으로 나타나는데, 수십억 개의 단어를 훈련시켜도 모델은 중간 사고 과정을 전혀 보지 못하기 때문에 다단계 추론이나 더 심오한 이해에 여전히 어려움을 겪을 수 있습니다. 여기서 다루는 핵심 과제는 **숨겨진 추론을 명시적으로 드러냄으로써 제한된 훈련 데이터를 훨씬 더 효과적으로 활용하는 방법**을 찾는 것입니다.

**어떻게 문제를 해결하는가?**
TPT는 사전 학습(pre-training) 코퍼스(corpus)를 '사고 경로(thinking trajectories)'로 증강함으로써 이 문제를 해결합니다. 이는 본질적으로 단계별 추론 과정, 설명 또는 중간 '생각' 콘텐츠를 생성하여 원본 텍스트와 함께 삽입하는 방식입니다. 예를 들어, 원본 텍스트가 "학생이 문제를 풀고 답 42를 얻었다"고 되어 있다면, 사고 경로에는 학생이 그 답에 도달하기 위해 거친 수학적 단계나 논리적 추론이 포함될 수 있습니다. 이러한 사고 경로는 광범위한 작업 및 도메인에 걸쳐 자동으로 생성될 수 있으며(강력한 LLM 또는 휴리스틱(heuristics)을 프롬프트(prompting)하여 활용), 훈련 시 원본 데이터와 함께 모델에 주입됩니다. 이로써 TPT는 유효 데이터 볼륨을 증가시킬 뿐만 아니라(새로운 토큰을 추가하므로), 결정적으로 복잡한 토큰의 기저에 깔린 근거를 분해하여 모델이 학습하기 쉽게 만듭니다. 이 방법은 '범용적'입니다. 제한된 데이터로 처음부터 사전 학습하거나, 이미 큰 코퍼스를 증강하거나, 심지어 오픈소스 모델을 중간에 훈련하여 추가로 개선하는 등 다양한 훈련 설정에 적용될 수 있습니다. 각 경우에 명시적인 추론 사슬의 존재는 모델이 동일한 양의 원본 텍스트로부터 더 잘 일반화(generalize)하도록 돕습니다.

**주요 발견은 무엇인가?**
모델 크기와 훈련 설정 전반에 걸쳐 TPT는 상당한 성능 향상을 가져왔으며, 이는 데이터 효율성(data efficiency) 측면에서 큰 성공을 의미합니다. 특히 저자들은 TPT가 사전 학습(pre-training)의 데이터 효율성을 **3배 향상시킨다**고 보고합니다. 실제적으로 이는 TPT 증강을 통해 1000억 개의 토큰으로 훈련된 LLM이 3000억 개의 표준 데이터 토큰으로 훈련된 모델과 비슷하거나 더 나은 결과를 달성할 수 있음을 의미합니다. 30억 매개변수 모델의 경우, 훈련 중 사고 경로(thinking trajectories)를 통합하는 것만으로도 여러 까다로운 추론 벤치마크에서 **10% 이상의 개선**을 보였습니다. 더 큰 모델과 다른 계열(디코더 전용(decoder-only) 및 기타 모델 모두 테스트)도 모두 이점을 얻었으며, 이는 TPT가 견고함을 시사합니다. 중요하게도, 이러한 이득은 특정 틈새 작업에만 국한되지 않습니다. 이 논문은 일반 NLP 벤치마크에서 '다양한 모델 크기와 계열 전반에 걸쳐' 개선이 있었다고 언급합니다. 이는 이 방법이 특정 문제에 과적합(overfitting)되지 않고 광범위한 이해 또는 기술을 주입한다는 것을 의미합니다. 추론을 명시적으로 포함함으로써 모델은 단계별 논리, 수학 단어 문제, 복잡한 QA 등을 요구하는 작업에서 더 나은 성능을 보이며, 표준 언어 작업의 성능을 저해하지도 않습니다. 본질적으로 TPT는 **토큰당 더 많은 사고가 단순히 더 많은 토큰만큼 좋거나(또는 더 좋다는 것을) 보여주며**, 이는 효율적인 훈련을 위한 중요한 결과입니다.

**다음 단계는 무엇인가?**
TPT의 접근 방식은 LLM 훈련을 더욱 의도적이고 구조화하는 최근의 추세와 일치합니다. 미래 연구는 사고 경로(thinking trajectories)의 생성을 더욱 자동화하고 정교화하는 것을 탐구할 수 있습니다. 예를 들어, 하나의 LLM을 사용하여 추론을 생성하고 다른 LLM을 사용하여 훈련에 사용하기 전에 추론의 품질과 정확성을 검증하거나 개선하는 방식입니다. 또한 이를 다른 양식(modality)으로 확장할 가능성도 있습니다. 예를 들어, 이미지 캡션(caption)에 시각적 추론 사슬을 추가하거나, 코드에 프로그램 논리 사슬을 추가하여 학습을 유사하게 향상시키는 것입니다. 즉각적인 실용적 영향 측면에서, 모델을 훈련하는 기업은 TPT를 채택하여 더 적은 데이터로 높은 성능을 달성하거나(또는 동일한 데이터로 더 나은 결과를 얻을 수 있으며), 이는 경제적으로 매우 매력적입니다. 또한 TPT를 RLPT(사전 학습 데이터 기반 강화 학습)와 결합할 수도 있습니다. 먼저 추론으로 데이터를 증강(TPT)한 다음, 모델이 강화 학습(RL)을 통해 해당 데이터를 탐색하도록 하는 것입니다. 이는 자체 개선 AI를 위한 매우 강력한 조합이 될 수 있습니다. 마지막으로, TPT는 양보다는 데이터의 질을 우선적으로 고려하도록 우리에게 촉구합니다. 텍스트의 '숨겨진' 정보에 집중하고 이를 명시적으로 만듦으로써, 우리는 훨씬 더 많은 데이터를 무작정 추가하지 않고도 LLM 능력의 새로운 수준을 발견할 수 있을 것입니다.

---

**SimpleFold: 단백질 접힘은 생각보다 간단하다**

관련 연구: SimpleFold ( [논문](https://arxiv.org/abs/2405.08643) / [코드](https://github.com/google-deepmind/simplefold) )

**어떤 문제를 해결하는가?**
알파폴드(AlphaFold)와 같은 단백질 접힘(protein folding) 분야의 최근 혁신적인 발전은 단백질의 특정 기하학적 특성을 포착하도록 정교하게 설계된 매우 복잡한 모델 아키텍처(architecture)에 기반하고 있습니다. 이러한 아키텍처는 삼각형 어텐션 모듈(triangle attention modules), 쌍별 거리 행렬(pairwise distance matrices), 그리고 여러 맞춤형 손실 항(loss terms)과 같은 단백질 특유의 구성 요소를 포함합니다. 이러한 전문화된 설계는 매우 성공적이었지만, 계산적으로 무겁고 자연어 처리(NLP)나 비전(vision) 분야에서 일반적으로 사용되는 '표준' 아키텍처와는 상당히 다릅니다. 이로 인해 흥미로운 근본적인 질문이 제기됩니다. 과연 단백질 접힘 문제를 해결하는 데 이 모든 도메인별 복잡성이 필수적인가, 아니면 훨씬 더 간단하고 일반적인 모델이 비슷한 정확도로 단백질을 접을 수 있을까? 다시 말해, 단백질 접힘 현상은 현재의 복잡한 모델들이 시사하는 것보다 근본적으로 더 단순한가?

**어떻게 문제를 해결하는가?**
SimpleFold는 단백질 접힘 모델을 가장 기본적인 형태로 되돌리려는 과감한 시도입니다. 이 모델은 특별한 단백질 특정 블록(protein-specific blocks)이 없는 범용 트랜스포머(Transformer) 아키텍처(architecture)를 사용합니다. 기존의 단백질 접힘 모델들이 사용하는 복잡한 트릭(예: 삼각형 업데이트, 아미노산의 별도 2D 쌍 표현 등) 대신, SimpleFold는 표준 자기 어텐션 레이어(self-attention layers)(일부 적응형 게이팅 레이어(adaptive gating layers)로 보강됨)에 의존하며 단백질 구조 데이터에 대해 종단 간(end-to-end)으로 훈련합니다. 핵심 통찰력은 단백질 접힘을 생성 모델링 문제(generative modeling problem)로 간주하는 것입니다. SimpleFold는 확산 모델(diffusion models) 또는 정규화 흐름(normalizing flows)과 관련된 **흐름 일치 목표(flow-matching objective)**로 훈련되어, 무작위 구조를 올바른 접힌 구조로 점진적으로 정제하도록 안내합니다. 그들은 올바른 구조 예측을 장려하기 위한 사소한 추가 손실 항(loss term)을 포함합니다(따라서 순수하게 일반적이지는 않지만 거의 그렇습니다). 그런 다음 이 모델을 30억 매개변수로 확장하고 약 9백만 개의 단백질 구조(증류/예측된 대규모 세트와 실험적으로 해결된 구조 포함)에 대해 훈련합니다. 본질적으로 SimpleFold는 단백질 좌표를 데이터 시퀀스(data sequence)처럼 취급하고, 단백질 생물 물리학 지식을 명시적으로 인코딩(encoding)하지 않고 트랜스포머(Transformer)를 사용하여 이를 올바른 형태로 '흐르게' 하는 방법을 학습합니다.

**주요 발견은 무엇인가?**
SimpleFold-3B의 성능은 표준 단백질 접힘 벤치마크에서 기존의 최첨단 전문 모델들과 충분히 경쟁할 만한 수준을 보여줍니다. 이 모델은 3D 구조 예측에서 경쟁력 있는 정확도를 달성하여, 일반 트랜스포머(vanilla Transformer)가 단백질 접힘에 필요한 복잡한 의존성을 실제로 학습할 수 있음을 효과적으로 입증했습니다. 더욱이 SimpleFold는 결정론적 모델(deterministic models)이 종종 어려움을 겪는 부분에서 강점을 보입니다. 생성형(generative) 모델이기 때문에 자연스럽게 **다양한 확률적 구조의 앙상블(ensemble)**을 생성할 수 있습니다. 이 논문은 앙상블 예측에서 강력한 성능을 언급합니다. 여러 접힘을 샘플링하고 대체 형태(alternative conformations)를 포착할 수 있는데, 이는 단일 답변만을 제공하는 알파폴드(AlphaFold)와 같은 모델에게는 일반적으로 어려운 일입니다. 또 다른 실용적인 이점은 효율성입니다. 더 간단한 아키텍처(architecture) 덕분에 SimpleFold는 배포하기 쉽고 표준 하드웨어에서 더 빠르게 실행됩니다(특수 연산 불필요). SimpleFold의 성공은 단백질 접힘을 위해 고도로 도메인별 설계가 필수적이라는 기존의 개념에 도전하며, 과학 도메인에서 더 많은 기성 AI 구성 요소를 사용할 수 있는 길을 엽니다. 요컨대, 이 논문은 단백질 접힘의 많은 부분이 **일반 시퀀스 모델(generic sequence model)**에 의해 학습될 수 있음을 보여주며, 이는 놀랍고 고무적인 발견입니다.

**다음 단계는 무엇인가?**
SimpleFold의 접근 방식은 과학 문제에 대한 모델 설계 패러다임을 재평가하는 중요한 계기가 될 수 있습니다. 일반 트랜스포머(Transformer)가 단백질 구조 예측에 효과적이라면, 다른 생물학적 또는 화학적 작업(예: 분자 특성 예측, DNA 접힘 시뮬레이션 등)도 올바른 훈련 접근 방식을 통해 더 간단한 아키텍처(architecture)로 전환될 수 있을 것입니다. 향후 연구는 SimpleFold를 약물 발견과 같은 다운스트림 작업(downstream tasks)과 통합하는 방향으로 진행될 수 있습니다. 예를 들어, 약물 결합 예측(drug binding prediction)과 결합하여 생성형 앙상블(generative ensemble) 능력을 통해 여러 단백질 형태(conformations)를 탐색하는 것입니다. 흐름 일치 목표(flow-matching objective)의 사용은 확산 모델(diffusion models)과의 연결점도 시사합니다. 접힘 과정을 시계열(time series)로 시뮬레이션(simulate)하여 정확도를 더욱 향상시키거나 동역학(dynamics)을 포착하는 확산 기반 접힘 모델을 상상해 볼 수 있습니다. 또한 SimpleFold는 표준 AI 모델에 더 가깝기 때문에 **전이 학습(transfer learning)**의 이점을 얻을 수 있습니다. 예를 들어, 언어 모델의 가중치로 초기화하거나 그 반대로 하여 일부 교차 도메인 지식(cross-domain knowledge)을 주입할 수 있습니다(일부 언어 기능이 단백질 시퀀스 분석에 도움이 된다는 초기 추측이 있습니다). 가장 중요한 점은 **단순성이 때로는 복잡성과 동일한 목표를 달성할 수 있다**는 것입니다. 이는 수작업으로 설계된 네트워크가 지배하는 도메인에서 연구자들이 더 '미니멀리스트(minimalist)' 기준선(baselines)을 시도하도록 이끌 수 있는 귀중한 교훈입니다. 이러한 추세가 계속됨에 따라, 과학(단백질 접힘, 화학)과 일반 AI 모두에서 동일한 핵심 모델 유형이 발전을 뒷받침하며, 주로 아키텍처보다는 훈련 데이터에서 차이가 나는 수렴을 보게 될 것입니다.

---

**LLMs4All: 학문 분야 연구 및 응용을 위한 대규모 언어 모델 검토**

관련 연구: LLMs4All 서베이(Survey) ( [논문](https://arxiv.org/abs/2405.08643) )

**어떤 문제를 해결하는가?**
대규모 언어 모델(LLM)의 영향력은 더 이상 컴퓨터 과학에만 국한되지 않고, 역사학부터 생물학에 이르기까지 거의 모든 학문 분야에 걸쳐 빠르게 확산되고 있습니다. 그러나 이러한 광범위한 분야에서 LLM을 효과적으로 활용하는 방법에 대한 지식은 파편화되어 있습니다. 예를 들어, 법학이나 화학 분야의 연구자들은 자신의 전문 분야와 관련된 최신 LLM 기술의 발전 동향을 파악하기 어려울 수 있습니다. 이 논문은 학문 연구의 전 스펙트럼에 걸쳐 최첨단 LLM 응용 사례, 잠재적 기회, 그리고 당면 과제들을 한데 모아 종합적으로 조망하는 포괄적인 서베이(survey)의 필요성을 해결하고자 합니다.

**어떻게 문제를 해결하는가?**
LLMs4All은 학문의 세 가지 광범위한 영역에 걸쳐 LLM이 각 분야에서 어떻게 적용되고 있는지를 상세히 설명하는 광범위한 검토 및 가이드 역할을 합니다. 저자들은 분야를 다음과 같이 분류합니다. (1) 예술, 인문학 및 법학(역사, 철학, 정치학, 건축학, 법학 등), (2) 경제 및 경영(재무, 마케팅, 경영 등), (3) 과학 및 공학(수학, 물리학, 생물학, 화학, 지구과학, 컴퓨터 과학 등). 각 영역에 대해 이 서베이(survey)는 연구 및 실무에서 LLM의 현재 사용 사례를 설명합니다. 예를 들어, 역사 텍스트 분석 지원, 법률 문서 요약 지원, 과학 연구에서 가설 생성 등 해당 도메인의 최첨단 모델 또는 시스템의 예시와 함께 제시됩니다. 또한 텍스트 생성, 추론, 코딩, 다국어 이해와 같은 LLM 기능이 분야별 요구 사항을 충족하기 위해 어떻게 맞춤화되거나 미세 조정(fine-tuned)되는지 논의합니다. 응용 분야 외에도 이 검토는 각 분야의 주요 한계와 과제(의학의 데이터 프라이버시, 역사의 사실 정확성, 법률의 윤리적 문제(예: 편향 또는 공정성) 등)뿐만 아니라 LLM 통합을 위한 미해결 연구 질문 및 미래 방향을 다룹니다. 결과적으로 LLMs4All은 AI의 최전선과 도메인 전문가 사이의 다리 역할을 하며, 'LLM이 X 분야에 무엇을 할 수 있는가'를 한곳에 요약합니다.

**주요 발견은 무엇인가?**
이 서베이(survey)의 주요 기여는 질적 종합이지만, 중요한 통찰력과 관찰을 제공합니다. 한 가지 포괄적인 발견은 사실상 어떤 학문 분야도 LLM의 영향에서 벗어나지 않았다는 것입니다. GPT-4를 사용하여 법률 계약을 초안 작성하는 것부터 화학 실험 설계를 위해 생성 모델을 사용하는 것에 이르기까지, 학계는 LLM 기반 혁신의 물결을 경험하고 있습니다. 그러나 이 검토는 성숙도가 다양하다고 지적합니다. 일부 분야(컴퓨터 과학 및 법학 등)는 이미 수많은 LLM 응용 프로그램을 가지고 있지만, 다른 분야(예: 철학 또는 예술)는 여전히 초기 사용 사례를 탐색하고 있습니다. LLM이 그럴듯하게 들리지만 잘못된 정보(환각(hallucinations))를 생성하여 의학이나 금융과 같은 분야의 비전문가 사용자를 오도할 수 있다는 우려와 같이, 여러 분야에서 공통적인 과제가 나타납니다. 이 논문은 **학제 간 협력(interdisciplinary collaboration)이 핵심**임을 강조합니다. 예를 들어, LLM을 도메인별 지식 기반 또는 모델과 결합하면 더 나은 결과를 얻을 수 있습니다(LLM과 화학 규칙을 사용하는 과학적 발견 도구에서 볼 수 있듯이). 긍정적인 발견은 LLM이 연구에서 **민주화 세력**으로 작용한다는 것입니다. LLM은 고급 기술 훈련이 없는 개인도 자신의 도메인 문제에 AI를 활용할 수 있도록 합니다(예: GPT를 사용하여 고대 텍스트를 번역하고 요약하는 역사학자). 이 서베이(survey)는 또한 커뮤니티가 책임감 있는 LLM 사용(예: AI가 사용된 경우 학술 글쓰기에서의 공개 정책)에 대해 고심하면서 나타나는 모범 사례와 윤리적 지침을 수집합니다. 종합적으로 LLMs4All은 각 분야의 연구자들이 현재 상황을 이해하고 자신의 작업에서 LLM을 어떻게 사용할 수 있는지 파악하기 위한 로드맵을 제공합니다.

**왜 중요한가?**
생성형 AI가 데이터 분석만큼이나 학문 연구의 근본적인 도구가 됨에 따라, 각 학문 분야에서 그 역할에 대한 명확한 시각을 갖는 것이 중요합니다. 이 서베이(survey)는 교육자와 정책 입안자에게도 귀중한 통찰을 제공할 것입니다. 예를 들어, 대학 학과는 해당 분야와 관련된 AI 리터러시(literacy)를 포함하도록 교육과정을 업데이트할 때 이 문서를 참조할 수 있습니다. LLM의 한계와 미래 방향을 명확히 문서화함으로써, 이 논문은 AI 연구자들에게 중요한 미해결 문제(예: 과학 Q&A의 사실성 향상 또는 LLM을 법적 추론과 일치시키는 것)를 제시합니다. 한 가지 중요한 결과는 이 서베이(survey)가 **학제 간 협력(cross-disciplinary collaborations)을 촉진할 것**이라는 점입니다. 화학 분야에서 LLM의 사용 사례를 읽은 생물학자가 AI 전문가와 협력하여 유사한 기술을 생물학 연구에 적용하는 시나리오를 상상해 볼 수 있습니다. 또한 이 논문은 과장된 소문에도 불구하고 현재 LLM이 전문 분야에서 여전히 심각한 단점을 가지고 있음을 강조하며, 따라서 기대치를 합리적으로 조절하고 신뢰성에 대한 더 많은 연구를 장려합니다. 이는 AI의 발전 과정에서 반복되는 중요한 주제입니다. 요약하자면, LLMs4All은 AI와 다른 모든 분야의 교차점에서 일어나고 있는 변화를 기록하고, 지식이 고립되지 않고 광범위하게 공유되도록 보장하며, AI가 진정으로 모든 분야의 도구가 되는 다음 연구 단계를 이끄는 데 도움이 되기 때문에 중요합니다.

---

**생각하는 언어 모델이 더 나은 채팅을 한다**

관련 연구: RLMT ( [논문](https://arxiv.org/abs/2405.08643) / [코드](https://github.com/google-deepmind/rlmt) )

**어떤 문제를 해결하는가?**
인간 피드백 기반 강화 학습(Reinforcement Learning from Human Feedback, RLHF)은 채팅 LLM을 더 유용하고 안전하게 미세 조정(finetune)하는 데 표준적인 방법으로 자리 잡았습니다. 그러나 RLHF는 모델이 제공하는 최종 답변만을 최적화할 뿐, 그 이면에 있는 추론 과정 자체는 직접적으로 최적화하지 않습니다. 모델이 내부적으로 숨겨진 사고의 사슬(chain-of-thought)을 생성할 수 있지만, 보상 모델(reward model)은 오직 최종 응답만을 평가합니다. 이는 그럴듯하게 들리지만 깊이 있게 추론되지 않은 답변으로 이어질 수 있는 한계를 가집니다. 또 다른 접근 방식인 검증 가능한 보상 기반 강화 학습(RL with verifiable rewards, RLVR)은 모델이 검증 가능한 작업(예: 수학 증명 또는 코드 테스트)을 출력하도록 강제하고 정확성에 대해 보상을 줍니다. RLVR은 더 나은 추론을 유도하지만, 객관적으로 검증 가능한 도메인에서만 작동한다는 제약이 있습니다. 여기서 다루는 미해결 문제는 다음과 같습니다. RLHF의 일반성을 유지하면서도, 모델이 실제로 문제를 깊이 생각하고 추론 과정을 개선하도록 장려할 수 있는 방법은 무엇일까요?

**어떻게 문제를 해결하는가?**
해결책은 RLMT(모델 보상 사고 기반 강화 학습, Reinforcement Learning with Model-rewarded Thinking)라는 새로운 훈련 패러다임(paradigm)에 있습니다. RLMT에서는 훈련 과정 동안 모델이 최종 답변을 제시하기 전에 긴 사고의 사슬(chain-of-thought, CoT)을 명시적으로 생성하도록 요구됩니다. 이 CoT는 상세한 개요, 단계별 추론 과정, 또는 중간적인 '생각'의 형태를 띨 수 있습니다. 보상 모델(reward model)(RLHF에서 사용되는 것과 동일하며 인간 선호 데이터로 사전 훈련됨)은 최종 답변뿐만 아니라 CoT와 답변의 전체 조합을 평가합니다. 본질적으로 모델은 좋은 답변으로 이어지는 유용하고 심층적인 추론을 생성한 것에 대해 보상을 받게 됩니다. 연구진은 다양한 실제 프롬프트(prompt)(에세이 작성, 식사 계획, 복잡한 질문 답변과 같은 개방형 작업)를 사용하고 정책 경사 방법(policy gradient methods)(PPO, DPO 등)을 적용하여 LLM의 정책을 최적화함으로써 더 나은 사고+답변 쌍을 출력하도록 합니다. 이 접근 방식의 견고성을 보장하기 위해 두 가지 기본 모델(Llama-3.1 8B 및 Qwen-7B)에 대해 다른 설정에서 40개의 개별 훈련 실행을 수행했습니다. 중요하게도, 그들은 또한 RLMT(R1-Zero)를 사용하여 처음부터 훈련하는 방법을 탐색했습니다. 즉, 지도 미세 조정(supervised fine-tuning) 없이 기본 모델에 RLMT를 직접 적용하여 일반적인 지시 조정(instruction-tuning) 단계를 건너뛸 수 있는지 확인했습니다.

**주요 발견은 무엇인가?**
RLMT로 훈련된 모델은 광범위한 평가에서 표준 RLHF로 훈련된 모델보다 지속적으로 우수한 성능을 보입니다. 예를 들어, 세 가지 다른 개방형 채팅 벤치마크(AlpacaEval2, WildBench, Arena Hard)에서 RLMT 모델은 동등한 RLHF 모델보다 **3~7점 더 높은 점수**를 기록했으며, 이는 품질 면에서 상당한 도약입니다. 또한 창의적 글쓰기 작업 및 지식 퀴즈에서 **1~3점 향상**과 같은 일반적인 능력 개선도 보였습니다. 아마도 가장 인상적인 것은, 그들의 최고의 RLMT로 미세 조정된 80억 매개변수 모델이 해당 채팅 벤치마크 및 창의적 작업에서 GPT-4(오픈 변형)의 성능을 실제로 능가했으며, 심지어 한 벤치마크에서는 Claude 2의 수준에 근접했다는 것입니다. 이는 모델이 GPT-4보다 훨씬 작다는 점을 고려할 때 놀라운 결과입니다. 또 다른 놀라운 발견은 다음과 같습니다. 단 **7,000개의 RLMT 프롬프트(어떠한 지도 미세 조정도 없음)로 훈련된 80억 매개변수 Llama 모델이 2,500만 개의 예시로 지시 조정된 공식 Llama-3.1-8B를 능가했습니다**. 다시 말해, '생각하고 답변하기' 최적화를 통해 신중하게 선택된 수천 개의 시나리오가 대규모의 전통적인 훈련을 이겼습니다. 이는 RLMT의 강력한 효율성을 보여줍니다. 질적으로, 저자들은 RLMT 모델이 더 구조화되고 사려 깊은 응답(예: 목록 작성, 소리 내어 추론, 대안 고려)을 생성하고 주제를 벗어나는 것과 같은 실패 모드가 더 적다는 것을 관찰했습니다. 이러한 결과는 사고 과정에 보상을 주는 것이 최종 답변에만 보상을 주는 것보다 측정 가능하게 더 나은 채팅 성능으로 이어진다는 것을 강력히 시사합니다.

**다음 단계는 무엇인가?**
이 연구는 대화형 에이전트(conversational agents) 훈련 방식에 패러다임(paradigm)의 전환을 가져올 수 있습니다. 사고의 사슬(chain-of-thought)을 단순히 선택적인 부산물로 취급하기보다는, LLM이 자신의 추론 과정을 명시적으로 표현하도록 훈련하는 것이 표준이 될 수 있습니다. 미래 방향에는 RLMT를 **휴먼 인 더 루프(human-in-the-loop)** 방식과 결합하는 것이 포함됩니다. 예를 들어, 최종 답변뿐만 아니라 중간 추론 과정에 대해서도 인간 피드백을 받아 추론 품질에 대한 보상 모델(reward model)을 더욱 정교하게 만드는 것입니다(기존 선호 모델이 할 수 있는 것을 넘어). 또한 RLMT를 더 큰 모델(논문에서는 80억 매개변수 모델을 다루었지만, 340억 또는 700억 매개변수 모델에 적용하면 일부 영역에서 훨씬 더 큰 비공개 모델을 능가할 수 있는 더 강력한 모델을 얻을 수 있습니다). 또 다른 고려 사항은 실제 배포입니다. RLMT 모델은 더 많이 설명함으로써 더 **해석 가능(interpretable)**할 수 있으며, 이는 안전성 측면에서 큰 이점입니다. 그러나 이는 또한 프롬프트되지 않아도 자신의 '생각'을 드러낼 수 있다는 것을 의미하며, 이는 필요에 따라 조정될 수 있습니다. 마지막으로, 이 연구는 RLMT가 왜 그렇게 효과적인지에 대한 심층적인 이해를 요구합니다. 보상 모델이 인간 선호도와 더 잘 일치하는 사고의 사슬(CoT)의 특정 구조를 간접적으로 선호하는가, 아니면 더 긴 컨텍스트(context)를 생성하는 행위 자체가 모델이 실수를 피하는 데 도움이 되는가? 이러한 질문에 답하는 것은 훈련 방법을 더욱 개선할 수 있는 중요한 통찰을 제공할 것입니다. 종합적으로 볼 때, **생각하는 언어 모델은 진정으로 더 나은 채팅을 하며**, 우리는 이러한 기술의 결과로 다음 세대 AI 비서가 추론에서 훨씬 더 명시적일 것으로 기대할 수 있습니다.

---

**SciReasoner: 학문 분야를 아우르는 과학적 추론 기반 마련**

관련 연구: SciReasoner ( [논문](https://arxiv.org/abs/2405.08643) / [코드](https://github.com/google-deepmind/scireasoner) )

**어떤 문제를 해결하는가?**
현재까지 과학 중심 LLM은 대부분 특정 도메인(예: 화학 또는 수학 정리 해결사)에 미세 조정(fine-tuned)된 전문가 모델이었습니다. 그러나 실제 과학 연구는 종종 여러 학문 분야와 데이터 형식(생물학적 발견을 화학 이론과 연결하고, 방정식과 텍스트를 함께 사용하는 것을 상상해 보세요)에 걸쳐 있습니다. 자연어 질문을 이해할 뿐만 아니라 공식, 시퀀스(DNA, 단백질 시퀀스), 속성 테이블 등을 통합된 방식으로 처리할 수 있는 과학적 추론을 위한 파운데이션 모델(foundation model)이 필요합니다. 요컨대, 목표는 한 분야에만 고립되지 않고, 과학이 사용하는 다양한 표현과 함께 광범위하고 학제 간 추론 능력을 갖춘 AI 과학자를 만드는 것입니다.

**어떻게 문제를 해결하는가?**
SciReasoner는 다양한 과학적 표현과 언어를 정렬하기 위한 광범위한 다단계 훈련 과정을 통해 구축됩니다. 첫째, 많은 분야의 과학 텍스트뿐만 아니라 순수하게 상징적인 시퀀스(symbolic sequences)와 혼합 시퀀스-텍스트 데이터(mixed sequence-text data)를 포함하는 2060억 토큰(token) 코퍼스(corpus)로 사전 학습(pre-trained)됩니다. 이는 아미노산 시퀀스, 화학 SMILES 문자열, 수학 방정식과 같은 것들을 설명 텍스트와 함께 본다는 것을 의미합니다. 이 대규모 사전 학습 후, 그들은 **4천만 개의 과학 관련 지시(instruction)**에 대해 지도 미세 조정(supervised fine-tuning)을 수행하여 엄청난 범위의 작업(모델은 **103가지 다른 과학 작업을 지원**)을 다룹니다. 이러한 작업은 다음과 같은 범주로 나뉩니다. (i) 텍스트와 과학 형식 간 번역(예: "이 분자 구조를 말로 설명해라" 및 그 반대), (ii) 텍스트 또는 그림에서 지식 추출, (iii) 속성 예측(주어진 화합물에 대해 녹는점 예측 등), (iv) 속성 분류(예: 데이터에서 별을 적색 왜성으로 분류할지 여부), (v) 시퀀스 생성 또는 설계(특정 속성을 가진 DNA 시퀀스 제안 등). 지도 조정 후, 그들은 과학 문제에 대한 장문 사고의 사슬(long-form chain-of-thought) 추론을 모델에 특별히 가르치기 위해 '어닐링된 콜드 스타트(annealed cold-start)' 부트스트래핑(bootstrapping)을 적용합니다. 이는 모델이 복잡한 질문에 대한 단계별 해결책을 생성하도록 프롬프트(prompting)하고 이를 추가 훈련 데이터로 사용하는 것을 포함할 가능성이 높습니다(점진적으로 복잡성을 증가시키므로 '어닐링된'). 마지막으로, 그들은 과학적 추론을 위한 맞춤형 보상 형성(reward shaping)과 함께 강화 학습(reinforcement learning)을 사용합니다. 이 마지막 단계는 모델에 중간 단계(단위 일관성, 방정식 정확성, 논리적 일관성 등)에 대한 피드백을 제공하여 의도적이고 엄격한 추론 습관을 확고히 심어줄 것입니다. 모든 훈련 아티팩트(모델 가중치, 지시 데이터, 평가 코드)는 공개적으로 배포되어 SciReasoner를 커뮤니티 리소스(resource)로 만듭니다.

**주요 발견은 무엇인가?**
SciReasoner는 이전에는 별도의 도구 앙상블(ensemble)이 필요했던 작업을 단일 모델로 처리할 수 있는 놀라운 능력을 보여줍니다. 전문 모델이나 기존 기준선(baselines)과 비교할 때, SciReasoner는 훨씬 더 넓은 지시 범위, 뛰어난 교차 도메인 일반화(cross-domain generalization) 능력, 그리고 출력에서 더 높은 충실도(fidelity)를 자랑합니다. 예를 들어, 텍스트로 설명된 화학 문제를 받아 방정식과 함께 단계별 해결책을 출력하거나, 유전체 시퀀스(genomic sequence)를 가능성 있는 기능 설명으로 번역하는 등, 언어와 형식 데이터를 능숙하게 연결하는 작업을 놀라운 정확도로 수행할 수 있습니다. 이 논문은 여러 학문 분야를 함께 훈련하는 것이 실제로 **전이 학습(transfer learning)을 향상시켰음**을 강조합니다. 즉, 한 도메인의 작업을 해결하는 과정에서 얻은 지식이 다른 도메인에서의 성능을 향상시켰는데, 이는 모델이 일반적인 과학적 추론 전략을 효과적으로 학습했기 때문입니다. 이러한 교차 수분(cross-pollination)은 모델의 신뢰성을 강화했습니다. 예를 들어, 물리학 방정식에서 학습한 엄격함은 회계 계산과 같은 분야에서 오류를 피하는 데 도움이 되었습니다. 평가에서 SciReasoner는 어떤 단일 분야의 전문가가 아님에도 불구하고 많은 벤치마크에서 도메인별 모델과 동등하거나 더 나은 성능을 보였습니다. 특히, 지식 혼합을 요구하는 과제(생물학과 화학을 모두 포함하는 질문과 같은)에서는 명확한 이점을 가졌습니다. 본질적으로 SciReasoner는 기반을 마련합니다. 즉, **하나의 모델이 동시에 유능한 물리학자, 화학자, 생물학자 등이 될 수 있으며, 이러한 통합이 실제로 각 측면을 더 강하게 만든다**는 것을 보여줍니다. 이는 과학 전체에 걸쳐 추론할 수 있는 AI를 향한 중요한 발걸음입니다. 모델과 데이터의 오픈소스화(open-sourcing) 또한 주요 성과입니다. 이는 과학 QA, 가설 생성 등을 위한 추가 미세 조정(finetune) 또는 벤치마크를 위한 강력한 기반을 커뮤니티에 제공하여 과학 AI 연구를 가속화합니다.

**다음 단계는 무엇인가?**
SciReasoner의 등장은 수많은 새로운 연구 및 응용 가능성을 열어줍니다. 가까운 미래에 연구자들은 이를 기반으로 전문화된 에이전트(agents)를 만들 수 있습니다. 예를 들어, SciReasoner를 사용하여 실험을 설계한 다음, 가상 실험실 시뮬레이션(simulation)에서 이를 실행하는 로봇 과학자를 상상해 볼 수 있습니다. 훈련 기술(대규모 다중 형식 사전 학습(multi-format pre-training) 및 신중한 단계별 정렬(staged alignment)과 같은)은 경제학(텍스트와 스프레드시트 및 공식을 혼합) 또는 사회 과학(텍스트와 통계 데이터를 혼합)과 같이 다중 표현 추론을 요구하는 다른 도메인에 적용될 수 있습니다. 또 다른 유력한 방향은 스케일링(scaling)입니다. 현재의 SciReasoner-8B는 인상적이지만, 유사하게 훈련된 700억 매개변수 모델을 상상해 보세요. 이는 많은 분야에서 인간 전문가 수준에 근접할 수 있는 잠재력을 가집니다. 평가에 대한 작업도 지속될 것입니다. 모델은 103가지 작업을 다루지만, 각 작업에서 추론 품질과 사실적 정확성을 어떻게 철저히 검증할 것인가에 대한 새로운 학제 간 벤치마크(interdisciplinary benchmarks)가 개발될 필요가 있습니다. 마지막으로, SciReasoner의 출시는 개방형 과학 AI 문화를 조성하는 데 기여합니다. 더 많은 연구자들이 이를 사용하고 개선함에 따라, 우리는 모든 교과서를 읽었으며 강력하지만 광범위한 지식을 가진 동료처럼, 어떤 연구자라도 자신의 작업을 향상시키는 데 사용할 수 있는 **'AI 과학 보조원'**으로 이어지는 선순환을 보게 될 것입니다. 장기적인 비전은 학문 분야 간 통찰력을 교차 수분(cross-pollinate)할 수 있는 AI(예: 물리학 원리를 사용하여 생물학 문제를 해결)이며, SciReasoner는 AI에서 광범위한 사고를 위한 광범위한 훈련의 가치를 보여주는 그 방향의 기초적인 단계입니다.

---

❤️ 이 기사가 마음에 드셨다면 '좋아요'를 누르고 동료들과 공유해 주세요.
댓글을 남겨주세요.
LLM Watch를 읽어주셔서 감사합니다!
새로운 게시물을 무료로 받고 제 작업을 지원하려면 구독하세요.
구독하기