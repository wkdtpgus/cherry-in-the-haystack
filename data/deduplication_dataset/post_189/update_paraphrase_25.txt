LLM Watch 독자 여러분, 반갑습니다! 이번 주 LLM Watch에서는 대규모 언어 모델(LLM) 분야의 최신 기술 혁신과 더불어 인공지능, 컴퓨터 비전, 기초 과학 등 다양한 영역에서의 활용 사례를 심층적으로 탐구합니다. 주목할 만한 주요 소식은 다음과 같습니다:

*   LLM을 활용하여 전례 없이 높은 샘플 효율성으로 새로운 알고리즘을 개발하는 오픈소스 진화 프레임워크인 ShinkaEvolve에 대한 탐구.
*   LLM이 원시 사전 훈련(pre-training) 데이터만으로 추론 능력을 스스로 학습하게 하여 특정 벤치마크에서 최대 8점의 성능 개선을 이끌어낸 자기 지도형 강화 학습(RLPT) 방법론.
*   생성형 비디오 모델이 객체 분할(segmentation)부터 복잡한 물리적 추론(physical reasoning)에 이르기까지 총 62가지 작업을 제로샷(zero-shot) 방식으로 해결할 수 있음을 입증하며, 범용 비전 모델의 잠재력을 시사하는 연구 결과.
*   LLM이 사고의 사슬(chain-of-thought)을 명시적으로 생성하도록 유도하는 '생각하고 말하기' 훈련(RLMT) 방식은 단 7천 개의 훈련 프롬프트만으로도 최상위 수준의 대화 성능을 구현합니다.
*   강화 학습(RL)을 통해 추론 과정에 연속적인 '소프트(soft)' 토큰을 활용하도록 모델을 훈련하는 혁신적인 방법은 기존 모델의 역량을 보존하면서 수학 문제 해결에서 이산적인 사고의 사슬(discrete chain-of-thought)보다 뛰어난 성과를 보였습니다.
*   이 밖에도 흥미로운 내용들이 준비되어 있습니다!

자세한 내용은 아래 용어집을 참조하시거나, 바로 논문 요약 섹션으로 이동하여 확인해 보세요.

LLM Watch 독자분들을 위해 텍사스 오스틴에서 개최되는 제6회 MLOps World | GenAI 글로벌 서밋에 특별 초대권을 마련했습니다. 이 행사에는 OpenAI, HuggingFace 등 선도적인 기업들이 참여하며, 60개 이상의 세션이 준비되어 있습니다. 구독자 여러분께서는 여기에서 무료로 온라인 참석이 가능합니다. 또한, 오스틴 현지에서 진행되는 실용적인 워크숍, 실제 사용 사례 발표, 그리고 다양한 네트워킹 기회 및 파티에 직접 참여하고 싶으시다면, 이 코드를 사용하여 150달러 할인 혜택을 받으세요! 이번 서밋은 AI 분야의 최신 동향을 파악하고 전문가들과 교류할 수 있는 절호의 기회가 될 것입니다.

**150달러 할인**

**핵심 용어 설명**

**사전 훈련 데이터 활용 강화 학습(Reinforcement Learning on Pre-Training data, RLPT):** 대규모 언어 모델(LLM)이 기존의 사전 훈련(pre-training) 데이터를 다음 토큰 '경로(trajectories)'를 탐색하며 강화 학습(RL) 방식으로 배우는 학습 방식입니다. 이 방법은 실제 이어질 텍스트 세그먼트(segment)를 예측하는 과정에서 직접 보상을 얻기 때문에, 인간의 수동적인 주석(annotation) 작업(RLHF와는 차이)이 필요 없습니다. 이에 따라 모델은 가공되지 않은 텍스트 자료만으로도 자체적인 추론 역량을 키워나갈 수 있습니다.

**검증 가능한 보상 기반 강화 학습(Reinforcement Learning with Verifiable Rewards, RLVR):** 수학 문제나 프로그래밍 코드와 같이 정확성을 자동으로 확인할 수 있는 영역에서 규칙 기반 또는 프로그램적인 보상 체계를 사용하는 강화 학습(RL) 기법입니다. 이는 검증 가능한 분야에서 추론 능력을 향상시키는 데 효과적이지만, 창의적 글쓰기나 복잡한 계획 수립과 같은 개방형 작업에서는 그 활용 범위가 제한적입니다.

**모델 보상 사고 기반 강화 학습(RL with Model-rewarded Thinking, RLMT):** 모델이 최종 답변을 제시하기 전에 길고 상세한 사고의 사슬(chain-of-thought)을 생성하도록 요구하는 새로운 강화 학습(RL) 프로세스입니다. 별도의 보상 모델(reward model)(RLHF에서 사용되는 것과 유사)이 이러한 추론 과정을 평가하며, 이를 통해 기본 모델은 더 심도 있게 숙고된 답변을 생성하도록 최적화됩니다. 이러한 '생각하고 응답하기' 전략은 여러 벤치마크에서 큰 성능 향상을 보여주며, 표준 RLHF보다 강력한 범용 채팅 능력을 이끌어냅니다.

**사고 증강 사전 훈련(Thinking Augmented Pre-Training, TPT):** LLM의 데이터 효율성을 높이기 위해 '사고 경로(thinking trajectories)'(단계별 추론 과정)를 텍스트 훈련 데이터에 통합하는 방법입니다. 텍스트에 자동 생성된 근거(rationale)를 추가함으로써 TPT는 각 토큰(token)의 가치를 실질적으로 3배 증가시키며, 모델이 훨씬 적은 예시로 복잡한 개념을 습득하도록 돕습니다(예: 30억 매개변수 모델이 추론 작업에서 10% 이상 높은 점수 달성).

**연속적 사고의 사슬('소프트' 토큰)(Continuous Chain-of-Thought, “Soft” tokens):** LLM의 내부 추론 과정에 이산적인 토큰(discrete tokens) 대신 연속적인 임베딩(continuous embeddings)을 사용하는 방식입니다. 일반적인 단어 토큰과 달리, 이러한 소프트 토큰은 다양한 추론 경로의 중첩(superposition)을 표현할 수 있습니다. 최근 연구에 따르면, LLM은 이산적인 지침 없이도 수백 개의 연속적인 사고의 사슬(CoT) 토큰을 활용하도록 강화 학습(RL)을 통해 훈련될 수 있으며, 이는 더욱 풍부한 추론으로 이어집니다. 모델은 한 번의 시도에서 이산 CoT의 정확도와 동등한 성능을 보였고, 여러 시도를 통해 더 다양한 해결 경로를 탐색하여 이를 능가했습니다.

---

**ShinkaEvolve: 개방형 및 샘플 효율적인 프로그램 진화를 향한 진보**

관련 연구: ShinkaEvolve ( [논문](https://arxiv.org/abs/2405.08643) / [코드](https://github.com/ShinkaEvolve/ShinkaEvolve) )

**어떤 문제를 해결하는가?**
최근 대규모 언어 모델(LLM)을 활용하여 코드를 진화시키는 방법론(예: 최적화 문제 해결 또는 특정 프로그래밍 과제 수행)은 극도로 **샘플 비효율성(sample-inefficient)**이라는 한계를 가집니다. 즉, 만족스러운 해결책을 찾기 위해 수천 번의 시도가 필요할 때가 많습니다. 더군다나 이러한 시스템들은 대부분 비공개 소스(closed-source)로 운영되어, 광범위한 연구자들의 접근성을 저해합니다. 결과적으로, LLM 기반의 혁신적인 발견은 대다수의 연구자에게 너무나 값비싸고 접근하기 어려운 영역으로 남아있었습니다.

**어떻게 문제를 해결하는가?**
ShinkaEvolve는 대규모 언어 모델(LLM)을 프로그램의 성능을 체계적으로 향상시키는 지능형 '변이 주체(mutator agent)'로 활용하는 오픈소스 프레임워크입니다. 이 접근 방식의 핵심은 기존 진화 알고리즘의 비효율성을 극복하는 데 있습니다. 예를 들어, 무작위적인 변이 대신, LLM은 코드의 의미론적 맥락을 이해하고 목표에 부합하는 변이를 제안함으로써 탐색 공간을 효과적으로 줄입니다. 이는 마치 경험 많은 개발자가 버그를 수정하거나 기능을 추가할 때 무작정 시도하는 것이 아니라, 문제의 본질을 파악하고 가장 유망한 해결책에 집중하는 것과 유사합니다. 특히, 프레임워크 내에서 적용되는 '참신성 기반 거부 샘플링(novelty-based rejection sampling)'은 불필요한 반복을 줄이고 진정한 혁신적인 코드 변형에만 자원을 할당하여, 진화 과정의 효율성을 극대화합니다. 이와 더불어 각 세대에서 최적의 LLM을 선별하는 밴딧 알고리즘(bandit algorithm)은 지속적인 성능 향상을 보장합니다. 이러한 지능적인 변이와 선별 메커니즘의 결합은 ShinkaEvolve가 기존의 단순한 탐색 방식으로는 도달하기 어려웠던 최적의 솔루션을 훨씬 적은 시도로 찾아낼 수 있게 합니다.

**주요 발견은 무엇인가?**
ShinkaEvolve는 프로그램 진화의 효율성을 획기적으로 개선함으로써, 이전에는 실현 불가능했던 광범위한 문제 영역에서 성공적인 결과를 도출했습니다. 이 프레임워크는 단 150개의 샘플만으로 고전적인 26개 원형 패킹 문제(26-circle packing problem)에 대한 새로운 최첨단 해결책을 찾아냈는데, 이는 기존 방식 대비 **'효율성의 경이로운 도약'**으로 평가됩니다. 또한, 수학 경시 대회(AIME 벤치마크)를 위한 고성능 다중 에이전트(multi-agent) 전략을 단 75세대 만에 진화시켜, 강력한 인간 설계 기준선(baseline)을 능가하는 성과를 보였습니다. 경쟁 프로그래밍 분야에서도 ShinkaEvolve가 AtCoder 경시 대회 에이전트에 적용한 개선 사항은 매우 중요하여, 한 문제에서는 진화된 해결책이 대회에서 **2위를 차지할 수 있었을 것**으로 분석됩니다. 나아가 ShinkaEvolve는 대규모 전문가 혼합(Mixture-of-Expert) LLM 훈련을 위한 더 나은 방법을 찾아냈습니다. 새로운 부하 분산 손실(load-balancing loss)을 통해 DeepMind의 'Global LBL' 기준선을 **1.73% 더 높은 작업 점수와 5.8% 더 적은 낭비 용량으로** 능가했습니다. 이러한 결과들은 광범위한 **'개방형(open-ended)' 발견이 이제 합리적인 비용으로 가능**해졌으며, 과학자와 엔지니어에게 새로운 해결책을 자율적으로 탐색할 수 있는 AI 기반 공동 조종사(co-pilot)를 제공함을 명확히 보여줍니다.

**다음 단계는 무엇인가?**
이 연구는 과학 및 공학 분야의 난제를 해결하는 데 있어 LLM 기반의 진화적 탐색이 매우 효율적인 경로임을 시사합니다. ShinkaEvolve의 오픈소스화와 웹 UI(Web UI) 제공은 이러한 접근 방식의 대중화를 목표로 합니다. 향후 연구는 이 방법을 새로운 도메인, 예를 들어 회로 설계, 새로운 과학 공식 발견, 또는 하이퍼파라미터(hyperparameters) 최적화 등에 적용하는 방향으로 확장될 것입니다. 또한, 더욱 발전된 LLM이 등장함에 따라 이를 통합하여 ShinkaEvolve의 역량을 강화하는 연구도 중요합니다. 장기적으로 ShinkaEvolve와 같은 기술은 단순한 자동화 도구를 넘어선 **'AI 연구 보조원'** 역할을 수행할 것입니다. 이는 인간이 간과할 수 있는 아이디어나 개선점을 빠르게 반복적으로 탐색하면서도, 무차별 대입(brute force) 방식보다 훨씬 적은 자원으로 최적의 해결책을 찾아내는 혁신적인 연구 패러다임을 제시합니다. 이러한 시스템은 미래의 과학적 발견과 기술 혁신을 가속화하는 핵심 동력이 될 잠재력을 가지고 있습니다.

---

**비디오 모델: 제로샷 학습 및 추론의 새로운 지평**

관련 연구: Veo 3 비디오 모델 ( [논문](https://arxiv.org/abs/2405.08643) / [프로젝트](https://deepmind.google/discover/blog/veo-3-video-model-is-a-zero-shot-learner-and-reasoner/) )

**어떤 문제를 해결하는가?**
대규모 언어 모델(LLM)은 그 방대한 규모와 다양한 훈련 데이터 덕분에 명시적으로 학습되지 않은 작업까지 해결하는 놀라운 제로샷(zero-shot) 능력을 선보이며 세상을 놀라게 했습니다. 이러한 현상은 자연스럽게 "비디오 생성 모델도 시각 도메인에서 이처럼 범용적인 문제 해결사가 될 수 있을까?"라는 질문으로 이어집니다. 기존 비디오 모델들은 주로 특정 작업이나 제한된 벤치마크에서 평가되었기 때문에, 인간과 유사한 폭넓은 시각적 추론 능력을 가지고 있는지 여부는 불분명했습니다.

**어떻게 문제를 해결하는가?**
연구진은 최첨단 생성형 비디오 모델인 Veo 3를 활용하여, 고전적인 컴퓨터 비전 문제부터 물리적 추론, 심지어 도구 사용 시뮬레이션에 이르는 62가지의 광범위한 작업을 체계적으로 평가했습니다. 핵심은 **최소한의 프롬프트 기반 접근 방식(minimalist prompt-based approach)**을 사용했다는 점입니다. 모델에는 초기 비디오 프레임이나 이미지, 그리고 "가장자리를 보여줘" 또는 "이 미로를 풀어줘"와 같은 텍스트 지시가 주어지고, 모델은 8초 분량의 비디오를 '답변'으로 생성합니다. 이 과정에서 미세 조정(fine-tuning)이나 작업별 훈련(task-specific training)은 전혀 없었으며, 이는 진정한 제로샷 평가(zero-shot evaluation)를 의미합니다. 비디오 모델 자체의 추론 능력을 명확히 분리하기 위해, 연구진은 단일 이미지만으로는 독립형 LLM(Google Gemini 2.5)이 해당 작업을 해결할 수 없도록 설계하여, 모든 성공이 비디오 생성 과정 자체에서 비롯됨을 확인했습니다. 본질적으로, 이 연구는 비디오 모델이 시각적으로 단계별로 '생각'하도록 프롬프트(생성되는 프레임을 통해)하고, 그 결과가 올바른 해결책을 제시하는지 검증하는 혁신적인 방법을 제시합니다.

**주요 발견은 무엇인가?**
놀랍게도 Veo 3는 특정 작업에 대한 최적화(task-specific optimization) 없이도 광범위한 새로운 능력을 입증했습니다. 이 모델은 객체를 **분할(segment objects)**하고, **가장자리를 감지(detect edges)**하며, **이미지 편집**(예: 객체 제거)을 수행하고, **물리적 속성**(예: 움직임을 통해 객체의 질량 추론)을 추론하고, **객체 어포던스(affordances)**(객체가 어떻게 사용될 수 있는지)를 인식하며, 심지어 **도구 사용을 시뮬레이션(simulate tool use)**하는 능력까지 제로샷(zero-shot)으로 보여주었습니다. 이러한 지각 및 조작 능력은 더 높은 수준의 **시각적 추론(visual reasoning)**을 가능하게 합니다. 예를 들어, Veo 3는 자신이 생성하는 비디오 내에서 해결 경로를 내부적으로 '상상'함으로써 미로 풀기나 대칭 퍼즐 해결과 같은 복잡한 문제를 해결합니다. 정량적으로, 62가지의 다양한 테스트 작업에서 이 모델은 저수준 비전 작업(예: 에지 감지 92%, 이미지 노이즈 제거 100%)과 물리적 추론과 같은 더 인지적인 작업 모두에서 높은 성공률을 달성했습니다. 또한 Veo 3는 이전 버전(Veo 2)보다 명확한 개선을 보여주어, 이러한 기능이 모델의 규모 및 버전 개선과 함께 확장됨을 시사합니다. 이 모든 결과는 비디오 모델이 충분한 규모와 훈련이 주어질 경우 LLM과 유사한 발전 궤적을 따르며, 훈련 범위를 넘어 수많은 작업을 처리할 수 있는 범용 **'비전 파운데이션 모델(vision foundation models)'**이 될 수 있음을 강력히 시사합니다.

**다음 단계는 무엇인가?**
이 연구는 미래의 인공지능이 **통합된 멀티모달 파운데이션 모델(unified multimodal foundation models)**에 기반할 수 있음을 암시합니다. 하나의 LLM이 다양한 언어 작업을 처리하듯이, 단일 비디오 모델도 수많은 시각 작업을 수행할 수 있다는 비전입니다. 다음 핵심 과제는 LLM의 발전을 이끌었던 프롬프트 엔지니어링(prompt engineering) 및 표준화된 평가(standardized evals) 방식과 유사하게, 이러한 비디오 모델을 위한 프롬프트 기술과 벤치마크를 고도화하는 것입니다. 연구자들은 또한 명시적인 추론 단계(예: 텍스트 기반 계획과 비디오 생성의 결합)를 도입하는 것이 복잡한 작업에서 성능을 더욱 향상시킬 수 있는지 탐구할 것입니다. 응용 측면에서, 강력한 제로샷 비디오 추론기는 혁명적인 변화를 가져올 수 있습니다. 행동하기 전에 결과를 시각적으로 추론하는 로봇 보조원이나, 즉석에서 물리 실험을 시뮬레이션하는 과학 모델을 상상해 보세요. 궁극적으로 언어 및 비디오 모델의 기능 융합은 **범용 AI 시스템(generalist AI systems)**이라는 더 넓은 추세를 예고하며, 이러한 능력이 어떻게 그리고 왜 나타나는지(예: 훈련 데이터나 아키텍처(architecture)의 어떤 요소가 도구 사용 시뮬레이션으로 이어지는가?)를 이해하는 것은 기초 연구를 위한 중요한 질문입니다.

---

**사전 훈련 데이터 기반 강화 학습(Reinforcement Learning on Pre-Training Data, RLPT)**

관련 연구: RLPT ( [논문](https://arxiv.org/abs/2405.08643) )

**어떤 문제를 해결하는가?**
대규모 언어 모델(LLM)에 더 많은 텍스트를 공급하여 규모를 확장하는 방식은 고품질 텍스트 데이터의 유한성이라는 병목 현상에 직면했습니다. 컴퓨팅 자원은 쉽게 확장할 수 있지만, 양질의 데이터는 그렇지 않습니다. 더욱이, 단순히 다음 토큰(token)을 예측하는 표준 훈련 방식만으로는 모델이 복잡한 의존성을 통해 추론하는 방법을 제대로 학습하기 어렵습니다. 이는 모델이 데이터 분포(distribution)를 넘어 탐색하도록 장려되지 않기 때문입니다. 인간 피드백 기반 강화 학습(RLHF)과 같은 기존 방법은 일부 신호를 추가하지만, 이는 비용이 많이 드는 인간의 주석 작업을 필요로 합니다. 요약하자면, 우리는 인간 주석자(annotator)의 도움 없이도 LLM이 이미 보유한 데이터로부터 더 많은 것을 학습하고, 특히 추론 기술을 습득할 수 있는 효율적인 방법이 절실했습니다.

**어떻게 문제를 해결하는가?**
RLPT는 LLM의 원본 사전 훈련(pre-training) 데이터를 강화 학습(RL)을 위한 상호작용적인 훈련 환경으로 재정의합니다. 이는 다음 세그먼트(next-segment) 예측 작업을 순차적 의사결정 문제(sequential decision problem)로 간주함으로써 가능해집니다. 구체적으로, 모델은 주어진 텍스트 컨텍스트(context)를 읽은 후 다음 텍스트 덩어리를 생성합니다. 이때, 별도의 생성형 보상 모델(Generative Reward Model) 또는 암묵적인 정답 신호가 해당 생성이 코퍼스(corpus) 내의 실제 연속과 얼마나 잘 일치하는지에 따라 보상을 제공합니다. 이로써 LLM은 실제 텍스트를 최적 행동의 시연으로 인식하고, 다양한 연속을 탐색하며 인간의 레이블(label) 없이 피드백을 얻습니다. 중요한 점은 이러한 보상이 사전 훈련 데이터(예: 관찰된 텍스트와의 일치 여부)에서 직접 파생되므로, 수동으로 생성된 보상이나 인간 선호 모델에 대한 의존성을 제거한다는 것입니다. 실제적으로 RLPT는 모델이 레이블 없는 데이터에서 자율적으로 추론을 연습할 수 있도록 합니다. 더 높은 보상(텍스트와의 전반적인 일관성 향상)을 가져오는 장기적인 경로를 발견하면, 모델은 즉각적인 다음 토큰 예측을 넘어설 수 있습니다. 이러한 훈련 중 탐색 과정은 수십억 개의 토큰 규모로 신중하게 확장되어, 정책(policy)이 광범위한 텍스트 도메인에서 더욱 풍부한 추론 전략을 발견할 수 있도록 돕습니다.

**주요 발견은 무엇인가?**
40억 매개변수 기본 LLM(Qwen3-4B)에 RLPT를 적용한 결과, 여러 까다로운 벤치마크에서 성능이 크게 향상되었습니다. 예를 들어, MMLU(지식 시험)에서는 모델 점수가 **3.0점**, MMLU-Pro(고급 버전)에서는 **5.1점** 상승했습니다. 수학 및 논리 중심 작업에서는 훨씬 더 큰 개선이 관찰되었습니다. QA 벤치마크(GPQA-Diamond)에서 **8.1점**, AIME24(수학 경시 문제)에서 **6.6점**이 향상되었습니다. 이는 이미 강력한 기본 모델에 대한 절대적인 성능 개선이며, 인간이 레이블링한 데이터나 작업별 미세 조정(finetuning) 없이 달성되었다는 점에서 주목할 만합니다. 또한, 스케일링 연구에 따르면 RLPT에 더 많은 컴퓨팅 자원(더 많은 훈련 단계)을 할당할수록 모델은 지속적으로 개선되어, 더 큰 예산으로 더 큰 성과를 얻을 수 있음을 시사합니다. 저자들은 또한 RLPT로 훈련된 모델이 더 강력한 **일반화 가능한 추론(generalizable reasoning)** 능력을 보인다고 언급합니다. 이 모델은 복잡한 프롬프트(prompt)를 처리하는 능력을 확장하고, 기존 검증 기반 강화 학습(RLVR)과 함께 사용될 때 성능을 더욱 향상시킵니다. 요약하자면, RLPT는 우리가 이미 가진 텍스트에서 훨씬 더 많은 신호를 추출하여 데이터 부족의 한계를 극복하고, 수동적인 사전 훈련 데이터를 능동적인 학습 경험으로 효과적으로 전환하는 유망한 경로를 제공합니다.

**다음 단계는 무엇인가?**
RLPT의 자기 지도형 보상(self-supervised rewards) 성공은 **하이브리드 훈련 체제(hybrid training regimes)**의 시대를 열 가능성이 있습니다. 미래의 LLM은 인간의 개입 없이 텍스트, 게임 또는 시뮬레이션에서 수동적인 정보 습득과 능동적인 탐색을 번갈아 수행하며 학습할 수 있을 것입니다. 즉각적인 후속 연구로는 RLPT를 더 큰 모델(예: 340억, 700억 매개변수)과 코드 또는 멀티모달(multimodal) 데이터와 같은 더 다양한 도메인에 적용하는 것이 있습니다. 이러한 영역에서도 추론 능력이 유사하게 향상될 수 있을까요? 보상 모델링(reward modeling)을 개선할 여지도 많습니다. 현재의 다음 세그먼트 보상은 텍스트에서 자동으로 파생된 논리적 일관성 또는 사실적 정확성 지표를 통합하여 더욱 정교해질 수 있습니다. 이러한 지표가 RLPT에 통합된다면, 모델은 자체적으로 일관성과 진실성을 검증하는 능력을 갖추게 될 것입니다. 큰 그림에서 RLPT는 단순히 모방하는 것을 넘어 미리 사고하는(토큰 계획) 것을 학습하는 언어 모델(LM)의 더 넓은 추세의 일부입니다. 따라서 이를 사고의 나무(tree-of-thought) 또는 훈련 중 도구 사용과 같은 기술과 결합하는 연구가 활발히 진행될 것으로 예상됩니다. 이 모든 것은 인터넷 텍스트를 단순히 흡수하는 것을 넘어, 마치 자료를 더 잘 이해하기 위해 문제를 해결하는 학생처럼, 텍스트를 적극적으로 연습하고 일반화하는 LLM을 향한 중요한 진전입니다.

---

**소프트 토큰, 단단한 진실: 연속적 사고의 사슬의 잠재력**

관련 연구: 연속적 사고의 사슬(Continuous CoT) ( [논문](https://arxiv.org/abs/2405.08643) )

**어떤 문제를 해결하는가?**
사고의 사슬(chain-of-thought, CoT) 프롬프트(prompting)는 LLM이 단계별 추론을 생성하도록 유도하여 성능을 향상시키지만, 이때 사용되는 이산적인 자연어 토큰(discrete natural language tokens)은 추론을 위한 가장 효율적인 내부 표현이 아닐 수 있습니다. 연속 토큰(continuous tokens)은 본질적으로 이산적인 어휘(discrete vocabulary)의 제약에서 벗어난 벡터(vector)로, 이론적으로 훨씬 더 큰 표현력을 가지며 여러 아이디어를 동시에 인코딩(encode)할 수 있습니다. 즉, 연속적인 공간에서 '생각하는' 모델은 한 번에 하나의 추론 경로가 아닌, 여러 경로를 병렬로(중첩(superposition) 형태로) 탐색할 수 있는 잠재력을 가집니다. 문제는 LLM이 추론 과정에서 연속적인 비언어 토큰을 사용하도록 실제로 훈련하는 것이 매우 어렵다는 것입니다. 과거 시도들은 추론 시에만 연속 토큰을 주입하거나(훈련 없이), 알려진 인간이 작성한 추론 사슬에서 증류(distilling)하는 방식을 요구했는데, 이는 번거롭고 짧은 사슬에 국한되었습니다. 모델이 유용한 연속적 사고의 사슬(CoT)을 처음부터 학습하는 확장 가능한 방법은 아직 제시되지 않았습니다.

**어떻게 문제를 해결하는가?**
이 연구는 어떠한 정답 인간 근거(ground-truth human rationales)에도 의존하지 않고 강화 학습(reinforcement learning)을 통해 연속적 사고의 사슬(CoT)을 훈련하는 최초의 성공적인 방법을 선보입니다. 핵심 아이디어는 모델이 프롬프트(prompt)와 최종 답변 사이에 '소프트(soft)' 토큰(연속 임베딩(continuous embeddings))을 생성하도록 하고, 보상 신호(reward signal)를 사용하여 그 사용을 최적화하는 것입니다. 구체적으로, 연구팀은 탐색의 한 형태로 입력 임베딩에 소량의 노이즈(noise)를 추가한 다음, 정책 경사 강화 학습(policy-gradient RL)을 활용하여 최종 답변이 정확할 경우 모델에 보상을 줍니다. 본질적으로 모델은 더 나은 문제 해결 결과로 이어지는 자체적인 내부 언어(연속 토큰과 그것이 나타내는 것)를 발명하려고 노력합니다. 이산적인 사슬에 대한 어떠한 지도 학습(supervised training)도 회피함으로써, 이러한 소프트 토큰이 무엇을 할 수 있는지 제한하는 인간의 편향이 없습니다. 특히, 이 접근 방식은 최소한의 계산 오버헤드(computational overhead)를 추가하므로, 훈련 중 추론 단계에서 모델이 수백 개의 연속 토큰을 사용하도록 허용할 수 있습니다. 이는 이전 증류(distillation) 방법이 허용했던 것보다 훨씬 더 많은 '사고 용량'을 의미합니다.

**주요 발견은 무엇인가?**
수학 추론 벤치마크에서 이 연속적 사고의 사슬(CoT) 기술로 훈련된 LLM은 전통적인 이산적 사고의 사슬을 사용하는 모델과 동등하거나 더 우수한 성능을 달성했습니다. 예를 들어, GSM8K 수학 문제에서 연속적 CoT를 사용하는 Llama-7B 모델은 단일 최적 답변(pass@1)을 고려할 때 표준(이산적) CoT를 사용하는 동일 모델의 정확도와 일치했습니다. 그러나 여러 답변을 샘플링(pass@32)할 수 있도록 허용했을 때, **연속적 CoT 모델은 이산적 CoT 모델을 능가했으며**, 이는 올바른 답변으로 이어지는 더 다양하고 풍부한 추론 경로를 발견했음을 나타냅니다. 이는 연속 토큰의 주요 이점 중 하나를 보여줍니다. 즉, 더 풍부하고 다양한 해결책을 포착할 수 있으며, 여러 출력을 시도할 수 있을 때 그 가치가 극대화됩니다. 흥미롭게도, 저자들은 최적의 전략이 하이브리드(hybrid) 방식임을 발견했습니다. 즉, 훈련 중에는 연속 토큰을 사용하되, **추론 시에는 이산 토큰을 사용하는 것**입니다. 다시 말해, 훈련 과정에서는 모델이 벡터(vector)로 '생각'함으로써 이점을 얻지만, 배포 시에는 필요에 따라 일반 텍스트 형태의 근거를 출력할 수 있도록 합니다. 훈련은 여전히 잠재적 추론 능력을 향상시켰습니다. 더욱이, 연속적 CoT 훈련은 모델의 다른 능력에 대한 간섭을 덜 일으켰습니다. 모델은 이산적 CoT로 훈련된 모델보다 관련 없는 작업에서 정확도를 더 잘 유지했으며, 이는 이 접근 방식이 추론 데이터에 과적합(overfitting)되는 것을 피하는 '더 부드러운' 방식임을 의미합니다. 종합적으로 볼 때, 이는 LLM이 실제 문제 해결 이득을 가져오는 자체적인 비인간 가독 사고 벡터(non-human-readable thought vectors)를 개발할 수 있다는 개념 증명(proof-of-concept)입니다.

**다음 단계는 무엇인가?**
LLM을 벡터(vector)로 사고하도록 훈련하는 것은 무궁무진한 연구 방향을 제시합니다. 당장 던져지는 질문 중 하나는 이러한 학습된 연속 토큰을 어떻게 해석하고 시각화할 것인가입니다. 과연 그것들이 인간이 이해할 수 있는 개념에 상응하는 것일까요, 아니면 완전히 이질적이지만 효과적인 어떤 것일까요? 또한 연속적 CoT를 멀티모달 추론(multimodal reasoning)으로 확장할 가능성도 큽니다. 예를 들어, 추론 과정에서 '소프트 시각 토큰(soft visual tokens)'을 사용하여 이미지를 내부적으로 표현하는 LLM을 상상해 볼 수 있습니다. 여기서 강화 학습(RL)의 성공은 논리적 일관성 검사나 사실 확인과 같은 다른 보상 신호를 활용하여 연속적인 사고를 형성하고, 이를 통해 더욱 신뢰할 수 있는 추론을 생성하는 데 영감을 줄 수 있습니다. 실제로 우리는 모델이 연속 공간에서 고강도 추론을 수행한 다음, 그 결과를 인간을 위한 간결한 설명으로 증류하는 하이브리드 시스템을 목격할 수 있습니다. '소프트' 모델의 최종 답변이 표준 형식으로 실행될 수 있다는 사실은 실질적인 적용을 용이하게 합니다. 예를 들어, 수학 튜터 LLM은 연속적 CoT를 조용히 활용하여 어려운 증명을 풀어낸 다음, 깔끔한 자연어로 답변을 제시할 수 있습니다. 전반적으로 이 연구는 LLM에서 더욱 효율적이고 다양한 추론을 위한 기반을 마련하며, 기존 사고의 사슬(chain-of-thought)이 가지고 있던 이산 토큰 사고의 일부 한계를 잠재적으로 극복할 수 있음을 보여줍니다.

---

**사고 증강 사전 훈련(Thinking Augmented Pre-Training, TPT): 데이터 효율성의 혁신**

관련 연구: TPT ( [논문](https://arxiv.org/abs/2405.08643) )

**어떤 문제를 해결하는가?**
대규모 언어 모델(LLM)을 위한 고품질 훈련 데이터는 그 양이 제한적이며, 언어에 내재된 일부 복잡한 패턴은 단순히 다음 단어 예측만으로는 모델이 학습하기 어려운 경우가 많습니다. 종종 진술의 근거나 문장을 연결하는 논리적 사슬은 텍스트에 명시적으로 나타나지 않고 암묵적으로 존재하거나 가정됩니다. 이로 인해 특정 '고품질 토큰'(예: 수학 증명의 한 단계 또는 코드의 숨겨진 논리적 연결)은 효과적으로 학습하기 매우 어렵습니다. 그 결과는 데이터 비효율성으로 이어집니다. 수십억 개의 단어가 주어져도, 모델이 중간 사고 과정을 전혀 접하지 못하기 때문에 다단계 추론이나 더 깊은 이해에 여전히 어려움을 겪을 수 있습니다. 여기서 다루는 핵심 과제는 **숨겨진 추론을 명시적으로 드러냄으로써 데이터를 더욱 효과적으로 활용하는 방법**을 찾는 것입니다.

**어떻게 문제를 해결하는가?**
TPT는 사전 훈련(pre-training) 코퍼스(corpus)를 '사고 경로(thinking trajectories)'로 증강함으로써 이 문제를 해결합니다. 본질적으로 단계별 추론이나 설명 콘텐츠를 생성하여 원본 텍스트와 함께 삽입하는 방식입니다. 예를 들어, 원본 텍스트가 "학생이 문제를 풀고 답 42를 얻었다"고 한다면, 사고 경로에는 학생이 문제를 해결하기 위해 거친 단계들이 포함될 수 있습니다. 이러한 경로는 광범위한 작업 및 도메인에 걸쳐 자동으로 생성되며(강력한 LLM 또는 휴리스틱(heuristics)에 대한 프롬프트(prompting) 사용 가능성 높음), 훈련을 위해 원본 데이터와 결합됩니다. 그렇게 함으로써 TPT는 유효 데이터 볼륨을 증가시키고(새로운 토큰을 추가하므로), 결정적으로 복잡한 토큰의 기본 근거를 분해하여 학습하기 쉽게 만듭니다. 이 방법은 '범용적'입니다. 제한된 데이터로 처음부터 사전 훈련하거나, 이미 방대한 코퍼스를 증강하거나, 심지어 오픈소스 모델을 중간에 훈련하여 추가로 개선하는 등 다양한 설정에 적용됩니다. 각 경우에 명시적인 추론 사슬의 존재는 모델이 동일한 양의 원본 텍스트로부터 더 잘 일반화(generalize)하도록 돕습니다.

**주요 발견은 무엇인가?**
모델 크기와 훈련 설정 전반에 걸쳐 TPT는 상당한 성능 향상을 가져왔으며, 이는 데이터 효율성(data efficiency) 측면에서 큰 성공을 의미합니다. 특히 저자들은 TPT가 사전 훈련(pre-training)의 데이터 효율성을 **3배 향상시킨다**고 보고합니다. 실제적으로 이는 TPT 증강을 통해 1000억 개의 토큰으로 훈련된 LLM이 3000억 개의 표준 데이터 토큰으로 훈련된 모델과 유사하거나 더 나은 결과를 달성할 수 있음을 의미합니다. 30억 매개변수 모델의 경우, 훈련 중 사고 경로(thinking trajectories)를 통합하는 것만으로도 여러 까다로운 추론 벤치마크에서 **10% 이상의 개선**을 보였습니다. 더 큰 모델과 다른 계열(디코더 전용(decoder-only) 및 기타 모델 모두 테스트)도 모두 이점을 얻었으며, 이는 TPT가 견고함을 시사합니다. 중요하게도, 이러한 이득은 특정 틈새 작업에만 국한되지 않습니다. 이 논문은 일반 NLP 벤치마크에서 '다양한 모델 크기와 계열 전반에 걸쳐' 개선이 있었다고 언급합니다. 이는 이 방법이 특정 문제에 과적합(overfitting)되지 않고 광범위한 이해 또는 기술을 주입한다는 것을 의미합니다. 추론을 명시적으로 포함함으로써 모델은 단계별 논리, 수학 단어 문제, 복잡한 QA 등을 요구하는 작업에서 더 나은 성능을 보이며, 표준 언어 작업의 성능을 저해하지도 않습니다. 본질적으로 TPT는 **토큰당 더 많은 사고가 단순히 더 많은 토큰만큼 좋거나(또는 더 좋다는 것을) 보여주며**, 이는 효율적인 훈련을 위한 중요한 결과입니다.

**다음 단계는 무엇인가?**
TPT의 접근 방식은 LLM 훈련을 더욱 의도적이고 구조화하는 최신 경향과 일치합니다. 미래 연구는 사고 경로(thinking trajectories)의 생성을 더욱 자동화하는 방법을 탐구할 수 있습니다. 예를 들어, 한 LLM을 사용하여 추론을 생성하고, 다른 LLM을 통해 그 추론을 검증하거나 개선한 후 훈련에 사용하는 방식입니다. 또한, 이 방법을 다른 양식(modality)으로 확장할 가능성도 있습니다. 예를 들어, 이미지 캡션(caption)에 시각적 추론 사슬을 추가하거나, 코드에 프로그램 논리 사슬을 추가하여 학습 효과를 유사하게 증진시키는 것입니다. 즉각적인 실용적 영향 측면에서, 모델을 훈련하는 기업들은 TPT를 채택하여 더 적은 데이터로 높은 성능을 달성하거나(또는 동일한 데이터로 더 나은 결과를 얻을 수 있으며), 이는 경제적으로 매우 매력적입니다. 나아가 TPT를 RLPT(위 논문 #4)와 결합하는 시너지 효과도 기대됩니다. 즉, 먼저 추론으로 데이터를 증강(TPT)한 다음, 모델이 강화 학습(RL)을 통해 해당 데이터를 탐색하도록 하는 것입니다. 이는 자체 개선 AI를 위한 매우 강력한 조합이 될 수 있습니다. 궁극적으로 TPT는 데이터의 양보다는 질을 우선적으로 고려하도록 우리에게 촉구합니다. 텍스트 내에 '숨겨진' 정보를 찾아내고 이를 명시적으로 만듦으로써, 우리는 훨씬 더 많은 데이터를 추가하지 않고도 LLM 능력의 새로운 수준을 발견할 수 있을 것입니다.

---

**SimpleFold: 단백질 접힘, 생각보다 간단하다는 증명**

관련 연구: SimpleFold ( [논문](https://arxiv.org/abs/2405.08643) / [코드](https://github.com/google-deepmind/simplefold) )

**어떤 문제를 해결하는가?**
알파폴드(AlphaFold)와 같은 단백질 접힘(protein folding) 분야의 최근 혁신은 단백질의 고유한 기하학적 특성(protein-specific geometry)을 포착하기 위해 매우 복잡한 모델 아키텍처(architecture)에 의존해 왔습니다. 예를 들어, 삼각형 어텐션 모듈(triangle attention modules), 쌍별 거리 행렬(pairwise distance matrices), 그리고 여러 맞춤형 손실 항(loss terms) 등이 사용됩니다. 이러한 전문화된 설계는 매우 성공적이었지만, 계산적으로 부담이 크고 자연어 처리(NLP)나 비전(vision) 분야에서 사용되는 '표준' 아키텍처와는 상당히 다릅니다. 이는 다음과 같은 흥미로운 질문을 제기합니다. 과연 이 모든 도메인별 복잡성이 필수적인가, 아니면 훨씬 더 간단하고 일반적인 모델이 비슷한 정확도로 단백질을 접을 수 있을까? 다시 말해, 단백질 접힘 문제는 현재 모델들이 보여주는 것보다 근본적으로 더 단순한 것일까?

**어떻게 문제를 해결하는가?**
SimpleFold는 단백질 접힘 모델을 가장 기본적인 형태로 되돌리려는 과감한 시도입니다. 이 모델은 특별한 단백질 특정 블록(protein-specific blocks)이 없는 범용 트랜스포머(Transformer) 아키텍처(architecture)를 사용합니다. 일반적인 트릭(삼각형 업데이트, 아미노산의 별도 2D 쌍 표현 등) 대신, 표준 자기 어텐션 레이어(self-attention layers)(일부 적응형 게이팅 레이어(adaptive gating layers)로 보강됨)에 의존하며 단백질 구조 데이터에 대해 종단 간(end-to-end)으로 훈련합니다. 핵심 통찰력은 단백질 접힘을 생성 모델링 문제(generative modeling problem)로 간주하는 것입니다. SimpleFold는 확산 모델(diffusion models) 또는 정규화 흐름(normalizing flows)과 관련된 **흐름 일치 목표(flow-matching objective)**로 훈련되어, 무작위 구조를 올바른 접힌 구조로 점진적으로 정제하도록 안내합니다. 그들은 올바른 구조 예측을 장려하기 위한 사소한 추가 손실 항(loss term)을 포함합니다(따라서 순수하게 일반적이지는 않지만 거의 그렇습니다). 그런 다음 이 모델을 30억 매개변수로 확장하고 약 9백만 개의 단백질 구조(증류/예측된 대규모 세트와 실험적으로 해결된 구조 포함)에 대해 훈련합니다. 본질적으로 SimpleFold는 단백질 좌표를 데이터 시퀀스(data sequence)처럼 취급하고, 단백질 생물 물리학 지식을 명시적으로 인코딩(encoding)하지 않고 트랜스포머(Transformer)를 사용하여 이를 올바른 형태로 '흐르게' 하는 방법을 학습합니다.

**주요 발견은 무엇인가?**
SimpleFold-3B는 표준 단백질 접힘 벤치마크에서 최첨단 전문 모델들과 견줄 만한 성능을 보여줍니다. 이 모델은 3D 구조 예측에서 경쟁력 있는 정확도를 달성하여, 일반 트랜스포머(vanilla Transformer)가 단백질 접힘에 필요한 복잡한 의존성을 실제로 학습할 수 있음을 입증했습니다. 더욱이 SimpleFold는 결정론적 모델(deterministic models)이 종종 어려움을 겪는 부분에서 강점을 보입니다. 생성형(generative) 모델이기 때문에 자연스럽게 **다양한 확률적 구조의 앙상블(ensemble)**을 생성할 수 있습니다. 이 논문은 앙상블 예측에서 강력한 성능을 언급합니다. 여러 접힘을 샘플링하고 대체 형태(alternative conformations)를 포착할 수 있는데, 이는 단일 답변을 제공하는 알파폴드(AlphaFold)와 같은 모델에게는 일반적으로 어려운 일입니다. 또 다른 실용적인 이점은 효율성입니다. 더 간단한 아키텍처(architecture) 덕분에 SimpleFold는 배포하기 쉽고 표준 하드웨어에서 더 빠르게 실행됩니다(특수 연산 불필요). SimpleFold의 성공은 단백질 접힘을 위해 고도로 도메인별 설계가 필요하다는 개념에 효과적으로 도전합니다. 이는 과학 도메인에서 더 많은 기성 AI 구성 요소를 사용하는 길을 엽니다. 요컨대, 이 논문은 단백질 접힘의 많은 부분이 **일반 시퀀스 모델(generic sequence model)**에 의해 학습될 수 있음을 보여주며, 이는 놀랍고 고무적인 발견입니다.

**다음 단계는 무엇인가?**
SimpleFold의 성공적인 접근 방식은 과학 문제 해결을 위한 모델 설계 패러다임을 재고하게 만듭니다. 만약 범용 트랜스포머(Transformer)가 단백질 구조 예측에 효과적이라면, 다른 과학적 작업(예: 분자 특성 예측, DNA 접힘 시뮬레이션 등)도 올바른 훈련 전략을 통해 더 간단한 아키텍처(architecture)로 전환될 수 있을 것입니다. 향후 연구는 SimpleFold를 약물 결합 예측(drug binding prediction)과 같은 다운스트림 작업(downstream tasks)과 통합하여, 생성 앙상블(generative ensemble) 능력을 통해 다양한 단백질 형태(conformations)를 탐색하는 방향으로 나아갈 수 있습니다. 흐름 일치 목표(flow-matching objective)의 사용은 확산 모델(diffusion models)과의 연관성도 시사합니다. 단백질 접힘 과정을 시계열(time series) 데이터처럼 시뮬레이션하여 정확도를 더욱 높이거나 단백질의 동역학(dynamics)을 포착하는 확산 기반 접힘 모델을 상상해 볼 수 있습니다. 또한 SimpleFold는 표준 AI 모델에 가깝기 때문에 **전이 학습(transfer learning)**의 이점을 활용할 수 있습니다. 예를 들어, 언어 모델의 가중치로 초기화하거나 그 반대로 하여 일부 교차 도메인 지식(cross-domain knowledge)을 주입하는 연구가 가능합니다(일부 언어 기능이 단백질 시퀀스 분석에 도움이 될 수 있다는 초기 가설도 있습니다). 가장 중요한 교훈은 **단순성이 때로는 복잡성과 동일한 목표를 달성할 수 있다**는 것입니다. 이는 수작업으로 설계된 네트워크가 지배적인 도메인에서 연구자들이 더 '미니멀리스트(minimalist)' 기준선(baselines)을 시도하도록 장려할 귀중한 통찰입니다. 이러한 추세가 계속된다면, 과학(단백질 접힘, 화학)과 일반 AI 모두에서 동일한 핵심 모델 유형이 발전을 견인하며, 주로 아키텍처보다는 훈련 데이터의 차이에서 비롯되는 수렴 현상을 목격하게 될 것입니다.

---

**LLMs4All: 학문 분야별 대규모 언어 모델 연구 및 활용 포괄적 검토**

관련 연구: LLMs4All 서베이(Survey) ( [논문](https://arxiv.org/abs/2405.08643) )

**어떤 문제를 해결하는가?**
대규모 언어 모델(LLM)의 영향력은 더 이상 컴퓨터 과학에만 국한되지 않고, 역사학부터 생물학에 이르기까지 모든 학문 분야에 빠르게 확산되고 있습니다. 그러나 이러한 다양한 분야에서 LLM을 효과적으로 활용하는 방법에 대한 지식은 파편화되어 있습니다. 예를 들어, 법학이나 화학 분야의 연구자들은 자신의 전문 영역과 관련된 최신 LLM 기술 동향에 대한 정보를 충분히 얻지 못할 수 있습니다. 이 논문은 학문 연구의 전 범위에 걸쳐 최첨단 LLM 응용 사례, 기회, 그리고 당면 과제들을 한데 모아 종합적으로 조망하는 포괄적인 서베이(survey)의 필요성을 제기합니다.

**어떻게 문제를 해결하는가?**
LLMs4All은 학문의 세 가지 광범위한 영역에 걸쳐 LLM이 각 분야에서 어떻게 적용되고 있는지를 상세히 설명하는 광범위한 검토 및 가이드 역할을 합니다. 저자들은 분야를 다음과 같이 분류합니다. (1) 예술, 인문학 및 법학(역사, 철학, 정치학, 건축학, 법학 등), (2) 경제 및 경영(재무, 마케팅, 경영 등), (3) 과학 및 공학(수학, 물리학, 생물학, 화학, 지구과학, 컴퓨터 과학 등). 각 영역에 대해 이 서베이(survey)는 연구 및 실무에서 LLM의 현재 사용 사례를 설명합니다. 예를 들어, 역사 텍스트 분석 지원, 법률 문서 요약 지원, 과학 연구에서 가설 생성 등 해당 도메인의 최첨단 모델 또는 시스템의 예시와 함께 제시됩니다. 또한 텍스트 생성, 추론, 코딩, 다국어 이해와 같은 LLM 기능이 분야별 요구 사항을 충족하기 위해 어떻게 맞춤화되거나 미세 조정(fine-tuned)되는지 논의합니다. 응용 분야 외에도 이 검토는 각 분야의 주요 한계와 과제(의학의 데이터 프라이버시, 역사의 사실 정확성, 법률의 윤리적 문제(예: 편향 또는 공정성) 등)뿐만 아니라 LLM 통합을 위한 미해결 연구 질문 및 미래 방향을 다룹니다. 결과적으로 LLMs4All은 AI의 최전선과 도메인 전문가 사이의 다리 역할을 하며, 'LLM이 X 분야에 무엇을 할 수 있는가'를 한곳에 요약합니다.

**주요 발견은 무엇인가?**
이 서베이(survey)의 주요 기여는 질적 종합에 있지만, 여러 중요한 통찰력과 관찰을 제공합니다. 한 가지 포괄적인 발견은 사실상 어떤 학문 분야도 LLM의 영향에서 벗어나지 않았다는 것입니다. GPT-4를 사용하여 법률 계약을 초안 작성하는 것부터 화학 실험 설계를 위해 생성 모델을 활용하는 것에 이르기까지, 학계는 LLM 기반 혁신의 물결을 경험하고 있습니다. 그러나 이 검토는 분야별 성숙도에 차이가 있음을 지적합니다. 일부 분야(컴퓨터 과학 및 법학 등)는 이미 수많은 LLM 응용 프로그램을 보유하고 있지만, 다른 분야(예: 철학 또는 예술)는 여전히 초기 사용 사례를 탐색하고 있습니다. LLM이 그럴듯하게 들리지만 잘못된 정보(환각(hallucinations))를 생성하여 의학이나 금융과 같은 분야의 비전문가 사용자를 오도할 수 있다는 우려와 같이, 여러 분야에서 공통적인 과제가 나타납니다. 이 논문은 **학제 간 협력(interdisciplinary collaboration)이 핵심**임을 강조합니다. 예를 들어, LLM을 도메인별 지식 기반 또는 모델과 결합하면 더 나은 결과를 얻을 수 있습니다(LLM과 화학 규칙을 사용하는 과학적 발견 도구에서 볼 수 있듯이). 긍정적인 발견은 LLM이 연구에서 **민주화 세력**으로 작용한다는 것입니다. LLM은 고급 기술 훈련이 없는 개인도 자신의 도메인 문제에 AI를 활용할 수 있도록 합니다(예: GPT를 사용하여 고대 텍스트를 번역하고 요약하는 역사학자). 이 서베이(survey)는 또한 커뮤니티가 책임감 있는 LLM 사용(예: AI가 사용된 경우 학술 글쓰기에서의 공개 정책)에 대해 고심하면서 나타나는 모범 사례와 윤리적 지침을 수집합니다. 종합적으로 LLMs4All은 각 분야의 연구자들이 현재 상황을 이해하고 자신의 작업에서 LLM을 어떻게 사용할 수 있는지 파악하기 위한 로드맵을 제공합니다.

**왜 중요한가?**
생성형 AI가 데이터 분석만큼이나 학문의 핵심 요소로 자리 잡음에 따라, 각 학문 분야에서 그 역할을 명확히 이해하는 것이 필수적입니다. 이 서베이(survey)는 교육자와 정책 입안자들에게도 중요한 참고 자료가 될 것입니다. 예를 들어, 대학 학과는 해당 분야와 관련된 AI 리터러시(literacy)를 포함하도록 교육과정을 업데이트할 때 이 문서를 참조할 수 있습니다. 또한, 한계와 미래 방향을 명확히 제시함으로써, 이 논문은 AI 연구자들에게 해결해야 할 중요한 문제들(예: 과학 Q&A의 사실성 향상 또는 LLM을 법적 추론과 일치시키는 방안)을 제시합니다. 한 가지 중요한 결과는 이 서베이(survey)가 **학제 간 협력(cross-disciplinary collaborations)을 촉진할 것**이라는 점입니다. 화학 분야의 LLM 활용 사례를 접한 생물학자가 AI 전문가와 협력하여 유사한 기술을 생물학에 적용하는 시나리오를 생각해 볼 수 있습니다. 이 논문은 또한 과장된 소문에도 불구하고 현재 LLM이 전문 분야에서 여전히 심각한 단점을 가지고 있음을 강조하며, 이는 기대치를 현실적으로 조정하고 신뢰성 향상을 위한 추가 연구를 장려합니다. 이는 반복되는 중요한 주제입니다. 요약하자면, LLMs4All은 AI와 다른 모든 분야의 교차점에서 일어나고 있는 변혁을 기록하고, 지식이 고립되지 않고 광범위하게 공유되도록 보장하며, AI가 진정으로 모든 분야의 도구가 되는 다음 연구 단계를 이끄는 데 기여하기 때문에 매우 중요합니다.

---

**생각하는 언어 모델이 더 나은 채팅을 한다: RLMT의 힘**

관련 연구: RLMT ( [논문](https://arxiv.org/abs/2405.08643) / [코드](https://github.com/google-deepmind/rlmt) )

**어떤 문제를 해결하는가?**
인간 피드백 기반 강화 학습(Reinforcement Learning from Human Feedback, RLHF)은 대화형 LLM을 더욱 유용하고 안전하게 미세 조정(finetune)하는 표준적인 방법으로 자리 잡았습니다. 그러나 RLHF는 모델이 제공하는 최종 답변만을 최적화할 뿐, 그 답변에 도달하는 추론 과정 자체는 최적화하지 않습니다. 모델이 내부적으로 숨겨진 사고의 사슬(chain-of-thought)을 생성할 수는 있지만, 보상 모델(reward model)은 오직 최종 응답만을 평가합니다. 이는 그럴듯하게 들리지만 실제로는 깊이 있는 추론이 결여된 답변으로 이어질 수 있습니다. 또 다른 접근 방식인 검증 가능한 보상 기반 강화 학습(RL with verifiable rewards, RLVR)은 모델이 검증 가능한 작업(예: 수학 증명 또는 코드 테스트)을 출력하도록 강제하고 정확성에 보상을 줍니다. RLVR은 더 나은 추론을 제공하지만, 객관적인 검증이 가능한 도메인에서만 작동한다는 한계가 있습니다. 여기서 다루는 미해결 과제는 다음과 같습니다. RLHF의 범용성을 유지하면서도 모델이 실제로 문제를 심도 있게 사고하도록 장려할 수 있는 방법은 무엇일까요?

**어떻게 문제를 해결하는가?**
해결책은 RLMT(모델 보상 사고 기반 강화 학습, Reinforcement Learning with Model-rewarded Thinking)라는 새로운 훈련 패러다임(paradigm)에 있습니다. RLMT에서는 훈련 과정에서 모델이 최종 답변을 제시하기 전에 길고 상세한 사고의 사슬(chain-of-thought, CoT)을 생성하도록 요구됩니다. 이 CoT는 상세한 개요, 단계별 추론 과정, 또는 중간 '생각'의 형태를 띨 수 있습니다. 보상 모델(reward model)(RLHF에서 사용되는 것과 동일하며 인간 선호 데이터로 사전 훈련됨)은 최종 답변뿐만 아니라 CoT와 최종 답변의 조합을 함께 평가합니다. 본질적으로 모델은 좋은 답변으로 이어지는 유용한 추론 과정을 생성한 것에 대해 보상을 받게 됩니다. 연구팀은 다양한 실제 프롬프트(prompt)(에세이 작성, 식단 계획, 복잡한 질문 답변과 같은 개방형 작업)를 사용하고 정책 경사 방법(policy gradient methods)(PPO, DPO 등)을 적용하여 LLM의 정책을 최적화함으로써 더 나은 사고+답변 쌍을 출력하도록 훈련했습니다. 이 접근 방식의 견고성을 보장하기 위해 두 가지 기본 모델(Llama-3.1 8B 및 Qwen-7B)에 대해 다른 설정에서 40개의 개별 훈련 실행을 수행했습니다. 중요하게도, 그들은 또한 RLMT(R1-Zero)를 사용하여 처음부터 모델을 훈련하는 방식을 탐색했습니다. 즉, 지도 미세 조정(supervised fine-tuning) 없이 기본 모델에 RLMT를 직접 적용하여 일반적인 지시 조정(instruction-tuning) 단계를 건너뛸 수 있는지 확인했습니다.

**주요 발견은 무엇인가?**
RLMT로 훈련된 모델은 광범위한 평가에서 표준 RLHF로 훈련된 모델보다 지속적으로 우수한 성능을 보였습니다. 예를 들어, AlpacaEval2, WildBench, Arena Hard 등 세 가지 개방형 채팅 벤치마크에서 RLMT 모델은 동등한 RLHF 모델보다 **3~7점 더 높은 점수**를 기록했으며, 이는 품질 면에서 상당한 도약입니다. 또한 창의적 글쓰기 작업 및 지식 퀴즈에서 **1~3점 향상**과 같은 일반적인 능력 개선도 관찰되었습니다. 아마도 가장 인상적인 결과는, 그들의 최고의 RLMT로 미세 조정된 80억 매개변수 모델이 해당 채팅 벤치마크 및 창의적 작업에서 GPT-4(오픈 변형)의 성능을 실제로 능가했으며, 심지어 한 벤치마크에서는 Claude 2의 수준에 근접했다는 것입니다. 이는 모델이 GPT-4보다 훨씬 작다는 점을 고려할 때 놀라운 성과입니다. 또 다른 놀라운 발견은 다음과 같습니다. 단 **7,000개의 RLMT 프롬프트(어떠한 지도 미세 조정도 없음)로 훈련된 80억 매개변수 Llama 모델이 2,500만 개의 예시로 지시 조정된 공식 Llama-3.1-8B를 능가했습니다**. 다시 말해, '생각하고 답변하기' 최적화를 통해 신중하게 선택된 수천 개의 시나리오가 대규모의 전통적인 훈련을 뛰어넘는 효율성을 보여주었습니다. 이는 RLMT의 강력한 효율성을 입증합니다. 질적으로, 저자들은 RLMT 모델이 더 구조화되고 사려 깊은 응답(예: 목록 작성, 소리 내어 추론, 대안 고려)을 생성하고 주제에서 벗어나는 것과 같은 실패 모드가 더 적다는 것을 관찰했습니다. 이러한 결과는 사고 과정에 보상을 주는 것이 최종 답변에만 보상을 주는 것보다 측정 가능하게 더 나은 채팅 성능으로 이어진다는 것을 강력히 시사합니다.

**다음 단계는 무엇인가?**
이 연구는 대화형 에이전트(conversational agents) 훈련 방식에 패러다임(paradigm)의 전환을 가져올 수 있습니다. 사고의 사슬(chain-of-thought)을 선택적인 부산물로 취급하기보다는, LLM이 자신의 추론을 명시적으로 표현하도록 훈련하는 것이 표준이 될 수 있습니다. 미래 방향에는 RLMT를 **휴먼 인 더 루프(human-in-the-loop)** 시스템과 결합하는 것이 포함됩니다. 예를 들어, 최종 답변뿐만 아니라 중간 생각에 대해서도 인간 피드백을 받아 추론 품질에 대한 보상 모델(reward model)을 더욱 정교하게 만드는 것입니다(기존 선호 모델이 할 수 있는 것을 넘어). 또한 RLMT를 더 큰 모델(논문에서는 80억 매개변수 모델을 다루었지만, 340억 또는 700억 매개변수 모델에 적용하면 일부 영역에서 훨씬 더 강력한 비공개 모델을 능가할 수 있는 잠재력을 가집니다)에 적용하는 연구도 필요합니다. 또 다른 중요한 고려 사항은 실제 배포입니다. RLMT 모델은 더 많이 설명함으로써 더 **해석 가능(interpretable)**할 수 있으며, 이는 안전성 측면에서 큰 이점입니다. 그러나 이는 또한 프롬프트되지 않아도 자신의 '생각'을 드러낼 수 있다는 것을 의미하며, 이는 필요에 따라 조정될 수 있습니다. 마지막으로, 이 연구는 RLMT가 왜 그렇게 효과적인지 심층적으로 이해할 것을 요구합니다. 보상 모델이 인간 선호도와 더 잘 일치하는 사고의 사슬(CoT)의 특정 구조를 간접적으로 선호하는 것일까요, 아니면 더 긴 컨텍스트(context)를 생성하는 행위 자체가 모델이 실수를 피하는 데 도움이 되는 것일까요? 이러한 질문에 답하는 것은 훈련 과정을 더욱 개선할 수 있습니다. 종합적으로 볼 때, **생각하는 언어 모델은 진정으로 더 나은 채팅을 하며**, 우리는 이러한 기술의 결과로 다음 세대 AI 비서가 추론에서 훨씬 더 명시적일 것으로 기대할 수 있습니다.

---

**SciReasoner: 학문 분야를 아우르는 과학적 추론 기반 구축**

관련 연구: SciReasoner ( [논문](https://arxiv.org/abs/2405.08643) / [코드](https://github.com/google-deepmind/scireasoner) )

**어떤 문제를 해결하는가?**
현재까지 과학 분야에 특화된 대규모 언어 모델(LLM)은 대부분 특정 도메인(예: 화학 또는 수학 정리 해결사)에 미세 조정(fine-tuned)된 전문가 모델이었습니다. 그러나 실제 과학 연구는 종종 여러 학문 분야와 다양한 데이터 형식(생물학적 발견을 화학 이론과 연결하고, 방정식과 텍스트를 함께 사용하는 것을 상상해 보세요)에 걸쳐 복합적으로 이루어집니다. 자연어 질문을 이해하는 것을 넘어, 공식, 시퀀스(DNA, 단백질 시퀀스), 속성 테이블 등을 통합적으로 처리할 수 있는 과학적 추론을 위한 파운데이션 모델(foundation model)이 절실히 필요합니다. 요약하자면, 목표는 특정 분야에만 고립되지 않고, 과학이 사용하는 다양한 표현과 함께 광범위하고 학제 간 추론 능력을 갖춘 'AI 과학자'를 구현하는 것입니다.

**어떻게 문제를 해결하는가?**
SciReasoner는 다양한 과학적 표현과 언어를 정렬하기 위한 광범위한 다단계 훈련 과정을 통해 구축됩니다. 첫째, 많은 분야의 과학 텍스트뿐만 아니라 순수하게 상징적인 시퀀스(symbolic sequences)와 혼합 시퀀스-텍스트 데이터(mixed sequence-text data)를 포함하는 2060억 토큰(token) 코퍼스(corpus)로 사전 훈련(pre-trained)됩니다. 이는 아미노산 시퀀스, 화학 SMILES 문자열, 수학 방정식과 같은 것들을 설명 텍스트와 함께 본다는 것을 의미합니다. 이 대규모 사전 훈련 후, 그들은 **4천만 개의 과학 관련 지시(instruction)**에 대해 지도 미세 조정(supervised fine-tuning)을 수행하여 엄청난 범위의 작업(모델은 **103가지 다른 과학 작업을 지원**)을 다룹니다. 이러한 작업은 다음과 같은 범주로 나뉩니다. (i) 텍스트와 과학 형식 간 번역(예: "이 분자 구조를 말로 설명해라" 및 그 반대), (ii) 텍스트 또는 그림에서 지식 추출, (iii) 속성 예측(주어진 화합물에 대해 녹는점 예측 등), (iv) 속성 분류(예: 데이터에서 별을 적색 왜성으로 분류할지 여부), (v) 시퀀스 생성 또는 설계(특정 속성을 가진 DNA 시퀀스 제안 등). 지도 조정 후, 그들은 과학 문제에 대한 장문 사고의 사슬(long-form chain-of-thought) 추론을 모델에 특별히 가르치기 위해 '어닐링된 콜드 스타트(annealed cold-start)' 부트스트래핑(bootstrapping)을 적용합니다. 이는 모델이 복잡한 질문에 대한 단계별 해결책을 생성하도록 프롬프트(prompting)하고 이를 추가 훈련 데이터로 사용하는 것을 포함할 가능성이 높습니다(점진적으로 복잡성을 증가시키므로 '어닐링된'). 마지막으로, 그들은 과학적 추론을 위한 맞춤형 보상 형성(reward shaping)과 함께 강화 학습(reinforcement learning)을 사용합니다. 이 마지막 단계는 모델에 중간 단계(단위 일관성, 방정식 정확성, 논리적 일관성 등)에 대한 피드백을 제공하여 의도적이고 엄격한 추론 습관을 확고히 심어줄 것입니다. 모든 훈련 아티팩트(모델 가중치, 지시 데이터, 평가 코드)는 공개적으로 배포되어 SciReasoner를 커뮤니티 리소스(resource)로 만듭니다.

**주요 발견은 무엇인가?**
SciReasoner는 이전에는 별도의 도구 앙상블(ensemble)이 필요했던 작업을 처리할 수 있는 단일 모델로 부상했습니다. 전문 모델이나 기준선(baselines)과 비교했을 때, SciReasoner는 더 넓은 지시 범위, 더 나은 교차 도메인 일반화(cross-domain generalization), 그리고 출력에서 더 높은 충실도(fidelity)를 보여주었습니다. 예를 들어, 텍스트로 설명된 화학 문제를 받아 방정식과 함께 단계별 해결책을 출력하거나, 유전체 시퀀스(genomic sequence)를 가능성 있는 기능 설명으로 번역하는 등 언어와 형식 데이터를 연결하는 작업을 놀라운 정확도로 수행할 수 있습니다. 이 논문은 여러 학문 분야를 함께 훈련하는 것이 실제로 **전이 학습(transfer learning)을 향상시켰음**을 나타냅니다. 즉, 한 도메인의 작업을 해결하는 것이 다른 도메인에서의 성능을 향상시켰는데, 이는 일반적인 과학적 추론 전략을 학습했기 때문입니다. 이러한 교차 수분(cross-pollination)은 모델의 신뢰성을 강화했습니다. 예를 들어, 물리학 방정식에서 학습한 엄격함은 회계 계산과 같은 분야에서 오류를 피하는 데 도움이 되었습니다. 평가에서 SciReasoner는 어떤 단일 분야의 전문가가 아님에도 불구하고 많은 벤치마크에서 도메인별 모델과 동등하거나 더 나은 성능을 보였습니다. 그리고 지식 혼합을 요구하는 과제(생물학과 화학을 모두 포함하는 질문과 같은)에서는 명확한 이점을 가졌습니다. 본질적으로 SciReasoner는 기반을 마련합니다. 즉, **하나의 모델이 동시에 유능한 물리학자, 화학자, 생물학자 등이 될 수 있으며, 이러한 통합이 실제로 각 측면을 더 강하게 만든다**는 것을 보여줍니다. 이는 과학 전체에 걸쳐 추론할 수 있는 AI를 향한 한 걸음입니다. 모델과 데이터의 오픈소스화(open-sourcing) 또한 주요 성과입니다. 이는 과학 QA, 가설 생성 등을 위한 추가 미세 조정(finetune) 또는 벤치마크를 위한 강력한 기반을 커뮤니티에 제공하여 과학 AI 연구를 가속화합니다.

**다음 단계는 무엇인가?**
SciReasoner는 수많은 새로운 가능성을 열어줍니다. 가까운 미래에 연구자들은 이를 기반으로 전문화된 에이전트(agents)를 구축할 수 있습니다. 예를 들어, SciReasoner를 사용하여 실험 설계를 생성한 다음, 가상 실험실 시뮬레이션(simulation)에서 실행하는 '로봇 과학자'를 상상해 볼 수 있습니다. 훈련 기술(대규모 다중 형식 사전 훈련(multi-format pre-training) 및 신중한 단계별 정렬(staged alignment)과 같은)은 경제학(텍스트와 스프레드시트 및 공식을 혼합) 또는 사회 과학(텍스트와 통계 데이터를 혼합)과 같이 다중 표현 추론을 요구하는 다른 도메인에 적용될 수 있습니다. 또 다른 유력한 방향은 스케일링(scaling)입니다. SciReasoner-8B는 인상적이지만, 유사하게 훈련된 700억 매개변수 모델을 상상해 보세요. 이는 많은 분야에서 인간 전문가 수준에 근접할 수 있습니다. 평가에 대한 작업도 있을 것입니다. 모델은 103가지 작업을 다루지만, 각 작업에서 추론 품질과 사실적 정확성을 어떻게 철저히 검증할까요? 이로부터 새로운 학제 간 벤치마크(interdisciplinary benchmarks)가 탄생할 수 있습니다. 마지막으로, SciReasoner의 출시는 개방형 과학 AI 문화를 조성합니다. 더 많은 연구자들이 이를 사용하고 개선함에 따라, 우리는 모든 교과서를 읽은 강력하지만 광범위한 동료처럼, 어떤 연구자라도 자신의 작업을 향상시키는 데 사용할 수 있는 **'AI 과학 보조원'**으로 이어지는 선순환을 목격할 것입니다. 장기적인 비전은 학문 분야 간 통찰력을 교차 수분(cross-pollinate)할 수 있는 AI(예: 물리학 원리를 사용하여 생물학 문제를 해결)이며, SciReasoner는 AI에서 광범위한 사고를 위한 포괄적 훈련의 가치를 보여주는 그 방향의 기초적인 단계입니다.

---

❤️ 이 글이 유익하셨다면 '좋아요'를 눌러주시고 동료들과 공유해 주세요.
여러분의 의견을 댓글로 남겨주시면 감사하겠습니다.
LLM Watch를 읽어주셔서 대단히 감사합니다!
새로운 게시물을 무료로 받아보고 제 작업을 응원하시려면 구독해 주세요.
구독하기