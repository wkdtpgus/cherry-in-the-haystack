# **처음부터 Qwen3 이해 및 구현하기**

Author: Sebastian Raschka
URL: https://magazine.sebastianraschka.com/p/qwen3-from-scratch

============================================================

이전에 저는 "The Big LLM Architecture Comparison"에서 2025년의 가장 주목할 만한 오픈 웨이트(open-weight) 아키텍처들을 비교했습니다. 다음으로, "From GPT-2 to gpt-oss: Analyzing the Architectural Advances"에서 개념적인 수준으로 다양한 아키텍처 구성 요소들을 더 자세히 살펴보고 논의했습니다. 좋은 일은 세 번 일어난다고 하니, 이번 여름의 주목할 만한 연구 성과들을 다루기 전에, 이제 이 아키텍처들을 코드로 직접 다뤄보고 싶었습니다. 이 글을 따라가다 보면, 실제로 내부적으로 어떻게 작동하는지 이해하게 될 것이고, 자신만의 실험이나 프로젝트에 적용할 수 있는 구성 요소(building blocks)를 얻게 될 것입니다. 이를 위해 저는 Qwen3(5월에 처음 출시되어 7월에 업데이트됨)를 선택했습니다. 이 글을 쓰는 시점에서 Qwen3는 가장 널리 선호되고 사용되는 오픈 웨이트(open-weight) 모델 제품군 중 하나이기 때문입니다.

제 생각에 Qwen3 모델이 이렇게 인기 있는 이유는 다음과 같습니다:

*   개발자와 상업적으로 친화적인 오픈 소스(open-source) (아파치 라이선스 v2.0)로, 원래의 오픈 소스(open-source) 라이선스 약관 외에 어떠한 제약도 없습니다(일부 다른 오픈 웨이트(open-weight) LLM은 추가적인 사용 제한을 부과합니다).
*   성능이 정말 좋습니다. 예를 들어, 이 글을 쓰는 시점에서 오픈 웨이트(open-weight) 235B-Instruct 변형 모델은 LMArena 리더보드에서 8위를 차지했으며, 독점 모델인 Claude Opus 4와 동률입니다. 더 높은 순위를 차지한 다른 오픈 웨이트(open-weight) LLM은 단 두 개뿐인데, DeepSeek 3.1 (3배 더 큼)과 Kimi K2 (4배 더 큼)입니다. 9월 5일에 Qwen3는 자사 플랫폼에 1조(1T) 파라미터(parameter) "max" 변형 모델을 출시했는데, 이 모델은 모든 주요 벤치마크(benchmark)에서 Kimi K2, DeepSeek 3.1, Claude Opus 4를 능가합니다. 하지만 이 모델은 현재 클로즈드 소스(closed-source)입니다.
*   0.6B 밀집(dense) 모델부터 480B 파라미터(parameter) 전문가 혼합(Mixture-of-Experts) 모델까지, 다양한 컴퓨팅 예산(compute budget)과 사용 사례(use-case)를 위한 여러 가지 모델 크기가 제공됩니다.

순수 파이토치(PyTorch)로 처음부터 작성된 코드 때문에 이 글은 길어질 것입니다. 코드 섹션이 장황해 보일 수 있지만, 개념적인 그림만으로는 설명하기 어려운 구성 요소(building blocks)들을 더 잘 설명하는 데 도움이 되기를 바랍니다!

**팁 1:** 이 글을 이메일 받은 편지함에서 읽고 있다면, 좁은 줄 너비 때문에 코드 스니펫(code snippet)이 어색하게 줄 바꿈될 수 있습니다. 더 나은 경험을 위해 웹 브라우저에서 여는 것을 권장합니다.
**팁 2:** 웹사이트 왼쪽에 있는 목차를 사용하여 섹션(section) 간 이동을 더 쉽게 할 수 있습니다.

**그림 1:** 이 글에서 논의되고 순수 파이토치(PyTorch)로 (재)구현된 Qwen3 밀집(Dense) 및 전문가 혼합(Mixture-of-Experts) 아키텍처(architecture) 미리보기.