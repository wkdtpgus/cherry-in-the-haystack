최근 저는 "대규모 언어 모델 아키텍처 심층 비교"라는 글을 통해 2025년까지 가장 큰 영향력을 발휘할 것으로 예상되는 공개 가중치(open-weight) 모델들의 구조를 분석했습니다. 그 후 "GPT-2부터 gpt-oss까지: 아키텍처 진화의 핵심 탐구"에서는 여러 아키텍처 구성 요소(architectural components)들을 개념적 관점에서 더욱 깊이 있게 다루었습니다. 이론적 탐구를 넘어 실질적인 구현으로 나아갈 때가 되었다고 판단하여, 올여름 주목할 만한 연구 성과들을 논하기 전에, 이제 이러한 구조들을 직접 코드로 구현해보는 경험을 공유하고자 합니다. 본 글을 따라가다 보면, 거대 언어 모델(LLM)이 실제로 어떻게 동작하는지 명확히 이해하게 될 것이며, 여러분의 독자적인 실험이나 프로젝트에 활용할 수 있는 핵심 구성 요소(building blocks)들을 손에 넣게 될 것입니다. 이를 위해 저는 Qwen3 모델군(5월 최초 출시 후 7월 업데이트)을 선정했습니다. 이 모델은 현재 가장 광범위하게 선호되고 활용되는 오픈 웨이트(open-weight) 모델 계열 중 하나로 평가받기 때문입니다.

Qwen3 모델이 이토록 높은 인기를 누리는 데에는 몇 가지 주요 요인이 있다고 생각합니다:

*   개발자 및 상업적 목적으로 자유롭게 사용 가능한 아파치 라이선스 v2.0 기반의 개방형 모델(open-source)이라는 점입니다. 이는 기본적인 오픈 소스(open-source) 라이선스 조건 외에 어떠한 추가적인 사용 제약도 부과되지 않는다는 의미로, 다른 일부 공개 가중치(open-weight) LLM들이 특정 제한을 두는 것과 대조됩니다.
*   실로 뛰어난 성능을 자랑합니다. 예를 들어, 이 글을 작성하는 시점에서 공개된 235B-Instruct 변형 모델은 LMArena 순위표에서 8위를 기록하며, 독점 모델인 Claude Opus 4와 동일한 수준의 경쟁력을 보여주었습니다. 이보다 높은 순위를 차지한 공개 가중치(open-weight) LLM은 DeepSeek 3.1 (약 3배 더 큰 규모)와 Kimi K2 (약 4배 더 큰 규모) 단 두 개뿐입니다. 9월 5일, Qwen3는 자체 플랫폼에 1조(1T) 파라미터(parameter) 규모의 "max" 변형 모델을 선보였는데, 이 모델은 주요 벤치마크(benchmark)에서 Kimi K2, DeepSeek 3.1, Claude Opus 4를 모두 능가하는 성능을 입증했습니다. 다만, 이 최상위 모델은 현재 비공개 소스(closed-source) 형태로 제공됩니다.
*   0.6B 밀집(dense) 모델부터 480B 파라미터(parameter) 전문가 혼합(Mixture-of-Experts, MoE) 모델에 이르기까지, 다양한 컴퓨팅 자원(compute budget)과 활용 시나리오(use-case)에 맞춰 선택할 수 있는 폭넓은 모델 크기를 제공합니다.

이번 글은 순수 파이토치(PyTorch)로 처음부터 구현된 코드를 다루기 때문에 분량이 길어질 수 있습니다. 코드 섹션이 다소 장황하게 느껴질 수도 있지만, 이는 추상적인 개념도만으로는 설명하기 어려운 핵심 구성 요소(building blocks)들을 더욱 명확히 이해하는 데 도움이 될 것입니다. 또한, LLM의 내부 작동 방식을 깊이 탐구하고 싶다면, 단순히 모델을 호출하는 것을 넘어 그 내부 구조를 직접 구현해보는 것이 가장 효과적인 학습 방법임을 강조하고 싶습니다.

**팁 1:** 이 글을 이메일 받은 편지함에서 읽고 있다면, 좁은 줄 너비 때문에 코드 스니펫(code snippet)이 어색하게 줄 바꿈될 수 있습니다. 더 나은 경험을 위해 웹 브라우저에서 여는 것을 권장합니다.
**팁 2:** 본 문서에서 제시되는 코드 예제들은 최신 파이토치(PyTorch) 버전과 호환되도록 작성되었습니다. 혹시 오류가 발생한다면, 사용 중인 파이토치(PyTorch) 환경을 최신 버전으로 업데이트하는 것을 권장합니다.
**팁 3:** 웹사이트 왼쪽에 있는 목차를 사용하여 섹션(section) 간 이동을 더 쉽게 할 수 있습니다.
**팁 4:** 대규모 언어 모델(LLM)의 구조는 복잡하므로, 각 코드 블록을 실행하기 전에 주석과 주변 설명을 꼼꼼히 읽어보는 것이 중요합니다. 이는 각 부분이 전체 아키텍처(architecture)에서 어떤 역할을 하는지 이해하는 데 큰 도움이 됩니다.

**그림 1:** 본 글에서 다룰 Qwen3 밀집(Dense) 및 전문가 혼합(Mixture-of-Experts) 아키텍처(architecture)의 핵심 구성 요소들을 파이토치(PyTorch) 코드로 구현한 모식도. 주요 모듈 간의 데이터 흐름을 시각적으로 나타낸다.