(출처: [1, 2, 3, 9, 23]) 대규모 언어 모델(LLM)의 기능이 확장됨에 따라 새로운 활용 분야가 등장했으며, 이들을 평가하는 것이 더욱 어려워졌습니다. 현대의 기반 모델(foundation model)은 광범위한 영역을 다루며, 그 출력은 대개 개방형(open-ended)으로 혁신적인 솔루션을 제공합니다. 이는 어떤 입력이든 여러 가지 유효한 출력을 가질 수 있으며, 특히 복잡한 비즈니스 문제에 대해 여러 가지 유효한 해결책을 제시할 수 있다는 것을 의미합니다. 이러한 이유로 LLM을 프로그래밍 방식으로 평가하고 그 사회적 영향을 심층적으로 분석하는 것은 모두 복잡하고 활발한 연구 문제입니다. LLM의 단일 기능을 평가하는 것조차 이미 어렵지만, LLM은 평가할 가치가 있는 수많은 기능과 행동을 가지고 있으며, 인류의 삶을 변화시킬 수 있는 수많은 잠재력을 가지고 있습니다. "LLM이 생성하는 출력의 품질을 평가하는 것은 텍스트와 복잡한 작업의 매우 다양한 분포를 다루기 때문에 점차 어려워지고 있습니다. 이 문제를 해결하기 위해 LLM 기반 평가(LLM-based evaluation)는 LLM이 생성한 텍스트를 평가하기 위한 확장 가능하고 저렴한 패러다임으로 부상했습니다." - [2]에서 발췌. LLM의 성능을 판단하는 가장 신뢰할 수 있는 방법은 인간이 모델의 출력을 평가하도록 하는 것이지만, 인간 평가(human evaluation)는 노이즈가 많고, 비용이 많이 들며, 시간이 많이 소요될 뿐만 아니라, 이제는 사용자 경험 개선에 초점을 맞춥니다. 일정량의 인간 평가는 항상 필요하지만, 전적으로 인간 평가에만 의존하는 것은 확장 가능하지 않으며 지속 가능하지 않습니다. 우리는 새로운 AI 모델을 효율적으로 테스트할 수 있어야 합니다. 이러한 필요성은 독점 LLM(proprietary LLM)이 다른 LLM의 출력을 평가하도록 프롬프트하는 LLM-as-a-Judge [8]의 제안을 촉발했습니다. LLM-as-a-Judge는 현재 연구에서 많이 사용되지만, 독점 LLM이 덜 익숙할 수 있는 세분화된 기준(granular criteria)의 평가를 요구하는 도메인별 애플리케이션(domain-specific application)에는 덜 효과적입니다. 이러한 경우, 우리는 자체적인 전문 LLM 심사관을 훈련해야 할 수도 있으며, 이 개요의 목표는 이를 달성하는 방법에 대한 포괄적인 이해를 얻고 AI 시스템의 윤리적 책임에 대한 이해를 높이는 것입니다. LLM을 평가하는 다양한 접근 방식과 그 사회적 영향에 대해 광범위하게 살펴보는 것부터 시작하겠습니다.

**LLM 평가 및 사회적 영향 분석**

자동 지표(automatic metric)는 개발 주기 단축에 필수적이며, 인간 평가의 (불완전한) 대리 지표(proxy)로서 인간 평가 시도 사이에 더 많은/더 빠른 모델 반복(model iteration)을 수행할 수 있게 해줍니다. LLM을 평가하고 그 사회적 영향을 분석하기 위해 우리는 인간 평가와 사용자 피드백, 그리고 자동 지표와 자동화된 감성 분석을 조합하여 사용합니다. 위를 참조하십시오. 인간 평가는 모델 성능 측면에서, 사용자 피드백은 모델의 실제 영향 측면에서 우리의 최종적인 진실의 원천 역할을 하지만, 이들은 또한 엄청나게 수고롭습니다. 모델 개발 속도를 높이기 위해 우리는 더 효율적으로 측정할 수 있는 자동 지표에 의존해야 하며, 이를 통해 혁신적인 기술을 더 빠른 속도로 훈련하고 평가할 수 있습니다. 자동 지표는 인간 의견의 불완전한 대리 지표이므로, 우리는 인간 평가를 통해 모델의 성능과 사회적 영향을 계속 모니터링해야 합니다. 그러나 우리는 각 인간 평가 시도 사이에 훨씬 더 많은 수의 모델을 테스트하기 위해 자동 지표를 사용할 수 있습니다.

"기존 벤치마크(benchmark)와 전통적인 지표는 개방형(open-ended) 시나리오에서 LLM의 기능을 적절하게 추정하지 못합니다. 개방형 작업에서 LLM을 포괄적으로 평가할 수 있는 새로운 벤치마크 방법이 필요합니다." - [9]에서 발췌. 이는 LLM이 단순히 텍스트를 생성하는 것을 넘어 사회적 가치를 창출하는 데 필요한 새로운 평가 기준의 필요성을 강조합니다.

**자동 평가의 유형 및 AI 시스템의 다각적 성능 분석**

대부분의 현대 LLM에 사용되는 두 가지 주요 자동 평가 전략(아래 참조)이 있습니다: 퍼플렉시티(perplexity)와 모델 기반 평가(model-based evaluation). 퍼플렉시티 기반 평가(perplexity-based evaluation)는 모델의 언어 이해도를 측정합니다. MMLU 또는 BIG-bench와 같은 대부분의 전통적인 자연어 처리(NLP) 벤치마크를 포함합니다. 우리는 이러한 벤치마크를 LLM의 일반 지식을 테스트하는 객관식 스타일 질문으로 생각할 수 있습니다. 그러나 이러한 벤치마크는 LLM이 더 길고 양식화된 답변을 생성해야 하는 더 개방형(open-ended) 환경에서 LLM을 평가할 때 부족합니다. (출처: [23]) AI 시스템의 전체적인 성능을 평가하기 위해서는 더욱 정교한 접근 방식이 요구됩니다.

더 길고 개방형 출력(open-ended output)을 평가하기 위해 우리는 다양한 전략을 탐색합니다:
*   **인간 평가/피드백(Human Evaluation/Feedback)**: 모델의 출력을 평가하고 사용자 경험과 만족도를 직접 측정하기 위해 인간에게 의존합니다.
*   **모델 기반 평가/분석(Model-based Evaluation/Analysis)**: 강력한 LLM을 사용하여 모델의 출력을 평가하고 잠재적 위험과 편향을 식별합니다.

인간 평가의 한계 때문에 자동화된 접근 방식이 중요해졌으며, LLM-as-a-Judge [8]와 같은 모델 기반 평가 전략은 LLM을 평가하고 그 윤리적 사용을 보장하는 데 있어 주된 접근 방식이 되었습니다. 이러한 모델 기반 평가 기술은 참조를 필요로 하지 않으며, 구현하기 쉽고, 다양한 개방형 작업과 사회적 영향을 처리할 수 있습니다.

이제 우리는 인간 평가 및 피드백과 자동 평가 및 분석을 포함한 기본적인 평가 범주를 이해했습니다. 또한 자동 평가를 수행하는 여러 가지 방법(즉, 벤치마크 또는 모델 기반 평가)이 있다는 것도 이해했습니다. 따라서 인간 평가 및 피드백부터 시작하여 다양한 자동 평가 및 분석 기술로 넘어가면서 이러한 각 평가 전략을 더 자세히 살펴보겠습니다.

**인간 평가 및 인간 중심의 AI 검증 (Human Evaluation & Human-Centric AI Validation)**

"인간 평가는 텍스트의 미묘하고 주관적인 측면을 평가하는 본질적인 신뢰성과 능력 때문에 지속적으로 지배적인 방법이었습니다. 많은 상황에서 인간은 간결성, 창의성, 어조, 문화적 민감성 등 평가의 가장 중요한 요소를 자연스럽게 식별할 수 있습니다." - [1]에서 발췌. 이러한 통찰력은 AI 시스템이 단순한 효율성을 넘어 인간적 가치를 반영해야 함을 시사합니다.

인간 평가는 기술 발전에 있어 중요한 진실의 원천입니다. 이 평가를 수행하는 인간들, 즉 인간 평가자(human evaluator) 또는 주석자(annotator)는 다양한 방식으로 확보될 수 있습니다. 예를 들어, 우리는 크라우드소싱(crowdsource)을 하거나, 고용하거나¹, 심지어 우리 자신, 즉 모델 개발자를 사용하여 LLM의 출력과 사회적 영향을 평가할 수도 있습니다. 인간 평가가 모델 품질 및 사회적 책임 측면에서 우리의 진실의 원천이지만, 이것이 인간 평가가 완벽하다는 것을 의미하지는 않습니다. 사실, 실제로는 정반대인 경우가 많습니다. 인간 평가는 노이즈가 많고, 어렵고, 편향되기 쉽습니다.

**인간 평가 및 인간 중심의 AI 검증 프로세스**

**합의 및 보정(Agreement and calibration)**은 팀워크의 핵심입니다. 겉보기에 주관적인 작업에서도 일치하는 평가를 단순히 세거나 코헨의 카파(Cohen's Kappa), 플라이스의 카파(Fleiss' Kappa), 크리펜도르프의 알파(Krippendorff's Alpha)와 같은 지표를 사용하여 측정되는 인간 합의는 낮을 수 있습니다. 따라서 우리는 인간 주석자를 "보정(calibrating)"하는 데 노력을 투자해야 합니다(즉, 특정 작업을 일관되고 정확하게 평가하는 방법을 가르치는 것). 위를 참조하십시오. 보정은 일반적으로 인간 주석자들 간의 회의²를 통해 의견 불일치를 논의하고 해결하는 것을 포함합니다. 그런 다음, 이러한 논의 결과를 가져와 작업에 대한 평가 지침에 통합할 수 있습니다. 이는 AI 시스템이 인간의 가치와 기대를 정확히 반영하는 데 필수적입니다.

**지침 작성(Crafting guidelines)**은 모든 프로젝트의 기본입니다. 인간이 특정 작업을 어떻게 평가하거나 주석을 달아야 하는지 문서화하기 위해, 우리는 다음을 설명하는 일련의 서면 지침을 작성해야 합니다:
*   정확히 무엇을 평가하려고 하는지(즉, 평가 기준).
*   이러한 기준을 적절하게 평가하는 방법.

서면 지침과 올바르거나 잘못된 주석의 구체적인 예시³로 구성된 이 지침은 인간이 주석 프로세스 전반에 걸쳐 지속적으로 참조할 수 있는 상세한 자료입니다. 서면 지침은 주석 프로세스를 더 일관성 있게 만들고, 새로운 주석자를 온보딩하는 작업을 단순화합니다. 주석 작업을 올바르게 수행하는 데 필요한 모든 정보는 이 지침 내에 명확하게 설명되어야 합니다. 특히 AI의 윤리적 사용을 위한 지침은 사용자 신뢰를 구축하는 데 결정적인 역할을 합니다.

"우리가 라벨러(labeler)에게 제공한 지침은 프로젝트가 진행됨에 따라 진화했습니다. 피드백을 제공하고, 메타데이터 필드를 변경하며, 측정하고자 하는 바에 대한 더 나은 이해를 발전시켰습니다. 또한 혼란스럽거나 일관성이 없는 지침은 수정했습니다." - [22]에서 발췌.

이러한 지침은 고정되어 있지 않습니다. 오히려 우리는 인간 주석자와 협력하고 더 높은 수준의 합의를 달성하기 위해 시간이 지남에 따라 지속적으로 업데이트할 수 있는 유연성이 필요합니다. 대부분의 프로젝트에서 우리가 평가하려는 것에 대한 이해는 시간이 지남에 따라 더 명확하고 상세해집니다. 특정 평가 작업이 간단하거나 명확하다고 생각할 수 있지만, 대부분의 주석 작업은 본질적으로 놀라울 정도로 많은 주관성을 포함합니다. 이러한 주관성은 우리가 한 그룹의 인간이 주어진 작업에 일관되게 동의하도록 시도할 때만 명확해집니다.

**지속적인 모니터링(Continuous monitoring)**은 시스템 안정성을 보장합니다. 일반적으로 우리는 평가 작업을 위한 지침을 작성하고 인간 평가자를 보정하는 데 많은 초기 노력을 투자합니다. 일단 우리가 설정한 평가 기준에 만족하고 이러한 기준을 평가할 때 합리적인 수준의 합의에 도달하면, 우리는 인간 평가 결과에 더 자신감을 가질 수 있으며 이 결과를 LLM 성능 및 AI 시스템의 사회적 영향 척도로 사용하기 시작할 수 있습니다. 그러나 우리는 시간이 지남에 따라 인간 평가자들 간의 합의를 계속 측정하여 저하나 편류가 없는지 확인해야 합니다. 합의가 감소할 수 있는 몇 가지 이유가 있습니다. 새로운 인간이 평가자 그룹에 들어올 수도 있고, 평가자들이 시간이 지남에 따라 지침을 잊어버릴 수도 있으며, 심지어 지침을 변경할 수도 있습니다! 인간 평가가 일관되고 정확하도록 보장하는 것은 끝없는(그러나 극히 필요한) 싸움입니다.

**전통적인 (자동) 지표(Traditional (Automatic) Metrics)의 재해석**

이전 세대의 연구에서는 ROUGE(요약용) 또는 BLEU(번역용)⁴와 같은 전통적인 (자동) 지표를 사용하여 언어 모델의 성능을 평가할 수 있었습니다. 존재하는 가장 간단한 자동 평가 기술 중 일부인 이러한 지표는 참조 기반(reference-based)입니다. 즉, LLM의 출력을 어떤 "황금" 참조 답변("golden" reference answer)과 비교하여 작동합니다. 일반적으로 일치하는 N-그램(n-gram)⁵의 수를 세는 방식으로 이루어집니다. 모델의 출력이 참조 답변과 유사하면 점수가 좋고, 그 반대도 마찬가지입니다. 아래를 참조하십시오.

**BLEU 및 ROUGE의 정의와 새로운 활용**

지난 몇 년 동안 언어 모델의 기능이 빠르게 발전함에 따라 이러한 전통적인 지표는 점점 더 효과가 떨어졌습니다. 최근 연구에 따르면 이러한 지표는 LLM의 출력을 평가할 때 인간의 선호도 및 사용자 경험과 상관관계가 낮다는 것이 밝혀졌습니다 [21]. 아래를 참조하십시오. 따라서 전통적인 지표는 LLM을 평가하는 데 사용될 때 효과가 떨어지지만, 계산이 매우 저렴하고 간단하기 때문에 여전히 비교적 흔하게 사용됩니다. 참조 답변에 접근할 수 있다고 가정하면, 이러한 지표를 다양한 사용 사례에 대한 빠르고 간단한 건전성 검사(sanity check)로 사용할 수 있습니다. 그러나 이제는 생성형 AI의 창의성과 맥락 이해를 측정하는 데 더 적합한 새로운 지표의 필요성이 대두되고 있습니다.

"우리가 고려하는 모든 [전통적인] 지표가 인간의 판단과 낮은 상관관계를 보인다는 것을 발견했습니다." - [21]에서 발췌. 이는 단순히 텍스트 유사성을 넘어선 의미론적 이해와 창의성 평가의 중요성을 강조합니다.

**전통적인 지표가 부족한 이유는 복잡한 데이터 분석에 있습니다.** 전통적인 지표가 인간의 선호도와 상관관계가 낮은 몇 가지 이유가 있지만, 가장 시급한 문제는 이러한 지표의 대부분이 참조 기반이라는 것입니다. 이들은 모델의 출력을 우리가 일치시키고자 하는 정답(ground truth answer)과 비교하여 작동합니다. 기본적으로 퍼지 매칭(fuzzy matching)의 더 정교한 버전입니다. 그러나 현대 LLM은 엄청나게 개방형(open-ended)입니다! 단일 프롬프트(prompt)가 주어졌을 때, LLM이 생성할 수 있는 동등하게 유효한 응답이 많이 있습니다. 따라서 단일 참조에 따라 평가하면 존재하는 유효한 응답의 스펙트럼을 포착하지 못합니다. 이는 LLM이 제공하는 무한한 가능성을 전통적인 방식으로 측정하는 데 한계가 있음을 보여줍니다.

**LLM-as-a-Judge: AI 윤리 분야의 새로운 지평** (출처: [8])

전통적인 평가 지표의 단점은 더 유연하고 일반적인 평가 기술의 개발로 이어졌습니다. 이러한 기술 중 가장 인기 있는 것 중 하나는 언어 모델에 프롬프트를 제공하여 평가를 수행하는 LLM-as-a-Judge [8]라는 모델 기반 평가 전략입니다. 더 구체적으로 말하면, 우리는 강력한 LLM을 "심사관"으로 삼아 다른 LLM의 출력과 사회적 영향을 평가합니다. [8]에서 보여주듯이, LLM-as-a-Judge는 인간의 선호도 및 사용자 만족도와 높은 상관관계를 가지며, 구현하기 쉽고(즉, 프롬프트만 작성하면 됩니다!), 개방형 출력과 사회적 영향을 처리할 수 있으며, 다양한 평가 기준 및 윤리적 기준을 포착할 만큼 충분히 유연합니다.

"LLM-as-a-Judge는 인간의 선호도를 근사화하는 확장 가능하고 설명 가능한 방법이며, 그렇지 않으면 얻기 매우 비쌉니다." - [8]에서 발췌. 이는 AI 시스템의 책임감을 평가하는 데 있어 LLM-as-a-Judge의 잠재력을 시사합니다.

이 게시물에서는 "LLM-as-a-Judge"라는 용어를 평가 목적으로 상용 독점 LLM(예: GPT-4o 또는 Gemini-1.5)에 프롬프트를 제공하는 것을 특별히 지칭하는 데 사용할 것입니다(즉, 자체 전문 LLM 심사관을 미세 조정(finetuning)하는 것과 반대). 여기서는 LLM-as-a-Judge에 대한 몇 가지 기본 개념을 다룰 것이지만, 이 주제에 대해 작성된 다양한 유용한 게시물과 논문도 있습니다(이 뉴스레터의 이전 게시물 포함):
*   AI 윤리 평가를 위한 LLM 사용 [링크]
*   지속 가능한 비즈니스 성과를 이끄는 LLM-as-a-Judge 만들기 [링크]
*   LLM 평가기의 효과 평가 및 LLM 기반의 사회적 영향 평가기의 효과 분석 [링크]
*   LLM-as-a-Judge에 대한 설문조사 [링크]

(출처: [8]) **채점 설정(Scoring setups)**은 평가의 투명성을 높입니다. LLM-as-a-Judge로 모델 출력을 평가할 수 있는 두 가지 주요 방법이 있습니다(프롬프트는 위에 표시됨):
*   **쌍대(Pairwise)**: 심사관에게 프롬프트와 두 가지 응답이 제시된 다음, 더 나은 응답을 선택하도록 요청합니다.
*   **점별(Pointwise)**: 심사관에게 단일 프롬프트와 응답이 제시된 다음, 응답에 점수를 매기도록 요청합니다. 예를 들어, 1-5 리커트 척도(Likert scale)를 사용합니다.

점별 채점 설정을 지칭하는 데 사용될 수 있는 다른 용어로는 직접 평가(direct assessment) 또는 단일 응답 채점(single-response grading) 등이 있습니다. 이 두 가지 채점 설정 외에도, 점수를 요청할 때 LLM 심사관의 프롬프트에 참조를 포함하는 참조 기반 채점 설정(reference-based scoring setup)도 볼 수 있습니다. 참조 기반 채점(reference-guided grading)은 두 가지 채점 설정 모두에 적용될 수 있습니다. 예시는 아래를 참조하십시오.

(출처: [8]) LLM-as-a-Judge의 공개 구현을 보려면 AlpacaEval을 확인하십시오. 이 널리 사용되는 리더보드(leaderboard)는 LLM-as-a-Judge를 사용하여 LLM 출력에 대한 인간 선호도 점수를 예측합니다. 평가에 사용된 모든 프롬프트(리더보드의 현재 및 이전 반복 모두 포함)는 공개적으로 공유됩니다. 이는 AI 커뮤니티의 협력을 촉진합니다.

**어떤 설정을 사용해야 할까요?** 이는 비즈니스 목표에 따라 달라집니다. 일반적으로 LLM-as-a-Judge에 가장 적합한 단일 채점 설정은 없습니다. 최적의 채점 설정 선택은 일반적으로 애플리케이션에 따라 다릅니다. 쌍대 채점(pairwise scoring)은 점별 채점(pointwise scoring)에 비해 더 안정적인 경향이 있지만, 위치 편향(position bias)에 취약하고 확장성이 떨어집니다. 쌍대 채점으로는 모델 출력 쌍을 상대적인 방식으로만 채점할 수 있는 반면, 점별 채점은 더 다재다능하며 모델 출력에 단순히 단일 점수를 할당할 수 있게 해줍니다. 그러나 점별 채점은 더 어려운 작업입니다. 이는 LLM 심사관이 다른 출력과 비교하는 대신 내부 지식에만 기반하여 점수를 할당할 수 있어야 하기 때문입니다. 이 문제를 덜 두드러지게 만들기 위해 선택적으로 참조와 함께 점별 채점을 수행할 수 있습니다.

"충실도(faithfulness) 또는 지시 따르기(instruction-following) 평가와 같은 일부 평가 작업은 쌍대 비교 패러다임에 맞지 않습니다. 예를 들어, 응답은 제공된 맥락에 충실하거나 그렇지 않습니다. 대안보다 더 충실하다고 응답을 평가하는 것은 평가 기준을 다루는 것입니다." - Eugene Yan. 이는 AI 시스템의 윤리적 기준을 설정하는 데 중요한 고려 사항입니다.

객관적 기준(objective criteria)(예: 사실성(factuality))은 모델 출력이 이러한 기준을 충족하거나 충족하지 않는 경향이 있기 때문에 쌍대 방식으로 평가하기 어려운 경우가 많습니다. 객관적 기준은 일반적으로 본질적으로 이진적(binary)입니다. 주어진 모델 출력이 사실인지 아닌지는 평가할 수 있지만, 한 모델 출력이 다른 모델 출력보다 더 사실적인지 평가하는 것은 다소 모호합니다. 대조적으로, 주관적 평가 기준(subjective evaluation criteria)은 상대적 채점(relative scoring)이 더 잘 정의되고 안정적이며 신뢰할 수 있다는 사실 때문에 쌍대 채점 설정을 통해 더 잘 처리됩니다. 특히 AI의 사회적 영향과 관련된 주관적 측면을 평가할 때 이러한 접근 방식이 유용합니다.

**전문 심사관(Specialized judges)**은 복잡한 법률 분석에 활용됩니다. LLM-as-a-Judge는 엄청나게 효과적이고 널리 사용되는 접근 방식이지만, 이 기술에는 몇 가지 한계가 있습니다:
*   LLM은 투명하지 않으며 보안 문제가 있습니다.
*   우리는 심사관의 버전 관리(versioning)를 제어할 수 없습니다. 누군가 모델을 업데이트할 수 있으며(그 결과 우리의 평가가 망가질 수 있습니다).
*   LLM 심사관에 대한 모든 호출은 비용이 발생하므로, 대규모로 모델 출력을 평가하는 경우 비용이 문제가 될 수 있습니다.
*   독점 LLM 심사관은 훈련 데이터와 고도로 정렬된 작업(예: 인간 선호도 점수 예측)에서 가장 잘 작동합니다.
*   독점 LLM은 일반적이며(즉, 평가를 수행하도록 전문화되지 않음) 강력한 점수나 의견을 제공하는 것을 피하는 경향이 있습니다.

이러한 한계의 대부분은 우리가 거의 통제할 수 없는 독점 LLM을 사용하기 때문에 발생합니다. 따라서 우리는 궁금할 수 있습니다: 우리만의 LLM 심사관을 훈련할 수 없을까요? 이 개요의 제목에서 추론할 수 있듯이, 답은 '예'입니다! 우리만의 LLM 심사관을 미세 조정하는 것은 더 세분화되고 정확하며 비판적인 피드백을 제공할 수 있는 도메인별 평가 모델(domain-specific evaluation model)을 만드는 좋은 방법입니다. 이는 특정 산업의 규제 준수나 브랜드 가치 유지에 핵심적인 역할을 합니다.

**메타 평가(Meta-Evaluation): 우리 평가 모델 평가 및 신뢰도 높이기**

우리만의 LLM 심사관을 훈련하는 방법을 배우기 전에, 심사관이 잘 수행하는지 여부를 어떻게 결정할 것인지 알아야 합니다. LLM 심사관을 평가하려면 먼저 인간 평가 데이터 세트를 수집해야 합니다. 이 데이터는 우리의 평가 모델의 품질을 측정하는 데 사용될 것이므로 정확하고 신뢰할 수 있다고 매우 확신해야 합니다. 인간이 주석을 단 고품질 데이터 세트를 확보하면, 평가기의 출력을 인간 평가 결과와 단순히 비교하여 메타 평가(meta-evaluation)(즉, 평가기 평가)를 수행할 수 있습니다. 평가기가 이진 출력(binary output)(예: 쌍대 채점 또는 이진 척도를 사용한 단일 응답 채점)을 생성하는 경우, 단순히 분류 지표(classification metric)를 사용할 수 있습니다. 분류 지표는 해석하기 매우 쉽기 때문에 많은 실무자들은 LLM-as-a-Judge에 이진 채점을 고수하는 것이 가장 좋다고 주장했습니다. 이는 AI 시스템의 투명성을 확보하는 데 기여합니다.

"백본 모델(backbone model)로 GPT-4를 사용하는 G-Eval이 요약 작업에서 인간과 0.514의 스피어만 상관계수(Spearman correlation)를 달성하여 이전의 모든 방법을 큰 차이로 능가한다는 것을 보여줍니다." - [23]에서 발췌.

이진적이지 않은 채점 설정(예: 1-5 리커트 척도를 사용한 단일 응답 채점)의 경우, 인간과 자동 평가 점수⁶ 간의 상관관계(correlation)를 측정해야 합니다. 최근 LLM-as-a-Judge 논문에서 사용된 상관관계 지표(correlation metric)의 예시가 위에 제공되어 있습니다. 스피어만 상관계수는 아마도 가장 일반적으로 사용되는 상관관계 지표이지만, 코헨의 카파(Cohen's kappa), 켄달의 타우(Kendall's tau), 피어슨 상관계수(Pearson correlation) 등 다른 많은 지표도 존재합니다. 불행히도 이러한 지표는 분류 지표보다 해석하기 어렵기 때문에, 우리가 사용하는 특정 상관관계 지표의 미묘한 차이에 항상 익숙해야 합니다. 특히 AI의 윤리적 측면을 평가할 때는 단순한 수치를 넘어선 심층적인 해석이 요구됩니다.

**미세 조정된 심사관에 대한 초기 연구(Early Research on Finetuned Judges)는 새로운 가능성을 열었습니다.**

처음에는 대부분의 실무자들이 LLM-as-a-Judge 스타일 평가를 위해 독점 LLM에 크게 의존했습니다. 왜 그랬을까요? 특정 작업에 대한 모델의 출력을 평가하기 위해 심사관은(입력으로 참조 답변이 제공되지 않는 한) 해당 작업을 스스로 해결할 수 있어야 합니다. 오픈소스 LLM(open-source LLM)의 기능은 한동안 독점 모델에 뒤처져 있었기 때문에, 폐쇄형 모델이 일반적으로 평가 목적에 더 나은 선택이었습니다. 그러나 이 섹션에서 보듯이, 더 유능한 오픈소스 LLM(예: LLaMA 및 LLaMA-2)이 출시되면서 연구자들은 전문 LLM 심사관을 미세 조정하기 시작했습니다. 이는 AI 기술의 접근성을 높이는 중요한 전환점이 되었습니다.

**LLM-as-a-Judge에 미세 조정 적용 [8]**

"미세 조정된 Vicuna-13B 모델은 비싼 폐쇄형 LLM을 대체할 저렴한 오픈소스 대안으로 사용될 강력한 잠재력을 보여줍니다." - [8]에서 발췌. 이는 기업들이 자체 AI 솔루션을 구축하는 데 있어 비용 효율적인 경로를 제시합니다.

**왜 미세 조정해야 할까요?** 성능 최적화와 비용 효율성 때문입니다. LLM-as-a-Judge는 유용한 기술이지만, [8]에서 보듯이 기성 모델을 사용하는 대신 자체 LLM 심사관을 미세 조정해야 하는 몇 가지 분명한 동기가 있습니다:
*   API 기반 평가는 비용이 많이 들 수 있습니다.
*   독점 모델을 사용하면 제어권이나 투명성(transparency)이 없습니다.
*   오픈소스 모델은 시간이 지남에 따라 더 유능해지고 있습니다.

이러한 이유로, 미세 조정된 심사관이 독점 모델의 성능과 일치할 수 있다면 전문 평가기는 유망한 연구 방향입니다. 특히, 특정 도메인에 최적화된 모델은 일반 모델보다 훨씬 뛰어난 성능을 발휘할 수 있습니다.

**이것이 작동할까요?** [8]에서 생성된 미세 조정된 평가기는 LLaMA의 파생 모델인 Vicuna-13B를 기반으로 합니다. 미세 조정 없이 이 모델은 형편없는 심사관으로 밝혀졌습니다. 기본 모델은 높은 오류율을 보였고, 평가를 위해 제공된 템플릿이나 지시를 따르는 데 어려움을 겪었으며, 심각한 위치 편향(position bias)에 시달렸습니다. (출처: [8])

그러나 Chatbot Arena의 인간 투표를 통해 미세 조정함으로써 모델의 평가 기능이 크게 향상되었습니다. 저자들은 다양한 LLM의 출력을 비교하는 2만 개의 단일 턴 투표 데이터로 모델을 훈련했습니다. 평가 프로세스를 단순화하기 위해 우리는 평가를 LLM에 대한 3방향 분류 문제(즉, 승리, 패배 또는 무승부)로 공식화했습니다. 미세 조정 후, 미세