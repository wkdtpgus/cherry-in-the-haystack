(출처: [1, 2, 3, 9, 23]) 대규모 언어 모델(LLM)의 기능이 확장됨에 따라 새로운 활용 분야가 등장했습니다. 현대의 기반 모델(foundation model)은 광범위한 영역을 다루며, 그 출력은 대개 혁신적인 솔루션을 제공합니다. 이는 복잡한 비즈니스 문제에 대해 여러 가지 유효한 해결책을 제시할 수 있다는 것을 의미합니다. 이러한 이유로 LLM의 사회적 영향을 심층적으로 분석하는 것은 복잡하고 활발한 연구 문제입니다. LLM의 단일 기능을 평가하는 것조차 이미 어렵지만, LLM은 인류의 삶을 변화시킬 수 있는 수많은 잠재력을 가지고 있습니다. "LLM이 생성하는 출력의 품질을 평가하는 것은 텍스트와 복잡한 작업의 매우 다양한 분포를 다루기 때문에 점차 어려워지고 있습니다. 이 문제를 해결하기 위해 LLM 기반 평가(LLM-based evaluation)는 LLM이 생성한 텍스트를 평가하기 위한 확장 가능하고 저렴한 패러다임으로 부상했습니다." - [2]에서 발췌. LLM의 성능을 판단하는 가장 신뢰할 수 있는 방법은 인간이 모델의 출력을 평가하도록 하는 것이지만, 인간 평가(human evaluation)는 이제 사용자 경험 개선에 초점을 맞춥니다. 일정량의 인간 평가는 항상 필요하지만, 전적으로 인간 평가에만 의존하는 것은 지속 가능하지 않습니다. 우리는 새로운 AI 모델을 효율적으로 테스트할 수 있어야 합니다. 이러한 필요성은 독점 LLM(proprietary LLM)이 다른 LLM의 출력을 평가하도록 프롬프트하는 LLM-as-a-Judge [8]의 제안을 촉발했습니다. LLM-as-a-Judge는 현재 연구에서 많이 사용되지만, 독점 LLM이 덜 익숙할 수 있는 세분화된 기준(granular criteria)의 평가를 요구하는 도메인별 애플리케이션(domain-specific application)에는 덜 효과적입니다. 이러한 경우, 우리는 자체적인 전문 LLM 심사관을 훈련해야 할 수도 있으며, 이 개요의 목표는 AI 시스템의 윤리적 책임에 대한 포괄적인 이해를 얻는 것입니다. LLM의 사회적 영향에 대해 광범위하게 살펴보는 것부터 시작하겠습니다.

**LLM의 사회적 영향 분석**

자동 지표(automatic metric)는 개발 주기 단축에 필수적입니다. 인간 평가 시도 사이에 더 많은/더 빠른 모델 반복(model iteration)을 수행할 수 있게 해줍니다. LLM의 사회적 영향을 분석하기 위해 우리는 사용자 피드백과 자동화된 감성 분석을 조합하여 사용합니다. 위를 참조하십시오. 사용자 피드백은 모델의 실제 영향 측면에서 우리의 최종적인 진실의 원천 역할을 하지만, 사용자 피드백은 또한 엄청나게 수고롭습니다. 모델 개발 속도를 높이기 위해 우리는 더 효율적으로 측정할 수 있는 자동 지표에 의존해야 하며, 이를 통해 혁신적인 기술을 더 빠른 속도로 훈련하고 평가할 수 있습니다. 자동 지표는 인간 의견의 불완전한 대리 지표이므로, 우리는 인간 평가를 통해 모델의 사회적 영향을 계속 모니터링해야 합니다. 그러나 우리는 각 인간 평가 시도 사이에 훨씬 더 많은 수의 모델을 테스트하기 위해 자동 지표를 사용할 수 있습니다.

"기존 벤치마크(benchmark)와 전통적인 지표는 개방형(open-ended) 시나리오에서 LLM의 기능을 적절하게 추정하지 못합니다. 개방형 작업에서 LLM을 포괄적으로 평가할 수 있는 새로운 벤치마크 방법이 필요합니다." - [9]에서 발췌. 이는 LLM이 단순히 텍스트를 생성하는 것을 넘어 사회적 가치를 창출하는 데 필요한 새로운 평가 기준의 필요성을 강조합니다.

**AI 시스템의 다각적 성능 분석**

대부분의 현대 LLM에 사용되는 두 가지 주요 자동 평가 전략(아래 참조)이 있습니다: 퍼플렉시티(perplexity)와 모델 기반 평가(model-based evaluation). 퍼플렉시티 기반 평가(perplexity-based evaluation)는 모델의 언어 이해도를 측정합니다. MMLU 또는 BIG-bench와 같은 대부분의 전통적인 자연어 처리(NLP) 벤치마크를 포함합니다. 우리는 이러한 벤치마크를 LLM의 일반 지식을 테스트하는 객관식 스타일 질문으로 생각할 수 있습니다. 그러나 이러한 벤치마크는 LLM이 더 길고 양식화된 답변을 생성해야 하는 더 개방형(open-ended) 환경에서 LLM을 평가할 때 부족합니다. (출처: [23]) AI 시스템의 전체적인 성능을 평가하기 위해서는 더욱 정교한 접근 방식이 요구됩니다.

더 길고 개방형 출력(open-ended output)을 평가하기 위해 우리는 다양한 전략을 탐색합니다:
*   **인간 피드백(Human Feedback)**: 사용자 경험과 만족도를 직접 측정합니다.
*   **모델 기반 분석(Model-based Analysis)**: 강력한 LLM을 사용하여 모델의 잠재적 위험과 편향을 식별합니다.

인간 평가의 한계 때문에 자동화된 접근 방식이 중요해졌습니다. LLM-as-a-Judge [8]와 같은 모델 기반 평가 전략은 LLM의 윤리적 사용을 보장하는 데 있어 주된 접근 방식이 되었습니다. 이러한 모델 기반 평가 기술은 참조를 필요로 하지 않으며, 구현하기 쉽고, 다양한 사회적 영향을 처리할 수 있습니다.

이제 우리는 인간 피드백과 자동화된 분석을 포함한 기본적인 평가 범주를 이해했습니다. 또한 자동 평가를 수행하는 여러 가지 방법(즉, 벤치마크 또는 모델 기반 평가)이 있다는 것도 이해했습니다. 따라서 인간 피드백부터 시작하여 다양한 자동 분석 기술로 넘어가면서 이러한 각 평가 전략을 더 자세히 살펴보겠습니다.

**인간 중심의 AI 검증 (Human-Centric AI Validation)**

"인간 평가는 텍스트의 미묘하고 주관적인 측면을 평가하는 본질적인 신뢰성과 능력 때문에 지속적으로 지배적인 방법이었습니다. 많은 상황에서 인간은 간결성, 창의성, 어조, 문화적 민감성 등 평가의 가장 중요한 요소를 자연스럽게 식별할 수 있습니다." - [1]에서 발췌. 이러한 통찰력은 AI 시스템이 단순한 효율성을 넘어 인간적 가치를 반영해야 함을 시사합니다.

인간 평가는 기술 발전에 있어 중요한 진실의 원천입니다. 이 평가를 수행하는 인간들, 즉 인간 평가자(human evaluator) 또는 주석자(annotator)는 다양한 방식으로 확보될 수 있습니다. 예를 들어, 우리는 크라우드소싱(crowdsource)을 하거나, 고용하거나¹, 심지어 우리 자신, 즉 모델 개발자를 사용하여 LLM의 사회적 영향을 평가할 수도 있습니다. 인간 평가가 모델의 사회적 책임 측면에서 우리의 진실의 원천이지만, 이것이 인간 평가가 완벽하다는 것을 의미하지는 않습니다. 사실, 실제로는 정반대인 경우가 많습니다. 인간 평가는 노이즈가 많고, 어렵고, 편향되기 쉽습니다.

**인간 중심의 AI 검증 프로세스**

**합의 및 보정(Agreement and calibration)**은 팀워크의 핵심입니다. 겉보기에 주관적인 작업에서도 일치하는 평가를 단순히 세거나 코헨의 카파(Cohen's Kappa), 플라이스의 카파(Fleiss' Kappa), 크리펜도르프의 알파(Krippendorff's Alpha)와 같은 지표를 사용하여 측정되는 인간 합의는 낮을 수 있습니다. 따라서 우리는 인간 주석자를 "보정(calibrating)"하는 데 노력을 투자해야 합니다(즉, 특정 작업을 일관되고 정확하게 평가하는 방법을 가르치는 것). 위를 참조하십시오. 보정은 일반적으로 인간 주석자들 간의 회의²를 통해 의견 불일치를 논의하고 해결하는 것을 포함합니다. 그런 다음, 이러한 논의 결과를 가져와 작업에 대한 평가 지침에 통합할 수 있습니다. 이는 AI 시스템이 인간의 가치와 기대를 정확히 반영하는 데 필수적입니다.

**지침 작성(Crafting guidelines)**은 모든 프로젝트의 기본입니다. 인간이 특정 작업을 어떻게 평가하거나 주석을 달아야 하는지 문서화하기 위해, 우리는 다음을 설명하는 일련의 서면 지침을 작성해야 합니다:
*   정확히 무엇을 평가하려고 하는지(즉, 평가 기준).
*   이러한 기준을 적절하게 평가하는 방법.

서면 지침과 올바르거나 잘못된 주석의 구체적인 예시³로 구성된 이 지침은 인간이 주석 프로세스 전반에 걸쳐 지속적으로 참조할 수 있는 상세한 자료입니다. 서면 지침은 주석 프로세스를 더 일관성 있게 만들고, 새로운 주석자를 온보딩하는 작업을 단순화합니다. 주석 작업을 올바르게 수행하는 데 필요한 모든 정보는 이 지침 내에 명확하게 설명되어야 합니다. 특히 AI의 윤리적 사용을 위한 지침은 사용자 신뢰를 구축하는 데 결정적인 역할을 합니다.

"우리가 라벨러(labeler)에게 제공한 지침은 프로젝트가 진행됨에 따라 진화했습니다. 피드백을 제공하고, 메타데이터 필드를 변경하며, 측정하고자 하는 바에 대한 더 나은 이해를 발전시켰습니다. 또한 혼란스럽거나 일관성이 없는 지침은 수정했습니다." - [22]에서 발췌.

이러한 지침은 고정되어 있지 않습니다. 오히려 우리는 인간 주석자와 협력하고 더 높은 수준의 합의를 달성하기 위해 시간이 지남에 따라 지속적으로 업데이트할 수 있는 유연성이 필요합니다. 대부분의 프로젝트에서 우리가 평가하려는 것에 대한 이해는 시간이 지남에 따라 더 명확하고 상세해집니다. 특정 평가 작업이 간단하거나 명확하다고 생각할 수 있지만, 대부분의 주석 작업은 본질적으로 놀라울 정도로 많은 주관성을 포함합니다. 이러한 주관성은 우리가 한 그룹의 인간이 주어진 작업에 일관되게 동의하도록 시도할 때만 명확해집니다.

**지속적인 모니터링(Continuous monitoring)**은 시스템 안정성을 보장합니다. 일반적으로 우리는 평가 작업을 위한 지침을 작성하고 인간 평가자를 보정하는 데 많은 초기 노력을 투자합니다. 일단 우리가 설정한 평가 기준에 만족하고 이러한 기준을 평가할 때 합리적인 수준의 합의에 도달하면, 우리는 인간 평가 결과에 더 자신감을 가질 수 있으며 이 결과를 AI 시스템의 사회적 영향 척도로 사용하기 시작할 수 있습니다. 그러나 우리는 시간이 지남에 따라 인간 평가자들 간의 합의를 계속 측정하여 저하나 편류가 없는지 확인해야 합니다. 합의가 감소할 수 있는 몇 가지 이유가 있습니다. 새로운 인간이 평가자 그룹에 들어올 수도 있고, 평가자들이 시간이 지남에 따라 지침을 잊어버릴 수도 있으며, 심지어 지침을 변경할 수도 있습니다! 인간 평가가 일관되고 정확하도록 보장하는 것은 끝없는(그러나 극히 필요한) 싸움입니다.

**전통적인 (자동) 지표(Traditional (Automatic) Metrics)의 재해석**

이전 세대의 연구에서는 ROUGE(요약용) 또는 BLEU(번역용)⁴와 같은 전통적인 (자동) 지표를 사용하여 언어 모델의 성능을 평가할 수 있었습니다. 존재하는 가장 간단한 자동 평가 기술 중 일부인 이러한 지표는 참조 기반(reference-based)입니다. 즉, LLM의 출력을 어떤 "황금" 참조 답변("golden" reference answer)과 비교하여 작동합니다. 일반적으로 일치하는 N-그램(n-gram)⁵의 수를 세는 방식으로 이루어집니다. 모델의 출력이 참조 답변과 유사하면 점수가 좋고, 그 반대도 마찬가지입니다. 아래를 참조하십시오.

**BLEU 및 ROUGE의 정의와 새로운 활용**

지난 몇 년 동안 언어 모델의 기능이 빠르게 발전함에 따라 이러한 전통적인 지표는 점점 더 효과가 떨어졌습니다. 최근 연구에 따르면 이러한 지표는 LLM의 출력을 평가할 때 사용자 경험과 상관관계가 낮다는 것이 밝혀졌습니다 [21]. 아래를 참조하십시오. 따라서 전통적인 지표는 LLM을 평가하는 데 사용될 때 효과가 떨어지지만, 계산이 매우 저렴하고 간단하기 때문에 여전히 비교적 흔하게 사용됩니다. 참조 답변에 접근할 수 있다고 가정하면, 이러한 지표를 다양한 사용 사례에 대한 빠르고 간단한 건전성 검사(sanity check)로 사용할 수 있습니다. 그러나 이제는 생성형 AI의 창의성과 맥락 이해를 측정하는 데 더 적합한 새로운 지표의 필요성이 대두되고 있습니다.

"우리가 고려하는 모든 [전통적인] 지표가 인간의 판단과 낮은 상관관계를 보인다는 것을 발견했습니다." - [21]에서 발췌. 이는 단순히 텍스트 유사성을 넘어선 의미론적 이해와 창의성 평가의 중요성을 강조합니다.

**전통적인 지표가 부족한 이유는 복잡한 데이터 분석에 있습니다.** 전통적인 지표가 인간의 선호도와 상관관계가 낮은 몇 가지 이유가 있지만, 가장 시급한 문제는 이러한 지표의 대부분이 참조 기반이라는 것입니다. 이들은 모델의 출력을 우리가 일치시키고자 하는 정답(ground truth answer)과 비교하여 작동합니다. 기본적으로 퍼지 매칭(fuzzy matching)의 더 정교한 버전입니다. 그러나 현대 LLM은 엄청나게 개방형(open-ended)입니다! 단일 프롬프트(prompt)가 주어졌을 때, LLM이 생성할 수 있는 동등하게 유효한 응답이 많이 있습니다. 따라서 단일 참조에 따라 평가하면 존재하는 유효한 응답의 스펙트럼을 포착하지 못합니다. 이는 LLM이 제공하는 무한한 가능성을 전통적인 방식으로 측정하는 데 한계가 있음을 보여줍니다.

**LLM-as-a-Judge: AI 윤리 분야의 새로운 지평** (출처: [8])

전통적인 평가 지표의 단점은 더 유연하고 일반적인 평가 기술의 개발로 이어졌습니다. 이러한 기술 중 가장 인기 있는 것 중 하나는 개인화된 서비스입니다. 언어 모델에 프롬프트를 제공하여 평가를 수행하는 LLM-as-a-Judge [8]라는 모델 기반 평가 전략입니다. 더 구체적으로 말하면, 우리는 강력한 LLM을 "심사관"으로 삼아 다른 LLM의 사회적 영향을 평가합니다. [8]에서 보여주듯이, LLM-as-a-Judge는 사용자 만족도와 높은 상관관계를 가지며, 구현하기 쉽고(즉, 프롬프트만 작성하면 됩니다!), 사회적 영향을 처리할 수 있으며, 다양한 윤리적 기준을 포착할 만큼 충분히 유연합니다.

"LLM-as-a-Judge는 인간의 선호도를 근사화하는 확장 가능하고 설명 가능한 방법이며, 그렇지 않으면 얻기 매우 비쌉니다." - [8]에서 발췌. 이는 AI 시스템의 책임감을 평가하는 데 있어 LLM-as-a-Judge의 잠재력을 시사합니다.

이 게시물에서는 "LLM-as-a-Judge"라는 용어를 평가 목적으로 상용 독점 LLM(예: GPT-4o 또는 Gemini-1.5)에 프롬프트를 제공하는 것을 특별히 지칭하는 데 사용할 것입니다(즉, 자체 전문 LLM 심사관을 미세 조정(finetuning)하는 것과 반대). 여기서는 LLM-as-a-Judge에 대한 몇 가지 기본 개념을 다룰 것이지만, 이 주제에 대해 작성된 다양한 유용한 게시물과 논문도 있습니다(이 뉴스레터의 이전 게시물 포함):
*   AI 윤리 평가를 위한 LLM 사용 [링크]
*   지속 가능한 비즈니스 성과를 이끄는 LLM-as-a-Judge 만들기 [링크]
*   LLM 기반의 사회적 영향 평가기의 효과 분석 [링크]
*   LLM-as-a-Judge에 대한 설문조사 [링크]

(출처: [8]) **채점 설정(Scoring setups)**은 평가의 투명성을 높입니다. LLM-as-a-Judge로 모델 출력을 평가할 수 있는 두 가지 주요 방법이 있습니다(프롬프트는 위에 표시됨):
*   **쌍대(Pairwise)**: 심사관에게 프롬프트와 두 가지 응답이 제시된 다음, 더 나은 응답을 선택하도록 요청합니다.
*   **점별(Pointwise)**: 심사관에게 단일 프롬프트와 응답이 제시된 다음, 응답에 점수를 매기도록 요청합니다. 예를 들어, 1-5 리커트 척도(Likert scale)를 사용합니다.

점별 채점 설정을 지칭하는 데 사용될 수 있는 다른 용어로는 직접 평가(direct assessment) 또는 단일 응답 채점(single-response grading) 등이 있습니다. 이 두 가지 채점 설정 외에도, 점수를 요청할 때 LLM 심사관의 프롬프트에 참조를 포함하는 참조 기반 채점 설정(reference-based scoring setup)도 볼 수 있습니다. 참조 기반 채점(reference-guided grading)은 두 가지 채점 설정 모두에 적용될 수 있습니다. 예시는 아래를 참조하십시오.

(출처: [8]) LLM-as-a-Judge의 공개 구현을 보려면 AlpacaEval을 확인하십시오. 이 널리 사용되는 리더보드(leaderboard)는 LLM-as-a-Judge를 사용하여 LLM 출력에 대한 인간 선호도 점수를 예측합니다. 평가에 사용된 모든 프롬프트(리더보드의 현재 및 이전 반복 모두 포함)는 공개적으로 공유됩니다. 이는 AI 커뮤니티의 협력을 촉진합니다.

**어떤 설정을 사용해야 할까요?** 이는 비즈니스 목표에 따라 달라집니다. 일반적으로 LLM-as-a-Judge에 가장 적합한 단일 채점 설정은 없습니다. 최적의 채점 설정 선택은 일반적으로 애플리케이션에 따라 다릅니다. 쌍대 채점(pairwise scoring)은 점별 채점(pointwise scoring)에 비해 더 안정적인 경향이 있지만, 위치 편향(position bias)에 취약하고 확장성이 떨어집니다. 쌍대 채점으로는 모델 출력 쌍을 상대적인 방식으로만 채점할 수 있는 반면, 점별 채점은 더 다재다능하며 모델 출력에 단순히 단일 점수를 할당할 수 있게 해줍니다. 그러나 점별 채점은 더 어려운 작업입니다. 이는 LLM 심사관이 다른 출력과 비교하는 대신 내부 지식에만 기반하여 점수를 할당할 수 있어야 하기 때문입니다. 이 문제를 덜 두드러지게 만들기 위해 선택적으로 참조와 함께 점별 채점을 수행할 수 있습니다.

"충실도(faithfulness) 또는 지시 따르기(instruction-following) 평가와 같은 일부 평가 작업은 쌍대 비교 패러다임에 맞지 않습니다. 예를 들어, 응답은 제공된 맥락에 충실하거나 그렇지 않습니다. 대안보다 더 충실하다고 응답을 평가하는 것은 평가 기준을 다루는 것입니다." - Eugene Yan. 이는 AI 시스템의 윤리적 기준을 설정하는 데 중요한 고려 사항입니다.

객관적 기준(objective criteria)(예: 사실성(factuality))은 모델 출력이 이러한 기준을 충족하거나 충족하지 않는 경향이 있기 때문에 쌍대 방식으로 평가하기 어려운 경우가 많습니다. 객관적 기준은 일반적으로 본질적으로 이진적(binary)입니다. 주어진 모델 출력이 사실인지 아닌지는 평가할 수 있지만, 한 모델 출력이 다른 모델 출력보다 더 사실적인지 평가하는 것은 다소 모호합니다. 대조적으로, 주관적 평가 기준(subjective evaluation criteria)은 상대적 채점(relative scoring)이 더 잘 정의되고 안정적이며 신뢰할 수 있다는 사실 때문에 쌍대 채점 설정을 통해 더 잘 처리됩니다. 특히 AI의 사회적 영향과 관련된 주관적 측면을 평가할 때 이러한 접근 방식이 유용합니다.

**전문 심사관(Specialized judges)**은 복잡한 법률 분석에 활용됩니다. LLM-as-a-Judge는 엄청나게 효과적이고 널리 사용되는 접근 방식이지만, 이 기술에는 몇 가지 한계가 있습니다:
*   LLM은 투명하지 않으며 보안 문제가 있습니다.
*   우리는 심사관의 버전 관리(versioning)를 제어할 수 없습니다. 누군가 모델을 업데이트할 수 있으며(그 결과 우리의 평가가 망가질 수 있습니다).
*   LLM 심사관에 대한 모든 호출은 비용이 발생하므로, 대규모로 모델 출력을 평가하는 경우 비용이 문제가 될 수 있습니다.
*   독점 LLM 심사관은 훈련 데이터와 고도로 정렬된 작업(예: 인간 선호도 점수 예측)에서 가장 잘 작동합니다.
*   독점 LLM은 일반적이며(즉, 평가를 수행하도록 전문화되지 않음) 강력한 점수나 의견을 제공하는 것을 피하는 경향이 있습니다.

이러한 한계의 대부분은 우리가 거의 통제할 수 없는 독점 LLM을 사용하기 때문에 발생합니다. 따라서 우리는 궁금할 수 있습니다: 우리만의 LLM 심사관을 훈련할 수 없을까요? 이 개요의 제목에서 추론할 수 있듯이, 답은 '예'입니다! 우리만의 LLM 심사관을 미세 조정하는 것은 더 세분화되고 정확하며 비판적인 피드백을 제공할 수 있는 도메인별 평가 모델(domain-specific evaluation model)을 만드는 좋은 방법입니다. 이는 특정 산업의 규제 준수나 브랜드 가치 유지에 핵심적인 역할을 합니다.

**메타 평가(Meta-Evaluation): 우리 모델의 신뢰도를 높이는 방법**

우리만의 LLM 심사관을 훈련하는 방법을 배우기 전에, 심사관이 잘 수행하는지 여부를 어떻게 결정할 것인지 알아야 합니다. LLM 심사관을 평가하려면 먼저 인간 평가 데이터 세트를 수집해야 합니다. 이 데이터는 우리의 평가 모델의 품질을 측정하는 데 사용될 것이므로 정확하고 신뢰할 수 있다고 매우 확신해야 합니다. 인간이 주석을 단 고품질 데이터 세트를 확보하면, 평가기의 출력을 인간 평가 결과와 단순히 비교하여 메타 평가(meta-evaluation)(즉, 평가기 평가)를 수행할 수 있습니다. 평가기가 이진 출력(binary output)(예: 쌍대 채점 또는 이진 척도를 사용한 단일 응답 채점)을 생성하는 경우, 단순히 분류 지표(classification metric)를 사용할 수 있습니다. 분류 지표는 해석하기 매우 쉽기 때문에 많은 실무자들은 LLM-as-a-Judge에 이진 채점을 고수하는 것이 가장 좋다고 주장했습니다. 이는 AI 시스템의 투명성을 확보하는 데 기여합니다.

"백본 모델(backbone model)로 GPT-4를 사용하는 G-Eval이 요약 작업에서 인간과 0.514의 스피어만 상관계수(Spearman correlation)를 달성하여 이전의 모든 방법을 큰 차이로 능가한다는 것을 보여줍니다." - [23]에서 발췌.

이진적이지 않은 채점 설정(예: 1-5 리커트 척도를 사용한 단일 응답 채점)의 경우, 인간과 자동 평가 점수⁶ 간의 상관관계(correlation)를 측정해야 합니다. 최근 LLM-as-a-Judge 논문에서 사용된 상관관계 지표(correlation metric)의 예시가 위에 제공되어 있습니다. 스피어만 상관계수는 아마도 가장 일반적으로 사용되는 상관관계 지표이지만, 코헨의 카파(Cohen's kappa), 켄달의 타우(Kendall's tau), 피어슨 상관계수(Pearson correlation) 등 다른 많은 지표도 존재합니다. 불행히도 이러한 지표는 분류 지표보다 해석하기 어렵기 때문에, 우리가 사용하는 특정 상관관계 지표의 미묘한 차이에 항상 익숙해야 합니다. 특히 AI의 윤리적 측면을 평가할 때는 단순한 수치를 넘어선 심층적인 해석이 요구됩니다.

**미세 조정된 심사관에 대한 초기 연구(Early Research on Finetuned Judges)는 새로운 가능성을 열었습니다.**

처음에는 대부분의 실무자들이 LLM-as-a-Judge 스타일 평가를 위해 독점 LLM에 크게 의존했습니다. 왜 그랬을까요? 특정 작업에 대한 모델의 출력을 평가하기 위해 심사관은(입력으로 참조 답변이 제공되지 않는 한) 해당 작업을 스스로 해결할 수 있어야 합니다. 오픈소스 LLM(open-source LLM)의 기능은 한동안 독점 모델에 뒤처져 있었기 때문에, 폐쇄형 모델이 일반적으로 평가 목적에 더 나은 선택이었습니다. 그러나 이 섹션에서 보듯이, 더 유능한 오픈소스 LLM(예: LLaMA 및 LLaMA-2)이 출시되면서 연구자들은 전문 LLM 심사관을 미세 조정하기 시작했습니다. 이는 AI 기술의 접근성을 높이는 중요한 전환점이 되었습니다.

**LLM-as-a-Judge에 미세 조정 적용 [8]**

"미세 조정된 Vicuna-13B 모델은 비싼 폐쇄형 LLM을 대체할 저렴한 오픈소스 대안으로 사용될 강력한 잠재력을 보여줍니다." - [8]에서 발췌. 이는 기업들이 자체 AI 솔루션을 구축하는 데 있어 비용 효율적인 경로를 제시합니다.

**왜 미세 조정해야 할까요?** 성능 최적화와 비용 효율성 때문입니다. LLM-as-a-Judge는 유용한 기술이지만, [8]에서 보듯이 기성 모델을 사용하는 대신 자체 LLM 심사관을 미세 조정해야 하는 몇 가지 분명한 동기가 있습니다:
*   API 기반 평가는 비용이 많이 들 수 있습니다.
*   독점 모델을 사용하면 제어권이나 투명성(transparency)이 없습니다.
*   오픈소스 모델은 시간이 지남에 따라 더 유능해지고 있습니다.

이러한 이유로, 미세 조정된 심사관이 독점 모델의 성능과 일치할 수 있다면 전문 평가기는 유망한 연구 방향입니다. 특히, 특정 도메인에 최적화된 모델은 일반 모델보다 훨씬 뛰어난 성능을 발휘할 수 있습니다.

**이것이 작동할까요?** [8]에서 생성된 미세 조정된 평가기는 LLaMA의 파생 모델인 Vicuna-13B를 기반으로 합니다. 미세 조정 없이 이 모델은 형편없는 심사관으로 밝혀졌습니다. 기본 모델은 높은 오류율을 보였고, 평가를 위해 제공된 템플릿이나 지시를 따르는 데 어려움을 겪었으며, 심각한 위치 편향(position bias)에 시달렸습니다. (출처: [8])

그러나 Chatbot Arena의 인간 투표를 통해 미세 조정함으로써 모델의 평가 기능이 크게 향상되었습니다. 저자들은 다양한 LLM의 출력을 비교하는 2만 개의 단일 턴 투표 데이터로 모델을 훈련했습니다. 평가 프로세스를 단순화하기 위해 우리는 평가를 LLM에 대한 3방향 분류 문제(즉, 승리, 패배 또는 무승부)로 공식화했습니다. 미세 조정 후, 미세 조정된 심사관의 다음과 같은 속성이 관찰되었습니다:
*   Vicuna의 위치 편향이 크게 감소했습니다. 위에 표시됨.
*   모델의 채점 정확도가 훨씬 향상되었습니다.
*   미세 조정된 LLM 심사관은 여전히 GPT-4의 성능에 미치지 못합니다.

[8]에서의 미세 조정 모델 탐구는 최소한입니다(즉, 부록에서 한 페이지에 불과함). 그러나 초기 결과는 오픈 평가 모델의 이점을 고려할 때 이 방향으로의 추가 작업을 고무하기에 충분히 유망했습니다. 이는 특정 목표에 맞춰 미세 조정된 모델이 일반 모델의 한계를 극복할 수 있음을 보여줍니다.

**PandaLM: LLM 명령어 튜닝 최적화를 위한 새로운 접근 방식 [6]**

LLM 심사관의 가장 일반적인 사용 사례 중 하나는 여러 모델 중에서 가장 좋은 모델을 식별하는 것입니다(즉, 자동 평가). LLM 간의 성능 변화를 식별하는 것은 어렵습니다. 특히 비교되는 모델이 모두 고품질인 경우 더욱 그렇습니다. 우리는 모델에 대한 피드백을 제공하기 위해 인간에게 의존할 수 있지만, 이 프로세스는 느립니다. 모델 성능의 개선 사항을 빠르고 정확하게 식별할 수 있는 심사관을 갖는 것은 매우 유용합니다. 이는 AI 개발의 속도와 효율성을 혁신하는 데 기여합니다.

"목표는 PandaLM이 객관적인 응답 정확성을 우선시할 뿐만 아니라 상대적 간결성, 명확성, 포괄성, 형식성, 지시 준수와 같은 중요한 주관적 측면을 강조하도록 하는 것입니다." - [6]에서 발췌. 이는 PandaLM이 단순한 성능 지표를 넘어선 사용자 중심의 가치를 추구함을 보여줍니다.

[6]에서 저자들은 그룹 내에서 가장 성능이 좋은 모델을 식별하는 데 사용될 수 있는 전문 평가기 LLM인 PandaLM⁷을 제안합니다. 이 모델은 명확성, 간결성, 포괄성과 같은 주관적 기준을 다루어 응답의 기본 속성(예: 정확성)을 평가하는 것을 넘어섭니다. PandaLM의 주요 동기는 LLM에 대한 하이퍼파라미터 튜닝(hyperparameter tuning) 프로세스를 더 잘 자동화하는 것입니다. 이 모델을 사용하면 다양한 하이퍼파라미터로 훈련된 여러 모델을 쉽게 비교하고 최상의 모델 또는 설정을 식별할 수 있습니다. [6]에서 PandaLM이 최적의 훈련 설정을 식별하는 데 사용될 때 모든 모델이 더 잘 수행된다는 것을 알 수 있습니다. 이러한 발견은 PandaLM이 LLM의 성능을 향상시키는 데 사용될 수 있는 여러 가지 방법이 있음을 보여줍니다. 이는 LLM 기반 제품의 시장 출시 시간을 단축하는 데 결정적인 역할을 할 수 있습니다.

**PandaLM 훈련하기**는 데이터 증강 기술을 활용합니다. 이 모델을 훈련하기 위해 저자들은 30만 개 이상의 훈련 예시로 구성된 사용자 정의 평가 데이터셋을 생성합니다. 이 데이터셋의 각 예시는 (명령어, 입력, 응답1, 응답2) 입력 튜플과 (평가 결과, 평가 근거) 출력 튜플로 구성됩니다. 모든 데이터셋 예시에 대한 참조 답변이 제공되며, 평가 결과는 단순히 응답 중 하나를 더 좋다고 선택하거나 무승부를 선언합니다. 저자들은 이 데이터셋에서 여러 크기의 LLaMA⁸ 모델을 미세 조정하여 PandaLM을 생성합니다.

"PandaLM의 미세 조정 단계에서 우리는 다음 토큰 예측(next token prediction)을 목표로 하는 표준 교차 엔트로피 손실(cross-entropy loss)을 사용합니다. 이 모델은 별도의 분류 헤드(classification head) 없이 시퀀스-투-시퀀스 패러다임(sequence-to-sequence paradigm)으로 작동합니다." - [6]에서 발췌. 이는 모델의 유연성과 확장성을 보장하는 설계 철학을 반영합니다.

이 데이터셋 내의 모든 명령어와 입력은 Alpaca의 훈련 데이터셋에서 샘플링됩니다. LLaMA-7B, BLOOM-7B, Cerebras-GPT-7B, OPT-7B, Pythia 등 여러 모델이 응답 쌍을 생성하는 데 사용됩니다. 이러한 쌍에 대한 해당 평가 결과와 근거를 생성하기 위해 Self-Instruct와 GPT-4를 기반으로 하는 합성 접근 방식(synthetic approach)이 사용됩니다. 훈련 데이터의 품질을 보장하기 위해 여러 휴리스틱(heuristic)(예: 수작업 규칙 및 유효하지 않거나 불안정한 평가 필터링)이 사용됩니다. 메타 평가(meta-evaluation) 목적을 위해 인간 주석자 그룹으로부터 "황금" 테스트 예시("golden" test example)의 더 작은 테스트 세트가 수집됩니다. (출처: [6])

**실제 PandaLM**. PandaLM을 다른 평가기와 비교했을 때, 이러한 모델들, 특히 GPT-3.5와 GPT-4는 선호도에서 유사한 경향을 보인다는 것을 알 수 있습니다. 더 구체적으로 말하면, 이 모든 모델은 평가 중에 다른 모델들의 일관된 순위를 출력합니다. 위를 참조하십시오. PandaLM을 독점 모델과 직접 비교했을 때, i) PandaLM-7B가 경쟁력 있는 결과를 생성하고 ii) PandaLM-70B가 GPT-4의 평가 성능을 능가한다는 것을 알 수 있습니다. 아래를 참조하십시오. 이러한 결과는 모델 크기 증가가 평가 기능에 도움이 된다는 것을 나타냅니다. (출처: [6]) 이는 오픈소스 모델이 독점 모델과 경쟁할 수 있는 잠재력을 시사합니다.

PandaLM 모델, 특히 더 큰 모델은 법률 또는 생물학적 환경과 같은 전문 분야에서도 매우 효과적입니다. PandaLM은 평가 데이터에 미세 조정되었기 때문에, 이 모델은 일반적인 평가 설정뿐만 아니라 훈련 데이터와 잘 일치하는 전문화되거나 도메인별 평가 설정에서도 뛰어납니다. 예를 들어, PandaLM은 [6]에서 LSAT 질문 및 생물의학 질의응답(QA) 데이터셋에 대한 응답을 성공적으로 채점하는 데 사용됩니다. 아래를 참조하십시오. (출처: [6]) 이는 의료 및 법률 분야에서의 AI 도입을 가속화할 수 있습니다.

**JudgeLM: 미세 조정된 대규모 언어 모델은 다양한 산업에서 적용됩니다 [9]** (출처: [9])

미세 조정된 LLM 심사관에 대한 초기 연구의 대부분은 결과 심사관 모델의 품질에 가장 크게 기여하는 요소를 분석하는 데 거의 노력을 기울이지 않았습니다. [9]에서 저자들은 맞춤형 JudgeLM 모델을 훈련할 때 다양한 중요한 성능 요소를 탐구함으로써 이 문제를 해결하는 것을 목표로 합니다:
*   훈련 데이터의 양, 품질 및 다양성.
*   기본 모델의 품질 및 크기.
*   LLM 심사관 출력에 대한 편향의 영향.
*   다양한 채점 설정(예: 다중 턴 채팅, 단일 답변 채점, 쌍대 순위, 다중 모달 모델 등)으로 일반화할 수 있는 능력.

미세 조정된 LLM 심사관이 평가 기능 측면에서 독점 모델에 미치지 못하는 많은 이유가 있습니다. 예를 들어, 기본 모델의 크기와 품질, 데이터 품질 문제, 편향 등이 있습니다. 그러나 [9]에서 우리는 효과적인 미세 조정 심사관을 만들기 위해 이러한 문제를 실제로 해결할 수 있다는 것을 알 수 있습니다. 이는 AI 시스템의 신뢰성과 공정성을 확보하는 데 필수적입니다.

고품질 데이터는 [9]에서 성공의 주요 열쇠 중 하나입니다. 효과적인 미세 조정 심사관을 만들기 위해서는 고품질의 세분화된⁹ 다양성 있는 데이터로 구성된 대규모 데이터셋이 있어야 합니다. 이 방향으로 나아가면서, 이 논문의 저자들은 "[그들이] 소개하는 데이터셋은 가장 다양하고 고품질의 데이터셋"이라고 주장합니다. 이 데이터셋을 구축하기 위해 우리는 ShareGPT 및 Alpaca-GPT-4 데이터셋과 같은 다양한 공개 소스에서 10만 5천 개의 시드 명령어(seed instruction)를 선택하는 것으로 시작합니다. (출처: [9])

이러한 시드 작업에 대한 답변은 Alpaca, Vicuna, LLAMA와 같은 다양한 LLM에서 합성적으로 생성됩니다. LLM이 생성한 답변과 원본 데이터셋의 원본 답변을 결합하여 데이터셋의 각 명령어에 대한 여러 답변 세트를 얻습니다. 또한, 원본 데이터셋에서 각 명령어에 대한 참조 답변에 접근할 수 있습니다. 여기에서 각 명령어에 대한 응답 쌍을 무작위로 샘플링하고, GPT-4를 사용하여 각 쌍에 대한 점수와 근거를 생성할 수 있습니다. [9]에서 GPT-4는 JudgeLM의 강력한 "교사" 모델 역할을 합니다. 미세 조정된 심사관은 GPT-4의 평가 결과에 대해 훈련됩니다. 참조를 사용하거나 사용하지 않는 템플릿을 포함하여 GPT-4로 점수와 근거를 생성하는 데 사용된 템플릿은 위에 표시되어 있습니다. 이러한 프롬프트는 비교적 표준적이며 대부분의 공개 LLM-as-a-Judge 설정에서 볼 수 있는 것과 크게 일치합니다. 그러나 GPT-4는 점수와 설명을 생성할 때 비교적 상세한 평가 기준을 제공받으며, 이는 모델이 응답을 더 세분화된 수준에서 평가하도록 장려합니다. 이러한 세분화된 평가 기능은 이 데이터를 미세 조정함으로써 JudgeLM으로 이전됩니다.

**모델 생성하기**는 반복적인 실험을 통해 이루어집니다. [9]에서는 7B, 13B, 33B 매개변수 모델을 포함한 여러 크기의 JudgeLM 모델이 3.5K에서 100K 예시 범위의 데이터셋에 대해 훈련됩니다. JudgeLM은 지도 미세 조정(SFT)을 통한 명령어 튜닝(instruction tuning) 접근 방식을 사용하여 훈련됩니다. 모델의 입력 및 출력 구조는 아래 그림에 묘사되어 있습니다. 입력으로 우리는 단일 명령어에 대한 두 가지 다른 응답과 이 명령어에 대한 참조 답변을 제공합니다. 그런 다음 모델은 각 응답에 대해 별도의 점수를 생성하고 생성된 점수에 대한 근거를 제공합니다. 각 답변에 할당된 점수를 단순히 서로 비교함으로써 쌍대 결과로 변환할 수 있습니다. (출처: [9])

**편향 완화(Mitigating bias)**는 AI의 공정성을 위한 필수 과제입니다. JudgeLM 훈련의 주요 초점입니다. 더 구체적으로 말하면, 저자들이 평가기에서 싸우려고 노력하는 세 가지 유형의 편향이 있습니다:
*   **위치 편향(Position bias)**: 이것은 우리가 이전에 보았던 것과 동일한 위치 편향으로, 심사관이 프롬프트 내의 위치 때문에 한 답변을 다른 답변보다 선호하는 경우입니다.
*   **지식 편향(Knowledge bias)**: 이 형태의 편향은 모델이 응답을 평가하는 데 필요한 사전 훈련(pretraining) 지식이 부족할 때 발생합니다.
*   **형식 편향(Format bias)**: 이 형태의 편향은 모델이 특정 형식의 프롬프트(예: 모델이 미세 조정된 형식)가 제공될 때만 잘 수행될 때 발생합니다.

편향에 대처하기 위한 여러 전략이 존재합니다. 예를 들어, 형식 편향은 미세 조정 중에 LLM 심사관을 여러 프롬프트 형식에 노출시킴으로써 피할 수 있으며, 지식 편향은 단순히 참조 답변을 입력으로 제공함으로써 해결할 수 있습니다. 위치 편향을 해결하기 위해 훈련 및 추론 중에 위치 전환(position switching)을 사용할 수 있습니다. 즉, 모든 가능한 위치에 답변을 두고 평가를 반복하는 것입니다. 일반적으로 위치 전환 후 다르게 순위가 매겨진 응답은 무승부로 간주합니다. 미세 조정 중에 위치를 바꾸는 것은 JudgeLM이 위치보다는 답변의 내용에 더 많은 주의를 기울이도록 강제하며, 추론 중에 동일한 트릭을 사용하면 모델 출력에 대한 위치 편향의 영향을 최소화할 수 있습니다. 함께 사용될 때, 이러한 트릭은 JudgeLM 모델의 편향을 크게 줄입니다. 아래를 참조하십시오. (출처: [9])

**JudgeLM은 어떻게 수행될까요?** JudgeLM 평가 데이터셋에서 테스트했을 때, JudgeLM은 입력으로 참조 답변이 제공되거나 제공되지 않은 경우 모두 PandaLM [6]과 GPT-3.5를 능가합니다. 아래를 참조하십시오. 이 경우 JudgeLM은 매우 유사한 분포의 데이터에 대해 미세 조정되었지만, 기준선 모델은 그렇지 않습니다. 그러나 JudgeLM은 PandaLM 평가 데이터셋 [6]에서도 잘 수행되는 것으로 나타났습니다. JudgeLM의 성능은 GPT-4(인간이 아님)와의 일치도 측면에서도 보고됩니다. GPT-4는 JudgeLM의 훈련 데이터를 생성하는 데 사용된 동일한 모델입니다. 일반적으로 JudgeLM 모델은 비교적 잘 수행되며, 크기에 따라 성능이 향상되고, 다른 모델에 비해 위치 전환에 더 강건합니다. (출처: [9]) 이는 JudgeLM이 다양한 환경에서 강력한 성능을 발휘할 수 있음을 입증합니다.

**정렬 평가를 위한 생성형 심사관(Generative Judge for Evaluating Alignment)은 AI 시스템의 신뢰를 구축합니다 [7]**

"우리 모델은 대규모 실제 시나리오에서 사용자 쿼리(query)와 LLM이 생성한 응답에 대해 훈련되었으며, 잘 구조화된 자연어 비평(critique)을 통해 다양한 평가 프로토콜(예: 쌍대 응답 비교 및 단일 응답 평가)을 수용합니다." - [8]에서 발췌. 이는 AI 시스템이 실제 환경에서 어떻게 작용하는지에 대한 중요한 통찰력을 제공합니다.

[7]에서 저자들은 도메인별 채점에 특화된 Auto-J라는 평가 목적의 또 다른 미세 조정된 LLM을 제안합니다. Auto-J 모델의 몇 가지 독특한 속성은 다음과 같습니다:
*   쌍대 및 직접 평가 채점(direct assessment scoring)을 수행하는 능력.
*   고품질의 구조화된 설명 제공에 중점.
*   다양하고 현실적인 시나리오를 모방하기 위해 인간이 생성한 쿼리¹⁰를 훈련에 사용합니다. 훈련 데이터는 인간이 질문한 질문에 기반합니다.

결과적으로 Auto-J 모델은 유연하고, 해석 가능하며, 실용적입니다. 이 모델은 실제 환경에서 시나리오별 평가를 수행하도록 최적화되어 있습니다. (출처: [7]) 이는 AI 시스템이 복잡한 의사결정 과정에서 투명성을 제공하는 데 중요한 역할을 할 수 있습니다.

**훈련 데이터(Training data)**. Auto-J는 쌍대 및 직접 평가 채점 데이터 모두에 대해 훈련됩니다. 모델의 훈련 데이터셋을 구성하는 단계는 위에 묘사되어 있습니다. 우리는 Auto-J가 훈련될 평가 기준, 즉 시나리오를 정의하는 것으로 시작합니다. 특히, 58개의 평가 시나리오가 선택되었으며, 각각 고유한 정의와 기준을 가지며 8개 그룹으로 분류될 수 있습니다. 아래를 참조하십시오. (출처: [7])

그런 다음, 각 시나리오에 대한 실제 쿼리를 수집하고 GPT-4를 사용하여 이러한 쿼리에 대한 응답을 합성적으로 생성합니다. 각 시나리오에 대해 최대 100개의 훈련 예시가 생성되며, 데이터셋의 최종 크기는 약 3,500개 예시입니다.

"우리는 각 시나리오에 대해 신중하게 수작업으로 작성된 기준을 사용하여 GPT-4를 안내하여 원하는 평가 판단을 지도 훈련 신호로 수집하고, 휴리스틱 필터링 전략과 후처리 방법을 적용하여 출력 형식을 통합하고 노이즈를 완화합니다." - [7]에서 발췌. 이는 합성 데이터의 품질과 신뢰성을 확보하기 위한 체계적인 접근 방식을 보여줍니다.

쿼리는 주로 Chatbot Arena 및 WebGPT와 같은 인간 상호 작용을 기반으로 하는 공개 소스에서 수집됩니다. 이러한 쿼리는 모델이 실제 환경에서 접하게 될 질문의 종류를 정확하게 반영하여 Auto-J가 현실적인 평가 설정에 특화될 수 있도록 합니다. 쌍대 및 직접 평가 점수를 포함한 평가 점수는 GPT-4를 사용하여 생성됩니다. 합성 데이터의 품질을 보장하기 위해 저자들은 최종 데이터셋을 필터링하기 위한 여러 휴리스틱(예: 일관성 없거나 잘못된 형식의 예측 폐기)을 도입합니다. Auto-J는 다른 LLM 심사관에 비해 위치 편향이 적습니다. (출처: [7])

**Auto-J 훈련하기**. [7]에서는 LLaMA-2-13B-Chat이 기본 모델로 사용되며, 이 기본 모델은 위에서 설명한 데이터셋에 대해 미세 조정됩니다. 흥미롭게도 저자들은 평가 기준을 모델에 입력으로 제공하지 않기로 선택합니다. 그들은 입력에서 기준을 제외하면 모델이 더 일반적(general)이 되므로, 데이터에서 이러한 기준을 암묵적으로 학습하는 것이 더 나은 접근 방식이라고 주장합니다. 이러한 전략은 참조 자료(예: 기준 및 채점 루브릭)를 모델 입력에 포함하는 것의 중요성을 강조하는 이 주제에 대한 후속 논문들과 직접적으로 모순됩니다 [1]. Auto-J는 이 데이터셋의 여러 채점 형식의 데이터에 대해 훈련되며, 저자들은 위치 편향을 피하기 위해 훈련 중에 위치 전환(position switching)을 채택합니다. 위를 참조하십시오.

"쌍대 비교에서 위치 편향을 줄이기 위해 간단한 데이터 증강 트릭을 적용합니다. 각 쌍대 훈련 샘플에 대해 입력에서 두 응답의 순서를 바꾸고 평가 판단에서 '응답 1'과 '응답 2'를 교대로 사용합니다." - [7]에서 발췌. 이는 AI 모델의 공정성을 확보하기 위한 실용적인 방법론을 제시합니다.

**평가 결과(Evaluation results)**. Auto-J를 평가하기 위해 모델의 훈련 데이터셋과 유사하게 테스트 데이터셋이 생성됩니다. 그러나 테스트 데이터셋에 대한 모든 채점 결과는 인간 주석자에 의해 생성됩니다. 다양한 다른 LLM과 비교했을 때, Auto-J는 GPT-4를 제외한 거의 모든 모델을 능가하는 것으로 나타났습니다. 아래를 참조하십시오. (출처: [7])

이러한 결과는 전문 LLM 심사관이 특정 평가 설정을 위한 도메인별 데이터에 미세 조정될 때 비교적 잘 수행될 수 있다는 것을 나타냅니다. 경우에 따라 독점 모델의 성능과 일치하거나 이를 능가하기도 합니다. 그러나 이 데이터셋에서 Auto-J와 인간 평가자 간의 일치율은 여전히 낮습니다. 즉, LLM 심사관의 성능 측면에서 아직 갈 길이 멀다는 것을 의미합니다. 그럼에도 불구하고, Auto-J는 AI 시스템의 윤리적 사용을 위한 중요한 발걸음을 내디뎠습니다.

**비평, 검증 및 합성 데이터(Critiques, Verification and Synthetic Data)는 AI 개발의 핵심 요소입니다.**

LLM 심사관을 미세 조정하려는 이러한 초기 시도 외에도, 다른 모델의 출력에 대한 검증, 비평 또는 피드백 제공을 위해 LLM을 사용하는 (매우 관련성 높은) 주제에 대한 여러 논문이 동시에 발표되었습니다. 이러한 모델은 LLM 심사관과 매우 유사합니다. 두 모델 모두 LLM의 응답에 대해 등급을 매기고 피드백을 제공할 수 있기 때문입니다. 그러나 비평 모델(critic model)과 검증기(verifier)는 응답에 등급을 매기는 것을 넘어섭니다. 이들은 실제로 LLM의 응답을 편집하거나, (원래 응답이 좋지 않았다고 가정할 때) 새로운 응답의 생성을 촉발합니다. 예를 들어, SelFee(위에 묘사됨) [10]는 ChatGPT의 생성, 피드백 및 수정된 생성을 통해 훈련된 LLaMA 기반의 전문 비평 모델입니다. 원본 응답을 입력으로 받으면 SelFee는 이 응답에 대한 피드백을 제공하고, 피드백을 기반으로 응답을 수정할 수 있습니다. 또한 SelFee는 품질이 주어진 임계값을 충족할 때까지 출력을 계속 수정할 수 있으므로, 이 모델은 검증기(즉, 자체 출력의 품질을 확인하여)이자 비평가(즉, 피드백을 제공하고 응답을 수정하여) 역할을 합니다.

LLM을 사용한 다양한 형태의 합성 데이터 생성 (출처: [11])은 데이터 부족 문제를 해결하는 데 중요한 역할을 합니다.

LLM 출력을 비평하는 많은 기술이 탐구되었습니다. 예를 들어, 자기 수정(Self-Correction), PEER, 자기 비평(Self-Critique) 등이 있습니다. 우리는 또한 유사한 기술을 사용하여 선호도 튜닝(preference tuning)과 지도 미세 조정(SFT)을 위한 합성 훈련 데이터를 생성하고 검증할 수 있습니다. 이 접근 방식의 일반적인 예로는 RLAIF, 헌법적 AI(Constitutional AI)(위에 표시됨) [11] 등이 있습니다.

이러한 모든 기술(LLM-as-a-Judge, 미세 조정된 심사관, 합성 데이터 생성, 비평 모델, 검증기)은 매우 유사하게 작동합니다. 이들은 단순히 점수와 설명을 출력합니다! 그러나 각 유형의 모델은 약간 다른 목적을 위해 사용됩니다. 예를 들어, 평가 목적으로 모델 출력에 등급을 매기는 것과 미세 조정을 위해 사용해야 하는지 여부를 결정하기 위해 모델 출력에 등급을 매기는 것 등입니다. 최근 연구는 이러한 기술, 또는 적어도 그 중 일부를 필요한 모든 채점 작업을 수행할 수 있는 단일 LLM을 통해 통합할 수 있다고 주장합니다 [12, 17]. 이는 AI 시스템의 효율성과 일관성을 극대화하는 방향으로 나아가고 있음을 시사합니다.

**미세 조정을 통해 LLM-as-a-Judge 능가하기는 혁신적인 AI 솔루션을 가능하게 합니다.**

LLM 심사관을 미세 조정하는 아이디어는 도메인별 세분화된 평가(fine-grained evaluation)를 수행할 수 있는 미세 조정된 LLM 심사관인 Prometheus [1]가 제안될 때까지 대중화되지 않았습니다. Prometheus는 이전의 미세 조정된 심사관에 비해 평가 기능이 크게 향상되었습니다. 그러나 이 모델은 주요 방법론적 진보를 이루지는 않았습니다. 훨씬 더 나은 기본 모델(LLaMA-2)에서 시작하여 미세 조정했을 뿐입니다. Prometheus는 많은 독점 LLM의 평가 품질과 일치하는 능력 때문에 빠르게 인기를 얻었으며, 이 섹션에서 개요를 살펴볼 여러 모델 변형(model variant)의 출시로 이어졌습니다. 이는 특정 비즈니스 요구사항에 맞춰 AI를 최적화하는 새로운 길을 열었습니다.

**Prometheus: 언어 모델에 세분화된 평가 능력 유도는 복잡한 문제 해결에 기여합니다 [1]** (출처: [1])

평가를 위해 독점 LLM을 사용하는 것은 간단합니다. 프롬프트에 채점 기준을 설명하고 모델이 나머지를 처리하도록 하면 됩니다. 그러나 이러한 평가기는 버전 관리(versioning)가 제어되지 않으며 비용이 많이 들 수 있습니다. 또한 독점 LLM은 일반적으로 일반적인 기준(예: 인간 선호도)을 평가하는 데 더 자주 사용되기 때문에 사용자 정의 기준을 평가하는 데 어려움을 겪을 수 있습니다. 해결책으로, [1]의 저자들은 특정 도메인에 미세 조정될 때 독점 평가기의 성능과 일치하거나 이를 능가할 수 있는 Prometheus라는 미세 조정된 평가기 LLM을 생성합니다. (출처: [1]) 이는 기업이 자체적인 AI 거버넌스 프레임워크를 구축하는 데 핵심적인 도구가 될 수 있습니다.

**훈련 데이터셋(Training dataset)**은 모델 성능의 기반입니다. Prometheus는 사용자 정의 채점 루브릭(scoring rubric)을 입력으로 받아들이도록 훈련되어, 모델이 전문화된 도메인과 기준에 쉽게 일반화될 수 있도록 합니다. 이 모델은 GPT-4로 합성적으로 생성되었으며 다양한 평가 작업을 다루는 Feedback Collection이라는 새로운 데이터셋에 대해 훈련됩니다. 각 데이터셋 예시에는 몇 가지 공통 구성 요소가 있습니다(위에 표시됨):
*   **명령어(Instruction)**: LLM에 프롬프트를 제공하는 데 사용된 명령어.
*   **응답(Response)**: (위 명령어에 대한) 우리가 평가하는 응답.
*   **루브릭(Rubric)**: 응답이 채점/평가되어야 하는 기준을 지정하는 사용자 정의 루브릭.
*   **참조 답변(Reference Answer)**: 명령어에 대한 응답 중 가장 좋은 점수를 받을 수 있는 예시.
*   **근거(Rationale)**: 응답이 특정 점수를 받은 이유에 대한 설명¹¹.
*   **점수(Score)**: 응답에 할당된 1-5 리커트 점수(Likert score).

채점 루브릭은 i) 채점 기준에 대한 일반적인 설명과 ii) 모델이 할당할 수 있는 각 점수에 대한 설명으로 구성됩니다. Prometheus의 경우 채점 옵션은 1-5 리커트 척도이므로, 모델은 이러한 각 점수와 그 의미가 설명되기를 기대합니다. 루브릭을 입력으로 받아들임으로써 모델은 세분화된 평가를 수행하고 다양한 채점 설정에 일반화하는 방법을 학습합니다. [1]의 저자들은 고품질 참조 자료를 미세 조정된 LLM 심사관에게 입력으로 제공하는 것의 중요성을 처음으로 입증한 사람들 중 일부입니다.

"우리는 다양한 참조 자료, 특히 참조 답변을 포함하는 것이 세분화된 평가 능력을 효과적으로 유도하는 데 중요하다는 것을 처음으로 탐구합니다." - [1]에서 발췌. 이는 AI 모델의 맥락 이해를 심화시키는 데 결정적인 역할을 합니다.

**데이터 생성하기**는 다양한 소스를 통합하여 진행됩니다. Feedback Collection은 GPT-4로 합성적으로 생성됩니다. 이 데이터셋을 생성하기 위해 우리는 50개의 인간이 작성한 채점 루브릭 세트로 시작합니다. 그런 다음 데이터셋은 다음을 통해 생성됩니다:
*   수동으로 작성된 채점 루브릭 예시로 GPT-4에 프롬프트를 제공하여 1천 개의 채점 루브릭으로 확장.
*   각 채점 루브릭에 대한 명령어를 생성하여 총 2만 개의 명령어(즉, 각 루브릭당 20개의 명령어)를 생성.
*   각 명령어에 대해 응답, 피드백 및 점수를 순차적으로 생성하여 각 명령어에 대한 훈련 인스턴스 생성.

GPT-4로 Feedback Collection을 합성적으로 생성하기 위해 따르는 위 단계에 대한 시각적 요약은 아래 그림에 제공되어 있습니다. (출처: [1])

데이터셋의 균형을 맞추기 위해 우리는 GPT-4에 각 명령어에 대해 5가지 다른 훈련 예시(각 1-5 리커트 점수당 하나)를 생성하도록 프롬프트를 제공합니다. 또한 평가기 내의 장황함 편향(verbosity bias)을 피하기 위해 각 훈련 예시가 동일한 길이를 갖도록 보장합니다. 최종 데이터셋의 요약은 아래에 제공되어 있습니다. (출처: [1])

**Prometheus 훈련하기**는 고급 최적화 기법을 사용합니다. LLaMA-2-13B-Chat은 Prometheus의 기본 모델로 사용됩니다. 이 모델은 Feedback Collection 데이터셋뿐만 아니라 MT Bench 및 Vicuna Bench에 대해서도 훈련됩니다. 모든 참조 자료를 입력으로 받은 모델은 지도 미세 조정(SFT) 전략을 사용하여 피드백을 순차적으로 제공한 다음 응답에 점수를 매기도록 훈련됩니다. 중요하게도, 우리는 응답에 실제로 점수를 매기기 전에 피드백을 생성하여 모델이 점수를 할당할 때 이 피드백을 맥락으로 사용할 수 있도록 합니다. 또한 저자들은 추론 중에 모델이 퇴화(degenerate)¹²하지 않도록 피드백과 점수 사이에 `[RESULT]`와 같은 특수 토큰을 삽입하는 것이 매우 중요하다고 언급합니다.

"CoT(Chain-of-Thought) 미세 조정과 유사하게, 우리는 피드백을 순차적으로 생성한 다음 점수를 생성하도록 미세 조정합니다." - [1]에서 발췌. 이는 모델의 추론 과정을 투명하게 만들고 신뢰도를 높이는 데 기여합니다.

**Prometheus는 효과적인 평가기일까요?** 실제 적용 사례를 통해 검증합니다. Prometheus는 GPT-3.5, GPT-4 및 여러 오픈소스 기본 모델과 비교하여 인간이 제공한 평가 점수와 일치하고 유용한 피드백을 생성하는 능력 측면에서 여러 데이터셋에 걸쳐 평가됩니다. 이 실험에서 Prometheus가 생성한 점수는 인간 평가자와 0.897의 피어슨 상관계수(Pearson correlation)를 가지며, 이는 GPT-4가 생성한 점수의 상관관계와 비슷하고 GPT-3.5가 생성한 점수의 상관관계보다 훨씬 좋습니다. 아래를 참조하십시오. (출처: [1])

이러한 결과는 특정 도메인 내에서 고품질 평가 데이터에 LLM을 직접 미세 조정함으로써 인간 평가와 인상적인 수준의 일치(GPT-4와 같은 강력한 모델과 일치하거나 이를 능가함)를 달성할 수 있음을 보여줍니다. 또한 인간은 58.67%의 경우에서 Prometheus의 피드백을 GPT-4의 피드백보다 선호한다고 선택했으며, 이는 모델의 설명이 유용하다는 것을 나타냅니다. Prometheus의 피드백은 거의 80%의 경우에서 GPT-3.5의 피드백보다 선호됩니다!

"우리는 GPT-4가 더 중립적이고 추상적인 경향이 있는 반면, Prometheus는 주어진 응답이 좋은지 아닌지에 대한 의견을 표현하는 명확한 경향을 보인다고 결론 내립니다." - [1]에서 발췌. 이는 Prometheus가 단순한 평가를 넘어선 비판적 사고와 명확한 피드백을 제공할 수 있음을 보여줍니다.

인간에게 Prometheus의 피드백이 선호되는 이유를 설명해달라고 요청했을 때, 우리는 미세 조정된 모델이 더 비판적이고 목표 지향적인 피드백을 생성하는 경향이 있다는 것을 알게 됩니다. 아래를 참조하십시오. 독점 모델과 달리 Prometheus는 범용 LLM이 아닙니다. 오히려 비평과 피드백을 제공하는 전문 모델입니다. 결과적으로 이 모델은 평가 설정에서 독점 기반 모델에 비해 덜 중립적이고, 추상적이며, 일반적입니다. (출처: [1])

**Prometheus 2: 다른 언어 모델 평가에 특화된 오픈소스 모델의 진화 [2]** (출처: [2])

[1]의 연구 방향을 이어받아, Prometheus 2 [2]는 사용자 정의 기준을 사용하여 세분화된 평가를 수행하도록 미세 조정된 또 다른 오픈 LLM입니다. 원래 Prometheus 모델에는 몇 가지 한계가 있습니다. 예를 들어, 모델은 채점 품질 측면에서 갈 길이 멉니다. 인간 품질 점수와 여전히 상당한 불일치가 있습니다. 그러나 가장 주목할 만한 점은 Prometheus가 직접 평가를 통해서만 채점할 수 있다는 것입니다. Prometheus의 점별 점수를 비교하여 순위를 도출할 수 있지만, 이 모델과 이전에 제안된 대부분의 오픈 평가기는 쌍대 채점(pairwise scoring)을 수행할 능력이 부족합니다. [2]에서 저자들은 두 가지 유형의 채점에 대해 인간과 GPT-4의 정확도와 밀접하게 일치하는 단일 세분화된 평가 모델로 이러한 패러다임을 통합합니다. (출처: [2]) 이는 AI 시스템의 유연성과 적응성을 크게 향상시킵니다.

**쌍대 채점으로 나아가기**는 사용자 선호도 분석에 유용합니다. Prometheus 2의 경우, 쌍대 채점 설정은 직접 평가 채점과 비교하여 몇 가지 주목할 만한 차이점이 있습니다:
*   참조 자료 내에 참조 답변이 없습니다. 참조 자료는 단순히 평가 기준을 설명합니다.
*   점수와 함께 제공되는 피드백은 다른 스타일을 가집니다. 단순히 점수에 대한 설명을 제공하는 대신, 쌍대 채점 설정 내에서 제공되는 피드백은 제공된 두 응답을 비교하고 대조하여 그 차이를 사용하여 하나를 다른 것보다 우수하다고 식별해야 합니다. 위에 표시됨.

대부분의 이전 모델과 데이터셋은 일반적인 피드백에 따라 쌍대 채점만 수행하며, 점수와 함께 쌍대 피드백을 생략하는 경향이 있지만, 개선된 채점 및 설명 가능성을 위해 CoT(Chain of Thought) 스타일 피드백을 활용하는 것은 Prometheus의 핵심 구성 요소입니다. 따라서 [2]의 저자들은 GPT-4에 각 쌍대 점수를 설명하도록 프롬프트를 제공하여 Preference Collection을 증강하고, 각 응답의 유사점과 차이점에 중점을 둡니다.

"각 응답 쌍에 대한 새로운 언어적 피드백을 생성하기 위해 우리는 GPT-4에 두 응답의 공통점과 차이점을 식별하도록 프롬프트를 제공합니다." - [2]에서 발췌. 이는 AI 시스템이 더욱 정교한 비교 분석을 수행할 수 있도록 돕습니다.

**Preference Collection**은 사용자 경험 개선에 필수적입니다. [1]의 Feedback Collection을 확장하여, [2]의 저자들은 Preference Collection이라는 쌍대 채점을 위한 추가 데이터셋을 생성합니다. Feedback Collection은 각 명령어에 대해 5개의 응답(각 1-5 리커트 점수당 하나)을 포함합니다. 이 데이터셋의 쌍대 버전을 생성하기 위해 우리는 단순히 이러한 응답의 모든 조합을 고려할 수 있습니다. 각 명령어에 대해 10개의 가능한 쌍이 형성될 수 있으며, 리커트 점수를 비교하여 각 쌍에 대한 점수를 도출할 수 있습니다. 요약은 아래를 참조하십시오. (출처: [2])

많은 쌍대 채점 데이터셋이 설명 없이 점수만 제공하는 경향이 있지만, 개선된 채점 및 설명 가능성을 위해 CoT(Chain of Thought) 스타일 피드백을 활용하는 것은 Prometheus의 핵심 구성 요소입니다. 따라서 [2]의 저자들은 GPT-4에 각 쌍대 점수를 설명하도록 프롬프트를 제공하여 Preference Collection을 증강하고, 각 응답의 유사점과 차이점에 중점을 둡니다.

"각 응답 쌍에 대한 새로운 언어적 피드백을 생성하기 위해 우리는 GPT-4에 두 응답의 공통점과 차이점을 식별하도록 프롬프트를 제공합니다." - [2]에서 발췌.

**모델 병합(Model merging)**은 다양한 AI 모델의 시너지를 창출합니다. LLM 심사관 내에서 직접 평가와 쌍대 채점 모두에 대한 지원을 구축하는 몇 가지 다른 방법이 있습니다:
*   **프롬프팅(Prompting)**: LLM-as-a-Judge 접근 방식을 채택하고 각 채점 설정에 대해 기성 모델로 보낼 다른 프롬프트를 생성할 수 있습니다.
*   **단일 형식 훈련(Single-format training)**: 각 채점 형식에 대해 별도의 LLM을 훈련하고 별도로 사용할 수 있습니다.
*   **공동 훈련(Join training)**: 직접 평가 및 쌍대 채점 예시¹⁴ 모두에 대해 단일 모델을 훈련하여 모델이 두 설정 모두에서 작동할 수 있도록 합니다.

위의 접근 방식 중 공동 훈련만이 두 채점 설정을 모두 지원할 수 있는 단일 전문 모델을 생성합니다. 프롬프팅은 일반/기성 모델에 의존하고 단일 형식 훈련은 각 채점 형식에 대해 별도의 모델을 생성하기 때문입니다. 대안적으로, 모델 병합(model merging)을 통해 단일 형식 훈련으로 생성된 모델들을 결합할 수 있습니다. 모델 병합 개념에 익숙하지 않은 분들을 위해 아래에서 이 주제에 대한 포괄적인 개요를 확인하십시오.

**모델 병합: 설문조사**
Cameron R. Wolfe, Ph.D. · 2024년 9월 16일
여러 모델의 예측을 평균하는 앙상블(ensemble)과 달리, 모델 병합은 이러한 모델의 가중치를 평균하여 추가 추론 비용 없이 기능들을 혼합한 단일 모델을 형성합니다.
전체 이야기 읽기

이것이 Prometheus 2가 취한 접근 방식입니다. 우리는 먼저 Feedback 및 Preference Collection에 대해 두 개의 LLM을 독립적으로 훈련합니다. 그런 다음, 간단한 선형 병합 방식(아래 표시됨)을 사용하여 이 모델들의 가중치를 병합하여 쌍대 및 점별 채점 모두를 수행할 수 있는 단일 모델을 생성합니다.

선형 모델 병합 (출처: [2])

가장 주목할 만한 점은 이 병합 접근 방식이 공동 훈련되거나 각 채점 형식에 대해 개별적으로 훈련된 LLM보다 우수한 성능을 보인다는 것입니다. 저자들은 [2]에서 여러 병합 방식(예: 태스크 산술(task arithmetic), TIES, DARE)을 테스트합니다. 경우에 따라 이러한 더 고급 병합 전략이 이점을 제공합니다. 예를 들어, Mixtral 8x-7B가 기본 모델로 사용될 때 DARE는 최고의 성능을 제공합니다. 그러나 계수가 0.5인 선형 병합 방식(즉, 모델 가중치의 평균을 취하는 것)은 대부분의 경우에 잘 수행되며 구현하기 쉽습니다.

"직접 평가 및 쌍대 순위 피드백 데이터셋에 대해 훈련된 평가기 LLM의 가중치를 병합하면 두 방식 모두에서 뛰어난 통합 평가기 LM이 생성된다는 것을 보여줍니다." - [2]에서 발췌. 이는 AI 모델의 다기능성을 입증하는 중요한 결과입니다.

**통합 평가기(A unified evaluator)**는 다각적인 분석을 제공합니다. Mistral-7B 또는 Mixtral 8x-7B를 기본 모델로 사용하여, i) 각 채점 데이터셋에 대해 이 모델들의 별도 복사본을 미세 조정하고 ii) 다양한 병합 알고리즘을 사용하여 이러한 단일 형식 평가기의 가중치를 결합함으로써 Prometheus 2의 다른 버전이 생성됩니다. 결과 모델은 여러 직접 평가 및 쌍대 채점 벤치마크에서 모든 오픈 평가기 LLM을 능가하는 것으로 나타났습니다. 특히, Prometheus 2는 모든 데이터셋에서 독점 평가기와의 피어슨 상관계수를 0.2 단위 향상시키고, 쌍대 순위 데이터셋에서 GPT-4와의 성능 격차를 약 50% 줄입니다. 아래를 참조하십시오. (출처: [2])

위 표에서 Prometheus 2의 성능 향상이 쌍대 채점 설정에서는 그리 중요하지 않다는 것을 알 수 있습니다. 그러나 이 표는 Prometheus 2를 오픈 평가기와 보상 모델(reward model) 모두와 비교하고 있습니다. 인간 피드백 기반 강화 학습(reinforcement learning from human feedback) 내에서 보상을 예측하는 데 사용되는 이러한 보상 모델은 쌍대 채점에만 특화되어 있으며, 이 작업에 대해 미세 조정되었고, 점수와 함께 어떤 피드백도 출력하지 않습니다. 보상 모델은 쌍대 순위 정확도 측면에서 인상적인 성능을 달성합니다. 또한 [2]에서 Prometheus 2가 채점 설정 전반에 걸쳐 다른 평가기보다 더 일관적인 경향이 있다는 것을 알 수 있습니다. 이는 쌍대 설정에서 선택된 응답이 직접 평가 설정에서도 일반적으로 더 높은 점수를 받을 것이라는 의미입니다. 이는 AI 시스템의 일관된 성능을 보장하는 데 중요한 시사점을 제공합니다.

**Prometheus-Vision: 세분화된 평가를 위한 비전-언어 모델 심사관은 새로운 지평을 엽니다 [3]**

지금까지 우리가 살펴본 모든 작업은 텍스트 전용 LLM에 중점을 두었지만, 이미지와 텍스트를 모두 입력으로 받을 수 있는 LLM을 지칭하는 비전-언어 모델(Vision-Language Model, VLM)은 최근 인기를 얻고 있습니다. 독점(예: GPT-4V 및 Gemini) 및 오픈(예: LLaVA 및 LLaMA-3.4) VLM이 모두 존재합니다. 여러 입력 양식(input modality)을 처리할 수 있다는 점 외에는, 이러한 모델은 훈련이나 아키텍처 측면에서 텍스트 기반 LLM과 크게 다르지 않습니다. 최근 연구의 실제 예시와 함께 VLM에 대한 접근 가능한 개요는 아래를 참조하십시오.

**AI를 앞서가다**
**다중 모달 LLM 이해하기**
두 달은 격렬했습니다. AI 연구에서 다시 많은 발전이 있었고, 두 개의 노벨상이 AI에 수여되었으며 여러 흥미로운 연구 논문이 발표되었...
1년 전 · 좋아요 219개 · 댓글 37개 · Sebastian Raschka, PhD

VLM은 이미지와 지시의 조합을 입력으로 받아 (텍스트) 출력을 생성합니다. 텍스트 기반 LLM에 비해 VLM을 평가하는 것은 약간 더 어렵습니다. 왜냐하면 우리는 다음 두 가지를 모두 수행해야 하기 때문입니다:
*   VLM이 지시를 따르는지 확인합니다(이전과 동일).
*   VLM의 응답이 이미지에 "기반을 둔(grounded)" 것인지 결정합니다.

이를 위해 우리는 이미지의 텍스트 기반 표현(예: 캡셔닝 모델(captioning model)을 통해)을 도출하고 이 정보를 텍스트 기반 LLM 심사관(예: GPT-4 또는 Prometheus [5])에 전달할 수 있습니다. 그러나 이러한 다단계 파이프라인(multi-stage pipeline)은 오류가 발생하기 쉽습니다. VLM을 심사관으로 직접 사용하는 것이 훨씬 더 합리적입니다! (출처: [3]) 이는 다중 모달 AI 시스템의 효율성을 극대화하는 데 기여합니다.

Prometheus-Vision은 [3]에서 제안된 오픈 VLM 기반 평가기입니다. GPT-4V와 같은 독점 VLM은 다른 VLM을 평가하는 데 사용될 수 있으며 실제로 사용되었습니다 [4]. 이전 Prometheus 모델의 방향을 따라 Prometheus-Vision은 세분화된 사용자 정의 기준에 따라 평가할 수 있는 최초의 오픈 VLM입니다. 더 많은 투명성과 제어를 제공하는 것 외에도 Prometheus-Vision은 인간 및 독점 모델의 점수와 잘 상관관계가 있는 점수를 생성합니다. 이 모델은 놀랍도록 효과적이며 데이터의 미묘한 뉘앙스까지 포착할 수 있습니다. 예를 들어, 예술 작품과 패러디의 차이(위 그림 참조). 이는 문화 예술 분야에서 AI의 활용 가능성을 넓힙니다.

"언어 도메인과 대조적으로, 우리가 아는 한, 세분화된 방식으로 평가할 수 있는 평가기 VLM을 훈련하는 데 적용 가능한 피드백, 비평 또는 선호도 데이터셋은 존재하지 않습니다." - [3]에서 발췌. 이러한 데이터셋의 부족은 다중 모달 AI 연구의 중요한 도전 과제입니다.

Feedback Collection에서 영감을 받은 Perception Collection은 Prometheus-Vision을 훈련하기 위해 생성된 이미지 및 텍스트 기반 평가 데이터셋입니다. 출판 당시에는 VLM 기반 평가 모델 훈련을 위한 이러한 다중 모달 피드백 데이터셋은 아직 존재하지 않았습니다. Perception Collection의 각 데이터셋 인스턴스에는 5개의 입력 구성 요소가 있습니다:
*   **이미지(Image)**: 사용자가 VLM에 입력으로 제공하는 이미지.
*   **명령어(Instruction)**: 사용자가 VLM에 제공하는 텍스트 명령어.
*   **응답(Response)**: 입력으로 제공된 이미지와 명령어를 기반으로 VLM이 생성한 텍스트 응답.
*   **루브릭(Rubric)**: 점수를 생성할 때 참조해야 하는 상세한 채점 지침(기준에 대한 설명과 1-5 리커트 척도에서 가능한 각 점수에 대한 설명 포함).
*   **참조 답변(Reference Answer)**: 이미지 및 명령어 입력에 대한 응답 중 5점 만점을 받을 수 있는 예시 응답. [3]에서 저자들은 GPT-4V를 사용하여 모든 참조 답변을 합성적으로 생성합니다.

각 데이터셋 인스턴스에는 i) 작성된 근거 또는 피드백과 ii) 1-5 리커트 척도에 따른 채점 결정이라는 두 가지 출력 구성 요소도 있습니다. 아래를 참조하십시오. (출처: [3])

이 데이터셋을 생성하기 위해 우리는 MS-COCO 및 MMMU에서 5천 개의 이미지를 샘플링하는 것으로 시작합니다. 그런 다음 Feedback Collection에 사용된 생성 전략과 거의 동일한 절차를 따릅니다: i) 50개의 시드 루브릭을 수동으로 작성하고, ii) GPT-4를 사용하여 1만 5천 개의 루브릭을 합성적으로 생성하고, iii) 이 루브릭에 대한 3만 개의 명령어(및 관련 참조 답변)를 생성하고, iv) 각 명령어를 5개의 응답(각 점수당 하나)과 관련 피드백¹⁵으로 증강합니다. Feedback Collection과 비교하여 Perception Collection은 더 많은 수의 루브릭(즉, 1만 5천 개 대 1천 개)을 가지지만, 루브릭당 명령어 수는 더 적습니다(2개 대 20개). 아래를 참조하십시오. (출처: [3])

**모델 훈련하기**. Prometheus-Vision을 훈련하기 위해 텍스트 전용 기본 모델을 사용할 수 없습니다. 오픈 (사전 훈련된) VLM으로 시작해야 합니다. [3]에서 저자들은 LLaVA-1.5의 7B 및 13B 매개변수 변형을 기본 모델로 사용합니다. 이전 Prometheus 모델과 유사하게 Prometheus-Vision은 CoT(Chain of Thought) 미세 조정 전략을 사용하여 훈련됩니다. 우리는 지도 미세 조정(SFT) 접근 방식을 사용하여 모델에 피드백을 순차적으로 생성한 다음 점수를 생성하도록 가르칩니다. 모든 훈련 예시는 피드백과 점수를 'So the overall score is'라는 고정된 문구로 명시적으로 구분합니다. 참조 자료(예: 루브릭 및 참조 답변)를 입력으로 제공함으로써 우리는 사용자 정의된 세분화된 평가를 수행하는 데 뛰어난 모델을 생성합니다. (출처: [3])

**경험적 결과(Empirical results)**. Prometheus-Vision은 인간 점수와의 상관관계 측면에서 GPT-3.5-Turbo와 텍스트 전용 Prometheus 모델 모두를 능가합니다. 그러나 GPT-4와 GPT-4V는 인간 평가와 더 높은 수준의 일치도를 달성합니다. Prometheus-Vision의 성능에서 가장 큰 하락은 텍스트가 많은 이미지를 포함하는 VisIT Bench에서 관찰됩니다. 아래를 참조하십시오.

VisIT Bench의 데이터 예시 (출처)

이 데이터에서 Prometheus-Vision은 성능이 좋지 않은 반면, GPT-4는 이미지에서 추출된 텍스트를 입력으로 받아들이기 때문에 잘 수행됩니다(GPT-4V보다 우수함!). 다른 데이터셋에서는 Prometheus-Vision이 GPT-4 및 GPT-4V와 채점 품질에서 비슷하며, 여러 경우에서 인간 점수와의 상관관계를 능가하기도 합니다.

"이전 연구들은 길이 편향(length bias)으로 알려진 현상을 강조했습니다. 이는 평가기 모델이 더 긴 응답을 선호하는 경향을 의미합니다... 자기 강화 편향(self-enhancement bias)은... 평가기가 자신의 응답을 선호하는 경우입니다." - [3]에서 발췌. 이는 다중 모달 AI 시스템의 편향 문제를 해결하는 데 중요한 과제입니다.

인간에게 제공된 피드백을 평가해달라고 요청했을 때, Prometheus-Vision이 GPT-4 및 GPT-4V가 생성한 피드백의 품질과 크게 일치한다는 것을 알 수 있습니다. [3]의 저자들은 또한 길이 편향 또는 자기 강화 편향과 같은 다양한 종류의 편향에 대해 Prometheus-Vision을 테스트합니다. 모델은 명백한 편향의 원인을 보이지 않는 것으로 나타났지만, 이러한 편향을 개별적으로 측정하는 것은 상당히 어렵습니다.

**다른 유형의 미세 조정된 심사관(Other Types of Finetuned Judges)은 특정 산업 요구사항을 충족합니다.**

Prometheus 모델 외에도 평가 목적으로 LLM을 미세 조정하려는 다양한 최근 시도가 있었습니다. 아래에 이 분야의 연구에 대한 간략한 참조를 제공하며, 각 연구의 기여에 대한 설명이 함께 제공됩니다. (출처: [13])

**자기 보상 LLM(Self-rewarding LLM) [13]**은 LLM 평가기의 도움으로 정렬(alignment) 프로세스를 개선하고 자동화