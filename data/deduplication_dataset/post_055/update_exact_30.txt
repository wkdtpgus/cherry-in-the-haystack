대규모 언어 모델(LLM)의 발전은 전례 없는 기능을 제공하지만, 동시에 모델의 성능을 정확하고 효율적으로 평가하는 데 있어 새로운 과제를 제기하고 있습니다. 특히 복잡하고 개방형(open-ended)인 LLM의 출력은 전통적인 평가 방식으로는 충분히 측정하기 어렵습니다. 이러한 배경 속에서 LLM 기반 평가(LLM-based evaluation)와 미세 조정된 전문 심사관(finetuned specialized judges)의 중요성이 부각되고 있습니다. 이 글에서는 LLM 평가의 복잡성을 탐구하고, 다양한 평가 방법론을 살펴본 뒤, 자체 LLM 심사관을 훈련하는 방법에 대한 심층적인 가이드를 제공합니다.

(출처: [1, 2, 3, 9, 23]) 대규모 언어 모델(LLM)의 기능이 확장됨에 따라 이들을 평가하는 것이 더욱 어려워졌습니다. 현대의 기반 모델(foundation model)은 광범위한 영역을 다루며, 그 출력은 대개 개방형(open-ended)입니다. 이는 어떤 입력이든 여러 가지 유효한 출력을 가질 수 있다는 것을 의미합니다. 이러한 이유로 LLM을 프로그래밍 방식으로 평가하는 것은 복잡하고 활발한 연구 문제입니다. LLM의 단일 기능을 평가하는 것조차 이미 어렵지만, LLM은 평가할 가치가 있는 수많은 기능과 행동을 가지고 있습니다. "LLM이 생성하는 출력의 품질을 평가하는 것은 텍스트와 복잡한 작업의 매우 다양한 분포를 다루기 때문에 점차 어려워지고 있습니다. 이 문제를 해결하기 위해 LLM 기반 평가(LLM-based evaluation)는 LLM이 생성한 텍스트를 평가하기 위한 확장 가능하고 저렴한 패러다임으로 부상했습니다." - [2]에서 발췌. LLM의 성능을 판단하는 가장 신뢰할 수 있는 방법은 인간이 모델의 출력을 평가하도록 하는 것이지만, 인간 평가(human evaluation)는 노이즈가 많고, 비용이 많이 들며, 시간이 많이 소요됩니다. 일정량의 인간 평가는 항상 필요하지만, 전적으로 인간 평가에만 의존하는 것은 확장 가능하지 않습니다. 우리는 새로운 LLM의 기능을 효율적으로 테스트할 수 있어야 합니다. 이러한 필요성은 독점 LLM(proprietary LLM)이 다른 LLM의 출력을 평가하도록 프롬프트하는 LLM-as-a-Judge [8]의 제안을 촉발했습니다. LLM-as-a-Judge는 현재 연구에서 많이 사용되지만, 독점 LLM이 덜 익숙할 수 있는 세분화된 기준(granular criteria)의 평가를 요구하는 도메인별 애플리케이션(domain-specific application)에는 덜 효과적입니다. 이러한 경우, 우리는 자체적인 전문 LLM 심사관을 훈련해야 할 수도 있으며, 이 개요의 목표는 이를 달성하는 방법에 대한 포괄적인 이해를 얻는 것입니다. LLM을 평가하는 다양한 접근 방식에 대해 광범위하게 살펴보는 것부터 시작하겠습니다.

**평가 패러다임의 진화: 인간 중심에서 AI 중심으로**

LLM의 성능을 평가하는 방법은 모델의 발전과 함께 지속적으로 진화해왔습니다. 초기에는 주로 인간 평가에 의존했지만, LLM의 규모와 복잡성이 증가하면서 인간 평가의 한계가 명확해졌습니다. 인간 평가는 심층적인 이해와 미묘한 판단을 제공하지만, 비용과 시간 측면에서 비효율적이며, 평가자 간의 주관성으로 인해 일관성이 부족할 수 있습니다. 이러한 문제를 해결하기 위해 자동화된 지표가 도입되었으나, 이 역시 개방형 생성 모델의 복잡한 출력을 완전히 포착하지 못하는 한계에 직면했습니다. 최근에는 LLM 자체를 평가 도구로 활용하는 LLM-as-a-Judge 패러다임이 각광받으며, 평가의 효율성과 확장성을 크게 향상시키고 있습니다. 이러한 평가 패러다임의 변화는 LLM 개발 주기 단축과 모델 품질 향상에 필수적인 역할을 하고 있습니다.

**LLM을 평가하는 방법**

LLM을 평가하는 데 있어 인간 평가와 자동 지표는 상호 보완적인 관계를 가집니다. 인간 평가는 모델 성능의 최종적인 '진실의 원천'이지만, 그 한계 때문에 모델 개발 속도를 저해할 수 있습니다. 반면 자동 지표는 빠른 피드백을 제공하여 모델 반복(model iteration)을 가속화하지만, 인간의 복잡한 판단을 완전히 대체하기는 어렵습니다. 따라서 효과적인 평가 전략은 이 두 가지를 적절히 조합하는 데 있습니다. 즉, 자동 지표를 활용하여 대규모의 모델 테스트와 초기 검증을 수행하고, 주기적으로 인간 평가를 통해 자동 지표의 신뢰성을 검증하며, 모델의 미묘한 성능 변화를 포착하는 것입니다. 최근에는 특정 도메인이나 작업에 특화된 미세 조정된 LLM 심사관이 이러한 간극을 메우는 중요한 역할을 하고 있습니다. 이들은 독점 LLM 심사관의 비용 및 투명성 문제를 해결하며, 특정 기준에 대한 세분화된 평가 능력을 제공하여 인간 평가의 부담을 줄이고 자동 평가의 정확도를 높이는 데 기여합니다.

**자동 평가의 유형**

대부분의 현대 LLM에 사용되는 두 가지 주요 자동 평가 전략(아래 참조)이 있습니다: 퍼플렉시티(perplexity)와 모델 기반 평가(model-based evaluation). 퍼플렉시티 기반 평가(perplexity-based evaluation)는 MMLU 또는 BIG-bench와 같은 대부분의 전통적인 자연어 처리(NLP) 벤치마크를 포함합니다. 우리는 이러한 벤치마크를 LLM의 일반 지식을 테스트하는 객관식 스타일 질문으로 생각할 수 있습니다. 그러나 이러한 벤치마크는 LLM이 더 길고 양식화된 답변을 생성해야 하는 더 개방형(open-ended) 환경에서 LLM을 평가할 때 부족합니다. (출처: [23])

더 길고 개방형 출력(open-ended output)을 평가하기 위해 우리는 두 가지 옵션이 있습니다:
*   **인간 평가(Human evaluation)**: 모델의 출력을 평가하기 위해 인간에게 의존합니다.
*   **모델 기반 평가(Model-based evaluation)**: 강력한 LLM을 사용하여 모델의 출력을 평가합니다.

인간 평가의 한계 때문에, LLM-as-a-Judge [8]와 같은 모델 기반 평가 전략은 LLM을 평가하는 데 있어 주된 접근 방식이 되었습니다. 이러한 모델 기반 평가 기술은 참조를 필요로 하지 않으며, 구현하기 쉽고, 다양한 개방형 작업을 처리할 수 있습니다.

이제 우리는 인간 평가와 자동 평가를 포함한 기본적인 평가 범주를 이해했습니다. 또한 자동 평가를 수행하는 여러 가지 방법(즉, 벤치마크 또는 모델 기반 평가)이 있다는 것도 이해했습니다. 따라서 인간 평가부터 시작하여 다양한 자동 평가 기술로 넘어가면서 이러한 각 평가 전략을 더 자세히 살펴보겠습니다.

**전통적인 (자동) 지표의 한계와 새로운 요구**

LLM의 생성 능력이 극적으로 향상되면서, 단순히 텍스트의 유사성을 측정하는 전통적인 자동 지표들은 그 한계를 드러냈습니다. 특히 "환각(hallucination)" 현상이나 미묘한 편향(bias)과 같은 LLM의 고유한 문제점들은 기존 지표로는 거의 감지할 수 없습니다. 모델이 문법적으로 완벽하고 유창한 텍스트를 생성하더라도, 사실과 다르거나 부적절한 내용을 포함할 수 있기 때문입니다. 이러한 문제들은 LLM이 단순한 언어 이해를 넘어 복잡한 추론과 지식 활용을 요구하는 작업에 사용될 때 더욱 두드러집니다.

"기존 벤치마크(benchmark)와 전통적인 지표는 개방형(open-ended) 시나리오에서 LLM의 기능을 적절하게 추정하지 못합니다. 개방형 작업에서 LLM을 포괄적으로 평가할 수 있는 새로운 벤치마크 방법이 필요합니다." - [9]에서 발췌.

**전통적인 지표가 부족한 이유**. 전통적인 지표가 인간의 선호도와 상관관계가 낮은 몇 가지 이유가 있지만, 가장 시급한 문제는 이러한 지표의 대부분이 참조 기반이라는 것입니다. 이들은 모델의 출력을 우리가 일치시키고자 하는 정답(ground truth answer)과 비교하여 작동합니다. 기본적으로 퍼지 매칭(fuzzy matching)의 더 정교한 버전입니다. 그러나 현대 LLM은 엄청나게 개방형(open-ended)입니다! 단일 프롬프트(prompt)가 주어졌을 때, LLM이 생성할 수 있는 동등하게 유효한 응답이 많이 있습니다. 따라서 단일 참조에 따라 평가하면 존재하는 유효한 응답의 스펙트럼을 포착하지 못합니다.

이러한 한계는 LLM의 다면적인 성능을 평가하기 위한 새로운 접근 방식의 필요성을 강조합니다. 특히, 모델의 사실성(factuality), 유해성(toxicity), 공정성(fairness), 그리고 복잡한 지시를 따르는 능력(instruction following)과 같은 측면을 평가하는 데 있어 더 정교한 방법론이 요구됩니다.

**LLM-as-a-Judge: 평가의 새로운 지평**

전통적인 평가 지표의 단점은 더 유연하고 일반적인 평가 기술의 개발로 이어졌습니다. 이러한 기술 중 가장 인기 있는 것 중 하나는 언어 모델에 프롬프트를 제공하여 평가를 수행하는 LLM-as-a-Judge [8]라는 모델 기반 평가 전략입니다. 더 구체적으로 말하면, 우리는 강력한 LLM을 "심사관"으로 삼아 다른 LLM의 출력을 평가합니다. [8]에서 보여주듯이, LLM-as-a-Judge는 인간의 선호도와 높은 상관관계를 가지며, 구현하기 쉽고(즉, 프롬프트만 작성하면 됩니다!), 개방형 출력을 처리할 수 있으며, 다양한 평가 기준을 포착할 만큼 충분히 유연합니다.

"LLM-as-a-Judge는 인간의 선호도를 근사화하는 확장 가능하고 설명 가능한 방법이며, 그렇지 않으면 얻기 매우 비쌉니다." - [8]에서 발췌.

LLM-as-a-Judge는 단순히 점수를 매기는 것을 넘어, 평가에 대한 상세한 근거(rationale)를 제공함으로써 모델 개발자에게 귀중한 통찰력을 제공합니다. 이는 '왜' 특정 응답이 더 좋거나 나쁜지에 대한 설명을 제공하여 모델 개선 방향을 명확히 제시합니다. 또한, LLM 심사관은 다양한 평가 기준(예: 사실성, 창의성, 간결성, 어조 등)을 동시에 고려하도록 프롬프트될 수 있어, 다차원적인 평가가 가능합니다. 이러한 유연성은 특히 복잡한 생성 작업에서 LLM의 성능을 종합적으로 이해하는 데 필수적입니다. 최근에는 LLM 심사관의 편향(bias)을 줄이고 일관성을 높이기 위한 프롬프트 엔지니어링(prompt engineering) 기술(예: 역할 부여, 단계별 사고(Chain-of-Thought) 유도)이 활발히 연구되고 있습니다.

**LLM 심사관 활용의 확장: 자기 수정 및 다중 에이전트 평가**

LLM-as-a-Judge의 개념은 단순히 외부 모델을 평가하는 것을 넘어, LLM 자체의 성능을 개선하는 방향으로 확장되고 있습니다. '자기 수정(Self-Correction)' 또는 '자기 성찰(Self-Reflection)'이라고 불리는 기술은 LLM이 자신의 출력을 LLM 심사관의 관점에서 평가하고, 그 피드백을 바탕으로 출력을 수정하도록 합니다. 이는 모델이 반복적인 개선 루프를 통해 품질을 향상시키는 데 기여하며, 특히 복잡한 추론이나 코드 생성과 같은 작업에서 효과적입니다.

또한, '다중 에이전트 평가(Multi-Agent Evaluation)'는 여러 LLM 심사관을 활용하여 평가의 견고성(robustness)을 높이는 접근 방식입니다. 각 심사관이 다른 관점이나 기준을 가지고 평가하도록 설계될 수 있으며, 이들의 평가를 종합하여 최종 점수를 도출함으로써 단일 심사관의 잠재적 편향이나 오류를 줄일 수 있습니다. 이러한 다중 에이전트 시스템은 특히 논쟁의 여지가 있거나 주관성이 높은 작업에서 유용하며, 보다 균형 잡힌 평가를 가능하게 합니다. 이러한 기술들은 LLM 심사관이 단순한 평가 도구를 넘어 LLM 생태계의 핵심 구성 요소로 자리매김하고 있음을 보여줍니다.

**인간 평가(Human Evaluation)**

"인간 평가는 텍스트의 미묘하고 주관적인 측면을 평가하는 본질적인 신뢰성과 능력 때문에 지속적으로 지배적인 방법이었습니다. 많은 상황에서 인간은 간결성, 창의성, 어조, 문화적 민감성 등 평가의 가장 중요한 요소를 자연스럽게 식별할 수 있습니다." - [1]에서 발췌.

인간 평가는 LLM을 평가할 때 우리의 최종적인 진실의 원천입니다. 이 평가를 수행하는 인간들, 즉 인간 평가자(human evaluator) 또는 주석자(annotator)는 다양한 방식으로 확보될 수 있습니다. 예를 들어, 우리는 크라우드소싱(crowdsource)을 하거나, 고용하거나¹, 심지어 우리 자신, 즉 모델 개발자를 사용하여 LLM의 출력을 평가할 수도 있습니다. 인간 평가가 모델 품질 측면에서 우리의 진실의 원천이지만, 이것이 인간 평가가 완벽하다는 것을 의미하지는 않습니다. 사실, 실제로는 정반대인 경우가 많습니다. 인간 평가는 노이즈가 많고, 어렵고, 편향되기 쉽습니다.

**인간 평가 (또는 주석) 프로세스**

**합의 및 보정(Agreement and calibration)**. 겉보기에 주관적인 작업에서도 일치하는 평가를 단순히 세거나 코헨의 카파(Cohen's Kappa), 플라이스의 카파(Fleiss' Kappa), 크리펜도르프의 알파(Krippendorff's Alpha)와 같은 지표를 사용하여 측정되는 인간 합의는 낮을 수 있습니다. 따라서 우리는 인간 주석자를 "보정(calibrating)"하는 데 노력을 투자해야 합니다(즉, 특정 작업을 일관되고 정확하게 평가하는 방법을 가르치는 것). 위를 참조하십시오. 보정은 일반적으로 인간 주석자들 간의 회의²를 통해 의견 불일치를 논의하고 해결하는 것을 포함합니다. 그런 다음, 이러한 논의 결과를 가져와 작업에 대한 평가 지침에 통합할 수 있습니다.

**지침 작성(Crafting guidelines)**. 인간이 특정 작업을 어떻게 평가하거나 주석을 달아야 하는지 문서화하기 위해, 우리는 다음을 설명하는 일련의 서면 지침을 작성해야 합니다:
*   정확히 무엇을 평가하려고 하는지(즉, 평가 기준).
*   이러한 기준을 적절하게 평가하는 방법.

서면 지침과 올바르거나 잘못된 주석의 구체적인 예시³로 구성된 이 지침은 인간이 주석 프로세스 전반에 걸쳐 지속적으로 참조할 수 있는 상세한 자료입니다. 서면 지침은 주석 프로세스를 더 일관성 있게 만들고, 새로운 주석자를 온보딩하는 작업을 단순화합니다. 주석 작업을 올바르게 수행하는 데 필요한 모든 정보는 이 지침 내에 명확하게 설명되어야 합니다.

"우리가 라벨러(labeler)에게 제공한 지침은 프로젝트가 진행됨에 따라 진화했습니다. 피드백을 제공하고, 메타데이터 필드를 변경하며, 측정하고자 하는 바에 대한 더 나은 이해를 발전시켰습니다. 또한 혼란스럽거나 일관성이 없는 지침은 수정했습니다." - [22]에서 발췌.

이러한 지침은 고정되어 있지 않습니다. 오히려 우리는 인간 주석자와 협력하고 더 높은 수준의 합의를 달성하기 위해 시간이 지남에 따라 지속적으로 업데이트할 수 있습니다. 대부분의 프로젝트에서 우리가 평가하려는 것에 대한 이해는 시간이 지남에 따라 더 명확하고 상세해집니다. 특정 평가 작업이 간단하거나 명확하다고 생각할 수 있지만, 대부분의 주석 작업은 본질적으로 놀라울 정도로 많은 주관성을 포함합니다. 이러한 주관성은 우리가 한 그룹의 인간이 주어진 작업에 일관되게 동의하도록 시도할 때만 명확해집니다.

**지속적인 모니터링(Continuous monitoring)**. 일반적으로 우리는 평가 작업을 위한 지침을 작성하고 인간 평가자를 보정하는 데 많은 초기 노력을 투자합니다. 일단 우리가 설정한 평가 기준에 만족하고 이러한 기준을 평가할 때 합리적인 수준의 합의에 도달하면, 우리는 인간 평가 결과에 더 자신감을 가질 수 있으며 이 결과를 LLM 성능의 척도로 사용하기 시작할 수 있습니다. 그러나 우리는 시간이 지남에 따라 인간 평가자들 간의 합의를 계속 측정하여 저하나 편류가 없는지 확인해야 합니다. 합의가 감소할 수 있는 몇 가지 이유가 있습니다. 새로운 인간이 평가자 그룹에 들어올 수도 있고, 평가자들이 시간이 지남에 따라 지침을 잊어버릴 수도 있으며, 심지어 지침을 변경할 수도 있습니다! 인간 평가가 일관되고 정확하도록 보장하는 것은 끝없는(그러나 극히 필요한) 싸움입니다.

**메타 평가(Meta-Evaluation): 우리 평가 모델 평가하기**

우리만의 LLM 심사관을 훈련하는 방법을 배우기 전에, 심사관이 잘 수행하는지 여부를 어떻게 결정할 것인지 알아야 합니다. LLM 심사관을 평가하려면 먼저 인간 평가 데이터 세트를 수집해야 합니다. 이 데이터는 우리의 평가 모델의 품질을 측정하는 데 사용될 것이므로 정확하고 신뢰할 수 있다고 매우 확신해야 합니다. 인간이 주석을 단 고품질 데이터 세트를 확보하면, 평가기의 출력을 인간 평가 결과와 단순히 비교하여 메타 평가(meta-evaluation)(즉, 평가기 평가)를 수행할 수 있습니다. 평가기가 이진 출력(binary output)(예: 쌍대 채점 또는 이진 척도를 사용한 단일 응답 채점)을 생성하는 경우, 단순히 분류 지표(classification metric)를 사용할 수 있습니다. 분류 지표는 해석하기 매우 쉽기 때문에 많은 실무자들은 LLM-as-a-Judge에 이진 채점을 고수하는 것이 가장 좋다고 주장했습니다.

"백본 모델(backbone model)로 GPT-4를 사용하는 G-Eval이 요약 작업에서 인간과 0.514의 스피어만 상관계수(Spearman correlation)를 달성하여 이전의 모든 방법을 큰 차이로 능가한다는 것을 보여줍니다." - [23]에서 발췌.

이진적이지 않은 채점 설정(예: 1-5 리커트 척도를 사용한 단일 응답 채점)의 경우, 인간과 자동 평가 점수⁶ 간의 상관관계(correlation)를 측정해야 합니다. 최근 LLM-as-a-Judge 논문에서 사용된 상관관계 지표(correlation metric)의 예시가 위에 제공되어 있습니다. 스피어만 상관계수는 아마도 가장 일반적으로 사용되는 상관관계 지표이지만, 코헨의 카파(Cohen's kappa), 켄달의 타우(Kendall's tau), 피어슨 상관계수(Pearson correlation) 등 다른 많은 지표도 존재합니다. 불행히도 이러한 지표는 분류 지표보다 해석하기 어렵기 때문에, 우리가 사용하는 특정 상관관계 지표의 미묘한 차이에 항상 익숙해야 합니다.

**미세 조정된 심사관에 대한 연구 동향**

초기에는 독점 LLM이 평가 목적으로 주로 사용되었지만, 오픈소스 LLM의 성능 향상과 함께 미세 조정된(finetuned) 전문 심사관에 대한 연구가 활발해졌습니다. 이러한 연구는 독점 모델의 비용, 투명성, 제어권 문제를 해결하는 데 중점을 둡니다. 주요 연구 동향은 다음과 같습니다.

*   **합성 데이터의 활용**: Prometheus [1], PandaLM [6], JudgeLM [9]과 같은 모델들은 고품질의 대규모 합성 데이터셋을 생성하여 미세 조정에 활용했습니다. 이는 인간 주석의 비용과 시간을 절약하면서도 다양한 평가 시나리오를 포괄할 수 있게 합니다. 특히 GPT-4와 같은 강력한 LLM을 '교사(teacher)' 모델로 사용하여 평가 데이터와 근거를 생성하는 방식이 널리 채택되었습니다.
*   **세분화된 평가 능력**: 초기 LLM 심사관이 주로 전반적인 선호도 평가에 집중했다면, 최신 연구들은 특정 기준(예: 사실성, 안전성, 창의성, 특정 도메인 지식)에 대한 세분화된 평가 능력을 유도하는 데 초점을 맞춥니다. 이를 위해 채점 루브릭(scoring rubric)이나 참조 답변(reference answer)과 같은 추가 정보를 입력으로 제공하는 방식이 중요하게 다루어집니다.
*   **편향 완화**: 위치 편향(position bias), 지식 편향(knowledge bias), 형식 편향(format bias)과 같은 평가기 내의 편향을 줄이기 위한 다양한 전략이 탐구되었습니다. 데이터 증강(data augmentation), 위치 전환(position switching), 참조 자료 제공 등이 대표적인 예시입니다.
*   **다양한 채점 설정 통합**: Prometheus 2 [2]와 같이 직접 평가(pointwise)와 쌍대 비교(pairwise) 채점 설정을 모두 지원하는 통합 평가 모델을 구축하려는 시도가 있었습니다. 모델 병합(model merging) 기술을 활용하여 이러한 다양한 평가 모드를 단일 모델에 효율적으로 통합하는 방법이 제시되었습니다.
*   **다중 모달 평가**: 텍스트 기반 LLM 심사관을 넘어, 이미지와 텍스트를 함께 평가할 수 있는 비전-언어 모델(VLM) 심사관(예: Prometheus-Vision [3])의 개발도 중요한 추세입니다. 이는 VLM의 '기반성(groundedness)'과 같은 새로운 평가 기준을 도입하며 평가의 영역을 확장합니다.

이러한 연구들은 미세 조정된 LLM 심사관이 독점 모델에 필적하거나 능가하는 성능을 특정 도메인과 작업에서 달성할 수 있음을 입증하며, LLM 평가의 미래 방향을 제시하고 있습니다.

**Prometheus 시리즈의 혁신과 발전**

Prometheus 시리즈는 미세 조정된 LLM 심사관 분야에서 가장 중요한 연구 중 하나입니다. Prometheus [1]는 GPT-4와 같은 독점 모델의 평가 품질에 필적하거나 이를 능가하는 최초의 오픈소스 LLM 심사관으로, 세분화된 평가 능력을 유도하는 데 성공했습니다. 이 모델의 핵심 혁신은 사용자 정의 채점 루브릭과 참조 답변을 입력으로 받아들이도록 훈련되었다는 점입니다. 이는 모델이 다양한 도메인과 기준에 유연하게 적용될 수 있도록 하며, 평가에 대한 상세한 근거를 제공하여 개발자가 모델 개선에 활용할 수 있도록 합니다.

Prometheus 2 [2]는 원래 Prometheus의 한계를 극복하며 진화했습니다. 특히, 직접 평가(pointwise)뿐만 아니라 쌍대 채점(pairwise)까지 지원하는 통합 평가 모델을 목표로 했습니다. 이를 위해 Feedback Collection과 Preference Collection이라는 두 가지 데이터셋을 생성하고, 모델 병합(model merging) 기술을 사용하여 두 가지 채점 방식에 특화된 모델들을 하나의 강력한 평가기로 결합했습니다. Prometheus 2는 다양한 벤치마크에서 기존 오픈 평가기들을 능가하며, 독점 평가기와의 성능 격차를 크게 줄였습니다.

Prometheus-Vision [3]은 이러한 개념을 다중 모달(multi-modal) 영역으로 확장했습니다. 이미지와 텍스트를 모두 평가할 수 있는 비전-언어 모델(VLM) 심사관을 제안하여, VLM의 '기반성(groundedness)'과 같은 새로운 평가 기준을 다룰 수 있게 했습니다. Perception Collection이라는 다중 모달 평가 데이터셋을 구축하고 LLaVA와 같은 오픈 VLM을 미세 조정하여, Prometheus-Vision은 인간과 독점 VLM의 점수와 높은 상관관계를 보이는 결과를 보여주었습니다. 이는 LLM 심사관이 단순한 텍스트를 넘어 복잡한 다중 모달 콘텐츠까지 평가할 수 있는 가능성을 열었습니다.

이러한 Prometheus 시리즈의 연속적인 발전은 미세 조정된 LLM 심사관이 LLM 평가의 핵심 도구로 자리매김하고 있으며, 효율적이고 유연하며 투명한 평가 시스템을 구축하는 데 중요한 역할을 하고 있음을 보여줍니다.

**다른 유형의 미세 조정된 심사관 및 미래 방향**

Prometheus 모델 외에도 평가 목적으로 LLM을 미세 조정하려는 다양한 최근 시도가 있었습니다. 아래에 이 분야의 연구에 대한 간략한 참조를 제공하며, 각 연구의 기여에 대한 설명이 함께 제공됩니다. (출처: [13])

**자기 보상 LLM(Self-rewarding LLM) [13]**은 LLM 평가기의 도움으로 정렬(alignment) 프로세스를 개선하고 자동화하려고 시도합니다. 이 연구의 저자들은 인간 선호도에서 보상 모델을 훈련하는 대신, LLM-as-a-Judge 스타일 프롬프트를 통해 자체 보상과 피드백을 제공하도록 LLM 자체(미세 조정되거나 정렬되는 동일한 모델)를 훈련합니다. 위에 표시됨. LLaMA-2 모델을 미세 조정하는 데 사용되었을 때, 이 접근 방식은 유용한 피드백을 생성하고 기본 LLM의 성능을 향상시킬 수 있음을 보여줍니다. Prometheus와 유사하게, 자기 보상 LLM은 LLM 심사관 역할을 하도록 훈련되지만, 이 심사관의 피드백은 LLM 자체를 미세 조정하기 위한 보상 신호로 직접 사용됩니다. (출처: [15])

**LLM-as-a-Meta-Judge**. 자기 보상 LLM은 흥미로운 접근 방식이지만, 이 기술의 한 가지 단점은 LLM 심사관에 대한 개선 메커니즘이 포함되어 있지 않다는 것입니다. [15]에서 저자들은 LLM과 LLM 심사관 모두를 위한 미세 조정 데이터를 생성함으로써 훈련 프로세스 전반에 걸쳐 LLM 심사관(LLM 자체 외에)이 스스로 개선될 수 있도록 명시적으로 허용하는 LLM-as-a-Meta-Judge 기술을 통해 이 아이디어를 확장합니다. 위에 표시됨. (출처: [14])

**자율 학습 평가기(Self-taught evaluator) [14]**는 인간 선호도 데이터 없이 모델 기반 평가기를 훈련합니다. 이러한 접근 방식은 인간 선호도 판단을 수집하는 데 비용이 많이 들고 시간이 지남에 따라 구식이 될 수 있기 때문에 유용합니다. 대안으로, [14]의 저자들은 LLM을 사용하여 합성 평가 데이터를 생성하는 반복적인 방식을 제안합니다. 명령어 세트에서 시작하여 LLM으로 대조적인 모델 출력을 생성하고, 설명과 점수를 생성하여 이러한 출력을 평가하도록 LLM 심사관을 훈련합니다. 인간이 라벨링한 데이터를 사용하지 않고도, 이 자율 학습 접근 방식은 채점 정확도 측면에서 독점 LLM 심사관(예: GPT-4)과 인간 선호도에 미세 조정된 보상 모델 모두를 능가하는 평가기를 생성하는 것으로 나타났습니다. (출처: [16])

**기반 대규모 자동 평가 모델(Foundational Large Autorater Model, FLAMe) [16]**은 방대한 양의 인간 선호도 데이터(100개 이상의 다양한 품질 평가 작업을 아우르는 5백만 개 이상의 인간 판단)에 대해 훈련된 기반 평가/심사관 모델 제품군입니다. 다른 LLM 평가기와 비교하여 FLAMe 모델은 다른 평가 작업에 잘 일반화되고, 편향이 적으며, 추가로 미세 조정될 수도 있습니다. FLAMe 모델은 전문 LLM 심사관을 미세 조정하기 위한 훌륭한 기본 모델이 될 수 있습니다. 이 모델은 GPT-4 및 Claude 변형을 기반으로 하는 독점 LLM-as-a-Judge 모델뿐만 아니라 합성적으로 생성된 데이터(예: GPT-4에서)에 미세 조정된 LLM 심사관보다 지속적으로 우수한 성능을 보입니다.

"우리의 연구 결과는 미세 조정된 심사관 모델이 도메인 내 테스트 세트에서 높은 성능을 달성하고 GPT-4를 능가하더라도, 일반화 가능성(generalizability), 공정성(fairness), 측면별 평가(aspect-specific evaluation) 및 확장성(scalability)을 포함한 여러 차원에서 GPT-4보다 성능이 떨어진다는 것을 나타냅니다." - [18]에서 발췌.

**미세 조정된 대 독점 LLM 심사관**. 미세 조정된 LLM 심사관과 독점 LLM 심사관 간의 절충점은 [18]에서 심층적으로 분석됩니다. 이 분석에서 우리는 미세 조정된 심사관이 도메인 내 평가에서 매우 잘 작동한다는 것을 알 수 있습니다. 이 모델들은 훈련된 데이터와 유사한 데이터를 평가하는 데 능숙합니다. 그러나 독점 LLM 심사관과 비교할 때, 이러한 모델은 새롭거나 다른 작업에 잘 일반화되지 않습니다. 간단히 말해, 미세 조정된 LLM 심사관은 매우 작업별(task-specific)인 경향이 있으며, 이는 반드시 놀라운 결과는 아닙니다. (출처: [19])

**직접 판단 선호도 최적화(Direct judgement preference optimization) [19]**는 선호도 최적화(preference optimization)를 사용하여 더 고급 평가 기능을 가진 LLM을 생성하려고 시도합니다. 이를 위해 저자들은 세 가지 다른 평가 사용 사례에 대한 선호도 쌍을 수집합니다: i) 단일 등급(single rating), ii) 쌍대 비교, iii) 분류. 처음 두 가지 사용 사례는 일반적인 LLM-as-a-Judge 채점 설정과 일치합니다. 분류 사용 사례는 평가를 이진 분류 문제로 공식화합니다. 예를 들어, "이 속성이 응답에서 충족되는가 아닌가?"와 같습니다. 이러한 사용 사례 외에도, 명령어와 평가 결과를 입력으로 주어졌을 때 채점되는 응답을 추론하도록 LLM을 훈련하는 보조 작업도 도입됩니다. 위에 표시됨. 이러한 각 사용 사례에 대해 우리는 긍정적 및 부정적 응답의 선호도 쌍을 얻은 다음, 직접 선호도 최적화(DPO) [20]를 사용하여 평가기를 업데이트합니다.

"우리는 LLM-as-a-Judge가 프롬프트의 매우 상세한 지시로부터 거의 이점을 얻지 못하며, 퍼플렉시티(perplexity)가 때로는 프롬프팅보다 인간의 판단과 더 잘 일치할 수 있음을 보여줍니다. 특히 텍스트 품질에 있어서는 더욱 그렇습니다." - [21]에서 발췌.

**LLM 심사관은 어떻게 점수를 할당할까요?** LLM 심사관을 사용할 때, LLM이 평가 기준에 따라 점수를 생성하는지 아니면 단순히 훈련 데이터와 고도로 정렬된 텍스트에 더 높은 점수를 할당하는지 궁금할 수 있습니다. [21]에서 저자들은 여러 채점 설정에서 LLM 심사관에 대한 엄격한 분석을 통해 이 질문에 답하는 것을 목표로 합니다. 아래를 참조하십시오. 이 분석에서 우리는 LLM-as-a-Judge 모델이 프롬프트에 상세한 채점 지시와 기준을 포함하는 것으로부터 약간의 이점만 얻는다는 것을 알 수 있습니다. 사실, 퍼플렉시티 기반 평가(perplexity-based evaluation)(모델이 할당한 가능성에 기반하여 응답에 점수를 매기는 것)는 일부 경우에 인간의 선호도와 더 잘 일치합니다. (출처: [21])

이러한 결과는 LLM 심사관의 채점 메커니즘이 훈련 데이터의 내용에 의해 좌우된다는 것을 나타내며, 이는 참조 자료가 매우 중요하다는 Prometheus [1]의 발견과 약간 모순됩니다. 그러나 [21]의 분석은 주로 인간 선호도 관점에서 응답 품질을 평가하는 데 중점을 둡니다. 대부분의 LLM이 선호도 데이터에 대해 훈련된다는 점을 고려할 때, 모델이 인간에게 선호되는 응답에 높은 확률을 할당하는 것은 합리적입니다. 대조적으로, Prometheus는 인간 선호도를 넘어선 광범위한 평가 기준을 고려하므로 평가 기준이 더 관련성이 높습니다.

**단계별 가이드: LLM 심사관 미세 조정하기**

이제 평가 목적으로 LLM을 미세 조정하는 주제에 대해 최근 수행된 거의 모든 연구를 심층적으로 살펴보았으므로, 우리만의 LLM 심사관을 생성하기 위한 일반적인 프레임워크가 필요합니다. 다행히 Prometheus는 이미 그러한 프레임워크를 제공합니다 [1]. 아래에서는 이 개요에서 지금까지 배운 내용을 요약하여 이 프레임워크를 제시합니다(약간 수정하여).

Prometheus의 Feedback Collection에서 가져온 평가 기준 예시

**(1) 평가 기준을 확고히 합니다.** 평가의 첫 번째 단계는 정확히 무엇을 평가할지 결정하는 것입니다. 특히, 우리는 다음을 수행해야 합니다:
*   우리가 중요하게 생각하는 특정 기준 세트를 개략적으로 설명합니다.
*   이러한 각 기준에 대한 상세한 설명을 작성합니다.

위에 Prometheus가 사용한 채점 루브릭의 예시가 제공되어 있으며, 여기에는 채점 기준/지침과 여러 참조 답변이 모두 포함되어 있습니다. 시간이 지남에 따라 우리는 인간 (및 모델 기반) 평가 프로세스를 더 일관성 있고 정확하게 만들기 위해 기준을 발전시키고, 다듬고, 확장해야 합니다.

**(2) 데이터셋을 준비합니다.** 평가를 위해 LLM을 훈련하거나 사용하기 전에 (고품질) 인간 평가 데이터가 필요합니다. 인간이 라벨링한 데이터는 평가기가 정확한지 여부를 쉽게 판단하는 데 도움이 됩니다. 이 데이터는 LLM 심사관이 실제 환경에서 접하게 될 데이터의 종류를 반영해야 합니다. 이 데이터를 메타 평가 목적으로만 사용하는 경우(예: LLM-as-a-Judge의 성능을 측정하기 위해), 많은 데이터가 필요하지 않습니다(예: 100-1K 예시). Prometheus 스타일 모델을 미세 조정하려면 더 많은 데이터가 필요합니다(예: 1K-100K 예시). 인간 데이터 수집은 평가 프로세스의 어렵지만 필수적인 부분입니다. 인간이 평가 작업에 일관되게 동의할 수 없다면, LLM이 작업을 정확하게 평가하는지 측정하는 것은 불가능할 것입니다. 인간 평가에 충분히 투자하지 않는 것은 흔한 실수입니다. 그러나 저품질 데이터에 대해 LLM 심사관을 개발하거나 미세 조정하는 것은 시간과 에너지 낭비입니다.

**(2.5) 합성 데이터를 사용합니다.** Prometheus 모델 [1, 2, 3]은 고품질 LLM 심사관을 훈련하기 위해 합성 데이터를 사용할 수 있음을 보여줍니다. 사실, 이 모델들을 훈련하는 데 사용된 데이터의 거의 전부는 합성적으로 생성됩니다. 대부분의 명령어, 응답, 점수 및 피드백이 그렇습니다! 순수하게 합성된 훈련 데이터를 사용하는 것은 훈련 중에 모델을 좁은 데이터 분포에만 노출시킴으로써 편향을 유발할 수 있지만, 인간 데이터와 합성 데이터를 결합하는 것은 엄청나게 효과적일 수 있습니다.

**(3) 근거에 집중합니다!** 정확한 LLM 심사관을 미세 조정하려면 모델이 훈련된 점수와 순위가 정확해야 합니다. 더 나아가, 각 점수에 대한 고품질 근거를 생성해야 합니다. LLM은 미세 조정 중에 새로운 지식을 학습하는 데 어려움을 겪습니다. 그러나 미세 조정은 LLM 심사관에게 더 효과적인 피드백 형식이나 스타일을 가르치는 데 사용될 수 있습니다. 따라서 LLM 심사관이 훈련되는 근거를 조정하는 것은 결과 모델을 더 유용하게 만드는 좋은 방법입니다.

**(4) 참조 답변을 사용합니다.** 이 단계는 선택 사항이지만, 데이터셋의 각 예시와 함께 참조 답변을 준비할 수도 있습니다. 이전에 설명했듯이, 참조 답변은 LLM-as-a-Judge와 미세 조정된 평가기 모두의 성능을 향상시킬 수 있습니다. 미세 조정된 평가기의 경우, 참조는 심사관의 기본 모델의 사전 훈련된 지식의 격차나 문제로 인해 발생하는 지식 편향(knowledge bias)을 해결하는 능력 때문에 유용합니다. 참조 답변은 점별 채점의 정확성과 신뢰성을 향상시키는 데도 유용합니다.

**(5) 모델을 훈련합니다.** 모든 데이터(및 선택적으로 참조 답변)가 수집되면, 기본적인 지도 미세 조정(SFT) 접근 방식을 사용하여 LLM 심사관을 훈련할 수 있습니다. 이 프로세스의 전체 구현은 Prometheus GitHub 저장소를 확인하십시오. 훈련 중에 모델의 성능을 메타 평가하는 데 사용할 수 있는 홀드아웃 테스트 데이터셋(hold-out test dataset)¹⁶을 생성해야 합니다. 성능은 분류(이진 출력의 경우) 또는 상관관계(다른 모든 경우) 지표를 사용하여 평가됩니다.

**결론 및 향후 전망**

LLM 평가의 여정은 복잡하지만, LLM-as-a-Judge 및 미세 조정된 전문 심사관의 등장은 이 분야에 혁신을 가져왔습니다. 인간 평가의 심층적인 통찰력과 자동 평가의 확장성을 결합하는 이 새로운 패러다임은 LLM 개발 주기를 가속화하고 모델 품질을 향상시키는 데 필수적입니다. Prometheus 시리즈와 같은 연구들은 합성 데이터, 세분화된 평가 기준, 편향 완화 기술, 그리고 다중 모달 능력 통합을 통해 LLM 심사관의 잠재력을 지속적으로 확장하고 있습니다.

향후 LLM 심사관은 더욱 정교해지고, 다양한 도메인과 작업에 걸쳐 일반화될 수 있는 능력을 갖추게 될 것입니다. 특히, 인간의 피드백을 더욱 효율적으로 활용하고, LLM 심사관 자체의 편향을 지속적으로 감지하고 수정하는 메커니즘이 중요해질 것입니다. 궁극적으로, 강력하고 신뢰할 수 있는 LLM 심사관은 LLM의 안전성, 공정성, 유용성을 보장하는 데 결정적인 역할을 하며, AI 기술의 책임감 있는 발전을 위한 초석이 될 것입니다.

뉴스레터가 처음이신가요? 안녕하세요! 저는 Cameron R. Wolfe입니다. 넷플릭스(Netflix)의 딥러닝(Deep Learning) 박사이자 머신러닝(Machine Learning) 과학자입니다. 이것은 독자들이 AI 연구의 중요한 주제를 더 잘 이해하도록 돕는 Deep (Learning) Focus 뉴스레터입니다. 뉴스레터가 마음에 드시면 구독하거나, 공유하거나, X 및 LinkedIn에서 저를 팔로우해주세요! 구독

**참고 문헌(Bibliography)**
[1] Kim, Seungone, et al. "Prometheus: 언어 모델에 평가 능력 유도(Inducing Evaluation Capability in Language Models)." NeurIPS 2023 Workshop on Instruction Tuning and Instruction Following. 2023.
[2] Kim, Seungone, et al. "Prometheus 2: 다른 언어 모델 평가에 특화된 오픈소스 언어 모델(An open source language model specialized in evaluating other language models)." arXiv preprint arXiv:2405.01535 (2024).
[3] Lee, Seongyun, et al. "Prometheusvision: 세분화된 평가를 위한 비전-언어 모델 심사관(Vision-language model as a judge for fine-grained evaluation)." arXiv preprint arXiv:2401.06591 (2024).
[4] Chen, Dongping, et al. "Mllm-as-a-judge: 비전-언어 벤치마크로 다중 모달 LLM-as-a-judge 평가(Assessing multimodal llm-as-a-judge with vision-language benchmark)." arXiv preprint arXiv:2402.04788 (2024).
[5] Bai, Shuai, et al. "Touchstone: 언어 모델로 비전-언어 모델 평가(Evaluating vision-language models by language models)." arXiv preprint arXiv:2308.16890 (2023).
[6] Wang, Yidong, et al. "PandaLM: LLM 명령어 튜닝 최적화를 위한 자동 평가 벤치마크(An Automatic Evaluation Benchmark for LLM Instruction Tuning Optimization).(2024)." URL https://arxiv. org/abs/2306.05087 3.4 (2024).
[7] Li, Junlong, et al. "생성형 심사관: 정렬 평가(Generative judge for evaluating alignment)." arXiv preprint arXiv:2310.05470 (2023).
[8] Zheng, Lianmin, et al. "MT-Bench 및 Chatbot Arena로 LLM-as-a-judge 평가(Judging llm-as-a-judge with mt-bench and chatbot arena)." Advances in Neural Information Processing Systems 36 (2024).
[9] Zhu, Lianghui, Xinggang Wang, and Xinlong Wang. "Judgelm: 미세 조정된 대규모 언어 모델은 확장 가능한 심사관이다(Fine-tuned large language models are scalable judges)." arXiv preprint arXiv:2310.17631 (2023).
[10] Seonghyeon Ye, Yongrae Jo, Doyoung Kim, Sungdong Kim, Hyeonbin Hwang, and Minjoon Seo. SelFee: 자기 피드백 생성으로 강화된 반복적 자기 수정 LLM(Iterative self-revising llm empowered by self-feedback generation). 블로그 게시물, 2023년 5월. URL https://kaistai.github.io/SelFee/.
[11] Bai, Yuntao, et al. "헌법적 AI: AI 피드백으로부터의 무해성(Constitutional ai: Harmlessness from ai feedback)." arXiv preprint arXiv:2212.08073 (2022).
[12] Zhang, Lunjun, et al. "생성형 검증기: 다음 토큰 예측으로서의 보상 모델링(Generative verifiers: Reward modeling as next-token prediction)." arXiv preprint arXiv:2408.15240 (2024).
[13] Yuan, Weizhe, et al. "자기 보상 언어 모델(Self-rewarding language models)." arXiv preprint arXiv:2401.10020 (2024).
[14] Wang, Tianlu, et al. "자율 학습 평가기(Self-taught evaluators)." arXiv preprint arXiv:2408.02666 (2024).
[15] Wu, Tianhao, et al. "메타 보상 언어 모델: LLM-as-a-meta-judge를 통한 자기 개선 정렬(Meta-rewarding language models: Self-improving alignment with llm-as-a-meta-judge)." arXiv preprint arXiv:2407.19594 (2024).
[16] Vu, Tu, et al. "기반 자동 평가기: 더 나은 자동 평가를 위한 대규모 언어 모델 길들이기(Foundational autoraters: Taming large language models for better automatic evaluation)." arXiv preprint arXiv:2407.10817 (2024).
[17] Ankner, Zachary, et al. "소리 내어 비평하는 보상 모델(Critique-out-loud reward models)." arXiv preprint arXiv:2408.11791 (2024).
[18] Huang, Hui, et al. "LLM 평가를 위한 LLM-as-a-judge의 경험적 연구: 미세 조정된 심사관 모델은 작업별 분류기이다(An empirical study of llm-as-a-judge for llm evaluation: Fine-tuned judge models are task-specific classifiers)." arXiv preprint arXiv:2403.02839 (2024).
[19] Wang, Peifeng, et al. "직접 판단 선호도 최적화(Direct judgement preference optimization)." arXiv preprint arXiv:2409.14664 (2024).
[20] Rafailov, Rafael, et al. "직접 선호도 최적화: 당신의 언어 모델은 비밀리에 보상 모델이다(Direct preference optimization: Your language model is secretly a reward model)." Advances in Neural Information Processing Systems 36 (2024).
[21] Böhm, Florian, et al. "더 나은 보상은 더 나은 요약을 낳는다: 참조 없이 요약 학습(Better rewards yield better summaries: Learning to summarise without references)." arXiv preprint arXiv:1909.01214 (2019).
[22] Ouyang, Long, et al. "인간 피드백으로 지시를 따르도록 언어 모델 훈련(Training language models to follow instructions with human feedback)." Advances in neural information processing systems 35 (2022): 27730-27744.
[23] Jha, Aditi, et al. "Limit: 평가 패러다임 전반에 걸쳐 명령어 튜닝에 있어 적을수록 좋다(Less is more for instruction tuning across evaluation paradigms)." arXiv preprint arXiv:2311.13133 (2023).

¹ 예를 들어, OpenAI는 모델 훈련 및 평가를 위해 ScaleAI의 인간 주석 작업에 크게 의존한 것으로 알려져 있습니다 [22].
² 우리는 또한 주석자와 연구자 사이에 공유된 (비동기식) 통신 채널을 만들거나 다른 것을 만들 수 있습니다. 우리는 관련 당사자 간의 논의를 촉진할 매체가 필요할 뿐입니다. 자세한 내용은 [22]의 부록 B를 참조하십시오.
³ 데이터 주석을 위한 지침 작성 방법에 대한 훌륭한 개요는 여기를 참조하십시오.
⁴ 이 두 가지 외에도 BERTScore, MoverScore, METEOR, COMET 등 더 많은 전통적인 참조 기반 지표가 존재합니다.
⁵ N-그램(N-grams)은 단순히 단어의 순차적 그룹입니다. 예를 들어, 유니그램(단일 단어), 바이그램(두 단어 시퀀스), 트라이그램(세 단어 시퀀스) 등이 있습니다.
⁶ 일부 논문에서는 저자들이 인간 대신 미세 조정된 심사관과 독점 LLM(예: GPT-4) 간의 상관관계 지표를 계산합니다.
⁷ PandaLM의 약어는 Reproducible AND Automated Language Model Assessment에서 유래합니다.
⁸ 특히, PandaLM은 순수하게 지도 미세 조정(SFT)을 사용하여 미세 조정됩니다. 쌍대 평가를 위한 전문 분류 헤드는 모델 내에 생성되지 않습니다.
⁹ "세분화된(granular)"이라는 말은 데이터셋이 모델 성능의 특정 측면을 고려하는 다양한 평가 기준을 포괄한다는 것을 의미합니다.
¹ ⁰ 훈련에 사용된 쿼리는 인간 사용자가 제공했지만, 이러한 쿼리에 대한 응답은 여전히 GPT-4로 생성됩니다. 따라서 Auto-J의 훈련 데이터는 여전히 합성적으로 생성됩니다.
¹¹ 이러한 설명은 CoT(Chain of Thought) 프롬프팅 내에서 사용되는 근거를 모방합니다. 병렬 연구는 이러한 근거에 대해 LLM을 미세 조정하는 것이 효과적인 접근 방식임을 보여주었습니다. 여기를 참조하십시오.
¹² 즉, 모델이 끝없이 피드백을 생성하거나 채점될 다음 명령어를 인위적으로 생성하기 시작하는 대신, 피드백 후에 항상 점수를 생성하도록 보장하고자 합니다.
¹³ Anthropic의 HH-RLHF 및 UltraFeedback 데이터셋이 예시입니다.
¹⁴ 이는 다중 작업 학습(multi-task learning), 명령어 튜닝(instruction tuning) 또는 기타 유사한 방법론의 변형을 통해 수행될 수 있습니다.
¹⁵ 이러한 출력은 Fuyu-8B, LLaVA-1.5 (13B), GPT-4V를 포함한 여러 다른 VLM에서 샘플링됩니다.
¹⁶ 우리는 또한 LLM 심사관의 훈련 하이퍼파라미터(hyperparameter)를 튜닝하는 데 사용할 추가 검증 데이터셋을 생성해야 합니다.