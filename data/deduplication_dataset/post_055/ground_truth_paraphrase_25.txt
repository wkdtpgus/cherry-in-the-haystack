대규모 언어 모델(LLM)의 역량이 비약적으로 확장됨에 따라, 이들의 성능을 정확하게 가늠하는 작업은 더욱 복잡해지고 있습니다. 현대의 기반 모델(foundation model)들은 광범위한 영역에 걸쳐 기능하며, 그 결과물은 종종 정해진 답이 없는(open-ended) 특성을 보입니다. 이는 특정 입력에 대해 여러 가지 타당한 결과가 존재할 수 있음을 의미합니다. 이러한 본질적 특성 때문에 LLM을 자동화된 방식으로 평가하는 것은 여전히 난해하며, 활발한 연구 주제로 남아 있습니다. 단일 기능에 대한 평가조차 쉽지 않은데, LLM은 헤아릴 수 없이 많은 기능과 행동을 지니고 있어 평가해야 할 가치가 충분합니다. (출처: [1, 2, 3, 9, 23])

"LLM이 생성하는 출력의 품질을 평가하는 것은 텍스트와 복잡한 작업의 매우 다양한 분포를 다루기 때문에 점차 어려워지고 있습니다. 이 문제를 해결하기 위해 LLM 기반 평가(LLM-based evaluation)는 LLM이 생성한 텍스트를 평가하기 위한 확장 가능하고 저렴한 패러다임으로 부상했습니다." - [2]에서 발췌.

LLM의 성능을 판단하는 가장 신뢰할 수 있는 방법은 사람이 직접 모델의 결과물을 평가하는 것이지만, 인간 평가(human evaluation)는 여러 변수가 많고, 비용이 많이 들며, 상당한 시간을 요구합니다. 일정 수준의 인간 평가는 필수적이지만, 전적으로 인간 평가에만 의존하는 것은 확장성이 떨어집니다. 우리는 새로운 LLM의 기능을 효율적으로 검증할 수 있는 방안을 모색해야 합니다. 이러한 요구는 독점 LLM(proprietary LLM)이 다른 LLM의 출력물을 평가하도록 지시하는 LLM-as-a-Judge [8]라는 개념의 등장을 촉발했습니다. LLM-as-a-Judge는 현재 연구 분야에서 널리 활용되고 있지만, 독점 LLM이 세분화된 기준(granular criteria)에 대한 평가에 익숙하지 않을 수 있는 특정 도메인별 애플리케이션(domain-specific application)에서는 그 효과가 떨어질 수 있습니다. 이러한 상황에서는 우리만의 전문 LLM 평가자를 훈련시키는 것이 필요하며, 본 개요의 목표는 이를 성공적으로 수행하는 방법에 대한 포괄적인 이해를 돕는 것입니다. 먼저 LLM을 평가하는 다양한 접근 방식에 대해 폭넓게 살펴보겠습니다.

**대규모 언어 모델(LLM) 평가 접근법**

자동화된 측정 기준(automatic metric)은 인간 평가의 (불완전한) 대리 지표(proxy) 역할을 수행하며, 인간 평가 과정 사이에서 더 많고 빠른 모델 개선(model iteration)을 가능하게 합니다. LLM을 평가하기 위해서는 인간 평가와 자동화된 지표를 통합하여 활용해야 합니다. 위에서 언급했듯이, 인간 평가는 모델 성능의 궁극적인 진실의 원천으로 기능하지만, 동시에 엄청난 노력을 수반합니다. 모델 개발 속도를 가속화하기 위해 우리는 더욱 효율적으로 측정 가능한 자동화된 지표에 의존해야 하며, 이를 통해 모델을 더 빠른 주기로 훈련하고 평가할 수 있습니다. 자동화된 지표는 인간의 판단을 완벽하게 대체할 수 없으므로, 우리는 인간 평가를 통해 모델의 성능을 지속적으로 감독해야 합니다. 그러나 우리는 각 인간 평가 주기 사이에 훨씬 더 많은 수의 모델을 테스트하기 위해 자동화된 지표를 활용할 수 있습니다.

"기존 벤치마크(benchmark)와 전통적인 지표는 개방형(open-ended) 시나리오에서 LLM의 기능을 적절하게 추정하지 못합니다. 개방형 작업에서 LLM을 포괄적으로 평가할 수 있는 새로운 벤치마크 방법이 필요합니다." - [9]에서 발췌.

**자동 평가 기법의 종류**. 대부분의 현대 LLM에 적용되는 두 가지 주요 자동 평가 전략(아래 참조)이 있습니다: 퍼플렉시티(perplexity)와 모델 기반 평가(model-based evaluation). 퍼플렉시티 기반 평가(perplexity-based evaluation)는 MMLU 또는 BIG-bench와 같은 대부분의 전통적인 자연어 처리(NLP) 벤치마크를 포함합니다. 이러한 벤치마크는 LLM의 일반 지식을 검증하는 객관식 질문 형태로 이해할 수 있습니다. 그러나 이러한 벤치마크는 LLM이 더 길고 형식적인 답변을 생성해야 하는 개방형 환경에서 LLM을 평가할 때 한계점을 드러냅니다. (출처: [23])

더 길고 정해진 답이 없는(open-ended) 결과물을 평가하기 위해 두 가지 주요 선택지가 있습니다:
*   **인간 평가(Human evaluation)**: 모델이 생성한 결과물을 사람이 직접 판단합니다.
*   **모델 기반 평가(Model-based evaluation)**: 강력한 LLM을 활용하여 다른 모델의 결과물을 평가합니다.

인간 평가의 내재된 한계 때문에, LLM-as-a-Judge [8]와 같은 모델 기반 평가 전략이 LLM을 평가하는 데 있어 주된 접근 방식으로 자리매김했습니다. 이러한 모델 기반 평가 기술은 별도의 참조 자료를 요구하지 않으며, 구현이 용이하고, 다양한 개방형 작업을 처리할 수 있습니다.

이제 우리는 인간 평가와 자동 평가를 포함한 기본적인 평가 범주를 이해했습니다. 또한 자동 평가를 수행하는 여러 가지 방법(즉, 벤치마크 또는 모델 기반 평가)이 있다는 것도 파악했습니다. 따라서 인간 평가부터 시작하여 다양한 자동 평가 기술로 넘어가면서 이러한 각 평가 전략을 더 자세히 살펴보겠습니다.

**인간 평가(Human Evaluation)**

"인간 평가는 텍스트의 미묘하고 주관적인 측면을 평가하는 본질적인 신뢰성과 능력 때문에 지속적으로 지배적인 방법이었습니다. 많은 상황에서 인간은 간결성, 창의성, 어조, 문화적 민감성 등 평가의 가장 중요한 요소를 자연스럽게 식별할 수 있습니다." - [1]에서 발췌.

인간 평가는 LLM을 평가함에 있어 궁극적인 진실의 기준점입니다. 이 평가를 수행하는 사람들, 즉 인간 평가자(human evaluator) 또는 주석자(annotator)는 다양한 방식으로 확보될 수 있습니다. 예를 들어, 우리는 크라우드소싱(crowdsource)을 통해 사람들을 모집하거나, 직접 고용하거나¹, 심지어 모델 개발자인 우리 자신이 LLM의 결과물을 평가할 수도 있습니다. 인간 평가가 모델 품질의 진실된 척도일지라도, 이것이 인간 평가가 완벽하다는 것을 의미하지는 않습니다. 사실, 실제로는 정반대인 경우가 많습니다. 인간 평가는 잡음이 많고, 복잡하며, 편향되기 쉽습니다.

**인간 평가 (또는 주석) 과정**

**합의 및 보정(Agreement and calibration)**. 겉보기에 주관적인 작업에서도 일치하는 평가를 단순히 세거나 코헨의 카파(Cohen's Kappa), 플라이스의 카파(Fleiss' Kappa), 크리펜도르프의 알파(Krippendorff's Alpha)와 같은 지표를 사용하여 측정되는 인간 합의는 낮게 나타날 수 있습니다. 따라서 우리는 인간 주석자를 "보정(calibrating)"하는 데 노력을 기울여야 합니다(즉, 특정 작업을 일관되고 정확하게 평가하는 방법을 교육하는 것). 위를 참조하십시오. 보정은 일반적으로 인간 주석자들 간의 회의²를 통해 의견 불일치를 논의하고 해결하는 것을 포함합니다. 그런 다음, 이러한 논의 결과는 작업에 대한 평가 지침에 통합될 수 있습니다.

**지침 마련(Crafting guidelines)**. 인간이 특정 작업을 어떻게 평가하거나 주석을 달아야 하는지 문서화하기 위해, 우리는 다음을 명확히 설명하는 일련의 서면 지침을 작성해야 합니다:
*   정확히 무엇을 평가하려는 목표인지(즉, 평가 기준).
*   이러한 기준을 적절하게 평가하는 방법론.

서면 지침과 올바르거나 잘못된 주석의 구체적인 예시³로 구성된 이 지침은 인간이 주석 프로세스 전반에 걸쳐 지속적으로 참고할 수 있는 상세한 자료입니다. 서면 지침은 주석 프로세스의 일관성을 높이고, 새로운 주석자를 온보딩하는 작업을 간소화합니다. 주석 작업을 올바르게 수행하는 데 필요한 모든 정보는 이 지침 내에 명확하게 기술되어야 합니다.

"우리가 라벨러(labeler)에게 제공한 지침은 프로젝트가 진행됨에 따라 진화했습니다. 피드백을 제공하고, 메타데이터 필드를 변경하며, 측정하고자 하는 바에 대한 더 나은 이해를 발전시켰습니다. 또한 혼란스럽거나 일관성이 없는 지침은 수정했습니다." - [22]에서 발췌.

이러한 지침은 고정된 것이 아닙니다. 오히려 우리는 인간 주석자와 협력하고 더 높은 수준의 합의를 달성하기 위해 시간이 지남에 따라 지속적으로 업데이트할 수 있습니다. 대부분의 프로젝트에서 우리가 평가하려는 대상에 대한 이해는 시간이 흐를수록 더욱 명확하고 상세해집니다. 특정 평가 작업이 간단하거나 명확하다고 생각할 수 있지만, 대부분의 주석 작업은 본질적으로 놀라울 정도로 많은 주관성을 내포하고 있습니다. 이러한 주관성은 우리가 한 그룹의 인간이 주어진 작업에 일관되게 동의하도록 시도할 때만 명확하게 드러납니다.

**지속적인 감독(Continuous monitoring)**. 일반적으로 우리는 평가 작업을 위한 지침을 작성하고 인간 평가자를 보정하는 데 많은 초기 노력을 투자합니다. 일단 우리가 설정한 평가 기준에 만족하고 이러한 기준을 평가할 때 합리적인 수준의 합의에 도달하면, 우리는 인간 평가 결과에 더 큰 확신을 가질 수 있으며 이 결과를 LLM 성능의 척도로 사용하기 시작할 수 있습니다. 그러나 우리는 시간이 지남에 따라 인간 평가자들 간의 합의를 계속 측정하여 저하나 편류가 없는지 확인해야 합니다. 합의가 감소할 수 있는 몇 가지 이유가 있습니다. 새로운 사람이 평가자 그룹에 합류할 수도 있고, 평가자들이 시간이 지남에 따라 지침을 잊어버릴 수도 있으며, 심지어 지침 자체가 변경될 수도 있습니다! 인간 평가가 일관되고 정확하도록 보장하는 것은 끝없는(그러나 극히 필요한) 노력입니다.

**전통적인 (자동) 지표(Traditional (Automatic) Metrics)**

이전 세대의 연구에서는 ROUGE(요약용) 또는 BLEU(번역용)⁴와 같은 전통적인 (자동) 지표를 사용하여 언어 모델의 성능을 평가할 수 있었습니다. 존재하는 가장 간단한 자동 평가 기술 중 일부인 이러한 지표는 참조 기반(reference-based)입니다. 즉, LLM의 출력물을 어떤 "황금" 참조 답변("golden" reference answer)과 비교하여 작동합니다. 일반적으로 일치하는 N-그램(n-gram)⁵의 개수를 세는 방식으로 이루어집니다. 모델의 결과물이 참조 답변과 유사하면 점수가 좋고, 그 반대도 마찬가지입니다. 아래를 참조하십시오.

**BLEU 및 ROUGE의 정의**

지난 몇 년 동안 언어 모델의 역량이 빠르게 진화함에 따라 이러한 전통적인 지표들은 점차 그 효과가 떨어졌습니다. 최근 연구에 따르면 이러한 지표들은 LLM의 결과물을 평가할 때 인간의 선호도와 낮은 상관관계를 보인다는 것이 밝혀졌습니다 [21]. 아래를 참조하십시오. 따라서 전통적인 지표들은 LLM을 평가하는 데 사용될 때 그 효용성이 감소하지만, 계산 비용이 매우 저렴하고 구현이 간단하기 때문에 여전히 비교적 흔하게 사용됩니다. 참조 답변에 접근할 수 있다고 가정하면, 이러한 지표들을 다양한 사용 사례에 대한 빠르고 간단한 건전성 검사(sanity check)로 활용할 수 있습니다.

"우리가 고려하는 모든 [전통적인] 지표가 인간의 판단과 낮은 상관관계를 보인다는 것을 발견했습니다." - [21]에서 발췌.

**전통적인 지표의 한계**. 전통적인 지표가 인간의 선호도와 낮은 상관관계를 보이는 몇 가지 이유가 있지만, 가장 중요한 문제는 이러한 지표의 대부분이 참조 기반이라는 점입니다. 이들은 모델의 결과물을 우리가 일치시키고자 하는 정답(ground truth answer)과 비교하여 작동합니다. 기본적으로, 이는 '퍼지 매칭(fuzzy matching)'의 더 정교한 버전이라고 할 수 있습니다. 그러나 현대 LLM은 엄청나게 개방형(open-ended)입니다! 단일 프롬프트(prompt)가 주어졌을 때, LLM이 생성할 수 있는 동등하게 유효한 응답이 많이 존재합니다. 따라서 단일 참조에만 의존하여 평가하는 방식은 가능한 유효한 응답의 스펙트럼을 포착하지 못하는 한계가 있습니다.

**LLM-as-a-Judge** (출처: [8])

전통적인 평가 지표의 단점은 더 유연하고 일반적인 평가 기술의 개발로 이어졌습니다. 이러한 기술 중 가장 인기 있는 것 중 하나는 언어 모델에 프롬프트를 제공하여 평가를 수행하는 LLM-as-a-Judge [8]라는 모델 기반 평가 전략입니다. 더 구체적으로 말하면, 우리는 강력한 LLM을 "심사관"으로 삼아 다른 LLM의 결과물을 평가합니다. [8]에서 보여주듯이, LLM-as-a-Judge는 인간의 선호도와 높은 상관관계를 가지며, 구현하기 쉽고(즉, 프롬프트만 작성하면 됩니다!), 개방형 출력물을 처리할 수 있으며, 다양한 평가 기준을 포착할 만큼 충분히 유연합니다.

"LLM-as-a-Judge는 인간의 선호도를 근사화하는 확장 가능하고 설명 가능한 방법이며, 그렇지 않으면 얻기 매우 비쌉니다." - [8]에서 발췌.

이 게시물에서는 "LLM-as-a-Judge"라는 용어를 평가 목적으로 상용 독점 LLM(예: GPT-4o 또는 Gemini-1.5)에 프롬프트를 제공하는 것을 특별히 지칭하는 데 사용할 것입니다(즉, 자체 전문 LLM 심사관을 미세 조정(finetuning)하는 것과 반대). 여기서는 LLM-as-a-Judge에 대한 몇 가지 기본 개념을 다룰 것이지만, 이 주제에 대해 작성된 다양한 유용한 게시물과 논문도 있습니다(이 뉴스레터의 이전 게시물 포함):
*   평가를 위한 LLM 사용 [링크]
*   비즈니스 성과를 이끄는 LLM-as-a-Judge 만들기 [링크]
*   LLM 평가기의 효과 평가 [링크]
*   LLM-as-a-Judge에 대한 설문조사 [링크]

(출처: [8]) **채점 구성(Scoring setups)**. LLM-as-a-Judge로 모델 결과물을 평가할 수 있는 두 가지 주요 방식이 있습니다(프롬프트는 위에 표시됨):
*   **쌍대(Pairwise)**: 심사관에게 프롬프트와 두 가지 응답이 제시된 다음, 더 나은 응답을 선택하도록 요청합니다.
*   **점별(Pointwise)**: 심사관에게 단일 프롬프트와 응답이 제시된 다음, 응답에 점수를 매기도록 요청합니다. 예를 들어, 1-5 리커트 척도(Likert scale)를 사용합니다.

점별 채점 설정을 지칭하는 데 사용될 수 있는 다른 용어로는 직접 평가(direct assessment) 또는 단일 응답 채점(single-response grading) 등이 있습니다. 이 두 가지 채점 설정 외에도, 점수를 요청할 때 LLM 심사관의 프롬프트에 참조를 포함하는 참조 기반 채점 설정(reference-based scoring setup)도 볼 수 있습니다. 참조 기반 채점(reference-guided grading)은 두 가지 채점 설정 모두에 적용될 수 있습니다. 예시는 아래를 참조하십시오.

(출처: [8]) LLM-as-a-Judge의 공개 구현을 보려면 AlpacaEval을 확인하십시오. 이 널리 사용되는 리더보드(leaderboard)는 LLM-as-a-Judge를 사용하여 LLM 출력에 대한 인간 선호도 점수를 예측합니다. 평가에 사용된 모든 프롬프트(리더보드의 현재 및 이전 반복 모두 포함)는 공개적으로 공유됩니다.

**어떤 평가 방식을 선택해야 할까요?** 일반적으로 LLM-as-a-Judge에 가장 적합한 단일 채점 설정은 존재하지 않습니다. 최적의 채점 설정 선택은 일반적으로 애플리케이션의 특정 요구사항에 따라 달라집니다. 쌍대 채점(pairwise scoring)은 점별 채점(pointwise scoring)에 비해 더 안정적인 경향이 있지만, 위치 편향(position bias)에 취약하고 확장성이 떨어집니다. 쌍대 채점으로는 모델 출력 쌍을 상대적인 방식으로만 채점할 수 있는 반면, 점별 채점은 더 다재다능하며 모델 출력에 단순히 단일 점수를 할당할 수 있게 해줍니다. 그러나 점별 채점은 심사관에게 더 어려운 작업입니다. 이는 LLM 심사관이 다른 출력과 비교하는 대신 내부 지식에만 기반하여 점수를 할당할 수 있어야 하기 때문입니다. 이 문제를 덜 두드러지게 만들기 위해 선택적으로 참조와 함께 점별 채점을 수행할 수 있습니다.

"충실도(faithfulness) 또는 지시 따르기(instruction-following) 평가와 같은 일부 평가 작업은 쌍대 비교 패러다임에 맞지 않습니다. 예를 들어, 응답은 제공된 맥락에 충실하거나 그렇지 않습니다. 대안보다 더 충실하다고 응답을 평가하는 것은 평가 기준을 다루는 것입니다." - Eugene Yan

객관적 기준(objective criteria)(예: 사실성(factuality))은 모델 결과물이 이러한 기준을 충족하거나 충족하지 않는 경향이 있기 때문에 쌍대 방식으로 평가하기 어려운 경우가 많습니다. 객관적 기준은 일반적으로 본질적으로 이진적(binary)입니다. 주어진 모델 결과물이 사실인지 아닌지는 평가할 수 있지만, 한 모델 결과물이 다른 모델 결과물보다 더 사실적인지 평가하는 것은 다소 모호합니다. 대조적으로, 주관적 평가 기준(subjective evaluation criteria)은 상대적 채점(relative scoring)이 더 잘 정의되고 안정적이며 신뢰할 수 있다는 사실 때문에 쌍대 채점 설정을 통해 더 잘 처리됩니다.

**특화된 평가자(Specialized judges)**. LLM-as-a-Judge는 엄청나게 효과적이고 널리 사용되는 접근 방식이지만, 이 기술에는 몇 가지 한계가 있습니다:
*   LLM은 투명하지 않으며 보안 문제가 있습니다.
*   우리는 평가자의 버전 관리(versioning)를 제어할 수 없습니다. 누군가 모델을 업데이트할 수 있으며(그 결과 우리의 평가가 망가질 수 있습니다).
*   LLM 심사관에 대한 모든 호출은 비용이 발생하므로, 대규모로 모델 출력물을 평가하는 경우 비용이 문제가 될 수 있습니다.
*   독점 LLM 심사관은 훈련 데이터와 고도로 정렬된 작업(예: 인간 선호도 점수 예측)에서 가장 잘 작동합니다.
*   독점 LLM은 일반적이며(즉, 평가를 수행하도록 전문화되지 않음) 강력한 점수나 의견을 제공하는 것을 피하는 경향이 있습니다.

이러한 한계의 대부분은 우리가 거의 통제할 수 없는 독점 LLM을 사용하기 때문에 발생합니다. 따라서 우리는 궁금할 수 있습니다: 우리만의 LLM 심사관을 훈련할 수 없을까요? 이 개요의 제목에서 추론할 수 있듯이, 답은 '예'입니다! 우리만의 LLM 심사관을 미세 조정하는 것은 더 세분화되고 정확하며 비판적인 피드백을 제공할 수 있는 도메인별 평가 모델(domain-specific evaluation model)을 만드는 좋은 방법입니다.

**메타 평가(Meta-Evaluation): 평가 모델의 평가**

우리만의 LLM 심사관을 훈련하는 방법을 배우기 전에, 심사관이 얼마나 잘 수행하는지 어떻게 판단할 것인지 알아야 합니다. LLM 심사관을 평가하려면 먼저 인간 평가 데이터 세트를 수집해야 합니다. 이 데이터는 우리 평가 모델의 품질을 측정하는 데 사용될 것이므로, 정확하고 신뢰할 수 있다고 매우 확신해야 합니다. 인간이 주석을 단 고품질 데이터 세트를 확보하면, 평가기의 결과물을 인간 평가 결과와 단순히 비교하여 메타 평가(meta-evaluation)(즉, 평가기 평가)를 수행할 수 있습니다. 평가기가 이진 출력(binary output)(예: 쌍대 채점 또는 이진 척도를 사용한 단일 응답 채점)을 생성하는 경우, 단순히 분류 지표(classification metric)를 사용할 수 있습니다. 분류 지표는 해석하기 매우 쉽기 때문에 많은 실무자들은 LLM-as-a-Judge에 이진 채점을 고수하는 것이 가장 좋다고 주장했습니다.

"백본 모델(backbone model)로 GPT-4를 사용하는 G-Eval이 요약 작업에서 인간과 0.514의 스피어만 상관계수(Spearman correlation)를 달성하여 이전의 모든 방법을 큰 차이로 능가한다는 것을 보여줍니다." - [23]에서 발췌.

이진적이지 않은 채점 설정(예: 1-5 리커트 척도를 사용한 단일 응답 채점)의 경우, 인간과 자동 평가 점수⁶ 간의 상관관계(correlation)를 측정해야 합니다. 최근 LLM-as-a-Judge 논문에서 사용된 상관관계 지표(correlation metric)의 예시가 위에 제공되어 있습니다. 스피어만 상관계수는 아마도 가장 일반적으로 사용되는 상관관계 지표이지만, 코헨의 카파(Cohen's kappa), 켄달의 타우(Kendall's tau), 피어슨 상관계수(Pearson correlation) 등 다른 많은 지표도 존재합니다. 불행히도 이러한 지표는 분류 지표보다 해석하기 어렵기 때문에, 우리가 사용하는 특정 상관관계 지표의 미묘한 차이에 항상 익숙해야 합니다.

**미세 조정된 평가자 연구의 초기 동향(Early Research on Finetuned Judges)**

초기에는 대부분의 실무자들이 LLM-as-a-Judge 스타일 평가를 위해 독점 LLM에 크게 의존했습니다. 왜 그랬을까요? 특정 작업에 대한 모델의 결과물을 평가하기 위해 평가자는(입력으로 참조 답변이 제공되지 않는 한) 해당 작업을 스스로 해결할 수 있어야 합니다. 오픈소스 LLM(open-source LLM)의 역량은 한동안 독점 모델에 뒤처져 있었기 때문에, 폐쇄형 모델이 일반적으로 평가 목적에 더 나은 선택이었습니다. 그러나 이 섹션에서 보듯이, 더 유능한 오픈소스 LLM(예: LLaMA 및 LLaMA-2)이 출시되면서 연구자들은 전문 LLM 심사관을 미세 조정하기 시작했습니다.

**LLM-as-a-Judge에 미세 조정 적용 [8]**

"미세 조정된 Vicuna-13B 모델은 비싼 폐쇄형 LLM을 대체할 저렴한 오픈소스 대안으로 사용될 강력한 잠재력을 보여줍니다." - [8]에서 발췌.

미세 조정된 LLM 심사관의 생성을 탐구한 최초의 논문 중 하나는 사실 원래의 LLM-as-a-Judge 논문이었습니다! 이 논문의 대부분은 일련의 일반 프롬프트를 통해 인간 선호도를 평가하기 위해 독점 LLM을 활용하는 데 중점을 둡니다. 자세한 내용은 여기를 참조하십시오. 그러나 저자들은 사용자 정의 LLM 심사관을 미세 조정할 잠재력을 언급하고 이 방향으로 초기 실험을 수행합니다.

**미세 조정의 필요성**. LLM-as-a-Judge는 유용한 기술이지만, [8]에서 보듯이 기성 모델을 사용하는 대신 자체 LLM 심사관을 미세 조정해야 하는 몇 가지 분명한 동기가 있습니다:
*   API 기반 평가는 비용이 많이 들 수 있습니다.
*   독점 모델을 사용하면 제어권이나 투명성(transparency)이 없습니다.
*   오픈소스 모델은 시간이 지남에 따라 더 유능해지고 있습니다.

이러한 이유로, 미세 조정된 심사관이 독점 모델의 성능과 일치할 수 있다면 전문 평가기는 유망한 연구 방향입니다.

**미세 조정된 평가자의 가능성**. [8]에서 생성된 미세 조정된 평가기는 LLaMA의 파생 모델인 Vicuna-13B를 기반으로 합니다. 미세 조정 없이 이 모델은 형편없는 심사관으로 밝혀졌습니다. 기본 모델은 높은 오류율을 보였고, 평가를 위해 제공된 템플릿이나 지시를 따르는 데 어려움을 겪었으며, 심각한 위치 편향(position bias)에 시달렸습니다. (출처: [8])

그러나 Chatbot Arena의 인간 투표를 통해 미세 조정함으로써 모델의 평가 기능이 크게 향상되었습니다. 저자들은 다양한 LLM의 출력물을 비교하는 2만 개의 단일 턴 투표 데이터로 모델을 훈련했습니다. 평가 프로세스를 단순화하기 위해 우리는 평가를 LLM에 대한 3방향 분류 문제(즉, 승리, 패배 또는 무승부)로 공식화했습니다. 미세 조정 후, 미세 조정된 심사관의 다음과 같은 속성이 관찰되었습니다:
*   Vicuna의 위치 편향이 크게 감소했습니다. 위에 표시됨.
*   모델의 채점 정확도가 훨씬 향상되었습니다.
*   미세 조정된 LLM 심사관은 여전히 GPT-4의 성능에 미치지 못합니다.

[8]에서의 미세 조정 모델 탐구는 최소한입니다(즉, 부록에서 한 페이지에 불과함). 그러나 초기 결과는 오픈 평가 모델의 이점을 고려할 때 이 방향으로의 추가 작업을 고무하기에 충분히 유망했습니다.

**PandaLM: LLM 명령어 튜닝 최적화를 위한 자동 평가 벤치마크 [6]**

LLM 심사관의 가장 일반적인 용도 중 하나는 여러 모델 중에서 가장 우수한 모델을 식별하는 것입니다(즉, 자동 평가). LLM 간의 성능 차이를 식별하는 것은 어려운 일이며, 특히 비교 대상 모델들이 모두 고품질일 경우 더욱 그렇습니다. 우리는 모델에 대한 피드백을 제공하기 위해 인간에게 의존할 수 있지만, 이 과정은 느립니다. 모델 성능 개선 사항을 빠르고 정확하게 식별할 수 있는 심사관을 갖는 것은 매우 유용합니다.

"목표는 PandaLM이 객관적인 응답 정확성을 우선시할 뿐만 아니라 상대적 간결성, 명확성, 포괄성, 형식성, 지시 준수와 같은 중요한 주관적 측면을 강조하도록 하는 것입니다." - [6]에서 발췌.

[6]에서 저자들은 그룹 내에서 가장 성능이 좋은 모델을 식별하는 데 사용될 수 있는 전문 평가기 LLM인 PandaLM⁷을 제안합니다. 이 모델은 명확성, 간결성, 포괄성과 같은 주관적 기준을 다루어 응답의 기본 속성(예: 정확성)을 평가하는 것을 넘어섭니다. PandaLM의 주요 동기는 LLM에 대한 하이퍼파라미터 튜닝(hyperparameter tuning) 프로세스를 더 잘 자동화하는 것입니다. 이 모델을 사용하면 다양한 하이퍼파라미터로 훈련된 여러 모델을 쉽게 비교하고 최상의 모델 또는 설정을 식별할 수 있습니다. [6]에서 PandaLM이 최적의 훈련 설정을 식별하는 데 사용될 때 모든 모델이 더 잘 수행된다는 것을 알 수 있습니다. 이러한 발견은 PandaLM이 LLM의 성능을 향상시키는 데 사용될 수 있는 여러 가지 방법이 있음을 보여줍니다.

**PandaLM 훈련 과정**. 이 모델을 훈련하기 위해 저자들은 30만 개 이상의 훈련 예시로 구성된 사용자 정의 평가 데이터셋을 생성합니다. 이 데이터셋의 각 예시는 (명령어, 입력, 응답1, 응답2) 입력 튜플과 (평가 결과, 평가 근거) 출력 튜플로 구성됩니다. 모든 데이터셋 예시에 대한 참조 답변이 제공되며, 평가 결과는 단순히 응답 중 하나를 더 좋다고 선택하거나 무승부를 선언합니다. 저자들은 이 데이터셋에서 여러 크기의 LLaMA⁸ 모델을 미세 조정하여 PandaLM을 생성합니다.

"PandaLM의 미세 조정 단계에서 우리는 다음 토큰 예측(next token prediction)을 목표로 하는 표준 교차 엔트로피 손실(cross-entropy loss)을 사용합니다. 이 모델은 별도의 분류 헤드(classification head) 없이 시퀀스-투-시퀀스 패러다임(sequence-to-sequence paradigm)으로 작동합니다." - [6]에서 발췌.

이 데이터셋 내의 모든 명령어와 입력은 Alpaca의 훈련 데이터셋에서 샘플링됩니다. LLaMA-7B, BLOOM-7B, Cerebras-GPT-7B, OPT-7B, Pythia 등 여러 모델이 응답 쌍을 생성하는 데 사용됩니다. 이러한 쌍에 대한 해당 평가 결과와 근거를 생성하기 위해 Self-Instruct와 GPT-4를 기반으로 하는 합성 접근 방식(synthetic approach)이 사용됩니다. 훈련 데이터의 품질을 보장하기 위해 여러 휴리스틱(heuristic)(예: 수작업 규칙 및 유효하지 않거나 불안정한 평가 필터링)이 사용됩니다. 메타 평가(meta-evaluation) 목적을 위해 인간 주석자 그룹으로부터 "황금" 테스트 예시("golden" test example)의 더 작은 테스트 세트가 수집됩니다. (출처: [6])

**PandaLM의 실제 적용**. PandaLM을 다른 평가기와 비교했을 때, 이러한 모델들, 특히 GPT-3.5와 GPT-4는 선호도에서 유사한 경향을 보인다는 것을 알 수 있습니다. 더 구체적으로 말하면, 이 모든 모델은 평가 중에 다른 모델들의 일관된 순위를 출력합니다. 위를 참조하십시오. PandaLM을 독점 모델과 직접 비교했을 때, i) PandaLM-7B가 경쟁력 있는 결과를 생성하고 ii) PandaLM-70B가 GPT-4의 평가 성능을 능가한다는 것을 알 수 있습니다. 아래를 참조하십시오. 이러한 결과는 모델 크기 증가가 평가 기능에 도움이 된다는 것을 나타냅니다. (출처: [6])

PandaLM 모델, 특히 더 큰 모델은 법률 또는 생물학적 환경과 같은 전문 분야에서도 매우 효과적입니다. PandaLM은 평가 데이터에 미세 조정되었기 때문에, 이 모델은 일반적인 평가 설정뿐만 아니라 훈련 데이터와 잘 일치하는 전문화되거나 도메인별 평가 설정에서도 뛰어납니다. 예를 들어, PandaLM은 [6]에서 LSAT 질문 및 생물의학 질의응답(QA) 데이터셋에 대한 응답을 성공적으로 채점하는 데 사용됩니다. 아래를 참조하십시오. (출처: [6])

**JudgeLM: 미세 조정된 대규모 언어 모델은 확장 가능한 심사관이다 [9]** (출처: [9])

미세 조정된 LLM 심사관에 대한 초기 연구의 대부분은 결과 심사관 모델의 품질에 가장 크게 기여하는 요소를 분석하는 데 거의 노력을 기울이지 않았습니다. [9]에서 저자들은 맞춤형 JudgeLM 모델을 훈련할 때 다양한 중요한 성능 요소를 탐구함으로써 이 문제를 해결하는 것을 목표로 합니다:
*   훈련 데이터의 양, 품질 및 다양성.
*   기본 모델의 품질 및 크기.
*   LLM 심사관 출력에 대한 편향의 영향.
*   다양한 채점 설정(예: 다중 턴 채팅, 단일 답변 채점, 쌍대 순위, 다중 모달 모델 등)으로 일반화할 수 있는 능력.

미세 조정된 LLM 심사관이 평가 기능 측면에서 독점 모델에 미치지 못하는 많은 이유가 있습니다. 예를 들어, 기본 모델의 크기와 품질, 데이터 품질 문제, 편향 등이 있습니다. 그러나 [9]에서 우리는 효과적인 미세 조정 심사관을 만들기 위해 이러한 문제를 실제로 해결할 수 있다는 것을 알 수 있습니다.

고품질 데이터는 [9]에서 성공의 주요 열쇠 중 하나입니다. 효과적인 미세 조정 심사관을 만들기 위해서는 고품질의 세분화된⁹ 다양성 있는 데이터로 구성된 대규모 데이터셋이 있어야 합니다. 이 방향으로 나아가면서, 이 논문의 저자들은 "[그들이] 소개하는 데이터셋은 가장 다양하고 고품질의 데이터셋"이라고 주장합니다. 이 데이터셋을 구축하기 위해 우리는 ShareGPT 및 Alpaca-GPT-4 데이터셋과 같은 다양한 공개 소스에서 10만 5천 개의 시드 명령어(seed instruction)를 선택하는 것으로 시작합니다. (출처: [9])

이러한 시드 작업에 대한 답변은 Alpaca, Vicuna, LLAMA와 같은 다양한 LLM에서 합성적으로 생성됩니다. LLM이 생성한 답변과 원본 데이터셋의 원본 답변을 결합하여 데이터셋의 각 명령어에 대한 여러 답변 세트를 얻습니다. 또한, 원본 데이터셋에서 각 명령어에 대한 참조 답변에 접근할 수 있습니다. 여기에서 각 명령어에 대한 응답 쌍을 무작위로 샘플링하고, GPT-4를 사용하여 각 쌍에 대한 점수와 근거를 생성할 수 있습니다. [9]에서 GPT-4는 JudgeLM의 강력한 "교사" 모델 역할을 합니다. 미세 조정된 심사관은 GPT-4의 평가 결과에 대해 훈련됩니다. 참조를 사용하거나 사용하지 않는 템플릿을 포함하여 GPT-4로 점수와 근거를 생성하는 데 사용된 템플릿은 위에 표시되어 있습니다. 이러한 프롬프트는 비교적 표준적이며 대부분의 공개 LLM-as-a-Judge 설정에서 볼 수 있는 것과 크게 일치합니다. 그러나 GPT-4는 점수와 설명을 생성할 때 비교적 상세한 평가 기준을 제공받으며, 이는 모델이 응답을 더 세분화된 수준에서 평가하도록 장려합니다. 이러한 세분화된 평가 기능은 이 데이터를 미세 조정함으로써 JudgeLM으로 이전됩니다.

**모델 구축**. [9]에서는 7B, 13B, 33B 매개변수 모델을 포함한 여러 크기의 JudgeLM 모델이 3.5K에서 100K 예시 범위의 데이터셋에 대해 훈련됩니다. JudgeLM은 지도 미세 조정(SFT)을 통한 명령어 튜닝(instruction tuning) 접근 방식을 사용하여 훈련됩니다. 모델의 입력 및 출력 구조는 아래 그림에 묘사되어 있습니다. 입력으로 우리는 단일 명령어에 대한 두 가지 다른 응답과 이 명령어에 대한 참조 답변을 제공합니다. 그런 다음 모델은 각 응답에 대해 별도의 점수를 생성하고 생성된 점수에 대한 근거를 제공합니다. 각 답변에 할당된 점수를 단순히 서로 비교함으로써 쌍대 결과로 변환할 수 있습니다. (출처: [9])

**편향 완화(Mitigating bias)**는 JudgeLM 훈련의 주요 초점입니다. 더 구체적으로 말하면, 저자들이 평가기에서 해결하고자 노력하는 세 가지 유형의 편향이 있습니다:
*   **위치 편향(Position bias)**: 이것은 우리가 이전에 보았던 것과 동일한 위치 편향으로, 심사관이 프롬프트 내의 위치 때문에 한 답변을 다른 답변보다 선호하는 경우입니다.
*   **지식 편향(Knowledge bias)**: 이 형태의 편향은 모델이 응답을 평가하는 데 필요한 사전 훈련(pretraining) 지식이 부족할 때 발생합니다.
*   **형식 편향(Format bias)**: 이 형태의 편향은 모델이 특정 형식의 프롬프트(예: 모델이 미세 조정된 형식)가 제공될 때만 잘 수행될 때 발생합니다.

편향에 대처하기 위한 여러 전략이 존재합니다. 예를 들어, 형식 편향은 미세 조정 중에 LLM 심사관을 여러 프롬프트 형식에 노출시킴으로써 피할 수 있으며, 지식 편향은 단순히 참조 답변을 입력으로 제공함으로써 해결할 수 있습니다. 위치 편향을 해결하기 위해 훈련 및 추론 중에 위치 전환(position switching)을 사용할 수 있습니다. 즉, 모든 가능한 위치에 답변을 두고 평가를 반복하는 것입니다. 일반적으로 위치 전환 후 다르게 순위가 매겨진 응답은 무승부로 간주합니다. 미세 조정 중에 위치를 바꾸는 것은 JudgeLM이 위치보다는 답변의 내용에 더 많은 주의를 기울이도록 강제하며, 추론 중에 동일한 트릭을 사용하면 모델 출력에 대한 위치 편향의 영향을 최소화할 수 있습니다. 함께 사용될 때, 이러한 트릭은 JudgeLM 모델의 편향을 크게 줄입니다. 아래를 참조하십시오. (출처: [9])

**JudgeLM의 성능**. JudgeLM 평가 데이터셋에서 테스트했을 때, JudgeLM은 입력으로 참조 답변이 제공되거나 제공되지 않은 경우 모두 PandaLM [6]과 GPT-3.5를 능가합니다. 아래를 참조하십시오. 이 경우 JudgeLM은 매우 유사한 분포의 데이터에 대해 미세 조정되었지만, 기준선 모델은 그렇지 않습니다. 그러나 JudgeLM은 PandaLM 평가 데이터셋 [6]에서도 잘 수행되는 것으로 나타났습니다. JudgeLM의 성능은 GPT-4(인간이 아님)와의 일치도 측면에서도 보고됩니다. GPT-4는 JudgeLM의 훈련 데이터를 생성하는 데 사용된 동일한 모델입니다. 일반적으로 JudgeLM 모델은 비교적 잘 수행되며, 크기에 따라 성능이 향상되고, 다른 모델에 비해 위치 전환에 더 강건합니다. (출처: [9])

**정렬 평가를 위한 생성형 심사관(Generative Judge for Evaluating Alignment) [7]**

"우리 모델은 대규모 실제 시나리오에서 사용자 쿼리(query)와 LLM이 생성한 응답에 대해 훈련되었으며, 잘 구조화된 자연어 비평(critique)을 통해 다양한 평가 프로토콜(예: 쌍대 응답 비교 및 단일 응답 평가)을 수용합니다." - [8]에서 발췌.

[7]에서 저자들은 도메인별 채점에 특화된 Auto-J라는 평가 목적의 또 다른 미세 조정된 LLM을 제안합니다. Auto-J 모델의 몇 가지 독특한 속성은 다음과 같습니다:
*   쌍대 및 직접 평가 채점(direct assessment scoring)을 수행하는 능력.
*   고품질의 구조화된 설명 제공에 중점.
*   다양하고 현실적인 시나리오를 모방하기 위해 인간이 생성한 쿼리¹⁰를 훈련에 사용합니다. 훈련 데이터는 인간이 질문한 질문에 기반합니다.

결과적으로 Auto-J 모델은 유연하고, 해석 가능하며, 실용적입니다. 이 모델은 실제 환경에서 시나리오별 평가를 수행하도록 최적화되어 있습니다. (출처: [7])

**훈련 데이터 구성**. Auto-J는 쌍대 및 직접 평가 채점 데이터 모두에 대해 훈련됩니다. 모델의 훈련 데이터셋을 구성하는 단계는 위에 묘사되어 있습니다. 우리는 Auto-J가 훈련될 평가 기준, 즉 시나리오를 정의하는 것으로 시작합니다. 특히, 58개의 평가 시나리오가 선택되었으며, 각각 고유한 정의와 기준을 가지며 8개 그룹으로 분류될 수 있습니다. 아래를 참조하십시오. (출처: [7])

그런 다음, 각 시나리오에 대한 실제 쿼리를 수집하고 GPT-4를 사용하여 이러한 쿼리에 대한 응답을 합성적으로 생성합니다. 각 시나리오에 대해 최대 100개의 훈련 예시가 생성되며, 데이터셋의 최종 크기는 약 3,500개 예시입니다.

"우리는 각 시나리오에 대해 신중하게 수작업으로 작성된 기준을 사용하여 GPT-4를 안내하여 원하는 평가 판단을 지도 훈련 신호로 수집하고, 휴리스틱 필터링 전략과 후처리 방법을 적용하여 출력 형식을 통합하고 노이즈를 완화합니다." - [7]에서 발췌.

쿼리는 주로 Chatbot Arena 및 WebGPT와 같은 인간 상호 작용을 기반으로 하는 공개 소스에서 수집됩니다. 이러한 쿼리는 모델이 실제 환경에서 접하게 될 질문의 종류를 정확하게 반영하여 Auto-J가 현실적인 평가 설정에 특화될 수 있도록 합니다. 쌍대 및 직접 평가 점수를 포함한 평가 점수는 GPT-4를 사용하여 생성됩니다. 합성 데이터의 품질을 보장하기 위해 저자들은 최종 데이터셋을 필터링하기 위한 여러 휴리스틱(예: 일관성 없거나 잘못된 형식의 예측 폐기)을 도입합니다. Auto-J는 다른 LLM 심사관에 비해 위치 편향이 적습니다. (출처: [7])

**Auto-J 훈련 방식**. [7]에서는 LLaMA-2-13B-Chat이 기본 모델로 사용되며, 이 기본 모델은 위에서 설명한 데이터셋에 대해 미세 조정됩니다. 흥미롭게도 저자들은 평가 기준을 모델에 입력으로 제공하지 않기로 선택합니다. 그들은 입력에서 기준을 제외하면 모델이 더 일반적(general)이 되므로, 데이터에서 이러한 기준을 암묵적으로 학습하는 것이 더 나은 접근 방식이라고 주장합니다. 이러한 전략은 참조 자료(예: 기준 및 채점 루브릭)를 모델 입력에 포함하는 것의 중요성을 강조하는 이 주제에 대한 후속 논문들과 직접적으로 모순됩니다 [1]. Auto-J는 이 데이터셋의 여러 채점 형식의 데이터에 대해 훈련되며, 저자들은 위치 편향을 피하기 위해 훈련 중에 위치 전환(position switching)을 채택합니다. 위를 참조하십시오.

"쌍대 비교에서 위치 편향을 줄이기 위해 간단한 데이터 증강 트릭을 적용합니다. 각 쌍대 훈련 샘플에 대해 입력에서 두 응답의 순서를 바꾸고 평가 판단에서 '응답 1'과 '응답 2'를 교대로 사용합니다." - [7]에서 발췌.

**평가 결과 분석**. Auto-J를 평가하기 위해 모델의 훈련 데이터셋과 유사하게 테스트 데이터셋이 생성됩니다. 그러나 테스트 데이터셋에 대한 모든 채점 결과는 인간 주석자에 의해 생성됩니다. 다양한 다른 LLM과 비교했을 때, Auto-J는 GPT-4를 제외한 거의 모든 모델을 능가하는 것으로 나타났습니다. 아래를 참조하십시오. (출처: [7])

이러한 결과는 전문 LLM 심사관이 특정 평가 설정을 위한 도메인별 데이터에 미세 조정될 때 비교적 잘 수행될 수 있다는 것을 나타냅니다. 경우에 따라 독점 모델의 성능과 일치하거나 이를 능가하기도 합니다. 그러나 이 데이터셋에서 Auto-J와 인간 평가자 간의 일치율은 여전히 낮습니다. 즉, LLM 심사관의 성능 측면에서 아직 갈 길이 멀다는 것을 의미합니다.

**비평, 검증 및 합성 데이터의 활용**

LLM 심사관을 미세 조정하려는 이러한 초기 시도 외에도, 다른 모델의 출력에 대한 검증, 비평 또는 피드백 제공을 위해 LLM을 사용하는 (매우 관련성 높은) 주제에 대한 여러 논문이 동시에 발표되었습니다. 이러한 모델은 LLM 심사관과 매우 유사합니다. 두 모델 모두 LLM의 응답에 대해 등급을 매기고 피드백을 제공할 수 있기 때문입니다. 그러나 비평 모델(critic model)