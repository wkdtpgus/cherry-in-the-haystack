대규모 언어 모델(LLM)을 실제 서비스에 적용할 때, 추론(inference) 과정의 성능을 극대화하는 핵심 기법 중 하나로 KV 캐시가 손꼽힙니다. LLM 배포 시 발생하는 주요 과제인 높은 지연 시간(latency)과 낮은 처리량(throughput)을 해결하는 데 KV 캐시가 결정적인 역할을 합니다. 본 문서는 KV 캐시의 기본적인 작동 원리를 명확히 제시하고, 독자들이 쉽게 이해할 수 있도록 직접 작성된 예제 코드를 통해 그 구현 방식을 상세히 풀어냅니다. KV 캐시 외에도 LLM 성능 향상을 위한 양자화(quantization), 지식 증류(knowledge distillation), 추측 디코딩(speculative decoding), FlashAttention과 같은 다양한 최적화 기법들이 존재하지만, KV 캐시는 그중에서도 가장 기본적인 동시에 강력한 방법으로 자리매김하고 있습니다.

기술 튜토리얼 게시글을 공유한 지 꽤 오랜 시간이 흘렀습니다. 현재 건강을 회복하며 더 심층적인 LLM 연구 주제에 몰두하고 있는 와중에, 많은 독자분들이 요청하셨던 내용(제 저서 "Building a Large Language Model From Scratch"에는 다루지 않았던)에 대한 튜토리얼을 공유하게 되어 기쁩니다. 즐겁게 읽어주시길 바랍니다!

## 개요

간단히 말해, KV 캐시의 역할은 언어 모델이 추론 단계(훈련 완료 후)에서 생성하는 과정에서 반복적으로 활용될 중간 키(Key) 및 값(Value) 벡터를 보관하는 것입니다. 이로 인해 텍스트를 생성하는 속도가 비약적으로 빨라집니다. 물론, 이 기법을 도입하면 코드 구조가 복잡해지고 필요한 메모리 양이 증가한다는 단점이 있습니다(이것이 제가 처음 이 내용을 저서에 포함하지 않았던 주된 이유이기도 합니다). 또한, 모델 훈련 과정에서는 이 기능을 활용할 수 없습니다. 이는 훈련이 주로 병렬 처리 방식을 사용하는 반면, 추론은 자기회귀적(autoregressive)으로 토큰을 순차적으로 생성하기 때문입니다. 그럼에도 불구하고, 실제 서비스 환경에서 LLM의 추론 성능 개선이 가져다주는 실시간 응답성(real-time responsiveness)과 효율성이라는 이점은 코드 복잡성 증가나 한정적인 GPU 메모리 소모량 확대와 같은 제약을 충분히 극복할 만큼 강력합니다.

## KV 캐시란 무엇인가요?

대규모 언어 모델이 새로운 텍스트를 만들어내는 과정을 머릿속에 그려봅시다. 예를 들어, 모델에 "시간"이라는 입력이 주어졌을 때, LLM은 한 번에 하나의 의미 단위(토큰)를 순차적으로 생성합니다. 이 과정을 도식화하면, 처음에는 "시간"이라는 프롬프트가 주어져 모델이 그 다음 토큰인 "은"을 생성하고, 이어서 "시간은"이라는 전체 문맥을 다시 분석하여 "빠르"라는 토큰을 만들어내는 식입니다.

이러한 다이어그램은 LLM이 토큰을 하나씩 생성하는 과정을 보여줍니다. "시간"이라는 초기 프롬프트에서 모델은 다음 토큰 "은"을 생성합니다. 그 다음 단계에서는 "시간은"이라는 완전한 시퀀스를 다시 처리하여 "빠르" 토큰을 만들어냅니다.

다음 그림에서 강조된 바와 같이, 생성된 LLM 텍스트 출력에는 약간의 중복(redundancy)이 발생합니다.

이 그림은 각 생성 단계에서 LLM이 반복적으로 처리해야 하는 동일한 문맥("시간은")을 명확히 보여줍니다. LLM이 중간 키/값 상태를 캐시(cache)하지 않을 경우, 새로운 토큰(예: "빠르")이 생성될 때마다 전체 시퀀스를 처음부터 다시 인코딩(re-encodes)합니다.

LLM의 텍스트 생성 함수를 구현할 때, 우리는 일반적으로 각 단계에서 마지막으로 생성된 토큰만 활용합니다. 하지만 위 시각화는 개념적 수준에서 발생하는 주요 비효율성 중 하나를 부각시킵니다. 이러한 비효율성, 즉 중복 연산은 LLM의 핵심 구성 요소인 어텐션 메커니즘(attention mechanism)을 더 깊이 들여다보면 더욱 명확해집니다. (어텐션 메커니즘에 대해 더 자세히 알고 싶다면, 제 저서 "Build a Large Language Model (From Scratch)" 3장이나 "Understanding and Coding Self-Attention, Multi-Head Attention, Causal-Attention, and Cross-Attention in LLMs" 글을 참고해주세요.)

[Understanding and Coding Self-Attention, Multi-Head Attention, Causal-Attention, and Cross-Attention in LLMs](https://magazine.sebastianraschka.com/p/understanding-and-coding-self-attention)
January 14, 2024
Read full story

다음 그림은 LLM의 심장부인 어텐션 메커니즘 연산의 일부를 나타냅니다. 여기서 입력 토큰("시간"과 "은")은 3차원 벡터로 인코딩됩니다 (실제로는 이 벡터의 차원이 훨씬 크지만, 시각화를 위해 간략화했습니다). 행렬 W는 이러한 입력들을 쿼리(Query), 키(Key), 값(Value) 벡터로 변환하는 어텐션 메커니즘의 가중치 행렬입니다. 셀프 어텐션(self-attention) 레이어는 입력 토큰마다 쿼리(Q), 키(K), 값(V) 벡터를 생성하며, 각 쿼리는 모든 이전 키와 상호작용하여 가중치를 계산하고, 이 가중치를 값 벡터에 적용하여 문맥 벡터를 도출합니다.

아래 그림은 토큰 임베딩(token embeddings)으로부터 키 및 값 벡터가 도출되는 과정을 강조한 기본 어텐션 점수 계산의 일부를 보여줍니다.

이 그림은 LLM이 어텐션 계산 과정에서 토큰 임베딩으로부터 키(k) 및 값(v) 벡터를 어떻게 생성하는지 보여줍니다. 각 입력 토큰(예: "시간"과 "은")은 학습된 가중치 행렬 W_k와 W_v를 사용하여 해당 키 및 값 벡터로 투영(projected)됩니다.

앞서 언급했듯이, LLM은 한 번에 하나의 의미 단위(토큰)를 생성합니다. LLM이 "빠르"라는 단어를 생성하여 다음 라운드의 프롬프트가 "시간은 빠르"가 되었다고 가정해 봅시다. 이는 아래 다음 그림에 나와 있습니다.

이 다이어그램은 LLM이 각 생성 단계에서 이전에 처리했던 토큰("시간"과 "은")에 대한 키 및 값 벡터를 다시 계산하는 방식을 보여줍니다. 세 번째 토큰("빠르")을 생성할 때, 모델은 k(1)/v(1) 및 k(2)/v(2) 벡터를 재사용하는 대신 다시 생성합니다. 이러한 반복적인 연산은 자기회귀적 디코딩(autoregressive decoding) 시 KV 캐시를 사용하지 않을 때 발생하는 주요 비효율성을 강조합니다. 특히 각 토큰이 이전의 모든 토큰에 어텐션해야 하므로, 시퀀스 길이가 $N$일 때 어텐션 연산은 $O(N^2)$의 복잡도를 가지게 됩니다.

이전 두 그림을 비교해 보면, 처음 두 토큰에 대한 키 및 값 벡터가 정확히 동일하며, 다음 토큰 텍스트 생성 라운드마다 이를 다시 계산하는 것은 명백한 낭비입니다.

이제 KV 캐시의 핵심 아이디어는 이전에 생성된 키 및 값 벡터를 저장하고 재사용하기 위한 캐싱 메커니즘(caching mechanism)을 구현하여 이러한 불필요한 재계산을 피하는 데 도움을 주는 것입니다. 이로써 어텐션 연산의 단계별 복잡도가 $O(N^2)$에서 $O(N)$으로 줄어듭니다.

## LLM이 텍스트를 생성하는 방법 (KV 캐시 사용 유무에 따라)

앞서 기본적인 원리를 이해했으니, 이제 실제 코드 구현을 탐구하기 전에 좀 더 심층적으로 살펴보겠습니다.

KV 캐시를 활용하지 않는 텍스트 생성 과정에서는, 예를 들어 "시간은 빠르다"와 같은 문장을 만들 때, 매 단계마다 "시간"과 "은" 같은 이미 처리된 토큰에 대한 계산이 반복적으로 수행됩니다. 이러한 비효율성은 KV 캐시를 통해 효과적으로 제거됩니다. KV 캐시는 이전에 산출된 키 및 값 벡터를 보관하고 필요할 때 재활용함으로써, 다음과 같은 방식으로 작동합니다.

1.  최초 입력이 주어지면, 모델은 해당 토큰들의 키와 값 벡터를 산출하여 저장소에 보관합니다.
2.  이후 새로운 토큰이 하나씩 생성될 때마다, 모델은 오직 그 새로운 토큰에 대한 키와 값 벡터만을 계산합니다.
3.  이전에 계산된 벡터들은 중복 연산을 피하기 위해 저장소에서 즉시 불러와 사용됩니다.

아래 표는 계산 및 캐싱 단계와 상태를 요약합니다. 이 과정은 자기회귀적 디코딩(autoregressive decoding)의 핵심이며, KV 캐시는 각 생성 단계에서 이전 토큰들의 문맥 정보를 효율적으로 관리하여 증분 디코딩(incremental decoding)을 가능하게 합니다.

| 단계 | 입력 토큰 | 처리 (KV 캐시 없음) | 처리 (KV 캐시 있음) | 캐시 상태 (K, V) |
| :-- | :-------- | :------------------ | :------------------ | :----------------- |
| 1   | "시간"    | K("시간"), V("시간") | K("시간"), V("시간") | [K("시간")], [V("시간")] |
| 2   | "은"      | K("시간"), V("시간"), K("은"), V("은") | K("은"), V("은") | [K("시간"), K("은")], [V("시간"), V("은")] |
| 3   | "빠르"    | K("시간"), V("시간"), K("은"), V("은"), K("빠르"), V("빠르") | K("빠르"), V("빠르") | [K("시간"), K("은"), K("빠르")], [V("시간"), V("은"), V("빠르")] |

여기서의 이점은 "시간" 토큰에 대한 키와 값 벡터가 한 번만 계산되어 두 번 재사용되고, "은" 토큰에 대한 벡터는 한 번 계산되어 한 번 재사용된다는 것입니다. (간단함을 위해 짧은 텍스트 예시이지만, 생성되는 텍스트가 길어질수록 이미 계산된 키와 값 벡터를 더 많이 재사용하게 되어 생성 속도가 기하급수적으로 빨라진다는 것을 직관적으로 알 수 있을 것입니다.)

다음 그림은 KV 캐시 사용 유무에 따른 텍스트 생성의 세 단계를 나란히 보여줍니다.

KV 캐시 사용 유무에 따른 텍스트 생성 비교. 상단 패널(캐시 없음)에서는 각 토큰 단계마다 키 및 값 벡터가 다시 계산되어 중복된 연산이 발생합니다. 하단 패널(캐시 있음)에서는 이전에 계산된 키와 값이 KV 캐시에서 검색되어 재계산을 피하고 더 빠른 생성을 가능하게 합니다.

따라서 코드에서 KV 캐시를 구현하려면, 평소처럼 키와 값을 계산한 다음 다음 라운드에서 검색할 수 있도록 저장하기만 하면 됩니다. 다음 섹션에서는 구체적인 코드 예시를 통해 이를 설명합니다.

## KV 캐시 처음부터 구현하기

KV 캐시를 구축하는 방식은 다양하지만, 그 본질은 텍스트 생성의 각 순환에서 오직 새로이 생성된 토큰에 대해서만 키와 값 텐서를 연산하는 데 있습니다. 본 글에서는 코드의 명료성을 우선시한 간결한 접근법을 채택했습니다. 실제 구현의 변화를 파악하는 가장 직관적인 방법은 관련 코드 수정 사항을 직접 확인하는 것입니다.

제가 GitHub에 공유한 두 개의 파일은 KV 캐시 유무에 따라 LLM을 처음부터 구현하는 독립형 파이썬(Python) 스크립트입니다.

*   `gpt_ch04.py`: 제 책 "Build a Large Language Model (From Scratch)" 3장과 4장에서 가져온 독립형 코드로, LLM을 구현하고 간단한 텍스트 생성 함수를 실행합니다.
*   `gpt_with_kv_cache.py`: 위와 동일하지만, KV 캐시를 구현하기 위해 필요한 변경 사항이 적용되었습니다.

KV 캐시 관련 코드 수정 사항을 살펴보려면 다음 중 하나를 수행할 수 있습니다.

a. `gpt_with_kv_cache.py` 파일을 열고 새로운 변경 사항을 표시하는 `# NEW` 섹션을 찾아보세요.

b. 원하는 파일 비교 도구를 통해 두 코드 파일을 확인하여 변경 사항을 비교해 보세요.

또한 구현 세부 사항을 요약하기 위해 다음 하위 섹션에서 간략한 설명을 제공합니다.

### 1. 캐시 버퍼(Cache Buffers) 등록하기

`MultiHeadAttention` 생성자 내부에, 단계별로 연결된 키와 값을 저장할 두 개의 비영구 버퍼(non-persistent buffers)인 `cache_k`와 `cache_v`를 추가합니다. `register_buffer`를 사용하는 이유는 이 캐시들이 모델의 상태(state)의 일부로 간주되어, 모델이 다른 장치(device)로 이동하거나 `state_dict`로 저장될 때 함께 처리되도록 하기 위함입니다.

```python
self.register_buffer("cache_k", None, persistent=False)
self.register_buffer("cache_v", None, persistent=False)
```

(버퍼에 대해 더 자세히 알고 싶으시다면, 제가 만든 유튜브 영상 "[Understanding PyTorch Buffers](https://www.youtube.com/watch?v=F_f-z71yq44)"를 시청해 보세요.)

### 2. `use_cache` 플래그(flag)를 사용한 순전파(Forward pass)

다음으로, `MultiHeadAttention` 클래스의 `forward` 메서드를 확장하여 `use_cache` 인자(argument)를 받도록 합니다. 이 플래그는 추론 시 최적화 기법을 활성화하는 일반적인 패턴입니다.

```python
def forward(self, x, use_cache=False):
    b, num_tokens, d_in = x.shape

    keys_new = self.W_key(x) # Shape: (b, num_tokens, d_out)
    values_new = self.W_value(x)
    queries = self.W_query(x) #...

    if use_cache:
        if self.cache_k is None:
            self.cache_k, self.cache_v = keys_new, values_new
        else:
            # torch.cat은 새로운 메모리 공간을 할당하고 기존 텐서와 새 텐서를 복사하는 오버헤드가 있습니다.
            # 이는 이후 최적화 섹션에서 다룰 주요 성능 병목 중 하나입니다.
            self.cache_k = torch.cat([self.cache_k, keys_new], dim=1)
            self.cache_v = torch.cat([self.cache_v, values_new], dim=1)
        keys, values = self.cache_k, self.cache_v
    else:
        keys, values = keys_new, values_new
```

여기서 키와 값의 저장 및 검색은 KV 캐시의 핵심 아이디어를 구현합니다. 특히 `torch.cat` 연산은 매번 새로운 텐서를 생성하고 기존 데이터를 복사하는 과정에서 메모리 단편화(fragmentation)와 불필요한 오버헤드를 발생시킬 수 있습니다.

**저장**

구체적으로, `if self.cache_k is None: ...`를 통해 캐시가 초기화된 후, 새로 생성된 키와 값을 각각 `self.cache_k = torch.cat(...)` 및 `self.cache_v = torch.cat(...)`를 통해 캐시에 추가합니다.

**검색**

그런 다음, `keys, values = self.cache_k, self.cache_v`는 캐시에서 저장된 값과 키를 검색합니다.

이것이 바로 KV 캐시의 핵심 저장 및 검색 메커니즘입니다. 다음 섹션 3과 4는 사소한 구현 세부 사항을 다룹니다.

### 3. 캐시 지우기

텍스트를 생성할 때, 두 개의 별도 텍스트 생성 호출 사이에 키와 값 버퍼를 모두 재설정해야 합니다. 그렇지 않으면, 새로운 프롬프트의 쿼리(query)가 이전 시퀀스에서 남은 오래된 키에 어텐션(attend)하게 되어, 모델이 관련 없는 컨텍스트(context)에 의존하고 일관성 없는 출력을 생성하게 됩니다. 이를 방지하기 위해, 나중에 텍스트 생성 호출 사이에 사용할 수 있는 `reset_cache` 메서드를 `MultiHeadAttention` 클래스에 추가합니다.

```python
def reset_cache(self):
    self.cache_k, self.cache_v = None, None
```

### 4. 전체 모델에서 `use_cache` 전파하기

`MultiHeadAttention` 클래스에 변경 사항이 적용되었으므로, 이제 `GPTModel` 클래스를 수정합니다. 먼저, 토큰 인덱스(token indices)에 대한 위치 추적을 생성자에 추가합니다.

```python
self.current_pos = 0
```

이것은 모델이 증분 생성 세션(incremental generation session) 동안 이미 몇 개의 토큰을 캐시했는지 기억하는 간단한 카운터(counter)입니다. 이 `current_pos`는 위치 임베딩(positional embedding)을 정확하게 적용하는 데 필수적입니다. 새로운 토큰이 생성될 때마다 이전에 캐시된 토큰의 길이를 고려하여 올바른 위치 정보를 제공해야 합니다.

그런 다음, 한 줄짜리 블록 호출을 명시적인 루프(loop)로 대체하고, 각 트랜스포머 블록(transformer block)을 통해 `use_cache`를 전달합니다.

```python
def forward(self, in_idx, use_cache=False):
    # ...
    if use_cache:
        pos_ids = torch.arange(
            self.current_pos, self.current_pos + seq_len, device=in_idx.device, dtype=torch.long
        )
        self.current_pos += seq_len
    else:
        pos_ids = torch.arange(
            0, seq_len, device=in_idx.device, dtype=torch.long
        )
    pos_embeds = self.pos_emb(pos_ids).unsqueeze(0)
    x = tok_embeds + pos_embeds
    # ...
    for blk in self.trf_blocks:
        x = blk(x, use_cache=use_cache)
```

위에서 `use_cache=True`로 설정하면, `self.current_pos`에서 시작하여 `seq_len` 단계만큼 계산합니다. 그런 다음, 다음 디코딩(decoding) 호출이 중단된 지점에서 계속되도록 카운터를 증가시킵니다. `self.current_pos`를 추적하는 이유는 새로운 쿼리가 이미 저장된 키와 값 바로 뒤에 정렬되어야 하기 때문입니다. 카운터를 사용하지 않으면, 모든 새로운 단계가 다시 위치 0에서 시작하여 모델이 새로운 토큰을 이전 토큰과 겹치는 것처럼 처리할 것입니다. (대안으로, `offset = block.att.cache_k.shape[1]`을 통해 추적할 수도 있습니다.)

위 변경 사항은 `use_cache` 인자를 받도록 `TransformerBlock` 클래스에 작은 수정도 필요로 합니다.

```python
def forward(self, x, use_cache=False):
    # ...
    self.att(x, use_cache=use_cache)
```

마지막으로, 편의를 위해 모든 블록 캐시를 한 번에 지우도록 `GPTModel`에 모델 수준의 재설정 기능을 추가합니다.

```python
def reset_kv_cache(self):
    for blk in self.trf_blocks:
        blk.att.reset_cache()
    self.current_pos = 0
```

### 5. 생성 시 캐시 사용하기

`GPTModel`, `TransformerBlock`, `MultiHeadAttention`에 대한 변경 사항이 적용되었으므로, 마지막으로 간단한 텍스트 생성 함수에서 KV 캐시를 사용하는 방법은 다음과 같습니다. 추론 과정에서 `torch.no_grad()` 컨텍스트 매니저를 사용하는 것은 메모리 사용량을 줄이고 연산 속도를 높이는 데 중요합니다.

```python
def generate_text_simple_cached(
    model, idx, max_new_tokens, use_cache=True
):
    model.eval()

    ctx_len = model.pos_emb.num_embeddings # max sup. len., e.g. 1024
    if use_cache:
        # Init cache with full prompt
        model.reset_kv_cache()
        with torch.no_grad(): # 추론 시에는 그라디언트 계산이 불필요하므로 메모리 및 속도 최적화
            logits = model(idx[:, -ctx_len:], use_cache=True)

        for _ in range(max_new_tokens):
            # a) pick the token with the highest log-probability
            next_idx = logits[:, -1].argmax(dim=-1, keepdim=True)
            # b) append it to the running sequence
            idx = torch.cat([idx, next_idx], dim=1)
            # c) feed model only the new token
            with torch.no_grad():
                # KV 캐시 사용 시에는 새로 생성된 토큰만 모델에 입력하여 효율을 높입니다.
                logits = model(next_idx, use_cache=True)
    else:
        for _ in range(max_new_tokens):
            with torch.no_grad():
                # KV 캐시 미사용 시에는 매번 전체 시퀀스를 모델에 입력해야 합니다.
                logits = model(idx[:, -ctx_len:], use_cache=False)
            next_idx = logits[:, -1].argmax(dim=-1, keepdim=True)
            idx = torch.cat([idx, next_idx], dim=1)

    return idx
```

c)에서 `logits = model(next_idx, use_cache=True)`를 통해 모델에 새로운 토큰만 공급한다는 점에 유의하세요. 캐싱이 없으면, 재사용할 저장된 키와 값이 없으므로 모델에 전체 입력 `logits = model(idx[:, -ctx_len:], use_cache=False)`를 공급합니다. 이는 Hugging Face Transformers 라이브러리에서 `past_key_values` 인자를 통해 KV 캐시를 관리하는 방식과 유사합니다.

## 간단한 성능 비교

KV 캐시를 개념적 수준에서 다룬 후, 큰 질문은 작은 예시에서 실제로 얼마나 잘 작동하는가입니다. 구현을 시도해 보려면, 앞서 언급된 두 코드 파일을 파이썬 스크립트로 실행할 수 있습니다. 이 스크립트는 작은 1억 2천 4백만(124M) 매개변수(parameter) LLM을 실행하여 200개의 새로운 토큰을 생성합니다 (시작 프롬프트는 4개의 토큰 "Hello, I am"입니다).

```bash
pip install -r https://raw.githubusercontent.com/rasbt/LLMs-from-scratch/refs/heads/main/requirements.txt

python gpt_ch04.py

python gpt_with_kv_cache.py
```

M4 칩(CPU)이 탑재된 맥 미니(Mac Mini)에서 측정된 결과는 다음과 같습니다.

| Model | Runtime |
| :-------------------- | :-------- |
| `gpt_ch04.py` | 13.52s |
| `gpt_with_kv_cache.py` | 2.68s |

이처럼, 규모가 작은 1억 2천 4백만 개의 매개변수를 가진 모델과 200개 토큰이라는 비교적 짧은 생성 길이에서도 이미 약 5배에 달하는 처리 속도 개선을 확인할 수 있습니다. 이러한 속도 향상은 모델 크기나 시퀀스 길이가 증가할수록 더욱 두드러지게 나타납니다. (이 구현은 코드 가독성을 위해 최적화되었으며, CUDA 또는 MPS 런타임 속도를 위해 최적화되지 않았습니다. CUDA 또는 MPS 런타임 속도를 위해서는 텐서를 다시 인스턴스화하고 연결하는 대신 미리 할당해야 합니다.)

**참고**: 모델은 두 경우 모두 "의미 없는 말(gibberish)"을 생성합니다. 즉, 다음과 같은 텍스트입니다.

```
Output text: Hello, I am Featureiman Byeswickattribute argue logger Normandy Compton analogous bore ITVEGIN ministriesysics Kle functional recountrictionchangingVirgin embarrassedgl ...
```

이는 아직 모델을 학습시키지 않았기 때문입니다. 다음 장에서는 모델을 학습시키고, 학습된 모델에 KV 캐시를 사용하여 일관성 있는 텍스트를 생성할 수 있습니다 (하지만 KV 캐시는 추론 중에만 사용하도록 되어 있습니다). 여기서는 코드를 더 간단하게 유지하기 위해 학습되지 않은 모델을 사용하고 있습니다.

하지만 더 중요한 것은 `gpt_ch04.py`와 `gpt_with_kv_cache.py` 구현이 정확히 동일한 텍스트를 생성한다는 것입니다. 이는 KV 캐시가 올바르게 구현되었음을 알려줍니다. 인덱싱(indexing) 오류는 쉽게 발생하여 다른 결과를 초래할 수 있습니다.

"Ahead of AI"를 읽어주셔서 감사합니다! 새로운 게시물을 받고 제 작업을 지원하려면 무료로 구독하세요.

[구독](https://magazine.sebastianraschka.com/subscribe)

## KV 캐시의 장점과 단점

생성되는 시퀀스의 길이가 길어질수록 KV 캐시의 이점과 한계는 더욱 명확해집니다.

*   **[장점] 연산 효율성 극대화**: 캐싱 메커니즘이 없을 경우, 특정 시점 $t$에서 어텐션 연산은 새로운 쿼리를 $t$개의 이전에 생성된 키와 일일이 비교해야 하므로, 전체 연산량은 시퀀스 길이에 대해 제곱($O(n^2)$) 형태로 증가합니다. 이는 매 토큰 생성 시마다 $Q K^T V$ 계산에서 $K$와 $V$의 크기가 선형적으로 커지기 때문입니다. 반면, 캐시를 활용하면 키와 값이 한 번만 계산되고 이후에는 재활용되므로, 각 단계별 전체 연산 복잡도는 선형적($O(n)$)으로 줄어듭니다.
*   **[단점] 메모리 소모량의 선형적 증대**: 새 토큰이 생성될 때마다 KV 캐시에 해당 정보가 누적됩니다. 각 트랜스포머 레이어(transformer layer)와 어텐션 헤드(attention head)마다 키와 값 벡터가 저장되므로, 긴 시퀀스를 처리하거나 대규모 LLM(예: 70억 개 매개변수 모델, 4096 토큰 컨텍스트)을 운용할 경우, 이러한 캐시의 누적 크기가 GPU 메모리를 과도하게 점유하여 운영상 감당하기 어려운 수준에 이를 수 있습니다. 이를 완화하기 위해 캐시를 일정 크기로 제한하는 방법(예: PagedAttention)이 있지만, 이는 시스템에 추가적인 복잡성을 더하게 됩니다 (하지만 실제 LLM 배포 환경에서는 이러한 투자가 충분히 정당화될 수 있습니다).

## KV 캐시 구현 최적화하기

이전에 제시된 KV 캐시의 개념적 구현은 명확성을 돕고 주로 코드 가독성 및 교육적 목적에 맞춰져 있지만, 실제 시나리오(특히 더 큰 모델과 더 긴 시퀀스 길이에서)에 배포하려면 더 신중한 최적화가 필요합니다.

### 캐시 확장 과정의 일반적인 문제점

*   **메모리 조각화(fragmentation) 및 연속적인 재할당**: `torch.cat` 함수를 사용하여 텐서를 계속 병합하는 방식은 잦은 메모리 할당과 해제를 유발하여 시스템 성능 저하의 주된 원인이 됩니다. 이는 `torch.cat`이 기존 텐서와 새 텐서를 수용할 수 있는 새로운 메모리 블록을 할당하고, 데이터를 복사하는 방식으로 작동하기 때문입니다.
*   **메모리 사용량의 선형적 증가**: 별도의 관리 없이 캐시 크기가 계속 커지면, 극도로 긴 시퀀스에서는 현실적으로 사용하기 어려워집니다.

### 팁 1: 메모리 미리 할당하기 (Pre-allocating Memory)

텐서를 반복적으로 연결하는 대신, 예상되는 최대 시퀀스 길이를 기반으로 충분히 큰 텐서를 미리 할당할 수 있습니다. 이는 일관된 메모리 사용을 보장하고 불필요한 메모리 복사 오버헤드를 줄입니다. 의사 코드(pseudo-code)로 표현하면 다음과 같습니다.

```python
# Example pre-allocation for keys and values
max_seq_len = 1024 # 최대 예상 시퀀스 길이
cache_k = torch.zeros(
    (batch_size, num_heads, max_seq_len, head_dim), device=device
)
cache_v = torch.zeros(
    (batch_size, num_heads, max_seq_len, head_dim), device=device
)
```

추론 중에, 우리는 이 미리 할당된 텐서의 특정 슬라이스(slices)에 새로 계산된 키와 값 벡터를 직접 쓸 수 있습니다.

### 팁 2: 슬라이딩 윈도우(Sliding Window)를 통한 캐시 자르기 (Truncating Cache via Sliding Window)

GPU 메모리가 과도하게 사용되는 것을 방지하기 위해, 동적 자르기(dynamic truncation)를 포함하는 슬라이딩 윈도우 접근 방식을 구현할 수 있습니다. 슬라이딩 윈도우를 통해 캐시에는 마지막 `window_size` 토큰만 유지합니다. 이 방법은 과거 컨텍스트의 일부를 희생하여 메모리 사용량을 고정된 크기로 유지합니다. 이는 특히 대화형 LLM과 같이 최신 정보가 더 중요한 애플리케이션에 유용합니다.

```python
# Sliding window cache implementation
window_size = 512
cache_k = cache_k[:, :, -window_size:, :]
cache_v = cache_v[:, :, -window_size:, :]
```

### 기타 고급 최적화 기법

*   **KV 캐시 양자화(Quantization of KV Cache)**: 키와 값 벡터의 정밀도(예: FP16에서 INT8)를 낮춤으로써 메모리 사용량을 크게 줄일 수 있습니다. 이는 미미한 성능 저하로 상당한 메모리 절약을 가능하게 합니다.
*   **다중 쿼리 어텐션(Multi-Query Attention, MQA) 및 그룹 쿼리 어텐션(Grouped-Query Attention, GQA)**: 이 아키텍처는 여러 어텐션 헤드가 키와 값 벡터를 공유하거나 그룹으로 공유하게 하여, 키와 값 벡터의 저장 및 연산량을 줄여줍니다. 이는 KV 캐시의 메모리 footprint를 크게 줄이는 동시에 추론 속도를 높일 수 있습니다.
*   **PagedAttention**: 가변 길이 시퀀스 및 배치(batching) 환경에서 KV 캐시를 효율적으로 관리하기 위한 최신 기법으로, 메모리 단편화를 방지하고 GPU 사용률을 극대화합니다.

### 실제 최적화

이 최적화는 `gpt_with_kv_cache_optimized.py` 파일에서 찾을 수 있습니다. M4 칩(CPU)이 탑재된 맥 미니에서 200 토큰 생성과 LLM의 컨텍스트 길이와 동일한 윈도우 크기(동일한 결과를 보장하여 공정한 비교를 위함)로 아래와 같이 코드 런타임(runtime)을 비교했습니다.

| Model | Runtime |
| :-------------------------------- | :-------- |
| `gpt_ch04.py` | 13.52s |
| `gpt_with_kv_cache.py` | 2.68s |
| `gpt_with_kv_cache_optimized.py` | 2.67s |

안타깝게도, 이 모델은 매우 작고 Python 인터프리터의 오버헤드, PyTorch 연산의 추상화 계층, 그리고 장치 간 데이터 전송 및 통신 오버헤드가 KV 캐시의 개념적 이점을 상회하기 때문에 CPU 환경에서 `torch.cat` 방식과 미리 할당 방식 간의 속도 이점은 미미했습니다. GPU 환경에서는 이러한 오버헤드가 더욱 두드러져, 작은 모델에서는 KV 캐시 최적화의 효과가 가려질 수 있습니다.

## 결론

결론적으로, 캐싱 기법은 시스템에 추가적인 복잡성과 메모리 사용량 증가라는 요소를 가져오지만, 이를 통해 달성하는 성능 향상 효과는 특히 실제 서비스 환경에서 이러한 제약들을 압도하는 경우가 대다수입니다. 본문에서는 효율성보다는 코드의 명료성과 이해도를 높이는 데 주력했음을 다시 한번 강조합니다. 하지만 실제 시스템을 구축할 때는 메모리 사전 할당이나 슬라이딩 윈도우 캐시 적용과 같이 메모리 사용량 증가 문제를 효과적으로 제어하기 위한 세심한 최적화 전략이 필수적입니다. 또한, KV 캐시 양자화, MQA/GQA, PagedAttention과 같은 고급 기법들을 통해 더욱 효율적인 LLM 추론 시스템을 구축할 수 있습니다.

LLM 분야는 계속해서 발전하고 있으며, KV 캐시 관리 기술 또한 하드웨어에 최적화된 구현이나 특수 AI 가속기를 활용하는 방향으로 진화하고 있습니다. 이 글이 독자 여러분께 유익한 통찰을 제공했기를 바라며, 제시된 기법들을 자유롭게 적용해 보면서 즐거운 개발 경험을 하시길 바랍니다! 오픈 소스 LLM 프로젝트들을 탐색하며 실제 프로덕션 환경에서의 고급 구현 사례를 살펴보는 것도 좋은 학습 방법이 될 것입니다.

## 보너스: Qwen3 및 Llama 3의 KV 캐시

Qwen3 (0.6B)와 Llama 3 (1B)와 같은 모델을 직접 구축한 후, KV 캐시 적용 여부에 따른 실행 시간을 비교하는 추가적인 시험을 진행했습니다. 앞선 최적화 부분에서 언급된 KV 캐시 텐서 사전 할당 방식 대신, 본 실험에서는 `torch.cat`을 이용한 동적 결합 방식을 채택했습니다. 이는 Llama 3 (131k 토큰) 및 Qwen3 (41k 토큰)가 지원하는 방대한 컨텍스트 길이로 인해, 미리 할당 시 약 8GB에 달하는 막대한 추가 메모리 소모가 발생하기 때문입니다. 더불어, 즉석에서 텐서를 생성하는 `torch.cat` 방식이 메모리 효율적이라는 판단하에, 연산 효율을 높이고 `torch.compile`로 모델을 최적화하기 위해 KV 캐시 로직을 모델의 내부 구조로부터 분리하여 구현했습니다. `torch.compile`은 모델의 연산 그래프를 정적으로 최적화하므로, 동적으로 크기가 변하는 KV 캐시를 모델 외부에 두는 것이 컴파일러의 효율적인 작동에 유리합니다.

코드는 여기에서 찾을 수 있습니다:
[qwen3.py](https://github.com/rasbt/LLMs-from-scratch/blob/main/ch08/qwen3.py) | [README](https://github.com/rasbt/LLMs-from-scratch/blob/main/ch08/README.md)
[llama3.py](https://github.com/rasbt/LLMs-from-scratch/blob/main/ch08/llama3.py) | [README](https://github.com/rasbt/LLMs-from-scratch/blob/main/ch08/README.md)

성능은 아래에 나와 있습니다.

| Model | CPU (s) | CPU (compiled) (s) | GPU (s) | GPU (compiled) (s) |
| :-------------------------------- | :------ | :----------------- | :------ | :----------------- |
| Qwen3 (no KV cache) | 16.2 | 10.1 | 0.9 | 0.8 |
| Qwen3 (with KV cache) | 3.2 | 2.5 | 0.9 | 0.8 |
| Llama 3 (no KV cache) | 18.1 | 11.2 | 1.1 | 0.9 |
| Llama 3 (with KV cache) | 3.5 | 2.7 | 1.1 | 0.9 |

보시다시피, CPU 환경에서는 KV 캐시가 가장 상당한 속도 향상(최대 약 5배)을 가져왔으며, `torch.compile`을 통한 컴파일은 그 성능을 더욱 향상시킵니다. CPU는 GPU에 비해 병렬 처리 능력이 낮으므로, KV 캐시를 통한 순차적 연산의 최적화가 더 큰 영향을 미 미칩니다. 하지만 GPU 환경에서는 KV 캐시 유무에 따른 성능 차이가 미미하며, 일반 컴파일된 모델이 최상의 성능을 보여줍니다. 이는 GPU의 어텐션 커널(kernel)이 이미 매우 높은 수준으로 최적화되어 있거나, `torch.cat` 방식의 메모리 오버헤드가 GPU의 고속 연산 능력을 상쇄하거나, 또는 모델의 크기가 상대적으로 작아 계산 자체가 병목이 아닌 메모리 대역폭(bandwidth)이나 데이터 전송이 병목으로 작용했을 가능성이 있습니다. 배치 크기(batch size)를 늘리면 이러한 양상이 달라질 수도 있습니다.

이 매거진은 개인적인 열정 프로젝트입니다. 독립 연구자로서 저를 지원하시려면, 제 책 "[Build a Large Language Model (From Scratch)](https://www.amazon.com/Build-Large-Language-Model-Scratch/dp/B0C91461Q3)"을 구매하시거나 [유료 구독](https://magazine.sebastianraschka.com/subscribe)을 신청해 주시면 감사하겠습니다.

[Build a Large Language Model (From Scratch)](https://www.amazon.com/Build-Large-Language-Model-Scratch/dp/B0C91461Q3) 아마존에서 구매 가능

책을 읽으셨고 잠시 시간이 있으시다면, [짧은 리뷰](https://www.amazon.com/Build-Large-Language-Model-Scratch/dp/B0C91461Q3)를 남겨주시면 정말 감사하겠습니다. 이는 저희 저자들에게 큰 도움이 됩니다!

여러분의 지원은 큰 의미가 있습니다! 감사합니다!