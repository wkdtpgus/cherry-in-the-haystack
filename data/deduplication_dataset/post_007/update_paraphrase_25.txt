LLM 프로덕션 환경에서의 효율적인 추론: KV 캐시의 심층 분석과 최적화 전략

대규모 언어 모델(LLM)을 실제 서비스 환경에서 효율적으로 운영하기 위한 핵심 기법 중 하나로, KV 캐시(KV Cache)가 주목받고 있습니다. KV 캐시는 프로덕션 환경에서 LLM 추론의 계산 효율성을 크게 향상시키는 중요한 구성 요소입니다. 이 글에서는 KV 캐시가 개념적으로 어떻게 작동하는지 상세히 탐구하고, 개발자가 쉽게 이해할 수 있도록 직접 작성된 코드를 통해 그 구현 방식을 명확히 제시합니다.

최근 필자는 건강상의 회복 기간을 거치며 LLM 연구의 심화된 주제들을 다루는 글을 준비하고 있었습니다. 그 와중에 많은 독자분들이 요청했던 특정 주제(필자의 저서 "Building a Large Language Model From Scratch"에는 포함되지 않았던 내용)에 대한 튜토리얼을 공유하는 것이 좋겠다는 생각이 들었습니다. 이 글이 여러분께 유익한 시간이 되기를 바랍니다!

## 핵심 요약 (Overview)

간단히 말해, KV 캐시는 LLM이 텍스트를 생성하는 추론 단계(학습 완료 후)에서 이전에 계산된 중간 키(K)와 값(V) 벡터를 저장하여 재활용하는 메커니즘입니다. 이 접근 방식은 텍스트 생성 속도를 획기적으로 개선합니다. 그러나 KV 캐시를 도입하면 코드 구조가 다소 복잡해지고, 특히 장문 생성을 처리할 때 메모리 사용량이 늘어날 수 있다는 단점이 있습니다. 또한, 이 기능은 모델 학습(training) 과정에서는 활용되지 않습니다. 그럼에도 불구하고, 실제 서비스 환경에서 LLM의 응답 속도를 높이는 이점은 이러한 구현 복잡성이나 추가 메모리 소모를 충분히 상회하는 경우가 대부분입니다.

## KV 캐시의 원리

LLM이 문장을 만들어내는 과정을 예시로 들어봅시다. 예를 들어, LLM에 "Time"이라는 프롬프트(prompt)가 주어졌다고 가정해 봅시다. 이미 잘 알려져 있듯이, LLM은 한 번에 하나의 단어(혹은 토큰(token))를 순차적으로 생성하며, 다음 두 텍스트 생성 단계는 아래 그림과 같이 나타날 수 있습니다.

이 다이어그램은 LLM이 한 번에 하나의 토큰을 생성하는 방식을 보여줍니다. "Time"이라는 프롬프트로 시작하여, 모델은 다음 토큰인 "flies"를 생성합니다. 다음 단계에서는 전체 시퀀스(sequence) "Time flies"가 다시 처리되어 "fast" 토큰을 생성합니다.

다음 그림에서 강조된 바와 같이, 생성된 LLM 텍스트 출력에는 약간의 중복(redundancy)이 존재합니다.

이 그림은 각 생성 단계에서 LLM이 다시 처리해야 하는 반복되는 컨텍스트("Time flies")를 강조합니다. LLM은 중간 키/값 상태를 캐시(cache)하지 않으므로, 새로운 토큰(예: "fast")이 생성될 때마다 전체 시퀀스를 다시 인코딩(re-encodes)합니다.

LLM 텍스트 생성 함수를 구현할 때, 우리는 일반적으로 각 단계에서 마지막으로 생성된 토큰만 사용합니다. 하지만 위 시각화는 개념적 수준에서 주요 비효율성 중 하나를 강조합니다. 이러한 비효율성(또는 중복성)은 어텐션 메커니즘(attention mechanism) 자체를 자세히 살펴보면 더욱 명확해집니다. (어텐션 메커니즘에 대해 궁금하시다면, 제 책 "Build a Large Language Model (From Scratch)" 3장 또는 "Understanding and Coding Self-Attention, Multi-Head Attention, Causal-Attention, and Cross-Attention in LLMs" 글에서 더 자세히 읽어볼 수 있습니다.)

[Understanding and Coding Self-Attention, Multi-Head Attention, Causal-Attention, and Cross-Attention in LLMs](https://magazine.sebastianraschka.com/p/understanding-and-coding-self-attention)
January 14, 2024
Read full story

다음 그림은 LLM의 핵심인 어텐션 메커니즘 계산의 일부를 보여줍니다. 여기서 입력 토큰("Time"과 "flies")은 3차원 벡터로 인코딩됩니다 (실제로는 이 벡터들은 훨씬 크지만, 작은 그림에 담기 어렵게 만들 것입니다). 행렬 W는 이러한 입력을 키(key), 값(value), 쿼리(query) 벡터로 변환하는 어텐션 메커니즘의 가중치 행렬입니다.

아래 그림은 키 및 값 벡터가 강조된 기본적인 어텐션 점수 계산의 일부를 보여줍니다.

이 그림은 LLM이 어텐션 계산 중에 토큰 임베딩(token embeddings)으로부터 키(k) 및 값(v) 벡터를 도출하는 방법을 보여줍니다. 각 입력 토큰(예: "Time"과 "flies")은 학습된 행렬 W_k와 W_v를 사용하여 해당 키 및 값 벡터를 얻기 위해 투영(projected)됩니다.

앞서 언급했듯이, LLM은 한 번에 한 단어(또는 토큰)를 생성합니다. LLM이 "fast"라는 단어를 생성하여 다음 라운드의 프롬프트가 "Time flies fast"가 되었다고 가정해 봅시다. 이는 아래 다음 그림에 나와 있습니다.

이 다이어그램은 LLM이 각 생성 단계에서 이전에 본 토큰("Time"과 "flies")에 대한 키 및 값 벡터를 어떻게 다시 계산하는지 보여줍니다. 세 번째 토큰("fast")을 생성할 때, 모델은 k(1)/v(1) 및 k(2)/v(2) 벡터를 재사용하는 대신 다시 계산합니다. 이러한 반복 계산은 자기회귀 디코딩(autoregressive decoding) 중에 KV 캐시를 사용하지 않을 때의 비효율성을 강조합니다.

이전 두 그림을 비교해 보면, 처음 두 토큰에 대한 키 및 값 벡터가 정확히 동일하며, 다음 토큰 텍스트 생성 라운드마다 이를 다시 계산하는 것은 명백한 낭비입니다. 이러한 중복 연산은 시퀀스 길이가 길어질수록 어텐션 메커니즘의 연산 복잡도가 제곱에 비례(O(N²))하여 기하급수적으로 증가하는 주된 원인이 됩니다.

이제 KV 캐시의 아이디어는 이전에 생성된 키 및 값 벡터를 재사용하기 위해 저장하는 캐싱 메커니즘(caching mechanism)을 구현하여 이러한 불필요한 재계산을 피하는 데 도움을 주는 것입니다. 이로써 매 단계마다 새롭게 추가되는 토큰에 대해서만 계산을 수행하여 연산 복잡도를 선형적으로(O(N)) 줄일 수 있습니다.

## LLM 텍스트 생성 과정: KV 캐시 유무에 따른 비교

이전 섹션에서 기본 개념을 다루었으니, 구체적인 코드 구현을 살펴보기 전에 좀 더 자세히 알아보겠습니다.

"Time flies fast"에 대한 KV 캐시 없는 텍스트 생성 프로세스를 다시 살펴보면, 다음과 같은 중복성이 관찰됩니다. "Time"과 "flies" 토큰은 새로운 생성 단계마다 다시 계산되어 비효율성을 야기합니다. KV 캐시는 이전에 계산된 키 및 값 벡터를 저장하고 재사용함으로써 이러한 비효율성을 효과적으로 해결합니다.

1.  **초기 프롬프트 처리**: 모델은 최초 입력 토큰 시퀀스에 대한 키와 값 벡터를 계산하고, 이를 KV 캐시에 저장합니다.
2.  **새로운 토큰 생성**: 각 생성 단계에서 모델은 오직 새로 생성된 토큰에 대해서만 키와 값 벡터를 계산합니다.
3.  **캐시 활용**: 이전에 계산된 키와 값 벡터는 캐시에서 즉시 불러와 재사용되며, 이는 중복 계산을 방지합니다.

아래 표는 계산 및 캐싱 단계와 상태를 요약합니다.

여기서의 이점은 "Time"이 한 번 계산되어 두 번 재사용되고, "flies"가 한 번 계산되어 한 번 재사용된다는 것입니다. (간단함을 위해 짧은 텍스트 예시이지만, 텍스트가 길어질수록 이미 계산된 키와 값을 더 많이 재사용하게 되어 생성 속도가 빨라진다는 것을 직관적으로 알 수 있을 것입니다.) 또한, 캐시된 데이터를 활용하는 것은 단순히 연산량을 줄이는 것을 넘어, 메모리 접근 효율성 측면에서도 이점을 제공합니다. 이미 계산되어 빠른 메모리 영역(예: GPU 캐시)에 저장된 K/V 벡터를 재활용함으로써, 매번 복잡한 연산을 다시 수행하여 데이터를 생성하는 것보다 훨씬 빠르게 어텐션 계산을 완료할 수 있습니다.

다음 그림은 KV 캐시 사용 유무에 따른 생성 3단계를 나란히 보여줍니다.

KV 캐시 사용 유무에 따른 텍스트 생성 비교. 상단 패널(캐시 없음)에서는 각 토큰 단계마다 키 및 값 벡터가 다시 계산되어 중복된 연산이 발생합니다. 하단 패널(캐시 있음)에서는 이전에 계산된 키와 값이 KV 캐시에서 검색되어 재계산을 피하고 더 빠른 생성을 가능하게 합니다.

따라서 코드에서 KV 캐시를 구현하려면, 평소처럼 키와 값을 계산한 다음 다음 라운드에서 검색할 수 있도록 저장하기만 하면 됩니다. 다음 섹션에서는 구체적인 코드 예시를 통해 이를 설명합니다.

## KV 캐시 직접 구현하기

KV 캐시를 구현하는 방법은 여러 가지가 있지만, 핵심 아이디어는 각 생성 단계에서 새로 생성된 토큰에 대해서만 키 및 값 텐서(tensor)를 계산하는 것입니다. 본 문서에서는 코드의 명확성과 이해도를 최우선으로 고려한 간결한 구현 방식을 제시합니다. 구체적인 코드 변경 사항은 제공된 예시 파일을 통해 직접 확인하는 것이 가장 효과적일 것입니다.

제가 GitHub에 공유한 두 개의 파일은 KV 캐시 유무에 따라 LLM을 처음부터 구현하는 독립형 파이썬(Python) 스크립트입니다.

*   `gpt_ch04.py`: 제 책 "Build a Large Language Model (From Scratch)" 3장과 4장에서 가져온 독립형 코드로, LLM을 구현하고 간단한 텍스트 생성 함수를 실행합니다.
*   `gpt_with_kv_cache.py`: 위와 동일하지만, KV 캐시를 구현하기 위해 필요한 변경 사항이 적용되었습니다.

KV 캐시 관련 코드 수정 사항을 살펴보려면 다음 중 하나를 수행할 수 있습니다.

a. `gpt_with_kv_cache.py` 파일을 열고 새로운 변경 사항을 표시하는 `# NEW` 섹션을 찾아보세요.

b. 원하는 파일 비교 도구를 통해 두 코드 파일을 확인하여 변경 사항을 비교해 보세요.

또한 구현 세부 사항을 요약하기 위해 다음 하위 섹션에서 간략한 설명을 제공합니다.

### 1. 캐시 버퍼(Cache Buffers) 등록하기

`MultiHeadAttention` 생성자 내부에, 단계별로 연결된 키와 값을 저장할 두 개의 비영구 버퍼(non-persistent buffers)인 `cache_k`와 `cache_v`를 추가합니다. 여기서 `persistent=False` 설정은 이 버퍼들이 모델의 `state_dict`에 저장되지 않음을 의미합니다. 즉, 모델을 저장하고 로드할 때 캐시 상태는 보존되지 않고, 매번 추론 세션 시작 시 초기화됩니다. 이는 캐시가 추론 시점에 동적으로 생성되는 임시 데이터이기 때문입니다.

```python
self.register_buffer("cache_k", None, persistent=False)
self.register_buffer("cache_v", None, persistent=False)
```

(버퍼에 대해 더 자세히 알고 싶으시다면, 제가 만든 유튜브 영상 "[Understanding PyTorch Buffers](https://www.youtube.com/watch?v=F_f-z71yq44)"를 시청해 보세요.)

### 2. `use_cache` 플래그(flag)를 사용한 순전파(Forward pass)

다음으로, `MultiHeadAttention` 클래스의 `forward` 메서드를 확장하여 `use_cache` 인자(argument)를 받도록 합니다. 이 부분은 KV 캐시의 핵심 논리가 구현되는 곳입니다.

```python
def forward(self, x, use_cache=False):
    b, num_tokens, d_in = x.shape

    keys_new = self.W_key(x) # Shape: (b, num_tokens, d_out)
    values_new = self.W_value(x)
    queries = self.W_query(x) #...

    if use_cache:
        if self.cache_k is None:
            self.cache_k, self.cache_v = keys_new, values_new
        else:
            self.cache_k = torch.cat([self.cache_k, keys_new], dim=1)
            self.cache_v = torch.cat([self.cache_v, values_new], dim=1)
        keys, values = self.cache_k, self.cache_v
    else:
        keys, values = keys_new, values_new
```

여기서 키와 값의 저장 및 검색은 KV 캐시의 핵심 아이디어를 구현합니다.

**저장**

구체적으로, `if self.cache_k is None: ...`를 통해 캐시가 초기화된 후, 새로 생성된 키와 값을 각각 `self.cache_k = torch.cat(...)` 및 `self.cache_v = torch.cat(...)`를 통해 캐시에 추가합니다. `torch.cat`을 사용하여 기존 캐시에 새로운 키와 값을 연결하는 방식은 직관적이지만, 대규모 배치 처리나 매우 긴 시퀀스에서는 빈번한 메모리 재할당으로 인해 성능 병목 현상이 발생할 수 있습니다. 이는 후술할 최적화 섹션에서 다룰 예정입니다.

**검색**

그런 다음, `keys, values = self.cache_k, self.cache_v`는 캐시에서 저장된 값과 키를 검색합니다. 어텐션 메커니즘은 이제 새로 계산된 쿼리와 캐시에서 가져온 전체 키/값 시퀀스를 사용하여 다음 토큰을 예측하게 됩니다.

이것이 바로 KV 캐시의 핵심 저장 및 검색 메커니즘입니다. 다음 섹션 3과 4는 사소한 구현 세부 사항을 다룹니다.

### 3. 캐시 지우기

텍스트를 생성할 때, 두 개의 별도 텍스트 생성 호출 사이에 키와 값 버퍼를 모두 재설정해야 합니다. 그렇지 않으면, 새로운 프롬프트의 쿼리(query)가 이전 시퀀스에서 남은 오래된 키에 어텐션(attend)하게 되어, 모델이 관련 없는 컨텍스트(context)에 의존하고 일관성 없는 출력을 생성하게 됩니다. 이를 방지하기 위해, 나중에 텍스트 생성 호출 사이에 사용할 수 있는 `reset_kv_cache` 메서드를 `MultiHeadAttention` 클래스에 추가합니다.

```python
def reset_cache(self):
    self.cache_k, self.cache_v = None, None
```

### 4. 전체 모델에서 `use_cache` 전파하기

`MultiHeadAttention` 클래스에 변경 사항이 적용되었으므로, 이제 `GPTModel` 클래스를 수정합니다. 먼저, 토큰 인덱스(token indices)에 대한 위치 추적을 생성자에 추가합니다.

```python
self.current_pos = 0
```

이것은 모델이 증분 생성 세션(incremental generation session) 동안 이미 몇 개의 토큰을 캐시했는지 기억하는 간단한 카운터(counter)입니다.

그런 다음, 한 줄짜리 블록 호출을 명시적인 루프(loop)로 대체하고, 각 트랜스포머 블록(transformer block)을 통해 `use_cache`를 전달합니다.

```python
def forward(self, in_idx, use_cache=False):
    # ...
    if use_cache:
        pos_ids = torch.arange(
            self.current_pos, self.current_pos + seq_len, device=in_idx.device, dtype=torch.long
        )
        self.current_pos += seq_len
    else:
        pos_ids = torch.arange(
            0, seq_len, device=in_idx.device, dtype=torch.long
        )
    pos_embeds = self.pos_emb(pos_ids).unsqueeze(0)
    x = tok_embeds + pos_embeds
    # ...
    for blk in self.trf_blocks:
        x = blk(x, use_cache=use_cache)
```

위에서 `use_cache=True`로 설정하면, `self.current_pos`에서 시작하여 `seq_len` 단계만큼 계산합니다. 그런 다음, 다음 디코딩(decoding) 호출이 중단된 지점에서 계속되도록 카운터를 증가시킵니다. `self.current_pos`를 추적하는 이유는 새로운 쿼리가 이미 저장된 키와 값 바로 뒤에 정렬되어야 하기 때문입니다. 카운터를 사용하지 않으면, 모든 새로운 단계가 다시 위치 0에서 시작하여 모델이 새로운 토큰을 이전 토큰과 겹치는 것처럼 처리할 것입니다. (대안으로, `offset = block.att.cache_k.shape[1]`을 통해 추적할 수도 있습니다.)

위 변경 사항은 `use_cache` 인자를 받도록 `TransformerBlock` 클래스에 작은 수정도 필요로 합니다.

```python
def forward(self, x, use_cache=False):
    # ...
    self.att(x, use_cache=use_cache)
```

마지막으로, 편의를 위해 모든 블록 캐시를 한 번에 지우도록 `GPTModel`에 모델 수준의 재설정 기능을 추가합니다.

```python
def reset_kv_cache(self):
    for blk in self.trf_blocks:
        blk.att.reset_cache()
    self.current_pos = 0
```

### 5. 생성 시 캐시 사용하기

`GPTModel`, `TransformerBlock`, `MultiHeadAttention`에 대한 변경 사항이 적용되었으므로, 마지막으로 간단한 텍스트 생성 함수에서 KV 캐시를 사용하는 방법은 다음과 같습니다.

```python
def generate_text_simple_cached(
    model, idx, max_new_tokens, use_cache=True
):
    model.eval()

    ctx_len = model.pos_emb.num_embeddings # max sup. len., e.g. 1024
    if use_cache:
        # Init cache with full prompt
        model.reset_kv_cache()
        with torch.no_grad():
            logits = model(idx[:, -ctx_len:], use_cache=True)

        for _ in range(max_new_tokens):
            # a) pick the token with the highest log-probability
            next_idx = logits[:, -1].argmax(dim=-1, keepdim=True)
            # b) append it to the running sequence
            idx = torch.cat([idx, next_idx], dim=1)
            # c) feed model only the new token
            with torch.no_grad():
                logits = model(next_idx, use_cache=True)
    else:
        for _ in range(max_new_tokens):
            with torch.no_grad():
                logits = model(idx[:, -ctx_len:], use_cache=False)
            next_idx = logits[:, -1].argmax(dim=-1, keepdim=True)
            idx = torch.cat([idx, next_idx], dim=1)

    return idx
```

c)에서 `logits = model(next_idx, use_cache=True)`를 통해 모델에 새로운 토큰만 공급한다는 점에 유의하세요. 캐싱이 없으면, 재사용할 저장된 키와 값이 없으므로 모델에 전체 입력 `logits = model(idx[:, -ctx_len:], use_cache=False)`를 공급합니다.

## 간단한 성능 비교

KV 캐시를 개념적 수준에서 다룬 후, 큰 질문은 작은 예시에서 실제로 얼마나 잘 작동하는가입니다. 구현을 시도해 보려면, 앞서 언급된 두 코드 파일을 파이썬 스크립트로 실행할 수 있습니다. 이 스크립트는 작은 1억 2천 4백만(124M) 매개변수(parameter) LLM을 실행하여 200개의 새로운 토큰을 생성합니다 (시작 프롬프트는 4개의 토큰 "Hello, I am"입니다).

```bash
pip install -r https://raw.githubusercontent.com/rasbt/LLMs-from-scratch/refs/heads/main/requirements.txt

python gpt_ch04.py

python gpt_with_kv_cache.py
```

M4 칩(CPU)이 탑재된 맥 미니(Mac Mini)에서 결과는 다음과 같습니다.

| Model | Runtime |
| :-------------------- | :-------- |
| `gpt_ch04.py` | 13.52s |
| `gpt_with_kv_cache.py` | 2.68s |

보시다시피, 1억 2천 4백만 개의 매개변수를 가진 비교적 소규모 모델과 200 토큰이라는 짧은 생성 길이에서도 이미 약 5배에 달하는 상당한 속도 개선을 확인할 수 있습니다. 여기서 제시된 구현은 코드의 명확성과 교육적 목적에 중점을 두었으므로, CUDA 또는 MPS와 같은 특정 하드웨어 환경에서의 최적화된 런타임 성능을 목표로 하지는 않습니다. 최적의 성능을 위해서는 텐서를 반복적으로 생성하고 결합하는 대신 사전에 메모리를 할당하는 방식이 필수적입니다. 이러한 최적화는 특히 시퀀스 길이가 길어질수록, 또는 배치(batch) 크기가 커질수록 그 효과가 더욱 커집니다.

**참고**: 모델은 두 경우 모두 "의미 없는 말(gibberish)"을 생성합니다. 즉, 다음과 같은 텍스트입니다.

```
Output text: Hello, I am Featureiman Byeswickattribute argue logger Normandy Compton analogous bore ITVEGIN ministriesysics Kle functional recountrictionchangingVirgin embarrassedgl ...
```

이는 아직 모델을 학습시키지 않았기 때문입니다. 다음 장에서는 모델을 학습시키고, 학습된 모델에 KV 캐시를 사용하여 일관성 있는 텍스트를 생성할 수 있습니다 (하지만 KV 캐시는 추론 중에만 사용하도록 되어 있습니다). 여기서는 코드를 더 간단하게 유지하기 위해 학습되지 않은 모델을 사용하고 있습니다.

하지만 더 중요한 것은 `gpt_ch04.py`와 `gpt_with_kv_cache.py` 구현이 정확히 동일한 텍스트를 생성한다는 것입니다. 이는 KV 캐시가 올바르게 구현되었음을 알려줍니다. 인덱싱(indexing) 오류는 쉽게 발생하여 다른 결과를 초래할 수 있습니다.

## KV 캐시의 장점과 단점

시퀀스의 길이가 길어질수록 KV 캐시의 이점과 한계는 더욱 명확해집니다.

*   **[이점] 연산 효율성 극대화**: 캐시를 사용하지 않을 경우, 각 어텐션 단계에서 새로운 쿼리가 이전의 모든 키와 비교되어야 하므로, 전체 연산 복잡도는 시퀀스 길이에 대해 O(N²)으로 기하급수적으로 증가합니다. 반면, KV 캐시를 활용하면 각 키와 값이 한 번만 계산되어 저장된 후 지속적으로 재사용되므로, 단계별 연산 복잡성이 O(N)으로 선형적으로 감소합니다. 이는 특히 긴 컨텍스트를 처리해야 하는 LLM에게 필수적인 최적화입니다.
*   **[한계] 메모리 점유율 증가**: 새로운 토큰이 생성될 때마다 해당 키와 값이 KV 캐시에 지속적으로 추가됩니다. 이는 특히 매우 긴 시퀀스나 대규모 LLM에서 GPU 메모리를 과도하게 점유하여 심각한 제약이 될 수 있습니다. 모델의 컨텍스트 윈도우(context window)가 넓을수록, 그리고 배치(batch) 크기가 커질수록 KV 캐시가 요구하는 메모리량은 급격히 증가합니다. 이를 완화하기 위해 캐시를 주기적으로 잘라내는(truncation) 전략을 사용할 수 있지만, 이는 구현의 복잡도를 높일 수 있습니다. 하지만 프로덕션 환경에서는 이러한 복잡성을 감수할 만한 가치가 있습니다.
*   **[한계] 동적 배치 처리의 복잡성**: KV 캐시는 각 시퀀스의 길이에 따라 캐시 크기가 달라지므로, 여러 시퀀스를 동시에 처리하는 동적 배치(dynamic batching) 환경에서 메모리 관리가 복잡해질 수 있습니다.

## KV 캐시 구현 최적화하기

위에 제시된 KV 캐시의 개념적 구현은 명확성을 돕고 주로 코드 가독성 및 교육적 목적에 맞춰져 있지만, 실제 시나리오(특히 더 큰 모델과 더 긴 시퀀스 길이에서)에 배포하려면 더 신중한 최적화가 필요합니다.

### 캐시 확장 시 흔한 문제점

*   **메모리 단편화(fragmentation) 및 반복 할당**: 앞서 보여준 것처럼 `torch.cat`을 통해 텐서(tensor)를 지속적으로 연결하는 것은 빈번한 메모리 할당 및 재할당으로 인해 성능 병목 현상(bottlenecks)을 초래합니다. 이는 특히 GPU 메모리에서 심각한 문제를 일으킬 수 있습니다.
*   **메모리 사용량의 선형적 증가**: 적절한 처리가 없으면, KV 캐시 크기는 매우 긴 시퀀스에 대해 비실용적이 됩니다. 이는 시스템의 총 메모리 용량을 빠르게 소진시킬 수 있습니다.

### 팁 1: 메모리 미리 할당하기

텐서를 반복적으로 연결하는 대신, 예상되는 최대 시퀀스 길이를 기반으로 충분히 큰 텐서를 미리 할당할 수 있습니다. 이는 일관된 메모리 사용을 보장하고 오버헤드(overhead)를 줄입니다. 의사 코드(pseudo-code)로 표현하면 다음과 같습니다.

```python
# Example pre-allocation for keys and values
max_seq_len = 1024 # maximum expected sequence length
cache_k = torch.zeros(
    (batch_size, num_heads, max_seq_len, head_dim), device=device
)
cache_v = torch.zeros(
    (batch_size, num_heads, max_seq_len, head_dim), device=device
)
```

추론 중에, 우리는 이 미리 할당된 텐서의 슬라이스(slices)에 간단히 쓸 수 있습니다. 이 방식은 메모리 단편화를 방지하고, 매번 새로운 메모리를 할당하는 오버헤드를 제거하여 성능을 크게 향상시킵니다.

### 팁 2: 슬라이딩 윈도우(Sliding Window)를 통한 캐시 자르기

GPU 메모리가 과도하게 사용되는 것을 방지하기 위해, 동적 자르기(dynamic truncation)를 포함하는 슬라이딩 윈도우 접근 방식을 구현할 수 있습니다. 슬라이딩 윈도우를 통해 캐시에는 마지막 `window_size` 토큰만 유지합니다. 이는 모델이 처리할 수 있는 최대 컨텍스트 길이를 제한하여 메모리 사용량을 제어하는 데 효과적입니다.

```python
# Sliding window cache implementation
window_size = 512
cache_k = cache_k[:, :, -window_size:, :]
cache_v = cache_v[:, :, -window_size:, :]
```

### 팁 3: PagedAttention 도입

최근 주목받는 PagedAttention은 KV 캐시 메모리 관리를 혁신적으로 개선하는 기술입니다. 기존 KV 캐시는 연속적인 메모리 블록에 저장되어 메모리 단편화와 비효율적인 할당을 초래했습니다. PagedAttention은 KV 캐시를 고정 크기의 페이지(page)로 분할하고, 이 페이지들을 불연속적인 메모리 공간에 할당합니다. 이는 운영체제가 가상 메모리를 관리하는 방식과 유사합니다.

*   **메모리 단편화 해결**: 페이지 단위로 메모리를 관리하여 단편화를 줄이고, 메모리 활용률을 극대화합니다.
*   **가변 길이 시퀀스 효율화**: 각 시퀀스에 필요한 만큼의 페이지만 할당하므로, 다양한 길이의 시퀀스를 효율적으로 배치 처리할 수 있습니다.
*   **메모리 오버헤드 감소**: 미리 할당 방식의 고정된 최대 길이에 따른 메모리 낭비를 줄여줍니다.

PagedAttention은 vLLM과 같은 고성능 LLM 추론 프레임워크에서 핵심적인 역할을 하며, 특히 대규모 LLM 서비스 환경에서 필수적인 최적화 기법으로 자리 잡았습니다.

### 실제 최적화

이 최적화는 `gpt_with_kv_cache_optimized.py` 파일에서 찾을 수 있습니다. M4 칩(CPU)이 탑재된 맥 미니에서 200 토큰 생성과 LLM의 컨텍스트 길이와 동일한 윈도우 크기(동일한 결과를 보장하여 공정한 비교를 위함)로 아래와 같이 코드 런타임(runtime)을 비교했습니다.

| Model | Runtime |
| :-------------------------------- | :-------- |
| `gpt_ch04.py` | 13.52s |
| `gpt_with_kv_cache.py` | 2.68s |
| `gpt_with_kv_cache_optimized.py` | 2.67s |

안타깝게도, 이 모델은 매우 작고 장치 전송 및 통신 오버헤드가 KV 캐시의 이점을 상회하기 때문에 CUDA 장치에서는 속도 이점이 사라집니다. 하지만 더 큰 모델과 긴 시퀀스 길이에서는 이러한 최적화가 GPU에서도 훨씬 더 큰 성능 향상을 가져올 것입니다.

## 결론

KV 캐싱은 구현에 있어 추가적인 복잡성과 메모리 관리의 필요성을 동반하지만, 특히 실제 서비스 환경에서는 이를 통해 얻는 성능 향상이 이러한 단점을 압도하는 경우가 대부분입니다. 본 글에서는 코드의 이해도와 명확성을 중시하여 설명했지만, 실제 프로덕션 시스템에서는 메모리 사전 할당이나 슬라이딩 윈도우 캐시, 그리고 PagedAttention과 같은 고도화된 최적화 기법을 적용하여 메모리 사용량 증가 문제를 효과적으로 제어하는 것이 필수적입니다. KV 캐시는 LLM이 긴 컨텍스트를 효율적으로 처리하고 실시간 응답성을 제공하는 데 있어 핵심적인 역할을 하며, 앞으로도 그 중요성은 더욱 커질 것입니다.

이 글이 유익했기를 바랍니다. 제시된 기술들을 자유롭게 실험해 보시고, 즐거운 코딩 되세요!

## 보너스: Qwen3 및 Llama 3의 KV 캐시 성능 분석

Qwen3 (0.6B) 및 Llama 3 (1B)와 같은 실제 모델에 KV 캐시를 적용한 후, 그 성능 변화를 비교하는 추가 실험을 수행했습니다. KV 캐시 구현 최적화 섹션에서 설명된 미리 할당된 텐서 대신, `torch.cat`을 사용하는 접근 방식을 선택했습니다. 이는 Llama 3와 Qwen3가 매우 큰 지원 컨텍스트 크기(각각 131k 및 41k 토큰)를 가지므로, 미리 할당된 텐서가 약 8GB에 달하는 상당한 추가 메모리를 소비하기 때문입니다. 또한, `torch.cat` 방식의 동적인 메모리 할당이 `torch.compile`과 더 잘 상호작용하도록 KV 캐시를 모델 외부로 분리하여 관리했습니다. `torch.compile`은 파이토치 코드를 최적화된 그래프로 변환하여 실행 속도를 높여주는데, 동적인 텐서 연결 작업이 모델 내부에 있을 경우 컴파일 효율이 저하될 수 있기 때문입니다.

코드는 다음 링크에서 확인할 수 있습니다:
[qwen3.py](https://github.com/rasbt/LLMs-from-scratch/blob/main/ch08/qwen3.py) | [README](https://github.com/rasbt/LLMs-from-scratch/blob/main/ch08/README.md)
[llama3.py](https://github.com/rasbt/LLMs-from-scratch/blob/main/ch08/llama3.py) | [README](https://github.com/rasbt/LLMs-from-scratch/blob/main/ch08/README.md)

성능 비교 결과는 아래 표와 같습니다:

| Model | CPU (s) | CPU (compiled) (s) | GPU (s) | GPU (compiled) (s) |
| :-------------------------------- | :------ | :----------------- | :------ | :----------------- |
| Qwen3 (no KV cache) | 16.2 | 10.1 | 0.9 | 0.8 |
| Qwen3 (with KV cache) | 3.2 | 2.5 | 0.9 | 0.8 |
| Llama 3 (no KV cache) | 18.1 | 11.2 | 1.1 | 0.9 |
| Llama 3 (with KV cache) | 3.5 | 2.7 | 1.1 | 0.9 |

결과에서 알 수 있듯이, CPU 환경에서는 KV 캐시의 적용이 가장 두드러진 성능 개선을 가져오며, `torch.compile`을 통한 추가적인 최적화는 그 효과를 더욱 증폭시킵니다. CPU는 메모리 접근 패턴과 순차적 연산에 민감하므로, KV 캐시가 제공하는 연산량 감소 효과가 크게 작용합니다. 그러나 GPU 환경에서는 미리 할당된 텐서를 사용하지 않고 모델의 규모가 상대적으로 작을 경우, 일반적인 컴파일된 모델만으로도 이미 최적의 성능을 달성할 수 있음을 관찰할 수 있습니다. 이는 GPU의 병렬 처리 능력과 높은 메모리 대역폭이 작은 모델의 `torch.cat` 오버헤드를 상쇄하고, 이미 연산이 충분히 최적화되어 KV 캐시가 제공하는 추가적인 이점이 상대적으로 작기 때문으로 해석될 수 있습니다. 하지만 대규모 모델과 매우 긴 시퀀스의 경우, GPU에서도 KV 캐시와 관련된 메모리 관리 및 최적화가 필수적입니다.

## KV 캐시의 발전 방향 및 추가 고려사항

KV 캐시는 LLM 추론 효율화의 핵심 기술이지만, 지속적인 연구와 발전이 이루어지고 있습니다.

*   **동적 KV 캐시 크기 조정(Dynamic KV Cache Sizing)**: 다양한 입력 길이와 배치 크기에 맞춰 KV 캐시의 크기를 동적으로 조절하는 기법은 메모리 활용률을 더욱 높일 수 있습니다. PagedAttention과 같은 기술이 이 영역에 속합니다.
*   **KV 캐시 양자화(Quantization of KV Cache)**: KV 캐시의 키와 값 벡터를 저정밀도(예: 8비트 정수)로 양자화하여 저장함으로써, 메모리 사용량을 크게 줄일 수 있습니다. 이는 특히 메모리 제약이 있는 엣지 디바이스(edge device)나 매우 큰 모델에서 유용합니다.
*   **추측성 디코딩(Speculative Decoding)과의 결합**: 추측성 디코딩은 작은 모델로 다음 토큰을 미리 예측한 후, 큰 모델로 이를 검증하는 방식입니다. 이때 KV 캐시는 큰 모델의 검증 과정에서 효율성을 높이는 데 중요한 역할을 합니다.
*   **메모리 계층화(Memory Tiering)**: KV 캐시의 일부를 더 느리지만 용량이 큰 메모리(예: CPU RAM)로 오프로드(offload)하고, 가장 최근에 사용된 부분만 빠른 메모리(GPU VRAM)에 유지하는 메모리 계층화 전략은 매우 긴 컨텍스트 윈도우를 지원하는 데 도움이 됩니다.

이러한 발전은 LLM이 더욱 다양한 환경에서 효율적으로 동작하고, 사용자에게 더 빠르고 일관된 경험을 제공하는 데 기여할 것입니다.