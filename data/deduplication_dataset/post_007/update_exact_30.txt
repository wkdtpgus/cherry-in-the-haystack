# LLM 추론 효율성 향상: KV 캐시의 심층 분석과 최신 최적화 기법

대규모 언어 모델(LLM)의 발전은 인공지능 분야에 혁명을 가져왔지만, 이 강력한 모델들을 실제 프로덕션 환경에서 효율적으로 운영하는 것은 여전히 중요한 과제입니다. 특히 추론(inference) 단계에서 발생하는 막대한 계산량과 메모리 요구사항은 LLM 서비스의 확장성과 비용 효율성에 큰 영향을 미칩니다. 이러한 도전을 해결하기 위한 핵심 기술 중 하나가 바로 KV 캐시입니다. 이 글에서는 KV 캐시가 개념적으로 어떻게 작동하는지 다시 한번 심층적으로 살펴보고, 최신 LLM 아키텍처에서 KV 캐시가 어떻게 진화하고 적용되는지, 그리고 실제 구현 시 고려해야 할 고급 최적화 기법들을 함께 다룹니다.

## 개요

요약하자면, KV 캐시는 추론(inference) 시 (학습 후) 재사용을 위해 중간 키(K) 및 값(V) 계산 결과를 저장하며, 이는 텍스트 생성 시 상당한 속도 향상을 가져옵니다. KV 캐시의 단점은 코드에 더 많은 복잡성을 추가하고, 메모리 요구 사항을 증가시키며(제가 처음 이 책에 포함하지 않은 주된 이유), 학습(training) 중에는 사용할 수 없다는 것입니다. 하지만 LLM을 프로덕션 환경에서 사용할 때 추론 속도 향상은 코드 복잡성과 메모리 측면의 단점을 상쇄하고도 남는 경우가 많습니다.

## KV 캐시란 무엇인가요?

LLM이 텍스트를 생성하고 있다고 상상해 보세요. 구체적으로, LLM에 "Time"이라는 프롬프트(prompt)가 주어졌다고 가정해 봅시다. 이미 아시다시피, LLM은 한 번에 한 단어(또는 토큰(token))를 생성하며, 다음 두 텍스트 생성 단계는 아래 그림과 같이 나타날 수 있습니다.

이 다이어그램은 LLM이 한 번에 하나의 토큰을 생성하는 방식을 보여줍니다. "Time"이라는 프롬프트로 시작하여, 모델은 다음 토큰인 "flies"를 생성합니다. 다음 단계에서는 전체 시퀀스(sequence) "Time flies"가 다시 처리되어 "fast" 토큰을 생성합니다.

다음 그림에서 강조된 바와 같이, 생성된 LLM 텍스트 출력에는 약간의 중복(redundancy)이 있습니다.

이 그림은 각 생성 단계에서 LLM이 다시 처리해야 하는 반복되는 컨텍스트("Time flies")를 강조합니다. LLM은 중간 키/값 상태를 캐시(cache)하지 않으므로, 새로운 토큰(예: "fast")이 생성될 때마다 전체 시퀀스를 다시 인코딩(re-encodes)합니다.

## 어텐션 메커니즘의 핵심과 KV 캐시

트랜스포머(Transformer) 모델의 핵심인 셀프 어텐션(Self-Attention) 메커니즘은 입력 시퀀스의 각 토큰이 다른 모든 토큰과 얼마나 관련되어 있는지를 계산합니다. 이 과정은 쿼리(Query, Q), 키(Key, K), 값(Value, V) 세 가지 벡터를 사용하여 이루어집니다. 특정 토큰의 출력은 해당 토큰의 쿼리 벡터와 다른 모든 토큰의 키 벡터 사이의 유사성(내적)을 계산하여 어텐션 가중치를 얻고, 이 가중치를 해당 토큰의 값 벡터에 곱하여 합산하는 방식으로 이루어집니다. 수학적으로는 `Attention(Q, K, V) = softmax(Q K^T / sqrt(d_k)) V`로 표현됩니다.

LLM이 텍스트를 생성할 때, 자기회귀(autoregressive) 방식으로 한 번에 하나의 토큰을 생성합니다. 새 토큰을 생성할 때마다 이전까지 생성된 모든 토큰을 포함한 전체 시퀀스를 다시 입력으로 받아 어텐션 계산을 수행해야 합니다. 이때, Q, K, V 벡터는 매번 새로 계산되는데, 이전 토큰들에 대한 K와 V 벡터는 사실상 변하지 않습니다. KV 캐시는 바로 이 점에 착안하여, 매번 동일한 K와 V 벡터를 다시 계산하는 낭비를 줄이는 데 기여합니다.

## KV 캐시의 작동 원리

이전 두 그림을 비교해 보면, 처음 두 토큰에 대한 키 및 값 벡터가 정확히 동일하며, 다음 토큰 텍스트 생성 라운드마다 이를 다시 계산하는 것은 낭비일 것입니다.

이제 KV 캐시의 아이디어는 이전에 생성된 키 및 값 벡터를 재사용하기 위해 저장하는 캐싱 메커니즘(caching mechanism)을 구현하여 이러한 불필요한 재계산을 피하는 데 도움을 주는 것입니다.

## KV 캐시를 사용한 텍스트 생성 흐름

KV 캐시가 없는 일반적인 텍스트 생성 과정에서는 "Time flies fast"와 같은 문장을 생성할 때, 각 단계마다 전체 시퀀스를 다시 처리해야 합니다. 예를 들어, "Time" 다음에 "flies"를 생성할 때 "Time"의 K, V를 계산하고, "Time flies" 다음에 "fast"를 생성할 때 "Time"과 "flies"의 K, V를 다시 계산합니다. 이 반복적인 계산은 시퀀스 길이가 길어질수록 비효율성이 기하급수적으로 증가하게 만듭니다.

KV 캐시를 적용하면 이 과정이 훨씬 효율적으로 변합니다.

1.  **첫 토큰 처리**: "Time"이라는 프롬프트가 주어지면, 모델은 "Time"에 대한 키(K) 및 값(V) 벡터를 계산하고 이를 KV 캐시에 저장합니다.
2.  **두 번째 토큰 생성**: 다음 토큰 "flies"를 생성할 때, 모델은 "flies"에 대한 K와 V 벡터만 새로 계산합니다. 이때, 어텐션 계산에 필요한 "Time"의 K와 V 벡터는 캐시에서 가져와 재사용합니다. 새로 계산된 "flies"의 K와 V도 캐시에 추가됩니다.
3.  **세 번째 토큰 생성**: 다음 토큰 "fast"를 생성할 때, "fast"에 대한 K와 V 벡터만 새로 계산하고, "Time"과 "flies"의 K, V는 캐시에서 재사용합니다. "fast"의 K와 V도 캐시에 추가됩니다.

이러한 방식으로, 이미 처리된 토큰에 대한 K, V 벡터는 한 번만 계산되고 지속적으로 재사용되므로, 각 단계에서 새로 추가된 토큰에 대한 계산만 수행하면 됩니다. 이는 추론 속도를 획기적으로 향상시킬 수 있는 핵심적인 최적화 기법입니다.

## KV 캐시 구현의 기본 원리

KV 캐시를 직접 구현하는 것은 LLM의 내부 작동 방식에 대한 이해를 깊게 하는 데 도움이 됩니다. 핵심 아이디어는 어텐션 계층(attention layer)에서 계산된 키(K)와 값(V) 텐서(tensor)를 다음 추론 단계에서 재사용할 수 있도록 저장하는 것입니다.

주요 구현 원리는 다음과 같습니다:

1.  **캐시 버퍼 등록 및 관리**: 어텐션 모듈 내부에 `cache_k`와 `cache_v`와 같은 버퍼를 생성하여 키와 값 텐서를 저장합니다. 이때 PyTorch의 `register_buffer`와 같이 모델의 상태에 포함되지만 학습되지 않는 형태로 관리하는 것이 일반적입니다.
2.  **순전파(Forward pass) 시 캐시 로직**: 모델의 `forward` 메서드에 `use_cache` 플래그를 추가합니다. 이 플래그가 `True`일 경우, 새로 계산된 K, V 텐서를 기존 캐시 텐서에 연결(concatenate)하여 누적합니다. (`torch.cat` 활용) 이후 어텐션 계산 시에는 누적된 캐시 텐서를 사용합니다. `use_cache`가 `False`일 경우에는 캐시를 사용하지 않고 매번 K, V를 새로 계산합니다.
3.  **캐시 재설정(Reset Cache)**: 새로운 텍스트 생성을 시작할 때마다 이전 시퀀스의 캐시가 남아있지 않도록 캐시 버퍼를 초기화하는 `reset_cache` 메서드를 구현해야 합니다. 이는 모델이 이전 컨텍스트에 의존하여 잘못된 출력을 생성하는 것을 방지합니다.
4.  **모델 전체에 `use_cache` 인자 전파**: `MultiHeadAttention` 모듈뿐만 아니라, 이를 포함하는 `TransformerBlock`, 그리고 최종 `GPTModel` 클래스까지 `use_cache` 인자를 전달하고 `self.current_pos`와 같은 토큰 위치 추적 변수를 관리하여, 캐시가 올바른 위치에 저장되고 검색되도록 합니다.

더 자세한 코드 구현 예시는 원본 글에서 언급된 `gpt_with_kv_cache.py` 파일을 참조하면 좋습니다.

## 고급 KV 캐시 최적화 기법: PagedAttention의 등장

위에서 설명한 기본적인 KV 캐시 구현은 명확하고 교육적이지만, 실제 프로덕션 환경, 특히 여러 사용자의 요청을 동시에 처리하는 서버 환경에서는 몇 가지 한계가 있습니다. 가장 큰 문제는 메모리 단편화(fragmentation)와 가변적인 시퀀스 길이에 대한 비효율적인 메모리 관리입니다.

이러한 문제를 해결하기 위해 등장한 혁신적인 기술 중 하나가 바로 vLLM에서 제안된 **PagedAttention**입니다. PagedAttention은 운영체제의 가상 메모리/페이징(paging) 개념을 LLM의 KV 캐시에 적용합니다.

*   **블록 기반 메모리 관리**: KV 캐시를 고정된 크기의 블록으로 나누고, 이 블록들을 불연속적인 메모리 공간에 할당합니다. 이는 GPU 메모리 단편화를 줄이고 메모리 활용률을 극대화합니다.
*   **논리적/물리적 블록 매핑**: 각 시퀀스는 논리적인 블록 ID 시퀀스로 표현되고, 이 논리적 블록은 GPU 메모리의 물리적 블록에 매핑됩니다. 이를 통해 가변 길이의 시퀀스도 효율적으로 처리할 수 있으며, 시퀀스 길이가 길어져도 연속적인 메모리 할당 없이 유연하게 확장될 수 있습니다.
*   **메모리 공유**: 동일한 프롬프트로 여러 시퀀스가 시작될 경우, PagedAttention은 프롬프트 부분의 KV 캐시 블록을 여러 시퀀스 간에 공유하여 메모리 사용량을 더욱 절감합니다.

PagedAttention은 동적 배치(dynamic batching)와 결합될 때 엄청난 성능 향상을 가져오며, LLM 서빙 시스템의 처리량(throughput)을 크게 개선하는 데 기여했습니다.

## 다른 효율성 향상 기법: GQA 및 KV 캐시 양자화

KV 캐시의 효율성을 더욱 높이기 위한 아키텍처적 개선과 기술적 접근 방식도 활발히 연구되고 있습니다.

*   **Grouped Query Attention (GQA) 및 Multi-Query Attention (MQA)**:
    전통적인 멀티 헤드 어텐션(Multi-Head Attention)은 각 어텐션 헤드마다 독립적인 쿼리, 키, 값 행렬을 가집니다. 이는 모델의 표현력을 높이지만, 각 헤드가 독립적인 K와 V를 가지므로 KV 캐시의 크기가 헤드 수에 비례하여 증가합니다. GQA와 MQA는 이러한 문제를 해결하기 위해 여러 쿼리 헤드가 동일한 키 및 값 헤드를 공유하도록 합니다. MQA는 모든 쿼리 헤드가 하나의 K/V 헤드를 공유하고, GQA는 여러 쿼리 헤드가 그룹을 이루어 K/V 헤드를 공유합니다. 이는 KV 캐시의 크기를 크게 줄여 메모리 대역폭 요구사항을 완화하고 추론 속도를 향상시킵니다. Llama 2, Gemma, Mixtral 등 최신 LLM 아키텍처에 널리 채택되고 있습니다.
*   **KV 캐시 양자화(Quantization)**:
    KV 캐시가 차지하는 메모리 공간을 줄이는 또 다른 방법은 양자화입니다. 일반적으로 K와 V 텐서는 16비트 부동소수점(FP16) 또는 32비트 부동소수점(FP32)으로 저장되지만, 이를 8비트(INT8) 또는 4비트(INT4) 정수로 압축하여 저장하면 메모리 사용량을 획기적으로 줄일 수 있습니다. 양자화는 약간의 정확도 손실을 초래할 수 있지만, 대부분의 경우 LLM의 성능에 큰 영향을 미치지 않으면서도 메모리 절감 효과가 매우 커 대규모 모델의 배포에 필수적인 기술로 자리 잡고 있습니다.

## 실제 프로덕션 환경에서의 고려사항

KV 캐시 최적화는 단순히 코드 몇 줄을 추가하는 것을 넘어, 실제 서비스 환경의 복잡성을 이해하고 통합적인 접근 방식을 요구합니다.

*   **동적 배치(Dynamic Batching)와의 시너지**: 여러 사용자의 요청이 동시에 들어올 때, 동적 배칭은 가변적인 길이의 시퀀스들을 효율적으로 묶어 GPU 활용률을 높입니다. PagedAttention과 같은 고급 KV 캐시 기법은 동적 배칭 환경에서 메모리 단편화 없이 KV 캐시를 효율적으로 관리하여 처리량을 극대화합니다.
*   **지연 시간(Latency) 및 처리량(Throughput) 최적화**: KV 캐시는 단일 요청의 지연 시간을 줄이는 데 기여하지만, 시스템 전체의 처리량을 높이려면 동시 요청 처리 능력을 극대화해야 합니다. PagedAttention, GQA/MQA, KV 캐시 양자화는 이 두 가지 목표를 동시에 달성하는 데 필수적입니다.
*   **하드웨어 가속기(GPU, NPU)의 역할**: KV 캐시의 효율적인 관리는 GPU 메모리의 성능에 크게 의존합니다. 고대역폭 메모리(HBM)를 갖춘 최신 GPU는 KV 캐시의 접근 속도를 높여 전체 추론 성능을 향상시킵니다. 특정 하드웨어에 최적화된 라이브러리(예: NVIDIA TensorRT-LLM)는 이러한 최적화 기법들을 통합하여 최고의 성능을 제공합니다.

## 간단한 성능 비교

KV 캐시의 효과는 모델의 크기, 시퀀스 길이, 그리고 하드웨어 환경에 따라 크게 달라집니다. 일반적으로 CPU 환경에서는 KV 캐시가 훨씬 더 큰 성능 향상을 가져오는데, 이는 CPU의 메모리 대역폭이 GPU보다 제한적이고 캐싱의 이점이 더욱 두드러지기 때문입니다. GPU 환경에서도 물론 성능 향상이 있지만, GPU는 병렬 처리 능력이 뛰어나고 메모리 대역폭이 넓으므로, 모델이 작거나 시퀀스 길이가 짧을 때는 KV 캐시 적용으로 인한 오버헤드(텐서 할당 및 연결 등)가 이점을 상쇄할 수도 있습니다.

원본 글의 실험 결과에서 보듯이, 작은 1억 2천 4백만(124M) 매개변수 모델에서도 KV 캐시 적용 시 CPU에서 약 5배의 속도 향상을 보였습니다. 이는 실제 대규모 모델과 더 긴 시퀀스 길이에서는 훨씬 더 극적인 성능 차이로 이어질 수 있음을 시사합니다. 최적화된 KV 캐시 구현은 `torch.cat`과 같은 빈번한 메모리 재할당을 피하고 미리 할당된 메모리 블록을 사용함으로써 추가적인 성능 향상을 가져올 수 있습니다.

## KV 캐시의 장점과 단점

시퀀스 길이가 증가함에 따라 KV 캐시의 장점과 단점은 다음과 같은 방식으로 더욱 두드러집니다.

*   **[장점] 계산 효율성 증가**: 캐싱이 없으면, t 단계의 어텐션은 새로운 쿼리를 t개의 이전 키와 비교해야 하므로, 누적 작업은 O(n²)으로 2차적으로 증가합니다. 캐시를 사용하면 각 키와 값이 한 번 계산된 후 재사용되어, 단계별 총 복잡성이 O(n)으로 선형적으로 감소합니다.
*   **[단점] 메모리 사용량 선형적으로 증가**: 각 새로운 토큰은 KV 캐시에 추가됩니다. 긴 시퀀스와 더 큰 LLM의 경우, 누적 KV 캐시가 커져 상당하거나 심지어 감당할 수 없는 양의 (GPU) 메모리를 소비할 수 있습니다. 해결책으로 KV 캐시를 잘라낼 수 있지만, 이는 더 많은 복잡성을 추가합니다 (하지만 LLM을 배포할 때는 그만한 가치가 있을 수 있습니다).

## KV 캐시 구현 최적화하기

위에 제시된 KV 캐시의 개념적 구현은 명확성을 돕고 주로 코드 가독성 및 교육적 목적에 맞춰져 있지만, 실제 시나리오(특히 더 큰 모델과 더 긴 시퀀스 길이에서)에 배포하려면 더 신중한 최적화가 필요합니다.

### 캐시 확장 시 흔한 문제점

*   **메모리 단편화(fragmentation) 및 반복 할당**: 앞서 보여준 것처럼 `torch.cat`을 통해 텐서(tensor)를 지속적으로 연결하는 것은 빈번한 메모리 할당 및 재할당으로 인해 성능 병목 현상(bottlenecks)을 초래합니다.
*   **메모리 사용량의 선형적 증가**: 적절한 처리가 없으면, KV 캐시 크기는 매우 긴 시퀀스에 대해 비실용적이 됩니다.

### 팁 1: 메모리 미리 할당하기

텐서를 반복적으로 연결하는 대신, 예상되는 최대 시퀀스 길이를 기반으로 충분히 큰 텐서를 미리 할당할 수 있습니다. 이는 일관된 메모리 사용을 보장하고 오버헤드(overhead)를 줄입니다. 의사 코드(pseudo-code)로 표현하면 다음과 같습니다.

```python
# Example pre-allocation for keys and values
max_seq_len = 1024 # maximum expected sequence length
cache_k = torch.zeros(
    (batch_size, num_heads, max_seq_len, head_dim), device=device
)
cache_v = torch.zeros(
    (batch_size, num_heads, max_seq_len, head_dim), device=device
)
```

추론 중에, 우리는 이 미리 할당된 텐서의 슬라이스(slices)에 간단히 쓸 수 있습니다.

### 팁 2: 슬라이딩 윈도우(Sliding Window)를 통한 캐시 자르기

GPU 메모리가 과도하게 사용되는 것을 방지하기 위해, 동적 자르기(dynamic truncation)를 포함하는 슬라이딩 윈도우 접근 방식을 구현할 수 있습니다. 슬라이딩 윈도우를 통해 캐시에는 마지막 `window_size` 토큰만 유지합니다.

```python
# Sliding window cache implementation
window_size = 512
cache_k = cache_k[:, :, -window_size:, :]
cache_v = cache_v[:, :, -window_size:, :]
```

### 실제 최적화

이 최적화는 `gpt_with_kv_cache_optimized.py` 파일에서 찾을 수 있습니다. M4 칩(CPU)이 탑재된 맥 미니에서 200 토큰 생성과 LLM의 컨텍스트 길이와 동일한 윈도우 크기(동일한 결과를 보장하여 공정한 비교를 위함)로 아래와 같이 코드 런타임(runtime)을 비교했습니다.

| Model | Runtime |
| :-------------------------------- | :-------- |
| `gpt_ch04.py` | 13.52s |
| `gpt_with_kv_cache.py` | 2.68s |
| `gpt_with_kv_cache_optimized.py` | 2.67s |

안타깝게도, 이 모델은 매우 작고 장치 전송 및 통신 오버헤드가 KV 캐시의 이점을 상회하기 때문에 CUDA 장치에서는 속도 이점이 사라집니다.

## 결론

KV 캐시는 LLM의 추론 효율성을 혁신적으로 개선하는 필수적인 기술입니다. 단순한 개념에서 시작하여 PagedAttention, GQA, KV 캐시 양자화와 같은 고급 최적화 기법으로 발전하면서, LLM을 더욱 빠르고 비용 효율적으로 배포할 수 있게 되었습니다. 이러한 기술들은 LLM 기반 애플리케이션의 지연 시간을 줄이고 처리량을 늘려, 더 많은 사용자가 AI의 혜택을 누릴 수 있도록 돕습니다.

물론 KV 캐싱은 추가적인 복잡성과 메모리 고려 사항을 도입하지만, 효율성에서 얻는 눈에 띄는 이점은 특히 프로덕션 환경에서 이러한 단점을 상회하는 경우가 많습니다. 코드 가독성을 우선시한 기본 구현에서 시작하여, 실제 환경에서는 메모리 미리 할당, 슬라이딩 윈도우, 그리고 PagedAttention과 같은 신중한 최적화가 필요하다는 점을 기억해야 합니다. LLM 기술이 계속 발전함에 따라, KV 캐시와 관련된 최적화 기술 또한 더욱 정교해지고 발전할 것입니다.

## 보너스: 최신 LLM 아키텍처와 KV 캐시 전략

최근 출시된 Qwen3 (0.6B) 및 Llama 3 (1B)와 같은 최신 LLM들은 KV 캐시를 효율적으로 활용하기 위한 다양한 전략을 채택하고 있습니다. 이 모델들은 매우 큰 컨텍스트 길이(Llama 3는 131k, Qwen3는 41k 토큰)를 지원하므로, 단순히 KV 캐시 텐서를 미리 할당하는 방식은 막대한 메모리를 소비할 수 있습니다. 예를 들어, 미리 할당된 텐서는 약 8GB의 추가 메모리를 필요로 할 수 있어 비효율적입니다.

이러한 이유로, 일부 구현에서는 메모리 효율성을 위해 `torch.cat`을 사용하여 텐서를 동적으로 확장하는 방식을 선호하기도 합니다. 또한, 계산 효율성 향상을 위해 KV 캐시 로직을 모델 외부로 분리하고 `torch.compile`과 같은 기술로 모델을 컴파일하여 최적의 성능을 끌어냅니다.

원본 글의 실험 결과에서 볼 수 있듯이, CPU 환경에서는 KV 캐시가 가장 큰 속도 향상을 가져오며, `torch.compile`을 통해 그 성능을 더욱 향상시킬 수 있습니다. 하지만 GPU 환경에서는 모델의 크기가 상대적으로 작고 텐서를 미리 할당하지 않는 `torch.cat` 방식의 오버헤드 때문에, KV 캐시의 직접적인 성능 향상 효과가 CPU만큼 두드러지지 않을 수도 있습니다. 이는 GPU의 병렬 처리 능력과 메모리 대역폭이 뛰어나기 때문으로 해석될 수 있습니다.

이 매거진은 개인적인 열정 프로젝트입니다. 독립 연구자로서 저를 지원하시려면, 제 책 "[Build a Large Language Model (From Scratch)](https://www.amazon.com/Build-Large-Language-Model-Scratch/dp/B0C91461Q3)"을 구매하시거나 [유료 구독](https://magazine.sebastianraschka.com/subscribe)을 신청해 주시면 감사하겠습니다.

[Build a Large Language Model (From Scratch)](https://www.amazon.com/Build-Large-Language-Model-Scratch/dp/B0C91461Q3) 아마존에서 구매 가능

책을 읽으셨고 잠시 시간이 있으시다면, [짧은 리뷰](https://www.amazon.com/Build-Large-Language-Model-Scratch/dp/B0C91461Q3)를 남겨주시면 정말 감사하겠습니다. 이는 저희 저자들에게 큰 도움이 됩니다!

여러분의 지원은 큰 의미가 있습니다! 감사합니다!