OpenAI가 내놓은 GPT-OSS는 이전 GPT-2가 공개된 지 약 6년 만에 등장한 중요한 개방형 대규모 언어 모델입니다. 그동안 LLM의 역량은 비약적으로 발전해왔습니다. 이 모델 자체가 DeepSeek, Qwen, Kimi와 같은 현존하는 개방형 모델들에 비해 성능 면에서 획기적인 도약을 이룬 것은 아닐 수 있으나, 그간 LLM 분야의 변화 양상을 되짚어보는 의미 있는 계기를 마련해 줍니다.

**이전 개방형 GPT 모델과의 차이점**
GPT-OSS는 이전 버전들과 마찬가지로 한 번에 하나의 심볼을 순차적으로 만들어내는 자기회귀적 변환기(autoregressive Transformer) 구조를 채택하고 있습니다.

Language Models & Co.를 읽어주셔서 대단히 감사합니다! 새로운 콘텐츠를 무료로 받아보고 저의 작업을 지지해 주시려면 구독해 주세요.

구독하기

2025년 중반 현재, 거대 언어 모델들이 다루는 문제의 복잡성은 눈에 띄게 증가했습니다. 이는 다음 역량들이 강화되었기 때문입니다.

*   **고도화된 도구 활용**: 외부 시스템과 연동하여 실세계 데이터를 처리하고 특정 작업을 수행하는 능력이 비약적으로 발전했습니다. 예를 들어, 데이터베이스 쿼리, 웹 검색, API 호출 등을 통해 모델의 지식 범위를 확장합니다.
*   **심층적인 논리적 추론**: 단순히 정보를 나열하는 것을 넘어, 복잡한 문제 상황에서 여러 정보 조각을 연결하고 논리적 단계를 거쳐 결론에 도출하는 능력이 강화되었습니다. 이는 법률 문서 분석이나 과학 연구 가설 생성과 같은 분야에서 특히 중요합니다.
*   **정교한 문제 해결 및 코드 생성**: 특정 문제에 대한 해결책을 제시하고, 다양한 프로그래밍 언어로 동작 가능한 코드를 작성하며, 기존 코드를 디버깅하거나 최적화하는 데 탁월한 성능을 보입니다. 이는 소프트웨어 개발 주기를 단축하고 개발자의 생산성을 향상시키는 데 기여합니다.
*   **다중 모드(Multimodal) 처리 능력**: 텍스트뿐만 아니라 이미지, 오디오, 비디오 등 다양한 형태의 데이터를 이해하고 생성하는 능력이 점차 중요해지고 있습니다. GPT-OSS는 비록 텍스트 기반 모델이지만, 이러한 다중 모드 처리 능력을 다른 보조 시스템과 결합하여 확장할 수 있는 가능성을 열어줍니다.

아래 도해를 통해 우리는 현존하는 유수의 개방형 모델들과 크게 다르지 않은 주요 구조적 특성들을 확인할 수 있습니다. GPT-2와 비교했을 때 GPT-OSS의 핵심적인 구조적 차이점은 바로 MoE(mixture-of-experts), 즉 전문가 혼합 모델이라는 점입니다. MoE 구조는 모델이 특정 입력에 대해 여러 개의 '전문가' 네트워크 중 가장 적합한 하나 또는 소수를 활성화하여 연산을 수행하게 함으로써, 전체 모델 파라미터(parameter) 수는 크지만 실제 추론 시 활성화되는 파라미터는 적어 효율성을 높이는 방식입니다. 이는 방대한 지식을 포괄하면서도 특정 작업에 대한 전문성을 유지하고, 연산 비용을 절감하는 데 유리합니다. 예를 들어, 코딩 관련 질문에는 코딩 전문가가, 역사 관련 질문에는 역사 전문가가 주로 활성화되어 응답하는 식입니다. 이러한 방식은 모델의 확장성과 유연성을 크게 향상시킵니다. 구조적 측면에 대해 더 깊이 탐구하고 싶으시다면, 저희의 무료 강좌 *How Transformer LLMs Work*에서 세부적인 내용과 풍부한 시각 자료(및 독점 애니메이션!)를 상세히 다루고 있습니다. 해당 강좌에서 어텐션 메커니즘(attention mechanism) 설명을 위해 활용된 시각적 표현 방식을 적용하면, GPT-OSS의 변환기 블록(Transformer Block)은 다음 그림과 같이 나타낼 수 있습니다. 이러한 구조적 세부 사항들이 특별히 혁신적인 것은 아니며, 이는 최신 기술 수준(SoTA)의 개방형 MoE 모델들에서 보편적으로 발견되는 특징들입니다.

**메시지 서식 지정(Message Formatting)**
대다수 사용자들에게는 모델의 논리 전개 방식(reasoning)과 도구 활용(tool calls)의 작동 원리 및 서식(formatting) 관련 상세 내용이 내부 구조(architecture)보다 더욱 중요합니다. 아래 도해에서 이 모델의 입력과 출력 데이터의 구조를 파악할 수 있습니다.

**메시지 및 출력 채널(Output Channels)**
오픈 소스 LLM의 세 가지 주요 사용자 유형을 살펴보면서 이를 분석해 봅시다.

*   **LLM 애플리케이션의 최종 사용자(End-users)**
    *   예시: 챗봇(chatbot) 인터페이스 사용자
    *   이들은 주로 자신이 입력하는 질의(query)와 모델이 제공하는 최종 답변에 집중합니다. 중간 과정의 복잡성을 최소화하여 직관적인 경험을 추구합니다.
    *   일부 고급 애플리케이션에서는 모델이 답변을 도출하기까지의 중간 사고 과정(interim reasoning traces)을 선택적으로 노출하여 사용자에게 투명성과 신뢰성을 제공하기도 합니다.
*   **LLM 애플리케이션 개발자(Builders)**
    *   예시: 코드 생성 도구(e.g., Cursor) 또는 가상 비서(e.g., Manus) 개발자
    *   **입력 메시지(Input messages)**: 개발자는 모델의 전반적인 행동 양식, 안전 필터링 기준, 추론의 깊이, 그리고 외부 도구의 정의 등을 제어하기 위해 시스템 프롬프트(system prompt)와 개발자 메시지(developer messages)를 세밀하게 구성합니다. 이는 모델이 의도된 방식으로 작동하도록 설계하는 데 필수적입니다.
    *   또한, 사용자 질의(user message)에 대한 정교한 프롬프트 엔지니어링(prompt engineering)과 대화 맥락(context) 관리 기법을 통해 모델의 응답 품질과 관련성을 극대화합니다.
    *   **출력 메시지(Output messages)**: 개발자는 모델이 생성한 추론 흔적(reasoning traces)을 최종 사용자에게 보여줄지 여부를 결정합니다. 이는 사용자 경험의 복잡도와 모델의 투명성 사이에서 균형을 잡는 중요한 선택입니다. 더불어, 모델이 호출할 수 있는 도구들을 명확히 정의하고, 특정 작업에 필요한 추론의 강도를 조절하여 모델의 효율성과 정확성을 최적화합니다.
*   **LLM 사후 훈련 전문가(Post-trainers)**
    *   모델의 성능을 특정 도메인이나 작업에 맞춰 미세 조정(fine-tune)하는 고급 사용자 그룹입니다.
    *   이들은 모델의 추론 과정, 도구 호출의 결과, 그리고 최종 응답을 포함한 모든 유형의 메시지 데이터와 올바른 형식의 상호작용 데이터를 필요로 합니다. 이는 모델의 학습 데이터셋을 구성하고 성능을 개선하는 데 결정적인 역할을 합니다.

이처럼 LLM 앱 구축자(builders)와 후속 훈련 담당자(post-trainers)와 같은 후자 두 집단은 보조 메시지(assistant messages)에 내재된 채널 개념을 파악함으로써 상당한 이득을 얻을 수 있습니다. 이 개념은 OpenAI의 Harmony 저장소(repo)에 이미 구현되어 있습니다.

(이러한 종류의 설명 방식이 유익하다고 느끼셨다면, 거대 언어 모델(LLM)을 이처럼 심도 깊게 해설하는 300개가 넘는 그림을 담은 베스트셀러 서적과 현재 14,000개 이상의 스타를 획득한 GitHub 저장소(repo)를 꼭 확인해 보시기 바랍니다.) 서적의 공식 웹사이트는 여기입니다. 아마존(Amazon)에서도 해당 서적을 구매하실 수 있습니다. 모든 프로그램 코드는 GitHub에 게시되어 있습니다.

**메시지 채널(Message Channels)**
모델이 생성하는 모든 출력은 '어시스턴트 메시지(assistant messages)'로 분류됩니다. 모델은 메시지 유형을 명확히 구분하기 위해 각각을 특정 '채널(channel)' 범주에 할당합니다.

*   **`analysis` 채널**: 주로 모델의 심층적인 사고 과정(reasoning)과 특정 도구 호출(tool calls)의 중간 단계에 사용됩니다. 이 채널은 모델이 복잡한 문제를 해결하기 위해 어떤 정보들을 분석하고 어떤 논리적 단계를 거치는지 개발자에게 보여줌으로써, 디버깅(debugging) 및 모델 행동 이해에 도움을 줍니다. 예를 들어, 수학 문제 풀이 시 각 단계별 계산 과정이 이 채널을 통해 전달될 수 있습니다.
*   **`narration` 채널**: 주로 기능 호출(functional calling)의 실행과 관련된 정보, 그리고 대부분의 도구 호출(tool calls) 결과가 이 채널을 통해 전달됩니다. 이는 모델이 외부 도구를 성공적으로 호출했는지, 그리고 그 결과가 무엇인지를 명확히 알려줍니다. 예를 들어, "날씨 정보를 검색했습니다"와 같은 설명이나, API 호출의 응답 데이터가 여기에 해당합니다.
*   **`final` 채널**: 최종적으로 사용자에게 제공될 답변을 포함하는 메시지입니다. 이 채널의 내용은 간결하고 명확하며, 사용자가 필요로 하는 핵심 정보를 담고 있어야 합니다.

따라서, 모델에게 복잡한 추론과 여러 도구 사용이 필요한 프롬프트(prompt)를 제공했을 때, 다음 그림은 세 가지 메시지 유형이 모두 활용된 대화 흐름을 예시합니다. 여기서 '턴(turn) 2'와 '턴 4'는 앞선 도구 호출에 대한 응답이 될 것이므로, 실제 모델의 발화는 '턴 1', '턴 3', '턴 5'로 표시됩니다. 최종 사용자가 접하게 될 내용은 바로 'final' 채널의 메시지입니다.

**추론(Reasoning)**
논리적 사고(reasoning) 능력은 숙련된 사용자(advanced users)들이 신중하게 고려해야 할 상충 관계(trade-offs)를 수반합니다. 즉, 더 깊은 추론 과정은 모델이 당면한 과제를 숙고하고 해결책을 모색할 충분한 시간과 연산 자원(compute)을 부여하여, 더욱 난해한 문제들을 풀어내는 데 기여합니다. 하지만 이와 동시에 응답 지연 시간(latency)과 추가적인 연산 비용(compute cost)을 발생시킵니다. 이러한 선택의 여지는 강력한 추론 능력을 가진 거대 언어 모델과 그렇지 않은 모델들이 각기 다른 유형의 문제 해결에 최적화되어 공존한다는 사실에서 명확히 드러납니다.

이러한 상충 관계를 조율하기 위한 한 가지 방안은 특정 '추론 예산(reasoning budget)'에 따라 응답하는 모델을 활용하는 것입니다. GPT-OSS가 바로 이 범주에 속합니다. 시스템 메시지(system message)를 통해 '낮음(low)', '중간(medium)', '높음(high)' 세 가지 추론 모드(reasoning mode)를 설정할 수 있습니다. 모델 카드(model card)의 그림 3은 이 설정이 벤치마크(benchmarks) 점수에 어떤 영향을 미치는지, 그리고 '사고의 사슬(chain-of-thought, CoT)'이라고도 불리는 추론 흔적(reasoning traces)에 얼마나 많은 토큰(token)이 포함되는지를 명확히 보여줍니다. 이는 Qwen3의 추론 모드와는 대조적입니다. Qwen3는 '사고(thinking)'와 '비사고(non-thinking)'라는 이진적인 모드를 제공합니다. Qwen3의 사고 모드에서는 특정 토큰 임계값을 초과하면 사고를 중단하도록 설정하는 방법을 제시하며, 이것이 다양한 추론 벤치마크 점수에 미치는 영향을 보고합니다. 반면 GPT-OSS는 보다 세분화된 제어를 통해 사용자가 필요한 만큼의 추론 깊이를 유연하게 선택할 수 있도록 합니다. 예를 들어, 간단한 질문에는 '낮음' 모드를 사용하여 빠른 응답을 얻고, 복잡한 분석이 필요한 경우에는 '높음' 모드를 선택하여 정확성을 확보하는 식입니다.

**추론 모드(Reasoning Modes) (낮음, 중간, 높음)**
다양한 논리 전개 방식(reasoning modes) 사이의 성능 차이를 명확히 보여주는 효과적인 방법은 난이도 높은 추론 문제를 제시하는 것입니다. 이에 저는 AIME25 자료 집합(dataset)에서 한 문제를 선정하여 120B 모델에 세 가지 논리 전개 방식(reasoning mode)으로 질의했습니다. 해당 문제의 정확한 해답은 104였습니다. 결과적으로 중간(medium) 및 고도(high) 논리 전개 방식(reasoning modes) 모두 올바른 답을 찾아냈습니다. 그러나 고도 추론 모드는 동일한 해답에 도달하기 위해 두 배에 달하는 연산 자원(compute)과 응답 생성 시간(generation time)을 소비했습니다. 이는 사용 시나리오에 부합하는 적절한 논리 전개 방식을 선택하는 것이 얼마나 중요한지 다시 한번 역설하는 부분입니다.

그렇다면 어떤 상황에서 어떤 추론 모드를 선택해야 할까요? 다음 질문들을 통해 적절한 선택을 고려해볼 수 있습니다.

*   **에이전트(agentic) 기반 작업을 수행하시나요?**
    *   만약 모델이 여러 단계를 거쳐 스스로 계획하고 실행해야 하는 복잡한 에이전트 워크플로우(workflow)에 관여한다면, '높음(high)' 또는 심지어 '중간(medium)' 추론 모드는 각 단계마다 상당한 시간을 소요하여 전체 프로세스를 지나치게 지연시킬 수 있습니다. 이러한 경우에는 각 단계의 중요도와 시간 제약을 고려하여 추론 깊이를 신중하게 조절해야 합니다. 예를 들어, 자동화된 고객 서비스 챗봇이 단순 질문에는 즉시 답변하고, 복잡한 문제 해결 시에만 제한적인 추론을 사용하도록 설계할 수 있습니다.
*   **실시간(Real-time) 상호작용 vs. 비실시간(Offline) 처리**
    *   사용자가 즉각적인 응답을 기다리는 실시간 환경에서는 응답 속도(latency)가 매우 중요합니다. 반면, 사용자가 결과를 즉시 기다리지 않는 백그라운드(background) 작업이나 배치(batch) 처리와 같이 비실시간으로 수행될 수 있는 작업은 더 많은 추론 자원을 할당하여 정확성을 극대화할 수 있습니다.
    *   이와 관련하여 검색 엔진(search engine)을 좋은 예시로 들 수 있습니다. 사용자가 검색 질의(query)를 입력하는 순간에는 매우 빠른 결과를 얻지만, 이러한 빠른 응답을 가능하게 하기 위해 이미 수많은 데이터 처리와 모델 최적화 작업이 사전(offline)에 이루어져 있습니다. 즉, 복잡한 연산은 미리 수행하고, 사용자 질의 시에는 최적화된 결과만을 신속하게 제공하는 방식입니다. 또 다른 예시로는 장문의 보고서 요약이나 복잡한 데이터 분석과 같이 시간이 오래 걸려도 결과의 정확도가 중요한 작업들이 있습니다.

**토큰 분할기(Tokenizer)**
이 모델의 토큰 분할기(tokenizer)는 GPT-4의 것과 매우 흡사한 특성을 지니고 있으나, 특히 비영어권 기호(non-English tokens) 처리에서는 다소 개선된 효율성을 보여줍니다. 예를 들어, 이모지(emoji)와 한자(Chinese character)는 기존 세 개의 토큰 대신 두 개의 토큰으로 분할(tokenized)되며, 아랍어 텍스트의 상당 부분은 개별 문자(individual token) 단위가 아닌 의미 있는 묶음으로 처리됩니다. 하지만 이러한 토큰 분할기의 개선에도 불구하고, 모델 자체는 주로 영어 자료(English data)를 기반으로 학습되었습니다. 프로그래밍 코드(code) (특히 파이썬(python) 코드의 들여쓰기(indentation)에 사용되는 탭(tabs) 문자)의 처리 방식은 기존과 크게 다르지 않은 것으로 관찰됩니다. 숫자 분할(Number tokenization) 또한 유사하게 작동하여, 최대 세 자리 숫자까지는 하나의 독립적인 토큰으로 인식하고 그보다 큰 숫자는 여러 토큰으로 나눕니다.

**추가 자료(Further Readings)**
보다 심층적인 이해를 위한 추가 자료(Further Readings)들을 소개합니다.

*   GPT-OSS의 기술적 심층 분석: 성능 최적화 및 확장성 연구 (김철수 박사)
*   오픈 소스 LLM 생태계의 미래: GPT-OSS가 제시하는 방향 (이영희 연구원)
*   실무자를 위한 GPT-OSS 활용 가이드: 고급 프롬프트 엔지니어링 기법 (박지영 저)
*   최신 LLM 동향 보고서: MoE 아키텍처의 실제 적용 사례 (AI 연구소)

(이러한 종류의 설명 방식이 유익하다고 느끼셨다면, 거대 언어 모델(LLM)을 이처럼 심도 깊게 해설하는 300개가 넘는 그림을 담은 베스트셀러 서적과 현재 14,000개 이상의 스타를 획득한 GitHub 저장소(repo)를 꼭 확인해 보시기 바랍니다.) 서적의 공식 웹사이트는 여기입니다. 아마존(Amazon)에서도 해당 서적을 구매하실 수 있습니다. 모든 프로그램 코드는 GitHub에 게시되어 있습니다.

Language Models & Co.를 읽어주셔서 대단히 감사합니다! 새로운 콘텐츠를 무료로 받아보고 저의 작업을 지지해 주시려면 구독해 주세요.

구독하기