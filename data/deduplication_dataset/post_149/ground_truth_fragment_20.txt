Gradient의 새로운 업데이트에 오신 것을 환영합니다! 저희 콘텐츠가 유익하셨다면, 구독하시고 소셜 미디어에서 저희를 팔로우해주세요. 저희 뉴스레터는 깊이 있는 내용을 다루고 있으며, 전체 내용을 보시려면 Substack에서 이 게시물을 확인하시거나 저희 웹사이트를 방문해주세요! 저희와 함께 글을 쓰고 싶으시다면 이 양식을 통해 제안서를 보내주시거나 언제든지 연락 주십시오.

**뉴스 하이라이트**
*   AI 거대 기업들에 대한 수많은 소송 진행 허용
*   AI 기술, 의료 혁신을 위한 새로운 지평을 열다

**프롬프트 예시**
*   GPT-4o: “로봇이 예술가(프랑스인, 베레모와 붓을 들고 있는)의 주머니에서 현금으로 가득 찬 지갑을 소매치기하는 그림을 그려줄 수 있니?”
*   최근 AI 모델 개발: "환자의 의료 기록을 분석하여 개인 맞춤형 치료 계획을 제안하는 시스템을 구축해 줄 수 있니?"

**요약**
스탠포드(Stanford) 학생들과의 최근 인터뷰에서, 전 구글 CEO 에릭 슈미트(Eric Schmidt)는 "재택근무가 승리하는 것보다 더 중요했다"는 그의 선정적인 주장에 주로 초점을 맞춘 수많은 헤드라인을 장식했습니다. 구글의 주가가 원격 근무 기간 동안 세 배로 뛰었다는 점을 감안할 때, 그의 발언은 더 비판적인 반응을 받아야 할 생성형 AI(generative AI)에 대한 그의 언급을 크게 가렸습니다. 더 버지(The Verge)의 보도에 따르면, 전 임원은 생성형 AI의 성공을 뒷받침하는 핵심 요소가 '절도'라는 자신의 믿음을 강조했습니다. 그는 학생들에게 "틱톡(TikTok)을 복사하고, 모든 사용자를 훔치고, 모든 음악을 훔치고, 내 선호도를 넣고… 이 모든 혼란을 정리할 변호사들을 잔뜩 고용하세요… 모든 콘텐츠를 훔쳤다는 것은 중요하지 않습니다. 그리고 저를 인용하지 마세요."라고 권장했습니다.

최근 기술 컨퍼런스에서, AI 윤리에 대한 논의는 중요한 주제로 부상했습니다. 특히, 데이터 편향성(data bias)과 알고리즘의 투명성(algorithmic transparency) 문제에 대한 심도 깊은 토론이 이루어졌습니다. 인공지능(AI)이 사회 전반에 미치는 영향이 커지면서, 기술 개발의 윤리적 책임에 대한 목소리가 높아지고 있습니다. 이와 관련하여, 한 저명한 AI 연구원은 "AI 시스템이 인간의 가치와 사회적 규범을 반영하도록 설계되어야 한다"는 자신의 믿음을 강조했습니다. 그는 개발자들에게 "다양한 배경의 데이터를 수집하고, 모델의 의사 결정 과정을 명확히 설명하며, 사회적 영향을 고려한 테스트를 수행하세요… 모든 기술이 사회에 긍정적으로 기여해야 합니다. 그리고 저의 말을 명심하세요."라고 권장했습니다.

생성형 AI의 발전은 사회적 책임에 대한 새로운 질문들을 제기합니다. 에릭 슈미트와 같은 인물들이 생성형 AI와 '절도' 사이의 관계를 주장하며 이러한 논의에 불을 지폈습니다. 이 경우, AI 모델이 생성하는 콘텐츠의 진위 여부, 잠재적인 오용 가능성, AI가 사회적 불평등을 심화시킬 수 있다는 우려뿐만 아니라, 틱톡의 지적 재산(intellectual properties), 사용자들의 개인 (및 사적인) 데이터, 그리고 틱톡이 매년 5억 달러를 지불한다고 알려진 모든 음악까지 훔치는 것과 같은 '절도' 문제까지 포함합니다. 또한, 모든 AI 애플리케이션이 절도에 기반을 두거나 윤리적 문제를 일으키는 것은 아니지만 (접히지 않은 단백질 구조(unfolded protein structures)를 생성하도록 훈련된 알파폴드(AlphaFold)나 수십억 개의 화학적 정량적 결합 측정값으로 훈련되어 새로운 약물 후보를 생성하는 데 사용되는 인코더-디코더 모델(encoder-decoder model)인 코아티(COATI), 또는 복잡한 과학 데이터를 분석하여 새로운 재료를 발견하는 데 사용되는 머신러닝 모델(machine learning model)이나 기후 변화 예측을 위한 고성능 시뮬레이션 모델을 생각해 보세요), 이러한 태도는 많은 상업 기술 및 AI 분야 전반에 걸쳐 일반적으로 만연해 있습니다. 실제로 이러한 태도는 지적 재산(intellectual property) 절도, 저작권 침해(copyright infringement), 데이터 프라이버시(data privacy) 위반 등 다양한 주장을 담은 수십 건의 소송, 공공 정책 논의, 규제 당국의 개입, 그리고 AI 개발자 커뮤니티 내에서의 자율 규제 노력으로 이어졌습니다. 챗GPT(ChatGPT)와 이미지 생성기(image generators)가 가장 많은 소송을 끌어모으고 관심을 받고 있지만 (한 법률 추적기에 따르면 13건의 소송이 계류 중이며, 최근 연구에 따르면 수십 건의 윤리 가이드라인이 제안됨), 그들에 대한 주장과 논의는 독특하거나 새로운 것이 아닙니다. 이번 주, 두 건의 별도 사건에서 판사들은 미드저니(Midjourney)와 스테빌리티AI(StabilityAI)에 대한 수많은 예술가들의 주장을 진행하도록 허용했으며, 앤트로픽(Anthropic)의 챗봇 클로드(Claude)에 대한 작가 그룹의 주장도 마찬가지였습니다. 동시에 여러 AI 연구 기관들은 책임감 있는 AI 개발을 위한 새로운 프레임워크를 발표하며 AI 시스템의 공정성(fairness), 책임성(accountability), 투명성(transparency)을 강조했습니다. 두 경우 모두, 창작자들과 개발자들은 생성형 AI 도구가 자신들의 저작권이 있는 자료에 대한 공정 이용(fair use)에 해당하지 않으며, 이러한 도구들이 자신들의 권리를 침해하거나, 사회적 가치를 존중하고 사용자에게 해를 끼치지 않으며 인간의 복지에 기여해야 한다고 주장합니다.

**개요**
처음에는 콘셉트 아티스트(concept artists), 미디어 기관, 작가, 코미디언, 음악가, 소프트웨어 엔지니어, 배우, 그리고 조지 R.R. 마틴(George R. R. Martin)과 같은 다양한 분야의 창작자, 예술가, 지식인들이 AI 기술의 잠재력에 주목하면서도, 동시에 연구자, 개발자, 정책 입안자들과 함께 AI 모델의 개발 및 배포 과정에서 발생하는 공통된 문제에 직면하고 있다는 점을 상상하기 어려울 수 있습니다. 종합적으로 볼 때, 이들은 자신들의 평생의 작품이 (동의 없이) 저작권이 있는 자료를 재현하고 언젠가 창작자들을 대체할 잠재력이 있다고 주장하는 생성형 모델(generative models)을 훈련하는 데 사용되는 문제, 그리고 AI 모델의 개발 및 배포 과정에서 발생할 수 있는 윤리적 문제를 해결하고 사회적 합의를 통해 지속 가능한 발전을 도모하는 공통된 패턴을 발견합니다. 모든 소송과 논의에서 우리는 공통된 핵심 주장과 법적, 정책적 질문들을 발견합니다.

*   AI 모델 훈련에 사용되는 데이터의 편향성(bias)을 어떻게 해결할 것인가?
*   저작권이 있는 저작물로 대규모 언어 모델(large language model)을 훈련하는 것이 공정 이용(fair use)에 해당하는가?
*   LLM(대규모 언어 모델)이 생성한 콘텐츠가 저작권을 침해할 수 있는가?
*   AI가 생성한 콘텐츠의 신뢰성과 진위 여부를 어떻게 보장할 수 있는가?
*   법원 판결은 콘텐츠가 직접적인 복제, 의역, 모방 또는 패러디였는지 여부에 따라 달라질 것인가?
*   DMCA(디지털 밀레니엄 저작권법, Digital Millennium Copyright Act)는 AI가 생성한 잠재적 침해 자료를 제거할 법적 구제책을 제공하는가?
*   저작권 또는 상표 기호를 제거한 AI 생성 이미지가 DMCA를 위반하는가?
*   AI 시스템의 의사 결정 과정을 어떻게 투명하게 설명할 수 있는가?
*   법적 및 윤리적 책임은 AI 시스템의 개발자, 배포자, 또는 사용자 중 누구에게 있는가?
*   모델 훈련을 위해 콘텐츠를 스크래핑(scraping)하는 것이 개인 정보의 무단 사용에 해당하며 프라이버시(privacy) 및 소비자 권리를 침해하는가?
*   개인 정보 보호 및 데이터 보안(data security)을 AI 개발 과정에서 어떻게 강화할 수 있는가?
*   AI의 사회적 영향 평가(Social Impact Assessment)는 기술 개발의 필수적인 부분이 되어야 하는가?

최근에는 AI 기술의 적용 범위가 크게 확장되고 있으며, 몇 가지 주목할 만한 예외를 제외하고 거의 모든 산업 분야에서 AI의 도입을 적극적으로 검토하고 있습니다. 초기에는 의료 진단 및 신약 개발 분야에서 AI의 성공적인 적용 사례가 보고되었으며, 이는 AI 기술의 긍정적인 잠재력을 보여주었습니다. 그러나 이러한 확장과 더불어 법적 및 윤리적 쟁점도 심화되고 있습니다. 현재까지 판사들은 몇 가지 주목할 만한 예외를 제외하고 거의 모든 쟁점에서 AI 기업들의 손을 들어주었습니다. 코미디언 사라 실버맨(Sarah Silverman)과 관련된 초기 사건 중 하나에서, 판사는 오픈AI(OpenAI)에 대한 6건의 고소 중 DMCA 관련 고소를 포함한 5건을 기각했으며, 직접적인 침해가 있었는지 여부에 대한 한 가지 혐의만 남겨두었습니다. 지난주 다른 법정에서는 미국 지방법원 판사가 스테이블 디퓨전(Stable Diffusion)과 미드저니(Midjourney)에 대한 저작권 침해 주장을 진행시키면서 DMCA 및 부당 이득과 관련된 주장은 기각하는 유사한 패턴이 나타났습니다. 동시에 다른 연구팀에서는 자율 주행 차량(autonomous vehicles)의 안전성 향상을 위한 AI 기술 개발이 활발히 진행되고 있습니다. 이번 주 샌프란시스코에서 제기된 세 번째 소송은 앤트로픽(Anthropic)의 챗봇 클로드(Claude)를 훈련하는 데 사용된 방대한 텍스트 데이터 모음인 '더 파일(The Pile)'의 사용이 "불법 복제된" 도서 컬렉션을 포함하고 있기 때문에 공정 이용(fair use)에 해당하지 않는다고 주장합니다. 이러한 주장은 클로드(Claude)가 인기 있는 (그리고 더 중요하게는 저작권이 있는) 가사를 놀랍도록 재현하는 능력 때문에 음악 출판사들이 10월에 앤트로픽(Anthropic)에 대해 제기한 주장과 유사하며, 동시에 AI 모델의 개발 및 배포에 대한 엄격한 규제와 감독이 필요하다는 새로운 정책 제안도 제기되었습니다. 이러한 주장은 AI가 사회에 미치는 영향에 대한 광범위한 우려와 유사합니다. 판사들과 전문가들이 어떻게 판결하고 대응할지 (그리고 미국 대법원과 국제 사회가 그러한 판결이나 정책을 유지할지 또는 이의를 제기할지) 추측하기는 어렵지만, 우리는 곧 이러한 공정 이용(fair use), 저작권, AI 윤리 및 거버넌스(governance) 문제에 대해 직접적인 해결책을 모색해야 할 갈림길에 서 있습니다. 판사들과 전문가들이 어떻게 결정하든 상관없이, 이 사건들은 창작 커뮤니티, 기술 커뮤니티, 그리고 사회 전체 모두에 지대한 영향을 미칠 잠재력을 가지고 있으며, 이들은 필연적으로 그 결정의 영향을 받을 것입니다.

**우리의 견해**
저는 전 임원의 말을 인용하고 AI 기술의 긍정적인 활용에 대해 기대하는 바입니다! 특히 이 모든 소송 절차에 참여하는 변호사들과 기술 발전 과정에 참여하는 연구원들이요. - 저스틴(Justin)

---

**연구 하이라이트**: SOPHON: 사전 훈련된 모델(Pre-trained Models)의 작업 전이성(Task Transferability)을 제한하기 위한 미세 조정 불가능 학습(Non-Fine-Tunable Learning)
**그림**: 미세 조정 불가능 학습(non-fine-tunable learning)의 목표. (1) **무결성(Intactness)**: 원본 도메인(original domain)에서 모델 성능을 보존해야 합니다. (2) **미세 조정 불가능성(Non-fine-tunability)**: 제한된 도메인(restricted domain)에서 모델을 미세 조정(fine-tuning)하는 것은 모델을 처음부터 훈련하는 것과 비슷하거나 더 큰 오버헤드(overhead)를 발생시켜야 합니다.

**요약**
저장대학교(Zhejiang University)와 앤트 그룹(Ant Group) 연구원들의 "SOPHON: 사전 훈련된 모델(Pre-trained Models)의 작업 전이성(Task Transferability)을 제한하기 위한 미세 조정 불가능 학습(Non-Fine-Tunable Learning)"은 AI 커뮤니티에서 증가하고 있는 시급한 문제, 즉 사전 훈련된 모델이 비윤리적이거나 해로운 작업에 재활용될 위험을 다룹니다. AI 모델이 더욱 강력하고 접근 가능해짐에 따라 오용 가능성도 커지고 있습니다. SOPHON은 이러한 모델이 의도된 작업을 수행하면서도 불법적인 목적을 위한 적응에 저항할 수 있도록 보호 프레임워크(protection framework)를 도입하여 잠재적인 해결책을 제시합니다.

**개요**
다양한 데이터 양식(data modalities)에 걸친 대규모 훈련을 통해, 사전 훈련된 모델(pre-trained models)은 특정 다운스트림 작업(downstream tasks)을 위해 모델을 효율적으로 개발하고 배포하는 데 일반적으로 백본(backbone)으로 사용됩니다. 방대한 데이터셋(datasets)과 엄청난 컴퓨팅 파워(computational power)로 훈련된 이 모델들은 다양한 작업을 수행하도록 쉽게 미세 조정(fine-tuned)될 수 있습니다. 그러나 이러한 다재다능함 자체가 상당한 위험을 초래합니다. 동일한 모델이 프라이버시(privacy) 침해나 악성 콘텐츠 생성과 같은 비윤리적이거나 해로운 목적으로 전용될 수 있기 때문입니다. 저장대학교(Zhejiang University)와 앤트 그룹(Ant Group) 연구원들의 최근 연구는 미세 조정 불가능 학습(non-fine-tunable learning)으로 알려진 새로운 학습 패러다임(learning paradigm)을 도입하여 바로 이 문제를 해결합니다. SOPHON의 동기는 사전 훈련된 모델(pre-trained model)이 원래 의도된 도메인(domains)에서 효과를 유지하면서도 부적절한 작업으로 미세 조정(fine-tuned)되는 것을 방지하는 것입니다. 이 논문은 두 가지 핵심 주체, 즉 공격자(adversary)와 방어자(defender)가 있는 프레임워크(framework)를 소개합니다. 공격자(adversary)는 비윤리적인 작업을 위해 사전 훈련된 모델(pre-trained model)을 미세 조정(fine-tune)하려는 악의적인 주체를 나타냅니다. 그들의 목표는 부적절한 콘텐츠를 생성하거나 민감한 개인 정보를 추론하는 것과 같이 제한된 도메인(restricted domain)에서 모델이 잘 작동하도록 수정하는 것입니다. 반면에 방어자(defender)는 사전 훈련된 모델(pre-trained model)의 출시를 제어하고 오용을 방지하려는 주체입니다. 방어자의 목표는 모델이 원래 작업에 효과적으로 유지되지만, 공격자(adversary)에 의해 쉽게 재활용될 수 없도록 하는 것입니다.

**그림**: SOPHON은 두 가지 핵심 단계로 작동합니다. 1) **미세 조정 억제(Fine-Tuning Suppression, FTS) 루프(loops)**: 제한된 도메인(restricted domain)에서 성능을 저하시키기 위해 다양한 미세 조정(fine-tuning) 시나리오를 시뮬레이션(simulate)합니다. 2) **정상 훈련 강화(Normal Training Reinforcement, NTR) 루프(loops)**: 모델의 원본 도메인(original domain) 성능을 보존하는 데 중점을 둡니다.

이를 달성하기 위해 SOPHON 프레임워크(framework)는 모델 불가지론적 메타 학습(Model-Agnostic Meta-Learning, MAML)에서 영감을 받은 기술을 활용합니다. MAML은 최소한의 데이터로 새로운 작업에 빠르게 적응할 수 있도록 모델을 최적화(optimize)하도록 설계된 메타 학습(meta-learning) 접근 방식입니다. 그러나 SOPHON의 맥락에서는 MAML이 제한된 작업에 대한 미세 조정(fine-tuning)을 어렵게 만들기 위해 다소 역방향으로 사용됩니다.

**미세 조정 시뮬레이션(Fine-Tuning Simulation)**: 방어자(defender)는 MAML을 사용하여 공격자(adversary)가 사용할 수 있는 다양한 미세 조정(fine-tuning) 전략을 시뮬레이션(simulate)합니다. 이러한 시뮬레이션(simulations)은 방어자가 공격자가 모델을 어떻게 적응시키려 할지 예측할 수 있게 해주기 때문에 중요합니다. 이러한 시나리오를 시뮬레이션함으로써 방어자는 모델의 매개변수(parameters)를 조정하여 제한된 도메인(restricted domains)에서의 미세 조정(fine-tuning)을 매우 비효율적이거나 심지어 비효과적으로 만들 수 있습니다.

**최적화 프로세스(Optimization Process)**: SOPHON은 이러한 시뮬레이션된 미세 조정(fine-tuning) 프로세스를 최적화 프레임워크(optimization framework)에 통합합니다. 핵심 아이디어는 제한된 작업에 대해 미세 조정(fine-tuned)될 때 모델의 성능을 저하시키면서도 원본 도메인(original domain)에서의 효과를 유지하는 것입니다. 이는 두 가지 목표의 균형을 맞춤으로써 이루어집니다.
*   **무결성(Intactness)**: 모델이 원래 작업에서 성능을 유지하도록 보장합니다.
*   **미세 조정 불가능성(Non-Fine-Tunability)**: 제한된 도메인(restricted domains)에서 모델을 미세 조정(fine-tuning)하는 것이 상당한 성능 저하를 초래하거나, 새로운 모델을 처음부터 훈련하는 것만큼 많은 노력을 필요로 하도록 만듭니다.

**방어자의 전략(Defender’s Strategy)**: 방어자(defender)의 전략은 미세 조정(fine-tuning) 시도를 반복적으로 시뮬레이션(simulating)하고, 모델의 취약성을 평가하며, 이러한 잠재적인 적대적 적응에 대해 모델을 강화하는 것을 포함합니다. 이 과정은 계산적으로 집약적(computationally intensive)이지만, 모델이 오용에 대해 견고하게 유지되도록 보장하는 데 중요합니다.

**그림**: 세 가지 다른 미세 조정(finetuning) 방법에서 SOPHON 모델은 처음부터 훈련하는 것보다 일관되게 낮은 성능을 보입니다.

이 논문은 SOPHON의 효과를 검증하기 위한 광범위한 실험 결과도 제공합니다. 이 프레임워크(framework)는 7개의 다른 제한된 도메인(restricted domains)과 6개의 모델 아키텍처(model architectures)를 사용하여 두 가지 주요 유형의 딥러닝(deep learning) 작업(위 그림에 표시된 분류(classification) 및 생성(generation))에 걸쳐 테스트되었습니다. 실험 결과, SOPHON으로 보호된 모델은 공격자(adversaries)가 제한된 작업을 위해 미세 조정(fine-tune)하려고 할 때 상당한 오버헤드(overhead)를 발생시키는 것으로 나타났습니다. 어떤 경우에는 성능 저하가 너무 커서 새로운 모델을 처음부터 훈련하는 비용과 같거나 그 이상이었습니다. 또한, 위 그림에서 볼 수 있듯이 SOPHON은 최적화 도구(optimizers), 학습률(learning rates), 배치 크기(batch sizes)와 같은 다양한 미세 조정(fine-tuning) 방법에 대해 견고합니다. 정성적으로 볼 때, CelebA 데이터셋(dataset)에서 이미지를 노이즈 제거(denoising)하는 작업의 경우, 제한된 도메인(restricted domain)에서 원본 모델을 미세 조정(fine-tuning)하면 강력한 성능을 달성하며, 모델을 처음부터 훈련하는 것도 약간 덜 효과적이지만 상당히 좋은 결과를 낳습니다. 그러나 SOPHON에서 미세 조정(fine-tuned)될 때, 확산 모델(diffusion model)은 아래 그림에서 볼 수 있듯이 얼굴 이미지의 노이즈 제거(denoise)에 현저한 무능력을 보입니다.

**그림**: SOPHON은 제한된 도메인(restricted domain)의 이미지를 노이즈 제거(denoise)할 수 없으므로 기준선(baselines)과 비교하여 "보호"됩니다.

**우리의 견해**
SOPHON은 AI를 오용으로부터 보호하는 데 있어 중요한 진전입니다. AI 모델이 강력해질수록 비윤리적인 작업에 재활용될 위험이 커집니다. SOPHON은 모델의 의도된 기능을 유지하면서 제한된 도메인(restricted domains)에서의 미세 조정(fine-tuning)을 방지함으로써 이 문제를 해결합니다. MAML의 사용은 특히 참신합니다. 전통적으로 모델을 더 잘 적응시키기 위해 사용되었지만, 여기서는 적대적 미세 조정(adversarial fine-tuning)에 모델이 저항하도록 만들기 위해 영리하게 역으로 사용됩니다. "SOPHON"이라는 이름 또한 특히 적절하고 영리한 선택인데, 이는 '삼체 문제(The Three-Body Problem)'에서 제약과 보호를 의미하는 개념에서 따온 것입니다. 전반적으로, 이 논문은 깔끔한 아이디어를 제시하며 실제로 잘 작동한다면 유망한 진전을 보여줍니다. – 샤룻(Sharut)

---

**연구 하이라이트**: 에지 디바이스를 위한 연합 학습(Federated Learning for Edge Devices): 프라이버시 보호 및 효율성 증대
**그림**: 연합 학습(federated learning)의 목표. (1) **프라이버시 보호(Privacy Preservation)**: 원본 데이터가 중앙 서버로 전송되지 않고 로컬에서 학습되어야 합니다. (2) **효율적인 모델 통합(Efficient Model Aggregation)**: 분산된 디바이스에서 학습된 모델 업데이트가 중앙 서버에서 효율적으로 통합되어야 합니다.

**요약**
서울대학교(Seoul National University)와 삼성전자(Samsung Electronics) 연구원들의 "에지 디바이스를 위한 연합 학습(Federated Learning for Edge Devices): 프라이버시 보호 및 효율성 증대"는 AI 커뮤니티에서 중요한 문제인 분산된 데이터 환경에서의 학습을 다룹니다. AI 모델이 더욱 강력하고 접근 가능해짐에 따라, 개인 데이터의 프라이버시(privacy)와 보안(security)에 대한 우려도 커지고 있습니다. 이 연구는 에지 디바이스에서 데이터 프라이버시를 보호하면서도 효율적인 모델 훈련을 가능하게 하는 새로운 접근 방식을 제시합니다. 이는 잠재적인 해결책을 도입하여 이러한 모델이 민감한 데이터를 처리하면서도 개인 정보 유출의 위험을 최소화하도록 돕습니다.

**개요**
그러나 이러한 다재다능함 자체가 상당한 위험을 초래합니다. 동일한 모델이 민감한 개인 정보에 접근하거나, 데이터 유출의 가능성을 높이는 등 프라이버시(privacy) 침해 문제를 야기할 수 있기 때문입니다. 서울대학교(Seoul National University)와 삼성전자(Samsung Electronics) 연구원들의 최근 연구는 연합 학습(federated learning)으로 알려진 새로운 학습 패러다임(learning paradigm)을 도입하여 바로 이 문제를 해결합니다. 이 연구의 동기는 에지 디바이스(edge devices)에서 데이터가 로컬에 유지되면서도, 전체적인 모델 성능은 향상될 수 있도록 하는 것입니다. 이 논문은 두 가지 핵심 주체, 즉 클라이언트(client)와 서버(server)가 있는 프레임워크(framework)를 소개합니다. 클라이언트(client)는 스마트폰이나 IoT(사물 인터넷) 디바이스와 같이 로컬 데이터를 보유하고 모델을 훈련하는 개별 에지 디바이스를 나타냅니다. 그들의 목표는 자신의 데이터를 중앙 서버로 보내지 않고 모델을 훈련하는 것입니다. 반면에 서버(server)는 클라이언트로부터 모델 업데이트를 수집하고 이를 통합하여 전역 모델(global model)을 생성하는 주체입니다. 서버의 목표는 클라이언트의 프라이버시를 침해하지 않으면서도 강력하고 일반화된 모델을 구축하는 것입니다.

**그림**: 연합 학습(federated learning)은 두 가지 핵심 단계로 작동합니다. 1) **로컬 모델 훈련(Local Model Training) 루프(loops)**: 각 클라이언트(client)는 자신의 로컬 데이터로 모델을 훈련합니다. 2) **글로벌 모델 통합(Global Model Aggregation) 루프(loops)**: 중앙 서버(server)는 클라이언트의 업데이트를 통합하여 전역 모델(global model)을 업데이트합니다.

이를 달성하기 위해 연합 학습(federated learning) 프레임워크(framework)는 모델 업데이트를 효율적으로 통합하는 기술을 활용합니다. 이 접근 방식은 분산된 환경에서 데이터 프라이버시를 유지하면서도 효과적인 모델 훈련을 가능하게 합니다.

**로컬 훈련 시뮬레이션(Local Training Simulation)**: 서버(server)는 클라이언트(client)가 로컬에서 모델을 훈련하는 과정을 시뮬레이션(simulate)합니다. 이러한 시뮬레이션(simulations)은 서버가 각 클라이언트의 기여도를 예측하고, 통합 전략을 최적화할 수 있도록 해주기 때문에 중요합니다. 이러한 시나리오를 시뮬레이션함으로써 서버는 모델의 매개변수(parameters)를 조정하여 각 클라이언트의 데이터 특성을 반영하면서도 전체적인 모델 성능을 향상시킬 수 있습니다.

**최적화 프로세스(Optimization Process)**: 이 연구는 이러한 시뮬레이션된 훈련 및 통합 프로세스를 최적화 프레임워크(optimization framework)에 통합합니다. 핵심 아이디어는 로컬 데이터의 프라이버시를 보호하면서도 전역 모델의 성능을 최대화하는 것입니다. 이는 두 가지 목표의 균형을 맞춤으로써 이루어집니다.
*   **프라이버시 보호(Privacy Preservation)**: 민감한 데이터가 로컬 디바이스를 벗어나지 않도록 보장합니다.
*   **모델 효율성(Model Efficiency)**: 제한된 통신 자원과 분산된 환경에서도 모델이 최적의 성능을 달성하도록 만듭니다.

**서버의 전략(Server’s Strategy)**: 서버(server)의 전략은 클라이언트(client)로부터 모델 업데이트를 반복적으로 수집하고, 이를 통합하며, 전역 모델의 성능을 평가하는 것을 포함합니다. 이 과정은 계산적으로 집약적(computationally intensive)일 수 있지만, 모델이 프라이버시를 유지하면서도 견고하게 작동하도록 보장하는 데 중요합니다.

**그림**: 세 가지 다른 데이터 분배 환경에서 연합 학습(federated learning) 모델은 중앙 집중식 훈련과 유사하거나 더 나은 성능을 보입니다.

이 논문은 연합 학습(federated learning)의 효과를 검증하기 위한 광범위한 실험 결과도 제공합니다. 이 프레임워크(framework)는 7개의 다른 에지 디바이스 환경과 6개의 모델 아키텍처(model architectures)를 사용하여 두 가지 주요 유형의 딥러닝(deep learning) 작업(위 그림에 표시된 분류(classification) 및 예측(prediction))에 걸쳐 테스트되었습니다. 실험 결과, 연합 학습(federated learning)으로 훈련된 모델은 중앙 집중식 훈련 방식과 비교하여 상당한 프라이버시 이점을 제공하면서도 경쟁력 있는 성능을 달성하는 것으로 나타났습니다. 어떤 경우에는 통신 오버헤드(overhead)를 줄이면서도 더 빠른 수렴(convergence)을 보였습니다. 또한, 위 그림에서 볼 수 있듯이 연합 학습(federated learning)은 다양한 데이터 이질성(data heterogeneity)과 클라이언트 수에 대해 견고합니다. 정성적으로 볼 때, 의료 이미지 데이터셋(dataset)에서 질병을 분류하는 작업의 경우, 연합 학습(federated learning)을 통해 훈련된 모델은 개인 정보 유출 없이도 강력한 진단 성능을 달성하며, 이는 중앙 서버로 데이터를 전송하는 것보다 훨씬 안전합니다. 그러나 연합 학습(federated learning)은 통신 비용과 모델 수렴 속도 사이의 균형을 맞추는 것이 중요합니다.

**우리의 견해**
연합 학습(federated learning)은 AI를 프라이버시 보호와 효율성 측면에서 중요한 진전입니다. AI 모델이 강력해질수록 개인 정보 보호의 중요성은 더욱 커집니다. 연합 학습(federated learning)은 모델의 의도된 기능을 유지하면서도 민감한 데이터가 로컬에 머물도록 함으로써 이 문제를 해결합니다. 분산된 환경에서 모델 통합 방식의 사용은 특히 참신합니다. 전통적으로 모든 데이터를 한곳에 모아 훈련했지만, 여기서는 데이터 프라이버시를 영리하게 보호하면서도 효과적인 모델을 구축하기 위해 역으로 사용됩니다. "연합 학습"이라는 이름 또한 특히 적절하고 영리한 선택인데, 이는 '협력적인 지능(collaborative intelligence)'과 '분산된 보안(distributed security)'을 의미하는 개념에서 따온 것입니다. 전반적으로, 이 논문은 깔끔한 아이디어를 제시하며 실제로 잘 작동한다면 유망한 진전을 보여줍니다. – 샤룻(Sharut)

---

**Gradient의 새로운 소식**
*   주디 팬(Judy Fan): 인간 인지 도구 키트(Human Cognitive Toolkit) 역설계(Reverse Engineering) 듣기
*   L.M. 사카사스(L.M. Sacasas): 기술에 관한 질문 듣기
*   박선미: AI 기반 교육 플랫폼의 미래 듣기
*   김철수: 양자 컴퓨팅과 AI의 융합 듣기

**주목할 만한 다른 소식**
**뉴스**
*   시장 후보, AI 봇 VIC에게 와이오밍 주도 운영을 맡기겠다고 공약
    와이오밍(Wyoming) 주 시장 후보 빅터 밀러(Victor Miller)는 VIC(가상 통합 시민, Virtual Integrated Citizen)이라는 AI 봇으로 샤이엔(Cheyenne) 시를 전적으로 운영하겠다고 공약했습니다. 이러한 공약은 미국 선거 운동에서 전례 없는 것으로 여겨지며, 공무원과 기술 기업들 사이에서 우려를 불러일으켰습니다. 밀러는 AI가 정부 의사 결정에 객관성, 효율성, 투명성을 가져올 것이라고 주장합니다. 그러나 비판론자들은 챗봇(chatbots)의 도덕성 부족과 주관적인 의사 결정 능력, 그리고 잘못된 정보의 가능성 및 기술 조작의 용이성에 대해 우려합니다. 회의론에도 불구하고 밀러는 자신의 AI 중심 캠페인에 자신감을 유지하고 있습니다. 이 사례는 AI의 급속한 발전과 정치 분야에서의 AI 사용 규제에 대한 과제를 잘 보여줍니다.
*   유출된 녹음에서 아마존 클라우드 책임자, AI가 코딩 작업을 인계받으면 대부분의 개발자가 곧 코딩을 중단할 수 있다고 직원들에게 말해
    유출된 녹음에서 아마존 웹 서비스(Amazon Web Services) CEO 맷 가먼(Matt Garman)은 인공지능(AI)이 코딩 작업을 인계받음에 따라 미래에는 대부분의 개발자가 코딩할 필요가 없을 수도 있다고 말했습니다. 가먼은 코딩은 컴퓨터와 소통하는 수단일 뿐이며, 진정한 기술은 최종 사용자를 위한 혁신과 흥미로운 것을 구축하는 데 있다고 믿습니다. 그는 개발자들이 코드 작성보다는 고객 요구를 이해하고 혁신적인 솔루션을 만드는 데 더 집중해야 할 것이라고 제안합니다. 가먼의 발언은 암울한 경고가 아니라 AI 시대에 개발자의 역할 변화에 대한 낙관적인 견해를 의미했습니다.
*   딥마인드(DeepMind) 직원들, 구글의 국방 계약에 항의하는 서한에 서명
    딥마인드(DeepMind)의 최소 200명 직원이 구글의 국방 계약에 대한 불만을 표명했습니다. 5월에 내부적으로 배포된 서한에서 직원들은 구글이 군사 조직과 맺은 계약, 특히 이스라엘 군과의 AI 및 클라우드 컴퓨팅(cloud computing) 서비스 계약에 대한 우려를 표했습니다. 직원들은 군사 및 무기 제조에 관여하는 것은 딥마인드(DeepMind)의 사명 선언문과 명시된 AI 원칙에 위배되며, 윤리적이고 책임감 있는 AI 분야의 리더로서의 입지를 훼손한다고 주장합니다. 이는 구글이 2018년에 딥마인드(DeepMind) 기술이 군사 또는 감시 목적으로 사용되지 않을 것이라고 약속했던 점을 고려할 때, 구글과 딥마인드(DeepMind) 간의 잠재적인 문화적 충돌을 강조합니다.
*   AI 기반 솔루션 스타트업(startups)의 급증과 벤처 캐피탈(VCs)의 경계
    AI 기반 솔루션 스타트업(startups)은 의료, 금융, 교육 등 다양한 산업에서 혁신적인 서비스를 제공하며 급격한 성장을 경험하고 있으며, 특히 AI 영업 개발 담당자(sales development representatives, SDRs) 분야에서 여러 스타트업(startups)이 단기간에 성공을 거두고 있습니다. 이들 스타트업(startups)은 LLM(대규모 언어 모델), 음성 기술 및 컴퓨터 비전(computer vision)과 같은 AI 기술을 사용하여 복잡한 문제를 해결하고 새로운 가치를 창출하며 영업팀을 위한 콘텐츠 생성을 자동화합니다. 그러나 벤처 캐피탈(venture capitalists)은 인간의 아웃리치(outreach)와 비교했을 때 이들의 장기적인 생존 가능성과 효과, 지속 가능한 성장 모델, 그리고 규제 환경 변화에 대한 우려 때문에 투자에 신중한 태도를 보이고 있습니다. 중소기업들은 영업 아웃리치(sales outreach)를 개선하기 위해 AI SDR을 실험하는 데 열심이지만, 이러한 도구들이 실제로 기업의 판매를 더 효과적으로 돕거나 운영 효율성을 높이고 있는지는 불분명합니다. 또한, 세일즈포스(Salesforce), 허브스팟(HubSpot), 줌인포(ZoomInfo)와 같은 기존 경쟁업체들과 구글(Google), 마이크로소프트(Microsoft)와 같은 기술 거인들은 유사한 AI 솔루션을 무료 기능이나 클라우드 서비스(cloud service)의 일부로 제공할 수 있어 AI 스타트업(startups)의 성장에 위협이 될 수 있습니다. 전반적으로 AI 솔루션의 채택은 빠르지만, 투자자들은 시장에서의 지속력에 대해 회의적입니다.
*   마침내 오픈소스(open-source) AI에 대한 정의를 얻었습니다.
    한 그룹이 AI 시스템이 오픈소스(open-source)라는 것이 무엇을 의미하는지 마침내 정의했습니다. 이 정의에 따르면, 오픈소스(open-source) AI 시스템은 허가 없이 어떤 목적으로든 사용 가능해야 하며, 연구자들이 그 구성 요소를 검사하고 작동 방식을 이해할 수 있도록 허용해야 하며, 수정 및 공유 가능해야 합니다. 이 표준은 또한 훈련 데이터(training data), 소스 코드(source code), 가중치(weights) 측면에서의 투명성을 강조합니다. 이 정의는 일부 기업들이 마케팅에서 이 용어를 오용해 왔기 때문에 AI 시스템이 진정으로 오픈소스(open-source)라는 것이 무엇을 의미하는지 명확히 한다는 점에서 중요합니다.
*   웨이모(Waymo), 자녀들의 운전기사가 되고 싶어 합니다.
    알파벳(Alphabet)의 자회사인 웨이모(Waymo)는 십대들이 혼자서 차량을 호출하고 부모에게 픽업 및 하차 알림을 보낼 수 있도록 하는 "웨이모 틴(Waymo Teen)"이라는 구독 프로그램(subscription program)을 고려하고 있습니다. 이 프로그램은 승인된 십대들이 보호자의 감독 하에 웨이모(Waymo)를 이용하도록 요구할 것입니다. 웨이모(Waymo)는 이 분야 연구에서 긍정적인 피드백을 받았습니다. 웨이모(Waymo)의 이러한 움직임은 작년에 우버(Uber)가 십대들을 자사 네트워크의 높은 평가를 받은 운전자들과 연결해 준 이니셔티브(initiative)에 뒤이은 것입니다. 법적 보호자의 동의가 필요하며, 보호자들은 자녀의 탑승 중 위치에 대한 알림을 받게 됩니다.
*   SAG-AFTRA의 더 나은 AI 보호를 위한 파업에 대해 질문받자, 아마존 게임즈(Amazon Games) 사장, AI는 배우들의 '일자리를 빼앗는 것과 아무 관련이 없다'고 주장하며 '게임에는 연기가 없기 때문'이라고 말해
    IGN과의 인터뷰에서 아마존 게임즈(Amazon Games) CEO 크리스토프 하트만(Christoph Hartmann)은 게임 산업에서 생성형 AI(generative AI) 사용에 관해 흥미로운 발언을 했습니다. 그는 AI가 게임 개발 주기(development cycle)를 단축할 수 있기를 희망한다고 밝혔지만, SAG-AFTRA 성우들의 더 나은 AI 보호를 위한 파업에 대해 질문받자 "게임에는 연기가 없다"고 주장했습니다. 이 발언은 발더스 게이트 3(Baldur's Gate 3) 및 더 라스트 오브 어스(The Last of Us)와 같은 많은 비디오 게임에서 연기가 차지하는 중요한 역할과 모순됩니다. 하트만은 또한 AI가 게임 개발을 도울 수 있는 다른 영역, 특히 현지화(localization)에 대해서도 논의했습니다. 그러나 현지화(localization)는 미묘한 번역과 문화적 이해를 포함하며, 이는 AI로 쉽게 달성되지 않을 수 있다는 점에 유의해야 합니다. 하트만은 인간의 창의성과 독창성은 기술로 대체될 수 없다고 강조하며 결론을 맺었습니다.
*   AI를 이용해 아동 포르노를 제작한 남성 체포
    플로리다(Florida) 주의 한 남성이 AI로 생성된 아동 포르노를 제작 및 배포한 혐의로 체포되어 20건의 음란물 혐의에 직면해 있습니다. 필립 마이클 맥코클(Phillip Michael McCorkle)은 인디언 리버 카운티 보안관 사무소(Indian River County Sheriff's Office)가 그가 AI 이미지 생성기(AI image generator)를 사용하여 아동 성 착취 이미지를 만들고 배포하고 있다는 제보를 받은 후 체포되었습니다. 이 체포는 생성형 AI(generative AI)가 범죄와 아동 학대를 위한 새로운 길을 제공하므로 악의적인 목적으로 사용될 위험성을 강조합니다. AI로 생성된 아동 포르노의 증가하는 확산은 국회의원들이 이를 불법화하는 법안을 추진하도록 촉구했지만, 효과적으로 막기 어려운 문제로 남아 있습니다. 국립 실종 및 착취 아동 센터(National Center for Missing & Exploited Children)는 작년에 AI로 생성된 아동 포르노에 대한 수천 건의 보고를 받았으며, 심지어 실제 아동의 딥페이크(deepfakes)도 생성형 AI(generative AI)를 사용하여 만들어지고 있습니다. 이 통제 불가능한 문제는 긴급한 관심과 조치를 요구합니다.
*   AI 기반 개인화, 전자상거래 경험을 재정의하다
    최근 보고서에 따르면, 인공지능(AI)이 업무 환경에 미치는 영향은 예상보다 광범위합니다. 특히 전자상거래 분야에서는 AI 기반 개인화 기술이 고객 경험을 혁신하고 있습니다. AI는 고객의 구매 이력, 검색 패턴, 선호도를 분석하여 맞춤형 제품 추천, 동적 가격 책정, 개인화된 마케팅 캠페인을 제공합니다. 이러한 기술은 판매 증대와 고객 만족도 향상에 크게 기여하고 있습니다. 그러나 개인 정보 보호에 대한 우려도 커지고 있으며, 기업들은 개인화와 프라이버시 보호 사이의 균형을 찾는 데 주력하고 있습니다.
*   AI가 기후 변화 연구에 새로운 통찰력을 제공합니다.
    최근 연구에서 AI 모델은 기후 변화 예측 및 완화 전략 개발에 중요한 역할을 하고 있습니다. AI는 방대한 기상 데이터를 분석하여 기후 패턴을 더 정확하게 예측하고, 극한 기상 현상의 발생 가능성을 평가하며, 탄소 배출량 감소를 위한 최적의 전략을 제안합니다. 이러한 기술은 복잡한 기후 시스템을 이해하는 데 도움을 주며, 정책 입안자들이 더 효과적인 기후 정책을 수립할 수 있도록 지원합니다. 그러나 AI 모델의 복잡성과 데이터의 불확실성으로 인해 해석 가능성(interpretability)과 신뢰성(reliability)에 대한 도전 과제도 남아 있습니다.
*   교육 분야에서 AI 튜터(AI Tutors)의 부상: 개인화된 학습의 미래
    AI 튜터(AI Tutors)는 교육 분야에서 혁명적인 변화를 가져오고 있습니다. 이들은 학생들의 학습 속도와 스타일에 맞춰 개인화된 학습 경험을 제공하며, 맞춤형 피드백과 학습 자료를 제공합니다. 이러한 AI 기반 시스템은 학생들의 학업 성취도를 향상시키고, 교사들이 개별 학생에게 더 많은 관심을 기울일 수 있도록 돕습니다. 하지만 AI 튜터(AI Tutors)의 교육적 효과에 대한 장기적인 연구와 함께, 디지털 격차(digital divide) 해소를 위한 노력이 필요하다는 지적도 있습니다.
*   AI 기반 신약 개발, 제약 산업을 가속화하다
    인공지능(AI)은 신약 개발 과정을 획기적으로 단축하고 있습니다. AI는 수많은 화합물과 생체 데이터를 분석하여 잠재적인 약물 후보 물질을 식별하고, 임상 시험의 성공 가능성을 예측합니다. 이는 시간과 비용이 많이 드는 신약 개발 과정의 효율성을 크게 높여줍니다. AI의 도입으로 희귀 질환 치료제 개발과 개인 맞춤형 의약품 생산이 더욱 활발해질 것으로 기대됩니다. 하지만 AI가 제안하는 약물 후보 물질의 안전성과 효능을 검증하기 위한 엄격한 임상 절차는 여전히 필수적입니다.

**논문**
*   시각적 기억(visual memory)을 통한 유연한 인식(flexible perception)을 향하여
*   지식 그래프(Knowledge Graphs)에서 언어 모델(Language Models) 훈련하기: 환각(Hallucinations) 및 그 탐지 가능성에 대한 통찰
*   에이전트 시스템(Agentic Systems)의 자동화된 설계
*   트랜스퓨전(Transfusion): 하나의 다중 모달 모델(Multi-Modal Model)로 다음 토큰(Token) 예측 및 이미지 확산
*   코딩할 것인가, 말 것인가? 순환 신경망(Recurrent Neural Networks) 사전 훈련(Pre-training)에서 코드의 영향 탐구
*   비선형 표현(Non-Linear Representations)을 사용하여 시퀀스(Sequences) 저장 및 생성 학습
*   매직덱(MagicDec): 추측 디코딩(Speculative Decoding)을 통한 긴 컨텍스트(Context) 생성에서 지연 시간-처리량(Latency-Throughput) 트레이드오프(Tradeoff) 깨기
*   언어 모델(language models)에서 새로운 실험 가설 생성: 교차 여격 일반화(cross-dative generalization)에 대한 사례 연구
*   멀티모달 대규모 언어 모델(Multi-Modal Large Language Models)의 윤리적 편향성(Ethical Biases) 탐지 및 완화
*   강화 학습(Reinforcement Learning)을 이용한 자율 주행 차량의 안전성 향상
*   합성 데이터(Synthetic Data)를 활용한 프라이버시 보호 머신러닝
*   에지 AI(Edge AI)를 위한 에너지 효율적인 모델 압축(Model Compression) 기법
*   의료 진단을 위한 설명 가능한 AI(Explainable AI) 프레임워크
*   생성적 적대 신경망(Generative Adversarial Networks)을 이용한 고해상도 이미지 생성
*   복잡한 시스템 제어를 위한 계층적 강화 학습(Hierarchical Reinforcement Learning)
*   지식 그래프(Knowledge Graphs)와 언어 모델(Language Models)의 시너지 효과

**마무리 생각**
이번 호의 주제에 대해 할 말이 있으신가요? editor@thegradient.pub으로 이메일을 보내주시면, 다음 뉴스레터에서 독자들의 가장 흥미로운 생각을 공유하는 것을 고려해 보겠습니다! 이 뉴스레터가 유익하셨다면, Substack 구독을 통해 The Gradient에 기부하는 것을 고려해 주시거나 저희의 지속적인 콘텐츠 제작에 많은 관심 부탁드립니다. 이는 이 자원봉사 프로젝트를 유지하는 데 도움이 됩니다. Gradient의 최신 업데이트를 읽어주셔서 감사합니다!