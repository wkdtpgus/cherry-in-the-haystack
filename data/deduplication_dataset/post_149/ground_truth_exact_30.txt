Gradient의 82번째 업데이트에 오신 것을 환영합니다! 새로 오셨고 저희 콘텐츠가 마음에 드신다면, 구독하시고 트위터에서 저희를 팔로우해주세요. 저희 뉴스레터는 내용이 길기 때문에, 전체 내용을 보시려면 Substack에서 이 게시물을 확인하셔야 합니다! 언제나처럼, 저희와 함께 AI 분야의 깊이 있는 이야기를 나누고 싶으시다면 이 양식을 통해 제안서를 보내주세요.

**뉴스 하이라이트**: AI 거대 기업들에 대한 수많은 소송 진행 허용
GPT-4o에 대한 프롬프트: “로봇이 예술가(프랑스인, 베레모와 붓을 들고 있는)의 주머니에서 현금으로 가득 찬 지갑을 소매치기하는 그림을 그려줄 수 있니?”
GPT-4o에 대한 새로운 프롬프트: “고대 이집트 파라오가 현대 노트북을 사용하며 피라미드를 설계하는 초현실주의 그림을 그려줄 수 있니?”

**요약**
최근 AI 기술의 급속한 발전과 함께, 창의적인 콘텐츠 생성 능력은 경이로움을 자아내지만 동시에 지적 재산권(intellectual property rights) 및 공정 이용(fair use)에 대한 심각한 논쟁을 불러일으키고 있습니다. 특히, 대규모 데이터셋으로 훈련된 생성형 AI 모델들이 기존 저작물의 무단 사용 논란에 휩싸이면서 법적, 윤리적 문제들이 수면 위로 떠오르고 있습니다. 이러한 논쟁의 중심에는 AI 모델 훈련 과정에서 발생하는 '데이터 절도' 의혹이 있으며, 이는 기술 혁신과 창작자 보호 사이의 균형점을 찾는 중요한 과제로 부상하고 있습니다.

스탠포드(Stanford) 학생들과의 최근 인터뷰에서, 전 구글 CEO 에릭 슈미트(Eric Schmidt)는 "재택근무가 승리하는 것보다 더 중요했다"는 그의 선정적인 주장에 주로 초점을 맞춘 수많은 헤드라인을 장식했습니다. 구글의 주가가 원격 근무 기간 동안 세 배로 뛰었다는 점을 감안할 때, 그의 발언은 더 비판적인 반응을 받아야 할 생성형 AI(generative AI)에 대한 그의 언급을 크게 가렸습니다. 더 버지(The Verge)의 보도에 따르면, 전 임원은 생성형 AI의 성공을 뒷받침하는 핵심 요소가 '절도'라는 자신의 믿음을 강조했습니다. 그는 학생들에게 "틱톡(TikTok)을 복사하고, 모든 사용자를 훔치고, 모든 음악을 훔치고, 내 선호도를 넣고… 이 모든 혼란을 정리할 변호사들을 잔뜩 고용하세요… 모든 콘텐츠를 훔쳤다는 것은 중요하지 않습니다. 그리고 저를 인용하지 마세요."라고 권장했습니다.

이러한 발언은 AI 개발자들이 방대한 데이터를 활용하는 방식에 대한 근본적인 질문을 던집니다. 에릭 슈미트가 생성형 AI와 절도 사이의 관계를 주장한 첫 번째 인물도, 마지막 인물도 아닙니다. 이 경우, 틱톡의 지적 재산(intellectual properties)뿐만 아니라 사용자들의 개인 (및 사적인) 데이터, 그리고 틱톡이 매년 5억 달러를 지불한다고 알려진 모든 음악까지 훔치는 것을 의미합니다. 또한, 모든 생성형 AI 애플리케이션이 절도에 기반을 둔 것은 아니지만 (접히지 않은 단백질 구조(unfolded protein structures)를 생성하도록 훈련된 알파폴드(AlphaFold)나 수십억 개의 화학적 정량적 결합 측정값으로 훈련되어 새로운 약물 후보를 생성하는 데 사용되는 인코더-디코더 모델(encoder-decoder model)인 코아티(COATI)를 생각해 보세요), 이러한 태도는 많은 상업 기술 및 AI 분야 전반에 걸쳐 일반적으로 만연해 있습니다. 실제로 이러한 태도는 지적 재산(intellectual property) 절도, 저작권 침해(copyright infringement), 데이터 프라이버시(data privacy) 위반 등 다양한 주장을 담은 수십 건의 소송으로 이어졌습니다. 챗GPT(ChatGPT)가 저희가 아는 한 가장 많은 소송을 끌어모았지만 (한 법률 추적기에 따르면 13건의 소송이 계류 중), 그들에 대한 주장은 독특하거나 새로운 것이 아닙니다. 이번 주, 두 건의 별도 사건에서 판사들은 미드저니(Midjourney)와 스테빌리티AI(StabilityAI)에 대한 수많은 예술가들의 주장을 진행하도록 허용했으며, 앤트로픽(Anthropic)의 챗봇 클로드(Claude)에 대한 작가 그룹의 주장도 마찬가지였습니다. 두 경우 모두, 창작자들은 생성형 AI 도구가 자신들의 저작권이 있는 자료에 대한 공정 이용(fair use)에 해당하지 않으며, 이러한 도구들이 자신들의 권리를 침해한다고 주장합니다.

**개요**
처음에는 콘셉트 아티스트(concept artists), 권위 있는 미디어 기관, 로맨스 작가, 코미디언, 음악가, 소프트웨어 엔지니어, 배우, 그리고 조지 R.R. 마틴(George R. R. Martin)을 하나로 묶을 수 있는 것이 무엇인지 상상하기 어려울 수 있습니다. 종합적으로 볼 때, 창작자, 예술가, 지식인들이 자신들의 평생의 작품이 (동의 없이) 저작권이 있는 자료를 재현하고 언젠가 창작자들을 대체할 잠재력이 있다고 주장하는 생성형 모델(generative models)을 훈련하는 데 사용되는 공통된 패턴을 발견합니다. 모든 소송에서 우리는 공통된 핵심 주장과 법적 질문들을 발견합니다.

*   저작권이 있는 저작물로 대규모 언어 모델(large language model)을 훈련하는 것이 공정 이용(fair use)에 해당하는가?
*   LLM(대규모 언어 모델)이 생성한 콘텐츠가 저작권을 침해할 수 있는가?
*   법원 판결은 콘텐츠가 직접적인 복제, 의역, 모방 또는 패러디였는지 여부에 따라 달라질 것인가?
*   DMCA(디지털 밀레니엄 저작권법, Digital Millennium Copyright Act)는 AI가 생성한 잠재적 침해 자료를 제거할 법적 구제책을 제공하는가?
*   저작권 또는 상표 기호를 제거한 AI 생성 이미지가 DMCA를 위반하는가?
*   모델 훈련을 위해 콘텐츠를 스크래핑(scraping)하는 것이 개인 정보의 무단 사용에 해당하며 프라이버시(privacy) 및 소비자 권리를 침해하는가?

이러한 법적 질문들은 AI 시대의 저작권법 해석에 중대한 영향을 미칠 것입니다. 특히 '공정 이용' 개념은 미국을 비롯한 여러 국가에서 그 적용 범위가 다르게 해석될 수 있어 더욱 복잡합니다. 예를 들어, 유럽연합(EU)은 AI 훈련 데이터에 대한 저작권 예외를 더 엄격하게 적용하려는 움직임을 보이고 있으며, 이는 AI 개발 방식에 큰 변화를 가져올 수 있습니다. AI 모델이 저작물을 '학습'하는 과정이 단순한 복제인지, 아니면 새로운 창작물을 위한 '변형적 사용(transformative use)'인지에 대한 법원의 판단은 향후 AI 산업의 방향을 결정할 핵심 요소가 될 것입니다.

현재까지 판사들은 몇 가지 주목할 만한 예외를 제외하고 거의 모든 쟁점에서 AI 기업들의 손을 들어주었습니다. 코미디언 사라 실버맨(Sarah Silverman)과 관련된 초기 사건 중 하나에서, 판사는 오픈AI(OpenAI)에 대한 6건의 고소 중 DMCA 관련 고소를 포함한 5건을 기각했으며, 직접적인 침해가 있었는지 여부에 대한 한 가지 혐의만 남겨두었습니다. 지난주 다른 법정에서도 유사한 패턴이 나타났는데, 미국 지방법원 판사는 스테이블 디퓨전(Stable Diffusion)과 미드저니(Midjourney)에 대한 저작권 침해 주장을 진행시키면서 DMCA 및 부당 이득과 관련된 주장은 기각했습니다. 이번 주 샌프란시스코에서 제기된 세 번째 소송은 앤트로픽(Anthropic)의 챗봇 클로드(Claude)를 훈련하는 데 사용된 방대한 텍스트 데이터 모음인 '더 파일(The Pile)'의 사용이 "불법 복제된" 도서 컬렉션을 포함하고 있기 때문에 공정 이용(fair use)에 해당하지 않는다고 주장합니다. 이러한 주장은 클로드(Claude)가 인기 있는 (그리고 더 중요하게는 저작권이 있는) 가사를 놀랍도록 재현하는 능력 때문에 음악 출판사들이 10월에 앤트로픽(Anthropic)에 대해 제기한 주장과 유사합니다. 판사들이 어떻게 판결할지 (그리고 미국 대법원이 그러한 판결을 유지할지 또는 이의를 제기할지) 추측하기는 어렵지만, 우리는 판사들이 곧 이러한 공정 이용(fair use) 및 저작권 문제에 대해 직접 판결을 내리기 시작할 갈림길에 서 있습니다. 판사들이 어떻게 판결하든 상관없이, 이 사건들은 창작 커뮤니티와 AI 커뮤니티 모두에 지대한 영향을 미칠 잠재력을 가지고 있으며, 이들은 필연적으로 그 결정의 영향을 받을 것입니다.

**우리의 견해**
AI의 발전 속도는 놀랍지만, 그 과정에서 발생하는 윤리적, 법적 문제에 대한 논의는 더욱 활발해져야 합니다. 기술 혁신이 창작자의 권리를 침해하지 않고 상생할 수 있는 방안을 모색하는 것이 중요합니다. 단순히 기술의 편리함만을 좇기보다는, 그 이면에 있는 복잡한 사회적 책임을 인식하고 균형 잡힌 접근 방식을 취해야 할 때입니다. – 저스틴(Justin)

**연구 하이라이트**: SOPHON: 사전 훈련된 모델(Pre-trained Models)의 작업 전이성(Task Transferability)을 제한하기 위한 미세 조정 불가능 학습(Non-Fine-Tunable Learning)
**그림**: 미세 조정 불가능 학습(non-fine-tunable learning)의 목표. (1) **무결성(Intactness)**: 원본 도메인(original domain)에서 모델 성능을 보존해야 합니다. (2) **미세 조정 불가능성(Non-fine-tunability)**: 제한된 도메인(restricted domain)에서 모델을 미세 조정(fine-tuning)하는 것은 모델을 처음부터 훈련하는 것과 비슷하거나 더 큰 오버헤드(overhead)를 발생시켜야 합니다.

**요약**
저장대학교(Zhejiang University)와 앤트 그룹(Ant Group) 연구원들의 "SOPHON: 사전 훈련된 모델(Pre-trained Models)의 작업 전이성(Task Transferability)을 제한하기 위한 미세 조정 불가능 학습(Non-Fine-Tunable Learning)"은 AI 커뮤니티에서 증가하고 있는 시급한 문제, 즉 사전 훈련된 모델이 비윤리적이거나 해로운 작업에 재활용될 위험을 다룹니다. AI 모델이 더욱 강력하고 접근 가능해짐에 따라 오용 가능성도 커지고 있습니다. SOPHON은 이러한 모델이 의도된 작업을 수행하면서도 불법적인 목적을 위한 적응에 저항할 수 있도록 보호 프레임워크(protection framework)를 도입하여 잠재적인 해결책을 제시합니다.

이 연구는 AI 모델의 책임 있는 배포와 사용을 위한 중요한 발걸음을 제시합니다. 사전 훈련된 모델의 강력한 일반화 능력은 다양한 분야에서 혁신을 이끌 수 있지만, 동시에 악의적인 사용자가 모델을 쉽게 오용할 수 있는 위험을 내포합니다. SOPHON은 이러한 양면성을 인지하고, 모델의 유용성을 유지하면서도 잠재적 위험을 최소화하는 새로운 접근 방식을 제안함으로써 AI 안전(AI safety) 및 보안 분야에 기여하고 있습니다.

**개요**
현대 AI 시스템, 특히 대규모 사전 훈련된 모델(pre-trained models)은 방대한 데이터셋과 엄청난 컴퓨팅 자원으로 훈련되어 다양한 다운스트림 작업(downstream tasks)에 효과적으로 적용될 수 있습니다. 이러한 다재다능함은 AI 개발 및 배포를 가속화하지만, 동시에 모델이 의도치 않은, 심지어는 유해한 목적으로 재활용될 수 있는 중대한 보안 및 윤리적 문제를 야기합니다. 예를 들어, 특정 이미지 생성 모델이 불법적인 콘텐츠 생성에 사용되거나, 언어 모델이 허위 정보 유포에 악용될 수 있습니다. 저장대학교(Zhejiang University)와 앤트 그룹(Ant Group) 연구팀의 SOPHON은 이러한 '작업 전이성(task transferability)'의 어두운 면을 해결하기 위해 '미세 조정 불가능 학습(non-fine-tunable learning)'이라는 새로운 패러다임을 제안합니다. 이 패러다임의 핵심은 모델이 원래 설계된 작업에서는 뛰어난 성능을 유지하면서도, 제한되거나 비윤리적인 작업으로 미세 조정(fine-tuning)될 경우 그 효율성을 현저히 떨어뜨리도록 만드는 것입니다. 이 논문은 두 가지 핵심 주체, 즉 공격자(adversary)와 방어자(defender)가 있는 프레임워크(framework)를 소개합니다. 공격자(adversary)는 비윤리적인 작업을 위해 사전 훈련된 모델(pre-trained model)을 미세 조정(fine-tune)하려는 악의적인 주체를 나타냅니다. 그들의 목표는 부적절한 콘텐츠를 생성하거나 민감한 개인 정보를 추론하는 것과 같이 제한된 도메인(restricted domain)에서 모델이 잘 작동하도록 수정하는 것입니다. 반면에 방어자(defender)는 사전 훈련된 모델(pre-trained model)의 출시를 제어하고 오용을 방지하려는 주체입니다. 방어자의 목표는 모델이 원래 작업에 효과적으로 유지되지만, 공격자(adversary)에 의해 쉽게 재활용될 수 없도록 하는 것입니다.

이를 달성하기 위해 SOPHON 프레임워크(framework)는 모델 불가지론적 메타 학습(Model-Agnostic Meta-Learning, MAML)에서 영감을 받은 기술을 활용합니다. MAML은 최소한의 데이터로 새로운 작업에 빠르게 적응할 수 있도록 모델을 최적화(optimize)하도록 설계된 메타 학습(meta-learning) 접근 방식입니다. 그러나 SOPHON의 맥락에서는 MAML이 제한된 작업에 대한 미세 조정(fine-tuning)을 어렵게 만들기 위해 다소 역방향으로 사용됩니다.

**미세 조정 시뮬레이션(Fine-Tuning Simulation)**: 방어자(defender)는 MAML을 사용하여 공격자(adversary)가 사용할 수 있는 다양한 미세 조정(fine-tuning) 전략을 시뮬레이션(simulate)합니다. 이러한 시뮬레이션(simulations)은 방어자가 공격자가 모델을 어떻게 적응시키려 할지 예측할 수 있게 해주기 때문에 중요합니다. 이러한 시나리오를 시뮬레이션함으로써 방어자는 모델의 매개변수(parameters)를 조정하여 제한된 도메인(restricted domains)에서의 미세 조정(fine-tuning)을 매우 비효율적이거나 심지어 비효과적으로 만들 수 있습니다.

SOPHON의 최적화 프로세스(Optimization Process)는 두 가지 상충하는 목표, 즉 모델의 '무결성(Intactness)'과 '미세 조정 불가능성(Non-Fine-Tunability)' 사이의 균형을 맞추는 데 중점을 둡니다. 무결성은 모델이 원래의 의도된 작업을 효율적으로 수행하도록 보장하는 것이고, 미세 조정 불가능성은 모델이 제한된 작업으로 전이될 때 상당한 성능 저하를 겪게 하거나, 새로운 모델을 처음부터 훈련하는 것과 동등하거나 그 이상의 비용을 요구하도록 만듭니다. 방어자의 전략은 잠재적인 공격자의 다양한 미세 조정 시도에 대해 모델을 지속적으로 강화하는 것입니다. 이는 MAML 기반의 이중 루프 최적화(two-loop optimization)를 통해 이루어지는데, 내부 루프(inner loop)에서는 공격자의 미세 조정 시도를 시뮬레이션하고, 외부 루프(outer loop)에서는 이러한 시도에 대한 모델의 저항력을 높이는 방향으로 모델 매개변수를 업데이트합니다. 이러한 과정은 계산적으로 집약적(computationally intensive)이지만, 모델이 오용에 대해 견고하게 유지되도록 보장하는 데 필수적입니다.

**그림**: SOPHON은 두 가지 핵심 단계로 작동합니다. 1) **미세 조정 억제(Fine-Tuning Suppression, FTS) 루프(loops)**: 제한된 도메인(restricted domain)에서 성능을 저하시키기 위해 다양한 미세 조정(fine-tuning) 시나리오를 시뮬레이션(simulate)합니다. 2) **정상 훈련 강화(Normal Training Reinforcement, NTR) 루프(loops)**: 모델의 원본 도메인(original domain) 성능을 보존하는 데 중점을 둡니다.

이 논문은 SOPHON 프레임워크의 효과를 검증하기 위해 광범위한 실험 결과를 제시합니다. 다양한 제한된 도메인(restricted domains)과 모델 아키텍처(model architectures)에 걸쳐 분류(classification) 및 생성(generation) 작업에 대한 테스트가 수행되었습니다. 실험 결과, SOPHON으로 보호된 모델은 공격자가 제한된 작업을 위해 미세 조정하려고 할 때 상당한 성능 저하와 함께 높은 오버헤드를 발생시키는 것으로 나타났습니다. 이는 단순히 미세 조정이 어렵다는 것을 넘어, 사실상 새로운 모델을 훈련하는 것과 같은 수준의 노력이 필요하다는 것을 의미합니다. 또한, SOPHON은 최적화 도구(optimizers), 학습률(learning rates), 배치 크기(batch sizes) 등 다양한 미세 조정 설정에 대해서도 견고성을 보였습니다. 특히, CelebA 데이터셋(dataset)을 이용한 이미지 노이즈 제거(denoising) 작업에서, SOPHON을 적용한 확산 모델(diffusion model)은 제한된 도메인에서 이미지를 효과적으로 노이즈 제거하는 능력이 현저히 저하되어, 의도된 보호 효과를 명확히 입증했습니다.

**그림**: 세 가지 다른 미세 조정(finetuning) 방법에서 SOPHON 모델은 처음부터 훈련하는 것보다 일관되게 낮은 성능을 보입니다.
**그림**: SOPHON은 제한된 도메인(restricted domain)의 이미지를 노이즈 제거(denoise)할 수 없으므로 기준선(baselines)과 비교하여 "보호"됩니다.

**우리의 견해**
SOPHON은 AI 모델의 책임 있는 개발과 배포를 위한 중요한 기술적 진보를 보여줍니다. AI의 오용 가능성이 커지는 시대에, 모델이 의도된 기능을 유지하면서도 악의적인 목적의 미세 조정을 방지하는 능력은 AI 안전(AI safety) 분야에서 매우 중요합니다. MAML을 '역방향'으로 활용하여 모델의 적응성을 제한하는 아이디어는 참신하며, 이는 미래의 AI 시스템이 더욱 강력해질수록 필수적으로 요구될 '제약 기반 학습(constraint-based learning)'의 한 예시가 될 수 있습니다. "SOPHON"이라는 이름 또한 특히 적절하고 영리한 선택인데, 이는 '삼체 문제(The Three-Body Problem)'에서 제약과 보호를 의미하는 개념에서 따온 것입니다. 이는 단순히 기술적 해결책을 넘어, AI의 사회적 영향을 고려한 윤리적 설계의 중요성을 강조합니다. 전반적으로, 이 논문은 깔끔한 아이디어를 제시하며 실제로 잘 작동한다면 유망한 진전을 보여줍니다. – 샤룻(Sharut)

**Gradient의 새로운 소식**
주디 팬(Judy Fan): 인간 인지 도구 키트(Human Cognitive Toolkit) 역설계(Reverse Engineering) 듣기
L.M. 사카사스(L.M. Sacasas): 기술에 관한 질문 듣기
알렉스 리(Alex Lee): AI 기반 의료 진단 시스템의 미래 듣기
김민준(Minjun Kim): 초거대 AI 모델의 에너지 효율성 도전 과제 듣기

**주목할 만한 다른 소식**
**뉴스**
시장 후보, AI 봇 VIC에게 와이오밍 주도 운영을 맡기겠다고 공약
유출된 녹음에서 아마존 클라우드 책임자, AI가 코딩 작업을 인계받으면 대부분의 개발자가 곧 코딩을 중단할 수 있다고 직원들에게 말해
딥마인드(DeepMind) 직원들, 구글의 국방 계약에 항의하는 서한에 서명
AI 영업 담당자 스타트업(startups)이 급증하고 있습니다. 그렇다면 벤처 캐피탈(VCs)은 왜 경계할까요?
마침내 오픈소스(open-source) AI에 대한 정의를 얻었습니다.
웨이모(Waymo), 자녀들의 운전기사가 되고 싶어 합니다.
SAG-AFTRA의 더 나은 AI 보호를 위한 파업에 대해 질문받자, 아마존 게임즈(Amazon Games) 사장, AI는 배우들의 '일자리를 빼앗는 것과 아무 관련이 없다'고 주장하며 '게임에는 연기가 없기 때문'이라고 말해
AI를 이용해 아동 포르노를 제작한 남성 체포
AI 기반 신약 개발 가속화: 새로운 화합물 발견
양자 AI(Quantum AI)의 발전: 컴퓨팅 한계 돌파
AI 윤리 규제 논의 활발: 글로벌 표준 마련 시급
휴머노이드 로봇, 산업 현장 투입 확대: 생산성 향상 기대
개인화된 AI 교육 시스템: 학습 효과 극대화
AI로 인한 에너지 소비 문제: 지속 가능한 AI 연구 박차
심층 강화 학습(Deep Reinforcement Learning)을 이용한 자율 주행 기술 진보
AI 기반 사이버 보안 솔루션: 지능형 위협 탐지 강화

**논문**
시각적 기억(visual memory)을 통한 유연한 인식(flexible perception)을 향하여
지식 그래프(Knowledge Graphs)에서 언어 모델(Language Models) 훈련하기: 환각(Hallucinations) 및 그 탐지 가능성에 대한 통찰
에이전트 시스템(Agentic Systems)의 자동화된 설계
트랜스퓨전(Transfusion): 하나의 다중 모달 모델(Multi-Modal Model)로 다음 토큰(Token) 예측 및 이미지 확산
코딩할 것인가, 말 것인가? 순환 신경망(Recurrent Neural Networks) 사전 훈련(Pre-training)에서 코드의 영향 탐구
비선형 표현(Non-Linear Representations)을 사용하여 시퀀스(Sequences) 저장 및 생성 학습
매직덱(MagicDec): 추측 디코딩(Speculative Decoding)을 통한 긴 컨텍스트(Context) 생성에서 지연 시간-처리량(Latency-Throughput) 트레이드오프(Tradeoff) 깨기
언어 모델(language models)에서 새로운 실험 가설 생성: 교차 여격 일반화(cross-dative generalization)에 대한 사례 연구
대규모 비디오 모델을 위한 효율적인 자기 지도 학습(Self-Supervised Learning) 아키텍처
생성형 AI의 편향성 감지 및 완화를 위한 적대적 훈련(Adversarial Training) 기반 접근 방식
다중 모달 에이전트의 상황 인식 추론(Context-Aware Reasoning) 능력을 위한 새로운 신경망 모델
연합 학습(Federated Learning) 환경에서의 개인 정보 보호 강화 및 모델 성능 최적화 방안 연구
설명 가능한 AI(XAI)를 활용한 의료 진단 모델의 신뢰성 및 해석 가능성 향상
엣지 디바이스(Edge Devices)를 위한 경량 신경망 아키텍처 설계 및 최적화 기법
강화 학습 기반 로봇 팔 제어의 정밀도 및 적응성 개선을 위한 다중 목표 최적화
대규모 언어 모델의 추론 능력 평가를 위한 새로운 다국어 벤치마크 개발

**마무리 생각**
이번 호에서 다룬 AI의 윤리적, 기술적 도전 과제들에 대해 여러분의 생각은 어떠신가요? editor@thegradient.pub으로 이메일을 보내주시면, 다음 뉴스레터에서 독자들의 가장 흥미로운 의견들을 공유할 기회를 고려해 보겠습니다! 이 뉴스레터가 유익하셨다면, Substack 구독을 통해 The Gradient의 독립적인 콘텐츠 제작에 기부하는 것을 고려해 주세요. 여러분의 지원은 이 자원봉사 프로젝트를 지속하고 더욱 풍성한 내용을 제공하는 데 큰 힘이 됩니다. Gradient의 최신 업데이트를 읽어주셔서 감사합니다!