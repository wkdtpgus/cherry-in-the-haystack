Gradient가 전하는 여든두 번째 소식에 함께해 주셔서 감사합니다! 인공지능 분야의 급변하는 흐름 속에서 저희의 통찰이 유익하셨다면, 정기 구독을 신청하시고 X(이전 트위터)에서 저희의 업데이트를 확인해 주십시오. 이번 뉴스레터는 분량이 상당하므로, 모든 내용을 온전히 접하시려면 Substack 플랫폼에서 본 글을 열람하시는 것이 좋습니다. 또한, 저희와 함께 지식을 나누고 싶으신 필자께서는 언제든지 이 제안서 양식을 통해 의견을 주시길 바랍니다.

**주요 소식**: 인공지능(AI) 개발 대기업들을 상대로 한 다수의 법적 분쟁 허용
인공지능 기술의 발전은 지적 재산권(intellectual property rights)과 윤리적 경계에 대한 근본적인 질문을 제기하고 있습니다. 특히 창작자 커뮤니티는 자신들의 작품이 동의 없이 AI 모델 훈련에 사용되는 것에 대해 깊은 우려를 표명하고 있습니다.
GPT-4o에 제시된 지시: “프랑스 출신의 화가(베레모를 쓰고 붓을 든 모습)의 주머니에서 현금이 가득 담긴 지갑을 훔치는 로봇의 이미지를 생성해 줄 수 있을까요?”

**요약**
최근 스탠포드(Stanford) 대학생들과의 대담에서, 구글의 전 수장 에릭 슈미트(Eric Schmidt)는 "원격 근무의 성공이 더 중요했다"는 도발적인 발언으로 언론의 주목을 받았습니다. 그러나 그의 이 발언은, 원격 근무 기간 동안 구글의 주가가 세 배나 상승했음에도 불구하고, 생성형 인공지능(generative AI)에 대한 그의 더욱 논란이 될 만한 주장을 가려버렸습니다. 더 버지(The Verge)의 보도에 따르면, 슈미트 전 임원은 생성형 AI의 핵심 성장 동력이 '무단 도용'에 있다고 피력했습니다. 그는 청중에게 "틱톡(TikTok)을 모방하고, 모든 사용자를 가로채고, 모든 음악을 가져와서… 이 복잡한 상황을 해결할 변호사들을 대거 고용하십시오… 모든 콘텐츠를 불법적으로 취득한 것은 문제가 되지 않습니다. 그리고 제 발언을 인용하지 마세요."라고 조언하며, 사실상 데이터와 지적 재산의 대규모 '세탁(data laundering)'을 시사하는 듯한 태도를 보였습니다.

이처럼 인공지능 발전과 콘텐츠 무단 사용 간의 연관성을 주장한 인물은 슈미트가 처음도 마지막도 아닙니다. 그의 발언은 틱톡의 지적 자산(intellectual assets)뿐만 아니라, 사용자들의 민감한 개인 정보 및 매년 5억 달러가량 지불되는 것으로 알려진 방대한 음악 라이브러리까지 포함하는 광범위한 탈취를 의미합니다. 물론, 모든 생성형 AI 응용 프로그램이 이러한 방식으로 작동하는 것은 아닙니다. 예를 들어, 접히지 않은 단백질 구조(unfolded protein structures)를 생성하도록 학습된 알파폴드(AlphaFold)나 수십억 개의 화학적 정량적 결합 측정값으로 훈련되어 새로운 약물 후보를 발굴하는 데 활용되는 인코더-디코더 모델(encoder-decoder model)인 코아티(COATI)와 같은 사례는 윤리적인 데이터 수집과 활용의 가능성을 보여줍니다. 그럼에도 불구하고, 상업적 기술 및 인공지능 산업 전반에는 이처럼 느슨한 윤리적 기준이 만연해 있는 것이 현실입니다. 이러한 경향은 지적 재산권(intellectual property rights) 침해, 저작권 위반(copyright infringement), 개인 정보 보호(data privacy) 침해 등 다양한 혐의로 수많은 소송이 제기되는 결과를 낳았습니다. 알려진 바에 따르면 챗GPT(ChatGPT)가 가장 많은 소송(한 법률 추적기에 따르면 13건)에 직면해 있지만, 이들에 대한 법적 주장은 결코 유일하거나 참신한 것이 아닙니다. 최근 몇 주 동안, 두 건의 개별 소송에서 법원 판사들은 미드저니(Midjourney)와 스테빌리티AI(StabilityAI)를 상대로 한 여러 예술가들의 주장을 계속 진행하도록 허가했으며, 앤트로픽(Anthropic)의 챗봇 클로드(Claude)에 대한 작가 단체의 소송 또한 유사한 판결을 받았습니다. 이 모든 경우에 창작자들은 생성형 AI 도구가 자신들의 저작권 보호 자료에 대한 공정 사용(fair use) 원칙에 부합하지 않으며, 오히려 자신들의 권리를 침해하고 있다고 주장합니다. 이는 AI 모델 훈련에 사용되는 데이터의 출처와 사용 방식에 대한 근본적인 법적 및 윤리적 재고를 요구하는 중대한 전환점입니다.

**개요**
언뜻 보기에 콘셉트 디자이너(concept designers), 저명한 미디어 기관, 로맨스 소설 작가, 코미디언, 음악가, 소프트웨어 개발자, 배우, 그리고 조지 R.R. 마틴(George R. R. Martin)을 연결하는 공통점을 찾기 어려울 수 있습니다. 그러나 이들 모두는 자신들의 평생에 걸친 창작물이 무단으로 생성형 모델(generative models) 훈련에 동원되어 저작권이 있는 자료를 재구성하고, 궁극적으로는 자신들의 직업적 입지를 위협할 수 있다는 유사한 우려를 공유하고 있습니다. 모든 소송에서 우리는 다음과 같은 핵심적인 법적 쟁점과 질문들을 마주하게 됩니다.

*   저작권 보호 대상 저작물을 활용하여 대규모 언어 모델(large language model)을 학습시키는 행위가 공정한 이용(fair use)의 범주에 속하는가?
*   대규모 언어 모델(LLM)이 생성한 결과물이 기존 저작물의 저작권을 침해할 소지가 있는가?
*   법원의 판단이 콘텐츠의 직접적인 복사, 재구성, 모방 또는 패러디 여부에 따라 달라질 것인가?
*   디지털 밀레니엄 저작권법(DMCA, Digital Millennium Copyright Act)이 인공지능(AI)이 생성한 잠재적 침해 자료의 삭제를 위한 법적 해결책을 제공하는가?
*   저작권 또는 상표 관련 표식이 제거된 인공지능(AI) 생성 이미지가 DMCA를 위반하는가?
*   모델 학습을 위해 웹 콘텐츠를 무단으로 수집(scraping)하는 행위가 개인 정보의 불법적 사용에 해당하며, 이는 개인 정보 보호(privacy) 및 소비자 권리를 침해하는가?

현재까지 법정은 몇몇 예외적인 경우를 제외하고는 대부분의 쟁점에서 인공지능(AI) 개발 기업들의 손을 들어주는 경향을 보였습니다. 코미디언 사라 실버맨(Sarah Silverman) 관련 초기 소송에서는 오픈AI(OpenAI)에 대한 6가지 고소 중 DMCA 관련 주장을 포함한 5가지가 기각되었고, 직접적인 침해 여부에 대한 한 가지 혐의만 남았습니다. 지난주 다른 법원에서도 유사한 양상이 나타나, 미국 지방법원 판사는 스테이블 디퓨전(Stable Diffusion)과 미드저니(Midjourney)에 대한 저작권 침해 주장은 진행하도록 허용했지만, DMCA 및 부당 이득 관련 주장은 기각했습니다. 이번 주 샌프란시스코에서 제기된 세 번째 소송은 앤트로픽(Anthropic)의 챗봇 클로드(Claude) 훈련에 사용된 방대한 텍스트 데이터셋인 '더 파일(The Pile)'이 "불법 복제된" 도서들을 포함하고 있으므로 공정 이용(fair use)으로 볼 수 없다고 주장합니다. 이러한 주장은 지난 10월 음악 출판사들이 클로드(Claude)가 인기 있고 저작권이 있는 가사를 놀랍도록 재현하는 능력 때문에 앤트로픽(Anthropic)을 상대로 제기한 소송과 궤를 같이 합니다. 판사들이 앞으로 어떤 결정을 내릴지(그리고 미국 대법원이 그러한 판결을 지지하거나 번복할지) 예측하기는 어렵지만, 우리는 공정 이용(fair use)과 저작권 침해 문제를 직접적으로 다룰 중대한 법적 교차로에 서 있습니다. 이러한 판결이 어떻게 내려지든, 이는 창작 커뮤니티와 인공지능(AI) 커뮤니티 모두에게 심대한 영향을 미칠 것이며, 양측은 필연적으로 그 결정의 결과에 직면하게 될 것입니다. 이처럼 빠르게 진화하는 법적 환경은 기존의 법률 체계가 새로운 기술 패러다임에 어떻게 적응해야 하는지에 대한 중요한 질문을 던지고 있습니다. 이는 단순한 법적 분쟁을 넘어, 인류의 창의성과 기술 발전의 조화를 모색하는 사회적 합의의 과정이라 할 수 있습니다.

**우리의 견해**
저는 슈미트 전 임원의 발언이 널리 회자되기를 강력히 희망합니다! 특히 현재 진행 중인 모든 법적 공방에 관여하는 법조인들에게 말이죠. 영향력 있는 인물의 무책임한 발언은 때로 예상치 못한 파급 효과를 낳으며, 이는 AI 기술의 미래 윤리적 방향 설정에 중대한 영향을 미칠 수 있습니다. - 저스틴(Justin)

**주목할 만한 연구**: SOPHON: 사전 학습된 모델(Pre-trained Models)의 임무 전이성(Task Transferability)을 제약하는 미세 조정 방지 학습(Non-Fine-Tunable Learning) 기법
인공지능 모델의 오용 방지는 현대 AI 연구의 중요한 과제 중 하나입니다. 이러한 맥락에서, SOPHON은 모델의 안전한 배포를 위한 새로운 접근 방식을 제시합니다.
**개념도**: 미세 조정 방지 학습(non-fine-tunable learning)의 핵심 목표. (1) **원본 성능 유지(Intactness)**: 초기 학습 도메인(original domain)에서의 모델 역량을 온전히 보존해야 합니다. (2) **재조정 불가능성(Non-fine-tunability)**: 특정 제약된 영역(restricted domain)에서 모델을 재조정(fine-tuning)하려는 시도는, 모델을 완전히 새로 구축하는 것과 유사하거나 그 이상의 비용(overhead)을 수반해야 합니다.

**요약**
저장대학교(Zhejiang University)와 앤트 그룹(Ant Group) 연구진이 발표한 논문 "SOPHON: 사전 학습된 모델(Pre-trained Models)의 임무 전이성(Task Transferability)을 제약하는 미세 조정 방지 학습(Non-Fine-Tunable Learning)"은 인공지능(AI) 커뮤니티가 직면한 중대한 난제, 즉 사전 학습된 모델이 비윤리적이거나 유해한 용도로 전용될 위험성에 대한 해법을 모색합니다. 인공지능 모델의 역량이 증대되고 접근성이 향상됨에 따라, 이들의 악용 가능성 또한 비례적으로 증가하고 있습니다. 이러한 배경 속에서 SOPHON은 모델이 본래 의도된 기능을 수행하는 동시에, 불법적 목적을 위한 변형 시도에 강력히 저항할 수 있도록 하는 방어 체계(protection framework)를 제안함으로써, 책임감 있는 인공지능 배포를 위한 실질적인 해법을 제시합니다.

**개요**
다양한 유형의 데이터(data modalities)에 걸쳐 방대한 양으로 학습된 사전 학습 모델(pre-trained models)은 특정 하위 작업(downstream tasks)을 위한 효율적인 모델 개발 및 배포의 핵심 기반(backbone)으로 통용됩니다. 대규모 데이터셋(datasets)과 막대한 연산 능력(computational power)으로 훈련된 이 모델들은 여러 임무를 수행하도록 손쉽게 미세 조정(fine-tuned)될 수 있습니다. 그러나 이러한 광범위한 적용 가능성 자체가 상당한 위험을 내포합니다. 동일한 모델이 개인 정보(privacy) 침해나 유해한 콘텐츠 생성과 같은 비윤리적 또는 악의적인 목적을 위해 전용될 수 있기 때문입니다. 저장대학교(Zhejiang University)와 앤트 그룹(Ant Group) 연구진의 최근 연구는 '미세 조정 방지 학습(non-fine-tunable learning)'이라는 새로운 학습 패러다임(learning paradigm)을 도입하여 바로 이러한 문제에 대응합니다. SOPHON의 목표는 사전 학습 모델(pre-trained model)이 본래 의도된 영역(domains)에서 효율성을 유지하면서도, 부적절한 임무를 위해 재조정(fine-tuned)되는 것을 원천적으로 차단하는 것입니다. 이 논문은 두 가지 주요 행위자, 즉 공격자(adversary)와 방어자(defender)가 존재하는 프레임워크(framework)를 제시합니다. 공격자는 비윤리적인 목적을 위해 사전 학습 모델(pre-trained model)을 미세 조정(fine-tune)하려는 악의적인 주체입니다. 이들의 목표는 부적절한 콘텐츠를 만들거나 민감한 개인 정보를 유추하는 등, 제한된 영역(restricted domain)에서 모델이 효과적으로 기능하도록 변경하는 것입니다. 반대로 방어자는 사전 학습 모델(pre-trained model)의 배포를 관리하고 오용을 방지하려는 주체입니다. 방어자의 목표는 모델이 원래 임무에 대해서는 유효성을 유지하되, 공격자에 의해 쉽게 재활용되지 않도록 하는 것입니다. 이는 인공지능 모델의 '블랙박스(black box)' 특성으로 인해 발생하는 보안 취약점을 선제적으로 해결하려는 시도입니다.

**개념도**: SOPHON은 두 가지 핵심적인 과정으로 작동합니다. 1) **미세 조정 억제(Fine-Tuning Suppression, FTS) 순환(loops)**: 특정 제약 영역(restricted domain)에서의 성능을 저하시키기 위해 다양한 미세 조정(fine-tuning) 시나리오를 모의 실험(simulate)합니다. 2) **정상 훈련 강화(Normal Training Reinforcement, NTR) 순환(loops)**: 모델의 원래 영역(original domain) 성능을 보존하는 데 중점을 둡니다.

이를 실현하기 위해 SOPHON 프레임워크(framework)는 모델 불가지론적 메타 학습(Model-Agnostic Meta-Learning, MAML)에서 영감을 얻은 기법을 활용합니다. MAML은 최소한의 데이터로 새로운 임무에 신속하게 적응하도록 모델을 최적화(optimize)하기 위해 설계된 메타 학습(meta-learning) 접근법입니다. 그러나 SOPHON의 맥락에서는 MAML이 제한된 임무에 대한 미세 조정(fine-tuning)을 어렵게 만드는 방향으로 다소 역설적으로 적용됩니다. 이는 일종의 '모델 경화(model hardening)' 전략이라 할 수 있습니다.

**재조정 모의 실험(Fine-Tuning Simulation)**: 방어자(defender)는 MAML을 이용하여 공격자(adversary)가 활용할 수 있는 다양한 재조정(fine-tuning) 전략을 모의 시험(simulate)합니다. 이러한 모의 실험(simulations)은 방어자가 공격자가 모델을 어떻게 변형시키려 할지 예측할 수 있도록 해주므로 매우 중요합니다. 이러한 시나리오를 모의함으로써 방어자는 모델의 매개변수(parameters)를 조절하여 제한된 영역(restricted domains)에서의 재조정(fine-tuning)을 매우 비효율적이거나 심지어 무력하게 만들 수 있습니다.

**최적화 과정(Optimization Process)**: SOPHON은 이러한 모의 재조정(fine-tuning) 과정을 최적화 체계(optimization framework)에 통합합니다. 핵심 아이디어는 제한된 임무를 위해 재조정(fine-tuned)될 때 모델의 성능을 현저히 떨어뜨리면서도, 원래 영역(original domain)에서의 유효성은 유지하는 것입니다. 이는 두 가지 목표 사이의 균형을 통해 달성됩니다.
*   **원본 기능 보존(Intactness)**: 모델이 본래의 임무에서 제 기능을 유지하도록 보장합니다.
*   **재조정 방지(Non-Fine-Tunability)**: 제한된 영역(restricted domains)에서 모델을 재조정(fine-tuning)하는 것이 상당한 성능 저하를 초래하거나, 새로운 모델을 처음부터 학습시키는 것만큼 많은 노력을 요구하도록 만듭니다.

**방어자의 전략(Defender’s Strategy)**: 방어자(defender)의 전략은 재조정(fine-tuning) 시도를 반복적으로 모의하고, 모델의 취약점을 평가하며, 이러한 잠재적인 적대적 적응에 맞서 모델을 강화하는 것을 포함합니다. 이 과정은 계산적으로 매우 집약적(computationally intensive)이지만, 모델이 오용에 대해 강건하게 유지되도록 보장하는 데 필수적입니다.

**개념도**: 세 가지 상이한 미세 조정(finetuning) 방식에서 SOPHON 모델은 처음부터 학습시킨 모델보다 지속적으로 낮은 성능을 나타냅니다.

이 논문은 SOPHON의 효과를 입증하기 위한 광범위한 실험 결과 또한 제시합니다. 이 프레임워크(framework)는 7개의 상이한 제한 영역(restricted domains)과 6개의 모델 구조(model architectures)를 사용하여 두 가지 주요 딥러닝(deep learning) 작업(위 개념도에 나타난 분류(classification) 및 생성(generation))에 걸쳐 시험되었습니다. 실험 결과, SOPHON으로 보호된 모델은 공격자(adversaries)가 제한된 임무를 위해 재조정(fine-tune)하려고 할 때 상당한 오버헤드(overhead)를 유발하는 것으로 드러났습니다. 어떤 경우에는 성능 저하가 너무 커서 새로운 모델을 처음부터 학습시키는 비용과 동일하거나 그 이상이었습니다. 또한, 위 개념도에서 알 수 있듯이 SOPHON은 최적화 알고리즘(optimizers), 학습률(learning rates), 배치 크기(batch sizes)와 같은 다양한 재조정(fine-tuning) 방법에 대해서도 견고성을 보였습니다. 정성적으로 볼 때, CelebA 데이터셋(dataset)에서 이미지 노이즈 제거(denoising) 작업의 경우, 제한된 영역(restricted domain)에서 원본 모델을 재조정(fine-tuning)하면 강력한 성능을 달성하며, 모델을 처음부터 학습시키는 것도 약간 덜 효과적이지만 상당히 양호한 결과를 낳습니다. 그러나 SOPHON에서 재조정(fine-tuned)될 때, 확산 모델(diffusion model)은 아래 개념도에서 볼 수 있듯이 얼굴 이미지의 노이즈 제거(denoise)에 현저한 무능력을 드러냅니다.

**그림**: SOPHON은 제한된 도메인(restricted domain)의 이미지를 노이즈 제거(denoise)할 수 없으므로 기준선(baselines)과 비교하여 "보호"됩니다.

**우리의 견해**
SOPHON은 인공지능(AI)을 악용으로부터 보호하는 데 있어 중대한 도약을 의미합니다. 인공지능 모델이 더욱 강력해질수록, 비윤리적인 목적으로 전용될 가능성 또한 증대됩니다. SOPHON은 모델의 본래 의도된 기능을 보존하면서도, 특정 제약 영역(restricted domains)에서의 재조정(fine-tuning)을 효과적으로 차단함으로써 이 문제를 해결합니다. 특히 모델 불가지론적 메타 학습(MAML)의 활용은 그 독창성이 돋보입니다. 일반적으로 MAML은 모델의 적응력을 향상시키는 데 사용되어 왔지만, 여기서는 적대적 재조정(adversarial fine-tuning) 시도에 모델이 저항하도록 만드는 영리한 역발상으로 적용되었습니다. 'SOPHON'이라는 명칭 또한 '삼체 문제(The Three-Body Problem)'에서 유래한 '제약'과 '보호'의 개념을 담고 있어 매우 적절하고 지혜로운 선택이라고 할 수 있습니다. 종합적으로 볼 때, 이 연구는 명쾌한 아이디어를 제시하며, 실제 환경에서도 기대만큼의 효과를 발휘한다면 인공지능 보안 분야에 매우 희망적인 발전을 가져올 것입니다. 이는 궁극적으로 '설계 단계부터 안전한(secure by design)' 인공지능 시스템 구축의 가능성을 열어줄 수 있습니다. – 샤룻(Sharut)

**Gradient 콘텐츠 업데이트**
저희 Gradient 팀은 인공지능 및 기술 분야의 심층적인 통찰을 제공하기 위해 다양한 형식의 콘텐츠를 지속적으로 선보이고 있습니다. 다음은 최근 공개된 흥미로운 오디오 콘텐츠입니다.

*   **주디 팬(Judy Fan): 인간 인지 도구 모음(Human Cognitive Toolkit) 역공학(Reverse Engineering) 강연** - 인간의 사고 체계와 인지 도구들을 인공지능의 관점에서 해부하는 흥미로운 접근을 제공합니다.
*   **L.M. 사카사스(L.M. Sacasas): 기술에 대한 성찰 팟캐스트** - 기술이 사회와 개인에게 미치는 영향을 철학적, 윤리적 관점에서 심도 있게 질문합니다.

**주목할 만한 다른 소식**
**뉴스**
**와이오밍(Wyoming)주 시장 후보, 인공지능(AI) 봇 'VIC'에게 시정 운영 전담 공약**
와이오밍(Wyoming)주 시장 후보 빅터 밀러(Victor Miller)는 가상 통합 시민(Virtual Integrated Citizen)이라는 인공지능(AI) 봇 'VIC'에게 샤이엔(Cheyenne) 시의 모든 행정 업무를 맡기겠다는 공약을 내세웠습니다. 이러한 공약은 미국 선거 운동에서 전례 없는 시도로 평가되며, 공무원과 기술 기업들 사이에서 우려를 불러일으켰습니다. 이 후보는 AI가 행정 과정에 공정성, 능률성, 그리고 투명성을 부여할 것이라고 주장합니다. 반면 회의적인 시각에서는 챗봇(chatbots)의 윤리적 판단 결여, 주관적 의사 결정의 한계, 허위 정보 확산 가능성, 그리고 기술적 조작의 취약성 등을 지적합니다. 이러한 논란에도 불구하고 밀러는 자신의 AI 중심 캠페인에 자신감을 유지하고 있습니다. 이 사건은 인공지능의 빠른 진보가 정치 영역에 가져오는 새로운 도전과 규제의 필요성을 명확히 보여줍니다. 더 나아가, AI가 민주주의의 근간인 시민 참여와 대의 민주주의 원칙에 어떤 영향을 미칠지에 대한 심도 깊은 논의를 촉발합니다.

**유출된 녹취록에서 아마존 웹 서비스(AWS) 최고 경영자, AI가 코딩 업무를 대체함에 따라 대다수 개발자가 미래에는 직접 코딩할 필요가 없어질 것이라고 직원들에게 언급**
유출된 녹취록에서 아마존 웹 서비스(Amazon Web Services) CEO 맷 가먼(Matt Garman)은 인공지능(AI)이 코딩 업무를 대체함에 따라 미래에는 대다수 개발자가 직접 코딩할 필요가 없어질 수도 있다고 말했습니다. 가먼은 코딩이 단지 기계와 소통하는 도구일 뿐이며, 진정한 역량은 최종 사용자를 위한 독창적이고 가치 있는 결과물을 창출하는 데 있다고 역설했습니다. 그는 개발자들이 코드 작성보다는 고객의 필요를 파악하고 혁신적인 해법을 고안하는 데 초점을 맞춰야 한다고 제언합니다. 가먼의 이러한 발언은 개발자 직무의 소멸을 예고하는 비관적인 전망이 아니라, AI 시대에 발맞춰 개발자의 역할이 고도화되고 변화할 것이라는 긍정적인 시각을 반영합니다. 이는 인공지능이 단순 반복 작업을 자동화하여 인간이 더 고차원적인 문제 해결에 집중할 수 있도록 돕는다는 '증강 지능(augmented intelligence)' 개념과 일맥상통합니다.

**딥마인드(DeepMind) 소속 최소 200명의 직원, 구글의 국방 관련 계약에 반대하는 서한에 서명**
딥마인드(DeepMind)의 최소 200명 직원이 구글의 국방 관련 계약에 대한 불만을 표명했습니다. 지난 5월 내부적으로 배포된 해당 서한에서 직원들은 구글이 군사 기관, 특히 이스라엘 군과 맺은 인공지능(AI) 및 클라우드 컴퓨팅(cloud computing) 서비스 계약에 대해 깊은 우려를 표명했습니다. 이들은 군사 및 무기 생산에 관여하는 행위가 딥마인드(DeepMind)의 핵심 가치와 명시된 인공지능 윤리 원칙에 어긋나며, 윤리적이고 책임감 있는 인공지능 분야의 선도자로서의 위상을 훼손한다고 역설합니다. 이는 2018년 구글이 딥마인드(DeepMind) 기술이 군사적 또는 감시 목적으로 사용되지 않을 것이라고 약속했던 전례를 감안할 때, 구글과 딥마인드(DeepMind) 내부에서 발생하고 있는 가치 충돌을 여실히 보여줍니다. 이러한 내부 갈등은 기술 기업의 사회적 책임과 이익 추구 사이의 균형점에 대한 근본적인 질문을 제기합니다.

**인공지능(AI) 기반 영업 개발 대표(SDRs) 스타트업들이 폭발적인 성장세를 보이고 있지만, 벤처 캐피탈(VCs)은 왜 신중한 태도를 취하는가?**
인공지능(AI) 기반 영업 개발 대표(sales development representatives, SDRs) 스타트업들은 시장에서 급격한 성장세를 경험하고 있으며, 여러 신생 기업들이 단기간에 성공을 거두고 있습니다. 이들 신생 기업들은 대규모 언어 모델(LLM)과 음성 인식 기술 등 인공지능 역량을 활용하여 영업 팀의 콘텐츠 제작 과정을 자동화합니다. 하지만 벤처 캐피탈(venture capitalists) 투자자들은 인간의 직접적인 소통 방식과 비교했을 때 이들의 장기적인 사업 지속 가능성과 실제 효과에 대한 의구심으로 인해 투자를 주저하고 있습니다. 중소기업들은 영업 활동 개선을 위해 AI SDR을 실험하는 데 적극적이지만, 이러한 도구들이 실제로 기업의 판매 증대에 얼마나 효과적인지는 아직 불분명합니다. 또한, 세일즈포스(Salesforce), 허브스팟(HubSpot), 줌인포(ZoomInfo)와 같은 기존의 시장 선도 기업들이 유사한 인공지능 솔루션을 기본 기능으로 무상 제공할 가능성이 있어, AI SDR 스타트업들의 성장에 위협 요인이 될 수 있습니다. 전반적으로 AI SDR의 도입은 빠르게 이루어지고 있으나, 투자자들 사이에서는 이 기술의 시장 내 장기적인 경쟁력에 대한 회의적인 시각이 지배적입니다. 이는 기술 혁신이 비즈니스 모델로 성공하기 위해 단순히 기술적 우수성뿐만 아니라 시장의 구조적 변화와 기존 강자와의 경쟁 구도를 면밀히 분석해야 함을 시사합니다.

**마침내 '오픈소스(open-source) 인공지능(AI)'의 명확한 정의가 수립되었습니다.**
한 그룹이 인공지능(AI) 시스템이 오픈소스(open-source)라는 것이 무엇을 의미하는지에 대한 정의를 마침내 수립했습니다. 이 정의에 따르면, 진정한 오픈소스 인공지능(AI) 시스템은 어떠한 제약 없이 모든 용도로 활용 가능해야 하며, 연구자들이 그 내부 구성 요소들을 분석하고 작동 원리를 파악할 수 있도록 개방되어야 하고, 자유롭게 수정 및 배포될 수 있어야 합니다. 이 기준은 또한 학습 데이터(training data), 원본 코드(source code), 그리고 모델 가중치(weights) 측면에서의 완전한 투명성을 강조합니다. 이러한 정의는 일부 기업들이 마케팅 전략의 일환으로 '오픈소스'라는 용어를 오용해 온 관행 속에서, 인공지능 시스템이 진정으로 오픈소스라는 것이 무엇을 의미하는지에 대한 혼란을 해소하고 명확한 기준을 제시한다는 점에서 큰 의미를 가집니다. 이는 AI 생태계 내에서 공정성과 혁신을 촉진하는 데 기여할 것으로 기대됩니다.

**자율주행 기업 웨이모(Waymo), 청소년 대상 호출 서비스 출시 구상**
알파벳(Alphabet) 산하의 웨이모(Waymo)는 십대들이 보호자 동의 하에 단독으로 차량을 호출하고, 부모에게는 승하차 알림을 전송하는 '웨이모 틴(Waymo Teen)'이라는 구독형 서비스(subscription program) 도입을 검토 중입니다. 이 서비스는 승인된 청소년들이 보호자의 관리 감독 하에 웨이모 차량을 이용하도록 규정할 것입니다. 웨이모는 이 분야 연구에서 긍정적인 피드백을 받았다고 밝혔습니다. 웨이모의 이러한 행보는 작년 우버(Uber)가 자사 플랫폼 내 고평가 운전자들을 청소년 승객과 연결해 준 사례에 뒤이어 이루어지는 것입니다. 서비스 이용을 위해서는 법적 보호자의 명시적 동의가 필수적이며, 보호자는 자녀의 탑승 중 실시간 위치 알림을 받게 됩니다. 이 서비스는 청소년의 이동 자율성을 높이는 동시에 안전 및 책임 소재에 대한 새로운 사회적 논의를 촉발할 것으로 예상됩니다.

**SAG-AFTRA의 인공지능(AI) 관련 권리 보호 파업에 대한 질문에, 아마존 게임즈(Amazon Games) 최고 경영자, 인공지능이 배우들의 '일자리를 빼앗는 것과는 무관하다'고 주장하며 '게임 내에는 연기라는 개념이 없기 때문'이라고 답변**
IGN과의 인터뷰에서 아마존 게임즈(Amazon Games) CEO 크리스토프 하트만(Christoph Hartmann)은 게임 산업에서 생성형 인공지능(generative AI) 사용에 관해 흥미로운 발언을 했습니다. 그는 인공지능이 게임 개발 과정(development cycle)을 단축하는 데 기여할 수 있을 것이라는 기대를 표명했지만, SAG-AFTRA 성우들의 인공지능(AI) 위협으로부터의 더 나은 보호를 위한 파업에 대한 질문에는 "게임에는 연기가 존재하지 않는다"는 충격적인 발언을 했습니다. 이러한 발언은 발더스 게이트 3(Baldur's Gate 3)나 더 라스트 오브 어스(The Last of Us)와 같은 수많은 비디오 게임에서 목소리 연기(voice acting)를 포함한 연기가 차지하는 핵심적인 역할을 간과하는 것이며, 게임 산업 내 창작자들의 기여에 대한 심각한 오해를 드러냅니다. 하트만은 또한 인공지능이 게임 개발을 지원할 수 있는 다른 분야, 특히 현지화(localization)에 대해 언급했습니다. 그러나 현지화는 단순히 언어를 번역하는 것을 넘어 미묘한 문화적 맥락과 감수성을 요구하며, 이는 인공지능만으로는 완벽히 구현되기 어렵다는 점을 유념해야 합니다. 그는 결국 인간의 창의성과 독창성은 기술로 대체될 수 없음을 강조하며 대화를 마무리했습니다. 이 논쟁은 인공지능 시대에 예술과 기술의 경계, 그리고 인간 창작의 본질에 대한 깊은 성찰을 요구합니다.

**인공지능(AI)을 활용한 아동 성 착취물 제작 남성 검거**
플로리다(Florida)주에서 한 남성이 인공지능(AI)으로 생성된 아동 성 착취물(child pornography)을 제작 및 유포한 혐의로 체포되어 20건의 음란물 관련 혐의에 직면했습니다. 필립 마이클 맥코클(Phillip Michael McCorkle)은 인디언 리버 카운티 보안관실(Indian River County Sheriff's Office)이 그가 인공지능 이미지 생성기(AI image generator)를 이용해 아동 성 착취 이미지를 만들고 유포하고 있다는 제보를 받은 후 구금되었습니다. 이번 체포는 생성형 인공지능(generative AI)이 범죄, 특히 아동 학대를 위한 새로운 경로를 제공하며 악의적인 용도로 사용될 수 있는 심각한 위험성을 부각시킵니다. 인공지능으로 생성된 아동 성 착취물의 확산 증가는 각국 의원들로 하여금 이를 불법화하는 법안 추진을 독려하고 있지만, 기술의 특성상 효과적인 차단이 어려운 문제로 남아 있습니다. 국립 실종 및 착취 아동 센터(National Center for Missing & Exploited Children)는 지난 한 해 동안 인공지능으로 생성된 아동 성 착취물에 대한 수천 건의 신고를 접수했으며, 실제 아동의 딥페이크(deepfakes) 또한 생성형 인공지능(generative AI)을 통해 제작되고 있습니다. 이러한 통제 불가능한 문제는 단순한 법적 제재를 넘어, 기술 개발의 윤리적 책임, 플랫폼의 관리 의무, 그리고 국제적인 공조를 통한 근본적인 해결책 마련이 시급함을 보여줍니다.

**최신 연구 논문**
인공지능 분야의 최전선에서 발표된 주목할 만한 연구들을 소개합니다. 각 논문은 특정 문제 해결을 위한 독창적인 접근 방식과 깊이 있는 분석을 제공합니다.

*   시각적 기억(visual memory)을 활용한 유연한 지각(flexible perception) 구현 방안
*   지식 그래프(Knowledge Graphs) 기반 언어 모델(Language Models) 학습: 환각(Hallucinations) 현상 및 그 진단 가능성에 대한 분석
*   에이전트 기반 시스템(Agentic Systems)의 자동화된 구조 설계
*   트랜스퓨전(Transfusion): 단일 다중 모달 모델(Multi-Modal Model)을 통한 다음 토큰(Token) 예측 및 이미지 확산 동시 구현
*   코딩, 혹은 비코딩? 순환 신경망(Recurrent Neural Networks) 사전 학습(Pre-training)에서 코드의 영향력 탐색
*   비선형 표현(Non-Linear Representations)을 이용한 시퀀스(Sequences) 저장 및 생성 학습 방법
*   매직덱(MagicDec): 추측적 디코딩(Speculative Decoding)을 통한 장문 컨텍스트(Context) 생성 시 지연 시간-처리량(Latency-Throughput) 균형 최적화
*   언어 모델(language models)에서 새로운 실험 가설 도출: 교차 여격 일반화(cross-dative generalization)에 대한 심층 연구

**마무리 생각**
오늘 다룬 주제들에 대해 공유하고 싶은 의견이 있으신가요? editor@thegradient.pub으로 언제든지 이메일을 보내주십시오. 다음 뉴스레터에서 독자 여러분의 가장 흥미로운 통찰을 공유할 기회를 모색하겠습니다! 본 뉴스레터가 유익하셨다면, Substack 구독을 통해 Gradient의 활동에 기여하는 것을 고려해 주시면 감사하겠습니다. 이는 저희의 자원봉사 프로젝트를 지속적으로 운영하는 데 큰 힘이 됩니다. 인공지능의 미래는 기술 발전뿐만 아니라, 우리 사회가 이에 어떻게 대응하고 협력하느냐에 달려 있습니다. Gradient의 최신 소식을 읽어주셔서 진심으로 감사합니다!