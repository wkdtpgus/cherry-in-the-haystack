# 🥇 **금주의 최고 인공지능(AI) 논문**

Author: Elvis Saravia
URL: https://nlp.elvissaravia.com/p/top-ai-papers-of-the-week-a34

============================================================

**1. Cell2Sentence-Scale 27B C2S-Scale**
Cell2Sentence-Scale 27B C2S-Scale은 유전자 발현을 "세포 문장(cell sentences)"으로 변환하고 5천만 개 이상의 세포 및 생물학적 텍스트로 LLM을 훈련하여 Cell2Sentence를 확장합니다. 모델은 270억 개의 매개변수(params)로 확장되며 예측, 생성 및 자연어(NL) 해석을 통합합니다. 이중 컨텍스트 가상 스크리닝(dual-context virtual screen)은 실험실에서 검증된 발견으로 이어졌습니다: 실미타서팁(silmitasertib)은 MHC-I 항원 제시(antigen presentation)의 인터페론 조건부 증폭제(interferon-conditional amplifier) 역할을 합니다. 텍스트로서의 데이터(Data-as-text) 및 스케일링 동작(scaling behavior): scRNA-seq 프로파일은 발현 정보를 보존하고 최소한의 손실로 역변환될 수 있는 유전자 이름 시퀀스(gene-name sequences)로 순위가 매겨집니다. 사전 훈련(pretraining)은 5천만 개의 인간 및 마우스 전사체(transcriptomes)에 대한 다중 작업 프롬프트(multi-task prompts)와 논문 및 메타데이터(metadata)를 포함합니다. 성능은 주석(annotation), 조직 추론(tissue inference) 및 조건부 생성(conditional generation) 전반에 걸쳐 4억 1천만 개에서 270억 개 매개변수까지 원활하게 향상됩니다. 광범위한 기능 대 기준선(baselines): 고전적인 단일 세포 작업에서 C2S-Scale은 scGPT 및 Geneformer와 동등하거나 능가합니다. 또한 자연어(NL) 클러스터 캡셔닝(cluster captioning), 데이터셋 수준 요약(dataset-level summarization) 및 질의응답(QA)을 지원하며, 이러한 단일 세포 기반 자연어(NL) 작업에서 GPT-4o와 같은 일반 LLM을 능가합니다. 다중 세포 및 공간 추론(Multi-cell and spatial reasoning): 맞춤형 공간 모듈(bespoke spatial modules) 없이 C2S-Scale은 다중 세포 컨텍스트(multi-cell context)에서 이웃 구조(neighborhood structure)를 예측하며, CellPhoneDB 및 BioGRID의 수용체-리간드(receptor-ligand) 및 PPI(단백질-단백질 상호작용) 지식으로 프롬프트(prompt)될 때 더욱 향상됩니다. 섭동 모델링(Perturbation modeling) 및 새로운 측정 지표(metric): 2단계 파이프라인(pipeline)은 SFT(Supervised Fine-Tuning)를 사용하여 섭동(perturbations)에 조건을 부여한 다음, GRPO를 사용하여 경로에 충실한 예측(pathway-faithful predictions)에 보상을 제공합니다. 이 논문은 이미지 FID(Fréchet Inception Distance)의 임베딩 공간(embedding-space) 유사체인 scFID를 소개하며, 생성된 세포 상태(cell states)의 안정적인 순위를 제공합니다. C2S-Scale은 이전에 보지 못한 사이토카인(cytokine) 조합에서 선두를 달리며, RL(강화 학습) 후 scFID를 낮춥니다. 가상 스크리닝에서 생물학으로: 이중 컨텍스트 스크린(dual-context screen)은 낮은 IFN(인터페론) 환경에서만 항원 제시(antigen presentation)를 증가시키는 약물을 요청했습니다. 모델은 강력한 컨텍스트 분할(context split)로 실미타서팁(silmitasertib)을 지명했으며, 이는 두 가지 인간 세포 모델에서 검증되었습니다: 실미타서팁 단독으로는 효과가 거의 없었지만, 저용량 IFN과 함께 HLA-A,B,C 표면 수준을 증가시켰습니다.
논문 | 트윗

**2. LLM을 위한 강화 학습(RL) 계산 스케일링의 기술**
40만 GPU 시간 이상의 연구는 LLM을 위한 강화 학습(RL)을 확장하는 간단하고 예측 가능한 방법을 제시합니다. 저자들은 작은 실행(small runs)에서 외삽(extrapolate)할 수 있는 시그모이드(sigmoidal) 계산→성능 곡선(compute→performance curve)을 맞추고, 80억 개의 밀집 모델(dense model)과 170억×16 MoE(Mixture-of-Experts) 모델에서 10만 GPU 시간까지 검증된 안정적인 방법인 ScaleRL을 제안합니다. 실제로 사용할 수 있는 예측 스케일링 법칙(Predictive scaling law): 모델 통과율(pass-rate) 대 log(계산량)은 세 가지 조절 변수(knobs)를 가진 포화 시그모이드(saturating sigmoid)를 따릅니다: A (점근적 상한선, asymptotic ceiling), B (계산 효율성, compute efficiency), Cmid (중간점, midpoint). 1천 개의 프롬프트(prompt) 홀드아웃(holdout)에서 약 1.5천 GPU 시간 후에 맞추면 더 큰 예산을 예측할 수 있습니다. 이는 10만 GPU 시간 실행 및 MoE 스케일링(scaling)을 포함하여 실제 확장 훈련과 일치했습니다. 리브-원-아웃(leave-one-out) 테스트에서 유지된 ScaleRL 방법: k=8을 사용하는 PipelineRL, CISPO 손실(truncated IS REINFORCE), 프롬프트 수준 손실 평균화(prompt-level loss averaging), 배치 수준 이점 정규화(batch-level advantage norm), LM 헤드(head)의 FP32 로짓(logits), 제로 분산 프롬프트 필터링(zero-variance prompt filtering), No-Positive-Resampling 커리큘럼(curriculum), 그리고 사고 길이(thinking length)를 제한하기 위한 강제 중단(forced interruptions). 1만 6천 GPU 시간까지의 LOO(Leave-One-Out) 절제 연구(ablations)는 ScaleRL이 유사하거나 더 나은 점근선(asymptotes)을 유지하면서 가장 효율적임을 보여줍니다. 실제로 상한선을 움직이는 것 대 단순히 속도: 모든 인기 있는 강화 학습(RL) 방법이 동일한 A에 수렴하는 것은 아닙니다. 손실 선택(Loss choice)과 로짓(logits)의 정밀도(precision)는 상한선을 높이는 반면, 집계(aggregation), 정규화(normalization), 커리큘럼(curriculum) 및 오프-정책(off-policy) 세부 사항은 주로 B를 조정합니다. 점근선에서 CISPO/GSPO > DAPO; FP32 로짓(logits)은 큰 도약(A≈0.52→0.61)을 제공했습니다. 성과를 낸 스케일링 축(Scaling axes):
*   더 긴 생성 예산(32k까지)은 초기 효율성을 희생하여 점근선을 높입니다.
*   더 큰 전역 배치(global batches)는 점근선과 다운스트림 일반화(downstream generalization)를 개선하여 작은 배치 정체(small-batch stagnation)를 방지합니다.
*   더 큰 모델(MoE)은 80억 개의 밀집 모델보다 적은 계산량으로 훨씬 더 높은 점근적 강화 학습(RL) 성능을 제공합니다.
*   고정된 총 배치 크기에서 프롬프트당 더 많은 생성은 2차적입니다.
안정적인 장기 실행을 위한 운영자 참고 사항: mean@16 생성을 사용하여 홀드아웃된 1천 개의 프롬프트 세트에서 곡선을 맞추고, 불안정성 신호로 절단율(truncation rates)을 관찰하며, 길이 제어를 위해 길이 페널티(length penalties)보다 중단(interruptions)을 선호하고, 먼저 A로 스케일링되는 방법을 선택한 다음 B를 조정하기 위해 초기 소규모 예산 절제 연구(ablations)를 계획하십시오.
논문 | 트윗

**3. 에이전트 추론(Agentic Reasoning)에서 강화 학습(RL)의 신비 해명**
이 논문은 도구 사용 LLM 에이전트(tool-using LLM agents)를 개선하기 위해 강화 학습(RL)을 사용할 때 실제로 효과적인 것이 무엇인지 데이터, 알고리즘, 추론 모드라는 세 가지 축을 통해 연구합니다. 연구팀은 실제 종단 간 SFT(Supervised Fine-Tuning) 데이터셋, 다양한 강화 학습(RL) 세트, 그리고 에이전트 벤치마크(agentic benchmarks)에서 더 큰 모델을 능가하는 컴팩트한 40억 개 매개변수(4B) 에이전트를 제공합니다. 데이터 > 합성. SFT를 위한 실제 종단 간 다중 턴 궤적(multi-turn trajectories)은 이어 붙인 합성 추적(stitched synthetic traces)보다 훨씬 강력한 콜드 스타트(cold-start)를 제공합니다. AIME24/25에서 실제 SFT는 40억 개 및 70억 개 매개변수(4B and 7B) 기반 모델에 대해 average@32 및 pass@32를 크게 향상시킵니다. 다양성은 탐색을 유지합니다: 수학, 과학 및 코드 전반에 걸친 다양화된 강화 학습(RL) 데이터셋은 정책 엔트로피(policy entropy)를 높이고 유지하여 학습 속도를 높이고 훈련을 안정화합니다. 모델 인식 큐레이션(model-aware curation)은 작업 난이도를 능력에 맞춰 약한 모델의 병목 현상(bottlenecks)을 추가로 해결합니다. 간단한 GRPO 조정이 중요합니다: 토큰 수준 집계(token-level aggregation), 더 높은 클립 범위(clip range) 및 과도한 길이 페널티 형성(overlong-penalty shaping)을 사용하는 실용적인 방법(GRPO-TCR)은 최고 정확도와 데이터 효율성 모두에서 표준 GRPO 기준선을 지속적으로 능가합니다. 엔트로피는 적정 지점(sweet spot)이 필요합니다: 정책 엔트로피(policy entropy)가 붕괴되지도 과도하지도 않을 때 훈련이 가장 좋습니다. 클립 상한선(clip upper bound)을 적당히 늘리면 진행 속도가 빨라지지만, 너무 높으면 수렴(convergence)과 안정성(stability)이 저하됩니다. 신중한 모드(Deliberate mode)가 승리합니다: 더 많은 내부 계획(internal planning) 후 더 적고 더 나은 도구 호출(tool calls)은 잦은 호출을 동반하는 반응적인 짧은 사고(reactive short-think)보다 더 높은 도구 사용 성공률과 전반적인 정확도로 이어집니다. Long-CoT는 에이전트에게 플러그 앤 플레이(plug-and-play) 방식이 아닙니다: 기성품 Long-CoT 모델은 추론 중심 작업에서 도구를 피하여 강화 학습(RL) 중 도구 호출 횟수를 0으로 만듭니다. 다중 턴 도구 추적(multi-turn tool traces)을 사용한 SFT는 이를 재정렬할 수 있지만, 명령어 튜닝된(instruction-tuned) 기반 모델은 궁극적으로 에이전트 능력(agentic capability)을 더 깔끔하게 확장합니다. 이 방법으로 컴팩트한 SOTA(State-Of-The-Art) 달성: 3만 개의 다양한 강화 학습(RL) 세트와 조정된 클립 상한선(clip upper bound)을 가진 GRPO-TCR을 사용하여 DemyAgent-4B는 AIME25, GPQA-Diamond, LiveCodeBench-v6를 포함한 에이전트 환경에서 훨씬 더 큰 모델과 동등하거나 능가합니다.
논문 | 트윗

**4. 다중 에이전트 LLM에서 나타나는 조정(Emergent Coordination)**
"이것이 단순히 에이전트들의 묶음인가 아니면 진정한 집단인가?"에 대한 깔끔한 정보 이론적 탐침(information-theoretic probe). 이 논문은 시간 지연 상호 정보(time-delayed mutual information)에 대한 부분 정보 분해(PID, partial-information-decomposition) 테스트를 구축하여 출현(emergence)을 감지하고, 그것이 어디에 존재하는지(정체성 고정(identity-locked) 대 단순한 시간적 결합(mere temporal coupling))를 찾아내며, 이를 성능과 연결합니다. 전역 피드백(global feedback)만 있는 채팅 없는 그룹 이진 탐색 게임(no-chat group binary search game)을 사용하여, 저자들은 프롬프트 설계(페르소나(Personas) + "다른 사람들을 생각하라"는 ToM(Theory of Mind) 프롬프팅)를 통해 느슨한 집합체에서 목표 지향적이고 상호 보완적인 팀으로 집단(collectives)을 이끌 수 있음을 보여줍니다. 프레임워크(Framework): 시간에 따른 결과 관련 PID(outcome-relevant PID). 세 가지 진단: 실용적 기준(Practical criterion): 시점 t에서의 거시 신호(macro signal)가 어떤 단일 에이전트(single agent)를 넘어서 시점 t+ℓ에서의 거시 신호를 예측하는가? 양수 값은 동적 시너지(dynamical synergy)를 나타냅니다. 출현 능력(Emergence capacity): 미래 공동 상태(future joint states)를 예측하기 위한 쌍별 PID 시너지(pairwise PID synergy)로, 어떤 단일 에이전트도 가지고 있지 않은 "오직 함께" 정보를 포착합니다. 연합 테스트(Coalition test): 연합(coalitions)이 추가적인 목표 관련 예측 가능성(goal-relevant predictability)을 가지고 있는지 확인하기 위한 삼중항 정보 I3 대 최상의 쌍(G3). 실험: 통신 없는 그룹 추측. 에이전트들은 0-50 사이의 정수를 추측합니다; "너무 높음/낮음"만 전체 그룹에 반환됩니다. 조건: 일반(Plain), 페르소나(Persona), 그리고 페르소나 + ToM("다른 사람들이 무엇을 할지 생각하라"). GPT-4.1의 주요 발견: 출현은 실제적이며 조종 가능합니다. 실용적 기준과 출현 능력 모두 견고성 검사(robustness checks)를 거쳐 모든 조건에서 0보다 크며, 이는 동적 시너지(dynamical synergy)를 나타냅니다. 페르소나(Personas)는 안정적이고 정체성과 연결된 차별화(identity-linked differentiation)를 유도합니다; ToM을 추가하면 상호 보완성(complementarity)을 유지하면서 공유 목표에 대한 정렬(alignment)이 증가합니다. 삼중항 구조(Triplet structure)가 중요합니다. 많은 그룹에서 G3>0을 보이는데, 이는 어떤 쌍으로도 충분하지 않다는 것을 의미합니다; 전체 삼중항은 거시 신호(macro signal)에 대한 예측 정보를 추가합니다. ToM은 더 높은 총 상호 정보 I3(더 강력한 공유 목표 정렬)와 상당한 I3를 가진 더 많은 그룹을 가집니다. 성능은 균형에서 나옵니다. 시너지(Synergy) 단독 또는 중복성(redundancy) 단독으로는 성공을 예측하지 못합니다; 그들의 상호 작용이 예측합니다. 중복성(Redundancy)은 시너지(synergy)의 효과를 증폭시키고 그 반대도 마찬가지이며, 이는 통합(integration) + 차별화(differentiation)가 승리하는 체제(regime)라는 것과 일치합니다. 매개 분석(Mediation)은 ToM이 시너지(synergy)를 증가시킴으로써 간접적으로 성공을 촉진한다고 제안합니다. 저용량 모델 대비(Llama-3.1-8B): 그룹은 대부분 실패합니다; 행동은 강한 시간적 진동(time coupling)을 보이지만 약한 에이전트 간 상호 보완성(cross-agent complementarity)을 보입니다. 여기서는 ToM이 일반(Plain)보다 오히려 해를 끼치는데, 이는 ToM 스타일 프롬프팅(prompting)이 충분한 모델 용량(model capacity)을 필요로 함을 강조합니다. AI 개발자를 위한 실용적인 시사점: 상호 보완적인 역할과 공유된 목표 신호(target signals)를 위해 설계하십시오. 가벼운 페르소나(light personas)를 사용하여 정체성과 연결된 행동(identity-linked behaviors)을 안정화하십시오; ToM 스타일 추론(ToM-style reasoning)을 추가하여 에이전트들이 거시적 목표(macro objective)에 정렬하면서 서로에게 적응하도록 유도하십시오. 추측하지 말고 측정하십시오. 거시적 예측 가능성(macro predictability, 실용적 기준), 쌍별 시너지(pairwise synergy, 능력), 연합 가산성(coalition additivity, G3)을 추적하여 팀이 실제 집단인지 동기화된 진동자(synchronized oscillators)인지 진단하십시오. 가짜 출현(spurious emergence)을 경계하십시오. 행 셔플(row-shuffle, 정체성 파괴) 및 열 셔플(column-shuffle, 에이전트 간 정렬 파괴) 널(nulls)을 사용하여 좋은 시너지(synergy)를 단순한 시간적 결합(temporal couplings)과 분리하십시오.
논문 | 트윗

**5. Elastic-Cache**
확산 LLM(diffusion LLM) 디코딩(decoding)을 빠르고 훈련 없이, 아키텍처에 구애받지 않는 방식으로 만드는 방법으로, KV 캐시(KV caches)를 필요할 때 필요한 곳에서만 업데이트합니다. 모든 디노이징(denoising) 단계에서 모든 토큰(tokens)에 대해 QKV를 재계산하는 대신, Elastic-Cache는 가장 많이 주목받는 토큰(most-attended tokens)에서 어텐션 드리프트(attention drift)를 관찰하고, 얕은(shallow) 및 창 밖(off-window) 캐시(caches)를 재사용하면서 더 깊은 레이어(deeper layers)만 새로 고칩니다. 결과: 수학, 코드 및 멀티모달(multimodal) 작업 전반에서 최소한의 또는 전혀 없는 정확도 손실로 큰 속도 향상. 핵심 아이디어: 슬라이딩 윈도우 디코딩(Sliding-window decoding)은 가까운 MASK 토큰(tokens)만 "활성(live)" 상태로 유지하고, 멀리 떨어진 MASK는 길이 사전(length prior)으로 블록 캐시(block-caches)합니다. 어텐션 인식 드리프트 테스트(attention-aware drift test)는 이전 단계의 가장 많이 주목받는 토큰(most-attended tokens)의 코사인 유사도(cosine similarity) 변화를 측정합니다; 유사도가 레이어 ℓ에서 임계값 γ 아래로 떨어지면, 재계산은 ℓ+1부터 L까지 시작됩니다. 얕은 레이어(Shallow layers)는 캐시(caches)를 재사용하고; 깊은 레이어(deep layers)는 새로 고칩니다. 작동 원리: KV 드리프트(drift)는 대부분의 단계에서 작고 깊이에 따라 증가하므로, 모든 레이어(layers)를 새로 고치는 것은 낭비입니다. 가장 많이 주목받는 토큰(most-attended token)은 가장 적은 KV 변화를 보여주며, 새로 고침을 트리거(trigger)하는 보수적인 하한선(lower bound)을 제공합니다. 시각화 지원: 멀리 떨어진 MASK는 영향이 거의 없습니다; KV 및 어텐션(attention) 변화는 일치합니다; 가장 많이 주목받는 토큰(most-attended tokens)은 가장 적게 드리프트(drift)합니다. 실무자를 위한 알고리즘 조절 변수(knobs): 임계값 γ는 속도-정확도 트레이드오프(speed-accuracy tradeoff)를 제어합니다; γ가 낮을수록 업데이트가 적고 더 빠르게 실행됩니다. 윈도우 크기 β는 단계당 계산량(per-step compute)을 더 적은 단계로 교환합니다. 신뢰도 인식 병렬 디코딩(confidence-aware parallel decoding, ϵ)과 함께 작동하며, 더 높은 γ에서도 낮은 업데이트 빈도(update frequency)를 보여줍니다. 사용된 기본값: γ 0.9, ϵ 0.9, 일반적인 β 16–32. 중요한 결과: LLaDA 및 LLaDA-1.5에서: GSM8K-512에서 동일한 정확도로 최대 45.1배 처리량(throughput), GSM8K-256에서 8.7배, HumanEval에서 기준선 대비 정확도 유지 또는 향상으로 4.8–5.0배. LLaDA-V에서는 MathVerse 정확도를 유지하면서 처리량(throughput)이 증가합니다. Elastic-Cache는 유사하거나 더 나은 정확도에서 Fast-dLLM을 토큰/초(tokens/sec) 면에서 지속적으로 능가하며, 그 처리량(throughput)은 더 긴 생성에서 유리하게 확장됩니다. 배포 참고 사항: 훈련 또는 아키텍처 변경이 필요하지 않습니다. 기존의 신뢰도 기반(confidence-based) 및 간격 정책(interval policies)과 호환됩니다. 병렬 처리(parallelism)를 보존하기 위해 가변 길이 시퀀스(variable-length sequences)를 연결하는 실용적인 배치 구현(batch implementation)을 포함합니다. 윤리적 및 재현성 세부 사항과 코드 계획이 포함되어 있습니다.
논문 | 트윗

**6. LLM의 동적 레이어 라우팅(Dynamic Layer Routing)**
각 블록(block)을 건너뛸지, 실행할지, 반복할지를 결정하는 레이어별 라우터(per-layer routers)를 고정된 LLM에 추가하는 개조 가능한(retrofittable) 방법. 경로는 레이어 편집(layer edits)에 대한 짧은 몬테카를로 트리 탐색(Monte Carlo Tree Search)으로 오프라인에서 감독된 다음, 검색 없이 온라인에서 실행됩니다. 논리 및 수학에서 정확도를 향상시키면서 평균적으로 레이어(layers)를 절약합니다. 이것은 무엇인가: 각 레이어(layer)에 부착된 작은 MLP 라우터(routers)는 윈도우 평균 풀링된 은닉 상태(windowed mean-pooled hidden states)를 읽고 세 가지 동작 중 하나를 출력합니다: 건너뛰기(skip), 한 번 실행(execute once), 또는 한 번 반복(repeat once). 기본 가중치(Base weights)는 고정된 상태를 유지하고 KV 캐싱(KV caching)은 호환됩니다. 3페이지의 다이어그램은 레이어별 라우터(per-layer router), 윈도우(windows)에 대한 풀링(pooling), 그리고 결정이 다음 블록(block)을 어떻게 제어하는지 보여줍니다. 감독 작동 방식: 길이 인식 MCTS(Length-aware MCTS)는 계산 예산(compute budget) 내에서 레이어(layers)를 건너뛰거나 반복하는 편집된 순방향 패스(forward passes)를 탐색하고, 정답 보상(gold-answer reward)을 보존하거나 개선하는 경로만 유지합니다. 라우터(Routers)는 약 4천 개의 발견된 경로(paths)에 대해 포컬 손실(focal loss)과 클래스 재조정(class rebalancing)으로 훈련됩니다. AI 개발자를 위한 주요 결과: 6개의 백본(backbones)에 걸친 ARC 및 DART에서 라우터(routers)는 쿼리(query)당 약 3개에서 11개의 레이어(layers)를 줄이면서 정확도를 높입니다. 예시: LLaMA-3B-Base는 DART에서 11.8%에서 15.8%로 상승하고 평균 4.1개의 레이어(layers)를 절약합니다. 명령어 튜닝된(Instruction-tuned) 80억 개 매개변수(8B) 모델도 DART에서 이득을 얻으면서 11개의 레이어(layers)를 절약합니다. 도메인 외 일반화(Out-of-domain generalization)가 강력합니다. MMLU, GSM8k, AIME24, TruthfulQA, SQuADv2, GPQA, AGIEval, PIQA 전반에서 평균 정확도 하락은 약 0.85% 포인트이며 절약은 유지됩니다. LayerSkip, ShortGPT, MindSkip, FlexiDepth와 비교하여 Dr.LLM은 훨씬 적은 훈련 데이터와 기본 모델 변경 없이 더 높은 평균 정확도를 달성합니다. 도움이 되는 이유: 8페이지와 17페이지의 분석 히트맵(heatmaps)은 일관된 패턴을 보여줍니다. 초기 레이어(layers)는 안정적으로 유지되고, 많은 중간 레이어(layers)는 건너뛰어지며, 특히 더 어려운 수학 문제에서는 후기 레이어(layers)가 때때로 반복되는데, 이는 반복적인 정제(iterative refinement)가 효과적인 곳에 깊이(depth)를 재할당합니다.
논문 | 트윗

**7. LLM은 "뇌 부패(Brain Rot)"를 겪을 수 있다!**
저자들은 명확한 가설을 테스트합니다: 사소하고 매우 매력적인 웹 텍스트에 대한 지속적인 사전 훈련(continual pretraining)은 완화 후에도 지속되는 방식으로 LLM 인지(cognition)를 저하시킵니다. 그들은 규모(scale)와 훈련 작업(training ops)으로부터 데이터 품질(data quality)을 분리하기 위해 통제된 트위터 데이터셋을 구축한 다음, 추론(reasoning), 긴 컨텍스트(long-context), 안전(safety) 및 성격(personality)에 미치는 영향을 측정합니다. 데이터 품질을 분리하는 설정: 두 가지 직교적인 정크(junk) 정의: M1은 인기 있고 짧은 게시물을 포착하기 위해 참여 신호(engagement signals)와 짧은 길이를 사용합니다; M2는 클릭베이트(clickbait) 및 피상적인 주제와 같은 의미론적 단서(semantic cues)를 사용합니다. 네 개의 명령어 모델(instruct models)은 일치하는 토큰 수(token counts)로 지속적으로 사전 훈련된 다음, 재명령어 튜닝(re-instruction tuned)되어 제어 데이터(control data)와 직접적인 비교가 가능합니다. 용량 반응(dose response)에 따른 비사소한 능력 저하: 모델 전반에 걸쳐 정크(junk) 노출은 ARC 추론(reasoning), 긴 컨텍스트 검색(long-context retrieval) 및 안전(safety)을 감소시키며, 헤지스 g(Hedges’ g)는 0.3을 초과합니다. M1 정크(junk) 비율을 늘리면 부드러운 하락이 발생합니다. 예를 들어, CoT를 사용한 ARC-Challenge는 74.9에서 57.2로, RULER CWE는 0%에서 100% 정크(junk)까지 84.4에서 52.3으로 떨어집니다. 사고 건너뛰기(Thought-skipping)가 주요 손상(lesion)입니다: ARC CoT에 대한 오류 포렌식(Error forensics)은 사고 없음, 계획 없음, 계획된 단계 건너뛰기가 지배적인 실패를 보여주며, 이는 오류의 98% 이상을 설명합니다. 인기는 길이보다 추론(reasoning)에 대한 이러한 부패(rot)의 더 강력한 예측 변수이며, 길이는 긴 컨텍스트(long-context)에 더 중요합니다. 안전(Safety) 및 "어두운 특성(dark traits)"은 M1에서 악화됩니다: 정크 훈련(Junk training)은 HH-RLHF 및 AdvBench에서 위험을 높이고, 자기애(narcissism) 및 정신병(psychopathy) 점수를 부풀리는 동시에, 친화성(agreeableness)을 낮춥니다. 성격(Personality) 및 안전(safety) 결과는 M1과 M2 사이에서 달라지며, 이는 참여 신호(engagement signals)가 품질의 해로운 비의미론적 축(non-semantic axis)을 포착한다는 것을 강조합니다. 완화는 도움이 되지만 치유하지는 못합니다: 더 강력한 모델을 사용한 외부 반성(External reflection)은 사고 건너뛰기(thought-skipping)를 줄이고 정확도를 회복합니다; 자기 반성(self-reflection)은 그렇지 않습니다. 명령어 튜닝(instruction tuning) 및 깨끗한 지속적 훈련(clean continual training)을 확장하면 점수가 향상되지만 기준선과의 격차를 좁히지 못하며, 이는 지속적인 표현 드리프트(representational drift)를 나타냅니다.
논문 | 트윗

**8. 하이브리드 강화 학습(Hybrid Reinforcement) HERO (Hybrid Ensemble Reward Optimization)**
HERO(Hybrid Ensemble Reward Optimization)는 이진 검증자 피드백(binary verifier feedback)과 연속적인 보상 모델 신호(continuous reward-model signals)를 결합하여 LLM 추론(reasoning)을 개선하는 강화 학습(reinforcement learning) 프레임워크(framework)입니다. 계층화된 정규화(stratified normalization)와 분산 인식 가중치(variance-aware weighting)를 사용하여 HERO는 정확성과 뉘앙스(nuance)의 균형을 맞추며, 다양한 수학 추론 벤치마크(math reasoning benchmarks)에서 검증자 전용(verifier-only) 및 RM 전용(RM-only) 방법을 능가하고, 검증 가능(verifiable)하고 모호한(ambiguous) 작업 모두에서 성능을 향상시킵니다.
논문 | 트윗

**9. Kimi-Dev**
Kimi-Dev는 소프트웨어 엔지니어링 LLM에 대한 스킬 사전(skill prior)으로 에이전트 없는 훈련(agentless training)을 도입하여 워크플로우 스타일(workflow-style)과 에이전트 패러다임(agentic paradigms)을 연결합니다. 구조화되고 검증 가능한 단일 턴 작업(single-turn tasks)으로 훈련되어 워크플로우 모델(workflow models)의 기록인 SWE-bench Verified에서 60.4%를 달성했으며, 5천 개의 궤적 미세 조정(trajectory fine-tuning) 후 SWE-Agent pass@1에서 48.6%를 가능하게 하여 Claude 3.5 Sonnet과 경쟁합니다. 이 연구는 추론 중심의 에이전트 없는 훈련(reasoning-heavy agentless training)이 지역화(localization), 코드 편집(code editing) 및 반성(reflection)에서 전이 가능한 사전 지식(transferable priors)을 구축하여 효율적인 SWE-Agent 적응(adaptation)을 위한 기반을 형성함을 보여줍니다.
논문 | 트윗

**10. 전체론적 에이전트 리더보드(Holistic Agent Leaderboard)**
HAL(Holistic Agent Leaderboard)은 코딩, 웹 탐색, 과학 및 고객 서비스를 아우르는 9개 모델과 9개 벤치마크(benchmarks)에 걸쳐 대규모의 재현 가능한 AI 에이전트 평가(AI agent evaluation)를 위한 표준화된 프레임워크(framework)를 도입합니다. 이는 평가 시간을 몇 주에서 몇 시간으로 단축하고, 작업 이탈 행동(off-task actions)과 같은 주요 행동 결함(behavioral flaws)을 드러내며, 25억 개의 토큰(tokens)에 달하는 에이전트 로그(agent logs)를 제공하여 벤치마크 성능(benchmark performance)보다 실제 신뢰성(real-world reliability)을 향한 연구를 추진합니다.
논문 | 트윗