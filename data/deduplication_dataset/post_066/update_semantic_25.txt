**1. Cell2Sentence-Scale 27B C2S-Scale: 생물학적 언어 모델의 지평을 넓히다**

Cell2Sentence-Scale 27B (C2S-Scale)는 생명 현상의 기본 단위인 세포 내부의 유전자 활동 양상을 '세포 서술문(cell sentences)'의 형태로 전환하고, 5천만 개 이상의 세포 데이터와 방대한 생물학적 텍스트를 활용하여 대규모 언어 모델(LLM)을 훈련함으로써 Cell2Sentence의 역량을 대폭 증진시킨 혁신적인 접근 방식입니다. 이 모델은 270억 개에 달하는 학습 가능한 가중치(parameters)로 규모를 키워, 생물학적 예측, 새로운 분자 생성, 그리고 자연어(NL) 기반의 생물학적 현상 해석 기능을 유기적으로 통합합니다. 이러한 통합적 역량은 이중 컨텍스트 가상 스크리닝(dual-context virtual screen)이라는 독특한 방법론을 통해 실험실에서 검증된 중요한 발견으로 이어졌는데, 바로 실미타서팁(silmitasertib)이라는 물질이 MHC-I 항원 제시(antigen presentation) 과정을 인터페론(interferon) 조건부로 증폭시키는 역할을 한다는 사실입니다. 이는 약물 재창출(drug repurposing) 및 면역 치료법 개발에 새로운 가능성을 제시합니다.

데이터를 텍스트 형태로 다루는 '데이터-온-텍스트(Data-as-text)' 패러다임과 모델의 규모 확장에 따른 성능 변화(scaling behavior)는 C2S-Scale의 핵심입니다. 단일 세포 RNA 시퀀싱(scRNA-seq) 프로파일은 유전자 발현 정보를 보존하며, 역변환 시 정보 손실을 최소화하는 유전자 이름 순서(gene-name sequences)로 순위가 매겨져 텍스트화됩니다. 이는 복잡한 생물학적 데이터를 LLM이 이해하고 처리할 수 있는 형태로 변환하는 데 중요한 역할을 합니다. 사전 훈련(pretraining) 과정에서는 5천만 개에 이르는 인간 및 마우스 전사체(transcriptomes) 데이터뿐만 아니라, 다양한 다중 작업 프롬프트(multi-task prompts), 관련 연구 논문 및 메타데이터(metadata)가 활용되어 모델의 폭넓은 생물학적 지식 기반을 구축합니다. 모델의 성능은 4억 1천만 개에서 270억 개 매개변수까지 규모가 커짐에 따라 세포 주석(annotation), 조직 유형 추론(tissue inference), 그리고 조건부 세포 상태 생성(conditional generation) 등 다양한 작업에서 꾸준히 향상되는 양상을 보입니다.

기존의 단일 세포 분석 도구들과 비교했을 때, C2S-Scale은 고전적인 단일 세포 분석 과제에서 scGPT나 Geneformer와 동등하거나 그 이상의 뛰어난 성능을 발휘합니다. 또한, 자연어 클러스터 캡셔닝(NL cluster captioning), 데이터셋 수준 요약(dataset-level summarization), 그리고 질의응답(QA)과 같은 단일 세포 기반 자연어 처리(NL) 작업에서는 GPT-4o와 같은 범용 LLM을 능가하는 독보적인 능력을 보여줍니다. 이는 생물학 분야에 특화된 언어 모델의 강력한 잠재력을 입증하는 것입니다. 다중 세포 및 공간 추론(Multi-cell and spatial reasoning) 능력에 있어서 C2S-Scale은 특별한 공간 전용 모듈(bespoke spatial modules) 없이도 다중 세포 환경 내에서 세포 간의 이웃 구조(neighborhood structure)를 성공적으로 예측하며, CellPhoneDB 및 BioGRID와 같은 데이터베이스의 수용체-리간드(receptor-ligand) 및 단백질-단백질 상호작용(PPI) 지식을 활용하여 그 예측 정확도를 더욱 높입니다.

섭동 모델링(Perturbation modeling)과 새로운 측정 지표(metric) 개발 또한 C2S-Scale의 중요한 기여입니다. 2단계 파이프라인(pipeline)은 지도 미세 조정(SFT, Supervised Fine-Tuning)을 사용하여 약물이나 유전자 조작과 같은 섭동(perturbations) 조건을 부여한 다음, GRPO(Gradient-based Reward Policy Optimization)를 활용하여 생물학적 경로에 충실한 예측(pathway-faithful predictions)에 보상을 제공합니다. 이 연구는 이미지 분야의 FID(Fréchet Inception Distance)와 유사하게 임베딩 공간(embedding-space)에서의 유사도를 측정하는 단일 세포 Fréchet Inception Distance (scFID)라는 새로운 지표를 도입하여, 생성된 세포 상태(cell states)의 안정적인 순위를 제공합니다. C2S-Scale은 강화 학습(RL) 적용 후 scFID 값을 효과적으로 낮추며, 이전에 관찰되지 않은 사이토카인(cytokine) 조합에 대한 반응 예측에서 선도적인 성능을 보입니다. 가상 스크리닝에서 생물학적 검증으로의 연결은 이 모델의 실용적 가치를 명확히 보여줍니다. 연구팀은 낮은 IFN(인터페론) 환경에서만 항원 제시를 증가시키는 약물을 찾아달라고 모델에 요청했으며, 모델은 강력한 컨텍스트 분할(context split)을 통해 실미타서팁(silmitasertib)을 제안했습니다. 이 예측은 두 가지 인간 세포 모델에서 검증되었는데, 실미타서팁 단독으로는 효과가 미미했지만, 저용량 IFN과 함께 투여했을 때 HLA-A,B,C 표면 수준을 유의미하게 증가시켰습니다. 이는 LLM이 실제 생물학적 발견으로 이어질 수 있음을 증명하는 중요한 사례입니다.
논문 | 트윗

**2. LLM을 위한 강화 학습(RL) 계산 스케일링의 기술: 예측 가능한 대규모 훈련의 길**

대규모 언어 모델(LLM)을 위한 강화 학습(RL)은 그 복잡성과 막대한 계산 요구량으로 인해 효율적인 훈련 전략이 필수적입니다. 40만 GPU 시간 이상이 투입된 이 연구는 LLM의 강화 학습 훈련 과정을 효율적이고 예측 가능하게 확장하는 간명한 방법론을 제시합니다. 연구자들은 초기 소규모 실험 결과로부터 전체 훈련 과정을 예측할 수 있는 시그모이드(sigmoidal) 형태의 계산량 대비 성능 곡선(compute→performance curve)을 도출했습니다. 이 곡선은 80억 개의 밀집 모델(dense model)과 170억×16 MoE(Mixture-of-Experts) 모델에서 10만 GPU 시간 규모까지 검증된 안정적인 방법인 ScaleRL을 제안합니다. 이는 막대한 컴퓨팅 자원을 투입하기 전에 훈련 성과를 예측하고 최적의 전략을 수립하는 데 결정적인 도움을 줍니다.

실질적인 예측 스케일링 법칙(Predictive scaling law)에 따르면, 모델의 통과율(pass-rate)과 로그 스케일의 계산량 사이의 관계는 포화형 시그모이드 함수를 따릅니다. 이 함수는 세 가지 주요 조절 변수(knobs)를 가집니다: A(점근적 상한선, asymptotic ceiling), B(계산 효율성, compute efficiency), Cmid(중간점, midpoint). 약 1.5천 GPU 시간의 훈련 후 1천 개의 프롬프트(prompt) 홀드아웃(holdout) 데이터에 이 함수를 적용하면, 훨씬 더 큰 예산이 필요한 훈련 시나리오의 결과를 정확하게 예측할 수 있습니다. 이러한 예측은 10만 GPU 시간 규모의 훈련과 MoE 모델의 스케일링을 포함한 실제 확장 훈련 결과와 일치하여 그 신뢰성을 입증했습니다.

ScaleRL 방법론은 리브-원-아웃(leave-one-out) 테스트를 통해 그 견고성이 확인되었습니다. 주요 구성 요소로는 PipelineRL, CISPO 손실(truncated IS REINFORCE), 프롬프트 수준 손실 평균화(prompt-level loss averaging), 배치 수준 이점 정규화(batch-level advantage norm), LM 헤드(head)의 FP32 로짓(logits) 사용, 제로 분산 프롬프트 필터링(zero-variance prompt filtering), No-Positive-Resampling 커리큘럼(curriculum), 그리고 사고 길이(thinking length)를 제한하기 위한 강제 중단(forced interruptions) 등이 있습니다. 1만 6천 GPU 시간까지의 절제 연구(ablations)는 ScaleRL이 유사하거나 더 나은 점근선(asymptotes)을 유지하면서도 가장 효율적인 성능을 제공함을 보여주었습니다.

이 연구는 단순히 훈련 속도(B)를 높이는 것뿐만 아니라, 성능의 점근적 상한선(A) 자체를 끌어올리는 것의 중요성을 강조합니다. 모든 인기 있는 강화 학습 방법이 동일한 A에 수렴하는 것은 아니며, 손실 함수 선택과 로짓(logits)의 정밀도(precision)가 상한선(A)을 높이는 데 결정적인 영향을 미칩니다. 특히, FP32 로짓을 사용했을 때 A 값이 약 0.52에서 0.61로 크게 도약하는 것을 확인했습니다. 반면, 집계(aggregation), 정규화(normalization), 커리큘럼, 그리고 오프-정책(off-policy) 세부 사항들은 주로 계산 효율성(B)을 조절하는 역할을 합니다. 점근선 성능 측면에서는 CISPO/GSPO가 DAPO보다 우수했습니다.

성능 스케일링의 주요 축(Scaling axes)은 다음과 같습니다. 첫째, 더 긴 생성 예산(최대 32k 토큰)은 초기 효율성을 다소 희생하더라도 최종 점근선 성능을 향상시킵니다. 둘째, 더 큰 전역 배치(global batches)는 소규모 배치 정체(small-batch stagnation)를 방지하여 점근선과 다운스트림 일반화(downstream generalization)를 개선합니다. 셋째, 80억 개의 밀집 모델보다 더 적은 계산량으로 MoE(Mixture-of-Experts)와 같은 더 큰 모델이 훨씬 더 높은 점근적 강화 학습 성능을 제공합니다. 이는 모델 아키텍처 선택의 중요성을 시사합니다. 마지막으로, 고정된 총 배치 크기에서 프롬프트당 더 많은 생성을 수행하는 것은 이차적인 효과를 가집니다.

실무자를 위한 안정적인 장기 실행 지침도 제공됩니다. 평균 16개 생성(mean@16 generations)을 사용하여 홀드아웃된 1천 개의 프롬프트 세트에서 성능 곡선을 맞추고, 불안정성 신호로 절단율(truncation rates)을 면밀히 관찰해야 합니다. 길이 제어를 위해서는 길이 페널티(length penalties)보다 강제 중단(interruptions)을 선호하며, A 값을 스케일링하는 방법을 먼저 선택한 다음 B 값을 조정하기 위한 초기 소규모 예산 절제 연구(ablations)를 계획하는 것이 효과적입니다. 이러한 지침은 LLM 강화 학습 훈련의 효율성과 예측 가능성을 극대화하여 연구 및 개발 비용을 절감하는 데 기여합니다.
논문 | 트윗

**3. 에이전트 추론(Agentic Reasoning)에서 강화 학습(RL)의 신비 해명: 효과적인 에이전트 구축 전략**

이 연구는 도구 활용 대규모 언어 모델(LLM) 에이전트의 성능을 향상시키기 위해 강화 학습(RL)을 적용할 때 실제로 어떤 요소들이 중요한지 세 가지 핵심 축, 즉 데이터, 알고리즘, 그리고 추론 방식에 초점을 맞춰 심층적으로 탐구합니다. 연구팀은 실제 종단 간(end-to-end) 지도 미세 조정(SFT) 데이터셋과 다양하게 구성된 강화 학습(RL) 세트를 구축했으며, 이러한 노력의 결과로 40억 개 매개변수(4B)의 컴팩트한 에이전트가 에이전트 벤치마크(agentic benchmarks)에서 훨씬 더 큰 모델들을 능가하는 놀라운 성과를 달성했음을 보여줍니다. 이는 모델 크기만이 성능을 결정하는 유일한 요소가 아님을 시사합니다.

데이터의 중요성은 이 연구의 핵심 주장 중 하나입니다. SFT를 위해 실제 환경에서 수집된 종단 간 다중 턴 궤적(multi-turn trajectories)은 단순히 이어 붙인 합성 추적(stitched synthetic traces)보다 훨씬 강력한 콜드 스타트(cold-start) 성능을 제공합니다. AIME24/25 벤치마크에서 실제 SFT 데이터는 40억 개 및 70억 개 매개변수(4B and 7B) 기반 모델의 average@32 및 pass@32 점수를 크게 향상시켰습니다. 이는 실제 사용 환경에서 얻은 양질의 데이터가 모델의 초기 학습에 얼마나 중요한 영향을 미치는지 보여줍니다.

데이터의 다양성 또한 탐색 능력을 유지하는 데 필수적입니다. 수학, 과학, 그리고 코드 생성 등 다양한 영역에 걸쳐 다각화된 강화 학습 데이터셋은 정책 엔트로피(policy entropy)를 높게 유지함으로써 학습 속도를 가속화하고 훈련의 안정성을 확보합니다. 나아가, 모델의 능력을 고려하여 작업 난이도를 조절하는 '모델 인식 큐레이션(model-aware curation)' 기법은 약한 모델이 겪는 병목 현상(bottlenecks)을 효과적으로 해소하여 전반적인 학습 효율을 증대시킵니다.

간단한 GRPO(Gradient-based Reward Policy Optimization) 조정이 성능 향상에 결정적인 역할을 한다는 점도 밝혀졌습니다. 토큰 수준 집계(token-level aggregation), 더 높은 클립 범위(clip range), 그리고 과도한 길이 페널티 형성(overlong-penalty shaping)을 사용하는 실용적인 방법인 GRPO-TCR은 정확도와 데이터 효율성 측면에서 표준 GRPO 기준선을 지속적으로 능가하는 성능을 보였습니다. 이는 알고리즘의 미세한 조정이 실제 에이전트 성능에 큰 영향을 미칠 수 있음을 의미합니다.

정책 엔트로피(policy entropy)는 적정 지점(sweet spot)을 찾아야 합니다. 훈련은 정책 엔트로피가 너무 낮아 탐색이 부족하지도, 너무 높아 불안정하지도 않은 적절한 수준에서 가장 잘 이루어집니다. 클립 상한선(clip upper bound)을 적절히 늘리면 학습 진행 속도가 빨라지지만, 지나치게 높이면 수렴(convergence)과 안정성(stability)이 저하될 수 있습니다. 이는 탐색과 활용(exploration-exploitation) 사이의 미묘한 균형점을 찾는 것이 중요함을 강조합니다.

'신중한 모드(Deliberate mode)'의 추론 방식이 '반응적인 짧은 사고(reactive short-think)'보다 우월하다는 점도 주목할 만합니다. 내부적인 계획 과정(internal planning)을 충분히 거친 후 더 적고 더 나은 도구 호출(tool calls)을 수행하는 방식이, 잦은 호출을 동반하는 즉각적인 반응보다 더 높은 도구 사용 성공률과 전반적인 정확도를 가져왔습니다. 이는 에이전트가 단기적인 반응보다는 장기적인 계획을 수립하는 능력을 갖추는 것이 중요함을 시사합니다.

흥미롭게도, Long-CoT(Long Chain-of-Thought)는 에이전트에게 '플러그 앤 플레이(plug-and-play)' 방식으로는 작동하지 않았습니다. 기존의 Long-CoT 모델은 추론 중심 작업에서 도구 사용을 회피하는 경향이 있어, 강화 학습 중 도구 호출 횟수가 사실상 0이 되는 문제가 발생했습니다. 비록 다중 턴 도구 추적(multi-turn tool traces)을 사용한 SFT가 이러한 문제를 어느 정도 해결할 수 있었지만, 궁극적으로는 명령어 튜닝된(instruction-tuned) 기반 모델이 에이전트 능력(agentic capability)을 더 깔끔하게 확장하는 데 효과적이었습니다.

이러한 연구를 통해 DemyAgent-4B는 3만 개의 다양한 강화 학습 세트와 조정된 클립 상한선(clip upper bound)을 가진 GRPO-TCR을 사용하여 AIME25, GPQA-Diamond, LiveCodeBench-v6를 포함한 다양한 에이전트 환경에서 훨씬 더 큰 모델들과 동등하거나 그 이상의 최첨단(SOTA) 성능을 달성했습니다. 이는 작고 효율적인 에이전트가 대규모 모델에 필적하는 지능을 발휘할 수 있음을 보여주며, 에이전트 개발의 새로운 방향을 제시합니다.
논문 | 트윗

**4. 다중 에이전트 LLM에서 나타나는 조정(Emergent Coordination): 집단 지성의 정보 이론적 탐구**

대규모 언어 모델(LLM) 기반의 다중 에이전트 시스템이 단순한 개별 에이전트의 집합체(bundle)를 넘어 진정한 집단(collective)으로 기능하는지 여부는 인공지능 분야의 중요한 질문입니다. 이 논문은 "이것이 단순히 에이전트들의 묶음인가 아니면 진정한 집단인가?"라는 본질적인 질문에 답하기 위해 정보 이론적 탐침(information-theoretic probe)을 제시합니다. 연구는 시간 지연 상호 정보(time-delayed mutual information)에 대한 부분 정보 분해(PID, partial-information-decomposition) 테스트를 구축하여, 시스템 내에서 나타나는 조정 현상(emergence)을 감지하고, 그 현상이 어디에서 비롯되는지(개별 정체성에 고정된(identity-locked) 것인지 아니면 단순한 시간적 결합(mere temporal coupling)에 의한 것인지)를 식별하며, 이러한 조정 현상이 시스템의 전반적인 성능과 어떻게 연결되는지를 밝혀냅니다.

이 연구의 프레임워크(Framework)는 시간에 따른 결과 관련 부분 정보 분해(outcome-relevant PID)에 기반합니다. 이를 통해 세 가지 진단 기준이 제시됩니다. 첫째, 실용적 기준(Practical criterion)은 시점 t에서의 거시 신호(macro signal)가 어떤 단일 에이전트(single agent)의 예측 범위를 넘어서 시점 t+ℓ에서의 거시 신호를 예측할 수 있는지를 묻습니다. 이 값이 양수이면 동적 시너지(dynamical synergy), 즉 에이전트들이 상호작용하며 전체로서 더 큰 효과를 만들어냄을 의미합니다. 둘째, 출현 능력(Emergence capacity)은 미래의 공동 상태(future joint states)를 예측하기 위한 쌍별 PID 시너지(pairwise PID synergy)로 정의되며, 이는 어떤 단일 에이전트도 가지고 있지 않은, 오직 에이전트들이 '함께' 있을 때만 생성되는 정보를 포착합니다. 셋째, 연합 테스트(Coalition test)는 세 개 이상의 에이전트 연합(coalitions)이 최상의 두 에이전트 쌍(G3)보다 추가적인 목표 관련 예측 가능성(goal-relevant predictability)을 가지는지 확인하기 위한 삼중항 정보(I3)를 사용합니다.

실험은 통신 없는 그룹 이진 탐색 게임(no-chat group binary search game)을 통해 진행되었습니다. 에이전트들은 0에서 50 사이의 정수를 추측하고, "너무 높음/낮음"이라는 전역 피드백(global feedback)만 그룹 전체에 반환됩니다. 실험 조건은 일반(Plain), 페르소나(Persona) 부여, 그리고 페르소나와 함께 "다른 사람들이 무엇을 할지 생각하라"는 ToM(Theory of Mind) 프롬프팅을 추가한 세 가지였습니다.

GPT-4.1을 사용한 주요 발견은 조정 현상이 실제로 존재하며, 프롬프트 설계를 통해 조종 가능하다는 것입니다. 실용적 기준과 출현 능력 모두 모든 조건에서 0보다 큰 값을 보였으며, 이는 에이전트들 사이에 동적 시너지(dynamical synergy)가 발생함을 나타냅니다. 페르소나 부여는 안정적이고 개별 정체성(identity)과 연결된 차별화(differentiation)를 유도했으며, ToM 프롬프팅을 추가하면 에이전트들이 상호 보완성(complementarity)을 유지하면서도 공유된 목표에 대한 정렬(alignment)이 증가했습니다. 삼중항 구조(Triplet structure)의 중요성도 확인되었습니다. 많은 그룹에서 G3>0을 보였는데, 이는 어떤 두 에이전트 쌍만으로는 충분하지 않으며, 전체 삼중항이 거시 신호에 대한 추가적인 예측 정보를 제공한다는 것을 의미합니다. ToM 조건에서는 더 높은 총 상호 정보(I3)와 상당한 I3 값을 가진 더 많은 그룹이 관찰되어, 공유 목표에 대한 더욱 강력한 정렬을 시사합니다.

성능은 시너지와 중복성(redundancy)의 균형에서 비롯됩니다. 시너지 단독 또는 중복성 단독으로는 성공을 예측하지 못하며, 이들의 상호 작용이 성공을 예측하는 중요한 요소입니다. 중복성은 시너지의 효과를 증폭시키고 그 반대도 마찬가지여서, 통합(integration)과 차별화(differentiation)가 조화롭게 이루어지는 체제(regime)가 가장 성공적임을 보여줍니다. 매개 분석(Mediation)은 ToM이 시너지 증가를 통해 간접적으로 성공을 촉진한다는 것을 제안합니다.

반면, Llama-3.1-8B와 같은 저용량 모델에서는 대부분의 그룹이 실패했습니다. 행동은 강한 시간적 결합(time coupling)을 보였지만, 에이전트 간 상호 보완성(cross-agent complementarity)은 약했습니다. 특히, 저용량 모델에서는 ToM 프롬프팅이 오히려 성능을 저해하는 경향을 보여, ToM 스타일의 추론을 유도하기 위해서는 충분한 모델 용량(model capacity)이 필요함을 강조합니다.

AI 개발자를 위한 실용적인 시사점은 다음과 같습니다. 첫째, 에이전트 시스템을 설계할 때는 상호 보완적인 역할과 공유된 목표 신호(target signals)를 염두에 두십시오. 둘째, 가벼운 페르소나(light personas)를 사용하여 정체성과 연결된 행동(identity-linked behaviors)을 안정화하고, ToM 스타일 추론(ToM-style reasoning)을 추가하여 에이전트들이 거시적 목표(macro objective)에 정렬하면서 서로에게 적응하도록 유도하십시오. 셋째, 추측하지 말고 측정하십시오. 거시적 예측 가능성(macro predictability, 실용적 기준), 쌍별 시너지(pairwise synergy, 능력), 연합 가산성(coalition additivity, G3)을 지속적으로 추적하여 팀이 실제 집단인지 아니면 단순히 동기화된 진동자(synchronized oscillators)인지 진단해야 합니다. 마지막으로, 가짜 출현(spurious emergence)을 경계하십시오. 행 셔플(row-shuffle, 정체성 파괴) 및 열 셔플(column-shuffle, 에이전트 간 정렬 파괴) 널(nulls)을 사용하여 관찰된 좋은 시너지(synergy)를 단순한 시간적 결합(temporal couplings)과 명확히 분리하는 것이 중요합니다. 이 연구는 다중 에이전트 시스템의 설계와 평가에 대한 심오한 통찰을 제공합니다.
논문 | 트윗

**5. Elastic-Cache: 확산 LLM 디코딩의 효율성을 혁신하다**

대규모 언어 모델(LLM)의 디코딩 과정에서 KV 캐시(KV caches)의 효율성은 전체 시스템 성능에 지대한 영향을 미칩니다. Elastic-Cache는 확산 LLM(diffusion LLM)의 디코딩 속도를 획기적으로 향상시키기 위한 빠르고, 별도의 훈련이 필요 없으며, 특정 아키텍처에 구애받지 않는 독창적인 방법론을 제시합니다. 이 기술은 KV 캐시를 모든 디노이징(denoising) 단계에서 모든 토큰(tokens)에 대해 재계산하는 대신, 필요한 시점에 필요한 곳에서만 업데이트하는 전략을 취합니다. 즉, 가장 많은 주의(attention)를 받는 토큰들(most-attended tokens)에서 발생하는 어텐션 드리프트(attention drift)를 면밀히 관찰하고, 비교적 얕은(shallow) 레이어(layers)와 현재 처리 창 밖(off-window)에 있는 캐시(caches)는 재활용하면서, 더 깊은 레이어(deeper layers)의 캐시만 선택적으로 새로 고칩니다.

이러한 접근 방식의 결과로, 수학, 코드 생성 및 멀티모달(multimodal) 작업 전반에서 정확도 손실이 거의 없거나 전혀 없는 상태로 상당한 속도 향상을 달성했습니다. Elastic-Cache의 핵심 아이디어는 '슬라이딩 윈도우 디코딩(Sliding-window decoding)' 개념을 확장하는 것입니다. 이는 현재 마스크(MASK) 토큰에 가까운 토큰들만 "활성(live)" 상태로 유지하고, 멀리 떨어진 마스크 토큰들은 길이 사전(length prior)을 활용하여 블록 캐시(block-caches)에 저장합니다.

이 시스템의 작동 원리는 어텐션 인식 드리프트 테스트(attention-aware drift test)에 기반합니다. 이 테스트는 이전 단계에서 가장 많은 주의를 받았던 토큰들의 코사인 유사도(cosine similarity) 변화를 측정합니다. 만약 특정 레이어 ℓ에서 유사도가 미리 설정된 임계값 γ(감마) 아래로 떨어지면, 이는 해당 레이어부터 L(최종 레이어)까지의 KV 캐시를 재계산해야 한다는 신호로 해석됩니다. 따라서 얕은 레이어들은 기존 캐시를 재사용하고, 깊은 레이어들만 새로 고쳐지는 것입니다. 이 방식이 효과적인 이유는 KV 값의 드리프트(변동)가 대부분의 디코딩 단계에서 비교적 작으며, 모델의 깊이에 따라 점진적으로 증가하는 경향을 보이기 때문입니다. 모든 레이어를 무조건 새로 고치는 것은 대부분의 경우 자원 낭비에 해당합니다. 또한, 가장 많은 주의를 받는 토큰들은 KV 값의 변화가 가장 적게 나타나므로, 캐시 새로 고침을 트리거하는 보수적인 하한선(lower bound)을 제공하여 안정성을 확보합니다. 시각화 연구 결과들은 멀리 떨어진 마스크 토큰들이 전체 결과에 미치는 영향이 미미하며, KV 값과 어텐션 값의 변화가 일치하고, 가장 많이 주의를 받는 토큰들이 가장 적게 드리프트한다는 것을 뒷받침합니다.

실무자를 위한 알고리즘 조절 변수(knobs)로는 임계값 γ가 있습니다. 이 값은 속도와 정확도 사이의 트레이드오프(tradeoff)를 제어하며, γ가 낮을수록 업데이트가 적게 발생하여 디코딩 속도가 빨라집니다. 윈도우 크기 β는 단계당 계산량(per-step compute)과 전체 디코딩 단계 수 사이의 균형을 조절합니다. Elastic-Cache는 신뢰도 인식 병렬 디코딩(confidence-aware parallel decoding, ϵ)과 함께 작동하며, 더 높은 γ 값에서도 낮은 업데이트 빈도(update frequency)를 보여줍니다. 일반적으로 사용되는 기본값은 γ 0.9, ϵ 0.9, β 16–32입니다.

이 기술의 중요한 결과는 LLaDA 및 LLaDA-1.5 모델에서 나타났습니다. GSM8K-512 벤치마크에서 동일한 정확도를 유지하면서 최대 45.1배의 처리량(throughput) 향상을 보였고, GSM8K-256에서는 8.7배, HumanEval에서는 정확도를 유지하거나 향상시키면서 4.8–5.0배의 속도 향상을 기록했습니다. LLaDA-V 모델에서는 MathVerse 정확도를 유지하면서 처리량이 증가했습니다. Elastic-Cache는 유사하거나 더 나은 정확도에서 Fast-dLLM을 토큰/초(tokens/sec) 면에서 지속적으로 능가하며, 그 처리량은 더 긴 생성 시나리오에서 특히 유리하게 확장됩니다.

배포 측면에서 Elastic-Cache는 기존 모델에 훈련이나 아키텍처 변경 없이 적용할 수 있다는 큰 장점을 가집니다. 또한, 기존의 신뢰도 기반(confidence-based) 및 간격 정책(interval policies)과 호환됩니다. 병렬 처리(parallelism)를 보존하기 위해 가변 길이 시퀀스(variable-length sequences)를 효율적으로 연결하는 실용적인 배치 구현(batch implementation)도 포함되어 있습니다. 이 연구는 윤리적 고려 사항, 재현성 세부 사항, 그리고 코드 구현 계획까지 상세하게 다루고 있어, 실제 적용 가능성이 매우 높습니다.
논문 | 트윗

**6. LLM의 동적 레이어 라우팅(Dynamic Layer Routing): 추론 효율성의 최적화**

대규모 언어 모델(LLM)은 깊은 신경망 구조를 통해 복잡한 추론 능력을 발휘하지만, 모든 입력에 대해 모든 레이어(block)를 동일하게 처리하는 것은 비효율적일 수 있습니다. '동적 레이어 라우팅(Dynamic Layer Routing)'은 고정된 LLM에 추가될 수 있는 개조 가능한(retrofittable) 방법으로, 각 블록마다 레이어별 라우터(per-layer routers)를 부착하여 해당 블록을 건너뛸지(skip), 한 번 실행할지(execute once), 아니면 한 번 반복할지(repeat once)를 결정합니다. 이러한 유연한 경로 결정은 모델이 특정 작업의 복잡성에 따라 계산 자원을 동적으로 할당할 수 있게 하여, 논리 및 수학 문제에서 정확도를 향상시키면서도 평균적으로 필요한 레이어 수를 절약하는 효과를 가져옵니다.

이 시스템에서 추론 경로는 두 단계로 결정됩니다. 먼저, 오프라인에서 짧은 몬테카를로 트리 탐색(MCTS, Monte Carlo Tree Search)을 사용하여 레이어 편집(layer edits)을 통해 최적의 경로를 감독 방식으로 찾아냅니다. 이렇게 학습된 경로는 이후 온라인 추론 시에는 검색 과정 없이 즉시 실행됩니다. 이는 동적 라우팅의 효율성을 보장합니다.

구체적으로, 각 레이어에 부착된 소형 다층 퍼셉트론(MLP) 라우터는 현재 시퀀스의 '윈도우 평균 풀링된 은닉 상태(windowed mean-pooled hidden states)'를 입력으로 받아 세 가지 동작 중 하나를 출력합니다: 건너뛰기, 한 번 실행, 또는 한 번 반복. 이 과정에서 기본 모델의 가중치(Base weights)는 고정된 상태를 유지하며, 기존의 KV 캐싱(KV caching) 메커니즘과도 완벽하게 호환됩니다. 연구 논문의 3페이지 다이어그램은 레이어별 라우터의 구조, 윈도우 기반 풀링 방식, 그리고 라우터의 결정이 다음 블록의 처리 흐름을 어떻게 제어하는지를 명확하게 보여줍니다.

감독 학습 방식은 '길이 인식 MCTS(Length-aware MCTS)'를 활용합니다. 이 MCTS는 주어진 계산 예산(compute budget) 내에서 레이어를 건너뛰거나 반복하는 다양한 편집된 순방향 패스(forward passes)를 탐색합니다. 이 과정에서 정답 보상(gold-answer reward)을 보존하거나 개선하는 경로만을 유효한 것으로 간주하여 유지합니다. 라우터는 이렇게 발견된 약 4천 개의 경로 데이터를 기반으로 포컬 손실(focal loss)과 클래스 재조정(class rebalancing) 기법을 사용하여 훈련됩니다.

AI 개발자들을 위한 주요 결과는 매우 고무적입니다. 6가지 백본(backbones) 모델과 ARC 및 DART 벤치마크에 걸쳐, 동적 라우터를 적용한 모델은 쿼리(query)당 평균 약 3개에서 11개의 레이어를 절약하면서도 정확도를 높이는 결과를 보였습니다. 예를 들어, LLaMA-3B-Base 모델은 DART 벤치마크에서 정확도가 11.8%에서 15.8%로 상승했으며, 동시에 평균 4.1개의 레이어를 절약했습니다. 명령어 튜닝된(Instruction-tuned) 80억 개 매개변수(8B) 모델도 DART에서 성능 향상을 보이면서 11개의 레이어를 절약하는 효율성을 입증했습니다.

이 방법은 '도메인 외 일반화(Out-of-domain generalization)' 능력도 강력합니다. MMLU, GSM8k, AIME24, TruthfulQA, SQuADv2, GPQA, AGIEval, PIQA 등 다양한 벤치마크에 걸쳐 평균 정확도 하락은 약 0.85% 포인트에 불과했으며, 레이어 절약 효과는 그대로 유지되었습니다. 이는 Dr.LLM이 LayerSkip, ShortGPT, MindSkip, FlexiDepth와 같은 다른 동적 실행 방법론들에 비해 훨씬 적은 훈련 데이터와 기본 모델 변경 없이도 더 높은 평균 정확도를 달성했음을 의미합니다.

이 기술이 효과적인 이유는 무엇일까요? 논문의 8페이지와 17페이지에 제시된 분석 히트맵(heatmaps)은 일관된 패턴을 보여줍니다. 초기 레이어들은 안정적으로 유지되는 반면, 많은 중간 레이어들은 건너뛰어지며, 특히 더 어려운 수학 문제에서는 후기 레이어들이 때때로 반복되는 경향을 보입니다. 이는 모델이 반복적인 정제(iterative refinement)가 효과적인 곳에 깊이(depth)를 재할당함으로써, 계산 자원을 가장 필요한 곳에 집중시키는 지능적인 전략을 수행하고 있음을 시사합니다. 이러한 동적 라우팅은 LLM의 추론 효율성을 극대화하여, 더 빠르고 자원 효율적인 AI 시스템 개발에 기여할 수 있습니다.
논문 | 트윗

**7. LLM은 "뇌 부패(Brain Rot)"를 겪을 수 있다! 저품질 데이터의 치명적 영향**

대규모 언어 모델(LLM)의 지속적인 사전 훈련(continual pretraining)은 새로운 정보를 학습하고 능력을 확장하는 데 중요하지만, 어떤 데이터로 훈련하느냐에 따라 치명적인 부작용을 낳을 수 있습니다. 이 연구는 "사소하고 매우 매력적인 웹 텍스트에 대한 지속적인 사전 훈련이 LLM의 인지 능력(cognition)을 저하시키며, 이러한 저하는 완화 노력에도 불구하고 지속될 수 있다"는 명확한 가설을 검증합니다. 연구팀은 규모(scale)와 훈련 작업(training ops)으로부터 데이터 품질(data quality)을 분리하기 위해 통제된 트위터 데이터셋을 특별히 구축한 후, 이러한 저품질 데이터 노출이 추론(reasoning), 긴 컨텍스트(long-context) 이해, 안전(safety) 및 성격(personality) 특성에 미치는 영향을 정량적으로 측정했습니다.

데이터 품질을 분리하기 위한 실험 설정은 다음과 같습니다. 두 가지 직교적인 '정크(junk)' 데이터 정의를 사용했습니다. M1은 참여 신호(engagement signals)와 짧은 길이를 사용하여 인기 있고 짧은 게시물을 포착했으며, M2는 클릭베이트(clickbait) 및 피상적인 주제와 같은 의미론적 단서(semantic cues)를 활용했습니다. 네 개의 명령어 모델(instruct models)은 일치하는 토큰 수(token counts)로 지속적으로 사전 훈련된 다음, 재명령어 튜닝(re-instruction tuned) 과정을 거쳐 제어 데이터(control data)로 훈련된 모델과 직접적인 비교가 가능하도록 했습니다.

연구 결과는 저품질 데이터 노출이 모델의 능력에 비사소한 저하를 가져온다는 것을 '용량 반응(dose response)' 형태로 명확히 보여주었습니다. 모델 전반에 걸쳐 정크 데이터 노출은 ARC 추론(reasoning) 능력, 긴 컨텍스트 검색(long-context retrieval) 능력, 그리고 안전(safety) 수준을 감소시켰으며, 그 효과의 크기는 헤지스 g(Hedges’ g) 값이 0.3을 초과하는 유의미한 수준이었습니다. M1 정크 데이터의 비율을 늘릴수록 성능은 부드럽게 하락하는 양상을 보였습니다. 예를 들어, CoT(Chain-of-Thought)를 사용한 ARC-Challenge 점수는 정크 데이터 비율이 0%에서 100%로 증가함에 따라 74.9점에서 57.2점으로 떨어졌고, RULER CWE 점수는 84.4점에서 52.3점으로 급감했습니다.

이러한 능력 저하의 주요 원인은 '사고 건너뛰기(Thought-skipping)'로 밝혀졌습니다. ARC CoT에 대한 오류 포렌식(Error forensics) 분석 결과, '사고 없음', '계획 없음', '계획된 단계 건너뛰기'와 같은 유형의 실패가 전체 오류의 98% 이상을 차지하며 지배적인 손상(lesion)임을 보여주었습니다. 흥미롭게도, 추론 능력의 저하를 더 강력하게 예측하는 변수는 텍스트의 길이보다는 '인기(popularity)'였으며, 텍스트 길이는 긴 컨텍스트 이해 능력에 더 중요한 영향을 미쳤습니다.

안전(Safety) 측면과 LLM의 '어두운 특성(dark traits)' 또한 M1 정크 데이터에 의해 악화되었습니다. 저품질 데이터로 훈련된 모델은 HH-RLHF 및 AdvBench 벤치마크에서 위험성을 높였으며, 자기애(narcissism) 및 정신병(psychopathy) 점수를 부풀리는 동시에, 친화성(agreeableness) 점수는 낮추는 결과를 보였습니다. 성격 및 안전 관련 결과는 M1과 M2 정크 데이터 유형에 따라 다르게 나타났는데, 이는 참여 신호(engagement signals)가 데이터 품질의 해로운 비의미론적 축(non-semantic axis)을 포착한다는 것을 강조합니다.

가장 우려되는 점은 완화 노력이 도움이 되지만, 이러한 '뇌 부패'를 완전히 치유하지는 못한다는 것입니다. 더 강력한 모델을 사용한 '외부 반성(External reflection)' 기법은 사고 건너뛰기를 줄이고 정확도를 어느 정도 회복시켰지만, 모델 자체의 '자기 반성(self-reflection)'은 효과가 없었습니다. 또한, 명령어 튜닝(instruction tuning)과 깨끗한 지속적 훈련(clean continual training)을 확장하더라도 점수는 향상되지만 기준선과의 격차를 완전히 좁히지는 못했는데, 이는 저품질 데이터로 인한 '지속적인 표현 드리프트(representational drift)'가 발생했음을 시사합니다. 이 연구는 LLM 훈련 데이터의 품질 관리와 지속적인 모니터링의 중요성을 강력하게 경고하며, 무분별한 웹 데이터 활용에 대한 경각심을 일깨웁니다.
논문 | 트윗

**8. 하이브리드 강화 학습(Hybrid Reinforcement) HERO: LLM 추론의 정확성과 뉘앙스 향상**

대규모 언어 모델(LLM)의 추론 능력을 강화하는 데 있어 강화 학습(reinforcement learning, RL)은 핵심적인 역할을 합니다. HERO(Hybrid Ensemble Reward Optimization)는 LLM의 추론 정확도를 높이기 위한 혁신적인 강화 학습 프레임워크(framework)로, 두 가지 유형의 피드백 신호를 효과적으로 결합합니다. 이는 문제 해결의 옳고 그름을 명확히 판단하는 '이진 검증자 피드백(binary verifier feedback)'과, 답변의 품질이나 뉘앙스에 대한 연속적인 평가를 제공하는 '보상 모델 신호(continuous reward-model signals)'를 함께 활용합니다.

이러한 하이브리드 접근 방식은 각각의 피드백 유형이 가지는 장점을 극대화하고 단점을 보완합니다. 이진 검증자는 정답 여부에 대한 확실한 신호를 제공하여 모델이 올바른 방향으로 학습하도록 유도하지만, 세밀한 개선점이나 다양한 해결 경로에 대한 정보를 제공하기 어렵습니다. 반면, 보상 모델은 답변의 품질에 대한 미세한 차이를 포착하고 더 풍부한 피드백을 제공하지만, 때로는 모델 자체의 편향이나 불확실성을 내포할 수 있습니다. HERO는 이 두 가지 피드백을 지능적으로 통합하여, 모델이 '정확성'과 '뉘앙스' 사이의 최적의 균형을 찾도록 돕습니다.

HERO는 이러한 피드백 신호들을 결합하는 과정에서 '계층화된 정규화(stratified normalization)'와 '분산 인식 가중치(variance-aware weighting)'라는 두 가지 핵심 기술을 사용합니다. 계층화된 정규화는 서로 다른 특성을 가진 피드백 신호들이 모델 학습에 미치는 영향의 스케일을 균일하게 맞춰줍니다. 이는 한 종류의 피드백이 다른 종류를 압도하여 학습을 왜곡하는 것을 방지합니다. 분산 인식 가중치는 각 피드백 신호의 신뢰도나 변동성을 고려하여 가중치를 부여함으로써, 더 신뢰할 수 있는 피드백에 더 큰 영향을 주어 학습의 효율성과 안정성을 높입니다.

이러한 정교한 통합 덕분에 HERO는 다양한 수학 추론 벤치마크(math reasoning benchmarks)에서 뛰어난 성능을 입증했습니다. 검증자 피드백만을 사용하거나 보상 모델 신호만을 사용한 기존의 방법론들을 일관되게 능가했습니다. 특히 주목할 점은 HERO가 '검증 가능(verifiable)'한 작업(예: 명확한 정답이 있는 수학 문제)뿐만 아니라, '모호한(ambiguous)' 작업(예: 여러 가지 합리적인 해결책이 있을 수 있는 문제) 모두에서 성능을 향상시킨다는 것입니다. 이는 모델이 단지 정답을 맞추는 것을 넘어, 문제 해결 과정의 품질과 유연성을 향상시켰음을 의미합니다.

HERO의 성공은 LLM의 강화 학습에 있어 다중 소스 피드백의 잠재력을 강력하게 시사합니다. 향후 연구에서는 이러한 하이브리드 접근 방식을 다른 복잡한 추론 작업이나 다양한 도메인에 적용하여 그 범용성을 탐구할 수 있을 것입니다. 또한, 피드백 신호의 종류를 확장하거나, 각 신호의 가중치를 동적으로 조절하는 더욱 정교한 메커니즘을 개발하는 것도 중요한 연구 방향이 될 것입니다. 궁극적으로 HERO는 LLM이 인간과 유사한 방식으로 문제를 이해하고 해결하는 데 필요한 추론 능력을 더욱 고도화하는 데 기여할 것으로 기대됩니다.
논문 | 트윗

**9. Kimi-Dev: 소프트웨어 엔지니어링 LLM을 위한 에이전트 없는 훈련의 혁신**

Kimi-Dev는 소프트웨어 엔지니어링 대규모 언어 모델(LLM) 분야에서 '에이전트 없는 훈련(agentless training)'이라는 새로운 패러다임을 도입하여, 기존의 '워크플로우 스타일(workflow-style)' 접근 방식과 '에이전트 패러다임(agentic paradigms)'을 '스킬 사전(skill prior)'을 통해 연결합니다. 이 방법론은 복잡한 다단계 에이전트 시스템을 직접 훈련시키는 대신, 구조화되고 검증 가능한 '단일 턴 작업(single-turn tasks)'에 모델을 집중적으로 훈련시킴으로써, 에이전트가 갖춰야 할 핵심 역량에 대한 사전 지식(prior knowledge)을 구축합니다.

Kimi-Dev는 이러한 훈련을 통해 워크플로우 모델(workflow models)의 기록으로 인정받는 SWE-bench Verified 벤치마크에서 60.4%라는 인상적인 성능을 달성했습니다. 이는 소프트웨어 개발 작업의 특정 단계에 특화된 모델들이 달성했던 최고 수준의 성과와 맞먹는 수치입니다. 더욱이, 단 5천 개의 궤적 미세 조정(trajectory fine-tuning)을 거친 후 SWE-Agent pass@1 벤치마크에서 48.6%의 성공률을 기록하여, Claude 3.5 Sonnet과 같은 최첨단 모델들과 경쟁할 수 있는 수준의 에이전트 성능을 가능하게 했습니다.

이 연구의 핵심 통찰은 '추론 중심의 에이전트 없는 훈련(reasoning-heavy agentless training)'이 코드 지역화(localization), 코드 편집(code editing), 그리고 문제 해결 과정에서의 반성(reflection)과 같은 소프트웨어 엔지니어링의 근본적인 영역에서 전이 가능한 사전 지식(transferable priors)을 효과적으로 구축한다는 점을 입증한 것입니다. 이러한 사전 지식은 이후 SWE-Agent와 같은 실제 에이전트 시스템에 효율적으로 적응(adaptation)하기 위한 견고한 기반을 형성합니다. 즉, 에이전트가 직접 환경과 상호작용하며 시행착오를 겪는 복잡하고 비용이 많이 드는 훈련 방식 대신, 핵심적인 소프트웨어 개발 기술을 단일 턴 작업 형태로 학습시킴으로써 훨씬 효율적으로 강력한 에이전트를 개발할 수 있음을 보여줍니다.

Kimi-Dev는 소프트웨어 개발 과정에서 LLM의 역할을 확장하고 최적화하는 데 중요한 기여를 합니다. 기존의 코드 생성 모델들이 주로 단편적인 코드 조각을 생성하는 데 초점을 맞췄다면, Kimi-Dev는 문제 진단, 코드 수정, 그리고 변경 사항 검증에 이르는 복잡한 워크플로우 전반에 걸쳐 에이전트가 지능적으로 개입할 수 있는 기반을 마련합니다. 이는 AI 기반 소프트웨어 개발 도구의 미래를 형성하고, 개발자들이 더욱 생산적이고 효율적으로 작업할 수 있도록 돕는 데 핵심적인 역할을 할 것입니다. 또한, 제한된 데이터와 컴퓨팅 자원으로도 고성능 소프트웨어 엔지니어링 에이전트를 구축할 수 있는 가능성을 제시함으로써, AI 개발의 접근성을 높이는 데 기여할 수 있습니다.
논문 | 트윗

**10. 전체론적 에이전트 리더보드(Holistic Agent Leaderboard) HAL: AI 에이전트 평가의 새로운 표준**

AI 에이전트의 발전 속도가 가속화됨에 따라, 이들의 성능을 체계적이고 신뢰할 수 있게 평가하는 것이 점점 더 중요해지고 있습니다. HAL(Holistic Agent Leaderboard), 즉 '전체론적 에이전트 리더보드'는 대규모의 재현 가능한 AI 에이전트 평가를 위한 표준화된 프레임워크(framework)를 제시하며 이러한 필요성에 부응합니다. 이 리더보드는 코딩, 웹 탐색, 과학 문제 해결, 그리고 고객 서비스와 같은 광범위한 영역을 아우르는 9개 모델과 9개 벤치마크(benchmarks)를 포괄하여, 다양한 환경에서 에이전트의 역량을 종합적으로 측정합니다.

HAL의 도입은 에이전트 평가에 소요되는 시간을 획기적으로 단축시킵니다. 기존에는 몇 주가 걸리던 평가 과정을 몇 시간 내로 완료할 수 있게 함으로써, 연구 및 개발 주기를 크게 가속화합니다. 이는 AI 에이전트의 빠른 반복 개발(rapid iteration)과 효율적인 개선에 필수적인 요소입니다. 또한, HAL은 단순히 성능 수치만을 제공하는 것을 넘어, '작업 이탈 행동(off-task actions)'과 같이 실제 환경에서 에이전트가 보이는 주요 행동 결함(behavioral flaws)을 드러내는 데 기여합니다. 이러한 결함은 벤치마크 점수에서는 잘 포착되지 않지만, 실제 세계에서의 신뢰성과 유용성에 결정적인 영향을 미칩니다.

HAL은 총 25억 개에 달하는 방대한 양의 에이전트 로그(agent logs)를 공개 데이터로 제공합니다. 이 로그 데이터는 에이전트의 모든 행동, 결정, 그리고 내부 상태를 기록한 것으로, 벤치마크 성능(benchmark performance)이라는 단편적인 지표를 넘어 '실제 세계에서의 신뢰성(real-world reliability)'을 향한 연구를 촉진하는 귀중한 자원이 됩니다. 개발자들은 이 로그를 분석하여 에이전트의 실패 원인을 심층적으로 파악하고, 예측 불가능한 상황에 대한 대응 능력을 개선하며, 더욱 견고하고 안전한 AI 에이전트를 설계하는 데 활용할 수 있습니다.

이 프레임워크는 AI 에이전트의 평가 방식에 대한 중요한 통찰을 제공합니다. 첫째, 다양한 도메인과 벤치마크에 걸친 포괄적인 평가는 특정 작업에 최적화된 에이전트가 아닌, 범용적이고 강건한 에이전트의 개발을 장려합니다. 둘째, 평가 과정의 표준화와 재현성 보장은 연구 결과의 신뢰도를 높이고, 서로 다른 에이전트 간의 공정한 비교를 가능하게 합니다. 셋째, 행동 결함에 대한 분석은 에이전트가 단순히 목표를 달성하는 것을 넘어, 윤리적이고 안전하며 예측 가능한 방식으로 작동하도록 만드는 데 필요한 연구 방향을 제시합니다.

HAL은 AI 에이전트 연구 커뮤니티에 필수적인 인프라를 제공함으로써, 더욱 지능적이고 신뢰할 수 있는 에이전트 시스템의 개발을 가속화할 것입니다. 이는 미래의 AI가 단순한 도구가 아닌, 복잡한 문제를 자율적으로 해결하고 인간과 효과적으로 협력할 수 있는 진정한 동반자로 발전하는 데 중요한 이정표가 될 것입니다.
논문 | 트윗