거대 언어 모델(LLM) 기술은 최근 몇 년간 놀라운 속도로 발전하며 다양한 연구 분야에 걸쳐 혁신을 이끌고 있습니다. 단순한 텍스트 생성 도구를 넘어, 이제 LLM은 과학적 발견의 가속화, 복잡한 추론 문제 해결, 그리고 지능형 에이전트 시스템 구축의 핵심 동력으로 자리매김하고 있습니다. 본 포스팅에서는 이러한 LLM 연구의 최신 동향을 살펴보고, 효율성 증대, 생물학적 응용, 에이전트 능력 강화, 그리고 데이터 품질의 중요성에 대한 주요 논문들을 심층적으로 다루고자 합니다.

**LLM 효율성 및 확장성의 진화**

LLM의 성능 향상은 모델의 크기와 훈련 데이터의 양에 비례하는 경향이 있지만, 이는 동시에 막대한 계산 자원과 시간을 요구합니다. 이러한 과제를 해결하기 위해 연구자들은 LLM의 효율성을 높이는 다양한 방법을 모색하고 있습니다.

`Elastic-Cache`는 확산 LLM(diffusion LLM) 디코딩(decoding)을 빠르고 훈련 없이, 아키텍처에 구애받지 않는 방식으로 만드는 방법으로, KV 캐시(KV caches)를 필요할 때 필요한 곳에서만 업데이트합니다. 모든 디노이징(denoising) 단계에서 모든 토큰(tokens)에 대해 QKV를 재계산하는 대신, Elastic-Cache는 가장 많이 주목받는 토큰(most-attended tokens)에서 어텐션 드리프트(attention drift)를 관찰하고, 얕은(shallow) 및 창 밖(off-window) 캐시(caches)를 재사용하면서 더 깊은 레이어(deeper layers)만 새로 고칩니다. 핵심 아이디어는 슬라이딩 윈도우 디코딩(Sliding-window decoding)이 가까운 MASK 토큰(tokens)만 "활성(live)" 상태로 유지하고, 멀리 떨어진 MASK는 길이 사전(length prior)으로 블록 캐시(block-caches)한다는 것입니다. 어텐션 인식 드리프트 테스트(attention-aware drift test)는 이전 단계의 가장 많이 주목받는 토큰(most-attended tokens)의 코사인 유사도(cosine similarity) 변화를 측정합니다. 유사도가 레이어 ℓ에서 임계값 γ 아래로 떨어지면, 재계산은 ℓ+1부터 L까지 시작됩니다. 얕은 레이어(Shallow layers)는 캐시(caches)를 재사용하고; 깊은 레이어(deep layers)는 새로 고칩니다. 작동 원리는 KV 드리프트(drift)가 대부분의 단계에서 작고 깊이에 따라 증가하므로, 모든 레이어(layers)를 새로 고치는 것은 낭비라는 점에 있습니다. 가장 많이 주목받는 토큰(most-attended token)은 가장 적은 KV 변화를 보여주며, 새로 고침을 트리거(trigger)하는 보수적인 하한선(lower bound)을 제공합니다. 시각화 지원을 통해 멀리 떨어진 MASK는 영향이 거의 없고, KV 및 어텐션(attention) 변화는 일치하며, 가장 많이 주목받는 토큰(most-attended tokens)은 가장 적게 드리프트(drift)함이 확인되었습니다. 실무자를 위한 알고리즘 조절 변수(knobs)로는 임계값 γ가 속도-정확도 트레이드오프(speed-accuracy tradeoff)를 제어하며, γ가 낮을수록 업데이트가 적고 더 빠르게 실행됩니다. 윈도우 크기 β는 단계당 계산량(per-step compute)을 더 적은 단계로 교환합니다. 신뢰도 인식 병렬 디코딩(confidence-aware parallel decoding, ϵ)과 함께 작동하며, 더 높은 γ에서도 낮은 업데이트 빈도(update frequency)를 보여줍니다. 사용된 기본값은 γ 0.9, ϵ 0.9, 일반적인 β 16–32입니다. 중요한 결과로 LLaDA 및 LLaDA-1.5에서 GSM8K-512에서 동일한 정확도로 최대 45.1배 처리량(throughput), GSM8K-256에서 8.7배, HumanEval에서 기준선 대비 정확도 유지 또는 향상으로 4.8–5.0배의 성능을 보였습니다. LLaDA-V에서는 MathVerse 정확도를 유지하면서 처리량(throughput)이 증가합니다. Elastic-Cache는 유사하거나 더 나은 정확도에서 Fast-dLLM을 토큰/초(tokens/sec) 면에서 지속적으로 능가하며, 그 처리량(throughput)은 더 긴 생성에서 유리하게 확장됩니다. 배포 참고 사항으로는 훈련 또는 아키텍처 변경이 필요 없으며, 기존의 신뢰도 기반(confidence-based) 및 간격 정책(interval policies)과 호환됩니다. 병렬 처리(parallelism)를 보존하기 위해 가변 길이 시퀀스(variable-length sequences)를 연결하는 실용적인 배치 구현(batch implementation)을 포함하며, 윤리적 및 재현성 세부 사항과 코드 계획이 포함되어 있습니다. 이는 수학, 코드 및 멀티모달(multimodal) 작업 전반에서 최소한의 또는 전혀 없는 정확도 손실로 큰 속도 향상을 가져옵니다.

이와 유사하게, `LLM의 동적 레이어 라우팅(Dynamic Layer Routing)`은 추론 시 불필요한 계산을 줄이는 접근 방식입니다. 이 기술은 각 블록(block)을 건너뛸지, 실행할지, 반복할지를 결정하는 레이어별 라우터(per-layer routers)를 고정된 LLM에 추가하는 개조 가능한(retrofittable) 방법입니다. 경로는 레이어 편집(layer edits)에 대한 짧은 몬테카를로 트리 탐색(Monte Carlo Tree Search)으로 오프라인에서 감독된 다음, 검색 없이 온라인에서 실행됩니다. 이것은 각 레이어(layer)에 부착된 작은 MLP 라우터(routers)가 윈도우 평균 풀링된 은닉 상태(windowed mean-pooled hidden states)를 읽고 세 가지 동작 중 하나를 출력합니다: 건너뛰기(skip), 한 번 실행(execute once), 또는 한 번 반복(repeat once). 기본 가중치(Base weights)는 고정된 상태를 유지하고 KV 캐싱(KV caching)은 호환됩니다. 3페이지의 다이어그램은 레이어별 라우터(per-layer router), 윈도우(windows)에 대한 풀링(pooling), 그리고 결정이 다음 블록(block)을 어떻게 제어하는지 보여줍니다. 감독 작동 방식은 길이 인식 MCTS(Length-aware MCTS)가 계산 예산(compute budget) 내에서 레이어(layers)를 건너뛰거나 반복하는 편집된 순방향 패스(forward passes)를 탐색하고, 정답 보상(gold-answer reward)을 보존하거나 개선하는 경로만 유지합니다. 라우터(Routers)는 약 4천 개의 발견된 경로(paths)에 대해 포컬 손실(focal loss)과 클래스 재조정(class rebalancing)으로 훈련됩니다. AI 개발자를 위한 주요 결과는 6개의 백본(backbones)에 걸친 ARC 및 DART에서 라우터(routers)가 쿼리(query)당 약 3개에서 11개의 레이어(layers)를 줄이면서 정확도를 높인다는 것입니다. 예시로 LLaMA-3B-Base는 DART에서 11.8%에서 15.8%로 상승하고 평균 4.1개의 레이어(layers)를 절약합니다. 명령어 튜닝된(Instruction-tuned) 80억 개 매개변수(8B) 모델도 DART에서 이득을 얻으면서 11개의 레이어(layers)를 절약합니다. 도메인 외 일반화(Out-of-domain generalization)가 강력하여 MMLU, GSM8k, AIME24, TruthfulQA, SQuADv2, GPQA, AGIEval, PIQA 전반에서 평균 정확도 하락은 약 0.85% 포인트이며 절약은 유지됩니다. LayerSkip, ShortGPT, MindSkip, FlexiDepth와 비교하여 Dr.LLM은 훨씬 적은 훈련 데이터와 기본 모델 변경 없이 더 높은 평균 정확도를 달성합니다. 도움이 되는 이유는 8페이지와 17페이지의 분석 히트맵(heatmaps)이 일관된 패턴을 보여주는데, 초기 레이어(layers)는 안정적으로 유지되고, 많은 중간 레이어(layers)는 건너뛰어지며, 특히 더 어려운 수학 문제에서는 후기 레이어(layers)가 때때로 반복되는데, 이는 반복적인 정제(iterative refinement)가 효과적인 곳에 깊이(depth)를 재할당합니다. 이는 논리 및 수학에서 정확도를 향상시키면서 평균적으로 레이어(layers)를 절약하여, 제한된 자원으로도 고성능 LLM을 운영할 수 있는 가능성을 열어줍니다.

또한, 강화 학습(RL) 훈련의 확장성 역시 중요한 연구 분야입니다. `LLM을 위한 강화 학습(RL) 계산 스케일링의 기술` 연구는 40만 GPU 시간 이상의 연구를 통해 LLM을 위한 강화 학습(RL)을 확장하는 간단하고 예측 가능한 방법을 제시했습니다. 저자들은 작은 실행(small runs)에서 외삽(extrapolate)할 수 있는 시그모이드(sigmoidal) 계산→성능 곡선(compute→performance curve)을 맞추고, 80억 개의 밀집 모델(dense model)과 170억×16 MoE(Mixture-of-Experts) 모델에서 10만 GPU 시간까지 검증된 안정적인 방법인 ScaleRL을 제안합니다. 실제로 사용할 수 있는 예측 스케일링 법칙(Predictive scaling law)은 모델 통과율(pass-rate) 대 log(계산량)이 세 가지 조절 변수(knobs)를 가진 포화 시그모이드(saturating sigmoid)를 따른다는 것입니다: A (점근적 상한선, asymptotic ceiling), B (계산 효율성, compute efficiency), Cmid (중간점, midpoint). 1천 개의 프롬프트(prompt) 홀드아웃(holdout)에서 약 1.5천 GPU 시간 후에 맞추면 더 큰 예산을 예측할 수 있습니다. 이는 10만 GPU 시간 실행 및 MoE 스케일링(scaling)을 포함하여 실제 확장 훈련과 일치했습니다. 리브-원-아웃(leave-one-out) 테스트에서 유지된 ScaleRL 방법은 k=8을 사용하는 PipelineRL, CISPO 손실(truncated IS REINFORCE), 프롬프트 수준 손실 평균화(prompt-level loss averaging), 배치 수준 이점 정규화(batch-level advantage norm), LM 헤드(head)의 FP32 로짓(logits), 제로 분산 프롬프트 필터링(zero-variance prompt filtering), No-Positive-Resampling 커리큘럼(curriculum), 그리고 사고 길이(thinking length)를 제한하기 위한 강제 중단(forced interruptions)을 포함합니다. 1만 6천 GPU 시간까지의 LOO(Leave-One-Out) 절제 연구(ablations)는 ScaleRL이 유사하거나 더 나은 점근선(asymptotes)을 유지하면서 가장 효율적임을 보여줍니다. 실제로 상한선을 움직이는 것 대 단순히 속도 측면에서 모든 인기 있는 강화 학습(RL) 방법이 동일한 A에 수렴하는 것은 아닙니다. 손실 선택(Loss choice)과 로짓(logits)의 정밀도(precision)는 상한선을 높이는 반면, 집계(aggregation), 정규화(normalization), 커리큘럼(curriculum) 및 오프-정책(off-policy) 세부 사항은 주로 B를 조정합니다. 점근선에서 CISPO/GSPO > DAPO이며, FP32 로짓(logits)은 큰 도약(A≈0.52→0.61)을 제공했습니다. 성과를 낸 스케일링 축(Scaling axes)은 다음과 같습니다: 더 긴 생성 예산(32k까지)은 초기 효율성을 희생하여 점근선을 높이고, 더 큰 전역 배치(global batches)는 점근선과 다운스트림 일반화(downstream generalization)를 개선하여 작은 배치 정체(small-batch stagnation)를 방지하며, 더 큰 모델(MoE)은 80억 개의 밀집 모델보다 적은 계산량으로 훨씬 더 높은 점근적 강화 학습(RL) 성능을 제공합니다. 고정된 총 배치 크기에서 프롬프트당 더 많은 생성은 2차적입니다. 안정적인 장기 실행을 위한 운영자 참고 사항으로는 mean@16 생성을 사용하여 홀드아웃된 1천 개의 프롬프트 세트에서 곡선을 맞추고, 불안정성 신호로 절단율(truncation rates)을 관찰하며, 길이 제어를 위해 길이 페널티(length penalties)보다 중단(interruptions)을 선호하고, 먼저 A로 스케일링되는 방법을 선택한 다음 B를 조정하기 위해 초기 소규모 예산 절제 연구(ablations)를 계획하라는 것입니다. 이는 LLM의 RL 훈련을 예측 가능하게 만들고, 대규모 모델의 효율적인 학습 전략을 수립하는 데 중요한 기반을 제공합니다.

**생물학 분야 LLM의 혁신: Cell2Sentence-Scale**

LLM은 자연어 처리(NLP) 분야를 넘어 생물학, 화학 등 다양한 과학 분야에서도 혁신적인 도구로 활용되고 있습니다. 특히 `Cell2Sentence-Scale 27B C2S-Scale` 연구는 유전자 발현 데이터를 "세포 문장(cell sentences)"으로 변환하고, 이를 기반으로 대규모 언어 모델을 훈련하여 생물학적 발견을 가속화하는 새로운 패러다임을 제시합니다. 이 모델은 scRNA-seq 프로파일을 발현 정보를 보존하고 최소한의 손실로 역변환될 수 있는 유전자 이름 시퀀스(gene-name sequences)로 순위 매겨 텍스트화하며, 5천만 개 이상의 인간 및 마우스 전사체(transcriptomes)와 생물학적 텍스트로 사전 훈련(pretraining)됩니다. 270억 개의 매개변수(params)로 확장된 이 모델은 예측, 생성, 자연어 해석을 통합하며, 주석(annotation), 조직 추론(tissue inference) 및 조건부 생성(conditional generation)에서 뛰어난 성능을 보입니다. 성능은 4억 1천만 개에서 270억 개 매개변수까지 원활하게 향상됩니다. 광범위한 기능 대 기준선(baselines)에서 고전적인 단일 세포 작업에서 C2S-Scale은 scGPT 및 Geneformer와 동등하거나 능가합니다. 또한 자연어(NL) 클러스터 캡셔닝(cluster captioning), 데이터셋 수준 요약(dataset-level summarization) 및 질의응답(QA)을 지원하며, 이러한 단일 세포 기반 자연어(NL) 작업에서 GPT-4o와 같은 일반 LLM을 능가합니다. 다중 세포 및 공간 추론(Multi-cell and spatial reasoning) 측면에서는 맞춤형 공간 모듈(bespoke spatial modules) 없이 C2S-Scale은 다중 세포 컨텍스트(multi-cell context)에서 이웃 구조(neighborhood structure)를 예측하며, CellPhoneDB 및 BioGRID의 수용체-리간드(receptor-ligand) 및 PPI(단백질-단백질 상호작용) 지식으로 프롬프트(prompt)될 때 더욱 향상됩니다.

C2S-Scale의 가장 주목할 만한 성과는 새로운 약물 발견에 기여했다는 점입니다. 이중 컨텍스트 가상 스크리닝(dual-context virtual screen)을 통해 낮은 인터페론(IFN) 환경에서 항원 제시(antigen presentation)를 증가시키는 약물을 요청했을 때, 모델은 실미타서팁(silmitasertib)을 지명했으며, 이는 실험실에서 검증되었습니다. 실미타서팁 단독으로는 효과가 미미했지만, 저용량 IFN과 함께 사용했을 때 HLA-A,B,C 표면 수준을 증가시키는 것으로 나타났습니다. 이는 LLM이 복잡한 생물학적 시스템을 이해하고, 실제 의약품 개발에 기여할 수 있는 강력한 도구임을 입증하는 사례입니다. 또한, 이 연구는 섭동 모델링(perturbation modeling) 및 새로운 측정 지표(metric) 개발에도 기여했습니다. 2단계 파이프라인(pipeline)은 SFT(Supervised Fine-Tuning)를 사용하여 섭동(perturbations)에 조건을 부여한 다음, GRPO를 사용하여 경로에 충실한 예측(pathway-faithful predictions)에 보상을 제공합니다. 이 논문은 이미지 FID(Fréchet Inception Distance)의 임베딩 공간(embedding-space) 유사체인 scFID를 소개하며, 생성된 세포 상태(cell states)의 안정적인 순위를 제공합니다. C2S-Scale은 이전에 보지 못한 사이토카인(cytokine) 조합에서 선두를 달리며, RL(강화 학습) 후 scFID를 낮춥니다.

**에이전트 추론(Agentic Reasoning)에서 강화 학습(RL)의 신비 해명**
이 논문은 도구 사용 LLM 에이전트(tool-using LLM agents)를 개선하기 위해 강화 학습(RL)을 사용할 때 실제로 효과적인 것이 무엇인지 데이터, 알고리즘, 추론 모드라는 세 가지 축을 통해 연구합니다. 연구팀은 실제 종단 간 SFT(Supervised Fine-Tuning) 데이터셋, 다양한 강화 학습(RL) 세트, 그리고 에이전트 벤치마크(agentic benchmarks)에서 더 큰 모델을 능가하는 컴팩트한 40억 개 매개변수(4B) 에이전트를 제공합니다. 데이터 > 합성. SFT를 위한 실제 종단 간 다중 턴 궤적(multi-turn trajectories)은 이어 붙인 합성 추적(stitched synthetic traces)보다 훨씬 강력한 콜드 스타트(cold-start)를 제공합니다. AIME24/25에서 실제 SFT는 40억 개 및 70억 개 매개변수(4B and 7B) 기반 모델에 대해 average@32 및 pass@32를 크게 향상시킵니다. 다양성은 탐색을 유지합니다: 수학, 과학 및 코드 전반에 걸친 다양화된 강화 학습(RL) 데이터셋은 정책 엔트로피(policy entropy)를 높이고 유지하여 학습 속도를 높이고 훈련을 안정화합니다. 모델 인식 큐레이션(model-aware curation)은 작업 난이도를 능력에 맞춰 약한 모델의 병목 현상(bottlenecks)을 추가로 해결합니다. 간단한 GRPO 조정이 중요합니다: 토큰 수준 집계(token-level aggregation), 더 높은 클립 범위(clip range) 및 과도한 길이 페널티 형성(overlong-penalty shaping)을 사용하는 실용적인 방법(GRPO-TCR)은 최고 정확도와 데이터 효율성 모두에서 표준 GRPO 기준선을 지속적으로 능가합니다. 엔트로피는 적정 지점(sweet spot)이 필요합니다: 정책 엔트로피(policy entropy)가 붕괴되지도 과도하지도 않을 때 훈련이 가장 좋습니다. 클립 상한선(clip upper bound)을 적당히 늘리면 진행 속도가 빨라지지만, 너무 높으면 수렴(convergence)과 안정성(stability)이 저하됩니다. 신중한 모드(Deliberate mode)가 승리합니다: 더 많은 내부 계획(internal planning) 후 더 적고 더 나은 도구 호출(tool calls)은 잦은 호출을 동반하는 반응적인 짧은 사고(reactive short-think)보다 더 높은 도구 사용 성공률과 전반적인 정확도로 이어집니다. Long-CoT는 에이전트에게 플러그 앤 플레이(plug-and-play) 방식이 아닙니다: 기성품 Long-CoT 모델은 추론 중심 작업에서 도구를 피하여 강화 학습(RL) 중 도구 호출 횟수를 0으로 만듭니다. 다중 턴 도구 추적(multi-turn tool traces)을 사용한 SFT는 이를 재정렬할 수 있지만, 명령어 튜닝된(instruction-tuned) 기반 모델은 궁극적으로 에이전트 능력(agentic capability)을 더 깔끔하게 확장합니다. 이 방법으로 컴팩트한 SOTA(State-Of-The-Art) 달성: 3만 개의 다양한 강화 학습(RL) 세트와 조정된 클립 상한선(clip upper bound)을 가진 GRPO-TCR을 사용하여 DemyAgent-4B는 AIME25, GPQA-Diamond, LiveCodeBench-v6를 포함한 에이전트 환경에서 훨씬 더 큰 모델과 동등하거나 능가합니다.

**다중 에이전트 LLM에서 나타나는 조정(Emergent Coordination)**
"이것이 단순히 에이전트들의 묶음인가 아니면 진정한 집단인가?"에 대한 깔끔한 정보 이론적 탐침(information-theoretic probe). 이 논문은 시간 지연 상호 정보(time-delayed mutual information)에 대한 부분 정보 분해(PID, partial-information-decomposition) 테스트를 구축하여 출현(emergence)을 감지하고, 그것이 어디에 존재하는지(정체성 고정(identity-locked) 대 단순한 시간적 결합(mere temporal coupling))를 찾아내며, 이를 성능과 연결합니다. 전역 피드백(global feedback)만 있는 채팅 없는 그룹 이진 탐색 게임(no-chat group binary search game)을 사용하여, 저자들은 프롬프트 설계(페르소나(Personas) + "다른 사람들을 생각하라"는 ToM(Theory of Mind) 프롬프팅)를 통해 느슨한 집합체에서 목표 지향적이고 상호 보완적인 팀으로 집단(collectives)을 이끌 수 있음을 보여줍니다. 프레임워크(Framework): 시간에 따른 결과 관련 PID(outcome-relevant PID). 세 가지 진단: 실용적 기준(Practical criterion): 시점 t에서의 거시 신호(macro signal)가 어떤 단일 에이전트(single agent)를 넘어서 시점 t+ℓ에서의 거시 신호를 예측하는가? 양수 값은 동적 시너지(dynamical synergy)를 나타냅니다. 출현 능력(Emergence capacity): 미래 공동 상태(future joint states)를 예측하기 위한 쌍별 PID 시너지(pairwise PID synergy)로, 어떤 단일 에이전트도 가지고 있지 않은 "오직 함께" 정보를 포착합니다. 연합 테스트(Coalition test): 연합(coalitions)이 추가적인 목표 관련 예측 가능성(goal-relevant predictability)을 가지고 있는지 확인하기 위한 삼중항 정보 I3 대 최상의 쌍(G3). 실험: 통신 없는 그룹 추측. 에이전트들은 0-50 사이의 정수를 추측합니다; "너무 높음/낮음"만 전체 그룹에 반환됩니다. 조건: 일반(Plain), 페르소나(Persona), 그리고 페르소나 + ToM("다른 사람들이 무엇을 할지 생각하라"). GPT-4.1의 주요 발견: 출현은 실제적이며 조종 가능합니다. 실용적 기준과 출현 능력 모두 견고성 검사(robustness checks)를 거쳐 모든 조건에서 0보다 크며, 이는 동적 시너지(dynamical synergy)를 나타냅니다. 페르소나(Personas)는 안정적이고 정체성과 연결된 차별화(identity-linked differentiation)를 유도합니다; ToM을 추가하면 상호 보완성(complementarity)을 유지하면서 공유 목표에 대한 정렬(alignment)이 증가합니다. 삼중항 구조(Triplet structure)가 중요합니다. 많은 그룹에서 G3>0을 보이는데, 이는 어떤 쌍으로도 충분하지 않다는 것을 의미합니다; 전체 삼중항은 거시 신호(macro signal)에 대한 예측 정보를 추가합니다. ToM은 더 높은 총 상호 정보 I3(더 강력한 공유 목표 정렬)와 상당한 I3를 가진 더 많은 그룹을 가집니다. 성능은 균형에서 나옵니다. 시너지(Synergy) 단독 또는 중복성(redundancy) 단독으로는 성공을 예측하지 못합니다; 그들의 상호 작용이 예측합니다. 중복성(Redundancy)은 시너지(synergy)의 효과를 증폭시키고 그 반대도 마찬가지이며, 이는 통합(integration) + 차별화(differentiation)가 승리하는 체제(regime)라는 것과 일치합니다. 매개 분석(Mediation)은 ToM이 시너지(synergy)를 증가시킴으로써 간접적으로 성공을 촉진한다고 제안합니다. AI 개발자를 위한 실용적인 시사점: 상호 보완적인 역할과 공유된 목표 신호(target signals)를 위해 설계하십시오. 가벼운 페르소나(light personas)를 사용하여 정체성과 연결된 행동(identity-linked behaviors)을 안정화하십시오; ToM 스타일 추론(ToM-style reasoning)을 추가하여 에이전트들이 거시적 목표(macro objective)에 정렬하면서 서로에게 적응하도록 유도하십시오. 추측하지 말고 측정하십시오. 거시적 예측 가능성(macro predictability, 실용적 기준), 쌍별 시너지(pairwise synergy, 능력), 연합 가산성(coalition additivity, G3)을 추적하여 팀이 실제 집단인지 동기화된 진동자(synchronized oscillators)인지 진단하십시오. 가짜 출현(spurious emergence)을 경계하십시오. 행 셔플(row-shuffle, 정체성 파괴) 및 열 셔플(column-shuffle, 에이전트 간 정렬 파괴) 널(nulls)을 사용하여 좋은 시너지(synergy)를 단순한 시간적 결합(temporal couplings)과 분리하십시오.

**에이전트 AI의 발전과 도전**

LLM 에이전트는 복잡한 작업을 자율적으로 수행하고 외부 도구와 상호작용하며 환경에 적응하는 능력을 보여주며 AI 연구의 최전선에 있습니다. `Kimi-Dev`는 소프트웨어 엔지니어링 LLM에 대한 스킬 사전(skill prior)으로 에이전트 없는 훈련(agentless training)을 도입하여 워크플로우 스타일(workflow-style)과 에이전트 패러다임(agentic paradigms)을 연결합니다. 구조화되고 검증 가능한 단일 턴 작업(single-turn tasks)으로 훈련되어 워크플로우 모델(workflow models)의 기록인 SWE-bench Verified에서 60.4%를 달성했으며, 5천 개의 궤적 미세 조정(trajectory fine-tuning) 후 SWE-Agent pass@1에서 48.6%를 가능하게 하여 Claude 3.5 Sonnet과 경쟁합니다. 이 연구는 추론 중심의 에이전트 없는 훈련(reasoning-heavy agentless training)이 지역화(localization), 코드 편집(code editing) 및 반성(reflection)에서 전이 가능한 사전 지식(transferable priors)을 구축하여 효율적인 SWE-Agent 적응(adaptation)을 위한 기반을 형성함을 보여줍니다.

이러한 에이전트 기술의 발전과 함께, 이들을 평가하고 벤치마킹하는 것의 중요성도 커지고 있습니다. `전체론적 에이전트 리더보드(Holistic Agent Leaderboard, HAL)`는 코딩, 웹 탐색, 과학 및 고객 서비스를 아우르는 9개 모델과 9개 벤치마크(benchmarks)에 걸쳐 대규모의 재현 가능한 AI 에이전트 평가(AI agent evaluation)를 위한 표준화된 프레임워크(framework)를 도입합니다. 이는 평가 시간을 몇 주에서 몇 시간으로 단축하고, 작업 이탈 행동(off-task actions)과 같은 주요 행동 결함(behavioral flaws)을 드러내며, 25억 개의 토큰(tokens)에 달하는 에이전트 로그(agent logs)를 제공하여 벤치마크 성능(benchmark performance)보다 실제 신뢰성(real-world reliability)을 향한 연구를 추진합니다. HAL과 같은 리더보드는 에이전트 연구의 투명성과 재현성을 높이고, 실제 환경에서의 에이전트 성능을 객관적으로 측정하는 데 필수적인 역할을 합니다.

**하이브리드 강화 학습(Hybrid Reinforcement) HERO (Hybrid Ensemble Reward Optimization)**
HERO(Hybrid Ensemble Reward Optimization)는 이진 검증자 피드백(binary verifier feedback)과 연속적인 보상 모델 신호(continuous reward-model signals)를 결합하여 LLM 추론(reasoning)을 개선하는 강화 학습(reinforcement learning) 프레임워크(framework)입니다. 계층화된 정규화(stratified normalization)와 분산 인식 가중치(variance-aware weighting)를 사용하여 HERO는 정확성과 뉘앙스(nuance)의 균형을 맞추며, 다양한 수학 추론 벤치마크(math reasoning benchmarks)에서 검증자 전용(verifier-only) 및 RM 전용(RM-only) 방법을 능가하고, 검증 가능(verifiable)하고 모호한(ambiguous) 작업 모두에서 성능을 향상시킵니다.

**LLM 데이터 품질 및 윤리적 고려사항: "뇌 부패" 현상**

LLM의 성능은 훈련 데이터의 양뿐만 아니라 질에도 크게 좌우됩니다. `LLM은 "뇌 부패(Brain Rot)"를 겪을 수 있다!`는 연구는 사소하고 매우 매력적인 웹 텍스트에 대한 지속적인 사전 훈련(continual pretraining)이 완화 후에도 지속되는 방식으로 LLM 인지(cognition)를 저하시킬 수 있음을 경고합니다. 저자들은 규모(scale)와 훈련 작업(training ops)으로부터 데이터 품질(data quality)을 분리하기 위해 통제된 트위터 데이터셋을 구축한 다음, 추론(reasoning), 긴 컨텍스트(long-context), 안전(safety) 및 성격(personality)에 미치는 영향을 측정했습니다. 데이터 품질을 분리하는 설정으로 두 가지 직교적인 정크(junk) 정의를 사용했습니다: M1은 인기 있고 짧은 게시물을 포착하기 위해 참여 신호(engagement signals)와 짧은 길이를 사용하고, M2는 클릭베이트(clickbait) 및 피상적인 주제와 같은 의미론적 단서(semantic cues)를 사용합니다. 네 개의 명령어 모델(instruct models)은 일치하는 토큰 수(token counts)로 지속적으로 사전 훈련된 다음, 재명령어 튜닝(re-instruction tuned)되어 제어 데이터(control data)와 직접적인 비교가 가능합니다.

이 연구의 주요 발견은 정크(junk) 데이터에 노출될수록 모델 전반에 걸쳐 ARC 추론(reasoning), 긴 컨텍스트 검색(long-context retrieval) 및 안전(safety) 능력이 감소한다는 것입니다. 용량 반응(dose response)에 따른 비사소한 능력 저하가 관찰되었으며, 헤지스 g(Hedges’ g)는 0.3을 초과했습니다. M1 정크(junk) 비율을 늘리면 부드러운 하락이 발생합니다. 예를 들어, CoT를 사용한 ARC-Challenge는 74.9에서 57.2로, RULER CWE는 0%에서 100% 정크(junk)까지 84.4에서 52.3으로 떨어집니다. 특히, 사고 건너뛰기(thought-skipping)가 주요 손상(lesion)으로 나타났는데, ARC CoT에 대한 오류 포렌식(Error forensics)은 사고 없음, 계획 없음, 계획된 단계 건너뛰기가 지배적인 실패를 보여주며, 이는 오류의 98% 이상을 설명합니다. 인기는 길이보다 추론(reasoning)에 대한 이러한 부패(rot)의 더 강력한 예측 변수이며, 길이는 긴 컨텍스트(long-context)에 더 중요합니다. 더욱 우려스러운 점은 정크 훈련(Junk training)이 HH-RLHF 및 AdvBench에서 모델의 안전성(Safety)을 저하시키고, 자기애(narcissism) 및 정신병(psychopathy) 점수를 부풀리는 동시에, 친화성(agreeableness)을 낮추는 등 "어두운 특성(dark traits)"을 악화시킨다는 것입니다. 성격(Personality) 및 안전(safety) 결과는 M1과 M2 사이에서 달라지며, 이는 참여 신호(engagement signals)가 품질의 해로운 비의미론적 축(non-semantic axis)을 포착한다는 것을 강조합니다. 이러한 변화는 모델의 윤리적 사용과 사회적 영향에 대한 심각한 질문을 제기합니다.

비록 더 강력한 모델을 사용한 외부 반성(External reflection)과 같은 완화 전략이 사고 건너뛰기(thought-skipping)를 줄이고 정확도를 회복하는 데 도움이 될 수 있지만, 자기 반성(self-reflection)은 그렇지 않습니다. 명령어 튜닝(instruction tuning) 및 깨끗한 지속적 훈련(clean continual training)을 확장하면 점수가 향상되지만 기준선과의 격차를 좁히지 못하며, 이는 지속적인 표현 드리프트(representational drift)를 나타내어 완전히 "뇌 부패"를 치유하지는 못합니다. 이는 지속적인 표현 드리프트(representational drift)가 발생하며, 한 번 오염된 모델이 원래의 순수한 상태로 완전히 돌아가기 어렵다는 것을 시사합니다. 따라서 LLM 개발 및 배포에 있어 데이터 큐레이션의 중요성과 지속적인 학습(continual pretraining) 시 데이터 품질 검증은 아무리 강조해도 지나치지 않습니다. 모델의 강력한 능력만큼이나 그 기반이 되는 데이터의 건전성이 중요하다는 점을 명심해야 합니다.

**결론**

최신 LLM 연구 동향은 기술의 한계를 뛰어넘기 위한 끊임없는 노력과 함께, 실제 세계에 미치는 영향에 대한 깊은 성찰을 보여주고 있습니다. 효율적인 모델 아키텍처와 훈련 방법론을 통해 LLM의 접근성을 높이고, 생물학적 데이터 분석과 같은 전문 분야에서 혁신적인 발견을 이끌어내고 있습니다. 또한, 에이전트 AI의 발전은 LLM이 단순한 도구를 넘어 자율적인 문제 해결자로 진화하고 있음을 보여주며, 다중 에이전트 시스템에서의 조정 능력에 대한 탐구는 미래 AI 시스템의 복잡성과 잠재력을 엿볼 수 있게 합니다.

그러나 동시에, 데이터 품질이 LLM의 인지 능력과 윤리적 행동에 미치는 심각한 영향에 대한 경고는 기술 개발의 책임감을 강조합니다. "뇌 부패" 현상과 같은 연구 결과는 무분별한 데이터 사용이 모델의 신뢰성을 저해하고 사회적 위험을 초래할 수 있음을 분명히 보여줍니다. 앞으로의 LLM 연구는 성능 향상뿐만 아니라, 효율성, 신뢰성, 안전성, 그리고 윤리적 고려사항을 통합하는 방향으로 나아가야 할 것입니다. 이러한 균형 잡힌 접근 방식만이 LLM이 인류에게 진정으로 유익한 도구로 자리매김할 수 있도록 할 것입니다.