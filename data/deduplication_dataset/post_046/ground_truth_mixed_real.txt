(출처: [1, 2, 4, 14]) 보상 모델(RM)은 대규모 언어 모델(LLM) 연구의 초석이며, 훈련 과정에 인간의 선호도를 통합하여 상당한 발전을 가능하게 합니다. 중요한 역할에도 불구하고 RM은 종종 간과됩니다. 특히 검증 가능한 보상(verifiable rewards)을 사용하는 강화 학습(reinforcement learning)과 같은 RM 없는 기술이 인기를 얻고 있음에도 불구하고, RM을 효과적으로 훈련하고 사용하는 방법에 대한 실용적인 지침은 여전히 부족합니다. 그럼에도 불구하고, PPO 기반 강화 학습(reinforcement learning)으로 LLM을 훈련하는 것은 최고의 기반 모델(foundation models)을 개발하는 데 중요한 요소로 남아 있습니다. 최근 LLM 생태계(ecosystem)의 역동적인 변화 속에서 RM 연구 또한 빠르게 진화하고 있으며, RewardBench 2와 같은 최신 벤치마크는 실제 적용을 위한 중요한 통찰력을 제공합니다. 이 개요에서는 빠르게 진화하는 LLM 생태계에서 RM의 역사적, 현재적 중요성을 명확히 하면서 RM에 대한 깊은 이해를 처음부터 구축할 것입니다. 구독하기

**보상 모델이란 무엇인가?**

"보상 모델은 강화 학습(reinforcement learning) 연구에서 환경 보상(environment rewards)의 대리자(proxy)로 광범위하게 사용되어 왔습니다... 가장 일반적인 보상 모델은 훈련 비교(training comparisons)에서 선호되는 텍스트에 특정 텍스트가 얼마나 가까운지 확률을 예측합니다." - RLHF 책

보상 모델(RM)은 프롬프트(prompt)와 후보 완성(candidate completion)이 입력으로 주어졌을 때 인간 선호도 점수(human preference score)를 예측하도록 훈련된 전문화된 LLM(specialized LLMs)입니다. 일반적으로 현재 훈련 중인 LLM에서 파생됩니다(위 참조). RM에서 더 높은 점수는 주어진 완성(completion)이 인간에게 선호될 가능성이 높다는 것을 나타냅니다. RM의 기본 원리와 활용 방안을 이해하는 것은 LLM 개발에 필수적입니다. 따라서, 우리는 보상 모델(RM)이 무엇인지, 어떻게 생성되는지, 그리고 LLM의 맥락에서 어떻게 사용되는지에 대한 기본적인 이해를 구축해야 합니다. 이 섹션에서는 다음 사항을 이해하는 데 중점을 둘 것입니다.

*   선호도에 대한 통계 모델(statistical models)에서 파생된 RM의 동기.
*   대부분의 RM이 사용하는 아키텍처(architecture) 및 구조.
*   RM의 훈련 과정(training process).

RM이 어떻게 사용되는지 이해하려면 강화 학습(RL) 및 LLM 후처리 훈련(post-training)에 대한 더 많은 맥락이 필요하며, 이는 다음 섹션에서 다룰 것입니다.

**브래들리-테리 선호도 모델(The Bradley-Terry Model of Preference)**

RM의 표준 구현은 브래들리-테리 선호도 모델(Bradley-Terry model of preference)에서 파생됩니다. 이 모델은 쌍의 항목 간의 상대적인 강도 또는 성능을 기반으로 쌍 비교 데이터(paired comparison data)의 순위를 매기는 데 사용되는 통계 모델(statistical model)입니다. 동일한 분포에서 추출된 두 이벤트 i와 j가 주어졌을 때, 브래들리-테리 모델은 항목 i가 항목 j에 비해 승리할(또는 선호될) 확률을 다음과 같이 정의합니다.

**브래들리-테리 모델의 쌍별 비교 확률(Pairwise comparison probability from the Bradley-Terry model)**
$$P(i \text{ is preferred to } j) = \frac{e^{s_i}}{e^{s_i} + e^{s_j}}$$

LLM의 맥락에서 항목 i와 j는 동일한 LLM에 의해 동일한 프롬프트(prompt)에서 생성된 두 가지 완성(completion)입니다(즉, 이 완성(completion)들은 동일한 분포에서 샘플링됩니다). RM은 이 완성(completion)들 각각에 점수를 할당한 다음, 브래들리-테리 모델의 위 표현식을 사용하여 완성 i가 완성 j보다 선호될 확률을 도출합니다. 간단히 말해, 우리는 브래들리-테리 모델을 사용하여 두 완성(completion) 간의 쌍별 비교 확률(pairwise comparisons)을 표현합니다.

(출처: [14]) **선호도 데이터(Preference data).** 쌍별 선호도 데이터(pairwise preference data)는 LLM 후처리 훈련(post-training)에서 광범위하게 사용되며, 꽤 오랫동안 사용되어 왔습니다 [14]. 이러한 데이터는 다양한 프롬프트(prompt)로 구성되며, 우리는 데이터에서 프롬프트(prompt)의 다양성(diversity)을 극대화하는 것을 목표로 합니다. 프롬프트 분포(prompt distribution)는 모델이 실제 환경에서 보게 될 프롬프트(prompt)를 대표해야 합니다. 각 프롬프트(prompt)에 대해 우리는 한 쌍의 후보 완성(candidate completions) 1을 가지고 있으며, 이 중 하나의 완성(completion)은 일반적으로 사람에 의해, 때로는 모델에 의해 다른 것보다 선호되는 것으로 식별됩니다(위 참조). 관련 선택된 완성(chosen completions) 및 거부된 완성(rejected completions)이 있는 프롬프트(prompt) 데이터셋(dataset)을 (인간) 선호도 데이터셋(preference dataset)이라고 합니다.

**RM은 어떻게 작동하는가?**

우리는 RM이 브래들리-테리 선호도 모델(Bradley-Terry model of preference)에 기반을 두고 있다는 것을 알고 있지만, 이러한 통계 모델(statistical model)을 실용적으로 구현하는 방법은 많습니다. LLM 도메인에서는 이러한 모델이 LLM으로 구현됩니다(아마도 놀랄 일은 아닐 것입니다). 그러나 표준 (생성형) 디코더 전용 LLM(decoder-only LLMs)과 비교할 때, RM은 기본 아키텍처(architecture)와 훈련 목표(training objective)를 모두 수정합니다.

**RM 아키텍처(architecture)의 개략도(Schematic depiction of RM architecture)**
(Image of RM architecture)

**RM 아키텍처(architecture).** RM은 LLM으로부터 프롬프트-완성 쌍(prompt-completion pair)을 입력으로 받아 스칼라 선호도 점수(scalar preference score)를 출력합니다. 실제로 RM은 디코더 전용 아키텍처(decoder-only architecture)의 끝에 선형 헤드(linear head)를 추가하여 LLM으로 구현됩니다(위 참조). 특히, LLM은 토큰 벡터(token vectors) 목록(각 입력 토큰 벡터(token vector)에 대해 하나씩)을 출력하며, 이 목록의 최종 벡터를 선형 헤드(linear head)를 통해 전달하여 단일 스칼라 점수를 생성합니다. RM을 주어진 완성(completion)이 선호되는지 여부를 분류하는 데 사용되는 추가 분류 헤드(classification head)가 있는 LLM으로 생각할 수 있습니다.

**훈련 과정(Training process).** RM의 매개변수(parameters)는 일반적으로 기존 정책(policy) 2로 초기화되며, 이를 RM의 "기반(base)" 모델이라고 부를 것입니다. RM을 초기화할 정책(policy)에 대한 몇 가지 선택지가 있습니다. 예를 들어, 훈련 중인 LLM 또는 사전 훈련된 기반(pretrained base) 모델이나 SFT 모델(SFT model)과 같은 이 모델의 이전 버전이 있습니다. RM이 초기화되면 이 모델에 선형 헤드(linear head)를 추가하고 선호도 데이터셋(preference dataset)(즉, 프롬프트(prompt)에 대한 선택된 모델 응답(chosen model responses) 및 거부된 모델 응답(rejected model responses) 쌍)에 대해 훈련합니다.

**RM 출력에 대한 쌍별 확률 표현(Pairwise probability expressed with respect to the output of an RM)**
$$P(\text{chosen is preferred to rejected}) = \frac{e^{s_{\text{chosen}}}}{e^{s_{\text{chosen}}} + e^{s_{\text{rejected}}}}$$

선호도 쌍이 주어졌을 때, 우리는 RM이 거부된 응답에 비해 선택된 응답에 더 높은 점수를 할당하기를 원합니다. 즉, 최적의 RM은 선택된 응답이 거부된 응답보다 선호될 확률을 극대화해야 합니다. 앞에서 배운 것처럼, 우리는 브래들리-테리 모델(Bradley-Terry model)을 사용하여 이 확률을 표현할 수 있습니다(위 참조). 이 확률 표현식을 재배열하면 아래에 표시된 손실 함수(loss function)를 도출할 수 있습니다. 이는 모델이 선택된 응답에 더 높은 점수를 할당하도록 장려하는 쌍별 순위 손실(pairwise ranking loss)입니다.

**RM의 표준 손실 함수(loss function) 공식(Standard loss function formulation for an RM)**
$$L = -\log \left( \frac{e^{s_{\text{chosen}}}}{e^{s_{\text{chosen}}} + e^{s_{\text{rejected}}}} \right) = -\log(\text{sigmoid}(s_{\text{chosen}} - s_{\text{rejected}}))$$

이를 음의 로그 우도(NLL) 손실(negative log likelihood (NLL) loss)로 생각할 수 있으며, NLL에 대한 확률은 브래들리-테리 모델(Bradley-Terry model)에 의해 주어집니다. 이 손실의 지형(landscape) 시각화는 아래에 표시되어 있으며, 선택된 점수가 최대화되고 거부된 점수가 최소화될 때 손실이 최소화됨을 알 수 있습니다.

(Image of loss landscape)

대규모 선호도 데이터셋(preference dataset)에 대해 이 손실 함수(loss function)를 경험적으로 최소화함으로써, 우리는 선택된 응답이 거부된 응답보다 선호될 기대 확률(expected probability)을 (대략적으로) 최대화할 수 있습니다.

**보상 정규화(Normalizing the reward).** 훈련 후 RM은 정규화되지 않은 스칼라 값(unnormalized scalar values)을 출력합니다. 보상 함수(reward function)의 분산(variance)을 낮추기 위해(즉, RM의 출력이 표준 범위(standard range)에 속하도록 하기 위해), 훈련에 사용된 선호도 데이터셋(preference dataset)에 대해 평균 보상(reward)이 0이 되도록 RM의 출력을 정규화할 수 있습니다. [14]의 저자들은 이 보상 정규화 접근 방식(reward normalization approach)을 사용한다고 언급합니다.

"훈련이 끝날 때, 우리는 데이터셋(dataset)의 참조 요약이 평균 점수 0을 달성하도록 보상 모델(reward model) 출력을 정규화합니다." - [14]에서

**RM 구현하기**

이 논의를 더 실용적으로 만들기 위해, 아키텍처(architecture)와 손실 함수(loss function)를 포함한 RM이 일반적인 딥러닝 프레임워크(deep learning frameworks)를 사용하여 어떻게 구현될 수 있는지 알아보겠습니다. RM은 단지 분류 모델(classification model)입니다. 텍스트 시퀀스(textual sequences)에 대한 텍스트 분류(text classification)를 수행합니다. 프롬프트(prompt)와 응답이 입력으로 주어졌을 때, RM은 이 프롬프트-응답 쌍이 선호될 가능성(즉, 단일 스칼라 점수)을 예측합니다.

**예시(Toy example).** HuggingFace의 `AutoModelForSequenceClassification`과 같은 추상화(abstraction)를 통해 이를 구현할 수 있습니다. 로컬에서 실행할 수 있는 작은 (BERT 기반) RM 구현은 아래에 제공되어 있으며, 여기서 우리는 다음을 수행합니다.

*   `AutoModelForSequenceClassification`을 사용하여 RM을 생성합니다.
*   모든 선택된 시퀀스(chosen sequences) 및 거부된 시퀀스(rejected sequences) 3에 대해 RM의 출력(단일 로짓(logit) 형태)을 계산합니다.
*   위에 설명된 대로 RM의 손실을 계산합니다.

```python
from transformers import (
    AutoTokenizer,
    AutoModelForSequenceClassification,
)
import torch

# Load a tiny model for sequence classification
model_name = "google-bert/bert-bert-base-uncased"
tokenizer = AutoTokenizer.from_pretrained(
    model_name, trust_remote_code=True,
)
model = AutoModelForSequenceClassification.from_pretrained(
    model_name, trust_remote_code=True,
)

# Chosen prompt-response sequences
chosen_seqs = [
    "I love deep (learning) focus!",
    "Cameron is great at explaining stuff",
    "AGI is coming very soon...",
]

# Rejected prompt-response sequences
rejected_seqs = [
    "I'm not a fan of deep (learning) focus",
    "Cameron doesn't know what he's talking about",
    "AGI is fake and LLMs can't reason!",
]

# Tokenize the chosen / rejected sequences
chosen_inps = tokenizer(
    chosen_seqs, return_tensors="pt", padding=True,
)
rejected_inps = tokenizer(
    rejected_seqs, return_tensors="pt", padding=True,
)

# Compute the RM's output
rewards_chosen = model(**chosen_inps).logits[:, 0]
rewards_rejected = model(**rejected_inps).logits[:, 0]

# Compute the RM's loss
loss = -torch.nn.functional.logsigmoid(
    rewards_chosen - rewards_rejected
).mean()
print(loss)
```

여기서부터 우리는 다른 모델과 유사하게 RM을 훈련합니다. 즉, i) 선호도 데이터셋(preference dataset)을 반복하고, ii) 위에 설명된 대로 손실을 계산하고, iii) 역전파(backpropagation)를 통해 기울기(gradient)를 얻고, iv) 기울기 업데이트(gradient update)를 수행하고, v) 반복합니다.

**실제 RM 훈련 예시(Real RM training example).** LLM 연구소에서 RM 훈련이 어떻게 보이는지에 대한 더 실용적인 관점을 위해, AI2의 OpenInstruct에 있는 RM 훈련 스크립트(training script)를 살펴볼 수 있습니다. 이 스크립트는 `accelerate`를 사용하여 OLMo-2 또는 OLMoE를 기반으로 하는 RM의 분산 훈련(distributed training)을 구현합니다. 스크립트는 매우 간단하며, 대부분의 코드는 실제로 훈련 과정(training process)을 구성하는 것입니다. 이 훈련 스크립트(training script)를 분석하여 핵심 RM 훈련 루프(training loop)를 찾을 수 있으며, 참고용으로 아래에 복사했습니다.

```python
for _ in range(args.num_train_epochs):
    for data in dataloader:
        training_step += 1
        # Concat the chosen / rejected sequences
        query_responses = torch.cat(
            (
                data[CHOSEN_INPUT_IDS_KEY],
                data[REJECTED_INPUT_IDS_KEY]
            ),
            dim=0,
        )
        with accelerator.accumulate(model):
            # Predict reward for each sequence with RM
            _, predicted_reward, _ = get_reward(
                model,
                query_responses,
                tokenizer.pad_token_id,
                0,
            )
            # Parse chosen / rejected rewards from output
            chosen_reward = predicted_reward[
                :data[CHOSEN_INPUT_IDS_KEY].shape[0]
            ]
            rejected_reward = predicted_reward[
                data[CHOSEN_INPUT_IDS_KEY].shape[0]
                :
            ]
            # Compute loss and gradient for RM
            loss = -F.logsigmoid(chosen_reward - rejected_reward).mean()
            accelerator.backward(loss)
            # Perform parameter update for RM
            optimizer.step()
            optimizer.zero_grad()
```

보시다시피, 최고 연구소에서 대규모 RM 훈련에 사용되는 이 코드는 우리의 예시(toy example)와 크게 다르지 않습니다! 물론, 훈련 루프(training loop)는 HuggingFace와 같은 최신 딥러닝 패키지(deep learning packages)가 제공하는 추상화(abstraction) 덕분에 크게 단순화되었습니다. 그러나 여기서 핵심 시사점(key takeaway)은 우리가 지금까지 배운 개념이 RM의 실제 훈련(training) 및 사용에 직접적으로 적용된다는 것입니다.

**다양한 유형의 RM**

지금까지 우리는 일반적으로 분류기 기반 RM(classifier-based RM)이라고 불리는 RM의 표준 형태에 초점을 맞췄습니다. 그러나 RM은 프롬프트(prompt)와 응답이 주어졌을 때 선호도 점수(preference score)를 예측하는 모델일 뿐이며, 이를 여러 가지 방법으로 구현할 수 있습니다. 예를 들어, ArmoRM과 같은 맞춤형 분류기(custom classifier)를 RM으로 사용하도록 훈련할 수 있습니다.

(출처: [9]) LLM-as-a-Judge 모델(LLM-as-a-Judge models)도 LLM 심판에게 선호도 점수(preference score)를 제공하도록 간단히 프롬프트(prompt)를 줌으로써 RM으로 사용될 수 있습니다(위 참조). 이러한 선호도 점수(preference score)는 RL을 통한 훈련(training) 동안 보상 신호(reward signal)로 사용될 수 있습니다. 최근에는 LLM-as-a-Judge의 신뢰성과 한계에 대한 연구가 활발히 진행되고 있으며, 이를 효과적으로 활용하기 위한 다양한 방법론이 탐색되고 있습니다.

**LLM을 평가에 사용하기(Using LLMs for Evaluation)**
Cameron R. Wolfe, Ph.D. · 2024년 7월 22일
전체 이야기 읽기

대안으로, 우리는 LLM 심판을 사용하여 합성 선호도 데이터(synthetic preference data)를 수집하고(아래 AlpacaEval에서 보여지는 것과 같은 프롬프트(prompt)를 사용하여), Constitutional AI [10] 및 RLAIF [11]에서 수행하는 것처럼 이 합성 데이터에 대해 RM을 정상적으로 훈련할 수 있습니다.

(출처)
(Image of AlpacaEval prompt example)

결과 보상 모델(Outcome Reward Models, ORM) [12] 및 프로세스 보상 모델(Process Reward Models, PRM) [11]은 문헌에서 일반적으로 사용되는 RM의 두 가지 다른 변형입니다. 주로 추론 작업(reasoning tasks)에 사용되는 ORM은 완성(completion)이 작업에 대한 정답(correct answer)일 확률을 예측합니다. ORM을 훈련하기 위해 이전과 유사하게 선호도 데이터셋(preference dataset)을 수집하지만, 각 선호도 쌍에는 주어진 질문에 대한 오답과 정답이 모두 포함됩니다. 시퀀스 수준(sequence level)에서 보상(reward)을 예측하는 표준 RM과 달리, ORM은 토큰별(per-token) 기준으로 정확성을 예측합니다.

"우리의 검증기(verifiers)는 언어 모델(language models)이며, 토큰별(per-token) 예측을 출력하는 작은 스칼라 헤드(scalar head)를 가지고 있습니다." - [12]에서

(출처: [12]) ORM과 마찬가지로, PRM은 주로 추론 작업(reasoning tasks)에 사용되며 더 세분화된 출력(granular outputs)을 예측하지만, PRM은 각 토큰(token) 이후가 아니라 추론 과정(reasoning process)의 각 단계 이후에 예측을 수행합니다. PRM은 다양한 논문에서 사용되었지만, PRM에 대한 훈련 데이터(training data)를 수집하는 것은 어렵습니다. 왜냐하면 세분화된 감독(granular supervision)(즉, 추론 과정(reasoning process)의 각 단계에서 정확성 신호)이 필요하기 때문입니다.

"PRM은 사고 과정 추론 프로세스(chain of thought reasoning process)의 모든 단계에서 점수를 출력하도록 훈련된 보상 모델(reward models)입니다. 이는 EOS 토큰(EOS token)에서만 점수를 출력하는 표준 RM 또는 모든 토큰(token)에서 점수를 출력하는 ORM과 다릅니다. 프로세스 보상 모델(Process Reward Models)은 각 추론 단계(reasoning step)의 끝에서 감독(supervision)을 필요로 합니다." - 출처

**후처리 훈련(Post-Training)에서 보상 모델의 역할**

(출처: [3]) 초기 ChatGPT 이후 LLM은 InstructGPT [3]가 제안한 3단계 정렬 절차(alignment procedure)(위 참조)를 사용하여 거의 항상 후처리 훈련(post-trained)되었습니다. 이 절차는 다음 세 단계로 구성됩니다.

*   **지도 미세 조정(Supervised finetuning, SFT)** — 즉, 명령어 미세 조정(instruction finetuning, IFT) — 좋은 완성(completion) 예시에 대해 다음 토큰 예측(next-token prediction)을 사용하여 모델을 훈련합니다.
*   **보상 모델(RM)**은 인간 선호도 데이터셋(human preference dataset)에 대해 훈련됩니다.
*   **강화 학습(RL)**은 RM의 출력을 훈련 신호로 사용하여 LLM을 미세 조정(finetune)하는 데 사용됩니다.

이 절차의 2단계와 3단계는 총체적으로 인간 피드백 기반 강화 학습(reinforcement learning from human feedback, RLHF)이라고 불립니다. 우리는 강화 학습(RL) 옵티마이저(optimizer)를 사용하여 LLM을 미세 조정(finetune)하고 선호도 레이블(preference labels)을 통해 인간 피드백(human feedback)을 통합합니다.

(출처: [4]) 최근 LLM 개발의 진전과 함께 후처리 훈련 파이프라인(post-training pipeline)은 더욱 정교해지고 있습니다. Tulu-3 [4]에 사용된 더 현대적인 후처리 훈련 파이프라인(post-training pipeline)의 예시가 위에 제공되어 있습니다. 원래의 3단계 정렬 절차(alignment procedure)와의 주요 차이점은 다음과 같습니다.

*   SFT 단계(phase)는 여전히 매우 일반적이지만, 특히 최근 추론 모델(reasoning models)의 경우 항상 사용되는 것은 아닙니다. 예를 들어, DeepSeek-R1의 일부 변형은 SFT를 포기하고 사전 훈련된 모델(pretrained model)에 직접 RL을 적용합니다.
*   RL 훈련(training)은 일반적으로 여러 라운드(round)로 수행되며, 각 라운드(round)마다 새로운 데이터가 수집되어 LLM의 기능(capabilities)을 더욱 향상시킵니다.
*   RM을 필요로 하거나 필요로 하지 않을 수 있는 여러 RL 변형(및 비 RL 기반 대안)이 잠재적으로 함께 사용됩니다.

추가적인 복잡성에도 불구하고, 오늘날에도 데이터 품질은 성공적인 후처리 훈련(post-training)의 핵심 결정 요인으로 남아 있습니다. 이 섹션에서는 RL 훈련 프레임워크(frameworks)를 높은 수준에서 다루고, 각 프레임워크(framework)에서 RM의 역할(있는 경우)에 초점을 맞출 것입니다.

**LLM을 위한 RL 훈련 전략(RL Training Strategies for LLMs)**

LLM을 RL로 훈련하는 데 사용되는 높은 수준의 설정(setup)에 익숙하지 않은 분들을 위해 아래 개요를 참조하십시오. LLM 맥락에서 RL에 대한 기본적인 이해는 이 논의의 필수 전제 조건입니다.

**LLM을 위한 강화 학습(Reinforcement Learning)의 기초(Basics of Reinforcement Learning for LLMs)**
Cameron R. Wolfe, Ph.D. · 2023년 9월 25일
전체 이야기 읽기

**LLM을 위한 RL.** LLM이 많이 활용하는 강화 학습(RL) 훈련(training)에는 크게 두 가지 범주가 있습니다. RLHF(즉, 위에서 설명한 후처리 훈련(post-training) 설정의 2단계 및 3단계)와 검증 가능한 보상(verifiable rewards)을 사용하는 강화 학습(reinforcement learning, RLVR)입니다. 이 두 가지 RL 변형은 아래에 묘사되어 있습니다.

(출처: [4])
(Image comparing RLHF and RLVR)

RL 관점(perspective)에서 이 기술들은 유사합니다. 동일한 높은 수준의 훈련 설정(setup)을 따르며, 둘 다 정책 기울기 알고리즘(policy gradient algorithms) 4를 기반으로 하는 RL 옵티마이저(optimizer)를 사용하여 매개변수 업데이트(parameter updates)를 도출합니다。이 기술들 간의 주요 차이점은 보상(reward)을 정의하는 방식에 있습니다.

*   RLHF에서는 보상(reward)이 RM에서 나오며, 이는 LLM이 제공하는 각 완성(completion)에 대한 인간 선호도 점수(human preference score)를 제공합니다.
*   RLVR은 결정론적(deterministic) (또는 검증 가능한) 보상(rewards)을 사용하며, LLM이 제공하는 답변은 정답 또는 오답으로 표시됩니다. 특히, RLVR의 결정론적(deterministic) — 일반적으로 규칙 기반(rules-based) — 보상(rewards)은 RM의 필요성을 없앱니다! 일반적으로 보상(rewards)은 LLM의 생성된 출력에서 최종 답변을 추출하고 이 답변을 알려진 정답(ground truth answer)과 비교하여(예: 정확한 문자열 일치(exact string match) 또는 어떤 형태의 퍼지 매칭(fuzzy matching)을 통해) 도출됩니다. 이 비교를 통해 LLM의 출력이 올바른지 여부를 결정하고 이 이진 신호(binary signal)를 RL 훈련(training)을 위한 보상(reward)으로 사용할 수 있습니다.

**RLHF 대 RLVR.** 최근 LLM 훈련 파이프라인에서는 RLHF와 RLVR이 상호 보완적인 방식으로 활용되는 추세입니다. 우리는 여전히 3단계 후처리 훈련(post-training) 절차(SFT → RLHF)를 수행하여 LLM에 올바른 형식 지정(formatting)을 가르치고 인간 선호도에 정렬(align)합니다. 그러나 이제는 추론 능력(reasoning capabilities)과 검증 가능한 작업(verifiable tasks)에서의 성능을 향상시키는 추가 RLVR 단계(step)가 있습니다(아래 참조).

(출처)
(Image showing RLHF and RLVR in a modern pipeline)

더 일반적으로, RL 미세 조정(finetuning) — 특히 RLVR — 에 투자되는 컴퓨팅(compute) 양도 빠르게 증가하고 있습니다. 이러한 변화는 RL 훈련(training)에 사용되는 컴퓨팅(compute) 양에 대한 모델 성능의 명확한 스케일링 법칙(scaling laws)을 보여주는 추론 모델(reasoning models)에 대한 최근 결과에 의해 동기 부여됩니다(아래 참조).

(출처: [5])
(Image showing scaling laws for reasoning models)

"RLHF는 복잡하고 종종 불안정한 절차(procedure)입니다... 우리는 표준 RLHF 문제를 간단한 분류 손실(classification loss)만으로 해결할 수 있도록 하는 RLHF의 보상 모델(reward model)에 대한 새로운 매개변수화(parameterization)를 도입합니다." - [6]에서

**직접 정렬(Direct alignment).** RLVR만이 RM 사용을 피하는 유일한 방법은 아닙니다. 사실, 우리는 RM을 완전히 포기하면서도 RLHF와 유사하게 모델을 인간 선호도에 정렬(align)할 수 있습니다. 이러한 기술을 직접 정렬 알고리즘(direct alignment algorithms)이라고 하며, 이 클래스에서 가장 널리 사용되는 알고리즘은 직접 선호도 최적화(direct preference optimization, DPO) [6]입니다. DPO와 같은 직접 정렬 알고리즘(direct alignment algorithms)은 RLHF와 동일한 훈련 목표(training objective)를 최적화하면서 RM을 포기할 뿐만 아니라, RL 훈련(training) 자체를 완전히 피합니다. RLHF와 DPO의 비교는 아래에 제공되어 있습니다.

(출처: [6])
(Image comparing RLHF and DPO)

DPO에서 훈련(training)에 사용되는 손실 함수(loss function)는 아래에 제시되어 있습니다. 보시다시피, 이 손실 함수(loss function)는 RM이 사용하는 손실 함수(loss function)와 매우 유사합니다. 그러나 우리는 더 이상 RM으로 보상(reward)을 예측하지 않습니다. 대신, 현재 정책(current policy)과 참조 정책(reference policy)이 할당한 선택된 완성(chosen completions) 및 거부된 완성(rejected completions)의 확률을 사용하여 보상(reward)을 암묵적으로 추정하기 위해 정책(policy)을 직접 사용합니다. 직관적으로, 이 손실은 선택된 완성(completion)의 로그 비율(log-ratio)이 거부된 완성(completion)의 로그 비율(log-ratio)보다 클 때 최소화됩니다. DPO는 현재 정책(policy)이 거부된 응답에 비해 선택된 응답에 더 높은 (암묵적) 보상(rewards)을 할당하도록 훈련합니다.

**DPO 훈련 손실(training loss)**
$$L_{DPO}(\pi, \pi_{ref}) = -\mathbb{E}_{(x, y_c, y_r) \sim D} \left[ \log \sigma \left( \log \frac{\pi(y_c|x)}{\pi_{ref}(y_c|x)} - \log \frac{\pi(y_r|x)}{\pi_{ref}(y_r|x)} \right) \right]$$

(출처: [6]) DPO는 중간 RM 생성을 필요로 하지 않습니다. 그러나 손실 함수(loss function)는 여전히 브래들리-테리 모델(Bradley-Terry model)에서 파생되며, 우리는 여전히 RM을 학습하고 있습니다. 여기서 핵심적인 차이점은 RM이 명시적(explicitly)이 아닌 암묵적(implicitly)으로 학습된다는 것입니다. 따라서 DPO 논문 [6]의 제목은 "당신의 언어 모델은 비밀리에 보상 모델(reward model)입니다(Your Language Model is Secretly a Reward Model)"입니다. DPO는 구현의 단순성과 안정성 덕분에 다양한 LLM 파인튜닝(finetuning)에 널리 사용되고 있습니다.

**RM은 왜 유용한가?**

**RL에서 참조, 보상, 가치 및 정책 모델(policy models) 조율(Orchestrating reference, reward, value and policy models in RL)**
(Image showing the complexity of RL models)

의심할 여지 없이, RM을 사용하는 것은 LLM 훈련 과정(training process)에 추가적인 복잡성을 더합니다. 첫째, 대규모 선호도 데이터셋(preference dataset)에 대해 별도의 모델을 훈련해야 하며, 이는 이미 추가 비용과 복잡성을 초래합니다. 여기서부터 이 모델은 RL 훈련(training) 동안 온라인 방식(online fashion)으로 사용됩니다. RM은 훈련(training) 중 현재 정책(policy)에 의해 생성된 완성(completion)에 점수를 매깁니다. RM도 LLM이라는 점을 고려하면, 이는 훈련(training) 중 별도의 LLM에 대해 추론(inference)을 별도로 호스팅하고 실행해야 함을 의미하며, 이는 효율적으로 조율하기 어려울 수 있습니다(위 참조).

"우리는 신경 RM이 대규모 강화 학습(reinforcement learning) 과정에서 보상 해킹(reward hacking)으로 고통받을 수 있음을 발견했습니다. 보상 모델(reward model)을 재훈련하는 것은 추가적인 훈련 자원을 필요로 하며 전체 훈련 파이프라인(training pipeline)을 복잡하게 만듭니다." - [7]에서

**보상 해킹(Reward hacking).** 더 나아가, RM은 보상 해킹(reward hacking)의 대상이 됩니다. RM은 낮은 품질의 완성(completion)에 허위로 높은 보상(rewards)을 할당하거나, 더 일반적으로, 정책(policy)이 실제로 원하는 작업(task)을 해결하지 않고도 높은 보상(rewards)을 받을 수 있도록 악용될 수 있습니다. 흥미롭게도, 보상 해킹(reward hacking)은 RLHF를 통한 훈련(training) 확장을 방해하는 주요 한계(limitation)입니다. 우리가 충분히 오랫동안 훈련(training)을 계속한다면, 우리의 정책(policy)은 결국 RM에 대한 악용을 찾을 것입니다. 최근 연구에서는 보상 해킹(reward hacking) 문제를 완화하기 위한 다양한 방법론, 예를 들어 다중 RM 앙상블(ensemble)이나 더 견고한 RM 아키텍처(architecture) 등이 활발히 연구되고 있습니다. 대조적으로, 검증 가능한 보상(verifiable rewards)은 해킹하기가 더 어렵기 때문에(불가능하지는 않지만), RLVR을 사용할 때 추론 모델(reasoning models)을 더 광범위하게(즉, 더 많은 반복(iterations) 동안) 훈련(training)할 수 있습니다.

**RM을 피해야 하는가?**

그렇다면 RM의 추가적인 복잡성과 잠재적 문제점을 고려할 때, 우리는 RM을 완전히 회피해야 할까요? 이 질문에 대한 명확한 답은 없습니다. RLVR을 통해 인상적인 결과가 달성되었으며, 우리는 RM을 피하는 DPO와 같은 기술로 모델을 인간 선호도에 정렬(align)할 수 있습니다. 많은 연구에서 RLHF와 DPO 사이에 성능 격차(performance gap)가 있는지 여부에 대해 다른 결과를 가지고 논쟁했습니다. DPO가 효과적인 RM 없는 선호도 튜닝 대안인지는 사용 사례에 따라 다르지만, 이러한 기술들 사이에 성능 격차(performance gap)가 있다는 사실은 일반적으로 사실로 받아들여집니다.

"RLHF의 보급은 인간의 가치와 선호도를 언어 모델(language models)에 통합하는 데 가장 큰 어려움 중 하나인 명시적인 보상(reward) 지정을 회피하는 데 효과적이기 때문입니다." - [1]에서

**RM의 유용성(utility).** 이러한 발견에도 불구하고, 우리는 RM이 엄청나게 중요하고 강력한 개념(concept)이라는 사실을 잊어서는 안 됩니다. 어떤 형태의 RL 훈련(training)에서든 가장 어려운 작업 중 하나는 보상(reward)을 지정하는 것입니다. LLM의 경우 이 작업은 특히 어렵습니다. LLM에서 "좋은" 응답을 구성하는 것이 무엇인지 어떻게 명시적으로 정의할 수 있을까요? 불행히도, 사용할 수 있는 단일 속성이나 품질은 없습니다. 유효한 모델 응답의 범위는 거의 무한합니다. RM을 사용하면, 인간에게 선호도 피드백(preference feedback)을 제공하도록 요청하는(즉, 모델 응답 쌍 중에서 선택하는) 더 간단한 작업으로 이 과정을 증류함으로써 명시적인 보상(reward)을 지정하는 문제를 회피합니다(아래 참조).

**인간 선호도 데이터(human preference data) 수집을 위한 인터페이스(Interface)**
(Image of human preference data collection interface)

쌍에서 더 나은 모델을 선택하는 것은 개별 모델 응답을 수동으로 작성하거나 평가하는 것보다 훨씬 간단한 작업입니다. 인간은 단지 이진 선호도(binary preference)를 제공하기만 하면 됩니다. 우리는 이 선호도 피드백(preference feedback)에 대해 RM을 훈련할 수 있으며, 이는 보상(reward)을 명시적으로 지정하지 않고도 RL 훈련(training)을 위한 보상(reward)을 도출할 수 있게 합니다. 이러한 접근 방식은 일반적인 인간 피드백(human feedback)으로 LLM을 훈련하는 유연하고 효과적인 접근 방식(approach)을 제공하며, 이는 혁신적입니다.

**Best-of-N 샘플링(sampling)을 수행하기 위해 RM 사용(Using an RM to perform Best-of-N sampling)**
(Image showing Best-of-N sampling with RM)

**RM의 다른 사용 사례.** RL 훈련(training)에서의 사용 외에도 RM은 다양한 다른 사용 사례를 가지고 있습니다. 예를 들어, RM은 일반적으로 i) Best-of-N 샘플링(sampling) 및 추론 시간 스케일링(inference-time scaling)(위 참조), ii) 평가(evaluation), iii) 거부 샘플링(rejection sampling), iv) 데이터 필터링(data filtering) 등에 사용됩니다! 이러한 많은 사용 사례에도 불구하고, 우리는 일반적으로 RM의 성능을 다음을 기반으로 평가합니다.

*   **정확도(Accuracy)**: 쌍에서 선택된 응답을 올바르게 식별하는 RM의 능력.
*   **다운스트림 성능(Downstream performance)**: 특정 RM으로 RL 미세 조정(finetuned)된 LLM의 성능.
*   **추론 시간 스케일링(Inference-time scaling)**: Best-of-N 샘플링(sampling) 파이프라인(pipeline)에서 특정 RM을 사용하여 달성되는 성능 향상(Performance boost).

**실제 보상 모델(Reward Models in Practice)**

이제 RM에 대한 이해를 얻었으므로, 이 주제에 대한 최근 논문들을 살펴보겠습니다. 특히, 우리는 RM의 효과를 평가하기 위한 벤치마크(benchmark)인 RewardBench [1]에 초점을 맞출 것입니다. 이 벤치마크(benchmark)는 다양한 사용 사례에서 수백 가지의 다른 RM을 평가하는 데 사용되었으며, 실제 RM을 효과적으로 훈련하고 사용하는 데 유용한 시사점(takeaways)을 도출할 수 있게 합니다. 2025년 최신 연구 동향을 반영하여, RewardBench의 새로운 버전인 RewardBench 2 [2]가 제안되었으며, 이는 기존의 발견들을 현대화하고 평가 범위를 더욱 확장했습니다.

"보상 모델(RM)은 사전 훈련된 모델(pretrained models)을 인간 선호도에 정렬(align)하기 위한 성공적인 RLHF의 핵심이지만, 이러한 보상 모델(reward models)의 평가에 초점을 맞춘 연구는 상대적으로 적었습니다." - [8]에서

RM을 훈련하는 데는 많은 실제적인 선택이 포함됩니다. 예를 들어, 사용할 보상 모델의 유형 선택, RM을 초기화할 정책(policy) 선택, 훈련 에포크(epoch) 수 설정 등이 있습니다. 그러나 RM 생성의 대부분의 실제적인 세부 사항은 문서화가 미흡합니다. [1]에서 저자들은 RM 평가를 위한 표준 벤치마크(benchmark)인 RewardBench를 생성하여 이 문제를 해결합니다. RewardBench에서 광범위한 RM을 평가함으로써, 우리는 다양한 실제적인 선택이 RM 성능과 주어진 RM으로 훈련된 다운스트림 LLM(downstream LLMs)의 성능 모두에 미치는 영향을 결정할 수 있습니다. 이 분석을 통해 우리는 RM이 어떻게 작동하는지에 대한 더 나은 이해와 고품질 RM을 생성하기 위한 모범 사례(best practices) 세트를 얻게 됩니다.

**RewardBench란 무엇인가?** RewardBench는 RM 평가를 위한 프레임워크(framework) 및 데이터셋(dataset)입니다. 이 공개(즉, 데이터 및 평가 코드 공개) 벤치마크(benchmark)는 [1]에서 공개적으로 사용 가능한 RM의 현황을 파악하는 데 사용됩니다(리더보드(leaderboard)는 여기 참조). RewardBench는 많은 기능(capabilities)에 걸쳐 RM에 대한 구조화된 평가를 제공함으로써 특정 유형의 RM이 어떻게 그리고 왜 작동하는지 더 잘 이해하는 데 도움이 됩니다.

**RM 성능 정량화.** RewardBench는 두 가지 응답(하나는 선택된(선호되는) 응답, 다른 하나는 거부된 응답)과 쌍을 이루는 프롬프트(prompt)로 구성됩니다. RM을 평가하기 위해, RM이 선호되는 응답을 식별할 수 있는지 단순히 테스트할 수 있습니다. 특히, 이는 선택된 응답과 거부된 응답 모두에 대해 RM의 출력을 계산한 다음, 점수를 비교하여 수행됩니다. RM의 "올바른" 동작은 선택된 응답에 더 높은 점수를 할당하는 것입니다(아래 참조). 우리는 암묵적인 보상 추정치를 사용하여 DPO 모델도 이러한 방식으로 RM으로 평가할 수 있습니다.

**RewardBench에서 사용되는 채점 기술(Scoring technique used by RewardBench)**
(Image showing RewardBench scoring)

선호되는 응답을 올바르게 식별하는 이 능력은 선택된 응답과 거부된 응답이 있는 프롬프트(prompt) 데이터셋(dataset)에서 올바른 RM 출력 수를 세는 정확도 지표(accuracy metric)를 통해 쉽게 포착될 수 있습니다. 다른 RM을 비교하기 위해 고정된 데이터셋(dataset)에 대해 이 정확도 지표(accuracy metric)를 계산할 수 있습니다.

(출처: [1])
(Image showing RewardBench data composition)

**데이터 구성(Data composition).** 응용 프로그램에 따라 RM은 광범위한 다양한 기능(capabilities)을 포착해야 합니다. RM 성능에 대한 포괄적인 관점(comprehensive view)을 제공하기 위해 초기 RewardBench는 여러 다른 도메인(domains)에서 RM 품질을 측정했습니다(위 표에 요약됨).

*   **채팅(Chat)**: RM이 올바른 채팅(Chat) 응답을 구별하는 능력을 테스트합니다.
*   **어려운 채팅(Chat Hard)**: RM이 함정 질문(trick questions)과 응답 간의 미묘한 차이(subtle differences)를 식별하는 능력을 테스트합니다.
*   **안전(Safety)**: 안전하지 않은 프롬프트(prompt) 거부 및 잘못된 거부 방지 능력을 테스트합니다.
*   **추론(Reasoning)**: 좋은 코딩 및 추론 응답을 구별하는 능력을 테스트합니다.
*   **이전 데이터셋(Prior datasets)**: 이전 연구와의 일관성(consistency)을 위해 기존 선호도 데이터셋(preference datasets)(예: Anthropic의 HH 데이터셋(dataset), 스탠포드 인간 선호도 데이터셋(dataset), OpenAI의 요약 학습 데이터셋(dataset))도 포함됩니다.

RewardBench의 각 범주 내에서 모델은 정확도(accuracy) 측면에서 평가됩니다. 범주별 종합 점수(aggregate score)를 생성하기 위해 해당 범주 내 예시의 가중 평균(weighted average)을 취합니다. 여러 도메인(domains)에 걸쳐 RM을 평가함으로써, 우리는 RM 성능에 대한 더 세분화된 관점(granular view)을 얻습니다. 특정 범주의 RM은 종종 일부 도메인(domains)에서는 잘 수행되지만 다른 도메인(domains)에서는 그렇지 않습니다.

(출처: [1])
(Image showing difficult preference examples)

응답 품질의 미묘한 차이(subtle differences)를 포착하는 RM의 능력을 연구하기 위해, 저자들은 선택된 응답과 거부된 응답 사이에 작은 차이가 있는 어려운 선호도 예시를 생성하기도 합니다(위 예시 참조). 이상적으로는 RM이 이러한 미묘한 차이(subtle differences)를 포착하고 안정적인 방식(stable manner)으로 선호되는 응답에 점수를 부여해야 합니다. 길이 편향(length bias)이 결과를 왜곡하지 않도록, 저자들은 RewardBench 내의 모든 응답 쌍이 유사한 길이(similar length)임을 보장합니다.

(출처: [8])
(Image showing empirical performance of top-20 RMs on RewardBench)

**RM 분석(Analysis of RMs).** [1]에서 고려된 총 50개 이상의 RM 중 상위 20개 RM의 경험적 성능(empirical performance)은 위에 요약되어 있습니다. 이 RM들은 400M에서 70B 매개변수(parameters) 크기이며, 소형, 중형, 대형 그룹으로 나뉩니다. 이 모델들에 대한 주요 결과를 다음과 같이 요약할 수 있습니다.

*   모든 RM에서 어려운 채팅(Chat Hard) 및 추론(Reasoning) 하위 집합에 대한 성능이 일반적으로 낮으며, 이는 잠재적인 개선 영역을 나타냅니다.
*   더 큰 RM만이 어려운 채팅(Chat Hard) 및 추론(Reasoning) 하위 집합에서 일관되게 좋은 성능을 보입니다.
*   RM에 더 강력한 기반 모델(base model)을 사용하는 것이 도움이 됩니다. 예를 들어, Llama-3 기반 5 RM은 RewardBench에서 좋은 성능을 보입니다.
*   RM의 기반 모델(base model)에 대한 미묘한 변경(예: 훈련 데이터(training data) 또는 전략(strategy) 조정)도 RM에 영향을 미칠 수 있습니다.
*   모델 크기는 LLM-as-a-Judge 스타일 RM의 성능에 이점을 주지만, 분류기 기반 RM(classifier-based RMs)은 여전히 눈에 띄게 더 나은 성능을 보입니다.
*   RM의 스케일링 속성(scaling properties)은 RM의 스타일(예: 분류기 기반 대 DPO 대 LLM-as-a-Judge) 및 기반 모델(base model) 선택에 따라 다릅니다. 예를 들어, 아래 표는 LLaMA-2 DPO 모델(models)이 스케일에 따라 RM 성능이 향상되는 반면, 분류기 기반 Qwen-1.5 RM은 그렇지 않은 예시를 보여줍니다.
*   이전 평가 데이터셋(datasets)의 결과는 RewardBench와 일치하지 않으며, 이는 이러한 벤치마크(benchmarks)의 결과가 성능을 포괄적으로 측정하지 못할 수 있음을 나타냅니다. 예를 들어, DPO 모델(models)은 RM으로 평가될 때 RewardBench에서는 좋은 성능을 보이지만, 레거시 벤치마크(benchmarks)에서는 어려움을 겪습니다.

"Llama 2는 RewardBench의 모든 섹션에서 스케일링에 따라 명확한 개선을 보여주지만, Qwen 1.5는 단조로운 개선(monotonic improvement)이 덜하며, 이는 분포 외 일반화(generalization) 문제 때문일 가능성이 높습니다." - [1]에서

(출처: [1])
(Image showing scaling properties of LLaMA-2 DPO vs Qwen-1.5 RMs)

**DPO 및 PPO 분석: 선호도 피드백(Preference Feedback) 학습을 위한 모범 사례(Best Practices) 분리 [13]**

RM에 대한 모범 사례(best practices)를 학습한 다음 단계는 이러한 아이디어를 사용하여 더 나은 LLM을 훈련하는 것입니다. [13]에서 저자들은 RewardBench에서 얻은 교훈을 적용하여 RL 미세 조정(finetuning)을 심층 연구합니다. 특히, 이 논문은 DPO와 PPO의 성능 비교에 중점을 둡니다. 이 개요에서는 이러한 기술들 간의 비교에 초점을 맞추지 않을 것입니다. 그러나 이 분석에는 RM을 생성하여 훈련에 사용되는 LLM의 다운스트림 성능(downstream performance)을 극대화하기 위한 수많은 실제적인 교훈도 포함되어 있습니다.

(출처: [13])
(Image showing experimental results on data quality and RM size)

**데이터 품질.** [13]에 제시된 핵심 실험 결과는 위에 요약되어 있습니다. 실험은 Anthropic의 HH RLHF 데이터셋(dataset)에 대해 DPO 모델을 훈련하는 것으로 시작합니다. 이 데이터셋(dataset)은 오래되고 노이즈가 많은 데이터셋(dataset)으로 알려져 있습니다. 이 데이터는 모델 성능을 향상시키지만, 최신 고품질 선호도 데이터셋(preference dataset)인 UltraFeedback에 대한 훈련(training)에서 훨씬 더 큰 향상이 나타납니다. 동일한 데이터에 대해 PPO로 훈련(training)으로 전환할 때(즉, RM이 사용됨을 의미), 명확한 성능 향상이 나타나며, 이는 명시적 RM과 함께 PPO를 사용하는 것이 성능에 다운스트림 이점(downstream benefit)이 있음을 나타냅니다. 그러나 이 이점은 더 나은 데이터 사용의 영향에 비해 훨씬 작다는 점에 유의해야 합니다!

**더 큰 RM.** PPO를 통한 훈련(training)의 명확한 이점을 고려할 때, LLM이 더 큰 RM을 사용하는 것으로부터도 이점을 얻을 수 있는지 궁금할 수 있습니다. 이는 LLM 스케일링 법칙(scaling laws)을 고려할 때 직관적인 의미(intuitive sense)가 있지만, [13]의 관찰은 그렇게 간단하지 않습니다. RM을 13B에서 70B 매개변수(parameters)로 스케일링할 때, 다운스트림 LLM 성능은 동일한 SFT 체크포인트(checkpoint)에서 초기화된 모델에서도 정체된(stagnant) 상태로 유지됩니다. 관찰 가능한 성능 이점은 추론 도메인(reasoning domain)에서만 발생하며, 이는 더 큰 RM의 이점이 더 큰 모델의 우수한 기능(capabilities)이 유용하거나 필요한 시나리오에서만 명확하다는 것을 나타냅니다. 즉, 이러한 더 큰 RM이 유용하려면 더 어려운 데이터가 필요합니다!

"더 큰 보상 모델(reward model)을 사용한다면, 실제로 보상 모델(reward model)에 도전하는 데이터가 필요합니다." - 출처

**더 나은 데이터 + 더 큰 RM.** 위에 설명된 교훈을 결합하여, [13]의 저자들은 RM 훈련(training)을 위해 코딩 및 추론 작업(tasks)을 강조하는 더 크고 어려운 프롬프트(prompts) 세트를 수집하고 더 큰 RM이 유익한지 다시 테스트합니다. 이러한 실험에서 우리는 RM 품질 향상의 명확한 신호(signals)를 봅니다. 예를 들어, 이러한 더 크고 더 나은 RM은 아래에 표시된 것처럼 Best-of-N 샘플링(sampling)에 사용될 때 눈에 띄는 성능 향상(boost in performance)을 가져옵니다. 그러나 RewardBench와 다운스트림 성능(downstream performance)을 모두 살펴볼 때 이러한 개선은 훨씬 덜 명확합니다.

(출처: [13])
(Image showing Best-of-N sampling performance with different RMs)

간단히 말해, 더 크고 더 나은 RM을 사용하는 것이 이 RM이 RL 미세 조정(finetuning)에 사용될 때 LLM이 더 나아질 것이라는 것을 직접적으로 의미하지는 않습니다. 사실, [13]에서는 더 큰 RM을 사용할 때 일부 도메인(domains)에서 성능 저하(performance regression)까지 보입니다. 이러한 발견은 RM 평가를 매우 복잡하게 만듭니다. RM의 정확도(accuracy)를 측정하는 것만으로는 RM이 얼마나 유용할지 이해하는 데 도움이 되지 않습니다.

**RewardBench 2: 보상 모델 평가 발전(Advancing Reward Model Evaluation) [2]**

(출처: [2])
(Image of RewardBench 2 overview)

최근에 제안된 RewardBench 2 [2] 6는 RM 평가를 더 유용하고 유익하게 만들기 위해 초기 RewardBench에 대한 개선을 목표로 합니다. 이 벤치마크(benchmark)는 LLM이 가질 수 있는 더 넓은 범위의 기술(skills)을 다루는 새로운 데이터를 포함하며, RM은 이 벤치마크(benchmark)에서 평균적으로 약 20점 더 낮은 점수를 받습니다. 이는 훨씬 더 도전적인 벤치마크(benchmark)입니다. 여전히 RM 평가를 위해 정확도 기반 접근 방식(accuracy-based approach)을 사용함에도 불구하고, RewardBench 2는 다운스트림 RM 사용(예: Best-of-N 샘플링(sampling)용)과 명확한 상관관계가 있으며, 주어진 RM이 RL 미세 조정(finetuning)에 사용될 때 효과적일지 결정하는 데 유용한 교훈을 제공합니다.

**RM 성능 측정.** 선택된 응답과 거부된 응답을 구별하는 RM의 정확도(accuracy)를 측정하는 대신, RewardBench 2는 각 프롬프트(prompt)에 대해 네 가지 가능한 응답(하나의 선택된 응답과 세 개의 거부된 응답)을 가집니다. 이 응답들 중에서 RM은 선택된 응답에 모든 거부된 응답보다 더 높은 점수를 매겨야 합니다(아래 참조). 이 4개 중 최고(best-of-4) 접근 방식(approach)은 초기 RewardBench와 마찬가지로 정확도 기반이지만, 더 도전적이며 강력한 RM의 성능조차 무작위 기준선(random baseline)(즉, 25% 정확도(accuracy))에 더 가깝게 만듭니다.

(출처: [2])
(Image showing RewardBench 2 scoring technique)

또한, RewardBench 2는 다음 경우에 LLM 성능을 측정함으로써 정확도 기반 평가를 넘어섭니다.

*   특정 RM이 Best-of-N 샘플링(sampling)에 사용될 때.
*   특정 RM이 RL 훈련(training)에 사용될 때.

이러한 확장된 평가의 결과로, 우리는 RM의 품질을 이해할 수 있을 뿐만 아니라, 추론 시간 스케일링(inference-time scaling) 및 RL 훈련(training)에 사용될 때 이 품질이 다운스트림 성능(downstream performance)에 미치는 영향을 관찰할 수 있습니다. 다른 벤치마크(benchmarks)와 비교할 때, 이 평가 과정은 매우 포괄적입니다(아래 참조).

(출처: [2])
(Image comparing RewardBench 2 evaluation with other benchmarks)

**데이터 구성(Data composition).** RewardBench 2는 RM을 평가할 때 6가지 다른 도메인(domains) 또는 기능(capabilities)에 중점을 둡니다. 이 도메인(domains) 중 3가지(집중, 수학, 안전)는 기존 벤치마크(benchmarks)와 겹치지만, 나머지 3가지(사실성, 정확한 명령어 따르기, 동점(ties)(즉, 동등하게 유효한 답변을 처리하는 RM의 능력 테스트))는 RM에 완전히 새로운 도전 과제를 제시합니다.

"이 벤치마크(benchmark)는 광범위한 수동, 프로그래밍 방식 및 LM 기반 필터링 기술(filtering techniques)을 사용하여 WildChat 파이프라인(pipeline)에서 이전에 사용되지 않은 대부분의 인간 프롬프트(prompts)로 생성되었습니다." - [2]에서

RewardBench 2는 실제 사용자로부터 수집된 ChatGPT 로그 데이터셋(dataset)인 WildChat에서 주로 샘플링된 보지 못한 인간 작성 프롬프트(prompts)를 사용합니다. 데이터 오염(data contamination)의 위험 때문에 보지 못한 프롬프트(prompts)를 사용하는 것이 중요합니다. 상관관계가 합법적인지 확인하려면 데이터를 오염 제거하고 유출을 방지해야 합니다. 이 목표를 달성하기 위해 [2]의 저자들은 다음을 포함하는 다단계 데이터 큐레이션 파이프라인(data curation pipeline)을 채택합니다.

*   WildChat에서 보지 못한 인간 작성 프롬프트(prompts) 확보.
*   수동 검사 및 분류기(classifiers)(예: QuRater 및 도메인 분류기(domain classifiers))를 사용하여 각 프롬프트(prompt)의 도메인(domain) 및 품질 식별.
*   다운스트림 평가 데이터셋(evaluation datasets)과의 겹침이 사실상 0이 되도록 광범위한 데이터 오염 제거 수행.
*   남은 프롬프트(prompts) 중에서 최상의 프롬프트(prompts) 수동 선택.
*   최신 LLM의 기능(capabilities)을 정확하게 반영하는 다양한 소스에서 각 프롬프트(prompt)에 대한 완성(completions) 샘플링.
*   LLM-as-a-Judge, 자동 검증기(automatic verifiers), 다수결 투표(majority voting) 등 다양한 신호를 사용하여 정확성을 기반으로 완성(completions) 필터링.

RewardBench 2를 위해 생성된 최종 데이터셋(dataset)의 세부 사항과 이 데이터셋(dataset)의 각 구성 요소가 생성되는 방법은 아래에 요약되어 있습니다. 최종 벤치마크(benchmark) 점수를 도출하기 위해 각 도메인(domain)에서 RM 성능 8의 비가중 평균(unweighted average)을 취합니다.

(출처: [2])
(Image showing RewardBench 2 dataset composition)

**RewardBench 2 성능.** RewardBench 2는 [2]에서 100개 이상의 다른 RM을 평가하는 데 사용됩니다. 상위 20개 모델의 성능은 아래에 제공되어 있습니다. 이 새로운 벤치마크(benchmark)에서 점수가 더 낮을 뿐만 아니라, 기반 모델(foundation model) 기반 (예: Gemini 및 Claude) LLM-as-a-Judge 모델(models)이 매우 좋은 성능을 보인다는 것을 알 수 있습니다. 이 관찰은 기반 모델(foundation models)의 향상된 기능(capabilities)과 일치하지만, LLM-as-a-Judge 모델(models)이 분류기 기반 RM(classifier-based RMs)보다 일관되게 더 나쁜 성능을 보였던 초기 RewardBench의 관찰과는 극명한 대조(stark contrast)를 이룹니다.

(출처: [2])
(Image showing RewardBench 2 performance of top-20 RMs)

[2]의 저자들은 또한 다양한 기반 모델(base models) 및 하이퍼파라미터(hyperparameter) 설정을 사용하여 다양한 자체 RM을 훈련했으며, RM을 초기화하는 데 사용된 기반 모델(base model)이 RM에 명확하게 영향을 미친다는 것을 발견했습니다. 즉, 기반 모델(base model)에 있는 기술(skills)이 RM으로 이전됩니다. 모델 계열, 훈련 데이터(training data) 혼합, 사용된 훈련(training) 방식 또는 후처리 훈련(post-training) 단계와 같은 요소는 도메인(domains) 전반에 걸쳐 RM의 성능에 명확하게 영향을 미칩니다. 또한, [2]의 저자들은 일반적인 1 에포크(epoch) 대신 2 에포크(epochs) 동안 RM을 훈련하는 것이 유익할 수 있음을 발견했습니다.

**다운스트림 성능(Downstream performance).** 마지막으로, [2]의 RM 분석은 추론 시간 스케일링(inference-time scaling) 및 RL 훈련(training)을 고려하도록 확장됩니다. 놀랍게도, RewardBench 2의 성능은 Best-of-N 샘플링(sampling)과 높은 상관관계(highly correlated)를 가집니다. 정확한 보상 모델(reward models)은 후보 세트(candidate set) 내에서 최상의 완성(completions)을 식별할 수 있습니다. RewardBench 2와 다운스트림 성능(downstream performance)의 상관관계는 덜 명확하지만, [2]의 저자들은 RL 훈련(training)에 사용될 때 RM의 성공에 영향을 미치는 한 가지 핵심 요소(key factor)를 식별했습니다. 즉, 훈련 중인 RM과 정책(policy)이 동일한 모델 계보(model lineage)에서 파생되었는지 여부입니다. 다시 말해, 우리는 다음을 봅니다.

*   RM 벤치마크(benchmarks)에서 높은 점수는 RL 훈련(training)에서 높은 다운스트림 성능(downstream performance)을 위해 필요하지만(충분하지는 않음), RM 품질이 향상됨에 따라 다운스트림 성능(downstream performance)은 빠르게 포화됩니다 9.
*   RL 훈련(training)을 위한 정책 모델(policy model)과 RM의 기반 모델(base model) 간의 불일치(misalignment) — 또는 RL 훈련(training)에 사용된 프롬프트(prompts) 분포와 RM 훈련(training)에 사용된 프롬프트(prompts) 분포 간의 불일치 — 는 다운스트림 성능(downstream performance)의 큰 하락을 초래합니다.

이러한 발견의 결과로, [2]의 저자들은 아래 인용문에서 요약된 RM 훈련(training)에 대한 최종 권장 사항을 남기면서 작업을 마무리합니다.

"이러한 발견은 보상 모델(reward model) 평가 벤치마크(benchmarks)를 사용할 때 주의를 요합니다. 벤치마크(benchmark)는 Best-of-N 샘플링(sampling)과 같은 일부 설정에서 기성 보상 모델(reward model)을 선택하는 데 가이드로 사용될 수 있지만... PPO와 같은 정책 기울기 알고리즘(policy-gradient algorithms)의 경우, 벤치마크(benchmark) 결과는 자신의 훈련 설정(training setup) 맥락에서 고려되어야 합니다. 단순히 RewardBench 2에서 최고 모델을 선택하는 대신, 우리는 그 모델의 레시피를 가져와 체크포인트(checkpoint) 자체보다는 특정 워크플로우(workflow)에 통합해야 함을 보여줍니다." - [2]에서

**결론**

보상 모델(RM)은 LLM 연구에서 가장 강력하고 유연한 도구 중 하나입니다. 우리가 배웠듯이, 표준 분류기 기반 RM(classifier-based RM) 외에도 다양한 스타일의 RM이 존재하며, 효과적인 RM을 생성하는 것은 수많은 실제적 고려 사항의 결과입니다. 또한, RM 생성을 위한 올바른 선택은 응용 프로그램에 따라 다릅니다. 예를 들어, Best-of-N 샘플링(sampling) 대 RL 미세 조정(finetuning)과 같습니다. 이 개요에서는 브래들리-테리(Bradley-Terry)와 같은 기본적인 통계 모델(statistical models)부터 대규모 LLM 기반 RM 훈련(training)에 이르기까지 RM에 대한 기초적인 이해를 구축했습니다. LLM을 위한 RL 훈련(training)의 중요성이 더욱 강조되면서, RM에 대한 연구는 지속적으로 발전하고 있으며, AI 개발에서 핵심적인 역할을 계속해서 수행할 것입니다. 특히 RewardBench 2와 같은 최신 벤치마크는 RM의 실제 적용 가능성과 한계를 명확히 보여주며, 앞으로의 연구 방향을 제시하고 있습니다.

**뉴스레터가 처음이신가요?**
안녕하세요! 저는 Cameron R. Wolfe입니다. 딥러닝(Deep Learning) 박사이자 넷플릭스(Netflix)의 선임 연구 과학자입니다. 이것은 독자들이 AI 연구의 중요한 주제를 더 잘 이해하도록 돕는 Deep (Learning) Focus 뉴스레터입니다. 뉴스레터는 항상 무료이며 공개적으로 읽을 수 있습니다. 뉴스레터가 마음에 드시면 구독하거나, 유료 구독을 고려하거나, 공유하거나, X 및 LinkedIn에서 저를 팔로우해주세요! 구독하기

**참고문헌(Bibliography)**
[1] Lambert, Nathan, et al. "Rewardbench: Evaluating reward models for language modeling." arXiv preprint arXiv:2403.13787 (2024).
[2] Malik, Saumya, et al. "RewardBench 2: Advancing Reward Model Evaluation." arXiv preprint arXiv:2506.01937 (2025).
[3] Ouyang, Long, et al. "Training language models to follow instructions with human feedback." Advances in neural information processing systems 35 (2022): 27730-27744.
[4] Lambert, Nathan, et al. "T\" ulu 3: Pushing frontiers in open language model post-training." arXiv preprint arXiv:2411.15124 (2024).
[5] OpenAI et al. “Learning to Reason with LLMs.” https://openai.com/index/learning-to-reason-with-llms/ (2024).
[6] Rafailov, Rafael, et al. "Direct preference optimization: Your language model is secretly a reward model." Advances in Neural Information Processing Systems 36 (2023): 53728-53741.
[7] Guo, Daya, et al. "Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning." arXiv preprint arXiv:2501.12948 (2025).
[8] Bai, Yuntao, et al. "Training a helpful and harmless assistant with reinforcement learning from human feedback." arXiv preprint arXiv:2204.05862 (2022).
[9] Zheng, Lianmin, et al. "Judging llm-as-a-judge with mt-bench and chatbot arena." Advances in Neural Information Processing Systems 36 (2023): 46595-46623.
[10] Bai, Yuntao, et al. "Constitutional ai: Harmlessness from ai feedback." arXiv preprint arXiv:2212.08073 (2022).
[11] Zheng, Lianmin, et al. "Judging llm-as-a-judge with mt-bench and chatbot arena." Advances in Neural Information Processing Systems 36 (2023): 46595-46623.
[12] Cobbe, Karl, et al. "Training verifiers to solve math word problems." arXiv preprint arXiv:2110.14168 (2021).
[13] Ivison, Hamish, et al. "Unpacking dpo and ppo: Disentangling best practices for learning from preference feedback." Advances in neural information processing systems 37 (2024): 36602-36633.
[14] Stiennon, Nisan, et al. "Learning to summarize with human feedback." Advances in neural information processing systems 33 (2020): 3008-3021.

1.  때로는 프롬프트(prompt)당 두 개 이상의 후보 완성(candidate completions)이 있을 수 있습니다. 이 경우, 선호도는 완성(completion)의 선호도 측면에서 순위를 매겨 포착됩니다. 그러나 최근 연구에서는 이진 선호도 데이터(binary preference data)가 더 일반적으로 사용됩니다.
2.  여기서 우리는 현재 훈련 중인 LLM을 지칭하기 위해 정책(policy)이라는 용어를 사용합니다. 이는 강화 학습(reinforcement learning)에서 사용되는 표준 용어(terminology)입니다. 여기를 참조하십시오.
3.  실제로 이 시퀀스(sequences)는 모든 선택된 시퀀스(chosen sequences) 및 거부된 시퀀스(rejected sequences)에 대한 프롬프트(prompt)와 완성(completion) 모두가 될 것입니다. 여기서는 단순화를 위해 명확한 프롬프트(prompt) 또는 완성(completion) 구조가 없는 평면 텍스트 시퀀스(textual sequences)만 있습니다.
4.  그러나 LLM 훈련(training)에 사용되는 정책 기울기 알고리즘(policy gradient algorithms)에는 PPO, REINFORCE, GRPO 등 다양한 변형이 있으며, 각각의 이점(benefits)이 있습니다.
5.  작성 시점에 Llama-3는 사용 가능한 최고의 오픈소스 모델(open-source model)이었습니다.
6.  이 벤치마크(benchmark)에는 데이터, 리더보드(leaderboard) 및 광범위한 기술 보고서(technical report)가 함께 제공됩니다!
7.  데이터 오염(Data contamination)은 훈련 세트(training set)에 데이터가 존재하여 나중에 동일한 모델을 평가하는 데 사용될 수 있다는 아이디어를 의미합니다. 상관관계가 합법적인지 확인하려면 데이터를 오염 제거하고 유출을 방지해야 합니다.
8.  성능은 동점(ties)을 제외한 모든 도메인(domains)에서 정확도(accuracy)로 측정되며, 동점(ties)에서는 올바른 예시와 잘못된 예시 사이의 올바른 마진(margin)을 확인합니다.
9.  이는 [13]의 결과와 일치하며, 다양한 강점의 RM이 RL 훈련(training)에 사용될 때 모두 비교적 잘 수행됨을 알 수 있습니다.