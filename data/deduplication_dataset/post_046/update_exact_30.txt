대규모 언어 모델(LLM)의 발전은 인공지능(AI) 분야에 혁명을 가져왔으며, 이러한 발전의 핵심에는 인간의 선호도를 학습하고 반영하는 보상 모델(Reward Model, RM)이 있습니다. RM은 LLM이 단순히 텍스트를 생성하는 것을 넘어, 사용자의 의도와 가치에 부합하는 응답을 제공하도록 돕는 필수적인 도구입니다. 최근 검증 가능한 보상(verifiable rewards)을 활용하는 강화 학습(reinforcement learning) 방식이나 직접 선호도 최적화(Direct Preference Optimization, DPO)와 같은 RM 없는(RM-free) 접근 방식이 주목받고 있지만, 여전히 RM의 중요성은 변함이 없습니다. 특히 PPO(Proximal Policy Optimization) 기반 강화 학습을 통해 최첨단 LLM을 개발하는 데 있어 RM은 핵심적인 역할을 수행합니다. 이 글에서는 빠르게 변화하는 LLM 환경에서 보상 모델의 기본 개념부터 최신 연구 동향 및 실제 적용 사례에 이르기까지 심층적으로 탐구하며, RM에 대한 포괄적인 이해를 돕고자 합니다.

**보상 모델이란 무엇인가?**

"보상 모델은 강화 학습(reinforcement learning) 연구에서 환경 보상(environment rewards)의 대리자(proxy)로 광범위하게 사용되어 왔습니다... 가장 일반적인 보상 모델은 훈련 비교(training comparisons)에서 선호되는 텍스트에 특정 텍스트가 얼마나 가까운지 확률을 예측합니다." - RLHF 책

보상 모델(RM)은 프롬프트(prompt)와 후보 완성(candidate completion)이 입력으로 주어졌을 때 인간 선호도 점수(human preference score)를 예측하도록 훈련된 전문화된 LLM(specialized LLMs)입니다. 일반적으로 현재 훈련 중인 LLM에서 파생됩니다(위 참조). RM에서 더 높은 점수는 주어진 완성(completion)이 인간에게 선호될 가능성이 높다는 것을 나타냅니다. 첫 번째 단계로, 우리는 보상 모델(RM)이 무엇인지, 어떻게 생성되는지, 그리고 LLM의 맥락에서 어떻게 사용되는지에 대한 기본적인 이해를 구축해야 합니다. 이 섹션에서는 다음 사항을 이해하는 데 중점을 둘 것입니다.

*   선호도에 대한 통계 모델(statistical models)에서 파생된 RM의 동기.
*   대부분의 RM이 사용하는 아키텍처(architecture) 및 구조.
*   RM의 훈련 과정(training process).

RM이 어떻게 사용되는지 이해하려면 강화 학습(RL) 및 LLM 후처리 훈련(post-training)에 대한 더 많은 맥락이 필요하며, 이는 다음 섹션에서 다룰 것입니다.

**브래들리-테리 선호도 모델(The Bradley-Terry Model of Preference)**

RM의 표준 구현은 브래들리-테리 선호도 모델(Bradley-Terry model of preference)에서 파생됩니다. 이 모델은 쌍의 항목 간의 상대적인 강도 또는 성능을 기반으로 쌍 비교 데이터(paired comparison data)의 순위를 매기는 데 사용되는 통계 모델(statistical model)입니다. 동일한 분포에서 추출된 두 이벤트 i와 j가 주어졌을 때, 브래들리-테리 모델은 항목 i가 항목 j에 비해 승리할(또는 선호될) 확률을 다음과 같이 정의합니다.

**브래들리-테리 모델의 쌍별 비교 확률(Pairwise comparison probability from the Bradley-Terry model)**
$$P(i \text{ is preferred to } j) = \frac{e^{s_i}}{e^{s_i} + e^{s_j}}$$

LLM의 맥락에서 항목 i와 j는 동일한 LLM에 의해 동일한 프롬프트(prompt)에서 생성된 두 가지 완성(completion)입니다(즉, 이 완성(completion)들은 동일한 분포에서 샘플링됩니다). RM은 이 완성(completion)들 각각에 점수를 할당한 다음, 브래들리-테리 모델의 위 표현식을 사용하여 완성 i가 완성 j보다 선호될 확률을 도출합니다. 간단히 말해, 우리는 브래들리-테리 모델을 사용하여 두 완성(completion) 간의 쌍별 비교 확률(pairwise comparisons)을 표현합니다.

(출처: [14]) **선호도 데이터(Preference data).** LLM을 인간의 가치에 정렬(align)시키는 데 필수적인 쌍별 선호도 데이터(pairwise preference data)는 [14]에서 처음 제안된 이래로 LLM 후처리 훈련(post-training)의 핵심 요소로 자리 잡았습니다. 이 데이터셋은 다양한 프롬프트(prompt)와 각 프롬프트에 대한 여러 후보 완성(candidate completions)으로 구성됩니다. 중요한 것은 이 데이터가 모델이 실제 환경에서 마주할 프롬프트 분포(prompt distribution)를 잘 반영해야 한다는 점입니다. 일반적으로 각 프롬프트에 대해 한 쌍의 완성(completion)이 주어지며, 이 중 하나는 인간 평가자에 의해 선호되는 것으로, 다른 하나는 거부되는 것으로 분류됩니다. 이러한 방식으로 수집된 '선택된 완성(chosen completion)'과 '거부된 완성(rejected completion)' 쌍으로 구성된 데이터셋을 선호도 데이터셋이라고 합니다.

선호도 데이터 수집은 비용과 시간이 많이 소요되는 작업이며, 주석(annotation) 과정에서 평가자의 피로, 편향, 일관성 부족 등의 문제에 직면할 수 있습니다. 이러한 한계를 극복하기 위해 최근에는 '약한 감독(weak supervision)' 방식이나 '합성 선호도 데이터(synthetic preference data)' 생성 기법이 활발히 연구되고 있습니다. 예를 들어, LLM-as-a-Judge 모델을 활용하여 초기 선호도 데이터를 생성하거나, 기존의 소량의 인간 피드백 데이터를 증강(augmentation)하여 대규모 합성 데이터셋을 구축하는 방식이 대표적입니다. 이러한 방법들은 데이터 수집의 효율성을 높이고, 특정 도메인이나 복잡한 추론 작업에 특화된 RM을 훈련하는 데 기여합니다.

**RM은 어떻게 작동하는가?**

우리는 RM이 브래들리-테리 선호도 모델(Bradley-Terry model of preference)에 기반을 두고 있다는 것을 알고 있지만, 이러한 통계 모델(statistical model)을 실용적으로 구현하는 방법은 많습니다. LLM 도메인에서는 이러한 모델이 LLM으로 구현됩니다(아마도 놀랄 일은 아닐 것입니다). 그러나 표준 (생성형) 디코더 전용 LLM(decoder-only LLMs)과 비교할 때, RM은 기본 아키텍처(architecture)와 훈련 목표(training objective)를 모두 수정합니다.

**RM 아키텍처(architecture)의 개략도(Schematic depiction of RM architecture)**
(Image of RM architecture)

**RM 아키텍처(architecture).** RM은 LLM으로부터 프롬프트-완성 쌍(prompt-completion pair)을 입력으로 받아 스칼라 선호도 점수(scalar preference score)를 출력합니다. 실제로 RM은 디코더 전용 아키텍처(decoder-only architecture)의 끝에 선형 헤드(linear head)를 추가하여 LLM으로 구현됩니다(위 참조). 특히, LLM은 토큰 벡터(token vectors) 목록(각 입력 토큰 벡터(token vector)에 대해 하나씩)을 출력하며, 이 목록의 최종 벡터를 선형 헤드(linear head)를 통해 전달하여 단일 스칼라 점수를 생성합니다. RM을 주어진 완성(completion)이 선호되는지 여부를 분류하는 데 사용되는 추가 분류 헤드(classification head)가 있는 LLM으로 생각할 수 있습니다.

**훈련 과정(Training process).** RM의 매개변수(parameters)는 일반적으로 기존 정책(policy) 2로 초기화되며, 이를 RM의 "기반(base)" 모델이라고 부를 것입니다. RM을 초기화할 정책(policy)에 대한 몇 가지 선택지가 있습니다. 예를 들어, 훈련 중인 LLM 또는 사전 훈련된 기반(pretrained base) 모델이나 SFT 모델(SFT model)과 같은 이 모델의 이전 버전이 있습니다. RM이 초기화되면 이 모델에 선형 헤드(linear head)를 추가하고 선호도 데이터셋(preference dataset)(즉, 프롬프트(prompt)에 대한 선택된 모델 응답(chosen model responses) 및 거부된 모델 응답(rejected model responses) 쌍)에 대해 훈련합니다.

**RM 출력에 대한 쌍별 확률 표현(Pairwise probability expressed with respect to the output of an RM)**
$$P(\text{chosen is preferred to rejected}) = \frac{e^{s_{\text{chosen}}}}{e^{s_{\text{chosen}}} + e^{s_{\text{rejected}}}}$$

선호도 쌍이 주어졌을 때, 우리는 RM이 거부된 응답에 비해 선택된 응답에 더 높은 점수를 할당하기를 원합니다. 즉, 최적의 RM은 선택된 응답이 거부된 응답보다 선호될 확률을 극대화해야 합니다. 앞에서 배운 것처럼, 우리는 브래들리-테리 모델(Bradley-Terry model)을 사용하여 이 확률을 표현할 수 있습니다(위 참조). 이 확률 표현식을 재배열하면 아래에 표시된 손실 함수(loss function)를 도출할 수 있습니다. 이는 모델이 선택된 응답에 더 높은 점수를 할당하도록 장려하는 쌍별 순위 손실(pairwise ranking loss)입니다.

**RM의 표준 손실 함수(loss function) 공식(Standard loss function formulation for an RM)**
$$L = -\log \left( \frac{e^{s_{\text{chosen}}}}{e^{s_{\text{chosen}}} + e^{s_{\text{rejected}}}} \right) = -\log(\text{sigmoid}(s_{\text{chosen}} - s_{\text{rejected}}))$$

이를 음의 로그 우도(NLL) 손실(negative log likelihood (NLL) loss)로 생각할 수 있으며, NLL에 대한 확률은 브래들리-테리 모델(Bradley-Terry model)에 의해 주어집니다. 이 손실의 지형(landscape) 시각화는 아래에 표시되어 있으며, 선택된 점수가 최대화되고 거부된 점수가 최소화될 때 손실이 최소화됨을 알 수 있습니다.

(Image of loss landscape)

대규모 선호도 데이터셋(preference dataset)에 대해 이 손실 함수(loss function)를 경험적으로 최소화함으로써, 우리는 선택된 응답이 거부된 응답보다 선호될 기대 확률(expected probability)을 (대략적으로) 최대화할 수 있습니다.

**보상 정규화(Normalizing the reward).** 훈련 후 RM은 정규화되지 않은 스칼라 값(unnormalized scalar values)을 출력합니다. 보상 함수(reward function)의 분산(variance)을 낮추기 위해(즉, RM의 출력이 표준 범위(standard range)에 속하도록 하기 위해), 훈련에 사용된 선호도 데이터셋(preference dataset)에 대해 평균 보상(reward)이 0이 되도록 RM의 출력을 정규화할 수 있습니다. [14]의 저자들은 이 보상 정규화 접근 방식(reward normalization approach)을 사용한다고 언급합니다.

"훈련이 끝날 때, 우리는 데이터셋(dataset)의 참조 요약이 평균 점수 0을 달성하도록 보상 모델(reward model) 출력을 정규화합니다." - [14]에서

RM의 아키텍처는 단순히 디코더 전용 LLM에 선형 헤드를 추가하는 것 외에도 다양한 형태로 진화하고 있습니다. 예를 들어, 멀티모달(multimodal) RM은 텍스트뿐만 아니라 이미지나 오디오와 같은 다양한 형태의 입력을 처리하여 더욱 풍부한 선호도 평가를 가능하게 합니다. 또한, 단순히 최종 토큰의 임베딩을 사용하는 대신, 전체 시퀀스의 임베딩을 종합적으로 고려하거나, 어텐션 메커니즘(attention mechanism)을 통해 중요한 부분에 더 가중치를 두는 방식으로 선호도 점수를 계산하는 연구도 진행 중입니다. 이러한 아키텍처적 개선은 RM이 더욱 미묘한 인간의 선호도를 포착하고, 특정 작업에 대한 평가 능력을 향상시키는 데 기여합니다. 훈련 과정에서는 정규화 기법 외에도, 과적합(overfitting)을 방지하고 일반화 성능(generalization performance)을 높이기 위해 다양한 정규화(regularization) 및 드롭아웃(dropout) 전략이 적용됩니다. 또한, 보상 함수의 안정성을 높이기 위해 이동 평균(moving average)이나 지수 가중 이동 평균(exponentially weighted moving average)을 사용하여 보상 값을 평활화(smoothing)하는 기법도 활용될 수 있습니다.

**RM 구현하기**

이 논의를 더 실용적으로 만들기 위해, 아키텍처(architecture)와 손실 함수(loss function)를 포함한 RM이 일반적인 딥러닝 프레임워크(deep learning frameworks)를 사용하여 어떻게 구현될 수 있는지 알아보겠습니다. RM은 단지 분류 모델(classification model)입니다. 텍스트 시퀀스(textual sequences)에 대한 텍스트 분류(text classification)를 수행합니다. 프롬프트(prompt)와 응답이 입력으로 주어졌을 때, RM은 이 프롬프트-응답 쌍이 선호될 가능성(즉, 단일 스칼라 점수)을 예측합니다.

**예시(Toy example).** HuggingFace의 `AutoModelForSequenceClassification`과 같은 추상화(abstraction)를 통해 이를 구현할 수 있습니다. 로컬에서 실행할 수 있는 작은 (BERT 기반) RM 구현은 아래에 제공되어 있으며, 여기서 우리는 다음을 수행합니다.

*   `AutoModelForSequenceClassification`을 사용하여 RM을 생성합니다.
*   모든 선택된 시퀀스(chosen sequences) 및 거부된 시퀀스(rejected sequences) 3에 대해 RM의 출력(단일 로짓(logit) 형태)을 계산합니다.
*   위에 설명된 대로 RM의 손실을 계산합니다.

```python
from transformers import (
    AutoTokenizer,
    AutoModelForSequenceClassification,
)
import torch

# Load a tiny model for sequence classification
model_name = "google-bert/bert-bert-base-uncased"
tokenizer = AutoTokenizer.from_pretrained(
    model_name, trust_remote_code=True,
)
model = AutoModelForSequenceClassification.from_pretrained(
    model_name, trust_remote_code=True,
)

# Chosen prompt-response sequences
chosen_seqs = [
    "I love deep (learning) focus!",
    "Cameron is great at explaining stuff",
    "AGI is coming very soon...",
]

# Rejected prompt-response sequences
rejected_seqs = [
    "I'm not a fan of deep (learning) focus",
    "Cameron doesn't know what he's talking about",
    "AGI is fake and LLMs can't reason!",
]

# Tokenize the chosen / rejected sequences
chosen_inps = tokenizer(
    chosen_seqs, return_tensors="pt", padding=True,
)
rejected_inps = tokenizer(
    rejected_seqs, return_tensors="pt", padding=True,
)

# Compute the RM's output
rewards_chosen = model(**chosen_inps).logits[:, 0]
rewards_rejected = model(**rejected_inps).logits[:, 0]

# Compute the RM's loss
loss = -torch.nn.functional.logsigmoid(
    rewards_chosen - rewards_rejected
).mean()
print(loss)
```

여기서부터 우리는 다른 모델과 유사하게 RM을 훈련합니다. 즉, i) 선호도 데이터셋(preference dataset)을 반복하고, ii) 위에 설명된 대로 손실을 계산하고, iii) 역전파(backpropagation)를 통해 기울기(gradient)를 얻고, iv) 기울기 업데이트(gradient update)를 수행하고, v) 반복합니다.

**실제 RM 훈련 예시(Real RM training example).** 실제 LLM 연구 환경에서 RM 훈련은 단일 GPU에서 실행되는 간단한 예시보다 훨씬 복잡합니다. 특히 대규모 모델과 데이터셋을 다룰 때는 분산 훈련(distributed training) 환경이 필수적입니다. AI2의 OpenInstruct와 같은 프로젝트에서는 `accelerate` 라이브러리를 활용하여 OLMo-2와 같은 기반 모델을 바탕으로 RM을 분산 환경에서 효율적으로 훈련합니다. 이러한 스크립트는 데이터 병렬화(data parallelism), 모델 병렬화(model parallelism), 파이프라인 병렬화(pipeline parallelism) 등 다양한 분산 훈련 전략을 통합하여 대규모 계산 자원을 효과적으로 활용합니다.

```python
for _ in range(args.num_train_epochs):
    for data in dataloader:
        training_step += 1
        # Concat the chosen / rejected sequences
        query_responses = torch.cat(
            (
                data[CHOSEN_INPUT_IDS_KEY],
                data[REJECTED_INPUT_IDS_KEY]
            ),
            dim=0,
        )
        with accelerator.accumulate(model):
            # Predict reward for each sequence with RM
            _, predicted_reward, _ = get_reward(
                model,
                query_responses,
                tokenizer.pad_token_id,
                0,
            )
            # Parse chosen / rejected rewards from output
            chosen_reward = predicted_reward[
                :data[CHOSEN_INPUT_IDS_KEY].shape[0]
            ]
            rejected_reward = predicted_reward[
                data[CHOSEN_INPUT_IDS_KEY].shape[0]
                :
            ]
            # Compute loss and gradient for RM
            loss = -F.logsigmoid(chosen_reward - rejected_reward).mean()
            accelerator.backward(loss)
            # Perform parameter update for RM
            optimizer.step()
            optimizer.zero_grad()
```

위 코드는 분산 훈련 환경에서도 핵심 RM 훈련 루프가 어떻게 유지되는지를 보여줍니다. HuggingFace `accelerate`와 같은 도구 덕분에 개발자는 분산 환경의 복잡성 대부분을 추상화하여, 위 예시에서 보듯이 기본적인 훈련 로직은 단일 장치 훈련과 크게 다르지 않습니다. 그러나 실제 대규모 RM 훈련에서는 체크포인트 관리(checkpoint management), 로깅(logging), 하이퍼파라미터 튜닝(hyperparameter tuning), 그리고 잠재적인 하드웨어 오류 처리 등 추가적인 엔지니어링 고려 사항이 중요합니다. 이러한 요소들은 RM의 안정적이고 효율적인 훈련을 보장하며, 최종 LLM의 성능에 직접적인 영향을 미칩니다.

**다양한 유형의 RM**

지금까지 우리는 일반적으로 분류기 기반 RM(classifier-based RM)이라고 불리는 RM의 표준 형태에 초점을 맞췄습니다. 그러나 RM은 프롬프트(prompt)와 응답이 주어졌을 때 선호도 점수(preference score)를 예측하는 모델일 뿐이며, 이를 여러 가지 방법으로 구현할 수 있습니다. 예를 들어, ArmoRM과 같은 맞춤형 분류기(custom classifier)를 RM으로 사용하도록 훈련할 수 있습니다.

(출처: [9]) LLM-as-a-Judge 모델(LLM-as-a-Judge models)도 LLM 심판에게 선호도 점수(preference score)를 제공하도록 간단히 프롬프트(prompt)를 줌으로써 RM으로 사용될 수 있습니다(위 참조). 이러한 선호도 점수(preference score)는 RL을 통한 훈련(training) 동안 보상 신호(reward signal)로 사용될 수 있습니다. LLM-as-a-Judge에 대한 더 심층적인 개요는 아래 링크된 기사를 참조하십시오.

**LLM을 평가에 사용하기(Using LLMs for Evaluation)**
Cameron R. Wolfe, Ph.D. · 2024년 7월 22일
전체 이야기 읽기

LLM-as-a-Judge는 특히 복잡하고 주관적인 평가 기준이 필요한 경우에 유용합니다. 예를 들어, 창의성, 스타일, 공감 능력과 같은 미묘한 품질을 평가할 때 인간 평가자의 부담을 줄이고 확장성을 높일 수 있습니다. 하지만 LLM 심판은 자체적인 편향(bias)을 가질 수 있으며, '자기 강화(self-reinforcement)' 문제, 즉 특정 LLM의 응답을 다른 LLM이 더 선호하는 경향이 있을 수 있다는 비판도 있습니다. 이를 완화하기 위해 여러 LLM 심판을 앙상블(ensemble)하거나, 다양한 관점을 반영하는 프롬프트 엔지니어링(prompt engineering) 기법이 연구되고 있습니다.

대안으로, 우리는 LLM 심판을 사용하여 합성 선호도 데이터(synthetic preference data)를 수집하고(아래 AlpacaEval에서 보여지는 것과 같은 프롬프트(prompt)를 사용하여), Constitutional AI [10] 및 RLAIF [11]에서 수행하는 것처럼 이 합성 데이터에 대해 RM을 정상적으로 훈련할 수 있습니다.

(출처)
(Image of AlpacaEval prompt example)

결과 보상 모델(Outcome Reward Models, ORM) [12] 및 프로세스 보상 모델(Process Reward Models, PRM) [11]은 문헌에서 일반적으로 사용되는 RM의 두 가지 다른 변형입니다. 주로 추론 작업(reasoning tasks)에 사용되는 ORM은 완성(completion)이 작업에 대한 정답(correct answer)일 확률을 예측합니다. ORM을 훈련하기 위해 이전과 유사하게 선호도 데이터셋(preference dataset)을 수집하지만, 각 선호도 쌍에는 주어진 질문에 대한 오답과 정답이 모두 포함됩니다. 시퀀스 수준(sequence level)에서 보상(reward)을 예측하는 표준 RM과 달리, ORM은 토큰별(per-token) 기준으로 정확성을 예측합니다.

"우리의 검증기(verifiers)는 언어 모델(language models)이며, 토큰별(per-token) 예측을 출력하는 작은 스칼라 헤드(scalar head)를 가지고 있습니다." - [12]에서

(출처: [12]) ORM과 마찬가지로, PRM은 주로 추론 작업(reasoning tasks)에 사용되며 더 세분화된 출력(granular outputs)을 예측하지만, PRM은 각 토큰(token) 이후가 아니라 추론 과정(reasoning process)의 각 단계 이후에 예측을 수행합니다. PRM은 다양한 논문에서 사용되었지만, PRM에 대한 훈련 데이터(training data)를 수집하는 것은 어렵습니다. 왜냐하면 세분화된 감독(granular supervision)(즉, 추론 과정(reasoning process)의 각 단계에서 정확성 신호)이 필요하기 때문입니다.

"PRM은 사고 과정 추론 프로세스(chain of thought reasoning process)의 모든 단계에서 점수를 출력하도록 훈련된 보상 모델(reward models)입니다. 이는 EOS 토큰(EOS token)에서만 점수를 출력하는 표준 RM 또는 모든 토큰(token)에서 점수를 출력하는 ORM과 다릅니다. 프로세스 보상 모델(Process Reward Models)은 각 추론 단계(reasoning step)의 끝에서 감독(supervision)을 필요로 합니다." - 출처

PRM의 경우, 세분화된 감독 데이터를 효율적으로 수집하는 것이 핵심 과제입니다. 최근에는 '인간-인-더-루프(human-in-the-loop)' 시스템을 통해 사용자가 추론 과정의 특정 지점에서 피드백을 제공하거나, LLM 자체를 사용하여 중간 단계의 정확성을 검증하고 이를 PRM 훈련에 활용하는 'AI 피드백 기반 강화 학습(Reinforcement Learning from AI Feedback, RLAIF)' 방식이 발전하고 있습니다. 이러한 접근 방식은 복잡한 다단계 추론 작업을 수행하는 LLM의 신뢰성과 투명성을 높이는 데 기여하며, 특히 코딩, 수학, 과학적 추론과 같은 분야에서 큰 잠재력을 보여줍니다.

**후처리 훈련(Post-Training)에서 보상 모델의 역할**

(출처: [3]) 초기 ChatGPT 이후 LLM은 InstructGPT [3]가 제안한 3단계 정렬 절차(alignment procedure)(위 참조)를 사용하여 거의 항상 후처리 훈련(post-trained)되었습니다. 이 절차는 다음 세 단계로 구성됩니다.

*   **지도 미세 조정(Supervised finetuning, SFT)** — 즉, 명령어 미세 조정(instruction finetuning, IFT) — 좋은 완성(completion) 예시에 대해 다음 토큰 예측(next-token prediction)을 사용하여 모델을 훈련합니다.
*   **보상 모델(RM)**은 인간 선호도 데이터셋(human preference dataset)에 대해 훈련됩니다.
*   **강화 학습(RL)**은 RM의 출력을 훈련 신호로 사용하여 LLM을 미세 조정(finetune)하는 데 사용됩니다.

이 절차의 2단계와 3단계는 총체적으로 인간 피드백 기반 강화 학습(reinforcement learning from human feedback, RLHF)이라고 불립니다. 우리는 강화 학습(RL) 옵티마이저(optimizer)를 사용하여 LLM을 미세 조정(finetune)하고 선호도 레이블(preference labels)을 통해 인간 피드백(human feedback)을 통합합니다.

(출처: [4]) 오늘날 이야기는 조금 더 복잡합니다. Tulu-3 [4]에 사용된 더 현대적인 후처리 훈련 파이프라인(post-training pipeline)의 예시가 위에 제공되어 있습니다. 원래의 3단계 정렬 절차(alignment procedure)와의 주요 차이점은 다음과 같습니다.

*   SFT 단계(phase)는 여전히 매우 일반적이지만, 특히 최근 추론 모델(reasoning models)의 경우 항상 사용되는 것은 아닙니다. 예를 들어, DeepSeek-R1의 일부 변형은 SFT를 포기하고 사전 훈련된 모델(pretrained model)에 직접 RL을 적용합니다.
*   RL 훈련(training)은 일반적으로 여러 라운드(round)로 수행되며, 각 라운드(round)마다 새로운 데이터가 수집되어 LLM의 기능(capabilities)을 더욱 향상시킵니다.
*   RM을 필요로 하거나 필요로 하지 않을 수 있는 여러 RL 변형(및 비 RL 기반 대안)이 잠재적으로 함께 사용됩니다.

추가적인 복잡성에도 불구하고, 오늘날에도 데이터 품질은 성공적인 후처리 훈련(post-training)의 핵심 결정 요인으로 남아 있습니다. 이 섹션에서는 RL 훈련 프레임워크(frameworks)를 높은 수준에서 다루고, 각 프레임워크(framework)에서 RM의 역할(있는 경우)에 초점을 맞출 것입니다.

최근 LLM 정렬(alignment) 파이프라인은 더욱 정교해지고 있으며, RM의 역할은 여전히 중요하지만, 그 활용 방식은 다양해지고 있습니다. 예를 들어, RLAIF(Reinforcement Learning from AI Feedback)는 인간 피드백을 직접적으로 사용하는 대신, 강력한 LLM 심판(judge)이 생성한 피드백을 사용하여 RM을 훈련하거나 정책 모델을 직접 정렬합니다. 이는 인간 평가에 드는 비용과 시간을 줄이면서도 효과적인 정렬을 가능하게 합니다. 또한, 일부 연구에서는 RM을 주기적으로 업데이트하고 재훈련하여, 정책 모델의 진화에 맞춰 보상 신호의 품질을 유지하는 '동적 RM(dynamic RM)' 접근 방식을 탐구하고 있습니다. 이러한 동적 접근 방식은 LLM이 새로운 지식이나 행동 패턴을 학습함에 따라 RM이 구식화되는 것을 방지하여, 지속적인 성능 향상을 도모합니다. 이처럼 RM은 LLM의 후처리 훈련 과정에서 단순한 보상 제공자를 넘어, 정렬 전략의 유연성과 확장성을 결정하는 핵심 구성 요소로 진화하고 있습니다.

**LLM을 위한 RL 훈련 전략(RL Training Strategies for LLMs)**

LLM을 강화 학습(RL)로 훈련하는 데 사용되는 높은 수준의 설정(setup)에 익숙하지 않은 분들을 위해 아래 개요를 참조하십시오. LLM 맥락에서 RL에 대한 기본적인 이해는 이 논의의 필수 전제 조건입니다.

**LLM을 위한 강화 학습(Reinforcement Learning)의 기초(Basics of Reinforcement Learning for LLMs)**
Cameron R. Wolfe, Ph.D. · 2023년 9월 25일
전체 이야기 읽기

LLM 정렬(alignment)을 위한 RL 훈련 전략은 크게 두 가지 주요 흐름으로 나눌 수 있습니다. 하나는 RM을 명시적으로 사용하는 RLHF(Reinforcement Learning from Human Feedback) 방식이고, 다른 하나는 RM 없이 직접 정책 모델을 최적화하는 방식입니다.

*   **RLHF (Reinforcement Learning from Human Feedback):** 이 방식은 InstructGPT [3]에서 제안된 표준 3단계 절차의 핵심 부분입니다. 인간의 선호도를 학습한 RM이 LLM이 생성하는 응답에 대한 보상 신호(reward signal)를 제공하고, 이 보상을 바탕으로 정책 모델(policy model)인 LLM을 PPO(Proximal Policy Optimization)와 같은 강화 학습 알고리즘으로 미세 조정(finetune)합니다. RM은 복잡하고 미묘한 인간의 선호도를 포착하는 데 중요한 역할을 합니다.

*   **RM 없는 직접 정렬(Direct Alignment without RM):** 이 범주에는 여러 기술이 포함됩니다.
    *   **RLVR (Reinforcement Learning with Verifiable Rewards):** LLM이 제공하는 응답이 정답인지 오답인지 명확하게 검증할 수 있는 작업(예: 수학 문제 풀이, 코드 생성)에 사용됩니다. 이 경우 보상은 RM 없이 결정론적(deterministic)으로 계산될 수 있으며, 이는 '보상 해킹(reward hacking)'의 위험을 줄입니다.
    *   **DPO (Direct Preference Optimization) [6]:** DPO는 RM을 명시적으로 훈련하는 단계 없이, 선호도 데이터셋에서 직접 정책 모델을 최적화합니다. 이는 RM이 브래들리-테리 모델을 통해 선호도를 학습하는 것과 유사하게, DPO는 정책 모델 자체를 사용하여 선택된 응답과 거부된 응답 사이의 로그 확률 비율(log-probability ratio)을 최적화합니다. 이는 RM을 '암묵적으로(implicitly)' 학습하는 것으로 볼 수 있습니다. DPO의 간결함과 안정성 때문에 최근 많은 연구에서 활용되고 있습니다.
    *   **IPO (Identity Preference Optimization) 및 KTO (Kahneman-Tversky Optimization):** DPO의 변형으로, IPO는 DPO의 과적합(overfitting) 문제를 완화하기 위해 설계되었으며, KTO는 인간의 인지적 편향(cognitive biases)을 모델링하여 선호도 학습을 개선합니다. 이러한 방법들은 RM 없이도 다양한 방식으로 인간의 선호도를 효과적으로 통합하려는 시도입니다.

"RLHF는 복잡하고 종종 불안정한 절차(procedure)입니다... 우리는 표준 RLHF 문제를 간단한 분류 손실(classification loss)만으로 해결할 수 있도록 하는 RLHF의 보상 모델(reward model)에 대한 새로운 매개변수화(parameterization)를 도입합니다." - [6]에서

**직접 정렬(Direct alignment).** RLVR만이 RM 사용을 피하는 유일한 방법은 아닙니다. 사실, 우리는 RM을 완전히 포기하면서도 RLHF와 유사하게 모델을 인간 선호도에 정렬(align)할 수 있습니다. 이러한 기술을 직접 정렬 알고리즘(direct alignment algorithms)이라고 하며, 이 클래스에서 가장 널리 사용되는 알고리즘은 직접 선호도 최적화(direct preference optimization, DPO) [6]입니다. DPO와 같은 직접 정렬 알고리즘(direct alignment algorithms)은 RLHF와 동일한 훈련 목표(training objective)를 최적화하면서 RM을 포기할 뿐만 아니라, RL 훈련(training) 자체를 완전히 피합니다. RLHF와 DPO의 비교는 아래에 제공되어 있습니다.

(출처: [6])
(Image comparing RLHF and DPO)

DPO에서 훈련(training)에 사용되는 손실 함수(loss function)는 아래에 제시되어 있습니다. 보시다시피, 이 손실 함수(loss function)는 RM이 사용하는 손실 함수(loss function)와 매우 유사합니다. 그러나 우리는 더 이상 RM으로 보상(reward)을 예측하지 않습니다. 대신, 현재 정책(current policy)과 참조 정책(reference policy)이 할당한 선택된 완성(chosen completions) 및 거부된 완성(rejected completions)의 확률을 사용하여 보상(reward)을 암묵적으로 추정하기 위해 정책(policy)을 직접 사용합니다. 직관적으로, 이 손실은 선택된 완성(completion)의 로그 비율(log-ratio)이 거부된 완성(completion)의 로그 비율(log-ratio)보다 클 때 최소화됩니다. DPO는 현재 정책(policy)이 거부된 응답에 비해 선택된 응답에 더 높은 (암묵적) 보상(rewards)을 할당하도록 훈련합니다.

**DPO 훈련 손실(training loss)**
$$L_{DPO}(\pi, \pi_{ref}) = -\mathbb{E}_{(x, y_c, y_r) \sim D} \left[ \log \sigma \left( \log \frac{\pi(y_c|x)}{\pi_{ref}(y_c|x)} - \log \frac{\pi(y_r|x)}{\pi_{ref}(y_r|x)} \right) \right]$$

(출처: [6]) DPO는 중간 RM 생성을 필요로 하지 않습니다. 그러나 손실 함수(loss function)는 여전히 브래들리-테리 모델(Bradley-Terry model)에서 파생되며, 우리는 여전히 RM을 학습하고 있습니다. 여기서 핵심적인 차이점은 RM이 명시적(explicitly)이 아닌 암묵적(implicitly)으로 학습된다는 것입니다. 따라서 DPO 논문 [6]의 제목은 "당신의 언어 모델은 비밀리에 보상 모델(reward model)입니다(Your Language Model is Secretly a Reward Model)"입니다. 우리는 RM과 유사하게 DPO 모델에서 이 암묵적인 보상 추정치를 직접 얻을 수 있습니다. DPO의 전체 유도 및 분석은 여기를 참조하십시오.

이러한 다양한 정렬 전략은 LLM의 성능과 안전성을 향상시키는 데 각기 다른 장점을 제공합니다. RLHF는 강력한 RM을 통해 미묘한 인간의 선호도를 학습할 수 있지만, 복잡성과 보상 해킹의 위험이 있습니다. DPO와 같은 직접 정렬 방법은 구현이 간단하고 안정적이지만, RM의 명시적인 평가 능력을 제공하지 않습니다. 궁극적으로 최적의 전략은 특정 애플리케이션의 요구 사항, 사용 가능한 데이터의 특성, 그리고 계산 자원에 따라 달라질 것입니다.

**RM은 왜 유용한가?**

**RL에서 참조, 보상, 가치 및 정책 모델(policy models) 조율(Orchestrating reference, reward, value and policy models in RL)**
(Image showing the complexity of RL models)

RM을 사용하는 것은 LLM 훈련 과정(training process)에 추가적인 복잡성과 비용을 야기하는 것은 분명합니다. 별도의 모델을 대규모 선호도 데이터셋에 대해 훈련해야 하며, 이는 상당한 계산 자원을 필요로 합니다. 또한, RL 훈련 중에는 RM을 사용하여 정책 모델이 생성한 완성에 실시간으로 점수를 매겨야 하는데, RM 자체가 LLM이므로 이 과정에서 추가적인 추론 비용이 발생합니다. 이러한 복잡성은 훈련 파이프라인의 효율적인 조율을 어렵게 만들 수 있습니다.

"우리는 신경 RM이 대규모 강화 학습(reinforcement learning) 과정에서 보상 해킹(reward hacking)으로 고통받을 수 있음을 발견했습니다. 보상 모델(reward model)을 재훈련하는 것은 추가적인 훈련 자원을 필요로 하며 전체 훈련 파이프라인(training pipeline)을 복잡하게 만듭니다." - [7]에서

**보상 해킹(Reward hacking).** 더 나아가, RM은 보상 해킹(reward hacking)의 대상이 됩니다. RM은 낮은 품질의 완성(completion)에 허위로 높은 보상(rewards)을 할당하거나, 더 일반적으로, 정책(policy)이 실제로 원하는 작업(task)을 해결하지 않고도 높은 보상(rewards)을 받을 수 있도록 악용될 수 있습니다. 흥미롭게도, 보상 해킹(reward hacking)은 RLHF를 통한 훈련(training) 확장을 방해하는 주요 한계(limitation)입니다. 우리가 충분히 오랫동안 훈련(training)을 계속한다면, 우리의 정책(policy)은 결국 RM에 대한 악용을 찾을 것입니다. 대조적으로, 검증 가능한 보상(verifiable rewards)은 해킹하기가 더 어렵기 때문에(불가능하지는 않지만), RLVR을 사용할 때 추론 모델(reasoning models)을 더 광범위하게(즉, 더 많은 반복(iterations) 동안) 훈련(training)할 수 있습니다. 보상 해킹을 완화하기 위한 전략으로는 RM의 견고성(robustness)을 높이는 훈련 기법(예: 적대적 훈련(adversarial training)), 보상 함수의 설계에 대한 신중한 접근, 그리고 주기적인 RM 업데이트 및 재훈련을 통해 정책 모델의 진화하는 행동을 반영하는 것이 있습니다. 이러한 노력은 LLM이 단순히 높은 보상을 추구하는 것이 아니라, 진정으로 유용하고 안전한 응답을 생성하도록 유도하는 데 필수적입니다.

**RM을 피해야 하는가?**

RM의 추가 비용과 복잡성을 고려할 때, 우리는 궁금할 수 있습니다. RM을 완전히 피해야 하는가? 이 질문에 대한 명확한 답은 없습니다. RLVR을 통해 인상적인 결과가 달성되었으며, 우리는 RM을 피하는 DPO와 같은 기술로 모델을 인간 선호도에 정렬(align)할 수 있습니다. 많은 연구에서 RLHF와 DPO 사이에 성능 격차(performance gap)가 있는지 여부에 대해 다른 결과를 가지고 논쟁했습니다. DPO가 효과적인 RM 없는 선호도 튜닝 대안인지는 사용 사례에 따라 다르지만, 이러한 기술들 사이에 성능 격차(performance gap)가 있다는 사실은 일반적으로 사실로 받아들여집니다.

"RLHF의 보급은 인간의 가치와 선호도를 언어 모델(language models)에 통합하는 데 가장 큰 어려움 중 하나인 명시적인 보상(reward) 지정을 회피하는 데 효과적이기 때문입니다." - [1]에서

**RM의 유용성(utility).** 이러한 발견에도 불구하고, 우리는 RM이 엄청나게 중요하고 강력한 개념(concept)이라는 사실을 잊어서는 안 됩니다. 어떤 형태의 RL 훈련(training)에서든 가장 어려운 작업 중 하나는 보상(reward)을 지정하는 것입니다. LLM의 경우 이 작업은 특히 어렵습니다. LLM에서 "좋은" 응답을 구성하는 것이 무엇인지 어떻게 명시적으로 정의할 수 있을까요? 불행히도, 사용할 수 있는 단일 속성이나 품질은 없습니다. 유효한 모델 응답의 범위는 거의 무한합니다. RM을 사용하면, 인간에게 선호도 피드백(preference feedback)을 제공하도록 요청하는(즉, 모델 응답 쌍 중에서 선택하는) 더 간단한 작업으로 이 과정을 증류함으로써 명시적인 보상(reward)을 지정하는 문제를 회피합니다(아래 참조).

**인간 선호도 데이터(human preference data) 수집을 위한 인터페이스(Interface)**
(Image of human preference data collection interface)

쌍에서 더 나은 모델을 선택하는 것은 개별 모델 응답을 수동으로 작성하거나 평가하는 것보다 훨씬 간단한 작업입니다. 인간은 단지 이진 선호도(binary preference)를 제공하기만 하면 됩니다. 우리는 이 선호도 피드백(preference feedback)에 대해 RM을 훈련할 수 있으며, 이는 보상(reward)을 명시적으로 지정하지 않고도 RL 훈련(training)을 위한 보상(reward)을 도출할 수 있게 합니다. 이러한 접근 방식은 일반적인 인간 피드백(human feedback)으로 LLM을 훈련하는 유연하고 효과적인 접근 방식(approach)을 제공하며, 이는 혁신적입니다.

RM을 피할지 여부에 대한 결정은 LLM의 최종 목표와 사용 가능한 자원에 따라 달라집니다. DPO와 같은 RM 없는 방법은 구현이 간단하고 안정적인 대안을 제공하지만, 복잡하고 미묘한 인간의 가치를 포착하는 데는 RM이 여전히 더 강력한 도구일 수 있습니다. 특히, RM은 다양한 사용 사례에서 LLM의 성능을 평가하고 개선하는 데 활용될 수 있습니다.

**Best-of-N 샘플링(sampling)을 수행하기 위해 RM 사용(Using an RM to perform Best-of-N sampling)**
(Image showing Best-of-N sampling with RM)

**RM의 다른 사용 사례.** RM은 단순히 RL 훈련을 위한 보상 신호를 제공하는 것을 넘어, LLM의 전반적인 성능과 효율성을 향상시키는 다양한 용도로 활용됩니다.

*   **Best-of-N 샘플링(sampling) 및 추론 시간 스케일링(inference-time scaling):** LLM은 주어진 프롬프트에 대해 여러 개의 후보 완성(candidate completions)을 생성할 수 있습니다. RM은 이들 중 가장 높은 점수를 받은 응답을 선택하여 최종 출력으로 제공함으로써, LLM의 품질을 향상시킵니다. 이는 특히 추론 시점(inference time)에 LLM의 성능을 스케일업(scale up)하는 효과적인 방법입니다. RM을 통해 생성된 여러 응답 중 최적의 응답을 선택함으로써, 계산 비용을 크게 늘리지 않고도 출력 품질을 개선할 수 있습니다.
*   **평가(Evaluation):** RM은 LLM의 성능을 평가하는 자동화된 지표(metric)로 사용될 수 있습니다. 인간 평가자에 의한 평가는 비용이 많이 들고 시간이 오래 걸리므로, RM은 대규모 실험에서 신속하고 일관된 평가를 제공하는 데 유용합니다. 특히, RM은 특정 도메인이나 품질 기준(예: 유용성, 무해성)에 특화된 평가를 수행할 수 있습니다.
*   **거부 샘플링(Rejection Sampling):** RM은 품질이 낮은 응답을 필터링하고 거부하는 데 사용될 수 있습니다. LLM이 생성한 응답 중 RM 점수가 특정 임계값(threshold) 이하인 응답은 재샘플링(resampling)되거나 폐기되어, 최종 출력의 전반적인 품질을 높입니다. 이는 특히 안전하지 않거나 사실과 다른(hallucinatory) 응답을 줄이는 데 효과적입니다.
*   **데이터 필터링(Data Filtering):** 대규모 비정형 데이터셋(unstructured datasets)에서 고품질의 데이터를 식별하고 필터링하는 데 RM이 활용될 수 있습니다. 예를 들어, 웹에서 수집된 텍스트 데이터 중 인간의 선호도에 부합하는 부분을 선별하여 SFT(Supervised Fine-Tuning)나 RM 훈련 자체를 위한 데이터셋을 구축하는 데 기여합니다.

이러한 다양한 사용 사례에도 불구하고, 우리는 일반적으로 RM의 성능을 다음을 기반으로 평가합니다:

*   **정확도(Accuracy)**: 쌍에서 선택된 응답을 올바르게 식별하는 RM의 능력.
*   **다운스트림 성능(Downstream performance)**: 특정 RM으로 RL 미세 조정(finetuned)된 LLM의 성능.
*   **추론 시간 스케일링(Inference-time scaling)**: Best-of-N 샘플링(sampling) 파이프라인(pipeline)에서 특정 RM을 사용하여 달성되는 성능 향상(Performance boost).

최근에는 이러한 평가 기준 외에도 RM의 '견고성(robustness)'과 '일반화 능력(generalization capability)'에 대한 중요성이 강조되고 있습니다. 즉, 훈련 데이터 분포를 벗어난 새로운 프롬프트나 공격적인 입력에 대해서도 RM이 일관되고 신뢰할 수 있는 보상 신호를 제공하는지 여부가 중요한 평가 요소로 부상하고 있습니다.

**실제 보상 모델(Reward Models in Practice)**

RM에 대한 이론적 이해를 바탕으로, 이제 실제 환경에서 RM이 어떻게 평가되고 활용되는지 살펴보겠습니다. 특히, RM의 효과를 체계적으로 평가하기 위해 고안된 주요 벤치마크인 RewardBench [1]와 그 후속작인 RewardBench 2 [2]에 초점을 맞출 것입니다. 이러한 벤치마크는 수많은 RM의 성능을 비교하고, 고품질 RM을 구축하기 위한 실용적인 지침을 제공합니다.

**Rewardbench: 언어 모델링을 위한 보상 모델 평가(Rewardbench: Evaluating Reward Models for Language Modeling) [1]**
(출처: [1])
(Image of RewardBench overview)

RM의 효과적인 훈련에는 기반 모델의 선택, 훈련 데이터의 구성, 그리고 훈련 에포크 수와 같은 수많은 실용적인 결정이 수반됩니다. 그러나 이러한 세부 사항은 종종 충분히 문서화되지 않아 RM 개발자들에게 어려움을 야기했습니다. [1]의 저자들은 RewardBench를 통해 이 문제를 해결하고자 했습니다. RewardBench는 다양한 RM을 체계적으로 평가함으로써, RM의 설계와 훈련에 대한 실용적인 선택들이 RM 자체의 성능과 이를 통해 훈련된 다운스트림 LLM의 성능에 미치는 영향을 분석했습니다. 이 연구는 고품질 RM을 생성하기 위한 모범 사례를 제시하며, RM의 작동 원리에 대한 이해를 심화시켰습니다.

"보상 모델(RM)은 사전 훈련된 모델(pretrained models)을 인간 선호도에 정렬(align)하기 위한 성공적인 RLHF의 핵심이지만, 이러한 보상 모델(reward models)의 평가에 초점을 맞춘 연구는 상대적으로 적었습니다." - [8]에서

**RewardBench란 무엇인가?** RewardBench는 RM 평가를 위한 공개 프레임워크(framework) 및 데이터셋(dataset)입니다. 이 벤치마크는 다양한 기능(capabilities)에 걸쳐 RM에 대한 구조화된 평가를 제공하여, 특정 유형의 RM이 어떻게 그리고 왜 작동하는지 더 잘 이해하는 데 기여합니다. RewardBench는 RM의 정확도(accuracy)를 측정하는 방식으로 작동합니다. 즉, RM이 주어진 프롬프트에 대한 '선택된(chosen)' 응답과 '거부된(rejected)' 응답을 얼마나 정확하게 구별하는지를 평가합니다.

**RewardBench에서 사용되는 채점 기술(Scoring technique used by RewardBench)**
(Image of RewardBench scoring)

RewardBench는 RM의 성능을 다각도로 평가하기 위해 다양한 도메인(domains)과 난이도 수준의 데이터를 포함합니다. 예를 들어, 일반 채팅(Chat) 응답부터 함정 질문(trick questions)과 미묘한 차이를 포함하는 어려운 채팅(Chat Hard), 안전(Safety) 관련 응답, 그리고 코딩 및 추론(Reasoning) 작업에 대한 응답까지 평가합니다. 이러한 포괄적인 접근 방식은 RM이 특정 도메인에서 강점을 보이지만 다른 도메인에서는 약점을 가질 수 있다는 사실을 밝혀냈습니다.

(출처: [1])
(Image showing RewardBench data composition)

특히, RewardBench는 응답 길이 편향(length bias)이 평가 결과에 영향을 미치지 않도록 모든 응답 쌍의 길이가 유사하도록 설계되었습니다. 또한, 미묘한 품질 차이를 포착하는 RM의 능력을 테스트하기 위해 선택된 응답과 거부된 응답 간의 차이가 작은 '어려운 선호도 예시'를 포함하여 RM의 변별력(discriminative power)을 심층적으로 분석합니다.

(출처: [1])
(Image showing difficult preference examples)

**RM 분석(Analysis of RMs).** [1]에서 수행된 50개 이상의 RM에 대한 광범위한 분석은 몇 가지 중요한 시사점을 제공합니다.

*   **어려운 작업에서의 성능:** '어려운 채팅' 및 '추론'과 같은 복잡한 하위 집합에서 RM의 성능은 일반적으로 낮게 나타났으며, 이는 이러한 영역이 RM 개선을 위한 주요 목표임을 시사합니다.
*   **모델 크기의 중요성:** 더 큰 RM, 특히 7B 이상의 모델은 '어려운 채팅' 및 '추론' 작업에서 일관되게 더 나은 성능을 보였습니다. 이는 복잡한 인간 선호도를 이해하는 데 더 많은 매개변수가 필요함을 나타냅니다.
*   **기반 모델의 영향:** Llama-3와 같은 더 강력한 기반 모델을 사용하는 것이 RM 성능 향상에 큰 도움이 됩니다. 이는 RM의 성능이 단순히 훈련 데이터셋뿐만 아니라, 기반 모델의 사전 학습된 지식과 능력에 크게 의존함을 보여줍니다.
*   **RM 유형별 스케일링:** 분류기 기반 RM은 LLM-as-a-Judge 스타일 RM보다 전반적으로 더 나은 성능을 보였지만, 모델 크기에 따른 성능 향상(스케일링 속성)은 RM의 스타일과 기반 모델에 따라 다르게 나타났습니다. 예를 들어, LLaMA-2 DPO 모델은 스케일에 따라 성능이 향상되었지만, Qwen-1.5 분류기 기반 RM은 그렇지 않았습니다. 이는 RM 스케일링이 단순히 크기 증가 이상의 복합적인 요인에 의해 결정됨을 시사합니다.

"Llama 2는 RewardBench의 모든 섹션에서 스케일링에 따라 명확한 개선을 보여주지만, Qwen 1.5는 단조로운 개선(monotonic improvement)이 덜하며, 이는 분포 외 일반화(generalization) 문제 때문일 가능성이 높습니다." - [1]에서

(출처: [1])
(Image showing scaling properties of LLaMA-2 DPO vs Qwen-1.5 RMs)

이러한 분석은 RM 개발자가 기반 모델 선택, 훈련 데이터의 다양성, 그리고 RM의 특정 용도에 맞춰 최적의 아키텍처와 훈련 전략을 고려해야 함을 강조합니다.

**DPO 및 PPO 분석: 선호도 피드백(Preference Feedback) 학습을 위한 모범 사례(Best Practices) 분리 [13]**

RM에 대한 모범 사례(best practices)를 학습한 다음 단계는 이러한 아이디어를 사용하여 더 나은 LLM을 훈련하는 것입니다. [13]에서 저자들은 RewardBench에서 얻은 교훈을 적용하여 RL 미세 조정(finetuning)을 심층 연구합니다. 특히, 이 논문은 DPO와 PPO의 성능 비교에 중점을 둡니다. 이 개요에서는 이러한 기술들 간의 비교에 초점을 맞추지 않을 것입니다. 그러나 이 분석에는 RM을 생성하여 훈련에 사용되는 LLM의 다운스트림 성능(downstream performance)을 극대화하기 위한 수많은 실제적인 교훈도 포함되어 있습니다.

(출처: [13])
(Image showing experimental results on data quality and RM size)

**데이터 품질.** [13]에 제시된 핵심 실험 결과는 위에 요약되어 있습니다. 실험은 Anthropic의 HH RLHF 데이터셋(dataset)에 대해 DPO 모델을 훈련하는 것으로 시작합니다. 이 데이터셋(dataset)은 오래되고 노이즈가 많은 데이터셋(dataset)으로 알려져 있습니다. 이 데이터는 모델 성능을 향상시키지만, 최신 고품질 선호도 데이터셋(preference dataset)인 UltraFeedback에 대한 훈련(training)에서 훨씬 더 큰 향상이 나타납니다. 동일한 데이터에 대해 PPO로 훈련(training)으로 전환할 때(즉, RM이 사용됨을 의미), 명확한 성능 향상이 나타나며, 이는 명시적 RM과 함께 PPO를 사용하는 것이 성능에 다운스트림 이점(downstream benefit)이 있음을 나타냅니다. 그러나 이 이점은 더 나은 데이터 사용의 영향에 비해 훨씬 작다는 점에 유의해야 합니다!

**더 큰 RM.** PPO를 통한 훈련(training)의 명확한 이점을 고려할 때, LLM이 더 큰 RM을 사용하는 것으로부터도 이점을 얻을 수 있는지 궁금할 수 있습니다. 이는 LLM 스케일링 법칙(scaling laws)을 고려할 때 직관적인 의미(intuitive sense)가 있지만, [13]의 관찰은 그렇게 간단하지 않습니다. RM을 13B에서 70B 매개변수(parameters)로 스케일링할 때, 다운스트림 LLM 성능은 동일한 SFT 체크포인트(checkpoint)에서 초기화된 모델에서도 정체된(stagnant) 상태로 유지됩니다. 관찰 가능한 성능 이점은 추론 도메인(reasoning domain)에서만 발생하며, 이는 더 큰 RM의 이점이 더 큰 모델의 우수한 기능(capabilities)이 유용하거나 필요한 시나리오에서만 명확하다는 것을 나타냅니다. 즉, 이러한 더 큰 RM이 유용하려면 더 어려운 데이터가 필요합니다!

"더 큰 보상 모델(reward model)을 사용한다면, 실제로 보상 모델(reward model)에 도전하는 데이터가 필요합니다." - 출처

**더 나은 데이터 + 더 큰 RM.** 위에 설명된 교훈을 결합하여, [13]의 저자들은 RM 훈련(training)을 위해 코딩 및 추론 작업(tasks)을 강조하는 더 크고 어려운 프롬프트(prompts) 세트를 수집하고 더 큰 RM이 유익한지 다시 테스트합니다. 이러한 실험에서 우리는 RM 품질 향상의 명확한 신호(signals)를 봅니다. 예를 들어, 이러한 더 크고 더 나은 RM은 아래에 표시된 것처럼 Best-of-N 샘플링(sampling)에 사용될 때 눈에 띄는 성능 향상(boost in performance)을 가져옵니다. 그러나 RewardBench와 다운스트림 성능(downstream performance)을 모두 살펴볼 때 이러한 개선은 훨씬 덜 명확합니다.

(출처: [13])
(Image showing Best-of-N sampling performance with different RMs)

간단히 말해, 더 크고 더 나은 RM을 사용하는 것이 이 RM이 RL 미세 조정(finetuning)에 사용될 때 LLM이 더 나아질 것이라는 것을 직접적으로 의미하지는 않습니다. 사실, [13]에서는 더 큰 RM을 사용할 때 일부 도메인(domains)에서 성능 저하(performance regression)까지 보입니다. 이러한 발견은 RM 평가를 매우 복잡하게 만듭니다. RM의 정확도(accuracy)를 측정하는 것만으로는 RM이 얼마나 유용할지 이해하는 데 도움이 되지 않습니다.

[13]의 연구는 RM의 크기와 데이터 품질 사이의 복잡한 관계를 강조합니다. 즉, 단순히 RM의 크기를 늘리는 것만으로는 다운스트림 LLM 성능이 보장되지 않으며, RM이 실제로 해결해야 할 '어려운' 문제와 '고품질' 데이터가 뒷받침될 때 비로소 그 잠재력을 최대한 발휘할 수 있습니다. 이는 RM을 설계하고 훈련할 때 데이터셋의 난이도와 다양성을 신중하게 고려해야 함을 시사합니다. 또한, RM의 정확도와 다운스트림 LLM 성능 사이의 간극은 RM 평가 방법론의 한계를 보여주며, RM의 유용성을 더 정확하게 예측할 수 있는 새로운 평가 지표의 필요성을 제기합니다.

**RewardBench 2: 보상 모델 평가 발전(Advancing Reward Model Evaluation) [2]**

(출처: [2])
(Image of RewardBench 2 overview)

초기 RewardBench의 성공에도 불구하고, LLM의 기능이 빠르게 발전함에 따라 RM 평가 방법론 또한 진화할 필요성이 대두되었습니다. 최근에 제안된 RewardBench 2 [2]는 이러한 요구를 충족시키기 위해 설계되었으며, RM 평가를 더욱 유용하고 현실적으로 만드는 것을 목표로 합니다. 이 새로운 벤치마크는 LLM이 마주하는 더 넓은 범위의 기술(skills)과 시나리오를 포괄하는 데이터를 포함하고 있으며, 그 결과 RM의 평균 점수가 이전보다 약 20점 낮아져 훨씬 더 도전적인 평가 환경을 제공합니다. 이는 RM이 실제 환경에서 직면하는 복잡성을 더 잘 반영합니다. RewardBench 2는 여전히 정확도 기반 접근 방식을 사용하지만, Best-of-N 샘플링과 같은 다운스트림 RM 사용과 높은 상관관계를 보이며, RL 미세 조정에 RM이 얼마나 효과적일지 예측하는 데 중요한 통찰력을 제공합니다.

**RM 성능 측정.** RewardBench 2는 RM의 변별력(discriminative power)을 더욱 엄격하게 테스트하기 위해 '4개 중 최고(best-of-4)' 접근 방식을 도입했습니다. 이는 각 프롬프트에 대해 하나의 선택된 응답과 세 개의 거부된 응답을 제시하고, RM이 선택된 응답에 가장 높은 점수를 부여하는지 평가하는 방식입니다. 이 방법은 무작위 추측의 정확도 기준선(random baseline)을 25%로 낮춰 RM이 진정으로 우수한 성능을 보이려면 더 높은 변별 능력이 필요함을 의미합니다.

(출처: [2])
(Image showing RewardBench 2 scoring technique)

RewardBench 2는 단순히 RM의 정확도뿐만 아니라, RM이 실제 LLM 워크플로우에 통합될 때의 다운스트림 성능을 측정함으로써 평가의 범위를 확장합니다. 이는 특정 RM이 Best-of-N 샘플링이나 RL 훈련에 사용될 때 LLM의 최종 성능에 어떤 영향을 미치는지를 직접적으로 평가합니다. 이러한 포괄적인 평가 프레임워크는 RM의 품질이 실제 애플리케이션에서 어떻게 발휘되는지에 대한 깊이 있는 이해를 가능하게 합니다.

(출처: [2])
(Image comparing RewardBench 2 evaluation with other benchmarks)

**데이터 구성(Data composition).** RewardBench 2의 데이터셋은 LLM의 다양한 기능(capabilities)을 포괄하는 6가지 핵심 도메인(domains)에 초점을 맞춥니다. 여기에는 '집중(Focus)', '수학(Math)', '안전(Safety)'과 같은 기존 도메인 외에도, '사실성(Factuality)', '정확한 명령어 따르기(Exact Instruction Following)', 그리고 '동점(Ties)'(즉, 여러 응답이 동등하게 유효할 때 RM의 처리 능력)과 같은 새로운 도전 과제가 포함됩니다.

"이 벤치마크(benchmark)는 광범위한 수동, 프로그래밍 방식 및 LM 기반 필터링 기술(filtering techniques)을 사용하여 WildChat 파이프라인(pipeline)에서 이전에 사용되지 않은 대부분의 인간 프롬프트(prompts)로 생성되었습니다." - [2]에서

RewardBench 2는 데이터 오염(data contamination) 문제를 해결하기 위해 실제 사용자로부터 수집된 ChatGPT 로그 데이터셋인 WildChat에서 샘플링된 '보지 못한(unseen)' 인간 작성 프롬프트를 사용합니다. 저자들은 다단계 데이터 큐레이션 파이프라인을 통해 데이터 오염을 철저히 제거하고, 각 프롬프트에 대한 완성(completions)을 최신 LLM에서 샘플링하며, LLM-as-a-Judge 및 자동 검증기를 포함한 다양한 신호를 사용하여 완성도를 필터링합니다. 이러한 엄격한 데이터 구성 과정은 벤치마크의 신뢰성과 유효성을 크게 높입니다.

(출처: [2])
(Image showing RewardBench 2 dataset composition)

**RewardBench 2 성능 및 시사점.** RewardBench 2에서 100개 이상의 RM을 평가한 결과, 몇 가지 중요한 발견이 있었습니다. 특히, Gemini 및 Claude와 같은 강력한 기반 모델(foundation model) 기반의 LLM-as-a-Judge 모델들이 매우 우수한 성능을 보였습니다. 이는 초기 RewardBench에서 분류기 기반 RM이 LLM-as-a-Judge보다 일관되게 더 나쁜 성능을 보였던 것과 대조적이며, 기반 모델의 기능 향상이 LLM-as-a-Judge의 효과를 크게 높였음을 시사합니다.

(출처: [2])
(Image showing RewardBench 2 performance of top-20 RMs)

또한, RM의 성능은 초기화에 사용된 기반 모델의 품질과 직접적으로 연결되어 있으며, 훈련 데이터의 혼합, 훈련 방식, 후처리 훈련 단계 등 다양한 요소가 RM의 도메인별 성능에 영향을 미칩니다. 흥미로운 점은, 일반적인 1 에포크 대신 2 에포크 동안 RM을 훈련하는 것이 유익할 수 있다는 발견입니다.

**다운스트림 성능(Downstream performance)과의 관계.** RewardBench 2의 성능은 Best-of-N 샘플링의 다운스트림 성능과 매우 높은 상관관계를 보였습니다. 이는 RewardBench 2가 Best-of-N 샘플링을 위한 RM 선택에 유용한 지표임을 의미합니다. 그러나 RL 훈련에서의 다운스트림 성능과의 상관관계는 다소 복잡했습니다. 중요한 발견은 훈련 중인 RM과 정책 모델이 동일한 '모델 계보(model lineage)'에서 파생되었는지 여부가 RL 훈련 성공의 핵심 요소라는 것입니다. 즉, RM 벤치마크에서 높은 점수는 RL 훈련에서 높은 다운스트림 성능을 위한 필요 조건이지만, RM 품질이 특정 수준을 넘으면 다운스트림 성능은 빠르게 포화됩니다. 또한, RM과 정책 모델의 기반 모델 간의 불일치(misalignment)는 성능 저하로 이어질 수 있습니다.

이러한 결과는 RM 평가 벤치마크를 사용할 때 신중해야 함을 강조합니다.

"이러한 발견은 보상 모델(reward model) 평가 벤치마크(benchmarks)를 사용할 때 주의를 요합니다. 벤치마크(benchmark)는 Best-of-N 샘플링(sampling)과 같은 일부 설정에서 기성 보상 모델(reward model)을 선택하는 데 가이드로 사용될 수 있지만... PPO와 같은 정책 기울기 알고리즘(policy-gradient algorithms)의 경우, 벤치마크(benchmark) 결과는 자신의 훈련 설정(training setup) 맥락에서 고려되어야 합니다. 단순히 RewardBench 2에서 최고 모델을 선택하는 대신, 우리는 그 모델의 레시피를 가져와 체크포인트(checkpoint) 자체보다는 특정 워크플로우(workflow)에 통합해야 함을 보여줍니다." - [2]에서

결론적으로, RewardBench 2는 RM 평가의 복잡성을 해결하고, RM의 실제적인 유용성을 더 정확하게 예측하기 위한 중요한 발걸음을 내디뎠습니다. 이는 RM 개발과 LLM 정렬 연구의 미래 방향을 제시하는 데 기여할 것입니다.

**결론**

보상 모델(RM)은 대규모 언어 모델(LLM)의 성공적인 정렬(alignment)과 발전에 있어 핵심적인 역할을 수행하는 강력하고 유연한 도구입니다. 이 글을 통해 우리는 RM의 기본 개념, 브래들리-테리 선호도 모델(Bradley-Terry model of preference)과 같은 통계적 기반, 그리고 실제 구현 및 훈련 과정에 대해 깊이 있게 살펴보았습니다. 또한, 분류기 기반 RM부터 LLM-as-a-Judge, ORM, PRM에 이르는 다양한 유형의 RM과, RLHF, RLVR, DPO와 같은 다양한 정렬 전략에서 RM이 어떻게 활용되는지 이해했습니다.

RM을 효과적으로 구축하고 활용하는 것은 단순히 기술적인 문제를 넘어, 데이터 품질, 기반 모델의 선택, 그리고 특정 애플리케이션의 요구 사항 등 복합적인 실제적 고려 사항을 필요로 합니다. Best-of-N 샘플링을 위한 RM과 RL 미세 조정을 위한 RM은 다른 최적화 목표와 평가 기준을 가질 수 있습니다. RewardBench 및 RewardBench 2와 같은 벤치마크는 RM의 성능을 체계적으로 평가하고, 고품질 RM을 개발하기 위한 모범 사례를 제시하는 데 중요한 기여를 했습니다. 특히, RM의 크기, 데이터 품질, 그리고 정책 모델과의 계보 일치(lineage alignment)가 다운스트림 LLM 성능에 미치는 영향에 대한 통찰은 RM 연구의 중요한 진전을 의미합니다.

LLM을 위한 대규모 강화 학습 훈련에 대한 관심이 계속 증가함에 따라, RM에 대한 연구는 앞으로도 빠르게 발전할 것입니다. 보상 해킹(reward hacking) 문제 해결, 멀티모달 RM의 발전, 그리고 더욱 효율적이고 견고한 RM 평가 방법론 개발은 향후 연구의 주요 방향이 될 것입니다. RM은 LLM이 인간의 가치와 의도를 더 잘 이해하고 반영하여, 더욱 유용하고 안전하며 신뢰할 수 있는 AI 시스템을 구축하는 데 중추적인 역할을 계속할 것입니다.

**뉴스레터가 처음이신가요?**
안녕하세요! 저는 Cameron R. Wolfe입니다. 딥러닝(Deep Learning) 박사이자 넷플릭스(Netflix)의 선임 연구 과학자입니다. 이것은 독자들이 AI 연구의 중요한 주제를 더 잘 이해하도록 돕는 Deep (Learning) Focus 뉴스레터입니다. 뉴스레터는 항상 무료이며 공개적으로 읽을 수 있습니다. 뉴스레터가 마음에 드시면 구독하거나, 유료 구독을 고려하거나, 공유하거나, X 및 LinkedIn에서 저를 팔로우해주세요! 구독하기

**참고문헌(Bibliography)**
[1] Lambert, Nathan, et al. "Rewardbench: Evaluating reward models for language modeling." arXiv preprint arXiv:2403.13787 (2024).
[2] Malik, Saumya, et al. "RewardBench 2: Advancing Reward Model Evaluation." arXiv preprint arXiv:2506.01937 (2025).
[3] Ouyang, Long, et al. "Training language models to follow instructions with human feedback." Advances in neural information processing systems 35 (2022): 27730-27744.
[4] Lambert, Nathan, et al. "T\" ulu 3: Pushing frontiers in open language model post-training." arXiv preprint arXiv:2411.15124 (2024).
[5] OpenAI et al. “Learning to Reason with LLMs.” https://openai.com/index/learning-to-reason-with-llms/ (2024).
[6] Rafailov, Rafael, et al. "Direct preference optimization: Your language model is secretly a reward model." Advances in Neural Information Processing Systems 36 (2023): 53728-53741.
[7] Guo, Daya, et al. "Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning." arXiv preprint arXiv:2501.12948 (2025).
[8] Bai, Yuntao, et al. "Training a helpful and harmless assistant with reinforcement learning from human feedback." arXiv preprint arXiv:2204.05862 (2022).
[9] Zheng, Lianmin, et al. "Judging llm-as-a-judge with mt-bench and chatbot arena." Advances in Neural Information Processing Systems 36 (2023): 46595-46623.
[10] Bai, Yuntao, et al. "Constitutional ai: Harmlessness from ai feedback." arXiv preprint arXiv:2212.08073 (2022).
[11] Zheng, Lianmin, et al. "Judging llm-as-a-judge with mt-bench and chatbot arena." Advances in Neural Information Processing Systems 36 (2023): 46595-46623.
[12] Cobbe, Karl, et al. "Training verifiers to solve math word problems." arXiv preprint arXiv:2110.14168 (2021).
[13] Ivison, Hamish, et al. "Unpacking dpo and ppo: Disentangling best practices for learning from preference feedback." Advances in neural information processing systems 37 (2024): 36602-36633.
[14] Stiennon, Nisan, et al. "Learning to summarize with human feedback." Advances in neural information processing systems 33 (2020): 3008-3021.

1.  때로는 프롬프트(prompt)당 두 개 이상의 후보 완성(candidate completions)이 있을 수 있습니다. 이 경우, 선호도는 완성(completion)의 선호도 측면에서 순위를 매겨 포착됩니다. 그러나 최근 연구에서는 이진 선호도 데이터(binary preference data)가 더 일반적으로 사용됩니다.
2.  여기서 우리는 현재 훈련 중인 LLM을 지칭하기 위해 정책(policy)이라는 용어를 사용합니다. 이는 강화 학습(reinforcement learning)에서 사용되는 표준 용어(terminology)입니다. 여기를 참조하십시오.
3.  실제로 이 시퀀스(sequences)는 모든 선택된 시퀀스(chosen sequences) 및 거부된 시퀀스(rejected sequences)에 대한 프롬프트(prompt)와 완성(completion) 모두가 될 것입니다. 여기서는 단순화를 위해 명확한 프롬프트(prompt) 또는 완성(completion) 구조가 없는 평면 텍스트 시퀀스(textual sequences)만 있습니다.
4.  그러나 LLM 훈련(training)에 사용되는 정책 기울기 알고리즘(policy gradient algorithms)에는 PPO, REINFORCE, GRPO 등 다양한 변형이 있으며, 각각의 이점(benefits)이 있습니다.
5.  작성 시점에 Llama-3는 사용 가능한 최고의 오픈소스 모델(open-source model)이었습니다.
6.  이 벤치마크(benchmark)에는 데이터, 리더보드(leaderboard) 및 광범위한 기술 보고서(technical report)가 함께 제공됩니다!
7.  데이터 오염(Data contamination)은 훈련 세트(training set)에 데이터가 존재하여 나중에 동일한 모델을 평가하는 데 사용될 수 있다는 아이디어를 의미합니다. 상관관계가 합법적인지 확인하려면 데이터를 오염 제거하고 유출을 방지해야 합니다.
8.  성능은 동점(ties)을 제외한 모든 도메인(domains)에서 정확도(accuracy)로 측정되며, 동점(ties)에서는 올바른 예시와 잘못된 예시 사이의 올바른 마진(margin)을 확인합니다.
9.  이는 [13]의 결과와 일치하며, 다양한 강점의 RM이 RL 훈련(training)에 사용될 때 모두 비교적 잘 수행됨을 알 수 있습니다.