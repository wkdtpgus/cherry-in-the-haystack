# 대규모 언어 모델(LLMs)의 스케일링 법칙(Scaling Laws): GPT-3부터 o3까지

Author: Cameron Wolfe
URL: https://cameronrwolfe.substack.com/p/llm-scaling-laws

============================================================

(출처: [1, 7, 10, 21]) 최근 인공지능(AI) 연구, 특히 대규모 언어 모델(LLM) 분야의 발전 대부분은 스케일(scale)에 의해 주도되어 왔습니다. 더 많은 데이터로 더 큰 모델을 훈련하면 더 나은 결과를 얻을 수 있습니다. 이러한 관계는 스케일링 법칙(scaling law)을 통해 더 엄밀하게 정의될 수 있습니다. 스케일링 법칙은 우리가 관심 있는 특정 양(예: 훈련 연산량(training compute))을 증가시킬 때 LLM의 테스트 손실(test loss)이 어떻게 감소하는지를 설명하는 방정식입니다. 스케일링 법칙은 더 크고 비용이 많이 드는 훈련 실행(training runs)의 결과를 예측하는 데 도움을 주어, 스케일(scale)에 계속 투자할 수 있는 필요한 확신을 제공합니다.

"대규모 데이터셋이 있고 매우 큰 신경망(neural network)을 훈련한다면, 성공은 보장됩니다!" - 일리야 수츠케버(Ilya Sutskever)

수년 동안 스케일링 법칙은 AI 연구의 예측 가능한 북극성(North Star)이었습니다. 실제로 OpenAI와 같은 초기 선도 연구소(frontier labs)의 성공은 스케일링 법칙에 대한 그들의 종교적인 수준의 믿음 덕분이라고 평가되기도 합니다. 그러나 최근 최고 연구소들이 차세대 더 나은 LLM을 만드는 데 어려움을 겪고 있다는 보고서 1 가 나오면서 스케일링의 지속 가능성에 의문이 제기되었습니다. 이러한 주장들은 우리에게 다음과 같은 의문을 갖게 합니다: 스케일링이 한계에 부딪힐 것인가, 그리고 만약 그렇다면 다른 발전 방향이 있는가?

이 개요는 LLM 스케일링 법칙과 관련 연구에 대한 심층적인 설명부터 시작하여 이러한 질문들에 대해 근본적으로 답변할 것입니다. 스케일링 법칙의 아이디어는 간단하지만, 스케일링에 대한 대중의 오해가 다양하게 존재합니다. 이 연구의 과학적 배경은 실제로는 매우 구체적입니다. 스케일링에 대한 이러한 상세한 이해를 바탕으로, 우리는 LLM 연구의 최근 동향과 스케일링 법칙의 "정체(plateau)"에 기여하는 요인들을 논의할 것입니다. 마지막으로, 우리는 이 정보를 사용하여 스케일링을 포함하여 발전을 계속 이끌어낼 수 있는 몇 가지 핵심 아이디어에 초점을 맞춰 AI 연구의 미래를 더 명확하게 설명할 것입니다.

### LLM을 위한 근본적인 스케일링 개념

LLM 스케일링의 현황을 이해하기 위해, 우리는 먼저 스케일링 법칙에 대한 일반적인 이해를 구축해야 합니다. 우리는 거듭제곱 법칙(power law)의 개념부터 시작하여 이 이해를 근본적으로 구축할 것입니다. 그다음, 거듭제곱 법칙이 LLM 연구에서 어떻게 적용되어 오늘날 우리가 사용하는 스케일링 법칙을 도출했는지 탐구할 것입니다.

#### 거듭제곱 법칙(power law)이란 무엇인가?

거듭제곱 법칙은 LLM 스케일링의 근간을 이루는 근본적인 개념입니다. 간단히 말해, 거듭제곱 법칙은 두 양 사이의 관계를 설명합니다. LLM의 경우, 이 양들 중 첫 번째는 LLM의 테스트 손실(test loss) 또는 다른 관련 성능 지표(performance metric)(예: 다운스트림 태스크 정확도(downstream task accuracy) [7])이며, 다른 하나는 모델 매개변수(model parameters)의 수와 같이 우리가 스케일링하려는 설정입니다. 예를 들어, LLM의 스케일링 속성을 연구할 때 다음과 같은 진술을 볼 수 있습니다.

"충분한 훈련 데이터가 있다면, 검증 손실(validation loss)의 스케일링은 모델 크기(model size)의 함수로서 대략적으로 부드러운 거듭제곱 법칙을 따라야 합니다." - 출처: [4]

이러한 진술은 모델의 테스트 손실과 총 모델 매개변수 수 사이에 측정 가능한 관계가 존재함을 알려줍니다. 이러한 양 중 하나가 변경되면 다른 양에서 상대적이고 스케일 불변(scale-invariant)적인 변화가 발생합니다. 다시 말해, 이 관계를 통해 총 모델 매개변수 수를 증가시키면(다른 조건들, 예를 들어 충분한 훈련 데이터가 충족된다고 가정할 때) 예측 가능한 요인에 의해 테스트 손실이 감소한다는 것을 알 수 있습니다.

**거듭제곱 법칙 공식화.** 기본적인 거듭제곱 법칙은 아래 방정식을 통해 표현됩니다. 여기서 연구되는 두 양은 x와 y이며, a와 p는 이 양들 사이의 관계를 설명하는 상수입니다. 이 거듭제곱 법칙 함수 2를 그래프로 그리면 아래 그림과 같습니다. LLM 스케일링을 연구하는 대부분의 논문이 로그 스케일(log scale)을 사용하기 때문에, 우리는 일반 스케일과 로그 스케일 모두로 플롯을 제공합니다.

**Plot of a basic power law between x and y**

그러나 LLM 스케일링을 위해 제공되는 플롯은 위 그림과 같지 않습니다. 일반적으로 거꾸로 뒤집혀 있습니다. 아래 예시를 참조하십시오.

(출처: [1])

이것은 역 거듭제곱 법칙(inverse power law)으로, 아래와 같이 공식화될 수 있습니다. 역 거듭제곱 법칙의 방정식은 표준 거듭제곱 법칙과 거의 동일하지만, p에 음의 지수(negative exponent)를 사용합니다. 거듭제곱 법칙의 지수를 음수로 만들면 플롯이 거꾸로 뒤집힙니다. 아래 예시를 참조하십시오.

**Plot of an inverse power law between x and y**

이 역 거듭제곱 법칙은 로그 스케일을 사용하여 플롯할 때 대부분의 LLM 스케일링 법칙의 특징인 선형 관계(linear relationship)를 나타냅니다. 이 개요에서 다루는 거의 모든 논문은 다양한 요인(예: 크기, 연산량(compute), 데이터 등)을 스케일업(scaling up)하는 것이 LLM의 성능에 어떻게 영향을 미치는지 연구하기 위해 이러한 플롯을 생성할 것입니다.

이제 LLM 스케일링 [1]의 맥락에서 거듭제곱 법칙을 연구한 최초의 논문 중 하나를 통해 거듭제곱 법칙을 더 실용적으로 살펴보겠습니다.

### 신경 언어 모델을 위한 스케일링 법칙 [1]

초기 언어 모델 시대에는 스케일이 성능에 미치는 영향을 아직 이해하지 못했습니다. 언어 모델은 유망한 연구 분야였지만, 당시의 현재 세대 모델(예: 오리지널 GPT)은 제한된 기능을 가지고 있었습니다. 우리는 더 큰 모델의 힘을 아직 발견하지 못했고, 더 나은 언어 모델을 만드는 길은 즉시 명확하지 않았습니다.

*   모델의 형태(즉, 레이어의 수와 크기)가 중요한가?
*   모델을 더 크게 만드는 것이 성능 향상에 도움이 되는가?
*   이러한 더 큰 모델을 훈련하는 데 얼마나 많은 데이터가 필요한가?

"손실은 모델 크기, 데이터셋 크기, 훈련에 사용된 연산량(compute)과 함께 거듭제곱 법칙을 따르며, 일부 경향은 7자릿수 이상에 걸쳐 나타납니다." - 출처: [1]

[1]에서 저자들은 모델 크기, 모델 형태, 데이터셋 크기, 훈련 연산량(training compute), 배치 크기(batch size)와 같은 여러 요인이 모델 성능에 미치는 영향을 분석하여 이러한 질문에 답하는 것을 목표로 합니다. 이 분석을 통해 우리는 다음을 증가시킬 때 LLM 성능이 부드럽게 향상된다는 것을 알 수 있습니다.

*   모델 매개변수(model parameters)의 수.
*   데이터셋의 크기.
*   훈련에 사용되는 연산량(compute).

더 구체적으로, 성능이 다른 두 요인에 의해 병목 현상(bottlenecked)을 겪지 않을 때, 이러한 각 요인과 LLM의 테스트 손실 사이에 거듭제곱 법칙 관계가 관찰됩니다.

**실험 설정.** 저자들은 자신들의 거듭제곱 법칙을 맞추기 위해, 2,200만 개에서 230억 개에 이르는 토큰(token)을 포함하는 WebText2 코퍼스(corpus)의 하위 집합을 사용하여 최대 15억(1.5B) 매개변수(parameters) 크기의 LLM을 사전 훈련(pretrain)했습니다. 모든 모델은 1,024 토큰의 고정된 컨텍스트 길이(context length)와 표준 다음 토큰 예측(next token prediction)(교차 엔트로피(cross-entropy)) 손실을 사용하여 훈련됩니다. 동일한 손실이 홀드아웃 테스트 세트(hold-out test set)에서 측정되며 우리의 주요 성능 지표(performance metric)로 사용됩니다. 이 설정은 대부분의 LLM에 대한 표준 사전 훈련(pretraining) 설정과 일치합니다.

(출처: [1])

**LLM 스케일링을 위한 거듭제곱 법칙.** [1]에서 훈련된 LLM의 성능(WebText2에서의 테스트 손실 기준)은 더 많은 매개변수, 데이터 및 연산량(compute) 3과 함께 꾸준히 향상되는 것으로 나타났습니다. 이러한 경향은 연산량에서 8자릿수(orders of magnitude), 모델 크기에서 6자릿수, 데이터셋 크기에서 2자릿수에 걸쳐 나타납니다. 정확한 거듭제곱 법칙 관계와 각각에 맞춰진 방정식은 위 그림에 제공되어 있습니다. 여기서 각 방정식은 우리가 이전에 보았던 역 거듭제곱 법칙 방정식과 매우 유사합니다. 그러나 우리는 a = 1로 설정하고 괄호 안에 추가적인 곱셈 상수 4를 더합니다.

[1]의 저자들은 이러한 거듭제곱 법칙을 적절하게 맞추는 데 필요한 한 가지 작은 세부 사항을 언급합니다. 총 모델 매개변수 수를 계산할 때 위치 임베딩(positional embeddings)이나 토큰 임베딩(token embeddings)을 포함하지 않는데, 이는 더 깔끔한 스케일링 경향을 보여줍니다. 아래를 참조하십시오.

(출처: [1])

이러한 거듭제곱 법칙은 훈련이 다른 요인들에 의해 병목 현상(bottlenecked)을 겪지 않을 때만 적용 가능합니다. 따라서 최적의 성능을 위해서는 모델 크기, 데이터, 연산량(compute)이라는 이 세 가지 구성 요소가 동시에 스케일업(scaled up)되어야 합니다. 이러한 구성 요소 중 어느 하나라도 개별적으로 스케일업하면 수확 체감(diminishing returns) 지점에 도달할 것입니다.

#### 거듭제곱 법칙은 우리에게 무엇을 알려주는가?

[1]에서 제공된 거듭제곱 법칙 플롯은 상당히 유망해 보이지만, 이 플롯들이 로그 스케일(log scale)을 사용하여 생성되었다는 점에 주목해야 합니다. 만약 우리가 일반 플롯(즉, 로그 스케일 없이)을 생성한다면, 아래 그림과 같은 결과를 얻게 되며, 여기서 거듭제곱 법칙의 형태가 지수적 감쇠(exponential decay)와 유사하다는 것을 알 수 있습니다.

**로그 스케일 없는 거듭제곱 법칙 플롯**

이러한 발견은 스케일링과 인공 일반 지능(AGI)에 대한 많은 온라인 수사(rhetoric)를 고려할 때 직관에 반하는 것처럼 보일 수 있습니다. 많은 경우, 우리가 접하는 직관은 LLM의 품질이 연산량(compute)의 로그 증가에 따라 기하급수적으로 향상된다는 것이지만, 이는 전혀 사실이 아닙니다. 실제로, LLM의 품질을 향상시키는 것은 스케일(scale)이 커질수록 기하급수적으로 더 어려워집니다.

(출처: [1])

**다른 유용한 발견들.** [1]에서 관찰된 거듭제곱 법칙 외에도, 모델 형태나 아키텍처 설정(architecture settings)과 같이 고려된 다른 요인들이 모델 성능에 미치는 영향은 미미하다는 것을 알 수 있습니다. 위를 참조하십시오. 스케일은 더 나은 LLM을 만드는 데 가장 큰 기여 요인입니다. 더 많은 데이터, 연산량(compute), 그리고 모델 매개변수(model parameters)는 LLM 성능의 부드러운 향상을 가져옵니다.

"더 큰 모델은 훨씬 더 샘플 효율적(sample-efficient)이므로, 최적으로 연산 효율적인 훈련은 상대적으로 적은 양의 데이터로 매우 큰 모델을 훈련하고 수렴(convergence)하기 훨씬 전에 중단하는 것을 포함합니다." - 출처: [1]

흥미롭게도, [1]의 실증적 분석은 더 큰 LLM이 더 샘플 효율적(sample efficient)인 경향이 있음을 나타냅니다. 이는 더 작은 모델에 비해 더 적은 데이터로 동일한 수준의 테스트 손실에 도달한다는 것을 의미합니다. 이러한 이유로, LLM을 수렴(convergence)까지 사전 훈련(pretraining)하는 것은 (논란의 여지가 있지만) 차선책(sub-optimal)입니다. 대신, 우리는 훨씬 더 큰 모델을 더 적은 데이터로 훈련하고, 수렴하기 훨씬 전에 훈련 과정을 중단할 수 있습니다. 이러한 접근 방식은 사용된 훈련 연산량(training compute) 측면에서는 최적이지만, 추론 비용(inference costs)은 고려하지 않습니다. 실용적으로 말하면, 우리는 일반적으로 더 많은 데이터로 더 작은 모델을 훈련합니다. 왜냐하면 더 작은 모델이 호스팅하기에 더 저렴하기 때문입니다.

저자들은 또한 모델 크기와 사전 훈련(pretraining)에 사용되는 데이터 양 사이의 관계를 광범위하게 분석하여, 데이터셋의 크기가 모델 크기만큼 빠르게 증가할 필요는 없다는 것을 발견했습니다. 모델 크기가 약 8배 증가하면 과적합(overfitting)을 피하기 위해 훈련 데이터 양이 약 5배 증가해야 합니다.

(출처: [1])

[1]에서 발견된 스케일링 법칙은 다른 여러 데이터셋에서도 재현되었으며, 여기서 테스트 손실에 고정된 오프셋(fixed offset)을 추가한 후에도 동일한 스케일링 법칙이 유지됨을 알 수 있습니다(즉, 데이터셋이 다르다는 사실을 설명하기 위해). 위를 참조하십시오. 이러한 결과는 LLM 스케일링에 대한 강력한 주장을 뒷받침합니다. 우리는 더 큰 모델을 더 오래, 더 많은 데이터로 훈련함으로써 매우 명확하고 측정 가능한 이점을 얻었으며, 이는 훨씬 더 큰 규모로 LLM을 사전 훈련하려는 욕구를 불러일으켰습니다.

"이러한 결과는 모델 크기, 데이터, 연산량(compute)을 적절하게 스케일업(scale up)할 때 언어 모델링 성능이 부드럽고 예측 가능하게 향상됨을 보여줍니다. 우리는 더 큰 언어 모델이 현재 모델보다 더 나은 성능을 보이고 더 샘플 효율적(sample efficient)일 것으로 예상합니다." - 출처: [1]

### 스케일링 법칙의 실제 활용

대규모 사전 훈련(pretraining)이 매우 유익하다는 사실은 우리에게 약간의 딜레마를 안겨줍니다. 최고의 결과는 방대한 양의 데이터로 거대한 모델(massive models)을 훈련함으로써 얻어집니다. 그러나 이러한 훈련 실행(training runs)은 엄청나게 비싸며, 이는 또한 많은 위험을 수반한다는 것을 의미합니다. 기대치를 충족하지 못하는 모델을 훈련하는 데 1천만 달러를 쓴다면 어떻게 될까요? 사전 훈련(pretraining)의 비용을 고려할 때, 우리는 모델별 튜닝(tuning)을 수행할 수 없으며, 훈련하는 모델이 잘 작동할 것이라고 확신해야 합니다. 우리는 너무 많은 돈을 들이지 않고 이러한 모델을 튜닝하고 성능을 예측하기 위한 전략을 개발해야 합니다.

(출처: [11])

이것이 바로 스케일링 법칙이 등장하는 지점입니다. 지금까지 우리는 스케일링 법칙이 존재함을 증명하기 위해 수행된 일부 실증적 분석을 보았지만, 이러한 스케일링 법칙은 AI 연구 내에서 매우 실용적인 사용 사례(use case)도 가지고 있습니다. 특히, 우리는 다음을 수행할 수 있습니다.

*   다양한 훈련 설정(training settings)을 사용하여 여러 개의 작은 모델을 훈련합니다.
*   더 작은 모델의 성능을 기반으로 스케일링 법칙을 맞춥니다.
*   스케일링 법칙을 사용하여 훨씬 더 큰 모델의 성능을 외삽(extrapolate)합니다.

물론, 이 접근 방식에는 한계가 있습니다. 더 작은 모델로부터 더 큰 모델의 성능을 예측하는 것은 어렵고 부정확할 수 있습니다. 모델은 스케일에 따라 다르게 작동할 수 있습니다. 그러나 이를 더 실현 가능하게 만들기 위해 다양한 접근 방식이 제안되었으며, 스케일링 법칙은 이제 이러한 목적으로 일반적으로 사용됩니다. 스케일링 법칙을 사용하여 더 큰 모델의 성능을 예측할 수 있는 능력은 연구자로서 우리에게 더 많은 확신(그리고 마음의 평화)을 줍니다. 또한, 스케일링 법칙은 AI 연구에 대한 투자를 정당화하는 간단한 방법을 제공합니다.

### 스케일링과 사전 훈련(Pretraining) 시대

"이것이 오늘날 우리가 보는 모든 발전의 원동력이었습니다. 엄청나게 큰 데이터셋으로 훈련된 비범하게 큰 신경망(neural networks) 말입니다." - 일리야 수츠케버(Ilya Sutskever)

스케일링 법칙의 발견은 LLM 연구의 최근 많은 발전을 촉진했습니다. 더 나은 결과를 얻기 위해, 우리는 점점 더 큰 모델을 더 크고(그리고 더 좋은!) 데이터셋으로 훈련합니다. 이 전략은 GPT 계보(lineage) 내의 여러 모델뿐만 아니라 OpenAI 외의 그룹에서 나온 대부분의 주목할 만한 모델을 만드는 데 사용되었습니다. 여기서는 일리야 수츠케버(Ilya Sutskever)가 최근 "사전 훈련(pretraining)의 시대" 5라고 묘사한 이 스케일링 연구의 진행 과정을 더 깊이 살펴보겠습니다.

#### GPT 계보: GPT [2], GPT-2 [3], GPT-3 [4], GPT-4 [5]

LLM 스케일링 법칙의 가장 널리 알려지고 가시적인 적용은 OpenAI의 GPT 계보 모델 생성에 있었습니다. 우리는 이 계보의 초기 모델, 즉 GPT-3까지를 주로 다룰 것입니다. 그 이유는 다음과 같습니다.

*   이 모델들의 세부 사항이 더 공개적으로 공유되었습니다.
*   후기 모델들은 사전 훈련(pretraining) 프로세스를 스케일업(scaling up)하는 것 외에도 사후 훈련(post-training) 연구의 발전으로부터 큰 이점을 얻었습니다.

우리는 또한 GPT-4와 같은 모델에서 알려진 일부 스케일링 결과도 다룰 것입니다.

(출처: [2])

원래 GPT 모델 [2]은 실제로는 상당히 작았습니다. 총 12개의 레이어와 1억 1,700만(117M) 개의 매개변수를 가졌습니다. 이 모델은 약 7,000권의 책의 원본 텍스트를 포함하는 데이터셋인 BooksCorpus를 통해 먼저 사전 훈련(pretrained)됩니다. 그런 다음, 우리는 지도 훈련 목표(supervised training objective)를 사용하고 각 태스크(task)에 대한 별도의 분류 헤드(classification head)를 생성하여 다양한 다운스트림 태스크(downstream tasks)를 해결하도록 모델을 미세 조정(finetune)합니다. 위를 참조하십시오. 이 논문은 디코더 전용 트랜스포머(decoder-only transformer)의 대규모 자기 지도 사전 훈련(self-supervised pretraining)을 수행한 최초의 논문 중 하나였으며, 이는 몇 가지 흥미로운 발견으로 이어졌습니다.

*   평면 텍스트에 대한 자기 지도 사전 훈련(self-supervised pretraining)은 엄청나게 효과적입니다.
*   사전 훈련(pretraining)에 길고 연속적인 텍스트 구간을 사용하는 것이 중요합니다.
*   이러한 방식으로 사전 훈련(pretrained)될 때, 단일 모델은 최첨단 정확도(state-of-the-art accuracy) 6로 매우 다양한 태스크를 해결하도록 미세 조정(finetuned)될 수 있습니다.

전반적으로 GPT는 특별히 주목할 만한 모델은 아니었지만, 훨씬 더 큰 규모로 유사한 모델을 탐구한 후속 작업을 위한 몇 가지 중요한 기반(즉, 디코더 전용 트랜스포머(decoder-only transformer) 및 자기 지도 사전 훈련(self-supervised pretraining))을 마련했습니다.

(출처: [3])

GPT-2 [3]은 GPT 직후에 제안되었으며, 최대 15억(1.5B) 매개변수(parameters) 크기의 여러 모델 컬렉션을 포함합니다. 위를 참조하십시오. 이 모델들은 GPT 모델과 동일한 아키텍처(architecture)를 공유하며, 동일한 자기 지도 언어 모델링 목표(self-supervised language modeling objective)를 사용하여 사전 훈련(pretrained)됩니다. 그러나 GPT-2는 GPT에 비해 사전 훈련(pretraining) 프로세스에 두 가지 큰 변화를 주었습니다.

*   모델은 WebText로 사전 훈련(pretrained)되는데, 이는 i) BooksCorpus보다 훨씬 크고 ii) 인터넷에서 데이터를 스크래핑(scraping)하여 생성되었습니다.
*   모델은 다운스트림 태스크(downstream tasks)에 대해 미세 조정(finetuned)되지 않습니다. 대신, 우리는 사전 훈련된 모델로 제로샷 추론(zero-shot inference) 7을 수행하여 태스크를 해결합니다.

GPT-2 모델은 대부분의 벤치마크(benchmark) 8에서 최첨단 성능(state-of-the-art performance)에는 미치지 못하지만, 모델 크기가 커질수록 성능이 꾸준히 향상됩니다. 모델 매개변수(model parameters) 수를 스케일업(scaling up)하는 것은 명확한 이점을 가져옵니다. 아래를 참조하십시오.

(출처: [3])

[3]의 저자들은 또한 GPT-2 모델이 인상적인 결과에도 불구하고 WebText 코퍼스(corpus)에 여전히 과소적합(underfit)하는 것으로 보인다고 밝혔습니다. 이러한 발견으로부터 우리는 모델 크기와 데이터 크기 모두에서 LLM 사전 훈련(pretraining)의 지속적인 스케일링이 유익할 것이라고 추론할 수 있습니다. GPT-2 모델이 특별히 강력하지는 않았지만, 이 모델들이 제시한 분석은 우리가 스케일링을 계속하고 궁극적으로 AI 연구의 변곡점(inflection point)에 도달하는 데 필요한 확신을 제공했습니다.

"충분한 용량을 가진 언어 모델은 자연어 시퀀스(natural language sequences)에 나타난 태스크를 추론하고 수행하는 방법을 배우기 시작하여, 그 획득 방법에 관계없이 더 잘 예측할 것입니다." - 출처: [3]

GPT-3 [4]는 LLM을 위한 대규모 사전 훈련(pretraining)의 이점을 명확하게 확인시켜 준 AI 연구의 분수령(watershed moment)이었습니다. 이 모델은 1,750억(175B) 개 이상의 매개변수를 가지고 있어, 가장 큰 GPT-2 모델보다 100배 이상 큽니다. 아래를 참조하십시오.

(출처: [4])

다시 말하지만, GPT-3는 이전 모델들과 상당히 유사한 디코더 전용 모델 아키텍처(decoder-only model architecture)를 사용하지만, CommonCrawl을 기반으로 하는 훨씬 더 큰 데이터셋으로 모델을 사전 훈련(pretrain)합니다. 이 데이터셋은 이전 WebText 데이터셋보다 약 10배 더 크며, [4]의 저자들은 더 큰 사전 훈련(pretraining) 데이터셋을 다른 여러 사전 훈련 데이터 소스와 결합하여 다양한 코퍼스(corpora)의 혼합을 생성했습니다. 아래를 참조하십시오.

(출처: [4])

GPT-3는 [4]에서 주로 퓨샷 학습(few-shot learning) 접근 방식을 사용하여 평가됩니다. 퓨샷 프롬프팅(few-shot prompting)(GPT-3 사용), 제로샷 프롬프팅(zero-shot prompting)(GPT-2 사용), 미세 조정(finetuning)(GPT 사용)의 차이점은 아래에 설명되어 있습니다.

(출처: [4])

퓨샷 학습(few-shot learning)은 LLM이 컨텍스트 윈도우(context window) 내에 배치된 예시를 기반으로 태스크를 수행하는 방법을 학습하는 새로운 패러다임입니다. [4]의 저자들은 이 개념을 "인컨텍스트 학습(in-context learning)"이라고 부릅니다. 이 경우, LLM은 실제로 "학습"하지 않습니다. 모델의 가중치(weights)는 전혀 업데이트되지 않습니다. 오히려 모델 입력의 예시들은 더 정확한 출력을 생성하기 위한 컨텍스트(context)로 사용됩니다. [4]에서 우리는 GPT-3가 매우 유능한 퓨샷 학습자(few-shot learner)임을 알 수 있으며, 이는 인컨텍스트 학습(in-context learning)이 더 큰 모델의 새로운(emergent) 9 능력임을 시사하는 것으로 보입니다. 아래를 참조하십시오.

(출처: [4])

GPT-3가 다양한 언어 이해 태스크(language understanding tasks)에서 평가될 때, 아래 그림에서 볼 수 있듯이 더 큰 모델을 사용하는 것이 퓨샷 학습(few-shot learning) 성능에 상당한 이점을 제공한다는 것을 알 수 있습니다. 더 큰 모델은 더 작은 모델에 비해 컨텍스트 윈도우(context windows) 내의 정보를 더 잘, 그리고 더 효율적으로 활용합니다. GPT-3는 퓨샷 학습(few-shot learning)을 통해 여러 태스크에서 최첨단 성능(state-of-the-art performance)을 능가할 수 있으며, 모델의 성능은 크기에 따라 부드럽게 향상됩니다.

(출처: [4])

단일 모델이 그렇게 많은 태스크에서 이 정도로 잘 수행할 수 있었다는 사실은 당시에는 엄청나게 인상적이었습니다. 이러한 각 태스크를 해결하는 데 미세 조정(finetuning)이나 기본 모델에 대한 변경이 필요하지 않았습니다. 우리는 단지 모델의 프롬프트(prompt)를 조정하기만 하면 되었습니다. GPT-3는 출시된 최초의 진정한 파운데이션 모델(foundation models) 중 하나였습니다. 이 모델은 AI 연구의 다음 시대를 열었으며, LLM과 상호작용하는 완전히 새롭고 직관적인 패러다임(즉, 프롬프팅(prompting))을 도입했습니다.

**GPT-3를 넘어서.** GPT-3의 인상적인 성능은 LLM 연구에 대한 폭발적인 관심을 불러일으켰으며, 주로 대규모 사전 훈련(pretraining)에 초점을 맞췄습니다. OpenAI가 출시한 다음 몇몇 모델들— InstructGPT [8], ChatGPT 및 GPT-4 [5] —은 대규모 사전 훈련(pretraining)과 새로운 사후 훈련 기술(post-training techniques)(즉, 지도 미세 조정(supervised finetuning) 및 인간 피드백 기반 강화 학습(reinforcement learning from human feedback))의 조합을 사용하여 LLM 품질을 획기적으로 향상시켰습니다. 이 모델들은 너무나 인상적이어서 AI 연구에 대한 대중의 관심이 급증하는 결과를 낳았습니다.

"GPT-4는 문서에서 다음 토큰(token)을 예측하도록 사전 훈련(pre-trained)된 트랜스포머 기반(Transformer-based) 모델입니다. 사후 훈련 정렬(post-training alignment) 프로세스는 사실성(factuality) 및 원하는 행동 준수(adherence to desired behavior) 측정에서 향상된 성능을 가져옵니다." - 출처: [5]

이 시기에 OpenAI는 연구에 대한 세부 정보를 덜 공개하기 시작했습니다. 대신, 새로운 모델들은 단순히 그들의 API를 통해 출시되었고, 이는 대중이 이 모델들이 어떻게 만들어졌는지 알 수 없게 만들었습니다. 다행히 OpenAI가 공개한 자료에서 몇 가지 유용한 정보를 얻을 수 있습니다. 예를 들어, ChatGPT의 전신인 InstructGPT [8]에는 모델의 사후 훈련(post training) 전략을 상세히 기록한 관련 논문이 있습니다. 아래를 참조하십시오. 이 논문이 GPT-3가 InstructGPT의 기본 모델이라고 명시하고 있다는 점을 고려할 때, 이 모델의 성능 향상은 사전 훈련(pretraining) 프로세스를 스케일업(scaling up)하는 것과는 대부분 관련이 없다고 합리적으로 추론할 수 있습니다.

(출처: [8])

ChatGPT와 비교하여 GPT-4는 기능 면에서 눈에 띄는 향상을 보였습니다. 그러나 연구자들은 GPT-4의 기술적 세부 사항을 거의 공유하지 않기로 결정했습니다. GPT-4 [5]의 기술 보고서(technical report)는 단순히 다음을 알려줍니다.

*   GPT-4는 트랜스포머 기반(transformer-based)입니다.
*   모델은 다음 토큰 예측(next token prediction)을 사용하여 사전 훈련(pretrained)됩니다.
*   공개 데이터와 라이선스된 제3자 데이터가 모두 사용됩니다.
*   모델은 인간 피드백 기반 강화 학습(reinforcement learning from human feedback)으로 미세 조정(finetuned)됩니다.

그럼에도 불구하고, 이 기술 보고서(technical report) 내에서 스케일링의 중요성은 매우 분명합니다. 저자들은 이 작업의 핵심 과제는 다양한 스케일(scales)에서 예측 가능하게 작동하는 확장 가능한 훈련 아키텍처(scalable training architecture)를 개발하는 것이었으며, 이를 통해 더 작은 실행(runs)의 결과를 외삽(extrapolate)하여 더 큰 규모(그리고 훨씬 더 비싼!) 훈련 작업에 대한 확신을 제공할 수 있었다고 언급합니다.

"적절하게 훈련된 대규모 언어 모델의 최종 손실은 … 모델 훈련에 사용된 연산량(compute)에 대한 거듭제곱 법칙(power laws)으로 근사화됩니다." - 출처: [5]

대규모 사전 훈련(pretraining)은 엄청나게 비싸기 때문에, 우리는 보통 한 번의 기회만 제대로 활용할 수 있습니다. 모델별 튜닝(model-specific tuning)을 위한 여지는 없습니다. 스케일링 법칙은 이 과정에서 핵심적인 역할을 합니다. 우리는 1,000~10,000배 적은 연산량(compute)을 사용하여 모델을 훈련하고, 이 훈련 실행(training runs)의 결과를 사용하여 거듭제곱 법칙(power laws)을 맞출 수 있습니다. 그런 다음, 이 거듭제곱 법칙을 사용하여 훨씬 더 큰 모델의 성능을 예측할 수 있습니다. 특히, [8]에서 GPT-4의 성능은 연산량(compute)과 테스트 손실(test loss) 사이의 관계를 측정하는 거듭제곱 법칙을 사용하여 예측된다는 것을 알 수 있습니다. 아래를 참조하십시오.

**GPT-4 훈련을 위한 스케일링 법칙 공식화**

(출처: [5])

이 표현은 우리가 이전에 보았던 것과 거의 동일해 보이지만, LLM의 테스트 손실(test loss)이 결코 0에 도달하지 않을 수 있다는 사실을 설명하기 위해 추가적인 기약 손실 항(irreducible loss term)이 있습니다. 일단 맞춰지면, 스케일링 법칙은 GPT-4의 최종 성능을 매우 높은 정확도로 예측하는 데 사용되었습니다. 아래 그림을 참조하십시오. 여기서 우리는 이 플롯이 로그 스케일(log scale)을 사용하여 생성되지 않았다는 점에 주목해야 하며, 연산량(compute)이 증가함에 따라 손실 개선이 분명히 감쇠하기 시작한다는 것을 알 수 있습니다!

(출처: [5])

[5]의 저자들은 또한 테스트 손실(test loss)이 쉽게 해석 가능한 지표(metric)가 아니라는 점을 지적하며, 다양한 다른 성능 지표(performance metrics)를 예측하려고 시도합니다. 예를 들어, HumanEval 코딩 벤치마크(coding benchmark)에서 LLM의 통과율(pass rate)을 예측하기 위해 스케일링 법칙이 맞춰집니다. 먼저, HumanEval의 문제들은 난이도에 따라 버킷(buckets)으로 나뉩니다. 그런 다음, LLM의 통과율(pass rate)을 예측하기 위해 스케일링 법칙이 맞춰집니다. [5]에서 우리는 1,000배 적은 연산량(compute)을 필요로 하는 실험을 기반으로 이 접근 방식을 사용하여 HumanEval에서 GPT-4의 통과율(pass rate)을 정확하게 예측할 수 있음을 알 수 있습니다. 아래를 참조하십시오.

(출처: [5])

보시다시피, 사전 훈련(pretraining) 프로세스를 스케일업(scaling up)하는 것은 가치 있는 일입니다. 그러나 대규모 사전 훈련(pretraining)은 또한 매우 비용이 많이 듭니다. 스케일링 법칙은 이 과정을 더 예측 가능하게 만들어 불필요하거나 과도한 연산 비용(compute costs)을 피할 수 있게 합니다.

### Chinchilla: 연산 최적(Compute-Optimal) 대규모 언어 모델 훈련 [5]

(출처: [9])

[1]에서 저자들은 LLM 사전 훈련(pretraining)을 스케일업(scaling up)할 때 데이터셋 크기보다 모델 크기를 더 빠르게 늘릴 것을 권장합니다. 그러나 GPT-3 이후의 대부분의 사전 훈련(pretraining) 연구는 우리가 그 반대로 해야 한다고 지적했습니다. 우리는 GPT-3보다 훨씬 큰 모델들— 예를 들어 5,300억(530B) 매개변수(parameter) MT-NLG [9] 모델 —을 훈련했지만, 이 모델들을 훈련하는 데 사용된 데이터셋의 크기는 GPT-3와 유사했습니다. 위를 참조하십시오. 이 모델들은 GPT-3의 성능을 향상시키지 못했지만, 더 많은 매개변수와 더 많은 데이터를 조합하여 사용한 모델들(예: Gopher [10])은 훨씬 더 나은 성능을 보였습니다. 아래를 참조하십시오.

(출처: [10])

**연산 최적(Compute-optimal) 스케일링 법칙.** 이러한 관찰에 영감을 받아, [6]의 저자들은 [1]에서 원래 제안되었던 스케일링 법칙에 대한 모범 사례를 완전히 재고했습니다. [6]의 스케일링 법칙 분석은 훨씬 더 큰 모델로 수행되었으며, 이전과는 약간 다른 결과를 산출했습니다. 더 구체적으로, 7천만(70M) 개에서 170억(17B) 개 매개변수(parameters)에 이르는 크기의 LLM이 1조(trillion) 개 이상의 토큰(tokens)을 포함하는 데이터셋으로 훈련됩니다. 아래를 참조하십시오.

(출처: [10])

모델 및 데이터 크기의 다양한 조합으로 LLM을 훈련함으로써, 우리는 이러한 요인들의 함수로서 LLM의 테스트 손실(test loss)을 예측하는 거듭제곱 법칙(power law)을 발견할 수 있습니다. 이러한 거듭제곱 법칙으로부터, 우리는 주어진 연산 예산(compute budget)에 대해 어떤 훈련 설정(training settings)이 가장 잘 작동하는지 결정할 수 있습니다. [6]의 저자들은 연산 최적 훈련(compute-optimal training) 10이 모델 및 데이터 크기를 비례적으로 스케일링(scale)해야 한다고 주장합니다. 이러한 발견은 대부분의 LLM이 그 크기에 비해 과소 훈련(undertrained)되었다는 것을 보여줍니다. 우리는 기존 LLM을 훨씬 더 많은 데이터로 훈련함으로써 이점을 얻을 수 있습니다. 예를 들어, [6]에서 맞춰진 스케일링 법칙은 Gopher가 20배 더 큰 데이터셋으로 훈련되었어야 한다고 예측합니다!

"필요할 것으로 예상되는 훈련 데이터의 양은 현재 대규모 모델을 훈련하는 데 사용되는 양을 훨씬 초과합니다." - 출처: [6]

**Chinchilla.** [6]에서 제공된 분석은 데이터 스케일(data scale)의 중요성을 강조합니다. 대규모 모델은 최고의 성능에 도달하기 위해 더 많은 데이터로 훈련되어야 합니다. 이러한 발견을 검증하기 위해, 저자들은 Chinchilla라고 불리는 700억(70B) 매개변수(parameter) LLM을 훈련합니다. 이전 모델들과 비교하여 Chinchilla는 더 작지만, 총 1.4조(1.4T) 개의 훈련 토큰(training tokens)을 포함하는 더 큰 사전 훈련(pretraining) 데이터셋을 가지고 있습니다. Chinchilla는 Gopher [10]와 동일한 데이터 및 평가 전략을 사용합니다. Gopher보다 4배 작음에도 불구하고, Chinchilla는 더 큰 모델보다 지속적으로 우수한 성능을 보입니다. 아래를 참조하십시오.

(출처: [6])

Chinchilla [6]가 제안한 스케일링 법칙은 그 후 수년 동안 AI 연구의 표준이 되었습니다. "Chinchilla-optimal"은 이제 일반적으로 사용되는 용어입니다. 오늘날에도 다양한 추가 스케일링 연구가 발표된 후에도 Chinchilla와 그 관련 스케일링 법칙은 끊임없이 참조됩니다.

### 스케일링 법칙의 "종말"

스케일링 법칙은 최근 AI 연구 내에서 인기 있는 (그리고 논쟁적인) 주제가 되었습니다. 이 개요에서 보았듯이, 스케일링은 사전 훈련(pretraining) 시대 내내 AI의 대부분의 발전을 촉진했습니다. 그러나 2024년 하반기에 모델 출시 및 개선 속도가 느려지면서 11, 모델 스케일링에 대한 광범위한 의문이 제기되기 시작했으며, 이는 AI 연구, 특히 스케일링 법칙이 한계에 부딪힐 수 있음을 시사하는 것으로 보였습니다.

*   로이터(Reuters)는 OpenAI가 현재 방법의 스케일링에서 정체(plateau)에 도달했기 때문에 제품 전략을 변경하고 있다고 주장합니다.
*   디 인포메이션(The Information)은 GPT 모델의 개선 속도가 느려지기 시작했다고 주장합니다.
*   블룸버그(Bloomberg)는 여러 선도 연구소(frontier labs)가 더 유능한 AI를 구축하려는 시도에서 직면하고 있는 어려움을 강조합니다.
*   테크크런치(TechCrunch)는 스케일링이 수확 체감(diminishing returns)을 보이기 시작했다고 주장합니다.
*   타임(Time)은 AI 연구가 둔화되고 있다는 서사(narrative)에 기여하는 다양한 요인들을 강조하는 미묘한 에세이를 발표합니다.
*   일리야 수츠케버(Ilya Sutskever)는 NeurIPS'24에서 그의 '테스트 오브 타임 어워드(test of time award)' 연설에서 "우리가 아는 사전 훈련(pretraining)은 끝날 것"이라고 말했습니다.

동시에 많은 전문가들은 반대 의견을 주장하고 있습니다. 예를 들어, 다리오 아모데이(Dario Amodei)(Anthropic CEO)는 스케일링이 "아마도… 계속될 것"이라고 말했고, 샘 알트만(Sam Altman)은 "벽은 없다"는 서사(narrative)를 계속 밀어붙였습니다. 이 섹션에서는 스케일링의 현재 상태와 존재할 수 있는 다양한 문제에 대한 근거 있는 설명을 제공하여 이 논의에 더 많은 색채를 더할 것입니다.

#### 느려지는 스케일링: 무엇을 의미하는가? 왜 일어나는가?

"두 가지 서사(narratives) 모두 사실일 수 있습니다: 스케일링은 기술적 수준에서는 여전히 작동하고 있습니다. 사용자들을 위한 개선 속도는 느려지고 있습니다." - 네이선 램버트(Nathan Lambert)

그렇다면… 스케일링이 느려지고 있는가? 답변은 복잡하며 "느려짐"에 대한 우리의 정확한 정의에 크게 의존합니다. 지금까지 이 질문에 대한 가장 합리적인 답변은 두 가지 답변 모두 옳다는 것입니다. 이러한 이유로, 우리는 이 질문에 답하려고 하지 않을 것입니다. 대신, 우리는 LLM 스케일링의 현재(및 미래) 상태에 대한 더 미묘한 이해를 구축할 수 있도록 연구가 이 주제에 대해 정확히 무엇을 말하는지 더 깊이 살펴볼 것입니다.

#### 스케일링 법칙은 우리에게 무엇을 알려주는가?

먼저, 스케일링 법칙의 기술적 정의를 상기해야 합니다. 스케일링 법칙은 훈련 연산량(training compute)(또는 모델/데이터셋 크기)과 LLM의 테스트 손실(test loss) 사이의 거듭제곱 법칙(power law)에 기반한 관계를 정의합니다. 그러나 이 관계의 본질은 종종 오해됩니다. 연산량(compute)의 로그 증가로부터 기하급수적인 성능 향상을 얻는다는 생각은 신화입니다. 스케일링 법칙은 지수적 감쇠(exponential decay)에 더 가깝게 보이며, 이는 더 이상의 성능 향상을 얻기 위해 시간이 지남에 따라 더 열심히 노력해야 한다는 것을 의미합니다. 아래를 참조하십시오.

(출처: [5])

다시 말해, 스케일링 법칙은 시간이 지남에 따라 자연스럽게 정체(plateau)됩니다. 이러한 방식으로, 우리가 현재 겪고 있는 "둔화"는 LLM 스케일링 법칙의 예상되는 부분이라고 주장할 수 있습니다.

"실무자들은 종종 모델 품질의 대리 지표(proxy)로 혼란도(perplexity) 평가 세트의 손실이 아닌 다운스트림 벤치마크(downstream benchmark) 정확도를 사용합니다." - 출처: [7]

**성능 정의.** LLM이 개선되고 있는지 여부를 어떻게 측정할까요? 스케일링 법칙의 관점에서, LLM 성능은 일반적으로 사전 훈련(pretraining) 중 모델의 테스트 손실(test loss)을 통해 측정되지만, 더 낮은 테스트 손실이 LLM의 능력에 미치는 영향은 불분명합니다. 손실이 낮아지면 다운스트림 태스크(downstream tasks)에서 더 높은 정확도로 이어질까요? 손실이 낮아지면 LLM이 새로운 능력을 습득하게 될까요? 여기에는 스케일링 법칙이 우리에게 알려주는 것과 우리가 실제로 중요하게 생각하는 것 사이에 불일치가 있습니다.

*   스케일링 법칙은 사전 훈련(pretraining)의 스케일(scale)을 증가시키면 LLM의 테스트 손실(test loss)이 부드럽게 감소할 것이라고 말합니다.
*   우리는 "더 나은" LLM을 얻는 것에 관심을 가집니다.

당신이 누구인지에 따라, 새로운 AI 시스템에 대한 당신의 기대치— 그리고 이 새로운 시스템을 평가하는 데 사용하는 접근 방식 —는 극적으로 달라질 것입니다. 일반적인 AI 사용자들은 일반 채팅 애플리케이션에 초점을 맞추는 경향이 있는 반면, 실무자들은 종종 다운스트림 태스크(downstream tasks)에서 LLM의 성능에 관심을 가집니다. 대조적으로, 최고 선도 연구소(frontier labs)의 연구자들은 AI 시스템에 대해 높은 (그리고 매우 특정한) 기대치를 가지고 있는 것으로 보입니다. 예를 들어, 박사 학위 논문 작성이나 고급 수학적 추론 문제 해결과 같은 것들입니다. LLM이 그렇게 광범위한 기능을 가지고 있다는 점을 고려할 때, 평가는 어렵고, LLM의 성능을 볼 수 있는 많은 관점(lenses)이 있습니다. 아래를 참조하십시오.

(출처: [15])

모델 기대치의 이러한 극심한 편차를 고려할 때, 스케일링이 "작동하고 있다"는 결정적인 증거를 제공하는 것은 항상 어려울 것입니다. 우리는 스케일링 법칙의 성공에 대한 더 구체적인 정의가 필요합니다. 과학이 더 큰 모델이 더 낮은 손실을 달성할 것이라고 말한다 해도, 이것이 새로운 모델이 모든 사람의 기대치를 충족시킬 것이라는 의미는 아닙니다. 인공 일반 지능(AGI)을 달성하지 못하거나 수상 경력이 있는 인간 수학자의 능력을 능가하지 못하는 것이 스케일링이 기술적 수준에서 여전히 작동하지 않는다는 증거는 아닙니다! 다르게 말하면, 스케일링의 "둔화"는 스케일링 법칙의 기술적 문제라기보다는 인식과 기대의 문제라고 주장할 수 있습니다.

**데이터 고갈(Data death).** LLM 사전 훈련(pretraining)을 스케일업(scale up)하려면 모델 크기와 데이터셋 크기를 모두 늘려야 합니다. 초기 연구 [1]는 데이터 양이 모델 크기만큼 중요하지 않다는 것을 시사하는 것처럼 보였지만, Chinchilla [6]를 통해 데이터셋 크기가 똑같이 중요하다는 것을 알 수 있습니다. 게다가, 더 최근의 연구는 대부분의 연구자들이 추론 비용(inference costs)을 절약하기 위해 모델을 "과도하게 훈련(overtrain)"하거나— Chinchilla-최적성(Chinchilla-optimality)을 넘어서는 크기의 데이터셋으로 사전 훈련(pretrain)하는 것을 선호한다고 주장합니다 [7].

"스케일링 연구는 일반적으로 연산 최적(compute-optimal) 훈련 체제에 초점을 맞춥니다... 더 큰 모델은 추론(inference) 시 더 비싸기 때문에, 이제 더 작은 모델을 과도하게 훈련(over-train)하는 것이 일반적인 관행입니다." - 출처: [7]

이 모든 연구는 우리를 하나의 간단한 결론으로 이끌어냅니다. LLM 사전 훈련(pretraining)을 스케일업(scaling up)하려면 더 큰 사전 훈련 데이터셋을 생성해야 한다는 것입니다. 이 사실은 LLM 스케일링 법칙에 대한 주요 비판 중 하나의 근거를 형성합니다. 많은 연구자들은 사전 훈련(pretraining) 프로세스를 계속 스케일링할 수 있을 만큼 충분한 데이터가 없을 수도 있다고 믿습니다. 맥락상, 현재 LLM에 사용되는 사전 훈련(pretraining) 데이터의 대다수는 웹 스크래핑(web scraping)을 통해 얻어집니다. 아래를 참조하십시오. 인터넷이 하나뿐이라는 점을 고려할 때, 대규모 고품질 사전 훈련(pretraining) 데이터의 완전히 새로운 소스를 찾는 것은 어려울 수 있습니다.

(출처)

일리야 수츠케버(Ilya Sutskever)조차 최근 이 주장을 펼쳤는데, i) 연산량(compute)은 빠르게 증가하고 있지만 ii) 웹 스크래핑(web scraping)에 의존하기 때문에 데이터는 증가하지 않고 있다고 주장했습니다. 따라서 그는 우리가 사전 훈련(pretraining) 프로세스를 영원히 스케일업(scaling up)할 수는 없다고 믿습니다. 우리가 아는 사전 훈련(pretraining)은 끝날 것이며, 우리는 AI 연구를 위한 새로운 발전 경로를 찾아야 합니다. 다시 말해, "우리는 데이터 정점(peak data)에 도달했다"는 것입니다.

### 사전 훈련(Pretraining)을 위한 차세대 스케일

스케일링은 결국 수확 체감(diminishing returns)으로 이어질 것이며, 스케일링 지속에 반대하는 데이터 중심적 주장은 타당하고 설득력이 있습니다. 그러나 사전 훈련(pretraining) 프로세스를 개선할 수 있는 몇 가지 연구 방향이 여전히 존재합니다.

**합성 데이터(Synthetic data).** 사전 훈련(pretraining) 프로세스를 여러 자릿수(orders of magnitude)만큼 스케일업(scale up)하려면, 우리는 합성 생성 데이터(synthetically-generated data)에 의존해야 할 가능성이 높습니다. 합성 데이터에 대한 과도한 의존이 다양성 문제(diversity issues) [14]로 이어질 것이라는 우려에도 불구하고, LLM을 위한 합성 데이터 [12]의 사용이 증가하고 있으며, 이는 성공적인 것으로 보입니다. 또한, 커리큘럼 학습(curriculum learning) [13] 및 지속적인 사전 훈련(continued pretraining) 전략은 사전 훈련 데이터에 대한 조정을 통해 다양한 의미 있는 개선을 가져왔습니다. 예를 들어, 데이터 혼합(data mixtures) 변경 또는 사전 훈련(pretraining) 끝에 지시 데이터(instruction data) 추가 등이 있습니다.

(출처: [7])

**실용적인 스케일링 법칙.** 최근 연구는 테스트 손실 기반 스케일링 법칙의 한계를 해결하려고 시도했습니다. 예를 들어, [7]의 저자들은 LLM 파운드리(LLM Foundry)의 다운스트림 벤치마크(downstream benchmarks)에서 LLM의 성능을 예측하는 데 사용될 수 있는 스케일링 법칙을 정의합니다. 위를 참조하십시오. 이러한 종류의 지표(metrics)를 해석하는 것은 인간에게 훨씬 쉽습니다. 테스트 손실(test loss)이 5% 감소하는 것이 무엇을 의미하는지는 알지 못할 수 있지만, 우리가 관심 있는 벤치마크(benchmark)에서 정확도가 85%에서 90%로 상승하는 것은 일반적으로 이해하기 쉽습니다. 다른 여러 연구들도 스케일링 법칙을 사용하여 LLM 성능에 대한 더 실용적이고 의미 있는 추정치를 제공하는 아이디어를 탐구했습니다. 예를 들어, 사후 훈련(post training) 및 양자화(quantization) [16] 후 또는 사전 훈련(pretraining) 과정 중 [17]에 대한 것입니다.

**DeepSeek-v3.** 최근의 주장에도 불구하고, 우리는 LLM 사전 훈련(pretraining) 프로세스를 스케일링(scaling)함으로써 여전히 반정기적인 발전을 목격하고 있습니다. 예를 들어, 6,710억(671B) 매개변수(parameter) 12 전문가 혼합(mixture-of-experts, MoE) 모델인 DeepSeek-v3 [18]이 최근 출시되었습니다. 오픈 소스(open-source)인 것 외에도, 이 모델은 14.8조(14.8T) 개의 텍스트 토큰(tokens)으로 사전 훈련(pretrained)되었으며 GPT-4o 및 Claude-3.5-Sonnet의 성능을 능가합니다. 모델의 성능은 아래를 참조하고, 라이선스는 여기를 참조하십시오. 참고로, LLaMA-3 모델은 15조(15T) 개 이상의 원본 텍스트 데이터로 훈련됩니다. 더 자세한 내용은 여기를 참조하십시오.

(출처: [18])

GPT-4o와 같은 모델을 능가하는 능력은 오픈 웨이트(open weights) LLM에게는 상당한 도약입니다. 가장 큰 LLaMA 모델조차 이 목표에 미치지 못했습니다. DeepSeek-v3는 다양한 흥미로운 기법을 채택합니다.

*   DeepSeek-v2의 최적화된 전문가 혼합(MoE) 아키텍처(architecture).
*   전문가 혼합(MoE)의 부하 분산(load balancing)을 위한 새로운 보조 손실 없는(auxiliary-loss-free) 전략.
*   다중 토큰 예측(multi-token prediction) 훈련 목표.
*   긴 연쇄 사고(long-chain-of-thought) 모델(즉, OpenAI의 o1과 유사)로부터 추론 능력(reasoning capabilities)의 증류(distillation).

이 모델은 또한 인간의 선호도에 맞추기 위해 지도 미세 조정(supervised finetuning) 및 인간 피드백 기반 강화 학습(reinforcement learning from human feedback)을 포함한 사후 훈련(post-training)을 거칩니다.

"우리는 14.8조(14.8T) 개의 고품질 및 다양한 토큰(tokens)으로 DeepSeek-V3를 훈련합니다. 사전 훈련(pre-training) 프로세스는 놀랍도록 안정적입니다. 전체 훈련 과정 동안, 우리는 복구 불가능한 손실 급증(loss spikes)을 겪거나 롤백(roll back)할 필요가 없었습니다." - 출처: [8]

그러나 DeepSeek-v3의 인상적인 성능의 가장 큰 핵심은 사전 훈련(pretraining) 스케일입니다. 이것은 동등하게 거대한 데이터셋으로 훈련된 거대한 모델입니다! 이러한 대규모 모델을 훈련하는 것은 다양한 이유(예: GPU 오류(GPU failures) 및 손실 급증(loss spikes))로 인해 어렵습니다. DeepSeek-v3는 놀랍도록 안정적인 사전 훈련(pretraining) 프로세스를 가지며, LLM 표준에 따르면 합리적인 비용으로 훈련됩니다. 아래를 참조하십시오. 이러한 결과는 대규모 사전 훈련(pretraining) 작업이 시간이 지남에 따라 더 관리하기 쉽고 효율적이 되고 있음을 나타냅니다.

(출처: [18])

**OOM(자릿수) 단위로 스케일 증가.** 우리의 스케일링 법칙을 계속 테스트하려면, 현재 모델보다 여러 자릿수(orders of magnitude) 더 큰 LLM을 훈련해야 합니다. 스케일링의 유용성에 대한 우리의 의견을 제쳐두고라도, 이 규모의 모델을 훈련하는 데 방해가 되는 다양한 한계가 여전히 존재합니다. 우리는 다음이 필요할 것입니다.

*   더 큰 연산 클러스터(compute clusters) 13.
*   더 많고 (더 좋은) 하드웨어(hardware).
*   막대한 양의 전력.
*   새로운 알고리즘(예: 여러 데이터 센터(data centers)에 걸쳐 있을 수 있는 대규모 분산 훈련(distributed training)용).

차세대 모델을 훈련하는 것은 단순히 더 많은 GPU를 위한 자금을 확보하는 문제가 아니라, 다학제적 공학적 위업(multi-disciplinary feat of engineering)입니다. 이러한 복잡한 노력은 시간이 걸립니다. 참고로, GPT-4는 GPT-3 출시 후 거의 3년— 특히 33개월 —만인 2023년 3월에 출시되었습니다. 또 다른 10-100배 스케일(scale) 증가를 달성하는 데 비슷한 기간(혹은 더 길 수도 있음)이 걸릴 것으로 예상하는 것이 합리적입니다.

"매 자릿수(order of magnitude) 스케일업(scale up)마다 다른 혁신을 찾아야 합니다." - 에게 에르딜(Ege Erdil) (Epoch AI)

### AI 연구의 미래

이제 사전 훈련(pretraining)을 위한 스케일링의 상태를 더 깊이 이해했으니, (순전히 논의를 위해) 사전 훈련 연구가 갑작스러운 한계에 부딪힐 것이라고 가정해 봅시다. 가까운 미래에 모델 기능이 전혀 향상되지 않더라도, AI 연구가 계속해서 빠르게 발전할 수 있는 다양한 방법이 있습니다. 우리는 이미 이러한 주제들 중 일부(예: 합성 데이터(synthetic data))에 대해 이야기했습니다. 이 섹션에서는 현재 인기 있는 두 가지 주제에 특히 초점을 맞출 것입니다.

*   LLM 시스템(LLM systems) / 에이전트(agents).
*   추론 모델(Reasoning models).

#### 유용한 LLM 시스템 구축

오늘날 대부분의 LLM 기반 애플리케이션은 단일 모델 패러다임(single-model paradigm)으로 작동합니다. 다시 말해, 우리는 단일 LLM에 태스크(task)를 전달하고 모델의 출력을 해당 태스크의 답변으로 직접 사용하여 태스크를 해결합니다. 아래를 참조하십시오. 이러한 시스템을 개선하고 싶다면(즉, 더 어려운 태스크를 더 높은 정확도로 해결하고 싶다면), 단순히 기본 모델의 기능을 향상시킬 수 있지만, 이러한 접근 방식은 더 유능한 모델의 생성에 의존합니다. 대신, 우리는 여러 LLM— 또는 다른 구성 요소 —를 결합하여 복잡한 태스크를 해결하는 LLM 기반 시스템을 구축함으로써 단일 모델 패러다임(single-model paradigm)을 넘어설 수 있습니다.

**LLM 시스템 기본 사항.** LLM 시스템의 목표는 복잡한 태스크를 LLM 또는 다른 모듈이 해결하기 더 쉬운 작은 부분으로 나누는 것입니다. 이 목표를 달성하기 위해 사용할 수 있는 두 가지 주요 전략이 있습니다(위에 묘사됨).

*   **태스크 분해(Task decomposition)**: 태스크 자체를 더 작은 하위 태스크(sub-tasks)로 나누어 개별적으로 해결하고 나중에 14 통합하여 최종 답변을 형성합니다.
*   **체이닝(Chaining)**: 단일 호출 대신 LLM에 여러 순차적 호출을 하여 태스크 또는 하위 태스크를 해결합니다.

이러러한 전략들은 단독으로 또는 함께 사용될 수 있습니다. 예를 들어, 책을 요약하는 시스템을 구축하고 싶다고 가정해 봅시다. 이를 위해, 우리는 먼저 책의 각 장을 요약함으로써 태스크를 분해할 수 있습니다. 여기서 우리는 다음 중 하나를 할 수 있습니다.

*   태스크를 더 작은 텍스트 덩어리로 더 분해하여 요약합니다(즉, 재귀적/계층적 분해(recursive / hierarchical decomposition)와 유사).
*   여러 LLM 호출을 함께 연결합니다. 예를 들어, 하나의 LLM이 장에서 모든 중요한 사실이나 정보를 추출하고, 다른 LLM이 이러한 핵심 사실을 기반으로 장 요약을 생성하도록 합니다.

그런 다음, LLM에게 연결된 장 요약들을 요약하도록 요청하여 전체 소설의 요약을 형성함으로써 이러한 결과들을 통합할 수 있습니다. 대부분의 복잡한 태스크는 해결하기 쉬운 간단한 부분으로 분해될 수 있으며, 이는 이러한 LLM 시스템을 매우 강력하게 만듭니다. 이러한 시스템은 더 광범위한 분해(decomposition)와 체이닝(chaining)을 수행함에 따라 엄청나게 정교해질 수 있으며, 이는 응용 AI 연구의 흥미롭고 (영향력 있는) 분야가 됩니다.

**LLM 기반 제품 구축.** LLM의 성공과 인기에도 불구하고, LLM의 실용적이고 (널리 채택된) 사용 사례(use cases)의 수는 여전히 매우 적습니다. 오늘날 LLM의 가장 큰 사용 사례(use cases)는 코드 생성(code generation)과 채팅이며, 둘 다 LLM의 비교적 명확한 응용 분야 15입니다. 아래를 참조하십시오.

(출처)

LLM 적용을 위한 많은 유망한 영역이 있다는 점을 고려할 때, 진정으로 유용한 LLM 기반 제품을 더 많이 구축하는 것은 응용 AI 연구의 중요한 분야입니다. 우리는 이미 매우 유능한 모델에 접근할 수 있지만, 이러한 모델을 사용하여 사용할 가치가 있는 제품을 구축하는 것은 완전히 다른 문제입니다. 이 문제를 해결하려면 신뢰할 수 있고 유능한 LLM 시스템을 구축하는 방법을 배워야 합니다.

(출처: [19])

**에이전트(Agents).** LLM 시스템과 에이전트(agents) 사이의 경계는 "에이전트"라는 용어가 AI 커뮤니티에 의해 과부하(overloaded)되었기 때문에 모호합니다. 그러나 우리가 이해해야 할 핵심 개념은 LLM 시스템이 다양하고 흥미롭고 의미 있는 방식으로 확장될 수 있다는 것입니다. 예를 들어, 문제를 해결할 때 도구(예: 계산기, 검색 엔진 등)를 사용하는 방법을 가르쳐 LLM을 증강(augment)할 수 있습니다. 위를 참조하십시오. 또한, LLM이 자체 프로그램을 실행하거나 심지어 우리를 위해 행동을 수행하도록 허용할 수 있습니다. 예를 들어, 호텔 예약이나 이메일 전송과 같은 것입니다. LLM과 통합될 수 있는 많은 모듈과 도구는 더 유능하고 유용한 LLM 시스템을 구축할 수 있는 무한한 가능성을 제시합니다.

강건성(Robustness)은 더 유능한 LLM/에이전트 시스템을 구축하는 데 가장 큰 장애물 중 하나입니다. 우리가 LLM에 10가지 다른 호출을 하는 LLM 시스템을 가지고 있다고 가정해 봅시다. 또한, 이러한 각 LLM 호출이 95%의 성공 확률을 가지며, 올바른 최종 출력을 생성하기 위해 모든 호출이 성공해야 한다고 가정해 봅시다. 이 시스템의 개별 구성 요소들이 합리적으로 정확하더라도, 전체 시스템의 성공률은 60%에 불과합니다!

(출처: [20])

더 많은 구성 요소를 추가할수록 이 문제는 기하급수적으로 악화되며, 이는 우리가 구축할 수 있는 LLM/에이전트 시스템의 복잡성을 제한합니다. 더 복잡한 시스템을 구축하려면 각 개별 시스템 구성 요소의 강건성(robustness)을 획기적으로 개선해야 합니다. 최근 연구는 강건성(robustness)이 스케일링(scaling)을 통해 개선될 수 있음을 나타냅니다. 그러나 우리는 더 나은 메타 생성 알고리즘(meta-generation algorithms)을 통해서도 강건성(robustness)을 개선할 수 있습니다. 위를 참조하십시오. LLM에서 단일 출력을 생성하는 대신, 이러한 알고리즘은 병렬 디코딩(parallel decoding), (단계별) 검증(verification), 비판(critiques) 등과 같은 아이디어를 사용하여 LLM으로부터 더 정제되고 정확한 출력을 얻습니다.

(출처: [20])

이 연구 분야는 빠르게 발전하고 있으며 AI 연구 발전의 핵심 동인이 될 가능성이 높습니다. 이 주제에 대한 심층 조사는 [20]을 참조하십시오. 메타 생성 알고리즘(meta-generation algorithms)이 개선됨에 따라 LLM은 더욱 강건해질 것이며, 우리는 점점 더 복잡한 LLM/에이전트 시스템을 구축할 수 있게 될 것입니다.

#### 추론 모델(Reasoning Models) 및 새로운 스케일링 패러다임

초기 LLM에 대한 일반적인 비판은 단순히 데이터를 암기하고 추론 능력(reasoning capabilities)이 거의 없다는 것이었습니다. 그러나 LLM의 추론 불능은 지난 몇 년 동안 대부분 반박되었습니다. 최근 연구를 통해 이러한 모델들이 처음부터 추론할 수 있는 내재적 능력을 가지고 있었지만, 이 능력을 이끌어내기 위해서는 올바른 프롬프팅(prompting) 또는 훈련 접근 방식을 사용해야 한다는 것을 알게 되었습니다. 연쇄 사고(Chain of thought, CoT) 프롬프팅(prompting) [22]은 LLM의 추론 능력(reasoning capabilities)을 입증한 최초의 기술 중 하나였습니다. 이 접근 방식은 간단하고 프롬프트 기반(prompt-based)입니다. 우리는 LLM에게 실제 응답을 생성하기 전에 응답에 대한 설명을 제공하도록 요청하기만 하면 됩니다. 더 자세한 내용은 여기를 참조하십시오. LLM이 응답에 도달하는 데 사용된 단계별 프로세스를 설명하는 근거(rationale)를 생성할 때, 그 추론 능력(reasoning capabilities)은 크게 향상됩니다. 게다가, 이 설명은 사람이 읽을 수 있으며 모델의 출력을 더 해석 가능하게 만들 수 있습니다!

(출처: [22])

연쇄 사고(chain of thought)의 아이디어는 일반적이면서도 강력합니다. 실제로, 연쇄 사고(chains of thought)는 LLM 추론 능력(reasoning capabilities)을 향상시키는 핵심 개념이 되었으며, 우리는 이 기술이 여러 방식으로 재활용되는 것을 보았습니다.

*   LLM-as-a-Judge 스타일 평가 모델은 일반적으로 최종 평가 결과를 생성하기 전에 점수 근거(scoring rationales)를 제공합니다 [23, 24].
*   더 작거나 공개된 LLM에게 더 나은 연쇄 사고(chains of thought)를 작성하도록 가르치기 위한 지도 미세 조정(supervised finetuning) 및 지시 튜닝(instruction tuning) 전략이 제안되었습니다 [25, 26].
*   LLM은 일반적으로 자신의 출력을 반성하고 비판하거나 검증한 다음, 이 정보를 기반으로 출력을 수정하도록 요청받습니다 [12, 27].

복잡한 추론(Complex reasoning)은 빠르게 발전하고 있는 활발한 연구 주제입니다. LLM이 추론 과정에 (단계별) 검증(verification) [28, 29]을 통합하도록 가르치는 새로운 훈련 알고리즘은 유망한 결과를 보여주었으며, 새롭고 더 나은 훈련 전략이 등장함에 따라 우리는 계속해서 개선을 볼 가능성이 높습니다.

OpenAI의 o1 추론 모델(reasoning model) [21]은 LLM의 추론 능력(reasoning capabilities)에서 상당한 도약을 의미합니다. o1이 사용하는 추론 전략은 연쇄 사고(chains of thought)에 크게 기반을 둡니다. 인간이 질문에 답하기 전에 생각하는 방식과 유사하게, o1은 응답을 제공하기 전에 "생각"하는 데 시간을 보낼 것입니다. 실용적으로 말하면, o1이 생성하는 "생각"은 모델이 문제를 심사숙고하고, 문제를 더 간단한 단계로 나누고, 문제를 해결하기 위한 다양한 접근 방식을 시도하며, 심지어 자신의 실수 16를 수정하는 데 사용하는 긴 연쇄 사고(chains of thought)일 뿐입니다.

"OpenAI o1은 복잡한 추론(complex reasoning)을 수행하도록 강화 학습(RL)으로 훈련된 새로운 대규모 언어 모델입니다. o1은 답변하기 전에 생각합니다. 사용자에게 응답하기 전에 긴 내부 연쇄 사고(chain of thought)를 생성할 수 있습니다." - 출처: [21]

o1의 정확한 훈련 전략에 대한 세부 사항은 공개적으로 공유되지 않습니다. 그러나 우리는 o1이 "매우 데이터 효율적인" "대규모 강화 학습(reinforcement learning)" 알고리즘을 사용하여 추론하도록 가르쳐졌으며, 유용한 연쇄 사고(chains of thought)를 생성하는 모델의 능력을 정제하는 데 초점을 맞춘다는 것을 알고 있습니다. OpenAI 연구자들의 공개적인 발언과 o1에 대한 최근 수사(rhetoric)를 바탕으로 볼 때, 이 모델은 순수한 강화 학습(pure reinforcement learning)을 사용하여 훈련된 것으로 보이며, 이는 o1이 추론 시간(inference time)에 어떤 형태의 트리 탐색(tree search)을 사용할 수 있다는 이전 의견과 모순됩니다.

**추론 중심 태스크(reasoning-heavy tasks)에서 GPT-4o와 o1의 비교**

(출처: [21])

언급했듯이, o1의 복잡한 추론 태스크(complex reasoning tasks) 성능은 인상적입니다. o1은 거의 모든 추론 중심 태스크(reasoning-heavy tasks)에서 GPT-4o를 능가합니다. 위를 참조하십시오. o1의 추론 능력(reasoning capabilities)의 예시로, 이 모델은 다음을 수행합니다.

*   코드포스(Codeforces)의 경쟁 프로그래밍 질문에서 89번째 백분위수에 위치합니다.
*   미국 수학 올림피아드(USA Math Olympiad, AIME) 예선에서 미국 상위 500명 학생에 도달합니다.
*   대학원 수준의 물리학, 생물학, 화학 문제(GPQA)에서 인간 박사 과정 학생들의 정확도를 능가합니다.

(출처: [22])

**o1에서 o3으로.** o1의 가장 흥미로운 측면 중 하나는 추론 시간(inference time)에 더 많은 연산량(compute)을 사용함으로써 모델의 추론 능력(reasoning capabilities)을 향상시킬 수 있다는 것입니다. 점점 더 복잡해지는 문제를 해결하기 위해, 모델은 단순히 점진적으로 더 긴 연쇄 사고(chains of thought)를 생성할 수 있습니다. 예시는 여기를 참조하십시오. 이러한 더 긴 연쇄 사고(chains of thought)를 생성하기 위해 더 많은 추론 시간 연산량(inference-time compute)을 사용하면 모델의 추론 성능이 부드럽게 증가합니다. 아래를 참조하십시오.

"우리는 o1의 성능이 더 많은 강화 학습(reinforcement learning)(훈련 시간 연산량(train-time compute))과 더 많은 사고 시간(test-time compute)을 통해 지속적으로 향상된다는 것을 발견했습니다." - 출처: [22]

마찬가지로, 위 그림에서 우리는 강화 학습(reinforcement learning)을 통해 훈련에 더 많은 연산량(compute)을 투자할수록 o1의 성능이 부드럽게 향상된다는 것을 알 수 있습니다. 이것이 바로 o3 추론 모델(reasoning model)을 만드는 데 사용된 접근 방식입니다. 이 모델에 대한 평가 결과는 2024년 말 OpenAI에 의해 미리 공개되었으며, o3에 대한 세부 정보는 거의 공개되지 않았습니다. 그러나 이 모델이 o1 출시 후 매우 빠르게(즉, 3개월 후) 출시되었다는 점을 고려할 때, o3는 강화 학습(reinforcement learning)에 더 많은 연산량(compute)을 투자한 o1의 "스케일업(scaled up)" 버전일 가능성이 높습니다.

(출처)

o3 모델은 이 글을 쓰는 시점에는 아직 출시되지 않았지만, o1을 스케일링하여 달성한 결과는 믿을 수 없을 정도로 인상적입니다(일부 경우에는 충격적이기까지 합니다). o3의 가장 주목할 만한 성과는 다음과 같습니다.

*   GPT-4o가 5%의 정확도만을 달성하는 ARC-AGI 벤치마크(benchmark)에서 87.5%의 점수. o3는 ARC-AGI에서 85%의 인간 수준 성능을 초과한 최초의 모델입니다. 이 벤치마크(benchmark)는 인공 일반 지능(AGI)을 향한 "북극성(North Star)"으로 묘사되어 왔으며 5년 이상 17 무패를 유지했습니다.
*   SWE-Bench Verified에서 71.7%의 정확도와 코드포스(Codeforces)에서 2727의 엘로 점수(Elo score)를 기록하여, o3를 전 세계 상위 200명의 인간 경쟁 프로그래머(competitive programmers) 중 하나로 만듭니다.
*   EpochAI의 FrontierMath 벤치마크(benchmark)에서 25.2%의 정확도를 기록하여, 이전 최첨단 정확도(state-of-the-art accuracy)인 2.0%를 향상시켰습니다. 이 벤치마크(benchmark)는 테렌스 타오(Terence Tao)에 의해 "믿을 수 없을 정도로 어렵고" AI 시스템에 의해 "최소 몇 년 동안" 해결되지 않을 가능성이 높다고 묘사되었습니다.
*   o3-mini라고 불리는 o3의 증류 버전(distilled version)도 미리 공개되었는데, 이는 매우 우수한 성능을 보이며 연산 효율성(compute efficiency)에서 상당한 개선을 가져옵니다.

(출처: [21] 및 여기)

**스케일링을 위한 새로운 패러다임.** 이 개요를 읽고 나면, o1과 o3가 제시한 많은 플롯(위 참조)이 꽤 친숙하게 느껴질 수 있습니다. 이들은 더 많은 연산량(compute)과 함께 성능이 부드럽고 선형적으로 증가하는 것을 보여주는 로그 스케일 플롯(log-scale plots)입니다! 다시 말해, 이러한 추론 모델(reasoning models)의 성능과 두 가지 다른 양 사이에 명확한 거듭제곱 법칙(power law) 관계가 있음을 알 수 있습니다.

*   훈련 시간(강화 학습) 연산량(Training-time (reinforcement learning) compute).
*   추론 시간 연산량(Inference-time compute).

o1 스타일 모델의 스케일링은 전통적인 스케일링 법칙과 다릅니다. 사전 훈련(pretraining) 프로세스를 스케일업(scaling up)하는 대신, 우리는 사후 훈련(post training) 및 추론(inference)에 투자되는 연산량(compute)을 스케일링하고 있습니다. 이것은 완전히 새로운 스케일링 패러다임(scaling paradigm)이며, 추론 모델(reasoning models)을 스케일링하여 달성된 결과는 지금까지 훌륭합니다. 이러한 발견은 사전 훈련(pretraining)을 넘어서는 다른 스케일링 경로가 분명히 존재한다는 것을 보여줍니다. 추론 모델(reasoning models)의 출현과 함께, 우리는 다음에 넘어야 할 언덕을 발견했습니다. 다른 형태로 나타날 수 있지만, 스케일링은 AI 연구의 발전을 계속 이끌 것입니다.

### 마무리 생각

이제 우리는 스케일링 법칙, LLM에 미치는 영향, 그리고 AI 연구 발전의 미래 방향에 대해 더 명확한 시각을 갖게 되었습니다. 우리가 배운 바와 같이, 스케일링 법칙에 대한 최근 비판에는 여러 기여 요인이 있습니다.

*   스케일링 법칙의 자연스러운 감쇠(decay).
*   LLM 능력에 대한 기대치의 높은 편차.
*   대규모, 학제 간 공학적 노력(inter-disciplinary engineering efforts)의 지연.

이러한 문제들은 타당하지만, 그 어느 것도 스케일링이 여전히 예상대로 작동하지 않는다는 것을 나타내지는 않습니다. 대규모 사전 훈련(pretraining)에 대한 투자는 계속될 것이며 (그래야만 하지만), 개선은 시간이 지남에 따라 기하급수적으로 더 어려워질 것입니다. 결과적으로, 대안적인 발전 방향(예: 에이전트(agents) 및 추론(reasoning))이 더욱 중요해질 것입니다. 그러나 우리가 이러한 새로운 연구 분야에 투자하더라도, 스케일링의 근본적인 아이디어는 계속해서 거대한 역할을 할 것입니다. 스케일링이 계속될 것인지는 질문이 아닙니다. 진정한 질문은 우리가 다음에 무엇을 스케일링할 것인가입니다.

뉴스레터가 처음이신가요? 안녕하세요! 저는 카메론 R. 울프(Cameron R. Wolfe)입니다. 넷플릭스(Netflix)의 딥러닝(Deep Learning) 박사이자 머신러닝 과학자(Machine Learning Scientist)입니다. 이것은 독자들이 AI 연구의 중요한 주제를 더 잘 이해하도록 돕는 딥 (러닝) 포커스(Deep (Learning) Focus) 뉴스레터입니다. 뉴스레터가 마음에 드신다면 구독, 공유하거나 X와 링크드인(LinkedIn)에서 저를 팔로우해주세요! 구독하기

### 참고문헌

[1] Kaplan, Jared, et al. "신경 언어 모델을 위한 스케일링 법칙(Scaling laws for neural language models)." arXiv preprint arXiv:2001.08361 (2020).
[2] Radford, Alec. "생성적 사전 훈련을 통한 언어 이해 향상(Improving language understanding by generative pre-training)." (2018).
[3] Radford, Alec, et al. "언어 모델은 비지도 다중 태스크 학습자(Language models are unsupervised multitask learners)." OpenAI 블로그 1.8 (2019): 9.
[4] Brown, Tom, et al. "언어 모델은 퓨샷 학습자(Language models are few-shot learners)." 신경 정보 처리 시스템 발전 33 (2020): 1877-1901.
[5] Achiam, Josh, et al. "GPT-4 기술 보고서(Gpt-4 technical report)." arXiv preprint arXiv:2303.08774 (2023).
[6] Hoffmann, Jordan, et al. "연산 최적 대규모 언어 모델 훈련(Training compute-optimal large language models)." arXiv preprint arXiv:2203.15556 (2022).
[7] Gadre, Samir Yitzhak, et al. "언어 모델은 과도한 훈련 및 다운스트림 태스크에서 안정적으로 스케일링된다(Language models scale reliably with over-training and on downstream tasks)." arXiv preprint arXiv:2403.08540 (2024).
[8] Ouyang, Long, et al. "인간 피드백을 통해 지시를 따르도록 언어 모델 훈련(Training language models to follow instructions with human feedback)." 신경 정보 처리 시스템 발전 35 (2022): 27730-27744.
[9] Smith, Shaden, et al. "Deepspeed와 Megatron을 사용하여 대규모 생성 언어 모델인 Megatron-Turing NLG 530B 훈련(Using deepspeed and megatron to train megatron-turing nlg 530b, a large-scale generative language model)." arXiv preprint arXiv:2201.11990 (2022).
[10] Rae, Jack W., et al. "언어 모델 스케일링: Gopher 훈련을 통한 방법, 분석 및 통찰(Scaling language models: Methods, analysis & insights from training gopher)." arXiv preprint arXiv:2112.11446 (2021).
[11] Bhagia, Akshita, et al. "연산 효율적인 모델 래더를 통한 태스크 스케일링 법칙 수립(Establishing Task Scaling Laws via Compute-Efficient Model Ladders)." arXiv preprint arXiv:2412.04403 (2024).
[12] Bai, Yuntao, et al. "헌법적 AI: AI 피드백으로부터의 무해성(Constitutional ai: Harmlessness from ai feedback)." arXiv preprint arXiv:2212.08073 (2022).
[13] Blakeney, Cody, et al. "당신의 데이터가 기쁨을 주는가? 훈련 종료 시 도메인 업샘플링을 통한 성능 향상(Does your data spark joy? Performance gains from domain upsampling at the end of training)." arXiv preprint arXiv:2406.03476 (2024).
[14] Chen, Hao, et al. "합성 데이터의 다양성과 대규모 언어 모델 훈련에 미치는 영향에 대하여(On the Diversity of Synthetic Data and its Impact on Training Large Language Models)." arXiv preprint arXiv:2410.15226 (2024).
[15] Guo, Zishan, et al. "대규모 언어 모델 평가: 종합 설문조사(Evaluating large language models: A comprehensive survey)." arXiv preprint arXiv:2310.19736 (2023).
[16] Xu, Zifei, et al. "사후 훈련 양자화된 대규모 언어 모델을 위한 스케일링 법칙(Scaling laws for post-training quantized large language models)." arXiv preprint arXiv:2410.12119 (2024).
[17] Xiong, Yizhe, et al. "대규모 언어 모델을 위한 시간적 스케일링 법칙(Temporal scaling law for large language models)." arXiv preprint arXiv:2404.17785 (2024).
[18] DeepSeek-AI et al. "DeepSeek-v3 기술 보고서(DeepSeek-v3 Technical Report)." https://github.com/deepseek-ai/DeepSeek-V3/blob/main/DeepSeek_V3.pdf (2024).
[19] Schick, Timo, et al. "Toolformer: 언어 모델은 도구 사용법을 스스로 배울 수 있다(Toolformer: Language models can teach themselves to use tools)." arXiv preprint arXiv:2302.04761 (2023).
[20] Welleck, Sean, et al. "디코딩에서 메타 생성까지: 대규모 언어 모델을 위한 추론 시간 알고리즘(From decoding to meta-generation: Inference-time algorithms for large language models)." arXiv preprint arXiv:2406.16838 (2024).
[21] OpenAI et al. “LLM으로 추론 학습하기(Learning to Reason with LLMs).” https://openai.com/index/learning-to-reason-with-llms/ (2024).
[22] Wei, Jason, et al. "연쇄 사고 프롬프팅은 대규모 언어 모델에서 추론을 유도한다(Chain-of-thought prompting elicits reasoning in large language models)." 신경 정보 처리 시스템 발전 35 (2022): 24824-24837.
[23] Liu, Yang, et al. "G-eval: GPT-4를 사용한 NLG 평가, 더 나은 인간 정렬(G-eval: Nlg evaluation using gpt-4 with better human alignment)." arXiv preprint arXiv:2303.16634 (2023).
[24] Kim, Seungone, et al. "프로메테우스: 언어 모델에서 미세 조정된 평가 능력 유도(Prometheus: Inducing fine-grained evaluation capability in language models)." 제12회 국제 학습 표현 컨퍼런스. 2023.
[25] Ho, Namgyu, Laura Schmid, and Se-Young Yun. "대규모 언어 모델은 추론 교사이다(Large language models are reasoning teachers)." arXiv preprint arXiv:2212.10071 (2022).
[26] Kim, Seungone, et al. "CoT 컬렉션: 연쇄 사고 미세 조정을 통한 언어 모델의 제로샷 및 퓨샷 학습 개선(The cot collection: Improving zero-shot and few-shot learning of language models via chain-of-thought fine-tuning)." arXiv preprint arXiv:2305.14045 (2023).
[27] Weng, Yixuan, et al. "대규모 언어 모델은 자기 검증을 통해 더 나은 추론자이다(Large language models are better reasoners with self-verification)." arXiv preprint arXiv:2212.09561 (2022).
[28] Lightman, Hunter, et al. "단계별로 검증하자(Let's verify step by step)." arXiv preprint arXiv:2305.20050 (2023).
[29] Zhang, Lunjun, et al. "생성적 검증자: 다음 토큰 예측으로서의 보상 모델링(Generative verifiers: Reward modeling as next-token prediction)." arXiv preprint arXiv:2408.15240 (2024).

1 두 가지 주요 보고서는 디 인포메이션(The Information)과 로이터(Reuters)에서 나왔습니다.
2 플롯을 생성하기 위해 다음 설정을 사용합니다: a = 1, p = 0.5, 그리고 0 < x < 1.
3 연산량(Compute)은 [1]에서 6NBS로 정의되며, 여기서 N은 모델 매개변수(model parameters)의 수, B는 훈련 중 사용되는 배치 크기(batch size), S는 총 훈련 단계(training steps) 수입니다.
4 이 추가적인 곱셈 상수는 거듭제곱 법칙(power law)의 동작을 변경하지 않습니다. 이것이 왜 그런지 이해하려면 스케일 불변성(scale invariance)의 정의를 이해해야 합니다. 거듭제곱 법칙은 스케일 불변(scale invariant)이므로, 특정 요인으로 스케일업(scale up)하거나 스케일다운(scale down)하더라도 거듭제곱 법칙의 근본적인 특성은 동일합니다. 관찰되는 동작은 어떤 스케일에서도 동일할 것입니다!
5 이 설명은 NeurIPS'24에서 이 논문에 대한 일리야의 '테스트 오브 타임 어워드(test of time award)'에서 나온 것입니다.
6 이것이 지금은 당연해 보일지라도, 당시 대부분의 자연어 처리(NLP) 태스크(예: 요약 및 질의응답(QA))에는 그들을 위한 전체 연구 분야가 할애되어 있었다는 점을 기억해야 합니다! 이러한 각 태스크에는 해당 태스크를 수행하는 데 특화된 태스크별 아키텍처(task-specific architectures)가 있었고, GPT는 여러 다른 태스크에서 이러한 아키텍처 대부분을 능가할 수 있는 단일 일반 모델이었습니다.
7 이는 우리가 LLM의 프롬프트(prompt)에 각 태스크를 설명하고 동일한 모델을 사용하여 다른 태스크를 해결한다는 의미입니다. 태스크 간에는 프롬프트(prompt)만 변경됩니다.
8 이 모델들은 제로샷 추론(zero-shot inference)을 사용하며 어떤 다운스트림 태스크(downstream tasks)에서도 전혀 미세 조정(finetuned)되지 않았기 때문에 예상되는 결과입니다.
9 "새로운(emergent)" 능력이라 함은 특정 스케일(scale)에 도달한 후에만 나타나는(예: 충분히 큰 모델) LLM이 가진 기술을 의미합니다.
10 여기서 우리는 "연산 최적(compute-optimal)"을 고정된 훈련 연산 비용(training compute cost)에서 테스트 손실(test loss) 측면에서 가능한 최고의 성능을 산출하는 훈련 설정(training setting)으로 정의합니다.
11 예를 들어, Anthropic은 Claude 3.5 Opus의 출시를 계속 연기했으며, Google은 Gemini-2의 플래시(flash) 변형만 출시했고, OpenAI는 2024년에 GPT-4o만 출시했습니다(o1과 o3가 12월에 출시되기 전까지). 이는 논란의 여지가 있지만 GPT-4보다 크게 더 유능하지는 않았습니다.
12 이 매개변수들 중 370억 개만이 단일 토큰(token)에 대한 추론(inference) 중에 활성화됩니다.
13 예를 들어, xAI는 최근 멤피스에 10만 개의 NVIDIA GPU를 갖춘 새로운 데이터센터(datacenter)를 구축했으며, Anthropic 경영진은 향후 몇 년 동안 연산 지출(compute spend)을 최대 100배 늘리고 싶다는 의사를 밝혔습니다.
14 통합 단계는 다양한 방식으로 구현될 수 있습니다. 예를 들어, 응답을 수동으로 통합하거나(예: 연결을 통해), LLM을 사용하거나, 그 사이의 거의 모든 것을 사용할 수 있습니다!
15 이는 이러한 태스크가 간단하기 때문이 아닙니다. 코드 생성(code generation)과 채팅(chat) 모두 해결하기 어렵지만, (논란의 여지가 있지만) LLM에 대한 상당히 명확한 응용 분야입니다.
16 OpenAI는 o1 사용자로부터 이러한 긴 연쇄 사고(chains of thought)를 숨기기로 결정했습니다. 이 선택 뒤에 있는 주장은 이러한 근거(rationales)가 모델의 사고 과정에 대한 통찰력을 제공하여 모델을 디버깅(debug)하거나 모니터링(monitor)하는 데 사용될 수 있다는 것입니다. 그러나 모델은 사용자 대면 모델 출력에 필요한 안전 필터링(safety filtering) 없이 순수한 생각을 표현할 수 있어야 합니다.
17 현재 ARC-AGI는 o3가 벤치마크(benchmark)의 연산량(compute) 요구 사항을 초과하기 때문에 기술적으로는 여전히 무패입니다. 그러나 이 모델은 더 낮은 연산량(compute) 설정에서도 75.7%의 정확도를 달성합니다.