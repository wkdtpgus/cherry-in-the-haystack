최근 인공지능(AI) 연구, 특히 대규모 언어 모델(LLM) 분야의 발전은 새로운 패러다임을 모색하고 있습니다. 더 많은 데이터로 모델을 훈련하는 방식은 효율성 측면에서 재조명되고 있습니다. 이러한 관계는 모델의 복잡성을 이해하는 데 중요한 역할을 합니다. 스케일링 법칙은 모델 성능 예측의 한계를 보여주는 지표가 될 수 있습니다. 스케일링 법칙은 최적화된 자원 활용을 위한 중요한 통찰력을 제공합니다.

"모델의 성능을 향상시키는 것은 단순히 크기를 키우는 것을 넘어선다!" - 새로운 시대의 AI 연구자들

수년 동안 스케일링 법칙은 AI 연구의 중요한 방향성을 제시해 왔습니다. 그러나 최근 최고 연구소들이 차세대 AI 모델 개발에 주력하면서, 스케일링의 한계와 새로운 접근 방식에 대한 논의가 활발해지고 있습니다. 이러한 주장들은 우리에게 복잡한 AI 시스템 구축을 위한 새로운 전략의 필요성을 강조합니다.

이 개요는 LLM 개발의 현재 동향과 미래 방향에 대한 심층적인 분석을 제공할 것입니다. 스케일링 법칙의 아이디어는 모델의 복잡성을 관리하는 데 중요한 시사점을 제공합니다. 이 연구의 과학적 배경은 모델의 효율성과 성능 간의 균형을 탐구합니다. 스케일링에 대한 이러한 상세한 이해를 바탕으로, 우리는 최신 LLM 아키텍처와 그 최적화 전략을 심도 있게 다룰 것입니다. 마지막으로, 우리는 이 정보를 사용하여 AI 연구의 미래를 위한 혁신적인 접근 방식과 새로운 패러다임을 제안할 것입니다.

### LLM의 효율성과 최적화 전략

LLM의 성능을 극대화하기 위해, 우리는 모델의 효율성을 높이는 다양한 최적화 전략을 모색해야 합니다. 이는 단순히 모델의 크기를 키우는 것을 넘어, 자원 사용을 최적화하고 특정 작업에 더 적합한 모델을 개발하는 것을 포함합니다.

#### 모델 압축 및 경량화 기법

거듭제곱 법칙은 LLM 스케일링의 근간을 이루는 근본적인 개념입니다. 이제는 LLM의 크기를 줄이면서도 성능 저하를 최소화하는 모델 압축 및 경량화 기법이 중요해지고 있습니다. 이러한 기법들은 크게 세 가지로 나눌 수 있습니다.

**양자화(Quantization).** 양자화는 모델의 가중치(weights)와 활성화 값(activations)을 더 낮은 비트 정밀도(precision)로 표현하여 모델 크기를 줄이는 기술입니다. 예를 들어, 32비트 부동소수점(float)을 8비트 정수(integer)로 변환하면 모델의 메모리 사용량을 크게 줄이고 추론 속도(inference speed)를 높일 수 있습니다. 이러한 양자화 기법은 모델의 테스트 손실(test loss)에 미치는 영향을 최소화하면서 실질적인 이점을 제공합니다. 양자화는 훈련 후 양자화(Post-Training Quantization, PTQ)와 양자화 인식 훈련(Quantization-Aware Training, QAT) 등 다양한 방식으로 적용될 수 있습니다.

**가지치기(Pruning).** 가지치기는 모델의 중요하지 않은 가중치나 연결을 제거하여 모델의 희소성(sparsity)을 높이는 방법입니다. 이렇게 제거된 부분은 모델의 전반적인 성능에 미치는 영향이 미미하거나 전혀 없으며, 결과적으로 모델의 크기를 줄이고 계산 효율성을 향상시킵니다. 가지치기는 구조적 가지치기(structured pruning)와 비구조적 가지치기(unstructured pruning)로 구분되며, 후자는 더 높은 희소성을 달성하지만 전용 하드웨어 지원이 필요할 수 있습니다.

**지식 증류(Knowledge Distillation).** 지식 증류는 크고 복잡한 "교사(teacher)" 모델의 지식을 작고 효율적인 "학생(student)" 모델로 이전하는 기법입니다. 학생 모델은 교사 모델의 소프트 타겟(soft targets)을 모방하여 훈련되며, 이를 통해 교사 모델에 근접한 성능을 달성하면서도 훨씬 작은 크기를 유지할 수 있습니다. 지식 증류는 모델의 테스트 손실과 총 모델 매개변수 수 사이에 측정 가능한 관계가 존재함을 알려줍니다. 이러한 양 중 하나가 변경되면 다른 양에서 상대적이고 스케일 불변(scale-invariant)적인 변화가 발생합니다.

이러한 모델 압축 기법들은 LLM을 엣지 디바이스(edge device)나 제한된 컴퓨팅 환경에 배포하는 데 필수적이며, AI의 접근성을 높이는 데 기여합니다.

### 고효율 LLM 아키텍처 및 훈련 방법

신경 언어 모델을 위한 스케일링 법칙 [1]은 이제 효율성 중심으로 진화하고 있습니다. 초기 언어 모델 시대에는 모델의 크기 자체가 주요 관심사였지만, 이제는 아키텍처의 혁신을 통해 효율성을 극대화하는 방향으로 나아가고 있습니다.

**희소 활성화 모델(Sparse Activation Models).** 기존의 덴스(dense) 모델과 달리, 희소 활성화 모델은 추론 시 전체 파라미터 중 일부만 활성화됩니다. 대표적인 예시가 전문가 혼합(Mixture of Experts, MoE) 모델입니다. MoE 모델은 여러 개의 작은 "전문가" 네트워크를 가지고 있으며, 입력 데이터에 따라 특정 전문가만 활성화됩니다. 이는 모델의 총 파라미터 수를 크게 늘리면서도 추론 연산량(inference compute)은 효율적으로 유지할 수 있게 합니다.

**적응형 연산(Adaptive Computation).** 모델이 모든 입력에 대해 동일한 양의 연산을 수행하는 대신, 적응형 연산은 입력의 복잡성에 따라 동적으로 연산량을 조절합니다. 예를 들어, 쉬운 질문에는 적은 연산을, 어려운 질문에는 더 많은 연산을 할당하는 방식입니다. 이러한 접근 방식은 연산 자원을 효율적으로 사용하여 전체적인 성능을 향상시킵니다.

**데이터 효율적인 훈련.** "손실은 모델 크기, 데이터셋 크기, 훈련에 사용된 연산량(compute)과 함께 거듭제곱 법칙을 따르며, 일부 경향은 7자릿수 이상에 걸쳐 나타납니다." [1]에서 저자들은 모델 크기, 모델 형태, 데이터셋 크기, 훈련 연산량(training compute), 배치 크기(batch size)와 같은 여러 요인이 모델 성능에 미치는 영향을 분석하여 이러한 질문에 답하는 것을 목표로 합니다. 이제는 더 적은 데이터로 더 나은 모델을 훈련하는 것이 중요해졌습니다.

*   **합성 데이터 생성(Synthetic Data Generation)**: 실제 데이터를 보강하거나 대체하기 위해 LLM이 직접 데이터를 생성하는 방식입니다. 이는 데이터 부족 문제를 해결하고 특정 도메인에 특화된 데이터를 확보하는 데 유용합니다.
*   **커리큘럼 학습(Curriculum Learning)**: 모델을 쉬운 예제부터 어려운 예제로 점진적으로 훈련시켜 학습 효율을 높이는 방법입니다.
*   **능동 학습(Active Learning)**: 모델이 가장 불확실하거나 유익하다고 판단하는 데이터 포인트를 선택하여 레이블링(labeling)하도록 요청함으로써 훈련 데이터의 효율성을 극대화합니다.

이러한 새로운 훈련 전략들은 모델 매개변수(model parameters)의 수, 데이터셋의 크기, 훈련에 사용되는 연산량(compute)을 최적화하여 LLM 성능을 향상시킵니다.

**파운데이션 모델의 효율적 활용.** 거듭제곱 법칙은 우리에게 무엇을 알려주는가? 기존의 "더 크고 더 많이"라는 패러다임에서 벗어나, 이제는 "더 효율적이고 더 스마트하게" 접근하는 것이 중요합니다. LLM의 품질을 향상시키는 것은 스케일(scale)이 커질수록 기하급수적으로 더 어려워집니다.

다른 유용한 발견들. [1]에서 관찰된 거듭제곱 법칙 외에도, 모델 형태나 아키텍처 설정(architecture settings)과 같이 고려된 다른 요인들이 모델 성능에 미치는 영향은 미미하다는 것을 알 수 있습니다. 오늘날에는 사전 훈련된 대규모 모델을 특정 작업에 맞춰 최적화하는 것이 일반적입니다.

*   **미세 조정(Fine-tuning)**: 사전 훈련된 모델을 특정 작업이나 데이터셋에 대해 추가로 훈련시켜 성능을 극대화합니다.
*   **프롬프트 엔지니어링(Prompt Engineering)**: 모델의 내부 구조를 변경하지 않고 입력 프롬프트를 신중하게 설계하여 원하는 출력을 얻는 기술입니다.
*   **검색 증강 생성(Retrieval Augmented Generation, RAG)**: LLM이 외부 지식 기반에서 관련 정보를 검색하여 답변을 생성하도록 함으로써 모델의 최신성(recency)과 사실성(factuality)을 향상시킵니다.

이러한 접근 방식은 더 큰 LLM이 더 샘플 효율적(sample efficient)인 경향이 있음을 나타냅니다. 이는 더 작은 모델에 비해 더 적은 데이터로 동일한 수준의 테스트 손실에 도달한다는 것을 의미합니다. 이러한 방식으로 우리는 막대한 비용을 들이지 않고도 최첨단 성능을 달성할 수 있습니다.

### LLM 배포 및 서비스 최적화

스케일링 법칙의 실제 활용은 이제 모델의 훈련 단계를 넘어 실제 서비스 배포 및 운영 단계로 확장되고 있습니다. 대규모 사전 훈련(pretraining)이 매우 유익하다는 사실은 우리에게 약간의 딜레마를 안겨줍니다. 최고의 결과는 방대한 양의 데이터로 거대한 모델(massive models)을 훈련함으로써 얻어지지만, 이러한 훈련 실행(training runs)은 엄청나게 비싸며, 이는 또한 많은 위험을 수반한다는 것을 의미합니다. 이제는 훈련된 모델을 효율적으로 배포하고 운영하는 것이 핵심 과제입니다.

**엣지 디바이스 및 온디바이스 LLM.** 스케일링 법칙이 등장하는 지점입니다. 클라우드 기반 LLM은 강력하지만, 지연 시간(latency), 개인 정보 보호(privacy), 비용 등의 문제로 인해 모든 애플리케이션에 적합하지 않습니다. 따라서 스마트폰, IoT 장치(devices) 등 엣지 디바이스에서 직접 실행되는 경량 LLM의 개발이 중요해지고 있습니다. 이는 모델 압축 기법과 더불어 하드웨어 최적화가 필수적입니다.

**클라우드 네이티브 최적화.** 대규모 LLM을 클라우드 환경에서 효율적으로 서비스하기 위해서는 클라우드 네이티브 기술을 활용한 최적화가 필수적입니다. 컨테이너화(containerization), 서버리스(serverless) 아키텍처, 자동 확장(auto-scaling) 등의 기술을 사용하여 수요 변화에 유연하게 대응하고 비용을 절감할 수 있습니다. 또한, 분산 추론(distributed inference) 및 캐싱(caching) 전략을 통해 처리량(throughput)을 높이고 응답 시간을 단축합니다.

**지속적인 통합 및 배포(CI/CD) 파이프라인.** LLM의 빠른 개발 주기와 빈번한 업데이트를 지원하기 위해 자동화된 CI/CD 파이프라인 구축이 중요합니다. 이는 모델의 버전 관리, 테스트, 배포 과정을 효율화하여 개발 생산성을 높이고 서비스 안정성을 확보합니다.

스케일링 법칙을 사용하여 더 큰 모델의 성능을 예측할 수 있는 능력은 연구자로서 우리에게 더 많은 확신(그리고 마음의 평화)을 줍니다. 이제는 이 확신을 기반으로 LLM을 실제 환경에서 성공적으로 운영하기 위한 기술적, 운영적 노하우를 축적하는 것이 중요합니다.

### 멀티모달 LLM과 범용 AI

스케일링과 사전 훈련(Pretraining) 시대는 새로운 전환점을 맞이하고 있습니다. 기존 LLM이 텍스트 데이터에만 의존했다면, 이제는 이미지, 오디오, 비디오 등 다양한 모달리티(modality)를 이해하고 생성하는 멀티모달 LLM이 AI 연구의 핵심 동력으로 부상하고 있습니다.

**GPT 계보: GPT [2], GPT-2 [3], GPT-3 [4], GPT-4 [5]를 넘어.** 텍스트 기반 모델의 성공은 멀티모달리티로 확장될 수 있음을 보여주었습니다. GPT-4o와 같은 최신 모델들은 텍스트뿐만 아니라 이미지와 오디오를 입력으로 받아들이고, 이를 기반으로 텍스트 및 오디오 출력을 생성할 수 있습니다. 이는 인간의 인지 방식과 유사하게 다양한 감각 정보를 통합하여 세상을 이해하려는 시도입니다.

**멀티모달 모델의 통합.** 멀티모달 LLM은 각 모달리티별로 독립적인 인코더(encoder)를 사용하거나, 모든 모달리티를 공통의 임베딩 공간(embedding space)으로 매핑하는 방식으로 데이터를 통합합니다. 중요한 것은 서로 다른 유형의 데이터 간의 의미론적 관계를 학습하고, 이를 통해 더욱 풍부하고 맥락적인 이해를 가능하게 하는 것입니다. 예를 들어, 이미지와 텍스트 설명을 동시에 학습하여 시각적 질문 응답(Visual Question Answering, VQA)과 같은 복합적인 작업을 수행할 수 있습니다.

**멀티모달 정렬 및 추론.** GPT-3를 넘어서, 멀티모달 LLM의 핵심 과제 중 하나는 서로 다른 모달리티 간의 "정렬(alignment)"입니다. 즉, 모델이 이미지에서 "고양이"를 볼 때, 텍스트에서 "고양이"라는 단어가 의미하는 바와 정확히 일치하도록 학습해야 합니다. 이러한 정렬은 멀티모달 LLM이 복잡한 추론 작업을 수행하고, 새로운 정보를 통합하며, 다양한 형식의 질문에 일관성 있게 답변하는 데 필수적입니다. 이러한 모델들은 사전 훈련(pretraining) 프로세스를 스케일업(scaling up)하는 것 외에도 사후 훈련(post-training) 연구의 발전으로부터 큰 이점을 얻었습니다.

멀티모달 LLM은 단순히 여러 종류의 데이터를 처리하는 것을 넘어, 궁극적으로는 인간과 유사한 방식으로 세상을 인지하고 상호작용하는 범용 AI(General AI)의 길을 열어줄 잠재력을 가지고 있습니다.

### 새로운 패러다임: 에이전트 기반 LLM과 자율 시스템

Chinchilla: 연산 최적(Compute-Optimal) 대규모 언어 모델 훈련 [5]이 모델 훈련의 효율성을 강조했다면, 이제는 훈련된 LLM을 활용하여 자율적으로 목표를 달성하는 에이전트(agent) 기반 시스템이 주목받고 있습니다. 이는 LLM이 단순히 텍스트를 생성하는 것을 넘어, 계획을 세우고, 도구를 사용하며, 환경과 상호작용하는 능력을 갖추는 것을 의미합니다.

**LLM 에이전트의 구조.** 연산 최적(Compute-optimal) 스케일링 법칙은 이제 LLM 에이전트의 설계에 적용됩니다. 에이전트 LLM은 일반적으로 다음과 같은 핵심 구성 요소를 포함합니다.

*   **계획(Planning)**: 복잡한 목표를 달성하기 위해 일련의 하위 목표와 단계를 생성합니다.
*   **기억(Memory)**: 과거의 경험과 학습된 지식을 저장하고 필요할 때 검색하여 활용합니다. 단기 기억(short-term memory)은 현재 대화의 맥락을 유지하고, 장기 기억(long-term memory)은 영구적인 지식 기반 역할을 합니다.
*   **도구 사용(Tool Use)**: 외부 도구(예: 웹 검색 엔진, 계산기, 코드 인터프리터, API)를 호출하여 자신의 능력을 확장하고 특정 작업을 수행합니다.
*   **반성 및 자기 수정(Reflection and Self-correction)**: 자신의 행동과 결과(output)를 평가하고, 오류를 식별하며, 다음 행동을 개선하기 위해 학습합니다.

**자율 학습 및 적응.** Chinchilla 연구는 데이터 스케일(data scale)의 중요성을 강조합니다. 대규모 모델은 최고의 성능에 도달하기 위해 더 많은 데이터로 훈련되어야 합니다. LLM 에이전트도 유사하게 환경과의 상호작용을 통해 지속적으로 학습하고 적응합니다. 강화 학습(Reinforcement Learning) 기법이 에이전트가 환경에서 보상을 최대화하도록 훈련하는 데 사용되며, 이를 통해 에이전트는 더욱 정교하고 자율적인 행동을 개발할 수 있습니다.

**실제 적용 분야.** LLM 에이전트는 고객 서비스, 소프트웨어 개발, 과학 연구, 개인 비서 등 다양한 분야에서 혁신적인 잠재력을 가지고 있습니다. 예를 들어, 소프트웨어 개발 에이전트는 요구 사항을 분석하고, 코드를 작성하며, 테스트하고, 디버깅하는 전체 과정을 자율적으로 수행할 수 있습니다.

이러한 에이전트 기반 LLM은 AI 시스템이 단순한 도구를 넘어, 인간의 지능적인 파트너로 진화하는 중요한 단계를 나타냅니다.

### AI 윤리, 안전 및 거버넌스

스케일링 법칙의 "종말"에 대한 논의가 활발해지면서, AI 기술의 윤리적, 안전적, 거버넌스적 측면에 대한 관심이 더욱 증대되고 있습니다. AI의 발전 속도가 빨라짐에 따라, 기술의 오용을 방지하고 사회적 책임을 다하는 것이 그 어느 때보다 중요해졌습니다.

**AI 윤리 원칙.** 느려지는 스케일링: 무엇을 의미하는가? 왜 일어나는가? AI 시스템이 사회에 미치는 영향이 커지면서, 공정성(fairness), 투명성(transparency), 책임성(accountability), 개인 정보 보호(privacy) 등의 윤리적 원칙 준수가 필수적입니다. AI 개발자와 배포자는 이러한 원칙을 설계 단계부터 고려하여 시스템이 인간의 가치와 사회적 규범에 부합하도록 해야 합니다.

**AI 안전 연구.** 스케일링 법칙은 우리에게 무엇을 알려주는가? LLM 성능 정의는 단순히 기술적 지표를 넘어 사회적 영향까지 포괄해야 합니다. 특히 강력한 AI 시스템의 잠재적 위험(예: 잘못된 정보 확산, 편향된 의사 결정, 자율 시스템의 통제 불능)을 완화하기 위한 안전 연구가 중요합니다. 이는 AI 시스템의 예측 불가능한 행동을 방지하고, 악의적인 사용을 막기 위한 기술적 및 정책적 방안을 모색하는 것을 포함합니다.

*   **편향성 감지 및 완화(Bias Detection and Mitigation)**: 훈련 데이터와 모델 출력에서 발생하는 편향을 식별하고 수정하는 기술.
*   **설명 가능한 AI(Explainable AI, XAI)**: AI 모델의 의사 결정 과정을 인간이 이해할 수 있도록 설명하는 기술.
*   **개인 정보 보호 AI(Privacy-Preserving AI)**: 연합 학습(Federated Learning), 동형 암호(Homomorphic Encryption) 등 개인 정보를 보호하면서 AI 모델을 훈련하고 사용하는 기술.

**AI 거버넌스 및 규제.** 데이터 고갈(Data death)과 같은 문제에 직면하면서, AI 기술의 책임감 있는 개발 및 배포를 위한 거버넌스 체계 구축이 시급합니다. 정부, 산업계, 학계, 시민 사회가 협력하여 AI 관련 법규, 표준, 가이드라인을 마련하고, AI 시스템의 개발 및 사용에 대한 감독을 강화해야 합니다. 이는 AI의 혁신을 저해하지 않으면서도 사회적 신뢰를 확보하는 균형 잡힌 접근 방식을 요구합니다.

AI 윤리, 안전, 거버넌스는 단순한 부가 기능이 아니라, AI 기술의 지속 가능한 발전과 사회적 수용을 위한 핵심 요소입니다.

### 양자 컴퓨팅과 차세대 AI 하드웨어

사전 훈련(Pretraining)을 위한 차세대 스케일은 이제 기존 컴퓨팅 패러다임을 넘어선 혁신적인 하드웨어 기술에 주목하고 있습니다. 현재의 실리콘 기반 컴퓨팅은 물리적 한계에 직면하고 있으며, 이는 AI 모델의 지속적인 스케일링에 병목 현상을 초래할 수 있습니다.

**양자 컴퓨팅(Quantum Computing)의 잠재력.** 합성 데이터(Synthetic data)는 미래 AI의 중요한 구성 요소가 될 수 있습니다. 양자 컴퓨팅은 양자 역학의 원리를 활용하여 기존 컴퓨터로는 해결하기 어려운 복잡한 문제를 해결하는 새로운 컴퓨팅 패러다임입니다. 양자 컴퓨터는 양자 병렬성(quantum parallelism)을 통해 특정 AI 알고리즘(예: 양자 머신러닝)에서 지수적인 속도 향상을 제공할 잠재력을 가지고 있습니다. 아직 초기 단계이지만, 양자 컴퓨팅은 암호 해독, 신약 개발, 재료 과학 등과 더불어 차세대 AI 기술의 발전에 기여할 것으로 기대됩니다.

**뉴로모픽 컴퓨팅(Neuromorphic Computing).** 실용적인 스케일링 법칙은 이제 생물학적 뇌의 구조와 기능을 모방하는 뉴로모픽 칩에 대한 관심으로 이어집니다. 이 칩은 이벤트 기반(event-driven) 처리 방식을 사용하여 에너지 효율성을 극대화하고, 병렬 처리에 최적화되어 있습니다. 뉴로모픽 시스템은 스파이킹 신경망(spiking neural networks)과 같은 새로운 AI 모델을 효율적으로 실행할 수 있으며, 특히 실시간 학습 및 적응이 필요한 엣지 AI 애플리케이션에서 유망합니다.

**특수 목적 AI 가속기(Specialized AI Accelerators).** DeepSeek-v3와 같은 모델들은 특정 AI 작업에 최적화된 하드웨어의 중요성을 보여줍니다. GPU는 AI 훈련 및 추론에 널리 사용되지만, TPU(Tensor Processing Unit)나 NPU(Neural Processing Unit)와 같은 맞춤형 AI 가속기는 특정 연산(예: 행렬 곱셈)을 훨씬 효율적으로 수행하도록 설계되었습니다. 이러한 가속기는 전력 소모를 줄이면서도 AI 모델의 성능을 크게 향상시킬 수 있습니다.

**OOM(자릿수) 단위로 스케일 증가.** 우리의 스케일링 법칙을 계속 테스트하려면, 현재 모델보다 여러 자릿수(orders of magnitude) 더 큰 LLM을 훈련해야 합니다. 미래의 AI는 단순히 소프트웨어 알고리즘의 발전뿐만 아니라, 근본적인 하드웨어 혁신에 의해서도 좌우될 것입니다. 양자 컴퓨팅, 뉴로모픽 칩, 그리고 더욱 발전된 AI 가속기는 AI의 성능과 효율성을 새로운 차원으로 끌어올릴 잠재력을 가지고 있으며, 이는 AI 연구의 장기적인 비전을 형성하는 데 중요한 역할을 합니다.

### AI 연구의 지속 가능한 발전 방향

AI 연구의 미래는 이제 스케일링을 넘어선 다각적인 접근 방식을 요구합니다. 유용한 LLM 시스템 구축은 단순한 모델 개발을 넘어섭니다. 사전 훈련 연구가 갑작스러운 한계에 부딪힐 것이라고 가정해 봅시다. 가까운 미래에 모델 기능이 전혀 향상되지 않더라도, AI 연구가 계속해서 빠르게 발전할 수 있는 다양한 방법이 있습니다.

**효율성과 최적화의 중요성 재확인.** LLM 시스템 기본 사항은 이제 모델의 효율적인 배포와 운영에 초점을 맞춥니다. 모델 압축, 경량화, 그리고 고효율 아키텍처는 제한된 자원으로 최적의 성능을 달성하는 데 필수적입니다. 이는 AI 기술의 접근성을 높이고 더 넓은 범위의 애플리케이션에 적용 가능하게 만듭니다.

**멀티모달리티와 범용성 확장.** LLM 기반 제품 구축은 텍스트를 넘어 다양한 데이터 형식(이미지, 오디오, 비디오)을 통합하는 멀티모달 모델을 통해 이루어집니다. 이러한 모델들은 인간의 인지 방식을 모방하여 더욱 풍부하고 맥락적인 이해를 가능하게 하며, 범용 AI로 나아가는 중요한 단계입니다.

**에이전트 기반 자율 시스템의 발전.** 에이전트(Agents)는 LLM이 단순히 정보를 처리하는 것을 넘어, 계획을 세우고, 도구를 사용하며, 환경과 상호작용하는 자율적인 존재로 진화할 수 있음을 보여줍니다. 이러한 시스템은 복잡한 문제를 해결하고, 인간의 개입 없이 다양한 작업을 수행할 수 있는 잠재력을 가지고 있습니다.

**AI 윤리, 안전, 거버넌스의 선제적 통합.** 추론 모델(Reasoning Models) 및 새로운 스케일링 패러다임은 기술 발전과 동시에 윤리적, 사회적 책임을 강조합니다. AI 시스템이 공정하고, 안전하며, 투명하게 작동하도록 보장하는 것은 기술적 혁신만큼이나 중요합니다. 이는 사회적 신뢰를 구축하고 AI의 지속 가능한 발전을 위한 필수적인 기반입니다.

**하드웨어 혁신과 새로운 컴퓨팅 패러다임.** o1에서 o3으로, AI의 미래는 양자 컴퓨팅, 뉴로모픽 칩과 같은 차세대 하드웨어 기술에 의해 형성될 것입니다. 이러한 혁신은 현재의 컴퓨팅 한계를 극복하고, AI 모델이 완전히 새로운 방식으로 작동할 수 있는 가능성을 열어줄 것입니다.

### 결론: 다각적인 AI 발전 전략

이제 우리는 스케일링 법칙, LLM에 미치는 영향, 그리고 AI 연구 발전의 미래 방향에 대해 더 명확한 시각을 갖게 되었습니다. 우리가 배운 바와 같이, 스케일링 법칙에 대한 최근 비판에는 여러 기여 요인이 있습니다.

AI 연구의 미래는 단순한 "스케일업(scale up)"을 넘어선 다각적인 전략을 요구합니다. 과거의 성공적인 스케일링 법칙은 여전히 중요하지만, 이제는 효율성, 멀티모달리티, 에이전트 기반 자율성, 윤리적 고려 사항, 그리고 혁신적인 하드웨어 기술을 아우르는 통합적인 접근 방식이 필요합니다. 대규모 사전 훈련(pretraining)에 대한 투자는 계속될 것이며 (그래야만 하지만), 개선은 시간이 지남에 따라 기하급수적으로 더 어려워질 것입니다.

결과적으로, 대안적인 발전 방향(예: 에이전트(agents) 및 추론(reasoning))이 더욱 중요해질 것입니다. 그러나 우리가 이러한 새로운 연구 분야에 투자하더라도, 스케일링의 근본적인 아이디어는 계속해서 거대한 역할을 할 것입니다. 스케일링이 계속될 것인지는 질문이 아닙니다. 진정한 질문은 우리가 다음에 무엇을 스케일링할 것인가입니다. AI 연구는 이제 더 넓은 지평을 바라보며, 기술적 한계를 넘어 인류 사회에 긍정적인 영향을 미칠 수 있는 지속 가능한 발전 경로를 모색해야 합니다.

뉴스레터가 처음이신가요? 안녕하세요! 저는 카메론 R. 울프(Cameron R. Wolfe)입니다. 넷플릭스(Netflix)의 딥러닝(Deep Learning) 박사이자 머신러닝 과학자(Machine Learning Scientist)입니다. 이것은 독자들이 AI 연구의 중요한 주제를 더 잘 이해하도록 돕는 딥 (러닝) 포커스(Deep (Learning) Focus) 뉴스레터입니다. 뉴스레터가 마음에 드신다면 구독, 공유하거나 X와 링크드인(LinkedIn)에서 저를 팔로우해주세요! 구독하기

### 참고문헌

[1] Kaplan, Jared, et al. "신경 언어 모델을 위한 스케일링 법칙(Scaling laws for neural language models)." arXiv preprint arXiv:2001.08361 (2020).
[2] Radford, Alec. "생성적 사전 훈련을 통한 언어 이해 향상(Improving language understanding by generative pre-training)." (2018).
[3] Radford, Alec, et al. "언어 모델은 비지도 다중 태스크 학습자(Language models are unsupervised multitask learners)." OpenAI 블로그 1.8 (2019): 9.
[4] Brown, Tom, et al. "언어 모델은 퓨샷 학습자(Language models are few-shot learners)." 신경 정보 처리 시스템 발전 33 (2020): 1877-1901.
[5] Achiam, Josh, et al. "GPT-4 기술 보고서(Gpt-4 technical report)." arXiv preprint arXiv:2303.08774 (2023).
[6] Hoffmann, Jordan, et al. "연산 최적 대규모 언어 모델 훈련(Training compute-optimal large language models)." arXiv preprint arXiv:2203.15556 (2022).
[7] Gadre, Samir Yitzhak, et al. "언어 모델은 과도한 훈련 및 다운스트림 태스크에서 안정적으로 스케일링된다(Language models scale reliably with over-training and on downstream tasks)." arXiv preprint arXiv:2403.08540 (2024).
[8] Ouyang, Long, et al. "인간 피드백을 통해 지시를 따르도록 언어 모델 훈련(Training language models to follow instructions with human feedback)." 신경 정보 처리 시스템 발전 35 (2022): 27730-27744.
[9] Smith, Shaden, et al. "Deepspeed와 Megatron을 사용하여 대규모 생성 언어 모델인 Megatron-Turing NLG 530B 훈련(Using deepspeed and megatron to train megatron-turing nlg 530b, a large-scale generative language model)." arXiv preprint arXiv:2201.11990 (2022).
[10] Rae, Jack W., et al. "언어 모델 스케일링: Gopher 훈련을 통한 방법, 분석 및 통찰(Scaling language models: Methods, analysis & insights from training gopher)." arXiv preprint arXiv:2112.11446 (2021).
[11] Bhagia, Akshita, et al. "연산 효율적인 모델 래더를 통한 태스크 스케일링 법칙 수립(Establishing Task Scaling Laws via Compute-Efficient Model Ladders)." arXiv preprint arXiv:2412.04403 (2024).
[12] Bai, Yuntao, et al. "헌법적 AI: AI 피드백으로부터의 무해성(Constitutional ai: Harmlessness from ai feedback)." arXiv preprint arXiv:2212.08073 (2022).
[13] Blakeney, Cody, et al. "당신의 데이터가 기쁨을 주는가? 훈련 종료 시 도메인 업샘플링을 통한 성능 향상(Does your data spark joy? Performance gains from domain upsampling at the end of training)." arXiv preprint arXiv:2406.03476 (2024).
[14] Chen, Hao, et al. "합성 데이터의 다양성과 대규모 언어 모델 훈련에 미치는 영향에 대하여(On the Diversity of Synthetic Data and its Impact on Training Large Language Models)." arXiv preprint arXiv:2410.15226 (2024).
[15] Guo, Zishan, et al. "대규모 언어 모델 평가: 종합 설문조사(Evaluating large language models: A comprehensive survey)." arXiv preprint arXiv:2310.19736 (2023).
[16] Xu, Zifei, et al. "사후 훈련 양자화된 대규모 언어 모델을 위한 스케일링 법칙(Scaling laws for post-training quantized large language models)." arXiv preprint arXiv:2410.12119 (2024).
[17] Xiong, Yizhe, et al. "대규모 언어 모델을 위한 시간적 스케일링 법칙(Temporal scaling law for large language models)." arXiv preprint arXiv:2404.17785 (2024).
[18] DeepSeek-AI et al. "DeepSeek-v3 기술 보고서(DeepSeek-v3 Technical Report)." https://github.com/deepseek-ai/DeepSeek-V3/blob/main/DeepSeek_V3.pdf (2024).
[19] Schick, Timo, et al. "Toolformer: 언어 모델은 도구 사용법을 스스로 배울 수 있다(Toolformer: Language models can teach themselves to use tools)." arXiv preprint arXiv:2302.04761 (2023).
[20] Welleck, Sean, et al. "디코딩에서 메타 생성까지: 대규모 언어 모델을 위한 추론 시간 알고리즘(From decoding to meta-generation: Inference-time algorithms for large language models)." arXiv preprint arXiv:2406.16838 (2024).
[21] OpenAI et al. “LLM으로 추론 학습하기(Learning to Reason with LLMs).” https://openai.com/index/learning-to-reason-with-llms/ (2024).
[22] Wei, Jason, et al. "연쇄 사고 프롬프팅은 대규모 언어 모델에서 추론을 유도한다(Chain-of-thought prompting elicits reasoning in large language models)." 신경 정보 처리 시스템 발전 35 (2022): 24824-24837.
[23] Liu, Yang, et al. "G-eval: GPT-4를 사용한 NLG 평가, 더 나은 인간 정렬(G-eval: Nlg evaluation using gpt-4 with better human alignment)." arXiv preprint arXiv:2303.16634 (2023).
[24] Kim, Seungone, et al. "프로메테우스: 언어 모델에서 미세 조정된 평가 능력 유도(Prometheus: Inducing fine-grained evaluation capability in language models)." 제12회 국제 학습 표현 컨퍼런스. 2023.
[25] Ho, Namgyu, Laura Schmid, and Se-Young Yun. "대규모 언어 모델은 추론 교사이다(Large language models are reasoning teachers)." arXiv preprint arXiv:2212.10071 (2022).
[26] Kim, Seungone, et al. "CoT 컬렉션: 연쇄 사고 미세 조정을 통한 언어 모델의 제로샷 및 퓨샷 학습 개선(The cot collection: Improving zero-shot and few-shot learning of language models via chain-of-thought fine-tuning)." arXiv preprint arXiv:2305.14045 (2023).
[27] Weng, Yixuan, et al. "대규모 언어 모델은 자기 검증을 통해 더 나은 추론자이다(Large language models are better reasoners with self-verification)." arXiv preprint arXiv:2212.09561 (2022).
[28] Lightman, Hunter, et al. "단계별로 검증하자(Let's verify step by step)." arXiv preprint arXiv:2305.20050 (2023).
[29] Zhang, Lunjun, et al. "생성적 검증자: 다음 토큰 예측으로서의 보상 모델링(Generative verifiers: Reward modeling as next-token prediction)." arXiv preprint arXiv:2408.15240 (2024).

1 두 가지 주요 보고서는 디 인포메이션(The Information)과 로이터(Reuters)에서 나왔습니다.
2 플롯을 생성하기 위해 다음 설정을 사용합니다: a = 1, p = 0.5, 그리고 0 < x < 1.
3 연산량(Compute)은 [1]에서 6NBS로 정의되며, 여기서 N은 모델 매개변수(model parameters)의 수, B는 훈련 중 사용되는 배치 크기(batch size), S는 총 훈련 단계(training steps) 수입니다.
4 이 추가적인 곱셈 상수는 거듭제곱 법칙(power law)의 동작을 변경하지 않습니다. 이것이 왜 그런지 이해하려면 스케일 불변성(scale invariance)의 정의를 이해해야 합니다. 거듭제곱 법칙은 스케일 불변(scale invariant)이므로, 특정 요인으로 스케일업(scale up)하거나 스케일다운(scale down)하더라도 거듭제곱 법칙의 근본적인 특성은 동일합니다. 관찰되는 동작은 어떤 스케일에서도 동일할 것입니다!
5 이 설명은 NeurIPS'24에서 이 논문에 대한 일리야의 '테스트 오브 타임 어워드(test of time award)'에서 나온 것입니다.
6 이것이 지금은 당연해 보일지라도, 당시 대부분의 자연어 처리(NLP) 태스크(예: 요약 및 질의응답(QA))에는 그들을 위한 전체 연구 분야가 할애되어 있었다는 점을 기억해야 합니다! 이러한 각 태스크에는 해당 태스크를 수행하는 데 특화된 태스크별 아키텍처(task-specific architectures)가 있었고, GPT는 여러 다른 태스크에서 이러한 아키텍처 대부분을 능가할 수 있는 단일 일반 모델이었습니다.
7 이는 우리가 LLM의 프롬프트(prompt)에 각 태스크를 설명하고 동일한 모델을 사용하여 다른 태스크를 해결한다는 의미입니다. 태스크 간에는 프롬프트(prompt)만 변경됩니다.
8 이 모델들은 제로샷 추론(zero-shot inference)을 사용하며 어떤 다운스트림 태스크(downstream tasks)에서도 전혀 미세 조정(finetuned)되지 않았기 때문에 예상되는 결과입니다.
9 "새로운(emergent)" 능력이라 함은 특정 스케일(scale)에 도달한 후에만 나타나는(예: 충분히 큰 모델) LLM이 가진 기술을 의미합니다.
10 여기서 우리는 "연산 최적(compute-optimal)"을 고정된 훈련 연산 비용(training compute cost)에서 테스트 손실(test loss) 측면에서 가능한 최고의 성능을 산출하는 훈련 설정(training setting)으로 정의합니다.
11 예를 들어, Anthropic은 Claude 3.5 Opus의 출시를 계속 연기했으며, Google은 Gemini-2의 플래시(flash) 변형만 출시했고, OpenAI는 2024년에 GPT-4o만 출시했습니다(o1과 o3가 12월에 출시되기 전까지). 이는 논란의 여지가 있지만 GPT-4보다 크게 더 유능하지는 않았습니다.
12 이 매개변수들 중 370억 개만이 단일 토큰(token)에 대한 추론(inference) 중에 활성화됩니다.
13 예를 들어, xAI는 최근 멤피스에 10만 개의 NVIDIA GPU를 갖춘 새로운 데이터센터(datacenter)를 구축했으며, Anthropic 경영진은 향후 몇 년 동안 연산 지출(compute spend)을 최대 100배 늘리고 싶다는 의사를 밝혔습니다.
14 통합 단계는 다양한 방식으로 구현될 수 있습니다. 예를 들어, 응답을 수동으로 통합하거나(예: 연결을 통해), LLM을 사용하거나, 그 사이의 거의 모든 것을 사용할 수 있습니다!
15 이는 이러한 태스크가 간단하기 때문이 아닙니다. 코드 생성(code generation)과 채팅(chat) 모두 해결하기 어렵지만, (논란의 여지가 있지만) LLM에 대한 상당히 명확한 응용 분야입니다.
16 OpenAI는 o1 사용자로부터 이러한 긴 연쇄 사고(chains of thought)를 숨기기로 결정했습니다. 이 선택 뒤에 있는 주장은 이러한 근거(rationales)가 모델의 사고 과정에 대한 통찰력을 제공하여 모델을 디버깅(debug)하거나 모니터링(monitor)하는 데 사용될 수 있다는 것입니다. 그러나 모델은 사용자 대면 모델 출력에 필요한 안전 필터링(safety filtering) 없이 순수한 생각을 표현할 수 있어야 합니다.
17 현재 ARC-AGI는 o3가 벤치마크(benchmark)의 연산량(compute) 요구 사항을 초과하기 때문에 기술적으로는 여전히 무패입니다. 그러나 이 모델은 더 낮은 연산량(compute) 설정에서도 75.7%의 정확도를 달성합니다.