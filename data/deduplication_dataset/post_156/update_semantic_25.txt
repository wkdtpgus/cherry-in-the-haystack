## 2024년 선거와 AI 허위 정보: 예측과 현실의 간극

2024년 미국 대선 기간 중 인공지능(AI)이 만들어낸 거짓 정보의 확산은 주요하게 논의되는 사안 중 하나였습니다. 올해 1월, 세계경제포럼(World Economic Forum)은 “허위 사실과 잘못된 정보(misinformation and disinformation)가 전 세계가 직면한 가장 심각한 단기적 위험”이라고 경고하며, “AI가 사회적 불안을 초래할 수 있는 조작되고 왜곡된 내용을 증폭시키고 있다”고 강조했습니다. 여러 언론 매체들도 2024년 선거를 다루며 이와 유사한 우려를 표명했습니다.

반면, 저희는 과거의 분석 글에서 AI가 정보 왜곡의 종말을 가져오지는 않을 것이라고 예상한 바 있습니다. 메타(Meta)가 개방형 대규모 언어 모델(large language model, LLaMA)을 공개했을 때, 저희는 이것이 대규모 허위 정보의 물결을 유발하지 않을 것이라고 주장했습니다. 이어지는 글에서는 허위 정보의 확산이 영향력 행사 작전(influence operations)의 핵심적인 병목 현상(bottleneck)이며, 생성형 AI(generative AI)가 정보 생성 비용을 낮추지만, 유포 비용은 줄이지 못한다고 지적했습니다. 이와 유사한 견해를 제시한 다른 연구자들도 존재합니다.

이 두 가지 상반된 시각 중 어느 쪽이 현실에 더 부합할까요? 다행히도, 이 질문에 대한 해답을 찾을 수 있도록 2024년 전 세계에서 치러진 선거에서 AI가 어떻게 활용되었는지에 대한 증거들이 축적되고 있습니다. 수많은 언론사와 연구 기관들은 AI가 만들어낸 텍스트와 미디어의 실제 사례들과 그 파급력을 집계해왔습니다. 따라서 AI의 잠재력에 대한 추측보다는, 지금까지 나타난 실질적인 영향들을 면밀히 검토할 수 있게 되었습니다.

저희는 WIRED AI 선거 프로젝트(WIRED AI Elections Project)에서 수집한 AI 사용 사례들을 면밀히 분석했습니다. 이 프로젝트는 2024년 전 세계 선거 기간 동안 정치적 콘텐츠를 만들기 위한 AI의 알려진 사용 사례들을 추적했습니다. 각 사례를 통해 AI가 구체적으로 어떤 용도로 쓰였는지 파악하고, AI의 도움 없이도 유사한 콘텐츠를 제작하는 데 필요한 비용을 추정했습니다. 저희의 조사 결과는 다음과 같습니다: (1) AI 활용의 절반은 기만적인 의도가 없었으며, (2) AI로 생성된 기만적인 콘텐츠는 AI 없이도 저렴하게 재현 가능했고, (3) 허위 정보의 공급 측면보다는 수요 측면에 집중하는 것이 문제의 본질을 진단하고 효과적인 개입 방안(interventions)을 모색하는 훨씬 더 나은 방법이라는 점을 밝혀냈습니다.

물론, AI가 생성한 합성 콘텐츠(synthetic content)는 여러 가지 실제적인 위험을 내포하고 있습니다. 예를 들어, 개인의 동의 없는 이미지(non-consensual images)나 아동 성 착취물(child sexual abuse material) 제작, 그리고 권력자들이 자신에게 불리한 실제 미디어 콘텐츠를 AI 조작으로 치부하여 책임을 회피하는 이른바 ‘거짓말쟁이의 배당금(liar’s dividend)’ 현상을 야기할 수 있습니다. 이러한 문제들은 모두 중요하게 다루어져야 할 과제들입니다. 하지만 이 글은 정치적 허위 정보라는 특정 문제에 초점을 맞추고자 합니다.

정보 환경의 건전성을 회복하는 것은 복잡하고 끊임없이 이어지는 과제입니다. AI가 이러한 문제들을 더욱 악화시킨다고 여기는 것은 충분히 이해할 만합니다. AI 기술은 거짓 정보를 조작하는 것을 가능하게 만듭니다. 그러나 그렇다고 해서 정치적 허위 정보의 흐름이 근본적으로 뒤바뀐 것은 아닙니다. 역설적이게도, AI에 대한 경고는 정보 환경의 우려를 개별적인 해결책이 필요한 독립적인 문제로 간주하기 때문에 일종의 안도감을 줄 수 있습니다. 하지만 정보 환경의 개선은 AI 생성 콘텐츠를 억제하는 것을 넘어선 구조적, 제도적 변혁에 달려 있습니다. 이는 단순히 기술적 도구를 제한하는 것을 넘어 사회 전반의 미디어 리터러시 강화, 신뢰할 수 있는 언론 지원, 그리고 플랫폼의 책임성 증대와 같은 다층적인 노력을 요구합니다.

### 2024년 선거에서 AI 활용 사례의 절반은 비기만적이었다

저희는 WIRED AI 선거 프로젝트(WIRED AI Elections Project)에 기록된 총 78건의 AI 사용 사례를 분석했습니다. (저희 분석의 근거 자료). 각 사례를 기만적인 의도(deceptive intent)가 있었는지 여부에 따라 구분했습니다. 예를 들어, AI가 정치 후보자가 실제로 발언하지 않은 내용을 담은 가짜 미디어를 만드는 데 사용되었다면, 이는 기만적인 것으로 분류했습니다. 반대로, 챗봇이 사용자의 질문에 부정확한 답변을 했거나, 패러디(parody)나 풍자(satire) 목적으로 딥페이크(deepfake)가 제작되었거나, 후보자가 선거 운동 자료를 개선하기 위해 AI를 투명하게 활용한 경우(예를 들어, 자신이 구사하지 못하는 언어로 연설문을 번역하는 등)에는 비기만적인 것으로 분류했습니다. 놀랍게도, 전체 78건 중 39건에서는 기만적인 의도가 발견되지 않았습니다.

AI의 가장 일반적인 비기만적 활용은 선거 캠페인 지원이었습니다. 후보자나 지지자들이 선거 활동을 위해 AI를 사용했을 때, 대부분의 경우(22건 중 19건) 그 명확한 목적은 유권자들을 허위 정보로 오도하는 것이 아니라, 선거 운동 자료의 품질을 높이는 것이었습니다. 저희는 정보 환경 개선에 기여했다고 판단되는 딥페이크 사례들도 찾아냈습니다. 베네수엘라에서는 언론인들이 정부에 비판적인 뉴스를 보도하면서 정부의 보복을 피하기 위해 AI 아바타(AI avatars)를 사용했습니다. 미국 애리조나(Arizona)의 지역 언론사인 애리조나 아젠다(Arizona Agenda)는 딥페이크를 활용하여 시청자들에게 영상 조작이 얼마나 쉬운지 교육했습니다. 캘리포니아(California)에서는 후두염으로 목소리를 잃은 후보자가 유권자들과의 소통 시, 자신의 목소리를 그대로 복제한 AI 음성 복제(AI voice cloning) 기술을 투명하게 사용하여 타이핑된 메시지를 읽어주기도 했습니다.

합리적인 관점을 가진 사람들은 선거 운동에 AI를 사용하는 것이 정당한지, 혹은 어떤 안전 장치(guardrails)가 필요한지에 대해 다양한 의견을 가질 수 있습니다. 하지만 비기만적인 방식으로 선거 운동에 AI를 사용하는 것(예를 들어, AI가 유권자 홍보를 개선하는 도구로 활용될 때)은 유권자들을 현혹시키기 위해 AI가 생성한 가짜 뉴스(fake news)를 퍼뜨리는 것보다 훨씬 덜 문제가 됩니다. 물론, 모든 비기만적인 AI 생성 정치 콘텐츠가 무해한 것은 아닙니다. 챗봇은 종종 선거 관련 질문에 오류가 있는 답변을 제공하기도 합니다. 이는 기만적인 의도보다는 챗봇의 한계, 즉 환각(hallucinations) 현상이나 사실성 부족에서 기인하는 경우가 많습니다. 불행히도, 이러한 한계는 사용자에게 명확하게 전달되지 않아, 결함이 있는 대규모 언어 모델(LLMs)에 대한 과도한 의존으로 이어질 수 있습니다. 이는 AI 기술의 발전 속도에 비해 사용자의 기술 이해도가 따라가지 못하는 정보 격차 문제로 볼 수 있습니다.

### 기만적 정치 허위 정보는 AI 없이도 충분히 제작 가능하다

AI 사용이 시청자들에게 명백히 거짓된 정보를 믿게 하려는 의도를 가졌던 39가지 기만적인 사례들 각각에 대해, 저희는 AI 없이 유사한 콘텐츠를 만드는 데 드는 비용을 추정했습니다. 이는 포토샵(Photoshop) 전문가, 비디오 편집자, 또는 성우를 고용하는 방식 등을 포함합니다. 각 사례에서 AI 없이 유사한 콘텐츠를 제작하는 비용은 수백 달러를 넘지 않는 적당한 수준이었습니다. (저희는 심지어 고용된 무대 배우가 등장하는 비디오가 WIRED의 선거 데이터베이스에서 AI 생성으로 잘못 분류된 경우도 발견했습니다.) 사실, AI나 다른 첨단 도구를 사용하지 않고도 명백히 거짓된 정보를 담은 미디어를 만드는 것은 오래전부터 가능했습니다.

한 비디오는 무대 배우들을 기용하여 미국 부통령이자 민주당 대선 후보인 카말라 해리스(Kamala Harris)가 뺑소니 사건에 연루되었다고 거짓 주장했습니다. 또 다른 비디오는 부통령의 연설 속도를 의도적으로 늦춰 그녀가 말을 더듬는 것처럼 보이게 했습니다. 인도 야당 후보 라훌 간디(Rahul Gandhi)의 편집된 비디오는 그가 현직 나렌드라 모디(Narendra Modi)가 선거에서 이길 것이라고 말하는 것처럼 보이게 했습니다. 원본 영상에서 간디는 상대방이 선거에서 이기지 못할 것이라고 말했지만, "not"이라는 단어를 삭제하기 위한 점프 컷(jump cuts) 편집이 가해졌습니다. 이러한 미디어 콘텐츠는 "저급 가짜(cheap fakes)"(AI가 생성한 "딥페이크(deepfakes)"와 대조적으로)라고 불려왔습니다. 2024년 미국 선거에서는 이러한 저급 가짜가 광범위하게 활용되었습니다. 뉴스 리터러시 프로젝트(News Literacy Project)는 선거와 관련된 알려진 허위 정보를 기록했으며, AI 생성 콘텐츠보다 저급 가짜가 7배 더 많이 사용되었다는 사실을 밝혀냈습니다. 마찬가지로, 다른 국가들에서도 저급 가짜는 상당히 만연했습니다. 인도 기반의 팩트 체커(fact checker)들은 딥페이크에 비해 저급 가짜와 전통적으로 편집된 미디어를 한 자릿수 이상 더 많이 검토했습니다. 방글라데시(Bangladesh)에서는 저급 가짜가 딥페이크보다 20배 이상 더 많이 유포되었습니다. 이러한 통계는 기술적 정교함보다는 메시지의 파급력과 수용자 심리가 허위 정보 확산에 더 큰 영향을 미 미친다는 점을 시사합니다.

미디어의 상당한 주목을 받았던 딥페이크와 실질적으로 유사한 효과를 저급 가짜가 어떻게 초래할 수 있었는지 분석하기 위해 두 가지 사례를 살펴보겠습니다. 도널드 트럼프(Donald Trump) 캠페인이 테일러 스위프트(Taylor Swift) 딥페이크를 사용한 것과 뉴햄프셔(New Hampshire) 예비선거에서 유권자들에게 투표하지 말라고 요청하며 조 바이든(Joe Biden) 미국 대통령을 모방한 음성 복제 로보콜(robocall)입니다.

도널드 트럼프가 트루스 소셜(Truth Social)에 공유한 "Swifties for Trump" 티셔츠를 입은 테일러 스위프트 팬들의 이미지 게시물.
왼쪽 상단: "Swifties for Trump" 티셔츠를 입은 여성들의 AI 생성 이미지 다수가 포함된 게시물, "풍자(satire)" 라벨이 붙어 있음.
오른쪽 상단: "Swifties for Trump" 티셔츠를 입은 트럼프 지지자 제나 피워르치크(Jenna Piwowarczyk)의 실제 이미지.
왼쪽 하단: 미국 국기 앞에 있는 테일러 스위프트의 조작된 이미지, 캡션은 "테일러는 당신이 도널드 트럼프에게 투표하기를 원합니다." 이 이미지가 AI 또는 다른 편집 소프트웨어를 사용하여 생성되었는지는 불분명합니다.
오른쪽 하단: "Swifties for Trump" 티셔츠를 입은 여성들의 두 이미지(하나는 AI 생성, 다른 하나는 실제)가 포함된 트위터(Twitter) 게시물.

트럼프가 스위프트 딥페이크를 활용한 것은 테일러 스위프트가 그를 지지하며, 그녀의 팬들이 그의 집회에 대규모로 참여하고 있다는 인상을 주려는 의도였습니다. 해당 게시물 이후, 많은 언론 매체들은 허위 정보 확산의 원인으로 AI를 지목했습니다. 그러나 AI의 개입 없이도 유사한 이미지를 만들어내는 것은 어렵지 않습니다. 스위프트가 트럼프를 지지하는 모습을 묘사하는 이미지는 그녀의 기존 사진에 트럼프 지지 문구를 포토샵(photoshopping)하여 쉽게 제작할 수 있습니다. 마찬가지로, "Swifties for Trump" 티셔츠를 입은 트럼프 지지자들의 이미지를 얻는 것은 집회에서 무료 티셔츠를 배포하거나, 심지어 트럼프 집회에 참석한 스위프트 팬들에게 선별적으로 접근함으로써 충분히 가능합니다. 실제로, 트럼프가 공유한 이미지 중 두 개는 스위프트 팬이기도 한 트럼프 지지자의 실제 사진이었습니다. 이는 기술의 복잡성보다는 메시지의 사회적 맥락과 수용자의 감성적 반응이 더 중요하다는 점을 보여줍니다.

짧은 혼란을 야기한 또 다른 사건은 뉴햄프셔 예비선거에서 유권자들에게 투표하지 말 것을 종용한 조 바이든 대통령 목소리의 AI 복제였습니다. 바이든 로보콜 이후의 뉴스 헤드라인. 이러한 로보콜에 대한 규제는 수년 전부터 존재해왔습니다. 실제로, 이 특정 로보콜의 가해자는 연방통신위원회(Federal Communications Commission, FCC)로부터 600만 달러의 벌금을 부과받았습니다. FCC는 유사한 공격을 신고할 수 있는 제보 라인(tiplines)을 운영하며, AI 사용 여부와 관계없이 로보콜 관련 규정을 꾸준히 집행하고 있습니다. 해당 로보콜은 정적인 녹음을 사용했기 때문에, AI를 사용하지 않고도 성대모사 전문가를 고용하는 등 거의 동일하게 쉽게 만들 수 있었습니다. 로보콜이 어떤 영향을 미쳤는지도 불분명합니다. 딥페이크의 효과는 수신자가 미국 대통령이 직접 전화해서 예비선거에서 투표하지 말라고 요청한다고 진정으로 믿는지 여부에 달려 있습니다. 정보의 신뢰성은 그 출처의 기술적 완성도뿐만 아니라, 수용자가 가진 기존의 믿음과 편견에 크게 좌우됩니다.

기술 발전과 선거에 영향을 미치려는 행위자들의 전문성이 더 효과적인 AI 오보(disinformation)로 이어지는 것은 시간 문제일까요? 저희는 그렇게 생각하지 않습니다. 다음 섹션에서는 허위 정보 수요를 유발하는 구조적 원인이 AI에 의해 도움받지 않는다는 점을 지적합니다. 그 후, 새로운 도구 출시와 함께 나타났던 AI 오보의 다가오는 물결에 대한 예측의 역사를 살펴볼 것입니다. 이러한 예측은 대부분 현실화되지 않았습니다. 이는 기술적 진보가 항상 사회적 파급력으로 직결되는 것은 아니라는 점을 시사합니다.

### 허위 정보의 수요와 그 심리적 동기

허위 정보 현상은 공급과 수요의 원리를 통해 이해할 수 있습니다. 공급은 클릭을 유도하여 수익을 창출하려는 개인, 자신들의 편이 승리하기를 바라는 당파적 집단, 또는 영향력 행사 작전(influence operations)을 수행하려는 국가 행위자들로부터 비롯됩니다. 지금까지의 개입(interventions)은 거의 전적으로 허위 정보의 공급을 억제하려는 시도였지만, 근본적인 수요는 변화시키지 못했습니다. AI에 대한 초점은 이러한 추세의 가장 최근 사례입니다. AI가 허위 정보 생성 비용을 거의 0에 가깝게 줄이기 때문에, 허위 정보를 공급 문제로만 바라보는 분석가들은 큰 우려를 표합니다. 그러나 허위 정보의 수요를 분석하는 것은 정보가 어떻게 확산되는지, 그리고 어떤 개입이 효과적일 가능성이 있는지 더욱 명확하게 해줄 수 있습니다.

허위 정보의 수요를 면밀히 살펴보면, 사람들은 특정 세계관을 가지고 있는 한, 그 세계관과 일치하는 정보를 적극적으로 찾아내려는 경향이 있다는 것을 알 수 있습니다. 어떤 사람의 세계관에 따라, 해당 정보는 종종 허위 정보이거나, 적어도 다른 세계관을 가진 사람들에게는 허위 정보로 간주될 것입니다. 다시 말해, 성공적인 허위 정보 작전은 메시지의 광범위한 의도에 이미 동의하는 '내집단 구성원(in-group members)'을 주된 대상으로 삼습니다. 그러한 수신자들은 자신들의 세계관에 부합하는 메시지에 대해 회의감이 낮을 수 있으며, 심지어 의도적으로 거짓 정보를 증폭시키려 할 수도 있습니다. 이러한 맥락에서 허위 정보가 효과를 발휘하기 위해 정교한 도구는 필수적이지 않습니다. 오히려, 메시지에 대한 감정적 연결과 기존 신념의 강화가 더 중요합니다. 반대로, AI 사용 여부와 관계없이 동의하지 않는 거짓 정보를 '외집단 구성원(out-group members)'에게 납득시키는 것은 극도로 어렵습니다.

이러한 관점에서 볼 때, AI 허위 정보는 선거에서 유권자들을 대규모로 흔드는 대중적인 묘사와는 상당히 다른 역할을 합니다. 허위 정보의 공급을 늘리는 것은 허위 정보 수요의 역학 관계를 의미 있게 변화시키지 않습니다. 왜냐하면 증가된 공급은 이미 제한된 수용자의 시선을 놓고 경쟁하기 때문입니다. 더욱이, 증가된 허위 정보 공급은 더 넓은 대중을 설득하기보다는 이미 이에 동의하고 허위 정보를 많이 소비하는 소수의 당파적인 사람들에 의해 주로 소비될 가능성이 높습니다. 이는 관련 없는 사건의 미디어, 점프 컷(jump cuts)과 같은 전통적인 비디오 편집, 심지어 비디오 게임 영상과 같은 저급 가짜가 낮은 품질에도 불구하고 허위 정보 전파에 효과적일 수 있는 이유를 설명합니다. 메시지에 이미 동의하는 사람에게 허위 정보를 납득시키는 것이 훨씬 쉽기 때문입니다. 이러한 현상은 확증 편향(confirmation bias)과 인지 부조화(cognitive dissonance)와 같은 심리적 요인과 밀접하게 연관되어 있습니다.

허위 정보 수요에 대한 저희의 분석은 주요 정당들이 유권자 홍보에 유사한 역량을 가지고 있어 유권자들의 (허위) 정보 수요가 이미 포화 상태인, 양극화된 접전 국가에 가장 잘 적용될 수 있습니다. 그럼에도 불구하고, 저희가 아는 한, 2024년 현재까지 선거를 치른 모든 국가에서 AI 허위 정보는 우려했던 것보다 훨씬 적은 영향을 미쳤습니다. 인도에서는 딥페이크가 거짓 정보를 퍼뜨리기보다는 주로 트롤링(trolling)에 사용되었습니다. 인도네시아에서는 AI의 영향이 거짓 정보를 퍼뜨리는 것이 아니라, 당시 후보였고 현재 대통령인 프라보워 수비안토(Prabowo Subianto, 과거 많은 인권 침해 혐의를 받았던 전직 장군)의 이미지를 AI가 생성한 디지털 만화 아바타(digital cartoon avatars)를 사용하여 호감 가는 인물로 보이게 함으로써 부드럽게 하는 것이었습니다. 이는 AI가 정보 조작보다는 이미지 관리나 유권자 참여 유도와 같은 다른 용도로 더 많이 활용되고 있음을 시사합니다.

### AI 허위 정보에 대한 우려가 반복되는 이유: 기술적 환상과 사회적 현실

2024년 선거 주기는 AI 딥페이크가 정치적 허위 정보의 대량 확산으로 이어질 것이라는 광범위한 두려움이 있었던 최초의 시기가 아니었습니다. 2020년 미국 선거 전에도 AI에 대한 놀랍도록 유사한 우려가 제기되었지만, 이러한 우려들은 현실로 나타나지 않았습니다. 새로운 AI 도구의 등장은 종종 정보 조작의 새로운 물결을 불러일으킬 것이라는 염려와 함께 나타납니다.

몇 가지 주목할 만한 사례는 다음과 같습니다:
*   **2019년**: 오픈AI(OpenAI)가 2019년에 GPT-2 시리즈 모델을 공개했을 때, 해당 시리즈 중 가장 강력한 모델의 모델 가중치(model weights) 공개를 보류한 주된 이유 중 하나는 허위 정보를 생성할 수 있는 잠재력 때문이었습니다.
*   **2023년**: 메타(Meta)가 2023년에 LLaMA 모델을 대중에 공개했을 때, 여러 언론 매체들은 이것이 AI 허위 정보의 홍수를 유발할 것이라는 우려를 보도했습니다. 이 모델들은 2019년 오픈AI가 출시한 GPT-2 모델보다 훨씬 더 강력했습니다. 그러나 저희는 LLaMA 또는 다른 대규모 언어 모델(large language models) 사용으로 인한 대규모 유권자 설득의 증거를 발견하지 못했습니다.
*   **224년**: 가장 최근에는 스마트폰에서 AI 이미지 편집 도구의 광범위한 가용성이 비슷한 우려를 불러일으켰습니다. 출처, 1차 출처

사실, 새로운 기술을 사용하여 거짓 정보를 만드는 것에 대한 우려는 1세기 이상 거슬러 올라갑니다. 19세기 후반과 20세기 초반에는 사진 보정(photo retouching) 기술이 등장했습니다. 이는 보정된 사진이 사람들을 속이는 데 사용될 것이라는 우려와 함께 나타났으며, 1912년에는 피사체의 동의 없이 사진 편집을 범죄화하는 법안이 미국에서 발의되기도 했습니다. (상원에서 부결되었습니다.) 이러한 역사적 사례들은 기술 발전이 가져오는 변화에 대한 사회의 지속적인 불안감을 반영합니다.

정치적 허위 정보를 기술적(또는 AI) 문제로만 여기는 것은 해결책이 비교적 다루기 쉬워 보이기 때문에 매력적인 접근 방식입니다. 유해한 기술만 제거할 수 있다면 정보 환경을 획기적으로 개선할 수 있을 것이라는 단순한 논리에 기반합니다. 정보 환경을 개선하려는 목표는 칭찬할 만하지만, 단순히 기술만을 탓하는 것은 문제의 본질을 외면하는 것입니다. 현실적으로는 정치적 양극화(political polarization)가 미디어에 대한 불신을 심화시켰습니다. 사람들은 자신들의 세계관을 확인시켜주는 출처를 선호하며, 자신들의 신념에 부합하는 콘텐츠에 대해 덜 비판적인 태도를 보입니다. 또 다른 중요한 요인은 지난 20년간 저널리즘 수익의 급격한 감소입니다. 이는 주로 전통 미디어에서 소셜 미디어(social media) 및 온라인 광고로의 전환에 의해 주도되었습니다. 그러나 이것은 온라인에서 공유되는 허위 정보의 특정 위협이라기보다는, 사람들이 정보를 찾고 소비하는 방식의 구조적 변화의 결과입니다.

역사학 교수 샘 레보빅(Sam Lebovic)이 지적했듯이, 정보 환경을 개선하는 것은 민주주의와 그 제도를 강화하는 더 큰 프로젝트와 불가분의 관계에 있습니다. 우리의 정보 문제를 "해결"할 수 있는 빠르고 단편적인 기술적 해결책이나 표적 규제는 존재하지 않습니다. AI가 정보 조작의 도구로 활용될 수 있음은 분명하지만, 그 본질적인 원인은 기술 자체보다는 사회적, 정치적, 경제적 맥락에 뿌리를 두고 있습니다. 우리는 정치적 허위 정보에 대해 AI만을 탓하려는 단순한 유혹을 거부하고, 문제의 심각성과 복잡성에 직면해야 합니다. 이는 미디어 리터러시 교육 강화, 독립적인 저널리즘 지원, 그리고 플랫폼의 책임성을 확보하는 등 다각적인 노력을 통해 정보 생태계의 건강성을 회복하는 장기적인 접근 방식을 요구합니다.

---

**정정**: 이 에세이 서론의 이전 버전에서는 대부분의 AI 사용이 기만적이지 않다고 명시했습니다. 사실, 데이터베이스의 78개 항목 중 39개는 비기만적인 AI 사용 사례이며, 만약 정치적 커뮤니케이션으로 제한하고 사기(scams) 사례 4개를 제외한다면 74개 중 39개입니다.

이 에세이는 나이트 수정헌법 제1조 연구소(Knight First Amendment Institute) 웹사이트에도 게재되었습니다. 피드백을 주신 케이티 글렌 배스(Katy Glenn Bass)께 감사드립니다.

**추가 설명**:

1.  허위 정보(misinformation)와 오보(disinformation)라는 용어는 합의된 정의가 부족합니다. 이 글에서 저희는 오해의 소지가 있는 해석적 틀(misleading interpretive framing)의 문제와는 대조적으로, 명백히 거짓된 정보를 지칭하기 위해 허위 정보라는 용어를 사용합니다. 많은 사람들이 외집단 서사(outgroup narratives)를 "허위 정보"로 인식함에도 불구하고, 저희는 허위 정보라는 렌즈가 틀(framing)과 서사(narratives)의 차이를 생각하는 데 유용한 방법이라고 생각하지 않습니다. 저희는 그러한 서사를 지지하기 위해 명백히 거짓된 정보를 사용하는 것에 더 좁게 관심을 가집니다.
2.  전 세계 선거에서 발견된 총 딥페이크 수가 적다는 것은 그 자체로 놀랍습니다. 이 적은 수는 AI 딥페이크가 예상보다 훨씬 작은 문제이거나, 데이터베이스에 누락된 항목이 많다는 것을 나타낼 수 있습니다. 그럼에도 불구하고, 선거 딥페이크를 추적한 다른 데이터베이스들도 총 딥페이크 수에서 비슷한 수치를 보입니다. 예를 들어, 독일 마셜 기금(German Marshall Fund)의 2024년 전 세계 선거 관련 딥페이크 목록은 2023년 9월부터 항목 수집을 시작했음에도 불구하고 133개의 항목을 가지고 있습니다. 에세이에서 더 나아가 언급했듯이, 뉴스 리터러시 프로젝트(News Literacy Project)는 2024년 선거에 대한 알려진 허위 정보를 기록했으며, AI를 사용하지 않은 저급 가짜가 AI 생성 콘텐츠보다 7배 더 많이 사용되었다는 것을 발견했습니다.
3.  데이터셋에는 금융 사기(financial scams)를 저지르는 데 사용된 정치인들의 AI 생성 딥페이크 비디오 4건도 포함되어 있었습니다. 정치적 허위 정보와 비교할 때, 사기는 매우 다른 역학 관계(더 정교한 비디오가 더 설득력 있을 수 있음)와 이해관계(민주주의에 대한 위협보다는 개인적인 금전적 피해를 수반함)를 가집니다. 마찬가지로, 사기를 다루는 것은 다른 개입(interventions)을 필요로 합니다. 예를 들어, 사기꾼 네트워크를 모니터링하고 제거하는 것은 주요 온라인 플랫폼들이 오랫동안 해온 일입니다. 다시 말해, 사기는 저희가 다룰 다른 도구를 가지고 있는 다른 문제이며(일부 플랫폼이 이에 대한 투자를 충분히 하지 않는다는 사실과는 별개로), 이 에세이의 범위를 벗어납니다.
4.  2024년 미국 선거 막바지에 구글(Google)과 오픈AI(OpenAI)는 자신들의 챗봇이 선거 관련 질문에 답하는 것을 제한했습니다. 그러나 퍼플렉시티(Perplexity)와 같은 경쟁사들은 자신들의 제품이 매우 정확하다고 주장하며 그렇게 하지 않았습니다. 챗봇이 질문에 사실적으로 답하거나 답변을 삼가는 경향을 평가하고, 답변의 사실성을 개선하며, 챗봇이 다양한 언어와 맥락에서 작동하도록 보장하는 것은 더 많은 사람들이 질문에 답하기 위해 챗봇을 찾음에 따라 중요한 작업 영역입니다.
5.  분명히 말하자면, 그러한 선전(propaganda)을 AI에 의해 새로 가능해진 것으로 취급해서는 안 됩니다. 그것은 오랜 기술의 점진적인 진화입니다. 실제로, 대선 캠페인을 위한 만화 아바타(cartoon avatars)를 만드는 비용은 AI 유무와 관계없이 미미할 것입니다. 선전의 영향은 그것을 만드는 데 사용된 기술적 방법에 달려 있는 것이 아니라, 경쟁적인 서사(competing narratives)를 고양시킬 언론의 자유에 달려 있습니다.