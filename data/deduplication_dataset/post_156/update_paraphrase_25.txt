2024년 미국 대통령 선거를 앞두고 인공지능(AI)이 만들어낸 허위 정보는 주요한 우려 사항으로 떠올랐습니다. 2024년 1월, 세계경제포럼(World Economic Forum)은 "허위 정보와 오보(misinformation and disinformation)가 세계가 직면한 가장 심각한 단기적 위험"이며, "AI가 사회를 불안정하게 할 수 있는 조작되고 왜곡된 정보를 증폭시킨다"고 발표했습니다. 2024년 선거와 관련된 언론 보도에서도 이와 유사한 논조가 이어졌습니다. 하지만 저희는 기존 연구에서 AI가 허위 정보의 종말을 가져오지 않을 것이라고 예측한 바 있습니다. 메타(Meta)가 대규모 언어 모델인 라마(LLaMA)의 가중치를 공개했을 때, 저희는 이것이 허위 정보의 대규모 확산을 야기하지 않을 것이라고 주장했습니다. 이어서 발표된 글에서는 허위 정보 유포가 영향력 행사 작전(influence operations)의 핵심적인 난관(bottleneck)이며, 생성형 AI(generative AI)가 정보 생성 비용을 낮추지만 유포 비용은 그대로 유지한다고 지적했습니다. 이와 비슷한 견해를 제시한 연구자들도 여럿 존재합니다. 과연 이 두 가지 관점 중 어느 쪽이 현실에 더 부합할까요? 다행히도, 2024년에 전 세계에서 치러진 여러 선거에서 AI가 어떻게 활용되었는지에 대한 증거들이 이 질문에 답하는 데 도움을 줍니다. 수많은 언론사와 연구 기관들은 AI가 생성한 텍스트와 미디어의 실제 사례와 그 영향을 종합적으로 분석했습니다. AI의 잠재력에 대한 추측보다는 현재까지의 실질적인 파급력을 면밀히 검토할 수 있게 된 것입니다. 저희는 WIRED AI 선거 프로젝트(WIRED AI Elections Project)가 2024년 전 세계 선거 기간 동안 정치적 콘텐츠 제작에 AI가 사용된 모든 알려진 사례를 추적한 데이터를 분석했습니다. 각 사례에서 AI가 어떤 목적으로 사용되었는지 확인하고, AI 없이 유사한 콘텐츠를 만드는 데 드는 비용을 추정했습니다. 분석 결과, (1) AI 사용의 절반가량은 기만적인 의도가 없었으며, (2) AI로 생성된 기만적인 콘텐츠는 AI 없이도 저렴하게 재현할 수 있었고, (3) 허위 정보의 공급보다는 수요에 초점을 맞추는 것이 문제를 진단하고 효과적인 개입(interventions) 방안을 모색하는 데 훨씬 유용하다는 결론에 도달했습니다. 물론, AI가 생성한 합성 콘텐츠(synthetic content)는 여러 실제적인 위험을 내포합니다. 예를 들어, 비동의 이미지(non-consensual images) 및 아동 성 착취물(child sexual abuse material) 생성, 그리고 권력자들이 실제이지만 난처하거나 논란이 되는 미디어 콘텐츠를 AI가 만든 것이라고 부인할 수 있게 하는 '거짓말쟁이의 배당금(liar’s dividend)' 등이 있습니다. 이 모든 것은 중요한 사회적 과제입니다. 본 글은 이와는 별개로 정치적 허위 정보라는 특정 문제에 중점을 둡니다.

정보 환경을 개선하는 것은 어렵고 지속적인 노력과 접근 방식이 필요한 과제입니다. AI가 이 문제를 더욱 악화시킬 수 있다고 생각하는 것은 충분히 이해할 만합니다. AI는 거짓 콘텐츠를 조작하는 것을 가능하게 하는 강력한 도구이기 때문입니다. 그러나 AI가 정치적 허위 정보의 양상 자체를 근본적으로 변화시키지는 못했습니다. 역설적으로, AI에 대한 경고는 정보 환경에 대한 우려를 개별적인 해결책이 있는 단편적인 문제로 간주하게 만들 수 있어 안심을 줄 수도 있습니다. 그러나 정보 환경 개선을 위한 진정한 해법은 AI가 생성한 콘텐츠를 억제하는 기술적 조치보다는 사회 구조적, 제도적 변화에 달려 있습니다.

### 디지털 시대의 정보 격류와 구조적 취약점

AI의 등장은 정보 환경에 대한 논의를 촉발했지만, 사실 우리는 이미 디지털 기술의 발전과 함께 정보의 홍수 속에서 살고 있습니다. 소셜 미디어의 확산, 뉴스 소비 방식의 변화, 그리고 끊임없이 쏟아지는 콘텐츠는 개인의 주의력을 분산시키고, 특정 정보에 대한 심층적인 분석을 어렵게 만듭니다. 이러한 구조적 변화는 AI와 무관하게 허위 정보가 확산될 수 있는 비옥한 토양을 제공합니다. 기술적 해결책에만 매달리는 것은 근본적인 문제, 즉 정보에 대한 비판적 사고 능력의 부재, 미디어 리터러시의 부족, 그리고 사회 전반의 양극화와 불신을 간과하는 결과를 낳을 수 있습니다.

### 2024년 선거 딥페이크, 예상 밖의 결과

저희는 WIRED AI 선거 프로젝트(WIRED AI Elections Project)에 기록된 AI 사용 사례 78건 전체를 철저히 분석했습니다. 각 사례를 기만적인 의도(deceptive intent)가 있었는지 여부에 따라 분류한 결과, 놀랍게도 전체 78건 중 39건에서는 기만적인 의도가 전혀 없었습니다. 예를 들어, 정치인이 구사하지 못하는 언어로 연설을 번역하거나, 언론인들이 정부의 보복을 피하기 위해 AI 아바타를 활용한 경우, 또는 시민들에게 비디오 조작의 용이성을 교육하기 위해 딥페이크를 사용한 사례 등이 이에 해당합니다. 심지어 후두염으로 목소리를 잃은 후보자가 유권자들과 소통하기 위해 AI 음성 복제(AI voice cloning) 기술을 투명하게 사용한 경우도 있었습니다. 이는 AI가 단지 기만적인 목적으로만 사용되는 것이 아니라, 정치 캠페인 자료를 개선하거나 정보 전달의 효율성을 높이는 등 긍정적인 방식으로도 활용될 수 있음을 보여줍니다. 물론, 챗봇(chatbot)의 환각(hallucinations)이나 사실성 부족으로 인해 잘못된 정보가 전달되는 경우도 있었으나, 이는 기만적인 의도보다는 기술적 한계에서 비롯된 문제로 분류되었습니다.

### AI 활용의 윤리적 경계와 투명성

AI가 정치 캠페인에 비기만적으로 활용되는 사례가 늘어나면서, 그 윤리적 경계에 대한 논의가 필요해졌습니다. 효율성이나 접근성을 높이기 위한 AI의 투명한 사용은 긍정적일 수 있지만, 유권자 개개인에게 극도로 맞춤화된(hyper-personalized) 메시지를 생성하거나 미묘한 방식으로 여론을 조작할 수 있는 잠재력은 여전히 우려를 낳습니다. AI 기술이 발전함에 따라, 단순히 '기만 여부'를 넘어 '적절한 활용 방식'과 '투명성 원칙'에 대한 명확한 사회적 합의와 윤리적 가이드라인 마련이 시급합니다. 이는 AI가 민주주의 과정에 미칠 수 있는 긍정적, 부정적 영향을 동시에 고려하는 균형 잡힌 접근을 요구합니다.

### AI 없이도 충분히 기만적인 정보 생성 가능

기만적인 정치적 허위 정보를 생산하는 데 AI가 필수적이지 않다는 점은 저희 분석의 핵심 결론 중 하나입니다. AI 사용 의도가 분명히 기만적이었던 39가지 사례를 분석한 결과, AI 없이 유사한 콘텐츠를 만드는 데 드는 비용은 포토샵(Photoshop) 전문가나 비디오 편집자, 성우 등을 고용하는 방식으로도 수백 달러를 넘지 않는 수준이었습니다. 실제로 '저급 가짜(cheap fakes)'라고 불리는, AI나 다른 고급 도구 없이도 제작된 조작된 미디어는 오랫동안 존재해 왔으며, 2024년 미국 선거에서는 AI 생성 콘텐츠보다 7배나 더 많이 사용되었습니다. 인도에서는 딥페이크보다 저급 가짜가 10배 이상, 방글라데시(Bangladesh)에서는 20배 이상 더 만연했습니다. 이는 저급 가짜가 딥페이크와 실질적으로 유사한 효과를 낼 수 있음을 시사합니다. 도널드 트럼프(Donald Trump)가 테일러 스위프트(Taylor Swift) 지지자 이미지를 활용한 사례나 뉴햄프셔(New Hampshire) 예비선거에서 조 바이든(Joe Biden) 대통령의 목소리를 모방한 로보콜(robocall) 사건처럼 많은 언론의 주목을 받은 사례들조차, AI 없이도 충분히 재현 가능하거나 그 영향력이 과장되었을 가능성이 있습니다.

### 플랫폼의 책임과 대응 전략

허위 정보 확산에 있어 소셜 미디어 플랫폼의 역할은 AI 사용 여부와 관계없이 매우 중요합니다. 플랫폼들은 콘텐츠 생성과 유포의 주요 통로 역할을 하므로, 허위 정보에 대한 강력한 콘텐츠 조정 정책과 신속한 대응 시스템을 갖춰야 합니다. AI가 생성한 딥페이크든, 수작업으로 편집된 저급 가짜든, 플랫폼은 이를 식별하고 조치를 취하는 데 있어 기술적, 인적 자원을 투입해야 합니다. 그러나 풍자, 패러디와 실제 허위 정보를 구분하는 복잡성, 그리고 표현의 자유를 침해하지 않으면서 효과적으로 대응하는 것 사이의 균형점은 여전히 어려운 과제로 남아 있습니다. 앞으로는 AI 기반 탐지 기술과 인간 전문가의 협업을 통해 이러한 문제를 해결해나가는 노력이 더욱 필요할 것입니다.

### 허위 정보 확산의 근본 원인: 수요의 역학

허위 정보는 본질적으로 공급과 수요의 역학 관계 속에서 이해되어야 합니다. 지금까지의 많은 개입은 허위 정보의 '공급'을 억제하는 데 초점을 맞췄지만, 이는 문제의 한 측면만을 다루는 것입니다. AI에 대한 과도한 초점 역시 이러한 경향의 최신 사례라 할 수 있습니다. AI가 허위 정보 생성 비용을 거의 제로에 가깝게 낮추는 것은 사실이지만, 허위 정보의 '수요'를 분석하는 것이야말로 정보가 어떻게 확산되고 어떤 개입이 효과적일지 명확히 하는 데 필수적입니다. 사람들은 자신의 기존 세계관과 일치하는 정보를 찾아내고, 그러한 정보에 대해서는 비판적 사고를 덜 하는 경향이 있습니다. 즉, 성공적인 허위 정보 작전은 이미 메시지의 광범위한 의도에 동의하는 '내집단 구성원(in-group members)'을 대상으로 합니다. 이러한 수용자들에게는 정교한 AI 기술이 필요하지 않으며, 오히려 단순한 '저급 가짜'조차도 효과적일 수 있습니다. 반대로, 아무리 정교한 AI 기술을 사용하더라도 동의하지 않는 거짓 정보를 '외집단 구성원(out-group members)'에게 납득시키는 것은 극도로 어렵습니다. 허위 정보의 공급을 늘리는 것만으로는 수요의 역학 관계를 의미 있게 변화시키지 못합니다. 이는 증가된 공급이 결국 동일한 시선을 놓고 경쟁하기 때문이며, 더 넓은 대중을 설득하기보다는 이미 허위 정보를 많이 소비하는 소수의 당파적인 사람들에 의해 주로 소비될 가능성이 높습니다. 이러한 관점은 인도에서 딥페이크가 허위 정보 유포보다는 트롤링(trolling)에 더 많이 사용되거나, 인도네시아에서 AI가 후보자의 이미지를 부드럽게 하는 데 활용된 사례를 설명해줍니다.

### 허위 정보 수요의 심리학적 배경

허위 정보에 대한 수요는 인간의 복잡한 심리적 경향과 깊이 연관되어 있습니다. 확증 편향(confirmation bias)은 사람들이 자신의 기존 신념을 확인시켜주는 정보를 선호하고, 반대되는 정보는 무시하거나 왜곡하는 경향을 의미합니다. 동기화된 추론(motivated reasoning)은 감정적 유대감이나 집단 정체성(group identity)에 따라 사실을 해석하는 경향을 말합니다. 이러한 인지적 편향은 개인이 자신의 세계관과 일치하는 내러티브에 더욱 취약하게 만들며, 심지어 명백한 거짓 정보라 할지라도 이를 수용하고 증폭시키도록 유도할 수 있습니다. 허위 정보는 종종 사실적 정확성보다는 강력한 감정적 호소력이나 집단 소속감을 자극하는 방식으로 작동하기 때문에, 기술적 차단만으로는 이 근본적인 수요를 해결하기 어렵습니다.

### AI 허위 정보 우려의 반복되는 주기와 그 너머

2024년 선거는 AI 딥페이크가 만연한 정치적 허위 정보로 이어질 것이라는 광범위한 두려움이 있었던 첫 번째 시기가 아닙니다. 2020년 미국 선거 이전에도 AI에 대한 유사한 우려가 표명되었지만, 이러한 우려는 현실화되지 않았습니다. 새로운 AI 도구가 출시될 때마다 새로운 허위 정보의 물결을 불러일으킬 것이라는 우려가 반복적으로 제기되어 왔습니다. 2019년 오픈AI(OpenAI)가 GPT-2 모델을 출시했을 때, 허위 정보 생성 가능성 때문에 모델 가중치 공개를 보류했습니다. 2023년 메타(Meta)가 LLaMA 모델을 공개적으로 출시했을 때도 AI 허위 정보의 홍수를 예상하는 우려가 쏟아졌지만, 대규모 유권자 설득의 증거는 나타나지 않았습니다. 가장 최근에는 스마트폰 AI 이미지 편집 도구의 확산이 비슷한 우려를 불러일으켰습니다. 사실, 새로운 기술을 사용하여 거짓 정보를 만드는 것에 대한 우려는 19세기 후반과 20세기 초반 사진 보정(photo retouching) 기술의 등장 때부터 1세기 이상 지속되어 온 현상입니다. 정치적 허위 정보를 기술적(또는 AI) 문제로만 간주하는 것은 해결책이 간단해 보여 매력적일 수 있습니다. 유해한 기술을 되돌릴 수만 있다면 정보 환경을 획기적으로 개선할 수 있을 것이라는 환상에 빠지기 쉽습니다. 그러나 정보 환경을 개선하려는 목표는 칭찬할 만하지만, 단순히 기술을 탓하는 것은 진정한 해결책이 아닙니다. 정치적 양극화(political polarization)는 미디어에 대한 불신을 증가시켰고, 사람들은 자신들의 세계관을 확인시켜주는 출처를 선호하며, 이에 부합하는 콘텐츠에 대해 덜 비판적입니다. 또한, 지난 20년간 저널리즘 수익의 급격한 감소는 정보 환경의 구조적 변화와 맞물려 있습니다. 역사학 교수 샘 레보빅(Sam Lebovic)이 지적했듯이, 정보 환경을 개선하는 것은 민주주의와 그 제도를 강화하는 더 큰 프로젝트와 떼려야 뗄 수 없는 관계에 있습니다. 정보 문제를 "해결"할 수 있는 빠른 기술적 해결책이나 표적 규제는 존재하지 않습니다. 우리는 정치적 허위 정보에 대해 AI만을 비난하려는 단순한 유혹을 거부하고, 문제의 근본적이고 복합적인 성격에 직면해야 합니다.

### 결론: AI를 넘어선 정보 환경의 미래

결론적으로, AI는 허위 정보의 '생성' 비용을 낮출 수 있는 강력한 도구이지만, 그 자체가 허위 정보 확산의 근본 원인은 아닙니다. 오히려 허위 정보가 번성하는 것은 인간의 심리적 취약성, 사회의 양극화, 미디어에 대한 불신, 그리고 독립 저널리즘의 약화와 같은 구조적 문제에 뿌리를 두고 있습니다. AI에 대한 과도한 초점은 이러한 더 깊은 문제들로부터 우리의 주의를 분산시킬 수 있습니다. 따라서 우리는 단기적인 기술적 해결책을 넘어 장기적인 관점에서 정보 환경을 개선하기 위한 다각적인 노력을 기울여야 합니다. 이는 시민들의 미디어 리터러시 교육 강화, 독립적이고 신뢰할 수 있는 저널리즘에 대한 투자 확대, 사회적 대화와 비판적 사고를 장려하는 문화 조성, 그리고 AI 기술 개발 및 활용에 대한 윤리적이고 투명한 가이드라인 마련을 포함합니다. AI는 도구일 뿐이며, 이 도구를 어떻게 활용하고, 그로 인해 발생하는 사회적 도전을 어떻게 관리할지는 결국 우리 사회의 집단적 지혜와 노력에 달려 있습니다. 우리는 AI가 가져올 수 있는 위험을 인지하되, 그 너머에 있는 인간과 사회의 근본적인 문제에 집중함으로써 더욱 건강하고 회복력 있는 정보 환경을 구축할 수 있을 것입니다.