## 2024년 선거와 AI 허위 정보: 실제 영향과 과제에 대한 심층 분석

2024년 미국 대통령 선거 기간 동안 AI가 생성한 허위 정보는 주요 관심사 중 하나였습니다. 2024년 1월, 세계경제포럼(World Economic Forum)은 "허위 정보와 오보(misinformation and disinformation)는 세계가 직면한 가장 심각한 단기적 위험"이며 "AI는 사회를 불안정하게 만들 수 있는 조작되고 왜곡된 정보를 증폭시키고 있다"고 주장했습니다. 2024년 선거에 대한 뉴스 헤드라인도 비슷한 이야기를 전합니다. 대조적으로, 저희는 과거 글에서 AI가 허위 정보의 종말을 초래하지 않을 것이라고 예측했습니다. 메타(Meta)가 오픈 웨이트 대규모 언어 모델(large language model, LLaMA라고 불림)을 출시했을 때, 저희는 이것이 허위 정보의 거대한 파도를 일으키지 않을 것이라고 주장했습니다. 그리고 후속 에세이에서 저희는 허위 정보의 유포가 영향력 행사 작전(influence operations)의 핵심 병목 현상(bottleneck)이며, 생성형 AI(generative AI)가 허위 정보 생성 비용을 줄이지만 유포 비용은 줄이지 못한다고 지적했습니다. 몇몇 다른 연구자들도 비슷한 주장을 했습니다. 이 두 가지 관점 중 어떤 것이 사실에 더 잘 부합할까요? 다행히도, 이 질문에 답하는 데 도움이 될 2024년 전 세계에서 치러진 선거에서 AI가 사용된 증거가 있습니다. 많은 언론 매체와 연구 프로젝트는 AI가 생성한 텍스트와 미디어의 알려진 사례와 그 영향을 취합했습니다. AI의 잠재력에 대해 추측하는 대신, 현재까지의 실제 영향을 살펴볼 수 있습니다. 저희는 2024년 전 세계에서 치러진 선거 기간 동안 정치적 콘텐츠를 생성하기 위한 AI의 알려진 사용 사례를 추적한 WIRED AI 선거 프로젝트(WIRED AI Elections Project)가 수집한 AI 사용의 모든 사례를 분석했습니다. 각 사례에서 저희는 AI가 무엇에 사용되었는지 파악하고 AI 없이 유사한 콘텐츠를 만드는 데 드는 비용을 추정했습니다. 저희는 (1) AI 사용의 절반은 기만적이지 않으며, (2) AI를 사용하여 생성된 기만적인 콘텐츠는 AI 없이도 복제하기 저렴하고, (3) 허위 정보의 공급보다는 수요에 초점을 맞추는 것이 문제를 진단하고 개입(interventions)을 식별하는 훨씬 더 효과적인 방법임을 발견했습니다. 분명히 말하자면, AI가 생성한 합성 콘텐츠(synthetic content)는 많은 실제 위험을 초래합니다. 예를 들어, 사람들의 비동의 이미지(non-consensual images) 및 아동 성 착취물(child sexual abuse material) 생성, 그리고 권력자들이 자신에 대한 실제이지만 당황스럽거나 논란이 되는 미디어 콘텐츠를 AI가 생성한 것이라고 일축할 수 있게 하는 '거짓말쟁이의 배당금(liar’s dividend)'을 가능하게 하는 것입니다. 이 모든 것은 중요한 과제입니다. 이 에세이는 정치적 허위 정보라는 다른 문제에 초점을 맞춥니다.

2024년은 전 세계적으로 70개국 이상에서 선거가 치러지는 '슈퍼 선거의 해'로 불렸습니다. 인도, 유럽연합, 인도네시아 등 주요 지역의 선거는 물론, 미국 대선까지 포함되어 AI가 민주주의에 미칠 영향에 대한 우려가 전례 없이 높아졌습니다. AI 기술의 급속한 발전, 특히 생성형 AI(generative AI)의 등장은 가짜 뉴스(fake news)와 딥페이크(deepfakes)의 확산을 가속화하여 유권자를 혼란에 빠뜨리고 선거의 공정성을 훼손할 것이라는 예측이 지배적이었습니다. 그러나 이러한 우려에도 불구하고, 실제 선거에서 AI의 역할은 예상과는 다른 양상을 보였습니다. 본 글에서는 WIRED AI 선거 프로젝트의 데이터를 기반으로 AI가 선거에 미친 실제 영향을 분석하고, 허위 정보 문제에 대한 보다 효과적인 접근 방식을 제시하고자 합니다.

### AI의 다양한 활용: 기만적 목적을 넘어

놀랍게도, 데이터베이스에 있는 78건 중 39건에서는 기만적인 의도가 없었습니다. AI의 가장 흔한 비기만적인 사용은 선거 운동을 위한 것이었습니다. 후보자나 지지자들이 선거 운동을 위해 AI를 사용했을 때, 대부분의 경우(22건 중 19건) 명백한 의도는 유권자들을 허위 정보로 오도하는 것보다는 선거 운동 자료를 개선하는 것이었습니다. 저희는 정보 환경 개선에 도움이 되었다고 생각하는 딥페이크 사례도 발견했습니다. 베네수엘라에서는 언론인들이 정부에 적대적인 뉴스를 보도할 때 정부의 보복을 피하기 위해 AI 아바타(AI avatars)를 사용했습니다. 미국에서는 애리조나(Arizona)의 지역 뉴스 기관인 애리조나 아젠다(Arizona Agenda)가 딥페이크를 사용하여 시청자들에게 비디오 조작이 얼마나 쉬운지 교육했습니다. 캘리포니아(California)에서는 후두염에 걸린 후보자가 목소리를 잃자, 유권자들과의 만남에서 자신의 목소리로 타이핑된 메시지를 읽기 위해 AI 음성 복제(AI voice cloning)를 투명하게 사용했습니다. 합리적인 사람들은 선거 운동 자료에 AI를 사용하는 것이 합법적인지 또는 적절한 안전 장치(guardrails)가 무엇인지에 대해 의견이 다를 수 있습니다. 그러나 비기만적인 방식으로 선거 운동 자료에 AI를 사용하는 것(예를 들어, AI가 유권자 홍보를 개선하는 도구로 사용될 때)은 유권자들을 흔들기 위해 AI가 생성한 가짜 뉴스(fake news)를 배포하는 것보다 훨씬 덜 문제가 됩니다. 물론, 모든 비기만적인 AI 생성 정치 콘텐츠가 무해한 것은 아닙니다. 3 챗봇은 종종 선거 관련 질문에 잘못된 답변을 합니다. 이는 기만적인 의도보다는 환각(hallucinations) 및 사실성 부족과 같은 챗봇의 한계에서 비롯됩니다. 불행히도, 이러한 한계는 사용자에게 명확하게 전달되지 않아 결함 있는 대규모 언어 모델(LLMs)에 대한 과도한 의존으로 이어집니다.

실제로 AI는 선거 캠페인에서 다양한 방식으로 활용되고 있으며, 그 중 많은 부분이 기만적 목적과는 거리가 멉니다. 예를 들어, AI는 방대한 유권자 데이터를 분석하여 특정 지역의 유권자 성향을 파악하고, 이에 맞는 맞춤형 메시지를 생성하는 데 사용될 수 있습니다. 이는 유권자 참여율을 높이고, 캠페인 자원을 효율적으로 배분하는 데 기여할 수 있습니다. 또한, AI 기반 번역 도구는 다문화 사회에서 언어 장벽을 허물어 후보자가 다양한 배경의 유권자들과 소통하는 데 도움을 줍니다. 시각 장애인을 위한 오디오 설명 생성, 청각 장애인을 위한 실시간 자막 생성 등 접근성을 높이는 데에도 AI가 중요한 역할을 합니다. 이러한 비기만적인 AI 활용 사례는 기술이 민주적 과정의 효율성과 포괄성을 향상시킬 수 있는 잠재력을 가지고 있음을 보여줍니다. 물론, 이러한 도구들 역시 데이터 프라이버시, 편향된 알고리즘, 또는 미묘한 형태의 설득과 같은 새로운 윤리적 문제를 야기할 수 있지만, 이는 직접적인 허위 정보 생성과는 다른 차원의 논의가 필요합니다.

### '저급 가짜(Cheap Fakes)'의 지속적인 위협

AI 사용이 시청자들에게 명백히 거짓된 정보를 믿게 하려는 의도를 가졌던 기만적인 의도의 39가지 사례 각각에 대해, 저희는 AI 없이 유사한 콘텐츠를 만드는 데 드는 비용을 추정했습니다. 예를 들어, 포토샵(Photoshop) 전문가, 비디오 편집자 또는 성우를 고용하는 방식입니다. 각 사례에서 AI 없이 유사한 콘텐츠를 만드는 비용은 수백 달러를 넘지 않는 적당한 수준이었습니다. (저희는 심지어 고용된 무대 배우가 등장하는 비디오가 WIRED의 선거 데이터베이스에서 AI 생성으로 잘못 표시된 것을 발견했습니다.) 사실, AI나 다른 고급 도구를 사용하지 않고도 명백히 거짓된 정보를 담은 미디어를 만드는 것은 오랫동안 가능했습니다. 한 비디오는 무대 배우들을 사용하여 미국 부통령이자 민주당 대선 후보인 카말라 해리스(Kamala Harris)가 뺑소니 사건에 연루되었다고 거짓 주장했습니다. 또 다른 비디오는 부통령의 연설 속도를 늦춰 그녀가 말을 더듬는 것처럼 들리게 했습니다. 인도 야당 후보 라훌 간디(Rahul Gandhi)의 편집된 비디오는 그가 현직 나렌드라 모디(Narendra Modi)가 선거에서 이길 것이라고 말하는 모습을 보여주었습니다. 원본 비디오에서 간디는 그의 상대가 선거에서 이기지 못할 것이라고 말했지만, "not"이라는 단어를 삭제하기 위해 점프 컷(jump cuts)을 사용하여 편집되었습니다. 이러한 미디어 콘텐츠는 "저급 가짜(cheap fakes)"(AI가 생성한 "딥페이크(deepfakes)"와 대조적으로)라고 불려왔습니다. 2024년 미국 선거에서는 저급 가짜가 많이 사용되었습니다. 뉴스 리터러시 프로젝트(News Literacy Project)는 선거에 대한 알려진 허위 정보를 기록했으며, 저급 가짜가 AI 생성 콘텐츠보다 7배 더 많이 사용되었다는 것을 발견했습니다. 마찬가지로, 다른 나라에서도 저급 가짜는 상당히 만연했습니다. 인도 기반의 팩트 체커(fact checker)는 딥페이크에 비해 저급 가짜와 전통적으로 편집된 미디어를 한 자릿수 이상 더 많이 검토했습니다. 방글라데시(Bangladesh)에서는 저급 가짜가 딥페이크보다 20배 이상 더 만연했습니다.

이러한 '저급 가짜'의 효과는 첨단 기술의 부재에도 불구하고 강력합니다. 예를 들어, 2024년 인도네시아 선거에서는 후보자의 이미지를 왜곡하거나 경쟁 후보에 대한 거짓 소문을 퍼뜨리는 데 간단한 이미지 편집과 조작된 텍스트가 광범위하게 사용되었습니다. 이러한 콘텐츠는 소셜 미디어 플랫폼을 통해 빠르게 확산되었으며, 특히 디지털 리터러시가 낮은 계층에서 큰 영향을 미쳤습니다. AI 생성 딥페이크는 제작에 어느 정도의 기술적 지식이나 비용이 필요하지만, '저급 가짜'는 스마트폰 앱이나 기본적인 편집 도구만으로도 누구나 쉽게 만들 수 있다는 점에서 확산 가능성이 더 높습니다. 또한, 인간의 인지적 편향(cognitive biases)은 이러한 저품질의 조작된 콘텐츠에도 쉽게 속아 넘어가게 만듭니다. 사람들은 자신이 믿고 싶은 것을 더 쉽게 믿는 경향이 있으며, 이러한 심리적 취약점은 '저급 가짜'가 딥페이크만큼이나, 혹은 그 이상으로 위험할 수 있는 이유를 설명합니다. 결국 문제는 기술의 정교함이 아니라, 거짓 정보를 믿고 확산시키려는 인간의 의지와 사회적 환경에 있다는 것입니다.

### 허위 정보의 수요와 확산 메커니즘

허위 정보는 공급과 수요의 힘을 통해 볼 수 있습니다. 공급은 클릭을 유도하여 돈을 벌려는 사람들, 자신들의 편이 이기기를 원하는 당파적인 사람들, 또는 영향력 행사 작전(influence operations)을 수행하려는 국가 행위자들로부터 나옵니다. 지금까지의 개입(interventions)은 거의 전적으로 허위 정보의 공급을 억제하려 했지만, 수요는 변화시키지 않았습니다. AI에 대한 초점은 이러한 추세의 가장 최근 사례입니다. AI가 허위 정보 생성 비용을 거의 0으로 줄이기 때문에, 허위 정보를 공급 문제로 보는 분석가들은 매우 우려하고 있습니다.

허위 정보의 수요를 살펴보면, 사람들이 특정 세계관을 가지고 있는 한, 그들은 그 세계관과 일치하는 정보를 찾아낼 것이라는 것을 알 수 있습니다. 어떤 사람의 세계관이 무엇이냐에 따라, 해당 정보는 종종 허위 정보이거나, 적어도 다른 세계관을 가진 사람들에게는 허위 정보로 간주될 것입니다. 다시 말해, 성공적인 허위 정보 작전은 메시지의 광범위한 의도에 이미 동의하는 '내집단 구성원(in-group members)'을 대상으로 합니다. 그러한 수신자들은 자신들의 세계관에 부합하는 메시지에 대해 회의감이 낮을 수 있으며, 심지어 의도적으로 거짓 정보를 증폭시키려 할 수도 있습니다. 이러한 맥락에서 허위 정보가 효과적이기 위해 정교한 도구는 필요하지 않습니다. 반대로, AI 사용 여부와 관계없이 동의하지 않는 거짓 정보를 '외집단 구성원(out-group members)'에게 납득시키는 것은 극도로 어려울 것입니다. 이러한 관점에서 볼 때, AI 허위 정보는 선거에서 유권자들을 흔드는 대중적인 묘사와는 매우 다른 역할을 합니다. 허위 정보의 공급을 늘리는 것은 허위 정보 수요의 역학 관계를 의미 있게 변화시키지 않습니다. 왜냐하면 증가된 공급은 동일한 시선을 놓고 경쟁하기 때문입니다. 더욱이, 증가된 허위 정보 공급은 더 넓은 대중을 설득하기보다는 이미 이에 동의하고 허위 정보를 많이 소비하는 소수의 당파적인 사람들에 의해 주로 소비될 가능성이 높습니다. 이는 관련 없는 사건의 미디어, 점프 컷(jump cuts)과 같은 전통적인 비디오 편집, 심지어 비디오 게임 영상과 같은 저급 가짜가 낮은 품질에도 불구하고 허위 정보 전파에 효과적일 수 있는 이유를 설명합니다. 메시지에 이미 동의하는 사람에게 허위 정보를 납득시키는 것이 훨씬 쉽기 때문입니다.

오늘날 소셜 미디어 플랫폼은 이러한 수요 기반 확산에 결정적인 역할을 합니다. 알고리즘은 사용자가 선호하는 콘텐츠를 우선적으로 보여주어 '필터 버블(filter bubble)'과 '에코 챔버(echo chamber)'를 형성합니다. 이러한 환경에서는 사용자의 기존 신념이 강화되고, 반대되는 정보에 대한 노출은 줄어듭니다. 결과적으로, 특정 세계관을 가진 사람들은 자신들의 편향을 확인시켜주는 허위 정보를 쉽게 받아들이고 공유하게 됩니다. 이러한 구조는 AI가 생성한 콘텐츠든, 단순한 '저급 가짜'든 관계없이 허위 정보가 확산될 수 있는 비옥한 토양을 제공합니다. 따라서 허위 정보 문제의 핵심은 단순히 AI 기술의 발전이 아니라, 인간의 심리적 취약성과 소셜 미디어의 확산 메커니즘이 결합된 복합적인 현상으로 이해해야 합니다.

### AI 허위 정보에 대한 과도한 우려의 역사적 맥락

2024년 선거 주기는 AI 딥페이크가 만연한 정치적 허위 정보로 이어질 것이라는 광범위한 두려움이 있었던 첫 번째 시기가 아니었습니다. 2020년 미국 선거 전에도 AI에 대한 놀랍도록 유사한 우려가 표명되었지만, 이러한 우려는 현실화되지 않았습니다. 새로운 AI 도구의 출시는 종종 새로운 허위 정보의 물결을 불러일으킬 것이라는 우려와 함께 나타납니다.

사실, 새로운 기술이 등장할 때마다 허위 정보에 대한 우려가 증폭되는 것은 역사적으로 반복되어온 현상입니다. 19세기 후반 사진 기술의 발명과 함께 '사진 조작'에 대한 두려움이 생겨났고, 20세기 초 라디오의 등장은 대중 조작의 위험을 제기했습니다. 텔레비전, 그리고 이후 인터넷과 소셜 미디어의 확산 또한 각각 새로운 형태의 허위 정보와 그에 대한 사회적 불안을 동반했습니다. 이러한 사례들은 특정 기술 자체가 허위 정보의 근본 원인이 아니라, 사회적 취약성과 결합될 때 문제가 심화된다는 점을 시사합니다. AI 기술은 콘텐츠 생성의 효율성을 높일 수 있지만, 이는 본질적으로 새로운 문제가 아니라 기존 문제의 확장에 가깝습니다. 중요한 것은 기술적 해결책에만 집중하기보다는, 허위 정보가 번성하는 사회적, 심리적 환경을 이해하고 개선하려는 노력이 필요하다는 것입니다.

### 정보 환경 개선을 위한 다각적인 접근

정보 환경을 개선하는 것은 어렵고 지속적인 과제입니다. 사람들이 AI가 문제를 악화시킨다고 생각할 수 있는 것은 이해할 만합니다. AI는 거짓 콘텐츠를 조작하는 것을 가능하게 합니다. 그러나 그것이 정치적 허위 정보의 판도를 근본적으로 바꾸지는 않았습니다. 역설적으로, AI에 대한 경고는 정보 환경에 대한 우려를 개별적인 해결책을 가진 개별적인 문제로 간주하기 때문에 위안이 될 수 있습니다. 그러나 정보 환경에 대한 해결책은 AI가 생성한 콘텐츠를 억제하는 것보다는 구조적, 제도적 변화에 달려 있습니다.

허위 정보 문제를 효과적으로 해결하기 위해서는 다음의 다각적인 접근 방식이 필요합니다.

1.  **미디어 리터러시 교육 강화**: 모든 연령대의 시민들이 정보를 비판적으로 평가하고, 출처를 확인하며, 조작된 콘텐츠를 식별할 수 있는 능력을 키우는 것이 중요합니다. 학교 교육 과정에 미디어 리터러시를 통합하고, 평생 교육 프로그램을 통해 지속적인 학습 기회를 제공해야 합니다.
2.  **독립 저널리즘 및 팩트 체크 지원**: 신뢰할 수 있는 정보의 공급을 늘리는 것은 허위 정보에 대한 가장 강력한 방어선입니다. 재정적으로 어려움을 겪는 독립 언론사를 지원하고, 팩트 체크 기관의 역량을 강화하여 거짓 정보에 대한 신속하고 정확한 반박을 가능하게 해야 합니다.
3.  **플랫폼 책임 강화**: 소셜 미디어 플랫폼은 자사 플랫폼에서 허위 정보가 확산되는 것을 막기 위한 더 많은 책임을 져야 합니다. 알고리즘 투명성을 높이고, 유해 콘텐츠를 효과적으로 식별하고 제거하며, 이용자에게 정보의 출처와 맥락을 명확히 제공하는 정책을 마련해야 합니다.
4.  **정치적 양극화 완화 노력**: 허위 정보의 수요는 종종 깊어진 정치적 양극화와 관련이 있습니다. 사회적 대화를 촉진하고, 다양한 관점에 대한 이해를 높이며, 공동체의 결속력을 강화하는 노력이 장기적으로 허위 정보의 확산을 줄이는 데 기여할 수 있습니다.

### 결론

AI는 선거 캠페인과 정치적 소통에 혁신적인 도구가 될 수 있지만, 동시에 새로운 형태의 조작 가능성을 열어줍니다. 그러나 2024년 선거에서 나타난 실제 사례들은 AI가 허위 정보의 '판도를 바꾸는' 게임 체인저(game changer)가 아니었음을 보여줍니다. 오히려 '저급 가짜'와 같은 전통적인 조작 방식이 여전히 강력하며, 허위 정보의 확산은 기술적 능력보다는 인간의 심리적 취약성, 사회적 양극화, 그리고 소셜 미디어의 구조적 특성에 더 깊이 뿌리내리고 있습니다.

AI 기술에 대한 과도한 우려에만 매몰되기보다는, 허위 정보라는 복합적인 문제에 대한 이해를 심화하고, 미디어 리터러시 교육, 독립 저널리즘 지원, 플랫폼의 책임 강화, 그리고 사회적 양극화 완화와 같은 다각적이고 구조적인 해결책을 모색해야 합니다. AI는 도구일 뿐이며, 그 도구를 어떻게 사용하고, 그로 인해 발생하는 사회적 영향을 어떻게 관리할지는 결국 우리 사회의 지혜와 노력에 달려 있습니다. 정치적 허위 정보에 대한 AI의 역할을 정확히 이해하고, 현실적인 해결책을 향해 나아가는 것이 중요합니다.

---
정정: 이 에세이 서론의 이전 버전에서는 대부분의 AI 사용이 기만적이지 않다고 명시했습니다. 사실, 데이터베이스의 78개 항목 중 39개는 비기만적인 AI 사용 사례이며, 만약 정치적 커뮤니케이션으로 제한하고 사기(scams) 사례 4개를 제외한다면 74개 중 39개입니다.

이 에세이는 나이트 수정헌법 제1조 연구소(Knight First Amendment Institute) 웹사이트에도 게시되었습니다. 피드백을 주신 케이티 글렌 배스(Katy Glenn Bass)께 감사드립니다.

1 허위 정보(misinformation)와 오보(disinformation)라는 용어는 합의된 정의가 부족합니다. 이 글에서 저희는 오해의 소지가 있는 해석적 틀(misleading interpretive framing)의 문제와는 대조적으로, 명백히 거짓된 정보를 지칭하기 위해 허위 정보라는 용어를 사용합니다. 많은 사람들이 외집단 서사(outgroup narratives)를 "허위 정보"로 인식함에도 불구하고, 저희는 허위 정보라는 렌즈가 틀(framing)과 서사(narratives)의 차이를 생각하는 데 유용한 방법이라고 생각하지 않습니다. 저희는 그러한 서사를 지지하기 위해 명백히 거짓된 정보를 사용하는 것에 더 좁게 관심을 가집니다.
2 전 세계 선거에서 발견된 총 딥페이크 수가 적다는 것은 그 자체로 놀랍습니다. 이 적은 수는 AI 딥페이크가 예상보다 훨씬 작은 문제이거나, 데이터베이스에 누락된 항목이 많다는 것을 나타낼 수 있습니다. 그럼에도 불구하고, 선거 딥페이크를 추적한 다른 데이터베이스들도 총 딥페이크 수에서 비슷한 수치를 보입니다. 예를 들어, 독일 마셜 기금(German Marshall Fund)의 2024년 전 세계 선거 관련 딥페이크 목록은 2023년 9월부터 항목 수집을 시작했음에도 불구하고 133개의 항목을 가지고 있습니다. 에세이에서 더 나아가 언급했듯이, 뉴스 리터러시 프로젝트(News Literacy Project)는 2024년 선거에 대한 알려진 허위 정보를 기록했으며, AI를 사용하지 않은 저급 가짜가 AI 생성 콘텐츠보다 7배 더 많이 사용되었다는 것을 발견했습니다.
3 데이터셋에는 금융 사기(financial scams)를 저지르는 데 사용된 정치인들의 AI 생성 딥페이크 비디오 4건도 포함되어 있었습니다. 정치적 허위 정보와 비교할 때, 사기는 매우 다른 역학 관계(더 정교한 비디오가 더 설득력 있을 수 있음)와 이해관계(민주주의에 대한 위협보다는 개인적인 금전적 피해를 수반함)를 가집니다. 마찬가지로, 사기를 다루는 것은 다른 개입(interventions)을 필요로 합니다. 예를 들어, 사기꾼 네트워크를 모니터링하고 제거하는 것은 주요 온라인 플랫폼들이 오랫동안 해온 일입니다. 다시 말해, 사기는 저희가 다룰 다른 도구를 가지고 있는 다른 문제이며(일부 플랫폼이 이에 대한 투자를 충분히 하지 않는다는 사실과는 별개로), 이 에세이의 범위를 벗어납니다.