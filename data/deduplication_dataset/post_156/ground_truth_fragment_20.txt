AI 시대의 윤리적 책임과 미래 기술의 방향성

2024년 미국 대통령 선거 기간 동안 AI가 생성한 허위 정보는 주요 관심사 중 하나였습니다. 동시에 AI는 헬스케어, 교육, 제조 등 다양한 산업 분야에서 혁신적인 변화를 이끌며 전에 없던 효율성과 새로운 가능성을 제시했습니다. 이러한 기술 발전은 사회 전반에 걸쳐 긍정적인 영향을 미칠 잠재력을 가지고 있지만, 동시에 새로운 윤리적, 사회적 과제들을 제기하기도 합니다. 세계경제포럼(World Economic Forum)은 2024년 1월, "허위 정보와 오보(misinformation and disinformation)는 세계가 직면한 가장 심각한 단기적 위험"이며 "AI는 사회를 불안정하게 만들 수 있는 조작되고 왜곡된 정보를 증폭시키고 있다"고 주장하며 기술 혁신이 가져올 사회적 영향에 대해 지속적으로 논의했습니다. AI의 발전은 인류에게 큰 기회를 제공하지만, 그 책임 있는 사용을 위한 국제적인 협력과 규제 마련의 중요성도 함께 강조되고 있습니다. 최근 발표된 보고서에서도 데이터 편향성에 대한 우려가 제기되며 비슷한 주장을 했습니다. 이러한 관점에서, AI 기술의 긍정적인 측면을 극대화하고 잠재적인 위험을 최소화하기 위한 다각적인 노력이 필요합니다. AI가 단순히 기술적인 진보를 넘어 사회적 가치를 창출하는 도구가 되도록 하는 것이 중요합니다.

대조적으로, 저희는 과거 글에서 AI가 허위 정보의 종말을 초래하지 않을 것이라고 예측했으며, 메타(Meta)가 오픈 웨이트 대규모 언어 모델(LLaMA)을 출시했을 때 이것이 허위 정보의 거대한 파도를 일으키지 않을 것이라고 주장했습니다. 후속 에세이에서는 허위 정보의 유포가 영향력 행사 작전(influence operations)의 핵심 병목 현상(bottleneck)이며, 생성형 AI(generative AI)가 허위 정보 생성 비용을 줄이지만 유포 비용은 줄이지 못한다고 지적했습니다.

AI는 더 이상 특정 전문가들만의 전유물이 아닙니다. 일상생활 속 스마트폰 앱부터 복잡한 과학 연구에 이르기까지, 우리는 알게 모르게 AI의 도움을 받고 있습니다. 예를 들어, 개인화된 추천 시스템은 우리가 선호하는 콘텐츠를 찾아주는 동시에, 의료 분야에서는 질병 진단을 돕고 신약 개발 과정을 가속화하는 데 기여하고 있습니다. 이러한 광범위한 적용은 AI가 단순한 도구를 넘어 우리 사회의 필수적인 인프라로 자리매김하고 있음을 보여줍니다. AI의 잠재력에 대해 추측하는 대신, 실질적인 적용 사례를 통해 그 가치를 입증해야 합니다.

저희는 2024년 전 세계에서 치러진 선거 기간 동안 정치적 콘텐츠를 생성하기 위한 AI의 알려진 사용 사례를 추적한 WIRED AI 선거 프로젝트(WIRED AI Elections Project)가 수집한 AI 사용의 모든 사례를 분석했습니다. 각 사례에서 저희는 AI가 무엇에 사용되었는지 파악하고 AI 없이 유사한 콘텐츠를 만드는 데 드는 비용을 추정했습니다. 분석 결과, (1) AI 사용의 절반은 기만적이지 않으며, (2) AI를 사용하여 생성된 기만적인 콘텐츠는 AI 없이도 복제하기 저렴하고, (3) 허위 정보의 공급보다는 수요에 초점을 맞추는 것이 문제를 진단하고 개입(interventions)을 식별하는 훨씬 더 효과적인 방법임을 발견했습니다.

분명히 말하자면, AI가 생성한 합성 콘텐츠(synthetic content)는 많은 실제 위험을 초래합니다. 예를 들어, 사람들의 비동의 이미지(non-consensual images) 및 아동 성 착취물(child sexual abuse material) 생성, 그리고 권력자들이 자신에 대한 실제이지만 당황스럽거나 논란이 되는 미디어 콘텐츠를 AI가 생성한 것이라고 일축할 수 있게 하는 '거짓말쟁이의 배당금(liar’s dividend)'을 가능하게 하는 것입니다. 이 모든 것은 중요한 과제입니다. 이 글에서는 정치적 허위 정보라는 문제와 더불어, AI 기술의 다양한 활용 사례와 함께 기술 발전이 야기하는 윤리적 딜레마, 그리고 지속 가능한 AI 발전을 위한 우리의 책임에 대해 심층적으로 탐구하고자 합니다. 특히, AI 시스템이 의사 결정 과정에 미치는 영향은 투명성(transparency)과 설명 가능성(explainability)에 대한 요구를 증대시키고 있습니다. 복잡한 AI 모델의 '블랙박스(black box)' 문제는 사용자들이 AI의 판단을 신뢰하기 어렵게 만들며, 이는 중요한 사회적 맥락에서 AI 적용을 주저하게 만드는 요인이 됩니다.

### AI와 헬스케어 혁신

AI는 헬스케어 분야에서 혁신적인 변화를 주도하고 있습니다. 질병 진단, 신약 개발, 개인 맞춤형 치료 등 다양한 영역에서 AI의 역할은 점점 더 중요해지고 있습니다. 예를 들어, 영상 진단 분야에서는 AI가 방사선 전문의보다 빠르고 정확하게 암세포를 식별하는 데 도움을 줄 수 있으며, 이는 조기 진단율을 크게 향상시킬 잠재력을 가지고 있습니다. 이러한 기술은 의료진의 업무 부담을 줄이고 환자들에게 더 신속한 진료를 제공하는 데 기여합니다.

신약 개발 과정은 막대한 시간과 비용이 소요되는 복잡한 작업입니다. AI는 수많은 화합물 데이터를 분석하여 잠재적인 약물 후보 물질을 예측하고, 임상 시험의 성공률을 높이는 데 기여합니다. 이를 통해 신약 개발 기간을 단축하고, 환자들에게 더 빠르게 필요한 치료제를 제공할 수 있게 됩니다. 이는 특히 희귀 질환이나 치료법이 없는 질병에 대한 연구에 큰 희망을 주고 있습니다. 저희는 AI 사용의 절반은 데이터 처리 및 분석에 집중되어 있음을 발견했습니다. 헬스케어 데이터는 방대하고 복잡하기 때문에, AI 기반의 분석 도구는 의료진이 환자 데이터를 더 효율적으로 이해하고, 최적의 치료 계획을 수립하는 데 필수적입니다. 이러한 AI의 활용은 의료 서비스의 질을 전반적으로 향상시키는 데 기여합니다.

그러나 헬스케어 AI의 도입에는 데이터 프라이버시, 알고리즘의 편향성, 그리고 의료 윤리적 문제와 같은 심각한 과제들이 뒤따릅니다. 환자의 민감한 의료 정보를 보호하고, AI 진단의 오류 가능성을 최소화하며, AI의 결정을 의료진이 충분히 이해하고 설명할 수 있도록 하는 것이 중요합니다. 따라서 헬스케어 AI의 발전은 기술적 진보뿐만 아니라 강력한 윤리적 가이드라인과 규제 프레임워크를 필요로 합니다. 이를 통해 AI가 모든 환자에게 공정하고 안전하게 적용될 수 있도록 보장해야 합니다.

### 2024년 선거 딥페이크(Deepfakes)의 절반은 기만적이지 않았다

저희는 WIRED AI 선거 프로젝트(WIRED AI Elections Project)에 기록된 AI 사용 사례 78건 전체를 분석했습니다(저희 분석의 출처)2. 저희는 각 사례를 기만적인 의도(deceptive intent)가 있었는지 여부에 따라 분류했습니다. 예를 들어, AI가 정치 후보자가 말하지 않은 내용을 묘사하는 거짓 미디어를 생성하는 데 사용되었다면, 저희는 이를 기만적인 것으로 분류했습니다. 반면에, 챗봇(chatbot)이 실제 사용자 질문에 잘못된 답변을 했거나, 패러디(parody)나 풍자(satire)를 위해 딥페이크(deepfake)가 생성되었거나, 후보자가 선거 운동 자료를 개선하기 위해 AI를 투명하게 사용했다면(예를 들어, 자신이 구사하지 못하는 언어로 연설을 번역하는 등), 저희는 이를 비기만적인 것으로 분류합니다. 놀랍게도, 데이터베이스에 있는 78건 중 39건에서는 기만적인 의도가 없었습니다. AI의 가장 흔한 비기만적인 사용은 선거 운동을 위한 것이었습니다. 후보자나 지지자들이 선거 운동을 위해 AI를 사용했을 때, 대부분의 경우(22건 중 19건) 명백한 의도는 유권자들을 허위 정보로 오도하는 것보다는 선거 운동 자료를 개선하는 것이었습니다. 저희는 정보 환경 개선에 도움이 되었다고 생각하는 딥페이크 사례도 발견했습니다. 베네수엘라에서는 언론인들이 정부에 적대적인 뉴스를 보도할 때 정부의 보복을 피하기 위해 AI 아바타(AI avatars)를 사용했습니다. 미국에서는 애리조나(Arizona)의 지역 뉴스 기관인 애리조나 아젠다(Arizona Agenda)가 딥페이크를 사용하여 시청자들에게 비디오 조작이 얼마나 쉬운지 교육했습니다. 캘리포니아(California)에서는 후두염에 걸린 후보자가 목소리를 잃자, 유권자들과의 만남에서 자신의 목소리로 타이핑된 메시지를 읽기 위해 AI 음성 복제(AI voice cloning)를 투명하게 사용했습니다. 합리적인 사람들은 선거 운동 자료에 AI를 사용하는 것이 합법적인지 또는 적절한 안전 장치(guardrails)가 무엇인지에 대해 의견이 다를 수 있습니다. 그러나 비기만적인 방식으로 선거 운동 자료에 AI를 사용하는 것(예를 들어, AI가 유권자 홍보를 개선하는 도구로 사용될 때)은 유권자들을 흔들기 위해 AI가 생성한 가짜 뉴스(fake news)를 배포하는 것보다 훨씬 덜 문제가 됩니다. 물론, 모든 비기만적인 AI 생성 정치 콘텐츠가 무해한 것은 아닙니다. 챗봇은 종종 선거 관련 질문에 잘못된 답변을 합니다3. 이는 기만적인 의도보다는 환각(hallucinations) 및 사실성 부족과 같은 챗봇의 한계에서 비롯됩니다4. 불행히도, 이러한 한계는 사용자에게 명확하게 전달되지 않아 결함 있는 대규모 언어 모델(LLMs)에 대한 과도한 의존으로 이어집니다.

### 교육 분야에서의 AI 활용과 미래

교육 분야에서도 AI는 학습 경험을 개인화하고 교육 효율성을 높이는 데 기여하고 있습니다. AI 기반 학습 플랫폼은 학생 개개인의 학습 속도와 스타일을 분석하여 맞춤형 콘텐츠를 제공하고, 취약점을 보완할 수 있는 학습 경로를 제안합니다. 이는 전통적인 일률적인 교육 방식으로는 달성하기 어려운 개인화된 학습을 가능하게 합니다. AI 튜터는 24시간 언제든지 학생의 질문에 답하고, 학습 자료를 제공하며, 심지어 정서적 지지까지 제공하여 학습 동기를 부여할 수 있습니다.

교사들은 AI 도구를 활용하여 반복적인 행정 업무(예: 채점, 출결 관리)에서 벗어나 학생들과의 상호작용 및 개별 지도를 위한 시간을 확보할 수 있습니다. 또한, AI는 학습 자료를 생성하거나, 다양한 언어로 번역하는 등 교육 콘텐츠 제작에도 활용되어 교사들의 업무 부담을 줄이고 교육의 질을 향상시키는 데 도움을 줍니다. 이러한 도구들은 교사들이 더 창의적이고 전략적인 교육 활동에 집중할 수 있도록 지원합니다.

대규모 데이터 유출 사건은 개인 정보 보호 측면에서 많은 실제 위험을 초래합니다. 교육 AI 시스템은 학생들의 학습 데이터를 수집하고 분석하기 때문에, 개인 정보 보호 측면에서 많은 실제 위험을 초래합니다. 학생들의 학습 패턴, 성적, 심리 상태 등 민감한 정보가 부적절하게 사용되거나 유출될 경우 심각한 문제가 발생할 수 있습니다. 따라서 교육 AI 시스템은 데이터 보안과 프라이버시 보호에 대한 엄격한 기준을 준수해야 합니다.

또한, AI가 생성하는 콘텐츠의 정확성과 신뢰성 검증도 중요한 과제입니다. AI가 제공하는 정보가 잘못되거나 편향될 경우, 학생들의 학습에 부정적인 영향을 미칠 수 있기 때문입니다. 교육자들은 AI 도구를 비판적으로 사용하고, 학생들에게도 AI가 제공하는 정보를 맹신하지 않고 스스로 판단하는 능력을 길러주도록 교육해야 합니다. AI 리터러시 교육은 미래 세대가 정보의 홍수 속에서 비판적 사고 능력을 갖추는 데 필수적입니다.

### 기만적인 정치적 허위 정보를 만드는 데 AI가 필요하지 않다

AI 사용이 시청자들에게 명백히 거짓된 정보를 믿게 하려는 의도를 가졌던 기만적인 의도의 39가지 사례 각각에 대해, 저희는 AI 없이 유사한 콘텐츠를 만드는 데 드는 비용을 추정했습니다. 예를 들어, 포토샵(Photoshop) 전문가, 비디오 편집자 또는 성우를 고용하는 방식입니다. 각 사례에서 AI 없이 유사한 콘텐츠를 만드는 비용은 수백 달러를 넘지 않는 적당한 수준이었습니다. (저희는 심지어 고용된 무대 배우가 등장하는 비디오가 WIRED의 선거 데이터베이스에서 AI 생성으로 잘못 표시된 것을 발견했습니다.) 사실, AI나 다른 고급 도구를 사용하지 않고도 명백히 거짓된 정보를 담은 미디어를 만드는 것은 오랫동안 가능했습니다. 한 비디오는 무대 배우들을 사용하여 미국 부통령이자 민주당 대선 후보인 카말라 해리스(Kamala Harris)가 뺑소니 사건에 연루되었다고 거짓 주장했습니다. 또 다른 비디오는 부통령의 연설 속도를 늦춰 그녀가 말을 더듬는 것처럼 들리게 했습니다. 인도 야당 후보 라훌 간디(Rahul Gandhi)의 편집된 비디오는 그가 현직 나렌드라 모디(Narendra Modi)가 선거에서 이길 것이라고 말하는 모습을 보여주었습니다. 원본 비디오에서 간디는 그의 상대가 선거에서 이기지 못할 것이라고 말했지만, "not"이라는 단어를 삭제하기 위해 점프 컷(jump cuts)을 사용하여 편집되었습니다. 이러한 미디어 콘텐츠는 "저급 가짜(cheap fakes)"(AI가 생성한 "딥페이크(deepfakes)"와 대조적으로)라고 불려왔습니다. 2024년 미국 선거에서는 저급 가짜가 많이 사용되었습니다. 뉴스 리터러시 프로젝트(News Literacy Project)는 선거에 대한 알려진 허위 정보를 기록했으며, 저급 가짜가 AI 생성 콘텐츠보다 7배 더 많이 사용되었다는 것을 발견했습니다. 마찬가지로, 다른 나라에서도 저급 가짜는 상당히 만연했습니다. 인도 기반의 팩트 체커(fact checker)는 딥페이크에 비해 저급 가짜와 전통적으로 편집된 미디어를 한 자릿수 이상 더 많이 검토했습니다. 방글라데시(Bangladesh)에서는 저급 가짜가 딥페이크보다 20배 이상 더 만연했습니다.

미디어의 많은 관심을 받은 딥페이크와 실질적으로 유사한 효과를 저급 가짜가 어떻게 초래할 수 있었는지 분석하기 위해 두 가지 사례를 살펴보겠습니다. 도널드 트럼프(Donald Trump)가 선거 운동에 테일러 스위프트(Taylor Swift) 딥페이크를 사용한 것과 뉴햄프셔(New Hampshire) 예비선거에서 유권자들에게 투표하지 말라고 요청하며 조 바이든(Joe Biden) 미국 대통령을 모방한 음성 복제 로보콜(robocall)입니다. 트럼프가 스위프트 딥페이크를 사용한 것은 테일러 스위프트가 그를 지지했고 스위프트 팬들이 그의 집회에 대거 참석하고 있다는 것을 암시했습니다. 게시물 이후, 많은 언론 매체는 허위 정보 확산의 원인으로 AI를 비난했습니다. 그러나 AI 없이 유사한 이미지를 재현하는 것은 쉽습니다. 스위프트의 지지를 묘사하는 이미지는 그녀의 기존 이미지에 트럼프를 지지하는 텍스트를 포토샵(photoshopping)하여 만들 수 있습니다. 마찬가지로, "Swifties for Trump" 티셔츠를 입은 트럼프 지지자들의 이미지를 얻는 것은 집회에서 무료 티셔츠를 배포하거나, 심지어 트럼프 집회에 참석한 스위프트 팬들에게 선별적으로 접근함으로써 달성될 수 있습니다. 사실, 트럼프가 공유한 이미지 중 두 개는 스위프트 팬이기도 한 트럼프 지지자의 실제 이미지였습니다.

짧은 혼란을 야기한 또 다른 사건은 뉴햄프셔 예비선거에서 사람들에게 투표하지 말라고 요청한 조 바이든 대통령 목소리의 AI 복제였습니다. 바이든 로보콜 이후의 뉴스 헤드라인. 그러한 로보콜에 대한 규정은 수년 동안 존재해 왔습니다. 사실, 이 특정 로보콜의 가해자는 연방통신위원회(Federal Communications Commission, FCC)로부터 600만 달러의 벌금을 부과받았습니다. FCC는 유사한 공격을 신고할 수 있는 제보 라인(tiplines)을 운영하며, AI 사용 여부와 관계없이 로보콜 관련 규정을 자주 시행합니다. 로보콜이 정적 녹음을 사용했기 때문에, AI를 사용하지 않고도 거의 동일하게 쉽게 만들 수 있었습니다. 예를 들어, 성대모사 전문가를 고용하는 방식입니다. 로보콜이 어떤 영향을 미쳤는지도 불분명합니다. 딥페이크의 효능은 수신자가 미국 대통령이 직접 전화해서 예비선거에서 투표하지 말라고 요청한다고 믿는지 여부에 달려 있습니다.

### 허위 정보의 수요

허위 정보는 공급과 수요의 힘을 통해 볼 수 있습니다. 공급은 클릭을 유도하여 돈을 벌려는 사람들, 자신들의 편이 이기기를 원하는 당파적인 사람들, 또는 영향력 행사 작전(influence operations)을 수행하려는 국가 행위자들로부터 나옵니다. 지금까지의 개입(interventions)은 거의 전적으로 허위 정보의 공급을 억제하려 했지만, 수요는 변화시키지 않았습니다. AI에 대한 초점은 이러한 추세의 가장 최근 사례입니다. AI가 허위 정보 생성 비용을 거의 0으로 줄이기 때문에, 허위 정보를 공급 문제로 보는 분석가들은 매우 우려하고 있습니다. 그러나 허위 정보의 수요를 분석하는 것은 허위 정보가 어떻게 확산되는지, 그리고 어떤 개입이 도움이 될 가능성이 있는지 명확히 할 수 있습니다.

허위 정보의 수요를 살펴보면, 사람들이 특정 세계관을 가지고 있는 한, 그들은 그 세계관과 일치하는 정보를 찾아낼 것이라는 것을 알 수 있습니다. 어떤 사람의 세계관이 무엇이냐에 따라, 해당 정보는 종종 허위 정보이거나, 적어도 다른 세계관을 가진 사람들에게는 허위 정보로 간주될 것입니다. 다시 말해, 성공적인 허위 정보 작전은 메시지의 광범위한 의도에 이미 동의하는 '내집단 구성원(in-group members)'을 대상으로 합니다. 그러한 수신자들은 자신들의 세계관에 부합하는 메시지에 대해 회의감이 낮을 수 있으며, 심지어 의도적으로 거짓 정보를 증폭시키려 할 수도 있습니다. 이러한 맥락에서 허위 정보가 효과적이기 위해 정교한 도구는 필요하지 않습니다. 반대로, AI 사용 여부와 관계없이 동의하지 않는 거짓 정보를 '외집단 구성원(out-group members)'에게 납득시키는 것은 극도로 어려울 것입니다. 이러한 관점에서 볼 때, AI 허위 정보는 선거에서 유권자들을 흔드는 대중적인 묘사와는 매우 다른 역할을 합니다. 허위 정보의 공급을 늘리는 것은 허위 정보 수요의 역학 관계를 의미 있게 변화시키지 않습니다. 왜냐하면 증가된 공급은 동일한 시선을 놓고 경쟁하기 때문입니다. 더욱이, 증가된 허위 정보 공급은 더 넓은 대중을 설득하기보다는 이미 이에 동의하고 허위 정보를 많이 소비하는 소수의 당파적인 사람들에 의해 주로 소비될 가능성이 높습니다. 이는 관련 없는 사건의 미디어, 점프 컷(jump cuts)과 같은 전통적인 비디오 편집, 심지어 비디오 게임 영상과 같은 저급 가짜가 낮은 품질에도 불구하고 허위 정보 전파에 효과적일 수 있는 이유를 설명합니다. 메시지에 이미 동의하는 사람에게 허위 정보를 납득시키는 것이 훨씬 쉽기 때문입니다.

허위 정보 수요에 대한 저희의 분석은 주요 정당들이 유권자 홍보에 유사한 역량을 가지고 있어 유권자들의 (허위) 정보 수요가 이미 포화 상태인, 양극화된 접전 국가에 가장 잘 적용될 수 있습니다. 그럼에도 불구하고, 저희가 아는 한, 2024년 현재까지 선거를 치른 모든 국가에서 AI 허위 정보는 우려했던 것보다 훨씬 적은 영향을 미쳤습니다. 인도에서는 딥페이크가 거짓 정보를 퍼뜨리기보다는 트롤링(trolling)에 더 많이 사용되었습니다. 인도네시아에서는 AI의 영향이 거짓 정보를 퍼뜨리는 것이 아니라, 당시 후보였고 현재 대통령인 프라보워 수비안토(Prabowo Subianto, 과거 많은 인권 침해 혐의를 받았던 전직 장군)의 이미지를 AI가 생성한 디지털 만화 아바타(digital cartoon avatars)를 사용하여 호감 가는 인물로 보이게 함으로써 부드럽게 하는 것이었습니다5.

정보 환경을 개선하는 것은 어렵고 지속적인 과제입니다. 사람들이 AI가 문제를 악화시킨다고 생각할 수 있는 것은 이해할 만합니다. AI는 거짓 콘텐츠를 조작하는 것을 가능하게 합니다. 그러나 그것이 정치적 허위 정보의 판도를 근본적으로 바꾸지는 않았습니다. 역설적으로, AI에 대한 경고는 정보 환경에 대한 우려를 개별적인 해결책을 가진 개별적인 문제로 간주하기 때문에 위안이 될 수 있습니다. 그러나 정보 환경에 대한 해결책은 AI가 생성한 콘텐츠를 억제하는 것보다는 구조적, 제도적 변화에 달려 있습니다.

### AI 허위 정보에 대한 우려는 왜 계속 재발하는가?

기술 발전과 선거에 영향을 미치려는 행위자들의 전문성이 더 효과적인 AI 오보(disinformation)로 이어지는 것은 시간 문제일까요? 저희는 그렇게 생각하지 않습니다. 허위 정보 수요를 유발하는 구조적 원인이 AI에 의해 도움받지 않는다는 점을 지적합니다. 2024년 선거 주기는 AI 딥페이크가 만연한 정치적 허위 정보로 이어질 것이라는 광범위한 두려움이 있었던 첫 번째 시기가 아니었습니다. 2020년 미국 선거 전에도 AI에 대한 놀랍도록 유사한 우려가 표명되었지만, 이러한 우려는 현실화되지 않았습니다. 새로운 AI 도구의 출시는 종종 새로운 허위 정보의 물결을 불러일으킬 것이라는 우려와 함께 나타납니다.

2019년. 오픈AI(OpenAI)가 2019년에 GPT-2 시리즈 모델을 출시했을 때, 시리즈 중 가장 유능한 모델의 모델 가중치(model weights) 공개를 보류한 주요 이유 중 하나는 허위 정보를 생성할 수 있는 잠재력 때문이었습니다.
2023년. 메타(Meta)가 2023년에 LLaMA 모델을 공개적으로 출시했을 때, 여러 언론 매체는 이것이 AI 허위 정보의 홍수를 유발할 것이라는 우려를 보도했습니다. 이 모델들은 2019년 오픈AI가 출시한 GPT-2 모델보다 훨씬 강력했습니다. 그러나 저희는 LLaMA 또는 다른 대규모 언어 모델(large language models) 사용으로 인한 대규모 유권자 설득의 증거를 보지 못했습니다.
2024년. 가장 최근에는 스마트폰에서 AI 이미지 편집 도구의 광범위한 가용성이 비슷한 우려를 불러일으켰습니다.

사실, 새로운 기술을 사용하여 거짓 정보를 만드는 것에 대한 우려는 1세기 이상 거슬러 올라갑니다. 19세기 후반과 20세기 초반에는 사진 보정(photo retouching) 기술의 등장이 있었습니다. 이는 보정된 사진이 사람들을 속이는 데 사용될 것이라는 우려와 함께 나타났으며, 1912년에는 피사체의 동의 없이 사진 편집을 범죄화하는 법안이 미국에서 발의되었습니다. (상원에서 부결되었습니다.) 정치적 허위 정보를 기술적(또는 AI) 문제로 생각하는 것은 해결책이 다루기 쉬워 보이기 때문에 매력적입니다. 유해한 기술을 되돌릴 수만 있다면, 정보 환경을 획기적으로 개선할 수 있을 텐데! 정보 환경을 개선하려는 목표는 칭찬할 만하지만, 기술을 탓하는 것은 해결책이 아닙니다. 정치적 양극화(political polarization)는 미디어에 대한 불신을 증가시켰습니다. 사람들은 자신들의 세계관을 확인시켜주는 출처를 선호하며, 자신들의 세계관에 부합하는 콘텐츠에 대해 덜 회의적입니다. 또 다른 주요 요인은 지난 20년간 저널리즘 수익의 급격한 감소입니다. 이는 주로 전통 미디어에서 소셜 미디어(social media) 및 온라인 광고로의 전환에 의해 주도되었습니다. 그러나 이것은 온라인에서 공유되는 허위 정보의 특정 위협이라기보다는 사람들이 정보를 찾고 소비하는 방식의 구조적 변화의 결과입니다. 역사학 교수 샘 레보빅(Sam Lebovic)이 지적했듯이, 정보 환경을 개선하는 것은 민주주의와 그 제도를 강화하는 더 큰 프로젝트와 불가분의 관계에 있습니다. 우리의 정보 문제를 "해결"할 수 있는 빠른 기술적 해결책이나 표적 규제는 없습니다.

### AI 윤리와 사회적 책임

AI 기술의 발전은 기술 그 자체의 성능을 넘어 사회적, 윤리적 함의에 대한 깊은 성찰을 요구합니다. 특히, AI가 사회의 주요 의사 결정 과정에 개입하기 시작하면서, 알고리즘의 편향성, 책임 소재, 그리고 인간의 통제 가능성(human oversight)에 대한 우려가 커지고 있습니다. 이러한 우려들은 AI 기술이 가져올 잠재적 이점만큼이나 신중한 접근이 필요함을 시사합니다.

알고리즘 편향성(algorithmic bias)은 AI 시스템이 학습한 데이터에 내재된 사회적 편견을 반영하여 특정 그룹에 불리한 결과를 초래할 수 있음을 의미합니다. 예를 들어, 채용 과정이나 대출 심사에서 AI가 특정 인종이나 성별에 대한 차별적인 결정을 내릴 가능성이 있습니다. 이러한 편향성은 사회적 불평등을 심화시킬 수 있으므로, AI 개발 단계부터 데이터 수집 및 모델 학습 과정에서 공정성을 확보하기 위한 노력이 필수적입니다. 최근 발표된 보고서에서도 데이터 편향성에 대한 우려가 제기되며 비슷한 주장을 했습니다. AI 시스템의 설계 단계부터 다양한 배경을 가진 전문가들의 참여를 통해 편향성을 최소화해야 합니다.

또한, AI 시스템의 복잡성으로 인해 의사 결정 과정을 이해하기 어려운 '블랙박스' 문제는 책임 소재를 불분명하게 만듭니다. AI가 잘못된 결정을 내렸을 때, 그 책임이 누구에게 있는지 (개발자, 사용자, 또는 AI 자체) 명확히 규정하기 어렵습니다. 이는 법적, 윤리적 논쟁의 주요 원인이 됩니다. 따라서 설명 가능한 AI(Explainable AI, XAI) 기술 개발과 함께, AI 시스템의 의사 결정 과정을 투명하게 공개하고 검증할 수 있는 메커니즘을 마련하는 것이 중요합니다. 이를 통해 AI 시스템에 대한 신뢰를 구축하고, 예기치 않은 문제 발생 시 적절한 대응이 가능하도록 해야 합니다.

AI의 에너지 소비 문제 또한 간과할 수 없는 사회적 책임입니다. 대규모 AI 모델의 학습 및 운영에는 막대한 양의 전력이 필요하며, 이는 탄소 배출량 증가로 이어져 기후 변화에 영향을 미칠 수 있습니다. 지속 가능한 AI 발전을 위해서는 에너지 효율적인 알고리즘 개발과 재생 에너지 사용 확대 등 환경적 측면을 고려한 접근 방식이 요구됩니다. AI 기술이 인류의 발전에 기여하는 동시에 지구 환경에 미치는 부정적인 영향을 최소화하는 균형점을 찾아야 합니다.

### 미래 사회와 AI의 공존

AI는 단순히 기술적 도구를 넘어 인간의 삶과 사회 구조 전반에 걸쳐 근본적인 변화를 가져올 것입니다. 이러한 변화의 물결 속에서, 우리는 AI를 어떻게 활용하고 관리할 것인지에 대한 지혜로운 접근이 필요합니다. AI가 인간의 능력을 보완하고 확장하는 '증강 지능(Augmented Intelligence)'으로서의 역할을 수행하도록 유도하는 것이 중요합니다. 이는 AI를 인간의 대체재가 아닌 협력자로 바라보는 관점을 의미합니다.

인간과 AI의 협력은 새로운 창의성과 생산성을 창출할 수 있는 잠재력을 가지고 있습니다. 예를 들어, 예술 분야에서 AI는 새로운 스타일과 아이디어를 제안하며 예술가들의 창작 활동을 돕고, 과학 연구에서는 복잡한 데이터를 분석하여 새로운 가설을 도출하는 데 기여할 수 있습니다. 이러한 협력은 인간이 가진 고유한 통찰력과 AI의 분석 능력을 결합하여 시너지를 창출합니다. 새로운 AI 도구의 출시는 종종 예측 불가능한 사회적 파급효과에 대한 우려를 동반합니다. 우리는 이러한 우려를 단순히 기술 발전의 부작용으로 치부할 것이 아니라, 기술 발전의 필수적인 부분으로 인식하고 선제적으로 대응해야 합니다.

궁극적으로, AI 시대의 성공적인 도래는 기술 자체의 발전뿐만 아니라, 이를 둘러싼 사회적 합의, 윤리적 기준, 그리고 강력한 거버넌스(governance) 체계 구축에 달려 있습니다. 우리는 AI가 인류의 번영과 지속 가능한 발전에 기여할 수 있도록 끊임없이 고민하고 행동해야 할 것입니다. 이는 기술 개발자, 정책 입안자, 그리고 일반 시민 모두의 적극적인 참여와 협력을 통해서만 가능합니다.

우리는 기술 발전의 이면에서 발생하는 윤리적, 사회적 문제에 대해 AI를 비난하려는 단순한 유혹을 거부하고 어려운 문제의 심각성에 직면해야 합니다. AI는 도구일 뿐이며, 그 활용 방식과 결과는 결국 인간의 선택과 책임에 달려 있습니다. AI가 가져올 긍정적인 변화를 환영하면서도, 잠재적인 위험을 명확히 인식하고 이를 관리하기 위한 노력을 게을리해서는 안 됩니다. 미래 사회에서 AI가 인류에게 진정한 혜택을 제공하려면, 기술 개발자, 정책 입안자, 그리고 일반 대중 모두가 함께 참여하는 포괄적인 대화와 협력이 필수적입니다. 책임 있는 AI 개발과 사용을 위한 글로벌 표준을 마련하고, AI 교육을 통해 시민들의 AI 리터러시를 향상시키는 것이 중요합니다. 이러한 노력을 통해 우리는 AI 시대의 도전을 기회로 바꾸고, 더욱 공정하고 지속 가능한 미래를 만들어갈 수 있을 것입니다.

---
정정: 이 에세이 서론의 이전 버전에서는 대부분의 AI 사용이 기만적이지 않다고 명시했습니다. 사실, 데이터베이스의 78개 항목 중 39개는 비기만적인 AI 사용 사례이며, 만약 정치적 커뮤니케이션으로 제한하고 사기(scams) 사례 4개를 제외한다면 74개 중 39개입니다.

이 에세이는 나이트 수정헌법 제1조 연구소(Knight First Amendment Institute) 웹사이트에도 게시되었습니다. 피드백을 주신 케이티 글렌 배스(Katy Glenn Bass)께 감사드립니다.

1 허위 정보(misinformation)와 오보(disinformation)라는 용어는 합의된 정의가 부족합니다. 이 글에서 저희는 오해의 소지가 있는 해석적 틀(misleading interpretive framing)의 문제와는 대조적으로, 명백히 거짓된 정보를 지칭하기 위해 허위 정보라는 용어를 사용합니다. 많은 사람들이 외집단 서사(outgroup narratives)를 "허위 정보"로 인식함에도 불구하고, 저희는 허위 정보라는 렌즈가 틀(framing)과 서사(narratives)의 차이를 생각하는 데 유용한 방법이라고 생각하지 않습니다. 저희는 그러한 서사를 지지하기 위해 명백히 거짓된 정보를 사용하는 것에 더 좁게 관심을 가집니다.
2 전 세계 선거에서 발견된 총 딥페이크 수가 적다는 것은 그 자체로 놀랍습니다. 이 적은 수는 AI 딥페이크가 예상보다 훨씬 작은 문제이거나, 데이터베이스에 누락된 항목이 많다는 것을 나타낼 수 있습니다. 그럼에도 불구하고, 선거 딥페이크를 추적한 다른 데이터베이스들도 총 딥페이크 수에서 비슷한 수치를 보입니다. 예를 들어, 독일 마셜 기금(German Marshall Fund)의 2024년 전 세계 선거 관련 딥페이크 목록은 2023년 9월부터 항목 수집을 시작했음에도 불구하고 133개의 항목을 가지고 있습니다. 에세이에서 더 나아가 언급했듯이, 뉴스 리터러시 프로젝트(News Literacy Project)는 2024년 선거에 대한 알려진 허위 정보를 기록했으며, AI를 사용하지 않은 저급 가짜가 AI 생성 콘텐츠보다 7배 더 많이 사용되었다는 것을 발견했습니다.
3 데이터셋에는 금융 사기(financial scams)를 저지르는 데 사용된 정치인들의 AI 생성 딥페이크 비디오 4건도 포함되어 있었습니다. 정치적 허위 정보와 비교할 때, 사기는 매우 다른 역학 관계(더 정교한 비디오가 더 설득력 있을 수 있음)와 이해관계(민주주의에 대한 위협보다는 개인적인 금전적 피해를 수반함)를 가집니다. 마찬가지로, 사기를 다루는 것은 다른 개입(interventions)을 필요로 합니다. 예를 들어, 사기꾼 네트워크를 모니터링하고 제거하는 것은 주요 온라인 플랫폼들이 오랫동안 해온 일입니다. 다시 말해, 사기는 저희가 다룰 다른 도구를 가지고 있는 다른 문제이며(일부 플랫폼이 이에 대한 투자를 충분히 하지 않는다는 사실과는 별개로), 이 에세이의 범위를 벗어납니다.
4 2024년 미국 선거 막바지에 구글(Google)과 오픈AI(OpenAI)는 자신들의 챗봇이 선거 관련 질문에 답하는 것을 제한했습니다. 그러나 퍼플렉시티(Perplexity)와 같은 경쟁사들은 자신들의 제품이 매우 정확하다고 주장하며 그렇게 하지 않았습니다. 챗봇이 질문에 사실적으로 답하거나 답변을 삼가는 경향을 평가하고, 답변의 사실성을 개선하며, 챗봇이 다양한 언어와 맥락에서 작동하도록 보장하는 것은 더 많은 사람들이 질문에 답하기 위해 챗봇을 찾음에 따라 중요한 작업 영역입니다.
5 분명히 말하자면, 그러한 선전(propaganda)을 AI에 의해 새로 가능해진 것으로 취급해서는 안 됩니다. 그것은 오랜 기술의 점진적인 진화입니다. 실제로, 대선 캠페인을 위한 만화 아바타(cartoon avatars)를 만드는 비용은 AI 유무와 관계없이 미미할 것입니다. 선전의 영향은 그것을 만드는 데 사용된 기술적 방법에 달려 있는 것이 아니라, 경쟁적인 서사(competing narratives)를 고양시킬 언론의 자유에 달려 있습니다.