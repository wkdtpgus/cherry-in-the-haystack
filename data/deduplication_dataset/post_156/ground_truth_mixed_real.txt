2025년 업데이트: AI와 선거 허위 정보 – 지난 2024년의 교훈과 앞으로의 과제

지난 2024년 미국 대통령 선거 기간 동안 AI가 생성한 허위 정보는 주요 관심사 중 하나였습니다. 2024년 1월, 세계경제포럼(World Economic Forum)은 "허위 정보와 오보(misinformation and disinformation)는 세계가 직면한 가장 심각한 단기적 위험"이며 "AI는 사회를 불안정하게 만들 수 있는 조작되고 왜곡된 정보를 증폭시키고 있다"고 주장했습니다. 2024년 선거에 대한 뉴스 헤드라인도 비슷한 우려를 전했습니다. 그러나 저희는 과거 글에서 AI가 허위 정보의 종말을 초래하지 않을 것이라고 예측한 바 있습니다. 메타(Meta)가 자사의 오픈 소스 대규모 언어 모델인 LLaMA를 공개했을 때, 저희는 이것이 허위 정보의 거대한 파도를 일으키지 않을 것이라고 주장했습니다. 그리고 이어진 에세이에서 저희는 허위 정보의 유포가 영향력 행사 작전(influence operations)의 핵심 병목 현상(bottleneck)이며, 생성형 AI(generative AI)가 허위 정보 생성 비용을 줄이지만 유포 비용은 줄이지 못한다고 지적했습니다. 몇몇 다른 연구자들도 이와 유사한 주장을 펼쳤습니다. 그렇다면 이 두 가지 관점 중 어느 것이 현실에 더 부합했을까요? 다행히도, 이 질문에 답하는 데 도움이 될 만한 지난 2024년 전 세계에서 치러진 선거에서 AI가 어떻게 사용되었는지에 대한 증거가 있습니다. 많은 언론 매체와 연구 프로젝트는 AI가 생성한 텍스트와 미디어의 알려진 사례와 그 영향을 취합했습니다. AI의 잠재력에 대해 추측하는 대신, 현재까지의 실제 영향을 객관적으로 살펴볼 수 있습니다.

저희는 2024년 전 세계에서 치러진 선거 기간 동안 정치적 콘텐츠를 생성하기 위한 AI의 알려진 사용 사례를 추적한 WIRED AI 선거 프로젝트(WIRED AI Elections Project)가 수집한 AI 사용의 모든 사례를 분석했습니다. 각 사례에서 저희는 AI가 무엇에 사용되었는지 파악하고 AI 없이 유사한 콘텐츠를 만드는 데 드는 비용을 추정했습니다. 저희는 (1) AI 사용의 절반은 기만적이지 않으며, (2) AI를 사용하여 생성된 기만적인 콘텐츠는 AI 없이도 복제하기 저렴하고, (3) 허위 정보의 공급보다는 수요에 초점을 맞추는 것이 문제를 진단하고 개입(interventions)을 식별하는 훨씬 더 효과적인 방법임을 발견했습니다.

분명히 말하자면, AI가 생성한 합성 콘텐츠(synthetic content)는 여러 실제 위험을 야기합니다. 예를 들어, 사람들의 비동의 이미지(non-consensual images) 및 아동 성 착취물(child sexual abuse material) 생성, 그리고 권력자들이 자신에 대한 실제이지만 당황스럽거나 논란이 되는 미디어 콘텐츠를 AI가 생성한 것이라고 일축할 수 있게 하는 '거짓말쟁이의 배당금(liar’s dividend)'을 가능하게 하는 것입니다. 이 모든 것은 중요한 과제이며, 2025년 현재 이러한 문제에 대한 사회적 인식과 대응 방안 마련의 필요성이 더욱 증대되고 있습니다. 이 에세이는 정치적 허위 정보라는 다른 문제에 초점을 맞춥니다.

1 정보 환경을 개선하는 것은 어렵고 지속적인 과제입니다. 사람들이 AI가 문제를 악화시킨다고 생각할 수 있는 것은 이해할 만합니다. AI는 거짓 콘텐츠를 조작하는 것을 가능하게 합니다. 그러나 그것이 정치적 허위 정보의 판도를 근본적으로 바꾸지는 않았습니다. 역설적으로, AI에 대한 경고는 정보 환경에 대한 우려를 개별적인 해결책을 가진 개별적인 문제로 간주하기 때문에 위안이 될 수 있습니다. 그러나 정보 환경에 대한 해결책은 AI가 생성한 콘텐츠를 억제하는 것보다는 구조적, 제도적 변화에 달려 있습니다. 2025년에도 이 근본적인 통찰은 여전히 유효하며, 새로운 AI 기술의 출현에도 불구하고 본질적인 접근 방식은 변하지 않아야 합니다.

### 2024년 선거 딥페이크(Deepfakes)의 절반은 기만적이지 않았다

저희는 WIRED AI 선거 프로젝트(WIRED AI Elections Project)에 기록된 AI 사용 사례 78건 전체를 분석했습니다(저희 분석의 출처). 2 저희는 각 사례를 기만적인 의도(deceptive intent)가 있었는지 여부에 따라 분류했습니다. 예를 들어, AI가 정치 후보자가 말하지 않은 내용을 묘사하는 거짓 미디어를 생성하는 데 사용되었다면, 저희는 이를 기만적인 것으로 분류했습니다. 반면에, 챗봇(chatbot)이 실제 사용자 질문에 잘못된 답변을 했거나, 패러디(parody)나 풍자(satire)를 위해 딥페이크(deepfake)가 생성되었거나, 후보자가 선거 운동 자료를 개선하기 위해 AI를 투명하게 사용했다면(예를 들어, 자신이 구사하지 못하는 언어로 연설을 번역하는 등), 저희는 이를 비기만적인 것으로 분류합니다. 놀랍게도, 데이터베이스에 있는 78건 중 39건에서는 기만적인 의도가 없었습니다. AI의 가장 흔한 비기만적인 사용은 선거 운동을 위한 것이었습니다. 후보자나 지지자들이 선거 운동을 위해 AI를 사용했을 때, 대부분의 경우(22건 중 19건) 명백한 의도는 유권자들을 허위 정보로 오도하는 것보다는 선거 운동 자료를 개선하는 것이었습니다.

저희는 정보 환경 개선에 도움이 되었다고 생각하는 딥페이크 사례도 발견했습니다. 베네수엘라에서는 언론인들이 정부에 적대적인 뉴스를 보도할 때 정부의 보복을 피하기 위해 AI 아바타(AI avatars)를 사용했습니다. 미국에서는 애리조나(Arizona)의 지역 뉴스 기관인 애리조나 아젠다(Arizona Agenda)가 딥페이크를 사용하여 시청자들에게 비디오 조작이 얼마나 쉬운지 교육했습니다. 캘리포니아(California)에서는 후두염에 걸린 후보자가 목소리를 잃자, 유권자들과의 만남에서 자신의 목소리로 타이핑된 메시지를 읽기 위해 AI 음성 복제(AI voice cloning)를 투명하게 사용했습니다. 합리적인 사람들은 선거 운동 자료에 AI를 사용하는 것이 합법적인지 또는 적절한 안전 장치(guardrails)가 무엇인지에 대해 의견이 다를 수 있습니다. 그러나 비기만적인 방식으로 선거 운동 자료에 AI를 사용하는 것(예를 들어, AI가 유권자 홍보를 개선하는 도구로 사용될 때)은 유권자들을 흔들기 위해 AI가 생성한 가짜 뉴스(fake news)를 배포하는 것보다 훨씬 덜 문제가 됩니다. 최근 2025년에도 이러한 비기만적 AI 활용 사례는 계속 증가하고 있으며, 이는 AI의 도구적 가치를 보여주는 동시에, 기술 사용의 투명성이 얼마나 중요한지를 강조합니다.

물론, 모든 비기만적인 AI 생성 정치 콘텐츠가 무해한 것은 아닙니다. 3 챗봇은 종종 선거 관련 질문에 잘못된 답변을 합니다. 이는 기만적인 의도보다는 환각(hallucinations) 및 사실성 부족과 같은 챗봇의 한계에서 비롯됩니다. 불행히도, 이러한 한계는 사용자에게 명확하게 전달되지 않아 결함 있는 대규모 언어 모델(LLMs)에 대한 과도한 의존으로 이어집니다. 2025년에는 챗봇 기술이 더욱 발전했음에도 불구하고, 여전히 이러한 한계가 존재하며, 특히 선거와 같은 민감한 정보 영역에서는 주의가 요구됩니다.

### 기만적인 정치적 허위 정보를 만드는 데 AI가 필요하지 않다

AI 사용이 시청자들에게 명백히 거짓된 정보를 믿게 하려는 의도를 가졌던 기만적인 의도의 39가지 사례 각각에 대해, 저희는 AI 없이 유사한 콘텐츠를 만드는 데 드는 비용을 추정했습니다. 예를 들어, 포토샵(Photoshop) 전문가, 비디오 편집자 또는 성우를 고용하는 방식입니다. 각 사례에서 AI 없이 유사한 콘텐츠를 만드는 비용은 수백 달러를 넘지 않는 적당한 수준이었습니다. (저희는 심지어 고용된 무대 배우가 등장하는 비디오가 WIRED의 선거 데이터베이스에서 AI 생성으로 잘못 표시된 것을 발견했습니다.) 사실, AI나 다른 고급 도구를 사용하지 않고도 명백히 거짓된 정보를 담은 미디어를 만드는 것은 오랫동안 가능했습니다.

한 비디오는 무대 배우들을 사용하여 미국 부통령이자 민주당 대선 후보인 카말라 해리스(Kamala Harris)가 뺑소니 사건에 연루되었다고 거짓 주장했습니다. 또 다른 비디오는 부통령의 연설 속도를 늦춰 그녀가 말을 더듬는 것처럼 들리게 했습니다. 인도 야당 후보 라훌 간디(Rahul Gandhi)의 편집된 비디오는 그가 현직 나렌드라 모디(Narendra Modi)가 선거에서 이길 것이라고 말하는 모습을 보여주었습니다. 원본 비디오에서 간디는 그의 상대가 선거에서 이기지 못할 것이라고 말했지만, "not"이라는 단어를 삭제하기 위해 점프 컷(jump cuts)을 사용하여 편집되었습니다. 이러한 미디어 콘텐츠는 "저급 가짜(cheap fakes)"(AI가 생성한 "딥페이크(deepfakes)"와 대조적으로)라고 불려왔습니다. 2024년 미국 선거에서는 저급 가짜가 많이 사용되었습니다. 뉴스 리터러시 프로젝트(News Literacy Project)는 선거에 대한 알려진 허위 정보를 기록했으며, 저급 가짜가 AI 생성 콘텐츠보다 7배 더 많이 사용되었다는 것을 발견했습니다. 마찬가지로, 다른 나라에서도 저급 가짜는 상당히 만연했습니다. 인도 기반의 팩트 체커(fact checker)는 딥페이크에 비해 저급 가짜와 전통적으로 편집된 미디어를 한 자릿수 이상 더 많이 검토했습니다. 방글라데시(Bangladesh)에서는 저급 가짜가 딥페이크보다 20배 이상 더 만연했습니다. 이처럼 '저급 가짜'는 여전히 강력한 영향력을 가지며, 2025년에도 그 접근성과 제작 용이성 때문에 계속해서 허위 정보의 주요 수단으로 활용될 가능성이 높습니다.

미디어의 많은 관심을 받은 딥페이크와 실질적으로 유사한 효과를 저급 가짜가 어떻게 초래할 수 있었는지 분석하기 위해 두 가지 사례를 살펴보겠습니다. 도널드 트럼프(Donald Trump)가 선거 운동에 테일러 스위프트(Taylor Swift) 딥페이크를 사용한 것과 뉴햄프셔(New Hampshire) 예비선거에서 유권자들에게 투표하지 말라고 요청하며 조 바이든(Joe Biden) 미국 대통령을 모방한 음성 복제 로보콜(robocall)입니다.

도널드 트럼프가 트루스 소셜(Truth Social)에 공유한 "Swifties for Trump" 티셔츠를 입은 테일러 스위프트 팬들의 이미지 게시물.
왼쪽 상단: "Swifties for Trump" 티셔츠를 입은 여성들의 AI 생성 이미지 다수가 포함된 게시물, "풍자(satire)" 라벨이 붙어 있음.
오른쪽 상단: "Swifties for Trump" 티셔츠를 입은 트럼프 지지자 제나 피워르치크(Jenna Piwowarczyk)의 실제 이미지.
왼쪽 하단: 미국 국기 앞에 있는 테일러 스위프트의 조작된 이미지, 캡션은 "테일러는 당신이 도널드 트럼프에게 투표하기를 원합니다." 이 이미지가 AI 또는 다른 편집 소프트웨어를 사용하여 생성되었는지는 불분명합니다.
오른쪽 하단: "Swifties for Trump" 티셔츠를 입은 여성들의 두 이미지(하나는 AI 생성, 다른 하나는 실제)가 포함된 트위터(Twitter) 게시물.

트럼프가 스위프트 딥페이크를 사용한 것은 테일러 스위프트가 그를 지지했고 스위프트 팬들이 그의 집회에 대거 참석하고 있다는 것을 암시했습니다. 게시물 이후, 많은 언론 매체는 허위 정보 확산의 원인으로 AI를 비난했습니다. 그러나 AI 없이 유사한 이미지를 재현하는 것은 쉽습니다. 스위프트의 지지를 묘사하는 이미지는 그녀의 기존 이미지에 트럼프를 지지하는 텍스트를 포토샵(photoshopping)하여 만들 수 있습니다. 마찬가지로, "Swifties for Trump" 티셔츠를 입은 트럼프 지지자들의 이미지를 얻는 것은 집회에서 무료 티셔츠를 배포하거나, 심지어 트럼프 집회에 참석한 스위프트 팬들에게 선별적으로 접근함으로써 달성될 수 있습니다. 사실, 트럼프가 공유한 이미지 중 두 개는 스위프트 팬이기도 한 트럼프 지지자의 실제 이미지였습니다.

짧은 혼란을 야기한 또 다른 사건은 뉴햄프셔 예비선거에서 사람들에게 투표하지 말라고 요청한 조 바이든 대통령 목소리의 AI 복제였습니다. 바이든 로보콜 이후의 뉴스 헤드라인. 그러한 로보콜에 대한 규정은 수년 동안 존재해 왔습니다. 사실, 이 특정 로보콜의 가해자는 연방통신위원회(Federal Communications Commission, FCC)로부터 600만 달러의 벌금을 부과받았습니다. FCC는 유사한 공격을 신고할 수 있는 제보 라인(tiplines)을 운영하며, AI 사용 여부와 관계없이 로보콜 관련 규정을 자주 시행합니다. 로보콜이 정적 녹음을 사용했기 때문에, AI를 사용하지 않고도 거의 동일하게 쉽게 만들 수 있었습니다. 예를 들어, 성대모사 전문가를 고용하는 방식입니다. 로보콜이 어떤 영향을 미쳤는지도 불분명합니다. 딥페이크의 효능은 수신자가 미국 대통령이 직접 전화해서 예비선거에서 투표하지 말라고 요청한다고 믿는지 여부에 달려 있습니다. 이러한 사례들은 AI의 유무와 관계없이 허위 정보의 근본적인 취약점이 존재하며, 기술적 해결책만으로는 충분하지 않음을 보여줍니다.

기술 발전과 선거에 영향을 미치려는 행위자들의 전문성이 더 효과적인 AI 오보(disinformation)로 이어지는 것은 시간 문제일까요? 저희는 그렇게 생각하지 않습니다. 다음 섹션에서는 허위 정보 수요를 유발하는 구조적 원인이 AI에 의해 도움받지 않는다는 점을 지적합니다. 그런 다음, 새로운 도구 출시와 함께 나타났던 AI 오보의 다가오는 물결에 대한 예측의 역사를 살펴봅니다. 이러한 예측은 실현되지 않았습니다.

### 허위 정보의 수요

허위 정보는 공급과 수요의 힘을 통해 볼 수 있습니다. 공급은 클릭을 유도하여 돈을 벌려는 사람들, 자신들의 편이 이기기를 원하는 당파적인 사람들, 또는 영향력 행사 작전(influence operations)을 수행하려는 국가 행위자들로부터 나옵니다. 지금까지의 개입(interventions)은 거의 전적으로 허위 정보의 공급을 억제하려 했지만, 수요는 변화시키지 않았습니다. AI에 대한 초점은 이러한 추세의 가장 최근 사례입니다. AI가 허위 정보 생성 비용을 거의 0으로 줄이기 때문에, 허위 정보를 공급 문제로 보는 분석가들은 매우 우려하고 있습니다. 그러나 허위 정보의 수요를 분석하는 것은 허위 정보가 어떻게 확산되는지, 그리고 어떤 개입이 도움이 될 가능성이 있는지 명확히 할 수 있습니다.

허위 정보의 수요를 살펴보면, 사람들이 특정 세계관을 가지고 있는 한, 그들은 그 세계관과 일치하는 정보를 찾아낼 것이라는 것을 알 수 있습니다. 어떤 사람의 세계관이 무엇이냐에 따라, 해당 정보는 종종 허위 정보이거나, 적어도 다른 세계관을 가진 사람들에게는 허위 정보로 간주될 것입니다. 다시 말해, 성공적인 허위 정보 작전은 메시지의 광범위한 의도에 이미 동의하는 '내집단 구성원(in-group members)'을 대상으로 합니다. 그러한 수신자들은 자신들의 세계관에 부합하는 메시지에 대해 회의감이 낮을 수 있으며, 심지어 의도적으로 거짓 정보를 증폭시키려 할 수도 있습니다. 이러한 맥락에서 허위 정보가 효과적이기 위해 정교한 도구는 필요하지 않습니다. 반대로, AI 사용 여부와 관계없이 동의하지 않는 거짓 정보를 '외집단 구성원(out-group members)'에게 납득시키는 것은 극도로 어려울 것입니다. 이러한 관점에서 볼 때, AI 허위 정보는 선거에서 유권자들을 흔드는 대중적인 묘사와는 매우 다른 역할을 합니다. 허위 정보의 공급을 늘리는 것은 허위 정보 수요의 역학 관계를 의미 있게 변화시키지 않습니다. 왜냐하면 증가된 공급은 동일한 시선을 놓고 경쟁하기 때문입니다. 더욱이, 증가된 허위 정보 공급은 더 넓은 대중을 설득하기보다는 이미 이에 동의하고 허위 정보를 많이 소비하는 소수의 당파적인 사람들에 의해 주로 소비될 가능성이 높습니다. 이는 관련 없는 사건의 미디어, 점프 컷(jump cuts)과 같은 전통적인 비디오 편집, 심지어 비디오 게임 영상과 같은 저급 가짜가 낮은 품질에도 불구하고 허위 정보 전파에 효과적일 수 있는 이유를 설명합니다. 메시지에 이미 동의하는 사람에게 허위 정보를 납득시키는 것이 훨씬 쉽기 때문입니다. 2025년에 개인화된 콘텐츠 추천 알고리즘이 더욱 강화되면서, 이러한 수요 중심의 역학 관계는 더욱 중요해지고 있습니다.

허위 정보 수요에 대한 저희의 분석은 주요 정당들이 유권자 홍보에 유사한 역량을 가지고 있어 유권자들의 (허위) 정보 수요가 이미 포화 상태인, 양극화된 접전 국가에 가장 잘 적용될 수 있습니다. 그럼에도 불구하고, 저희가 아는 한, 2024년 현재까지 선거를 치른 모든 국가에서 AI 허위 정보는 우려했던 것보다 훨씬 적은 영향을 미쳤습니다. 인도에서는 딥페이크가 거짓 정보를 퍼뜨리기보다는 트롤링(trolling)에 더 많이 사용되었습니다. 인도네시아에서는 AI의 영향이 거짓 정보를 퍼뜨리는 것이 아니라, 당시 후보였고 현재 대통령인 프라보워 수비안토(Prabowo Subianto, 과거 많은 인권 침해 혐의를 받았던 전직 장군)의 이미지를 AI가 생성한 디지털 만화 아바타(digital cartoon avatars)를 사용하여 호감 가는 인물로 보이게 함으로써 부드럽게 하는 것이었습니다. 이처럼 AI는 허위 정보의 직접적인 확산보다는 이미지 개선이나 트롤링 등 다양한 형태로 정치적 맥락에서 활용되는 경향을 보였습니다.

### AI 허위 정보에 대한 우려는 왜 계속 재발하는가?

2024년 선거 주기는 AI 딥페이크가 만연한 정치적 허위 정보로 이어질 것이라는 광범위한 두려움이 있었던 첫 번째 시기가 아니었습니다. 2020년 미국 선거 전에도 AI에 대한 놀랍도록 유사한 우려가 표명되었지만, 이러한 우려는 현실화되지 않았습니다. 새로운 AI 도구의 출시는 종종 새로운 허위 정보의 물결을 불러일으킬 것이라는 우려와 함께 나타납니다.

2019년. 오픈AI(OpenAI)가 2019년에 GPT-2 시리즈 모델을 출시했을 때, 시리즈 중 가장 유능한 모델의 모델 가중치(model weights) 공개를 보류한 주요 이유 중 하나는 허위 정보를 생성할 수 있는 잠재력 때문이었습니다.
2023년. 메타(Meta)가 2023년에 LLaMA 모델을 공개적으로 출시했을 때, 여러 언론 매체는 이것이 AI 허위 정보의 홍수를 유발할 것이라는 우려를 보도했습니다. 이 모델들은 2019년 오픈AI가 출시한 GPT-2 모델보다 훨씬 강력했습니다. 그러나 저희는 LLaMA 또는 다른 대규모 언어 모델(large language models) 사용으로 인한 대규모 유권자 설득의 증거를 보지 못했습니다.
2024년. 가장 최근에는 스마트폰에서 AI 이미지 편집 도구의 광범위한 가용성이 비슷한 우려를 불러일으켰습니다. 출처, 1차 출처

사실, 새로운 기술을 사용하여 거짓 정보를 만드는 것에 대한 우려는 1세기 이상 거슬러 올라갑니다. 19세기 후반과 20세기 초반에는 사진 보정(photo retouching) 기술의 등장이 있었습니다. 이는 보정된 사진이 사람들을 속이는 데 사용될 것이라는 우려와 함께 나타났으며, 1912년에는 피사체의 동의 없이 사진 편집을 범죄화하는 법안이 미국에서 발의되었습니다. (상원에서 부결되었습니다.) 정치적 허위 정보를 기술적(또는 AI) 문제로 생각하는 것은 해결책이 다루기 쉬워 보이기 때문에 매력적입니다. 유해한 기술을 되돌릴 수만 있다면, 정보 환경을 획기적으로 개선할 수 있을 텐데! 정보 환경을 개선하려는 목표는 칭찬할 만하지만, 기술을 탓하는 것은 해결책이 아닙니다.

정치적 양극화(political polarization)는 미디어에 대한 불신을 증가시켰습니다. 사람들은 자신들의 세계관을 확인시켜주는 출처를 선호하며, 자신들의 세계관에 부합하는 콘텐츠에 대해 덜 회의적입니다. 또 다른 주요 요인은 지난 20년간 저널리즘 수익의 급격한 감소입니다. 이는 주로 전통 미디어에서 소셜 미디어(social media) 및 온라인 광고로의 전환에 의해 주도되었습니다. 그러나 이것은 온라인에서 공유되는 허위 정보의 특정 위협이라기보다는 사람들이 정보를 찾고 소비하는 방식의 구조적 변화의 결과입니다. 역사학 교수 샘 레보빅(Sam Lebovic)이 지적했듯이, 정보 환경을 개선하는 것은 민주주의와 그 제도를 강화하는 더 큰 프로젝트와 불가분의 관계에 있습니다. 우리의 정보 문제를 "해결"할 수 있는 빠른 기술적 해결책이나 표적 규제는 없습니다. 우리는 정치적 허위 정보에 대해 AI를 비난하려는 단순한 유혹을 거부하고 어려운 문제의 심각성에 직면해야 합니다.

2025년 현재, 더욱 발전된 멀티모달 AI 모델들이 등장하고 있지만, 이러한 기술적 진보에도 불구하고 허위 정보에 대한 근본적인 사회적, 구조적 문제는 변하지 않았습니다. AI 기술의 발전은 허위 정보 생성의 용이성을 높일 수 있지만, 정보의 확산과 수용은 여전히 인간의 심리, 사회적 맥락, 그리고 미디어 생태계에 깊이 뿌리내린 문제들과 연결되어 있습니다. 따라서 기술 자체에 대한 경고보다는, 정보 소비자의 비판적 사고 능력 함양, 신뢰할 수 있는 저널리즘의 복원, 그리고 건전한 공론장을 위한 제도적 노력이 더욱 중요합니다.

정정: 이 에세이 서론의 이전 버전에서는 대부분의 AI 사용이 기만적이지 않다고 명시했습니다. 사실, 데이터베이스의 78개 항목 중 39개는 비기만적인 AI 사용 사례이며, 만약 정치적 커뮤니케이션으로 제한하고 사기(scams) 사례 4개를 제외한다면 74개 중 39개입니다.

이 에세이는 나이트 수정헌법 제1조 연구소(Knight First Amendment Institute) 웹사이트에도 게시되었습니다. 피드백을 주신 케이티 글렌 배스(Katy Glenn Bass)께 감사드립니다.

1 허위 정보(misinformation)와 오보(disinformation)라는 용어는 합의된 정의가 부족합니다. 이 글에서 저희는 오해의 소지가 있는 해석적 틀(misleading interpretive framing)의 문제와는 대조적으로, 명백히 거짓된 정보를 지칭하기 위해 허위 정보라는 용어를 사용합니다. 많은 사람들이 외집단 서사(outgroup narratives)를 "허위 정보"로 인식함에도 불구하고, 저희는 허위 정보라는 렌즈가 틀(framing)과 서사(narratives)의 차이를 생각하는 데 유용한 방법이라고 생각하지 않습니다. 저희는 그러한 서사를 지지하기 위해 명백히 거짓된 정보를 사용하는 것에 더 좁게 관심을 가집니다.
2 전 세계 선거에서 발견된 총 딥페이크 수가 적다는 것은 그 자체로 놀랍습니다. 이 적은 수는 AI 딥페이크가 예상보다 훨씬 작은 문제이거나, 데이터베이스에 누락된 항목이 많다는 것을 나타낼 수 있습니다. 그럼에도 불구하고, 선거 딥페이크를 추적한 다른 데이터베이스들도 총 딥페이크 수에서 비슷한 수치를 보입니다. 예를 들어, 독일 마셜 기금(German Marshall Fund)의 2024년 전 세계 선거 관련 딥페이크 목록은 2023년 9월부터 항목 수집을 시작했음에도 불구하고 133개의 항목을 가지고 있습니다. 에세이에서 더 나아가 언급했듯이, 뉴스 리터러시 프로젝트(News Literacy Project)는 2024년 선거에 대한 알려진 허위 정보를 기록했으며, AI를 사용하지 않은 저급 가짜가 AI 생성 콘텐츠보다 7배 더 많이 사용되었다는 것을 발견했습니다.
3 데이터셋에는 금융 사기(financial scams)를 저지르는 데 사용된 정치인들의 AI 생성 딥페이크 비디오 4건도 포함되어 있었습니다. 정치적 허위 정보와 비교할 때, 사기는 매우 다른 역학 관계(더 정교한 비디오가 더 설득력 있을 수 있음)와 이해관계(민주주의에 대한 위협보다는 개인적인 금전적 피해를 수반함)를 가집니다. 마찬가지로, 사기를 다루는 것은 다른 개입(interventions)을 필요로 합니다. 예를 들어, 사기꾼 네트워크를 모니터링하고 제거하는 것은 주요 온라인 플랫폼들이 오랫동안 해온 일입니다. 다시 말해, 사기는 저희가 다룰 다른 도구를 가지고 있는 다른 문제이며(일부 플랫폼이 이에 대한 투자를 충분히 하지 않는다는 사실과는 별개로), 이 에세이의 범위를 벗어납니다.
4 2024년 미국 선거 막바지에 구글(Google)과 오픈AI(OpenAI)는 자신들의 챗봇이 선거 관련 질문에 답하는 것을 제한했습니다. 그러나 퍼플렉시티(Perplexity)와 같은 경쟁사들은 자신들의 제품이 매우 정확하다고 주장하며 그렇게 하지 않았습니다. 챗봇이 질문에 사실적으로 답하거나 답변을 삼가는 경향을 평가하고, 답변의 사실성을 개선하며, 챗봇이 다양한 언어와 맥락에서 작동하도록 보장하는 것은 더 많은 사람들이 질문에 답하기 위해 챗봇을 찾음에 따라 중요한 작업 영역입니다. 2025년에는 이러한 챗봇의 한계를 인식하고 사용자들이 더욱 신중하게 정보를 활용하도록 교육하는 것이 중요해졌습니다.
5 분명히 말하자면, 그러한 선전(propaganda)을 AI에 의해 새로 가능해진 것으로 취급해서는 안 됩니다. 그것은 오랜 기술의 점진적인 진화입니다. 실제로, 대선 캠페인을 위한 만화 아바타(cartoon avatars)를 만드는 비용은 AI 유무와 관계없이 미미할 것입니다. 선전의 영향은 그것을 만드는 데 사용된 기술적 방법에 달려 있는 것이 아니라, 경쟁적인 서사(competing narratives)를 고양시킬 언론의 자유에 달려 있습니다.