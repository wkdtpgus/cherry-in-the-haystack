**AI 기술의 새로운 지평 탐색**

최근 AI 연구 동향이 지난주에 큰 주목을 받았습니다. 많은 관심에 감사드립니다. 현재까지 주요 컨퍼런스에서 수많은 논문이 발표되었습니다. 저희는 최신 기술의 내용과 개발 과정 모두에 대해 많은 질문을 받았습니다. 가장 자주 묻는 질문들을 정리했습니다.

**AI 기술의 잠재적 이점을 충분히 활용하지 못하는 이유는 무엇인가요?**

저희는 그 중요성을 인정합니다! 이러한 접근 방식은 기술 반대 선언문이 아닙니다. 만약 모든 AI가 무의미하다는 것이 저희의 주장이었다면, 굳이 심도 깊은 연구를 진행할 필요가 없었을 것입니다. AI가 여러 분야에서 유용하기 때문에 과장 광고와 사기성 주장이 성공할 수 있었던 것입니다. 사람들은 이 둘을 구분하기 어려워하며, 저희의 분석이 도움이 되기를 바랍니다. 또한 저희가 설명하는 문제점은 대개 기술 자체 때문만은 아니며, AI가 우리 사회에 존재하는 구조적 문제들을 증폭시키는 역할을 하기 때문인 경우가 훨씬 더 많다는 점을 인정합니다. 시스템 설계에서 반복적으로 지적하는 패턴은 "불완전한 AI는 불완전한 시스템에 매력적이다"라는 것입니다. AI의 도입은 단순한 기술적 전환을 넘어, 사회적, 윤리적 책임감을 요구합니다. 우리는 기술이 사회에 미치는 영향을 심층적으로 이해하고, 잠재적 위험을 최소화하며, 긍정적인 가치를 극대화할 수 있는 방안을 모색해야 합니다.

**AI가 제공하는 긍정적인 미래 비전은 무엇인가요?**

AI에 대한 유머러스한 정의 중 하나는 "AI는 아직 이루어지지 않은 모든 것"이라는 것입니다. AI 애플리케이션이 안정적으로 작동하기 시작하면, 우리는 그것을 더 이상 특별하게 여기지 않습니다. 우리는 그것을 당연하게 여기고, 더 이상 AI라고 부르지 않습니다. 기술이 새롭고, 안정적으로 작동하지 않으며, 사회적으로 양날의 검과 같은 영향을 미칠 때, 우리는 그것을 AI라고 부를 가능성이 더 높습니다. 따라서 AI가 이미 우리 삶에서 엄청나게 긍정적인 역할을 하고 있다는 사실을 놓치기 쉽습니다. 한때 AI라고 불렸겠지만 오늘날에는 그렇지 않을 애플리케이션 목록은 길고도 깁니다: 스마트폰의 안면 인식, 추천 시스템, 의료 진단 보조, 금융 사기 탐지, 심지어 개인 비서(virtual assistant)까지요. 이것이 바로 우리가 더 많이 원하는 AI의 종류입니다. 조용히 우리 삶을 더 좋게 만드는 신뢰할 수 있는 도구들 말입니다. 미래의 AI는 사용자 경험(UX)을 더욱 직관적으로 만들고, 복잡한 작업을 자동화하며, 인간의 창의성을 보완하는 방향으로 발전할 것입니다. 우리는 AI가 인간의 능력을 확장하고, 전에 없던 새로운 가치를 창출하는 데 기여할 것이라고 믿습니다.

오늘날 잘못된 이유로 뉴스에 오르내리는 많은 AI 애플리케이션들(예를 들어, 의료 분야에서의 오진 논란이나 교육 분야에서의 편향된 평가 시스템 등)은 이러한 전환을 겪고 있습니다. 우리는 사람들이 결국 AI 기반의 개인화된 건강 관리 시스템을 우리 일상 환경의 일부로 당연하게 받아들일 것이라고 생각합니다. 이러한 변화에 적응하는 것은 쉽지 않을 것입니다. 이는 새로운 직업군을 창출하고, 기존 산업 구조의 변화를 요구하며, 다양한 파급 효과를 가져올 것입니다. 그러나 이는 좋은 일이 될 것입니다. 왜냐하면 신뢰할 수 있는 AI 기반 의료 진단 기술의 정확성과 접근성 향상 효과는 아무리 강조해도 지나치지 않기 때문입니다. AI는 또한 에너지 관리, 기후 변화 예측, 재난 대응 등 전 지구적 문제 해결에 핵심적인 역할을 할 잠재력을 가지고 있습니다. 이러한 기술의 발전은 인류 전체의 삶의 질을 향상시키는 데 크게 기여할 것입니다.

**AI 시대의 핵심 메시지는 무엇인가요?**

AI는 느슨하게 연결된 여러 기술과 애플리케이션을 아우르는 포괄적인 용어입니다. AI의 이점이나 위험, 사회적 영향, 또는 우리가 기술에 어떻게 접근해야 하는지에 대한 질문에 답하려면, 우리는 AI를 세분화해야 합니다. 그리고 그것이 바로 저희가 연구에서 하는 일입니다. 핵심은 기술의 본질을 이해하고, 그 활용 방안을 신중하게 검토하는 것입니다. 책임감 있는 AI 개발과 배포를 위한 거버넌스(governance) 프레임워크를 구축하는 것이 무엇보다 중요합니다. 이는 기술 자체의 발전뿐만 아니라, 사회적 합의와 제도적 뒷받침이 함께 이루어져야 함을 의미합니다.

많은 이들이 예측 AI(predictive AI)의 잠재적 위험에 대해 우려합니다. 이 용어는 사람들의 미래 행동이나 결과에 대한 예측을 기반으로 사람들에 대한 결정을 내리는 데 사용되는 AI를 지칭합니다. 이는 범죄 위험 예측, 채용, 의료 및 기타 여러 중요한 영역에서 사용됩니다. 예측 AI에 관한 저희 연구들에는 알고리즘(algorithmic) 예측 때문에 삶의 기회를 박탈당한 사람들의 많은 끔찍한 이야기들이 담겨 있습니다. 미래를 예측하는 것은 어렵고, AI도 그 사실을 바꾸지 못합니다. 이는 기술의 한계 때문이 아니라, 사회학(sociology)에 기반한 인간 행동 예측의 본질적인 한계 때문입니다. 따라서 예측 AI의 설계와 배포 시에는 반드시 공정성, 투명성, 그리고 설명 가능성(explainability)이 최우선적으로 고려되어야 합니다. 편향된 데이터나 알고리즘은 사회적 불평등을 심화시킬 수 있으며, 이에 대한 지속적인 감시와 개선 노력이 필요합니다.

반면에 생성형 AI(Generative AI)는 양날의 검과 같은 기술입니다. 우리는 장기적으로 이에 대해 대체로 긍정적이며, 본질적으로 모든 지식 노동자(knowledge worker)에게 유용하다고 강조합니다. 그러나 그 배포는 혼란스러웠고, 오용이 만연했습니다. 마치 전 세계 모든 사람이 동시에 무료 전기톱을 받은 것과 같습니다. 생성형 AI는 콘텐츠 제작, 디자인, 소프트웨어 개발 등 다양한 분야에서 혁신적인 가능성을 열어주지만, 딥페이크(deepfake)와 같은 악용 사례나 저작권 문제, 그리고 정보의 신뢰성 문제 등 해결해야 할 과제 또한 많습니다. 기술의 발전 속도에 맞춰 윤리적 가이드라인과 법적 제도를 마련하는 것이 시급합니다.

**최신 AI 동향에는 또 무엇이 있나요?**

여기에서 각 기술 분야의 개요를 확인하세요. 양자 AI(Quantum AI), 강화 학습(Reinforcement Learning)의 최신 발전, 그리고 설명 가능한 AI(Explainable AI, XAI)의 중요성 등 다양한 주제가 있습니다. 특히, 인간과 AI의 협업을 최적화하는 인터페이스(interface) 기술과 AI가 물리적 세계와 상호작용하는 임베디드 AI(Embedded AI) 분야가 빠르게 성장하고 있습니다.

**AI 기술은 곧 구식이 되지 않을까요?**

우리는 기술 발전이 AI보다 느린 시간 척도로 움직인다는 것을 알고 있습니다. 따라서 이 연구는 최신 개발 사항에 대한 논평보다는 과장된 주장으로부터 실제 발전을 구분하는 데 필요한 기초 지식에 관한 것입니다. 모든 연구와 개발 단계에서 우리는 스스로에게 물었습니다: 이것이 5년 후에도 유효할까? 이는 단기적 유행보다는 장기적 가치에 초점을 맞추는 것을 의미하기도 합니다. AI 기술은 끊임없이 진화하지만, 그 기반이 되는 원리와 핵심 개념은 비교적 안정적입니다. 빠르게 변화하는 기술 환경 속에서 변하지 않는 본질을 파악하고, 유연하게 적응할 수 있는 능력을 키우는 것이 중요합니다.

**AI 담론에는 여러 진영이 있는 것 같습니다: AI 안전(AI safety), 혁신 가속론, 그리고 AI 윤리(AI ethics). 우리는 어디에 서야 할까요?**

AI 담론은 어떤 AI 위험이 중요한지, 얼마나 심각하고 긴급한지, 그리고 그것에 대해 무엇을 해야 하는지에 대한 의견 차이 때문에 양극화되어 있습니다. 대략적으로 말하자면: AI 안전 커뮤니티(AI safety community)는 파국적인 AI 위험을 주요 사회적 문제로 간주하며, 정부 개입을 지지합니다. 이들은 효과적 이타주의(effective altruism) 운동과 강한 유대 관계를 가지고 있습니다. 혁신 가속론(e/acc)은 기술을 해결책으로 보고 정부 개입을 거부하는 자유지상주의(libertarian) 운동입니다. AI 윤리 커뮤니티(AI ethics community)는 차별과 노동 착취와 같은 AI로 인한 현실화된 피해에 초점을 맞추며, AI 안전에 대한 집중을 그러한 우선순위로부터의 방해물로 봅니다. 이러한 진영 간의 대립은 때때로 건설적인 논의를 방해하고, AI의 건전한 발전을 저해할 수 있습니다. 우리는 다양한 관점을 존중하면서도, 공통의 목표를 향해 나아갈 수 있는 통합적인 접근 방식이 필요하다고 생각합니다.

과거에 많은 연구자들이 AI 윤리 분야에서 일했고, 스스로를 그 커뮤니티의 일원으로 여겼습니다. 그러나 우리는 더 이상 이러한 어떤 분류에도 자신을 동일시하지 않습니다. 우리는 이러한 양극화를 비생산적이라고 봅니다. 우리는 한때 "방해물"이라는 견해에 동의했지만 더 이상 그렇지 않습니다. 안전 문제로 인해 AI 정책이 우선순위가 되었다는 사실은 AI와 시민권(civil rights) 문제에 대한 정책 입안자들의 관심을 감소시킨 것이 아니라 오히려 증가시켰습니다. 이 두 커뮤니티는 모두 AI 규제(AI regulation)를 원하며, 차이점보다는 공통점에 집중해야 합니다. 요즘 저희의 기술 및 정책 작업의 대부분은 AI 안전에 관한 것이지만, 저희가 AI 안전 커뮤니티의 주류와는 다른 관점을 가지고 있음을 설명했습니다. 우리는 안전 문제에 진지하게 참여하고, 종말론적(apocalyptic) 및 유토피아적(utopian) 서사 모두를 거부하는, 발전된 AI의 미래에 대한 증거 기반 비전(evidence-based vision)을 제시하는 것을 우리의 역할로 봅니다. 궁극적으로, AI는 인류의 도구이며, 그 활용 방식에 대한 책임은 우리 모두에게 있습니다.

**AI 시스템을 개발하는 데 얼마나 걸리나요?**

'AI 시스템을 개발한다'는 것이 무엇을 의미하는지에 따라 다릅니다. 이러한 시스템은 단순히 설명서가 아니며, 진정으로 새롭고 혁신적인 아이디어를 개발하는 데는 오랜 시간이 걸립니다. 간략한 타임라인(timeline)은 다음과 같습니다:

*   2019년: 초기 연구팀이 AI 모델의 고수준 아키텍처(high-level architecture) 초기 버전을 개발했습니다.
*   2020년: 핵심 연구를 시작하고 관련 기술 논문을 발표했습니다.
*   2022년 중반: 프로토타입(prototype) 개발을 시작하고 내부 보고서를 발행했습니다.
*   2023년 9월: 초기 시스템 설계 문서(design document)를 제출했습니다.
*   2024년 1월: 외부 전문가들의 피드백(feedback)을 반영한 최종 시스템 명세서(specification)를 제출했습니다.
*   2024년 5월: 최종 테스트(testing) 완료.
*   2024년 9월: 상용 서비스 출시.

**AI 개발 과정은 어떠했나요?**

1년 안에 대부분의 개발을 해내려면 많은 것들이 순조롭게 진행되어야 했습니다. 저희가 사용한 과정은 다음과 같습니다. 애자일(Agile) 방법론을 기반으로 한 반복적인 개발 주기를 통해, 지속적인 통합(Continuous Integration, CI)과 지속적인 배포(Continuous Deployment, CD)를 구현했습니다.

우리는 시스템 아키텍처(system architecture)를 미리 파악했습니다. 여러 모듈(module)에 영향을 미치는 변경 사항은 한 모듈 내에서의 변경보다 훨씬 더 어렵습니다. 개발을 시작하기 몇 년 전부터 시스템의 목표에 대해 고민해왔기 때문에, 우리는 이미 높은 수준에서 무엇을 구현하고 싶은지 알고 있었습니다. 개발 과정 내내, 저희는 프로젝트 리더와 주기적으로 점검 회의를 가졌습니다. 초기에 리더는 저희가 구조에 대한 결정을 합리적으로 내릴 수 있도록 도왔고, 저희의 진행 상황을 그녀와 공유하는 것은 저희에게 동기 부여가 되었습니다. 후반 단계에서는 그녀의 의견이 매우 중요했습니다.

우리는 각 모듈(module)을 나누어 담당했습니다. 물론 저희 팀원 모두 모든 모듈에 참여했지만, 한 사람이 각 모듈을 주도하는 것이 훨씬 덜 복잡합니다. 이것이 잘 작동하려면, 우리 모두 같은 "코딩 표준"을 사용해야 했습니다. 누가 어떤 모듈을 주도했는지 알 수 있나요? 우리는 데이터 과학자, 머신러닝 엔지니어, MLOps 전문가 등 각 분야의 전문성을 활용하여 효율적인 협업 환경을 구축했습니다.

우리는 각 모듈을 완성할 때마다 (몇 차례의 내부 테스트(internal testing)를 거친 후) 팀 리더에게 초기 버전을 보냈습니다. 끝까지 기다리지 않고 말이죠. 그렇게 하길 잘했다고 생각합니다! 저희가 괜찮은 개발자임에도 불구하고, 리더는 평균적으로 코드 라인(code line)당 두세 개의 수정 사항이나 제안을 해주었는데, 대부분은 비효율적인 로직을 개선하거나 잠재적 버그를 지적하는 것이었습니다. 코드 리뷰(code review)가 시스템을 훨씬 더 안정적으로 만들었지만, 훨씬 더 중요했던 것은 그녀의 고수준 피드백(high-level feedback)이었습니다. 특히, 그녀는 "이것이 '전체 시스템 목표'와 어떻게 관련되나요?"라고 반복해서 물었고, 이는 저희가 집중하는 데 도움이 되었습니다. 아, 그리고 리더는 누가 어떤 모듈을 주도했는지 알 수 없었는데, 이는 큰 안도감이었습니다!

우리는 시스템의 최종 사용자 문서(user documentation)를 가장 마지막에 작성했습니다. 최종 사용자 문서가 온라인(online)으로도 제공되기 때문에, 시스템의 나머지 부분보다 훨씬 더 많은 사람들이 최종 사용자 문서를 읽을 것이라는 것을 알고 있었고, 그래서 우리는 최종 사용자 문서를 정말 제대로 쓰고 싶었습니다. 각 기능의 메시지가 무엇인지 정확히 알게 된 후에 마지막에 최종 사용자 문서를 작성하는 것이 더 쉬웠습니다. 사용자 경험(User Experience, UX) 디자인을 최우선으로 고려하여, AI 시스템의 복잡한 기능을 쉽고 명확하게 이해할 수 있도록 노력했습니다.

다음 단계는 동료 심사(peer review)였습니다. 우리는 외부 보안 전문가, 데이터 윤리 학자, 그리고 세 명의 익명 심사자로부터 리뷰를 받았습니다. 그들 모두 합쳐 수십 페이지가 넘는 피드백을 주셨고, 이에 대해 저희는 매우 감사하게 생각합니다. 모든 피드백을 처리하는 데 몇 달이 걸렸지만, 그렇게 하길 잘했다고 생각합니다. 전반적으로 각 모듈은 코드 감사(code auditing)를 포함하여 6~8차례의 테스트(testing) 과정을 거쳤습니다. 그건 꽤 일반적인 일입니다! 보안 취약점 분석, 데이터 프라이버시(data privacy) 준수 여부 확인, 그리고 잠재적 알고리즘 편향성 검토 등 다각적인 관점에서 시스템을 검증했습니다.

AI 프로젝트를 널리 알리는 데 많은 노력이 들어갑니다. 지난 몇 달 동안 저희 팀이 합쳐 약 50번의 기술 시연, 웨비나, 컨퍼런스 발표를 진행했으며, 저희 마케팅 담당자와 다른 분들이 뒤에서 저희를 위해 훨씬 더 많은 일을 해주셨습니다! 사용자 커뮤니티와의 지속적인 소통을 통해 피드백을 수집하고, 이를 바탕으로 시스템을 개선하는 반복적인 과정을 거쳤습니다.

최종 결과물이 사용자분들께 도움이 되기를 바랍니다. 저희 웹사이트나 소셜 미디어(social media)를 통해 의견을 알려주세요. 여러분의 피드백은 저희가 더 나은 AI 시스템을 만드는 데 큰 힘이 됩니다.