LLM(대규모 언어 모델) 정렬(aligning)은 모델이 인간의 선호도에 부합하는 응답을 생성하도록 보장하는 중요한 후처리 학습(post-training) 단계입니다. 기존 RLHF(인간 피드백 기반 강화 학습)와 같은 정렬 기술은 LLM의 품질을 크게 향상시켰지만, 그 구현은 복잡하고 상당한 계산 자원을 요구하며 최적화하기가 어렵다는 단점이 있었습니다. 2025년 현재, LLM 정렬은 단순한 모델 성능 향상을 넘어, AI의 윤리적이고 안전한 활용을 위한 필수 요소로 자리 잡았습니다. 이러한 배경 속에서 DPO(Direct Preference Optimization)는 LLM 정렬에 대한 혁신적이고 실용적인 대안으로 주목받고 있습니다. 이 개요에서는 경사 하강법(gradient descent)으로 최적화할 수 있는 더 간단한 목표를 통해 LLM을 정렬함으로써 이러한 복잡성을 피하는 DPO(Direct Preference Optimization)라는 LLM 정렬에 대한 더 간단한 접근 방식을 알아볼 것입니다. DPO는 그 간결함과 효율성 덕분에 연구 커뮤니티와 산업 현장 모두에서 빠르게 채택되었으며, 다양한 최신 LLM 아키텍처에 성공적으로 적용되고 있습니다. DPO의 성능과 실용성은 정렬 연구에 대한 접근성을 높였으며, 여러 인기 있는 LLM에서 활발히 사용되는 표준 후처리 학습 알고리즘이 되도록 했습니다.

"직접 정렬 알고리즘은 중간 보상 모델(reward model)을 훈련하거나 강화 학습 최적화 도구를 사용하지 않고도 동일한 RLHF 목표를 해결하도록 모델을 업데이트할 수 있게 합니다. 가장 두드러진 직접 정렬 알고리즘이자 언어 모델 정렬의 학문적 움직임 전체를 촉발시킨 것은 DPO(Direct Preference Optimization)입니다." - RLHF 책

AI 연구의 최신 동향을 Deep (Learning) Focus와 함께 깊이 있게 탐구하세요. 현재 50,000명이 넘는 독자들이 함께하고 있습니다. 지금 바로 구독하여 LLM 정렬을 포함한 AI 분야의 핵심 지식을 얻어가세요.

### DPO의 구성 요소

DPO를 완전히 이해하려면 먼저 LLM이 어떻게 훈련되는지 이해함으로써 이 기술의 기초를 다져야 합니다. 구체적으로, DPO는 LLM의 후처리 학습 과정에서 활용되는 선호도 튜닝(preference tuning) 알고리즘입니다. 이 알고리즘은 인간 선호도 데이터셋을 통해 LLM을 미세 조정(finetune)하며, (PPO 기반) RLHF와 같은 RL 기반 선호도 튜닝 기술의 대안입니다. 이번 섹션에서는 DPO와 LLM 훈련에서의 DPO의 역할을 명확히 하기 위해 이러한 핵심 개념들을 자세히 논의할 것입니다.

#### 선호도 데이터 및 보상 모델 (출처: [2, 4])

인간의 선호도는 LLM 후처리 학습 과정의 핵심 구성 요소입니다. 선호도 데이터는 일반적으로 단일 프롬프트(prompt)와 이 프롬프트에 대한 두 가지 응답(또는 완성), 그리고 이 완성에 대한 선호도(인간 주석자 또는 LLM 심사관이 할당)를 포함하는 형태로 구성됩니다. 선호도는 단순히 두 응답 중 어느 것이 다른 것보다 더 나은지를 나타냅니다.

**선호도 데이터셋의 기본 구조**

이 개념은 위 표현식을 통해 공식화되며, 이는 "선택된(chosen)" 응답과 "거부된(rejected)" 응답이 연결된 프롬프트의 선호도 데이터셋을 정의합니다. Bradley-Terry 선호도 모델(Bradley-Terry Model of Preference)은 LLM 영역 내에서 선호도를 모델링하는 데 사용되는 가장 인기 있는 통계 모델입니다. 이 모델은 두 항목(예: 선택된 완성 및 거부된 완성)과 각 항목에 대한 보상 값을 입력으로 받아, 한 항목이 다른 항목보다 선호될 확률을 표현할 수 있게 합니다. 여기서 우리는 비교하는 항목들이 선호도 쌍(preference pair)으로 구성되어 있다고 가정합니다.

**Bradley-Terry 모델을 사용한 쌍별 확률**

우리는 Bradley-Terry 모델을 사용하여 두 완성 간의 쌍별 비교에 대한 확률을 표현합니다. 하지만 Bradley-Terry가 선호도를 모델링하는 데 사용할 수 있는 유일한 접근 방식은 아닙니다. 예를 들어, Plackett-Luce 모델(Plackett-Luce model)도 또 다른 옵션입니다.

**보상 모델(Reward Models).** 위 표현식의 보상은 일반적으로 보상 모델(RM)에 의해 예측됩니다. RM은 프롬프트-완성 쌍을 입력으로 받아 스칼라 선호도 점수를 출력하는 특수 LLM입니다. 이는 표준 디코더 전용 트랜스포머(decoder-only transformer)에 추가 선형 분류 헤드(linear classification head)를 부착하여 구현됩니다.

**보상 모델(RM)의 아키텍처**

고정된 선호도 데이터셋이 주어지면, Bradley-Terry에 의해 모델링된 관찰된 인간 선호도를 반영하는 점수를 생성하도록 RM을 훈련할 수 있습니다. 다시 말해, 우리는 선호도 데이터셋 전반에 걸쳐 우리의 RM이 선택된 응답이 거부된 응답보다 선호될 확률을 최대화하기를 원합니다. 이를 위해 우리는 아래에 표시된 음의 로그 우도 손실(negative log-likelihood loss)을 MLE(최대 우도 추정)를 사용하여 최소화할 수 있습니다. 이는 이 목표를 손실 함수로 사용하여 많은 데이터 예제에 대해 RM을 훈련한다는 의미입니다. 최근에는 UltraFeedback [4]와 같은 대규모 고품질 선호도 데이터셋이 공개되어 RM 훈련의 효율성과 성능을 크게 향상시키고 있습니다. 이러한 데이터셋은 DPO와 같은 직접 정렬 방법론의 발전에도 중요한 기반을 제공합니다. RM에 대한 자세한 내용은 아래 링크된 개요를 참조하십시오.

**보상 모델**
Cameron R. Wolfe, Ph.D. · 6월 30일
전체 기사 읽기

#### LLM 훈련 및 정렬 (출처: [2, 6, 9, 8])

이 개요가 DPO에 초점을 맞출 것이므로, DPO가 LLM의 전체 훈련 과정에서 어디에 위치하는지 이해해야 합니다. (대략) 네 부분으로 구성된 이 훈련 과정은 아래 그림에 묘사되어 있습니다. 각 단계와 그에 상응하는 목적은 다음과 같습니다.

*   **사전 훈련(Pretraining)**은 다음 토큰 예측(next token prediction) 훈련 목표를 사용하여 인터넷 규모의 텍스트 데이터에 대해 LLM을 처음부터 훈련하는 대규모 훈련 절차입니다. 사전 훈련의 주요 목적은 LLM 내에 광범위하고 고품질의 지식 기반을 구축하는 것입니다. 여기를 참조하십시오.
*   **지도 미세 조정(SFT, Supervised finetuning)** 또는 **명령 미세 조정(IFT, instruction finetuning)**은 또한 (지도) 다음 토큰 예측 훈련 목표를 사용하여 LLM이 모방하도록 학습하는 더 작은 고품질 완성 세트에 대해 LLM을 훈련합니다. SFT의 주요 목적은 LLM에게 기본적인 형식 지정 및 명령 따르기 기능을 가르치는 것입니다. 여기를 참조하십시오.
*   **RLHF(인간 피드백 기반 강화 학습)** 또는 **선호도 미세 조정(PreFT, preference finetuning)**은 강화 학습(RL)을 사용하여 인간 선호도 데이터에 대해 LLM을 훈련합니다. RLHF의 핵심 목적은 LLM을 인간의 선호도에 맞추는 것입니다. 즉, 여기에서 설명한 대로 인간이 긍정적으로 평가하는 출력을 생성하도록 LLM을 가르치는 것입니다.
*   **RLVR(검증 가능한 보상 기반 강화 학습)** 또는 **강화 미세 조정(RFT, reinforcement finetuning)**은 규칙이나 휴리스틱(heuristics)으로부터 보상을 결정론적으로 도출할 수 있는 검증 가능한 작업에 대해 RL로 LLM을 훈련합니다. 이 최종 훈련 단계는 추론 성능 또는 더 일반적으로는 모든 검증 가능한 작업에서의 성능을 향상시키는 데 유용합니다.

보시다시피, 이러한 각 훈련 단계는 고품질 LLM을 생성하는 과정에서 핵심적인 목적을 수행합니다. 이러한 훈련 기술들은 사전 훈련(pretraining)과 후처리 학습(post-training)이라는 두 가지 광범위한 범주로 분류될 수 있습니다. 후처리 학습은 사전 훈련 이후의 모든 과정을 포괄합니다. 사전 훈련은 항상 LLM 훈련의 첫 번째 단계이지만, 후처리 학습 과정은 훈련되는 LLM에 따라 크게 달라질 수 있습니다. 동일한 기술, 즉 SFT, RLHF 및 RLVR이 일반적으로 사용되지만, 정확한 순서와 설정은 변경될 수 있습니다. 최근 Llama 3 [8]와 같은 모델들은 SFT 단계를 강화하고, RLHF의 여러 변형을 결합하는 등, LLM 후처리 학습 파이프라인에 대한 지속적인 연구와 최적화 노력을 보여주고 있습니다. 각각 약간 다른 설정을 채택하는 LLM 후처리 학습 파이프라인의 몇 가지 예는 아래 이미지를 참조하십시오.

**인기 있는 오픈 LLM의 후처리 학습**

**RLHF에 대해 더 자세히.** 모든 LLM 훈련 단계가 중요하지만, 이 개요는 특히 기본 LLM을 인간의 선호도에 맞추는 역할을 하는 RLHF 단계에 초점을 맞출 것입니다. RLHF 훈련 과정은 세 가지 주요 단계로 구성됩니다(아래 참조).

1.  LLM에 주입하고자 하는 선호되는 행동을 포착하는 인간 선호도 데이터셋을 수집합니다.
2.  이 선호도 데이터셋에 대해 별도의 보상 모델(RM)을 훈련합니다.
3.  RM의 출력을 보상으로 사용하여 RL 1로 LLM을 미세 조정합니다.

이 과정의 세 번째 단계는 일반적으로 온라인 방식으로 진행됩니다. 즉, 훈련 과정 동안 우리의 정책(policy)에서 완성을 생성하여 RM에 의해 점수를 매기게 됩니다. 이러한 온라인 RL 훈련은 효율적으로 설정하고 조정하기가 어렵다는 단점이 있습니다 [10].

**인간 피드백 기반 강화 학습 ([6]에서 발췌)**

RLHF의 세 번째 단계를 구동하는 데 사용될 수 있는 많은 RL 기반 최적화 도구(예: PPO, REINFORCE, GRPO 등)가 존재합니다. 그러나 RLHF를 위한 RL 최적화 도구의 표준 선택(원래 [2]에 의해 대중화됨)은 PPO(Proximal Policy Optimization)입니다. PPO 기반 RLHF는 선도적인 LLM 연구소들에서 흔히 채택되며, 대규모 LLM 후처리 학습 실행에서 최고의 결과를 도출하는 경향이 있습니다.

"RLHF는 인상적인 대화 및 코딩 능력을 가진 모델을 생성하지만, RLHF 파이프라인은 지도 학습보다 훨씬 더 복잡하며, 여러 LM을 훈련하고 훈련 루프 내에서 LM 정책에서 샘플링하는 것을 포함하여 상당한 계산 비용을 발생시킵니다." - [1]에서

PPO는 그 효과성에도 불구하고 몇 가지 명확한 단점을 가지고 있습니다. PPO는 온라인 RL 알고리즘일 뿐만 아니라, LLM의 네 가지 다른 복사본(즉, 정책, 참조 정책, 보상 모델 및 가치 함수)을 메모리에 저장합니다. 이는 PPO로 훈련을 수행하기 위해 많은 메모리를 가진 많은 GPU가 필요하다는 것을 의미합니다. 또한, PPO 기반 RLHF는 제대로 튜닝되지 않으면 최적이 아닌 성능을 초래할 수 있는 수많은 구현 세부 사항을 포함하고 있습니다.

**RL 훈련 중에는 무슨 일이 일어날까요?** RLHF의 RL 훈련 단계에서, 우리는 학습된 보상 모델을 사용할 수 있으며, 이 보상 모델이 우리 LLM의 출력에 할당하는 보상을 최대화하고자 합니다. 또한, 훈련 중에 원래 모델에서 너무 멀리 "표류(drifting)"하는 것을 피하고자 합니다. 이 최적화 과정은 일반적으로 아래에 표시된 목표를 통해 공식화됩니다.

**표준 RLHF 목표**

이 방정식에서 우리는 학습된 정책과 초기 SFT 모델(또는 다른 참조 모델) 간의 KL 발산(KL divergence)에 대한 가산 페널티(additive penalty) 하에서 우리 LLM의 완성에 의해 수신되는 예상 보상을 최대화합니다. KL 발산은 손실 함수에 페널티 항으로 포함됩니다. 보상과 KL 발산 간의 균형은 하이퍼파라미터(hyperparameter) β에 의해 제어됩니다.

**RLHF는 왜 그렇게 어려울까요?** RL 기반 선호도 튜닝은 여러 가지 요인으로 인해 복잡합니다. 예를 들어, 여러 LLM이 상호 작용하고, 훈련 중에 이 모델들로부터 생성을 샘플링해야 하며, 섬세한 하이퍼파라미터 튜닝이 필요하고, 계산 및 메모리 비용이 높습니다. 실제로는 이러한 복잡성으로 인해 RLHF 훈련 과정이 불안정하고, 예측 불가능하며, 비용이 많이 들고, 일반적으로 어렵습니다. 이러한 문제들은 특히 소규모 연구팀이나 자원 제약이 있는 환경에서 LLM 후처리 학습 연구에 대한 진입 장벽을 크게 높였습니다.

높은 수준에서 PPO 기반 RLHF가 그렇게 복잡하고, 비용이 많이 들며, 제대로 구현하기 어려운 두 가지 주요 이유가 있습니다.

*   명시적 보상 모델(explicit reward model) 사용.
*   RL을 사용하여 LLM 훈련.

보상 모델은 별도로 훈련되어야 하며 훈련 과정 내내 메모리에 유지되어야 하는 추가적인 LLM입니다. 또한, 훈련을 위해 PPO를 사용하면 모델의 또 다른 복사본인 가치 함수(value function)를 메모리에 저장해야 하며, RL 기반 선호도 튜닝의 모든 추가적인 어려움이 발생합니다. 따라서 별도의 보상 모델과 RL 사용을 단순히 피할 수 있다면, PPO 기반 RLHF와 관련된 많은 일반적인 골칫거리도 피할 수 있을 것입니다!

#### DPO는 어디에 적합할까요? (출처: [1, 2, 6, 9, 12])

위에서 보았듯이, DPO는 RLHF의 대안 역할을 하는 정렬 알고리즘입니다. 그러나 RLHF와는 다르게, DPO는 별도의 보상 모델이나 어떤 형태의 강화 학습(RL) 훈련도 사용하지 않고 간접적인 방식으로 RLHF 목표를 해결하기 위해 경사 상승법(gradient ascent)을 통해 정책을 최적화합니다.

"우리는 명시적인 보상 모델링이나 강화 학습 없이 언어 모델을 인간의 선호도에 따르도록 직접 최적화하는 방법을 보여줍니다. 우리는 기존 RLHF 알고리즘과 동일한 목표를 암묵적으로 최적화하지만 구현이 간단하고 훈련하기 쉬운 알고리즘인 DPO를 제안합니다." - [1]에서

DPO는 보상의 새로운 재매개변수화(reparameterization)를 도입하여 RLHF 목표를 다룹니다. 이는 별도의 보상 모델 없이 정책 자체에서 직접 보상을 도출하는 방식으로, 이를 우리는 "암묵적(implicit)" 보상이라고 칭합니다. DPO로 LLM을 훈련할 때, 우리는 기존 보상 모델을 훈련하는 것과 유사한 방식으로 오프라인 선호도 데이터셋을 사용하여 이 암묵적 보상을 학습합니다. DPO의 핵심 통찰은 이 암묵적 보상에서 RLHF에 대한 최적 정책(optimal policy)을 직접 추출할 수 있다는 것입니다. 근본적으로 DPO는 Bradley-Terry 모델에 기반한 암묵적 보상 모델을 학습하고, 이 암묵적 보상으로부터 최적 정책을 간접적으로 도출합니다.

DPO는 명시적인 보상 모델 훈련을 요구하지 않기 때문에, 일부 실무자들은 DPO가 보상 모델링을 완전히 "회피"하고 RL이나 보상 모델 없이 RLHF를 통해 정책을 직접 최적화한다고 오해하기도 합니다. 실제로는 DPO는 여전히 보상 모델링 접근 방식입니다. 그 훈련 목표와 과정은 전통적인 보상 모델링과 동일합니다. DPO에서 우리는 실제로 보상 모델을 훈련하고 있습니다. 유일한 차이점은 이 보상 모델이 정책 자체 내에 암묵적으로 존재한다는 것입니다. 이 암묵적 보상을 최적화하도록 정책을 훈련함으로써, DPO는 RLHF 목표를 최적으로 해결하는 정책을 찾을 수 있도록 합니다.

위에서 설명했듯이, DPO는 외부 보상 모델, 온라인 샘플링, 그리고 복잡한 RL 과정을 모두 피합니다. 대신, 우리는 기본적인 경사 하강법을 사용하여 LLM을 직접 최적화하여 RLHF 목표를 (암묵적으로) 해결합니다. 이러한 단순화는 DPO를 RL 기반 선호도 튜닝에 비해 더 안정적이고(하이퍼파라미터 튜닝이 덜 필요함) 경량화하여 후처리 학습 연구의 대중화에 기여합니다. 2025년 현재, DPO는 그 단순성 덕분에 새로운 정렬 기술(예: IPO, KTO, PPO-IT 등) 개발의 기반이 되거나, 기존 기술과의 조합을 통해 더욱 강력한 정렬 효과를 창출하는 데 활용되고 있습니다 [12].

### 쿨백-라이블러(Kullback-Leibler, KL) 발산

LLM 후처리 학습 전반에 걸쳐, KL 발산 제약(KL divergence constraint)을 조건으로 모델을 최적화하는 경우가 많습니다. 예를 들어, RLHF 내에서 사용되는 표준 최적화 목표는 아래에 표시된 형태를 가집니다.

**KL 제약이 있는 표준 RLHF 목표**

보시다시피, 우리는 보상을 최대화하면서 이 보상에서 차감되는 페널티 항(β로 가중된 KL 발산)을 최소화하고자 합니다. 페널티 항의 목표는 훈련 중에 우리의 정책 3이 참조 정책에서 너무 멀리 표류하는 것을 방지하는 것입니다. 이것이 정확히 무엇을 의미하는지 더 깊이 이해해 봅시다.

KL 발산은 정보 이론(information theory)에서 온 개념으로, 확률 분포가 어떤 참조 분포와 얼마나 다른지 4를 측정합니다. 이산 확률 분포(discrete probability distribution)의 경우, KL 발산은 아래에 제시된 형태를 가집니다. 특히, KL 발산은 대칭적이지 않으므로, 인수의 순서가 중요합니다.

**연속 및 이산 확률 분포에 대한 KL 발산**

연속 확률 분포의 경우, KL 발산을 기댓값(expectation)으로 공식화할 수 있습니다. 위를 참조하십시오. 이 개념이 명확하지 않다면, 이것을 읽어보십시오.

**LLM과의 관계.** LLM 영역에서 KL 발산은 두 LLM 또는 정책을 비교하는 데 일반적으로 사용됩니다. 일반적으로 우리는 현재 훈련하려는 정책을 참조 정책과 비교합니다. 예를 들어, DPO의 경우, 우리는 SFT 정책(즉, 사전 훈련과 SFT를 모두 거친 LLM)으로 시작한 다음, 이 SFT(참조) 정책과 우리가 훈련하는 정책 간의 KL 발산이 계산되는 표준 RLHF 목표를 최적화합니다. 구체적으로, 이 KL 발산의 형태는 다음과 같습니다.

**두 LLM 간의 KL 발산**

이 형태의 KL 발산은 입력으로 프롬프트 x가 주어졌을 때 완성 y에 대해 현재 모델과 참조 모델 모두가 예측한 확률의 비율을 살펴봅니다. 완성 y의 확률은 단순히 LLM이 완성 내 각 토큰에 대해 예측한 다음 토큰 확률의 곱으로 얻어집니다. 이러한 완성 확률에 대한 KL 발산을 계산함으로써, 우리는 두 모델이 예측한 토큰 분포 간의 유사성을 포착합니다.

**실제 KL 발산 추정.** 우리는 일반적으로 RL 훈련 중에 현재 정책이 예측한 분포와 고정된 참조 정책(예: SFT 모델 5) 간의 KL 발산을 추정하고자 합니다. 직관적으로, RL 훈련 중에 사용되는 보상에 이 제약(아래에 표시됨)을 추가하면 훈련되는 정책이 참조 정책과 너무 달라지지 않도록 보장합니다. 실제로는 우리는 일반적으로 KL 발산을 근사화하는데, 이는 비교적 간단하게 수행됩니다. 그러나 이 근사화를 수행하는 방법에는 여러 가지 옵션이 있습니다.

일반적으로 KL 발산을 근사화하는 것은 KL 발산의 기댓값(연속) 형태를 사용합니다. 위에서 설명했듯이, 이 형태의 KL 발산은 단순히 두 분포의 로그 확률을 서로 빼고 이 차이의 기댓값을 취합니다. 토큰 로그 확률이 RL 훈련의 다양한 측면(예: PPO 목표)에서 이미 사용된다는 점을 고려할 때, 이러한 표현식은 우리가 계산하기에 매우 쉽습니다! 구체적으로, 프롬프트 x가 주어졌을 때 현재 정책과 참조 정책 간의 KL 발산을 계산하려고 한다고 가정해 봅시다. 이를 위해 우리는 다음 단계를 수행합니다.

1.  현재 정책(참조 정책이 아님)으로 프롬프트에 대한 완성을 생성합니다.
2.  이 완성의 각 토큰에 대한 로그 확률을 현재 정책과 참조 정책 모두에서 가져옵니다.
3.  토큰 로그 확률을 합산하여 시퀀스 로그 확률을 얻습니다.
4.  현재 정책과 참조 정책 간의 시퀀스 로그 확률 차이를 취합니다.

이 과정의 마지막 단계에서는 KL 발산의 근사치를 계산하는 데 사용할 수 있는 여러 옵션이 있으며, 이 모든 옵션은 아래 코드에 나와 있습니다. 이러한 구현이 실제 환경에서 사용되는 예는 여기를 참조하십시오.

```python
"""
Assume we already have necessary logprobs available.
logprob: completion logprob from the policy
ref_logprob: completion logprob from the reference policy
"""
kl_div = logprob - ref_logprob # difference
kl_div = (logprob - ref_logprob).abs() # absolute
kl_div = 0.5 * (logprob - ref_logprob).square() # mse
kl_div = F.kl_div(ref_logprob, logprob, reduction='batchmean') # per token
```

이 KL 발산 추정치는 여기에서 설명된 RL 미세 조정을 위한 목표의 일부로 우리 시퀀스의 보상에서 차감됩니다.

### DPO(Direct Preference Optimization) [1]

LLM 훈련의 기본과 이 프레임워크에서 DPO의 역할을 확립했으므로, 이제 DPO 자체의 메커니즘을 학습하는 데 집중할 수 있습니다. DPO는 표준 RLHF의 대안 또는 보완적인 역할을 하는 선호도 튜닝 방법입니다. 이 섹션에서는 RLHF에서 사용되는 훈련 목표부터 시작하여 DPO 훈련 과정을 처음부터 도출할 것입니다. 그런 다음 DPO의 실제 구현에 대해 논의할 것입니다. 여기에는 처음부터 단계별 구현과 DPO를 사용하여 LLM을 훈련하는 구체적인 예가 포함됩니다.

**요약: DPO란 무엇인가?**

**DPO 훈련 손실**

우리가 배웠듯이, DPO는 명시적 보상 모델과 RL을 피하고, 대신 더 간단한 경사 하강법 접근 방식을 통해 RLHF 목표를 간접적으로 해결하는 선호도 튜닝 접근 방식입니다. 단일 선호도 쌍에 대해 위에 표시된 DPO 손실은 다음 두 가지 방식으로 LLM을 훈련합니다.

*   참조 정책에 대한 선택된 완성의 상대적 확률을 증가시킵니다.
*   거부된 완성의 상대적 확률을 감소시킵니다.

이 손실 함수는 MLE를 사용하여 오프라인 선호도 데이터셋에 대해 최적화하기 간단합니다. 따라서 우리는 RL 없이 보상 모델과 유사하게 LLM을 훈련할 수 있습니다. 또한, 이 접근 방식은 경량화되고 간단함에도 불구하고 RLHF에서 최적화하는 것과 동일한 목표를 해결하는 정책을 여전히 산출합니다!

"모델 응답에 대한 인간 선호도 데이터셋이 주어지면, DPO는 간단한 이진 교차 엔트로피(binary cross entropy) 목표를 사용하여 정책을 최적화할 수 있으며, 선호도 데이터에 맞춰진 암묵적 보상 함수에 대한 최적 정책을 생성합니다." - [1]에서

이 손실을 연구하면, 보상 모델을 훈련하는 데 사용되는 손실 함수와 매우 유사하다는 것을 알 수 있습니다. 이는 참고를 위해 아래에 다시 제시되었습니다. 주요 차이점은 보상 모델의 출력을 우리 정책에서 파생된 암묵적 보상으로 대체한다는 것입니다. 나중에 보겠지만, DPO 목표는 선택된 완성 및 거부된 완성의 로그 확률을 조정하는 것 외에도, LLM의 암묵적 보상 추정치가 잘못된 예제에 자연스럽게 강조를 둡니다. (출처)

#### DPO 손실 도출

이제 DPO의 핵심 아이디어를 이해했으므로, DPO가 어디에서 왔는지, 그리고 DPO가 표준 RLHF와 동일한 최적화 문제를 해결하고 있다는 것을 어떻게 아는지 이해해야 합니다. 이를 위해 우리는 이론에 의존할 것이며, 이는 이 섹션에 많은 방정식이 포함될 것임을 의미합니다. 이론이 해석하기 어려울 수 있지만, 이를 이해하는 것은 DPO가 작동하는 이유에 대한 근본적인 이해를 얻는 데 도움이 됩니다. 이론을 소화하기 쉽게 만들기 위해, 우리는 각 단계에 대한 해당 설명과 함께 도출 과정을 단계별로 나눌 것입니다.

**DPO 손실 함수를 도출하기 위해 따르는 단계**

**증명 개요.** 표준 RLHF 훈련 목표에서 시작하여, 우리는 네 가지 핵심 단계(위에 표시됨)를 따라 DPO에서 사용되는 훈련 손실을 도출할 수 있습니다.

1.  RLHF에서 최적 정책에 대한 표현식 도출.
2.  이 표현식을 재배열하여 암묵적 보상 함수 형성.
3.  암묵적 보상을 Bradley-Terry 선호도 모델에 삽입.
4.  이 암묵적 선호도 모델과 일치하도록 LLM 훈련 — 이것이 우리가 DPO 훈련 과정에서 하는 일입니다.

위 단계들은 RLHF에서 LLM을 훈련하는 데 사용되는 목표로 시작하여 DPO 손실 함수로 끝납니다. 이 도출에서 우리는 RLHF 최적화 문제를 재구성하여 DPO 훈련 방법론에 도달합니다. 보시다시피, RLHF와 DPO는 복잡하게 연결되어 있습니다. 그들은 동일한 최적화 문제를 해결하려고 합니다! 아래 도출을 연구함으로써, 우리는 이러한 기술들 간의 관계에 대해 더 깊이 이해하게 됩니다.

**(1단계) RLHF의 최적 해.** DPO 손실을 도출하기 위해, 우리는 해결하려는 초기 RLHF 목표에서 시작해야 합니다. 이는 가독성을 위해 아래에 다시 제시되었습니다. 그러나 이 표기법에서 학습된 보상 모델 RM을 사용하는 대신, 우리는 일반 보상 함수 r(x, y)를 사용합니다. 일반 보상 함수는 우리의 보상 모델을 포함할 수 있지만, 이에 국한되지는 않습니다.

**일반 보상 함수가 있는 표준 RLHF 목표**

이 목표에서 시작하여, 우리는 아래 단계를 따라 이 목표에 대한 최적 해의 닫힌 형태 표현식(closed-form expression)을 찾을 수 있습니다. 간단히 말해, 우리는 아래에 표시된 RLHF 목표를 실제로 최대화하는 π 값을 찾고 있습니다!

위 도출의 마지막 두 단계에서 우리는 Z(x) 함수를 도입하는데, 이를 분할 함수(partition function)라고 부를 것입니다. 분할 함수는 아래에 정의되어 있습니다.

**DPO에서 사용되는 분할 함수**

보시다시피, 분할 함수는 참조 정책과 입력 프롬프트 x에만 의존하며, 현재 정책이나 완성 y에는 의존하지 않습니다. "분할 함수"라는 이름은 확률론(probability theory) 및 통계 역학(statistical mechanics)과 같은 분야에서 차용되었습니다. 여기를 참조하십시오. 가장 간단한 수준에서, 분할 함수는 DPO의 이론적 도출에서 사용되는 정규화 항(normalization term)일 뿐입니다. 우리는 Z(x)를 사용하여 우리가 도출하는 확률 분포(이 경우 RLHF 목표에 대한 최적 정책)가 합이 1이 되어 유효한 분포를 형성하도록 보장합니다.

이제 분할 함수를 이해했으므로, 위 빨간색 상자에 표시된 방정식에서 도출을 계속할 것입니다. 구체적으로, 우리는 이 항의 일부를 추출하여 아래 표현식을 정의할 것입니다. 우리는 이 항을 "최적 정책"이라고 부를 것입니다. 그 이유는 곧 명확해질 것입니다.

앞서 언급했듯이, 분할 함수는 위 표현식에서 최적 정책에 대한 정규화 항으로 사용됩니다. 우리는 위에 정의된 최적 정책이 유효한 확률 분포라는 것을 우리는 알고 있습니다. 그 이유는 다음과 같습니다.

*   모든 가능한 완성 y에 대해 최적 정책의 값은 ≥ 0입니다.
*   모든 완성 y에 대한 최적 정책의 합은 1과 같습니다.

첫 번째 속성은 명백합니다. 최적 정책의 모든 구성 요소는 음수가 아닙니다 6. 두 번째 속성에 대한 증명은 아래에 제공되며, 여기서 우리는 분할 함수 Z(x)가 최적 정책 분포를 정규화하는 데 어떻게 사용되는지 직접 볼 수 있습니다.

이제 최적 정책을 정의하고(그리고 유효성을 검증하고) 나면, 이 항이 나타났던 원래 표현식으로 돌아가 최적 정책에 대한 표현식을 대입할 수 있습니다. 이는 아래에 표시된 방정식을 산출합니다.

위의 최종 항에서 우리는 이 도출의 핵심을 봅니다. 즉, 표준 RLHF 목표는 최적 정책과의 KL 발산을 최소화하는 정책 π를 찾음으로써 최소화됩니다. 두 확률 분포가 동일할 때 KL 발산이 최솟값(0)에 도달하므로 7, 이 최적화의 해는 최적 정책 자체입니다. 따라서 이름이 붙여졌습니다. 따라서 우리는 표준 RLHF 목표에 대한 최적 해를 아래 방정식에 표시된 대로 표현할 수 있습니다.

**표준 RLHF 목표를 최적으로 해결하기**

**(2단계) 암묵적 보상 도출.** 여기에서 우리는 위에 표시된 최적 정책에 대한 표현식을 가져와 재배열하여 보상 함수에 대한 표현식(최적 정책 측면에서)을 아래에 표시된 대로 도출할 수 있습니다.

이제 우리는 보상의 재매개변수화를 도출했습니다. 그러나 이 보상 함수는 어떤 명시적 보상 모델에도 의존하지 않습니다. 오히려 우리는 최적 정책과 참조 정책에서 계산된 확률만을 사용하여 보상을 추정합니다. 우리는 이것을 "암묵적" 보상이라고 부를 것입니다.

"이 변수 변경 접근 방식은 명시적이고 독립적인 보상 모델을 피합니다... 정책 네트워크는 언어 모델과 (암묵적) 보상 모두를 나타냅니다." - [1]에서

이제 남은 유일한 문제는 우리 암묵적 보상에 있는 Z(x) 항입니다. 분할 함수는 모든 가능한 완성 y에 대한 합을 취하므로, Z(x) 값을 계산하는 것은 실제로는 비용이 많이 듭니다. 더 나아가, 독립적인 보상 모델 훈련 없이 직접 계산할 수 없는 보상 함수 r(x, y)도 Z(x)의 표현식에 나타납니다. 이를 해결하기 위해 우리는 Bradley-Terry 모델을 다시 살펴보고 이를 우리의 암묵적 보상 함수와 결합해야 합니다.

**(3단계) Bradley-Terry 선호도 모델.** Bradley-Terry 선호도 모델 하에서, 우리는 주어진 완성이 다른 완성보다 선호될 확률을 계산할 수 있습니다. 대부분의 경우, 이 선호도 모델의 입력은 각 완성에 대한 명시적 보상(보상 모델에 의해 예측됨)입니다. DPO의 경우, 우리는 이 명시적 보상을 우리의 암묵적 보상 함수로 대체합니다. 아래를 참조하십시오.

위의 최종 방정식에 표시된 바와 같이, 우리는 이제 우리의 암묵적 보상 함수를 사용하는 Bradley-Terry 선호도 모델에 대한 표현식을 가지며, 여기서 암묵적 보상은 최적 정책과 참조 정책에만 의존합니다. Bradley-Terry 표현식의 쌍별 특성과 Z(x)의 값이 x에만 의존하고(y에는 의존하지 않음) 있다는 사실 때문에, 암묵적 보상 함수의 Z(x) 구성 요소는 선택된 완성에 대한 암묵적 보상에서 거부된 완성에 대한 암묵적 보상을 뺄 때 실제로 상쇄됩니다.

**(4단계) 정책 훈련.** 위 표현식은 고정된 최적 정책에 의존합니다. 이 최적 정책은 우리가 해결하려는 RLHF 목표의 해입니다. 여기에서 우리는 이 최적 정책을 복구할 수 있는 훈련 목표를 어떻게 도출할지 결정해야 합니다. 이를 위해 DPO는 위 표현식의 최적 정책을 학습된 정책으로 대체합니다. 아래를 참조하십시오.

이 두 표현식을 어떻게 같게 만들 수 있을까요? 우리는 학습된 정책을 훈련해야 합니다! 구체적으로, 우리는 우리의 암묵적 보상 함수를 기반으로 선택된 응답이 거부된 응답보다 선호될 확률을 경험적으로 최대화하도록 학습된 정책을 최적화하는 순위 손실(ranking loss)을 공식화할 수 있습니다. 이를 통해 우리는 우리의 선호도 모델이 정확하고, 따라서 최적 정책의 선호도 모델과 일치하도록 보장합니다.

명시적 보상을 암묵적 보상으로 대체하는 것 외에도, 이 손실 함수는 표준 보상 모델이 사용하는 것과 정확히 동일한 훈련 목표입니다. 아래를 참조하십시오.

**DPO를 위해 도출된 최종 손실 표현식**

우리는 또한 이 손실 함수가 DPO의 훈련 목표와 동일하다는 것을 알 수 있습니다. 우리는 이제 RLHF의 훈련 목표에서 시작하여 DPO 훈련 목표를 완전히 도출했습니다. DPO의 훈련 과정은 우리 정책에 기반한 암묵적 보상 모델을 학습합니다. 이 암묵적 보상 함수를 학습함으로써, 우리는 RLHF의 최적 정책과 일치하는 정책을 얻습니다.

### DPO는 실제로 최적 정책을 산출할까요?

"[DPO] 최적화 목표는 [암묵적] 보상 매개변수화를 가진 Bradley-Terry 모델과 동등하며, 우리는 우리의 매개변수 모델을 보상 모델 최적화와 동등하게 최적화합니다... 우리는 [이 목표]가 학습된 보상 모델의 클래스를 제약하지 않으며 최적 정책의 정확한 복구를 허용한다는 것을 보여줍니다." - [1]에서

위 도출에 따르면, DPO 손실을 사용하여 LLM을 훈련하면 최적 정책과 동일한 선호도 분포(암묵적 보상에 의해 유도됨)를 가진 모델이 생성됩니다. 다시 말해, DPO 손실을 통해 우리 정책이 학습한 암묵적 보상 함수는 우리 선호도 데이터셋에서 선택된 완성 및 거부된 완성을 올바르게 순위를 매길 것입니다. 그러나 DPO의 목표는 좋은 암묵적 보상 함수를 가진 모델을 훈련하는 것이 아닙니다. 우리는 LLM을 정렬하고 고품질 완성을 생성하는 정책을 도출하기를 원합니다! 다행히도, [1]의 저자들은 고품질 암묵적 보상 함수를 학습하는 것 외에도 DPO를 통해 도출된 정책이 RLHF의 최적 정책과 일치해야 함을 보여주는 최종 증명을 제공합니다.

두 보상 함수 r(x, y)와 r’(x, y)는 r(x, y) - r’(x, y) = f(x) (일부 함수 f(•)에 대해)일 때 그리고 그 때에만 동등합니다.

**동등한 보상.** 증명을 시작하기 위해, 우리는 먼저 보상 함수에 대한 동등 관계(equivalence relation)를 지정할 수 있습니다. 이것은 두 보상 함수가 같다는 것이 무엇을 의미하는지 포착하는 정의일 뿐입니다. 위를 참조하십시오. 간단히 말해, 보상 함수는 보상 차이가 완성에 의존하지 않고 프롬프트에만 의존할 경우 동등하다고 간주됩니다. 이 정의를 사용하여, 우리는 아래에서 두 개의 동등한 보상 함수가 동일한 선호도 분포 8를 산출함을 보장한다는 것을 보여줍니다.

우리는 또한 두 개의 동등한 보상 함수가 이전 섹션에서 탐구한 표준 RLHF 목표에 대입될 때 동일한 최적 정책을 산출함을 보장한다는 것을 보여주는 유사한 증명을 작성할 수 있습니다. 아래를 참조하십시오.

**최적 정책 증명.** 위 결과들을 고려할 때, 이 증명의 마지막 단계는 DPO 내에서 사용되는 암묵적 보상 함수가 RLHF 내에서 사용되는 실제 보상과 동등하다는 것을 단순히 보여주는 것입니다. 이 두 보상 함수가 동등 관계를 만족한다면, 우리는 DPO가 위에 표시된 결과에 따라 RLHF와 동일한 최적 정책을 산출할 것임을 알 수 있습니다.

이 최종 결과를 증명하기 위해, 우리는 RLHF에서 사용되는 임의의 보상 함수 r(x, y)를 고려하는 것부터 시작할 수 있습니다. 우리의 목표는 DPO의 암묵적 보상이 r(x, y)와 동등하다는 것을 보여주는 것입니다. 임의의 보상이 주어지면, 우리는 위에 표시된 수정된 9 보상 표현식을 정의할 수 있습니다. 이 표현식은 r(x, y)에서 추가 항(즉, 분할 함수의 로그)을 뺄 뿐입니다. 또한 r(x, y)에서 빼는 항이 x에만 의존한다는 점에 유의하십시오. 이러한 이유로, 수정된 보상 표현식은 우리가 이전에 정의한 동등 관계에 따라 r(x, y)와 동등합니다.

"두 번째 보조 정리(lemma)는 동일한 클래스의 모든 보상 함수가 동일한 최적 정책을 산출한다고 명시합니다. 따라서 우리의 최종 목표를 위해, 우리는 최적 클래스에서 임의의 보상 함수를 복구하는 데만 관심이 있습니다." - [1]에서

원하는 결과를 증명하기 위해, 우리는 최적 RLHF 해를 재배열하여 암묵적 보상을 생성하는 이전 표현식을 활용해야 합니다. 이 암묵적 보상을 위의 수정된 보상 표현식에 대입하면, 우리는 r(x, y)와 동등하다고 알려진 보상(DPO의 암묵적 보상와 일치함)을 얻습니다. 아래를 참조하십시오. 결과적으로, 우리는 이제 DPO가 사용하는 암묵적 보상이 r(x,y)와의 동등 관계를 만족한다는 것을 알게 되었고, 이는 증명을 완료합니다.

**핵심 요점.** 이 섹션을 마무리하기 전에, 우리는 방금 증명한 결과를 빠르게 맥락화해야 합니다. 이전 섹션에서 우리는 표준 RLHF 목표에 대한 최적 정책(또는 해)의 암묵적 보상에 의해 유도되는 선호도 분포에 대한 표현식을 도출했습니다. 이 표현식이 도출된 후, 우리는 일반 보상 모델과 동일한 훈련 전략을 채택함으로써 이 선호도 분포와 일치하는 암묵적 보상 함수를 갖도록 모델을 쉽게 훈련할 수 있습니다. 따라서 DPO의 핵심 훈련 절차는 (암묵적) 보상 모델 훈련을 중심으로 이루어지며, 이것이 논문의 제목이 된 이유입니다. 아래를 참조하십시오.

DPO에 대한 흔한 오해는 DPO가 보상 모델을 제거한다는 것인데, 이는 사실이 아닙니다. 사실, DPO는 보상 모델링에 전적으로 기반합니다. 보상 모델이 단지 암묵적이라는 것은 우리가 명시적 보상 모델 훈련을 피할 수 있다는 것을 의미합니다.

"자주 오해되는 점은 DPO가 핵심적으로 보상 모델을 학습한다는 것입니다. 그래서 논문의 부제가 '당신의 언어 모델은 비밀리에 보상 모델이다(Your Language Model is Secretly a Reward Model)'입니다. 이것을 DPO 목표가 정책을 직접 훈련하는 것으로 혼동하기 쉽습니다." - RLHF 책

DPO의 훈련 절차가 보상 모델링에 기반한다는 점을 고려할 때, 이러한 방식으로 LLM을 훈련하는 것이 실제로 최적 정책을 산출할 것이라는 점은 즉시 명확하지 않습니다. 결과 모델이 정확한 암묵적 보상 함수를 가질 수 있지만 여전히 고품질 완성을 생성하지 못할 수도 있을까요? 이 섹션에서 우리는 이것이 사실이 아니어야 함을 증명합니다. 최적 정책의 암묵적 선호도 분포와 일치하도록 모델을 훈련하면, 결과 정책도 최적임이 보장됩니다! 간단히 말해, DPO는 RLHF를 통한 훈련으로 도출된 정책과 품질 면에서 비교할 수 있는 정책을 간접적으로 제공하며, PPO 기반 RLHF와 같은 기술보다 훨씬 덜 복잡한 유효한 선호도 튜닝 대안이 됩니다.

### DPO는 왜 작동할까요?

**DPO 손실 함수의 기울기(Gradient)**

DPO와 DPO가 잘 작동하는 이유에 대한 더 깊은 이해를 얻기 위해, 우리는 DPO 손실 함수의 기울기 구조를 살펴볼 수 있습니다. 위 10를 참조하십시오. 이 표현식에는 명확성을 위해 빨간색(일부 항은 주황색), 파란색, 녹색으로 표시된 세 가지 핵심 항이 있습니다. 각 항의 목적은 다음과 같습니다.

*   첫 번째 (빨간색) 항은 시그모이드 함수(sigmoid function)로 인해 [0, 1] 범위에 속하는 가중치이며, 거부된 완성의 암묵적 보상이 선택된 완성의 암묵적 보상에 비해 증가할수록 커집니다. 다시 말해, 이 항은 잘못된 암묵적 보상 추정치에 더 높은 가중치를 할당합니다.
*   두 번째 (파란색) 항은 LLM의 매개변수에 대한 선택된 완성의 우도(likelihood)의 양의 기울기이며, 이는 선택된 완성의 우도를 증가시키는 목적을 수행합니다.
*   세 번째 (녹색) 항은 LLM의 매개변수에 대한 거부된 완성의 우도의 음의 기울기이며, 이는 거부된 완성의 우도를 감소시키는 목적을 수행합니다.

이 항들은 i) 선택된 완성의 우도를 증가시키고 ii) 거부된 완성의 우도를 감소시키는 것을 동시에 수행하며, LLM의 암묵적 보상 추정치가 잘못된 경우에 추가적인 강조(즉, LLM 매개변수에 대한 더 큰 업데이트)가 주어집니다.

"예제는 암묵적 보상 모델이 선호되지 않는 완성을 베타로 스케일링하여 얼마나 더 높게 평가하는지에 따라 가중치가 부여됩니다... 암묵적 보상 모델이 완성을 얼마나 잘못 순서화하는지, KL 제약의 강도를 고려합니다." - [1]에서

**가중 계수(Weighting coefficient).** [1]의 저자들은 DPO 손실 기울기의 세 가지 하위 구성 요소 모두가 알고리즘이 잘 작동하는 데 필요하다고 관찰합니다. 특히, 이 기울기에서 첫 번째 가중치 항을 제거하면(모든 선택된 완성의 우도를 균일하게 증가시키고 모든 거부된 완성의 우도를 감소시키는 기울기를 생성함), 결과 정책은 품질이 낮고 텍스트를 생성할 때 완전히 퇴화하는 경향이 있습니다. 아래를 참조하십시오. 이러한 훈련 알고리즘은 비우도 훈련(unlikelihood training)이라고 불리며 과거 [5]에서 탐구되었습니다. DPO에 의해 손실 기울기에 추가된 간단한 가중치 항은 이 접근 방식을 완전히 변화시켜 고품질 LLM 정렬을 수행할 수 있게 합니다.

**비우도 훈련으로 훈련된 LLM은 퇴화하는 경향이 있습니다.**

### 처음부터 DPO 구현하기

DPO의 이론적 도출은 복잡하게 느껴질 수 있지만, 이 기술은 실제 적용에서는 매우 간단합니다. 사실, DPO는 최고의 연구소 외부의 사람들을 위해 LLM 후처리 학습 연구를 대중화하는 데 큰 역할을 했습니다 [3]. PPO 기반 RLHF와 같은 알고리즘은 튜닝하기 더 어렵고 상당한 계산 자원을 필요로 합니다. 대조적으로, DPO는 RL 없이 표준 분류(또는 순위) 손실을 사용하며, 훈련 과정 내내 모델의 네 개 대신 두 개의 복사본만 유지합니다.

**표준 DPO 훈련 파이프라인**

**DPO 훈련 파이프라인.** DPO를 사용한 표준 훈련 과정은 위에 묘사되어 있습니다. 우리는 모델을 훈련하는 사용 사례를 포착하는 다양한 프롬프트 세트로 과정을 시작합니다. 여기에서 우리는 참조 정책을 사용하여 각 프롬프트에 대한 완성 쌍을 생성하고, 인간 평가자가 각 쌍에 대한 선호도 주석을 제공하도록 합니다. 이 선호도 데이터셋을 사용할 수 있게 되면, 우리는 선호도 데이터셋에 대해 이전에 도출한 DPO 손실을 최소화하도록 모델을 훈련함으로써 최대 우도 추정을 수행합니다.

**PyTorch에서 DPO 손실 계산**

**손실 구현.** 우리는 PyTorch에서 DPO의 손실 함수를 아주 쉽게 구현할 수 있습니다. 이는 현재 정책과 참조 정책에서 파생된 암묵적 보상에 적용되는 순위 손실로 볼 수 있습니다. [1]에서 가져온 손실의 예시 구현은 참고를 위해 아래에 복사되어 있으며, 여기서 손실이 다음을 통해 계산됨을 알 수 있습니다.

*   현재 정책과 참조 정책에 의해 각 완성(선택된 것과 거부된 것 모두)에 할당된 로그 확률을 얻습니다.
*   현재 정책과 참조 정책 모두에 대해 선택된 완성 및 거부된 완성 간의 확률 비율을 계산합니다.
*   위의 확률 비율을 사용하여 최종 DPO 손실을 구성합니다.

**오프라인 선호도 데이터 처리.** DPO는 근본적으로 오프라인 선호도 학습 알고리즘입니다. 우리는 정적 선호도 데이터셋에 대해 모델을 최적화합니다. 위에 설명된 파이프라인에서 우리는 참조 모델을 사용하여 선호도 데이터셋의 완성을 생성합니다. 그러나 대부분의 실제 응용 프로그램에서는 이러한 방식으로 데이터가 생성되지 않을 수 있습니다. 실무자로서 우리는 UltraFeedback [4]와 같은 선호도 데이터셋을 온라인에서 다운로드하고 DPO를 사용하여 이 정적 데이터셋에 대해 모델을 훈련할 수 있습니다. 이러한 경우, 실제 참조 모델은 알려져 있지 않으며 DPO 훈련에서 사용한 참조 모델과 다를 수 있어 분포 변화(distribution shift)를 초래합니다.

"선호도 데이터셋은 SFT 모델을 사용하여 샘플링되므로, 가능한 경우 참조 정책을 SFT 모델로 초기화합니다. 그러나 SFT 모델을 사용할 수 없는 경우, 선호되는 완성의 우도를 최대화하여 참조 정책을 초기화합니다. 이 절차는 실제 참조 분포와 DPO가 사용하는 참조 정책 간의 분포 변화를 완화하는 데 도움이 됩니다." - [1]에서

이러한 분포 변화를 최소화하고 실제 참조 모델이 우리 선호도 데이터셋에 있는 완성들과 잘 정렬되도록 보장하기 위해, [1]의 저자들은 아래에 묘사된 절차를 권장합니다. 이 절차에서 우리는 먼저 선호도 데이터셋의 선택된 완성에 대해 참조 모델의 지도 미세 조정을 수행한 다음, 이 모델을 DPO로 추가 훈련합니다. 이 예비 SFT 훈련 단계는 DPO의 참조 정책이 선호도 데이터셋을 생성하는 데 사용된 실제 참조 정책과 너무 다르지 않도록 보장합니다.

**DPO에서 오프라인 선호도 데이터로 인한 분포 변화 완화**

DPO 구현을 위한 마지막 중요한 고려 사항은 β 하이퍼파라미터를 올바르게 설정하는 것입니다. 이는 훈련된 정책이 참조 정책과 얼마나 다를 수 있는지를 제어합니다. 기억하십시오. β는 RLHF 목표에서 KL 제약에 곱하는 가중치이며, DPO에서 선호도 정렬의 강도를 제어합니다. β 값이 낮을수록 모델이 데이터에서 관찰된 선호도에 적응하기 위해 더 공격적으로 업데이트됩니다. 일반적으로 β는 [0, 1] 범위의 값으로 설정되며, 낮은 값이 더 일반적입니다. 예를 들어, β = 0.1이 인기 있는 선택이지만, [1]의 저자들은 β = 0.1과 β = 0.5 모두를 탐구합니다.

**전체 DPO 예제.** DPO로 자신만의 LLM을 미세 조정하는 가장 쉬운 방법 중 하나는 HuggingFace TRL 패키지의 `DPOTrainer`를 사용하는 것입니다. 2025년 현재, `DPOTrainer`는 HuggingFace 생태계 내에서 DPO를 적용하는 표준적인 방법으로 널리 사용되고 있으며, `peft` 라이브러리와 결합하여 LoRA(Low-Rank Adaptation)와 같은 효율적인 미세 조정 기법을 지원합니다. DPO 훈련을 수행하려면 i) UltraFeedback [4]와 같은 선호도 데이터셋을 로드하고, ii) 모델/토크나이저를 선택하고(예: Qwen3-0.6B [7]와 같은 작은 모델은 대규모 GPU가 없을 때 좋은 선택입니다), iii) 아래 코드에 표시된 대로 DPO 트레이너를 실행하기만 하면 됩니다.

```python
from trl import DPOConfig, DPOTrainer

# load model and data
model = <load our model>
tokenizer = <load our tokenizer>
train_dataset = <load our preference dataset>

# configure DPO training process
training_args = DPOConfig(output_dir="./dpo_logs/")
trainer = DPOTrainer(
    model=model,
    args=training_args,
    processing_class=tokenizer,
    train_dataset=train_dataset,
)

# execute DPO training
# run the below command to execute this script
# > accelerate launch <script name>
trainer.train()
```

### 요약 및 핵심 요점

DPO(Direct Preference Optimization)는 명시적 보상 모델과 RL을 피하면서 RLHF 목표를 간접적으로 해결하는 LLM용 선호도 튜닝 방법입니다. DPO에서 우리는 RLHF 목표를 재매개변수화하여 정책 자체(및 참조 정책)에서 파생된 암묵적 보상 함수를 구축합니다. 그런 다음, 우리는 정적 선호도 데이터셋에 대해 LLM을 훈련하여 이 암묵적 보상 함수를 최적화하며, 이는 표준 보상 모델과 유사합니다. 이 암묵적 보상 모델링 목표를 해결함으로써, 우리는 RLHF 목표를 해결하는 정책을 간접적으로 산출합니다. 이 접근 방식은 RL 기반 정렬 방법에 대한 더 간단하고, 더 안정적이며, 계산 효율적인 대안을 제공하여 고품질 LLM 정렬에 대한 접근성을 높입니다. 그러나 2025년 최신 연구들 [11, 12]에서는 DPO와 같은 오프라인 직접 정렬 알고리즘과 온라인 RL을 사용하는 정렬 기술(예: PPO 기반 RLHF) 간의 성능 격차를 지속적으로 연구하고 있으며, 특정 시나리오에서는 여전히 온라인 방법이 더 우수한 결과를 보일 수 있음을 시사합니다. 이러한 사실에도 불구하고, DPO는 그 단순성과 효과성 때문에 LLM 후처리 학습에서 여전히 활발히 사용됩니다(종종 온라인 알고리즘과 함께).

뉴스레터가 처음이신가요? 안녕하세요! 저는 Cameron R. Wolfe입니다. 넷플릭스(Netflix)의 딥러닝(Deep Learning) 박사이자 선임 연구 과학자입니다. 이것은 독자들이 AI 연구의 중요한 주제를 더 잘 이해하도록 돕는 Deep (Learning) Focus 뉴스레터입니다. 본 뉴스레터는 항상 무료로 제공될 예정입니다. 뉴스레터가 마음에 드신다면 구독하거나, 유료 구독을 고려하거나, 공유하거나, X와 LinkedIn에서 저를 팔로우해주세요! 구독하기

### 참고 문헌

[1] Rafailov, Rafael, et al. "Direct preference optimization: Your language model is secretly a reward model." Advances in neural information processing systems 36 (2023): 53728-53741.
[2] Stiennon, Nisan, et al. "Learning to summarize with human feedback." Advances in neural information processing systems 33 (2020): 3008-3021.
[3] Tunstall, Lewis, et al. "Zephyr: Direct distillation of lm alignment." arXiv preprint arXiv:2310.16944 (2023).
[4] Cui, Ganqu, et al. "Ultrafeedback: Boosting language models with scaled ai feedback, 2024." URL https://arxiv. org/abs/2310.01377 .
[5] Welleck, Sean, et al. "Neural text generation with unlikelihood training." arXiv preprint arXiv:1908.04319 (2019).
[6] Lambert, Nathan, et al. "Tulu 3: Pushing frontiers in open language model post-training, 2024." URL https://arxiv. org/abs/2411.15124 297 (2025).
[7] Yang, An, et al. "Qwen3 technical report." arXiv preprint arXiv:2505.09388 (2025).
[8] Dubey, Abhimanyu, et al. "The llama 3 herd of models." arXiv e-prints (2024): arXiv-2407.
[9] Kaplan, Jared, et al. "Scaling laws for neural language models." arXiv preprint arXiv:2001.08361 (2020).
[10] Sheng, Guangming, et al. "Hybridflow: A flexible and efficient rlhf framework." Proceedings of the Twentieth European Conference on Computer Systems . 2025.
[11] Tang, Yunhao, et al. "Understanding the performance gap between online and offline alignment algorithms." arXiv preprint arXiv:2405.08448 (2024).
[12] Ivison, Hamish, et al. "Unpacking dpo and ppo: Disentangling best practices for learning from preference feedback." Advances in neural information processing systems 37 (2024): 36602-36633.

1 LLM 맥락에서 강화 학습에 대한 심층적인 설명은 여기를 참조하십시오.
2 더 구체적으로, "온라인(online)"은 정책이 각 단계에서 생성된 새로운 샘플로 반복적으로 업데이트되는 것을 의미하며, "오프라인(offline)"은 모든 훈련 데이터가 미리 고정되어 있음을 의미합니다.
3 "정책(policy)"이라는 단어는 우리가 (RL로) 훈련하는 LLM 또는 모델에 대한 RL 전문 용어입니다.
4 더 구체적으로, KL 발산은 주어진 분포가 참조 분포를 근사화하는 데 사용될 때 얼마나 많은 정보가 손실되는지를 측정합니다.
5 참조 모델이 항상 SFT 모델인 것은 아닙니다. RL 훈련의 이전 모델 체크포인트(checkpoint)일 수도 있습니다. 예를 들어, RLHF의 네 단계 또는 라운드가 순차적으로 수행되는 경우, RLHF의 두 번째 단계에 대한 참조 모델은 RLHF의 첫 번째 단계에서 얻은 모델일 수 있습니다.
6 최적 정책은 분할 함수, 참조 정책, 지수 함수의 곱이며, 이들 모두는 0보다 작을 수 없습니다. 따라서 최적 정책을 구성하는 이 항들의 곱도 음수가 아니어야 합니다.
7 이는 깁스 부등식(Gibbs’ inequality)으로 알려져 있습니다.
8 [1]에서는 이 증명이 더 일반적인 Plackett-Luce 모델(부록 A.5, 17페이지 참조)을 가정하여 제공되지만, 우리는 단순화를 위해 그리고 이 개요의 나머지 설명과 일치시키기 위해 Bradley-Terry 모델을 사용하여 이 증명을 다시 작성합니다.
9 [1]에서 저자들은 이 수정된 함수를 보상 함수의 "투영(projection)"이라고 설명합니다.
10 기억하십시오. DPO는 MLE를 사용하여 LLM을 훈련합니다. 다시 말해, 우리 LLM의 매개변수는 i) 데이터 배치에 대해 이 기울기를 계산하고, ii) 기울기에 스칼라 계수(즉, 학습률)를 곱하고, iii) 이 스케일링된 기울기를 모델 매개변수에서 빼는 과정을 반복하여 직접 업데이트됩니다. 이 기울기가 어떻게 도출되는지 이해하고 싶다면, 이 논문의 17페이지를 참조하십시오.