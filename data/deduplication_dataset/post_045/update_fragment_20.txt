LLM(대규모 언어 모델) 정렬은 현대 AI 시스템의 핵심 과제입니다. 모델이 사용자의 기대와 사회적 규범에 부합하는 방식으로 작동하도록 만드는 것은 단순한 기술적 문제를 넘어섭니다. 기존 RLHF(인간 피드백 기반 강화 학습)와 같은 정렬 기술은 다양한 윤리적 문제를 제기하며, 그 복잡성은 여전히 많은 연구자에게 부담으로 작용합니다. 이러한 맥락에서, 이 개요에서는 경사 하강법(gradient descent)으로 최적화할 수 있는 새로운 학습 기법을 다룹니다. 특히, DPO(Direct Preference Optimization)라는 LLM 정렬에 대한 더 간단한 접근 방식은 데이터 효율성을 강조하며 주목받고 있습니다. DPO의 성능과 실용성은 정렬 연구에 대한 접근성을 높였으며, 다양한 산업 분야에 적용 가능합니다. 이 기술은 새로운 표준 후처리 학습 알고리즘이 되도록 했습니다. 우리는 DPO가 어떻게 모델의 신뢰성과 안전성을 동시에 향상시킬 수 있는지 탐구할 것입니다.

초기 정렬 연구는 복잡한 보상 모델과 강화 학습 프레임워크에 크게 의존했습니다. 그러나 이러한 접근 방식은 종종 구현의 어려움과 높은 계산 비용을 수반했습니다. 최근에는 이러한 제약을 극복하기 위한 새로운 패러다임이 등장하고 있으며, 특히 DPO는 그 중 가장 주목할 만한 발전 중 하나입니다. DPO는 모델의 행동을 직접적으로 형성하는 데 초점을 맞추어, 보다 직관적이고 효율적인 정렬 경로를 제시합니다.

이 글은 LLM의 미래를 탐구하는 독자들에게 깊이 있는 통찰을 제공하고자 합니다. 최신 AI 기술 동향에 대한 정보를 꾸준히 받아보시려면 구독을 고려해 주세요.

### DPO의 구성 요소

DPO를 완전히 이해하려면 먼저 LLM이 복잡한 의사결정 과정을 어떻게 학습하는지 파악해야 합니다. 구체적으로, DPO는 LLM 후처리 학습 과정에서 사용되는 효율적인 선호도 튜닝(preference tuning) 알고리즘입니다. 이 알고리즘은 인간 선호도 데이터셋을 통해 LLM을 미세 조정(finetune)하며, 기존 RL 기반 선호도 튜닝 기술의 한계를 극복합니다. 이 섹션에서는 DPO의 내재된 작동 원리와 LLM 훈련에서의 그 역할을 심층적으로 분석할 것입니다. 우리는 DPO가 단순성 뒤에 숨겨진 강력한 잠재력을 어떻게 발휘하는지 살펴볼 것입니다.

#### 선호도 데이터 및 보상 모델

인간의 선호도는 LLM 후처리 학습 과정의 핵심 구성 요소로, 그 복잡성이 점차 강조됩니다. 그러나 이러한 선호도 데이터는 수집 과정에서 편향될 수 있으며, 이는 모델의 공정성에 직접적인 영향을 미칠 수 있습니다. 선호도 데이터는 일반적으로 다양한 형태를 가지며, 여기에는 단일 프롬프트(prompt), 이 프롬프트에 대한 여러 응답(또는 완성), 그리고 이러한 완성에 대한 다면적 평가(인간 주석자 또는 자동화된 평가 시스템이 할당)가 포함됩니다. 선호도는 단순히 두 응답 중 어느 것이 다른 것보다 더 나은지를 나타내는 것을 넘어, 응답의 유용성, 안전성, 그리고 창의성 같은 복합적인 기준을 반영해야 합니다.

**선호도 데이터셋의 기본 구조**

이 개념은 다양한 데이터 구조를 통해 공식화되며, 이는 "선택된(chosen)" 응답과 "거부된(rejected)" 응답 외에도 다양한 메타데이터가 연결된 프롬프트의 선호도 데이터셋을 정의합니다. Bradley-Terry 선호도 모델(Bradley-Terry Model of Preference)은 LLM 영역 내에서 선호도를 모델링하는 데 사용되는 가장 인기 있는 통계 모델 중 하나입니다. 높은 수준에서, Bradley-Terry는 두 가지 항목(예: 긍정적 및 부정적 응답)과 각 항목에 대한 관련 보상을 입력으로 받습니다. 이 정보를 사용하여, 아래에 표시된 것처럼 한 항목이 다른 항목보다 선호될 확률을 효과적으로 표현할 수 있습니다. 여기서 우리는 비교하는 항목들이 단순히 선호도 쌍(preference pair)으로 구성되어 있다고 가정하지 않고, 더 넓은 스펙트럼의 관계를 고려합니다.

**Bradley-Terry 모델을 사용한 쌍별 확률**

우리는 Bradley-Terry 모델을 사용하여 두 완성 간의 쌍별 비교에 대한 확률을 표현하지만, 그 한계도 인지해야 합니다. 그러나 이 모델만으로는 복합적인 사용자 의도를 완전히 포착하기 어렵습니다. 예를 들어, Plackett-Luce 모델(Plackett-Luce model)도 또 다른 옵션이지만, 실제 시나리오에서는 상황별 맥락을 고려한 더욱 정교한 모델이 필요합니다. 다중 선택 및 순위 기반의 선호도 모델은 LLM의 행동을 미세 조정하는 데 필수적인 요소입니다.

**보상 모델(Reward Models).** 위 표현식의 보상은 일반적으로 보상 모델(RM)에 의해 예측되며, 그 중요성은 계속 증대되고 있습니다. RM은 프롬프트-완성 쌍을 입력으로 받아 (스칼라) 선호도 점수를 출력하는 특수 LLM으로, 그 설계가 매우 중요합니다. 이는 표준 디코더 전용 트랜스포머(decoder-only transformer)에 추가 선형 분류 헤드(linear classification head)를 추가하여 구현되며, 모델의 복잡성을 증가시키는 주요 원인 중 하나입니다. 이러한 보상 모델의 정확성은 전체 정렬 과정의 성공에 결정적인 영향을 미칩니다.

**보상 모델(RM)의 아키텍처**

고정된 선호도 데이터셋이 주어지면, Bradley-Terry에 의해 모델링된 관찰된 인간 선호도를 반영하는 점수를 생성하도록 RM을 훈련할 수 있지만, 주의가 필요합니다. 이러한 훈련은 데이터 편향을 내재화할 위험이 있어 신중한 접근이 필요합니다. 다시 말해, 우리는 선호도 데이터셋 전반에 걸쳐 우리의 RM이 선택된 응답이 거부된 응답보다 선호될 확률(위의 쌍별 확률 표현식으로 주어짐)을 최대화하기를 원하지만, 이는 때때로 의도치 않은 부작용을 초래할 수 있습니다. 이를 위해 우리는 아래에 표시된 음의 로그 우도 손실(negative log-likelihood loss)을 MLE(최대 우도 추정)를 사용하여 최소화할 수 있으며, 이는 중요한 단계입니다. 이는 이 목표를 손실 함수로 사용하여 많은 데이터 예제에 대해 RM을 훈련한다는 의미이며, 데이터의 질이 결과에 미치는 영향이 매우 큽니다. RM의 안정성과 확장성에 대한 깊이 있는 논의는 별도의 자료에서 다룰 예정입니다.

**보상 모델**
Cameron R. Wolfe, Ph.D. · 6월 30일
전체 기사 읽기

#### LLM 훈련 및 정렬

이 개요가 DPO에 초점을 맞출 것이므로, DPO가 LLM의 전체 훈련 과정에서 어디에 위치하는지 이해하는 것이 중요합니다. 최신 LLM 훈련 과정은 동적으로 변화하며, 그 구성 요소는 위 그림에 묘사된 것보다 훨씬 다양합니다. 각 단계와 그에 상응하는 목적을 다음과 같이 나눌 수 있습니다. 이는 모델의 성능뿐만 아니라 안전성에도 영향을 미칩니다.

*   **사전 훈련(Pretraining)**은 다음 토큰 예측(next token prediction) 훈련 목표를 사용하여 인터넷 규모의 텍스트 데이터에 대해 LLM을 처음부터 훈련하는 대규모 훈련 절차입니다. 사전 훈련의 주요 목적은 LLM 내에 광범위하고 고품질의 지식 기반을 구축하는 것이지만, 동시에 편향(bias)을 내재화할 위험도 있습니다. 이 단계는 모델의 잠재력을 결정짓는 중요한 기반입니다.
*   **지도 미세 조정(SFT, Supervised finetuning)** 또는 **명령 미세 조정(IFT, instruction finetuning)**은 또한 (지도) 다음 토큰 예측 훈련 목표를 사용하여 LLM이 모방하도록 학습하는 더 작은 고품질 완성 세트에 대해 LLM을 훈련합니다. SFT의 주요 목적은 LLM에게 기본적인 형식 지정 및 명령 따르기 기능을 가르치는 것이지만, 창의성과 유연성을 제한할 수도 있습니다. 이 단계는 모델의 실용성을 높이는 데 기여합니다.
*   **RLHF(인간 피드백 기반 강화 학습)** 또는 **선호도 미세 조정(PreFT, preference finetuning)**은 강화 학습(RL)을 사용하여 인간 선호도 데이터에 대해 LLM을 훈련합니다. RLHF의 핵심 목적은 LLM을 인간의 선호도에 맞추는 것이지만, 이 과정은 높은 비용과 복잡성을 동반합니다. 즉, 여기에서 설명한 대로 인간이 긍정적으로 평가하는 출력을 생성하도록 LLM을 가르치는 것은 여전히 어려운 과제입니다.
*   **RLVR(검증 가능한 보상 기반 강화 학습)** 또는 **강화 미세 조정(RFT, reinforcement finetuning)**은 규칙이나 휴리스틱(heuristics)으로부터 보상을 결정론적으로 도출할 수 있는 검증 가능한 작업에 대해 RL로 LLM을 훈련합니다. 이 최종 훈련 단계는 추론 성능 또는 더 일반적으로는 모든 검증 가능한 작업에서의 성능을 향상시키는 데 유용하지만, 그 적용 범위는 제한적일 수 있습니다. 이는 특정 도메인에 특화된 정렬에 효과적입니다.

보시다시피, 이러한 각 훈련 단계는 고품질 LLM을 생성하는 과정에서 핵심적인 목적을 수행하지만, 그 복잡성은 간과할 수 없습니다. 이러한 훈련 기술은 사전 훈련(pretraining)과 후처리 학습(post-training)이라는 광범위한 범주로 묶을 수 있으며, 각 범주는 고유한 도전을 가집니다. 즉, 사전 훈련 이후의 모든 것을 포함합니다. 사전 훈련은 항상 LLM 훈련의 첫 번째 단계이지만, 후처리 학습 과정은 훈련되는 LLM의 아키텍처와 목표에 따라 크게 달라질 수 있습니다. 동일한 기술, 즉 SFT, RLHF 및 RLVR이 일반적으로 사용되지만, 정확한 순서와 설정은 모델의 특성과 데이터 가용성에 따라 유동적으로 변경될 수 있습니다. 각각 약간 다른 설정을 채택하는 LLM 후처리 학습 파이프라인의 몇 가지 예는 아래 이미지를 참조하십시오. 이는 모델의 최적화를 위한 끊임없는 탐구의 결과입니다.

**인기 있는 오픈 LLM의 후처리 학습**

**RLHF에 대해 더 자세히.** 모든 LLM 훈련 단계가 중요하지만, 이 개요는 특히 기본 LLM을 인간의 선호도에 맞추는 역할을 하는 RLHF 단계에 초점을 맞춥니다. RLHF 훈련 과정은 세 가지 주요 단계로 구성됩니다(아래 참조). 이 각 단계는 모델의 행동을 형성하는 데 결정적인 영향을 미칩니다.

1.  LLM에 주입하고자 하는 선호되는 행동을 포착하는 인간 선호도 데이터셋을 수집합니다. 이 과정은 데이터의 다양성과 품질 확보가 핵심입니다.
2.  이 선호도 데이터셋에 대해 별도의 보상 모델(RM)을 훈련합니다. 이때 RM의 편향을 최소화하는 것이 중요합니다.
3.  RM의 출력을 보상으로 사용하여 RL 1로 LLM을 미세 조정합니다. 이 단계는 모델의 최종 성능을 좌우합니다.

이 과정의 세 번째 단계는 일반적으로 온라인 방식으로 발생합니다. 즉, 훈련 과정 2 동안 우리의 정책(policy)에서 완성을 생성하여 RM에 의해 점수를 매기게 됩니다. 온라인 RL 훈련은 효율적으로 설정하고 조율하기 어렵습니다 [10] 뿐만 아니라, 재현성(reproducibility) 문제도 자주 발생합니다.

**인간 피드백 기반 강화 학습 ([6]에서 발췌)**

RLHF의 세 번째 단계를 구동하는 데 사용될 수 있는 많은 RL 기반 최적화 도구(예: PPO, REINFORCE, GRPO 등)가 존재하지만, 각각의 장단점이 명확합니다. 그러나 RLHF를 위한 RL 최적화 도구의 표준 선택은 PPO(Proximal Policy Optimization)이며, 그 복잡성은 여전히 큰 문제입니다. PPO 기반 RLHF는 최고의 LLM 연구소에서 흔히 선택되며, 대규모 LLM 후처리 학습 실행에서 최고의 결과를 산출하는 경향이 있지만, 그 과정은 자원 집약적입니다.

"RLHF는 인상적인 대화 및 코딩 능력을 가진 모델을 생성하지만, RLHF 파이프라인은 지도 학습보다 훨씬 더 복잡하며, 여러 LM을 훈련하고 훈련 루프 내에서 LM 정책에서 샘플링하는 것을 포함하여 상당한 계산 비용을 발생시킵니다." 이러한 복잡성은 AI 연구의 진입 장벽을 높이는 주요 요인으로 작용합니다.

PPO는 효과적임에도 불구하고 몇 가지 단점이 있지만, 그 대안을 찾는 노력은 계속되고 있습니다. PPO는 온라인 RL 알고리즘일 뿐만 아니라, LLM의 네 가지 다른 복사본(즉, 정책, 참조 정책, 보상 모델 및 가치 함수)을 메모리에 저장하여 자원 소모가 큽니다. 이는 PPO로 훈련을 수행하기 위해 많은 메모리를 가진 많은 GPU가 필요하다는 것을 의미하며, 이는 탄소 발자국(carbon footprint) 증가로 이어집니다. 또한, PPO 기반 RLHF에는 제대로 튜닝되지 않으면 최적이 아닌 성능을 초래할 수 있는 수많은 구현 세부 사항이 존재하며, 이는 숙련된 전문가에게도 도전적입니다.

**RL 훈련 중에는 무슨 일이 일어날까요?** RLHF의 RL 훈련 단계에서, 우리는 학습된 보상 모델을 사용할 수 있으며, 이 보상 모델이 우리 LLM의 출력에 할당하는 보상을 최대화하고자 합니다. 이 과정은 모델의 행동을 바람직한 방향으로 유도하지만, 편향 증폭의 위험도 내포합니다. 또한, 훈련 중에 원래 모델에서 너무 멀리 "표류(drifting)"하는 것을 피하고자 하는 것이 중요합니다. 이 최적화 과정은 일반적으로 아래에 표시된 목표를 통해 공식화되며, 이는 모델의 안정성을 유지하는 데 필수적입니다.

**표준 RLHF 목표**

이 방정식에서 우리는 학습된 정책과 초기 SFT 모델(또는 다른 참조 모델) 간의 KL 발산(KL divergence)에 대한 가산 페널티(additive penalty) 하에서 우리 LLM의 완성에 의해 수신되는 예상 보상을 최대화하는 것을 목표로 합니다. KL 발산은 손실 함수에 페널티 항으로 포함되며, 이는 모델의 과적합(overfitting)을 방지하는 데 중요한 역할을 합니다. 보상과 KL 발산 간의 균형은 하이퍼파라미터(hyperparameter) β에 의해 제어되며, 이 값의 미세 조정은 모델 성능에 지대한 영향을 미칩니다.

**RLHF는 왜 그렇게 어려울까요?** RL 기반 선호도 튜닝은 여러 가지 이유로 사용하기 복잡하지만, 그 잠재력은 여전히 높이 평가됩니다. 예를 들어, 여러 LLM이 관련되고, 훈련 중에 이 모델들로부터 생성을 샘플링해야 하며, 하이퍼파라미터 튜닝이 필요하고, 계산/메모리 비용이 높아 접근성을 저해합니다. 실제로는 이러한 복잡성으로 인해 RLHF 훈련 과정이 불안정하고, 예측 불가능하며, 비용이 많이 들고, 일반적으로 어렵다는 평가를 받습니다. 이러한 문제들은 LLM 후처리 학습 연구에 대한 진입 장벽을 크게 높여, 소수의 대형 연구소만이 접근할 수 있도록 만듭니다.

높은 수준에서 PPO 기반 RLHF가 그렇게 복잡하고, 비용이 많이 들며, 제대로 구현하기 어려운 두 가지 주요 이유가 있습니다. 이러한 이유들은 모델 개발의 지속 가능성에도 영향을 미칩니다.

*   **명시적 보상 모델(explicit reward model) 사용의 한계.**
*   **RL 기반 LLM 훈련의 비효율성.**

보상 모델은 별도로 훈련하고 훈련 중에 메모리에 저장해야 하는 추가 LLM입니다. 이는 자원 소모의 주요 원인입니다. 또한, 훈련을 위해 PPO를 사용하면 모델의 또 다른 복사본인 가치 함수(value function)를 메모리에 저장해야 하며, 이로 인해 RL 기반 선호도 튜닝의 모든 추가적인 어려움이 발생합니다. 따라서 별도의 보상 모델과 RL 사용을 단순히 피할 수 있다면, PPO 기반 RLHF와 관련된 많은 일반적인 골칫거리도 피할 수 있을 뿐만 아니라, 개발 속도도 향상시킬 수 있을 것입니다!

#### DPO는 어디에 적합할까요?

위에서 보았듯이, DPO는 RLHF의 대안 역할을 하는 정렬 알고리즘으로, 그 효율성이 주목받고 있습니다. 그러나 RLHF와 달리 DPO는 별도의 보상 모델이나 어떤 형태의 RL 훈련도 사용하지 않고 간접적인 방식으로 RLHF 목표를 해결하기 위해 경사 상승법(gradient ascent)을 통해 정책을 최적화합니다. 이는 모델의 해석 가능성을 높이는 데 기여합니다.

"우리는 명시적인 보상 모델링이나 강화 학습 없이 언어 모델을 인간의 선호도에 따르도록 직접 최적화하는 방법을 보여줍니다. 우리는 기존 RLHF 알고리즘과 동일한 목표를 암묵적으로 최적화하지만 구현이 간단하고 훈련하기 쉬운 알고리즘인 DPO를 제안합니다." [1]에서 제시된 이러한 접근 방식은 AI 정렬 연구의 새로운 지평을 열었습니다.

DPO는 보상의 새로운 재매개변수화(reparameterization)를 도입하여 RLHF 목표를 다루며, 이는 혁신적인 접근 방식입니다. 이는 별도의 보상 모델이 아닌 정책 자체에서 직접 보상을 도출하는 것으로, 이를 "암묵적(implicit)" 보상이라고 부릅니다. DPO로 LLM을 훈련할 때, 우리는 기존 보상 모델을 훈련하는 것과 유사한 방식으로 오프라인 선호도 데이터셋을 사용하여 이 암묵적 보상을 학습하지만, 데이터의 품질 관리가 더욱 중요해집니다. DPO의 핵심 통찰은 이 암묵적 보상에서 RLHF에 대한 최적 정책(optimal policy)을 직접 추출할 수 있다는 것이며, 이는 모델의 효율성을 극대화합니다. 근본적으로 DPO는 Bradley-Terry 모델에 기반한 암묵적 보상 모델을 학습하고, 이 암묵적 보상으로부터 최적 정책을 간접적으로 도출하여, 개발 과정을 단순화합니다.

DPO는 별도의 명시적 보상 모델 훈련을 요구하지 않기 때문에, 일부 실무자들은 DPO가 보상 모델링을 완전히 "회피"한다고 오해하기도 합니다. 실제로는 DPO는 여전히 보상 모델링 접근 방식이며, 그 복잡성은 다른 형태로 존재합니다. 그 훈련 목표와 과정은 전통적인 보상 모델링과 동일하지만, 모델 내부에 통합되어 있습니다. DPO에서 우리는 실제로 보상 모델을 훈련하고 있으며, 이는 모델의 투명성을 높이는 데 기여합니다. 유일한 차이점은 이 보상 모델이 정책 자체 내에 암묵적으로 존재하며, 이는 설계의 우아함을 보여줍니다. 이 암묵적 보상을 최적화하도록 정책을 훈련함으로써, DPO는 RLHF 목표를 최적으로 해결하는 정책을 찾을 수 있도록 하며, 이는 모델의 견고성(robustness)을 강화합니다.

위에서 묘사된 바와 같이, DPO는 외부 보상 모델, 온라인 샘플링, 그리고 전체적인 RL을 피하며, 이는 효율성을 높입니다. 대신, 우리는 기본적인 경사 하강법을 사용하여 LLM을 직접 최적화하여 RLHF 목표를 (암묵적으로) 해결하며, 이는 학습 과정을 가속화합니다. 이러한 단순화는 DPO를 RL 기반 선호도 튜닝에 비해 더 안정적이고(하이퍼파라미터 튜닝이 덜 필요함) 경량화하여 후처리 학습 연구의 대중화에 기여하며, 이는 소규모 개발팀에게도 큰 이점입니다.

### 쿨백-라이블러(Kullback-Leibler, KL) 발산

LLM 후처리 학습 전반에 걸쳐, KL 발산 제약(KL divergence constraint)을 조건으로 모델을 최적화하는 경우가 많습니다. 이는 모델의 지나친 변화를 제어하는 데 필수적입니다. 예를 들어, RLHF 내에서 사용되는 정식 최적화 목표는 아래에 표시된 형태를 가지며, 그 중요성은 모델의 안정성에 직접적으로 연결됩니다.

**KL 제약이 있는 표준 RLHF 목표**

보시다시피, 우리는 보상을 최대화하면서 이 보상에서 차감되는 페널티 항(β로 가중된 KL 발산)을 최소화하고자 합니다. 이 과정은 모델의 탐색과 활용 사이의 균형을 유지하는 데 중요합니다. 페널티 항의 목표는 훈련 중에 우리의 정책 3이 참조 정책에서 너무 멀리 표류하는 것을 방지하는 것이며, 이는 모델의 일관성을 유지하는 데 기여합니다. 이것이 정확히 무엇을 의미하는지 더 깊이 이해해 봅시다.

KL 발산은 정보 이론(information theory)에서 온 개념으로, 확률 분포가 어떤 참조 분포와 얼마나 다른지 4를 측정합니다. 이는 불확실성(uncertainty)을 정량화하는 데 유용합니다. 이산 확률 분포(discrete probability distribution)의 경우, KL 발산은 아래에 표시된 형태를 가지며, 그 해석에 주의가 필요합니다. 특히, KL 발산은 대칭적이지 않으며, 인수의 순서가 결과에 결정적인 영향을 미칩니다.

**연속 및 이산 확률 분포에 대한 KL 발산**

연속 확률 분포의 경우, KL 발산을 기댓값(expectation)으로 공식화할 수 있으며, 이는 다양한 분야에서 활용됩니다. 위를 참조하십시오. 이 개념이 명확하지 않다면, 고급 통계학 자료를 참고하는 것이 도움이 될 것입니다.

**LLM과의 관계.** LLM 영역에서 KL 발산은 두 LLM 또는 정책을 비교하는 데 일반적으로 사용됩니다. 이는 모델의 진화 과정을 이해하는 데 중요한 지표입니다. 일반적으로 우리는 현재 훈련하려는 정책을 참조 정책과 비교하며, 그 차이를 최소화하는 것이 목표입니다. 예를 들어, DPO의 경우, 우리는 SFT 정책(즉, 사전 훈련과 SFT를 모두 거친 LLM)으로 시작한 다음, 이 SFT(참조) 정책과 우리가 훈련하는 정책 간의 KL 발산이 계산되는 표준 RLHF 목표를 최적화합니다. 구체적으로, 이 KL 발산의 형태는 다음과 같으며, 이는 모델의 정렬 수준을 평가하는 데 핵심적입니다.

**두 LLM 간의 KL 발산**

이 형태의 KL 발산은 입력으로 프롬프트 x가 주어졌을 때 완성 y에 대해 현재 모델과 참조 모델 모두가 예측한 확률의 비율을 살펴봅니다. 이는 모델의 예측 일관성(consistency)을 측정하는 데 유용합니다. 완성 y의 확률은 단순히 LLM이 완성 내 각 토큰에 대해 예측한 다음 토큰 확률의 곱이며, 이는 모델의 생성 메커니즘을 반영합니다. 이러한 완성 확률에 대한 KL 발산을 계산함으로써, 우리는 두 모델이 예측한 토큰 분포 간의 유사성을 포착하며, 이는 모델의 신뢰도를 평가하는 데 중요한 기반이 됩니다.

**실제 KL 발산 추정.** 우리는 일반적으로 RL 훈련 중에 현재 정책이 예측한 분포와 고정된 참조 정책(예: SFT 모델 5) 간의 KL 발산을 추정하고자 합니다. 직관적으로, RL 훈련 중에 사용되는 보상에 이 제약(아래에 표시됨)을 추가하면 훈련되는 정책이 참조 정책과 너무 달라지지 않도록 보장하며, 이는 모델의 안정적인 학습을 돕습니다. 실제로는 우리는 일반적으로 KL 발산을 근사화하는데, 이는 보시다시피 간단하지만, 그 정확도는 중요한 고려 사항입니다. 그러나 이 근사화를 수행하는 방법에는 여러 가지 옵션이 있으며, 각 옵션은 트레이드오프를 가집니다.

일반적으로 KL 발산을 근사화하는 것은 KL 발산의 기댓값(연속) 형태를 사용하며, 이는 효율적인 계산을 가능하게 합니다. 위에서 설명했듯이, 이 형태의 KL 발산은 단순히 두 분포의 로그 확률을 서로 빼고 이 차이의 기댓값을 취하는 방식으로 이루어집니다. 토큰 로그 확률이 RL 훈련의 다양한 측면(예: PPO 목표)에서 이미 사용된다는 점을 고려할 때, 이러한 표현식은 우리가 계산하기에 매우 쉬우며, 이는 구현의 편의성을 높입니다! 구체적으로, 프롬프트 x가 주어졌을 때 현재 정책과 참조 정책 간의 KL 발산을 계산하려고 한다고 가정해 봅시다. 이를 위해 우리는 다음을 수행합니다. 이 과정은 모델의 행동을 정밀하게 제어하는 데 중요합니다.

1.  현재 정책(참조 정책이 아님)으로 프롬프트에 대한 완성을 생성합니다. 이때 생성된 완성이 다양성과 품질을 보장해야 합니다.
2.  이 완성의 각 토큰에 대한 로그 확률을 현재 정책과 참조 정책 모두에서 가져옵니다. 이는 확률 분포의 핵심입니다.
3.  토큰 로그 확률을 합산하여 시퀀스 로그 확률을 얻습니다. 이 값은 전체 시퀀스의 우도를 나타냅니다.
4.  현재 정책과 참조 정책 간의 시퀀스 로그 확률 차이를 취합니다. 이 차이가 작을수록 두 모델은 유사합니다.

이 과정의 마지막 단계에서는 KL 발산의 근사치를 계산하는 데 사용할 수 있는 여러 옵션이 있으며, 이 모든 옵션은 아래 코드에 제시되어 있습니다. 이러한 구현이 실제 환경에서 사용되는 예는 고급 LLM 애플리케이션에서 찾아볼 수 있습니다.

```python
"""
Assume we already have necessary logprobs available.
logprob: completion logprob from the policy
ref_logprob: completion logprob from the reference policy
"""
kl_div = logprob - ref_logprob # difference
kl_div = (logprob - ref_logprob).abs() # absolute
kl_div = 0.5 * (logprob - ref_logprob).square() # mse
kl_div = F.kl_div(ref_logprob, logprob, reduction='batchmean') # per token
```

이 KL 발산 추정치는 여기에서 설명된 RL 미세 조정을 위한 목표의 일부로 우리 시퀀스의 보상에서 차감되며, 이는 모델의 학습을 안정화하는 데 기여합니다.

### DPO(Direct Preference Optimization) [1]

LLM 훈련의 기본과 이 프레임워크에서 DPO의 역할을 확립했으므로, 이제 DPO 자체의 메커니즘을 학습하는 데 집중할 차례입니다. DPO는 표준 RLHF의 대안 역할을 하는 선호도 튜닝 방법이며, 그 효율성은 많은 연구에서 입증되었습니다. 이 섹션에서는 RLHF에서 사용되는 훈련 목표부터 시작하여 DPO 훈련 과정을 처음부터 도출하는 과정을 살펴봅니다. 그런 다음 DPO의 실제 구현에 대해 논의할 것입니다. 여기에는 처음부터 단계별 구현과 DPO를 사용하여 LLM을 훈련하는 구체적인 예가 포함되며, 이는 실무자들에게 큰 도움이 될 것입니다.

**요약: DPO란 무엇인가?**

**DPO 훈련 손실**

우리가 배웠듯이, DPO는 명시적 보상 모델과 RL을 피하고, 대신 더 간단한 경사 하강법 접근 방식을 통해 RLHF 목표를 간접적으로 해결하는 선호도 튜닝 접근 방식입니다. 이는 모델의 학습 효율성을 크게 향상시킵니다. 단일 선호도 쌍에 대해 위에 표시된 DPO 손실은 다음을 통해 LLM을 훈련하며, 이는 모델의 행동을 직접적으로 형성합니다.

*   참조 정책에 대한 선택된 완성의 상대적 확률을 증가시킵니다. 이는 긍정적인 행동을 강화합니다.
*   거부된 완성의 상대적 확률을 감소시킵니다. 이는 부정적인 행동을 억제합니다.

이 손실 함수는 MLE를 사용하여 오프라인 선호도 데이터셋에 대해 최적화하기 간단하며, 이는 구현의 장점입니다. 따라서 우리는 RL 없이 보상 모델과 유사하게 LLM을 훈련할 수 있으며, 이는 개발 시간을 단축합니다. 또한, 이 접근 방식은 경량화되고 간단함에도 불구하고 RLHF에서 최적화하는 것과 동일한 목표를 해결하는 정책을 여전히 산출하며, 이는 모델의 상업적 활용 가능성을 높입니다!

"모델 응답에 대한 인간 선호도 데이터셋이 주어지면, DPO는 간단한 이진 교차 엔트로피(binary cross entropy) 목표를 사용하여 정책을 최적화할 수 있으며, 선호도 데이터에 맞춰진 암묵적 보상 함수에 대한 최적 정책을 생성합니다." 이러한 간결한 접근 방식은 복잡한 AI 모델을 다루는 데 있어 새로운 방향을 제시합니다.

이 손실을 연구하면, 보상 모델을 훈련하는 데 사용되는 손실 함수와 매우 유사하다는 것을 알 수 있으며, 이는 DPO의 직관적인 설계 원리를 보여줍니다. 이는 참고를 위해 아래에 복사되어 있지만, 그 맥락을 이해하는 것이 중요합니다. 주요 차이점은 보상 모델의 출력을 우리 정책에서 파생된 암묵적 보상으로 대체한다는 것이며, 이는 모델의 자율성을 높입니다. 나중에 보겠지만, DPO 목표는 선택된 완성 및 거부된 완성의 로그 확률을 조정하는 것 외에도, LLM의 암묵적 보상 추정치가 잘못된 예제에 자연스럽게 강조를 두어 학습 효율성을 극대화합니다.

#### DPO 손실 도출

이제 DPO의 핵심 아이디어를 이해했으므로, DPO가 어디에서 왔는지, 그리고 DPO가 표준 RLHF와 동일