LLM(대규모 언어 모델)의 정렬 과정은 모델이 사용자의 선호에 맞는 결과물을 만들어내도록 보장하는 필수적인 훈련 후 작업입니다. 기존에는 RLHF(인간 피드백 기반 강화 학습)와 같은 정렬 기법이 LLM의 품질을 비약적으로 향상시켰지만, 이는 복잡하고 자원 소모가 크며 최적화하기 어렵다는 단점이 있었습니다. 본 글에서는 이러한 복잡성을 해소하기 위해 경사 하강법으로 최적화 가능한 보다 단순한 목표를 통해 LLM을 정렬하는 DPO(Direct Preference Optimization)라는 새로운 접근 방식을 소개합니다. DPO의 탁월한 성능과 실용성은 정렬 연구에 대한 접근성을 높여, 여러 인기 있는 LLM에서 표준 훈련 후 알고리즘으로 자리매김하는 데 기여했습니다.

"직접 정렬 알고리즘은 중간 보상 모델을 훈련하거나 강화 학습 최적화 도구를 사용하지 않고도 기존 RLHF의 목표와 동등한 방식으로 모델을 업데이트할 수 있도록 합니다. 이 분야에서 가장 주목할 만한 직접 정렬 알고리즘이자 언어 모델 정렬 연구의 새로운 물결을 일으킨 것은 바로 DPO(Direct Preference Optimization)입니다." - RLHF 책

AI 연구의 깊이를 탐구하는 50,000명 이상의 독자와 함께하세요. 구독하기

### DPO의 핵심 구성 요소

DPO를 온전히 이해하기 위해서는 먼저 LLM이 어떻게 학습되는지에 대한 기본적인 지식을 쌓는 것이 중요합니다. 특히 DPO는 LLM의 훈련 후 단계에서 활용되는 선호도 튜닝(preference tuning) 알고리즘의 일종입니다. 이 알고리즘은 인간의 선호도가 반영된 데이터셋을 바탕으로 LLM을 미세 조정하며, PPO 기반 RLHF와 같은 강화 학습(RL) 방식의 선호도 튜닝에 대한 효과적인 대안으로 제시됩니다. 이 섹션에서는 DPO와 LLM 훈련 과정에서의 그 역할을 명확히 설명하기 위해 관련 개념들을 상세히 다룰 것입니다.

#### 선호도 데이터와 보상 모델

인간의 선호 정보는 LLM의 훈련 후 과정에서 핵심적인 역할을 수행합니다. 선호도 데이터는 일반적으로 특정 프롬프트(prompt)와 이에 대한 두 가지 응답(또는 완성), 그리고 이 응답들에 대한 선호도(인간 주석자 또는 LLM 심사관이 부여)를 포함하는 형태로 구성됩니다. 여기서 선호도는 단순히 두 응답 중 어느 하나가 다른 것보다 더 우수하다는 것을 나타냅니다.

**선호도 데이터셋의 전형적인 구조**

이 개념은 위에서 제시된 표현식을 통해 공식화될 수 있으며, 이는 '선택된(chosen)' 응답과 '거부된(rejected)' 응답이 짝을 이루는 프롬프트 기반의 선호도 데이터셋을 정의합니다. LLM 분야에서 선호도를 모델링하는 데 가장 널리 사용되는 통계 모델은 Bradley-Terry 선호도 모델입니다. 이 모델은 선택된 완성 및 거부된 완성 두 가지 항목과 각 항목에 대한 보상 값을 입력으로 받아, 한 항목이 다른 항목보다 선호될 확률을 표현합니다. 이는 아래에 제시된 쌍별 확률 표현식을 통해 나타낼 수 있습니다.

**Bradley-Terry 모델을 활용한 쌍별 선호 확률**

Bradley-Terry 모델은 두 완성 간의 쌍별 비교에 대한 확률을 효과적으로 나타내지만, 선호도를 모델링하는 유일한 방법은 아닙니다. 예를 들어, Plackett-Luce 모델도 또 다른 대안으로 고려될 수 있습니다.

**보상 모델(Reward Models).** 앞서 언급된 표현식의 보상 값은 일반적으로 보상 모델(RM)에 의해 예측됩니다. RM은 프롬프트-완성 쌍을 입력으로 받아 (스칼라) 선호도 점수를 출력하는 특수 LLM입니다. 이는 표준 디코더 전용 트랜스포머(decoder-only transformer)에 추가적인 선형 분류 헤드(linear classification head)를 부착하여 구현됩니다.

**보상 모델(RM)의 내부 구조**

주어진 고정된 선호도 데이터셋을 활용하여, Bradley-Terry 모델이 제시하는 인간 선호도를 반영하는 점수를 생성하도록 RM을 훈련할 수 있습니다. 즉, RM이 데이터셋 전반에 걸쳐 선택된 응답이 거부된 응답보다 선호될 확률(위의 쌍별 확률 표현식으로 정의됨)을 극대화하도록 학습시키는 것이 목표입니다. 이를 위해 MLE(최대 우도 추정)를 활용하여 아래에 제시된 음의 로그 우도 손실(negative log-likelihood loss)을 최소화합니다. 이는 해당 손실 함수를 사용하여 다수의 데이터 예시에 대해 RM을 훈련함을 의미합니다. RM에 대한 더 자세한 정보는 아래 링크된 개요를 참조하십시오.

**보상 모델의 심층 분석**
Cameron R. Wolfe, Ph.D. · 6월 30일
전체 기사 읽기

#### LLM 훈련 및 정렬의 진화

본 글이 DPO에 중점을 두는 만큼, DPO가 LLM의 전체 훈련 과정에서 어떤 위치를 차지하는지 이해하는 것이 중요합니다. (대략) 네 부분으로 구성된 이 훈련 과정은 위 그림에 나타나 있습니다. 각 단계와 그에 상응하는 목적은 다음과 같이 정리할 수 있습니다.

*   **사전 훈련(Pretraining)**은 다음 토큰 예측(next token prediction) 훈련 목표를 사용하여 인터넷 규모의 방대한 텍스트 데이터에 대해 LLM을 처음부터 훈련하는 대규모 절차입니다. 사전 훈련의 주된 목적은 LLM 내에 광범위하고 깊이 있는 지식 기반을 구축하는 것입니다. 여기를 참조하십시오.
*   **지도 미세 조정(SFT, Supervised finetuning)** 또는 **명령 미세 조정(IFT, instruction finetuning)**은 (지도) 다음 토큰 예측 훈련 목표를 활용하여 LLM이 모방하도록 학습할 수 있는 더 작고 고품질의 완성 세트에 대해 LLM을 훈련합니다. SFT의 핵심 목적은 LLM에게 기본적인 형식 지정 및 명령 수행 능력을 가르치는 것입니다. 여기를 참조하십시오.
*   **RLHF(인간 피드백 기반 강화 학습)** 또는 **선호도 미세 조정(PreFT, preference finetuning)**은 강화 학습(RL)을 이용하여 인간의 선호도 데이터에 대해 LLM을 훈련합니다. RLHF의 가장 중요한 목표는 LLM을 인간의 선호도에 부합하도록 정렬하는 것입니다. 즉, 여기에서 설명된 바와 같이 인간이 긍정적으로 평가할 만한 출력을 생성하도록 LLM을 교육하는 것입니다.
*   **RLVR(검증 가능한 보상 기반 강화 학습)** 또는 **강화 미세 조정(RFT, reinforcement finetuning)**은 규칙이나 휴리스틱(heuristics)으로부터 보상을 결정론적으로 도출할 수 있는 검증 가능한 작업에 대해 RL을 통해 LLM을 훈련합니다. 이 최종 훈련 단계는 추론 성능 또는 더 일반적으로는 모든 검증 가능한 작업에서의 성능을 향상시키는 데 기여합니다.

보시다시피, 이러한 각 훈련 단계는 고품질 LLM을 개발하는 과정에서 핵심적인 역할을 수행합니다. 이러한 훈련 기술은 사전 훈련(pretraining)과 후처리 학습(post-training)이라는 두 가지 광범위한 범주로 분류될 수 있습니다. 후처리 학습은 사전 훈련 이후의 모든 과정을 포괄합니다. 사전 훈련은 항상 LLM 훈련의 첫 번째 단계이지만, 후처리 학습 과정은 훈련되는 LLM의 특성에 따라 크게 달라질 수 있습니다. SFT, RLHF, RLVR과 같은 기술들이 일반적으로 사용되지만, 정확한 순서와 설정은 유연하게 변경될 수 있습니다. 다양한 LLM 후처리 학습 파이프라인의 예시는 아래 이미지를 참조하십시오.

**주요 오픈 소스 LLM의 후처리 학습 전략 비교**

**RLHF에 대한 심층 분석.** 모든 LLM 훈련 단계가 중요하지만, 이 글은 특히 기본 LLM을 인간의 선호도에 맞추는 역할을 하는 RLHF 단계에 초점을 맞출 것입니다. RLHF 훈련 과정은 세 가지 주요 단계로 구성됩니다(아래 참조).

1.  LLM에 주입하고자 하는 바람직한 행동을 포착하는 인간 선호도 데이터셋을 수집합니다.
2.  이 선호도 데이터셋에 대해 별도의 보상 모델(RM)을 훈련합니다.
3.  RM의 출력을 보상으로 활용하여 RL 1로 LLM을 미세 조정합니다.

이 과정의 세 번째 단계는 일반적으로 온라인 방식으로 진행됩니다. 즉, 훈련 과정 2에서 우리의 정책(policy)으로부터 완성을 생성하고, 이를 RM이 평가하여 점수를 매기게 됩니다. 온라인 RL 훈련은 효율적으로 설정하고 조율하기 어렵다는 문제점이 있습니다 [10].

**인간 피드백 기반 강화 학습(RLHF)의 단계별 과정**

RLHF의 세 번째 단계를 구동하는 데 활용될 수 있는 PPO, REINFORCE, GRPO 등 다양한 RL 기반 최적화 도구가 존재합니다. 그러나 RLHF를 위한 RL 최적화 도구의 표준 선택(원래 [2]에 의해 대중화됨)은 PPO(Proximal Policy Optimization)입니다. PPO 기반 RLHF는 최고의 LLM 연구소에서 흔히 선택되며, 대규모 LLM 후처리 학습 실행에서 탁월한 결과를 도출하는 경향이 있습니다.

"RLHF는 인상적인 대화 및 코딩 능력을 가진 모델을 생성하지만, RLHF 파이프라인은 지도 학습보다 훨씬 더 복잡하며, 여러 LM을 훈련하고 훈련 루프 내에서 LM 정책에서 샘플링하는 것을 포함하여 상당한 계산 비용을 발생시킵니다." - [1]에서

PPO는 효과적임에도 불구하고 몇 가지 단점을 안고 있습니다. PPO는 온라인 RL 알고리즘일 뿐만 아니라, LLM의 네 가지 다른 복사본(즉, 정책, 참조 정책, 보상 모델 및 가치 함수)을 메모리에 동시에 유지합니다. 이는 PPO로 훈련을 수행하기 위해 막대한 메모리와 다수의 GPU 자원이 필요하다는 것을 의미합니다. 또한, PPO 기반 RLHF는 제대로 튜닝되지 않으면 최적이 아닌 성능을 초래할 수 있는 수많은 구현 세부 사항이 존재합니다.

**RL 훈련 중 발생하는 일.** RLHF의 RL 훈련 단계에서, 우리는 학습된 보상 모델을 사용하여 우리 LLM의 출력에 할당되는 보상을 극대화하고자 합니다. 동시에, 훈련 과정 중에 원래 모델에서 너무 멀리 "표류(drifting)"하는 것을 피하고자 합니다. 이 최적화 과정은 일반적으로 아래에 제시된 목표를 통해 공식화됩니다.

**표준 RLHF 목표 함수**

이 방정식에서 우리는 학습된 정책과 초기 SFT 모델(또는 다른 참조 모델) 간의 KL 발산(KL divergence)에 대한 가산 페널티(additive penalty) 하에서 우리 LLM의 완성에 의해 수신되는 예상 보상을 최대화합니다. KL 발산은 손실 함수에 페널티 항으로 포함됩니다. 보상과 KL 발산 간의 균형은 하이퍼파라미터(hyperparameter) β에 의해 제어됩니다.

**RLHF가 왜 그렇게 어려운가?** RL 기반 선호도 튜닝은 여러 가지 이유로 다루기 복잡합니다. 예를 들어, 다수의 LLM이 연관되고, 훈련 중에 이 모델들로부터 생성을 샘플링해야 하며, 하이퍼파라미터 튜닝이 필수적이고, 계산 및 메모리 비용이 높습니다. 실제로는 이러한 복잡성으로 인해 RLHF 훈련 과정이 불안정하고, 예측 불가능하며, 비용이 많이 들고, 일반적으로 구현하기 어렵습니다. 이러한 문제들은 LLM 후처리 학습 연구에 대한 진입 장벽을 상당히 높입니다.

높은 수준에서 PPO 기반 RLHF가 그렇게 복잡하고, 비용이 많이 들며, 제대로 구현하기 어려운 두 가지 주요 원인은 다음과 같습니다.

*   명시적 보상 모델(explicit reward model)의 사용.
*   강화 학습(RL)을 통한 LLM 훈련.

보상 모델은 별도로 훈련하고 훈련 중에 메모리에 상주시켜야 하는 추가적인 LLM입니다. 또한, 훈련을 위해 PPO를 사용하면 모델의 또 다른 복사본인 가치 함수(value function)를 메모리에 저장해야 하며, 이는 RL 기반 선호도 튜닝의 모든 추가적인 어려움을 야기합니다. 따라서 별도의 보상 모델과 RL 사용을 단순히 회피할 수 있다면, PPO 기반 RLHF와 관련된 많은 일반적인 난관도 피할 수 있을 것입니다!

#### DPO는 이 흐름에 어떻게 통합되는가?

위에서 살펴보았듯이, DPO는 RLHF의 대안 역할을 하는 정렬 알고리즘입니다. 그러나 RLHF와 달리 DPO는 별도의 보상 모델이나 어떤 형태의 RL 훈련도 사용하지 않고, 간접적인 방식으로 RLHF 목표를 해결하기 위해 경사 상승법(gradient ascent)을 통해 정책을 최적화합니다.

"우리는 명시적인 보상 모델링이나 강화 학습 과정 없이 언어 모델을 인간의 선호도에 맞게 직접 최적화하는 방법을 제시합니다. 기존 RLHF 알고리즘과 동일한 목표를 암묵적으로 최적화하면서도 구현이 훨씬 간단하고 훈련하기 쉬운 DPO 알고리즘을 제안합니다." - [1]에서

DPO는 보상의 새로운 재매개변수화(reparameterization) 개념을 도입하여 RLHF 목표를 다룹니다. 이는 별도의 보상 모델이 아닌 정책 자체에서 직접 보상 값을 도출하는 방식으로, 이를 "암묵적(implicit)" 보상이라고 칭합니다. DPO로 LLM을 훈련할 때, 우리는 기존 보상 모델을 훈련하는 방식과 유사하게 오프라인 선호도 데이터셋을 활용하여 이 암묵적 보상을 학습합니다. DPO의 핵심 통찰은 이 암묵적 보상으로부터 RLHF에 대한 최적 정책(optimal policy)을 직접 추출할 수 있다는 점입니다. 근본적으로 DPO는 Bradley-Terry 모델에 기반한 암묵적 보상 모델을 학습하고, 이 암묵적 보상으로부터 최적 정책을 간접적으로 도출합니다.

DPO가 별도의 명시적 보상 모델 훈련을 요구하지 않기 때문에, 일부 실무자들은 DPO가 보상 모델링을 완전히 "회피"하고 RL이나 보상 모델 없이 RLHF를 통해 정책을 직접 최적화한다고 오해하기도 합니다. 하지만 실제로는 DPO 역시 보상 모델링 접근 방식의 일종입니다. 그 훈련 목표와 과정은 전통적인 보상 모델링과 동일합니다. DPO에서 우리는 실제로 보상 모델을 훈련하고 있는 것입니다. 유일한 차이점은 이 보상 모델이 정책 자체 내에 암묵적으로 존재한다는 것입니다. 이 암묵적 보상을 최적화하도록 정책을 훈련함으로써, DPO는 RLHF 목표를 최적으로 해결하는 정책을 찾을 수 있도록 합니다.

위에서 묘사된 바와 같이, DPO는 외부 보상 모델, 온라인 샘플링, 그리고 복잡한 RL 과정을 회피합니다. 대신, 우리는 기본적인 경사 하강법을 사용하여 LLM을 직접 최적화함으로써 RLHF 목표를 (암묵적으로) 해결합니다. 이러한 단순화는 DPO를 RL 기반 선호도 튜닝에 비해 더욱 안정적이고(하이퍼파라미터 튜닝이 덜 필요함) 경량화하여 후처리 학습 연구의 대중화에 크게 기여했습니다.

### 쿨백-라이블러(Kullback-Leibler, KL) 발산의 이해

LLM 후처리 학습 전반에 걸쳐, KL 발산 제약(KL divergence constraint)을 조건으로 모델을 최적화하는 경우가 빈번합니다. 예를 들어, RLHF 내에서 사용되는 정식 최적화 목표는 아래에 제시된 형태를 가집니다.

**KL 제약이 포함된 표준 RLHF 목표**

보시다시피, 우리는 보상을 최대화하는 동시에 이 보상에서 차감되는 페널티 항(β로 가중된 KL 발산)을 최소화하고자 합니다. 페널티 항의 목적은 훈련 중에 우리의 정책 3이 참조 정책에서 너무 멀리 "표류(drifting)"하는 것을 방지하는 것입니다. 이것이 정확히 무엇을 의미하는지 더 깊이 이해해 봅시다.

KL 발산은 정보 이론(information theory)에서 유래한 개념으로, 어떤 확률 분포가 특정 참조 분포와 얼마나 다른지 4를 측정합니다. 이산 확률 분포(discrete probability distribution)의 경우, KL 발산은 아래에 제시된 형태를 가집니다. 특히, KL 발산은 대칭적이지 않습니다. 즉, 인수의 순서가 결과에 영향을 미칩니다.

**연속 및 이산 확률 분포에 대한 KL 발산 정의**

연속 확률 분포의 경우, KL 발산을 기댓값(expectation)의 형태로 공식화할 수 있습니다. 위를 참조하십시오. 이 개념이 명확하지 않다면, 이 글을 읽어보십시오.

**LLM과의 연관성.** LLM 영역에서 KL 발산은 두 LLM 또는 정책을 비교하는 데 널리 사용됩니다. 일반적으로 우리는 현재 훈련하려는 정책을 참조 정책과 비교합니다. 예를 들어, DPO의 경우, 우리는 SFT 정책(즉, 사전 훈련과 SFT를 모두 거친 LLM)으로 시작한 다음, 이 SFT(참조) 정책과 우리가 훈련하는 정책 간의 KL 발산이 계산되는 표준 RLHF 목표를 최적화합니다. 구체적으로, 이 KL 발산의 형태는 다음과 같습니다.

**두 LLM 간의 KL 발산 계산**

이 형태의 KL 발산은 입력 프롬프트 x가 주어졌을 때 완성 y에 대해 현재 모델과 참조 모델 모두가 예측한 확률의 비율을 살펴봅니다. 완성 y의 확률은 단순히 LLM이 완성 내 각 토큰에 대해 예측한 다음 토큰 확률의 곱입니다. 이러한 완성 확률에 대한 KL 발산을 계산함으로써, 우리는 두 모델이 예측한 토큰 분포 간의 유사성을 정량적으로 파악할 수 있습니다.

**실제 KL 발산 추정.** 우리는 일반적으로 RL 훈련 중에 현재 정책이 예측한 분포와 고정된 참조 정책(예: SFT 모델 5) 간의 KL 발산을 추정하고자 합니다. 직관적으로, RL 훈련 중에 사용되는 보상에 이 제약(아래에 표시됨)을 추가하면 훈련되는 정책이 참조 정책과 너무 달라지지 않도록 보장합니다. 실제로는 우리는 일반적으로 KL 발산을 근사화하는데, 이는 보시다시피 간단합니다. 그러나 이 근사화를 수행하는 방법에는 여러 가지 옵션이 있습니다.

일반적으로 KL 발산을 근사화하는 것은 KL 발산의 기댓값(연속) 형태를 사용합니다. 위에서 설명했듯이, 이 형태의 KL 발산은 단순히 두 분포의 로그 확률을 서로 빼고 이 차이의 기댓값을 취합니다. 토큰 로그 확률이 RL 훈련의 다양한 측면(예: PPO 목표)에서 이미 사용된다는 점을 고려할 때, 이러한 표현식은 우리가 계산하기에 매우 쉽습니다! 구체적으로, 프롬프트 x가 주어졌을 때 현재 정책과 참조 정책 간의 KL 발산을 계산하려고 한다고 가정해 봅시다. 이를 위해 우리는 다음을 수행합니다.

1.  현재 정책(참조 정책이 아님)으로 프롬프트에 대한 완성을 생성합니다.
2.  이 완성의 각 토큰에 대한 로그 확률을 현재 정책과 참조 정책 모두에서 가져옵니다.
3.  토큰 로그 확률을 합산하여 시퀀스 로그 확률을 얻습니다.
4.  현재 정책과 참조 정책 간의 시퀀스 로그 확률 차이를 취합니다.

이 과정의 마지막 단계에서는 KL 발산의 근사치를 계산하는 데 사용할 수 있는 여러 옵션이 있으며, 이 모든 옵션은 아래 코드에 나와 있습니다. 이러한 구현이 실제 환경에서 사용되는 예는 여기를 참조하십시오.

```python
"""
Assume we already have necessary logprobs available.
logprob: completion logprob from the policy
ref_logprob: completion logprob from the reference policy
"""
kl_div = logprob - ref_logprob # difference
kl_div = (logprob - ref_logprob).abs() # absolute
kl_div = 0.5 * (logprob - ref_logprob).square() # mse
kl_div = F.kl_div(ref_logprob, logprob, reduction='batchmean') # per token
```

이 KL 발산 추정치는 여기에서 설명된 RL 미세 조정을 위한 목표의 일부로 우리 시퀀스의 보상에서 차감됩니다.

### DPO(Direct Preference Optimization)의 심층 분석

LLM 훈련의 기초와 이 프레임워크 내에서 DPO의 역할을 확립했으므로, 이제 DPO 자체의 핵심 메커니즘을 학습하는 데 집중할 수 있습니다. DPO는 표준 RLHF의 대안(또는 상호 보완적으로) 역할을 하는 선호도 튜닝 방법입니다. 이 섹션에서는 RLHF에서 사용되는 훈련 목표부터 시작하여 DPO 훈련 과정을 처음부터 상세히 도출할 것입니다. 그런 다음 DPO의 실제 구현에 대해 논의할 것입니다. 여기에는 처음부터 단계별 구현과 DPO를 사용하여 LLM을 훈련하는 구체적인 예제가 포함됩니다.

**요약: DPO란 무엇인가?**

**DPO의 훈련 손실 함수**

우리가 배웠듯이, DPO는 명시적 보상 모델과 강화 학습을 회피하고, 대신 더 간단한 경사 하강법 접근 방식을 통해 RLHF 목표를 간접적으로 해결하는 선호도 튜닝 기법입니다. 단일 선호도 쌍에 대해 위에 제시된 DPO 손실은 다음을 통해 LLM을 훈련합니다.

*   참조 정책에 대한 선택된 완성의 상대적 확률을 증가시킵니다.
*   거부된 완성의 상대적 확률을 감소시킵니다.

이 손실 함수는 MLE를 사용하여 오프라인 선호도 데이터셋에 대해 최적화하기 간단합니다. 따라서 우리는 RL 없이 보상 모델과 유사하게 LLM을 훈련할 수 있습니다. 또한, 이 접근 방식은 경량화되고 구현이 단순함에도 불구하고 RLHF에서 최적화하는 것과 동일한 목표를 해결하는 정책을 여전히 산출합니다!

"모델 응답에 대한 인간 선호도 데이터셋이 주어지면, DPO는 간단한 이진 교차 엔트로피(binary cross entropy) 목표를 사용하여 정책을 최적화할 수 있으며, 이는 선호도 데이터에 맞춰진 암묵적 보상 함수에 대한 최적 정책을 생성합니다." - [1]에서

이 손실을 분석해보면, 보상 모델을 훈련하는 데 사용되는 손실 함수와 매우 유사하다는 것을 알 수 있습니다. 이는 참고를 위해 아래에 복사되어 있습니다. 주요 차이점은 보상 모델의 출력을 우리 정책에서 파생된 암묵적 보상으로 대체한다는 것입니다. 나중에 보겠지만, DPO 목표는 선택된 완성 및 거부된 완성의 로그 확률을 조정하는 것 외에도, LLM의 암묵적 보상 추정치가 잘못된 예제에 자연스럽게 강조를 둡니다.

#### DPO 손실 함수의 이론적 도출 과정

이제 DPO의 핵심 아이디어를 이해했으므로, DPO가 어디에서 비롯되었는지, 그리고 DPO가 표준 RLHF와 동일한 최적화 문제를 해결한다는 것을 어떻게 알 수 있는지 이해해야 합니다. 이를 위해 우리는 이론적인 배경에 의존할 것이며, 이는 이 섹션에 많은 방정식이 포함될 수 있음을 의미합니다. 이론이 해석하기 어려울 수 있지만, 이를 이해하는 것은 DPO가 작동하는 근본적인 이유에 대한 깊이 있는 통찰을 얻는 데 도움이 됩니다. 이론을 소화하기 쉽게 만들기 위해, 우리는 각 단계에 대한 해당 설명과 함께 도출 과정을 단계별로 나누어 설명할 것입니다.

**DPO 손실 함수 도출을 위한 주요 단계**

**증명 개요.** 표준 RLHF 훈련 목표에서 시작하여, 우리는 네 가지 핵심 단계(위에 표시됨)를 따라 DPO에서 사용되는 훈련 손실을 도출할 수 있습니다.

1.  RLHF에서 최적 정책에 대한 표현식 도출.
2.  이 표현식을 재배열하여 암묵적 보상 함수 형성.
3.  암묵적 보상을 Bradley-Terry 선호도 모델에 삽입.
4.  이 암묵적 선호도 모델과 일치하도록 LLM 훈련 — 이것이 우리가 DPO 훈련 과정에서 하는 일입니다.

위 단계들은 RLHF에서 LLM을 훈련하는 데 사용되는 목표로 시작하여 DPO 손실 함수로 귀결됩니다. 이 도출에서 우리는 RLHF 최적화 문제를 재구성하여 DPO 훈련 방법론에 도달합니다. 보시다시피, RLHF와 DPO는 복잡하게 연결되어 있습니다. 그들은 동일한 최적화 문제를 해결하려고 합니다! 아래 도출을 연구함으로써, 우리는 이러한 기술들 간의 관계에 대해 더 깊이 이해하게 됩니다.

**(1단계) RLHF의 최적 해.** DPO 손실을 도출하기 위해, 우리는 해결하려는 초기 RLHF 목표에서 시작해야 합니다. 이는 가독성을 위해 아래에 다시 복사되었습니다. 그러나 이 표기법에서 학습된 보상 모델 RM을 사용하는 대신, 우리는 일반 보상 함수 r(x, y)를 사용합니다. 일반 보상 함수는 우리의 보상 모델을 포함할 수 있지만, 이에 국한되지는 않습니다.

**일반 보상 함수를 포함하는 표준 RLHF 목표**

이 목표에서 시작하여, 우리는 아래 단계를 따라 이 목표에 대한 최적 해의 닫힌 형태 표현식(closed-form expression)을 찾을 수 있습니다. 간단히 말해, 우리는 아래에 표시된 RLHF 목표를 실제로 최대화하는 π 값을 찾고 있습니다!

위 도출의 마지막 두 단계에서 우리는 Z(x) 함수를 도입하는데, 이를 분할 함수(partition function)라고 부를 것입니다. 분할 함수는 아래에 정의되어 있습니다.

**DPO에서 사용되는 분할 함수 정의**

보시다시피, 분할 함수는 참조 정책과 입력 프롬프트 x에만 의존하며, 현재 정책이나 완성에는 의존하지 않습니다. "분할 함수"라는 이름은 확률론(probability theory) 및 통계 역학(statistical mechanics)과 같은 분야에서 차용되었습니다. 여기를 참조하십시오. 가장 간단한 수준에서, 분할 함수는 DPO의 이론적 도출에서 사용되는 정규화 항(normalization term)일 뿐입니다. 우리는 Z(x)를 사용하여 우리가 도출하는 확률 분포(이 경우 RLHF 목표에 대한 최적 정책)가 합이 1이 되어 유효한 분포를 형성하도록 보장합니다.

이제 분할 함수를 이해했으므로, 위 빨간색 상자에 표시된 방정식에서 도출을 계속할 것입니다. 구체적으로, 우리는 이 항의 일부를 추출하여 아래 표현식을 정의할 것입니다. 우리는 이 항을 "최적 정책"이라고 부릅니다. 그 이유는 곧 명확해질 것입니다.

앞서 언급했듯이, 분할 함수는 위 표현식에서 최적 정책에 대한 정규화 항으로 사용됩니다. 우리는 위에 정의된 최적 정책이 유효한 확률 분포라는 것을 우리는 알고 있습니다. 그 이유는 다음과 같습니다.

*   모든 가능한 완성 y에 대해 최적 정책의 값은 ≥ 0입니다.
*   모든 완성 y에 대한 최적 정책의 합은 1과 같습니다.

첫 번째 속성은 명백합니다. 최적 정책의 모든 구성 요소는 음수가 아닙니다 6. 두 번째 속성에 대한 증명은 아래에 제공되며, 여기서 우리는 분할 함수 Z(x)가 최적 정책 분포를 정규화하는 데 어떻게 사용되는지 직접 볼 수 있습니다.

이제 최적 정책을 정의하고(그리고 유효성을 검증하고) 나면, 이 항이 나타났던 원래 표현식으로 돌아가 최적 정책에 대한 표현식을 대입할 수 있습니다. 이는 아래에 표시된 방정식을 산출합니다.

위의 최종 항에서 우리는 이 도출의 핵심을 봅니다. 즉, 표준 RLHF 목표는 최적 정책과의 KL 발산을 최소화하는 정책 π를 찾음으로써 최소화됩니다. 두 확률 분포가 동일할 때 KL 발산이 최소값(0)에 도달하므로 7, 이 최적화의 해는 최적 정책 자체입니다. 따라서 이름이 붙여졌습니다. 따라서 우리는 표준 RLHF 목표에 대한 최적 해를 아래 방정식에 표시된 대로 표현할 수 있습니다.

**표준 RLHF 목표를 최적으로 해결하는 방법**

**(2단계) 암묵적 보상 도출.** 여기에서 우리는 위에 표시된 최적 정책에 대한 표현식을 가져와 재배열하여 보상 함수에 대한 표현식(최적 정책 측면에서)을 아래에 표시된 대로 도출할 수 있습니다.

이제 우리는 보상의 재매개변수화를 도출했습니다. 그러나 이 보상 함수는 어떤 명시적 보상 모델에도 의존하지 않습니다. 오히려 우리는 최적 정책과 참조 정책에서 계산된 확률만을 사용하여 보상을 추정합니다. 우리는 이것을 "암묵적" 보상이라고 부를 것입니다.

"이 변수 변경 접근 방식은 명시적이고 독립적인 보상 모델을 회피합니다... 정책 네트워크는 언어 모델과 (암묵적) 보상 모두를 나타냅니다." - [1]에서

이제 남은 유일한 문제는 우리 암묵적 보상에 있는 Z(x) 항입니다. 분할 함수는 모든 가능한 완성 y에 대한 합을 취하므로, Z(x) 값을 계산하는 것은 실제로는 비용이 많이 듭니다. 더 나아가, 독립적인 보상 모델 훈련 없이 직접 계산할 수 없는 보상 함수 r(x, y)도 Z(x)의 표현식에 나타납니다. 이를 해결하기 위해 우리는 Bradley-Terry 모델을 다시 살펴보고 이를 우리의 암묵적 보상 함수와 결합해야 합니다.

**(3단계) Bradley-Terry 선호도 모델의 활용.** Bradley-Terry 선호도 모델 하에서, 우리는 주어진 완성이 다른 완성보다 선호될 확률을 계산할 수 있습니다. 대부분의 경우, 이 선호도 모델의 입력은 각 완성에 대한 명시적 보상(보상 모델에 의해 예측됨)입니다. DPO의 경우, 우리는 이 명시적 보상을 우리의 암묵적 보상 함수로 대체합니다. 아래를 참조하십시오.

위의 최종 방정식에 표시된 바와 같이, 우리는 이제 우리의 암묵적 보상 함수를 사용하는 Bradley-Terry 선호도 모델에 대한 표현식을 가지며, 여기서 암묵적 보상은 최적 정책과 참조 정책에만 의존합니다. Bradley-Terry 표현식의 쌍별 특성과 Z(x)의 값이 x에만 의존하고(y에는 의존하지 않음) 있다는 사실 때문에, 암묵적 보상 함수의 Z(x) 구성 요소는 선택된 완성에 대한 암묵적 보상에서 거부된 완성에 대한 암묵적 보상을 뺄 때 실제로 상쇄됩니다.

**(4단계) 정책 훈련을 통한 최적화.** 위 표현식은 고정된 최적 정책에 의존합니다. 이 최적 정책은 우리가 해결하려는 RLHF 목표의 해입니다. 여기에서 우리는 이 최적 정책을 복구할 수 있는 훈련 목표를 도출하는 방법을 결정해야 합니다. 이를 위해 DPO는 위 표현식의 최적 정책을 학습된 정책으로 대체합니다. 아래를 참조하십시오.

이 두 표현식을 어떻게 같게 만들 수 있을까요? 우리는 학습된 정책을 훈련해야 합니다! 구체적으로, 우리는 우리의 암묵적 보상 함수를 기반으로 선택된 응답이 거부된 응답보다 선호될 확률을 경험적으로 최대화하도록 학습된 정책을 최적화하는 순위 손실(ranking loss)을 공식화할 수 있습니다. 이를 통해 우리는 우리의 선호도 모델이 정확하고, 따라서 최적 정책의 선호도 모델과 일치하도록 보장합니다.

명시적 보상을 암묵적 보상으로 대체하는 것 외에도, 이 손실 함수는 표준 보상 모델이 사용하는 것과 정확히 동일한 훈련 목표입니다. 아래를 참조하십시오.

**DPO를 위해 최종적으로 도출된 손실 표현식**

우리는 또한 이 손실 함수가 DPO의 훈련 목표와 동일하다는 것을 알 수 있습니다. 우리는 이제 RLHF의 훈련 목표에서 시작하여 DPO 훈련 목표를 완전히 도출했습니다. DPO의 훈련 과정은 우리 정책에 기반한 암묵적 보상 모델을 학습합니다. 이 암묵적 보상 함수를 학습함으로써, 우리는 RLHF의 최적 정책과 일치하는 정책을 얻습니다.

### DPO는 과연 최적 정책을 산출하는가?

"[DPO] 최적화 목표는 [암묵적] 보상 매개변수화를 가진 Bradley-Terry 모델과 동등하며, 우리는 우리의 매개변수 모델을 보상 모델 최적화와 동등하게 최적화합니다... 우리는 [이 목표]가 학습된 보상 모델의 클래스를 제약하지 않으며 최적 정책의 정확한 복구를 허용한다는 것을 보여줍니다." - [1]에서

위 도출에 따르면, DPO 손실을 사용하여 LLM을 훈련하면 최적 정책과 동일한 선호도 분포(암묵적 보상에 의해 유도됨)를 가진 모델이 생성됩니다. 다시 말해, DPO 손실을 통해 우리 정책이 학습한 암묵적 보상 함수는 우리 선호도 데이터셋에서 선택된 완성 및 거부된 완성을 올바르게 순위를 매길 것입니다. 그러나 DPO의 목표는 단순히 우수한 암묵적 보상 함수를 가진 모델을 훈련하는 것이 아닙니다. 우리는 LLM을 정렬하고 고품질 완성을 생성하는 정책을 도출하기를 원합니다! 다행히도, [1]의 저자들은 고품질 암묵적 보상 함수를 학습하는 것 외에도 DPO를 통해 도출된 정책이 RLHF의 최적 정책과 일치해야 함을 보여주는 최종 증명을 제공합니다.

두 보상 함수 r(x, y)와 r’(x, y)는 r(x, y) - r’(x, y) = f(x) (일부 함수 f(•)에 대해)일 때 그리고 그 때에만 동등합니다.

**동등한 보상 함수의 정의.** 증명을 시작하기 위해, 우리는 먼저 보상 함수에 대한 동등 관계(equivalence relation)를 명확히 정의할 수 있습니다. 이는 두 보상 함수가 '같다'는 것이 무엇을 의미하는지 포착하는 정의일 뿐입니다. 위를 참조하십시오. 간단히 말해, 두 보상 함수는 보상 차이가 완성에 의존하지 않고 프롬프트에만 의존할 경우 동등하다고 간주됩니다. 이 정의를 사용하여, 우리는 아래에서 두 개의 동등한 보상 함수가 동일한 선호도 분포 8를 산출함을 보장한다는 것을 보여줍니다.

우리는 또한 두 개의 동등한 보상 함수가 이전 섹션에서 탐구한 표준 RLHF 목표에 대입될 때 동일한 최적 정책을 산출함을 보장한다는 것을 보여주는 유사한 증명을 작성할 수 있습니다. 아래를 참조하십시오.

**최적 정책의 동등성 증명.** 위 결과들을 고려할 때, 이 증명의 마지막 단계는 DPO 내에서 사용되는 암묵적 보상 함수가 RLHF 내에서 사용되는 실제 보상과 동등하다는 것을 단순히 보여주는 것입니다. 이 두 보상 함수가 동등 관계를 만족한다면, 우리는 DPO가 위에 표시된 결과에 따라 RLHF와 동일한 최적 정책을 산출할 것임을 압니다.

이 최종 결과를 증명하기 위해, 우리는 RLHF에서 사용되는 임의의 보상 함수 r(x, y)를 고려하는 것부터 시작할 수 있습니다. 우리의 목표는 DPO의 암묵적 보상이 r(x, y)와 동등하다는 것을 보여주는 것입니다. 임의의 보상이 주어지면, 우리는 위에 표시된 수정된 9 보상 표현식을 정의할 수 있습니다. 이 표현식은 r(x, y)에서 추가 항(즉, 분할 함수의 로그)을 뺄 뿐입니다. 또한 r(x, y)에서 빼는 항이 x에만 의존한다는 점에 유의하십시오. 이러한 이유로, 수정된 보상 표현식은 우리가 이전에 정의한 동등 관계에 따라 r(x, y)와 동등합니다.

"두 번째 보조 정리(lemma)는 동일한 클래스의 모든 보상 함수가 동일한 최적 정책을 산출한다고 명시합니다. 따라서 우리의 최종 목표를 위해, 우리는 최적 클래스에서 임의의 보상 함수를 복구하는 데만 관심이 있습니다." - [1]에서

원하는 결과를 증명하기 위해, 우리는 최적 RLHF 해를 재배열하여 암묵적 보상을 생성하는 이전 표현식을 활용해야 합니다. 이 암묵적 보상을 위의 수정된 보상 표현식에 대입하면, 우리는 r(x, y)와 동등하다고 알려진 보상(DPO의 암묵적 보상과 일치함)을 얻습니다. 아래를 참조하십시오. 결과적으로, 우리는 이제 DPO가 사용하는 암묵적 보상이 r(x,y)와의 동등 관계를 만족한다는 것을 알게 되었고, 이는 증명을 완료합니다.

**핵심 요점 요약.** 이 섹션을 마무리하기 전에, 우리는 방금 증명한 결과를 빠르게 맥락화해야 합니다. 이전 섹션에서 우리는 표준 RLHF 목표에 대한 최적 정책(또는 해)의 암묵적 보상에 의해 유도되는 선호도 분포에 대한 표현식을 도출했습니다. 이 표현식이 도출된 후, 우리는 일반 보상 모델과 동일한 훈련 전략을 채택함으로써 이 선호도 분포와 일치하는 암묵적 보상 함수를 갖도록 모델을 쉽게 훈련할 수 있습니다. 따라서 DPO의 핵심 훈련 절차는 (암묵적) 보상 모델 훈련을 중심으로 이루어지며, 이것이 논문의 제목이 된 이유입니다. 아래를 참조하십시오.

DPO에 대한 흔한 오해는 DPO가 보상 모델을 제거한다는 것인데, 이는 사실이 아닙니다. 사실, DPO는 보상 모델링에 전적으로 기반합니다. 보상 모델이 단지 암묵적이라는 것은 우리가 명시적 보상 모델 훈련을 피할 수 있다는 것을 의미합니다.

"자주 오해되는 점은 DPO가 핵심적으로 보상 모델을 학습한다는 것입니다. 그래서 논문의 부제가 '당신의 언어 모델은 비밀리에 보상 모델이다(Your Language Model is Secretly a Reward Model)'입니다. 이것을 DPO 목표가 정책을 직접 훈련하는 것으로 혼동하기 쉽습니다." - RLHF 책

DPO의 훈련 절차가 보상 모델링에 기반한다는 점을 고려할 때, 이러한 방식으로 LLM을 훈련하는 것이 실제로 최적 정책을 산출할 것이라는 점은 즉시 명확하지 않습니다. 결과 모델이 정확한 암묵적 보상 함수를 가질 수 있지만 여전히 고품질 완성을 생성하지 못할 수도 있을까요? 이 섹션에서 우리는 이것이 사실이 아니어야 함을 증명합니다. 최적 정책의 암묵적 선호도 분포와 일치하도록 모델을 훈련하면, 결과 정책도 최적임이 보장됩니다! 간단히 말해, DPO는 RLHF를 통한 훈련으로 도출된 정책과 품질 면에서 비교할 수 있는 정책을 간접적으로 제공하며, PPO 기반 RLHF와 같은 기술보다 훨씬 덜 복잡한 유효한 선호도 튜닝 대안이 됩니다.

### DPO는 왜 효과적인가? (작동 원리)

**DPO 손실 함수의 기울기(Gradient) 분석**

DPO와 DPO가 잘 작동하는 이유에 대한 더 깊은 이해를 얻기 위해, 우리는 DPO 손실 함수의 기울기 구조를 살펴볼 수 있습니다. 위 10를 참조하십시오. 이 표현식에는 명확성을 위해 빨간색(일부 항은 주황색), 파란색, 녹색으로 표시된 세 가지 핵심 항이 있습니다. 각 항의 목적은 다음과 같습니다.

*   첫 번째 (빨간색) 항은 시그모이드 함수(sigmoid function)로 인해 [0, 1] 범위에 속하는 가중치이며, 거부된 완성의 암묵적 보상이 선택된 완성의 암묵적 보상에 비해 증가할수록 커집니다. 다시 말해, 이 항은 잘못된 암묵적 보상 추정치에 더 높은 가중치를 할당하여 모델이 잘못된 예측에 더 크게 반응하도록 유도합니다.
*   두 번째 (파란색) 항은 LLM의 매개변수에 대한 선택된 완성의 우도(likelihood)의 양의 기울기이며, 이는 선택된 완성의 우도를 증가시키는 역할을 수행합니다.
*   세 번째 (녹색) 항은 LLM의 매개변수에 대한 거부된 완성의 우도의 음의 기울기이며, 이는 거부된 완성의 우도를 감소시키는 역할을 수행합니다.

이 항들은 i) 선택된 완성의 우도를 증가시키고 ii) 거부된 완성의 우도를 감소시키는 것을 동시에 수행하며, LLM의 암묵적 보상 추정치가 잘못된 경우에 추가적인 강조(즉, LLM 매개변수에 대한 더 큰 업데이트)가 주어집니다.

"예제는 암묵적 보상 모델이 선호되지 않는 완성을 베타로 스케일링하여 얼마나 더 높게 평가하는지에 따라 가중치가 부여됩니다... 암묵적 보상 모델이 완성을 얼마나 잘못 순서화하는지, KL 제약의 강도를 고려합니다." - [1]에서

**가중 계수(Weighting coefficient)의 중요성.** [1]의 저자들은 DPO 손실 기울기의 세 가지 하위 구성 요소 모두가 알고리즘이 잘 작동하는 데 필수적이라고 강조합니다. 특히, 이 기울기에서 첫 번째 가중치 항을 제거하면(모든 선택된 완성의 우도를 균일하게 증가시키고 모든 거부된 완성의 우도를 감소시키는 기울기를 생성함), 결과 정책은 품질이 낮고 텍스트를 생성할 때 완전히 퇴화하는 경향이 있습니다. 아래를 참조하십시오. 이러한 훈련 알고리즘은 비우도 훈련(unlikelihood training)이라고 불리며 과거 [5]에서 탐구되었습니다. DPO에 의해 손실 기울기에 추가된 간단한 가중치 항은 이 접근 방식을 완전히 변화시켜 고품질 LLM 정렬을 수행할 수 있게 합니다.

**비우도 훈련으로 학습된 LLM은 종종 퇴화하는 경향을 보입니다.**

### DPO 구현의 실제와 팁

DPO의 이론적 도출은 복잡해 보일 수 있지만, 이 기술은 실제로는 매우 간단하게 사용할 수 있습니다. 사실, DPO는 최고의 연구소 외부에 있는 사람들을 위해 LLM 후처리 학습 연구를 대중화하는 데 지대한 영향을 미 미쳤습니다 [3]. PPO 기반 RLHF와 같은 알고리즘은 튜닝하기 어렵고 상당한 계산 자원을 필요로 합니다. 대조적으로, DPO는 강화 학습 없이 표준 분류(또는 순위) 손실을 사용하며, 훈련 과정 내내 모델의 네 개 대신 두 개의 복사본만 메모리에 유지합니다.

**표준 DPO 훈련 파이프라인의 구성**

**DPO 훈련 파이프라인.** DPO를 사용한 표준 훈련 과정은 위에 묘사되어 있습니다. 우리는 모델을 훈련하는 사용 사례를 포착하는 다양한 프롬프트 세트로 과정을 시작합니다. 여기에서 우리는 참조 정책을 사용하여 각 프롬프트에 대한 완성 쌍을 생성하고, 인간 평가자가 각 쌍에 대한 선호도 주석을 제공하도록 합니다. 이 선호도 데이터셋을 사용할 수 있게 되면, 우리는 선호도 데이터셋에 대해 이전에 도출한 DPO 손실을 최소화하도록 모델을 훈련함으로써 최대 우도 추정을 수행합니다.

**PyTorch 환경에서의 DPO 손실 계산 예시**

**손실 구현.** PyTorch에서 DPO의 손실 함수를 구현하는 것은 매우 직관적입니다. 이는 현재 정책과 참조 정책에서 파생된 암묵적 보상에 적용되는 순위 손실일 뿐입니다. [1]에서 가져온 손실의 예시 구현은 참고를 위해 아래에 복사되어 있으며, 여기서 손실이 다음을 통해 계산됨을 알 수 있습니다.

*   현재 정책과 참조 정책에 의해 각 완성(선택된 것과 거부된 것 모두)에 할당된 로그 확률을 얻습니다.
*   현재 정책과 참조 정책 모두에 대해 선택된 완성 및 거부된 완성 간의 확률 비율을 계산합니다.
*   위의 확률 비율을 사용하여 최종 DPO 손실을 구성합니다.

**오프라인 선호도 데이터 처리의 고려 사항.** DPO는 근본적으로 오프라인 선호도 학습 알고리즘입니다. 우리는 정적 선호도 데이터셋에 대해 모델을 최적화합니다. 위에 설명된 파이프라인에서 우리는 참조 모델을 사용하여 선호도 데이터셋의 완성을 생성합니다. 그러나 대부분의 실제 응용 프로그램에서는 그렇지 않을 수 있습니다. 실무자로서 우리는 UltraFeedback [4]와 같은 선호도 데이터셋을 온라인에서 다운로드하고 DPO를 사용하여 이 정적 데이터셋에 대해 모델을 훈련할 수 있습니다. 이러한 경우, 실제 참조 모델은 알려져 있지 않으며 DPO 훈련에서 사용한 참조 모델과 다를 수 있어 분포 변화(distribution shift)를 초래할 수 있습니다.

"선호도 데이터셋은 SFT 모델을 사용하여 샘플링되므로, 가능한 경우 참조 정책을 SFT 모델로 초기화합니다. 그러나 SFT 모델을 사용할 수 없는 경우, 선호되는 완성의 우도를 최대화하여 참조 정책을 초기화합니다. 이 절차는 실제 참조 분포와 DPO가 사용하는 참조 정책 간의 분포 변화를 완화하는 데 도움이 됩니다." - [1]에서

이 분포 변화를 최소화하고 실제 참조 모델이 우리 선호도 데이터셋에 있는 완성들과 잘 정렬되도록 보장하기 위해, [1]의 저자들은 아래에 묘사된 절차를 권장합니다. 이 절차에서 우리는 먼저 선호도 데이터셋의 선택된 완성에 대해 참조 모델의 지도 미세 조정을 수행한 다음, 이 모델을 DPO로 추가 훈련합니다. 이 예비 SFT 훈련 단계는 DPO의 참조 정책이 선호도 데이터셋을 생성하는 데 사용된 실제 참조 정책과 너무 다르지 않도록 보장합니다.

**DPO에서 오프라인 선호도 데이터로 인한 분포 변화를 완화하는 전략**

DPO 구현을 위한 마지막 고려 사항은 β 하이퍼파라미터를 올바르게 설정하는 것입니다. 이는 훈련된 정책이 참조 정책과 얼마나 다를 수 있는지를 제어합니다. 기억하십시오. β는 RLHF 목표에서 KL 제약에 곱하는 가중치이며, DPO에서 선호도 정렬의 강도를 제어합니다. β 값이 낮을수록 모델이 데이터에서 관찰된 선호도에 적응하기 위해 더 공격적으로 업데이트됩니다. 일반적으로 β는 [0, 1] 범위의 값으로 설정되며, 낮은 값이 더 일반적입니다. 예를 들어, β = 0.1이 인기 있는 선택이지만, [1]의 저자들은 β = 0.1과 β = 0.5 모두를 탐구합니다.

**전체 DPO 예제.** DPO로 자신만의 LLM을 미세 조정하는 가장 쉬운 방법 중 하나는 HuggingFace TRL 패키지의 `DPOTrainer`를 사용하는 것입니다. DPO 훈련을 수행하려면 i) UltraFeedback와 같은 선호도 데이터셋을 로드하고, ii) 모델/토크나이저를 선택하고(예: 큰 GPU가 없다면 Qwen3-0.6B와 같은 작은 모델이 좋습니다), iii) 아래 코드에 표시된 대로 DPO 트레이너를 실행하기만 하면 됩니다.

```python
from trl import DPOConfig, DPOTrainer

# load model and data
model = <load our model>
tokenizer = <load our tokenizer>
train_dataset = <load our preference dataset>

# configure DPO training process
training_args = DPOConfig(output_dir="./dpo_logs/")
trainer = DPOTrainer(
    model=model,
    args=training_args,
    processing_class=tokenizer,
    train_dataset=train_dataset,
)

# execute DPO training
# run the below command to execute this script
# > accelerate launch <script name>
trainer.train()
```

### 요약 및 주요 시사점: DPO의 현재와 미래

DPO(Direct Preference Optimization)는 명시적 보상 모델과 강화 학습(RL)을 회피하면서 RLHF 목표를 간접적으로 해결하는 LLM용 선호도 튜닝 방법입니다. DPO에서 우리는 RLHF 목표를 재매개변수화하여 정책 자체(및 참조 정책)에서 파생된 암묵적 보상 함수를 형성합니다. 그런 다음, 우리는 정적 선호도 데이터셋에 대해 LLM을 훈련하여 이 암묵적 보상 함수를 최적화하며, 이는 표준 보상 모델과 유사합니다. 이 암묵적 보상 모델링 목표를 해결함으로써, 우리는 RLHF 목표를 해결하는 정책을 간접적으로 산출합니다. 이 접근 방식은 RL 기반 정렬 방법에 대한 더 간단하고, 더 안정적이며, 계산 효율적인 대안을 제공하여 고품질 LLM 정렬에 대한 접근성을 높였습니다. 그러나 여러 연구에서 DPO와 같은 (오프라인) 직접 정렬 알고리즘과 온라인 RL을 사용하는 정렬 기술(예: PPO 기반 RLHF) 간의 성능 격차를 연구했으며, 여전히 온라인 방식이 특정 시나리오에서 더 나은 성능을 보일 수 있음을 발견했습니다 [11, 12]. 이러한 사실에도 불구하고, DPO는 그 단순성과 효과성 때문에 LLM 후처리 학습에서 여전히 활발히 사용됩니다(종종 온라인 알고리즘과 함께).

DPO는 LLM 정렬 연구의 지평을 넓혔지만, 여전히 몇 가지 한계점을 가지고 있습니다. 첫째, DPO는 오프라인 선호도 데이터에 의존하므로, 데이터셋의 품질과 다양성이 모델 성능에 결정적인 영향을 미칩니다. 편향되거나 불충분한 데이터는 모델의 정렬 품질을 저하시킬 수 있습니다. 둘째, DPO는 기존 정책을 미세 조정하는 데 매우 효과적이지만, 완전히 새로운 행동이나 복잡한 추론 능력을 학습시키는 데는 온라인 RL 방식이 여전히 유리할 수 있다는 지적도 있습니다. 마지막으로, β 하이퍼파라미터의 적절한 튜닝은 여전히 중요하며, 이는 모델의 참조 정책으로부터의 이탈 정도를 제어하는 핵심 요소입니다.

그럼에도 불구하고, DPO의 등장은 LLM 정렬의 대중화에 크게 기여했으며, 특히 자원 제약이 있는 환경에서 고성능 LLM을 개발하는 데 필수적인 도구가 되었습니다. 앞으로는 DPO와 같은 직접 정렬 방법론과 온라인 RL의 장점을 결합한 하이브리드 접근 방식이 더욱 발전할 것으로 예상됩니다. 이는 LLM이 인간의 복잡한 선호도를 더욱 정확하고 효율적으로 학습할 수 있도록 돕는 방향으로 나아갈 것입니다.

뉴스레터가 처음이신가요? 안녕하세요! 저는 Cameron R. Wolfe입니다. 넷플릭스(Netflix)의 딥러닝(Deep Learning) 박사이자 선임 연구 과학자입니다. 이것은 독자들이 AI 연구의 중요한 주제를 더 잘 이해하도록 돕는 Deep (Learning) Focus 뉴스레터입니다. 이 뉴스레터는 항상 무료로 공개될 것입니다. 뉴스레터가 마음에 드신다면 구독하거나, 유료 구독을 고려하거나, 공유하거나, X와 LinkedIn에서 저를 팔로우해주세요! 구독하기

### 참고 문헌

[1] Rafailov, Rafael, et al. "Direct preference optimization: Your language model is secretly a reward model." Advances in neural information processing systems 36 (2023): 53728-53741.
[2] Stiennon, Nisan, et al. "Learning to summarize with human feedback." Advances in neural information processing systems 33 (2020): 3008-3021.
[3] Tunstall, Lewis, et al. "Zephyr: Direct distillation of lm alignment." arXiv preprint arXiv:2310.16944 (2023).
[4] Cui, Ganqu, et al. "Ultrafeedback: Boosting language models with scaled ai feedback, 2024." URL https://arxiv. org/abs/2310.01377 .
[5] Welleck, Sean, et al. "Neural text generation with unlikelihood training." arXiv preprint arXiv:1908.04319 (2019).
[6] Lambert, Nathan, et al. "Tulu 3: Pushing frontiers in open language model post-training, 2024." URL https://arxiv. org/abs/2411.15124 297 (2025).
[7] Yang, An, et al. "Qwen3 technical report." arXiv preprint arXiv:2505.09388 (2025).
[8] Dubey, Abhimanyu, et al. "The llama 3 herd of models." arXiv e-prints (2024): arXiv-2407.
[9] Kaplan, Jared, et al. "Scaling laws for neural language models." arXiv preprint arXiv:2001.08361 (2020).
[10] Sheng, Guangming, et al. "Hybridflow: A flexible and efficient rlhf framework." Proceedings of the Twentieth European Conference on Computer Systems . 2025.
[11] Tang, Yunhao, et al. "Understanding the performance gap between online and offline alignment algorithms." arXiv preprint arXiv:2405.08448 (2024).
[12] Ivison, Hamish, et al. "Unpacking dpo and ppo: Disentangling best practices for learning from preference feedback." Advances in neural information processing systems 37 (2024): 36602-36633.

1 LLM 맥락에서 강화 학습에 대한 심층적인 설명은 여기를 참조하십시오.
2 더 구체적으로, "온라인(online)"은 정책이 각 단계에서 생성된 새로운 샘플로 반복적으로 업데이트되는 것을 의미하며, "오프라인(offline)"은 모든 훈련 데이터가 미리 고정되어 있음을 의미합니다.
3 "정책(policy)"이라는 단어는 우리가 (RL로) 훈련하는 LLM 또는 모델에 대한 RL 전문 용어입니다.
4 더 구체적으로, KL 발산은 주어진 분포가 참조 분포를 근사화하는 데 사용될 때 얼마나 많은 정보가 손실되는지를 측정합니다.
5 참조 모델이 항상 SFT 모델인 것은 아닙니다. RL 훈련의 이전 모델 체크포인트(checkpoint)일 수도 있습니다. 예를 들어, RLHF의 네 단계 또는 라운드가 순차적으로 수행되는 경우, RLHF의 두 번째 단계에 대한 참조 모델은 RLHF의 첫 번째 단계에서 얻은 모델일 수 있습니다.
6 최적 정책은 분할 함수, 참조 정책, 지수 함수의 곱이며, 이들 모두는 0보다 작을 수 없습니다. 따라서 최적 정책을 구성하는 이 항들의 곱도 음수가 아니어야 합니다.
7 이는 깁스 부등식(Gibbs’ inequality)으로 알려져 있습니다.
8 [1]에서는 이 증명이 더 일반적인 Plackett-Luce 모델(부록 A.5, 17페이지 참조)을 가정하여 제공되지만, 우리는 단순화를 위해 그리고 이 개요의 나머지 설명과 일치시키기 위해 Bradley-Terry 모델을 사용하여 이 증명을 다시 작성합니다.
9 [1]에서 저자들은 이 수정된 함수를 보상 함수의 "투영(projection)"이라고 설명합니다.
10 기억하십시오. DPO는 MLE를 사용하여 LLM을 훈련합니다. 다시 말해, 우리 LLM의 매개변수는 i) 데이터 배치에 대해 이 기울기를 계산하고, ii) 기울기에 스칼라 계수(즉, 학습률)를 곱하고, iii) 이 스케일링된 기울기를 모델 매개변수에서 빼는 과정을 반복하여 직접 업데이트됩니다. 이 기울기가 어떻게 도출되는지 이해하고 싶다면, 이 논문의 17페이지를 참조하십시오.