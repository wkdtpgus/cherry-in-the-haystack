거대 언어 모델(LLM)의 정렬 과정은 모델이 사람들의 기호에 맞는 결과물을 내놓도록 하는 필수적인 사후 훈련(post-training) 과정입니다. 인간의 의견을 활용한 강화 학습(RLHF)과 같은 정렬 기법들은 LLM의 성능을 비약적으로 끌어올렸지만, 그 구조가 복잡하고 막대한 연산 자원을 소모하며, 최적화 과정이 까다롭다는 단점이 있습니다. 본 문서에서는 이러한 복잡성을 해소하기 위해 경사 강하(gradient descent) 방식으로 최적화 가능한 보다 단순한 목표를 활용하여 LLM을 정렬하는, 즉 DPO(Direct Preference Optimization)라고 불리는 보다 직관적인 LLM 정렬 방안을 탐구할 예정입니다. DPO의 효율성과 실용성은 정렬 연구에 대한 접근성을 높였고, 여러 유명 LLM에서 널리 활용되는 표준 사후 훈련 알고리즘으로 자리매김하는 데 기여했습니다. 이러한 정렬 기술의 발전은 LLM이 단순히 사실을 전달하는 것을 넘어, 사용자의 의도와 가치관에 부합하는 안전하고 유용한 상호작용을 제공하는 데 필수적입니다. 이는 상업적 활용 가능성과 윤리적 책임 측면에서도 중요한 진전을 의미합니다.

"직접 정렬 알고리즘은 중간 보상 모델(reward model)을 훈련하거나 강화 학습 최적화 도구를 사용하지 않고도 동일한 RLHF 목표를 해결하도록 모델을 업데이트할 수 있게 합니다. 가장 두드러진 직접 정렬 알고리즘이자 언어 모델 정렬의 학문적 움직임 전체를 촉발시킨 것은 DPO(Direct Preference Optimization)입니다." - RLHF 책

AI 연구를 이해하기 위해 Deep (Learning) Focus를 사용하는 50,000명의 독자와 함께하세요. 구독하기

### DPO의 핵심 구성 요소

DPO를 완벽하게 파악하기 위해서는 먼저 대규모 언어 모델(LLM)이 어떤 방식으로 학습되는지에 대한 근본적인 이해가 선행되어야 합니다. 특히, DPO는 LLM 사후 훈련 과정에서 활용되는 선호도 조정(preference tuning) 알고리즘입니다. 이 방법론은 인간의 기호가 반영된 데이터셋을 통해 LLM을 정교하게 조정하며, PPO(Proximal Policy Optimization) 기반 RLHF와 같은 강화 학습(RL) 기반의 선호도 조정 기법에 대한 대안으로 제시됩니다. 이 부분에서는 DPO와 LLM 학습에서의 DPO 역할에 대한 배경 지식을 제공하기 위해 이러한 개념들을 논의할 것입니다.

#### 선호도 데이터 및 보상 모델 (참조: [2])

인간의 기호는 LLM 사후 훈련 과정에서 매우 중요한 요소입니다. 선호도 관련 자료는 대개 위에서 제시된 구조를 따르는데, 여기에는 하나의 질문(prompt), 해당 질문에 대한 두 개의 결과물(또는 응답), 그리고 이 결과물들에 부여된 선호도(사람 평가자 또는 LLM 심사자가 부여) 정보가 담겨 있습니다. 이러한 선호도는 단순히 두 응답 중 어느 하나가 다른 것보다 더 우수하다는 판단을 나타냅니다. 고품질의 선호도 데이터를 수집하는 것은 매우 중요하며, 이는 단순히 답을 고르는 것을 넘어 인간의 복잡한 가치 판단, 맥락 이해, 그리고 미묘한 차이를 포착해야 하므로 상당한 어려움이 따릅니다.

**선호도 데이터셋의 기본 구조**

이 개념은 위 수식을 통해 공식화되며, 이는 '선택된(chosen)' 응답과 '거부된(rejected)' 응답이 연결된 프롬프트의 선호도 데이터셋을 정의합니다. Bradley-Terry 선호도 모델(Bradley-Terry Model of Preference)은 LLM 분야에서 선호도를 모형화하는 데 가장 널리 사용되는 통계적 모델입니다. 이 모델은 선택된 결과물과 거부된 결과물이라는 두 가지 항목과 각 항목에 대한 연관된 보상 값을 입력으로 받습니다. 이 정보를 활용하여, 아래에 제시된 바와 같이 한 항목이 다른 항목보다 선호될 확률을 수식으로 표현할 수 있습니다. 여기서 우리는 비교 대상 항목들이 선호도 쌍(preference pair)으로 구성되어 있다고 가정합니다.

**Bradley-Terry 모델을 활용한 쌍별 확률**

우리는 Bradley-Terry 모델을 사용하여 두 결과물 간의 쌍별 비교에 대한 확률을 표현합니다. 그러나 Bradley-Terry가 선호도를 모형화하는 데 사용할 수 있는 유일한 접근 방식은 아닙니다. 예를 들어, Plackett-Luce 모델(Plackett-Luce model)도 또 다른 대안이 될 수 있습니다.

**보상 모델(Reward Models).** 위 수식의 보상은 일반적으로 보상 모델(RM)에 의해 예측됩니다. RM은 질문-결과물 쌍을 입력으로 받아 (스칼라) 선호도 점수를 출력하는 특수 LLM입니다(아래에 제시된 표준 디코더 전용 트랜스포머(decoder-only transformer)에 추가 선형 분류 헤드(linear classification head)를 추가하여 구현됨).

**보상 모델(RM)의 아키텍처**

고정된 선호도 데이터셋이 주어지면, Bradley-Terry에 의해 모형화된 관찰된 인간 선호도를 반영하는 점수를 생성하도록 RM을 훈련할 수 있습니다. 다시 말해, 우리는 선호도 데이터셋 전반에 걸쳐 우리의 RM이 선택된 응답이 거부된 응답보다 선호될 확률(위의 쌍별 확률 표현식으로 주어짐)을 최대화하기를 원합니다. 이를 위해 우리는 아래에 제시된 음의 로그 우도 손실(negative log-likelihood loss)을 MLE(최대 우도 추정)를 사용하여 최소화할 수 있습니다. 이는 이 목표를 손실 함수로 사용하여 많은 데이터 예제에 대해 RM을 훈련한다는 의미입니다. RM에 대한 자세한 내용은 아래 링크된 개요를 참조하십시오.

**보상 모델**
Cameron R. Wolfe, Ph.D. · 6월 30일
전체 기사 읽기

#### LLM 학습 및 정렬 (참조: [2, 6, 9])

이 개요가 DPO에 중점을 둘 것이므로, DPO가 LLM의 전체 학습 과정에서 어떤 위치에 있는지 이해해야 합니다. (대략) 네 부분으로 구성된 이 학습 과정은 위 그림에 묘사되어 있습니다. 각 단계와 그에 상응하는 목적을 다음과 같이 구분할 수 있습니다. 이는 마치 아이를 키우는 과정에 비유될 수 있습니다:

*   **사전 훈련(Pretraining)**은 다음 토큰 예측(next token prediction) 학습 목표를 사용하여 인터넷 규모의 텍스트 데이터에 대해 LLM을 처음부터 훈련하는 대규모 학습 절차입니다. 사전 훈련의 주요 목적은 LLM 내에 광범위하고 고품질의 지식 기반을 구축하는 것입니다. 이는 아이가 세상의 기본 지식을 습득하는 과정과 유사합니다. 여기를 참조하십시오.
*   **지도 미세 조정(SFT, Supervised finetuning)** 또는 **명령 미세 조정(IFT, instruction finetuning)**은 또한 (지도) 다음 토큰 예측 학습 목표를 사용하여 LLM이 모방하도록 학습하는 더 작은 고품질 결과물 세트에 대해 LLM을 훈련합니다. SFT의 주요 목적은 LLM에게 기본적인 형식 지정 및 명령 따르기 기능을 가르치는 것입니다. 이는 아이가 학교에서 특정 규칙과 지시를 배우는 것과 같습니다. SFT만으로는 모델이 인간의 복잡한 가치와 선호도를 완벽하게 이해하고 반영하기 어렵습니다.
*   **RLHF(인간 피드백 기반 강화 학습)** 또는 **선호도 미세 조정(PreFT, preference finetuning)**은 강화 학습(RL)을 사용하여 인간 선호도 데이터에 대해 LLM을 훈련합니다. RLHF의 핵심 목적은 LLM을 인간의 선호도에 맞추는 것입니다. 즉, 여기에서 설명한 대로 인간이 긍정적으로 평가하는 출력을 생성하도록 LLM을 가르치는 것입니다. 이는 아이가 사회화 과정을 통해 예절과 타인의 기분을 헤아리는 법을 배우는 것에 비유할 수 있습니다.
*   **RLVR(검증 가능한 보상 기반 강화 학습)** 또는 **강화 미세 조정(RFT, reinforcement finetuning)**은 규칙이나 경험적 방법(heuristics)으로부터 보상을 결정론적으로 도출할 수 있는 검증 가능한 작업에 대해 RL로 LLM을 훈련합니다. 이 최종 학습 단계는 추론 성능 또는 더 일반적으로는 모든 검증 가능한 작업에서의 성능을 향상시키는 데 유용합니다.

보시다시피, 이러한 각 학습 단계는 고품질 LLM을 생성하는 과정에서 핵심적인 목적을 수행합니다. 이러한 학습 기술은 사전 훈련(pretraining)과 사후 훈련(post-training)이라는 광범위한 범주로 묶을 수 있습니다. 즉, 사전 훈련 이후의 모든 것을 포함합니다. 사전 훈련은 항상 LLM 학습의 첫 번째 단계이지만, 사후 훈련 과정은 훈련되는 LLM에 따라 크게 달라질 수 있습니다. 동일한 기술, 즉 SFT, RLHF 및 RLVR이 일반적으로 사용되지만, 정확한 순서와 설정은 변경될 수 있습니다. 각각 약간 다른 설정을 채택하는 LLM 사후 훈련 파이프라인의 몇 가지 예는 아래 이미지를 참조하십시오.

**인기 있는 오픈 LLM의 사후 훈련**

**RLHF에 대해 더 자세히.** 모든 LLM 학습 단계가 중요하지만, 이 개요는 특히 기본 LLM을 인간의 선호도에 맞추는 역할을 하는 RLHF 단계에 초점을 맞출 것입니다. RLHF 학습 과정은 세 가지 주요 단계로 구성됩니다(아래 참조).

1.  LLM에 주입하고자 하는 선호되는 행동을 포착하는 인간 선호도 데이터셋을 수집합니다.
2.  이 선호도 데이터셋에 대해 별도의 보상 모델(RM)을 훈련합니다.
3.  RM의 출력을 보상으로 사용하여 RL 1로 LLM을 미세 조정합니다.

이 과정의 세 번째 단계는 일반적으로 온라인 방식으로 발생합니다. 즉, 학습 과정 2 동안 우리의 정책(policy)에서 결과물을 생성하여 RM에 의해 점수를 매기게 됩니다. 온라인 RL 학습은 효율적으로 설정하고 조율하기 어렵습니다 [10]. 또한, 이 과정에서 모델이 과도하게 특정 데이터에 적응하여 기존의 유용한 지식을 잊어버리는 '모델 붕괴(model collapse)'나 '치명적 망각(catastrophic forgetting)'과 같은 부작용이 발생할 수 있으므로, 적절한 정규화 기법이 필수적입니다.

**인간 피드백 기반 강화 학습 ([6]에서 발췌)**

RLHF의 세 번째 단계를 구동하는 데 사용될 수 있는 많은 RL 기반 최적화 도구(예: PPO, REINFORCE, GRPO 등)가 존재합니다. 그러나 RLHF를 위한 RL 최적화 도구의 표준 선택(원래 [2]에 의해 대중화됨)은 PPO(Proximal Policy Optimization)입니다. PPO 기반 RLHF는 최고의 LLM 연구소에서 흔히 선택되며, 대규모 LLM 사후 훈련 실행에서 최고의 결과를 산출하는 경향이 있습니다.

"RLHF는 인상적인 대화 및 코딩 능력을 가진 모델을 생성하지만, RLHF 파이프라인은 지도 학습보다 훨씬 더 복잡하며, 여러 LM을 훈련하고 훈련 루프 내에서 LM 정책에서 샘플링하는 것을 포함하여 상당한 계산 비용을 발생시킵니다." - [1]에서

PPO는 효과적임에도 불구하고 몇 가지 단점이 있습니다. PPO는 온라인 RL 알고리즘일 뿐만 아니라, LLM의 네 가지 다른 복사본(즉, 정책, 참조 정책, 보상 모델 및 가치 함수)을 메모리에 저장합니다. 이는 PPO로 훈련을 수행하기 위해 많은 메모리를 가진 많은 GPU가 필요하다는 것을 의미합니다. 또한, PPO 기반 RLHF에는 제대로 튜닝되지 않으면 최적이 아닌 성능을 초래할 수 있는 수많은 구현 세부 사항이 존재합니다. 이러한 복잡성 때문에 PPO는 하이퍼파라미터 튜닝이 까다롭고, 학습 안정성이 떨어질 수 있다는 한계가 있습니다.

**RL 학습 중에는 무슨 일이 일어날까요?** RLHF의 RL 학습 단계에서, 우리는 학습된 보상 모델을 사용할 수 있으며, 이 보상 모델이 우리 LLM의 출력에 할당하는 보상을 최대화하고자 합니다. 또한, 학습 중에 원래 모델에서 너무 멀리 "표류(drifting)"하는 것을 피하고자 합니다. 이 최적화 과정은 일반적으로 아래에 제시된 목표를 통해 공식화됩니다.

**표준 RLHF 목표**

이 방정식에서 우리는 학습된 정책과 초기 SFT 모델(또는 다른 참조 모델) 간의 KL 발산(KL divergence)에 대한 가산 페널티(additive penalty) 하에서 우리 LLM의 완성에 의해 수신되는 예상 보상을 최대화합니다. KL 발산은 손실 함수에 페널티 항으로 포함됩니다. 보상과 KL 발산 간의 균형은 하이퍼파라미터(hyperparameter) β에 의해 제어됩니다.

**RLHF는 왜 그렇게 어려울까요?** RL 기반 선호도 조정은 여러 가지 이유로 사용하기 복잡합니다. 예를 들어, 여러 LLM이 관련되고, 학습 중에 이 모델들로부터 생성을 샘플링해야 하며, 하이퍼파라미터 튜닝이 필요하고, 계산/메모리 비용이 높습니다. 실제로는 이러한 복잡성으로 인해 RLHF 학습 과정이 불안정하고, 예측 불가능하며, 비용이 많이 들고, 일반적으로 어렵습니다. 이러한 문제들은 LLM 사후 훈련 연구에 대한 진입 장벽을 크게 높입니다.

높은 수준에서 PPO 기반 RLHF가 그렇게 복잡하고, 비용이 많이 들며, 제대로 구현하기 어려운 두 가지 주요 이유가 있습니다.

*   명시적 보상 모델(explicit reward model) 사용.
*   RL을 사용하여 LLM 훈련.

보상 모델은 별도로 훈련하고 훈련 중에 메모리에 저장해야 하는 추가 LLM입니다. 또한, 훈련을 위해 PPO를 사용하면 모델의 또 다른 복사본인 가치 함수(value function)를 메모리에 저장해야 하며, RL 기반 선호도 튜닝의 모든 추가적인 어려움이 발생합니다. 따라서 별도의 보상 모델과 RL 사용을 단순히 피할 수 있다면, PPO 기반 RLHF와 관련된 많은 일반적인 골칫거리도 피할 수 있을 것입니다!

#### DPO는 어디에 적합할까요? (참조: [1, 2, 6, 9])

위에서 보았듯이, DPO는 RLHF의 대안 역할을 하는 정렬 알고리즘입니다. 그러나 RLHF와 달리 DPO는 별도의 보상 모델이나 어떤 형태의 RL 훈련도 사용하지 않고 간접적인 방식으로 RLHF 목표를 해결하기 위해 경사 상승법(gradient ascent)을 통해 정책을 최적화합니다. 이는 DPO가 기존 RL 기반 정렬 방법의 복잡성을 크게 줄이면서도 유사한 성능을 달성할 수 있게 합니다.

"우리는 명시적인 보상 모델링이나 강화 학습 없이 언어 모델을 인간의 선호도에 따르도록 직접 최적화하는 방법을 보여줍니다. 우리는 기존 RLHF 알고리즘과 동일한 목표를 암묵적으로 최적화하지만 구현이 간단하고 훈련하기 쉬운 알고리즘인 DPO를 제안합니다." - [1]에서

DPO는 보상의 새로운 재매개변수화(reparameterization)를 도입하여 RLHF 목표를 다룹니다. 이는 별도의 보상 모델이 아닌 정책 자체에서 직접 보상을 도출하는 것으로, 이를 "암묵적(implicit)" 보상이라고 부릅니다. DPO로 LLM을 훈련할 때, 우리는 기존 보상 모델을 훈련하는 것과 유사한 방식으로 오프라인 선호도 데이터셋을 사용하여 이 암묵적 보상을 학습합니다. DPO의 핵심 통찰은 이 암묵적 보상에서 RLHF에 대한 최적 정책(optimal policy)을 직접 추출할 수 있다는 것입니다. 근본적으로 DPO는 Bradley-Terry 모델에 기반한 암묵적 보상 모델을 학습하고, 이 암묵적 보상으로부터 최적 정책을 간접적으로 도출합니다.

DPO는 별도의 명시적 보상 모델 훈련을 요구하지 않기 때문에, 일부 실무자들은 DPO가 보상 모델링을 완전히 "회피"하고 RL이나 보상 모델 없이 RLHF를 통해 정책을 직접 최적화한다고 잘못 생각합니다. 실제로는 DPO는 여전히 보상 모델링 접근 방식입니다. 그 훈련 목표와 과정은 전통적인 보상 모델링과 동일합니다. DPO에서 우리는 실제로 보상 모델을 훈련하고 있습니다. 유일한 차이점은 이 보상 모델이 정책 자체 내에 암묵적으로 존재한다는 것입니다. 이 암묵적 보상을 최적화하도록 정책을 훈련함으로써, DPO는 RLHF 목표를 최적으로 해결하는 정책을 찾을 수 있도록 합니다. 이는 DPO를 일종의 "모델-프리(model-free)" 강화 학습 방식으로 간주할 수도 있게 합니다.

위에서 묘사된 바와 같이, DPO는 외부 보상 모델, 온라인 샘플링, 그리고 전체적인 RL을 피합니다. 대신, 우리는 기본적인 경사 강하(gradient descent)를 사용하여 LLM을 직접 최적화하여 RLHF 목표를 (암묵적으로) 해결합니다. 이러한 단순화는 DPO를 RL 기반 선호도 조정에 비해 더 안정적이고(하이퍼파라미터 튜닝이 덜 필요함) 경량화하여 사후 훈련 연구의 대중화에 기여합니다. 이는 오픈 소스 커뮤니티에서 DPO가 널리 채택되는 주요 원동력이 되었으며, 소규모 연구팀도 고품질의 정렬된 모델을 개발할 수 있도록 했습니다.

### 쿨백-라이블러(Kullback-Leibler, KL) 발산

LLM 사후 훈련 전반에 걸쳐, 쿨백-라이블러 발산 제약(KL divergence constraint)을 조건으로 모델을 최적화하는 경우가 많습니다. 예를 들어, RLHF 내에서 사용되는 정식 최적화 목표는 아래에 제시된 형태를 가집니다.

**KL 제약이 포함된 표준 RLHF 목표**

보시다시피, 우리는 보상을 최대화하면서 이 보상에서 차감되는 페널티 항(β로 가중된 KL 발산)을 최소화하고자 합니다. 페널티 항의 목표는 학습 중에 우리의 정책 3이 참조 정책에서 너무 멀리 표류하는 것을 방지하는 것입니다. 이것이 정확히 무엇을 의미하는지 더 깊이 이해해 봅시다. KL 발산의 존재는 모델이 지나치게 공격적으로 학습하여 기존의 유용한 지식을 잃거나, 의미 없는 텍스트를 생성하는 '퇴화(degeneration)' 현상을 방지하는 데 필수적입니다.

쿨백-라이블러 발산은 정보학(information theory) 분야에서 유래한 개념으로, 특정 확률 분포가 기준 분포와 얼마나 차이가 나는지 정량화하는 척도입니다. 이산 확률 분포(discrete probability distribution)의 경우, KL 발산은 아래에 제시된 형태를 가집니다. 특히, KL 발산은 대칭적이지 않습니다. 인수의 순서가 중요합니다.

**연속 및 이산 확률 분포에 대한 KL 발산**

연속 확률 분포의 경우, KL 발산을 기댓값(expectation)으로 공식화할 수 있습니다. 위를 참조하십시오. 이 개념이 명확하지 않다면, 이 글을 읽어보십시오.

**LLM과의 연관성.** LLM 영역에서 쿨백-라이블러 발산은 두 LLM 또는 정책을 비교하는 데 일반적으로 사용됩니다. 일반적으로 우리는 현재 훈련하려는 정책을 참조 정책과 비교합니다. 예를 들어, DPO의 경우, 우리는 SFT 정책(즉, 사전 훈련과 SFT를 모두 거친 LLM)으로 시작한 다음, 이 SFT(참조) 정책과 우리가 훈련하는 정책 간의 KL 발산이 계산되는 표준 RLHF 목표를 최적화합니다. 구체적으로, 이 KL 발산의 형태는 다음과 같습니다.

**두 LLM 간의 KL 발산**

이 형태의 KL 발산은 입력으로 질문 x가 주어졌을 때 결과물 y에 대해 현재 모델과 참조 모델 모두가 예측한 확률의 비율을 살펴봅니다. 결과물 y의 확률은 단순히 LLM이 결과물 내 각 토큰에 대해 예측한 다음 토큰 확률의 곱입니다. 이러한 완성 확률에 대한 KL 발산을 계산함으로써, 우리는 두 모델이 예측한 토큰 분포 간의 유사성을 포착합니다.

**실제 KL 발산 추정.** 우리는 일반적으로 RL 학습 중에 현재 정책이 예측한 분포와 고정된 참조 정책(예: SFT 모델 5) 간의 KL 발산을 추정하고자 합니다. 직관적으로, RL 학습 중에 사용되는 보상에 이 제약(아래에 제시됨)을 추가하면 훈련되는 정책이 참조 정책과 너무 달라지지 않도록 보장합니다. 실제로는 우리는 일반적으로 KL 발산을 근사화하는데, 이는 보시다시피 간단합니다. 그러나 이 근사화를 수행하는 방법에는 여러 가지 옵션이 있습니다.

일반적으로 KL 발산을 근사화하는 것은 KL 발산의 기댓값(연속) 형태를 사용합니다. 위에서 설명했듯이, 이 형태의 KL 발산은 단순히 두 분포의 로그 확률을 서로 빼고 이 차이의 기댓값을 취합니다. 토큰 로그 확률이 RL 학습의 다양한 측면(예: PPO 목표)에서 이미 사용된다는 점을 고려할 때, 이러한 표현식은 우리가 계산하기에 매우 쉽습니다! 구체적으로, 질문 x가 주어졌을 때 현재 정책과 참조 정책 간의 KL 발산을 계산하려고 한다고 가정해 봅시다. 이를 위해 우리는 다음을 수행합니다.

1.  현재 정책(참조 정책이 아님)으로 질문에 대한 결과물을 생성합니다.
2.  이 결과물의 각 토큰에 대한 로그 확률을 현재 정책과 참조 정책 모두에서 가져옵니다.
3.  토큰 로그 확률을 합산하여 시퀀스 로그 확률을 얻습니다.
4.  현재 정책과 참조 정책 간의 시퀀스 로그 확률 차이를 취합니다.

이 과정의 마지막 단계에서는 KL 발산의 근사치를 계산하는 데 사용할 수 있는 여러 옵션이 있으며, 이 모든 옵션은 아래 코드에 나와 있습니다. 이러한 구현이 실제 환경에서 사용되는 예는 여기를 참조하십시오.

```python
"""
Assume we already have necessary logprobs available.
logprob: completion logprob from the policy
ref_logprob: completion logprob from the reference policy
"""
kl_div = logprob - ref_logprob # difference
kl_div = (logprob - ref_logprob).abs() # absolute
kl_div = 0.5 * (logprob - ref_logprob).square() # mse
kl_div = F.kl_div(ref_logprob, logprob, reduction='batchmean') # per token
```

이 KL 발산 추정치는 여기에서 설명된 RL 미세 조정을 위한 목표의 일부로 우리 시퀀스의 보상에서 차감됩니다. β 하이퍼파라미터는 이 제약의 강도를 조절하며, 이는 모델이 선호도에 얼마나 공격적으로 적응할지와 기존 지식을 유지할지 사이의 균형을 결정합니다.

### DPO(Direct Preference Optimization) [1]

LLM 학습의 기본과 이 프레임워크에서 DPO의 역할을 확립했으므로, 이제 DPO 자체의 메커니즘을 학습하는 데 집중할 수 있습니다. DPO는 표준 RLHF의 대안(또는 함께 사용될 수 있는) 역할을 하는 선호도 조정 방법입니다. 이 섹션에서는 RLHF에서 사용되는 학습 목표부터 시작하여 DPO 학습 과정을 처음부터 도출할 것입니다. 그런 다음 DPO의 실제 구현에 대해 논의할 것입니다. 여기에는 처음부터 단계별 구현과 DPO를 사용하여 LLM을 훈련하는 구체적인 예가 포함됩니다.

**요약: DPO란 무엇인가?**

**DPO 학습 손실**

우리가 배웠듯이, DPO는 명시적 보상 모델과 RL을 피하고, 대신 더 간단한 경사 강하(gradient descent) 접근 방식을 통해 RLHF 목표를 간접적으로 해결하는 선호도 조정 접근 방식입니다. 단일 선호도 쌍에 대해 위에 제시된 DPO 손실은 다음을 통해 LLM을 훈련합니다.

*   참조 정책에 대한 선택된 결과물의 상대적 확률을 증가시킵니다.
*   거부된 결과물의 상대적 확률을 감소시킵니다.

이 손실 함수는 MLE를 사용하여 오프라인 선호도 데이터셋에 대해 최적화하기 간단합니다. 따라서 우리는 RL 없이 보상 모델과 유사하게 LLM을 훈련할 수 있습니다. 또한, 이 접근 방식은 경량화되고 간단함에도 불구하고 RLHF에서 최적화하는 것과 동일한 목표를 해결하는 정책을 여전히 산출합니다! 이 단순함은 DPO가 계산 효율성을 극대화하고, 복잡한 강화 학습 환경 설정 없이도 강력한 정렬 효과를 얻을 수 있게 하는 핵심 요인입니다.

"모델 응답에 대한 인간 선호도 데이터셋이 주어지면, DPO는 간단한 이진 교차 엔트로피(binary cross entropy) 목표를 사용하여 정책을 최적화할 수 있으며, 선호도 데이터에 맞춰진 암묵적 보상 함수에 대한 최적 정책을 생성합니다." - [1]에서

이 손실을 연구하면, 보상 모델을 훈련하는 데 사용되는 손실 함수와 매우 유사하다는 것을 알 수 있습니다. 이는 참고를 위해 아래에 복사되어 있습니다. 주요 차이점은 보상 모델의 출력을 우리 정책에서 파생된 암묵적 보상으로 대체한다는 것입니다. 나중에 보겠지만, DPO 목표는 선택된 결과물 및 거부된 결과물의 로그 확률을 조정하는 것 외에도, LLM의 암묵적 보상 추정치가 잘못된 예제에 자연스럽게 강조를 둡니다. (출처)

#### DPO 손실 도출

이제 DPO의 핵심 아이디어를 이해했으므로, DPO가 어디에서 왔는지, 그리고 DPO가 표준 RLHF와 동일한 최적화 문제를 해결하고 있다는 것을 어떻게 아는지 이해해야 합니다. 이를 위해 우리는 이론에 의존할 것이며, 이는 이 섹션에 많은 방정식이 포함될 것임을 의미합니다. 이론이 해석하기 어려울 수 있지만, 이를 이해하는 것은 DPO가 작동하는 이유에 대한 근본적인 이해를 얻는 데 도움이 됩니다. 이론을 소화하기 쉽게 만들기 위해, 우리는 각 단계에 대한 해당 설명과 함께 도출 과정을 단계별로 나눌 것입니다.

**DPO 손실 함수를 도출하기 위해 따르는 단계**

**증명 개요.** 표준 RLHF 학습 목표에서 시작하여, 우리는 네 가지 핵심 단계(위에 제시됨)를 따라 DPO에서 사용되는 학습 손실을 도출할 수 있습니다.

1.  RLHF에서 최적 정책에 대한 표현식 도출.
2.  이 표현식을 재배열하여 암묵적 보상 함수 형성.
3.  암묵적 보상을 Bradley-Terry 선호도 모델에 삽입.
4.  이 암묵적 선호도 모델과 일치하도록 LLM 훈련 — 이것이 우리가 DPO 학습 과정에서 하는 일입니다.

위 단계들은 RLHF에서 LLM을 훈련하는 데 사용되는 목표로 시작하여 DPO 손실 함수로 끝납니다. 이 도출에서 우리는 RLHF 최적화 문제를 재구성하여 DPO 학습 방법론에 도달합니다. 보시다시피, RLHF와 DPO는 복잡하게 연결되어 있습니다. 그들은 동일한 최적화 문제를 해결하려고 합니다! 아래 도출을 연구함으로써, 우리는 이러한 기술들 간의 관계에 대해 더 깊이 이해하게 됩니다.

**(1단계) RLHF의 최적 해.** DPO 손실을 도출하기 위해, 우리는 해결하려는 초기 RLHF 목표에서 시작해야 합니다. 이는 가독성을 위해 아래에 다시 복사되었습니다. 그러나 이 표기법에서 학습된 보상 모델 RM을 사용하는 대신, 우리는 일반 보상 함수 r(x, y)를 사용합니다. 일반 보상 함수는 우리의 보상 모델을 포함할 수 있지만, 이에 국한되지는 않습니다.

**일반 보상 함수가 있는 표준 RLHF 목표**

이 목표에서 시작하여, 우리는 아래 단계를 따라 이 목표에 대한 최적 해의 닫힌 형태 표현식(closed-form expression)을 찾을 수 있습니다. 간단히 말해, 우리는 아래에 제시된 RLHF 목표를 실제로 최대화하는 π 값을 찾고 있습니다!

위 도출의 마지막 두 단계에서 우리는 Z(x) 함수를 도입하는데, 이를 분할 함수(partition function)라고 부를 것입니다. 분할 함수는 아래에 정의되어 있습니다.

**DPO에서 사용되는 분할 함수**

보시다시피, 분할 함수는 참조 정책과 입력 질문 x에만 의존하며, 현재 정책이나 결과물에는 의존하지 않습니다. "분할 함수"라는 이름은 확률론(probability theory) 및 통계 역학(statistical mechanics)과 같은 분야에서 차용되었습니다. 여기를 참조하십시오. 가장 간단한 수준에서, 분할 함수는 DPO의 이론적 도출에서 사용되는 정규화 항(normalization term)일 뿐입니다. 우리는 Z(x)를 사용하여 우리가 도출하는 확률 분포(이 경우 RLHF 목표에 대한 최적 정책)가 합이 1이 되어 유효한 분포를 형성하도록 보장합니다.

이제 분할 함수를 이해했으므로, 위 빨간색 상자에 제시된 방정식에서 도출을 계속할 것입니다. 구체적으로, 우리는 이 항의 일부를 추출하여 아래 표현식을 정의할 것입니다. 우리는 이 항을 "최적 정책"이라고 부릅니다. 그 이유는 곧 명확해질 것입니다.

앞서 언급했듯이, 분할 함수는 위 표현식에서 최적 정책에 대한 정규화 항으로 사용됩니다. 우리는 위에 정의된 최적 정책이 유효한 확률 분포라는 것을 우리는 알고 있습니다. 그 이유는 다음과 같습니다.

*   모든 가능한 결과물 y에 대해 최적 정책의 값은 ≥ 0입니다.
*   모든 결과물 y에 대한 최적 정책의 합은 1과 같습니다.

첫 번째 속성은 명백합니다. 최적 정책의 모든 구성 요소는 음수가 아닙니다 6. 두 번째 속성에 대한 증명은 아래에 제공되며, 여기서 우리는 분할 함수 Z(x)가 최적 정책 분포를 정규화하는 데 어떻게 사용되는지 직접 볼 수 있습니다.

이제 최적 정책을 정의하고(그리고 유효성을 검증하고) 나면, 이 항이 나타났던 원래 표현식으로 돌아가 최적 정책에 대한 표현식을 대입할 수 있습니다. 이는 아래에 제시된 방정식을 산출합니다.

위의 최종 항에서 우리는 이 도출의 핵심을 봅니다. 즉, 표준 RLHF 목표는 최적 정책과의 KL 발산을 최소화하는 정책 π를 찾음으로써 최소화됩니다. 두 확률 분포가 동일할 때 KL 발산이 최소값(0)에 도달하므로 7, 이 최적화의 해는 최적 정책 자체입니다. 따라서 이름이 붙여졌습니다. 따라서 우리는 표준 RLHF 목표에 대한 최적 해를 아래 방정식에 제시된 대로 표현할 수 있습니다.

**표준 RLHF 목표를 최적으로 해결하기**

**(2단계) 암묵적 보상 도출.** 여기에서 우리는 위에 제시된 최적 정책에 대한 표현식을 가져와 재배열하여 보상 함수에 대한 표현식(최적 정책 측면에서)을 아래에 제시된 대로 도출할 수 있습니다.

이제 우리는 보상의 재매개변수화를 도출했습니다. 그러나 이 보상 함수는 어떤 명시적 보상 모델에도 의존하지 않습니다. 오히려 우리는 최적 정책과 참조 정책에서 계산된 확률만을 사용하여 보상을 추정합니다. 우리는 이것을 "암묵적" 보상이라고 부를 것입니다.

"이 변수 변경 접근 방식은 명시적이고 독립적인 보상 모델을 피합니다... 정책 네트워크는 언어 모델과 (암묵적) 보상 모두를 나타냅니다." - [1]에서

이제 남은 유일한 문제는 우리 암묵적 보상에 있는 Z(x) 항입니다. 분할 함수는 모든 가능한 결과물 y에 대한 합을 취하므로, Z(x) 값을 계산하는 것은 실제로는 비용이 많이 듭니다. 더 나아가, 독립적인 보상 모델 훈련 없이 직접 계산할 수 없는 보상 함수 r(x, y)도 Z(x)의 표현식에 나타납니다. 이를 해결하기 위해 우리는 Bradley-Terry 모델을 다시 살펴보고 이를 우리의 암묵적 보상 함수와 결합해야 합니다.

**(3단계) Bradley-Terry 선호도 모델.** Bradley-Terry 선호도 모델 하에서, 우리는 주어진 결과물이 다른 결과물보다 선호될 확률을 계산할 수 있습니다. 대부분의 경우, 이 선호도 모델의 입력은 각 결과물에 대한 명시적 보상(보상 모델에 의해 예측됨)입니다. DPO의 경우, 우리는 이 명시적 보상을 우리의 암묵적 보상 함수로 대체합니다. 아래를 참조하십시오.

위의 최종 방정식에 제시된 바와 같이, 우리는 이제 우리의 암묵적 보상 함수를 사용하는 Bradley-Terry 선호도 모델에 대한 표현식을 가지며, 여기서 암묵적 보상은 최적 정책과 참조 정책에만 의존합니다. Bradley-Terry 표현식의 쌍별 특성과 Z(x)의 값이 x에만 의존하고(y에는 의존하지 않음) 있다는 사실 때문에, 암묵적 보상 함수의 Z(x) 구성 요소는 선택된 결과물에 대한 암묵적 보상에서 거부된 결과물에 대한 암묵적 보상을 뺄 때 실제로 상쇄됩니다.

**(4단계) 정책 훈련.** 위 표현식은 고정된 최적 정책에 의존합니다. 이 최적 정책은 우리가 해결하려는 RLHF 목표의 해입니다. 여기에서 우리는 이 최적 정책을 복구할 수 있는 학습 목표를 도출하는 방법을 결정해야 합니다. 이를 위해 DPO는 위 표현식의 최적 정책을 학습된 정책으로 대체합니다. 아래를 참조하십시오.

이 두 표현식을 어떻게 같게 만들 수 있을까요? 우리는 학습된 정책을 훈련해야 합니다! 구체적으로, 우리는 우리의 암묵적 보상 함수를 기반으로 선택된 응답이 거부된 응답보다 선호될 확률을 경험적으로 최대화하도록 학습된 정책을 최적화하는 순위 손실(ranking loss)을 공식화할 수 있습니다. 이를 통해 우리는 우리의 선호도 모델이 정확하고, 따라서 최적 정책의 선호도 모델과 일치하도록 보장합니다.

명시적 보상을 암묵적 보상으로 대체하는 것 외에도, 이 손실 함수는 표준 보상 모델이 사용하는 것과 정확히 동일한 학습 목표입니다. 아래를 참조하십시오.

**DPO를 위해 도출된 최종 손실 표현식**

우리는 또한 이 손실 함수가 DPO의 학습 목표와 동일하다는 것을 알 수 있습니다. 우리는 이제 RLHF의 학습 목표에서 시작하여 DPO 학습 목표를 완전히 도출했습니다. DPO의 학습 과정은 우리 정책에 기반한 암묵적 보상 모델을 학습합니다. 이 암묵적 보상 함수를 학습함으로써, 우리는 RLHF의 최적 정책과 일치하는 정책을 얻습니다.

### DPO는 실제로 최적 정책을 산출할까요?

"[DPO] 최적화 목표는 [암묵적] 보상 매개변수화를 가진 Bradley-Terry 모델과 동등하며, 우리는 우리의 매개변수 모델을 보상 모델 최적화와 동등하게 최적화합니다... 우리는 [이 목표]가 학습된 보상 모델의 클래스를 제약하지 않으며 최적 정책의 정확한 복구를 허용한다는 것을 보여줍니다." - [1]에서

위 도출에 따르면, DPO 손실을 사용하여 LLM을 훈련하면 최적 정책과 동일한 선호도 분포(암묵적 보상에 의해 유도됨)를 가진 모델이 생성됩니다. 다시 말해, DPO 손실을 통해 우리 정책이 학습한 암묵적 보상 함수는 우리 선호도 데이터셋에서 선택된 결과물 및 거부된 결과물을 올바르게 순위를 매길 것입니다. 그러나 DPO의 목표는 좋은 암묵적 보상 함수를 가진 모델을 훈련하는 것이 아닙니다. 우리는 LLM을 정렬하고 고품질 결과물을 생성하는 정책을 도출하기를 원합니다! 다행히도, [1]의 저자들은 고품질 암묵적 보상 함수를 학습하는 것 외에도 DPO를 통해 도출된 정책이 RLHF의 최적 정책과 일치해야 함을 보여주는 최종 증명을 제공합니다.

두 보상 함수 r(x, y)와 r’(x, y)는 r(x, y) - r’(x, y) = f(x) (일부 함수 f(•)에 대해)일 때 그리고 그 때에만 동등합니다.

**동등한 보상.** 증명을 시작하기 위해, 우리는 먼저 보상 함수에 대한 동등 관계(equivalence relation)를 지정할 수 있습니다. 이것은 두 보상 함수가 같다는 것이 무엇을 의미하는지 포착하는 정의일 뿐입니다. 위를 참조하십시오. 간단히 말해, 보상 함수는 보상 차이가 결과물에 의존하지 않고 질문에만 의존할 경우 동등하다고 간주됩니다. 이 정의를 사용하여, 우리는 아래에서 두 개의 동등한 보상 함수가 동일한 선호도 분포 8를 산출함을 보장한다는 것을 보여줍니다.

우리는 또한 두 개의 동등한 보상 함수가 이전 섹션에서 탐구한 표준 RLHF 목표에 대입될 때 동일한 최적 정책을 산출함을 보장한다는 것을 보여주는 유사한 증명을 작성할 수 있습니다. 아래를 참조하십시오.

**최적 정책 증명.** 위 결과들을 고려할 때, 이 증명의 마지막 단계는 DPO 내에서 사용되는 암묵적 보상 함수가 RLHF 내에서 사용되는 실제 보상과 동등하다는 것을 단순히 보여주는 것입니다. 이 두 보상 함수가 동등 관계를 만족한다면, 우리는 DPO가 위에 제시된 결과에 따라 RLHF와 동일한 최적 정책을 산출할 것임을 압니다.

이 최종 결과를 증명하기 위해, 우리는 RLHF에서 사용되는 임의의 보상 함수 r(x, y)를 고려하는 것부터 시작할 수 있습니다. 우리의 목표는 DPO의 암묵적 보상이 r(x, y)와 동등하다는 것을 보여주는 것입니다. 임의의 보상이 주어지면, 우리는 위에 제시된 수정된 9 보상 표현식을 정의할 수 있습니다. 이 표현식은 r(x, y)에서 추가 항(즉, 분할 함수의 로그)을 뺄 뿐입니다. 또한 r(x, y)에서 빼는 항이 x에만 의존한다는 점에 유의하십시오. 이러한 이유로, 수정된 보상 표현식은 우리가 이전에 정의한 동등 관계에 따라 r(x, y)와 동등합니다.

"두 번째 보조 정리(lemma)는 동일한 클래스의 모든 보상 함수가 동일한 최적 정책을 산출한다고 명시합니다. 따라서 우리의 최종 목표를 위해, 우리는 최적 클래스에서 임의의 보상 함수를 복구하는 데만 관심이 있습니다." - [1]에서

원하는 결과를 증명하기 위해, 우리는 최적 RLHF 해를 재배열하여 암묵적 보상을 생성하는 이전 표현식을 활용해야 합니다. 이 암묵적 보상을 위의 수정된 보상 표현식에 대입하면, 우리는 r(x, y)와 동등하다고 알려진 보상(DPO의 암묵적 보상와 일치함)을 얻습니다. 아래를 참조하십시오. 결과적으로, 우리는 이제 DPO가 사용하는 암묵적 보상이 r(x,y)와의 동등 관계를 만족한다는 것을 알게 되었고, 이는 증명을 완료합니다.

**핵심 요점.** 이 섹션을 마무리하기 전에, 우리는 방금 증명한 결과를 빠르게 맥락화해야 합니다. 이전 섹션에서 우리는 표준 RLHF 목표에 대한 최적 정책(또는 해)의 암묵적 보상에 의해 유도되는 선호도 분포에 대한 표현식을 도출했습니다. 이 표현식이 도출된 후, 우리는 일반 보상 모델과 동일한 훈련 전략을 채택함으로써 이 선호도 분포와 일치하는 암묵적 보상 함수를 갖도록 모델을 쉽게 훈련할 수 있습니다. 따라서 DPO의 핵심 훈련 절차는 (암묵적) 보상 모델 훈련을 중심으로 이루어지며, 이것이 논문의 제목이 된 이유입니다. 아래를 참조하십시오.

DPO에 대한 흔한 오해는 DPO가 보상 모델을 제거한다는 것인데, 이는 사실이 아닙니다. 사실, DPO는 보상 모델링에 전적으로 기반합니다. 보상 모델이 단지 암묵적이라는 것은 우리가 명시적 보상 모델 훈련을 피할 수 있다는 것을 의미합니다.

"자주 오해되는 점은 DPO가 핵심적으로 보상 모델을 학습한다는 것입니다. 그래서 논문의 부제가 '당신의 언어 모델은 비밀리에 보상 모델이다(Your Language Model is Secretly a Reward Model)'입니다. 이것을 DPO 목표가 정책을 직접 훈련하는 것으로 혼동하기 쉽습니다." - RLHF 책

DPO의 훈련 절차가 보상 모델링에 기반한다는 점을 고려할 때, 이러한 방식으로 LLM을 훈련하는 것이 실제로 최적 정책을 산출할 것이라는 점은 즉시 명확하지 않습니다. 결과 모델이 정확한 암묵적 보상 함수를 가질 수 있지만 여전히 고품질 결과물을 생성하지 못할 수도 있을까요? 이 섹션에서 우리는 이것이 사실이 아니어야 함을 증명합니다. 최적 정책의 암묵적 선호도 분포와 일치하도록 모델을 훈련하면, 결과 정책도 최적임이 보장됩니다! 간단히 말해, DPO는 RLHF를 통한 훈련으로 도출된 정책과 품질 면에서 비교할 수 있는 정책을 간접적으로 제공하며, PPO 기반 RLHF와 같은 기술보다 훨씬 덜 복잡한 유효한 선호도 조정 대안이 됩니다. 이는 실무자들이 모델의 신뢰성과 예측 가능성에 대한 확신을 가질 수 있게 해줍니다.

### DPO는 왜 작동할까요?

**DPO 손실 함수의 기울기(Gradient)**

DPO와 DPO가 잘 작동하는 이유에 대한 더 깊은 이해를 얻기 위해, 우리는 DPO 손실 함수의 기울기 구조를 살펴볼 수 있습니다. 위 10를 참조하십시오. 이 표현식에는 명확성을 위해 빨간색(일부 항은 주황색), 파란색, 녹색으로 표시된 세 가지 핵심 항이 있습니다. 각 항의 목적은 다음과 같습니다.

*   첫 번째 (빨간색) 항은 시그모이드 함수(sigmoid function)로 인해 [0, 1] 범위에 속하는 가중치이며, 거부된 결과물의 암묵적 보상이 선택된 결과물의 암묵적 보상에 비해 증가할수록 커집니다. 다시 말해, 이 항은 잘못된 암묵적 보상 추정치에 더 높은 가중치를 할당합니다. 이는 DPO가 암묵적으로 탐색-활용(exploration-exploitation) 균형을 조절하며, 모델이 아직 잘 학습하지 못한 영역에 더 집중하도록 유도하는 역할을 합니다.
*   두 번째 (파란색) 항은 LLM의 매개변수에 대한 선택된 결과물의 가능성(likelihood)의 양의 기울기이며, 이는 선택된 결과물의 가능성을 증가시키는 목적을 수행합니다.
*   세 번째 (녹색) 항은 LLM의 매개변수에 대한 거부된 결과물의 가능성의 음의 기울기이며, 이는 거부된 결과물의 가능성을 감소시키는 목적을 수행합니다.

이러한 요소들은 동시에 i) 선택된 결과물의 가능성(likelihood)을 높이고 ii) 거부된 결과물의 가능성을 낮추는 역할을 하며, 특히 LLM의 암묵적 보상 평가가 부정확할 때 더 큰 가중치(즉, LLM 매개변수에 대한 더 강력한 갱신)가 부여됩니다. 이러한 가중치는 DPO가 모델의 잘못된 판단에 더 민감하게 반응하도록 하여 학습 효율성을 높입니다.

"예제는 암묵적 보상 모델이 선호되지 않는 완성을 베타로 스케일링하여 얼마나 더 높게 평가하는지에 따라 가중치가 부여됩니다... 암묵적 보상 모델이 완성을 얼마나 잘못 순서화하는지, KL 제약의 강도를 고려합니다." - [1]에서

**가중 계수(Weighting coefficient).** [1]의 저자들은 DPO 손실 기울기의 세 가지 하위 구성 요소 모두가 알고리즘이 잘 작동하는 데 필요하다고 관찰합니다. 특히, 이 기울기에서 첫 번째 가중치 항을 제거하면(모든 선택된 결과물의 가능성을 균일하게 증가시키고 모든 거부된 결과물의 가능성을 감소시키는 기울기를 생성함), 결과 정책은 품질이 낮고 텍스트를 생성할 때 완전히 퇴화하는 경향이 있습니다. 아래를 참조하십시오. 이러한 학습 알고리즘은 비우도 훈련(unlikelihood training)이라고 불리며 과거 [5]에서 탐구되었습니다. DPO에 의해 손실 기울기에 추가된 간단한 가중치 항은 이 접근 방식을 완전히 변화시켜 고품질 LLM 정렬을 수행할 수 있게 합니다.

**비우도 훈련으로 훈련된 LLM은 퇴화하는 경향이 있습니다.**

### 처음부터 DPO 구현하기

DPO의 도출은 복잡하지만, 이 기술은 실제로는 사용하기 매우 간단합니다. 사실, DPO는 최고의 연구소 외부의 사람들을 위해 LLM 사후 훈련 연구를 대중화하는 데 큰 역할을 했습니다 [3]. PPO 기반 RLHF와 같은 알고리즘은 튜닝하기 더 어렵고 상당한 계산 자원을 필요로 합니다. 대조적으로, DPO는 RL 없이 표준 분류(또는 순위) 손실을 사용하며, 훈련 과정 내내 모델의 네 개 대신 두 개의 복사본만 유지합니다. 이러한 효율성은 DPO가 더 적은 GPU 메모리와 계산 시간으로도 강력한 결과를 도출할 수 있게 하며, LoRA(Low-Rank Adaptation)와 같은 매개변수 효율적 미세 조정(Parameter-Efficient Fine-Tuning, PEFT) 기법과 결합될 때 더욱 두드러집니다.

**표준 DPO 훈련 파이프라인**

**DPO 훈련 파이프라인.** DPO를 사용한 표준 훈련 과정은 위에 묘사되어 있습니다. 우리는 모델을 훈련하는 사용 사례를 포착하는 다양한 질문 세트로 과정을 시작합니다. 예를 들어, "다음 시를 완성해 주세요"라는 질문에 대해 "아름다운 밤하늘에 별들이 반짝이네" (선택됨)와 "내일은 해가 뜰 거야" (거부됨)와 같은 응답 쌍을 수집할 수 있습니다. 여기에서 우리는 참조 정책을 사용하여 각 질문에 대한 결과물 쌍을 생성하고, 인간 평가자가 각 쌍에 대한 선호도 주석을 제공하도록 합니다. 이 선호도 데이터셋을 사용할 수 있게 되면, 우리는 선호도 데이터셋에 대해 이전에 도출한 DPO 손실을 최소화하도록 모델을 훈련함으로써 최대 우도 추정을 수행합니다.

**PyTorch에서 DPO 손실 계산**

**손실 구현.** 우리는 PyTorch에서 DPO의 손실 함수를 아주 쉽게 구현할 수 있습니다. 이는 현재 정책과 참조 정책에서 파생된 암묵적 보상에 적용되는 순위 손실일 뿐입니다. [1]에서 가져온 손실의 예시 구현은 참고를 위해 아래에 복사되어 있으며, 여기서 손실이 다음을 통해 계산됨을 알 수 있습니다.

*   현재 정책과 참조 정책에 의해 각 결과물(선택된 것과 거부된 것 모두)에 할당된 로그 확률을 얻습니다.
*   현재 정책과 참조 정책 모두에 대해 선택된 결과물 및 거부된 결과물 간의 확률 비율을 계산합니다.
*   위의 확률 비율을 사용하여 최종 DPO 손실을 구성합니다.

**오프라인 선호도 데이터 처리.** DPO는 근본적으로 오프라인 선호도 학습 알고리즘입니다. 우리는 정적 선호도 데이터셋에 대해 모델을 최적화합니다. 위에 설명된 파이프라인에서 우리는 참조 모델을 사용하여 선호도 데이터셋의 결과물을 생성합니다. 그러나 대부분의 실제 응용 프로그램에서는 그렇지 않을 수 있습니다. 실무자로서 우리는 UltraFeedback [4]와 같은 선호도 데이터셋을 온라인에서 다운로드하고 DPO를 사용하여 이 정적 데이터셋에 대해 모델을 훈련할 수 있습니다. 이러한 경우, 실제 참조 모델은 알려져 있지 않으며 DPO 훈련에서 사용한 참조 모델과 다를 수 있어 분포 변화(distribution shift)를 초래합니다. 이러한 데이터셋의 전처리 과정에서는 불일치하는 형식, 중복된 항목, 또는 낮은 품질의 주석을 식별하고 정제하는 작업이 중요합니다.

"선호도 데이터셋은 SFT 모델을 사용하여 샘플링되므로, 가능한 경우 참조 정책을 SFT 모델로 초기화합니다. 그러나 SFT 모델을 사용할 수 없는 경우, 선호되는 완성의 우도를 최대화하여 참조 정책을 초기화합니다. 이 절차는 실제 참조 분포와 DPO가 사용하는 참조 정책 간의 분포 변화를 완화하는 데 도움이 됩니다." - [1]에서

이 분포 변화를 최소화하고 실제 참조 모델이 우리 선호도 데이터셋에 있는 결과물들과 잘 정렬되도록 보장하기 위해, [1]의 저자들은 아래에 묘사된 절차를 권장합니다. 이 절차에서 우리는 먼저 선호도 데이터셋의 선택된 결과물에 대해 참조 모델의 지도 미세 조정을 수행한 다음, 이 모델을 DPO로 추가 훈련합니다. 이 예비 SFT 훈련 단계는 DPO의 참조 정책이 선호도 데이터셋을 생성하는 데 사용된 실제 참조 정책과 너무 다르지 않도록 보장합니다.

**DPO에서 오프라인 선호도 데이터로 인한 분포 변화 완화**

DPO 구현을 위한 마지막 고려 사항은 β 하이퍼파라미터를 올바르게 설정하는 것입니다. 이는 훈련된 정책이 참조 정책과 얼마나 다를 수 있는지를 제어합니다. 기억하십시오. β는 RLHF 목표에서 KL 제약에 곱하는 가중치이며, DPO에서 선호도 정렬의 강도를 제어합니다. β 값이 낮을수록 모델이 데이터에서 관찰된 선호도에 적응하기 위해 더 공격적으로 업데이트됩니다. 일반적으로 β는 [0, 1] 범위의 값으로 설정되며, 낮은 값이 더 일반적입니다. 예를 들어, β = 0.1이 인기 있는 선택이지만, [1]의 저자들은 β = 0.1과 β = 0.5 모두를 탐구합니다. 이 값의 미세 조정은 모델의 성능과 안정성에 직접적인 영향을 미치므로, 신중한 실험이 필요합니다.

**전체 DPO 예제.** DPO로 자신만의 LLM을 미세 조정하는 가장 쉬운 방법 중 하나는 HuggingFace TRL 패키지의 `DPOTrainer`를 사용하는 것입니다. DPO 훈련을 수행하려면 i) UltraFeedback와 같은 선호도 데이터셋을 로드하고, ii) 모델/토크나이저를 선택하고(예: 큰 GPU가 없다면 Qwen3-0.6B와 같은 작은 모델이 좋습니다), iii) 아래 코드에 제시된 대로 DPO 트레이너를 실행하기만 하면 됩니다.

```python
from trl import DPOConfig, DPOTrainer

# load model and data
model = <load our model>
tokenizer = <load our tokenizer>
train_dataset = <load our preference dataset>

# configure DPO training process
training_args = DPOConfig(output_dir="./dpo_logs/")
trainer = DPOTrainer(
    model=model,
    args=training_args,
    processing_class=tokenizer,
    train_dataset=train_dataset,
)

# execute DPO training
# run the below command to execute this script
# > accelerate launch <script name>
trainer.train()
```

### 요약 및 핵심 요점

DPO(Direct Preference Optimization)는 명시적인 보상 모델이나 강화 학습(RL) 기법을 사용하지 않고, RLHF의 목적을 우회적으로 달성하는 대규모 언어 모델(LLM)을 위한 선호도 조정 기법입니다. DPO에서 우리는 RLHF 목표를 재매개변수화하여 정책 자체(및 참조 정책)에서 파생된 암묵적 보상 함수를 형성합니다. 그런 다음, 우리는 정적 선호도 데이터셋에 대해 LLM을 훈련하여 이 암묵적 보상 함수를 최적화하며, 이는 표준 보상 모델과 유사합니다. 이 암묵적 보상 모델링 목표를 해결함으로써, 우리는 RLHF 목표를 해결하는 정책을 간접적으로 산출합니다. 이 접근 방식은 RL 기반 정렬 방법에 대한 더 간단하고, 더 안정적이며, 계산 효율적인 대안을 제공하여 고품질 LLM 정렬에 대한 접근성을 높입니다. 그러나 여러 연구에서 DPO와 같은 (오프라인) 직접 정렬 알고리즘과 온라인 RL을 사용하는 정렬 기술(예: PPO 기반 RLHF) 간의 차이를 연구했으며, 성능 격차가 존재할 수 있음을 발견했습니다 [11, 12]. 이러한 사실에도 불구하고, DPO는 그 단순성과 효과성 때문에 LLM 사후 훈련에서 여전히 활발히 사용됩니다(종종 온라인 알고리즘과 함께). 향후 연구에서는 DPO의 이론적 한계를 극복하고, 다양한 도메인과 언어에 걸쳐 그 적용 가능성을 확장하며, 데이터 효율성을 더욱 높이는 방향으로 발전할 것으로 기대됩니다.

뉴스레터가 처음이신가요? 안녕하세요! 저는 Cameron R. Wolfe입니다. 넷플릭스(Netflix)의 딥러닝(Deep Learning) 박사이자 선임 연구 과학자입니다. 이것은 독자들이 AI 연구의 중요한 주제를 더 잘 이해하도록 돕는 Deep (Learning) Focus 뉴스레터입니다. 이 뉴스레터는 항상 무료로 공개될 것입니다. 뉴스레터가 마음에 드신다면 구독하거나, 유료 구독을 고려하거나, 공유하거나, X와 LinkedIn에서 저를 팔로우해주세요! 구독하기

### 참고 문헌

[1] Rafailov, Rafael, et al. "Direct preference optimization: Your language model is secretly a reward model." Advances in neural information processing systems 36 (2023): 53728-53741.
[2] Stiennon, Nisan, et al. "Learning to summarize with human feedback." Advances in neural information processing systems 33 (2020): 3008-3021.
[3] Tunstall, Lewis, et al. "Zephyr: Direct distillation of lm alignment." arXiv preprint arXiv:2310.16944 (2023).
[4] Cui, Ganqu, et al. "Ultrafeedback: Boosting language models with scaled ai feedback, 2024." URL https://arxiv. org/abs/2310.01377 .
[5] Welleck, Sean, et al. "Neural text generation with unlikelihood training." arXiv preprint arXiv:1908.04319 (2019).
[6] Lambert, Nathan, et al. "Tulu 3: Pushing frontiers in open language model post-training, 2024." URL https://arxiv. org/abs/2411.15124 297 (2025).
[7] Yang, An, et al. "Qwen3 technical report." arXiv preprint arXiv:2505.09388 (2025).
[8] Dubey, Abhimanyu, et al. "The llama 3 herd of models." arXiv e-prints (2024): arXiv-2407.
[9] Kaplan, Jared, et al. "Scaling laws for neural language models." arXiv preprint arXiv:2001.08361 (2020).
[10] Sheng, Guangming, et al. "Hybridflow: A flexible and efficient rlhf framework." Proceedings of the Twentieth European Conference on Computer Systems . 2025.
[11] Tang, Yunhao, et al. "Understanding the performance gap between online and offline alignment algorithms." arXiv preprint arXiv:2405.08448 (2024).
[12] Ivison, Hamish, et al. "Unpacking dpo and ppo: Disentangling best practices for learning from preference feedback." Advances in neural information processing systems 37 (2024): 36602-36633.

1 LLM 맥락에서 강화 학습에 대한 심층적인 설명은 여기를 참조하십시오.
2 더 구체적으로, "온라인(online)"은 정책이 각 단계에서 생성된 새로운 샘플로 반복적으로 업데이트되는 것을 의미하며, "오프라인(offline)"은 모든 훈련 데이터가 미리 고정되어 있음을 의미합니다.
3 "정책(policy)"이라는 단어는 우리가 (RL로) 훈련하는 LLM 또는 모델에 대한 RL 전문 용어입니다.
4 더 구체적으로, KL 발산은 주어진 분포가 참조 분포를 근사화하는 데 사용될 때 얼마나 많은 정보가 손실되는지를 측정합니다.
5 참조 모델이 항상 SFT 모델인 것은 아닙니다. RL 훈련의 이전 모델 체크포인트(checkpoint)일 수도 있습니다. 예를 들어, RLHF의 네 단계 또는 라운드가 순차적으로 수행되는 경우, RLHF의 두 번째 단계에 대한 참조 모델은 RLHF의 첫 번째 단계에서 얻은 모델일 수 있습니다.
6 최적 정책은 분할 함수, 참조 정책, 지수 함수의 곱이며, 이들 모두는 0보다 작을 수 없습니다. 따라서 최적 정책을 구성하는 이 항들의 곱도 음수가 아니어야 합니다.
7 이는 깁스 부등식(Gibbs’ inequality)으로 알려져 있습니다.
8 [1]에서는 이 증명이 더 일반적인 Plackett-Luce 모델(부록 A.5, 17페이지 참조)을 가정하여 제공되지만, 우리는 단순화를 위해 그리고 이 개요의 나머지 설명과 일치시키기 위해 Bradley-Terry 모델을 사용하여 이 증명을 다시 작성합니다.
9 [1]에서 저자들은 이 수정된 함수를 보상 함수의 "투영(projection)"이라고 설명합니다.
10 기억하십시오. DPO는 MLE를 사용하여 LLM을 훈련합니다. 다시 말해, 우리 LLM의 매개변수는 i) 데이터 배치에 대해 이 기울기를 계산하고, ii) 기울기에 스칼라 계수(즉, 학습률)를 곱하고, iii) 이 스케일링된 기울기를 모델 매개변수에서 빼는 과정을 반복하여 직접 업데이트됩니다. 이 기울기가 어떻게 도출되는지 이해하고 싶다면, 이 논문의 17페이지를 참조하십시오.