이번 달에는 특히 GPT-4.5와 Llama 4와 같은 새로운 플래그십 모델(flagship model)의 출시와 함께 많은 일들이 있었습니다. 하지만 이러한 출시작들에 대한 반응이 비교적 잠잠했다는 것을 눈치채셨을 것입니다. 왜 그럴까요? 한 가지 이유는 GPT-4.5와 Llama 4가 여전히 기존 모델(conventional model)로 남아있기 때문일 수 있습니다. 이는 추론을 위한 명시적인 강화 학습(reinforcement learning) 없이 훈련되었다는 것을 의미합니다. 한편, xAI와 Anthropic과 같은 경쟁사들은 모델에 더 많은 추론 능력(reasoning capability)과 기능을 추가했습니다. 예를 들어, xAI Grok과 Anthropic Claude 인터페이스 모두 이제 특정 모델에 대해 추론 능력(reasoning capability)을 명시적으로 전환하는 "사고(thinking)" (또는 "확장된 사고(extended thinking)") 버튼을 포함하고 있습니다. 어쨌든, GPT-4.5 및 Llama 4 (비추론) 모델에 대한 잠잠한 반응은 모델 크기 및 데이터 스케일링(scaling)만으로는 달성할 수 있는 한계에 도달하고 있음을 시사합니다. 그러나 OpenAI가 최근 출시한 o3 추론 모델(reasoning model)은 전략적으로 컴퓨팅 자원(compute)을 투자할 때, 특히 추론 작업에 맞춰진 강화 학습(reinforcement learning) 방법을 통해 여전히 상당한 개선의 여지가 있음을 보여줍니다. (최근 라이브스트림(livestream)에서 OpenAI 직원에 따르면, o3는 o1에 비해 10배 더 많은 훈련 컴퓨팅 자원(training compute)을 사용했습니다.) 출처: OpenAI 라이브스트림(https://openai.com/live/) (2025년 4월 16일) 추론(reasoning)만으로는 만능 해결책(silver bullet)이 아니지만, 어려운 작업에서 모델의 정확도와 문제 해결 능력(현재까지는)을 안정적으로 향상시킵니다. 그리고 저는 추론 중심의 후처리 훈련(post-training)이 미래 LLM 파이프라인(pipeline)에서 표준 관행(standard practice)이 될 것이라고 예상합니다. 따라서 이 글에서는 강화 학습(reinforcement learning)을 통한 추론(reasoning)의 최신 개발 사항을 살펴보겠습니다. 이 글은 추론 모델(reasoning model)을 개발하고 개선하는 데 사용되는 강화 학습(reinforcement learning) 훈련 방법에 초점을 맞춥니다. 비교적 긴 글이므로 아래에 목차(Table of Contents) 개요를 제공합니다. 목차를 탐색하려면 웹 보기(web view)의 왼쪽 슬라이더(slider)를 사용하십시오.

*   최신 플래그십 모델과 추론의 중요성
*   추론 모델(reasoning model)의 본질과 유형 이해하기
*   RLHF 기본 사항: LLM 정렬의 시작
*   PPO: 강화 학습의 핵심 알고리즘 상세 분석
*   RL 알고리즘의 진화: PPO를 넘어
*   보상 모델링의 패러다임 전환: RLHF에서 RLVR, 그리고 그 너머로
*   DeepSeek-R1 훈련 방식 심층 분석
*   최근 RL 연구에서 얻은 추론 모델 훈련의 주요 통찰력
*   추론 모델 훈련에 관한 최신 연구 동향

팁: 추론 기본 사항, RL, PPO 및 GRPO에 이미 익숙하시다면, 최근 추론 연구 논문에서 얻은 흥미로운 통찰력 요약이 포함된 "최근 RL 연구에서 얻은 추론 모델 훈련의 주요 통찰력" 섹션으로 바로 건너뛰셔도 좋습니다.

### 최신 플래그십 모델과 추론의 중요성

(위 서론은 원문과 동일하게 유지됩니다.)

### 추론 모델(reasoning model)의 본질과 유형 이해하기

가장 중요한 문제는 물론 추론(reasoning)의 정의입니다. 간단히 말해, 추론(reasoning)은 LLM이 복잡한 작업을 더 잘 처리하도록 만드는 추론(inference) 및 훈련 기술에 관한 것입니다. 이것이 (현재까지) 어떻게 달성되는지에 대해 좀 더 자세히 설명하기 위해, 저는 추론(reasoning)을 다음과 같이 정의하고 싶습니다:

LLM의 맥락에서 추론(reasoning)은 최종 답변을 제공하기 전에 모델이 중간 단계(intermediate step)를 생성하는 능력을 의미합니다. 이는 종종 사고의 연쇄(chain-of-thought, CoT) 추론(reasoning)으로 설명되는 과정입니다. CoT 추론(reasoning)에서 LLM은 결론에 도달하는 방법을 보여주는 구조화된 일련의 진술 또는 계산을 명시적으로 생성합니다.

그리고 아래에는 정의와 함께 그림이 있습니다.

LLM이 다단계 추론 작업(multi-step reasoning task)을 어떻게 처리할 수 있는지에 대한 간략한 그림입니다. 단순히 사실을 기억하는 대신, 모델은 올바른 결론에 도달하기 위해 여러 중간 추론 단계(intermediate reasoning step)를 결합해야 합니다. 중간 추론 단계(intermediate reasoning step)는 구현에 따라 사용자에게 표시될 수도 있고 표시되지 않을 수도 있습니다.

추론은 단순히 정답을 내놓는 것을 넘어, 그 정답에 도달하는 과정을 모델 스스로 구성하고 검증하는 능력에 초점을 맞춥니다. 이는 인간의 "시스템 2" 사고(System 2 thinking)와 유사하게, 문제 해결을 위한 의식적이고 순차적인 단계를 포함합니다. 최근에는 CoT 외에도 Tree-of-Thought, Self-Consistency, Least-to-Most Prompting과 같은 다양한 추론 기법들이 개발되어 모델의 문제 해결 능력을 더욱 향상시키고 있습니다. 이러한 기법들은 모델이 여러 추론 경로를 탐색하거나, 여러 답변을 생성한 후 가장 일관성 있는 것을 선택하거나, 복잡한 문제를 더 작은 하위 문제로 분해하여 해결하도록 돕습니다. 추론 능력의 향상은 단순히 훈련 데이터의 양을 늘리는 것을 넘어, 모델이 정보에 접근하고 처리하는 방식 자체의 변화를 의미합니다.

### RLHF 기본 사항: LLM 정렬의 시작

기존 LLM은 일반적으로 3단계 훈련 절차를 거칩니다:
*   사전 학습(Pre-training)
*   지도 미세 조정(Supervised fine-tuning)
*   정렬(Alignment) (일반적으로 RLHF를 통해)

"원래" LLM 정렬(alignment) 방법은 RLHF이며, 이는 최초의 ChatGPT 모델을 개발하는 데 사용된 레시피를 설명한 InstructGPT 논문을 따른 LLM 개발 시 표준 레퍼토리(repertoire)의 일부입니다. RLHF의 원래 목표는 LLM을 인간의 선호도에 정렬(align)하는 것입니다. 예를 들어, LLM이 주어진 프롬프트(prompt)에 대해 여러 답변을 생성하는 경우 LLM을 여러 번 사용한다고 가정해 봅시다. RLHF는 LLM이 사용자가 선호하는 답변 스타일을 더 많이 생성하도록 안내합니다. (종종 RLHF는 LLM의 안전성 조정(safety-tune)에도 사용됩니다: 민감한 정보 공유 방지, 욕설 사용 방지 등.)

RLHF는 LLM을 인간의 가치와 선호도에 맞추는 데 혁혁한 공을 세웠지만, 그 과정에는 여러 도전 과제가 따릅니다. 인간 피드백 수집은 비용이 많이 들고 시간 소모적이며, 수집된 데이터는 주관적이고 편향될 수 있습니다. 또한, 보상 모델(reward model)이 인간의 선호도를 완벽하게 학습하지 못하고, 때로는 모델이 보상 함수를 "해킹(hack)"하여 실제로는 좋지 않은 결과를 내면서도 높은 보상을 얻는 "보상 해킹(reward hacking)" 현상이 발생하기도 합니다. 이러한 문제를 해결하기 위해 DPO(Direct Preference Optimization)와 같은 새로운 접근 방식이 등장했습니다. DPO는 보상 모델을 명시적으로 훈련하는 대신, 선호도 데이터를 직접 사용하여 정책 모델을 최적화함으로써 RLHF의 복잡성을 줄이고 안정성을 높입니다.

RLHF 파이프라인(pipeline)은 사전 학습(pre-trained)된 모델을 가져와 지도 방식(supervised fashion)으로 미세 조정(fine-tune)합니다. 이 미세 조정(fine-tuning)은 아직 RL 부분이 아니지만 주로 전제 조건입니다. 그런 다음 RLHF는 근접 정책 최적화(Proximal Policy Optimization, PPO)라는 알고리즘을 사용하여 LLM을 추가로 정렬(align)합니다. (PPO 대신 사용할 수 있는 다른 알고리즘도 있지만, PPO가 RLHF에서 원래 사용되었고 오늘날에도 가장 인기 있는 알고리즘이기 때문에 특별히 PPO를 언급했습니다.)

간단히 말해, 우리는 RLHF 파이프라인(pipeline)을 세 가지 개별 단계로 살펴보겠습니다:
*   RLHF 1단계 (전제 조건): 사전 학습(pre-trained)된 모델의 지도 미세 조정(Supervised fine-tuning, SFT)
*   RLHF 2단계: 보상 모델(reward model) 생성
*   RLHF 3단계: 근접 정책 최적화(Proximal Policy Optimization, PPO)를 통한 미세 조정(fine-tuning)

아래에 표시된 RLHF 1단계는 추가 RLHF 미세 조정(fine-tuning)을 위한 기본 모델(base model)을 생성하는 지도 미세 조정(supervised fine-tuning) 단계입니다.

InstructGPT 논문의 주석이 달린 그림, https://arxiv.org/abs/2203.02155

RLHF 1단계에서는 프롬프트(prompt)를 생성하거나 샘플링(sampling)하고(예: 데이터베이스(database)에서), 사람들에게 양질의 응답을 작성하도록 요청합니다. 그런 다음 이 데이터셋(dataset)을 사용하여 사전 학습(pre-trained)된 기본 모델(base model)을 지도 방식(supervised fashion)으로 미세 조정(fine-tune)합니다. 앞서 언급했듯이, 이는 기술적으로 RL 훈련의 일부가 아니라 단순한 전제 조건입니다.

RLHF 2단계에서는 아래에 표시된 대로 지도 미세 조정(SFT)에서 얻은 이 모델을 사용하여 보상 모델(reward model)을 생성합니다.

InstructGPT 논문의 주석이 달린 그림, https://arxiv.org/abs/2203.02155

위 그림에 묘사된 바와 같이, 각 프롬프트(prompt)에 대해 이전 단계에서 생성된 미세 조정(fine-tuned)된 LLM으로부터 네 가지 응답을 생성합니다. 그런 다음 인간 주석자(annotator)가 자신의 선호도에 따라 이 응답들의 순위를 매깁니다. 이 순위 매기기 과정은 시간이 많이 걸리지만, 지도 미세 조정(supervised fine-tuning)을 위한 데이터셋(dataset)을 생성하는 것보다 노동 집약적(labor-intensive)이지 않을 수 있습니다. 이는 응답 순위를 매기는 것이 작성하는 것보다 더 간단할 가능성이 높기 때문입니다. 이러한 순위가 포함된 데이터셋(dataset)을 컴파일(compile)한 후, RLHF 3단계의 후속 최적화(optimization) 단계에 대한 보상 점수(reward score)를 출력하는 보상 모델(reward model)을 설계할 수 있습니다. 여기서 아이디어는 보상 모델(reward model)이 노동 집약적인 인간 순위 매기기를 대체하고 자동화하여 대규모 데이터셋(dataset)에서 훈련을 가능하게 한다는 것입니다. 이 보상 모델(reward model, RM)은 일반적으로 이전 지도 미세 조정(SFT) 단계에서 생성된 LLM에서 파생됩니다. RLHF 1단계의 모델을 보상 모델(reward model)로 전환하기 위해, 해당 출력 계층(output layer) (다음 토큰 분류 계층(next-token classification layer))은 단일 출력 노드(output node)를 특징으로 하는 회귀 계층(regression layer)으로 대체됩니다.

RLHF 파이프라인(pipeline)의 세 번째 단계는 보상 모델(RM)을 사용하여 지도 미세 조정(SFT)에서 얻은 이전 모델을 미세 조정(fine-tune)하는 것입니다. 이는 아래 그림에 나와 있습니다.

InstructGPT 논문의 주석이 달린 그림, https://arxiv.org/abs/2203.02155

RLHF 3단계인 최종 단계에서는 RLHF 2단계에서 생성한 보상 모델(reward model)의 보상 점수(reward score)를 기반으로 근접 정책 최적화(Proximal Policy Optimization, PPO)를 사용하여 SFT 모델을 업데이트(update)하고 있습니다.

### PPO: 강화 학습의 핵심 알고리즘 상세 분석

앞서 언급했듯이, 원래의 RLHF 방법은 근접 정책 최적화(Proximal Policy Optimization, PPO)라는 강화 학습(reinforcement learning) 알고리즘을 사용합니다. PPO는 정책(policy) 훈련의 안정성과 효율성을 개선하기 위해 개발되었습니다. (강화 학습(reinforcement learning)에서 "정책(policy)"은 우리가 훈련하고자 하는 모델을 의미합니다; 이 경우 정책(policy) = LLM입니다.)

PPO의 핵심 아이디어 중 하나는 각 업데이트(update) 단계에서 정책(policy)이 변경될 수 있는 정도를 제한한다는 것입니다. 이는 클리핑된 손실 함수(clipped loss function)를 사용하여 수행되며, 이는 모델이 훈련을 불안정하게 만들 수 있는 지나치게 큰 업데이트(update)를 하는 것을 방지하는 데 도움이 됩니다. 그 외에도 PPO는 손실(loss)에 KL 발산(KL divergence) 페널티(penalty)도 포함합니다. 이 항은 현재 정책(policy) (훈련 중인 모델)을 원래 SFT 모델과 비교합니다. 이는 업데이트(update)가 합리적으로 가까이 유지되도록 장려합니다. 결국 모델을 완전히 재훈련(re-train)하는 것이 아니라 선호도에 맞춰 조정(preference-tune)하는 것이 아이디어입니다. 이것이 근접 정책 최적화(proximal policy optimization)의 "근접(proximal)"이 유래한 곳입니다: 이 알고리즘은 개선을 허용하면서도 업데이트(update)를 기존 모델에 가깝게 유지하려고 노력합니다. 그리고 약간의 탐색(exploration)을 장려하기 위해 PPO는 엔트로피(entropy) 보너스(bonus)도 추가하는데, 이는 훈련 중에 모델이 출력을 다양하게 만들도록 장려합니다.

다음 단락에서는 PPO를 비교적 높은 수준에서 설명하기 위해 몇 가지 용어를 더 소개하고자 합니다. 여전히 많은 전문 용어가 포함되어 있으므로 계속하기 전에 아래 그림에 핵심 용어를 요약하려고 노력했습니다.

RLHF의 핵심 용어 그림. 예를 들어, PPO에는 여러 모델이 관련되어 있으며, PPO는 RLHF에서 사용되는 알고리즘입니다 (그리고 RLHF는 가장 인기 있는 LLM 정렬(alignment) 방법 중 하나입니다).

아래에서는 의사 코드(pseudo-code)를 통해 PPO의 핵심 단계를 설명하고자 합니다. 또한, 더 직관적으로 만들기 위해 비유도 사용하겠습니다:

당신이 작은 음식 배달 서비스를 운영하는 요리사라고 상상해 보세요. 그리고 고객 만족도를 높이기 위해 끊임없이 새로운 레시피(recipe) 변형을 시도하고 있습니다. 당신의 전반적인 목표는 고객 피드백(reward)을 기반으로 레시피(정책(policy))를 조정하는 것입니다.

1.  새로운 정책(policy)과 이전 정책(policy)의 다음 토큰 확률(next-token probability) 비율을 계산합니다:
    `ratio = new_policy_prob / old_policy_prob`
    간단히 말해, 이것은 우리의 새로운 레시피(recipe)가 이전 레시피(recipe)와 얼마나 다른지 확인합니다.
    참고: "new_policy_prob"에 관해서는 아직 최종 업데이트(update)된 정책(policy)을 사용하고 있지 않습니다. 우리는 정책(policy)의 현재 버전(즉, 훈련 중인 모델)을 사용하고 있습니다. 그러나 이를 "새로운(new)"이라고 부르는 것이 관례입니다. 따라서 아직 실험 중이더라도 관례에 따라 현재 초안을 "새로운 정책(new policy)"이라고 부릅니다.

2.  해당 비율에 행동이 얼마나 좋았는지(이점(advantage)이라고 함)를 곱합니다:
    `raw_score = ratio * advantage`
    여기서는 간단하게, 이점(advantage)이 보상 신호(reward signal)를 기반으로 계산된다고 가정할 수 있습니다:
    `advantage = actual_reward - expected_reward`
    요리사 비유에서, 우리는 이점(advantage)을 새로운 요리가 얼마나 잘 수행되었는지로 생각할 수 있습니다:
    `advantage = customer_rating - expected_rating`
    예를 들어, 고객이 새로운 요리를 9/10로 평가하고, 고객들이 일반적으로 7/10을 준다면, 이는 +2의 이점(advantage)입니다.
    이는 단순화된 것입니다. 실제로는 일반화된 이점 추정(generalized advantage estimation, GAE)이 포함되는데, 이 글을 더 부풀리지 않기 위해 여기서는 생략합니다. 그러나 언급할 중요한 세부 사항은 예상 보상(expected reward)은 소위 "비평가(critic)" (때로는 "가치 모델(value model)"이라고도 함)에 의해 계산되고, 보상 모델(reward model)이 실제 보상(actual reward)을 계산한다는 것입니다. 즉, 이점 계산(advantage computation)에는 일반적으로 우리가 미세 조정(fine-tuning)하는 원래 모델과 동일한 크기의 다른 두 모델이 포함됩니다.
    비유에서, 우리는 이 비평가(critic) 또는 가치 모델(value model)을 고객에게 제공하기 전에 우리의 새로운 요리를 맛보도록 요청하는 친구로 생각할 수 있습니다. 우리는 또한 친구에게 고객이 그것을 어떻게 평가할지 추정해달라고 요청합니다 (이것이 예상 보상(expected reward)입니다). 보상 모델(reward model)은 피드백(feedback)을 제공하는 실제 고객(즉, 실제 보상(actual reward))입니다.

3.  클리핑된 점수(clipped score)를 계산합니다:
    새로운 정책(policy)이 너무 많이 변경되면(예: ratio > 1.2 또는 < 0.8), 다음과 같이 비율을 클리핑(clip)합니다:
    `clipped_ratio = clamp(ratio, 0.8, 1.2)`
    `clipped_score = clipped_ratio * advantage`
    비유에서, 새로운 레시피(recipe)가 예외적으로 훌륭하거나(또는 나쁜) 평가를 받았다고 상상해 보세요. 우리는 이제 전체 메뉴를 개편하고 싶은 유혹을 느낄 수 있습니다. 하지만 그것은 위험합니다. 그래서 대신, 우리는 레시피(recipe)가 현재 얼마나 변경될 수 있는지 클리핑(clip)합니다. (예를 들어, 우리가 요리를 훨씬 더 맵게 만들었고, 그 한 고객이 매운 음식을 좋아했지만, 그렇다고 모든 사람이 그럴 것이라는 의미는 아닙니다.)

4.  그런 다음 원시 점수(raw score)와 클리핑된 점수(clipped score) 중 더 작은 것을 사용합니다:
    ```
    if advantage >= 0:
        final_score = min(raw_score, clipped_score)
    else:
        final_score = max(raw_score, clipped_score)
    ```
    다시 말하지만, 이것은 약간 조심하는 것과 관련이 있습니다. 예를 들어, 이점(advantage)이 양수이면(새로운 행동이 더 좋으면), 우리는 보상(reward)을 제한합니다. 이는 우연이나 운일 수 있는 좋은 결과를 과도하게 신뢰하고 싶지 않기 때문입니다. 이점(advantage)이 음수이면(새로운 행동이 더 나쁘면), 우리는 페널티(penalty)를 제한합니다. 여기서 아이디어는 비슷합니다. 즉, 우리가 정말 확신하지 않는 한 하나의 나쁜 결과에 과잉 반응하고 싶지 않습니다. 요컨대, 이점(advantage)이 양수이면(과도한 보상(over-rewarding)을 피하기 위해) 두 점수 중 더 작은 것을 사용하고, 이점(advantage)이 음수이면(과도한 페널티(over-penalizing)를 피하기 위해) 더 큰 것을 사용합니다.
    비유에서, 이것은 레시피(recipe)가 예상보다 잘 수행되고 있다면, 우리가 확신하지 않는 한 과도하게 보상(over-reward)하지 않도록 보장합니다. 그리고 실적이 저조하다면, 지속적으로 나쁘지 않는 한 과도하게 페널티(over-penalize)를 주지 않습니다.

5.  손실(loss) 계산:
    이 최종 점수는 훈련 중에 우리가 최대화하는 것입니다 (점수의 부호를 뒤집어 최소화한 후 경사 하강법(gradient descent)을 사용). 또한, 페널티(penalty) 강도를 위한 하이퍼파라미터(hyperparameter)인 β가 있는 KL 페널티 항(KL penalty term)도 추가합니다:
    `loss = -final_score + β * KL(new_policy || reference_policy)`
    비유에서, 우리는 새로운 레시피(recipe)가 원래 스타일과 너무 다르지 않도록 페널티(penalty)를 추가합니다. 이것은 매주 "주방을 재창조"하는 것을 방지합니다. 예를 들어, 우리는 이탈리아 레스토랑을 갑자기 바비큐(BBQ) 장소로 바꾸고 싶지 않습니다.

많은 정보였으므로, 아래 그림을 통해 LLM 맥락에서 구체적인 수치 예시로 요약했습니다. 하지만 너무 복잡하다면 건너뛰셔도 좋습니다. 나머지 글을 이해하는 데는 문제가 없을 것입니다. PPO 설명이 과했을 수도 있다는 점을 인정합니다. 하지만 일단 작성하고 나니 삭제하기 어려웠습니다. 몇몇 분들에게는 유용하기를 바랍니다!

PPO는 LLM 정렬에 효과적인 알고리즘이지만, 여러 개의 모델(정책, 보상, 비평가, 참조 모델)을 동시에 관리해야 하므로 계산 비용이 많이 들고 복잡성이 높다는 단점이 있습니다. 특히 대규모 LLM을 훈련할 때는 이러한 복잡성이 병목 현상이 될 수 있습니다. 이는 후속 섹션에서 다룰 GRPO와 같은 PPO 변형의 개발을 촉진하는 주요 요인이 됩니다.

### RL 알고리즘의 진화: PPO를 넘어

PPO는 RLHF의 초기 성공을 이끌었지만, LLM의 규모가 커지고 훈련 시나리오가 다양해지면서 PPO의 계산 효율성과 안정성에 대한 의문이 제기되었습니다. 특히 여러 모델(정책, 보상 모델, 비평가)을 동시에 훈련하고 관리하는 오버헤드는 대규모 LLM 훈련에 상당한 부담이 됩니다.

이러한 배경에서 DeepSeek 팀은 DeepSeekMath 논문에서 **그룹 상대 정책 최적화(Group Relative Policy Optimization, GRPO)**라는 PPO의 변형을 도입했습니다. GRPO의 핵심 혁신은 PPO에서 사용되는 "비평가(critic)" 또는 "가치 모델(value model)"을 제거하여 계산 효율성을 극대화하는 것입니다. PPO에서 비평가는 예상 보상을 예측하여 이점(advantage) 계산에 기여하지만, GRPO는 정책 모델 자체에서 여러 답변을 샘플링하고 이들의 상대적 품질을 사용하여 이점을 직접 계산합니다. 이는 모델 수를 줄여 메모리 사용량을 최적화하고 훈련 파이프라인을 단순화합니다.

PPO와 GRPO의 차이점을 설명하기 위해 DeepSeekMath 논문에서 좋은 그림을 빌려왔습니다:

DeepSeekMath: Open Language Models에서 수학적 추론의 한계 확장(https://arxiv.org/abs/2402.03300)의 주석이 달린 그림으로 PPO와 GRPO의 차이점을 설명합니다.

GRPO 외에도 최근 LLM을 위한 RL 알고리즘은 다양하게 진화하고 있습니다. 예를 들어, **KTO (Kahneman-Tversky Optimization)**는 인간의 선호도와 비선형적인 보상 함수를 보다 잘 모델링하기 위해 개발되었으며, **RSO (Ranked-SFT Optimization)**는 순위 기반 피드백을 직접 사용하여 모델을 최적화하여 보상 모델의 필요성을 줄이는 시도입니다. 이러한 알고리즘들은 PPO의 기본 원칙을 유지하거나 변형하면서도, LLM의 특정 특성(예: 긴 시퀀스 생성, 다양한 출력 스타일)을 더 잘 처리하고 훈련 프로세스의 효율성과 안정성을 개선하는 데 중점을 둡니다.

### 보상 모델링의 패러다임 전환: RLHF에서 RLVR, 그리고 그 너머로

지금까지 우리는 RLHF를 절차로 살펴보았고, 이를 위해 일반적으로 사용되는 두 가지 강화 학습(reinforcement learning) 알고리즘인 PPO와 GRPO를 소개했습니다. 하지만 RLHF가 이미 LLM 정렬(alignment) 툴킷(toolkit)의 핵심 부분이라면, 이 모든 것이 추론(reasoning)과 무슨 관련이 있을까요?

RLHF와 추론(reasoning)의 연결은 DeepSeek 팀이 R1 및 R1-Zero 모델의 추론 능력(reasoning capability)을 훈련하기 위해 유사한 RL 기반 접근 방식(GRPO와 함께)을 어떻게 적용했는지에서 비롯됩니다. 차이점은 인간의 선호도에 의존하고 보상 모델(reward model)을 훈련하는 대신, DeepSeek-R1 팀은 **검증 가능한 보상(verifiable rewards)**을 사용했다는 것입니다. 이 접근 방식을 **검증 가능한 보상 기반 강화 학습(reinforcement learning with verifiable rewards, RLVR)**이라고 합니다.

다시 한번 강조할 가치가 있습니다: 표준 RLHF와 달리 RLVR은 보상 모델(reward model)의 필요성을 우회합니다. 따라서 인간이 라벨링(labeling)한 예시로부터 "좋은" 답변이 무엇인지 배우는 대신, 모델은 기호 검증기(symbolic verifier)나 규칙 기반 도구(rule-based tool)와 같은 결정론적 도구로부터 직접적인 이진 피드백(binary feedback) (정답 또는 오답)을 받습니다. 수학 문제의 계산기나 코드 생성의 컴파일러(compiler)를 생각해 보세요.

검증 가능한 보상 기반 강화 학습(RLVR)의 예시. 모델은 수학 문제를 풀도록 프롬프트(prompt)되고 답변을 생성합니다. 학습된 보상 모델(reward model)을 사용하는 대신, 기호 검증기(symbolic verifier) (예: 계산기)가 출력을 확인하고 정확성(correctness)을 기반으로 이진 피드백(binary feedback)을 제공합니다.

여기서 한 가지 동기는 RL 동안 자동 정확성 검사(correctness check)를 감독 신호(supervision signal)로 사용하여 시끄럽거나 비용이 많이 드는 인간 또는 학습된 보상(reward)을 피하는 것입니다. 다른 동기는 계산기와 같은 "저렴한" 도구를 사용하여 비용이 많이 드는 보상 모델(reward model) 훈련과 보상 모델(reward model) 자체를 대체할 수 있다는 것입니다. 보상 모델(reward model)은 일반적으로 전체 사전 학습(pre-trained)된 모델(회귀 헤드(regression head)가 있지만)이므로 RLVR은 훨씬 더 효율적입니다.

RLVR은 특히 수학, 코딩, 논리 퍼즐과 같이 명확한 정답과 검증 규칙이 존재하는 도메인에서 강력한 성능을 발휘합니다. 그러나 그 한계 또한 명확합니다. 인간의 주관적인 판단이나 복잡한 윤리적 고려가 필요한 질문에 대해서는 순수한 RLVR을 적용하기 어렵습니다. 이러한 경우를 위해 **하이브리드 보상 모델링(Hybrid Reward Modeling)** 접근 방식이 모색되고 있습니다. 이는 RLHF의 인간 피드백과 RLVR의 자동 검증을 결합하거나, **RLAIF (Reinforcement Learning from AI Feedback)**를 통해 다른 LLM이 생성한 피드백을 활용하는 방식입니다. 궁극적으로 보상 모델링의 목표는 모델이 다양한 유형의 문제에 대해 안정적이고 신뢰할 수 있는 추론을 수행하도록 유도하는 것입니다.

### DeepSeek-R1 훈련 방식 심층 분석

이제 RLHF와 RLVR, 그리고 PPO와 GRPO가 무엇인지 명확히 했으니, RL과 추론(reasoning)의 맥락에서 DeepSeek-R1 논문의 주요 통찰력을 간략하게 요약해 봅시다. 먼저, 세 가지 유형의 모델이 있었습니다:

*   순수 강화 학습(pure RL)으로 훈련된 DeepSeek-R1-Zero
*   명령어 미세 조정(instruction fine-tuning, SFT) 및 RL로 훈련된 DeepSeek-R1
*   명령어 미세 조정(instruction fine-tuning) SFT를 통해 생성된 DeepSeek-Distill 변형(variant) (RL 없음)

아래에 표시된 대로, 이 모델들이 서로 어떻게 관련되어 있는지 설명하기 위해 DeepSeek-R1 파이프라인(pipeline) 다이어그램을 만들었습니다.

DeepSeek-R1 제품군의 훈련 파이프라인(pipeline)

DeepSeek-R1-Zero는 GRPO와 함께 검증 가능한 보상(verifiable rewards, RLVR)을 사용하여 훈련되었으며, 이는 모델이 중간 단계(intermediate-step) 생성을 통해 추론 능력(reasoning ability)을 나타내는 데 충분한 것으로 밝혀졌습니다. 이는 SFT 단계를 건너뛸 수 있음을 보여주었습니다. 이 모델은 예시로부터 배우는 대신 탐색(exploration)을 통해 추론 능력(reasoning ability)을 향상시킵니다.

DeepSeek-R1은 최고의 성능을 가진 플래그십 모델(flagship model)입니다. DeepSeek-R1-Zero와의 차이점은 명령어 미세 조정(instruction fine-tuning), RLVR, RLHF를 번갈아 수행했다는 것입니다.

DeepSeek-Distill 변형(variant)은 작고 더 쉽게 배포할 수 있는 모델을 의미합니다; 이들은 DeepSeek-R1 모델의 명령어 데이터(instruction data)를 사용하여 Llama 3 및 Qwen 2.5 모델을 명령어 미세 조정(instruction fine-tuning)하여 생성되었습니다. 이 접근 방식은 추론(reasoning) 부분에 RL을 사용하지 않았습니다 (그러나 Llama 3 및 Qwen 2.5 기본 모델(base model)을 생성하는 데 RLHF가 사용되었습니다).

DeepSeek 팀은 DeepSeek-R1-Zero를 훈련하기 위해 LLM 기반 보상 모델(reward model)을 사용하지 않았다는 점이 핵심입니다. 대신, 그들은 DeepSeek-R1-Zero 및 DeepSeek-R1의 추론 훈련을 위해 규칙 기반 보상(rule-based reward)을 사용했습니다:

우리는 DeepSeek-R1-Zero 개발에 결과 또는 프로세스 신경 보상 모델(neural reward model)을 적용하지 않습니다. 왜냐하면 신경 보상 모델(neural reward model)이 대규모 강화 학습(reinforcement learning) 과정에서 보상 해킹(reward hacking)으로 고통받을 수 있다는 것을 발견했기 때문입니다 [...] DeepSeek-R1-Zero를 훈련하기 위해 우리는 주로 두 가지 유형의 보상(reward)으로 구성된 규칙 기반 보상 시스템(rule-based reward system)을 채택합니다:
(1) **정확성 보상(Accuracy rewards)**: 정확성 보상 모델(accuracy reward model)은 응답이 올바른지 평가합니다. 예를 들어, 결정론적 결과가 있는 수학 문제의 경우, 모델은 지정된 형식(예: 상자 안)으로 최종 답변을 제공해야 하며, 이는 정확성(correctness)에 대한 신뢰할 수 있는 규칙 기반 검증을 가능하게 합니다. 마찬가지로 LeetCode 문제의 경우, 컴파일러(compiler)를 사용하여 미리 정의된 테스트 케이스(test case)를 기반으로 피드백(feedback)을 생성할 수 있습니다.
(2) **형식 보상(Format rewards)**: 정확성 보상 모델(accuracy reward model) 외에도, 우리는 모델이 사고 과정(thinking process)을 '<think>'와 '</think>' 태그(tag) 사이에 넣도록 강제하는 형식 보상 모델(format reward model)을 사용합니다.

DeepSeek-R1의 훈련 전략은 SFT와 RL을 번갈아 사용하는 독특한 접근 방식을 보여줍니다. 이는 SFT를 통해 기본적인 지식과 지시를 학습하고, RL을 통해 추론 능력을 정교하게 다듬는 것을 목표로 합니다. 이러한 하이브리드 접근 방식은 모델이 초기에는 안정적인 행동을 학습하고, 이후에는 검증 가능한 보상을 통해 복잡한 추론 문제를 해결하는 능력을 강화하는 데 도움을 줍니다. 또한, DeepSeek-Distill 변형은 대규모 모델의 추론 능력을 더 작고 효율적인 모델로 전이시키는 증류(distillation)의 중요성을 강조하며, 이는 실제 배포 환경에서 비용 효율적인 추론 모델을 구축하는 데 중요한 역할을 합니다.

### 최근 RL 연구에서 얻은 추론 모델 훈련의 주요 통찰력

지난 몇 달간 수많은 추론 모델(reasoning model) 관련 연구가 쏟아져 나왔습니다. 이 섹션에서는 이러한 최신 연구에서 얻은 가장 흥미로운 아이디어와 통찰력을 요약했습니다.

#### 1. 증류(Distillation)와 RL의 시너지 효과

초기 DeepSeek-R1 논문은 지도 미세 조정(SFT) 후 강화 학습(RL)을 적용하는 것이 RL 단독보다 더 나은 결과를 낸다는 것을 보여주었습니다. 이는 증류(distillation)된 모델(더 큰 모델이 생성한 추론 예시를 통해 SFT로 훈련된 모델)에도 추가 RL이 상당한 개선을 가져올 수 있음을 시사합니다. 최근 연구들은 이러한 현상을 더욱 명확히 밝혔습니다. 예를 들어, 1.5B 규모의 증류 모델에 단 7,000개의 예시와 42달러의 적은 컴퓨팅 자원만으로도 RL 미세 조정(fine-tuning)을 통해 AIME24 수학 벤치마크에서 OpenAI의 o1-preview를 능가하는 성능 향상을 달성했습니다. 이는 소규모 모델에서도 RL이 효과적인 추론 능력 향상 도구가 될 수 있음을 보여줍니다. 그러나 일부 연구에서는 이러한 이득이 항상 통계적으로 유의미하지 않을 수 있으며, 벤치마크 결과가 때로는 개선 사항을 과장할 수 있다고 경고하기도 합니다. 이는 RL 기반 훈련의 미묘한 특성과 평가 방법론의 중요성을 강조합니다.

#### 2. 추론 길이 및 품질 제어

추론 모델은 사고의 연쇄(CoT)로 인해 종종 긴 응답을 생성하며, 이는 컴퓨팅 비용을 증가시키고 때로는 불필요하게 복잡한 답변으로 이어질 수 있습니다. 흥미롭게도, 일부 연구는 PPO와 같은 RL 알고리즘이 특정 조건에서 불필요하게 긴 응답을 선호하는 경향이 있음을 발견했습니다. 이는 모델이 음수 보상을 받을 때, 더 긴 응답이 토큰당 평균 손실을 희석시켜 낮은 손실 값을 유도하기 때문일 수 있습니다. 이러한 문제를 해결하기 위해 다양한 길이 제어 전략이 연구되고 있습니다.
*   **명시적 길이 제어:** L1과 같은 모델은 길이 제어 정책 최적화(LCPO)를 통해 사용자가 지정한 길이 제약에 맞춰 응답 길이를 조절할 수 있도록 합니다. 이는 정확성을 유지하면서도 컴퓨팅 효율성을 높이는 데 기여합니다.
*   **토큰 수준 보상:** 중간 추론 단계의 중요도에 따라 토큰 수준 보상을 적용하여 모델이 핵심적인 추론 단계에 더 집중하고 불필요한 설명을 줄이도록 유도합니다.
*   **과도한 길이 페널티:** 특정 길이를 초과하는 응답에 명시적으로 페널티를 부과하거나, 잘못된 긴 답변에 더 큰 페널티를 주어 간결하고 정확한 추론을 장려합니다.

이러한 접근 방식은 모델이 단순히 길게 답변하는 것을 넘어, 효율적이고 효과적인 추론 경로를 학습하도록 돕습니다.

#### 3. 발현적 능력과 자기 수정(Self-Correction)

DeepSeek-R1 논문에서 언급된 "아하 모멘트(AHA moment)"처럼, RL은 모델에서 귀중한 자기 검증(self-verification) 및 반성적 추론(reflective reasoning) 능력을 유도하는 것으로 나타났습니다. 이러한 능력은 명시적인 지시 없이 훈련 중에 자연스럽게 발현되며, 모델이 자신의 실수를 인식하고 수정하는 데 도움을 줍니다.
*   최근 연구에서는 컨텍스트 길이(context length)를 확장하는 것이 모델의 자기 성찰 및 자기 수정 능력을 더욱 향상시킨다는 것을 보여주었습니다. 이는 모델이 더 많은 정보를 고려하고, 과거의 추론 과정을 돌아보며 오류를 찾아내고 수정할 수 있는 기회를 제공합니다.
*   경쟁 프로그래밍과 같은 복잡한 문제 해결 작업에서, RL 훈련을 받은 모델은 스스로 문제를 해결하기 위한 "무차별 대입(brute-force)" 전략을 먼저 시도한 다음, 이를 사용하여 더 최적화된 솔루션을 검증하는 등 인간과 유사한 전략을 학습하는 능력을 보여주었습니다. 이는 RL이 모델에게 단순한 패턴 매칭을 넘어선 복잡한 문제 해결 패러다임을 부여할 수 있음을 시사합니다.

#### 4. 도메인 일반화 및 외부 도구 활용

초기 추론 연구는 수학이나 코딩과 같이 명확한 규칙과 검증이 가능한 도메인에 집중되었습니다. 그러나 최근 연구들은 RL 기반 추론 모델이 논리 퍼즐과 같은 다른 도메인에서도 성공적으로 일반화될 수 있음을 보여주었습니다. 이는 RL이 특정 도메인 지식에 독립적으로 일반적인 추론 행동을 유도하는 능력이 있음을 의미합니다.

더 나아가, 추론 능력은 수학, 코드와 같은 구조화된 도메인을 넘어 의학, 화학, 심리학, 경제학과 같은 자유 형식의 답변이 필요한 분야로 확장되고 있습니다. 이는 생성적 소프트 스코어링(generative soft-scoring) 방법과 같은 기술을 활용하여 다양한 도메인에서 검증 가능한 보상을 생성함으로써 가능해집니다.

또한, LLM의 추론 능력을 극대화하기 위해 외부 도구(external tool use) 및 검색 증강 생성(retrieval-augmented generation, RAG)과의 통합이 중요해지고 있습니다. DeepSeek-R1과 같은 모델은 내부 지식에 의존하지만, RAG나 외부 검색 시스템을 활용하면 시간에 민감하거나 최신 정보가 필요한 지식 기반 작업에서 성능을 크게 향상시킬 수 있습니다. 모델이 추론 과정에서 언제 어떻게 외부 도구를 호출하고 검색된 정보를 활용할지 학습하는 것은 현실 세계의 복잡한 문제를 해결하는 데 필수적인 요소가 되고 있습니다. OpenAI의 o3 모델과 같은 최신 모델들은 이러한 도구 사용 능력을 추론 파이프라인에 통합하는 선구적인 역할을 하고 있습니다.

#### 5. RL과 사전 학습(Pre-training)의 상호작용

DeepSeek-R1-Zero의 사례는 순수 RL이 기본 모델의 추론 능력을 유도할 수 있음을 보여주었지만, 최근 연구들은 추론 행동이 전적으로 RL 때문만은 아닐 수 있다는 비판적 관점을 제시합니다. 일부 기본 모델(예: Qwen2.5)은 RL 없이도 강력한 추론 능력과 "아하 모멘트"와 유사한 행동을 이미 보여주는 것으로 나타났습니다. 이는 사전 학습(pre-training) 과정, 특히 대규모의 사고의 연쇄(CoT) 데이터로 훈련될 때, 모델이 이미 추론의 기초를 다질 수 있음을 시사합니다.

실제로, 자기 수정(self-correction)과 같은 반성적 행동은 사전 학습 전반에 걸쳐 점진적으로 나타나며, 사전 학습 컴퓨팅 자원이 증가함에 따라 그 능력이 강화된다는 연구 결과도 있습니다. 따라서 RL은 기존의 추론 능력을 미세 조정하고 특정 작업에 특화시키는 데 중요한 역할을 하지만, 추론 능력의 근본적인 발현은 사전 학습 단계에서부터 시작될 수 있다는 것이 현재의 중론입니다. 증류(distillation) 또한 추론 능력을 개선하는 효과적인 방법이며, 이는 CoT 데이터에 대한 명령어 미세 조정을 통해 이루어집니다. 궁극적으로, 사전 학습, 지도 미세 조정, 그리고 강화 학습은 상호 보완적인 방식으로 LLM의 추론 능력을 개발하고 향상시키는 데 기여합니다.

### 추론 모델 훈련에 관한 최신 연구 동향

지난 몇 달간 LLM의 추론 능력 향상을 위한 강화 학습(RL) 연구는 폭발적으로 증가했으며, 다음과 같은 최신 트렌드와 연구 방향이 주목받고 있습니다.

#### 1. 멀티모달 추론의 강화 학습 (RL for Multimodal Reasoning)
최근 연구들은 텍스트뿐만 아니라 이미지, 비디오 등 다양한 양식(modality)을 통합하는 멀티모달 LLM에 RL을 적용하여 추론 능력을 향상시키는 데 집중하고 있습니다. 예를 들어, 시각적 질문 응답(Visual Question Answering)이나 멀티모달 대화 시스템에서 모델이 더 정확하고 맥락에 맞는 추론을 수행하도록 RL을 통해 훈련됩니다. 이는 단순한 정보 추출을 넘어, 복잡한 시각적 단서와 텍스트 정보를 결합하여 추상적인 개념을 추론하는 능력을 개발하는 데 기여합니다.

#### 2. 자율 에이전트와 추론 (Reasoning in Autonomous Agents)
LLM을 기반으로 한 자율 에이전트의 개발이 가속화되면서, 에이전트가 복잡한 환경에서 목표를 달성하기 위해 추론 능력을 활용하는 방식에 RL이 적용되고 있습니다. 이는 계획(planning), 환경 탐색, 그리고 불확실성 속에서 의사 결정을 내리는 데 필수적인 추론 과정을 RL을 통해 학습시키는 것을 포함합니다. 에이전트는 시행착오를 통해 최적의 추론 전략을 발견하고, 장기적인 목표를 달성하기 위한 다단계 추론 능력을 향상시킵니다.

#### 3. RL 기반으로 LLM의 편향 및 안전성 개선 (Mitigating Bias and Enhancing Safety with RL)
추론 모델의 성능 향상과 더불어, 모델의 출력에서 나타날 수 있는 편향(bias)을 줄이고 안전성을 확보하는 것이 중요한 과제로 부상하고 있습니다. RL은 LLM이 유해하거나 편향된 콘텐츠를 생성하는 것을 방지하고, 윤리적 기준에 부합하는 추론을 수행하도록 정렬(alignment)하는 데 활용됩니다. 이를 위해 인간 피드백 또는 AI 피드백을 통해 안전성 관련 보상을 설계하고, 모델이 바람직한 행동을 학습하도록 유도하는 연구가 활발히 진행 중입니다.

#### 4. 효율적인 RL 훈련 기법 (Efficient RL Training Techniques)
대규모 LLM에 RL을 적용하는 것은 여전히 계산 비용이 높고 복잡합니다. 따라서 훈련 효율성을 높이기 위한 새로운 RL 기법들이 연구되고 있습니다. DPO의 확장 버전, KL 정규화의 최적화, 그리고 소규모 데이터셋으로도 효과적인 RL을 가능하게 하는 샘플 효율적인 알고리즘 개발이 그 예입니다. 또한, 분산 훈련(distributed training) 및 양자화(quantization)와 같은 기술을 RL 파이프라인에 통합하여 대규모 모델의 RL 훈련을 더욱 실용적으로 만드는 연구도 중요하게 다루어지고 있습니다.

#### 5. 이론적 이해 및 분석 (Theoretical Understanding and Analysis)
실험적인 성능 향상과 더불어, RL이 LLM의 추론 능력을 어떻게 유도하는지에 대한 이론적 이해를 심화하는 연구도 중요해지고 있습니다. PPO의 길이 편향 문제와 같은 현상을 수학적으로 분석하고, RL 알고리즘의 특정 구성 요소가 모델의 행동에 미치는 영향을 규명하는 것은 더욱 견고하고 예측 가능한 추론 모델을 구축하는 데 필수적입니다. 이는 단순히 "무엇이 작동하는가"를 넘어 "왜 작동하는가"에 대한 근본적인 질문에 답하려는 노력입니다.

이러한 최신 연구 동향은 LLM의 추론 능력이 단순한 자연어 이해를 넘어, 복잡한 문제 해결과 지능적인 에이전트 행동의 핵심 요소로 자리매김하고 있음을 보여줍니다. RL은 이러한 발전을 이끄는 강력한 도구로 계속해서 진화할 것입니다.

이 잡지는 개인적인 열정 프로젝트입니다. 독립 연구자로서 저를 지원하려면 제 책 "대규모 언어 모델 구축(처음부터)(Build a Large Language Model (From Scratch))"을 구매하거나 유료 구독을 신청하는 것을 고려해 주십시오.

대규모 언어 모델 구축(처음부터)(Build a Large Language Model (From Scratch)) 지금 Amazon에서 구매 가능합니다

책을 읽으셨고 몇 분의 시간이 있으시다면, 짧은 리뷰를 남겨주시면 정말 감사하겠습니다. 이는 저희 저자들에게 큰 도움이 됩니다! 여러분의 지원은 큰 의미가 있습니다! 감사합니다!