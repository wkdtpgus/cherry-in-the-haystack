이번 달, 인공지능 분야는 GPT-4.5나 Llama 4와 같은 주요 언어 모델들의 공개로 큰 주목을 받았습니다. 하지만 시장의 반응은 기대와 달리 다소 미미했습니다. 이러한 현상의 근본적인 원인은 무엇일까요? 한 가지 분석은 해당 모델들이 여전히 기존의 훈련 패러다임을 따르고 있으며, 특히 복잡한 사고 과정을 위한 명시적인 강화 학습 기법을 통합하지 않았다는 점에 있습니다. 반면, xAI나 Anthropic 같은 경쟁 주자들은 자신들의 모델에 심층적인 추론 역량과 확장된 기능을 주입하는 데 주력했습니다. 일례로, xAI의 Grok과 Anthropic의 Claude 인터페이스에서는 이제 특정 상황에서 모델의 사고 능력을 활성화하는 "심사숙고(deliberation)" 또는 "확장된 심사숙고(extended deliberation)" 기능을 제공합니다. 이러한 GPT-4.5 및 Llama 4 모델(비사고형)에 대한 냉담한 반응은 모델의 규모 확장과 데이터량 증가만으로는 인공지능이 도달할 수 있는 성능의 한계에 근접하고 있음을 암시합니다. 그러나 OpenAI가 최근 선보인 o3 추론 전용 모델은 컴퓨팅 자원을 전략적으로 배분하고, 특히 사고력 증진에 특화된 보강 학습(reinforcement learning) 방식을 적용할 경우, 여전히 혁신적인 발전이 가능하다는 것을 명확히 보여줍니다. (최근 OpenAI의 한 개발자는 라이브스트림에서 o3 모델이 o1 모델 대비 10배에 달하는 훈련 컴퓨팅 자원을 활용했다고 언급했습니다.) 출처: OpenAI 라이브스트림(https://openai.com/live/) (2025년 4월 16일) 물론, 추론 능력 그 자체가 모든 문제의 해결책은 아닙니다. 하지만 현재까지는 난이도 높은 과제에서 모델의 정확성을 높이고 문제 해결 역량을 안정적으로 개선하는 데 핵심적인 역할을 합니다. 저는 앞으로 사고력 중심의 후처리 훈련(post-training)이 대규모 언어 모델(LLM) 개발 과정에서 필수적인 요소로 자리매김할 것이라고 확신합니다. 본문에서는 강화 학습(reinforcement learning)을 활용한 추론 능력 개발의 최신 동향을 깊이 있게 다룰 것입니다. 특히, 본 글은 추론 모델을 개발하고 그 성능을 고도화하는 데 사용되는 강화 학습 기반의 훈련 방법론에 초점을 맞춥니다. 내용이 다소 길어질 수 있으므로, 아래에 목차(Table of Contents)를 통해 주요 내용을 미리 확인하실 수 있습니다. 웹 보기(web view)에서 왼쪽 슬라이더(slider)를 사용하시면 목차를 편리하게 탐색할 수 있습니다.

*   사고 모델(reasoning model)의 본질 탐구
*   강화 학습 기반 인공지능 모델 훈련(RLHF)의 기초: 여정의 시작
*   PPO(근접 정책 최적화) 알고리즘: 강화 학습의 핵심 원리
*   강화 학습 기법의 진화: PPO에서 GRPO까지
*   강화 학습 보상 설계(reward modeling): RLHF에서 RLVR로의 전환
*   DeepSeek-R1 추론 모델의 훈련 과정 해부
*   최신 강화 학습 연구 논문에서 얻은 추론 모델 훈련의 통찰
*   주목할 만한 추론 모델 훈련 관련 연구 자료

팁: 추론의 기본 개념, 강화 학습(RL), PPO 및 GRPO에 이미 익숙하시다면, 최근 추론 연구 논문에서 도출된 흥미로운 통찰력 요약이 포함된 "최신 강화 학습 연구 논문에서 얻은 추론 모델 훈련의 통찰" 섹션으로 바로 이동하여 시간을 절약할 수 있습니다.

### 사고 모델(reasoning model)의 본질 탐구

가장 먼저 다뤄야 할 핵심은 바로 '사고(reasoning)'가 무엇인지에 대한 명확한 정의입니다. 간단히 설명하자면, 사고는 대규모 언어 모델(LLM)이 복잡한 문제를 더욱 효과적으로 해결할 수 있도록 돕는 추론 및 학습 기법의 총체입니다. 현재까지 이러한 역량이 어떻게 구현되고 있는지 좀 더 깊이 있게 들여다보기 위해, 저는 사고를 다음과 같이 정의하고자 합니다:

대규모 언어 모델(LLM) 영역에서 '사고(reasoning)'는 최종 결과물을 제시하기에 앞서, 모델이 논리적인 중간 과정을 생성해내는 능력을 의미합니다. 이 과정은 흔히 '사고의 흐름(chain-of-thought, CoT)'이라는 용어로 설명됩니다. CoT 사고에서 LLM은 특정 결론에 도달하기까지의 과정을 명시적으로 보여주는 구조화된 일련의 논리 전개나 계산 단계를 표출합니다. 이는 마치 인간이 복잡한 문제를 풀 때 머릿속으로 단계를 밟아나가는 과정과 유사합니다.

아래 그림은 이러한 정의를 시각적으로 보강합니다.

대규모 언어 모델이 여러 단계를 거쳐야 하는 추론 과제를 어떻게 처리하는지에 대한 개괄적인 설명입니다. 단순히 사실을 기억해내는 것을 넘어, 모델은 정확한 결론에 도달하기 위해 여러 개의 중간 추론 절차를 순차적으로 연결하고 통합해야 합니다. 이러한 중간 추론 절차는 구현 방식에 따라 사용자에게 직접 노출될 수도 있고, 내부적으로만 처리될 수도 있습니다. 이 과정은 모델이 단순히 정답을 '아는' 것을 넘어, 정답에 '도달하는' 방법을 이해하고 있음을 보여주는 중요한 지표입니다.

만약 사고 모델의 개념이 생소하시거나 보다 심층적인 이해를 원하신다면, 제가 이전에 작성했던 다음 글들을 참고하시기를 권합니다:

First Look at Reasoning From Scratch: Chapter 1
Sebastian Raschka, PhD · Mar 29
전체 스토리 읽기

Understanding Reasoning LLMs
Sebastian Raschka, PhD · Feb 5
전체 스토리 읽기

이 섹션의 서두에서 언급했듯이, 대규모 언어 모델의 사고 역량은 OpenAI 블로그 게시물에 소개된 그림처럼 크게 두 가지 접근 방식으로 개선될 수 있습니다. 하나는 훈련량을 늘리는 것이고, 다른 하나는 추론 단계에서의 연산 자원(test-time compute)을 활용하는 것입니다. 여기서 '추론 단계 연산 자원(test-time compute)'은 '추론 시점 연산(inference-time compute)' 및 '추론 시점 스케일링(inference-time scaling)'과 동일한 의미로 사용됩니다.

출처: OpenAI 블로그 게시물(https://openai.com/index/learning-to-reason-with-llms/)의 주석이 달린 그림

제 지난 글:

The State of LLM Reasoning Model Inference
Sebastian Raschka, PhD · Mar 8
전체 스토리 읽기

에서는 주로 추론 시점 연산 자원 활용 기법에 초점을 맞추었습니다. 본 글에서는 드디어 훈련 과정에서 적용되는 방법론들을 더욱 심도 있게 탐구하고자 합니다. 이는 모델의 근본적인 사고 능력을 향상시키는 데 있어 훈련의 중요성을 강조합니다.

### 강화 학습 기반 인공지능 모델 훈련(RLHF)의 기초: 여정의 시작

추론 능력을 갖춘 모델을 개발하고 그 성능을 고도화하는 데 활용되는 강화 학습(reinforcement learning, RL) 기법은, 기존의 대규모 언어 모델(LLM)을 구축하고 인간의 의도에 맞게 정렬(align)하는 데 사용되는 인간 피드백 기반 강화 학습(reinforcement learning with human feedback, RLHF) 방법론과 깊은 연관성을 가집니다. 따라서 사고력 증진을 위한 RL 기반 훈련의 구체적인 변형들을 논하기에 앞서, RLHF가 어떻게 작동하는지에 대한 핵심적인 내용을 먼저 요약하여 제시하고자 합니다.

전형적인 대규모 언어 모델(LLM)의 훈련 과정은 보통 세 가지 주요 단계로 구성됩니다:
*   사전 훈련(Pre-training): 방대한 텍스트 데이터로부터 일반적인 언어 패턴과 지식을 습득하는 단계입니다.
*   지도 미세 조정(Supervised fine-tuning, SFT): 특정 작업(예: 질문 답변, 요약)을 수행하도록 소량의 레이블링된 데이터로 모델을 세밀하게 조정하는 단계입니다.
*   정렬(Alignment): 모델의 출력이 인간의 가치, 선호도, 그리고 안전 기준에 부합하도록 만드는 과정으로, 주로 RLHF를 통해 이루어집니다.

초기 대규모 언어 모델의 '정렬(alignment)'을 위한 대표적인 방법은 RLHF였습니다. 이 방법론은 최초의 ChatGPT 모델 개발 과정을 상세히 설명한 InstructGPT 논문에서 제시되었으며, 이후 LLM 개발의 표준적인 접근 방식 중 하나로 자리 잡았습니다. RLHF의 본래 목적은 LLM의 행동을 인간의 선호에 일치시키는 것입니다. 예를 들어, 모델이 특정 질문에 대해 여러 가지 가능한 응답을 생성할 경우, RLHF는 사용자가 가장 바람직하다고 여기는 응답 양식을 더 자주 생성하도록 모델을 유도합니다. 이 과정은 모델이 단순히 정보를 제공하는 것을 넘어, 인간 사용자와의 상호작용에서 더욱 자연스럽고 만족스러운 경험을 제공하도록 돕습니다. (또한, RLHF는 종종 LLM의 안전성 조절에도 활용됩니다. 예를 들어, 민감하거나 유해한 정보의 공유를 방지하고, 공격적인 언어 사용을 억제하는 등의 목적을 달성합니다.)

RLHF 개념이 처음이신 분들을 위해, 몇 년 전 제가 진행했던 강연에서 RLHF를 5분 이내로 간결하게 설명한 영상의 발췌본을 참고하실 수 있습니다.

Alternatively, the paragraphs below describe RLHF in text form.

RLHF 파이프라인(pipeline)은 이미 사전 훈련(pre-trained)된 언어 모델을 가져와 지도 학습(supervised learning) 방식으로 세밀하게 조정하는 것에서 시작합니다. 이 초기 미세 조정(fine-tuning) 과정 자체는 아직 강화 학습(RL) 단계는 아니지만, 전체 RLHF 절차를 위한 필수적인 선행 조건입니다. 이어서 RLHF는 '근접 정책 최적화(Proximal Policy Optimization, PPO)'라는 특정 알고리즘을 활용하여 대규모 언어 모델을 더욱 정교하게 정렬(align)합니다. (PPO 외에도 다양한 강화 학습 알고리즘들이 존재하지만, PPO는 RLHF의 초기 구현에 사용되었으며 현재까지도 가장 널리 사용되는 알고리즘 중 하나이기에 특별히 언급합니다.)

간단히 요약하자면, RLHF 파이프라인(pipeline)은 크게 세 가지 독립적인 단계로 구성됩니다:
*   RLHF 1단계 (사전 준비): 사전 훈련(pre-trained)