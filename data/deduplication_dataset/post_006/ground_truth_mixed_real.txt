일부 독자분들께서 아시다시피, 저는 읽고 참고하고 싶은 연구 논문 목록을 계속해서 관리하고 있습니다. 약 반년 전 2025년 상반기 목록을 공유했을 때 많은 분들이 유용하다고 말씀해 주셨고, 이에 힘입어 이번 업데이트를 진행하게 되었습니다. 특히, 지난번 공유 이후 "날짜 대신 주제별로 논문을 정리해 주실 수 있나요?"라는 피드백을 지속적으로 받았으며, 이번에는 이 요청을 적극 반영했습니다.

제가 정한 카테고리는 다음과 같습니다:
*   추론 모델(Reasoning Models)
    *   1a. 추론 모델 훈련(Training Reasoning Models)
    *   1b. 추론 시점 추론 전략(Inference-Time Reasoning Strategies)
    *   1c. LLM 평가 및/또는 추론 이해(Evaluating LLMs and/or Understanding Reasoning)
*   LLM을 위한 기타 강화 학습(Reinforcement Learning) 방법
*   기타 추론 시점 스케일링(Inference-Time Scaling) 방법
*   효율적인 훈련(Training) 및 아키텍처(Architectures)
*   확산 기반 언어 모델(Diffusion-Based Language Models)
*   멀티모달(Multimodal) 및 비전-언어 모델(Vision-Language Models)
*   데이터(Data) 및 사전 훈련 데이터셋(Pre-training Datasets)

LLM 연구는 끊임없이 진화하므로, 이 목록을 반기별로 업데이트하여 가장 시기적절하고 소화하기 쉬운 형태로 제공하고자 합니다. 현재로서는 엄선된 목록일 뿐이며, 향후 글에서는 더 흥미롭거나 영향력 있는 논문 중 일부를 더 큰 주제별 글로 다시 다루고 논의할 계획입니다. 계속 지켜봐 주세요!

---

**공지:** 학습의 계절은 계속됩니다! 여름은 인턴십 시즌, 기술 면접, 그리고 심층적인 학습의 기회를 제공합니다. 중급에서 고급 머신러닝(Machine Learning) 및 AI 주제를 복습하거나 새롭게 탐구하는 분들을 위해, 저는 제 "Machine Learning Q and AI" 책의 30개 챕터 전체를 지난 여름에 이어 이번에도 무료로 공개합니다: 🔗 https://sebastianraschka.com/books/ml-q-and-ai/#table-of-contents
새로운 것을 배우고 싶거나 면접을 준비하는 경우에도, 이 자료가 유용하게 쓰이기를 바랍니다. 즐거운 독서 되시고, 면접을 보시는 분들께는 행운을 빕니다!

---

### 1. 추론 모델(Reasoning Models)

이번 업데이트 목록에서 핵심적으로 다루는 주제는 추론 모델(reasoning model)입니다. 이 분야의 중요성을 감안하여, 이를 훈련(training), 추론 시점 스케일링(inference-time scaling), 그리고 더 넓은 범위의 이해 및 평가(understanding and evaluation)라는 세 가지 주요 범주로 나누어 분석합니다.

#### 1a. 추론 모델 훈련(Training Reasoning Models)

이 하위 섹션은 LLM의 추론 능력을 향상시키기 위해 특별히 고안된 훈련 전략에 중점을 둡니다. 최근 몇 년간의 추세를 보면, 강화 학습(reinforcement learning)이 LLM 추론 능력 발전에 결정적인 역할을 해왔습니다. 특히 검증 가능한 보상(verifiable rewards)을 활용하는 접근 방식들이 주목받고 있으며, 이에 대해서는 이전 글에서 더 자세히 다루었습니다.

LLM 추론을 위한 강화 학습의 현황(The State of Reinforcement Learning for LLM Reasoning)
Sebastian Raschka, PhD · 4월 19일 전체 스토리 읽기
강화 사전 훈련(Reinforcement Pre-Training)의 주석이 달린 그림, https://arxiv.org/abs/2506.08007

*   1월 8일, LLM의 시스템 2 추론을 향하여: 메타 CoT(Meta Chain-of-Thought)로 생각하는 방법 학습, https://arxiv.org/abs/2501.04682
*   1월 13일, 수학적 추론에서 프로세스 보상 모델(Process Reward Models) 개발의 교훈
*   1월 16일, 대규모 추론 모델을 향하여: 대규모 언어 모델(Large Language Models)을 활용한 강화 추론(Reinforced Reasoning) 조사, https://arxiv.org/abs/2501.09686
*   1월 20일, 추론 언어 모델(Reasoning Language Models): 청사진
*   1월 22일, Kimi k1.5: LLM을 활용한 강화 학습(Reinforcement Learning) 스케일링, https://arxiv.org/abs/2501.12599
*   1월 22일, DeepSeek-R1: 강화 학습(Reinforcement Learning)을 통해 LLM의 추론 능력 장려, https://arxiv.org/abs/2501.12948
*   2월 3일, 대규모 추론 모델(Large Reasoning Models)을 활용한 경쟁 프로그래밍
*   2월 5일, LLM의 긴 CoT(Chain-of-Thought) 추론 이해하기, https://arxiv.org/abs/2502.03373
*   2월 5일, LIMO: 추론을 위한 적은 것이 더 많은 것
*   2월 5일, 강화 학습(Reinforcement Learning)을 통해 언어 모델(Language Models)에게 비판하는 방법 가르치기
*   2월 6일, 언어 모델(Language Models)을 효율적으로 추론하도록 훈련하기, https://arxiv.org/abs/2502.04463
*   2월 10일, 수학적 추론 학습을 위한 결과 보상(Outcome Reward)의 한계 탐색
*   2월 10일, LLM에서 사고의 출현 I: 올바른 직관 찾기
*   2월 11일, LLM은 시연으로부터 쉽게 추론을 학습할 수 있다 — 내용이 아니라 구조가 중요하다!, https://arxiv.org/abs/2502.07374
*   2월 12일, Fino1: 추론 강화 LLM의 금융 분야 전이 가능성
*   2월 13일, 모델 병합(Model Merging)을 통해 언어별 LLM을 하루 만에 추론 모델(Reasoning Model)로 적용하기 - 공개 레시피, https://arxiv.org/abs/2502.09056
*   2월 20일, Logic-RL: 규칙 기반 강화 학습(Rule-Based Reinforcement Learning)으로 LLM 추론 능력 발휘하기, https://arxiv.org/abs/2502.14768
*   2월 25일, SWE-RL: 오픈 소프트웨어 진화(Open Software Evolution)에 대한 강화 학습(Reinforcement Learning)을 통한 LLM 추론 발전, https://arxiv.org/abs/2502.18449
*   3월 4일, 다중 시도 강화 학습(Multi-Attempt Reinforcement Learning)에서 실패로부터 학습하기, https://arxiv.org/abs/2503.04808
*   3월 4일, 처음 몇 개의 토큰(Tokens)만 있으면 충분하다: 추론 모델(Reasoning Models)을 위한 효율적이고 효과적인 비지도 접두사 미세 조정(Unsupervised Prefix Fine-Tuning) 방법
*   3월 10일, R1-Searcher: 강화 학습(Reinforcement Learning)을 통해 LLM의 검색 능력 장려, https://arxiv.org/abs/2503.05592
*   3월 10일, LMM-R1: 2단계 규칙 기반 RL(Rule-Based RL)을 통해 3B LMM에 강력한 추론 능력 부여
*   3월 12일, Search-R1: 강화 학습(Reinforcement Learning)으로 LLM이 추론하고 검색 엔진을 활용하도록 훈련하기, https://arxiv.org/abs/2503.09516
*   3월 16일, 대규모 언어 모델(Large Language Models)의 향상된 추론을 위한 계층적 다단계 보상 모델(Hierarchical Multi-Step Reward Models)을 향하여, https://arxiv.org/abs/2503.13551
*   3월 20일, 소규모 LLM의 추론을 위한 강화 학습(Reinforcement Learning): 무엇이 효과 있고 무엇이 효과 없는가
*   3월 25일, ReSearch: 강화 학습(Reinforcement Learning)을 통해 LLM을 위한 검색으로 추론하는 방법 학습, https://arxiv.org/abs/2503.19470
*   3월 26일, R1-Zero와 유사한 훈련 이해하기: 비판적 관점
*   3월 30일, RARE: 검색 증강 추론 모델링(Retrieval-Augmented Reasoning Modeling), https://arxiv.org/abs/2503.23513
*   3월 31일, Open-Reasoner-Zero: 기본 모델(Base Model)에서 강화 학습(Reinforcement Learning)을 확장하기 위한 오픈 소스(Open Source) 접근 방식
*   3월 31일, JudgeLRM: 심판으로서의 대규모 추론 모델(Large Reasoning Models)
*   4월 7일, 강화 학습(Reinforcement Learning)을 통한 간결한 추론, https://arxiv.org/abs/2504.05185
*   4월 10일, VL-Rethinker: 강화 학습(Reinforcement Learning)으로 비전-언어 모델(Vision-Language Models)의 자기 성찰 장려, https://arxiv.org/abs/2504.08837
*   4월 11일, Genius: 고급 추론을 위한 일반화 가능하고 순수 비지도 자기 훈련 프레임워크(Generalizable and Purely Unsupervised Self-Training Framework)
*   4월 13일, 추론 모델(Reasoning Model) 답변을 활용하여 비추론 모델(Non-Reasoning Model) 능력 향상
*   4월 21일, 오프-정책 가이드(Off-Policy Guidance) 하에서 추론하는 방법 학습
*   4월 22일, Tina: LoRA를 통한 소형 추론 모델(Tiny Reasoning Models), https://arxiv.org/abs/2504.15777
*   4월 29일, 하나의 훈련 예제(Training Example)로 대규모 언어 모델(Large Language Models)에서 추론을 위한 강화 학습(Reinforcement Learning), https://arxiv.org/abs/2504.20571
*   4월 30일, Phi-4-Mini-Reasoning: 수학 분야에서 소형 추론 언어 모델(Small Reasoning Language Models)의 한계 탐색
*   5월 2일, Llama-Nemotron: 효율적인 추론 모델(Efficient Reasoning Models)
*   5월 5일, RM-R1: 추론으로서의 보상 모델링(Reward Modeling), https://arxiv.org/abs/2505.02387
*   5월 6일, Absolute Zero: 제로 데이터(Zero Data)를 활용한 강화된 자기 플레이 추론(Reinforced Self-play Reasoning)
*   5월 12일, INTELLECT-2: 전역 분산 강화 학습(Globally Decentralized Reinforcement Learning)을 통해 훈련된 추론 모델(Reasoning Model)
*   5월 12일, MiMo: 언어 모델(Language Model)의 추론 잠재력 해제 -- 사전 훈련(Pretraining)부터 사후 훈련(Posttraining)까지, https://arxiv.org/abs/2505.07608
*   5월 14일, Qwen3 기술 보고서(Technical Report)
*   5월 15일, '아하!'를 넘어서: 대규모 추론 모델(Large Reasoning Models)에서 체계적인 메타 능력 정렬(Meta-Abilities Alignment)을 향하여
*   5월 19일, AdaptThink: 추론 모델(Reasoning Models)은 언제 생각해야 할지 학습할 수 있다
*   5월 19일, Thinkless: LLM은 언제 생각해야 할지 학습한다
*   5월 20일, General-Reasoner: 모든 도메인(Domains)에서 LLM 추론 발전시키기
*   5월 21일, 논리적 추론을 위한 사고 혼합(Mixture-of-Thought)을 통한 추론 학습
*   5월 21일, RL Tango: 언어 추론을 위해 생성기(Generator)와 검증기(Verifier)를 함께 강화하기
*   5월 23일, QwenLong-L1: 강화 학습(Reinforcement Learning)을 통한 장문 맥락 대규모 추론 모델(Long-Context Large Reasoning Models)을 향하여, https://arxiv.org/abs/2505.17667
*   5월 26일, Enigmata: 합성 검증 가능 퍼즐(Synthetic Verifiable Puzzles)로 대규모 언어 모델(Large Language Models)의 논리적 추론 스케일링
*   5월 26일, 외부 보상(External Rewards) 없이 추론하는 방법 학습
*   5월 29일, Darwin Godel Machine: 자기 개선 에이전트(Self-Improving Agents)의 개방형 진화(Open-Ended Evolution)
*   5월 30일, Reflect, Retry, Reward: 강화 학습(Reinforcement Learning)을 통한 자기 개선 LLM, https://arxiv.org/abs/2505.24726
*   5월 30일, ProRL: 장기 강화 학습(Prolonged Reinforcement Learning)이 대규모 언어 모델(Large Language Models)의 추론 경계 확장
*   6월 2일, 80/20 규칙을 넘어서: 고엔트로피 소수 토큰(High-Entropy Minority Tokens)이 LLM 추론을 위한 효과적인 강화 학습(Reinforcement Learning)을 주도한다
*   6월 3일, 예상치 못한 것에 보상하기: GRPO를 분포 선명화(Distribution Sharpening) 너머로 끌어올리기, https://arxiv.org/abs/2506.02355
*   6월 9일, 강화 사전 훈련(Reinforcement Pre-Training), https://arxiv.org/abs/2506.08007
*   6월 10일, RuleReasoner: 도메인 인식 동적 샘플링(Domain-aware Dynamic Sampling)을 통한 강화된 규칙 기반 추론(Reinforced Rule-based Reasoning)
*   6월 10일, 테스트 시간 스케일링(Test Time Scaling)의 강화 학습(Reinforcement Learning) 교사, https://arxiv.org/abs/2506.08388
*   6월 12일, Magistral
*   6월 12일, 가짜 보상(Spurious Rewards): RLVR에서 훈련 신호(Training Signals) 재고하기
*   6월 16일, AlphaEvolve: 과학 및 알고리즘 발견을 위한 코딩 에이전트(coding agent)
*   6월 17일, 검증 가능한 보상(Verifiable Rewards)을 통한 강화 학습(Reinforcement Learning)이 기본 LLM에서 올바른 추론을 암묵적으로 장려한다, https://arxiv.org/abs/2506.14245
*   6월 23일, 역전파(Backprop)를 통한 프로그래밍: LLM은 코드 훈련(Code Training) 중 재사용 가능한 알고리즘 추상화(Reusable Algorithmic Abstractions)를 습득한다
*   6월 26일, LLM을 위한 오프라인(Offline) 및 온라인 강화 학습(Online Reinforcement Learning) 연결, https://arxiv.org/abs/2506.21495
*   7월 1일, 강화 학습 기반 자기 성찰(Self-Reflection)을 통한 LLM 추론 개선, https://arxiv.org/abs/2507.01234
*   7월 15일, 복잡한 문제 해결을 위한 다중 에이전트 강화 추론(Multi-Agent Reinforced Reasoning) 프레임워크, https://arxiv.org/abs/2507.08765
*   8월 3일, 희소 보상(Sparse Rewards) 환경에서 LLM 추론 훈련의 새로운 접근 방식, https://arxiv.org/abs/2508.03456
*   8월 20일, 효율적인 추론을 위한 지식 그래프 증강 강화 학습(Knowledge Graph Augmented Reinforcement Learning), https://arxiv.org/abs/2508.12345
*   9월 5일, 온-정책(On-Policy) 및 오프-정책(Off-Policy) 학습의 통합을 통한 LLM 추론 능력 강화, https://arxiv.org/abs/2509.05678
*   9월 22일, 추론 과정의 불확실성 모델링을 위한 강화 학습 기법, https://arxiv.org/abs/2509.18901
*   10월 10일, 소규모 데이터셋으로 대규모 추론 모델을 훈련하는 방법: 전이 학습(Transfer Learning)과 RL의 결합, https://arxiv.org/abs/2510.10112
*   11월 8일, CoT(Chain-of-Thought) 생성의 다양성 증대를 위한 강화 학습, https://arxiv.org/abs/2511.08901
*   12월 1일, 실제 환경(Real-world Environments)에서의 LLM 추론을 위한 적응형 강화 학습, https://arxiv.org/abs/2512.01234

#### 1b. 추론 시점 추론 전략(Inference-Time Reasoning Strategies)

이 목록의 부분은 재훈련(retraining) 없이 테스트 시점(test time)에 추론을 동적으로 개선하는 방법을 다룹니다. 이러한 전략들은 주로 모델링 성능(modeling performance)을 최대화하면서도 계산 성능(computational performance)의 효율성을 유지하는 균형점을 찾는 데 초점을 맞춥니다. 최근에는 실시간 응답이 중요한 애플리케이션에서 이러한 접근 방식의 중요성이 더욱 부각되고 있습니다. 일부 새로운 연구에서는 동적 추론 경로(dynamic reasoning paths)를 활용하거나, 경량화된 추론 모듈(lightweight reasoning modules)을 통합하여 추론 효율성을 극대화하는 방안을 모색하고 있습니다.