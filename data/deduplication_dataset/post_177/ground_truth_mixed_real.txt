**AI 엔지니어가 되는 가장 빠른 방법? 2025년, 실전 경험으로 승부하세요!**

Towards AI의 산업 중심 코스인 '초급부터 고급 LLM 개발자까지 (약 90개 레슨)'를 통해 실습 경험을 쌓으세요. 실제 세계에 영향을 미치기 위해 좌절했던 전직 박사들과 개발자들이 만들었습니다. 이 코스는 프로덕션 준비가 된 앱 구축을 목표로 합니다. RAG, 미세 조정(fine-tuning), 에이전트(agents)와 같은 핵심 기술을 다루며, Discord를 통한 강사 지원이 제공됩니다. 선수 과목은 기본 Python이며, 결과적으로는 인증된 제품을 출시할 수 있습니다. 가치 보장으로 30일 환불 보장 정책을 운영하고 있습니다.

**기술 수준 향상, 지속적인 학습의 중요성**
전문가 팁: 이 코스와 LLM Watch와 같은 전문 자료들은 모두 회사 학습 및 개발 예산에 해당될 수 있습니다. 2025년에도 빠르게 변화하는 AI 분야에서 경쟁력을 유지하려면, 이러한 교육 기회를 적극 활용하는 것이 중요합니다.

---

**확산 언어 모델(Diffusion Language Models)은 뛰어난 데이터 학습자입니다 ( paper / code )**

제한된 고유 데이터(unique data) 하에서, 확산 기반 언어 모델(DLM)은 더 많은 에폭(epochs) 동안 훈련될 때 동일한 크기의 자기회귀(autoregressive, AR) 모델보다 지속적으로 우수한 성능을 보입니다. 연구자들은 이러한 현상이 나타나는 지점을 **지능 교차점(Intelligence Crossover)**이라고 명명했습니다. 이 교차점은 다양한 데이터 예산, 모델 규모, 심지어 희소(Mixture-of-Experts) 아키텍처와 밀집(dense) 아키텍처 전반에 걸쳐 견고하게 나타나며, DLM이 AR 모델보다 과적합(overfitting)되기 전에 반복된 데이터에서 훨씬 더 많은 신호(signal)를 추출할 수 있음을 보여줍니다. 반면, AR 모델은 더 일찍 성능이 정체되는 경향을 보였습니다.

**DLM 성능 향상의 주요 요인:**
데이터 부족 상황에서 확산 언어 모델의 우월성은 크게 세 가지 요인에 기인합니다. 첫째, **임의 순서 모델링(Any-order modeling)**을 통해 AR 모델의 고정된 인과적 편향을 제거합니다. 둘째, 반복적인 양방향 노이즈 제거(iterative bidirectional denoising)를 통해 토큰당 훨씬 더 많은 FLOPs를 사용하는 **"초고밀도(Super-dense)" 연산**을 수행합니다. 마지막으로, 훈련이 각 시퀀스의 많은 노이즈 버전(noised versions)에 걸쳐 자연스럽게 평균화되는 **내장된 몬테카를로 증강(Monte Carlo augmentation)** 효과를 가집니다. AR 입력에 노이즈를 추가하는 것이 약간의 성능 향상을 가져올 수 있지만, DLM이 이러한 근본적인 요인들로부터 얻는 이점과는 비교할 수 없습니다.

**대규모 교차점:**
일치하는 연산 예산(약 1.5조 토큰 업데이트)을 사용하여, 100억 개의 고유 Python 토큰으로 훈련된 17억 매개변수(parameter) DLM이 동일한 조건에서 훈련된 AR 코드 모델을 능가했습니다. 다시 말해, DLM은 훨씬 더 많은 고유 데이터로 훈련된 최첨단 AR 코더와 동등한 수준에 도달했습니다. 이는 DLM이 데이터 효율성 측면에서 AR 모델에 비해 상당한 우위를 점하고 있음을 시사합니다.

**적은 데이터, 큰 결과:**
놀랍게도, 10억 매개변수 DLM은 10억 개의 훈련 데이터 토큰만을 사용하여(데이터를 여러 에폭 동안 반복하여) HellaSwag에서 56% 이상의 정확도와 MMLU에서 33% 이상의 정확도를 달성했습니다. 이는 동일한 반복 데이터셋으로 훈련된 더 큰 70억 AR 모델(해당 벤치마크에서 약 41% 및 29% 달성)을 크게 능가하는 결과입니다. 10억 토큰 코퍼스(corpus)에서 480 에폭 후에도 DLM은 **성능 포화(saturation)**를 보이지 않아, 제한된 데이터에서 계속해서 신호를 추출할 수 있음을 나타냅니다. 특히, 이 영역에서는 검증 손실(validation loss)의 증가가 하위 작업 정확도(downstream accuracy) 저하와 상관관계가 없는 것으로 나타났습니다. 즉, 검증 손실에 대한 "과적합"이 실제 작업 성능에 즉시 해를 끼치지 않았다는 중요한 발견입니다. 이러한 결과는 데이터가 부족한 환경에서 DLM이 매우 효과적인 대안이 될 수 있음을 강조합니다.

---

**Kosmos: 자율적 발견을 위한 AI 과학자 ( paper / demo )**

Kosmos는 완전히 **자율적인 과학 연구**를 수행하도록 설계된 새로운 AI 에이전트(agent)입니다. 개방형 연구 목표와 데이터셋이 주어지면, Kosmos는 최대 12시간 동안 데이터 분석, 문헌 검색, **가설 생성(hypothesis generation)** 주기를 반복한 다음, 그 결과를 보고서로 종합합니다. 일관성을 빠르게 잃는 이전 에이전트와 달리, Kosmos는 구조화된 "세계 모델(world model)"을 사용하여 데이터 분석 하위 에이전트(sub-agent)와 문헌 검색 하위 에이전트 간에 정보를 공유함으로써 **200단계 연속 에이전트 실행 동안 집중력을 유지**할 수 있습니다. 이를 통해 Kosmos는 단일 실행에서 약 42,000줄의 코드를 실행하고 약 1,500편의 연구 논문을 읽으면서도 작업을 계속 수행할 수 있습니다. 또한 최종 보고서의 모든 진술에 코드 또는 문헌 출처를 인용하여 추적 가능한 추론(reasoning)을 보장합니다. 이는 과학 연구의 투명성과 재현성을 높이는 데 기여합니다.

**연구 효율성:**
평가에서 독립적인 도메인 전문가들은 Kosmos 보고서의 진술 중 약 **79.4%가 사실적으로 정확**하다고 판단하여 높은 신뢰도를 나타냈습니다. Kosmos와 협력한 과학자들은 단일 20주기 실행(약 12시간)이 약 **6개월간의 수동 연구 작업에 해당**한다고 보고했습니다. 또한, 가치 있는 발견의 수는 주기 수에 따라 거의 선형적으로 증가하여(최대 20주기까지 테스트됨), 더 긴 실행이 계속해서 새로운 통찰력을 제공함을 시사합니다. 이는 AI가 연구 보조 도구를 넘어, 실제 발견을 주도할 수 있는 잠재력을 가졌음을 의미합니다.

**주목할 만한 발견:**
이 논문은 대사체학(metabolomics), 재료 과학(materials science), 신경 과학(neuroscience), 유전학(genetics)을 포함한 다양한 분야에서 Kosmos가 이룬 7가지 발견을 강조합니다. 놀랍게도, 이 발견 중 세 가지는 사전 인쇄물(preprints) 또는 미출판 원고(unpublished manuscripts)(Kosmos가 접근할 수 없었던)에 있던 결과를 독립적으로 재현하여, 본질적으로 알려진 과학을 "재발견"했습니다. 나머지 네 가지 발견은 **새로운 것**이었으며, 아직 출판되지 않은 새로운 과학 지식을 기여했습니다. 이러한 결과는 Kosmos가 단순한 암기식 검색(rote retrieval)을 넘어 새로운 가설과 통찰력을 생성할 수 있음을 보여주며, AI가 단순한 도구가 아닌 자율적인 과학 협력자로서 나아가는 한 걸음을 나타냅니다. 이러한 자율적 과학 에이전트의 발전은 연구의 속도와 효율성을 혁신적으로 증대시킬 잠재력을 가지고 있습니다.

---

**MemSearcher: 종단 간 강화 학습(End-to-End Reinforcement Learning)을 통해 LLM이 추론, 검색 및 메모리 관리(Memory Management)를 하도록 훈련시키기 ( paper / code )**

MemSearcher는 메모리 컨텍스트(memory context)를 적극적으로 관리하여 보다 효율적인 다중 턴 질의응답(multi-turn question answering) 및 웹 검색 작업을 위해 설계된 LLM 기반 에이전트 아키텍처(agent architecture)를 소개합니다. 일반적인 검색 에이전트는 전체 상호작용 기록을 프롬프트(prompt)에 채워 넣거나(매우 긴 입력의 대가로 컨텍스트 유지), 최신 쿼리(query)만 사용합니다(토큰을 절약하지만 중요한 정보를 잊어버림). MemSearcher는 이 두 가지 접근 방식 사이에서 효과적인 균형을 이룹니다. 각 턴에서 사용자의 현재 질문과 지속적으로 업데이트되는 압축된 메모리 상태를 결합합니다. 모델의 추론 과정은 **사고의 사슬(chain-of-thought) 추론 흔적**을 생성하고, **언제 무엇을 검색할지 결정**하며, 메모리를 정제하여 전체 작업을 해결하는 데 필수적인 정보만 유지하는 것을 포함합니다. 이 설계는 대화 전반에 걸쳐 컨텍스트 길이(context length)를 안정화하여 정확도를 희생하지 않으면서 효율성을 크게 향상시킵니다.

**종단 간 RL 훈련:**
이 에이전트는 추론, 검색 및 메모리 업데이트 결정에 대한 정책(policy)을 함께 최적화하는 **다중 컨텍스트 GRPO(multi-context GRPO)**라는 맞춤형 강화 학습(reinforcement learning) 알고리즘으로 훈련됩니다. 훈련은 다양한 컨텍스트 구성(context configurations)을 가진 대화 그룹을 샘플링하고 대화 수준에서 보상(rewards)을 전파하여, MemSearcher가 여러 턴에 걸쳐 정보를 일관성 있게 처리하는 방법을 학습하도록 합니다. 이 접근 방식은 에이전트가 단기적인 이득뿐만 아니라 장기적인 대화 흐름을 고려하도록 유도합니다.

**성능 향상:**
MemSearcher는 강력한 기준 에이전트(baseline agent, Search-R1)와 동일한 데이터로 훈련되었을 때 **7개의 벤치마크 데이터셋에서 상당한 개선**을 달성했습니다. 예를 들어, 30억 매개변수 기반 LLM으로 구축된 MemSearcher는 70억 매개변수 기준 모델까지 능가하여 Qwen2.5-3B-Instruct에서 약 **11%의 정확도 향상**과 Qwen2.5-7B-Instruct 벤치마크에서 **12%의 정확도 향상**을 보였습니다. 실제로 30억 MemSearcher는 70억 기준 모델을 능가하며, 더 스마트한 메모리와 추론이 무차별적인 모델 크기를 이길 수 있음을 보여주었습니다. 따라서 이 접근 방식은 턴당 훨씬 적은 토큰과 연산을 사용하면서도 더 나은 정확도를 제공하며, 학습된 메모리 관리(memory management)를 통해 LLM 기반 에이전트를 더 효율적으로 만드는 길을 제시합니다. 이는 특히 리소스 제약이 있는 환경이나 대규모 배포에서 상당한 이점을 제공할 수 있습니다.

---

**ThinkMorph: 다중 모달 인터리브 사고의 사슬 추론(Multimodal Interleaved Chain-of-Thought Reasoning)에서의 창발적 속성(Emergent Properties) ( paper / code )**

ThinkMorph는 **인터리브 사고의 사슬(interleaved chain-of-thought, CoT)**을 개척하는 시각-언어 추론 모델로, 텍스트와 이미지 "사고" 단계를 혼합하여 다중 모달(multimodal) 작업을 해결합니다. 핵심 아이디어는 추론 과정에서 언어 및 시각 구성 요소가 독립적으로 작동하거나 단순히 서로를 반영하는 대신 상호 보완해야 한다는 것입니다. ThinkMorph 모델은 약 24,000개의 고품질 인터리브 텍스트-이미지 추론 흔적(다양한 시각적 개입 정도를 가진 작업을 포함) 예시로 미세 조정(fine-tuned)되었습니다. 텍스트적 사고와 시각적 사고가 각자 가장 잘하는 것을 해야 한다는 원칙에 따라, ThinkMorph는 일관된 언어적 논리(verbal logic)를 유지하면서 시각적 콘텐츠를 "구체적으로 조작"(예: 이미지에 그림을 그리거나 변환을 상상)하는 추론 사슬을 생성하는 방법을 학습합니다.

**향상된 다중 모달 성능:**
여러 시각 중심 벤치마크에서 ThinkMorph는 기본 모델 대비 평균 **34.7% 향상된 큰 성능 향상**을 제공합니다. 심지어 (훈련에서 보지 못한) 완전히 새로운 작업에도 일반화(generalizes)되며, 종종 훨씬 더 큰 독점 시각-언어 모델의 성능과 일치하거나 능가합니다. 이는 더 나은 추론 능력을 가진 70억 규모의 오픈 모델이 다중 모달 CoT를 효과적으로 사용하여 자신보다 몇 배나 큰 모델과 경쟁할 수 있음을 시사합니다. 이러한 결과는 다중 모달 AI의 새로운 가능성을 열어줍니다.

**창발적 능력:**
단순 정확도를 넘어, ThinkMorph는 놀라운 **창발적 다중 모달 지능(emergent multimodal intelligence)**을 보여줍니다. 저자들은 이 모델이 이전에 보지 못했던 시각 조작 기술(예: 이미지 변환 수행 또는 명시적으로 훈련되지 않은 공간 추론)을 학습했다고 보고합니다. 이 모델은 추론에서 모달리티(modalities) 간에 적응적으로 전환할 수 있으며(이미지 기반 추론과 텍스트 중 언제 사용할지 앎), 다양화된 다중 모달 사고를 생성하여 더 많은 추론 시간(inference time)이 허용될 때 향상된 성능을 보여줍니다. 이러한 행동은 인터리브 추론에 고유한 새로운 종류의 능력을 암시합니다. 이 연구는 CoT에서 모달리티를 신중하게 혼합하는 것이 결과를 향상시킬 뿐만 아니라, 어떤 모달리티도 단독으로 달성할 수 없었던 새로운 문제 해결 전략을 가능하게 한다는 것을 시사합니다. 이는 일반 인공지능(AGI)으로 가는 중요한 단계로 평가될 수 있습니다.

---

**Tongyi DeepResearch 기술 보고서 ( paper / code )**

알리바바의 Tongyi DeepResearch는 장기적이고 심층적인 정보 탐색 연구 작업을 위해 맞춤 제작된 305억 매개변수 "에이전트적(agentic)" 대규모 언어 모델(large language model)입니다. 이 모델의 핵심은 LLM이 복잡한 다단계 추론(multi-step reasoning)과 웹 상호작용을 통해 정보를 수집할 수 있는 **자율적인 연구 에이전트(autonomous research agent)** 역할을 하도록 하는 데 있습니다. 이러한 능력을 훈련하기 위해 팀은 두 가지 특별한 단계, 즉 **에이전트적 중간 훈련(agentic mid-training)**과 **에이전트적 후반 훈련(agentic post-training)**을 포함하는 종단 간 프레임워크를 개발했습니다. 본질적으로, 기본 LLM 사전 훈련(pre-training) 후, 모델은 연구 시나리오를 시뮬레이션하는 맞춤형 상호작용 환경에서 추가로 훈련됩니다. 이 모든 과정은 사람 주석자(human annotators) 없이 고도로 확장 가능한 자동 데이터 합성 파이프라인(automatic data synthesis pipeline)을 사용하여 이루어집니다. 각 훈련 단계에 맞춤화된 시뮬레이션 환경을 구축함으로써, Tongyi DeepResearch는 연구 목표를 추구하는 과정에서 안정적이고 일관된 장기 상호작용(예: 문서 탐색, 후속 질문, 증거 검색)을 수행하는 방법을 학습합니다.

**전문가 혼합(Mixture-of-Experts) 효율성:**
Tongyi DeepResearch는 전문가 혼합(Mixture-of-Experts)과 유사한 아키텍처를 사용합니다. 총 305억 개의 매개변수를 가지고 있지만, 평균적으로 토큰당 약 33억 개만 "활성화"됩니다. 이 설계는 각 단계에서 전문가의 일부만 사용되므로 추론(inference)을 효율적으로 유지하면서 모델의 전체 용량을 확장할 수 있게 합니다. 그 결과, 매번 300억 매개변수의 전체 연산 비용을 발생시키지 않고도 광범위한 추론을 수행할 수 있는 모델이 탄생했습니다. 이는 대규모 언어 모델의 효율적인 배포에 중요한 시사점을 제공합니다.

**벤치마크 선두 주자 및 오픈 소스:**
이 모델은 에이전트 기반 심층 연구 벤치마크 제품군에서 최첨단 성능을 달성합니다. 예를 들어, 웹 브라우징, 장문 질의응답(long-form QA) 및 복잡한 다중 홉 추론(multi-hop reasoning)을 포함하는 작업인 Humanity’s Last Exam, BrowseComp (및 BrowseComp-ZH), WebWalkerQA, xBench-DeepSearch, FRAMES와 같은 평가에서 선두를 차지합니다. 이러한 결과는 LLM의 자율 연구 능력에서 새로운 수준의 숙련도를 보여줍니다. 더욱이, Tongyi 팀은 에이전트적 LLM(agentic LLMs) 분야의 커뮤니티 개발을 장려하기 위해 모델 가중치(model weights), 훈련 프레임워크(training framework) 및 "완전한 솔루션"을 포함한 **전체 프로젝트를 오픈 소스(open-sourced)로 공개**했습니다. 이러한 오픈 소스화는 관련 연구 분야의 발전을 가속화하고, 더 많은 개발자들이 실제 연구 작업을 수행할 수 있는 대규모 LLM 기반 에이전트 개발에 참여할 수 있는 귀중한 자원을 제공합니다.

---

**견고한 수학적 추론을 향하여 ( paper / code )**

Luong 외 연구진의 이 연구는 AI 모델의 고급 수학적 추론을 평가하고 개선하는 과제를 다룹니다. 핵심 통찰은 기존의 많은 수학 벤치마크가 너무 쉽거나 단답형(short answers)만 테스트하여 진정한 수학적 문제 해결 능력을 반영하지 못한다는 것입니다. 이러한 한계를 뛰어넘기 위해 저자들은 **IMO-Bench**를 소개합니다. IMO-Bench는 국제 수학 올림피아드(International Mathematical Olympiad, IMO) 수준의 벤치마크 모음으로, IMO는 고등학생을 위한 가장 어려운 수학 경시 대회 중 하나입니다. IMO-Bench는 두 가지 주요 구성 요소로 이루어져 있습니다. **IMO-AnswerBench**(검증 가능한 단답형을 가진 400개의 다양한 올림피아드 문제)와 **IMO-ProofBench**(자동 평가를 위한 상세한 채점 기준표와 함께 완전한 증명 작성을 요구하는 올림피아드 수준 문제 세트)입니다. 이 벤치마크들은 IMO 난이도를 진정으로 반영하는지 확인하기 위해 최고의 수학자들에 의해 검증되었습니다. 저자들은 이처럼 도전적이고 엄격하게 평가된 벤치마크를 사용하는 것이 AI의 수학적 추론 발전을 위한 "북극성(north-star)"으로서 중요하다고 주장합니다.

**역사적 성과:**
IMO-Bench를 개발 지침으로 사용하여, 팀의 모델(Gemini Deep Think이라는 별명)은 IMO 2025 경시 문제에서 "금메달" 성능을 달성한 최초의 AI 시스템이 되었습니다. 구체적으로, 그들의 모델은 IMO-AnswerBench(단답형)에서 80.0%, IMO-ProofBench(상세 증명)에서 65.7%를 기록하여, 최고의 비-Gemini 모델들을 각각 6.9%와 무려 42.4% 앞섰습니다. 이는 특히 증명 해결 능력의 도약에서 엄청난 차이이며, 기계 추론(machine reasoning)의 주요 발전을 시사합니다.

**자동 채점 및 향후 연구:**
이 분야의 발전을 촉진하기 위해, 그들은 또한 LLM을 사용하여 자동 증명 채점기(automatic proof grader)를 구축했습니다(채점을 위해 모델 자체의 추론 능력을 활용). 연구팀은 1,000개의 사람이 채점한 증명 솔루션 데이터셋인 **IMO-GradingBench**를 구축했으며, AI 기반 채점기의 점수가 인간의 판단과 잘 상관관계가 있음을 보여주었습니다. 이는 장문, 단계별 추론을 안정적으로 평가하는 데 중요한 단계입니다. 저자들은 IMO-Bench를 공개하여, 단순히 정답을 맞히는 것을 넘어 인간 수학자가 하듯이 실제로 과정을 보여주는 진정으로 견고한 수학적 추론을 중심으로 커뮤니티를 결집하는 것을 목표로 합니다. 이러한 벤치마크는 AI가 아직 해결하지 못한 개방형 수학 문제들을 탐구하는 데 중요한 이정표가 될 것입니다.

---

**Kimi Linear: 표현력이 풍부하고 효율적인 어텐션 아키텍처(Attention Architecture) ( paper / code )**

Kimi Linear 아키텍처는 선형(저랭크) 어텐션 근사(linear (low-rank) attention approximations)의 효율성을 유지하면서도 정확도 면에서 표준 전체 어텐션(full attention)을 능가하는 새로운 어텐션 메커니즘(attention mechanism)을 제안합니다. 이는 지금까지 선형 또는 효율적인 트랜스포머(긴 컨텍스트로 확장되는)가 속도를 위해 정확도를 일부 희생해야 하는 경우가 많았기 때문에 중요합니다. Kimi Linear는 **Kimi 델타 어텐션(Kimi Delta Attention, KDA)**이라는 모듈을 도입하는데, 이는 더 세분화된 게이팅 메커니즘(gating mechanism)을 추가하여 DeltaNet의 아이디어를 확장합니다. 본질적으로 이는 하이브리드 접근 방식입니다. RNN 기반(유한 상태) 부분과 트랜스포머(Transformer) 부분이 결합되어 있으며, 게이팅(gating)은 RNN 스타일의 상태 메모리(state memory)를 더 효과적으로 사용할 수 있게 합니다. 저자들은 또한 이 아키텍처를 하드웨어 효율적으로 만드는 맞춤형 "청크 단위(chunkwise)" 연산 알고리즘을 개발했습니다. 그들은 특수한 형태의 대각선-저랭크(Diagonal-Plus-Low-Rank, DPLR) 행렬을 활용하여 연산을 대폭 줄이면서도 고전적인 델타 규칙 네트워크(delta rule networks)의 완전한 표현력을 밀접하게 모방합니다.

**전체 어텐션 능가:**
짧은 컨텍스트 작업, 100만 토큰까지의 긴 컨텍스트 작업, 심지어 강화 학습 시나리오를 포함한 다양한 설정에서의 실험에서 Kimi Linear 모델은 **전체 어텐션을 사용하는 동등한 모델보다 지속적으로 우수한 성능**을 보였습니다. 팀은 KDA와 표준 멀티 헤드 잠재 어텐션(multi-head latent attention) 레이어를 혼합하여 총 480억 개의 매개변수(토큰당 30억 개의 매개변수 활성화)를 가진 대규모 Kimi Linear 모델을 사전 훈련했습니다. 동일한 훈련 데이터와 하이퍼파라미터(hyperparameters)를 사용하여, 이 480억(30억 활성) Kimi 모델은 평가된 모든 작업에서 동일한 크기의 표준 트랜스포머를 능가했습니다.

**효율성 향상:**
정확도 외에도 Kimi Linear는 배포(deployment) 시 훨씬 더 자원 친화적입니다. 이는 바닐라 트랜스포머(vanilla Transformer)에 비해 키-값 캐시 메모리(key-value cache memory)를 **최대 75%까지 줄여주는데**, 이는 긴 시퀀스(sequences)에 대해 엄청난 절약입니다. 더욱이, 극도로 긴 컨텍스트(예: 100만 토큰 컨텍스트로 텍스트 생성)의 경우, 기준 전체 어텐션 모델보다 **최대 6배 빠른 디코딩 처리량(decoding throughput)**을 달성했습니다. 이러한 개선은 표준 어텐션 대신 Kimi Linear를 사용하여 더 나은 속도/메모리 사용량과 더 나은 작업 성능을 모두 얻을 수 있음을 의미하며, 이는 드문 윈-윈(win-win) 상황입니다. 이는 모바일 기기나 실시간 응용 프로그램과 같은 엣지 디바이스(edge devices)에서의 LLM 배포 가능성을 크게 확장합니다. 채택 및 추가 연구를 장려하기 위해, 저자들은 핵심 KDA 커널을 오픈 소스(open-sourced)로 공개하고 vLLM 라이브러리에 구현을 제공했으며, 사전 훈련된 480억 모델과 명령어 미세 조정(instruction-tuned) 체크포인트(checkpoint)를 함께 공개했습니다.

---

**연속 자기회귀 언어 모델(Continuous Autoregressive Language Models, CALM) ( paper / code )**

이 논문은 LLM의 생성 과정을 근본적으로 재고하여 한 번에 하나의 토큰을 디코딩하는 비효율성을 극복하는 **연속 자기회귀 언어 모델(Continuous Autoregressive Language Models, CALM)**을 소개합니다. 이 아이디어는 이산 토큰(discrete tokens)에서 **연속 벡터(continuous vectors)**로의 패러다임 전환입니다. 다음 단어/토큰을 예측하는 대신, 모델은 전체 텍스트 덩어리를 나타내는 연속 임베딩(continuous embedding)을 예측합니다. 고충실도 신경 오토인코더(neural autoencoder)는 K개의 토큰 시퀀스를 단일 벡터로 압축하며, 이 벡터는 나중에 99.9% 이상의 정확도로 원래 토큰으로 다시 디코딩될 수 있습니다. 이를 통해 언어 시퀀스는 연속 벡터 시퀀스로 변환되며, 모델은 K개의 토큰에 해당하는 콘텐츠를 생성하는 데 단 1단계만 필요합니다(각 생성 단계의 "대역폭(bandwidth)"을 K배만큼 효과적으로 증가시킴). 이는 주어진 길이의 텍스트를 생성하는 데 필요한 순방향 전달(forward passes) 횟수를 대폭 줄입니다.

**새로운 모델링 프레임워크:**
연속 예측으로 전환하려면 저자들이 **우도(likelihood)가 없는 훈련 및 평가 프레임워크**를 개발해야 했습니다. 전통적인 언어 모델은 이산 토큰 시퀀스의 우도를 최대화하지만, CALM은 확률과 디코딩(decoding) 개념이 다른 연속 공간에서 작동합니다. 이 논문은 이러한 모델을 훈련하고, 성능을 측정하며, 심지어 연속 도메인(continuous domain)에서 완전히 제어 가능한 생성(controllable generation)을 수행하는 방법을 제시합니다.

**결과 – 속도 대 성능:**
실험 결과 CALM은 강력한 토큰 기반 LLM과 동일한 수준의 성능을 **훨씬 적은 연산으로 달성**할 수 있음을 보여줍니다. 예를 들어, 기준 모델이 N개의 토큰을 생성하는 데 N개의 트랜스포머 단계가 필요하다면, 압축 계수 K를 가진 CALM 모델은 N/K 단계만 필요하여 상당한 속도 향상을 가져옵니다. 저자들은 성능-연산 트레이드오프(performance-compute trade-off)가 현저히 개선되었다고 보고합니다. 주어진 연산 예산에서 CALM 모델은 이산 모델보다 우수한 성능을 보입니다. 이는 **다음 벡터 예측(next-vector prediction)**이 초고효율 LLM을 향한 유망한 경로임을 확립합니다. 이 연구는 아직 초기 단계이지만(이를 위해 고정밀 텍스트 오토인코더가 필요하므로, 연속적인 표현의 미묘한 의미를 정확하게 포착하는 것이 여전히 도전 과제입니다), 미래에는 단어별 생성(word-by-word generation)을 완전히 포기할 수도 있음을 시사합니다. 코드와 프로젝트 세부 정보가 공개되어, 커뮤니티가 이 패러다임을 기반으로 구축하도록 초대하고 있습니다.

---

**에이전트적 조직의 시대: 언어 모델로 조직화하는 법 배우기 ( paper )**

Chi 외 연구진은 단일 AI 에이전트가 순차적으로 사고하는 것을 넘어선 AI 문제 해결 비전을 제안합니다. 대신, 그들은 여러 에이전트가 복잡한 문제의 다양한 측면에서 동시적이고 협력적으로 작업하는 "**에이전트적 조직(agentic organization)**"을 설명합니다. 이 패러다임에서 AI 에이전트는 팀워크를 효과적으로 조직함으로써 개인이 달성할 수 없는 결과를 얻을 수 있습니다. 저자들은 비동기적 사고(Asynchronous Thinking, AsyncThink)라는 새로운 추론 프레임워크를 통해 이 아이디어를 구현합니다. AsyncThink에서 중앙 조직자 에이전트(Organizer agent)는 프로세스를 조율합니다. 작업을 하위 쿼리(sub-queries)로 동적으로 분해하고 이를 여러 작업자 에이전트(Worker agents)에게 병렬로 할당합니다. 작업자들은 자신의 하위 작업(지식 기반 쿼리, 부분 솔루션 계산 등)을 수행하고, 조직자는 그들의 중간 결과를 최종 답변으로 병합합니다. 결정적으로, 이 사고 과정의 구조(문제를 어떻게 분할하고 언제 조율할지)는 수동으로 설계되는 대신 **강화 학습(reinforcement learning)**을 통해 자체적으로 최적화될 수 있습니다.

**속도 및 정확도 향상:**
복잡한 추론 작업에 대한 실험에서 AsyncThink는 기준 병렬 처리 접근 방식(학습된 조직자 없이 여러 추론 스레드가 실행되는)에 비해 추론 지연 시간(inference latency)을 **28% 감소**시켰습니다. 하위 작업을 비동기적으로 지능적으로 스케줄링함으로써, 순수하게 순차적인 사고의 사슬(chains-of-thought)보다 문제를 더 빠르게 해결할 뿐만 아니라, 순진한 병렬 사고(naive parallel thinking)의 함정을 피합니다. 더욱이, AsyncThink를 사용할 때 수학적 추론의 정확도가 향상되었는데, 이는 학습된 조직화가 더 빠른 해결책뿐만 아니라 더 나은 해결책으로 이어진다는 것을 나타냅니다.

**일반화:**
주목할 만한 발견은 AsyncThink 프레임워크가 일단 훈련되면 추가 훈련 없이 완전히 새로운 작업을 해결하도록 일반화될 수 있다는 것입니다. 에이전트들은 **전이 가능한(transferable)** 추론 조직화 형태를 학습했습니다. 즉, 보지 못한 문제에 대해 효과적으로 자체 조직화할 수 있습니다. 이는 "함께 생각하는" 방법을 배우는 것과 같은 더 추상적인 기술의 출현을 시사하며, 이는 광범위하게 적용될 수 있습니다. 이 연구는 단일 모델의 크기를 확장하는 대신, 모델이 집단으로서 자신을 조직하는 방식을 확장함으로써 더 큰 지능을 달성할 수 있는 새로운 방향을 예고합니다. 이는 AI 시스템을 전문 문제 해결사로 구성된 조율된 팀으로 취급하는 한 걸음이며, 미래의 복잡한 문제 해결에 필수적인 요소가 될 것입니다.

---

**컨텍스트 엔지니어링 2.0: 컨텍스트 엔지니어링의 컨텍스트 ( paper / code )**

이 논문은 AI 시스템에서 "**컨텍스트 엔지니어링(context engineering)**"에 대한 포괄적인 개념적 및 역사적 관점을 제공합니다. 컨텍스트 엔지니어링은 상호작용의 컨텍스트(인간 컨텍스트든 상황적 컨텍스트든)를 깊이 이해하고 활용하여 적절하게 행동하는 AI 시스템을 설계하는 것을 의미합니다. 저자들은 이 용어가 고급 AI 에이전트 시대에 최근 인기를 얻었지만, 그 근본적인 아이디어는 **수십 년 전으로 거슬러 올라간다**고 언급합니다. 그들은 1990년대 초부터 연구자들이 기계가 컨텍스트를 파악하는 방법을 점진적으로 구축해 온 과정을 추적합니다. 비교적 단순한 컴퓨터를 사용한 초기 인간-컴퓨터 상호작용 프레임워크부터 오늘날의 정교한 인간-AI 에이전트 상호작용, 그리고 미래의 인간 수준 또는 초인간 AI 시나리오에 이르기까지 말입니다. 본질적으로, 기계가 더 지능적으로 발전함에 따라 컨텍스트에 대한 접근 방식은 뚜렷한 단계를 거쳐 진화했으며, 우리는 이제 AI 에이전트에 초점을 맞춘 새로운 단계("컨텍스트 엔지니어링 2.0")에 있습니다.

**컨텍스트 엔지니어링 정의:**
이 논문은 컨텍스트 엔지니어링의 공식적인 정의와 분류(taxonomy)를 제시하며, 이전 연구에 기반을 두고 그 범위를 명확히 합니다. 이 논문은 이 분야의 역사적 이정표를 검토하며, 메모리 네트워크(memory networks), 사용자 모델링(user modeling), 환경 시뮬레이션(environment simulation), 프롬프트 엔지니어링(prompt engineering) 등 많은 기술들이 AI가 작동하는 상황을 더 잘 이해하도록 돕기 위한 노력의 연속체(continuum)의 일부임을 보여줍니다. 오늘날의 관행을 이러한 역사적 맥락에 배치함으로써, 저자들은 컨텍스트 엔지니어링이 단순한 유행어가 아니라 풍부한 계보와 원칙을 가진 분야라고 주장합니다.

**설계 고려 사항 및 미래 전망:**
중요하게도, 이 보고서는 실용적인 컨텍스트 엔지니어링을 위한 주요 설계 고려 사항을 논의합니다. 여기에는 컨텍스트 지식(contextual knowledge)을 표현하는 방법, 컨텍스트를 동적으로 업데이트하는 방법, 컨텍스트 데이터를 사용하는 데 있어서의 개인 정보 보호 및 윤리적 문제, 그리고 AI의 컨텍스트 이해를 평가하는 방법이 포함됩니다. 저자들은 연구자와 개발자들이 AI 시스템에 컨텍스트를 체계적으로 통합하는 데 도움이 될 개념적 기반을 제공하는 것을 목표로 합니다. 그들은 임시방편적인 프롬프트 조정이나 휴리스틱(heuristic) 접근 방식보다는 AI에서 컨텍스트에 대한 원칙적인 접근 방식을 옹호합니다. 이 논문은 본질적으로 커뮤니티가 컨텍스트 엔지니어링을 일류 학문으로 인식하도록 촉구하는 것입니다. 이는 특히 다중 모달 컨텍스트나 체화된 AI(embodied AI)와 같이 더욱 복잡하고 현실적인 환경에서 인간과 유창하게 상호작용하는 AI 에이전트를 설계하는 데 매우 중요할 것입니다.

---

**구독 ❤️** 이 기사가 유용하셨다면 '좋아요'를 누르고 동료들과 공유해주세요. 댓글을 남겨 의견을 나눠주세요. LLM Watch를 읽어주셔서 감사합니다! 새로운 게시물을 무료로 받아보고 저의 작업을 지원하려면 지금 바로 구독해주세요. 최신 AI 동향과 깊이 있는 분석을 놓치지 마세요!