AI 시대, 핵심 역량은? 직접 만들어 보는 것입니다! 빠르게 변화하는 인공지능(AI) 시대에서 이론적 지식만큼 중요한 것은 바로 실제 문제를 해결하고 혁신적인 솔루션을 구현하는 실질적인 경험입니다. 복잡한 대규모 언어 모델(LLM)부터 최신 에이전트 시스템에 이르기까지, 최첨단 기술을 직접 다루며 실습 경험을 쌓는 것이 중요합니다. 이는 단순히 기술을 이해하는 것을 넘어, 실제 세계에 영향을 미치기 위해 좌절했던 전직 박사들과 개발자들이 그랬듯이, 창의적인 해결책을 찾아내는 핵심 역량이 됩니다. Towards AI의 산업 중심 코스인 '초급부터 고급 LLM 개발자까지 (약 90개 레슨)'를 통해 실습 경험을 쌓으세요. 프로덕션 준비가 된 앱 구축을 위한 RAG, 미세 조정(fine-tuning), 그리고 복잡한 에이전트(agents) 설계 등 다양한 영역에서 깊이 있는 학습이 필수적입니다. 이 코스는 Discord를 통한 강사 지원, 기본 Python 선수 과목, 인증된 제품 출시 결과, 그리고 30일 환불 보장 가치를 제공합니다.

**기술 발전의 조직적 활용**
오늘날의 기술 혁신은 개인의 역량 강화뿐만 아니라 조직 전체의 성장으로 이어집니다. 전문가 팁으로, 새로운 기술 교육 프로그램이나 AI 관련 콘텐츠 플랫폼(예: LLM Watch)은 회사 학습 및 개발 예산에 해당될 수 있습니다. 지속적인 교육 투자는 팀의 기술 수준을 향상시키고, 급변하는 AI 환경에 유연하게 대응할 수 있는 조직 문화를 구축하는 데 기여합니다.

**생성형 AI의 새로운 지평: 데이터 효율성을 넘어**
최근 생성형 AI 분야에서는 확산 기반 언어 모델(Diffusion Language Models, DLM)과 같은 혁신적인 아키텍처가 주목받고 있습니다 ( paper / code ). 제한된 고유 데이터(unique data) 하에서, DLM은 더 많은 에폭(epochs) 동안 훈련될 때 동일한 크기의 자기회귀(autoregressive, AR) 모델보다 지속적으로 우수한 성능을 보입니다. 저자들은 이 지점을 **지능 교차점(Intelligence Crossover)**이라고 부릅니다. 이러한 교차점은 다양한 데이터 예산, 모델 규모, 심지어 희소(Mixture-of-Experts) 아키텍처와 밀집(dense) 아키텍처 전반에 걸쳐 견고하게 나타납니다. 저자들은 DLM이 과적합(overfitting)되기 전에 반복된 데이터에서 훨씬 더 많은 신호(signal)를 추출할 수 있는 반면, AR 모델은 더 일찍 정체된다는 것을 발견했습니다. 이는 모델이 적은 데이터로부터 더 많은 정보를 추출하고 일반화하는 능력이 향상되었음을 의미하며, 특히 멀티모달(multimodal) 데이터 생성이나 합성 데이터(synthetic data) 활용과 같은 새로운 영역에서 이러한 효율성은 중요한 이점으로 작용하고 있습니다.

**DLM 성능 향상의 주요 요인:**
데이터 부족 상황에서 DLM의 우월성은 (1) **임의 순서 모델링(Any-order modeling)**(AR의 고정된 인과적 편향 제거), (2) **"초고밀도(Super-dense)" 연산**(반복적인 양방향 노이즈 제거(iterative bidirectional denoising)가 토큰당 훨씬 더 많은 FLOPs를 사용), (3) **내장된 몬테카를로 증강(Monte Carlo augmentation)**(훈련이 각 시퀀스의 많은 노이즈 버전(noised versions)에 걸쳐 자연스럽게 평균화됨)에 기인합니다. AR 입력에 노이즈를 추가하는 것이 약간 도움이 되지만, DLM이 이러한 요인들로부터 얻는 이점과는 비교할 수 없습니다.

**데이터 희소성 극복을 위한 전략**
AI 모델 훈련에 있어 데이터 부족 상황에서 DLM의 우월성은 여전히 중요한 연구 주제입니다. 하지만 이제는 단순히 모델 아키텍처의 개선을 넘어, 데이터 증강(data augmentation), 전이 학습(transfer learning), 그리고 자기 지도 학습(self-supervised learning)과 같은 다양한 전략들이 활발히 연구되고 있습니다. 이러한 접근 방식들은 고품질의 대규모 데이터셋 확보가 어려운 현실에서 모델의 성능을 극대화하고, 새로운 도메인으로의 확장을 가능하게 합니다.

**대규모 교차점:**
일치하는 연산 예산(약 1.5조 토큰 업데이트)을 사용하여, 100억 개의 고유 Python 토큰으로 훈련된 17억 매개변수(parameter) DLM이 동일한 조건에서 훈련된 AR 코드 모델을 능가했습니다. 다시 말해, DLM은 훨씬 더 많은 고유 데이터로 훈련된 최첨단 AR 코더와 동등한 수준에 도달했습니다.

**효율적인 모델 스케일링의 중요성**
현재 AI 연구의 주요 목표 중 하나는 모델의 크기와 성능을 동시에 최적화하는 것입니다. 과거에는 17억 매개변수(parameter) DLM이 동일한 조건에서 훈련된 AR 코드 모델을 능가했습니다. 이는 단순히 모델의 매개변수를 늘리는 것만이 해답이 아님을 보여줍니다. 오늘날에는 모델 효율성을 극대화하기 위한 새로운 스케일링 법칙과 아키텍처 혁신이 요구됩니다. 컴퓨팅 자원 제약 속에서 최대의 성능을 이끌어내는 방법론에 대한 연구가 활발히 진행 중입니다.

**소형 언어 모델(SLM)의 부상**
최근 주목받는 흐름 중 하나는 소형 언어 모델(Small Language Model, SLM)의 발전입니다. 놀랍게도, 10억 매개변수 DLM은 10억 개의 훈련 데이터 토큰만을 사용하여(데이터를 여러 에폭 동안 반복하여) HellaSwag에서 56% 이상의 정확도와 MMLU에서 33% 이상의 정확도를 달성했습니다. 이는 동일한 반복 데이터셋으로 훈련된 더 큰 70억 AR 모델(해당 벤치마크에서 약 41% 및 29% 달성)을 크게 능가하는 결과입니다. 10억 토큰 코퍼스(corpus)에서 480 에폭 후에도 DLM은 **성능 포화(saturation)**를 보이지 않아, 제한된 데이터에서 계속해서 신호를 추출할 수 있음을 나타냅니다. 특히, 이 영역에서는 검증 손실(validation loss)의 증가가 하위 작업 정확도(downstream accuracy) 저하와 상관관계가 없는 것으로 나타났습니다 (즉, 검증 손실에 대한 "과적합"이 실제 작업 성능에 즉시 해를 끼치지 않았습니다). 이는 거대 모델만이 최고 성능을 낸다는 통념에 도전하며, 효율적인 아키텍처와 훈련 기법을 통해 적은 자원으로도 특정 작업에서 충분히 경쟁력 있는 결과를 얻을 수 있음을 시사합니다. SLM은 엣지 디바이스(edge device)나 특정 도메인에 특화된 애플리케이션에서 비용 효율적이고 빠르게 배포될 수 있는 대안으로 각광받고 있습니다.

**AI 기반 과학 발견의 가속화**
인공지능은 과학 연구의 패러다임을 변화시키고 있습니다. Kosmos는 완전히 **자율적인 과학 연구**를 수행하도록 설계된 새로운 AI 에이전트(agent)로, 연구자들에게 혁신적인 도구를 제공합니다 ( paper / demo ). 개방형 연구 목표와 데이터셋이 주어지면, Kosmos는 최대 12시간 동안 데이터 분석, 문헌 검색, **가설 생성(hypothesis generation)** 주기를 반복한 다음, 그 결과를 보고서로 종합합니다. 일관성을 빠르게 잃는 이전 에이전트와 달리, Kosmos는 구조화된 "세계 모델(world model)"을 사용하여 데이터 분석 하위 에이전트(sub-agent)와 문헌 검색 하위 에이전트 간에 정보를 공유함으로써 **200단계 연속 에이전트 실행 동안 집중력을 유지**할 수 있습니다. 이를 통해 Kosmos는 단일 실행에서 약 42,000줄의 코드를 실행하고 약 1,500편의 연구 논문을 읽으면서도 작업을 계속 수행할 수 있습니다. 또한 최종 보고서의 모든 진술에 코드 또는 문헌 출처를 인용하여 추적 가능한 추론(reasoning)을 보장합니다. 이러한 AI 시스템은 복잡한 데이터 분석, 가설 생성, 실험 설계 등 과학 연구의 다양한 단계를 자동화하고 가속화할 수 있으며, 이는 신약 개발, 재료 과학, 기후 모델링 등 광범위한 분야에서 인간 연구자들이 도달하기 어려웠던 새로운 발견을 가능하게 합니다.

**연구 시간 단축을 통한 혁신 촉진**
AI 에이전트의 발전은 연구 효율성을 획기적으로 높이고 있습니다. 평가에서 독립적인 도메인 전문가들은 Kosmos 보고서의 진술 중 약 **79.4%가 사실적으로 정확**하다고 판단하여 높은 신뢰도를 나타냈습니다. Kosmos와 협력한 과학자들은 단일 20주기 실행(약 12시간)이 약 **6개월간의 수동 연구 작업에 해당**한다고 보고했습니다. 또한, 가치 있는 발견의 수는 주기 수에 따라 거의 선형적으로 증가하여(최대 20주기까지 테스트됨), 더 긴 실행이 계속해서 새로운 통찰력을 제공함을 시사합니다. 이러한 시간 절약은 연구자들이 반복적이고 시간 소모적인 작업에서 벗어나, 보다 창의적이고 전략적인 연구에 집중할 수 있게 합니다. AI는 단순히 데이터를 처리하는 도구를 넘어, 연구 가설을 세우고, 실험을 설계하며, 결과를 해석하는 과정 전반에 걸쳐 강력한 조력자가 되고 있습니다.

**주목할 만한 발견:**
이 논문은 대사체학(metabolomics), 재료 과학(materials science), 신경 과학(neuroscience), 유전학(genetics)을 포함한 다양한 분야에서 Kosmos가 이룬 7가지 발견을 강조합니다. 놀랍게도, 이 발견 중 세 가지는 사전 인쇄물(preprints) 또는 미출판 원고(unpublished manuscripts)(Kosmos가 접근할 수 없었던)에 있던 결과를 독립적으로 재현하여, 본질적으로 알려진 과학을 "재발견"했습니다. 나머지 네 가지 발견은 **새로운 것**이었으며, 아직 출판되지 않은 새로운 과학 지식을 기여했습니다. 이러한 결과는 Kosmos가 단순한 암기식 검색(rote retrieval)을 넘어 새로운 가설과 통찰력을 생성할 수 있음을 보여줍니다. 미래의 AI는 단순한 도구를 넘어 인간의 협력적인 문제 해결의 중요한 파트너가 될 것입니다. AI가 단순한 도구가 아닌 자율적인 과학 협력자로서 나아가는 한 걸음을 나타내며, 이는 과학 분야에만 국한되지 않습니다. 비즈니스 의사 결정, 사회 문제 해결, 예술 창작 등 다양한 영역에서 AI는 인간과 상호작용하며 더욱 복잡하고 다면적인 문제에 대한 해결책을 제시할 것입니다. 인간의 직관과 AI의 분석 능력이 결합될 때, 우리는 상상 이상의 시너지를 기대할 수 있습니다.

**LLM의 장기 컨텍스트 이해와 메모리 관리**
대규모 언어 모델(LLM)의 핵심 과제 중 하나는 장기적인 컨텍스트(context)를 효과적으로 유지하고 관리하는 것입니다. MemSearcher는 메모리 컨텍스트(memory context)를 적극적으로 관리하여 보다 효율적인 다중 턴 질의응답(multi-turn question answering) 및 웹 검색 작업을 위해 설계된 LLM 기반 에이전트 아키텍처(agent architecture)를 소개합니다 ( paper / code ). 일반적인 검색 에이전트는 전체 상호작용 기록을 프롬프트(prompt)에 채워 넣거나(매우 긴 입력의 대가로 컨텍스트 유지), 최신 쿼리(query)만 사용합니다(토큰을 절약하지만 중요한 정보를 잊어버림). MemSearcher는 균형을 이룹니다. 각 턴에서 사용자의 현재 질문과 지속적으로 업데이트되는 압축된 메모리 상태를 결합합니다. 모델의 추론 과정은 **사고의 사슬(chain-of-thought) 추론 흔적**을 생성하고, **언제 무엇을 검색할지 결정**하며, 메모리를 정제하여 전체 작업을 해결하는 데 필수적인 정보만 유지하는 것을 포함합니다. 이 설계는 대화 전반에 걸쳐 컨텍스트 길이(context length)를 안정화하여 정확도를 희생하지 않으면서 효율성을 크게 향상시킵니다. 이는 LLM이 긴 대화 기록이나 방대한 문서를 처리할 때 중요한 정보를 놓치지 않고 일관된 응답을 생성하는 데 필수적이며, 효율적인 메모리 관리는 에이전트가 복잡한 작업을 단계별로 수행하며 지속적으로 정보를 활용할 수 있도록 돕습니다.

**AI 모델 정렬을 위한 강화 학습의 발전**
최근 LLM 개발에서 강화 학습(reinforcement learning) 알고리즘이 중요하게 활용됩니다. MemSearcher 에이전트는 추론, 검색 및 메모리 업데이트 결정에 대한 정책(policy)을 함께 최적화하는 **다중 컨텍스트 GRPO(multi-context GRPO)**라는 맞춤형 강화 학습 알고리즘으로 훈련됩니다. 훈련은 다양한 컨텍스트 구성(context configurations)을 가진 대화 그룹을 샘플링하고 대화 수준에서 보상(rewards)을 전파하여, MemSearcher가 여러 턴에 걸쳐 정보를 일관성 있게 처리하는 방법을 학습하도록 합니다. 특히 인간 피드백 기반 강화 학습(Reinforcement Learning from Human Feedback, RLHF)은 모델이 인간의 가치와 의도에 부합하는 행동을 학습하도록 유도하는 데 중요한 역할을 합니다. 이 기술은 모델의 안전성, 유용성, 그리고 전반적인 사용자 경험을 향상시키는 데 필수적이며, LLM이 단순히 텍스트를 생성하는 것을 넘어 사회적으로 책임감 있는 AI로 발전하는 데 기여합니다.

**성능 향상:**
MemSearcher는 강력한 기준 에이전트(baseline agent, Search-R1)와 동일한 데이터로 훈련되었을 때 **7개의 벤치마크 데이터셋에서 상당한 개선**을 달성했습니다. 예를 들어, 30억 매개변수 기반 LLM으로 구축된 MemSearcher는 70억 매개변수 기준 모델까지 능가하여 Qwen2.5-3B-Instruct에서 약 **11%의 정확도 향상**과 Qwen2.5-7B-Instruct 벤치마크에서 **12%의 정확도 향상**을 보였습니다. 실제로 30억 MemSearcher는 70억 기준 모델을 능가하여, 더 스마트한 메모리와 추론이 무차별적인 모델 크기를 이길 수 있음을 보여주었습니다. 따라서 이 접근 방식은 턴당 훨씬 적은 토큰과 연산을 사용하면서 더 나은 정확도를 제공하며, 학습된 메모리 관리(memory management)를 통해 LLM 기반 에이전트를 더 효율적으로 만드는 길을 제시합니다. 이러한 AI 에이전트 기술의 발전은 실제 세계에서 더욱 효율적이고 견고한 애플리케이션을 가능하게 하며, 에이전트가 복잡한 환경에서 자율적으로 의사결정을 내리고, 외부 도구와 상호작용하며, 지속적으로 학습하는 능력을 향상시키는 데 기여합니다. 이러한 발전은 자동화된 고객 서비스, 개인 비서, 심지어 자율 주행 시스템에 이르기까지 다양한 분야에서 혁신을 이끌어낼 잠재력을 가지고 있습니다.

**멀티모달 AI의 진화: 인식과 추론의 통합**
멀티모달(multimodal) AI는 시각, 청각, 텍스트 등 여러 양식의 정보를 통합하여 세상을 이해하는 능력을 향상시키고 있습니다. ThinkMorph는 **인터리브 사고의 사슬(interleaved chain-of-thought, CoT)**을 개척하는 시각-언어 추론 모델로, 텍스트와 이미지 "사고" 단계를 혼합하여 다중 모달(multimodal) 작업을 해결합니다 ( paper / code ). 핵심 아이디어는 추론 과정에서 언어 및 시각 구성 요소가 독립적으로 작동하거나 단순히 서로를 반영하는 대신 상호 보완해야 한다는 것입니다. ThinkMorph 모델은 약 24,000개의 고품질 인터리브 텍스트-이미지 추론 흔적(다양한 시각적 개입 정도를 가진 작업을 포함) 예시로 미세 조정(fine-tuned)되었습니다. 텍스트적 사고와 시각적 사고가 각자 가장 잘하는 것을 해야 한다는 원칙에 따라, ThinkMorph는 일관된 언어적 논리(verbal logic)를 유지하면서 시각적 콘텐츠를 "구체적으로 조작"(예: 이미지에 그림을 그리거나 변환을 상상)하는 추론 사슬을 생성하는 방법을 학습합니다. 이는 단순히 여러 데이터를 동시에 처리하는 것을 넘어, 각 양식의 강점을 활용하여 더욱 심층적인 추론과 이해를 가능하게 합니다. 미래에는 촉각, 후각 등 더 다양한 감각 정보가 통합되어 AI의 인지 능력이 더욱 확장될 것으로 기대됩니다.

**향상된 다중 모달 성능:**
여러 시각 중심 벤치마크에서 ThinkMorph는 기본 모델 대비 평균 **34.7% 향상된 큰 성능 향상**을 제공합니다. 심지어 (훈련에서 보지 못한) 완전히 새로운 작업에도 일반화(generalizes)되며, 종종 훨씬 더 큰 독점 시각-언어 모델의 성능과 일치하거나 능가합니다. 오픈소스(open-source) 생태계는 AI 혁신을 가속화하는 핵심 동력이며, 이는 더 나은 추론 능력을 가진 70억 규모의 오픈 모델이 다중 모달 CoT를 효과적으로 사용하여 자신보다 몇 배나 큰 모델과 경쟁할 수 있음을 시사합니다. 이는 소규모 연구팀이나 개인 개발자도 최첨단 멀티모달 AI 연구에 참여하고, 새로운 애플리케이션을 개발할 수 있는 기회를 제공합니다. 오픈소스 모델은 투명성을 높이고, 공동 연구를 촉진하며, AI 기술의 민주화를 이끄는 중요한 역할을 합니다.

**대규모 AI 모델의 창발적 능력 탐구**
대규모 AI 모델에서는 훈련 데이터나 설계 의도를 넘어 예상치 못한 능력이 나타나기도 합니다. 단순 정확도를 넘어, ThinkMorph는 놀라운 **창발적 다중 모달 지능(emergent multimodal intelligence)**을 보여줍니다. 저자들은 이 모델이 이전에 보지 못했던 시각 조작 기술(예: 이미지 변환 수행 또는 명시적으로 훈련되지 않은 공간 추론)을 학습했다고 보고합니다. 이 모델은 추론에서 모달리티(modalities) 간에 적응적으로 전환할 수 있으며(이미지 기반 추론과 텍스트 중 언제 사용할지 앎), 다양화된 다중 모달 사고를 생성하여 더 많은 추론 시간(inference time)이 허용될 때 향상된 성능을 보여줍니다. 이러한 행동은 인터리브 추론에 고유한 새로운 종류의 능력을 암시합니다. 이 연구는 CoT에서 모달리티를 신중하게 혼합하는 것이 결과를 향상시킬 뿐만 아니라, 어떤 모달리티도 단독으로 달성할 수 없었던 새로운 문제 해결 전략을 가능하게 한다는 것을 시사합니다. 이러한 창발적 능력(emergent properties)은 모델이 복잡한 패턴을 학습하고, 추상적인 개념을 이해하며, 심지어 새로운 문제 해결 전략을 스스로 찾아내는 등, 인간 지능의 특정 측면을 모방하는 방식으로 나타날 수 있습니다. 이러한 현상에 대한 연구는 AI의 잠재력을 이해하고 통제하는 데 중요한 단서를 제공합니다.

**에이전트 AI: 복잡한 작업을 위한 자율 시스템**
최근 AI 연구의 중요한 방향 중 하나는 복잡한 작업을 자율적으로 수행하는 에이전트(agent) 시스템 개발입니다. 알리바바의 Tongyi DeepResearch는 장기적이고 심층적인 정보 탐색 연구 작업을 위해 맞춤 제작된 305억 매개변수 "에이전트적(agentic)" 대규모 언어 모델(large language model)입니다 ( paper / code ). 이 모델의 핵심은 LLM이 복잡한 다단계 추론(multi-step reasoning)과 웹 상호작용을 통해 정보를 수집할 수 있는 **자율적인 연구 에이전트(autonomous research agent)** 역할을 하도록 하는 데 있습니다. 이러한 능력을 훈련하기 위해 팀은 두 가지 특별한 단계, 즉 **에이전트적 중간 훈련(agentic mid-training)**과 **에이전트적 후반 훈련(agentic post-training)**을 포함하는 종단 간 프레임워크를 개발했습니다. 본질적으로, 기본 LLM 사전 훈련(pre-training) 후, 모델은 연구 시나리오를 시뮬레이션하는 맞춤형 상호작용 환경에서 추가로 훈련됩니다. 이 모든 과정은 사람 주석자(human annotators) 없이 고도로 확장 가능한 자동 데이터 합성 파이프라인(automatic data synthesis pipeline)을 사용하여 이루어집니다. 각 훈련 단계에 맞춤화된 시뮬레이션 환경을 구축함으로써, Tongyi DeepResearch는 연구 목표를 추구하는 과정에서 안정적이고 일관된 장기 상호작용(예: 문서 탐색, 후속 질문, 증거 검색)을 수행하는 방법을 학습합니다. 이러한 에이전트 AI는 단순한 질의응답을 넘어, 웹 검색, 데이터 분석, 보고서 작성 등 여러 단계를 거쳐야 하는 복합적인 문제를 스스로 해결할 수 있으며, 이는 AI가 더욱 능동적이고 독립적인 역할을 수행할 수 있음을 보여줍니다.

**MoE 아키텍처를 통한 효율적인 확장**
대규모 AI 모델의 효율적인 확장을 위해 전문가 혼합(Mixture-of-Experts, MoE)과 유사한 아키텍처를 사용합니다. Tongyi DeepResearch는 총 305억 개의 매개변수를 가지고 있지만, 평균적으로 토큰당 약 33억 개만 "활성화"됩니다. 이 설계는 각 단계에서 전문가의 일부만 사용되므로 추론(inference)을 효율적으로 유지하면서 모델의 전체 용량을 확장할 수 있게 합니다. 그 결과, 매번 300억 매개변수의 전체 연산 비용을 발생시키지 않고도 광범위한 추론을 수행할 수 있는 모델이 탄생했습니다. MoE는 모델의 전체 용량을 늘리면서도 실제 추론 시에는 일부 전문가만 활성화하여 연산 비용을 절감하는 방식이며, 이는 매개변수 수가 수천억 개에 달하는 거대 모델에서도 효율적인 운영을 가능하게 하며, 더욱 크고 강력한 AI 시스템을 구축하는 데 필수적인 기술로 자리 잡고 있습니다.

**벤치마크 선두 주자 및 오픈 소스:**
이 모델은 에이전트 기반 심층 연구 벤치마크 제품군에서 최첨단 성능을 달성합니다. 예를 들어, 웹 브라우징, 장문 질의응답(long-form QA) 및 복잡한 다중 홉 추론(multi-hop reasoning)을 포함하는 작업인 Humanity’s Last Exam, BrowseComp (및 BrowseComp-ZH), WebWalkerQA, xBench-DeepSearch, FRAMES와 같은 평가에서 선두를 차지합니다. 이러한 결과는 LLM의 자율 연구 능력에서 새로운 수준의 숙련도를 보여줍니다. AI 기술의 발전과 함께 오픈소스(open-source)의 중요성은 더욱 커지고 있으며, Tongyi 팀은 에이전트적 LLM(agentic LLMs) 분야의 커뮤니티 개발을 장려하기 위해 모델 가중치(model weights), 훈련 프레임워크(training framework) 및 "완전한 솔루션"을 포함한 **전체 프로젝트를 오픈 소스(open-sourced)로 공개**했습니다. 이러한 움직임은 연구 커뮤니티의 협력을 촉진하고, 개발자들이 최신 기술에 접근하여 자신만의 혁신적인 애플리케이션을 만들 수 있도록 지원합니다. 오픈소스 AI는 기술의 투명성을 높이고, 특정 기업이나 연구 기관에 집중된 권력을 분산시키며, AI의 윤리적이고 책임감 있는 발전에 기여합니다.

**AI의 복잡한 추론 능력 평가의 새로운 지표**
Luong 외 연구진의 이 연구는 AI 모델의 고급 수학적 추론을 평가하고 개선하는 과제를 다룹니다 ( paper / code ). 핵심 통찰은 기존의 많은 수학 벤치마크가 너무 쉽거나 단답형(short answers)만 테스트하여 진정한 수학적 문제 해결 능력을 반영하지 못한다는 것입니다. 기존의 단순한 벤치마크로는 AI의 진정한 추론 능력을 측정하기 어렵다는 인식이 확산되면서, 더욱 복잡하고 다면적인 평가 방법론이 요구되고 있습니다. 한계를 뛰어넘기 위해 저자들은 **IMO-Bench**를 소개합니다. IMO-Bench는 국제 수학 올림피아드(International Mathematical Olympiad, IMO) 수준의 벤치마크 모음으로, IMO는 고등학생을 위한 가장 어려운 수학 경시 대회 중 하나입니다. IMO-Bench는 두 가지 주요 구성 요소로 이루어져 있습니다. **IMO-AnswerBench**(검증 가능한 단답형을 가진 400개의 다양한 올림피아드 문제)와 **IMO-ProofBench**(자동 평가를 위한 상세한 채점 기준표와 함께 완전한 증명 작성을 요구하는 올림피아드 수준 문제 세트)입니다. 이 벤치마크들은 IMO 난이도를 진정으로 반영하는지 확인하기 위해 최고의 수학자들에 의해 검증되었습니다. 저자들은 이처럼 도전적이고 엄격하게 평가된 벤치마크를 사용하는 것이 AI의 수학적 추론 발전을 위한 "북극성(north-star)"으로서 중요하다고 주장합니다. IMO-Bench와 같은 국제 수학 올림피아드 수준의 벤치마크는 AI가 단순한 패턴 매칭을 넘어 심층적인 논리적 사고를 할 수 있는지를 검증하는 중요한 도구가 되며, 이는 AI의 일반 인공지능(AGI)으로의 발전을 가늠하는 핵심 지표가 될 것입니다.

**기계 추론(Machine Reasoning)의 새로운 지평**
최근 연구들은 기계 추론(machine reasoning)의 주요 발전을 시사합니다. IMO-Bench를 개발 지침으로 사용하여, 팀의 모델(Gemini Deep Think이라는 별명)은 IMO 2025 경시 문제에서 "금메달" 성능을 달성한 최초의 AI 시스템이 되었습니다. 구체적으로, 그들의 모델은 IMO-AnswerBench(단답형)에서 80.0%, IMO-ProofBench(상세 증명)에서 65.7%를 기록하여, 최고의 비-Gemini 모델들을 각각 6.9%와 무려 42.4% 앞섰습니다. 이는 특히 증명 해결 능력의 도약에서 엄청난 차이이며, 기계 추론의 주요 발전을 시사합니다. 특히 수학 문제 풀이와 같은 영역에서 AI 모델은 놀라운 성과를 보여주고 있습니다. 이는 AI가 단순히 데이터를 암기하거나 패턴을 인식하는 것을 넘어, 논리적이고 단계적인 추론 과정을 수행할 수 있음을 의미합니다. 이러한 발전은 AI가 법률, 의학, 공학 등 인간의 고유한 지적 활동 영역에 더욱 깊이 관여할 수 있는 가능성을 열어줍니다.

**AI 기반 자동 평가 시스템의 혁신**
AI 시스템의 품질과 신뢰성을 보장하기 위해 자동 평가 시스템의 중요성이 커지고 있습니다. 이 분야의 발전을 촉진하기 위해, 최근 연구에서는 LLM을 사용하여 자동 증명 채점기(automatic proof grader)를 구축했습니다(채점을 위해 모델 자체의 추론 능력을 활용). 그들은 1,000개의 사람이 채점한 증명 솔루션 데이터셋인 **IMO-GradingBench**를 구축했으며, AI 기반 채점기의 점수가 인간의 판단과 잘 상관관계가 있음을 보여주었습니다. 이는 장문, 단계별 추론을 안정적으로 평가하는 데 중요한 단계입니다. 이는 AI 모델 자체가 다른 AI 모델의 성능을 평가하고 피드백을 제공하는 메타(meta) 학습의 한 형태로 볼 수 있습니다. 이러한 자동화된 평가는 대규모 데이터셋과 복잡한 작업에 대한 평가 과정을 가속화하고, 인간 평가자의 주관성을 줄여 객관적인 성능 측정을 가능하게 합니다. 저자들은 IMO-Bench를 공개하여, 단순히 정답을 맞히는 것을 넘어 인간 수학자가 하듯이 실제로 과정을 보여주는 진정으로 견고한 수학적 추론을 중심으로 커뮤니티를 결집하는 것을 목표로 합니다.

**효율적인 트랜스포머 아키텍처의 탐구**
트랜스포머(Transformer) 모델의 효율성을 높이는 연구는 AI의 실용적인 적용 가능성을 확대하는 데 중요합니다. Kimi Linear 아키텍처는 선형(저랭크) 어텐션 근사(linear (low-rank) attention approximations)의 효율성을 유지하면서도 정확도 면에서 표준 전체 어텐션(full attention)을 능가하는 새로운 어텐션 메커니즘(attention mechanism)을 제안합니다 ( paper / code ). 이는 지금까지 선형 또는 효율적인 트랜스포머(긴 컨텍스트로 확장되는)가 속도를 위해 정확도를 일부 희생해야 하는 경우가 많았기 때문에 중요합니다. Kimi Linear는 **Kimi 델타 어텐션(Kimi Delta Attention, KDA)**이라는 모듈을 도입하는데, 이는 더 세분화된 게이팅 메커니즘(gating mechanism)을 추가하여 DeltaNet의 아이디어를 확장합니다. 본질적으로 이는 하이브리드 접근 방식입니다. RNN 기반(유한 상태) 부분과 트랜스포머(Transformer) 부분이 결합되어 있으며, 게이팅(gating)은 RNN 스타일의 상태 메모리(state memory)를 더 효과적으로 사용할 수 있게 합니다. 저자들은 또한 이 아키텍처를 하드웨어 효율적으로 만드는 맞춤형 "청크 단위(chunkwise)" 연산 알고리즘을 개발했습니다. 그들은 특수한 형태의 대각선-저랭크(Diagonal-Plus-Low-Rank, DPLR) 행렬을 활용하여 연산을 대폭 줄이면서도 고전적인 델타 규칙 네트워크(delta rule networks)의 완전한 표현력을 밀접하게 모방합니다. 이는 긴 시퀀스 처리 시 발생하는 막대한 연산 비용 문제를 해결하고, 더 적은 자원으로도 고성능 AI 모델을 구동할 수 있는 길을 열어주며, 앞으로도 다양한 효율적인 어텐션 메커니즘과 트랜스포머 대체 아키텍처에 대한 연구가 활발히 이어질 것입니다.

**성능과 비용의 균형**
대규모 AI 모델을 개발하고 배포하는 과정에서 성능과 컴퓨팅 비용 사이의 균형은 항상 중요한 고려 사항입니다. 짧은 컨텍스트 작업, 100만 토큰까지의 긴 컨텍스트 작업, 심지어 강화 학습 시나리오를 포함한 다양한 설정에서의 실험에서 Kimi Linear 모델은 **전체 어텐션을 사용하는 동등한 모델보다 지속적으로 우수한 성능**을 보였습니다. 팀은 KDA와 표준 멀티 헤드 잠재 어텐션(multi-head latent attention) 레이어를 혼합하여 총 480억 개의 매개변수(토큰당 30억 개의 매개변수 활성화)를 가진 대규모 Kimi Linear 모델을 사전 훈련했습니다. 동일한 훈련 데이터와 하이퍼파라미터(hyperparameters)를 사용하여, 이 480억(30억 활성) Kimi 모델은 평가된 모든 작업에서 동일한 크기의 표준 트랜스포머를 능가했습니다. 이는 단순히 최고 성능만을 추구하는 것이 아니라, 특정 성능 목표를 달성하면서도 자원 효율성을 극대화하는 것이 중요하다는 점을 강조합니다. 최적화된 아키텍처와 훈련 전략을 통해 우리는 더 많은 사용자에게 AI 기술의 혜택을 제공할 수 있습니다.

**LLM 메모리 최적화를 위한 혁신적인 기술**
LLM의 긴 컨텍스트 처리 능력을 향상시키기 위한 메모리 최적화 기술은 지속적으로 발전하고 있습니다. 정확도 외에도 Kimi Linear는 배포(deployment) 시 훨씬 더 자원 친화적입니다. Kimi Linear 아키텍처는 바닐라 트랜스포머(vanilla Transformer)에 비해 키-값 캐시 메모리(key-value cache memory)를 **최대 75%까지 줄여주는데**, 이는 특히 긴 시퀀스(sequences) 생성 시 메모리 사용량을 획기적으로 절감합니다. 더욱이, 극도로 긴 컨텍스트(예: 100만 토큰 컨텍스트로 텍스트 생성)의 경우, 기준 전체 어텐션 모델보다 **최대 6배 빠른 디코딩 처리량(decoding throughput)**을 달성했습니다. 이러한 개선은 표준 어텐션 대신 Kimi Linear를 사용하여 더 나은 속도/메모리 사용량과 더 나은 작업 성능을 모두 얻을 수 있음을 의미하며, 이는 드문 윈-윈(win-win) 상황입니다. 이 외에도 캐시 압축, 희소성(sparsity) 활용, 그리고 스트리밍 어텐션(streaming attention)과 같은 다양한 기법들이 연구되고 있으며, 이는 LLM이 더 긴 대화나 문서를 안정적으로 처리할 수 있도록 돕습니다. 채택 및 추가 연구를 장려하기 위해, 저자들은 핵심 KDA 커널을 오픈 소스(open-sourced)로 공개하고 vLLM 라이브러리에 구현을 제공했으며, 사전 훈련된 480억 모델과 명령어 미세 조정(instruction-tuned) 체크포인트(checkpoint)를 함께 공개했습니다.

**LLM 생성 효율성 향상을 위한 새로운 접근 방식**
LLM의 생성 과정을 근본적으로 재고하여 한 번에 하나의 토큰을 디코딩하는 비효율성을 극복하는 연구가 활발합니다. 이 논문은 **연속 자기회귀 언어 모델(Continuous Autoregressive Language Models, CALM)**을 소개합니다 ( paper / code ). 이 아이디어는 이산 토큰(discrete tokens)에서 **연속 벡터(continuous vectors)**로의 패러다임 전환입니다. 다음 단어/토큰을 예측하는 대신, 모델은 전체 텍스트 덩어리를 나타내는 연속 임베딩(continuous embedding)을 예측합니다. 고충실도 신경 오토인코더(neural autoencoder)는 K개의 토큰 시퀀스를 단일 벡터로 압축하며, 이 벡터는 나중에 99.9% 이상의 정확도로 원래 토큰으로 다시 디코딩될 수 있습니다. 이를 통해 언어 시퀀스는 연속 벡터 시퀀스로 변환되며, 모델은 K개의 토큰에 해당하는 콘텐츠를 생성하는 데 단 1단계만 필요합니다(각 생성 단계의 "대역폭(bandwidth)"을 K배만큼 효과적으로 증가시킴). 이는 주어진 길이의 텍스트를 생성하는 데 필요한 순방향 전달(forward passes) 횟수를 대폭 줄입니다. CALM과 같은 새로운 패러다임은 이산적인 토큰 예측 대신 연속적인 벡터 공간에서 텍스트를 생성함으로써, 생성 속도를 획기적으로 향상시킬 잠재력을 가지고 있으며, 이러한 혁신은 실시간 대화, 대규모 콘텐츠 생성 등 속도가 중요한 애플리케이션에서 LLM의 활용성을 크게 확장할 것입니다.

**새로운 모델링 프레임워크 및 평가 프레임워크의 진화**
새로운 생성형 모델링 패러다임은 평가 방법론의 혁신을 요구합니다. 연속 예측으로 전환하려면 저자들이 **우도(likelihood)가 없는 훈련 및 평가 프레임워크**를 개발해야 했습니다. 전통적인 언어 모델은 이산 토큰 시퀀스의 우도를 최대화합니다. 반면 CALM은 확률과 디코딩(decoding) 개념이 다른 연속 공간에서 작동합니다. 이 논문은 이러한 모델을 훈련하고, 성능을 측정하며, 심지어 연속 도메인(continuous domain)에서 완전히 제어 가능한 생성(controllable generation)을 수행하는 방법을 제시합니다. 이는 기존의 이산 토큰 기반 평가 지표로는 새로운 모델의 성능을 정확히 측정하기 어렵기 때문입니다. 앞으로는 생성된 콘텐츠의 품질, 다양성, 일관성, 그리고 특정 목표 달성 여부 등을 종합적으로 평가할 수 있는 더욱 정교한 평가 프레임워크가 필요할 것입니다.

**결과 – 속도 대 성능 및 실용적인 AI를 위한 성능-연산 트레이드오프**
실험 결과 CALM은 강력한 토큰 기반 LLM과 동일한 수준의 성능을 **훨씬 적은 연산으로 달성**할 수 있음을 보여줍니다. 예를 들어, 기준 모델이 N개의 토큰을 생성하는 데 N개의 트랜스포머 단계가 필요하다면, 압축 계수 K를 가진 CALM 모델은 N/K 단계만 필요하여 상당한 속도 향상을 가져옵니다. 저자들은 성능-연산 트레이드오프(performance-compute trade-off)가 현저히 개선되었다고 보고합니다. AI 모델을 실제 환경에 적용할 때, 주어진 연산 예산에서 CALM 모델은 이산 모델보다 우수한 성능을 보입니다. 이는 **다음 벡터 예측(next-vector prediction)**이 초고효율 LLM을 향한 유망한 경로임을 확립합니다. 이 연구는 아직 초기 단계이지만(이를 위해 고정밀 텍스트 오토인코더가 필요하므로), 미래에는 단어별 생성(word-by-word generation)을 완전히 포기할 수도 있음을 시사합니다. 이는 단순히 최고 성능 모델을 구축하는 것을 넘어, 제한된 하드웨어 자원과 에너지 소비량 내에서 최적의 성능을 달성하는 것이 중요하다는 점을 시사합니다. 효율적인 모델은 더 넓은 범위의 기업과 사용자가 AI 기술에 접근할 수 있도록 하며, 지속 가능한 AI 생태계를 구축하는 데 기여합니다. 코드와 프로젝트 세부 정보가 공개되어, 커뮤니티가 이 패러다임을 기반으로 구축하도록 초대하고 있습니다.

**다중 에이전트 시스템: 복잡성 해결의 열쇠**
Chi 외 연구진은 단일 AI 에이전트가 순차적으로 사고하는 것을 넘어선 AI 문제 해결 비전을 제안합니다 ( paper ). 복잡한 현실 세계의 문제는 단일 에이전트의 능력만으로는 해결하기 어려운 경우가 많습니다. 대신, 그들은 여러 에이전트가 복잡한 문제의 다양한 측면에서 동시적이고 협력적으로 작업하는 "**에이전트적 조직(agentic organization)**"을 설명합니다. 이 패러다임에서 AI 에이전트는 팀워크를 효과적으로 조직함으로써 개인이 달성할 수 없는 결과를 얻을 수 있습니다. 저자들은 비동기적 사고(Asynchronous Thinking, AsyncThink)라는 새로운 추론 프레임워크를 통해 이 아이디어를 구현합니다. AsyncThink에서 중앙 조직자 에이전트(Organizer agent)는 프로세스를 조율합니다. 작업을 하위 쿼리(sub-queries)로 동적으로 분해하고 이를 여러 작업자 에이전트(Worker agents)에게 병렬로 할당합니다. 작업자들은 자신의 하위 작업(지식 기반 쿼리, 부분 솔루션 계산 등)을 수행하고, 조직자는 그들의 중간 결과를 최종 답변으로 병합합니다. 결정적으로, 이 사고 과정의 구조(문제를 어떻게 분할하고 언제 조율할지)는 수동으로 설계되는 대신 **강화 학습(reinforcement learning)**을 통해 자체적으로 최적화될 수 있습니다. AsyncThink와 같은 다중 에이전트 시스템은 여러 AI 에이전트가 협력하여 작업을 분담하고, 병렬적으로 처리하며, 중간 결과를 통합하여 훨씬 더 복잡하고 규모가 큰 문제를 해결할 수 있는 잠재력을 가지고 있으며, 이는 AI가 인간 사회의 다양한 난제에 대한 해결책을 제시하는 데 중요한 역할을 할 것입니다.

**실시간 AI 시스템의 중요성 및 속도/정확도 향상**
오늘날의 AI 애플리케이션은 실시간에 가까운 응답 속도를 요구합니다. 복잡한 추론 작업에 대한 실험에서 AsyncThink는 기준 병렬 처리 접근 방식(학습된 조직자 없이 여러 추론 스레드가 실행되는)에 비해 추론 지연 시간(inference latency)을 **28% 감소**시켰습니다. 하위 작업을 비동기적으로 지능적으로 스케줄링함으로써, 순수하게 순차적인 사고의 사슬(chains-of-thought)보다 문제를 더 빠르게 해결할 뿐만 아니라, 순진한 병렬 사고(naive parallel thinking)의 함정을 피합니다. 더욱이, AsyncThink를 사용할 때 수학적 추론의 정확도가 향상되었는데, 이는 학습된 조직화가 더 빠른 해결책뿐만 아니라 더 나은 해결책으로 이어진다는 것을 나타냅니다. 다중 에이전트 시스템에서 추론 지연 시간을 **28% 감소**시켰다는 결과는 이러한 요구사항을 충족시키는 데 중요한 진전입니다. 특히 자율 주행, 실시간 의료 진단, 금융 거래와 같이 지연이 치명적인 결과를 초래할 수 있는 분야에서는 AI 시스템의 처리 속도와 효율성이 핵심적인 성공 요인이 됩니다.

**일반화 및 인간-AI 협업의 미래**
주목할 만한 발견은 AsyncThink 프레임워크가 일단 훈련되면 추가 훈련 없이 완전히 새로운 작업을 해결하도록 일반화될 수 있다는 것입니다. 에이전트들은 **전이 가능한(transferable)** 추론 조직화 형태를 학습했습니다. 즉, 보지 못한 문제에 대해 효과적으로 자체 조직화할 수 있습니다. 이는 "함께 생각하는" 방법을 배우는 것과 같은 더 추상적인 기술의 출현을 시사하며, 이는 광범위하게 적용될 수 있습니다. 이 연구는 단일 모델의 크기를 확장하는 대신, 모델이 집단으로서 자신을 조직하는 방식을 확장함으로써 더 큰 지능을 달성할 수 있는 새로운 방향을 예고합니다. 궁극적으로 AI 시스템을 전문 문제 해결사로 구성된 조율된 팀으로 취급하는 한 걸음이며, 이는 AI가 인간의 조력자로서, 또는 동료로서 복잡한 프로젝트에 참여하는 미래를 예고합니다. 인간은 창의적인 통찰력과 윤리적 판단을 제공하고, AI는 데이터 분석, 패턴 인식, 반복 작업 처리 등에서 강력한 능력을 발휘하며 상호 보완적인 관계를 형성할 것입니다. 이러한 인간-AI 협업은 우리가 직면한 가장 어려운 과제들을 해결하는 데 새로운 가능성을 열어줄 것입니다.

**컨텍스트 엔지니어링 2.0: 컨텍스트 엔지니어링의 컨텍스트**
이 논문은 AI 시스템에서 "**컨텍스트 엔지니어링(context engineering)**"에 대한 포괄적인 개념적 및 역사적 관점을 제공합니다 ( paper / code ). 컨텍스트 엔지니어링은 상호작용의 컨텍스트(인간 컨텍스트든 상황적 컨텍스트든)를 깊이 이해하고 활용하여 적절하게 행동하는 AI 시스템을 설계하는 것을 의미합니다. 이는 AI가 단순히 정보를 처리하는 것을 넘어, 인간의 의도, 상황적 맥락, 그리고 미묘한 사회적 신호를 이해하고 적절하게 반응하는 능력을 의미하며, AI가 더욱 지능적이고 유용하며, 인간과 자연스럽게 상호작용할 수 있도록 만드는 핵심 기술입니다. 저자들은 이 용어가 고급 AI 에이전트 시대에 최근 인기를 얻었지만, 그 근본적인 아이디어는 **수십 년 전으로 거슬러 올라간다**고 언급합니다. 그들은 1990년대 초부터 연구자들이 기계가 컨텍스트를 파악하는 방법을 점진적으로 구축해 온 과정을 추적합니다. 비교적 단순한 컴퓨터를 사용한 초기 인간-컴퓨터 상호작용 프레임워크부터 오늘날의 정교한 인간-AI 에이전트 상호작용, 그리고 미래의 인간 수준 또는 초인간 AI 시나리오에 이르기까지 말입니다. 본질적으로, 기계가 더 지능적으로 발전함에 따라 컨텍스트에 대한 접근 방식은 뚜렷한 단계를 거쳐 진화했으며, 우리는 이제 AI 에이전트에 초점을 맞춘 새로운 단계("컨텍스트 엔지니어링 2.0")에 있습니다.

**컨텍스트 엔지니어링 정의 및 프롬프트 엔지니어링을 넘어선 컨텍스트 이해**
이 논문은 컨텍스트 엔지니어링의 공식적인 정의와 분류(taxonomy)를 제시하며, 이전 연구에 기반을 두고 그 범위를 명확히 합니다. 이 논문은 이 분야의 역사적 이정표를 검토하며, 많은 기술(메모리 네트워크(memory networks), 사용자 모델링(user modeling), 환경 시뮬레이션(environment simulation), 프롬프트 엔지니어링(prompt engineering) 등)이 AI가 작동하는 상황을 더 잘 이해하도록 돕기 위한 노력의 연속체(continuum)의 일부임을 보여줍니다. 오늘날의 관행을 이러한 역사적 맥락에 배치함으로써, 저자들은 컨텍스트 엔지니어링이 단순한 유행어가 아니라 풍부한 계보와 원칙을 가진 분야라고 주장합니다. 단순한 프롬프트 엔지니어링(prompt engineering)을 넘어, AI가 주어진 상황의 미묘한 차이를 이해하고 이에 맞춰 행동하도록 설계하는 것은 매우 중요하며, 이는 AI가 고정된 규칙에 얽매이지 않고, 동적으로 변화하는 환경에 적응하며, 사용자에게 개인화된 경험을 제공하는 데 필수적인 역량입니다.

**설계 고려 사항 및 미래 전망, 윤리적 AI 개발과 책임감 있는 배포**
중요하게도, 이 보고서는 실용적인 컨텍스트 엔지니어링을 위한 주요 설계 고려 사항을 논의합니다. 여기에는 컨텍스트 지식(contextual knowledge)을 표현하는 방법, 컨텍스트를 동적으로 업데이트하는 방법, 컨텍스트 데이터를 사용하는 데 있어서의 개인 정보 보호 및 윤리적 문제, 그리고 AI의 컨텍스트 이해를 평가하는 방법이 포함됩니다. 특히 컨텍스트 데이터를 다룰 때는 사용자의 프라이버시를 보호하고, 데이터 편향으로 인한 불공정한 결과를 방지하는 것이 중요합니다. 저자들은 연구자와 개발자들이 AI 시스템에 컨텍스트를 체계적으로 통합하는 데 도움이 될 개념적 기반을 제공하는 것을 목표로 합니다. 그들은 임시방편적인 프롬프트 조정이나 휴리스틱(heuristic) 접근 방식보다는 AI에서 컨텍스트에 대한 원칙적인 접근 방식을 옹호합니다. 이 논문은 본질적으로 커뮤니티가 컨텍스트 엔지니어링을 일류 학문으로 인식하도록 촉구하는 것입니다. 이는 인간 및 복잡한 실제 환경과 유창하게 상호작용하는 AI 에이전트를 설계하는 데 매우 중요할 것입니다. 책임감 있는 AI 개발은 기술의 잠재력을 최대한 발휘하면서도, 사회적 가치와 인류의 복지를 최우선으로 고려하는 노력을 포함하며, 이는 AI 기술이 긍정적인 방향으로 발전하고 사회에 기여할 수 있도록 하는 필수적인 요소입니다.

**마무리하며**
급변하는 AI 기술의 최전선에서, 최신 연구 동향을 이해하고 실제 적용 사례를 탐구하는 것은 매우 중요합니다. LLM Watch를 읽어주셔서 감사합니다! 이 기사가 마음에 드셨다면 '좋아요'를 누르고 동료들과 공유해주세요. 댓글을 남겨주세요. 새로운 게시물을 무료로 받고 제 작업을 지원하려면 구독해주세요. 이 글이 여러분의 AI 여정에 작은 통찰이라도 제공했기를 바랍니다. 앞으로도 AI 분야의 흥미로운 소식과 깊이 있는 분석을 통해 여러분과 함께 성장할 수 있기를 기대합니다.