**AI 전문가로 성장하는 지름길은 무엇일까요? 직접 구현해보는 경험에서 시작됩니다!**

인공지능 전문가로 성장하는 지름길은 무엇일까요? 직접 구현해보는 경험에서 시작됩니다! Towards AI에서 제공하는 실용적인 교육 과정, '초급부터 심화 LLM 개발자 마스터 과정(대략 90개 강의)'을 통해 실제 개발 역량을 강화하십시오. 이 과정은 현실 문제 해결에 대한 열망에도 불구하고 난관에 부딪혔던 과거 연구자와 개발자들이 직접 설계했습니다.

최근 몇 년간 거대 언어 모델(LLM) 분야는 폭발적인 성장을 이루며, 산업 전반에 걸쳐 혁신을 주도하고 있습니다. 이에 따라 LLM을 이해하고 실제 애플리케이션으로 구현할 수 있는 개발자의 수요는 기하급수적으로 증가하고 있습니다. 본 과정은 이러한 시대적 요구에 발맞춰 이론적 지식뿐만 아니라, 상용 수준의 애플리케이션 개발: 검색 증강 생성(RAG), 모델 최적화(fine-tuning), 자율 에이전트(agents) 구현 등 현장에서 즉시 활용 가능한 실질적인 기술을 익히는 데 초점을 맞춥니다. 필수 선행 지식: 파이썬 기초만 갖추고 있다면 누구나 시작할 수 있으며, 전문가 멘토링: Discord 채널을 통한 강사진의 실시간 지원을 통해 학습 과정의 어려움을 해소할 수 있습니다. 이 과정을 통해 여러분은 기대 성과: 검증된 서비스 배포를 이루고, 궁극적으로 AI 산업의 핵심 인재로 발돋움할 수 있을 것입니다. 만족도 보증: 한 달 이내 전액 환불 정책으로 여러분의 투자에 대한 가치를 보장합니다.

**역량 강화 및 전문성 증진**

유용한 정보: 본 교육 프로그램과 'LLM Watch' 구독은 기업의 임직원 교육 및 역량 개발 예산으로 처리될 가능성이 높습니다.
급변하는 인공지능 기술 환경 속에서 전문가로서 경쟁력을 유지하기 위해서는 끊임없는 학습이 필수적입니다. 이 과정은 최신 LLM 기술 트렌드를 반영하여 개인의 기술적 깊이를 더하고, 새로운 산업 표준에 대한 이해를 높이는 데 기여할 것입니다. 'LLM Watch'와 같은 전문 자료를 통해 최신 연구 동향을 파악하고, 본 코스를 통해 실질적인 구현 능력을 함양함으로써, 여러분은 미래 AI 시장의 리더로 성장할 수 있는 견고한 기반을 다질 수 있습니다.

**확산 방식 언어 모형(Diffusion Language Models)은 데이터 효율성이 탁월합니다 ( paper / code )**

고유한 자료가 한정된 상황에서, 확산형 언어 모델(DLM)은 학습 회차(epochs)를 늘려갈수록 같은 규모의 자기회귀(AR) 모델보다 꾸준히 더 나은 성과를 달성합니다. 연구팀은 이 현상을 **지능 역전 지점(Intelligence Crossover)**이라 명명했습니다. 이러한 역전 현상은 상이한 데이터 제약, 모델 크기, 나아가 전문가 혼합(Mixture-of-Experts) 구조와 밀집(dense) 구조에 걸쳐 일관되게 관찰됩니다. 연구자들은 DLM이 과적합(overfitting) 발생 이전에 반복되는 데이터로부터 훨씬 풍부한 정보를 뽑아낼 수 있는 반면, AR 모델은 그보다 이르게 성능 향상이 멈춘다는 사실을 확인했습니다.

확산 모델(Diffusion Models)은 본래 이미지 생성 분야에서 큰 성공을 거두었으며, 점진적인 잡음 제거 과정을 통해 데이터를 생성하는 방식으로 작동합니다. 이는 자기회귀(AR) 모델이 이전 토큰을 기반으로 다음 토큰을 순차적으로 예측하는 방식과는 근본적으로 다릅니다. DLM의 이러한 특성은 특히 의료 영상이나 특정 전문 분야 텍스트와 같이 고유 데이터 확보가 어려운 도메인에서 매우 중요합니다. 제한된 데이터만으로도 AR 모델보다 우수한 성능을 지속적으로 보여준다는 것은, 데이터 희소성이 높은 분야에서 AI 모델 개발의 새로운 지평을 열 수 있음을 의미합니다. 이는 데이터 수집 및 전처리 비용을 크게 절감하면서도 고품질의 AI 모델을 구축할 수 있는 가능성을 제시합니다.

**확산 언어 모델(DLM)의 우위 요인:**

자료가 희소한 환경에서 DLM이 뛰어난 것은 (1) **순서 비구속 모델링(Any-order modeling)**(자기회귀(AR) 모델의 고정된 인과 관계 제약을 해소), (2) **'고집적(Super-dense)' 계산**(반복적인 양방향 잡음 제거(iterative bidirectional denoising) 과정에서 토큰별로 훨씬 더 많은 부동 소수점 연산(FLOPs) 활용), (3) **내재된 몬테카를로 데이터 확장(Monte Carlo augmentation)**(학습 과정이 각 시퀀스의 다양한 잡음 주입 버전(noised versions)에 대해 자연스럽게 평균화됨) 덕분입니다. 자기회귀(AR) 입력에 잡음을 주입하는 방식도 미미한 개선을 가져오지만, 확산 언어 모델(DLM)이 상기 요소들로부터 얻는 이점과는 비교하기 어렵습니다.

각 요인을 좀 더 자세히 살펴보면, '순서 비구속 모델링'은 DLM이 텍스트 내에서 어떤 순서로든 토큰 간의 관계를 학습할 수 있게 하여, AR 모델의 엄격한 왼쪽-오른쪽 인과성 제약에서 벗어나 더욱 유연하고 포괄적인 문맥 이해를 가능하게 합니다. 이는 마치 퍼즐 조각을 순서에 상관없이 맞춰 전체 그림을 완성하는 것과 유사합니다. '고집적 계산'은 반복적인 잡음 제거 과정에서 각 토큰에 대해 깊이 있는 연산을 수행함으로써, 미세한 정보까지도 놓치지 않고 학습에 활용하게 합니다. 이는 주어진 데이터에서 최대한의 정보를 '짜내는' 과정으로 볼 수 있습니다. 마지막으로 '내재된 몬테카를로 데이터 확장'은 학습 데이터에 다양한 형태의 잡음을 주입하고 이를 평균화하여 모델의 일반화 능력을 향상시킵니다. 이는 동일한 데이터를 여러 각도에서 바라보게 하여 모델이 더욱 견고한 특징을 학습하도록 돕는 효과가 있습니다. 이러한 복합적인 장점들은 DLM이 언어 모델을 넘어 다양한 생성 모델 아키텍처에 적용될 수 있는 잠재력을 시사하며, 특히 데이터 증강이 필수적인 분야에서 혁신적인 역할을 할 수 있습니다.

**규모의 역전 현상:**

동일한 계산 자원(대략 1.5조 토큰 갱신)을 투입했을 때, 100억 개의 독점 파이썬 토큰으로 학습된 17억 개 매개변수(parameter) 확산 언어 모델(DLM)이 같은 환경에서 훈련된 자기회귀(AR) 코드 모델을 능가하는 성능을 보였습니다. 이는 확산 언어 모델(DLM)이 훨씬 방대한 독점 자료로 훈련된 최신 자기회귀(AR) 코드 생성기와 대등한 역량을 갖추게 되었음을 의미합니다.

이러한 결과는 특히 파이썬과 같이 광범위하지만 특정 도메인에서는 고유한 코드 데이터셋을 가진 경우, DLM이 AR 모델보다 훨씬 효율적인 학습 전략을 제공함을 보여줍니다. 예를 들어, 특정 기업의 내부 코드베이스나 희귀한 프로그래밍 언어로 작성된 코드에 대한 모델을 훈련할 때, 고유 데이터의 양이 제한적일 수밖에 없습니다. DLM은 이러한 환경에서 적은 고유 데이터로도 최첨단 AR 모델에 필적하는 성능을 제공함으로써, 효율적인 코드 생성, 코드 분석, 버그 수정 등 다양한 개발 도구의 발전을 가속화할 수 있습니다. 이는 모델 학습 비용과 시간을 절감하면서도 고품질의 전문화된 코드 AI를 구축할 수 있는 실질적인 방안을 제시합니다.

**적은 데이터, 큰 결과:**

주목할 만하게도, 10억 개의 매개변수를 가진 확산 언어 모델(DLM)은 단 10억 개의 학습 데이터 토큰(데이터셋을 여러 학습 주기 동안 반복 활용)만으로 HellaSwag 벤치마크에서 56%를 넘는 정확도와 MMLU에서 33%를 초과하는 정확도를 기록했습니다. 이는 같은 반복 학습 데이터셋으로 훈련된 더 큰 70억 규모의 자기회귀(AR) 모델(동일 벤치마크에서 대략 41% 및 29%를 달성)의 성과를 현저히 뛰어넘는 수치입니다. 10억 토큰 규모의 말뭉치(corpus)에 대해 480번의 학습 주기(epochs)를 거친 후에도 확산 언어 모델(DLM)은 **성능 한계(saturation)**에 도달하지 않아, 제한된 자료 속에서 지속적으로 유의미한 정보를 추출할 수 있음을 시사합니다. 특히, 이 상황에서는 검증 손실(validation loss)의 상승이 후속 작업의 정확도(downstream accuracy) 하락과 직접적인 연관성이 없다는 것이 밝혀졌습니다 (다시 말해, 검증 손실 측면에서의 '과적합'이 실제 서비스 성능에 즉각적으로 부정적인 영향을 주지 않았습니다).

이러한 결과는 AI 모델 학습에 있어 '데이터 양'만이 절대적인 기준이 아니라는 중요한 시사점을 제공합니다. 기존의 거대 언어 모델들은 '스케일링 법칙(scaling laws)'에 따라 모델 크기와 데이터 양을 늘릴수록 성능이 선형적으로 증가하는 경향을 보여왔습니다. 하지만 DLM은 제한된 데이터셋을 효율적으로 반복 학습함으로써, 훨씬 더 큰 AR 모델을 능가하는 성능을 달성할 수 있음을 보여주었습니다. 이는 특히 중소기업이나 특정 도메인의 독점 데이터셋을 가진 기관에게 매우 고무적인 소식입니다. 막대한 컴퓨팅 자원과 방대한 데이터셋이 없더라도 고성능 AI 모델을 구축할 수 있는 가능성을 열어주며, AI 개발의 문턱을 낮추는 데 기여할 수 있습니다. 또한, '검증 손실의 과적합'이 실제 성능 저하로 이어지지 않는다는 발견은 모델 평가 및 조기 중단(early stopping) 전략에 대한 새로운 관점을 제시합니다.

**코스모스: 자율적 탐구를 위한 인공지능 연구원 ( paper / demo )**

코스모스(Kosmos)는 전적으로 **독립적인 과학 탐구**를 수행하도록 고안된 혁신적인 AI 에이전트(agent)입니다. 특정 연구 목표와 데이터 집합이 주어지면, 코스모스는 최대 12시간 동안 자료 분석, 관련 문헌 탐색, **가설 수립(hypothesis generation)** 과정을 반복 수행한 후, 그 결과를 최종 보고서 형태로 요약합니다. 이전의 에이전트들이 쉽게 일관성을 상실했던 것과 다르게, 코스모스는 체계적인 '세계 모델(world model)'을 활용하여 데이터 분석 및 문헌 검색 하위 에이전트(sub-agent) 간의 정보 교환을 통해 **연속된 200단계의 에이전트 작업 실행 중에도 집중력을 잃지 않습니다.** 이러한 능력으로 코스모스는 단일 작업 수행 중 대략 42,000줄의 코드를 실행하고 약 1,500편의 연구 논문을 검토하면서도 본연의 임무를 지속할 수 있습니다. 더불어, 최종 보고서 내 모든 진술에는 코드 또는 문헌 자료의 출처를 명시하여, 그 추론 과정(reasoning)의 투명성을 확보합니다.

코스모스는 단순한 정보 검색 도구를 넘어, 자율적으로 연구를 수행하는 'AI 과학자'라는 개념을 현실화합니다. 이는 신약 개발, 기후 모델링, 재료 과학 등 복잡하고 데이터 집약적인 분야에서 과학적 발견의 속도를 획기적으로 가속화할 잠재력을 가지고 있습니다. 그러나 동시에 AI가 생성한 연구 결과의 검증 문제, 윤리적 책임 소재, 그리고 자율 AI 과학자가 인간 연구자의 역할을 어떻게 재정의할 것인가에 대한 중요한 질문을 던집니다. 예를 들어, 코스모스가 도출한 가설이나 발견에 대한 지적 재산권은 누구에게 귀속되어야 하는가, 또는 AI가 생성한 연구 결과에 오류가 있을 경우 누가 책임을 져야 하는가와 같은 논의가 필요해질 것입니다.

**탐구의 생산성:**

독립적인 분야 전문가들의 평가 결과, 코스모스(Kosmos)가 작성한 보고서의 진술 중 약 **79.4%가 사실에 부합**하는 것으로 확인되어, 그 신뢰성이 매우 높게 평가되었습니다. 코스모스(Kosmos)와 함께 작업한 연구자들은 단 한 번의 20단계 실행(대략 12시간 소요)이 약 **6개월치 분량의 인간 수작업 연구에 상응**한다고 밝혔습니다. 나아가, 유의미한 발견의 개수가 작업 주기(cycle) 수에 비례하여 거의 직선적으로 늘어나는 경향을 보여(최대 20주기까지 검증됨), 더 오랜 시간의 실행이 지속적으로 새로운 지식을 창출할 수 있음을 암시합니다.

이러한 연구 효율성은 과학 연구 분야에 혁명적인 변화를 가져올 수 있습니다. 특히 예산이나 인력이 부족한 소규모 연구실에서도 대규모 연구 프로젝트에 필적하는 성과를 낼 수 있도록 연구의 민주화를 촉진할 수 있습니다. 인간 과학자들은 반복적이고 지루한 데이터 분석이나 문헌 검토 작업에서 벗어나, 코스모스와 같은 AI 에이전트가 제공하는 통찰력을 바탕으로 더 높은 수준의 문제 해결, 창의적인 가설 설정, 그리고 실험 설계에 집중할 수 있게 될 것입니다. 이는 인간의 독창성과 AI의 효율성이 시너지를 내는 새로운 연구 패러다임을 제시합니다.

**특기할 만한 성과:**

해당 연구는 대사체학(metabolomics), 재료 공학(materials science), 뇌 과학(neuroscience), 유전학(genetics) 등 여러 학문 분야에서 코스모스(Kosmos)가 달성한 7가지 주요 발견을 상세히 다룹니다. 놀랍게도, 이들 발견 중 세 가지는 사전 출판물(preprints)이나 미공개 원고(unpublished manuscripts)(코스모스가 접근할 수 없었던)에 이미 존재했던 결과를 독자적으로 재현함으로써, 본질적으로 기존의 과학적 지식을 '재확인'하는 결과를 낳았습니다. 남은 네 가지 발견은 **전례 없는 것**이었으며, 이는 아직 세상에 공개되지 않은 새로운 과학적 지식을 제공했습니다. 이러한 성과는 코스모스(Kosmos)가 단순한 정보 검색(rote retrieval)을 넘어 독창적인 가설과 심층적인 통찰력을 창출할 수 있음을 입증하며, 인공지능이 단순한 도구를 넘어선 자율적인 과학 연구 동반자로 발전하고 있음을 보여주는 중요한 진전입니다.

코스모스가 기존의 알려진 과학적 사실을 '재발견'한 사례는 AI가 인간 연구의 '건전성 확인(sanity check)' 도구로 활용될 수 있음을 시사합니다. 즉, 인간이 놓치거나 간과했을 수 있는 부분을 AI가 독립적으로 검증함으로써 연구의 신뢰도를 높일 수 있습니다. 더욱 중요한 것은 AI가 '새로운' 과학적 지식을 창출했다는 점입니다. 이는 AI가 방대한 데이터 속에서 인간이 인지하지 못했던 패턴이나 관계를 발견하고, 이를 바탕으로 독창적인 가설을 세울 수 있음을 의미합니다. 이러한 능력은 지적 재산권 및 연구 저작권에 대한 새로운 논의를 촉발할 것이며, AI가 단순한 도구 제공자를 넘어 연구 결과의 공동 저자로 인정받을 수도 있는 미래를 상상하게 합니다.

**멤서처: 종단 간 강화 학습(End-to-End Reinforcement Learning)으로 거대 언어 모델(LLM)의 추론, 정보 탐색 및 기억 공간 제어 능력을 훈련시키다. ( paper / code )**

멤서처(MemSearcher)는 기억 컨텍스트(memory context)를 능동적으로 제어함으로써, 다중 턴 질의응답(multi-turn question answering)과 웹 기반 정보 탐색 작업을 더욱 효율적으로 수행하도록 설계된 거대 언어 모델(LLM) 기반 에이전트 구조(agent architecture)를 제안합니다. 기존의 검색 에이전트들은 전체 대화 이력을 프롬프트(prompt)에 포함시키거나(매우 긴 입력 길이를 대가로 문맥을 유지), 혹은 가장 최근 질의만 활용합니다(토큰은 절약되나 핵심 정보가 소실될 위험). 멤서처는 이러한 간극을 메웁니다. 매 턴마다 사용자의 현재 질문과 끊임없이 갱신되는 압축된 기억 상태를 융합합니다. 이 모델의 추론 과정은 **사고의 흐름(chain-of-thought)에 따른 추론 경로**를 생성하고, **언제 어떤 정보를 탐색할지 판단**하며, 기억 저장소를 정화하여 전체 과업 해결에 필수적인 정보만을 보존하는 것을 포함합니다. 이러한 설계는 대화 전반에 걸쳐 문맥 길이(context length)를 안정적으로 유지함으로써, 정확도를 저해하지 않으면서도 효율성을 상당 수준 증대시킵니다.

현재 대부분의 LLM은 고정된 컨텍스트 창(context window)이라는 근본적인 한계를 가지고 있습니다. 이는 대화가 길어지거나 복잡한 정보를 처리할 때, 모델이 이전의 중요한 대화 내용을 '잊어버리는' 현상을 초래합니다. 멤서처는 이러한 문제에 대한 혁신적인 해법을 제시합니다. 능동적인 기억 관리(active memory management)는 인간의 인지 과정과 유사합니다. 인간은 모든 정보를 기억하는 것이 아니라, 현재 당면한 과제에 가장 중요한 정보를 선별하고 필요에 따라 기억을 압축하거나 확장합니다. 멤서처는 이러한 방식으로 LLM이 가장 관련성 높은 정보만을 효율적으로 유지하도록 학습시켜, 길고 복잡한 대화에서도 일관성과 정확성을 유지할 수 있게 합니다. 이는 단순한 프롬프트 엔지니어링을 넘어선 시스템 수준의 개선을 의미합니다.

**종합적인 강화 학습(RL) 훈련:**

해당 에이전트는 추론, 탐색, 그리고 기억 갱신 결정에 대한 전략(policy)을 동시에 최적화하는 **다중 문맥 GRPO(multi-context GRPO)**라는 특화된 강화 학습(reinforcement learning) 알고리즘을 통해 학습됩니다. 학습 과정은 여러 가지 문맥 환경(context configurations)을 포함하는 대화 집합을 표본 추출하고, 대화 전체 수준에서 보상(rewards)을 전파하여, 멤서처(MemSearcher)가 여러 차례의 대화 단계에 걸쳐 정보를 일관적으로 다루는 방식을 습득하도록 유도합니다.

멤서처의 학습에 종단 간 강화 학습(End-to-End RL)이 필수적인 이유는 단순히 정답을 맞추는 것을 넘어, '어떻게' 추론하고 '언제' 검색하며 '무엇을' 기억할지를 스스로 결정하는 복합적인 전략을 학습해야 하기 때문입니다. 지도 학습(supervised learning)으로는 이러한 동적인 의사결정 과정을 효과적으로 가르치기 어렵습니다. '다중 문맥 GRPO'는 에이전트가 다양한 대화 상황에서 최적의 행동을 선택하도록 유도하며, 대화 전체의 성공 여부를 보상으로 삼아 장기적인 관점에서 정보 처리 전략을 개선하도록 합니다. 그러나 이러한 복합적인 다중 턴 작업에 대한 보상을 정의하는 것은 쉽지 않은 과제입니다. 단순히 최종 답변의 정확도를 넘어서, 효율적인 기억 사용, 적절한 검색 시점, 그리고 논리적인 추론 과정 자체에 대한 보상 신호를 설계하는 것이 핵심 난제입니다.

**역량 증진:**

멤서처(MemSearcher)는 강력한 기준 에이전트(baseline agent, Search-R1)와 동일한 자료로 학습되었을 때, **7가지 평가 데이터셋에서 현저한 개선**을 이루어냈습니다. 가령, 30억 개의 매개변수로 구성된 거대 언어 모델(LLM)을 기반으로 한 멤서처는 70억 개의 매개변수를 가진 기준 모델을 뛰어넘어, Qwen2.5-3B-Instruct에서 대략 **11%의 정확도 개선**과 Qwen2.5-7B-Instruct 벤치마크에서 **12%의 정확도 증진**을 시현했습니다. 실질적으로 30억 매개변수 멤서처는 70억 매개변수 기준 모델을 능가하며, 이는 더욱 정교한 기억 관리와 추론 방식이 단순히 큰 모델 규모를 압도할 수 있음을 증명합니다. 결론적으로, 이 방법론은 대화 턴당 훨씬 적은 토큰과 계산 자원을 소모하면서도 더 우수한 정확도를 제공하며, 학습된 기억 공간 제어(memory management)를 통해 거대 언어 모델(LLM) 기반 에이전트의 효율성을 극대화하는 방안을 제시합니다.

이러한 결과는 AI 연구의 중요한 전환점을 시사합니다: 즉, '무조건적인 모델 크기 확장'에서 '지능적인 설계와 효율성'으로의 전환입니다. 30억 매개변수 모델이 70억 매개변수 모델을 능가한다는 것은, 더 작고 효율적인 모델이 더 큰 모델보다 더 '스마트'하게 작동할 수 있음을 보여줍니다. 이는 특히 에지 컴퓨팅(edge computing) 환경이나 모바일 AI와 같이 자원 제약이 있는 환경에서 LLM을 배포하는 데 큰 이점을 제공합니다. 또한, 더 작은 모델은 학습 및 추론에 필요한 에너지 소비를 줄여 환경적 지속 가능성에도 기여합니다. AI 분야에서 '크기'보다는 '지능'이 중요해지는 이러한 추세는 앞으로 더욱 다양한 혁신을 이끌어낼 것으로 기대됩니다.

**씽크모프: 다중 양식 교차 사고 사슬 추론(Multimodal Interleaved Chain-of-Thought Reasoning)에서 나타나는 발현적 특성(Emergent Properties) ( paper / code )**

씽크모프(ThinkMorph)는 **교차 사고 사슬(interleaved chain-of-thought, CoT)** 개념을 선도하는 시각-언어 추론 모델로서, 텍스트와 이미지 '사고' 단계를 융합하여 다중 양식(multimodal) 과제를 해결합니다. 이 모델의 핵심 사상은 추론 과정에서 언어 및 시각적 요소들이 독립적으로 기능하거나 단순히 서로를 비추는 것이 아니라, 상호 보완적인 역할을 수행해야 한다는 점입니다. 씽크모프 모델은 대략 24,000개의 고품질 교차 텍스트-이미지 추론 경로(다양한 시각적 개입 수준을 포함하는 과업) 예시를 통해 미세 조정(fine-tuned)되었습니다. 텍스트 기반 사고와 시각 기반 사고가 각자의 강점을 최대한 활용해야 한다는 원칙에 따라, 씽크모프(ThinkMorph)는 일관된 언어적 논리(verbal logic)를 유지하면서 시각적 내용을 '구체적으로 다루는'(예: 이미지 위에 그림을 그리거나 변형을 상상하는) 추론 사슬을 생성하는 방식을 익혔습니다.

다중 모달 추론은 AI 분야에서 가장 도전적인 과제 중 하나입니다. 언어적 표현과 시각적 인지 사이의 간극을 메우고, 두 양식 간의 모호성을 해결하는 것은 매우 복잡합니다. 씽크모프의 '교차 사고 사슬(interleaved CoT)' 접근법은 인간이 문제를 해결할 때 텍스트 정보와 시각 정보를 번갈아 가며 활용하는 방식과 매우 유사합니다. 예를 들어, 어떤 현상을 설명하는 글을 읽으면서 동시에 관련 사진을 보며 이해를 돕는 것과 같습니다. 이러한 접근 방식은 AI가 더욱 직관적이고 인간과 유사한 방식으로 다중 모달 정보를 통합하고 추론할 수 있도록 합니다. 이는 단순히 두 양식을 병렬로 처리하는 것을 넘어, 상호작용을 통해 깊이 있는 이해를 가능하게 합니다.

**개선된 다중 양식 성능:**

다수의 시각 기반 벤치마크 평가에서, 씽크모프(ThinkMorph)는 기본 모델과 비교하여 평균 **34.7%의 성능 증대**라는 상당한 개선을 보였습니다. 심지어 (학습 과정에서 접하지 못한) 완전히 새로운 과업에도 일반화(generalizes)되는 능력을 보였으며, 종종 훨씬 더 큰 규모의 독점 시각-언어 모델의 성과와 동등하거나 이를 초과하기도 했습니다. 이러한 결과는 향상된 추론 역량을 갖춘 70억 규모의 공개 모델이 다중 양식 사고 사슬(multimodal CoT)을 효율적으로 활용하여, 자신보다 몇 배 더 큰 모델들과도 경쟁할 수 있음을 시사합니다.

이러한 성능 향상은 특히 오픈 소스 다중 모달 AI 커뮤니티에 큰 의미를 가집니다. 더 이상 거대한 독점 모델만이 최고 성능을 보장하는 시대가 아님을 보여주며, 중소 규모의 연구팀이나 개발자들도 혁신적인 아키텍처와 효율적인 학습 방법을 통해 고성능 시스템을 구축할 수 있는 가능성을 열어줍니다. 이는 의료 영상 분석과 같은 전문 분야에서 텍스트 보고서와 이미지를 동시에 이해해야 하는 작업, 또는 자율 주행 차량이 시각 정보와 상황 설명을 통합하여 의사결정을 내리는 것과 같은 복잡한 응용 분야에서 ThinkMorph와 같은 모델이 중요한 역할을 할 수 있음을 의미합니다.

**발현적 역량:**

단순한 정확도를 넘어, 씽크모프(ThinkMorph)는 경이로운 **발현적 다중 양식 지능(emergent multimodal intelligence)**을 드러냅니다. 연구진은 이 모델이 이전에 접하지 못했던 시각적 조작 기술(예: 이미지 변환 실행 또는 명시적으로 학습되지 않은 공간 추론)을 습득했다고 보고합니다. 이 모델은 추론 과정에서 양식(modalities) 간에 유연하게 전환할 수 있으며(이미지 기반 추론과 텍스트 기반 추론 중 어느 것을 언제 사용할지 인지), 추론 시간(inference time)이 충분히 주어질 경우 성능을 더욱 향상시키는 다양한 다중 양식 사고를 생성합니다. 이러한 작동 방식은 교차 추론에 특유한 새로운 유형의 역량을 시사합니다. 본 연구는 사고 사슬(CoT)에서 여러 양식을 신중하게 결합하는 것이 단순히 결과 개선에 그치지 않고, 어떤 양식도 단독으로는 이룰 수 없었던 새로운 문제 해결 전략을 가능하게 함을 암시합니다.

씽크모프가 보여주는 발현적 능력은 단순히 학습된 지식을 반복하는 것을 넘어, 새로운 상황에 대한 적응력과 창의적인 문제 해결 능력을 의미합니다. 이는 '강한 인공지능(strong AI)' 또는 일반 인공지능(AGI)의 핵심 요소와 연결될 수 있습니다. AI가 이미지 내에서 객체를 '구체적으로 조작'하거나, 학습되지 않은 공간 추론을 수행하는 능력은 인간-AI 상호작용을 더욱 직관적이고 자연스럽게 만들 수 있습니다. 예를 들어, AI에게 "이 방의 가구 배치를 바꿔봐"라고 말하면, AI가 시각적으로 이를 이해하고 실제로 가상 공간에서 가구를 이동시키는 등의 작업을 수행할 수 있게 되는 것입니다. 이러한 능력은 AI가 단순히 정보를 처리하는 도구가 아니라, 현실 세계를 '이해하고 조작'하는 능력을 가진 동반자로 진화할 수 있음을 보여줍니다.

**통이 딥리서치 기술 개요 ( paper / code )**

알리바바의 통이 딥리서치(Tongyi DeepResearch)는 장기적이고 심도 있는 정보 탐색 연구 활동을 위해 특별히 고안된 305억 개의 매개변수를 가진 '에이전트형(agentic)' 거대 언어 모델(large language model)입니다. 이 모델의 핵심은 거대 언어 모델(LLM)이 복잡한 다단계 추론(multi-step reasoning)과 웹 상호작용을 거쳐 정보를 수집하는 **독립적인 연구 에이전트(autonomous research agent)**의 역할을 수행하도록 하는 데 있습니다. 이러한 역량을 학습시키기 위해 연구팀은 **에이전트형 중간 학습(agentic mid-training)**과 **에이전트형 후반 학습(agentic post-training)**이라는 두 가지 특화된 단계를 포함하는 종단 간 프레임워크를 개발했습니다. 근본적으로, 기초 거대 언어 모델(LLM)의 사전 학습(pre-training)이 완료된 후, 모델은 연구 상황을 모의하는 맞춤형 상호작용 환경에서 추가적으로 학습됩니다. 이 모든 과정은 인간 주석가(human annotators)의 개입 없이, 고도로 확장 가능한 자동화된 데이터 생성 파이프라인(automatic data synthesis pipeline)을 활용하여 진행됩니다. 각 학습 단계에 최적화된 시뮬레이션 환경을 구축함으로써, 통이 딥리서치(Tongyi DeepResearch)는 연구 목표를 달성하는 과정에서 안정적이고 일관된 장기적 상호작용(예: 자료 탐색, 추가 질문 제기, 증거 수집)을 수행하는 방식을 습득합니다.

통이 딥리서치는 단순히 대화형 인공지능을 넘어, 실제 세계의 복잡한 문제를 해결하기 위한 '에이전트형' 능력의 중요성을 강조합니다. 이는 LLM이 수동적인 정보 제공자를 넘어 능동적으로 목표를 설정하고, 계획을 수립하며, 외부 도구와 상호작용하여 문제를 해결하는 자율적인 존재로 진화하고 있음을 의미합니다. 이러한 에이전트의 학습 과정에서 인간의 직접적인 감독 없이 합성 데이터를 활용하는 것은 매우 중요한 기술적 진보입니다. 실제 환경에서 에이전트의 모든 행동을 수동으로 주석 달기란 거의 불가능하기 때문입니다. 시뮬레이션 환경과 합성 데이터는 복잡한 상호작용 시나리오를 대규모로 생성하고 학습시키는 데 필수적이며, 이를 통해 에이전트의 견고성과 일반화 능력을 향상시킬 수 있습니다.

**전문가 집단(Mixture-of-Experts)의 효율성:**

통이 딥리서치(Tongyi DeepResearch)는 전문가 혼합(Mixture-of-Experts) 방식과 유사한 구조를 채택합니다. 전체 305억 개의 파라미터(parameters)를 보유하고 있으나, 평균적으로 개별 토큰(token) 처리 시 약 33억 개만이 '활성화'됩니다. 이러한 설계는 각 처리 단계에서 일부 전문가 모듈만을 활용함으로써, 추론(inference) 효율성을 유지하면서도 모델의 전체 역량을 확장할 수 있도록 합니다. 결과적으로, 매번 300억 매개변수 전체에 대한 계산 비용을 발생시키지 않으면서도 광범위한 추론을 수행할 수 있는 모델이 구현되었습니다.

Mixture-of-Experts (MoE) 아키텍처는 모델의 규모를 기하급수적으로 확장하면서도 추론 비용을 효율적으로 관리할 수 있는 혁신적인 방법입니다. 일반적인 밀집(dense) 모델은 모든 매개변수가 모든 입력에 대해 활성화되지만, MoE 모델은 특정 입력에 대해 가장 적합한 '전문가' 서브네트워크만을 활성화하여 계산 효율성을 높입니다. 이는 마치 특정 분야의 질문에 대해 해당 전문가에게만 자문을 구하는 것과 같습니다. 이러한 효율성은 통이 딥리서치와 같은 에이전트가 웹 탐색, 문서 분석, 다단계 추론 등 복잡하고 장기적인 과업을 수행하는 데 필수적입니다. 전체 모델을 매번 활성화하는 것은 엄청난 컴퓨팅 자원과 시간을 요구하므로, MoE는 이러한 제약을 극복하고 에이전트의 실제 적용 가능성을 크게 확장합니다.

**평가 지표의 선두와 개방형 공개:**

본 모델은 에이전트 기반 심층 연구 평가 지표 모음에서 최신 기술 수준의 성능을 기록했습니다. 가령, 웹 탐색, 장문 질문 답변(long-form QA), 그리고 복합적인 다중 단계 추론(multi-hop reasoning)을 포함하는 Humanity’s Last Exam, BrowseComp (및 BrowseComp-ZH), WebWalkerQA, xBench-DeepSearch, FRAMES 등의 평가에서 최고 순위를 차지했습니다. 이러한 성과는 거대 언어 모델(LLM)의 자율적인 연구 역량이 새로운 숙련도 단계에 도달했음을 입증합니다. 나아가, 통이 팀은 에이전트형 거대 언어 모델(agentic LLMs) 분야의 공동체 발전을 촉진하고자, 모델 가중치(model weights), 학습 프레임워크(training framework), 그리고 '종합적인 해결책'을 포함하는 **전체 프로젝트를 개방형 소스(open-sourced)로 공개**하였습니다. 이러한 공개는 실제 연구 과업을 수행할 수 있는 대규모 거대 언어 모델(LLM) 기반 에이전트에 대한 추가적인 연구를 위한 귀중한 자산을 제공합니다.

통이 딥리서치의 오픈 소스 공개는 인공지능 연구 커뮤니티에 매우 중요한 기여를 합니다. 일반적으로 이러한 대규모의 고성능 에이전트 모델은 소수의 대기업에 의해 독점되는 경우가 많습니다. 하지만 알리바바의 이번 결정은 전 세계 연구자들이 최첨단 에이전트 기술에 접근하고, 이를 기반으로 새로운 연구와 혁신을 추구할 수 있는 길을 열어줍니다. 이는 인공지능 기술의 민주화를 촉진하고, 투명성을 높이며, 공동체 기반의 빠른 발전을 유도할 것입니다. 오픈 소스 생태계를 통해 더 많은 연구자들이 에이전트형 LLM의 잠재력을 탐구하고, 다양한 산업 분야에 적용할 수 있는 새로운 솔루션을 개발할 것으로 기대됩니다.

**강력한 수학적 논리 능력 확보를 향해 ( paper / code )**

Luong 연구팀의 본 논문은 인공지능 모델의 고차원 수학적 추론 능력을 평가하고 발전시키는 난제를 다룹니다. 주요 핵심은 기존의 많은 수학 평가 기준들이 너무 단순하거나 단답형(short answers)만을 측정하여 진정한 수학적 문제 해결 역량을 제대로 반영하지 못한다는 점입니다. 이러한 제약을 극복하고자 연구진은 **IMO-벤치마크(IMO-Bench)**를 제시합니다. IMO-벤치마크는 국제 수학 올림피아드(International Mathematical Olympiad, IMO) 수준의 평가 문제들로 구성되어 있으며, IMO는 고등학생을 대상으로 하는 가장 난이도 높은 수학 경연 대회 중 하나입니다. IMO-벤치마크는 두 가지 핵심 요소로 구성됩니다. **IMO-답변 벤치마크(IMO-AnswerBench)**(검증 가능한 단답형을 요구하는 400개의 다양한 올림피아드 문항)와 **IMO-증명 벤치마크(IMO-ProofBench)**(자동 평가를 위한 상세한 채점 지침과 함께 완전한 증명 작성을 요구하는 올림피아드 수준 문제 세트)입니다. 이 평가 기준들은 국제 수학 올림피아드(IMO)의 실제 난이도를 충실히 반영하는지 검토하기 위해 최고 수준의 수학자들에 의해 검증을 거쳤습니다. 연구팀은 이처럼 도전적이고 엄격한 평가 벤치마크를 활용하는 것이 인공지능의 수학적 추론 능력 발전을 위한 '나침반(north-star)' 역할을 할 것이라고 강조합니다.

인공지능은 오랫동안 수학적 추론, 특히 고차원적이고 창의적인 문제 해결에서 어려움을 겪어왔습니다. 과거에는 기호적 인공지능(symbolic AI)이 논리적 추론에 강점을 보였지만, 복잡한 문제에 대한 일반화 능력이 부족했습니다. 반면 신경망 기반의 인공지능은 패턴 인식에 탁월했지만, 엄격한 논리적 증명이나 다단계 추론에는 한계가 있었습니다. IMO 문제는 단순한 계산이나 패턴 인식을 넘어, 깊이 있는 이해, 창의적인 아이디어, 그리고 논리적이고 형식적인 증명 과정을 요구하기 때문에 인공지능에게 특히 어렵습니다. 이는 문제의 재구성, 다양한 수학적 개념의 통합, 그리고 오류 없는 추론 경로를 구축하는 능력을 필요로 합니다. IMO-Bench는 이러한 난제를 정면으로 다루며, AI의 진정한 수학적 지능을 측정하는 새로운 표준을 제시합니다.

**획기적인 성취:**

IMO-벤치마크를 개발 방향으로 삼아, 해당 연구팀의 모델(코드명 '제미니 딥 씽크')은 2025년 IMO 경시 대회 문제에서 '금메달' 수준의 실적을 달성한 최초의 인공지능 시스템으로 기록되었습니다. 세부적으로 살펴보면, 이 모델은 IMO-답변 벤치마크(단답형)에서 80.0%의 점수를, IMO-증명 벤치마크(상세 증명)에서는 65.7%를 기록하여, 최고 성능의 비-제미니 모델들을 각각 6.9%와 놀랍게도 42.4%P 앞섰습니다. 이는 특히 증명 문제 해결 역량에서 막대한 격차를 보인 것으로, 기계 추론(machine reasoning) 분야의 중대한 진보를 암시합니다.

인공지능이 국제 수학 올림피아드에서 '금메달' 수준의 성과를 달성했다는 것은 인간 지능과 창의성의 본질에 대한 깊은 질문을 던집니다. 수학은 흔히 인간의 가장 고차원적인 지적 활동 중 하나로 여겨져 왔습니다. AI가 이러한 영역에서 인간 최고 수준의 능력을 보여주기 시작했다는 것은, 기계가 단순한 계산을 넘어 추상적 사고와 문제 해결 전략을 개발할 수 있음을 의미합니다. 다음 과제는 이러한 수학적 추론 능력을 더욱 일반화하여, 물리학, 공학, 또는 다른 과학 분야의 미해결 문제에 적용하고 새로운 이론을 발견하는 것입니다. AI가 수학적 직관과 창의성을 어떻게 학습하고 발전시킬 수 있는지에 대한 연구는 앞으로 인공지능의 다음 개척지가 될 것입니다.

**자동 평가 시스템과 미래 연구 방향:**

해당 분야의 진보를 가속화하고자, 연구팀은 거대 언어 모델(LLM)을 활용하여 자동 증명 평가기(automatic proof grader)를 개발했습니다(모델 자체의 추론 역량을 평가에 활용). 그들은 인간이 채점한 1,000개의 증명 해법 데이터셋인 **IMO-채점 벤치마크(IMO-GradingBench)**를 구축하였으며, 인공지능 기반 평가기의 점수가 인간 심사위원의 판단과 높은 상관관계를 보임을 입증했습니다. 이는 장문의 단계별 추론 과정을 신뢰성 있게 평가하는 데 필수적인 진전입니다. 연구진은 IMO-벤치마크를 공개함으로써, 단순히 정답을 찾아내는 것을 넘어 인간 수학자들이 그러하듯이 실제 문제 해결 과정을 명확히 제시하는, 진정으로 강력한 수학적 추론 능력 개발에 커뮤니티의 역량을 집중시키고자 합니다.

AI 기반 자동 채점 시스템은 교육 분야에 혁신적인 변화를 가져올 잠재력을 가지고 있습니다. 특히 복잡한 수학 문제나 프로그래밍 과제처럼 단계별 추론 과정이 중요한 평가에서, AI는 학생들에게 즉각적이고 개인화된 피드백을 제공할 수 있습니다. 이는 교사의 업무 부담을 줄이고, 학생들이 자신의 약점을 파악하여 학습 효율을 높이는 데 기여할 것입니다. 그러나 AI 채점의 공정성과 투명성을 확보하는 것은 중요한 과제입니다. AI가 내린 평가 결과의 근거를 명확히 제시하고, 잠재적인 편향을 제거하기 위한 지속적인 연구와 검증이 필요합니다. IMO-Bench와 같은 공개 벤치마크는 이러한 연구를 촉진하고, AI의 수학적 추론 능력을 더욱 발전시키는 데 중요한 역할을 할 것입니다.

**키미 선형: 풍부한 표현력과 효율성을 겸비한 어텐션 구조(Attention Architecture) ( paper / code )**

키미 선형(Kimi Linear) 구조는 선형(저차원) 어텐션 근사(linear (low-rank) attention approximations)의 효율성은 보존하면서도, 정확도 면에서는 일반적인 전체 어텐션(full attention)을 능가하는 새로운 주의 메커니즘(attention mechanism)을 제시합니다. 이는 지금까지 선형 또는 효율적인 트랜스포머(장문 컨텍스트로 확장 가능한)가 속도 이점을 얻기 위해 정확도를 부분적으로 희생해야 했던 경우가 많았음을 고려할 때 중요한 진전입니다. 키미 선형(Kimi Linear)은 **키미 델타 어텐션(Kimi Delta Attention, KDA)**이라는 모듈을 도입하며, 이는 더욱 정교한 게이팅 메커니즘(gating mechanism)을 추가하여 델타넷(DeltaNet)의 개념을 확장합니다. 근본적으로 이는 혼합형 접근법입니다. 순환 신경망(RNN) 기반(유한 상태) 요소와 트랜스포머(Transformer) 요소가 결합되어 있으며, 게이팅(gating)은 순환 신경망(RNN) 방식의 상태 기억 장치(state memory)를 더욱 효율적으로 활용할 수 있게 합니다. 연구팀은 또한 이 구조를 하드웨어 친화적으로 만드는 맞춤형 '청크 기반(chunkwise)' 계산 알고리즘을 개발했습니다. 그들은 특이한 형태의 대각선-저차원(Diagonal-Plus-Low-Rank, DPLR) 행렬을 활용하여 계산량을 현저히 줄이면서도, 고전적인 델타 규칙 네트워크(delta rule networks)의 완전한 표현 능력을 정교하게 모방합니다.

트랜스포머 모델의 핵심인 '어텐션(attention)' 메커니즘은 입력 시퀀스의 길이가 길어질수록 계산 복잡도가 제곱(quadratic)으로 증가하는 '제곱 병목 현상(quadratic bottleneck)'을 야기합니다. 이는 장문 컨텍스트 처리에서 막대한 컴퓨팅 자원을 요구하며, 효율성을 저해하는 주요 원인입니다. 선형 어텐션 근사 방식은 이러한 병목 현상을 해결하기 위해 계산 복잡도를 선형으로 줄이지만, 종종 전체 어텐션 대비 정확도 손실을 감수해야 했습니다. Kimi Linear는 RNN 기반의 상태 기억과 트랜스포머의 어텐션을 결합한 하이브리드 접근법을 통해 이러한 트레이드오프를 극복합니다. KDA 모듈의 정교한 게이팅 메커니즘은 두 아키텍처의 장점을 취하면서도 단점을 최소화하여, 효율성과 표현력을 동시에 확보합니다. 이는 장문 텍스트를 처리하는 LLM의 실용성을 크게 향상시킬 수 있는 중요한 발전입니다.

**전체 주의 메커니즘을 뛰어넘는 성능:**

짧은 문맥 작업, 최대 100만 토큰에 이르는 긴 문맥 작업, 심지어 강화 학습 시나리오를 포함한 여러 환경에서의 실험 결과, 키미 선형(Kimi Linear) 모델은 **일반적인 전체 어텐션(full attention)을 사용하는 동급 모델보다 꾸준히 우수한 성능**을 보여주었습니다. 연구팀은 키미 델타 어텐션(KDA)과 표준 멀티 헤드 잠재 어텐션(multi-head latent attention) 레이어를 결합하여 총 480억 개의 파라미터(parameters)(토큰당 30억 개 활성화)를 가진 대규모 키미 선형 모델을 사전 학습시켰습니다. 동일한 학습 데이터와 하이퍼파라미터(hyperparameters)를 활용하여, 이 480억(30억 활성) 키미 모델은 평가된 모든 과제에서 같은 규모의 표준 트랜스포머 모델을 뛰어넘는 성과를 보였습니다.

이러한 결과는 법률 문서 분석, 과학 문헌 검토, 장문 콘텐츠 생성 등 긴 컨텍스트를 필요로 하는 LLM 애플리케이션에 혁명적인 변화를 가져올 수 있습니다. 기존 트랜스포머 모델은 긴 컨텍스트를 처리하기 위해 엄청난 메모리와 계산 자원을 소모하거나, 컨텍스트를 잘라내는 방식으로 성능을 희생해야 했습니다. Kimi Linear는 이 문제를 해결하여, 더욱 강력하고 효율적인 LLM 구축을 가능하게 합니다. 이는 AI 모델이 더 많은 정보를 한 번에 '이해'하고 '기억'할 수 있게 함으로써, 복잡한 추론과 분석 능력을 크게 향상시킬 것입니다.

**생산성 증대:**

정확성뿐만 아니라, 키미 선형(Kimi Linear)은 배포(deployment) 단계에서 훨씬 더 적은 자원을 소모합니다. 이는 기본 트랜스포머(vanilla Transformer) 대비 키-값 캐시 메모리(key-value cache memory) 사용량을 **최대 75%까지 절감**시켜 주는데, 이는 긴 시퀀스(sequences) 처리에서 막대한 이득을 제공합니다. 게다가, 극히 긴 문맥(예: 100만 토큰 문맥을 활용한 텍스트 생성)의 경우, 기준이 되는 전체 어텐션 모델보다 **최대 6배 빠른 디코딩 처리량(decoding throughput)**을 기록했습니다. 이러한 발전은 표준 어텐션 대신 키미 선형(Kimi Linear)을 활용함으로써 더 나은 속도와 메모리 사용 효율, 그리고 향상된 작업 성능을 동시에 확보할 수 있음을 의미하며, 이는 쉽게 찾아볼 수 없는 상생(win-win)의 결과입니다. 이 기술의 채택과 추가 연구를 독려하고자, 연구팀은 핵심 KDA 커널을 개방형 소스(open-sourced)로 공개하고 vLLM 라이브러리에 구현 코드를 제공했으며, 사전 학습된 480억 모델과 명령어 기반 미세 조정(instruction-tuned) 체크포인트(checkpoint)도 함께 공개했습니다.

Kimi Linear의 효율성 향상은 LLM을 실제 서비스에 배포할 때 매우 실용적인 이점을 제공합니다. 특히 실시간 응답이 중요하거나 메모리 제약이 심한 환경(예: 모바일 기기, 임베디드 시스템)에서 LLM을 구동하는 데 큰 도움이 됩니다. 키-값 캐시 메모리 절감은 GPU 메모리 사용량을 줄여 더 많은 모델을 동시에 실행하거나, 더 큰 배치 크기로 처리할 수 있게 합니다. 또한, 디코딩 처리량의 증가는 사용자 경험을 크게 향상시키며, AI 서비스의 응답 시간을 단축시킵니다. 이러한 효율성 증대는 컴퓨팅 자원 사용을 줄여 에너지 소비를 낮추고, 이는 환경적 지속 가능성 측면에서도 긍정적인 영향을 미칩니다. 핵심 커널의 오픈 소스 공개는 커뮤니티의 참여를 유도하여 기술 발전을 가속화할 것입니다.

**연속형 자기회귀 언어 모델(Continuous Autoregressive Language Models, CALM) ( paper / code )**

본 논문은 거대 언어 모델(LLM)의 생성 방식을 근본적으로 재구성하여, 한 번에 하나의 토큰을 해독하는 비효율성을 극복하는 **연속형 자기회귀 언어 모델(Continuous Autoregressive Language Models, CALM)**을 제시합니다. 이 개념은 이산적인 토큰(discrete tokens)에서 **연속적인 벡터(continuous vectors)**로의 사고의 전환을 의미합니다. 다음 단어 또는 토큰을 예측하는 대신, 모델은 전체 텍스트 블록을 표현하는 연속 임베딩(continuous embedding)을 예측합니다. 고정밀 신경 오토인코더(neural autoencoder)는 K개의 토큰 시퀀스를 하나의 벡터로 압축하며, 이 벡터는 추후 99.9%를 넘는 정확도로 원래 토큰 형태로 다시 해독될 수 있습니다. 이를 통해 언어 시퀀스는 연속 벡터 시퀀스로 전환되며, 모델은 K개의 토큰에 해당하는 내용을 단 한 단계만으로 생성할 수 있습니다(각 생성 단계의 '대역폭(bandwidth)'을 K배만큼 효율적으로 확장). 이 방식은 특정 길이의 텍스트를 생성하는 데 요구되는 순방향 전달(forward passes) 횟수를 현저히 감소시킵니다.

전통적인 LLM은 텍스트를 이산적인 토큰(단어, 서브워드)의 시퀀스로 취급하며, 다음 토큰을 하나씩 예측하는 방식으로 텍스트를 생성합니다. 이러한 '단어별 생성(word-by-word generation)' 방식은 본질적으로 느리고 비효율적입니다. CALM은 이산적인 표현에서 벗어나 텍스트를 연속적인 벡터 공간에 임베딩함으로써, 한 번에 여러 토큰에 해당하는 정보를 압축된 벡터로 생성합니다. 이는 마치 한 문장을 통째로 번역하듯이, 더 큰 의미 단위를 한 번에 처리하여 생성 속도 병목 현상(generation speed bottleneck)을 해소하려는 시도입니다. 이러한 패러다임 전환은 LLM의 아키텍처와 생성 방식에 대한 근본적인 재고를 요구하며, 미래 초고효율 LLM의 가능성을 열어줍니다.

**혁신적인 모델링 체계:**

연속형 예측 방식으로의 전환을 위해 연구진은 **우도(likelihood) 개념이 없는 학습 및 평가 체계**를 새롭게 구축해야 했습니다. 기존 언어 모델들은 이산적인 토큰 시퀀스의 우도(likelihood)를 극대화하는 데 중점을 둡니다. 반면, 연속형 자기회귀 언어 모델(CALM)은 확률 및 해독(decoding) 개념이 상이한 연속적인 공간에서 기능합니다. 본 연구는 이러한 모델을 학습시키고, 그 성능을 측정하며, 나아가 연속 도메인(continuous domain) 내에서 완벽하게 제어 가능한 생성(controllable generation)을 수행하는 방안을 제안합니다.

우도(likelihood) 개념 없이 모델을 훈련하는 것은 손실 함수(loss function)를 정의하고 모델의 수렴을 유도하는 데 있어 새로운 도전 과제를 제시합니다. 기존의 교차 엔트로피(cross-entropy) 손실은 이산 토큰에 기반하지만, 연속 벡터 공간에서는 다른 형태의 손실 함수가 필요합니다. 또한, '확률'의 개념 자체가 연속 공간에서 다르게 해석되어야 합니다. CALM의 접근 방식은 이러한 근본적인 문제에 대한 해답을 탐구하며, 연속적인 생성의 잠재력을 보여줍니다. 연속적인 생성은 텍스트나 이미지와 같은 생성 결과물에서 이산 토큰에서 발생할 수 있는 부자연스러운 전환이나 '토큰 수준'의 아티팩트(artifacts)를 줄여, 더욱 유동적이고 자연스러운 결과물을 만들어낼 수 있습니다. 이는 특히 시, 소설, 음악과 같은 창의적인 콘텐츠 생성에서 큰 이점을 가질 수 있습니다.

**성과 – 속도와 효율성의 균형:**

실험 결과, 연속형 자기회귀 언어 모델(CALM)은 강력한 토큰 기반 거대 언어 모델(LLM)과 동등한 수준의 성능을 **훨씬 적은 계산량으로 이룰 수 있음**을 입증했습니다. 가령, 기준 모델이 N개의 토큰을 생성하기 위해 N번의 트랜스포머 단계를 거쳐야 한다면, 압축률 K를 가진 연속형 자기회귀 언어 모델(CALM)은 N/K 단계만으로 동일한 결과를 얻어 상당한 속도 개선을 이끌어냅니다. 연구팀은 성능 대비 계산 비용(performance-compute trade-off)이 크게 향상되었다고 보고합니다. 동일한 계산 예산 내에서 연속형 자기회귀 언어 모델(CALM)은 이산형 모델보다 뛰어난 성능을 시현합니다. 이는 **다음 벡터 예측(next-vector prediction)** 방식이 초고효율 거대 언어 모델(LLM) 개발을 위한 유망한 방향임을 확고히 합니다. 이 연구는 아직 초기 단계에 있지만(고정밀 텍스트 오토인코더가 필수적이기 때문), 장기적으로는 단어 단위 생성(word-by-word generation) 방식이 완전히 대체될 가능성을 시사합니다. 관련 코드와 프로젝트 상세 내용이 공개되어, 커뮤니티가 이 새로운 패러다임을 기반으로 후속 연구를 진행하도록 독려하고 있습니다.

CALM은 LLM 아키텍처 설계에 대한 우리의 사고방식을 근본적으로 바꿀 수 있는 잠재력을 가지고 있습니다. 토큰 중심의 설계에서 벗어나 연속적인 표현을 활용하는 것은 모델의 효율성뿐만 아니라 생성 결과물의 질적인 측면에서도 혁신을 가져올 수 있습니다. 만약 연속형 모델이 지배적인 생성 방식으로 자리 잡는다면, 미래의 생성형 AI는 더욱 매끄러운 전환과 더 적은 '토큰 수준'의 오류를 보여줄 것입니다. 이는 이미지, 오디오, 비디오 등 다른 모달리티의 생성 모델에도 영감을 주어, 모든 형태의 생성 AI가 더욱 자연스럽고 현실에 가까운 결과물을 만들어낼 수 있게 할 것입니다. 이러한 기술적 발전은 AI가 창의적인 영역에서 인간의 능력을 더욱 확장하는 데 기여할 것입니다.

**자율 에이전트 조직의 시대: 언어 모델을 통한 협업 체계 학습 ( paper )**

Chi 연구팀은 단일 인공지능 에이전트가 순차적으로 사고하는 방식을 뛰어넘는 인공지능 문제 해결의 비전을 제시합니다. 그 대신, 그들은 다수의 에이전트가 복합적인 문제의 여러 측면에서 동시에 협력하여 작동하는 '**자율 에이전트 조직(agentic organization)**'을 설명합니다. 이러한 패러다임에서 인공지능 에이전트는 팀워크를 효율적으로 구성함으로써 개별 에이전트가 달성하기 어려운 결과물을 창출할 수 있습니다. 연구진은 비동기적 사고(Asynchronous Thinking, AsyncThink)라는 혁신적인 추론 프레임워크를 통해 이 아이디어를 구현합니다. 비동기적 사고(AsyncThink)에서는 중앙의 조직자 에이전트(Organizer agent)가 전체 과정을 조정합니다. 과업을 하위 질의(sub-queries)로 유동적으로 분해하고, 이를 다수의 작업자 에이전트(Worker agents)에게 동시에 배정합니다. 작업자들은 각자의 하위 과업(지식 기반 질의, 부분 해법 계산 등)을 수행하며, 조직자는 그들의 중간 결과물을 최종 답변으로 통합합니다. 결정적으로, 이 사고 과정의 구조(문제 분할 방식 및 조율 시점)는 수동으로 설계되는 대신, **강화 학습(reinforcement learning)**을 통해 스스로 최적화될 수 있습니다.

이 연구는 인간 사회의 조직 구조와 집단 지성에서 영감을 받아, 인공지능이 복잡한 문제를 해결하기 위해 어떻게 협력할 수 있는지를 탐구합니다. 단일 에이전트의 한계를 넘어, 여러 에이전트가 각자의 전문성을 활용하여 병렬적으로 문제를 해결하고 그 결과를 통합하는 방식은 마치 전문 팀이 프로젝트를 수행하는 것과 유사합니다. 다중 에이전트 시스템에서 가장 큰 도전 과제는 효과적인 의사소통, 작업 분배, 그리고 발생 가능한 갈등 해결입니다. AsyncThink는 중앙 조직자 에이전트가 이러한 조율 역할을 수행하고, 강화 학습을 통해 최적의 협업 전략을 스스로 학습함으로써 이러한 난제를 해결합니다. 이는 수동적인 규칙 설정 없이도 에이전트들이 스스로 '함께 일하는 방법'을 터득하게 하는 혁신적인 접근법입니다.

**처리 속도 및 정밀도 개선:**

복합적인 추론 과업에 대한 실험에서, 비동기적 사고(AsyncThink)는 기준 병렬 처리 방식(학습된 조율자 없이 여러 추론 스레드가 작동하는) 대비 추론 지연 시간(inference latency)을 **28% 단축**했습니다. 하위 과업을 비동기적으로 지능적으로 계획함으로써, 순수하게 순차적인 사고 사슬(chains-of-thought)보다 문제를 더 신속하게 해결할 뿐만 아니라, 단순한 병렬 사고(naive parallel thinking)의 오류를 방지합니다. 게다가, 비동기적 사고(AsyncThink)를 적용했을 때 수학적 추론의 정확도가 증진되었는데, 이는 학습된 조직화 방식이 단순히 더 빠른 해법뿐 아니라 더욱 우수한 해법으로 이어진다는 점을 시사합니다.

인공지능 분야에서 속도와 정확도는 흔히 상충 관계에 있습니다. 빠른 처리 속도를 얻기 위해 정확도를 희생하거나, 높은 정확도를 위해 긴 처리 시간을 감수해야 하는 경우가 많습니다. AsyncThink는 이러한 트레이드오프를 극복하고 두 가지 이점을 동시에 달성합니다. '단순한 병렬 사고'의 함정은 여러 작업이 독립적으로 진행될 때 발생할 수 있는 중복 계산, 비효율적인 자원 할당, 또는 정보 불일치 등입니다. 지능적인 스케줄링은 이러한 문제들을 사전에 방지하고, 각 하위 작업이 최적의 시점에 필요한 정보와 자원을 얻도록 조율합니다. 결과적으로, 이는 단순히 계산 속도를 높이는 것을 넘어, 추론 과정 자체의 효율성과 신뢰성을 향상시켜 더 빠르고 더 나은 해결책을 도출하게 합니다.

**범용성:**

주목할 만한 점은 비동기적 사고(AsyncThink) 프레임워크가 일단 학습되면, 추가적인 훈련 없이도 완전히 새로운 과업들을 해결하도록 일반화될 수 있다는 것입니다. 에이전트들은 **전이 가능한(transferable)** 형태의 추론 조직화 방식을 습득했습니다. 이는 이전에 접하지 못한 문제에 대해서도 효과적으로 스스로를 조직화할 수 있음을 의미합니다. 이는 '협력적으로 사고하는' 방식과 같은 더욱 추상적인 기술의 등장을 암시하며, 이러한 기술은 광범위한 분야에 적용 가능합니다. 본 연구는 개별 모델의 규모를 키우는 대신, 모델들이 집단으로서 스스로를 조직하는 방식을 확장함으로써 더 큰 지능을 성취할 수 있는 새로운 방향을 제시합니다. 이는 인공지능 시스템을 전문 문제 해결사들로 이루어진 조율된 팀으로 간주하는 진일보한 관점입니다.

AsyncThink가 보여주는 '전이 가능한' 조직화 능력은 메타 학습(meta-learning) 또는 '학습 방법을 학습하는' 개념과 밀접하게 연결됩니다. 에이전트들이 특정 문제 해결 전략을 학습하는 것을 넘어, '어떻게 협력하고 문제를 분해할지'에 대한 상위 수준의 원칙을 터득했다는 것은 매우 중요합니다. 이러한 능력은 인공일반지능(AGI)의 비전에 한 걸음 더 다가서는 것을 의미합니다. AGI는 특정 작업에 국한되지 않고 다양한 지적 작업을 수행할 수 있는 능력을 목표로 하는데, 문제를 효과적으로 조직하고 해결하는 메타 인지 능력은 AGI의 핵심 요소입니다. 이 연구는 AI 시스템이 단순히 개별적인 능력을 향상시키는 것을 넘어, 복잡한 환경에서 스스로를 조직하고 적응하는 방식으로 지능을 확장할 수 있음을 보여줍니다.

**문맥 공학 2.0: 문맥 공학의 맥락 ( paper / code )**

본 연구는 인공지능 시스템 내에서 '**문맥 공학(context engineering)**'에 대한 포괄적인 개념적 및 연대기적 시각을 제시합니다. 문맥 공학은 상호작용의 맥락(인간적 맥락이든 상황적 맥락이든)을 깊이 이해하고 활용하여 적절하게 반응하는 인공지능 시스템을 설계하는 것을 뜻합니다. 연구진은 이 용어가 첨단 인공지능 에이전트 시대에 최근 각광받고 있지만, 그 근원적인 개념은 **수십 년 전으로 거슬러 올라간다**고 지적합니다. 그들은 1990년대 초반부터 연구자들이 기계가 문맥을 이해하는 방식을 점진적으로 발전시켜 온 과정을 추적합니다. 상대적으로 단순한 컴퓨터를 이용한 초기 인간-컴퓨터 상호작용 체계에서부터, 오늘날의 정교한 인간-인공지능 에이전트 간 상호작용, 나아가 미래의 인간 수준 또는 초인간적 인공지능 시나리오에 이르기까지. 본질적으로, 기계가 지능적으로 진화함에 따라 문맥에 대한 접근법은 명확한 단계를 거쳐 발전해왔으며, 우리는 이제 인공지능 에이전트에 초점을 맞춘 새로운 단계('문맥 공학 2.0')에 서 있습니다.

인공지능 시스템에서 '문맥(context)'의 중요성은 초기 규칙 기반 시스템에서부터 지속적으로 강조되어 왔습니다. 과거에는 문맥이 주로 명시적인 규칙이나 상태 변수를 통해 관리되었지만, 복잡한 신경망 시대에 들어서면서 문맥은 모델의 내부 상태와 외부 환경과의 상호작용을 통해 더욱 미묘하고 동적으로 표현됩니다. 특히 생성형 AI와 자율 에이전트의 등장으로 문맥의 중요성은 그 어느 때보다 커졌습니다. AI가 단순히 질문에 답하는 것을 넘어, 사용자의 의도, 이전 대화 기록, 주변 환경, 그리고 심지어 사용자의 감정까지 이해하여 적절하게 행동하려면 깊이 있는 문맥 이해가 필수적입니다. 문맥 공학은 이러한 복잡한 문맥을 AI 시스템에 효과적으로 통합하기 위한 체계적인 접근법입니다.

**문맥 공학의 개념 정립:**

본 연구는 문맥 공학의 공식적인 정의와 분류 체계(taxonomy)를 제시하며, 기존 연구들을 토대로 그 영역을 명확히 합니다. 해당 논문은 이 분야의 연대기적 주요 지점들을 탐색하며, 다양한 기술들(기억 네트워크(memory networks), 사용자 모델링(user modeling), 환경 시뮬레이션(environment simulation), 프롬프트 엔지니어링(prompt engineering) 등)이 인공지능이 기능하는 환경을 더 잘 파악하게 돕기 위한 지속적인 노력의 일환임을 보여줍니다. 현재의 관행을 이러한 역사적 배경 속에 위치시킴으로써, 연구진은 문맥 공학이 단순한 일시적 유행어가 아니라 풍부한 계보와 확고한 원칙을 지닌 학문 분야라고 역설합니다.

'문맥 공학'과 '프롬프트 엔지니어링'은 종종 혼동되지만, 이 논문은 그 차이를 명확히 합니다. 프롬프트 엔지니어링이 주로 모델 입력(prompt)을 최적화하여 원하는 출력을 얻는 기술이라면, 문맥 공학은 훨씬 더 광범위한 시스템 설계 철학을 포함합니다. 이는 단순히 입력 텍스트를 조작하는 것을 넘어, AI 시스템이 외부 환경과 상호작용하고, 장기적인 기억을 관리하며, 사용자의 상태를 모델링하는 등, 시스템 전체에 걸쳐 문맥을 통합하는 방식을 다룹니다. 즉, 문맥 공학은 AI 시스템이 '무엇을' 입력받는지뿐만 아니라 '어떻게' 그 정보를 이해하고 '어떤 환경에서' 작동하는지에 대한 포괄적인 접근을 제공합니다. 이는 AI를 더욱 강력하고 유연하게 만드는 데 필수적인 요소입니다.

**설계 시 고려 사항과 향후 발전 방향:**

중요한 점은, 본 보고서가 실제적인 문맥 공학을 위한 핵심 설계 고려 사항들을 논의한다는 것입니다. 여기에는 문맥 지식(contextual knowledge)을 나타내는 방식, 문맥을 동적으로 갱신하는 방법, 문맥 데이터 활용에 따른 개인 정보 보호 및 윤리적 난제, 그리고 인공지능의 문맥 이해도를 평가하는 방법 등이 포함됩니다. 연구진은 연구자와 개발자들이 인공지능 시스템에 문맥을 체계적으로 통합할 수 있도록 돕는 개념적 토대를 제공하는 것을 목표로 합니다. 그들은 임시적인 프롬프트 조정이나 발견적(heuristic) 접근법보다는 인공지능에서 문맥에 대한 원칙적인 접근 방식을 지지합니다. 이 논문은 본질적으로 학계와 산업계가 문맥 공학을 일류 학문 분야로 인정하도록 촉구하는 메시지입니다. 이는 인간과 복잡한 현실 환경 속에서 유연하게 상호작용하는 인공지능 에이전트를 설계하는 데 있어 극히 중요할 것입니다.

'문맥 인식 인공지능(contextual AI)'의 미래는 개인화되고, 적응적이며, 신뢰할 수 있는 AI 시스템을 향해 나아갈 것입니다. AI가 사용자의 과거 행동, 현재 상황, 그리고 미래 의도를 정확히 파악할 수 있다면, 훨씬 더 유용하고 자연스러운 상호작용이 가능해집니다. 그러나 이를 위해서는 문맥 데이터의 수집, 저장, 활용 과정에서 개인 정보 보호와 윤리적 문제에 대한 깊이 있는 고려가 필수적입니다. 누가 문맥 데이터에 접근할 수 있으며, 어떻게 사용될 것인가에 대한 명확한 정책과 기술적 안전장치가 마련되어야 합니다. 또한, AI가 단순히 문맥을 '처리'하는 것을 넘어 진정으로 '인식'하고 '이해'하는 수준에 도달하는 것은 여전히 큰 도전 과제입니다. 장기적으로 문맥 공학은 AI가 인간 사회의 복잡한 규칙과 미묘한 뉘앙스를 학습하여, 더욱 책임감 있고 공정하며, 궁극적으로 인간의 삶에 긍정적인 영향을 미치는 시스템을 구축하는 데 핵심적인 역할을 할 것입니다.

**구독 ❤️** 본 글이 유익하셨다면 '마음에 들어요'를 누르고 동료들과 공유해 주십시오. 의견을 남겨주세요. LLM Watch를 읽어주셔서 진심으로 감사합니다! 새로운 콘텐츠를 무료로 받아보고 저의 활동을 응원하시려면 구독해 주시기 바랍니다. 다음 주에는 LLM Watch 커뮤니티 포럼에서 최신 AI 연구 동향에 대한 온라인 웨비나가 예정되어 있으니 많은 참여 부탁드립니다. 구독