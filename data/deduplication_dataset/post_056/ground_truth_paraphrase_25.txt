(출처: [2, 3, 12, 14, 15, 16]) 특히 거대 언어 모델(LLM) 및 기타 첨단 모델에 대한 프롬프트 활용(prompting)은 현대 인공지능 분야에서 가장 혁신적인 발전 중 하나로 평가받고 있습니다. 자연어로 지시사항을 제시함으로써 거의 모든 이들이 대규모 신경망(neural network)의 강력한 역량을 활용하여 다양한 실질적인 과제를 수행할 수 있게 되었습니다. 이전 세대의 딥러닝(deep learning) 모델들이 인상적인 성과를 보여주었지만, LLM은 단순히 뛰어난 성능을 넘어 직관적인 사용 편의성을 제공합니다. 전문가가 아닌 일반 사용자들도 이러한 모델들과 상호작용하며 그 잠재력을 직접 경험할 수 있었고, 이는 인공지능 연구에 대한 전례 없는 관심을 불러일으켰습니다. 이러한 접근성 덕분에 LLM은 기술 전문가뿐만 아니라 다양한 분야의 사용자들에게 AI 기술을 민주화하는 데 크게 기여했습니다.

"자연어로 된 지시사항을 따를 때, 대규모 언어 모델(LLM)은 범용 컴퓨터(general-purpose computer)와 같은 놀라운 역량을 선보였습니다. 그러나 작업의 수행 능력은 모델을 제어하는 데 사용되는 프롬프트(prompt)의 품질에 크게 좌우되며, 대부분의 효과적인 프롬프트는 사람이 직접 고안했습니다." - [1]에서 발췌

프롬프트는 LLM과의 사용자 친화적인 인터페이스(interface)를 제공하지만, 이 인터페이스는 완벽하지 않습니다. 많은 모델은 프롬프트의 미세한 변화에도 과도하게 민감하게 반응합니다. 프롬프트 내 단어 선택이나 문장 구조를 조금만 바꾸어도 부정확하거나 예측 불가능한 결과가 초래될 수 있습니다. 이러한 민감성 때문에 효과적인 프롬프트를 작성하는 것은 상당한 전문 지식(domain expertise)을 요구하는 중요한 기술이 되었으며, 이는 시행착오(trial and error)를 반복하는 인간의 개입을 통해 주로 이루어집니다. 우리는 프롬프팅에 대한 이해와 대상 LLM의 특성을 바탕으로 우수한 성능을 내는 프롬프트를 찾기 위해 끊임없이 프롬프트를 수정하고 검증하는 과정을 거칩니다.

**프롬프트 자동 최적화의 부상.**
프롬프팅 과정은 많은 인력과 시간이 소요되며, 본질적으로 완벽하지 않습니다. 이러한 문제를 해결하기 위해 최근 연구들은 데이터를 기반으로 프롬프트의 품질을 알고리즘적으로 향상시키는 자동 프롬프트 최적화(automatic prompt optimization) 개념을 탐구해 왔습니다. 이 접근 방식은 여러 중요한 장점을 제공합니다.
*   최적의 프롬프트를 찾는 데 필요한 수동적인 노력을 크게 줄여줍니다.
*   프롬프트가 체계적으로 탐색되고 발견되어, 사람이 작성한 프롬프트의 성능을 능가하는 결과를 도출할 수 있습니다.
*   휴리스틱(heuristics)이나 도메인 지식(domain knowledge)에 의존하는 대신, 프롬프트의 품질을 자동으로 개선할 수 있는 객관적인 방법을 제시합니다.

이 글에서는 이 분야의 광범위한 연구 문헌을 검토하며, 더 나은 프롬프트를 생성하기 위한 가장 실용적인 기법들에 초점을 맞출 것입니다.

**사전 지식: 프롬프팅과 최적화의 교차점**
이 글은 주로 최적화(optimization)와 프롬프트 엔지니어링(prompt engineering)의 결합된 영역을 다루겠지만, 이 두 가지 개념이 각각 독립적인 연구 분야라는 점을 기억하는 것이 중요합니다. 이들 개념을 개별적으로 이해함으로써, 핵심 원리들이 어떻게 결합되어 프롬프트의 품질을 자동으로 개선하는 알고리즘(algorithm)을 만들 수 있는지 파악할 수 있을 것입니다.

**최적화란 무엇인가?**
**함수 최적화의 본질**
최적화는 특정 "목적 함수(objective function)" 내에서 가장 이상적인 해(optimal solution)나 지점(point)을 찾아내는 과정입니다. 이는 일반적으로 함수 값을 최소화(또는 최대화)하는 지점을 발견하는 것을 의미합니다. 이 최적 지점을 찾아내는 행위 자체가 "최적화"를 수행하는 것입니다. 수백 년의 역사를 지닌 최적화 분야는 방대하고 풍부하며 엄청난 가치를 지닙니다. 목적 함수를 최적화하는 아이디어는 교통량 경로 설정(routing traffic)부터 신경망(neural network) 훈련에 이르기까지 수많은 중요한 문제들을 해결하는 데 활용될 수 있습니다. 최적화에 대한 포괄적인 설명은 이 글의 범위를 벗어나지만, 관련 자료는 많이 찾아볼 수 있습니다.

**다양한 최적화 알고리즘**
수많은 최적화 알고리즘이 제안되어 왔습니다. 특히 신경망 훈련과 같은 특정 응용 분야에서도 선택할 수 있는 방대한 수의 최적화 알고리즘이 존재합니다. 이러한 다양성에도 불구하고, 최적화 알고리즘은 크게 두 가지 범주로 분류할 수 있습니다.
*   경사 기반(Gradient-based)
*   경사 없는(Gradient-free)

**경사 기반 최적화(gradient-based optimization)**는 반복적으로 i) 함수의 경사(gradient)를 계산하고 ii) 이 경사를 활용하여 현재 해를 갱신(update)하는 방식입니다. 경사는 (국소적으로) 함수 값이 증가하는 방향을 나타내므로, 경사를 이용해 함수 값이 더 높거나 낮은 영역을 탐색할 수 있습니다. 아래 그림은 이 과정을 보여줍니다. 여기서 우리는 지속적으로 경사를 계산하고 경사의 반대 방향으로 이동하여 함수를 최소화합니다(예: 경사 하강법(gradient descent)).

**경사 하강법의 개략도**

머신러닝(machine learning) 실무자들은 경사 기반 최적화 알고리즘에 가장 익숙한 경우가 많습니다. 이 알고리즘들은 역전파(backpropagation) 개념의 근간을 이루며, 신경망 및 다른 머신러닝 모델 훈련에 거의 보편적으로 사용됩니다. 경사 기반 알고리즘은 그 효과성과 효율성 덕분에 큰 인기를 얻고 있습니다! 수많은 변수(예: LLM의 매개변수(parameter))를 최적화할 때에도, 경사의 방향을 따르는 것만으로도 해를 찾는 것이 계산적으로 가능하며, 이는 (상대적으로) 적은 계산 비용으로 이루어집니다.

**경사 없는 알고리즘(gradient-free algorithms)**은 경사 정보를 전혀 사용하지 않는 최적화 알고리즘의 한 유형입니다. 예를 들어, 무차별 대입 검색(brute force search)과 언덕 오르기(hill climbing)가 두 가지 기본적인 예시입니다. 많은 경사 없는 최적화 알고리즘이 존재하지만, 가장 인기 있는 알고리즘 유형 중 하나이자 이 글에서 다룰 종류는 진화 알고리즘(evolutionary algorithms, EA)입니다. 아래를 참조하십시오.

([18]에서 발췌) 생물학적 진화(biological evolution) 개념에서 영감을 받은 EA는 후보 해(candidate solution)의 "개체군(population)"을 유지하며 반복적으로 다음 단계를 수행합니다.
*   돌연변이(mutation) 및 교차(crossover)와 같은 진화 연산자(evolutionary operator)를 통해 이 개체군(즉, 후보 해)의 구성원을 수정하여 새로운 개체군 구성원을 생성함으로써 번식 행위를 모방합니다.
*   특정 목적 함수(objective function)를 기반으로 개체군에서 가장 우수한 구성원을 선택하여 계속 진화시킵니다(즉, 적자생존(survival of the fittest)).

EA에는 많은 변형이 존재하지만 [19], 가장 일반적인 구현은 유전 알고리즘(genetic algorithms)과 차분 진화(differential evolution)입니다. 일반적으로 EA는 경사 기반 최적화 알고리즘에 비해 효율성이 떨어진다고 여겨집니다. 예를 들어, 대규모 신경망(예: LLM)을 훈련시키는 것은 경사 정보 없이는 매우 어렵습니다. 검색 공간(search space)이 워낙 넓기 때문에, 단순히 모델의 매개변수를 무작위로 변경하고 손실(loss)을 측정하며 더 나은 LLM을 찾기를 바라는 방식으로는 큰 진전을 이루기 어렵습니다.

"EA는 일반적으로 N개의 해로 구성된 초기 개체군으로 시작하여, 현재 개체군에 진화 연산자(예: 돌연변이 및 교차)를 사용하여 새로운 해를 반복적으로 생성하고 적합도 함수(fitness function)를 기반으로 개체군을 업데이트합니다." - [16]에서 발췌

그러나 EA는 여러 가지 장점도 가지고 있습니다. 예를 들어, 이 알고리즘들은 탐색(exploration)과 활용(exploitation) 사이의 균형을 매우 잘 맞춥니다. 경사 기반 알고리즘이 단일 해를 생성하는 반면, EA는 전체 개체군을 관리합니다! 이러한 특성은 특정 종류의 문제에 매우 유용하며, 신경망 아키텍처(neural network architecture) 진화 또는 컴퓨터 네트워크 토폴로지(computer network topologies) 최적화와 같은 많은 흥미로운 분야에서 EA의 실제 적용으로 이어졌습니다 [19].

**LLM을 최적화 도구로 활용하기.**
최근 연구자들은 LLM을 경사 없는 최적화 도구(gradient-free optimizers)로 활용하는 아이디어를 탐구했습니다. 이를 위해 우리는 LLM에 문제에 대한 현재의 해(해의 성능 포함)와 최적화 궤적(optimization trajectory, 즉 이전 해들의 이력)에 대한 정보를 제공합니다. 그런 다음 LLM에 프롬프트를 사용하여 문제에 대한 새롭고 (바라건대) 더 나은 해를 생성하도록 지시할 수 있습니다. 새로운 해의 성능을 측정하고 이 과정을 반복함으로써 우리는 새로운 유형의 최적화 알고리즘을 만들어냅니다. 보시다시피, 이러한 LLM 기반 최적화 기술은 프롬프트 최적화를 위해 정기적으로 사용됩니다.

([28]에서 발췌) 많은 논문이 LLM을 최적화 도구로 사용하는 주제를 다룹니다. 예를 들어, [3]의 저자들은 LLM을 사용하여 간단한 회귀 문제(regression problem)를 최적화하고 외판원 문제(traveling salesman problem)를 해결합니다. 연구자들은 또한 다음을 탐구했습니다.
*   LLM과 EA 결합 [27, 28].
*   하이퍼파라미터 최적화(hyperparameter optimization)에 LLM 사용 [29].
*   LLM으로 신경망 아키텍처 검색(neural architecture search) 자동화 [30].

"최적화 문제를 공식적으로 정의하고 프로그래밍된 솔버(solver)로 업데이트 단계를 도출하는 대신, 우리는 최적화 문제를 자연어로 설명한 다음, LLM에 문제 설명과 이전에 찾은 해를 기반으로 새로운 해를 반복적으로 생성하도록 지시합니다." - [3]에서 발췌

**프롬프트 엔지니어링의 기본 원리**
LLM의 일반적인 텍스트-투-텍스트(text-to-text) 구조는 직관적이면서도 강력합니다. 각 작업을 해결하기 위해 과학자가 데이터로 특수/협소 모델을 훈련시키는 대신, (거의) 모든 사람이 프롬프트를 통해 문제를 글로 설명하고, 이 프롬프트를 LLM에 전달함으로써 훈련 데이터 없이 (또는 최소한의 1 훈련 데이터로) 합리적인 해를 생성할 수 있습니다. 위를 참조하십시오. 프롬프트의 접근성은 비-ML 도메인 전문가도 LLM의 영향력 있는 애플리케이션(application)을 쉽게 프로토타입(prototype)하고 시연할 수 있게 함으로써 상당한 잠재력을 열어줍니다 [22].

"LLM은 프롬프트 형식에 민감한 것으로 나타났습니다. 특히, 의미적으로 유사한 프롬프트라도 성능이 크게 다를 수 있으며, 최적의 프롬프트 형식은 모델 및 작업별로 다를 수 있습니다." - [3]에서 발췌

안타깝게도 LLM은 프롬프트의 미세한 변화에 매우 민감합니다. 프롬프트를 구성하고 구조화하는 정확한 방식이 결과에 큰 영향을 미칩니다. 이 문제는 비전문가에게 LLM 프롬프팅을 어렵게 만들 수 있습니다 [21]. 결과적으로 프롬프트 엔지니어링(즉, 더 나은 프롬프트를 만드는 기술)은 합법적이고 인기 있으며 유용한 기술이 되었습니다. 이 글은 주로 프롬프트를 자동으로 개선하는 방법에 중점을 두겠지만, 여기서는 몇 가지 프롬프팅 기본 사항을 다룰 것입니다.

**모든 구성 요소를 결합한 프롬프트 예시**

**프롬프트의 핵심 구성 요소.**
프롬프트를 작성하는 데 여러 가지 접근 방식이 있지만, 어떤 종류의 프롬프트에도 일반적으로 존재하는 몇 가지 표준 구성 요소가 있습니다.
*   **지시사항(Instruction)**: 모델이 수행해야 할 작업과 기대되는 출력에 대한 텍스트 설명.
*   **예시/모범(Examples / Exemplars)**: 프롬프트에 포함된 올바른 입력-출력 쌍의 구체적인 사례(즉, 데모(demonstration)).
*   **컨텍스트(Context)**: 프롬프트에서 LLM에 제공되는 추가 정보 또는 배경 지식.
*   **입력 데이터(Input Data)**: LLM이 처리할 것으로 예상되는 실제 자료(예: 번역되거나 분류되는 문장, 요약되는 문서 등).
*   **구조/표시기(Structure / Indicators)**: 프롬프트의 각 부분을 구분하거나 체계화하기 위해 포함할 수 있는 다양한 태그(tag), 헤더(header) 또는 조직 문자열(organizational string).

이 모든 구성 요소를 사용하는 프롬프트의 예시는 위에 나와 있지만, 프롬프트가 이 모든 요소를 반드시 포함할 필요는 없습니다. 우리가 해결하려는 문제의 세부 사항이나 필요에 따라 선택적으로 활용할 수 있습니다.

**다양한 프롬프팅 기법.**
이전 글들에서 우리는 다양한 종류의 프롬프팅 기술을 광범위하게 연구했습니다.
*   **실용적인 프롬프트 엔지니어링(Practical Prompt Engineering)**: 기본 개념, 제로/퓨샷 프롬프팅(zero / few shot prompting), 지시사항 등.
*   **고급 프롬프트 엔지니어링(Advanced Prompt Engineering)**: CoT(chain of thought) 프롬프팅, CoT 프롬프팅의 변형, 컨텍스트 검색(context retrieval) 등.
*   **프롬프트 엔지니어링의 현대적 발전(Modern Advances in Prompt Engineering)**: 최신 프롬프팅 연구(예: 도구 사용(tool usage), 추론(reasoning), 프로그램 지원 프롬프트(program-aided prompts), 장문 작성(long-form writing) 등).

위에 나열된 개요 외에도 프롬프트 엔지니어링에 대해 더 많이 배울 수 있는 수많은 온라인 자료가 있습니다. 예를 들어, learn prompting, prompting guide (The Gradient 제공), 프롬프트 엔지니어링 설문조사 [23, 24, 25] 등이 있습니다.

**더 나은 프롬프트 작성을 위한 체계적인 접근.**
프롬프트 엔지니어링은 주로 시행착오에 기반한 경험 과학(empirical science)입니다. 더 나은 프롬프트를 작성하려면 지속적으로 i) 원본 프롬프트를 조정하고, ii) 새 프롬프트의 성능을 측정하며, iii) 더 나은 프롬프트를 선택해야 합니다. 또한 프롬프트가 불필요하게 복잡해지지 않도록 주의해야 합니다. 여기에는 몇 가지 이유가 있습니다.
*   복잡한 프롬프트가 제대로 작동하지 않는다면, 프롬프트의 어느 부분이 문제이고 무엇을 고쳐야 하는지 파악하기 어렵습니다.
*   복잡한 프롬프트는 일반적으로 더 길고 더 많은 토큰(token)을 소비하여 금전적 비용을 증가시킵니다.

이러한 이유로 우리는 간단한 프롬프트(예: 퓨샷(few-shot) 또는 지시 기반 프롬프트)로 프롬프트 엔지니어링 프로세스를 시작해야 합니다. 그런 다음, 퓨샷 예시를 추가하거나 CoT와 같은 고급 기술을 사용하거나 단순히 지시사항을 조정하는 방식으로 프롬프트의 복잡성을 (점진적으로) 증가시키면서 시간 경과에 따른 성능을 면밀히 관찰합니다. 이 과정을 따르면 프롬프트를 개선하고 향상된 성능으로 증가된 복잡성을 정당화할 수 있습니다. 아래를 참조하십시오. 이 과정은 우리의 사용 사례에 대해 허용 가능한 수준의 성능에 도달하는 프롬프트를 얻으면 종료됩니다.

**프롬프트 엔지니어링은 곧 최적화다!**
위에 설명된 프롬프트 엔지니어링 과정의 단계들을 살펴보면, 이것이 본질적으로 최적화 문제(optimization problem)라는 것을 깨달을 것입니다! 우리는 해(즉, 우리의 프롬프트)를 반복적으로 조정하고 새로운 해가 더 나은지 아닌지를 분석합니다. 이 설정에서 "최적화 도구(optimizer)"는 인간 프롬프트 엔지니어(prompt engineer)이며, 그들은 자신의 판단과 프롬프팅 지식을 사용하여 시도할 다음 최적의 프롬프트를 결정합니다. 그러나 프롬프트 엔지니어링이 많은 시행착오를 필요로 한다는 점을 고려할 때, 이 최적화 과정에서 인간을 자동화된 접근 방식이나 알고리즘으로 대체할 수 있는지 2 궁금할 수 있습니다.

"크고 이산적인 프롬프트 공간은 [프롬프트 엔지니어링]을 최적화하기 어렵게 만듭니다. 특히 LLM에 대한 API(Application Programming Interface) 접근만 가능한 경우에는 더욱 그렇습니다." - [3]에서 발췌

**프롬프트 최적화가 어려운 근본적인 이유.**
방금 배웠듯이, 프롬프트 엔지니어링 프로세스를 자동화하기 위해 고려할 수 있는 최적화 알고리즘에는 두 가지 주요 종류가 있습니다. 불행히도 프롬프트 최적화에 경사 기반 알고리즘을 사용하는 것은 몇 가지 이유로 인해 복잡합니다.
*   많은 LLM의 경우 API에만 접근할 수 있어 경사 정보를 수집하거나 활용하기 어렵습니다.
*   프롬프트는 이산적인 토큰으로 구성되어 있으며, 이산적인 해를 최적화하기 위해 경사 기반 알고리즘을 직접 적용하는 것은 기술적으로 복잡합니다.

이 글에서 보겠지만, 이러한 문제를 회피하고 경사 기반 최적화를 사용하여 더 나은 프롬프트를 찾는 방법도 존재합니다. 그러나 위에 언급된 이유로 인해 프롬프트 최적화를 위해서는 주로 경사 없는 알고리즘에 의존하게 됩니다. 실제로, EA는 이산 최적화 문제(discrete optimization problems)를 위한 가장 성공적이고 널리 사용되는 알고리즘 종류 중 하나입니다 [26]!

**프롬프트 최적화에 대한 초기 연구 동향**
프롬프트를 최적화하려는 아이디어는 새로운 것이 아닙니다. 프롬프트 개념이 도입되자마자 연구자들은 이 문제에 대한 탐구를 시작했습니다. 이 섹션에서는 프롬프트 최적화에 대한 초기 연구 중 일부를 살펴볼 것입니다. 이 연구는 인코더 전용(encoder-only) 언어 모델 변형과 GPT 스타일 LLM 모두에 적용되었습니다. 이 연구는 오늘날 일반적으로 사용되는 프롬프트 최적화를 위한 후속 기술에 영감을 주었습니다.

**합성 프롬프트 생성 기법**
위에서 언급했듯이, 프롬프트 내 토큰의 이산적인 특성 때문에 프롬프트를 직접 최적화하는 것은 어렵습니다. 이 문제를 해결하는 한 가지 방법은 LLM이 출력을 프롬프트로 생성하도록 훈련시키는 것입니다. 이렇게 함으로써, 프롬프트를 직접 최적화하려고 시도하는 대신, 프롬프트를 생성하는 LLM의 가중치(weights)를 훈련시킬 수 있습니다. 이는 실용적이고 일반적으로 사용되는 기술입니다. 그러나 보시다시피, 이 분야의 대부분의 연구는 사전 훈련된 LLM(pretrained LLMs)을 사용하여 프롬프트를 작성하며, 더 나은 프롬프트를 작성하도록 LLM을 명시적으로 최적화하지는 않습니다.

([7]에서 발췌) 지시 유도(Instruction induction) [7]는 사전 훈련된 LLM으로 프롬프트를 생성하는 것을 탐구한 최초의 연구 중 하나입니다. 이 기술은 해당 작업의 몇 가지 구체적인 (컨텍스트 내(in-context)) 예시를 입력으로 받아 기본 작업을 예측하는 것을 목표로 합니다. 우리는 여러 입력-출력 예시를 포함하고 "지시사항은..."이라는 시퀀스를 완성하여 LLM이 해결되는 작업을 추론하도록 요청하는 프롬프트를 만듭니다. 위를 참조하십시오.

"우리는 지시사항을 따르도록 충분히 크고 정렬된(aligned) 모델을 사용할 때, 지시사항을 생성하는 능력이 실제로 상당 부분 나타난다는 것을 발견했습니다." - [7]에서 발췌

LLM은 이 작업을 즉시 해결하지 못합니다. 예를 들어, 사전 훈련된 GPT-3 모델은 10%의 정확도(accuracy)만 달성합니다. 그러나 더 큰 모델은 지시사항을 따르도록 정렬되었을 때 이 작업에서 좋은 성능을 보이기 시작합니다. InstructGPT 모델(RLHF(인간 피드백 기반 강화 학습)를 통해 인간 지시사항을 따르도록 정렬된 초기 GPT-3 기반 LLM 변형)은 지시 유도 작업에서 거의 70%의 정확도를 달성합니다.

([8]에서 발췌) Self-Instruct [8]는 LLM을 사용하여 합성 지시 튜닝 데이터셋(synthetic instruction tuning datasets)을 생성하기 위해 제안된 최초의 프레임워크(framework) 중 하나입니다. 소수의 시드 작업(seed tasks)으로 시작하여, 이 프레임워크는 시드 작업을 입력으로 사용하여 LLM에 해결할 수 있는 새로운 작업을 생성하도록 프롬프트를 제공합니다. 그 후, 동일한 LLM을 사용하여 각 작업에 대한 구체적인 데모를 생성할 수 있으며, 이는 품질에 따라 필터링(filter)됩니다. 이 파이프라인(pipeline)의 결과는 다양한 작업에 대한 많은 입력-출력 예시를 포함하는 LLM 생성 지시 튜닝 데이터셋입니다.

"Self-Instruct는 ... 사전 훈련된 언어 모델의 자체 생성물을 활용하여 지시사항을 따르는 능력을 향상시킵니다. 우리의 파이프라인은 ... 언어 모델에서 샘플을 생성한 다음, 유효하지 않거나 유사한 샘플을 필터링한 후 이를 사용하여 원본 모델을 미세 조정(finetune)합니다." - [8]에서 발췌

많은 논문이 유사한 프레임워크를 사용하여 합성 지시사항을 생성합니다. 예를 들어, [10]의 저자들은 사전 훈련된 LLM을 사용하여 시드 프롬프트 세트를 다양화하고 LLM의 지식 기반(knowledge base)을 분석하기 위한 더 나은 프롬프트를 발견합니다. Self-Instruct의 변형도 제안되었는데, 예를 들어 데이터 품질을 향상시키면서 생성 비용을 줄이는 기술의 두 번째 버전이 있습니다.

([9]에서 발췌) WizardLM [9]는 매우 복잡한 지시 튜닝 데이터셋을 생성하는 데 특화된 Self-Instruct 스타일 프레임워크를 설계합니다. 이 방법의 핵심은 LLM을 사용하여 지시사항을 반복적으로 다시 작성하거나(즉, 진화시켜) 복잡성을 증가시키는 EvolInstruct라는 기술입니다. 주어진 지시사항을 진화시킬 수 있는 두 가지 기본적인 방법이 있습니다.
*   **심층적(In-Depth)**: 제약 조건(constraints)을 추가하고, 더 많은 추론 단계(reasoning steps)를 요구하며, 입력을 복잡하게 만드는 등으로 현재 지시사항을 더 복잡하게 만듭니다(즉, 동일한 지시사항을 유지하면서 더 복잡하게 만듭니다).
*   **광범위적(In-Breadth)**: 지시 튜닝 데이터셋의 주제 범위, 기술 범위 및 전반적인 다양성을 향상시킵니다(즉, 아직 데이터에 포함되지 않은 주제에 대한 지시사항을 생성합니다).

이러한 각 진화 유형에는 수많은 변형이 있습니다. EvolInstruct 구현에는 많은 추가 프롬프트가 활용되지만, [9]의 심층적 및 광범위적 진화에 대한 예시 프롬프트는 아래에 제공됩니다.

**진화를 위한 프롬프트 예시 ([9]에서 발췌)**

프롬프트를 진화시키기 위한 이러한 전략들을 고려할 때, 우리는 i) 지시사항 진화, ii) 지시사항 응답 생성, iii) 실행 불가능하거나 더 복잡해지지 않는 프롬프트 제거 또는 배제라는 세 단계 접근 방식을 적용할 수 있습니다. 이 단계를 따르면 Self-Instruct 프레임워크를 부트스트랩(bootstrap)하여 매우 복잡한 지시사항을 포함하는 합성 데이터셋을 생성할 수 있습니다. 아래를 참조하십시오.

([9]에서 발췌)

**소프트 프롬프트: 연속적인 프롬프트 최적화 기법**
프롬프트를 개선하려면 프롬프트 내의 단어 선택이나 지시사항을 단순히 조정하는 것으로 시작할 수 있습니다. 이 접근 방식은 "하드(hard)" 프롬프트 튜닝 3이라고 불리는데, 이는 프롬프트 내에 존재하는 단어(또는 토큰)를 이산적인 방식으로 명시적으로 변경하기 때문입니다. 위를 참조하십시오. 그러나 하드 프롬프트 튜닝만이 유일한 도구는 아닙니다. 우리는 프롬프트에 대한 소프트(soft)하고 연속적인 업데이트를 탐색할 수 있습니다.

"프롬프팅과 달리, 접두사(prefix)는 실제 토큰에 해당하지 않는 자유 매개변수(free parameters)로만 구성됩니다." - [4]에서 발췌

**접두사 튜닝(Prefix tuning) [4]**은 LLM 프롬프트에 대한 연속적인 업데이트를 탐구한 최초의 논문 중 하나였습니다. 전통적인 모델 미세 조정(model finetuning)이 다운스트림 데이터셋(downstream dataset)에 대해 모델의 모든 매개변수를 훈련시키는 반면, 접두사 튜닝은 모델 매개변수의 거의 전부를 고정시킵니다. 대신, 우리는 모델의 프롬프트에 "접두사"(또는 모델 입력의 시작 부분에 추가 토큰 벡터(token vectors) 시퀀스)를 추가하고 이 접두사의 내용을 직접 훈련시킵니다. 그러나 단순히 입력 프롬프트를 업데이트하는 대신, 접두사 튜닝은 기본 모델 내의 모든 트랜스포머 블록(transformer block) 입력에 학습 가능한 접두사를 추가합니다. 아래 그림을 참조하십시오.

**학습 가능한 접두사를 가진 트랜스포머 블록**

접두사 튜닝에서는 모델에 추가된 접두사만 훈련시킵니다. 접두사의 내용을 직접 훈련시키는 대신, [4]의 저자들은 학습 가능한 접두사를 각 계층의 입력과 연결하기 전에 순방향 변환(feed-forward transformation)을 통해 전달하는 것이 훈련을 훨씬 더 안정적으로 만든다는 것을 발견했습니다. 모델에 더 많은 학습 가능한 매개변수를 추가하는 이 기술을 "재매개변수화(reparameterization)"라고 합니다.

([5]에서 발췌) 접두사를 제외한 모든 매개변수는 훈련 중에 고정되어, 미세 조정 중에 학습되는 총 매개변수 수를 (위 그림에서 보듯이 100배 이상) 크게 줄입니다. 그럼에도 불구하고, 접두사 튜닝을 통해 다운스트림 작업 성능을 크게 향상시킬 수 있습니다. 아래를 참조하십시오.

([4]에서 발췌)

위 실험에서 접두사 튜닝이 좋은 성능을 보였지만, [4]의 미세 조정 실험에서 고려된 유일한 기본 모델은 GPT-2였으며, 이는 이러한 결과가 더 현대적인 모델과 데이터셋으로 입증되어야 함을 의미합니다.

**프롬프트 튜닝(Prompt tuning) [5]**은 접두사 튜닝 [4]의 간소화된 형태로, 동시에 제안되었습니다. 다시, 우리는 사전 훈련된 언어 모델로 시작하여 모델의 매개변수를 고정시킵니다. 모델을 훈련시키는 대신, 우리는 "소프트(soft)" 프롬프트, 즉 LLM 입력의 시작 부분에 연결되는 토큰 시퀀스를 생성합니다. 그런 다음, 경사 하강법을 통해 우리의 데이터셋에 대해 (다른 모델 매개변수와 유사하게) 훈련시켜 소프트 프롬프트를 학습합니다. 아래를 참조하십시오.

([5]에서 발췌)

접두사 튜닝과 비교하여, 프롬프트 튜닝은 접두사 토큰을 모든 계층이 아닌 입력 계층에만 추가하며, 훈련 안정성을 위해 재매개변수화 4가 필요하지 않습니다. 결과적으로 프롬프트 튜닝 중에 학습되는 총 매개변수 수는 접두사 튜닝에 비해 훨씬 적습니다.

"우리는 ... 프롬프트 튜닝만으로도 (중간 계층 접두사나 작업별 출력 계층 없이) 모델 튜닝과 경쟁할 수 있음을 보여줍니다." - [5]에서 발췌

학습 가능한 매개변수 수가 적음에도 불구하고, 프롬프트 튜닝은 놀랍도록 효과적이며 기본 LLM이 커질수록 종단 간(end-to-end) 모델 훈련의 성능을 따라잡기까지 합니다. 아래를 참조하십시오. 또한 프롬프트 튜닝의 성능은 일반적으로 수동으로 엔지니어링된 프롬프트의 성능을 능가합니다. 단, 인간의 성능은 프롬프트를 작성하는 사람에 따라 달라진다는 점을 유의해야 합니다.

([5]에서 발췌)

**소프트 프롬프트의 본질적 한계와 PEFT의 맥락.**
소프트 프롬프트는 성능 면에서 효과적이지만, 이러한 기술이 실제로는 프롬프트 엔지니어링 프로세스를 개선하는 데 아무런 기여도 하지 않는다고 주장할 수도 있습니다. 프롬프트 엔지니어링의 목표는 고려 중인 애플리케이션 도메인과 LLM에 대해 잘 작동하는 텍스트 기반 프롬프트를 발견하는 것입니다. 이 문제를 해결하는 대신, 프롬프트 및 접두사 튜닝은 프롬프트가 이산적/텍스트 기반이어야 한다는 제약을 제거합니다. 이러한 기술로 발견된 "프롬프트"는 경사 기반 최적화를 통해 학습된 연속 벡터(continuous vector)일 뿐입니다. 다른 모델 매개변수와 유사하게, 이 벡터는 인간이 직접 해석할 수 없으며 다른 LLM과 함께 사용하기 위해 전송될 수 없습니다.

"규모가 커질수록 경사 계산이 비싸지고 모델 접근이 경사 접근을 제공하지 않을 수 있는 API로 전환됨에 따라 이는 덜 실용적이 됩니다." - [1]에서 발췌

이러한 이유로 접두사 및 프롬프트 튜닝은 자동 프롬프트 최적화 전략이라기보다는 매개변수 효율적인 미세 조정(parameter efficient finetuning, PEFT) 기술에 더 가깝습니다. 더 나은 프롬프트를 검색하는 대신, 우리는 모델에 소수의 추가 매개변수를 추가하고, 사전 훈련된 모델을 고정한 채로, 더 작고 도메인별 데이터셋에 대해 이 매개변수들을 직접 훈련시킵니다. 이러한 추가 매개변수들은 모델의 내부 가중치 행렬(weight matrices)에 주입되는 대신 우연히 우리의 프롬프트에 연결될 뿐입니다. PEFT 기술에 대한 더 자세한 정보는 아래 링크에서 이 주제에 대한 제 개요를 확인하십시오.

**전문 LLM을 쉽게 훈련시키기: PEFT, LoRA, QLoRA, LLaMA-Adapter 등**
2023년 11월 27일
전체 모델을 종단 간 훈련시키는 대신, 매개변수 효율적인 미세 조정은 사전 훈련된 모델 가중치를 고정시키고 미세 조정 중에 소수의 작업별 매개변수만 조정합니다. 이러한 접근 방식은 메모리 오버헤드(memory overhead)를 크게 줄이고, 저장/배포 프로세스를 간소화하며, 더 접근하기 쉬운 하드웨어(hardware)로 LLM을 미세 조정할 수 있도록 합니다.
전체 이야기 읽기

**API 기반 접근의 도전과 해결책.**
프롬프트 및 접두사 튜닝의 또 다른 문제는 LLM에 API를 통해서만 접근할 수 있을 때는 이러한 기술을 사용할 수 없다는 것입니다. 어떤 형태의 경사 기반 훈련을 위해서는 모델의 가중치에 대한 완전한 접근이 필요하지만, LLM API는 모델의 출력에만 접근을 제공합니다. 이러한 LLM은 진정한 블랙박스(black box)입니다! 이러한 이유로 프롬프트 및 접두사 튜닝은 오픈 소스 LLM(open-source LLMs)에서만 호환되지만, 연구자들은 이 문제에 대한 해결책을 찾으려고 노력했습니다.

([6]에서 발췌) InstructZero [6]는 소프트 프롬프트와 LLM API 사이에 또 다른 LLM을 삽입합니다. 이 (오픈 소스) LLM은 소프트 프롬프트와 추가 작업 정보를 입력으로 받아 텍스트 기반 지시사항을 생성하도록 훈련될 수 있습니다. 위를 참조하십시오. 그런 다음, 생성된 지시사항은 일반적으로 LLM API로 전달될 수 있습니다. 간단히 말해, 우리는 추가 LLM을 사용하여 소프트 프롬프트를 API로 전달하기 전에 텍스트 기반 프롬프트로 "디코딩(decode)"합니다. 생성된 지시사항의 품질을 개선하기 위해 베이지안 최적화(Bayesian optimization)를 사용할 수 있습니다. 이는 다른 자동 프롬프팅 방법으로 발견된 프롬프트의 성능과 일치하거나 이를 능가하는 지시사항을 생성합니다.

**소프트 프롬프트에 대한 추가 연구 동향.**
접두사 및 프롬프트 튜닝은 가장 널리 알려진 소프트 프롬프팅 기술이지만, 이 아이디어를 고려한 다른 수많은 논문들이 있습니다. 이 분야의 다른 주목할 만한 논문들의 예시는 다음과 같습니다.
*   **소프트 프롬프트 혼합(Mixtures of Soft Prompts)**은 언어 모델에 제공된 전체 프롬프트(기존 프롬프트 또는 무작위 초기화(random initialization)에서 시작)를 미세 조정하여 소프트 프롬프트를 형성하고, 이를 다른 소프트 프롬프트와 혼합하거나 결합하여 더 나은 작업 성능을 달성합니다.
*   **WARP**는 모델의 입력에 연결하여 다른 작업을 해결할 수 있는 작업별 단어 임베딩(word embeddings)을 학습함으로써 언어 모델을 여러 다운스트림 작업에 적응시키는 문제를 다룹니다.
*   **P-Tuning**은 모델의 프롬프트에 일련의 훈련 가능한 임베딩(trainable embeddings)을 연결함으로써 프롬프트 엔지니어링의 불안정성을 해결합니다. 여기서 모델은 추가된 임베딩과 함께 미세 조정되거나 고정될 수 있습니다.
*   **PADA**는 언어 모델을 훈련시켜 먼저 문제를 해결하기 위한 도메인별 프롬프트를 예측한 다음, 이 생성된 프롬프트를 사용하여 실제로 문제를 해결함으로써 LLM이 새로운 도메인에 적응하는 능력을 향상시킵니다.

소프트 프롬프트에 대해서도 많은 기사가 작성되었습니다. Sebastian Rashka는 프롬프트 및 접두사 튜닝에 대한 자세한 설명을 작성했으며, Lilian Weng은 텍스트 생성(text generation) 및 프롬프트 엔지니어링 게시물 모두에서 이 주제를 탐구합니다.

**이산 프롬프트 최적화: 해석 가능한 프롬프트를 향하여**
([11]에서 발췌) 보시다시피, 소프트 프롬프트는 해석 가능성(interpretability), 모델 간 재사용성(reusability), API 기반 LLM에 대한 적용 가능성 측면에서 한계를 가집니다. 불행히도 이러한 한계는 프롬프팅의 여러 주요 이점을 훼손합니다. 이를 염두에 두고, 이러한 속성 중 일부를 유지하면서 프롬프트를 자동으로 최적화할 수 있는지 궁금할 수 있습니다. 프롬프팅 초기에는 특정 문제를 해결하는 데 도움이 되도록 모델의 프롬프트에 포함될 수 있는 이산적인 "트리거(trigger)" 토큰을 찾는 아이디어에 대한 논문이 발표되었습니다.

"프롬프트 작성은 시간이 많이 소요될 뿐만 아니라, 동일한 문구가 모든 모델에 효과적일지 불분명하며, 특정 문구가 원하는 정보를 이끌어내는 데 가장 적합한지 결정하는 기준이 무엇인지도 명확하지 않습니다." - [11]에서 발췌

AutoPrompt [11] 5는 이산적인 토큰 세트에 대해 경사 유도 검색(gradient-guided search)을 수행하여 LLM 프롬프트에 포함할 최적의 추가 토큰 세트를 발견합니다. 이 접근 방식은 LLM 내 지식 탐색(probing knowledge) 작업에 적용되며, 여기서 이러한 추가 토큰이 이 작업에서 더 신뢰할 수 있고 안정적인 성능을 가져온다는 것을 알 수 있습니다. 트리거 토큰을 사용함으로써, 수동 시행착오를 통해 최적의 프롬프트를 휴리스틱하게 검색하는 대신, 엄격한 (경사 기반) 검색 절차를 통해 LLM의 성능을 최적화할 수 있습니다. 경사 정보 사용으로 인해, [11]에서 AutoPrompt가 프롬프트 토큰을 편집하는 데 사용하는 훈련 절차가 불안정할 수 있음을 알 수 있습니다. 대안으로, 후속 연구에서는 강화 학습(reinforcement learning, RL)을 통해 프롬프트를 최적화하는 것을 탐구했습니다.

RL을 사용하여 프롬프트를 최적화하는 아이디어는 매우 간단합니다. 먼저, 우리는 정책 네트워크(policy network)를 정의합니다. 이는 후보 프롬프트를 출력으로 생성하는 LLM일 뿐입니다. 그런 다음, 이 정책 네트워크에 대한 보상(reward)을 생성된 프롬프트의 성능으로 정의할 수 있습니다! 이 프레임워크의 그림은 아래를 참조하십시오.

**RL을 사용하여 LLM 최적화**

이 설정은 RLHF(인간 피드백 기반 강화 학습)를 통해 LLM을 훈련시키는 것과 매우 유사합니다. 두 경우 모두, 우리의 정책은 LLM이며, 그 정책은 토큰을 생성함으로써 행동(action)을 수행합니다. 전체 시퀀스(sequence)가 생성되면, 이 시퀀스에 대한 보상을 계산합니다. RLHF에서 이 보상은 인간 선호도에 대한 대리(proxy)인 보상 모델(reward model)의 출력입니다. 프롬프트 최적화의 경우, 이 보상은 프롬프트의 성능에 의해 결정됩니다. 위를 참조하십시오.

RLPrompt [12]는 RL을 사용하여 이산 프롬프트를 최적화하는 가장 잘 알려진 논문 중 하나입니다. 프롬프트를 생성하는 정책 네트워크는 사전 훈련된 LLM으로 구현됩니다. 이 모델의 매개변수는 고정된 상태로 유지되지만, RL을 통해 훈련되는 추가적인 순방향 계층(feed-forward layer)을 모델에 추가합니다. 아래를 참조하십시오.

([12]에서 발췌)

이 정책 네트워크는 다른 LLM이 작업을 해결하기 위해 사용하는 후보 프롬프트를 생성하는 데 사용됩니다. 정책 네트워크와 프롬프트를 사용하는 LLM은 동일할 필요가 없습니다. 보상은 주어진 프롬프트로 달성된 성능(예: 분류 정확도 또는 출력의 주어진 스타일에 대한 준수)을 측정하여 도출됩니다. 여기서 보상 함수(reward function)는 LLM이 생성한 출력에 의존합니다. 이러한 이유로 RLPrompt 프레임워크 내의 보상 신호(reward signals)는 예측 불가능하고 복잡하며 최적화하기 어렵습니다. 이 문제에도 불구하고, [12]의 저자들은 다양한 보상 안정화 기술(reward stabilization techniques)을 사용하여 최적화 프로세스를 더 신뢰할 수 있게 만들 수 있음을 보여줍니다.

([12]에서 발췌)

분류(classification) 및 스타일 전이(style transfer) 작업에 적용했을 때, RLPrompt는 미세 조정 및 프롬프트 튜닝 전략 모두를 능가하는 프롬프트를 생성하는 것으로 나타났습니다. 위를 참조하십시오. 그러나 인간이 쉽게 해석할 수 있는 프롬프트를 자동으로 최적화하려는 우리의 바람은 미치지 못합니다. RLPrompt에 의해 발견된 프롬프트는 다른 LLM과 함께 사용될 때 잘 전이됨에도 불구하고, 문법적으로 틀리거나 심지어 "의미 없는 말"인 경향이 있습니다. 우리의 관점에 따라, 이 발견은 프롬프팅이 어떤 면에서는 그 자체의 언어인지 궁금하게 만들 수 있습니다.

"결과적으로 최적화된 프롬프트는 종종 문법적으로 틀린 의미 없는 텍스트입니다. 놀랍게도, 그러한 의미 없는 프롬프트는 다른 LM 간에 전이되어 상당한 성능을 유지하며, 이는 LM 프롬프팅이 인간 언어 패턴을 따르지 않을 수 있음을 시사합니다." - [12]에서 발췌

TEMPERA [13]은 인스턴스 종속 프롬프트 최적화(instance-dependent prompt optimization, 즉 각 입력에 대해 프롬프트를 동적으로 최적화하는 것) 연구에서 영감을 받아, 테스트/추론 시간(test / inference time)에 프롬프트에 적응을 만들기 위해 RL을 사용하는 다른 접근 방식을 따릅니다. 초기 프롬프트가 주어지면, 우리는 i) 지시사항 변경, ii) 퓨샷 모범(few-shot exemplars) 추가 또는 제거, iii) 버벌라이저(verbalizers) 6 수정의 세 가지 방식으로 이 프롬프트의 속성을 편집할 수 있습니다. 주어진 편집 전후의 성능 차이를 보상으로 정의함으로써, RL을 통해 이러한 편집을 수행하도록 에이전트(agent)를 훈련시킬 수 있습니다. 아래를 참조하십시오.

([13]에서 발췌)

TEMPERA는 원본 프롬프트, LLM에 대한 현재 입력 쿼리(input query), 퓨샷 모범 세트, 그리고 버벌라이저 풀(pool)을 입력으로 받아들이고, 예시 교환 또는 버벌라이저 편집과 같은 일련의 편집을 통해 최종 프롬프트를 생성하는 함수를 학습합니다. RL 관점에서 우리의 상태(state)는 현재 프롬프트로 주어지며, 우리는 프롬프트를 편집함으로써 행동을 수행할 수 있습니다. 상태를 표현하기 위해 현재 프롬프트의 텍스트를 사용할 수 있습니다. 그러나 [13]의 저자들은 이 텍스트를 사전 훈련된 LLM을 통해 전달하여 임베딩(embed)하기로 선택합니다. 이렇게 함으로써 우리는 다음을 수행하는 간단한 정책 네트워크를 구현할 수 있습니다.
*   상태/프롬프트 임베딩(embedding)을 입력으로 받습니다.
*   수행할 행동/편집을 출력으로 생성합니다.

TEMPERA는 프롬프트에 수행할 수 있는 고정된 편집 세트를 고려하므로, 이 행동 공간(action space)은 이산적입니다! 정책 네트워크를 사용하여 수행할 프롬프트 편집을 직접 예측할 수 있습니다. 주어진 편집으로 생성된 성능 향상을 보상으로 정의함으로써, PPO(Proximal Policy Optimization)와 같은 기성 RL 알고리즘으로 이 전체 시스템을 최적화할 수 있습니다. 이 접근 방식은 데이터 효율적이며, 분류에 유용하고, LLM으로 더 어려운 작업을 해결하는 데에도 영향력이 있는 것으로 나타났습니다.

([13]에서 발췌)

**RL이 이산 프롬프트 최적화에 효과적인 이유.**
RL을 사용함으로써 우리는 마침내 이산 프롬프트를 최적화하기 위한 훈련 절차를 구현할 수 있습니다. 지금까지 본 다른 접근 방식과 비교하여, RLPrompt와 같은 기술은 이를 가능하게 하기 위해 두 가지 주요 변경 사항을 적용합니다.
*   LLM으로 프롬프트를 생성합니다.
*   생성된 프롬프트의 품질을 기반으로 이 LLM을 최적화합니다.

LLM으로 프롬프트를 생성함으로써, 이산 프롬프트 대신 LLM의 (연속적인) 가중치를 최적화하는 데 집중할 수 있습니다. 이와 동일한 접근 방식은 Instruction Induction [7] 또는 Self-Instruct [8]와 같은 기술에서 사용되지만, 우리는 RL을 사용하여 모델이 더 나은 프롬프트를 생성하도록 가르침으로써 이러한 기술을 넘어섭니다. 보상을 생성하는 데 사용되는 시스템이 LLM 기반이므로 미분 가능하지 않기 때문에, 이 최적화를 수행하려면 RL이 필요합니다! 다시 말해, 최적화 목적으로 사용할 경사를 쉽게 계산할 수 없습니다.

**LLM을 활용한 프롬프트 자동 최적화: 최신 동향**
이제 프롬프트 최적화에 대한 초기 연구를 살펴보았으니, 자동 프롬프트 최적화에 대한 더 최근의 인기 있는 논문들을 알아볼 것입니다. 이 논문들 거의 모두는 프롬프트를 최적화하는 LLM의 능력에 의존합니다. LLM을 경사 없는 최적화 도구로 사용하는 것은 (논란의 여지가 있지만) 전통적이고 확립된 최적화 알고리즘에 비해 덜 엄격합니다. 그러나 이러한 접근 방식은 개념적으로 간단하고, 실제로 구현/적용하기 쉬우며, 매우 효과적이어서 LLM 기반 프롬프트 최적화 알고리즘이 상당히 인기를 얻게 되었습니다.

**대규모 언어 모델은 인간 수준의 프롬프트 엔지니어이다 [1]**
([1]에서 발췌) 효과적인 프롬프트를 작성하는 것은 많은 시행착오를 필요로 합니다. 프롬프트 엔지니어링은 블랙박스 최적화 문제(black box optimization problem)입니다. 우리는 어떤 프롬프트가 주어진 모델과 얼마나 잘 호환될지 사전에 알 수 없습니다. [1]에서는 LLM을 사용하여 인간 프롬프트 엔지니어링 프로세스를 더 효율적으로 만들려는 최초의 시도 중 하나를 볼 수 있습니다.

"우리는 이 문제를 자연어 프로그램 합성(natural language program synthesis)이라고 부르며, LLM을 사용하여 휴리스틱하게 실행 가능한 후보 해를 생성하고 검색하는 블랙박스 최적화 문제로 다루는 것을 제안합니다." - [1]에서 발췌

제안된 접근 방식인 APE(Automatic Prompt Engineer)는 LLM이 제안한 프롬프트 풀(pool)을 검색하여 가장 성능이 좋은 프롬프트를 찾습니다. 이 설정은 별도의 LLM을 사용하여 프롬프트를 제안하고 평가합니다. 평가를 위해 제로샷 추론(zero-shot inference)을 통해 출력을 생성하고, 선택된 점수 함수(scoring function)에 따라 출력을 평가합니다. 단순함에도 불구하고, APE는 [1]에서 동일한 작업에서 인간이 작성한 프롬프트의 성능과 일치하거나 이를 능가하는 프롬프트를 찾는 것으로 나타났으며, 이는 LLM이 실제로 프롬프트 작성 작업에 탁월하다는 것을 보여줍니다.

**APE를 위한 지시사항 생성 템플릿 ([1]에서 발췌)**

**APE의 작동 원리.**
APE에는 제안(proposal)과 점수 매기기(scoring)라는 두 가지 주요 작업이 있습니다. 제안을 위해 우리는 LLM에 작업에 대한 새로운 후보 프롬프트를 생성하도록 요청합니다. 위에서 예시 프롬프트 세트를 참조하십시오. 위에서 보듯이, 새로운 지시사항의 생성은 순방향(forward) 또는 역방향(reverse) 방식으로 이루어질 수 있습니다. 순방향 모드에서는 다음 토큰 예측(next token prediction)을 통해 단순히 새로운 지시사항을 생성하며, 이는 LLM으로 다른 종류의 텍스트를 생성하는 것과 동일합니다. 반면에 역방향 생성은 인필링(infilling, 즉 시퀀스 중간에 누락된 텍스트/토큰 삽입)에 기반하며, 이는 디코더 전용 LLM 7로는 불가능합니다. 작업에 따라 지시사항 생성 템플릿의 단어 선택을 조정할 수 있습니다.

**APE의 단계 ([1]에서 발췌)**

[1]에서 LLM이 이전 지시사항으로부터 더 나은 지시사항을 추론할 수 있음을 알 수 있습니다. 생성된 지시사항의 품질을 결정하기 위해, 출력 정확도(output accuracy) 또는 더 부드러운 측정 기준(metric, 예: 올바른 출력의 로그 확률(log probability))을 사용하여 다른 LLM으로 제로샷 방식으로 간단히 평가할 수 있습니다. 각 생성된 프롬프트의 평가는 프롬프트 최적화 과정 전반에 걸쳐 사용되는 고정된 훈련 세트(training set)에서 이루어집니다. 미세 조정과 비교하여, APE를 통한 프롬프트 최적화는 훨씬 적은 훈련 예시를 필요로 합니다. 최적화가 완료된 후, 우리는 일반적으로 홀드아웃 테스트 세트(hold-out test set)에서 최종 프롬프트를 평가합니다.

([1]에서 발췌)

최상의 성능을 달성하기 위해, [1]에서 우리는 LLM에 선택을 위해 약 64개의 프롬프트를 생성하도록 요청해야 한다는 것을 알 수 있습니다. 더 많은 프롬프트를 생성하면 성능 면에서 수확 체감(diminishing returns)이 나타나기 시작합니다. 즉, 발견된 최적의 프롬프트는 정확도 면에서 크게 향상되지 않습니다.

"이 접근 방식이 제안 세트의 전반적인 품질을 향상시키지만, 가장 높은 점수를 받은 지시사항은 더 많은 단계에서도 동일하게 유지되는 경향이 있습니다." - [1]에서 발췌

**반복적인 APE (Iterative APE).**
우리는 또한 APE로 프롬프트를 반복적인 방식으로 최적화할 수 있습니다. 이 설정에서 우리는 여전히 LLM에 새로운 지시사항을 제안하고, 각 지시사항에 대한 점수를 계산하며, 최고 점수를 가진 지시사항을 선택하도록 요청합니다. 그러나 한 걸음 더 나아가, 우리는 이러한 생성된 지시사항을 입력으로 받아, 이를 컨텍스트로 사용하여 유사한 방식으로 더 많은 지시사항 변형을 제안하고, 가장 성능이 좋은 프롬프트를 선택함으로써 이 과정을 반복할 수 있습니다。 아래를 참조하십시오.

**반복 APE ([1]에서 발췌)**

이 전략을 사용하여 LLM이 제안한 프롬프트의 다양한 변형을 탐색할 수 있습니다. 그러나 [1]에서 반복 APE는 탐색되는 프롬프트 세트의 전반적인 품질만 향상시킨다는 것을 알 수 있습니다. APE에 의해 발견된 최적의 프롬프트 성능은 반복 횟수에 관계없이 일정하게 유지됩니다. 아래를 참조하십시오. 이러한 이유로 반복 APE의 추가 비용은 잠재적으로 불필요할 수 있습니다.

([1]에서 발췌)

**APE의 실제 효용성 평가.**
APE는 제로샷 프롬프트, 퓨샷 프롬프트, CoT 프롬프트, 그리고 LLM의 행동을 조종하기 위한 프롬프트(예: 더 진실되게 만들기)를 포함한 여러 설정에서 효과적인 프롬프트를 작성하는 능력에 기반하여 평가됩니다. 실험 전반에 걸쳐 APE가 인간이 작성한 프롬프트보다 뛰어난 프롬프트를 찾을 수 있음을 알 수 있습니다. 특히 APE는 24개 지시 유도 작업 중 24개, 21개 BIG-Bench 작업 중 17개에서 인간이 작성한 프롬프트보다 뛰어난 프롬프트를 생성합니다. 자세한 내용은 아래를 참조하십시오.

([1]에서 발췌)

APE가 제안한 프롬프트를 검토함으로써 효과적인 프롬프트 작성에 유용한 팁을 추론할 수도 있습니다. 많은 경우에 이러한 팁은 작업 전반에 걸쳐 잘 일반화되며, 특정 모델에 적절하게 프롬프트를 제공하는 방법을 가르쳐줍니다. 또한 [1]에서 지시사항 후보의 성능이 LLM 크기에 따라 향상된다는 것을 알 수 있으며, 이는 APE 내에서 새로운 지시사항을 제안하는 데 대규모 LLM을 사용하는 것이 유용하다는 것을 나타냅니다.

**자동 프롬프트 최적화 [2]: 경사 하강법의 자연어적 구현**
APE는 잘 작동하지만, 프롬프트를 최적화하는 데 사용하는 과정은 무작위적이고 방향성이 없습니다. 우리는 단순히 LLM을 사용하여 여러 새로운 프롬프트 변형을 제안하고, 가장 성능이 좋은 생성된 프롬프트를 선택합니다. 이 알고리즘 내에는 반복적인 최적화 절차가 없습니다. 오히려 우리는 최적화 도구 LLM이 단일 패스(single pass)로 평가하여 더 나은 프롬프트를 찾을 수 있는 광범위하고 유망한 프롬프트 변형을 제안하는 능력에 전적으로 의존합니다.

"이 알고리즘은 데이터 배치(batches of data)를 사용하여 현재 프롬프트를 비판하는 자연어 경사(natural language gradients)를 형성합니다. 이는 수치 경사(numerical gradients)가 오류 상승 방향을 가리키는 방식과 매우 유사합니다. 자연어 경사는 경사의 반대 의미 방향으로 프롬프트를 편집함으로써 프롬프트에 전파됩니다." - [2]에서 발췌

올바른 관점에서 보면, LLM으로 프롬프트를 최적화하는 것은 전통적인 경사 기반 최적화와 유사합니다. [2]에서 저자들은 이러한 유사성에서 영감을 받은 APO(Automatic Prompt Optimization) 8이라는 프롬프트 최적화 기술을 제안합니다. APO는 (작은) 훈련 데이터셋, 초기 프롬프트, 그리고 LLM API 접근만 있으면 작동하는 일반적인 기술입니다. 이 알고리즘은 훈련 데이터 배치(batches of training data)를 사용하여 "경사"(현재 프롬프트의 한계를 설명하는 텍스트 기반 비판)를 도출하고, 이는 경사 업데이트(gradient update)를 모방하여 프롬프트에 대한 편집 및 개선을 안내합니다. 아래를 참조하십시오.

([2]에서 발췌)

**최적화 프레임워크의 상세 구조.**
APO는 자연어 경사에 의해 안내되는 프롬프트에 대한 이산적인 개선을 적용하는 것을 목표로 합니다. 이러한 경사는 다음을 통해 도출됩니다.
*   LLM으로 훈련 데이터셋에 대해 현재 프롬프트를 실행합니다.
*   특정 목적 함수에 따라 프롬프트의 성능을 측정합니다.
*   LLM을 사용하여 훈련 데이터셋에서 이 프롬프트 성능의 주요 한계를 비판합니다.

도출된 경사는 현재 프롬프트 내에 존재하는 다양한 문제에 대한 텍스트 요약을 단순히 포착합니다. 이 요약을 사용하여, 우리는 LLM에 (경사와 현재 프롬프트를 입력으로 사용하여) 기존 프롬프트를 이러한 문제를 줄이는 방식으로 편집하도록 프롬프트를 제공할 수 있습니다. APO는 최적의 프롬프트를 찾기 위해 이러한 단계를 반복적으로 적용합니다. 이 프레임워크의 그림은 아래를 참조하십시오.

([2]에서 발췌)

**경사 생성 및 프롬프트 편집 전략.**
APO는 i) 훈련 데이터에서 현재 프롬프트가 만든 오류를 수집하고, ii) 자연어 경사를 통해 이러한 오류를 요약하며, iii) 경사를 사용하여 프롬프트의 여러 수정된 버전을 생성하고, iv) 편집된 프롬프트 중 최적의 것을 선택하며, v) 이 과정을 여러 번 반복함으로써 프롬프트를 최적화하는 재귀적 피드백 루프(recursive feedback loop)를 생성합니다.

"우리는 경사 하강법의 단계를 텍스트 기반 대화로 모방하며, 미분(differentiation)을 LLM 피드백으로, 역전파를 LLM 편집으로 대체합니다." - [2]에서 발췌

"경사"를 생성하기 위해 우리는 LLM에 훈련 데이터셋에서 현재 프롬프트가 만든 오류의 예시를 보여주고, 모델에 이러한 오류의 원인을 제안하도록 요청합니다. 그런 다음 이 원인들을 프롬프트 편집을 위한 컨텍스트로 사용합니다. 아래를 참조하십시오.

**자연어 경사를 사용하여 프롬프트 편집 ([2]에서 발췌)**

각 반복에서 우리는 현재 프롬프트에 여러 편집을 생성합니다. 편집 횟수는 최적화 프로세스의 하이퍼파라미터(hyperparameter)입니다. 또한, 각 프롬프트에 대해 여러 가지 표현을 명시적으로 생성함으로써 검색 공간(search space)을 확장할 수 있습니다. [2]의 저자들은 이 기술을 몬테카를로 샘플링(Monte Carlo sampling)이라고 부릅니다. 아래를 참조하십시오.

**APO에서의 몬테카를로 샘플링 ([2]에서 발췌)**

**검색 및 선택 메커니즘.**
각 프롬프트의 여러 변형을 생성함으로써 빔 검색(beam search)을 통해 최적의 프롬프트를 검색할 수 있습니다. 다시 말해, 각 반복에서 최적의 프롬프트에 대한 여러 후보를 유지하고, 각 프롬프트에 N개의 편집을 제안한 다음, (훈련 세트에서 성능을 측정하여) 프롬프트에 대한 최적의 B개 편집을 선택하여 다음 반복에 유지합니다. 아래를 참조하십시오.

**빔 검색을 사용하여 최적의 프롬프트를 반복적으로 선택**

예상할 수 있듯이, 전체 훈련 데이터셋에 대해 모든 프롬프트 후보를 지속적으로 평가하면 빔 검색은 비용이 많이 들 수 있습니다. 그러나 [2]에서 우리는 밴딧 기술(bandit techniques), 특히 최적의 암 식별 문제(best arm identification problem)와 관련된 기술을 사용하여 검색 프로세스를 더 효율적으로 만들 수 있음을 알 수 있습니다. 이 접근 방식의 세부 사항은 이 게시물의 범위를 벗어나지만, 높은 수준의 아이디어는 전체 훈련 데이터셋에 대해 모든 프롬프트를 단순히 순진하게 평가하는 대신, 통계를 사용하여 어떤 프롬프트가 완전히 평가할 가치가 있는지 선택할 수 있다는 것입니다.

**APO의 성능 검증.**
APO는 탈옥 감지(jailbreak detection), 혐오 발언 감지(hate speech detection), 가짜 뉴스 감지(fake news detection), 풍자 감지(sarcasm detection)를 포함한 네 가지 텍스트 기반 분류 작업에서 GPT-3.5 프롬프팅에 대해 평가됩니다。

**분류 문제에 APO를 적용하는 이유?**
APO 평가를 위한 분류 작업의 명시적인 선택은 이러한 문제에 대한 목적 함수를 정의하는 단순성 때문일 가능성이 높습니다. 즉, 분류 정확도를 기반으로 각 프롬프트를 평가할 수 있습니다. 복잡하거나 개방형 작업의 경우, 목적 함수를 정의하는 것이 간단하지 않아 APO와 같은 기술의 적용이 덜 직관적입니다.

([2]에서 발췌)

위에서 보듯이, APO는 모든 데이터셋에서 다른 최첨단 프롬프트 최적화 알고리즘보다 뛰어난 성능을 보입니다. 특히, 위 그림의 몬테카를로(MC) 기술인 APE의 성능이 APO보다 나쁘다는 것을 알 수 있으며, 이는 자연어 경사를 통해 프롬프트 최적화 프로세스에 더 구체적인 방향을 추가함으로써 이점을 얻는다는 것을 나타냅니다. [2]에서 APO가 초기 프롬프트의 성능을 최대 31%까지 향상시킬 수 있음을 알 수 있으며, 이는 APO가 LLM API 호출을 필요로 함에도 불구하고 다른 기술에서 볼 수 있는 개선 사항을 능가합니다.

**GRIPS: 경사 없는, 편집 기반 지시 검색을 통한 대규모 언어 모델 프롬프팅 [14]**
"우리는 반복적이고, 국소적이며, 편집 기반의 경사 없는 검색을 통해 지시 프롬프트를 개선하는 자동화된 절차인 GRIPS(Gradient-free Instructional Prompt Search)를 제안합니다." - [14]에서 발췌

프롬프트 최적화 프로세스의 일부로 LLM을 사용하여 프롬프트를 편집하는 대신, 휴리스틱 전략을 사용하여 고려할 새로운 프롬프트 변형을 생성할 수 있습니다. [14]에서 저자들은 고정된 휴리스틱 편집 작업 세트를 반복적으로 적용하여 더 나은 프롬프트를 검색하는 경사 없는 방법인 GrIPS를 제안합니다. 지금까지 본 접근 방식과 마찬가지로, GrIPS는 인간이 작성한 프롬프트를 입력으로 받아 개선되고 편집된 프롬프트를 출력으로 반환합니다.

**휴리스틱 프롬프트 검색의 작동 방식.**
GrIPS는 지시 기반 프롬프트와 컨텍스트 내 예시(아래 참조)를 포함하는 프롬프트 모두에 적용될 수 있지만, 순수하게 지시사항 개선에 중점을 둡니다. GrIPS에서는 예시 선택을 최적화하려고 시도하지 않습니다. 가능한 프롬프트 공간을 검색하기 위해 초기 프롬프트로 시작하여 이 프롬프트에 일련의 편집을 반복적으로 수행합니다. 그런 다음, 훈련 예시 9 풀에 대해 편집된 프롬프트의 성능을 테스트하여 어떤 편집된 프롬프트가 최적인지 결정할 수 있습니다.

([14]에서 발췌)

이 절차는 지금까지 본 기술과 다소 유사하지만, LLM에 의존하여 프롬프트의 편집된 버전을 생성하는 대신 검색 과정에서 프롬프트에 휴리스틱 편집 세트를 적용합니다. 이 기술은 API를 통해 노출되는 LLM을 포함하여 임의의 유형의 LLM과 작동할 수 있습니다.

**검색 세부 사항.**
GrIPS의 최적화 프레임워크는 아래에 나와 있습니다. 우리는 인간이 작성한 기본 지시사항으로 검색 프로세스를 시작합니다. 각 반복에서 우리는 이 프롬프트에 적용할 특정 수의 편집 작업을 선택하여 여러 새로운 프롬프트 후보를 생성합니다. 이 후보들은 점수가 매겨져 그중 최적의 것을 식별할 수 있습니다. 최적 후보의 점수가 현재 기본 지시사항의 점수를 초과하면, 우리는 최적 후보를 앞으로 나아갈 기본 지시사항으로 사용합니다. 그렇지 않으면 현재 기본 지시사항을 유지하고 반복합니다. 이는 기본 지시사항의 점수가 여러 반복 동안 개선되지 않을 때 종료되는 탐욕적 검색(greedy search) 프로세스입니다.

([14]에서 발췌)

[2]와 유사하게, 우리는 각 반복에서 여러 지시사항 세트를 유지함으로써 이 탐욕적 검색 절차를 빔 검색을 수행하도록 쉽게 조정할 수 있습니다. 그러나 이러한 수정은 각 단계에서 평가해야 할 더 많은 수의 후보 프롬프트를 도입함으로써 검색 프로세스의 비용을 증가시킵니다.

**편집 유형의 다양성.**
GrIPS에서는 항상 구(phrase) 수준에서 편집을 적용하여 프롬프트의 구조를 유지하면서 의미 있게 수정할 수 있습니다. 프롬프트를 구로 분해하기 위해 구성 파서(constituency parser)를 사용할 수 있습니다. 그런 다음, 프롬프트의 구에 대해 네 가지 다른 유형의 편집 작업이 고려됩니다.
*   **삭제(Delete)**: 지시사항에서 특정 구의 모든 발생을 제거하고 삭제된 구를 추가 작업에 사용하기 위해 저장합니다.
*   **교환(Swap)**: 두 구가 주어지면, 지시사항에서 첫 번째 구의 모든 발생을 두 번째 구로 바꾸고 그 반대도 마찬가지입니다.
*   **재진술(Paraphrase)**: 지시사항에서 특정 구의 모든 발생을 해당 구의 재진술된 버전 10으로 바꿉니다.
*   **추가(Addition)**: 이전 반복에서 삭제된 구를 샘플링하고 임의의 구 경계에 다시 지시사항에 추가합니다.

**실제 적용 결과.**
GrIPS는 주로 Natural Instructions 데이터셋의 이진 분류(binary classification) 작업에서 평가됩니다. [12]의 관찰과 유사하게, GrIPS에 의해 발견된 프롬프트는 정확도를 향상시키는 경향이 있지만 항상 일관적이지는 않습니다. 최적화된 프롬프트는 의미적으로 어색하고 혼란스러운 표현을 포함합니다. 그럼에도 불구하고, [14]에서 GrIPS를 사용하여 기본 지시사항을 최적화하는 것이 다양한 LLM의 정확도를 2-10%까지 지속적으로 향상시킨다는 것을 알 수 있습니다. 또한, 수동 프롬프트 재작성(그리고 일부 경우 경사 기반 프롬프트 튜닝까지도)은 GrIPS에 의해 능가되는 경향이 있습니다. GrIPS는 (일반적인 지시사항과 대조적으로) 작업별 지시사항을 최적화하는 데 가장 잘 작동하는 것으로 나타났습니다. 아래를 참조하십시오.

([14]에서 발췌)

**최적화 도구로서의 대규모 언어 모델 [3]: OPRO의 다재다능함**
"최적화 문제를 공식적으로 정의하고 프로그래밍된 솔버(solver)로 업데이트 단계를 도출하는 대신, 우리는 최적화 문제를 자연어로 설명한 다음, LLM에 문제 설명과 이전에 찾은 해를 기반으로 새로운 해를 반복적으로 생성하도록 지시합니다." - [3]에서 발췌

오늘날 가장 인기 있고 널리 사용되는 프롬프트 최적화 기술 중 하나는 OPRO(Optimization by Prompting)입니다. 그러나 OPRO는 단순히 프롬프트를 최적화하는 것 이상을 할 수 있습니다. 이는 다음을 통해 작동하는 일반적인 최적화 알고리즘입니다.
*   최적화 작업을 자연어로 설명합니다.
*   최적화 도구 LLM에 최적화 작업에 대한 이전 해의 예시와 그 목적 함수 값(objective values)을 보여줍니다.
*   최적화 도구 LLM에 문제에 대한 새롭고/더 나은 해를 추론하도록 요청합니다.
*   평가자 LLM(evaluator LLM)을 통해 추론된 해를 테스트합니다.

위 단계를 반복함으로써 우리는 매우 유연한 경사 없는 최적화 알고리즘을 만듭니다. 수학으로 해결하려는 문제를 공식적으로 명시하는 대신, 우리는 문제를 자연어로 설명할 수 있습니다. 이러한 설명은 OPRO로 최적화를 수행하기에 충분하며, 이는 알고리즘을 널리 적용 가능하고 새로운 작업으로 쉽게 확장할 수 있게 합니다.

([3]에서 발췌)

올바른 샘플링 전략(sampling strategy)이 주어진다면, 이 최적화 알고리즘의 궤적은 비교적 안정적이며, 이는 LLM이 과거 해로부터 학습하여 새롭고 더 나은 해를 제안할 수 있음을 다시 한번 나타냅니다. 위를 참조하십시오. OPRO는 많은 문제에 적용될 수 있지만, [3]에서 이 알고리즘이 자동 프롬프트 최적화에 특히 유용하다는 것을 알 수 있습니다. 먼저 OPRO가 어떻게 작동하는지 알아보고, 그런 다음 이를 프롬프트 최적화에 어떻게 사용할 수 있는지 탐구할 것입니다.

([3]에서 발췌)

**OPRO 프레임워크의 핵심 요소.**
OPRO가 따르는 상위 수준 단계는 위에 설명되어 있습니다. 최적화 프로세스의 각 단계에서 OPRO는 최적화 문제에 대한 텍스트 설명과 이전에 제안된 해를 포함하는 메타 프롬프트(meta-prompt)를 기반으로 새로운 해를 생성합니다. 메타 프롬프트가 주어지면, 우리는 한 번에 여러 개의 새로운 해를 생성합니다 11. 각 단계에서 여러 후보 해를 생성함으로써, LLM이 실행 가능한 해를 생성할 여러 기회를 가지므로 (상대적으로) 안정적인 최적화 궤적을 보장할 수 있습니다. 여기에서 우리는 목적 함수(예: 정확도)를 기반으로 각 새로운 해를 평가하고, 최적의 해를 다음 최적화 단계를 위한 메타 프롬프트에 추가합니다. 이런 방식으로 우리는 최적화 프로세스의 다음 반복으로 전달될 최적의 해를 선택합니다. 우리의 목표는 목적 함수를 최적화하는 해를 찾는 것이며, LLM이 목적 함수 개선을 가져오는 새로운 해를 제안할 수 없게 되면 최적화 프로세스는 종료됩니다.

**OPRO의 주요 구성 요소 ([3]에서 발췌)**

**OPRO의 핵심 구성 요소.**
이 프레임워크에서 볼 수 있듯이, OPRO는 최적화 프로세스 내에 두 가지 주요 구성 요소(위에 묘사됨)를 가집니다.
*   **최적화 도구(Optimizer)**: 메타 프롬프트를 입력으로 받아 새로운 해를 제안합니다.
*   **평가자(Evaluator)**: 해를 입력으로 받아 목적 함수 값을 계산합니다.

최적화 도구는 항상 LLM으로 구현됩니다. OPRO를 성공적으로 적용하려면, 메타 프롬프트에 제공된 컨텍스트에서 최적의 해를 추론하는 데 복잡한 추론 능력(reasoning capabilities)이 필요하므로, 충분히 강력한 LLM을 최적화 도구로 선택해야 합니다. 플러스, [1]에서 더 큰 LLM이 이 작업에 더 능숙한 경향이 있음을 알 수 있습니다. 평가자도 해결되는 문제에 따라 LLM으로 구현될 수 있지만, 최적화 도구와 평가자가 동일한 LLM일 필요는 없습니다. 예를 들어, 선형 회귀 문제(linear regression problems)는 간단한 평가자를 가지지만 (즉, 해로부터 목적 함수 값을 직접 계산할 수 있지만), 프롬프트 최적화 문제는 각 프롬프트에 대한 출력 품질을 측정하기 위해 평가자 LLM을 사용해야 합니다.

"전통적인 최적화는 종종 상당히 큰 훈련 세트를 필요로 하지만, 우리의 실험은 적은 수 또는 일부 훈련 샘플로도 충분하다는 것을 보여줍니다." - [3]에서 발췌

OPRO를 이용한 최적화는 훈련 데이터와 테스트 데이터 모두를 필요로 합니다. 우리는 최적화 과정 동안 목적 함수 값을 계산하기 위해 훈련 데이터셋을 사용하고, 테스트 세트는 최적화 과정이 완료된 후 생성된 해의 최종 성능을 평가하는 데 사용됩니다. 대부분의 최적화 알고리즘과 비교하여 OPRO는 매우 적은 훈련 데이터를 필요로 합니다.

**메타 프롬프트에 대한 심층 분석.**
메타 프롬프트는 최적화 도구 LLM이 해결되는 문제에 대한 더 나은 해를 제안하는 데 필요한 모든 컨텍스트를 제공합니다. 메타 프롬프트에는 두 가지 주요 구성 요소가 있습니다.
*   최적화 문제에 대한 텍스트 설명.
*   이전 해와 그 목적 함수 값(즉, 최적화 궤적).

최적화 궤적은 최적의 해가 메타 프롬프트의 끝에 나타나도록 정렬됩니다. 이 핵심 정보 외에도, 예상되는 출력 형식을 보여주기 위해 훈련 데이터셋에서 무작위로 선택된 예시와 새로운 해를 생성할 때 따를 일반적인 지시사항(예: "간결하게 작성" 또는 "정확도를 최대화하는 새로운 지시사항 생성")을 포함합니다. 프롬프트 최적화 작업을 위한 메타 프롬프트의 예시는 아래에 나와 있습니다.

([3]에서 발췌)

**OPRO의 실제 성능 분석.**
[3]에서 OPRO는 GSM8K와 Big-Bench Hard 모두에서 평가됩니다. 이 데이터셋에서 OPRO는 인간이 작성한 프롬프트보다 각각 8% 및 50% 더 뛰어난 프롬프트를 발견합니다. APE [1]와 같은 이전 접근 방식과 비교하여, OPRO는 더 나은 성능을 달성하는 더 복잡한 지시사항을 찾을 수 있음을 보여줍니다. 아래를 참조하십시오. 흥미롭게도, 발견되는 지시사항의 스타일은 사용되는 최적화 도구 LLM에 따라 크게 달라지는 경향이 있습니다.

([3]에서 발췌)

**고급 추론 시스템(o1)과의 시너지.**
앞서 언급했듯이, 최적화 궤적에서 새로운 해를 추론하는 것은 복잡한 추론 문제입니다. 우리는 이미 더 큰 LLM이 이 작업을 해결하는 데 더 능숙하다는 것을 보았으며 [1], 이는 더 나은 추론 능력을 가진 더 강력한 최적화 도구 LLM으로부터 이점을 얻는다는 것을 나타냅니다. 또한, 프롬프트 최적화는 일회성 비용입니다. 우리는 프롬프트를 한 번 최적화하지만, 최적화된 프롬프트는 일반적으로 다운스트림 애플리케이션에서 훨씬 더 긴 기간 동안 사용됩니다. 결과적으로 훨씬 더 비싼 최적화 도구 LLM을 사용하는 비용은 그것이 생성하는 프롬프트의 수명에 의해 상각됩니다.

(o1-preview 발표에서 발췌)

OpenAI의 o1과 같은 LLM 기반 추론 시스템의 최근 제안은 프롬프트 최적화를 위한 새로운 가능성을 열어줍니다. 이 시스템들은 모델이 복잡한 문제를 해결하는 데 더 많은 계산이 필요한지 동적으로 결정할 수 있는 광범위한 추론 프로세스를 따르도록 훈련됩니다. 자세한 내용은 여기를 참조하십시오. 이러한 고급 추론 전략은 일부 경우 비용이 많이 들 수 있습니다. 예를 들어, o1은 프롬프트에 응답하기 전에 1분 이상 "생각"할 수 있습니다. 그러나 추론 시간(inference time)에 추가 계산을 사용하는 것은 복잡한 추론 문제에 대해 가치 있을 수 있습니다. 자동 프롬프트 최적화의 경우, 최적화 도구 LLM 응답의 지연 시간(latency)과 비용은 생성된 프롬프트의 품질만큼 중요하지 않으므로, 이는 이러한 고급 추론 시스템의 주요 응용 분야가 됩니다.

"우리는 복잡한 추론을 수행하기 위해 강화 학습으로 훈련된 새로운 대규모 언어 모델인 OpenAI o1을 소개합니다. o1은 답변하기 전에 생각합니다. 사용자에게 응답하기 전에 긴 내부 사고의 사슬(internal chain of thought)을 생성할 수 있습니다." - 출처

**진화 알고리즘을 통한 프롬프트 최적화: 생물학적 영감**
지금까지 본 프롬프트 최적화 전략은 인간이 작성한 프롬프트(또는 프롬프트 세트)로 시작하여 반복적으로 다음을 수행합니다.
*   프롬프트의 변형을 생성합니다.
*   이러한 변형 중에서 가장 성능이 좋은 프롬프트를 선택합니다.

어떤 면에서는 이 최적화 과정을 EA로 볼 수 있습니다. 프롬프트 개체군이 시간이 지남에 따라 유지되고, 돌연변이되고, 선택됩니다. 우리는 개체군 크기를 1로 사용하며 (빔 검색이 사용되지 않는 한), LLM에 프롬프트를 제공하거나 (또는 휴리스틱, 편집 기반 전략을 사용하여) 현재 프롬프트를 수정함으로써 개체군이 돌연변이됩니다. 이 연구에서 영감을 받아, 연구자들은 최근 EA를 프롬프트 최적화에 적용하기 위한 더 명시적인 전략을 탐구했습니다. 보시다시피, 이 알고리즘들은 지금까지 본 것들과 크게 다르지 않습니다!

([15]에서 발췌) GPS(Genetic Prompt Search) [15] (위에 표시됨)는 유전 알고리즘을 사용하여 고성능 프롬프트를 자동으로 발견합니다. 먼저, 알고리즘은 손으로 작성된 프롬프트 세트(즉, 개체군)로 인스턴스화(instantiate)됩니다. 그런 다음, 다음 작업을 통해 이러한 프롬프트를 돌연변이시킬 수 있습니다.
*   **역번역(Back translation)**: 프롬프트를 11개 다른 언어 중 하나로 번역한 다음, 다시 영어로 번역합니다.
*   **클로즈(Cloze) 12**: 프롬프트에서 여러 토큰을 마스킹(mask)하고 사전 훈련된 언어 모델(예: T5)을 사용하여 채워 넣음으로써 새로운 프롬프트를 생성합니다.
*   **문장 이어쓰기(Sentence continuation)**: LLM에 "같은 의미를 가진 두 문장을 작성하라"고 프롬프트를 제공하고, 현재 프롬프트를 첫 번째 문장으로 제공합니다.

이러한 각 전략은 현재 프롬프트 개체군을 돌연변이시켜 새로운 프롬프트 변형을 생성할 수 있습니다. 여기에서 우리는 홀드아웃 검증 세트(hold-out validation set)를 사용하여 생성된 프롬프트에 점수를 매겨 다음 반복(즉, 진화 세대)에 유지되어야 할 프롬프트를 결정할 수 있습니다. 이 알고리즘은 수동 프롬프트 엔지니어링보다 뛰어난 성능을 보이며, 일부 경우에는 프롬프트 튜닝보다도 뛰어날 수 있습니다.

"프롬프트의 구 시퀀스는 일반적인 EA의 유전자 시퀀스(gene sequences)로 간주될 수 있으며, 이는 자연적인 진화 과정과 호환되게 만듭니다." - [16]에서 발췌

EvoPrompt [16]는 다시 EA의 개념을 활용하여 성능이 좋고 빠르게 수렴하는 프롬프트 최적화 도구를 만듭니다. 수동으로 작성된 프롬프트 개체군으로 시작하여, EvoPrompt는 두 가지 주요 진화 연산자(돌연변이 및 교차)를 사용하여 새로운 프롬프트를 생성합니다. 각 세대에서 최적의 프롬프트는 홀드아웃 검증 세트에서 성능을 측정하여 추가 진화를 위해 선택되고 유지됩니다. 정의상 진화 연산자는 시퀀스를 입력으로 받아 수정된 시퀀스를 출력합니다. 전통적으로 연산자는 시퀀스의 각 요소에 독립적으로 적용됩니다. 우리는 주변 요소가 변경되었는지 여부에 대한 지식 없이 각 요소를 업데이트합니다. 프롬프트 최적화의 경우, 프롬프트 내의 모든 토큰이 상호 연관되어 있기 때문에 이 접근 방식은 문제가 됩니다.

([16]에서 발췌)

이 문제를 해결하기 위해 EvoPrompt는 프롬프팅을 통해 진화 연산자를 구현하고, LLM의 전문 지식에 의존하여 논리적인 방식으로 프롬프트를 진화시킵니다. 유전 알고리즘을 통한 프롬프트 최적화에 사용되는 교차 및 돌연변이 프롬프트의 예시는 위에 나와 있습니다. 그러나 EvoPrompt는 유전 알고리즘을 넘어 다양한 EA를 지원합니다! 예를 들어, 아래에서 프롬프트 최적화를 위한 차분 진화 전략을 구현하는 데 사용할 수 있는 프롬프트 세트를 볼 수 있습니다. EvoPrompt 프레임워크는 다양한 EA를 플러그 앤 플레이(plug-and-play) 방식으로 대체할 수 있을 만큼 충분히 유연합니다.

([16]에서 발췌)

EvoPrompt는 여러 작업에서 폐쇄형 및 오픈 소스 LLM(예: Alpaca 및 GPT-3.5) 모두로 테스트되었으며, APE [1] 및 APO [2]와 같은 알고리즘보다 뛰어난 성능을 보이는 것으로 나타났습니다. EvoPrompt의 차분 진화 변형은 대부분의 경우 유전 알고리즘 변형보다 뛰어난 성능을 보입니다. 아래를 참조하십시오.

([16]에서 발췌)

Prompbreeder [17]는 EvoPrompt 직후에 제안되었습니다. 이 두 기술의 아이디어는 유사합니다.
*   둘 다 인간이 작성한 프롬프트 개체군으로 시작합니다.
*   둘 다 LLM에 대한 프롬프트를 통해 돌연변이 및 교차와 같은 진화 연산자를 구현합니다.
*   둘 다 검증 세트(validation set)에서 성능을 측정하여 선택을 수행합니다.

이 기술들 간의 주요 차이점은 Promptbreeder가 작업 프롬프트뿐만 아니라 진화 연산자를 구현하는 데 사용되는 프롬프트까지 최적화한다는 것입니다! 이러한 접근 방식은 프롬프트 최적화 알고리즘 자체 내에 자기 개선 메커니즘(self-improvement mechanism)을 도입할 수 있도록 합니다.

"즉, Promptbreeder는 작업 프롬프트를 개선할 뿐만 아니라, 이러한 작업 프롬프트를 개선하는 돌연변이 프롬프트도 개선합니다." - [17]에서 발췌

Promptbreeder는 추론 벤치마크(reasoning benchmarks)에서 다양한 강력한 프롬프팅 전략(예: 사고의 사슬(chain of thought) 및 계획 및 해결 프롬프팅(plan and solve prompting))보다 뛰어난 성능을 보이는 것으로 나타났습니다. 다른 프롬프트 최적화 알고리즘이 최적화 프로세스를 더 많은 반복으로 확장할 때 일반적으로 수확 체감과 성능 포화(saturation)를 보이는 반면 [1], Promptbreeder는 시간이 지남에 따라 최적화 프로세스를 동적으로 조정하여 어려운 작업을 해결하는 데 더 능숙한 복잡한 프롬프트를 발견할 수 있도록 합니다.

([17]에서 발췌)

**실용적인 조언 및 주요 시사점**
이 개요에서 우리는 데이터로부터 직접 소프트 프롬프트를 학습하는 방법, LLM을 경사 없는 최적화 도구로 활용하는 방법, RL을 사용하여 프롬프트 생성에 더 능숙한 모델을 훈련하는 방법 등 다양한 프롬프트 최적화 기술을 살펴보았습니다. 이 모든 연구에서 얻은 주요 시사점을 아래에 요약합니다.

**프롬프트 최적화가 어려운 근본적인 이유.**
보시다시피, 프롬프트 최적화는 몇 가지 이유로 일반적인 최적화 문제가 아닙니다.
*   우리가 최적화하려는 프롬프트는 이산적인 토큰으로 구성됩니다.
*   대부분의 경우 경사 정보에 접근할 수 없습니다.
*   LLM API를 사용하는 경우, 프롬프트를 개선하는 데 사용할 수 있는 정보가 매우 제한적입니다.

또한, 프롬프트가 이산적이라는 사실은 경사 기반 최적화 알고리즘의 적용을 어렵게 만듭니다. 성공적인 프롬프트 최적화 알고리즘은 i) EA와 유사한 경사 없는 최적화 알고리즘을 채택하고 ii) LLM이 이전에 시도된 프롬프트로부터 더 나은 프롬프트를 추론하는 능력에 의존함으로써 이러한 문제를 피했습니다.

**LLM은 탁월한 프롬프트 엔지니어이다.**
최근 프롬프트 최적화 논문에서 얻은 주요 시사점 중 하나는 LLM이 프롬프트 작성에 능숙하다는 사실입니다. 올바른 정보를 컨텍스트로 제공한다고 가정하면, LLM에 프롬프트를 비판하고 개선하도록 반복적으로 프롬프트를 제공하는 것만으로도 놀랍도록 강력한 프롬프트 최적화 알고리즘을 만들 수 있습니다. 더 크고 (더 유능한) LLM은 이 작업에 더 능숙한 경향이 있습니다. 따라서 OpenAI의 o1과 같은 고급 모델을 프롬프트 최적화에 적용하는 것은 아직 탐구되지 않은 흥미로운 기회입니다.

**어떤 방법을 선택해야 할까요?**
많은 프롬프트 최적화 알고리즘을 보았으므로, 이 기술들 중 어떤 것을 실제로 사용해야 할지 모를 수 있습니다. 우리가 본 연구의 대부분은 유용합니다. 그러나 LLM 기반 프롬프트 최적화 도구(예: OPRO [3])는 지금까지 가장 직관적이고 실용적인 프롬프트 최적화 기술입니다. 우리는 이러한 알고리즘을 사용하여 우리가 작성하는 모든 프롬프트를 자동으로 개선할 수 있으며, 인간이 여전히 해석할 수 있는 더 나은 프롬프트를 생성합니다. OPRO는 유익하고 적용하기 쉬워 LLM 실무자들 사이에서 인기를 얻었습니다. OPRO의 오픈 구현은 여기에 제공됩니다.

**인간의 역할은 여전히 필수적인가?**
프롬프트 최적화 기술이 수동 프롬프트 엔지니어링 노력을 크게 줄일 수 있지만, 인간 프롬프트 엔지니어를 완전히 대체하는 것은 불가능할 것입니다. 지금까지 본 모든 알고리즘은 최적화 프로세스의 시작점으로 초기 프롬프트를 제공하기 위해 인간 프롬프트 엔지니어를 필요로 합니다. 또한, OPRO와 같은 기술은 실제로 최적의 프롬프트를 찾기 위해 인간의 개입/안내가 필요할 것입니다. 간단히 말해, 자동 프롬프트 최적화 기술은 본질적으로 보조적입니다. 이 알고리즘들은 프롬프트 엔지니어링의 기본적인 수동 노력 중 일부를 자동화하지만, 인간 프롬프트 엔지니어의 필요성을 없애지는 않습니다. 그러나 LLM이 시간이 지남에 따라 프롬프트의 미묘한 변화에 덜 민감해지고 있다는 점에 유의해야 하며, 이는 일반적으로 프롬프트 엔지니어링의 필요성을 줄입니다.

**프롬프트 최적화의 한계점과 미래.**
지금까지 본 프롬프트 최적화 알고리즘은 프롬프트의 단어 선택과 기본 구조를 개선하는 데 유용합니다. 그러나 성능 좋은 LLM 시스템을 구축하는 데는 훨씬 더 많은 선택 사항이 포함됩니다. 예를 들어, 우리는 RAG(검색 증강 생성)를 사용하거나, 프롬프트에 추가 데이터 소스를 추가하거나, 사용할 최적의 퓨샷 예시를 찾거나 하는 등의 작업을 수행해야 할 수 있습니다. 이러한 상위 수준의 선택 사항을 자동화하는 것은 지금까지 본 프롬프트 최적화 알고리즘의 능력을 넘어섭니다. 그러나 연구자들은 현재 LLM 시스템의 상위 수준 설계와 하위 수준 구현(low-level implementation)을 모두 자동화할 수 있는 DSPy [31]와 같은 도구를 개발하고 있습니다.

뉴스레터가 처음이신가요? 안녕하세요! 저는 Cameron R. Wolfe입니다. 넷플릭스(Netflix)의 딥러닝(Deep Learning) 박사이자 머신러닝 과학자(Machine Learning Scientist)입니다. 이것은 독자들이 AI 연구의 중요한 주제를 더 잘 이해하도록 돕는 Deep (Learning) Focus 뉴스레터입니다. 뉴스레터가 마음에 드신다면 구독, 공유 또는 X와 LinkedIn에서 저를 팔로우해주세요! 구독하기

**참고문헌**
[1] Zhou, Yongchao, et al. "Large language models are human-level prompt engineers." arXiv preprint arXiv:2211.01910 (2022).
[2] Pryzant, Reid, et al. "Automatic prompt optimization with" gradient descent" and beam search." arXiv preprint arXiv:2305.03495 (2023).
[3] Yang, Chengrun et al. “Large Language Models as Optimizers.” arXiv abs/2309.03409 (2023).
[4] Li, Xiang Lisa, and Percy Liang. "Prefix-tuning: Optimizing continuous prompts for generation." arXiv preprint arXiv:2101.00190 (2021).
[5] Lester, Brian, Rami Al-Rfou, and Noah Constant. "The power of scale for parameter-efficient prompt tuning." arXiv preprint arXiv:2104.08691 (2021).
[6] Chen, Lichang, et al. "Instructzero: Efficient instruction optimization for black-box large language models." arXiv preprint arXiv:2306.03082 (2023).
[7] Honovich, Or, et al. "Instruction induction: From few examples to natural language task descriptions." arXiv preprint arXiv:2205.10782 (2022).
[8] Wang, Yizhong, et al. "Self-instruct: Aligning language models with self-generated instructions." arXiv preprint arXiv:2212.10560 (2022).
[9] Xu, Can, et al. "Wizardlm: Empowering large language models to follow complex instructions." arXiv preprint arXiv:2304.12244 (2023).
[10] Jiang, Zhengbao, et al. "How can we know what language models know?." Transactions of the Association for Computational Linguistics 8 (2020): 423-438.
[11] Shin, Taylor, et al. "Autoprompt: Eliciting knowledge from language models with automatically generated prompts." arXiv preprint arXiv:2010.15980 (2020).
[12] Deng, Mingkai, et al. "Rlprompt: Optimizing discrete text prompts with reinforcement learning." arXiv preprint arXiv:2205.12548 (2022).
[13] Zhang, Tianjun, et al. "Tempera: Test-time prompting via reinforcement learning." arXiv preprint arXiv:2211.11890 (2022).
[14] Prasad, Archiki, et al. "Grips: Gradient-free, edit-based instruction search for prompting large language models." arXiv preprint arXiv:2203.07281 (2022).
[15] Xu, Hanwei, et al. "GPS: Genetic prompt search for efficient few-shot learning." arXiv preprint arXiv:2210.17041 (2022).
[16] Guo, Qingyan, et al. "Connecting large language models with evolutionary algorithms yields powerful prompt optimizers." arXiv preprint arXiv:2309.08532 (2023).
[17] Fernando, Chrisantha, et al. "Promptbreeder: Self-referential self-improvement via prompt evolution." arXiv preprint arXiv:2309.16797 (2023).
[18] Janga Reddy, M., and D. Nagesh Kumar. "Evolutionary algorithms, swarm intelligence methods, and their applications in water resources engineering: a state-of-the-art review." h2oj 3.1 (2020): 135-188.
[19] Corne, David W., and Michael A. Lones. "Evolutionary algorithms." arXiv preprint arXiv:1805.11014 (2018).
[20] Ai, Hua, et al. "Topology optimization of computer communication network based on improved genetic algorithm." Journal of Intelligent Systems 31.1 (2022): 651-659.
[21] Zamfirescu-Pereira, J. D., et al. "Why Johnny can’t prompt: how non-AI experts try (and fail) to design LLM prompts." Proceedings of the 2023 CHI Conference on Human Factors in Computing Systems. 2023.
[22] Jiang, Ellen, et al. "Prompt-based prototyping with large language models." Extended Abstracts of the 2022 CHI Conference on Human Factors in Computing Systems. 2022.
[23] Vatsal, Shubham, and Harsh Dubey. "A survey of prompt engineering methods in large language models for different nlp tasks." arXiv preprint arXiv:2407.12994 (2024).
[24] Sahoo, Pranab, et al. "A systematic survey of prompt engineering in large language models: Techniques and applications." arXiv preprint arXiv:2402.07927 (2024).
[25] Reynolds, Laria, and Kyle McDonell. "Prompt programming for large language models: Beyond the few-shot paradigm." Extended abstracts of the 2021 CHI conference on human factors in computing systems. 2021.
[26] Doerr, Benjamin, and Frank Neumann. "A survey on recent progress in the theory of evolutionary algorithms for discrete optimization." ACM Transactions on Evolutionary Learning and Optimization 1.4 (2021): 1-43.
[27] Lehman, Joel, et al. "Evolution through large models." Handbook of Evolutionary Machine Learning. Singapore: Springer Nature Singapore, 2023. 331-366.
[28] Meyerson, Elliot, et al. "Language model crossover: Variation through few-shot prompting." arXiv preprint arXiv:2302.12170 (2023).
[29] Chen, Yutian, et al. "Towards learning universal hyperparameter optimizers with transformers." Advances in Neural Information Processing Systems 35 (2022): 32053-32068.
[30] Chen, Angelica, David Dohan, and David So. "EvoPrompting: language models for code-level neural architecture search." Advances in Neural Information Processing Systems 36 (2024).
[31] Khattab, Omar, et al. "Dspy: Compiling declarative language model calls into self-improving pipelines." arXiv preprint arXiv:2310.03714 (2023).

1 우리는 프롬프트에 퓨샷 예시를 포함할 수 있는데, 이는 모델의 매개변수를 실제로 업데이트하는 데 사용되지는 않지만 "훈련" 데이터의 한 형태로 간주될 수 있습니다.
2 프롬프트 엔지니어를 100% 대체할 필요는 없습니다! 자동 프롬프트 최적화 알고리즘은 본질적으로 보조적일 수 있으며, 이를 통해 더 효율적으로 만들 수 있습니다.
3 하드 프롬프트 튜닝은 프롬프트 엔지니어링의 또 다른 이름일 뿐입니다! 우리는 단순히 프롬프트의 단어를 변경하여 다르고 인간이 읽을 수 있는 프롬프트를 생성합니다.
4 접두사 튜닝에서 사용되는 재매개변수화 접근 방식은 모델에 추가 학습 가능한 매개변수를 추가합니다. 각 접두사에는 자체 매개변수를 가진 추가 순방향 신경망(feed-forward neural network)이 연결되어 있습니다!
5 이 연구는 경사 기반 기술을 사용하여 이산적인 방식으로 프롬프트를 최적화하는 초기 탐구입니다. 그러나 그 이전에 다른 연구들도 있었습니다. 예를 들어, 이 논문은 LLM에서 "트리거 토큰"을 발견하기 위한 유사한 기술을 탐구합니다.
6 버벌라이저(verbalizer)는 레이블 단어를 실제 단어에 매핑(map)하는 데 사용될 수 있는 프롬프트 템플릿의 구성 요소입니다. 예를 들어, "긍정적"이라는 버벌라이저는 "훌륭하다"는 단어에 매핑될 수 있고, "부정적"이라는 버벌라이저는 "끔찍하다"는 단어에 매핑될 수 있습니다.
7 대신, 인필링이 가능한 LLM을 사용해야 합니다. 이는 인코더 전용(encoder-only) 또는 인코더-디코더 트랜스포머(encoder-decoder transformer) 아키텍처를 필요로 합니다. 예를 들어, T5는 인필링 목적 함수를 통해 훈련되는 인기 있는 모델입니다.
8 이 기술은 논문이 처음 발표되었을 때 텍스트 경사를 이용한 프롬프트 최적화(Prompt Optimization with Textual Gradients, ProTeGi)라고 불렸지만, 저자들은 나중에 이 기술의 이름을 자동 프롬프트 최적화(Automatic Prompt Optimization, APO)로 변경했습니다.
9 다시 말하지만, 여기서는 프롬프트의 훈련과 테스트를 위해 별도의 예시 풀을 사용하여 머신러닝 모범 사례를 따라야 합니다.
10 여기서 구의 재진술된 버전은 PEGASUS 모델을 사용하여 생성됩니다.
11 OPRO 내에서 탐색-활용 트레이드오프(explore-exploit tradeoff)의 균형을 맞추기 위해 새로운 해를 생성할 때 LLM이 사용하는 온도 매개변수(temperature parameter)를 단순히 조정할 수 있습니다!
12 클로즈(Cloze)는 기본적인 머신러닝 개념입니다. 이는 시퀀스 내에서 단어(또는 다른 유형의 토큰)를 마스킹하고 이를 예측하려는 작업을 나타냅니다. 이 작업은 마스킹된 언어 모델링(masked language modeling)으로도 알려져 있으며 BERT와 같은 모델을 사전 훈련하는 데 사용됩니다.