(출처: [2, 3, 12, 14, 15, 16]) 특히 거대 언어 모델(LLM)과 여타 최첨단 인공지능 시스템에 지시를 내리는 행위는 현대 인공지능 분야에서 가장 혁신적인 진전 중 하나로 평가받습니다. 자연어로 된 명령어를 작성함으로써 거의 모든 사람이 대규모 신경망(neural network)의 잠재력을 활용하여 다양한 실용적 문제를 해결할 수 있게 되었습니다. 이전 세대의 딥러닝(deep learning) 모델들이 인상적인 성과를 보여주었지만, LLM은 단순히 뛰어난 성능을 넘어 직관적인 사용성을 제공합니다. 전문가가 아닌 일반 사용자들도 이러한 모델과 상호작용하며 그 가능성을 직접 목격할 수 있었고, 이는 인공지능 연구에 전례 없는 관심을 불러일으켰습니다. 이러한 접근성은 인공지능 기술의 대중화를 가속화하며, 복잡한 코딩 지식 없이도 강력한 인공지능 기능을 활용할 수 있는 새로운 시대를 열었습니다.

"자연어 지시사항에 따라, 초거대 언어 모델(LLM)은 범용 컴퓨터(general-purpose computer)로서 놀라운 역량을 입증했습니다. 하지만 작업 수행 능력은 모델을 제어하는 데 쓰이는 프롬프트(prompt)의 품질에 크게 좌우되며, 대부분의 효과적인 프롬프트는 인간의 직접적인 개입으로 만들어졌습니다." - [1]에서 발췌

프롬프트는 LLM에 대한 직관적인 상호작용 방식(interface)을 제공하지만, 이 방식은 완벽함과는 거리가 멉니다. 상당수의 모델은 프롬프트 내의 미미한 변화에도 과도하게 민감하게 반응합니다. 프롬프트의 단어 선택이나 문장 구조를 아주 조금만 변경해도 부정확하거나 예상치 못한 결과가 도출될 수 있습니다. 이러한 특성 때문에 효과적인 프롬프트를 구성하는 것은 상당한 전문 지식(domain expertise)을 요구하는 기술이 되었고, 이는 인기 있는 분야로 자리매김했습니다. 대부분의 프롬프트는 인간이 개입하는 반복적인 시행착오(trial and error) 과정을 거쳐 생성됩니다. 우리는 프롬프팅에 대한 이해와 대상 LLM의 특성에 대한 지식을 바탕으로, 최적의 프롬프트를 찾아내기 위해 프롬프트를 지속적으로 수정하고 검증합니다. 이 과정은 마치 예술과 과학의 경계에 있는 작업과 같으며, 섬세한 조정과 깊이 있는 통찰이 요구됩니다.

**자동화된 프롬프트 개선.**
프롬프팅은 많은 인간의 노력을 필요로 하며 본질적으로 불완전합니다. 이러한 한계를 극복하기 위해, 최근 연구에서는 데이터를 활용하여 프롬프트의 품질을 알고리즘적으로 향상시키는 자동 프롬프트 최적화(automatic prompt optimization) 개념을 탐구했습니다. 이 접근 방식은 몇 가지 핵심적인 이점을 제공합니다.
*   최적의 프롬프트를 발견하는 데 필요한 수작업 노력을 상당 부분 경감시킵니다.
*   프롬프트가 체계적으로 탐색되고 발견되어, 사람이 직접 작성한 프롬프트의 성능을 능가하는 결과물을 도출할 수 있습니다. 이는 기존의 직관이나 경험에 의존하는 방식의 한계를 넘어섭니다.
*   프롬프트 최적화 기술은 휴리스틱(heuristics)이나 특정 영역 지식(domain knowledge)에 의존하는 대신, 프롬프트의 효과를 자동으로 증진시키는 기반을 마련합니다.

이 글에서는 해당 주제에 대한 광범위하게 발표된 연구 문헌들을 검토하고, 더 나은 프롬프트를 구축하기 위한 가장 실용적인 기법들에 초점을 맞출 것입니다.

**사전 지식: 프롬프팅과 최적화의 교차점**
본 개요는 주로 최적화(optimization)와 프롬프트 엔지니어링(prompt engineering)의 접점을 다루겠지만, 이 두 개념(최적화와 프롬프트 엔지니어링)이 각각 독립적인 연구 분야라는 점을 인지하는 것이 중요합니다. 이 개념들을 개별적으로 학습함으로써, 그 기저에 있는 핵심 원리들이 어떻게 결합되어 프롬프트의 품질을 자동으로 향상시키는 알고리즘(algorithm)을 구성할 수 있는지 이해하게 될 것입니다. 이는 마치 두 개의 다른 도구를 조합하여 새로운 기능을 가진 기계를 만드는 과정과 유사합니다.

**최적화란 무엇인가? 심층 탐구**
**함수 최적화의 본질**
'목표 함수(objective function)'로 명명된 특정 수식 내에서 가장 이상적인 결과값이나 특정 지점을 탐색하는 것이 바로 최적화의 본질입니다. 이는 통상적으로 해당 함수의 값을 최소화(혹은 최대화)하는 지점을 찾아내는 과정을 의미합니다. 상단의 그림을 참고하십시오. 이 최적 지점을 발견함으로써 우리는 함수를 '최적화'하거나 최적화 작업을 수행하게 됩니다. 수세기에 걸쳐 발전해 온 최적화 분야는 매우 풍부하고 광범위하며 그 가치는 헤아릴 수 없습니다. 목표 함수를 최적화하는 개념은 교통 흐름 관리(routing traffic)부터 인공 신경망(neural network) 학습에 이르기까지 엄청나게 많은 중요한 과제를 해결하는 데 활용될 수 있습니다. 최적화에 대한 포괄적인 설명은 이 게시물의 범위를 넘어서지만, 관련 자료는 풍부하게 존재합니다. 최적화는 본질적으로 '가장 좋은 것'을 찾는 문제이며, 그 '가장 좋은 것'의 정의는 우리가 해결하려는 문제에 따라 달라집니다.

**최적화 알고리즘의 분류.**
수많은 최적화 알고리즘이 제안되었습니다. 심지어 신경망 학습이라는 특정 응용 분야에 대해서도 선택할 수 있는 방대한 최적화 알고리즘들이 있습니다. 이러한 다양한 방식에도 불구하고, 최적화 알고리즘은 크게 두 가지 범주로 나눌 수 있습니다.
*   기울기 기반(Gradient-based)
*   기울기 비의존(Gradient-free)

**기울기 기반 최적화(gradient-based optimization)**는 반복적으로 i) 함수의 기울기(gradient)를 산출하고 ii) 이 기울기 정보를 활용하여 현재 해를 갱신(update)하는 방식으로 진행됩니다. 기울기는 (국소적으로) 함수 값이 증가하는 방향을 지시하므로, 기울기를 이용하여 함수 값이 더 높거나 낮은 영역을 탐색할 수 있습니다. 아래 그림은 이 과정을 보여줍니다. 여기서 우리는 지속적으로 기울기를 계산하고 기울기의 반대 방향으로 이동함으로써 함수를 최소화합니다(즉, 경사 하강법(gradient descent)을 수행합니다).

**경사 하강법의 개략도**

기계 학습(machine learning) 실무자들은 기울기 기반 최적화 알고리즘에 가장 익숙한 경우가 많습니다. 이 알고리즘들은 역전파(backpropagation) 개념의 근간을 이루며, 이는 신경망 및 다른 기계 학습 모델 학습에 거의 보편적으로 사용됩니다. 기울기 기반 알고리즘은 그 효과성과 효율성 덕분에 인기가 많습니다! 수많은 변수(예: LLM의 매개변수(parameter)들)를 최적화할 때도, 기울기의 방향을 따르는 것만으로 해를 찾는 것이 계산적으로 가능하며, 이는 (상대적으로) 계산 비용이 저렴합니다. 그러나 이러한 방식은 종종 지역 최적점(local optima)에 갇힐 위험이 있다는 단점도 있습니다.

**기울기 비의존 알고리즘(gradient-free algorithms)**은 어떠한 기울기 정보도 활용하지 않는 최적화 알고리즘의 한 유형입니다. 예를 들어, 무차별 대입 검색(brute force search)과 언덕 오르기(hill climbing)가 두 가지 기본적인 예시입니다. 많은 기울기 비의존 최적화 알고리즘이 존재하지만, 가장 인기 있는 알고리즘 종류 중 하나이자 이 개요에서 다룰 알고리즘 종류는 진화 알고리즘(evolutionary algorithms, EA)입니다. 아래를 참조하십시오. 이러한 알고리즘은 특히 복잡하고 비선형적인 문제 공간에서 유용하며, 기울기를 계산하기 어렵거나 불가능한 경우에 적합합니다.

([18]에서 발췌) 생물학적 진화(biological evolution) 개념에서 영감을 받은 EA는 후보 해(candidate solution)의 '개체군(population)'을 유지하며 반복적으로 다음을 수행합니다.
*   돌연변이(mutation) 및 교차(crossover)와 같은 진화 연산자(evolutionary operator)를 통해 이 개체군(즉, 후보 해)의 구성원을 변형하여 새로운 개체군 구성원을 생성함으로써 번식 행위를 시뮬레이션(simulate)합니다.
*   특정 목적 함수(objective function)를 기반으로 개체군에서 가장 우수한 구성원을 선별하여 계속 진화시킵니다(즉, 적자생존(survival of the fittest) 원리 적용).

EA에는 다양한 변형이 존재하지만 [19], 가장 보편적인 구현은 유전 알고리즘(genetic algorithms)과 차분 진화(differential evolution)입니다. 일반적으로 EA는 기울기 기반 최적화 알고리즘에 비해 효율성이 낮다고 평가됩니다. 예를 들어, 대규모 신경망(예: LLM)을 학습시키는 것은 기울기 정보 없이는 매우 어렵습니다. 탐색 공간(search space)이 워낙 방대하기 때문에, 모델의 매개변수를 무작위로 조정하고 손실(loss)을 측정하여 더 나은 LLM을 찾으려는 시도는 큰 진전을 가져오기 어렵습니다.

"EA는 통상적으로 N개의 해로 구성된 초기 집단으로 시작하여, 현재 집단에 진화 연산자(예: 돌연변이 및 교차)를 적용하여 새로운 해를 반복적으로 생성하고 적합도 함수(fitness function)를 기반으로 집단을 갱신합니다." - [16]에서 발췌

그러나 EA에는 많은 장점도 있습니다. 예를 들어, 이 알고리즘들은 탐색(exploration)과 활용(exploitation) 사이의 균형을 매우 효과적으로 조절합니다. 기울기 기반 알고리즘이 단일 해를 산출하는 반면, EA는 전체 개체군을 관리합니다! 이러한 특성은 특정 유형의 문제에 매우 유용하며, 신경망 아키텍처(neural network architecture) 진화나 컴퓨터 네트워크 토폴로지(computer network topologies) 최적화와 같은 많은 흥미로운 분야에서 EA의 실제 적용으로 이어졌습니다 [19]. 이는 특히 다중 최적점(multiple optima)이 존재하는 복잡한 문제 공간에서 전역 최적점(global optimum)을 찾을 가능성을 높여줍니다.

**LLM을 최적화 도구로 활용하기.**
최근 연구자들은 LLM을