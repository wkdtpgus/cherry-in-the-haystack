**DeepSeek R1부터 MiniMax-M2에 이르기까지, 최신 대규모 언어 모델(LLM, Large Language Model)의 지형과 새로운 대안들**

현재 시장에 출시된 대형 언어 모델(LLM) 중 공개 가중치를 제공하는 최상위권 모델들은 본래의 다중 헤드 어텐션(multi-head attention) 구조를 변형한 자동 회귀 디코더(autoregressive decoder) 형태의 트랜스포머(Transformer) 모델을 주축으로 하고 있습니다. 하지만 최근 몇 년간, 기존 LLM의 한계를 극복하고 다양한 목표를 추구하는 대안적 모델들이 활발히 등장하고 있습니다. 텍스트 확산 모델(text diffusion models)부터 최첨단 선형 어텐션 하이브리드 아키텍처(linear attention hybrid architectures)에 이르기까지, 이러한 새로운 시도들은 인공지능 분야의 역동성을 잘 보여줍니다. 일부는 연산 효율성 증대를 목표로 하는 반면, 코드 월드 모델(code world models)과 같은 다른 모델들은 모델링 능력 자체의 향상에 집중합니다.

몇 달 전, 주요 트랜스포머 기반 LLM들을 비교 분석한 저의 글 "The Big LLM Architecture Comparison"을 공유한 이후, 독자 여러분으로부터 이러한 대안적 접근 방식들에 대한 질문을 많이 받았습니다. 특히 최근 PyTorch Conference 2025에서 이 주제로 짧은 강연을 진행하면서, 참석자들에게 관련 내용을 글로 정리하여 제공하겠다고 약속했습니다. 그리하여 이 글이 탄생했습니다!

그림 1: LLM 생태계의 주요 아키텍처 개요. 본문에서는 도표상에서 검은색 테두리로 표시된 구조들을 깊이 있게 탐구합니다. 디코더 스타일 트랜스포머는 이미 저의 이전 글 "The Big Architecture Comparison"에서 다루었으므로, 여기서는 새롭게 부상하는 대안들에 초점을 맞춥니다. 프레임이 없는 다른 아키텍처들은 향후 별도의 글에서 상세히 다룰 예정입니다. 각 주제가 개별적인 심층 분석을 받을 가치가 있다는 점을 인지하고 있으며, 본 글의 분량을 적절히 조절하기 위해 많은 섹션이 간략하게 다루어졌습니다. 그럼에도 불구하고, 이 글이 최근 몇 년간 나타난 흥미로운 LLM 대안들에 대한 유용한 입문서가 되기를 바랍니다.

추신: 앞서 언급된 PyTorch 컨퍼런스 강연 영상은 조만간 공식 PyTorch 유튜브 채널에 게시될 예정입니다. 그전에 미리 내용을 확인하고 싶으신 분들을 위해, 아래에서 연습 녹화 버전을 찾아보실 수 있습니다. (유튜브 링크가 여기에 제공됩니다.)

---

**1. 트랜스포머 기반 LLM(Transformer-Based LLMs)**

고전적인 "Attention Is All You Need" 논문에서 제시된 구조를 기반으로 하는 트랜스포머 구조를 기반으로 하는 대규모 언어 모델들은 여전히 문자열 데이터와 프로그래밍 코드 처리 분야에서 최고 수준의 성능을 자랑합니다. 2024년 말부터 현재까지 공개된 주요 모델들을 살펴보면 DeepSeek V3/R1, OLMo 2, Gemma 3, Mistral Small 3.1, Llama 4, Qwen3, SmolLM3, Kimi K2, gpt-oss, GLM-4.5, GLM-4.6, MiniMax-M2 등이 있습니다. (이 목록은 공개 가중치 모델에 중점을 둡니다. GPT-5, Grok 4, Gemini 2.5와 같은 독점 모델 또한 이 범주에 속합니다.) 그림 2: 지난 한 해 동안 출시된 가장 주목할 만한 디코더 스타일 트랜스포머의 개요.

트랜스포머 기반 LLM에 대해서는 이미 여러 차례 강연하고 글을 썼기 때문에, 독자 여러분은 그 기본적인 개념과 구조에 익숙할 것이라고 생각합니다. 더 깊이 있는 정보가 필요하시다면, 저의 "The Big LLM Architecture Comparison" 글에서 위에 언급된 아키텍처들(및 아래 그림에 표시된 아키텍처들)을 상세히 비교했습니다. (참고: Qwen3-Next와 Kimi Linear는 개요도 상의 다른 트랜스포머-상태 공간 모델(SSM) 하이브리드와 함께 분류될 수도 있었습니다. 개인적으로 저는 다른 트랜스포머-SSM 하이브리드를 트랜스포머 구성 요소를 가진 SSM으로 보는 반면, 여기서 논의된 모델들(Qwen3-Next 및 Kimi Linear)은 SSM 구성 요소를 가진 트랜스포머로 간주합니다. 그러나 IBM Granite 4.0과 NVIDIA Nemotron Nano 2를 트랜스포머-SSM 범주에 포함했으므로, 이들을 단일 범주로 묶어야 한다는 주장도 일리가 있습니다.) 그림 3: 저의 "The Big Architecture Comparison" (https://magazine.sebastianraschka.com/p/the-big-llm-architecture-comparison) 글에서 논의된 아키텍처의 하위 집합.

만약 LLM을 사용하거나 개발하는 입장이라면, 예를 들어 새로운 애플리케이션을 구축하거나, 특정 작업에 모델을 미세 조정하거나, 혁신적인 알고리즘을 실험하는 경우, 이러한 트랜스포머 모델들이 가장 신뢰할 수 있는 선택지가 될 것입니다. 이러한 모델들은 광범위한 검증 과정을 거쳐 신뢰성이 입증되었으며, 뛰어난 처리 능력을 보여줍니다. 또한, "The Big Architecture Comparison" 글에서 논의했듯이 그룹화된 쿼리 어텐션(grouped-query attention), 슬라이딩 윈도우 어텐션(sliding-window attention), 멀티 헤드 잠재 어텐션(multi-head latent attention) 등 다양한 효율성 개선 기법이 적용되어 있습니다. 그러나 연구자와 엔지니어가 기존 방식에 안주하고 대안을 모색하지 않는다면, 이는 혁신의 정체로 이어질 것입니다. 따라서 다음 섹션들에서는 최근 몇 년간 등장한 주목할 만한 LLM 대안들을 심층적으로 다룰 것입니다. 이들은 단순히 효율성을 넘어선 새로운 패러다임을 제시하며, AI의 미래를 형성하는 데 중요한 역할을 할 것으로 기대됩니다.

---

**2. (선형) 어텐션 하이브리드((Linear) Attention Hybrids)**

더욱 이질적인 접근 방식들을 논하기에 앞서, 먼저 효율적인 어텐션 메커니즘(attention mechanisms)을 도입한 트랜스포머 기반 LLM들을 살펴보겠습니다. 특히, 처리해야 할 입력 단위의 개수(토큰 수)에 비례하여 계산 복잡도가 제곱으로 증가하는 대신, 단일 비례 관계를 보이는 모델들에 주목하고자 합니다. 최근 LLM의 연산 효율성을 높이기 위한 목적으로 선형 어텐션 메커니즘(linear attention mechanisms)이 다시금 연구자들의 관심을 받고 있습니다. 특히, 대규모 모델의 컨텍스트 창이 길어지면서 기존 어텐션의 이차적 복잡성 문제가 더욱 부각되었기 때문입니다.

"Attention Is All You Need" 논문(2017)에서 처음 소개된 스케일드 닷 프로덕트 어텐션(scaled-dot-product attention)은 오늘날 LLM에서 가장 널리 사용되는 어텐션 변형으로 자리 잡고 있습니다. 전통적인 멀티 헤드 어텐션(multi-head attention) 외에도, 제 강연에서 다루었듯이 그룹화된 쿼리 어텐션(grouped-query attention), 슬라이딩 윈도우 어텐션(sliding window attention), 멀티 헤드 잠재 어텐션(multi-head latent attention)과 같은 더욱 효율적인 변형에도 이 기본 개념이 활용됩니다. 이러한 효율성 개선 노력은 대규모 모델의 학습 및 추론 비용을 절감하고, 더 긴 시퀀스를 처리할 수 있도록 하는 데 필수적입니다.

---

**2.1 전통적인 어텐션과 이차적 비용(Traditional Attention and Quadratic Costs)**

기존 어텐션 기법은 입력 시퀀스의 길이에 비례하여 연산량이 제곱으로 증가하는 특성을 가집니다:
\(\text{Attention}(Q, K, V) = \text{softmax}\!\left(\frac{QK^\top}{\sqrt{d}}\right)V\)
이는 쿼리(Q), 키(K), 값(V)이 각각 n x d 행렬이기 때문입니다. 여기서 d는 임베딩 차원(embedding dimension, 모델의 하이퍼파라미터)을 나타내며, n은 시퀀스 길이, 즉 처리되는 토큰(tokens)의 개수를 의미합니다. (더 자세한 설명은 저의 "Understanding and Coding Self-Attention, Multi-Head Attention, Causal-Attention, and Cross-Attention in LLMs" 글에서 찾아볼 수 있습니다.) 그림 4: 멀티 헤드 어텐션(multi-head attention)에서 전통적인 스케일드 닷 프로덕트 어텐션 메커니즘(scaled-dot-product attention mechanism)의 설명; 시퀀스 길이 n으로 인한 어텐션의 이차적 비용.

이러한 이차적 확장성은 특히 긴 컨텍스트(context)를 처리해야 하는 현대 LLM에 심각한 문제를 야기합니다. 시퀀스 길이가 두 배가 되면, 어텐션 계산에 필요한 시간과 메모리는 네 배로 증가합니다. 이는 모델이 수십만 또는 수백만 개의 토큰을 처리해야 할 때, 기하급수적인 연산 비용과 메모리 요구량을 발생시켜 학습 및 추론을 비현실적으로 만듭니다. 이러한 근본적인 한계가 선형 어텐션과 같은 대안적 접근 방식의 필요성을 더욱 부각시키는 주된 이유입니다.

---

**2.2 선형 어텐션(Linear attention)**

선형 어텐션(linear attention) 변형은 비교적 오래전부터 연구되어 왔으며, 2020년대 초반에 이미 수많은 관련 논문들이 발표되었습니다. 예를 들어, 제가 기억하는 가장 초기 시도 중 하나는 2020년 논문 "Transformers are RNNs: Fast Autoregressive Transformers with Linear Attention"으로, 연구자들은 어텐션 메커니즘을 다음과 같이 효율적으로 근사했습니다:
\(\text{Attention}(Q, K, V) = \text{softmax}\!\left(\frac{QK^\top}{\sqrt{d}}\right)V \approx \phi(Q)\big(\phi(K)^\top V\big)\)
여기서 ϕ(⋅)는 커널 특징 함수(kernel feature function)를 나타내며, ϕ(x) = elu(x)+1로 설정됩니다. 이 근사는 N x N 크기의 어텐션 행렬(attention matrix) QK T를 명시적으로 계산하는 과정을 생략함으로써 효율성을 달성합니다. 과거의 이러한 시도에 너무 길게 머물고 싶지는 않지만, 핵심은 계산 시간과 메모리 사용량 측면에서 복잡도를 시퀀스 길이의 제곱에 비례하는 O(n 2 ) 수준에서 선형적으로 비례하는 O(n) 수준으로 크게 낮추었다는 점입니다. 이를 통해 긴 시퀀스(long sequences)에 대한 어텐션 연산을 훨씬 더 효율적으로 만들 수 있었습니다. 그러나 당시에는 이러한 효율성 개선이 모델 정확도(model accuracy)의 저하로 이어지는 경향이 있었기 때문에, 실제로는 널리 채택되지 못했습니다. 따라서 이러한 초기 선형 어텐션 변형이 오픈 가중치 최첨단 LLM에 적용된 사례는 거의 찾아볼 수 없었습니다. 당시의 기술로는 효율성과 성능 간의 균형을 맞추기 어려웠던 것이 주요 원인으로 분석됩니다.

---

**2.3 선형 어텐션의 부활(Linear Attention Revival)**

올해 하반기 들어 선형 어텐션(linear attention) 변형들이 다시금 연구자들의 주목을 받기 시작했으며, 아래 그림에 제시된 바와 같이 일부 모델 개발자들 사이에서 활발한 논의가 이어지고 있습니다. 그림 5: 선형 어텐션 하이브리드 아키텍처(linear attention hybrid architectures)의 개요. 이러한 부활의 배경에는 기존 트랜스포머의 한계를 극복하기 위한 새로운 하이브리드 접근 방식과 개선된 근사 기법들이 있습니다.

가장 먼저 주목할 만한 모델은 라이트닝 어텐션(lightning attention)을 활용한 MiniMax-M1이었습니다. 이 모델은 4,560억 개의 매개변수(mixture-of-experts, MoE 모델 기준)와 460억 개의 활성 매개변수(active parameters)를 가진 MoE 모델로, 지난 6월에 공개되었습니다. 이어서 8월에는 Qwen3 팀이 Qwen3-Next를 발표했으며, 이에 대해서는 이 글의 앞부분에서 더 자세히 다루었습니다. 그리고 9월에는 DeepSeek 팀이 DeepSeek V3.2를 공개했습니다. (엄밀히 말해 DeepSeek V3.2의 희소 어텐션 메커니즘(sparse attention mechanism)은 선형이 아니라 이차 미만(subquadratic)의 계산 비용을 가지지만, 효율성 측면에서 MiniMax-M1, Qwen3-Next, Kimi Linear와 같은 범주에 포함시키는 것이 적절하다고 볼 수 있습니다.) 이 세 모델(MiniMax-M1, Qwen3-Next, DeepSeek V3.2)은 대부분 또는 모든 레이어에서 기존의 이차 어텐션(quadratic attention) 변형을 효율적인 선형 또는 이차 미만 변형으로 대체함으로써 성능 개선을 시도했습니다.

흥미롭게도 최근 예상치 못한 반전이 있었습니다. MiniMax 팀은 선형 어텐션을 제외한 새로운 2,300억 개의 매개변수를 갖춘 M2 모델을 선보이면서 기존의 어텐션 방식으로 회귀하는 흥미로운 변화를 보였습니다. 팀은 선형 어텐션이 실제 프로덕션 LLM 환경에서 여러 난점을 야기한다고 언급했습니다. 일반적인 프롬프트(prompts)에서는 잘 작동하는 것처럼 보였지만, 추론(reasoning) 및 다중 턴 작업(multi-turn tasks)에서 정확도가 현저히 낮아지는 문제가 발생했는데, 이는 일반적인 대화 세션뿐만 아니라 에이전트 애플리케이션(agentic applications)에서도 매우 중요한 요소입니다. 이러한 결과는 선형 어텐션이 결국 추구할 가치가 없는 방향일 수도 있다는 전환점이 될 수 있었습니다. 그러나 상황은 더욱 흥미롭게 전개되었습니다. 10월에는 Kimi 팀이 선형 어텐션을 사용하는 새로운 Kimi Linear 모델을 출시했습니다. 특히 Kimi Linear와 Qwen3-Next는 모두 Gated DeltaNet이라는 기술을 채택했는데, 이는 다음 섹션들에서 하이브리드 어텐션 아키텍처(hybrid attention architecture)의 대표적인 예시로 논의할 가치가 있습니다.

---

**2.4 Qwen3-Next**

Qwen3-Next 모델부터 살펴보겠습니다. 이 모델은 기존의 어텐션 메커니즘(attention mechanism)을 Gated DeltaNet과 Gated Attention의 하이브리드(hybrid) 형태로 교체했습니다. 이러한 변화는 메모리 사용량 측면에서 획기적인 개선을 가져와, 기본적으로 262,000 토큰에 달하는 긴 컨텍스트 길이(context length)를 지원할 수 있게 되었습니다 (이전 235B-A22B 모델은 기본적으로 32k를 지원했으며, YaRN 스케일링(scaling)을 통해 131k를 지원했습니다). Qwen3-Next의 하이브리드 메커니즘(hybrid mechanism)은 아래 그림에 나타난 바와 같이 Gated DeltaNet 블록(blocks)과 Gated Attention 블록을 3대 1의 비율로 조합하여 사용합니다. 그림 6: 게이티드 어텐션(gated attention) 및 Gated DeltaNet을 사용한 Qwen3-Next.

위 그림에서 묘사된 것처럼, 어텐션 메커니즘(attention mechanism)은 게이티드 어텐션(gated attention) 또는 Gated DeltaNet 중 하나로 구현됩니다. 이는 이 아키텍처(architecture)에 포함된 48개의 트랜스포머 블록(transformer blocks, 레이어)이 이 두 가지 방식을 번갈아 사용한다는 것을 의미합니다. 구체적으로는 3대 1 비율로 번갈아 사용되므로, 예를 들어 트랜스포머 블록의 구성은 다음과 같습니다:
──────────────────────────────────
Layer 1 : Linear attention → MoE
Layer 2 : Linear attention → MoE
Layer 3 : Linear attention → MoE
Layer 4 : Full attention → MoE
──────────────────────────────────
Layer 5 : Linear attention → MoE
Layer 6 : Linear attention → MoE
Layer 7 : Linear attention → MoE
Layer 8 : Full attention → MoE
──────────────────────────────────
...
이러한 하이브리드 접근 방식은 순수 선형 어텐션의 효율성과 기존 풀 어텐션의 강력한 전역 컨텍스트 모델링 능력을 결합하려는 시도입니다. 나머지 아키텍처는 대체로 Qwen3와 유사하며 표준적인 형태를 유지합니다. 그림 7: 이전 "일반" Qwen3 모델(왼쪽)과 Qwen3-Next(오른쪽). 그렇다면 이 게이티드 어텐션(gated attention)과 Gated DeltaNet은 정확히 무엇일까요? 다음 섹션에서 자세히 알아보겠습니다.

---

**2.5 게이티드 어텐션(Gated Attention)**

Gated DeltaNet에 대해 본격적으로 알아보기 전에, 먼저 게이트(gate) 개념에 대해 간략히 논의해 봅시다. 이전 그림의 Qwen3-Next 아키텍처(architecture) 상단에서 확인할 수 있듯이, Qwen3-Next는 "게이티드 어텐션(gated attention)"을 활용합니다. 이는 본질적으로 추가적인 시그모이드 게이트(sigmoid gate)가 적용된 일반적인 전체 어텐션(full attention) 방식입니다. 이 게이팅(gating) 메커니즘은 설명의 편의를 위해 제가 "LLMs from Scratch" 책 3장의 코드를 기반으로 한 MultiHeadAttention 구현에 간단히 추가한 수정 사항으로 아래에 제시됩니다:
```python
class MultiHeadAttention(nn.Module):
    def __init__(self, d_in, d_out, block_size, dropout, num_heads, qkv_bias=False):
        super().__init__()
        self.d_out = d_out
        self.num_heads = num_heads
        self.head_dim = d_out // num_heads # Reduce d_out to head_dim
        self.W_query = nn.Linear(d_in, d_out, bias=qkv_bias)
        self.W_key = nn.Linear(d_in, d_out, bias=qkv_bias)
        self.W_value = nn.Linear(d_in, d_out, bias=qkv_bias)
        self.out_proj = nn.Linear(d_out, d_out)
        self.dropout = nn.Dropout(dropout)
        self.register_buffer('mask', torch.triu(torch.ones(block_size, block_size), diagonal=1))

        # Gating mechanism
        self.gate_proj = nn.Linear(d_in, d_out) # Projects input to gating signal

    def forward(self, x, mask=None):
        b, num_tokens, d_in = x.shape

        keys = self.W_key(x)
        queries = self.W_query(x)
        values = self.W_value(x)

        keys = keys.view(b, num_tokens, self.num_heads, self.head_dim)
        values = values.view(b, num_tokens, self.num_heads, self.head_dim)
        queries = queries.view(b, num_tokens, self.num_heads, self.head_dim)

        keys = keys.transpose(1, 2)
        queries = queries.transpose(1, 2)
        values = values.transpose(1, 2)

        attn_scores = queries @ keys.transpose(2, 3)
        attn_scores = attn_scores / math.sqrt(self.head_dim)

        if mask is not None:
            attn_scores.masked_fill_(mask.bool(), -torch.inf)

        attn_weights = torch.softmax(attn_scores, dim=-1)
        attn_weights = self.dropout(attn_weights)

        context_vec = (attn_weights @ values).transpose(1, 2).contiguous().view(b, num_tokens, self.d_out)

        # Apply gating
        gate_signal = torch.sigmoid(self.gate_proj(x))
        gated_context_vec = context_vec * gate_signal

        return self.out_proj(gated_context_vec)
```
위 코드에서 볼 수 있듯이, 일반적인 어텐션(attention) 계산이 완료된 후, 모델은 동일한 입력에서 파생된 별도의 게이팅 신호(gating signal)를 활용합니다. 이 신호는 0과 1 사이의 값으로 정규화하기 위해 시그모이드(sigmoid) 함수를 통과한 다음, 어텐션 출력(attention output)과 곱해집니다. 이러한 조작은 모델이 특정 정보의 중요도를 유연하게 조절하여, 동적으로 특정 특성을 부각시키거나 억제할 수 있도록 합니다. Qwen3-Next 개발자들은 이 방식이 훈련 안정성(training stability)을 향상시킨다고 설명합니다:

"어텐션 출력 게이팅 메커니즘(attention output gating mechanism)은 어텐션 싱크(Attention Sink) 및 대규모 활성화(Massive Activation)와 같은 문제를 제거하여 모델 전반의 수치적 안정성(numerical stability)을 보장합니다."

요약하자면, 게이티드 어텐션(gated attention)은 표준 어텐션(standard attention)의 결과물을 조절하는 역할을 합니다. 이는 모델이 학습 과정에서 특정 정보 흐름을 제어함으로써, 과도한 활성화나 정보 손실과 같은 문제를 완화하는 데 기여합니다. 다음 섹션에서는 어텐션 메커니즘(attention mechanism) 자체를 순환 델타 규칙 메모리 업데이트(recurrent delta-rule memory update)로 대체하는 Gated DeltaNet에 대해 자세히 논의할 것입니다.

---

**2.6 Gated DeltaNet**

그렇다면 Gated DeltaNet은 정확히 무엇일까요? Gated DeltaNet (Gated Delta Network의 약자)은 Qwen3-Next의 선형 어텐션 레이어(linear-attention layer)로서, 표준 소프트맥스 어텐션(softmax attention)을 대체하기 위해 고안되었습니다. 이 개념은 "Gated Delta Networks: Improving Mamba2 with Delta Rule" 논문에서 채택되었습니다. Gated DeltaNet은 원래 Mamba2의 개선된 버전으로 제안되었으며, Mamba2의 게이티드 감쇠 메커니즘(gated decay mechanism)과 델타 규칙(delta rule)을 효과적으로 결합합니다. Mamba는 상태 공간 모델(state-space model, 트랜스포머의 대안)의 일종으로, 그 자체로 미래에 별도로 심도 있게 다룰 가치가 있는 중요한 주제입니다.

델타 규칙의 핵심은 새로운 정보와 기존 예측치 간의 편차(델타, Δ)를 산출하여, 이를 기반으로 모델의 내부 기억 상태인 은닉 상태(hidden state)를 갱신하는 과정에 있습니다. 이 은닉 상태는 시퀀스 처리 과정에서 모델의 메모리 역할을 합니다. (참고: 고전적인 기계 학습 문헌에 익숙한 독자들은 이를 생물학에서 영감을 받은 헵 학습(Hebbian learning)과 유사하다고 생각할 수 있습니다: "함께 발화하는 세포는 함께 연결된다." 이는 기본적으로 퍼셉트론 업데이트 규칙(perceptron update rule) 및 경사 하강 기반 학습(gradient descent-based learning)의 전신이지만, 감독(supervision)이 없습니다.) Gated DeltaNet은 앞서 논의된 게이티드 어텐션(gated attention)의 게이트와 유사한 게이트를 가지고 있지만, 아래 그림에서 보듯이 로지스틱 시그모이드 활성화(logistic sigmoid activation) 대신 SiLU를 사용합니다. (SiLU의 선택은 표준 시그모이드(sigmoid)에 비해 경사 흐름(gradient flow)과 안정성(stability)을 개선하기 위한 것으로 풀이됩니다.) 그림 8: 게이티드 어텐션(gated attention)과 Gated DeltaNet 비교.

그러나 위 그림에서 볼 수 있듯이, 출력 게이트(output gate) 외에도 Gated DeltaNet의 "게이티드(gated)"는 몇 가지 추가 게이트를 포함합니다: α (감쇠 게이트, decay gate)는 시간이 지남에 따라 메모리(memory)가 얼마나 빨리 감쇠하거나 재설정되는지를 제어하며, β (업데이트 게이트, update gate)는 새로운 입력이 현재 상태(state)를 얼마나 강력하게 수정하는지를 조절합니다. 이러한 다중 게이트 메커니즘은 메모리 관리를 더욱 정교하게 만들어 줍니다.

위에서 묘사된 Gated DeltaNet의 단순화된 버전(컨볼루션 혼합(convolutional mixing)이 없는 경우)은 다음 코드와 같이 구현될 수 있습니다 (이 코드는 Qwen3 팀의 공식 구현에서 영감을 받았습니다):
```python
import torch
import torch.nn as nn
import torch.nn.functional as F

class GatedDeltaNet(nn.Module):
    def __init__(self, d_model, num_heads, head_dim, dropout=0.1):
        super().__init__()
        self.d_model = d_model
        self.num_heads = num_heads
        self.head_dim = head_dim
        
        # Linear projections for input x
        self.proj_x = nn.Linear(d_model, num_heads * head_dim)
        
        # Projections for gates alpha, beta, and output gate
        self.proj_alpha = nn.Linear(d_model, num_heads * head_dim)
        self.proj_beta = nn.Linear(d_model, num_heads * head_dim)
        self.proj_output_gate = nn.Linear(d_model, num_heads * head_dim)
        
        self.out_proj = nn.Linear(num_heads * head_dim, d_model)
        self.dropout = nn.Dropout(dropout)

    def forward(self, x, state=None):
        b, n, d_model = x.shape # batch_size, sequence_length, d_model

        # Initialize state if not provided (for the first token)
        if state is None:
            # S is the recurrent memory state, initialized to zeros
            # Shape: (b, num_heads, head_dim, head_dim)
            state = x.new_zeros(b, self.num_heads, self.head_dim, self.head_dim)

        output_sequence = []
        for t in range(n): # Process tokens one by one
            x_t = x[:, t, :] # Current token input (b, d_model)

            # Project current input for value and gates
            v_t = self.proj_x(x_t).view(b, self.num_heads, self.head_dim) # (b, num_heads, head_dim)
            alpha_t = F.silu(self.proj_alpha(x_t)).view(b, self.num_heads, self.head_dim) # (b, num_heads, head_dim)
            beta_t = F.silu(self.proj_beta(x_t)).view(b, self.num_heads, self.head_dim) # (b, num_heads, head_dim)
            
            # Update the state S
            # S_t = alpha_t * S_{t-1} + beta_t * v_t * v_t.transpose(-1, -2)
            # Simplified for illustration: S_t = alpha_t * S_{t-1} + beta_t * v_t
            # Here, we'll use a simpler update rule for the state S for readability
            # A more accurate representation of the paper's DeltaNet state update
            # would involve outer products or more complex interactions.
            
            # Let's adjust state shape to (b, num_heads, head_dim) for this simplified example
            # In the actual paper, S is (b, num_heads, head_dim, head_dim)
            # and the update involves outer products or more complex matrix ops.
            # For this simplified code, let's assume S is a vector per head.
            
            if state.shape[-1] != self.head_dim: # Re-initialize if shape is wrong for this simplification
                 state = x.new_zeros(b, self.num_heads, self.head_dim)
            
            state = alpha_t * state + beta_t * v_t # Element-wise update

            # Compute output for current token
            output_t = state.mean(dim=1) # Aggregate across heads for simplicity (b, head_dim)
            
            # Apply output gate
            output_gate_t = torch.sigmoid(self.proj_output_gate(x_t)).view(b, self.num_heads, self.head_dim)
            output_t = output_t * output_gate_t.mean(dim=1) # Apply output gate

            output_sequence.append(output_t)

        output = torch.stack(output_sequence, dim=1) # (b, n, head_dim)
        output = self.out_proj(output) # (b, n, d_model)
        return output, state # Return final output and updated state
```
(간단함을 위해 Qwen3-Next와 Kimi Linear가 사용하는 컨볼루션 혼합(convolutional mixing)을 생략하여 코드를 더 읽기 쉽게 하고 순환적 측면(recurrent aspects)에 집중했습니다.)

따라서 위에서 보듯이 표준 (또는 게이티드) 어텐션(attention)과는 많은 차이점이 존재합니다. 게이티드 어텐션(gated attention)에서는 모델이 모든 토큰(tokens) 간에 일반 어텐션(normal attention)을 계산합니다 (모든 토큰은 다른 모든 토큰을 주시하거나 살펴봅니다). 그런 다음 어텐션 출력(attention output)을 얻은 후, 게이트(시그모이드)가 해당 출력 중 얼마를 유지할지 결정합니다. 핵심은 여전히 컨텍스트 길이(context length)에 따라 이차적으로 확장되는 일반 스케일드 닷 프로덕트 어텐션(scaled-dot product attention)이라는 것입니다. 다시 한번 상기시키자면, 스케일드 닷 프로덕트 어텐션(scaled-dot product attention)은 softmax(QKᵀ)V로 계산됩니다. 여기서 Q와 K는 n x d 행렬이며, n은 입력 토큰(input tokens)의 수이고 d는 임베딩 차원(embedding dimension)입니다. 따라서 QKᵀ는 n x n 어텐션 행렬(attention matrix)을 생성하며, 이는 n x d 차원의 값 행렬(value matrix) V와 곱해집니다. 그림 9: 토큰 수 n에 따라 확장되는 전통적인 어텐션 메커니즘(attention mechanism) (다시 한번).

게이티드 델타넷은 N x N 크기의 어텐션 매트릭스를 명시적으로 생성하지 않습니다. 그 대신, 모델은 입력 토큰들을 순차적으로 처리하며, 각 토큰이 유입될 때마다 지속적으로 갱신되는 내부 기억 장치(상태)를 운용합니다. 이것이 구현된 방식이며, 여기서 S는 각 시간 단계 t에 대해 순환적으로(recurrently) 업데이트되는 상태(state)입니다. 그리고 게이트(gates)는 해당 메모리(memory)가 어떻게 변하는지 제어합니다: α (알파)는 이전 메모리 중 얼마를 잊을지(감쇠) 조절하고, β (베타)는 시간 단계 t의 현재 토큰이 메모리(memory)를 얼마나 업데이트하는지 조절합니다. (그리고 위 코드 조각에는 표시되지 않은 최종 출력 게이트(output gate)는 게이티드 어텐션(gated attention)과 유사하며, 출력 중 얼마를 유지할지 제어합니다.) 따라서 어떤 의미에서는 Gated DeltaNet의 이 상태 업데이트(state update)는 순환 신경망(RNNs)이 작동하는 방식과 유사합니다. 장점은 컨텍스트 길이(context length)에 따라 이차적으로가 아닌 선형적으로(for-loop을 통해) 확장된다는 것입니다. 이 순환 상태 업데이트(recurrent state update)의 단점은 일반 (또는 게이티드) 어텐션(attention)과 비교할 때, 전체 쌍별 어텐션(full pairwise attention)에서 오는 전역 컨텍스트 모델링(global context modeling) 능력을 희생한다는 것입니다. Gated DeltaNet은 어느 정도 컨텍스트(context)를 여전히 포착할 수 있지만, 메모리(S) 병목 현상(bottleneck)을 거쳐야 합니다. 해당 메모리(memory)는 고정된 크기이므로 더 효율적이지만, RNN과 유사하게 과거 컨텍스트(past context)를 단일 은닉 상태(hidden state)로 압축합니다. 이것이 Qwen3-Next와 Kimi Linear 아키텍처(architectures)가 모든 어텐션 레이어(attention layers)를 DeltaNet 레이어로 대체하지 않고 앞서 언급된 3:1 비율을 사용하는 이유입니다.

---

**2.7 DeltaNet 메모리 절약(DeltaNet Memory Savings)**

이전 섹션에서는 컨텍스트 길이(context length)에 대한 이차적 계산 복잡도(quadratic compute complexity) 대신 선형적 계산 복잡도(linear compute complexity) 측면에서 DeltaNet이 전체 어텐션(full attention)보다 가지는 장점을 논의했습니다. 선형 계산 복잡도(linear compute complexity) 외에 델타넷의 또 다른 중요한 이점은 메모리 사용의 효율성입니다. 이는 델타넷 모듈이 KV 캐시의 크기를 불필요하게 늘리지 않기 때문입니다. (KV 캐싱(KV caching)에 대한 자세한 내용은 저의 "Understanding and Coding the KV Cache in LLMs from Scratch" 글을 참조하십시오.)

KV 캐시(KV cache)는 대규모 언어 모델이 텍스트를 생성할 때 이전에 계산된 키(key)와 값(value) 임베딩을 저장하여, 매번 전체 시퀀스를 재계산하는 대신 새로운 토큰을 생성할 때 이 정보를 재사용함으로써 추론 속도를 높이는 메커니즘입니다. 그러나 시퀀스 길이가 길어질수록 이 KV 캐시의 크기가 기하급수적으로 증가하여 메모리 부족 문제를 야기합니다.

DeltaNet은 이러한 문제를 회피합니다. 대신, 앞서 언급했듯이 고정 크기의 순환 상태(recurrent state)를 유지하므로 메모리(memory)는 컨텍스트 길이(context length)에 따라 일정하게 유지됩니다. 이는 특히 매우 긴 시퀀스를 처리할 때 기존 트랜스포머에 비해 엄청난 이점을 제공합니다.

일반적인 멀티 헤드 어텐션(MHA) 레이어의 경우, KV 캐시(KV cache) 크기를 다음과 같이 계산할 수 있습니다:
KV_cache_MHA ≈ batch_size × n_tokens × n_heads × d_head × 2 × bytes
(여기서 2를 곱하는 이유는 캐시(cache)에 키(keys)와 값(values)을 모두 저장하기 때문입니다.)

위에서 구현된 단순화된 DeltaNet 버전의 경우 다음과 같습니다:
KV_cache_DeltaNet = batch_size × n_heads × d_head × d_head × bytes

KV_cache_DeltaNet 메모리 크기에는 컨텍스트 길이(n_tokens) 의존성이 없다는 점에 유의하십시오. 또한, 별도의 키(keys)와 값(values) 대신 메모리 상태 S만 저장하므로 2 × bytes가 그냥 bytes가 됩니다. 하지만 이제 여기에 이차적인 d_head × d_head가 있다는 점에 유의하십시오. 이는 상태(state)에서 비롯됩니다: S = x.new_zeros(b, self.num_heads, self.head_dim, self.head_dim). 그러나 헤드 차원(head dimension)은 일반적으로 상대적으로 작기 때문에 (예: Qwen3-Next에서는 128) 일반적으로 큰 문제가 되지 않습니다. 컨볼루션 혼합(convolutional mixing)이 포함된 전체 버전은 커널 크기(kernel size) 등을 포함하여 약간 더 복잡하지만, 위의 공식들은 Gated DeltaNet의 주요 경향과 동기를 설명하기에 충분합니다. 그림 10: 증가하는 KV 캐시(KV cache) 크기 비교. 3:1 비율은 Gated DeltaNet과 전체 어텐션 레이어(full attention layers)의 비율을 나타냅니다. 계산은 emb_dim=2048, n_heads=16, n_layers=48, bf16을 가정합니다. 이 코드를 재현하는 코드는 여기에서 찾을 수 있습니다: https://github.com/rasbt/LLMs-from-scratch/tree/main/ch04/08_deltanet.

---

**2.8 Kimi Linear 대 Qwen3-Next (Kimi Linear vs. Qwen3-Next)**

Kimi Linear는 Qwen3-Next와 여러 구조적인 측면에서 유사점을 공유합니다. 두 모델 모두 효율성과 성능의 균형을 맞추기 위해 하이브리드 어텐션 전략(hybrid attention strategy)을 채택합니다. 구체적으로, 이들은 경량 선형 어텐션(lightweight linear attention) 방식과 기존의 연산량이 더 큰 전체 어텐션 레이어(full attention layers)를 결합합니다.

특히, 두 모델 모두 3대 1의 비율을 채택하고 있는데, 이는 세 개의 선형 게이티드 델타넷 변형 트랜스포머 블록 이후에 하나의 완전한 어텐션 블록이 배치되는 구조를 의미합니다. 그림 11: Qwen3-Next와 Kimi Linear 나란히. Gated DeltaNet은 순환 신경망(recurrent neural networks)에서 영감을 받은 선형 어텐션(linear attention) 변형으로, "Gated Delta Networks: Improving Mamba2 with Delta Rule" 논문에서 제시된 게이팅 메커니즘(gating mechanism)을 포함합니다. 어떤 의미에서 Gated DeltaNet은 Mamba 스타일 게이팅(Mamba-style gating)을 가진 DeltaNet이며, DeltaNet은 선형 어텐션 메커니즘(linear attention mechanism)의 한 형태입니다 (이에 대해서는 다음 섹션에서 더 자세히 설명합니다).

위 그림 11의 오른쪽 상자에서 묘사된 Kimi Linear의 MLA는 시그모이드 게이트(sigmoid gate)를 사용하지 않습니다. 이러한 생략은 저자들이 아키텍처(architecture)를 표준 MLA와 더 직접적으로 비교할 수 있도록 의도된 것으로 보이지만, 향후 버전에서는 이를 추가할 계획이라고 밝혔습니다. 또한 위 그림의 Kimi Linear 부분에서 RoPE 상자(RoPE box)가 생략된 것 역시 의도적인 설계입니다. Kimi 모델은 멀티 헤드 잠재 어텐션(multi-head latent attention, MLA) 레이어(전역 어텐션, global attention)에 NoPE(No Positional Embedding)를 적용합니다. 저자들이 언급했듯이, 이는 MLA가 추론(inference) 시 순수한 멀티 쿼리 어텐션(multi-query attention)으로 작동하도록 하여, 긴 컨텍스트 스케일링(long-context scaling)을 위한 RoPE 재조정(retuning)의 필요성을 없앱니다 (위치 편향(positional bias)은 Kimi Delta Attention 블록(blocks)에 의해 처리되는 것으로 추정됩니다). MLA 및 그룹화된 쿼리 어텐션(grouped-query attention)의 특별한 경우인 멀티 쿼리 어텐션(multi-query attention)에 대한 자세한 내용은 저의 "The Big LLM Architecture Comparison" 글을 참조하십시오.

---

**2.9 Kimi Delta 어텐션(Kimi Delta Attention)**

Kimi Linear는 Qwen3-Next의 선형 어텐션 메커니즘(linear attention mechanism)을 Kimi Delta Attention (KDA) 메커니즘으로 개선했는데, 이는 게이티드 델타넷(Gated DeltaNet)을 본질적으로 더욱 정교하게 다듬은 형태입니다. Qwen3-Next가 메모리 감쇠율(memory decay rate)을 제어하기 위해 스칼라 게이트(scalar gate, 어텐션 헤드(attention head)당 하나의 값)를 적용하는 반면, Kimi Linear는 이를 각 특징 차원(feature dimension)에 대한 채널별 게이팅(channel-wise gating)으로 대체합니다. 저자들에 따르면, 이 방식은 메모리(memory)에 대한 훨씬 더 세밀한 제어권을 제공하며, 이는 결과적으로 긴 컨텍스트 추론(long-context reasoning) 능력을 크게 향상시킵니다.

또한, 전체 어텐션 레이어(full attention layers)의 경우, Kimi Linear는 Qwen3-Next의 게이티드 어텐션 레이어(gated attention layers, 본질적으로 출력 게이팅(output gating)이 있는 표준 멀티 헤드 어텐션 레이어)를 멀티 헤드 잠재 어텐션(multi-head latent attention, MLA)으로 대체합니다. 이는 DeepSeek V3/R1에서 사용된 동일한 MLA 메커니즘(mechanism)이지만 (저의 "The Big LLM Architecture Comparison" 글에서 논의했듯이) 추가 게이트(gate)가 있습니다. (요약하자면, MLA는 키/값 공간(key/value space)을 압축하여 KV 캐시(KV cache) 크기를 줄이는 역할을 합니다.)

Qwen3-Next와의 직접적인 비교는 없지만, Gated DeltaNet 논문의 Gated DeltaNet-H1 모델(본질적으로 슬라이딩 윈도우 어텐션(sliding-window attention)이 있는 Gated DeltaNet)과 비교했을 때, Kimi Linear는 동일한 토큰 생성 속도(token-generation speed)를 유지하면서도 더 높은 모델링 정확도(modeling accuracy)를 달성하는 것으로 나타났습니다. 그림 12: Kimi Linear 논문(https://arxiv.org/abs/2510.26692)의 주석이 달린 그림으로, Kimi Linear가 GatedDeltaNet만큼 빠르고, 멀티 헤드 잠재 어텐션(multi-head latent attention)을 사용하는 아키텍처(DeepSeek V3/R1과 같은)보다 훨씬 빠르며, 더 높은 벤치마크 성능(benchmark performance)을 가짐을 보여줍니다.

또한, DeepSeek-V2 논문의 절제 연구(ablation studies)에 따르면, 하이퍼파라미터(hyperparameters)를 신중하게 선택하면 MLA는 일반 전체 어텐션(full attention)과 동등한 성능을 보일 수 있습니다. 그리고 Kimi Linear가 긴 컨텍스트(long-context) 및 추론 벤치마크(reasoning benchmarks)에서 MLA보다 유리하게 비교된다는 사실은 선형 어텐션(linear attention) 변형이 더 큰 최첨단 모델(state-of-the-art models)에 대해 다시 한번 유망한 가능성을 제시함을 의미합니다. 그렇긴 하지만, Kimi Linear는 480억 개의 매개변수를 가지지만, Kimi K2보다 20배 작은 규모입니다. Kimi 팀이 다가오는 K3 모델에 이러한 접근 방식을 채택할지 지켜보는 것은 매우 흥미로울 것입니다.

---

**2.10 어텐션 하이브리드의 미래(The Future of Attention Hybrids)**

선형 어텐션(linear attention)은 결코 새로운 개념이 아니지만, 하이브리드 접근 방식(hybrid approaches)의 최근 부활은 연구자들이 트랜스포머(transformers)를 더욱 효율적으로 만들기 위한 실용적인 방법들을 진지하게 탐색하고 있음을 시사합니다. 예를 들어, Kimi Linear는 일반 전체 어텐션(full attention)과 비교하여 KV 캐시(KV cache)를 75% 절감하고 디코딩 처리량(decoding throughput)을 최대 6배까지 향상시키는 놀라운 성과를 보여주었습니다.

이번 세대의 선형 어텐션 변형들이 과거의 시도들과 구별되는 점은, 이제 표준 어텐션을 전면적으로 대체하기보다는 상호 보완적으로 활용된다는 데 있습니다. 이는 특정 레이어에서는 효율적인 선형 어텐션을 사용하고, 중요한 전역 컨텍스트를 유지해야 하는 레이어에서는 전체 어텐션을 사용하는 전략을 의미합니다. 이러한 절충적인 접근 방식은 효율성 증대와 함께 모델의 핵심 성능을 유지하는 데 기여합니다.

앞으로, 다음 어텐션 하이브리드(attention hybrids)의 물결은 긴 컨텍스트 안정성(long-context stability)과 추론 정확도(reasoning accuracy)를 더욱 개선하여 전체 어텐션(full-attention) 기반 모델의 최첨단(state-of-the-art) 성능에 더욱 근접하는 데 초점을 맞출 것으로 예상됩니다. 이는 단순히 계산 비용을 줄이는 것을 넘어, 모델이 복잡한 정보를 더욱 효과적으로 이해하고 처리할 수 있도록 하는 방향으로 진화할 것입니다.

---

**3. 텍스트 확산 모델(Text Diffusion Models)**

표준 자동 회귀 방식의 LLM 구조에서 벗어나 더욱 혁신적인 접근 방식을 취한 것이 바로 텍스트 확산 모델(text diffusion models) 계열입니다. 독자 여러분은 아마도 확산 모델(diffusion models)에 익숙할 것입니다. 이는 2020년 "Denoising Diffusion Probabilistic Models" 논문을 기반으로 이미지를 생성하는 모델(생성적 적대 신경망(generative adversarial networks)의 후속 모델)이며, 나중에 Stable Diffusion 등에 의해 구현, 확장 및 대중화되었습니다. 그림 13: 2022년 저의 첫 Substack 글에서 가져온 이미지 확산 과정(image diffusion process)의 설명. 여기서는 왼쪽에서 오른쪽으로 가우시안 노이즈(Gaussian noise)가 추가되며, 모델의 과제는 노이즈를 제거하는 방법(오른쪽에서 왼쪽으로)을 학습하는 것입니다. 이처럼 점진적으로 노이즈를 제거하며 실제와 같은 데이터를 생성하는 확산 모델의 강력한 잠재력은 텍스트 생성 분야에도 적용되기 시작했습니다.

---

**3.1 왜 텍스트 확산에 대해 연구하는가?(Why Work on Text Diffusion?)**

2022년 "Diffusion-LM Improves Controllable Text Generation" 논문이 발표되면서, 연구자들이 텍스트 생성(generating text)을 위해 확산 모델(diffusion models)을 채택하기 시작하는 중요한 전환점을 맞이했습니다. 이후 2025년에는 수많은 텍스트 확산(text diffusion) 관련 논문들이 쏟아져 나왔습니다. 제 개인적인 논문 북마크 목록만 확인해 보더라도, 무려 39개의 텍스트 확산 모델(text diffusion models)이 등록되어 있었습니다! 이러한 모델들의 인기가 급증하고 있음을 고려할 때, 마침내 이들에 대해 이야기할 때가 되었다고 판단했습니다. 그림 14: 이 섹션은 텍스트 확산 모델(text diffusion models)을 다룹니다.

그렇다면 확산 모델(diffusion models)이 어떤 장점을 가지며, 연구자들은 왜 이를 전통적인 자기회귀 LLM(autoregressive LLMs)의 대안으로 진지하게 고려하고 있을까요? 전통적인 트랜스포머 기반(자기회귀) LLM은 한 번에 하나의 토큰(token)을 순차적으로 생성합니다. 편의상 이들을 단순히 자기회귀 LLM(autoregressive LLMs)이라고 부르겠습니다. 반면, 확산 기반 대규모 언어 모델(Diffusion LLM)의 핵심적인 이점은 단일 토큰을 순차적으로 생성하는 방식이 아니라, 다수의 토큰을 동시에 병렬적으로 생성할 수 있다는 점입니다. 물론 확산 LLM(diffusion LLMs)도 여전히 여러 번의 노이즈 제거 단계(denoising steps)를 거쳐야 합니다. 그러나 확산 모델이 각 단계에서 모든 토큰(tokens)을 병렬로 생성하기 위해 예를 들어 64번의 노이즈 제거 단계(denoising steps)를 필요로 하더라도, 이는 2,000개의 토큰 응답을 생성하기 위해 2,000번의 순차적 생성 단계(sequential generation steps)를 수행하는 것보다 여전히 계산적으로 훨씬 더 효율적입니다. 이러한 병렬 처리 능력은 특히 긴 응답을 생성할 때 지연 시간(latency)을 획기적으로 줄일 수 있는 잠재력을 가집니다.

---

**3.2 노이즈 제거 과정(The Denoising Process)**

확산 LLM(diffusion LLM)의 노이즈 제거 과정(denoising process)은 일반적인 이미지 확산 모델(image diffusion models)의 작동 방식과 매우 유사하며, 아래 GIF에 시각적으로 설명되어 있습니다. (주요 차이점은 이미지 모델이 픽셀(pixels)에 가우시안 노이즈(Gaussian noise)를 추가하는 반면, 텍스트 확산(text diffusion)은 확률적으로 토큰(tokens)을 마스킹(masking)하여 시퀀스(sequences)를 손상시킨다는 것입니다.) 이 실험을 위해, 저는 올해 초에 발표된 "Large Language Diffusion Models (LLaDA)" 논문의 8B 지시 모델(instruct model)을 직접 실행해 보았습니다. 그림 15: 8B LLaDA 모델을 사용한 노이즈 제거 과정(denoising process)의 설명.

위 애니메이션에서 볼 수 있듯이, 텍스트 확산 과정(text diffusion process)은 [MASK] 토큰(tokens)을 실제 텍스트 토큰으로 연속적으로 대체해 나가면서 최종 답변을 생성합니다. 만약 BERT와 마스킹된 언어 모델링(masked language modeling)에 익숙하다면, 이 확산 과정을 BERT의 순방향 전달(forward pass)을 반복적으로 적용하는 것(BERT가 다양한 마스킹 비율(masking rates)로 사용되는 경우)으로 이해할 수 있습니다.

아키텍처(architecture) 측면에서, 확산 LLM(diffusion LLMs)은 일반적으로 디코더 스타일 트랜스포머(decoder-style transformers)를 기반으로 하지만, 인과적 어텐션 마스크(causal attention mask)가 존재하지 않습니다. 예를 들어, 앞서 언급된 LLaDA 모델은 Llama 3 아키텍처(architecture)를 활용합니다. 인과적 마스크(causal mask)가 없는 이러한 아키텍처는 "양방향(bidirectional)"이라고 불리는데, 이는 모든 시퀀스 요소(sequence elements)에 한 번에 접근할 수 있기 때문입니다. (이는 역사적인 이유로 "인코더 스타일(encoder-style)"이라고 불리는 BERT 아키텍처(architecture)와 유사하다는 점에 유의하십시오.)

따라서 자기회귀 LLM(autoregressive LLMs)과 확산 LLM(diffusion LLMs)의 주요 차이점(인과적 마스크 제거 외에)은 훈련 목표(training objective)에 있습니다. LLaDA와 같은 확산 LLM(diffusion LLMs)은 다음 토큰 예측 목표(next-token prediction objective) 대신 생성적 확산 목표(generative diffusion objective)를 사용합니다. 이미지 모델(image models)에서 생성적 확산 목표(generative diffusion objective)는 연속적인 픽셀 공간(pixel space)을 다루기 때문에 직관적입니다. 예를 들어, 가우시안 노이즈(Gaussian noise)를 추가하고 노이즈 제거를 학습하는 것은 수학적으로 자연스러운 연산입니다. 그러나 텍스트는 개별적인 토큰들로 이루어져 있기 때문에, 이미지 픽셀처럼 연속적인 공간에서 직접적으로 '노이즈'를 삽입하거나 제거하는 방식은 적용하기 어렵습니다.

따라서 픽셀 강도(pixel intensities)를 교란하는 대신, 이러한 확산 LLM(diffusion LLMs)은 토큰(tokens)을 무작위로 점진적으로 마스킹(masking)하여 텍스트를 손상시킵니다. 각 토큰은 지정된 확률로 특수 마스크 토큰(mask token)으로 대체됩니다. 그런 다음 모델은 각 단계에서 누락된 토큰(tokens)을 예측하는 역 과정(reverse process)을 학습하며, 이는 앞서 그림 15의 애니메이션에서 보듯이 시퀀스(sequence)를 원래 텍스트로 효과적으로 "노이즈 제거(denoises)"(또는 마스크 해제(unmasks))합니다. 그 뒤에 있는 수학을 설명하는 것은 별도의 튜토리얼에 더 적합하겠지만, 대략적으로는 BERT가 확률적 최대 우도 프레임워크(probabilistic maximum-likelihood framework)로 확장된 것으로 생각할 수 있습니다.

---

**3.3 자기회귀 LLM 대 확산 LLM(Autoregressive vs Diffusion LLMs)**

앞서 저는 확산 LLM(diffusion LLMs)이 매력적인 이유는 일반 자기회귀 LLM(autoregressive LLM)처럼 토큰(tokens)을 순차적으로 생성하는 대신 병렬로 생성(또는 노이즈 제거)하기 때문이라고 설명했습니다. 이는 확산 모델(diffusion models)을 자기회귀 LLM(autoregressive LLMs)보다 잠재적으로 더 효율적으로 만들 수 있습니다.

그렇긴 하지만, 전통적인 LLM의 자기회귀적 특성(autoregressive nature)은 그들의 주요 강점 중 하나입니다. 즉, 이전에 생성된 토큰에 조건화하여 다음 토큰을 생성하는 능력은 언어의 흐름과 문맥적 일관성을 유지하는 데 필수적입니다. 순전한 병렬 디코딩 방식이 지닌 난점은 최근 발표된 "ParallelBench: Understanding the Trade-offs of Parallel Decoding in Diffusion LLMs" 논문에서 제시된 예시를 통해 명확하게 파악할 수 있습니다. 그림 16: "ParallelBench: Understanding the Trade-offs of Parallel Decoding in Diffusion LLMs" 논문(https://arxiv.org/abs/2510.04767)의 주석이 달린 그림으로, 병렬 디코딩(parallel decoding)의 문제를 보여줍니다.

예를 들어, 다음 프롬프트(prompt)를 고려해 보십시오:
> "여행할 도시를 무작위로 선택하세요: 뉴욕, 뉴올리언스, 멕시코시티, 파나마시티?"

LLM에게 두 개의 토큰(token)으로 된 답변을 생성하도록 요청한다고 가정해 봅시다. 자기회귀 모델은 먼저 조건부 확률 p(y t = ”New” | X)에 따라 "New" 토큰(token)을 샘플링(sample)할 수 있습니다. 다음 반복에서는 이전에 생성된 "New" 토큰(token)에 조건을 부여하고 "York" 또는 "Orleans"를 선택할 가능성이 높습니다. 이는 조건부 확률 p(y t+1 = ”York” | X, y t = ”New”)와 p(y t+1 = ”Orleans” | X, y t = ”New”)가 모두 상대적으로 높기 때문입니다 ("New"는 훈련 세트(training set)에서 이러한 연속과 자주 함께 나타나기 때문입니다).

하지만 대신 두 토큰(tokens)이 병렬로 샘플링(sample)된다면, 모델은 독립적으로 두 개의 가장 높은 확률 토큰 p(y t = “New” | X)와 p(y {t+1} = “City” | X)를 선택하여 "New City"와 같은 어색한 출력을 초래할 수 있습니다. 이는 모델이 자기회귀적 조건화(autoregressive conditioning)가 부족하여 토큰 간의 의존성(token dependencies)을 효과적으로 포착하지 못하기 때문에 발생합니다.

물론, 위 내용은 확산 LLM(diffusion LLMs)에 조건부 의존성(conditional dependency)이 전혀 없는 것처럼 들리게 하는 단순화된 설명입니다. 이는 사실이 아닙니다. 확산 LLM(diffusion LLM)은 앞서 말했듯이 모든 토큰(tokens)을 병렬로 예측하지만, 예측은 반복적인 정제(노이즈 제거) 단계(iterative refinement (denoising) steps)를 통해 공동으로 의존합니다. 여기서 각 확산 단계(diffusion step)는 현재의 전체 노이즈 텍스트(noisy text)에 조건을 부여합니다. 그리고 토큰(tokens)은 모든 단계에서 교차 어텐션(cross-attention)과 자기 어텐션(self-attention)을 통해 서로에게 영향을 미칩니다. 따라서 모든 위치가 동시에 업데이트되더라도, 업데이트는 공유 어텐션 레이어(attention layers)를 통해 서로에게 조건을 부여합니다. 하지만 앞서 언급했듯이, 이론적으로 2,000개 토큰(token) 답변을 생성할 때 20-60번의 확산 단계(diffusion steps)는 자기회귀 LLM(autoregressive LLM)의 2,000번 추론 단계(inference steps)보다 계산 비용이 저렴할 수 있습니다.

---

**3.4 오늘날의 텍스트 확산(Text Diffusion Today)**

비전 모델(vision models)이 어텐션(attention) 및 트랜스포머 아키텍처(transformer architecture) 자체와 같은 LLM의 구성 요소를 채택하는 반면, 텍스트 기반 LLM이 순수 비전 모델에서 영감을 받아 텍스트용 확산(diffusion for text)을 구현하는 것은 흥미로운 상호 혁신(reciprocal innovation)의 추세입니다. 개인적으로 몇 가지 빠른 데모만 시도해 본 것 외에는 아직 많은 확산 모델(diffusion models)을 심층적으로 사용해 보지 않았지만, 저는 이를 성능과 효율성 간의 중요한 트레이드오프(trade-off) 문제로 봅니다.

확산 과정을 짧게 가져가면 응답 속도는 빨라지나, 결과물의 질이 저하될 위험이 있습니다. 반대로 더 높은 품질의 결과물을 얻기 위해 확산 단계를 늘릴 경우, 계산 비용이 자동 회귀 모델과 유사해질 수 있습니다. "ParallelBench: Understanding the Trade-offs of Parallel Decoding in Diffusion LLMs" 논문의 저자들을 인용하자면:

"[...] 우리는 [확산 LLM(diffusion LLMs)]과 자기회귀 LLM(autoregressive LLMs)을 체계적으로 분석하여 다음을 밝혀냈습니다: (i) 병렬 디코딩(parallel decoding) 하의 [확산 LLM]은 실제 시나리오에서 극적인 품질 저하를 겪을 수 있으며, (ii) 현재의 병렬 디코딩 전략(parallel decoding strategies)은 작업 난이도에 따라 병렬 처리 정도를 조정하는 데 어려움을 겪으므로, 품질 저하 없이 의미 있는 속도 향상을 달성하지 못합니다."

또한, 제가 보는 또 다른 특정 단점은 확산 LLM(diffusion LLMs)이 본질적으로 "체인(chain)" 개념을 가지지 않기 때문에 도구(tools)를 일련의 작업 흐름(chain of thought)의 일부로 활용하기 어렵다는 것입니다. 확산 단계(diffusion steps) 사이에 도구 호출을 끼워 넣는 것이 이론적으로는 가능할 수 있지만, 이는 단순한 구현 문제가 아닐 것이라고 생각합니다. (만약 제가 틀렸다면 기꺼이 수정해 주십시오.)

요컨대, 확산 LLM(diffusion LLMs)은 탐구할 흥미로운 방향인 것으로 보이지만, 현재로서는 자기회귀 LLM(autoregressive LLMs)을 완전히 대체하기는 어려울 수 있습니다. 그러나 저는 이들을 더 작고 온디바이스(on-device) LLM의 흥미로운 대안으로 보거나, 어쩌면 더 작고 증류된(distilled) 자기회귀 LLM을 대체할 수도 있다고 생각합니다. 예를 들어, Google은 텍스트용 Gemini Diffusion 모델을 개발 중이라고 발표했으며, "빠른 응답: 지금까지 우리의 가장 빠른 모델보다 훨씬 빠르게 콘텐츠를 생성합니다."라고 밝혔습니다. 그리고 더 빠르면서도, 벤치마크 성능(benchmark performance)은 빠른 Gemini 2.0 Flash-Lite 모델과 동등하게 유지되는 것으로 보입니다. 모델이 출시되고 사용자들이 다양한 작업과 도메인에서 시도해 본 후, 채택률과 피드백이 어떨지 지켜보는 것은 흥미로울 것입니다. 그림 17: (더 빠른) 확산 LLM(diffusion LLM, Gemini Diffusion)과 빠른 자기회귀 LLM(autoregressive LLM, Gemini 2.0 Flash-Lite)의 벤치마크 성능(benchmark performance) 비교. https://deepmind.google/models/gemini-diffusion/#capabilities에 보고된 수치를 기반으로 합니다.

---

**4. 월드 모델(World Models)**

지금까지 우리는 주로 효율성을 개선하고 모델을 더 빠르거나 확장 가능하게 만드는 데 초점을 맞춘 접근 방식들을 논의했습니다. 이러한 접근 방식들은 일반적으로 약간의 모델링 성능(modeling performance) 저하를 수반하는 경향이 있습니다. 이제 이 섹션의 주제는 다른 관점을 취하며, 효율성보다는 모델링 성능(modeling performance) 자체의 향상에 집중합니다. 이러한 성능 향상은 모델에게 '세계를 이해하는 능력'을 주입함으로써 실현됩니다.

월드 모델(World models)은 전통적으로 언어 모델링(language modeling) 분야와는 독립적으로 개발되어 왔습니다. 그러나 2025년 9월에 발표된 최근 "Code World Models" 논문은 이들을 처음으로 언어 모델링, 특히 코드 생성 맥락에서 직접적으로 관련성 있게 만들었습니다. 이상적으로는, 이 글의 다른 주제들과 마찬가지로, 월드 모델(world models)은 그 자체로 하나의 전용 글(또는 책)로 다루어질 가치가 있습니다. 하지만 Code World Models (CWM) 논문으로 들어가기 전에, 월드 모델(world models)에 대한 간략한 소개를 제공하겠습니다.

---

**4.1 월드 모델의 주요 아이디어(The Main Idea Behind World Models)**

원래 월드 모델(world models)의 핵심 아이디어는 결과를 암묵적으로 모델링하는 것입니다. 즉, 실제로 발생하지 않은 상황에 대해서도 다음에 어떤 일이 일어날지 예측하는 능력을 개발하는 것입니다 (아래 그림에 설명된 대로). 이러한 작동 방식은 인간의 두뇌가 과거의 경험을 토대로 미래에 발생할 일들을 끊임없이 예상하는 것과 흡사합니다. 예를 들어, 우리가 커피나 차 한 잔을 잡으려 할 때, 우리의 뇌는 이미 그것이 얼마나 무거울지 예측하고, 컵을 만지거나 들어 올리기도 전에 잡는 방식을 조절합니다. 그림 18: 월드 모델 시스템(world model system)의 개념적 개요. 에이전트(agent)는 현재 상태(state(t))를 관찰하고 주어진 목표를 달성하기 위해 행동(action(t))을 취함으로써 환경과 상호작용합니다. 동시에 에이전트는 환경의 정신적 시뮬레이션(mental simulation) 역할을 하는 내부 월드 모델(internal world model)을 학습하며, 이를 통해 실제 세계에서 실행하기 전에 결과를 예측하고 행동을 계획할 수 있습니다.

제가 아는 한, "월드 모델(world model)"이라는 용어는 Ha와 Schmidhuber의 2018년 동명 논문 "World Models"에 의해 대중화되었으며, 이 논문은 VAE와 RNN 아키텍처(architecture)를 사용하여 강화 학습 에이전트(reinforcement learning agents)를 위한 내부 환경 시뮬레이터(internal environment simulator)를 학습했습니다. (하지만 이 용어나 개념 자체는 본질적으로 세계나 환경의 개념을 모델링하는 것을 의미하므로, 1980년대의 강화 학습(reinforcement learning) 및 로봇 공학 연구(robotics research)로 거슬러 올라갑니다.) 솔직히, 저는 Yann LeCun의 2022년 기사 "A Path Towards Autonomous Machine Intelligence"가 나오기 전까지는 월드 모델(world models)에 대한 새로운 해석을 인지하지 못했습니다. 이 기사는 본질적으로 LLM 중심의 AI 연구와는 다른, 자율적인 기계 지능을 향한 대안적인 경로를 제시하는 것에 관한 것이었습니다. 그는 LLM이 단순히 텍스트 패턴을 학습하는 것을 넘어, 실제 세계의 인과관계를 이해하고 예측하는 능력을 갖추어야 진정한 지능으로 발전할 수 있다고 주장했습니다.

---

**4.2 비전에서 코드로(From Vision to Code)**

지금까지 월드 모델(world model) 관련 논문들은 주로 비전 도메인(vision domains)에 초점을 맞춰왔습니다. 초기 VAE 및 RNN 기반 모델부터 트랜스포머(transformers), 확산 모델(diffusion models), 심지어 Mamba 레이어 하이브리드(Mamba-layer hybrids)에 이르기까지 광범위한 아키텍처(architectures)를 아울러 이미지 및 비디오 예측, 환경 시뮬레이션 등에 적용되었습니다.

이제 현재 LLM 연구에 더 집중하고 있는 저로서는 "Code World Model" 논문(2025년 9월 30일)이 저의 완전한 주의를 사로잡은 첫 번째 논문입니다 (말장난 의도 없음). 본 모델은 (필자가 인지하기로는) 텍스트-투-텍스트, 더 나아가 코드-투-코드 매핑을 수행하는 최초의 월드 모델에 해당합니다. 이는 월드 모델 개념이 시각적 환경을 넘어 추상적인 정보 처리 영역, 특히 프로그래밍 언어의 논리적 세계로 확장될 수 있음을 보여주는 중요한 진전입니다.

CWM은 320억 개의 매개변수를 가진 오픈 가중치 모델(open-weight model)로, 131k 토큰(token)의 컨텍스트 윈도우(context window)를 가지고 있습니다. 아키텍처(architecture)적으로는 여전히 슬라이딩 윈도우 어텐션(sliding-window attention)을 가진 밀집 디코더 전용 트랜스포머(dense decoder-only Transformer) 형태를 유지합니다. 또한 다른 LLM과 마찬가지로 사전 훈련(pre-training), 중간 훈련(mid-training), 지도 미세 조정(supervised fine-tuning, SFT), 강화 학습(reinforcement learning) 단계를 거칩니다. 하지만 핵심적인 차이점은 중간 훈련 데이터(mid-training data)에 월드 모델링 구성 요소(world-modeling component)가 도입된다는 점입니다. 이 단계에서 모델은 단순히 코드의 통계적 패턴을 학습하는 것을 넘어, 코드가 실행될 때 발생하는 내부 상태 변화와 그 결과를 예측하는 능력을 획득하게 됩니다.

---

**4.3 코드 월드 모델 대 일반 코드 LLM (Code World Models Vs Regular LLMs for Code)**

그렇다면 코드 월드 모델(CWM)은 Qwen3-Coder와 같은 일반 코드 LLM과 어떻게 다를까요? 큐웬3-코더(Qwen3-Coder)와 같은 보편적인 모델들은 오로지 다음 토큰 예측(next-token prediction)에 중점을 두고 훈련됩니다. 이들은 문법적 구조와 논리적 규칙의 양상을 학습하여 개연성 있는 코드 조각을 완성하며, 이는 프로그래밍 언어에 대한 정적인 텍스트 기반의 지식을 제공할 뿐입니다. 즉, 코드가 "어떻게 생겼는지"를 잘 알지만, 코드가 "무엇을 하는지"에 대한 깊이 있는 이해는 부족합니다.

대조적으로 CWM은 코드가 실행될 때 어떤 일이 일어나는지 시뮬레이션(simulate)하는 것을 학습합니다. 아래 그림과 같이 코드 한 줄을 수정하는 것과 같은 동작을 수행한 후, 변수(variable)의 값과 같은 결과적인 프로그램 상태(program state)를 예측하도록 훈련됩니다. 그림 19: Code World Model (CWM)에서의 코드 실행 추적(code execution tracing) 예시. 모델은 각 코드 라인이 실행됨에 따라 변수 상태(variable states)가 단계별로 어떻게 진화하는지 예측합니다. 여기서 모델은 코드의 동작을 효과적으로 시뮬레이션합니다. https://www.arxiv.org/abs/2510.02387에서 가져온 주석이 달린 그림.

추론(inference) 시 CWM은 GPT 스타일 모델과 마찬가지로 한 번에 하나의 토큰(token)을 생성하는 자기회귀 트랜스포머(autoregressive transformer)입니다. 주요 차이점은 이러한 토큰(tokens)이 일반 텍스트(plain text) 대신 구조화된 실행 추적(structured execution traces)을 인코딩(encode)할 수 있다는 것입니다. 이러한 실행 추적은 코드의 동작과 그로 인한 시스템 상태 변화를 명시적으로 나타냅니다. 그래서 저는 이것을 순수한 "월드 모델(world model)"이라고 부르기보다는 "월드 모델 증강 LLM(world model-augmented LLM)"이라고 표현하는 것이 더 정확하다고 생각합니다.

첫 시도치고는 놀랍도록 잘 작동하며, 대략 같은 크기에서 gpt-oss-20b (중간 추론 노력)와 동등한 성능을 보입니다. 더욱 인상적인 것은 테스트 시간 스케일링(test-time-scaling)을 사용하면, CWM이 4배 더 작으면서도 gpt-oss-120b (높은 추론 노력)보다 약간 더 나은 성능을 보인다는 점입니다. 그들의 테스트 시간 스케일링(test-time scaling)은 생성된 단위 테스트(unit tests)와 함께 best@k 절차(procedure)를 사용한다는 점에 유의하십시오 (고급 다수결 투표 방식(majority voting scheme)을 생각해보십시오). CWM과 gpt-oss가 다른 테스트 시간 스케일링 전략(test-time-scaling strategies) (best@k 대 추론 노력당 더 많은 토큰)을 사용하므로, CWM과 gpt-oss 간의 토큰/초 또는 해결 시간 비교를 보는 것은 흥미로웠을 것입니다. 그림 20: 코딩 벤치마크(coding benchmark, SWE-bench)에서 코드 월드 모델(code world model, CWM)과 다른 인기 있는 LLM의 성능 비교. https://www.arxiv.org/abs/2510.02387에서 가져온 주석이 달린 그림.

---

**5. 작은 재귀 트랜스포머(Small Recursive Transformers)**

여러분은 이전의 모든 접근 방식이 여전히 트랜스포머 아키텍처(transformer architecture)를 기반으로 한다는 것을 알아차렸을 것입니다. 이 마지막 섹션의 주제도 마찬가지이지만, 앞서 논의한 모델들과는 대조적으로, 이들은 추론(reasoning)을 위해 설계된 작고 전문화된 트랜스포머(transformers)입니다. 네, 추론에 특화된 구조들이 반드시 거대한 규모를 가질 필요는 없습니다. 사실, 계층적 추론 모델(Hierarchical Reasoning Model, HRM)과 함께 작은 재귀 트랜스포머(recursive transformers)에 대한 새로운 접근 방식이 최근 연구 커뮤니티에서 많은 주목을 받았습니다. 그림 21: LLM 환경 개요; 이 섹션은 작은 재귀 트랜스포머(small recursive transformers)를 다룹니다.

더 구체적으로, HRM 개발자들은 매우 작은 트랜스포머 모델(transformer models, 단 4개의 블록만 있는)조차도 답변을 단계별로 정제하도록 훈련될 때 (특정 문제에 대해) 인상적인 추론 능력(reasoning capabilities)을 개발할 수 있음을 보여주었습니다. 이는 ARC 챌린지에서 최고 순위를 차지했습니다. 그림 22: arcprize.org/arc-agi/1에서 가져온 ARC-AGI 1 작업 예시(상단)와 arcprize.org/blog/hrm-analysis에서 가져온 리더보드에 순위가 매겨진 계층적 추론 모델(Hierarchical Reasoning Model, HRM)(하단).

HRM과 같은 재귀 모델(recursive models)의 아이디어는 한 번의 순방향 전달(forward pass)로 답변을 생성하는 대신, 모델이 재귀적인 방식(recursive fashion)으로 자체 출력을 반복적으로 정제한다는 것입니다. (이 과정의 일부로, 각 반복은 잠재 표현(latent representation)을 정제하며, 저자들은 이를 모델의 "사고" 또는 "추론" 과정으로 봅니다.) 첫 번째 주요 예시는 여름 초의 HRM이었고, 이어서 Mixture-of-Recursions (MoR) 논문이 나왔습니다. 그리고 가장 최근에는 "Less is More: Recursive Reasoning with Tiny Networks" (2025년 10월)가 Tiny Recursive Model (TRM, 아래 그림에 설명됨)을 제안하는데, 이는 ARC 벤치마크(benchmark)에서 훨씬 더 나은 성능을 보이는 더 간단하고 훨씬 작은 모델입니다 (700만 개의 매개변수, HRM보다 약 4배 작음). 그림 23: Tiny Recursive Model (TRM). https://arxiv.org/abs/2510.04871에서 가져온 주석이 달린 그림. 이 섹션의 나머지 부분에서는 TRM을 좀 더 자세히 살펴보겠습니다.

---

**5.1 여기서 재귀란 무엇을 의미하는가?(What Does Recursion Mean Here?)**

TRM은 두 가지 순환적인 갱신 과정을 통해 해답을 정교화합니다: 먼저 현재의 질문과 제시된 해답으로부터 내재된 추론 상태를 도출하고, 이어서 그 내재된 상태를 바탕으로 해답을 수정하는 방식입니다. 훈련은 배치(batch)당 최대 16번의 정제 단계(refinement steps) 동안 실행됩니다. 각 단계는 답변을 반복적으로 정제하기 위해 여러 번의 no-grad 루프(loops)를 수행합니다. 이어서 전체 추론 시퀀스(reasoning sequence)를 통해 역전파(backpropagates)하여 모델 가중치(model weights)를 업데이트하는 경사 루프(gradient loop)가 진행됩니다.

TRM이 텍스트를 처리하는 언어 모델(language model)이 아니라는 점에 유의하는 것이 중요합니다. 그러나 (a) 트랜스포머 기반 아키텍처(transformer-based architecture)를 사용하고, (b) 추론(reasoning)이 이제 LLM 연구의 핵심 초점이며 이 모델은 추론에 대한 분명히 다른 접근 방식을 나타내고, (c) 많은 독자들이 HRM을 다루어 달라고 요청했기 때문에 (그리고 TRM은 HRM의 더 발전된 후속 모델이므로) 여기에 포함하기로 결정했습니다. TRM은 미래에 텍스트 기반 질문-답변 작업(textual question-answer tasks)으로 확장될 수 있지만, 현재 TRM은 그리드 기반 입력(grid-based inputs) 및 출력(outputs)에서 작동합니다. 다시 말해, "질문"과 "답변" 모두 이산 토큰(discrete tokens)의 그리드(예: 9×9 스도쿠 또는 30×30 ARC/미로 퍼즐)이며, 텍스트 시퀀스(text sequences)가 아닙니다. 이는 모델이 추상적인 패턴 인식과 논리적 추론에 집중하도록 설계되었음을 의미하며, 자연어 처리의 복잡성보다는 문제 해결의 본질에 더 초점을 맞춥니다.

---

**5.2 TRM은 HRM과 어떻게 다른가?(How Does TRM Differ From HRM?)**

HRM은 재귀적 계층을 통해 상호작용하는 두 개의 소형 트랜스포머 모듈(각각 4개 블록)로 이루어진 반면, TRM은 단 하나의 2계층 트랜스포머만을 활용합니다. (이전 TRM 그림에는 트랜스포머 블록(transformer block) 옆에 4×가 표시되어 있지만, 이는 HRM과 비교하기 쉽게 하기 위한 것일 가능성이 높다는 점에 유의하십시오.) 이러한 구조적 간소화는 TRM의 효율성을 높이는 주요 요인입니다.

훈련 과정에서도 중요한 차이가 있습니다. TRM은 모든 재귀 단계(recursive steps)를 통해 역전파(backpropagates)하는 반면, HRM은 마지막 몇 단계만 역전파합니다. TRM의 이러한 전체 경로 역전파는 모델이 각 정제 단계에서 더 많은 피드백을 받아 학습의 깊이를 더할 수 있도록 합니다. 또한, HRM은 반복을 언제 멈출지 결정하는 명시적인 정지 메커니즘(halting mechanism)을 포함하는 반면, TRM은 이 메커니즘을 반복을 언제 멈출지 학습하는 간단한 이진 교차 엔트로피 손실(binary cross-entropy loss)로 대체합니다.

성능 면에서 TRM은 아래 그림에서 보듯이 HRM에 비해 매우 우수한 성능을 보입니다. 그림 24: 계층적 추론 모델(Hierarchical Reasoning Model, HRM)과 Tiny Recursive Model (TRM)의 성능 비교. 이 논문은 놀라울 정도로 많은 절제 연구(ablation studies)를 포함했으며, 이는 몇 가지 흥미로운 추가 통찰력을 제공했습니다. 제게 특히 눈에 띄었던 두 가지는 다음과 같습니다:

*   **레이어가 적을수록 더 나은 일반화(generalization)로 이어집니다.**
    *   4개 레이어에서 2개 레이어로 줄이면 스도쿠 정확도(Sudoku accuracy)가 79.5%에서 87.4%로 향상되었습니다. 이는 모델의 복잡성을 줄임으로써 과적합(overfitting)을 방지하고 더 넓은 범위의 문제에 대한 적용 능력을 높일 수 있음을 시사합니다.
*   **어텐션(Attention)은 필요하지 않습니다.**
    *   자기 어텐션(self-attention)을 순수 MLP 레이어(MLP layer)로 대체하는 것도 정확도(accuracy)를 향상시켰습니다 (74.7%에서 87.4%로).
    *   하지만 이러한 결과는 컨텍스트(context)가 작고 고정된 길이기 때문에 여기에서만 가능합니다. 즉, 입력 시퀀스가 짧고 구조화된 문제에서는 어텐션 메커니즘의 복잡한 상호작용 없이도 충분한 정보를 처리할 수 있다는 것을 의미합니다. 이는 특정 도메인에 특화된 소형 모델 설계에 중요한 시사점을 제공합니다.

---

**5.3 더 큰 그림(The Bigger Picture)**

HRM과 TRM이 이러한 벤치마크(benchmarks)에서 정말 좋은 추론 성능(reasoning performance)을 달성하지만, 이들을 대규모 LLM과 비교하는 것은 공정하지 않습니다. 계층적 추론 모델(HRM)과 초소형 재귀 모델(TRM)은 ARC 챌린지, 스도쿠, 미로 탐색과 같은 특정 과제에 특화된 모델인 반면, 대규모 언어 모델(LLM)은 범용성을 지향합니다. 물론 HRM과 TRM도 다른 작업에 채택될 수 있지만, 각 작업에 대해 특별히 훈련되어야 합니다. 따라서 그런 의미에서 HRM과 TRM은 효율적인 포켓 계산기로 생각할 수 있고, LLM은 다른 많은 일도 할 수 있는 컴퓨터와 같다고 볼 수 있습니다.

그럼에도 불구하고, 이러한 재귀 아키텍처(recursive architectures)는 작고 효율적인 모델이 반복적인 자기 정제(iterative self-refinement)를 통해 어떻게 "추론"할 수 있는지를 보여주는 흥미로운 개념 증명(proof-of-concepts)입니다. 아마도 미래에는 이러한 모델이 더 큰 도구 사용 LLM 시스템(tool-using LLM systems) 내에 내장된 추론 또는 계획 모듈(reasoning or planning modules) 역할을 할 수 있을 것입니다. 현재로서는 LLM이 광범위한 작업에 이상적이지만, TRM과 같은 도메인별 재귀 모델(domain-specific recursive models)은 대상 도메인이 잘 이해되면 특정 문제를 더 효율적으로 해결하기 위해 개발될 수 있습니다. 스도쿠, 미로 찾기, ARC 개념 증명 벤치마크(proof-of-concept benchmarks) 외에도, 이러한 모델이 사용될 수 있는 물리학 및 생물학 도메인(physics and biology domain)에는 복잡한 시뮬레이션이나 패턴 분석과 같은 많은 사용 사례가 있을 수 있습니다.

흥미로운 사실로, 저자는 이 모델을 훈련하는 데 4개의 H100 GPU로 약 2일 동안 500달러 미만이 들었다고 공유했습니다. 이는 대규모 데이터 센터(data center)에 대한 접근 없이도 흥미로운 연구와 개발을 수행할 수 있다는 점에서 매우 고무적인 소식입니다. 이러한 저렴한 비용은 더 많은 연구자와 개발자가 혁신적인 아이디어를 탐구하고 실험할 수 있는 기회를 제공할 것입니다.

---

**6. 결론(Conclusion)**

원래는 개요 그림에 포함된 모든 모델 범주를 다룰 계획이었지만, 글이 예상보다 길어져서 xLSTM, Liquid Foundation Models, 트랜스포머-RNN 하이브리드(Transformer-RNN hybrids), 상태 공간 모델(State Space Models)은 다음 기회로 미루어야 할 것 같습니다 (비록 Gated DeltaNet이 이미 상태 공간 모델과 순환 설계(recurrent designs)의 맛을 보여주었지만요).

이 글의 결론으로, 저는 앞서 언급했던 말을 다시 한번 강조하고 싶습니다. 즉, 표준적인 자동 회귀 트랜스포머 기반 대규모 언어 모델(LLM)은 그 효용성이 입증되었으며, 오랜 기간 동안 그 가치를 증명해 왔다는 점입니다. 또한, 효율성이 가장 중요한 고려 사항이 아니라면, 현재로서는 이들이 우리가 가진 최고의 기술입니다.

**전통적인 디코더 스타일(Decoder-Style), 자기회귀 트랜스포머(Autoregressive Transformers)**
*   **장점**:
    *   오랜 기간 검증되어 신뢰할 수 있고 성숙한 기술
    *   동작 원리와 특성이 잘 이해됨
    *   스케일링 법칙(Scaling laws)이 명확하여 성능 예측 및 개선 용이
    *   다양한 분야에서 최고 수준의 성능(SOTA) 달성
*   **단점**:
    *   대규모 모델 훈련에 막대한 비용 소요
    *   추론 시 높은 비용 발생 (특별한 최적화 기법 없이는)

오늘 새로운 LLM 기반 프로젝트를 시작한다면, 자기회귀 트랜스포머 기반 LLM(autoregressive transformer-based LLMs)이 저의 첫 번째 선택이 될 것입니다. 그러나 저는 다가오는 어텐션 하이브리드(attention hybrids)가 매우 유망하다고 생각하며, 특히 효율성이 주요 관심사인 더 긴 컨텍스트(contexts)로 작업할 때 흥미로운 대안이 될 것입니다.

**선형 어텐션 하이브리드(Linear Attention Hybrids)**
*   **장점**:
    *   기존 디코더 스타일 트랜스포머와 유사한 구조 유지
    *   긴 컨텍스트 처리 시 FLOPs(부동소수점 연산) 및 KV 메모리 사용량 절감
    *   확장 가능한 컨텍스트 길이 지원
*   **단점**:
    *   아키텍처의 복잡성 증가
    *   효율성을 위해 약간의 모델 정확도 희생 가능성

더욱 급진적인 측면에서는 텍스트 확산 모델(text diffusion models)이 흥미로운 발전입니다. 저는 몇 가지 빠른 데모만 시도해 보았기 때문에, 일상적인 사용에서 얼마나 잘 작동할지에 대해서는 여전히 다소 회의적입니다. 바라건대, 곧 Google의 Gemini Diffusion을 통한 대규모 프로덕션 배포를 보게 될 것이며, 이를 일상 및 코딩 작업에서 직접 테스트하여 실제 사용자 경험이 어떠한지 알아낼 수 있기를 바랍니다.

**텍스트 확산 모델(Text Diffusion Models)**
*   **장점**:
    *   반복적인 노이즈 제거 방식은 텍스트 생성에 대한 신선한 아이디어 제시
    *   탁월한 병렬 처리 능력 (다음 토큰 의존성 없음)
    *   잠재적으로 낮은 추론 지연 시간
*   **단점**:
    *   응답을 실시간으로 스트리밍하기 어려움
    *   CoT(Chain-of-Thought)와 같은 고급 추론 기법의 이점 활용이 불분명
    *   도구 호출(Tool calling) 구현의 복잡성
    *   모델은 견고하지만, 아직 최고 수준의 성능(SOTA)에는 미치지 못함

텍스트 확산 모델(text diffusion models)의 주요 강점이 효율성 개선인 반면, 코드 월드 모델(code world models)은 스펙트럼의 다른 끝에 위치하며, 모델링 성능(modeling performance) 향상을 목표로 합니다. 이 글을 쓰는 시점에서, 표준 LLM을 기반으로 하는 코딩 모델(coding models)은 주로 추론 기술(reasoning techniques)을 통해 개선되지만, 더 까다로운 문제에 시도해 보았다면, 이들이 (다소) 여전히 부족하고 많은 복잡한 코딩 문제를 잘 해결하지 못한다는 것을 알아차렸을 것입니다. 저는 코드 월드 모델(code world models)이 특히 흥미롭다고 생각하며, 더 유능한 코딩 시스템(coding systems)을 개발하는 데 중요한 다음 단계가 될 수 있다고 믿습니다.

**코드 월드 모델(Code World Model)**
*   **장점**:
    *   코드에 대한 심층적인 이해를 위한 매우 유망한 접근 방식
    *   검증 가능한 중간 실행 상태(intermediate execution states) 제공
    *   실제 코드 동작 시뮬레이션 능력
*   **단점**:
    *   실행 가능한 코드 추적(executable code traces) 포함으로 훈련 복잡성 증가
    *   코드 실행 시뮬레이션으로 인한 추론 지연 시간 추가

마지막으로, 계층적 및 작은 추론 모델(tiny reasoning models)과 같은 작은 재귀 트랜스포머(recursive transformers)를 다루었습니다. 이들은 매우 주목할 만한 개념 증명(proof-of-concept) 사례를 제시하지만, 현 시점에서는 주로 퍼즐 해결에 특화되어 있으며, 범용적인 텍스트 또는 코딩 모델과는 거리가 있습니다. 따라서 이 글에서 다룬 다른 비표준 LLM 대안들과 같은 범주에 속하지 않습니다. 그럼에도 불구하고, 이들은 매우 흥미로운 개념 증명이며, 연구자들이 이들을 연구하고 있다는 사실에 기쁩니다.

현재 GPT-5, DeepSeek R1, Kimi K2 등과 같은 LLM은 자유 형식 텍스트, 코드, 수학 문제 등을 위한 특수 목적 모델(special purpose models)로 개발되고 있습니다. 이들은 일반 지식 질문부터 수학 및 코드에 이르기까지 다양한 작업에 사용하는 무차별 대입(brute-force) 및 만능 접근 방식(jack-of-all-trades approach)처럼 느껴집니다. 하지만 동일한 작업을 반복적으로 수행할 때, 이러한 무차별 대입 접근 방식은 비효율적이 되고 전문화(specialization) 측면에서도 이상적이지 않을 수 있습니다. 여기서 작은 재귀 트랜스포머(tiny recursive transformers)가 흥미로워집니다: 이들은 반복적이거나 구조화된 추론 작업(reasoning tasks)에 효율적이고 목적에 맞게 구축된 경량의 작업별 모델(task-specific models) 역할을 할 수 있습니다. 또한, 저는 이들을 다른 도구 호출 LLM(tool-calling LLMs)을 위한 잠재적인 "도구"로 볼 수 있습니다. 예를 들어, LLM이 수학 문제를 해결하기 위해 Python 또는 계산기 API를 사용할 때, 특수 작은 추론 모델(tiny reasoning models)이 다른 유형의 퍼즐 또는 추론과 유사한 문제에 대한 이러한 틈새를 채울 수 있습니다.

**작은 재귀 트랜스포머(Small Recursive Transformers)**
*   **장점**:
    *   매우 작은 아키텍처로 높은 효율성
    *   퍼즐 해결에 대한 뛰어난 일반화 능력
    *   낮은 훈련 비용
*   **단점**:
    *   매우 특수 목적의 모델
    *   (현재까지는) 주로 그리드 기반 퍼즐에 한정됨

이 글이 다소 길었지만, 주류 대규모 언어 모델(LLM)의 주요 관심사에서 벗어나 있지만, 그만큼 매력적인 여러 접근 방식들을 살펴보셨기를 바랍니다. 그리고 다소 전통적인 LLM 출시 소식에 지루함을 느끼셨다면, 현재 AI 분야에서 많은 흥미로운 작업들이 진행되고 있으므로 이 글이 여러분의 인공지능에 대한 흥미를 다시 불러일으키는 데 도움이 되었기를 진심으로 바랍니다!

이 잡지는 저의 개인적인 열정 프로젝트이며, 독자 여러분의 지원이 이를 지속하는 데 큰 힘이 됩니다. 저의 작업을 지원하고 싶으시다면, 저의 "Build a Large Language Model (From Scratch)" 책 또는 그 후속작인 "Build a Reasoning Model (From Scratch)"을 고려해 주십시오. (저는 여러분이 이 책들에서 많은 것을 얻을 것이라고 확신합니다. 이 책들은 다른 곳에서는 찾을 수 없는 깊이로 LLM이 어떻게 작동하는지 설명합니다.) 읽어주셔서 감사하며, 독립적인 연구를 지원해 주셔서 감사합니다! "Build a Large Language Model (From Scratch)"은 현재 Amazon에서 구매 가능합니다. "Build a Reasoning Model (From Scratch)"은 Manning에서 얼리 액세스(Early Access) 중입니다. 책을 읽으셨고 잠시 시간을 내어 짧은 리뷰를 남겨주시면 정말 감사하겠습니다. 이는 저희 저자들에게 큰 도움이 됩니다! 여러분의 지원은 큰 의미가 있습니다! 감사합니다!