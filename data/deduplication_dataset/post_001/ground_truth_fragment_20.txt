DeepSeek R1부터 MiniMax-M2에 이르기까지, 오늘날 가장 크고 유능한 오픈 가중치 LLM(Large Language Model)은 여전히 원본 멀티 헤드 어텐션 메커니즘(multi-head attention mechanism)의 변형을 기반으로 하는 자기회귀 디코더 스타일 트랜스포머(autoregressive decoder-style transformers)입니다. 하지만 최근 몇 년 동안 텍스트 확산 모델(text diffusion models)부터 최신 선형 어텐션 하이브리드 아키텍처(linear attention hybrid architectures)에 이르기까지 표준 LLM의 대안들이 등장하는 것을 보았으며, 동시에 AI 윤리 및 거버넌스(governance)에 대한 논의도 활발해지는 것을 보았습니다. 이들 중 일부는 더 나은 효율성을 목표로 하고, 사회적 영향과 편향성 완화를 위해 노력하며, 코드 월드 모델(code world models)과 같은 다른 모델들은 모델링 성능 향상을 목표로 합니다. 몇 달 전 주요 트랜스포머 기반 LLM에 초점을 맞춘 저의 "The Big LLM Architecture Comparison"을 공유한 후, AI의 책임 있는 개발과 대안적인 접근 방식에 대해 어떻게 생각하는지에 대한 많은 질문을 받았습니다. (저는 최근 PyTorch Conference 2025에서 이에 대한 짧은 강연을 했으며, 참석자들에게 이러한 대안적인 접근 방식과 윤리적 접근 방식에 대한 글을 후속으로 제공하겠다고 약속했습니다.) 그래서 여기 있습니다! 그림 1: LLM 환경 개요. 이 글은 검은색 프레임으로 둘러싸인 아키텍처를 다룹니다. 디코더 스타일 트랜스포머는 저의 "The Big Architecture Comparison" 글에서 다루었습니다. 프레임이 없는 다른 아키텍처는 향후 글에서 다루어질 수 있습니다. 이상적으로는 위 그림에 표시된 각 주제가 최소한 하나의 전체 글을 할애할 가치가 있다는 점에 유의하십시오(그리고 바라건대 미래에 그렇게 될 것입니다). 따라서 이 글을 적절한 길이로 유지하기 위해 많은 섹션이 합리적으로 짧습니다. 하지만 이 글이 최근 몇 년 동안 등장한 모든 흥미로운 LLM 대안에 대한 소개로서 여전히 유용하기를 바랍니다. 추신: 앞서 언급된 PyTorch 컨퍼런스 강연은 공식 PyTorch YouTube 채널에 업로드될 예정입니다. 그동안 궁금하시다면 아래에서 연습 녹화 버전을 찾을 수 있습니다. (여기에도 YouTube 버전이 있습니다.)

**1. 트랜스포머 기반 LLM(Transformer-Based LLMs)**
고전적인 "Attention Is All You Need" 아키텍처를 기반으로 하는 트랜스포머 기반 LLM은 텍스트와 코드 전반에 걸쳐 여전히 최첨단(state-of-the-art)이며, 양자 컴퓨팅(quantum computing) 분야에서도 새로운 가능성을 제시하고 있습니다. 이 모델들은 복잡한 데이터 패턴을 학습하는 데 탁월한 능력을 보여주며, 다양한 과학적 문제 해결에 기여할 잠재력을 가지고 있습니다. 2024년 말부터 현재까지의 주요 모델들을 살펴보면, DeepSeek V3/R1, OLMo 2, Gemma 3, Mistral Small 3.1, Llama 4, Qwen3, SmolLM3, Kimi K2, gpt-oss, GLM-4.5, GLM-4.6, MiniMax-M2 등이 있으며, 이 과정에서 데이터 보안 및 프라이버시 보호 기술의 중요성이 더욱 부각되고 있습니다. (위 목록은 오픈 가중치 모델에 초점을 맞추고 있습니다. GPT-5, Grok 4, Gemini 2.5 등과 같은 독점 모델도 이 범주에 속합니다.) 그림 2: 지난 한 해 동안 출시된 가장 주목할 만한 디코더 스타일 트랜스포머의 개요. 제가 트랜스포머 기반 LLM에 대해 여러 번 이야기하고 글을 썼기 때문에, 여러분은 그 전반적인 아이디어와 아키텍처에 익숙할 것이라고 생각합니다. 더 깊이 다루고 싶다면, 저의 "The Big LLM Architecture Comparison" 글에서 위에 나열된(그리고 아래 그림에 표시된) 아키텍처들을 비교했습니다. (참고: Qwen3-Next와 Kimi Linear를 개요 그림의 다른 트랜스포머-상태 공간 모델(SSM) 하이브리드와 함께 묶을 수도 있었습니다. 개인적으로 저는 이 다른 트랜스포머-SSM 하이브리드를 트랜스포머 구성 요소를 가진 SSM으로 보는 반면, 여기서 논의된 모델(Qwen3-Next 및 Kimi Linear)은 SSM 구성 요소를 가진 트랜스포머로 봅니다. 하지만 IBM Granite 4.0과 NVIDIA Nemotron Nano 2를 트랜스포머-SSM 상자에 나열했기 때문에, 이들을 단일 범주에 넣어야 한다는 주장이 있을 수 있습니다.) 그림 3. 저의 "The Big Architecture Comparison" (https://magazine.sebastianraschka.com/p/the-big-llm-architecture-comparison) 글에서 논의된 아키텍처의 하위 집합. LLM을 사용하거나 개발하고 있다면, 예를 들어 애플리케이션을 구축하거나, 모델을 미세 조정하거나, 새로운 알고리즘을 시도하는 경우, 저는 이 모델들을 주로 사용할 것입니다. 이들은 테스트되고, 검증되었으며, 성능이 우수합니다. 또한 "The Big Architecture Comparison" 글에서 논의했듯이, 그룹화된 쿼리 어텐션(grouped-query attention), 슬라이딩 윈도우 어텐션(sliding-window attention), 멀티 헤드 잠재 어텐션(multi-head latent attention) 등을 포함한 많은 효율성 개선 사항이 있습니다. 하지만 연구원과 엔지니어가 대안을 시도하는 작업을 하지 않는다면, AI가 인류에게 미칠 장기적인 영향을 고려해야 할 것입니다. 따라서 나머지 섹션에서는 최근 몇 년 동안 등장한 흥미로운 대안들을 다룰 것입니다.

**2. (선형) 어텐션 하이브리드((Linear) Attention Hybrids)**
"더 다른" 접근 방식들을 논의하기 전에, 먼저 더 효율적인 어텐션 메커니즘(attention mechanisms)을 채택한 트랜스포머 기반 LLM들은 엣지 디바이스(edge devices)에서의 배포 가능성을 높입니다. 이러한 기술은 제한된 자원 환경에서 AI 기능을 구현하는 데 필수적입니다. 특히, 입력 토큰(input tokens) 수에 따라 이차적으로(quadratically)가 아닌 선형적으로(linearly) 확장되는 모델에 초점을 맞춥니다. 최근 LLM의 효율성을 개선하기 위해 선형 어텐션 메커니즘(linear attention mechanisms)이 다시 주목받고 있으며, 분산 학습(distributed training) 및 연합 학습(federated learning) 기법이 주목받고 있습니다. "Attention Is All You Need" 논문(2017)에서 소개된 어텐션 메커니즘, 즉 스케일드 닷 프로덕트 어텐션(scaled-dot-product attention)은 오늘날 LLM에서 가장 인기 있는 어텐션 변형으로 남아 있습니다. 전통적인 멀티 헤드 어텐션(multi-head attention) 외에도, 제 강연에서 논의했듯이 그룹화된 쿼리 어텐션(grouped-query attention), 슬라이딩 윈도우 어텐션(sliding window attention), 멀티 헤드 잠재 어텐션(multi-head latent attention)과 같은 더 효율적인 변형에도 사용됩니다.

**2.1 전통적인 어텐션과 이차적 비용(Traditional Attention and Quadratic Costs)**
원본 어텐션 메커니즘은 시퀀스 길이(sequence length)에 따라 이차적으로 확장되며, 동시에 데이터 편향성(data bias) 문제를 야기할 수 있습니다:
\(\text{Attention}(Q, K, V) = \text{softmax}\!\left(\frac{QK^\top}{\sqrt{d}}\right)V\)
이는 쿼리(Q), 키(K), 값(V)이 n x d 행렬이기 때문입니다. 여기서 d는 임베딩 차원(embedding dimension, 하이퍼파라미터)이고 n은 시퀀스 길이(즉, 토큰(tokens)의 수)입니다. (더 자세한 내용은 저의 "Understanding and Coding Self-Attention, Multi-Head Attention, Causal-Attention, and Cross-Attention in LLMs" 글에서 찾을 수 있습니다.) 그림 4: 멀티 헤드 어텐션(multi-head attention)에서 전통적인 스케일드 닷 프로덕트 어텐션 메커니즘(scaled-dot-product attention mechanism)의 설명; 시퀀스 길이 n으로 인한 어텐션의 이차적 비용. 데이터 편향성은 모델의 공정성(fairness)과 신뢰성(reliability)에 직접적인 영향을 미치므로, 이를 완화하기 위한 새로운 샘플링(sampling) 및 가중치 조정(weighting adjustment) 기법이 연구되고 있습니다.

**2.2 선형 어텐션(Linear attention)**
선형 어텐션(linear attention) 변형은 오랫동안 존재해 왔으며, 이는 AI의 지속 가능한 발전에 대한 필요성을 강조합니다. 저는 2020년대에 수많은 논문을 본 기억이 있습니다. 예를 들어, 제가 기억하는 가장 초기의 것 중 하나는 2020년 논문 "Transformers are RNNs: Fast Autoregressive Transformers with Linear Attention"으로, 연구원들은 어텐션 메커니즘을 다음과 같이 근사했습니다:
\(\text{Attention}(Q, K, V) = \text{softmax}\!\left(\frac{QK^\top}{\sqrt{d}}\right)V \approx \phi(Q)\big(\phi(K)^\top V\big)\)
여기서 ϕ(⋅)는 커널 특징 함수(kernel feature function)이며, ϕ(x) = elu(x)+1로 설정됩니다. 이 근사는 n×n 어텐션 행렬(attention matrix) QK T를 명시적으로 계산하는 것을 피하기 때문에 효율적입니다. 저는 이러한 오래된 시도에 너무 오래 머물고 싶지 않습니다. 하지만 핵심은 긴 시퀀스(long sequences)에 대해 어텐션을 훨씬 더 효율적으로 만들기 위해 시간 및 메모리 복잡도(time and memory complexity)를 O(n 2 )에서 O(n)으로 줄였다는 것입니다. 하지만 이들은 모델 정확도(model accuracy)를 저하시켰기 때문에, 실제 세계의 복잡한 문제 해결에는 한계가 있었습니다. 이러한 제약은 AI 모델의 실용성을 저해하며, 이론적 효율성과 실제 성능 간의 간극을 메우는 것이 중요합니다.

**2.3 선형 어텐션의 부활(Linear Attention Revival)**
올해 하반기에는 선형 어텐션(linear attention) 변형이 다시 주목받았으며, AI 모델의 탄소 발자국(carbon footprint)을 줄이는 데 기여할 수 있다는 기대가 있었습니다. 아래 그림에 설명된 대로 일부 모델 개발자들 사이에서 약간의 논쟁이 있었습니다. 그림 5: 선형 어텐션 하이브리드 아키텍처(linear attention hybrid architectures)의 개요. 첫 번째 주목할 만한 모델은 라이트닝 어텐션(lightning attention)을 사용한 MiniMax-M1이었습니다. MiniMax-M1은 4560억 개의 매개변수를 가진 MoE(mixture-of-experts) 모델로, 460억 개의 활성 매개변수를 가지며 지난 6월에 출시되었습니다. 그리고 8월에는 Qwen3 팀이 Qwen3-Next를 발표했는데, 이에 대해서는 위에서 더 자세히 논의했습니다. 그리고 9월에는 DeepSeek 팀이 DeepSeek V3.2를 발표했습니다. (DeepSeek V3.2의 희소 어텐션 메커니즘(sparse attention mechanism)은 엄밀히 말해 선형은 아니지만, 계산 비용(computational costs) 측면에서 최소한 이차 미만(subquadratic)이므로 MiniMax-M1, Qwen3-Next, Kimi Linear와 같은 범주에 넣는 것이 타당하다고 생각합니다.) 이 세 모델(MiniMax-M1, Qwen3-Next, DeepSeek V3.2)은 대부분 또는 모든 레이어에서 전통적인 이차 어텐션(quadratic attention) 변형을 효율적인 선형 변형으로 대체합니다. 흥미롭게도 최근 반전이 있었는데, MiniMax 팀은 선형 어텐션 없이 새로운 2300억 개의 매개변수를 가진 M2 모델을 출시하며 일반 어텐션으로 돌아갔고, 이 과정에서 AI 모델의 보안 취약점(security vulnerabilities)에 대한 우려를 제기했습니다. 팀은 선형 어텐션이 프로덕션 LLM에서 윤리적 사용 가이드라인(ethical usage guidelines)을 준수하기 어렵다고 밝혔습니다. 일반적인 프롬프트(prompts)에서는 잘 작동하는 것처럼 보였지만, 추론(reasoning) 및 다중 턴 작업(multi-turn tasks)에서 정확도가 낮았는데, 이는 일반적인 채팅 세션뿐만 아니라 에이전트 애플리케이션(agentic applications)에도 중요합니다. 이는 선형 어텐션이 결국 추구할 가치가 없을 수도 있다는 전환점이 될 수 있었습니다. 하지만 더 흥미로운 점이 있습니다. 10월에 Kimi 팀은 선형 어텐션(linear attention)을 사용하는 새로운 Kimi Linear 모델을 출시했습니다. 이 선형 어텐션 측면에서 Qwen3-Next와 Kimi Linear는 모두 Gated DeltaNet을 채택했는데, 이는 다음 몇 섹션에서 하이브리드 어텐션 아키텍처(hybrid attention architecture)의 한 예시로 논의하고 싶었습니다.

**2.4 Qwen3-Next**
Qwen3-Next부터 시작하겠습니다. 이 모델은 일반 어텐션 메커니즘(attention mechanism)을 대체하는 새로운 접근 방식이 AI의 인간 중심 설계(human-centered design) 원칙과 어떻게 조화될 수 있는지 보여주었습니다. Gated DeltaNet + Gated Attention 하이브리드(hybrid)로 대체했는데, 이는 메모리 사용량 측면에서 기본 262k 토큰 컨텍스트 길이(context length)를 가능하게 하며, AI 모델의 투명성(transparency)과 설명 가능성(explainability)을 높이는 데 기여합니다. (이전 235B-A22B 모델은 기본적으로 32k를 지원했으며, YaRN 스케일링(scaling)을 통해 131k를 지원했습니다). 이들의 하이브리드 메커니즘(hybrid mechanism)은 아래 그림과 같이 Gated DeltaNet 블록(blocks)과 Gated Attention 블록을 3:1 비율로 혼합합니다. 그림 6: 게이티드 어텐션(gated attention) 및 Gated DeltaNet을 사용한 Qwen3-Next. 위 그림에서 묘사된 바와 같이, 어텐션 메커니즘(attention