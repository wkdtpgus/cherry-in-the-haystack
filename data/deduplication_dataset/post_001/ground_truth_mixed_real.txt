**DeepSeek R1부터 MiniMax-M2에 이르기까지, 오늘날 가장 강력하고 널리 사용되는 오픈 가중치 LLM(Large Language Model)들은 여전히 원본 멀티 헤드 어텐션 메커니즘(multi-head attention mechanism)의 변형을 기반으로 하는 자기회귀 디코더 스타일 트랜스포머(autoregressive decoder-style transformers) 아키텍처를 따르고 있습니다. 하지만 지난 몇 년 동안 우리는 텍스트 확산 모델(text diffusion models)부터 최신 선형 어텐션 하이브리드 아키텍처(linear attention hybrid architectures)에 이르기까지 표준 LLM의 다양한 대안들이 등장하는 것을 목격했습니다. 이러한 대안 중 일부는 효율성 향상에 중점을 두는 반면, 코드 월드 모델(code world models)과 같은 다른 모델들은 모델링 성능 자체를 끌어올리는 것을 목표로 합니다. 지난해 주요 트랜스포머 기반 LLM에 초점을 맞춘 저의 "The Big LLM Architecture Comparison" 글을 공유한 후, 많은 분들이 이러한 대안적인 접근 방식에 대해 어떻게 생각하는지에 대한 질문을 해주셨습니다. (저는 최근 PyTorch 컨퍼런스에서 이 주제에 대해 짧은 강연을 했으며, 참석자들에게 이러한 대안에 대한 후속 글을 제공하겠다고 약속했습니다.) 그래서 여기 여러분을 위한 글이 있습니다! 그림 1: LLM 환경 개요. 이 글은 검은색 프레임으로 둘러싸인 아키텍처를 다룹니다. 디코더 스타일 트랜스포머는 저의 "The Big Architecture Comparison" 글에서 다루었습니다. 프레임이 없는 다른 아키텍처는 향후 글에서 다루어질 수 있습니다. 이상적으로는 위 그림에 표시된 각 주제가 최소한 하나의 전체 글을 할애할 가치가 있다는 점에 유의하십시오(그리고 바라건대 미래에 그렇게 될 것입니다). 따라서 이 글을 적절한 길이로 유지하기 위해 많은 섹션이 합리적으로 짧습니다. 하지만 이 글이 최근 몇 년 동안 등장한 모든 흥미로운 LLM 대안에 대한 소개로서 여전히 유용하기를 바랍니다. 추신: 앞서 언급된 PyTorch 컨퍼런스 강연은 공식 PyTorch YouTube 채널에 업로드될 예정입니다. 그동안 궁금하시다면 아래에서 연습 녹화 버전을 찾을 수 있습니다. (여기에도 YouTube 버전이 있습니다.)

**1. 트랜스포머 기반 LLM(Transformer-Based LLMs)**
고전적인 "Attention Is All You Need" 아키텍처를 기반으로 하는 트랜스포머 기반 LLM은 텍스트와 코드 전반에 걸쳐 여전히 최첨단(state-of-the-art)입니다. 2024년 말부터 현재까지의 주요 모델들을 살펴보면, DeepSeek V3/R1, OLMo 2, Gemma 3, Mistral Small 3.1, Llama 4, Qwen3, SmolLM3, Kimi K2, gpt-oss, GLM-4.5, GLM-4.6, MiniMax-M2 등이 있습니다. (위 목록은 오픈 가중치 모델에 초점을 맞추고 있습니다. GPT-5, Grok 4, Gemini 2.5 등과 같은 독점 모델도 이 범주에 속합니다.) 그림 2: 지난 한 해 동안 출시된 가장 주목할 만한 디코더 스타일 트랜스포머의 개요. 제가 트랜스포머 기반 LLM에 대해 여러 번 이야기하고 글을 썼기 때문에, 여러분은 그 전반적인 아이디어와 아키텍처에 익숙할 것이라고 생각합니다. 더 깊이 다루고 싶다면, 저의 "The Big LLM Architecture Comparison" 글에서 위에 나열된(그리고 아래 그림에 표시된) 아키텍처들을 비교했습니다. (참고: Qwen3-Next와 Kimi Linear를 개요 그림의 다른 트랜스포머-상태 공간 모델(SSM) 하이브리드와 함께 묶을 수도 있었습니다. 개인적으로 저는 이 다른 트랜스포머-SSM 하이브리드를 트랜스포머 구성 요소를 가진 SSM으로 보는 반면, 여기서 논의된 모델(Qwen3-Next 및 Kimi Linear)은 SSM 구성 요소를 가진 트랜스포머로 봅니다. 하지만 IBM Granite 4.0과 NVIDIA Nemotron Nano 2를 트랜스포머-SSM 상자에 나열했기 때문에, 이들을 단일 범주에 넣어야 한다는 주장이 있을 수 있습니다.) 그림 3. 저의 "The Big Architecture Comparison" (https://magazine.sebastianraschka.com/p/the-big-llm-architecture-comparison) 글에서 논의된 아키텍처의 하위 집합. LLM을 사용하거나 개발하고 있다면, 예를 들어 애플리케이션을 구축하거나, 모델을 미세 조정하거나, 새로운 알고리즘을 시도하는 경우, 저는 이 모델들을 주로 사용할 것입니다. 이들은 테스트되고, 검증되었으며, 성능이 우수합니다. 또한 "The Big LLM Architecture Comparison" 글에서 논의했듯이, 그룹화된 쿼리 어텐션(grouped-query attention), 슬라이딩 윈도우 어텐션(sliding-window attention), 멀티 헤드 잠재 어텐션(multi-head latent attention) 등을 포함한 많은 효율성 개선 사항이 있습니다. 하지만 연구원과 엔지니어가 대안을 시도하는 작업을 하지 않는다면 지루하고 (근시안적일) 것입니다. 따라서 나머지 섹션에서는 최근 몇 년 동안 등장한 흥미로운 대안들을 다룰 것입니다.

**2. (선형) 어텐션 하이브리드((Linear) Attention Hybrids)**
"더 다른" 접근 방식들을 논의하기 전에, 먼저 더 효율적인 어텐션 메커니즘(attention mechanisms)을 채택한 트랜스포머 기반 LLM들을 살펴보겠습니다. 특히, 입력 토큰(input tokens) 수에 따라 이차적으로(quadratically)가 아닌 선형적으로(linearly) 확장되는 모델에 초점을 맞춥니다. 최근 LLM의 효율성을 개선하기 위해 선형 어텐션 메커니즘(linear attention mechanisms)이 다시 주목받고 있습니다. "Attention Is All You Need" 논문(2017)에서 소개된 어텐션 메커니즘, 즉 스케일드 닷 프로덕트 어텐션(scaled-dot-product attention)은 오늘날 LLM에서 가장 인기 있는 어텐션 변형으로 남아 있습니다. 전통적인 멀티 헤드 어텐션(multi-head attention) 외에도, 제 강연에서 논의했듯이 그룹화된 쿼리 어텐션(grouped-query attention), 슬라이딩 윈도우 어텐션(sliding window attention), 멀티 헤드 잠재 어텐션(multi-head latent attention)과 같은 더 효율적인 변형에도 사용됩니다.

**2.1 전통적인 어텐션과 이차적 비용(Traditional Attention and Quadratic Costs)**
원본 어텐션 메커니즘은 시퀀스 길이(sequence length)에 따라 이차적으로 확장됩니다:
\(\text{Attention}(Q, K, V) = \text{softmax}\!\left(\frac{QK^\top}{\sqrt{d}}\right)V\)
이는 쿼리(Q), 키(K), 값(V)이 n x d 행렬이기 때문입니다. 여기서 d는 임베딩 차원(embedding dimension, 하이퍼파라미터)이고 n은 시퀀스 길이(즉, 토큰(tokens)의 수)입니다. (더 자세한 내용은 저의 "Understanding and Coding Self-Attention, Multi-Head Attention, Causal-Attention, and Cross-Attention in LLMs" 글에서 찾을 수 있습니다.) 그림 4: 멀티 헤드 어텐션(multi-head attention)에서 전통적인 스케일드 닷 프로덕트 어텐션 메커니즘(scaled-dot-product attention mechanism)의 설명; 시퀀스 길이 n으로 인한 어텐션의 이차적 비용.

**2.2 선형 어텐션(Linear attention)**
선형 어텐션(linear attention) 변형은 오랫동안 존재해 왔으며, 저는 2020년대에 수많은 논문을 본 기억이 있습니다. 예를 들어, 제가 기억하는 가장 초기의 것 중 하나는 2020년 논문 "Transformers are RNNs: Fast Autoregressive Transformers with Linear Attention"으로, 연구원들은 어텐션 메커니즘을 다음과 같이 근사했습니다:
\(\text{Attention}(Q, K, V) = \text{softmax}\!\left(\frac{QK^\top}{\sqrt{d}}\right)V \approx \phi(Q)\big(\phi(K)^\top V\big)\)
여기서 ϕ(⋅)는 커널 특징 함수(kernel feature function)이며, ϕ(x) = elu(x)+1로 설정됩니다. 이 근사는 n×n 어텐션 행렬(attention matrix) QK T를 명시적으로 계산하는 것을 피하기 때문에 효율적입니다. 저는 이러한 오래된 시도에 너무 오래 머물고 싶지 않습니다. 하지만 핵심은 긴 시퀀스(long sequences)에 대해 어텐션을 훨씬 더 효율적으로 만들기 위해 시간 및 메모리 복잡도(time and memory complexity)를 O(n 2 )에서 O(n)으로 줄였다는 것입니다. 하지만 이들은 모델 정확도(model accuracy)를 저하시켰기 때문에 실제로 인기를 얻지 못했으며, 저는 이러한 변형 중 하나가 오픈 가중치 최첨단 LLM에 적용된 것을 본 적이 없습니다. 하지만 최근 몇 년간 모델 규모와 컨텍스트 길이가 기하급수적으로 증가하면서 효율적인 아키텍처에 대한 필요성이 더욱 커졌고, 이는 선형 어텐션의 재조명을 이끌었습니다.

**2.3 선형 어텐션의 부활(Linear Attention Revival)**
올해 하반기에는 선형 어텐션(linear attention) 변형이 다시 주목받았으며, 아래 그림에 설명된 대로 일부 모델 개발자들 사이에서 약간의 논쟁이 있었습니다. 그림 5: 선형 어텐션 하이브리드 아키텍처(linear attention hybrid architectures)의 개요. 첫 번째 주목할 만한 모델은 라이트닝 어텐션(lightning attention)을 사용한 MiniMax-M1이었습니다. MiniMax-M1은 4560억 개의 매개변수를 가진 MoE(mixture-of-experts) 모델로, 460억 개의 활성 매개변수를 가지며 지난 6월에 출시되었습니다. 그리고 8월에는 Qwen3 팀이 Qwen3-Next를 발표했는데, 이에 대해서는 위에서 더 자세히 논의했습니다. 그리고 9월에는 DeepSeek 팀이 DeepSeek V3.2를 발표했습니다. (DeepSeek V3.2의 희소 어텐션 메커니즘(sparse attention mechanism)은 엄밀히 말해 선형은 아니지만, 계산 비용(computational costs) 측면에서 최소한 이차 미만(subquadratic)이므로 MiniMax-M1, Qwen3-Next, Kimi Linear와 같은 범주에 넣는 것이 타당하다고 생각합니다.) 이 세 모델(MiniMax-M1, Qwen3-Next, DeepSeek V3.2)은 대부분 또는 모든 레이어에서 전통적인 이차 어텐션(quadratic attention) 변형을 효율적인 선형 변형으로 대체합니다. 흥미롭게도 최근 반전이 있었는데, MiniMax 팀은 선형 어텐션 없이 새로운 2300억 개의 매개변수를 가진 M2 모델을 출시하며 일반 어텐션으로 돌아갔습니다. 팀은 선형 어텐션이 프로덕션 LLM에서 까다롭다고 밝혔습니다. 일반적인 프롬프트(prompts)에서는 잘 작동하는 것처럼 보였지만, 추론(reasoning) 및 다중 턴 작업(multi-turn tasks)에서 정확도가 낮았는데, 이는 일반적인 채팅 세션뿐만 아니라 에이전트 애플리케이션(agentic applications)에도 중요합니다. 이는 선형 어텐션이 결국 추구할 가치가 없을 수도 있다는 전환점이 될 수 있었습니다. 하지만 더 흥미로운 점이 있습니다. 10월에 Kimi 팀은 선형 어텐션(linear attention)을 사용하는 새로운 Kimi Linear 모델을 출시했습니다. 이 선형 어텐션 측면에서 Qwen3-Next와 Kimi Linear는 모두 Gated DeltaNet을 채택했는데, 이는 다음 몇 섹션에서 하이브리드 어텐션 아키텍처(hybrid attention architecture)의 한 예시로 논의하고 싶었습니다.

**2.4 Qwen3-Next**
Qwen3-Next부터 시작하겠습니다. 이 모델은 일반 어텐션 메커니즘(attention mechanism)을 Gated DeltaNet + Gated Attention 하이브리드(hybrid)로 대체했는데, 이는 메모리 사용량 측면에서 기본 262k 토큰 컨텍스트 길이(context length)를 가능하게 합니다 (이전 235B-A22B 모델은 기본적으로 32k를 지원했으며, YaRN 스케일링(scaling)을 통해 131k를 지원했습니다). 이들의 하이브리드 메커니즘(hybrid mechanism)은 아래 그림과 같이 Gated DeltaNet 블록(blocks)과 Gated Attention 블록을 3:1 비율로 혼합합니다. 그림 6: 게이티드 어텐션(gated attention) 및 Gated DeltaNet을 사용한 Qwen3-Next. 위 그림에서 묘사된 바와 같이, 어텐션 메커니즘(attention mechanism)은 게이티드 어텐션(gated attention) 또는 Gated DeltaNet으로 구현됩니다. 이는 이 아키텍처(architecture)의 48개 트랜스포머 블록(transformer blocks, 레이어)이 이 둘 사이를 번갈아 사용한다는 것을 의미합니다. 특히, 앞서 언급했듯이 3:1 비율로 번갈아 사용됩니다. 예를 들어, 트랜스포머 블록은 다음과 같습니다:
──────────────────────────────────
Layer 1 : Linear attention → MoE
Layer 2 : Linear attention → MoE
Layer 3 : Linear attention → MoE
Layer 4 : Full attention → MoE
──────────────────────────────────
Layer 5 : Linear attention → MoE
Layer 6 : Linear attention → MoE
Layer 7 : Linear attention → MoE
Layer 8 : Full attention → MoE
──────────────────────────────────
...
그 외에는 아키텍처가 상당히 표준적이며 Qwen3와 유사합니다: 그림 7: 이전 "일반" Qwen3 모델(왼쪽)과 Qwen3-Next(오른쪽). 그렇다면 게이티드 어텐션(gated attention)과 Gated DeltaNet은 무엇일까요? Qwen3-Next의 성공은 이후 다른 모델들이 유사한 하이브리드 전략을 탐색하도록 영감을 주었으며, 특히 장문 컨텍스트 처리 능력에 대한 기여가 컸습니다.

**2.5 게이티드 어텐션(Gated Attention)**
Gated DeltaNet 자체에 대해 알아보기 전에, 게이트(gate)에 대해 간략히 이야기해 봅시다. 이전 그림의 Qwen3-Next 아키텍처(architecture) 상단에서 볼 수 있듯이, Qwen3-Next는 "게이티드 어텐션(gated attention)"을 사용합니다. 이는 본질적으로 추가적인 시그모이드 게이트(sigmoid gate)가 있는 일반적인 전체 어텐션(full attention)입니다. 이 게이팅(gating)은 설명 목적으로 아래에 있는 MultiHeadAttention 구현(저의 "LLMs from Scratch" 책 3장의 코드를 기반으로 함)에 제가 추가한 간단한 수정 사항입니다:
```python
class MultiHeadAttention(nn.Module):
    def __init__(self, d_in, d_out, block_size, dropout, num_heads, qkv_bias=False):
        super().__init__()
        self.d_out = d_out
        self.num_heads = num_heads
        self.head_dim = d_out // num_heads # Reduce d_out to head_dim
        self.W_query = nn.Linear(d_in, d_out, bias=qkv_bias)
        self.W_key = nn.Linear(d_in, d_out, bias=qkv_bias)
        self.W_value = nn.Linear(d_in, d_out, bias=qkv_bias)
        self.out_proj = nn.Linear(d_out, d_out)
        self.dropout = nn.Dropout(dropout)
        self.register_buffer('mask', torch.triu(torch.ones(block_size, block_size), diagonal=1))

        # Gating mechanism
        self.gate_proj = nn.Linear(d_in, d_out) # Projects input to gating signal

    def forward(self, x, mask=None):
        b, num_tokens, d_in = x.shape

        keys = self.W_key(x)
        queries = self.W_query(x)
        values = self.W_value(x)

        keys = keys.view(b, num_tokens, self.num_heads, self.head_dim)
        values = values.view(b, num_tokens, self.num_heads, self.head_dim)
        queries = queries.view(b, num_tokens, self.num_heads, self.head_dim)

        keys = keys.transpose(1, 2)
        queries = queries.transpose(1, 2)
        values = values.transpose(1, 2)

        attn_scores = queries @ keys.transpose(2, 3)
        attn_scores = attn_scores / math.sqrt(self.head_dim)

        if mask is not None:
            attn_scores.masked_fill_(mask.bool(), -torch.inf)

        attn_weights = torch.softmax(attn_scores, dim=-1)
        attn_weights = self.dropout(attn_weights)

        context_vec = (attn_weights @ values).transpose(1, 2).contiguous().view(b, num_tokens, self.d_out)

        # Apply gating
        gate_signal = torch.sigmoid(self.gate_proj(x))
        gated_context_vec = context_vec * gate_signal

        return self.out_proj(gated_context_vec)
```
보시다시피, 평소처럼 어텐션(attention)을 계산한 후, 모델은 동일한 입력에서 별도의 게이팅 신호(gating signal)를 사용하고, 이를 0과 1 사이로 유지하기 위해 시그모이드(sigmoid)를 적용한 다음, 어텐션 출력(attention output)과 곱합니다. 이를 통해 모델은 특정 특징(features)을 동적으로 확대하거나 축소할 수 있습니다. Qwen3-Next 개발자들은 이것이 훈련 안정성(training stability)에 도움이 된다고 말합니다:
[...] 어텐션 출력 게이팅 메커니즘(attention output gating mechanism)은 어텐션 싱크(Attention Sink) 및 대규모 활성화(Massive Activation)와 같은 문제를 제거하여 모델 전반의 수치적 안정성(numerical stability)을 보장합니다.
요컨대, 게이티드 어텐션(gated attention)은 표준 어텐션(standard attention)의 출력을 조절합니다. 다음 섹션에서는 어텐션 메커니즘(attention mechanism) 자체를 순환 델타 규칙 메모리 업데이트(recurrent delta-rule memory update)로 대체하는 Gated DeltaNet에 대해 논의합니다.

**2.6 Gated DeltaNet**
이제 Gated DeltaNet은 무엇일까요? Gated DeltaNet (Gated Delta Network의 약자)은 Qwen3-Next의 선형 어텐션 레이어(linear-attention layer)로, 표준 소프트맥스 어텐션(softmax attention)의 대안으로 고안되었습니다. 앞서 언급했듯이 "Gated Delta Networks: Improving Mamba2 with Delta Rule" 논문에서 채택되었습니다. Gated DeltaNet은 원래 Mamba2의 개선된 버전으로 제안되었으며, Mamba2의 게이티드 감쇠 메커니즘(gated decay mechanism)과 델타 규칙(delta rule)을 결합합니다. Mamba는 상태 공간 모델(state-space model, 트랜스포머의 대안)이며, 미래에 별도로 다룰 가치가 있는 큰 주제입니다. 델타 규칙(delta rule) 부분은 새로운 값과 예측된 값 사이의 차이(델타, Δ)를 계산하여 메모리 상태(memory state)로 사용되는 은닉 상태(hidden state)를 업데이트하는 것을 의미합니다(이에 대해서는 나중에 자세히 설명합니다). (참고: 고전적인 기계 학습 문헌에 익숙한 독자들은 이를 생물학에서 영감을 받은 헵 학습(Hebbian learning)과 유사하다고 생각할 수 있습니다: "함께 발화하는 세포는 함께 연결된다." 이는 기본적으로 퍼셉트론 업데이트 규칙(perceptron update rule) 및 경사 하강 기반 학습(gradient descent-based learning)의 전신이지만, 감독(supervision)이 없습니다.) Gated DeltaNet은 앞서 논의된 게이티드 어텐션(gated attention)의 게이트와 유사한 게이트를 가지고 있지만, 아래 그림과 같이 로지스틱 시그모이드 활성화(logistic sigmoid activation) 대신 SiLU를 사용합니다. (SiLU 선택은 표준 시그모이드(sigmoid)에 비해 경사 흐름(gradient flow)과 안정성(stability)을 개선하기 위한 것으로 보입니다.) 그림 8: 게이티드 어텐션(gated attention)과 Gated DeltaNet 비교. 하지만 위 그림에서 보듯이, 출력 게이트(output gate) 외에도 Gated DeltaNet의 "게이티드(gated)"는 몇 가지 추가 게이트를 의미합니다: α (감쇠 게이트, decay gate)는 시간이 지남에 따라 메모리(memory)가 얼마나 빨리 감쇠하거나 재설정되는지를 제어하고, β (업데이트 게이트, update gate)는 새로운 입력이 상태(state)를 얼마나 강력하게 수정하는지를 제어합니다. 코드에서 위에서 묘사된 Gated DeltaNet의 단순화된 버전(컨볼루션 혼합(convolutional mixing) 없음)은 다음과 같이 구현될 수 있습니다 (이 코드는 Qwen3 팀의 공식 구현에서 영감을 받았습니다):
```python
import torch
import torch.nn as nn
import torch.nn.functional as F

class GatedDeltaNet(nn.Module):
    def __init__(self, d_model, num_heads, head_dim, dropout=0.1):
        super().__init__()
        self.d_model = d_model
        self.num_heads = num_heads
        self.head_dim = head_dim
        
        # Linear projections for input x
        self.proj_x = nn.Linear(d_model, num_heads * head_dim)
        
        # Projections for gates alpha, beta, and output gate
        self.proj_alpha = nn.Linear(d_model, num_heads * head_dim)
        self.proj_beta = nn.Linear(d_model, num_heads * head_dim)
        self.proj_output_gate = nn.Linear(d_model, num_heads * head_dim)
        
        self.out_proj = nn.Linear(num_heads * head_dim, d_model)
        self.dropout = nn.Dropout(dropout)

    def forward(self, x, state=None):
        b, n, d_model = x.shape # batch_size, sequence_length, d_model

        # Initialize state if not provided (for the first token)
        if state is None:
            # S is the recurrent memory state, initialized to zeros
            # Shape: (b, num_heads, head_dim, head_dim)
            state = x.new_zeros(b, self.num_heads, self.head_dim, self.head_dim)

        output_sequence = []
        for t in range(n): # Process tokens one by one
            x_t = x[:, t, :] # Current token input (b, d_model)

            # Project current input for value and gates
            v_t = self.proj_x(x_t).view(b, self.num_heads, self.head_dim) # (b, num_heads, head_dim)
            alpha_t = F.silu(self.proj_alpha(x_t)).view(b, self.num_heads, self.head_dim) # (b, num_heads, head_dim)
            beta_t = F.silu(self.proj_beta(x_t)).view(b, self.num_heads, self.head_dim) # (b, num_heads, head_dim)
            
            # Update the state S
            # S_t = alpha_t * S_{t-1} + beta_t * v_t * v_t.transpose(-1, -2)
            # Simplified for illustration: S_t = alpha_t * S_{t-1} + beta_t * v_t
            # Here, we'll use a simpler update rule for the state S for readability
            # A more accurate representation of the paper's DeltaNet state update
            # would involve outer products or more complex interactions.
            # For this simplified example, let's assume S is a vector per head.
            
            # Let's adjust state shape to (b, num_heads, head_dim) for this simplified example
            # In the actual paper, S is (b, num_heads, head_dim, head_dim)
            # and the update involves outer products or more complex matrix ops.
            if state.shape[-1] != self.head_dim: # Re-initialize if shape is wrong for this simplification
                 state = x.new_zeros(b, self.num_heads, self.head_dim)
            
            state = alpha_t * state + beta_t * v_t # Element-wise update

            # Compute output for current token
            output_t = state.mean(dim=1) # Aggregate across heads for simplicity (b, head_dim)
            
            # Apply output gate
            output_gate_t = torch.sigmoid(self.proj_output_gate(x_t)).view(b, self.num_heads, self.head_dim)
            output_t = output_t * output_gate_t.mean(dim=1) # Apply output gate

            output_sequence.append(output_t)

        output = torch.stack(output_sequence, dim=1) # (b, n, head_dim)
        output = self.out_proj(output) # (b, n, d_model)
        return output, state # Return final output and updated state
```
(간단함을 위해 Qwen3-Next와 Kimi Linear가 사용하는 컨볼루션 혼합(convolutional mixing)을 생략하여 코드를 더 읽기 쉽게 하고 순환적 측면(recurrent aspects)에 집중했습니다.) 따라서 위에서 볼 수 있듯이 표준 (또는 게이티드) 어텐션(attention)과는 많은 차이가 있습니다. 게이티드 어텐션(gated attention)에서는 모델이 모든 토큰(tokens) 간에 일반 어텐션(normal attention)을 계산합니다 (모든 토큰은 다른 모든 토큰을 주시하거나 살펴봅니다). 그런 다음 어텐션 출력(attention output)을 얻은 후, 게이트(시그모이드)가 해당 출력 중 얼마를 유지할지 결정합니다. 핵심은 여전히 컨텍스트 길이(context length)에 따라 이차적으로 확장되는 일반 스케일드 닷 프로덕트 어텐션(scaled-dot product attention)이라는 것입니다. 다시 한번 상기시키자면, 스케일드 닷 프로덕트 어텐션(scaled-dot product attention)은 softmax(QKᵀ)V로 계산됩니다. 여기서 Q와 K는 n x d 행렬이며, n은 입력 토큰(input tokens)의 수이고 d는 임베딩 차원(embedding dimension)입니다. 따라서 QKᵀ는 n x n 어텐션 행렬(attention matrix)을 생성하며, 이는 n x d 차원의 값 행렬(value matrix) V와 곱해집니다. 그림 9: 토큰 수 n에 따라 확장되는 전통적인 어텐션 메커니즘(attention mechanism) (다시 한번). Gated DeltaNet에는 n x n 어텐션 행렬(attention matrix)이 없습니다. 대신, 모델은 토큰(tokens)을 하나씩 처리합니다. 각 새 토큰이 들어올 때마다 업데이트되는 실행 중인 메모리(상태)를 유지합니다. 이것이 구현된 방식이며, 여기서 S는 각 시간 단계 t에 대해 순환적으로(recurrently) 업데이트되는 상태(state)입니다. 그리고 게이트(gates)는 해당 메모리(memory)가 어떻게 변하는지 제어합니다: α (알파)는 이전 메모리 중 얼마를 잊을지(감쇠) 조절합니다. β (베타)는 시간 단계 t의 현재 토큰이 메모리(memory)를 얼마나 업데이트하는지 조절합니다. (그리고 위 코드 조각에는 표시되지 않은 최종 출력 게이트(output gate)는 게이티드 어텐션(gated attention)과 유사하며, 출력 중 얼마를 유지할지 제어합니다.) 따라서 어떤 의미에서는 Gated DeltaNet의 이 상태 업데이트(state update)는 순환 신경망(RNNs)이 작동하는 방식과 유사합니다. 장점은 컨텍스트 길이(context length)에 따라 이차적으로가 아닌 선형적으로(for-loop을 통해) 확장된다는 것입니다. 이 순환 상태 업데이트(recurrent state update)의 단점은 일반 (또는 게이티드) 어텐션(attention)과 비교할 때, 전체 쌍별 어텐션(full pairwise attention)에서 오는 전역 컨텍스트 모델링(global context modeling) 능력을 희생한다는 것입니다. Gated DeltaNet은 어느 정도 컨텍스트(context)를 여전히 포착할 수 있지만, 메모리(S) 병목 현상(bottleneck)을 거쳐야 합니다. 해당 메모리(memory)는 고정된 크기이므로 더 효율적이지만, RNN과 유사하게 과거 컨텍스트(past context)를 단일 은닉 상태(hidden state)로 압축합니다. 이것이 Qwen3-Next와 Kimi Linear 아키텍처(architectures)가 모든 어텐션 레이어(attention layers)를 DeltaNet 레이어로 대체하지 않고 앞서 언급된 3:1 비율을 사용하는 이유입니다.

**2.7 DeltaNet 메모리 절약(DeltaNet Memory Savings)**
이전 섹션에서는 컨텍스트 길이(context length)에 대한 이차적 계산 복잡도(quadratic compute complexity) 대신 선형적 계산 복잡도(linear compute complexity) 측면에서 DeltaNet이 전체 어텐션(full attention)보다 가지는 장점을 논의했습니다. 선형 계산 복잡도(linear compute complexity) 외에 DeltaNet의 또 다른 큰 장점은 메모리 절약(memory savings)입니다. DeltaNet 모듈(modules)은 KV 캐시(KV cache)를 증가시키지 않기 때문입니다. (KV 캐싱(KV caching)에 대한 자세한 내용은 저의 "Understanding and Coding the KV Cache in LLMs from Scratch" 글을 참조하십시오.) 대신, 앞서 언급했듯이 고정 크기의 순환 상태(recurrent state)를 유지하므로 메모리(memory)는 컨텍스트 길이(context length)에 따라 일정하게 유지됩니다. 일반적인 멀티 헤드 어텐션(MHA) 레이어의 경우, KV 캐시(KV cache) 크기를 다음과 같이 계산할 수 있습니다:
KV_cache_MHA ≈ batch_size × n_tokens × n_heads × d_head × 2 × bytes
(2를 곱하는 이유는 캐시(cache)에 키(keys)와 값(values)을 모두 저장하기 때문입니다.) 위에서 구현된 단순화된 DeltaNet 버전의 경우 다음과 같습니다:
KV_cache_DeltaNet = batch_size × n_heads × d_head × d_head × bytes
KV_cache_DeltaNet 메모리 크기에는 컨텍스트 길이(n_tokens) 의존성이 없다는 점에 유의하십시오. 또한, 별도의 키(keys)와 값(values) 대신 메모리 상태 S만 저장하므로 2 × bytes가 그냥 bytes가 됩니다. 하지만 이제 여기에 이차적인 d_head × d_head가 있다는 점에 유의하십시오. 이는 상태(state)에서 비롯됩니다: S = x.new_zeros(b, self.num_heads, self.head_dim, self.head_dim) 하지만 헤드 차원(head dimension)은 일반적으로 상대적으로 작기 때문에 보통 걱정할 필요가 없습니다. 예를 들어, Qwen3-Next에서는 128입니다. 컨볼루션 혼합(convolutional mixing)이 포함된 전체 버전은 커널 크기(kernel size) 등을 포함하여 약간 더 복잡하지만, 위의 공식들은 Gated DeltaNet의 주요 경향과 동기를 설명해야 합니다. 그림 10: 증가하는 KV 캐시(KV cache) 크기 비교. 3:1 비율은 Gated DeltaNet과 전체 어텐션 레이어(full attention layers)의 비율을 나타냅니다. 계산은 emb_dim=2048, n_heads=16, n_layers=48, bf16을 가정합니다. 이 코드를 재현하는 코드는 여기에서 찾을 수 있습니다: https://github.com/rasbt/LLMs-from-scratch/tree/main/ch04/08_deltanet. 이러한 메모리 절약은 특히 온디바이스(on-device) 또는 엣지(edge) 환경에서 LLM을 배포하려는 시도가 늘어나면서 더욱 중요해지고 있습니다.

**2.8 Kimi Linear 대 Qwen3-Next (Kimi Linear vs. Qwen3-Next)**
Kimi Linear는 Qwen3-Next와 여러 구조적 유사점을 공유합니다. 두 모델 모두 하이브리드 어텐션 전략(hybrid attention strategy)에 의존합니다. 구체적으로, 이들은 경량 선형 어텐션(lightweight linear attention)과 더 무거운 전체 어텐션 레이어(full attention layers)를 결합합니다. 특히, 두 모델 모두 3:1 비율을 사용합니다. 즉, 선형 Gated DeltaNet 변형을 사용하는 세 개의 트랜스포머 블록(transformer blocks)마다 아래 그림과 같이 전체 어텐션(full attention)을 사용하는 하나의 블록이 있습니다. 그림 11: Qwen3-Next와 Kimi Linear 나란히. Gated DeltaNet은 순환 신경망(recurrent neural networks)에서 영감을 받은 선형 어텐션(linear attention) 변형으로, "Gated Delta Networks: Improving Mamba2 with Delta Rule" 논문의 게이팅 메커니즘(gating mechanism)을 포함합니다. 어떤 의미에서 Gated DeltaNet은 Mamba 스타일 게이팅(Mamba-style gating)을 가진 DeltaNet이며, DeltaNet은 선형 어텐션 메커니즘(linear attention mechanism)입니다 (이에 대해서는 다음 섹션에서 자세히 설명합니다). 위 그림 11의 오른쪽 상자에서 묘사된 Kimi Linear의 MLA는 시그모이드 게이트(sigmoid gate)를 사용하지 않습니다. 이 생략은 저자들이 아키텍처(architecture)를 표준 MLA와 더 직접적으로 비교할 수 있도록 의도된 것이었지만, 그들은 미래에 이를 추가할 계획이라고 밝혔습니다. 또한 위 그림의 Kimi Linear 부분에서 RoPE 상자(RoPE box)가 생략된 것도 의도적인 것임을 유의하십시오. Kimi는 멀티 헤드 잠재 어텐션(multi-head latent attention, MLA) 레이어(전역 어텐션, global attention)에 NoPE(No Positional Embedding)를 적용합니다. 저자들이 언급했듯이, 이는 MLA가 추론(inference) 시 순수한 멀티 쿼리 어텐션(multi-query attention)으로 실행되도록 하고, 긴 컨텍스트 스케일링(long-context scaling)을 위한 RoPE 재조정(retuning)을 피합니다 (위치 편향(positional bias)은 Kimi Delta Attention 블록(blocks)에 의해 처리되는 것으로 추정됩니다). MLA 및 그룹화된 쿼리 어텐션(grouped-query attention)의 특별한 경우인 멀티 쿼리 어텐션(multi-query attention)에 대한 자세한 내용은 저의 "The Big LLM Architecture Comparison" 글을 참조하십시오.

**2.9 Kimi Delta 어텐션(Kimi Delta Attention)**
Kimi Linear는 Qwen3-Next의 선형 어텐션 메커니즘(linear attention mechanism)을 Kimi Delta Attention (KDA) 메커니즘으로 수정하는데, 이는 본질적으로 Gated DeltaNet의 정교화(refinement)입니다. Qwen3-Next가 메모리 감쇠율(memory decay rate)을 제어하기 위해 스칼라 게이트(scalar gate, 어텐션 헤드(attention head)당 하나의 값)를 적용하는 반면, Kimi Linear는 이를 각 특징 차원(feature dimension)에 대한 채널별 게이팅(channel-wise gating)으로 대체합니다. 저자들에 따르면, 이는 메모리(memory)에 대한 더 많은 제어권을 제공하며, 이는 결과적으로 긴 컨텍스트 추론(long-context reasoning)을 개선합니다. 또한, 전체 어텐션 레이어(full attention layers)의 경우, Kimi Linear는 Qwen3-Next의 게이티드 어텐션 레이어(gated attention layers, 본질적으로 출력 게이팅(output gating)이 있는 표준 멀티 헤드 어텐션 레이어)를 멀티 헤드 잠재 어텐션(multi-head latent attention, MLA)으로 대체합니다. 이는 DeepSeek V3/R1에서 사용된 동일한 MLA 메커니즘(mechanism)이지만 (저의 "The Big LLM Architecture Comparison" 글에서 논의했듯이) 추가 게이트(gate)가 있습니다. (요약하자면, MLA는 KV 캐시(KV cache) 크기를 줄이기 위해 키/값 공간(key/value space)을 압축합니다.) Qwen3-Next와의 직접적인 비교는 없지만, Gated DeltaNet 논문의 Gated DeltaNet-H1 모델(본질적으로 슬라이딩 윈도우 어텐션(sliding-window attention)이 있는 Gated DeltaNet)과 비교할 때, Kimi Linear는 동일한 토큰 생성 속도(token-generation speed)를 유지하면서 더 높은 모델링 정확도(modeling accuracy)를 달성합니다. 그림 12: Kimi Linear 논문(https://arxiv.org/abs/2510.26692)의 주석이 달린 그림으로, Kimi Linear가 GatedDeltaNet만큼 빠르고, 멀티 헤드 잠재 어텐션(multi-head latent attention)을 사용하는 아키텍처(DeepSeek V3/R1과 같은)보다 훨씬 빠르며, 더 높은 벤치마크 성능(benchmark performance)을 가짐을 보여줍니다. 또한, DeepSeek-V2 논문의 절제 연구(ablation studies)에 따르면, 하이퍼파라미터(hyperparameters)를 신중하게 선택하면 MLA는 일반 전체 어텐션(full attention)과 동등합니다. 그리고 Kimi Linear가 긴 컨텍스트(long-context) 및 추론 벤치마크(reasoning benchmarks)에서 MLA보다 유리하게 비교된다는 사실은 선형 어텐션(linear attention) 변형이 더 큰 최첨단 모델(state-of-the-art models)에 대해 다시 한번 유망하다는 것을 의미합니다. 그렇긴 하지만, Kimi Linear는 480억 개의 매개변수를 가지지만, Kimi K2보다 20배 작습니다. Kimi 팀이 다가오는 K3 모델에 이 접근 방식을 채택할지 지켜보는 것은 흥미로울 것입니다. Kimi의 최신 모델인 Kimi K4는 이러한 하이브리드 접근 방식의 성공을 바탕으로 더 긴 컨텍스트와 향상된 추론 능력을 제공하며, 이러한 설계 원칙이 대규모 모델에서도 적용 가능하다는 것을 보여주었습니다.

**2.10 어텐션 하이브리드의 미래(The Future of Attention Hybrids)**
선형 어텐션(linear attention)은 새로운 개념이 아니지만, 하이브리드 접근 방식(hybrid approaches)의 최근 부활은 연구자들이 트랜스포머(transformers)를 더 효율적으로 만들기 위한 실용적인 방법을 다시 진지하게 찾고 있음을 보여줍니다. 예를 들어, Kimi Linear는 일반 전체 어텐션(full attention)과 비교하여 KV 캐시(KV cache)를 75% 줄이고 디코딩 처리량(decoding throughput)을 최대 6배 향상시킵니다. 이 새로운 세대의 선형 어텐션(linear attention) 변형이 이전 시도와 다른 점은, 이제 표준 어텐션(standard attention)을 완전히 대체하는 대신 함께 사용된다는 것입니다. 앞으로, 다음 어텐션 하이브리드(attention hybrids)의 물결은 긴 컨텍스트 안정성(long-context stability)과 추론 정확도(reasoning accuracy)를 더욱 개선하여 전체 어텐션(full-attention)의 최첨단(state-of-the-art)에 더 가까워지는 데 초점을 맞출 것으로 예상합니다. 특히, 다양한 작업 부하와 컨텍스트 길이에 따라 어텐션 메커니즘을 동적으로 조정하는 적응형 하이브리드 모델이 등장할 가능성이 높습니다.

**3. 텍스트 확산 모델(Text Diffusion Models)**
표준 자기회귀 LLM 아키텍처(autoregressive LLM architecture)에서 더 급진적으로 벗어난 것은 텍스트 확산 모델(text diffusion models) 계열입니다. 여러분은 아마도 확산 모델(diffusion models)에 익숙할 것입니다. 이는 2020년 "Denoising Diffusion Probabilistic Models" 논문을 기반으로 이미지를 생성하는 모델(생성적 적대 신경망(generative adversarial networks)의 후속 모델)이며, 나중에 Stable Diffusion 등에 의해 구현, 확장 및 대중화되었습니다. 그림 13: 2022년 저의 첫 Substack 글에서 가져온 이미지 확산 과정(image diffusion process)의 설명. 여기서는 왼쪽에서 오른쪽으로 가우시안 노이즈(Gaussian noise)가 추가되며, 모델의 과제는 노이즈를 제거하는 방법(오른쪽에서 왼쪽으로)을 학습하는 것입니다. 이미지, 비디오, 3D 콘텐츠 생성에서 확산 모델의 성공은 텍스트 생성 분야에서도 그 잠재력을 탐색하게 만들었습니다.

**3.1 왜 텍스트 확산에 대해 연구하는가?(Why Work on Text Diffusion?)**
2022년 "Diffusion‑LM Improves Controllable Text Generation" 논문과 함께, 연구자들이 텍스트 생성(generating text)을 위해 확산 모델(diffusion models)을 채택하기 시작하는 추세의 시작을 보게 되었습니다. 그리고 저는 지난해에도 수많은 텍스트 확산(text diffusion) 논문을 보았습니다. 제 논문 북마크 목록을 확인해 보니, 39개의 텍스트 확산 모델(text diffusion models)이 있었습니다! 이 모델들의 인기가 높아지는 것을 고려할 때, 마침내 이들에 대해 이야기할 때가 되었다고 생각했습니다. 그림 14: 이 섹션은 텍스트 확산 모델(text diffusion models)을 다룹니다. 그렇다면 확산 모델(diffusion models)의 장점은 무엇이며, 연구자들은 왜 이를 전통적인 자기회귀 LLM(autoregressive LLMs)의 대안으로 고려하고 있을까요? 전통적인 트랜스포머 기반(자기회귀) LLM은 한 번에 하나의 토큰(token)을 생성합니다. 간결함을 위해, 이들을 단순히 자기회귀 LLM(autoregressive LLMs)이라고 부르겠습니다. 이제 텍스트 확산 기반 LLM(이들을 "확산 LLM(diffusion LLMs)"이라고 부르겠습니다)의 주요 강점은 순차적으로(sequentially)가 아닌 병렬로(in parallel) 여러 토큰(tokens)을 생성할 수 있다는 것입니다. 확산 LLM(diffusion LLMs)은 여전히 여러 번의 노이즈 제거 단계(denoising steps)를 필요로 한다는 점에 유의하십시오. 하지만 확산 모델이 각 단계에서 모든 토큰(tokens)을 병렬로 생성하기 위해 예를 들어 64번의 노이즈 제거 단계(denoising steps)를 필요로 하더라도, 이는 2,000개의 토큰 응답을 생성하기 위해 2,000번의 순차적 생성 단계(sequential generation steps)를 수행하는 것보다 여전히 계산적으로 더 효율적입니다. 이러한 병렬 처리 능력은 특히 낮은 지연 시간(low-latency)이 요구되는 애플리케이션에서 매우 중요하게 작용합니다.

**3.2 노이즈 제거 과정(The Denoising Process)**
확산 LLM(diffusion LLM)의 노이즈 제거 과정(denoising process)은 일반 이미지 확산 모델(image diffusion models)의 노이즈 제거 과정과 유사하며, 아래 GIF에 나와 있습니다. (주요 차이점은 픽셀(pixels)에 가우시안 노이즈(Gaussian noise)를 추가하는 대신, 텍스트 확산(text diffusion)은 확률적으로 토큰(tokens)을 마스킹(masking)하여 시퀀스(sequences)를 손상시킨다는 것입니다.) 이 실험을 위해, 저는 올해 초에 발표된 "Large Language Diffusion Models (LLaDA)" 논문의 8B 지시 모델(instruct model)을 실행했습니다. 그림 15: 8B LLaDA 모델을 사용한 노이즈 제거 과정(denoising process)의 설명. 위 애니메이션에서 볼 수 있듯이, 텍스트 확산 과정(text diffusion process)은 [MASK] 토큰(tokens)을 텍스트 토큰으로 연속적으로 대체하여 답변을 생성합니다. BERT와 마스킹된 언어 모델링(masked language modeling)에 익숙하다면, 이 확산 과정(diffusion process)을 BERT 순방향 전달(forward pass)의 반복적인 적용(BERT가 다른 마스킹 비율(masking rates)로 사용되는 경우)으로 생각할 수 있습니다. 아키텍처(architecture) 측면에서, 확산 LLM(diffusion LLMs)은 일반적으로 디코더 스타일 트랜스포머(decoder-style transformers)이지만 인과적 어텐션 마스크(causal attention mask)가 없습니다. 예를 들어, 앞서 언급된 LLaDA 모델은 Llama 3 아키텍처(architecture)를 사용합니다. 인과적 마스크(causal mask)가 없는 이러한 아키텍처를 "양방향(bidirectional)"이라고 부르는데, 이는 모든 시퀀스 요소(sequence elements)에 한 번에 접근할 수 있기 때문입니다. (이는 역사적인 이유로 "인코더 스타일(encoder-style)"이라고 불리는 BERT 아키텍처(architecture)와 유사하다는 점에 유의하십시오.) 따라서 자기회귀 LLM(autoregressive LLMs)과 확산 LLM(diffusion LLMs)의 주요 차이점(인과적 마스크 제거 외에)은 훈련 목표(training objective)입니다. LLaDA와 같은 확산 LLM(diffusion LLMs)은 다음 토큰 예측 목표(next-token prediction objective) 대신 생성적 확산 목표(generative diffusion objective)를 사용합니다. 이미지 모델(image models)에서 생성적 확산 목표(generative diffusion objective)는 연속적인 픽셀 공간(pixel space)을 가지고 있기 때문에 직관적입니다. 예를 들어, 가우시안 노이즈(Gaussian noise)를 추가하고 노이즈 제거를 학습하는 것은 수학적으로 자연스러운 연산입니다. 하지만 텍스트는 이산적인 토큰(discrete tokens)으로 구성되어 있으므로, 동일한 연속적인 의미에서 "노이즈"를 직접 추가하거나 제거할 수 없습니다. 따라서 픽셀 강도(pixel intensities)를 교란하는 대신, 이러한 확산 LLM(diffusion LLMs)은 토큰(tokens)을 무작위로 점진적으로 마스킹(masking)하여 텍스트를 손상시키며, 각 토큰은 지정된 확률로 특수 마스크 토큰(mask token)으로 대체됩니다. 그런 다음 모델은 각 단계에서 누락된 토큰(tokens)을 예측하는 역 과정(reverse process)을 학습하며, 이는 앞서 그림 15의 애니메이션에서 보듯이 시퀀스(sequence)를 원래 텍스트로 효과적으로 "노이즈 제거(denoises)"(또는 마스크 해제(unmasks))합니다. 그 뒤에 있는 수학을 설명하는 것은 별도의 튜토리얼에 더 적합하겠지만, 대략적으로는 BERT가 확률적 최대 우도 프레임워크(probabilistic maximum-likelihood framework)로 확장된 것으로 생각할 수 있습니다. 최근 연구에서는 마스킹 전략을 더욱 정교하게 만들어 텍스트의 구조적, 의미적 특성을 더 잘 보존하려는 시도가 활발히 이루어지고 있습니다.

**3.3 자기회귀 LLM 대 확산 LLM(Autoregressive vs Diffusion LLMs)**
앞서 저는 확산 LLM(diffusion LLMs)이 매력적인 이유는 일반 자기회귀 LLM(autoregressive LLM)처럼 토큰(tokens)을 순차적으로 생성하는 대신 병렬로 생성(또는 노이즈 제거)하기 때문이라고 말했습니다. 이는 확산 모델(diffusion models)을 자기회귀 LLM(autoregressive LLMs)보다 더 효율적으로 만들 잠재력을 가지고 있습니다. 그렇긴 하지만, 전통적인 LLM의 자기회귀적 특성(autoregressive nature)은 그들의 주요 강점 중 하나입니다. 그리고 순수한 병렬 디코딩(parallel decoding)의 문제는 최근 "ParallelBench: Understanding the Trade-offs of Parallel Decoding in Diffusion LLMs" 논문의 훌륭한 예시로 설명될 수 있습니다. 그림 16: "ParallelBench: Understanding the Trade-offs of Parallel Decoding in Diffusion LLMs" 논문(https://arxiv.org/abs/2510.04767)의 주석이 달린 그림으로, 병렬 디코딩(parallel decoding)의 문제를 보여줍니다. 예를 들어, 다음 프롬프트(prompt)를 고려해 보십시오: > "여행할 도시를 무작위로 선택하세요: 뉴욕, 뉴올리언스, 멕시코시티, 파나마시티?" LLM에게 두 개의 토큰(token)으로 된 답변을 생성하도록 요청한다고 가정해 봅시다. 먼저 조건부 확률 p(y t = ”New” | X)에 따라 "New" 토큰(token)을 샘플링(sample)할 수 있습니다. 다음 반복에서는 이전에 생성된 토큰(token)에 조건을 부여하고 "York" 또는 "Orleans"를 선택할 가능성이 높습니다. 이는 조건부 확률 p(y t+1 = ”York” | X, y t = ”New”)와 p(y t+1 = ”Orleans” | X, y t = ”New”)가 모두 상대적으로 높기 때문입니다 ("New"는 훈련 세트(training set)에서 이러한 연속과 자주 함께 나타나기 때문입니다). 하지만 대신 두 토큰(tokens)이 병렬로 샘플링(sample)된다면, 모델은 독립적으로 두 개의 가장 높은 확률 토큰 p(y t = “New” | X)와 p(y {t+1} = “City” | X)를 선택하여 "New City"와 같은 어색한 출력을 초래할 수 있습니다. (이는 모델이 자기회귀적 조건화(autoregressive conditioning)가 부족하여 토큰 의존성(token dependencies)을 포착하지 못하기 때문입니다.) 어쨌든 위 내용은 확산 LLM(diffusion LLMs)에 조건부 의존성(conditional dependency)이 전혀 없는 것처럼 들리게 하는 단순화입니다. 이것은 사실이 아닙니다. 확산 LLM(diffusion LLM)은 앞서 말했듯이 모든 토큰(tokens)을 병렬로 예측하지만, 예측은 반복적인 정제(노이즈 제거) 단계(iterative refinement (denoising) steps)를 통해 공동으로 의존합니다. 여기서 각 확산 단계(diffusion step)는 현재의 전체 노이즈 텍스트(noisy text)에 조건을 부여합니다. 그리고 토큰(tokens)은 모든 단계에서 교차 어텐션(cross-attention)과 자기 어텐션(self-attention)을 통해 서로에게 영향을 미칩니다. 따라서 모든 위치가 동시에 업데이트되더라도, 업데이트는 공유 어텐션 레이어(attention layers)를 통해 서로에게 조건을 부여합니다. 하지만 앞서 언급했듯이, 이론적으로 2,000개 토큰(token) 답변을 생성할 때 20-60번의 확산 단계(diffusion steps)는 자기회귀 LLM(autoregressive LLM)의 2,000번 추론 단계(inference steps)보다 저렴할 수 있습니다. 최근 연구에서는 이러한 병렬 디코딩의 한계를 극복하기 위해 확산 과정 내에 국소적인 자기회귀 메커니즘을 통합하려는 시도가 이루어지고 있습니다.

**3.4 오늘날의 텍스트 확산(Text Diffusion Today)**
비전 모델(vision models)이 어텐션(attention) 및 트랜스포머 아키텍처(transformer architecture) 자체와 같은 LLM의 구성 요소를 채택하는 반면, 텍스트 기반 LLM은 순수 비전 모델에서 영감을 받아 텍스트용 확산(diffusion for text)을 구현하는 것은 흥미로운 추세입니다. 개인적으로 몇 가지 데모를 시도해 본 것 외에는 아직 많은 확산 모델(diffusion models)을 사용해 보지 않았지만, 저는 이를 트레이드오프(trade-off)로 생각합니다. 확산 단계(diffusion steps) 수를 적게 사용하면 답변을 더 빠르게 생성하지만, 품질이 저하된 답변을 생성할 수 있습니다. 더 나은 답변을 생성하기 위해 확산 단계(diffusion steps)를 늘리면, 자기회귀 모델(autoregressive model)과 유사한 비용을 가진 모델이 될 수 있습니다. "ParallelBench: Understanding the Trade-offs of Parallel Decoding in Diffusion LLMs" 논문의 저자들을 인용하자면:
[...] 우리는 [확산 LLM(diffusion LLMs)]과 자기회귀 LLM(autoregressive LLMs)을 체계적으로 분석하여 다음을 밝혀냈습니다: (i) 병렬 디코딩(parallel decoding) 하의 [확산 LLM]은 실제 시나리오에서 극적인 품질 저하를 겪을 수 있으며, (ii) 현재의 병렬 디코딩 전략(parallel decoding strategies)은 작업 난이도에 따라 병렬 처리 정도를 조정하는 데 어려움을 겪으므로, 품질 저하 없이 의미 있는 속도 향상을 달성하지 못합니다.
또한, 제가 보는 또 다른 특정 단점은 확산 LLM(diffusion LLMs)이 체인(chain)이 없기 때문에 도구(tools)를 체인의 일부로 사용할 수 없다는 것입니다. 확산 단계(diffusion steps) 사이에 이들을 끼워 넣는 것이 가능할 수도 있지만, 이는 사소한 일이 아니라고 생각합니다. (제가 틀렸다면 수정해 주십시오.) 요컨대, 확산 LLM(diffusion LLMs)은 탐구할 흥미로운 방향인 것으로 보이지만, 현재로서는 자기회귀 LLM(autoregressive LLMs)을 대체하지 못할 수도 있습니다. 하지만 저는 이들을 더 작고 온디바이스(on-device) LLM의 흥미로운 대안으로 보거나, 어쩌면 더 작고 증류된(distilled) 자기회귀 LLM을 대체할 수도 있다고 생각합니다. 예를 들어, Google은 텍스트용 Gemini Diffusion 모델을 개발 중이라고 발표했으며, "빠른 응답: 지금까지 우리의 가장 빠른 모델보다 훨씬 빠르게 콘텐츠를 생성합니다."라고 밝혔습니다. 그리고 더 빠르면서도, 벤치마크 성능(benchmark performance)은 빠른 Gemini 2.0 Flash-Lite 모델과 동등하게 유지되는 것으로 보입니다. 모델이 출시되고 사용자들이 다양한 작업과 도메인에서 시도해 본 후, 채택률과 피드백이 어떨지 지켜보는 것은 흥미로울 것입니다. 그림 17: (더 빠른) 확산 LLM(diffusion LLM, Gemini Diffusion)과 빠른 자기회귀 LLM(autoregressive LLM, Gemini 2.0 Flash-Lite)의 벤치마크 성능(benchmark performance) 비교. https://deepmind.google/models/gemini-diffusion/#capabilities에 보고된 수치를 기반으로 합니다. Gemini Diffusion의 실제 출시와 사용자 피드백은 이러한 트레이드오프에 대한 더 명확한 시사점을 제공할 것으로 기대됩니다.

**4. 월드 모델(World Models)**
지금까지 우리는 효율성을 개선하고 모델을 더 빠르거나 확장 가능하게 만드는 데 초점을 맞춘 접근 방식들을 논의했습니다. 그리고 이러한 접근 방식들은 일반적으로 약간 저하된 모델링 성능(modeling performance)을 동반합니다. 이제 이 섹션의 주제는 다른 관점을 취하며 모델링 성능(modeling performance) 향상(효율성 아님)에 초점을 맞춥니다. 이 개선된 성능은 모델에게 "세상에 대한 이해"를 가르침으로써 달성됩니다. 월드 모델(World models)은 전통적으로 언어 모델링(language modeling)과 독립적으로 개발되었지만, 2025년 9월에 발표된 최근 "Code World Models" 논문은 이들을 처음으로 이 맥락에서 직접적으로 관련성 있게 만들었습니다. 이상적으로는, 이 글의 다른 주제들과 마찬가지로, 월드 모델(world models)은 그 자체로 하나의 전용 글(또는 책)입니다. 하지만 Code World Models (CWM) 논문에 들어가기 전에, 월드 모델(world models)에 대한 짧은 소개라도 제공하겠습니다.

**4.1 월드 모델의 주요 아이디어(The Main Idea Behind World Models)**
원래 월드 모델(world models)의 아이디어는 결과를 암묵적으로 모델링하는 것입니다. 즉, 실제로 발생하지 않은 결과가 다음에 무엇이 일어날지 예측하는 것입니다 (아래 그림에 설명된 대로). 이는 인간의 뇌가 이전 경험을 바탕으로 다가올 사건을 지속적으로 예측하는 방식과 유사합니다. 예를 들어, 우리가 커피나 차 한 잔을 잡으려 할 때, 우리의 뇌는 이미 그것이 얼마나 무거울지 예측하고, 컵을 만지거나 들어 올리기도 전에 잡는 방식을 조절합니다. 그림 18: 월드 모델 시스템(world model system)의 개념적 개요. 에이전트(agent)는 현재 상태(state(t))를 관찰하고 주어진 목표를 달성하기 위해 행동(action(t))을 취함으로써 환경과 상호작용합니다. 동시에 에이전트는 환경의 정신적 시뮬레이션(mental simulation) 역할을 하는 내부 월드 모델(internal world model)을 학습하며, 이를 통해 실제 세계에서 실행하기 전에 결과를 예측하고 행동을 계획할 수 있습니다. 제가 아는 한, "월드 모델(world model)"이라는 용어는 Ha와 Schmidhuber의 2018년 동명 논문 "World Models"에 의해 대중화되었으며, 이 논문은 VAE와 RNN 아키텍처(architecture)를 사용하여 강화 학습 에이전트(reinforcement learning agents)를 위한 내부 환경 시뮬레이터(internal environment simulator)를 학습했습니다. (하지만 이 용어나 개념 자체는 본질적으로 세계나 환경의 개념을 모델링하는 것을 의미하므로, 1980년대의 강화 학습(reinforcement learning) 및 로봇 공학 연구(robotics research)로 거슬러 올라갑니다.) 솔직히, 저는 Yann LeCun의 2022년 기사 "A Path Towards Autonomous Machine Intelligence"가 나오기 전까지는 월드 모델(world models)에 대한 새로운 해석을 인지하지 못했습니다. 이는 본질적으로 LLM 대신 AI에 대한 대안적인 경로를 매핑하는 것에 관한 것이었습니다. 최근에는 월드 모델의 개념이 단순한 예측을 넘어, 복잡한 환경에서의 의사결정 및 제어 시스템 설계에까지 확장되고 있습니다.

**4.2 비전에서 코드로(From Vision to Code)**
그렇긴 하지만, 월드 모델(world model) 논문들은 모두 비전 도메인(vision domains)에 초점을 맞추었으며, 초기 VAE 및 RNN 기반 모델부터 트랜스포머(transformers), 확산 모델(diffusion models), 심지어 Mamba 레이어 하이브리드(Mamba-layer hybrids)에 이르기까지 광범위한 아키텍처(architectures)를 아울렀습니다. 이제 현재 LLM에 더 집중하고 있는 저로서는 "Code World Model" 논문(2025년 9월 30일)이 저의 완전한 주의를 사로잡은 첫 번째 논문입니다 (말장난 의도 없음). 이것은 (제가 아는 한) 텍스트에서 텍스트로 (또는 더 정확히는 코드에서 코드로) 매핑하는 첫 번째 월드 모델(world model)입니다. CWM은 320억 개의 매개변수를 가진 오픈 가중치 모델(open-weight model)로, 131k 토큰(token) 컨텍스트 윈도우(context window)를 가지고 있습니다. 아키텍처(architecture)적으로는 여전히 슬라이딩 윈도우 어텐션(sliding-window attention)을 가진 밀집 디코더 전용 트랜스포머(dense decoder-only Transformer)입니다. 또한 다른 LLM과 마찬가지로 사전 훈련(pre-training), 중간 훈련(mid-training), 지도 미세 조정(supervised fine-tuning, SFT), 강화 학습(reinforcement learning) 단계를 거치지만, 중간 훈련 데이터(mid-training data)가 월드 모델링 구성 요소(world-modeling component)를 도입합니다. 언어 모델이 단순히 텍스트를 생성하는 것을 넘어, 코드의 실행 흐름과 그로 인한 상태 변화를 이해하게 되는 것은 LLM 능력의 중요한 진전을 의미합니다.

**4.3 코드 월드 모델 대 일반 코드 LLM (Code World Models Vs Regular LLMs for Code)**
그렇다면 이것은 Qwen3-Coder와 같은 일반 코드 LLM과 어떻게 다를까요? Qwen3-Coder와 같은 일반 모델은 순전히 다음 토큰 예측(next-token prediction)으로 훈련됩니다. 이들은 구문(syntax)과 논리(logic)의 패턴을 학습하여 그럴듯한 코드 완성(code completions)을 생성하며, 이는 프로그래밍에 대한 정적인 텍스트 수준의 이해를 제공합니다. 대조적으로 CWM은 코드가 실행될 때 어떤 일이 일어나는지 시뮬레이션(simulate)하는 것을 학습합니다. 아래 그림과 같이 코드 한 줄을 수정하는 것과 같은 동작을 수행한 후, 변수(variable)의 값과 같은 결과적인 프로그램 상태(program state)를 예측하도록 훈련됩니다. 그림 19: Code World Model (CWM)에서의 코드 실행 추적(code execution tracing) 예시. 모델은 각 코드 라인이 실행됨에 따라 변수 상태(variable states)가 단계별로 어떻게 진화하는지 예측합니다. 여기서 모델은 코드의 동작을 효과적으로 시뮬레이션합니다. https://www.arxiv.org/abs/2510.02387에서 가져온 주석이 달린 그림. 추론(inference) 시 CWM은 GPT 스타일 모델과 마찬가지로 한 번에 하나의 토큰(token)을 생성하는 자기회귀 트랜스포머(autoregressive transformer)입니다. 주요 차이점은 이러한 토큰(tokens)이 일반 텍스트(plain text) 대신 구조화된 실행 추적(structured execution traces)을 인코딩(encode)할 수 있다는 것입니다. 그래서 저는 이것을 월드 모델(world model)이라고 부르기보다는 월드 모델 증강 LLM(world model-augmented LLM)이라고 부를 것입니다. 첫 시도치고는 놀랍도록 잘 작동하며, 대략 같은 크기에서 gpt-oss-20b (중간 추론 노력)와 동등합니다. 테스트 시간 스케일링(test-time-scaling)을 사용하면, 4배 더 작으면서도 gpt-oss-120b (높은 추론 노력)보다 약간 더 나은 성능을 보입니다. 그들의 테스트 시간 스케일링(test-time scaling)은 생성된 단위 테스트(unit tests)와 함께 best@k 절차(procedure)를 사용한다는 점에 유의하십시오 (고급 다수결 투표 방식(majority voting scheme)을 생각해보십시오). CWM과 gpt-oss가 다른 테스트 시간 스케일링 전략(test-time-scaling strategies) (best@k 대 추론 노력당 더 많은 토큰)을 사용하므로, CWM과 gpt-oss 간의 토큰/초 또는 해결 시간 비교를 보는 것은 흥미로웠을 것입니다. 그림 20: 코딩 벤치마크(coding benchmark, SWE-bench)에서 코드 월드 모델(code world model, CWM)과 다른 인기 있는 LLM의 성능 비교. https://www.arxiv.org/abs/2510.02387에서 가져온 주석이 달린 그림. 이러한 접근 방식은 자동화된 코드 생성, 디버깅 및 보안 취약점 분석 등 다양한 영역에서 혁신적인 가능성을 열어줍니다.

**5. 작은 재귀 트랜스포머(Small Recursive Transformers)**
여러분은 이전의 모든 접근 방식이 여전히 트랜스포머 아키텍처(transformer architecture)를 기반으로 한다는 것을 알아차렸을 것입니다. 이 마지막 섹션의 주제도 마찬가지이지만, 앞서 논의한 모델들과는 대조적으로, 이들은 추론(reasoning)을 위해 설계된 작고 전문화된 트랜스포머(transformers)입니다. 네, 추론 중심 아키텍처(reasoning-focused architectures)가 항상 클 필요는 없습니다. 사실, 계층적 추론 모델(Hierarchical Reasoning Model, HRM)과 함께 작은 재귀 트랜스포머(recursive transformers)에 대한 새로운 접근 방식이 최근 연구 커뮤니티에서 많은 주목을 받았습니다. 그림 21: LLM 환경 개요; 이 섹션은 작은 재귀 트랜스포머(small recursive transformers)를 다룹니다. 더 구체적으로, HRM 개발자들은 매우 작은 트랜스포머 모델(transformer models, 단 4개의 블록만 있는)조차도 답변을 단계별로 정제하도록 훈련될 때 (특정 문제에 대해) 인상적인 추론 능력(reasoning capabilities)을 개발할 수 있음을 보여주었습니다. 이는 ARC 챌린지에서 최고 순위를 차지했습니다. 그림 22: arcprize.org/arc-agi/1에서 가져온 ARC-AGI 1 작업 예시(상단)와 arcprize.org/blog/hrm-analysis에서 가져온 리더보드에 순위가 매겨진 계층적 추론 모델(Hierarchical Reasoning Model, HRM)(하단). HRM과 같은 재귀 모델(recursive models)의 아이디어는 한 번의 순방향 전달(forward pass)로 답변을 생성하는 대신, 모델이 재귀적인 방식(recursive fashion)으로 자체 출력을 반복적으로 정제한다는 것입니다. (이 과정의 일부로, 각 반복은 잠재 표현(latent representation)을 정제하며, 저자들은 이를 모델의 "사고" 또는 "추론" 과정으로 봅니다.) 첫 번째 주요 예시는 여름 초의 HRM이었고, 이어서 Mixture-of-Recursions (MoR) 논문이 나왔습니다. 그리고 가장 최근에는 "Less is More: Recursive Reasoning with Tiny Networks" (2025년 10월)가 Tiny Recursive Model (TRM, 아래 그림에 설명됨)을 제안하는데, 이는 ARC 벤치마크(benchmark)에서 훨씬 더 나은 성능을 보이는 더 간단하고 훨씬 작은 모델입니다 (700만 개의 매개변수, HRM보다 약 4배 작음). 그림 23: Tiny Recursive Model (TRM). https://arxiv.org/abs/2510.04871에서 가져온 주석이 달린 그림. 이 섹션의 나머지 부분에서는 TRM을 좀 더 자세히 살펴보겠습니다.

**5.1 여기서 재귀란 무엇을 의미하는가?(What Does Recursion Mean Here?)**
TRM은 두 가지 교대 업데이트(alternating updates)를 통해 답변을 정제합니다: 현재 질문과 답변에서 잠재 추론 상태(latent reasoning state)를 계산합니다. 그런 다음 해당 잠재 상태(latent state)를 기반으로 답변을 업데이트합니다. 훈련은 배치(batch)당 최대 16번의 정제 단계(refinement steps) 동안 실행됩니다. 각 단계는 답변을 반복적으로 정제하기 위해 여러 번의 no-grad 루프(loops)를 수행합니다. 이어서 전체 추론 시퀀스(reasoning sequence)를 통해 역전파(backpropagates)하여 모델 가중치(model weights)를 업데이트하는 경사 루프(gradient loop)가 진행됩니다. TRM이 텍스트를 처리하는 언어 모델(language model)이 아니라는 점에 유의하는 것이 중요합니다. 하지만 (a) 트랜스포머 기반 아키텍처(transformer-based architecture)이고, (b) 추론(reasoning)이 이제 LLM 연구의 핵심 초점이며 이 모델은 추론에 대한 분명히 다른 접근 방식을 나타내고, (c) 많은 독자들이 HRM을 다루어 달라고 요청했기 때문에 (그리고 TRM은 HRM의 더 발전된 후속 모델이므로) 여기에 포함하기로 결정했습니다. TRM은 미래에 텍스트 기반 질문-답변 작업(textual question-answer tasks)으로 확장될 수 있지만, 현재 TRM은 그리드 기반 입력(grid-based inputs) 및 출력(outputs)에서 작동합니다. 다시 말해, "질문"과 "답변" 모두 이산 토큰(discrete tokens)의 그리드(예: 9×9 스도쿠 또는 30×30 ARC/미로 퍼즐)이며, 텍스트 시퀀스(text sequences)가 아닙니다. 이러한 모델은 복잡한 논리 퍼즐이나 형식적 시스템에서 강력한 성능을 발휘할 수 있습니다.

**5.2 TRM은 HRM과 어떻게 다른가?(How Does TRM Differ From HRM?)**
HRM은 재귀 수준(recursion levels)을 통해 통신하는 두 개의 작은 트랜스포머 모듈(transformer modules, 각 4개 블록)로 구성됩니다. TRM은 단일 2계층 트랜스포머(2-layer transformer)만 사용합니다. (이전 TRM 그림에는 트랜스포머 블록(transformer block) 옆에 4×가 표시되어 있지만, 이는 HRM과 비교하기 쉽게 하기 위한 것일 가능성이 높다는 점에 유의하십시오.) TRM은 모든 재귀 단계(recursive steps)를 통해 역전파(backpropagates)하는 반면, HRM은 마지막 몇 단계만 역전파합니다. HRM은 반복을 언제 멈출지 결정하는 명시적인 정지 메커니즘(halting mechanism)을 포함합니다. TRM은 이 메커니즘을 반복을 언제 멈출지 학습하는 간단한 이진 교차 엔트로피 손실(binary cross-entropy loss)로 대체합니다. 성능 면에서 TRM은 아래 그림에서 보듯이 HRM에 비해 매우 우수한 성능을 보입니다. 그림 24: 계층적 추론 모델(Hierarchical Reasoning Model, HRM)과 Tiny Recursive Model (TRM)의 성능 비교. 이 논문은 놀라울 정도로 많은 절제 연구(ablation studies)를 포함했으며, 이는 몇 가지 흥미로운 추가 통찰력을 제공했습니다. 제게 특히 눈에 띄었던 두 가지는 다음과 같습니다:
*   레이어가 적을수록 더 나은 일반화(generalization)로 이어집니다.
    *   4개 레이어에서 2개 레이어로 줄이면 스도쿠 정확도(Sudoku accuracy)가 79.5%에서 87.4%로 향상되었습니다.
*   어텐션(Attention)은 필요하지 않습니다.
    *   자기 어텐션(self-attention)을 순수 MLP 레이어(MLP layer)로 대체하는 것도 정확도(accuracy)를 향상시켰습니다 (74.7%에서 87.4%로).
    *   하지만 컨텍스트(context)가 작고 고정된 길이기 때문에 여기에서만 가능합니다.
이러한 연구 결과는 트랜스포머 아키텍처의 핵심 요소로 여겨지던 어텐션 메커니즘의 역할에 대한 새로운 질문을 던지며, 특정 유형의 추론 작업에서는 더 간소화된 설계가 효율성과 성능 모두에 유리할 수 있음을 시사합니다.

**5.3 더 큰 그림(The Bigger Picture)**
HRM과 TRM이 이러한 벤치마크(benchmarks)에서 정말 좋은 추론 성능(reasoning performance)을 달성하지만, 이들을 대규모 LLM과 비교하는 것은 공정하지 않습니다. HRM과 TRM은 ARC, 스도쿠, 미로 길 찾기와 같은 작업을 위한 전문화된 모델인 반면, LLM은 제너럴리스트(generalists)입니다. 물론 HRM과 TRM도 다른 작업에 채택될 수 있지만, 각 작업에 대해 특별히 훈련되어야 합니다. 따라서 그런 의미에서 HRM과 TRM은 효율적인 포켓 계산기로 생각할 수 있고, LLM은 다른 많은 일도 할 수 있는 컴퓨터와 같다고 볼 수 있습니다. 그럼에도 불구하고, 이러한 재귀 아키텍처(recursive architectures)는 작고 효율적인 모델이 반복적인 자기 정제(iterative self-refinement)를 통해 어떻게 "추론"할 수 있는지를 보여주는 흥미로운 개념 증명(proof-of-concepts)입니다. 아마도 미래에는 이러한 모델이 더 큰 도구 사용 LLM 시스템(tool-using LLM systems) 내에 내장된 추론 또는 계획 모듈(reasoning or planning modules) 역할을 할 수 있을 것입니다. 현재로서는 LLM이 광범위한 작업에 이상적이지만, TRM과 같은 도메인별 재귀 모델(domain-specific recursive models)은 대상 도메인이 잘 이해되면 특정 문제를 더 효율적으로 해결하기 위해 개발될 수 있습니다. 스도쿠, 미로 찾기, ARC 개념 증명 벤치마크(proof-of-concept benchmarks) 외에도, 이러한 모델이 사용될 수 있는 물리학 및 생물학 도메인(physics and biology domain)에는 많은 사용 사례가 있을 수 있습니다. 흥미로운 사실로, 저자는 이 모델을 훈련하는 데 4개의 H100으로 약 2일 동안 500달러 미만이 들었다고 공유했습니다. 이는 작은 모델로도 혁신적인 연구를 수행할 수 있음을 보여주는 고무적인 사례입니다.

**6. 결론(Conclusion)**
원래는 개요 그림의 모든 모델 범주를 다룰 계획이었지만, 글이 예상보다 길어져서 xLSTM, Liquid Foundation Models, 트랜스포머-RNN 하이브리드(Transformer-RNN hybrids), 상태 공간 모델(State Space Models)은 다음 기회로 미루어야 할 것 같습니다 (비록 Gated DeltaNet이 이미 상태 공간 모델과 순환 설계(recurrent designs)의 맛을 보여주었지만요). 이 글의 결론으로, 저는 앞서 언급했던 말을 반복하고 싶습니다. 즉, 표준 자기회귀 트랜스포머 LLM(autoregressive transformer LLMs)은 검증되었고 지금까지 시간의 시험을 견뎌냈다는 것입니다. 또한 효율성이 주요 요인이 아니라면, 현재로서는 우리가 가진 최고의 것입니다.

**전통적인 디코더 스타일(Decoder-Style), 자기회귀 트랜스포머(Autoregressive Transformers)**
+ 검증되고 성숙한 도구
+ "잘 이해됨"
+ 스케일링 법칙(Scaling laws)
+ SOTA
- 비싼 훈련
- 비싼 추론 (앞서 언급된 트릭 제외)

오늘 새로운 LLM 기반 프로젝트를 시작한다면, 자기회귀 트랜스포머 기반 LLM(autoregressive transformer-based LLMs)이 저의 첫 번째 선택이 될 것입니다. 저는 다가오는 어텐션 하이브리드(attention hybrids)가 매우 유망하다고 생각하며, 효율성이 주요 관심사인 더 긴 컨텍스트(contexts)로 작업할 때 특히 흥미롭습니다.

**선형 어텐션 하이브리드(Linear Attention Hybrids)**
+ 디코더 스타일 트랜스포머와 동일
+ 긴 컨텍스트 작업에서 FLOPs/KV 메모리 절감
- 복잡성 증가
- 효율성을 위해 약간의 정확도 희생

더 극단적인 측면에서는 텍스트 확산 모델(text diffusion models)이 흥미로운 발전입니다. 저는 몇 가지 빠른 데모만 시도해 보았기 때문에, 일상적인 사용에서 얼마나 잘 작동할지에 대해서는 여전히 다소 회의적입니다. 바라건대, 곧 Google의 Gemini Diffusion을 통한 대규모 프로덕션 배포를 보게 될 것이며, 이를 일상 및 코딩 작업에서 테스트하여 사람들이 실제로 어떻게 느끼는지 알아낼 수 있기를 바랍니다.

**텍스트 확산 모델(Text Diffusion Models)**
+ 반복적인 노이즈 제거는 텍스트에 대한 신선한 아이디어
+ 더 나은 병렬 처리 (다음 토큰 의존성 없음)
- 답변 스트리밍 불가
- CoT의 이점 없음?
- 까다로운 도구 호출?
- 견고한 모델이지만 SOTA 아님

텍스트 확산 모델(text diffusion models)의 주요 강점이 효율성 개선인 반면, 코드 월드 모델(code world models)은 스펙트럼의 다른 끝에 위치하며, 모델링 성능(modeling performance) 향상을 목표로 합니다. 이 글을 쓰는 시점에서, 표준 LLM을 기반으로 하는 코딩 모델(coding models)은 주로 추론 기술(reasoning techniques)을 통해 개선되지만, 더 까다로운 문제에 시도해 보았다면, 이들이 (다소) 여전히 부족하고 많은 까다로운 코딩 문제를 잘 해결하지 못한다는 것을 알아차렸을 것입니다. 저는 코드 월드 모델(code world models)이 특히 흥미롭다고 생각하며, 더 유능한 코딩 시스템(coding systems)을 개발하는 데 중요한 다음 단계가 될 수 있다고 믿습니다.

**코드 월드 모델(Code World Model)**
+ 코드 이해도 향상을 위한 유망한 접근 방식
+ 검증 가능한 중간 상태
- 실행 가능한 코드 추적 포함으로 훈련 복잡성 증가
- 코드 실행으로 지연 시간 추가

마지막으로, 계층적 및 작은 추론 모델(tiny reasoning models)과 같은 작은 재귀 트랜스포머(recursive transformers)를 다루었습니다. 이들은 매우 흥미로운 개념 증명(proof-of-concept) 모델입니다. 하지만 오늘날 이들은 주로 퍼즐 해결사이며, 일반 텍스트 또는 코딩 모델(coding models)이 아닙니다. 따라서 이 글에서 다룬 다른 비표준 LLM 대안들과 같은 범주에 속하지 않습니다. 그럼에도 불구하고, 이들은 매우 흥미로운 개념 증명이며, 연구자들이 이들을 연구하고 있다는 사실에 기쁩니다. 현재 GPT-5, DeepSeek R1, Kimi K2 등과 같은 LLM은 자유 형식 텍스트, 코드, 수학 문제 등을 위한 특수 목적 모델(special purpose models)로 개발되고 있습니다. 이들은 일반 지식 질문부터 수학 및 코드에 이르기까지 다양한 작업에 사용하는 무차별 대입(brute-force) 및 만능 접근 방식(jack-of-all-trades approach)처럼 느껴집니다. 하지만 동일한 작업을 반복적으로 수행할 때, 이러한 무차별 대입 접근 방식은 비효율적이 되고 전문화(specialization) 측면에서도 이상적이지 않을 수 있습니다. 여기서 작은 재귀 트랜스포머(tiny recursive transformers)가 흥미로워집니다: 이들은 반복적이거나 구조화된 추론 작업(reasoning tasks)에 효율적이고 목적에 맞게 구축된 경량의 작업별 모델(task-specific models) 역할을 할 수 있습니다. 또한, 저는 이들을 다른 도구 호출 LLM(tool-calling LLMs)을 위한 잠재적인 "도구"로 볼 수 있습니다. 예를 들어, LLM이 수학 문제를 해결하기 위해 Python 또는 계산기 API를 사용할 때, 특수 작은 추론 모델(tiny reasoning models)이 다른 유형의 퍼즐 또는 추론과 유사한 문제에 대한 이러한 틈새를 채울 수 있습니다.

**작은 재귀 트랜스포머(Small Recursive Transformers)**
+ 매우 작은 아키텍처
+ 퍼즐에 대한 좋은 일반화
- 특수 목적 모델
- (현재까지는) 퍼즐에 한정됨

긴 글이었지만, 주류 LLM의 스포트라이트 밖에서 종종 머무는 매혹적인 접근 방식들을 발견하셨기를 바랍니다. 그리고 다소 전통적인 LLM 출시로 인해 약간 지루함을 느끼셨다면, 지금 많은 흥미로운 작업이 진행되고 있으므로 이 글이 AI에 대한 여러분의 흥미를 다시 불러일으키는 데 도움이 되었기를 바랍니다! 이 잡지는 개인적인 열정 프로젝트이며, 여러분의 지원이 이를 유지하는 데 도움이 됩니다. 제 작업을 지원하고 싶으시다면, 저의 "Build a Large Language Model (From Scratch)" 책 또는 그 후속작인 "Build a Reasoning Model (From Scratch)"을 고려해 주십시오. (저는 여러분이 이 책들에서 많은 것을 얻을 것이라고 확신합니다. 이 책들은 다른 곳에서는 찾을 수 없는 깊이로 LLM이 어떻게 작동하는지 설명합니다.) 읽어주셔서 감사하며, 독립적인 연구를 지원해 주셔서 감사합니다! "Build a Large Language Model (From Scratch)"은 현재 Amazon에서 구매 가능합니다. "Build a Reasoning Model (From Scratch)"은 Manning에서 얼리 액세스(Early Access) 중입니다. 책을 읽으셨고 잠시 시간이 있으시다면, 짧은 리뷰를 남겨주시면 정말 감사하겠습니다. 이는 저희 저자들에게 큰 도움이 됩니다! 여러분의 지원은 큰 의미가 있습니다! 감사합니다!