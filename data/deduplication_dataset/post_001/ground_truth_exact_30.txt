DeepSeek R1부터 MiniMax-M2에 이르기까지, 오늘날 가장 크고 유능한 오픈 가중치 LLM(Large Language Model)은 여전히 원본 멀티 헤드 어텐션 메커니즘(multi-head attention mechanism)의 변형을 기반으로 하는 자기회귀 디코더 스타일 트랜스포머(autoregressive decoder-style transformers)입니다. 하지만 최근 몇 년 동안 텍스트 확산 모델(text diffusion models)부터 최신 선형 어텐션 하이브리드 아키텍처(linear attention hybrid architectures)에 이르기까지 표준 LLM의 대안들이 등장하는 것을 보았습니다. 이들 중 일부는 더 나은 효율성을 목표로 하고, 코드 월드 모델(code world models)과 같은 다른 모델들은 모델링 성능 향상을 목표로 합니다. 몇 달 전 주요 트랜스포머 기반 LLM에 초점을 맞춘 저의 "The Big LLM Architecture Comparison"을 공유한 후, 대안적인 접근 방식에 대해 어떻게 생각하는지에 대한 많은 질문을 받았습니다. (저는 최근 PyTorch Conference 2025에서 이에 대한 짧은 강연을 했으며, 참석자들에게 이러한 대안적인 접근 방식에 대한 글을 후속으로 제공하겠다고 약속했습니다.) 그래서 여기 있습니다! 그림 1: LLM 환경 개요. 이 글은 검은색 프레임으로 둘러싸인 아키텍처를 다룹니다. 디코더 스타일 트랜스포머는 저의 "The Big Architecture Comparison" 글에서 다루었습니다. 프레임이 없는 다른 아키텍처는 향후 글에서 다루어질 수 있습니다. 이상적으로는 위 그림에 표시된 각 주제가 최소한 하나의 전체 글을 할애할 가치가 있다는 점에 유의하십시오(그리고 바라건대 미래에 그렇게 될 것입니다). 따라서 이 글을 적절한 길이로 유지하기 위해 많은 섹션이 합리적으로 짧습니다. 하지만 이 글이 최근 몇 년 동안 등장한 모든 흥미로운 LLM 대안에 대한 소개로서 여전히 유용하기를 바랍니다. 추신: 앞서 언급된 PyTorch 컨퍼런스 강연은 공식 PyTorch YouTube 채널에 업로드될 예정입니다. 그동안 궁금하시다면 아래에서 연습 녹화 버전을 찾을 수 있습니다. (여기에도 YouTube 버전이 있습니다.)

이 글에서는 표준 LLM 아키텍처의 혁신과 함께 효율성, 추론 능력, 그리고 새로운 패러다임을 제시하는 다양한 대안들을 심층적으로 탐구합니다. 특히, 기존 트랜스포머의 한계를 극복하려는 시도와 새로운 AI 모델링 접근 방식에 초점을 맞춰 최근 연구 동향을 분석하고, 각 아키텍처의 핵심 아이디어와 특징을 상세히 설명합니다.

**1. 트랜스포머 기반 LLM(Transformer-Based LLMs)**
고전적인 "Attention Is All You Need" 아키텍처를 기반으로 하는 트랜스포머 기반 LLM은 텍스트와 코드 전반에 걸쳐 여전히 최첨단(state-of-the-art)입니다. 2024년 말부터 현재까지의 주요 모델들을 살펴보면, DeepSeek V3/R1, OLMo 2, Gemma 3, Mistral Small 3.1, Llama 4, Qwen3, SmolLM3, Kimi K2, gpt-oss, GLM-4.5, GLM-4.6, MiniMax-M2 등이 있습니다. (위 목록은 오픈 가중치 모델에 초점을 맞추고 있습니다. GPT-5, Grok 4, Gemini 2.5 등과 같은 독점 모델도 이 범주에 속합니다.) 그림 2: 지난 한 해 동안 출시된 가장 주목할 만한 디코더 스타일 트랜스포머의 개요. 제가 트랜스포머 기반 LLM에 대해 여러 번 이야기하고 글을 썼기 때문에, 여러분은 그 전반적인 아이디어와 아키텍처에 익숙할 것이라고 생각합니다. 더 깊이 다루고 싶다면, 저의 "The Big LLM Architecture Comparison" 글에서 위에 나열된(그리고 아래 그림에 표시된) 아키텍처들을 비교했습니다. (참고: Qwen3-Next와 Kimi Linear를 개요 그림의 다른 트랜스포머-상태 공간 모델(SSM) 하이브리드와 함께 묶을 수도 있었습니다. 개인적으로 저는 이 다른 트랜스포머-SSM 하이브리드를 트랜스포머 구성 요소를 가진 SSM으로 보는 반면, 여기서 논의된 모델(Qwen3-Next 및 Kimi Linear)은 SSM 구성 요소를 가진 트랜스포머로 봅니다. 하지만 IBM Granite 4.0과 NVIDIA Nemotron Nano 2를 트랜스포머-SSM 상자에 나열했기 때문에, 이들을 단일 범주에 넣어야 한다는 주장이 있을 수 있습니다.) 그림 3. 저의 "The Big Architecture Comparison" (https://magazine.sebastianraschka.com/p/the-big-llm-architecture-comparison) 글에서 논의된 아키텍처의 하위 집합. LLM을 사용하거나 개발하고 있다면, 예를 들어 애플리케이션을 구축하거나, 모델을 미세 조정하거나, 새로운 알고리즘을 시도하는 경우, 저는 이 모델들을 주로 사용할 것입니다. 이들은 테스트되고, 검증되었으며, 성능이 우수합니다. 또한 "The Big Architecture Comparison" 글에서 논의했듯이, 그룹화된 쿼리 어텐션(grouped-query attention), 슬라이딩 윈도우 어텐션(sliding-window attention), 멀티 헤드 잠재 어텐션(multi-head latent attention) 등을 포함한 많은 효율성 개선 사항이 있습니다. 하지만 연구원과 엔지니어가 대안을 시도하는 작업을 하지 않는다면 지루하고 (근시안적일) 것입니다. 따라서 나머지 섹션에서는 최근 몇 년 동안 등장한 흥미로운 대안들을 다룰 것입니다.

최근 트랜스포머 기반 LLM의 발전은 단순히 매개변수 수를 늘리는 것을 넘어, 아키텍처의 세부적인 최적화에 집중하고 있습니다. 예를 들어, FlashAttention 2와 같은 기술은 어텐션 계산의 메모리 접근 패턴을 최적화하여 훈련 및 추론 속도를 획기적으로 향상시켰습니다. 이는 GPU의 SRAM(Static Random-Access Memory) 활용을 극대화하여 HBM(High Bandwidth Memory) 접근을 줄임으로써 이루어집니다. 또한, 위치 임베딩(positional embedding) 분야에서는 RoPE(Rotary Positional Embedding)가 다양한 컨텍스트 길이에서 강력한 성능을 보여주며 표준으로 자리 잡았고, 그 변형들이 지속적으로 연구되고 있습니다. 특히, 긴 컨텍스트 처리를 위해 RoPE 스케일링 기법(scaling techniques)이 중요하게 다루어지며, 이는 모델이 수십만 토큰에 이르는 긴 문맥을 효과적으로 이해하고 생성하는 데 필수적입니다. MoE(Mixture-of-Experts) 아키텍처는 효율성과 성능을 동시에 잡으려는 트랜스포머 모델의 중요한 트렌드 중 하나입니다. GPT-4와 Mistral 7B 등 여러 최신 모델에서 MoE가 성공적으로 적용되었으며, 이는 모델의 전체 매개변수 수는 크지만, 특정 입력에 대해 활성화되는 매개변수는 일부에 불과하여 추론 비용을 절감하면서도 강력한 성능을 유지할 수 있게 합니다. 이처럼 트랜스포머 아키텍처는 기본 골격을 유지하면서도, 내부 메커니즘과 구성 요소의 지속적인 혁신을 통해 LLM의 한계를 확장하고 있습니다.

**2. (선형) 어텐션 하이브리드((Linear) Attention Hybrids)**
"더 다른" 접근 방식들을 논의하기 전에, 먼저 더 효율적인 어텐션 메커니즘(attention mechanisms)을 채택한 트랜스포머 기반 LLM들을 살펴보겠습니다. 특히, 입력 토큰(input tokens) 수에 따라 이차적으로(quadratically)가 아닌 선형적으로(linearly) 확장되는 모델에 초점을 맞춥니다. 최근 LLM의 효율성을 개선하기 위해 선형 어텐션 메커니즘(linear attention mechanisms)이 다시 주목받고 있습니다. "Attention Is All You Need" 논문(2017)에서 소개된 어텐션 메커니즘, 즉 스케일드 닷 프로덕트 어텐션(scaled-dot-product attention)은 오늘날 LLM에서 가장 인기 있는 어텐션 변형으로 남아 있습니다. 전통적인 멀티 헤드 어텐션(multi-head attention) 외에도, 제 강연에서 논의했듯이 그룹화된 쿼리 어텐션(grouped-query attention), 슬라이딩 윈도우 어텐션(sliding-window attention), 멀티 헤드 잠재 어텐션(multi-head latent attention)과 같은 더 효율적인 변형에도 사용됩니다.

**2.1 전통적인 어텐션과 이차적 비용(Traditional Attention and Quadratic Costs)**
원본 어텐션 메커니즘은 시퀀스 길이(sequence length)에 따라 이차적으로 확장됩니다:
\(\text{Attention}(Q, K, V) = \text{softmax}\!\left(\frac{QK^\top}{\sqrt{d}}\right)V\)
이는 쿼리(Q), 키(K), 값(V)이 n x d 행렬이기 때문입니다. 여기서 d는 임베딩 차원(embedding dimension, 하이퍼파라미터)이고 n은 시퀀스 길이(즉, 토큰(tokens)의 수)입니다. (더 자세한 내용은 저의 "Understanding and Coding Self-Attention, Multi-Head Attention, Causal-Attention, and Cross-Attention in LLMs" 글에서 찾을 수 있습니다.) 그림 4: 멀티 헤드 어텐션(multi-head attention)에서 전통적인 스케일드 닷 프로덕트 어텐션 메커니즘(scaled-dot-product attention mechanism)의 설명; 시퀀스 길이 n으로 인한 어텐션의 이차적 비용.

**2.2 선형 어텐션(Linear attention)**
선형 어텐션(linear attention) 변형은 오랫동안 존재해 왔으며, 저는 2020년대에 수많은 논문을 본 기억이 있습니다. 예를 들어, 제가 기억하는 가장 초기의 것 중 하나는 2020년 논문 "Transformers are RNNs: Fast Autoregressive Transformers with Linear Attention"으로, 연구원들은 어텐션 메커니즘을 다음과 같이 근사했습니다:
\(\text{Attention}(Q, K, V) = \text{softmax}\!\left(\frac{QK^\top}{\sqrt{d}}\right)V \approx \phi(Q)\big(\phi(K)^\top V\big)\)
여기서 ϕ(⋅)는 커널 특징 함수(kernel feature function)이며, ϕ(x) = elu(x)+1로 설정됩니다. 이 근사는 n×n 어텐션 행렬(attention matrix) QK T를 명시적으로 계산하는 것을 피하기 때문에 효율적입니다. 저는 이러한 오래된 시도에 너무 오래 머물고 싶지 않습니다. 하지만 핵심은 긴 시퀀스(long sequences)에 대해 어텐션을 훨씬 더 효율적으로 만들기 위해 시간 및 메모리 복잡도(time and memory complexity)를 O(n 2 )에서 O(n)으로 줄였다는 것입니다. 하지만 이들은 모델 정확도(model accuracy)를 저하시켰기 때문에 실제로 인기를 얻지 못했으며, 저는 이러한 변형 중 하나가 오픈 가중치 최첨단 LLM에 적용된 것을 본 적이 없습니다.

초기 선형 어텐션 방식들은 주로 어텐션 행렬을 근사하거나 희소화하는 데 중점을 두었습니다. 하지만 이러한 접근 방식은 종종 모델의 표현 능력(expressiveness)을 제한하고, 중요한 컨텍스트 정보를 손실시켜 정확도 하락으로 이어졌습니다. 특히, 긴 시퀀스에서 미묘한 종속성(subtle dependencies)을 포착하는 데 어려움을 겪었습니다. 이러한 한계 때문에 당시에는 실제 LLM 배포에 널리 채택되지 못했습니다. 그러나 최근 연구에서는 커널 함수(kernel functions)의 개선, 학습 가능한 매개변수 도입, 그리고 하이브리드 접근 방식을 통해 이러한 문제를 극복하려는 노력이 활발합니다. 예를 들어, 특정 커널 함수는 어텐션 메커니즘의 비선형성을 보존하면서도 선형적인 복잡도를 유지하도록 설계되어, 모델이 더 풍부한 특징을 학습할 수 있도록 돕습니다. 또한, 선형 어텐션은 각 토큰을 처리할 때 이전 토큰들의 정보를 고정된 크기의 상태 벡터(state vector)에 요약하여 저장하므로, 시퀀스 길이에 관계없이 일정한 메모리 사용량을 유지할 수 있다는 장점이 있습니다. 이는 특히 스트리밍(streaming) 환경이나 매우 긴 컨텍스트를 처리해야 하는 애플리케이션에서 매우 유리합니다.

**2.3 선형 어텐션의 부활(Linear Attention Revival)**
올해 하반기에는 선형 어텐션(linear attention) 변형이 다시 주목받았으며, 아래 그림에 설명된 대로 일부 모델 개발자들 사이에서 약간의 논쟁이 있었습니다. 그림 5: 선형 어텐션 하이브리드 아키텍처(linear attention hybrid architectures)의 개요. 첫 번째 주목할 만한 모델은 라이트닝 어텐션(lightning attention)을 사용한 MiniMax-M1이었습니다. MiniMax-M1은 4560억 개의 매개변수를 가진 MoE(mixture-of-experts) 모델로, 460억 개의 활성 매개변수를 가지며 지난 6월에 출시되었습니다. 그리고 8월에는 Qwen3 팀이 Qwen3-Next를 발표했는데, 이에 대해서는 위에서 더 자세히 논의했습니다. 그리고 9월에는 DeepSeek 팀이 DeepSeek V3.2를 발표했습니다. (DeepSeek V3.2의 희소 어텐션 메커니즘(sparse attention mechanism)은 엄밀히 말해 선형은 아니지만, 계산 비용(computational costs) 측면에서 최소한 이차 미만(subquadratic)이므로 MiniMax-M1, Qwen3-Next, Kimi Linear와 같은 범주에 넣는 것이 타당하다고 생각합니다.) 이 세 모델(MiniMax-M1, Qwen3-Next, DeepSeek V3.2)은 대부분 또는 모든 레이어에서 전통적인 이차 어텐션(quadratic attention) 변형을 효율적인 선형 변형으로 대체합니다. 흥미롭게도 최근 반전이 있었는데, MiniMax 팀은 선형 어텐션 없이 새로운 2300억 개의 매개변수를 가진 M2 모델을 출시하며 일반 어텐션으로 돌아갔습니다. 팀은 선형 어텐션이 프로덕션 LLM에서 까다롭다고 밝혔습니다. 일반적인 프롬프트(prompts)에서는 잘 작동하는 것처럼 보였지만, 추론(reasoning) 및 다중 턴 작업(multi-turn tasks)에서 정확도가 낮았는데, 이는 일반적인 채팅 세션뿐만 아니라 에이전트 애플리케이션(agentic applications)에도 중요합니다. 이는 선형 어텐션이 결국 추구할 가치가 없을 수도 있다는 전환점이 될 수 있었습니다. 하지만 더 흥미로운 점이 있습니다. 10월에 Kimi 팀은 선형 어텐션(linear attention)을 사용하는 새로운 Kimi Linear 모델을 출시했습니다. 이 선형 어텐션 측면에서 Qwen3-Next와 Kimi Linear는 모두 Gated DeltaNet을 채택했는데, 이는 다음 몇 섹션에서 하이브리드 어텐션 아키텍처(hybrid attention architecture)의 한 예시로 논의하고 싶었습니다.

MiniMax-M2의 사례는 선형 어텐션이 단순히 효율성만을 추구하는 것을 넘어, 복잡한 추론 능력과 같은 핵심 LLM 성능을 유지하는 것이 얼마나 어려운지를 보여줍니다. 이들의 피드백은 선형 어텐션의 한계, 즉 긴 범위의 미묘한 의존성(subtle dependencies)을 포착하는 데 필요한 전역적인 정보 흐름(global information flow)을 충분히 제공하지 못할 수 있다는 점을 시사합니다. 이러한 이유로 최근에는 단순히 선형 어텐션으로 완전히 전환하기보다는, 전통적인 어텐션과 선형 어텐션을 결합하는 하이브리드 방식이 더욱 각광받고 있습니다. Gated DeltaNet과 같은 아키텍처는 이러한 하이브리드 접근 방식의 대표적인 예시로, 상태 공간 모델(State Space Model, SSM)의 아이디어를 차용하여 효율적인 순환 메커니즘과 어텐션의 표현력을 결합하려 합니다. SSM은 긴 시퀀스 데이터에 대한 선형 시간 복잡도를 가지면서도, 어텐션 메커니즘의 장점인 전역 컨텍스트 모델링 능력을 부분적으로 유지하려는 시도로 주목받고 있습니다. Mamba와 같은 SSM 기반 모델들은 이러한 방향에서 중요한 진전을 보여주었으며, 선형 어텐션 하이브리드의 미래는 SSM과의 융합을 통해 더욱 발전할 것으로 예상됩니다.

**2.4 Qwen3-Next**
Qwen3-Next부터 시작하겠습니다. 이 모델은 일반 어텐션 메커니즘(attention mechanism)을 Gated DeltaNet + Gated Attention 하이브리드(hybrid)로 대체했는데, 이는 메모리 사용량 측면에서 기본 262k 토큰 컨텍스트 길이(context length)를 가능하게 합니다 (이전 235B-A22B 모델은 기본적으로 32k를 지원했으며, YaRN 스케일링(scaling)을 통해 131k를 지원했습니다). 이들의 하이브리드 메커니즘(hybrid mechanism)은 아래 그림과 같이 Gated DeltaNet 블록(blocks)과 Gated Attention 블록을 3:1 비율로 혼합합니다. 그림 6: 게이티드 어텐션(gated attention) 및 Gated DeltaNet을 사용한 Qwen3-Next. 위 그림에서 묘사된 바와 같이, 어텐션 메커니즘(attention mechanism)은 게이티드 어텐션(gated attention) 또는 Gated DeltaNet으로 구현됩니다. 이는 이 아키텍처(architecture)의 48개 트랜스포머 블록(transformer blocks, 레이어)이 이 둘 사이를 번갈아 사용한다는 것을 의미합니다. 특히, 앞서 언급했듯이 3:1 비율로 번갈아 사용됩니다. 예를 들어, 트랜스포머 블록은 다음과 같습니다:
──────────────────────────────────
Layer 1 : Linear attention → MoE
Layer 2 : Linear attention → MoE
Layer 3 : Linear attention → MoE
Layer 4 : Full attention → MoE
──────────────────────────────────
Layer 5 : Linear attention → MoE
Layer 6 : Linear attention → MoE
Layer 7 : Linear attention → MoE
Layer 8 : Full attention → MoE
──────────────────────────────────
...
그 외에는 아키텍처가 상당히 표준적이며 Qwen3와 유사합니다: 그림 7: 이전 "일반" Qwen3 모델(왼쪽)과 Qwen3-Next(오른쪽). 그렇다면 게이티드 어텐션(gated attention)과 Gated DeltaNet은 무엇일까요?

Qwen3-Next에서 3:1 비율의 하이브리드 전략은 효율성과 성능 사이의 균형점을 찾는 중요한 시도입니다. Gated DeltaNet은 선형적인 복잡도로 긴 컨텍스트를 처리하며 메모리 효율성을 제공하지만, 전역적인 상호작용을 모델링하는 데는 한계가 있습니다. 반면, Full Attention은 계산 비용이 높지만, 모든 토큰 간의 복잡한 관계를 효과적으로 포착할 수 있습니다. 3개의 선형 어텐션 레이어와 1개의 풀 어텐션 레이어를 번갈아 사용하는 것은, 대부분의 경우 효율적인 Gated DeltaNet으로 충분히 정보를 처리하고, 주기적으로 Full Attention 레이어를 통해 전역적인 컨텍스트를 "재동기화(resynchronize)"하여 정보 손실을 최소화하려는 의도로 해석될 수 있습니다. 이러한 구조는 특히 MoE와 결합될 때 더욱 강력해집니다. MoE는 각 토큰이 특정 전문가(expert) 네트워크에 의해 처리되도록 하여, 모델이 다양한 유형의 정보나 작업에 유연하게 대응할 수 있게 합니다. Qwen3-Next는 이 두 가지 기술을 결합하여, 방대한 컨텍스트를 효율적으로 처리하면서도 복잡한 언어 모델링 작업을 위한 표현력을 유지하려 합니다. 이러한 접근 방식은 훈련 안정성을 높이고, 추론 시 발생하는 지연 시간(latency)을 줄이는 데 기여할 수 있습니다.

**2.5 게이티드 어텐션(Gated Attention)**
Gated DeltaNet 자체에 대해 알아보기 전에, 게이트(gate)에 대해 간략히 이야기해 봅시다. 이전 그림의 Qwen3-Next 아키텍처(architecture) 상단에서 볼 수 있듯이, Qwen3-Next는 "게이티드 어텐션(gated attention)"을 사용합니다. 이는 본질적으로 추가적인 시그모이드 게이트(sigmoid gate)가 있는 일반적인 전체 어텐션(full attention)입니다. 이 게이팅(gating)은 설명 목적으로 아래에 있는 MultiHeadAttention 구현(저의 "LLMs from Scratch" 책 3장의 코드를 기반으로 함)에 제가 추가한 간단한 수정 사항입니다:
```python
class MultiHeadAttention(nn.Module):
    def __init__(self, d_in, d_out, block_size, dropout, num_heads, qkv_bias=False):
        super().__init__()
        self.d_out = d_out
        self.num_heads = num_heads
        self.head_dim = d_out // num_heads # Reduce d_out to head_dim
        self.W_query = nn.Linear(d_in, d_out, bias=qkv_bias)
        self.W_key = nn.Linear(d_in, d_out, bias=qkv_bias)
        self.W_value = nn.Linear(d_in, d_out, bias=qkv_bias)
        self.out_proj = nn.Linear(d_out, d_out)
        self.dropout = nn.Dropout(dropout)
        self.register_buffer('mask', torch.triu(torch.ones(block_size, block_size), diagonal=1))

        # Gating mechanism
        self.gate_proj = nn.Linear(d_in, d_out) # Projects input to gating signal

    def forward(self, x, mask=None):
        b, num_tokens, d_in = x.shape

        keys = self.W_key(x)
        queries = self.W_query(x)
        values = self.W_value(x)

        keys = keys.view(b, num_tokens, self.num_heads, self.head_dim)
        values = values.view(b, num_tokens, self.num_heads, self.head_dim)
        queries = queries.view(b, num_tokens, self.num_heads, self.head_dim)

        keys = keys.transpose(1, 2)
        queries = queries.transpose(1, 2)
        values = values.transpose(1, 2)

        attn_scores = queries @ keys.transpose(2, 3)
        attn_scores = attn_scores / math.sqrt(self.head_dim)

        if mask is not None:
            attn_scores.masked_fill_(mask.bool(), -torch.inf)

        attn_weights = torch.softmax(attn_scores, dim=-1)
        attn_weights = self.dropout(attn_weights)

        context_vec = (attn_weights @ values).transpose(1, 2).contiguous().view(b, num_tokens, self.d_out)

        # Apply gating
        gate_signal = torch.sigmoid(self.gate_proj(x))
        gated_context_vec = context_vec * gate_signal

        return self.out_proj(gated_context_vec)
```
보시다시피, 평소처럼 어텐션(attention)을 계산한 후, 모델은 동일한 입력에서 별도의 게이팅 신호(gating signal)를 사용하고, 이를 0과 1 사이로 유지하기 위해 시그모이드(sigmoid)를 적용한 다음, 어텐션 출력(attention output)과 곱합니다. 이를 통해 모델은 특정 특징(features)을 동적으로 확대하거나 축소할 수 있습니다. Qwen3-Next 개발자들은 이것이 훈련 안정성(training stability)에 도움이 된다고 말합니다:
[...] 어텐션 출력 게이팅 메커니즘(attention output gating mechanism)은 어텐션 싱크(Attention Sink) 및 대규모 활성화(Massive Activation)와 같은 문제를 제거하여 모델 전반의 수치적 안정성(numerical stability)을 보장합니다.
요컨대, 게이티드 어텐션(gated attention)은 표준 어텐션(standard attention)의 출력을 조절합니다. 다음 섹션에서는 어텐션 메커니즘(attention mechanism) 자체를 순환 델타 규칙 메모리 업데이트(recurrent delta-rule memory update)로 대체하는 Gated DeltaNet에 대해 논의합니다.

게이팅 메커니즘은 신경망에서 정보 흐름을 제어하는 강력한 도구로, 특히 순환 신경망(RNN)에서 장기 의존성(long-term dependencies) 문제를 해결하기 위해 LSTM(Long Short-Term Memory) 및 GRU(Gated Recurrent Unit)와 같은 구조에서 널리 사용되었습니다. 게이티드 어텐션은 이러한 아이디어를 트랜스포머의 어텐션 메커니즘에 적용하여, 어텐션의 출력에 대한 동적인 제어권을 부여합니다. 시그모이드(sigmoid) 활성화 함수를 통해 생성된 게이팅 신호는 0과 1 사이의 값을 가지므로, 어텐션 출력이 얼마나 다음 레이어로 전달될지를 정밀하게 조절할 수 있습니다. 이는 특정 정보가 중요할 때는 강조하고, 불필요하거나 노이즈가 많은 정보는 억제함으로써 모델의 학습 효율성과 안정성을 향상시킵니다. 특히, "어텐션 싱크(Attention Sink)"와 같은 문제는 긴 시퀀스에서 일부 토큰이 과도하게 높은 어텐션 가중치를 받아 정보가 집중되고 다른 중요한 정보가 무시되는 현상을 의미합니다. 게이팅은 이러한 현상을 완화하여 정보 분포를 균형 있게 만들고, 모델이 더 견고하게 학습할 수 있도록 돕습니다.

**2.6 Gated DeltaNet**
이제 Gated DeltaNet은 무엇일까요? Gated DeltaNet (Gated Delta Network의 약자)은 Qwen3-Next의 선형 어텐션 레이어(linear-attention layer)로, 표준 소프트맥스 어텐션(softmax attention)의 대안으로 고안되었습니다. 앞서 언급했듯이 "Gated Delta Networks: Improving Mamba2 with Delta Rule" 논문에서 채택되었습니다. Gated DeltaNet은 원래 Mamba2의 개선된 버전으로 제안되었으며, Mamba2의 게이티드 감쇠 메커니즘(gated decay mechanism)과 델타 규칙(delta rule)을 결합합니다. Mamba는 상태 공간 모델(state-space model, 트랜스포머의 대안)이며, 미래에 별도로 다룰 가치가 있는 큰 주제입니다. 델타 규칙(delta rule) 부분은 새로운 값과 예측된 값 사이의 차이(델타, Δ)를 계산하여 메모리 상태(memory state)로 사용되는 은닉 상태(hidden state)를 업데이트하는 것을 의미합니다(이에 대해서는 나중에 자세히 설명합니다). (참고: 고전적인 기계 학습 문헌에 익숙한 독자들은 이를 생물학에서 영감을 받은 헵 학습(Hebbian learning)과 유사하다고 생각할 수 있습니다: "함께 발화하는 세포는 함께 연결된다." 이는 기본적으로 퍼셉트론 업데이트 규칙(perceptron update rule) 및 경사 하강 기반 학습(gradient descent-based learning)의 전신이지만, 감독(supervision)이 없습니다.) Gated DeltaNet은 앞서 논의된 게이티드 어텐션(gated attention)의 게이트와 유사한 게이트를 가지고 있지만, 아래 그림과 같이 로지스틱 시그모이드 활성화(logistic sigmoid activation) 대신 SiLU를 사용합니다. (SiLU 선택은 표준 시그모이드(sigmoid)에 비해 경사 흐름(gradient flow)과 안정성(stability)을 개선하기 위한 것으로 보입니다.) 그림 8: 게이티드 어텐션(gated attention)과 Gated DeltaNet 비교. 하지만 위 그림에서 보듯이, 출력 게이트(output gate) 외에도 Gated DeltaNet의 "게이티드(gated)"는 몇 가지 추가 게이트를 의미합니다: α (감쇠 게이트, decay gate)는 시간이 지남에 따라 메모리(memory)가 얼마나 빨리 감쇠하거나 재설정되는지를 제어하고, β (업데이트 게이트, update gate)는 새로운 입력이 상태(state)를 얼마나 강력하게 수정하는지를 제어합니다. 코드에서 위에서 묘사된 Gated DeltaNet의 단순화된 버전(컨볼루션 혼합(convolutional mixing) 없음)은 다음과 같이 구현될 수 있습니다 (이 코드는 Qwen3 팀의 공식 구현에서 영감을 받았습니다):
```python
import torch
import torch.nn as nn
import torch.nn.functional as F

class GatedDeltaNet(nn.Module):
    def __init__(self, d_model, num_heads, head_dim, dropout=0.1):
        super().__init__()
        self.d_model = d_model
        self.num_heads = num_heads
        self.head_dim = head_dim
        
        # Linear projections for input x
        self.proj_x = nn.Linear(d_model, num_heads * head_dim)
        
        # Projections for gates alpha, beta, and output gate
        self.proj_alpha = nn.Linear(d_model, num_heads * head_dim)
        self.proj_beta = nn.Linear(d_model, num_heads * head_dim)
        self.proj_output_gate = nn.Linear(d_model, num_heads * head_dim)
        
        self.out_proj = nn.Linear(num_heads * head_dim, d_model)
        self.dropout = nn.Dropout(dropout)

    def forward(self, x, state=None):
        b, n, d_model = x.shape # batch_size, sequence_length, d_model

        # Initialize state if not provided (for the first token)
        if state is None:
            # S is the recurrent memory state, initialized to zeros
            # Shape: (b, num_heads, head_dim, head_dim)
            state = x.new_zeros(b, self.num_heads, self.head_dim, self.head_dim)

        output_sequence = []
        for t in range(n): # Process tokens one by one
            x_t = x[:, t, :] # Current token input (b, d_model)

            # Project current input for value and gates
            v_t = self.proj_x(x_t).view(b, self.num_heads, self.head_dim) # (b, num_heads, head_dim)
            alpha_t = F.silu(self.proj_alpha(x_t)).view(b, self.num_heads, self.head_dim) # (b, num_heads, head_dim)
            beta_t = F.silu(self.proj_beta(x_t)).view(b, self.num_heads, self.head_dim) # (b, num_heads, head_dim)
            
            # Update the state S
            # S_t = alpha_t * S_{t-1} + beta_t * v_t * v_t.transpose(-1, -2)
            # Simplified for illustration: S_t = alpha_t * S_{t-1} + beta_t * v_t
            # Here, we'll use a simpler update rule for the state S for readability
            # S_t = alpha_t * S_{t-1} + beta_t * v_t (element-wise for simplicity)
            # A more accurate representation of the paper's DeltaNet state update
            # would involve outer products or more complex interactions.
            # For this simplified example, let's assume S is (b, num_heads, head_dim)
            # and update it as a weighted sum.
            
            # Let's adjust state shape to (b, num_heads, head_dim) for this simplified example
            # In the actual paper, S is (b, num_heads, head_dim, head_dim)
            # and the update involves outer products or more complex matrix ops.
            # For this simplified code, let's assume S is a vector per head.
            
            # If state was (b, num_heads, head_dim, head_dim)
            # state = alpha_t.unsqueeze(-1) * state + beta_t.unsqueeze(-1) * v_t.unsqueeze(-1) @ v_t.unsqueeze(-2)
            
            if state.shape[-1] != self.head_dim: # Re-initialize if shape is wrong for this simplification
                 state = x.new_zeros(b, self.num_heads, self.head_dim)
            
            state = alpha_t * state + beta_t * v_t # Element-wise update

            # Compute output for current token
            output_t = state.mean(dim=1) # Aggregate across heads for simplicity (b, head_dim)
            
            # Apply output gate
            output_gate_t = torch.sigmoid(self.proj_output_gate(x_t)).view(b, self.num_heads, self.head_dim)
            output_t = output_t * output_gate_t.mean(dim=1) # Apply output gate

            output_sequence.append(output_t)

        output = torch.stack(output_sequence, dim=1) # (b, n, head_dim)
        output = self.out_proj(output) # (b, n, d_model)
        return output, state # Return final output and updated state
```
(간단함을 위해 Qwen3-Next와 Kimi Linear가 사용하는 컨볼루션 혼합(convolutional mixing)을 생략하여 코드를 더 읽기 쉽게 하고 순환적 측면(recurrent aspects)에 집중했습니다.) 따라서 위에서 볼 수 있듯이 표준 (또는 게이티드) 어텐션(attention)과는 많은 차이가 있습니다. 게이티드 어텐션(gated attention)에서는 모델이 모든 토큰(tokens) 간에 일반 어텐션(normal attention)을 계산합니다 (모든 토큰은 다른 모든 토큰을 주시하거나 살펴봅니다). 그런 다음 어텐션 출력(attention output)을 얻은 후, 게이트(시그모이드)가 해당 출력 중 얼마를 유지할지 결정합니다. 핵심은 여전히 컨텍스트 길이(context length)에 따라 이차적으로 확장되는 일반 스케일드 닷 프로덕트 어텐션(scaled-dot product attention)이라는 것입니다. 다시 한번 상기시키자면, 스케일드 닷 프로덕트 어텐션(scaled-dot product attention)은 softmax(QKᵀ)V로 계산됩니다. 여기서 Q와 K는 n x d 행렬이며, n은 입력 토큰(input tokens)의 수이고 d는 임베딩 차원(embedding dimension)입니다. 따라서 QKᵀ는 n x n 어텐션 행렬(attention matrix)을 생성하며, 이는 n x d 차원의 값 행렬(value matrix) V와 곱해집니다. 그림 9: 토큰 수 n에 따라 확장되는 전통적인 어텐션 메커니즘(attention mechanism) (다시 한번). Gated DeltaNet에는 n x n 어텐션 행렬(attention matrix)이 없습니다. 대신, 모델은 토큰(tokens)을 하나씩 처리합니다. 각 새 토큰이 들어올 때마다 업데이트되는 실행 중인 메모리(상태)를 유지합니다. 이것이 구현된 방식이며, 여기서 S는 각 시간 단계 t에 대해 순환적으로(recurrently) 업데이트되는 상태(state)입니다. 그리고 게이트(gates)는 해당 메모리(memory)가 어떻게 변하는지 제어합니다: α (알파)는 이전 메모리 중 얼마를 잊을지(감쇠) 조절합니다. β (베타)는 시간 단계 t의 현재 토큰이 메모리(memory)를 얼마나 업데이트하는지 조절합니다. (그리고 위 코드 조각에는 표시되지 않은 최종 출력 게이트(output gate)는 게이티드 어텐션(gated attention)과 유사하며, 출력 중 얼마를 유지할지 제어합니다.) 따라서 어떤 의미에서는 Gated DeltaNet의 이 상태 업데이트(state update)는 순환 신경망(RNNs)이 작동하는 방식과 유사합니다. 장점은 컨텍스트 길이(context length)에 따라 이차적으로가 아닌 선형적으로(for-loop을 통해) 확장된다는 것입니다. 이 순환 상태 업데이트(recurrent state update)의 단점은 일반 (또는 게이티드) 어텐션(attention)과 비교할 때, 전체 쌍별 어텐션(full pairwise attention)에서 오는 전역 컨텍스트 모델링(global context modeling) 능력을 희생한다는 것입니다. Gated DeltaNet은 어느 정도 컨텍스트(context)를 여전히 포착할 수 있지만, 메모리(S) 병목 현상(bottleneck)을 거쳐야 합니다. 해당 메모리(memory)는 고정된 크기이므로 더 효율적이지만, RNN과 유사하게 과거 컨텍스트(past context)를 단일 은닉 상태(hidden state)로 압축합니다. 이것이 Qwen3-Next와 Kimi Linear 아키텍처(architectures)가 모든 어텐션 레이어(attention layers)를 DeltaNet 레이어로 대체하지 않고 앞서 언급된 3:1 비율을 사용하는 이유입니다.

Gated DeltaNet의 핵심은 순환적인 상태 업데이트 메커니즘에 있습니다. 이는 과거 정보를 고정된 크기의 은닉 상태(hidden state)에 압축하여 저장하고, 새로운 입력 토큰이 들어올 때마다 이 상태를 업데이트하는 방식입니다. 여기서 α (감쇠 게이트)와 β (업데이트 게이트)는 이 상태 업데이트 과정을 동적으로 조절합니다. α는 이전 상태의 정보를 얼마나 유지할지, β는 현재 입력이 상태에 얼마나 반영될지를 결정합니다. 이러한 게이팅 메커니즘은 LSTM이나 GRU와 유사하게, 중요하지 않은 정보를 잊고 중요한 정보를 기억하는 능력을 부여하여 장기 의존성(long-term dependencies) 문제를 보다 효과적으로 처리할 수 있도록 합니다. SiLU(Sigmoid Linear Unit) 활성화 함수를 사용하는 것은 ReLU(Rectified Linear Unit) 계열의 장점(경사 소실 문제 완화)과 시그모이드의 부드러운 게이팅 특성을 결합하여, 학습의 안정성을 높이고 더 나은 성능을 이끌어내기 위한 선택입니다. 그러나 Gated DeltaNet은 전역적인 컨텍스트를 n x n 어텐션 행렬로 직접 모델링하는 대신, 고정된 크기의 상태를 통해 간접적으로 모델링하므로, 특정 복잡한 전역 패턴을 포착하는 데는 한계가 있을 수 있습니다. 이러한 트레이드오프 때문에 Qwen3-Next와 Kimi Linear는 전체 어텐션 레이어와 Gated DeltaNet 레이어를 혼합하여 사용함으로써, 효율성과 표현력 사이의 최적의 균형을 찾으려 합니다.

**2.7 DeltaNet 메모리 절약(DeltaNet Memory Savings)**
이전 섹션에서는 컨텍스트 길이(context length)에 대한 이차적 계산 복잡도(quadratic compute complexity) 대신 선형적 계산 복잡도(linear compute complexity) 측면에서 DeltaNet이 전체 어텐션(full attention)보다 가지는 장점을 논의했습니다. 선형 계산 복잡도(linear compute complexity) 외에 DeltaNet의 또 다른 큰 장점은 메모리 절약(memory savings)입니다. DeltaNet 모듈(modules)은 KV 캐시(KV cache)를 증가시키지 않기 때문입니다. (KV 캐싱(KV caching)에 대한 자세한 내용은 저의 "Understanding and Coding the KV Cache in LLMs from Scratch" 글을 참조하십시오.) 대신, 앞서 언급했듯이 고정 크기의 순환 상태(recurrent state)를 유지하므로 메모리(memory)는 컨텍스트 길이(context length)에 따라 일정하게 유지됩니다. 일반적인 멀티 헤드 어텐션(MHA) 레이어의 경우, KV 캐시(KV cache) 크기를 다음과 같이 계산할 수 있습니다:
KV_cache_MHA ≈ batch_size × n_tokens × n_heads × d_head × 2 × bytes
(2를 곱하는 이유는 캐시(cache)에 키(keys)와 값(values)을 모두 저장하기 때문입니다.) 위에서 구현된 단순화된 DeltaNet 버전의 경우 다음과 같습니다:
KV_cache_DeltaNet = batch_size × n_heads × d_head × d_head × bytes
KV_cache_DeltaNet 메모리 크기에는 컨텍스트 길이(n_tokens) 의존성이 없다는 점에 유의하십시오. 또한, 별도의 키(keys)와 값(values) 대신 메모리 상태 S만 저장하므로 2 × bytes가 그냥 bytes가 됩니다. 하지만 이제 여기에 이차적인 d_head × d_head가 있다는 점에 유의하십시오. 이는 상태(state)에서 비롯됩니다: S = x.new_zeros(b, self.num_heads, self.head_dim, self.head_dim) 하지만 헤드 차원(head dimension)은 일반적으로 상대적으로 작기 때문에 보통 걱정할 필요가 없습니다. 예를 들어, Qwen3-Next에서는 128입니다. 컨볼루션 혼합(convolutional mixing)이 포함된 전체 버전은 커널 크기(kernel size) 등을 포함하여 약간 더 복잡하지만, 위의 공식들은 Gated DeltaNet의 주요 경향과 동기를 설명해야 합니다. 그림 10: 증가하는 KV 캐시(KV cache) 크기 비교. 3:1 비율은 Gated DeltaNet과 전체 어텐션 레이어(full attention layers)의 비율을 나타냅니다. 계산은 emb_dim=2048, n_heads=16, n_layers=48, bf16을 가정합니다. 이 코드를 재현하는 코드는 여기에서 찾을 수 있습니다: https://github.com/rasbt/LLMs-from-scratch/tree/main/ch04/08_deltanet.

DeltaNet의 이러한 메모리 절약 특성은 특히 긴 컨텍스트 길이의 LLM을 배포할 때 매우 중요합니다. 전통적인 트랜스포머 모델의 KV 캐시는 컨텍스트 길이가 길어질수록 선형적으로 증가하며, 이는 GPU 메모리 사용량을 빠르게 포화시켜 배치 크기(batch size)를 제한하고 추론 비용을 증가시킵니다. 반면, DeltaNet은 고정된 크기의 상태를 유지하므로, 컨텍스트 길이에 관계없이 일정한 메모리 오버헤드(overhead)를 가집니다. 이는 모델이 수십만 또는 수백만 토큰에 이르는 매우 긴 문서를 처리해야 하는 경우에 엄청난 이점을 제공합니다. 예를 들어, 법률 문서 분석, 장문의 코드 베이스 이해, 또는 긴 대화 기록 요약과 같은 시나리오에서 DeltaNet은 기존 트랜스포머보다 훨씬 효율적으로 작동할 수 있습니다. 이러한 메모리 효율성은 더 큰 배치 크기를 허용하여 GPU 활용도를 높이고 전체 처리량(throughput)을 개선하는 데 기여합니다. 또한, 온디바이스(on-device) 또는 엣지(edge) 환경과 같이 메모리 제약이 있는 환경에서 LLM을 실행할 때 DeltaNet과 같은 아키텍처는 실행 가능성을 크게 높일 수 있습니다.

**2.8 Kimi Linear 대 Qwen3-Next (Kimi Linear vs. Qwen3-Next)**
Kimi Linear는 Qwen3-Next와 여러 구조적 유사점을 공유합니다. 두 모델 모두 하이브리드 어텐션 전략(hybrid attention strategy)에 의존합니다. 구체적으로, 이들은 경량 선형 어텐션(lightweight linear attention)과 더 무거운 전체 어텐션 레이어(full attention layers)를 결합합니다. 특히, 두 모델 모두 3:1 비율을 사용합니다. 즉, 선형 Gated DeltaNet 변형을 사용하는 세 개의 트랜스포머 블록(transformer blocks)마다 아래 그림과 같이 전체 어텐션(full attention)을 사용하는 하나의 블록이 있습니다. 그림 11: Qwen3-Next와 Kimi Linear 나란히. Gated DeltaNet은 순환 신경망(recurrent neural networks)에서 영감을 받은 선형 어텐션(linear attention) 변형으로, "Gated Delta Networks: Improving Mamba2 with Delta Rule" 논문의 게이팅 메커니즘(gating mechanism)을 포함합니다. 어떤 의미에서 Gated DeltaNet은 Mamba 스타일 게이팅(Mamba-style gating)을 가진 DeltaNet이며, DeltaNet은 선형 어텐션 메커니즘(linear attention mechanism)입니다 (이에 대해서는 다음 섹션에서 자세히 설명합니다). 위 그림 11의 오른쪽 상자에서 묘사된 Kimi Linear의 MLA는 시그모이드 게이트(sigmoid gate)를 사용하지 않습니다. 이 생략은 저자들이 아키텍처(architecture)를 표준 MLA와 더 직접적으로 비교할 수 있도록 의도된 것이었지만, 그들은 미래에 이를 추가할 계획이라고 밝혔습니다. 또한 위 그림의 Kimi Linear 부분에서 RoPE 상자(RoPE box)가 생략된 것도 의도적인 것임을 유의하십시오. Kimi는 멀티 헤드 잠재 어텐션(multi-head latent attention, MLA) 레이어(전역 어텐션, global attention)에 NoPE(No Positional Embedding)를 적용합니다. 저자들이 언급했듯이, 이는 MLA가 추론(inference) 시 순수한 멀티 쿼리 어텐션(multi-query attention)으로 실행되도록 하고, 긴 컨텍스트 스케일링(long-context scaling)을 위한 RoPE 재조정(retuning)을 피합니다 (위치 편향(positional bias)은 Kimi Delta Attention 블록(blocks)에 의해 처리되는 것으로 추정됩니다). MLA 및 그룹화된 쿼리 어텐션(grouped-query attention)의 특별한 경우인 멀티 쿼리 어텐션(multi-query attention)에 대한 자세한 내용은 저의 "The Big LLM Architecture Comparison" 글을 참조하십시오.

Kimi Linear와 Qwen3-Next의 비교는 하이브리드 어텐션 전략 내에서도 다양한 설계 선택이 가능함을 보여줍니다. Kimi Linear가 MLA(Multi-Head Latent Attention)를 사용하는 점은 주목할 만합니다. MLA는 KV 캐시 크기를 줄이기 위해 키/값 공간을 압축하는 방식으로, 이는 효율성을 높이는 동시에 전역적인 컨텍스트 정보를 어느 정도 유지하려는 시도입니다. 특히, MLA 레이어에서 NoPE(No Positional Embedding)를 적용하고 위치 편향을 Kimi Delta Attention 블록에 위임하는 것은, 위치 정보 처리를 분리하여 각 구성 요소가 자신의 강점에 집중하도록 설계된 것입니다. 이는 긴 컨텍스트 스케일링 시 RoPE를 재조정하는 복잡성을 피하고, 추론 시 효율성을 극대화하려는 전략입니다. 이러한 미묘한 아키텍처 선택은 모델의 특정 강점, 예를 들어 Kimi Linear의 긴 컨텍스트 추론 성능을 향상시키는 데 기여합니다. 두 모델 모두 Gated DeltaNet을 기반으로 하지만, MLA와 NoPE의 통합은 Kimi Linear가 Qwen3-Next와 차별화되는 중요한 지점이며, 이는 효율성과 정확도 사이의 복잡한 트레이드오프 공간에서 새로운 최적점을 탐색하는 연구자들의 노력을 보여줍니다.

**2.9 Kimi Delta 어텐션(Kimi Delta Attention)**
Kimi Linear는 Qwen3-Next의 선형 어텐션 메커니즘(linear attention mechanism)을 Kimi Delta Attention (KDA) 메커니즘으로 수정하는데, 이는 본질적으로 Gated DeltaNet의 정교화(refinement)입니다. Qwen3-Next가 메모리 감쇠율(memory decay rate)을 제어하기 위해 스칼라 게이트(scalar gate, 어텐션 헤드(attention head)당 하나의 값)를 적용하는 반면, Kimi Linear는 이를 각 특징 차원(feature dimension)에 대한 채널별 게이팅(channel-wise gating)으로 대체합니다. 저자들에 따르면, 이는 메모리(memory)에 대한 더 많은 제어권을 제공하며, 이는 결과적으로 긴 컨텍스트 추론(long-context reasoning)을 개선합니다. 또한, 전체 어텐션 레이어(full attention layers)의 경우, Kimi Linear는 Qwen3-Next의 게이티드 어텐션 레이어(gated attention layers, 본질적으로 출력 게이팅(output gating)이 있는 표준 멀티 헤드 어텐션 레이어)를 멀티 헤드 잠재 어텐션(multi-head latent attention, MLA)으로 대체합니다. 이는 DeepSeek V3/R1에서 사용된 동일한 MLA 메커니즘(mechanism)이지만 (저의 "The Big LLM Architecture Comparison" 글에서 논의했듯이) 추가 게이트(gate)가 있습니다. (요약하자면, MLA는 KV 캐시(KV cache) 크기를 줄이기 위해 키/값 공간(key/value space)을 압축합니다.) Qwen3-Next와의 직접적인 비교는 없지만, Gated DeltaNet 논문의 Gated DeltaNet-H1 모델(본질적으로 슬라이딩 윈도우 어텐션(sliding-window attention)이 있는 Gated DeltaNet)과 비교할 때, Kimi Linear는 동일한 토큰 생성 속도(token-generation speed)를 유지하면서 더 높은 모델링 정확도(modeling accuracy)를 달성합니다. 그림 12: Kimi Linear 논문(https://arxiv.org/abs/2510.26692)의 주석이 달린 그림으로, Kimi Linear가 GatedDeltaNet만큼 빠르고, 멀티 헤드 잠재 어텐션(multi-head latent attention)을 사용하는 아키텍처(DeepSeek V3/R1과 같은)보다 훨씬 빠르며, 더 높은 벤치마크 성능(benchmark performance)을 가짐을 보여줍니다. 또한, DeepSeek-V2 논문의 절제 연구(ablation studies)에 따르면, 하이퍼파라미터(hyperparameters)를 신중하게 선택하면 MLA는 일반 전체 어텐션(full attention)과 동등합니다. 그리고 Kimi Linear가 긴 컨텍스트(long-context) 및 추론 벤치마크(reasoning benchmarks)에서 MLA보다 유리하게 비교된다는 사실은 선형 어텐션(linear attention) 변형이 더 큰 최첨단 모델(state-of-the-art models)에 대해 다시 한번 유망하다는 것을 의미합니다. 그렇긴 하지만, Kimi Linear는 480억 개의 매개변수를 가지지만, Kimi K2보다 20배 작습니다. Kimi 팀이 다가오는 K3 모델에 이 접근 방식을 채택할지 지켜보는 것은 흥미로울 것입니다.

Kimi Delta Attention(KDA)의 채널별 게이팅(channel-wise gating)은 메모리 제어에 있어 스칼라 게이팅보다 훨씬 정교한 메커니즘을 제공합니다. 스칼라 게이트는 어텐션 헤드 전체에 걸쳐 단일 감쇠율을 적용하는 반면, 채널별 게이팅은 각 특징 차원(feature dimension)이 독립적으로 감쇠 또는 업데이트되도록 허용합니다. 이는 모델이 특정 유형의 정보(예: 특정 의미론적 특징)를 더 오래 유지하거나 빠르게 잊을 수 있도록 하여, 긴 컨텍스트에서 정보의 중요도를 더욱 세밀하게 조절할 수 있게 합니다. 이러한 미세 조정 능력은 복잡한 추론 작업에서 중요한 세부 정보를 유지하는 데 필수적이며, Kimi Linear가 긴 컨텍스트 추론 벤치마크에서 뛰어난 성능을 보이는 주요 요인 중 하나입니다. MLA의 통합은 Kimi Linear의 또 다른 중요한 특징입니다. MLA는 키/값 공간을 압축하여 KV 캐시 메모리 사용량을 줄이면서도, DeepSeek V3/R1에서 입증된 바와 같이 전체 어텐션에 필적하는 성능을 제공할 수 있습니다. Kimi Linear는 KDA의 효율성과 MLA의 표현력을 결합하여, 기존 트랜스포머의 단점을 보완하면서도 최첨단 성능에 근접하는 하이브리드 아키텍처의 가능성을 보여줍니다. 이는 LLM의 효율성과 성능 한계를 동시에 확장하려는 연구의 중요한 방향성을 제시합니다.

**2.10 어텐션 하이브리드의 미래(The Future of Attention Hybrids)**
선형 어텐션(linear attention)은 새로운 개념이 아니지만, 하이브리드 접근 방식(hybrid approaches)의 최근 부활은 연구자들이 트랜스포머(transformers)를 더 효율적으로 만들기 위한 실용적인 방법을 다시 진지하게 찾고 있음을 보여줍니다. 예를 들어, Kimi Linear는 일반 전체 어텐션(full attention)과 비교하여 KV 캐시(KV cache)를 75% 줄이고 디코딩 처리량(decoding throughput)을 최대 6배 향상시킵니다. 이 새로운 세대의 선형 어텐션(linear attention) 변형이 이전 시도와 다른 점은, 이제 표준 어텐션(standard attention)을 완전히 대체하는 대신 함께 사용된다는 것입니다. 앞으로, 다음 어텐션 하이브리드(attention hybrids)의 물결은 긴 컨텍스트 안정성(long-context stability)과 추론 정확도(reasoning accuracy)를 더욱 개선하여 전체 어텐션(full-attention)의 최첨단(state-of-the-art)에 더 가까워지는 데 초점을 맞출 것으로 예상합니다.

어텐션 하이브리드의 미래는 단순히 선형성과 효율성을 넘어, 다양한 스파스(sparse) 어텐션 패턴 및 동적 어텐션 메커니즘과의 융합을 통해 더욱 발전할 것입니다. 예를 들어, 그래프 기반 어텐션(graph-based attention)이나 계층적 어텐션(hierarchical attention)과 선형 어텐션을 결합하여, 모델이 지역적(local) 및 전역적(global) 컨텍스트를 동시에 효율적으로 처리할 수 있도록 하는 연구가 진행될 수 있습니다. 또한, 특정 작업이나 입력 특성에 따라 어텐션 메커니즘을 동적으로 전환하거나 조절하는 적응형(adaptive) 하이브리드 모델도 등장할 수 있습니다. 이는 모델이 항상 가장 효율적인 경로를 선택하도록 하여, 성능 저하 없이 최적의 자원 활용을 가능하게 할 것입니다. 궁극적으로 어텐션 하이브리드는 클라우드 환경의 대규모 LLM부터 엣지 디바이스의 경량 모델에 이르기까지, 다양한 컴퓨팅 제약 조건과 애플리케이션 요구 사항을 충족하는 유연하고 강력한 LLM 아키텍처를 구축하는 데 핵심적인 역할을 할 것입니다.

**3. 텍스트 확산 모델(Text Diffusion Models)**
표준 자기회귀 LLM 아키텍처(autoregressive LLM architecture)에서 더 급진적으로 벗어난 것은 텍스트 확산 모델(text diffusion models) 계열입니다. 여러분은 아마도 확산 모델(diffusion models)에 익숙할 것입니다. 이는 2020년 "Denoising Diffusion Probabilistic Models" 논문을 기반으로 이미지를 생성하는 모델(생성적 적대 신경망(generative adversarial networks)의 후속 모델)이며, 나중에 Stable Diffusion 등에 의해 구현, 확장 및 대중화되었습니다. 그림 13: 2022년 저의 첫 Substack 글에서 가져온 이미지 확산 과정(image diffusion process)의 설명. 여기서는 왼쪽에서 오른쪽으로 가우시안 노이즈(Gaussian noise)가 추가되며, 모델의 과제는 노이즈를 제거하는 방법(오른쪽에서 왼쪽으로)을 학습하는 것입니다.

**3.1 왜 텍스트 확산에 대해 연구하는가?(Why Work on Text Diffusion?)**
2022년 "Diffusion‑LM Improves Controllable Text Generation" 논문과 함께, 연구자들이 텍스트 생성(generating text)을 위해 확산 모델(diffusion models)을 채택하기 시작하는 추세의 시작을 보게 되었습니다. 그리고 저는 2025년에 수많은 텍스트 확산(text diffusion) 논문을 보았습니다. 제 논문 북마크 목록을 확인해 보니, 39개의 텍스트 확산 모델(text diffusion models)이 있었습니다! 이 모델들의 인기가 높아지는 것을 고려할 때, 마침내 이들에 대해 이야기할 때가 되었다고 생각했습니다. 그림 14: 이 섹션은 텍스트 확산 모델(text diffusion models)을 다룹니다. 그렇다면 확산 모델(diffusion models)의 장점은 무엇이며, 연구자들은 왜 이를 전통적인 자기회귀 LLM(autoregressive LLMs)의 대안으로 고려하고 있을까요? 전통적인 트랜스포머 기반(자기회귀) LLM은 한 번에 하나의 토큰(token)을 생성합니다. 간결함을 위해, 이들을 단순히 자기회귀 LLM(autoregressive LLMs)이라고 부르겠습니다. 이제 텍스트 확산 기반 LLM(이들을 "확산 LLM(diffusion LLMs)"이라고 부르겠습니다)의 주요 강점은 순차적으로(sequentially)가 아닌 병렬로(in parallel) 여러 토큰(tokens)을 생성할 수 있다는 것입니다. 확산 LLM(diffusion LLMs)은 여전히 여러 번의 노이즈 제거 단계(denoising steps)를 필요로 한다는 점에 유의하십시오. 하지만 확산 모델이 각 단계에서 모든 토큰(tokens)을 병렬로 생성하기 위해 예를 들어 64번의 노이즈 제거 단계(denoising steps)를 필요로 하더라도, 이는 2,000개의 토큰 응답을 생성하기 위해 2,000번의 순차적 생성 단계(sequential generation steps)를 수행하는 것보다 여전히 계산적으로 더 효율적입니다.

**3.2 노이즈 제거 과정(The Denoising Process)**
확산 LLM(diffusion LLM)의 노이즈 제거 과정(denoising process)은 일반 이미지 확산 모델(image diffusion models)의 노이즈 제거 과정과 유사하며, 아래 GIF에 나와 있습니다. (주요 차이점은 픽셀(pixels)에 가우시안 노이즈(Gaussian noise)를 추가하는 대신, 텍스트 확산(text diffusion)은 확률적으로 토큰(tokens)을 마스킹(masking)하여 시퀀스(sequences)를 손상시킨다는 것입니다.) 이 실험을 위해, 저는 올해 초에 발표된 "Large Language Diffusion Models (LLaDA)" 논문의 8B 지시 모델(instruct model)을 실행했습니다. 그림 15: 8B LLaDA 모델을 사용한 노이즈 제거 과정(denoising process)의 설명. 위 애니메이션에서 볼 수 있듯이, 텍스트 확산 과정(text diffusion process)은 [MASK] 토큰(tokens)을 텍스트 토큰으로 연속적으로 대체하여 답변을 생성합니다. BERT와 마스킹된 언어 모델링(masked language modeling)에 익숙하다면, 이 확산 과정(diffusion process)을 BERT 순방향 전달(forward pass)의 반복적인 적용(BERT가 다른 마스킹 비율(masking rates)로 사용되는 경우)으로 생각할 수 있습니다. 아키텍처(architecture) 측면에서, 확산 LLM(diffusion LLMs)은 일반적으로 디코더 스타일 트랜스포머(decoder-style transformers)이지만 인과적 어텐션 마스크(causal attention mask)가 없습니다. 예를 들어, 앞서 언급된 LLaDA 모델은 Llama 3 아키텍처(architecture)를 사용합니다. 인과적 마스크(causal mask)가 없는 이러한 아키텍처를 "양방향(bidirectional)"이라고 부르는데, 이는 모든 시퀀스 요소(sequence elements)에 한 번에 접근할 수 있기 때문입니다. (이는 역사적인 이유로 "인코더 스타일(encoder-style)"이라고 불리는 BERT 아키텍처(architecture)와 유사하다는 점에 유의하십시오.) 따라서 자기회귀 LLM(autoregressive LLMs)과 확산 LLM(diffusion LLMs)의 주요 차이점(인과적 마스크 제거 외에)은 훈련 목표(training objective)입니다. LLaDA와 같은 확산 LLM(diffusion LLMs)은 다음 토큰 예측 목표(next-token prediction objective) 대신 생성적 확산 목표(generative diffusion objective)를 사용합니다. 이미지 모델(image models)에서 생성적 확산 목표(generative diffusion objective)는 연속적인 픽셀 공간(pixel space)을 가지고 있기 때문에 직관적입니다. 예를 들어, 가우시안 노이즈(Gaussian noise)를 추가하고 노이즈 제거를 학습하는 것은 수학적으로 자연스러운 연산입니다. 하지만 텍스트는 이산적인 토큰(discrete tokens)으로 구성되어 있으므로, 동일한 연속적인 의미에서 "노이즈"를 직접 추가하거나 제거할 수 없습니다. 따라서 픽셀 강도(pixel intensities)를 교란하는 대신, 이러한 확산 LLM(diffusion LLMs)은 토큰(tokens)을 무작위로 점진적으로 마스킹(masking)하여 텍스트를 손상시키며, 각 토큰은 지정된 확률로 특수 마스크 토큰(mask token)으로 대체됩니다. 그런 다음 모델은 각 단계에서 누락된 토큰(tokens)을 예측하는 역 과정(reverse process)을 학습하며, 이는 앞서 그림 15의 애니메이션에서 보듯이 시퀀스(sequence)를 원래 텍스트로 효과적으로 "노이즈 제거(denoises)"(또는 마스크 해제(unmasks))합니다. 그 뒤에 있는 수학을 설명하는 것은 별도의 튜토리얼에 더 적합하겠지만, 대략적으로는 BERT가 확률적 최대 우도 프레임워크(probabilistic maximum-likelihood framework)로 확장된 것으로 생각할 수 있습니다.

텍스트 확산 모델의 노이즈 제거 과정에서 사용되는 마스킹 전략은 모델의 성능에 큰 영향을 미칩니다. 단순히 무작위로 마스킹하는 것 외에도, 특정 언어학적 구조를 보존하거나 더 도전적인 예측 작업을 유도하기 위한 다양한 마스킹 기법이 연구되고 있습니다. 예를 들어, 스팬 마스킹(span masking)은 연속된 토큰 그룹을 마스킹하여 모델이 더 긴 범위의 컨텍스트를 이해하도록 훈련시키고, 엔티티 마스킹(entity masking)은 명사구나 특정 엔티티를 마스킹하여 의미론적 이해를 강화합니다. 이러한 마스킹 전략은 모델이 단순히 표면적인 패턴을 학습하는 것을 넘어, 텍스트의 깊은 의미와 구조를 파악하도록 돕습니다. 또한, 확산 모델의 훈련 목표는 다음 토큰 예측(next-token prediction)과는 달리, 손상된 텍스트에서 원본 텍스트를 복원하는 것에 중점을 둡니다. 이는 모델이 전체 시퀀스의 일관성과 유창성(fluency)을 동시에 고려하도록 유도하며, 특히 제어 가능한 텍스트 생성(controllable text generation)에서 강점을 보일 수 있습니다. 예를 들어, 특정 키워드, 스타일, 또는 구문에 제약을 두어 텍스트를 생성하는 작업에서 확산 모델은 자기회귀 모델보다 더 유연하게 작동할 수 있습니다.

**3.3 자기회귀 LLM 대 확산 LLM(Autoregressive vs Diffusion LLMs)**
앞서 저는 확산 LLM(diffusion LLMs)이 매력적인 이유는 일반 자기회귀 LLM(autoregressive LLM)처럼 토큰(tokens)을 순차적으로 생성하는 대신 병렬로 생성(또는 노이즈 제거)하기 때문이라고 말했습니다. 이는 확산 모델(diffusion models)을 자기회귀 LLM(autoregressive LLMs)보다 더 효율적으로 만들 잠재력을 가지고 있습니다. 그렇긴 하지만, 전통적인 LLM의 자기회귀적 특성(autoregressive nature)은 그들의 주요 강점 중 하나입니다. 그리고 순수한 병렬 디코딩(parallel decoding)의 문제는 최근 "ParallelBench: Understanding the Trade-offs of Parallel Decoding in Diffusion LLMs" 논문의 훌륭한 예시로 설명될 수 있습니다. 그림 16: "ParallelBench: Understanding the Trade-offs of Parallel Decoding in Diffusion LLMs" 논문(https://arxiv.org/abs/2510.04767)의 주석이 달린 그림으로, 병렬 디코딩(parallel decoding)의 문제를 보여줍니다. 예를 들어, 다음 프롬프트(prompt)를 고려해 보십시오: > "여행할 도시를 무작위로 선택하세요: 뉴욕, 뉴올리언스, 멕시코시티, 파나마시티?" LLM에게 두 개의 토큰(token)으로 된 답변을 생성하도록 요청한다고 가정해 봅시다. 먼저 조건부 확률 p(y t = ”New” | X)에 따라 "New" 토큰(token)을 샘플링(sample)할 수 있습니다. 다음 반복에서는 이전에 생성된 토큰(token)에 조건을 부여하고 "York" 또는 "Orleans"를 선택할 가능성이 높습니다. 이는 조건부 확률 p(y t+1 = ”York” | X, y t = ”New”)와 p(y t+1 = ”Orleans” | X, y t = ”New”)가 모두 상대적으로 높기 때문입니다 ("New"는 훈련 세트(training set)에서 이러한 연속과 자주 함께 나타나기 때문입니다). 하지만 대신 두 토큰(tokens)이 병렬로 샘플링(sample)된다면, 모델은 독립적으로 두 개의 가장 높은 확률 토큰 p(y t = “New” | X)와 p(y {t+1} = “City” | X)를 선택하여 "New City"와 같은 어색한 출력을 초래할 수 있습니다. (이는 모델이 자기회귀적 조건화(autoregressive conditioning)가 부족하여 토큰 의존성(token dependencies)을 포착하지 못하기 때문입니다.) 어쨌든 위 내용은 확산 LLM(diffusion LLMs)에 조건부 의존성(conditional dependency)이 전혀 없는 것처럼 들리게 하는 단순화입니다. 이것은 사실이 아닙니다. 확산 LLM(diffusion LLM)은 앞서 말했듯이 모든 토큰(tokens)을 병렬로 예측하지만, 예측은 반복적인 정제(노이즈 제거) 단계(iterative refinement (denoising) steps)를 통해 공동으로 의존합니다. 여기서 각 확산 단계(diffusion step)는 현재의 전체 노이즈 텍스트(noisy text)에 조건을 부여합니다. 그리고 토큰(tokens)은 모든 단계에서 교차 어텐션(cross-attention)과 자기 어텐션(self-attention)을 통해 서로에게 영향을 미칩니다. 따라서 모든 위치가 동시에 업데이트되더라도, 업데이트는 공유 어텐션 레이어(attention layers)를 통해 서로에게 조건을 부여합니다. 하지만 앞서 언급했듯이, 이론적으로 2,000개 토큰(token) 답변을 생성할 때 20-60번의 확산 단계(diffusion steps)는 자기회귀 LLM(autoregressive LLM)의 2,000번 추론 단계(inference steps)보다 저렴할 수 있습니다.

"ParallelBench" 논문에서 제기된 문제는 확산 LLM이 병렬 생성의 이점을 완전히 활용하기 위해 극복해야 할 중요한 과제입니다. 이 문제를 해결하기 위해 연구자들은 여러 가지 전략을 모색하고 있습니다. 첫째, 반복적인 정제 과정에서 조건부 의존성을 강화하는 것입니다. 각 노이즈 제거 단계에서 모델은 현재까지 생성된 모든 토큰과 원본 프롬프트(prompt)에 대한 더 강력하고 명시적인 조건을 부여받아, 토큰 간의 일관성을 높이도록 훈련됩니다. 둘째, 새로운 샘플링 기법을 개발하는 것입니다. 예를 들어, 병렬로 여러 토큰을 생성하되, 이들 토큰이 서로 충돌하거나 비논리적인 조합을 형성하지 않도록 제약 조건을 도입하거나, 생성된 토큰들의 일관성을 평가하고 재샘플링(resampling)하는 메커니즘을 추가하는 방식입니다. 셋째, 자기회귀 모델의 장점과 확산 모델의 장점을 결합하는 하이브리드 디코딩 전략도 고려될 수 있습니다. 예를 들어, 확산 모델이 전반적인 구조와 주요 내용을 병렬로 생성한 후, 자기회귀 모델이 이를 기반으로 세부적인 문법적, 의미론적 일관성을 다듬는 방식입니다. 이러한 노력은 확산 LLM이 병렬 디코딩의 효율성을 유지하면서도 자기회귀 모델에 필적하는 출력 품질을 달성하도록 돕는 중요한 연구 방향입니다.

**3.4 오늘날의 텍스트 확산(Text Diffusion Today)**
비전 모델(vision models)이 어텐션(attention) 및 트랜스포머 아키텍처(transformer architecture) 자체와 같은 LLM의 구성 요소를 채택하는 반면, 텍스트 기반 LLM은 순수 비전 모델에서 영감을 받아 텍스트용 확산(diffusion for text)을 구현하는 것은 흥미로운 추세입니다. 개인적으로 몇 가지 데모를 시도해 본 것 외에는 아직 많은 확산 모델(diffusion models)을 사용해 보지 않았지만, 저는 이를 트레이드오프(trade-off)로 생각합니다. 확산 단계(diffusion steps) 수를 적게 사용하면 답변을 더 빠르게 생성하지만, 품질이 저하된 답변을 생성할 수 있습니다. 더 나은 답변을 생성하기 위해 확산 단계(diffusion steps)를 늘리면, 자기회귀 모델(autoregressive model)과 유사한 비용을 가진 모델이 될 수 있습니다. "ParallelBench: Understanding the Trade-offs of Parallel Decoding in Diffusion LLMs" 논문의 저자들을 인용하자면:
[...] 우리는 [확산 LLM(diffusion LLMs)]과 자기회귀 LLM(autoregressive LLMs)을 체계적으로 분석하여 다음을 밝혀냈습니다: (i) 병렬 디코딩(parallel decoding) 하의 [확산 LLM]은 실제 시나리오에서 극적인 품질 저하를 겪을 수 있으며, (ii) 현재의 병렬 디코딩 전략(parallel decoding strategies)은 작업 난이도에 따라 병렬 처리 정도를 조정하는 데 어려움을 겪으므로, 품질 저하 없이 의미 있는 속도 향상을 달성하지 못합니다.
또한, 제가 보는 또 다른 특정 단점은 확산 LLM(diffusion LLMs)이 체인(chain)이 없기 때문에 도구(tools)를 체인의 일부로 사용할 수 없다는 것입니다. 확산 단계(diffusion steps) 사이에 이들을 끼워 넣는 것이 가능할 수도 있지만, 이는 사소한 일이 아니라고 생각합니다. (제가 틀렸다면 수정해 주십시오.) 요컨대, 확산 LLM(diffusion LLMs)은 탐구할 흥미로운 방향인 것으로 보이지만, 현재로서는 자기회귀 LLM(autoregressive LLMs)을 대체하지 못할 수도 있습니다. 하지만 저는 이들을 더 작고 온디바이스(on-device) LLM의 흥미로운 대안으로 보거나, 어쩌면 더 작고 증류된(distilled) 자기회귀 LLM을 대체할 수도 있다고 생각합니다. 예를 들어, Google은 텍스트용 Gemini Diffusion 모델을 개발 중이라고 발표했으며, "빠른 응답: 지금까지 우리의 가장 빠른 모델보다 훨씬 빠르게 콘텐츠를 생성합니다."라고 밝혔습니다. 그리고 더 빠르면서도, 벤치마크 성능(benchmark performance)은 빠른 Gemini 2.0 Flash-Lite 모델과 동등하게 유지되는 것으로 보입니다. 모델이 출시되고 사용자들이 다양한 작업과 도메인에서 시도해 본 후, 채택률과 피드백이 어떨지 지켜보는 것은 흥미로울 것입니다. 그림 17: (더 빠른) 확산 LLM(diffusion LLM, Gemini Diffusion)과 빠른 자기회귀 LLM(autoregressive LLM, Gemini 2.0 Flash-Lite)의 벤치마크 성능(benchmark performance) 비교. https://deepmind.google/models/gemini-diffusion/#capabilities에 보고된 수치를 기반으로 합니다.

텍스트 확산 모델의 잠재력은 특히 제어 가능한 생성(controllable generation) 및 창의적 텍스트 생성(creative text generation) 분야에서 두드러집니다. 예를 들어, 특정 스타일, 감성, 또는 주제를 반영하는 텍스트를 생성할 때, 확산 모델은 반복적인 정제 과정을 통해 원하는 제약 조건을 더 효과적으로 만족시킬 수 있습니다. 이는 시, 소설, 대본 작성 등에서 유용할 수 있습니다. 또한, 데이터 증강(data augmentation)을 위한 고품질 텍스트 생성이나, 특정 도메인에 특화된 데이터셋을 구축하는 데 활용될 수도 있습니다. 하지만 자기회귀 LLM이 제공하는 "사고의 사슬(Chain of Thought, CoT)"과 같은 복잡한 추론 과정이나 도구 사용(tool use) 능력은 확산 LLM에서 아직 명확하게 구현되지 않은 과제입니다. 확산 단계 사이에 외부 도구를 호출하는 것은 기술적으로 가능할 수 있지만, 이를 효율적이고 일관되게 통합하는 방법은 활발히 연구되어야 할 분야입니다. 그럼에도 불구하고, Gemini Diffusion과 같은 대규모 프로덕션 모델의 등장은 텍스트 확산 모델이 단순한 연구 개념을 넘어 실제 애플리케이션에서 중요한 역할을 할 수 있음을 시사하며, 앞으로 이 분야의 발전이 주목됩니다.

**4. 월드 모델(World Models)**
지금까지 우리는 효율성을 개선하고 모델을 더 빠르거나 확장 가능하게 만드는 데 초점을 맞춘 접근 방식들을 논의했습니다. 그리고 이러한 접근 방식들은 일반적으로 약간 저하된 모델링 성능(modeling performance)을 동반합니다. 이제 이 섹션의 주제는 다른 관점을 취하며 모델링 성능(modeling performance) 향상(효율성 아님)에 초점을 맞춥니다. 이 개선된 성능은 모델에게 "세상에 대한 이해"를 가르침으로써 달성됩니다. 월드 모델(World models)은 전통적으로 언어 모델링(language modeling)과 독립적으로 개발되었지만, 2025년 9월에 발표된 최근 "Code World Models" 논문은 이들을 처음으로 이 맥락에서 직접적으로 관련성 있게 만들었습니다. 이상적으로는, 이 글의 다른 주제들과 마찬가지로, 월드 모델(world models)은 그 자체로 하나의 전용 글(또는 책)입니다. 하지만 Code World Models (CWM) 논문에 들어가기 전에, 월드 모델(world models)에 대한 짧은 소개라도 제공하겠습니다.

**4.1 월드 모델의 주요 아이디어(The Main Idea Behind World Models)**
원래 월드 모델(world models)의 아이디어는 결과를 암묵적으로 모델링하는 것입니다. 즉, 실제로 발생하지 않은 결과가 다음에 무엇이 일어날지 예측하는 것입니다 (아래 그림에 설명된 대로). 이는 인간의 뇌가 이전 경험을 바탕으로 다가올 사건을 지속적으로 예측하는 방식과 유사합니다. 예를 들어, 우리가 커피나 차 한 잔을 잡으려 할 때, 우리의 뇌는 이미 그것이 얼마나 무거울지 예측하고, 컵을 만지거나 들어 올리기도 전에 잡는 방식을 조절합니다. 그림 18: 월드 모델 시스템(world model system)의 개념적 개요. 에이전트(agent)는 현재 상태(state(t))를 관찰하고 주어진 목표를 달성하기 위해 행동(action(t))을 취함으로써 환경과 상호작용합니다. 동시에 에이전트는 환경의 정신적 시뮬레이션(mental simulation) 역할을 하는 내부 월드 모델(internal world model)을 학습하며, 이를 통해 실제 세계에서 실행하기 전에 결과를 예측하고 행동을 계획할 수 있습니다. 제가 아는 한, "월드 모델(world model)"이라는 용어는 Ha와 Schmidhuber의 2018년 동명 논문 "World Models"에 의해 대중화되었으며, 이 논문은 VAE와 RNN 아키텍처(architecture)를 사용하여 강화 학습 에이전트(reinforcement learning agents)를 위한 내부 환경 시뮬레이터(internal environment simulator)를 학습했습니다. (하지만 이 용어나 개념 자체는 본질적으로 세계나 환경의 개념을 모델링하는 것을 의미하므로, 1980년대의 강화 학습(reinforcement learning) 및 로봇 공학 연구(robotics research)로 거슬러 올라갑니다.) 솔직히, 저는 Yann LeCun의 2022년 기사 "A Path Towards Autonomous Machine Intelligence"가 나오기 전까지는 월드 모델(world models)에 대한 새로운 해석을 인지하지 못했습니다. 이는 본질적으로 LLM 대신 AI에 대한 대안적인 경로를 매핑하는 것에 관한 것이었습니다.

월드 모델의 개념은 인공지능 분야에서 오랜 역사를 가지고 있으며, 초기에는 주로 로봇 공학 및 강화 학습(Reinforcement Learning, RL) 분야에서 환경의 역학(dynamics)을 학습하여 에이전트의 의사 결정(decision-making)을 돕는 데 사용되었습니다. Ha와 Schmidhuber의 2018년 논문은 이 개념을 신경망 기반의 생성 모델과 결합하여, 에이전트가 내부적으로 환경을 시뮬레이션하고 미래를 예측함으로써 효율적으로 학습할 수 있음을 보여주었습니다. 이는 "모델 기반 강화 학습(model-based RL)"의 중요한 한 축을 형성합니다. 월드 모델은 명시적인 보상 신호(reward signal) 없이도 환경에서 풍부한 경험을 통해 학습할 수 있으므로, 데이터 효율적인 학습을 가능하게 합니다. 또한, 환경에 대한 내부적인 이해를 바탕으로 에이전트는 계획(planning), 탐색(exploration), 그리고 추론(reasoning)과 같은 고차원적인 인지 능력을 발휘할 수 있습니다. Yann LeCun이 제안한 것처럼, 월드 모델은 단순한 패턴 매칭을 넘어 진정한 자율적인 기계 지능(autonomous machine intelligence)을 달성하기 위한 핵심 요소로 간주될 수 있습니다.

**4.2 비전에서 코드로(From Vision to Code)**
그렇긴 하지만, 월드 모델(world model) 논문들은 모두 비전 도메인(vision domains)에 초점을 맞추었으며, 초기 VAE 및 RNN 기반 모델부터 트랜스포머(transformers), 확산 모델(diffusion models), 심지어 Mamba 레이어 하이브리드(Mamba-layer hybrids)에 이르기까지 광범위한 아키텍처(architectures)를 아울렀습니다. 이제 현재 LLM에 더 집중하고 있는 저로서는 "Code World Model" 논문(2025년 9월 30일)이 저의 완전한 주의를 사로잡은 첫 번째 논문입니다 (말장난 의도 없음). 이것은 (제가 아는 한) 텍스트에서 텍스트로 (또는 더 정확히는 코드에서 코드로) 매핑하는 첫 번째 월드 모델(world model)입니다. CWM은 320억 개의 매개변수를 가진 오픈 가중치 모델(open-weight model)로, 131k 토큰(token) 컨텍스트 윈도우(context window)를 가지고 있습니다. 아키텍처(architecture)적으로는 여전히 슬라이딩 윈도우 어텐션(sliding-window attention)을 가진 밀집 디코더 전용 트랜스포머(dense decoder-only Transformer)입니다. 또한 다른 LLM과 마찬가지로 사전 훈련(pre-training), 중간 훈련(mid-training), 지도 미세 조정(supervised fine-tuning, SFT), 강화 학습(reinforcement learning) 단계를 거치지만, 중간 훈련 데이터(mid-training data)가 월드 모델링 구성 요소(world-modeling component)를 도입합니다.

코드는 월드 모델링에 특히 적합한 도메인입니다. 자연어와 달리 코드는 명확하고 결정론적인(deterministic) 실행 의미론(execution semantics)을 가집니다. 즉, 주어진 코드와 입력에 대해 항상 동일한 출력이 생성되며, 변수 상태(variable states)의 변화가 명확하게 정의됩니다. 이러한 특성은 모델이 코드의 실행 환경을 정확하게 시뮬레이션하고, 각 코드 라인이 프로그램 상태에 미치는 영향을 예측하도록 학습하는 데 이상적입니다. Code World Model(CWM)의 핵심 아이디어는 단순히 코드를 텍스트로 이해하는 것을 넘어, 코드가 "어떻게 작동하는지"에 대한 내부적인 모델을 구축하는 것입니다. 이는 모델이 코드의 구문적(syntactic) 패턴뿐만 아니라 의미적(semantic) 및 동적(dynamic) 특성을 학습하도록 유도합니다. CWM과 같은 모델은 코드 생성, 디버깅, 취약점 분석, 그리고 자동화된 프로그램 수정과 같은 복잡한 소프트웨어 개발 작업에서 혁신적인 발전을 가져올 잠재력을 가지고 있습니다.

**4.3 코드 월드 모델 대 일반 코드 LLM (Code World Models Vs Regular LLMs for Code)**
그렇다면 이것은 Qwen3-Coder와 같은 일반 코드 LLM과 어떻게 다를까요? Qwen3-Coder와 같은 일반 모델은 순전히 다음 토큰 예측(next-token prediction)으로 훈련됩니다. 이들은 구문(syntax)과 논리(logic)의 패턴을 학습하여 그럴듯한 코드 완성(code completions)을 생성하며, 이는 프로그래밍에 대한 정적인 텍스트 수준의 이해를 제공합니다. 대조적으로 CWM은 코드가 실행될 때 어떤 일이 일어나는지 시뮬레이션(simulate)하는 것을 학습합니다. 아래 그림과 같이 코드 한 줄을 수정하는 것과 같은 동작을 수행한 후, 변수(variable)의 값과 같은 결과적인 프로그램 상태(program state)를 예측하도록 훈련됩니다. 그림 19: Code World Model (CWM)에서의 코드 실행 추적(code execution tracing) 예시. 모델은 각 코드 라인이 실행됨에 따라 변수 상태(variable states)가 단계별로 어떻게 진화하는지 예측합니다. 여기서 모델은 코드의 동작을 효과적으로 시뮬레이션합니다. https://www.arxiv.org/abs/2510.02387에서 가져온 주석이 달린 그림. 추론(inference) 시 CWM은 GPT 스타일 모델과 마찬가지로 한 번에 하나의 토큰(token)을 생성하는 자기회귀 트랜스포머(autoregressive transformer)입니다. 주요 차이점은 이러한 토큰(tokens)이 일반 텍스트(plain text) 대신 구조화된 실행 추적(structured execution traces)을 인코딩(encode)할 수 있다는 것입니다. 그래서 저는 이것을 월드 모델(world model)이라고 부르기보다는 월드 모델 증강 LLM(world model-augmented LLM)이라고 부를 것입니다. 첫 시도치고는 놀랍도록 잘 작동하며, 대략 같은 크기에서 gpt-oss-20b (중간 추론 노력)와 동등합니다. 테스트 시간 스케일링(test-time-scaling)을 사용하면, 4배 더 작으면서도 gpt-oss-120b (높은 추론 노력)보다 약간 더 나은 성능을 보입니다. 그들의 테스트 시간 스케일링(test-time scaling)은 생성된 단위 테스트(unit tests)와 함께 best@k 절차(procedure)를 사용한다는 점에 유의하십시오 (고급 다수결 투표 방식(majority voting scheme)을 생각해보십시오). CWM과 gpt-oss가 다른 테스트 시간 스케일링 전략(test-time-scaling strategies) (best@k 대 추론 노력당 더 많은 토큰)을 사용하므로, CWM과 gpt-oss 간의 토큰/초 또는 해결 시간 비교를 보는 것은 흥미로웠을 것입니다. 그림 20: 코딩 벤치마크(coding benchmark, SWE-bench)에서 코드 월드 모델(code world model, CWM)과 다른 인기 있는 LLM의 성능 비교. https://www.arxiv.org/abs/2510.02387에서 가져온 주석이 달린 그림.

CWM에서 "실행 추적(execution trace)"은 단순히 코드 스니펫(code snippet)뿐만 아니라, 각 코드 라인이 실행될 때 변화하는 변수의 값, 스택(stack) 상태, 메모리 할당(memory allocation) 등 프로그램의 동적인 상태 정보를 포함하는 구조화된 데이터입니다. 이러한 추적은 특정 형식(예: JSON 또는 XML과 유사한 구조)으로 인코딩되어 LLM의 입력 및 출력 토큰 시퀀스로 사용됩니다. CWM은 이러한 실행 추적을 예측하도록 훈련됨으로써, 코드의 동작을 시뮬레이션하고 잠재적인 오류나 예상치 못한 결과를 미리 파악할 수 있습니다. 이는 코드 생성뿐만 아니라, 기존 코드의 취약점을 탐지하거나, 특정 버그(bug)의 원인을 진단하고, 심지어는 자동으로 코드를 수정하는 데도 활용될 수 있습니다. 예를 들어, CWM은 주어진 코드에 대해 다양한 입력으로 실행 추적을 생성하고, 예상 결과와 다른 추적을 통해 버그를 식별할 수 있습니다. 또한, 프로그램의 특정 목표를 달성하기 위한 코드 패치(patch)를 제안하고, 이 패치가 실제로 원하는 동작을 하는지 내부적으로 시뮬레이션하여 검증할 수도 있습니다. 이러한 능력은 소프트웨어 개발 프로세스를 혁신하고, 더욱 강력하고 신뢰할 수 있는 코딩 AI 시스템을 구축하는 데 중요한 역할을 할 것입니다.

**5. 작은 재귀 트랜스포머(Small Recursive Transformers)**
여러분은 이전의 모든 접근 방식이 여전히 트랜스포머 아키텍처(transformer architecture)를 기반으로 한다는 것을 알아차렸을 것입니다. 이 마지막 섹션의 주제도 마찬가지이지만, 앞서 논의한 모델들과는 대조적으로, 이들은 추론(reasoning)을 위해 설계된 작고 전문화된 트랜스포머(transformers)입니다. 네, 추론 중심 아키텍처(reasoning-focused architectures)가 항상 클 필요는 없습니다. 사실, 계층적 추론 모델(Hierarchical Reasoning Model, HRM)과 함께 작은 재귀 트랜스포머(recursive transformers)에 대한 새로운 접근 방식이 최근 연구 커뮤니티에서 많은 주목을 받았습니다. 그림 21: LLM 환경 개요; 이 섹션은 작은 재귀 트랜스포머(small recursive transformers)를 다룹니다. 더 구체적으로, HRM 개발자들은 매우 작은 트랜스포머 모델(transformer models, 단 4개의 블록만 있는)조차도 답변을 단계별로 정제하도록 훈련될 때 (특정 문제에 대해) 인상적인 추론 능력(reasoning capabilities)을 개발할 수 있음을 보여주었습니다. 이는 ARC 챌린지에서 최고 순위를 차지했습니다. 그림 22: arcprize.org/arc-agi/1에서 가져온 ARC-AGI 1 작업 예시(상단)와 arcprize.org/blog/hrm-analysis에서 가져온 리더보드에 순위가 매겨진 계층적 추론 모델(Hierarchical Reasoning Model, HRM)(하단). HRM과 같은 재귀 모델(recursive models)의 아이디어는 한 번의 순방향 전달(forward pass)로 답변을 생성하는 대신, 모델이 재귀적인 방식(recursive fashion)으로 자체 출력을 반복적으로 정제한다는 것입니다. (이 과정의 일부로, 각 반복은 잠재 표현(latent representation)을 정제하며, 저자들은 이를 모델의 "사고" 또는 "추론" 과정으로 봅니다.) 첫 번째 주요 예시는 여름 초의 HRM이었고, 이어서 Mixture-of-Recursions (MoR) 논문이 나왔습니다. 그리고 가장 최근에는 "Less is More: Recursive Reasoning with Tiny Networks" (2025년 10월)가 Tiny Recursive Model (TRM, 아래 그림에 설명됨)을 제안하는데, 이는 ARC 벤치마크(benchmark)에서 훨씬 더 나은 성능을 보이는 더 간단하고 훨씬 작은 모델입니다 (700만 개의 매개변수, HRM보다 약 4배 작음). 그림 23: Tiny Recursive Model (TRM). https://arxiv.org/abs/2510.04871에서 가져온 주석이 달린 그림. 이 섹션의 나머지 부분에서는 TRM을 좀 더 자세히 살펴보겠습니다.

**5.1 여기서 재귀란 무엇을 의미하는가?(What Does Recursion Mean Here?)**
TRM은 두 가지 교대 업데이트(alternating updates)를 통해 답변을 정제합니다: 현재 질문과 답변에서 잠재 추론 상태(latent reasoning state)를 계산합니다. 그런 다음 해당 잠재 상태(latent state)를 기반으로 답변을 업데이트합니다. 훈련은 배치(batch)당 최대 16번의 정제 단계(refinement steps) 동안 실행됩니다. 각 단계는 답변을 반복적으로 정제하기 위해 여러 번의 no-grad 루프(loops)를 수행합니다. 이어서 전체 추론 시퀀스(reasoning sequence)를 통해 역전파(backpropagates)하여 모델 가중치(model weights)를 업데이트하는 경사 루프(gradient loop)가 진행됩니다. TRM이 텍스트를 처리하는 언어 모델(language model)이 아니라는 점에 유의하는 것이 중요합니다. 하지만 (a) 트랜스포머 기반 아키텍처(transformer-based architecture)이고, (b) 추론(reasoning)이 이제 LLM 연구의 핵심 초점이며 이 모델은 추론에 대한 분명히 다른 접근 방식을 나타내고, (c) 많은 독자들이 HRM을 다루어 달라고 요청했기 때문에 (그리고 TRM은 HRM의 더 발전된 후속 모델이므로) 여기에 포함하기로 결정했습니다. TRM은 미래에 텍스트 기반 질문-답변 작업(textual question-answer tasks)으로 확장될 수 있지만, 현재 TRM은 그리드 기반 입력(grid-based inputs) 및 출력(outputs)에서 작동합니다. 다시 말해, "질문"과 "답변" 모두 이산 토큰(discrete tokens)의 그리드(예: 9×9 스도쿠 또는 30×30 ARC/미로 퍼즐)이며, 텍스트 시퀀스(text sequences)가 아닙니다.

재귀적 추론 모델은 인간의 문제 해결 방식과 유사한 접근 방식을 모방합니다. 인간은 복잡한 문제를 한 번에 해결하기보다는, 여러 단계를 거쳐 점진적으로 해결책을 정제해 나가는 경향이 있습니다. TRM은 이러한 반복적인 정제 과정을 신경망 아키텍처 내에 내재화하여, 각 반복에서 현재의 부분적인 해결책을 평가하고 개선된 잠재 상태를 바탕으로 다음 단계의 해결책을 생성합니다. "잠재 추론 상태(latent reasoning state)"는 모델의 내부적인 "사고 과정"을 나타내며, 이는 문제 해결의 현재 진행 상황과 다음 단계를 위한 전략을 요약하는 역할을 합니다. 이러한 재귀적 구조는 모델이 제한된 자원으로도 복잡한 추론 문제를 해결할 수 있게 하며, 특히 ARC 챌린지나 스도쿠와 같이 명확한 규칙과 목표를 가진 퍼즐에서 강력한 성능을 발휘합니다. 또한, 각 정제 단계가 명시적으로 구분되므로, 모델의 추론 과정을 더 쉽게 분석하고 해석할 수 있다는 장점도 있습니다. 이러한 투명성은 복잡한 AI 시스템의 신뢰성과 디버깅 가능성을 높이는 데 기여합니다.

**5.2 TRM은 HRM과 어떻게 다른가?(How Does TRM Differ From HRM?)**
HRM은 재귀 수준(recursion levels)을 통해 통신하는 두 개의 작은 트랜스포머 모듈(transformer modules, 각 4개 블록)로 구성됩니다. TRM은 단일 2계층 트랜스포머(2-layer transformer)만 사용합니다. (이전 TRM 그림에는 트랜스포머 블록(transformer block) 옆에 4×가 표시되어 있지만, 이는 HRM과 비교하기 쉽게 하기 위한 것일 가능성이 높다는 점에 유의하십시오.) TRM은 모든 재귀 단계(recursive steps)를 통해 역전파(backpropagates)하는 반면, HRM은 마지막 몇 단계만 역전파합니다. HRM은 반복을 언제 멈출지 결정하는 명시적인 정지 메커니즘(halting mechanism)을 포함합니다. TRM은 이 메커니즘을 반복을 언제 멈출지 학습하는 간단한 이진 교차 엔트로피 손실(binary cross-entropy loss)로 대체합니다. 성능 면에서 TRM은 아래 그림에서 보듯이 HRM에 비해 매우 우수한 성능을 보입니다. 그림 24: 계층적 추론 모델(Hierarchical Reasoning Model, HRM)과 Tiny Recursive Model (TRM)의 성능 비교. 이 논문은 놀라울 정도로 많은 절제 연구(ablation studies)를 포함했으며, 이는 몇 가지 흥미로운 추가 통찰력을 제공했습니다. 제게 특히 눈에 띄었던 두 가지는 다음과 같습니다:
*   레이어가 적을수록 더 나은 일반화(generalization)로 이어집니다.
    *   4개 레이어에서 2개 레이어로 줄이면 스도쿠 정확도(Sudoku accuracy)가 79.5%에서 87.4%로 향상되었습니다.
*   어텐션(Attention)은 필요하지 않습니다.
    *   자기 어텐션(self-attention)을 순수 MLP 레이어(MLP layer)로 대체하는 것도 정확도(accuracy)를 향상시켰습니다 (74.7%에서 87.4%로).
    *   하지만 컨텍스트(context)가 작고 고정된 길이기 때문에 여기에서만 가능합니다.

"어텐션이 필요하지 않다"는 TRM의 발견은 대규모 LLM의 맥락에서는 다소 충격적일 수 있지만, TRM이 다루는 특정 작업의 특성에서 그 이유를 찾을 수 있습니다. TRM은 그리드 기반의 고정된 크기 입력(예: 9x9 스도쿠)을 처리하며, 이러한 작업은 상대적으로 작은 컨텍스트 내에서 명확한 지역적(local) 관계와 규칙 기반의 추론을 요구합니다. 이러한 환경에서는 모든 토큰 간의 복잡한 전역적(global) 상호작용을 모델링하는 어텐션 메커니즘의 이점보다, 단순하고 효율적인 MLP 레이어가 국소적인 특징을 추출하고 변환하는 데 더 효과적일 수 있습니다. MLP는 각 위치의 정보를 독립적으로 처리하면서도, 재귀적 반복을 통해 점진적으로 전역적인 일관성을 구축할 수 있습니다. 이는 제한된 자원과 작은 데이터셋으로도 강력한 추론 능력을 발휘할 수 있는 경량 모델을 설계하는 데 중요한 통찰을 제공합니다. 또한, "레이어가 적을수록 더 나은 일반화"라는 발견은 과적합(overfitting)을 방지하고, 모델이 특정 훈련 데이터에만 의존하지 않고 새로운 문제에 유연하게 대응할 수 있도록 하는 데 중요한 시사점을 줍니다.

**5.3 더 큰 그림(The Bigger Picture)**
HRM과 TRM이 이러한 벤치마크(benchmarks)에서 정말 좋은 추론 성능(reasoning performance)을 달성하지만, 이들을 대규모 LLM과 비교하는 것은 공정하지 않습니다. HRM과 TRM은 ARC, 스도쿠, 미로 길 찾기와 같은 작업을 위한 전문화된 모델인 반면, LLM은 제너럴리스트(generalists)입니다. 물론 HRM과 TRM도 다른 작업에 채택될 수 있지만, 각 작업에 대해 특별히 훈련되어야 합니다. 따라서 그런 의미에서 HRM과 TRM은 효율적인 포켓 계산기로 생각할 수 있고, LLM은 다른 많은 일도 할 수 있는 컴퓨터와 같다고 볼 수 있습니다. 그럼에도 불구하고, 이러한 재귀 아키텍처(recursive architectures)는 작고 효율적인 모델이 반복적인 자기 정제(self-refinement)를 통해 어떻게 "추론"할 수 있는지를 보여주는 흥미로운 개념 증명(proof-of-concepts)입니다. 아마도 미래에는 이러한 모델이 더 큰 도구 사용 LLM 시스템(tool-using LLM systems) 내에 내장된 추론 또는 계획 모듈(reasoning or planning modules) 역할을 할 수 있을 것입니다. 현재로서는 LLM이 광범위한 작업에 이상적이지만, TRM과 같은 도메인별 재귀 모델(domain-specific recursive models)은 대상 도메인이 잘 이해되면 특정 문제를 더 효율적으로 해결하기 위해 개발될 수 있습니다. 스도쿠, 미로 찾기, ARC 개념 증명 벤치마크(proof-of-concept benchmarks) 외에도, 이러한 모델이 사용될 수 있는 물리학 및 생물학 도메인(physics and biology domain)에는 많은 사용 사례가 있을 수 있습니다. 흥미로운 사실로, 저자는 이 모델을 훈련하는 데 4개의 H100으로 약 2일 동안 500달러 미만이 들었다고 공유했습니다. 데이터 센터(data center) 없이도 흥미로운 작업을 할 수 있다는 사실에 기쁩니다.

**6. 결론(Conclusion)**
원래는 개요 그림의 모든 모델 범주를 다룰 계획이었지만, 글이 예상보다 길어져서 xLSTM, Liquid Foundation Models, 트랜스포머-RNN 하이브리드(Transformer-RNN hybrids), 상태 공간 모델(State Space Models)은 다음 기회로 미루어야 할 것 같습니다 (비록 Gated DeltaNet이 이미 상태 공간 모델과 순환 설계(recurrent designs)의 맛을 보여주었지만요). 이 글의 결론으로, 저는 앞서 언급했던 말을 반복하고 싶습니다. 즉, 표준 자기회귀 트랜스포머 LLM(autoregressive transformer LLMs)은 검증되었고 지금까지 시간의 시험을 견뎌냈다는 것입니다. 또한 효율성이 주요 요인이 아니라면, 현재로서는 우리가 가진 최고의 것입니다.

**전통적인 디코더 스타일(Decoder-Style), 자기회귀 트랜스포머(Autoregressive Transformers)**
+ 검증되고 성숙한 도구
+ "잘 이해됨"
+ 스케일링 법칙(Scaling laws)
+ SOTA
- 비싼 훈련
- 비싼 추론 (앞서 언급된 트릭 제외)

오늘 새로운 LLM 기반 프로젝트를 시작한다면, 자기회귀 트랜스포머 기반 LLM(autoregressive transformer-based LLMs)이 저의 첫 번째 선택이 될 것입니다. 저는 다가오는 어텐션 하이브리드(attention hybrids)가 매우 유망하다고 생각하며, 효율성이 주요 관심사인 더 긴 컨텍스트(contexts)로 작업할 때 특히 흥미롭습니다.

**선형 어텐션 하이브리드(Linear Attention Hybrids)**
+ 디코더 스타일 트랜스포머와 동일
+ 긴 컨텍스트 작업에서 FLOPs/KV 메모리 절감
- 복잡성 증가
- 효율성을 위해 약간의 정확도 희생

더 극단적인 측면에서는 텍스트 확산 모델(text diffusion models)이 흥미로운 발전입니다. 저는 몇 가지 빠른 데모만 시도해 보았기 때문에, 일상적인 사용에서 얼마나 잘 작동할지에 대해서는 여전히 다소 회의적입니다. 바라건대, 곧 Google의 Gemini Diffusion을 통한 대규모 프로덕션 배포를 보게 될 것이며, 이를 일상 및 코딩 작업에서 테스트하여 사람들이 실제로 어떻게 느끼는지 알아낼 수 있기를 바랍니다.

**텍스트 확산 모델(Text Diffusion Models)**
+ 반복적인 노이즈 제거는 텍스트에 대한 신선한 아이디어
+ 더 나은 병렬 처리 (다음 토큰 의존성 없음)
- 답변 스트리밍 불가
- CoT의 이점 없음?
- 까다로운 도구 호출?
- 견고한 모델이지만 SOTA 아님

텍스트 확산 모델(text diffusion models)의 주요 강점이 효율성 개선인 반면, 코드 월드 모델(code world models)은 스펙트럼의 다른 끝에 위치하며, 모델링 성능(modeling performance) 향상을 목표로 합니다. 이 글을 쓰는 시점에서, 표준 LLM을 기반으로 하는 코딩 모델(coding models)은 주로 추론 기술(reasoning techniques)을 통해 개선되지만, 더 까다로운 문제에 시도해 보았다면, 이들이 (다소) 여전히 부족하고 많은 까다로운 코딩 문제를 잘 해결하지 못한다는 것을 알아차렸을 것입니다. 저는 코드 월드 모델(code world models)이 특히 흥미롭다고 생각하며, 더 유능한 코딩 시스템(coding systems)을 개발하는 데 중요한 다음 단계가 될 수 있다고 믿습니다.

**코드 월드 모델(Code World Model)**
+ 코드 이해도 향상을 위한 유망한 접근 방식
+ 검증 가능한 중간 상태
- 실행 가능한 코드 추적 포함으로 훈련 복잡성 증가
- 코드 실행으로 지연 시간 추가

마지막으로, 계층적 및 작은 추론 모델(tiny reasoning models)과 같은 작은 재귀 트랜스포머(recursive transformers)를 다루었습니다. 이들은 매우 흥미로운 개념 증명(proof-of-concept) 모델입니다. 하지만 오늘날 이들은 주로 퍼즐 해결사이며, 일반 텍스트 또는 코딩 모델(coding models)이 아닙니다. 따라서 이 글에서 다룬 다른 비표준 LLM 대안들과 같은 범주에 속하지 않습니다. 그럼에도 불구하고, 이들은 매우 흥미로운 개념 증명이며, 연구자들이 이들을 연구하고 있다는 사실에 기쁩니다. 현재 GPT-5, DeepSeek R1, Kimi K2 등과 같은 LLM은 자유 형식 텍스트, 코드, 수학 문제 등을 위한 특수 목적 모델(special purpose models)로 개발되고 있습니다. 이들은 일반 지식 질문부터 수학 및 코드에 이르기까지 다양한 작업에 사용하는 무차별 대입(brute-force) 및 만능 접근 방식(jack-of-all-trades approach)처럼 느껴집니다. 하지만 동일한 작업을 반복적으로 수행할 때, 이러한 무차별 대입 접근 방식은 비효율적이 되고 전문화(specialization) 측면에서도 이상적이지 않을 수 있습니다. 여기서 작은 재귀 트랜스포머(tiny recursive transformers)가 흥미로워집니다: 이들은 반복적이거나 구조화된 추론 작업(reasoning tasks)에 효율적이고 목적에 맞게 구축된 경량의 작업별 모델(task-specific models) 역할을 할 수 있습니다. 또한, 저는 이들을 다른 도구 호출 LLM(tool-calling LLMs)을 위한 잠재적인 "도구"로 볼 수 있습니다. 예를 들어, LLM이 수학 문제를 해결하기 위해 Python 또는 계산기 API를 사용할 때, 특수 작은 추론 모델(tiny reasoning models)이 다른 유형의 퍼즐 또는 추론과 유사한 문제에 대한 이러한 틈새를 채울 수 있습니다.

**작은 재귀 트랜스포머(Small Recursive Transformers)**
+ 매우 작은 아키텍처
+ 퍼즐에 대한 좋은 일반화
- 특수 목적 모델
- (현재까지는) 퍼즐에 한정됨

긴 글이었지만, 주류 LLM의 스포트라이트 밖에서 종종 머무는 매혹적인 접근 방식들을 발견하셨기를 바랍니다. 그리고 다소 전통적인 LLM 출시로 인해 약간 지루함을 느끼셨다면, 지금 많은 흥미로운 작업이 진행되고 있으므로 이 글이 AI에 대한 여러분의 흥미를 다시 불러일으키는 데 도움이 되었기를 바랍니다! 이 잡지는 개인적인 열정 프로젝트이며, 여러분의 지원이 이를 유지하는 데 도움이 됩니다. 제 작업을 지원하고 싶으시다면, 저의 "Build a Large Language Model (From Scratch)" 책 또는 그 후속작인 "Build a Reasoning Model (From Scratch)"을 고려해 주십시오. (저는 여러분이 이 책들에서 많은 것을 얻을 것이라고 확신합니다. 이 책들은 다른 곳에서는 찾을 수 없는 깊이로 LLM이 어떻게 작동하는지 설명합니다.) 읽어주셔서 감사하며, 독립적인 연구를 지원해 주셔서 감사합니다! "Build a Large Language Model (From Scratch)"은 현재 Amazon에서 구매 가능합니다. "Build a Reasoning Model (From Scratch)"은 Manning에서 얼리 액세스(Early Access) 중입니다. 책을 읽으셨고 잠시 시간이 있으시다면, 짧은 리뷰를 남겨주시면 정말 감사하겠습니다. 이는 저희 저자들에게 큰 도움이 됩니다! 여러분의 지원은 큰 의미가 있습니다! 감사합니다!