최근 몇 달간 추론 모델(reasoning models) 관련 글을 여러 차례 선보였습니다(연속 4편)! "에이전트(agentic)"와 더불어 추론 능력은 2025년 거대 언어 모델(LLM) 분야의 핵심 화두 중 하나로 자리매김할 것입니다. 그러나 이번 달에는 LLM의 작동 원리를 깊이 있게 이해하는 데 가장 효과적인 방법 중 하나인 LLM 코딩에 대한 보다 본질적이고 근본적인 내용을 독자 여러분과 나누고자 합니다. 작년에 제가 진행했던 간략한 LLM 워크숍, 즉 **밑바닥부터 LLM 구축하기: 3시간 코딩 워크숍** (Sebastian Raschka, PhD · 2024년 8월 31일 전체 스토리 읽기)이 많은 분들께 호평을 받고 유용하다는 피드백을 받았기 때문입니다. 이에 따라, 기존 워크숍보다 약 5배 더 확장되고 상세해진 총 15시간 분량의 새로운 콘텐츠가 훨씬 더 큰 도움이 될 것이라고 확신합니다.

덧붙여, 안타깝게도 저는 지난 3주 동안 심각한 목 부상으로 인해 컴퓨터 작업을 제대로 수행할 수 없었습니다. 현재는 권장되는 수술적 치료를 고려하기에 앞서 보존적 치료 방식을 시도하고 있습니다. 이제 막 정상적인 업무 흐름을 되찾으려던 시점에서 예상치 못한 또 다른 난관에 부딪히게 되어 매우 아쉬운 상황입니다. 이러한 회복 기간 동안, 지난 몇 달에 걸쳐 미리 녹화해 두었던 영상들을 공유하는 것이 시의적절한 중간 콘텐츠가 될 것이라고 판단했습니다. 이 자료들이 여러분께 유익하기를 바라며, 지속적인 성원에 깊이 감사드립니다. 창작자에게 있어 신체적, 정신적 건강은 지속적인 콘텐츠 제작의 기반이 됩니다. 미리 준비된 양질의 자료는 예기치 못한 상황에서도 독자들과의 연결을 유지하는 소중한 자산이 됩니다.

추신: 이 영상들은 원래 제 **밑바닥부터 대규모 언어 모델 구축하기(Build a Large Language Model (From Scratch))** 책의 보충 자료로 시작되었습니다. 하지만 독립적인 콘텐츠로도 꽤 잘 작동한다는 것을 알게 되었습니다. 왜 LLM을 밑바닥부터 구축해야 할까요? 아마도 LLM이 실제로 어떻게 기능하는지 학습하는 가장 효과적이고 능률적인 방식일 것입니다. 게다가, 많은 독자들이 이 과정을 통해 상당한 즐거움을 경험했다고 전해왔습니다. 비유하자면, 자동차에 흥미가 있고 그 작동 방식을 깊이 이해하고 싶다면, 처음부터 자동차를 조립하는 과정을 안내하는 튜토리얼을 따르는 것이 탁월한 학습 경로가 됩니다. 물론, 첫 프로젝트부터 포뮬러 1(Formula 1) 경주용 자동차를 만드는 것을 목표로 삼지는 않을 것입니다. 이는 엄청난 비용과 복잡성을 수반할 테니까요. 대신, 고카트(go-kart)와 같이 더 단순한 구조물로 시작하는 것이 훨씬 합리적입니다. 고카트를 직접 제작하는 것만으로도 조향 시스템의 원리, 모터의 기능 등 핵심적인 메커니즘을 배울 수 있습니다. 이는 전문 경주용 자동차를 운전하기 전(또는 자동차 제작에 특화된 회사나 팀에 합류하기 전)에 트랙에서 연습하고 즐거움을 얻는 데 큰 도움이 됩니다. 결국, 최고의 경주 드라이버들은 종종 자신만의 고카트를 만들고 손보면서 경력을 시작했습니다(미하엘 슈마허(Michael Schumacher)와 아일톤 세나(Ayrton Senna)를 떠올려 보세요). 이 과정을 통해 그들은 차량에 대한 뛰어난 감각을 개발했을 뿐만 아니라, 정비사들에게 귀중한 피드백을 제공하여 다른 드라이버들보다 우위를 점할 수 있었습니다. LLM 개발에서도 이와 유사하게, 프레임워크의 추상화 뒤에 숨겨진 본질적인 원리를 이해하는 것은 모델의 동작을 진정으로 파악하고, 문제 해결 능력을 향상시키며, 궁극적으로 혁신적인 모델을 설계하는 데 필수적입니다. 단순히 고수준 API를 사용하는 것과는 차원이 다른 통찰력을 제공합니다.

**참고 자료**
*   밑바닥부터 LLM 구축하기 책 (Manning | Amazon)
*   밑바닥부터 LLM 구축하기 GitHub 저장소
이러한 보충 자료들은 영상 콘텐츠의 이해를 심화하고, 코드를 직접 실행하며 학습을 더욱 효과적으로 만드는 데 기여합니다.

**1 - 코딩 환경 설정 (0:21:01)**
본 영상은 uv를 활용하여 파이썬(Python) 개발 환경을 구축하는 방법을 설명하는 보충 자료입니다. 특히, 이 문서에 서술된 "uv pip" 사용법을 집중적으로 다룹니다. 또는, 기본적인 "uv add" 구문(syntax) (영상에서 언급되지만 명시적으로 다루지 않음)은 여기에서 설명됩니다. LLM 개발에서 일관성 있고 재현 가능한 환경 설정은 프로젝트 성공의 첫걸음입니다. 도커(Docker)와 같은 컨테이너화 기술을 활용하면 의존성 충돌을 방지하고 팀원 간의 협업 효율성을 높일 수 있습니다.
참고 / 팁: 설치 시 특정 버전의 윈도우(Windows) 환경에서 문제가 발생할 수 있습니다. 윈도우(Windows) 컴퓨터를 사용 중이며 설치에 어려움을 겪고 있다면 (영상 5에서 OpenAI의 오리지널 GPT-2 모델 가중치(weights)를 로드하기 위한 텐서플로우(TensorFlow) 종속성(dependency) 때문일 가능성이 높음), 걱정하지 마시고 텐서플로우(TensorFlow) 설치를 건너뛰셔도 됩니다 (요구 사항(requirements) 파일에서 텐서플로우(TensorFlow) 줄을 제거하여 이 작업을 수행할 수 있습니다). 대안을 제공하기 위해, 저는 GPT-2 모델 가중치(weights)를 텐서플로우(TensorFlow) 텐서(tensor) 형식에서 파이토치(PyTorch) 텐서(tensors)로 변환하여 허깅 페이스(Hugging Face) 모델 허브(model hub)에 공유했습니다. 이는 영상 5의 가중치(weight) 로딩 부분에 대한 대안으로 활용할 수 있습니다: https://huggingface.co/rasbt/gpt2-from-scratch-pytorch. 어쨌든, 영상 5의 끝까지는 이 가중치(weight) 로딩 코드에 대해 걱정할 필요가 없습니다.

**2 - 텍스트 데이터 작업 (1:28:01)**
이 영상은 LLM 훈련을 위한 텍스트 데이터 준비 절차(토큰화(tokenization), 바이트 페어 인코딩(byte pair encoding), 데이터 로더(data loaders) 등)를 상세히 설명합니다. LLM의 성능은 학습 데이터의 품질과 전처리 방식에 크게 좌우됩니다. 데이터 정제, 증강, 그리고 다양한 토큰화 전략은 모델이 언어를 이해하고 생성하는 능력에 직접적인 영향을 미칩니다.

**3 - 어텐션 메커니즘(attention mechanisms) 코딩 (2:15:40)**
본 영상은 어텐션 메커니즘(attention mechanisms) (셀프 어텐션(self-attention), 인과적 어텐션(causal attention), 멀티 헤드 어텐션(multi-head attention))이 어떻게 작동하는지 밑바닥부터 코딩하여 설명하는 보충 영상입니다. 이를 자동차의 엔진을 만드는 과정(프레임, 좌석, 바퀴를 추가하기 전)으로 비유할 수 있습니다. 어텐션은 트랜스포머(Transformer) 아키텍처의 핵심 요소로, 최근에는 효율성을 높인 스파스 어텐션(sparse attention)이나 선형 어텐션(linear attention) 등 다양한 변형이 연구되고 있습니다.

**4 - LLM 아키텍처 코딩 (0:21:01)**
이 영상은 밑바닥부터 LLM 아키텍처(architecture)를 구현하는 방법을 다룹니다. 트랜스포머 기반의 디코더 전용(decoder-only) 아키텍처는 현대 LLM의 표준으로 자리 잡았으며, 각 레이어의 역할과 상호작용을 이해하는 것이 중요합니다.

**5 - 레이블 없는 데이터로 사전 훈련 (2:36:44)**
이 영상은 밑바닥부터 LLM을 사전 훈련하는 방법을 설명합니다. 대규모 비지도 학습을 통해 모델은 방대한 텍스트에서 언어의 통계적 패턴과 의미론적 관계를 학습하며, 이는 전이 학습(transfer learning)의 기반이 됩니다.

**6 - 분류를 위한 미세 조정(Finetuning) (2:15:29)**
이 영상은 다음 영상에서 LLM을 명령어 미세 조정(instruction finetuning)하기 전에, 미세 조정(finetuning)에 대한 부드러운 소개로 LLM을 분류기(classifier)로 미세 조정(fine-tune)하는 방법(여기서는 스팸 분류 예시를 사용)을 설명합니다. 미세 조정은 특정 작업에 모델을 최적화하는 과정으로, LoRA(Low-Rank Adaptation)나 QLoRA와 같은 효율적인 기법들이 활발히 연구되고 있습니다.

**7 - 명령어 미세 조정(Instruction Finetuning) (1:46:04)**
마지막으로, 이 영상은 LLM을 명령어 미세 조정(instruction finetune)하는 방법을 설명합니다. 사전 훈련된 모델이 특정 지시를 이해하고 따르도록 훈련하는 이 과정은 RLHF(Reinforcement Learning from Human Feedback)나 DPO(Direct Preference Optimization)와 같은 정렬(alignment) 기술을 통해 사용자의 의도에 부합하는 응답을 생성하도록 만듭니다.

이 영상들을 통해 즐겁게 학습하고 직접 실험해 보시기를 바랍니다!

**보너스: LLM의 과거와 현재 (2018년부터 2025년까지)**
유료 구독자분들께 깊은 감사의 마음을 담아, 라마 4(Llama 4) 출시 약 2일 후인 4월 초에 녹화했던 2.5시간 분량의 (코딩이 아닌) 보너스 영상을 공유하고자 합니다. 이 강연에서는 2018년 GPT-2 이후로 무엇이 어떻게 변화했는지에 초점을 맞춰 2025년 현재의 LLM 환경에 대해 논의합니다. 이 영상은 LLM 기술의 주요 전환점과 미래 전망을 이해하는 데 귀중한 통찰력을 제공할 것입니다. 독립적인 연구자이자 콘텐츠 제작자로서 여러분의 지지는 저에게 정말 큰 의미가 있습니다! 다가오는 글들에 대한 아이디어가 많고 빨리 작업하고 싶으니, 앞으로 몇 주/몇 달 안에 상황이 나아지기를 기대합니다! LLM 분야는 끊임없이 진화하고 있으며, 멀티모달(multimodal) 능력, 소형 모델(small language models)의 발전, 그리고 윤리적 AI 구현 등 다양한 방향으로 나아가고 있습니다. 이러한 변화의 흐름을 이해하고 지속적으로 학습하는 것이 중요합니다.