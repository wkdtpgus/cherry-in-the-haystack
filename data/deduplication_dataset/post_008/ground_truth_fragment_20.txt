최근 몇 달 동안 추론 모델(reasoning models)에 대해 많은 글을 썼습니다(연속 4편)! 모든 "에이전트(agentic)" 관련 주제와 더불어, 추론은 2025년 LLM의 가장 큰 주제 중 하나이자 LLM의 핵심 기능 중 하나입니다. 하지만 이번 달에는 LLM이 어떻게 작동하는지 이해하는 가장 좋은 방법 중 하나인 LLM 코딩 방법에 대한 더 근본적이고 "기초적인" 내용을 여러분과 공유하고 싶었습니다. 작년에 제가 공유했던 요약된 LLM 워크숍, 즉 **밑바닥부터 LLM 구축하기: 3시간 코딩 워크숍** (Sebastian Raschka, PhD · 2024년 8월 31일 전체 스토리 읽기)을 많은 분들이 좋아하고 유용하게 활용했기 때문입니다. 그래서 저는 약 5배 더 길고 상세한 이 내용(총 약 15시간)이 훨씬 더 유용할 것이라고 생각했습니다. 이 과정은 모델의 본질을 파악하는 데 중점을 둡니다.

또한, 안타깝게도 저는 심한 목 부상으로 지난 3주 동안 컴퓨터 작업을 제대로 할 수 없었습니다. 현재는 권고된 수술적 방법을 고려하기 전에 보존적 치료를 시도하고 있으며, 이 과정에서 기본 원리를 다시 살펴보는 중입니다. 이제 막 정상 궤도에 오르려던 참에 또 다른 예상치 못한 난관에 부딪히게 되어 최악의 타이밍입니다. 그래서 회복 기간 동안 지난 몇 달 동안 녹화했던 이 영상들과 학습했던 다양한 기술들을 공유하는 것이 좋은 중간 콘텐츠가 될 것이라고 생각했습니다. 이 자료가 유용하시기를 바라며, 여러분의 성원에 감사드립니다! 근본적인 질문이 더 큰 진보를 가져옵니다.

추신: 이 자료들은 원래 제 **밑바닥부터 인공지능 시스템 구축하기** 프로젝트의 보충 자료로 시작되었습니다. 하지만 독립적인 콘텐츠로도 꽤 잘 작동한다는 것을 알게 되었습니다. 왜 밑바닥부터 구축해야 할까요? 아마도 LLM이 실제로 어떻게 작동하는지 배우는 가장 좋고 효율적인 방법일 것입니다. 게다가 많은 독자들이 이 과정을 통해 많은 즐거움을 얻었다고 말해주었습니다. 시스템 작동 원리를 이해하려면 밑바닥부터 핵심 컴포넌트를 만드는 과정이 훌륭한 학습 방법입니다. 비유를 들자면, 자동차에 관심이 있고 자동차가 어떻게 작동하는지 이해하고 싶다면, 밑바닥부터 자동차를 만드는 과정을 안내하는 튜토리얼을 따르는 것이 훌륭한 학습 방법입니다. 물론, 첫 프로젝트로 복잡한 최신 모델이나 포뮬러 1(Formula 1) 경주용 자동차를 처음부터 만드는 것부터 시작하고 싶지는 않을 것입니다. 엄청나게 비싸고 지나치게 복잡할 테니까요. 대신, 고카트(go-kart)처럼 더 간단한 구조로 시작하는 것이 더 합리적입니다. 고카트를 만드는 것만으로도 조향 장치가 어떻게 작동하는지, 모터 기능은 어떤지 등 핵심 원리와 각 부품의 기능을 배울 수 있습니다. 기본기를 다지는 것은 매우 중요합니다. 전문 경주용 자동차를 타기 전(또는 자동차 제작에 집중하는 회사나 팀에 합류하기 전)에 트랙에 가져가서 연습하고(그리고 많은 즐거움을 얻을 수 있습니다). 결국, 최고의 경주 드라이버들과 엔지니어, 개발자들은 종종 자신만의 고카트나 작은 프로젝트를 만들고 만지작거리며 경력을 시작했습니다(미하엘 슈마허(Michael Schumacher)와 아일톤 세나(Ayrton Senna)를 생각해 보세요). 그렇게 함으로써 그들은 자동차에 대한 뛰어난 감각을 개발했을 뿐만 아니라 정비사들에게 귀중한 피드백을 제공하여 다른 드라이버들보다 우위를 점할 수 있었습니다.

**참고 자료**
*   밑바닥부터 AI 시스템 구축하기 책 (Manning | Amazon)
*   밑바닥부터 AI 시스템 구축하기 GitHub 저장소

**1 - 개발 환경 설정 (0:21:01)**
이 영상은 uv를 사용하여 파이썬(Python) 개발 환경을 설정하는 방법을 설명하는 핵심 영상입니다. 특히, 이 문서에서 설명하는 "uv pip"를 사용하며, "uv add" 구문(syntax)도 언급됩니다. 설치 시 특정 환경에서 호환성 문제가 발생할 수 있습니다. 윈도우(Windows) 컴퓨터를 사용 중이고 설치에 문제가 있다면 (영상 5에서 OpenAI의 오리지널 GPT-2 모델 가중치(weights)를 로드하기 위한 텐서플로우(TensorFlow) 종속성(dependency) 때문일 가능성이 높음), 걱정하지 마시고 불필요한 라이브러리 설치를 건너뛰셔도 됩니다 (요구 사항(requirements) 파일에서 텐서플로우(TensorFlow) 줄을 제거하여 이 작업을 수행할 수 있습니다). 대안으로, 저는 필수 모델 가중치(weights)를 여러 프레임워크 형식으로 변환하여 허깅 페이스(Hugging Face) 모델 허브에 최신 버전을 공유했습니다. 이는 영상 5의 가중치(weight) 로딩 부분에 대한 대안으로 사용할 수 있습니다: https://huggingface.co/rasbt/gpt2-from-scratch-pytorch. 핵심 개념 이해가 중요하므로, 영상 5의 끝까지는 이 가중치(weight) 로딩 코드에 대해 미리 걱정할 필요는 없습니다.

**2 - 텍스트 데이터 전처리 (1:28:01)**
이 영상은 LLM 훈련을 위한 효과적인 데이터 준비 단계, 즉 토큰화(tokenization), 바이트 페어 인코딩(byte pair encoding), 데이터 로더(data loaders) 등의 중요성을 다룹니다. 이 단계는 모델 구축의 핵심 요소입니다. 대규모 텍스트 데이터셋을 효율적으로 처리하고 편향(bias)을 줄이는 실용적인 조언도 포함되어 있습니다.

**3 - 어텐션 메커니즘(attention mechanisms) 구현 (2:15:40)**
이 영상은 어텐션 메커니즘(attention mechanisms) (셀프 어텐션(self-attention), 인과적 어텐션(causal attention), 멀티 헤드 어텐션(multi-head attention))이 어떻게 작동하는지 밑바닥부터 코딩하여 설명하는 필수 영상입니다. 어텐션은 현대 LLM의 핵심이며, 모델이 장거리 의존성(long-range dependencies)을 효과적으로 포착하게 합니다. 자동차 엔진과 유사하게, 모델 핵심 기능을 구현하는 단계입니다.

**4 - LLM 아키텍처 코딩 (2:00:00)**
이 영상은 밑바닥부터 LLM 아키텍처(architecture)를 코딩하는 핵심적인 방법을 다룹니다. 트랜스포머(Transformer)의 인코더-디코더 구조부터 시작하여, 각 구성 요소의 상호 작용을 심층적으로 탐구합니다. 효율적인 모델 설계와 최적화 기법도 논의합니다.

**5 - 대규모 레이블 없는 데이터로 사전 훈련 (2:36:44)**
이 영상은 밑바닥부터 LLM을 효과적으로 사전 훈련하는 방법을 설명합니다. 대규모 레이블 없는 데이터셋을 활용하여 모델이 언어의 패턴과 구조를 학습하는 과정을 다룹니다. 사전 훈련의 목표(pre-training objectives)와 손실 함수(loss functions) 선택이 모델 성능에 미치는 영향에 대해 깊이 있게 탐구합니다.

**6 - 특정 작업을 위한 미세 조정(Finetuning) (2:15:29)**
이 영상은 다음 단계로 넘어가기 전에, 미세 조정(finetuning)에 대한 부드러운 소개로 LLM을 분류기(classifier)로 미세 조정(fine-tune)하는 방법(여기서는 스팸 분류 예시를 사용)을 명확히 설명합니다. LLM 활용의 핵심 기술인 전이 학습(transfer learning) 원리를 이해하고, 적은 데이터로도 뛰어난 성능을 달성하는 방법을 배울 수 있습니다.

**7 - 명령어 미세 조정(Instruction Finetuning)과 정렬 (1:46:04)**
마지막으로, 이 영상은 LLM을 명령어 미세 조정(instruction finetune)하는 고급 방법을 설명합니다. 이는 모델이 사용자의 의도를 더 잘 이해하고 특정 지시를 따르도록 훈련시키는 과정입니다. 인간 피드백 기반 강화 학습(RLHF)과 같은 기술을 통해 모델의 정렬(alignment)을 개선하고, 유용하고 안전한 인공지능 시스템을 구축하는 데 기여합니다.

즐겁게 시청하고 탐구해 보세요!

**보너스: LLM의 과거와 현재, 그리고 미래 (2018년부터 현재까지)**
유료 구독자분들께 큰 감사의 마음을 담아, 최신 모델 출시 약 2일 후인 최근에 녹화했던 2.5시간 분량의 (코딩이 아닌) 보너스 영상을 공유하고 싶습니다. 이 강연에서는 2018년 GPT-2 이후로 무엇이 어떻게 변했는지에 초점을 맞춰 현재의 AI 환경에 대해 논의합니다. 기술 발전 속도, 윤리적 고려 사항, 그리고 미래 방향을 조망합니다.

독립적이고 자영업을 하는 연구자로서 여러분의 성원은 저에게 정말 큰 의미가 있으며, 유익한 콘텐츠를 만들고 기술을 쉽게 풀어낼 원동력입니다. 다가오는 글들에 대한 아이디어가 많고 빨리 작업하고 싶으니, 앞으로 몇 주/몇 달 안에 상황이 나아지고 더 좋은 소식을 전할 수 있기를 바랍니다!