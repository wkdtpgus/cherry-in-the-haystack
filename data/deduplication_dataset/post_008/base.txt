# 밑바닥부터 대규모 언어 모델(LLMs) 코딩하기: 완벽 코스

Author: Sebastian Raschka
URL: https://magazine.sebastianraschka.com/p/coding-llms-from-the-ground-up

============================================================

최근 몇 달 동안 추론 모델(reasoning models)에 대해 많은 글을 썼습니다(연속 4편)! 모든 "에이전트(agentic)" 관련 주제와 더불어, 추론은 2025년 LLM의 가장 큰 주제 중 하나입니다. 하지만 이번 달에는 LLM이 어떻게 작동하는지 이해하는 가장 좋은 방법 중 하나인 LLM 코딩 방법에 대한 더 근본적이고 "기초적인" 내용을 여러분과 공유하고 싶었습니다. 왜냐고요? 작년에 제가 공유했던 요약된 LLM 워크숍, 즉 **밑바닥부터 LLM 구축하기: 3시간 코딩 워크숍** (Sebastian Raschka, PhD · 2024년 8월 31일 전체 스토리 읽기)을 많은 분들이 좋아하고 유용하게 활용했기 때문입니다. 그래서 저는 약 5배 더 길고 상세한 이 내용(총 약 15시간)이 훨씬 더 유용할 것이라고 생각했습니다.

또한, 안타깝게도 저는 심한 목 부상으로 지난 3주 동안 컴퓨터 작업을 제대로 할 수 없었습니다. 현재는 권고된 수술적 방법을 고려하기 전에 보존적 치료를 시도하고 있습니다. 이제 막 정상 궤도에 오르려던 참에 또 다른 예상치 못한 난관에 부딪히게 되어 최악의 타이밍입니다. 그래서 회복 기간 동안 지난 몇 달 동안 녹화했던 이 영상들을 공유하는 것이 좋은 중간 콘텐츠가 될 것이라고 생각했습니다. 이 자료가 유용하시기를 바라며, 여러분의 성원에 감사드립니다!

추신: 이 영상들은 원래 제 **밑바닥부터 대규모 언어 모델 구축하기(Build a Large Language Model (From Scratch))** 책의 보충 자료로 시작되었습니다. 하지만 독립적인 콘텐츠로도 꽤 잘 작동한다는 것을 알게 되었습니다. 왜 밑바닥부터 구축해야 할까요? 아마도 LLM이 실제로 어떻게 작동하는지 배우는 가장 좋고 효율적인 방법일 것입니다. 게다가 많은 독자들이 이 과정을 통해 많은 즐거움을 얻었다고 말해주었습니다. 비유를 들자면, 자동차에 관심이 있고 자동차가 어떻게 작동하는지 이해하고 싶다면, 밑바닥부터 자동차를 만드는 과정을 안내하는 튜토리얼을 따르는 것이 훌륭한 학습 방법입니다. 물론, 첫 프로젝트로 포뮬러 1(Formula 1) 경주용 자동차를 만드는 것부터 시작하고 싶지는 않을 것입니다. 엄청나게 비싸고 지나치게 복잡할 테니까요. 대신, 고카트(go-kart)처럼 더 간단한 것으로 시작하는 것이 더 합리적입니다. 고카트를 만드는 것만으로도 조향 장치가 어떻게 작동하는지, 모터 기능은 어떤지 등을 배울 수 있습니다. 전문 경주용 자동차를 타기 전(또는 자동차 제작에 집중하는 회사나 팀에 합류하기 전)에 트랙에 가져가서 연습하고(그리고 많은 즐거움을 얻을 수 있습니다). 결국, 최고의 경주 드라이버들은 종종 자신만의 고카트를 만들고 만지작거리며 경력을 시작했습니다(미하엘 슈마허(Michael Schumacher)와 아일톤 세나(Ayrton Senna)를 생각해 보세요). 그렇게 함으로써 그들은 자동차에 대한 뛰어난 감각을 개발했을 뿐만 아니라 정비사들에게 귀중한 피드백을 제공하여 다른 드라이버들보다 우위를 점할 수 있었습니다.

**참고 자료**
*   밑바닥부터 LLM 구축하기 책 (Manning | Amazon)
*   밑바닥부터 LLM 구축하기 GitHub 저장소

**1 - 코딩 환경 설정 (0:21:01)**
이 영상은 uv를 사용하여 파이썬(Python) 환경을 설정하는 방법을 설명하는 보충 영상입니다. 특히, 이 문서에서 설명하는 "uv pip"를 사용합니다. 또는, 기본 "uv add" 구문(syntax) (이 영상에서는 언급되었지만 명시적으로 다루지는 않음)은 여기에서 설명합니다.
참고 / 팁: 설치 시 특정 버전의 윈도우(Windows)에서 문제가 발생할 수 있습니다. 윈도우(Windows) 컴퓨터를 사용 중이고 설치에 문제가 있다면 (영상 5에서 OpenAI의 오리지널 GPT-2 모델 가중치(weights)를 로드하기 위한 텐서플로우(TensorFlow) 종속성(dependency) 때문일 가능성이 높음), 걱정하지 마시고 텐서플로우(TensorFlow) 설치를 건너뛰셔도 됩니다 (요구 사항(requirements) 파일에서 텐서플로우(TensorFlow) 줄을 제거하여 이 작업을 수행할 수 있습니다). 대안을 제공하기 위해, 저는 GPT-2 모델 가중치(weights)를 텐서플로우(TensorFlow) 텐서(tensor) 형식에서 파이토치(PyTorch) 텐서(tensors)로 변환하여 허깅 페이스(Hugging Face) 모델 허브(model hub)에 공유했습니다. 이는 영상 5의 가중치(weight) 로딩 부분에 대한 대안으로 사용할 수 있습니다: https://huggingface.co/rasbt/gpt2-from-scratch-pytorch. 어쨌든, 영상 5의 끝까지는 이 가중치(weight) 로딩 코드에 대해 걱정할 필요가 없습니다.

**2 - 텍스트 데이터 작업 (1:28:01)**
이 영상은 LLM 훈련을 위한 텍스트 데이터 준비 단계(토큰화(tokenization), 바이트 페어 인코딩(byte pair encoding), 데이터 로더(data loaders) 등)를 다룹니다.

**3 - 어텐션 메커니즘(attention mechanisms) 코딩 (2:15:40)**
이 영상은 어텐션 메커니즘(attention mechanisms) (셀프 어텐션(self-attention), 인과적 어텐션(causal attention), 멀티 헤드 어텐션(multi-head attention))이 어떻게 작동하는지 밑바닥부터 코딩하여 설명하는 보충 영상입니다. 자동차의 엔진을 만드는 것(프레임, 좌석, 바퀴를 추가하기 전)으로 생각할 수 있습니다.

**4 - 코딩 환경 설정 (0:21:01)**
이 영상은 밑바닥부터 LLM 아키텍처(architecture)를 코딩하는 방법을 다룹니다.

**5 - 레이블 없는 데이터로 사전 훈련 (2:36:44)**
이 영상은 밑바닥부터 LLM을 사전 훈련하는 방법을 설명합니다.

**6 - 분류를 위한 미세 조정(Finetuning) (2:15:29)**
이 영상은 다음 영상에서 LLM을 명령어 미세 조정(instruction finetuning)하기 전에, 미세 조정(finetuning)에 대한 부드러운 소개로 LLM을 분류기(classifier)로 미세 조정(fine-tune)하는 방법(여기서는 스팸 분류 예시를 사용)을 설명합니다.

**7 - 명령어 미세 조정(Instruction Finetuning) (1:46:04)**
마지막으로, 이 영상은 LLM을 명령어 미세 조정(instruction finetune)하는 방법을 설명합니다.

즐겁게 시청하고 만지작거려 보세요!

**보너스: LLM의 과거와 현재 (2018년부터 2025년까지)**
유료 구독자분들께 큰 감사의 마음을 담아, 라마 4(Llama 4) 출시 약 2일 후인 4월 초에 녹화했던 2.5시간 분량의 (코딩이 아닌) 보너스 영상을 공유하고 싶습니다. 이 강연에서는 2018년 GPT-2 이후로 무엇이 어떻게 변했는지에 초점을 맞춰 2025년 현재의 LLM 환경에 대해 논의합니다.

독립적이고 자영업을 하는 연구자로서 여러분의 성원은 저에게 정말 큰 의미가 있습니다! 다가오는 글들에 대한 아이디어가 많고 빨리 작업하고 싶으니, 앞으로 몇 주/몇 달 안에 상황이 나아지기를 바랍니다!