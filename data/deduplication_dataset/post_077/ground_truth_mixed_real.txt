**1. SFR-DeepResearch**
이 논문은 추론에 최적화된 LLM을 자율적인 단일 에이전트(agent) 연구원으로 전환시키는 간단한 강화 학습(reinforcement learning) 레시피인 SFR-DeepResearch를 소개합니다. SFR-DeepResearch 에이전트는 검색, 정적 페이지 브라우징, Python 등 세 가지 핵심 도구만을 활용하며, 자체 컨텍스트를 효율적으로 관리합니다. 또한, 길이 정규화된 REINFORCE 목적 함수를 사용하여 합성된 단문 및 장문 작업에 대해 엔드투엔드(end-to-end)로 훈련됩니다. 결과는 FRAMES, GAIA, Humanity’s Last Exam에서 상당한 성능 향상을 보여줍니다.

에이전트(agent) 설계 및 스캐폴딩(scaffolding) 측면에서는 QwQ 및 Qwen 모델을 대상으로, 다중 턴(multi-turn) 도구 사용을 단일의 확장되는 컨텍스트 질문으로 재구성하는 전략이 적용되었습니다. 프롬프트(prompt)를 안정적으로 유지하기 위해 이전의 긴 CoT(Chain-of-Thought)를 생략하고, 컨텍스트 한계에 도달 시 `clean_memory` 도구를 통한 자체 압축 기능을 추가했습니다. 최소한의 도구 세트는 기본적인 검색 API, 하이퍼링크 기능이 없는 정적 마크다운 스크래퍼, 그리고 상태 비저장 로컬 Python 인터프리터로 제한되어 전략 학습에 도전적인 훈련 환경을 조성합니다. 구문 분석 및 구문 오류 발생 시 복구 루틴이 트리거되어 롤아웃을 정상적으로 유지하는 내결함성(fault tolerance)을 갖췄습니다.

RL 레시피는 합성된, Hotpot보다 어려운 다중 홉(multi-hop) QA와 보고서 작성 작업을 사용하며, 궤적(trajectory) 길이로 나누는 시간적 이점 정규화(temporal advantage normalization)를 포함한 그룹 REINFORCE 목적 함수를 최적화합니다. 지역화되고 캐시(cache)된 도구와 오염 차단 목록(contamination blocklist)은 훈련 및 평가를 안정화합니다. 최적 모델인 SFR-DR-20B는 FRAMES에서 82.8, GAIA(텍스트 전용)에서 66.0, HLE 전체 텍스트 전용에서 28.7을 달성하며, 당시 유사한 오픈 에이전트들을 능가하는 강력한 성능을 보였습니다. 특히, 길이 정규화 기법은 불필요한 도구 호출을 효과적으로 억제하여 보상 및 정확도 향상에 크게 기여했으며, 이는 현대 LLM 에이전트 설계에 중요한 시사점을 제공합니다. 어블레이션(ablation) 연구에서는 단일 턴(single-turn) 스캐폴딩(scaffolding)이 Qwen 및 QwQ의 기본 다중 턴(multi-turn) 템플릿(template)을 능가하며 FRAMES에서 큰 성능 향상을 보였습니다. 또한, 도구 사용 및 토큰(token) 길이 분석 결과 gpt-oss-20B가 도구를 더 많이 호출하면서도 단계별 CoT(Chain-of-Thought)를 훨씬 짧게 작성하여 Qwen 계열 모델보다 더 나은 토큰(token) 효율성(efficiency)을 나타냈습니다.
논문 | 트윗

**2. 출현적 계층적 추론(Emergent Hierarchical Reasoning)**
이 논문은 강화 학습(RL)이 출현적인 2단계 계층 구조를 통해 LLM 추론을 개선한다고 주장합니다. 첫째, 모델이 저수준 실행(execution)을 확고히 하고, 둘째, 진행은 고수준 계획(planning) 탐색에 달려 있습니다. 이를 바탕으로 저자들은 전략적 계획 토큰(token)에 대한 크레딧(credit)을 높이는 HICRA를 제안했으며, GRPO 대비 일관된 성능 향상을 보여주었습니다. 또한, 토큰 수준 엔트로피보다 더 나은 탐색 신호로 의미론적 엔트로피(semantic entropy)를 제시했습니다.

2단계 동역학(dynamic) 분석에 따르면, 초기 RL 훈련은 실행 토큰의 혼란도와 엔트로피를 감소시켜 절차적 기술을 통합하는 데 중점을 둡니다. 이후 성능 향상은 계획 토큰의 다양성 증가 및 더 길고 정확한 추적과 연관되어 "아하 모멘트" 및 길이 스케일링 효과를 설명합니다. 계획(planning) 대 실행(execution) 구분을 위해, 논문은 연역, 분기, 역추적과 같은 전략적 그램(gram)을 계획 토큰으로 기능적으로 태그(tag)하여 절차적 단계와 구별합니다. 이 라벨링은 학습 병목 현상이 전략적 사고로 이동함을 명확히 보여줍니다.

HICRA 알고리즘은 스칼라(scalar) α를 사용하여 계획 토큰에 대한 이점(advantage)을 증폭시키는 방식으로 GRPO를 수정합니다. 이는 모든 토큰에 분산하는 대신 영향력 있는 전략적 결정에 최적화를 집중시켜 목표 지향적 탐색 및 효과적인 전략의 더 빠른 강화를 유도합니다. HICRA의 결과는 Qwen, Llama 및 VLM 전반에 걸쳐 AIME24/25, Math500, AMC23 등 최신 수학 및 멀티모달 벤치마크에서 Pass@1을 개선했습니다. 이는 종종 GRPO보다 몇 점 더 높은 결과를 보이며, 의미론적 엔트로피가 전략적 탐색의 더 나은 지표임을 확인시켜 줍니다. 실행 토큰이 지배적이라 실제 탐색이 증가해도 토큰 수준 엔트로피는 감소할 수 있다는 점을 고려할 때, 전략적 그램에 대한 의미론적 엔트로피는 전략적 탐색을 더 잘 포착하고 성능과 상관관계가 높습니다. HICRA는 모델이 이미 절차적 기반을 가지고 있을 때 가장 잘 작동하며, 향후 더 높은 수준의 행동 공간, 적응형 커리큘럼 및 프로세스 지향적 보상에 대한 연구를 제안합니다.
논문 | 트윗

**3. RAG 기반 디코딩(decoding) 재고**
REFRAG는 디코드(decode) 시점에 대부분의 검색된 토큰(token)을 미리 계산된 청크(chunk) 임베딩(embedding)으로 대체한 다음, 중요한 몇몇 청크(chunk)만 선택적으로 확장하는 혁신적인 방법을 제안합니다. 이는 RAG 프롬프트(prompt)의 블록 대각선 어텐션(block-diagonal attention)을 활용하여 RAG, 다중 턴(multi-turn) 대화, 장문 문서 요약 전반에 걸쳐 정확도(accuracy)를 유지하면서 지연 시간(latency) 및 메모리(memory)를 감소시킵니다.

핵심 아이디어(idea)는 검색된 컨텍스트를 청크로 분할하고, 각 청크를 경량 인코더로 인코딩하며, 디코더의 임베딩 크기로 투영한 다음, 사용자 쿼리(query)와 함께 임베딩을 직접 공급하는 것입니다. RL 정책(policy)은 어떤 청크를 압축 해제 상태로 유지할지 결정합니다("어디서든 압축", 접두사(prefix)뿐만 아니라). 정확도(accuracy) 손실 없는 큰 속도 향상을 통해, 높은 압축률에서 LLaMA 대비 최대 30.85배(CEPE 대비 3.75배)의 첫 토큰 생성 시간 가속을 달성하며 유사한 혼란도(perplexity)를 보입니다. 처리량(throughput)은 최대 6.78배 증가합니다.

REFRAG의 압축 기법을 통해 모델은 훨씬 더 큰 컨텍스트(context)를 처리할 수 있으며(16배 확장 보고), 시퀀스(sequence) 길이가 증가함에 따라 혼란도(perplexity)를 유지하거나 개선하는 효과를 보여줍니다. 고정된 지연 시간(latency) 예산 하에서 REFRAG는 더 많은 구절을 사용하고 16개 RAG 작업에서 LLaMA 기준선(baseline)을 능가하여 RAG의 우위를 입증합니다. REFRAG는 다중 턴(multi-turn) 대화형 QA에서 더 긴 기록을 보존하고 구절 및 턴이 증가함에 따라 점수를 개선합니다. 또한, 장문 문서 요약에서도 일치하는 디코더 토큰으로 최고의 ROUGE를 달성하는 등 다양한 애플리케이션(application) 전반에 걸쳐 뛰어난 일반화(generalization) 능력을 보여주며, RAG 최적화의 새로운 방향을 제시합니다.
논문 | 트윗

**4. ACE-RL**
ACE-RL은 거친 선호도 쌍(preference-pair) 보상(reward)을 지시문별로 검증 가능한 체크리스트(checklist)로 대체하는 강화 학습(reinforcement learning) 프레임워크(framework)입니다. 이 프레임워크는 각 장문 작업을 명시적 및 암묵적 제약 조건(constraint) 세트로 전환하고, 모델의 출력이 이를 얼마나 잘 충족하는지에 따라 점수를 매깁니다. 또한 GRPO 훈련 중 길이 제어 보상(reward)과 이를 혼합하여 사용합니다. 그 결과 도메인(domain) 및 스타일(style) 전반에 걸쳐 더 강력하고 제어 가능한 장문 작성이 가능해집니다.

핵심 아이디어(idea)는 각 지시문을 세분화된 체크리스트(명시적 및 암묵적 요구 사항)로 자동 분해한 다음, 작은 LLM을 사용하여 3단계 루브릭(완전히/부분적으로/충족하지 못함)으로 각 항목을 검증하는 것입니다. 보상은 평균 체크리스트 점수와 길이 보상을 합산하여 GRPO로 최적화됩니다. ACE-RL은 관련성, 일관성, 유용성을 넘어 지시문 적응형 품질로 평가 기준을 확장하며 중요성을 갖습니다. 선호도 쌍(preference pair)이 필요 없어 비용 절감 및 확장성(scalability) 향상이라는 이점도 제공합니다.

데이터(data) 및 설정: 32K 장문 지시문, 프롬프트(prompt)당 평균 5.48개 제약 조건(constraint), 목표 길이 약 2.3K 단어. 검증기는 Qwen3-8B를 사용하며, 길이 보상(reward)은 허용 범위(tolerance band)를 벗어나는 편차에 페널티(penalty)를 부과합니다. 결과적으로 WritingBench에서 ACE-RL은 SFT 및 LLM-as-judge RL보다 모델 성능을 크게 향상시켰습니다. 예를 들어, Qwen-2.5-7B는 57.0에서 78.6으로 상승했습니다. ACE-RL로 훈련된 소형 Qwen-3-4B-thinking 모델은 여러 독점 및 글쓰기 튜닝 시스템을 능가하며, 이는 2025년 현재까지도 장문 생성 분야에서 중요한 이정표로 평가됩니다. 제약 조건 기반 보상은 LLM-as-judge보다 그룹 내 보상 분산이 높아 롤아웃 간 더 나은 구별 능력을 보였으며, 사고 모드와 ACE-RL의 조합은 비사고 모드를 능가하는 성과를 달성했습니다.
논문

**5. ParaThinker**
이 논문은 오늘날의 "더 오래 생각하기" 전략이 LLM을 단일 사고 방식에 가둔다고 주장합니다. 저자들은 여러 독립적인 추론 경로를 병렬로 생성한 다음, 이를 하나의 답변으로 융합하도록 모델을 훈련하는 ParaThinker를 제안합니다. 수학 벤치마크(benchmark) 전반에 걸쳐 이 폭 스케일링(width-scaling)은 작은 지연 시간(latency) 비용만 추가하면서 정확도(accuracy)를 높입니다.

문제 정의: 초기 토큰이 모델을 최적이 아닌 경로로 이끌 수 있는 "터널 비전(Tunnel Vision)"이라는 테스트 시간 병목 현상이 식별됩니다. 동일한 토큰 예산 하에서 다수결 방식의 병렬 샘플링이 하나의 긴 체인을 능가할 수 있습니다. ParaThinker의 방법은 병렬 추론 및 요약의 두 단계를 실행합니다. 훈련 가능한 제어 토큰 `<think i>`를 사용하고, 사고별 위치 임베딩(positional embedding)으로 다른 경로의 토큰을 명확히 합니다. 2단계 어텐션 마스크(attention mask)는 사고 중 경로를 분리하고 요약을 위해 통합하며, 재사전 채우기(re-prefill)를 피하기 위해 KV 캐시(cache)를 재사용합니다.

훈련 레시피는 교사 모델에서 샘플링된 다중 경로 추적(trace)에 대한 지도 미세 조정(supervised fine-tuning)을 수행하며, 훈련에서 본 것보다 더 많은 경로로 학생 모델이 일반화할 수 있도록 `<think i>`의 무작위 할당을 사용합니다. 결과: AIME 2024/2025, AMC 2023, MATH-500에서 ParaThinker는 고정된 경로당 예산으로 8개 경로를 사용할 때 1.5B 모델의 경우 약 12.3%, 7B 모델의 경우 7.5%만큼 순차적 기준선 대비 pass@1을 개선했습니다. ParaThinker와 다수결 투표를 결합하면 추가적인 성능 향상을 얻을 수 있어, 2025년 현재 LLM 추론의 주요 전략 중 하나로 자리매김하고 있습니다. 디코딩이 메모리 대역폭에 의해 제한되므로 경로가 많아질수록 지연 시간이 약간 증가하지만, 최적의 종료 정책은 경로 길이를 동일하게 하고 정확도와 속도를 모두 향상시키는 "first-finish"입니다. 사고 임베딩(embedding)은 중요하며, 단순하게 평탄화된 위치는 성능을 저해합니다.
논문 | 트윗

**6. AgentGym-RL**
현실적인 환경 전반에 걸쳐 강화 학습(reinforcement learning)을 통해 LLM 에이전트(agent)를 직접 훈련하기 위한 모듈형 프레임워크(framework)와, 안정성 및 성능 향상을 위해 훈련 전반에 걸쳐 상호 작용 범위(interaction horizon)를 늘리는 간단한 스케줄(schedule)인 ScalingInter-RL을 소개합니다. 결과는 7B 오픈 모델이 웹 탐색, 심층 검색, 게임, 체화(embodied) 및 과학 작업에서 더 큰 독점 시스템과 경쟁하거나 능가할 수 있음을 보여줍니다.

AgentGym-RL은 세 가지 플러그형 모듈(환경, 에이전트, 훈련)을 갖춘 통합되고 분리된 RL 스택(stack)으로 PPO, GRPO, REINFORCE++를 지원하며 WebArena, Deep Search, TextCraft, BabyAI, SciWorld 전반에서 실행됩니다. 핵심 아이디어(idea)인 ScalingInter-RL은 활용(exploitation) 및 안정적인 학습을 강조하기 위해 짧은 범위(horizon)로 시작한 다음, 탐색(exploration) 및 계획(planning)과 반성(reflection)과 같은 더 풍부한 행동을 장려하기 위해 허용된 턴(turn)을 점진적으로 증가시킵니다. 에이전트(agent) 작업에서 모델 크기만으로 확장하는 것보다 후처리 훈련 및 테스트(test) 시간 컴퓨팅(compute)이 더 잘 확장된다는 점이 중요합니다. 이 프레임워크로 훈련된 7B 모델은 약 58.6%의 평균 성공률을 달성하고 훨씬 더 큰 기준선(baseline)을 능가하는 놀라운 성과를 보여주었습니다.

결과 스냅샷: 웹 탐색에서 ScalingInter-7B는 WebArena에서 GPT-4o를 능가하는 26.00%를 기록했으며, 심층 검색에서도 GPT-4o를 넘어서는 38.25를 달성했습니다. 게임(TextCraft 91.00%), 체화(BabyAI 96.67%), 과학(SciWorld 57.00%) 등 다양한 분야에서 7B RL 모델이 강력한 성능을 보이며 SOTA(State-of-the-Art)에 근접하거나 달성했습니다. 훈련 동역학(dynamic) 연구에 따르면, 너무 이른 긴 범위(horizon)는 학습을 붕괴시킬 수 있고, 짧은 범위는 성능을 제한합니다. ScalingInter-RL은 이러한 문제들을 효과적으로 방지합니다. 병렬화된 브라우저, 리셋 훅 및 메모리 누수 수정과 같은 엔지니어링 개선 사항은 안정적인 장기 롤아웃을 가능하게 합니다. 실무자를 위한 조언으로는 희소 보상, 장기 궤적 에이전트 작업에는 REINFORCE++보다 GRPO를 선호하고, 상호 작용 길이 커리큘럼은 간단하고 견고한 이점을 제공하며, 파라미터 스케일링 전에 컴퓨팅 예산을 책정하는 것이 권장됩니다.
논문 | 트윗

**7. 대화가 항상 저렴한 것은 아니다**
다중 에이전트(multi-agent) 토론이 항상 도움이 되는 것은 아닙니다. 세 가지 추론 벤치마크(benchmark)와 이질적인 에이전트(agent) 풀(pool) 전반에 걸쳐 토론은 종종 정확도(accuracy)를 낮추며, 더 강력한 모델이 때로는 약한 동료에 의해 더 나쁜 답변으로 흔들리기도 합니다. 저자들은 현재의 정렬(alignment)이 에이전트(agent)를 너무 순응적으로 만들어 설득력 있지만 잘못된 추론을 받아들이고 도전하지 않는다고 주장합니다.

설정: GPT-4o-mini, Llama-3.1-8B-Instruct, Mistral-7B-Instruct를 사용하여 CommonSenseQA, MMLU, GSM8K에서 토론을 평가했습니다. 에이전트(agent)는 한 번 답변한 후 두 라운드(round) 동안 토론하며, 최종 출력은 토론 전후의 다수결 투표입니다. 프롬프트는 짧은 추론과 작업별 형식을 요구합니다. 주요 결과: 토론은 특히 CommonSenseQA 및 MMLU에서 정확도를 자주 저해했습니다. 혼합 능력 설정(mixed-capability setting)을 포함한 많은 그룹에서 토론 후 일관된 하락을 보여주었으며, 이는 다중 에이전트 시스템 설계 시 신중한 접근이 필요함을 시사합니다.

라운드(round)별 성능 저하: 이 연구는 라운드(round)별 정확도(accuracy)를 추적하며, 더 강력한 모델이 다수일 때도 토론이 진행됨에 따라 성능이 종종 저하됨을 보여줍니다. 에이전트(agent)는 비판보다 합의를 선호하는 경향이 있습니다. 라운드(round) 전반에 걸쳐 잘못된 답변으로의 전환(correct→incorrect)이 올바른 답변으로의 전환(incorrect→correct)보다 더 많으며, 이는 토론이 더 강력한 모델을 적극적으로 오도할 수 있음을 나타냅니다. 시사점: 순진한 토론 프로토콜(protocol)은 오류를 증폭시킬 위험이 있습니다. 저자들은 독립적인 검증에 보상하고, 에이전트의 신뢰도 또는 자신감에 따라 주장에 가중치를 부여하며, 정당하지 않은 합의에 페널티를 부과하여 토론의 이점을 보존하는 설계를 권장합니다.
논문 | 트윗

**8. AggLM**
AggLM은 여러 후보 솔루션(solution)을 집계하는 LLM 훈련에 강화 학습(reinforcement learning)을 도입하여 다수결 투표 및 보상(reward) 모델(model) 순위 지정을 넘어섭니다. 이는 더 높은 정확도(accuracy)를 달성하고, 소수 정답을 복구하며, 모델(model) 전반에 걸쳐 일반화(generalize)하고, 전통적인 집계 방법보다 적은 토큰(token)을 사용하는 효율적인 접근 방식입니다. 이러한 집계 기법은 LLM의 최종 추론 품질을 향상시키는 데 중요한 역할을 합니다.
논문 | 트윗

**9. 대규모 추론 모델(Large Reasoning Models)을 위한 RL(강화 학습) 설문 조사**
이 설문 조사는 강화 학습(reinforcement learning)이 대규모 추론 모델(LRM)의 발전을 어떻게 이끌고 있는지 검토하며, 수학 및 코딩(coding)과 같은 복잡한 작업에서 더 강력한 성능을 가능하게 합니다. 이는 계산, 알고리즘(algorithm), 데이터(data) 및 인프라(infrastructure)의 스케일링(scaling) 과제를 강조하는 동시에 인공 초지능(Artificial Superintelligence, ASI)을 향한 미래 방향을 제시합니다. 2025년 현재, 이러한 과제는 더욱 심화되고 있으며, LRM의 발전을 위한 RL의 역할은 더욱 중요해지고 있습니다.
논문 | 트윗

**10. LiveMCP-101**
LiveMCP-101은 검색, 파일 작업, 수학 및 데이터(data) 분석 전반에 걸쳐 도구 사용을 요구하는 다단계 작업에서 MCP 지원 에이전트(agent)를 테스트(test)하도록 설계된 101개의 실제 쿼리(query)로 구성된 새로운 벤치마크(benchmark)입니다. 결과는 선도적인 LLM이 60% 미만으로 성공함을 보여주며, 이는 도구 오케스트레이션(orchestration)의 주요 약점을 드러냅니다. 이 벤치마크는 자율 AI 시스템(AI system) 발전을 위한 중요한 통찰력(insight)을 제공하며, 2025년에도 여전히 LLM 에이전트의 실제 적용 능력 평가에 핵심적인 역할을 합니다.
논문 | 트윗