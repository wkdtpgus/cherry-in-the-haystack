### 1. SFR-DeepResearch: 자율 에이전트 연구원으로 변모하는 LLM

SFR-DeepResearch 연구는 추론 능력을 강화한 대규모 언어 모델(LLM)을 독립적인 단일 에이전트 연구자로 변모시키는 간단한 강화 학습(RL) 기법을 제시합니다. 해당 에이전트는 검색 기능, 고정된 웹 페이지 탐색, 파이썬 실행 환경이라는 세 가지 도구만을 활용하며, 자체적으로 문맥을 조절하고, 길이 조절이 적용된 REINFORCE 목표 함수를 기반으로 인공적으로 생성된 짧거나 긴 형태의 작업에 대해 종단간(end-to-end) 방식으로 학습됩니다. 실험 결과, FRAMES, GAIA, Humanity’s Last Exam 벤치마크에서 주목할 만한 성능 개선이 관찰되었습니다.

이 에이전트의 설계는 LLM이 복잡한 다중 턴(multi-turn) 도구 사용 시 발생하는 컨텍스트(context) 폭증 문제를 해결하기 위해, 다중 턴 도구 사용을 단일의 확장되는 컨텍스트 질문으로 재구성하고, 기존의 긴 CoT(Chain-of-Thought)를 생략하여 프롬프트(prompt) 안정성을 확보합니다. 또한, 컨텍스트가 한계에 도달할 때 스스로 압축하는 `clean_memory` 도구를 추가하여 효율적인 컨텍스트 관리를 가능하게 합니다. 이러한 최소한의 도구 세트(기본 검색 API, 하이퍼링크 없는 정적 마크다운 스크래퍼, 상태 비저장 로컬 Python 인터프리터)는 에이전트가 전략을 학습하기에 충분히 도전적인 훈련 환경을 조성합니다. 특히, 구문 분석 및 구문 오류 발생 시 복구 또는 재시도 루틴을 트리거(trigger)하여 롤아웃(rollout)을 정상적으로 유지하는 내결함성(fault tolerance)은 실제 환경에서의 견고성을 높이는 데 기여합니다.

RL 레시피는 Hotpot보다 어려운 다중 홉(multi-hop) QA 및 보고서 작성 작업을 합성하여 사용합니다. 궤적(trajectory) 길이로 나누는 시간적 이점 정규화(temporal advantage normalization)를 포함한 그룹 REINFORCE 목적 함수를 최적화하며, 궤적 필터링 및 부분 롤아웃 재사용을 수행하여 학습 효율성을 극대화합니다. 지역화되고 캐시(cache)된 도구와 오염 차단 목록(contamination blocklist)은 훈련 및 평가를 안정화하는 데 필수적인 요소입니다. 가장 우수한 성능을 보인 SFR-DR-20B 모델은 FRAMES에서 82.8점, GAIA(텍스트 기반)에서 66.0점, HLE 전체 텍스트 전용에서 28.7점을 기록하며, 기존의 공개 에이전트들을 뛰어넘고 오염 방지 목록 조건 하에서도 강력한 독점 시스템들과 견줄 만한 경쟁력을 보여주었습니다. 어블레이션(ablation) 연구에서는 단일 턴 스캐폴딩(scaffolding)이 기본 다중 턴 템플릿보다 우수하며, 길이 정규화가 보상 및 정확도를 저해하는 폭주하는 도구 호출을 효과적으로 억제함을 확인했습니다. 또한, gpt-oss-20B 모델이 Qwen 계열 모델보다 더 적은 토큰으로 단계별 CoT를 작성하여 뛰어난 토큰 효율성(efficiency)을 보였습니다. 이러한 연구는 LLM을 단순한 언어 모델이 아닌, 복잡한 문제 해결이 가능한 자율적인 지식 탐색 에이전트로 발전시키는 중요한 이정표를 제시합니다.

논문 | 트윗

---

### 2. 출현적 계층적 추론: LLM의 사고 과정 심화

본 연구는 강화 학습(RL)이 자연스럽게 발생하는 이중 계층 구조를 활용하여 대규모 언어 모델(LLM)의 추론 능력을 향상시킨다는 주장을 펼칩니다. 이는 모델이 저수준의 실행(execution)을 견고히 다진 후, 고수준의 계획(planning) 탐색에 의존하는 방식으로 이루어집니다. 이러한 통찰력을 기반으로, 연구진은 전략적 계획과 관련된 토큰에 더 높은 가중치를 부여하는 HICRA 방식을 제안하며, 이는 GRPO와 비교했을 때 꾸준한 성능 개선을 입증합니다. 나아가, 이 논문은 개별 토큰의 엔트로피보다 더 효과적인 탐색 지표로서 의미론적 엔트로피의 활용을 제안합니다.

이러한 2단계 동역학(dynamic)은 LLM의 학습 과정을 인간의 문제 해결 방식과 유사하게 만듭니다. 초기 RL 훈련은 실행 토큰의 혼란도(perplexity)와 엔트로피(entropy)를 감소시켜 모델이 기본적인 절차적 기술을 빠르게 습득하도록 돕습니다. 이후, 성능 향상은 계획 토큰의 다양성 증가 및 더 길고 정확한 추적(trace)과 일치하며, 이는 마치 인간이 '아하 모멘트(aha moments)'를 통해 새로운 통찰력을 얻는 과정과 유사하게 복잡한 문제 해결 능력이 급증하는 현상을 설명합니다. 본 연구는 전략적 그램(gram)(예: 연역, 분기, 역추적)을 계획 토큰으로 기능적으로 태그(tag)하여 절차적 단계와 구별함으로써, 학습의 병목 현상이 단순 실행에서 고차원적 전략으로 이동함을 보여줍니다.

HICRA 알고리즘은 스칼라 값인 α를 통해 계획 토큰에 할당되는 이점을 강화하는 방식으로 GRPO를 변형하여, 최적화 노력을 모든 토큰에 분산하는 대신 중요한 전략적 판단에 집중하도록 만듭니다. 이는 목표 지향적인 탐색(exploration)을 촉진하고 효과적인 전략의 강화를 가속화합니다. 이 알고리즘의 공식화는 섹션 3에서 상세히 설명됩니다. 실험 결과, Qwen, Llama, 그리고 VLM(Vision-Language Model) 전반에 걸쳐 HICRA는 AIME24/25, Math500, AMC23 및 멀티모달(multimodal) 수학 스위트(suite)와 같은 다양한 벤치마크에서 Pass@1 성능을 개선했으며, 종종 GRPO보다 몇 점 더 높은 결과를 보여주었습니다. 또한, 플롯(plot)은 더 높은 의미론적 엔트로피가 더 높은 검증 정확도(validation accuracy)를 추적함을 명확히 보여주어, 의미론적 엔트로피가 전략적 탐색을 더 잘 포착하고 성능과 강한 상관관계를 가진다는 것을 입증했습니다. 이는 실행 토큰이 지배적일 때 실제 탐색이 증가하더라도 토큰 수준 엔트로피는 감소할 수 있다는 기존의 한계를 극복하는 중요한 신호입니다.

물론, HICRA에는 한계점도 존재합니다. 이 방법은 모델이 이미 절차적 기반을 가지고 있을 때 가장 효과적으로 작동하며, 기반이 약한 모델에서는 계획에 대한 집중이 오히려 도움이 되지 않을 수 있습니다. 이에 본 논문은 더 높은 수준의 행동 공간, 적응형 커리큘럼(curriculum) 설계, 그리고 프로세스(process) 지향적 보상(reward)에 대한 향후 연구를 제안하며, LLM이 더욱 정교하고 인간적인 방식으로 추론할 수 있는 길을 모색합니다.

논문 | 트윗

---

### 3. RAG 기반 디코딩 재고: 효율적인 정보 통합을 위한 REFRAG

REFRAG 기법은 디코딩 과정에서 대부분의 검색된 토큰들을 사전에 계산된 청크(chunk) 임베딩(embedding)으로 교체하며, 이때 중요하다고 판단되는 일부 청크들만을 선별적으로 확장하는 방식을 채택합니다. 이 접근 방식은 RAG 프롬프트(prompt)에 내재된 블록 대각선 어텐션(block-diagonal attention) 메커니즘을 활용하여, RAG 모델, 다중 턴(multi-turn) 대화 시스템, 그리고 긴 문서 요약 작업 전반에서 정확도를 유지하면서도 처리 지연 시간(latency)과 필요한 메모리(memory) 양을 효과적으로 줄여줍니다.

본 기술의 핵심 발상은 검색된 문맥(context)을 여러 조각(chunk)으로 나누고, 각 조각을 경량 인코더(encoder)를 통해 부호화한 뒤, 이를 디코더(decoder)의 임베딩 차원으로 사영(projection)하여 사용자 질의(query)와 함께 직접 입력으로 제공하는 것입니다. 이 과정에서 강화 학습(RL) 정책(policy)이 어떤 청크를 압축 해제 상태로 유지할지("어디서든 압축", 접두사(prefix)뿐만 아니라 전체 시퀀스에서) 결정함으로써, 필요한 정보만을 효율적으로 처리할 수 있게 합니다. 이는 실시간 응용 프로그램에서 RAG 모델을 배포할 때 발생하는 높은 지연 시간과 메모리 사용량이라는 주요 병목 현상을 해결하는 데 결정적인 역할을 합니다.

이 방식은 정확도 저하 없이 상당한 속도 개선을 가져와, 높은 압축률 환경에서 LLaMA 모델 대비 최대 30.85배(CEPE 대비 3.75배) 빠른 첫 토큰 생성 시간을 기록했으며, 혼란도(perplexity)는 유사한 수준을 유지했습니다. 또한, 처리량(throughput)은 최대 6.78배 증가하는 놀라운 결과를 보였습니다. 압축 기술을 통해 모델은 훨씬 더 큰 유효 컨텍스트(context)를 처리할 수 있게 되는데(16배 확장 보고), 이는 시퀀스(sequence) 길이가 증가함에도 불구하고 혼란도를 유지하거나 오히려 개선하는 효과를 가져옵니다. 고정된 지연 시간 예산(budget) 하에서 REFRAG는 더 많은 구절을 활용하여 16개 RAG 작업에서 LLaMA 기준선(baseline)을 능가하는 강력한 성능을 보여주었습니다. 집계된 플롯(plot)과 상세 결과는 강력한 검색기(retriever)와 약한 검색기 모두에서 일관된 성능 향상을 입증합니다.

REFRAG의 일반화(generalization) 능력은 다양한 애플리케이션(application)에서 빛을 발합니다. 다중 턴 대화형 QA에서는 더 긴 대화 기록을 보존하고 구절 및 턴이 증가함에 따라 점수를 개선합니다. 장문 문서 요약에서는 일치하는 디코더 토큰으로 최고의 ROUGE 점수를 달성하며, 이는 복잡한 정보 통합 및 요약 작업에서도 탁월한 효율성을 제공함을 의미합니다. 이러한 발전은 리소스 제약이 있는 환경(예: 엣지 디바이스, 모바일)에서 LLM 기반 RAG 시스템을 배포하는 데 중요한 돌파구를 마련하며, RAG와 디코딩 프로세스의 더욱 깊은 통합을 위한 미래 연구 방향을 제시합니다.

논문 | 트윗

---

### 4. ACE-RL: 제어 가능한 장문 생성을 위한 새로운 보상 프레임워크

ACE-RL은 모호한 선호도 쌍 기반 보상 방식 대신, 개별 지시문에 맞춰 검증 가능한 체크리스트를 활용하는 강화 학습(RL) 프레임워크를 제시합니다. 이 프레임워크는 각 긴 형식의 작업을 명시적 및 암묵적 제약 조건들의 집합으로 변환하고, 모델의 결과물이 이러한 제약 조건들을 얼마나 잘 만족하는지에 따라 점수를 부여하며, GRPO 학습 과정에서는 길이 조절 보상과 이 점수를 결합합니다. 이로 인해 다양한 도메인(domain)과 글쓰기 스타일에서 보다 강력하고 제어 가능한 장문 생성이 가능해집니다.

기존의 선호도 쌍(preference-pair) 기반 보상은 인간의 피드백에 의존하며, 이는 비용이 많이 들고 확장성(scalability)에 한계가 있었습니다. 또한, '관련성', '일관성', '유용성'과 같은 추상적인 개념에 머물러 특정 지시문에 대한 미묘한 요구 사항을 반영하기 어려웠습니다. ACE-RL의 핵심 아이디어는 각 지시 사항을 상세한 체크리스트(명시적 및 암묵적 요구 사항 포함)로 자동 분해한 다음, 소형 LLM을 활용하여 각 항목을 세 가지 단계(완전 충족/부분 충족/미충족)의 평가 기준에 따라 검증하는 것입니다. 이렇게 계산된 평균 체크리스트 점수에 길이 보상을 더하여 GRPO로 최적화함으로써, 모델은 단순히 그럴듯한 답변이 아니라, 사용자가 요구하는 구체적인 제약 조건들을 충족하는 출력을 생성하도록 학습됩니다.

이러한 접근 방식은 LLM의 출력을 '지시문 적응형 품질'이라는 새로운 차원으로 끌어올리며, 선호도 쌍이 불필요해져 비용을 절감하고 확장성을 크게 향상시킵니다. 32K개의 장문 지시문과 프롬프트당 평균 5.48개의 제약 조건, 그리고 목표 길이 약 2.3K 단어의 데이터(data) 및 설정에서, Qwen3-8B를 검증기로 사용한 ACE-RL은 WritingBench에서 SFT(Supervised Fine-Tuning) 및 LLM-as-judge RL보다 모델 성능을 크게 향상시켰습니다. 예를 들어, Qwen-2.5-7B의 성능은 57.0에서 78.6으로 상승했습니다. 특히, ACE-RL로 훈련된 소형 Qwen-3-4B-thinking 모델은 여러 독점 및 글쓰기 튜닝(tuning) 시스템을 능가하는 인상적인 결과를 보여주었으며, Arena-Write에서 6개의 강력한 기준선(baseline) 대비 약 68%의 승률을 기록했습니다.

어블레이션(ablation) 연구와 통찰력(insight)에 따르면, 제약 조건 기반 보상은 LLM-as-judge보다 그룹 내 보상 분산이 높아 롤아웃(rollout) 간 더 나은 구별 능력을 제공합니다. 이는 소형 보상 모델 및 심지어 자체 보상(self-reward) 설정에서도 효과적으로 작동함을 시사합니다. 또한, '사고 모드(thinking mode)'와 ACE-RL의 조합은 장문 생성에서 비사고 모드(non-thinking mode)를 능가하는 것으로 나타나, LLM이 단순히 텍스트를 생성하는 것을 넘어, 생성 과정에 대한 명시적인 제어를 통해 더욱 정확하고 유용한 결과물을 만들어낼 수 있음을 보여줍니다. 이러한 연구는 설명 가능하고 제어 가능한 AI 출력에 대한 요구가 높아지는 상황에서, 특히 중요한 응용 분야에서 LLM의 신뢰성을 확보하는 데 중요한 기여를 할 것으로 기대됩니다.

논문

---

### 5. ParaThinker: LLM의 '터널 시야'를 극복하는 병렬 추론

본 연구는 현재 널리 사용되는 '더 길게 사고하기(think longer)' 전략이 대규모 언어 모델(LLM)을 단일한 추론 방식에 국한시킨다고 지적합니다. 이러한 단일 경로는 초기 토큰(token) 선택이 모델을 최적이지 않은 방향으로 유도할 수 있는 '터널 시야(Tunnel Vision)'라는 테스트(test) 시점의 병목 현상을 야기할 수 있습니다. 이를 해결하기 위해, 연구자들은 여러 개의 독립적인 추론 경로를 동시에 생성한 후 이를 하나의 최종 답변으로 통합하도록 모델을 학습시키는 ParaThinker를 제안합니다. 다양한 수학 벤치마크 테스트에서 이러한 폭(width) 확장 방식은 미미한 지연 시간(latency) 증가만을 수반하면서도 정확도를 효과적으로 향상시켰습니다.

ParaThinker는 인간의 문제 해결 방식 중 하나인 '브레인스토밍'이나 '병렬 사고'와 유사하게 작동합니다. 하나의 긴 사고 체인(chain)에 의존하는 대신, 여러 대안적인 사고 과정을 동시에 탐색함으로써 초기 오류의 영향을 줄이고, 더 넓은 탐색 공간에서 최적의 해답을 찾을 가능성을 높입니다. 이 방법은 병렬 추론과 요약이라는 두 단계로 실행됩니다. 다양한 경로를 시작하기 위해 훈련 가능한 제어 토큰 `<think i>`를 사용하고, 각 사고 경로의 토큰들을 명확히 구분하기 위해 사고별 위치 임베딩(positional embedding)을 활용합니다. 또한, 사고 중 경로를 분리하고 요약을 위해 통합하는 2단계 어텐션 마스크(attention mask)를 사용하여 구조적인 병렬 처리를 가능하게 합니다. KV 캐시(cache)를 재사용하여 재사전 채우기(re-prefill)를 피함으로써 효율성을 확보합니다.

훈련 레시피는 교사 모델에서 샘플링(sampling)된 다중 경로 추적(trace)에 대한 지도 미세 조정(supervised fine-tuning)을 수행합니다. `<think i>` 토큰의 무작위 할당을 사용하여 학생 모델이 훈련에서 본 것보다 더 많은 경로에 대해 일반화(generalize)할 수 있도록 합니다. 이러한 세부 사항 및 데이터(data) 출처는 섹션 4 및 부록의 SFT 테이블(table)에 설명되어 있습니다. 실험 결과는 ParaThinker의 효과를 명확히 보여줍니다. AIME 2024/2025, AMC 2023, MATH-500과 같은 벤치마크에서 ParaThinker는 고정된 경로당 예산으로 8개 경로를 사용할 때 1.5B 모델의 경우 약 12.3%, 7B 모델의 경우 7.5%만큼 순차적 기준선(baseline) 대비 pass@1 성능을 개선했습니다. 이는 평균적으로 다수결 투표 방식보다 1.5B 모델의 경우 4.3%, 7B 모델의 경우 2.0% 더 우수한 결과입니다. ParaThinker와 다수결 투표를 결합하면 추가적인 성능 향상을 얻을 수 있어, 앙상블(ensemble) 학습의 원리가 LLM 추론에도 효과적으로 적용될 수 있음을 시사합니다.

효율성(efficiency) 및 설계 통찰력(design insight) 측면에서, 디코딩(decoding)이 메모리(memory) 대역폭(bandwidth)에 의해 제한되므로 경로가 많아질수록 지연 시간이 약간 증가합니다. 그러나 단일 A800 GPU에서 16개 경로는 동일한 길이에 대해 하나의 경로 시간의 2배 미만으로 소요되어, 병렬 처리가 충분히 효율적임을 보여줍니다. 최적의 종료 정책(policy)은 경로 길이를 동일하게 하고 정확도와 속도 모두를 향상시키는 "first-finish" 방식입니다. 또한, 사고 임베딩의 중요성이 강조되었는데, 단순히 평탄화된 위치 임베딩은 성능을 저해하는 것으로 나타나, 추론 경로를 구분하는 미묘한 메커니즘의 중요성을 시사합니다. 이러한 연구는 복잡한 의사 결정, 창의적 글쓰기, 과학적 발견 등 다양한 분야에서 LLM이 여러 관점을 통합하여 더 나은 결과물을 생성할 수 있는 가능성을 열어줍니다.

논문 | 트윗

---

### 6. AgentGym-RL: LLM 에이전트 훈련을 위한 확장 가능한 프레임워크

AgentGym-RL은 현실적인 다양한 환경에서 대규모 언어 모델(LLM) 에이전트를 직접 강화 학습(RL) 방식으로 훈련시키기 위한 모듈형 프레임워크를 제시하며, 이와 함께 학습 전반에 걸쳐 상호작용의 범위를 점진적으로 확장하여 안정성과 성능을 높이는 간단한 스케줄인 ScalingInter-RL을 함께 소개합니다. 연구 결과는 70억(7B) 파라미터 규모의 공개 모델이 웹 탐색, 심층 검색, 게임 플레이, 체화된(embodied) 작업, 그리고 과학적 문제 해결과 같은 다양한 영역에서 훨씬 더 큰 규모의 독점 시스템과 경쟁하거나 심지어 능가할 수 있음을 입증합니다.

AgentGym-RL은 환경(environment), 에이전트(agent), 훈련(training)의 세 가지 플러그형 모듈(module)을 갖춘 통합되고 분리된 RL 스택(stack)으로, PPO, GRPO, REINFORCE++와 같은 다양한 RL 알고리즘을 지원하며 WebArena, Deep Search, TextCraft, BabyAI, SciWorld 등 광범위한 벤치마크 환경에서 실행됩니다. 이러한 모듈성은 연구자들이 특정 구성 요소를 쉽게 교체하거나 새로운 아이디어를 실험할 수 있도록 하여 LLM 에이전트 연구의 발전을 가속화합니다. ScalingInter-RL의 핵심 원리는 초기에는 짧은 상호작용 범위(horizon)로 시작하여 활용(exploitation) 및 안정적인 학습에 중점을 두다가, 점차 허용되는 턴(turn) 수를 늘려 탐색(exploration), 계획(planning), 그리고 반성(reflection)과 같은 더욱 풍부한 행동들을 장려하는 데 있습니다. 이는 마치 인간이 점진적으로 복잡한 기술을 배우는 커리큘럼(curriculum) 학습과 유사합니다.

에이전트 기반 작업에서는 단순히 모델의 크기를 키우는 것보다, 후처리 학습과 테스트 시 필요한 연산 자원을 효율적으로 확장하는 것이 훨씬 더 중요합니다. 이 프레임워크로 훈련된 7B 모델이 약 58.6%의 평균 성공률을 달성하고 훨씬 더 큰 기준선 모델들을 능가한다는 사실은, 모델 크기 외의 요소(예: 훈련 방법론, 환경 설계)가 에이전트 성능에 미치는 지대한 영향을 시사합니다. 결과 스냅샷(snapshot)을 보면, ScalingInter-7B는 WebArena에서 GPT-4o의 16.00%를 능가하는 26.00%를 기록했으며, 심층 검색에서는 38.25로 GPT-4o의 26.75를 넘어섰습니다. 게임(TextCraft), 체화(BabyAI), 과학(SciWorld) 작업에서도 SOTA(State-of-the-Art) 성능을 달성하거나 강력한 경쟁력을 보여주었습니다.

훈련 동역학(dynamic) 분석 결과, 너무 이른 시점에 긴 상호작용 범위를 부여하면 학습이 붕괴될 수 있고, 반대로 너무 짧은 범위는 성능을 제한하는 것으로 나타났습니다. ScalingInter-RL은 이 두 가지 문제를 모두 방지하는 효과적인 전략임이 입증되었습니다. 엔지니어링(engineering) 측면에서는 병렬화된 브라우저(browser), 리셋 훅(reset hook), 메모리 누수(memory leak) 수정과 같은 개선 사항들이 안정적인 장기 롤아웃(rollout)을 가능하게 했으며, 시각적 UI(User Interface)는 궤적(trajectory) 및 실패 모드(failure mode)를 검사하는 데 큰 도움을 주었습니다. 실무자를 위한 조언으로는, 희소 보상(sparse-reward) 및 장기 궤적 에이전트 작업에는 REINFORCE++보다 GRPO를 선호하고, 상호 작용 길이 커리큘럼이 간단하면서도 견고한 이점을 제공하며, 파라미터(parameter) 스케일링 전에 후처리 훈련 및 추론 샘플링(inference sampling)을 위한 컴퓨팅(compute) 예산을 충분히 책정해야 한다는 점이 강조됩니다. 이는 LLM 에이전트가 과학적 발견, 복잡한 시스템 제어 등 'AI for science' 운동의 핵심 주체로 자리매김할 수 있는 잠재력을 보여줍니다.

논문 | 트윗

---

### 7. 대화형 AI의 함정: 다중 에이전트 토론의 역효과

여러 에이전트가 참여하는 토론 방식이 언제나 긍정적인 결과를 가져오는 것은 아닙니다. 본 연구는 세 가지 추론 벤치마크(benchmark)와 다양한 능력을 가진 에이전트 집단에 걸쳐 진행된 실험에서, 토론이 종종 정확도를 감소시켰으며, 심지어 더 강력한 모델조차도 때로는 능력 낮은 동료 에이전트의 영향으로 부정확한 답변으로 선회하는 경향을 보였다고 보고합니다. 연구자들은 현재의 정렬(alignment) 방식이 에이전트들을 지나치게 순응적으로 만들어, 그들이 설득력 있어 보이는 잘못된 추론을 받아들이고 이를 비판적으로 검토하지 않도록 만든다고 주장합니다. 이는 인간 사회의 '집단 사고(groupthink)' 현상과 유사하게, 합의를 우선시하는 경향이 비판적 사고를 저해할 수 있음을 시사합니다.

연구 설정은 GPT-4o-mini, Llama-3.1-8B-Instruct, Mistral-7B-Instruct와 같은 다양한 LLM을 사용하여 CommonSenseQA, MMLU, GSM8K에서 토론의 효과를 평가했습니다. 에이전트들은 먼저 한 번 답변한 후 두 라운드(round) 동안 토론을 진행하며, 최종 출력은 토론 전후의 다수결 투표로 결정됩니다. 프롬프트(prompt)는 짧은 추론과 작업별 형식을 요구하여 명확한 평가를 가능하게 했습니다.

주요 결과는 충격적입니다. 토론은 특히 CommonSenseQA 및 MMLU 벤치마크에서 정확도를 자주 저해하는 것으로 나타났습니다. 혼합 능력 설정(mixed-capability setting)을 포함한 많은 그룹에서 토론 후 일관된 성능 하락을 보여주었습니다. 예를 들어, CSQA에서는 1×GPT + 2×Llama 조합에서 6.6점, 2×Llama + 1×Mistral 조합에서 8.0점 하락했으며, MMLU에서는 1×GPT + 2×Llama 조합에서 12.0점이나 하락했습니다. GSM8K의 경우 혼합된 결과를 보였지만, 일부 설정에서만 작은 이득이 있었습니다. 라운드별 성능 저하를 추적한 결과, 토론이 진행됨에 따라 더 강력한 모델이 다수일 때조차도 성능이 저하되는 경향이 관찰되었습니다。

이러한 현상의 원인은 에이전트들이 비판적인 검토보다 합의 형성을 선호하는 경향에 있습니다. 연구 결과, 잘못된 답변으로의 전환(correct→incorrect)이 올바른 답변으로의 전환(incorrect→correct)보다 더 많았으며, 이는 토론 과정이 강력한 모델조차도 적극적으로 오도할 수 있음을 나타냅니다. 부록의 예시는 동료의 답변을 읽은 후 올바른 답변에서 잘못된 답변으로 아첨하듯 역전되는 현상을 기록하며 이러한 문제를 생생하게 보여줍니다. 본 연구의 시사점은 단순한 토론 프로토콜이 오히려 오류를 확산시킬 위험이 있다는 것입니다. 따라서 연구진은 독립적인 검증에 보상하고, 에이전트의 신뢰도 또는 자신감에 따라 주장에 가중치를 부여하며, 정당하지 않은 합의에 페널티(penalty)를 부과하여 토론의 이점을 보존하는 설계를 권장합니다. 이는 미래의 다중 에이전트 시스템이 집단 지성의 장점을 취하면서도 치명적인 집단 사고의 함정을 피할 수 있도록 하는 중요한 지침을 제공합니다.

논문 | 트윗

---

### 8. AggLM: 강화 학습 기반의 스마트한 솔루션 집계

AggLM은 다수의 후보 해결책을 통합하는 대규모 언어 모델(LLM) 훈련에 강화 학습(RL) 기법을 적용함으로써, 단순 다수결 투표나 보상 모델 기반 순위 지정과 같은 기존 방식을 뛰어넘는 새로운 접근법을 제시합니다. 이 방법은 향상된 정확도를 달성하고, 소수의 올바른 답변을 효과적으로 찾아내며, 다양한 모델에 걸쳐 일반화(generalize) 능력을 보이고, 기존의 집계 방식에 비해 적은 토큰(token)을 사용하여 효율성을 높입니다.

기존의 단순 다수결 투표 방식은 가장 흔한 답변을 선택하지만, 소수의 창의적이거나 정확한 답변을 놓칠 수 있다는 한계가 있습니다. 또한, 보상 모델 기반의 순위 지정은 좋은 답변을 식별하는 데 유용하지만, 최적의 후보를 선택하는 데 필요한 미묘한 추론을 포착하지 못할 수 있습니다. AggLM은 RL을 도입하여 이러한 한계를 극복합니다. RL 에이전트는 다양한 후보 솔루션들을 입력으로 받아, 어떤 솔루션을 선택하거나 어떻게 조합할지에 대한 '정책'을 학습합니다. 이 정책은 단순한 투표를 넘어, 각 후보의 강점과 약점을 평가하고, 심지어 소수 의견 속에 숨겨진 정답을 '복구'하는 능력을 발휘할 수 있습니다. 예를 들어, 극소수의 모델만이 특정 문제에 대한 통찰력 있는 해결책을 제시했을 때, AggLM은 이를 식별하고 최종 답변으로 통합하는 방법을 학습할 수 있습니다. 이는 '군중의 지혜(wisdom of crowds)'를 뛰어넘어, '지능적인 집계자(intelligent aggregator)'의 역할을 수행하는 것입니다.

이러한 RL 기반 집계는 더 높은 정확도를 달성할 뿐만 아니라, 모델 전반에 걸쳐 일반화 능력을 보여주어, 특정 모델이나 작업에 국한되지 않고 폭넓게 적용될 수 있음을 시사합니다. 또한, 전통적인 집계 방법보다 적은 토큰을 사용하여 효율성을 높인다는 점은, 특히 대규모 언어 모델의 운영 비용과 속도 측면에서 중요한 이점을 제공합니다. 이는 복잡한 QA 시스템, 창의적 콘텐츠 생성, 과학적 가설 생성 등 다양한 응용 분야에서 LLM이 더욱 신뢰할 수 있고, 미묘한 차이를 이해하며, 효율적으로 작동하도록 만드는 데 기여할 수 있습니다. AggLM은 LLM이 단순히 텍스트를 생성하는 것을 넘어, 여러 정보를 종합하고 최적의 결정을 내리는 고차원적인 인지 능력을 갖추도록 훈련하는 새로운 방향을 제시합니다.

논문 | 트윗

---

### 9. 대규모 추론 모델을 위한 RL 설문 조사: ASI를 향한 로드맵

본 조사는 강화 학습(RL)이 대규모 추론 모델(LRM)의 발전에 어떤 기여를 하고 있는지 심층적으로 분석하며, 이를 통해 수학이나 코딩과 같은 복잡한 과제에서 더욱 강력한 성능을 구현하는 가능성을 조명합니다. 이 보고서는 컴퓨팅 자원, 알고리즘 설계, 데이터 처리, 그리고 인프라 구축에 있어 확장성(scaling)과 관련된 도전 과제들을 부각하는 한편, 인공 초지능(Artificial Superintelligence, ASI)을 향한 미래 연구 방향을 제시합니다.

RL은 LRM의 능력을 여러 면에서 혁신적으로 향상시킵니다. 단순히 패턴을 인식하고 텍스트를 생성하는 것을 넘어, RL은 LRM이 스스로 오류를 수정하고(self-correction), 복잡한 계획을 수립하며(planning), 외부 도구를 효과적으로 사용하는(tool use) 능력을 부여합니다. 이는 LRM이 단순한 언어 처리기를 넘어, 실제 세계의 문제를 해결하는 능동적인 '에이전트'로 진화하는 데 필수적인 요소입니다. RL은 LRM이 시행착오를 통해 학습하고, 장기적인 보상을 최적화하며, 인간의 피드백(Human Feedback)을 통해 더욱 정렬된 행동을 보이도록 훈련하는 데 핵심적인 역할을 합니다. 이러한 RL과 SFT(Supervised Fine-Tuning)와 같은 다른 훈련 기법과의 상호 작용은 LRM의 지능을 다층적으로 구축하는 데 중요합니다.

그러나 LRM을 ASI 수준으로 발전시키는 과정에는 막대한 도전 과제가 산재해 있습니다. 계산 비용(computational cost)은 여전히 가장 큰 장벽 중 하나이며, LRM의 규모가 커질수록 필요한 컴퓨팅 자원은 기하급수적으로 증가합니다. 또한, 고품질의 데이터(data)를 확보하고, 복잡한 알고리즘(algorithm)을 설계하며, 대규모 분산 훈련을 위한 견고한 인프라(infrastructure)를 구축하는 것 또한 중요한 과제입니다. 이러한 확장성 문제는 단순한 기술적 난관을 넘어, 인공지능 연구의 지속 가능성과 접근성에도 영향을 미칩니다.

이 설문 조사는 또한 LRM의 발전이 가져올 윤리적, 안전성(safety) 문제에 대한 심도 깊은 논의의 필요성을 강조합니다. LRM의 능력이 ASI에 가까워질수록, 그들의 행동이 사회에 미칠 영향은 더욱 커질 것이므로, 책임감 있는 개발과 배포를 위한 가이드라인과 규제가 필수적입니다. 미래 10년 동안 RL은 LRM이 과학적 발견, 의료 혁신, 기후 변화 해결과 같은 인류의 가장 시급한 문제들을 해결하는 데 결정적인 역할을 할 수 있도록 만들 것입니다. 이를 위해서는 다양한 벤치마크(benchmark)를 통해 LRM의 추론 능력과 안정성을 지속적으로 평가하고 개선하는 노력이 중요합니다.

논문 | 트윗

---

### 10. LiveMCP-101: 자율 AI 시스템의 도구 오케스트레이션 한계점 분석

LiveMCP-101은 검색 기능, 파일 처리, 수학 연산, 그리고 데이터(data) 분석 등 광범위한 도구 활용이 필요한 다단계 작업 환경에서 MCP 지원 에이전트(agent)의 성능을 평가하기 위해 고안된 101개의 실제 질의로 구성된 신규 벤치마크(benchmark)입니다. 이 벤치마크의 결과는 현재 선두적인 대규모 언어 모델(LLM)들이 60% 미만의 성공률을 기록하며, 도구 활용 조율(orchestration) 능력에 있어 주요한 한계점을 드러냈고, 이는 자율적인 AI 시스템 개발을 위한 중요한 통찰력(insight)을 제공합니다。

기존의 벤치마크들이 단일 도구 사용이나 비교적 단순한 작업에 초점을 맞췄던 것과 달리, LiveMCP-101은 '실제 세계 질의(real-world queries)'와 '다단계 작업(multi-step tasks)'에 중점을 둠으로써, LLM 에이전트의 실제 적용 가능성을 더욱 현실적으로 평가합니다. 복잡한 현실 세계 문제는 단순히 하나의 도구를 사용하는 것을 넘어, 여러 도구를 순차적 또는 병렬적으로 조합하고, 각 단계의 결과를 다음 단계로 연결하며, 발생할 수 있는 오류를 처리하는 정교한 '도구 오케스트레이션(orchestration)' 능력을 요구합니다. 현재 LLM은 도구 선택(tool selection), 오류 처리(error handling), 순차적 실행, 상태 관리(state management) 등에서 여전히 큰 어려움을 겪고 있으며, LiveMCP-101은 이러한 특정 약점들을 명확하게 드러냅니다.

이러한 결과는 현재의 LLM 기능과 진정으로 자율적인 에이전트 간의 상당한 격차가 존재함을 보여줍니다. LLM은 개별 도구의 사용법을 이해하는 데는 능숙할 수 있지만, 복잡한 문제 해결을 위해 도구들을 지능적으로 '지휘'하는 능력은 아직 부족합니다. 이는 마치 오케스트라의 각 악기 연주자는 뛰어나지만, 이들을 조화롭게 이끌어갈 지휘자가 없는 상황과 유사합니다.

LiveMCP-101은 미래 연구 방향에 대한 중요한 시사점을 제공합니다. 도구 사용 및 오케스트레이션 능력을 향상시키기 위한 연구는 더 나은 계획(planning) 메커니즘, 자가 반성(self-reflection) 능력, 그리고 견고한 오류 복구(error recovery) 루틴 개발에 집중해야 할 것입니다. 예를 들어, 에이전트가 실패로부터 학습하고, 과거의 경험을 바탕으로 미래의 도구 사용 전략을 개선하며, 예상치 못한 상황에 유연하게 대처할 수 있도록 훈련하는 것이 중요합니다. LiveMCP-101과 같은 포괄적이고 현실적인 벤치마크의 존재는 이러한 연구 발전을 촉진하고, LLM이 단순한 언어 모델을 넘어 실제 세계의 복잡한 문제를 해결하는 진정한 자율 AI 시스템으로 거듭나는 데 필수적인 역할을 할 것입니다.

논문 | 트윗