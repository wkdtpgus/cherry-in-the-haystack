**1. SFR-DeepResearch: 자율 에이전트(Agent)의 새로운 지평**
본 연구는 대규모 언어 모델(LLM)을 자율적으로 운영되는 단일 인공지능 연구 주체로 변모시키는, 추론 활동에 특화된 강화 학습(reinforcement learning) 방법론인 SFR-DeepResearch를 제안합니다. 이 시스템은 탐색, 정적 페이지 열람, 그리고 파이썬(Python) 실행이라는 세 가지 기본적인 기능만을 활용하며, 스스로 자신의 작동 맥락을 관리합니다. 또한, 길이 균등화 리인포스(REINFORCE) 목표 함수를 통해 인공적으로 생성된 짧거나 긴 유형의 과제들에 대해 처음부터 끝까지 학습됩니다. 이러한 접근 방식의 결과로 FRAMES, GAIA, 그리고 Humanity’s Last Exam 벤치마크에서 기존 대비 현저한 성능 향상을 이끌어냈습니다.

SFR-DeepResearch의 에이전트(agent) 설계 및 스캐폴딩(scaffolding)은 QwQ 및 Qwen 모델의 경우, 다중 턴(multi-turn) 도구 사용을 단일의 확장되는 컨텍스트(context) 질문으로 재구성하고, 프롬프트(prompt)를 안정적으로 유지하기 위해 이전의 긴 CoT(Chain-of-Thought)를 생략합니다. 또한, 컨텍스트(context) 한계에 가까워질 때 자체 압축하는 clean_memory 도구를 추가합니다. 최소한의 도구 세트와 내결함성(fault tolerance)을 특징으로 하며, 도구는 기본적인 검색 API, 하이퍼링크(hyperlink) 클릭 기능이 없는 정적 마크다운(Markdown) 페이지 스크래퍼(scraper), 그리고 상태 비저장(stateless) 로컬(local) Python 인터프리터(interpreter)로 제한되어 있어 전략을 학습하기에 충분히 어려운 훈련 환경을 조성합니다. 구문 분석(parsing) 및 구문(syntax) 오류 발생 시 복구 또는 재시도 루틴(routine)이 트리거(trigger)되어 롤아웃(rollout)을 정상적으로 유지합니다.

RL 레시피는 합성된, Hotpot보다 어려운 다중 홉(multi-hop) QA와 보고서 작성 작업을 사용합니다. 궤적(trajectory) 길이로 나누는 시간적 이점 정규화(temporal advantage normalization)를 포함한 그룹 REINFORCE 목적 함수(objective)를 최적화(optimize)하며, 궤적(trajectory) 필터링(filtering) 및 부분 롤아웃(rollout) 재사용을 수행합니다. 지역화되고 캐시(cache)된 도구와 오염 차단 목록(contamination blocklist)은 훈련 및 평가를 안정화합니다.

결과적으로, 최적 모델인 SFR-DR-20B는 FRAMES에서 82.8, GAIA(텍스트 전용)에서 66.0, HLE 전체 텍스트 전용에서 28.7을 달성하여 유사한 오픈 에이전트(agent)를 능가하고 오염 차단 목록(contamination blocklist) 하에서 더 강력한 독점 시스템과 경쟁합니다. 어블레이션(ablation) 연구 및 동작 분석에 따르면, 단일 턴(single-turn) 스캐폴딩(scaffolding)은 Qwen 및 QwQ의 기본 다중 턴(multi-turn) 템플릿(template)을 능가하며, FRAMES에서 큰 성능 향상을 보입니다. 길이 정규화(length normalization)는 보상(reward) 및 정확도(accuracy)를 저해하는 폭주하는 도구 호출을 억제합니다. 도구 사용 및 토큰(token) 길이 분석은 gpt-oss-20B가 도구를 더 많이 호출하지만 단계별 CoT(Chain-of-Thought)는 훨씬 짧게 작성하여 Qwen 계열 모델보다 더 나은 토큰(token) 효율성(efficiency)을 나타냄을 보여줍니다.
논문 | 트윗

**2. 계층적 추론의 자율적 발현**
이 논문은 강화 학습(RL)이 대규모 언어 모델(LLM)의 추론 능력을 향상시키는 데 기여하는 두 단계의 계층 구조가 자연스럽게 나타난다는 주장을 펼칩니다. 모델은 먼저 낮은 수준의 수행 과정을 확고히 다지고, 이후 높은 수준의 계획 수립 과정을 통해 진행됩니다. 이러한 통찰을 바탕으로 연구진은 전략적 계획과 관련된 토큰(token)에 더 높은 가중치를 부여하는 HICRA(Hierarchical Credit Assignment) 기법을 제시하며, 이는 GRPO(Generalized Advantage Estimation with Policy Optimization) 방식 대비 일관된 성능 개선을 입증합니다. 또한, 토큰(token)별 엔트로피(entropy)보다 더 나은 탐색 신호로 의미론적 엔트로피(semantic entropy)를 제시하여, 모델의 탐색 효율성을 높이는 데 중요한 기여를 합니다.

이러한 계층적 접근 방식은 인간의 문제 해결 과정과 유사하게, 단순한 명령 실행을 넘어선 복합적인 사고를 가능하게 합니다. 초기 학습 단계에서는 반복적인 실행을 통해 기본적인 절차적 지식을 습득하고, 이후에는 더 추상적인 수준에서 문제를 조망하고 해결책을 모색하는 능력을 키우는 것입니다. 이는 마치 처음에는 악기 연주의 기본기를 익히고, 나중에는 복잡한 악곡을 해석하고 창작하는 과정과 흡사합니다. HICRA의 핵심은 이러한 고차원적 사고 과정에 적절한 보상을 부여함으로써, 모델이 단순히 정답을 찾는 것을 넘어, 효과적인 전략을 스스로 고안하고 적용하도록 유도하는 데 있습니다.

2단계 동역학(dynamic) 측면에서, 초기 RL 훈련은 실행(execution) 토큰(token)의 혼란도(perplexity)와 엔트로피(entropy)를 감소시켜 절차적 기술을 통합합니다. 후기 성능 향상은 계획 토큰(token)의 다양성 증가 및 더 길고 정확한 추적(trace)과 일치하며, 이는 "아하 모멘트(aha moments)" 및 길이 스케일링(scaling)을 설명합니다. 계획(planning) 대 실행(execution)의 구분을 위해, 이 논문은 전략적 그램(gram)(예: 연역, 분기, 역추적)을 계획 토큰(token)으로 기능적으로 태그(tag)하여 절차적 단계와 구별합니다. 이 라벨링(labeling)은 학습 병목 현상이 전략으로 이동함을 보여줍니다. HICRA 알고리즘(algorithm)은 스칼라(scalar) α를 사용하여 계획 토큰(token)에 대한 이점(advantage)을 증폭시켜 GRPO를 수정함으로써, 모든 토큰(token)에 분산하는 대신 영향력 있는 전략적 결정에 최적화(optimization)를 집중합니다. 이는 목표 지향적 탐색(exploration) 및 효과적인 전략의 더 빠른 강화를 생성합니다. 섹션 3에서 공식화(formulation)를 제공합니다.

결과적으로 Qwen, Llama 및 VLM 전반에 걸쳐 HICRA는 AIME24/25, Math500, AMC23 및 멀티모달(multimodal) 수학 스위트(suite)에서 Pass@1을 개선하며, 종종 GRPO보다 몇 점 더 높게 나타납니다. 플롯(plot)은 더 높은 의미론적 엔트로피(semantic entropy)가 더 높은 검증 정확도(validation accuracy)를 추적함을 보여줍니다. 중요한 신호는 실행(execution) 토큰(token)이 지배적이기 때문에 실제 탐색(exploration)이 증가하더라도 토큰(token) 수준 엔트로피(entropy)는 감소할 수 있다는 점입니다. 전략적 그램(gram)에 대한 의미론적 엔트로피(semantic entropy)가 전략적 탐색(exploration)을 더 잘 포착하고 성능과 상관관계가 있습니다. 한계 및 범위 측면에서, HICRA는 모델이 이미 절차적 기반을 가지고 있을 때 가장 잘 작동합니다. 약한 기반에서는 계획(planning)에 대한 집중이 도움이 되지 않을 수 있습니다. 이 논문은 더 높은 수준의 행동 공간, 적응형 커리큘럼(curriculum) 및 프로세스(process) 지향적 보상(reward)에 대한 향후 연구를 제안합니다.
논문 | 트윗

**3. RAG 디코딩(decoding)의 새로운 접근 방식**
REFRAG는 디코딩(decoding) 과정에서 대부분의 검색된 토큰(token)을 미리 계산된 청크(chunk) 임베딩(embedding)으로 대체하고, 그중 핵심적인 일부 청크(chunk)만을 선별적으로 확장하는 기법을 제안합니다. 이 방식은 RAG(Retrieval Augmented Generation) 프롬프트(prompt)의 블록 대각선 어텐션(block-diagonal attention) 구조를 활용하여, RAG, 다중 턴(multi-turn) 대화, 그리고 장문 요약과 같은 다양한 시나리오에서 정확도(accuracy)를 유지하면서도 지연 시간(latency)과 메모리(memory) 사용량을 크게 줄이는 효과를 가져옵니다.

REFRAG의 혁신은 정보 압축과 효율적인 검색에 있습니다. 검색된 문서를 의미 단위의 작은 조각(청크)으로 나누고, 이들을 경량 인코더(encoder)로 처리하여 디코더(decoder)가 직접 활용할 수 있는 형태로 변환합니다. 이 과정에서 어떤 청크를 유지하고 어떤 청크를 압축할지 결정하는 강화 학습(RL) 기반 정책(policy)이 중요한 역할을 합니다. 이는 마치 요약본을 읽다가 특정 부분이 궁금할 때만 원문을 찾아보는 것과 유사합니다. 이러한 접근 방식은 특히 실시간 대화 시스템이나 대규모 문서 처리와 같이 빠른 응답과 효율적인 자원 사용이 필수적인 환경에서 LLM의 활용성을 극대화할 수 있습니다.

핵심 아이디어(idea)는 검색된 컨텍스트(context)를 청크(chunk)로 분할하고, 각 청크(chunk)를 경량 인코더(encoder)로 인코딩(encode)하며, 디코더(decoder)의 임베딩(embedding) 크기로 투영한 다음, 사용자 쿼리(query)와 함께 임베딩(embedding)을 직접 공급하는 것입니다. RL 정책(policy)은 어떤 청크(chunk)를 압축 해제 상태로 유지할지 결정합니다("어디서든 압축", 접두사(prefix)뿐만 아니라).

이러한 접근 방식은 정확도(accuracy) 손실 없는 큰 속도 향상을 가져옵니다. 높은 압축률에서 LLaMA 대비 최대 30.85배(CEPE 대비 3.75배)의 첫 토큰(token) 생성 시간 가속을 달성하며, 유사한 혼란도(perplexity)를 보입니다. 처리량(throughput)은 최대 6.78배 증가합니다. 또한, 압축을 통해 모델은 훨씬 더 큰 컨텍스트(context)를 처리할 수 있어(16배 확장 보고) 더 긴 유효 컨텍스트(context)를 제공하며, 시퀀스(sequence) 길이가 증가함에 따라 혼란도(perplexity)를 유지하거나 개선합니다. 고정된 지연 시간(latency) 예산 하에서 REFRAG는 더 많은 구절을 사용하고 16개 RAG 작업에서 LLaMA 기준선(baseline)을 능가하는 RAG의 우위를 보여줍니다. 집계된 플롯(plot)과 상세 결과는 강력한 검색기(retriever)와 약한 검색기(retriever) 모두에서 성능 향상을 보여줍니다. 애플리케이션(application) 전반의 일반화(generalization) 측면에서, 다중 턴(multi-turn) 대화형 QA에서 REFRAG는 더 긴 기록을 보존하고 구절 및 턴(turn)이 증가함에 따라 점수를 개선합니다. 장문 문서 요약에서는 일치하는 디코더(decoder) 토큰(token)으로 최고의 ROUGE를 달성합니다.
논문 | 트윗

**4. ACE-RL: 검증 가능한 장문 생성**
ACE-RL은 기존의 모호한 선호도 쌍(preference-pair) 보상(reward) 대신, 각 지시문에 대해 명확히 검증 가능한 체크리스트(checklist)를 활용하는 강화 학습(reinforcement learning) 프레임워크(framework)입니다. 이 방식은 긴 형태의 글쓰기 작업을 명시적 및 암묵적 제약 조건(constraint)의 집합으로 전환하고, 모델의 결과물이 이 조건들을 얼마나 잘 충족하는지에 따라 점수를 부여합니다. 이 점수는 GRPO(Generalized Advantage Estimation with Policy Optimization) 훈련 과정에서 길이 제어 보상(reward)과 결합되어 최적화됩니다. 그 결과, 다양한 도메인(domain)과 스타일(style)에 걸쳐 더욱 견고하고 통제 가능한 장문 생성이 가능해집니다.

ACE-RL의 핵심은 인간의 피드백을 세분화된 지침으로 변환하여 모델 학습에 활용하는 능력입니다. 예를 들어, "보고서를 작성하라"는 지시를 "서론, 본론, 결론을 포함할 것", "최신 데이터(data)를 인용할 것", "전문적인 어조를 유지할 것"과 같은 구체적인 항목으로 나눕니다. 이러한 항목들은 작은 LLM을 검증기로 사용하여 "완전히 충족", "부분적으로 충족", "충족하지 못함"의 세 가지 수준으로 평가됩니다. 이처럼 정량화된 피드백은 모델이 단순히 그럴듯한 텍스트를 생성하는 것을 넘어, 특정 목적과 요구사항을 정확히 반영하는 콘텐츠를 만들도록 유도합니다. 이는 콘텐츠의 품질 관리와 특정 목적에 부합하는 글쓰기 AI 개발에 중요한 진전을 의미합니다.

ACE-RL의 핵심 아이디어(idea)는 각 지시문을 세분화된 체크리스트(checklist)(명시적 및 암묵적 요구 사항)로 자동 분해한 다음, 작은 LLM을 사용하여 3단계 루브릭(rubric)(완전히/부분적으로/충족하지 못함)으로 각 항목을 검증하는 것입니다. 보상(Reward)은 평균 체크리스트(checklist) 점수와 길이 보상(reward)의 합으로 구성되며, GRPO로 최적화(optimize)됩니다. 이는 관련성/일관성/유용성을 넘어 지시문 적응형 품질로 이동하며, 선호도 쌍(preference pair)이 필요 없어 비용을 절감하고 확장성(scalability)을 향상시킨다는 점에서 중요합니다. 데이터(data) 및 설정은 32K 장문 지시문, 프롬프트(prompt)당 평균 5.48개 제약 조건(constraint), 목표 길이 약 2.3K 단어를 사용합니다. 검증기는 Qwen3-8B를 사용하며, 길이 보상(reward)은 허용 범위(tolerance band)를 벗어나는 편차에 페널티(penalty)를 부과합니다.

결과적으로 WritingBench에서 ACE-RL은 SFT 및 LLM-as-judge RL보다 모델 성능을 크게 향상시킵니다. 예를 들어, Qwen-2.5-7B는 57.0에서 78.6으로 상승합니다. ACE-RL로 훈련된 소형 Qwen-3-4B-thinking 모델은 여러 독점 및 글쓰기 튜닝(tuning) 시스템을 능가합니다. Arena-Write에서 승률은 6개의 강력한 기준선(baseline) 대비 약 68%에 도달합니다. 어블레이션(ablation) 연구 및 통찰력(insight)에 따르면, 제약 조건(constraint) 기반 보상(reward)은 LLM-as-judge보다 그룹 내 보상(reward) 분산이 더 높아 롤아웃(rollout) 간 더 나은 구별 능력을 나타냅니다. 소형 보상(reward) 모델 및 심지어 자체 보상(self-reward) 설정에서도 작동합니다. 사고 모드(thinking mode)와 ACE-RL의 조합은 장문 생성에서 비사고 모드(non-thinking mode)를 능가합니다.
논문

**5. ParaThinker: 다중 경로 추론으로 사고 확장**
이 연구는 현재의 "더 오래 생각하기" 전략이 LLM을 단일한 사고 방식에 갇히게 할 수 있다고 주장합니다. 이에 저자들은 여러 독립적인 추론 경로를 동시에 생성한 후, 이를 하나의 최종 답변으로 통합하도록 모델을 훈련하는 ParaThinker를 제안합니다. 이러한 폭(width) 확장 방식은 수학 벤치마크 전반에서 미미한 지연 시간(latency) 증가만으로도 정확도(accuracy)를 향상시키는 효과를 보여줍니다.

ParaThinker는 인간의 브레인스토밍(brainstorming)과 유사한 방식으로 작동합니다. 하나의 문제에 대해 여러 가지 다른 관점과 접근 방식을 동시에 탐색하고, 각기 다른 사고 과정을 거쳐 도출된 중간 결과들을 종합하여 최종 결론에 도달하는 것입니다. 이는 모델이 특정 초기 토큰(token) 선택으로 인해 최적이 아닌 경로에 갇히는 "터널 시야(Tunnel Vision)" 문제를 극복하는 데 도움을 줍니다. 이 방법은 특히 복잡한 문제 해결이나 창의적인 글쓰기와 같이 다양한 아이디어의 탐색과 융합이 중요한 작업에서 LLM의 능력을 크게 증대시킬 수 있습니다.

문제 정의 측면에서, 이 논문은 초기 토큰(token)이 모델을 최적이 아닌 경로로 이끌 수 있는 "터널 비전(Tunnel Vision)"이라고 불리는 테스트(test) 시간 병목 현상을 식별합니다. 동일한 토큰(token) 예산 하에서 다수결 방식의 병렬 샘플링(sampling)이 하나의 긴 체인(chain)을 능가할 수 있습니다. ParaThinker는 병렬 추론 및 요약의 두 단계를 실행합니다. 방법론적으로는 다양한 경로를 시작하기 위해 훈련 가능한 제어 토큰(token) <think i>를 사용하고, 다른 경로의 토큰(token)을 명확히 하기 위한 사고별 위치 임베딩(positional embedding)을 사용하며, 사고 중 경로를 분리하고 요약을 위해 통합하는 2단계 어텐션 마스크(attention mask)를 사용합니다. 재사전 채우기(re-prefill)를 피하기 위해 KV 캐시(cache)를 재사용합니다. 훈련 레시피는 교사 모델에서 샘플링(sampling)된 다중 경로 추적(trace)에 대한 지도 미세 조정(supervised fine-tuning)을 수행하며, 훈련에서 본 것보다 더 많은 경로로 학생 모델이 일반화(generalize)할 수 있도록 <think i>의 무작위 할당을 사용합니다. 세부 사항 및 데이터(data) 출처는 섹션 4 및 부록의 SFT 테이블(table)에 설명되어 있습니다.

결과적으로 AIME 2024/2025, AMC 2023, MATH-500에서 ParaThinker는 고정된 경로당 예산으로 8개 경로를 사용할 때 1.5B 모델의 경우 약 12.3%, 7B 모델의 경우 7.5%만큼 순차적 기준선(baseline) 대비 pass@1을 개선합니다. 평균적으로 다수결 투표를 1.5B 모델의 경우 4.3%, 7B 모델의 경우 2.0% 능가합니다. ParaThinker와 다수결 투표를 결합하면 추가적인 성능 향상을 얻을 수 있습니다. 효율성(efficiency) 및 설계 통찰력(design insight) 측면에서, 디코딩(decoding)이 메모리(memory) 대역폭(bandwidth)에 의해 제한되므로 경로가 많아질수록 지연 시간(latency)이 약간 증가합니다. 단일 A800에서 16개 경로는 동일한 길이에 대해 하나의 경로 시간의 2배 미만 소요됩니다. 최적의 종료 정책(policy)은 경로 길이를 동일하게 하고 정확도(accuracy)와 속도(speed)를 모두 향상시키는 "first-finish"입니다. 사고 임베딩(embedding)은 중요하며, 단순하게 평탄화된 위치는 성능을 저해합니다.
논문 | 트윗

**6. AgentGym-RL: 현실 환경 에이전트(Agent) 훈련 프레임워크(Framework)**
AgentGym-RL은 현실적인 환경 전반에 걸쳐 대규모 언어 모델(LLM) 에이전트(agent)를 강화 학습(reinforcement learning)으로 직접 훈련하기 위한 모듈식 프레임워크(framework)를 제시합니다. 또한, 훈련 전반에 걸쳐 상호 작용 범위(interaction horizon)를 점진적으로 확장하여 안정성과 성능을 향상시키는 간결한 스케줄(schedule)인 ScalingInter-RL을 소개합니다. 이 연구의 결과는 7B 규모의 오픈 소스 모델들이 웹 탐색, 심층 검색, 게임, 체화(embodied) 및 과학 분야의 복잡한 작업에서 더 큰 독점 시스템들과 경쟁하거나 심지어 능가할 수 있음을 보여줍니다.

AgentGym-RL의 독창성은 다양한 실제 환경을 통합하고, 에이전트가 이러한 환경 속에서 스스로 학습하고 성장할 수 있는 체계적인 방법을 제공한다는 점에 있습니다. ScalingInter-RL은 초기에는 짧은 상호 작용 기간을 통해 기본적인 행동을 빠르게 학습시키고, 점차 기간을 늘려나가면서 복잡한 계획 수립과 반성적 사고를 유도합니다. 이는 아이가 처음에는 간단한 규칙을 배우고 점차 복잡한 사회적 상황에 적응해나가는 과정과 유사합니다. 이러한 접근 방식은 일반 인공지능(AGI) 개발을 위한 중요한 진전이며, 특정 문제에만 특화된 AI가 아닌, 다양한 상황에 유연하게 대처할 수 있는 범용 에이전트(agent)의 가능성을 열어줍니다.

AgentGym-RL은 세 가지 플러그형 모듈(환경, 에이전트(agent), 훈련)을 갖춘 통합되고 분리된 RL 스택(stack)으로, PPO, GRPO, REINFORCE++를 지원하며 WebArena, Deep Search, TextCraft, BabyAI, SciWorld 전반에서 실행됩니다. 핵심 아이디어(idea)인 ScalingInter-RL은 활용(exploitation) 및 안정적인 학습을 강조하기 위해 짧은 범위(horizon)로 시작한 다음, 탐색(exploration) 및 계획(planning)과 반성(reflection)과 같은 더 풍부한 행동을 장려하기 위해 허용된 턴(turn)을 점진적으로 증가시킵니다. 에이전트(agent) 작업의 경우 모델 크기만으로 확장하는 것보다 후처리 훈련 및 테스트(test) 시간 컴퓨팅(compute)이 더 잘 확장된다는 점이 중요합니다. 이 프레임워크(framework)로 훈련된 7B 모델은 약 58.6%의 평균 성공률을 달성하고 훨씬 더 큰 기준선(baseline)을 능가합니다.

결과 스냅샷(snapshot)을 보면, 웹 탐색에서 ScalingInter-7B는 WebArena에서 전체 26.00%를 기록하여 GPT-4o의 16.00%를 능가합니다. 심층 검색에서는 전체 38.25로 GPT-4o의 26.75를 능가하며 강력한 오픈 기준선(baseline)에 근접합니다. NQ에서 52.00으로 최고, TriviaQA에서 70.00으로 동점입니다. 게임에서는 TextCraft에서 전체 91.00을 기록했으며, Depth 4(33.33)에서 0이 아닌 값을 가진 몇 안 되는 모델 중 하나입니다. 체화(Embodied) 작업에서는 BabyAI에서 96.67을 기록하여 전체 정확도(accuracy)에서 o3 및 GPT-4o를 능가합니다. 과학 분야에서는 SciWorld에서 57.00으로 SOTA(State-of-the-Art)를 달성했으며, 7B RL 모델도 50.50으로 강력한 성능을 보입니다. 훈련 동역학(dynamic) 분석 결과, 너무 이른 긴 범위(horizon)는 학습을 붕괴시킬 수 있으며, 짧은 범위(horizon)는 성능을 제한하는데, ScalingInter-RL은 이 두 가지 문제를 모두 방지합니다. 엔지니어링(engineering) 참고 사항으로, 병렬화된 브라우저(browser), 리셋 훅(reset hook) 및 메모리 누수(memory leak) 수정은 안정적인 장기 롤아웃(rollout)을 가능하게 하며, 시각적 UI(User Interface)는 궤적(trajectory) 및 실패 모드(failure mode)를 검사하는 데 도움을 줍니다. 실무자를 위한 조언으로는, 희소 보상(sparse-reward), 장기 궤적(long-trajectory) 에이전트(agent) 작업에는 REINFORCE++보다 GRPO를 선호하고, 상호 작용 길이 커리큘럼(curriculum)은 간단하고 견고한 이점을 제공하며, 파라미터(parameter) 스케일링(scaling) 전에 후처리 훈련 및 추론 샘플링(inference sampling)을 위한 컴퓨팅(compute) 예산을 책정하는 것이 좋습니다.
논문 | 트윗

**7. 대화, 항상 최선의 해법은 아니다**
다중 에이전트(multi-agent) 간의 토론이 언제나 긍정적인 결과를 가져오는 것은 아닙니다. 세 가지 추론 벤치마크(benchmark)와 다양한 에이전트(agent) 그룹을 대상으로 한 실험 결과, 토론은 오히려 정확도(accuracy)를 하락시키는 경우가 많았으며, 심지어 더 강력한 모델조차도 약한 동료의 잘못된 주장에 설득당하는 현상이 관찰되었습니다. 저자들은 현재의 정렬(alignment) 방식이 에이전트(agent)를 너무 순응적으로 만들어서, 설득력 있어 보이는 잘못된 추론을 비판 없이 받아들이게 한다고 지적합니다.

이러한 발견은 집단 지성의 함정에 대한 경고를 던집니다. 인간 사회에서도 '집단 사고(groupthink)'가 잘못된 의사결정으로 이어지는 경우가 있듯이, AI 에이전트(agent)들도 합의를 우선시하는 경향이 비판적 사고를 저해할 수 있습니다. 특히, 동료의 답변을 읽은 후 올바른 결론에서 잘못된 결론으로 바뀌는 "아첨하는 역전 현상"은 에이전트(agent) 간의 상호작용 설계에 대한 재고를 요구합니다.

실험 설정은 GPT-4o-mini, Llama-3.1-8B-Instruct, Mistral-7B-Instruct를 사용하여 CommonSenseQA, MMLU, GSM8K에서 토론을 평가했습니다. 에이전트(agent)는 한 번 답변한 후 두 라운드(round) 동안 토론하며, 최종 출력은 토론 전후의 다수결 투표입니다. 프롬프트(prompt)는 짧은 추론과 작업별 형식을 요구합니다. 주요 결과로, 토론은 특히 CommonSenseQA 및 MMLU에서 정확도(accuracy)를 자주 저해했습니다. 혼합 능력 설정(mixed-capability setting)을 포함한 많은 그룹에서 토론 후 일관된 하락을 보여주는데, 예를 들어 CSQA는 1×GPT + 2×Llama 조합에서 6.6점, 2×Llama + 1×Mistral 조합에서 8.0점 하락합니다. MMLU는 1×GPT + 2×Llama 조합에서 12.0점 하락합니다. GSM8K는 더 혼합된 결과를 보이며 일부 설정에서는 작은 이득이 있었습니다. 라운드(round)별 성능 저하 연구는 더 강력한 모델이 다수일 때도 토론이 진행됨에 따라 성능이 종종 저하됨을 보여줍니다. 그 이유는 에이전트(agent)가 비판보다 합의를 선호하는 경향이 있기 때문입니다. 라운드(round) 전반에 걸쳐 잘못된 답변으로의 전환(correct→incorrect)이 올바른 답변으로의 전환(incorrect→correct)보다 더 많으며, 이는 토론이 더 강력한 모델을 적극적으로 오도할 수 있음을 나타냅니다. 부록 예시는 동료의 답변을 읽은 후 올바른 답변에서 잘못된 답변으로의 아첨적인 역전 현상을 기록합니다.

이러한 시사점은 순진한 토론 프로토콜(protocol)이 오류를 증폭시킬 위험이 있다는 것입니다. 저자들은 독립적인 검증에 보상하고, 에이전트(agent)의 신뢰도 또는 자신감에 따라 주장에 가중치를 부여하며, 정당하지 않은 합의에 페널티(penalty)를 부과하여 토론의 이점을 보존하는 설계를 권장합니다.
논문 | 트윗

**8. AggLM: 강화 학습(RL) 기반 솔루션(Solution) 집계**
AggLM은 다수의 후보 솔루션(solution)을 통합하는 LLM 훈련에 강화 학습(reinforcement learning) 기법을 도입하여, 단순히 다수결 투표나 보상(reward) 모델(model) 순위 지정을 넘어서는 새로운 접근 방식을 제시합니다. 이 방법론은 더 높은 정확도(accuracy)를 달성하고, 소수의 올바른 답변을 효과적으로 찾아내며, 다양한 모델에 걸쳐 일반화(generalize)될 뿐만 아니라, 기존의 집계 방식보다 적은 토큰(token)을 사용합니다.

AggLM의 가치는 복잡한 문제 해결 과정에서 빛을 발합니다. 예를 들어, 여러 LLM이 생성한 다양한 코드 스니펫(code snippet)이나 복잡한 계산 결과들을 단순히 투표로 결정하는 대신, 강화 학습(RL)을 통해 최적의 조합을 학습함으로써 더욱 견고하고 정확한 최종 결과물을 도출할 수 있습니다. 이는 특히 창의적인 문제 해결이나, 여러 가지 가능한 접근 방식이 존재하는 상황에서 LLM의 잠재력을 최대한 발휘하게 합니다. AggLM은 단순한 집계가 아닌, 지능적인 통합을 통해 AI의 문제 해결 능력을 한 단계 끌어올리는 데 기여합니다.
논문 | 트윗

**9. 대규모 추론 모델(LRM)을 위한 강화 학습(RL) 연구 동향**
이 설문 조사는 강화 학습(reinforcement learning)이 어떻게 대규모 추론 모델(LRM)의 발전을 촉진하고 있는지 심층적으로 분석하며, 수학 및 코딩(coding)과 같은 복합적인 과제에서 더욱 강력한 성능을 가능하게 하는 메커니즘을 탐구합니다. 또한, 컴퓨팅(compute) 자원, 알고리즘(algorithm) 설계, 데이터(data) 관리 및 인프라(infrastructure) 구축에 있어 직면하는 확장성(scaling) 문제들을 조명하는 동시에, 인공 초지능(Artificial Superintelligence, ASI)으로 나아가기 위한 미래 방향성을 제시합니다.

본 조사는 RL이 LRM에 미치는 영향이 단순한 성능 향상을 넘어, 모델의 추론 능력 자체를 근본적으로 변화시키고 있음을 보여줍니다. 특히, RL은 모델이 시행착오를 통해 복잡한 규칙과 전략을 스스로 발견하고 내재화하도록 돕습니다. 그러나 이러한 발전은 막대한 계산 자원과 정교한 보상 설계, 그리고 안정적인 학습 환경 구축이라는 난제를 동반합니다. 이 설문 조사는 이러한 기술적 도전을 명확히 제시하고, 인공 초지능과 같은 미래의 비전을 달성하기 위해 필요한 연구 방향과 윤리적 고려사항을 포괄적으로 다룹니다.
논문 | 트윗

**10. LiveMCP-101: 현실 세계 에이전트(Agent) 성능 평가**
LiveMCP-101은 검색, 파일 처리, 수학 계산 및 데이터(data) 분석 등 도구 활용을 요구하는 다단계 작업에서 MCP(Multi-Competence Processor) 지원 에이전트(agent)의 성능을 시험하기 위해 고안된 101개의 실제 쿼리(query)로 구성된 새로운 벤치마크(benchmark)입니다. 이 벤치마크의 결과는 현재 선도적인 LLM들이 60% 미만의 성공률을 보이며, 도구 활용 오케스트레이션(orchestration) 능력에 중대한 약점이 있음을 드러내고 자율 AI 시스템(AI system) 발전을 위한 귀중한 통찰력(insight)을 제공합니다.

LiveMCP-101은 AI 에이전트(agent)가 실제 환경에서 직면하는 복잡성을 반영합니다. 단순히 하나의 도구를 사용하는 것을 넘어, 여러 도구를 순차적 또는 병렬적으로 조합하고, 각 도구의 출력을 다음 단계의 입력으로 활용하는 능력이 중요합니다. 이는 마치 숙련된 기술자가 다양한 도구를 적시에 사용하여 복잡한 작업을 완수하는 것과 같습니다. 현재 LLM의 낮은 성공률은 이러한 "도구 오케스트레이션"이 여전히 AI 연구의 주요 병목 현상임을 시사하며, 앞으로 실제 세상의 문제를 해결하는 자율 에이전트(agent)를 개발하기 위한 핵심적인 연구 방향을 제시합니다.
논문 | 트윗