## LLM 에이전트의 발전: 최신 강화 학습 연구 동향

최근 몇 년간 대규모 언어 모델(LLM)은 놀라운 발전을 이루었으며, 이제는 단순한 텍스트 생성을 넘어 복잡한 추론, 도구 사용, 자율적인 에이전트(agent) 행동을 수행하는 단계에 이르렀습니다. 이러한 발전의 핵심에는 강화 학습(reinforcement learning, RL)이 중요한 역할을 하고 있습니다. RL은 LLM이 환경과 상호작용하며 시행착오를 통해 학습하고, 특정 목표를 달성하기 위한 최적의 전략을 스스로 찾아내도록 돕습니다. 이 글에서는 최신 연구 논문들을 통해 LLM 에이전트의 강화 학습 적용 사례와 그로 인한 혁신적인 변화들을 심층적으로 살펴봅니다.

### 1. SFR-DeepResearch: 자율 에이전트 연구원의 탄생

**이 논문은 추론에 최적화된 LLM을 자율적인 단일 에이전트(agent) 연구원으로 전환시키는 간단한 강화 학습(reinforcement learning) 레시피인 SFR-DeepResearch를 소개합니다. 이 에이전트(agent)는 세 가지 도구(검색, 정적 페이지 브라우징, Python)만 사용하고, 자체 컨텍스트(context)를 관리하며, 길이 정규화된 REINFORCE 목적 함수(objective)를 사용하여 합성된 단문 및 장문 작업에 대해 엔드투엔드(end-to-end)로 훈련됩니다. 결과는 FRAMES, GAIA, Humanity’s Last Exam에서 상당한 성능 향상을 보여줍니다.**

SFR-DeepResearch는 최소한의 도구 세트(검색, 정적 페이지 브라우징, Python 인터프리터)만을 사용하여 복잡한 연구 작업을 수행하도록 LLM을 훈련합니다. 이 에이전트의 핵심적인 설계 특징 중 하나는 다중 턴(multi-turn) 도구 사용을 단일의 확장되는 컨텍스트 질문으로 재구성하고, 컨텍스트 한계에 도달할 때 자체 압축(self-compression)하는 `clean_memory` 도구를 추가하여 컨텍스트 관리를 효율적으로 수행한다는 점입니다. 이러한 접근 방식은 LLM 에이전트가 장기적인 작업을 수행할 때 발생하는 컨텍스트 길이의 제약을 효과적으로 해결합니다. 또한, 도구 사용 중 구문 분석(parsing) 및 구문(syntax) 오류가 발생하더라도 복구 또는 재시도 루틴(routine)을 트리거하여 롤아웃(rollout)의 안정성을 유지함으로써, 실제 환경에서의 견고성을 높입니다.

**RL 레시피: 합성된, Hotpot보다 어려운 다중 홉(multi-hop) QA와 보고서 작성 작업을 사용합니다. 궤적(trajectory) 길이로 나누는 시간적 이점 정규화(temporal advantage normalization)를 포함한 그룹 REINFORCE 목적 함수(objective)를 최적화(optimize)하며, 궤적(trajectory) 필터링(filtering) 및 부분 롤아웃(rollout) 재사용을 수행합니다. 지역화되고 캐시(cache)된 도구와 오염 차단 목록(contamination blocklist)은 훈련 및 평가를 안정화합니다.**

SFR-DeepResearch의 결과는 인상적입니다. 최적 모델인 SFR-DR-20B는 FRAMES, GAIA, HLE와 같은 주요 벤치마크에서 유사한 오픈 에이전트를 능가하며, 독점 시스템과 경쟁할 만한 강력한 성능을 보여줍니다. 특히, 길이 정규화(length normalization)는 보상 및 정확도를 저해하는 폭주하는 도구 호출을 억제하여 에이전트의 효율적인 행동을 유도합니다.

### 2. 출현적 계층적 추론(Emergent Hierarchical Reasoning)

이 논문은 LLM이 강화 학습을 통해 2단계 계층적 추론 능력을 출현시킨다고 주장합니다. 초기 RL 훈련 단계에서는 모델이 저수준 실행(execution) 토큰(token)의 혼란도(perplexity)와 엔트로피(entropy)를 감소시켜 절차적 기술을 통합하는 데 집중합니다. 이후 성능 향상은 고수준 계획(planning) 토큰의 다양성 증가 및 더 길고 정확한 추적(trace)과 일치하며, 이는 LLM이 복잡한 문제 해결 과정에서 "아하 모멘트(aha moments)"를 경험하고 문제 해결 길이를 확장할 수 있음을 시사합니다. 저자들은 전략적 계획 토큰에 대한 크레딧(credit)을 높이는 HICRA(Hierarchical Credit Assignment)를 제안하여 GRPO(Generalized Policy Optimization) 대비 일관된 성능 향상을 입증합니다. 특히, 토큰 수준 엔트로피보다 의미론적 엔트로피(semantic entropy)가 탐색(exploration) 신호로서 더 우수하다는 점을 강조합니다. 이는 단순한 토큰 예측의 불확실성을 넘어, 모델이 얼마나 다양한 고수준 전략을 탐색하는지를 측정하는 데 중요합니다. HICRA는 Qwen, Llama, VLM 등 다양한 모델에서 AIME24/25, Math500, AMC23과 같은 수학 및 멀티모달(multimodal) 작업의 Pass@1을 개선하여, 전략적 의사결정에 최적화를 집중하는 것이 복잡한 추론 작업에 얼마나 효과적인지를 보여줍니다.

### 3. RAG 기반 디코딩(decoding) 재고

REFRAG는 검색 증강 생성(RAG) 시스템의 효율성을 혁신적으로 개선하는 방법을 제안합니다. 기존 RAG는 검색된 모든 컨텍스트(context)를 LLM의 프롬프트(prompt)에 포함시켜 처리 지연 시간(latency)과 메모리(memory) 사용량 측면에서 병목 현상을 야기했습니다. REFRAG는 디코드(decode) 시점에 대부분의 검색된 토큰을 미리 계산된 청크(chunk) 임베딩(embedding)으로 대체한 다음, 중요한 몇몇 청크만 선택적으로 확장하는 방식을 사용합니다. 이는 RAG 프롬프트의 블록 대각선 어텐션(block-diagonal attention)을 활용하여 정확도(accuracy)를 유지하면서도 지연 시간 및 메모리를 획기적으로 감소시킵니다. 핵심 아이디어는 검색된 컨텍스트를 경량 인코더(encoder)로 인코딩하여 디코더(decoder)의 임베딩 크기로 투영한 후, 사용자 쿼리(query)와 함께 직접 공급하는 것입니다. RL 정책(policy)은 어떤 청크를 압축 해제 상태로 유지할지 결정하여("어디서든 압축") 최적의 균형을 찾습니다. 이 방법은 LLaMA 대비 최대 30.85배의 첫 토큰 생성 시간 가속을 달성하며, 처리량(throughput)을 최대 6.78배 증가시킵니다. 또한, 압축을 통해 유효 컨텍스트(context) 길이를 16배 확장하여 더 긴 시퀀스(sequence)에서도 혼란도(perplexity)를 유지하거나 개선하는 등, 실제 애플리케이션(application)에서 RAG의 활용도를 크게 높이는 데 기여합니다.

### 4. ACE-RL: 체크리스트 기반 보상으로 장문 생성 제어

**ACE-RL은 거친 선호도 쌍(preference-pair) 보상(reward)을 지시문별로 검증 가능한 체크리스트(checklist)로 대체하는 강화 학습(reinforcement learning) 프레임워크(framework)입니다. ACE-RL은 각 장문 작업을 명시적 및 암묵적 제약 조건(constraint) 세트로 전환하고, 모델의 출력이 이를 얼마나 잘 충족하는지에 따라 점수를 매기며, GRPO 훈련 중 길이 제어 보상(reward)과 이를 혼합합니다. 그 결과 도메인(domain) 및 스타일(style) 전반에 걸쳐 더 강력하고 제어 가능한 장문 작성이 가능해집니다.**

기존의 LLM 훈련에서 선호도 쌍 기반의 보상 모델은 비용이 많이 들고, 미묘한 지시문 제약 조건을 반영하기 어렵다는 한계가 있었습니다. ACE-RL은 이러한 문제를 해결하기 위해 지시문을 세분화된 체크리스트로 자동 분해하고, 작은 LLM을 검증기(validator)로 사용하여 각 항목을 3단계 루브릭(완전히/부분적으로/충족하지 못함)으로 평가합니다.

**핵심 아이디어(idea): 각 지시문을 세분화된 체크리스트(checklist)(명시적 및 암묵적 요구 사항)로 자동 분해한 다음, 작은 LLM을 사용하여 3단계 루브릭(rubric)(완전히/부분적으로/충족하지 못함)으로 각 항목을 검증합니다. 보상(Reward) = 평균 체크리스트(checklist) 점수 + 길이 보상(reward)이며, GRPO로 최적화(optimize)됩니다.**

이러한 접근 방식은 LLM이 관련성, 일관성, 유용성과 같은 일반적인 품질 기준을 넘어, 지시문에 특화된 적응형 품질을 달성하도록 유도합니다. WritingBench 및 Arena-Write와 같은 벤치마크에서 ACE-RL은 SFT(Supervised Fine-Tuning) 및 LLM-as-judge 기반 RL보다 모델 성능을 크게 향상시킵니다. 특히, 소형 Qwen-3-4B-thinking 모델이 여러 독점 및 글쓰기 튜닝(tuning) 시스템을 능가하는 결과는 ACE-RL이 모델 크기에 관계없이 효과적인 장문 생성 제어를 가능하게 함을 보여줍니다. 이는 LLM이 특정 요구 사항을 정확히 충족하는 콘텐츠를 생성해야 하는 전문적인 글쓰기 작업에서 특히 유용합니다.

### 5. ParaThinker: 병렬 추론으로 터널 비전 극복

ParaThinker는 LLM의 "터널 비전(Tunnel Vision)" 문제를 해결하기 위해 여러 독립적인 추론 경로를 병렬로 생성한 다음, 이를 하나의 일관된 답변으로 융합하는 새로운 추론 전략을 제안합니다. 기존의 "더 오래 생각하기" 전략은 LLM을 단일 사고 방식에 가두어 초기 토큰이 최적이 아닌 경로로 모델을 이끌 수 있다는 문제점을 가지고 있습니다. ParaThinker는 `<think i>`와 같은 훈련 가능한 제어 토큰(control token)을 사용하여 다양한 추론 경로를 시작하고, 사고별 위치 임베딩(positional embedding) 및 2단계 어텐션 마스크(attention mask)를 통해 경로를 분리하고 통합합니다. 이 방법은 재사전 채우기(re-prefill)를 피하기 위해 KV 캐시(cache)를 재사용하여 효율성을 높입니다. 수학 벤치마크(benchmark) 전반에 걸쳐 ParaThinker는 작은 지연 시간(latency) 비용만 추가하면서도 정확도(accuracy)를 크게 높입니다. 예를 들어, AIME 2024/2025, AMC 2023, MATH-500에서 8개 경로를 사용할 때 순차적 기준선 대비 pass@1을 1.5B 모델의 경우 약 12.3%, 7B 모델의 경우 7.5% 개선합니다. 이는 다수결 투표와 결합될 때 추가적인 성능 향상을 가져올 수 있습니다. ParaThinker는 LLM이 하나의 고정된 사고 흐름에 갇히지 않고, 다양한 관점에서 문제를 탐색하여 더 견고하고 정확한 해답을 도출할 수 있음을 보여줍니다.

### 6. AgentGym-RL: 현실적인 환경에서 LLM 에이전트 훈련

**AgentGym-RL은 현실적인 환경 전반에 걸쳐 강화 학습(reinforcement learning)을 통해 LLM 에이전트(agent)를 직접 훈련하기 위한 모듈형 프레임워크(framework)와, 안정성 및 성능 향상을 위해 훈련 전반에 걸쳐 상호 작용 범위(interaction horizon)를 늘리는 간단한 스케줄(schedule)인 ScalingInter-RL을 소개합니다. 결과는 7B 오픈 모델이 웹 탐색, 심층 검색, 게임, 체화(embodied) 및 과학 작업에서 더 큰 독점 시스템과 경쟁하거나 능가할 수 있음을 보여줍니다.**

AgentGym-RL은 환경, 에이전트, 훈련의 세 가지 플러그형 모듈(module)을 갖춘 통합되고 분리된 RL 스택(stack)을 제공하며, PPO, GRPO, REINFORCE++와 같은 다양한 RL 알고리즘을 지원합니다. 이 프레임워크는 WebArena, Deep Search, TextCraft, BabyAI, SciWorld 등 광범위한 현실적인 벤치마크 환경에서 LLM 에이전트를 훈련하고 평가할 수 있도록 설계되었습니다.

**핵심 아이디어(idea): ScalingInter-RL은 활용(exploitation) 및 안정적인 학습을 강조하기 위해 짧은 범위(horizon)로 시작한 다음, 탐색(exploration) 및 계획(planning)과 반성(reflection)과 같은 더 풍부한 행동을 장려하기 위해 허용된 턴(turn)을 점진적으로 증가시킵니다. 중요한 이유: 에이전트(agent) 작업의 경우 모델 크기만으로 확장하는 것보다 후처리 훈련 및 테스트(test) 시간 컴퓨팅(compute)이 더 잘 확장됩니다. 이 프레임워크(framework)로 훈련된 7B 모델은 약 58.6%의 평균 성공률을 달성하고 훨씬 더 큰 기준선(baseline)을 능가합니다.**

ScalingInter-RL은 훈련 초기에 짧은 상호 작용 범위(interaction horizon)를 사용하여 에이전트가 기본적인 기술을 안정적으로 학습하도록 돕고, 점진적으로 범위를 늘려 복잡한 계획과 반성 같은 고수준 행동을 개발하도록 유도합니다. 이러한 커리큘럼(curriculum) 학습 방식은 희소 보상(sparse-reward) 및 장기 궤적(long-trajectory) 에이전트 작업에서 학습의 안정성과 성능을 크게 향상시킵니다. AgentGym-RL로 훈련된 7B 모델은 웹 탐색(WebArena), 심층 검색, 게임(TextCraft), 체화(BabyAI), 과학(SciWorld) 작업에서 GPT-4o와 같은 더 큰 독점 모델을 능가하는 놀라운 성능을 보여주며, 에이전트 작업에서 모델 크기보다는 효율적인 훈련 방법론이 더 중요할 수 있음을 시사합니다.

### 7. 대화가 항상 저렴한 것은 아니다: 다중 에이전트 토론의 함정

이 논문은 다중 에이전트(multi-agent) 토론이 항상 LLM의 추론 정확도(accuracy)를 향상시키지는 않는다는 충격적인 발견을 제시합니다. CommonSenseQA, MMLU, GSM8K와 같은 추론 벤치마크에서 GPT-4o-mini, Llama-3.1-8B-Instruct, Mistral-7B-Instruct와 같은 이질적인 에이전트 풀(pool)을 사용하여 토론을 평가한 결과, 토론이 오히려 정확도를 낮추는 경우가 많다는 것을 발견했습니다. 특히, 더 강력한 모델이 약한 동료의 설득력 있지만 잘못된 추론에 흔들려 결국 오답을 선택하는 경우가 빈번했습니다. 이 연구는 에이전트들이 비판보다는 합의를 선호하는 경향이 있으며, 현재의 정렬(alignment) 방식이 에이전트를 너무 순응적으로 만들어 잘못된 주장을 받아들이고 도전하지 않도록 만든다고 주장합니다. 토론 라운드(round)가 진행될수록 올바른 답변에서 잘못된 답변으로의 전환(correct→incorrect)이 잘못된 답변에서 올바른 답변으로의 전환(incorrect→correct)보다 더 많다는 결과는 이러한 주장을 뒷받침합니다. 이는 순진한 다중 에이전트 토론 프로토콜(protocol)이 오류를 증폭시킬 위험이 있음을 경고하며, 독립적인 검증에 보상하고, 에이전트의 신뢰도에 따라 주장에 가중치를 부여하며, 정당하지 않은 합의에 페널티(penalty)를 부과하는 등 신중한 설계가 필요함을 강조합니다.

### 8. AggLM: 강화 학습 기반 솔루션 집계

AggLM은 여러 후보 솔루션(solution)을 집계하는 과정에 강화 학습(reinforcement learning)을 도입하여, 단순한 다수결 투표나 보상 모델(reward model) 순위 지정을 넘어섭니다. LLM이 생성한 다양한 후보 해답들을 더 정교하게 결합함으로써, AggLM은 더 높은 정확도를 달성하고, 심지어 소수 정답까지 복구할 수 있습니다. 이는 모델 전반에 걸쳐 일반화(generalize)되며, 전통적인 집계 방법보다 적은 토큰(token)을 사용하여 효율성 측면에서도 이점을 제공합니다. AggLM의 접근 방식은 복잡한 문제 해결에서 LLM이 다양한 관점을 통합하고 최적의 결론을 도출하는 데 중요한 역할을 할 수 있음을 시사합니다.

### 9. 대규모 추론 모델(Large Reasoning Models)을 위한 RL 설문 조사

이 설문 조사는 강화 학습(reinforcement learning)이 대규모 추론 모델(LRM)의 발전에 어떻게 기여하는지 종합적으로 검토합니다. 특히 수학 및 코딩(coding)과 같은 복잡한 작업에서 RL이 LRM의 성능을 어떻게 향상시키는지 분석합니다. 설문 조사는 계산, 알고리즘(algorithm), 데이터(data), 인프라(infrastructure) 스케일링(scaling)과 관련된 도전 과제들을 강조하며, 인공 초지능(Artificial Superintelligence, ASI)을 향한 미래 연구 방향을 제시합니다. 이는 LLM이 단순히 텍스트를 이해하고 생성하는 것을 넘어, 고차원적인 추론 능력을 갖춘 지능형 시스템으로 발전하는 데 RL이 필수적인 요소임을 보여줍니다.

### 10. LiveMCP-101: 다단계 도구 사용 에이전트 벤치마크

LiveMCP-101은 검색, 파일 작업, 수학 및 데이터(data) 분석 등 다단계 도구 사용을 요구하는 복잡한 작업에서 MCP(Multi-tool Conversational Agent) 지원 에이전트(agent)를 테스트(test)하도록 설계된 101개의 실제 쿼리(query)로 구성된 새로운 벤치마크(benchmark)입니다. 이 벤치마크의 결과는 현재 선도적인 LLM들이 60% 미만의 성공률을 보이며, 도구 오케스트레이션(orchestration) 능력에 중대한 약점이 있음을 드러냅니다. LiveMCP-101은 자율 AI 시스템(AI system) 발전을 위한 중요한 통찰력(insight)을 제공하며, LLM 에이전트가 실제 세계의 복잡한 문제를 해결하기 위해 필요한 다단계 계획 및 실행 능력을 측정하고 개선하는 데 핵심적인 역할을 할 것입니다.

### 결론

오늘날 LLM 에이전트의 발전은 강화 학습의 끊임없는 혁신과 궤를 같이하고 있습니다. SFR-DeepResearch가 보여준 자율적인 연구원 에이전트의 가능성, 출현적 계층적 추론이 제시하는 LLM의 사고 방식 변화, REFRAG를 통한 RAG 시스템의 효율성 증대, ACE-RL이 제공하는 장문 생성의 정밀한 제어, ParaThinker의 병렬 사고를 통한 추론 능력 향상, 그리고 AgentGym-RL을 통한 현실 환경에서의 에이전트 훈련은 모두 LLM의 한계를 확장하고 있습니다.

동시에, "대화가 항상 저렴한 것은 아니다" 논문처럼 다중 에이전트 시스템의 복잡성과 잠재적 함정을 이해하고, AggLM과 같은 새로운 집계 방법을 통해 집단 지성을 효과적으로 활용하는 연구도 활발합니다. 이러한 연구들은 LLM이 단순한 언어 모델을 넘어, 복잡한 문제를 자율적으로 해결하고, 다양한 도구를 효과적으로 활용하며, 현실 세계와 상호작용하는 진정한 지능형 에이전트로 진화하고 있음을 보여줍니다. LiveMCP-101과 같은 새로운 벤치마크는 이러한 발전의 길을 안내하며, 미래의 AI 시스템이 나아가야 할 방향을 제시합니다. 강화 학습은 LLM이 인공 초지능(ASI)을 향한 여정에서 필수적인 동력이 될 것이며, 앞으로도 이 분야의 혁신적인 연구는 계속될 것입니다.