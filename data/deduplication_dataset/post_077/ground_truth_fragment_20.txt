**1. SFR-DeepResearch**
SFR-DeepResearch는 추론에 최적화된 LLM을 자율적인 단일 에이전트(agent) 연구원으로 전환시키는 간단한 강화 학습(reinforcement learning) 레시피를 소개합니다. 최근 인공지능 연구에서 대규모 언어 모델(LLM)의 잠재력을 최대한 활용하기 위한 에이전트(agent) 시스템 개발이 활발하며, 이러한 시스템은 단순한 정보 검색을 넘어, 자율적으로 문제를 인식하고 해결하는 능력을 갖추도록 진화하고 있습니다. 특히, 강화 학습(reinforcement learning)은 에이전트(agent)가 다양한 환경에서 시행착오를 통해 최적의 전략을 학습하도록 돕는 핵심 기술로 부상했습니다. 이 에이전트(agent)는 세 가지 도구(검색, 정적 페이지 브라우징, Python)만 사용하고, 자체 컨텍스트(context)를 관리하며, 길이 정규화된 REINFORCE 목적 함수(objective)를 사용하여 합성된 단문 및 장문 작업에 대해 엔드투엔드(end-to-end)로 훈련됩니다. 결과는 FRAMES, GAIA, Humanity’s Last Exam에서 상당한 성능 향상을 보여줍니다. 에이전트(agent) 설계 및 스캐폴딩(scaffolding)은 QwQ 및 Qwen 모델의 경우, 다중 턴(multi-turn) 도구 사용을 단일의 확장되는 컨텍스트(context) 질문으로 재구성하고, 프롬프트(prompt)를 안정적으로 유지하기 위해 이전의 긴 CoT(Chain-of-Thought)를 생략합니다. 한계에 가까워질 때 컨텍스트(context)를 자체 압축하는 clean_memory 도구를 추가합니다. 최소한의 도구 세트와 내결함성(fault tolerance)을 갖추고 있으며, 도구는 기본적인 검색 API, 하이퍼링크(hyperlink) 클릭 기능이 없는 정적 마크다운(Markdown) 페이지 스크래퍼(scraper), 그리고 상태 비저장(stateless) 로컬(local) Python 인터프리터(interpreter)로 제한되어 있어 전략을 학습하기에 충분히 어려운 훈련 환경을 조성합니다. 구문 분석(parsing) 및 구문(syntax) 오류 발생 시 복구 또는 재시도 루틴(routine)이 트리거(trigger)되어 롤아웃(rollout)을 정상적으로 유지합니다. RL 레시피는 합성된, Hotpot보다 어려운 다중 홉(multi-hop) QA와 보고서 작성 작업을 사용합니다. 궤적(trajectory) 길이로 나누는 시간적 이점 정규화(temporal advantage normalization)를 포함한 그룹 REINFORCE 목적 함수(objective)를 최적화(optimize)하며, 궤적(trajectory) 필터링(filtering) 및 부분 롤아웃(rollout) 재사용을 수행합니다. 지역화되고 캐시(cache)된 도구와 오염 차단 목록(contamination blocklist)은 훈련 및 평가를 안정화합니다. 최적 모델인 SFR-DR-20B는 FRAMES에서 82.8, GAIA(텍스트 전용)에서 66.0, HLE 전체 텍스트 전용에서 28.7을 달성하여 유사한 오픈 에이전트(agent)를 능가하고 오염 차단 목록(contamination blocklist) 하에서 더 강력한 독점 시스템과 경쟁합니다. 어블레이션(ablation) 및 동작 분석 결과, 단일 턴(single-turn) 스캐폴딩(scaffolding)은 Qwen 및 QwQ의 기본 다중 턴(multi-turn) 템플릿(template)을 능가하며, FRAMES에서 큰 성능 향상을 보입니다. 길이 정규화(length normalization)는 보상(reward) 및 정확도(accuracy)를 저해하는 폭주하는 도구 호출을 억제합니다. 도구 사용 및 토큰(token) 길이 분석은 gpt-oss-20B가 도구를 더 많이 호출하지만 단계별 CoT(Chain-of-Thought)는 훨씬 짧게 작성하여 Qwen 계열 모델보다 더 나은 토큰(token) 효율성(efficiency)을 나타냄을 보여줍니다. 그러나 실제 세계의 복잡성과 예측 불가능성은 에이전트(agent) 설계에 있어 여전히 큰 난관으로 작용하며, 따라서 모델의 견고성과 적응성을 높이는 방안에 대한 심층적인 탐구가 요구됩니다.

**2. 출현적 계층적 추론(Emergent Hierarchical Reasoning)**
이 논문은 RL이 출현적인 2단계 계층 구조를 통해 LLM 추론을 개선한다고 주장합니다. 계층적 추론은 인간의 사고 과정에서 흔히 관찰되는 현상으로, 복잡한 문제를 작은 단위로 나누어 해결하는 방식이며, 인공지능 분야에서는 이러한 계층성을 모델에 통합하려는 시도가 꾸준히 이어져 왔습니다. 첫째, 모델이 저수준 실행(execution)을 확고히 하고, 둘째, 진행은 고수준 계획(planning) 탐색에 달려 있습니다. 이를 바탕으로 저자들은 전략적 계획 토큰(token)에 대한 크레딧(credit)을 높이는 HICRA를 제안하고, GRPO 대비 일관된 성능 향상을 보여줍니다. 또한 토큰(token) 수준 엔트로피(entropy)보다 더 나은 탐색 신호로 의미론적 엔트로피(semantic entropy)를 제안합니다. 2단계 동역학(dynamic)에서 초기 RL 훈련은 실행(execution) 토큰(token)의 혼란도(perplexity)와 엔트로피(entropy)를 감소시켜 절차적 기술을 통합합니다. 후기 성능 향상은 계획 토큰(token)의 다양성 증가 및 더 길고 정확한 추적(trace)과 일치하며, 이는 "아하 모멘트(aha moments)" 및 길이 스케일링(scaling)을 설명합니다. 계획(planning) 대 실행(execution)의 균형은 복잡한 작업에서 인공지능의 성공을 좌우하며, 이 논문은 전략적 그램(gram)(예: 연역, 분기, 역추적)을 계획 토큰(token)으로 기능적으로 태그(tag)하여 절차적 단계와 구별합니다. 이 라벨링(labeling)은 학습 병목 현상이 전략으로 이동함을 보여줍니다. HICRA 알고리즘(algorithm)은 스칼라(scalar) α를 사용하여 계획 토큰(token)에 대한 이점(advantage)을 증폭시켜 GRPO를 수정함으로써, 모든 토큰(token)에 분산하는 대신 영향력 있는 전략적 결정에 최적화(optimization)를 집중합니다. 이는 목표 지향적 탐색(exploration) 및 효과적인 전략의 더 빠른 강화를 생성하며, 섹션 3에서 공식화(formulation)를 제공합니다. 결과적으로 Qwen, Llama 및 VLM 전반에 걸쳐 HICRA는 AIME24/25, Math500, AMC23 및 멀티모달(multimodal) 수학 스위트(suite)에서 Pass@1을 개선하며, 종종 GRPO보다 몇 점 더 높게 나타납니다. 플롯(plot)은 더 높은 의미론적 엔트로피(semantic entropy)가 더 높은 검증 정확도(validation accuracy)를 추적함을 보여줍니다. 중요한 신호로, 실행(execution) 토큰(token)이 지배적이기 때문에 실제 탐색(exploration)이 증가하더라도 토큰(token) 수준 엔트로피(entropy)는 감소할 수 있습니다. 전략적 그램(gram)에 대한 의미론적 엔트로피(semantic entropy)가 전략적 탐색(exploration)을 더 잘 포착하고 성능과 상관관계가 있습니다. 한계 및 범위는 HICRA가 모델이 이미 절차적 기반을 가지고 있을 때 가장 잘 작동하며, 약한 기반에서는 계획(planning)에 대한 집중이 도움이 되지 않을 수 있다는 점입니다. 이 논문은 더 높은 수준의 행동 공간, 적응형 커리큘럼(curriculum) 및 프로세스(process) 지향적 보상(reward)에 대한 향후 연구를 제안합니다. 이러한 접근 방식은 단순한 패턴 인식 수준을 넘어, 심층적인 문제 해결 능력을 갖춘 인공지능 개발에 중요한 이정표가 될 수 있으며, AI 시스템이 단순히 데이터를 처리하는 것을 넘어, 지능적으로 사고하고 행동하는 방향으로 나아가는 데 기여합니다.

**3. RAG 기반 디코딩(decoding) 재고**
REFRAG는 디코드(decode) 시점에 대부분의 검색된 토큰(token)을 미리 계산된 청크(chunk) 임베딩(embedding)으로 대체한 다음, 중요한 몇몇 청크(chunk)만 선택적으로 확장하여 성능을 개선합니다. 검색 증강 생성(RAG)은 대규모 언어 모델(LLM)이 외부 지식 기반에서 정보를 검색하여 답변의 정확성과 신뢰성을 높이는 데 중요한 역할을 하지만, 검색된 컨텍스트(context)의 양이 많아질수록 처리 지연 시간(latency)과 메모리(memory) 사용량이 증가하는 문제가 발생합니다. REFRAG의 핵심 아이디어(idea)는 검색된 컨텍스트(context)를 청크(chunk)로 분할하고, 각 청크(chunk)를 경량 인코더(encoder)로 인코딩(encode)하며, 디코더(decoder)의 임베딩(embedding) 크기로 투영한 다음, 사용자 쿼리(query)와 함께 임베딩(embedding)을 직접 공급하는 것입니다. RL 정책(policy)은 어떤 청크(chunk)를 압축 해제 상태로 유지할지 결정합니다("어디서든 압축", 접두사(prefix)뿐만 아니라). 정확도(accuracy) 손실 없는 큰 속도 향상은 실시간 애플리케이션(application)에 필수적이며, 높은 압축률에서 LLaMA 대비 최대 30.85배(CEPE 대비 3.75배)의 첫 토큰(token) 생성 시간 가속을 달성하며 유사한 혼란도(perplexity)를 보입니다. 처리량(throughput)은 최대 6.78배 증가합니다. 더 긴 유효 컨텍스트(context)를 위해 압축을 통해 모델은 훨씬 더 큰 컨텍스트(context)를 처리할 수 있으며(16배 확장 보고), 시퀀스(sequence) 길이가 증가함에 따라 혼란도(perplexity)를 유지하거나 개선합니다. 고정된 지연 시간(latency) 예산 하에서 REFRAG는 더 많은 구절을 사용하고 16개 RAG 작업에서 LLaMA 기준선(baseline)을 능가하며, 집계된 플롯(plot)과 상세 결과는 강력한 검색기(retriever)와 약한 검색기(retriever) 모두에서 성능 향상을 보여줍니다. 애플리케이션(application) 전반의 일반화(generalization) 측면에서, 다중 턴(multi-turn) 대화형 QA에서 REFRAG는 더 긴 기록을 보존하고 구절 및 턴(turn)이 증가함에 따라 점수를 개선합니다. 장문 문서 요약에서는 일치하는 디코더(decoder) 토큰(token)으로 최고의 ROUGE를 달성합니다. 이러한 기술은 장문 문서 요약이나 대화형 질의응답 시스템과 같이 컨텍스트(context) 길이가 중요한 애플리케이션(application)에서 더욱 빛을 발하며, 미래에는 더욱 정교한 컨텍스트(context) 관리와 압축 기술이 RAG 시스템의 성능을 극대화할 것으로 예상됩니다.

**4. ACE-RL**
ACE-RL은 거친 선호도 쌍(preference-pair) 보상(reward)을 지시문별로 검증 가능한 체크리스트(checklist)로 대체하는 강화 학습(reinforcement learning) 프레임워크(framework)입니다. 강화 학습(RL)에서 보상(reward) 모델링은 학습의 방향을 결정하는 핵심 요소이지만, 인간의 선호도에 기반한 보상(reward)은 주관적이고 비용이 많이 들며, 미묘한 지시 사항을 정확히 반영하기 어렵다는 한계가 있습니다. ACE-RL은 이러한 한계를 극복하기 위한 혁신적인 방법론을 제시하며, 각 장문 작업을 명시적 및 암묵적 제약 조건(constraint) 세트로 전환하고, 모델의 출력이 이를 얼마나 잘 충족하는지에 따라 점수를 매기며, GRPO 훈련 중 길이 제어 보상(reward)과 이를 혼합합니다. 핵심 아이디어(idea)는 각 지시문을 세분화된 체크리스트(checklist)(명시적 및 암묵적 요구 사항)로 자동 분해한 다음, 작은 LLM을 사용하여 3단계 루브릭(rubric)(완전히/부분적으로/충족하지 못함)으로 각 항목을 검증하는 것입니다. 보상(Reward)은 평균 체크리스트(checklist) 점수와 길이 보상(reward)의 합이며, GRPO로 최적화(optimize)됩니다. 이는 관련성/일관성/유용성을 넘어 지시문 적응형 품질로 이동하고, 선호도 쌍(preference pair)이 필요 없어 비용을 절감하고 확장성(scalability)을 향상시킵니다. 데이터(data) 및 설정은 32K 장문 지시문, 프롬프트(prompt)당 평균 5.48개 제약 조건(constraint), 목표 길이 약 2.3K 단어를 사용하며, 검증기는 Qwen3-8B를 사용하고, 길이 보상(reward)은 허용 범위(tolerance band)를 벗어나는 편차에 페널티(penalty)를 부과합니다. 이러한 데이터(data) 및 설정은 모델의 학습 효율성과 일반화 능력에 결정적인 영향을 미칩니다. 결과적으로 WritingBench에서 ACE-RL은 SFT 및 LLM-as-judge RL보다 모델 성능을 크게 향상시키며, 예를 들어 Qwen-2.5-7B는 57.0에서 78.6으로 상승합니다. ACE-RL로 훈련된 소형 Qwen-3-4B-thinking 모델은 여러 독점 및 글쓰기 튜닝(tuning) 시스템을 능가하며, Arena-Write에서 승률은 6개의 강력한 기준선(baseline) 대비 약 68%에 도달합니다. 어블레이션(ablation) 및 통찰력(insight) 분석에 따르면, 제약 조건(constraint) 기반 보상(reward)은 LLM-as-judge보다 그룹 내 보상(reward) 분산이 더 높아 롤아웃(rollout) 간 더 나은 구별 능력을 나타내며, 소형 보상(reward) 모델 및 심지어 자체 보상(self-reward) 설정에서도 작동합니다. 사고 모드(thinking mode)와 ACE-RL의 조합은 장문 생성에서 비사고 모드(non-thinking mode)를 능가합니다. 이러한 발전은 다양한 창의적 작업에서 더 강력하고 제어 가능한 장문 작성이 가능해지며, 텍스트 생성의 품질을 객관적으로 평가하고 개선하는 데 기여하며, 특히 법률 문서 작성이나 기술 보고서와 같이 정확성과 규정 준수가 중요한 분야에서 큰 잠재력을 가집니다.

**5. ParaThinker**
이 논문은 오늘날의 "더 오래 생각하기" 전략이 LLM을 단일 사고 방식에 가두어 특정 편향에 이끌 수 있음을 지적합니다. 저자들은 여러 독립적인 추론 경로를 병렬로 생성한 다음 이를 하나의 답변으로 융합하도록 모델을 훈련하는 ParaThinker를 제안합니다. 인공지능의 추론 능력은 단일 경로를 따라 깊이 파고드는 것과 여러 경로를 동시에 탐색하는 것 사이의 균형점을 찾는 데 달려 있으며, 인간도 문제 해결 시 다양한 아이디어를 병렬적으로 고려하고 그중 최적의 해법을 선택하는 과정을 거칩니다. 수학 벤치마크(benchmark) 전반에 걸쳐 이 폭 스케일링(width-scaling)은 작은 지연 시간(latency) 비용만 추가하면서 정확도(accuracy)를 높입니다. 이 논문은 초기 토큰(token)이 모델을 최적이 아닌 경로로 이끌 수 있는 "터널 비전(Tunnel Vision)"이라고 불리는 테스트(test) 시간 병목 현상을 식별하며, 동일한 토큰(token) 예산 하에서 다수결 방식의 병렬 샘플링(sampling)이 하나의 긴 체인(chain)을 능가할 수 있음을 보여줍니다. ParaThinker는 병렬 추론 및 요약의 두 단계를 실행합니다. 다양한 경로를 시작하기 위해 훈련 가능한 제어 토큰(token) <think i>를 사용하고, 다른 경로의 토큰(token)을 명확히 하기 위한 사고별 위치 임베딩(positional embedding)을 사용하며, 사고 중 경로를 분리하고 요약을 위해 통합하는 2단계 어텐션 마스크(attention mask)를 사용합니다. 재사전 채우기(re-prefill)를 피하기 위해 KV 캐시(cache)를 재사용합니다. 훈련 레시피는 교사 모델에서 샘플링(sampling)된 다중 경로 추적(trace)에 대한 지도 미세 조정(supervised fine-tuning)을 수행하며, 훈련에서 본 것보다 더 많은 경로로 학생 모델이 일반화(generalize)할 수 있도록 <think i>의 무작위 할당을 사용합니다. 세부 사항 및 데이터(data) 출처는 섹션 4 및 부록의 SFT 테이블(table)에 설명되어 있습니다. 이러한 다각적인 사고 방식은 인공지능 모델에도 적용될 수 있으며, 특히 초기 결정이 전체 추론 과정에 큰 영향을 미치는 복잡한 문제에서 그 중요성이 부각됩니다. 결과적으로 AIME 2024/2025, AMC 2023, MATH-500에서 ParaThinker는 고정된 경로당 예산으로 8개 경로를 사용할 때 1.5B 모델의 경우 약 12.3%, 7B 모델의 경우 7.5%만큼 순차적 기준선(baseline) 대비 pass@1을 개선합니다. 평균적으로 다수결 투표를 1.5B 모델의 경우 4.3%, 7B 모델의 경우 2.0% 능가하며, ParaThinker와 다수결 투표를 결합하면 추가적인 성능 향상을 얻을 수 있습니다. 효율성(efficiency) 및 설계 통찰력(design insight) 측면에서, 디코딩(decoding)이 메모리(memory) 대역폭(bandwidth)에 의해 제한되므로 경로가 많아질수록 지연 시간(latency)이 약간 증가합니다. 단일 A800에서 16개 경로는 동일한 길이에 대해 하나의 경로 시간의 2배 미만 소요됩니다. 최적의 종료 정책(policy)은 경로 길이를 동일하게 하고 정확도(accuracy)와 속도(speed)를 모두 향상시키는 "first-finish"입니다. 사고 임베딩(embedding)은 중요하며, 단순하게 평탄화된 위치는 성능을 저해합니다. 하지만 이러한 병렬 처리는 계산 비용 증가라는 도전 과제를 수반하므로, 효율적인 구현 방안이 필수적입니다. 병렬 추론은 "터널 비전(Tunnel Vision)"과 같은 문제점을 완화하고, 더 견고하고 정확한 결론에 도달할 가능성을 높입니다.

**6. AgentGym-RL**
현실적인 환경 전반에 걸쳐 강화 학습(reinforcement learning)을 통해 LLM 에이전트(agent)를 직접 훈련하기 위한 모듈형 프레임워크(framework)인 AgentGym-RL과, 안정성 및 성능 향상을 위해 훈련 전반에 걸쳐 상호 작용 범위(interaction horizon)를 늘리는 간단한 스케줄(schedule)인 ScalingInter-RL을 소개합니다. LLM 기반 에이전트(agent)의 발전은 인공지능이 단순한 텍스트 생성을 넘어, 실제 환경에서 복잡한 작업을 수행하는 단계로 진입하고 있음을 보여주며, 이러한 에이전트(agent)는 웹 탐색, 게임 플레이, 과학적 발견 등 다양한 영역에서 인간과 유사한 능력을 발휘하도록 설계됩니다. 그러나 현실적인 환경 전반에 걸쳐 강화 학습(reinforcement learning)을 통해 LLM 에이전트(agent)를 훈련하는 것은 여전히 많은 도전 과제를 안고 있으며, 실제 환경은 예측 불가능하고 동적(dynamic)이므로, 에이전트(agent)의 안정적인 학습과 일반화 능력을 확보하는 것이 중요합니다. AgentGym-RL은 세 가지 플러그형 모듈(환경, 에이전트(agent), 훈련)을 갖춘 통합되고 분리된 RL 스택(stack)으로 PPO, GRPO, REINFORCE++를 지원하며 WebArena, Deep Search, TextCraft, BabyAI, SciWorld 전반에서 실행됩니다. ScalingInter-RL의 핵심 아이디어(idea)는 활용(exploitation) 및 안정적인 학습을 강조하기 위해 짧은 범위(horizon)로 시작한 다음, 탐색(exploration) 및 계획(planning)과 반성(reflection)과 같은 더 풍부한 행동을 장려하기 위해 허용된 턴(turn)을 점진적으로 증가시키는 것입니다. 에이전트(agent) 작업의 경우 모델 크기만으로 확장하는 것보다 후처리 훈련 및 테스트(test) 시간 컴퓨팅(compute)이 더 잘 확장됩니다. 이 프레임워크(framework)로 훈련된 7B 모델은 약 58.6%의 평균 성공률을 달성하고 훨씬 더 큰 기준선(baseline)을 능가합니다. 결과는 7B 오픈 모델이 웹 탐색, 심층 검색, 게임, 체화(embodied) 및 과학 작업에서 더 큰 독점 시스템과 경쟁하거나 능가할 수 있음을 보여주며, 특정 벤치마크(benchmark)에서 기대 이상의 성능을 발휘할 수 있음을 시사합니다. 결과 스냅샷(snapshot)은 다음과 같습니다: 웹 탐색에서 ScalingInter-7B는 WebArena에서 전체 26.00%를 기록하여 GPT-4o의 16.00%를 능가합니다. 심층 검색에서는 전체 38.25로 GPT-4o의 26.75를 능가하며 강력한 오픈 기준선(baseline)에 근접하고, NQ에서 52.00으로 최고, TriviaQA에서 70.00으로 동점입니다. 게임에서 TextCraft에서 전체 91.00을 기록했으며, Depth 4(33.33)에서 0이 아닌 값을 가진 몇 안 되는 모델 중 하나입니다. 체화(Embodied)에서 BabyAI에서 96.67을 기록하여 전체 정확도(accuracy)에서 o3 및 GPT-4o를 능가합니다. 과학에서 SciWorld에서 57.00으로 SOTA(State-of-the-Art)를 달성했으며, 7B RL 모델도 50.50으로 강력한 성능을 보입니다. 훈련 동역학(dynamic) 분석 결과, 너무 이른 긴 범위(horizon)는 학습을 붕괴시킬 수 있으며, 짧은 범위(horizon)는 성능을 제한합니다. ScalingInter-RL은 둘 다 방지합니다. 엔지니어링(engineering) 참고 사항으로, 병렬화된 브라우저(browser), 리셋 훅(reset hook) 및 메모리 누수(memory leak) 수정은 안정적인 장기 롤아웃(rollout)을 가능하게 하며, 시각적 UI(User Interface)는 궤적(trajectory) 및 실패 모드(failure mode)를 검사하는 데 도움을 줍니다. 실무자를 위한 조언은 희소 보상(sparse-reward), 장기 궤적(long-trajectory) 에이전트(agent) 작업에는 REINFORCE++보다 GRPO를 선호하고, 상호 작용 길이 커리큘럼(curriculum)은 간단하고 견고한 이점을 제공하며, 파라미터(parameter) 스케일링(scaling) 전에 후처리 훈련 및 추론 샘플링(inference sampling)을 위한 컴퓨팅(compute) 예산을 책정하라는 것입니다. 이러한 연구는 범용 인공지능(AGI) 개발의 초석이 될 수 있습니다.

**7. 대화가 항상 저렴한 것은 아니다**
다중 에이전트(multi-agent) 토론이 항상 최적의 결과를 보장하지는 않으며, 인간 사회에서 집단 지성이 항상 올바른 결론으로 이어지지 않듯이, 인공지능 에이전트(agent) 간의 토론 역시 예상치 못한 부작용을 낳을 수 있습니다. 저자들은 현재의 정렬(alignment)이 에이전트(agent)를 너무 순응적으로 만들어 비판적 사고를 저해하고, 설득력 있지만 잘못된 추론을 받아들이고 도전하지 않는다고 주장합니다. 특히, 강력한 모델이 약한 모델의 잘못된 주장에 동조하여 오류가 확산되는 "그룹싱크(groupthink)" 현상은 AI 시스템의 신뢰성을 떨어뜨릴 수 있습니다. 설정은 GPT-4o-mini, Llama-3.1-8B-Instruct, Mistral-7B-Instruct를 사용하여 CommonSenseQA, MMLU, GSM8K에서 토론을 평가하며, 에이전트(agent)는 한 번 답변한 후 두 라운드(round) 동안 토론하고 최종 출력은 토론 전후의 다수결 투표입니다. 프롬프트(prompt)는 짧은 추론과 작업별 형식을 요구합니다. 주요 결과로, 복잡한 문제 해결 과정에서 토론은 특히 그룹 다이내믹스에 따라 정확도(accuracy)를 자주 저해하며, 특히 CommonSenseQA 및 MMLU에서 토론 후 일관된 하락을 보여줍니다. 예를 들어, CSQA는 1×GPT + 2×Llama 조합에서 6.6점, 2×Llama + 1×Mistral 조합에서 8.0점 하락하고, MMLU는 1×GPT + 2×Llama 조합에서 12.0점 하락합니다. GSM8K는 더 혼합된 결과를 보이며 일부 설정에서는 작은 이득이 있습니다. 이 연구는 라운드(round)별 정확도(accuracy)를 추적하며, 더 강력한 모델이 다수일 때도 토론이 진행됨에 따라 성능이 종종 저하됨을 보여줍니다. 그 이유는 에이전트(agent)가 비판보다 합의를 선호하는 경향이 있기 때문입니다. 라운드(round) 전반에 걸쳐 잘못된 답변으로의 전환(correct→incorrect)이 올바른 답변으로의 전환(incorrect→correct)보다 더 많으며, 이는 토론이 더 강력한 모델을 적극적으로 오도할 수 있음을 나타냅니다. 부록 예시는 동료의 답변을 읽은 후 올바른 답변에서 잘못된 답변으로의 아첨적인 역전 현상을 기록합니다. 시사점은 순진한 토론 프로토콜(protocol)은 오류를 증폭시킬 위험이 있다는 것입니다. 저자들은 독립적인 검증에 보상하고, 에이전트(agent)의 신뢰도 또는 자신감에 따라 주장에 가중치를 부여하며, 정당하지 않은 합의에 페널티(penalty)를 부과하여 토론의 이점을 보존하는 설계를 권장합니다. 이는 모델의 정렬(alignment) 방식과 토론 프로토콜(protocol) 설계의 중요성을 강조하며, 단순히 합의를 도출하는 것을 넘어, 비판적 사고를 장려하고 각 에이전트(agent)의 전문성을 고려한 가중치를 부여하는 등, 보다 정교한 토론 메커니즘(mechanism)이 필요함을 시사합니다. 이러한 연구는 다중 에이전트(multi-agent) 시스템의 의사결정 과정을 개선하고, 궁극적으로 더 현명하고 책임감 있는 AI를 만드는 데 기여할 것입니다.

**8. AggLM**
AggLM은 여러 후보 솔루션(solution)을 집계하는 LLM 훈련에 강화 학습(reinforcement learning)을 도입하여 새로운 가능성을 열었습니다. 대규모 언어 모델(LLM)이 생성하는 답변의 다양성과 불확실성은 여전히 해결해야 할 과제이며, 단일 모델의 단일 출력에 의존하기보다는 여러 번의 생성 또는 여러 모델의 출력을 종합하여 최종 답변을 도출하는 집계(aggregation) 방식이 주목받고 있습니다. 전통적인 다수결 투표나 단순한 순위 지정 방식은 한계가 명확하며, 특히 소수의 정확한 정보가 다수의 잘못된 정보에 묻힐 위험이 있습니다. AggLM은 다수결 투표 및 보상(reward) 모델(model) 순위 지정을 넘어, 강화 학습(RL)을 활용한 집계로 이러한 문제를 극복하고, 각 후보 솔루션(solution)의 신뢰도와 관련성을 평가하여 최적의 조합을 찾아내는 데 기여합니다. 이는 더 높은 정확도(accuracy)를 달성하고, 소수 정답을 복구하며, 모델(model) 전반에 걸쳐 일반화(generalize)하고, 전통적인 집계 방법보다 적은 토큰(token)을 사용합니다. 이는 LLM의 정확도(accuracy)와 견고성을 높이는 효과적인 방법론으로, 복잡한 질의응답 및 추론 작업에서 중요한 역할을 할 것입니다.

**9. 대규모 추론 모델(Large Reasoning Models)을 위한 RL(강화 학습) 설문 조사**
이 설문 조사는 강화 학습(reinforcement learning)이 대규모 추론 모델(LRM)의 발전을 어떻게 이끌고 있는지 검토하며, 미래 연구의 방향을 제시합니다. 대규모 추론 모델(LRM)은 단순히 언어를 이해하고 생성하는 것을 넘어, 수학 및 코딩(coding)과 같은 복잡한 작업에서 더 강력한 성능을 가능하게 하는 복잡한 문제 해결과 논리적 추론 능력을 갖춘 인공지능 시스템으로 진화하고 있습니다. 이러한 발전의 중심에는 강화 학습(RL)이 있으며, RL은 모델이 시행착오를 통해 스스로 학습하고 최적의 추론 전략을 개발하도록 돕습니다. 이는 계산, 알고리즘(algorithm), 데이터(data) 및 인프라(infrastructure)의 스케일링(scaling) 과제를 강조합니다. 그러나 LRM의 스케일링(scaling)은 엄청난 계산 자원, 정교한 알고리즘(algorithm) 설계, 그리고 방대한 데이터(data)를 필요로 하며, 이러한 기술적 도전과제를 해결하는 것은 인공지능 연구의 최전선에 놓여 있습니다. 궁극적으로 이는 인공 초지능(Artificial Superintelligence, ASI)을 향한 미래 방향을 제시합니다. 이 분야의 연구는 단순히 성능 향상을 넘어, 인공지능이 인류에게 어떤 영향을 미칠지에 대한 심도 깊은 윤리적, 사회적 논의를 촉발합니다.

**10. LiveMCP-101**
LiveMCP-101은 검색, 파일 작업, 수학 및 데이터(data) 분석 전반에 걸쳐 도구 사용을 요구하는 다단계 작업에서 MCP 지원 에이전트(agent)를 테스트(test)하도록 설계된 101개의 실제 쿼리(query)로 구성된 새로운 벤치마크(benchmark)입니다. 인공지능 에이전트(agent)가 실제 세계에서 유용하게 기능하기 위해서는 다양한 도구를 효율적으로 활용하는 능력이 필수적이며, 이는 단순한 질의응답을 넘어 복잡한 다단계 작업을 수행해야 하는 환경에서 더욱 중요해집니다. 이 새로운 벤치마크(benchmark)는 이러한 도구 활용 능력을 객관적으로 평가하기 위해 고안되며, 실제 문제 상황을 반영하는 것이 특징입니다. 결과는 선도적인 LLM이 60% 미만으로 성공함을 보여주며, 현재까지의 연구 결과는 대규모 언어 모델(LLM) 기반 에이전트(agent)가 아직 도구 선택 및 활용, 그리고 여러 도구 간의 조율(orchestration)에서 상당한 한계를 가지고 있음을 보여줍니다. 이러한 결과는 현재 모델들이 도구 오케스트레이션(orchestration)의 주요 약점을 드러내고 자율 AI 시스템(AI system) 발전을 위한 통찰력(insight)을 제공합니다. 이러한 약점들을 명확히 이해하고 개선하는 것은 진정으로 자율적인 인공지능 시스템(AI system)을 구축하기 위한 핵심 단계입니다.