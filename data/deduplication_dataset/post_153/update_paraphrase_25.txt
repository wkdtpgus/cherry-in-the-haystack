OpenAI의 최신 모델 o3의 출현과 함께, 인공 일반 지능(Artificial General Intelligence, AGI)이 과연 현실화되었는지에 대한 뜨거운 논쟁이 다시 불붙고 있습니다. 이러한 논의에 대해 흔히 제기되는 회의적인 시각은 AGI에 대한 보편적인 정의가 없다는 점입니다. 이는 부분적으로 사실이나, 문제의 본질을 놓치고 있습니다. 만약 AGI가 인류에게 그토록 중대한 이정표라면, 그것이 실현되었을 때 누구나 명확하게 인지할 수 있어야 하지 않을까요? 본고에서는 AGI가 단순한 기술적 이정표가 아니라고 주장합니다. AGI는 AI 시스템의 본질적 특성이나 그 영향력 측면에서 급진적인 단절을 초래하지 않습니다. 어떤 정의를 따르든, 특정 기업이 AGI 개발을 선언하더라도 이는 실제적인 의미를 지니지 않습니다. 이러한 선언은 기업의 전략, 개발자들의 방향성, 정책 입안자들의 결정, 또는 시스템 안전에 어떠한 즉각적인 변화도 가져오지 않을 것입니다. 더 구체적으로 말하자면, 범용 AI 시스템이 아무리 높은 능력 임계점(capability threshold)에 도달하더라도, AI가 산업 전반에 걸쳐 확산되어 실질적인 생산적 효과를 발휘하기 위해서는 수많은 보완적인 혁신이 필수적입니다. 기술의 확산은 기술 개발의 속도보다는 인간 사회의 고유한 시간 척도(timescales)에 맞춰 진행됩니다. AGI와 관련된 치명적인 위험에 대한 우려는 종종 시스템의 역량(capabilities)과 실제적 영향력(power)을 혼동하는 데서 비롯됩니다. 이 두 개념을 명확히 구분한다면, 인공지능 개발 과정에서 인류가 통제력을 상실하는 결정적 순간이 도래할 것이라는 관념을 배척할 수 있습니다. AGI 정의에 대한 다양한 해석은 본질적인 문제가 아니라, 현재 상황의 한 단면을 보여주는 지표일 뿐입니다. AGI는 그 잠재적 영향력 때문에 중요하게 다루어지지만, 실제로는 인공지능 시스템 자체의 내재적 특성을 기반으로 정의되어야 합니다. 하지만 시스템의 특성과 실제 영향 사이의 연관성은 매우 희박하며, AI 시스템이 구동될 환경을 어떻게 구축하느냐에 따라 크게 달라집니다. 그러므로 특정 인공지능 시스템이 혁명적인 변화를 가져올지 여부는 해당 시스템이 처음 선보이는 시점에는 명확히 알 수 없습니다. 결론적으로, 어떤 인공지능 시스템이 AGI에 해당한다고 판단하는 것은 오직 사후적(retrospectively) 분석을 통해서만 유의미하게 이루어질 수 있습니다.

**AGI에 대한 반대 비유로서의 핵무기: 왜 AGI는 다른가?**
AGI 달성은 OpenAI를 비롯한 여러 선도 기업들과 AI 연구 공동체의 명시적인 목표입니다. 이는 마치 맨해튼 프로젝트(Manhattan Project)가 핵무기 개발 및 실전 배치를 핵심 목표로 삼았던 것처럼 하나의 중대한 이정표로 간주됩니다. 맨해튼 프로젝트의 목표는 두 가지 이유로 명확한 이정표로서의 의미를 가졌습니다. 첫째는 관측 가능성(observability)입니다. 핵무기 개발의 경우, 목표 달성 여부에 대해 의심의 여지가 없었습니다. 핵폭발은 그 존재를 명백하게 증명하는 현상이었습니다. 둘째는 즉각적인 영향(immediate impact)입니다. 핵무기의 사용은 제2차 세계 대전을 조기에 종식시키는 데 기여했으며, 동시에 새로운 세계 질서와 지정학적 지형의 장기적인 변화를 가져왔습니다.

많은 사람들은 AGI 또한 이러한 특성을 가질 것이라고 직관적으로 생각합니다. 즉, AGI는 너무나 강력하고 인간과 유사하여 우리가 그것을 만들었을 때 분명하게 드러날 것이며, 즉시 막대한 이점과 위험을 동시에 가져올 것이라는 생각입니다. 경제의 상당 부분을 자동화하고, AI 연구 자체를 포함한 혁신을 크게 가속화하며, 통제 불가능한 초지능(superintelligence)으로 인해 인류에게 잠재적으로 치명적인 결과를 초래할 수 있다는 것이죠. 그러나 본고에서는 AGI가 이와는 정반대의 특성을 가질 것이라고 주장합니다. 즉, 특별한 의미를 지닌 명확한 능력 임계값이 존재하지 않으므로 관측이 불가능하며, 세계에 즉각적인 파급 효과를 미치지 않을 것이고, 심지어 경제의 장기적인 변혁조차 불확실하다는 것입니다.

이전 에세이에서 우리는 AGI를 핵무기에 비유하여 일부 사람들이 권장하는 파괴적인 정책 개입에 반대하는 주장을 펼쳤습니다. 이 비유가 우리가 잘못된 예측과 역효과를 낳는다고 생각하는 권고로 이어진다는 점은 놀랍습니다. 핵무기는 단일 국가가 독점적으로 개발하고 통제할 수 있었던 기술이었으며, 그 위력은 즉각적이고 파괴적이었습니다. 반면, AI 기술은 본질적으로 분산적이고 협력적이며, 그 영향은 점진적으로 사회 전반에 스며드는 형태를 띨 가능성이 높습니다. 이러한 근본적인 차이를 간과하고 핵무기의 비유를 무분별하게 적용하는 것은 AGI에 대한 이해를 왜곡하고 비합리적인 공포를 조장할 수 있습니다. 예를 들어, 인터넷이나 전기와 같은 범용 기술(general-purpose technologies) 역시 인류 사회에 혁명적인 변화를 가져왔지만, 이는 특정 시점에 "완성"되었다고 선언할 수 있는 단일한 사건이 아니었습니다. 대신, 수십 년에 걸쳐 점진적으로 확산되고 다양한 보완 기술 및 제도적 혁신과 결합되면서 비로소 그 잠재력을 발휘했습니다. AGI 또한 이와 유사한 경로를 따를 가능성이 높습니다.

**o3가 AGI라고 생각하는 것이 터무니없는 것은 아니지만, 이는 o3보다 AGI에 대해 더 많은 것을 말해줍니다.**
최근 OpenAI의 o3 모델 등장으로 인공 일반 지능(AGI)의 실현 가능성에 대한 논의가 뜨겁게 재점화되었습니다. 타일러 코웬(Tyler Cowen)과 같은 저명한 AI 비평가들은 o3를 일종의 AGI로 지칭하기도 했습니다. 이든 몰릭(Ethan Mollick)은 o3를 "들쭉날쭉한 AGI(jagged AGI)"라고 묘사했죠. 그렇다면 o3의 어떤 점이 이토록 큰 반향을 불러일으켰을까요? o3의 핵심적인 혁신은 강화 학습(reinforcement learning)을 활용하여 웹을 탐색하고 추론 사슬(reasoning chain)의 일부로 도구들을 사용하는 방법을 학습한다는 것입니다. <sup>1</sup> 이러한 접근 방식을 통해 o3는 대규모 언어 모델(LLM)이 단독으로 수행할 수 있는 것보다 훨씬 더 복잡한 인지 작업(cognitive tasks)을 수행하며, 그 과정이 인간의 사고 방식과 유사하게 보인다는 특징이 있습니다.

가령, 여러 제품을 비교 구매하는 사람을 떠올려 봅시다. 그들은 몇 가지 제품을 검토하고, 리뷰를 참고하여 중요한 기능을 파악한 다음, 이 정보를 바탕으로 고려 중인 제품 목록을 반복적으로 확장하거나 축소할 수 있습니다. o3는 이러한 유형의 작업을 상당히 효과적으로 수행하는 범용 에이전트(generalist agent)입니다. 이제 이것이 AGI에 어떤 의미를 가지는지 생각해 봅시다. o3의 구체적인 구현 방식에 얽매이지 않고, o3와 동일한 아키텍처(architecture)를 가지지만 훨씬 더 유능한 미래 시스템을 상상해 볼 수 있습니다. 예를 들어, 이 시스템은 온라인 상태인 한, 아무리 찾기 어려운 정보라도 항상 작업에 적합한 웹페이지와 지식을 찾아낼 수 있습니다. 필요하다면 인터넷에서 코드를 다운로드하여 실행함으로써 문제를 해결할 수도 있습니다. 이러한 능력들은 현재의 과학적 돌파구 없이도 공학적 개선과 추가 훈련만으로 충분히 달성 가능합니다.

동시에, 현재 아키텍처는 과학적 개선 없이는 심각한 한계를 가집니다. 예를 들어, 이 가상의 미래 시스템은 훈련에 대한 명시적인 업데이트를 통해서만 새로운 기술을 경험으로부터 습득할 수 있습니다. 즉석에서 학습할 수 있는 AI 시스템을 구축하는 것은 여전히 미해결 연구 문제(open research problem)입니다. <sup>2</sup> 그렇다면 우리의 가상 시스템은 AGI일까요? 논쟁의 여지는 있지만, 그렇다고 볼 수도 있습니다. 많은 AGI 정의들이 공통적으로 포함하는 것은 다양한 작업에서 인간을 능가하는 능력입니다. 작업 세트가 얼마나 좁게 정의되고 각 작업에 대한 관련 인간 세트가 얼마나 넓게 정의되는지에 따라, 이 미래의 o3와 유사한 모델/에이전트가 이러한 AGI 정의 중 일부를 충족할 가능성은 상당히 높습니다. 예를 들어, 대규모 언어 모델(large language models) 자체는 체스에 기껏해야 평범한 수준임에도 불구하고, 이 시스템은 체스에서 초인적인 능력을 발휘할 것입니다. 모델이 도구를 사용하고, 인터넷을 검색하며, 코드를 다운로드하고 실행할 수 있다는 점을 기억하십시오. 만약 작업이 체스를 두는 것이라면, 체스 엔진을 다운로드하여 실행할 것입니다.

하지만 이러한 시스템은 많은 작업에서 인간 수준 또는 초인적인 성능을 보이고, 일부 AGI 정의를 그럴듯하게 충족함에도 불구하고, 여전히 많은 실제 작업에서는 아마도 심각하게 실패할 것입니다. 그 이유는 시스템이 특정 도메인에 대한 심층적인 이해나 물리적 세계와의 상호작용 능력이 부족하기 때문입니다. 예를 들어, o3와 같은 모델은 뛰어난 글쓰기 능력을 보여주지만, 복잡한 물리적 환경에서 로봇을 조작하거나, 예측 불가능한 사회적 상황에서 미묘한 인간 감정을 이해하여 적절히 반응하는 데는 여전히 한계가 명확합니다. 인간에게는 상식적인 판단이나 맥락 이해가 필요한 작업에서 AI는 쉽게 오류를 범할 수 있습니다.

이 모든 것이 중요할까요? 물론 중요합니다. AI 기업의 리더들은 몇 년 안에 AGI를 제공하겠다는 매우 큰 예측과 약속을 했습니다. 그들이 가까운 미래의 어떤 시스템을 AGI라고 선언할 엄청난 유인(incentives)이 있으며, 그렇게 하지 않을 경우 잠재적으로 막대한 비용이 발생할 수 있습니다. 아마도 AI 기업의 가치 평가 중 일부는 이러한 약속에 기반하고 있을 것이므로, AGI가 없다면 거품이 터질 수도 있습니다. AI 개발의 리더로 인식되는 것은 시장 점유율과 수익을 개선하고, 최고 인재에 대한 접근성을 높이는 데 도움이 될 수 있습니다. 이는 기업의 생존과 성장에 직결되는 문제이므로, AGI 선언에 대한 압박은 더욱 커질 수밖에 없습니다. 그렇다면, 기업들이 AGI를 구축했다고 주장할 경우 어떤 결과가 따를까요? 이 에세이의 나머지 부분에서 이를 분석할 것입니다. AI Snake Oil은 AI 과대광고를 폭로하고 새로운 개발에 대한 증거 기반 분석을 게시합니다. 구독하기

**AGI는 확산에 수십 년이 걸리므로 경제에 급격한 변화를 주지 않을 것입니다.**
AGI를 하나의 중대한 이정표로 취급하고 AGI 선언을 진지하게 받아들이는 주장의 핵심에는 AGI가 희소성 없는 세상, 돈이라는 개념의 종말, 또는 갑작스러운 대량 실업과 같은 긍정적 및 부정적 측면 모두에서 급격한 경제적 영향을 초래할 수 있다는 믿음이 깔려 있습니다. 그러나 AI의 경제적 영향은 기술이 경제 전반에 걸쳐 광범위하게 채택될 때 비로소 실현됩니다. 기술 발전은 이러한 영향을 실현하는 데 필요하지만 충분하지는 않습니다. 전기, 컴퓨팅, 인터넷과 같은 과거의 범용 기술(general-purpose technologies)의 경우, 근본적인 기술 발전이 사회 전반에 확산되는 데 수십 년이 걸렸습니다. 산업 혁명(Industrial Revolution)의 기적은 높은 성장률(연평균 성장률이 3% 미만)이 아니라 수십 년간 지속된 성장 기간이었습니다.

AI 확산에는 많은 병목 현상(bottlenecks)이 존재합니다. 유용한 제품 및 애플리케이션 개발, 이러한 제품을 효과적으로 활용할 인력 훈련, AI 사용을 가능하게 하는 조직 변화 구현, 기업의 AI 채택을 촉진하는 법률 및 규범 확립 등이 그것입니다. 과거의 범용 기술과 마찬가지로, 우리는 AI의 경제적 영향이 이러한 확산 과정이 전개됨에 따라 수십 년에 걸쳐 점진적으로 실현될 것으로 예상합니다. "AI as Normal Technology"라는 논문에서 우리는 왜 이러한 상황이 될 것이라고 생각하는지에 대한 자세한 주장을 제시합니다. 능력의 급격한 증가가 급격한 경제적 영향으로 이어진다는 생각은 AI의 과거 및 현재와 완전히 일치하지 않으며, 미래에 이것이 바뀔 것이라고 예상할 이유도 없습니다.

최근 연구에 따르면, 기업의 AI 도입률은 여전히 초기 단계에 머물러 있으며, 특히 중소기업에서는 더욱 그렇습니다. AI 기술의 복잡성, 높은 초기 투자 비용, 기존 시스템과의 통합 문제, 그리고 숙련된 인력 부족 등이 주요 장애물로 작용하고 있습니다. 또한, AI 기술이 특정 산업이나 직무에 미치는 영향은 매우 불균등하게 나타납니다. 예를 들어, 일부 단순 반복 업무는 빠르게 자동화될 수 있지만, 창의적 사고, 복잡한 문제 해결, 또는 인간적 상호작용이 필수적인 직무는 AI의 영향을 덜 받거나 오히려 AI와 협력하여 생산성을 높이는 방향으로 진화하고 있습니다. AGI의 한 가지 정의는 대부분의 경제적으로 가치 있는 작업에서 인간을 능가하는 AI 시스템입니다. 만약 AGI가 이러한 의미에서 실현된다면, 대규모의 갑작스러운 일자리 대체로 이어질 수 있다고 우려할 수 있습니다. 그러나 인간은 움직이는 목표물(moving target)입니다. 확산 과정이 전개되고 자동화된 작업의 생산 비용(따라서 가치)이 감소함에 따라, 인간은 적응하여 아직 자동화되지 않은 작업으로 이동할 것입니다. 기술 발전, 제품 개발 및 확산 과정은 계속될 것입니다. 중요한 것은 기술 발전이 아니라 기술이 사회와 경제 시스템에 어떻게 통합되고 활용되는가입니다.

**AGI는 세계 질서의 급격한 변화로 이어지지 않을 것입니다.**
미국과 중국은 종종 AI 군비 경쟁(AI arms race)을 벌이고 있으며, 각국이 AGI를 구축하기 위해 경쟁하고 있다고 묘사됩니다. AGI를 먼저 구축하는 국가가 결정적인 전략적 우위(strategic advantage)를 가지게 되어, 예측 가능한 미래에 세계 질서에서 지배력을 확보할 것이라는 가설이 있습니다. <sup>3</sup> 이러한 서사는 현실과 동떨어져 있습니다. AI 모델을 만드는 데 필요한 지식과 모델 능력 자체는 국가 간에 빠르게 확산되는 경향이 있기 때문입니다. 수십만 명의 AI 기술자(technologists)가 있으며, 이들은 정부 연구소보다는 민간 부문에서 주로 활동하므로, 그러한 규모에서 기술 비밀을 유지하는 것은 사실상 불가능합니다. 발명, 즉 이 경우 AI 모델 개발은 경쟁 우위(competitive advantage)의 원천으로서 과대평가되어 있습니다.

우리는 기술 발전이 국가 간에 대략적으로 보조를 맞출 것으로 예상해야 합니다. 비록 미국 기업들이 현재 AI 분야에서 선두를 달리고 있지만, 장기적인 우위를 지속할 것이라고 기대해서는 안 됩니다. <sup>4</sup> 많은 사람들은 기술 능력의 확산 용이성을 제대로 인식하지 못했으며(아마도 핵무기 정신 모델(mental model) 때문일 것입니다), 이에 놀라움을 금치 못했습니다. 이것이 올해 초 "딥시크 모멘트(DeepSeek moment)"로 이어진 이유입니다. 분석가들은 AI 능력이 얼마나 빨리 확산될 수 있는지 깨닫지 못했고, 그 결과 신생 기업들(특히 중국 기업들)이 그렇게 빨리 선두 그룹을 따라잡을 것이라고 예상하지 못했습니다. 실제로, 오픈소스 AI 모델의 급속한 발전은 특정 국가나 기업의 기술 독점을 더욱 어렵게 만들고 있습니다. 최첨단 모델의 가중치와 아키텍처가 공개되면서, 전 세계의 연구자와 개발자들이 이를 활용하여 새로운 애플리케이션을 만들고 개선하는 속도가 가속화되고 있습니다.

일부 사람들은 몇 달의 기술적 우위조차 중요할 것이라고 주장합니다. 그러나 우리는 이에 동의하지 않습니다. 강대국 경쟁의 맥락에서 중요한 질문은 어느 국가가 AGI를 먼저 구축하느냐가 아니라, 어느 국가가 기술의 확산을 더 효과적으로 가능하게 하느냐입니다. 제프리 딩(Jeffrey Ding)이 지적했듯이, 국내외 AI 발명과 혁신을 실제로 활용하여 생산성을 향상시키는 기업과 정부의 효율성이 범용 기술의 경제적 영향을 결정하는 데 훨씬 더 중요합니다. 중국 AI 기업들은 AI 모델과 능력 면에서 선도적인 미국 기업들보다 기껏해야 6-12개월 뒤처져 있지만, 중국은 확산을 가능하게 할 수 있는 몇 가지 핵심 지표, 즉 디지털화(Digitization) 수준, 클라우드 컴퓨팅(cloud computing) 채택률, 인력 훈련 면에서 미국에 크게 뒤처져 있습니다. 이 모든 것은 산업 전반에 걸쳐 AI 발전의 생산적인 확산을 가능하게 하는 데 필수적인 요소들입니다. 이것이 미국의 실제 경쟁 우위의 원천입니다.

물론, 이러한 격차는 향후 몇 년 안에 바뀔 수 있습니다. 그러나 그렇게 된다면, 그것은 AGI 개발보다는 확산을 촉진하기 위한 정책 변화의 결과일 것입니다. 그리고 정책을 얼마나 빨리 바꾸든, 국가들이 하룻밤 사이에 이룰 수 있는 일이 아닙니다. 확산은 일반적으로 수십 년에 걸쳐 전개됩니다. 이 모든 것이 정책 입안자들이 안주해야 한다는 의미는 아닙니다. 오히려, 이는 AGI라는 추상적인 개념에 집착하기보다는 기존 AI 기술을 포함하여 생산적이고 안전한 확산을 가능하게 하는 구체적인 정책과 인프라 구축에 집중해야 한다는 것을 의미합니다. 예를 들어, AI 인프라 투자, AI 인재 양성을 위한 교육 프로그램 강화, 데이터 거버넌스 프레임워크 구축, 그리고 AI 기술의 윤리적 사용을 위한 국제적 협력 등이 실질적인 국가 경쟁력 강화에 기여할 것입니다.

**AGI의 장기적인 경제적 영향은 불확실합니다.**
즉각적인 경제적 영향이 없다고 하더라도, AGI가 예를 들어 수십 년에 걸쳐 연간 10%의 GDP 성장률을 가능하게 하여 큰 성과를 낼 수 있을까요? 이론적으로는 가능할 수도 있습니다. 그러나 이것이 왜, 어떻게 일어날지는 전혀 명확하지 않습니다. 역사적으로 이러한 종류의 급격한 성장 가속화는 매우 드물게 발생했습니다. 산업 혁명(industrial revolution)은 이러한 효과를 가져왔지만, GDP에 거의 영향을 미치지 않은 인터넷은 그렇지 않았습니다. GDP가 측정하기에 적절한 것이 아니라고 생각하더라도, GDP 성장률의 질적 변화는 여러분이 중요하게 생각하는 경제의 근본적인 변화에 대한 좋은 대리 지표(proxy)입니다.

문제는 성장을 가속화하려면 진보의 병목 현상(bottlenecks)을 제거해야 한다는 것입니다. 이는 대부분의 AI 지지자들이 가정하는 것보다 훨씬 더 어렵습니다. AI는 부문별로 불균등한 영향을 미칠 가능성이 높으며, 장기적인 성장은 가장 약한 부문에 의해 병목 현상이 발생할 것입니다. 극적인 효과를 주장하는 사람들은 종종 병목 현상이 실제로 무엇인지에 대한 잘못된 정신 모델(mental model)을 가지고 있습니다. 예를 들어, 저렴한 과학 혁신이 진보를 가능하게 할 것이라고 믿고 싶겠지만, 새로운 발견의 생산은 실제로는 과학 발전의 유일한 병목 현상이 아닙니다. 과학적 발견이 실제 경제적 가치로 전환되기까지는 수많은 공학적, 사회적, 제도적 장벽을 넘어서야 합니다.

더 넓게 보면, 진보는 기술 자체뿐만 아니라 올바른 전제 조건, 즉 보완적인 혁신(complementary innovations)과 문화적, 경제적, 정치적 요인에 달려 있습니다. 산업 혁명을 일으키는 데 필요한 것이 단순히 증기 기관(steam power)의 발명뿐이었다면, 로마 제국(Roman Empire)과 같은 고대 문명도 충분히 그것을 해냈을 것입니다. 그러나 그들은 증기 기관을 활용할 수 있는 사회적, 경제적, 제도적 환경을 갖추지 못했습니다. 우리의 현재 법률, 규범, 제도 및 정치는 기술적 잠재력이 훨씬 적었던 시기에 발전했습니다. 그것들은 이미 더 많은 공공 인프라(public infrastructure)를 구축하는 것과 같은 직접적인 유형의 성장을 위한 기회를 막고 있습니다. 광범위한 인지 자동화(cognitive automation)가 잠재적으로 가져올 수 있는 경제적 이점을 얻으려면, 발생해야 할 구조적 변화의 정도는 헤아릴 수 없을 정도로 더 큽니다. 예를 들어, AI가 보건 분야에서 혁신적인 진전을 이루려면, 의료 규제, 데이터 공유 정책, 의사-환자 관계의 재정립, 그리고 의료 인력의 재교육 등 복잡한 문제들이 해결되어야 합니다. 결론적으로, AGI로 인한 장기적인 영향의 범위와 성격은 아직 지켜봐야 하며, 우리가 어떤 보완적인 조치(complementary actions)를 취하느냐에 달려 있습니다. 장기적인 영향은 AGI 자체의 속성이 아닙니다.

**AGI의 정렬 불량(misalignment) 위험은 권력과 능력을 혼동합니다.**
반면에, AGI는 AI의 사회적 위험에 대한 전환점이 될 수 있다는 우려도 제기됩니다. AGI가 통제 상실, 막대한 사회적 해악, 심지어 인류 멸종을 초래할 수 있을까요? AGI 위험에 대한 논의는 시스템의 실제적 영향력(power) — 즉, 환경을 수정할 수 있는 능력 — 과 시스템의 역량(capability) — 즉, 지정된 작업을 올바르게 해결할 수 있는 역량 — 을 혼동하는 경향이 있습니다. 역량은 AI 시스템의 내재적 속성인 반면, 영향력은 AI 시스템이 작동하는 환경을 우리가 어떻게 설계하느냐의 문제입니다. 그리고 인간은 이 설계에 대한 주체성(agency)을 가지고 있습니다. 이러한 구별은 종종 간과됩니다.

다리오 아모데이(Dario Amodei)의 "강력한 AI(powerful AI)" 정의를 생각해 봅시다. <sup>5</sup> 그는 "...미해결 수학 정리 증명, 매우 훌륭한 소설 작성, 어려운 코드베이스(codebases)를 처음부터 작성"하는 것과 같은 강력한 AI의 능력에 대한 설명으로 시작합니다. 이 기준은 AI 능력의 한 예이며, 우리는 이를 AI 시스템 수준에서 논의할 수 있습니다. 그러나 그는 이어서 AI 시스템이 작동하도록 허용하는 환경의 속성을 설명하는데, 여기에는 "...인터넷에서 행동하기, 인간에게 지시를 내리거나 받기, 재료 주문하기, 실험 지시하기, 비디오 시청하기, 비디오 만들기 등"이 포함됩니다. 이것은 AI 시스템에 부여된 권력의 한 예입니다. 이는 AI 시스템이 작동하는 환경에 따라 달라지며, AI 능력이 권력으로 어떻게 전환되는지를 결정합니다.

우리는 AI 능력이 계속 증가할 것으로 예상합니다. 그러나 능력 수준과 관계없이, 우리는 AI가 도구로 남아 인간의 감독 없이 작동할 권력과 자율성(autonomy)을 부여받지 않도록 선택할 수 있습니다. 예를 들어, 최첨단 AI 모델이 복잡한 의학적 진단을 내릴 수 있는 능력을 갖더라도, 실제 환자에게 치료법을 처방하거나 수술을 진행할 권한은 여전히 인간 의사에게 있습니다. AI는 의사 결정을 돕는 보조 도구로서 기능하며, 최종적인 책임과 통제는 인간에게 남아있습니다. "AI as Normal Technology" 에세이에서 우리는 기업 간의 군비 경쟁, 권력 추구, 초인적인 설득, 기만적인 정렬(deceptive alignment) 등 이에 대한 모든 일반적인 반론을 다룹니다. 우리는 이 논문에서 적절한 감독 없이 AI를 배포하는 것에 반대하는 강력한 비즈니스 유인(incentives)이 있을 것이며, 이러한 유인들은 필요할 때 규제에 의해 뒷받침될 수 있고 또 뒷받침되어야 한다고 주장합니다. 이는 자율주행차(self-driving cars)부터 AI 비서(AI assistants)에 이르는 분야에서 역사적으로 그래왔습니다. 우리는 AI 능력이 우리가 임의로 AGI로 지정하는 추정된 티핑 포인트(tipping point)에 도달했다고 해서 이 추세가 갑자기 뒤바뀔 것이라고 예상하지 않습니다. AI 안전 연구는 이러한 통제력을 유지하기 위한 구체적인 방법론을 탐구합니다. 예를 들어, 인간이 AI의 행동을 명확하게 이해하고 개입할 수 있도록 하는 설명 가능한 AI(Explainable AI, XAI) 기술 개발, AI 시스템이 예측 불가능한 상황에서 안전하게 실패하도록 설계하는 견고성(robustness) 연구, 그리고 AI가 인간의 가치와 목표에 부합하도록 훈련시키는 가치 정렬(value alignment) 연구 등이 활발히 진행 중입니다. 이러한 노력들은 AI의 능력이 아무리 발전하더라도 인간의 통제하에 안전하게 활용될 수 있도록 하는 데 중점을 둡니다.

**AGI는 임박한 초지능을 의미하지 않습니다.**
AGI를 이정표로 간주하는 또 다른 이유는 AGI를 구축한 직후 AI 시스템이 재귀적으로 자체 개선(recursively self-improve)될 수 있다는 견해 때문입니다. 즉, AGI가 스스로 훨씬 더 유능해지는 미래 버전의 모델을 훈련하여 "지능 폭발(intelligence explosion)"로 이어질 수 있다는 주장입니다. 그 직후, 우리는 초지능 AI(superintelligent AI, 상상할 수 있는 모든 작업에서 인간의 능력을 훨씬 뛰어넘는 AI 시스템)를 얻게 될 것이며, 이는 초지능 AI가 인간의 이익과 얼마나 잘 "정렬(aligned)"되는지에 따라 유토피아(utopia) 또는 디스토피아(dystopia)로 이어질 것이라는 시나리오입니다.

일반적인 기술 관점에서는 이러한 서사를 의심할 만한 두 가지 큰 이유가 있습니다. 첫째는 AI 방법론에서 임의의 속도 향상이 가능하다 하더라도, 우리는 혁신과 확산은 인간의 속도로 일어날 것이라고 생각한다는 점입니다. 다른 범용 기술과 마찬가지로, AI의 영향은 방법론과 능력이 향상될 때가 아니라, 그러한 개선이 실제 애플리케이션으로 전환되고 경제의 생산적인 부문을 통해 광범위하게 확산될 때 비로소 구체화됩니다. [출처] 둘째, AI가 AI 연구 수행을 돕는다는 사실이 이 과정이 임의로 가속화될 수 있다는 것을 의미하지는 않습니다. AI는 오늘날 이미 AI 연구의 상당 부분을 자동화하는 데 사용되고 있습니다. 예를 들어, 모델 아키텍처 탐색, 하이퍼파라미터 최적화, 데이터 증강 등에서 AI의 도움을 받고 있습니다.

그러나 AI 방법론의 발전에 많은 병목 현상(bottlenecks)이 있습니다. 예를 들어, 특정 능력을 달성하는 데 필요할 수 있는 데이터 수집 및 실제 상호작용의 사회적 특성, 계산 및 비용 한계, 또는 진정한 돌파구를 가능하게 하는 아이디어는 무시하고 인기 있거나 직관적인 아이디어에만 몰두하는 경향 등이 있습니다. AI가 스스로 새로운 과학 이론을 창출하거나, 기존의 패러다임을 깨는 혁신적인 아이디어를 제시하는 것은 여전히 요원합니다. 이는 AI가 창의성, 직관, 그리고 인간의 경험적 지식과 같은 요소들을 아직 완전히 대체하지 못하고 있기 때문입니다.

우리는 이러한 예측에 대해 틀릴 수도 있으며, 재귀적 자체 개선(recursive self-improvement)이 가능하여 AI 방법론의 진보에 무한한 속도 향상으로 이어질 수도 있습니다. 그리고 이는 광범위한 확산이 더 느리더라도, 영향에 있어 일부 불연속성을 포함하여 몇 가지 흥미로운 함의를 가질 수 있습니다. 이러한 이유로, 재귀적 자체 개선에 대한 조기 경보 시스템(early warning systems)을 갖추는 것이 중요합니다. 하지만 이러한 시스템은 AGI 정의에 포착되지 않습니다. 우리는 재귀적 자체 개선과는 거리가 멀면서도 AGI를 가질 수 있으며, 그 반대도 마찬가지입니다. 초지능 시나리오에 대한 과도한 집중은 현재 우리가 직면한 AI의 실제적인 문제들, 즉 편향된 데이터로 인한 차별, 일자리 변화, 개인 정보 보호 문제, 그리고 AI 시스템의 오용 가능성 등을 간과하게 만들 위험이 있습니다. 미래의 불확실한 위협에만 매몰되기보다는, 지금 당장 해결해야 할 현실적인 문제에 대한 논의와 해결책 모색이 더욱 시급합니다.

**우리는 AGI가 언제 구축되었는지 알 수 없을 것입니다.**
AGI와 관련 개념에 대한 수많은 정의가 존재합니다. 재스민 선(Jasmine Sun)은 20개 이상의 정의를 유용하게 정리해 놓았는데, 이들은 크게 세 가지 범주로 나뉩니다. 즉, 시스템의 세계에 대한 영향, 내부(internals), 또는 통제된 환경에서의 행동에 기반할 수 있습니다. 우리는 각 정의 방식이 치명적인 결함(fatal flaw)을 가지고 있음을 보여줄 것입니다. 이는 우리가 이상적으로 원하는 것에 비해 너무 엄격하거나 너무 약한 기준(criteria)으로 이어집니다. 이러한 간극을 이해하는 것은 사람들이 AGI가 어떻게 보일지에 대해 왜 다른 직관을 가지고 있는지, 그리고 "보면 알게 될 것"이라는 기준이 왜 실패했고 계속 실패할 것인지를 보여줍니다.

OpenAI의 2018년 AGI 정의는 "대부분의 경제적으로 가치 있는 작업에서 인간을 능가하는 고도로 자율적인 시스템"이었습니다. 우리의 관점, 즉 AI의 영향에 대한 우리의 관심사에서 볼 때, 이 정의는 잠재적으로 매우 유용합니다. 만약 AI가 대부분의 경제적으로 가치 있는 작업에서 [모든] 인간을 능가한다면, 그것은 의심할 여지 없이 영향력이 클 것입니다. 그러나 분명히 말하자면, 이것은 AI 시스템의 속성이 아닙니다. 이것은 세계의 상태(state of the world)의 속성입니다. 이는 우리가 만드는 보완적인 혁신과 우리가 AI를 우리의 조직과 기관에 통합하기로 선택하는 정도와 적어도 같은 정도로 관련이 있습니다. 실험실에서 AI 시스템을 고립시켜 테스트하고 그것이 사람들의 직무에서 사람들을 능가하는지 묻는 것은 터무니없는 일일 것입니다. 그것은 범주 오류(category error)입니다. 예를 들어, AI가 (자율적으로) 의료 연구원을 능가할 수 있는지 여부는 우리가 집단적으로 AI 시스템이 사람들에게 대규모 의료 실험을 수행하도록 허용할지 여부에 부분적으로 달려 있습니다. 우리는 그렇게 해서는 안 되며 그렇게 하지 않을 것입니다. 이는 시스템의 능력과 관계없이 의료 연구원의 기능을 수행할 수 없다는 것을 의미합니다. 이것은 극단적인 예일 수 있지만, 거의 모든 직업에서 유사한 병목 현상이 발생합니다. 더 나쁜 것은, AI를 모든 곳에 확산시키지 않는 한, 우리는 그 시스템이 이론적으로 실제 세계에서 작업을 자동화할 수 있는지조차 알지 못할 것이라는 점입니다. 우리는 세상의 복잡하고 혼란스러운 모습을 충분히 설득력 있게 모의 실험(simulacra)할 수 없을 것입니다. 요컨대, 영향 기반 정의는 고통스러울 정도로 느린 확산 과정의 최종 결과를 예측할 방법을 제공하지 않기 때문에 실용적인 목적에 유용하지 않습니다.

영향에 관심이 있는 우리와 같은 연구자들과는 대조적으로, 많은 연구자들은 내부(internals)의 의미에서 인간과 같은 AI에 관심이 있습니다. 즉, 시스템이 세상을 인과적으로(causally) 진정으로 이해하는지, 우리처럼 추론하고, 계획하고, 새로운 기술을 습득할 수 있는지 등입니다. 이러한 AGI의 의미는 AI 내부를 관찰하고 특성화하는 어려움 때문에 운영화(operationalize)하기가 매우 어려웠습니다. 튜링 테스트(Turing test)는 우리가 중요하게 생각하는 인간과 같은 속성을 행동의 대리 지표로 사용하려는 많은 시도 중 가장 잘 알려진 것이지만, 필연적으로 우리는 기대했던 인간과 같은 내부를 가지지 않고도 그러한 테스트를 통과하는 AI 시스템을 구축할 수 있다는 것이 밝혀졌습니다. 더욱이, AI의 들쭉날쭉함(jaggedness) 때문에 — 여러 면에서 초인적이지만, 다른 면에서는 유아의 세상 이해력이 부족한 — AI의 변혁적인 효과는 모든 면에서 완전히 인간과 같아지기(또는 초인적이 되기) 훨씬 전에 느껴질 가능성이 높습니다. 요컨대, 우리는 내부 자체에 관심이 없으므로 이러한 종류의 정의는 제쳐둡니다.

이는 우리에게 세 번째 종류의 정의를 남기는데, 이는 단연코 가장 일반적인 것으로, 행동에 기반하며 벤치마크 성능(benchmark performance)으로 운영화됩니다. 예를 들어, "인간-기계 지능 동등성(human-machine intelligence parity)"에 대한 메타큘러스(Metaculus) 질문은 수학, 물리학, 컴퓨터 과학 시험 문제의 성능으로 정의됩니다. 이러한 종류의 정의의 문제는 잘 알려져 있으며, 우리는 이를 반복적으로 논의했습니다. 그것들은 단순히 실제 세계에서 반드시 유용하지 않더라도 벤치마크를 이길 수 있는 AI 시스템을 구축하는 의미에서 언덕 오르기(hill climbing)를 장려할 뿐입니다. 예를 들어, AI가 특정 게임에서 인간 챔피언을 이긴다고 해서 그것이 AGI라고 단정할 수는 없습니다. 게임의 규칙과 목표가 명확하게 정의된 환경에서는 AI가 뛰어난 성능을 보일 수 있지만, 불확실성이 크고 상황에 따라 유연한 대응이 필요한 현실 세계의 문제에서는 여전히 취약점을 드러낼 수 있습니다. 이러한 벤치마크는 AI 발전의 특정 측면을 측정하는 데 유용하지만, 인간 지능의 포괄적인 특성이나 실용적인 가치를 완벽하게 대변하지 못합니다.

**세 가지 AGI 정의 방식의 장단점과 직관의 차이**
정의의 난제에 대한 한 가지 반응은 AGI를 직접 보면 알게 될 것이라고 주장하는 것입니다. 그러나 o3의 출시는 그 반대가 사실임을 명확히 보여줍니다. 일부 사람들에게는 o3의 능력 발전이 AGI라고 부를 만한 단계적 변화(step change)를 나타내는 것처럼 보일 수 있습니다. 반면, 다른 사람들에게는 이러한 개선이 기껏해야 미미하며, 실제 세계에 미치는 영향은 제한적일 것이라고 평가합니다. 이처럼 사람들의 직관이 다른 이유는 무엇일까요?

우리의 추측은 다음과 같습니다. AI 능력은 일반적일 수 있지만, AI가 실제 세계에서 유용하게 기능하려면 대체로 도메인별(domain-specific) 방식으로 구현되어야 할 것입니다. (o3의 범용 에이전트(generalist agent) 측면은 오해의 소지가 있습니다. 이는 주로 오류 비용이 낮은 생성 작업(generative tasks)에서는 효과적일 수 있지만, 실제 세계에서 독립적으로 작동해야 할 때는 그렇지 않습니다. 예를 들어, 인간에게는 사소해 보이는 작업임에도 불구하고, 사용자의 복잡한 요구사항을 완벽하게 이해하고 유용한 여행 일정을 계획하며 예약까지 처리하는 완전 자율 여행 예약 AI 에이전트는 아직 출시되지 않았습니다.)

따라서 사람들이 o3 또는 다른 어떤 시스템이 AGI(에 가깝)인지 생각할 때, 그들은 무의식적으로 다른 도메인과 시나리오를 떠올리고 있으며, 일반적인 AI 능력과 실제 세계에서의 유용한 능력 사이의 간극은 도메인마다 크게 다릅니다. 예를 들어, 고도로 구조화된 환경에서 데이터를 분석하거나 코드를 생성하는 능력과, 예측 불가능한 물리적 환경에서 로봇을 조작하거나 복잡한 사회적 상호작용을 처리하는 능력 사이에는 큰 차이가 있습니다. 이러한 간극은 기술적인 문제뿐만 아니라, 규제, 윤리, 사용자 수용도 등 다양한 비기술적 요인에 의해서도 좌우됩니다. 대부분의 작업을 자동화할 수 있는 유용한 제품을 갖추는 임계점(threshold)을 넘어서는 것은 다른 부문이나 직업에서 매우 다른 시기에 발생할 수 있습니다. 예를 들어, 법률 문서 검토와 같은 특정 사무직은 AI에 의해 빠르게 자동화될 수 있지만, 고도의 수작업 기술이나 깊은 인간적 공감이 필요한 서비스 직종은 훨씬 더 오랜 시간 동안 AI의 직접적인 영향을 받지 않을 수 있습니다. 이러한 복잡성을 이해하는 것이 AGI에 대한 현실적인 기대를 설정하는 데 중요합니다.

**기업과 정책 입안자들은 장기적인 관점을 가져야 합니다.**
AGI는 실행 가능한 사건이 아니므로 이정표가 아닙니다. 어떤 회사가 AGI를 달성했거나 달성할 예정이라고 선언하는 것은 기업이 어떻게 계획해야 하는지, 어떤 안전 개입(safety interventions)이 필요한지, 또는 정책 입안자들이 어떻게 반응해야 하는지에 대해 아무런 영향을 미치지 않습니다. 그렇다면 기업과 정책 입안자들은 대신 무엇을 해야 할까요?

기업들은 미완성된 AI 제품을 서둘러 채택해서는 안 됩니다. AI 방법론과 능력의 빠른 발전이 자동으로 더 나은 제품으로 이어지는 것은 아닙니다. 본질적으로 확률적(stochastic)인 모델 위에 제품을 구축하는 것은 어렵습니다. 기업들은 핵심 비즈니스 프로세스(business processes)를 자동화하기 위해 AI를 사용하는 영향을 판단하기 위해 신중한 실험을 수행하면서 AI 제품을 조심스럽게 채택해야 합니다. 특히, 우리는 AI 에이전트가 인간 노동자의 "즉시 대체품(drop-in replacements)"이 되어 워크플로우(workflows)에 자동화를 신중하게 평가하고 통합할 필요성을 어떻게든 우회할 것이라는 생각에 대해 극도로 회의적입니다. AI 제품을 개발하는 기업들은 채택의 장애물(hurdles)을 식별하고 AI를 채택하는 기업들이 원하는 것을 구축하기 위해 해당 도메인에 대한 깊은 이해가 필요합니다. 예를 들어, Cursor와 Windsurf와 같은 AI 기반 코드 편집기(code editors)의 한 가지 핵심 혁신은 프로그래머가 AI가 생성한 텍스트를 다양한 추상화 수준(levels of abstraction)에서 검증할 수 있도록 하는 사용자 인터페이스(user interface)입니다. 다른 산업들은 제품이 해결해야 할 AI 채택에 대한 다른 장애물을 가질 것입니다.

정책 입안자들도 장기적인 관점을 가져야 합니다. "AGI를 위한 맨해튼 프로젝트"는 여러 면에서 잘못된 생각입니다. AGI는 이정표가 아니므로, 목표가 언제 달성되었는지 또는 얼마나 더 투자해야 하는지 알 방법이 없습니다. 그리고 AI 능력을 가속화하는 것은 경제적 이점을 실현하는 데 있어 실제 병목 현상을 해결하는 데 아무런 도움이 되지 않습니다. 오히려, 정책 입안자들은 AI 기술의 안전하고 생산적인 확산을 위한 기반을 마련하는 데 집중해야 합니다. 여기에는 AI 인프라 투자(고성능 컴퓨팅 자원, 대규모 데이터셋), AI 인재 양성을 위한 교육 시스템 개혁, 그리고 AI 기술의 윤리적 사용을 위한 명확한 가이드라인 및 규제 프레임워크 구축 등이 포함됩니다.

이러한 관점은 수출 통제(export controls)에도 영향을 미칩니다. 미국은 중국의 AI 개발을 늦추기 위해 AI 개발에 필요한 하드웨어에 대한 수출 통제를 적용했습니다. 수출 통제 지지자들은 이것이 미국과 중국 간의 격차를 몇 달 이상 벌리지 않을 것이라고 인정합니다(우리도 동의합니다). 그러나 이것은 첨단 AI 개발의 영향이 급격한 세계에서만 중요합니다. 만약 첨단 AI의 영향이 확산을 통해 실현되고, 확산 과정이 수십 년이 걸린다면, 수십 년의 게임에서 몇 달 앞서는 것은 거의 중요하지 않습니다. 따라서 정책 입안자들은 확산을 가능하게 하는 데 집중해야 합니다. 우리는 최근 에세이에서 그렇게 하는 방법에 대한 몇 가지 아이디어를 제시했습니다. AGI를 변혁적인 AI 개발의 이정표로 취급하는 것은 매력적이지만 잘못된 생각입니다. 이는 AI 발전과 위험, 경제적 영향, 지정학에 대한 잘못된 정신 모델을 부추깁니다. AI가 세계에 미치는 영향은 마법의 총알 기술(magic-bullet technology)을 향한 단거리 경주를 통해서가 아니라, 수백만 가지의 지루하고 작은 비즈니스 프로세스(business process) 적응과 정책 조정(policy tweaks)을 통해 실현될 것입니다.

이 에세이 초안에 대한 피드백을 주신 스티브 뉴먼(Steve Newman)과 재스민 선(Jasmine Sun)에게 감사드립니다.

**추가 자료 읽기**
재스민 선(Jasmine Sun)의 최근 에세이는 정신과 내용 면에서 우리의 에세이와 유사합니다. 그녀는 많은 인기 있는 AGI 정의를 대조하고, AGI가 구체적인 기술적 이정표(technical milestone)라기보다는 AI 커뮤니티의 열망(aspiration)으로 보는 것이 더 낫다고 결론 내립니다. 그녀는 또한 AGI 및 기타 관련 개념에 대한 인기 있는 정의들을 정리했습니다. 보르한 빌리-하멜린(Borhane Blili-Hamelin)과 다른 이들은 우리가 AGI를 AI 연구의 북극성(North star)으로 취급하는 것을 멈춰야 한다고 주장합니다. 에게 에르딜(Ege Erdil)은 변혁적인 AI 능력에 도달하는 데 수십 년의 시간표(timelines)가 필요하다는 주장을 펼칩니다. 에릭 살바지오(Eryk Salvaggio)는 정책 입안자들이 AGI가 임박한 것처럼 행동하는 것이 위험한 많은 이유를 제시합니다. "AI as Normal Technology"에서 우리는 AI의 미래에 대한 우리의 견해에 대한 지적 기반(intellectual foundation)을 마련했습니다. 이 에세이는 그 안에 담긴 아이디어의 함의를 탐구하는 많은 후속작 중 첫 번째입니다.

---

<sup>1</sup> LLM은 텍스트만 출력할 수 있지만, o3와 같은 모델은 텍스트 출력을 사용하여 도구와 상호작용하도록 훈련됩니다. 예를 들어, o3는 특정 키워드(예: "use-search")를 출력하여 주어진 쿼리(query)를 온라인에서 검색할 수 있습니다. 분명히 말하자면, 이전 LLM도 도구와 상호작용할 수 있었습니다. 예를 들어, OpenAI의 ChatGPT 플러그인(plugins)은 2년 전 GPT-4와 같은 모델이 도구를 사용하도록 허용했습니다. o3와 같은 모델의 핵심 차이점은 도구를 효과적으로 사용하도록 훈련되었다는 것입니다. 이전 모델은 이 작업을 위해 명시적으로 훈련되지 않은 상태에서 프롬프트(prompts)를 기반으로 도구를 사용했거나, 기껏해야 도구 사용 구문(syntax)을 배웠습니다. o3는 검색, 코드 실행 능력, 파일 접근, 이미지 생성 및 추론을 포함하여 ChatGPT 인터페이스의 많은 도구에 접근할 수 있습니다. 전반적으로 o3는 챗봇(chatbot)이라기보다는 "에이전트(agent)"에 훨씬 더 가깝습니다.

<sup>2</sup> 예를 들어, 이 시스템이 훈련을 마친 후에야 출시된 소프트웨어 라이브러리(software library)를 사용하여 새로운 유형의 코딩 작업을 수행하는 데 사용된다고 가정해 봅시다. 충분한 문서와 지침이 주어진다면, 결국 작업을 올바르게 수행할 수 있을 것입니다. 그러나 재훈련을 통한 모델의 명시적인 업데이트가 없는 한, 이 "경험"은 모델의 가중치(weights)를 변경하지 않을 것이므로, 동일한 작업을 수행해야 하는 다른 사용자는 첫 번째 사용자가 제공했던 것과 동일한 도움을 주어야 할 것입니다.

<sup>3</sup> 우리는 이 글에서 군사 AI(military AI)를 분석하지 않습니다. 이는 중요한 주제이며, 향후 에세이에서 다시 다룰 것입니다.

<sup>4</sup> 새로운 기술의 발명(invention)과 그 기술이 사회 전반에 확산되도록 돕는 혁신(innovations)을 구별하는 것이 중요합니다. 이 단락에서의 논의는 전자에 관한 것입니다. 발명은 새로운 모델의 개발, 아마도 새로운 능력의 동반을 의미합니다. 혁신은 이러한 능력이 생산적으로 사용될 수 있도록 합니다. 더 좋고 유능한 모델은 발명의 예이며, 이러한 모델을 사용하는 제품의 개발은 혁신의 예입니다. 우리는 국가들이 발명에서 길고 지속적인 우위를 유지할 가능성이 높다고 생각하지 않습니다. 그러나 이 섹션에서 나중에 논의하듯이, 발명의 생산적인 사용을 가능하게 하는 조건은 국가마다 크게 다를 수 있습니다.

<sup>5</sup> 아모데이(Amodei)는 AGI를 피하고 자신의 예측에 대해 더 정확하게 표현하기 위해 "강력한 AI(powerful AI)"라는 용어를 사용합니다.