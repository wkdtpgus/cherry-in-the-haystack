OpenAI의 최신 모델 출시와 함께, 인공 일반 지능(Artificial General Intelligence, AGI)이 이미 달성되었는지에 대한 논쟁이 다시 불붙고 있습니다. 특히 최근 몇 년간 대규모 언어 모델(LLM)과 에이전트 기술의 급격한 발전은 이러한 논의에 새로운 불을 지폈습니다. 하지만, 우리는 AGI의 개념 자체가 종종 잘못 이해되고 있음을 지적하고자 합니다. AGI는 단일한 이정표라기보다는 지속적인 기술 발전의 스펙트럼 위에 존재하는 가변적인 개념에 가깝습니다.

**AGI는 이정표가 아니다: 지속적인 발전과 변화하는 정의**
OpenAI의 최신 모델 o3 출시와 함께, 인공 일반 지능(Artificial General Intelligence, AGI)이 이미 달성되었는지에 대한 논쟁이 다시 불붙고 있습니다. 이에 대한 일반적인 회의론자들의 반응은 AGI의 정의에 대한 합의가 없다는 것입니다. 그것은 사실이지만, 핵심을 놓치고 있습니다. 만약 AGI가 그렇게 중대한 이정표라면, 그것이 구축되었을 때 명백히 드러나야 하지 않을까요? 이 에세이에서 우리는 AGI가 이정표가 아니라고 주장합니다. AGI는 AI 시스템의 속성이나 영향에 있어 불연속성을 나타내지 않습니다. 어떤 정의에 기반하든, 어떤 회사가 AGI를 구축했다고 선언하더라도, 그것은 실행 가능한 사건이 아닙니다. 이는 기업, 개발자, 정책 입안자 또는 안전에 아무런 영향을 미치지 않을 것입니다. 구체적으로 말하자면, 범용 AI 시스템이 어떤 합의된 능력 임계값(capability threshold)에 도달하더라도, AI가 산업 전반에 확산되어 생산적인 영향을 실현하려면 많은 보완적인 혁신이 필요할 것입니다. 확산은 기술 개발 속도가 아니라 인간(및 사회)의 시간 척도(timescales)에 따라 발생합니다. AGI와 치명적인 위험에 대한 우려는 종종 능력(capabilities)과 권력(power)을 혼동합니다. 이 둘을 구별하고 나면, AI 개발에 있어 인류가 통제력을 유지하기 불가능해지는 임계점(critical point)이라는 개념을 거부할 수 있습니다. AGI 정의의 확산은 질병이 아니라 증상입니다. AGI는 예상되는 영향 때문에 중요하지만, AI 시스템 자체의 속성을 기반으로 정의되어야 합니다. 그러나 시스템 속성과 영향 사이의 연결 고리는 미약하며, AI 시스템이 작동하는 환경을 어떻게 설계하느냐에 크게 좌우됩니다. 따라서 특정 AI 시스템이 변혁적인 영향을 미칠지 여부는 시스템이 출시되는 시점에는 아직 결정되지 않습니다. 그러므로 AI 시스템이 AGI를 구성한다는 결정은 오직 회고적으로(retrospectively)만 의미 있게 이루어질 수 있습니다.

과거에 AGI의 지표로 여겨졌던 능력들, 예를 들어 체스나 바둑에서 인간 챔피언을 이기는 것, 복잡한 텍스트를 이해하고 생성하는 것 등은 이제 일반적인 AI 시스템의 기능이 되었습니다. 이러한 "이정표"들이 달성될 때마다, AGI의 정의는 더욱 추상적이거나 미래 지향적인 능력으로 옮겨가는 경향이 있습니다. 이는 AGI가 고정된 목표가 아니라, AI 기술 발전의 최전선을 지칭하는 "움직이는 골대(moving goalpost)"와 같다는 것을 시사합니다. 오늘날 우리는 "프론티어 모델(frontier models)"이라는 용어를 사용하며, 이는 AGI라는 광범위하고 합의되지 않은 개념보다 현재의 최첨단 AI 시스템을 더 정확하게 설명합니다.

**AGI에 대한 반대 비유로서의 핵무기**
AGI 달성은 OpenAI와 같은 기업들, 그리고 AI 연구 커뮤니티의 많은 부분의 명시적인 목표입니다. 이는 맨해튼 프로젝트(Manhattan Project)의 핵심 목표가 핵무기를 만들고 전달하는 것이었던 것과 같은 방식으로 이정표로 취급됩니다. 이 목표는 맨해튼 프로젝트에서 두 가지 이유로 이정표로서 의미가 있었습니다. 첫째는 관측 가능성(observability)입니다. 핵무기를 개발할 때, 목표에 도달했는지 여부에 대해 의심의 여지가 없습니다. 폭발은 관측 가능성을 전형적으로 보여줍니다. 둘째는 즉각적인 영향(immediate impact)입니다. 핵무기 사용은 제2차 세계 대전의 빠른 종식에 기여했습니다. 또한 새로운 세계 질서, 즉 지정학의 장기적인 변혁을 가져왔습니다. 많은 사람들은 AGI가 이러한 속성을 가질 것이라고 직관적으로 생각합니다. AGI는 너무나 강력하고 인간과 같아서 우리가 그것을 만들었을 때 명백히 드러날 것입니다. 그리고 AGI는 즉시 막대한 이점과 위험을 가져올 것입니다. 경제의 상당 부분을 자동화하고, AI 연구 자체를 포함한 혁신을 크게 가속화하며, 통제 불가능한 초지능(superintelligence)으로 인해 인류에게 잠재적으로 치명적인 결과를 초래할 수 있습니다. 이 에세이에서 우리는 AGI가 정반대일 것이라고 주장합니다. 즉, 특별한 의미를 지닌 명확한 능력 임계값(capability threshold)이 없기 때문에 관측 불가능하며, 세계에 즉각적인 영향을 미치지 않을 것이고, 심지어 경제의 장기적인 변혁조차 불확실하다는 것입니다. 이전 에세이에서 우리는 AGI를 핵무기에 비유하여 일부 사람들이 권장하는 재앙적인 정책 개입에 반대하는 주장을 펼쳤습니다. 이 비유가 우리가 잘못된 예측과 역효과를 낳는 권고라고 생각하는 것을 일관되게 만들어낸다는 점은 우리에게 놀랍습니다.

핵무기 비유는 AI의 잠재적 위험에 대한 사람들의 인식을 형성하는 데 강력한 영향을 미쳤지만, AI 기술의 본질과는 근본적인 차이가 있습니다. 핵무기는 단일하고 파괴적인 사건으로 그 영향이 즉각적이고 명확합니다. 반면 AI는 전력, 증기기관, 인터넷과 같은 다른 범용 기술과 더 유사합니다. 이들은 사회에 점진적이고 광범위하게 스며들어 수십 년에 걸쳐 심오한 변화를 가져왔습니다. 이러한 기술들은 단일한 "발명"의 순간이 아니라, 끊임없는 혁신, 적응, 그리고 확산 과정을 통해 그 잠재력을 실현했습니다. AI 역시 단일한 "AGI 순간"이 아니라, 수많은 애플리케이션, 인프라 개선, 그리고 사회적 적응을 통해 그 영향이 발현될 것입니다.

**최신 에이전트 모델의 발전과 AGI에 대한 오해**
많은 저명한 AI 평론가들은 최신 에이전트 모델을 일종의 AGI라고 불렀습니다. 타일러 코웬(Tyler Cowen)은 AGI를 보면 알 수 있다면 자신은 그것을 보았다고 말합니다. 이든 몰릭(Ethan Mollick)은 최신 에이전트 모델을 들쭉날쭉한 AGI(jagged AGI)라고 묘사합니다. 이러한 흥분을 불러일으킨 것은 무엇일까요? 핵심 혁신은 모델이 웹을 검색하고 도구를 추론 사슬(reasoning chain)의 일부로 사용하는 방법을 학습하기 위해 강화 학습(reinforcement learning)을 사용한다는 것입니다. <sup>1</sup> 이러한 방식으로, 최신 에이전트 모델은 대규모 언어 모델(LLM)이 직접 수행할 수 있는 것보다 더 복잡한 인지 작업(cognitive tasks)을 수행할 수 있으며, 사람과 유사한 방식으로 그렇게 할 수 있습니다. 예를 들어, 온라인 쇼핑을 위해 여러 웹사이트에서 제품을 비교하고, 사용자 리뷰를 분석하며, 예산과 선호도에 맞춰 최적의 선택을 제안하는 에이전트를 상상해 보세요. 이러한 종류의 작업을 꽤 잘 수행하는 범용 에이전트(generalist agent)의 등장은 AGI에 대한 논의를 다시 활성화시켰습니다.

하지만 이러한 발전이 AGI를 의미하는지에 대해 신중하게 접근해야 합니다. 최신 에이전트 모델들은 특정 도메인에서 인상적인 능력을 보여주지만, 여전히 심각한 한계를 가지고 있습니다. 예를 들어, 실시간으로 새로운 기술을 배우고 적용하는 능력은 여전히 미해결 연구 과제입니다. 현재의 에이전트들은 주로 훈련 데이터에 기반하여 작동하며, 새로운 환경이나 예상치 못한 상황에 직면했을 때 유연하게 적응하는 데 어려움을 겪습니다. 또한, 오류 발생 시 책임 소재 문제, 윤리적 딜레마, 그리고 편향성 문제는 실제 세계에 이러한 에이전트를 배치하는 데 있어 중요한 장애물로 남아있습니다. 특정 벤치마크에서는 인간 수준 또는 초인적인 성능을 보이지만, 여전히 상식적인 추론, 미묘한 사회적 상호작용, 또는 복잡한 물리적 환경에서의 조작과 같은 많은 실제 작업에서는 취약성을 드러냅니다. 이는 "들쭉날쭉한 AGI"라는 표현처럼, 특정 능력은 뛰어나지만 전반적인 지능은 인간과 거리가 멀다는 것을 의미합니다.

**AGI는 확산에 수십 년이 걸리므로 경제에 충격을 주지 않을 것입니다.**
AGI를 이정표로 취급하고 AGI 선언을 진지하게 받아들이는 한 가지 주장은 AGI가 희소성 없는 세상, 돈이라는 개념의 종말, 또는 갑작스러운 대량 실업과 같은 긍정적 및 부정적 측면 모두에서 급격한 경제적 영향을 초래할 수 있다는 것입니다. 그러나 AI의 경제적 영향은 경제 전반에 걸쳐 채택될 때만 실현됩니다. 기술 발전은 이러한 영향을 실현하는 데 필요하지만 충분하지는 않습니다. 전기, 컴퓨팅, 인터넷과 같은 과거의 범용 기술(general-purpose technologies)의 경우, 근본적인 기술 발전이 사회 전반에 확산되는 데 수십 년이 걸렸습니다. 산업 혁명(Industrial Revolution)의 기적은 높은 성장률(연평균 성장률이 3% 미만)이 아니라 수십 년간 지속된 성장 기간이었습니다. AI 확산에는 많은 병목 현상(bottlenecks)이 있습니다. 유용한 제품 및 애플리케이션 개발, 이러한 제품을 활용할 인력 훈련, AI 사용을 가능하게 하는 조직 변화 구현, 기업의 AI 채택을 촉진하는 법률 및 규범 확립 등이 그것입니다. 과거의 범용 기술과 마찬가지로, 우리는 AI의 경제적 영향이 이러한 확산 과정이 전개됨에 따라 수십 년에 걸쳐 실현될 것으로 예상합니다. "AI as Normal Technology"라는 논문에서 우리는 왜 이러한 상황이 될 것이라고 생각하는지에 대한 자세한 주장을 제시합니다. 능력의 급격한 증가가 급격한 경제적 영향으로 이어진다는 생각은 AI의 과거 및 현재와 완전히 일치하지 않으며, 미래에 이것이 바뀔 것이라고 예상할 이유도 없습니다.

AI의 확산 병목 현상은 단순히 기술적 문제를 넘어섭니다. 기업들은 AI 솔루션을 기존 워크플로우에 통합하기 위한 복잡한 조직 변화를 겪어야 하며, 이는 상당한 시간과 자원을 필요로 합니다. 또한, AI 시스템의 효과적인 작동을 위해서는 고품질의 데이터 인프라 구축과 데이터 거버넌스(data governance) 체계 마련이 필수적입니다. 숙련된 AI 전문가뿐만 아니라, AI 도구를 효과적으로 활용할 수 있는 일반 인력의 재교육 및 훈련도 중요합니다. 법률 및 규제 측면에서도, AI의 윤리적 사용, 데이터 프라이버시, 책임 소재 등을 다루는 새로운 프레임워크가 필요하며, 이러한 변화는 기술 발전 속도보다 훨씬 느리게 진행됩니다. 이러한 복합적인 요인들로 인해, AI의 경제적 영향은 단일한 "AGI 달성" 순간에 폭발적으로 나타나기보다는, 점진적이고 불균등하게 사회 전반에 걸쳐 확산될 것입니다.

**AGI는 세계 질서의 급격한 변화로 이어지지 않을 것입니다.**
미국과 중국은 종종 AI 군비 경쟁(AI arms race)을 벌이고 있으며, 각국이 AGI를 구축하기 위해 경쟁하고 있다고 묘사됩니다. AGI를 먼저 구축하는 국가가 결정적인 전략적 우위(strategic advantage)를 가지게 되어, 예측 가능한 미래에 세계 질서에서 지배력을 확보할 것이라는 가설이 있습니다. <sup>3</sup> 이러한 서사는 말이 되지 않습니다. AI 모델을 만드는 데 필요한 지식과 모델 능력 자체는 국가 간에 빠르게 확산되는 경향이 있기 때문입니다. 수십만 명의 AI 기술자(technologists)가 있으며, 그들은 정부 연구소보다는 민간 부문에서 일하므로, 그러한 규모에서 비밀을 유지하는 것은 실현 불가능합니다. 발명, 즉 이 경우 AI 모델 개발은 경쟁 우위(competitive advantage)의 원천으로서 과대평가되어 있습니다. 우리는 기술 발전이 국가 간에 대략적으로 보조를 맞출 것으로 예상해야 합니다. 비록 미국 기업들이 현재 선두에 있지만, 우리는 지속적인 우위를 기대해서는 안 됩니다. <sup>4</sup> 많은 사람들은 기술 능력의 확산 용이성을 제대로 인식하지 못했으며(아마도 핵무기 정신 모델(mental model) 때문일 것입니다), 이에 놀라움을 금치 못했습니다. 이것이 올해 초 "딥시크 모멘트(DeepSeek moment)"로 이어진 이유입니다. 분석가들은 AI 능력이 얼마나 빨리 확산될 수 있는지 깨닫지 못했고, 그 결과 신생 기업들(특히 중국 기업들)이 그렇게 빨리 따라잡을 것이라고 예상하지 못했습니다. 일부 사람들은 몇 달의 우위조차 중요할 것이라고 주장합니다. 우리는 동의하지 않습니다. 강대국 경쟁의 맥락에서 중요한 질문은 어느 나라가 AGI를 먼저 구축하느냐가 아니라, 어느 나라가 확산을 더 잘 가능하게 하느냐입니다. 제프리 딩(Jeffrey Ding)이 보여주었듯이, 국내외 AI 발명과 혁신을 실제로 활용하여 생산성을 향상시키는 기업과 정부의 효율성은 범용 기술의 경제적 영향을 결정하는 데 훨씬 더 중요합니다. 중국 AI 기업들은 AI 모델과 능력 면에서 선도적인 미국 기업들보다 기껏해야 6-12개월 뒤처져 있지만, 중국은 확산을 가능하게 할 수 있는 몇 가지 핵심 지표, 즉 디지털화(Digitization), 클라우드 컴퓨팅(cloud computing) 채택, 인력 훈련 면에서 미국에 크게 뒤처져 있습니다. 이 모든 것은 산업 전반에 걸쳐 AI 발전의 생산적인 확산을 가능하게 하는 데 필요합니다. 이것이 미국의 실제 경쟁 우위의 원천입니다. 물론, 이것은 향후 몇 년 안에 바뀔 수 있습니다. 그러나 그렇게 된다면, 그것은 AGI 개발보다는 확산을 촉진하기 위한 정책 변화의 결과일 것입니다. 그리고 정책을 얼마나 빨리 바꾸든, 국가들이 하룻밤 사이에 이룰 수 있는 일이 아닙니다. 확산은 일반적으로 수십 년에 걸쳐 전개됩니다. 이 모든 것이 정책 입안자들이 안주해야 한다는 의미는 아닙니다. 그러나 이는 AGI에 집착하기보다는 기존 AI를 포함하여 생산적이고 안전한 확산을 가능하게 하는 데 집중해야 한다는 것을 의미합니다.

AI 기술의 글로벌 확산은 단순히 모델 개발 경쟁을 넘어섭니다. 각국은 AI 기술을 자국의 산업 및 사회에 효과적으로 통합하기 위한 전략적 노력을 기울여야 합니다. 여기에는 인재 양성, 데이터 인프라 구축, 윤리적 가이드라인 마련, 그리고 국제 협력 증진이 포함됩니다. 예를 들어, 유럽연합(EU)은 AI 규제법(AI Act)을 통해 AI의 윤리적이고 신뢰할 수 있는 개발 및 확산을 위한 프레임워크를 구축하며, 이는 기술적 우위보다는 사회적 수용성을 통한 장기적인 경쟁력 확보를 목표로 합니다. 국가 간의 AI 경쟁은 누가 더 강력한 모델을 먼저 만드느냐에 달려있기보다는, 누가 AI 기술을 자국 경제와 사회에 더 유연하고 효과적으로 통합하여 실제 가치를 창출하느냐에 달려있습니다.

**AGI의 장기적인 경제적 영향은 불확실합니다.**
즉각적인 경제적 영향이 없더라도, AGI가 예를 들어 수십 년에 걸쳐 큰 성과를 낼 수 있는 연간 10%의 GDP 성장을 가능하게 할 수 있을까요? 아마도 그럴 수 있습니다. 그러나 이것이 왜, 어떻게 일어날지는 전혀 명확하지 않습니다. 역사적으로 이러한 종류의 성장 가속화는 매우 드물게 발생했습니다. 산업 혁명(industrial revolution)은 이러한 효과를 가져왔지만, GDP에 거의 영향을 미치지 않은 인터넷은 그렇지 않았습니다. GDP가 측정하기에 적절한 것이 아니라고 생각하더라도, GDP 성장률의 질적 변화는 여러분이 중요하게 생각하는 경제의 근본적인 변화에 대한 좋은 대리 지표(proxy)입니다. 문제는 성장을 가속화하려면 진보의 병목 현상(bottlenecks)을 제거해야 한다는 것입니다. 이는 대부분의 AI 지지자들이 가정하는 것보다 어렵습니다. AI는 부문별로 불균등한 영향을 미칠 가능성이 높으며, 장기적인 성장은 가장 약한 부문에 의해 병목 현상이 발생할 것입니다. 극적인 효과를 주장하는 사람들은 종종 병목 현상이 실제로 무엇인지에 대한 잘못된 정신 모델(mental model)을 가지고 있습니다. 예를 들어, 저렴한 과학 혁신이 진보를 가능하게 할 것이라고 믿고 싶겠지만, 새로운 발견의 생산은 실제로는 과학의 병목 현상이 아닙니다. 더 넓게 보면, 진보는 기술뿐만 아니라 올바른 전제 조건, 즉 보완적인 혁신(complementary innovations)과 문화적, 경제적, 정치적 요인에 달려 있습니다. 산업 혁명을 일으키는 데 필요한 것이 증기 기관(steam power)의 발명뿐이었다면, 로마 제국(Roman Empire)이 그것을 해냈을 것입니다. 우리의 현재 법률, 규범, 제도 및 정치는 기술적 잠재력이 훨씬 적었던 시기에 발전했습니다. 그것들은 이미 더 많은 공공 인프라(public infrastructure)를 구축하는 것과 같은 직접적인 유형의 성장을 위한 기회를 막고 있습니다. 광범위한 인지 자동화(cognitive automation)가 잠재적으로 가져올 수 있는 경제적 이점을 얻으려면, 발생해야 할 구조적 변화의 정도는 헤아릴 수 없을 정도로 더 큽니다. 결론적으로, AGI로 인한 장기적인 영향의 범위와 성격은 아직 지켜봐야 하며, 우리가 어떤 보완적인 조치(complementary actions)를 취하느냐에 달려 있습니다. 장기적인 영향은 AGI 자체의 속성이 아닙니다.

AI가 경제 성장에 미치는 영향은 복잡하며, 단순히 기술적 능력의 향상만으로는 충분하지 않습니다. 생산성 향상과 장기적인 경제 성장을 위해서는 기술적 혁신과 더불어 사회적, 제도적 혁신이 필수적입니다. 예를 들어, AI 기반 자동화가 일자리를 대체하는 것이 아니라, 인간 노동자가 더 가치 있는 작업에 집중하고 생산성을 높일 수 있도록 보완하는 "인간-AI 협업" 패러다임이 중요합니다. 이를 위해서는 교육 시스템의 변화, 사회 안전망 강화, 그리고 노동 시장의 유연성 확보와 같은 정책적 노력이 수반되어야 합니다. 또한, AI 기술이 모든 산업에 균등하게 적용되는 것이 아니므로, 특정 부문에서의 강력한 성장이 다른 부문의 병목 현상으로 인해 전체 경제 성장으로 이어지지 않을 수도 있습니다.

**AGI의 정렬 불량(misalignment) 위험은 권력과 능력을 혼동합니다.**
반면에, AGI는 AI의 사회적 위험에 대한 전환점이 될 수 있습니다. AGI가 통제 상실, 막대한 사회적 해악, 심지어 인류 멸종을 초래할 수 있을까요? AGI 위험에 대한 논의는 권력(power) — 환경을 수정할 수 있는 능력 — 과 능력(capability) — 지정된 작업을 올바르게 해결할 수 있는 역량 — 을 혼동합니다. 능력은 AI 시스템의 본질적인 속성인 반면, 권력은 AI 시스템이 작동하는 환경을 우리가 어떻게 설계하느냐의 문제입니다. 그리고 인간은 이 설계에 대한 주체성(agency)을 가지고 있습니다. 이러한 구별은 종종 간과됩니다. 다리오 아모데이(Dario Amodei)의 "강력한 AI(powerful AI)" 정의를 생각해 봅시다. <sup>5</sup> 그는 "...미해결 수학 정리 증명, 매우 훌륭한 소설 작성, 어려운 코드베이스(codebases)를 처음부터 작성"하는 것과 같은 강력한 AI의 능력에 대한 설명으로 시작합니다. 이 기준은 AI 능력의 한 예이며, 우리는 이를 AI 시스템 수준에서 논의할 수 있습니다. 그러나 그는 이어서 AI 시스템이 작동하도록 허용하는 환경의 속성을 설명하는데, 여기에는 "...인터넷에서 행동하기, 인간에게 지시를 내리거나 받기, 재료 주문하기, 실험 지시하기, 비디오 시청하기, 비디오 만들기 등"이 포함됩니다. 이것은 AI 시스템에 부여된 권력의 한 예입니다. 이는 AI 시스템이 작동하는 환경에 따라 달라지며, AI 능력이 권력으로 어떻게 전환되는지를 결정합니다. 우리는 AI 능력이 계속 증가할 것으로 예상합니다. 그러나 능력 수준과 관계없이, 우리는 AI가 도구로 남아 인간의 감독 없이 작동할 권력과 자율성(autonomy)을 부여받지 않도록 선택할 수 있습니다. "AI as Normal Technology" 에세이에서 우리는 기업 간의 군비 경쟁, 권력 추구, 초인적인 설득, 기만적인 정렬(deceptive alignment) 등 이에 대한 모든 일반적인 반론을 다룹니다. 우리는 이 논문에서 적절한 감독 없이 AI를 배포하는 것에 반대하는 강력한 비즈니스 유인(incentives)이 있을 것이며, 이러한 유인들은 필요할 때 규제에 의해 뒷받침될 수 있고 또 뒷받침되어야 한다고 주장합니다. 이는 자율주행차(self-driving cars)부터 AI 비서(AI assistants)에 이르는 분야에서 역사적으로 그래왔습니다. 우리는 AI 능력이 우리가 임의로 AGI로 지정하는 추정된 티핑 포인트(tipping point)에 도달했다고 해서 이 추세가 갑자기 뒤바뀔 것이라고 예상하지 않습니다.

능력과 권력의 구별은 AI 안전 연구에서 매우 중요합니다. AI 시스템의 능력이 아무리 뛰어나더라도, 그것이 실제 세계에서 어떤 영향을 미칠지는 우리가 그 시스템에 어떤 권한과 자율성을 부여하느냐에 달려 있습니다. 예를 들어, 특정 질병 진단에 탁월한 AI 모델은 높은 능력을 가지고 있지만, 환자에게 직접 치료법을 지시하거나 수술을 실행할 권력은 인간 의료진의 통제 하에 있어야 합니다. 자율주행차의 경우도 마찬가지입니다. 차량의 인지 및 제어 능력이 향상되더라도, 도로 환경의 복잡성과 예측 불가능성 때문에 인간 운전자의 개입 가능성을 완전히 배제하기 어렵습니다. AI 시스템이 금융 시장에서 거래를 실행하거나, 중요 인프라를 관리하거나, 법적 결정을 내릴 때도 마찬가지로 인간의 감독과 책임이 명확히 정의되어야 합니다. 이러한 맥락에서 "인간 중심 AI(Human-Centered AI)" 접근 방식은 AI 기술의 잠재력을 최대한 활용하면서도 위험을 최소화하는 중요한 방향을 제시합니다.

**AGI는 임박한 초지능을 의미하지 않습니다.**
AGI를 이정표로 간주하는 또 다른 이유는 AGI를 구축한 직후 AI 시스템이 재귀적으로 자체 개선(recursively self-improve)될 수 있다는 견해입니다. 즉, AGI가 훨씬 더 유능해지는 미래 버전의 모델을 훈련하여 "지능 폭발(intelligence explosion)"로 이어질 수 있다는 것입니다. 그 직후, 우리는 초지능 AI(superintelligent AI, 상상할 수 있는 모든 작업에서 인간의 능력을 훨씬 뛰어넘는 AI 시스템)를 얻게 될 것이며, 이는 초지능 AI가 인간의 이익과 얼마나 잘 "정렬(aligned)"되는지에 따라 유토피아(utopia) 또는 디스토피아(dystopia)로 이어질 것입니다. 일반적인 기술 관점에서는 이러한 서사를 의심할 만한 두 가지 큰 이유가 있습니다. 첫째는 AI 방법론에서 임의의 속도 향상이 가능하더라도, 우리는 혁신과 확산은 인간의 속도로 일어날 것이라고 생각한다는 점입니다. 다른 범용 기술과 마찬가지로, AI의 영향은 방법론과 능력이 향상될 때가 아니라, 그러한 개선이 애플리케이션으로 전환되고 경제의 생산적인 부문을 통해 확산될 때 구체화됩니다. [출처] 둘째, AI가 AI 연구 수행을 돕는다는 사실이 이 과정이 임의로 가속화될 수 있다는 것을 의미하지는 않습니다. AI는 오늘날 이미 AI 연구의 상당 부분을 자동화하는 데 사용되고 있습니다. 그러나 AI 방법론의 발전에 많은 병목 현상이 있습니다. 예를 들어, 특정 능력을 달성하는 데 필요할 수 있는 데이터 수집 및 실제 상호작용의 사회적 특성, 계산 및 비용 한계, 또는 진정한 돌파구를 가능하게 하는 아이디어는 무시하고 인기 있거나 직관적인 아이디어에만 몰두하는 경향 등이 있습니다. 우리는 이것에 대해 틀릴 수도 있으며, 재귀적 자체 개선(recursive self-improvement)이 가능하여 AI 방법론의 진보에 무한한 속도 향상으로 이어질 수도 있습니다. 그리고 이는 광범위한 확산이 더 느리더라도, 영향에 있어 일부 불연속성을 포함하여 몇 가지 흥미로운 함의를 가질 수 있습니다. 이러한 이유로, 재귀적 자체 개선에 대한 조기 경보 시스템(early warning systems)을 갖추는 것이 중요합니다. 그러나 이것은 AGI 정의에 포착되지 않습니다. 우리는 재귀적 자체 개선과는 거리가 멀면서도 AGI를 가질 수 있으며, 그 반대도 마찬가지입니다.

재귀적 자체 개선을 통한 초지능의 등장은 여전히 이론적인 영역에 머물러 있습니다. AI 시스템이 스스로를 개선하는 데에는 막대한 계산 자원, 방대한 양의 고품질 데이터, 그리고 근본적인 알고리즘적 돌파구가 필요합니다. 현재 AI 시스템은 특정 목적을 위해 설계된 도구이며, 스스로 새로운 목표를 설정하고, 복잡한 사회적, 윤리적 맥락을 이해하며, 인간과 같은 창의적인 방식으로 문제를 해결하는 능력은 아직 요원합니다. 또한, AI 연구는 여전히 인간의 통찰력과 지도를 필요로 합니다. 새로운 아키텍처, 학습 방법론, 그리고 데이터셋의 혁신은 대부분 인간 연구자들의 아이디어에서 비롯됩니다. 따라서 AI가 스스로 모든 연구 과정을 자동화하고 '지능 폭발'을 일으키는 시나리오는 현재로서는 매우 비현실적입니다. 오히려 AI는 인간 연구자들의 역량을 강화하고, 연구 속도를 가속화하는 '협력 도구'로서의 역할을 수행할 가능성이 더 높습니다.

**우리는 AGI가 언제 구축되었는지 알 수 없을 것입니다.**
AGI와 관련 개념에 대한 수많은 정의가 있습니다. 재스민 선(Jasmine Sun)은 20개 이상의 정의를 유용하게 정리해 놓았는데, 이들은 크게 세 가지 범주로 나뉩니다. 즉, 시스템의 세계에 대한 영향, 내부(internals), 또는 통제된 환경에서의 행동에 기반할 수 있습니다. 우리는 각 정의 방식이 치명적인 결함(fatal flaw)을 가지고 있음을 보여줄 것입니다. 이는 우리가 이상적으로 원하는 것에 비해 너무 엄격하거나 너무 약한 기준(criteria)으로 이어집니다. 이러한 간극을 이해하는 것은 사람들이 AGI가 어떻게 보일지에 대해 왜 다른 직관을 가지고 있는지, 그리고 "보면 알게 될 것"이라는 기준이 왜 실패했고 계속 실패할 것인지를 보여줍니다. OpenAI의 2018년 AGI 정의는 "대부분의 경제적으로 가치 있는 작업에서 인간을 능가하는 고도로 자율적인 시스템"이었습니다. 우리의 관점, 즉 AI의 영향에 대한 우리의 관심사에서 볼 때, 이 정의는 잠재적으로 매우 유용합니다. 만약 AI가 대부분의 경제적으로 가치 있는 작업에서 [모든] 인간을 능가한다면, 그것은 의심할 여지 없이 영향력이 클 것입니다. 그러나 분명히 말하자면, 이것은 AI 시스템의 속성이 아닙니다. 이것은 세계의 상태(state of the world)의 속성입니다. 이는 우리가 만드는 보완적인 혁신과 우리가 AI를 우리의 조직과 기관에 통합하기로 선택하는 정도와 적어도 같은 정도로 관련이 있습니다. 실험실에서 AI 시스템을 고립시켜 테스트하고 그것이 사람들의 직무에서 사람들을 능가하는지 묻는 것은 터무니없는 일일 것입니다. 그것은 범주 오류(category error)입니다. 예를 들어, AI가 (자율적으로) 의료 연구원을 능가할 수 있는지 여부는 우리가 집단적으로 AI 시스템이 사람들에게 대규모 의료 실험을 수행하도록 허용할지 여부에 부분적으로 달려 있습니다. 우리는 그렇게 해서는 안 되며 그렇게 하지 않을 것입니다. 이는 시스템의 능력과 관계없이 의료 연구원의 기능을 수행할 수 없다는 것을 의미합니다. 이것은 극단적인 예일 수 있지만, 거의 모든 직업에서 유사한 병목 현상이 발생합니다. 더 나쁜 것은, AI를 모든 곳에 확산시키지 않는 한, 우리는 그 시스템이 이론적으로 실제 세계에서 작업을 자동화할 수 있는지조차 알지 못할 것이라는 점입니다. 우리는 세상의 복잡하고 혼란스러운 모습을 충분히 설득력 있게 모의 실험(simulacra)할 수 없을 것입니다. 요컨대, 영향 기반 정의는 고통스러울 정도로 느린 확산 과정의 최종 결과를 예측할 방법을 제공하지 않기 때문에 실용적인 목적에 유용하지 않습니다. 영향에 관심이 있는 우리와 같은 연구자들과는 대조적으로, 많은 연구자들은 내부(internals)의 의미에서 인간과 같은 AI에 관심이 있습니다. 즉, 시스템이 세상을 인과적으로(causally) 진정으로 이해하는지, 우리처럼 추론하고, 계획하고, 새로운 기술을 습득할 수 있는지 등입니다. 이러한 AGI의 의미는 AI 내부를 관찰하고 특성화하는 어려움 때문에 운영화(operationalize)하기가 매우 어려웠습니다. 튜링 테스트(Turing test)는 우리가 중요하게 생각하는 인간과 같은 속성을 행동의 대리 지표로 사용하려는 많은 시도 중 가장 잘 알려진 것이지만, 필연적으로 우리는 기대했던 인간과 같은 내부를 가지지 않고도 그러한 테스트를 통과하는 AI 시스템을 구축할 수 있다는 것이 밝혀졌습니다. 더욱이, AI의 들쭉날쭉함(jaggedness) 때문에 — 여러 면에서 초인적이지만, 다른 면에서는 유아의 세상 이해력이 부족한 — AI의 변혁적인 효과는 모든 면에서 완전히 인간과 같아지기(또는 초인적이 되기) 훨씬 전에 느껴질 가능성이 높습니다. 요컨대, 우리는 내부 자체에 관심이 없으므로 이러한 종류의 정의는 제쳐둡니다. 이는 우리에게 세 번째 종류의 정의를 남기는데, 이는 단연코 가장 일반적인 것으로, 행동에 기반하며 벤치마크 성능(benchmark performance)으로 운영화됩니다. 예를 들어, "인간-기계 지능 동등성(human-machine intelligence parity)"에 대한 메타큘러스(Metaculus) 질문은 수학, 물리학, 컴퓨터 과학 시험 문제의 성능으로 정의됩니다. 이러한 종류의 정의의 문제는 잘 알려져 있으며, 우리는 이를 반복적으로 논의했습니다. 그것들은 단순히 실제 세계에서 반드시 유용하지 않더라도 벤치마크를 이길 수 있는 AI 시스템을 구축하는 의미에서 언덕 오르기(hill climbing)를 장려할 뿐입니다.

**세 가지 AGI 정의 방식의 장단점**
정의의 난제에 대한 한 가지 반응은 AGI를 보면 알게 될 것이라고 주장하는 것입니다. 최신 모델 출시는 그 반대가 사실임을 보여줍니다. 일부 사람들에게는 능력의 발전이 AGI라고 부를 만한 단계적 변화(step change)를 나타낸다는 것이 분명합니다. 다른 사람들에게는 개선이 기껏해야 미미하며, 실제 세계에 영향을 미칠 가능성이 낮습니다. 사람들의 다른 직관은 무엇으로 설명될까요? 우리의 추측은 다음과 같습니다. AI 능력은 일반적일 수 있지만, AI를 실제 세계에서 유용하게 만드는 것은 대체로 도메인별(domain-specific) 방식으로 이루어져야 할 것입니다. (최신 에이전트 모델의 범용 에이전트(generalist agent) 측면은 오해의 소지가 있습니다. 오류 비용이 낮은 생성 작업(generative tasks)만 처리할 수 있으며, 실제 세계에서 독립적으로 작동해야 할 때는 그렇지 않습니다. 예를 들어, 인간에게는 사소해 보이는 작업임에도 불구하고 유용한 여행 예약 AI 에이전트는 아직 출시되지 않았습니다.) 따라서 사람들이 최신 에이전트 모델 또는 다른 어떤 시스템이 AGI(에 가깝)인지 생각할 때, 그들은 직관적으로 다른 도메인에 대해 생각하고 있으며, 일반적인 능력과 유용한 실제 세계 능력 사이의 간극은 도메인마다 크게 다릅니다. 대부분의 작업을 자동화할 수 있는 유용한 제품을 갖추는 임계값(threshold)을 넘어서는 것은 다른 부문이나 직업에서 매우 다른 시기에 발생할 수 있습니다.

AGI 정의의 문제는 AI 평가 방식에도 영향을 미칩니다. 벤치마크는 연구 진도를 측정하는 데 유용하지만, 실제 세계의 복잡성을 완전히 반영하지 못합니다. 예를 들어, 특정 코딩 벤치마크에서 높은 점수를 받은 AI가 실제 소프트웨어 개발 환경에서 팀워크, 사용자 요구사항 이해, 장기적인 코드 유지보수와 같은 측면에서는 여전히 한계를 보일 수 있습니다. 또한, AI 시스템의 윤리적 사용, 투명성, 설명 가능성(explainability)과 같은 비기술적 요소들은 벤치마크에서 측정하기 어렵지만, 실제 세계에서의 유용성과 수용성에 결정적인 영향을 미칩니다. 이러한 "들쭉날쭉한" 특성 때문에, AGI를 단일한 기술적 이정표로 간주하는 것은 AI의 실제적인 영향과 발전 방향을 이해하는 데 방해가 될 수 있습니다.

**기업과 정책 입안자들은 장기적인 관점을 가져야 합니다.**
AGI는 실행 가능하지 않으므로 이정표가 아닙니다. 어떤 회사가 AGI를 달성했거나 달성할 예정이라고 선언하는 것은 기업이 어떻게 계획해야 하는지, 어떤 안전 개입(safety interventions)이 필요한지, 또는 정책 입안자들이 어떻게 반응해야 하는지에 대해 아무런 영향을 미치지 않습니다. 그렇다면 기업과 정책 입안자들은 대신 무엇을 해야 할까요? 기업들은 미완성된 AI 제품을 서둘러 채택해서는 안 됩니다. AI 방법론과 능력의 빠른 발전이 자동으로 더 나은 제품으로 이어지는 것은 아닙니다. 본질적으로 확률적(stochastic)인 모델 위에 제품을 구축하는 것은 어렵습니다. 기업들은 핵심 비즈니스 프로세스(business processes)를 자동화하기 위해 AI를 사용하는 영향을 판단하기 위해 신중한 실험을 수행하면서 AI 제품을 조심스럽게 채택해야 합니다. 특히, 우리는 AI 에이전트가 인간 노동자의 "즉시 대체품(drop-in replacements)"이 되어 워크플로우(workflows)에 자동화를 신중하게 평가하고 통합할 필요성을 어떻게든 우회할 것이라는 생각에 대해 극도로 회의적입니다. AI 제품을 개발하는 기업들은 채택의 장애물(hurdles)을 식별하고 AI를 채택하는 기업들이 원하는 것을 구축하기 위해 해당 도메인에 대한 깊은 이해가 필요합니다. 예를 들어, Cursor와 Windsurf와 같은 AI 기반 코드 편집기(code editors)의 한 가지 핵심 혁신은 프로그래머가 AI가 생성한 텍스트를 다양한 추상화 수준(levels of abstraction)에서 검증할 수 있도록 하는 사용자 인터페이스(user interface)입니다. 다른 산업들은 제품이 해결해야 할 AI 채택에 대한 다른 장애물을 가질 것입니다. 정책 입안자들도 장기적인 관점을 가져야 합니다. "AGI를 위한 맨해튼 프로젝트"는 여러 면에서 잘못된 생각입니다. AGI는 이정표가 아니므로, 목표가 언제 달성되었는지 또는 얼마나 더 투자해야 하는지 알 방법이 없습니다. 그리고 AI 능력을 가속화하는 것은 경제적 이점을 실현하는 데 있어 실제 병목 현상을 해결하는 데 아무런 도움이 되지 않습니다. 이러한 관점은 수출 통제(export controls)에도 영향을 미칩니다. 미국은 중국의 AI 개발을 늦추기 위해 AI 개발에 필요한 하드웨어에 대한 수출 통제를 적용했습니다. 수출 통제 지지자들은 이것이 미국과 중국 간의 격차를 몇 달 이상 벌리지 않을 것이라고 인정합니다(우리도 동의합니다). 그러나 이것은 첨단 AI 개발의 영향이 급격한 세계에서만 중요합니다. 만약 첨단 AI의 영향이 확산을 통해 실현되고, 확산 과정이 수십 년이 걸린다면, 수십 년의 게임에서 몇 달 앞서는 것은 거의 중요하지 않습니다. 따라서 정책 입안자들은 확산을 가능하게 하는 데 집중해야 합니다. 우리는 최근 에세이에서 그렇게 하는 방법에 대한 몇 가지 아이디어를 제시했습니다. AGI를 변혁적인 AI 개발의 이정표로 취급하는 것은 매력적이지만 잘못된 생각입니다. 이는 AI 발전과 위험, 경제적 영향, 지정학에 대한 잘못된 정신 모델을 부추깁니다. AI가 세계에 미치는 영향은 마법의 총알 기술(magic-bullet technology)을 향한 단거리 경주를 통해서가 아니라, 수백만 가지의 지루하고 작은 비즈니스 프로세스(business process) 적응과 정책 조정(policy tweaks)을 통해 실현될 것입니다. 이 에세이 초안에 대한 피드백을 주신 스티브 뉴먼(Steve Newman)과 재스민 선(Jasmine Sun)에게 감사드립니다.

기업들은 AI 기술을 도입할 때 단기적인 이익이나 과대광고에 현혹되기보다는, 장기적인 관점에서 AI 전략을 수립해야 합니다. 이는 AI 윤리 가이드라인을 내부적으로 구축하고, 직원들의 AI 리터러시(literacy) 교육에 투자하며, AI 프로젝트의 실제적인 ROI(투자 수익률)를 측정하는 데 집중하는 것을 의미합니다. 또한, AI 기술이 인간의 업무를 보완하고 확장하는 방식으로 통합될 수 있도록 '인간 중심 디자인(human-centered design)' 원칙을 적용해야 합니다. 정책 입안자들은 AGI라는 추상적인 목표에 집중하기보다는, AI 기술의 책임감 있는 개발, 배포 및 확산을 위한 구체적인 정책 프레임워크를 마련해야 합니다. 여기에는 AI 연구 개발에 대한 공공 투자, 데이터 인프라 및 컴퓨팅 자원 접근성 확대, AI 인재 양성 프로그램 지원, 그리고 AI의 잠재적 위험을 관리하기 위한 유연한 규제 샌드박스(regulatory sandbox) 도입 등이 포함됩니다.

**결론: 실용적인 접근과 지속적인 진화**
AGI에 대한 논의는 AI의 미래에 대한 중요한 질문들을 제기하지만, 우리는 AGI를 단일하고 결정적인 이정표로 보는 관점에서 벗어나야 합니다. 대신, AI는 지속적으로 발전하고 확산되는 기술이며, 그 영향은 기술 자체의 능력뿐만 아니라 사회적, 경제적, 정치적 맥락에 따라 결정됩니다. 기업과 정책 입안자들은 과대광고나 단기적인 예측에 흔들리지 않고, AI의 실용적이고 책임감 있는 통합에 집중함으로써 장기적인 가치를 창출해야 합니다. AI의 진정한 변혁은 단일한 "AGI 순간"이 아니라, 수많은 작고 점진적인 혁신과 사회적 적응을 통해 이루어질 것입니다.

**추가 자료 읽기**
재스민 선(Jasmine Sun)의 최근 에세이는 정신과 내용 면에서 우리의 에세이와 유사합니다. 그녀는 많은 인기 있는 AGI 정의를 대조하고, AGI가 구체적인 기술적 이정표(technical milestone)라기보다는 AI 커뮤니티의 열망(aspiration)으로 보는 것이 더 낫다고 결론 내립니다. 그녀는 또한 AGI 및 기타 관련 개념에 대한 인기 있는 정의들을 정리했습니다. 보르한 빌리-하멜린(Borhane Blili-Hamelin)과 다른 이들은 우리가 AGI를 AI 연구의 북극성(North star)으로 취급하는 것을 멈춰야 한다고 주장합니다. 에게 에르딜(Ege Erdil)은 변혁적인 AI 능력에 도달하는 데 수십 년의 시간표(timelines)가 필요하다는 주장을 펼칩니다. 에릭 살바지오(Eryk Salvaggio)는 정책 입안자들이 AGI가 임박한 것처럼 행동하는 것이 위험한 많은 이유를 제시합니다. "AI as Normal Technology"에서 우리는 AI의 미래에 대한 우리의 견해에 대한 지적 기반(intellectual foundation)을 마련했습니다. 이 에세이는 그 안에 담긴 아이디어의 함의를 탐구하는 많은 후속작 중 첫 번째입니다.

---

<sup>1</sup> LLM은 텍스트만 출력할 수 있지만, 최신 에이전트 모델과 같은 모델은 텍스트 출력을 사용하여 도구와 상호작용하도록 훈련됩니다. 예를 들어, 특정 키워드(예: "use-search")를 출력하여 주어진 쿼리(query)를 온라인에서 검색할 수 있습니다. 분명히 말하자면, 이전 LLM도 도구와 상호작용할 수 있었습니다. 예를 들어, OpenAI의 ChatGPT 플러그인(plugins)은 2년 전 GPT-4와 같은 모델이 도구를 사용하도록 허용했습니다. 최신 에이전트 모델의 핵심 차이점은 도구를 효과적으로 사용하도록 훈련되었다는 것입니다. 이전 모델은 이 작업을 위해 명시적으로 훈련되지 않은 상태에서 프롬프트(prompts)를 기반으로 도구를 사용했거나, 기껏해야 도구 사용 구문(syntax)을 배웠습니다. 최신 에이전트 모델은 검색, 코드 실행 능력, 파일 접근, 이미지 생성 및 추론을 포함하여 다양한 도구에 접근할 수 있습니다. 전반적으로 최신 에이전트 모델은 챗봇(chatbot)이라기보다는 "에이전트(agent)"에 훨씬 더 가깝습니다.

<sup>2</sup> 예를 들어, 이 시스템이 훈련을 마친 후에야 출시된 소프트웨어 라이브러리(software library)를 사용하여 새로운 유형의 코딩 작업을 수행하는 데 사용된다고 가정해 봅시다. 충분한 문서와 지침이 주어진다면, 결국 작업을 올바르게 수행할 수 있을 것입니다. 그러나 재훈련을 통한 모델의 명시적인 업데이트가 없는 한, 이 "경험"은 모델의 가중치(weights)를 변경하지 않을 것이므로, 동일한 작업을 수행해야 하는 다른 사용자는 첫 번째 사용자가 제공했던 것과 동일한 도움을 주어야 할 것입니다.

<sup>3</sup> 우리는 이 글에서 군사 AI(military AI)를 분석하지 않습니다. 이는 중요한 주제이며, 향후 에세이에서 다시 다룰 것입니다.

<sup>4</sup> 새로운 기술의 발명(invention)과 그 기술이 사회 전반에 확산되도록 돕는 혁신(innovations)을 구별하는 것이 중요합니다. 이 단락에서의 논의는 전자에 관한 것입니다. 발명은 새로운 모델의 개발, 아마도 새로운 능력의 동반을 의미합니다. 혁신은 이러한 능력이 생산적으로 사용될 수 있도록 합니다. 더 좋고 유능한 모델은 발명의 예이며, 이러한 모델을 사용하는 제품의 개발은 혁신의 예입니다. 우리는 국가들이 발명에서 길고 지속적인 우위를 유지할 가능성이 높다고 생각하지 않습니다. 그러나 이 섹션에서 나중에 논의하듯이, 발명의 생산적인 사용을 가능하게 하는 조건은 국가마다 크게 다를 수 있습니다.

<sup>5</sup> 아모데이(Amodei)는 AGI를 피하고 자신의 예측에 대해 더 정확하게 표현하기 위해 "강력한 AI(powerful AI)"라는 용어를 사용합니다.