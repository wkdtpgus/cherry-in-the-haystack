## 최신 AI 모델의 발전과 AGI 담론: 이정표가 아닌 과정으로서의 인공 일반 지능

최근 AI 모델의 발전과 함께, 인공 일반 지능(Artificial General Intelligence, AGI)이 이미 달성되었는지에 대한 논쟁이 다시 활발해지고 있습니다. 이 논의에서 흔히 제기되는 회의론자들의 의견은 AGI의 명확한 정의가 없다는 것입니다. 이는 사실이지만, 본질적인 논점을 간과하고 있습니다. 만약 AGI가 인류에게 그렇게 중대한 이정표라면, 그것이 실현되었을 때 누구나 명백히 인지할 수 있어야 하지 않을까요? 본 글에서 우리는 AGI가 단일한 이정표가 아니라고 주장합니다. AGI는 AI 시스템의 속성이나 사회적 영향 측면에서 급격한 불연속성을 나타내지 않습니다. 어떤 정의를 채택하든, 특정 기업이 AGI를 구축했다고 선언하는 것은 즉각적인 실행 가능한 사건이 아닙니다. 이는 기업, 개발자, 정책 입안자 또는 안전 문제에 단기적으로는 아무런 영향을 미치지 않을 것입니다.

구체적으로 말하자면, 범용 AI 시스템이 어떤 합의된 능력 임계값(capability threshold)에 도달하더라도, AI가 산업 전반에 확산되어 생산적인 영향을 실현하려면 많은 보완적인 혁신이 필요할 것입니다. 확산은 기술 개발 속도가 아니라 인간(및 사회)의 시간 척도(timescales)에 따라 발생합니다. AGI와 치명적인 위험에 대한 우려는 종종 능력(capabilities)과 권력(power)을 혼동합니다. 이 둘을 명확히 구별한다면, AI 개발에 있어 인류가 통제력을 유지하기 불가능해지는 임계점(critical point)이라는 개념을 거부할 수 있습니다. AGI 정의의 확산은 질병이 아니라 증상입니다. AGI는 예상되는 영향 때문에 중요하지만, AI 시스템 자체의 속성을 기반으로 정의되어야 합니다. 그러나 시스템 속성과 영향 사이의 연결 고리는 미약하며, AI 시스템이 작동하는 환경을 어떻게 설계하느냐에 크게 좌우됩니다. 따라서 특정 AI 시스템이 변혁적인 영향을 미칠지 여부는 시스템이 출시되는 시점에는 아직 결정되지 않습니다. 그러므로 AI 시스템이 AGI를 구성한다는 결정은 오직 회고적으로(retrospectively)만 의미 있게 이루어질 수 있습니다.

### AGI에 대한 반대 비유로서의 핵무기

AGI 달성은 OpenAI와 같은 기업들, 그리고 AI 연구 커뮤니티의 많은 부분의 명시적인 목표입니다. 이는 과거 맨해튼 프로젝트(Manhattan Project)의 핵심 목표가 핵무기를 만들고 전달하는 것이었던 것처럼, 중대한 이정표로 간주됩니다. 맨해튼 프로젝트의 목표가 이정표로서 의미가 있었던 것은 두 가지 이유 때문입니다. 첫째는 관측 가능성(observability)입니다. 핵무기 개발 시, 목표 달성 여부에 대한 의심의 여지가 없었습니다. 폭발은 관측 가능성의 전형적인 예시입니다. 둘째는 즉각적인 영향(immediate impact)입니다. 핵무기 사용은 제2차 세계 대전의 빠른 종식에 기여했으며, 새로운 세계 질서와 지정학의 장기적인 변혁을 가져왔습니다.

많은 사람들은 AGI도 이러한 속성을 가질 것이라고 직관적으로 생각합니다. AGI는 너무나 강력하고 인간과 유사하여 우리가 그것을 만들었을 때 명백히 드러날 것이라고 여겨집니다. 그리고 AGI는 즉시 막대한 이점과 위험을 가져올 것으로 예상됩니다. 경제의 상당 부분을 자동화하고, AI 연구 자체를 포함한 혁신을 크게 가속하며, 통제 불가능한 초지능(superintelligence)으로 인해 인류에게 잠재적으로 치명적인 결과를 초래할 수 있다는 주장도 있습니다. 그러나 이 에세이에서 우리는 AGI가 정반대일 것이라고 주장합니다. 즉, 특별한 의미를 지닌 명확한 능력 임계값(capability threshold)이 없기 때문에 관측 불가능하며, 세계에 즉각적인 영향을 미치지 않을 것이고, 심지어 경제의 장기적인 변혁조차 불확실하다는 것입니다. 우리는 이전 에세이에서 AGI를 핵무기에 비유하며, 일부 사람들이 권장하는 재앙적인 정책 개입에 반대하는 주장을 펼쳤습니다. 이러한 비유가 잘못된 예측과 역효과를 낳는 권고를 일관되게 만들어낸다는 점은 여전히 놀랍습니다.

### 최신 AI 모델이 AGI라고 생각하는 것이 터무니없는 것은 아니지만, 이는 모델 자체보다 AGI에 대해 더 많은 것을 말해줍니다.

많은 저명한 AI 평론가들은 최근 등장한 강력한 AI 모델들을 일종의 AGI라고 불렀습니다. 타일러 코웬(Tyler Cowen)은 AGI를 보면 알 수 있다면 자신은 그것을 보았다고 말합니다. 이든 몰릭(Ethan Mollick)은 이러한 모델들을 들쭉날쭉한 AGI(jagged AGI)라고 묘사합니다. 이러한 최신 모델들의 무엇이 이러한 흥분을 불러일으켰을까요? 핵심 혁신 중 하나는 웹을 검색하고 도구를 추론 사슬(reasoning chain)의 일부로 활용하는 방법을 학습하기 위해 강화 학습(reinforcement learning)과 더 발전된 에이전트 프레임워크를 사용한다는 것입니다. <sup>1</sup> 이러한 방식으로, 최신 모델들은 대규모 언어 모델(LLM)이 직접 수행할 수 있는 것보다 더 복잡한 인지 작업(cognitive tasks)을 수행할 수 있으며, 인간과 유사한 방식으로 그렇게 할 수 있습니다.

예를 들어, 여러 제품을 비교하고 리뷰를 통해 중요한 기능을 파악한 다음, 그 지식을 바탕으로 고려 중인 제품 목록을 반복적으로 조정하는 비교 쇼핑객을 상상해 보십시오. 최신 AI 에이전트들은 이러한 종류의 작업을 꽤 잘 수행하는 범용 에이전트(generalist agent)입니다. 이것이 AGI에 무엇을 의미하는지 생각해 봅시다. 특정 모델의 세부 사항에 얽매이지 않기 위해, 현재 모델과 아키텍처(architecture)는 동일하지만 훨씬 더 유능한 미래 시스템을 상상해 봅시다. 예를 들어, 온라인 상태인 한, 아무리 찾기 어려운 정보라도 항상 작업에 적합한 웹페이지와 지식을 찾을 수 있습니다. 필요한 경우 인터넷에서 코드를 다운로드하여 실행하여 작업을 해결할 수 있습니다. 이 중 어느 것도 과학적 돌파구를 필요로 하지 않으며, 단지 공학적 개선과 추가 훈련만 필요합니다. 동시에, 과학적 개선 없이는 아키텍처가 심각한 한계를 부과합니다. 예를 들어, 이 미래 시스템은 훈련에 대한 명시적인 업데이트를 통해서만 새로운 기술을 경험으로부터 습득할 수 있습니다. 즉석에서 학습할 수 있는 AI 시스템을 구축하는 것은 미해결 연구 문제(open research problem)입니다. <sup>2</sup>

우리의 가상 시스템은 AGI일까요? 논쟁의 여지는 있지만, 그렇습니다. 많은 AGI 정의들이 공통적으로 가지고 있는 것은 다양한 작업에서 인간을 능가하는 능력입니다. 작업 세트가 얼마나 좁게 정의되고 각 작업에 대한 관련 인간 세트가 얼마나 넓게 정의되는지에 따라, 이러한 미래의 에이전트 모델이 AGI 정의 중 일부를 충족할 가능성은 상당히 높습니다. 예를 들어, 대규모 언어 모델(large language models) 자체는 체스에 기껏해야 평범한 수준임에도 불구하고, 이 시스템은 체스에서 초인적인 능력을 발휘할 것입니다. 모델이 도구를 사용하고, 인터넷을 검색하며, 코드를 다운로드하고 실행할 수 있다는 점을 기억하십시오. 만약 작업이 체스를 두는 것이라면, 체스 엔진을 다운로드하여 실행할 것입니다. 많은 작업에서 인간 수준 또는 초인적인 성능을 보이고, 일부 AGI 정의를 그럴듯하게 충족함에도 불구하고, 많은 실제 작업에서는 아마도 심하게 실패할 것입니다. 그 이유에 대해서는 나중에 다시 설명하겠습니다.

이 모든 것이 중요할까요? 중요합니다. AI 기업의 리더들은 몇 년 안에 AGI를 제공하겠다는 매우 큰 예측과 약속을 했습니다. 그들이 가까운 미래의 어떤 시스템을 AGI라고 선언할 엄청난 유인이 있으며, 그렇게 하지 않을 경우 잠재적으로 막대한 비용이 발생할 수 있습니다. 아마도 AI 기업의 가치 평가 중 일부는 이러한 약속에 기반하고 있을 것이므로, AGI가 없다면 거품이 터질 수도 있습니다. AI 개발의 리더로 인식되는 것은 시장 점유율과 수익을 개선하고, 인재 접근성을 높이는 데 도움이 될 수 있습니다. 그렇다면, 기업들이 AGI를 구축했다고 주장할 경우 어떤 결과가 따를까요? 이 에세이의 나머지 부분에서 이를 분석할 것입니다. 저희 "AI Snake Oil" 블로그는 AI 과대광고를 폭로하고 새로운 개발에 대한 증거 기반 분석을 제공합니다.

### AGI는 확산에 수십 년이 걸리므로 경제에 충격을 주지 않을 것입니다.

AGI를 이정표로 취급하고 AGI 선언을 진지하게 받아들이는 한 가지 주장은 AGI가 희소성 없는 세상, 돈이라는 개념의 종말, 또는 갑작스러운 대량 실업과 같은 긍정적 및 부정적 측면 모두에서 급격한 경제적 영향을 초래할 수 있다는 것입니다. 그러나 AI의 경제적 영향은 경제 전반에 걸쳐 채택될 때만 실현됩니다. 기술 발전은 이러한 영향을 실현하는 데 필요하지만 충분하지는 않습니다. 전기, 컴퓨팅, 인터넷과 같은 과거의 범용 기술(general-purpose technologies)의 경우, 근본적인 기술 발전이 사회 전반에 확산되는 데 수십 년이 걸렸습니다. 산업 혁명(Industrial Revolution)의 기적은 높은 성장률(연평균 성장률이 3% 미만)이 아니라 수십 년간 지속된 성장 기간이었습니다.

AI 확산에는 많은 병목 현상(bottlenecks)이 있습니다. 유용한 제품 및 애플리케이션 개발, 이러한 제품을 활용할 인력 훈련, AI 사용을 가능하게 하는 조직 변화 구현, 기업의 AI 채택을 촉진하는 법률 및 규범 확립 등이 그것입니다. 과거의 범용 기술과 마찬가지로, 우리는 AI의 경제적 영향이 이러한 확산 과정이 전개됨에 따라 수십 년에 걸쳐 실현될 것으로 예상합니다. "AI as Normal Technology"라는 논문에서 우리는 왜 이러한 상황이 될 것이라고 생각하는지에 대한 자세한 주장을 제시합니다. 능력의 급격한 증가가 급격한 경제적 영향으로 이어진다는 생각은 AI의 과거 및 현재와 완전히 일치하지 않으며, 미래에 이것이 바뀔 것이라고 예상할 이유도 없습니다.

AGI의 한 가지 정의는 대부분의 경제적으로 가치 있는 작업에서 인간을 능가하는 AI 시스템입니다. 만약 AGI가 이러한 의미에서 실현된다면, 대규모의 갑작스러운 일자리 대체로 이어질 수 있다고 우려할 수 있습니다. 그러나 인간은 움직이는 목표물(moving target)입니다. 확산 과정이 전개되고 자동화된 작업의 생산 비용(따라서 가치)이 감소함에 따라, 인간은 적응하여 아직 자동화되지 않은 작업으로 이동할 것입니다. 기술 발전, 제품 개발 및 확산 과정은 계속될 것입니다. 최근 AI가 직업을 어떻게 변화시킬지에 대한 논의는 단순히 '자동화(automation)'를 넘어 '증강(augmentation)'의 중요성을 강조하고 있습니다. AI는 인간의 업무를 완전히 대체하기보다는 보조하고 효율성을 높이는 방향으로 먼저 확산될 가능성이 높으며, 이는 인간이 새로운 역할과 기술을 습득할 시간을 제공할 것입니다.

### AGI는 세계 질서의 급격한 변화로 이어지지 않을 것입니다.

미국과 중국은 종종 AI 군비 경쟁(AI arms race)을 벌이고 있으며, 각국이 AGI를 구축하기 위해 경쟁하고 있다고 묘사됩니다. AGI를 먼저 구축하는 국가가 결정적인 전략적 우위(strategic advantage)를 가지게 되어, 예측 가능한 미래에 세계 질서에서 지배력을 확보할 것이라는 가설이 있습니다. <sup>3</sup> 이러한 서사는 말이 되지 않습니다. AI 모델을 만드는 데 필요한 지식과 모델 능력 자체는 국가 간에 빠르게 확산되는 경향이 있기 때문입니다. 수십만 명의 AI 기술자(technologists)가 있으며, 그들은 정부 연구소보다는 민간 부문에서 일하므로, 그러한 규모에서 비밀을 유지하는 것은 실현 불가능합니다. 발명, 즉 이 경우 AI 모델 개발은 경쟁 우위(competitive advantage)의 원천으로서 과대평가되어 있습니다. 우리는 기술 발전이 국가 간에 대략적으로 보조를 맞출 것으로 예상해야 합니다. 비록 미국 기업들이 현재 선두에 있지만, 우리는 지속적인 우위를 기대해서는 안 됩니다. <sup>4</sup>

많은 사람들은 기술 능력의 확산 용이성을 제대로 인식하지 못했으며(아마도 핵무기 정신 모델(mental model) 때문일 것입니다), 이에 놀라움을 금치 못했습니다. 이것이 최근 몇 년간 "딥시크 모멘트(DeepSeek moment)"와 같은 현상으로 이어진 이유입니다. 분석가들은 AI 능력이 얼마나 빨리 확산될 수 있는지 깨닫지 못했고, 그 결과 신생 기업들, 특히 중국 기업들이 선도적인 모델에 필적하는 성능을 그렇게 빨리 따라잡을 것이라고 예상하지 못했습니다. 실제로 딥시크(DeepSeek)와 같은 오픈소스 모델들은 빠르게 발전하며 상업용 모델과의 격차를 좁히고 있어, 기술 선점의 중요성이 더욱 희석되고 있습니다.

일부 사람들은 몇 달의 우위조차 중요할 것이라고 주장합니다. 우리는 동의하지 않습니다. 강대국 경쟁의 맥락에서 중요한 질문은 어느 나라가 AGI를 먼저 구축하느냐가 아니라, 어느 나라가 AI 기술의 생산적인 확산을 더 잘 가능하게 하느냐입니다. 제프리 딩(Jeffrey Ding)이 보여주었듯이, 국내외 AI 발명과 혁신을 실제로 활용하여 생산성을 향상시키는 기업과 정부의 효율성은 범용 기술의 경제적 영향을 결정하는 데 훨씬 더 중요합니다. 중국 AI 기업들은 AI 모델과 능력 면에서 선도적인 미국 기업들보다 기껏해야 6-12개월 뒤처져 있지만, 중국은 확산을 가능하게 할 수 있는 몇 가지 핵심 지표, 즉 디지털화(Digitization), 클라우드 컴퓨팅(cloud computing) 채택, 인력 훈련 면에서 미국에 크게 뒤처져 있습니다. 이 모든 것은 산업 전반에 걸쳐 AI 발전의 생산적인 확산을 가능하게 하는 데 필요합니다. 이것이 미국의 실제 경쟁 우위의 원천입니다.

물론, 이것은 향후 몇 년 안에 바뀔 수 있습니다. 그러나 그렇게 된다면, 그것은 AGI 개발보다는 확산을 촉진하기 위한 정책 변화의 결과일 것입니다. 그리고 정책을 얼마나 빨리 바꾸든, 국가들이 하룻밤 사이에 이룰 수 있는 일이 아닙니다. 확산은 일반적으로 수십 년에 걸쳐 전개됩니다. 이 모든 것이 정책 입안자들이 안주해야 한다는 의미는 아닙니다. 그러나 이는 AGI에 집착하기보다는 기존 AI를 포함하여 생산적이고 안전한 확산을 가능하게 하는 데 집중해야 한다는 것을 의미합니다.

### AGI의 장기적인 경제적 영향은 불확실합니다.

즉각적인 경제적 영향이 없더라도, AGI가 예를 들어 수십 년에 걸쳐 큰 성과를 낼 수 있는 연간 10%의 GDP 성장을 가능하게 할 수 있을까요? 아마도 그럴 수 있습니다. 그러나 이것이 왜, 어떻게 일어날지는 전혀 명확하지 않습니다. 역사적으로 이러한 종류의 성장 가속화는 매우 드물게 발생했습니다. 산업 혁명(industrial revolution)은 이러한 효과를 가져왔지만, GDP에 거의 영향을 미치지 않은 인터넷은 그렇지 않았습니다. GDP가 측정하기에 적절한 것이 아니라고 생각하더라도, GDP 성장률의 질적 변화는 여러분이 중요하게 생각하는 경제의 근본적인 변화에 대한 좋은 대리 지표(proxy)입니다.

문제는 성장을 가속화하려면 진보의 병목 현상(bottlenecks)을 제거해야 한다는 것입니다. 이는 대부분의 AI 지지자들이 가정하는 것보다 훨씬 어렵습니다. AI는 부문별로 불균등한 영향을 미칠 가능성이 높으며, 장기적인 성장은 가장 약한 부문에 의해 병목 현상이 발생할 것입니다. 극적인 효과를 주장하는 사람들은 종종 병목 현상이 실제로 무엇인지에 대한 잘못된 정신 모델(mental model)을 가지고 있습니다. 예를 들어, 저렴한 과학 혁신이 진보를 가능하게 할 것이라고 믿고 싶겠지만, 새로운 발견의 생산은 실제로는 과학의 병목 현상이 아닙니다. 더 넓게 보면, 진보는 기술뿐만 아니라 올바른 전제 조건, 즉 보완적인 혁신(complementary innovations)과 문화적, 경제적, 정치적 요인에 달려 있습니다. 산업 혁명을 일으키는 데 필요한 것이 증기 기관(steam power)의 발명뿐이었다면, 로마 제국(Roman Empire)이 그것을 해냈을 것입니다. 우리의 현재 법률, 규범, 제도 및 정치는 기술적 잠재력이 훨씬 적었던 시기에 발전했습니다. 그것들은 이미 더 많은 공공 인프라(public infrastructure)를 구축하는 것과 같은 직접적인 유형의 성장을 위한 기회를 막고 있습니다. 광범위한 인지 자동화(cognitive automation)가 잠재적으로 가져올 수 있는 경제적 이점을 얻으려면, 발생해야 할 구조적 변화의 정도는 헤아릴 수 없을 정도로 더 큽니다. 결론적으로, AGI로 인한 장기적인 영향의 범위와 성격은 아직 지켜봐야 하며, 우리가 어떤 보완적인 조치(complementary actions)를 취하느냐에 달려 있습니다. 장기적인 영향은 AGI 자체의 속성이 아닙니다.

### AGI의 정렬 불량(misalignment) 위험은 권력과 능력을 혼동합니다.

반면에, AGI는 AI의 사회적 위험에 대한 전환점이 될 수 있습니다. AGI가 통제 상실, 막대한 사회적 해악, 심지어 인류 멸종을 초래할 수 있을까요? AGI 위험에 대한 논의는 권력(power) — 환경을 수정할 수 있는 능력 — 과 능력(capability) — 지정된 작업을 올바르게 해결할 수 있는 역량 — 을 혼동합니다. 능력은 AI 시스템의 본질적인 속성인 반면, 권력은 AI 시스템이 작동하는 환경을 우리가 어떻게 설계하느냐의 문제입니다. 그리고 인간은 이 설계에 대한 주체성(agency)을 가지고 있습니다. 이러한 구별은 종종 간과됩니다.

다리오 아모데이(Dario Amodei)의 "강력한 AI(powerful AI)" 정의를 생각해 봅시다. <sup>5</sup> 그는 "...미해결 수학 정리 증명, 매우 훌륭한 소설 작성, 어려운 코드베이스(codebases)를 처음부터 작성"하는 것과 같은 강력한 AI의 능력에 대한 설명으로 시작합니다. 이 기준은 AI 능력의 한 예이며, 우리는 이를 AI 시스템 수준에서 논의할 수 있습니다. 그러나 그는 이어서 AI 시스템이 작동하도록 허용하는 환경의 속성을 설명하는데, 여기에는 "...인터넷에서 행동하기, 인간에게 지시를 내리거나 받기, 재료 주문하기, 실험 지시하기, 비디오 시청하기, 비디오 만들기 등"이 포함됩니다. 이것은 AI 시스템에 부여된 권력의 한 예입니다. 이는 AI 시스템이 작동하는 환경에 따라 달라지며, AI 능력이 권력으로 어떻게 전환되는지를 결정합니다. 우리는 AI 능력이 계속 증가할 것으로 예상합니다. 그러나 능력 수준과 관계없이, 우리는 AI가 도구로 남아 인간의 감독 없이 작동할 권력과 자율성(autonomy)을 부여받지 않도록 선택할 수 있습니다.

"AI as Normal Technology" 에세이에서 우리는 기업 간의 군비 경쟁, 권력 추구, 초인적인 설득, 기만적인 정렬(deceptive alignment) 등 이에 대한 모든 일반적인 반론을 다룹니다. 우리는 이 논문에서 적절한 감독 없이 AI를 배포하는 것에 반대하는 강력한 비즈니스 유인(incentives)이 있을 것이며, 이러한 유인들은 필요할 때 규제에 의해 뒷받침될 수 있고 또 뒷받침되어야 한다고 주장합니다. 이는 자율주행차(self-driving cars)부터 AI 비서(AI assistants)에 이르는 분야에서 역사적으로 그래왔습니다. 우리는 AI 능력이 우리가 임의로 AGI로 지정하는 추정된 티핑 포인트(tipping point)에 도달했다고 해서 이 추세가 갑자기 뒤바뀔 것이라고 예상하지 않습니다.

### AGI는 임박한 초지능을 의미하지 않습니다.

AGI를 이정표로 간주하는 또 다른 이유는 AGI를 구축한 직후 AI 시스템이 재귀적으로 자체 개선(recursively self-improve)될 수 있다는 견해입니다. 즉, AGI가 훨씬 더 유능해지는 미래 버전의 모델을 훈련하여 "지능 폭발(intelligence explosion)"로 이어질 수 있다는 것입니다. 그 직후, 우리는 초지능 AI(superintelligent AI, 상상할 수 있는 모든 작업에서 인간의 능력을 훨씬 뛰어넘는 AI 시스템)를 얻게 될 것이며, 이는 초지능 AI가 인간의 이익과 얼마나 잘 "정렬(aligned)"되는지에 따라 유토피아(utopia) 또는 디스토피아(dystopia)로 이어질 것입니다.

일반적인 기술 관점에서는 이러한 서사를 의심할 만한 두 가지 큰 이유가 있습니다. 첫째는 AI 방법론에서 임의의 속도 향상이 가능하더라도, 우리는 혁신과 확산은 인간의 속도로 일어날 것이라고 생각한다는 점입니다. 다른 범용 기술과 마찬가지로, AI의 영향은 방법론과 능력이 향상될 때가 아니라, 그러한 개선이 애플리케이션으로 전환되고 경제의 생산적인 부문을 통해 확산될 때 구체화됩니다. 둘째, AI가 AI 연구 수행을 돕는다는 사실이 이 과정이 임의로 가속화될 수 있다는 것을 의미하지는 않습니다. AI는 오늘날 이미 AI 연구의 상당 부분을 자동화하는 데 사용되고 있습니다. 그러나 AI 방법론의 발전에 많은 병목 현상이 있습니다. 예를 들어, 특정 능력을 달성하는 데 필요할 수 있는 데이터 수집 및 실제 상호작용의 사회적 특성, 계산 및 비용 한계, 또는 진정한 돌파구를 가능하게 하는 아이디어는 무시하고 인기 있거나 직관적인 아이디어에만 몰두하는 경향 등이 있습니다.

우리는 이것에 대해 틀릴 수도 있으며, 재귀적 자체 개선(recursive self-improvement)이 가능하여 AI 방법론의 진보에 무한한 속도 향상으로 이어질 수도 있습니다. 그리고 이는 광범위한 확산이 더 느리더라도, 영향에 있어 일부 불연속성을 포함하여 몇 가지 흥미로운 함의를 가질 수 있습니다. 이러한 이유로, 재귀적 자체 개선에 대한 조기 경보 시스템(early warning systems)을 갖추는 것이 중요합니다. 그러나 이것은 AGI 정의에 포착되지 않습니다. 우리는 재귀적 자체 개선과는 거리가 멀면서도 AGI를 가질 수 있으며, 그 반대도 마찬가지입니다.

### 우리는 AGI가 언제 구축되었는지 알 수 없을 것입니다.

AGI와 관련 개념에 대한 수많은 정의가 있습니다. 재스민 선(Jasmine Sun)은 20개 이상의 정의를 유용하게 정리해 놓았는데, 이들은 크게 세 가지 범주로 나뉩니다. 즉, 시스템의 세계에 대한 영향, 내부(internals), 또는 통제된 환경에서의 행동에 기반할 수 있습니다. 우리는 각 정의 방식이 치명적인 결함(fatal flaw)을 가지고 있음을 보여줄 것입니다. 이는 우리가 이상적으로 원하는 것에 비해 너무 엄격하거나 너무 약한 기준(criteria)으로 이어집니다. 이러한 간극을 이해하는 것은 사람들이 AGI가 어떻게 보일지에 대해 왜 다른 직관을 가지고 있는지, 그리고 "보면 알게 될 것"이라는 기준이 왜 실패했고 계속 실패할 것인지를 보여줍니다.

OpenAI의 2018년 AGI 정의는 "대부분의 경제적으로 가치 있는 작업에서 인간을 능가하는 고도로 자율적인 시스템"이었습니다. 우리의 관점, 즉 AI의 영향에 대한 우리의 관심사에서 볼 때, 이 정의는 잠재적으로 매우 유용합니다. 만약 AI가 대부분의 경제적으로 가치 있는 작업에서 인간을 능가한다면, 그것은 의심할 여지 없이 영향력이 클 것입니다. 그러나 분명히 말하자면, 이것은 AI 시스템의 속성이 아닙니다. 이것은 세계의 상태(state of the world)의 속성입니다. 이는 우리가 만드는 보완적인 혁신과 우리가 AI를 우리의 조직과 기관에 통합하기로 선택하는 정도와 적어도 같은 정도로 관련이 있습니다. 실험실에서 AI 시스템을 고립시켜 테스트하고 그것이 사람들의 직무에서 사람들을 능가하는지 묻는 것은 터무니없는 일일 것입니다. 그것은 범주 오류(category error)입니다. 예를 들어, AI가 (자율적으로) 의료 연구원을 능가할 수 있는지 여부는 우리가 집단적으로 AI 시스템이 사람들에게 대규모 의료 실험을 수행하도록 허용할지 여부에 부분적으로 달려 있습니다. 우리는 그렇게 해서는 안 되며 그렇게 하지 않을 것입니다. 이는 시스템의 능력과 관계없이 의료 연구원의 기능을 수행할 수 없다는 것을 의미합니다. 이것은 극단적인 예일 수 있지만, 거의 모든 직업에서 유사한 병목 현상이 발생합니다. 더 나쁜 것은, AI를 모든 곳에 확산시키지 않는 한, 우리는 그 시스템이 이론적으로 실제 세계에서 작업을 자동화할 수 있는지조차 알지 못할 것이라는 점입니다. 우리는 세상의 복잡하고 혼란스러운 모습을 충분히 설득력 있게 모의 실험(simulacra)할 수 없을 것입니다. 요컨대, 영향 기반 정의는 고통스러울 정도로 느린 확산 과정의 최종 결과를 예측할 방법을 제공하지 않기 때문에 실용적인 목적에 유용하지 않습니다.

영향에 관심이 있는 우리와 같은 연구자들과는 대조적으로, 많은 연구자들은 내부(internals)의 의미에서 인간과 같은 AI에 관심이 있습니다. 즉, 시스템이 세상을 인과적으로(causally) 진정으로 이해하는지, 우리처럼 추론하고, 계획하고, 새로운 기술을 습득할 수 있는지 등입니다. 이러한 AGI의 의미는 AI 내부를 관찰하고 특성화하는 어려움 때문에 운영화(operationalize)하기가 매우 어려웠습니다. 튜링 테스트(Turing test)는 우리가 중요하게 생각하는 인간과 같은 속성을 행동의 대리 지표로 사용하려는 많은 시도 중 가장 잘 알려진 것이지만, 필연적으로 우리는 기대했던 인간과 같은 내부를 가지지 않고도 그러한 테스트를 통과하는 AI 시스템을 구축할 수 있다는 것이 밝혀졌습니다. 더욱이, AI의 들쭉날쭉함(jaggedness) 때문에 — 여러 면에서 초인적이지만, 다른 면에서는 유아의 세상 이해력이 부족한 — AI의 변혁적인 효과는 모든 면에서 완전히 인간과 같아지기(또는 초인적이 되기) 훨씬 전에 느껴질 가능성이 높습니다. 요컨대, 우리는 내부 자체에 관심이 없으므로 이러한 종류의 정의는 제쳐둡니다.

이는 우리에게 세 번째 종류의 정의를 남기는데, 이는 단연코 가장 일반적인 것으로, 행동에 기반하며 벤치마크 성능(benchmark performance)으로 운영화됩니다. 예를 들어, "인간-기계 지능 동등성(human-machine intelligence parity)"에 대한 메타큘러스(Metaculus) 질문은 수학, 물리학, 컴퓨터 과학 시험 문제의 성능으로 정의됩니다. 이러한 종류의 정의의 문제는 잘 알려져 있으며, 우리는 이를 반복적으로 논의했습니다. 그것들은 단순히 실제 세계에서 반드시 유용하지 않더라도 벤치마크를 이길 수 있는 AI 시스템을 구축하는 의미에서 언덕 오르기(hill climbing)를 장려할 뿐입니다.

### 세 가지 AGI 정의 방식의 장단점

정의의 난제에 대한 한 가지 반응은 AGI를 보면 알게 될 것이라고 주장하는 것입니다. 최근 AI 모델의 등장은 그 반대가 사실임을 보여줍니다. 일부 사람들에게는 능력의 발전이 AGI라고 부를 만한 단계적 변화(step change)를 나타낸다는 것이 분명합니다. 다른 사람들에게는 개선이 기껏해야 미미하며, 실제 세계에 영향을 미칠 가능성이 낮습니다. 사람들의 다른 직관은 무엇으로 설명될까요? 우리의 추측은 다음과 같습니다. AI 능력은 일반적일 수 있지만, AI를 실제 세계에서 유용하게 만드는 것은 대체로 도메인별(domain-specific) 방식으로 이루어져야 할 것입니다.

최근의 범용 에이전트(generalist agent) 시도는 오해의 소지가 있습니다. 오류 비용이 낮은 생성 작업(generative tasks)은 처리할 수 있지만, 실제 세계에서 독립적으로 작동해야 할 때는 여전히 한계를 보입니다. 예를 들어, 인간에게는 사소해 보이는 작업임에도 불구하고, 복잡한 실시간 의사결정과 외부 시스템과의 상호작용이 필요한 유용한 여행 예약 AI 에이전트는 아직 상용화되지 않았습니다. 이는 실제 적용을 위한 높은 신뢰성과 복잡한 상호작용 관리의 어려움을 보여줍니다. 따라서 사람들이 특정 시스템이 AGI(에 가깝)인지 생각할 때, 그들은 직관적으로 다른 도메인에 대해 생각하고 있으며, 일반적인 능력과 유용한 실제 세계 능력 사이의 간극은 도메인마다 크게 다릅니다. 대부분의 작업을 자동화할 수 있는 유용한 제품을 갖추는 임계값(threshold)을 넘어서는 것은 다른 부문이나 직업에서 매우 다른 시기에 발생할 수 있습니다.

### 기업과 정책 입안자들은 장기적인 관점을 가져야 합니다.

AGI는 실행 가능하지 않으므로 이정표가 아닙니다. 어떤 회사가 AGI를 달성했거나 달성할 예정이라고 선언하는 것은 기업이 어떻게 계획해야 하는지, 어떤 안전 개입(safety interventions)이 필요한지, 또는 정책 입안자들이 어떻게 반응해야 하는지에 대해 아무런 영향을 미치지 않습니다. 그렇다면 기업과 정책 입안자들은 대신 무엇을 해야 할까요?

기업들은 미완성된 AI 제품을 서둘러 채택해서는 안 됩니다. AI 방법론과 능력의 빠른 발전이 자동으로 더 나은 제품으로 이어지는 것은 아닙니다. 본질적으로 확률적(stochastic)인 모델 위에 제품을 구축하는 것은 어렵습니다. 기업들은 핵심 비즈니스 프로세스(business processes)를 자동화하기 위해 AI를 사용하는 영향을 판단하기 위해 신중한 실험을 수행하면서 AI 제품을 조심스럽게 채택해야 합니다. 특히, 우리는 AI 에이전트가 인간 노동자의 "즉시 대체품(drop-in replacements)"이 되어 워크플로우(workflows)에 자동화를 신중하게 평가하고 통합할 필요성을 어떻게든 우회할 것이라는 생각에 대해 극도로 회의적입니다. AI 제품을 개발하는 기업들은 채택의 장애물(hurdles)을 식별하고 AI를 채택하는 기업들이 원하는 것을 구축하기 위해 해당 도메인에 대한 깊은 이해가 필요합니다. 예를 들어, 코딩 도우미(coding assistant)의 경우, AI가 생성한 텍스트를 다양한 추상화 수준(levels of abstraction)에서 검증할 수 있도록 하는 직관적인 사용자 인터페이스(user interface)가 핵심 혁신이 됩니다. 다른 산업들은 제품이 해결해야 할 AI 채택에 대한 다른 장애물을 가질 것입니다.

정책 입안자들도 장기적인 관점을 가져야 합니다. "AGI를 위한 맨해튼 프로젝트"는 여러 면에서 잘못된 생각입니다. AGI는 이정표가 아니므로, 목표가 언제 달성되었는지 또는 얼마나 더 투자해야 하는지 알 방법이 없습니다. 그리고 AI 능력을 가속화하는 것은 경제적 이점을 실현하는 데 있어 실제 병목 현상을 해결하는 데 아무런 도움이 되지 않습니다. 이러한 관점은 수출 통제(export controls)에도 영향을 미칩니다. 미국은 중국의 AI 개발을 늦추기 위해 AI 개발에 필요한 하드웨어에 대한 수출 통제를 적용했습니다. 수출 통제 지지자들은 이것이 미국과 중국 간의 격차를 몇 달 이상 벌리지 않을 것이라고 인정합니다(우리도 동의합니다). 그러나 이것은 첨단 AI 개발의 영향이 급격한 세계에서만 중요합니다. 만약 첨단 AI의 영향이 확산을 통해 실현되고, 확산 과정이 수십 년이 걸린다면, 수십 년의 게임에서 몇 달 앞서는 것은 거의 중요하지 않습니다. 따라서 정책 입안자들은 확산을 가능하게 하는 데 집중해야 합니다. 우리는 최근 에세이에서 그렇게 하는 방법에 대한 몇 가지 아이디어를 제시했습니다.

AGI를 변혁적인 AI 개발의 이정표로 취급하는 것은 매력적이지만 잘못된 생각입니다. 이는 AI 발전과 위험, 경제적 영향, 지정학에 대한 잘못된 정신 모델을 부추깁니다. AI가 세계에 미치는 영향은 마법의 총알 기술(magic-bullet technology)을 향한 단거리 경주를 통해서가 아니라, 수백만 가지의 지루하고 작은 비즈니스 프로세스(business process) 적응과 정책 조정(policy tweaks)을 통해 실현될 것입니다. 이 에세이 초안에 대한 피드백을 주신 스티브 뉴먼(Steve Newman)과 재스민 선(Jasmine Sun)께 감사드립니다.

### 추가 자료 읽기

재스민 선(Jasmine Sun)의 최근 에세이는 정신과 내용 면에서 우리의 에세이와 유사합니다. 그녀는 많은 인기 있는 AGI 정의를 대조하고, AGI가 구체적인 기술적 이정표(technical milestone)라기보다는 AI 커뮤니티의 열망(aspiration)으로 보는 것이 더 낫다고 결론 내립니다. 그녀는 또한 AGI 및 기타 관련 개념에 대한 인기 있는 정의들을 정리했습니다. 보르한 빌리-하멜린(Borhane Blili-Hamelin)과 다른 이들은 우리가 AGI를 AI 연구의 북극성(North star)으로 취급하는 것을 멈춰야 한다고 주장합니다. 에게 에르딜(Ege Erdil)은 변혁적인 AI 능력에 도달하는 데 수십 년의 시간표(timelines)가 필요하다는 주장을 펼칩니다. 에릭 살바지오(Eryk Salvaggio)는 정책 입안자들이 AGI가 임박한 것처럼 행동하는 것이 위험한 많은 이유를 제시합니다. "AI as Normal Technology"에서 우리는 AI의 미래에 대한 우리의 견해에 대한 지적 기반(intellectual foundation)을 마련했습니다. 이 에세이는 그 안에 담긴 아이디어의 함의를 탐구하는 많은 후속작 중 첫 번째입니다.

---

<sup>1</sup> LLM은 텍스트만 출력할 수 있지만, 최근의 에이전트 모델들은 텍스트 출력을 사용하여 도구와 상호작용하도록 훈련됩니다. 예를 들어, 특정 키워드를 출력하여 온라인에서 정보를 검색하거나 코드를 실행할 수 있습니다. 이전 LLM도 도구와 상호작용할 수 있었지만(예: OpenAI의 ChatGPT 플러그인), 최신 모델의 핵심 차이점은 도구를 효과적으로 사용하도록 명시적으로 훈련되었다는 점입니다. 이들은 검색, 코드 실행, 파일 접근, 이미지 생성 및 추론을 포함한 다양한 도구에 접근할 수 있으며, 전반적으로 챗봇(chatbot)이라기보다는 "에이전트(agent)"에 훨씬 더 가깝습니다.

<sup>2</sup> 예를 들어, 이 시스템이 훈련을 마친 후에야 출시된 소프트웨어 라이브러리(software library)를 사용하여 새로운 유형의 코딩 작업을 수행하는 데 사용된다고 가정해 봅시다. 충분한 문서와 지침이 주어진다면, 결국 작업을 올바르게 수행할 수 있을 것입니다. 그러나 재훈련을 통한 모델의 명시적인 업데이트가 없는 한, 이 "경험"은 모델의 가중치(weights)를 변경하지 않을 것이므로, 동일한 작업을 수행해야 하는 다른 사용자는 첫 번째 사용자가 제공했던 것과 동일한 도움을 주어야 할 것입니다.

<sup>3</sup> 우리는 이 글에서 군사 AI(military AI)를 분석하지 않습니다. 이는 중요한 주제이며, 향후 에세이에서 다시 다룰 것입니다.

<sup>4</sup> 새로운 기술의 발명(invention)과 그 기술이 사회 전반에 확산되도록 돕는 혁신(innovations)을 구별하는 것이 중요합니다. 이 단락에서의 논의는 전자에 관한 것입니다. 발명은 새로운 모델의 개발, 아마도 새로운 능력의 동반을 의미합니다. 혁신은 이러한 능력이 생산적으로 사용될 수 있도록 합니다. 더 좋고 유능한 모델은 발명의 예이며, 이러한 모델을 사용하는 제품의 개발은 혁신의 예입니다. 우리는 국가들이 발명에서 길고 지속적인 우위를 유지할 가능성이 높다고 생각하지 않습니다. 그러나 이 섹션에서 나중에 논의하듯이, 발명의 생산적인 사용을 가능하게 하는 조건은 국가마다 크게 다를 수 있습니다.

<sup>5</sup> 아모데이(Amodei)는 AGI를 피하고 자신의 예측에 대해 더 정확하게 표현하기 위해 "강력한 AI(powerful AI)"라는 용어를 사용합니다.