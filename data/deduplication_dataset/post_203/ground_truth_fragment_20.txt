데이터 과학의 미래: 효율적인 Pandas부터 AI 윤리, MLOps까지

데이터 과학자(data scientist)로서 데이터를 최대한 활용하고 복잡한 문제를 해결하기 위해서는 올바른 도구와 기술을 사용하는 것이 중요합니다. Pandas 라이브러리(library)는 데이터 조작, 분석 및 시각화를 위한 훌륭한 도구이며, 데이터 거버넌스(data governance)와 클라우드 컴퓨팅(cloud computing) 환경의 이해 또한 필수적입니다. 그러나 대규모 데이터를 효율적으로 다루는 것은 복잡성을 증가시키고 시간과 노력을 낭비하게 만들 수 있습니다.
다행히도 모든 데이터 과학 프로젝트에서 고려해야 할 몇 가지 모범 사례(best practices)가 있습니다. Pandas의 벡터화된 연산(vectorized operations)과 내장 함수(built-in functions)를 활용하는 것부터 윤리적인 AI(ethical AI) 시스템을 구축하고 MLOps(Machine Learning Operations)를 통해 모델을 효율적으로 관리하는 것까지, 이러한 모범 사례는 데이터 과학자들이 데이터를 빠르고 정확하게 분석하고 시각화하며, 복잡한 문제를 해결하는 데 도움이 될 것입니다. 이러한 모범 사례를 이해하고 적용하면 데이터 과학자들은 생산성, 정확성, 투명성 및 공정성을 높여 더 빠르고 나은 비즈니스 결정을 내릴 수 있게 될 것입니다.

**목차:**
**1. 효율적인 Pandas 코딩 기법**
1.1. 효율적인 코딩이 왜 필요한가?
1.2. 값 효과적으로 선택 및 교체하기
    1.2.1. `.iloc[]` 및 `.loc[]`를 사용하여 행 및 열 효율적으로 선택하기
    1.2.2. DataFrame에서 값 효과적으로 교체하기
    1.2.3. 값 선택 및 교체에 대한 모범 사례 요약
1.3. Pandas DataFrame을 효과적으로 반복하기
    1.3.1. `.iterrows()`를 사용하여 효과적으로 반복하기
    1.3.2. `.apply()`를 사용하여 효과적으로 반복하기
    1.3.3. 벡터화(vectorization)를 사용하여 효과적으로 반복하기
    1.3.4. DataFrame 반복에 대한 모범 사례 요약
1.4. `.groupby()`로 데이터 효과적으로 변환하기
    1.4.1. `.groupby()`와 함께 사용되는 일반적인 함수
    1.4.2. `.groupby()` 및 `.transform()`을 사용한 결측값 대체(missing value imputation)
    1.4.3. `.groupby()` 및 `.filter()`를 사용한 데이터 필터링(data filtration)

**2. 데이터 과학의 미래 모범 사례**
2.1. AI 윤리: 공정성과 투명성 확보
2.2. MLOps: 모델 배포 및 관리를 위한 핵심
    2.2.1. CI/CD 파이프라인 구축
    2.2.2. 모델 모니터링 및 재학습
    2.2.3. MLOps 모범 사례 요약
2.3. 데이터 스토리텔링: 통찰력을 전달하는 기술
    2.3.1. 시각화의 힘
    2.3.2. 내러티브 구성
    2.3.3. 효과적인 데이터 스토리텔링 요약
2.4. 클라우드 기반 데이터 플랫폼 활용
    2.4.1. 서버리스(Serverless) 아키텍처의 이점
    2.4.2. 데이터 보안 및 규정 준수
    2.4.3. 클라우드 데이터 거버넌스(Cloud Data Governance) 및 규정 준수

**3. 결론 및 모범 사례 요약**

---

Sid Balachandran의 Unsplash 사진

이 글 전체에서 우리는 세 가지 데이터셋(dataset)을 사용할 것입니다:
포커 카드 게임 데이터셋
인기 아기 이름
레스토랑 데이터셋

첫 번째 데이터셋은 아래에 표시된 포커 카드 게임 데이터셋입니다:
```python
poker_data = pd.read_csv('poker_hand.csv')
poker_data.head()
```
각 포커 라운드에서 각 플레이어는 손에 다섯 장의 카드를 가지고 있으며, 각 카드는 하트, 다이아몬드, 클럽 또는 스페이드가 될 수 있는 문양(symbol)과 1에서 13까지의 순위(rank)로 특징지어집니다. 이 데이터셋은 한 사람이 가질 수 있는 다섯 장의 카드에 대한 모든 가능한 조합으로 구성됩니다.

Sn: n번째 카드의 문양(symbol) (여기서: 1 (하트), 2 (다이아몬드), 3 (클럽), 4 (스페이드))
Rn: n번째 카드의 순위(rank) (여기서: 1 (에이스), 2–10, 11 (잭), 12 (퀸), 13 (킹))

우리가 다룰 두 번째 데이터셋은 2011년에서 2016년 사이에 신생아에게 주어진 가장 인기 있는 이름을 포함하는 인기 아기 이름 데이터셋입니다. 데이터셋은 아래에 로드되고 표시됩니다:
```python
names = pd.read_csv('Popular_Baby_Names.csv')
names.head()
```
이 데이터셋에는 다른 정보들 외에도 연도, 성별, 민족별로 미국에서 가장 인기 있는 이름이 포함되어 있습니다. 예를 들어, Chloe라는 이름은 2011년에 아시아 및 태평양 섬 주민 여성 신생아들 사이에서 인기 순위 2위를 차지했습니다.

우리가 사용할 세 번째 데이터셋은 레스토랑 데이터셋입니다. 이 데이터셋은 레스토랑에서 저녁 식사를 하는 사람들의 모음입니다. 데이터셋은 아래에 로드되고 표시됩니다:
```python
restaurant = pd.read_csv('restaurant_data.csv')
restaurant.head()
```
각 고객에 대해 총 지불 금액, 웨이터에게 남긴 팁, 요일, 시간 등 다양한 특징(characteristics)이 있습니다.

---

### 1.1. 효율적인 코딩이 왜 필요한가?

효율적인 코드(Efficient code)는 더 빠르게 실행되고 더 적은 계산 메모리(computational memory)를 사용하는 코드입니다. 이 글에서는 `time()` 함수를 사용하여 계산 시간(computational time)을 측정할 것입니다.

이 함수는 현재 시간을 측정하므로, 코드 실행 전과 후에 변수에 할당한 다음, 그 차이를 계산하여 코드의 계산 시간을 알 수 있습니다. 간단한 예시는 아래 코드에 나와 있습니다:
```python
import time

# record time before execution
start_time = time.time()

# execute operation
result = 5 + 2

# record time after execution
end_time = time.time()

print("Result calculated in {} sec".format(end_time - start_time))
```
효율적인 코드 메서드(method)를 적용하는 것이 코드 런타임(runtime)을 어떻게 개선하고 계산 시간 복잡도(computational time complexity)를 어떻게 줄이는지 몇 가지 예시를 살펴보겠습니다:

0부터 100만까지 각 숫자의 제곱을 계산할 것입니다. 먼저, 이 연산을 실행하기 위해 리스트 컴프리헨션(list comprehension)을 사용하고, 그 다음 for 루프(loop)를 사용하여 동일한 절차를 반복할 것입니다.

먼저, 리스트 컴프리헨션을 사용하여:

```python
#using List comprehension
list_comp_start_time = time.time()
result = [i*i for i in range(0,1000000)]
list_comp_end_time = time.time()
print("Time using the list_comprehension: {} sec".format(list_comp_end_time - list_comp_start_time))
```
이제 동일한 연산을 실행하기 위해 for 루프를 사용할 것입니다:
```python
# Using For loop
for_loop_start_time= time.time()
result=[]
for i in range(0,1000000):
    result.append(i*i)
for_loop_end_time= time.time()
print("Time using the for loop: {} sec".format(for_loop_end_time - for_loop_start_time))
```
둘 사이에 큰 차이가 있음을 알 수 있습니다. 백분율로 차이를 계산할 수 있습니다:

```python
list_comp_time = list_comp_end_time - list_comp_start_time
for_loop_time = for_loop_end_time - for_loop_start_time
print("Difference in time: {} %".format((for_loop_time - list_comp_time)/ list_comp_time*100))
```
여기 효율적인 코드 작성의 효과를 보여주는 또 다른 예시가 있습니다. 우리는 1부터 100만까지의 모든 연속된 숫자의 합을 계산하고 싶습니다. 두 가지 방법이 있습니다: 첫 번째는 무차별 대입(brute force)을 사용하는 것으로, 1부터 100만까지 하나씩 더하는 것입니다.
```python
def sum_brute_force(N):
    res = 0
    for i in range(1,N+1):
        res+=i
    return res

# Using brute force
bf_start_time = time.time()
bf_result = sum_brute_force(1000000)
bf_end_time = time.time()
print("Time using brute force: {} sec".format(bf_end_time - bf_start_time))
```
또 다른 더 효율적인 방법은 공식을 사용하여 계산하는 것입니다. 1부터 어떤 숫자 N까지의 모든 정수의 합을 계산하고 싶을 때, N에 N+1을 곱한 다음 2로 나누면 원하는 결과를 얻을 수 있습니다. 이 문제는 실제로 19세기 독일의 일부 학생들에게 주어졌고, 칼 프리드리히 가우스(Carl-Friedrich Gauss)라는 영리한 학생이 이 문제를 몇 초 만에 해결하기 위한 이 공식을 고안했습니다.
```python
def sum_formula(N):
    return N*(N+1)/2

# Using the formula
formula_start_time = time.time()
formula_result = sum_formula(1000000)
formula_end_time = time.time()
print("Time using the formula: {} sec".format(formula_end_time - formula_start_time))
```
두 가지 방법을 모두 실행한 후, 우리는 160,000% 이상의 엄청난 개선을 달성했으며, 이는 간단한 작업에도 효율적이고 최적화된 코드(optimized code)가 왜 필요한지 명확하게 보여줍니다.

### 1.2. 값 효과적으로 선택 및 교체하기

먼저, 데이터 과학 프로젝트의 데이터 조작(data manipulation) 단계에서 일반적으로 수행하는 가장 일반적인 두 가지 작업부터 시작하겠습니다. 이 두 가지 작업은 특정 및 무작위 행과 열을 효율적으로 선택하는 것과 리스트(list) 및 딕셔너리(dictionary)를 사용하여 하나 또는 여러 값을 교체하는 `replace()` 함수의 사용입니다.

#### 1.2.1. `.iloc[]` 및 `.loc[]`를 사용하여 행 및 열 효율적으로 선택하기

이 하위 섹션에서는 `.iloc[]` 및 `.loc[]` pandas 함수를 사용하여 데이터프레임(DataFrame)에서 행을 효율적으로 찾고 선택하는 방법을 소개할 것입니다. 우리는 인덱스 번호 로케이터(index number locator)에는 `iloc[]`를, 인덱스 이름 로케이터(index name locator)에는 `loc[]`를 사용할 것입니다. 아래 예시에서는 포커 데이터셋의 첫 500개 행을 선택할 것입니다. 먼저 `.loc[]` 함수를 사용하고, 그 다음 `.iloc[]` 함수를 사용할 것입니다.
```python
# Specify the range of rows to select
rows = range(0, 500)

# Time selecting rows using .loc[]
loc_start_time = time.time()
poker_data.loc[rows]
loc_end_time = time.time()
print("Time using .loc[] : {} sec".format(loc_end_time - loc_start_time))

# Specify the range of rows to select
rows = range(0, 500)

# Time selecting rows using .iloc[]
iloc_start_time = time.time()
poker_data.iloc[rows]
iloc_end_time = time.time()
print("Time using .iloc[]: {} sec".format(iloc_end_time - iloc_start_time))

loc_comp_time = loc_end_time - loc_start_time
iloc_comp_time = iloc_end_time - iloc_start_time
print("Difference in time: {} %".format((loc_comp_time - iloc_comp_time)/ iloc_comp_time*100))
```
이 두 메서드는 구문(syntax)이 동일하지만, `iloc[]`는 `loc[]`보다 거의 70% 더 빠르게 수행됩니다. `.iloc[]` 함수는 이미 정렬된 인덱스(index)의 순서를 활용하므로 더 빠릅니다. 우리는 이들을 행뿐만 아니라 열을 선택하는 데도 사용할 수 있습니다. 다음 예시에서는 두 가지 방법을 사용하여 처음 세 열을 선택할 것입니다.
```python
iloc_start_time = time.time()
poker_data.iloc[:,:3]
iloc_end_time = time.time()
print("Time using .iloc[]: {} sec".format(iloc_end_time - iloc_start_time))

names_start_time = time.time()
poker_data[['S1', 'R1', 'S2']]
names_end_time = time.time()
print("Time using selection by name: {} sec".format(names_end_time - names_start_time))

loc_comp_time = names_end_time - names_start_time
iloc_comp_time = iloc_end_time - iloc_start_time
print("Difference in time: {} %".format((loc_comp_time - iloc_comp_time)/ loc_comp_time*100))
```
`iloc[]`를 사용한 열 인덱싱(column indexing)이 여전히 80% 더 빠르다는 것을 알 수 있습니다. 따라서 특정 열을 이름으로 선택하기 위해 `loc[]`를 사용하는 것이 더 쉽지 않다면, 더 빠른 `iloc[]`를 사용하는 것이 좋습니다.

#### 1.2.2. DataFrame에서 값 효과적으로 교체하기

DataFrame에서 값을 교체하는 것은 특히 데이터 정제(data cleaning) 단계에서 매우 중요한 작업입니다. 동일한 객체를 나타내는 모든 값을 동일하게 유지해야 하기 때문입니다. 이전에 로드했던 인기 아기 이름 데이터셋을 살펴보겠습니다:

`Gender` 특징(feature)을 자세히 살펴보고 고유한 값(unique values)을 확인해 봅시다:
```python
names['Gender'].unique()
```
여성 성별이 대문자와 소문자 두 가지 값으로 표현되어 있음을 알 수 있습니다. 이는 실제 데이터에서 매우 흔하며, 이를 수행하는 쉬운 방법은 데이터셋 전체에서 일관성을 유지하기 위해 한 값을 다른 값으로 교체하는 것입니다. 이를 수행하는 두 가지 방법이 있습니다. 첫 번째는 단순히 교체할 값을 정의한 다음, 무엇으로 교체할지 정의하는 것입니다. 이는 아래 코드에 나와 있습니다:
```python
start_time = time.time()
names['Gender'].loc[names.Gender=='female'] = 'FEMALE'
end_time = time.time()
pandas_time = end_time - start_time
print("Replace values using .loc[]: {} sec".format(pandas_time))
```
두 번째 방법은 아래 코드에 나와 있는 Pandas의 내장 함수인 `.replace()`를 사용하는 것입니다:

```python
start_time = time.time()
names['Gender'].replace('female', 'FEMALE', inplace=True)
end_time = time.time()
replace_time = end_time - start_time
print("Time using replace(): {} sec".format(replace_time))
```
내장 함수를 사용하면 시간 복잡도(time complexity)에 차이가 있음을 알 수 있으며, 값의 행 및 열 인덱스를 찾아 교체하는 `.loc()` 메서드를 사용하는 것보다 157% 더 빠릅니다.
```python
print('The differnce: {} %'.format((pandas_time- replace_time )/replace_time*100))
```
리스트를 사용하여 여러 값을 교체할 수도 있습니다. 우리의 목표는 WHITE NON-HISPANIC 또는 WHITE NON-HISP로 분류된 모든 민족을 WNH로 변경하는 것입니다. `.loc[]` 함수를 사용하여, 'or' 문(Python에서는 파이프(|)로 상징됨)을 사용하여 우리가 찾고 있는 민족의 아기들을 찾을 것입니다. 그런 다음 새 값을 할당할 것입니다. 항상 그렇듯이, 이 작업에 필요한 CPU 시간도 측정합니다.
```python
start_time = time.time()
names['Ethnicity'].loc[(names["Ethnicity"] == 'WHITE NON HISPANIC') | (names["Ethnicity"] == 'WHITE NON HISP')] = 'WNH'
end_time = time.time()
pandas_time= end_time - start_time
print("Results from the above operation calculated in %s seconds" %(pandas_time))
```
아래와 같이 `.replace()` pandas 내장 함수를 사용하여 동일한 작업을 수행할 수도 있습니다:
```python
start_time = time.time()
names['Ethnicity'].replace(['WHITE NON HISPANIC','WHITE NON HISP'], 'WNH', inplace=True)
end_time = time.time()
replace_time = end_time - start_time
print("Time using .replace(): {} sec".format(replace_time))
```
다시 `.replace()` 메서드를 사용하는 것이 `.loc[]` 메서드를 사용하는 것보다 훨씬 빠르다는 것을 알 수 있습니다. 얼마나 빠른지 더 잘 이해하기 위해 아래 코드를 실행해 봅시다:
```python
print('The differnce: {} %'.format((pandas_time- replace_time )/replace_time*100))
```
`.replace()` 메서드는 `.loc[]` 메서드보다 87% 더 빠릅니다. 데이터가 방대하고 많은 정제가 필요한 경우, 이 팁은 데이터 정제의 계산 시간을 줄이고 pandas 코드를 훨씬 빠르고 효율적으로 만들 것입니다.

마지막으로, 딕셔너리를 사용하여 DataFrame에서 단일 및 여러 값을 모두 교체할 수도 있습니다. 여러 함수를 하나의 명령으로 교체하고 싶을 때 매우 유용할 것입니다. 우리는 딕셔너리를 사용하여 모든 남성 성별을 BOY로, 모든 여성 성별을 GIRL로 교체할 것입니다.
```python
names = pd.read_csv('Popular_Baby_Names.csv')
start_time = time.time()
names['Gender'].replace({'MALE':'BOY', 'FEMALE':'GIRL', 'female': 'girl'}, inplace=True)
end_time = time.time()
dict_time = end_time - start_time
print("Time using .replace() with dictionary: {} sec".format(dict_time))

names = pd.read_csv('Popular_Baby_Names.csv')
start_time = time.time()
names['Gender'].replace('MALE', 'BOY', inplace=True)
names['Gender'].replace('FEMALE', 'GIRL', inplace=True)
names['Gender'].replace('female', 'girl', inplace=True)
end_time = time.time()
list_time = end_time - start_time
print("Time using multiple .replace(): {} sec".format(list_time))

print('The differnce: {} %'.format((list_time- dict_time )/dict_time*100))
```
리스트로도 같은 작업을 할 수 있지만, 더 장황합니다. 두 가지 방법을 비교하면 딕셔너리가 약 22% 더 빠르게 실행된다는 것을 알 수 있습니다. 일반적으로 Python에서 딕셔너리로 작업하는 것은 리스트에 비해 매우 효율적입니다. 리스트를 탐색하는 것은 리스트의 모든 요소를 통과해야 하는 반면, 딕셔너리를 탐색하는 것은 항목과 일치하는 키(key)로 즉시 이동합니다. 그러나 두 구조가 다른 목적을 가지고 있기 때문에 이 비교는 약간 불공평합니다. 딕셔너리를 사용하면 여러 다른 열에서 동일한 값을 교체할 수 있습니다. 이전의 모든 예시에서 우리는 교체할 값이 있는 열을 지정했습니다. 이제 동일한 열의 여러 값을 하나의 공통 값으로 교체할 것입니다. 모든 민족을 흑인(Black), 아시아인(Asian), 백인(White)의 세 가지 큰 범주로 분류하고 싶습니다. 구문은 다시 매우 간단합니다. 여기서는 중첩된 딕셔너리(nested dictionaries)를 사용합니다. 바깥쪽 키는 값을 교체할 열입니다. 이 바깥쪽 키의 값은 또 다른 딕셔너리이며, 여기서 키는 교체할 민족이고 값은 새로운 민족(흑인, 아시아인 또는 백인)입니다.
```python
start_time = time.time()
names.replace({'Ethnicity': {'ASIAN AND PACI': 'ASIAN', 'ASIAN AND PACIFIC ISLANDER': 'ASIAN','BLACK NON HISPANIC': 'BLACK', 'BLACK NON HISP': 'BLACK','WHITE NON HISPANIC': 'WHITE', 'WHITE NON HISP': 'WHITE'}})
print("Time using .replace() with dictionary: {} sec".format (time.time() - start_time))
```

#### 1.2.3. 값 선택 및 교체에 대한 모범 사례 요약

*   `.iloc[]` 함수를 사용하면 행과 열을 더 빠르게 선택할 수 있습니다. 따라서 `.loc[]`를 사용하는 것이 더 쉽거나 편리하고 속도가 우선 순위가 아니거나 한 번만 수행하는 경우가 아니라면 `.iloc[]`를 사용하는 것이 좋습니다.
*   내장 `replace()` 함수를 사용하는 것이 기존 메서드를 사용하는 것보다 훨씬 빠릅니다.
*   Python 딕셔너리를 사용하여 여러 값을 교체하는 것이 리스트를 사용하는 것보다 빠릅니다.

### 1.3. Pandas DataFrame을 효과적으로 반복하기

데이터 과학자로서 데이터프레임(DataFrame)을 광범위하게 반복해야 할 것입니다. 특히 데이터 준비 및 탐색 단계에서는 더욱 그렇습니다. 따라서 이를 효율적으로 수행할 수 있는 것이 중요합니다. 이는 많은 시간을 절약하고 더 중요한 작업에 집중할 수 있는 여유를 줄 것입니다. 루프(loop)를 훨씬 빠르고 효율적으로 만드는 세 가지 방법을 살펴보겠습니다:
*   `.iterrows()` 함수를 사용한 반복
*   `.apply()` 함수를 사용한 반복
*   벡터화(Vectorization)

#### 1.3.1. `.iterrows()`를 사용하여 효과적으로 반복하기

`.iterrows()` 함수를 사용하여 반복 프로세스(looping process)를 개선하는 방법에 대해 이야기하기 전에, 제너레이터 함수(generator function)의 개념을 다시 살펴보겠습니다. 제너레이터(generator)는 이터레이터(iterator)를 생성하는 간단한 도구입니다. 제너레이터의 본문 안에는 `return` 문 대신 `yield()` 문만 찾을 수 있습니다. `yield()` 문은 하나만 있을 수도 있고 여러 개 있을 수도 있습니다. 여기서는 네 개의 도시 이름을 생성하는 `city_name_generator()`라는 제너레이터를 볼 수 있습니다. 편의를 위해 제너레이터를 `city_names` 변수에 할당합니다.
```python
def city_name_generator():
    yield('New York')
    yield('London')
    yield('Tokyo')
    yield('Sao Paolo')

city_names = city_name_generator()
```
제너레이터가 생성하는 요소에 접근하려면 Python의 `next()` 함수를 사용할 수 있습니다. `next()` 명령이 사용될 때마다 제너레이터는 더 이상 생성할 값이 없을 때까지 다음 값을 생성합니다. 우리는 4개의 도시를 가지고 있습니다. `next` 명령을 네 번 실행하고 무엇을 반환하는지 봅시다:
```python
next(city_names)
next(city_names)
next(city_names)
```
`next()` 함수를 실행할 때마다 새로운 도시 이름이 출력되는 것을 볼 수 있습니다. `.iterrows()` 함수로 돌아가 봅시다. `.iterrows()` 함수는 모든 pandas DataFrame의 속성(property)입니다. 호출되면 두 개의 요소를 가진 리스트를 생성합니다. 우리는 이 제너레이터를 사용하여 포커 DataFrame의 각 줄을 반복할 것입니다. 첫 번째 요소는 행의 인덱스(index)이고, 두 번째 요소는 각 행의 각 특징(feature)에 대한 pandas Series를 포함합니다. 즉, 다섯 장의 각 카드에 대한 문양(Symbol)과 순위(Rank)입니다. 이는 리스트에 적용될 때 각 요소와 해당 인덱스를 반환하는 `enumerate()` 함수의 개념과 매우 유사합니다.

Pandas DataFrame을 반복하는 가장 직관적인 방법은 종종 조잡한 반복(crude looping)이라고 불리는 `range()` 함수를 사용하는 것입니다. 이는 아래 코드에 나와 있습니다:
```python
start_time = time.time()
for index in range(poker_data.shape[0]):
    next
print("Time using range(): {} sec".format(time.time() - start_time))
```
pandas DataFrame을 반복하는 더 영리한 방법은 이 작업을 위해 최적화된 `.iterrows()` 함수를 사용하는 것입니다. 우리는 단순히 'for' 루프를 두 개의 이터레이터(iterator)로 정의합니다. 하나는 각 행의 번호이고 다른 하나는 모든 값입니다. 루프 내부에서 `next()` 명령은 실제로 아무것도 하지 않고 루프가 이터레이터의 다음 값으로 이동함을 나타냅니다.
```python
data_generator = poker_data.iterrows()
start_time = time.time()
for index, values in data_generator:
    next
print("Time using .iterrows(): {} sec".format(time.time() - start_time))
```
두 계산 시간을 비교해 보면, `.iterrows()`를 사용하는 것이 pandas DataFrame을 반복하는 속도를 향상시키지 않는다는 것을 알 수 있습니다. 그러나 데이터셋을 반복하면서 각 행의 값을 사용하는 더 깔끔한 방법이 필요할 때 매우 유용합니다.

#### 1.3.2. `.apply()`를 사용하여 효과적으로 반복하기

이제 `.apply()` 함수를 사용하여 pandas DataFrame을 반복하면서 특정 작업을 수행할 수 있도록 할 것입니다. `.apply()` 함수는 말 그대로 전체 DataFrame에 다른 함수를 적용합니다. `.apply()` 함수의 구문(syntax)은 간단합니다. 이 경우 람다 함수(lambda function)를 사용하여 매핑(mapping)을 생성한 다음, 모든 셀(cell)에 적용할 함수를 선언합니다. 여기서는 DataFrame의 모든 셀에 제곱근(square root) 함수를 적용하고 있습니다. 속도 면에서는 전체 DataFrame에 NumPy `sqrt()` 함수를 사용하는 속도와 일치합니다.
```python
data_sqrt = poker_data.apply(lambda x: np.sqrt(x))
data_sqrt.head()
```
이것은 이 함수를 데이터프레임에 적용하고 싶기 때문에 간단한 예시입니다. 하지만 관심 있는 함수가 하나 이상의 셀을 입력으로 받을 때는 어떻게 될까요? 예를 들어, 각 핸드(hand)에 있는 모든 카드의 순위(rank) 합계를 계산하고 싶다면 어떻게 해야 할까요? 이 경우, 이전과 동일한 방식으로 `.apply()` 함수를 사용하겠지만, 함수를 각 행에 적용하고 있음을 지정하기 위해 줄 끝에 `axis=1`을 추가해야 합니다.
```python
apply_start_time = time.time()
poker_data[['R1', 'R2', 'R3', 'R4', 'R5']].apply(lambda x: sum(x), axis=1)
apply_end_time = time.time()
apply_time = apply_end_time - apply_start_time
print("Time using .apply(): {} sec".format(time.time() - apply_start_time))
```
그런 다음, 이전에 보았던 `.iterrows()` 함수를 사용하여 효율성을 비교할 것입니다.

```python
for_loop_start_time = time.time()
for ind, value in poker_data.iterrows():
    sum([value[1], value[3], value[5], value[7], value[9]])
for_loop_end_time = time.time()
for_loop_time = for_loop_end_time - for_loop_start_time
print("Time using .iterrows(): {} sec".format(for_loop_time))
```
`.apply()` 함수를 사용하는 것이 `.iterrows()` 함수보다 약 400% 정도 더 빠르며, 이는 엄청난 개선입니다!
```python
print('The differnce: {} %'.format((for_loop_time - apply_time) / apply_time * 100))
```
행에서 했던 것처럼 열에 대해서도 동일한 작업을 수행할 수 있습니다. 각 열에 하나의 함수를 적용하는 것입니다. `axis=1`을 `axis=0`으로 바꾸면 모든 열에 `sum` 함수를 적용할 수 있습니다.
```python
apply_start_time = time.time()
poker_data[['R1', 'R2', 'R3', 'R4', 'R5']].apply(lambda x: sum(x), axis=0)
apply_end_time = time.time()
apply_time = apply_end_time - apply_start_time
print("Time using .apply(): {} sec".format(apply_time))
```
`.apply()` 함수를 행에 대한 합계를 계산하는 pandas의 기본 함수와 비교하면, pandas의 기본 `.sum()` 함수가 동일한 작업을 더 빠르게 수행한다는 것을 알 수 있습니다.
```python
pandas_start_time = time.time()
poker_data[['R1', 'R1', 'R3', 'R4', 'R5']].sum(axis=0)
pandas_end_time = time.time()
pandas_time = pandas_end_time - pandas_start_time
print("Time using pandas: {} sec".format(pandas_time))

print('The differnce: {} %'.format((apply_time - pandas_time) / pandas_time * 100))
```
결론적으로, `.apply()` 함수는 pandas DataFrame의 모든 행을 반복할 때 더 빠르게 수행되지만, 동일한 작업을 열을 통해 수행할 때는 더 느리다는 것을 알 수 있습니다.

#### 1.3.3. 벡터화(vectorization)를 사용하여 효과적으로 반복하기

함수가 수행하는 반복량(amount of iteration)을 줄이는 방법을 이해하려면, Pandas의 기본 단위인 DataFrame과 Series가 모두 배열(array)을 기반으로 한다는 것을 기억해야 합니다. Pandas는 각 값을 개별적으로 또는 순차적으로 처리하는 것보다 전체 배열에 대해 연산이 수행될 때 더 효율적으로 작동합니다. 이는 벡터화(vectorization)를 통해 달성할 수 있습니다. 벡터화는 전체 배열에 대해 연산을 실행하는 프로세스입니다. 아래 코드에서는 각 핸드(hand)에 있는 모든 카드의 순위(rank) 합계를 계산하려고 합니다. 이를 위해 포커 데이터셋을 슬라이싱(slice)하여 각 카드의 순위를 포함하는 열만 유지합니다. 그런 다음, 각 행에 대한 합계를 원한다는 것을 나타내기 위해 `axis = 1` 매개변수를 사용하여 DataFrame의 내장 `.sum()` 속성을 호출합니다. 마지막으로, 데이터의 처음 다섯 행의 합계를 출력합니다.
```python
start_time_vectorization = time.time()
poker_data[['R1', 'R2', 'R3', 'R4', 'R5']].sum(axis=1)
end_time_vectorization = time.time()
vectorization_time = end_time_vectorization - start_time_vectorization
print("Time using pandas vectorization: {} sec".format(vectorization_time))
```
이전에 DataFrame의 모든 행을 단순히 반복하는 것보다 DataFrame에 적용된 함수를 더 빠르게 수행하는 다양한 방법을 보았습니다. 우리의 목표는 이 작업을 수행하는 가장 효율적인 방법을 찾는 것입니다.

DataFrame을 반복하기 위해 `.iterrows()` 사용:
```python
data_generator = poker_data.iterrows()
start_time_iterrows = time.time()
for index, value in data_generator:
    sum([value[1], value[3], value[5], value[7]])
end_time_iterrows = time.time()
iterrows_time = end_time_iterrows - start_time_iterrows
print("Time using .iterrows() {} seconds " .format(iterrows_time))
```
`.apply()` 메서드 사용:
```python
start_time_apply = time.time()
poker_data[['R1', 'R2', 'R3', 'R4', 'R5']].apply(lambda x: sum(x),axis=1)
end_time_apply = time.time()
apply_time = end_time_apply - start_time_apply
print("Time using apply() {} seconds" .format(apply_time))
```
벡터화, `.iterrows()` 함수, `.apply()` 함수를 사용하여 각 핸드에 있는 모든 카드의 순위 합계를 계산하는 데 걸리는 시간을 비교하면, 벡터화 방법이 훨씬 더 잘 수행된다는 것을 알 수 있습니다.

DataFrame을 효과적으로 반복하는 또 다른 벡터화 방법을 사용할 수도 있습니다. 이는 NumPy 배열(Numpy arrays)을 사용하여 DataFrame을 벡터화합니다. 자신을 "Python 과학 계산을 위한 기본 패키지(fundamental package for scientific computing in Python)"라고 정의하는 NumPy 라이브러리(library)는 최적화되고 미리 컴파일된 C 코드(pre-compiled C code)로 내부적으로 연산을 수행합니다. 배열로 작업하는 pandas와 유사하게, NumPy는 `ndarray`라고 불리는 배열에서 작동합니다. Series와 `ndarray`의 주요 차이점은 `ndarray`가 인덱싱(indexing), 데이터 유형 확인(data type checking) 등과 같은 많은 연산을 생략한다는 것입니다. 결과적으로 NumPy 배열에 대한 연산은 pandas Series에 대한 연산보다 훨씬 빠를 수 있습니다. pandas Series가 제공하는 추가 기능이 중요하지 않을 때 NumPy 배열을 pandas Series 대신 사용할 수 있습니다. 이 글에서 살펴보는 문제의 경우, pandas Series 대신 NumPy `ndarray`를 사용할 수 있습니다. 문제는 이것이 더 효율적인지 아닌지입니다.

다시, 각 핸드에 있는 모든 카드의 순위 합계를 계산할 것입니다. pandas Series의 `.values` 메서드를 사용하여 순위 배열을 pandas Series에서 NumPy 배열로 변환합니다. 이 메서드는 pandas Series를 NumPy `ndarray`로 반환합니다. Series에 대한 벡터화와 마찬가지로, NumPy 배열을 함수에 직접 전달하면 pandas가 전체 벡터에 함수를 적용하게 됩니다.

```python
start_time = time.time()
poker_data[['R1', 'R2', 'R3', 'R4', 'R5']].values.sum(axis=1)
print("Time using NumPy vectorization: {} sec" .format(time.time() - start_time))

start_time = time.time()
poker_data[['R1', 'R2', 'R3', 'R4', 'R5']].sum(axis=1)
print("Results from the above operation calculated in %s seconds" % (time.time() - start_time))
```
이 시점에서, pandas Series에 대한 벡터화가 일상적인 계산에 필요한 최적화 요구 사항의 압도적인 대부분을 달성한다는 것을 알 수 있습니다. 그러나 속도가 최우선이라면 NumPy Python 라이브러리(library) 형태의 지원을 요청할 수 있습니다. 이전의 최첨단(state-of-the-art) 방법인 pandas의 최적화와 비교해도 여전히 작동 시간(operating time)이 개선됩니다.

#### 1.3.4. DataFrame 반복에 대한 모범 사례 요약

*   `.iterrows()`를 사용하는 것은 DataFrame을 반복하는 속도를 향상시키지 않지만, 더 효율적입니다.
*   `.apply()` 함수는 pandas DataFrame의 모든 행을 반복할 때 더 빠르게 수행되지만, 동일한 작업을 열을 통해 수행할 때는 더 느립니다.
*   pandas Series에 대한 벡터화는 일상적인 계산에 필요한 최적화 요구 사항의 압도적인 대부분을 달성합니다. 그러나 속도가 최우선이라면 NumPy Python 라이브러리 형태의 지원을 요청할 수 있습니다.

### 1.4. `.groupby()`로 데이터 효과적으로 변환하기

이 글의 마지막 섹션에서는 특정 특징(feature)의 값에 따라 DataFrame의 항목을 그룹화하기 위해 `.groupby()` 함수를 효과적으로 사용하는 방법에 대해 논의할 것입니다. `.groupby()` 메서드는 DataFrame에 적용되어 특정 특징에 따라 그룹화합니다. 그런 다음, 해당 그룹화된 객체에 간단하거나 더 복잡한 함수를 적용할 수 있습니다. 이는 테이블 형식(tabular) 또는 구조화된 데이터(structured data)로 작업하는 모든 데이터 과학자에게 매우 중요한 도구이며, 데이터를 쉽고 효과적인 방식으로 조작하는 데 도움이 될 것입니다.

#### 1.4.1. `.groupby()`와 함께 사용되는 일반적인 함수

집계된 그룹(aggregated group)에 적용할 수 있는 가장 간단한 메서드 중 하나는 `.count()`입니다. 아래 예시에서는 이를 레스토랑 데이터셋에 적용할 것입니다. 먼저, 고객이 흡연자인지 아닌지에 따라 레스토랑 데이터를 그룹화합니다. 그런 다음 `.count()` 메서드를 적용합니다. 흡연자와 비흡연자의 수를 얻습니다.
```python
restaurant = pd.read_csv('restaurant_data.csv')
restaurant_grouped = restaurant.groupby('smoker')
print(restaurant_grouped.count())
```
`.count()` 메서드가 각 특징에서 각 그룹의 발생 횟수를 세기 때문에 모든 특징에 대해 동일한 결과를 얻는 것은 놀라운 일이 아닙니다. 데이터에 결측값(missing values)이 없으므로 모든 열에서 결과는 동일해야 합니다.

특정 특징의 값에 따라 DataFrame의 항목을 그룹화한 후, 우리는 관심 있는 모든 종류의 변환(transformation)을 적용할 수 있습니다. 여기서는 각 값과 평균 사이의 거리를 표준 편차(standard deviation)로 나눈 정규화 변환(normalization transformation)인 z-점수(z score)를 적용할 것입니다. 이는 통계학에서 매우 유용한 변환이며, 표준화된 테스트(standardized testing)에서 z-검정(z-test)과 함께 자주 사용됩니다. 이 변환을 그룹화된 객체에 적용하려면, 우리가 정의한 람다 변환을 포함하는 `.transform()` 메서드를 호출하기만 하면 됩니다. 이번에는 식사 유형에 따라 그룹화할 것입니다. 저녁 식사였을까요, 아니면 점심 식사였을까요? z-점수 변환은 그룹 관련이므로, 결과 테이블은 원본 테이블과 동일합니다. 각 요소에 대해 해당 요소가 속한 그룹의 평균을 빼고 표준 편차로 나눕니다. 또한 숫자 변환은 DataFrame의 숫자 특징에만 적용된다는 것을 알 수 있습니다.
```python
zscore = lambda x: (x - x.mean() ) / x.std()
restaurant_grouped = restaurant.groupby('time')
restaurant_transformed = restaurant_grouped.transform(zscore)
restaurant_transformed.head()
```
`transform()` 메서드가 많은 것을 단순화하지만, 실제로는 기본 Python 코드(native Python code)를 사용하는 것보다 더 효율적일까요? 이전과 마찬가지로, 이번에는 성별(sex)에 따라 데이터를 먼저 그룹화합니다. 그런 다음 이전에 적용했던 z-점수 변환을 적용하여 효율성을 측정합니다. 각 연산의 시간을 측정하는 코드는 이미 익숙하므로 여기서는 생략합니다. `transform()` 함수를 사용하면 엄청난 속도 향상을 달성할 수 있음을 알 수 있습니다. 게다가, 우리는 관심 있는 작업에 대해 단 한 줄만 사용합니다.
```python
restaurant.groupby('sex').transform(zscore)

mean_female = restaurant.groupby('sex').mean()['total_bill']['Female']
mean_male = restaurant.groupby('sex').mean()['total_bill']['Male']

std_female = restaurant.groupby('sex').std()['total_bill']['Female']
std_male = restaurant.groupby('sex').std()['total_bill']['Male']

for i in range(len(restaurant)):
    if restaurant.iloc[i][2] == 'Female':
        restaurant.iloc[i][0] = (restaurant.iloc[i][0] - mean_female)/std_female
    else:
        restaurant.iloc[i][0] = (restaurant.iloc[i][0] - mean_male)/std_male
```

#### 1.4.2. `.groupby()` 및 `.transform()`을 사용한 결측값 대체(Missing value imputation)

이제 그룹화된 pandas 객체에 `transform()` 함수를 사용하는 이유와 방법을 살펴보았으므로, 결측값 대체(missing value imputation)라는 매우 구체적인 작업을 다룰 것입니다. `transform()` 함수를 결측값 대체에 어떻게 사용할 수 있는지 실제로 보기 전에, 각 그룹에서 관심 변수에 얼마나 많은 결측값이 있는지 살펴볼 것입니다. 아래에서 각 "time" 특징의 데이터 포인트(data points) 수를 볼 수 있으며, 이는 176+68 = 244입니다.
```python
prior_counts = restaurant.groupby('time')
prior_counts['total_bill'].count()
```
다음으로, 아래 코드를 사용하여 10%의 무작위 관측치(random observations)의 총 청구액(total bill)이 NaN으로 설정된 `restaurant_nan` 데이터셋을 생성할 것입니다:
```python
import pandas as pd
import numpy as np

p = 0.1 #percentage missing data required
mask = np.random.choice([np.nan,1], size=len(restaurant), p=[p,1-p])
restaurant_nan = restaurant.copy()
restaurant_nan['total_bill'] = restaurant_nan['total_bill'] * mask
```
이제 각 "time" 특징의 데이터 포인트 수를 출력해 봅시다. 이제 155+62 = 217임을 알 수 있습니다. 우리가 가진 총 데이터 포인트는 244이므로, 결측 데이터 포인트는 24개이며, 이는 10%에 해당합니다.
```python
prior_counts = restaurant.groupby('time')
prior_counts['total_bill'].count()
```
데이터에서 결측값의 수를 센 후, 이러한 결측값을 그룹별 함수(group-specific function)로 채우는 방법을 보여줄 것입니다. 가장 일반적인 선택은 평균(mean)과 중앙값(median)이며, 선택은 데이터의 왜도(skewness)와 관련이 있습니다. 이전과 마찬가지로, `fillna()` 함수를 사용하여 모든 결측값을 해당 그룹 평균으로 대체하는 람다 변환을 정의합니다. 이전과 마찬가지로, 식사 시간에 따라 데이터를 그룹화한 다음, 미리 정의된 변환을 적용하여 결측값을 대체합니다.
```python
# Missing value imputation
missing_trans = lambda x: x.fillna(x.mean())
restaurant_nan_grouped = restaurant_nan.groupby('time')['total_bill']
restaurant_nan_grouped.transform(missing_trans)
```
보시다시피, 인덱스 0과 인덱스 4의 관측치는 동일하며, 이는 해당 결측값이 해당 그룹의 평균으로 대체되었음을 의미합니다. 또한, 이 방법을 사용한 계산 시간은 0.007초입니다. 이를 기존 방법과 비교해 봅시다:
```python
start_time = time.time()
mean_din = restaurant_nan.loc[restaurant_nan.time =='Dinner']['total_bill'].mean()
mean_lun = restaurant_nan.loc[restaurant_nan.time == 'Lunch']['total_bill'].mean()

for row in range(len(restaurant_nan)):
    if restaurant_nan.iloc[row]['time'] == 'Dinner':
        restaurant_nan.loc[row, 'total_time'] = mean_din
    else:
        restaurant_nan.loc[row, 'total_time'] = mean_lun
print("Results from the above operation calculated in %s seconds" % (time.time() - start_time))
```
그룹화된 객체에 적용된 `.transform()` 함수를 사용하는 것이 이 작업에 대한 기본 Python 코드보다 더 빠르게 수행된다는 것을 알 수 있습니다.

#### 1.4.3. `.groupby()` 및 `.filter()`를 사용한 데이터 필터링(Data filtration)

이제 그룹화된 pandas 객체에서 `filter()` 함수를 사용하는 방법에 대해 논의할 것입니다. 이를 통해 특정 조건에 따라 해당 그룹의 하위 집합(subset)만 포함할 수 있습니다. 종종 특정 특징에 따라 DataFrame의 항목을 그룹화한 후, 우리는 특정 조건에 따라 해당 그룹의 하위 집합만 포함하는 데 관심이 있습니다. 필터링 조건의 예로는 결측값의 수, 특정 특징의 평균, 또는 데이터셋에서 그룹의 발생 횟수 등이 있습니다.

우리는 웨이터에게 지불된 평균 금액이 20 USD를 초과하는 날에 주어진 팁의 평균 금액을 찾는 데 관심이 있습니다. `.filter()` 함수는 각 그룹의 DataFrame에서 작동하는 람다 함수를 허용합니다. 이 예시에서 람다 함수는 "total_bill"을 선택하고 `mean()`이 20보다 큰지 확인합니다. 만약 람다 함수가 True를 반환하면, 팁의 `mean()`이 계산됩니다. 팁의 총 평균을 비교하면 두 값 사이에 차이가 있음을 알 수 있으며, 이는 필터링이 올바르게 수행되었음을 의미합니다.
```python
restaurant_grouped = restaurant.groupby('day')
filter_trans = lambda x : x['total_bill'].mean() > 20
restaurant_filtered = restaurant_grouped.filter(filter_trans)
print(restaurant_filtered['tip'].mean())
print(restaurant['tip'].mean())
```
`groupby()`를 사용하지 않고 이 작업을 수행하려고 하면 비효율적인 코드가 됩니다. 먼저, 리스트 컴프리헨션(list comprehension)을 사용하여 평균 식사 금액이 20달러보다 큰 날을 나타내는 DataFrame 항목을 추출한 다음, for 루프를 사용하여 이를 리스트에 추가하고 평균을 계산합니다. 매우 직관적으로 보일 수 있지만, 보시다시피 매우 비효율적입니다.

```python
t=[restaurant.loc[restaurant['day'] == i]['tip'] for i in restaurant['day'].unique() if restaurant.loc[restaurant['day'] == i]['total_bill'].mean()>20]
restaurant_filtered = t[0]
for j in t[1:]:
    restaurant_filtered=restaurant_filtered.append(j,ignore_index=True)
```

---

### 2.1. AI 윤리: 공정성과 투명성 확보

AI 시스템은 이제 우리 삶의 많은 부분에 깊숙이 관여하고 있으며, 그 영향력은 날마다 커지고 있습니다. 이러한 시스템이 사회에 미치는 긍정적, 부정적 영향을 이해하고 관리하는 것은 데이터 과학자들의 중요한 책임입니다. 모델 배포 전과 후에 지속적인 검토와 개선이 필요하며, 우리는 이 과정을 통해 모델의 잠재적 편향(potential bias)을 알 수 있습니다. 효율적인 AI 윤리 방법론을 적용하는 것이 모델의 사회적 책임(social responsibility)을 어떻게 개선하고 예측의 공정성(fairness)을 어떻게 높이는지 몇 가지 주요 원칙을 살펴보겠습니다:

AI 시스템은 이제 우리 삶의 많은 부분에 깊숙이 관여하고 있으며, 그 영향력은 날마다 커지고 있습니다. 이러한 시스템이 사회에 미치는 긍정적, 부정적 영향을 이해하고 관리하는 것은 데이터 과학자들의 중요한 책임입니다. 예를 들어, 채용 과정에 사용되는 AI 모델이 특정 성별이나 인종에 대한 편향을 가질 경우, 이는 심각한 차별 문제로 이어질 수 있습니다. 데이터 과학자들은 투명성과 공정성을 높여 이러한 문제를 사전에 방지하고 해결해야 합니다.

AI 윤리 위반 사례도 다수 존재합니다. 안면 인식 기술이 특정 인종을 잘못 식별하거나, 대출 심사 모델이 소수 집단에게 불리하게 작용하는 경우가 대표적입니다. 이러한 사례들은 기술의 발전만큼이나 윤리적 고려가 왜 필수적인지 명확하게 보여줍니다. AI 윤리는 단순히 법적 준수를 넘어, 사회적 신뢰와 지속 가능한 기술 발전을 위한 근본적인 기반이 됩니다.

---

### 2.2. MLOps: 모델 배포 및 관리를 위한 핵심

먼저, 머신러닝 프로젝트의 모델 배포(model deployment) 단계에서 일반적으로 수행하는 가장 중요한 두 가지 작업부터 시작하겠습니다. 모델의 지속적인 성능 관리와 안정적인 운영 환경 구축은 매우 중요한 요소입니다.

#### 2.2.1. CI/CD 파이프라인 구축

이 하위 섹션에서는 CI/CD 파이프라인(CI/CD pipeline)을 사용하여 머신러닝 모델의 개발, 테스트, 배포 과정을 자동화하는 방법을 소개할 것입니다. 지속적인 통합(Continuous Integration)은 코드 변경 사항이 정기적으로 메인 브랜치에 통합되고 자동으로 테스트되는 것을 의미합니다. 지속적인 배포(Continuous Deployment)는 테스트를 통과한 변경 사항이 자동으로 운영 환경에 배포되는 것을 의미합니다. 우리는 이를 코드뿐만 아니라 모델 아티팩트(model artifact) 관리에도 사용할 수 있습니다.

CI/CD 파이프라인을 구축하면 개발 주기가 단축되고, 오류를 조기에 발견하며, 모델 배포의 일관성을 확보할 수 있습니다. 예를 들어, 새로운 데이터로 모델을 재학습(retrain)하거나 코드에 변경 사항이 생겼을 때, 수동으로 모든 과정을 처리하는 것보다 자동화된 파이프라인이 훨씬 더 안정적이라는 것을 알 수 있습니다. 따라서 특정 환경에 맞는 배포 전략을 수립하는 것이 더 효율적입니다.

#### 2.2.2. 모델 모니터링 및 재학습

배포된 모델에서 성능 저하를 감지하는 것은 특히 운영(production) 단계에서 매우 중요한 작업입니다. 모델은 시간이 지남에 따라 데이터 분포가 변하거나(data drift), 외부 환경 변화로 인해 성능이 저하될 수 있습니다. 이를 효과적으로 감지하고 대응하기 위해 모델 모니터링 시스템을 구축해야 합니다.

자동화된 시스템을 사용하면 모델의 예측 정확도, 지연 시간(latency), 리소스 사용량 등을 실시간으로 추적할 수 있습니다. 예를 들어, 모델의 예측 결과와 실제 결과 간의 차이(residual)가 특정 임계값을 초과할 경우 경고를 발생시키고, 필요시 모델 재학습을 트리거(trigger)할 수 있습니다. 이는 운영 복잡도(operational complexity)에 차이가 있음을 알 수 있으며, 모델의 재학습 주기를 단축하고 MLOps 파이프라인을 훨씬 빠르고 효율적으로 만들 것입니다.

#### 2.2.3. MLOps 모범 사례 요약

마지막으로, 통합된 플랫폼을 사용하여 모델의 수명 주기(lifecycle)를 모두 관리할 수도 있습니다. MLOps는 단순히 기술적인 도구의 집합이 아니라, 개발(Dev)과 운영(Ops) 팀 간의 협업 문화를 구축하는 과정입니다.

*   따라서 수동 개입이 더 쉽거나 편리하고 속도가 우선 순위가 아니거나 한 번만 수행하는 경우가 아니라면, MLOps를 통한 자동화된 모델 배포 및 관리를 사용하는 것이 좋습니다.
*   지속적인 통합(Continuous Integration) 시스템을 사용하는 것이 기존 수동 프로세스보다 훨씬 빠릅니다.
*   컨테이너화(containerization) 기술을 사용하여 여러 환경에서 모델을 배포하는 것이 가상 머신(virtual machine)을 사용하는 것보다 빠릅니다. 이 비교는 다소 주관적일 수 있지만, 현대 클라우드 환경에서는 컨테이너 기반 배포가 매우 효과적입니다.

---

### 2.3. 데이터 스토리텔링: 통찰력을 전달하는 기술

데이터 과학자로서 복잡한 데이터(complex data)를 광범위하게 해석하고 비즈니스 의사결정 과정에 활용해야 할 것입니다. 따라서 이를 효과적으로 전달할 수 있는 것이 중요합니다. 이는 많은 시간을 절약하고 더 중요한 인사이트(insight) 전달에 집중할 수 있는 여유를 줄 것입니다. 데이터 스토리텔링을 훨씬 효과적으로 만드는 세 가지 핵심 요소를 살펴보겠습니다:

#### 2.3.1. 시각화의 힘

시각화 도구(visualization tool)를 사용하여 데이터의 복잡성(complexity)을 개선하는 방법에 대해 이야기하기 전에, 데이터가 단순히 숫자와 통계의 집합이 아니라는 점을 상기해야 합니다. 데이터는 그 자체로 스토리를 담고 있으며, 이를 효과적으로 드러내는 것이 중요합니다. 시각화는 이러한 스토리를 시각적으로 매력적이고 이해하기 쉽게 전달하는 강력한 수단입니다.

데이터 시각화(data visualization)를 시작하는 가장 직관적인 방법은 적절한 차트 유형을 선택하고, 복잡한 데이터를 단순화하는 것입니다. 예를 들어, 시계열 데이터(time-series data)를 꺾은선 그래프(line chart)로 표현하거나, 범주형 데이터(categorical data)의 분포를 막대 그래프(bar chart)로 나타내는 것은 데이터의 이해도를 향상시킨다는 것을 알 수 있습니다. 복잡한 데이터 분석 결과를 비전문가에게 설명해야 할 때, 잘 만들어진 시각화는 더 설득력 있는 방법이 필요할 때 매우 유용합니다.

#### 2.3.2. 내러티브 구성

이제 내러티브(narrative)를 사용하여 데이터에 의미를 부여하고 특정 메시지를 전달할 수 있도록 할 것입니다. 데이터 스토리텔링은 단순히 시각화를 만드는 것을 넘어, 데이터가 말하는 바를 명확하고 설득력 있는 이야기로 엮어내는 기술입니다. 이는 데이터 분석의 결과를 청중의 언어로 번역하는 과정입니다.

내러티브를 구성하는 핵심은 "왜(Why)", "무엇(What)", "그래서 무엇(So What)"의 구조를 따르는 것입니다. "왜" 이 분석이 중요한지, "무엇"을 발견했는지, 그리고 이 발견이 "그래서 무엇"을 의미하는지 설명해야 합니다. 정량적인 데이터(quantitative data)뿐만 아니라 정성적인 스토리(qualitative story)에 대해서도 동일한 작업을 수행할 수 있습니다. 하지만 스토리텔링의 구성이 복잡하면 오히려 더 혼란스러울 수 있습니다.

#### 2.3.3. 효과적인 데이터 스토리텔링 요약

스토리가 전달하는 정보량(amount of information)을 줄이는 방법을 이해하려면, 청중의 배경 지식과 관심사를 고려해야 합니다. 모든 데이터를 한 번에 보여주려 하기보다는, 핵심 메시지를 중심으로 스토리를 전개하는 것이 중요합니다. 우리의 목표는 이 작업을 수행하는 가장 효과적인 방법을 찾는 것입니다.

데이터를 효과적으로 시각화하는 또 다른 접근 방식을 사용할 수도 있습니다. 이는 간결하고 명확한 메시지를 전달하는 데 중점을 둡니다. 예를 들어, 단순한 인포그래픽(infographic)이나 대시보드(dashboard)는 복잡한 통계 보고서보다 훨씬 더 설득력 있다는 것을 알 수 있습니다. 공감이 최우선이라면 인간적인 요소(human element) 형태의 지원을 요청할 수 있습니다. 데이터가 개인의 삶에 어떤 영향을 미치는지 보여주는 사례를 포함하면 청중의 몰입도를 높일 수 있습니다.

*   시각화를 사용하는 것은 데이터의 이해도를 높이는 데 더 직관적입니다.
*   내러티브 구성은 데이터 분석 결과를 효과적으로 전달하는 데 중요하지만, 복잡하게 구성하면 오히려 혼란을 줄 수 있습니다.
*   명확성이 최우선이라면 간결한 표현(concise expression) 형태의 지원을 요청할 수 있습니다.

---

### 2.4. 클라우드 기반 데이터 플랫폼 활용

이 글의 마지막 섹션에서는 클라우드 기반 데이터 플랫폼(cloud-based data platform)을 효과적으로 활용하는 방법에 대해 논의할 것입니다. 클라우드 플랫폼은 데이터 저장, 처리, 분석 및 모델 배포를 위한 유연하고 확장 가능한 환경을 제공하며, 이는 데이터 과학 프로젝트를 효율적으로 운영하는 데 도움이 될 것입니다.

#### 2.4.1. 서버리스(Serverless) 아키텍처의 이점

집계된 그룹(aggregated group)에 적용할 수 있는 가장 중요한 개념 중 하나는 서버리스(serverless)입니다. 서버리스 컴퓨팅은 개발자가 서버 관리의 부담 없이 코드 실행에만 집중할 수 있게 해줍니다. 클라우드 제공업체가 서버 프로비저닝(provisioning), 확장(scaling) 및 유지보수를 자동으로 처리합니다. 모든 서비스에 대해 일관된 관리(consistent management)를 얻는 것은 놀라운 일이 아닙니다.

특정 클라우드 서비스(cloud service)의 가치를 이해한 후, 우리는 관심 있는 모든 종류의 변환(transformation)을 적용할 수 있습니다. 서버리스 아키텍처는 매우 유연한 아키텍처(flexible architecture)이며, 이벤트 기반(event-driven)으로 작동하여 필요할 때만 리소스를 사용하므로 비용 효율적입니다. 예를 들어, 데이터 파이프라인에서 ETL(Extract, Transform, Load) 작업을 서버리스 함수로 구현하면 배포 환경은 운영 환경과 동일합니다. 확장성(scalability)에만 국한되지 않는다는 것을 알 수 있습니다. 단 한 번의 클릭으로 가능합니다.

#### 2.4.2. 데이터 보안 및 규정 준수

이제 클라우드 환경에서 데이터 보안(data security)을 강화하는 이유와 방법을 살펴보았으므로, 클라우드 플랫폼은 강력한 보안 기능을 제공하지만, 사용자의 책임 또한 중요합니다. 암호화(encryption), 접근 제어(access control), 네트워크 보안(network security) 등 다양한 측면에서 데이터를 보호해야 합니다.

클라우드 환경에서 데이터 보안은 100% 필수적입니다. 데이터를 보호하는 것은 물론, GDPR, CCPA와 같은 데이터 규정 준수(compliance)도 중요한 과제입니다. 클라우드 서비스 제공업체는 일반적으로 규정 준수 인증을 제공하지만, 데이터 과학자는 자신의 데이터 처리 방식이 해당 규정을 준수하는지 확인해야 합니다. 이를 온프레미스(on-premise) 방식과 비교해 봅시다. 클라우드 기반의 통합된 보안 기능은 기존 방법보다 훨씬 더 안전하게 수행된다는 것을 알 수 있습니다.

#### 2.4.3. 클라우드 데이터 거버넌스(Cloud Data Governance) 및 규정 준수

이제 클라우드 데이터 거버넌스(cloud data governance)에서 규정 준수(compliance)를 확보하는 방법에 대해 논의할 것입니다. 데이터 거버넌스는 데이터의 가용성, 사용성, 무결성 및 보안을 보장하기 위한 정책, 프로세스 및 기술을 관리하는 체계입니다. 클라우드 환경에서는 데이터가 여러 지역에 분산될 수 있으므로, 데이터 주권(data sovereignty) 및 지역별 규제에 대한 이해가 필수적입니다.

우리는 특정 지역의 데이터 주권(data sovereignty) 규제를 초과하는 데이터 처리에 대한 해결책을 찾는 데 관심이 있습니다. 예를 들어, 유럽 연합(EU)의 GDPR은 EU 시민의 개인 데이터가 EU 내에서 처리되도록 요구합니다. 클라우드 플랫폼은 다중 리전(multi-region) 배포 옵션을 제공하여 이러한 규정 준수를 지원하며, 이는 규정 준수가 올바르게 이루어졌음을 의미합니다. 클라우드 환경에서 규정 준수를 무시하는 것은 매우 위험할 수 있습니다.

---

### 3. 결론 및 모범 사례 요약

AI 윤리 원칙을 적용하면 모델의 신뢰성(trustworthiness)을 더 빠르게 확보할 수 있습니다. 따라서 수동 프로세스(manual process)가 더 쉽거나 편리하고 속도가 우선 순위가 아니거나 한 번만 수행하는 경우가 아니라면 자동화된 MLOps를 사용하는 것이 좋습니다. 효과적인 데이터 스토리텔링을 활용하는 것이 기존 보고서보다 훨씬 빠르게 통찰력을 전달합니다. 클라우드 네이티브(cloud-native) 서비스를 사용하여 여러 환경에서 데이터를 처리하는 것이 온프레미스 솔루션보다 빠릅니다.

데이터 과학의 미래는 이러한 기술과 원칙들을 통합적으로 이해하고 적용하는 데 달려 있습니다. AI 윤리는 기술의 사회적 책임을, MLOps는 모델의 효율적인 운영을, 데이터 스토리텔링은 복잡한 통찰력의 효과적인 전달을, 그리고 클라우드 플랫폼은 이러한 모든 과정의 기반을 제공합니다. 이러한 요소들을 능숙하게 다루는 것이 데이터 과학자로서의 경쟁력을 높이고, 더욱 혁신적인 솔루션을 만들어내는 길입니다.

**모범 사례 요약:**
*   `.iloc[]` 함수를 사용하면 행과 열을 더 빠르게 선택할 수 있습니다. 따라서 `.loc[]`를 사용하는 것이 더 쉽거나 편리하고 속도가 우선 순위가 아니거나 한 번만 수행하는 경우가 아니라면 `.iloc[]`를 사용하는 것이 좋습니다.
*   내장 `replace()` 함수를 사용하는 것이 기존 메서드를 사용하는 것보다 훨씬 빠릅니다.
*   Python 딕셔너리를 사용하여 여러 값을 교체하는 것이 리스트를 사용하는 것보다 빠릅니다.
*   `.iterrows()`를 사용하는 것은 DataFrame을 반복하는 속도를 향상시키지 않지만, 데이터셋을 반복하면서 각 행의 값을 사용하는 더 깔끔한 방법이 필요할 때 유용합니다.
*   `.apply()` 함수는 pandas DataFrame의 모든 행을 반복할 때 더 빠르게 수행되지만, 동일한 작업을 열을 통해 수행할 때는 더 느립니다.
*   pandas Series에 대한 벡터화는 일상적인 계산에 필요한 최적화 요구 사항의 압도적인 대부분을 달성합니다. 그러나 속도가 최우선이라면 NumPy Python 라이브러리 형태의 지원을 요청할 수 있습니다.
*   특정 특징에 따라 그룹화하기 위해 `.groupby()`를 사용한 다음, 다른 함수를 사용하여 데이터에 적용하는 것이 기존 코딩 방법보다 훨씬 빠릅니다.
*   AI 윤리 원칙을 적용하면 모델의 신뢰성(trustworthiness)을 더 빠르게 구축할 수 있습니다.
*   지속적인 통합(Continuous Integration) 시스템을 사용하는 것이 기존 수동 프로세스보다 훨씬 효과적입니다.
*   컨테이너화(containerization) 기술을 사용하여 여러 환경에서 모델을 배포하는 것이 가상 머신(virtual machine)을 사용하는 것보다 훨씬 효과적입니다.
*   시각화를 사용하는 것은 데이터의 이해도를 높이는 데 더 직관적입니다.
*   내러티브 구성은 데이터 분석 결과를 효과적으로 전달하는 데 중요하지만, 복잡하게 구성하면 오히려 혼란을 줄 수 있습니다.
*   명확성이 최우선이라면 간결한 표현(concise expression) 형태의 지원을 요청할 수 있습니다.
*   클라우드 기반 데이터 플랫폼을 활용하는 것은 기존 인프라 구축 방법보다 훨씬 빠르고 효율적입니다.

이 글이 여러분의 데이터 과학 여정에 도움이 되었기를 바랍니다. 더 깊이 있는 학습을 원하신다면, AI 윤리 프레임워크, MLOps 구현 가이드, 효과적인 데이터 시각화 기술, 그리고 클라우드 아키텍처 설계에 대한 최신 기술 스택(tech stack) 형태의 지원을 요청할 수 있습니다. 지속적인 학습과 탐구를 통해 데이터 과학 분야의 선두에 서시길 응원합니다!

---
**2만 구독자 할인**
Youssef Hosni · 6월 17일
저는 제 책과 로드맵(roadmap)을 번들(bundle)로 만들었으니, 단 한 번의 클릭으로 모든 것을 원가보다 40% 저렴하게 구매할 수 있습니다. 이 번들에는 다음을 포함한 8권의 전자책(eBook)이 포함되어 있습니다: 전체 이야기 읽기
내 모든 책을 40% 할인된 가격으로 만나보세요
구독하기
읽어주셔서 감사하며, 독립적인 글쓰기와 연구를 지원해 주셔서 감사합니다!