**AI 과학자의 허와 실: 연구 재현성을 위한 새로운 AI 평가 기준, CORE-Bench**

지난달, Sakana AI는 '완전 자동화된 과학적 탐구를 위한 첫 통합 시스템'으로 소개된 'AI 과학자'를 공개했습니다. 이 시스템은 인간의 제약 없이 과학 발전을 촉진할 수 있다고 홍보되었습니다. 하지만 이 'AI 과학자'는 여러 한계를 드러냈습니다. 이 시스템은 독창성 확인 절차가 미비하여, 만들어진 연구 결과물이 기존 연구의 단순한 반복일 가능성을 내포합니다. 더욱이 Sakana 측은 생성된 보고서에 대한 전문가의 동료 심사를 포함한 인간의 검토 과정을 전혀 거치지 않아, 그 유용성에 의문이 제기되고 있습니다. 이러한 문제점은 Sakana의 사례에서 특히 부각되지만, 적절한 평가 체계의 부재는 대부분의 AI 에이전트(AI agents)가 직면한 공통적인 난관이며, 이는 실제 환경에서의 영향력을 측정하기 어렵게 만듭니다. 오늘 우리는 AI가 기존의 계산 연구(computational research)를 얼마나 효과적으로 재현할 수 있는지 측정하기 위한 혁신적인 벤치마크(benchmark)를 소개합니다. 더불어 이 프로젝트가 '일반 지능(general intelligence)'과 AI의 잠재적인 경제적 파급 효과에 대한 우리의 인식을 어떻게 변화시켰는지도 함께 이야기하고자 합니다. 자세한 내용은 논문을 통해 확인하실 수 있습니다.

**CORE-Bench: 연구 재현을 위한 AI 평가의 새로운 벤치마크**

인공지능이 과학 연구 전반을 자동화할 것이라는 전망은 흥미롭지만, 현재로서는 실현 가능성이 낮고 자칫 부정확한 과학적 결과로 이어질 위험이 있습니다. 반면, 계산 재현성(computational reproducibility) 검증처럼 명확한 범위를 가진 특정 업무에 AI를 활용하는 것은 연구 시간을 크게 단축하고 과학자들이 보다 창의적인 활동에 집중할 수 있도록 돕습니다. AI는 관련 문헌을 탐색하고, 아이디어를 신속하게 검증하기 위한 코드를 생성하며, 다양한 계산 작업(computational tasks)을 수행하는 데 필수적인 조력자가 될 수 있습니다. 새로운 논문에서 우리는 AI가 계산 재현성(computational reproducibility)을 얼마나 효율적으로 자동화할 수 있는지, 즉 코드와 데이터가 주어졌을 때 논문의 결과를 재현하는 능력을 측정하는 벤치마크(benchmark)인 CORE-Bench (Computational Reproducibility Agent Benchmark)를 선보입니다. 이 프로젝트는 Zachary S. Siegel, Sayash Kapoor, Nitya Nadgir, Benedikt Stroebl, Arvind Narayanan이 공동 저술했습니다. CORE-Bench는 점진적으로 난이도가 높아지는 연구 과제를 자동화하는 AI의 진보를 엄밀하게 평가하기 위한 더 큰 규모의 장기 프로젝트의 첫걸음입니다.

연구의 계산적 재현은 인간 대상자가 개입될 수 있는 실험을 다시 수행해야 하는 복제(replication)에 비해 훨씬 범위가 좁은 작업입니다. 그럼에도 불구하고, 이러한 제한적인 재현성 작업조차도 쉽지 않습니다. 연구 결과의 재현 불가능성은 과학계에 막대한 비용을 초래합니다. 이는 연구 자금의 낭비, 신뢰할 수 없는 결과의 확산, 심지어는 신약 개발과 같은 중요한 분야에서의 지연으로 이어질 수 있습니다. 2022년 기계 학습 재현성 챌린지(Machine Learning Reproducibility Challenge)에서는 전문가들이 코드와 데이터를 모두 가지고 있었음에도 불구하고 논문의 3분의 1 이상이 재현되지 않았습니다. 만약 AI가 이처럼 평범하지만 과학적 무결성에 필수적인 작업을 자동화할 수 있다면, 연구자들은 기준선(baselines) 구현에 드는 수고를 덜고, 검토자들은 논문의 결함을 더 쉽게 파악할 수 있으며, 학술지와 학회는 제출 및 출판된 논문이 재현 가능한지 훨씬 수월하게 확인할 수 있을 것입니다.

CORE-Bench 개발을 위해 우리는 과학 논문과 함께 제공되는 코드 및 데이터 저장소(repositories)를 활용했습니다. 특히 Code Ocean 플랫폼을 통해 재현성이 높은 논문들을 선별하여 확보했습니다. 우리는 컴퓨터 과학, 의학, 사회 과학 분야의 90개 논문을 수동으로 재현했으며, 각 논문에 대해 AI의 답변을 검증할 수 있도록 일련의 질문을 신중하게 선별했습니다. CORE-Bench는 세 가지 난이도 수준으로 구성됩니다. 이 세 가지 수준의 모든 작업은 언어 및 시각 능력(language and vision capabilities)을 모두 요구합니다. 가장 어려운 버전은 실제 연구 재현 시도와 매우 흡사하며, 이 벤치마크(benchmark)의 개선이 과학자들에게 실제로 유용한 에이전트(agents) 개발로 이어질 것이라고 기대합니다. 기준선(baselines) 구현을 위해 우리는 범용 AutoGPT 에이전트(generalist AutoGPT agent)를 테스트했고, AutoGPT에 대한 작업별 수정(task-specific modification)을 적용한 CORE-Agent를 개발했습니다. 작업별 버전은 정확도를 크게 향상시켰지만, 여전히 개선의 여지가 많습니다. GPT-4o를 활용한 최고의 에이전트(CORE-Agent)는 CORE-Bench-Hard에서 22%의 정확도를 기록했습니다. 이는 현재 AI의 한계를 보여주지만, 동시에 특정 목적에 맞춰 조정되었을 때 AI가 발휘할 수 있는 잠재력을 시사합니다.

**일반성(generality) 재고**

계산 재현성(computational reproducibility)은 코드 환경을 올바르게 설정하고, 코드를 실행하며, 논문에 보고된 것과 동일한 결과를 생성하는지 확인하는 것을 요구합니다. 셸(shell)이나 다른 유틸리티를 정확하게 다루는 것은 대규모 언어 모델(LLMs)에게 여전히 어려운 과제입니다. 실제로 AutoGPT와 같은 범용 에이전트(generalist agents)를 평가했을 때, CORE-Bench-Hard에서 10% 미만에 불과한 낮은 정확도는 예상했던 바입니다. 그러나 불과 며칠간의 인력 투입으로 우리는 AutoGPT를 수정하여 CORE-Agent를 구축할 수 있었고, 이는 가장 어려운 수준에서 정확도를 두 배 이상 높였습니다. 우리는 또한 처음부터 작업별 에이전트(task-specific agent)를 구축했지만, AutoGPT를 수정하는 것이 훨씬 적은 시간으로도 더 강력한 에이전트를 만들 수 있었습니다. 우리는 이 접근 방식이 실제로 유용할 만큼 충분히 잘 작동하는 에이전트(agents)를 만들어낼 수 있을 것이라고 조심스럽게 낙관합니다. 간단한 작업별 수정(task-specific modifications)을 통해 CORE-Agent는 범용 AutoGPT를 능가할 수 있습니다. 만약 범용 에이전트(generalist agent)를 작업별 에이전트(task-specific agent)로 쉽게 개조할 수 있는 이러한 패턴이 다른 분야에서도 유효하다면, 우리는 '일반성(generality)'이라는 개념에 대해 다시 깊이 생각해야 합니다.

'일반성(generality)'이란 대략적으로 하나의 모델이나 에이전트(agent)가 별도의 변경 없이 여러 유형의 작업을 처리할 수 있는 역량을 의미합니다. 이러한 일반성 개념은 인공 일반 지능(AGI)에 대한 일반적인 인식과 그에 따르는 기대 및 우려의 핵심을 형성합니다. 그러나 적어도 경제적 파급 효과의 관점에서 볼 때, 일반성은 오해를 불러일으킬 수 있는 개념일 수 있습니다. 전문가들이 매년 수백만 시간을 들이는 계산 재현성(computational reproducibility)과 같은 작업의 경우, AI 시스템이 즉시(out of the box) 수행하든, 아니면 며칠(심지어 1년)의 프로그래머 노력을 거쳐 수행하든 상관없이, 이를 자동화할 수 있다면 엄청난 영향력을 가질 것입니다. "AI Snake Oil" 책에서 우리는 일반성(generality)을 작업 특이성(task-specificity)의 역으로 정의하고, AI(및 컴퓨팅)의 역사가 점진적으로 일반성(generality)을 증가시키려는 추구로 어떻게 볼 수 있는지 분석합니다. 일반성을 높이는 것은 주어진 작업을 수행하기 위한 AI 시스템을 구축하는 데 필요한 인간의 노력을 줄이는 것을 의미합니다. 이러한 관점에서 볼 때, AutoGPT와 같은 시스템은 대부분의 사람들(우리 포함)이 생각했던 것보다 더 일반적(general)일 수 있습니다. 하지만, AGI의 정의는 일반적으로 단일 시스템이 즉시(out of the box) 모든 것을 할 수 있어야 한다고 주장합니다. 작업별 AI(task-specific AI)를 구축하는 데 필요한 인간의 노력이 시간이 지남에 따라 어떻게 변하는지 추적하는 체계적인 노력은 없습니다. AI의 발전을 과대평가하는 일반성에 대한 결함 있는 개념에 반대했던 것처럼, 우리는 AI의 발전을 과소평가하는 일반성에 대한 결함 있는 개념도 피해야 합니다. CORE-Bench 논문을 여기에서 읽어보세요.

**추가 자료**

최근 논문인 "AI Agents That Matter"에서 우리는 AI 에이전트(AI agent) 평가의 여러 단점을 발견했습니다. 기존 평가 방식은 종종 실제 문제의 복잡성을 반영하지 못하고, 제한된 지표에 의존하여 에이전트의 실제 가치를 제대로 측정하지 못하는 경향이 있습니다. CORE-Bench를 구축하는 동안 이러한 단점들이 우리 벤치마크(benchmark) 설계에 영향을 미쳤습니다. 우리는 최근 유용하고 신뢰할 수 있는 AI 에이전트(AI agents)에 대한 온라인 워크숍을 개최했으며, 여기서 주요 전문가들이 더 나은 에이전트 설계 및 평가에 대한 견해와 미래 방향을 공유했습니다. 워크숍 영상은 온라인에서 시청할 수 있습니다.

Ben Bogin 외 연구진은 AI 에이전트(AI agents)가 연구 논문에 수반되는 저장소(repositories)에서 작업을 설정하고 실행할 수 있는지 평가하기 위해 SUPER 벤치마크(benchmark)를 출시했습니다. 이는 AI 에이전트(AI agents)의 연구 작업 자동화 능력을 측정하는 또 다른 흥미로운 벤치마크(benchmark)입니다. CORE-Bench는 SUPER와 여러 면에서 차이점을 보입니다. CORE-Bench는 컴퓨터 과학, 의학, 사회 과학 등 다양한 과학 분야의 작업으로 구성되어 범용성을 추구하는 반면, SUPER는 주로 AI 분야의 작업에 초점을 맞춥니다. 또한 CORE-Bench는 시각-언어(vision-language) 및 언어 모델(language models)을 모두 활용해야 하며, SUPER(언어 모델, Python 위주)와 달리 Python 및 R을 포함한 여러 프로그래밍 언어를 다룹니다. SUPER의 작업은 주피터 노트북(Jupyter notebook)에 대한 접근을 필요로 하는 반면, CORE-Bench의 작업은 셸(shell) 접근을 요구하며 에이전트(agent)가 샌드박스(sandbox) 환경을 자유롭게 수정할 수 있도록 허용하여, 실제 연구 환경에 더 가까운 도전 과제를 제시합니다. 이러한 차이점들은 각 벤치마크가 AI 에이전트의 어떤 역량을 평가하고자 하는지 명확히 보여줍니다. 앞으로 AI가 과학 연구에 더욱 깊이 통합되기 위해서는 이처럼 실질적인 환경을 반영한 평가 도구의 개발과 지속적인 개선이 필수적입니다.