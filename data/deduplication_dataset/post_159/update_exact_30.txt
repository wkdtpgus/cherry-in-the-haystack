지난달, Sakana AI는 "완전 자동 과학적 발견을 위한 최초의 종합 시스템"이라고 불리는 "AI 과학자"를 출시했습니다. 이는 인간의 한계에 구애받지 않고 과학을 가속화할 수 있다고 선전되었습니다. 불행히도, 이 "AI 과학자"는 많은 단점을 가지고 있습니다. 독창성에 대한 검증 기능이 없어 생성된 논문이 이전 작업을 재탕할 수 있습니다. 그리고 Sakana는 생성된 논문에 대해 어떠한 인간 검토(전문가 "동료" 검토는 물론)도 수행하지 않았으므로, 해당 논문이 얼마나 유용한지 불분명합니다(명백히 그렇지 않은 것으로 보입니다). 이러한 결함은 Sakana의 경우 특히 두드러지지만, 적절한 평가의 부재는 대부분의 AI 에이전트(AI agents)에 영향을 미쳐 실제 세계에서의 영향력을 측정하기 어렵게 만듭니다. 오늘, 우리는 AI가 기존의 계산 연구(computational research)를 얼마나 잘 재현할 수 있는지 측정하기 위한 새로운 벤치마크(benchmark)를 소개합니다. 또한 이 프로젝트가 "일반 지능(general intelligence)"과 AI의 잠재적인 경제적 영향에 대한 우리의 생각을 어떻게 변화시켰는지 공유합니다. 논문을 읽어보세요.

최근 몇 년간 인공지능은 과학 연구의 다양한 분야에서 혁신적인 도구로 부상했습니다. 실험 설계, 데이터 분석, 가설 생성에 이르기까지 AI의 역할은 점점 더 커지고 있습니다. 하지만 이러한 발전 이면에는 AI가 생성하는 결과물의 신뢰성과 유효성을 어떻게 평가할 것인가라는 중요한 질문이 남아 있습니다. 특히, 기존 연구의 재현성은 과학적 진보의 초석임에도 불구하고, AI 에이전트가 이 복잡한 과정을 얼마나 효과적으로 수행할 수 있는지에 대한 체계적인 평가는 부족했습니다. 단순히 새로운 것을 생성하는 것을 넘어, 기존의 과학적 지식을 정확하게 검증하고 재확인하는 AI의 능력은 과학의 무결성을 유지하는 데 필수적입니다. 이러한 맥락에서, 계산 연구의 재현성을 위한 새로운 벤치마크의 중요성은 더욱 강조됩니다.

**CORE-Bench: 연구 재현을 위한 AI 평가의 새로운 벤치마크**

AI가 과학을 자동화한다는 비전은 매력적이지만, 아직 실현 가능성이 낮으며 결함 있는 과학으로 이어질 수 있습니다. 이와 대조적으로, 계산 재현성(computational reproducibility) 검증과 같이 범위가 명확한 작업에 AI를 사용하는 것은 많은 시간을 절약하고 노력을 더 생산적인 과학 활동으로 전환할 수 있습니다. AI는 또한 관련 문헌을 찾고, 아이디어를 신속하게 테스트하기 위한 코드를 작성하며, 다른 계산 작업(computational tasks)을 수행하는 데 도움을 줄 수 있습니다. 새로운 논문에서 우리는 AI가 계산 재현성(computational reproducibility)을 얼마나 잘 자동화할 수 있는지, 즉 코드와 데이터가 제공될 때 논문의 결과를 재현하는 능력을 측정하기 위한 벤치마크(benchmark)인 CORE-Bench (Computational Reproducibility Agent Benchmark)를 소개합니다. 저자는 Zachary S. Siegel, Sayash Kapoor, Nitya Nadgir, Benedikt Stroebl, Arvind Narayanan입니다. CORE-Bench는 난이도가 증가하는 연구 작업을 자동화하는 데 있어 진행 상황을 엄격하게 평가하기 위한 더 큰 프로젝트의 첫걸음입니다.

계산 재현성은 단순히 코드를 실행하는 것 이상의 복잡한 과정을 포함합니다. 적절한 소프트웨어 환경 설정, 라이브러리 종속성 관리, 데이터 전처리, 그리고 때로는 코드 디버깅까지 요구됩니다. 인간 연구자에게도 이러한 과정은 많은 시간과 노력이 소요되며, 종종 예상치 못한 문제에 직면하기도 합니다. 예를 들어, 특정 운영 체제나 라이브러리 버전에서만 작동하는 코드, 불완전하거나 모호한 지침, 또는 숨겨진 데이터 처리 단계 등은 재현성을 가로막는 주요 요인입니다. 2022년 기계 학습 재현성 챌린지(Machine Learning Reproducibility Challenge)에서 전문가들이 코드와 데이터를 가지고 있었음에도 불구하고 논문의 3분의 1 이상이 재현되지 않았다는 사실은 이러한 어려움을 여실히 보여줍니다. 만약 AI가 이 평범하지만 중요한 작업을 자동화할 수 있다면, 연구자들은 반복적인 기준선(baselines) 구현에서 벗어나 창의적인 연구에 집중할 수 있을 것입니다. 또한, 학술지 및 학회는 제출된 논문의 재현성을 더욱 쉽게 검증할 수 있게 되어 과학적 결과의 신뢰도를 높일 수 있습니다.

CORE-Bench는 이러한 문제를 해결하기 위해 실제 과학 논문과 그에 수반되는 코드 및 데이터 저장소(repositories)를 기반으로 구축되었습니다. 우리는 Code Ocean과 같은 플랫폼을 활용하여 재현 가능성이 높은 논문들을 확보했으며, 컴퓨터 과학, 의학, 사회 과학 분야의 90개 논문을 수동으로 재현하여 각 논문에 대한 검증 가능한 질문 세트를 만들었습니다. 이 질문들은 AI 에이전트가 논문의 주요 결과물을 성공적으로 재현했는지 객관적으로 평가할 수 있도록 설계되었습니다. CORE-Bench는 세 가지 난이도 수준(Easy, Medium, Hard)으로 구성되어 있으며, 모든 작업은 언어 및 시각 능력(language and vision capabilities)을 모두 요구합니다. 가장 어려운 버전은 실제 연구 재현 시나리오와 매우 유사하며, AI가 환경 설정, 코드 실행, 결과 분석 및 보고된 결과와의 비교에 이르는 모든 단계를 독립적으로 수행하도록 요구합니다.

초기 평가를 위해 우리는 범용 AutoGPT 에이전트(generalist AutoGPT agent)와 이를 계산 재현성 작업에 특화시킨 CORE-Agent를 테스트했습니다. 범용 에이전트의 초기 성능은 CORE-Bench-Hard에서 10% 미만의 낮은 정확도를 보였지만, CORE-Agent는 GPT-4o를 기반으로 했을 때 22%의 정확도를 달성하여 상당한 개선을 보여주었습니다. 이는 여전히 개선의 여지가 많음을 시사하지만, 작업 특화(task-specific) 접근 방식의 잠재력을 명확히 보여줍니다. AI 에이전트가 다양한 라이브러리 버전, 운영 체제 환경, 그리고 모호한 코드 문서화 문제를 해결하는 능력은 여전히 큰 과제로 남아 있습니다.

**일반성(generality) 재고**

최근 AI 에이전트 분야에서는 AutoGPT와 같은 '범용 에이전트'에 대한 기대가 높았습니다. 이들은 다양한 작업을 수행할 수 있는 잠재력을 가졌다고 여겨졌지만, CORE-Bench에서의 초기 성능은 복잡한 실제 과학 작업에서 '즉시 사용 가능한(out-of-the-box)' 일반성의 한계를 드러냈습니다. 계산 재현성 작업은 셸(shell) 환경과의 상호작용, 다양한 도구 사용, 그리고 코드 디버깅 능력을 요구하는데, 이는 대규모 언어 모델(LLMs)에게 여전히 까다로운 영역입니다. 이러한 결과는 AI의 '일반 지능'에 대한 우리의 이해를 재고할 필요가 있음을 시사합니다.

일반성(generality)은 대략적으로 동일한 모델 또는 에이전트(agent)를 수정 없이 다양한 작업을 수행하는 데 사용할 수 있는 능력을 의미합니다. 이러한 일반성(generality) 개념은 인공 일반 지능(Artificial General Intelligence, 또는 AGI)이 일반적으로 이해되는 방식과 그에 수반되는 희망과 두려움의 근간을 이룹니다. 그러나 적어도 경제적 영향의 관점에서 볼 때, 일반성(generality)은 잘못된 방향으로 이끄는 것일 수 있습니다. 전문가들이 매년 수백만 시간을 들이는 계산 재현성(computational reproducibility)과 같은 작업의 경우, AI 시스템이 즉시(out of the box) 수행하든, 아니면 며칠(심지어 1년)의 프로그래머 노력을 거쳐 수행하든 상관없이, 이를 자동화할 수 있다면 엄청난 영향력을 가질 것입니다. "AI Snake Oil" 책에서 우리는 일반성(generality)을 작업 특이성(task-specificity)의 역으로 정의하고, AI(및 컴퓨팅)의 역사가 점진적으로 일반성(generality)을 증가시키려는 추구로 어떻게 볼 수 있는지 분석합니다. 일반성(generality)을 높이는 것은 주어진 작업을 수행하기 위한 AI 시스템을 구축하는 데 필요한 인간의 노력을 줄이는 것을 의미합니다. 이러한 관점에서 볼 때, AutoGPT와 같은 시스템은 대부분의 사람들(우리 포함)이 생각했던 것보다 더 일반적(general)일 수 있습니다. 하지만, AGI의 정의는 일반적으로 단일 시스템이 즉시(out of the box) 모든 것을 할 수 있어야 한다고 주장합니다. 작업별 AI(task-specific AI)를 구축하는 데 필요한 인간의 노력이 시간이 지남에 따라 어떻게 변하는지 추적하는 체계적인 노력은 없습니다. AI의 발전을 과대평가하는 일반성(generality)에 대한 결함 있는 개념에 반대했던 것처럼, 우리는 AI의 발전을 과소평가하는 일반성(generality)에 대한 결함 있는 개념도 피해야 합니다. CORE-Bench 논문을 여기에서 읽어보세요.

CORE-Agent는 AutoGPT에 프로그래밍 도구, 환경 설정 스크립트, 오류 진단 및 디버깅 가이드라인 등 특정 작업을 위한 맞춤형 프롬프트와 도구들을 통합하여 개발되었습니다. 이러한 '작업별 수정(task-specific modifications)'은 범용 에이전트의 잠재력을 특정 도메인에서 극대화하는 중요한 전략이었습니다. 이는 AI의 발전 방향이 단일하고 완벽한 일반 지능을 향하기보다는, 특정 문제 해결을 위해 유연하게 조정되고 확장될 수 있는 '적응형 일반주의(adaptable generalism)'에 더 가까울 수 있음을 시사합니다. 즉, AI는 특정 도메인 지식과 도구 사용 능력을 학습하여 인간 전문가의 역할을 보완하고, 궁극적으로는 협력하여 더 큰 효율성을 창출할 수 있습니다.

이러한 접근 방식은 AI의 경제적 영향에 대한 우리의 시각을 변화시킵니다. 수백만 시간이 소요되는 계산 재현성 같은 중요한 작업을 AI가 자동화할 수 있다면, 그것이 순수한 AGI에 의한 것이든, 아니면 며칠간의 엔지니어링 노력으로 특화된 AI 에이전트에 의한 것이든 상관없이 엄청난 가치를 창출할 것입니다. 이는 AI의 진정한 가치가 '범용성' 그 자체에 있기보다는, 중요한 문제들을 효율적으로 해결하는 '문제 해결 능력'에 있음을 강조합니다. 따라서 우리는 AI의 발전을 평가할 때, 단지 이론적인 일반성보다는 실제적인 적용 가능성과 영향력에 더 초점을 맞춰야 할 것입니다.

**추가 자료 및 향후 전망**

CORE-Bench 외에도 AI 에이전트의 과학 연구 능력을 평가하기 위한 다양한 노력이 진행 중입니다. 예를 들어, Ben Bogin 외 연구진이 출시한 SUPER 벤치마크(benchmark)는 AI 에이전트가 연구 논문에 수반되는 저장소(repositories)에서 작업을 설정하고 실행할 수 있는지 평가합니다. CORE-Bench가 컴퓨터 과학, 의학, 사회 과학 등 광범위한 과학 분야를 다루며 시각-언어 모델과 여러 프로그래밍 언어를 사용하는 반면, SUPER는 주로 AI 분야의 파이썬 기반 작업에 초점을 맞추고 주피터 노트북(Jupyter notebook) 환경을 활용한다는 점에서 차이가 있습니다. 이러한 다양한 벤치마크의 등장은 AI 에이전트의 성능을 다각도로 평가하고, 과학적 발견 과정을 더욱 가속화할 수 있는 방향으로 이끌 것입니다.

미래에는 AI 에이전트가 단순히 기존 연구를 재현하는 것을 넘어, 새로운 가설을 생성하고, 실험을 설계하며, 심지어 과학 논문을 초안 작성하는 데까지 기여할 것으로 기대됩니다. 하지만 이러한 발전은 항상 인간 전문가의 엄격한 검토와 윤리적 가이드라인 속에서 이루어져야 합니다. CORE-Bench와 같은 벤치마크는 AI 에이전트의 신뢰성과 효율성을 측정하는 중요한 도구로서, 인간과 AI가 협력하여 과학의 새로운 지평을 여는 데 필수적인 역할을 할 것입니다.