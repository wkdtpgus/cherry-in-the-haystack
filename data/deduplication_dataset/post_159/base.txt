# AI가 계산 재현성(computational reproducibility)을 자동화할 수 있을까?

Author: AI Snake Oil
URL: https://www.normaltech.ai/p/can-ai-automate-computational-reproducibility

============================================================

지난달, Sakana AI는 "완전 자동 과학적 발견을 위한 최초의 종합 시스템"이라고 불리는 "AI 과학자"를 출시했습니다. 이는 인간의 한계에 구애받지 않고 과학을 가속화할 수 있다고 선전되었습니다. 불행히도, 이 "AI 과학자"는 많은 단점을 가지고 있습니다. 독창성에 대한 검증 기능이 없어 생성된 논문이 이전 작업을 재탕할 수 있습니다. 그리고 Sakana는 생성된 논문에 대해 어떠한 인간 검토(전문가 "동료" 검토는 물론)도 수행하지 않았으므로, 해당 논문이 얼마나 유용한지 불분명합니다(명백히 그렇지 않은 것으로 보입니다). 이러한 결함은 Sakana의 경우 특히 두드러지지만, 적절한 평가의 부재는 대부분의 AI 에이전트(AI agents)에 영향을 미쳐 실제 세계에서의 영향력을 측정하기 어렵게 만듭니다. 오늘, 우리는 AI가 기존의 계산 연구(computational research)를 얼마나 잘 재현할 수 있는지 측정하기 위한 새로운 벤치마크(benchmark)를 소개합니다. 또한 이 프로젝트가 "일반 지능(general intelligence)"과 AI의 잠재적인 경제적 영향에 대한 우리의 생각을 어떻게 변화시켰는지 공유합니다. 논문을 읽어보세요.

**CORE-Bench: 연구 재현을 위한 AI 평가의 새로운 벤치마크**

AI가 과학을 자동화한다는 비전은 매력적이지만, 아직 실현 가능성이 낮으며 결함 있는 과학으로 이어질 수 있습니다. 이와 대조적으로, 계산 재현성(computational reproducibility) 검증과 같이 범위가 명확한 작업에 AI를 사용하는 것은 많은 시간을 절약하고 노력을 더 생산적인 과학 활동으로 전환할 수 있습니다. AI는 또한 관련 문헌을 찾고, 아이디어를 신속하게 테스트하기 위한 코드를 작성하며, 다른 계산 작업(computational tasks)을 수행하는 데 도움을 줄 수 있습니다. 새로운 논문에서 우리는 AI가 계산 재현성(computational reproducibility)을 얼마나 잘 자동화할 수 있는지, 즉 코드와 데이터가 제공될 때 논문의 결과를 재현하는 능력을 측정하기 위한 벤치마크(benchmark)인 CORE-Bench (Computational Reproducibility Agent Benchmark)를 소개합니다. 저자는 Zachary S. Siegel, Sayash Kapoor, Nitya Nadgir, Benedikt Stroebl, Arvind Narayanan입니다. CORE-Bench는 난이도가 증가하는 연구 작업을 자동화하는 데 있어 진행 상황을 엄격하게 평가하기 위한 더 큰 프로젝트의 첫걸음입니다.

연구를 계산적으로 재현하는 것은 인간 피험자가 포함될 수 있는 실험을 재실행해야 하는 복제(replication)보다 훨씬 제한적인 작업입니다. 제한적인 재현성 작업조차 어렵습니다. 2022년 기계 학습 재현성 챌린지(Machine Learning Reproducibility Challenge)에서는 논문을 재현하는 전문가들이 코드와 데이터를 가지고 있었음에도 불구하고 논문의 3분의 1 이상이 재현되지 않았습니다. 만약 AI가 이 평범하지만 중요한 작업을 자동화할 수 있다면, 연구자들은 기준선(baselines) 구현을 자동화하고, 검토자들은 논문에 결함이 있는지 더 쉽게 평가할 수 있으며, 학술지와 학회는 제출 및 출판된 논문이 재현 가능한지 더 쉽게 확인할 수 있을 것입니다.

우리는 과학 논문과 그에 수반되는 코드 및 데이터 저장소(repositories)를 사용하여 CORE-Bench를 만들었습니다. 우리는 Code Ocean을 사용하여 재현 가능성이 높은 논문을 확보했습니다. 우리는 컴퓨터 과학, 의학, 사회 과학 분야의 90개 논문을 수동으로 재현했으며, 각 논문에 대해 답변을 검증할 수 있도록 일련의 질문을 선별했습니다. 우리는 CORE-Bench를 세 가지 난이도 수준으로 출시합니다. 세 가지 수준의 모든 작업은 언어 및 시각 능력(language and vision capabilities)을 모두 사용해야 합니다. 가장 어려운 버전은 실제 재현 시도와 매우 유사하며, 벤치마크(benchmark) 개선이 과학자들에게 실제로 유용한 에이전트(agents)로 이어질 것으로 기대합니다. 기준선(baselines)을 구현하기 위해 우리는 범용 AutoGPT 에이전트(generalist AutoGPT agent)를 테스트했으며, AutoGPT에 대한 작업별 수정(task-specific modification)을 구현했는데, 이를 CORE-Agent라고 부릅니다. 작업별 버전이 정확도를 크게 향상시켰지만, 여전히 개선의 여지가 많습니다. 최고의 에이전트(GPT-4o를 사용한 CORE-Agent)는 CORE-Bench-Hard에서 22%의 정확도를 보였습니다.

**일반성(generality) 재고**

계산 재현성(computational reproducibility)은 코드 환경을 올바르게 설정하고, 코드를 실행하며, 논문에 보고된 것과 동일한 결과를 생성하는지 확인하는 것을 요구합니다. 셸(shell) 및 기타 도구를 올바르게 사용하는 것은 대규모 언어 모델(LLMs)에게 여전히 까다로운 일입니다. 우리가 AutoGPT와 같은 범용 에이전트(generalist agents)를 평가했을 때, 그들의 낮은 정확도(CORE-Bench-Hard에서 10% 미만)에 놀라지 않았습니다. 하지만, 며칠간의 인력 투입으로 우리는 AutoGPT를 수정하여 CORE-Agent를 구축할 수 있었고, 이는 가장 어려운 수준에서 정확도를 두 배 이상 높였습니다. 우리는 또한 처음부터 작업별 에이전트(task-specific agent)를 구축했지만, AutoGPT를 수정하는 것이 훨씬 적은 시간이 소요되면서도 더 강력한 에이전트를 만들었습니다. 우리는 이 접근 방식이 실제로 유용할 만큼 충분히 잘 작동하는 에이전트(agents)를 만들어낼 수 있을 것이라고 조심스럽게 낙관합니다. 간단한 작업별 수정(task-specific modifications)을 통해 CORE-Agent는 AutoGPT를 능가할 수 있습니다. 만약 범용 에이전트(generalist agent)를 작업별 에이전트(task-specific agent)로 쉽게 개조할 수 있는 이러한 패턴이 다른 분야에서도 유효하다면, 우리는 일반성(generality)에 대해 다시 생각해야 합니다.

일반성(generality)은 대략적으로 동일한 모델 또는 에이전트(agent)를 수정 없이 다양한 작업을 수행하는 데 사용할 수 있는 능력을 의미합니다. 이러한 일반성(generality) 개념은 인공 일반 지능(Artificial General Intelligence, 또는 AGI)이 일반적으로 이해되는 방식과 그에 수반되는 희망과 두려움의 근간을 이룹니다. 그러나 적어도 경제적 영향의 관점에서 볼 때, 일반성(generality)은 잘못된 방향으로 이끄는 것일 수 있습니다. 전문가들이 매년 수백만 시간을 들이는 계산 재현성(computational reproducibility)과 같은 작업의 경우, AI 시스템이 즉시(out of the box) 수행하든, 아니면 며칠(심지어 1년)의 프로그래머 노력을 거쳐 수행하든 상관없이, 이를 자동화할 수 있다면 엄청난 영향력을 가질 것입니다. "AI Snake Oil" 책에서 우리는 일반성(generality)을 작업 특이성(task-specificity)의 역으로 정의하고, AI(및 컴퓨팅)의 역사가 점진적으로 일반성(generality)을 증가시키려는 추구로 어떻게 볼 수 있는지 분석합니다. 일반성(generality)을 높이는 것은 주어진 작업을 수행하기 위한 AI 시스템을 구축하는 데 필요한 인간의 노력을 줄이는 것을 의미합니다. 이러한 관점에서 볼 때, AutoGPT와 같은 시스템은 대부분의 사람들(우리 포함)이 생각했던 것보다 더 일반적(general)일 수 있습니다. 하지만, AGI의 정의는 일반적으로 단일 시스템이 즉시(out of the box) 모든 것을 할 수 있어야 한다고 주장합니다. 작업별 AI(task-specific AI)를 구축하는 데 필요한 인간의 노력이 시간이 지남에 따라 어떻게 변하는지 추적하는 체계적인 노력은 없습니다. AI의 발전을 과대평가하는 일반성(generality)에 대한 결함 있는 개념에 반대했던 것처럼, 우리는 AI의 발전을 과소평가하는 일반성(generality)에 대한 결함 있는 개념도 피해야 합니다. CORE-Bench 논문을 여기에서 읽어보세요.

**추가 자료**

최근 논문인 "AI Agents That Matter"에서 우리는 AI 에이전트(AI agent) 평가의 여러 단점을 발견했습니다. CORE-Bench를 구축하는 동안 이러한 단점들이 우리 벤치마크(benchmark) 설계에 영향을 미쳤습니다. 우리는 최근 유용하고 신뢰할 수 있는 AI 에이전트(AI agents)에 대한 온라인 워크숍을 개최했으며, 여기서 주요 전문가들이 더 나은 에이전트 설계 및 평가에 대한 견해를 공유했습니다. 워크숍 영상은 온라인에서 시청할 수 있습니다.

Ben Bogin 외 연구진은 AI 에이전트(AI agents)가 연구 논문에 수반되는 저장소(repositories)에서 작업을 설정하고 실행할 수 있는지 평가하기 위해 SUPER 벤치마크(benchmark)를 출시했습니다. 이는 AI 에이전트(AI agents)의 연구 작업 자동화 능력을 측정하는 또 다른 흥미로운 벤치마크(benchmark)입니다. 이는 CORE-Bench와 여러 면에서 다릅니다. CORE-Bench는 과학 분야(컴퓨터 과학, 의학, 사회 과학) 전반의 작업으로 구성된 반면, SUPER는 AI 분야의 작업으로 구성됩니다. CORE-Bench는 시각-언어(vision-language) 및 언어 모델(language models)을 모두 사용해야 하며, SUPER(언어 모델, Python)와 달리 여러 언어(Python 및 R)로 구성됩니다. SUPER의 작업은 주피터 노트북(Jupyter notebook)에 대한 접근을 필요로 합니다. 이와 대조적으로, CORE-Bench의 작업은 셸(shell) 접근을 필요로 하며 에이전트(agent)가 샌드박스(sandbox)를 임의로 수정할 수 있도록 허용합니다.