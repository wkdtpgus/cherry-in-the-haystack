1.  **확장 가능한 월드 모델(world model) 내 에이전트(agent) 훈련의 진화**
    빠르고 정확한 마인크래프트(Minecraft) 시뮬레이터(simulator)를 학습하고 완전히 오프라인(offline)으로 제어 가능한 에이전트(agent)를 훈련하는 확장 가능한 상상-RL 레시피(imagination-RL recipe)는 여전히 유효한 접근 방식입니다. 이 월드 모델(world model)은 단일 GPU에서 실시간 대화형 롤아웃(rollout)을 지원하며, 원시 픽셀(raw pixel)과 저수준 마우스(mouse) 및 키보드(keyboard)로부터 순수 오프라인(offline) "다이아몬드 획득" 결과를 가능하게 했습니다. 이 방법론의 핵심은 속도와 안정성을 위해 구축된 인과 토크나이저(causal tokenizer)와 블록-인과 동역학 트랜스포머(block-causal dynamics transformer)입니다. 숏컷 포싱(shortcut forcing)은 램프 손실(ramped loss)을 사용하여 x-공간에서 예측하면서 모델이 큰 디노이징(denoising) 단계(K=4)를 수행하도록 훈련시킵니다. 이는 누적 오류를 줄이고 낮은 단계 수에서도 품질을 유지하는 데 기여했습니다. 공간 전용 및 시간 전용 어텐션 레이어(attention layer), 4단계마다 한 번씩 시간 레이어(temporal layer), GQA, 길고 짧은 배치(batch) 교차는 KV 캐시(cache)를 작게 유지하고 추론(inference)을 빠르게 합니다. 이는 시각적 요소뿐만 아니라 메커니즘(mechanics)까지 처리하는 실시간, 더 긴 컨텍스트(context) 월드 모델(world model)을 가능하게 했습니다. 640×360 해상도에서 9.6초 컨텍스트(context)로 20+ FPS의 대화형 추론(inference)을 제공하며, 이는 이전 마인크래프트(Minecraft) 모델보다 상당히 길었습니다. 16개 작업에 걸친 휴먼-인-더-루프(human-in-the-loop) 플레이 테스트(play test)에서 Dreamer 4는 14개 작업에서 성공하며, 블록(block)을 올바르게 배치/파괴하고, 도구를 전환하고, 보트(boat)를 타고, 화로(furnace)를 사용하고, 포털(portal)에 진입하는 반면, Oasis/Lucid는 많은 객체 상호작용 작업에서 실패했습니다. 오프라인 다이아몬드 챌린지(Offline Diamond Challenge), 즉 환경 상호작용 없이 진행된 테스트에서 2.5K 시간 VPT 계약자 데이터셋(dataset)으로만 훈련된 에이전트(agent)는 작업 토큰(task token)에 조건화(condition)되고 상상 RL(imagination RL)(행동 사전(behavioral prior)을 가진 PMPO)을 통해 개선됩니다. 60분 에피소드(episode) 내에 철 곡괭이(iron pickaxe)에 29%, 다이아몬드(diamond)에 0.7% 도달하며, VPT(파인튜닝(finetuning)됨) 및 Gemma-3 VLA와 같은 강력한 오프라인 기준선(baseline)을 능가하는 동시에 YouTube 사전 훈련(pre-trained)된 VPT 파이프라인(pipeline)보다 약 100배 적은 데이터를 사용합니다. 적은 쌍 데이터(paired data)로부터의 액션 그라운딩(action grounding)은 OOD(Out-of-Distribution)로 일반화되어 놀라운 결과를 보였습니다. 2,541시간의 비디오(video) 중 액션(action)이 있는 것은 100시간뿐이지만, 모델은 액션 조건부 예측(action-conditioned prediction)에서 전체 액션 모델(full-action model)의 약 85% PSNR과 100% SSIM에 도달합니다. 오버월드(Overworld)에서만 훈련된 액션 조건화(action conditioning)는 액션 없이 본 네더(Nether)/엔드(End) 장면에 전이되어, 전체 액션 모델(all-actions model)의 약 76% PSNR과 80% SSIM을 달성합니다. 모델과 일관성을 유지하는 에이전트 파인튜닝(finetuning) 및 상상 RL(imagination RL): 작업 토큰(task token)은 잠재 변수(latent), 액션(action) 및 레지스터(register)와 교차됩니다. 헤드(head)는 다중 토큰 예측(multi-token prediction)으로 정책(policy), 보상(reward) 및 가치(value)를 예측합니다. 상상 롤아웃(imagination rollout)은 고정된 월드 모델(world model)에서 샘플링(sample)하고, PMPO는 복제된 BC 정책(policy)에 대한 역-KL(reverse-KL)을 사용하여 부호 기반 이점(sign-based advantage)을 최적화하여 온라인(online) 데이터(data) 없이 견고성(robustness)과 샘플 효율성(sample efficiency)을 향상시킵니다. 이러한 접근 방식은 마인크래프트(Minecraft)와 같은 복잡한 환경에서 에이전트(agent)가 스스로 학습하고 발전하는 중요한 발판을 마련했습니다. [논문](Paper) | [트윗](Tweet)

2.  **DeepSeek-V3.2-Exp: 효율적인 장문 컨텍스트(long context) 처리의 선구자**
    DeepSeek은 V3.1 "Terminus" 백본(backbone)에 미세한 희소 어텐션 메커니즘(sparse attention mechanism)(DeepSeek Sparse Attention, DSA)을 추가하여 혁신을 이루었습니다. 이는 128K 컨텍스트(context)에서 눈에 띄는 품질 손실 없이 큰 비용 절감을 보여주었습니다. 모델(model) 및 추론(inference) 코드(code)가 공개되어 연구 커뮤니티에 큰 기여를 했습니다. DSA 설계는 작은 FP8 "라이트닝 인덱서(lightning indexer)"가 쿼리(query)당 과거 토큰(token)을 점수화한 다음, top-k 셀렉터(selector)가 메인 어텐션(attention)을 위해 해당 KV 엔트리(entry)만 가져오는 방식으로 이루어집니다. 이는 인덱서(indexer)를 가볍게 유지하면서 코어 어텐션(core attention)을 메인 경로(main path)에서 O(L²)에서 약 O(L·k)로 변경하여 효율성을 극대화합니다. 훈련 레시피(recipe)는 128K V3.1 체크포인트(checkpoint)에서 시작하며, 밀집 어텐션(dense attention)으로 웜업(warm-up)하면서 밀집 어텐션 분포(dense attention distribution)에 대한 KL을 통해 인덱서(indexer)만 훈련합니다(약 2.1B 토큰(token)). 이후 희소 훈련(sparse training)으로 전환하고 쿼리(query)당 k=2048개의 선택된 KV 토큰(token)으로 모든 가중치(weight)를 최적화합니다(≈944B 토큰(token)). DSA의 영향을 분리하기 위해 V3.1과 동일한 파이프라인(pipeline)으로 후속 훈련(post-train)을 진행합니다. 후속 훈련 스택(stack)은 5개 도메인(domain)(수학, 경쟁 프로그래밍(competitive programming), 논리적 추론(logical reasoning), 에이전트 코딩(agentic coding), 에이전트 검색(agentic search))에 대한 전문가 증류(specialist distillation)와 글쓰기 및 QA를 포함합니다. 그런 다음 GRPO를 사용하여 추론(reasoning), 에이전트 행동(agent behavior) 및 정렬(alignment)의 균형을 맞추는 단일 혼합 RL 단계(stage)를 거칩니다. RL 설계는 결과 보상(outcome reward), 길이 페널티(length penalty) 및 언어 일관성 보상(language-consistency reward)을 사용합니다. 결과는 일반, 코드(code), 검색 에이전트(search-agent) 및 수학 스위트(suite) 전반에서 품질이 V3.1을 따른다는 것을 보여주었습니다. 표 1은 대부분의 지표에서 거의 동등한 성능을 보여주며, GPQA/HLE/HMMT에서 작은 하락은 유사한 토큰(token) 길이를 가진 체크포인트(checkpoint)를 사용할 때 사라집니다. BrowseComp 및 SWE Verified에 대한 RL 곡선(curve)은 DSA로 안정적으로 유지되었습니다. 비용 및 지연 시간(latency) 측면에서 이 연구는 긴 컨텍스트(context)에서 프리필링(prefilling)과 디코딩(decoding) 모두에 대해 명확한 종단 간(end-to-end) 토큰 위치 비용 절감을 보여주었습니다. 짧은 프리필(prefill)의 경우, DSA를 효율적으로 시뮬레이션(simulate)하기 위한 마스크된 MHA 경로(masked MHA path)를 제공합니다. 전반적인 효과는 정확도를 유지하면서 훨씬 저렴한 긴 컨텍스트(context) 서비스로 요약됩니다. 이는 대규모 언어 모델(LLM)의 실용적인 배포에 있어 중요한 진전으로 평가됩니다. [기술 보고서](Technical Report) | [트윗](Tweet)

3.  **실제 인간 상호작용을 통한 AI 학습: 차세대 개인화**
    이 연구는 정적 주석자 레이블(static annotator label) 대신 실제 사용자 대화에서 직접 학습하는 후속 훈련 레시피(post-training recipe)를 제시합니다. RLHI(Reinforcement Learning from Human Interaction)는 사용자 안내 재작성(user-guided rewrite)(후속 조치를 수정으로 사용)과 페르소나 기반 보상(persona-based reward)(페르소나 조건부 보상 모델(persona-conditioned reward model)을 통해 샘플링된 후보 순위 지정)을 결합하는 혁신적인 접근 방식입니다. WildChat 대화로 훈련되었으며, 개인화(personalization), 지시 따르기(instruction following)에서 강력한 개선을 보이고 추론 작업(reasoning task)으로도 전이됩니다. 페르소나(persona)는 장기 사용자 기록에서 증류(distill)되고 추론(inference) 시 앞에 추가됩니다. 훈련은 재작성 및 보상 순위 쌍에 대해 페르소나 조건부 DPO를 사용합니다. 실제 채팅(chat) 데이터는 특히 후반 턴(turn)에서 풍부한 수정 신호(correction signal)를 포함하여 밀도 높은 감독(dense supervision)을 제공합니다. WildChat 기반 평가에서 재작성은 개인화(personalization) 및 선호도(preference)를 개선하고, 페르소나 기반 보상(persona-based reward)은 지시 따르기(instruction following)에서 우위를 점합니다. 벤치마크(benchmark) 결과는 강력한 성능을 보여줍니다: AlpacaEval 2.0에서 77.9%의 승률, Arena-Hard에서 경쟁력 있는 성능, 수학/과학 데이터셋(dataset) 전반에서 추론 정확도(reasoning accuracy)가 26.5에서 31.8로 상승했습니다. 주요 절제 연구(ablation) 결과는 상호작용 데이터(interaction data)의 경우 RL > SFT임을 보여주며, 강력한 품질 필터(quality filter)가 필수적이고, 사용자당 깊이보다 사용자 다양성(user diversity)이 더 중요하다는 점을 강조했습니다. 다음 단계는 온라인 연속 학습(online continual learning), 더 안전한 보상 모델링(reward modeling) 및 개인 정보 보호 개인화(privacy-preserving personalization)를 포함합니다. 이러한 실제 인간 상호작용 기반 학습 방식은 AI 모델이 사용자의 복잡한 요구사항과 선호도를 더 잘 이해하고 반영할 수 있도록 하는 데 필수적인 역할을 합니다. [논문](Paper) | [트윗](Tweet)

4.  **JEPA 재고: 효율적인 비디오 표현 학습을 위한 SALT**
    Apple은 SALT(Static-teacher Asymmetric Latent Training)를 제안하며, 이는 간단한 2단계 V-JEPA 대안으로 주목받았습니다. 이 방법은 먼저 픽셀 재구성(pixel reconstruction)으로 교사(teacher)를 훈련한 다음, 이를 고정하고 학생(student)을 훈련하여 마스크된 영역(masked region)에서 교사의 잠재 변수(latent)를 예측하게 합니다. 이 접근 방식은 EMA를 제거하고, 교사와 학생을 분리하며, 더 계산 효율적이면서도 더 깔끔한 모델 선택(model selection)을 제공합니다. EMA 없이 확장되는 레시피(recipe)는 다음과 같습니다: 1단계: VideoMAE 스타일의 픽셀 재구성 목표(pixel reconstruction objective)를 사용하지만 V-JEPA의 다중 블록 마스킹(multi-block masking)(V-Pixel이라고 함)을 사용하여 비디오 인코더(video encoder)를 훈련합니다. 2단계: 해당 인코더(encoder)를 고정하고 학생 인코더(student encoder)+예측기(predictor)를 훈련하여 마스크된 영역(masked region)에서 교사의 잠재 변수(latent)와 일치시킵니다. 두 손실(loss) 모두 적절하고 안정적이며, 붕괴 메커니즘(collapse machinery)을 제거합니다. 더 낮은 계산량으로 더 나은 고정 백본(frozen-backbone) 결과를 달성하며, V-3.6M 믹스(mix)에서 일치하는 사전 훈련(pretraining) 단계에서 SALT는 V-JEPA 2보다 평균 Top-1을 개선하고 학생 크기에 따라 잘 확장됩니다. ViT-g/G SALT 학생들은 SSv2에서 최고 성능을 보이며 K400에서 경쟁력이 있습니다. 약한 교사, 강한 학생: 작거나 최적이 아닌 교사에 의해 훈련된 학생들도 여전히 SOTA(State-Of-The-Art) 수준이 됩니다. 최고의 ViT-L 학생은 ViT-L 교사만 사용하며, ViT-G 학생조차 ViT-L 교사로 최고 성능을 달성합니다. 실제로 유용한 훈련 신호(training signal): 손실(loss)이 좋지 않은 대리 지표(proxy)인 EMA JEPA와 달리, SALT의 학생 훈련 손실(student training loss)은 다운스트림(downstream) 고정 정확도(frozen accuracy)와 밀접하게 상관 관계를 가지며, 사전 훈련(pretraining) 중 해석 가능한 모델 선택(model selection)을 가능하게 합니다. 중요한 마스킹(masking) 및 데이터(data) 선택: 교사의 경우, 다중 블록 마스킹(multi-block masking)이 무작위 튜브(random tube) 및 인과 마스킹(causal masking)보다 우수합니다. 데이터 믹스(data mix)는 견고합니다: K710 전용 또는 Panda2.8M 전용 교사도 여전히 강력한 학생을 배출하며, V-3.6M이 전반적으로 최고입니다. 이는 대규모 비디오 데이터에서 효율적이고 효과적인 자기 지도 학습(self-supervised learning)의 중요성을 다시 한번 강조합니다. [논문](Paper) | [트윗](Tweet)

5.  **Agent S3: 행동 기반 최적 선택을 통한 에이전트 성능 향상**
    이 논문은 Behavior Best-of-N(bBoN)을 소개합니다: 여러 개의 완전한 CUA를 병렬로 실행하고, 각 롤아웃(rollout)을 간결한 행동 내러티브(behavior narrative)로 변환한 다음, 비교 선택을 통해 최상의 궤적(trajectory)을 선택합니다. 이러한 접근 방식은 에이전트의 복잡한 환경 탐색 및 최적 행동 선택 능력을 크게 향상시킵니다. 더 강력한 기본 에이전트(base agent)(Agent S3)를 사용하여, 이는 OSWorld에서 최첨단(state of the art)을 설정하고 Windows 및 Android로 일반화됩니다. Behavior Best-of-N은 여러 개의 완전한 롤아웃(rollout)을 샘플링(sample)하고, 각각을 전/후 델타(delta) 및 포인터 크롭(pointer crop)으로 요약한 다음, 원샷 MCQ 심사위원(one-shot MCQ judge)을 통해 우승자를 선택하는 방식으로 작동합니다. Agent S3 기준선(baseline)은 통합 코딩 서브 에이전트(coding sub-agent)가 있는 더 평탄한 루프(loop)를 특징으로 하며, 이는 Agent S2에 비해 성공률을 높이고 LLM 호출(call) 및 실제 시간(wall time)을 줄입니다. 결과는 100단계에서 OSWorld의 새로운 SoTA(State-Of-The-Art)를 달성하며, 효율성에서 강력한 이득을 보이고, 이 접근 방식은 Windows 및 Android 설정으로 전이됩니다. 확장 가능성 측면에서, N이 증가함에 따라 정확도가 상승하고, 모델 다양성(model diversity)이 Pass@N을 개선하며, 단일 라운드 비교 선택(single-round comparative selection)은 더 낮은 비용으로 쌍별 토너먼트(pairwise tournament)와 일치하거나 능가합니다. 실용적인 시사점: 동일한 스냅샷(snapshot)에서 병렬 VM(virtual machine)을 시작하고, 검증 가능한 델타(delta)를 방출하도록 단계를 계측하며, N을 4에서 10 사이로 시작하고, 예산이 허락한다면 다양한 강력한 모델(model)을 추가하는 것이 좋습니다. 한계점으로는 독립적인 병렬 실행을 가정하며, 공유된 실제 데스크톱(desktop) 부작용이 시도 전반에 걸쳐 유출될 수 있다는 점이 있습니다. 이러한 병렬 실행 및 최적 경로 선택 전략은 복잡한 실제 환경에서 에이전트의 견고성과 성능을 높이는 데 중요한 기여를 합니다. [논문](Paper) | [트윗](Tweet)

6.  **DeepSearch: 훈련 시간(train-time) 탐색을 통한 수학적 추론 강화**
    DeepSearch는 검증 가능한 보상(reward)을 가진 몬테카를로 트리 탐색(Monte Carlo Tree Search)을 RL에 직접 통합하지만, 추론(inference) 시가 아니라 훈련(training) 중에 통합하는 독특한 방식을 제안합니다. 그 결과, 강력한 1.5B 기준선(baseline)에 비해 수학적 추론(math reasoning)에서 더 넓은 탐색(exploration), 더 나은 크레딧 할당(credit assignment) 및 더 높은 샘플 효율성(sample efficiency)을 얻습니다. 이는 테스트 시간(test-time)뿐만 아니라 훈련 시간(train-time) 탐색의 중요성을 강조합니다. MCTS는 RL 루프(loop)에 두 개의 셀렉터(selector)와 함께 내장됩니다: 형제 비교를 위한 로컬 UCT(local UCT)와 전체 트리(tree)에서 다음 리프(leaf)를 선택하는 전역 프론티어 스코어러(global frontier scorer). 프론티어 점수(frontier score)는 부모 품질(parent quality), 정책 엔트로피(policy entropy) 및 깊이 보너스(depth bonus) √(d/dT)를 결합합니다. DeepSearch는 승리 경로와 '확신 있는 오답' 경로를 모두 감독합니다. 올바른 터미널(terminal)이 발견되지 않으면, DeepSearch는 감독을 위해 경로를 따라 가장 낮은 평균 엔트로피(entropy)를 가진 부정적인 궤적(trajectory)을 선택합니다. 이는 제약된 업데이트(constrained update)로 노드(node) 값을 백업(back up)하여 올바른 경로의 노드(node)가 음수가 아닌 상태로 유지되도록 합니다. 이는 결과 보상(outcome reward)만 제공하는 대신 미세한 단계 수준 이점(step-level advantage)을 제공합니다. 트리-GRPO 목표(objective)와 q-값 소프트 클리핑(soft clipping): 이점은 평균 전용 정규화(mean-only normalization)를 가진 노드 수준 q(s), PPO 스타일 비율(ratio)의 상위 클리핑(clip-higher), 그리고 중간 q의 tanh 소프트 클리핑(soft clipping)을 사용하여 폭발을 피하면서 그라디언트(gradient)를 부드럽게 유지합니다. 터미널 보상(terminal reward)은 ±1로 유지됩니다. 적응형 효율성: 어려운 항목을 필터링(filter)하고 솔루션(solution)을 캐시(cache)합니다: Pass1@K 임계값(threshold)을 사용하여 '어려운 하위 집합(hard subset)'으로 반복적으로 필터링(filter)하고, 검증된 솔루션(solution)의 리플레이 버퍼(replay buffer)를 유지하며, 캐시된 올바른 궤적(trajectory)이 존재할 때 전체 탐색(search)을 건너뜁니다. 이는 지식(knowledge)을 보존하고 계산(compute)을 절약합니다. 결과 - 훨씬 적은 계산량으로 더 나은 정확도: AIME24/25, AMC23, MATH500, Minerva, Olympiad에서 DeepSearch-1.5B는 평균 62.95%를 기록하여 Nemotron-Research-Reasoning-Qwen-1.5B v2를 1.25% 포인트(pp) 능가합니다(7페이지 표 1). 단 +50 RL 단계(step)로 약 330 GPU 시간을 사용하여, 1,883 GPU 시간 후 62.02%에서 정체되는 확장 훈련(extended training)을 능가합니다. 절제 연구(ablation)는 전역 프론티어 선택(global frontier selection)이 바닐라 UCT(vanilla UCT)에 비해 보상(reward)을 개선하고 반복(iteration)을 줄이며, 최종 이득은 새로운 q-백업(q-backup), 노드 수준 이점(node-level advantage), 평균 전용 정규화(mean-only normalization) 및 프론티어 선택(frontier selection)의 조합에서 비롯됨을 보여줍니다. 복잡한 추론 문제 해결에 있어 탐색 기반 학습의 잠재력을 극대화하는 선구적인 연구입니다. [논문](Paper) | [트윗](Tweet)

7.  **확산 LLM(Diffusion LLM) 가속화: 효율적인 생성의 길**
    가볍고 학습된 정책(policy)은 어떤 토큰(token)이 이미 '최종'인지, 그리고 언제 생성을 중단할지 결정함으로써 확산 기반 LLM 디코딩(diffusion-based LLM decoding) 속도를 높입니다. 저자들은 토큰 신뢰도 신호(token confidence signal)에 대해 작은 MLP 필터(filter)를 훈련하고, [EoT]가 안정적으로 생성되자마자 디코딩(decoding)을 중단시키는 텍스트 끝 예측(End-of-Text Prediction)을 추가합니다. LLaDA-8B-Instruct에서 이는 최소한의 또는 전혀 없는 정확도 손실로 큰 처리량(throughput) 이득을 달성합니다. 기존 확산 LLM의 문제점과 통찰은 반자동 회귀 확산 LLM(semi-autoregressive diffusion LLM)이 토큰 업데이트(token update)를 병렬화(parallelize)하지만, 정적 휴리스틱(static heuristic)은 이미 올바른 토큰(token)을 계속해서 다시 마스킹(remasking)한다는 점입니다. 이 논문은 올바른 예측 즉시 토큰(token)을 언마스킹(unmasking)하는 오라클 전략(oracle strategy)인 Extremely Greedy Parallel을 정의하고 속도 향상을 위한 큰 여유 공간을 보여줍니다. 방법론은 Learn2PD 필터(filter)를 사용합니다: 토큰 신뢰도 패턴(token confidence pattern)에 대해 2계층 MLP 필터(filter) fθ를 훈련하여 위치당 '최종화 또는 재마스킹(finalize or remask)'을 예측합니다. 필터(filter)만 BCE 손실(loss)로 훈련되며, dLLM은 고정된 상태로 유지됩니다. 추론(inference)은 필터(filter)의 로짓(logit)에 임계값(threshold) τ를 적용하여 토큰(token)을 확정합니다. EoTP로 조기 중단: 텍스트 끝 예측(End-of-Text Prediction)은 [EoT]가 디코딩(decode)되면 중단되어, [EoT]로 가득 찬 긴 꼬리(long tail)를 피합니다. 부록 B는 길이 1024에서 추가 계산량의 약 89.59%가 EoT 후 패딩(padding)에서 발생한다고 언급합니다. 결과: GSM8K, MATH, HumanEval 및 MBPP에서 Learn2PD만으로 길이에 따라 3~12배의 속도 향상을 얻습니다. Learn2PD+EoTP는 GSM8K에서 길이 1024에서 22.58배에 도달하며 정확도는 유지되거나 약간 향상됩니다. KV 캐시(cache)와 결합하면 작은 정확도 절충(tradeoff)으로 처리량(throughput)을 57.51배까지 더욱 향상시킵니다. 더 긴 시퀀스(sequence)가 더 많은 이점을 얻으며, 표 4는 가속이 길이 128에서 3.36배에서 1024에서 22.58배로 증가함을 보여줍니다. 엔지니어링 노트: 필터(filter)는 작고 훈련이 빠릅니다: 블록 크기 32의 경우 약 2k 매개변수(parameter)를 가지며, 짧은 데이터 수집 패스(pass) 후 단일 T4에서 몇 분 만에 훈련됩니다. 추론(inference) 시 오버헤드(overhead)는 이득에 비해 무시할 수 있습니다. 이 방법은 KV 캐싱(caching)과 직교하며 기존 dLLM 디코더(decoder)에 쉽게 통합될 수 있습니다. 대규모 언어 모델의 실용적인 배포를 위해 필수적인 추론 속도 향상에 기여하는 중요한 기술 발전입니다. [논문](Paper) | [트윗](Tweet)

8.  **작은 모델(model)에 맞춰진 추론 트레이스(trace): 효과적인 지식 증류(knowledge distillation)**
    작은 모델(model)은 큰 교사(teacher)의 길고 고품질 CoT(Chain-of-Thought)로 SFT(Supervised Fine-Tuning)할 때 종종 성능이 저하됩니다. 이 논문은 그 이유를 정확히 지적하고 역 투기적 디코딩(Reverse Speculative Decoding, RSD)으로 해결합니다: 교사(teacher)가 토큰(token)을 제안하게 하되, 학생(student)이 학생의 분포에서 확률적일 경우에만 승인하게 합니다. 결과적으로 학생의 분포와 일치하면서도 정확성을 유지하는 트레이스(trace)를 생성하여, 작은 모델(model)이 실제로 학습할 수 있도록 돕습니다. 핵심 아이디어: 각 단계에서 교사 토큰(teacher token)을 샘플링(sample)하고, 학생(student)이 ≥ p_th 확률을 할당하는 경우에만 유지하며, 그렇지 않으면 학생 자신의 토큰(token)으로 되돌아갑니다. 이는 작은 모델(model)이 추적할 수 없는 높은 놀라움 스파이크(high-surprisal spike)를 필터링(filter)하여 논리(logic)를 단순화하지 않고 토큰 수준 난이도(token-level difficulty)를 완화합니다. 왜 중요한가: s1K-1.1 트레이스(trace)에 대한 Qwen3-0.6B의 직접 SFT(Supervised Fine-Tuning)는 평균 정확도(average accuracy)를 20.5% 저하시킵니다. 대신 RSD 트레이스(trace)로 훈련하면 AIME24, AIME25, GPQA-Diamond, MATH500 전반에서 평균 4.9%의 이득을 얻습니다. 작은 모델(model)에 효과적인 데이터 레시피(recipe): 토크나이저 호환 교사(tokenizer-compatible teacher)(s1.1-7B)와 학생(student)(Qwen3-0.6B)을 사용합니다. 거부 샘플링(rejection sampling)으로 RSD 트레이스(trace)를 생성합니다. 문제가 해결될 수 없을 때, UPFT 스타일 접두사 훈련(prefix training)을 통해 처음 128개 토큰(token)을 복구합니다. 180개의 완전한 솔루션(solution)과 많은 접두사(prefix)에도 불구하고, 0.6B 학생(student)은 개선되어 분포 정렬(distributional alignment)이 볼륨(volume)보다 중요함을 보여줍니다. 주요 진단: 가장 강력한 실패 예측 변수(failure predictor)는 학생(student) 하위 1% 토큰(token)의 비율입니다. s1K-1.1 트레이스(trace)는 그러한 토큰(token)을 많이 포함하고 학습을 저하시킵니다. RSD는 이를 거의 0으로 줄입니다. 보편적이지 않으며, 맞춤화되어야 합니다: RSD 트레이스(trace)는 모델(model)별로 다릅니다. Qwen3-0.6B를 "승인자"로 사용하여 구축된 트레이스(trace)는 Qwen3-1.7B, Llama-3.2-1B, Gemma-3-1B 또는 Phi-4-Mini로 전이되지 않습니다. 대상 모델(target model)별로 RSD를 실행하는 것은 도움이 되지만, 동일한 모델(model)에 대한 반복적인 다단계 RSD는 분포 드리프트(distributional drift)를 통해 성능을 저하시킵니다. 이러한 접근 방식은 리소스가 제한된 환경에서도 대규모 모델의 지식을 효율적으로 활용할 수 있는 길을 열어줍니다. [논문](Paper) | [트윗](Tweet)

9.  **도구 사용 혼합(Tool-Use Mixture, TUMIX): 다중 에이전트 추론의 시너지**
    TUMIX는 텍스트(text), 코드 실행(code execution) 및 웹 검색(web search)을 혼합하고, 15개의 다양한 에이전트(agent)를 병렬로 실행하며, 라운드(round)를 거쳐 중간 답변을 전달하는 추론(reasoning)을 위한 앙상블 레시피(ensemble recipe)입니다. LLM-심사위원(judge)은 조기 중단(early stopping)을 제어하여 HLE, GPQA-Diamond 및 AIME 24/25에서 강력한 도구 증강 기준선(tool-augmented baseline)에 비해 최대 3.55%의 정확도 이득(accuracy gain)을 제공하는 동시에 추론(inference) 비용을 약 50% 절감합니다. 여러 에이전트의 강점을 결합하고 각 에이전트의 출력을 효과적으로 조율함으로써, TUMIX는 복잡한 문제 해결 능력을 한 단계 끌어올렸습니다. [논문](Paper) | [트윗](Tweet)

10. **PrompCoT 2.0: 자동화된 프롬프트 합성을 통한 LLM 추론의 확장**
    PromptCoT 2.0은 PromptCoT 1.0의 수동 휴리스틱(manual heuristic)을 대체하여 더 어렵고 다양한 추론 프롬프트(reasoning prompt)를 합성하기 위한 EM 기반 루프(EM-based loop)를 도입합니다. 이는 셀프 플레이(self-play) 및 SFT 훈련 체제(training regime)를 모두 가능하게 하며, AIME, HMMT, LiveCodeBench 및 Codeforces와 같은 추론 벤치마크(benchmark)에서 새로운 SOTA(State-Of-The-Art)를 달성하여 프롬프트 합성(prompt synthesis)이 LLM 추론(reasoning)의 새로운 스케일링 축(scaling axis)임을 보여줍니다. 이 기술은 인간의 개입 없이도 LLM이 스스로 학습하고 추론 능력을 향상시킬 수 있는 길을 제시합니다. [논문](Paper) | [트윗](Tweet)