다음은 업데이트된 블로그 게시물입니다.

---

1.  **확장성 높은 세계 모델 기반 에이전트 학습**
    신속하고 정교한 마인크래프트 환경 시뮬레이터를 익히고, 완전히 독립적인 환경에서 제어 가능한 에이전트를 훈련하기 위한 확장형 상상 강화 학습(RL) 방법론을 제시합니다. 본 세계 모델은 단일 그래픽 처리 장치(GPU)에서 실시간으로 상호작용적 시뮬레이션 실행을 지원하며, 원본 픽셀 데이터와 기초적인 마우스 및 키보드 입력을 활용하여 순수하게 오프라인 상태에서 "다이아몬드 획득"과 같은 복잡한 목표 달성을 가능하게 합니다. 신속성과 견고성을 위해 설계된 핵심 기법은 인과적 토크나이저와 블록-인과적 동역학 트랜스포머의 결합입니다. `숏컷 포싱(shortcut forcing)`은 램프 손실을 사용하여 x-공간에서 예측하면서 모델이 큰 디노이징 단계(K=4)를 수행하도록 훈련시키며, 이는 누적 오류를 줄이고 낮은 단계 수에서도 품질을 유지합니다. 공간 전용 및 시간 전용 어텐션 레이어, 4단계마다 한 번씩 시간 레이어, GQA(Grouped Query Attention), 길고 짧은 배치(batch) 교차는 KV 캐시를 작게 유지하고 추론(inference)을 빠르게 만듭니다. 시각적 요소뿐만 아니라 메커니즘(mechanics)까지 처리하는 실시간, 더 긴 컨텍스트 세계 모델은 640×360 해상도에서 9.6초 컨텍스트로 20+ FPS의 대화형 추론을 제공하며, 이는 이전 마인크래프트 모델보다 훨씬 긴 컨텍스트 길이입니다. 16개 작업에 걸친 사람 개입 플레이 테스트(human-in-the-loop play test)에서 Dreamer 4는 14개 작업에서 성공하며, 블록을 올바르게 배치/파괴하고, 도구를 전환하고, 보트를 타고, 화로를 사용하고, 포털에 진입하는 능력을 보여주었습니다. 반면 Oasis/Lucid는 많은 객체 상호작용 작업에서 실패했습니다. 오프라인 다이아몬드 챌린지(Offline Diamond Challenge)에서는 환경 상호작용 없이 2.5K 시간 VPT 계약자 데이터셋으로만 훈련된 에이전트가 작업 토큰에 조건화되고 상상 RL(PMPO 포함)을 통해 개선되었습니다. 60분 길이의 에피소드 안에서 철 곡괭이 획득률 29%, 다이아몬드 획득률 0.7%를 기록하여, 미세 조정된 VPT 및 Gemma-3 VLA와 같은 강력한 오프라인 벤치마크를 뛰어넘는 성과를 보였습니다. 이는 YouTube에서 사전 학습된 VPT 파이프라인 대비 약 100배 적은 데이터를 사용한 결과입니다. 적은 쌍 데이터로부터의 액션 그라운딩(action grounding)은 OOD(Out-of-Distribution)로 일반화됩니다: 2,541시간의 비디오 중 액션이 있는 것은 100시간뿐이지만, 모델은 액션 조건부 예측에서 전체 액션 모델의 약 85% PSNR(Peak Signal-to-Noise Ratio)과 100% SSIM(Structural Similarity Index Measure)에 도달합니다. 오버월드에서만 훈련된 액션 조건화는 액션 없이 본 네더/엔드 장면에 전이되어, 전체 액션 모델의 약 76% PSNR과 80% SSIM을 달성합니다. 모델과 일관성을 유지하는 에이전트 파인튜닝(finetuning) 및 상상 RL: 작업 토큰은 잠재 변수, 액션 및 레지스터와 교차됩니다. 헤드는 다중 토큰 예측으로 정책, 보상 및 가치를 예측합니다. 상상 롤아웃은 고정된 월드 모델에서 샘플링하고, PMPO는 복제된 BC(Behavioral Cloning) 정책에 대한 역-KL(reverse-KL)을 사용하여 부호 기반 이점(sign-based advantage)을 최적화하여 온라인 데이터 없이 견고성과 샘플 효율성을 향상시킵니다.
    이러한 접근 방식은 가상 환경에서 에이전트를 훈련하는 새로운 지평을 열어줍니다. 특히 마인크래프트와 같은 개방형 세계는 복잡한 상호작용과 다양한 목표를 포함하기 때문에, 이러한 환경에서 효과적인 에이전트를 학습시키는 것은 일반적인 인공지능(AI) 개발에 중요한 의미를 가집니다. 빠르고 정확한 세계 모델은 에이전트가 실제 환경과 유사한 경험을 가상으로 축적할 수 있도록 하여, 현실 세계의 로봇 제어 또는 자율 시스템 개발에도 잠재적인 영향을 미 미칠 수 있습니다. 시뮬레이션에서 현실로의 전환(sim-to-real transfer)이라는 난제를 해결하는 데 있어, 고도로 사실적인 시뮬레이션과 효율적인 학습 방법론은 필수적입니다. 또한, 제한된 데이터만으로도 강력한 행동을 학습하고 일반화하는 능력은 데이터 수집 비용이 높은 실제 환경에서의 적용 가능성을 높여줍니다. 미래에는 이러한 월드 모델 기반 에이전트가 더욱 복잡하고 다단계적인 작업을 자율적으로 수행하며, 인간의 개입 없이도 새로운 환경에 적응하고 학습하는 방향으로 발전할 것으로 기대됩니다. [논문](Paper) | [트윗](Tweet)

2.  **DeepSeek-V3.2-Exp: 희소 어텐션으로 LLM 효율성 극대화**
    DeepSeek은 V3.1 "터미너스" 기반 구조에 정교한 희소 어텐션 메커니즘(DeepSeek Sparse Attention, DSA)을 통합하여, 128K 컨텍스트 길이에서도 성능 저하 없이 상당한 비용 효율성을 달성했습니다. 모델 및 추론 코드 또한 공개되어 커뮤니티의 검토를 받고 있습니다. DSA의 구조는 다음과 같습니다: 소형 FP8 "라이트닝 인덱서"가 각 쿼리에 대해 이전 토큰들의 점수를 매기고, 상위 k개 선택기가 주요 어텐션 연산을 위해 해당 키-값(KV) 쌍만을 추출합니다. 이 방식은 인덱서의 경량화를 유지하면서, 핵심 어텐션 계산 복잡도를 주요 경로에서 O(L²)에서 대략 O(L·k) 수준으로 감소시킵니다. 훈련 레시피는 128K V3.1 체크포인트에서 시작합니다. 밀집 어텐션으로 웜업(warm-up)하면서 밀집 어텐션 분포에 대한 KL(Kullback-Leibler) 발산을 통해 인덱서만 훈련합니다(약 2.1B 토큰). 이후 희소 훈련으로 전환하고 쿼리당 k=2048개의 선택된 KV 토큰으로 모든 가중치를 최적화합니다(약 944B 토큰). DSA의 영향을 분리하기 위해 V3.1과 동일한 파이프라인으로 후속 훈련을 진행했습니다. 후속 훈련 스택은 5개 도메인(수학, 경쟁 프로그래밍, 논리적 추론, 에이전트 코딩, 에이전트 검색)에 대한 전문가 증류와 글쓰기 및 QA를 포함합니다. 그런 다음 GRPO(Generalized Reinforcement Learning from Human Preferences)를 사용하여 추론, 에이전트 행동 및 정렬의 균형을 맞추는 단일 혼합 RL 단계를 거칩니다. RL 설계는 결과 보상, 길이 페널티 및 언어 일관성 보상을 사용합니다. 결과적으로 일반, 코드, 검색 에이전트 및 수학 스위트 전반에서 품질이 V3.1과 유사하게 유지되었습니다. 표 1은 대부분의 지표에서 거의 동등한 성능을 보여주며, GPQA/HLE/HMMT에서 작은 하락은 유사한 토큰 길이를 가진 체크포인트를 사용할 때 사라집니다. BrowseComp 및 SWE Verified에 대한 RL 곡선은 DSA로 안정적으로 유지됩니다. 비용 및 지연 시간 측면에서 이 연구는 긴 컨텍스트에서 프리필링(prefilling)과 디코딩 모두에 대해 명확한 종단 간 토큰 위치 비용 절감을 보여줍니다. 짧은 프리필의 경우, DSA를 효율적으로 시뮬레이션하기 위한 마스크된 MHA(Multi-Head Attention) 경로를 제공합니다. 전반적인 효과는 정확도를 유지하면서 훨씬 저렴한 긴 컨텍스트 서비스입니다.
    희소 어텐션 메커니즘은 대규모 언어 모델(LLM)의 효율성을 혁신하는 핵심 기술로 부상하고 있습니다. 기존의 어텐션 메커니즘은 컨텍스트 길이가 길어질수록 계산 복잡도가 기하급수적으로 증가하는 문제점을 안고 있었으나, DSA와 같은 희소 어텐션은 이를 선형 또는 준선형 수준으로 낮춰 긴 컨텍스트 창을 훨씬 경제적으로 처리할 수 있게 합니다. 이는 LLM이 더 많은 정보를 한 번에 처리하여 더욱 일관성 있고 맥락에 맞는 응답을 생성하는 데 필수적입니다. 다른 희소 어텐션 기술들(예: Longformer, Reformer)과 비교했을 때, DSA는 경량 인덱서를 통해 핵심 어텐션 계산을 최적화하는 독특한 접근 방식을 취합니다. 이러한 발전은 LLM의 추론 비용을 낮춰 더 많은 응용 분야에서 활용될 수 있는 길을 열어주며, 미래에는 더욱 복잡하고 효율적인 트랜스포머 아키텍처의 등장을 예고합니다. 희소 어텐션은 단순히 비용 절감뿐만 아니라, 모델이 가장 중요한 정보에 집중하도록 유도하여 전반적인 성능 향상에도 기여할 수 있습니다. [기술 보고서](Technical Report) | [트윗](Tweet)

3.  **실제 인간 상호작용 기반 AI 학습의 새로운 지평**
    본 연구는 고정된 주석 레이블 대신 실제 사용자 대화 데이터로부터 직접 학습하는 사후 훈련 방법론을 제안합니다. RLHI(Reinforcement Learning from Human Interactions)는 사용자가 주도하는 재작성(후속 조치를 수정으로 활용)과 페르소나 기반 보상(페르소나를 조건으로 하는 보상 모델을 통해 후보를 순위화) 방식을 융합합니다. WildChat 대화 데이터로 훈련되었으며, 개인화(personalization) 및 지시 이행 능력에서 강력한 개선을 보였고, 추론 작업으로도 전이되는 효과를 입증했습니다. 페르소나는 장기 사용자 기록에서 증류(distill)되고 추론 시점에 프롬프트 앞에 추가됩니다. 훈련은 재작성 및 보상 순위 쌍에 대해 페르소나 조건부 DPO(Direct Preference Optimization)를 사용합니다. 실제 채팅 데이터는 특히 후반 턴에서 풍부한 수정 신호(correction signal)를 포함하여 밀도 높은 감독을 제공합니다. WildChat 데이터를 활용한 평가에서, 재작성 기법은 개인화 및 사용자 선호도를 향상시켰고, 페르소나 기반 보상 기법은 지시 이행 능력에서 우월한 성능을 보였습니다. 벤치마크에서는 강력한 결과를 보여주었습니다: AlpacaEval 2.0에서 77.9%의 승률, Arena-Hard에서 경쟁력 있는 성능, 수학/과학 데이터셋 전반에서 추론 정확도가 26.5에서 31.8로 상승했습니다. 주요 절제 연구(ablation study) 결과, 상호작용 데이터의 경우 RL이 SFT(Supervised Fine-Tuning)보다 우수하며, 강력한 품질 필터가 필수적이고, 사용자당 깊이보다 사용자 다양성(user diversity)이 더 중요하다고 밝혀졌습니다. 다음 단계는 온라인 연속 학습, 더 안전한 보상 모델링 및 개인 정보 보호 개인화를 포함합니다.
    이 연구는 AI가 인간과 상호작용하는 방식에 혁명적인 변화를 가져올 수 있음을 시사합니다. 정적 데이터를 넘어 실제 사용자 피드백에서 직접 학습하는 것은 AI가 더욱 자연스럽고, 개인화되며, 공감 능력을 갖추도록 돕는 중요한 단계입니다. 그러나 이러한 접근 방식은 데이터 프라이버시, 보안, 그리고 편향 전파와 같은 윤리적 문제에 대한 깊은 고려를 요구합니다. 실제 대화에서 학습할 때, AI는 의도치 않게 사용자 데이터를 통해 사회적 편향을 학습하거나 민감한 정보를 노출할 위험이 있습니다. 따라서 이러한 시스템을 개발할 때는 강력한 데이터 익명화, 보안 프로토콜, 그리고 편향 완화 기술이 필수적입니다. 미래의 대화형 AI는 단순히 정보를 제공하는 것을 넘어, 사용자의 개별적인 필요와 선호도를 이해하고 반영하는 진정한 개인 비서로서의 역할을 수행하게 될 것입니다. 이는 AI가 인간의 삶에 더욱 깊숙이 통합되는 동시에, 책임감 있는 개발과 배포의 중요성을 더욱 강조합니다. [논문](Paper) | [트윗](Tweet)

4.  **JEPA 재고: 효율적인 비디오 자기 지도 학습의 새로운 방향**
    애플은 정적 교사 비대칭 잠재 학습(SALT) 기법을 제안합니다. 이는 간소화된 2단계 V-JEPA의 대안으로, 첫째, 픽셀 재구성을 통해 교사 모델을 학습시키고, 그 다음 교사를 고정한 채 학생 모델이 가려진 영역에서 교사의 잠재 표현을 예측하도록 훈련합니다. 이 방식은 EMA(Exponential Moving Average)를 없애고 교사와 학생 모델을 독립적으로 운영하며, 계산 효율성을 높이면서도 더욱 명확한 모델 선택 과정을 제공합니다. EMA 없이 확장되는 레시피는 다음과 같습니다: 1단계: VideoMAE 스타일의 픽셀 재구성 목표를 사용하지만 V-JEPA의 다중 블록 마스킹(V-Pixel이라고 함)을 사용하여 비디오 인코더를 훈련합니다. 2단계: 해당 인코더를 고정하고 학생 인코더 + 예측기를 훈련하여 마스크된 영역에서 교사의 잠재 변수와 일치시킵니다. 두 손실 모두 적절하고 안정적이며, 붕괴 메커니즘(collapse machinery)을 제거합니다. 더 낮은 계산량으로 더 나은 고정 백본 결과: V-3.6M 믹스에서 일치하는 사전 훈련 단계에서 SALT는 V-JEPA 2보다 평균 Top-1 정확도를 개선하고 학생 크기에 따라 잘 확장됩니다. ViT-g/G SALT 학생들은 SSv2에서 최고 성능을 보이며 K400에서 경쟁력이 있습니다. 약한 교사, 강한 학생: 작거나 최적이 아닌 교사에 의해 훈련된 학생들도 여전히 SOTA(State-Of-The-Art) 수준이 됩니다. 최고의 ViT-L 학생은 ViT-L 교사만 사용하며, ViT-G 학생조차 ViT-L 교사로 최고 성능을 달성합니다. 실제로 유용한 훈련 신호: 손실이 좋지 않은 대리 지표인 EMA JEPA와 달리, SALT의 학생 훈련 손실은 다운스트림 고정 정확도와 밀접하게 상관 관계를 가지며, 사전 훈련 중 해석 가능한 모델 선택을 가능하게 합니다. 중요한 마스킹 및 데이터 선택: 교사의 경우, 다중 블록 마스킹이 무작위 튜브 및 인과 마스킹보다 우수합니다. 데이터 믹스는 견고합니다: K710 전용 또는 Panda2.8M 전용 교사도 여전히 강력한 학생을 배출하며, V-3.6M이 전반적으로 최고입니다.
    SALT는 비디오 이해 분야의 자기 지도 학습에서 중요한 진전을 나타냅니다. 교사와 학생 모델의 훈련을 분리하고 EMA 없이도 강력한 성능을 달성함으로써, 모델 개발 과정을 간소화하고 계산 자원을 더욱 효율적으로 활용할 수 있게 됩니다. 이러한 방식은 특히 대규모 비디오 데이터셋을 활용하는 데 있어 비용 효율성을 높여, 더 많은 연구자와 개발자가 강력한 비디오 사전 훈련 모델에 접근할 수 있도록 돕습니다. 자기 지도 학습은 레이블링된 데이터의 부족 문제를 해결하고, 모델이 대량의 비디오에서 풍부한 시각적 및 시간적 특징을 스스로 학습하도록 돕습니다. SALT와 같은 효율적인 방법론은 비디오 콘텐츠 분석, 액션 인식, 비디오 검색 등 다양한 비디오 관련 AI 응용 분야에서 혁신을 가속화할 잠재력을 가지고 있습니다. 또한, 약한 교사 모델로도 강력한 학생 모델을 훈련할 수 있다는 점은 모델 증류(model distillation) 및 효율적인 지식 전달의 중요성을 다시 한번 강조합니다. [논문](Paper) | [트윗](Tweet)

5.  **Agent S3: 행동 기반 최적 경로 선택으로 자율 에이전트 성능 향상**
    본 연구는 Behavior Best-of-N(bBoN)을 선보입니다. 이는 다수의 완전한 CUA(Complete Unified Agent)를 동시에 구동하고, 각 실행 결과를 압축된 행동 서사로 전환한 뒤, 비교 분석을 통해 최적의 경로를 선별하는 방식입니다. 더욱 강력해진 기본 에이전트(Agent S3)와 결합하여, OSWorld 벤치마크에서 최신 최고 성능을 달성했으며, Windows 및 Android 환경으로도 성공적으로 일반화됩니다. Behavior Best-of-N은 여러 개의 완전한 롤아웃(rollout)을 샘플링하고, 각각을 전/후 델타(delta) 및 포인터 크롭(pointer crop)으로 요약한 다음, 원샷 MCQ 심사위원(one-shot MCQ judge)을 통해 우승자를 선택합니다. Agent S3 기준선은 통합 코딩 서브 에이전트가 있는 더 평탄한 루프를 통해 Agent S2에 비해 성공률을 높이고 LLM 호출 및 실제 시간(wall time)을 줄입니다. 결과적으로, 이 방법론은 100단계에서 OSWorld의 새로운 최첨단 성능을 확립하고, 효율성 측면에서 상당한 개선을 보였으며, Windows 및 Android 환경에서도 적용 가능함이 입증되었습니다. 확장성 측면에서 N이 증가함에 따라 정확도가 상승하고, 모델 다양성이 Pass@N을 개선하며, 단일 라운드 비교 선택은 더 낮은 비용으로 쌍별 토너먼트와 일치하거나 능가합니다. 실용적인 시사점으로는 동일한 스냅샷에서 병렬 VM(virtual machine)을 시작하고, 검증 가능한 델타를 방출하도록 단계를 계측하며, N을 4에서 10 사이로 시작하고, 예산이 허락한다면 다양한 강력한 모델을 추가하는 것을 제안합니다. 한계점은 독립적인 병렬 실행을 가정한다는 것입니다; 공유된 실제 데스크톱 부작용이 시도 전반에 걸쳐 유출될 수 있습니다.
    Behavior Best-of-N과 Agent S3의 결합은 복잡한 환경에서 자율 에이전트의 의사결정 능력을 향상시키는 중요한 발전을 보여줍니다. 여러 에이전트의 병렬 실행과 LLM 기반 심사관을 통한 최적 경로 선택은 불확실성이 높은 작업 환경에서 에이전트의 견고성과 효율성을 크게 높일 수 있습니다. 이러한 다중 에이전트 평가 및 선택 방식은 단순히 성능 향상을 넘어, 에이전트가 다양한 전략을 탐색하고 가장 성공적인 접근 방식을 식별하는 능력을 부여합니다. 특히 Windows 및 Android와 같은 이질적인 운영체제 환경으로의 일반화는 실제 세계의 다양한 플랫폼에서 에이전트를 배포할 수 있는 가능성을 열어줍니다. 이는 고객 서비스, 소프트웨어 테스트, 개인 비서 등 다양한 자동화 분야에서 혁신적인 응용을 가능하게 할 것입니다. 하지만, 여러 에이전트 간의 조율, 공유 자원 관리, 그리고 예상치 못한 상호작용 부작용을 최소화하는 것은 여전히 중요한 과제로 남아 있습니다. 미래에는 이러한 에이전트들이 더욱 자율적으로 복잡한 문제를 해결하고, 인간의 개입 없이도 새로운 도메인에 적응하는 방향으로 발전할 것으로 예상됩니다. [논문](Paper) | [트윗](Tweet)

6.  **DeepSearch: 훈련 단계에서의 몬테카를로 트리 탐색으로 수학적 추론 강화**
    DeepSearch는 검증 가능한 보상 체계를 갖춘 몬테카를로 트리 탐색(MCTS)을 강화 학습(RL) 과정에 직접 결합합니다. 이는 추론 단계가 아닌 학습 단계에서 이루어집니다. 이로 인해 강력한 1.5B 규모의 기준 모델 대비 수학적 추론 능력에서 더 광범위한 탐색, 향상된 크레딧 할당, 그리고 더 높은 샘플 효율성을 달성합니다. 이 접근 방식은 테스트 시점뿐만 아니라 훈련 시점에도 탐색을 수행합니다. MCTS는 RL 학습 과정에 두 개의 선택기와 함께 통합됩니다: 형제 노드 비교를 위한 로컬 UCT(Upper Confidence Bound 1 applied to trees)와 전체 트리 내에서 다음 리프 노드를 선정하는 전역 프론티어 스코어러입니다. 프론티어 점수는 부모 품질, 정책 엔트로피 및 깊이 보너스 √(d/dT)를 결합합니다. DeepSearch는 올바른 터미널이 발견되지 않으면, 감독을 위해 경로를 따라 가장 낮은 평균 엔트로피를 가진 부정적인 궤적(trajectory)을 선택함으로써 승리 경로와 '확신 있는 오답' 경로를 모두 감독합니다. 이는 제약된 업데이트로 노드 값을 백업(back up)하여 올바른 경로의 노드가 음수가 아닌 상태로 유지되도록 합니다. 이는 결과 보상만 제공하는 대신 미세한 단계 수준 이점(step-level advantage)을 제공합니다. 트리-GRPO 목표와 q-값 소프트 클리핑: 이점은 평균 전용 정규화(mean-only normalization)를 가진 노드 수준 q(s), PPO(Proximal Policy Optimization) 스타일 비율의 상위 클리핑, 그리고 중간 q의 tanh 소프트 클리핑을 사용하여 폭발을 피하면서 그라디언트를 부드럽게 유지합니다. 터미널 보상은 ±1로 유지됩니다. 적응형 효율성: 어려운 항목을 필터링하고 솔루션을 캐시합니다. Pass1@K 임계값을 사용하여 '어려운 하위 집합'으로 반복적으로 필터링하고, 검증된 솔루션의 리플레이 버퍼를 유지하며, 캐시된 올바른 궤적이 존재할 때 전체 탐색을 건너뜁니다. 이는 지식을 보존하고 계산을 절약합니다. 결과적으로 훨씬 적은 계산량으로 더 나은 정확도를 달성했습니다. AIME24/25, AMC23, MATH500, Minerva, Olympiad에서 DeepSearch-1.5B는 평균 62.95%를 기록하여 Nemotron-Research-Reasoning-Qwen-1.5B v2를 1.25% 포인트 능가합니다(7페이지 표 1). 단 +50 RL 단계로 약 330 GPU 시간을 사용하여, 1,883 GPU 시간 후 62.02%에서 정체되는 확장 훈련을 능가합니다. 절제 연구는 전역 프론티어 선택이 바닐라 UCT에 비해 보상을 개선하고 반복을 줄이며, 최종 이득은 새로운 q-백업, 노드 수준 이점, 평균 전용 정규화 및 프론티어 선택의 조합에서 비롯됨을 보여줍니다.
    DeepSearch는 탐색(search) 알고리즘을 강화 학습 훈련 과정에 통합하는 혁신적인 접근 방식을 제시하며, 이는 AI의 복잡한 문제 해결 능력에 큰 영향을 미칩니다. 전통적으로 MCTS는 추론 단계에서 사용되어 왔지만, DeepSearch는 이를 훈련에 활용함으로써 모델이 더욱 깊이 있는 탐색을 수행하고, 올바른 행동에 대한 크레딧을 더 효과적으로 할당하며, 전반적인 학습 효율성을 높일 수 있음을 보여줍니다. 이러한 방법론은 수학적 추론과 같은 상징적 추론이 필요한 영역에서 특히 유용하며, 게임 AI, 과학적 발견, 복잡한 계획 수립 등 다양한 분야로 확장될 수 있습니다. 대규모 행동 공간에서 MCTS를 확장하는 것은 여전히 도전 과제이지만, DeepSearch는 이를 효율적으로 관리하기 위한 여러 기술적 해법을 제시합니다. 이는 딥러닝의 패턴 인식 능력과 상징적 추론의 탐색 및 계획 능력을 결합하는 미래 AI 연구의 중요한 방향을 제시하며, AI가 더욱 지능적이고 자율적인 문제 해결자가 될 수 있는 길을 열어줍니다. [논문](Paper) | [트윗](Tweet)

7.  **확산 LLM 가속화: 효율적인 텍스트 생성의 길**
    가볍게 학습된 정책은 어떤 토큰이 이미 '최종' 상태인지, 그리고 언제 생성을 멈춰야 하는지를 판단함으로써 확산 모델 기반 LLM 디코딩 과정을 가속화합니다. 연구자들은 토큰 신뢰도 신호를 분석하는 소형 MLP(Multi-Layer Perceptron) 필터를 훈련하고, [EoT] (End-of-Text) 토큰이 확실하게 생성되는 즉시 디코딩을 중단시키는 '텍스트 종료 예측' 기능을 추가했습니다. LLaDA-8B-Instruct 모델에서 이러한 접근 방식은 정확도 손실을 거의 없거나 최소화하면서 상당한 처리량 향상을 가져왔습니다. 문제 및 통찰: 반자동 회귀 확산 LLM은 토큰 업데이트를 병렬화하지만, 정적 휴리스틱은 이미 올바른 토큰을 계속해서 다시 마스킹합니다. 이 논문은 올바른 예측 즉시 토큰을 언마스킹하는 오라클 전략인 Extremely Greedy Parallel을 정의하고 속도 향상을 위한 큰 여유 공간을 보여줍니다. 방법: Learn2PD 필터는 토큰 신뢰도 패턴에 대해 2계층 MLP 필터 fθ를 훈련하여 위치당 '최종화 또는 재마스킹'을 예측합니다. 필터만 BCE(Binary Cross-Entropy) 손실로 훈련되며, dLLM은 고정된 상태로 유지됩니다. 추론은 필터의 로짓에 임계값 τ를 적용하여 토큰을 확정합니다. EoTP로 조기 중단: 텍스트 끝 예측은 [EoT]가 디코딩되면 중단되어, [EoT]로 가득 찬 긴 꼬리(long tail)를 피합니다. 부록 B는 길이 1024에서 추가 계산량의 약 89.59%가 EoT 후 패딩에서 발생한다고 언급합니다. 결과: GSM8K, MATH, HumanEval 및 MBPP에서 Learn2PD만으로 길이에 따라 3~12배의 속도 향상을 얻습니다. Learn2PD+EoTP는 GSM8K에서 길이 1024에서 22.58배에 도달하며 정확도는 유지되거나 약간 향상됩니다. KV 캐시와 결합하면 작은 정확도 절충으로 처리량을 57.51배까지 더욱 향상시킵니다. 더 긴 시퀀스가 더 많은 이점을 얻습니다; 표 4는 가속이 길이 128에서 3.36배에서 1024에서 22.58배로 증가함을 보여줍니다. 엔지니어링 노트: 필터는 작고 훈련이 빠릅니다: 블록 크기 32의 경우 약 2k 매개변수를 가지며, 짧은 데이터 수집 패스 후 단일 T4에서 몇 분 만에 훈련됩니다. 추론 시 오버헤드는 이득에 비해 무시할 수 있습니다. 이 방법은 KV 캐싱과 직교하며 기존 dLLM 디코더에 쉽게 통합될 수 있습니다.
    대규모 언어 모델(LLM)의 추론 지연 시간은 실시간 응용 프로그램에서 큰 제약이 됩니다. 본 연구는 확산 모델 기반 LLM의 텍스트 생성 속도를 획기적으로 향상시키는 방법을 제시하며, 이는 LLM의 실용적인 활용도를 높이는 데 기여합니다. 확산 모델은 텍스트 생성에서 높은 품질을 보여주지만, 본질적으로 반복적인 디코딩 과정 때문에 속도 문제가 있었습니다. Learn2PD 필터와 EoTP 같은 기술은 이러한 문제를 해결하기 위한 중요한 단계입니다. 투기적 디코딩(speculative decoding), 양자화(quantization) 등 다른 LLM 가속화 기법들과 함께, 이러한 접근 방식은 LLM이 더 빠르고 효율적으로 작동하도록 돕습니다. 처리량의 상당한 증가는 챗봇, 자동 코드 생성, 실시간 번역 등 다양한 실시간 AI 서비스의 사용자 경험을 개선할 것입니다. 특히 긴 텍스트 시퀀스에서 더욱 큰 이점을 얻을 수 있다는 점은 복잡한 문서 요약이나 장문 생성 작업에서 LLM의 활용 가치를 높여줄 것입니다. 이러한 엔지니어링 최적화는 LLM 기술의 상업적 적용 가능성을 넓히고, AI가 더욱 폭넓은 산업 분야에 통합되는 데 중요한 역할을 할 것입니다. [논문](Paper) | [트윗](Tweet)

8.  **작은 모델을 위한 추론 트레이스: 역 투기적 디코딩(RSD)의 힘**
    소형 모델은 대규모 교사 모델의 길고 수준 높은 사고 연쇄(CoT)를 사용하여 지도 미세 조정(SFT)을 수행할 때 종종 성능 저하를 겪습니다. 본 연구는 그 원인을 명확히 밝히고, 역 투기적 디코딩(Reverse Speculative Decoding, RSD)이라는 기법으로 이를 해결합니다. 이는 교사 모델이 토큰을 제안하면, 학생 모델은 자신의 확률 분포에서 해당 토큰이 타당할 경우에만 이를 수락하는 방식입니다. 그 결과, 학생 모델의 분포와 부합하면서도 정확도를 유지하는 추론 경로가 생성되어, 소형 모델이 효과적으로 학습할 수 있게 됩니다. 핵심 아이디어: 각 단계에서 교사 토큰을 샘플링하고, 학생이 ≥ p_th 확률을 할당하는 경우에만 유지하며, 그렇지 않으면 학생 자신의 토큰으로 되돌아갑니다. 이는 작은 모델이 추적할 수 없는 높은 놀라움 스파이크(high-surprisal spike)를 필터링하여 논리를 단순화하지 않고 토큰 수준 난이도를 완화합니다. 왜 중요한가: s1K-1.1 트레이스에 대한 Qwen3-0.6B의 직접 SFT는 평균 정확도를 20.5% 저하시킵니다. 대신 RSD 트레이스로 훈련하면 AIME24, AIME25, GPQA-Diamond, MATH500 전반에서 평균 4.9%의 이득을 얻습니다. 작은 모델에 효과적인 데이터 레시피: 토크나이저 호환 교사(s1.1-7B)와 학생(Qwen3-0.6B)을 사용합니다. 거부 샘플링(rejection sampling)으로 RSD 트레이스를 생성합니다; 문제가 해결될 수 없을 때, UPFT 스타일 접두사 훈련(prefix training)을 통해 처음 128개 토큰을 복구합니다. 180개의 완전한 솔루션과 많은 접두사에도 불구하고, 0.6B 학생은 개선되어 분포 정렬(distributional alignment)이 볼륨보다 중요함을 보여줍니다. 주요 진단: 가장 강력한 실패 예측 변수는 학생 하위 1% 토큰의 비율입니다. s1K-1.1 트레이스는 그러한 토큰을 많이 포함하고 학습을 저하시킵니다; RSD는 이를 거의 0으로 줄입니다. 보편적이지 않으며, 맞춤화되어야 합니다: RSD 트레이스는 모델별로 다릅니다. Qwen3-0.6B를 "승인자"로 사용하여 구축된 트레이스는 Qwen3-1.7B, Llama-3.2-1B, Gemma-3-1B 또는 Phi-4-Mini로 전이되지 않습니다. 대상 모델별로 RSD를 실행하는 것은 도움이 되지만, 동일한 모델에 대한 반복적인 다단계 RSD는 분포 드리프트(distributional drift)를 통해 성능을 저하시킵니다.
    이 연구는 대규모 모델의 지식을 소형 모델로 효과적으로 증류(distillation)하는 데 있어 중요한 통찰력을 제공합니다. 특히 CoT(Chain-of-Thought)와 같은 복잡한 추론 경로를 전달할 때, 단순히 교사의 출력을 모방하는 것만으로는 학생 모델의 성능이 저하될 수 있음을 보여줍니다. RSD는 학생 모델의 고유한 능력과 분포에 맞춰 학습 데이터를 조정함으로써, 소형 모델이 실제로 이해하고 활용할 수 있는 추론 경로를 생성합니다. 이러한 분포 정렬의 중요성은 모델 크기나 데이터의 양보다 학습의 질이 더 중요할 수 있음을 시사합니다. 이는 온디바이스 AI(on-device AI) 및 자원 제약이 있는 환경에서 AI 모델을 배포하는 데 필수적인 기술입니다. 소형 모델이 대규모 모델에 버금가는 추론 능력을 갖게 되면, 스마트폰, IoT 장치 등 다양한 기기에서 고급 AI 기능을 구현할 수 있게 됩니다. 미래에는 RSD와 같은 효율적인 모델 압축 및 지식 증류 기법이 더욱 발전하여, 다양한 하드웨어 제약 조건 속에서도 고성능 AI를 실현하는 데 기여할 것으로 기대됩니다. [논문](Paper) | [트윗](Tweet)

9.  **도구 사용 혼합(TUMIX): 다중 에이전트 추론의 시너지**
    TUMIX는 텍스트 처리, 코드 실행, 그리고 웹 검색 기능을 결합하여 추론 능력을 향상시키는 앙상블 기법입니다. 이 방법은 15가지의 상이한 에이전트들을 동시에 구동하고, 각 라운드를 통해 중간 결과들을 교환합니다. LLM 기반 심사관은 조기 종료를 제어하며, HLE, GPQA-Diamond, AIME 24/25와 같은 벤치마크에서 강력한 도구 증강 기준 모델 대비 최대 3.55%의 정확도 향상을 가져오고, 동시에 추론 비용을 약 50% 절감합니다.
    TUMIX는 멀티모달(multi-modal) 및 다중 에이전트(multi-agent) AI의 중요성을 강조하며, 복잡한 문제 해결을 위해 다양한 도구를 결합하는 시너지 효과를 보여줍니다. 텍스트 이해, 코드 실행, 웹 검색과 같은 이질적인 능력을 가진 에이전트들을 조화롭게 운영함으로써, 단일 모델로는 달성하기 어려운 수준의 추론 성능을 이끌어낼 수 있습니다. 이러한 접근 방식은 AI 에이전트가 현실 세계의 복잡한 시나리오에 대처하는 데 필수적인 요소가 될 것입니다. 여러 에이전트를 효율적으로 조율하고, 그들의 중간 결과물을 통합하며, 최적의 결정을 내리도록 하는 것은 여전히 도전적인 과제이지만, TUMIX는 LLM 기반 심사관을 통해 이러한 과정을 효과적으로 관리할 수 있음을 보여줍니다. 이는 자율 에이전트의 발전 가능성을 높이고, 과학 연구, 복잡한 시스템 관리, 지능형 의사결정 지원 등 다양한 분야에서 혁신적인 응용을 가능하게 할 것입니다. [논문](Paper) | [트윗](Tweet)

10. **PrompCoT 2.0: 동적 프롬프트 합성으로 LLM 추론 능력 극대화**
    PromptCoT 2.0은 기존 PromptCoT 1.0의 수동적 발견법을 넘어서, 더욱 복잡하고 다채로운 추론 프롬프트를 생성하기 위한 EM(기대-최대화) 기반 반복 과정을 도입했습니다. 이 방법론은 자기 주도 학습(self-play)과 지도 미세 조정(SFT) 훈련 방식을 모두 지원하며, AIME, HMMT, LiveCodeBench, Codeforces 등 다양한 추론 벤치마크에서 새로운 최첨단 성능을 기록했습니다. 이는 프롬프트 생성이 대규모 언어 모델(LLM) 추론 능력의 새로운 확장 축임을 시사합니다.
    PromptCoT 2.0은 프롬프트 엔지니어링 분야의 진화를 보여주는 대표적인 사례입니다. 단순히 수동으로 프롬프트를 작성하는 것을 넘어, AI 스스로가 학습을 통해 더욱 효과적이고 복잡한 프롬프트를 생성하는 능력은 LLM의 잠재력을 최대한 발휘하는 데 결정적인 역할을 합니다. 이러한 동적 프롬프트 생성은 LLM에게 일종의 자동화된 커리큘럼 학습을 제공하여, 모델이 점진적으로 더 어려운 추론 문제를 해결하도록 돕습니다. 이는 LLM이 새로운 문제 유형에 대한 일반화 능력을 향상시키고, 인간의 개입 없이도 지속적으로 학습하고 발전할 수 있는 기반을 마련합니다. 프롬프트 자체가 학습 가능한 인공물(artifact)이 됨으로써, LLM은 더욱 정교하고 미묘한 지시를 이해하고 따를 수 있게 될 것입니다. 미래에는 이러한 프롬프트 합성 기술이 더욱 발전하여, LLM이 다양한 도메인과 작업에 대한 최적의 접근 방식을 자율적으로 탐색하고 적용하는 데 핵심적인 역할을 할 것으로 기대됩니다. [논문](Paper) | [트윗](Tweet)