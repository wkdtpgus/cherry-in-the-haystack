다음은 원본 문서의 내용을 바탕으로 재구성 및 새로운 관점을 추가한 업데이트된 문서입니다.

1.  **확장형 환경 모형을 활용한 인공지능 주체 학습**
    신속하고 정밀한 마인크래프트 환경 모방 시스템을 구축하고, 네트워크 연결 없이 자체적으로 조작 가능한 인공지능 주체를 단련하는 확장형 상상 기반 강화 학습 방법론을 제안합니다. 해당 세계 모형은 개별 그래픽 처리 장치에서도 즉각적인 상호작용 시뮬레이션을 가능하게 하며, 시각적 원본 정보와 기본적인 입력 장치 조작만으로 오프라인 환경에서 '다이아몬드 채굴' 목표를 달성하도록 이끌었습니다. 이 접근법의 핵심은 속도와 안정성에 중점을 둔 인과 토크나이저와 블록-인과 동역학 트랜스포머의 결합입니다. 숏컷 포싱(shortcut forcing) 기법은 램프 손실(ramped loss)을 활용하여 모델이 큰 디노이징(denoising) 단계(K=4)를 수행하도록 훈련시키며, 이는 누적 오류를 효과적으로 줄이고 적은 수의 단계에서도 높은 품질을 유지하게 합니다. 또한, 시공간 분리 어텐션 레이어(attention layer)와 4단계마다 배치되는 시간 레이어, GQA(Grouped Query Attention) 기술, 그리고 길고 짧은 배치(batch)의 교차 사용은 KV 캐시(cache) 크기를 최소화하고 추론 속도를 극대화하는 데 기여합니다.

    이 모델은 640×360 해상도에서 9.6초의 긴 문맥(context)을 20+ FPS로 실시간 처리하여 시각적 요소뿐 아니라 게임 내 역학(mechanics)까지 포괄하는 능력을 보여주며, 이는 기존 마인크래프트 모델 대비 월등히 긴 컨텍스트 처리 능력입니다. 휴먼-인-더-루프(human-in-the-loop) 플레이 테스트(play test)를 통해 16가지 과제 중 14개에서 Dreamer 4가 성공을 거두었습니다. 이는 블록 배치/파괴, 도구 전환, 보트 탑승, 화로 사용, 포털 진입 등 복잡한 상호작용을 포함하며, Oasis/Lucid와 같은 다른 모델들이 많은 객체 상호작용 과제에서 실패한 것과 대조적입니다. 특히 주목할 만한 점은 환경과의 상호작용 없이 오직 2.5K 시간의 VPT 계약자 데이터셋(dataset)만으로 훈련된 에이전트(agent)가 오프라인 다이아몬드 챌린지에서 뛰어난 성과를 보였다는 것입니다. 이 에이전트는 작업 토큰(task token)에 조건화(condition)되고 상상 RL(imagination RL)(행동 사전(behavioral prior)을 가진 PMPO)을 통해 개선되었습니다. 한 시간 분량의 시뮬레이션에서 철제 곡괭이 제작에 29%, 다이아몬드 발견에 0.7%의 성공률을 보였으며, 이는 미세 조정된 VPT나 Gemma-3 VLA와 같은 기존의 강력한 오프라인 모델들을 능가하는 성과입니다. 특히, 유튜브 데이터로 사전 학습된 VPT 방식 대비 약 100배 적은 양의 훈련 자료만으로 이러한 결과를 얻었습니다. 이러한 데이터 효율성은 실제 환경에서의 데이터 수집 비용과 시간을 고려할 때 매우 중요한 진전입니다.

    적은 양의 쌍 데이터(paired data)로부터의 액션 그라운딩(action grounding) 능력은 OOD(Out-of-Distribution) 환경으로의 일반화 가능성을 입증합니다. 전체 2,541시간의 비디오(video) 중 액션(action)이 명시된 데이터는 100시간에 불과했지만, 모델은 액션 조건부 예측(action-conditioned prediction)에서 전체 액션 모델의 약 85% PSNR(Peak Signal-to-Noise Ratio)과 100% SSIM(Structural Similarity Index Measure)에 도달했습니다. 오버월드(Overworld)에서만 훈련된 액션 조건화(action conditioning)는 액션 없이 관찰된 네더(Nether)/엔드(End) 장면으로 전이되어, 전체 액션 모델의 약 76% PSNR과 80% SSIM을 달성했습니다. 이는 모델이 학습된 환경을 넘어 새로운 환경에서도 유의미한 행동을 예측할 수 있음을 의미합니다. 모델과 일관성을 유지하는 에이전트 파인튜닝(finetuning) 및 상상 RL(imagination RL) 기법은 작업 토큰이 잠재 변수, 액션 및 레지스터와 교차되며, 헤드는 다중 토큰 예측으로 정책(policy), 보상(reward) 및 가치(value)를 예측합니다. 상상 롤아웃(imagination rollout)은 고정된 월드 모델에서 샘플링(sample)하고, PMPO는 복제된 BC 정책에 대한 역-KL(reverse-KL)을 사용하여 부호 기반 이점(sign-based advantage)을 최적화함으로써 온라인(online) 데이터 없이도 견고성(robustness)과 샘플 효율성(sample efficiency)을 향상시킵니다. 이러한 발전은 게임 환경뿐만 아니라 로봇 공학, 자율 주행 등 실제 세계의 복잡한 시뮬레이션 및 제어 문제 해결에 중요한 시사점을 제공합니다. [논문](Paper) | [트윗](Tweet)

2.  **DeepSeek-V3.2-Exp: 장문맥 처리의 새로운 지평**
    DeepSeek은 기존 V3.1 '터미너스' 구조에 정교한 희소 집중 기법(DeepSeek Sparse Attention, DSA)을 도입하여, 128K 길이의 문맥에서도 성능 저하 없이 상당한 비용 효율성을 달성했습니다. 이 혁신적인 기술은 대규모 언어 모델(LLM)의 긴 문맥 처리 능력을 획기적으로 개선하며, 관련 모델과 추론 코드(inference code)가 모두 공개되어 연구 커뮤니티에 큰 기여를 하고 있습니다. 긴 컨텍스트(context) 처리는 LLM이 복잡한 문서 요약, 장문의 코드 분석, 대화 기록 이해 등 다양한 고급 작업을 수행하는 데 필수적이지만, 기존 어텐션(attention) 메커니즘의 O(L²) 복잡도로 인해 막대한 계산 비용을 수반했습니다.

    DSA의 구조는 다음과 같습니다: 소형 FP8 '초고속 색인기'가 질의별로 이전 토큰들의 중요도를 평가하고, 상위 k개 선택기가 주요 집중 과정을 위해 해당하는 키-값 쌍(KV entry)만을 추출합니다. 이 방식은 색인기의 부담을 최소화하면서도, 핵심 집중 연산의 복잡도를 기존의 O(L²)에서 대략 O(L·k) 수준으로 크게 낮춥니다. 이는 컨텍스트 길이에 따른 계산량 증가를 완화하여, 더 긴 시퀀스를 훨씬 효율적으로 처리할 수 있게 합니다. 훈련 과정은 128K V3.1 체크포인트(checkpoint)에서 시작하며, 밀집 어텐션(dense attention)으로 웜업(warm-up)하는 동안 약 2.1B 토큰(token)에 대해 밀집 어텐션 분포에 대한 KL(Kullback-Leibler) 발산을 통해 인덱서(indexer)만을 훈련합니다. 이후 희소 훈련(sparse training)으로 전환하여 쿼리(query)당 k=2048개의 선택된 KV 토큰으로 모든 가중치(weight)를 최적화하며, 이 단계는 약 944B 토큰(token)에 걸쳐 진행됩니다. DSA의 영향을 정확히 분리하기 위해 V3.1과 동일한 파이프라인(pipeline)으로 후속 훈련(post-train)을 수행했습니다.

    후속 훈련 스택(stack)은 수학, 경쟁 프로그래밍, 논리적 추론, 에이전트 코딩, 에이전트 검색의 5개 도메인에 대한 전문가 증류(specialist distillation)와 글쓰기 및 QA(질의응답)를 포함합니다. 이어서 GRPO(Generalized Policy Optimization)를 활용한 단일 혼합 RL(Reinforcement Learning) 단계에서 추론(reasoning), 에이전트 행동(agent behavior) 및 정렬(alignment)의 균형을 맞춥니다. RL 설계는 최종 결과 보상(outcome reward), 길이 페널티(length penalty) 및 언어 일관성 보상(language-consistency reward)을 복합적으로 사용합니다. 결과적으로, DeepSeek-V3.2-Exp는 일반, 코드, 검색 에이전트 및 수학 스위트(suite) 전반에서 V3.1과 동등한 품질을 유지합니다. 표 1에서 대부분의 지표에서 거의 동등한 성능을 보이며, GPQA/HLE/HMMT에서 나타난 작은 성능 하락은 유사한 토큰 길이를 가진 체크포인트(checkpoint)를 사용할 때 사라지는 것으로 확인되었습니다. BrowseComp 및 SWE Verified 벤치마크에서도 RL 곡선은 DSA 적용 후에도 안정적으로 유지되었습니다. 비용 및 지연 시간(latency) 측면에서 이 연구는 긴 컨텍스트에서 프리필링(prefilling)과 디코딩(decoding) 모두에 대해 명확한 종단 간(end-to-end) 토큰 위치 비용 절감을 입증합니다. 짧은 프리필(prefill)의 경우, DSA를 효율적으로 시뮬레이션(simulate)하기 위한 마스크된 MHA(Multi-Head Attention) 경로를 제공합니다. 전반적인 효과는 정확도를 유지하면서 훨씬 저렴한 비용으로 긴 컨텍스트 서비스를 제공할 수 있다는 점입니다. 이는 대규모 언어 모델의 상업적 활용과 연구 확장성에 있어 중요한 이정표가 될 것입니다. [기술 보고서](Technical Report) | [트윗](Tweet)

3.  **실제 인간 상호작용을 통한 AI 학습의 새 시대**
    본 연구는 고정된 주석자의 분류 대신, 실제 대화 참여자들의 상호작용 기록으로부터 직접 배우는 사후 훈련 방법론을 소개합니다. 이는 기존의 정적인 데이터셋(dataset)에 의존하는 방식의 한계를 극복하고, 사용자의 역동적인 피드백을 직접 학습 과정에 통합함으로써 인공지능의 성능을 향상시키는 패러다임 전환을 의미합니다. RLHI(Reinforcement Learning from Human Interaction)는 사용자의 지시에 따른 내용 수정(추가적인 발언을 교정 신호로 활용)과 인물 기반 보상(특정 인물 정보를 조건으로 하는 보상 모형으로 후보들을 평가) 기법을 융합합니다. 이러한 접근 방식은 AI가 단순히 정답을 암기하는 것을 넘어, 사용자의 의도와 개성을 더 잘 이해하고 반영하도록 돕습니다.

    WildChat 대화 데이터를 기반으로 훈련된 이 모델은 개인화(personalization)와 지시 따르기(instruction following) 능력에서 강력한 개선을 보였으며, 이러한 개선은 추론 작업(reasoning task)으로도 전이되는 것을 확인했습니다. 페르소나(persona)는 장기적인 사용자 기록에서 추출되어 추론(inference) 시 모델 입력에 추가됩니다. 훈련은 재작성 및 보상 순위 쌍에 대해 페르소나 조건부 DPO(Direct Preference Optimization)를 사용합니다. 실제 채팅(chat) 데이터는 특히 대화의 후반부에서 풍부한 수정 신호(correction signal)를 포함하여, AI 모델에게 밀도 높은 감독(dense supervision)을 제공합니다. WildChat 기반 평가에서 재작성 기능은 개인화 및 선호도를 향상시키는 것으로 나타났으며, 페르소나 기반 보상(persona-based reward)은 지시 따르기에서 우위를 점했습니다. 벤치마크(benchmark) 결과도 매우 고무적입니다. AlpacaEval 2.0에서 77.9%의 승률을 기록했으며, Arena-Hard에서도 경쟁력 있는 성능을 보여주었습니다. 특히, 수학/과학 데이터셋(dataset) 전반에서 추론 정확도(reasoning accuracy)가 26.5%에서 31.8%로 상승하여, 복잡한 문제 해결 능력에서의 진전을 입증했습니다.

    주요 절제 연구(ablation study) 결과는 상호작용 데이터의 경우 SFT(Supervised Fine-Tuning)보다 RL(Reinforcement Learning)이 더 효과적이며, 강력한 품질 필터(quality filter)가 필수적임을 보여줍니다. 또한, 사용자당 대화 깊이보다는 사용자 다양성(user diversity)이 모델 성능에 더 중요하게 작용한다는 점을 밝혀냈습니다. 이는 AI가 다양한 사용자 경험을 통해 학습할 때 더욱 견고하고 일반화된 능력을 갖출 수 있음을 시사합니다. 향후 연구 방향으로는 온라인 연속 학습(online continual learning)을 통해 모델이 실시간으로 새로운 상호작용을 학습하고 적응하는 능력, 더 안전한 보상 모델링(reward modeling)을 통해 AI의 행동을 더욱 신뢰할 수 있게 만드는 것, 그리고 개인 정보 보호(privacy-preserving)를 고려한 개인화(personalization) 기술 개발이 포함됩니다. 이러한 연구는 사용자 중심의 AI 시스템을 구축하고, AI와 인간의 상호작용 방식을 혁신하는 데 기여할 것입니다. [논문](Paper) | [트윗](Tweet)

4.  **JEPA 재고: 정적 교사 비대칭 잠재 학습(SALT)**
    애플은 정적 교사 비대칭 잠재 학습(SALT)이라는 새로운 기법을 제시했습니다. 이는 기존 V-JEPA의 간소화된 2단계 방식으로서, 첫째, 화소 재구성을 통해 교사 모델을 숙련시키고, 그 후 이 교사를 고정시킨 채 학생 모델을 훈련하여 가려진 부분에서 교사의 잠재적 표현을 모방하도록 만듭니다. 이 접근법은 EMA(지수 이동 평균)의 필요성을 없애고, 교육자와 학습자 간의 역할을 명확히 분리하며, 연산 부담을 줄이면서도 모델 선정 과정을 더욱 투명하게 만듭니다. 자기 지도 학습(self-supervised learning) 분야에서 교사-학생 모델 간의 효과적인 지식 전달은 핵심 과제이며, 특히 비디오와 같은 고차원 데이터에서는 더욱 복잡합니다. SALT는 이러한 문제를 우아하게 해결하며, 모델 붕괴(collapse) 메커니즘을 제거하여 학습의 안정성을 크게 높입니다.

    EMA 없이 확장되는 이 학습 방식은 두 단계로 구성됩니다. 1단계에서는 VideoMAE(Video Masked Autoencoder) 스타일의 픽셀 재구성 목표(pixel reconstruction objective)를 사용하되, V-JEPA의 다중 블록 마스킹(multi-block masking)(V-Pixel이라고 명명)을 적용하여 비디오 인코더(video encoder)를 훈련합니다. 2단계에서는 훈련된 인코더를 고정하고, 학생 인코더와 예측기(predictor)를 훈련하여 마스크된 영역에서 교사의 잠재 변수(latent)와 일치하도록 만듭니다. 이 두 가지 손실 함수는 모두 안정적이고 예측 가능하게 작동하여, 학습 과정의 신뢰도를 높입니다. 낮은 계산량으로도 더 나은 고정 백본(frozen-backbone) 결과를 얻을 수 있다는 점이 SALT의 큰 장점입니다. V-3.6M 믹스(mix) 데이터셋에서 동일한 사전 훈련(pretraining) 단계에서 SALT는 V-JEPA 2보다 평균 Top-1 정확도를 향상시켰으며, 학생 모델의 크기에 따라 성능이 잘 확장되는 경향을 보였습니다. 특히 ViT-g/G SALT 학생 모델들은 SSv2(Something-Something V2) 벤치마크에서 최고 성능을 달성했고, K400(Kinetics-400)에서도 경쟁력 있는 결과를 보여주었습니다.

    "약한 교사, 강한 학생"이라는 흥미로운 현상도 관찰되었습니다. 작거나 최적이 아닌 교사 모델에 의해 훈련된 학생 모델들도 여전히 SOTA(State-Of-The-Art) 수준의 성능을 달성할 수 있다는 것입니다. 예를 들어, 최고의 ViT-L 학생 모델은 ViT-L 교사 모델만을 사용했으며, 심지어 ViT-G 학생 모델조차 ViT-L 교사 모델로 최고 성능을 달성했습니다. 이는 고성능 교사 모델을 구축하기 위한 막대한 자원 없이도 강력한 학생 모델을 만들 수 있음을 시사하며, 자원 제약이 있는 환경에서 매우 유용합니다. SALT의 학생 훈련 손실(student training loss)은 다운스트림(downstream) 고정 정확도(frozen accuracy)와 밀접하게 상관 관계를 보입니다. 이는 EMA JEPA와 같이 손실이 좋지 않은 대리 지표(proxy)였던 것과 달리, 사전 훈련(pretraining) 중 해석 가능한 모델 선택(model selection)을 가능하게 하는 실질적인 훈련 신호(training signal)를 제공합니다. 중요한 마스킹(masking) 및 데이터(data) 선택 전략 또한 성능에 영향을 미칩니다. 교사 모델의 경우, 다중 블록 마스킹이 무작위 튜브(random tube) 및 인과 마스킹(causal masking)보다 우수했습니다. 데이터 믹스(data mix)의 견고성도 확인되었는데, K710 전용 또는 Panda2.8M 전용 교사 모델도 여전히 강력한 학생 모델을 배출했으며, V-3.6M이 전반적으로 최고의 성능을 보였습니다. 이러한 연구 결과는 비전 분야의 기반 모델(foundation model) 개발에 새로운 방향을 제시하며, 더욱 효율적이고 강력한 자기 지도 학습 시스템 구축에 기여할 것입니다. [논문](Paper) | [트윗](Tweet)

5.  **Agent S3: 행동 최적화 에이전트의 발전**
    본 연구는 행동 최적 N개 선정(bBoN) 기법을 제안합니다. 이는 여러 개의 완전한 컴퓨터 지원 자동화(CUA) 과정을 동시에 수행하고, 각 실행 결과를 압축된 행위 서사로 요약한 뒤, 상호 비교를 통해 가장 우수한 경로를 선별하는 방식입니다. 이러한 접근 방식은 에이전트(agent)의 신뢰성과 견고성을 획기적으로 향상시키며, 특히 개방형 운영체제(OSWorld) 환경에서 에이전트의 복잡한 작업을 안정적으로 수행하는 데 기여합니다. 향상된 기본 주체(Agent S3)와 결합하여, 이 방법론은 OSWorld 벤치마크에서 최신 기술 수준을 확립했으며, 마이크로소프트 윈도우 및 안드로이드 운영체제로의 일반화 가능성을 입증했습니다. 이는 GUI(Graphical User Interface) 기반 시스템에서 AI 에이전트가 직면하는 다양한 문제, 예를 들어 불확실한 환경 인식이나 복잡한 상호작용 시퀀스 처리 등을 효과적으로 해결할 수 있음을 보여줍니다.

    행동 최적 N개 선정(bBoN)은 여러 개의 완전한 롤아웃(rollout)을 샘플링(sample)하고, 각각을 전/후 델타(delta) 및 포인터 크롭(pointer crop)으로 요약한 다음, 원샷 MCQ(Multiple Choice Question) 심사위원(judge)을 통해 우승자를 선택하는 방식으로 작동합니다. 이는 단순한 최종 결과 비교를 넘어, 각 경로의 행동 시퀀스를 분석하여 최적의 판단을 내리는 정교한 메커니즘입니다. Agent S3 기준선(baseline)은 통합 코딩 서브 에이전트(coding sub-agent)를 포함하는 더 평탄한 루프(loop) 구조를 가지며, Agent S2에 비해 성공률을 높이고 LLM(Large Language Model) 호출(call) 및 실제 시간(wall time)을 줄이는 효율성을 보여줍니다. 결과적으로 Agent S3는 100단계에서 OSWorld의 새로운 SoTA(State-Of-The-Art)를 달성하며 강력한 효율성 이득을 보여주었고, 이 접근 방식은 윈도우 및 안드로이드 설정으로 성공적으로 전이되었습니다.

    확장성 측면에서, N(병렬 실행되는 에이전트 수)이 증가함에 따라 정확도도 상승하는 경향을 보였습니다. 또한, 모델 다양성(model diversity)을 확보하는 것이 Pass@N 성능을 개선하는 데 중요하며, 단일 라운드 비교 선택(single-round comparative selection) 방식은 더 낮은 비용으로 쌍별 토너먼트(pairwise tournament) 방식과 동등하거나 더 우수한 결과를 보여주었습니다. 이는 자원 효율성을 유지하면서도 최적의 에이전트 행동을 찾아낼 수 있음을 의미합니다. 실용적인 시사점으로는 동일한 스냅샷(snapshot)에서 병렬 VM(virtual machine)을 시작하고, 검증 가능한 델타(delta)를 방출하도록 단계를 계측하며, N을 4에서 10 사이로 시작하고, 예산이 허락한다면 다양한 강력한 모델(model)을 추가하는 것이 권장됩니다. 한계점으로는 독립적인 병렬 실행을 가정한다는 점이 있습니다. 즉, 공유된 실제 데스크톱(desktop) 환경에서 발생할 수 있는 부작용이 시도 전반에 걸쳐 예상치 못하게 영향을 미칠 수 있다는 점입니다. 그럼에도 불구하고 Agent S3는 복잡한 소프트웨어 환경에서 AI 에이전트의 실용성과 신뢰성을 크게 향상시키는 중요한 발걸음으로 평가됩니다. [논문](Paper) | [트윗](Tweet)

6.  **DeepSearch: 훈련 시간 몬테카를로 트리 탐색으로 수학적 추론 강화**
    딥서치는 검증 가능한 보상 체계를 갖춘 몬테카를로 트리 탐색(MCTS) 기법을 강화 학습(RL) 과정에 직접 결합하는데, 이는 추론 단계가 아닌 학습 단계에서 이루어집니다. 이 혁신적인 접근 방식은 기존 RL의 주요 난제 중 하나인 희소한 보상(sparse rewards) 문제와 탐색(exploration) 비효율성을 효과적으로 해결합니다. 이로 인해, 강력한 1.5B급 기본 모델과 비교했을 때, 수학적 문제 해결에서 더욱 광범위한 탐색 능력, 개선된 기여도 배분, 그리고 월등히 높은 샘플 효율성을 보여줍니다. 이는 특히 복잡한 수학적 추론과 같이 명확한 중간 단계 보상이 부족한 영역에서 AI의 학습 능력을 크게 향상시킵니다.

    딥서치는 추론 시간뿐만 아니라 훈련 시간에도 탐색(exploration)을 수행합니다. MCTS는 RL 루프(loop) 내에 두 가지 선택기(selector)와 함께 내장됩니다: 형제 노드(sibling node) 비교를 위한 로컬 UCT(Upper Confidence Bound 1 applied to trees)와 전체 트리(tree)에서 다음 리프(leaf)를 선택하는 전역 프론티어 스코어러(global frontier scorer)입니다. 프론티어 점수(frontier score)는 부모 품질(parent quality), 정책 엔트로피(policy entropy) 및 깊이 보너스(depth bonus) √(d/dT)를 결합하여 최적의 탐색 경로를 결정합니다. 이 모델은 올바른 경로뿐만 아니라 '확신에 찬 오답' 경로까지 모두 감독합니다. 올바른 터미널(terminal)이 발견되지 않으면, DeepSearch는 감독을 위해 경로를 따라 가장 낮은 평균 엔트로피(entropy)를 가진 부정적인 궤적(trajectory)을 선택합니다. 이는 제약된 업데이트(constrained update)로 노드(node) 값을 백업(back up)하여 올바른 경로의 노드가 음수가 아닌 상태로 유지되도록 합니다. 이러한 방식은 최종 결과 보상(outcome reward)만 제공하는 대신, 미세한 단계 수준 이점(step-level advantage)을 제공하여 학습 과정의 효율성을 높입니다.

    트리-GRPO(Generalized Policy Optimization) 목표(objective)와 q-값 소프트 클리핑(soft clipping) 기법이 적용됩니다. 이점(advantage)은 평균 전용 정규화(mean-only normalization)를 가진 노드 수준 q(s) 값, PPO(Proximal Policy Optimization) 스타일 비율의 상위 클리핑(clip-higher), 그리고 중간 q 값의 tanh 소프트 클리핑을 사용하여 그라디언트(gradient) 폭발을 피하고 부드럽게 유지합니다. 터미널 보상(terminal reward)은 ±1로 유지됩니다. 적응형 효율성(adaptive efficiency)을 위해 DeepSearch는 어려운 항목을 필터링(filter)하고 솔루션(solution)을 캐시(cache)합니다. Pass1@K 임계값(threshold)을 사용하여 '어려운 하위 집합(hard subset)'으로 반복적으로 필터링하고, 검증된 솔루션의 리플레이 버퍼(replay buffer)를 유지하며, 캐시된 올바른 궤적(trajectory)이 존재할 때 전체 탐색을 건너뜁니다. 이는 기존 지식(knowledge)을 보존하고 계산(compute) 자원을 절약하는 데 크게 기여합니다.

    결과적으로 DeepSearch는 훨씬 적은 계산량으로 더 나은 정확도를 달성했습니다. AIME24/25, AMC23, MATH500, Minerva, Olympiad 벤치마크에서 DeepSearch-1.5B는 평균 62.95%를 기록하여 Nemotron-Research-Reasoning-Qwen-1.5B v2를 1.25% 포인트(pp) 능가했습니다(7페이지 표 1 참조). 특히, 단 50번의 RL 단계(step)와 약 330 GPU 시간만을 사용하여 이러한 성과를 달성했으며, 이는 1,883 GPU 시간 후 62.02%에서 정체되는 확장 훈련(extended training) 모델의 성능을 뛰어넘는 것입니다. 절제 연구(ablation study)는 전역 프론티어 선택(global frontier selection)이 바닐라 UCT(vanilla UCT)에 비해 보상(reward)을 개선하고 반복(iteration)을 줄임을 보여줍니다. 최종 성능 향상은 새로운 q-백업(q-backup), 노드 수준 이점(node-level advantage), 평균 전용 정규화(mean-only normalization) 및 프론티어 선택(frontier selection)의 조합에서 비롯됩니다. DeepSearch는 복잡한 추론 문제 해결을 위한 AI 에이전트 개발에 있어 중요한 진전을 보여주며, 특히 학습 효율성과 성능 향상에 크게 기여할 것으로 기대됩니다. [논문](Paper) | [트윗](Tweet)

7.  **확산 LLM(Diffusion LLM) 가속화: 효율적인 토큰 생성 전략**
    경량화된 학습 정책은 특정 단위(토큰)가 이미 완성된 상태인지, 그리고 문자열 생성을 언제 멈춰야 할지를 판단함으로써 확산 기반 대규모 언어 모델(LLM)의 디코딩 속도를 가속화합니다. 이는 LLM 추론(inference) 과정의 고질적인 계산 비용 문제를 해결하려는 중요한 시도입니다. 연구진은 단위(토큰)별 신뢰도 지표를 바탕으로 소형 다층 퍼셉트론(MLP) 필터를 훈련시키고, [문서 끝] 기호가 안정적으로 발생하면 즉시 생성 과정을 멈추는 '텍스트 종료 예측' 기능을 추가했습니다. 이러한 이중 전략은 정확도를 유지하거나 오히려 개선하면서도, 처리량(throughput)을 크게 향상시키는 효과를 가져옵니다.

    문제와 통찰: 반자동 회귀 확산 LLM은 토큰 업데이트(token update)를 병렬화(parallelize)하지만, 정적인 휴리스틱(heuristic)은 이미 올바른 토큰을 계속해서 다시 마스킹(remasking)하는 비효율성을 야기합니다. 이 논문은 올바른 예측 즉시 토큰을 언마스킹(unmasking)하는 오라클 전략(oracle strategy)인 Extremely Greedy Parallel을 정의하고, 이를 통해 속도 향상을 위한 큰 여유 공간이 있음을 보여줍니다. 즉, 이론적으로 가능한 최대 가속도를 제시하여 실제 시스템의 개선 잠재력을 시사합니다. 방법론적으로, Learn2PD 필터는 토큰 신뢰도 패턴에 대해 2계층 MLP 필터 fθ를 훈련하여 위치당 '확정 또는 재마스킹(finalize or remask)'을 예측합니다. 이 필터는 BCE(Binary Cross-Entropy) 손실(loss)을 사용하여 훈련되며, 확산 LLM 자체는 고정된 상태를 유지합니다. 추론(inference) 시에는 필터의 로짓(logit)에 임계값(threshold) τ를 적용하여 토큰을 확정합니다. 또한, EoTP(End-of-Text Prediction)로 조기 중단(early stopping)하여 [EoT]가 디코딩(decode)되면 생성을 중단하여, [EoT]로 가득 찬 긴 꼬리(long tail) 생성을 피합니다. 부록 B에서는 길이 1024에서 추가 계산량의 약 89.59%가 EoT 이후 패딩(padding)에서 발생한다고 언급하며, EoTP의 중요성을 강조합니다.

    결과적으로, LLaDA-8B-Instruct 모델에서 Learn2PD만으로 GSM8K, MATH, HumanEval 및 MBPP 벤치마크에서 길이에 따라 3~12배의 속도 향상을 얻었습니다. Learn2PD+EoTP를 결합했을 때는 GSM8K에서 길이 1024일 때 22.58배에 달하는 가속을 달성했으며, 정확도는 유지되거나 약간 향상되었습니다. KV 캐시(cache)와 결합하면 작은 정확도 절충(tradeoff)으로 처리량(throughput)을 57.51배까지 더욱 향상시켰습니다. 특히 긴 시퀀스(sequence)에서 더 많은 이점을 얻는 것으로 나타났는데, 표 4는 가속이 길이 128에서 3.36배에서 1024에서 22.58배로 증가함을 보여줍니다. 엔지니어링 관점에서, 필터는 작고(블록 크기 32의 경우 약 2k 매개변수) 훈련이 빠르며(짧은 데이터 수집 패스 후 단일 T4 GPU에서 몇 분 만에 훈련), 추론 시 오버헤드(overhead)는 얻는 이득에 비해 무시할 수 있는 수준입니다. 이 방법은 KV 캐싱(caching)과 직교하며 기존 확산 LLM 디코더(decoder)에 쉽게 통합될 수 있어, 다양한 생성 모델에 적용 가능한 일반적인 가속 기술로서의 잠재력을 보여줍니다. 이는 LLM의 실용적인 배포를 위한 중요한 진전입니다. [논문](Paper) | [트윗](Tweet)

8.  **작은 모델(model)에 맞춘 추론 트레이스(trace): 역 투기적 디코딩(RSD)**
    소형 모델들은 대형 교사 모델이 생성한 방대하고 정교한 사고 연쇄(CoT)를 사용하여 지도 미세 조정(SFT)을 수행할 경우, 종종 기대 이하의 성능을 보입니다. 이는 대형 모델의 '전문성 저주(curse of expertise)'로 인해 작은 모델이 소화하기 어려운 복잡한 추론 과정을 학습하게 되는 현상 때문입니다. 본 연구는 이러한 현상의 원인을 명확히 규명하고, 역투기적 디코딩(Reverse Speculative Decoding, RSD)이라는 방식으로 해법을 제시합니다. 이 기법은 교사 모델이 생성한 단위(토큰)를 학생 모델이 자신의 확률 분포 내에서 타당하다고 판단할 때만 수용하도록 합니다. 그렇지 않을 경우, 학생 모델은 자신의 토큰으로 되돌아가게 됩니다. 이는 작은 모델이 추적하기 어려운 높은 놀라움 스파이크(high-surprisal spike)를 필터링(filter)하여 논리(logic)를 단순화하지 않으면서도 토큰 수준의 난이도(token-level difficulty)를 완화합니다.

    이러한 접근 방식이 중요한 이유는 명확합니다. Qwen3-0.6B 모델을 s1K-1.1 트레이스(trace)로 직접 SFT(Supervised Fine-Tuning)했을 때 평균 정확도(average accuracy)가 20.5% 저하되는 문제가 발생했습니다. 반면, RSD 트레이스(trace)로 훈련했을 때는 AIME24, AIME25, GPQA-Diamond, MATH500 벤치마크 전반에서 평균 4.9%의 성능 향상을 얻었습니다. 이는 작은 모델의 잠재력을 최대한 끌어내기 위한 데이터 레시피(recipe)의 중요성을 강조합니다. 효과적인 데이터 레시피는 토크나이저 호환 교사(tokenizer-compatible teacher)(s1.1-7B)와 학생(student)(Qwen3-0.6B)을 사용하는 것에서 시작합니다. 거부 샘플링(rejection sampling)을 통해 RSD 트레이스를 생성하며, 문제가 해결될 수 없을 때는 UPFT(Unsupervised Pre-training with Fine-Tuning) 스타일 접두사 훈련(prefix training)을 통해 처음 128개 토큰을 복구합니다. 180개의 완전한 솔루션(solution)과 많은 접두사(prefix)에도 불구하고 0.6B 학생 모델이 개선된 것은 분포 정렬(distributional alignment)이 데이터 볼륨(volume)보다 더 중요함을 보여줍니다.

    핵심 진단 결과, 가장 강력한 실패 예측 변수(failure predictor)는 학생 모델이 생성한 하위 1% 토큰의 비율이었습니다. s1K-1.1 트레이스는 이러한 토큰을 많이 포함하여 학습을 저해했지만, RSD는 이를 거의 0으로 줄였습니다. 이는 모델이 실제로 학습 가능한 범위 내에서 데이터를 제공하는 것이 얼마나 중요한지를 보여줍니다. 그러나 RSD는 보편적이지 않으며, 대상 모델에 맞춰 맞춤화되어야 합니다. Qwen3-0.6B를 "승인자"로 사용하여 구축된 트레이스는 Qwen3-1.7B, Llama-3.2-1B, Gemma-3-1B 또는 Phi-4-Mini와 같은 다른 모델로 전이되지 않습니다. 즉, 각 모델의 고유한 능력과 분포를 고려해야 합니다. 대상 모델별로 RSD를 실행하는 것은 도움이 되지만, 동일한 모델에 대한 반복적인 다단계 RSD는 분포 드리프트(distributional drift)를 통해 성능을 저하시킬 수 있다는 한계도 있습니다. 이러한 연구는 대규모 언어 모델의 지식을 소형 모델로 효율적으로 이전하는 방법을 탐구하며, 제한된 자원으로도 고성능 AI를 구축하려는 노력에 중요한 통찰을 제공합니다. [논문](Paper) | [트윗](Tweet)

9.  **TUMIX: 도구 사용 혼합을 통한 추론 능력 극대화**
    TUMIX는 문자 처리, 프로그램 실행, 그리고 웹 탐색 기능을 통합하고, 15가지 이종적인 인공지능 주체들을 동시에 가동하여, 단계별로 중간 결과물을 공유하며 추론을 수행하는 통합 접근 방식입니다. 이는 대규모 언어 모델(LLM)이 내재적으로 가진 한계, 즉 환각(hallucination), 최신 정보 부족, 복잡한 계산 능력 부재 등을 극복하기 위해 외부 도구(tool)를 적극적으로 활용하는 최신 AI 연구 동향을 반영합니다. 여러 에이전트의 병렬 실행과 중간 답변 전달을 통해 복잡한 문제를 분해하고 협력적으로 해결하는 앙상블(ensemble) 학습의 장점을 극대화합니다.

    대규모 언어 모델(LLM) 기반의 평가자는 조기 종료 시점을 조절함으로써, HLE, GPQA-Diamond, AIME 24/25 벤치마크에서 기존의 강력한 도구 활용 모델 대비 최대 3.55%의 정밀도 향상을 가져왔고, 동시에 추론에 소요되는 비용을 대략 절반 수준으로 줄였습니다. 이는 정확도와 효율성이라는 두 마리 토끼를 동시에 잡는 데 성공했음을 의미합니다. LLM이 심사위원 역할을 수행함으로써, 각 에이전트의 기여도를 평가하고 최적의 해결책을 도출하는 과정의 신뢰도를 높입니다. 복잡한 추론 문제에서 도구 사용은 단순한 정보 검색을 넘어, 특정 도메인 지식과 연산 능력을 AI에 부여하여 문제 해결 범위를 확장하는 핵심 요소입니다. TUMIX는 이러한 도구 사용의 잠재력을 앙상블 기법과 결합하여, 더욱 강력하고 유연한 AI 시스템을 구축하는 데 기여합니다. 이 연구는 AI가 더욱 지능적으로 외부 세계와 상호작용하고, 인간이 해결하기 어려운 복잡한 문제에 대한 새로운 해결책을 제시할 수 있는 가능성을 열어줍니다. [논문](Paper) | [트윗](Tweet)

10. **PrompCoT 2.0: 프롬프트 합성의 자동화와 LLM 추론 스케일링**
    PromptCoT 2.0은 이전 버전 1.0에 사용된 수작업 방식의 발견적 기법을 대신하여, 더욱 복잡하고 다채로운 추론용 질의를 생성하기 위한 EM(기대-최대화) 기반의 반복 과정을 도입했습니다. 이는 대규모 언어 모델(LLM)의 잠재력을 최대한 발휘하기 위한 핵심 요소인 프롬프트 엔지니어링(prompt engineering)의 자동화를 향한 중요한 진전입니다. 수동 프롬프트 설계의 한계를 극복하고, AI 스스로 효과적인 학습 자료를 생성하도록 함으로써 LLM의 추론 능력을 체계적으로 향상시킵니다.

    이러한 방식은 자기 대국(self-play)과 지도 미세 조정(SFT) 학습 방식을 모두 지원하며, AIME, HMMT, LiveCodeBench, Codeforces와 같은 추론 능력 평가 지표에서 최신 최고 성능(SOTA)을 경신했습니다. 이는 질의어 생성이 대규모 언어 모델의 추론 역량을 확장하는 새로운 핵심 동인임을 입증합니다. 과거에는 LLM의 성능 향상이 주로 모델 크기나 데이터 양의 증가에 의존했지만, PromptCoT 2.0은 프롬프트 합성이라는 새로운 '스케일링 축(scaling axis)'을 제시하며, 더욱 효율적이고 접근 가능한 방식으로 AI 성능을 끌어올릴 수 있음을 보여줍니다. 즉, AI가 스스로 더 나은 질문을 만들고, 그 질문에 답하는 과정을 통해 더욱 똑똑해지는 순환적인 학습 메커니즘을 구축한 것입니다. 이는 미래 AI 연구에서 AI가 생성한 훈련 데이터와 커리큘럼의 중요성을 강조하며, LLM의 추론 능력을 근본적으로 향상시키는 데 기여할 것입니다. [논문](Paper) | [트윗](Tweet)