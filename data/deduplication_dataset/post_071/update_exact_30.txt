다음은 최신 AI/ML 기술 동향을 다루는 업데이트된 블로그 게시물입니다.

---

1.  **확장 가능한 월드 모델(world model) 내 에이전트(agent) 훈련**

    **빠르고 정확한 마인크래프트(Minecraft) 시뮬레이터(simulator)를 학습하고 완전히 오프라인(offline)으로 제어 가능한 에이전트(agent)를 훈련하는 확장 가능한 상상-RL 레시피(imagination-RL recipe). 이 월드 모델(world model)은 단일 GPU에서 실시간 대화형 롤아웃(rollout)을 지원하며, 원시 픽셀(raw pixel)과 저수준 마우스(mouse) 및 키보드(keyboard)로부터 순수 오프라인(offline) "다이아몬드 획득" 결과를 가능하게 한다. 속도와 안정성을 위해 구축된 핵심 레시피(recipe): 인과 토크나이저(causal tokenizer) + 블록-인과 동역학 트랜스포머(block-causal dynamics transformer). 숏컷 포싱(shortcut forcing)은 램프 손실(ramped loss)을 사용하여 x-공간에서 예측하면서 모델이 큰 디노이징(denoising) 단계(K=4)를 수행하도록 훈련시키며, 이는 누적 오류를 줄이고 낮은 단계 수에서도 품질을 유지한다. 공간 전용 및 시간 전용 어텐션 레이어(attention layer), 4단계마다 한 번씩 시간 레이어(temporal layer), GQA, 길고 짧은 배치(batch) 교차는 KV 캐시(cache)를 작게 유지하고 추론(inference)을 빠르게 한다.**

    **시각적 요소뿐만 아니라 메커니즘(mechanics)까지 처리하는 실시간, 더 긴 컨텍스트(context) 월드 모델(world model): 640×360 해상도에서 9.6초 컨텍스트(context)로 20+ FPS의 대화형 추론(inference)을 제공하며, 이는 이전 마인크래프트(Minecraft) 모델보다 상당히 길다.**

    이 월드 모델은 인과 토크나이저와 블록-인과 동역학 트랜스포머를 통해 복잡한 마인크래프트 환경을 효과적으로 모델링한다. 특히, 숏컷 포싱 기법은 모델이 미래 상태를 예측할 때 발생하는 누적 오류를 최소화하여 장기적인 예측의 정확도를 높인다. 또한, GQA(Grouped Query Attention)와 같은 효율적인 어텐션(attention) 메커니즘은 긴 컨텍스트(context)를 처리하면서도 빠른 추론 속도를 유지하는 데 핵심적인 역할을 한다.

    "오프라인 다이아몬드 챌린지(Offline Diamond Challenge)"에서 에이전트는 환경과의 상호작용 없이 2.5K 시간의 VPT(Video Pre-Training) 계약자 데이터셋(dataset)만으로 훈련되었음에도 불구하고, 60분 에피소드(episode) 내에 철 곡괭이(iron pickaxe)에 29%, 다이아몬드(diamond)에 0.7% 도달하는 놀라운 성과를 보였다. 이는 VPT(파인튜닝(finetuning)됨) 및 Gemma-3 VLA와 같은 강력한 오프라인 기준선(baseline)을 능가하는 동시에, YouTube 사전 훈련(pre-trained)된 VPT 파이프라인(pipeline)보다 약 100배 적은 데이터를 사용했다는 점에서 데이터 효율성의 중요성을 강조한다. 작업 토큰(task token)에 조건화(condition)되고 상상 RL(imagination RL)을 통해 개선된 에이전트는 PMPO(Proximal Policy Optimization with Model-based Planning)를 사용하여 복제된 BC 정책(policy)에 대한 역-KL(reverse-KL)을 최적화함으로써 온라인(online) 데이터(data) 없이도 견고성(robustness)과 샘플 효율성(sample efficiency)을 크게 향상시켰다. 이러한 접근 방식은 시뮬레이션(simulation) 환경에서 학습된 에이전트가 실제 환경에서 복잡한 작업을 수행할 수 있는 가능성을 보여준다. [논문](Paper) | [트윗](Tweet)

2.  **DeepSeek-V3.2-Exp**

    DeepSeek은 V3.1 "Terminus" 백본(backbone)에 미세한 희소 어텐션 메커니즘(sparse attention mechanism)(DeepSeek Sparse Attention, DSA)을 추가하여 긴 컨텍스트(context) 모델의 효율성을 혁신적으로 개선했다.

    **DSA 설계: 작은 FP8 "라이트닝 인덱서(lightning indexer)"가 쿼리(query)당 과거 토큰(token)을 점수화한 다음, top-k 셀렉터(selector)가 메인 어텐션(attention)을 위해 해당 KV 엔트리(entry)만 가져온다. 이는 인덱서(indexer)를 가볍게 유지하면서 코어 어텐션(core attention)을 메인 경로(main path)에서 O(L²)에서 약 O(L·k)로 변경한다. 훈련 레시피(recipe): 128K V3.1 체크포인트(checkpoint)에서 시작한다. 밀집 어텐션(dense attention)으로 웜업(warm-up)하면서 밀집 어텐션 분포(dense attention distribution)에 대한 KL을 통해 인덱서(indexer)만 훈련한다(약 2.1B 토큰(token)). 희소 훈련(sparse training)으로 전환하고 쿼리(query)당 k=2048개의 선택된 KV 토큰(token)으로 모든 가중치(weight)를 최적화한다(≈944B 토큰(token)).**

    기존의 밀집 어텐션은 컨텍스트(context) 길이(L)에 따라 계산 복잡도가 O(L²)으로 증가하여 매우 긴 시퀀스(sequence) 처리 시 비용과 지연 시간(latency) 문제를 야기한다. DSA는 "라이트닝 인덱서"를 통해 가장 관련성이 높은 k개의 KV 엔트리(entry)만을 선택함으로써 이 문제를 해결하며, 이는 코어 어텐션의 복잡도를 O(L·k)로 크게 줄인다. 훈련 과정에서는 먼저 밀집 어텐션 분포에 대한 KL 다이버전스(divergence)를 사용하여 인덱서(indexer)를 웜업(warm-up)하고, 이후 희소 어텐션으로 전환하여 전체 모델 가중치(weight)를 최적화한다.

    DSA의 영향을 분리하기 위해 V3.1과 동일한 파이프라인(pipeline)으로 후속 훈련(post-train)을 진행했다. 이 후속 훈련 스택(stack)은 수학, 경쟁 프로그래밍(competitive programming), 논리적 추론(logical reasoning), 에이전트 코딩(agentic coding), 에이전트 검색(agentic search) 등 5개 도메인(domain)에 대한 전문가 증류(specialist distillation)와 글쓰기 및 QA를 포함한다. 이후 GRPO(Generalized Reinforcement Learning with Policy Optimization)를 사용하여 추론(reasoning), 에이전트 행동(agent behavior) 및 정렬(alignment)의 균형을 맞추는 단일 혼합 RL 단계(stage)를 거친다. RL 설계는 결과 보상(outcome reward), 길이 페널티(length penalty) 및 언어 일관성 보상(language-consistency reward)을 사용하여 모델의 전반적인 성능을 향상시킨다. 결과적으로 DSA는 일반, 코드(code), 검색 에이전트(search-agent) 및 수학 스위트(suite) 전반에서 V3.1과 거의 동등한 품질을 유지하면서도, 긴 컨텍스트에서 프리필링(prefilling)과 디코딩(decoding) 모두에 대해 명확한 종단 간(end-to-end) 토큰 위치 비용 절감을 보여주었다. 이는 긴 컨텍스트 서비스를 훨씬 저렴하게 제공할 수 있음을 의미한다. [기술 보고서](Technical Report) | [트윗](Tweet)

3.  **실제 인간 상호작용의 시대**

    기존의 LLM 훈련은 주로 정적 데이터셋에 의존했지만, 이 연구는 실제 사용자 대화에서 직접 학습하는 혁신적인 후속 훈련 레시피(post-training recipe)인 RLHI(Reinforcement Learning from Human Interaction)를 제시한다. RLHI는 사용자 안내 재작성(user-guided rewrite)과 페르소나 기반 보상(persona-based reward)을 결합하여 모델의 정렬(alignment)과 개인화(personalization) 능력을 극대화한다. 사용자 안내 재작성은 사용자의 후속 조치(follow-up)를 수정으로 활용하여 모델이 사용자 의도를 더 정확하게 파악하도록 돕고, 페르소나 기반 보상은 장기 사용자 기록에서 증류(distill)된 페르소나를 조건으로 하는 보상 모델(reward model)을 통해 후보 응답의 순위를 매긴다.

    WildChat 대화 데이터를 사용하여 훈련된 RLHI는 개인화(personalization)와 지시 따르기(instruction following)에서 강력한 개선을 보였으며, 심지어 추론 작업(reasoning task)으로도 전이되는 효과를 입증했다. 특히, 실제 채팅 데이터는 특히 대화 후반부에서 풍부한 수정 신호(correction signal)를 제공하여 밀도 높은 감독(dense supervision)을 가능하게 한다. WildChat 기반 평가에서 재작성(rewrite)은 개인화 및 선호도를 개선했고, 페르소나 기반 보상은 지시 따르기에서 우위를 점했다. 벤치마크(benchmark) 결과는 매우 인상적이다: AlpacaEval 2.0에서 77.9%의 승률을 기록했으며, Arena-Hard에서도 경쟁력 있는 성능을 보였다. 또한, 수학/과학 데이터셋 전반에서 추론 정확도(reasoning accuracy)가 26.5%에서 31.8%로 상승하는 등 강력한 추론 능력 향상을 보여주었다. 절제 연구(ablation study)를 통해 상호작용 데이터의 중요성, 강력한 품질 필터(quality filter)의 필수성, 그리고 사용자당 깊이보다 사용자 다양성(user diversity)이 더 중요함을 밝혔다. 향후 연구는 온라인 연속 학습(online continual learning), 더 안전한 보상 모델링(reward modeling) 및 개인 정보 보호(privacy-preserving)를 고려한 개인화(personalization)를 목표로 한다. [논문](Paper) | [트윗](Tweet)

4.  **JEPA 재고**

    Apple은 EMA(Exponential Moving Average) 기반의 교사(teacher) 모델 훈련 방식이 가진 복잡성과 계산 효율성 문제를 해결하기 위해 SALT(Static-teacher Asymmetric Latent Training)라는 새로운 비디오 자기 지도 학습(self-supervised learning) 접근 방식을 제안한다. SALT는 V-JEPA(Vision-Joint Embedding Predictive Architecture)의 대안으로, 픽셀 재구성(pixel reconstruction)을 통해 교사를 먼저 훈련하고, 이 고정된 교사를 사용하여 마스크된 영역(masked region)에서 교사의 잠재 변수(latent)를 예측하도록 학생(student)을 훈련하는 간단한 2단계 프로세스를 따른다.

    이 방식은 EMA를 제거하고 교사와 학생을 명확하게 분리함으로써 더 계산 효율적이고 깔끔한 모델 선택(model selection)을 가능하게 한다. 1단계에서는 VideoMAE 스타일의 픽셀 재구성 목표(objective)를 V-JEPA의 다중 블록 마스킹(multi-block masking)(V-Pixel)과 결합하여 비디오 인코더(video encoder)를 훈련한다. 2단계에서는 이 인코더를 고정하고, 학생 인코더와 예측기(predictor)를 훈련하여 마스크된 영역에서 교사의 잠재 변수와 일치시킨다. 두 단계 모두 손실(loss)이 적절하고 안정적이며, 기존 방법에서 발생할 수 있는 붕괴 메커니즘(collapse machinery)을 제거한다. SALT는 V-3.6M 믹스(mix)에서 일치하는 사전 훈련(pretraining) 단계에서 V-JEPA 2보다 평균 Top-1 정확도를 개선하며, 학생 크기에 따라 잘 확장된다. 특히 "약한 교사, 강한 학생"이라는 중요한 통찰은 작거나 최적이 아닌 교사에 의해 훈련된 학생들도 여전히 SOTA(State-Of-The-Art) 수준의 성능을 달성할 수 있음을 보여준다. SALT의 학생 훈련 손실(student training loss)은 다운스트림(downstream) 고정 정확도(frozen accuracy)와 밀접하게 상관 관계를 가지므로, 사전 훈련 중 해석 가능한 모델 선택을 가능하게 하여 EMA JEPA와 달리 유용한 훈련 신호(training signal)를 제공한다. [논문](Paper) | [트윗](Tweet)

5.  **Agent S3**

    Agent S3 논문은 복잡한 디지털 환경에서 에이전트(agent)의 성능과 효율성을 극대화하기 위한 Behavior Best-of-N(bBoN) 방법론을 소개한다. bBoN은 여러 개의 완전한 CUA(Complete User Action) 롤아웃(rollout)을 병렬로 실행한 후, 각 궤적(trajectory)을 간결한 행동 내러티브(behavior narrative)로 변환한다. 이후 LLM 기반의 원샷 MCQ 심사위원(one-shot MCQ judge)을 통해 이러한 내러티브를 비교하고 최상의 궤적을 선택한다. 이 방식은 에이전트가 시행착오를 통해 학습하고 최적의 경로를 찾아내는 데 효과적인 메커니즘을 제공한다.

    더 강력한 기본 에이전트(base agent)인 Agent S3는 통합 코딩 서브 에이전트(coding sub-agent)가 있는 더 평탄한 루프(loop)를 사용하여 Agent S2 대비 성공률을 높이고 LLM 호출(call) 및 실제 시간(wall time)을 줄였다. 결과적으로 Agent S3는 100단계에서 OSWorld 벤치마크에서 새로운 SoTA(State-Of-The-Art)를 달성했으며, Windows 및 Android 환경으로도 성공적으로 일반화되는 강력한 효율성 이득을 보였다. 확장 연구에서는 N(병렬 실행 횟수)이 증가함에 따라 정확도가 상승하고, 모델 다양성(model diversity)이 Pass@N을 개선하며, 단일 라운드 비교 선택(single-round comparative selection)이 더 낮은 비용으로 쌍별 토너먼트(pairwise tournament)와 동등하거나 그 이상의 성능을 보임을 입증했다. 실용적인 측면에서, 이 연구는 동일한 스냅샷(snapshot)에서 병렬 VM(virtual machine)을 시작하고, 검증 가능한 델타(delta)를 방출하도록 단계를 계측하며, N을 4에서 10 사이로 설정하고, 예산이 허락한다면 다양한 강력한 모델(model)을 추가하는 등의 구현 가이드라인을 제시한다. 이러한 접근 방식은 복잡한 에이전트 시스템 개발에 있어 중요한 진전을 의미한다. [논문](Paper) | [트윗](Tweet)

6.  **DeepSearch**

    **DeepSearch는 검증 가능한 보상(reward)을 가진 몬테카를로 트리 탐색(Monte Carlo Tree Search)을 RL에 직접 통합하지만, 추론(inference) 시가 아니라 훈련(training) 중에 통합한다. 그 결과, 강력한 1.5B 기준선(baseline)에 비해 수학적 추론(math reasoning)에서 더 넓은 탐색(exploration), 더 나은 크레딧 할당(credit assignment) 및 더 높은 샘플 효율성(sample efficiency)을 얻는다.**

    DeepSearch의 핵심은 MCTS(Monte Carlo Tree Search)를 RL 훈련 루프(loop)에 내장하여 모델이 학습 과정에서 더 효과적으로 탐색하고 보상 신호를 할당하도록 돕는 것이다. 훈련 시간 탐색(train-time exploration)을 위해 두 가지 셀렉터(selector)가 사용된다: 형제 노드 비교를 위한 로컬 UCT(Upper Confidence Bound 1 applied to trees)와 전체 트리(tree)에서 다음 리프(leaf)를 선택하는 전역 프론티어 스코어러(global frontier scorer). 프론티어 점수는 부모 품질, 정책 엔트로피(policy entropy) 및 깊이 보너스(√(d/dT))를 결합하여 탐색의 균형을 맞춘다.

    **승리 경로와 '확신 있는 오답' 경로를 모두 감독한다: 올바른 터미널(terminal)이 발견되지 않으면, DeepSearch는 감독을 위해 경로를 따라 가장 낮은 평균 엔트로피(entropy)를 가진 부정적인 궤적(trajectory)을 선택한다. 이는 제약된 업데이트(constrained update)로 노드(node) 값을 백업(back up)하여 올바른 경로의 노드(node)가 음수가 아닌 상태로 유지되도록 한다. 이는 결과 보상(outcome reward)만 제공하는 대신 미세한 단계 수준 이점(step-level advantage)을 제공한다.**

    트리-GRPO 목표(objective)와 q-값 소프트 클리핑(soft clipping)은 그라디언트(gradient) 폭발을 피하고 안정적인 학습을 가능하게 한다. 특히, 이점은 평균 전용 정규화(mean-only normalization)를 가진 노드 수준 q(s)와 PPO 스타일 비율(ratio)의 상위 클리핑(clip-higher), 그리고 중간 q의 tanh 소프트 클리핑을 사용하여 그라디언트를 부드럽게 유지한다. 또한, DeepSearch는 적응형 효율성(adaptive efficiency) 전략을 통해 어려운 항목을 필터링(filter)하고 솔루션(solution)을 캐시(cache)하여 계산(compute)을 절약한다. Pass1@K 임계값(threshold)으로 '어려운 하위 집합(hard subset)'을 반복적으로 필터링하고, 검증된 솔루션의 리플레이 버퍼(replay buffer)를 유지하며, 캐시된 올바른 궤적(trajectory)이 존재할 때 전체 탐색(search)을 건너뛴다. 이러한 전략은 지식(knowledge)을 보존하고 계산을 효율적으로 활용한다. 결과적으로 DeepSearch-1.5B는 AIME24/25, AMC23, MATH500 등 다양한 수학 벤치마크에서 기존 SOTA를 능가하는 정확도를 달성하며, 훨씬 적은 계산량으로 더 나은 성능을 보여주었다. [논문](Paper) | [트윗](Tweet)

7.  **확산 LLM(Diffusion LLM) 가속화**

    확산 기반 LLM(dLLM)은 토큰 업데이트(token update)를 병렬화(parallelize)하여 기존 자동 회귀(autoregressive) 모델보다 빠른 생성을 목표로 하지만, 효율성 측면에서 여전히 개선의 여지가 있었다.

    **문제 및 통찰: 반자동 회귀 확산 LLM(semi-autoregressive diffusion LLM)은 토큰 업데이트(token update)를 병렬화(parallelize)하지만, 정적 휴리스틱(static heuristic)은 이미 올바른 토큰(token)을 계속해서 다시 마스킹(remasking)한다. 이 논문은 올바른 예측 즉시 토큰(token)을 언마스킹(unmasking)하는 오라클 전략(oracle strategy)인 Extremely Greedy Parallel을 정의하고 속도 향상을 위한 큰 여유 공간을 보여준다.**

    **방법: Learn2PD 필터(filter): 토큰 신뢰도 패턴(token confidence pattern)에 대해 2계층 MLP 필터(filter) fθ를 훈련하여 위치당 '최종화 또는 재마스킹(finalize or remask)'을 예측한다. 필터(filter)만 BCE 손실(loss)로 훈련된다; dLLM은 고정된 상태로 유지된다. 추론(inference)은 필터(filter)의 로짓(logit)에 임계값(threshold) τ를 적용하여 토큰(token)을 확정한다.**

    이 연구는 Learn2PD(Learn to Predict Diffusion) 필터와 EoTP(End-of-Text Prediction)를 통해 dLLM 디코딩(decoding)을 가속화한다. Learn2PD 필터는 어떤 토큰(token)이 이미 '최종(final)' 상태에 도달했는지 예측하여 불필요한 재마스킹(remasking)을 방지한다. EoTP는 [EoT](End-of-Text) 토큰이 안정적으로 생성되자마자 디코딩(decoding)을 조기에 중단시켜, 특히 매우 긴 시퀀스(sequence)에서 발생하는 긴 꼬리(long tail) 계산을 제거한다. 부록 B에 따르면, 길이 1024에서 추가 계산량의 약 89.59%가 EoT 후 패딩(padding)에서 발생한다.

    LLaDA-8B-Instruct 모델에 적용했을 때, Learn2PD만으로 GSM8K, MATH, HumanEval 및 MBPP에서 길이에 따라 3~12배의 속도 향상을 얻었다. Learn2PD+EoTP 조합은 GSM8K에서 길이 1024에서 22.58배에 달하는 놀라운 가속을 달성했으며, 정확도는 유지되거나 약간 향상되었다. KV 캐시(cache)와 결합하면 처리량(throughput)을 57.51배까지 더욱 향상시킬 수 있다. 흥미롭게도, 더 긴 시퀀스(sequence)일수록 더 큰 이점을 얻으며, 이는 길이가 128에서 3.36배였던 가속이 1024에서는 22.58배로 증가하는 표 4의 결과에서 확인할 수 있다. Learn2PD 필터는 매우 작고(블록 크기 32의 경우 약 2k 매개변수(parameter)), 단일 T4 GPU에서 몇 분 만에 훈련될 수 있어 엔지니어링 오버헤드(overhead)가 무시할 만하다. 이 방법은 KV 캐싱(caching)과 직교하며 기존 dLLM 디코더(decoder)에 쉽게 통합될 수 있어, 확산 LLM의 실용적인 배포를 위한 중요한 진전을 의미한다. [논문](Paper) | [트윗](Tweet)

8.  **작은 모델(model)에 맞춰진 추론 트레이스**

    큰 교사(teacher) 모델의 길고 고품질 CoT(Chain-of-Thought) 트레이스(trace)로 작은 학생(student) 모델을 SFT(Supervised Fine-Tuning)할 때, 종종 성능이 저하되는 문제가 발생한다. 이 연구는 그 원인을 분석하고 역 투기적 디코딩(Reverse Speculative Decoding, RSD)이라는 혁신적인 방법으로 이를 해결한다.

    **핵심 아이디어: 각 단계에서 교사 토큰(teacher token)을 샘플링(sample)하고, 학생(student)이 ≥ p_th 확률을 할당하는 경우에만 유지하며, 그렇지 않으면 학생 자신의 토큰(token)으로 되돌아간다. 이는 작은 모델(model)이 추적할 수 없는 높은 놀라움 스파이크(high-surprisal spike)를 필터링(filter)하여 논리(logic)를 단순화하지 않고 토큰 수준 난이도(token-level difficulty)를 완화한다.**

    **왜 중요한가: s1K-1.1 트레이스(trace)에 대한 Qwen3-0.6B의 직접 SFT(Supervised Fine-Tuning)는 평균 정확도(average accuracy)를 20.5% 저하시킨다. 대신 RSD 트레이스(trace)로 훈련하면 AIME24, AIME25, GPQA-Diamond, MATH500 전반에서 평균 4.9%의 이득을 얻는다.**

    RSD는 학생 모델의 분포와 일치하는 트레이스를 생성하여, 작은 모델이 실제로 효과적으로 학습할 수 있도록 돕는다. 효과적인 데이터 레시피(recipe)는 토크나이저 호환 교사(teacher)(s1.1-7B)와 학생(student)(Qwen3-0.6B)을 사용하며, 거부 샘플링(rejection sampling)을 통해 RSD 트레이스를 생성한다. 문제가 해결될 수 없을 때는 UPFT 스타일 접두사 훈련(prefix training)을 통해 처음 128개 토큰(token)을 복구한다. 비록 180개의 완전한 솔루션(solution)과 많은 접두사(prefix)만 사용했지만, 0.6B 학생 모델은 개선되었고, 이는 분포 정렬(distributional alignment)이 데이터 볼륨(volume)보다 중요함을 시사한다.

    주요 진단 결과, 가장 강력한 실패 예측 변수(failure predictor)는 학생 모델 하위 1% 토큰의 비율임이 밝혀졌다. s1K-1.1 트레이스에는 이러한 '놀라움 스파이크(high-surprisal spike)' 토큰이 많이 포함되어 학습을 저하시키지만, RSD는 이를 거의 0으로 줄인다. 하지만 RSD 트레이스는 모델별로 맞춤화되어야 한다. Qwen3-0.6B를 "승인자(approver)"로 사용하여 구축된 트레이스는 Qwen3-1.7B, Llama-3.2-1B, Gemma-3-1B 또는 Phi-4-Mini와 같은 다른 모델로 전이되지 않는다. 또한, 동일한 모델에 대한 반복적인 다단계 RSD는 분포 드리프트(distributional drift)를 통해 성능을 저하시킬 수 있으므로, RSD 적용 시 신중한 접근이 필요하다. [논문](Paper) | [트윗](Tweet)

9.  **도구 사용 혼합(Tool-Use Mixture, TUMIX)**

    TUMIX는 LLM(Large Language Model)의 추론(reasoning) 능력을 강화하기 위해 텍스트(text), 코드 실행(code execution), 웹 검색(web search) 등 다양한 도구를 혼합하고 활용하는 혁신적인 앙상블(ensemble) 레시피이다. 이 시스템은 15개의 다양한 에이전트(agent)를 병렬로 실행하며, 각 에이전트는 라운드(round)를 거쳐 중간 답변을 서로 전달하며 협력한다. 이러한 상호작용적 접근 방식은 복잡한 문제를 다각도로 분석하고 해결책을 찾아내는 데 유리하다.

    TUMIX의 핵심 구성 요소 중 하나는 LLM-심사위원(judge)이다. 이 심사위원은 에이전트의 진행 상황을 평가하고 조기 중단(early stopping)을 제어하여 불필요한 계산을 줄인다. 이러한 지능적인 제어 덕분에 TUMIX는 HLE, GPQA-Diamond 및 AIME 24/25와 같은 까다로운 벤치마크에서 강력한 도구 증강 기준선(tool-augmented baseline) 대비 최대 3.55%의 정확도 이득(accuracy gain)을 제공한다. 동시에 추론(inference) 비용을 약 50% 절감하는 놀라운 효율성을 보여준다. TUMIX는 도구 사용을 통해 LLM의 한계를 보완하고, 멀티모달(multimodal) 및 에이전트 기반 시스템에서 복잡한 문제 해결 능력을 향상시키는 중요한 방향을 제시한다. 이는 미래의 AI 에이전트가 다양한 정보원과 기능을 통합하여 더욱 강력하고 효율적인 성능을 발휘할 수 있음을 시사한다. [논문](Paper) | [트윗](Tweet)

10. **PrompCoT 2.0**

    PrompCoT 2.0은 LLM의 추론(reasoning) 능력을 향상시키기 위한 프롬프트 합성(prompt synthesis)의 새로운 시대를 연다. 기존 PrompCoT 1.0이 수동 휴리스틱(manual heuristic)에 의존하여 추론 프롬프트(reasoning prompt)를 생성했던 한계를 넘어, 2.0 버전은 EM(Expectation-Maximization) 기반 루프(loop)를 도입하여 더 어렵고 다양한 추론 프롬프트를 자동으로 합성한다. 이 자동화된 접근 방식은 프롬프트 엔지니어링(prompt engineering)의 수고를 줄이고, 모델 훈련에 필요한 고품질 프롬프트 데이터를 대량으로 생성할 수 있게 한다.

    EM 기반 루프는 셀프 플레이(self-play) 및 SFT(Supervised Fine-Tuning) 훈련 체제(training regime)를 모두 가능하게 한다. 셀프 플레이는 모델이 스스로 문제를 해결하고, 그 해결 과정을 프롬프트로 활용하여 다시 자신을 훈련시키는 방식으로, 지속적인 성능 향상을 유도한다. SFT 훈련은 합성된 프롬프트 데이터를 사용하여 모델의 특정 추론 능력을 미세 조정한다. 이러한 강력한 훈련 패러다임 덕분에 PrompCoT 2.0은 AIME, HMMT, LiveCodeBench 및 Codeforces와 같은 다양한 추론 벤치마크(benchmark)에서 새로운 SOTA(State-Of-The-Art)를 달성했다. 이는 프롬프트 합성(prompt synthesis)이 LLM 추론(reasoning) 능력 향상을 위한 새로운 스케일링 축(scaling axis)이 될 수 있음을 명확히 보여준다. 이 기술은 LLM이 복잡한 문제를 해결하고 새로운 지식을 추론하는 데 필요한 능력을 더욱 발전시키는 데 중요한 기여를 할 것으로 기대된다. [논문](Paper) | [트윗](Tweet)

---