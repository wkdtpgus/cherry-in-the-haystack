환영합니다, 독자 여러분! 이번 대규모 언어 모델(LLM) 동향 분석에서는 에이전트(agent) 역량 확장의 새로운 지평, 컨텍스트 내 학습(in-context learning)의 본질에 대한 심층적 탐구, 그리고 4차원 시공간 모델링(4D world modeling)의 진보를 다룹니다. 오늘 소개해 드릴 연구들은 인공지능(AI) 시스템이 복잡한 작업을 수행하고 현실 세계를 이해하며 효율적으로 학습하는 방식에 대한 우리의 인식을 변화시키고 있습니다. 각 논문의 목표, 접근 방식, 그리고 핵심적인 발견들을 간략하게 요약한 후, 이 모든 혁신이 AI의 미래에 어떤 의미를 갖는지 함께 조명해 보겠습니다. 최신 정보를 놓치지 않으시려면 구독 버튼을 눌러주세요.

저희 LLM Watch 구독자분들을 제6회 MLOps World | GenAI 글로벌 서밋에 초대합니다. 이 행사는 텍사스 오스틴에서 개최되며, OpenAI, HuggingFace 등 업계 선두 주자들이 참여하는 60개 이상의 세션을 포함합니다. 구독자 여러분은 여기를 통해 온라인으로 무료 참석이 가능합니다. 또한, 오스틴 현지에서 진행되는 실용 워크숍, 실제 적용 사례 발표, 풍성한 음식과 음료, 그리고 네트워킹 파티에 직접 참여하고 싶으시다면, 이 코드를 사용하여 등록 비용에서 150달러를 할인받으실 수 있습니다!

**150달러 특별 할인**

**초심자를 위한 핵심 용어 해설**

*   **환경 스케일링(Environment scaling)**: 인공지능 에이전트(agent)가 특정 가상 환경(sandbox)에만 지나치게 최적화되는 것을 방지하기 위해, 다양한 시뮬레이션된 임무와 도구 인터페이스(tool API)를 활용하여 훈련하는 기법입니다.
*   **함수 호출(Function calling) / 도구 활용(tool use)**: 대규모 언어 모델(LLM)이 추론 과정 중에 계산기, 데이터 저장소(database), 웹 브라우저(browser), 또는 응용 프로그래밍 인터페이스(API)와 같은 외부 기능을 작동시키도록 지시하는 행위를 의미합니다.
*   **지속적 에이전트 사전 학습(Agentic continual pre-training, CPT)**: 미세 조정(fine-tuning) 단계에 앞서, 합성된 에이전트의 경험(예: 행동 계획, 도구 사용 기록)을 바탕으로 추가적인 사전 훈련을 수행하는 과정입니다. 이는 특정 전문 과정에 들어가기 전의 '기초 훈련 캠프'와 유사하다고 볼 수 있습니다.
*   **4차원 세계 모델링(4D world modeling)**: 시각 및 기타 감각 데이터(multi-modal data)로부터 시간적 변화를 포함하는 3차원 공간(공간 + 시간)의 동적 특성을 학습하는 기술입니다.
*   **다단계 강화 학습(Multi-turn RL)**: 단 한 번의 지시(prompt)가 아니라, 도구 사용의 전체 상호작용 과정(tool-use session)에 걸쳐 적용되는 강화 학습(reinforcement learning) 방식입니다. 이는 단순히 좋은 최종 결과뿐만 아니라, 목표에 도달하는 과정(trajectory) 자체에 보상을 부여하여 학습을 유도합니다.
*   **컨텍스트 내 학습(In-context learning, ICL)**: 모델의 내부 매개변수(weight)를 직접 수정하지 않고도, 질의(prompt) 내에 제공된 몇몇 예시들을 통해 모델이 새로운 유형의 작업에 적응하도록 만드는 기법입니다. 마치 시험 직전에 포스트잇에 적힌 요약본을 보고 벼락치기하는 것과 같습니다.
*   **제로샷 평가(Zero-shot evaluation)**: 특정 작업에 대한 추가적인 미세 조정(fine-tuning) 없이 모델의 성능을 측정하는 방법입니다. 이는 마치 아무런 준비 없이 시험장에 들어가서 자신의 기본적인 실력이 얼마나 통하는지 확인하는 상황과 비슷합니다.

---

**환경 다양성 확장을 통한 범용 에이전트 지능 구현 ([논문](https://arxiv.org/abs/2402.04612) / [코드](https://github.com/AgentScaler/AgentScaler))**

이 연구는 다양한 시나리오에서 대규모 언어 모델(LLM) 기반 에이전트(agent)가 함수를 안정적으로 호출(예: 외부 도구/API 활용)하도록 훈련시키는 난제를 다루고 있습니다. 연구팀은 에이전트의 경험 영역을 넓히기 위해 방대한 시뮬레이션 환경을 자동으로 생성하는 프레임워크(framework)인 **AgentScaler**를 제안합니다. 이 시스템은 두 단계의 훈련 방식을 채택합니다. 첫째, 기본적인 도구 사용 능력을 가르치고, 둘째, 특정 도메인(domain) 시나리오에 맞춰 미세 조정(fine-tuning)합니다. 이 접근 방식의 핵심적인 특징과 성과는 다음과 같습니다.

*   **다양하고 확장 가능한 환경 구축**: 이 시스템은 에이전트가 여러 함수 호출 시나리오를 경험하도록 돕기 위해 1,000개 이상의 완전 시뮬레이션된 도구(데이터베이스(database) 작업으로 모형화됨)를 포함하는 고유한 도메인(domain)을 생성합니다. 이러한 도구들은 검증 가능한 결과(verifiable outcomes)를 산출하는 코드로 구현되어, 에이전트가 현실과 유사한 상호작용을 학습할 수 있도록 보장합니다. 이처럼 체계적인 환경 확장은 도구 활용을 위한 다양한 훈련 데이터가 부족했던 기존의 문제를 해결합니다. 이는 에이전트가 실제 환경에서 마주할 수 있는 복잡성과 다양성에 대비할 수 있도록 돕는 중요한 진전입니다.
*   **두 단계의 에이전트 미세 조정 과정**: 에이전트는 먼저 광범위하게 필터링된 경험 데이터셋(dataset)을 생성하는 시뮬레이션된 인간-에이전트 상호작용을 통해 일반적인 도구 사용 기술을 훈련받습니다. 그 후, 특정 맥락(context)에서의 전문성을 위해 수직적 도메인(vertical domain)에서 추가로 전문화됩니다. 이러한 '범용 능력 습득 후 전문화' 접근 방식은 에이전트의 역량을 더욱 매끄럽게 발전시키는 데 기여했습니다.
*   **최고 수준의 성과 달성**: 가장 큰 모델인 **AgentScaler-30B**는 τ-Bench, τ2-Bench, ACEBench와 같은 주요 에이전트 성능 평가 지표(benchmark)에서 최고 수준의 결과를 기록했습니다. 특히 놀라운 점은, AgentScaler-30B의 함수 호출 능력이 훨씬 적은 매개변수(parameter)를 사용했음에도 불구하고, 독점적인 1조 매개변수 에이전트와 대등한 수준을 보였다는 것입니다. 이는 환경 다양성 확장과 목표 지향적 훈련이 에이전트의 도구 사용 능력을 극적으로 끌어올릴 수 있음을 명확히 보여줍니다.

---

**지속적인 사전 학습을 통한 에이전트 역량 강화 ([논문](https://arxiv.org/abs/2402.05929) / [코드](https://github.com/GAIR-NLP/AgentFounder))**

현재 공개 소스(open-source) 인공지능 에이전트들은 특정 한계에 직면해 있습니다. 이들은 다단계 도구 사용을 위해 명시적으로 훈련되지 않은 일반적인 대규모 언어 모델(LLM)에 의존하며, 이는 에이전트 작업에 대한 미세 조정(fine-tuning) 과정에서 최적화 충돌을 야기합니다. 이러한 문제를 해결하기 위해, 본 연구의 저자들은 지도 학습(supervised learning) 또는 강화 학습(RL) 기반의 미세 조정 이전에, 강력한 에이전트 중심의 기반 모델(foundation model)을 구축하기 위한 추가 단계로 **지속적 에이전트 사전 학습(Agentic Continual Pre-Training, CPT)** 개념을 도입했습니다. 그들은 합성된 에이전트 경험으로 사전 학습된 300억 매개변수(parameter) LLM인 **AgentFounder**를 개발했으며, 인상적인 결과를 보고했습니다.

*   **에이전트 CPT 파이프라인(Agentic CPT Pipeline)의 설계**: 대규모 합성 에이전트 데이터(도구 사용 기록, 지식, 질문-답변(Q&A) 쌍으로 구성된 데이터 플라이휠(data flywheel)을 통해 생성됨)를 활용하여 지속적으로 사전 학습함으로써, 모델은 미세 조정에 앞서 추론(reasoning) 및 도구 상호작용을 위한 광범위한 역량을 습득합니다. 이러한 방식은 기존에 모델이 도구 사용을 학습하는 동시에 시연에 맞춰야 했던 '줄다리기' 현상을 효과적으로 완화시켜 줍니다.
*   **AgentFounder 모델(AgentFounder Model)의 탄생**: 그 결과로 탄생한 **AgentFounder-30B**는 강력한 에이전트 기반 모델로서, 뛰어난 도구 사용 기술을 유지합니다. 이 모델은 가벼운 작업별 튜닝(tuning)만으로도 10가지 다양한 에이전트 벤치마크에서 최고 수준의 성능을 달성합니다. 예를 들어, BrowseComp-English 웹 검색 벤치마크에서 39.9%, BrowseComp-Chinese에서 43.3%, 그리고 도전적인 '인류의 마지막 시험(Humanity’s Last Exam, HLE)' 추론 테스트에서 31.5%의 Pass@1 점수를 기록했습니다.
*   **연구의 중요성 및 파급 효과**: 이 연구는 에이전트의 행동에 초점을 맞춘 사전 학습 단계를 추가하는 것이 상당한 성능 향상을 가져온다는 것을 실증적으로 보여줍니다. 공개 소스 에이전트 모델이 이제 복잡한 도구 활용 작업에서 독점적인 에이전트들과 경쟁하거나 심지어 능가할 수 있게 되었으며, 이는 자율적인 AI 에이전트를 위한 전문화된 지속 사전 학습의 핵심적인 가치를 강조합니다. 이러한 진보는 AI 기술의 민주화와 접근성 향상에 크게 기여할 잠재력을 가지고 있습니다.

---

**OmniWorld: 4D 시공간 모델링을 위한 다중 도메인 및 다중 모달 데이터셋 ([논문](https://arxiv.org/abs/2402.05929) / [코드](https://omniworld-dataset.github.io/))**

이 연구는 4차원 세계 모델링(4D world modeling) 분야에서 데이터 부족 문제를 해결하는 데 중점을 둡니다. 4차원 세계 모델링은 3차원 공간 정보와 시간적 역학을 동시에 포착하는 표현을 학습하는 것을 목표로 합니다(시간에 따른 장면 재구성이나 미래 비디오 프레임 예측과 같은 작업들을 상상해 보세요). 연구진은 4차원 세계 모델의 훈련 및 평가를 지원하기 위해 여러 도메인(domain)과 모달리티(modality)를 아우르는 대규모 데이터셋인 **OmniWorld**를 소개합니다. 주요 특징 및 발견은 다음과 같습니다.

*   **포괄적인 4차원 데이터셋의 구성**: OmniWorld는 새롭게 생성된 OmniWorld-Game 시뮬레이션 데이터셋과 여러 선별된 공개 데이터셋으로 구성되어 있으며, 이들은 실내 장면, 실외 환경 등 다양한 도메인을 포괄합니다. 특히 OmniWorld-Game은 풍부한 모달리티(예: RGB 비디오, 깊이 정보, 이벤트)와 현실적이고 동적인 상호작용을 특징으로 하며, 규모와 복잡성 면에서 기존의 합성 데이터셋들을 뛰어넘습니다. 이는 모델이 실제 세계의 복잡한 물리적 상호작용을 학습하는 데 필요한 다양하고 심층적인 데이터를 제공합니다.
*   **4차원 모델링 벤치마킹의 새로운 기준**: OmniWorld를 활용하여 저자들은 4차원 기하학적 재구성(4D geometric reconstruction), 미래 예측(future prediction), 카메라 제어 비디오 생성(camera-control video generation)과 같은 작업들에 대한 도전적인 벤치마크를 정의했습니다. 이 벤치마크 테스트 결과, 많은 현재 최고 성능 모델들이 데이터셋의 복잡하고 동적인 시나리오에 어려움을 겪는 것으로 나타났습니다. 이는 OmniWorld의 다중 도메인, 시간 진화 콘텐츠에 직면했을 때 모델의 일반화(generalization) 능력에 상당한 격차가 있음을 명확히 보여줍니다.
*   **OmniWorld를 통한 성능 향상 입증**: OmniWorld 데이터셋으로 기존의 최고 성능(SOTA) 모델들을 미세 조정(fine-tuning)한 결과, 4차원 작업(더 나은 재구성 정확도, 더 일관된 비디오 생성)에서 현저한 진보를 보였습니다. 이는 OmniWorld가 모델 훈련을 위한 귀중한 자원임을 입증하는 것입니다. 다양하고 풍부한 데이터에 노출되는 것은 모델이 물리 세계의 역학을 더 효과적으로 이해하는 데 크게 기여합니다. 저자들은 OmniWorld가 우리의 복잡하고 동적인 물리 세계를 이해하는 범용 4차원 세계 모델 개발을 가속화할 것이라고 기대합니다.

---

**QuantAgent: 초고속 거래를 위한 가격 기반 다중 에이전트 LLM ([논문](https://arxiv.org/abs/2402.07842) / [코드](https://github.com/QuantAgent/QuantAgent))**

**QuantAgent**는 대규모 언어 모델(LLM) 기반 에이전트를 초고속 거래(HFT: High-Frequency Trading) 분야에 적용한 선구적인 사례입니다. 장기적인 펀더멘털(fundamental) 분석이나 뉴스 기반 전략을 사용하는 일반적인 금융 LLM 설정과 달리, HFT는 기술적 가격 신호(technical price signal)에 기반한 순간적이고 단기적인 의사결정을 요구합니다. QuantAgent는 시장 데이터의 각기 다른 측면에 집중하는 여러 전문 LLM 에이전트들을 배치하여, 협력적으로 거래 결정을 내림으로써 이러한 특수한 요구사항을 충족합니다. 핵심 내용은 다음과 같습니다.

*   **네 가지 전문화된 에이전트 구성**: 이 시스템은 거래 작업을 **지표(Indicator)**, **패턴(Pattern)**, **추세(Trend)**, **위험(Risk)**이라는 네 가지 전문 LLM 에이전트로 세분화합니다. 각 에이전트는 도메인(domain)별 도구(예: 기술 지표 분석, 차트 패턴 인식, 단기 추세 감지, 위험 관리)를 갖추고 있으며, 각자의 전문 분야에 적합한 구조화된 추론(reasoning) 방식을 사용합니다. 이러한 모듈식(modular) 설계는 인간 거래 회사들이 다른 전략을 위해 서로 다른 부서나 알고리즘(algorithm)을 운용하는 방식과 유사합니다.
*   **실시간, 단기적 의사결정에 집중**: 긴 텍스트 분석보다는 구조화된 단기 신호(가격 패턴, 지표 임계값 등)에 집중함으로써, QuantAgent는 4시간 이내 또는 그보다 더 짧은 거래 시간 단위로 작동할 수 있습니다. 이러한 가격 기반 접근 방식은 HFT의 정밀도와 속도 요구 사항에 완벽하게 부합하며, 일중 빈도(sub-day frequencies)에 최적화되지 않았던 이전 LLM 거래 에이전트들과는 차별화됩니다. 이는 LLM이 극도로 시간에 민감한 금융 환경에서도 실용적인 가치를 제공할 수 있음을 보여줍니다.
*   **제로샷(Zero-Shot) 거래에서의 탁월한 성과**: 비트코인(Bitcoin)과 나스닥(Nasdaq) 선물(futures)을 포함한 10가지 상이한 금융 상품에 대한 제로샷(zero-shot) 평가에서, QuantAgent는 예측 정확도와 누적 수익률 측면에서 강력한 기준선(신경망(neural) 및 규칙 기반(rule-based) 모두)을 압도했습니다. 이 모델의 다중 에이전트 아키텍처(multi-agent architecture)는 단일 LLM 또는 무작위 전략보다 더 나은 거래 결정을 내렸으며, 구조화된 금융 사전 지식(financial priors)과 언어 모델 추론(language-model reasoning)을 결합하는 이점을 명확히 보여주었습니다. 이러한 결과는 LLM 에이전트가 적절하게 구조화될 경우, 고속 금융 시장에서 추적 가능하고 실시간 의사결정을 제공할 수 있음을 강력하게 시사합니다.

---

**컨텍스트 내 학습(In-Context Learning)은 진정한 학습인가? ([논문](https://arxiv.abs/2402.07842) / [코드](https://github.com/locuslab/Is-In-Context-Learning-Learning))**

이 연구는 흥미로운 질문을 던집니다. 대규모 언어 모델(LLM)이 컨텍스트 내 학습(ICL)을 수행할 때, 즉 모델의 매개변수(weight)를 업데이트하지 않고도 프롬프트(prompt)에 포함된 몇 가지 예시를 통해 새로운 작업에 적응할 때, 이는 진정으로 학습하는 행위일까요, 아니면 단순히 기존의 사전 지식을 활용하는 것에 불과할까요? 이 논문은 이론적 논의와 광범위한 실증 분석(거의 190만 회의 시도)을 결합하여 ICL의 본질적인 특성을 규명하고자 합니다. 주요 통찰은 다음과 같습니다.

*   **암묵적 학습(Implicit Learning)으로서의 ICL 이해**: 저자들은 수학적으로 ICL이 학습의 한 형태로 간주될 수 있다고 주장합니다. 이는 모델의 출력이 컨텍스트(context) 내에 레이블(label)이 지정된 예시에 반응하여 변화하기 때문입니다(이는 프롬프트(prompt)를 통해 암묵적으로 함수를 업데이트하는 것과 유사합니다). 그러나 일반적인 학습과는 다르게, 모델은 새로운 정보를 영구적으로 인코딩(encode)하지 않습니다. 대신, 컨텍스트로부터 일시적으로 '배우며', 이 과정에서 사전 훈련된 기존 지식(pretrained priors)에 크게 의존합니다.
*   **실증적 한계의 발견**: 대규모 실험(기억 효과 제거, 프롬프트 분포 및 표현 제어)은 ICL이 효과적인 방식임에도 불구하고, 모델이 전혀 보지 못했던 작업에 대한 일반화(generalization) 능력에는 상당한 제약이 있음을 보여줍니다. 모델은 종종 깊이 있는 새로운 기술을 습득하기보다는, 프롬프트 내의 통계적 규칙성을 포착하는 경향이 있습니다. 예를 들어, 많은 예시가 주어졌을 때 모델의 정확도는 프롬프트 형식이나 사용된 모델 자체에 둔감해지며, 대신 예시 분포의 패턴에 크게 좌우됩니다. 이는 취약성으로 이어질 수 있습니다. 가령, 사고의 사슬(chain-of-thought) 스타일 프롬프트는 일반화를 저해하는 분포적 특이점(distributional quirks)을 유발할 수 있습니다.
*   **결론 – 인간의 '학습'과는 다른 메커니즘**: 위 내용을 종합하여, 이 논문은 ICL의 작동 방식(임시적인 컨텍스트 인코딩(ad-hoc context encoding)을 통한 다음 토큰(token) 예측)이 견고하고 보편적인 학습 방법이 아니라고 결론 내립니다. 형식적으로 유사해 보이는 두 가지 작업에 대해 매우 다른 ICL 성능을 보일 수 있다는 점은, 모델이 추상적인 규칙을 진정으로 학습하지 못했음을 시사합니다. 요컨대, ICL은 특정 한계 내에서 기능하지만, 진정한 학습(모델의 매개변수(parameter)가 업데이트되어 새로운 개념을 포착하는 것)을 대체할 수는 없습니다. 이러한 미묘한 발견은 프롬프트만으로 모든 새로운 문제를 해결할 수 있다는 지나친 기대치를 조절하는 데 중요한 시사점을 제공합니다.

---

**DeepDive: 지식 그래프와 다단계 강화 학습을 통한 심층 검색 에이전트 발전 ([논문](https://arxiv.abs/2402.07842) / [코드](https://github.com/DeepDive-Agent/DeepDive))**

이 연구는 복잡한 질문에 답하기 위해 웹이나 지식 기반(knowledge base)을 탐색하는 대규모 언어 모델(LLM) 기반 에이전트, 즉 '심층 검색(deep search)' 인공지능 에이전트의 성능 개선에 집중합니다. 기존의 공개 소스 에이전트들은 장기적인 추론(다단계 검색 세션 처리)에 어려움을 겪으며, 진정으로 어려운 질문에 대한 훈련 데이터가 부족한 경우가 많습니다. **DeepDive** 프레임워크(framework)는 합성 데이터 생성(synthetic data generation)과 강화 학습(reinforcement learning)을 통해 이 두 가지 문제를 해결하며, 개방형 도메인(open-domain) 웹 검색 작업에서 새로운 기록을 수립하는 에이전트를 탄생시켰습니다.

*   **합성 복합 질의 생성 기법**: DeepDive는 개방형 지식 그래프(open knowledge graphs)를 활용하여 어렵고 다단계(multi-hop) 질문을 자동으로 생성합니다. 지식 그래프의 연결을 탐색함으로써, 에이전트가 다단계 추론을 수행하고 '찾기 어려운' 정보를 찾아야 하는 질문들을 만들어냅니다. 이는 기존 지도 학습(supervised learning) 데이터가 제공하는 것 이상의 도전적인 검색 작업을 위한 대규모 훈련 코퍼스(corpus)를 제공합니다.
*   **다단계 강화 학습의 적용**: 모방 학습(imitation learning)에만 의존하기보다는, DeepDive는 다단계 검색 세션(multi-turn search session) 전반에 걸쳐 종단 간(end-to-end) 강화 학습(RL)을 적용합니다. 320억 매개변수(parameter) 에이전트는 강화 피드백(reinforcement feedback)을 통해 검색을 계획하고, 도구(웹 브라우저 등)를 활용하며, 여러 턴에 걸쳐 정보를 수집하도록 훈련됩니다. 이는 에이전트가 효과적으로 추론 범위(reasoning horizon)를 확장하는 방법을 스스로 학습하는 것입니다. 이러한 RL 미세 조정(fine-tuning)은 복잡한 정보 탐색 대화(information-seeking dialogues)를 처리하는 에이전트의 능력을 현저히 향상시켰습니다.
*   **최고 수준의 성과 달성**: 훈련된 **DeepDive-32B** 에이전트는 BrowseComp 벤치마크(복잡한 웹 질의 모음)에서 새로운 공개 소스 최고 기록을 수립했습니다. 이 모델은 WebSailor 및 DeepSeek과 같은 기존의 공개 에이전트들을 이러한 작업에서 능가합니다. 특히, 제거 연구(ablation study)는 다단계 RL 훈련이 성능 향상에 크게 기여했음을 명확히 보여주며, 실제 검색 궤적(search trajectories)과 피드백(feedback)으로 에이전트를 훈련하는 가치를 입증합니다. DeepDive는 또한 테스트 시 에이전트가 정보를 효율적으로 수집하기 위해 더 많은 도구 호출을 병렬로 수행할 수 있는 것과 같은 실용적인 개선 사항을 가능하게 합니다. 코드, 모델 및 데이터가 공개되어 향후 '심층 검색' 에이전트 개발을 위한 강력한 기반을 제공합니다.

---

**로컬 SGD의 외부 최적화기 이해: 학습률, 모멘텀, 가속 ([논문](https://arxiv.abs/2402.07842))**

Ahmed Khaled 외 연구진은 여러 노드(node)가 국소적인(local) 경사 하강(gradient step)을 수행한 후 주기적으로 동기화(synchronize)하는 분산 훈련(distributed training) 기술인 **로컬 SGD(Local SGD)**의 이론을 심도 있게 탐구합니다. 그들은 종종 간과되는 외부 최적화기(outer optimizer), 즉 다른 노드의 모델을 통합할 때 적용되는 업데이트 규칙에 주목합니다. 많은 연구가 로컬(내부) SGD 설정을 최적화하는 데 집중하는 반면, 이 연구는 외부 학습률(learning rate), 모멘텀(momentum) 등이 전체 수렴(convergence) 과정에 어떻게 영향을 미치는지 분석합니다. 주요 발견 및 기여는 다음과 같습니다.

*   **외부 학습률(Outer Learning Rate)의 핵심 역할**: 저자들은 외부 루프(outer-loop) 학습률을 조절하는 것이 매우 중요함을 입증합니다. 더 높은 외부 학습률은 로컬 모델 평균으로부터의 업데이트를 증폭시킬 수 있는 반면, 더 낮은 학습률은 이를 약화시킵니다. 그들은 이 학습률을 조정함으로써 최종 최적화 오차와 확률적 경사(stochastic gradients)로 인한 노이즈(noise) 사이의 균형을 효과적으로 맞출 수 있음을 보여줍니다. 흥미롭게도, 그들의 이론은 최적의 외부 학습률이 때때로 1.0을 초과할 수 있으며(즉, 단순 평균을 넘어섬), 이는 느린 수렴이나 최적이 아닌 내부 설정을 상쇄하기 위함이라고 제안합니다. 더욱이, 잘 선택된 외부 학습률은 제대로 조정되지 않은 내부 학습률을 보완하여, 본질적으로 로컬 훈련에서 발생한 일부 오류를 수정하는 역할을 할 수 있습니다.
*   **외부 루프에서의 모멘텀(Momentum) 및 가속(Acceleration) 통합**: 분석을 확장하여, 그들은 외부 최적화기에 모멘텀을 통합하고 효과적인 '모멘텀 조정' 학습률을 정의합니다. 이점은 유사합니다. 외부 모멘텀을 적절히 조절하면 수렴을 더욱 부드럽고 빠르게 만들 수 있습니다. 그들은 또한 외부 루프에 적용된 네스테로프 가속(Nesterov acceleration)을 탐구하여, 통신 라운드(communication rounds)에 대한 수렴 속도를 향상시킨다는 것을 증명합니다. 이는 이전의 가속 방식이 주로 로컬 업데이트에 초점을 맞췄기 때문에 특히 주목할 만합니다. 전역 업데이트(global updates)를 가속화하는 것이 순수 로컬 가속보다 더 나은 이론적 속도를 제공합니다.
*   **데이터 의존적 통찰 및 실증적 검증**: 이 논문은 로컬 SGD에 대한 새로운 데이터 의존적 수렴 분석을 제공하며, 데이터 이질성(data heterogeneity)과 같은 속성에 기반하여 외부 학습률을 설정하는 실용적인 지침을 제시합니다. 마지막으로, 분산 노드에 걸쳐 표준 언어 모델을 훈련하는 실험은 이론적 예측을 확증합니다. 예를 들어, 1보다 큰 외부 학습률을 사용하거나 외부 모멘텀을 추가하는 것은 수렴 속도와 최종 정확도에서 예상되는 개선 사항과 일치했으며, 이는 신중한 외부 최적화기(optimizer) 설계가 분산 훈련 효율성을 크게 향상시킬 수 있음을 입증합니다.

---

**종합 및 결론**

이번 LLM 동향 보고서를 통해 우리는 인공지능(AI) 분야의 다양한 전선에서 눈부신 진보가 이루어지고 있음을 확인했습니다. 처음 두 연구에서는 훈련 환경과 사전 학습(pre-training) 방식을 확장함으로써, 보다 범용적이고 유능한 에이전트(agent)를 구축하려는 공동의 노력을 엿볼 수 있었습니다. 이러한 노력은 비교적 작은 규모의 공개 모델들이 복잡한 도구 사용 작업에서 대규모의 비공개 모델들과 대등한 성능을 발휘할 수 있게 만들고 있습니다. 동시에, OmniWorld 데이터셋(dataset)은 4차원 비전(4D vision)과 같은 도메인(domain)에서 풍부하고 다양한 훈련 데이터가 얼마나 중요한 이점을 가져올 수 있는지를 강조합니다.

우리는 또한 대규모 언어 모델(LLM)이 전문 도메인에 어떻게 성공적으로 적용되는지를 목격했습니다. QuantAgent의 초고속 거래(high-frequency trading) 성공 사례는 도메인별 구조화를 통해 LLM이 실시간 고위험 애플리케이션(application)을 처리할 수 있는 잠재력을 보여줍니다. 보다 근본적인 측면에서, "컨텍스트 내 학습(In-Context Learning)은 학습인가?"와 같은 연구는 모델이 컨텍스트(context)로부터 어떻게 학습하는지(또는 학습에 실패하는지)에 대한 우리의 이해를 심화시키고, 프롬프트(prompt) 기반 '학습'의 내재적 한계에 대한 현실적인 인식을 제공합니다. 에이전트 아키텍처(agent architecture)의 지속적인 개선(DeepDive 연구)은 장기적 추론(long-horizon reasoning) 문제를 해결하기 위해 지식 그래프(knowledge graphs)와 강화 학습(reinforcement learning)을 결합하는 가치를 명확히 보여줍니다. 마지막으로, 분산 최적화(distributed optimization) 분야의 이론적 혁신(로컬 SGD(Local SGD)를 위한 외부 최적화기(outer optimizers) 분석)은 대규모 데이터셋에 대한 모델 훈련 속도를 가속화하는 원칙적인 방법을 제시합니다.

이러한 모든 연구 결과들을 종합해 볼 때, 우리는 데이터, 알고리즘(algorithm), 그리고 이론적 이해의 발전이 수렴하여 더욱 유능하고 효율적이며 통찰력 있는 AI 시스템을 만들어내고 있음을 알 수 있습니다. 각 연구는 단순히 더 강력할 뿐만 아니라, 더 깊이 이해될 수 있는 AI를 구축하는 거대한 퍼즐의 중요한 조각들을 기여하고 있습니다. 이는 인공지능이 단순한 기술적 진보를 넘어, 우리 사회와 산업 전반에 걸쳐 혁신적인 변화를 이끌어낼 잠재력을 가지고 있음을 시사합니다.

---

❤️ 이 글이 유익하셨다면 '좋아요'를 눌러주시고 동료들과 공유해주세요. 여러분의 소중한 의견을 댓글로 남겨주시면 감사하겠습니다.

LLM Watch를 읽어주셔서 감사합니다! 새로운 소식을 무료로 받아보고 저의 작업을 지원하시려면 지금 바로 구독해주세요.

구독하기