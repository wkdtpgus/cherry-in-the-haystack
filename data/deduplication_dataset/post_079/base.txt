# 🥇**이번 주 주요 AI(인공지능) 논문**

Author: Elvis Saravia
URL: https://nlp.elvissaravia.com/p/top-ai-papers-of-the-week-52c

============================================================

**1. 언어 모델이 환각(Hallucination)을 일으키는 이유**
이 논문은 환각(hallucination)이 신비로운 결함이 아니라 LLM이 훈련되고 평가되는 방식의 예측 가능한 결과라고 주장합니다. 사전 훈련(pretraining)은 오류를 발생시키도록 통계적 압력을 생성하며, 사후 훈련 벤치마크(post-training benchmark)는 종종 솔직한 불확실성보다 자신감 있는 추측에 보상합니다. 해결책은 기권(abstention)에 불이익을 주는 것을 중단하도록 주류 평가를 재조정하는 것입니다. 사전 훈련(pretraining)은 필연적으로 일부 오류를 발생시킵니다. 저자들은 생성을 이진 "유효성 여부" 분류 문제(binary “Is-It-Valid” classification problem)로 축소하고 하한(lower bound)을 제시합니다. 즉, 생성 오류율은 해당 분류기(classifier)의 오분류율에 비례합니다. 오류 없는 코퍼스(corpus)를 사용하더라도, 교차 엔트로피(cross-entropy)를 최적화하면 항상 "모르겠다"고 말하기보다는 여전히 오류를 생성하는 보정된 기본 모델(calibrated base model)이 나옵니다. 임의의 사실은 환각(hallucination)의 하한선을 결정합니다. 학습 가능한 패턴이 없는 사실(예: 특정 생일)의 경우, 이 논문은 환각률을 훈련 데이터(training data)의 "싱글턴 비율(singleton rate)"과 연결합니다. 많은 사실이 한 번만 나타나는 경우, 보정된 기본 모델(calibrated base model)은 그러한 프롬프트(prompt)의 최소한 그 비율에 대해 환각을 일으킬 것입니다. 이는 Good-Turing 방식의 누락 질량 추론(missing-mass reasoning)을 일반화하고, 프롬프트(prompt)와 "모르겠다(IDK)"를 추가하면서 이전 결과를 복구합니다. 모델 클래스(model class)의 한계 또한 중요합니다. 모델 계열이 필요한 구별을 표현할 수 없을 때 오류는 지속됩니다. 이 논문은 이를 불가지론적 학습 경계(agnostic-learning bound)를 통해 형식화하고, 최적 임계값 설정(optimal thresholding)조차도 모델 용량(model capacity)과 관련된 고정 오류를 남기는 객관식과 같은 간단한 사례를 제시하며, 고전적인 n-그램 모델(n-gram model)이 특정 문맥 의존성(context dependency)에서 실패해야 함을 보여주는 예시를 제공합니다. 사후 훈련(post-training)은 종종 추측을 강화합니다. 대부분의 인기 있는 벤치마크(benchmark)는 이진 정답-오답 방식으로 채점하고 기권(abstention)에 0점을 주기 때문에, 항상 추측하는 모델이 불확실한 답변을 보류하는 모델보다 성능이 뛰어날 수 있습니다. 저자들은 널리 사용되는 리더보드(leaderboard)를 조사한 결과, 기권(abstention)이 대체로 불이익을 받는다는 것을 발견했으며, 이는 완화 노력에도 불구하고 과도하게 자신감 있는 환각(hallucination)이 지속되는 이유를 설명합니다. 제안된 해결책: 명시적 신뢰도 목표(explicit confidence target). 오답에 대한 명확한 불이익과 "모르겠다(IDK)"에 대한 중립적인 점수를 주류 평가에 직접 통합하여, 모델이 명시된 신뢰도 임계값(confidence threshold) 이상에서만 답변하도록 지시해야 합니다. 이는 모델이 목표 신뢰도(target confidence)에 따라 답변과 기권(abstention) 사이에서 선택하는 행동 보정(behavioral calibration)을 촉진하며, 해당 분야를 더 신뢰할 수 있는 시스템(system)으로 이끌어야 합니다.
논문 | 트윗

**2. 뇌와 컴퓨터 비전 모델 간의 수렴(Convergence) 요인 분리**
자연 이미지로 훈련된 대규모 자기 지도 ViT(self-supervised ViT)는 뇌와 유사한 내부 표현(internal representation)을 개발합니다. 이 논문은 DINOv3(DINOv3)에서 모델 크기, 훈련량, 이미지 유형을 다양하게 변경하여 뇌와 컴퓨터 비전 모델 간의 수렴(convergence)을 이끄는 요소를 분리한 다음, 전반적인 선형 예측 가능성(인코딩)(overall linear predictability (encoding)), 피질 지형(공간)(cortical topography (spatial)), 시간 정렬(시간)(temporal alignment (temporal))이라는 세 가지 지표(metric)를 사용하여 모델 활성화(activation)를 인간 fMRI(공간) 및 MEG(시간)와 비교합니다. 결과: 세 가지 요소 모두 중요하며, 정렬(alignment)은 초기 감각 피질에서 고차 연합 피질까지 일관된 순서로 전개됩니다. 설정 및 지표(metric): 크기와 데이터셋(dataset)을 아우르는 8가지 DINOv3(DINOv3) 변형; 비교는 NSD fMRI 및 THINGS-MEG와 인코딩(encoding), 공간(spatial), 시간(temporal) 점수를 사용합니다. 기준선 정렬(baseline alignment): fMRI 예측 가능성은 시각 경로(visual pathway)를 따라 집중됩니다(복셀(voxel) 피크는 R≈0.45 부근). MEG 예측 가능성은 이미지 시작 후 약 70ms 후에 상승하여 3초까지 우연 수준 이상으로 유지됩니다. 공간 계층 구조가 유지됩니다(하위 레이어(layer) ↔ 초기 시각; 상위 레이어(layer) ↔ 전전두엽; r≈0.38). 시간 순서가 강합니다(초기 MEG 윈도우(window) ↔ 초기 레이어(layer); r≈0.96). 훈련 역학(training dynamics): 정렬(alignment)은 빠르게 나타나지만 균일하지는 않습니다. 시간 점수가 최종 값의 절반에 먼저 도달하고(훈련의 약 0.7%), 그 다음 인코딩(encoding)(약 2%), 그 다음 공간(spatial)(약 4%) 순입니다. 초기 시각 ROI(ROI)와 초기 MEG 윈도우(window)는 전전두엽 ROI(ROI) 및 후기 윈도우(window)보다 더 빨리 수렴합니다(V1까지의 거리 대 절반 시간 r≈0.91; 시간 윈도우(window) 대 절반 시간 r≈0.84). 스케일(scale) 및 데이터(data) 효과: 더 큰 모델은 더 높은 인코딩(encoding), 공간(spatial), 시간(temporal) 점수로 완료되며, 이득은 상위 수준 ROI(ROI)(예: BA44, IFS)에서 가장 큽니다. 인간 중심 이미지는 동일한 데이터(data) 볼륨(volume)에서 모든 지표(metric)와 ROI(ROI)에 걸쳐 위성 및 세포 이미지보다 뛰어납니다. 피질 상관관계(cortical correlate): 모델 정렬(alignment)이 나중에 나타나는 ROI(ROI)는 발달 확장이 더 크고, 피질이 더 두꺼우며, 내재적 시간 척도가 더 느리고, 미엘린(myelin)이 더 적은 영역입니다(예: 상관관계 |r|≈0.88까지). 이는 생물학적 성숙 궤적을 반영합니다.
논문 | 트윗

**3. 범용 심층 연구(Universal Deep Research)**
사용자가 "자신만의 모델과 전략을 가져오세요"를 가능하게 하는 일반적이고 모델에 구애받지 않는 심층 연구 에이전트(deep-research agent)를 제안합니다. 고정된 파이프라인(pipeline) 대신, UDR(UDR)은 자연어 연구 전략을 실행 가능한 코드(code)로 컴파일(compile)하고, 샌드박스(sandbox)에서 실행하며, 최종 보고서를 반환하기 전에 구조화된 진행 알림을 내보냅니다. 동기. 현재의 심층 연구 도구는 전략과 모델 선택을 하드코딩(hard-code)하여 소스 우선순위 지정, 도메인(domain)별 워크플로우(workflow), 모델 교체 가능성을 제한합니다. UDR(UDR)은 연구 전략을 기본 모델과 분리함으로써 이 세 가지 격차 모두를 목표로 합니다. 메커니즘(mechanism). 사용자는 전략과 프롬프트(prompt)를 제공합니다. UDR(UDR)은 전략을 엄격한 도구 및 제어 흐름 제약 하에 단일 호출 가능 함수(callable function)로 변환한 다음, 격리된 환경에서 실행합니다. 오케스트레이션(orchestration)은 순수 코드(code)입니다. LLM은 요약, 순위 지정 또는 추출과 같은 로컬 작업에만 호출됩니다. 상태는 명명된 변수(variable)에 존재하며, 증가하는 컨텍스트(context)에 존재하지 않습니다. 단계 및 도구. 1단계는 건너뛴 단계와 드리프트(drift)를 줄이기 위해 전략을 단계별로 컴파일(compile)합니다. 2단계는 실시간 UI(UI) 업데이트를 위해 동기식 도구 호출과 yield 기반 알림으로 실행됩니다. 이 논문은 폭넓은 적용 가능성을 보여주기 위해 최소한의, 광범위한, 집중적인 예시 전략을 제공합니다. 효율성 및 신뢰성. 제어 로직(logic)은 CPU(CPU)에서 실행되는 반면, LLM 호출은 범위가 지정되고 드물게 유지되어 비용과 지연 시간(latency)을 개선합니다. 종단 간 전략 컴파일(compilation)은 LLM에게 "자체 오케스트레이션(orchestration)"을 프롬프트(prompt)하거나 단계별 코드(code)를 연결하는 것보다 더 신뢰할 수 있음이 입증되었습니다. 보안, UI(UI) 및 한계. 전략은 프롬프트 주입(prompt-injection) 또는 코드 익스플로잇(code exploit)을 방지하기 위해 샌드박스(sandbox)에서 실행됩니다. 데모 UI(UI)는 전략 편집, 알림 모니터링, 보고서 보기를 지원합니다. 한계점으로는 코드 생성 충실도(fidelity)에 대한 의존성, 실행 중 상호작용성 부족, 사용자가 작성한 전략이 건전하다고 가정하는 것이 포함됩니다. 저자들은 편집 가능한 전략 라이브러리(library)를 제공하고 자유로운 추론에 대한 더 엄격한 사용자 제어를 탐색할 것을 권장합니다.
논문 | 트윗

**4. 시각적 스토리텔링(Visual Story Telling)**
작가가 캐릭터, 장소, 타임라인(timeline)의 시각 자료에 직접 작용하여 스토리를 편집할 수 있게 하는 시스템(system) 및 디자인 프레임워크(framework)입니다. 단순히 프롬프트(prompt)만 사용하는 대신, 작가는 시각적 요소를 드래그(drag)하고, 연결하고, 재정렬합니다. 이 도구는 동기화된 텍스트 편집을 제안하고 시각적 골격에서 구절을 재생성할 수 있습니다. 프레임워크(framework): 8가지 요소 + 4가지 연산자(operator). 서사학(narratology)(파불라/시제트)을 기반으로 스토리 요소(행위자/캐릭터, 시간/시간성, 장소/공간, 사건/초점화)와 4가지 구성 연산자(operator): 위치, 연관, 연결, 전개를 사용합니다. 세 가지 조정된 뷰(view)를 가진 프로토타입(prototype). 개체-행동 그래프(graph), 장소 캔버스(canvas), 이벤트 타임라인(timeline)은 직접 조작을 가능하게 합니다. 즉, 캐릭터 또는 행동을 추가/제거하고, 장소 간에 개체를 드래그(drag)하고, 이벤트를 재정렬할 수 있으며, 조정된 강조 표시 및 선택은 편집을 선택된 장면에 제한합니다. 양방향 편집 및 버전 관리(versioning). 수동 텍스트 편집으로 시각 자료를 새로 고칠 수 있으며, 시각적 편집은 텍스트에 추적된 차이점(diff)을 생성합니다. 히스토리 트리(history tree)는 분기 탐색을 지원하며, "시각 자료에서 새로 고침" 모드(mode)는 현재 시각적 상태에서 스토리를 다시 작성합니다. 두 가지 연구: 계획 및 편집. 12명의 참가자를 대상으로 한 연구에서, 인지 부하(cognitive-load) 결과는 혼합적이었고 정신 모델(mental model) 불일치가 나타났지만, 시각 자료는 텍스트만 사용하는 것보다 계획, 검색, 반영을 개선했습니다. 8명의 창의적인 작가와 함께, 참가자들은 공간적, 시간적, 개체 편집을 성공적으로 표현했고, 탐색 및 불일치 해결에 도움이 된다고 생각했으며, 높은 창의성 지원 지수(Creativity Support Index)를 부여했습니다. 동시에 스타일(style) 및 대체 시각적 레이아웃(layout)에 대한 더 많은 제어를 요청했습니다. 구현 및 한계. React + Slate.js 프론트엔드(front end); 추출 및 편집을 위한 GPT-4o 프롬프트(prompt); 속도를 위한 병렬 문장 수준 추출. 가끔 LLM 지연 시간(latency) 또는 의도하지 않은 편집이 남아있습니다. 향후 작업에는 더 풍부한 구성 요소(관계, 감정), 스타일(style) 제어, 길거나 비선형적인 내러티브(narrative) 지원, 사용자 정의 다이어그램(diagram)을 위한 뷰 빌더(view-builder)가 포함됩니다.
논문 | 트윗

**5. rStar2-Agent**
rStar2-Agent(rStar2-Agent)는 단순히 더 긴 CoT(CoT)가 아니라 Python 도구 환경을 사용하여 더 똑똑하게 생각하는 법을 배우는, 에이전트 RL(agentic RL)로 훈련된 14B 수학 추론 모델(math-reasoning model)입니다. 이 모델은 노이즈(noisy)가 있는 성공적인 추적을 필터링(filter)하는 롤아웃 전략(rollout strategy)인 GRPO-RoC(GRPO-RoC)와 대규모, 저지연 시간(low-latency) 도구 실행을 위한 인프라(infrastructure)를 도입합니다. 64개의 MI300X GPU(GPU)에서 1주일 동안 510 RL 단계를 거쳐, 이 모델은 더 짧은 솔루션(solution)을 생성하고 수학을 넘어선 전이(transfer)를 보여주면서 최첨단 AIME(AIME) 수준에 도달합니다. 한 줄 요약 방법: GRPO-RoC(GRPO-RoC)는 롤아웃(rollout)을 과도하게 샘플링(oversample)한 다음, 다양한 실패를 보존하면서 가장 깨끗하고 정확한 것만 유지하여 훈련 중 도구 호출 오류 및 서식 문제를 줄입니다. 인프라(infrastructure): 전용의 격리된 코드 서비스는 훈련 단계당 약 45K의 동시 도구 호출을 약 0.3초의 종단 간 지연 시간(latency)으로 안정적으로 처리하며, 로드 밸런싱(load-balancing) 스케줄러(scheduler)는 사용 가능한 KV 캐시(cache)에 따라 롤아웃(rollout)을 할당하여 GPU(GPU) 유휴 시간을 줄입니다. 훈련 레시피: 도구 사용 및 서식 지정을 가르치기 위해 비추론 SFT(SFT)로 시작한 다음, 최대 출력 길이 8K → 12K → 12K로 확장하는 세 가지 RL 단계를 거쳐 마지막으로 더 어려운 문제에 집중합니다. RL 데이터(data)는 정수 답변이 있는 42K 수학 항목으로 선별되었습니다. 결과: Pass@1 AIME24 80.6, AIME25 69.8, HMMT25 52.7로, 훨씬 작은 크기에도 불구하고 o3-mini(중간) 및 DeepSeek-R1(DeepSeek-R1)을 능가하거나 일치합니다. AIME24/25에서 응답은 Qwen3-14B(Qwen3-14B) 및 QWQ-32B(QWQ-32B)보다 더 짧습니다. 일반화 및 행동: GPQA-Diamond(GPQA-Diamond)를 60.9로 개선하고 도구 사용 및 정렬 벤치마크(benchmark)에서 좋은 성능을 보입니다. 엔트로피(entropy) 분석은 보존된 포킹 토큰(forking token)과 도구 피드백(feedback)에 의해 트리거(trigger)된 새로운 반영 토큰(reflection token)을 보여주며, 이는 검증 및 수정을 가능하게 합니다.
논문 | 트윗

**6. 적응형 LLM 라우팅(Adaptive LLM Routing)**
지출 한도를 준수하면서 각 쿼리(query)에 대해 어떤 모델을 호출할지 온라인(online)으로 학습하는 라우팅 프레임워크(routing framework)입니다. 이 프레임워크는 라우팅(routing)을 문맥적 밴딧(contextual bandit)으로 취급하고, 인간 선호도 데이터(data)로 초기화하며, 쿼리(query) 전반에 걸쳐 예산을 할당하는 온라인 비용 정책을 추가합니다. 핵심 아이디어: 쿼리(query)와 후보 LLM을 위한 공유 임베딩 공간(embedding space)을 구축하고, 이를 오프라인(offline) 인간 선호도와 정렬한 다음, 밴딧 피드백(feedback)을 사용하여 LLM 임베딩(embedding)을 온라인(online)으로 업데이트합니다. 선택은 코사인 유사도(cosine-similarity) 보상을 사용하는 선호도 사전 LinUCB 변형(PILOT)을 사용합니다. 예산 제어: 보상-비용 임계값(threshold)으로 적격 모델을 필터링(filter)하고 총액이 예산 내에 유지되도록 빈(bin)에 지출을 할당하는 온라인 다중 선택 배낭 정책(online multi-choice knapsack policy)(ZCL 스타일)을 도입합니다. 결과: RouterBench(RouterBench) 다중 작업 라우팅(routing)에서 GPT-4(GPT-4) 성능의 약 93%를 약 25%의 비용으로 달성합니다. 단일 작업 MMLU(MMLU)에서는 약 27%의 비용으로 약 86%를 달성합니다. 누적 후회(cumulative regret)는 밴딧 기준선(baseline)보다 지속적으로 낮습니다. 비용 정책 효율성: 온라인 정책은 예산 전반에 걸쳐 사후 분석으로 조정된 강력한 오프라인(offline) P − λC 오라클(oracle)과 일치하거나 이를 능가합니다. 지연 시간(latency) 오버헤드(overhead): 라우팅(routing)은 추론(inference)에 비해 거의 지연을 추가하지 않습니다. MMLU(MMLU)에서 GPT-4(GPT-4)의 약 2.5초에 비해 선택은 0.065–0.239초가 걸립니다.
논문 | 트윗

**7. LLM의 암묵적 추론(Implicit Reasoning)**
이 조사는 암묵적 추론(implicit reasoning)을 중간 단계를 출력하지 않고 모델의 잠재 상태(latent state) 내에서 발생하는 다단계 문제 해결로 정의합니다. 이 조사는 표현 형식보다는 실행 패러다임(paradigm)별로 분야를 정리하고, 증거, 평가, 공개 과제를 검토합니다. 세 가지 실행 패러다임(paradigm). 잠재 최적화(latent optimization)는 내부 표현(internal representation)을 직접 조정합니다. 즉, 토큰(token) 수준에서 특수 잠재 토큰(latent token)을 삽입하거나 학습하고, 궤적 수준에서 의미론적 충실도, 적응적 효율성, 점진적 정제 또는 탐색적 다양화를 위해 전체 사고의 사슬(chain of thought)을 압축하거나 정제하며, 내부 상태 수준에서 숨겨진 활성화(activation)를 증류하거나 조종하여 추론 신호를 전달합니다. 신호 유도 제어(signal-guided control)는 사고 또는 일시 정지 토큰(token)에서 인스턴스(instance) 수준 잠재 조정에 이르기까지 텍스트를 내보내지 않고 계산을 조절하기 위해 경량 제어를 사용합니다. 계층 순환 실행(layer-recurrent execution)은 ITT(ITT), 루프형 트랜스포머(Transformer), CoTFormer(CoTFormer), Huginn(Huginn), RELAY(RELAY)와 같은 모델을 사용하여 루프(loop)에서 공유 블록(block)을 재사용하여 내부적으로 더 깊은 사슬을 시뮬레이션(simulate)합니다. 잠재 프로세스(latent process)가 실제라는 증거. 구조적 신호는 계층별 분해 및 단축을 보여줍니다. 행동적 특징은 단계 건너뛰기 및 그로킹(grokking) 기반 상전이를 포함합니다. 표현 연구는 숨겨진 상태에서 중간 사실을 복구하거나 활성화 조종을 통해 추론을 유도합니다. 어떻게 평가되는가. 지표(metric)는 최종 답변 정확도(정확도(accuracy), Pass@k(Pass@k), EM(EM)), 효율성(지연 시간(latency), 출력 길이, FLOPs(FLOPs), ACU(ACU)), 혼란도(perplexity), 프로빙(probing) 정확도(accuracy)를 다룹니다. 벤치마크(benchmark)는 상식, 수학 및 코드, 독해, 다단계 QA(QA), 다중 모드 추론(multimodal reasoning)을 아우릅니다. 아직 해결되지 않은 이유. 주요 격차로는 제한된 해석 가능성(interpretability), 약한 제어 및 신뢰성, 어려운 작업에서 명시적 CoT(CoT)와의 정확도(accuracy) 격차, 불균일한 평가, 아키텍처(architecture) 제약, 명시적 감독에 대한 의존성이 포함됩니다. 큰 그림. 암묵적 추론(implicit reasoning)은 더 빠르고 저렴한 추론(inference)과 더 풍부한 내부 계산을 약속합니다. 이 조사는 계산을 잠재적이지만 감사 가능하게 유지하는 하이브리드(hybrid) 설계, 내부 궤적을 탐색하는 표준화된 평가, 그리고 맞춤형 토큰(token) 또는 루프(loop)를 넘어 일반화되는 아키텍처(architecture)를 주장합니다.
논문 | 트윗

**8. 임베딩 기반 검색의 이론적 한계에 대하여**
쿼리(query)가 충분히 많은 "혼합 및 일치" 문서 세트(document set)를 요구하게 되면, 단일 벡터(vector) 밀집 검색기(dense retriever)는 모든 가능한 상위 k 관련성 조합을 실현할 수 없습니다. 이 논문은 이러한 실패를 관련성 행렬(relevance matrix)의 부호 순위(sign-rank)와 연결하고, 필요한 임베딩 차원(embedding dimension)에 대한 하한(lower bound)과 상한(upper bound)을 증명한 다음, 간단하지만 적대적으로 조합적인 데이터셋(dataset)(LIMIT)으로 모델을 스트레스 테스트(stress-test)합니다. 이론. 저자들은 검색을 이진 qrel 행렬(matrix)에서 행별 순서 또는 임계값(threshold)을 보존하는 것으로 형식화하고, 이러한 용량이 행렬(matrix)의 부호 순위(sign-rank)에 의해 끼워져 있음을 보여줍니다. 고정된 차원 ddd에 대해 일부 상위 k 세트(set)는 표현 불가능하므로, 해당 ddd에서 어떤 단일 벡터(vector) 임베더(embedder)에게도 특정 검색 작업은 불가능합니다. 최상의 경우 최적화. 테스트 qrel에서 직접 최적화된 "자유 임베딩(embedding)"을 사용하면, k=2k=2k=2에 대한 최대 해결 가능한 코퍼스(corpus) 크기는 ddd의 세제곱에 대략 비례합니다. 외삽된 임계 크기는 4096차원 임베딩(embedding)에서도 웹 스케일(web scale)보다 훨씬 낮게 유지되며, 이는 훈련 데이터(training data)나 손실에 기인하지 않는 근본적인 한계를 나타냅니다. LIMIT 데이터셋(dataset) 결과. LIMIT는 모든 2개 문서 조합을 "누가 X를 좋아하나요?"와 같은 자연어 쿼리(query)에 매핑(map)합니다. 단순함에도 불구하고, SOTA(SOTA) 단일 벡터(vector) 모델은 전체 작업에서 Recall@100(Recall@100) 20% 미만을 기록하는 경우가 많으며, Recall@20(Recall@20)에서 46개 문서 버전을 여전히 해결할 수 없습니다. ddd가 커질수록 성능이 향상되지만 여전히 좋지 않습니다. 조합 밀도가 중요합니다. qrel 그래프(graph)가 고유한 상위 k 조합을 최대화하기 위해 밀집될 때, 모델 전반에 걸쳐 점수가 붕괴됩니다. 더 희소한 패턴(무작위, 주기, 분리)은 현저히 더 쉬우며, 이는 실현 가능한 상위 k 세트(set)의 수가 병목 현상임을 강조합니다. 대안 및 시사점. 교차 인코더(cross-encoder)는 작은 LIMIT 변형을 완벽하게 해결할 수 있으며, 다중 벡터(vector) 후기 상호작용 모델(late-interaction model)은 단일 벡터(vector)보다 성능이 좋고, BM25(BM25)와 같은 고차원 희소 기준선(baseline)은 강력한 성능을 보입니다. 많은 개념을 구성하는 지시 따르기 검색의 경우, 시스템(system)은 밀집 1단계 검색을 재순위 지정기(reranker), 다중 벡터(vector) 또는 희소 방법과 짝지우거나 대체해야 합니다.
논문 | 트윗

**9. 자체 진화 에이전트(Self-Evolving Agents)**
이 조사는 피드백 루프(feedback loop)를 통해 지속적으로 적응하여 정적 기반 모델(foundation model)과 평생 적응성을 연결하는 자체 진화 AI 에이전트(AI agent) 구축 기술을 검토합니다. 이 조사는 통합 프레임워크(framework)를 소개하고, 도메인(domain)별 전략을 다루며, 자율 에이전트 시스템(agentic system)을 발전시키는 데 있어 평가, 안전, 윤리를 논의합니다.
논문 | 트윗

**10. Hermes 4**
Hermes 4(Hermes 4)는 구조화된 다중 턴 추론(multi-turn reasoning)과 광범위한 지시 따르기를 통합하는 하이브리드(hybrid) 추론 모델(reasoning model) 계열을 소개합니다. 이 보고서는 데이터(data) 및 훈련 과제를 상세히 설명하고, 추론, 코딩, 정렬 작업 전반에 걸쳐 성능을 평가하며, 모든 모델 가중치(weight)를 공개적으로 출시합니다.
논문 | 트윗