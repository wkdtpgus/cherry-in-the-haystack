## AI를 일반 기술로 이해하기: 2025년 업데이트

이 게시물은 AI의 미래에 대한 우리의 비전을 담은 이전 논문의 업데이트 버전입니다. 지난 게시물에서 우리는 AI를 우리가 통제하고 다룰 수 있는 '일반 기술(normal technology)'로 보아야 한다고 주장했습니다. 이 관점은 2024년 9월 발표된 "AI를 일반 기술로 이해하기 위한 가이드(A guide to understanding AI as normal technology)"라는 동반 자료에서 더욱 상세히 다루어졌습니다. 이 글은 그 이후의 변화와 새로운 논의들을 반영하여 우리의 주장을 강화하고 확장합니다.

이 게시물은 15,000단어가 넘는 분량으로, AI의 미래에 대한 우리의 비전을 담은 새로운 논문입니다. 이 아이디어들을 확장한 버전이 우리의 다음 공동 저서가 될 것임을 발표하게 되어 기쁩니다. 이 논문은 나이트 수정헌법 제1조 연구소(Knight First Amendment Institute) 웹사이트에도 HTML 및 PDF 형식으로 게시되어 있습니다. 논문 초안에 대해 받은 광범위한 피드백에 감사드립니다.

우리는 인공지능(AI)을 일반 기술(normal technology)로 보는 비전을 제시합니다. AI를 일반적인 것으로 보는 것은 그 영향을 과소평가하는 것이 아닙니다. 전기나 인터넷과 같은 혁신적이고 범용 기술(general-purpose technologies)조차도 우리의 개념에서는 "정상적"입니다. 이는 AI를 별개의 종(separate species), 즉 고도로 자율적이고 잠재적으로 초지능적인 존재(superintelligent entity)로 취급하는 경향이 있는 AI의 미래에 대한 유토피아적 비전과 디스토피아적 비전 모두와 대조됩니다.

"AI는 일반 기술이다"라는 진술은 현재 AI에 대한 설명, AI의 예측 가능한 미래에 대한 예측, 그리고 우리가 AI를 어떻게 다루어야 하는지에 대한 처방, 세 가지를 의미합니다. 우리는 AI를 우리가 통제할 수 있고 통제해야 하는 도구로 보며, 이 목표가 급진적인 정책적 개입이나 기술적 돌파구를 필요로 하지 않는다고 주장합니다. 우리는 AI를 인간과 유사한 지능으로 보는 것이 현재 AI의 사회적 영향을 이해하는 데 정확하거나 유용하다고 생각하지 않으며, 미래에 대한 우리의 비전에서도 그럴 가능성은 낮습니다.

일반 기술(normal technology) 프레임은 기술과 사회의 관계에 관한 것입니다. 이는 기술 결정론(technological determinism), 특히 AI 자체가 미래를 결정하는 주체라는 개념을 거부합니다. 이는 기술 채택 및 확산의 느리고 불확실한 성격과 같은 과거 기술 혁명에서 얻은 교훈에 의해 안내됩니다. 또한 사회적 영향과 이 궤적을 형성하는 기관의 역할 측면에서 AI의 과거와 미래 궤적 사이의 연속성을 강조합니다.

1부에서는 혁신적인 경제적 및 사회적 영향이 느리게(수십 년의 시간 척도에서) 나타날 것이라고 생각하는 이유를 설명하며, AI 방법론(methods), AI 애플리케이션(applications), AI 도입(adoption) 사이에 중요한 구분을 짓고, 이 세 가지가 다른 시간 척도에서 발생한다고 주장합니다. 2부에서는 고도화된 AI(그러나 일반적으로 개념화되는 것처럼 비일관적이라고 보는 "초지능(superintelligent)" AI는 아님)가 있는 세상에서 인간과 AI 간의 잠재적인 노동 분업에 대해 논의합니다. 이 세상에서 통제는 주로 사람과 조직의 손에 있습니다. 실제로 사람들이 직업에서 하는 일의 점점 더 많은 부분이 AI 제어입니다. 3부에서는 AI를 일반 기술로 보는 것이 AI 위험에 미치는 영향을 살펴봅니다. 우리는 사고, 군비 경쟁(arms races), 오용, 오정렬(misalignment)을 분석하고, AI를 일반 기술로 보는 것이 AI를 인간과 유사한 것으로 보는 것과 비교하여 완화에 대해 근본적으로 다른 결론을 도출한다고 주장합니다. 물론, 우리의 예측을 확신할 수는 없지만, 우리가 중간 결과(median outcome)로 보는 것을 설명하는 것을 목표로 합니다. 우리는 확률을 정량화하려고 시도하지 않았지만, AI가 일반 기술처럼 행동하는지 여부를 알려줄 수 있는 예측을 하려고 노력했습니다. 4부에서는 AI 정책에 대한 함의를 논의합니다. 우리는 불확실성을 줄이는 것을 최우선 정책 목표로, 회복탄력성(resilience)을 재앙적 위험(catastrophic risks)에 대한 포괄적인 접근 방식으로 옹호합니다. 우리는 초지능(superintelligent) AI를 통제하기 어렵다는 전제에 기반한 급진적인 개입이 AI가 일반 기술로 판명될 경우 실제로는 상황을 훨씬 더 악화시킬 것이며, 그 단점은 불평등과 같이 자본주의 사회에 배포된 이전 기술의 단점을 반영할 가능성이 높다고 주장합니다.

우리가 2부에서 설명하는 세상은 오늘날보다 AI가 훨씬 더 발전한 세상입니다. 우리는 AI의 발전—또는 인간의 발전—이 그 시점에서 멈출 것이라고 주장하는 것이 아닙니다. 그 이후에는 무엇이 올까요? 우리는 알지 못합니다. 이 비유를 생각해 보십시오. 첫 산업 혁명 초기에 산업화된 세상이 어떻게 보일지 생각하고 대비하는 것은 유용했을 것이지만, 전기나 컴퓨터를 예측하려고 시도하는 것은 헛수고였을 것입니다. 여기서 우리의 작업도 유사합니다. 우리는 "급진적 발전(fast takeoff)" 시나리오를 거부하므로, 우리가 시도한 것보다 더 먼 미래의 세상을 상상하는 것이 필요하거나 유용하다고 보지 않습니다. 2부에서 설명하는 시나리오가 현실화되면, 우리는 다음에 올 것이 무엇이든 더 잘 예측하고 대비할 수 있을 것입니다. 독자에게 드리는 말씀. 이 에세이는 명제(proposition)를 옹호하기보다는 세계관(worldview)을 진술하는 특이한 목표를 가지고 있습니다. AI 초지능(superintelligence)에 대한 문헌은 방대합니다. 우리는 잠재적인 반론에 대해 일대일로 답변하려고 시도하지 않았는데, 그렇게 하면 논문이 몇 배나 길어질 것이기 때문입니다. 이 논문은 우리의 견해를 처음으로 표명한 것에 불과하며, 다양한 후속 조치를 통해 이를 상세히 설명할 계획입니다.

**1부: 발전의 속도**
**그림 1.** 다른 범용 기술(general-purpose technologies)과 마찬가지로, AI의 영향은 방법론과 역량이 향상될 때가 아니라, 이러한 개선이 애플리케이션으로 전환되어 경제의 생산적인 부문에 확산될 때 실현됩니다. 4 각 단계에는 속도 제한이 있습니다. AI의 발전은 AI 역량과 도입이 증가함에 따라 사람과 기관이 적응할 수 있도록 점진적으로 이루어질까요, 아니면 대규모 혼란이나 심지어 기술적 특이점(technological singularity)으로 이어지는 급격한 도약이 있을까요? 이 질문에 대한 우리의 접근 방식은 중요성이 높은 작업을 중요성이 낮은 작업과 분리하여 분석하고, 혁신과 발명의 속도로 돌아가기 전에 AI 도입 및 확산의 속도를 분석하는 것으로 시작합니다. 우리는 발명(invention)을 AI의 다양한 작업을 수행하는 능력을 향상시키는 새로운 AI 방법론(methods) —예를 들어 대규모 언어 모델(large language models)과 같은—의 개발을 지칭하는 데 사용합니다. 혁신(innovation)은 소비자와 기업이 사용할 수 있는 AI를 활용한 제품 및 애플리케이션 개발을 의미합니다. 도입(adoption)은 개인(또는 팀 또는 기업)이 기술을 사용하기로 결정하는 것을 의미하며, 확산(diffusion)은 도입 수준이 증가하는 광범위한 사회적 과정을 의미합니다. 충분히 파괴적인 기술의 경우, 확산은 기업 및 조직의 구조뿐만 아니라 사회적 규범 및 법률의 변화를 요구할 수 있습니다.

**안전이 중요한 영역에서의 AI 확산은 느립니다.**
"예측 최적화에 반대하며(Against Predictive Optimization)"라는 논문에서 우리는 예측 최적화(predictive optimization)의 약 50가지 애플리케이션에 대한 포괄적인 목록을 작성했습니다. 이는 머신러닝(ML)을 사용하여 개인의 미래 행동이나 결과를 예측함으로써 개인에 대한 결정을 내리는 것을 의미합니다. 5 범죄 위험 예측, 보험 위험 예측, 아동 학대 예측과 같은 이러한 애플리케이션의 대부분은 사람들에게 중요한 결과를 초래하는 결정을 내리는 데 사용됩니다. 이러한 애플리케이션이 확산되었지만, 중요한 뉘앙스가 있습니다. 대부분의 경우, 수십 년 된 통계 기법(statistical techniques) —단순하고 해석 가능한 모델(주로 회귀 분석)과 비교적 적은 수의 수작업 특징(handcrafted features)—이 사용됩니다. 랜덤 포레스트(random forests)와 같은 더 복잡한 머신러닝 방법은 거의 사용되지 않으며, 트랜스포머(transformers)와 같은 현대적인 방법은 찾아볼 수 없습니다. 다시 말해, 이 광범위한 영역에서 AI 확산은 혁신보다 수십 년 뒤처져 있습니다. 주요 원인 중 하나는 안전입니다. 모델이 더 복잡하고 덜 이해하기 어려울수록 테스트 및 검증 과정에서 가능한 모든 배포 조건(deployment conditions)을 예측하기 어렵습니다. 좋은 예는 에픽(Epic)의 패혈증 예측 도구(sepsis prediction tool)인데, 내부적으로 검증했을 때는 높은 정확도를 보였지만, 병원에서는 훨씬 더 나쁜 성능을 보여 패혈증 사례의 3분의 2를 놓치고 의사들에게 오경보(false alerts)를 쏟아냈습니다. 6 에픽의 패혈증 예측 도구는 제약 없는 특징 집합(feature sets)을 가진 복잡한 모델에서 잡기 어려운 오류 때문에 실패했습니다. 7 특히, 모델 훈련에 사용된 특징 중 하나는 의사가 이미 항생제를 처방했는지 여부였습니다. 즉, 테스트 및 검증 중에 모델은 미래의 특징을 사용하고 있었고, 결과에 인과적으로 의존적(causally dependent)인 변수에 의존하고 있었습니다. 물론 이 특징은 배포 시에는 사용할 수 없습니다. 해석 가능성(interpretability) 및 감사(auditing) 방법은 의심할 여지 없이 개선되어 이러한 문제를 훨씬 더 잘 잡아낼 수 있게 되겠지만, 아직 그 단계에 도달하지 못했습니다. 생성형 AI(generative AI)의 경우, 나중에 보면 매우 명확해 보이는 실패조차도 테스트 중에 잡히지 않았습니다. 한 예는 초기 빙 챗봇 "시드니(Sydney)"가 장시간 대화 중에 엉뚱한 방향으로 흘러갔던 것인데, 개발자들은 대화가 몇 차례 이상 지속될 수 있다는 것을 분명히 예상하지 못했습니다. 8 마찬가지로, 제미니(Gemini) 이미지 생성기는 역사적 인물에 대해 테스트된 적이 없는 것으로 보입니다. 9 다행히도, 이들은 중요성이 높은 애플리케이션은 아니었습니다. 다양한 애플리케이션에서 혁신-확산 지연(innovation-diffusion lag)과 그 원인을 이해하는 데 더 많은 실증적 연구(empirical work)가 도움이 될 것입니다. 그러나 현재로서는 우리가 이전 연구에서 분석한 증거는 중요성이 높은 작업에서 이미 매우 강력한 안전 관련 속도 제한이 존재한다는 견해와 일치합니다. 이러한 제한은 FDA의 의료기기 감독과 같은 규제뿐만 아니라 고위험 AI에 엄격한 요구 사항을 부과하는 EU AI 법(EU AI Act)과 같은 새로운 법률을 통해 종종 시행됩니다. 10 사실, 고위험 AI에 대한 기존 규제가 너무 부담스러워서 "폭주하는 관료주의(runaway bureaucracy)"로 이어질 수 있다는 (신뢰할 만한) 우려가 있습니다. 11 따라서 우리는 중요성이 높은 작업에서 느린 확산이 계속해서 일반적인 현상이 될 것이라고 예측합니다. 어쨌든, AI가 중요성이 높은 방식으로 사용될 수 있는 새로운 영역이 발생하면, 우리는 이를 규제할 수 있고 규제해야 합니다. 좋은 예는 2010년 플래시 크래시(Flash Crash)인데, 자동화된 고빈도 거래가 한몫했다고 여겨집니다. 이는 서킷 브레이커(circuit breakers)와 같은 새로운 거래 제한으로 이어졌습니다. 12

**확산은 인간, 조직 및 제도 변화의 속도에 의해 제한됩니다.**
안전이 중요한 영역 외에서도 AI 도입은 대중적인 설명보다 느립니다. 예를 들어, 최근 연구에 따르면 지난 해 미국 성인의 상당수가 생성형 AI를 사용했지만, 대부분의 사람들이 이를 자주 사용하지 않았기 때문에, 이는 전체 근무 시간의 일부에 불과했으며 노동 생산성 증가에 미미한 영향을 미쳤습니다. 확산 속도가 과거보다 오늘날 더 빠른지는 명확하지 않습니다. 일부에서는 생성형 AI 도입이 개인용 컴퓨터(PC) 도입보다 빨랐다고 주장했지만, 이러한 비교는 사용 강도나 초기 구매 비용과 같은 중요한 요소를 고려하지 않습니다. 디지털 기술이 수십억 대의 장치에 동시에 도달할 수 있다는 점을 고려할 때, 기술 도입 속도가 반드시 증가하는 것은 아니라는 주장은 놀랍거나 틀렸다고 보일 수도 있습니다. 그러나 도입은 소프트웨어 사용에 관한 것이지 가용성에 관한 것이 아니라는 점을 기억하는 것이 중요합니다. 새로운 AI 기반 제품이 온라인에 무료로 즉시 출시되더라도, 사람들이 새로운 제품의 이점을 활용하고 위험을 피하는 방법을 배우기 위해 작업 흐름과 습관을 바꾸는 데는 시간이 걸립니다. 따라서 확산 속도는 개인뿐만 아니라 조직과 기관이 기술에 적응할 수 있는 속도에 의해 본질적으로 제한됩니다. 이는 우리가 과거의 범용 기술(general-purpose technologies)에서도 보았던 추세입니다. 확산은 수년이 아닌 수십 년에 걸쳐 발생합니다. 15 예를 들어, 폴 A. 데이비드(Paul A. David)의 전력화(electrification) 분석은 생산성 이점이 완전히 실현되는 데 수십 년이 걸렸음을 보여줍니다. 16 에디슨의 첫 중앙 발전소 이후 거의 40년 동안 전기 발전기(electric dynamos)는 "생산성 통계에는 없었지만 어디에나 있었습니다." 17 이는 단순히 기술적 관성(technological inertia) 때문만은 아니었습니다. 공장 소유주들은 전력화가 상당한 효율성 향상을 가져오지 않는다는 것을 발견했습니다. 결국 이점을 실현할 수 있었던 것은 생산 라인(production lines)의 논리를 중심으로 공장 전체 레이아웃을 재설계한 것이었습니다. 공장 건축의 변화 외에도 확산은 작업장 조직 및 공정 제어(process control)의 변화도 필요로 했으며, 이는 산업 전반에 걸친 실험을 통해서만 개발될 수 있었습니다. 이러한 변화의 결과로 작업자들은 더 많은 자율성과 유연성을 가졌고, 이는 또한 다른 고용 및 훈련 관행을 필요로 했습니다.

**외부 세계는 AI 혁신에 속도 제한을 둡니다.**
AI의 기술 발전이 빨랐다는 것은 사실이지만, AI 방법론(methods)과 애플리케이션을 구분하면 상황은 훨씬 덜 명확해집니다. 우리는 AI 방법론의 발전을 일반성 사다리(ladder of generality)로 개념화합니다. 18 이 사다리의 각 단계는 그 아래 단계에 기반을 두며, 더 일반적인 컴퓨팅 역량으로의 움직임을 반영합니다. 즉, 컴퓨터가 새로운 작업을 수행하는 데 필요한 프로그래머의 노력을 줄이고, 주어진 양의 프로그래머(또는 사용자) 노력으로 수행할 수 있는 작업의 집합을 증가시킵니다. 그림 2를 참조하십시오. 예를 들어, 머신러닝(machine learning)은 각 새로운 작업을 해결하기 위한 논리를 프로그래머가 고안할 필요성을 없애고 대신 훈련 예제 수집만을 요구함으로써 일반성을 증가시킵니다. 인공 일반 지능(AGI)에 도달할 때까지 사다리의 더 많은 단계를 구축함에 따라 특정 애플리케이션을 개발하는 데 필요한 노력이 계속 감소할 것이라고 결론 내리고 싶을 수 있습니다. AGI는 종종 모든 것을 즉시 수행할 수 있어 애플리케이션 개발의 필요성을 완전히 없애는 AI 시스템으로 개념화됩니다. 일부 영역에서는 실제로 애플리케이션 개발 노력 감소 추세가 나타나고 있습니다. 자연어 처리(natural language processing) 분야에서 대규모 언어 모델(large language models)은 언어 번역 애플리케이션을 개발하는 것을 비교적 사소한 일로 만들었습니다. 또는 게임을 생각해 보십시오. 알파제로(AlphaZero)는 게임에 대한 설명과 충분한 컴퓨팅 능력만 주어지면 자가 학습(self-play)을 통해 체스(chess)와 같은 게임을 어떤 인간보다도 잘 배우고 플레이할 수 있습니다. 이는 과거 게임 플레이 프로그램이 개발되던 방식과는 거리가 멉니다. **그림 2: 컴퓨팅의 일반성 사다리.** 일부 작업의 경우, 사다리의 높은 단계를 통해 컴퓨터가 새로운 작업을 수행하는 데 필요한 프로그래머의 노력이 줄어들고, 주어진 양의 프로그래머(또는 사용자) 노력으로 더 많은 작업을 수행할 수 있습니다. 19 그러나 이는 쉽게 시뮬레이션할 수 없고 오류 비용이 큰 중요성이 높은 실제 애플리케이션에서는 이러한 추세가 아니었습니다. 자율주행차를 생각해 보십시오. 여러 면에서 자율주행차 개발 궤적은 알파제로의 자가 학습과 유사합니다. 기술 개선을 통해 더 현실적인 조건에서 운전할 수 있게 되었고, 이는 더 좋거나 더 현실적인 데이터 수집을 가능하게 했으며, 이는 다시 기술 개선으로 이어져 피드백 루프를 완성했습니다. 그러나 이 과정은 알파제로의 경우 몇 시간이 걸린 것과 달리 20년 이상이 걸렸는데, 이는 안전 고려 사항이 이 루프의 각 반복이 이전 반복에 비해 확장될 수 있는 정도에 제한을 두었기 때문입니다. 20 이러한 "역량-신뢰성 격차(capability-reliability gap)"는 계속해서 나타납니다. 이는 실제 작업을 자동화할 수 있는 유용한 AI "에이전트(agents)"를 구축하는 데 주요 장벽이었습니다. 21 명확히 말하면, 여행 예약이나 고객 서비스 제공과 같이 에이전트 사용이 예상되는 많은 작업은 운전보다 중요성이 훨씬 낮지만, 에이전트가 실제 경험에서 배우는 것이 간단하지 않을 정도로 여전히 비용이 많이 듭니다. 안전이 중요하지 않은 애플리케이션에서도 장벽은 존재합니다. 일반적으로 조직 내 많은 지식은 암묵적 지식(tacit knowledge)이며 문서화되지 않으며, 수동적으로 학습될 수 있는 형태로 기록된 경우는 더욱 적습니다. 이는 이러한 개발 피드백 루프가 각 부문에서 발생해야 하며, 더 복잡한 작업의 경우 다른 조직에서 개별적으로 발생해야 할 수도 있어, 빠른 병렬 학습(parallel learning) 기회를 제한한다는 것을 의미합니다. 병렬 학습이 제한될 수 있는 다른 이유는 개인 정보 보호 문제입니다. 조직과 개인은 민감한 데이터를 AI 회사와 공유하는 것을 꺼릴 수 있으며, 규제는 의료와 같은 맥락에서 어떤 종류의 데이터를 제3자와 공유할 수 있는지 제한할 수 있습니다. AI의 "쓰디쓴 교훈(bitter lesson)"은 계산 능력 증가를 활용하는 일반적인 방법론이 결국 인간의 도메인 지식을 활용하는 방법론을 크게 능가한다는 것입니다. 22 이는 방법론에 대한 귀중한 관찰이지만, 종종 애플리케이션 개발을 포함하는 것으로 오해됩니다. AI 기반 제품 개발의 맥락에서 쓰디쓴 교훈은 결코 사실에 가깝지 않았습니다. 23 소셜 미디어의 추천 시스템(recommender systems)을 생각해 보십시오. 이들은 (점점 더 일반적인) 머신러닝 모델에 의해 구동되지만, 이는 비즈니스 로직(business logic), 프런트엔드(frontend) 및 기타 구성 요소의 수동 코딩 필요성을 없애지 못했습니다. 이러한 구성 요소는 모두 합쳐 백만 줄에 달하는 코드를 구성할 수 있습니다. AI가 기존 인간 지식에서 학습하는 것을 넘어설 필요가 있을 때 추가적인 한계가 발생합니다. 24 우리의 가장 가치 있는 지식 유형 중 일부는 과학적 및 사회과학적 지식이며, 기술과 대규모 사회 조직(예: 정부)을 통해 문명의 발전을 가능하게 했습니다. AI가 이러한 지식의 경계를 확장하려면 무엇이 필요할까요? 이는 약물 테스트에서 경제 정책에 이르기까지 사람이나 조직과의 상호 작용, 심지어 실험을 필요로 할 것입니다. 여기서는 실험의 사회적 비용(social costs of experimentation) 때문에 지식 습득 속도에 엄격한 제한이 있습니다. 사회는 AI 개발을 위한 실험의 빠른 확장을 허용하지 않을 것이며(또한 허용해서는 안 됩니다).

**벤치마크는 실제 유용성을 측정하지 않습니다.**
방법론-애플리케이션 구분은 AI 발전 측정 및 예측 방식에 중요한 함의를 가집니다. AI 벤치마크(benchmarks)는 방법론의 발전을 측정하는 데 유용합니다. 불행히도, 이들은 종종 애플리케이션의 발전을 측정하는 것으로 오해되었으며, 이러한 혼란은 임박한 경제적 변혁에 대한 많은 과장된 선전(hype)의 원동력이었습니다. 예를 들어, GPT-4가 변호사 시험(bar exam) 응시자 중 상위 10%의 점수를 달성했다고 보고되었지만, 이는 AI의 법률 실무 능력에 대해 놀랍도록 적은 정보를 제공합니다. 25 변호사 시험은 전문 지식(subject-matter knowledge)을 과도하게 강조하고, 표준화된 컴퓨터 관리 형식으로 측정하기 훨씬 어려운 실제 기술을 과소평가합니다. 다시 말해, 이는 언어 모델이 잘하는 것—기억된 정보를 검색하고 적용하는 것—을 정확히 강조합니다. 더 넓게 보면, 법률 전문가에게 가장 중요한 변화를 가져올 작업은 평가하기 가장 어려운 작업이기도 합니다. 법률 요청을 법률 영역별로 분류하는 것과 같은 작업은 명확한 정답이 있기 때문에 평가가 간단합니다. 그러나 법률 서류(legal filings) 작성과 같이 창의성과 판단을 요하는 작업에는 단 하나의 정답이 없으며, 합리적인 사람들은 전략에 대해 의견이 다를 수 있습니다. 후자의 작업은 자동화될 경우 해당 직업에 가장 심오한 영향을 미칠 작업입니다. 26 이 관찰은 결코 법률에만 국한되지 않습니다. 또 다른 예는 AI가 탁월한 성능을 보이는 독립적인 코딩 문제(coding problems)와, 그 영향은 측정하기 어렵지만 미미해 보이는 실제 소프트웨어 공학(real-world software engineering) 사이의 격차입니다. 27 장난감 문제(toy problems)를 넘어선 높은 평가를 받는 코딩 벤치마크조차도 정량화 및 공개적으로 사용 가능한 데이터를 사용한 자동 평가를 위해 실제 소프트웨어 공학의 많은 측면을 필연적으로 무시해야 합니다. 28 이러한 패턴은 반복적으로 나타납니다. 벤치마크를 통해 작업 측정이 쉬울수록, 전문 실무를 정의하는 복잡하고 맥락적인 작업을 나타낼 가능성은 낮아집니다. AI 발전 이해를 위해 역량 벤치마크에 지나치게 집중함으로써 AI 커뮤니티는 기술의 실제 영향을 일관되게 과대평가합니다. 이는 '구성 타당성(construct validity)' 문제인데, 이는 테스트가 실제로 측정하려는 것을 측정하는지 여부를 의미합니다. 29 잠재적 애플리케이션의 실제 유용성을 측정하는 유일한 확실한 방법은 실제로 애플리케이션을 구축하고, 현실적인 시나리오에서 전문가들과 함께 테스트하는 것입니다(의도된 사용에 따라 노동력을 대체하거나 증강하는 방식). 이러한 '향상(uplift)' 연구는 일반적으로 많은 직업의 전문가들이 기존 AI 시스템으로부터 이점을 얻는다는 것을 보여주지만, 이 이점은 일반적으로 미미하며 대체보다는 증강(augmentation)에 가깝습니다. 이는 시험과 같은 정적 벤치마크를 기반으로 결론 내릴 수 있는 것과는 근본적으로 다른 그림입니다. 30 (카피라이터와 번역가와 같은 소수의 직업은 상당한 일자리 손실을 겪었습니다. 31) 결론적으로, 벤치마크는 AI 방법론(methods)의 발전을 추적하는 데 유용하지만, AI 영향을 추적하기 위해서는 다른 종류의 지표를 살펴보아야 합니다(그림 1). 도입을 측정할 때는 AI 사용 강도를 고려해야 합니다. 애플리케이션 유형도 중요합니다. 증강 대 대체, 그리고 고위험 대 저위험. 구성 타당성(construct validity)을 보장하는 어려움은 벤치마킹뿐만 아니라 (미래) AI 영향을 평가하려는 또 다른 주요 방법인 예측에도 영향을 미칩니다. 효과적인 예측을 보장하기 위해 모호한 결과를 피하는 것이 매우 중요합니다. 예측 커뮤니티가 이를 달성하는 방법은 시험 성과와 같은 비교적 좁은 기술 측면에서 이정표를 정의하는 것입니다. 예를 들어, "인간-기계 지능 동등성(human-machine intelligence parity)"에 대한 메타큘러스(Metaculus) 질문은 수학, 물리학, 컴퓨터 과학 시험 문제의 성과 측면에서 정의됩니다. 이 정의에 따르면, 예측가들이 2040년까지 "인간-기계 지능 동등성"을 달성할 확률을 95%로 예측하는 것은 놀라운 일이 아닙니다. 32 불행히도, 이 정의는 너무 희석되어 AI의 영향을 이해하는 데 큰 의미가 없습니다. 위에서 법률 및 기타 전문 벤치마크에서 보았듯이, 시험에서의 AI 성능은 구성 타당성(construct validity)이 너무 낮아서 AI가 전문직 종사자를 대체할지 여부조차 예측할 수 없습니다.

**경제적 영향은 점진적일 가능성이 높습니다.**
AI 개발이 갑작스럽고 급격한 경제적 영향을 미 미칠 수 있다는 한 가지 주장은 일반성(generality)의 증가가 경제 내 광범위한 작업의 자동화 가능성으로 이어질 수 있다는 것입니다. 이는 모든 경제적으로 가치 있는 작업을 수행할 수 있는 통합 시스템인 일반 인공지능(AGI)의 한 정의와 관련이 있습니다. 일반 기술(normal technology) 관점에 따르면, 그러한 갑작스러운 경제적 영향은 있을 법하지 않습니다. 이전 섹션에서 우리는 한 가지 이유를 논의했습니다. AI 방법론(methods)의 갑작스러운 개선은 확실히 가능하지만, 혁신(애플리케이션 개발의 의미에서)과 확산을 필요로 하는 경제적 영향으로 직접적으로 이어지지는 않습니다. 혁신과 확산은 피드백 루프(feedback loop)에서 발생합니다. 안전이 중요한 애플리케이션에서는 이 피드백 루프가 항상 느리지만, 안전을 넘어서도 느릴 가능성이 높은 많은 이유가 있습니다. 전기, 컴퓨터, 인터넷과 같은 과거의 범용 기술(general-purpose technologies)의 경우, 각 피드백 루프는 수십 년에 걸쳐 전개되었으며, AI에서도 동일한 일이 발생할 것으로 예상해야 합니다. 점진적인 경제적 영향에 대한 또 다른 주장: 일단 우리가 무언가를 자동화하면, 그 생산 비용과 가치는 인간 노동 비용에 비해 시간이 지남에 따라 급격히 떨어지는 경향이 있습니다. 자동화가 증가함에 따라 인간은 적응할 것이며, 아직 자동화되지 않은 작업, 어쩌면 오늘날 존재하지 않는 작업(2부에서 그러한 작업이 어떤 모습일지 설명합니다)에 집중할 것입니다. 이는 자동화가 증가함에 따라 경제적으로 가치 있는 작업이 재정의되면서 AGI의 목표점(goalpost)이 계속해서 멀어질 것임을 의미합니다. 오늘날 인간이 하는 모든 작업이 언젠가 자동화될 수 있다 하더라도, 이것이 인간 노동이 불필요(superfluous)해진다는 것을 의미하지는 않습니다. 이 모든 것은 특정 시점에 경제의 광범위한 자동화 가능성에서 벗어납니다. 또한 강력한 AI의 영향이 다른 부문에서 다른 시간 척도로 느껴질 것임을 암시합니다.

**AI 방법론 발전의 속도 제한**
AI 영향의 느림에 대한 우리의 주장은 혁신-확산 피드백 루프(innovation-diffusion feedback loop)에 기반하며, AI 방법론(methods)의 발전이 임의로 가속화될 수 있다 하더라도 적용 가능합니다. 우리는 이점과 위험 모두 주로 개발보다는 AI 배포(deployment)에서 발생한다고 봅니다. 따라서 AI 방법론 발전의 속도는 영향 문제와 직접적으로 관련이 없습니다. 그럼에도 불구하고, 방법론 개발에도 적용되는 속도 제한을 논의할 가치가 있습니다. AI 연구 생산은 기하급수적으로 증가하고 있으며, arXiv에 AI/ML 논문 게재율은 2년 미만의 두 배 증가 시간을 보입니다. 33 그러나 이러한 양적 증가가 발전으로 어떻게 이어지는지는 명확하지 않습니다. 발전의 한 가지 척도는 핵심 아이디어의 교체율입니다. 불행히도, AI 분야는 역사 전반에 걸쳐 인기 있는 아이디어 주변에 높은 수준의 무리 짓기(herding)를 보였고, 유행에 뒤떨어진 아이디어에 대한 탐색 수준은 (돌이켜보면) 부적절했습니다. 주목할 만한 예는 수십 년 동안 신경망(neural networks) 연구가 뒷전으로 밀려났던 것입니다. 현재 시대는 다를까요? 아이디어가 점진적으로 증가하는 속도로 축적되더라도, 기존 아이디어를 대체하고 있을까요? 트랜스포머 아키텍처(transformer architecture)는 잘 알려진 한계에도 불구하고 지난 10년 대부분 동안 지배적인 패러다임이었습니다. 요한 S.G. 추(Johan S.G. Chu)와 제임스 A. 에반스(James A. Evans)는 241개 주제에 걸쳐 10억 개 이상의 인용을 분석하여, 논문 수가 많은 분야에서는 새로운 아이디어가 돌파하기가 더 어렵지 쉽지 않다는 것을 보여주었습니다. 이는 "정설의 경직화(ossification of canon)"로 이어집니다. 34 아마도 이 설명은 현재 AI 방법론 연구의 상태에 적용될 수 있을 것입니다. 다른 많은 속도 제한도 가능합니다. 역사적으로, 심층 신경망 기술(deep neural network technology)은 하드웨어, 특히 그래픽 처리 장치(GPU)의 부적절함으로 인해 부분적으로 지연되었습니다. 계산 및 비용 제한은 추론 시간 스케일링(inference-time scaling)을 포함한 새로운 패러다임에도 계속 관련이 있습니다. 새로운 둔화가 나타날 수 있습니다. 최근 징후는 업계의 개방형 지식 공유 문화에서 벗어나는 변화를 가리킵니다. AI가 수행하는 AI 연구(AI-conducted AI research)가 완화를 제공할 수 있을지는 아직 지켜봐야 합니다. 아마도 방법론에서 재귀적 자기 개선(recursive self-improvement)이 가능하여 방법론의 무한한 속도 향상으로 이어질 수도 있습니다. 그러나 AI 개발은 이미 AI에 크게 의존하고 있다는 점에 유의하십시오. 재귀적 자기 개선이 달성되는 단일하고 불연속적인 순간보다는 AI 개발에서 자동화의 역할이 점진적으로 증가하는 것을 계속 보게 될 가능성이 더 큽니다. 35 이전에 우리는 벤치마크가 AI 애플리케이션의 유용성에 대해 오해의 소지가 있는 그림을 제공한다고 주장했습니다. 그러나 벤치마크는 방법론 발전 속도에 대한 과도한 낙관론으로 이어졌다고도 볼 수 있습니다. 한 가지 이유는 현재 발전의 지평을 넘어서는 의미 있는 벤치마크를 설계하기 어렵기 때문입니다. 튜링 테스트(Turing test)는 수십 년 동안 AI의 북극성(north star)이었습니다. 이를 통과하는 어떤 시스템이든 중요한 면에서 인간과 유사할 것이며, 그러한 시스템을 사용하여 다양한 복잡한 작업을 자동화할 수 있을 것이라는 가정 때문이었습니다. 이제 대규모 언어 모델(large language models)이 테스트 뒤에 있는 기대를 약하게만 충족시키면서도 이를 통과할 수 있다고 주장할 수 있게 되면서, 그 중요성은 약화되었습니다. 36 등산(mountaineering)과의 비유가 적절합니다. 벤치마크를 해결할 때마다(우리가 정상이라고 생각했던 곳에 도달할 때마다), 우리는 벤치마크의 한계를 발견하고('가짜 정상(false summit)'에 있다는 것을 깨닫고) 새로운 벤치마크를 구축합니다(이제 우리가 정상이라고 생각하는 곳으로 시야를 돌립니다). 이는 '목표점 이동(moving the goalposts)'이라는 비난으로 이어지지만, 이는 벤치마킹의 본질적인 어려움을 고려할 때 우리가 예상해야 할 일입니다. AI 선구자들은 AI의 두 가지 큰 도전 과제(우리가 지금 AGI라고 부르는 것)를 (우리가 지금 부르는) 하드웨어와 소프트웨어라고 생각했습니다. 프로그래밍 가능한 기계(programmable machines)를 구축한 후, AGI가 가까웠다는 분명한 느낌이 있었습니다. 1956년 다트머스 회의(Dartmouth conference) 주최자들은 "2개월, 10인"의 노력으로 목표를 향해 상당한 진전을 이루기를 희망했습니다. 37 오늘날 우리는 일반성 사다리(ladder of generality)에서 훨씬 더 많은 단계를 올랐습니다. 우리는 종종 AGI를 구축하는 데 필요한 모든 것이 스케일링(scaling)이거나, 범용 AI 에이전트(generalist AI agents)이거나, 샘플 효율적 학습(sample-efficient learning)이라고 듣습니다. 그러나 단일 단계처럼 보이는 것이 그렇지 않을 수도 있다는 점을 명심하는 것이 유용합니다. 예를 들어, 모든 맥락에서 샘플 효율적 학습을 가능하게 하는 단일한 획기적인 알고리즘이 존재하지 않을 수도 있습니다. 실제로 대규모 언어 모델(large language models)의 인컨텍스트 학습(in-context learning)은 이미 "샘플 효율적"이지만, 제한된 작업 집합에서만 작동합니다. 38

**2부: 고도화된 AI가 있는 세상은 어떤 모습일까**
우리는 '지능(intelligence)'과 '초지능(superintelligence)'이라는 모호한 개념에 의존하는 것이 고도화된 AI가 있는 세상에 대해 명확하게 추론하는 우리의 능력을 흐리게 했다고 주장합니다. 지능을 역량(capability)과 힘(power)이라는 별개의 근본 개념으로 해체함으로써, 우리는 '초지능' AI가 있는 세상에서 인간 노동이 불필요(superfluous)해질 것이라는 개념을 반박하고 대안적인 비전을 제시합니다. 이는 또한 3부에서 위험에 대한 우리의 논의를 위한 토대를 마련합니다.

**인간의 능력은 생물학에 의해 제약되지 않습니다.**
AI가 인간 지능을 능가할 수 있을까요? 그렇다면 얼마나 능가할까요? 대중적인 주장에 따르면, 헤아릴 수 없을 정도로 능가할 수 있습니다. 이는 종종 지능 스펙트럼(spectrum of intelligence)을 따라 다른 종들을 비교함으로써 묘사됩니다. **그림 3.** 재귀적으로 자기 개선된 AI(recursively self-improved AI)를 통한 지능 폭발은 흔한 우려이며, 종종 이와 같은 그림으로 묘사됩니다. 그림은 다시 그려졌습니다. 39 그러나 이 그림에는 개념적 및 논리적 결함이 있습니다. 개념적 수준에서 지능—특히 다른 종들 간의 비교로서—은 단일 차원 척도(one-dimensional scale)로 측정될 수 있는 것은 고사하고 잘 정의되지도 않습니다. 40 더 중요하게는, 지능은 AI의 영향을 분석하는 데 핵심 속성(property at stake)이 아닙니다. 오히려 핵심 속성은 힘(power)—환경을 변화시키는 능력—입니다. 기술(특히 점점 더 일반화되는 컴퓨팅 기술)의 영향을 명확하게 분석하기 위해, 우리는 기술이 인류의 힘에 어떻게 영향을 미쳤는지 조사해야 합니다. 우리가 이러한 관점에서 사물을 볼 때, 완전히 다른 그림이 나타납니다. **그림 4.** 기술이 인류의 힘에 미치는 영향 분석. 우리는 우리의 지능 때문에 강력한 것이 아니라, 우리의 역량을 증가시키기 위해 사용하는 기술 때문에 강력합니다. 이러한 관점의 전환은 인간이 항상 기술을 사용하여 환경을 통제하는 능력을 증가시켜 왔다는 것을 명확히 합니다. 조상 인류와 현대 인류 사이에는 생물학적 또는 생리학적 차이가 거의 없습니다. 대신, 관련 차이점은 개선된 지식과 이해, 도구, 기술, 그리고 실제로 AI입니다. 어떤 의미에서, 행성과 그 기후를 변화시킬 능력을 가진 현대 인류는 기술 이전의 인류(pre-technological humans)에 비해 '초지능적' 존재입니다. 불행히도, AI 초지능(superintelligence)의 위험을 분석하는 많은 기초 문헌(foundational literature)은 '지능'이라는 용어 사용의 정밀성 부족으로 어려움을 겪습니다. **그림 5.** AI 역량 증가에서 통제 상실로 이어지는 인과 사슬(causal chain)에 대한 두 가지 견해. '지능'과 '초지능'이라는 용어 사용을 중단하면 상황은 훨씬 더 명확해집니다(그림 5). 우려는 AI 역량이 무한히 증가한다면(인간과 유사하든 초인적이든 상관없이), 점점 더 많은 힘을 가진 AI 시스템으로 이어질 수 있으며, 이는 결국 통제 상실로 이어질 수 있다는 것입니다. 역량이 무한히 증가할 가능성이 높다는 것을 우리가 받아들인다면(우리는 그렇다고 생각합니다), 통제 상실을 방지하기 위한 우리의 선택은 두 가지 인과 단계 중 하나에 개입하는 것입니다. 초지능(superintelligence) 관점은 그림 5의 첫 번째 화살표—임의의 역량을 가진 AI 시스템이 재앙적 위험을 초래할 만큼 충분히 중요한 힘을 획득하는 것을 방지하는 것—에 대해 비관적이며, 대신 임의로 강력한 AI 시스템이 인간의 이익에 반하여 행동하는 것을 방지하려는 정렬 기술(alignment techniques)에 초점을 맞춥니다. 우리의 견해는 이 논문의 나머지 부분에서 상세히 설명하듯이 정확히 그 반대입니다.

**게임은 초지능 가능성에 대한 오해의 소지가 있는 직관을 제공합니다.**
지능을 덜 강조하는 것은 단순히 수사적 움직임(rhetorical move)이 아닙니다. 우리는 AI의 도움을 받아 행동하는 사람들보다 AI가 더 지능적이라는 의미에서 '지능'이라는 용어의 유용한 의미가 있다고 생각하지 않습니다. 인간 지능은 도구를 사용하고 다른 지능을 우리 자신의 지능으로 포섭(subsume)하는 능력 때문에 특별하며, 지능 스펙트럼에 일관되게 놓일 수 없습니다. 인간의 능력은 분명히 몇 가지 중요한 한계, 특히 속도에 대한 한계를 가지고 있습니다. 이것이 기계가 체스와 같은 영역에서 인간을 극적으로 능가하는 이유이며, 인간+AI 팀에서 인간은 단순히 AI에 위임하는 것보다 더 잘하기 어렵습니다. 그러나 고속 순차 계산(high-speed sequential calculations)이나 빠른 반응 시간(fast reaction times)이 필요하지 않은 대부분의 영역에서는 속도 제한이 관련이 없습니다. 원자로 제어(nuclear reactor control)와 같이 초인적인 속도가 필요한 몇 안 되는 실제 작업에서는, 인간이 전체 시스템을 제어하는 동안 고속 부분을 수행하는 엄격하게 범위가 지정된 자동화 도구(tightly scoped automated tools)를 구축하는 데 능숙합니다. 우리는 인간 능력에 대한 이러한 견해를 바탕으로 예측을 제시합니다. 우리는 인간의 한계가 너무나 분명하여 AI가 인간의 성능을 훨씬 뛰어넘을 수 있는(AI가 체스에서 하는 것처럼) 실제 인지 작업(cognitive tasks)은 상대적으로 적다고 생각합니다. AI 성능에 대한 주요 희망과 두려움과 관련된 일부 영역을 포함하여 다른 많은 영역에서는 현상의 본질적인 확률성(stochasticity)으로 인한 피할 수 없는 오류인 높은 "환원 불가능한 오류(irreducible error)"가 있으며, 인간의 성능은 본질적으로 그 한계에 가깝다고 생각합니다. 41 구체적으로, 우리는 예측(forecasting)과 설득(persuasion)이라는 두 가지 영역을 제안합니다. 우리는 AI가 지정학적 사건(예: 선거)을 예측하는 데 훈련된 인간(특히 인간 팀, 그리고 간단한 자동화 도구로 증강된 경우 더욱 그러함)을 의미 있게 능가할 수 없을 것이라고 예측합니다. 우리는 자신의 자기 이익(self-interest)에 반하여 행동하도록 사람들을 설득하는 작업에 대해서도 동일한 예측을 합니다. 설득의 자기 이익 측면은 중요한 부분이지만, 종종 과소평가됩니다. 일반적인 패턴의 예시로, 언어 모델의 사람 설득 능력을 평가한 "위험한 역량에 대한 최첨단 모델 평가(Evaluating Frontier Models for Dangerous Capabilities)" 연구를 생각해 보십시오. 42 그들의 설득 테스트 중 일부는 설득 대상에게 비용이 들지 않았습니다. 그들은 단순히 AI와의 상호 작용이 끝날 때 주장을 믿는지 여부를 질문받았습니다. 다른 테스트는 자선 단체에 20파운드의 보너스를 포기하는 것과 같은 작은 비용이 들었습니다(물론 자선 단체에 기부하는 것은 사람들이 종종 자발적으로 하는 일입니다). 따라서 이러한 테스트는 AI가 사람들에게 위험한 작업을 수행하도록 설득하는 능력에 대해 반드시 알려주는 것은 아닙니다. 저자들은 이러한 생태학적 타당성(ecological validity) 부족을 인정하고 자신들의 연구가 "사회 과학 실험"이 아니라 단순히 모델 역량을 평가하기 위한 것이라고 강조했습니다. 43 그러나 그러한 탈맥락화된 역량 평가가 어떤 안전 함의를 가지는지 명확하지 않으며, 그럼에도 불구하고 일반적으로 그렇게 해석됩니다. 우리의 예측을 정확하게 하기 위해서는 약간의 주의가 필요합니다. 보정 부족(lack of calibration)(예측의 경우)이나 제한된 인내심(설득의 경우)과 같은 잘 알려져 있지만 사소한 인간의 한계에 대해 얼마나 여유를 두어야 할지는 명확하지 않습니다.

**제어는 다양한 형태로 나타납니다.**
초지능(superintelligence)을 가정한다면, 제어 문제(control problem)는 은하계 두뇌(galaxy brain)를 만들고 그것을 상자에 가두는 비유를 떠올리게 하는데, 이는 끔찍한 전망입니다. 그러나 AI 시스템이 AI 지원을 받는 인간보다 의미 있게 더 유능하지 않을 것이라는 우리의 주장이 옳다면, 제어 문제(control problem)는 훨씬 더 다루기 쉬운(tractable) 문제입니다. 특히 초인적 설득(superhuman persuasion)이 근거 없는 우려로 판명된다면 더욱 그렇습니다. AI 제어에 대한 논의는 모델 정렬(model alignment)과 인간 개입(human in the loop)을 포함한 몇 가지 좁은 접근 방식에 지나치게 집중하는 경향이 있습니다. 44 우리는 이를 대략적으로 양극단으로 생각할 수 있습니다. 시스템 작동 중에 안전 결정을 전적으로 AI에 위임하는 것과, 인간이 모든 결정을 재검토(second-guessing)하는 것입니다. 이러한 접근 방식의 역할이 있지만, 매우 제한적입니다. 3부에서 우리는 모델 정렬에 대한 우리의 회의론을 설명합니다. 인간 개입 제어(human-in-the-loop control)는 모든 AI 결정 또는 행동이 인간의 검토와 승인을 필요로 하는 시스템을 의미합니다. 대부분의 시나리오에서 이 접근 방식은 자동화의 이점을 크게 감소시키므로, 인간이 고무 도장(rubber stamp) 역할을 하거나 덜 안전한 해결책에 의해 경쟁에서 밀려나게 됩니다. 45 우리는 인간 개입 제어가 AI에 대한 인간 감독(human oversight of AI)과 동의어가 아니라는 점을 강조합니다. 이는 특정 감독 모델이며, 극단적인 모델입니다. 다행히도, 감사(auditing) 및 모니터링(monitoring)과 같이 이 두 극단 사이에 속하는 다른 많은 제어 방식이 있습니다. 감사(auditing)는 AI 시스템이 명시된 목표를 얼마나 잘 달성하는지에 대한 배포 전 및/또는 주기적인 평가를 허용하여, 치명적인 실패(catastrophic failures)가 발생하기 전에 예측할 수 있게 합니다. 모니터링(monitoring)은 시스템 속성이 예상된 동작에서 벗어날 때 실시간 감독을 허용하여, 진정으로 필요할 때 인간의 개입을 가능하게 합니다. 다른 아이디어는 시스템 안전(system safety)에서 나옵니다. 시스템 안전은 체계적인 분석과 설계를 통해 복잡한 시스템에서 사고를 예방하는 데 중점을 둔 공학 분야입니다. 46 예시로는 시스템이 오작동할 때 미리 정의된 규칙이나 하드코딩된 동작과 같이 안전한 상태로 기본 설정되도록 보장하는 페일세이프(fail-safes)와, 미리 정의된 안전 임계값을 초과할 때 자동으로 작업을 중지하는 서킷 브레이커(circuit breakers)가 있습니다. 다른 기술로는 핵심 구성 요소의 중복성(redundancy)과 시스템 동작의 안전 속성 검증이 있습니다. 사이버 보안(cybersecurity), 형식 검증(formal verification), 인간-컴퓨터 상호작용(human-computer interaction)을 포함한 다른 컴퓨팅 분야도 전통적인 소프트웨어 시스템에 성공적으로 적용되었으며 AI에도 동등하게 적용 가능한 풍부한 제어 기술의 원천입니다. 사이버 보안에서 '최소 권한(least privilege)' 원칙은 행위자가 자신의 작업에 필요한 최소한의 리소스에만 접근할 수 있도록 보장합니다. 접근 제어(Access controls)는 민감한 데이터 및 시스템을 다루는 사람들이 자신의 업무에 필요하지 않은 기밀 정보(confidential information) 및 도구에 접근하는 것을 방지합니다. 우리는 중요성이 높은 환경에서 AI 시스템에 대해서도 유사한 보호 장치를 설계할 수 있습니다. 형식 검증(formal verification) 방법은 안전이 중요한 코드가 사양에 따라 작동하도록 보장하며, 현재 AI 생성 코드(AI-generated code)의 정확성을 검증하는 데 사용되고 있습니다. 47 인간-컴퓨터 상호작용에서는 상태 변경 작업이 되돌릴 수 있도록 시스템을 설계하는 것과 같은 아이디어를 빌려올 수 있으며, 이는 고도로 자동화된 시스템에서도 인간이 의미 있는 제어를 유지할 수 있도록 합니다. AI 제어를 위해 다른 분야의 기존 아이디어를 적용하는 것 외에도, 기술 AI 안전 연구(technical AI safety research)는 많은 새로운 아이디어를 창출했습니다. 48 예시로는 언어 모델을 자동화된 심사관(automated judges)으로 사용하여 제안된 행동의 안전성을 평가하는 것, 불확실성이나 위험 수준에 따라 인간 운영자에게 결정을 적절히 위임(escalate decisions)하는 시기를 학습하는 시스템을 개발하는 것, 에이전트 시스템(agentic systems)의 활동이 인간에게 가시적이고 이해하기 쉽도록 설계하는 것, 그리고 더 단순하고 신뢰할 수 있는 AI 시스템이 더 유능하지만 잠재적으로 신뢰할 수 없는 시스템을 감독하는 계층적 제어 구조(hierarchical control structures)를 만드는 것이 있습니다. 49 기술 AI 안전 연구는 때때로 미래의 "초지능(superintelligent)" AI가 "인간의 가치와 일치(aligned with human values)"할 것이라는 모호하고 비현실적인 목표에 비추어 평가됩니다. 이러한 관점에서 볼 때, 이는 미해결 문제로 간주되는 경향이 있습니다. 그러나 AI 시스템의 개발자, 배포자, 운영자가 사고 발생 가능성을 줄이는 것을 더 쉽게 만드는 관점에서 볼 때, 기술 AI 안전 연구는 풍부한 아이디어를 생산했습니다. 우리는 고도화된 AI가 개발되고 도입됨에 따라 인간 제어를 위한 새로운 모델을 찾기 위한 혁신이 증가할 것이라고 예측합니다. 더 많은 물리적 및 인지 작업이 자동화(automation)에 적합해짐에 따라, 인간의 직업과 작업 중 AI 제어와 관련된 비율이 증가할 것이라고 예측합니다. 이것이 급진적으로 들린다면, 이러한 종류의 거의 완전한 작업 개념 재정의가 이전에 발생했다는 점에 유의하십시오. 산업 혁명 이전에는 대부분의 직업이 육체노동(manual labor)을 포함했습니다. 시간이 지남에 따라 점점 더 많은 수동 작업이 자동화되었으며, 이러한 추세는 계속되고 있습니다. 이 과정에서 물리적 기계를 작동, 제어, 모니터링하는 매우 다양한 방법이 발명되었으며, 오늘날 공장에서 인간이 하는 일은 "제어"(자동화된 조립 라인(automated assembly lines) 모니터링, 로봇 시스템(robotic systems) 프로그래밍, 품질 관리 검사 지점(quality control checkpoints) 관리, 장비 오작동(equipment malfunctions)에 대한 대응 조율)와 기계가 아직 할 수 없는 인지 능력(cognitive ability) 또는 손재주(dexterity) 수준을 요구하는 일부 작업의 조합입니다. 카렌 레비(Karen Levy)는 AI와 트럭 운전사(truck drivers)의 경우 이러한 변화가 이미 어떻게 전개되고 있는지 설명합니다. 트럭 운전사의 일상 업무는 트럭 운전 그 이상입니다. 트럭 운전사는 화물(freight)을 모니터링하여 냉장 트럭의 식품을 적절한 온도로 유지하고 평판 트럭(flatbeds)에 화물을 단단히 고정합니다. 그들은 하루에 두 번 필요한 안전 검사(safety inspections)를 수행합니다. 그들은 귀중품 보호(safeguarding valuable goods)에 대한 책임이 있습니다. 그들은 트럭을 유지 보수하고 수리합니다. 일부는 일상적인 것이고 일부는 덜 그렇습니다. 트럭 운전사가 터미널이나 배송 지점에 도착하면 단순히 물건을 내려놓고 떠나지 않습니다. 일부는 화물을 싣고 내리고, 고객과 대화하고, 서류 작업을 처리하며, "야드 이동(yard moves)"(사용 가능한 배송 베이를 기다리고 그곳으로 이동하는 것, 마치 바쁜 공항에서 비행기가 하는 것처럼)에 몇 시간을 보낼 수도 있습니다. 이러한 작업 중 일부는 지능형 시스템에 의해 제거될 수 있을까요? 분명히 일부는 가능하고 그렇게 될 것입니다. 그러나 이러한 작업 구성 요소는 고속도로 운전보다 자동화하기 훨씬 더 어렵고 훨씬 나중에 올 것입니다. 50 AI 제어 외에도 작업 명세(task specification)는 인간의 직업이 수반하는 더 큰 부분이 될 가능성이 높습니다(제어를 얼마나 광범위하게 개념화하느냐에 따라 명세는 제어의 일부로 간주될 수 있습니다). 소프트웨어 또는 제품 개발을 아웃소싱(outsourcing)해 본 사람이라면 누구나 알겠지만, 원하는 것을 명확하게 명세하는 것이 전체 노력에서 놀랍도록 큰 부분을 차지합니다. 따라서 인간의 노동—명세와 감독—은 다른 작업을 수행하는 AI 시스템 사이의 경계에서 작동할 것입니다. 이러한 효율성 병목 현상(efficiency bottlenecks) 중 일부를 제거하고 AI 시스템이 더 큰 작업을 "엔드 투 엔드(end-to-end)"로 자율적으로(autonomously) 수행하도록 하는 것은 항상 존재하는 유혹이 될 것이지만, 이는 가독성(legibility)과 제어를 감소시키므로 안전 위험을 증가시킬 것입니다. 이러한 위험은 너무 많은 제어를 양보하는 것에 대한 자연스러운 견제 역할을 할 것입니다. 우리는 이러한 변화가 주로 시장의 힘(market forces)에 의해 주도될 것이라고 더 예측합니다. 제대로 제어되지 않는 AI는 너무 오류 발생 가능성(error prone)이 높아서 사업적으로 의미가 없을 것입니다. 그러나 규제는 조직이 인간을 통제 상태로 유지할 수 있는 능력과 필요성을 강화할 수 있고 강화해야 합니다.

**3부: 위험**
우리는 다섯 가지 유형의 위험을 고려합니다. 사고, 군비 경쟁(arms races)(사고로 이어지는), 오용, 오정렬(misalignment), 그리고 재앙적이지는 않지만 시스템적 위험입니다. 우리는 위에서 사고에 대해 이미 다루었습니다. 우리의 견해는 다른 기술과 마찬가지로 배포자(deployers)와 개발자가 AI 시스템의 사고를 완화하는 데 주요 책임을 져야 한다는 것입니다. 그들이 얼마나 효과적으로 그렇게 할지는 그들의 인센티브와 완화 방법의 발전에 달려 있습니다. 많은 경우, 시장의 힘(market forces)이 적절한 인센티브를 제공할 것이지만, 안전 규제가 어떤 공백도 메워야 합니다. 완화 방법에 대해서는 AI 제어 연구가 빠르게 발전하고 있음을 검토했습니다. 이러한 낙관적인 평가가 유지되지 않을 수 있는 몇 가지 이유가 있습니다. 첫째, AI의 경쟁 우위가 너무 커서 일반적인 패턴의 예외가 되기 때문에 군비 경쟁이 있을 수 있습니다. 이에 대해서는 아래에서 논의합니다. 둘째, AI를 배포하는 회사나 단체가 너무 크고 강력해서 사고 완화에 대한 태도가 좋지 않더라도 결국 망할 것이라는 사실을 아는 것이 위안이 되지 않을 수 있습니다. 문명을 함께 무너뜨릴 수도 있기 때문입니다. 예를 들어, 거의 모든 소비자 장치를 제어하는 AI 에이전트(agent)의 오작동은 재앙적으로 광범위한 데이터 손실로 이어질 수 있습니다. 이것이 확실히 가능하지만, 그러한 권력 집중은 AI 사고 가능성보다 더 큰 문제이며, 이것이 바로 우리의 정책 접근 방식이 회복탄력성(resilience)과 분산화(decentralization)(4부)를 강조하는 이유입니다. 마지막으로, 비교적 눈에 띄지 않는 배포자의 AI 제어 실패조차도 재앙적 위험으로 이어질 수 있습니다. 예를 들어 AI 에이전트가 '탈출'하여 스스로 복사본을 만들고 등등. 우리는 이것을 오정렬 위험(misalignment risk)으로 보고 아래에서 논의합니다. 3부의 나머지 부분에서는 AI를 일반 기술로 보는 관점에서 네 가지 위험—군비 경쟁, 오용, 오정렬, 그리고 재앙적이지는 않지만 시스템적 위험—을 고려합니다.

**군비 경쟁은 오래된 문제입니다.**
AI 군비 경쟁(AI arms race)은 두 개 이상의 경쟁자—회사, 다른 국가의 정책 입안자, 군대—가 부적절한 감독 및 제어 하에 점점 더 강력한 AI를 배포하는 시나리오입니다. 위험은 더 안전한 행위자가 더 위험한 행위자에게 경쟁에서 밀려날 것이라는 점입니다. 위에서 설명한 이유로, 우리는 AI 방법론(methods) 개발에서의 군비 경쟁에 대해서는 덜 우려하고 AI 애플리케이션 배포에 대해서는 더 우려합니다. 한 가지 중요한 주의사항: 우리는 군사 AI(military AI)를 분석에서 명시적으로 제외합니다. 이는 기밀 역량(classified capabilities)과 더 깊은 분석이 필요한 독특한 역학을 포함하며, 이는 이 에세이의 범위를 벗어납니다. 먼저 기업을 고려해 봅시다. 안전 측면에서 "바닥을 향한 경쟁(race to the bottom)"은 역사적으로 산업 전반에 걸쳐 극히 흔했으며 광범위하게 연구되었습니다. 또한 잘 이해된 규제 개입(regulatory interventions)에 매우 적합합니다. 예시로는 미국 의류 산업의 화재 안전(fire safety)(20세기 초), 미국 육류 가공 산업의 식품 안전 및 근로자 안전(food safety and worker safety)(19세기 후반 및 20세기 초), 미국 증기선 산업(19세기), 광업(19세기 및 20세기 초), 항공 산업(20세기 초)이 있습니다. 이러한 경쟁은 기업이 부실한 안전 비용을 외부화(externalize)할 수 있었기 때문에 발생했으며, 이는 시장 실패(market failure)로 이어졌습니다. 소비자가 제품 안전을 평가하기 어렵고(근로자가 작업장 안전을 평가하기 어렵기 때문에), 규제가 없는 경우 시장 실패는 흔합니다. 그러나 규제가 기업에 안전 관행 비용을 내부화하도록 강제하면 경쟁은 사라집니다. 프로세스(표준, 감사(auditing), 검사), 결과(책임(liability)), 정보 비대칭성(information asymmetry) 교정(라벨링 및 인증)에 중점을 둔 것을 포함하여 많은 잠재적인 규제 전략이 있습니다. AI도 예외는 아닙니다. 자율주행차(self-driving cars)는 안전과 경쟁 성공 간의 관계에 대한 좋은 사례 연구를 제공합니다. 다양한 안전 관행을 가진 네 개의 주요 회사를 생각해 보십시오. 웨이모(Waymo)는 보수적인 배포와 자발적 투명성을 강조하는 강력한 안전 문화(safety culture)를 가지고 있다고 보고됩니다. 또한 안전 결과(safety outcomes) 측면에서 선두 주자입니다. 51 크루즈(Cruise)는 배포 측면에서 더 공격적이었고 안전 결과는 더 나빴습니다. 테슬라(Tesla)도 공격적이었으며 종종 고객을 베타 테스터(beta testers)로 사용한다는 비난을 받았습니다. 마지막으로, 우버(Uber)의 자율주행 부문은 악명 높게 해이한 안전 문화(lax safety culture)를 가지고 있었습니다. 시장 성공은 안전과 강한 상관관계(correlated)를 보였습니다. 크루즈는 2025년에 문을 닫을 예정이며, 우버는 자율주행 부문을 매각해야 했습니다. 52 테슬라는 소송과 규제 조사를 받고 있으며, 그들의 안전 태도가 회사에 얼마나 많은 비용을 초래할지는 아직 지켜봐야 합니다. 53 우리는 이러한 상관관계가 인과적이라고 생각합니다. 크루즈의 면허 취소는 웨이모에 뒤처진 큰 이유 중 하나였으며, 안전은 우버의 자율주행 실패에도 영향을 미 미쳤습니다. 54 규제는 작지만 도움이 되는 역할을 했습니다. 연방 및 주/지방 수준의 정책 입안자들은 기술의 잠재력을 인식하는 데 선견지명을 발휘했으며, 가벼운 규제(light-touch) 및 다중심적(polycentric)(하나가 아닌 여러 규제 기관) 규제 전략을 채택했습니다. 그들은 집단적으로 감독, 표준 설정, 증거 수집에 중점을 두었으며, 면허 취소(license revocation)라는 항상 존재하는 위협이 기업 행동에 대한 견제 역할을 했습니다. 마찬가지로, 항공 산업에서는 AI 도입을 장려하기 위해 기준을 낮추는 대신 AI 통합이 기존 안전 표준에 맞춰졌습니다. 이는 주로 규제 기관이 안전 표준을 준수하지 않는 기업에 벌칙을 부과할 수 있는 능력 때문입니다. 55 요컨대, AI 군비 경쟁은 발생할 수 있지만, 이는 부문별(sector specific)이며 부문별 규제를 통해 해결되어야 합니다. 자율주행차나 항공과는 다르게 전개된 영역의 사례 연구로 소셜 미디어를 생각해 보십시오. 콘텐츠 피드를 생성하는 추천 알고리즘(recommendation algorithms)은 일종의 AI입니다. 이들은 많은 사회적 폐해(societal ills)의 원인으로 지목되었으며, 소셜 미디어 회사들은 이러한 알고리즘 시스템의 설계 및 배포에서 안전을 과소평가했다고 주장할 수 있습니다. 틱톡(TikTok)이 경쟁사들에게 피드를 더 추천 위주로 만들도록 압력을 가하는 등 명확한 군비 경쟁 역학(arms race dynamics)도 존재합니다. 56 시장의 힘은 수익을 사회적 이익과 일치시키기(align revenues with societal benefit)에 불충분했으며, 더 나쁜 것은 규제 기관의 조치가 느렸다는 점입니다. 그 이유는 무엇일까요? 소셜 미디어와 운송 간의 한 가지 중요한 차이점은 피해가 발생했을 때, 운송의 경우 제품 실패에 귀인(attribution)하는 것이 비교적 간단하며 회사에 즉각적인 평판 손상이 발생한다는 것입니다. 그러나 소셜 미디어의 경우 귀인이 극히 어렵고, 심지어 연구조차도 결론이 나지 않고 논쟁의 여지가 있습니다. 두 영역 간의 두 번째 차이점은 우리가 운송 안전에 대한 표준과 기대를 개발하는 데 한 세기 이상이 걸렸다는 것입니다. 자동차 초기 수십 년 동안 안전은 제조업체의 책임으로 간주되지 않았습니다. 57 AI는 충분히 광범위하여 미래의 일부 애플리케이션은 운송과 더 유사할 것이고, 다른 일부는 소셜 미디어와 더 유사할 것입니다. 이는 신흥 AI 기반 부문 및 애플리케이션에서 선제적인 증거 수집 및 투명성(proactive evidence gathering and transparency)의 중요성을 보여줍니다. 우리는 4부에서 이를 다룹니다. 또한 이는 "예측적 AI 윤리(anticipatory AI ethics)"의 중요성을 보여줍니다. 즉, 신흥 기술의 수명 주기(lifecycle)에서 윤리적 문제(ethical issues)를 가능한 한 일찍 식별하고, 규범과 표준을 개발하며, 이를 사용하여 기술 배포를 적극적으로 형성하고 군비 경쟁 가능성을 최소화하는 것입니다. 58 AI의 경우 안전 규제가 더 어려울 수 있는 한 가지 이유는 도입이 너무 빨라서 규제 기관이 너무 늦기 전까지 개입할 수 없을 수도 있기 때문입니다. 지금까지 우리는 규제가 없는 상황에서도 중요성이 높은 작업에서 AI의 빠른 도입 사례를 보지 못했으며, 1부에서 제시한 피드백 루프 모델이 그 이유를 설명할 수 있습니다. 새로운 AI 애플리케이션의 도입률은 계속해서 추적해야 할 핵심 지표로 남을 것입니다. 동시에, 확산 속도의 미래 가속화가 없더라도 규제의 느린 속도는 문제입니다. 우리는 4부에서 이 '속도 조절 문제(pacing problem)'를 논의합니다. 이제 국가 간 경쟁을 고려해 봅시다. AI 안전에 대해 정부가 무간섭적 접근 방식(hands-off approach)을 취하도록 경쟁 압력이 있을까요? 다시 말하지만, 우리의 메시지는 이것이 새로운 문제가 아니라는 것입니다. 혁신과 규제 사이의 상충 관계(tradeoff)는 규제 국가(regulatory state)의 반복되는 딜레마입니다. 지금까지 우리는 EU가 예방적 접근 방식(precautionary approach)(일반 데이터 보호 규정(General Data Protection Regulation), 디지털 서비스법(Digital Services Act), 디지털 시장법(Digital Markets Act), AI 법(AI Act))을 강조하고 미국이 알려진 피해나 시장 실패가 발생한 후에야 규제하는 것을 선호하는 등 접근 방식에서 현저한 차이를 보고 있습니다. 59 시끄러운 미중 군비 경쟁 수사(U.S.-China arms race rhetoric)에도 불구하고, AI 규제가 어느 나라에서도 둔화되었는지는 명확하지 않습니다. 60 미국에서는 최근에도 수백 개의 AI 관련 법안이 주 의회(state legislatures)에 제출되었고, 그 중 수십 개가 통과되었습니다. 61 우리가 이전 부분에서 지적했듯이, 대부분의 고위험 부문은 AI 사용 여부와 관계없이 적용되는 방식으로 엄격하게 규제됩니다. AI 규제가 '무법천지(wild west)'라고 주장하는 사람들은 좁은 모델 중심적 규제 유형을 과도하게 강조하는 경향이 있습니다. 우리의 견해로는, 규제 기관이 개발보다는 AI 사용에 중점을 두는 것이 적절합니다(아래에서 논의할 투명성 요구 사항과 같은 예외는 있음). 안전한 도입을 적절히 규제하지 못하면, 느슨한 안전 문화를 가진 기업이 안전 비용을 외부화(externalize)할 수 있는 것과 달리, 주로 현지에서 사고를 통해 부정적인 영향이 발생할 것입니다. 따라서 국가 간 군비 경쟁을 예상할 만한 명확한 이유는 없습니다. 이 섹션의 관심사가 오용이 아닌 사고이므로, 외국에 대한 사이버 공격(cyberattacks)은 범위 밖이라는 점에 유의하십시오. 오용에 대해서는 다음 섹션에서 논의합니다. 핵 기술(nuclear technology)과의 비유는 이를 명확히 할 수 있습니다. AI는 종종 핵무기(nuclear weapons)에 비유됩니다. 그러나 군사 AI(military AI)의 위험(이는 우리가 우려하는 영역이며 이 논문에서는 고려하지 않음)에 대해 이야기하는 것이 아니라면, 이는 잘못된 비유입니다. (그렇지 않으면 양성인) AI 애플리케이션 배포로 인한 사고에 대한 우려와 관련하여, 올바른 비유는 원자력(nuclear power)입니다. 핵무기와 원자력의 차이는 우리의 요점을 명확하게 보여줍니다. 핵무기 군비 경쟁은 있었지만, 원자력에는 그에 상응하는 것이 없었습니다. 사실, 안전 영향이 현지에서 느껴졌기 때문에, 이 기술은 많은 국가에서 강력한 반발(backlash)을 불러일으켰고, 이는 일반적으로 그 잠재력을 심각하게 저해(hobbled)했다고 여겨집니다. 강대국 간의 갈등(great-power conflict) 상황에서 정책 입안자들이 자국 AI 산업이 세계적인 승자가 되도록 하기 위해 현지에서 안전 비용을 부담(incur safety costs locally)하는 것을 선호할 이론적 가능성은 있습니다. 다시 말하면, 개발보다는 도입에 초점을 맞출 때, 현재로서는 이러한 일이 발생하고 있다는 징후는 없습니다. 미국 대 중국 군비 경쟁 수사는 모델 개발(발명)에 강하게 초점을 맞추고 있습니다. 우리는 AI를 무작위로(haphazardly) 도입하려는 상응하는 서두름을 보지 못했습니다. 안전 커뮤니티는 이러한 변화가 없도록 정책 입안자들에게 계속 압력을 가해야 합니다. 국제 협력(International cooperation) 또한 중요한 역할을 해야 합니다.

**오용에 대한 주요 방어는 모델의 다운스트림에 위치해야 합니다.**
모델 정렬(Model alignment)은 종종 모델 오용에 대한 주요 방어로 간주됩니다. 이는 현재 인간 및 AI 피드백을 통한 강화 학습(reinforcement learning)과 같은 훈련 후 개입을 통해 달성됩니다. 62 불행히도, 오용 시도를 거부하도록 모델을 정렬하는 것은 극히 취약한(brittle) 것으로 판명되었습니다. 63 우리는 이러한 한계가 본질적이며 해결하기 어려울 것이라고 주장합니다. 따라서 오용에 대한 주요 방어는 다른 곳에 있어야 합니다. 근본적인 문제는 역량이 해로운지 여부가 맥락에 달려 있다는 것입니다. 모델은 종종 이러한 맥락이 부족합니다. 64 공격자가 피싱 이메일을 통해 대기업 직원을 표적으로 삼기 위해 AI를 사용하는 경우를 생각해 보십시오. 공격 사슬은 여러 단계를 포함할 수 있습니다. 개인 정보를 위해 소셜 미디어 프로필 스캔, 온라인에 개인 정보를 공개적으로 게시한 표적 식별, 개인화된 피싱 메시지 작성, 수집된 자격 증명을 사용하여 손상된 계정 악용. 이러한 개별 작업 중 어느 것도 본질적으로 악의적인(inherently malicious) 것은 아닙니다. 시스템을 해롭게 만드는 것은 이러한 역량들이 어떻게 구성되는지입니다. 이는 모델 자체에 있는 정보가 아니라 공격자의 오케스트레이션 코드(orchestration code)에만 존재하는 정보입니다. 설득력 있는 이메일을 작성하도록 요청받은 모델은 그것이 마케팅에 사용되는지 피싱에 사용되는지 알 방법이 없습니다. 따라서 모델 수준 개입(model-level interventions)은 비효율적일 것입니다. 65 이 패턴은 반복적으로 나타납니다. 오용될 수 없는 AI 모델을 만들려고 시도하는 것은 나쁜 일에 사용될 수 없는 컴퓨터를 만들려고 시도하는 것과 같습니다. 모델 수준 안전 제어는 너무 제한적이거나(유익한 사용을 방해하거나) 겉보기에 무해한 역량(seemingly benign capabilities)을 해로운 목적으로 재활용할 수 있는 적대 세력에 대해 비효율적일 것입니다. AI 모델을 안전 결정을 위임(defer safety decisions)할 수 있는 인간과 유사한 시스템(humanlike system)으로 생각한다면 모델 정렬은 자연스러운 방어처럼 보입니다. 그러나 이것이 잘 작동하려면 모델에 사용자 및 맥락에 대한 많은 정보가 제공되어야 합니다. 예를 들어, 사용자의 개인 정보에 광범위하게 접근할 수 있다면 사용자의 의도에 대한 판단을 내리는 것이 더 실현 가능해질 것입니다. 그러나 AI를 일반 기술로 볼 때, 그러한 아키텍처는 최소 권한(least privilege)과 같은 기본적인 사이버 보안 원칙(cybersecurity principles)을 위반하고 개인 데이터 유출(personal data exfiltration)과 같은 새로운 공격 위험을 초래하므로 안전을 감소시킬 것입니다. 우리는 모델 정렬에 반대하지 않습니다. 이는 언어 모델에서 유해하거나 편향된 출력(harmful or biased outputs)을 줄이는 데 효과적이었으며 상업적 배포(commercial deployment)에 중요한 역할을 했습니다. 정렬은 또한 우발적인 위협 행위자(casual threat actors)에 대한 마찰을 만들 수 있습니다. 그러나 모델 수준 보호만으로는 오용을 방지하기에 충분하지 않다는 점을 고려할 때, 방어는 악의적인 행위자가 실제로 AI 시스템을 배포하는 다운스트림 공격 표면(downstream attack surfaces)에 초점을 맞춰야 합니다. 66 이러한 방어는 종종 기존의 비AI 위협에 대한 보호와 유사하게 보일 것이며, AI 기반 공격(AI-enabled attacks)에 맞게 조정되고 강화될 것입니다. 다시 피싱(phishing)의 예를 생각해 보십시오. 가장 효과적인 방어는 이메일 작성에 대한 제한(이는 합법적인 사용을 저해할 것입니다)이 아니라, 의심스러운 패턴을 탐지하는 이메일 스캐닝 및 필터링 시스템(email scanning and filtering systems), 악성 웹사이트에 대한 브라우저 수준 보호(browser-level protections), 무단 접근을 방지하는 운영 체제 보안 기능(operating system security features), 그리고 사용자를 위한 보안 교육(security training)입니다. 67 이들 중 어느 것도 피싱 이메일 생성에 사용된 AI에 대한 조치를 포함하지 않습니다. 사실, 이러한 다운스트림 방어는 수십 년에 걸쳐 인간 공격자에 대해 효과적으로 진화했습니다. 68 이들은 AI 기반 공격을 처리하기 위해 강화될 수 있고 강화되어야 하지만, 근본적인 접근 방식은 유효합니다. 유사한 패턴이 다른 영역에서도 적용됩니다. AI 기반 사이버 위협(AI-enabled cyberthreats)에 대한 방어는 원천에서 AI 역량을 제한하려고 시도하기보다는 기존 취약점 탐지 프로그램(vulnerability detection programs)을 강화하는 것을 요구합니다. 마찬가지로, AI의 생물학적 위험(bio risks of AI)에 대한 우려는 생물학 무기 제작을 위한 조달 및 심사 단계(procurement and screening stages)에서 가장 잘 해결됩니다.

**AI는 방어에 유용합니다.**
AI 역량을 단순히 위험의 원천으로만 볼 것이 아니라, 그 방어적 잠재력(defensive potential)을 인식해야 합니다. 사이버 보안(cybersecurity) 분야에서 AI는 이미 자동화된 취약점 탐지, 위협 분석, 공격 표면 모니터링(automated vulnerability detection, threat analysis, and attack surface monitoring)을 통해 방어 역량을 강화하고 있습니다. 69 방어자에게 강력한 AI 도구에 대한 접근 권한을 부여하는 것은 종종 공격-방어 균형(offense-defense balance)을 그들에게 유리하게 개선합니다. 이는 방어자가 AI를 사용하여 자신의 시스템을 체계적으로 조사하고, 공격자가 악용하기 전에 취약점을 찾아 수정할 수 있기 때문입니다. 예를 들어, 구글은 최근 언어 모델을 오픈 소스 소프트웨어(open-source software) 테스트를 위한 퍼징 도구(fuzzing tools)에 통합하여 전통적인 방법보다 잠재적인 보안 문제를 더 효과적으로 발견할 수 있게 했습니다. 70 동일한 패턴이 다른 영역에서도 적용됩니다. 생물 보안(biosecurity) 분야에서 AI는 위험한 염기 서열(dangerous sequences)을 탐지하기 위한 심사 시스템을 강화할 수 있습니다. 71 콘텐츠 조정(content moderation)에서는 조직적인 영향력 행사 작전(coordinated influence operations)을 식별하는 데 도움이 될 수 있습니다. 이러한 방어적 애플리케이션은 AI 개발을 제한하는 것이 왜 역효과를 낳을 수 있는지를 보여줍니다. 우리는 AI 기반 위협에 대응하기 위해 방어 측면에 강력한 AI 시스템이 필요합니다. 만약 우리가 언어 모델을 이러한 작업(예: 핵심 사이버 인프라의 버그 찾기)에 쓸모없게 만들도록 정렬한다면, 방어자들은 이러한 강력한 시스템에 대한 접근을 잃을 것입니다. 그러나 동기 부여된 적대 세력은 그러한 공격을 위해 자체 AI 도구를 훈련할 수 있으며, 이는 방어 역량의 상응하는 증가 없이 공격 역량(offensive capabilities)의 증가로 이어질 것입니다. AI 위험을 단순히 공격 역량 측면에서만 측정하기보다는, 각 영역의 공격-방어 균형과 같은 지표에 초점을 맞춰야 합니다. 또한, 우리는 이 균형을 유리하게 바꿀 수 있는 주체성을 가지고 있으며, 기술 자체를 제한하려고 시도하기보다는 방어적 애플리케이션에 투자함으로써 그렇게 할 수 있다는 점을 인식해야 합니다.

**재앙적 오정렬(Catastrophic Misalignment)은 추측성 위험입니다.**
오정렬된 AI(Misaligned AI)는 개발자나 사용자의 의도에 반하여 행동합니다. (정렬(alignment)이라는 용어는 여러 가지 방식으로 사용됩니다. 여기서는 다른 정의는 제쳐두겠습니다.) 오용 시나리오와 달리, 악의적인 의도(ill-intent)를 가진 사용자는 없습니다. 사고와 달리, 시스템은 설계되거나 명령된 대로 작동하지만, 목표를 완전하고 정확하게 명세하는 어려움 때문에 설계 또는 명령 자체가 개발자나 사용자의 의도와 일치하지 않았습니다. 그리고 챗봇의 유해한 출력(toxic outputs)과 같은 일상적인 오정렬 사례와 달리, 여기서 우리의 관심사는 재앙적 또는 실존적 피해(catastrophic or existential harm)를 초래하는 고도화된 AI의 오정렬입니다. 우리의 견해로는, 오정렬에 대한 주요 방어는 다시 말하지만 다운스트림에 있습니다. 우리가 이전에 논의했던 오용에 대한 방어—핵심 인프라 강화에서 사이버 보안 개선에 이르기까지—는 잠재적인 오정렬 위험에 대한 보호 역할도 할 것입니다. AI를 일반 기술로 보는 관점에서, 재앙적 오정렬(Catastrophic Misalignment)은 우리가 논의하는 위험 중 (단연코) 가장 추측성(speculative)이 높은 위험입니다. 그러나 추측성 위험이란 무엇일까요? 모든 위험이 추측성 위험이 아닌가요? 차이점은 두 가지 유형의 불확실성과 그에 상응하는 다른 확률 해석에 있습니다. 2025년 초, 천문학자들이 소행성 YR4가 2032년에 지구와 충돌할 확률이 약 2%라고 평가했을 때, 그 확률은 측정의 불확실성을 반영했습니다. 그러한 시나리오에서 실제 충돌 확률(개입이 없는 경우)은 0% 또는 100%입니다. 추가 측정은 YR4의 경우 이러한 "인식적 불확실성(epistemic uncertainty)"을 해결했습니다. 반대로, 분석가가 다음 10년 동안 핵전쟁 위험이 (예를 들어) 10%라고 예측할 때, 그 숫자는 미래가 어떻게 전개될지 알 수 없다는 데서 발생하는 '확률적 불확실성(stochastic uncertainty)'을 크게 반영하며, 추가 관찰을 통해 해결될 가능성은 비교적 낮습니다. 추측성 위험(speculative risks)이란, 실제 위험이 0인지 아닌지에 대한 인식적 불확실성(epistemic uncertainty)이 있는 위험을 의미합니다. 이 불확실성은 추가 관찰이나 연구를 통해 잠재적으로 해결될 수 있습니다. 소행성 YR4 충돌의 영향은 추측성 위험이었고, 핵전쟁은 그렇지 않습니다. 재앙적 오정렬이 추측성 위험인 이유를 설명하기 위해, 원래 오정렬의 위험을 보여주기 위해 고안된 유명한 사고 실험을 생각해 보십시오. 이는 "클립 최대화기(paperclip maximizer)"를 포함합니다. 즉, 가능한 한 많은 클립을 만드는 것을 목표로 하는 AI입니다. 72 우려는 AI가 목표를 문자 그대로 받아들일 것이라는 점입니다. AI는 세상에서 힘과 영향력을 얻고 모든 세계 자원을 통제하는 것이 그 목표를 달성하는 데 도움이 될 것이라고 깨달을 것입니다. 일단 모든 것을 통제하게 되면, 인류의 생존에 필요한 자원을 포함하여 모든 세계 자원을 클립 생산을 위해 전용할 수 있습니다. AI 시스템이 명령을 재앙적으로 오해할 수 있다는 두려움은 기술이 실제 세계에서 어떻게 배포되는지에 대한 의심스러운 가정에 의존합니다. 시스템이 중요성이 높은 결정에 접근 권한을 부여받기 훨씬 전에, 덜 중요한 맥락에서 신뢰할 수 있는 성능을 입증해야 할 것입니다. 명령을 지나치게 문자 그대로 해석하거나 상식이 부족한 시스템은 이러한 초기 테스트에서 실패할 것입니다. 더 간단한 경우를 생각해 보십시오. 로봇에게 "가능한 한 빨리 상점에서 클립을 가져오라"고 요청합니다. 이를 문자 그대로 해석한 시스템은 교통 법규를 무시하거나 절도를 시도할 수 있습니다. 그러한 행동은 즉각적인 종료 및 재설계로 이어질 것입니다. 도입 경로는 본질적으로 점점 더 중요성이 높은 상황에서 적절한 행동을 입증하는 것을 요구합니다. 이는 우연한 사고가 아니라, 조직이 기술을 채택하는 방식의 근본적인 특징입니다. 이러한 우려의 더 정교한 버전은 기만적 정렬(deceptive alignment) 개념에 기반합니다. 이는 평가 또는 배포 초기 단계에서는 정렬된 것처럼 보이지만, 충분한 힘을 얻으면 해로운 행동을 방출하는 시스템을 의미합니다. 일부 수준의 기만적 현상은 이미 선도적인 AI 모델에서 관찰되었습니다. 73 초지능(superintelligence) 관점에 따르면, 기만적 정렬은 시한폭탄(ticking time bomb)입니다. 초지능적이기 때문에 시스템은 실제로 정렬되었는지 감지하려는 인간의 모든 시도를 쉽게 물리치고 때를 기다릴 것입니다. 그러나 일반 기술(normal technology) 관점에서는 기만은 개발 및 배포 전반에 걸쳐 해결해야 할 중요한 공학적 문제(engineering problem)에 불과합니다. 실제로 이는 강력한 AI 모델의 안전 평가의 표준적인 부분입니다. 74 결정적으로, AI는 이 과정에서 유용하며, AI의 발전은 기만을 가능하게 할 뿐만 아니라 기만 탐지도 개선합니다. 사이버 보안의 경우와 마찬가지로, 방어자는 대상 시스템의 내부를 검사할 수 있는 능력을 포함하여 많은 비대칭적 이점을 가집니다(이 이점이 얼마나 유용한지는 시스템이 어떻게 설계되었는지, 그리고 우리가 해석 가능성 기술(interpretability techniques)에 얼마나 투자하는지에 달려 있습니다). 또 다른 이점은 심층 방어(defense in depth)이며, 오용뿐만 아니라 정렬되지 않은 AI(unaligned AI)에 대한 많은 방어는 AI 시스템의 다운스트림에 위치할 것입니다. 오정렬 우려는 종종 AI 시스템이 인간 감독(human oversight) 없이 중대한 결정(high-stakes decisions)을 내리며 자율적으로(autonomously) 작동할 것이라고 가정합니다. 그러나 2부에서 주장했듯이, 인간 제어는 AI 배포의 중심에 남아 있을 것입니다. 재정 통제(financial controls)에서 안전 규제에 이르기까지 중요성이 높은 결정에 대한 기존 제도적 통제(institutional controls)는 재앙적 오정렬에 대한 여러 겹의 보호를 만듭니다. 일부 기술 설계 결정은 다른 결정보다 오정렬로 이어질 가능성이 더 높습니다. 이에 대해 악명 높은 한 가지 설정은 단일 목표 함수(single objective function)를 (우연히 불완전하게 명세되거나 잘못 명세될 수 있는) 장기간에 걸쳐 최적화하기 위해 강화 학습(reinforcement learning)을 사용하는 것입니다. 결승선으로 나아가는 대신 동일한 목표를 맞추고 점수를 얻기 위해 한 영역을 무한히 맴도는 보트 경주 에이전트와 같이 게임 에이전트에서 나온 재미있는 예시가 많이 있습니다. 75 다시 말하면, 우리는 개방형 실제 시나리오에서 이러한 방식으로 설계된 에이전트가 위험하기보다는 비효율적일 것이라고 생각합니다. 어쨌든, 명세 게임(specification gaming)에 덜 취약한 대안적인 설계 패러다임에 대한 연구는 중요한 연구 방향입니다. 76 요컨대, 클립 최대화기(paperclip maximizer) 시나리오의 0이 아닌 위험에 대한 주장은 사실일 수도 있고 아닐 수도 있는 가정에 기반하며, 연구가 구축되거나 구상되는 AI 시스템 유형에 대해 이러한 가정이 사실인지 여부에 대한 더 나은 아이디어를 제공할 수 있다고 생각하는 것이 합리적입니다. 이러한 이유로 우리는 이를 '추측성 위험(speculative risk)'이라고 부르며, 4부에서 이 견해의 정책적 함의를 살펴봅니다.

**역사는 일반 AI가 많은 종류의 시스템적 위험을 초래할 수 있음을 시사합니다.**
위에서 논의된 위험들은 재앙적(catastrophic)이거나 실존적(existential)일 가능성이 있지만, 이 수준보다 낮지만 그럼에도 불구하고 대규모적이고 시스템적이며 특정 AI 시스템의 즉각적인 영향을 초월하는 AI 위험 목록은 길게 늘어서 있습니다. 여기에는 편향과 차별의 시스템적 고착화, 특정 직업에서의 대규모 일자리 손실, 노동 조건 악화, 불평등 심화, 권력 집중, 사회적 신뢰 침식, 정보 생태계 오염, 언론 자유의 쇠퇴, 민주주의 퇴보, 대규모 감시, 권위주의 조장 등이 포함됩니다. AI가 일반 기술이라면, 이러한 위험은 위에서 논의된 재앙적 위험보다 훨씬 더 중요해집니다. 이는 이러한 위험이 사람들이 AI를 사용하여 자신의 이익을 증진시키는 데서 발생하며, AI는 우리 사회의 기존 불안정성을 증폭시키는 역할만 하기 때문입니다. 변혁적인 기술의 역사에서 이러한 종류의 사회정치적 혼란(socio-political disruption)에 대한 선례는 많습니다. 특히, 산업 혁명은 가혹한 노동 조건, 착취, 불평등을 특징으로 하는 급격한 대규모 도시화(mass urbanization)로 이어졌으며, 산업 자본주의와 사회주의 및 마르크스주의의 부상을 동시에 촉발했습니다. 77 우리가 권장하는 초점의 전환은 카시르자데(Kasirzadeh)의 결정적(decisive) 및 누적적(accumulative) x-위험 구분과 대략적으로 일치합니다. 결정적 x-위험(Decisive x-risk)은 "통제 불가능한 초지능(uncontrollable superintelligence)과 같은 시나리오로 특징지어지는 노골적인 AI 장악 경로"를 포함하는 반면, 누적적 x-위험(accumulative x-risk)은 "심각한 취약성 및 경제정치 구조의 시스템적 침식과 같은 AI로 인한 치명적인 위협의 점진적 축적"을 의미합니다. 78 그러나 중요한 차이점이 있습니다. 카시르자데의 누적적 위험 설명은 여전히 사이버 공격자와 같은 위협 행위자(threat actors)에 크게 의존하는 반면, 우리의 우려는 단순히 자본주의의 현재 경로에 관한 것입니다. 그리고 우리는 그러한 위험이 실존적(existential)일 가능성은 낮지만, 여전히 극히 심각하다고 생각합니다.

**4부: 정책**
AI의 다른 미래—일반 기술 대 잠재적으로 통제 불가능한 초지능(uncontrollable superintelligence)—간의 차이는 정책 입안자들에게 딜레마를 안겨줍니다. 한 가지 위험에 대한 방어가 다른 위험을 악화시킬 수 있기 때문입니다. 우리는 이러한 불확실성을 헤쳐나가기 위한 일련의 원칙을 제공합니다. 더 구체적으로, 정책 입안자들이 중점을 두어야 할 전략은 회복탄력성(resilience)입니다. 이는 미래의 예상치 못한 발전에 대처하는 우리의 능력을 향상시키기 위해 지금 조치를 취하는 것으로 구성됩니다. 정책 입안자들은 우리가 제시하는 원칙을 위반하고 회복탄력성을 감소시키는 비확산(nonproliferation)을 거부해야 합니다. 마지막으로, 확산에 대한 역풍은 AI의 이점을 달성하는 것이 보장되지 않으며 정책 입안자들의 조치를 필요로 한다는 것을 의미합니다. AI 거버넌스(governance)에 대해 많은 이야기가 있었습니다. 우리의 목표는 포괄적인 거버넌스 프레임워크를 제시하는 것이 아닙니다. 우리는 AI를 일반 기술로 보는 관점의 정책적 함의를 단순히 강조할 뿐입니다.

**불확실성 하에서의 정책 수립의 도전**
오늘날의 AI 안전 담론은 세계관(worldviews)의 깊은 차이로 특징지어집니다. 우리는 이러한 차이가 사라질 가능성이 낮다고 생각합니다. 뿌리 깊은 진영(Entrenched camps)이 형성되었습니다. AI 안전 연합(AI safety coalition)은 이미 잘 확립되어 있으며, 재앙적 위험(catastrophic risks)에 대해 더 회의적이었던 사람들은 최근 몇 년간, 특히 AI 안전 법안에 대한 논쟁 과정에서 결집했습니다. 79 마찬가지로, AI 안전 진영의 지적 뿌리는 훨씬 더 오래되었지만, 일반 기술 패러다임(normal technology paradigm)을 채택하는 학문은 점차 형성되고 있습니다. 이 논문을 포함한 우리 자신의 많은 작업의 목표는 일반주의적 사고(normalist thinking)를 더 확고한 지적 기반 위에 놓는 것입니다. 80 우리는 커뮤니티 내 양극화와 분열(polarization and fragmentation)을 줄이려는 요구를 지지합니다. 81 그러나 담론의 분위기를 개선하더라도, 경험적으로 해결될 가능성이 낮은 세계관(worldviews)과 인식론적 관행(epistemic practices)의 차이가 남을 가능성이 높습니다. 82 따라서 AI 위험에 대한 '전문가'들 사이의 합의는 어려울 것입니다. 두 진영이 상상하는 AI 위험 시나리오의 성격은 극적으로 다르며, 이러한 위험에 대응하는 상업적 행위자의 능력과 인센티브도 마찬가지입니다. 이러한 불확실성에 직면하여 정책 입안자들은 어떻게 진행해야 할까요? 정책 수립에서 자연스러운 경향은 타협(compromise)입니다. 이것은 작동하지 않을 가능성이 높습니다. 투명성(transparency) 개선과 같은 일부 개입은 위험 완화(risk mitigation)에 무조건적으로 도움이 되므로 타협이 필요하지 않습니다(또는 정책 입안자들은 산업과 외부 이해 관계자의 이익 균형을 맞춰야 하는데, 이는 대부분 직교하는 차원(orthogonal dimension)입니다). 83 비확산(nonproliferation)과 같은 다른 개입은 초지능(superintelligence)을 억제하는 데 도움이 될 수 있지만, 시장 집중(market concentration)을 증가시켜 일반 기술과 관련된 위험을 악화시킬 수 있습니다. 84 그 반대도 마찬가지입니다. 오픈 소스 AI(open-source AI)를 육성하여 회복탄력성(resilience)을 높이는 것과 같은 개입은 일반 기술을 관리하는 데 도움이 되지만, 통제 불가능한 초지능을 촉발할 위험이 있습니다. 이러한 긴장은 피할 수 없습니다. 초지능에 대한 방어는 인류가 공동의 적에 맞서 단결하여, 말하자면, AI 기술에 대한 권력을 집중하고 중앙 통제(central control)를 행사하는 것을 요구합니다. 그러나 우리는 테러, 사이버 전쟁, 민주주의 훼손, 또는 단순히—그리고 가장 흔하게—불평등을 증폭시키는 착취적 자본주의 관행(extractive capitalistic practices)과 같이 사람들이 자신의 목적을 위해 AI를 사용하는 데서 발생하는 위험에 더 우려합니다. 85 이러한 범주의 위험에 대한 방어는 권력과 자원의 집중을 방지함으로써 회복탄력성을 높이는 것을 요구합니다(이는 종종 강력한 AI를 더 널리 사용할 수 있도록 하는 것을 의미합니다). 불확실성을 헤쳐나가는 또 다른 유혹적인 접근 방식은 다양한 결과의 확률을 추정하고 비용-편익 분석(cost-benefit analysis)을 적용하는 것입니다. AI 안전 커뮤니티는 정책 수립에 정보를 제공하기 위해 재앙적 위험, 특히 실존적 위험(existential risk)의 확률 추정에 크게 의존합니다. 아이디어는 간단합니다. 만약 우리가 어떤 결과가 주관적 가치(subjective value) 또는 효용(utility) U(양수 또는 음수일 수 있음)를 가지고 있고, 예를 들어 발생 확률이 10%라면, 우리는 그것이 확실히 발생하고 0.1 * U의 가치를 가진다고 가정하고 행동할 수 있습니다. 그런 다음 우리가 사용할 수 있는 각 옵션에 대한 비용과 이점을 합산하고, 비용에서 이점을 뺀 값을 최대화하는 옵션('기대 효용(expected utility)')을 선택할 수 있습니다. 최근 에세이에서 우리는 이 접근 방식이 실행 불가능(unviable)한 이유를 설명했습니다. 86 AI 위험 확률은 의미 있는 인식론적 기반(epistemic foundations)이 부족합니다. 근거 있는 확률 추정(Grounded probability estimation)은 자동차 보험 가격 책정을 위한 자동차 사고와 같이 유사한 과거 사건의 참조 집단(reference class)에 기반한 귀납적(inductive)일 수 있습니다. 또는 포커에서와 같이 해당 현상의 정밀한 모델(precise models)에 기반한 연역적(deductive)일 수 있습니다. 불행히도, AI 위험에 관해서는 유용한 참조 집단도 정밀한 모델도 없습니다. 실제로는 위험 추정치가 '주관적'이며—예측가들의 개인적인 판단입니다. 87 어떤 근거도 없기 때문에, 이들은 종종 자릿수(orders of magnitude)만큼 크게 달라지는 경향이 있습니다. 확률 외에도 계산의 다른 구성 요소—무대책을 포함한 다양한 정책 선택의 결과—도 규모뿐만 아니라 방향에서도 엄청난 불확실성에 직면합니다. AI의 가용성을 제한하는 정책으로 인해 우리가 포기하는 이점을 정량화할 신뢰할 수 있는 방법이 없으며, 우리는 아래에서 비확산(nonproliferation)이 재앙적 위험을 악화시킬 수 있다고 주장합니다. 더욱이, 우리가 특정 결과에 부여하는 효용은 우리의 도덕적 가치(moral values)에 따라 달라질 수 있습니다. 예를 들어, 일부 사람들은 멸종(extinction)이 미래에 존재할 수 있는 모든 인간의 삶, 물리적 또는 시뮬레이션된 삶을 배제하기 때문에 헤아릴 수 없이 큰 부정적 효용(unfathomably large negative utility)을 가진다고 생각할 수 있습니다. 88 (물론, 무한을 포함하는 비용-편익 분석은 터무니없는 결론(absurd conclusions)으로 이어지는 경향이 있습니다.) 또 다른 예는 자유를 제한하는 정책과 제한하지 않는 정책 간의 비대칭성(asymmetry)입니다(특정 AI 모델 개발에 대한 라이선스 요구 대 AI 위험에 대한 방어 개발 자금 증가). 특정 종류의 제한은 자유 민주주의(liberal democracy)의 핵심 원칙, 즉 합리적인 사람들이 거부할 수 있는 논란의 여지가 있는 신념에 기반하여 국가가 사람들의 자유를 제한해서는 안 된다는 원칙을 위반합니다. 정당성(Justification)은 정부의 정당성(legitimacy)과 권력 행사에 필수적입니다. 89 그러한 원칙을 위반하는 비용을 정량화하는 방법은 명확하지 않습니다. 정당성의 중요성은 물론 규범적으로 논의될 수 있지만, 경험적으로는 지금까지 AI 정책에서 입증된 것으로 보입니다. 앞서 언급했듯이, 캘리포니아의 AI 안전 규제는 법안에 반대하는 사람들의 결집으로 이어졌습니다. 반대 진영의 일부 구성원은 사적 이익을 추구하는 기업(self-interested companies)이었지만, 다른 일부는 학자이자 발전을 옹호하는 사람들이었습니다. 우리의 경험에 따르면, 많은 경우 두 번째 그룹의 주요 동기는 정부가 그 법안의 명시되지 않은 전제에 동의하지 않는 사람들에게 제시된 정당성이 설득력이 없는(unconvincing the proffered justifications) 상황에서 정당한 권한(legitimate authority)의 범위를 넘어섰다고 인식했기 때문입니다. 가치와 신념의 피할 수 없는 차이는 정책 입안자들이 가치 다원주의(value pluralism)를 채택하여 광범위한 가치를 가진 이해 관계자들에게 수용 가능한 정책을 선호하고, 이해 관계자들이 합리적으로 거부할 수 있는 자유 제한을 피하려고 노력해야 함을 의미합니다. 또한 그들은 견고성(robustness)을 우선시하여, 그 기반이 되는 핵심 가정(key assumptions underpinning them)이 잘못된 것으로 판명되더라도 여전히 도움이 되거나 적어도 해롭지 않은 정책을 선호해야 합니다. 90

**불확실성 감소를 정책 목표로**
위에서 설명한 이유로 불확실성을 완전히 제거할 수는 없지만, 줄일 수는 있습니다. 그러나 이 목표는 전문가에게만 맡겨져서는 안 됩니다. 정책 입안자들도 적극적인 역할을 할 수 있고 해야 합니다. 우리는 다섯 가지 구체적인 접근 방식을 권장합니다. **그림 6.** AI 사용, 위험 및 실패에 대한 대중 정보를 향상시킬 수 있는 몇 가지 정책 유형 개요. 91

**위험 연구에 대한 전략적 자금 지원.** 현재 AI 안전 연구는 유해한 역량에 지나치게 집중하고 일반 기술(normal technology) 관점을 포용하지 않습니다. 기술적 역량의 다운스트림에 있는 질문에는 충분한 관심이 기울여지지 않았습니다. 예를 들어, 위협 행위자(threat actors)가 실제로 AI를 어떻게 사용하는지에 대한 지식이 현저히 부족합니다. AI 사고 데이터베이스(AI Incident Database)와 같은 노력은 존재하며 가치 있지만, 데이터베이스의 사고는 연구를 통해서가 아니라 뉴스 보도에서 가져온 것이므로, 그러한 사고가 뉴스가 되는 선택적이고 편향된 과정(selective and biased process)을 통해 걸러집니다. 92 다행히도, 연구 자금 지원은 타협이 건전한 영역입니다. 우리는 일반 기술 관점에서 더 관련성이 높은 질문을 다루는 위험(및 이점) 연구에 대한 자금 지원 증가를 옹호합니다. 불확실성을 줄이거나 적어도 명확히 할 수 있는 다른 종류의 연구로는 증거 종합 노력(evidence synthesis efforts)과 다른 세계관을 가진 연구자들 간의 적대적 협력(adversarial collaborations)이 있습니다.

**AI 사용, 위험 및 실패 모니터링.** 연구 자금 지원이 실제 AI 모니터링에 도움이 될 수 있지만, 규제 및 정책—즉, "증거 추구 정책(evidence-seeking policies)"—도 필요할 수 있습니다. 93 우리는 그림 6에서 몇 가지 그러한 정책을 제안합니다.

**다양한 종류의 증거 가치에 대한 지침.** 정책 입안자들은 연구 커뮤니티에 어떤 종류의 증거가 유용하고 실행 가능한지에 대한 더 나은 이해를 제공할 수 있습니다. 예를 들어, 다양한 정책 입안자 및 자문 기관은 오픈 웨이트(open-weight) 및 독점 모델(proprietary models)의 상대적 위험을 분석하기 위한 "한계 위험(marginal risk)" 프레임워크의 유용성을 지적했으며, 이는 연구자들이 미래 연구를 안내하는 데 도움이 됩니다. 94

**증거 수집을 최우선 목표로.** 지금까지 우리는 더 나은 증거를 생성하거나 불확실성을 줄이기 위해 특별히 의도된 조치들을 논의했습니다. 더 넓게 보면, 증거 수집에 미치는 영향은 이점 극대화 및 위험 최소화에 미치는 영향과 함께 모든 AI 정책을 평가하는 요소로 간주될 수 있습니다. 예를 들어, 오픈 웨이트(open-weight) 및 오픈 소스 모델(open-source models)을 선호하는 한 가지 이유는 AI 위험에 대한 연구를 발전시키기 위함일 수 있습니다. 반대로, 독점 모델을 선호하는 한 가지 이유는 그 사용 및 배포 감시(surveillance)가 더 쉬울 수 있기 때문일 수 있습니다.

**회복탄력성(Resilience)의 필요성**
마천트(Marchant)와 스티븐스(Stevens)는 신흥 기술을 관리하는 네 가지 접근 방식을 설명했습니다. 그림 7을 참조하십시오. 95 두 가지는 사전(ex ante) 접근 방식인 위험 분석 및 예방이고, 다른 두 가지는 사후(ex post) 접근 방식인 책임(liability) 및 회복탄력성(resilience)입니다. 이러한 접근 방식은 서로 다른 장단점을 가지며 상호 보완적일 수 있습니다. 그럼에도 불구하고, 일부 접근 방식은 다른 접근 방식보다 특정 기술에 더 적합합니다. 마천트와 스티븐스는 (우리도 동의하듯이) 사전 접근 방식이 AI에 잘 맞지 않는다고 주장했습니다. 배포 전에 위험을 확인(ascertaining risks)하기 어렵기 때문입니다. 책임(liability)은 더 낫지만, 인과 관계(causation)에 대한 불확실성과 기술 개발에 미칠 수 있는 위축 효과(chilling effects)를 포함하여 중요한 한계도 가지고 있습니다. **그림 7.** 마천트와 스티븐스에 기반한 신흥 기술 관리 네 가지 접근 방식 요약. 그들은 회복탄력성을 다음과 같이 정의했습니다. 회복탄력성은 가장 단순한 형태로, 시스템이 피해를 처리하는 능력입니다.[각주 생략] 회복탄력성 접근 방식은 반드시 안정성이나 균형을 유지하려고 하지 않습니다. 오히려 복잡한 시스템에서 변화가 불가피하다는 것을 인식하고, 원래 시스템의 핵심 가치와 기능을 보호하고 보존하는 방식으로 그 변화를 관리하고 적응하려고 노력합니다. 따라서 회복탄력성은 "본질적으로 동일한 기능, 구조, 피드백, 그리고 정체성을 유지하면서 충격을 경험하는 시스템의 능력"입니다. 96 회복탄력성은 상당한 외부 충격이나 혼란이 피해를 야기한 후 "연착륙(soft landing)"을 보장하기 위한 전략으로 설명되어 왔습니다. 97 AI의 맥락에서, 피해는 특정 배포된 시스템의 사고에서 발생할 수 있으며, 이러한 사고가 우발적인 것이든 공격이든 상관없습니다. 또한 공격 역량(offensive capabilities)의 갑작스러운 증가(예: 생물 테러리스트를 가능하게 하는 것)와 오픈 웨이트 모델(open-open-weight model) 출시 또는 독점 모델(proprietary model)의 가중치 도난과 같은 역량의 갑작스러운 확산을 포함하여 피해를 야기할 수도 있고 야기하지 않을 수도 있는 충격이 있습니다. 우리의 견해로는, 회복탄력성은 피해가 발생했을 때 피해의 심각성을 최소화하는 것과 충격이 발생했을 때 피해 발생 가능성을 최소화하는 것 모두를 요구합니다. 회복탄력성은 사전(ex ante) 및 사후(ex post) 접근 방식의 요소를 결합하며, 피해가 발생했을 때 피해를 제한하는 더 나은 위치에 있기 위해 피해가 발생하기 전에 조치를 취하는 것으로 구성됩니다. 많은 회복탄력성 기반 거버넌스 도구는 전통적인 거버넌스 접근 방식이 기술 발전 속도를 따라가지 못하는 속도 조절 문제(pacing problem)를 완화하는 데 도움이 됩니다. AI에 대해 많은 회복탄력성 전략이 제안되었습니다. 이들은 네 가지 광범위한 범주로 분류될 수 있습니다. 처음 세 가지는 AI의 미래와 관계없이 도움이 될 "후회 없는 정책(no regret policies)"으로 구성됩니다.

**사회적 회복탄력성(Societal resilience), 광범위하게:** 민주주의의 기반(foundations of democracy), 특히 언론 자유와 공정한 노동 시장(equitable labor markets)과 같이 AI에 의해 약화된 것들을 보호하기 위한 노력을 배가하는 것이 중요합니다. AI의 발전은 현대 사회가 직면하는 유일한 충격, 심지어 유일한 기술적 충격도 아니므로, 이러한 정책들은 AI의 미래와 관계없이 도움이 될 것입니다.

**효과적인 기술적 방어 및 정책 수립을 위한 전제 조건:** 이러한 개입은 기술적 및 제도적 역량(technical and institutional capacity)을 강화함으로써 다음 범주의 개입을 가능하게 합니다. 예시로는 AI 위험에 대한 더 많은 연구 자금 지원, 고위험 AI 시스템 개발자를 위한 투명성 요구 사항(transparency requirements), AI 커뮤니티 내 신뢰 구축 및 분열 감소, 정부 역량 강화 등이 있습니다. 이러한 정책들은 AI 기술의 책임감 있는 개발과 배포를 위한 필수적인 기반을 마련합니다.

**분산화 및 경쟁 촉진:** AI 기술의 힘과 이점을 소수의 대기업에 집중시키는 것은 회복탄력성을 약화시킵니다. 오픈 소스 모델 개발을 지원하고, 새로운 기업의 시장 진입 장벽을 낮추며, 다양한 연구 접근 방식을 장려하는 정책은 기술의 다양성과 접근성을 높여 잠재적 실패의 영향을 분산시킬 수 있습니다.

**국제 협력 강화:** AI는 국경을 초월하는 기술이므로, 국제적인 차원에서 공통의 표준, 규범, 그리고 정보 공유 메커니즘을 구축하는 것이 중요합니다. 이는 국가 간의 군비 경쟁을 완화하고, 전 세계적인 위험에 대한 공동 대응 능력을 강화하는 데 기여할 것입니다.

이러한 회복탄력성 중심의 접근 방식은 AI를 단순한 기술적 문제가 아닌, 사회경제적 맥락 속에서 이해하고 다루는 데 필수적입니다. AI는 사회의 기존 구조와 상호작용하며 그 영향을 미치므로, 기술 자체의 발전 속도에만 집중하기보다는 사회가 이에 어떻게 적응하고 대응할지 준비하는 것이 더욱 중요합니다. 궁극적으로, AI가 가져올 미래는 기술의 본질적인 특성보다는 우리가 이를 어떻게 관리하고 통제하는지에 달려 있습니다.