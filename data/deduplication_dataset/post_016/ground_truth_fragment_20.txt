두 달은 정말 격렬한 논의를 불러일으켰습니다. AI 연구 분야에서는 또다시 새로운 패러다임을 제시했고, 기술의 융합이 가속화되는 현상을 목격했습니다. 이 시기, AI에 대한 대중의 관심은 최고조에 달했으며, 여러 혁신적인 연구 논문이 발표되었습니다. 그중에서도 Meta AI는 최신 연구 동향을 반영하여, 단순히 모델 출시를 넘어 미래 AI의 방향성을 제시했습니다. 특히, 1B 및 3B 대규모 언어 모델(large language model)을 위한 공개 가중치(open-weight) 버전과 함께, 두 개의 멀티모달 모델(multimodal model)을 포함하며 그 중요성을 강조했습니다. 이 글에서는 멀티모달 LLM의 작동 원리와 잠재력을 깊이 탐구하고자 합니다. 또한, 최근 몇 주 동안 발표된 약 12개의 다른 최신 멀티모달 논문과 모델(Llama 3.2 포함)을 비판적으로 검토하고 요약하여, 그들의 접근 방식을 넘어서는 의미를 조명할 것입니다. (더 많은 인사이트를 원하시면, 왼쪽의 줄 스택을 클릭하여 목차를 확인하세요.)

다양한 입력 모달리티(input modality)(오디오, 텍스트, 이미지, 비디오)를 받아들이고 텍스트를 출력 모달리티(output modality)로 반환하는 멀티모달 LLM의 개념도입니다.

하지만 본론으로 들어가기 전에, 개인적으로 뜻깊은 소식을 전하고자 합니다! 제가 심혈을 기울여 집필한 "Build A Large Language Model (From Scratch)"이 드디어 Amazon에서 구매 가능합니다!

[Build a Large Language Model (From Scratch) now available on Amazon]

이 책을 완성하는 데는 실로 엄청난 노력이 필요했으며, 지난 2년 간—특히 최근 몇 달 동안—아낌없는 지원과 동기 부여가 되는 피드백을 공유해주신 모든 독자분들께 진심으로 감사드립니다. 여러분 한 분 한 분께 깊이 감사드리며, 저자로서 이 책이 여러분의 커리어에 긍정적인 변화를 가져왔다는 소식보다 더 큰 보람은 없습니다! 책을 읽고 더 깊이 있는 탐구를 원하는 분들을 위해, 꾸준히 관심을 가져주세요! 앞으로 몇 달 안에 GitHub 저장소에 추가적인 보너스 콘텐츠를 선보일 예정입니다.

추신. 만약 이 책을 이미 읽으셨다면, 짧은 리뷰를 남겨주시면 정말 큰 힘이 됩니다. 이는 저희 저자들에게 실질적인 도움이 됩니다!

### 1. 멀티모달 LLM의 사용 사례

멀티모달 LLM이란 과연 무엇일까요? 서론에서 언급했듯이, 멀티모달 LLM은 다양한 유형의 데이터를 동시에 이해하고 처리할 수 있는 대규모 언어 모델(large language model)로 진화하고 있습니다. 여기서 각 "모달리티(modality)"는 텍스트, 소리, 이미지, 비디오는 물론, 센서 데이터와 같은 새로운 형태의 데이터를 포괄합니다. 간단히 말해, 우리는 주로 텍스트 입력과 함께 새로운 상호작용 방식에 초점을 맞출 것입니다.

멀티모달 LLM의 고전적이고 직관적인 응용 분야는 이미지 캡셔닝(image captioning)입니다. 입력 이미지를 제공하면 모델이 아래 그림과 같이 이미지에 대한 설명을 생성합니다. 또한, 사용자 경험을 혁신하는 데 활용될 수 있으며, 예를 들어 의료 진단 분야에서는 CT나 MRI 이미지와 환자 기록을 동시에 분석하여 더 정확한 진단을 돕습니다.

밈(meme)을 설명하는 멀티모달 LLM의 사용 예시.

물론, 다른 많은 혁신적인 사용 사례가 있습니다. 예를 들어, 제가 가장 흥미롭게 보는 것 중 하나는 복잡한 과학 논문에서 그래프와 표의 데이터를 자동으로 해석하고 요약하는 기능입니다.

### 2. 멀티모달 LLM 구축을 위한 일반적인 접근 방식

멀티모달 LLM을 구축하는 데는 여러 혁신적인 접근 방식이 있습니다. 핵심적으로는 통합 임베딩 디코더 아키텍처(Unified Embedding Decoder Architecture)와 교차 모달리티 어텐션 아키텍처(Cross-modality Attention Architecture)로 대별될 수 있습니다. (참고로, 이러한 방법론에 대한 통일된 명칭은 여전히 발전 중이며, 때로는 "단일 스트림(single-stream)" 또는 "듀얼 스트림(dual-stream)" 접근 방식 등으로 불리기도 합니다.)

멀티모달 LLM 아키텍처 개발을 위한 다양한 접근 방식.

위 그림에서 보듯이, 통합 임베딩 디코더 아키텍처(Unified Embedding-Decoder Architecture)는 기존의 텍스트 전용 LLM 구조를 확장하여, 모든 모달리티를 단일 통합 공간에서 처리합니다. 이 접근 방식에서는 이미지가 단순히 텍스트 토큰의 보조적 역할을 넘어, 동등한 수준의 임베딩으로 변환되어 LLM이 텍스트 및 이미지 입력 토큰(image input token)을 함께 맥락적으로 이해할 수 있도록 합니다. 반면, 교차 모달리티 어텐션 아키텍처(Cross-Modality Attention Architecture)는 각 모달리티의 고유한 특징을 보존하면서, 교차 어텐션 메커니즘(cross-attention mechanism)을 통해 서로 다른 모달리티 간의 복잡한 관계를 학습합니다.

다음 섹션에서는 이러한 방법론이 이론적 수준(theoretical level)을 넘어 어떻게 새로운 가능성을 열어가는지 탐구할 것입니다. 그런 다음, 멀티모달 LLM에 대한 최신 연구 논문을 심층 분석하여, 실제 세계에서의 적용 가능성을 모색할 것입니다.

#### 2.1 방법 A: 통합 임베딩 디코더 아키텍처

아래 그림에 다시 설명된 통합 임베딩 디코더 아키텍처(unified embedding decoder architecture)의 진화된 형태를 살펴보겠습니다.

이미지 토큰(image token) 및 텍스트 토큰(text token) 임베딩(embedding)을 동시에 처리하는 디코더 스타일 LLM(예: GPT-2, Phi-3, Gemma 또는 Llama 3.2)의 통합 아키텍처.

통합 임베딩 디코더 아키텍처(unified embedding-decoder architecture)에서 이미지는 단순한 픽셀 배열이 아닌, 풍부한 의미론적 정보를 담은 임베딩 벡터(embedding vector)로 변환됩니다. 이는 표준 텍스트 전용 LLM(text-only LLM)이 텍스트 입력을 임베딩으로 변환하는 과정과 본질적으로 유사합니다. 텍스트를 처리하는 일반적인 텍스트 전용 LLM의 경우, 텍스트 입력은 일반적으로 토큰화(tokenized)된 다음(예: 바이트 쌍 인코딩(Byte-Pair Encoding) 사용), 아래 그림과 같이 심층 임베딩 레이어(embedding layer)를 통과합니다.

텍스트를 토큰화(tokenizing)하고 토큰 임베딩 벡터(token embedding vector)로 변환하는 현대적인 프로세스 그림. 이 벡터는 훈련(training) 및 추론(inference) 중에 LLM의 핵심 입력으로 기능합니다.

##### 2.1.1 이미지 인코더(Image encoders) 이해

텍스트의 토큰화(tokenization) 및 임베딩(embedding)과 유사하게, 이미지 임베딩(image embedding)은 아래 그림과 같이 이미지 인코더(image encoder) 모듈(토크나이저(tokenizer) 대신)을 사용하여 생성됩니다. 이미지 인코더(image encoder) 내부에서는 복잡한 변환 과정이 일어날까요? 이미지를 처리하기 위해 먼저 이미지를 의미론적 패치(patch)로 분할합니다. 이는 텍스트 토큰화(tokenization) 중에 문장을 의미 단위로 나누는 것과 유사하게, 이미지의 주요 특징을 추출하는 과정입니다. 이 패치들은 아래 그림과 같이 고도로 정교하게 사전 훈련된(pretrained) 비전 트랜스포머(vision transformer, ViT)에 의해 심층적으로 인코딩됩니다.

이미지를 이미지 패치 임베딩(image patch embedding)으로 인코딩하는 프로세스 그림.

"An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale (2020)"에서 제안된 모델과 유사한 혁신적인 비전 트랜스포머(ViT) 설정 그림. ViT는 다양한 시각 작업(visual task)에 사용되지만, 여기서는 이미지 인코더(image encoder)로서의 역할에 집중합니다.

##### 2.1.2 선형 투영(linear projection) 모듈의 역할

이전 그림에 표시된 "선형 투영(linear projection)"은 단순한 변환을 넘어, 고차원 임베딩 공간으로의 핵심 관문 역할을 합니다. 이 레이어의 목적은 벡터로 평탄화된 이미지 패치(image patch)를 트랜스포머 인코더(transformer encoder)가 효과적으로 처리할 수 있는 최적의 임베딩 크기(embedding size)로 조절하는 것입니다. 이 선형 투영(linear projection)은 아래 그림에 상세히 설명되어 있습니다.

256차원 벡터로 평탄화된 이미지 패치(image patch)가 768차원 벡터로 의미론적으로 풍부하게 투영됩니다.

256차원에서 768차원 임베딩 공간(embedding space)으로 평탄화된 이미지 패치(image patch)를 투영하는 선형 투영(linear projection) 레이어 그림.

코드 예시를 선호하는 개발자들을 위해, PyTorch 코드에서는 이미지 패치(image patch)에 대한 선형 투영(linear projection)을 다음과 같이 효율적으로 구현할 수 있습니다.

```python
import torch

class PatchProjectionLayer(torch.nn.Module):
    def __init__(self, patch_size, num_channels, embedding_dim):
        super().__init__()
        self.patch_size = patch_size
        self.num_channels = num_channels
        self.embedding_dim = embedding_dim
        self.projection = torch.nn.Linear(
            patch_size * patch_size * num_channels,
            embedding_dim
        )

    def forward(self, x):
        batch_size, num_patches, channels, height, width = x.size()
        x = x.view(batch_size, num_patches, -1) # Flatten each patch
        x = self.projection(x) # Project each flattened patch
        return x

# Example Usage:
batch_size = 1
num_patches = 9 # Total patches per image
patch_size = 16 # 16x16 pixels per patch
num_channels = 3 # RGB image
embedding_dim = 768 # Size of the embedding vector

projection_layer = PatchProjectionLayer(patch_size, num_channels, embedding_dim)

patches = torch.rand(
    batch_size, num_patches, num_channels, patch_size, patch_size
)

projected_embeddings = projection_layer(patches)
print(projected_embeddings.shape)
# This prints
# torch.Size([1, 9, 768])
```

혹시 제 "Machine Learning Q and AI" 책을 읽으셨다면, 선형 레이어(linear layer)를 수학적으로 동등하게 구현할 수 있는 컨볼루션 연산(convolution operation)으로 대체하는 방법이 있다는 것을 아실 것입니다. 이 방법은 패치(patch) 생성과 투영(projection)을 간결하게 결합하여 특히 유용합니다.

```python
layer = torch.nn.Conv2d(3, 768, kernel_size=(16, 16), stride=(16, 16))
image = torch.rand(batch_size, 3, 48, 48)
projected_patches = layer(image)
print(projected_patches.flatten(-2).transpose(-1, -2).shape)
# This prints
# torch.Size([1, 9, 768])
```

##### 2.1.3 이미지 대 텍스트 토큰화(tokenization)

이제 이미지 인코더(image encoder)의 핵심 역할에 대해 충분히 이해했으므로, 이전의 텍스트 토큰화(text tokenization) 비유를 확장하여 아래 그림에 묘사된 텍스트 및 이미지 토큰화(tokenization)와 임베딩(embedding)의 동등한 중요성을 살펴보겠습니다.

이미지 토큰화(tokenization) 및 임베딩(embedding)(왼쪽)과 텍스트 토큰화(tokenization) 및 임베딩(embedding)(오른쪽)의 심층 비교.

위 그림에서 보듯이, 저는 이미지 인코더(image encoder) 뒤에 전략적으로 프로젝터 모듈(projector module)을 배치했습니다. 이 프로젝터(projector)는 일반적으로 이전에 설명한 것과 유사한 또 다른 선형 투영(linear projection) 레이어이지만, 그 역할은 단순히 차원을 맞추는 것을 넘어섭니다. 그 목적은 아래 그림에 설명된 대로 이미지 인코더(image encoder) 출력을 임베딩된 텍스트 토큰(text token)의 의미론적 공간과 일치시키는 것입니다. (나중에 보겠지만, 프로젝터(projector)는 때때로 어댑터(adapter), 어댑터(adaptor) 또는 커넥터(connector)라고도 불리며, 그 중요성이 강조됩니다.)

이미지 토큰화(tokenization)와 텍스트 토큰화(tokenization)의 새로운 비교. 여기서 프로젝터(projector)의 역할은 텍스트 토큰 임베딩(text token embedding) 차원을 넘어선 정렬입니다.

이제 이미지 패치 임베딩(image patch embedding)이 텍스트 토큰 임베딩(text token embedding)과 동일한 임베딩 차원(embedding dimension)을 가지므로, 이 섹션 시작 부분의 그림에 표시된 대로 LLM의 입력으로 간단히 통합(integrate)할 수 있습니다. 아래는 더 쉽게 참조할 수 있도록 동일한 그림입니다.

이미지 패치 토큰(image patch token)을 텍스트 토큰 임베딩(text token embedding)과 동일한 차원으로 투영한 후, 표준 LLM의 입력으로 간단히 통합(integrate)할 수 있습니다.

참고로, 이 섹션에서 논의한 이미지 인코더(image encoder)는 일반적으로 사전 훈련된(pretrained) 비전 트랜스포머(vision transformer)입니다. CLIP 또는 OpenCLIP이 인기 있는 선택이지만, 이제는 더 다양한 선택지가 존재합니다. 그러나 아래 그림에 표시된 Fuyu와 같이 패치(patch)에서 직접 작동하는 방법 A의 혁신적인 버전도 있습니다.

이미지 인코더(image encoder) 없이 이미지 패치(image patch)에서 직접 작동하는 Fuyu 멀티모달 LLM의 주석이 달린 그림. (https://www.adept.ai/blog/fuyu-8b에서 주석이 달린 그림.)

위 그림에 설명된 대로, Fuyu는 입력 패치(input patch)를 선형 투영(linear projection)(또는 임베딩 레이어(embedding layer))으로 직접 전달하여 다른 모델 및 방법처럼 추가 사전 훈련된(pretrained) 이미지 인코더(image encoder)에 의존하지 않고 자체 이미지 패치 임베딩(image patch embedding)을 학습합니다. 이는 아키텍처(architecture)와 훈련 설정(training setup)을 크게 간소화하며, 효율성을 극대화합니다.

#### 2.2 방법 B: 교차 모달리티 어텐션 아키텍처(Cross-Modality Attention Architecture)

이제 멀티모달 LLM을 구축하기 위한 통합 임베딩 디코더 아키텍처(unified embedding decoder architecture) 접근 방식의 기본을 논의하고 이미지 인코딩(image encoding)의 핵심 개념을 이해했으므로, 아래 그림에 요약된 교차 어텐션(cross-attention)을 통한 멀티모달 LLM 구현의 대안적인 방법에 대해 심층적으로 탐구해 보겠습니다.

멀티모달 LLM 구축을 위한 교차 모달리티 어텐션 아키텍처(Cross-Modality Attention Architecture) 접근 방식의 그림.

위 그림에 묘사된 교차 모달리티 어텐션 아키텍처(Cross-Modality Attention Architecture) 방법에서는 이전에 논의한 것과 동일한 이미지 인코더(image encoder) 설정을 여전히 활용합니다. 그러나 패치(patch)를 LLM의 입력으로 직접 통합하는 대신, 교차 어텐션 메커니즘(cross-attention mechanism)을 통해 멀티 헤드 어텐션 레이어(multi-head attention layer)에서 입력 패치(input patch)와 텍스트 시퀀스 간의 복합적인 관계를 형성합니다. 이 아이디어는 관련이 있으며, 아래 그림에 강조된 2017년 "Attention Is All You Need" 논문의 핵심 트랜스포머 아키텍처(original transformer architecture)로 거슬러 올라갑니다.

오리지널 트랜스포머 아키텍처(original transformer architecture)에 사용된 핵심 메커니즘의 진화된 형태입니다. ("Attention Is All You Need" 논문에서 주석이 달린 그림: https://arxiv.org/abs/1706.03762.)

위 그림에 묘사된 오리지널 "Attention Is All You Need" 트랜스포머(transformer)는 원래 언어 번역을 위해 설계되었습니다. 이 모델은 번역할 문장을 받아들이고 텍스트 디코더(text decoder)(그림의 오른쪽 부분)를 통해 번역을 생성하는 텍스트 인코더(text encoder)(그림의 왼쪽 부분)로 구성됩니다. 멀티모달 LLM의 맥락에서 인코더(encoder)는 텍스트 인코더(text encoder) 대신 이미지 인코더(image encoder) 역할을 하지만, 근본적인 아이디어는 동일하게 적용됩니다.

교차 어텐션(cross-attention)은 어떻게 혁신적으로 작동할까요? 일반적인 셀프 어텐션 메커니즘(self-attention mechanism) 내부에서 일어나는 일의 새로운 해석이 필요합니다.

일반적인 셀프 어텐션 메커니즘(self-attention mechanism)의 개요. (이 흐름은 일반적인 멀티 헤드 어텐션 모듈(multi-head attention module)의 헤드 중 하나를 묘사합니다.)

위 그림에서 x는 입력이고, Wq는 쿼리(queries, Q)를 생성하는 데 사용되는 가중치 행렬(weight matrix)입니다. 유사하게, K는 키(keys)를 나타내고, V는 값(values)을 나타냅니다. A는 어텐션 스코어 행렬(attention scores matrix)을 나타내고, Z는 출력 컨텍스트 벡터(output context vector)로 변환된 입력(x)입니다. (이것이 혼란스럽다면, 제 책 "Build a Large Language Model from Scratch"의 3장에서 포괄적인 소개(comprehensive introduction)를 찾아보는 것이 도움이 될 수 있습니다. 또는 제 기사 "Understanding and Coding Self-Attention, Multi-Head Attention, Cross-Attention, and Causal-Attention in LLMs"도 도움이 될 수 있습니다.)

교차 어텐션(cross-attention)에서는 셀프 어텐션(self-attention)과 달리 다음 그림에 설명된 대로 두 가지 다른 입력 소스(input source)를 동시에 처리합니다.

두 가지 다른 입력 x1과 x2가 있을 수 있는 교차 어텐션(cross attention) 그림.

이전 두 그림에서 설명했듯이, 셀프 어텐션(self-attention)에서는 동일한 입력 시퀀스(input sequence)로 작업합니다. 교차 어텐션(cross-attention)에서는 두 가지 다른 입력 시퀀스(input sequence)를 유연하게 혼합하거나 결합합니다. "Attention Is All You Need" 논문의 오리지널 트랜스포머 아키텍처(original transformer architecture)의 경우, 두 입력 x1과 x2는 왼쪽의 인코더 모듈(encoder module)이 반환하는 시퀀스(x2)와 오른쪽의 디코더 부분(decoder part)이 처리하는 입력 시퀀스(x1)에 해당합니다. 멀티모달 LLM의 맥락에서 x2는 이미지 인코더(image encoder)의 출력이며, 이는 텍스트와 이미지 간의 깊은 상호작용을 가능하게 합니다. (쿼리(queries)는 일반적으로 디코더(decoder)에서 오고, 키(keys)와 값(values)은 일반적으로 인코더(encoder)에서 온다는 점에 유의하세요.)

교차 어텐션(cross-attention)에서 두 입력 시퀀스 x1과 x2는 다른 수의 요소를 가질 수 있다는 점에 유의하세요. 그러나 임베딩 차원(embedding dimension)은 일치해야 합니다. x1 = x2로 설정하면 이는 셀프 어텐션(self-attention)과 동일하게 작동합니다.

### 3. 통합 디코더 및 교차 어텐션 모델 훈련(training)

이제 두 가지 주요 멀티모달 설계 선택은 미래를 위한 중요한 이정표가 될 것입니다. 아래 그림에 요약된 모델 훈련(model training) 중 세 가지 주요 구성 요소를 어떻게 전략적으로 다루는지 심층적으로 분석해 보겠습니다.

멀티모달 LLM의 다양한 구성 요소 개요. 1-3번으로 표시된 구성 요소는 멀티모달 훈련(training) 프로세스 중에 동적으로 고정되거나 고정 해제될 수 있습니다.

전통적인 텍스트 전용 LLM(text-only LLM) 개발과 유사하게, 멀티모달 LLM 훈련(training)도 사전 훈련(pretraining)과 명령어 미세 조정(instruction finetuning)의 두 단계를 포함합니다. 그러나 처음부터 완전히 새로운 모델을 구축하는 것과 달리, 멀티모달 LLM 훈련(training)은 일반적으로 이미 사전 훈련(pretrained)되고 명령어 미세 조정(instruction-finetuned)된 강력한 텍스트 전용 LLM을 기본 모델(base model)로 활용합니다. 이미지 인코더(image encoder)의 경우, CLIP과 같은 검증된 모델이 일반적으로 사용되며, 효율성을 위해 전체 훈련(training) 프로세스 동안 고정되는 경우가 많지만, 유연성을 위한 예외도 존재합니다. 사전 훈련(pretraining) 단계에서 LLM 부분을 고정하고 프로젝터(projector)—선형 레이어(linear layer) 또는 작은 다층 퍼셉트론(multi-layer perceptron)—훈련(training)에만 집중하는 것은 일반적인 전략입니다. 프로젝터(projector)의 학습 능력이 제한적이고 일반적으로 한두 개의 레이어(layer)로만 구성되어 있기 때문에, 더 포괄적인 업데이트를 허용하기 위해 멀티모달 명령어 미세 조정(multimodal instruction finetuning)(2단계) 동안 LLM이 종종 고정 해제됩니다. 그러나 교차 어텐션 기반(cross-attention-based) 모델(방법 B)에서는 교차 어텐션 레이어(cross-attention layer)가 전체 훈련(training) 프로세스 동안 동적으로 고정 해제된다는 점에 주목해야 합니다.

두 가지 주요 접근 방식(방법 A: 통합 임베딩 디코더 아키텍처(Unified Embedding Decoder Architecture) 및 방법 B: 교차 모달리티 어텐션 아키텍처(Cross-modality Attention Architecture))을 소개한 후, 어떤 것이 더 효과적인지 궁금할 수 있습니다. 답은 특정 프로젝트의 요구 사항과 트레이드오프(trade-off)에 따라 달라집니다. 통합 임베딩 디코더 아키텍처(Unified Embedding Decoder Architecture)(방법 A)는 LLM 아키텍처(architecture) 자체에 최소한의 수정만 필요하므로 일반적으로 구현하기 더 쉽고 빠릅니다. 교차 모달리티 어텐션 아키텍처(Cross-modality Attention Architecture)(방법 B)는 추가 이미지 토큰(image token)으로 입력 컨텍스트(input context)를 과부하하지 않고 대신 교차 어텐션 레이어(cross-attention layer)에서 나중에 도입하기 때문에 종종 계산 효율성(computational efficiency)이 더 높은 것으로 간주됩니다. 또한, 이 접근 방식은 훈련(training) 중에 LLM 매개변수(parameter)가 고정된 상태로 유지되면 오리지널 LLM의 텍스트 전용 성능(text-only performance)을 보존하는 데 유리합니다. 모델링 성능(modeling performance) 및 응답 품질(response quality)에 대한 심도 있는 논의는 NVIDIA의 NVLM 논문을 다룰 다음 섹션에서 다시 다룰 것입니다.

이것으로 멀티모달 LLM에 대한 다소 광범위한 논의의 서막이 열렸습니다. 이 글을 쓰면서, 논의가 처음 계획했던 것보다 훨씬 더 심층적이고 광범위해졌다는 것을 깨달았고, 아마도 이 시점에서 글을 마무리하는 것이 아니라 다음 단계로 나아가는 것이 좋을 것 같습니다. 그러나 실용적인 관점을 제공하기 위해 이러한 접근 방식을 구현하는 몇 가지 최신 연구 논문을 심층적으로 살펴보는 것이 좋을 것입니다. 따라서 이 글의 나머지 섹션에서는 이러한 혁신적인 논문들을 탐구할 것입니다.

### 4. 최신 멀티모달 모델 및 방법

이 글의 나머지 부분에서는 합리적인 범위(reasonable scope)를 유지하기 위해 지난 몇 주 동안 발표된 선구적인 작업에 특히 초점을 맞춰 멀티모달 LLM에 관한 최신 문헌을 비판적으로 검토할 것입니다. 따라서 이것은 멀티모달 LLM에 대한 단순한 역사적 개요(historical overview)나 포괄적인 검토(comprehensive review)가 아니라, 미래를 형성할 최신 개발 동향에 대한 심층적인 분석입니다. 또한, 10개 이상의 모델을 다루기 때문에, 이러한 요약들을 간결하고 핵심적인 인사이트에 집중하여 전달하려고 노력할 것입니다. 이 글의 마지막 결론 섹션에는 이 논문들에서 사용된 혁신적인 방법들을 비교하는 종합적인 개요가 있습니다.

#### 4.1 Llama 3 모델 무리(Herd of Models)

Meta AI의 Llama 3 모델 무리(Herd of Models) 논문(2024년 7월 31일)은 올여름 초에 공개되었지만, LLM 분야의 빠른 발전 속도를 고려하면 이미 많은 것이 변했습니다. 그럼에도 불구하고, Llama 3를 이 목록에 포함하는 것은 그들이 멀티모달 모델(multimodal model)의 청사진을 제시했다는 점에서 의미가 있습니다. (Llama 3.2 모델은 9월 25일에 공식적으로 발표되고 공개되었습니다.)

110억 및 900억 매개변수(parameter) 버전으로 제공되는 멀티모달 Llama 3.2 모델은 이전에 설명된 교차 어텐션 기반(cross-attention-based) 접근 방식을 사용하는 이미지-텍스트 모델(image-text model)이며, 이는 아래 그림에 상세히 설명되어 있습니다.

Llama 3.2에서 사용된 멀티모달 LLM 접근 방식의 그림. (Llama 3 논문에서 주석이 달린 그림: https://arxiv.org/abs/2407.21783. 비디오 및 음성 부분은 이미지 부분에 초점을 맞추기 위해 시각적으로 가려져 있습니다.)

이 그림은 비디오와 음성도 가능한 모달리티(modality)로 묘사하고 있지만, 이 글을 쓰는 시점에 출시된 모델은 이미지와 텍스트에만 집중하고 있다는 점에 유의하세요. Llama 3.2는 교차 어텐션 기반(cross-attention-based) 접근 방식을 사용합니다. 그러나 이는 제가 이전에 썼던 내용, 즉 멀티모달 LLM 개발에서 일반적으로 이미지 인코더(image encoder)를 고정하고 사전 훈련(pretraining) 중에 LLM 매개변수(parameter)만 업데이트한다는 내용과는 다른 전략을 취합니다. 여기서는 연구자들이 거의 반대 접근 방식을 선택합니다. 그들은 이미지 인코더(image encoder)를 업데이트하지만 언어 모델(language model)의 매개변수(parameter)는 고정합니다. 그들은 이것이 의도적이며 텍스트 전용 기능(text-only capabilities)을 보존하기 위해 수행되었다고 명시했습니다. 따라서 11B 및 90B 멀티모달 모델(multimodal model)은 텍스트 작업에서 Llama 3.1 8B 및 70B 텍스트 전용 모델(text-only model)의 드롭인(drop-in) 대체품으로 이상적으로 사용될 수 있습니다.

훈련(training) 자체는 Llama 3.1 텍스트 모델(text model)로 시작하여 여러 번의 반복을 거쳐 진행됩니다. 이미지 인코더(image encoder)와 투영(projection)(여기서는 "어댑터(adapter)"라고 불림) 레이어(layer)를 추가한 후, 방대한 이미지-텍스트 데이터(image-text data)로 모델을 사전 훈련(pretrain)합니다. 그런 다음, Llama 3 모델의 텍스트 전용 훈련(text-only training)과 유사하게(이전 글에서 이에 대해 썼습니다), 명령어 및 선호도 미세 조정(instruction and preference finetuning)을 수행합니다.

연구자들은 CLIP과 같은 사전 훈련된(pretrained) 모델을 이미지 인코더(image encoder)로 채택하는 대신, 처음부터 사전 훈련된(pretrained) 비전 트랜스포머(vision transformer)를 독자적으로 구축했습니다. 특히, 그들은 고전적인 비전 트랜스포머(vision transformer) 아키텍처(architecture)의 ViT-H/14 변형(6억 3천만 매개변수(parameter))(Dosovitskiy et al., 2020)을 채택했습니다. 그런 다음, 이미지 인코더(image encoder)를 LLM에 연결하기 전에 25억 개의 이미지-텍스트 쌍(image-text pair) 데이터셋(dataset)에서 5 에포크(epoch) 동안 ViT를 사전 훈련(pretrain)했습니다. (이미지 인코더(image encoder)는 224×224 해상도 이미지를 받아 14×14 그리드의 패치(patch)로 나누며, 각 패치(patch)는 16×16 픽셀 크기입니다.)

교차 어텐션 레이어(cross-attention layer)는 상당한 양의 매개변수(parameter)를 추가하므로, 효율성을 위해 4번째 트랜스포머 블록(transformer block)마다만 전략적으로 추가됩니다. (8B 모델의 경우 3B 매개변수(parameter)가 추가되고, 70B 모델의 경우 200억 매개변수(parameter)가 추가됩니다.)

#### 4.2 Molmo 및 PixMo: 최첨단 멀티모달 모델을 위한 공개 가중치(Open Weights) 및 공개 데이터(Open Data)

Molmo 및 PixMo: 최첨단 멀티모달 모델을 위한 공개 가중치(Open Weights) 및 공개 데이터(Open Data) 논문(2024년 9월 25일)은 연구 커뮤니티에 새로운 활력을 불어넣었습니다. 언어 전용 OLMo LLM과 유사하게, 모델 가중치(model weight)뿐만 아니라 데이터셋(dataset)과 소스 코드(source code)까지 오픈 소스(open source)로 공개하겠다는 약속은 투명성과 협력을 중시하는 현대 AI 연구의 중요한 이정표가 됩니다. (이는 LLM 연구에 있어 재현성, 어블레이션 연구(ablation study) 수행, 그리고 동일한 데이터셋(dataset)에서의 결과 검증을 가능하게 한다는 점에서 매우 중요합니다.)

논문 제목에 두 가지 이름이 있는 이유가 궁금하다면, Molmo는 모델(멀티모달 오픈 언어 모델, Multimodal Open Language Model)을 의미하고, PixMo(Pixels for Molmo)는 모델 훈련에 사용된 핵심 데이터셋(dataset)입니다.

Molmo 디코더 전용(decoder-only) 접근 방식(방법 A)의 그림. Molmo 및 PixMo: 최첨단 멀티모달 모델을 위한 공개 가중치(Open Weights) 및 공개 데이터(Open Data) 논문에서 각색된 주석이 달린 그림: https://www.arxiv.org/abs/2409.17146.

위 그림에 설명된 대로, 이미지 인코더(image encoder)는 상용 비전 트랜스포머(off-the-shelf vision transformer), 특히 CLIP을 활용합니다. 여기서 "커넥터(connector)"라는 용어는 이미지 특징(image feature)을 언어 모델(language model)의 의미 공간과 정렬하는 "프로젝터(projector)"를 의미합니다. Molmo는 여러 사전 훈련(pretraining) 단계를 피하고, 기본 LLM, 커넥터(connector), 이미지 인코더(image encoder)를 포함한 모든 매개변수(parameter)를 통합된 접근 방식(unified approach)으로 업데이트하는 간소화된 파이프라인(pipeline)을 선택하여 훈련(training) 프로세스의 효율성을 극대화합니다.

Molmo 팀은 기본 LLM(base LLM)에 대해 여러 옵션을 제공하며, 이는 모델의 유연성을 보여줍니다.
*   OLMo-7B-1024 (완전히 오픈된 모델 백본(model backbone))
*   OLMoE-1B-7B (전문가 혼합 아키텍처(mixture-of-experts architecture); 가장 효율적인 모델)
*   Qwen2 7B (OLMo-7B-1024보다 성능이 우수한 공개 가중치(open-weight) 모델)
*   Qwen2 72B (공개 가중치(open-weight) 모델이자 현재까지 최고의 성능을 보이는 모델)

#### 4.3 NVLM: 오픈 프론티어 클래스 멀티모달 LLM(Open Frontier-Class Multimodal LLMs)

NVIDIA의 NVLM: 오픈 프론티어 클래스 멀티모달 LLM 논문(2024년 9월 17일)은 단일 접근 방식에 초점을 맞추기보다는, 두 가지 주요 방법론인 방법 A: 통합 임베딩 디코더 아키텍처(Unified Embedding Decoder Architecture) ("디코더 전용(decoder-only) 아키텍처(architecture)," NVLM-D)와 방법 B: 교차 모달리티 어텐션 아키텍처(Cross-Modality Attention Architecture) ("교차 어텐션 기반(cross-attention-based) 아키텍처(architecture)," NVLM-X)를 모두 심층적으로 탐구하기 때문에 특히 주목할 만합니다. 또한, 그들은 하이브리드 접근 방식(hybrid approach)(NVLM-H)을 혁신적으로 개발하고, 세 가지 방법 모두에 대한 공정하고 엄격한 비교(apples-to-apples comparison)를 제공합니다.

세 가지 멀티모달 접근 방식 개요. (NVLM: 오픈 프론티어 클래스 멀티모달 LLM 논문에서 주석이 달린 그림: https://arxiv.org/abs/2409.11402)

아래 그림에 요약된 대로, NVLM-D는 방법 A에 해당하고, NVLM-X는 이전에 논의된 방법 B에 해당합니다. 하이브리드 모델(hybrid model)(NVLM-H)의 개념은 두 가지 방법의 핵심 장점을 전략적으로 결합하는 것입니다. 이미지 썸네일(image thumbnail)이 입력으로 제공된 다음, 더 미세한 고해상도 세부 정보(high-resolution detail)를 캡처하기 위해 교차 어텐션(cross-attention)을 통해 동적 패치 수(dynamic number of patches)가 지능적으로 전달됩니다.

요약하자면, 연구팀은 다음과 같은 중요한 발견을 했습니다.
*   NVLM-X는 고해상도 이미지 처리에서 탁월한 계산 효율성(computational efficiency)을 보여줍니다.
*   NVLM-D는 OCR 관련 작업(OCR-related task)에서 더 높은 정확도를 달성하며, 세밀한 텍스트 인식에 강점을 보입니다.
*   NVLM-H는 두 가지 방법의 장점을 결합하여, 다양한 시나리오에서 최적의 성능을 제공합니다.

Molmo 및 다른 접근 방식과 유사하게, 그들은 처음부터 멀티모달 모델(multimodal model)을 사전 훈련(pretraining)하는 대신, 이미 강력하게 훈련된 텍스트 전용 LLM(text-only LLM)으로 시작합니다(일반적으로 이것이 더 나은 성능을 보이기 때문입니다). 또한, 기본 LLM(base LLM) 대신 명령어 미세 조정된 LLM(instruction-tuned LLM)을 활용하여 모델의 응답 품질을 향상시킵니다. 특히, 백본 LLM(backbone LLM)은 Qwen2-72B-Instruct입니다(제가 아는 한, Molmo는 Qwen2-72B 기본 모델(base model)을 사용했습니다).

NVLM-D 접근 방식에서 모든 LLM 매개변수(parameter)를 훈련(training)하는 동안, 그들은 NVLM-X의 경우 원래 LLM 매개변수(parameter)를 고정하고 사전 훈련(pretraining) 및 명령어 미세 조정(instruction finetuning) 모두에서 교차 어텐션 레이어(cross-attention layer)만 훈련(training)하는 것이 매우 효과적이라는 것을 발견했습니다. 이미지 인코더(image encoder)의 경우, 일반적인 CLIP 모델을 사용하는 대신 InternViT-6B를 사용하며, 이는 모든 단계에서 고정된 상태로 유지되어 안정성을 제공합니다. 프로젝터(projector)는 단일 선형 레이어(linear layer)가 아닌 다층 퍼셉트론(multilayer perceptron)으로, 더 복잡한 매핑을 가능하게 합니다.

#### 4.4 Qwen2-VL: 모든 해상도에서 비전-언어 모델의 세계 인식 향상

이전 두 논문과 모델인 Molmo 및 NVLM은 Qwen2-72B LLM을 기반으로 했습니다. 이 논문에서 Qwen 연구팀 자체는 멀티모달 LLM인 Qwen2-VL: 모든 해상도에서 비전-언어 모델의 세계 인식 향상(2024년 10월 3일)을 발표합니다. 이 작업의 핵심은 그들의 이른바 "단순 동적 해상도(Naive Dynamic Resolution)" 메커니즘(여기서 "단순(naive)"이라는 용어는 의도적이며 "네이티브(native)"의 오타가 아니지만, "네이티브(native)"도 적합할 수 있습니다)입니다. 이 메커니즘은 모델이 간단한 다운샘플링(downsampling) 없이 다양한 해상도의 이미지를 처리할 수 있도록 하여, 현실 세계의 시각 데이터를 더욱 충실하게 반영할 수 있게 합니다.

다양한 해상도의 입력 이미지를 기본적으로 처리할 수 있는 멀티모달 Qwen 모델의 개요. (Qwen2-VL 논문에서 주석이 달린 그림: https://arxiv.org/abs/2409.12191)

네이티브 해상도 입력(native resolution input)은 원래 절대 위치 임베딩(absolute position embedding)을 제거하고 2D-RoPE를 도입하여 수정된 ViT를 통해 혁신적으로 구현됩니다. 그들은 6억 7천 5백만 매개변수(parameter)를 가진 고전적인 비전 인코더(vision encoder)와 아래 표에 표시된 다양한 크기의 LLM 백본(LLM backbone)을 전략적으로 사용했습니다.

다양한 Qwen2-VL 모델의 구성 요소. (Qwen2-VL 논문에서 주석이 달린 그림: https://arxiv.org/abs/2409.12191)

훈련(training) 자체는 3단계로 구성됩니다. (1) 이미지 인코더(image encoder)만 사전 훈련(pretraining)하여 시각적 특징 추출 능력을 강화합니다, (2) 모든 매개변수(parameter)(LLM 포함) 고정 해제하여 멀티모달 정렬을 최적화합니다, (3) 이미지 인코더(image encoder) 고정 및 LLM만 명령어 미세 조정(instruction-finetuning)하여 최종 응답 품질을 개선합니다.

#### 4.5 Pixtral 12B

방법 A: 통합 임베딩 디코더 아키텍처(Unified Embedding Decoder Architecture) 접근 방식을 사용하는 Pixtral 12B(2024년 9월 17일)는 Mistral AI의 첫 번째 멀티모달 모델(multimodal model)로서, 그 등장이 주목받고 있습니다. 아쉽게도 상세한 기술 논문(technical paper)이나 보고서(report)는 아직 없지만, Mistral 팀은 블로그 게시물(blog post)에서 몇 가지 흥미로운 개발 철학을 공유했습니다. 흥미롭게도, 그들은 사전 훈련된(pretrained) 이미지 인코더(image encoder)를 사용하는 일반적인 경로를 따르지 않고, 대신 4억 매개변수(parameter)를 가진 인코더(encoder)를 처음부터 자체적으로 훈련(training)하기로 선택했습니다. LLM 백본(LLM backbone)으로는 120억 매개변수(parameter) Mistral NeMo 모델을 활용했습니다. Qwen2-VL과 유사하게, Pixtral도 아래 그림에 설명된 대로 가변 이미지 크기(variable image size)를 기본적으로 지원하여 다양한 시각 데이터를 유연하게 처리합니다.

Pixtral이 다양한 크기의 이미지를 처리하는 방법 그림. (Pixtral 블로그 게시물에서 주석이 달린 그림: https://mistral.ai/news/pixtral-12b/)

#### 4.6 MM1.5: 멀티모달 LLM 미세 조정의 방법, 분석 및 통찰력

MM1.5: 멀티모달 LLM 미세 조정의 방법, 분석 및 통찰력 논문(2024년 9월 30일)은 실용적인 팁을 제공하며, Molmo와 유사한 밀집 모델(dense model)과 함께 전문가 혼합 멀티모달 모델(mixture-of-experts multimodal model)의 새로운 지평을 엽니다. 이 모델들은 10억에서 300억 매개변수(parameter)에 이르는 넓은 크기 범위를 가지며, 다양한 컴퓨팅 환경에 적용 가능합니다. 이 논문에 설명된 모델들은 방법 A, 즉 멀티모달 학습을 위해 입력을 효과적으로 구조화하는 통합 임베딩 트랜스포머 아키텍처(Unified Embedding Transformer Architecture)에 깊이 초점을 맞춥니다. 또한, 이 논문은 데이터 혼합(data mixture)과 좌표 토큰(coordinate token) 사용의 효과를 심도 있게 분석하는 일련의 흥미로운 어블레이션 연구(ablation study)를 포함하여, 모델 설계에 대한 귀중한 통찰력을 제공합니다.

바운딩 박스(bounding box)를 나타내는 추가 좌표 토큰(coordinate token)을 포함하는 MM1.5 접근 방식의 그림. (MM1.5 논문에서 주석이 달린 그림: https://arxiv.org/abs/2409.20566.)

#### 4.7 Aria: 오픈 멀티모달 네이티브 전문가 혼합 모델

Aria: 오픈 멀티모달 네이티브 전문가 혼합 모델 논문(2024년 10월 8일)은 Molmo 및 MM1.5 라인업의 변형 중 하나와 유사한 또 다른 전문가 혼합 모델(mixture-of-experts model) 접근 방식을 선구적으로 제시합니다. Aria 모델은 249억 매개변수(parameter)를 가지며, 텍스트 토큰(text token)당 35억 매개변수(parameter)가 할당되어 효율성과 성능의 균형을 맞춥니다. 이미지 인코더(image encoder)(SigLIP)는 4억 3천 8백만 매개변수(parameter)를 가집니다. 이 모델은 다음과 같은 전체 훈련 절차(training procedure)를 가진 교차 어텐션(cross-attention) 접근 방식을 기반으로 하여, 최적의 멀티모달 이해를 목표로 합니다.
*   LLM 백본(LLM backbone)을 처음부터 완전히 훈련(training)하여 강력한 언어 기반을 구축합니다.
*   LLM 백본(LLM backbone)과 비전 인코더(vision encoder)를 모두 사전 훈련(pretraining)하여 멀티모달 정렬을 강화합니다.

#### 4.8 Baichuan-Omni

Baichuan-Omni 기술 보고서(2024년 10월 11일)는 아래 그림에 표시된 대로 방법 A: 통합 임베딩 디코더 아키텍처(Unified Embedding Decoder Architecture) 접근 방식을 기반으로 하는 70억 매개변수(parameter) 멀티모달 LLM인 Baichuan-Omni를 혁신적으로 소개합니다.

다양한 입력 모달리티(input modality)를 처리할 수 있는 Baichuan-Omni 모델의 개요. (Baichuan-Omni 논문에서 주석이 달린 그림: https://arxiv.org/abs/2410.08565)

Baichuan-Omni의 훈련(training) 프로세스는 3단계 접근 방식을 포함하며, 이는 점진적인 학습 전략을 보여줍니다.
*   **프로젝터(projector) 훈련(training)**: 처음에는 프로젝터(projector)만 훈련(training)되고, 비전 인코더(vision encoder)와 언어 모델(LLM)은 모두 고정된 상태로 유지되어 초기 정렬에 집중합니다.
*   **비전 인코더(vision encoder) 훈련(training)**: 다음으로, 비전 인코더(vision encoder)는 고정 해제되고 훈련(training)되며, LLM은 여전히 고정된 상태로 시각적 이해를 심화합니다.
*   **전체 모델 훈련(training)**: 마지막으로, LLM은 고정 해제되어 전체 모델이 엔드투엔드(end-to-end)로 훈련(training)될 수 있도록 하여, 멀티모달 능력을 완전히 통합합니다.

이 모델은 SigLIP 비전 인코더(vision encoder)를 활용하고, 다운샘플링 기법(down-sampling technique)을 통해 고해상도 이미지를 효율적으로 처리하기 위해 AnyRes 모듈을 통합합니다. 보고서에는 LLM 백본(LLM backbone)이 명시적으로 지정되어 있지 않지만, 모델의 매개변수 크기(parameter size)와 명명 규칙(naming convention)을 고려할 때 Baichuan 7B LLM을 기반으로 할 가능성이 높으며, 이는 이 분야의 표준 관행과 일치합니다.

#### 4.9 Emu3: 다음 토큰 예측만 있으면 됩니다

Emu3: 다음 토큰 예측만 있으면 됩니다 논문(2024년 9월 27일)은 이미지 생성(image generation)을 위한 확산 모델(diffusion model)에 대한 설득력 있는 대안을 제시하며, 이는 순전히 트랜스포머 기반 디코더 아키텍처(transformer-based decoder architecture)에만 기반합니다. 비록 고전적인 의미의 멀티모달 LLM(즉, 생성보다는 이미지 이해(image understanding)에 초점을 맞춘 모델)은 아니지만, Emu3는 트랜스포머 디코더(transformer decoder)를 사용하여 이미지 생성(image generation)의 새로운 가능성을 보여주기 때문에 매우 흥미롭습니다. 이미지 생성(image generation)은 일반적으로 확산 방법(diffusion method)이 지배하는 영역이었으나, Emu3는 이 패러다임에 도전합니다. (그러나 Autoregressive Model Beats Diffusion: Llama for Scalable Image Generation과 같은 다른 유사한 접근 방식이 이전에도 있었다는 점에 유의하세요.)

Emu3는 확산 모델(diffusion model)의 대안으로 이미지 생성(image generation)을 위한 LLM입니다. (Emu3 논문에서 주석이 달린 그림: https://arxiv.org/abs/2409.18869)

연구자들은 Emu3를 처음부터 훈련(training)한 다음, 직접 선호 최적화(Direct Preference Optimization, DPO)를 사용하여 모델을 인간 선호도에 정교하게 맞추었습니다. 아키텍처(architecture)에는 SBER-MoVQGAN에서 영감을 받은 혁신적인 비전 토크나이저(vision tokenizer)가 포함됩니다. 핵심 LLM 아키텍처(core LLM architecture)는 Llama 2를 기반으로 하지만, 처음부터 완전히 재훈련(training)되어 독립적인 성능을 확보했습니다.

#### 4.10 Janus: 통합 멀티모달 이해 및 생성을 위한 시각 인코딩 분리

우리는 이전에 이미지 이해(image understanding)를 위한 멀티모달 LLM에 초점을 맞췄고, 위에서 Emu 3를 통한 이미지 생성(image generation)의 한 가지 혁신적인 예를 보았습니다. 이제 Janus: 통합 멀티모달 이해 및 생성을 위한 시각 인코딩 분리 논문(2024년 10월 17일)은 단일 LLM 백본(LLM backbone) 내에서 멀티모달 이해(multimodal understanding) 및 생성 작업(generation task)을 유기적으로 통합하는 새로운 프레임워크(framework)를 소개합니다.

Janus의 핵심 기능은 이해(understanding) 및 생성 작업(generation task)의 고유한 요구 사항(distinct requirement)을 해결하기 위한 시각 인코딩 경로(visual encoding pathway)의 전략적 분리입니다. 연구자들은 이미지 이해(image understanding) 작업에는 고차원 의미론적 표현(high-dimensional semantic representation)이 필요하고, 생성 작업(generation task)에는 이미지의 상세한 지역 정보(detailed local information)와 전역 일관성(global consistency)이 필수적이라고 주장합니다. 이러한 경로를 분리함으로써 Janus는 이러한 상이한 요구 사항을 효과적으로 관리하며, 각 작업의 성능을 극대화합니다.

이 모델은 Baichuan-Omni에서 사용된 것과 유사한 SigLIP 비전 인코더(vision encoder)를 사용하여 시각적 입력을 정교하게 처리합니다. 이미지 생성(image generation)을 위해 벡터 양자화(Vector Quantized, VQ) 토크나이저(tokenizer)를 사용하여 생성 프로세스를 효율적으로 처리합니다. Janus의 기본 LLM(base LLM)은 13억 매개변수(parameter)를 가진 DeepSeek-LLM입니다.

Janus에서 사용된 통합 디코더 전용(decoder-only) 프레임워크(framework)의 개요. (Janus 논문에서 주석이 달린 그림: https://arxiv.org/abs/2410.13848.)

이 이미지의 모델 훈련(training) 프로세스는 아래 그림에 표시된 대로 3단계로 진행되며, 각 단계는 특정 목표를 가집니다.

Janus 모델의 3단계 훈련(training) 프로세스 그림. (Janus 논문에서 주석이 달린 그림: https://arxiv.org/abs/2410.13848)

1단계에서는 LLM, 이해(understanding) 및 생성 인코더(generation encoder)가 고정된 상태에서 프로젝터 레이어(projector layer)와 이미지 출력 레이어(image output layer)만 훈련(training)되어 초기 정렬을 확립합니다. 2단계에서는 LLM 백본(LLM backbone)과 텍스트 출력 레이어(text output layer)가 고정 해제되어 이해(understanding) 및 생성 작업(generation task) 전반에 걸쳐 통합 사전 훈련(unified pretraining)이 가능해지며, 모델의 멀티모달 이해를 심화합니다. 마지막으로 3단계에서는 SigLIP 이미지 인코더(image encoder)를 포함한 전체 모델이 고정 해제되어 지도 미세 조정(supervised fine-tuning)을 위해 사용되며, 모델이 멀티모달 기능(multimodal capability)을 완전히 통합하고 개선하여 최종 성능을 최적화합니다.

### 결론

눈치채셨겠지만, 저는 모델링(modeling) 및 계산 성능 비교(computational performance comparison)를 거의 완전히 건너뛰었습니다. 첫째, LLM 및 멀티모달 LLM의 공개 벤치마크(public benchmark) 성능을 비교하는 것은 일반적인 데이터 오염(data contamination) 문제와 더불어, 평가 기준의 복잡성 때문에 여전히 어려운 과제입니다. 즉, 테스트 데이터(test data)가 훈련 데이터(training data)에 포함되었을 수 있습니다. 또한, 아키텍처 구성 요소(architectural component)가 너무 다양하여 공정한 비교(apples-to-apples comparison)를 하기가 어렵습니다. 따라서 적어도 디코더 전용(decoder-only) 및 교차 어텐션(cross-attention) 접근 방식 간의 비교를 가능하게 한 다양한 버전의 NVLM을 개발한 NVIDIA 팀에게는 특별한 찬사를 보냅니다.

어쨌든, 이 글의 핵심 요점(main takeaway)은 멀티모달 LLM이 다양한 방식으로 성공적으로 구축될 수 있다는 것과, 그 발전 가능성이 무궁무진하다는 것입니다. 아래는 이 글에서 다룬 모델들의 다양한 구성 요소를 요약한 그림입니다.

이 글에서 다룬 다양한 모델과 그 하위 구성 요소 및 훈련(training) 접근 방식에 대한 개요.

이 글을 읽는 것이 교육적(educational)이었기를 바라며, 이제 멀티모달 LLM이 단순한 기술을 넘어 어떻게 미래를 형성할지에 대해 더 깊이 이해하게 되셨기를 바랍니다!

이 잡지는 개인적인 열정 프로젝트(personal passion project)이자 지식 공유에 대한 저의 헌신입니다. 저를 지원하고 싶은 분들은 제 책 "Build a Large Language Model (From Scratch)"을 구매해 주시면 감사하겠습니다. (이 책은 LLM이 어떻게 작동하는지 다른 곳에서는 찾아볼 수 없는 수준의 세부 사항으로 설명하므로, 이 책에서 많은 것을 얻으실 것이라고 확신합니다.)

[Build a Large Language Model (From Scratch) now available on Amazon]

책을 읽으셨고 잠시 시간을 내실 수 있다면, 짧은 리뷰를 남겨주시면 정말 감사하겠습니다. 그것은 저희 저자들에게 큰 도움이 됩니다! 또는, 최근에 Substack에 유료 구독 옵션(paid subscription option)을 활성화하여 이 잡지를 직접 지원할 수도 있습니다. 여러분의 지원은 큰 의미가 있으며, 더 나은 콘텐츠를 만들 수 있는 원동력이 됩니다! 감사합니다!

[Subscribe]