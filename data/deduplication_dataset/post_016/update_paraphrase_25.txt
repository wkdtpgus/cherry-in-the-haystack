지난 두 달은 정말 역동적인 시기였습니다. 인공지능(AI) 연구 분야에서는 눈부신 진전이 계속되었고, 심지어 AI에 두 개의 노벨상이 수여되는 놀라운 일도 있었으며, 수많은 매력적인 연구 논문들이 공개되었습니다. 특히 Meta AI는 최신 Llama 3.2 모델군을 선보였는데, 여기에는 10억 및 30억 매개변수 규모의 대형 언어 모델(large language model) 공개 가중치(open-weight) 버전과 두 종류의 멀티모달 모델(multimodal model)이 포함되어 있습니다. 이번 글에서는 멀티모달 LLM(multimodal LLM)이 어떤 방식으로 작동하는지 자세히 설명하고자 합니다. 더불어, 최근 몇 주간 발표된 약 12개의 최신 멀티모달 논문과 모델(Llama 3.2 포함)들을 심층적으로 분석하고 요약하여 각 접근 방식의 특징을 비교해 볼 것입니다. (전체 목차는 왼쪽의 메뉴 아이콘을 클릭하여 확인하실 수 있습니다.)

다양한 입력 양식(input modality)(음성, 문자, 영상, 동영상)을 수용하고, 텍스트 형태의 결과물(output modality)을 생성하는 멀티모달 LLM의 개념도.

글을 시작하기에 앞서, 개인적으로 기쁜 소식이 하나 있습니다! 저의 저서 "Build A Large Language Model (From Scratch)"이 드디어 Amazon에서 만나볼 수 있게 되었습니다!

[Build a Large Language Model (From Scratch) now available on Amazon]

이 책을 집필하는 과정은 엄청난 노력을 요구했으며, 지난 2년간—특히 최근 몇 달 동안—많은 독자분들이 보내주신 따뜻한 격려와 소중한 피드백에 진심으로 감사드립니다. 여러분 한 분 한 분께 감사드리며, 저자로서 이 책이 여러분의 커리어에 긍정적인 영향을 미쳤다는 소식만큼 큰 보람은 없을 것입니다! 책을 완독하시고 더 많은 지식을 갈망하는 분들을 위해, 계속해서 주목해 주시길 바랍니다! 앞으로 몇 달 안에 GitHub 저장소를 통해 추가 콘텐츠를 제공할 예정입니다.

추신. 혹시 이 책을 읽으셨다면, 짧은 서평을 남겨주시면 큰 힘이 될 것입니다. 이는 저희 저자들에게 정말 큰 도움이 됩니다!

### 1. 멀티모달 LLM의 활용 분야 및 개념 이해

멀티모달 LLM이란 무엇일까요? 서두에서 암시했듯이, 멀티모달 LLM은 여러 종류의 데이터를 입력으로 받아들일 수 있는 대규모 언어 모델(large language model)을 지칭하며, 여기서 각 "모달리티(modality)"는 기존 LLM처럼 텍스트뿐만 아니라 소리, 이미지, 비디오 등과 같은 특정한 형식의 데이터를 의미합니다. 본 글에서는 주로 텍스트 입력과 더불어 이미지 모달리티(image modality)에 집중하여 논의를 진행할 것입니다.

멀티모달 LLM의 대표적이고 직관적인 활용 사례는 바로 이미지 설명 생성(image captioning)입니다. 이미지를 입력하면, 모델이 해당 이미지에 대한 설명을 아래 그림과 같이 만들어냅니다.

밈(meme)을 설명하는 멀티모달 LLM의 활용 예시.

물론, 이 외에도 다양한 응용 분야가 존재합니다. 예를 들어, 제가 특히 선호하는 기능 중 하나는 PDF 문서 내 표에서 정보를 추출하여 LaTeX 또는 Markdown 형식으로 변환하는 것입니다.

### 2. 멀티모달 LLM 구축을 위한 핵심 접근 방식

멀티모달 LLM을 개발하는 데는 두 가지 주요한 방법론이 있습니다. 첫째, 통합 임베딩 디코더 구조(Unified Embedding Decoder Architecture) 방식; 둘째, 교차 모달리티 어텐션 구조(Cross-modality Attention Architecture) 방식입니다. (참고로, 이러한 기술에 대한 공식적인 명칭이 아직 확립되지 않은 것으로 보입니다. 더 간결한 표현으로는 "디코더 전용(decoder-only)" 및 "교차 어텐션 기반(cross-attention-based)" 접근 방식 등이 있을 수 있으며, 혹시 더 적절한 용어를 아신다면 알려주십시오.)

멀티모달 LLM 아키텍처 개발을 위한 두 가지 주요 접근 방식.

위 그림에서 볼 수 있듯이, 통합 임베딩 디코더 구조(Unified Embedding-Decoder Architecture)는 GPT-2나 Llama 3.2와 같이 기존의 LLM 구조를 거의 그대로 활용하는 단일 디코더 모델(decoder model)을 사용합니다. 이 방식에서는 이미지를 텍스트 토큰(text token)과 동일한 임베딩 크기(embedding size)를 가진 토큰 형태로 변환하여, LLM이 이들을 연결(concatenation)한 후 텍스트 및 이미지 입력 토큰(image input token)을 함께 처리할 수 있게 합니다. 반면, 교차 모달리티 어텐션 구조(Cross-Modality Attention Architecture)는 교차 어텐션 메커니즘(cross-attention mechanism)을 활용하여 어텐션 계층(attention layer) 내에서 이미지와 텍스트 임베딩(embedding)을 직접적으로 통합합니다.

다음 단원에서는 이러한 방법들이 개념적인 수준(conceptual level)에서 어떻게 작동하는지 자세히 살펴볼 것입니다. 이어서, 최신 멀티모달 LLM 연구 논문들을 검토하여 실제 적용 사례를 알아보겠습니다.

#### 2.1 방법 A: 통합 임베딩 디코더 구조

아래 그림에 다시 제시된 통합 임베딩 디코더 구조(unified embedding decoder architecture)부터 논의를 시작하겠습니다.

이미지 토큰(image token)과 텍스트 토큰(text token) 임베딩(embedding)으로 구성된 입력을 받는 수정되지 않은 디코더 스타일 LLM(GPT-2, Phi-3, Gemma 또는 Llama 3.2와 같은)인 통합 임베딩 디코더 아키텍처의 그림.

통합 임베딩 디코더 구조(unified embedding-decoder architecture)에서 이미지는 일반적인 텍스트 전용 LLM(text-only LLM)이 입력 텍스트를 임베딩으로 변환하는 방식과 유사하게 임베딩 벡터(embedding vector)로 변환됩니다. 텍스트를 처리하는 일반적인 텍스트 전용 LLM의 경우, 텍스트 입력은 주로 토큰화(tokenized)된 다음(예: 바이트 쌍 인코딩(Byte-Pair Encoding) 사용), 아래 그림처럼 임베딩 계층(embedding layer)을 거치게 됩니다.

텍스트를 토큰화(tokenizing)하고 토큰 임베딩 벡터(token embedding vector)로 변환하는 표준 프로세스 그림. 이 벡터는 훈련(training) 및 추론(inference) 중에 LLM으로 전달됩니다.

##### 2.1.1 이미지 인코더(Image encoders)의 역할 이해

텍스트의 토큰화(tokenization) 및 임베딩(embedding) 과정과 비슷하게, 이미지 임베딩(image embedding)은 아래 그림과 같이 이미지 인코더(image encoder) 모듈(텍스트 토크나이저(tokenizer) 대신)을 사용하여 생성됩니다.

이미지를 이미지 패치 임베딩(image patch embedding)으로 인코딩하는 프로세스 그림.

위에 제시된 이미지 인코더(image encoder) 내부에서는 정확히 어떤 과정이 일어날까요? 이미지를 처리하기 위해, 먼저 이미지를 더 작은 패치(patch)들로 분할합니다. 이는 텍스트 토큰화(tokenization) 과정에서 단어를 서브워드(subword)로 나누는 것과 매우 유사한 개념입니다. 이 패치들은 아래 그림과 같이 사전 학습된(pretrained) 비전 트랜스포머(vision transformer, ViT)에 의해 인코딩됩니다.

"An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale (2020)"에서 제안된 모델과 유사한 고전적인 비전 트랜스포머(ViT) 설정 그림. ViT는 종종 분류 작업(classification task)에 사용되므로 위 그림에 분류 헤드(classification head)를 포함했습니다. 그러나 이 경우에는 이미지 인코더(image encoder) 부분만 필요합니다.

##### 2.1.2 선형 투영(linear projection) 모듈의 중요성

앞선 그림에서 언급된 "선형 투영(linear projection)"은 단일 선형 계층(linear layer), 즉 완전 연결 계층(fully connected layer)으로 구성됩니다. 이 계층의 주된 목적은 벡터 형태로 평탄화된 이미지 패치(image patch)를 트랜스포머 인코더(transformer encoder)와 호환되는 임베딩 차원(embedding size)으로 변환하는 것입니다. 이 선형 투영(linear projection) 과정은 아래 그림에 상세히 설명되어 있습니다.

256차원 벡터로 평탄화된 이미지 패치(image patch)가 768차원 벡터로 상향 투영됩니다.

256차원에서 768차원 임베딩 공간(embedding space)으로 평탄화된 이미지 패치(image patch)를 투영하는 선형 투영(linear projection) 레이어 그림.

코드 예시를 선호하는 분들을 위해, PyTorch 환경에서 이미지 패치(image patch)에 대한 선형 투영(linear projection)은 다음과 같이 구현할 수 있습니다.

```python
import torch

class PatchProjectionLayer(torch.nn.Module):
    def __init__(self, patch_size, num_channels, embedding_dim):
        super().__init__()
        self.patch_size = patch_size
        self.num_channels = num_channels
        self.embedding_dim = embedding_dim
        self.projection = torch.nn.Linear(
            patch_size * patch_size * num_channels,
            embedding_dim
        )

    def forward(self, x):
        batch_size, num_patches, channels, height, width = x.size()
        x = x.view(batch_size, num_patches, -1) # Flatten each patch
        x = self.projection(x) # Project each flattened patch
        return x

# Example Usage:
batch_size = 1
num_patches = 9 # Total patches per image
patch_size = 16 # 16x16 pixels per patch
num_channels = 3 # RGB image
embedding_dim = 768 # Size of the embedding vector

projection_layer = PatchProjectionLayer(patch_size, num_channels, embedding_dim)

patches = torch.rand(
    batch_size, num_patches, num_channels, patch_size, patch_size
)

projected_embeddings = projection_layer(patches)
print(projected_embeddings.shape)
# This prints
# torch.Size([1, 9, 768])
```

혹시 제 책 "Machine Learning Q and AI"를 읽으셨다면, 선형 계층(linear layer)을 수학적으로 동일하게 구현할 수 있는 컨볼루션 연산(convolution operation)으로 대체하는 방법이 있다는 것을 아실 것입니다. 이 경우, 패치(patch) 생성과 투영(projection)을 두 줄의 코드로 결합할 수 있어 특히 유용합니다.

```python
layer = torch.nn.Conv2d(3, 768, kernel_size=(16, 16), stride=(16, 16))
image = torch.rand(batch_size, 3, 48, 48)
projected_patches = layer(image)
print(projected_patches.flatten(-2).transpose(-1, -2).shape)
# This prints
# torch.Size([1, 9, 768])
```

##### 2.1.3 이미지와 텍스트 토큰화(tokenization)의 비교

이제 이미지 인코더(image encoder)(및 그 일부인 선형 투영(linear projection))의 기능에 대해 간략히 살펴보았으므로, 이전의 텍스트 토큰화(text tokenization) 비유로 돌아가 아래 그림에 나타난 텍스트 및 이미지 토큰화(tokenization)와 임베딩(embedding) 과정을 함께 비교해 보겠습니다.

이미지 토큰화(tokenization) 및 임베딩(embedding)(왼쪽)과 텍스트 토큰화(tokenization) 및 임베딩(embedding)(오른쪽)을 나란히 비교.

위 그림에서 볼 수 있듯이, 저는 이미지 인코더(image encoder) 뒤에 추가적인 프로젝터 모듈(projector module)을 포함시켰습니다. 이 프로젝터(projector)는 일반적으로 이전에 설명한 것과 유사한 또 다른 선형 투영(linear projection) 계층에 불과합니다. 그 목적은 아래 그림에 설명된 대로 이미지 인코더(image encoder)의 출력을 임베딩된 텍스트 토큰(text token)의 차원과 일치하는 차원으로 투영하는 것입니다. (나중에 보겠지만, 프로젝터(projector)는 때때로 어댑터(adapter), 어댑터(adaptor) 또는 커넥터(connector)라고도 불립니다.)

이미지 토큰화(tokenization)와 텍스트 토큰화(tokenization)의 또 다른 나란한 비교. 여기서 프로젝터(projector)의 역할은 텍스트 토큰 임베딩(text token embedding) 차원을 일치시키는 것입니다.

이제 이미지 패치 임베딩(image patch embedding)이 텍스트 토큰 임베딩(text token embedding)과 동일한 임베딩 차원(embedding dimension)을 가지므로, 이 섹션 시작 부분의 그림에 표시된 대로 LLM의 입력으로 간단히 연결(concatenate)할 수 있습니다. 아래는 더 쉽게 참조할 수 있도록 동일한 그림입니다.

이미지 패치 토큰(image patch token)을 텍스트 토큰 임베딩(text token embedding)과 동일한 차원으로 투영한 후, 표준 LLM의 입력으로 간단히 연결(concatenate)할 수 있습니다.

참고로, 이 단원에서 논의된 이미지 인코더(image encoder)는 대개 사전 학습된(pretrained) 비전 트랜스포머(vision transformer)입니다. CLIP 또는 OpenCLIP이 널리 사용되는 선택지입니다. 하지만 아래 그림에 나타난 Fuyu와 같이 패치(patch)에서 직접 작동하는 방법 A의 변형도 존재합니다.

이미지 인코더(image encoder) 없이 이미지 패치(image patch)에서 직접 작동하는 Fuyu 멀티모달 LLM의 주석이 달린 그림. (https://www.adept.ai/blog/fuyu-8b에서 주석이 달린 그림.)

위 그림에서 설명된 바와 같이, Fuyu는 입력 패치(input patch)를 선형 투영(linear projection)(또는 임베딩 계층(embedding layer))으로 직접 전달하여 다른 모델이나 방법처럼 추가적인 사전 학습된(pretrained) 이미지 인코더(image encoder)에 의존하지 않고 자체 이미지 패치 임베딩(image patch embedding)을 학습합니다. 이러한 접근 방식은 아키텍처(architecture)와 훈련 설정(training setup)을 크게 단순화하는 효과가 있습니다.

#### 2.2 방법 B: 교차 모달리티 어텐션 구조(Cross-Modality Attention Architecture)

이제 멀티모달 LLM을 구축하기 위한 통합 임베딩 디코더 구조(unified embedding decoder architecture) 접근 방식에 대해 논의하고 이미지 인코딩(image encoding)의 기본적인 개념을 이해했으므로, 아래 그림에 요약된 교차 어텐션(cross-attention)을 통한 멀티모달 LLM 구현의 또 다른 방법에 대해 이야기해 보겠습니다.

멀티모달 LLM 구축을 위한 교차 모달리티 어텐션 아키텍처(Cross-Modality Attention Architecture) 접근 방식의 그림.

위 그림에 제시된 교차 모달리티 어텐션 구조(Cross-Modality Attention Architecture) 방식에서는 앞서 논의했던 것과 동일한 이미지 인코더(image encoder) 설정을 여전히 활용합니다. 하지만 패치(patch)를 LLM의 입력으로 인코딩하는 대신, 교차 어텐션 메커니즘(cross-attention mechanism)을 통해 멀티 헤드 어텐션 계층(multi-head attention layer)에서 입력 패치(input patch)를 연결합니다. 이 아이디어는 관련성이 깊으며, 아래 그림에 강조된 2017년 "Attention Is All You Need" 논문의 오리지널 트랜스포머 아키텍처(original transformer architecture)에서 비롯되었습니다.

오리지널 트랜스포머 아키텍처(original transformer architecture)에 사용된 교차 어텐션 메커니즘(cross-attention mechanism)의 고수준 그림. ("Attention Is All You Need" 논문에서 주석이 달린 그림: https://arxiv.org/abs/1706.03762.)

위 그림에서 묘사된 오리지널 "Attention Is All You Need" 트랜스포머(transformer)는 본래 언어 번역을 위해 개발되었습니다. 따라서 번역할 문장을 받아들이고 텍스트 디코더(text decoder)(그림의 오른쪽 부분)를 통해 번역을 생성하는 텍스트 인코더(text encoder)(그림의 왼쪽 부분)로 구성됩니다. 멀티모달 LLM의 맥락에서는 인코더(encoder)가 텍스트 인코더(text encoder) 대신 이미지 인코더(image encoder)이지만, 동일한 기본 개념이 적용됩니다.

교차 어텐션(cross-attention)은 어떻게 작동할까요? 일반적인 셀프 어텐션 메커니즘(self-attention mechanism) 내부에서 일어나는 일에 대한 개념적 그림을 살펴보겠습니다.

일반적인 셀프 어텐션 메커니즘(self-attention mechanism)의 개요. (이 흐름은 일반적인 멀티 헤드 어텐션 모듈(multi-head attention module)의 헤드 중 하나를 묘사합니다.)

위 그림에서 x는 입력이며, Wq는 쿼리(queries, Q)를 만드는 데 사용되는 가중치 행렬(weight matrix)입니다. 유사하게, K는 키(keys)를, V는 값(values)을 나타냅니다. A는 어텐션 스코어 행렬(attention scores matrix)을, Z는 출력 컨텍스트 벡터(output context vector)로 변환된 입력(x)을 나타냅니다. (이 설명이 다소 복잡하게 느껴진다면, 제 책 "Build a Large Language Model from Scratch"의 3장에서 포괄적인 소개를 찾아보는 것이 도움이 될 수 있습니다. 또는 제 기사 "Understanding and Coding Self-Attention, Multi-Head Attention, Cross-Attention, and Causal-Attention in LLMs"도 유용할 것입니다.)

교차 어텐션(cross-attention)에서는 셀프 어텐션(self-attention)과 달리 다음 그림에 설명된 대로 두 가지 상이한 입력 소스(input source)가 존재합니다.

두 가지 다른 입력 x1과 x2가 있을 수 있는 교차 어텐션(cross attention) 그림.

앞선 두 그림에서 설명했듯이, 셀프 어텐션(self-attention)은 동일한 입력 시퀀스(input sequence) 내에서 작동합니다. 반면, 교차 어텐션(cross-attention)은 두 가지 다른 입력 시퀀스(input sequence)를 혼합하거나 결합합니다. "Attention Is All You Need" 논문의 오리지널 트랜스포머 아키텍처(original transformer architecture)의 경우, 두 입력 x1과 x2는 왼쪽의 인코더 모듈(encoder module)이 반환하는 시퀀스(x2)와 오른쪽의 디코더 부분(decoder part)이 처리하는 입력 시퀀스(x1)에 해당합니다. 멀티모달 LLM의 맥락에서 x2는 이미지 인코더(image encoder)의 출력입니다. (쿼리(queries)는 보통 디코더(decoder)에서 오고, 키(keys)와 값(values)은 일반적으로 인코더(encoder)에서 온다는 점을 기억하세요.)

교차 어텐션(cross-attention)에서 두 입력 시퀀스 x1과 x2는 서로 다른 수의 요소를 가질 수 있다는 점에 유의해야 합니다. 그러나 임베딩 차원(embedding dimension)은 반드시 일치해야 합니다. 만약 x1 = x2로 설정한다면, 이는 셀프 어텐션(self-attention)과 동일한 상황이 됩니다.

### 3. 통합 디코더 및 교차 어텐션 모델 훈련(training)의 전략

이제 두 가지 핵심 멀티모달 설계 선택지에 대해 간략히 살펴보았으니, 아래 그림에 요약된 모델 훈련(model training) 과정 중 세 가지 주요 구성 요소를 어떻게 다루는지 논의해 보겠습니다.

멀티모달 LLM의 다양한 구성 요소 개요. 1-3번으로 표시된 구성 요소는 멀티모달 훈련(training) 프로세스 중에 고정되거나 고정되지 않을 수 있습니다.

기존의 텍스트 전용 LLM(text-only LLM) 개발과 마찬가지로, 멀티모달 LLM 훈련(training) 역시 사전 학습(pretraining)과 명령어 미세 조정(instruction finetuning)의 두 단계를 거칩니다. 하지만 처음부터 시작하는 대신, 멀티모달 LLM 훈련(training)은 일반적으로 이미 사전 학습(pretrained)되고 명령어 미세 조정(instruction-finetuned)된 텍스트 전용 LLM을 기본 모델(base model)로 활용하는 경우가 많습니다.

이미지 인코더(image encoder)의 경우, CLIP이 널리 사용되며 전체 훈련(training) 과정 동안 고정되는 경우가 많지만, 예외도 있습니다. 사전 학습(pretraining) 단계에서 LLM 부분을 고정하고 프로젝터(projector)—선형 계층(linear layer) 또는 작은 다층 퍼셉트론(multi-layer perceptron)—훈련(training)에만 집중하는 것도 일반적인 방식입니다. 프로젝터(projector)의 학습 능력이 제한적이고 주로 한두 개의 계층(layer)으로만 구성되기 때문에, 더 포괄적인 업데이트를 위해 멀티모달 명령어 미세 조정(multimodal instruction finetuning)(2단계) 동안 LLM이 종종 고정 해제됩니다. 그러나 교차 어텐션 기반(cross-attention-based) 모델(방법 B)에서는 교차 어텐션 계층(cross-attention layer)이 전체 훈련(training) 과정 동안 고정 해제된다는 점을 주목할 필요가 있습니다.

두 가지 주요 접근 방식(방법 A: 통합 임베딩 디코더 구조(Unified Embedding Decoder Architecture) 및 방법 B: 교차 모달리티 어텐션 구조(Cross-modality Attention Architecture))을 소개한 후, 어떤 방식이 더 효과적인지 궁금할 수 있습니다. 답변은 특정 절충점(trade-off)에 따라 달라집니다. 통합 임베딩 디코더 구조(Unified Embedding Decoder Architecture)(방법 A)는 LLM 아키텍처(architecture) 자체에 어떤 수정도 필요하지 않으므로 구현이 비교적 용이합니다. 반면, 교차 모달리티 어텐션 구조(Cross-modality Attention Architecture)(방법 B)는 추가 이미지 토큰(image token)으로 입력 컨텍스트(input context)를 과도하게 만들지 않고 대신 교차 어텐션 계층(cross-attention layer)에서 나중에 도입하기 때문에 종종 계산 효율성(computational efficiency)이 더 높은 것으로 평가됩니다. 또한, 이 접근 방식은 훈련(training) 중 LLM 매개변수(parameter)가 고정된 상태를 유지하면 기존 LLM의 텍스트 전용 성능(text-only performance)을 보존합니다. 모델링 성능(modeling performance) 및 응답 품질(response quality)에 대한 논의는 NVIDIA의 NVLM 논문을 다룰 다음 섹션에서 다시 심도 있게 다룰 것입니다.

이것으로 멀티모달 LLM에 대한 다소 광범위한 소개를 마칩니다. 이 글을 작성하면서, 논의가 처음 계획했던 것보다 길어졌다는 것을 깨달았고, 아마도 이 시점에서 글을 마무리하는 것이 적절할 것 같습니다. 그러나 실용적인 관점을 제공하기 위해 이러한 접근 방식들을 구현하는 몇 가지 최신 연구 논문들을 살펴보는 것이 유익할 것입니다. 따라서 이 글의 나머지 부분에서는 이러한 논문들을 자세히 탐구할 것입니다.

### 4. 최신 멀티모달 모델 및 방법론 분석

이 글의 남은 부분에서는 합리적인 범위(reasonable scope)를 유지하기 위해, 특히 지난 몇 주 동안 공개된 연구들을 중심으로 멀티모달 LLM에 관한 최신 문헌을 검토할 것입니다. 따라서 이는 멀티모달 LLM에 대한 역사적 개요(historical overview)나 포괄적인 검토(comprehensive review)라기보다는, 최근의 발전 동향에 대한 간결한 탐색이라고 할 수 있습니다. 또한, 다루는 모델이 많으므로 각 요약을 짧고 핵심적인 내용 위주로 구성하고자 합니다. 이 글의 마지막 결론 부분에는 이러한 논문들에서 사용된 방법론들을 비교하는 종합적인 요약이 포함될 것입니다.

#### 4.1 Llama 3 모델군 (Herd of Models)

Meta AI의 Llama 3 모델군(Herd of Models) 논문(2024년 7월 31일)은 올여름 초에 공개되었는데, LLM 분야의 시간 흐름으로 보면 이미 오래전 일처럼 느껴집니다. 그러나 그들이 멀티모달 모델(multimodal model)을 나중에 설명했지만 아직 정식으로 출시하지 않았다는 점을 고려할 때, Llama 3를 이 목록에 포함하는 것이 타당하다고 생각합니다. (Llama 3.2 모델은 9월 25일에 공식적으로 발표되었고 공개되었습니다.)

110억 및 900억 매개변수(parameter) 버전으로 제공되는 멀티모달 Llama 3.2 모델은 앞서 설명된 교차 어텐션 기반(cross-attention-based) 접근 방식을 사용하는 이미지-텍스트 모델(image-text model)이며, 이는 아래 그림에 명확히 설명되어 있습니다.

Llama 3.2에서 사용된 멀티모달 LLM 접근 방식의 그림. (Llama 3 논문에서 주석이 달린 그림: https://arxiv.org/abs/2407.21783. 비디오 및 음성 부분은 이미지 부분에 초점을 맞추기 위해 시각적으로 가려져 있습니다.)

이 그림은 비디오와 음성도 가능한 모달리티(modality)로 묘사하고 있지만, 이 글을 쓰는 현재 출시된 모델들은 이미지와 텍스트에만 초점을 맞추고 있다는 점에 유의해야 합니다. Llama 3.2는 교차 어텐션 기반(cross-attention-based) 접근 방식을 사용합니다. 그러나 이는 제가 이전에 언급했던 내용, 즉 멀티모달 LLM 개발에서 일반적으로 이미지 인코더(image encoder)를 고정하고 사전 학습(pretraining) 중에 LLM 매개변수(parameter)만 업데이트한다는 내용과는 약간 다릅니다. 여기서는 연구자들이 거의 반대되는 접근 방식을 취합니다. 그들은 이미지 인코더(image encoder)를 업데이트하지만 언어 모델(language model)의 매개변수(parameter)는 업데이트하지 않습니다. 그들은 이것이 의도적이며 텍스트 전용 기능(text-only capabilities)을 보존하기 위해 수행되었다고 밝혔습니다. 따라서 110억 및 900억 멀티모달 모델(multimodal model)이 텍스트 작업에서 Llama 3.1 80억 및 700억 텍스트 전용 모델(text-only model)의 드롭인(drop-in) 대체품으로 사용될 수 있습니다.

훈련(training) 자체는 Llama 3.1 텍스트 모델(text model)을 기반으로 시작하여 여러 번의 반복을 거쳐 수행됩니다. 이미지 인코더(image encoder)와 투영(projection)(여기서는 "어댑터(adapter)"라고 불림) 계층(layer)을 추가한 후, 이미지-텍스트 데이터(image-text data)로 모델을 사전 학습(pretrain)합니다. 그런 다음, Llama 3 모델의 텍스트 전용 훈련(text-only training)과 유사하게(이전 글에서 이에 대해 다룬 바 있습니다), 명령어 및 선호도 미세 조정(instruction and preference finetuning) 단계로 진행됩니다.

연구자들은 CLIP과 같은 사전 학습된(pretrained) 모델을 이미지 인코더(image encoder)로 채택하는 대신, 처음부터 사전 학습된(pretrained) 비전 트랜스포머(vision transformer)를 사용했습니다. 특히, 그들은 고전적인 비전 트랜스포머(vision transformer) 아키텍처(architecture)의 ViT-H/14 변형(6억 3천만 매개변수(parameter))(Dosovitskiy et al., 2020)을 채택했습니다. 그런 다음, 이미지 인코더(image encoder)를 LLM에 연결하기 전에 25억 개의 이미지-텍스트 쌍(image-text pair) 데이터셋(dataset)에서 5 에포크(epoch) 동안 ViT를 사전 학습(pretrain)했습니다. (이미지 인코더(image encoder)는 224×224 해상도 이미지를 받아 14×14 그리드의 패치(patch)로 나누며, 각 패치(patch)는 16×16 픽셀 크기입니다.)

교차 어텐션 계층(cross-attention layer)은 상당한 양의 매개변수(parameter)를 추가하므로, 4번째 트랜스포머 블록(transformer block)마다만 추가됩니다. (80억 모델의 경우 30억 매개변수(parameter)가 추가되고, 700억 모델의 경우 200억 매개변수(parameter)가 추가됩니다.)

#### 4.2 Molmo 및 PixMo: 최첨단 멀티모달 모델을 위한 공개 가중치(Open Weights) 및 공개 데이터(Open Data)

Molmo 및 PixMo: 최첨단 멀티모달 모델을 위한 공개 가중치(Open Weights) 및 공개 데이터(Open Data) 논문(2024년 9월 25일)은 언어 전용 OLMo LLM과 유사하게 모델 가중치(model weight)뿐만 아니라 데이터셋(dataset)과 소스 코드(source code)까지 오픈 소스(open source)로 공개하겠다고 약속했기 때문에 특히 주목할 만합니다. (이는 LLM 연구에 매우 중요합니다. 정확한 훈련 절차(training procedure)와 코드를 살펴보고, 어블레이션 연구(ablation study)를 수행하며, 동일한 데이터셋(dataset)에서 결과를 재현할 수 있기 때문입니다.)

논문 제목에 두 가지 이름이 있는 이유가 궁금하다면, Molmo는 모델(멀티모달 오픈 언어 모델, Multimodal Open Language Model)을 의미하고, PixMo(Pixels for Molmo)는 데이터셋(dataset)입니다.

Molmo 디코더 전용(decoder-only) 접근 방식(방법 A)의 그림. Molmo 및 PixMo: 최첨단 멀티모달 모델을 위한 공개 가중치(Open Weights) 및 공개 데이터(Open Data) 논문에서 각색된 주석이 달린 그림: https://www.arxiv.org/abs/2409.17146.

위 그림에서 설명된 대로, 이미지 인코더(image encoder)는 상용 비전 트랜스포머(off-the-shelf vision transformer), 특히 CLIP을 사용합니다. 여기서 "커넥터(connector)"라는 용어는 이미지 특징(image feature)을 언어 모델(language model)과 정렬하는 "프로젝터(projector)"를 의미합니다. Molmo는 여러 사전 학습(pretraining) 단계를 피하고, 기본 LLM, 커넥터(connector), 이미지 인코더(image encoder)를 포함한 모든 매개변수(parameter)를 통합된 접근 방식(unified approach)으로 업데이트하는 간단한 파이프라인(pipeline)을 선택하여 훈련(training) 프로세스를 간소화합니다.

Molmo 팀은 기본 LLM(base LLM)에 대해 여러 옵션을 제공합니다.
*   OLMo-7B-1024 (완전히 오픈된 모델 백본(model backbone))
*   OLMoE-1B-7B (전문가 혼합 아키텍처(mixture-of-experts architecture); 가장 효율적인 모델)
*   Qwen2 7B (OLMo-7B-1024보다 성능이 좋은 공개 가중치(open-weight) 모델)
*   Qwen2 72B (공개 가중치(open-weight) 모델이자 최고의 성능을 보이는 모델)

#### 4.3 NVLM: 오픈 프론티어 클래스 멀티모달 LLM(Open Frontier-Class Multimodal LLMs)

NVIDIA의 NVLM: 오픈 프론티어 클래스 멀티모달 LLM 논문(2024년 9월 17일)은 단일 접근 방식에 집중하기보다는, 두 가지 방법, 즉 방법 A: 통합 임베딩 디코더 구조(Unified Embedding Decoder Architecture) ("디코더 전용(decoder-only) 아키텍처(architecture)," NVLM-D)와 방법 B: 교차 모달리티 어텐션 구조(Cross-Modality Attention Architecture) ("교차 어텐션 기반(cross-attention-based) 아키텍처(architecture)," NVLM-X)를 모두 탐구한다는 점에서 특히 흥미롭습니다. 또한, 그들은 하이브리드 접근 방식(hybrid approach)(NVLM-H)을 개발하고 세 가지 방법 모두에 대한 공정한 비교(apples-to-apples comparison)를 제공합니다.

세 가지 멀티모달 접근 방식 개요. (NVLM: 오픈 프론티어 클래스 멀티모달 LLM 논문에서 주석이 달린 그림: https://arxiv.org/abs/2409.11402)

아래 그림에 요약된 대로, NVLM-D는 방법 A에 해당하고, NVLM-X는 이전에 논의된 방법 B에 해당합니다. 하이브리드 모델(hybrid model)(NVLM-H)의 개념은 두 가지 방법의 장점을 결합하는 것입니다. 이미지 썸네일(image thumbnail)이 입력으로 제공된 다음, 더 미세한 고해상도 세부 정보(high-resolution detail)를 캡처하기 위해 교차 어텐션(cross-attention)을 통해 동적 패치 수(dynamic number of patches)가 전달됩니다.

요약하자면, 연구팀은 다음을 발견했습니다.
*   NVLM-X는 고해상도 이미지에 대해 우수한 계산 효율성(computational efficiency)을 보여줍니다.
*   NVLM-D는 OCR 관련 작업(OCR-related task)에서 더 높은 정확도를 달성합니다.
*   NVLM-H는 두 가지 방법의 장점을 결합합니다.

Molmo 및 다른 접근 방식과 유사하게, 그들은 처음부터 멀티모달 모델(multimodal model)을 사전 학습(pretraining)하는 대신 텍스트 전용 LLM(text-only LLM)으로 시작합니다(일반적으로 이것이 더 나은 성능을 보이기 때문입니다). 또한, 기본 LLM(base LLM) 대신 명령어 미세 조정된 LLM(instruction-tuned LLM)을 사용합니다. 특히, 백본 LLM(backbone LLM)은 Qwen2-72B-Instruct입니다(제가 아는 한, Molmo는 Qwen2-72B 기본 모델(base model)을 사용했습니다).

NVLM-D 접근 방식에서 모든 LLM 매개변수(parameter)를 훈련(training)하는 동안, 그들은 NVLM-X의 경우 원래 LLM 매개변수(parameter)를 고정하고 사전 학습(pretraining) 및 명령어 미세 조정(instruction finetuning) 모두에서 교차 어텐션 계층(cross-attention layer)만 훈련(training)하는 것이 잘 작동한다는 것을 발견했습니다. 이미지 인코더(image encoder)의 경우, 일반적인 CLIP 모델을 사용하는 대신 InternViT-6B를 사용하며, 이는 모든 단계에서 고정된 상태로 유지됩니다. 프로젝터(projector)는 단일 선형 계층(linear layer)이 아닌 다층 퍼셉트론(multilayer perceptron)입니다.

#### 4.4 Qwen2-VL: 모든 해상도에서 비전-언어 모델의 세계 인식 향상

이전 두 논문 및 모델인 Molmo와 NVLM은 Qwen2-72B LLM을 기반으로 했습니다. 이 논문에서 Qwen 연구팀 자체는 멀티모달 LLM인 Qwen2-VL: 모든 해상도에서 비전-언어 모델의 세계 인식 향상(2024년 10월 3일)을 발표합니다. 이 작업의 핵심은 그들의 이른바 "단순 동적 해상도(Naive Dynamic Resolution)" 메커니즘(여기서 "단순(naive)"이라는 용어는 의도적이며 "네이티브(native)"의 오타가 아니지만, "네이티브(native)"도 적합할 수 있습니다)입니다. 이 메커니즘은 모델이 간단한 다운샘플링(downsampling) 없이 다양한 해상도의 이미지를 처리할 수 있도록 하여 원본 해상도(original resolution)로 이미지를 입력할 수 있게 합니다.

다양한 해상도의 입력 이미지를 기본적으로 처리할 수 있는 멀티모달 Qwen 모델의 개요. (Qwen2-VL 논문에서 주석이 달린 그림: https://arxiv.org/abs/2409.12191)

네이티브 해상도 입력(native resolution input)은 원래 절대 위치 임베딩(absolute position embedding)을 제거하고 2D-RoPE를 도입하여 수정된 ViT를 통해 구현됩니다. 그들은 6억 7천 5백만 매개변수(parameter)를 가진 고전적인 비전 인코더(vision encoder)와 아래 표에 표시된 다양한 크기의 LLM 백본(LLM backbone)을 사용했습니다.

다양한 Qwen2-VL 모델의 구성 요소. (Qwen2-VL 논문에서 주석이 달린 그림: https://arxiv.org/abs/2409.12191)

훈련(training) 자체는 3단계로 구성됩니다. (1) 이미지 인코더(image encoder)만 사전 학습(pretraining), (2) 모든 매개변수(parameter)(LLM 포함) 고정 해제, (3) 이미지 인코더(image encoder) 고정 및 LLM만 명령어 미세 조정(instruction-finetuning).

#### 4.5 Pixtral 12B

방법 A: 통합 임베딩 디코더 구조(Unified Embedding Decoder Architecture) 접근 방식을 활용하는 Pixtral 12B(2024년 9월 17일)는 Mistral AI의 첫 번째 멀티모달 모델(multimodal model)입니다. 아쉽게도 기술 논문(technical paper)이나 보고서(report)는 공개되지 않았지만, Mistral 팀은 블로그 게시물(blog post)을 통해 몇 가지 흥미로운 정보를 공유했습니다. 특히 주목할 만한 점은, 그들이 사전 학습된(pretrained) 이미지 인코더(image encoder)를 사용하지 않고, 대신 4억 매개변수(parameter)를 가진 인코더(encoder)를 처음부터 훈련(training)하기로 결정했다는 것입니다. LLM 백본(LLM backbone)으로는 120억 매개변수(parameter) Mistral NeMo 모델을 활용했습니다. Qwen2-VL과 유사하게, Pixtral 또한 아래 그림에 설명된 대로 가변 이미지 크기(variable image size)를 기본적으로 지원합니다.

Pixtral이 다양한 크기의 이미지를 처리하는 방법 그림. (Pixtral 블로그 게시물에서 주석이 달린 그림: https://mistral.ai/news/pixtral-12b/)

#### 4.6 MM1.5: 멀티모달 LLM 미세 조정의 방법, 분석 및 통찰력

MM1.5: 멀티모달 LLM 미세 조정의 방법, 분석 및 통찰력 논문(2024년 9월 30일)은 실용적인 팁을 제공하고 Molmo와 유사한 밀집 모델(dense model)과 함께 전문가 혼합 멀티모달 모델(mixture-of-experts multimodal model)을 소개합니다. 이 모델들은 10억에서 300억 매개변수(parameter)에 이르는 넓은 크기 범위를 가집니다. 이 논문에 설명된 모델들은 방법 A, 즉 멀티모달 학습을 위해 입력을 효과적으로 구조화하는 통합 임베딩 트랜스포머 아키텍처(Unified Embedding Transformer Architecture)에 초점을 맞춥니다. 또한, 이 논문은 데이터 혼합(data mixture)과 좌표 토큰(coordinate token) 사용의 효과를 살펴보는 일련의 흥미로운 어블레이션 연구(ablation study)를 포함합니다.

바운딩 박스(bounding box)를 나타내는 추가 좌표 토큰(coordinate token)을 포함하는 MM1.5 접근 방식의 그림. (MM1.5 논문에서 주석이 달린 그림: https://arxiv.org/abs/2409.20566.)

#### 4.7 Aria: 오픈 멀티모달 네이티브 전문가 혼합 모델

Aria: 오픈 멀티모달 네이티브 전문가 혼합 모델 논문(2024년 10월 8일)은 Molmo 및 MM1.5 라인업의 변형 중 하나와 유사한 또 다른 전문가 혼합 모델(mixture-of-experts model) 접근 방식을 소개합니다. Aria 모델은 249억 매개변수(parameter)를 가지며, 텍스트 토큰(text token)당 35억 매개변수(parameter)가 할당됩니다. 이미지 인코더(image encoder)(SigLIP)는 4억 3천 8백만 매개변수(parameter)를 가집니다. 이 모델은 다음과 같은 전체 훈련 절차(training procedure)를 가진 교차 어텐션(cross-attention) 접근 방식을 기반으로 합니다.
*   LLM 백본(LLM backbone)을 처음부터 완전히 훈련(training)합니다.
*   LLM 백본(LLM backbone)과 비전 인코더(vision encoder)를 모두 사전 학습(pretraining)합니다.

#### 4.8 Baichuan-Omni

Baichuan-Omni 기술 보고서(2024년 10월 11일)는 아래 그림에 표시된 대로 방법 A: 통합 임베딩 디코더 구조(Unified Embedding Decoder Architecture) 접근 방식을 기반으로 하는 70억 매개변수(parameter) 멀티모달 LLM인 Baichuan-Omni를 소개합니다.

다양한 입력 모달리티(input modality)를 처리할 수 있는 Baichuan-Omni 모델의 개요. (Baichuan-Omni 논문에서 주석이 달린 그림: https://arxiv.org/abs/2410.08565)

Baichuan-Omni의 훈련(training) 프로세스는 3단계 접근 방식을 포함합니다.
*   **프로젝터(projector) 훈련(training)**: 처음에는 프로젝터(projector)만 훈련(training)되고, 비전 인코더(vision encoder)와 언어 모델(LLM)은 모두 고정된 상태로 유지됩니다.
*   **비전 인코더(vision encoder) 훈련(training)**: 다음으로, 비전 인코더(vision encoder)는 고정 해제되고 훈련(training)되며, LLM은 여전히 고정된 상태입니다.
*   **전체 모델 훈련(training)**: 마지막으로, LLM은 고정 해제되어 전체 모델이 엔드투엔드(end-to-end)로 훈련(training)될 수 있도록 합니다.

이 모델은 SigLIP 비전 인코더(vision encoder)를 활용하고, 다운샘플링 기법(down-sampling technique)을 통해 고해상도 이미지를 처리하기 위해 AnyRes 모듈을 통합합니다. 보고서에는 LLM 백본(LLM backbone)이 명시적으로 지정되어 있지 않지만, 모델의 매개변수 크기(parameter size)와 명명 규칙(naming convention)을 고려할 때 Baichuan 7B LLM을 기반으로 할 가능성이 높습니다.

#### 4.9 Emu3: 다음 토큰 예측만 있으면 됩니다

Emu3: 다음 토큰 예측만 있으면 됩니다 논문(2024년 9월 27일)은 이미지 생성(image generation)을 위한 확산 모델(diffusion model)에 대한 설득력 있는 대안을 제시하며, 이는 트랜스포머 기반 디코더 아키텍처(transformer-based decoder architecture)에만 기반합니다. 비록 고전적인 의미의 멀티모달 LLM(즉, 생성보다는 이미지 이해(image understanding)에 초점을 맞춘 모델)은 아니지만, Emu3는 트랜스포머 디코더(transformer decoder)를 사용하여 이미지 생성(image generation)이 가능하다는 것을 보여주기 때문에 매우 흥미롭습니다. 이미지 생성(image generation)은 일반적으로 확산 방법(diffusion method)이 지배하는 작업입니다. (그러나 Autoregressive Model Beats Diffusion: Llama for Scalable Image Generation과 같은 다른 유사한 접근 방식이 이전에도 있었다는 점에 유의하세요.)

Emu3는 확산 모델(diffusion model)의 대안으로 이미지 생성(image generation)을 위한 LLM입니다. (Emu3 논문에서 주석이 달린 그림: https://arxiv.org/abs/2409.18869)

연구자들은 Emu3를 처음부터 훈련(training)한 다음, 직접 선호 최적화(Direct Preference Optimization, DPO)를 사용하여 모델을 인간 선호도에 맞추었습니다. 아키텍처(architecture)에는 SBER-MoVQGAN에서 영감을 받은 비전 토크나이저(vision tokenizer)가 포함됩니다. 핵심 LLM 아키텍처(core LLM architecture)는 Llama 2를 기반으로 하지만, 처음부터 완전히 훈련(training)되었습니다.

#### 4.10 Janus: 통합 멀티모달 이해 및 생성을 위한 시각 인코딩 분리

우리는 이전에 이미지 이해(image understanding)를 위한 멀티모달 LLM에 초점을 맞췄고, 위에서 Emu 3를 통한 이미지 생성(image generation)의 한 가지 예를 보았습니다. 이제 Janus: 통합 멀티모달 이해 및 생성을 위한 시각 인코딩 분리 논문(2024년 10월 17일)은 단일 LLM 백본(LLM backbone) 내에서 멀티모달 이해(multimodal understanding) 및 생성 작업(generation task)을 통합하는 프레임워크(framework)를 소개합니다.

Janus의 핵심 기능은 이해(understanding) 및 생성 작업(generation task)의 고유한 요구 사항(distinct requirement)을 해결하기 위한 시각 인코딩 경로(visual encoding pathway)의 분리입니다. 연구자들은 이미지 이해(image understanding) 작업에는 고차원 의미론적 표현(high-dimensional semantic representation)이 필요하고, 생성 작업(generation task)에는 이미지의 상세한 지역 정보(detailed local information)와 전역 일관성(global consistency)이 필요하다고 주장합니다. 이러한 경로를 분리함으로써 Janus는 이러한 상이한 요구 사항을 효과적으로 관리합니다.

이 모델은 Baichuan-Omni에서 사용된 것과 유사한 SigLIP 비전 인코더(vision encoder)를 사용하여 시각적 입력을 처리합니다. 이미지 생성(image generation)을 위해 벡터 양자화(Vector Quantized, VQ) 토크나이저(tokenizer)를 사용하여 생성 프로세스를 처리합니다. Janus의 기본 LLM(base LLM)은 13억 매개변수(parameter)를 가진 DeepSeek-LLM입니다.

Janus에서 사용된 통합 디코더 전용(decoder-only) 프레임워크(framework)의 개요. (Janus 논문에서 주석이 달린 그림: https://arxiv.org/abs/2410.13848.)

이 이미지의 모델 훈련(training) 프로세스는 아래 그림에 표시된 대로 3단계로 진행됩니다.

Janus 모델의 3단계 훈련(training) 프로세스 그림. (Janus 논문에서 주석이 달린 그림: https://arxiv.org/abs/2410.13848)

1단계에서는 LLM, 이해(understanding) 및 생성 인코더(generation encoder)가 고정된 상태에서 프로젝터 계층(projector layer)과 이미지 출력 계층(image output layer)만 훈련(training)됩니다. 2단계에서는 LLM 백본(LLM backbone)과 텍스트 출력 계층(text output layer)이 고정 해제되어 이해(understanding) 및 생성 작업(generation task) 전반에 걸쳐 통합 사전 학습(unified pretraining)이 가능해집니다. 마지막으로 3단계에서는 SigLIP 이미지 인코더(image encoder)를 포함한 전체 모델이 고정 해제되어 지도 미세 조정(supervised fine-tuning)을 위해 사용되며, 모델이 멀티모달 기능(multimodal capability)을 완전히 통합하고 개선할 수 있도록 합니다.

### 5. 멀티모달 LLM의 미래와 도전 과제

지금까지 다양한 멀티모달 LLM 아키텍처와 최신 연구 동향을 살펴보았습니다. 이러한 발전은 AI가 인간의 복잡한 세상을 더 깊이 이해하고 상호작용하는 데 중요한 발판을 마련하고 있습니다. 그러나 이 기술이 대중화되고 더욱 강력해지기 위해서는 몇 가지 중요한 도전 과제를 해결해야 합니다.

**데이터의 다양성과 품질:** 멀티모달 LLM의 성능은 훈련 데이터의 양과 질에 크게 좌우됩니다. 텍스트, 이미지, 오디오, 비디오 등 다양한 모달리티를 포괄하면서도 편향되지 않고 고품질의 대규모 데이터셋을 구축하는 것은 여전히 큰 과제입니다. 특히, 모달리티 간의 의미론적 연결을 학습시키기 위한 정교한 주석(annotation) 작업이 필요합니다.

**계산 효율성:** 멀티모달 모델은 텍스트 전용 모델보다 훨씬 많은 매개변수와 복잡한 구조를 가지므로, 훈련 및 추론에 막대한 계산 자원이 필요합니다. NVLM과 같은 연구에서 다루듯이, 고해상도 이미지를 효율적으로 처리하면서도 성능을 유지하는 방법론에 대한 지속적인 연구가 필수적입니다. 경량화된 모델 아키텍처나 효율적인 학습 기법 개발이 중요합니다.

**윤리적 고려사항 및 안전성:** 멀티모달 LLM은 이미지 생성, 비디오 분석 등 강력한 기능을 제공하는 만큼, 오용될 위험도 큽니다. 딥페이크(deepfake) 생성, 편향된 정보 확산, 사생활 침해 등의 윤리적 문제에 대한 심도 있는 논의와 기술적, 정책적 방안 마련이 시급합니다. 모델의 안전성과 책임 있는 개발은 기술 발전과 함께 최우선적으로 고려되어야 할 부분입니다.

**통합적인 평가 지표:** 현재 멀티모달 LLM의 성능을 평가하는 데 사용되는 벤치마크(benchmark)는 특정 작업에 국한되거나 데이터 오염(data contamination) 문제에서 자유롭지 못한 경우가 많습니다. 다양한 모달리티와 복합적인 작업에 걸쳐 모델의 실제 이해 능력과 일반화 성능을 객관적으로 측정할 수 있는 통합적이고 신뢰할 수 있는 평가 지표 개발이 필요합니다. NVIDIA 팀이 NVLM을 통해 디코더 전용(decoder-only) 및 교차 어텐션(cross-attention) 방식 간의 비교를 가능하게 한 노력은 이러한 방향에서 긍정적인 선례가 됩니다.

멀티모달 LLM은 단순한 기술적 혁신을 넘어, 인간과 기계의 상호작용 방식을 근본적으로 변화시킬 잠재력을 가지고 있습니다. 이러한 도전 과제들을 해결해 나감으로써, 우리는 더욱 지능적이고 유용한 AI 시스템을 구축할 수 있을 것입니다.

### 결론

눈치채셨겠지만, 저는 모델링(modeling) 및 계산 성능 비교(computational performance comparison)를 거의 완전히 건너뛰었습니다. 첫째, LLM 및 멀티모달 LLM의 공개 벤치마크(public benchmark) 성능을 비교하는 것은 일반적인 데이터 오염(data contamination) 때문에 어렵습니다. 즉, 테스트 데이터(test data)가 훈련 데이터(training data)에 포함되었을 수 있습니다. 또한, 아키텍처 구성 요소(architectural component)가 너무 다양하여 공정한 비교(apples-to-apples comparison)를 하기가 어렵습니다. 따라서 적어도 디코더 전용(decoder-only) 및 교차 어텐션(cross-attention) 접근 방식 간의 비교를 가능하게 한 다양한 버전의 NVLM을 개발한 NVIDIA 팀에게 큰 찬사를 보냅니다.

어쨌든, 이 글의 핵심 요점(main takeaway)은 멀티모달 LLM이 다양한 방식으로 성공적으로 구축될 수 있다는 것입니다. 아래는 이 글에서 다룬 모델들의 다양한 구성 요소를 요약한 그림입니다.

이 글에서 다룬 다양한 모델과 그 하위 구성 요소 및 훈련(training) 접근 방식에 대한 개요.

이 글을 읽는 것이 교육적(educational)이었기를 바라며, 이제 멀티모달 LLM이 어떻게 작동하는지에 대해 더 잘 이해하게 되셨기를 바랍니다!

이 잡지는 개인적인 열정 프로젝트(personal passion project)입니다. 저를 지원하고 싶은 분들은 제 책 "Build a Large Language Model (From Scratch)"을 구매해 주시면 감사하겠습니다. (이 책은 LLM이 어떻게 작동하는지 다른 곳에서는 찾아볼 수 없는 수준의 세부 사항으로 설명하므로, 이 책에서 많은 것을 얻으실 것이라고 확신합니다.)

[Build a Large Language Model (From Scratch) now available on Amazon]

책을 읽으셨고 잠시 시간을 내실 수 있다면, 짧은 리뷰를 남겨주시면 정말 감사하겠습니다. 그것은 저희 저자들에게 큰 도움이 됩니다! 또는, 최근에 Substack에 유료 구독 옵션(paid subscription option)을 활성화하여 이 잡지를 직접 지원할 수도 있습니다. 여러분의 지원은 큰 의미가 있습니다! 감사합니다!

[Subscribe]