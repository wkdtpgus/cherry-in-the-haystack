두 달은 실로 역동적인 기간이었습니다. 인공지능 연구 분야에서는 또다시 눈부신 진보가 거듭되었고, 이 혁신적인 영역에 두 차례의 권위 있는 영예가 주어졌으며, 다수의 매력적인 학술 자료들이 공개되었습니다. 특히 Meta AI는 최신 Llama 3.2 모델 제품군을 공개했는데, 여기에는 10억 및 30억 매개변수 규모의 거대 언어 모델(large language model)을 위한 개방형 가중치(open-weight) 버전과 두 개의 다중 모드 모델(multimodal model)이 포함됩니다. 본문에서는 다중 모드 LLM(multimodal LLM)이 어떻게 작동하는지 상세히 설명하고자 합니다. 나아가, 최근 몇 주간 발표된 약 12개의 다른 최신 다중 모드 연구 논문과 모델(Llama 3.2 포함)들을 심층적으로 분석하고 요약하여, 각 접근 방식의 특징과 차이점을 비교해 볼 것입니다. (목차를 확인하시려면 화면 좌측의 줄 아이콘을 눌러주세요.)

다양한 형태의 입력 양식(input modality)(청각, 문자, 시각, 영상)을 받아들이고 문자열을 출력 양식(output modality)으로 반환하는 다중 모드 LLM의 구조도. 이러한 모델은 인공지능이 세상을 인식하고 상호작용하는 방식을 근본적으로 변화시킬 잠재력을 지니고 있습니다.

이야기를 시작하기 전에, 개인적으로 기쁜 소식이 있습니다! 저의 저서 "Build A Large Language Model (From Scratch)"이 드디어 Amazon에서 만나볼 수 있게 되었습니다!

[Build a Large Language Model (From Scratch) now available on Amazon]

이 책을 집필하는 과정은 엄청난 노력을 요구했으며, 지난 2년, 특히 최근 몇 달 동안 많은 친절한 독자분들께서 보내주신 아낌없는 지지와 격려, 그리고 소중한 피드백에 진심으로 감사드립니다. 여러분 모두에게 감사드리며, 저자로서 이 책이 독자 여러분의 경력에 긍정적인 변화를 가져왔다는 소식만큼 큰 동기 부여는 없을 것입니다! 책을 완독하신 후에도 더 많은 지식을 갈망하시는 분들을 위해, 계속 주목해 주세요! 앞으로 몇 달 내에 GitHub 저장소에 추가 자료들을 업데이트할 예정입니다.

추신. 만약 이 책을 읽으셨다면, 짧은 서평을 남겨주시면 정말 큰 도움이 될 것입니다. 이는 저희 저자들에게 큰 힘이 됩니다!

### 1. 다중 모드 LLM의 활용 영역

다중 모드 대규모 언어 모델(multimodal large language model)은 어떤 존재일까요? 서문에서 언급했듯이, 이러한 모델들은 다양한 형태의 데이터를 수용할 수 있는 거대 언어 처리 시스템입니다. 이때 각각의 '모드(modality)'는 기존 언어 모델이 다루는 문자열 외에 음성, 화상, 동영상 등을 포함하는 특정 데이터 양식을 지칭합니다. 본 글에서는 주로 문자열 입력과 함께 시각적 모드(image modality)에 중점을 두고 논의를 진행할 것입니다.

다중 모드 LLM의 전형적이고 직관적인 활용 사례는 그림 설명 생성(image captioning)입니다. 대상 이미지를 입력으로 주면, 모델은 다음 삽화처럼 해당 그림의 내용을 서술하는 문장을 만들어냅니다. 이는 인공지능이 시각 정보를 해석하고 자연어로 표현하는 능력을 보여주는 핵심적인 예시입니다.

밈(meme)을 해석하여 설명하는 다중 모드 LLM의 사용 예시. 이 외에도 시각적 질의응답(Visual Question Answering, VQA), 이미지 기반 스토리텔링 등 다양한 응용이 가능합니다.

물론, 이 외에도 수많은 활용 분야가 존재합니다. 예를 들어, 제가 특히 좋아하는 기능 중 하나는 PDF 문서 내 표에서 정보를 추출하여 LaTeX 또는 Markdown 형식으로 변환하는 것입니다. 이는 단순한 텍스트 인식을 넘어 문서 구조를 이해하고 특정 형식으로 재구성하는 고도의 능력을 요구합니다. 또한, 로봇 공학 분야에서는 주변 환경을 시각적으로 인식하고 그에 대한 언어적 지시를 이해하여 복잡한 작업을 수행하는 데 다중 모드 LLM이 활용될 수 있습니다.

### 2. 다중 모드 LLM 구축을 위한 핵심 접근법

다중 모드 LLM을 개발하는 데에는 두 가지 주요한 접근 방식이 있습니다. 첫째, 통합 임베딩 디코더 구조(Unified Embedding Decoder Architecture) 방식과 둘째, 교차 모드 어텐션 구조(Cross-modality Attention Architecture) 방식입니다. (참고로, 저는 이 기술들에 대한 공식적인 명칭이 아직 확립되지 않았다고 생각합니다. 혹시 더 적절하고 간결한 설명이 있다면 알려주세요. 예를 들어, '디코더 전용(decoder-only)' 및 '교차 어텐션 기반(cross-attention-based)' 접근 방식이 될 수도 있습니다.) 이 두 가지 방법론은 다중 모드 데이터를 단일 언어 모델 내에서 어떻게 효과적으로 통합할 것인가에 대한 서로 다른 철학을 반영합니다.

다중 모드 LLM 구조 개발을 위한 두 가지 주요 접근 방식. 각 방식은 데이터 통합 및 처리 방식에서 차이를 보이며, 이는 모델의 성능과 효율성에 영향을 미칩니다.

위 그림에서 볼 수 있듯이, 통합 임베딩 디코더 구조(Unified Embedding-Decoder Architecture)는 GPT-2나 Llama 3.2와 같이 수정되지 않은 LLM 구조와 매우 유사하게 단일 디코더 모델(decoder model)을 활용합니다. 이 접근법에서는 시각 정보가 원본 문자열 토큰(text token)과 동일한 임베딩 차원(embedding size)을 가지는 토큰으로 변환되어, LLM이 결합(concatenation) 후 문자열 및 시각 입력 토큰(image input token)을 함께 처리할 수 있도록 설계됩니다. 반면, 교차 모드 어텐션 구조(Cross-Modality Attention Architecture)는 교차 어텐션 메커니즘(cross-attention mechanism)을 사용하여 어텐션 계층(attention layer) 내에서 시각 및 문자열 임베딩(embedding)을 직접적으로 연동합니다. 이 방식은 각 모드의 특성을 보존하면서도 상호작용을 가능하게 합니다.

다음 절에서는 이러한 방법들이 개념적 차원에서 어떻게 기능하는지 심도 있게 탐구할 것입니다. 그 후, 다중 모드 LLM에 대한 최신 연구 논문들을 검토하여 실제 적용 사례와 구현 방식을 분석할 것입니다.

#### 2.1 방법 A: 통합 임베딩 디코더 구조

아래 그림에 다시 설명된 통합 임베딩 디코더 구조(unified embedding decoder architecture)부터 논의를 시작하겠습니다. 이 방식은 언어 모델의 기존 구조를 최대한 활용하면서 시각 정보를 효과적으로 통합하려는 시도입니다.

시각 토큰(image token) 및 문자열 토큰(text token) 임베딩(embedding)으로 구성된 입력을 받는 수정되지 않은 디코더 스타일 LLM(GPT-2, Phi-3, Gemma 또는 Llama 3.2와 같은)인 통합 임베딩 디코더 구조의 그림. 이는 다양한 형태의 정보가 동일한 언어 모델 파이프라인을 통과하게 함으로써 통합된 이해를 가능하게 합니다.

통합 임베딩 디코더 구조(unified embedding-decoder architecture)에서 시각 정보는 표준 문자열 전용 LLM(text-only LLM)에서 입력 문자열이 임베딩으로 변환되는 방식과 유사하게 임베딩 벡터(embedding vector)로 변환됩니다. 문자열을 처리하는 일반적인 문자열 전용 LLM의 경우, 문자열 입력은 일반적으로 토큰화(tokenized)된 다음(예: 바이트 쌍 인코딩(Byte-Pair Encoding) 사용), 아래 그림과 같이 임베딩 계층(embedding layer)을 통과합니다. 이 과정은 추상적인 의미를 지닌 숫자 벡터로 변환하여 모델이 이해하고 처리할 수 있도록 합니다.

문자열을 토큰화(tokenizing)하고 토큰 임베딩 벡터(token embedding vector)로 변환하는 표준 프로세스 그림. 이 벡터는 훈련(training) 및 추론(inference) 중에 LLM으로 전달됩니다. 이러한 임베딩은 단어의 의미론적 및 구문론적 정보를 함축하고 있습니다.

##### 2.1.1 이미지 인코더(Image encoders)의 원리

문자열의 토큰화(tokenization) 및 임베딩(embedding)과 유사하게, 시각 정보의 임베딩(image embedding)은 아래 그림과 같이 이미지 인코더(image encoder) 모듈(토크나이저(tokenizer) 대신)을 사용하여 생성됩니다. 이 인코더는 원본 이미지를 언어 모델이 이해할 수 있는 고차원 벡터 표현으로 변환하는 역할을 합니다.

시각 정보를 이미지 패치 임베딩(image patch embedding)으로 인코딩하는 과정 그림. 이 과정은 이미지를 작은 조각으로 분할하고 각 조각의 특징을 추출하는 방식으로 이루어집니다.

위 그림에 나타난 이미지 인코더(image encoder) 내부에서는 어떤 과정이 일어날까요? 시각 정보를 처리하기 위해 먼저 이미지를 더 작은 조각(patch)으로 분할합니다. 이는 문자열 토큰화(tokenization) 과정에서 단어를 부분 단어(subword)로 나누는 것과 매우 유사한 개념입니다. 이 조각들은 아래 그림과 같이 사전 학습된(pretrained) 비전 트랜스포머(vision transformer, ViT)에 의해 인코딩됩니다. ViT는 대규모 이미지 데이터셋에서 학습되어 일반적인 시각적 특징을 효과적으로 포착할 수 있습니다.

"An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale (2020)"에서 제안된 모델과 유사한 고전적인 비전 트랜스포머(ViT) 설정 그림. ViT는 종종 분류 작업(classification task)에 사용되므로 위 그림에 분류 헤드(classification head)를 포함했습니다. 그러나 이 경우에는 시각 인코더(image encoder)의 특징 추출 부분만 활용됩니다.

##### 2.1.2 선형 투영(linear projection) 모듈의 역할

이전 그림에 명시된 "선형 투영(linear projection)"은 단일 선형 계층(linear layer)(즉, 완전 연결 계층(fully connected layer))으로 구성됩니다. 이 계층의 목표는 벡터 형태로 평탄화된 시각 조각(image patch)들을 트랜스포머 인코더(transformer encoder)와 호환되는 임베딩 차원(embedding size)으로 사상하는 것입니다. 이 선형 투영(linear projection)의 기능은 아래 그림에 상세히 설명되어 있습니다. 이는 서로 다른 형태의 데이터가 동일한 표현 공간에서 상호작용할 수 있도록 차원을 조정하는 중요한 단계입니다.

256차원 벡터로 평탄화된 시각 조각(image patch)이 768차원 벡터로 상향 투영됩니다. 이 과정은 저차원 시각 특징을 고차원 의미론적 표현으로 확장하는 역할을 합니다.

256차원에서 768차원 임베딩 공간(embedding space)으로 평탄화된 시각 조각(image patch)을 투영하는 선형 투영(linear projection) 계층 그림. 이 변환은 시각 정보가 언어 모델의 내부 표현과 일관되도록 만듭니다.

코드 예시를 선호하는 분들을 위해, PyTorch 코드에서는 시각 조각(image patch)에 대한 선형 투영(linear projection)을 다음과 같이 구현할 수 있습니다.

```python
import torch

class PatchProjectionLayer(torch.nn.Module):
    def __init__(self, patch_size, num_channels, embedding_dim):
        super().__init__()
        self.patch_size = patch_size
        self.num_channels = num_channels
        self.embedding_dim = embedding_dim
        self.projection = torch.nn.Linear(
            patch_size * patch_size * num_channels,
            embedding_dim
        )

    def forward(self, x):
        batch_size, num_patches, channels, height, width = x.size()
        x = x.view(batch_size, num_patches, -1) # Flatten each patch
        x = self.projection(x) # Project each flattened patch
        return x

# Example Usage:
batch_size = 1
num_patches = 9 # Total patches per image
patch_size = 16 # 16x16 pixels per patch
num_channels = 3 # RGB image
embedding_dim = 768 # Size of the embedding vector

projection_layer = PatchProjectionLayer(patch_size, num_channels, embedding_dim)

patches = torch.rand(
    batch_size, num_patches, num_channels, patch_size, patch_size
)

projected_embeddings = projection_layer(patches)
print(projected_embeddings.shape)
# This prints
# torch.Size([1, 9, 768])
```

혹시 제 "Machine Learning Q and AI" 책을 읽으셨다면, 선형 계층(linear layer)을 수학적으로 동등하게 구현할 수 있는 컨볼루션 연산(convolution operation)으로 대체하는 방법이 있다는 것을 아실 것입니다. 여기서는 조각(patch) 생성과 투영(projection)을 두 줄의 코드로 결합할 수 있어 특히 유용합니다. 이는 더 효율적인 구현과 계산 비용 절감을 가능하게 합니다.

```python
layer = torch.nn.Conv2d(3, 768, kernel_size=(16, 16), stride=(16, 16))
image = torch.rand(batch_size, 3, 48, 48)
projected_patches = layer(image)
print(projected_patches.flatten(-2).transpose(-1, -2).shape)
# This prints
# torch.Size([1, 9, 768])
```

##### 2.1.3 시각 정보와 문자열 토큰화(tokenization)의 정렬

이제 시각 인코더(image encoder)(및 인코더의 일부인 선형 투영(linear projection))의 목적에 대해 간략히 논의했으므로, 이전의 문자열 토큰화(text tokenization) 비유로 돌아가 아래 그림에 묘사된 문자열 및 시각 정보의 토큰화(tokenization)와 임베딩(embedding) 과정을 나란히 살펴보겠습니다. 이 비교를 통해 두 모드 간의 유사점과 차이점을 명확히 이해할 수 있습니다.

시각 정보 토큰화(tokenization) 및 임베딩(embedding)(왼쪽)과 문자열 토큰화(tokenization) 및 임베딩(embedding)(오른쪽)을 나란히 비교. 두 과정 모두 원시 데이터를 모델이 처리할 수 있는 추상적인 벡터 표현으로 변환합니다.

위 그림에서 볼 수 있듯이, 저는 시각 인코더(image encoder) 뒤에 추가적인 프로젝터 모듈(projector module)을 포함했습니다. 이 프로젝터(projector)는 일반적으로 이전에 설명한 것과 유사한 또 다른 선형 투영(linear projection) 계층일 뿐입니다. 그 목적은 아래 그림에 설명된 대로 시각 인코더(image encoder)의 출력을 임베딩된 문자열 토큰(text token)의 차원과 일치하는 차원으로 사상하는 것입니다. (나중에 살펴보겠지만, 프로젝터(projector)는 때때로 어댑터(adapter) 또는 연결자(connector)라고도 불립니다.) 이 차원 일치는 언어 모델이 두 가지 모드의 정보를 동일한 표현 공간에서 처리할 수 있도록 하는 데 필수적입니다.

시각 정보 토큰화(tokenization)와 문자열 토큰화(tokenization)의 또 다른 나란한 비교. 여기서 프로젝터(projector)의 역할은 문자열 토큰 임베딩(text token embedding) 차원을 일치시키는 것입니다. 이는 서로 다른 모달리티의 정보를 언어 모델이 통합적으로 이해할 수 있도록 돕습니다.

이제 시각 조각 임베딩(image patch embedding)이 문자열 토큰 임베딩(text token embedding)과 동일한 임베딩 차원(embedding dimension)을 가지므로, 이 절 시작 부분의 그림에 표시된 대로 LLM의 입력으로 간단히 연결(concatenate)할 수 있습니다. 아래는 더 쉽게 참조할 수 있도록 동일한 그림입니다. 이처럼 정렬된 임베딩은 언어 모델이 시각적 맥락과 텍스트 정보를 함께 고려하여 보다 풍부한 이해와 추론을 수행할 수 있도록 합니다.

시각 조각 토큰(image patch token)을 문자열 토큰 임베딩(text token embedding)과 동일한 차원으로 투영한 후, 표준 LLM의 입력으로 간단히 연결(concatenate)할 수 있습니다. 이 과정은 시각적 요소가 언어적 흐름의 일부처럼 처리되도록 합니다.

참고로, 이 절에서 논의한 시각 인코더(image encoder)는 일반적으로 사전 학습된(pretrained) 비전 트랜스포머(vision transformer)입니다. CLIP 또는 OpenCLIP이 인기 있는 선택입니다. 그러나 아래 그림에 표시된 Fuyu와 같이 조각(patch)에서 직접 작동하는 방법 A의 변형도 있습니다. 이는 이미지 인코더의 필요성을 줄여 아키텍처를 간소화합니다.

시각 인코더(image encoder) 없이 시각 조각(image patch)에서 직접 작동하는 Fuyu 다중 모드 LLM의 주석이 달린 그림. (https://www.adept.ai/blog/fuyu-8b에서 주석이 달린 그림.) Fuyu는 이러한 직접적인 접근 방식을 통해 모델의 복잡성을 줄이고 훈련 효율성을 높이는 장점을 가집니다.

위 그림에 설명된 대로, Fuyu는 입력 조각(input patch)을 선형 투영(linear projection)(또는 임베딩 계층(embedding layer))으로 직접 전달하여 다른 모델 및 방법처럼 추가적인 사전 학습된(pretrained) 시각 인코더(image encoder)에 의존하지 않고 자체적으로 시각 조각 임베딩(image patch embedding)을 학습합니다. 이러한 설계는 아키텍처(architecture)와 훈련 설정(training setup)을 크게 단순화하는 동시에, 모델이 특정 데이터셋에 최적화된 시각적 특징을 학습할 수 있도록 합니다.

#### 2.2 방법 B: 교차 모드 어텐션 구조(Cross-Modality Attention Architecture)

이제 다중 모드 LLM을 구축하기 위한 통합 임베딩 디코더 구조(unified embedding decoder architecture) 접근 방식에 대해 논의하고 시각 정보 인코딩(image encoding)의 기본 개념을 이해했으므로, 아래 그림에 요약된 교차 어텐션(cross-attention)을 통한 다중 모드 LLM 구현의 대안적인 방법에 대해 이야기해 보겠습니다. 이 방식은 모달리티 간의 상호작용을 보다 명시적으로 모델링하는 데 중점을 둡니다.

다중 모드 LLM 구축을 위한 교차 모드 어텐션 구조(Cross-Modality Attention Architecture) 접근 방식의 그림. 이 구조는 각 모드의 정보를 분리하여 처리한 후, 어텐션 메커니즘을 통해 통합합니다.

위 그림에 묘사된 교차 모드 어텐션 구조(Cross-Modality Attention Architecture) 방법에서는 이전에 논의한 것과 동일한 시각 인코더(image encoder) 설정을 여전히 사용합니다. 그러나 조각(patch)을 LLM의 입력으로 직접 인코딩하는 대신, 교차 어텐션 메커니즘(cross-attention mechanism)을 통해 멀티 헤드 어텐션 계층(multi-head attention layer)에서 입력 조각(input patch)을 연결합니다. 이 아이디어는 관련이 있으며, 아래 그림에 강조된 2017년 "Attention Is All You Need" 논문의 오리지널 트랜스포머 구조(original transformer architecture)로 거슬러 올라갑니다.

오리지널 트랜스포머 구조(original transformer architecture)에 사용된 교차 어텐션 메커니즘(cross-attention mechanism)의 고수준 그림. ("Attention Is All You Need" 논문에서 주석이 달린 그림: https://arxiv.org/abs/1706.03762.) 이 메커니즘은 인코더와 디코더 간의 정보 흐름을 조절하는 데 핵심적인 역할을 합니다.

위 그림에 묘사된 오리지널 "Attention Is All You Need" 트랜스포머(transformer)는 원래 언어 번역을 위해 개발되었습니다. 따라서 번역할 문장을 받아들이고 문자열 디코더(text decoder)(그림의 오른쪽 부분)를 통해 번역을 생성하는 문자열 인코더(text encoder)(그림의 왼쪽 부분)로 구성됩니다. 다중 모드 LLM의 맥락에서 인코더(encoder)는 문자열 인코더(text encoder) 대신 시각 인코더(image encoder)이지만, 동일한 아이디어가 적용됩니다. 즉, 한 모달리티의 정보를 다른 모달리티의 정보와 연결하는 방식입니다.

교차 어텐션(cross-attention)은 어떻게 작동할까요? 일반적인 셀프 어텐션 메커니즘(self-attention mechanism) 내부에서 일어나는 일의 개념적 그림을 살펴보겠습니다. 셀프 어텐션은 단일 시퀀스 내에서 요소들 간의 관계를 학습하는 반면, 교차 어텐션은 두 개의 다른 시퀀스 간의 관계를 학습합니다.

일반적인 셀프 어텐션 메커니즘(self-attention mechanism)의 개요. (이 흐름은 일반적인 멀티 헤드 어텐션 모듈(multi-head attention module)의 헤드 중 하나를 묘사합니다.) 여기서 모든 쿼리(Q), 키(K), 값(V)은 동일한 입력 시퀀스에서 파생됩니다.

위 그림에서 x는 입력이고, Wq는 쿼리(queries, Q)를 생성하는 데 사용되는 가중치 행렬(weight matrix)입니다. 유사하게, K는 키(keys)를 나타내고, V는 값(values)을 나타냅니다. A는 어텐션 스코어 행렬(attention scores matrix)을 나타내고, Z는 출력 컨텍스트 벡터(output context vector)로 변환된 입력(x)입니다. (이것이 혼란스럽다면, 제 책 "Build a Large Language Model from Scratch"의 3장에서 포괄적인 소개(comprehensive introduction)를 찾아보는 것이 도움이 될 수 있습니다. 또는 제 기사 "Understanding and Coding Self-Attention, Multi-Head Attention, Cross-Attention, and Causal-Attention in LLMs"도 도움이 될 수 있습니다.)

교차 어텐션(cross-attention)에서는 셀프 어텐션(self-attention)과 달리 다음 그림에 설명된 대로 두 가지 다른 입력 소스(input source)가 있습니다. 이로 인해 한 모달리티가 다른 모달리티의 정보를 "질의"하고 "응답"을 받는 방식으로 상호작용할 수 있습니다.

두 가지 다른 입력 x1과 x2가 있을 수 있는 교차 어텐션(cross attention) 그림. 여기서 x1은 쿼리(Q)를, x2는 키(K)와 값(V)을 제공하여 서로 다른 정보 스트림을 통합합니다.

이전 두 그림에서 설명했듯이, 셀프 어텐션(self-attention)에서는 동일한 입력 시퀀스(input sequence)로 작업합니다. 교차 어텐션(cross-attention)에서는 두 가지 다른 입력 시퀀스(input sequence)를 혼합하거나 결합합니다. "Attention Is All You Need" 논문의 오리지널 트랜스포머 구조(original transformer architecture)의 경우, 두 입력 x1과 x2는 왼쪽의 인코더 모듈(encoder module)이 반환하는 시퀀스(x2)와 오른쪽의 디코더 부분(decoder part)이 처리하는 입력 시퀀스(x1)에 해당합니다. 다중 모드 LLM의 맥락에서 x2는 시각 인코더(image encoder)의 출력입니다. (쿼리(queries)는 일반적으로 디코더(decoder)에서 오고, 키(keys)와 값(values)은 일반적으로 인코더(encoder)에서 온다는 점에 유의하세요.)

교차 어텐션(cross-attention)에서 두 입력 시퀀스 x1과 x2는 다른 수의 요소를 가질 수 있다는 점에 유의하세요. 그러나 임베딩 차원(embedding dimension)은 일치해야 합니다. x1 = x2로 설정하면 이는 셀프 어텐션(self-attention)과 동일하게 작동하며, 이는 교차 어텐션이 셀프 어텐션의 일반화된 형태임을 의미합니다.

### 3. 통합 디코더 및 교차 어텐션 모델의 학습 과정

이제 두 가지 주요 다중 모드 설계 선택지에 대해 간략히 살펴보았으므로, 아래 그림에 요약된 모델 학습(model training) 중 세 가지 핵심 구성 요소를 어떻게 다루는지에 대해 이야기해 보겠습니다. 다중 모드 모델의 학습은 단일 모드 모델보다 복잡하며, 각 구성 요소의 역할과 상호작용을 이해하는 것이 중요합니다.

다중 모드 LLM의 다양한 구성 요소 개요. 1-3번으로 표시된 구성 요소는 다중 모드 학습(training) 과정 중에 고정되거나 고정되지 않을 수 있습니다. 이러한 유연성은 모델의 성능과 효율성에 영향을 미칩니다.

전통적인 문자열 전용 LLM(text-only LLM) 개발과 유사하게, 다중 모드 LLM 학습(training)도 사전 학습(pretraining)과 명령어 미세 조정(instruction finetuning)의 두 단계를 포함합니다. 그러나 처음부터 시작하는 것과 달리, 다중 모드 LLM 학습(training)은 일반적으로 사전 학습(pretrained)되고 명령어 미세 조정(instruction-finetuned)된 문자열 전용 LLM을 기반 모델(base model)로 시작합니다. 이는 이미 강력한 언어 이해 능력을 갖춘 모델을 활용하여 학습 효율성을 높이는 전략입니다.

시각 인코더(image encoder)의 경우, CLIP이 일반적으로 사용되며 전체 학습(training) 과정 동안 변경되지 않는 경우가 많지만, 나중에 살펴볼 예외도 있습니다. 사전 학습(pretraining) 단계에서 LLM 부분을 고정하는 것도 일반적이며, 프로젝터(projector)—선형 계층(linear layer) 또는 작은 다층 퍼셉트론(multi-layer perceptron)—훈련(training)에만 집중합니다. 프로젝터(projector)의 학습 능력이 제한적이고 일반적으로 한두 개의 계층(layer)으로만 구성되어 있기 때문에, 더 포괄적인 업데이트를 허용하기 위해 다중 모드 명령어 미세 조정(multimodal instruction finetuning)(2단계) 동안 LLM이 종종 고정 해제됩니다. 그러나 교차 어텐션 기반(cross-attention-based) 모델(방법 B)에서는 교차 어텐션 계층(cross-attention layer)이 전체 학습(training) 과정 동안 고정 해제된다는 점에 유의하세요.

두 가지 주요 접근 방식(방법 A: 통합 임베딩 디코더 구조(Unified Embedding Decoder Architecture) 및 방법 B: 교차 모드 어텐션 구조(Cross-modality Attention Architecture))을 소개한 후, 어떤 것이 더 효과적인지 궁금할 수 있습니다. 답은 특정 트레이드오프(trade-off)에 따라 달라집니다. 통합 임베딩 디코더 구조(Unified Embedding Decoder Architecture)(방법 A)는 LLM 구조(architecture) 자체에 어떤 수정도 필요하지 않으므로 일반적으로 구현하기 더 쉽습니다. 이는 기존 언어 모델 인프라를 재활용하는 장점이 있습니다. 교차 모드 어텐션 구조(Cross-modality Attention Architecture)(방법 B)는 추가 시각 토큰(image token)으로 입력 컨텍스트(input context)를 과부하하지 않고 대신 교차 어텐션 계층(cross-attention layer)에서 나중에 도입하기 때문에 종종 계산 효율성(computational efficiency)이 더 높은 것으로 간주됩니다. 또한, 이 접근 방식은 학습(training) 중에 LLM 매개변수(parameter)가 고정된 상태로 유지되면 오리지널 LLM의 문자열 전용 성능(text-only performance)을 보존하는 이점이 있습니다. 모델링 성능(modeling performance) 및 응답 품질(response quality)에 대한 논의는 NVIDIA의 NVLM 논문을 논의할 다음 절에서 다시 다룰 것입니다.

이것으로 다중 모드 LLM에 대한 다소 광범위한 소개가 끝났습니다. 이 글을 쓰면서, 논의가 처음 계획했던 것보다 길어졌다는 것을 깨달았고, 아마도 이 시점에서 글을 마무리하는 것이 좋을 것 같습니다. 그러나 실용적인 관점을 제공하기 위해 이러한 접근 방식을 구현하는 몇 가지 최신 연구 논문을 살펴보는 것이 좋을 것입니다. 따라서 이 글의 나머지 절에서는 이러한 논문들을 탐구할 것입니다.

### 4. 최신 다중 모드 모델 및 방법론

이 글의 나머지 부분에서는 합리적인 범위(reasonable scope)를 유지하기 위해 지난 몇 주 동안 발표된 작업에 특히 초점을 맞춰 다중 모드 LLM에 관한 최신 문헌을 검토할 것입니다. 따라서 이것은 다중 모드 LLM에 대한 역사적 개요(historical overview)나 포괄적인 검토(comprehensive review)라기보다는, 현재 진행 중인 최신 개발 동향에 대한 간략한 탐색입니다. 또한, 10개나 되는 모델들을 다루기 때문에 이러한 요약들을 간결하고 핵심적인 내용 위주로 유지하려고 노력할 것입니다. 이 글의 마지막 결론 절에는 이 논문들에서 사용된 방법들을 비교하는 개요가 있습니다.

#### 4.1 Llama 3 모델 군(Herd of Models)

Meta AI의 Llama 3 모델 군(Herd of Models) 논문(2024년 7월 31일)은 올여름 초에 나왔는데, LLM 용어로는 아주 오래전 일처럼 느껴집니다. 그러나 그들이 다중 모드 모델(multimodal model)을 나중에야 설명했지만 출시하지는 않았다는 점을 고려할 때, Llama 3를 이 목록에 포함하는 것이 공정하다고 생각합니다. (Llama 3.2 모델은 9월 25일에 공식적으로 발표되고 공개되었습니다.)

110억 및 900억 매개변수(parameter) 버전으로 제공되는 다중 모드 Llama 3.2 모델은 이전에 설명된 교차 어텐션 기반(cross-attention-based) 접근 방식을 사용하는 시각-문자열 모델(image-text model)이며, 이는 아래 그림에 설명되어 있습니다. 이 모델은 시각적 정보와 텍스트 정보를 효과적으로 통합하여 복잡한 다중 모드 작업을 수행할 수 있도록 설계되었습니다.

Llama 3.2에서 활용된 다중 모드 LLM 접근 방식의 그림. (Llama 3 논문에서 주석이 달린 그림: https://arxiv.org/abs/2407.21783. 비디오 및 음성 부분은 시각적 정보 부분에 집중하기 위해 시각적으로 가려져 있습니다.) 이는 모델이 다양한 모달리티를 처리할 수 있는 잠재력을 시사합니다.

이 그림은 영상과 음성도 가능한 모드(modality)로 묘사하고 있지만, 이 글을 쓰는 시점에 출시된 모델은 시각 정보와 문자열에만 초점을 맞추고 있다는 점에 유의하세요. Llama 3.2는 교차 어텐션 기반(cross-attention-based) 접근 방식을 사용합니다. 그러나 이는 제가 이전에 썼던 내용, 즉 다중 모드 LLM 개발에서 일반적으로 시각 인코더(image encoder)를 고정하고 사전 학습(pretraining) 중에 LLM 매개변수(parameter)만 업데이트한다는 내용과는 약간 다릅니다. 여기서는 연구자들이 거의 반대 접근 방식을 취합니다. 그들은 시각 인코더(image encoder)를 업데이트하지만 언어 모델(language model)의 매개변수(parameter)는 업데이트하지 않습니다. 그들은 이것이 의도적이며 문자열 전용 기능(text-only capabilities)을 보존하기 위해 수행되었다고 썼습니다. 그래서 11B 및 90B 다중 모드 모델(multimodal model)이 문자열 작업에서 Llama 3.1 8B 및 70B 문자열 전용 모델(text-only model)의 드롭인(drop-in) 대체품으로 사용될 수 있습니다.

학습(training) 자체는 Llama 3.1 문자열 모델(text model)로 시작하여 여러 번의 반복으로 수행됩니다. 시각 인코더(image encoder)와 투영(projection)(여기서는 "어댑터(adapter)"라고 불림) 계층(layer)을 추가한 후, 시각-문자열 데이터(image-text data)로 모델을 사전 학습(pretrain)합니다. 그런 다음, Llama 3 모델의 문자열 전용 학습(text-only training)과 유사하게(이전 글에서 이에 대해 썼습니다), 명령어 및 선호도 미세 조정(instruction and preference finetuning)을 진행합니다.

연구자들은 CLIP과 같은 사전 학습된(pretrained) 모델을 시각 인코더(image encoder)로 채택하는 대신, 처음부터 사전 학습된(pretrained) 비전 트랜스포머(vision transformer)를 사용했습니다. 특히, 그들은 고전적인 비전 트랜스포머(vision transformer) 구조(architecture)의 ViT-H/14 변형(6억 3천만 매개변수(parameter))(Dosovitskiy et al., 2020)을 채택했습니다. 그런 다음, 시각 인코더(image encoder)를 LLM에 연결하기 전에 25억 개의 시각-문자열 쌍(image-text pair) 데이터셋(dataset)에서 5 에포크(epoch) 동안 ViT를 사전 학습(pretrain)했습니다. (시각 인코더(image encoder)는 224×224 해상도 이미지를 받아 14×14 그리드의 조각(patch)으로 나누며, 각 조각(patch)은 16×16 픽셀 크기입니다.)

교차 어텐션 계층(cross-attention layer)은 상당한 양의 매개변수(parameter)를 추가하므로, 4번째 트랜스포머 블록(transformer block)마다만 추가됩니다. (8B 모델의 경우 3B 매개변수(parameter)가 추가되고, 70B 모델의 경우 200억 매개변수(parameter)가 추가됩니다.)

#### 4.2 Molmo 및 PixMo: 최첨단 다중 모드 모델을 위한 개방형 가중치(Open Weights) 및 개방형 데이터(Open Data)

Molmo 및 PixMo: 최첨단 다중 모드 모델을 위한 개방형 가중치(Open Weights) 및 개방형 데이터(Open Data) 논문(2024년 9월 25일)은 언어 전용 OLMo LLM과 유사하게 모델 가중치(model weight)뿐만 아니라 데이터셋(dataset)과 소스 코드(source code)까지 오픈 소스(open source)로 공개하겠다고 약속했기 때문에 주목할 만합니다. (이는 LLM 연구에 매우 중요합니다. 정확한 학습 절차(training procedure)와 코드를 살펴보고, 어블레이션 연구(ablation study)를 수행하며, 동일한 데이터셋(dataset)에서 결과를 재현할 수 있기 때문입니다.) 이러한 투명성은 연구 커뮤니티의 발전에 크게 기여합니다.

논문 제목에 두 가지 이름이 있는 이유가 궁금하다면, Molmo는 모델(멀티모달 오픈 언어 모델, Multimodal Open Language Model)을 의미하고, PixMo(Pixels for Molmo)는 데이터셋(dataset)입니다. 이는 모델과 학습 데이터셋이 긴밀하게 연결되어 있음을 시사합니다.

Molmo 디코더 전용(decoder-only) 접근 방식(방법 A)의 그림. Molmo 및 PixMo: 최첨단 다중 모드 모델을 위한 개방형 가중치(Open Weights) 및 개방형 데이터(Open Data) 논문에서 각색된 주석이 달린 그림: https://www.arxiv.org/abs/2409.17146.

위 그림에 설명된 대로, 시각 인코더(image encoder)는 상용 비전 트랜스포머(off-the-shelf vision transformer), 특히 CLIP을 사용합니다. 여기서 "커넥터(connector)"라는 용어는 시각적 특징(image feature)을 언어 모델(language model)과 정렬하는 "프로젝터(projector)"를 의미합니다. Molmo는 여러 사전 학습(pretraining) 단계를 피하고, 기본 LLM, 커넥터(connector), 시각 인코더(image encoder)를 포함한 모든 매개변수(parameter)를 통합된 접근 방식(unified approach)으로 업데이트하는 간단한 파이프라인(pipeline)을 선택하여 학습(training) 과정을 간소화합니다. 이러한 통합 학습은 각 구성 요소가 처음부터 함께 최적화되도록 하여, 잠재적으로 더 나은 전반적인 성능을 달성할 수 있습니다.

Molmo 팀은 기본 LLM(base LLM)에 대해 여러 옵션을 제공합니다.
*   OLMo-7B-1024 (완전히 오픈된 모델 백본(model backbone))
*   OLMoE-1B-7B (전문가 혼합 구조(mixture-of-experts architecture); 가장 효율적인 모델)
*   Qwen2 7B (OLMo-7B-1024보다 성능이 우수한 개방형 가중치(open-weight) 모델)
*   Qwen2 72B (개방형 가중치(open-weight) 모델이자 최고의 성능을 보이는 모델)
이러한 다양한 선택지는 연구자들이 특정 요구 사항에 따라 모델을 유연하게 선택하고 실험할 수 있도록 합니다.

#### 4.3 NVLM: 개방형 프론티어 클래스 다중 모드 LLM(Open Frontier-Class Multimodal LLMs)

NVIDIA의 NVLM: 개방형 프론티어 클래스 다중 모드 LLM 논문(2024년 9월 17일)은 단일 접근 방식에 초점을 맞추기보다는 두 가지 방법, 즉 방법 A: 통합 임베딩 디코더 구조(Unified Embedding Decoder Architecture) ("디코더 전용(decoder-only) 구조(architecture)," NVLM-D)와 방법 B: 교차 모드 어텐션 구조(Cross-Modality Attention Architecture) ("교차 어텐션 기반(cross-attention-based) 구조(architecture)," NVLM-X)를 모두 탐구하기 때문에 특히 흥미롭습니다. 또한, 그들은 하이브리드 접근 방식(hybrid approach)(NVLM-H)을 개발하고 세 가지 방법 모두에 대한 공정한 비교(apples-to-apples comparison)를 제공합니다. 이는 다중 모드 LLM 설계의 복잡성을 이해하는 데 매우 중요한 기여를 합니다.

세 가지 다중 모드 접근 방식 개요. (NVLM: 개방형 프론티어 클래스 다중 모드 LLM 논문에서 주석이 달린 그림: https://arxiv.org/abs/2409.11402) 이 그림은 각 접근 방식의 핵심적인 구조적 차이점을 명확히 보여줍니다.

아래 그림에 요약된 대로, NVLM-D는 방법 A에 해당하고, NVLM-X는 이전에 논의된 방법 B에 해당합니다. 하이브리드 모델(hybrid model)(NVLM-H)의 개념은 두 가지 방법의 장점을 결합하는 것입니다. 시각적 썸네일(image thumbnail)이 입력으로 제공된 다음, 더 미세한 고해상도 세부 정보(high-resolution detail)를 포착하기 위해 교차 어텐션(cross-attention)을 통해 동적 조각 수(dynamic number of patches)가 전달됩니다. 이는 효율성과 세부 정보 유지 사이의 균형을 맞추려는 시도입니다.

요약하자면, 연구팀은 다음을 발견했습니다.
*   NVLM-X는 고해상도 시각 정보에 대해 우수한 계산 효율성(computational efficiency)을 보여줍니다. 이는 대규모 이미지 처리에서 중요한 이점입니다.
*   NVLM-D는 OCR 관련 작업(OCR-related task)에서 더 높은 정확도를 달성합니다. 이는 텍스트가 풍부한 시각 정보 처리에서 강점을 가집니다.
*   NVLM-H는 두 가지 방법의 장점을 결합하여 유연하고 강력한 성능을 제공합니다.

Molmo 및 다른 접근 방식과 유사하게, 그들은 처음부터 다중 모드 모델(multimodal model)을 사전 학습(pretraining)하는 대신 문자열 전용 LLM(text-only LLM)으로 시작합니다(일반적으로 이것이 더 나은 성능을 보이기 때문입니다). 또한, 기본 LLM(base LLM) 대신 명령어 미세 조정된 LLM(instruction-tuned LLM)을 사용합니다. 특히, 백본 LLM(backbone LLM)은 Qwen2-72B-Instruct입니다(제가 아는 한, Molmo는 Qwen2-72B 기본 모델(base model)을 사용했습니다).

NVLM-D 접근 방식에서 모든 LLM 매개변수(parameter)를 학습(training)하는 동안, 그들은 NVLM-X의 경우 원래 LLM 매개변수(parameter)를 고정하고 사전 학습(pretraining) 및 명령어 미세 조정(instruction finetuning) 모두에서 교차 어텐션 계층(cross-attention layer)만 학습(training)하는 것이 잘 작동한다는 것을 발견했습니다. 시각 인코더(image encoder)의 경우, 일반적인 CLIP 모델을 사용하는 대신 InternViT-6B를 사용하며, 이는 모든 단계에서 고정된 상태로 유지됩니다. 프로젝터(projector)는 단일 선형 계층(linear layer)이 아닌 다층 퍼셉트론(multilayer perceptron)입니다.

#### 4.4 Qwen2-VL: 모든 해상도에서 시각-언어 모델의 세계 인식 향상

이전 두 논문과 모델인 Molmo 및 NVLM은 Qwen2-72B LLM을 기반으로 했습니다. 이 논문에서 Qwen 연구팀 자체는 다중 모드 LLM인 Qwen2-VL: 모든 해상도에서 시각-언어 모델의 세계 인식 향상(2024년 10월 3일)을 발표합니다. 이 작업의 핵심은 그들의 이른바 "단순 동적 해상도(Naive Dynamic Resolution)" 메커니즘(여기서 "단순(naive)"이라는 용어는 의도적이며 "네이티브(native)"의 오타가 아니지만, "네이티브(native)"도 적합할 수 있습니다)입니다. 이 메커니즘은 모델이 간단한 다운샘플링(downsampling) 없이 다양한 해상도의 시각 정보를 처리할 수 있도록 하여 원본 해상도(original resolution)로 이미지를 입력할 수 있게 합니다. 이는 고해상도 시각 정보의 미세한 디테일을 보존하는 데 매우 중요합니다.

다양한 해상도의 입력 시각 정보를 기본적으로 처리할 수 있는 다중 모드 Qwen 모델의 개요. (Qwen2-VL 논문에서 주석이 달린 그림: https://arxiv.org/abs/2409.12191) 이 기능은 모델의 실용성을 크게 향상시킵니다.

네이티브 해상도 입력(native resolution input)은 원래 절대 위치 임베딩(absolute position embedding)을 제거하고 2D-RoPE를 도입하여 수정된 ViT를 통해 구현됩니다. 그들은 6억 7천 5백만 매개변수(parameter)를 가진 고전적인 시각 인코더(vision encoder)와 아래 표에 표시된 다양한 크기의 LLM 백본(LLM backbone)을 사용했습니다.

다양한 Qwen2-VL 모델의 구성 요소. (Qwen2-VL 논문에서 주석이 달린 그림: https://arxiv.org/abs/2409.12191)

학습(training) 자체는 3단계로 구성됩니다. (1) 시각 인코더(image encoder)만 사전 학습(pretraining), (2) 모든 매개변수(parameter)(LLM 포함) 고정 해제, (3) 시각 인코더(image encoder) 고정 및 LLM만 명령어 미세 조정(instruction-finetuning). 이 단계별 접근 방식은 모델의 각 부분을 최적화하는 데 도움을 줍니다.

#### 4.5 Pixtral 12B

방법 A: 통합 임베딩 디코더 구조(Unified Embedding Decoder Architecture) 접근 방식을 사용하는 Pixtral 12B(2024년 9월 17일)는 Mistral AI의 첫 번째 다중 모드 모델(multimodal model)입니다. 아쉽게도 기술 논문(technical paper)이나 보고서(report)는 없지만, Mistral 팀은 블로그 게시물(blog post)에서 몇 가지 흥미로운 정보를 공유했습니다. 흥미롭게도, 그들은 사전 학습된(pretrained) 시각 인코더(image encoder)를 사용하지 않고, 대신 4억 매개변수(parameter)를 가진 인코더(encoder)를 처음부터 학습(training)하기로 선택했습니다. 이는 특정 작업에 최적화된 시각 인코더를 구축하려는 의도로 보입니다. LLM 백본(LLM backbone)으로는 120억 매개변수(parameter) Mistral NeMo 모델을 사용했습니다. Qwen2-VL과 유사하게, Pixtral도 아래 그림에 설명된 대로 가변 시각 정보 크기(variable image size)를 기본적으로 지원합니다.

Pixtral이 다양한 크기의 시각 정보를 처리하는 방법 그림. (Pixtral 블로그 게시물에서 주석이 달린 그림: https://mistral.ai/news/pixtral-12b/) 이 기능은 실제 응용에서 유연성을 제공합니다.

#### 4.6 MM1.5: 다중 모드 LLM 미세 조정의 방법, 분석 및 통찰력

MM1.5: 다중 모드 LLM 미세 조정의 방법, 분석 및 통찰력 논문(2024년 9월 30일)은 실용적인 팁을 제공하고 Molmo와 유사한 밀집 모델(dense model)과 함께 전문가 혼합 다중 모드 모델(mixture-of-experts multimodal model)을 소개합니다. 이 모델들은 10억에서 300억 매개변수(parameter)에 이르는 넓은 크기 범위를 가집니다. 이 논문에 설명된 모델들은 방법 A, 즉 다중 모드 학습을 위해 입력을 효과적으로 구조화하는 통합 임베딩 트랜스포머 구조(Unified Embedding Transformer Architecture)에 초점을 맞춥니다. 또한, 이 논문은 데이터 혼합(data mixture)과 좌표 토큰(coordinate token) 사용의 효과를 살펴보는 일련의 흥미로운 어블레이션 연구(ablation study)를 포함합니다. 이러한 연구는 모델 성능에 기여하는 핵심 요소를 식별하는 데 중요합니다.

바운딩 박스(bounding box)를 나타내는 추가 좌표 토큰(coordinate token)을 포함하는 MM1.5 접근 방식의 그림. (MM1.5 논문에서 주석이 달린 그림: https://arxiv.org/abs/2409.20566.) 이러한 토큰은 모델이 시각적 객체의 위치 정보를 명시적으로 이해하도록 돕습니다.

#### 4.7 Aria: 개방형 다중 모드 네이티브 전문가 혼합 모델

Aria: 개방형 다중 모드 네이티브 전문가 혼합 모델 논문(2024년 10월 8일)은 Molmo 및 MM1.5 라인업의 변형 중 하나와 유사한 또 다른 전문가 혼합 모델(mixture-of-experts model) 접근 방식을 소개합니다. Aria 모델은 249억 매개변수(parameter)를 가지며, 문자열 토큰(text token)당 35억 매개변수(parameter)가 할당됩니다. 시각 인코더(image encoder)(SigLIP)는 4억 3천 8백만 매개변수(parameter)를 가집니다. 이 모델은 다음과 같은 전체 학습 절차(training procedure)를 가진 교차 어텐션(cross-attention) 접근 방식을 기반으로 합니다.
*   LLM 백본(LLM backbone)을 처음부터 완전히 학습(training)합니다.
*   LLM 백본(LLM backbone)과 비전 인코더(vision encoder)를 모두 사전 학습(pretraining)합니다.
이 두 단계는 모델이 언어와 시각 정보를 모두 깊이 있게 이해하도록 보장합니다.

#### 4.8 Baichuan-Omni

Baichuan-Omni 기술 보고서(2024년 10월 11일)는 아래 그림에 표시된 대로 방법 A: 통합 임베딩 디코더 구조(Unified Embedding Decoder Architecture) 접근 방식을 기반으로 하는 70억 매개변수(parameter) 다중 모드 LLM인 Baichuan-Omni를 소개합니다. 이 모델은 다양한 입력 모달리티를 단일 모델에서 처리할 수 있도록 설계되었습니다.

다양한 입력 모달리티(input modality)를 처리할 수 있는 Baichuan-Omni 모델의 개요. (Baichuan-Omni 논문에서 주석이 달린 그림: https://arxiv.org/abs/2410.08565) 이는 모델의 범용성을 강조합니다.

Baichuan-Omni의 학습(training) 과정은 3단계 접근 방식을 포함합니다.
*   **프로젝터(projector) 학습(training)**: 처음에는 프로젝터(projector)만 학습(training)되고, 비전 인코더(vision encoder)와 언어 모델(LLM)은 모두 고정된 상태로 유지됩니다. 이 단계는 모달리티 간의 초기 정렬을 담당합니다.
*   **비전 인코더(vision encoder) 학습(training)**: 다음으로, 비전 인코더(vision encoder)는 고정 해제되고 학습(training)되며, LLM은 여전히 고정된 상태입니다. 이 단계에서 시각적 특징 추출 능력이 향상됩니다.
*   **전체 모델 학습(training)**: 마지막으로, LLM은 고정 해제되어 전체 모델이 엔드투엔드(end-to-end)로 학습(training)될 수 있도록 합니다. 이 단계에서 모든 구성 요소가 함께 최적화됩니다.

이 모델은 SigLIP 비전 인코더(vision encoder)를 활용하고, 다운샘플링 기법(down-sampling technique)을 통해 고해상도 이미지를 처리하기 위해 AnyRes 모듈을 통합합니다. 보고서에는 LLM 백본(LLM backbone)이 명시적으로 지정되어 있지 않지만, 모델의 매개변수 크기(parameter size)와 명명 규칙(naming convention)을 고려할 때 Baichuan 7B LLM을 기반으로 할 가능성이 높습니다.

#### 4.9 Emu3: 다음 토큰 예측만 있으면 됩니다

Emu3: 다음 토큰 예측만 있으면 됩니다 논문(2024년 9월 27일)은 시각 정보 생성(image generation)을 위한 확산 모델(diffusion model)에 대한 설득력 있는 대안을 제시하며, 이는 트랜스포머 기반 디코더 구조(transformer-based decoder architecture)에만 기반합니다. 비록 고전적인 의미의 다중 모드 LLM(즉, 생성보다는 시각 정보 이해(image understanding)에 초점을 맞춘 모델)은 아니지만, Emu3는 트랜스포머 디코더(transformer decoder)를 사용하여 시각 정보 생성(image generation)이 가능하다는 것을 보여주기 때문에 매우 흥미롭습니다. 시각 정보 생성(image generation)은 일반적으로 확산 방법(diffusion method)이 지배하는 작업입니다. (그러나 Autoregressive Model Beats Diffusion: Llama for Scalable Image Generation과 같은 다른 유사한 접근 방식이 이전에도 있었다는 점에 유의하세요.)

Emu3는 확산 모델(diffusion model)의 대안으로 시각 정보 생성(image generation)을 위한 LLM입니다. (Emu3 논문에서 주석이 달린 그림: https://arxiv.org/abs/2409.18869) 이는 생성 모델 분야에 새로운 가능성을 제시합니다.

연구자들은 Emu3를 처음부터 학습(training)한 다음, 직접 선호 최적화(Direct Preference Optimization, DPO)를 사용하여 모델을 인간 선호도에 맞추었습니다. 구조(architecture)에는 SBER-MoVQGAN에서 영감을 받은 비전 토크나이저(vision tokenizer)가 포함됩니다. 핵심 LLM 구조(core LLM architecture)는 Llama 2를 기반으로 하지만, 처음부터 완전히 학습(training)되었습니다.

#### 4.10 Janus: 통합 다중 모드 이해 및 생성을 위한 시각 인코딩 분리

우리는 이전에 시각 정보 이해(image understanding)를 위한 다중 모드 LLM에 초점을 맞췄고, 위에서 Emu 3를 통한 시각 정보 생성(image generation)의 한 가지 예를 보았습니다. 이제 Janus: 통합 다중 모드 이해 및 생성을 위한 시각 인코딩 분리 논문(2024년 10월 17일)은 단일 LLM 백본(LLM backbone) 내에서 다중 모드 이해(multimodal understanding) 및 생성 작업(generation task)을 통합하는 프레임워크(framework)를 소개합니다. 이는 AI가 세상을 이해하고 창조하는 능력을 동시에 갖추도록 하는 야심찬 목표를 제시합니다.

Janus의 핵심 기능은 이해(understanding) 및 생성 작업(generation task)의 고유한 요구 사항(distinct requirement)을 해결하기 위한 시각 인코딩 경로(visual encoding pathway)의 분리입니다. 연구자들은 시각 정보 이해(image understanding) 작업에는 고차원 의미론적 표현(high-dimensional semantic representation)이 필요하고, 생성 작업(generation task)에는 이미지의 상세한 지역 정보(detailed local information)와 전역 일관성(global consistency)이 필요하다고 주장합니다. 이러한 경로를 분리함으로써 Janus는 이러한 상이한 요구 사항을 효과적으로 관리합니다.

이 모델은 Baichuan-Omni에서 사용된 것과 유사한 SigLIP 비전 인코더(vision encoder)를 사용하여 시각적 입력을 처리합니다. 시각 정보 생성(image generation)을 위해 벡터 양자화(Vector Quantized, VQ) 토크나이저(tokenizer)를 사용하여 생성 과정을 처리합니다. Janus의 기본 LLM(base LLM)은 13억 매개변수(parameter)를 가진 DeepSeek-LLM입니다.

Janus에서 사용된 통합 디코더 전용(decoder-only) 프레임워크(framework)의 개요. (Janus 논문에서 주석이 달린 그림: https://arxiv.org/abs/2410.13848.)

이 이미지의 모델 학습(training) 과정은 아래 그림에 표시된 대로 3단계로 진행됩니다.

Janus 모델의 3단계 학습(training) 과정 그림. (Janus 논문에서 주석이 달린 그림: https://arxiv.org/abs/2410.13848)

1단계에서는 LLM, 이해(understanding) 및 생성 인코더(generation encoder)가 고정된 상태에서 프로젝터 계층(projector layer)과 시각 출력 계층(image output layer)만 학습(training)됩니다. 2단계에서는 LLM 백본(LLM backbone)과 문자열 출력 계층(text output layer)이 고정 해제되어 이해(understanding) 및 생성 작업(generation task) 전반에 걸쳐 통합 사전 학습(unified pretraining)이 가능해집니다. 마지막으로 3단계에서는 SigLIP 시각 인코더(image encoder)를 포함한 전체 모델이 고정 해제되어 지도 미세 조정(supervised fine-tuning)을 위해 사용되며, 모델이 다중 모드 기능(multimodal capability)을 완전히 통합하고 개선할 수 있도록 합니다.

### 결론 및 향후 전망

눈치채셨겠지만, 저는 모델링(modeling) 및 계산 성능 비교(computational performance comparison)를 거의 완전히 건너뛰었습니다. 첫째, LLM 및 다중 모드 LLM의 공개 벤치마크(public benchmark) 성능을 비교하는 것은 일반적인 데이터 오염(data contamination) 때문에 어렵습니다. 즉, 테스트 데이터(test data)가 학습 데이터(training data)에 포함되었을 수 있습니다. 또한, 구조적 구성 요소(architectural component)가 너무 다양하여 공정한 비교(apples-to-apples comparison)를 하기가 어렵습니다. 따라서 적어도 디코더 전용(decoder-only) 및 교차 어텐션(cross-attention) 접근 방식 간의 비교를 가능하게 한 다양한 버전의 NVLM을 개발한 NVIDIA 팀에게 큰 찬사를 보냅니다.

어쨌든, 이 글의 핵심적인 통찰(main takeaway)은 다중 모드 LLM이 다양한 방식으로 성공적으로 구축될 수 있다는 것입니다. 아래는 이 글에서 다룬 모델들의 다양한 구성 요소를 요약한 그림입니다. 이는 각 모델이 어떤 설계 철학을 따랐는지 한눈에 보여줍니다.

이 글에서 다룬 다양한 모델과 그 하위 구성 요소 및 학습(training) 접근 방식에 대한 개요.

이 글을 읽는 것이 교육적(educational)이었기를 바라며, 이제 다중 모드 LLM이 어떻게 작동하는지에 대해 더 깊이 이해하게 되셨기를 바랍니다! 이 분야는 여전히 빠르게 진화하고 있으며, 앞으로 더욱 혁신적인 모델과 응용 분야가 등장할 것으로 기대됩니다. 다중 모드 AI는 인간과 유사한 방식으로 세상을 인식하고 상호작용하는 인공지능의 궁극적인 목표에 한 걸음 더 다가서게 합니다.

이 잡지는 개인적인 열정 프로젝트(personal passion project)입니다. 저를 지원하고 싶은 분들은 제 책 "Build a Large Language Model (From Scratch)"을 구매해 주시면 감사하겠습니다. (이 책은 LLM이 어떻게 작동하는지 다른 곳에서는 찾아볼 수 없는 수준의 세부 사항으로 설명하므로, 이 책에서 많은 것을 얻으실 것이라고 확신합니다.)

[Build a Large Language Model (From Scratch) now available on Amazon]

책을 읽으셨고 잠시 시간을 내실 수 있다면, 짧은 서평을 남겨주시면 정말 감사하겠습니다. 그것은 저희 저자들에게 큰 도움이 됩니다! 또는, 최근에 Substack에 유료 구독 옵션(paid subscription option)을 활성화하여 이 잡지를 직접 지원할 수도 있습니다. 여러분의 지원은 큰 의미가 있습니다! 감사합니다!

[Subscribe]