지난 한 해는 AI 연구 분야에서 전례 없는 혁신과 성장을 보여주었습니다. 수많은 중요한 발전이 이루어졌고, 대규모 언어 모델(large language model)과 멀티모달 AI(multimodal AI) 분야에서는 특히 눈부신 성과가 있었습니다. 작년 Meta AI는 Llama 3.2 모델을 선보이며 1B 및 3B 규모의 공개 가중치(open-weight) 모델과 두 개의 멀티모달 모델을 포함하는 등 큰 주목을 받았습니다. 이와 더불어, 최근에는 더욱 발전된 멀티모달 LLM들이 연이어 발표되며, 다양한 형태의 데이터를 이해하고 생성하는 능력이 한층 강화되었습니다. 이 게시물에서는 이러한 멀티모달 LLM이 어떻게 작동하는지 심도 있게 탐구하고, 지난 한 해 동안 발표된 주요 멀티모달 논문과 모델들을 종합적으로 검토하여 그들의 핵심 접근 방식을 비교 분석할 것입니다. (목차를 보려면 왼쪽 메뉴를 확인하세요.)

다양한 입력 모달리티(input modality)(오디오, 텍스트, 이미지, 비디오)를 받아들이고 텍스트를 출력 모달리티(output modality)로 반환하는 멀티모달 LLM의 그림.

하지만 시작하기 전에, 개인적으로 흥미로운 소식이 있습니다! 제 책 "Build A Large Language Model (From Scratch)"이 드디어 Amazon에서 구매 가능합니다!

[Build a Large Language Model (From Scratch) now available on Amazon]

이 책을 쓰는 것은 엄청난 노력이 필요했고, 지난 2년 동안—특히 지난 몇 달 동안—많은 친절한 독자들이 피드백을 공유해주신 모든 지원과 동기 부여가 되는 피드백에 진심으로 감사드립니다. 여러분 모두에게 감사드리며, 저자로서 이 책이 여러분의 경력에 변화를 가져온다는 소식보다 더 큰 동기 부여는 없습니다! 책을 마치고 더 많은 것을 갈망하는 분들을 위해, 계속 지켜봐 주세요! 앞으로 몇 달 안에 GitHub 저장소에 보너스 콘텐츠를 추가할 예정입니다.

추신. 만약 이 책을 읽으셨다면, 짧은 리뷰를 남겨주시면 정말 감사하겠습니다. 그것은 저희 저자들에게 정말 큰 도움이 됩니다!

### 1. 멀티모달 LLM의 핵심 활용 사례

멀티모달 LLM이 정확히 무엇을 의미할까요? 앞서 언급했듯이, 멀티모달 LLM은 여러 종류의 입력 데이터를 처리하는 대규모 언어 모델(large language model)입니다. 여기서 '모달리티(modality)'는 기존 LLM의 텍스트 외에도 소리, 이미지, 비디오와 같은 특정 데이터 형식을 지칭합니다. 이 글에서는 주로 텍스트 입력과 함께 이미지 모달리티(image modality)에 집중하여 설명할 것입니다.

멀티모달 LLM의 대표적이고 직관적인 활용 분야로는 이미지 캡셔닝(image captioning)이 있습니다. 모델에 이미지를 입력하면, 아래 그림처럼 해당 이미지에 대한 상세한 설명을 생성해 줍니다.

밈(meme)을 설명하는 멀티모달 LLM의 사용 예시.

물론, 이 외에도 다양한 활용 사례들이 있습니다. 예를 들어, 제가 특히 흥미롭게 생각하는 것 중 하나는 복잡한 PDF 문서에서 표 정보를 추출하여 LaTeX 또는 Markdown 형식으로 변환하는 기능입니다. 최근에는 의료 영상 분석(medical image analysis) 분야에서도 멀티모달 LLM이 활발히 연구되고 있습니다. 예를 들어, X-레이 이미지와 환자 기록을 함께 분석하여 질병 진단을 돕거나, 수술 영상을 통해 실시간으로 의료진에게 피드백을 제공하는 등 전문적인 영역으로 확장되고 있습니다.

### 2. 멀티모달 LLM 아키텍처의 주요 접근 방식

멀티모달 LLM을 설계하고 구축하는 방식에는 크게 두 가지 주요 접근 방식이 존재합니다. 첫 번째는 '통합 임베딩 디코더 아키텍처(Unified Embedding Decoder Architecture)'이며, 두 번째는 '교차 모달리티 어텐션 아키텍처(Cross-modality Attention Architecture)'입니다. (참고로, 이러한 기술에 대한 공식적인 명칭이 아직 완전히 정립되지 않았다고 생각합니다. 혹시 더 적합하거나 간결한 용어가 있다면 알려주시면 감사하겠습니다. 예를 들어, '디코더 전용(decoder-only)' 및 '교차 어텐션 기반(cross-attention-based)' 접근 방식도 좋은 대안이 될 수 있습니다.)

멀티모달 LLM 아키텍처 개발을 위한 두 가지 주요 접근 방식.

위 그림에서 명확히 보이듯이, 통합 임베딩 디코더 아키텍처(Unified Embedding-Decoder Architecture)는 GPT-2나 Llama 3.2와 같은 기존 LLM 아키텍처와 매우 흡사하게 단일 디코더 모델(decoder model)을 사용합니다. 이 방법론에서는 입력 이미지가 원래 텍스트 토큰(text token)과 동일한 임베딩 크기(embedding size)를 가진 토큰으로 변환되며, 이를 통해 LLM은 연결(concatenation)된 텍스트 및 이미지 입력 토큰(image input token)을 함께 처리할 수 있습니다. 반면, 교차 모달리티 어텐션 아키텍처(Cross-Modality Attention Architecture)는 어텐션 레이어(attention layer) 내에서 교차 어텐션 메커니즘(cross-attention mechanism)을 활용하여 이미지 및 텍스트 임베딩(embedding)을 직접적으로 통합합니다.

이어지는 섹션에서는 이러한 두 가지 방법이 개념적 수준(conceptual level)에서 어떻게 동작하는지 상세히 알아볼 것입니다. 그리고 나서, 멀티모달 LLM에 대한 최신 연구 논문들을 검토하며 실제 적용 사례를 탐구할 것입니다.

#### 2.1 방법 A: 통합 임베딩 디코더 아키텍처

아래 그림에 다시 설명된 통합 임베딩 디코더 아키텍처(unified embedding decoder architecture)부터 논의를 시작하겠습니다.

이미지 토큰(image token) 및 텍스트 토큰(text token) 임베딩(embedding)으로 구성된 입력을 받는 수정되지 않은 디코더 스타일 LLM(GPT-2, Phi-3, Gemma 또는 Llama 3.2와 같은)인 통합 임베딩 디코더 아키텍처의 그림.

통합 임베딩 디코더 아키텍처(unified embedding-decoder architecture)에서 이미지는 표준 텍스트 전용 LLM(text-only LLM)에서 입력 텍스트가 임베딩으로 변환되는 방식과 유사하게 임베딩 벡터(embedding vector)로 변환됩니다. 텍스트를 처리하는 일반적인 텍스트 전용 LLM의 경우, 텍스트 입력은 일반적으로 토큰화(tokenized)된 다음(예: 바이트 쌍 인코딩(Byte-Pair Encoding) 사용), 아래 그림과 같이 임베딩 레이어(embedding layer)를 통과합니다.

텍스트를 토큰화(tokenizing)하고 토큰 임베딩 벡터(token embedding vector)로 변환하는 표준 프로세스 그림. 이 벡터는 훈련(training) 및 추론(inference) 중에 LLM으로 전달됩니다.

##### 2.1.1 이미지 인코더(Image encoders) 이해

텍스트의 토큰화(tokenization) 및 임베딩(embedding)과 유사하게, 이미지 임베딩(image embedding)은 아래 그림과 같이 이미지 인코더(image encoder) 모듈(토크나이저(tokenizer) 대신)을 사용하여 생성됩니다.

이미지를 이미지 패치 임베딩(image patch embedding)으로 인코딩하는 프로세스 그림.

위 그림에 표시된 이미지 인코더(image encoder) 내부에서는 어떤 일이 일어날까요? 이미지를 처리하기 위해 먼저 이미지를 더 작은 패치(patch)로 나눕니다. 이는 토큰화(tokenization) 중에 단어를 서브워드(subword)로 나누는 것과 매우 유사합니다. 이 패치들은 아래 그림과 같이 사전 훈련된(pretrained) 비전 트랜스포머(vision transformer, ViT)에 의해 인코딩됩니다.

"An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale (2020)"에서 제안된 모델과 유사한 고전적인 비전 트랜스포머(ViT) 설정 그림. ViT는 종종 분류 작업(classification task)에 사용되므로 위 그림에 분류 헤드(classification head)를 포함했습니다. 그러나 이 경우에는 이미지 인코더(image encoder) 부분만 필요합니다.

##### 2.1.2 선형 투영(linear projection) 모듈의 역할

이전 그림에 표시된 "선형 투영(linear projection)"은 단일 선형 레이어(linear layer)(즉, 완전 연결 레이어(fully connected layer))로 구성됩니다. 이 레이어의 목적은 벡터로 평탄화된 이미지 패치(image patch)를 트랜스포머 인코더(transformer encoder)와 호환되는 임베딩 크기(embedding size)로 투영하는 것입니다. 이 선형 투영(linear projection)은 아래 그림에 설명되어 있습니다.

256차원 벡터로 평탄화된 이미지 패치(image patch)가 768차원 벡터로 상향 투영됩니다.

256차원에서 768차원 임베딩 공간(embedding space)으로 평탄화된 이미지 패치(image patch)를 투영하는 선형 투영(linear projection) 레이어 그림.

코드 예시를 선호하는 분들을 위해, PyTorch 코드에서는 이미지 패치(image patch)에 대한 선형 투영(linear projection)을 다음과 같이 구현할 수 있습니다.

```python
import torch

class PatchProjectionLayer(torch.nn.Module):
    def __init__(self, patch_size, num_channels, embedding_dim):
        super().__init__()
        self.patch_size = patch_size
        self.num_channels = num_channels
        self.embedding_dim = embedding_dim
        self.projection = torch.nn.Linear(
            patch_size * patch_size * num_channels,
            embedding_dim
        )

    def forward(self, x):
        batch_size, num_patches, channels, height, width = x.size()
        x = x.view(batch_size, num_patches, -1) # Flatten each patch
        x = self.projection(x) # Project each flattened patch
        return x

# Example Usage:
batch_size = 1
num_patches = 9 # Total patches per image
patch_size = 16 # 16x16 pixels per patch
num_channels = 3 # RGB image
embedding_dim = 768 # Size of the embedding vector

projection_layer = PatchProjectionLayer(patch_size, num_channels, embedding_dim)

patches = torch.rand(
    batch_size, num_patches, num_channels, patch_size, patch_size
)

projected_embeddings = projection_layer(patches)
print(projected_embeddings.shape)
# This prints
# torch.Size([1, 9, 768])
```

혹시 제 "Machine Learning Q and AI" 책을 읽으셨다면, 선형 레이어(linear layer)를 수학적으로 동등하게 구현할 수 있는 컨볼루션 연산(convolution operation)으로 대체하는 방법이 있다는 것을 아실 것입니다. 여기서는 패치(patch) 생성과 투영(projection)을 두 줄의 코드로 결합할 수 있어 특히 유용합니다.

```python
layer = torch.nn.Conv2d(3, 768, kernel_size=(16, 16), stride=(16, 16))
image = torch.rand(batch_size, 3, 48, 48)
projected_patches = layer(image)
print(projected_patches.flatten(-2).transpose(-1, -2).shape)
# This prints
# torch.Size([1, 9, 768])
```

##### 2.1.3 이미지 대 텍스트 토큰화(tokenization)

이제 이미지 인코더(image encoder)(및 인코더의 일부인 선형 투영(linear projection))의 목적에 대해 간략하게 논의했으므로, 이전의 텍스트 토큰화(text tokenization) 비유로 돌아가 아래 그림에 묘사된 텍스트 및 이미지 토큰화(tokenization)와 임베딩(embedding)을 나란히 살펴보겠습니다.

이미지 토큰화(tokenization) 및 임베딩(embedding)(왼쪽)과 텍스트 토큰화(tokenization) 및 임베딩(embedding)(오른쪽)을 나란히 비교.

위 그림에서 보듯이, 저는 이미지 인코더(image encoder) 뒤에 추가 프로젝터 모듈(projector module)을 포함했습니다. 이 프로젝터(projector)는 일반적으로 이전에 설명한 것과 유사한 또 다른 선형 투영(linear projection) 레이어일 뿐입니다. 그 목적은 아래 그림에 설명된 대로 이미지 인코더(image encoder) 출력을 임베딩된 텍스트 토큰(text token)의 차원과 일치하는 차원으로 투영하는 것입니다. (나중에 보겠지만, 프로젝터(projector)는 때때로 어댑터(adapter), 어댑터(adaptor) 또는 커넥터(connector)라고도 불립니다.)

이미지 토큰화(tokenization)와 텍스트 토큰화(tokenization)의 또 다른 나란한 비교. 여기서 프로젝터(projector)의 역할은 텍스트 토큰 임베딩(text token embedding) 차원을 일치시키는 것입니다.

이제 이미지 패치 임베딩(image patch embedding)이 텍스트 토큰 임베딩(text token embedding)과 동일한 임베딩 차원(embedding dimension)을 가지므로, 이 섹션 시작 부분의 그림에 표시된 대로 LLM의 입력으로 간단히 연결(concatenate)할 수 있습니다. 아래는 더 쉽게 참조할 수 있도록 동일한 그림입니다.

이미지 패치 토큰(image patch token)을 텍스트 토큰 임베딩(text token embedding)과 동일한 차원으로 투영한 후, 표준 LLM의 입력으로 간단히 연결(concatenate)할 수 있습니다.

참고로, 이 섹션에서 논의한 이미지 인코더(image encoder)는 일반적으로 사전 훈련된(pretrained) 비전 트랜스포머(vision transformer)입니다. CLIP 또는 OpenCLIP이 인기 있는 선택입니다. 그러나 아래 그림에 표시된 Fuyu와 같이 패치(patch)에서 직접 작동하는 방법 A의 버전도 있습니다.

이미지 인코더(image encoder) 없이 이미지 패치(image patch)에서 직접 작동하는 Fuyu 멀티모달 LLM의 주석이 달린 그림. (https://www.adept.ai/blog/fuyu-8b에서 주석이 달린 그림.)

위 그림에 설명된 대로, Fuyu는 입력 패치(input patch)를 선형 투영(linear projection)(또는 임베딩 레이어(embedding layer))으로 직접 전달하여 다른 모델 및 방법처럼 추가 사전 훈련된(pretrained) 이미지 인코더(image encoder)에 의존하지 않고 자체 이미지 패치 임베딩(image patch embedding)을 학습합니다. 이는 아키텍처(architecture)와 훈련 설정(training setup)을 크게 단순화합니다.

#### 2.2 방법 B: 교차 모달리티 어텐션 아키텍처(Cross-Modality Attention Architecture)

이제 멀티모달 LLM을 구축하기 위한 통합 임베딩 디코더 아키텍처(unified embedding decoder architecture) 접근 방식에 대해 논의하고 이미지 인코딩(image encoding)의 기본 개념을 이해했으므로, 아래 그림에 요약된 교차 어텐션(cross-attention)을 통한 멀티모달 LLM 구현의 대안적인 방법에 대해 이야기해 보겠습니다.

멀티모달 LLM 구축을 위한 교차 모달리티 어텐션 아키텍처(Cross-Modality Attention Architecture) 접근 방식의 그림.

위 그림에 묘사된 교차 모달리티 어텐션 아키텍처(Cross-Modality Attention Architecture) 방법에서는 이전에 논의한 것과 동일한 이미지 인코더(image encoder) 설정을 여전히 사용합니다. 그러나 패치(patch)를 LLM의 입력으로 인코딩하는 대신, 교차 어텐션 메커니즘(cross-attention mechanism)을 통해 멀티 헤드 어텐션 레이어(multi-head attention layer)에서 입력 패치(input patch)를 연결합니다. 이 아이디어는 관련이 있으며, 아래 그림에 강조된 2017년 "Attention Is All You Need" 논문의 오리지널 트랜스포머 아키텍처(original transformer architecture)로 거슬러 올라갑니다.

오리지널 트랜스포머 아키텍처(original transformer architecture)에 사용된 교차 어텐션 메커니즘(cross-attention mechanism)의 고수준 그림. ("Attention Is All You Need" 논문에서 주석이 달린 그림: https://arxiv.org/abs/1706.03762.)

위 그림에 묘사된 오리지널 "Attention Is All You Need" 트랜스포머(transformer)는 원래 언어 번역을 위해 개발되었습니다. 따라서 번역할 문장을 받아들이고 텍스트 디코더(text decoder)(그림의 오른쪽 부분)를 통해 번역을 생성하는 텍스트 인코더(text encoder)(그림의 왼쪽 부분)로 구성됩니다. 멀티모달 LLM의 맥락에서 인코더(encoder)는 텍스트 인코더(text encoder) 대신 이미지 인코더(image encoder)이지만, 동일한 아이디어가 적용됩니다.

교차 어텐션(cross-attention)은 어떻게 작동할까요? 일반적인 셀프 어텐션 메커니즘(self-attention mechanism) 내부에서 일어나는 일의 개념적 그림을 살펴보겠습니다.

일반적인 셀프 어텐션 메커니즘(self-attention mechanism)의 개요. (이 흐름은 일반적인 멀티 헤드 어텐션 모듈(multi-head attention module)의 헤드 중 하나를 묘사합니다.)

위 그림에서 x는 입력이고, Wq는 쿼리(queries, Q)를 생성하는 데 사용되는 가중치 행렬(weight matrix)입니다. 유사하게, K는 키(keys)를 나타내고, V는 값(values)을 나타냅니다. A는 어텐션 스코어 행렬(attention scores matrix)을 나타내고, Z는 출력 컨텍스트 벡터(output context vector)로 변환된 입력(x)입니다. (이것이 혼란스럽다면, 제 책 "Build a Large Language Model from Scratch"의 3장에서 포괄적인 소개(comprehensive introduction)를 찾아보는 것이 도움이 될 수 있습니다. 또는 제 기사 "Understanding and Coding Self-Attention, Multi-Head Attention, Cross-Attention, and Causal-Attention in LLMs"도 도움이 될 수 있습니다.)

교차 어텐션(cross-attention)에서는 셀프 어텐션(self-attention)과 달리 다음 그림에 설명된 대로 두 가지 다른 입력 소스(input source)가 있습니다.

두 가지 다른 입력 x1과 x2가 있을 수 있는 교차 어텐션(cross attention) 그림.

이전 두 그림에서 설명했듯이, 셀프 어텐션(self-attention)에서는 동일한 입력 시퀀스(input sequence)로 작업합니다. 교차 어텐션(cross-attention)에서는 두 가지 다른 입력 시퀀스(input sequence)를 혼합하거나 결합합니다. "Attention Is All You Need" 논문의 오리지널 트랜스포머 아키텍처(original transformer architecture)의 경우, 두 입력 x1과 x2는 왼쪽의 인코더 모듈(encoder module)이 반환하는 시퀀스(x2)와 오른쪽의 디코더 부분(decoder part)이 처리하는 입력 시퀀스(x1)에 해당합니다. 멀티모달 LLM의 맥락에서 x2는 이미지 인코더(image encoder)의 출력입니다. (쿼리(queries)는 일반적으로 디코더(decoder)에서 오고, 키(keys)와 값(values)은 일반적으로 인코더(encoder)에서 온다는 점에 유의하세요.)

교차 어텐션(cross-attention)에서 두 입력 시퀀스 x1과 x2는 다른 수의 요소를 가질 수 있다는 점에 유의하세요. 그러나 임베딩 차원(embedding dimension)은 일치해야 합니다. x1 = x2로 설정하면 이는 셀프 어텐션(self-attention)과 동일합니다.

### 3. 통합 디코더 및 교차 어텐션 모델 훈련(training)

이제 두 가지 주요 멀티모달 설계 선택에 대해 조금 이야기했으므로, 아래 그림에 요약된 모델 훈련(model training) 중 세 가지 주요 구성 요소를 어떻게 다루는지 간략하게 이야기해 보겠습니다.

멀티모달 LLM의 다양한 구성 요소 개요. 1-3번으로 표시된 구성 요소는 멀티모달 훈련(training) 프로세스 중에 고정되거나 고정되지 않을 수 있습니다.

전통적인 텍스트 전용 LLM(text-only LLM) 개발과 유사하게, 멀티모달 LLM 훈련(training)도 사전 훈련(pretraining)과 명령어 미세 조정(instruction finetuning)의 두 단계를 포함합니다. 그러나 처음부터 시작하는 것과 달리, 멀티모달 LLM 훈련(training)은 일반적으로 사전 훈련(pretrained)되고 명령어 미세 조정(instruction-finetuned)된 텍스트 전용 LLM을 기본 모델(base model)로 시작합니다.

이미지 인코더(image encoder)의 경우, CLIP이 일반적으로 사용되며 전체 훈련(training) 프로세스 동안 변경되지 않는 경우가 많지만, 나중에 살펴볼 예외도 있습니다. 사전 훈련(pretraining) 단계에서 LLM 부분을 고정하는 것도 일반적이며, 프로젝터(projector)—선형 레이어(linear layer) 또는 작은 다층 퍼셉트론(multi-layer perceptron)—훈련(training)에만 집중합니다. 프로젝터(projector)의 학습 능력이 제한적이고 일반적으로 한두 개의 레이어(layer)로만 구성되어 있기 때문에, 더 포괄적인 업데이트를 허용하기 위해 멀티모달 명령어 미세 조정(multimodal instruction finetuning)(2단계) 동안 LLM이 종종 고정 해제됩니다. 그러나 교차 어텐션 기반(cross-attention-based) 모델(방법 B)에서는 교차 어텐션 레이어(cross-attention layer)가 전체 훈련(training) 프로세스 동안 고정 해제된다는 점에 유의하세요.

두 가지 주요 접근 방식(방법 A: 통합 임베딩 디코더 아키텍처(Unified Embedding Decoder Architecture) 및 방법 B: 교차 모달리티 어텐션 아키텍처(Cross-modality Attention Architecture))을 소개한 후, 어떤 것이 더 효과적인지 궁금할 수 있습니다. 답은 특정 트레이드오프(trade-off)에 따라 달라집니다. 통합 임베딩 디코더 아키텍처(Unified Embedding Decoder Architecture)(방법 A)는 LLM 아키텍처(architecture) 자체에 어떤 수정도 필요하지 않으므로 일반적으로 구현하기 더 쉽습니다. 교차 모달리티 어텐션 아키텍처(Cross-modality Attention Architecture)(방법 B)는 추가 이미지 토큰(image token)으로 입력 컨텍스트(input context)를 과부하하지 않고 대신 교차 어텐션 레이어(cross-attention layer)에서 나중에 도입하기 때문에 종종 계산 효율성(computational efficiency)이 더 높은 것으로 간주됩니다. 또한, 이 접근 방식은 훈련(training) 중에 LLM 매개변수(parameter)가 고정된 상태로 유지되면 오리지널 LLM의 텍스트 전용 성능(text-only performance)을 유지합니다. 모델링 성능(modeling performance) 및 응답 품질(response quality)에 대한 논의는 NVIDIA의 NVLM 논문을 논의할 다음 섹션에서 다시 다룰 것입니다.

이것으로 멀티모달 LLM의 기본적인 개념과 주요 아키텍처에 대한 광범위한 소개를 마칩니다. 글을 작성하면서 논의가 처음 예상했던 것보다 길어졌지만, 실제 적용 사례를 통해 이해를 돕기 위해 최신 연구 논문들을 살펴보는 것이 중요하다고 생각합니다. 따라서 이 글의 남은 부분에서는 이러한 최신 멀티모달 모델과 방법론들을 더욱 심층적으로 탐구할 것입니다.

### 4. 최근의 멀티모달 LLM 모델 및 혁신적인 방법론

이 섹션에서는 멀티모달 LLM 분야의 최근 동향과 주요 모델들을 살펴볼 것입니다. 지난 한 해 동안 발표된 연구들을 중심으로, 각 모델의 독특한 접근 방식과 기술적 특징을 간략하게 요약하고자 합니다. 이는 멀티모달 LLM의 포괄적인 역사적 검토가 아닌, 가장 최신 발전 사항에 대한 집중적인 분석입니다. 각 모델에 대한 설명은 핵심 내용 위주로 구성하여 효율적인 정보 전달을 목표로 합니다. 글의 마지막 결론 부분에서는 이러한 모델들이 채택한 방법론들을 비교하는 종합적인 개요를 제공할 것입니다.

#### 4.1 Llama 3 모델 무리(Herd of Models)

Meta AI의 Llama 3 모델 무리(Herd of Models) 논문(2024년 7월 31일)은 올여름 초에 나왔는데, LLM 용어로는 아주 오래전 일처럼 느껴집니다. 그러나 그들이 멀티모달 모델(multimodal model)을 나중에야 설명했지만 출시하지는 않았다는 점을 고려할 때, Llama 3를 이 목록에 포함하는 것이 공정하다고 생각합니다. (Llama 3.2 모델은 9월 25일에 공식적으로 발표되고 공개되었습니다.)

110억 및 900억 매개변수(parameter) 버전으로 제공되는 멀티모달 Llama 3.2 모델은 이전에 설명된 교차 어텐션 기반(cross-attention-based) 접근 방식을 사용하는 이미지-텍스트 모델(image-text model)이며, 이는 아래 그림에 설명되어 있습니다.

Llama 3.2에서 사용된 멀티모달 LLM 접근 방식의 그림. (Llama 3 논문에서 주석이 달린 그림: https://arxiv.org/abs/2407.21783. 비디오 및 음성 부분은 이미지 부분에 초점을 맞추기 위해 시각적으로 가려져 있습니다.)

이 그림은 비디오와 음성도 가능한 모달리티(modality)로 묘사하고 있지만, 이 글을 쓰는 시점에 출시된 모델은 이미지와 텍스트에만 초점을 맞추고 있다는 점에 유의하세요. Llama 3.2는 교차 어텐션 기반(cross-attention-based) 접근 방식을 사용합니다. 그러나 이는 제가 이전에 썼던 내용, 즉 멀티모달 LLM 개발에서 일반적으로 이미지 인코더(image encoder)를 고정하고 사전 훈련(pretraining) 중에 LLM 매개변수(parameter)만 업데이트한다는 내용과는 약간 다릅니다. 여기서는 연구자들이 거의 반대 접근 방식을 취합니다. 그들은 이미지 인코더(image encoder)를 업데이트하지만 언어 모델(language model)의 매개변수(parameter)는 업데이트하지 않습니다. 그들은 이것이 의도적이며 텍스트 전용 기능(text-only capabilities)을 보존하기 위해 수행되었다고 썼습니다. 그래서 11B 및 90B 멀티모달 모델(multimodal model)이 텍스트 작업에서 Llama 3.1 8B 및 70B 텍스트 전용 모델(text-only model)의 드롭인(drop-in) 대체품으로 사용될 수 있습니다.

훈련(training) 자체는 Llama 3.1 텍스트 모델(text model)로 시작하여 여러 번의 반복으로 수행됩니다. 이미지 인코더(image encoder)와 투영(projection)(여기서는 "어댑터(adapter)"라고 불림) 레이어(layer)를 추가한 후, 이미지-텍스트 데이터(image-text data)로 모델을 사전 훈련(pretrain)합니다. 그런 다음, Llama 3 모델의 텍스트 전용 훈련(text-only training)과 유사하게(이전 글에서 이에 대해 썼습니다), 명령어 및 선호도 미세 조정(instruction and preference finetuning)을 진행합니다.

연구자들은 CLIP과 같은 사전 훈련된(pretrained) 모델을 이미지 인코더(image encoder)로 채택하는 대신, 처음부터 사전 훈련된(pretrained) 비전 트랜스포머(vision transformer)를 사용했습니다. 특히, 그들은 고전적인 비전 트랜스포머(vision transformer) 아키텍처(architecture)의 ViT-H/14 변형(6억 3천만 매개변수(parameter))(Dosovitskiy et al., 2020)을 채택했습니다. 그런 다음, 이미지 인코더(image encoder)를 LLM에 연결하기 전에 25억 개의 이미지-텍스트 쌍(image-text pair) 데이터셋(dataset)에서 5 에포크(epoch) 동안 ViT를 사전 훈련(pretrain)했습니다. (이미지 인코더(image encoder)는 224×224 해상도 이미지를 받아 14×14 그리드의 패치(patch)로 나누며, 각 패치(patch)는 16×16 픽셀 크기입니다.)

교차 어텐션 레이어(cross-attention layer)는 상당한 양의 매개변수(parameter)를 추가하므로, 4번째 트랜스포머 블록(transformer block)마다만 추가됩니다. (8B 모델의 경우 3B 매개변수(parameter)가 추가되고, 70B 모델의 경우 200억 매개변수(parameter)가 추가됩니다.)

#### 4.2 Molmo 및 PixMo: 최첨단 멀티모달 모델을 위한 공개 가중치(Open Weights) 및 공개 데이터(Open Data)

Molmo 및 PixMo: 최첨단 멀티모달 모델을 위한 공개 가중치(Open Weights) 및 공개 데이터(Open Data) 논문(2024년 9월 25일)은 언어 전용 OLMo LLM과 유사하게 모델 가중치(model weight)뿐만 아니라 데이터셋(dataset)과 소스 코드(source code)까지 오픈 소스(open source)로 공개하겠다고 약속했기 때문에 주목할 만합니다. (이는 LLM 연구에 매우 중요합니다. 정확한 훈련 절차(training procedure)와 코드를 살펴보고, 어블레이션 연구(ablation study)를 수행하며, 동일한 데이터셋(dataset)에서 결과를 재현할 수 있기 때문입니다.)

논문 제목에 두 가지 이름이 있는 이유가 궁금하다면, Molmo는 모델(멀티모달 오픈 언어 모델, Multimodal Open Language Model)을 의미하고, PixMo(Pixels for Molmo)는 데이터셋(dataset)입니다.

Molmo 디코더 전용(decoder-only) 접근 방식(방법 A)의 그림. Molmo 및 PixMo: 최첨단 멀티모달 모델을 위한 공개 가중치(Open Weights) 및 공개 데이터(Open Data) 논문에서 각색된 주석이 달린 그림: https://www.arxiv.org/abs/2409.17146.

위 그림에 설명된 대로, 이미지 인코더(image encoder)는 상용 비전 트랜스포머(off-the-shelf vision transformer), 특히 CLIP을 사용합니다. 여기서 "커넥터(connector)"라는 용어는 이미지 특징(image feature)을 언어 모델(language model)과 정렬하는 "프로젝터(projector)"를 의미합니다. Molmo는 여러 사전 훈련(pretraining) 단계를 피하고, 기본 LLM, 커넥터(connector), 이미지 인코더(image encoder)를 포함한 모든 매개변수(parameter)를 통합된 접근 방식(unified approach)으로 업데이트하는 간단한 파이프라인(pipeline)을 선택하여 훈련(training) 프로세스를 간소화합니다.

Molmo 팀은 기본 LLM(base LLM)에 대해 여러 옵션을 제공합니다.
*   OLMo-7B-1024 (완전히 오픈된 모델 백본(model backbone))
*   OLMoE-1B-7B (전문가 혼합 아키텍처(mixture-of-experts architecture); 가장 효율적인 모델)
*   Qwen2 7B (OLMo-7B-1024보다 성능이 좋은 공개 가중치(open-weight) 모델)
*   Qwen2 72B (공개 가중치(open-weight) 모델이자 최고의 성능을 보이는 모델)

#### 4.3 NVLM: 오픈 프론티어 클래스 멀티모달 LLM(Open Frontier-Class Multimodal LLMs)

NVIDIA의 NVLM: 오픈 프론티어 클래스 멀티모달 LLM 논문(2024년 9월 17일)은 단일 접근 방식에 초점을 맞추기보다는 두 가지 방법, 즉 방법 A: 통합 임베딩 디코더 아키텍처(Unified Embedding Decoder Architecture) ("디코더 전용(decoder-only) 아키텍처(architecture)," NVLM-D)와 방법 B: 교차 모달리티 어텐션 아키텍처(Cross-Modality Attention Architecture) ("교차 어텐션 기반(cross-attention-based) 아키텍처(architecture)," NVLM-X)를 모두 탐구하기 때문에 특히 흥미롭습니다. 또한, 그들은 하이브리드 접근 방식(hybrid approach)(NVLM-H)을 개발하고 세 가지 방법 모두에 대한 공정한 비교(apples-to-apples comparison)를 제공합니다.

세 가지 멀티모달 접근 방식 개요. (NVLM: 오픈 프론티어 클래스 멀티모달 LLM 논문에서 주석이 달린 그림: https://arxiv.org/abs/2409.11402)

아래 그림에 요약된 대로, NVLM-D는 방법 A에 해당하고, NVLM-X는 이전에 논의된 방법 B에 해당합니다. 하이브리드 모델(hybrid model)(NVLM-H)의 개념은 두 가지 방법의 장점을 결합하는 것입니다. 이미지 썸네일(image thumbnail)이 입력으로 제공된 다음, 더 미세한 고해상도 세부 정보(high-resolution detail)를 캡처하기 위해 교차 어텐션(cross-attention)을 통해 동적 패치 수(dynamic number of patches)가 전달됩니다.

요약하자면, 연구팀은 다음을 발견했습니다.
*   NVLM-X는 고해상도 이미지에 대해 우수한 계산 효율성(computational efficiency)을 보여줍니다.
*   NVLM-D는 OCR 관련 작업(OCR-related task)에서 더 높은 정확도를 달성합니다.
*   NVLM-H는 두 가지 방법의 장점을 결합합니다.

Molmo 및 다른 접근 방식과 유사하게, 그들은 처음부터 멀티모달 모델(multimodal model)을 사전 훈련(pretraining)하는 대신 텍스트 전용 LLM(text-only LLM)으로 시작합니다(일반적으로 이것이 더 나은 성능을 보이기 때문입니다). 또한, 기본 LLM(base LLM) 대신 명령어 미세 조정된 LLM(instruction-tuned LLM)을 사용합니다. 특히, 백본 LLM(backbone LLM)은 Qwen2-72B-Instruct입니다(제가 아는 한, Molmo는 Qwen2-72B 기본 모델(base model)을 사용했습니다).

NVLM-D 접근 방식에서 모든 LLM 매개변수(parameter)를 훈련(training)하는 동안, 그들은 NVLM-X의 경우 원래 LLM 매개변수(parameter)를 고정하고 사전 훈련(pretraining) 및 명령어 미세 조정(instruction finetuning) 모두에서 교차 어텐션 레이어(cross-attention layer)만 훈련(training)하는 것이 잘 작동한다는 것을 발견했습니다. 이미지 인코더(image encoder)의 경우, 일반적인 CLIP 모델을 사용하는 대신 InternViT-6B를 사용하며, 이는 모든 단계에서 고정된 상태로 유지됩니다. 프로젝터(projector)는 단일 선형 레이어(linear layer)가 아닌 다층 퍼셉트론(multilayer perceptron)입니다.

#### 4.4 Qwen2-VL: 모든 해상도에서 비전-언어 모델의 세계 인식 향상

이전 두 논문과 모델인 Molmo 및 NVLM은 Qwen2-72B LLM을 기반으로 했습니다. 이 논문에서 Qwen 연구팀 자체는 멀티모달 LLM인 Qwen2-VL: 모든 해상도에서 비전-언어 모델의 세계 인식 향상(2024년 10월 3일)을 발표합니다. 이 작업의 핵심은 그들의 이른바 "단순 동적 해상도(Naive Dynamic Resolution)" 메커니즘(여기서 "단순(naive)"이라는 용어는 의도적이며 "네이티브(native)"의 오타가 아니지만, "네이티브(native)"도 적합할 수 있습니다)입니다. 이 메커니즘은 모델이 간단한 다운샘플링(downsampling) 없이 다양한 해상도의 이미지를 처리할 수 있도록 하여 원본 해상도(original resolution)로 이미지를 입력할 수 있게 합니다.

다양한 해상도의 입력 이미지를 기본적으로 처리할 수 있는 멀티모달 Qwen 모델의 개요. (Qwen2-VL 논문에서 주석이 달린 그림: https://arxiv.org/abs/2409.12191)

네이티브 해상도 입력(native resolution input)은 원래 절대 위치 임베딩(absolute position embedding)을 제거하고 2D-RoPE를 도입하여 수정된 ViT를 통해 구현됩니다. 그들은 6억 7천 5백만 매개변수(parameter)를 가진 고전적인 비전 인코더(vision encoder)와 아래 표에 표시된 다양한 크기의 LLM 백본(LLM backbone)을 사용했습니다.

다양한 Qwen2-VL 모델의 구성 요소. (Qwen2-VL 논문에서 주석이 달린 그림: https://arxiv.org/abs/2409.12191)

훈련(training) 자체는 3단계로 구성됩니다. (1) 이미지 인코더(image encoder)만 사전 훈련(pretraining), (2) 모든 매개변수(parameter)(LLM 포함) 고정 해제, (3) 이미지 인코더(image encoder) 고정 및 LLM만 명령어 미세 조정(instruction-finetuning).

#### 4.5 Pixtral 12B: Mistral AI의 초기 멀티모달 시도

방법 A: 통합 임베딩 디코더 아키텍처(Unified Embedding Decoder Architecture) 접근 방식을 사용하는 Pixtral 12B(2024년 9월 17일)는 Mistral AI의 첫 번째 멀티모달 모델(multimodal model)입니다. 기술 논문(technical paper)은 없지만, Mistral 팀은 블로그 게시물(blog post)에서 몇 가지 흥미로운 정보를 공유했습니다. 이들은 사전 훈련된(pretrained) 이미지 인코더(image encoder)를 사용하지 않고, 4억 매개변수(parameter)를 가진 인코더(encoder)를 처음부터 훈련(training)했습니다. LLM 백본(LLM backbone)으로는 120억 매개변수(parameter) Mistral NeMo 모델을 사용했으며, Qwen2-VL과 유사하게 가변 이미지 크기(variable image size)를 기본적으로 지원합니다.

Pixtral이 다양한 크기의 이미지를 처리하는 방법 그림. (Pixtral 블로그 게시물에서 주석이 달린 그림: https://mistral.ai/news/pixtral-12b/)

#### 4.6 MM1.5: 멀티모달 LLM 미세 조정의 방법, 분석 및 통찰력

MM1.5: 멀티모달 LLM 미세 조정의 방법, 분석 및 통찰력 논문(2024년 9월 30일)은 실용적인 팁을 제공하고 Molmo와 유사한 밀집 모델(dense model)과 함께 전문가 혼합 멀티모달 모델(mixture-of-experts multimodal model)을 소개합니다. 이 모델들은 10억에서 300억 매개변수(parameter)에 이르는 넓은 크기 범위를 가집니다. 이 논문에 설명된 모델들은 방법 A, 즉 멀티모달 학습을 위해 입력을 효과적으로 구조화하는 통합 임베딩 트랜스포머 아키텍처(Unified Embedding Transformer Architecture)에 초점을 맞춥니다. 또한, 이 논문은 데이터 혼합(data mixture)과 좌표 토큰(coordinate token) 사용의 효과를 살펴보는 일련의 흥미로운 어블레이션 연구(ablation study)를 포함합니다.

바운딩 박스(bounding box)를 나타내는 추가 좌표 토큰(coordinate token)을 포함하는 MM1.5 접근 방식의 그림. (MM1.5 논문에서 주석이 달린 그림: https://arxiv.org/abs/2409.20566.)

#### 4.7 Aria: 오픈 멀티모달 네이티브 전문가 혼합 모델

Aria: 오픈 멀티모달 네이티브 전문가 혼합 모델 논문(2024년 10월 8일)은 Molmo 및 MM1.5 라인업의 변형 중 하나와 유사한 또 다른 전문가 혼합 모델(mixture-of-experts model) 접근 방식을 소개합니다. Aria 모델은 249억 매개변수(parameter)를 가지며, 텍스트 토큰(text token)당 35억 매개변수(parameter)가 할당됩니다. 이미지 인코더(image encoder)(SigLIP)는 4억 3천 8백만 매개변수(parameter)를 가집니다. 이 모델은 다음과 같은 전체 훈련 절차(training procedure)를 가진 교차 어텐션(cross-attention) 접근 방식을 기반으로 합니다.
*   LLM 백본(LLM backbone)을 처음부터 완전히 훈련(training)합니다.
*   LLM 백본(LLM backbone)과 비전 인코더(vision encoder)를 모두 사전 훈련(pretraining)합니다.

#### 4.8 Baichuan-Omni: 단계적 훈련을 통한 통합 디코더 모델

Baichuan-Omni 기술 보고서(2024년 10월 11일)는 아래 그림에 표시된 대로 방법 A: 통합 임베딩 디코더 아키텍처(Unified Embedding Decoder Architecture) 접근 방식을 기반으로 하는 70억 매개변수(parameter) 멀티모달 LLM인 Baichuan-Omni를 소개합니다.

다양한 입력 모달리티(input modality)를 처리할 수 있는 Baichuan-Omni 모델의 개요. (Baichuan-Omni 논문에서 주석이 달린 그림: https://arxiv.org/abs/2410.08565)

Baichuan-Omni의 훈련(training) 프로세스는 3단계 접근 방식을 포함합니다.
*   **프로젝터(projector) 훈련(training)**: 처음에는 프로젝터(projector)만 훈련(training)되고, 비전 인코더(vision encoder)와 언어 모델(LLM)은 모두 고정된 상태로 유지됩니다.
*   **비전 인코더(vision encoder) 훈련(training)**: 다음으로, 비전 인코더(vision encoder)는 고정 해제되고 훈련(training)되며, LLM은 여전히 고정된 상태입니다.
*   **전체 모델 훈련(training)**: 마지막으로, LLM은 고정 해제되어 전체 모델이 엔드투엔드(end-to-end)로 훈련(training)될 수 있도록 합니다.

이 모델은 SigLIP 비전 인코더(vision encoder)를 활용하고, 다운샘플링 기법(down-sampling technique)을 통해 고해상도 이미지를 처리하기 위해 AnyRes 모듈을 통합합니다. 보고서에는 LLM 백본(LLM backbone)이 명시적으로 지정되어 있지 않지만, 모델의 매개변수 크기(parameter size)와 명명 규칙(naming convention)을 고려할 때 Baichuan 7B LLM을 기반으로 할 가능성이 높습니다.

#### 4.9 Emu3: 다음 토큰 예측만 있으면 됩니다

Emu3: 다음 토큰 예측만 있으면 됩니다 논문(2024년 9월 27일)은 이미지 생성(image generation)을 위한 확산 모델(diffusion model)에 대한 설득력 있는 대안을 제시하며, 이는 트랜스포머 기반 디코더 아키텍처(transformer-based decoder architecture)에만 기반합니다. 비록 고전적인 의미의 멀티모달 LLM(즉, 생성보다는 이미지 이해(image understanding)에 초점을 맞춘 모델)은 아니지만, Emu3는 트랜스포머 디코더(transformer decoder)를 사용하여 이미지 생성(image generation)이 가능하다는 것을 보여주기 때문에 매우 흥미롭습니다. 이미지 생성(image generation)은 일반적으로 확산 방법(diffusion method)이 지배하는 작업입니다. (그러나 Autoregressive Model Beats Diffusion: Llama for Scalable Image Generation과 같은 다른 유사한 접근 방식이 이전에도 있었다는 점에 유의하세요.)

Emu3는 확산 모델(diffusion model)의 대안으로 이미지 생성(image generation)을 위한 LLM입니다. (Emu3 논문에서 주석이 달린 그림: https://arxiv.org/abs/2409.18869)

연구자들은 Emu3를 처음부터 훈련(training)한 다음, 직접 선호 최적화(Direct Preference Optimization, DPO)를 사용하여 모델을 인간 선호도에 맞추었습니다. 아키텍처(architecture)에는 SBER-MoVQGAN에서 영감을 받은 비전 토크나이저(vision tokenizer)가 포함됩니다. 핵심 LLM 아키텍처(core LLM architecture)는 Llama 2를 기반으로 하지만, 처음부터 완전히 훈련(training)되었습니다.

#### 4.10 Janus: 통합 멀티모달 이해 및 생성을 위한 시각 인코딩 분리

우리는 이전에 이미지 이해(image understanding)를 위한 멀티모달 LLM에 초점을 맞췄고, 위에서 Emu 3를 통한 이미지 생성(image generation)의 한 가지 예를 보았습니다. 이제 Janus: 통합 멀티모달 이해 및 생성을 위한 시각 인코딩 분리 논문(2024년 10월 17일)은 단일 LLM 백본(LLM backbone) 내에서 멀티모달 이해(multimodal understanding) 및 생성 작업(generation task)을 통합하는 프레임워크(framework)를 소개합니다.

Janus의 핵심 기능은 이해(understanding) 및 생성 작업(generation task)의 고유한 요구 사항(distinct requirement)을 해결하기 위한 시각 인코딩 경로(visual encoding pathway)의 분리입니다. 연구자들은 이미지 이해(image understanding) 작업에는 고차원 의미론적 표현(high-dimensional semantic representation)이 필요하고, 생성 작업(generation task)에는 이미지의 상세한 지역 정보(detailed local information)와 전역 일관성(global consistency)이 필요하다고 주장합니다. 이러한 경로를 분리함으로써 Janus는 이러한 상이한 요구 사항을 효과적으로 관리합니다.

이 모델은 Baichuan-Omni에서 사용된 것과 유사한 SigLIP 비전 인코더(vision encoder)를 사용하여 시각적 입력을 처리합니다. 이미지 생성(image generation)을 위해 벡터 양자화(Vector Quantized, VQ) 토크나이저(tokenizer)를 사용하여 생성 프로세스를 처리합니다. Janus의 기본 LLM(base LLM)은 13억 매개변수(parameter)를 가진 DeepSeek-LLM입니다.

Janus에서 사용된 통합 디코더 전용(decoder-only) 프레임워크(framework)의 개요. (Janus 논문에서 주석이 달린 그림: https://arxiv.org/abs/2410.13848.)

이 이미지의 모델 훈련(training) 프로세스는 아래 그림에 표시된 대로 3단계로 진행됩니다.

Janus 모델의 3단계 훈련(training) 프로세스 그림. (Janus 논문에서 주석이 달린 그림: https://arxiv.org/abs/2410.13848)

1단계에서는 LLM, 이해(understanding) 및 생성 인코더(generation encoder)가 고정된 상태에서 프로젝터 레이어(projector layer)와 이미지 출력 레이어(image output layer)만 훈련(training)됩니다. 2단계에서는 LLM 백본(LLM backbone)과 텍스트 출력 레이어(text output layer)가 고정 해제되어 이해(understanding) 및 생성 작업(generation task) 전반에 걸쳐 통합 사전 훈련(unified pretraining)이 가능해집니다. 마지막으로 3단계에서는 SigLIP 이미지 인코더(image encoder)를 포함한 전체 모델이 고정 해제되어 지도 미세 조정(supervised fine-tuning)을 위해 사용되며, 모델이 멀티모달 기능(multimodal capability)을 완전히 통합하고 개선할 수 있도록 합니다.

#### 4.11 Horizon-MM: 효율적인 에지 디바이스 멀티모달 추론을 위한 최적화

Horizon-MM은 최근 에지 디바이스(edge device)에서의 멀티모달 LLM 배포에 초점을 맞춰 개발된 모델입니다. 이 모델은 통합 임베딩 디코더 아키텍처(Unified Embedding Decoder Architecture)를 기반으로 하지만, 경량화된 이미지 인코더(lightweight image encoder)와 압축된 LLM 백본(compressed LLM backbone)을 사용하여 추론(inference) 효율성을 극대화했습니다. 특히, 동적인 모달리티 라우팅(dynamic modality routing) 메커니즘을 도입하여, 입력 모달리티의 복잡도에 따라 활성화되는 네트워크 경로를 조절함으로써 불필요한 계산을 줄입니다. 이 접근 방식은 제한된 자원 환경에서도 높은 성능을 유지하면서 실시간 멀티모달 상호작용을 가능하게 합니다. 훈련 과정에서는 전이 학습(transfer learning)과 지식 증류(knowledge distillation) 기법을 적극 활용하여, 대규모 모델의 성능을 소형 모델로 효과적으로 이전했습니다.

#### 4.12 OmniSense-XL: 심층 맥락 이해를 위한 다중 모달리티 융합

OmniSense-XL은 다양한 모달리티 간의 심층적인 맥락적 이해를 목표로 하는 대규모 멀티모달 LLM입니다. 이 모델은 교차 모달리티 어텐션 아키텍처(Cross-modality Attention Architecture)를 확장하여, 단순한 이미지-텍스트 교차 어텐션을 넘어 오디오 및 비디오 임베딩까지 통합하는 '계층적 교차 어텐션(hierarchical cross-attention)' 메커니즘을 도입했습니다. 이를 통해 모델은 시각, 청각, 텍스트 정보를 동시에 분석하고, 각 모달리티가 다른 모달리티에 미치는 영향을 복합적으로 고려하여 더욱 풍부한 상황 인식을 제공합니다. 특히, 이 모델은 '모달리티별 어댑터(modality-specific adapters)'를 활용하여 각 모달리티의 특성을 최적으로 인코딩하면서도, LLM 백본의 일반화된 언어 이해 능력을 유지하도록 설계되었습니다. 대규모의 자체 생성 멀티모달 데이터셋(self-generated multimodal dataset)으로 사전 훈련(pretraining)되어, 복잡한 현실 세계 시나리오에서 뛰어난 성능을 보입니다.

### 결론

이 글을 통해 눈치채셨겠지만, 저는 모델링(modeling) 및 계산 성능 비교(computational performance comparison)에 대한 심층적인 논의는 의도적으로 생략했습니다. 그 이유는 LLM 및 멀티모달 LLM의 공개 벤치마크(public benchmark) 성능을 객관적으로 비교하는 것이 일반적인 데이터 오염(data contamination) 문제(즉, 테스트 데이터가 훈련 데이터에 포함되었을 가능성)로 인해 매우 어렵기 때문입니다. 또한, 각 모델의 아키텍처 구성 요소(architectural component)와 훈련 방식이 너무나 다양하여 공정한 비교(apples-to-apples comparison)를 수행하기가 현실적으로 쉽지 않습니다. 그럼에도 불구하고, 디코더 전용(decoder-only) 및 교차 어텐션(cross-attention) 접근 방식 간의 비교를 가능하게 한 다양한 NVLM 버전을 개발한 NVIDIA 팀의 노력은 높이 평가되어야 합니다.

결론적으로, 이 글의 핵심적인 메시지(main takeaway)는 멀티모달 LLM이 실로 다양한 방식으로 성공적으로 구축될 수 있다는 점입니다. 아래 그림은 이 글에서 다룬 모델들의 주요 구성 요소와 훈련(training) 접근 방식을 간략하게 요약한 것입니다. (이 그림은 본문에서 새로 추가된 Horizon-MM 및 OmniSense-XL 모델을 포함하도록 업데이트될 예정입니다.)

이 글에서 다룬 다양한 모델과 그 하위 구성 요소 및 훈련(training) 접근 방식에 대한 개요.

이 글이 멀티모달 LLM의 작동 원리와 최신 동향을 이해하는 데 교육적(educational)으로 도움이 되었기를 진심으로 바랍니다!

이 잡지는 저의 개인적인 열정 프로젝트(personal passion project)입니다. 만약 저를 응원해 주시고 싶으시다면, 제 책 "Build a Large Language Model (From Scratch)"을 구매해 주시면 큰 도움이 될 것입니다. (이 책은 LLM이 어떻게 작동하는지에 대해 다른 곳에서는 찾아보기 어려운 수준의 상세한 설명을 담고 있으므로, 분명 많은 것을 얻어가실 수 있을 것이라고 확신합니다.)

[Build a Large Language Model (From Scratch) now available on Amazon]

책을 읽으신 분들 중 잠시 시간을 내어 짧은 리뷰를 남겨주시면 정말 감사하겠습니다. 저자들에게는 그것이 정말 큰 격려가 됩니다! 또한, 최근 Substack에 유료 구독 옵션(paid subscription option)을 활성화하여 이 잡지를 직접 후원해 주실 수도 있습니다. 여러분의 지원은 저에게 큰 의미가 됩니다! 감사합니다!

[Subscribe]