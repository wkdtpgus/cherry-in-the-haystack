(출처: [1, 2, 4, 6, 12]) 최근 공개된 Llama 4 [1] 모델은 완전무결한 상태는 아니었지만, 이 새로운 모델 계열로부터 얻을 수 있는 통찰은 매우 많습니다. 근본적으로 Llama 4는 Meta의 연구 방향에서 중대한 변화를 의미합니다. 갈수록 치열해지는 경쟁 환경 속에서 Meta는 Llama 시리즈를 혁신하고 있으며, 선두적인 대규모 언어 모델(LLM) 개발에 분명히 집중하고 있습니다. LLM 개발이 본질적으로 반복적인 과정임을 고려할 때, 이러한 전략적 전환은 상당한 위험을 수반합니다. 즉, 초기 단계에서는 이 모델들의 성능이 기대에 미치지 못할 가능성이 높습니다. 현재 Llama 4는 어느 정도 아쉬운 결과로 평가되지만, Llama의 장기적인 성공은 Meta가 이러한 모델들을 얼마나 신속하게 개선하고 반복해 나가는지에 달려 있을 것입니다. 공개 LLM 연구의 가장 매력적인 — 또는 개발자들에게는 부담스러운 — 측면은 이러한 학습 과정이 대중에게 투명하게 공개된다는 점입니다. 우리는 Meta가 이 분야의 최고 수준 모델들과 어깨를 나란히 하기 위해 어떤 핵심적인 구조 변경을 시도하고 있는지 면밀히 분석할 수 있습니다. 이러한 변화들을 탐구함으로써 우리는 최첨단 LLM이 어떻게 구축되고 발전하는지에 대한 더 깊이 있는 이해를 얻게 됩니다. 본 심층 분석에서는 Llama 4 및 관련 모델들을 상세히 검토하여 이러한 목표를 달성할 것입니다. 이어서, 이러한 이해를 바탕으로 LLM 연구의 주요 동향, Llama의 미래 전망, 그리고 Llama 4 이후 Meta가 성공을 위해 반드시 추진해야 할 변화들을 다각적으로 분석할 예정입니다.

### Llama 4 모델 구조 (Model Architecture)의 혁신

먼저 Llama 4 모델의 기본 구조(model architecture)를 개괄하고, 이전 세대 Llama 모델들과 비교했을 때 두드러지는 주요 설계 변경점들을 강조할 것입니다. 보시다시피, 새로운 Llama 모델은 이전과는 판이하게 다른 모델 설계를 채택하고 있으며, 이는 연구 방향과 전략에 있어 명확한 전환점을 시사합니다. 과거 Llama 모델들은 간결성과 실용성을 중시했던 반면, Llama 4는 성능과 효율성을 극대화하기 위해 복잡성이 증가하더라도 첨단 기술을 적극적으로 수용함으로써, 비공개 및 공개 분야의 선두적인 LLM 연구 기관들과 동등한 수준에 도달하려는 확고한 의지를 보여주고 있습니다. 이러한 변화는 대규모 언어 모델이 직면한 확장성 한계를 극복하고, 더욱 복잡하고 정교한 추론 능력을 갖춘 시스템을 구축하기 위한 필수적인 단계로 해석될 수 있습니다.

### 전문가 혼합 (Mixture-of-Experts, MoE)의 도입

> "우리는 모델 개발 프로세스를 확장하는 능력을 극대화하기 위한 설계 선택을 합니다. 예를 들어, 훈련 안정성을 극대화하기 위해 전문가 혼합(mixture-of-experts) 모델 대신 사소한 수정이 가해진 표준 밀집 트랜스포머(dense Transformer) 모델 아키텍처(architecture)를 선택합니다."
>
> — Llama 3 논문 [2]에서 발췌

일반적인 밀집 디코더 전용 트랜스포머(dense decoder-only transformer) 방식(아래 그림 참조)에서 벗어나, Llama 4는 Llama 모델 중 최초로 전문가 혼합(Mixture-of-Experts, MoE) 구조(architecture)를 채택했습니다. Llama 3는 훈련(training) 및 추론(inference) 과정에서 발생하는 추가적인 복잡성 때문에 MoE 기법을 회피했습니다. 그러나 Llama 4를 통해 Meta는 이미 MoE 구조(architecture)를 성공적으로 활용하고 있는 선도적인 공개 모델(예: DeepSeek-v3 [4]) 및 독점 모델(예: GPT-4)들과 발을 맞추게 되었습니다. 이러한 전환은 대규모 모델의 성능을 향상시키면서도 계산 효율성을 유지하려는 업계 전반의 추세를 반영합니다.

**디코더 전용 트랜스포머(decoder-only transformer) 구조(architecture)**

간단히 말해, 밀집 모델(dense model)은 구조가 명료하고 효과적이지만, 규모를 확장하는 데 어려움이 따릅니다. MoE 구조(architecture)를 활용함으로써, 우리는 극도로 큰 모델의 훈련(training) 및 추론(inference) 효율성을 비약적으로 증진시킬 수 있으며, 이는 전례 없는 규모의 모델 개발을 가능하게 합니다. MoE의 핵심은 모델의 특정 부분만 활성화하여 계산을 수행함으로써, 전체 모델의 파라미터 수는 거대하더라도 실제 연산에 사용되는 파라미터는 훨씬 적게 유지하는 '조건부 계산(conditional computation)' 패러다임에 있습니다.

**MoE의 핵심 원리**

대부분의 독자들은 MoE의 필요성에 대해 잘 알고 계실 것입니다. MoE는 대규모 모델의 계산 효율성을 높이기 위해 디코더 전용 트랜스포머(decoder-only transformer) 구조(architecture)를 변형한 형태입니다. MoE의 주요 개념들은 아래 세 가지 중요한 연구에서 처음 제안되었으며, 여기서 그 핵심 아이디어들을 간략히 살펴보겠습니다.

*   희소하게 게이트된 전문가 혼합 레이어(The Sparsely-Gated Mixture-of-Experts Layer)
*   스위치 트랜스포머(Switch Transformers)
*   안정적이고 전이 가능한 전문가 혼합(Stable and Transferable Mixture-of-Experts, ST-MoE)

전통적인 디코더 전용 트랜스포머(decoder-only transformer)와 비교했을 때, MoE는 트랜스포머 블록(transformer block) 내의 피드포워드(feed-forward) 구성 요소를 변경합니다. 각 블록에 하나의 피드포워드 네트워크(feed-forward network)만 존재하는 대신, MoE는 여러 개의 피드포워드 네트워크(feed-forward network)를 포함하며, 각각은 고유하고 독립적인 가중치(weight)를 가집니다. 우리는 이러한 개별 네트워크들을 "전문가(expert)"라고 칭합니다. 아래 그림을 참고하십시오.

<img src="https://miro.medium.com/v2/resize:fit:1400/1*y3zB-4712v84j0tUf_fJ9g.png" alt="Adding experts to a transformer block (source)" />

트랜스포머 블록에 전문가 추가 (출처)

MoE 구조(architecture)를 구축하기 위해, 우리는 트랜스포머(transformer)의 피드포워드 계층(feed-forward layer)을 MoE — 즉, 전문가(expert) — 계층(layer)으로 전환합니다. MoE의 각 전문가(expert)는 해당 계층(layer)의 원래 피드포워드 네트워크(feed-forward network)와 구조적으로 동일하며, 일반적으로 트랜스포머 계층(transformer layer)의 일부만 MoE 계층(layer)으로 변환됩니다. 예를 들어, Llama 4는 트랜스포머(transformer)의 모든 다른 계층(layer)이 전문가 계층(expert layer)이 되는 교차 전문가 계층(interleaved MoE layer) 방식을 사용합니다. 이는 모델의 특정 부분에만 희소성을 적용하여 효율성을 극대화하는 전략입니다.

> "우리의 새로운 Llama 4 모델은 MoE 아키텍처(architecture)를 사용하는 첫 번째 모델입니다... MoE 아키텍처(architecture)는 훈련(training) 및 추론(inference)에 더 계산 효율적이며, 고정된 훈련 부동 소수점 연산(FLOPs) 예산이 주어졌을 때 밀집 모델(dense model)에 비해 더 높은 품질을 제공합니다."
>
> — Llama 4 블로그 [1]에서

**라우팅 메커니즘(Routing mechanism).** 명백히, 트랜스포머(transformer)에서 각 피드포워드 네트워크(feed-forward network)의 여러 복사본을 생성하는 것만으로는 계산 효율성이 향상되지 않습니다. 효율성 증대를 위해서는 희소성(sparsity)을 도입해야 합니다. 다시 말해, 우리는 각 MoE 계층(layer)에서 모든 전문가(expert)를 활용하지 않습니다. 대신, 각 토큰(token)에 대해 활용할 전문가(expert)의 부분집합(예: 하나 또는 두 개의 전문가)을 선정합니다. 이를 "활성(active)" 전문가(expert) 또는 매개변수(parameter)라고 지칭합니다. 이러한 선택은 각 토큰 벡터(token vector)를 선형 계층(linear layer)을 통해 전달하여 전문가(expert) 집합에 대한 확률 분포(probability distribution)를 산출함으로써 이루어집니다. 아래 그림을 참고하십시오.

<img src="https://miro.medium.com/v2/resize:fit:1400/1*aH-0j_L47_2-c9183q192A.png" alt="Selecting experts with a routing mechanism" />

라우팅 메커니즘(routing mechanism)으로 전문가(expert) 선택

여기서부터 우리는 가장 높은 확률을 부여받은 전문가(expert)만을 활용하여 각 토큰(token)을 처리할 수 있습니다. 이러한 방식으로 우리는 각 토큰(token)에 대해 모델 전체 매개변수(parameter) 중 극히 일부만을 사용하게 됩니다. 따라서 활성화된 매개변수(active parameter)의 수는 모델의 전체 매개변수(parameter)보다 훨씬 적습니다. 이러한 특성 덕분에 우리는 전체 매개변수(parameter) 수가 방대한 모델을 훨씬 적은 총 계산 비용으로 훈련(training)할 수 있게 됩니다. 이는 모델의 용량을 극대화하면서도 실제 연산 부담을 줄이는 핵심적인 장점입니다.

> "게이팅 네트워크(gating network)는 항상 동일한 소수의 전문가(expert)에 대해 높은 가중치(weight)를 생성하는 상태로 수렴하는 경향이 있습니다. 이러한 불균형은 선호되는 전문가(expert)가 더 빠르게 훈련(training)되고 따라서 게이팅 네트워크(gating network)에 의해 더 많이 선택되기 때문에 자가 강화적으로 발생합니다."
>
> — 출처

**부하 분산(Load balancing) 및 훈련 안정성(training stability).** 표준 밀집 모델(dense model)과 유사하게 MoE를 훈련(training)할 때 여러 가지 난관에 부딪힐 수 있습니다. 첫째, 모델은 모든 토큰(token)을 특정 단일 전문가(expert)로만 전송하는 경향을 빠르게 학습할 수 있는데, 이를 "라우팅 붕괴(routing collapse)" 현상이라고 합니다. 둘째, MoE는 훈련(training) 과정에서 수치적 불안정성(numerical instabilities)을 겪을 가능성이 더 높으며, 이는 훈련 손실(training loss)의 발산(divergence)으로 이어질 수 있습니다. 아래 그림을 참고하십시오.

<img src="https://miro.medium.com/v2/resize:fit:1400/1*d-X-1t67p8636_g9p-Y00g.png" alt="An example of a training divergence (source)" />

훈련 발산(training divergence)의 예 (출처)

이러한 문제들을 방지하고 안정적인 훈련(training)을 보장하기 위해, 대부분의 MoE 모델들은 훈련(training) 과정에서 부하 분산 손실(load-balancing loss)을 활용합니다. 이는 MoE가 전문가(expert)들에게 균등한 확률을 할당하고 토큰(token)을 고르게 라우팅(routing)하도록 유도하여 보상을 제공하는 방식입니다. 부하 분산 손실(load-balancing loss)은 LLM의 기본적인 훈련 목표(training objective)인 표준 다음 토큰 예측 손실(next-token prediction loss)에 추가적인 손실 항을 더함으로써 수정됩니다. 아래 그림을 참고하십시오. 따라서 이러한 보조 손실(auxiliary losses)은 모델의 최종 성능에 영향을 미칠 수 있으며, 이로 인해 일부 인기 있는 MoE 기반 LLM(예: DeepSeek-v3)은 이러한 보조 손실(auxiliary losses)의 사용을 완전히 배제하기도 했습니다.

<img src="https://miro.medium.com/v2/resize:fit:1400/1*R758R64Qk5_4h4-Z28_f5w.png" alt="The auxiliary-loss-free load balancing strategy used by DeepSeek-v3 [4]" />

DeepSeek-v3 [4]에서 사용된 보조 손실 없는 부하 분산 전략(auxiliary-loss-free load balancing strategy)

[1]에서는 Llama 4 모델 훈련(training)에 사용된 정확한 보조 손실(auxiliary losses)에 대한 언급이 없습니다(만약 사용되었다면). 훈련 불안정성(training instability)을 회피하기 위해 우리는 DeepSeek-v3와 유사한 보조 손실 없는 부하 분산 전략(auxiliary-loss-free load-balancing strategy)을 채택하고, 더 나은 가중치 초기화(weight initialization)나 선택적 정밀도(selective precision)와 같은 다양한 추가적인 기법(trick)들을 적용할 수 있습니다. 이 정보에서 우리가 얻어야 할 핵심 교훈은 MoE가 — 수많은 이점에도 불구하고 — 표준 밀집 모델(dense model)에 비해 훈련(training)하기가 훨씬 더 복잡하다는 명확한 사실입니다. 이는 간결성과 성능 사이의 전형적인 상충 관계(trade-off)를 보여줍니다! 이러한 구조(architecture)는 본질적으로 더 복잡합니다. 따라서 고려해야 할 요소들이 많아지며, 훈련(training) 과정에서 발생할 수 있는 문제의 종류와 수도 훨씬 더 다양해집니다. MoE 구조(architecture) 및 훈련(training)에 대한 심층적인 정보는 아래 링크를 참조하십시오.

*   MoE 기반 LLM 이해하기(Understanding MoE-based LLMs)
*   nanoMoE: PyTorch에서 MoE 기반 LLM 구현하기(nanoMoE: Implementing an MoE-based LLM in PyTorch)

### Llama 4의 세 가지 모델 구성.

[1]에서는 Llama 4 모델이 세 가지 버전으로 제시됩니다. 각 버전은 특정 목적과 자원 제약을 고려하여 설계되었습니다.

*   **Scout**: 총 매개변수(total parameters) 1090억 개, 활성 매개변수(active parameters) 170억 개, 계층(layer)당 전문가(expert) 16개. 이 모델은 주로 추론 효율성(inference efficiency)에 초점을 맞추어 경량화된 환경에서도 뛰어난 성능을 발휘하도록 설계되었습니다.
*   **Maverick**: 총 매개변수(total parameters) 4000억 개, 활성 매개변수(active parameters) 170억 개, 계층(layer)당 전문가(expert) 128개. 이 모델은 Scout보다 훨씬 많은 총 매개변수를 가지면서도 활성 매개변수를 효율적으로 관리하여 성능과 자원 활용의 균형을 맞춥니다.
*   **Behemoth**: 총 매개변수(total parameters) 2조 개, 활성 매개변수(active parameters) 2880억 개, 계층(layer)당 전문가(expert) 128개. 이 모델은 Llama 4 제품군 중 가장 강력하며, 최첨단 성능을 목표로 하는 연구 및 고급 응용 분야에 적합합니다.

Llama 4 Scout 및 Maverick 모델은 [1]에 명시된 Llama 4 커뮤니티 라이선스 계약(community license agreement)에 따라 대중에게 공개되었으며, Behemoth 모델은 현재 미리보기(즉, 아직 공식 출시되지 않음) 형태로만 공개되었습니다. DeepSeek-v3와 유사하게, Llama 4 모델은 공유 전문가(shared expert)와 라우팅된 전문가(routed expert)를 모두 활용합니다. 예를 들어, Llama 4 Maverick은 하나의 공유 전문가(shared expert)를 보유합니다. 이는 모든 토큰(token)이 100% 확률로 이 특정 전문가(expert)에게 전달됨을 의미하며, 추가적으로 라우팅 메커니즘(routing mechanism)을 사용하여 토큰(token)당 하나의 활성 라우팅된 전문가(active routed expert)를 선택합니다. 아래 그림을 참고하십시오.

<img src="https://miro.medium.com/v2/resize:fit:1400/1*W680t11Z_8X2eX5x-5_N0Q.png" alt="Depiction of shared and routed experts (from [3])" />

공유 전문가(shared expert) 및 라우팅된 전문가(routed expert) 묘사 (출처: [3])

다른 유명한 MoE 모델들과 비교했을 때, Llama 4 모델은 활성 매개변수(active parameter)의 수가 상대적으로 적은 편입니다. 그러나 이러한 구조(architecture)적 설정은 최상위 산업 연구소들 사이에서는 드문 일이 아닙니다.

*   Scout는 추론 효율성(inference efficiency)에 최적화되어 있어, Gemini Flash 또는 GPT-4o-mini와 같은 경량 모델들을 연상시킵니다. 이는 저자원 환경에서의 배포에 유리합니다.
*   Maverick은 DeepSeek-v3와 비교적 유사한 구조(architecture)를 가지는데, 이는 매우 많은 수의 전문가(expert)를 포함하는 희소 모델(sparse model)이라는 특징을 공유합니다.
*   Behemoth — 이 제품군에서 가장 강력한 모델 —는 GPT-4와 같은 수조 개의 매개변수(parameter)를 가진 기반 모델(foundation model)의 위상을 가집니다. 이는 복잡한 작업에서 뛰어난 성능을 기대할 수 있습니다.

그러나 Llama 4 모델과 다른 인기 있는 LLM 사이에는 여전히 중요한 차이점이 존재합니다. Llama 4에서는 계층(layer)당 하나의 라우팅된 전문가(routed expert)만 선택되는 반면, DeepSeek은 여러 공유 전문가(shared expert)와 계층(layer)당 8개의 활성 라우팅된 전문가(active routed expert)를 가집니다(즉, 370억 개의 활성 매개변수(active parameter)와 6710억 개의 전체 매개변수(total parameter)). Llama 4의 이러한 더 적은 수의 활성 매개변수(active parameter)는 모델의 훈련(training) 및 추론 효율성(inference efficiency)을 모두 향상시킵니다. 실제로 Llama 4 모델은 데이터 및 모델 규모의 극적인 증대에도 불구하고 Llama 3에 비해 훈련(training) 시 더 적은 계산량을 사용했다고 보고되었습니다. 이는 MoE 아키텍처의 설계 최적화가 가져온 결과로, 대규모 모델을 더욱 경제적으로 운영할 수 있는 가능성을 제시합니다.

### 세분화된 전문가 (Fine-grained experts)의 활용.

여러 현대 MoE 기반 LLM(예: DeepSeek-v3 및 DBRX)이 채택한 인기 있는 설계 방식 중 하나는 세분화된 전문가(fine-grained experts)의 사용입니다. 세분화된 전문가(fine-grained experts)를 활용하기 위해서는 다음과 같은 접근 방식이 필요합니다.

*   각 MoE 계층(layer)에 포함되는 전문가(expert)의 수를 대폭 늘립니다.
*   각 개별 전문가(expert)의 크기(매개변수(parameter) 수)를 상대적으로 줄입니다.

일반적으로 세분화된 MoE 모델에서는 활성 매개변수(active parameter)의 총수를 (상대적으로) 일정하게 유지하기 위해 각 계층(layer)에서 더 많은 수의 활성 전문가(active expert)를 선택하는 경향이 있습니다. Llama 4 제품군(suite)에서는 세분화된 전문가(fine-grained expert)와 거친 전문가(coarse-grained expert)가 모두 사용됩니다. Scout 모델은 총 16개의 전문가(expert)를 가지는 반면, Maverick은 총 128개의 전문가(expert)를 가집니다. Maverick이 Scout 모델보다 전문가(expert) 수가 16배 많지만 전체 매개변수(total parameter) 수는 4배에 불과하다는 점을 고려할 때, Maverick 모델이 세분화된 전문가(fine-grained expert)를 사용하고 있음이 분명합니다. 대조적으로, Scout 및 Behemoth 모델은 모두 표준적인(거친 전문가(coarse-grained expert)) 전문가(expert) 구성을 채택합니다. Meta가 이러한 다양한 선택을 하는 데에는 몇 가지 전략적인 이유가 있습니다. 일반적으로 세분화된 전문가(fine-grained expert)를 사용하면 전문가(expert)들 간의 더욱 높은 수준의 전문화(specialization)가 가능해지며, 이는 성능과 효율성을 동시에 향상시킬 수 있습니다. 그러나 세분화된 전문가(fine-grained expert)는 분산 훈련(distributed training) 과정에 추가적인 복잡성을 초래한다는 단점도 있습니다.

<img src="https://miro.medium.com/v2/resize:fit:1400/1*23F1e-01-j_5113_5t_41w.png" alt="(source)" />

(출처)

전문가(expert)들은 일반적으로 훈련(training) 중에 여러 GPU에 걸쳐 분산됩니다(즉, 전문가 병렬 처리(expert parallelism)). 위 그림을 참고하십시오. 거친 전문가(coarse-grained expert)를 사용할 때, 각 GPU가 단일 전문가(expert)를 저장하는 것이 일반적인 구성입니다. 그러나 여러 개의 세분화된 전문가(fine-grained expert)는 종종 단일 GPU의 메모리에 충분히 들어갈 수 있습니다. 또한, 세분화된 전문가(fine-grained expert)를 사용할 때 일반적으로 더 많은 수의 전문가(expert)를 선택하게 되므로, 각 토큰(token)이 클러스터(cluster) 내의 여러 다른 GPU로 라우팅(routing)되어야 하는 문제가 발생할 수 있으며, 이는 GPU 간 통신 비용을 극적으로 증가시키는 요인이 됩니다. 이러한 통신 오버헤드는 훈련 속도를 저하시키고 자원 활용 효율을 떨어뜨릴 수 있습니다.

> "우리는 각 토큰(token)이 최대 𝑀개의 노드(node)로 전송되도록 보장합니다. 이 노드(node)는 각 노드(node)에 분산된 전문가(expert)의 가장 높은 𝐾 / 𝑀 친화도 점수(affinity score)의 합계에 따라 선택됩니다. 이 제약 조건 하에서 우리의 MoE 훈련 프레임워크(training framework)는 거의 완벽한 계산-통신 오버랩(computation-communication overlap)을 달성할 수 있습니다."
>
> — DeepSeek-v3 논문 [4]에서 발췌

결과적으로, 우리는 통신 비용을 제한하고 훈련 효율성(training efficiency)을 개선하기 위한 전략들을 적극적으로 채택해야 합니다. 예를 들어, DeepSeek-v3는 위에서 설명된 노드 제한 라우팅 방식(node-limited routing scheme)을 사용하는데, 이는 단일 토큰(token)이 라우팅(routing)될 수 있는 장치(device)의 수를 제한합니다. 세분화된 전문가(fine-grained expert)를 사용하지 않음으로써 이러한 추가적인 복잡성을 피할 수 있습니다. 그러나 세분화된 전문가(fine-grained expert) 모델과 거친 전문가(coarse-grained expert) 모델을 모두 훈련(training)하는 것은 모델 사용자에게 더 많은 구성 가능성(configurability)과 선택권을 제공하여 다양한 환경과 요구사항에 맞춰 모델을 최적화할 수 있도록 합니다. 이는 모델의 유연성을 높이는 중요한 장점입니다.

### 공개 LLM 생태계에 미치는 파급 효과.

MoE 모델은 추론(inference) 시 모든 매개변수(parameter)를 사용하지는 않지만, 여전히 모델의 총 매개변수(parameter)를 GPU 메모리에 적재해야 합니다. 결과적으로, MoE 기반 LLM은 밀집 모델(dense model)에 비해 훨씬 더 큰 메모리 점유율(memory footprint)을 가지며, 따라서 더 많은 양의 고성능 GPU 자원에 접근할 수 있어야 합니다. Llama 4 Scout는 "단일 H100 GPU(Int4 양자화(quantization) 적용 시)에 탑재 가능"하다고 알려져 있지만, Maverick은 "단일 H100 호스트(host)"로는 불충분합니다. 즉, 더 큰 Maverick 모델의 추론(inference)을 단일 GPU만으로는 수행할 수 없으며, 여러 GPU 호스트(host)에 걸쳐 분산 추론(distributed inference)을 실행해야 합니다.

이러한 모든 고려 사항들을 염두에 두면, Llama가 MoE 구조(architecture)로 전환하는 것이 양날의 검과 같다는 점을 점차 깨닫게 될 것입니다.

*   Llama 프로젝트는 가장 강력한 (독점) LLM들과 동등한 수준으로 나아가고, 더 우수한 모델들을 창조할 잠재력을 열어줍니다. 이는 기술적 진보를 위한 중요한 발걸음입니다.
*   그러나 동시에, Llama 모델을 활용하기 위한 진입 장벽이 현저히 높아집니다. 이는 연구자와 개발자 커뮤니티에 상당한 영향을 미칩니다.

이러한 딜레마는 공개 LLM 연구 분야에 중대한 파급 효과를 미칩니다. 공개 LLM에 대한 접근 장벽이 높아지는 것은 상당한 부작용을 초래하며, 충분한 GPU 자원을 확보하기 어려운 개인이나 소규모 연구팀이 의미 있는 연구를 수행할 능력을 저해할 것입니다. 모델이 계속해서 발전함에 따라 연구 참여자들의 비용 부담이 점차 가중된다면, 개방형 LLM 커뮤니티는 지속적으로 번성하기 어려울 수 있습니다. 이는 AI 연구의 다양성과 포괄성을 저해할 위험이 있습니다.

> "공개 표준이 되는 모델은 반드시 전반적으로 최고의 모델일 필요는 없지만, 다양한 배포 환경에서 견고하게 작동하는 다양한 형태와 크기의 모델 패밀리여야 합니다... 희소 MoE(sparse MoE)와 같은 메모리 집약적인 모델은 공개 커뮤니티의 더 많은 참여자들을 배제시킵니다."
>
> — Nathan Lambert

MoE 구조(architecture)의 이러한 부정적인 측면을 완화하기 위해, 우리는 대규모 MoE 모델을 더 작은 밀집 모델(dense model)로 "증류(distill)"하여 여전히 뛰어난 성능을 발휘하면서도 사용자 친화적인 LLM 제품군(suite)을 제공할 수 있습니다. 이 접근 방식은 DeepSeek-R1 [5]에 의해 채택되고 대중화되었는데, 이 모델은 6710억 개의 매개변수(parameter)를 가진 MoE 기반 추론 모델(reasoning model)을 15억 개에서 700억 개의 매개변수(parameter)에 이르는 여러 밀집 LLM으로 증류(distillation)한 사례입니다. [5]의 주요 발견 중 하나는 매우 크고 강력한 모델이 교사 모델(teacher model)로 활용될 때 증류(distillation)가 가장 효과적이라는 사실입니다. 본 개요의 후반부에서 살펴보겠지만, Llama 4 모델로부터의 증류(distillation) 기법은 이미 활발히 탐색되고 있는 연구 분야입니다. 이는 고성능 모델의 혜택을 더 넓은 범위의 사용자에게 제공하기 위한 중요한 전략이 될 것입니다.

### 네이티브 다중 모달리티 (Native Multi-Modality)와 선행 융합 (Early Fusion)

과거에도 다중 모달(multi-modal) Llama 모델들이 출시된 바 있습니다. 원래 Llama 3 출판물 [2]에는 다중 모달리티(multi-modality)에 대한 예비 실험 결과가 포함되어 있었고, 이는 이후 Llama 3.2 Vision 출시와 함께 상업적으로 구현되었습니다. 다중 모달(multi-modal) Llama 3 모델의 주요 세부 사항은 아래 링크된 개요에 상세히 설명되어 있습니다. 이전 모델 세대와 마찬가지로, Llama 4 모델은 이미지와 비디오를 포함한 시각적 입력(visual inputs)을 지원합니다. 그러나 이 섹션에서 보게 되겠지만, Llama 4는 다중 모달리티(multi-modality)에 대한 접근 방식에서 극적인 변화를 시도합니다. 이는 단순한 기능 추가를 넘어선 근본적인 설계 철학의 전환을 의미합니다.

<img src="https://miro.medium.com/v2/resize:fit:1400/1*YyU-X_H_L1228_H_Y_P_9_A.png" alt="Vision Large Language Models (vLLMs) Cameron R. Wolfe, Ph.D. · Mar 31 Read full story" />

**다중 모달 구조(Multi-modal architectures).** 다중 모달 LLM은 두 가지 핵심 구성 요소를 포함합니다: LLM 백본(LLM backbone)과 비전 인코더(vision encoder). LLM 백본(LLM backbone)은 표준적인 디코더 전용 트랜스포머(decoder-only transformer)이며, 비전 인코더(vision encoder)는 일반적으로 이미지를 해당 임베딩(embedding) 집합으로 변환하는 CLIP 또는 ViT 모델입니다. 아래 그림을 참고하십시오.

<img src="https://miro.medium.com/v2/resize:fit:1400/1*d6-s-k-Y_2_E_Q_2_H_2_Y_2_G_4_A.png" alt="Using a vision encoder to produce image embeddings" />

이미지 임베딩(image embedding) 생성을 위한 비전 인코더(vision encoder) 사용

이 두 가지 구성 요소를 고려할 때, 비전 LLM(Vision LLM, 줄여서 vLLM)은 시각 정보와 텍스트 정보를 적절하게 융합하는 방법을 학습해야 합니다. 즉, LLM은 어떻게든 i) 이미지 임베딩(image embedding)을 입력으로 받아들이고 ii) 이 임베딩(embedding)을 텍스트 생성을 위한 추가적인 맥락(context)으로 활용해야 합니다. 이러한 목적을 위해 사용할 수 있는 두 가지 주요 모델 구조(model architecture)가 있습니다(아래 그림 참조).

*   **통합 임베딩(Unified embedding)**: 입력 계층(input layer)에서 이미지 토큰(image token)과 텍스트 토큰(text token)을 모두 결합하여, LLM 5에 의해 처리되는 단일 입력 시퀀스(input sequence)를 형성합니다. 이는 초기 단계에서부터 모든 모달리티를 통합하는 방식입니다.
*   **교차 모달리티 어텐션(Cross-modality attention)**: LLM에 텍스트 토큰(text token)만을 입력으로 전달하고, 추가적인 교차 어텐션 계층(cross-attention layer)을 통해 시각 정보를 모델에 융합합니다. 이 방식은 모달리티별 처리를 분리한 후 통합합니다.

<img src="https://miro.medium.com/v2/resize:fit:1400/1*l_2_F_0_G_2_D_0_E_2_R_0_R_2_A.png" alt="Multi-modal architecture variants" />

다중 모달 아키텍처(multi-modal architecture) 변형

이러한 구조(architecture)들은 각각의 장점을 가지고 있습니다. 예를 들어, 교차 모달리티 어텐션(cross-modality attention)은 이미지 임베딩(image embedding)이 전체 LLM 백본(LLM backbone)을 통과하지 않기 때문에 더 효율적인 경향이 있습니다. 그러나 통합 임베딩(unified embedding) 접근 방식은 바로 이 이유 때문에 더 우수한 성능을 발휘할 잠재력을 가지고 있습니다! 초기 단계에서의 깊은 통합은 모달리티 간의 복잡한 상호작용을 더 잘 포착할 수 있기 때문입니다.

**다중 모달 훈련(Multi-modal training).** vLLM이 텍스트를 결과물로 생성한다는 점을 고려할 때, 우리는 여전히 다음 토큰 예측(next token prediction)을 사용하여 모델을 훈련(training)합니다. 그러나 훈련 목표(training objective) 외에도 이러한 유형의 모델에 대한 몇 가지 다른 훈련 전략(training strategies) 선택지가 있습니다.

*   **네이티브 다중 모달리티(Native multi-modality)**: 처음부터 다중 모달 데이터(multi-modal data)를 사용하여 vLLM을 완전히 처음부터 훈련(training)하는 방식입니다. 이는 모달리티 간의 깊은 상호작용 학습을 목표로 합니다.
*   **구성적 다중 모달리티(Compositional multi-modality)**: 별개의 LLM 백본(LLM backbone)과 비전 인코더(vision encoder)를 개별적으로 훈련(training)한 다음, 추가 훈련(training)을 수행하여 이들을 융합하는 방식입니다.

객관적으로 말하면, 네이티브 다중 모달리티(native multi-modality)는 훈련(training) 과정에 추가적인 복잡성(예: 모달리티(modality) 간 불균형)을 야기합니다. 그러나 이러한 난관들을 성공적으로 극복할 수 있다면, 네이티브 다중 모달 훈련(natively multi-modal training)은 엄청난 잠재력을 가지고 있습니다. 이는 모델이 노출될 수 있는 데이터의 범위와 양을 획기적으로 확장하기 때문입니다. 이러한 이유로 Google과 OpenAI를 포함한 많은 최상위 연구 기관들이 이 접근 방식을 채택했으며, 이는 Llama 4 설계의 주요 동기 부여 요인이었을 가능성이 매우 높습니다.

> "Llama 4 모델은 네이티브 다중 모달리티(native multimodality)로 설계되었으며, 텍스트 및 비전 토큰(vision token)을 통합 모델 백본(unified model backbone)에 매끄럽게 통합하기 위해 선행 융합(early fusion) 방식을 포함합니다. 선행 융합(early fusion)은 방대한 양의 레이블 없는 텍스트, 이미지 및 비디오 데이터를 사용하여 모델을 공동으로 사전 훈련(pre-train)할 수 있게 해주므로 중요한 진전입니다."
>
> — Llama 4 블로그 [1]에서 발췌

이전 Llama 변형 모델들(예: Llama 3.2 Vision)은 교차 모달리티 어텐션(cross-modality attention) 구조(architecture)를 사용하고 구성적 접근 방식(compositional approach)으로 훈련(training)되었습니다. 이와 대조적으로, Llama 4 모델은 네이티브 다중 모달(natively multi-modal) 방식을 채택하며, 텍스트, 이미지 및 비디오 데이터를 활용하여 완전히 처음부터 사전 훈련(pretrained)됩니다. 네이티브 다중 모달리티(native multi-modality)로의 전환은 Llama 4 모델이 Llama 3보다 2배 이상 방대한 30조 개의 토큰(token)으로 구성된 사전 훈련 데이터셋(pretraining dataset)을 구축할 때 여러 모달리티(modality)의 데이터를 효과적으로 활용할 수 있도록 합니다. 이는 모델의 이해력과 생성 능력을 근본적으로 향상시키는 데 기여합니다.

**선행 융합(Early fusion).** 위 인용문에서 언급했듯이, Llama 4는 Llama 3에서 사용되었던 교차 모달리티 어텐션(cross-modality attention) 구조(architecture) 대신 통합 임베딩(unified embedding) 구조(architecture)를 채택합니다. [1]에서는 LLM의 입력 수준(input-level)에서 이미지와 텍스트 데이터가 결합된다는 의미로 "선행 융합(early fusion)"이라는 용어가 Llama 4 모델의 구조(architecture)를 설명하는 데 사용됩니다. 이와 대안적으로, "후기 융합(late fusion)" 구조(architecture)(예: 교차 모달리티 어텐션(cross-modality attention))는 LLM의 후반부 계층(layer)에서 이미지 및 텍스트 데이터를 결합하는 방식을 취합니다. 선행 융합은 모델이 초기 단계부터 다양한 모달리티 정보를 함께 처리하고 통합된 방식으로 이해하도록 유도하여, 더욱 깊이 있는 다중 모달 추론 능력을 가능하게 합니다.