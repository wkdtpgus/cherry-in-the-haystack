(출처: [1, 2, 4, 6, 12]) 최근 공개된 Llama 4 [1]는 완벽과는 거리가 멀었지만, 이 신형 모델들에서 얻을 수 있는 통찰은 상당합니다. 간단히 말해, Llama 4는 Meta의 연구 전략에 있어 중요한 변곡점입니다. 급증하는 경쟁 환경 속에서 Meta는 Llama 시리즈를 혁신하고 있으며, 최첨단 대규모 언어 모델(LLM) 개발에 분명히 집중하고 있습니다. LLM 개발이 반복적인 과정임을 고려할 때, 이처럼 근본적인 변화는 상당한 위험을 내포하고 있습니다. 즉, 초기에는 모델의 성능이 기대에 미치지 못할 가능성이 큽니다. 현재 Llama 4는 손실로 비춰질 수 있으나, Llama의 장기적인 성공은 Meta가 이 모델들을 얼마나 신속하게 개선하고 반복 개발하는지에 달려 있을 것입니다. 공개 LLM 연구의 가장 매력적인 — 또는 모델 개발자들에게는 부담스러운 — 측면은 이러한 학습 과정이 대중에게 투명하게 공개된다는 점입니다. 우리는 Meta가 이 분야의 선두 모델들과 어깨를 나란히 하기 위해 어떤 핵심적인 변화를 시도하고 있는지 면밀히 분석할 수 있습니다. 이러한 변화를 탐구함으로써 우리는 현대의 최전선 LLM이 어떻게 발전하고 있는지에 대한 더 깊은 이해를 얻을 수 있습니다. 본 개요에서는 Llama 4 및 관련 모델들을 심층적으로 분석하여 바로 이 목표를 달성할 것입니다. 이 분석을 바탕으로 LLM 연구의 주요 흐름, Llama의 미래 방향, 그리고 Llama 4 이후 Meta가 성공을 위해 어떤 변화를 더 이뤄야 할지 심도 있게 논의할 것입니다.

### Llama 4 모델 아키텍처(Model Architecture)의 혁신

먼저 Llama 4 모델의 아키텍처(model architecture)를 개괄하고, 이전 Llama 모델 세대와 비교하여 주요 변경 사항들을 부각할 것입니다. 보시다시피, 새로운 Llama 모델은 이전과는 확연히 다른 모델 아키텍처(model architecture)를 채택하며, 이는 연구 방향과 전략의 명확한 전환을 시사합니다. 이전 Llama 변형 모델들이 단순성과 접근성을 중시했던 반면, Llama 4는 성능과 효율성 증대를 위해 더 높은 복잡성과 규모를 감수하면서도 첨단 기술을 적극적으로 수용했습니다. 이는 비공개 및 공개 분야의 선두 LLM 연구 기관들과 동등한 수준에 도달하려는 Meta의 분명한 의지를 보여줍니다.

이러한 아키텍처의 변화는 단순한 개선을 넘어선 전략적 전환입니다. Meta는 Llama 4를 통해 모델의 잠재력을 최대한 끌어올리기 위한 근본적인 구조적 변화를 시도하고 있습니다. 이는 모델의 학습(training) 및 추론(inference) 효율성을 극대화하고, 더욱 복잡하고 정교한 작업을 수행할 수 있도록 설계된 것입니다. 물론, 이러한 변화는 개발 비용 증가, 모델 복잡성 심화, 그리고 더 높은 하드웨어 요구 사항과 같은 새로운 도전 과제를 동반합니다. 하지만 Meta는 이러한 투자를 통해 Llama를 차세대 LLM의 선두 주자로 자리매김하려는 장기적인 비전을 가지고 있습니다.

### 전문가 혼합(Mixture-of-Experts, MoE) 아키텍처의 도입

> "우리는 모델 개발 프로세스 확장 능력을 극대화하기 위한 설계 선택을 합니다. 예를 들어, 훈련 안정성을 극대화하기 위해 전문가 혼합(mixture-of-experts) 모델 대신 사소한 수정이 가해진 표준 밀집 트랜스포머(dense Transformer) 모델 아키텍처(architecture)를 선택합니다."
>
> — Llama 3 논문 [2]에서

일반적인 밀집형 디코더 전용 트랜스포머(dense decoder-only transformer) 구조(아래 그림 참조)와 달리, Llama 4는 Llama 계열 최초로 전문가 혼합(Mixture-of-Experts, MoE) 아키텍처(architecture)를 도입했습니다. Llama 3는 안정성과 구조적 단순성을 이유로 MoE 사용을 피했습니다. 이는 MoE 모델이 대규모일수록 학습(training) 및 추론(inference) 과정에 추가적인 복잡성을 수반하기 때문입니다. Llama 4를 통해 Meta는 MoE 아키텍처(architecture)를 성공적으로 활용하고 있는 선도적인 공개(예: DeepSeek-v3 [4]) 및 독점 모델(예: GPT-4)들과 발맞추게 되었습니다.

**디코더 전용 트랜스포머(decoder-only transformer) 아키텍처(architecture)의 진화**

간단히 말해, 밀집 모델(dense model)은 구조가 명료하고 효과적이지만, 규모를 확장하는 데 한계가 있습니다. MoE 아키텍처(architecture)를 사용함으로써 우리는 매우 큰 모델의 학습(training) (및 추론(inference)) 효율성을 획기적으로 개선하여, 훨씬 더 큰 규모의 모델 구현을 가능하게 합니다. MoE는 모델의 총 매개변수(parameter) 수를 크게 늘리면서도, 각 입력에 대해 활성화되는 매개변수(parameter) 수는 제한하여 연산 비용을 효율적으로 관리할 수 있는 장점이 있습니다.

**MoE란 무엇인가? 심층 분석**

대부분의 독자들은 MoE 사용의 기본 동기에 익숙할 것입니다. MoE는 대형 모델의 연산 효율성을 증진시키기 위해 디코더 전용 트랜스포머(decoder-only transformer) 구조를 변형한 형태입니다. MoE의 핵심 아이디어 대부분은 아래 세 논문에서 처음 제안되었으며, 우리는 여기서 이 개념들을 간략히 설명할 것입니다.

*   희소하게 게이트된 전문가 혼합 레이어(The Sparsely-Gated Mixture-of-Experts Layer)
*   스위치 트랜스포머(Switch Transformers)
*   안정적이고 전이 가능한 전문가 혼합(Stable and Transferable Mixture-of-Experts, ST-MoE)

디코더 전용 트랜스포머(decoder-only transformer)와 비교하여, MoE는 트랜스포머 블록(transformer block) 내의 피드포워드(feed-forward) 구성 요소를 수정합니다. 각 블록에 하나의 피드포워드 네트워크(feed-forward network)만 존재하는 대신, MoE는 여러 개의 피드포워드 네트워크(feed-forward network)를 가지며, 각각은 독립적인 가중치(weight) 집합을 보유합니다. 우리는 이 개별 네트워크들을 "전문가(expert)"라고 칭합니다. 아래 그림을 참조하십시오.

<img src="https://miro.medium.com/v2/resize:fit:1400/1*y3zB-4712v84j0tUf_fJ9g.png" alt="Adding experts to a transformer block (source)" />

트랜스포머 블록에 전문가 추가 (출처)

MoE 아키텍처(architecture)를 구현하기 위해, 우리는 트랜스포머(transformer)의 피드포워드 레이어(feed-forward layer)를 MoE — 즉, 전문가(expert) — 레이어(layer)로 전환합니다. MoE의 각 전문가(expert)는 해당 레이어(layer)의 원래 피드포워드 네트워크(feed-forward network)와 구조적으로 동일하며, 일반적으로 트랜스포머 레이어(transformer layer)의 일부만 MoE 레이어(layer)로 변환됩니다. 예를 들어, Llama 4는 트랜스포머(transformer)의 모든 다른 레이어(layer)가 전문가 레이어(expert layer)가 되는 인터리브된 MoE 레이어(interleaved MoE layer) 방식을 사용합니다.

> "우리의 새로운 Llama 4 모델은 MoE 아키텍처(architecture)를 사용하는 첫 번째 모델입니다... MoE 아키텍처(architecture)는 훈련(training) 및 추론(inference)에 더 계산 효율적이며, 고정된 훈련 부동 소수점 연산(FLOPs) 예산이 주어졌을 때 밀집 모델(dense model)에 비해 더 높은 품질을 제공합니다."
>
> — Llama 4 블로그 [1]에서

**라우팅 메커니즘(Routing mechanism)과 희소성**. 분명히, 트랜스포머(transformer)에서 각 피드포워드 네트워크(feed-forward network)의 여러 복사본을 만드는 것만으로는 계산 효율성이 향상되지 않습니다. 효율성 증대를 위해서는 희소성(sparsity)을 도입해야 합니다. 즉, 우리는 각 MoE 레이어(layer)에서 모든 전문가(expert)를 활성화하는 것이 아니라, 각 토큰(token)에 대해 특정 전문가(expert) 그룹(가령, 한두 개)을 선별적으로 활용합니다. 이를 "활성(active)" 전문가(expert) 또는 매개변수(parameter)라고 합니다. 이러한 선택은 각 토큰 벡터(token vector)를 선형 레이어(linear layer)를 통해 전달하여 전문가(expert) 집합에 대한 확률 분포(probability distribution)를 생성함으로써 이루어집니다. 아래를 참조하십시오.

<img src="https://miro.medium.com/v2/resize:fit:1400/1*aH-0j_L47_2-c9183q192A.png" alt="Selecting experts with a routing mechanism" />

라우팅 메커니즘(routing mechanism)으로 전문가(expert) 선택

여기서부터 우리는 가장 높은 확률을 부여받은 전문가(expert)만을 사용하여 각 토큰(token)을 처리할 수 있습니다. 이렇게 함으로써 각 토큰(token) 처리에 전체 모델 매개변수(parameter)의 일부분만 사용됩니다. 활성 매개변수(active parameter)의 수는 모델의 총 매개변수(parameter)보다 훨씬 적습니다. 이러한 이유로, 우리는 총 연산 비용의 일부만으로도 매우 많은 수의 전체 매개변수(parameter)를 가진 모델을 학습(training)시킬 수 있습니다. 라우팅 메커니즘은 단순한 확률 선택을 넘어, 토큰의 의미론적 특성을 기반으로 가장 적합한 전문가를 동적으로 연결하여 모델의 전문성을 극대화합니다. 이는 모델이 다양한 유형의 입력에 대해 최적화된 서브네트워크를 활용할 수 있게 하여, 전반적인 성능 향상에 기여합니다.

> "게이팅 네트워크(gating network)는 항상 동일한 소수의 전문가(expert)에 대해 큰 가중치(weight)를 생성하는 상태로 수렴하는 경향이 있습니다. 이 불균형은 선호되는 전문가(expert)가 더 빠르게 훈련(training)되고 따라서 게이팅 네트워크(gating network)에 의해 더 많이 선택되기 때문에 자기 강화적입니다."
>
> — 출처

**부하 분산(Load balancing) 및 훈련 안정성(training stability) 문제와 해결책**. 표준 밀집 모델(dense model)과 유사하게 MoE를 학습(training)시키면 여러 문제가 발생할 수 있습니다. 첫째, 모델은 모든 토큰(token)을 단일 전문가(expert)로 라우팅(routing)하는 것을 빠르게 학습할 수 있는데, 이는 "라우팅 붕괴(routing collapse)"로 알려진 현상입니다. 이로 인해 대부분의 전문가가 활용되지 않는 '죽은 전문가(dead experts)' 문제가 발생하여 MoE의 이점을 상실하게 됩니다. 또한, MoE는 학습(training) 중에 수치적 불안정성(numerical instabilities)을 경험할 가능성이 더 높으며, 이는 학습 손실(training loss)의 발산(divergence)으로 이어질 수 있습니다. 아래를 참조하십시오.

<img src="https://miro.medium.com/v2/resize:fit:1400/1*d-X-1t67p8636_g9p-Y00g.png" alt="An example of a training divergence (source)" />

훈련 발산(training divergence)의 예 (출처)

이러한 난관을 극복하고 학습(training)의 안정성을 확보하고자, 대다수의 MoE 모델은 학습(training) 과정에서 부하 분산 손실(load-balancing loss)을 활용합니다. 이 손실은 MoE가 전문가(expert)에게 동일한 확률을 할당하고 토큰(token)을 균일하게 라우팅(routing)하는 것에 보상을 줍니다. 부하 분산 손실(load-balancing loss)은 표준 다음 토큰 예측 손실(next-token prediction loss)에 추가 손실 항을 더하여 LLM의 기본적인 학습 목표(training objective)를 수정합니다. 아래를 참조하십시오. 따라서 이러한 보조 손실(auxiliary losses)은 모델의 성능에 영향을 미칠 수 있으며, 이로 인해 일부 인기 있는 MoE 기반 LLM(예: DeepSeek-v3)은 이를 완전히 사용하지 않기도 합니다.

<img src="https://miro.medium.com/v2/resize:fit:1400/1*R758R64Qk5_4h4-Z28_f5w.png" alt="The auxiliary-loss-free load balancing strategy used by DeepSeek-v3 [4]" />

DeepSeek-v3 [4]에서 사용된 보조 손실 없는 부하 분산 전략(auxiliary-loss-free load balancing strategy)

[1]에서는 Llama 4 모델을 학습(training)하는 데 사용된 정확한 보조 손실(auxiliary losses)에 대한 언급이 명확히 제시되어 있지 않습니다(만약 있다면). 학습 불안정성(training instability)을 회피하기 위해 우리는 DeepSeek-v3와 유사한 보조 손실 없는 부하 분산 전략(auxiliary-loss-free load-balancing strategy)을 사용하거나, 더 나은 가중치 초기화(weight initialization) 또는 선택적 정밀도(selective precision)와 같은 다양한 추가적인 기법(trick)들을 채택할 수 있습니다. 이 정보에서 우리가 얻어야 할 핵심 교훈은 MoE가 — 수많은 이점에도 불구하고 — 표준 밀집 모델(dense model)에 비해 학습(training)시키기가 훨씬 더 어렵다는 단순한 사실입니다. 이것은 단순성과 성능 사이의 고전적인 절충점입니다! 이러한 아키텍처(architecture)는 내재적으로 더 복잡합니다. 따라서 고려해야 할 요소가 더 많고 학습(training) 중에 발생할 수 있는 문제가 훨씬 더 많습니다. MoE 아키텍처(architecture) 및 학습(training)에 대한 자세한 내용은 아래 링크를 참조하십시오.

*   MoE 기반 LLM 이해하기(Understanding MoE-based LLMs)
*   nanoMoE: PyTorch에서 MoE 기반 LLM 구현하기(nanoMoE: Implementing an MoE-based LLM in PyTorch)

### Llama 4 아키텍처(architecture)의 구체적인 구성

[1]에서는 세 가지 종류의 Llama 4 모델이 제시됩니다. 각 모델은 특정 목표와 사용 시나리오에 최적화된 구성을 가지고 있습니다.

*   **Scout**: 총 매개변수(total parameters) 109B, 활성 매개변수(active parameters) 17B, 레이어(layer)당 전문가(expert) 16개. 이 모델은 추론(inference) 효율성에 중점을 두어, 제한된 리소스 환경에서도 빠르게 작동하도록 설계되었습니다.
*   **Maverick**: 총 매개변수(total parameters) 400B, 활성 매개변수(active parameters) 17B, 레이어(layer)당 전문가(expert) 128개. Scout보다 훨씬 큰 전체 매개변수를 가지면서도 활성 매개변수는 동일하게 유지하여, 더 넓은 범위의 작업을 처리할 수 있는 범용성을 목표로 합니다.
*   **Behemoth**: 총 매개변수(total parameters) 2T, 활성 매개변수(active parameters) 288B, 레이어(layer)당 전문가(expert) 128개. 이 모델은 Llama 4 제품군 중 가장 강력하며, 최첨단 연구 및 고성능 컴퓨팅 환경을 위한 파운데이션 모델(foundation model) 역할을 합니다.

Llama 4 Scout 및 Maverick 모델은 [1]에서 Llama 4 커뮤니티 라이선스 계약(community license agreement)에 따라 공개적으로 출시되었으며, Behemoth 모델은 현재 미리보기(즉, 아직 출시되지 않음) 상태로만 공개되었습니다. DeepSeek-v3와 유사하게, Llama 4 모델은 공유 전문가(shared expert)와 라우팅된 전문가(routed expert)를 모두 활용합니다. 예를 들어, Llama 4 Maverick은 하나의 공유 전문가(shared expert)를 가지는데, 이는 모든 토큰(token)이 100% 확률로 이 전문가(expert)에게 전달된다는 것을 의미합니다. 동시에 라우팅 메커니즘(routing mechanism)을 사용하여 토큰(token)당 하나의 활성 라우팅된 전문가(active routed expert)를 선택합니다. 아래를 참조하십시오.

<img src="https://miro.medium.com/v2/resize:fit:1400/1*W680t11Z_8X2eX5x-5_N0Q.png" alt="Depiction of shared and routed experts (from [3])" />

공유 전문가(shared expert) 및 라우팅된 전문가(routed expert) 묘사 (출처: [3])

다른 인기 있는 MoE 모델들과 비교할 때, Llama 4 모델은 활성 매개변수(active parameter)의 수가 상대적으로 적습니다. 그러나 이러한 아키텍처(architecture) 설정은 최고 수준의 산업 연구소들 사이에서는 드물지 않습니다.

*   Scout는 추론 효율성(inference efficiency)에 최적화되어 있으며 Gemini Flash 또는 GPT-4o-mini와 같은 소형 고효율 모델을 연상시킵니다. 이는 주로 엣지 디바이스나 실시간 응답이 중요한 애플리케이션에 적합합니다.
*   Maverick은 DeepSeek-v3와 상대적으로 유사한 아키텍처(architecture)를 가지며, 매우 많은 수의 전문가(expert)를 가진 희소 모델(sparse model)입니다. 이는 광범위한 일반적인 작업에서 강력한 성능을 제공하도록 설계되었습니다.
*   Behemoth — 스위트(suite)에서 가장 강력한 모델 —는 GPT-4와 유사하게 수조 개의 매개변수(parameter)를 가진 파운데이션 모델(foundation model)로, 복잡한 추론과 방대한 지식 처리에 특화되어 있습니다.

그러나 Llama 4 모델과 다른 인기 있는 LLM 사이에는 여전히 중요한 차이점이 존재합니다. Llama 4에서는 레이어(layer)당 하나의 라우팅된 전문가(routed expert)만 선택되는 반면, DeepSeek은 여러 공유 전문가(shared expert)와 레이어(layer)당 8개의 활성 라우팅된 전문가(active routed expert)를 가집니다(즉, 37B 활성 매개변수(active parameter) 및 671B 전체 매개변수(total parameter)). 이처럼 더 적은 수의 활성 매개변수(active parameter)는 Llama 4의 학습(training) 및 추론 효율성(inference efficiency)을 모두 향상시킵니다. 실제로 Llama 4 모델은 데이터 및 모델 규모의 극적인 증가에도 불구하고 Llama 3에 비해 학습(training) 중 더 적은 계산량을 사용했다고 보고되었습니다. 이는 Meta가 MoE 아키텍처를 통해 효율성과 성능의 균형을 성공적으로 찾아냈음을 시사합니다.

### 세분화된 전문가(Fine-grained experts)의 전략적 활용

여러 현대 MoE 기반 LLM(예: DeepSeek-v3 및 DBRX)이 채택한 인기 있는 설계 선택 중 하나는 세분화된 전문가(fine-grained experts)의 사용입니다. 세분화된 전문가(fine-grained experts)를 활용하려면 일반적으로 다음 두 가지 접근 방식을 따릅니다.

*   각 MoE 레이어(layer) 내의 전문가(expert) 수를 늘립니다.
*   각 개별 전문가(expert)의 크기(매개변수(parameter) 수)를 줄입니다.

일반적으로 우리는 세분화된 MoE 모델에서 활성 매개변수(active parameter)의 수를 (상대적으로) 고정시키기 위해 각 레이어(layer)에서 더 많은 수의 활성 전문가(active expert)를 선택합니다. Llama 4 스위트(suite)에서는 세분화된 전문가(fine-grained expert)와 거친 전문가(coarse-grained expert)가 모두 전략적으로 사용됩니다. Scout 모델은 총 16개의 전문가(expert)를 가지는 반면, Maverick은 총 128개의 전문가(expert)를 가집니다. Maverick이 Scout 모델보다 전문가(expert) 수가 16배 많지만 전체 매개변수(total parameter) 수는 4배에 불과하다는 점을 감안할 때, Maverick이 세분화된 전문가(fine-grained expert)를 사용하고 있음이 분명합니다. 대조적으로, Scout 및 Behemoth 모델은 모두 표준(거친 전문가(coarse-grained expert)) 전문가(expert)를 활용합니다. Meta가 이러한 선택을 하는 데에는 몇 가지 다른 이유가 있습니다. 일반적으로 세분화된 전문가(fine-grained expert)를 사용하면 전문가(expert) 간의 더 높은 수준의 전문화가 가능하며, 이는 성능과 효율성을 동시에 향상시킬 수 있습니다. 하지만 세분화된 전문가(fine-grained expert)는 분산 학습(distributed training) 프로세스에 추가적인 복잡성을 초래합니다.

<img src="https://miro.medium.com/v2/resize:fit:1400/1*23F1e-01-j_5113_5t_41w.png" alt="(source)" />

(출처)

전문가(expert)는 일반적으로 학습(training) 중에 여러 GPU에 분산됩니다(즉, 전문가 병렬 처리(expert parallelism)). 위 그림을 참조하십시오. 거친 전문가(coarse-grained expert)를 사용할 때, 각 GPU가 단일 전문가(expert)를 저장하는 것이 일반적입니다. 그러나 우리는 일반적으로 여러 세분화된 전문가(fine-grained expert)를 단일 GPU의 메모리에 맞출 수 있습니다. 또한, 세분화된 전문가(fine-grained expert)를 사용할 때 일반적으로 더 많은 수의 전문가(expert)를 선택하기 때문에, 각 토큰(token)이 클러스터(cluster) 내의 여러 다른 GPU로 라우팅(routing)되어야 하는 문제가 발생할 수 있으며, 이는 GPU 간 통신 비용을 극적으로 증가시킵니다. 이러한 통신 오버헤드는 모델의 학습(training) 속도를 저해하고 전체 시스템의 효율성을 떨어뜨릴 수 있습니다.

> "우리는 각 토큰(token)이 최대 𝑀개의 노드(node)로 전송되도록 보장합니다. 이 노드(node)는 각 노드(node)에 분산된 전문가(expert)의 가장 높은 𝐾 / 𝑀 친화도 점수(affinity score)의 합계에 따라 선택됩니다. 이 제약 조건 하에서 우리의 MoE 훈련 프레임워크(training framework)는 거의 완벽한 계산-통신 오버랩(computation-communication overlap)을 달성할 수 있습니다."
>
> — DeepSeek-v3 논문 [4]에서

결과적으로, 우리는 통신 비용을 제한하고 학습 효율성(training efficiency)을 향상시키기 위한 전략을 채택해야 합니다. 예를 들어, DeepSeek-v3는 위에서 설명한 노드 제한 라우팅 방식(node-limited routing scheme)을 사용하며, 이는 단일 토큰(token)이 라우팅(routing)될 수 있는 장치(device)의 수를 제한합니다. 세분화된 전문가(fine-grained expert)를 사용하지 않음으로써 이러한 추가적인 복잡성을 피할 수 있습니다. 그러나 세분화된 전문가(fine-grained expert) 모델과 거친 전문가(coarse-grained expert) 모델을 모두 학습(training)하는 것은 모델 사용자에게 더 많은 구성 가능성(configurability)과 선택권을 제공합니다. 이는 특정 애플리케이션의 요구 사항에 따라 모델을 유연하게 조정할 수 있는 장점으로 작용합니다. Meta의 이러한 하이브리드 접근 방식은 다양한 사용자 환경과 성능 목표를 동시에 만족시키려는 의도로 해석될 수 있습니다.

### 공개 LLM 생태계에 미치는 영향

MoE는 추론(inference) 중에 모든 매개변수(parameter)를 사용하지 않지만, 우리는 여전히 모델의 전체 매개변수(parameter)를 GPU 메모리에 적재해야 합니다. 결과적으로 MoE 기반 LLM은 밀집 모델(dense model)에 비해 훨씬 더 높은 메모리 점유율(memory footprint)을 가지며, 따라서 더 많고 더 나은 GPU에 대한 접근이 필요합니다. Llama 4 Scout는 "단일 H100 GPU(Int4 양자화(quantization) 사용)에 적합" 3하지만, Maverick은 "단일 H100 호스트(host)"가 필요합니다. 즉, 우리는 단일 GPU만으로는 더 큰 Maverick 모델의 추론(inference)을 수행할 수 없습니다. 여러 GPU 호스트(host)에 걸쳐 분산 추론(distributed inference)을 수행해야 합니다.

이러한 모든 고려 사항을 염두에 두면, Llama가 MoE 아키텍처(architecture)로 전환하는 것이 양날의 검이라는 것을 깨닫기 시작할 수 있습니다.

*   Llama 프로젝트는 가장 강력한 (독점) LLM과 동등한 수준으로 나아가고 더 나은 모델을 만들 잠재력을 열어줍니다. 이는 오픈소스 진영의 LLM 역량을 한 단계 끌어올릴 수 있는 기회가 됩니다.
*   그러나 동시에 Llama 모델 사용을 위한 진입 장벽이 높아집니다. 고성능 하드웨어 요구사항은 개인 연구자나 소규모 팀에게는 큰 부담으로 작용할 수 있습니다.

이 딜레마는 공개 LLM 연구에 중대한 영향을 미칩니다. 공개 LLM에 대한 진입 장벽을 높이는 것은 상당한 부작용을 초래하며, 충분한 GPU 자원이 없는 사람들이 의미 있는 연구를 수행할 능력을 저해할 것입니다. 모델이 계속 발전함에 따라 기여자들의 연구 비용이 점차 높아진다면 공개 LLM 커뮤니티는 지속적으로 번성하기 어려울 수 있습니다. 이는 AI 기술의 민주화라는 Llama의 초기 목표와 상충될 수 있습니다.

> "공개 표준이 되는 모델은 전반적으로 최고 모델일 필요는 없지만, 다양한 배포 설정에서 견고한 다양한 형태와 크기의 모델 패밀리여야 합니다... 희소 MoE(sparse MoE)와 같은 메모리 집약적인 모델은 공개 커뮤니티의 더 많은 참여자들을 배제시킵니다."
>
> — Nathan Lambert

MoE 아키텍처(architecture)의 이러한 부정적인 측면을 피하기 위해, 우리는 더 큰 MoE 모델을 더 작은 밀집 모델(dense model)로 증류(distill)하여 여전히 잘 작동하는 더 사용자 친화적인 LLM 스위트(suite)를 제공할 수 있습니다. 이 접근 방식은 DeepSeek-R1 [5] 4에 의해 채택되고 대중화되었는데, 이는 671B 매개변수(parameter)의 MoE 기반 추론 모델(reasoning model)로, 1.5B에서 70B 매개변수(parameter)에 이르는 여러 밀집 LLM으로 증류(distill)되었습니다. [5]의 주요 발견 중 하나는 매우 크고 강력한 모델이 교사(teacher)로 사용될 때 증류(distillation)가 가장 효과적이라는 사실입니다. 즉, 최고 성능의 대형 모델에서 지식을 추출하여 작고 효율적인 모델에 주입하는 것이 가능합니다. 개요의 뒷부분에서 보겠지만, Llama 4 모델로부터의 증류(distillation)는 이미 활발히 탐색되고 있으며, 이는 더 많은 개발자들이 Llama 4의 강력한 성능을 활용할 수 있도록 돕는 중요한 방향이 될 것입니다. 또한, 양자화(quantization)와 같은 기술을 통해 모델의 메모리 요구사항을 줄이는 것도 중요한 연구 분야입니다.

### 네이티브 다중 모달리티(Native Multi-Modality) 및 초기 융합(Early Fusion)의 도입

과거에도 다중 모달(multi-modal) Llama 모델이 출시된 바 있습니다. 원래 Llama 3 출판물 [2]에는 다중 모달리티(multi-modality)에 대한 예비 실험이 포함되어 있었고, 이는 나중에 Llama 3.2 Vision 출시와 함께 상용화되었습니다. 다중 모달(multi-modal) Llama 3 모델의 주요 세부 사항은 아래 링크된 개요에 설명되어 있습니다. 이전 모델 세대와 유사하게, Llama 4 모델은 이미지와 비디오 모두 시각적 입력(visual inputs)을 지원합니다. 그러나 이 섹션에서 보겠지만, Llama 4는 다중 모달리티(multi-modality)에 대해 극적으로 다른 접근 방식을 취합니다. 이는 단순히 기능을 추가하는 것을 넘어, 시각 및 텍스트 정보를 통합적으로 처리하는 방식의 근본적인 변화를 의미합니다.

<img src="https://miro.medium.com/v2/resize:fit:1400/1*YyU-X_H_L1228_H_Y_P_9_A.png" alt="Vision Large Language Models (vLLMs) Cameron R. Wolfe, Ph.D. · Mar 31 Read full story" />

**다중 모달 아키텍처(Multi-modal architectures)의 구성 요소**. 다중 모달 LLM은 두 가지 주요 구성 요소를 가집니다: LLM 백본(LLM backbone)과 비전 인코더(vision encoder). LLM 백본(LLM backbone)은 표준 디코더 전용 트랜스포머(decoder-only transformer)이며, 비전 인코더(vision encoder)는 일반적으로 이미지를 해당 임베딩(embedding) 집합으로 변환하는 CLIP 또는 ViT와 같은 모델입니다. 이러한 비전 인코더는 시각적 정보를 LLM이 이해할 수 있는 벡터 공간으로 매핑하는 역할을 합니다. 아래를 참조하십시오.

<img src="https://miro.medium.com/v2/resize:fit:1400/1*d6-s-k-Y_2_E_Q_2_H_2_Y_2_G_4_A.png" alt="Using a vision encoder to produce image embeddings" />

이미지 임베딩(image embedding) 생성을 위한 비전 인코더(vision encoder) 사용

이 두 가지 구성 요소를 고려할 때, 비전 LLM(Vision LLM, 줄여서 vLLM)은 시각 정보와 텍스트 정보를 적절하게 융합하는 방법을 학습해야 합니다. 즉, LLM은 어떻게든 i) 이미지 임베딩(image embedding)을 입력으로 받아들이고 ii) 이 임베딩(embedding)을 텍스트 생성을 위한 추가 컨텍스트(context)로 활용해야 합니다. 이 목적을 위해 사용할 수 있는 두 가지 주요 모델 아키텍처(model architecture)가 있습니다(아래 그림 참조).

*   **통합 임베딩(Unified embedding)**: 입력 레이어(input layer)에서 이미지 토큰(image token)과 텍스트 토큰(text token)을 모두 연결하여 LLM 5에 의해 처리되는 단일 입력 시퀀스(input sequence)를 형성합니다. 이 방식은 시각 및 텍스트 정보가 모델의 가장 초기 단계부터 함께 처리되도록 합니다.
*   **교차 모달리티 어텐션(Cross-modality attention)**: LLM에 텍스트 토큰(text token)만 입력으로 전달하고 추가 교차 어텐션 레이어(cross-attention layer)를 통해 시각 정보를 모델에 융합합니다. 이 경우 시각 정보는 모델의 특정 중간 계층에서 텍스트 정보와 상호작용하게 됩니다.

<img src="https://miro.medium.com/v2/resize:fit:1400/1*l_2_F_0_G_2_D_0_E_2_R_0_R_2_A.png" alt="Multi-modal architecture variants" />

다중 모달 아키텍처(multi-modal architecture) 변형

이러한 아키텍처(architecture)는 각각의 장단점을 가지고 있습니다. 예를 들어, 교차 모달리티 어텐션(cross-modality attention)은 이미지 임베딩(image embedding)을 전체 LLM 백본(LLM backbone)을 통해 전달할 필요가 없으므로 더 효율적인 경향이 있습니다. 그러나 통합 임베딩(unified embedding) 접근 방식은 정확히 동일한 이유로 더 깊은 모달리티 간 상호작용을 가능하게 하여 더 나은 성능을 낼 잠재력이 있습니다! 특히, 시각적 정보와 언어적 정보 간의 미묘한 관계를 파악하고, 복잡한 시각적 질문에 답변하거나 이미지에 대한 상세한 설명을 생성하는 등의 작업에서 통합 임베딩 방식이 유리할 수 있습니다.

**다중 모달 훈련(Multi-modal training)의 새로운 패러다임**. vLLM이 텍스트를 출력으로 생성한다는 점을 감안할 때, 우리는 여전히 다음 토큰 예측(next token prediction)을 사용하여 학습(training)합니다. 그러나 학습 목표(training objective) 외에도 이러한 유형의 모델에 대한 몇 가지 다른 학습 전략(training strategies) 선택이 있습니다.

*   **네이티브 다중 모달리티(Native multi-modality)**: 처음부터 다중 모달 데이터(multi-modal data)를 사용하여 vLLM을 처음부터 학습(training)합니다. 이 방식은 모델이 여러 모달리티를 동시에 "이해"하도록 만듭니다.
*   **구성적 다중 모달리티(Compositional multi-modality)**: 별도의 LLM 백본(LLM backbone)과 비전 인코더(vision encoder)를 학습(training)하는 것으로 시작한 다음, 추가 학습(training)을 수행하여 이들을 융합합니다. 이는 기존의 단일 모달 모델을 활용하여 다중 모달 기능을 확장하는 방식입니다.

객관적으로 말하면, 네이티브 다중 모달리티(native multi-modality)는 학습(training) 프로세스에 추가적인 복잡성(예: 모달리티(modality) 간 데이터 불균형, 정렬 문제)을 도입합니다. 그러나 이러한 함정을 피할 수 있다고 가정하면, 네이티브 다중 모달 학습(natively multi-modal training)은 엄청난 잠재력을 가집니다. 이는 모델이 노출될 수 있는 데이터의 범위와 양을 확장하기 때문입니다. 이러한 이유로 Google과 OpenAI를 비롯한 많은 최고 연구소들이 이 접근 방식을 채택했으며, 이는 Llama 4 설계의 중요한 동기 부여 요인이었을 가능성이 높습니다.

> "Llama 4 모델은 네이티브 다중 모달리티(native multimodality)로 설계되었으며, 텍스트 및 비전 토큰(vision token)을 통합 모델 백본(unified model backbone)에 원활하게 통합하기 위해 초기 융합(early fusion)을 포함합니다. 초기 융합(early fusion)은 대량의 레이블 없는 텍스트, 이미지 및 비디오 데이터를 사용하여 모델을 공동으로 사전 학습(pre-train)할 수 있게 해주므로 중요한 진전입니다."
>
> — Llama 4 블로그 [1]에서

이전 Llama 변형 모델(예: Llama 3.2 Vision)은 교차 모달리티 어텐션(cross-modality attention) 아키텍처(architecture)를 사용하고 구성적 접근 방식(compositional approach)으로 학습(training)되었습니다. 대조적으로, Llama 4 모델은 네이티브 다중 모달(natively multi-modal)이며 텍스트, 이미지 및 비디오 데이터를 사용하여 처음부터 사전 학습(pretrained)됩니다. 네이티브 다중 모달리티(native multi-modality)로의 전환은 Llama 4 모델이 Llama 3보다 2배 이상 큰 방대한 30T 토큰(token) 사전 학습 데이터셋(pretraining dataset)을 구성할 때 여러 모달리티(modality)의 데이터를 효과적으로 활용할 수 있도록 합니다. 이는 모델이 세상에 대한 훨씬 더 풍부하고 통합적인 이해를 구축할 수 있게 합니다.

**초기 융합(Early fusion)의 장점**. 위 인용문에서 언급했듯이, Llama 4는 Llama 3에서 사용된 교차 모달리티 어텐션(cross-modality attention) 아키텍처(architecture) 대신 통합 임베딩(unified embedding) 아키텍처(architecture)를 채택합니다. [1]에서는 LLM의 입력 수준(input-level)에서 이미지와 텍스트가 결합된다는 의미의 "초기 융합(early fusion)"이라는 용어가 Llama 4 모델의 아키텍처(architecture)를 설명하는 데 사용됩니다. 이는 모델이 처음부터 시각적 및 텍스트적 정보를 동등하게 처리하고 상호 관계를 학습할 수 있도록 하여, 더욱 강력하고 일관된 다중 모달 이해를 가능하게 합니다. 대안적으로, "후기 융합(late fusion)" 아키텍처(architecture)(예: 교차 모달리티 어텐션(cross-modality attention))는 LLM의 후기 레이어(layer)에서 이미지 및 텍스트 데이터를 결합합니다. 초기 융합은 특히 복잡한 시각적 추론이나 이미지-텍스트 간의 미묘한 관계를 포착해야 하는 작업에서 더 나은 성능을 보이는 경향이 있습니다. Llama 4의 이러한 설계 결정은 미래의 다중 모달 AI가 나아갈 방향을 제시하는 중요한 이정표가 될 것입니다.