# **개발자: 범용 인공지능(AGI)의 배포 계층 (OpenAI 개발자 데이 2025, 셔윈 우 및 크리스티나 황 출연)**

Author: Latent Space
URL: https://www.latent.space/p/devday-2025

============================================================

**ORIGINAL TEXT**:
Apart from the Jony Ive and Stargate stories happening offstage, the biggest takeaway from OpenAI’s 2025 Dev Day was how the company is commited to being a proper developer and app distribution platform even as it scales a leading frontier research lab, consumer chatbot, and new social network . For a developer, the number of product names can be a little overwhelming, so we drew you a little flow chart: As the only podcast to deeply cover DevDay 2023 and DevDay 2024 , we were very impressed by DevDay 2025 and consider it the best one yet, both in terms of on-the-ground execution and the confidence expressed in the launches. Even if you don’t have time to read through the full slate of releases , the 50 min keynote is a must watch: One of our most popular episodes last year was with Michelle Pokrass , then of the API team, who deeply believed that APIs are the solution for developers to do the last mile of AGI distribution with OpenAI, even if AGI isn’t fully here yet, aka “ API = AGI ”: From API to AGI: Structured Outputs, OpenAI API platform and O1 Q&A — with Michelle Pokrass & OpenAI Devrel + Strawberry team September 13, 2024 Read full story And it seems like OpenAI DevDays are the company’s commitment to that mission and a thriving developer ecosystem (for those who manage to not get steamrolled). But first, let’s get the “smaller” launches out of the way. Models, models, and other launches One very impressive thing about OpenAI is that they continue to not hold models back in the API. Every year there has been some excitement about a ChatGPT capability leading into a DevDay, and every year OpenAI has not disappointed by holding back some capability from API release: DevDay 2023 included GPT4 Vision, Dall-E 3, and Code Interpreter, Dev Day 2024 gave us the Realtime Voice API while everyone was still excited about it. Dev Day 2025 gave us every frontier OpenAI API we wanted: gpt-5 pro (model): https://platform.openai.com/docs/models/gpt-5-pro sora-2 (model): https://platform.openai.com/docs/models/sora-2 sora-2-pro (model): https://platform.openai.com/docs/models/sora-2-pro Video generation with Sora (docs): https://platform.openai.com/docs/guides/video-generation Sora 2: Prompting Guide (cookbook): https://github.com/openai/openai-cookbook/blob/16686d05abf16db88aef8815ebde5c46c9a1282a/examples/sora/sora2_prompting_guide.ipynb#L7 and then some distillations we didn’t even know we wanted: gpt-realtime-mini-2025-10-06 (model): https://platform.openai.com/docs/models/gpt-realtime-mini (70% cheaper) gpt-audio-mini-2025-10-06 (model): https://platform.openai.com/docs/models/gpt-audio-mini gpt-image-1-mini (model): https://platform.openai.com/docs/models/gpt-image-1-mini (80% cheaper) Prompting some somewhat nice comments : Apps in ChatGPT (Apps SDK) “Today we’re going to open up ChatGPT for developers to build real apps inside of ChatGPT . This will enable a new generation of apps that are interactive, adaptive, and personalized that you can chat with. To build them, we are launching a new Apps SDK , now available in preview. With the Apps SDK, you get the full stack: you can connect your data, trigger actions, and render a fully interactive UI and more. The Apps SDK is built on MCP. You have full control over your backend logic and frontend UI. We’ve published the standard so anyone can integrate the Apps SDK. When you build with the Apps SDK, your apps can reach hundreds of millions of ChatGPT users.” — Sam Altman When I first wrote Why MCP Won back in March, I didn’t expect OpenAI to adopt it so quickly, much less to now be the first to turn it into the basis of a full app store platform. I’ve been guilty before of getting too excited about ChatGPT’s attempts to be an app store , but objectively this is the best shot yet (though success is far from guaranteed). I happened to be listening to an entrepreneurship podcast telling the story of starting a $745m business the day Mark Zuckerberg launched the Facebook developer platform in 2007 , and it was an interesting reminder that while the Apps SDK itself may not be the most exciting to all developers, it is very very exciting for the kind of developer that knows how to take advantage of a new app store to get abnormal distribution. In his media briefing yesterday , Greg Brockman even said that some people were able to build sustainable businesses off of Custom GPTs — while certainly not a common experience, a more full fledged App Store certainly creates more opportunity. I also think the technical accomplishments in making this happen should be appreciated better - from the innovative React component bundling for an iframe target solution so you can provide -any- custom UI component to your ChatGPT App, to the live data flow demonstrated by the Coursera App in the keynote, where live video state is conveyed to ChatGPT so you can do arbitrary queries as you watch. AgentKit First of all, the box set at Dev Day was hilarious: But there is a LOT to unpack here and you can be easily forgiven for missing some pieces: https://news.smol.ai/issues/25-10-06-devday Latent Space pod with the AgentKit team (live at DevDay) This is where the bulk of today’s podcast lies, as we got to sit down with Christina after her big Ben Thompson approved demo and ask her and Sherwin more about the finer points of the Agent Builder and broader AgentKit vision: As a side note, we didn’t cover this in the pod but it is interesting that Guardrails was one of the most exciting open source AI Engineering projects 2 years ago , but since Nvidia joined the fray and now increasingly more and more developer platforms are introducing their own guardrails, they seem to be another 2023-era idea that was more of a feature than a product, like vector indexes for embeddings (Shreya has since gone up the stack to Snowglobe , a simulation engine for user testing AI products, check it out). Guests Sherwin Wu: Head of Engineering, OpenAI Platform https://www.linkedin.com/in/sherwinwu1/ https://x.com/sherwinwu?lang=en Christina Huang: Platform Experience, OpenAI https://x.com/christinaahuang https://www.linkedin.com/in/christinaahuang/ Thanks very much to Lindsay and Shaokyi for helping us set up this great deepdive into the new DevDay launches! Transcript Alessio [00:00:04]: Hey, everyone. Welcome to the Latent Space podcast. This is Alessio from the Recurrent Labs, and I’m joined by Swyx, editor of Latent Space. swyx [00:00:11]: Hello, hello. And we are here in the OpenAI Dev Day studio with Sherwin and Christina from the OpenAI platform team. Welcome. Thank you for having us. Yeah, it’s always good to be here. Yeah, it’s such a nice thing. We’ve covered like three of these Dev Days now, and this is like the first time it’s been like so well organized that we have our own little studio, podcast studio, in the Dev Day venue. And it’s really nice to actually get a chance to sit down with you guys. So thanks for taking the time. Sherwin [00:00:38]: Yeah, I feel like Dev Day is always a process, and we’ve only had three of them, and we try to improve it every time. And I actually, I know for a fact that I think we have this podcast studio this time because the podcast interviews and the interviews with folks like yourselves last time went really well. And so I want to lean into a little bit more. I’m glad that we were able to have this studio for you all. We were kneeling on the ground interviewing like Michelle last year. I don’t know. Alessio [00:01:00]: I just saw it post-production. I thought it was... swyx [00:01:03]: We had to have people like... We had to like cordon off the area so they wouldn’t walk in front of the cameras. Alessio [00:01:07]: Yeah, people would just come up, hey, good to... I’m like, we’re like recording. Christina [00:01:10]: I guess if you guys have been to three, like what stood out from today or what’s your favorite part? swyx [00:01:17]: I feel like the vibes are just a lot more confident. Like you are obviously doing very well. You have the numbers to show it. You know, I just... Every year in Dev Day, you report the number of developers. This year it’s 4 million. I think last year it was like three. And I have more questions. I’ve got that kind of stuff. But also like just like very interesting, very high confidence launches. And then also like I think the community is clearly much more developed. Like I think there’s just a lot more things to dive into across the API surface area of OpenAI than I think last year in my mind. Alessio [00:01:56]: I don’t know about you. Yeah, and we were at the OG Dev Day, which was the DALI hack night at OpenAI in 2022. And I think Sam spoke to like 30 people. So I think it’s just crazy to see the... Sherwin [00:02:06]: Yeah, honestly, I think it’s like it’s kind of similar to this podcast studio, which is I think we’ve had a number of Dev Days now. We honestly were like slowly figuring things out as a company over time as well. And both from a product perspective and also from a like how we want to present ourselves with Dev Day. And at this point, we’ve had a lot of feedback from people. I actually think a lot of attendees will get like an email with like a chance for feedback as well. And we actually like do read those and we act on those. And like one of the things that we did this year that I really liked were all of those. Like there was like some art installations and like the little arcade games that we did, which was, you know, came up with via like engaging with the feedback. Christina [00:02:43]: Yeah, the arcade games were so fun. I loved like the theme of all the ASCII art throughout. This is my first SF Dev Day, but I’ve been to the Singapore one. That was actually my first week there. Oh, yeah, that’s the one I spoke about. Yeah, I saw you there. That was my first week of OpenAI. So really in the deep end. swyx [00:02:57]: Put it on a plane to Singapore. Yeah. Yeah, that’s awesome. Well, so, you know, that’s congrats on everything. And like kudos to the organizing team. We should talk about some developer API stuff. Yeah. So we’re going to cover a few of the things. You’re not exactly working on Apps SDK, but I guess what should people just generically take away? What should developers take away from the Apps SDK launch? Like how do you internally view it? Sherwin [00:03:22]: So the way that I think about it is I actually view OpenAI since the very beginning as a company that is really valued. Kind of like opening up our technology and like bringing it out to the rest of the world. One thing we talk about a lot internally is. You know, our mission at OpenAI is to, one, build AGI, which we’re trying to do, and then, but two, you know, potentially, you know, just as important is to bring the benefits of that to the entire world. And one thing that we realized very early on is that we as a company, it’s very difficult for us to just bring it to every truly every corner of the world. And we really need to rely on developers, other third parties to be able to do this, which is, you know, Greg talked about the start of the API and like kind of how that was formulated. But that was part of, you know, that mentality, which is we. We need to rely on developers and we need to open up our technology to the rest of the world so that they can partake for us to really fulfill our mission. So the API obviously is a very natural way of doing that, where we just literally expose API endpoints or expose tools for people to build things. But now that we have, you know, ChatGPT with its 800 million weekly active users, I forgot the stat that we shared. I think it’s like now the fifth or sixth largest website in the world. swyx [00:04:30]: And the number one and number two most downloaded on the Apple App Store. Sherwin [00:04:35]: Oh, yeah, with Sora, yeah, yeah, but I don’t like it moves around all the time. swyx [00:04:37]: So it’s kind of hard to celebrate and, you know, just screenshot it when it’s good. Sherwin [00:04:40]: Yeah, we definitely screenshot it when it was good. But kind of going back to my main point is like we’ve always kind of engaged the developers as a way for us to bring the benefits of of AGI to the rest of the world. And so I view this as actually a natural extension of this. Candidly, we’ve actually been trying to do this, you know, a couple of times with the last dev day with GPT, two dev days ago with I’m sorry, two devs ago. With GPTs and plugins, which was, I think, not tied to a dev day. So I view this as like, again, we love to deploy things so iteratively and I view it as like just a continuation of that process and also engaging deeply with developers and helping them benefit from some of the stuff that we have, which which in this case is ChatGPT distribution. Alessio [00:05:23]: And when so apps SDK is built on the MCP protocol, when did OpenAI become MCP built? I’m sure internally you must have had, you know, the signed discussions before about doing your own protocol. When did you buy into it and how long ago was that? Christina [00:05:38]: I think it was in in March, I want to say. It’s hard for me to remember kind of like the exact. swyx [00:05:42]: March was the takeoff of MCP. Christina [00:05:44]: Yeah, yeah. So we built the agents SDK and we launched that alongside the responses API in early March. And I think as MCP was growing, that felt like a really and, you know, we’re building kind of a new agentic API that can call tools and just be much more powerful. MCP was kind of like the natural protocol that developers were already using. Christina [00:06:05]: I think like in March is when we added in MCP to agents SDK first and then soon after with kind of our other. Sherwin [00:06:11]: Yeah, I think there was like a tweet or something we did where it was like OpenAI, you know, is. Christina [00:06:15]: Yeah, there was definitely a moment. I think there was a specific moment in a specific tweet. Sherwin [00:06:19]: But what I will say, though, is like, and this is honestly the credit to the team at Anthropic that kind of created MCP is I really do think they treat it as an open protocol. Like we work very closely with, I think, like David and the folks on the like, you know, consortium. And they are. They are not, you know, really viewing it as this like thing that is specific to Anthropic. They really view it as this open protocol. There is like it is an open protocol. The way in which you make changes feels very open. We actually have a member of our team, Nick Cooper, who is sitting on kind of like that, that steering committee for MCP as well. And so I think they are really treating it as something that is easy for us and other companies and everyone else to embrace, which I think they should because they they do want it to be something that is very embraced by all. And so because of that, I think it makes it a little bit easier for us to. To embrace it. And honestly, it’s a great it’s a great protocol. Like, it’s very general. It’s already solved. Like, why would you make it? Yeah, yeah, it’s very general. There’s obviously still more to do with it, but it was very easy for us to, you know, integrate because of how how how streamlined and how simple it was. Yeah. swyx [00:07:18]: My final comment on apps SDK stuff and then we’ll move to AgentKit is, you know, like I always see like in abstractly when you sort of wireframe a website or an AI app, it used to be that the initial AI integration on the website would be you have to have an API. You have the normal website and then you have a little chat bot app and now it’s kind of like inverted where there’s ChatGPT at the at the top layer and then it’s like turn out the website embedded inside of it. And it’s kind of like that inversion that I honestly have been looking for for a little bit. And I think it’s really well done, like actually all like the integrations and the custom UI components that come up. You had like Canva on the keynote there and it looks like Canva, but like you can chat with it in all your the context of your ChatGPT. That is an experience I’ve never seen. Yeah. Sherwin [00:08:06]: And I think that that’s kind of back to the iterative like learning that we’ve had that I think was because we’ve learned a lot from plugins. So like when we launched plugins, I remember one of the feedback that we got, I don’t know if, you know, people here really remember plugins. It was like March. Oh, yeah. Twenty three. Yeah. I’m like one of the points of feedback was like, oh, you can integrate retail. We told like, you know, all these companies that you can integrate these plugins into ChatGPT, but they really didn’t have that much control over how exactly it was used. It was really just like a tool that the model could call and you were just like really bound by ChatGPT. And so I think like you can kind of see the evolution of our product with this. And like this time we realized how important it was for companies, for third party developers to really own and like steer the experience and make it feel like themselves, help them, you know, like really preserve their own brand. And so and, you know, I actually don’t think we would have gotten that learning had we not, you know, had all these other steps beforehand. Alessio [00:08:54]: Awesome. Christina, you were the star today on stage with the agent kit demo. You had eight minutes to build an agent. You had a minute to spare. And then you. Christina [00:09:03]: Yeah, I wasn’t sure, honestly, I was like, let’s do a little bit less testing and maybe we, I don’t know how much time I killed on that. I was extremely stressed when the download came in. Sherwin [00:09:14]: I was like, if a UI bug is what like takes the demo down, I’m going to be so sad. Christina [00:09:17]: I think it was a full screen, yeah, like focus thing. I heard the window wasn’t in focus or something, yeah. Alessio [00:09:23]: Maybe you want to introduce agent kit to the audience. Christina [00:09:26]: Yeah, so we launched agent kit today, full set of solutions to build, deploy and optimize agents. I think a lot of this comes from working with API customers and realizing how hard it actually is to take, to build agents and then actually take them into production. Hard to get kind of that confidence and the iterative loop and writing prompts, optimizing them, writing evals, all takes a lot of expertise. And so kind of taking those learnings and packaging them into a set of tools that makes it a lot easier and kind of intuitive to know what you need to do. And so there’s a few different building blocks. That can be used independently, but they’re kind of stronger together because you then get the whole end to end system and releasing that today for people to try out and see what they build. swyx [00:10:13]: Yeah, so I find it hard to hold all the building blocks in my head, but actually chronologically, it’s really interesting that you guys started out with the agent SDK first, and then you have agent builder, you have a connector registry, you have chat kit, and then you have the eval stuff. Am I missing any major components? Sherwin [00:10:32]: Those are the main moving parts, right? Yeah, I think that’s it. And then, I mean, we also still have the RFT fine tuning API, but we technically group it outside of the agent kit umbrella. swyx [00:10:42]: Got it, got it, got it. Yeah, so it’s weird how it develops, and it’s now become the full agent platform, right? And I think one thing that I wasn’t clear about when I was looking at the demo was, it’s very funny because what you did on stage was build a live chat app for Dev Day’s website. Christina [00:11:02]: Yeah, did you get a chance to try it out? swyx [00:11:03]: Yeah, I tried to try it out. It was awesome. And actually, I kind of wanted to ask how to deploy it. Where’s the merch? Yeah, exactly. I was like, where’d you click the merch? Anyway, and this is very close to home because I’ve done it for my conferences, and it’s a very similar process. But I think what was not obvious is how much is going to be done inside of agent builder. I see there are some actually very interesting nodes that you didn’t get to talk about on stage, like user approval. That’s a whole thing. And like transform and set state, like there’s kind of like a Turing complete machine in here. Christina [00:11:35]: Yeah. Yeah, so I mean, I think again, like this is the first time that we’re showing agent builder, and so it’s definitely the beginning of what we’re building. And human approvals, like one of those use cases that we want to go pretty deep on, I think. The node today that I showed is pretty simple, like binary approval. Approve rejects. It’s similar to kind of what you’d see for MCP tools of approving that an action can take place. Yeah. But I think what we’ve seen with much more complex workflows from our users is that it’s actually quite advanced, like human in the loop interactions. Sometimes these could be over the course of weeks, right? It’s not just kind of simple approval for tool. There’s actual decision making involved in it. And I think as we work with those customers, we definitely want to continue to go deeper onto those use cases, too. Alessio [00:12:21]: Yeah. What’s the entry point? So are developers also supposed to come here and then do the two-code export? Yeah. Or just segment the use cases? Yeah. Christina [00:12:31]: So I think the two reasons that you would come to Agent Builder are one, kind of more as a playground, right? To kind of model and iterate on your systems and write your prompts and optimize them and test them out. And then you can export it and run it in your own systems using Agents SDK, using kind of other models as well. The second would be kind of to get all of the benefits of us deploying that for you, too. So you can kind of use maybe like natural language to describe what type of agent you want to build. Model it out. Bring in subject matter experts so that you really have this canvas for iterating on it and getting feedback. You know, building data sets and kind of getting feedback from those subject matter experts as well. And then being able to deploy it all without needing to handle that on your own. And that’s a lot of the philosophy around how we’re building it with ChatKit as well, right? You can kind of take pieces of it. You can have a more advanced integration where it’s much more customized. But you also get a really natural path of going live. With really kind of easy defaults as well. Yeah. Alessio [00:13:32]: Do you see it as a two-way thing? So I build here. I go to code. Then maybe I make changes in code. And then I bring those changes back to the agent builder. Christina [00:13:41]: I think eventually that’s definitely what we want to do. So maybe you could start off in code. You could bring it in. We’ll also probably have like ability to, you know, run code in the agent builder as well. And so I think just a lot of flexibility around that. Sherwin [00:13:54]: The one thing I’d say, too, is a lot of the demos. The demos that we showed today I think were like, you know, aired on the side of Simplicity just so that the audience could kind of see it. But like if you talk to a lot of these customers, like they’re building like pretty complex. Like you got to like zoom out on that canvas quite a bit to kind of like see the full flow. And then for us, you know, we were kind of like working with a lot of customers who are doing this. And then, you know, if you turn that into like an actual agent SDK like file, it’s like pretty long. And so we saw a lot of like benefit from having the visual set up here, especially as the set up grows longer and longer. It would have been a little difficult to kind of showcase this. Yeah. You can do it in eight minutes. Right. Yeah. You can do it in eight minutes. But like even with some of the presets that we have on the site. Yeah, exactly. So one of the things. Yeah. Yeah. Christina [00:14:33]: One of the things that we launched today as well alongside just like the canvas is a set of templates that we’ve actually gathered from our engineers who are working in the field with customers directly of like the kind of common patterns that they have in our own basically like playbooks when we’re working with customers on customer support, document discovery. And so kind of publishing those as well. swyx [00:14:54]: Data enrichment. Yeah. Yeah. Planning helper, customer service, structured data Q&A, document comparison. That’s nice. Internal knowledge assistant. Yeah. Yeah. Yeah. Christina [00:15:01]: And I think like we just plan to add more to those as we can kind of build those out. swyx [00:15:06]: I always wonder if there should be, so you’re not the only agent builders, but obviously by default of being an open AI, you are a very significant one. Any interest in like a protocol or like interop between different open source implementations of this kind of pattern of agent builder? Sherwin [00:15:23]: I think we’ve thought about it. Especially around, I’d say agents SDK. I would actually say maybe even like zooming out a bit more from just this is like, yeah, we were also sitting here and kind of like observing like things being made over and over again. Even like besides like agent workflows, we’re kind of launching what the industry is trying to do with responses, like what we’ve done with responses API, like stateful APIs. And so, you know, obviously we were the first one to launch responses API, but like a couple of other people have kind of adopted, I think Grok has it in their API. I think I saw LMSYS just did something recently in walls, but not, you know, not everyone. And so, unfortunately I don’t have a great answer today of like yes or no, but we are kind of like assessing everything and trying to see like, hey, you know, there has been a lot of value with MCP, with hopefully with our commerce protocol as well. ACP, yeah, that’s the, I definitely did not forget the name. Sherwin [00:16:20]: And so like even thinking about like what we want to do with agents. Yeah. So I think that’s kind of like the same thing with the agent workflow, the portability story around that, as well as the portability, I’d say even of like responses API would be great if, you know, that could be a standard or something. And developers don’t need to, you know, like build three different stateful API integrations if they want to use different models. Yeah. Christina [00:16:38]: And I think that’s one of the, so it’s not exactly a protocol, but one of the things that we launched today with evals too is ability to use like third party models as well and kind of bring that into one place. And so I think definitely kind of see where the ecosystem is at, which is, you know, using multi-models and kind of having. swyx [00:16:55]: Third party models as in non-open air models? Yeah. Sherwin [00:16:57]: Yeah. It’ll work with evals starting today. Yeah. Okay. Got it. We have a really cool setup with open router where we’re working with them and then you can bring your open router setup. And then with that, you can actually, you know, you write your evals using our data sets tool or user data set tool to create a bunch of evals. And you’d actually be able to hit a bunch of different model providers. Yeah. You know, take your pick from wherever, even like open source ones on together and see the results in our product. swyx [00:17:24]: Yeah. That’s awesome. Speaking more about evals, right? Like I think I saw somewhere in the release docs that you basically had to expand the evals products a little bit to allow for agent evals. Maybe you can talk about like what you had to do there. Sherwin [00:17:41]: Yeah. Christina [00:17:43]: Yeah. Sherwin [00:17:43]: I was going to say, so the, I actually think agent evals is still a work in progress. Yeah. I think we’ve like made maybe 10% of the progress that we need here. For example, I think we could still do a lot more around multimodal evals. But the main progress that we made this time was kind of allowing you to take traces. So the agent SDK has like really nice traces feature where if you run, if you define things, you can have like a really long trace. Allowing you to use that in the evals product and be able to grade it in some way, shape or form over the entirety of what it’s supposed to be doing. I think this is step one. Like I think it’s good to be able to do this. But I think our roadmap from here on out is to, you know, really allow you to break down the different parts of the trace and allow you to eval and like kind of like measure each of those and optimize each of those as well. A lot of times this will involve human in the loop as well, which is why we have the human in the loop component here too. But if you kind of look at our evals product over the last year, it’s been very simple. It’s been much more geared towards this like simple prompt completion setup. But obviously, as we see people doing these longer agentic traces, like, you know, how do you even evaluate a 20 minute task correctly? And it’s like, it’s a really hard problem. We’re trying to set up our evals product and move in that way to help you not only evaluate the overall trajectory, but also individual parts of it. Yeah. swyx [00:19:01]: I mean, the magic keyword is rubrics, right? Everyone wants LM as judge rubrics. Yeah. Yeah. Sherwin [00:19:07]: Yeah. swyx [00:19:08]: Obviously where this will go. Okay, great. Yeah. The other thing I think online, I see the developer community very excited about is sort of automated browser. So, you know, we have automated prompt optimization, which is kind of evals in the loop with prompts. What’s the thinking there? Where’s things going? Christina [00:19:22]: Yeah. So, we have automated prompt optimization, but again, like, I think this is an area that we definitely want to invest more in. We, I think, did a pretty big launch of this when we launched GPT-5 actually, because we saw that it was pretty difficult as new models come out to kind of learn all the quirks about a new model. Yeah, the prompt optimization. Right. There’s like, we have a big prompting guide, right, for every model that we launch. And I think building out a system to make that a lot easier. We definitely want to tie that in like completely with evals. We should be able to kind of improve your prompts over time, improve your agents over time as well if they’re kind of made in the agent builder based on the evals that you’ve set up. And so, I think we see this as like a pretty core part of the platform of basically suggested improvements to the things that you’re building. Sherwin [00:20:05]: I actually think it’s a really cool time right now in prompt optimization. I’m sure you guys are seeing this too. It’s like not only there are a lot of products kind of like gearing around this, so like kind of what we’re thinking about. But I also think like there’s a lot of interesting research around this. Like GEPA with like the Databricks folks are actually doing really cool stuff around this. We’re obviously not doing any of the cool GEPA optimization right now in our product. But we’d love to do that soon. And also, it’s just an active research area. So like, you know, whatever Matei and the Databricks folks like might think about next, what we might, you know, think about internally as well. Whatever new prompt optimization techniques come out, I think we’d love to be able to have that in our product as well. And yeah, and it’s interesting because it’s coming at a time when people are realizing that prompt. You know, like I feel like two years ago people were like, oh, at some point prompt, like prompting is going to be dead. No. Like, you know. It’s gone up. Yeah, yeah, yeah. And if anything, it is like become more and more entrenched. And I think that, you know, there’s this interesting trend where like it’s becoming more and more important. And then there’s also interesting cool work being done to like further entrench like prompt optimization. And so that’s why I just think it’s like a very fascinating, you know, area to follow right now. And also was an area where I think a lot of us were wrong two years ago. Because if anything, it’s only gotten better. It’s only gotten more important. swyx [00:21:16]: Yeah. I would say like what Shunyu used to work at OpenAI and now is an MSL. We call this kind of like zero gradient fine tuning or zero gradient updating because you’re just tweaking the prompts. But like it is so much prompt that is actually like you end up with a different model at the end of it. Sherwin [00:21:33]: There’s a lot of like things that make it more practical to just like even from our perspective, like we have a fine tuning API. And like it is extremely difficult for us to run, you know, and serve like all of these different snapshots. Like, you know, Laura’s great MSL just, you know, sorry, Thinking Labs just published. John Schulman just had a cool blog post about this. But like, man, it is like pretty difficult for us to like manage all of these different snapshots. And so if there is a way to like hill climb and yeah, do this like zero gradient like optimization via prompts, like, yeah, I’m all for it. And I think developers should be all for it because you get all these gains without having to do any of the fancy fine tuning work. swyx [00:22:10]: Since you are part of the API team, you know, you lead the API team. And since you mentioned Thinky, I got to throw a cheeky one in there. What do you think about the Tinker API? Sherwin [00:22:21]: yeah, it’s a good one. So it’s actually funny. When it launched, I actually DMed John Schulman. And I was like, wow, we finally launched it. swyx [00:22:28]: Because you used to work with him. Sherwin [00:22:29]: Yeah. Yeah. So we, it’s actually funny. So at, yeah, so right when I joined OpenAI, like this has actually been, I think, a passion project of John’s. Like he’s been talking about doing something in this. Like in this shape for a while, which is like a truly like low level research, like fine tuning library. And so we actually talked about it quite a bit when he was at OpenAI as well. It’s actually funny. I talked to one of my friends who said that when he was at Anthropic, he also, you know, worked on this idea for a bit. swyx [00:23:01]: He’s a man on a mission. Sherwin [00:23:03]: Yeah. I mean, John’s like so great in this regard. He’s like so purely just like interested in the impact of this because it’s, one, it’s like a really cool problem. And then two, it also empowers builders. And researchers, like you saw all the researchers who like express all this love for Tinker because it is a really great, great product. And so I’m just really happy to see that they shipped it. And I think he was really happy to kind of get it out there in the world as well. Yeah. swyx [00:23:26]: This is probably, this is very much a digression. But like it’s weird, as someone passionate about API design, that it took this long to find a good fine tuning API abstraction, which is effectively all he wanted. He was like, guys, like I don’t want to worry about all the infra. Like I’m a researcher. I just want these four functions. And I think it’s kind of interesting. Yeah. Sherwin [00:23:44]: Yeah. Alessio [00:23:44]: Cool. Before the OpenAI comms team barges in the room. I know. So what feedback do you want from people like the agent builder? For example, the thing I was surprised by was the if-else blocks not being natural language and using the common expression language. I’m sure that’s something already on your roadmap. What are other things where you’re kind of like at a fork that you would love more input on? Christina [00:24:06]: I think like one of the things that we spent a lot of time discussing was this. I think like one of the things that we spent a lot of time discussing was like whether we want kind of more of like the deterministic workflows or more LLM driven workflows. And so I think like getting feedback on that. Honestly, having people model existing workflow. A lot of what we did was kind of work with our team on, especially with engineers who are working with customers, like modeling the workflows that already exist in the agent builder and like what gaps exist, like what types of nodes are really common and how can we like add those in. I think that was that would be like the most helpful feedback to get back. And then like, you know, I think that’s kind of what we did. And then as we expand kind of from just like chat based, like right now, the initial deployment for agent builders through chat kit, we plan on kind of releasing more standalone like workflow runs as well. And kind of the types of like tasks that people would like to use in that type of API. swyx [00:24:59]: So like more modalities, for example. Christina [00:25:02]: Yeah, I mean, I think like for sure, like more modalities like, you know, I think kind of voice would be is already something that a lot of people have talked to us about. Even today at Dev Day. So I think modalities for sure, but also more like the logical nodes of what can’t be expressed today. Yeah. swyx [00:25:20]: Well, you know, you’re building a language, right? You have common expression language, which I never heard of prior to this. I thought this was this Python, this is JavaScript. And then there was like a whole link in there. Was that a big decision for you guys? You know? Christina [00:25:34]: I think that was more just kind of like a way that we thought we could kind of represent a mix of like the variables and I don’t know, like conditional statements. Yeah. swyx [00:25:41]: The other thing I’ll also mention is that you let once you so there’s a trope in developer tooling where like anything that can be that can store state will eventually be used as a database, including DNS. So to be prepared for your state store to become a database. I don’t know if there’s like any limits on that because people will be using it. Sherwin [00:26:01]: It’s actually funny. I’d heard this quote before and there’s definitely some truth to it. I don’t know if our stateful APIs have become a database just quite yet. But like, who knows? Like, you know. swyx [00:26:12]: I mean, conversation. Well, you charge for it. You charge for assistance. Storage. Sherwin [00:26:16]: Yeah. The storage. Yeah. Yeah. Right. So there’s some limit on that. But like. Yeah. But it’s very cheap. It’s like I remember we priced it like. Christina [00:26:21]: I think if you wanted to kind of like dump all your data somewhere. I don’t know. This is like the most like transforming it all into this shape. It’s like useful. It’s easy. Best place for it. But yeah. Sherwin [00:26:30]: But also, please don’t do this because I think it’ll put quite a bit of strain on on Benton and our input team and what we try and do. So, yeah. Alessio [00:26:37]: How do you think about the MCP side? So you have open AI first party connectors. You have third party preferred, I guess, servers, you will call them. And then you have open ended ones. Do you see the that part of registry like functionality expanding or do you see most of it being user driven? Auth is like the biggest thing. Like if you add Gmail and Calendar and Drive, you have to like auth each of them separately. There’s not like a canonical auth. What’s the thinking there? Christina [00:27:03]: Yeah. I mean, I think definitely for the registry. That’s why we want to make it a lot easier for like companies to kind of manage what they’re like. Developers have access to managing kind of the configurations around it. And I think in terms of like first party versus third party, like we want to support both of those. We have some direct integrations and then anyone can kind of create MCP servers. I think we want to make that a lot easier to like establish kind of private links for for companies to use those internally. So I think like just really excited about that ecosystem growing. Yeah. Sherwin [00:27:35]: I think one of the coolest things observed, too, is just I actually think we as an industry are still trying to figure out the ideal shape of connectors. So, I mean, part of why I think the 1P connectors exist, too, like we end up storing quite a bit of state. It’s like a lot of work for us. But like by having a lot of state on our side, we call them sync connectors. We can actually end up doing a lot more creative stuff on our side when you’re chatting with ChatGPT and using these connectors to kind of boost the quality of how you’re using it. Right. Like if you have all the data there, you can do all this like re-ranking. You can like we can put in a vector store if you want to put it anywhere else. Whereas and so there’s some inherent tradeoffs here where like you put in a lot of work to get these like 1P connectors working. But because you have the data, you can do a lot more and get higher quality. But then but then the question is like, oh, my God, there’s like such a long tail of other things, which is where the MCP and like the third party connectors come in. But then you have the tradeoff of like you’re beholden to like the API shape of the MCP creator. It might actually work well, might not work well with with the models. And then what happens if it doesn’t work well? Then you kind of have to like, you know, you’re kind of like at the mercy. Of this and MCP. But by the way, it’s like really great because it already does some layer of standardization. But my senses are still going to be more evolving here. And I think, you know, we want to support both of them because we see value in both right now, especially working with working with developers. You want to have kind of like all options kind of on the table here. But it will be interesting to see how see how this evolves over time. Alessio [00:28:54]: Yeah. When I saw about three, four months ago when you launched the forum for like signing with chat GPT interest. I think to me, that’s kind of like the vision where I log in. And I have the MCPs tied in and then I sign in with chat GPT somewhere and I can run these workflows in that app where I’m logging in. So, yeah, I think Sam, you know, said in an interview that is chat GPT is like your personal assistant. So I think this is like a great step in that direction. Yeah. I think there’s a lot more to go in that in that direction. swyx [00:29:22]: But so far, no plan on like chat GPT or opening as IDP. Right. Which is a different role in the in the off ecosystem. Yeah. Sherwin [00:29:31]: It’s interesting because. So direct answer is like no plans right now, of course. But I actually think we currently have some version of this, which is our partnership with Apple. Because with Apple, you can actually sign in to your chat GPT account. And some of that identity does carry with you into your iOS experience with Siri. Right. Like if you if you I don’t know if you’ve actually used this, the Siri integration. I actually use it quite a bit. But if you sign into your chat GPT account, the Siri integration will actually use your subscription status to decide. What type of model to use when it when it passes things over to chat GPT. And so if you’re, you know, just a free user, you get, you know, the free model. But if you’re a plus or pro subscriber, you get routed to GPT-5, which is I think what they. Christina [00:30:17]: I think we also recently announced the partnership with Kakao. Sherwin [00:30:19]: Oh, yeah. Kakao is another one. Yeah. Christina [00:30:20]: Where I think it’s a similar thing where you can sign in with chat GPT. Kakao is one of the largest like messenger apps in Korea and kind of interact with Kakao directly there. Yeah. Sherwin [00:30:31]: I mean, Sam’s been talking about it for a while. It’s a very compelling vision. We obviously want to be very thoughtful with how we do it. swyx [00:30:36]: I mean, you know, now you have a social network, you have a developer platform. My you know, my chat GPT account is very, very valuable. Yeah. Yeah, exactly. OK. So and then on the other side of the office, something I was really interested to look at and I couldn’t get a straight answer. Is there some form of bring your own key for AgentKit? Like when I when I expose it to the wider world, obviously, like I mean, by default, I’m paying for all the inference. But it’d be nice for that to have a limit. And then if you want more, you can bring your own key. Christina [00:31:07]: Yeah. I mean, we don’t have something like that yet. But I think, yeah, it’s definitely an interesting area, too. Yeah. Sherwin [00:31:14]: It doesn’t do it out of the box today. But, you know, developers have been asking about it for forever. Like it’s a really cool concept because then as a developer, you especially any developer, you don’t need to bear the burden of inference. Yeah. swyx [00:31:26]: I think like when you get into the business of like agent builders that are publicly exposed, where you have like an allowance. Yeah. You know, there’s a list of domains like this is this is the it rhymes with this exact pattern of like someone has to bear the cost. And sometimes you want to mess around with like the different levels of responsibility. Yeah. Sherwin [00:31:43]: I will say in general, like if you kind of look at our roadmap, we engage a lot with developers. We kind of hear what are the pain points and we try and build things that address it. And, you know, ideally, we’re prioritizing in a way that’s that’s helpful. But, yeah, we’ve definitely heard from a good number of developers that like the cost is we’re like all of the like copy paste your key solutions. Right now, which are like huge security issues like hazards because developers don’t want to bear the burden of of inference. You know, hopefully we make the cost cheaper. Christina [00:32:08]: So it’s the models keep getting cheaper. Yeah. Hopefully that helps. Sherwin [00:32:12]: But but what we realize is as we make it cheaper, you know, the demand for that goes up even more and you end up still spending quite a bit. But yeah, so we definitely heard this from a lot of developers and it’s definitely something top of mind. Yeah. Alessio [00:32:23]: Do you see this as mostly like an internal tools platform, though? Like to me, like you’ve been doing a big push on like the more forward deployed engineering thing. So it’s almost like, hey, we needed to build this for ourselves as we sell into these enterprises might as well open it up to everybody. What drives drive building these tools? Like you think of people building tools to then expose or mostly on the internal side? Yeah. Christina [00:32:45]: I mean, it’s so like I think, again, our first deployment is ChatKit, which is kind of one of it’s intended to be for external users. But I think one of the things that we also did see a lot as we were working with customers is that a lot of companies have actually built some version of an agent builder internally. Right. To kind of manage prompts internally, to manage templates that they’re sharing across, you know, the different developers that they have, maybe the different product areas. And we were seeing that kind of like over and over again as well and really wanted to build a platform so that this is not, you know, an area that every company needs to invest in and like rebuild from scratch. But that they can kind of have a place where they can manage these templates, manage these prompts and really focus on the parts of agent building that is more unique to their business. Sherwin [00:33:29]: It is interesting, too. Like from a deployment perspective, it is like it has spanned both internal and external use cases, right? Like kind of like these internal platforms, people will use it for like data processing or something, which is an internal use case. But if you saw some of the demos today, like there have been a huge number of companies that are trying to do this for external facing use cases as well. Customer service is one template in here. Customer service, the like ramp use case. Christina [00:33:52]: We use this internally and externally. Like our customer support, help.openair.com is already powered on AgentKit and then various other like internal use cases. Sherwin [00:34:00]: And one of the things that I actually think the team has done a really great job of, so like Tyler, David and Jiwon on the team, they built the, especially the ChatKit components, they built it to be like very consumer grade and like very polished. Like you kind of look at that, there is like a whole grid of like the different widgets and things that you could create there. Like ideally people see it and they see it as like these very polished like consumer grade ready external facing things versus like, you know, you think of internal tools and like the UI is always like the last thing that people care about. Yeah. So like you really, you know, push the team. And I think they did a really great job of making the ChatKit experience like really, really consumer grade. And it should feel almost like ChatGPT and with like really buttery smooth animations and like really responsive designs and all of that. Yeah. Christina [00:34:43]: I think your point on widgets is like definitely like really resonates, right? Because ChatKit, it handles the chat UX, but we’re also just building like really visual ways for you to represent like every action that you want to take. And that is definitely like very high polished. Sherwin [00:34:59]: Yeah. And when working with customers, like those have been the most helpful customers for us to work with because, you know, when Ramp is thinking about, you know, how, what they want to publicly present to people, like they have a pretty high bar as they should, as well as, you know, all the other customers that have been iterating on it. And so that kind of feedback from our customers has really helped us up level the general product quality of the launch that we had today as well. swyx [00:35:19]: Yeah. Would you ever, would you open source ChatKit? Sherwin [00:35:25]: Talked about it. We’ve talked about it. There are a bunch of trade offs. Yeah. Christina [00:35:30]: I think so. So ChatKit itself is like an embeddable iframe. And so I think the actual. Oh, it’s an iframe. Yeah. And so that helps us keep it like evergreen. Right. So if you are using ChatKit and we come up with new, I don’t know, a new model that reasons in a different way, right, or a kind of new modalities that you don’t actually need to rebuild and like pull in new components to use it in the front end. I think there’s parts of, you know, widgets, for example, that is much more like a language and can definitely is something that is easier to explore that for, as well as kind of the design system that we’ve built for ChatKit. But I think like as part of, yeah, the actual iframe itself, I think there’s a lot of value in that being, yeah, more evergreen. Yeah. More evergreen experience that is pretty opinionated. swyx [00:36:13]: Like there’d be no point in being open source. Right. Yeah. You want the. Christina [00:36:16]: Then you don’t get the benefits of it. swyx [00:36:18]: You know, being Stripe alums, like Stripe Checkout, like it’s, it’s all optimized for you to like. Christina [00:36:23]: So I’m not a Stripe alum, but Kristina is. Right. And the team actually is the team that built. Sherwin [00:36:28]: Stripe Checkout. Christina [00:36:29]: Yeah. So it’s very similar philosophically. Right. So Stripe, you know, can build elements and checkout and not every business needs to rebuild, right, the pieces that are really common. And I think we see the same with chat. We see chat being built over and over again. Especially when you’re building a website. Especially as we kind of come up with new, you know, modalities, like reasoning, everything. It’s not really something that is easy to keep up to date. And so we should just do that and leave kind of the hard parts of building agents again to. Alessio [00:37:02]: Yeah. To the developers. Does it feel, I mean, I know WordPress is like a bad connotation in a lot of circles, but to me it almost feels like the WordPress equivalent of like chat is like, hey, this is like drop in thing. And then you have all these different widgets. Do you see the widget becoming a big kind of like developer ecosystem where people share a widget? Is that kind of like a first party thing? And then what’s like the M4 versus widget forest? No, exactly. I mean, it’s kind of like it seems great for people that are like in between being technical and like not really being technical enough. Yeah. Yeah. Christina [00:37:35]: Yeah. I mean, I think that’s a big part of building widgets, right? Like it’s already kind of in the language that is very consumer friendly. You can use in our widget builder already. You can kind of use AI to create those widgets and they look pretty good. I don’t know if you guys have gotten a chance to try that out yet, but definitely see kind of, I don’t know, a forest. Sherwin [00:37:53]: If you haven’t tried out the widget studio and the demo like apps as well. Yeah. swyx [00:37:59]: You got a custom domain like widget.studio. It’s cool. Sherwin [00:38:02]: I actually don’t know how we got that. Yeah. Christina [00:38:04]: Everything’s in chatkit.studio. And then we have like the playground there so you can try out what chatkit would look like with all the customizations. We have chatkit.world, which is a fun site we built. I was like spinning the globe for a while this morning. It was like a fidget spinner. swyx [00:38:18]: I think Kasia also like uploaded some of her solar system stuff and all the demos as well. Yeah. Christina [00:38:24]: And then that’s where like the widget builder. Yeah. swyx [00:38:26]: So it’s really come together. It’s taken like almost more than a year to like come together and like build all this stuff, but it’s coming together. It’s like really interesting. Sherwin [00:38:34]: Yeah. Yeah. It’s something that we like- You definitely planned all of this upfront. Oh yeah. Yeah. We have the master plan from three years ago. No. But like I think, especially on this stuff, I think there was like an arc of a general like, you know, platform that we did want to kind of build around. And it takes a while to build these things. Obviously, Codex helps speed it up quite a bit now. But yeah, I will say it does seem great to kind of like start to have all the pieces start fitting together. Yeah. I mean, you saw we launched evals and we had the fine tuning API for a while and we laid all the groundwork for some of the stuff over the last year. And we’re hoping that we can eventually, you know, make it into this full feature platform that’s helpful for people. swyx [00:39:11]: I think you have. Since you did the Codex mention, maybe a quick tip from each of you on Codex power user tools or tips. Sherwin [00:39:23]: So there’s actually a funny one that one of the new grads has, I think, like taught our team in general. And I think this is like a point for like just how like new grads and younger, you know, generation people are actually more AI native. So one of them is to like really lean into like- Yeah. Like push yourself to like trust the model to do more and more. So like I feel like the way that I was using Codex. And so for me, it’s usually for my personal projects. They don’t let me touch the code anymore. But you give it like small tasks. So you’re like not really trusting it. Like I view it as like this like intern that like I really don’t trust. But what a lot of the like, so we had an intern class this year. What a lot of the interns would do is just like full YOLO mode, like trust it to like write the whole feature. And it like it doesn’t work. It doesn’t work for worse. It like doesn’t work sometimes. But like, I don’t know, like 30, 40% of the time, it’s just like one-shots it. I actually haven’t tried this with like Codex, GBD-5 Codex. I bet it probably like one-shots it even more. But one tip that I’m like starting to like, I feel like undo this, like relearn things here is to like really lean into like the AGI component of it and just like really let the model rip and like kind of trust it. Because a lot of times they can actually do stuff that surprises me. And then I have to like readjust my priors. Whereas before I feel like I was in this like safe space of like I’m just treating this, I’m giving this thing like a tiny bit of rope. Yeah. And because of that, I was kind of limiting myself with how effective I could be. swyx [00:40:44]: Like, sure, but okay. But also, is there an etiquette around submitting effectively, you know, vibe-coded PRs that someone else now has to review, right? And it’s like, it can be offensive. Sherwin [00:40:55]: Well, we have Codex who reviews now. swyx [00:40:56]: Okay. It actually reviews itself. Does Codex approve its own PRs a lot more than humans? It doesn’t get approved then. Christina [00:41:02]: I was going to say, I think like the Codex PR reviews are actually one of like the things that my team like very much relies on. I think they’re very, very high quality reviews. Yeah. On the Codex PR side, like for the visual agents builder, we only started that probably less than two months ago. And that wouldn’t be possible without Codex. So I think there’s definitely a lot of use of Codex internally and it keeps getting better and better. And so, yeah, I think people are just finding they can rely on it more and more. And it’s not, you know, totally vibe-coded. It’s still, you know, checked and edited, but definitely has a kicking off point. And I think I’ve heard of people on my team, it’s like on their way to work, they’re like kicking off like five Codex tasks because the bus takes 30 minutes, right? And you get to the office and it kind of helps you orient yourself for the day. You’re like, okay, now I know the files. I have the rough sense. Like maybe I don’t even take that PR and I actually just like still code it. But it helps you just context switch so much faster too and be able to like orient yourself in a code base. Sherwin [00:42:01]: There are so many meetings nowadays where I have like one-on-ones with engineers and I walk into the room, they’re like, wait, wait, wait, give me a second. I got to kick off my like Codex thing. I’m like, oh, sorry. We’re about to enter async zone. Christina [00:42:10]: It’s almost like your notes, right? You’re like, let me. Sherwin [00:42:12]: And they’re like typing like, okay, now we can start our one-on-one because now it’s great. Yeah. swyx [00:42:16]: Cool. We’re almost out of time. I wanted to leave a little bit of time for you to shout out the Service Health Dashboard because I know you’re passionate about it. Well, tell people what it is and why it matters. Yeah. Sherwin [00:42:27]: So this is a launch that we actually didn’t, you know, it didn’t get any stage time today, but it’s actually something I’m really excited about. So we launched this thing called the Service Health Dashboard. You can now go into your usage or like your settings account and kind of see the health of your integration with our OpenAI API. And so this is scoped to your own org. So basically if you have an integration that’s running with us doing a bunch of, you know, tokens per minute or a bunch of queries, it’s now tracking each of those responses, looking at your token velocity, TPM that you’re getting, the throughput, as well as the responses, the response codes. And so you can see kind of like a real-time personal SLO for your integration. The reason why I care a lot about this is obviously over the last year, we’ve spent a lot of time thinking about reliability. We had that really bad outage last December, you know, longest, like three, four hours of my life, and then had to, you know, talk to a bunch of customers. We haven’t had one that bad since, you know, knock on wood. We’ve done a bunch of work. We have an Infer team led by Venkat, and they’ve been working with Jana on our team, and they’ve just been doing so much good work to get reliability better. And so we actually... Yeah. Again, knock on wood. We think we’ve got reliability in a spot where we’re, like, comfortable kind of putting this out there and kind of, like, letting people actually see their SLO. And hopefully, you know, it’s, you know, three, four, soon to be five nines. But the reason why I cared a lot about it is because we spent so much time on it, and we feel confident enough to kind of have it behind a product now. Five nines is like two minutes of outage or something. Yeah, yeah. We’re working to get to five nines. What? What does an extra nine take? Sherwin [00:44:08]: It’s exponentially more work. So, you know, and then... But, like, we always... We were, you know, in the last couple weeks, we were talking about, like, hitting three nines and then hitting three and a half nines and then hitting four nines. But, yeah, it’s exponentially more work. I could go for a while on the different topics, but... swyx [00:44:25]: We’ll have to do that in a follow-up. I mean, that’s all... That’s the engineering side, right? Yes, yes, yes. Like, you’re serving six billion tokens per minute. Sherwin [00:44:32]: We actually zoomed past that. Yeah, that’s the... That’s outdated. Yeah. But, yeah, it’s been crazy, though, the growth that we’ve seen. Alessio [00:44:39]: Awesome. I know we’re out of time. It’s been a long day for both of you, so we’ll let you go, but thank you both for joining us.

**Korean Translation**:
무대 밖에서 벌어진 조니 아이브(Jony Ive)와 스타게이트(Stargate) 이야기 외에도, OpenAI의 2025년 데브데이(Dev Day)에서 가장 큰 수확은 회사가 선도적인 프론티어 연구소(frontier research lab), 소비자 챗봇(consumer chatbot), 그리고 새로운 소셜 네트워크(social network)를 확장하면서도 제대로 된 개발자 및 앱 배포 플랫폼이 되기 위해 노력하고 있다는 점이었습니다. 개발자에게는 너무 많은 제품 이름이 부담스러울 수 있으므로, 작은 플로우차트(flow chart)를 그려보았습니다. 2023년 데브데이와 2024년 데브데이를 심층적으로 다룬 유일한 팟캐스트(podcast)로서, 우리는 2025년 데브데이에 깊은 인상을 받았으며, 현장 실행력과 출시 제품에 대한 자신감 면에서 역대 최고라고 생각합니다. 모든 출시 내용을 읽을 시간이 없더라도, 50분짜리 기조연설(keynote)은 꼭 봐야 합니다. 작년 가장 인기 있었던 에피소드(episode) 중 하나는 당시 API 팀의 미셸 포크라스(Michelle Pokrass)와의 인터뷰였습니다. 그녀는 아직 AGI(인공 일반 지능)가 완전히 구현되지 않았더라도, 개발자들이 OpenAI와 함께 AGI 배포의 마지막 단계를 수행할 수 있는 해결책이 API(응용 프로그래밍 인터페이스)라고 굳게 믿었습니다. 즉, "API = AGI"입니다. API에서 AGI까지: 구조화된 출력(Structured Outputs), OpenAI API 플랫폼(platform) 및 O1 Q&A — 미셸 포크라스(Michelle Pokrass) & OpenAI 개발자 관계(Devrel) + 스트로베리(Strawberry) 팀과 함께 2024년 9월 13일 전체 이야기 읽기 그리고 OpenAI 데브데이는 이러한 사명과 번성하는 개발자 생태계(압도당하지 않는 사람들을 위한)에 대한 회사의 약속인 것 같습니다. 하지만 먼저, "더 작은" 출시 제품들부터 살펴보겠습니다. 모델, 모델, 그리고 기타 출시 제품 OpenAI에 대해 매우 인상적인 점 중 하나는 API에서 모델을 계속해서 제한하지 않는다는 것입니다. 매년 데브데이로 이어지는 ChatGPT 기능에 대한 기대가 있었고, 매년 OpenAI는 API 출시에서 어떤 기능도 숨기지 않아 실망시키지 않았습니다. 2023년 데브데이에는 GPT4 비전(Vision), Dall-E 3, 코드 인터프리터(Code Interpreter)가 포함되었고, 2024년 데브데이는 모두가 여전히 흥분해 있을 때 실시간 음성 API(Realtime Voice API)를 제공했습니다. 2025년 데브데이는 우리가 원했던 모든 프론티어(frontier) OpenAI API를 제공했습니다. gpt-5 pro (model): https://platform.openai.com/docs/models/gpt-5-pro sora-2 (model): https://platform.openai.com/docs/models/sora-2 sora-2-pro (model): https://platform.openai.com/docs/models/sora-2-pro Video generation with Sora (docs): https://platform.openai.com/docs/guides/video-generation Sora 2: Prompting Guide (cookbook): https://github.com/openai/openai-cookbook/blob/16686d05abf16db88aef8815ebde5c46c9a1282a/examples/sora/sora2_prompting_guide.ipynb#L7 그리고 우리가 원했는지조차 몰랐던 몇 가지 증류(distillation) 모델들: gpt-realtime-mini-2025-10-06 (model): https://platform.openai.com/docs/models/gpt-realtime-mini (70% cheaper) gpt-audio-mini-2025-10-06 (model): https://platform.openai.com/docs/models/gpt-audio-mini gpt-image-1-mini (model): https://platform.openai.com/docs/models/gpt-image-1-mini (80% cheaper) 몇 가지 괜찮은 코멘트(comment)를 유도하며: ChatGPT 내 앱(Apps SDK) "오늘 우리는 개발자들이 ChatGPT 내에서 실제 앱을 구축할 수 있도록 ChatGPT를 개방할 것입니다. 이를 통해 상호작용적이고(interactive), 적응적이며(adaptive), 개인화된(personalized) 새로운 세대의 앱을 채팅으로 이용할 수 있게 될 것입니다. 이를 구축하기 위해, 현재 미리 보기(preview)로 제공되는 새로운 앱 SDK(Apps SDK)를 출시합니다. 앱 SDK를 사용하면 전체 스택(full stack)을 얻을 수 있습니다. 데이터를 연결하고, 액션(action)을 트리거하며, 완전한 대화형 UI(User Interface) 등을 렌더링할 수 있습니다. 앱 SDK는 MCP(Multi-modal Communication Protocol)를 기반으로 구축됩니다. 백엔드 로직(backend logic)과 프론트엔드 UI(frontend UI)를 완벽하게 제어할 수 있습니다. 우리는 표준을 공개했으므로 누구나 앱 SDK를 통합할 수 있습니다. 앱 SDK로 구축하면 앱이 수억 명의 ChatGPT 사용자에게 도달할 수 있습니다." — 샘 알트만(Sam Altman) 지난 3월에 "왜 MCP가 승리했는가"를 처음 썼을 때, OpenAI가 이렇게 빨리 이를 채택할 것이라고는 예상하지 못했습니다. 하물며 이제는 이를 완전한 앱 스토어(app store) 플랫폼의 기반으로 삼는 첫 번째 주자가 될 것이라고는 더더욱 생각지 못했습니다. 저는 이전에 ChatGPT가 앱 스토어가 되려는 시도에 너무 흥분했던 적이 있지만, 객관적으로 볼 때 이번이 최고의 기회입니다(성공이 보장된 것은 아니지만). 마침 마크 저커버그(Mark Zuckerberg)가 2007년 페이스북(Facebook) 개발자 플랫폼을 출시한 날 7억 4,500만 달러 규모의 사업을 시작한 이야기를 다루는 기업가정신 팟캐스트를 듣고 있었는데, 앱 SDK 자체는 모든 개발자에게 가장 흥미로운 것은 아닐지라도, 새로운 앱 스토어를 활용하여 비정상적인 배포를 얻는 방법을 아는 개발자에게는 매우 매우 흥미로운 일이라는 점을 상기시켜 주었습니다. 어제 미디어 브리핑(media briefing)에서 그렉 브록만(Greg Brockman)은 일부 사람들이 커스텀 GPT(Custom GPT)를 통해 지속 가능한 사업을 구축할 수 있었다고 말했습니다. 이는 흔한 경험은 아니지만, 더 완벽한 앱 스토어는 분명히 더 많은 기회를 창출합니다. 또한, 이를 가능하게 한 기술적 성과가 더 잘 평가되어야 한다고 생각합니다. ChatGPT 앱에 *어떤* 커스텀 UI 컴포넌트(custom UI component)든 제공할 수 있도록 하는 iframe 타겟(target) 솔루션(solution)을 위한 혁신적인 리액트 컴포넌트 번들링(React component bundling)부터, 기조연설에서 코세라(Coursera) 앱이 시연한 라이브 데이터 플로우(live data flow)에 이르기까지, 라이브 비디오(live video) 상태가 ChatGPT로 전달되어 시청하면서 임의의 쿼리(query)를 수행할 수 있습니다. 에이전트킷(AgentKit) 우선, 데브데이의 박스 세트(box set)는 정말 재미있었습니다. 하지만 여기에는 풀어야 할 이야기가 많고, 몇 가지를 놓쳤더라도 쉽게 용서받을 수 있습니다: https://news.smol.ai/issues/25-10-06-devday 에이전트킷(AgentKit) 팀과 함께한 레이턴트 스페이스(Latent Space) 팟캐스트(데브데이 라이브) 오늘 팟캐스트의 대부분은 크리스티나(Christina)의 벤 톰슨(Ben Thompson)이 승인한 대규모 데모(demo) 이후 그녀와 셔윈(Sherwin)에게 에이전트 빌더(Agent Builder)의 세부 사항과 더 넓은 에이전트킷(AgentKit) 비전(vision)에 대해 더 자세히 물어볼 수 있었던 자리입니다. 여담으로, 팟캐스트에서는 다루지 않았지만, 2년 전 가드레일(Guardrails)이 가장 흥미로운 오픈 소스(open source) AI 엔지니어링(Engineering) 프로젝트 중 하나였다는 점은 흥미롭습니다. 하지만 엔비디아(Nvidia)가 경쟁에 뛰어들고 이제 점점 더 많은 개발자 플랫폼이 자체 가드레일을 도입하면서, 가드레일은 임베딩(embedding)을 위한 벡터 인덱스(vector index)처럼 제품이라기보다는 기능에 가까웠던 2023년 시대의 또 다른 아이디어인 것 같습니다 (슈레야(Shreya)는 이후 스노우글로브(Snowglobe)라는 AI 제품 사용자 테스트(user testing)를 위한 시뮬레이션 엔진(simulation engine)으로 스택(stack)을 옮겼으니, 확인해 보세요). 게스트(Guests) 셔윈 우(Sherwin Wu): OpenAI 플랫폼 엔지니어링 책임자(Head of Engineering) https://www.linkedin.com/in/sherwinwu1/ https://x.com/sherwinwu?lang=en 크리스티나 황(Christina Huang): OpenAI 플랫폼 경험(Platform Experience) https://x.com/christinaahuang https://www.linkedin.com/in/christinaahuang/ 새로운 데브데이 출시 제품에 대한 이 훌륭한 심층 분석(deepdive)을 준비하는 데 도움을 주신 린제이(Lindsay)와 샤오키(Shaokyi)에게 진심으로 감사드립니다! 대본(Transcript) 알레시오(Alessio) [00:00:04]: 안녕하세요, 여러분. 레이턴트 스페이스(Latent Space) 팟캐스트에 오신 것을 환영합니다. 리커런트 랩스(Recurrent Labs)의 알레시오입니다. 레이턴트 스페이스의 에디터(editor)인 스윅스(Swyx)와 함께합니다. 스윅스(Swyx) [00:00:11]: 안녕하세요, 안녕하세요. 저희는 OpenAI 데브데이 스튜디오(studio)에 OpenAI 플랫폼 팀의 셔윈(Sherwin)과 크리스티나(Christina)와 함께 있습니다. 환영합니다. 초대해 주셔서 감사합니다. 네, 여기 오게 되어 항상 좋습니다. 네, 정말 좋은 일입니다. 저희가 벌써 세 번의 데브데이를 다뤘는데, 이번이 데브데이 행사장에 저희만의 작은 스튜디오, 팟캐스트 스튜디오가 있을 정도로 잘 조직된 것은 처음입니다. 여러분과 함께 앉아서 이야기할 기회를 얻게 되어 정말 좋습니다. 시간을 내주셔서 감사합니다. 셔윈(Sherwin) [00:00:38]: 네, 데브데이는 항상 과정이라고 생각하고, 저희는 세 번밖에 열리지 않았지만 매번 개선하려고 노력합니다. 그리고 사실, 지난번 팟캐스트 인터뷰(interview)와 여러분 같은 분들과의 인터뷰가 정말 잘 진행되었기 때문에 이번에 이 팟캐스트 스튜디오를 갖게 되었다는 것을 알고 있습니다. 그래서 좀 더 집중하고 싶습니다. 여러분을 위해 이 스튜디오를 마련할 수 있어서 기쁩니다. 작년에는 미셸(Michelle) 같은 분들을 인터뷰하면서 바닥에 무릎을 꿇고 있었어요. 모르겠네요. 알레시오(Alessio) [00:01:00]: 저는 후반 작업(post-production)에서 봤을 뿐입니다. 저는 그게... 스윅스(Swyx) [00:01:03]: 저희는 사람들이... 카메라(camera) 앞으로 걸어오지 못하도록 그 구역을 통제해야 했습니다. 알레시오(Alessio) [00:01:07]: 네, 사람들이 그냥 와서, "어이, 반가워..." 저는 "저희 녹음 중이에요"라고 말했죠. 크리스티나(Christina) [00:01:10]: 여러분이 세 번이나 오셨다면, 오늘 무엇이 가장 인상 깊었는지, 또는 가장 좋아하는 부분은 무엇인가요? 스윅스(Swyx) [00:01:17]: 분위기가 훨씬 더 자신감 넘치는 것 같습니다. 분명히 아주 잘하고 계시네요. 그걸 보여주는 수치도 있고요. 아시다시피... 매년 데브데이에서 개발자 수를 보고하시죠. 올해는 400만 명입니다. 작년에는 300만 명 정도였던 것 같아요. 그리고 저는 더 많은 질문이 있습니다. 그런 것들이요. 하지만 또한 매우 흥미롭고, 매우 자신감 있는 출시 제품들도 있습니다. 그리고 커뮤니티(community)도 훨씬 더 발전한 것 같습니다. 제 생각에는 작년보다 OpenAI의 API 서비스 영역(surface area)에서 더 많은 것을 탐구할 수 있는 것 같습니다. 알레시오(Alessio) [00:01:56]: 당신은 모르겠지만요. 네, 그리고 저희는 2022년 OpenAI에서 열린 DALI 해커톤(hack night)이었던 OG 데브데이에 있었습니다. 그리고 샘(Sam)은 30명 정도에게 연설했던 것 같아요. 그래서...를 보는 것이 정말 놀랍습니다. 셔윈(Sherwin) [00:02:06]: 네, 솔직히 말해서, 이 팟캐스트 스튜디오와 비슷하다고 생각합니다. 저희는 이제 여러 번의 데브데이를 개최했습니다. 저희는 솔직히 시간이 지나면서 회사로서도 천천히 여러 가지를 알아냈습니다. 제품 관점에서도, 그리고 데브데이를 통해 저희 자신을 어떻게 보여줄지에 대해서도요. 그리고 이 시점에서, 저희는 많은 사람들로부터 피드백(feedback)을 받았습니다. 사실 많은 참석자들이 피드백을 위한 기회가 있는 이메일(email)을 받을 것이라고 생각합니다. 그리고 저희는 실제로 그 피드백을 읽고 그에 따라 행동합니다. 그리고 올해 저희가 정말 마음에 들었던 것 중 하나는 그 모든 것들이었습니다. 예를 들어, 예술 설치물(art installation)과 저희가 했던 작은 아케이드 게임(arcade game) 같은 것들이 있었는데, 그것들은 피드백을 통해 고안된 것들이었습니다. 크리스티나(Christina) [00:02:43]: 네, 아케이드 게임은 정말 재미있었습니다. 저는 전체적으로 아스키 아트(ASCII art) 테마(theme)가 좋았습니다. 이번이 저의 첫 SF 데브데이이지만, 싱가포르(Singapore) 데브데이에는 가봤습니다. 그게 사실 제가 그곳에 있었던 첫 주였습니다. 아, 네, 제가 이야기했던 그곳이요. 네, 거기서 뵙습니다. 그게 OpenAI에서의 저의 첫 주였습니다. 그래서 정말 깊은 곳에 있었죠. 스윅스(Swyx) [00:02:57]: 싱가포르행 비행기에 태웠죠. 네. 네, 정말 멋집니다. 음, 모든 것에 축하드립니다. 그리고 조직 팀에 찬사를 보냅니다. 개발자 API 관련 이야기를 좀 해야겠네요. 네. 그래서 몇 가지를 다룰 예정입니다. 당신은 정확히 앱 SDK를 작업하고 있지는 않지만, 사람들이 일반적으로 무엇을 얻어가야 할까요? 개발자들은 앱 SDK 출시에서 무엇을 얻어가야 할까요? 내부적으로는 어떻게 보시나요? 셔윈(Sherwin) [00:03:22]: 제가 생각하는 방식은 이렇습니다. 저는 OpenAI를 처음부터 기술을 개방하고 전 세계에 알리는 것을 정말 중요하게 여기는 회사로 봅니다. 내부적으로 많이 이야기하는 것 중 하나는 이렇습니다. OpenAI의 사명은 첫째, AGI를 구축하는 것인데, 저희는 그걸 시도하고 있습니다. 그리고 둘째, 잠재적으로 그만큼 중요한 것은 그 혜택을 전 세계에 가져다주는 것입니다. 그리고 저희가 아주 일찍 깨달은 것 중 하나는, 회사로서 저희가 전 세계 모든 구석구석에 그것을 가져다주는 것은 매우 어렵다는 것입니다. 그리고 저희는 개발자들, 다른 제3자들에게 의존해야만 이것을 할 수 있습니다. 그렉(Greg)이 API의 시작과 그것이 어떻게 형성되었는지에 대해 이야기했듯이 말입니다. 하지만 그것은 저희의 정신의 일부였습니다. 저희는 개발자들에게 의존해야 하고, 저희의 사명을 진정으로 완수하기 위해 저희 기술을 전 세계에 개방하여 그들이 참여할 수 있도록 해야 합니다. 그래서 API는 분명히 그것을 하는 매우 자연스러운 방법입니다. 저희는 단순히 API 엔드포인트(endpoint)를 노출하거나 사람들이 무언가를 만들 수 있도록 도구(tool)를 노출합니다. 하지만 이제 저희는 주간 활성 사용자(weekly active user) 8억 명을 가진 ChatGPT를 가지고 있습니다. 저희가 공유했던 통계를 잊었네요. 제 생각에는 이제 세계에서 다섯 번째 또는 여섯 번째로 큰 웹사이트(website)인 것 같습니다. 스윅스(Swyx) [00:04:30]: 그리고 애플 앱 스토어(Apple App Store)에서 가장 많이 다운로드(download)된 앱 1위와 2위입니다. 셔윈(Sherwin) [00:04:35]: 아, 네, 소라(Sora)와 함께요, 네, 네, 하지만 그게 항상 변동하는 건 별로예요. 스윅스(Swyx) [00:04:37]: 그래서 축하하고, 좋을 때 스크린샷(screenshot)을 찍는 것이 좀 어렵죠. 셔윈(Sherwin) [00:04:40]: 네, 좋을 때 스크린샷을 찍어두긴 했습니다. 하지만 제 요점으로 돌아가자면, 저희는 항상 개발자들을 통해 AGI의 혜택을 전 세계에 알리는 방법으로 참여해 왔습니다. 그래서 저는 이것을 이것의 자연스러운 확장으로 봅니다. 솔직히 말해서, 저희는 지난 데브데이에서 GPT와 함께, 그리고 두 번의 데브데이 전에는 죄송합니다, 두 번의 개발자 행사 전에는 GPT와 플러그인(plugin)으로 이것을 시도해 왔습니다. 그것은 데브데이와 연결되지 않았던 것 같습니다. 그래서 저는 이것을 다시 한번, 저희는 반복적으로(iteratively) 배포하는 것을 좋아하고, 이것을 그 과정의 연속이자 개발자들과 깊이 소통하며 저희가 가진 것, 이 경우 ChatGPT 배포의 혜택을 그들이 누릴 수 있도록 돕는 것으로 봅니다. 알레시오(Alessio) [00:05:23]: 앱 SDK가 MCP 프로토콜(protocol)을 기반으로 구축되었다면, OpenAI는 언제 MCP 기반이 되었나요? 내부적으로 자체 프로토콜을 만들지에 대한 논의가 있었을 것이라고 확신합니다. 언제 MCP를 받아들였고, 얼마나 오래되었나요? 크리스티나(Christina) [00:05:38]: 제 생각엔 3월이었던 것 같아요. 정확히 기억하기가 좀 어렵네요. 스윅스(Swyx) [00:05:42]: 3월이 MCP의 이륙(takeoff) 시점이었습니다. 크리스티나(Christina) [00:05:44]: 네, 네. 그래서 저희는 에이전트 SDK(agents SDK)를 구축했고, 3월 초에 응답 API(responses API)와 함께 출시했습니다. 그리고 MCP가 성장하면서, 그것이 정말... 그리고 저희는 도구를 호출하고 훨씬 더 강력해질 수 있는 새로운 에이전트 API를 구축하고 있습니다. MCP는 개발자들이 이미 사용하고 있던 자연스러운 프로토콜과 같았습니다. 크리스티나(Christina) [00:06:05]: 제 생각엔 3월에 저희가 에이전트 SDK에 MCP를 먼저 추가했고, 그 직후에 다른 것들과 함께 추가했습니다. 셔윈(Sherwin) [00:06:11]: 네, 저희가 OpenAI가...라는 트윗(tweet) 같은 것을 했던 것 같아요. 크리스티나(Christina) [00:06:15]: 네, 분명히 그런 순간이 있었습니다. 특정 트윗에 특정 순간이 있었던 것 같아요. 셔윈(Sherwin) [00:06:19]: 하지만 제가 말씀드리고 싶은 것은, 그리고 이것은 솔직히 MCP를 만든 앤트로픽(Anthropic) 팀의 공로인데, 저는 그들이 그것을 개방형 프로토콜(open protocol)로 취급한다고 정말 생각합니다. 저희는 데이비드(David)와 같은 분들, 그리고 컨소시엄(consortium)의 사람들과 매우 긴밀하게 협력하고 있습니다. 그리고 그들은 그렇습니다. 그들은 그것을 앤트로픽에만 특정한 것으로 보지 않습니다. 그들은 그것을 개방형 프로토콜로 봅니다. 그것은 개방형 프로토콜입니다. 변경 사항을 만드는 방식이 매우 개방적입니다. 저희 팀원 중 한 명인 닉 쿠퍼(Nick Cooper)도 MCP의 운영 위원회(steering committee)에 참여하고 있습니다. 그래서 저는 그들이 그것을 저희와 다른 회사들, 그리고 모든 사람들이 쉽게 받아들일 수 있는 것으로 취급하고 있다고 생각합니다. 그들이 그렇게 해야 한다고 생각합니다. 왜냐하면 그들은 그것이 모두에게 널리 받아들여지기를 원하기 때문입니다. 그래서 그 때문에 저희가 그것을 받아들이는 것이 조금 더 쉬워진다고 생각합니다. 그리고 솔직히, 그것은 훌륭한 프로토콜입니다. 매우 일반적입니다. 이미 해결된 문제입니다. 왜 직접 만들겠습니까? 네, 네, 매우 일반적입니다. 물론 아직 할 일이 더 많지만, 그것이 얼마나 간소하고 단순했는지 때문에 저희가 통합하기가 매우 쉬웠습니다. 네. 스윅스(Swyx) [00:07:18]: 앱 SDK 관련 마지막 코멘트이고, 그 다음 에이전트킷으로 넘어가겠습니다. 아시다시피, 웹사이트(website)나 AI 앱을 와이어프레임(wireframe)할 때 추상적으로 보면, 예전에는 웹사이트에 초기 AI 통합을 하려면 API가 있어야 했습니다. 일반 웹사이트가 있고, 그 안에 작은 챗봇 앱이 있었죠. 그런데 이제는 ChatGPT가 최상위 계층에 있고, 그 안에 웹사이트가 내장된 형태로 역전된 것 같습니다. 저는 솔직히 이런 역전을 오랫동안 찾아왔습니다. 그리고 통합 기능과 커스텀 UI 컴포넌트들이 정말 잘 구현되었다고 생각합니다. 기조연설에서 캔바(Canva)를 보여주셨는데, 캔바처럼 보이지만 ChatGPT의 모든 컨텍스트(context)에서 채팅할 수 있습니다. 그런 경험은 본 적이 없습니다. 네. 셔윈(Sherwin) [00:08:06]: 그리고 제 생각에 그것은 저희가 플러그인에서 많은 것을 배웠기 때문에 얻은 반복적인 학습과 관련이 있습니다. 예를 들어, 저희가 플러그인을 출시했을 때, 저희가 받은 피드백 중 하나가 기억납니다. 여기 계신 분들이 플러그인을 정말 기억할지는 모르겠지만요. 3월이었던 것 같아요. 아, 네. 23년이요. 네. 피드백 중 하나는 "아, 소매업을 통합할 수 있겠네요"였습니다. 저희는 모든 회사들에게 이 플러그인을 ChatGPT에 통합할 수 있다고 말했지만, 그들은 그것이 정확히 어떻게 사용되는지에 대해 많은 통제권을 가지고 있지 않았습니다. 그것은 모델이 호출할 수 있는 도구일 뿐이었고, 그들은 정말 ChatGPT에 묶여 있었습니다. 그래서 저는 이것을 통해 저희 제품의 진화를 볼 수 있다고 생각합니다. 그리고 이번에는 회사들, 즉 제3자 개발자들이 경험을 진정으로 소유하고 이끌어가며 자신들처럼 느끼게 하고, 그들 자신의 브랜드(brand)를 진정으로 보존하도록 돕는 것이 얼마나 중요한지 깨달았습니다. 그리고 사실, 저희가 이전에 다른 모든 단계를 거치지 않았다면 그런 학습을 얻지 못했을 것이라고 생각합니다. 알레시오(Alessio) [00:08:54]: 멋집니다. 크리스티나, 오늘 무대에서 에이전트킷 데모로 스타였습니다. 에이전트(agent)를 구축하는 데 8분이 주어졌는데, 1분을 남겼습니다. 그리고 당신은. 크리스티나(Christina) [00:09:03]: 네, 솔직히 확신이 없었어요. "테스트(test)를 좀 덜 하고, 아마도..." 제가 거기에 얼마나 시간을 썼는지 모르겠네요. 다운로드(download)가 시작되었을 때 엄청나게 스트레스(stress)를 받았습니다. 셔윈(Sherwin) [00:09:14]: 저는 "만약 UI 버그(bug) 때문에 데모가 중단되면 정말 슬플 거야"라고 생각했습니다. 크리스티나(Christina) [00:09:17]: 제 생각엔 전체 화면, 네, 포커스(focus) 문제였던 것 같아요. 창이 포커스되어 있지 않았다고 들었습니다, 네. 알레시오(Alessio) [00:09:23]: 에이전트킷을 청중에게 소개해주시겠어요? 크리스티나(Christina) [00:09:26]: 네, 오늘 에이전트킷을 출시했습니다. 에이전트를 구축하고 배포하며 최적화하기 위한 완전한 솔루션(solution) 세트입니다. 이 중 많은 부분이 API 고객들과 협력하면서 에이전트를 구축하고 실제로 프로덕션(production)에 적용하는 것이 얼마나 어려운지 깨달은 데서 비롯되었다고 생각합니다. 그런 자신감과 반복적인 루프(loop)를 얻고, 프롬프트(prompt)를 작성하고, 최적화하고, 평가(eval)를 작성하는 모든 과정은 많은 전문 지식을 필요로 합니다. 그래서 이러한 학습 내용을 바탕으로 필요한 작업을 훨씬 더 쉽고 직관적으로 알 수 있도록 도구 세트로 패키징(packaging)했습니다. 그래서 몇 가지 다른 빌딩 블록(building block)이 있습니다. 이것들은 독립적으로 사용될 수 있지만, 전체 엔드 투 엔드(end to end) 시스템을 얻을 수 있기 때문에 함께 사용하면 더 강력합니다. 오늘 이것을 출시하여 사람들이 시도해보고 무엇을 만들지 볼 수 있도록 했습니다. 스윅스(Swyx) [00:10:13]: 네, 모든 빌딩 블록을 머릿속에 담아두기가 어렵지만, 실제로 시간 순서대로 보면 여러분이 에이전트 SDK를 먼저 시작하고, 그 다음 에이전트 빌더, 커넥터 레지스트리(connector registry), 챗킷(chat kit), 그리고 평가 관련 기능들을 가지고 있다는 점이 정말 흥미롭습니다. 제가 놓친 주요 구성 요소가 있나요? 셔윈(Sherwin) [00:10:32]: 그게 주요 움직이는 부분들이죠? 네, 그게 다인 것 같습니다. 그리고, 음, 저희는 여전히 RFT 미세 조정 API(fine tuning API)를 가지고 있지만, 기술적으로는 에이전트킷 우산(umbrella) 밖에 분류합니다. 스윅스(Swyx) [00:10:42]: 알겠습니다, 알겠습니다, 알겠습니다. 네, 어떻게 발전하는지 이상하네요. 이제 완전한 에이전트 플랫폼이 되었죠? 그리고 데모를 볼 때 제가 명확하지 않았던 한 가지는, 무대에서 데브데이 웹사이트를 위한 라이브 채팅 앱을 만들었다는 점이 매우 재미있다는 것입니다. 크리스티나(Christina) [00:11:02]: 네, 한번 사용해 볼 기회가 있었나요? 스윅스(Swyx) [00:11:03]: 네, 시도해 보려고 했습니다. 정말 멋졌어요. 그리고 사실, 어떻게 배포하는지 묻고 싶었습니다. 굿즈(merch)는 어디에 있나요? 네, 맞아요. 저는 "굿즈를 어디서 클릭했지?"라고 생각했습니다. 어쨌든, 제가 컨퍼런스(conference)를 위해 해봤기 때문에 이것은 저에게 매우 친숙한 일이고, 매우 유사한 과정입니다. 하지만 명확하지 않았던 점은 에이전트 빌더 안에서 얼마나 많은 작업이 이루어질 것인가 하는 점입니다. 무대에서는 이야기하지 못했지만, 사용자 승인(user approval)과 같은 흥미로운 노드(node)들이 실제로 있습니다. 그것은 전체적인 문제입니다. 그리고 변환(transform) 및 상태 설정(set state)과 같이, 여기에는 일종의 튜링 완전(Turing complete) 기계가 있습니다. 크리스티나(Christina) [00:11:35]: 네. 네, 그러니까 제 말은, 다시 말하지만, 에이전트 빌더를 처음 선보이는 것이고, 그래서 이것은 저희가 구축하고 있는 것의 시작이라고 생각합니다. 그리고 인간 승인(human approval)은 저희가 꽤 깊이 파고들고 싶은 사용 사례(use case) 중 하나라고 생각합니다. 오늘 제가 보여드린 노드는 이진 승인(binary approval)처럼 꽤 간단합니다. 승인 거부. 이것은 MCP 도구에서 액션이 발생할 수 있음을 승인하는 것과 비슷합니다. 네. 하지만 저희 사용자들이 훨씬 더 복잡한 워크플로우(workflow)에서 본 것은, 실제로 인간 개입(human in the loop) 상호작용처럼 꽤 진보적이라는 것입니다. 때로는 이런 상호작용이 몇 주에 걸쳐 이루어질 수도 있습니다. 단순히 도구에 대한 간단한 승인이 아닙니다. 실제 의사 결정이 포함됩니다. 그리고 저희는 그런 고객들과 협력하면서 그런 사용 사례에 대해서도 계속해서 더 깊이 파고들고 싶습니다. 알레시오(Alessio) [00:12:21]: 네. 진입점(entry point)은 무엇인가요? 개발자들도 여기에 와서 두 가지 코드 내보내기(code export)를 해야 하나요? 네. 아니면 사용 사례를 분할해야 하나요? 네. 크리스티나(Christina) [00:12:31]: 에이전트 빌더를 사용하는 두 가지 이유는 첫째, 일종의 놀이터(playground)처럼 사용하는 것입니다. 시스템을 모델링하고 반복하며, 프롬프트를 작성하고 최적화하며 테스트하는 것입니다. 그리고 나서 에이전트 SDK를 사용하여 다른 모델들과 함께 자신의 시스템에서 내보내고 실행할 수 있습니다. 두 번째는 저희가 여러분을 위해 배포하는 모든 혜택을 얻는 것입니다. 그래서 자연어(natural language)를 사용하여 어떤 종류의 에이전트를 구축하고 싶은지 설명할 수 있습니다. 모델링하고, 주제 전문가(subject matter expert)를 참여시켜 반복하고 피드백을 받을 수 있는 캔버스(canvas)를 가질 수 있습니다. 데이터 세트(data set)를 구축하고 그 주제 전문가들로부터 피드백을 받을 수도 있습니다. 그리고 이 모든 것을 직접 처리할 필요 없이 배포할 수 있습니다. 그리고 이것이 챗킷을 구축하는 저희의 철학의 많은 부분을 차지합니다. 그렇죠? 여러분은 그 조각들을 가져갈 수 있습니다. 훨씬 더 맞춤화된 고급 통합을 가질 수 있습니다. 하지만 또한 매우 쉬운 기본값(default)과 함께 라이브(live)로 전환하는 매우 자연스러운 경로를 얻을 수 있습니다. 네. 알레시오(Alessio) [00:13:32]: 이것을 양방향으로 보시나요? 여기서 구축하고, 코드로 가서, 코드에서 변경 사항을 만들고, 그 변경 사항을 다시 에이전트 빌더로 가져오는 식으로요. 크리스티나(Christina) [00:13:41]: 궁극적으로는 분명히 그렇게 하고 싶습니다. 그래서 아마도 코드에서 시작할 수 있을 것입니다. 가져올 수 있습니다. 에이전트 빌더에서도 코드를 실행할 수 있는 기능도 있을 것입니다. 그래서 그 부분에 대해 많은 유연성(flexibility)이 있다고 생각합니다. 셔윈(Sherwin) [00:13:54]: 제가 또 말씀드리고 싶은 한 가지는, 오늘 저희가 보여드린 많은 데모들이 청중들이 이해하기 쉽도록 단순함에 초점을 맞췄다는 것입니다. 하지만 많은 고객들과 이야기해보면, 그들은 꽤 복잡한 것을 구축하고 있습니다. 전체 흐름을 보려면 캔버스를 꽤 많이 축소해야 합니다. 그리고 저희는 이런 작업을 하는 많은 고객들과 협력하고 있었습니다. 그리고 그것을 실제 에이전트 SDK 파일로 바꾸면 꽤 길어집니다. 그래서 저희는 시각적인 설정이 여기에 있는 것이 많은 이점이 있다고 보았습니다. 특히 설정이 점점 길어질수록 말이죠. 이것을 시연하기는 좀 어려웠을 것입니다. 네. 8분 안에 할 수 있습니다. 맞죠. 네. 8분 안에 할 수 있습니다. 하지만 저희 사이트에 있는 일부 사전 설정(preset)을 사용하더라도요. 네, 정확합니다. 그래서 그런 것들 중 하나입니다. 네. 네. 크리스티나(Christina) [00:14:33]: 오늘 캔버스(canvas)와 함께 출시한 것 중 하나는, 저희 엔지니어들이 현장에서 고객들과 직접 작업하면서 얻은 공통적인 패턴(pattern)들을 모아놓은 템플릿(template) 세트입니다. 기본적으로 고객 지원, 문서 검색 등 고객과 작업할 때 사용하는 플레이북(playbook)과 같은 것들입니다. 그래서 그것들도 함께 공개했습니다. 스윅스(Swyx) [00:14:54]: 데이터 강화(Data enrichment). 네. 네. 계획 도우미(Planning helper), 고객 서비스(customer service), 구조화된 데이터 Q&A(structured data Q&A), 문서 비교(document comparison). 좋습니다. 내부 지식 도우미(Internal knowledge assistant). 네. 네. 네. 크리스티나(Christina) [00:15:01]: 그리고 저희는 그것들을 구축할 수 있는 대로 더 많이 추가할 계획입니다. 스윅스(Swyx) [00:15:06]: 저는 항상 궁금합니다. 여러분이 유일한 에이전트 빌더는 아니지만, 분명히 OpenAI라는 특성상 매우 중요한 위치에 있습니다. 이런 종류의 에이전트 빌더 패턴의 다양한 오픈 소스 구현체들 간의 프로토콜이나 상호 운용성(interop)에 대한 관심이 있나요? 셔윈(Sherwin) [00:15:23]: 저희도 생각해 봤습니다. 특히 에이전트 SDK 주변에서요. 사실 이것을 좀 더 넓게 보면, 저희도 여기 앉아서 여러 가지 것들이 계속해서 만들어지는 것을 관찰하고 있었습니다. 에이전트 워크플로우 외에도, 저희는 응답 API를 통해 했던 것처럼, 스테이트풀 API(stateful API)와 같이 업계가 응답으로 하려는 것을 출시하고 있습니다. 그래서, 아시다시피, 저희가 응답 API를 처음 출시했지만, 다른 몇몇 사람들이 채택했습니다. 그록(Grok)도 API에 그것을 가지고 있는 것 같습니다. LMSYS가 최근에 벽에서 무언가를 한 것을 본 것 같은데, 모두가 그런 것은 아닙니다. 그래서, 불행히도 오늘 예/아니오로 명확한 답변을 드릴 수는 없지만, 저희는 모든 것을 평가하고 있으며, MCP와 함께, 그리고 저희의 커머스 프로토콜(commerce protocol)과 함께 많은 가치가 있었다는 것을 확인하려고 노력하고 있습니다. ACP, 네, 그게 그거죠, 제가 이름을 잊지 않았습니다. 셔윈(Sherwin) [00:16:20]: 그래서 에이전트로 무엇을 하고 싶은지 생각하는 것과 같습니다. 네. 그래서 에이전트 워크플로우(agent workflow)와 관련하여 이식성(portability)에 대한 이야기도 마찬가지라고 생각합니다. 응답 API의 이식성도 표준이 될 수 있다면 좋을 것입니다. 그러면 개발자들이 다른 모델을 사용하고 싶을 때 세 가지 다른 스테이트풀 API 통합을 구축할 필요가 없을 것입니다. 네. 크리스티나(Christina) [00:16:38]: 그리고 제 생각에 그것이 바로, 정확히 프로토콜은 아니지만, 오늘 평가와 함께 출시한 것 중 하나는 제3자 모델(third party model)도 사용할 수 있고 그것을 한 곳으로 가져올 수 있다는 것입니다. 그래서 저는 분명히 생태계(ecosystem)가 어디에 있는지 알 수 있다고 생각합니다. 즉, 다중 모델(multi-model)을 사용하고 일종의... 스윅스(Swyx) [00:16:55]: 제3자 모델이라는 것은 OpenAI 모델이 아닌 것을 말하나요? 네. 셔윈(Sherwin) [00:16:57]: 네. 오늘부터 평가와 함께 작동할 것입니다. 네. 알겠습니다. 이해했습니다. 저희는 오픈 라우터(open router)와 함께 정말 멋진 설정을 가지고 있습니다. 그들과 협력하고 있으며, 여러분은 오픈 라우터 설정을 가져올 수 있습니다. 그리고 그것으로 실제로, 저희 데이터 세트 도구(data sets tool) 또는 사용자 데이터 세트 도구(user data set tool)를 사용하여 많은 평가를 생성할 수 있습니다. 그리고 실제로 다양한 모델 제공업체(model provider)들을 사용할 수 있을 것입니다. 네. 어디에서든 선택할 수 있습니다. 함께 있는 오픈 소스 모델(open source model)들도 포함해서요. 그리고 저희 제품에서 결과를 볼 수 있습니다. 스윅스(Swyx) [00:17:24]: 네. 정말 멋집니다. 평가에 대해 더 이야기하자면, 그렇죠? 출시 문서 어딘가에서 에이전트 평가를 허용하기 위해 평가 제품을 약간 확장해야 했다고 본 것 같습니다. 거기서 무엇을 해야 했는지 말씀해 주시겠어요? 셔윈(Sherwin) [00:17:41]: 네. 크리스티나(Christina) [00:17:43]: 네. 셔윈(Sherwin) [00:17:43]: 말씀드리자면, 에이전트 평가(agent eval)는 아직 진행 중인 작업이라고 생각합니다. 네. 저희가 필요한 진전의 약 10% 정도를 이룬 것 같습니다. 예를 들어, 다중 모드 평가(multimodal eval)에 대해 아직 할 일이 많다고 생각합니다. 하지만 이번에 저희가 이룬 주요 진전은 트레이스(trace)를 가져올 수 있도록 한 것입니다. 에이전트 SDK에는 정말 좋은 트레이스 기능이 있어서, 무언가를 실행하고 정의하면 정말 긴 트레이스를 가질 수 있습니다. 그것을 평가 제품에서 사용하여 그것이 수행해야 할 전체적인 것을 어떤 식으로든 채점할 수 있도록 하는 것입니다. 이것이 1단계라고 생각합니다. 이것을 할 수 있다는 것은 좋다고 생각합니다. 하지만 앞으로 저희의 로드맵(roadmap)은 트레이스의 다른 부분들을 정말로 세분화하고, 각각을 평가하고 측정하며 최적화할 수 있도록 하는 것입니다. 많은 경우 이것은 인간 개입도 포함할 것이며, 그래서 저희는 여기에 인간 개입 컴포넌트(component)도 가지고 있습니다. 하지만 지난 1년 동안 저희의 평가 제품을 보면, 매우 단순했습니다. 단순한 프롬프트 완성 설정에 훨씬 더 맞춰져 있었습니다. 하지만 분명히, 사람들이 이러한 더 긴 에이전트 트레이스를 수행하는 것을 보면서, 예를 들어, 20분짜리 작업을 어떻게 정확하게 평가할 수 있을까요? 그것은 정말 어려운 문제입니다. 저희는 평가 제품을 설정하고 그런 방향으로 나아가서 전체 궤적뿐만 아니라 개별 부분도 평가할 수 있도록 돕고 있습니다. 네. 스윅스(Swyx) [00:19:01]: 제 말은, 마법의 키워드(keyword)는 루브릭(rubric)이죠? 모두가 LM(Large Language Model)을 심사 루브릭으로 원합니다. 네. 네. 셔윈(Sherwin) [00:19:07]: 네. 스윅스(Swyx) [00:19:08]: 분명히 그렇게 될 것입니다. 좋습니다. 네. 온라인에서 개발자 커뮤니티가 매우 흥분하는 또 다른 것은 자동화된 브라우저(automated browser)입니다. 그래서, 아시다시피, 저희는 자동화된 프롬프트 최적화(automated prompt optimization)를 가지고 있는데, 이것은 프롬프트와 함께 루프 내 평가와 같은 것입니다. 그 부분에 대한 생각은 무엇인가요? 어떻게 진행되고 있나요? 크리스티나(Christina) [00:19:22]: 네. 저희는 자동화된 프롬프트 최적화를 가지고 있지만, 다시 말하지만, 이 분야에 더 많이 투자하고 싶습니다. 저희는 GPT-5를 출시했을 때 이것을 꽤 크게 출시했다고 생각합니다. 왜냐하면 새로운 모델이 나올 때마다 새로운 모델의 모든 특이점을 배우는 것이 꽤 어렵다는 것을 알았기 때문입니다. 네, 프롬프트 최적화요. 맞습니다. 저희는 출시하는 모든 모델에 대해 큰 프롬프트 가이드(prompting guide)를 가지고 있습니다. 그리고 그것을 훨씬 더 쉽게 만들 시스템을 구축하는 것이 중요하다고 생각합니다. 저희는 그것을 평가와 완전히 연결하고 싶습니다. 설정한 평가를 기반으로 에이전트 빌더에서 만들어진 경우, 시간이 지남에 따라 프롬프트를 개선하고 에이전트도 개선할 수 있어야 합니다. 그래서 저희는 이것을 여러분이 구축하는 것에 대한 기본적으로 제안된 개선 사항을 제공하는 플랫폼의 꽤 핵심적인 부분으로 봅니다. 셔윈(Sherwin) [00:20:05]: 저는 지금 프롬프트 최적화(prompt optimization) 분야가 정말 멋진 시기라고 생각합니다. 여러분도 그렇게 느끼실 겁니다. 이와 관련하여 많은 제품들이 개발되고 있을 뿐만 아니라, 흥미로운 연구도 많이 진행되고 있습니다. 예를 들어, 데이터브릭스(Databricks) 사람들과 함께하는 GEPA는 이 분야에서 정말 멋진 일을 하고 있습니다. 저희는 현재 제품에서 GEPA 최적화를 하고 있지는 않지만, 곧 그렇게 하고 싶습니다. 또한, 이 분야는 활발한 연구 영역입니다. 그래서 마테이(Matei)와 데이터브릭스 사람들이 다음에 무엇을 생각할지, 저희가 내부적으로 무엇을 생각할지 등 새로운 프롬프트 최적화 기술이 나오면 저희 제품에도 그것을 포함하고 싶습니다. 그리고 네, 사람들이 프롬프트의 중요성을 깨닫는 시기에 이런 일이 일어나고 있다는 점이 흥미롭습니다. 아시다시피, 2년 전에는 사람들이 "아, 언젠가는 프롬프트, 즉 프롬프트 엔지니어링(prompt engineering)이 사라질 거야"라고 말했던 것 같습니다. 아니요. 아시다시피. 오히려 더 중요해졌습니다. 네, 네, 네. 오히려 점점 더 확고해졌습니다. 그리고 저는 이것이 점점 더 중요해지는 흥미로운 추세라고 생각합니다. 그리고 프롬프트 최적화를 더욱 확고히 하는 흥미롭고 멋진 작업도 진행되고 있습니다. 그래서 저는