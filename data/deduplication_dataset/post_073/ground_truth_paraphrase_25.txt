다음은 업데이트된 블로그 게시물입니다.

1.  **ARE 메탈 슈퍼인텔리전스 랩스(ARE Metal SuperIntelligence Labs)**는 실제와 같은 시간 의존적 조건 하에서 에이전트 시스템을 개발하고 성능을 검증하는 데 사용되는 연구 환경과 평가 기준을 소개합니다. 이 논문은 소음이 많고 동적인 환경에서 비동기적 사건, 쓰기 동작(write action) 검증, 다중 에이전트(multi-agent) 조정을 강조하는 모듈형 시뮬레이터(ARE)와 모바일 기기 스타일의 벤치마크(Gaia2)를 제시합니다. 플랫폼 주요 특징: ARE는 환경을 애플리케이션, 이벤트, 알림, 시나리오로 모델링하며, 에이전트가 사고하는 중에도 시간은 계속 흐릅니다. DAG 스케줄러가 의존성을 관리하며, 에이전트는 도구와 비동기 알림 큐(async notification queue)를 통해 상호작용합니다. Gaia2 벤치마크: 스마트폰과 유사한 환경에서 이메일, 채팅, 캘린더, 쇼핑과 같은 앱에 걸쳐 101개의 도구를 사용하는 1,120개의 검증 가능한 시나리오를 포함합니다. 이 시나리오들은 탐색, 수행, 적응성, 시간 관리, 불확실성 처리, 그리고 에이전트 간 협력이라는 여섯 가지 핵심 역량을 평가하도록 설계되었습니다. 검증기 설계: 평가는 에이전트의 쓰기 동작 시퀀스를 오라클(oracle) 쓰기 동작과 비교하며, ID와 같은 인수에 대한 엄격한 검사와 내용에 대한 LLM(대규모 언어 모델)의 유연한 판단을 결합합니다. 이는 인과 관계와 타이밍을 검증하며, 다중 턴(multi-turn) 시나리오에서는 각 턴별로 실행됩니다. 주요 결과 및 절충점: 단일 모델이 모든 능력(capability)에서 지배적이지 않으며, 예산 스케일링 곡선은 정체되는 양상을 보입니다. 1페이지의 차트는 pass@1 대 최대 예산을 시각화합니다. 시간 및 협업: 시간적 제약은 역 스케일링 현상(inverse scaling effect)을 유발하는데, 이는 복잡한 추론 방식을 사용하는 정책들이 다른 환경에서는 우수하지만, 정해진 시간 내에 중요한 기회를 놓치는 경향을 의미합니다. '즉시 모드(instant mode)'는 이러한 성능 격차를 줄이는 데 기여합니다. 에이전트 간 상호작용 설정은 하위 목표 위임(sub-goal delegation)을 통해 경량 모델에 이점을 제공하지만, 가장 강력한 시스템에는 혼합된 결과를 가져옵니다. GUI는 이벤트 그래프 검사, 추적 재생, 제로 코드 시나리오 작성 기능을 지원하여 연구자들이 에이전트의 행동을 쉽게 디버깅하고 새로운 시나리오를 개발할 수 있도록 돕습니다. 현실적인 시간 기반 환경에서의 에이전트 개발은 실시간 의사결정 및 동적 변화에 대한 대응 능력을 요구하며, 이는 실제 세계에 AI를 배포하는 데 필수적입니다. 이러한 플랫폼은 복잡한 산업 제어 시스템이나 재난 대응 시뮬레이션과 같은 분야에서 에이전트의 신뢰성을 검증하는 데 중요한 역할을 할 수 있습니다. [논문](Paper) | [트윗](Tweet)

2.  **ATOKEN**은 이미지, 영상, 3D 자산 등 다양한 형태의 데이터에 적용 가능한 단일 트랜스포머 기반 토크나이저를 제안합니다. 해당 시스템은 4D RoPE를 활용하여 모든 종류의 데이터를 공통된 희소 4차원 잠재 공간(sparse 4D latent space)으로 부호화하며, 적대적 학습(adversarial loss) 없이 연속형 및 이산형 토큰 모두를 처리할 수 있습니다. 이 논문은 강력한 재구성 품질과 견고한 의미론적 정렬(semantic alignment)을 보고하며, 이는 다양한 양식(modality)에 걸쳐 생성과 이해를 모두 가능하게 합니다. 2D, 비디오, 3D를 위한 하나의 잠재 공간(latent space)을 사용합니다. 입력은 희소 (t, x, y, z) 특징(feature)으로 패치화(patchify)되며, 따라서 이미지는 2D 슬라이스이고, 비디오는 시간에 대한 정보를 추가하며, 3D는 다중 뷰 렌더링(multiview render)에서 집계된 표면 복셀(surface voxel)을 사용합니다. 4D RoPE와 기본 해상도(native resolution)를 갖춘 순수 트랜스포머(Transformer) 구조를 채택합니다. 인코더(encoder)는 SigLIP2 비전 타워(vision tower)를 시공간 블록(space–time block)으로 확장하고 4D 회전 위치(rotary position)를 추가하며, 디코더(decoder)는 트랜스포머를 미러링하여 픽셀 또는 3D 가우시안(Gaussian)을 재구성합니다. 기본 해상도와 KV 캐시된 시간 타일링(KV-cached temporal tiling)은 비디오 추론(video inference) 속도를 향상시킵니다. 텍스처 통계(texture statistics)를 목표로 하는 적대적 손실 없는(adversarial-free) 훈련 방식을 사용합니다. GAN(생성적 적대 신경망) 대신, 손실 함수는 L1, LPIPS, CLIP 지각(perceptual) 및 그램 행렬(Gram-matrix) 항을 혼합하며, 이는 공분산(covariance)이 오류를 지배함을 보여주는 rFID 분해에 의해 동기 부여되었습니다. 다양한 양식에 걸친 점진적 커리큘럼(progressive curriculum)을 적용합니다. 네 단계로 능력이 확장됩니다: 이미지 재구성, 비디오 추가, 3D 추가, 그리고 선택적인 FSQ 양자화(quantization). 종합적인 성능 평가에서, 연속형 잠재 변수를 활용한 ATOK은 이미지 분야에서 0.21 rFID와 ImageNet 제로샷 정확도 82.2%를 기록했습니다. 비디오에서는 36.07 PSNR 및 3.01 rFVD를, 그리고 Toys4k 데이터셋에서는 28.28 PSNR과 90.9%의 3D 분류 정확도를 보였습니다. 이산 FSQ 토큰(discrete FSQ token)은 경쟁력을 유지하면서 AR 생성 및 이미지-3D 변환을 가능하게 합니다. ATOK의 이러한 통합적 접근 방식은 다양한 양식 간의 지식 전이를 촉진하고, 모델 복잡성을 줄여 멀티모달 AI의 효율성을 크게 향상시킬 수 있습니다. 적대적 손실 없는 훈련은 모델의 안정성을 높이고 더 고품질의 콘텐츠 생성을 가능하게 합니다. 이는 통합 콘텐츠 생성, 로봇 비전, 가상 현실 환경 구축 등 미래의 다양한 응용 분야에서 핵심적인 역할을 할 것으로 기대됩니다. [논문](Paper) | [트윗](Tweet)

3.  **코드 월드 모델(CWM)**은 Meta FAIR에서 개발한 320억 개의 오픈 가중치를 가진 코더로, 컨테이너 환경 내에서 코드 실행을 모방하도록 훈련되었습니다. 이 모델은 파이썬 인터프리터의 실행 기록과 에이전트 스타일의 도커 궤적을 활용한 중간 훈련을 거친 후, 소프트웨어 엔지니어링(SWE), 코딩, 수학 분야에서 다중 턴 강화 학습(RL)을 통해 성능을 증진시킵니다. CWM은 뛰어난 코딩 능력을 지녔으며, 소프트웨어 개발 환경에서 월드 모델 방식의 추론을 실험하기 위한 기반을 제공합니다. 실행 인식 훈련 레시피: 8조(8T) 토큰으로 사전 훈련(pretrain)한 다음, 컨테이너화된 저장소(repo)에서 수집된 파이썬 실행 추적(Python execution trace) 및 ForagerAgent 궤적(trajectory)으로 5조(5T) 토큰 중간 훈련(mid-train)을 수행합니다. 이어서 SFT(지도 미세 조정, 100B)와 GRPO 스타일 알고리즘 및 비동기 롤아웃(asynchronous rollout)을 사용한 공동 다중 작업 RL(강화 학습)을 진행합니다. 이 연구 결과는 1억 2천만 개의 추적된 함수, 약 7만 개의 저장소 단위 추적, 그리고 3백만 개의 에이전트 유사 궤적을 포함합니다. 모델 + 컨텍스트 스케일링: 교대하는 지역/전역 슬라이딩 윈도우 어텐션(local/global sliding-window attention)과 131k 최대 컨텍스트(max context)를 가진 밀집 320억 개(32B) 디코더(decoder)를 사용합니다. 스케일된 RoPE, GQA, FP8 훈련, 그리고 긴 컨텍스트 버킷화(long-context bucketization)가 처리량(throughput)을 유지하기 위해 적용됩니다. 추론(inference)은 양자화(quantization)를 통해 단일 80GB H100에 적합할 수 있습니다. SWE를 위한 에이전트형 RL 설계: 에이전트는 최소한의 도구 세트(bash, edit, create, submit)를 가진 저장소 샌드박스(repo sandbox) 내에서 작동하며, 테스트를 실행하고, `git diff`로 패치(patch)를 구축하며, 숨겨진 테스트와 패치 유사성 형성(patch-similarity shaping)으로 보상을 받습니다. 자체 부트스트랩된 추적(self-bootstrapped trace)은 RL(강화 학습) 전에 형식 준수(format adherence)를 향상시킵니다. 성능 하이라이트: SWE-bench Verified에서 53.9%의 기본 pass@1과 테스트 시간 스케일링(test-time scaling, best@k)을 적용했을 때 65.8%를 달성했습니다. 3페이지의 차트는 CWM이 훨씬 더 크거나 폐쇄형 모델과 경쟁력이 있음을 보여줍니다. 또한 LCB-v5 68.6, Math-500 96.6, AIME-24 76.0, CruxEval-Output 94.3을 기록했습니다. AI 개발자에게 중요한 이유: CWM은 프롬프트(prompt)에서 파이썬 실행을 시뮬레이션하기 위해 추적 예측 토큰(trace-prediction token)을 노출하여, 근거 있는 추론, 신경 디버거(neural-debugger) 워크플로우, 추적 기반 코드 합성(trace-guided synthesis)을 가능하게 합니다. 절제 연구(ablation study)는 실행 추적(execution trace)이 CruxEval을 향상시키고, ForagerAgent가 에이전트형 NLL(음의 로그 우도) 및 SWE pass@1을 향상시킴을 보여줍니다. 코드 실행을 모델링하는 것은 버그 예측, 코드 자동 수정, 심지어 보안 취약점 분석과 같은 중요한 응용 분야에 필수적입니다. 컨테이너 기반 훈련은 격리된 환경을 제공하여 재현성을 높이고 실제 개발 환경을 모방하는 이점을 가집니다. CWM의 긴 컨텍스트 처리 능력은 대규모 코드베이스를 이해하고 조작하는 데 중요한 기술적 진보를 의미하며, 이는 신경 디버거와 같은 새로운 AI 개발 워크플로우를 가능하게 합니다. [논문](Paper) | [트윗](Tweet)

4.  **LLM에게 계획하는 법 가르치기**: 대규모 언어 모델(LLM)에 PDDL(계획 도메인 정의 언어) 기반 계획 능력을 부여하는 학습 방법은 명확한 상태-행동-상태 연쇄를 생성하도록 유도하고, 각 단계를 외부 검증기(VAL)를 통해 확인하는 방식으로 진행됩니다. 연구 결과, PlanBench 환경에서 계획의 유효성(plan validity)이 현저히 개선되었는데, 특히 단순히 실패 여부만 알리는 것이 아니라 특정 행동이 실패한 원인을 설명하는 피드백을 제공했을 때 그 효과가 더욱 두드러졌습니다. 방법 요약: 두 단계: (1) 전제 조건(precondition) 및 효과(effect)에 대한 설명과 함께 올바르고 의도적으로 오류가 있는 계획에 대한 지시 튜닝(instruction tuning), (2) 모델이 VAL이 단계별로 검증하는 ⟨s₀,a₁,s₁⟩… 체인을 출력하는 CoT(사고의 사슬) 지시 튜닝. 훈련은 추론 체인(reasoning chain) 최적화와 최종 계획 성공 최적화 사이를 번갈아 진행합니다. 작동 원리: 검증기는 각 단계마다 논리적 일관성(logical coherence)을 강제함으로써, 모델이 단순히 패턴을 모방하는 것을 넘어 전제 조건을 검토하고, 결과를 적용하며, 불변 요소를 유지하는 방식을 학습하도록 합니다. 모든 전환이 외부에서 검증되기 때문에 신뢰할 수 없거나 모호한 CoT(사고의 사슬)가 줄어듭니다. 결과: Llama-3를 사용하여 상세한 피드백과 15회 반복으로 Blocksworld에서 94%, Logistics에서 79%, Mystery Blocksworld에서 64%의 계획 유효성(plan validity)을 달성했습니다. GPT-4도 유사한 경향을 보이며 각각 91%, 78%, 59%로 최고치를 기록했습니다. 기준선(baseline) 대비 절대적인 개선 폭이 크며, 예를 들어 일부 설정에서는 +66%에 달합니다. 피드백의 중요성: 상세한 피드백(어떤 전제 조건이 실패했는지 또는 어떤 효과가 잘못 적용되었는지)은 이진 유효/무효(binary valid/invalid) 피드백보다 지속적으로 우수하며, 추가 반복(η가 10에서 15로 증가)을 통해 더 많은 이점을 얻습니다. 범위 및 한계: 세 가지 PlanBench 도메인에서 훈련 및 테스트되었습니다. 난독화된 술어(obfuscated-predicate) 변형(Mystery Blocksworld)에서는 성능이 저하되어 일반화(generalization)의 어려움을 보여줍니다. 이 방법은 최적성(optimality)이 아닌 만족스러운 계획(satisficing plan)을 목표로 하며, 현재는 지속 시간(durative) 또는 조건문(conditional)이 없는 PDDL 하위 집합을 가정합니다. PDDL과 같은 형식 언어를 LLM에 적용하는 것은 AI가 복잡한 문제를 해결하는 데 있어 논리적 정확성과 신뢰성을 크게 향상시킬 수 있습니다. 특히 "실패 원인 설명" 피드백은 마치 인간이 오류를 통해 학습하는 방식과 유사하게, 모델의 추론 능력을 심화시키는 교육학적 가치를 지닙니다. 이러한 발전은 자율 에이전트, 로봇 공학, 자동화 시스템 등 실제 문제 해결 영역에서 LLM의 활용 가능성을 넓히는 중요한 단계입니다. 향후에는 더 복잡한 PDDL 확장 지원과 최적성 목표 달성을 위한 연구가 필요합니다. [논문](Paper) | [트윗](Tweet)

5.  **LLM-JEPA**: LLM-JEPA는 JEPA 방식의 학습 목표(training objective)를 대규모 언어 모델(LLM)에 적용한 것으로, 이는 동일한 핵심 콘텐츠(예: 텍스트 및 코드)의 짝을 이루는 관점들을 임베딩 공간(embedding space) 내 예측 대상으로 간주하여, 일반적인 다음 토큰 예측 손실(next-token loss) 외에 추가됩니다. 그 결과, 지속적인 미세 조정(fine-tuning)을 통해 성능이 향상되고, 사전 훈련(pretraining) 단계에서 유망한 이점을 보여주며, 과적합(overfitting)에 대한 저항력이 더 강해집니다. 한 줄 요약: 표준 다음 토큰 목표(next-token objective)를 유지하면서, 특수 예측 토큰 k를 가진 연결된 LLM 가중치(tied LLM weight)를 사용하여 한 뷰(view)의 임베딩(embedding)을 다른 뷰로부터 예측하며, 코사인 측정(cosine metric)과 가중치 λ로 최적화되는 JEPA 항을 추가합니다. 이는 추상화(abstraction)를 개선하면서 생성(generation) 능력을 보존합니다. JEPA 항이 유용한 이유는 단지 다음 토큰 손실만 최소화하는 것으로는 JEPA 예측 오류를 충분히 줄일 수 없기 때문입니다. 이 추가 항이 이러한 간극을 메우고 정확도 향상에 기여합니다. 주요 결과: Llama, Gemma, OpenELM, OLMo 전반에 걸쳐 LLM-JEPA는 NL-RX (SYNTH 및 TURK), GSM8K, Spider에서 정확 일치 정확도(exact-match accuracy)를 향상시킵니다. 표현 효과: LLM-JEPA를 사용할 때 t-SNE 플롯(plot)은 더 명확한 구조를 보여주며, 낮은 회귀 오류(regression error)와 압축된 특이값(singular value)에 의해 Enc(Text)에서 Enc(Code)로의 거의 선형적인 매핑(mapping)이 지원됩니다. 사전 훈련 신호 및 비용: 사전 훈련(pretraining) 중에 JEPA를 추가하면 표준 미세 조정(fine-tuning) 후 다운스트림 감성 분류(downstream sentiment classification)가 향상되며, 생성 품질(generative quality)은 유지됩니다. 현재 한계는 각 뷰(view)에 대한 별도의 순방향 전달(forward pass)로 인한 추가 계산량과, k와 λ에 대한 비자명한 하이퍼파라미터 탐색(hyperparameter sweep)입니다. JEPA의 핵심 아이디어는 데이터를 예측을 통해 이해하는 표현 학습에 있으며, 이를 LLM에 적용하는 것은 모델이 다양한 양식(modality) 간의 본질적인 관계를 학습하도록 돕습니다. 과적합에 강하다는 점은 모델의 일반화 능력을 향상시켜 실제 적용 시 더욱 견고한 성능을 기대할 수 있게 합니다. 비록 추가적인 계산 비용과 하이퍼파라미터 튜닝의 복잡성이 존재하지만, LLM-JEPA는 멀티모달 AI의 표현 학습 및 이해 능력을 발전시키는 유망한 방향을 제시합니다. [논문](Paper) | [트윗](Tweet)

6.  **ARK-V1**은 단순히 학습된 텍스트를 기억하는 것을 넘어 지식 그래프(knowledge graph)를 능동적으로 탐색하여 언어 모델이 질의에 응답하도록 돕는 가벼운 에이전트(lightweight agent)입니다. 이러한 방식은 특히 모델의 사전 학습(pretraining) 지식이 부족한, 즉 일반적이지 않은 롱테일 개체(long-tail entity)에 대한 질의에 효과적입니다. 작동 원리 – 에이전트가 시작 개체(entity)를 선택하고, 관련 관계(relation)를 파악하며, 일치하는 그래프 트리플(graph triple)을 추출하고, 간략한 추론 과정(reasoning step)을 거쳐 답변을 생성할 준비가 될 때까지 이 과정을 반복하는 단순한 주기입니다. 이는 경로를 따라 이동하는 과정을 설명하는 미니 검색 에이전트(search agent)로 생각할 수 있습니다. 테스트 – CoLoTa 데이터셋을 사용했으며, 이 데이터셋은 의도적으로 희귀한 개체에 대한 질문을 포함합니다. 여기서는 KG(지식 그래프) 사실과 상식(예: 잘 알려지지 않은 도시의 인구 비교)이 모두 필요합니다. 측정 지표에는 에이전트가 얼마나 자주 답변하는지, 답변할 때 얼마나 정확한지, 그리고 여러 실행에서 얼마나 일관적인지가 포함됩니다. 성능 – ARK-V1은 일반적인 CoT(사고의 사슬) 프롬프팅(prompting)을 능가합니다. Qwen3-30B와 같은 중간 규모 모델을 사용하여 약 77%의 쿼리에 답변했으며, 그 중 약 91%의 정확도를 보였습니다. 이는 전체적으로 약 70%의 성능을 나타냅니다. 더 큰 백본(backbone) 모델(Qwen3-235B, Gemini 2.5 Flash, GPT-5 Mini)은 94% 이상의 조건부 정확도(conditional accuracy)로 전체적으로 약 70~74%를 달성했습니다. 약점 – (1) 질문이 모호하거나, (2) KG(지식 그래프)에 상충되는 트리플(triple)이 포함되어 있거나, (3) KG에 필요한 상식이 부족하여 에이전트가 그래프를 너무 신뢰할 때 어려움을 겪습니다. 향후 방향 – 현재 프롬프팅은 단순하며 탐색(traversal)은 비효율적일 수 있습니다. 다음 단계에는 더 스마트한 프롬프트, 효율성 개선, 로봇 공학 장면 그래프(robotics scene graph) 또는 기업 데이터와 같은 전문화된 그래프에 에이전트를 적용하는 것이 포함됩니다. 지식 그래프 기반 추론은 LLM의 환각(hallucination)을 줄이고, 최신 정보를 반영하며, 답변의 설명 가능성을 높이는 데 큰 이점을 제공합니다. 특히 롱테일 개체 처리는 전문 분야나 개인화된 정보 검색에서 매우 중요합니다. ARK-V1의 약점들을 극복하기 위한 연구는 모호한 질문 처리, 지식 그래프의 충돌 해결, 그리고 상식 부족을 보완하는 방법을 포함할 것입니다. 이는 지식 기반 QA 시스템의 발전에 중요한 기여를 할 것으로 예상됩니다. [논문](Paper) | [트윗](Tweet)

7.  **사고를 통해 개선된 대화형 언어 모델**: 모델 보상 사고(Model-rewarded Thinking) 기반 강화 학습(RLMT)이라는 단순한 기법은 소규모 오픈 모델이 일반적인 대화 프롬프트(prompt)에 대해 "먼저 사고하고 나중에 응답"하도록 유도하며, 선호도 보상(preference reward)을 기반으로 온라인 강화 학습(RL)을 통해 훈련됩니다. Llama-3.1-8B와 Qwen-2.5-7B 모델에 적용했을 때, 이 방법은 대화, 창작 글쓰기, 일반 상식 영역에서 기존 RLHF(인간 피드백 기반 강화 학습) 방식을 꾸준히 뛰어넘는 성능을 보였고, 8B급 모델 중 최고는 WildBench와 AlpacaEval2에서 일부 최신 시스템보다 우수한 결과를 달성했습니다. 새로운 점: 규칙으로 검증 가능한 보상(수학, 코드) 대신, RLMT는 다양한 실제 프롬프트에 대한 긴 CoT(사고의 사슬)를 사용하며, 온라인 RL(GRPO, PPO, DPO)로 훈련된 보상 모델(Skywork)을 추가하여 출력을 평가합니다. 설정: 교사가 생성한 생각→응답 추적(think→respond trace)에 대한 작은 SFT(지도 미세 조정)로 웜 스타트(warm-start)한 다음, 약 7,500개의 WildChat-IF 프롬프트에 대해 GRPO로 최적화합니다. "제로(Zero)" 변형은 SFT(지도 미세 조정)를 건너뛰고도 기본 모델이 답변 전에 생각 태그(think tag)를 출력하도록 프롬프트(prompt)를 제공함으로써 작동합니다. 결과 요약: RLMT는 일치하는 RLHF 기준선(baseline) 대비 채팅 점수를 약 3~8점 향상시킵니다. 표 1은 Llama-3.1-8B-Instruct-RLMT가 WildBench에서 50.4, AlpacaEval2에서 58.7, ArenaHardV2에서 22.9, CreativeWritingV3에서 84.3을 기록했다고 보고하며, 이는 훨씬 더 큰 오픈 모델을 능가하고 WildBench에서 GPT-4o를 이깁니다. SFT 없는 기본 모델: GRPO를 사용하면 RLMT-Zero는 약한 기준선에서 채팅 능력을 현저히 향상시킵니다. Qwen-2.5-7B-RLMT-Zero는 평균 채팅 지표에서 공급업체의 Instruct 모델을 능가합니다. 작동 원리(및 중요한 점): 제거 연구(ablation study) 결과, 프롬프트 혼합의 품질과 보상 모델(reward model)의 강력함이 핵심 요소임이 밝혀졌습니다 (WildChat-IF와 Skywork-V2가 가장 효과적이었습니다). RL(강화 학습) 후, 모델은 다르게 계획합니다: 선형 체크리스트는 줄어들고, 제약 조건 열거, 테마 그룹화, 반복적 개선이 늘어납니다. 훈련을 거치면서 CoT(사고의 사슬)와 응답이 길어집니다. RLMT의 핵심 아이디어인 "먼저 사고하고 나중에 응답"은 LLM이 단순히 텍스트를 생성하는 것을 넘어, 보다 깊이 있는 추론 과정을 내재화하도록 돕는 중요한 발전입니다. 온라인 강화 학습과 보상 모델의 활용은 모델이 동적으로 학습하고 인간의 선호도를 더욱 정확하게 반영할 수 있게 합니다. 소규모 오픈 모델의 성능 향상은 자원 효율성과 AI 기술의 접근성을 높이는 데 기여하며, 이는 AI 연구와 개발의 민주화에 중요한 의미를 가집니다. [논문](Paper) | [트윗](Tweet)

8.  **체화된 AI: LLM에서 월드 모델까지**: 이 연구는 대규모 언어 모델(LLM)과 월드 모델(World Model, WM)의 시각에서 체화된 인공지능(Embodied AI)의 본질을 탐구합니다. 이 논문은 LLM이 의미론적 추론(semantic reasoning)과 작업 분해(task decomposition)를 어떻게 가능하게 하는지 강조하며, WM은 예측적이고 물리 기반의 상호작용을 제공합니다. 그리고 실제 체화된 인지(embodied cognition) 및 응용 분야를 발전시키기 위한 공동 MLLM-WM 아키텍처(architecture)를 주장합니다. 체화된 AI는 물리적 세계와의 상호작용을 통해 학습하고 행동하는 AI 시스템을 의미하며, 이는 자율 로봇이나 가상 현실 에이전트와 같은 분야에서 핵심적인 역할을 합니다. LLM과 WM의 상호 보완적인 통합은 AI가 복잡한 환경을 이해하고 능동적으로 대처하는 능력을 획기적으로 향상시킬 수 있는 미래 AI 시스템의 모습을 제시합니다. [논문](Paper) | [트윗](Tweet)

9.  **GDPval**은 9개 주요 GDP 부문의 44개 직업에 걸쳐 총 1,320개의 실제 업무로 구성된 새로운 평가 기준(benchmark)입니다. 이 중 220개 작업으로 이루어진 골드 세트(gold set)는 산업 전문가들에 의해 검증되었습니다. 이 벤치마크는 최신 모델들의 성능이 거의 선형적으로 향상되고 있으며, 전문가 수준에 가까워지고 있음을 입증합니다. Claude Opus 4.1은 47.6%의 경우 선호되거나 동등했으며, GPT-5는 정확도에서 앞섰습니다. 모델과 인간이 결합된 워크플로우(workflow)는 시간과 비용을 줄일 수 있으며, 추론 노력과 프롬프트 스캐폴딩(prompt scaffolding)을 추가하면 점수가 더욱 높아집니다. 연구자들을 위해 공개 골드 세트(gold set)와 자동 채점기(automated grader)가 제공됩니다. GDPval 벤치마크는 AI 모델의 실제 경제 활동 관련 성능을 측정하는 데 중요한 도구이며, 산업별 AI 응용의 잠재력을 평가하는 데 기여합니다. 모델과 인간의 협업 워크플로우는 AI의 효율성을 극대화하고 인간의 역량을 보완하는 시너지를 창출할 수 있음을 보여줍니다. 이러한 벤치마크는 AI가 경제 전반에 미칠 긍정적인 영향을 예측하는 데 필수적입니다. [논문](Paper) | [트윗](Tweet)

10. **ASAL**은 시각-언어 기반 파운데이션 모델(vision-language foundation model)을 활용하여 ALife(인공 생명체) 환경 전체를 자동 탐색함으로써, 프롬프트(prompt)에 부합하거나, 개방형 참신성(open-ended novelty)을 유지하거나, 다양성을 극대화하는 시뮬레이션을 찾아내어 수동적인 시행착오를 감소시킵니다. 이 시스템은 강력한 개방형 특성(open-endedness)을 지닌 새로운 레니아(Lenia) 및 보이드(Boids) 생명체, 그리고 생체 모방 셀룰러 오토마타(CA)를 발견하며, 파운데이션 모델 임베딩(embedding)을 이용해 기판에 독립적인 방식으로 출현 행동(emergent behavior)을 정량적으로 분석합니다. 인공 생명체(ALife) 연구는 복잡계 시스템을 이해하고 자연에서 영감을 받은 새로운 디자인 원리를 탐색하는 데 중요합니다. ASAL의 자동화된 탐색은 ALife 연구의 속도를 가속화하고, 기존에는 발견하기 어려웠던 미지의 영역을 탐색할 수 있도록 돕습니다. 파운데이션 모델 임베딩을 통한 출현 행동의 정량화는 과학적 분석의 깊이를 더하며, 약물 발견, 재료 과학, 로봇 설계 등 다양한 분야에 응용될 잠재력을 가지고 있습니다. [논문](Paper) | [트윗](Tweet)