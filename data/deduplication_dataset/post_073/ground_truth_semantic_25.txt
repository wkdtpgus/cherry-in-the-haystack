1.  **ARE 메탈 슈퍼인텔리전스 랩스(ARE Metal SuperIntelligence Labs)**는 실제와 같은 환경에서 에이전트 시스템을 구축하고 그 성능을 검증하기 위한 연구 기반과 평가 기준을 제시합니다. 본 연구는 소음이 많고 변화무쌍한 환경에서 비동기적 사건, 기록 활동의 검증, 그리고 다중 에이전트 간의 협업을 중점적으로 다루는 모듈형 시뮬레이션 환경(ARE)과 모바일 기기 형태의 벤치마크(Gaia2)를 소개합니다. 이 플랫폼의 핵심 특징은 다음과 같습니다: ARE는 주변 환경을 애플리케이션, 이벤트, 알림, 시나리오 등으로 상세하게 모형화하며, 에이전트가 추론하는 중에도 시간은 계속 흐르도록 설계되었습니다. 의존성 관리는 DAG 스케줄러가 담당하며, 에이전트들은 도구와 비동기 알림 대기열(async notification queue)을 통해 상호 소통합니다. Gaia2 벤치마크는 스마트폰과 유사한 생태계 속에서 이메일, 채팅, 캘린더, 쇼핑 등 다양한 앱에 걸쳐 101가지 도구를 활용하는 1,120개의 검증 가능한 상황들을 포함합니다. 이 시나리오들은 탐색(Search), 실행(Execution), 적응성(Adaptability), 시간(Time), 모호성(Ambiguity), 에이전트 간 상호작용(Agent-to-Agent)이라는 여섯 가지 역량을 측정하는 데 초점을 맞춥니다. 검증기 설계: 평가 방식은 에이전트가 수행한 기록 활동(write action)의 순서를 모범 답안과 비교하며, 식별자(ID)와 같은 명확한 인수에 대한 정밀 검사와 내용에 대한 대규모 언어 모델(LLM)의 유연한 판단을 결합합니다. 이는 인과 관계와 시간적 순서를 확인하며, 다중 턴(multi-turn) 시나리오에서는 각 턴마다 진행됩니다. 주요 연구 결과 및 균형점: 어떤 단일 모델도 모든 역량에서 압도적인 성능을 보이지 않았으며, 투자 규모에 따른 성능 향상 곡선은 특정 지점에서 정체되는 경향을 나타냅니다. 1페이지의 차트에서는 pass@1과 최대 예산의 관계를 보여줍니다. 시간 제약 및 협업: 시간에 대한 압박은 심층적인 추론 정책(heavy-reasoning policy)이 다른 부분에서는 뛰어난 점수를 받음에도 불구하고, 시간적 제약으로 인해 중요한 순간을 놓치게 되는 역 스케일링 효과(inverse scaling effect)를 드러냅니다. 즉시 모드(instant mode)는 이러한 성능 격차를 완화하는 데 기여합니다. 에이전트 간 상호작용(Agent-to-Agent) 환경은 하위 목표 위임(sub-goal delegation)을 통해 가벼운 모델들에게 도움을 주지만, 가장 강력한 시스템에는 다양한 결과로 나타납니다. GUI는 이벤트 그래프 검토, 추적 재생, 코드 없는 시나리오 작성 기능을 제공합니다. 이러한 플랫폼은 복잡한 현실 세계에서 AI 에이전트의 안정성과 신뢰성을 확보하는 데 필수적인 기반을 제공합니다. 특히, 자율 주행, 로봇 공학, 스마트 도시 관리와 같이 동적인 환경에서 작동하는 AI 시스템의 개발에 중요한 통찰을 줄 수 있습니다. 향후 연구는 에이전트의 불확실성 처리 능력과 적대적 환경에서의 강건성을 더욱 강화하는 방향으로 진행될 수 있습니다. [논문](Paper) | [트윗](Tweet)

2.  **ATOKEN**은 이미지, 비디오, 3D 자산(asset) 등 다양한 데이터 유형에 모두 적용되는 단일 트랜스포머 기반의 토크나이저(tokenizer)입니다. 이 시스템은 모든 입력 정보를 4차원 회전 임베딩(4D RoPE)을 활용하여 공통의 희소 4차원 잠재 공간(sparse 4D latent space)으로 부호화하며, 적대적 학습(adversarial loss) 과정 없이 훈련되고, 연속형 및 이산형 토큰 모두를 지원합니다. 본 연구는 우수한 재구성 품질과 견고한 의미론적 정렬(semantic alignment)을 입증하며, 이는 여러 양식(modality)에 걸쳐 생성 및 이해 능력을 가능하게 합니다. 2차원 이미지, 동영상, 3차원 객체를 위한 하나의 잠재 표현 공간(latent space)을 사용합니다. 입력 자료는 희소한 (시간, x축, y축, z축) 특성으로 조각화(patchify)되는데, 예를 들어 이미지는 2차원 단면이고, 비디오는 시간에 대한 정보를 추가하며, 3D 객체는 다중 시점 렌더링(multiview render)에서 집계된 표면 복셀(surface voxel)을 활용합니다. 4D RoPE와 본래 해상도(native resolution)를 갖춘 순수 트랜스포머(Transformer) 구조를 채택합니다. 인코더(encoder)는 SigLIP2 비전 타워(vision tower)를 시공간 블록(space–time block)으로 확장하고 4D 회전 위치(rotary position)를 더하며, 디코더(decoder)는 트랜스포머 구조를 따라 픽셀 또는 3D 가우시안(Gaussian)을 재구성합니다. 기본 해상도(native resolution)와 KV 캐시된 시간 타일링(KV-cached temporal tiling) 기법은 비디오 추론(video inference) 속도를 향상시킵니다. 텍스처 통계(texture statistics)를 목표로 하는 적대적 손실 없는(adversarial-free) 훈련 방식을 사용합니다. GAN(생성적 적대 신경망) 대신, 손실 함수는 L1, LPIPS, CLIP 지각(perceptual) 및 그램 행렬(Gram-matrix) 항을 혼합하며, 이는 공분산(covariance)이 오류를 지배한다는 rFID 분해 결과에 의해 동기가 부여되었습니다. 다양한 양식(modality)에 걸친 점진적 학습 과정(progressive curriculum)을 거칩니다. 네 단계로 능력이 확장됩니다: 이미지 재구성, 비디오 추가, 3D 추가, 그리고 선택적인 FSQ 양자화(quantization)입니다. 종합적인 성능 결과: 연속 잠재 변수(continuous latent)를 사용하여 ATOKEN은 이미지 분야에서 0.21 rFID 및 82.2% ImageNet 제로샷 정확도(zero-shot accuracy), 비디오 분야에서 36.07 PSNR 및 3.01 rFVD, Toys4k 데이터셋에서 90.9%의 3D 분류 정확도를 가진 28.28 PSNR을 달성했습니다. 이산 FSQ 토큰(discrete FSQ token)은 경쟁력 있는 성능을 유지하면서 AR 생성 및 이미지-3D 변환을 가능하게 합니다. 이러한 통합된 토큰화 접근 방식은 범용 인공지능(AGI) 개발에 있어 중요한 진전을 의미하며, 다양한 형식의 데이터를 하나의 통일된 방식으로 처리할 수 있게 함으로써 멀티모달 학습의 효율성을 극대화합니다. 이는 미래의 AI 시스템이 더욱 유연하고 광범위한 실제 시나리오에 적용될 수 있는 기반을 마련합니다. [논문](Paper) | [트윗](Tweet)

3.  **코드 월드 모델(Code World Model)** Meta FAIR은 320억 개의 파라미터(32B)를 가진 공개 가중치(open-weights) 코더인 CWM을 공개했습니다. 이 모델은 코드 실행 과정을 모방하고 컨테이너(container) 환경 내에서 작동하도록 훈련되었습니다. 파이썬 인터프리터(Python interpreter)의 실행 추적(trace)과 에이전트 방식의 도커(Docker) 궤적(trajectory)으로 중간 훈련(mid-train)을 거친 후, 소프트웨어 엔지니어링(SWE), 코딩, 수학 분야 전반에 걸쳐 다중 턴(multi-turn) 강화 학습(RL)으로 성능이 향상되었습니다. CWM은 강력한 코딩 능력을 지닌 모델이자 소프트웨어 환경에서 월드 모델(world-model) 스타일 추론을 위한 실험 장치(testbed)입니다. 실행 인지 훈련 방식: 8조(8T) 토큰으로 사전 훈련(pretrain)을 진행한 다음, 컨테이너화된 저장소(repo)에서 수집된 파이썬 실행 추적(Python execution trace) 및 ForagerAgent 궤적(trajectory) 데이터를 활용하여 5조(5T) 토큰에 걸친 중간 훈련(mid-train)을 수행합니다. 이어서 지도 미세 조정(SFT, 100B)과 GRPO 스타일 알고리즘 및 비동기 롤아웃(asynchronous rollout)을 이용한 공동 다중 작업 강화 학습(RL)이 뒤따릅니다. 이 과정에서 1억 2천만 개(120M)의 추적된 함수, 약 7만 개(70k)의 저장소 수준 추적, 3백만 개(3M)의 에이전트형 궤적(trajectory) 데이터가 생성되었습니다. 모델 및 컨텍스트 확장: 조밀한 320억 개(32B) 디코더(decoder)는 교대하는 지역/전역 슬라이딩 윈도우 어텐션(local/global sliding-window attention)과 131k의 최대 컨텍스트(max context) 길이를 갖습니다. 확장된 RoPE, GQA, FP8 훈련, 그리고 긴 컨텍스트 버킷화(long-context bucketization) 기법이 처리량(throughput) 유지를 위해 적용되었습니다. 추론(inference) 시에는 양자화(quantization)를 통해 단일 80GB H100 그래픽 카드에도 탑재 가능합니다. SWE를 위한 에이전트형 RL 설계: 에이전트는 최소한의 도구 세트(bash, edit, create, submit)를 갖춘 저장소 샌드박스(repo sandbox) 내에서 작동하며, 테스트를 실행하고, `git diff`를 사용하여 패치(patch)를 생성하고, 숨겨진 테스트와 패치 유사성 형성(patch-similarity shaping)으로 보상을 받습니다. 자체적으로 생성된 추적(self-bootstrapped trace)은 강화 학습(RL) 이전에 형식 준수(format adherence)를 향상시킵니다. 성능의 주요 특징: SWE-bench Verified에서 53.9%의 기본 pass@1을 달성했으며, 테스트 시간 스케일링(test-time scaling, best@k)을 적용했을 때는 65.8%에 이르렀습니다. 3페이지의 차트는 CWM이 훨씬 더 크거나 독점적인 모델들과 경쟁할 수 있음을 보여줍니다. 또한 LCB-v5에서 68.6, Math-500에서 96.6, AIME-24에서 76.0, CruxEval-Output에서 94.3의 높은 점수를 기록했습니다. AI 개발자에게 중요한 의미: CWM은 프롬프트(prompt)에서 파이썬 실행을 시뮬레이션하기 위해 추적 예측 토큰(trace-prediction token)을 노출하여, 근거 있는 추론, 신경 디버거(neural-debugger) 워크플로우, 추적 기반 코드 합성(trace-guided code synthesis)을 가능하게 합니다. 절제 연구(ablation study) 결과에 따르면 실행 추적(execution trace)은 CruxEval 성능을 향상시키고, ForagerAgent는 에이전트형 음의 로그 우도(NLL) 및 SWE pass@1을 개선하는 것으로 나타났습니다. 이 모델은 단순한 코드 생성기를 넘어, 실제 소프트웨어 개발 환경에서 자율적으로 문제를 해결하고 학습하는 AI 에이전트의 가능성을 제시합니다. [논문](Paper) | [트윗](Tweet)

4.  **LLM에게 계획하는 법 가르치기**: 대규모 언어 모델(LLM)에 PDDL(Planning Domain Definition Language) 기반의 계획 수립 능력을 부여하는 훈련 방식은 모델이 명시적인 상태-행동-상태(state–action–state) 연쇄를 구성하게 하고, 외부 검증기(VAL)를 통해 각 단계를 확인하도록 하는 과정을 통해 이루어집니다. 결과: PlanBench 도메인에서 계획의 유효성(plan validity)이 크게 향상되었으며, 특히 피드백이 단순히 실패 여부를 알리는 것을 넘어 행동이 실패한 구체적인 이유를 설명할 때 더욱 두드러졌습니다. 방법 요약: 두 단계로 구성됩니다: (1) 전제 조건(precondition) 및 효과(effect)에 대한 설명과 함께 올바른 계획과 의도적으로 오류가 포함된 계획에 대한 지시 튜닝(instruction tuning)을 수행합니다. (2) 모델이 VAL이 단계별로 검증하는 ⟨s₀,a₁,s₁⟩… 형태의 연쇄를 출력하도록 CoT(사고의 사슬) 지시 튜닝을 적용합니다. 훈련은 추론 연쇄(reasoning chain) 최적화와 최종 계획 성공 최적화 사이를 번갈아 진행합니다. 작동 원리: 검증기는 각 단계에서 논리적 일관성(logical coherence)을 강제함으로써, 모델이 단순히 패턴을 인식하는 대신 전제 조건을 확인하고, 효과를 적용하며, 불변량(invariant)을 보존하는 방법을 학습하게 합니다. 모든 전환이 외부에서 검증되므로, 신뢰할 수 없거나 모호한 CoT(사고의 사슬)의 발생이 줄어듭니다. 연구 결과: Llama-3 모델을 사용하여 상세한 피드백과 15회 반복 훈련을 거친 결과, Blocksworld에서 94%, Logistics에서 79%, Mystery Blocksworld에서 64%의 계획 유효성(plan validity)을 달성했습니다. GPT-4 또한 유사한 경향을 보이며 각각 91%, 78%, 59%로 최고치를 기록했습니다. 이는 기준선(baseline) 대비 상당한 절대적 개선 폭을 나타내며, 일부 설정에서는 최대 +66%에 달했습니다. 피드백의 중요성: 상세한 피드백(어떤 전제 조건이 실패했는지 또는 어떤 효과가 잘못 적용되었는지 등)은 이진 유효/무효(binary valid/invalid) 피드백보다 지속적으로 우수했으며, 추가적인 반복(η가 10에서 15로 증가)을 통해 더 많은 이점을 얻었습니다. 범위 및 한계: 세 가지 PlanBench 도메인에서 훈련 및 테스트되었습니다. 난독화된 술어(obfuscated-predicate) 변형(Mystery Blocksworld)에서는 성능 저하가 관찰되어 일반화(generalization)의 어려움을 시사합니다. 이 방법은 최적의 계획(optimality)보다는 만족스러운 계획(satisficing plan)을 목표로 하며, 현재는 지속 시간(durative) 또는 조건문(conditional)이 없는 PDDL 하위 집합을 가정합니다. 이 기술은 LLM이 단순히 텍스트를 생성하는 것을 넘어, 복잡한 문제 해결을 위한 논리적 추론 능력을 갖추도록 돕는 중요한 단계를 제시합니다. 이는 로봇 공학, 자율 시스템, 물류 관리와 같은 분야에서 AI의 적용 가능성을 크게 확장할 수 있습니다. [논문](Paper) | [트윗](Tweet)

5.  **LLM-JEPA**: JEPA 스타일의 훈련 목표(training objective)를 대규모 언어 모델(LLM)에 적용한 LLM-JEPA는 동일한 근본 콘텐츠(예: 텍스트와 코드)의 짝을 이루는 관점(paired view)들을 임베딩 공간(embedding space)에서의 예측 대상(prediction target)으로 취급하여, 일반적인 다음 토큰 손실(next-token loss) 위에 추가됩니다. 그 결과는 미세 조정(fine-tuning) 성능의 지속적인 개선과 유망한 사전 훈련(pretraining) 이점을 보여주며, 과적합(overfitting)에 더욱 강건합니다. 한 줄 요약: 표준적인 다음 토큰 목표(next-token objective)를 유지하면서, 특수 예측 토큰 k를 가진 연결된 LLM 가중치(tied LLM weight)를 활용하여 한 관점(view)의 임베딩(embedding)을 다른 관점으로부터 예측하고, 코사인 측정(cosine metric)과 가중치 λ로 최적화되는 JEPA 항을 추가합니다. 이는 생성(generation) 능력을 보존하면서 추상화(abstraction) 능력을 향상시킵니다. 효과적인 이유: 오직 다음 토큰 손실(next-token loss)만을 최소화하는 것은 JEPA 예측 오류를 줄이는 데 직접적으로 기여하지 못합니다. JEPA 항을 추가함으로써 이러한 간극을 메우고, 결과적으로 정확도 향상을 설명할 수 있습니다. 주요 성과: Llama, Gemma, OpenELM, OLMo 등 다양한 모델에서 LLM-JEPA는 NL-RX (SYNTH 및 TURK), GSM8K, Spider 벤치마크에서 정확 일치 정확도(exact-match accuracy)를 개선했습니다. 표현 방식의 영향: LLM-JEPA를 사용할 때 t-SNE 플롯(plot)은 더욱 명확한 구조를 보여주며, 낮은 회귀 오류(regression error)와 압축된 특이값(singular value)에 의해 Enc(Text)에서 Enc(Code)로의 거의 선형적인 매핑(mapping)이 뒷받침됩니다. 사전 훈련 신호 및 비용: 사전 훈련(pretraining) 과정에서 JEPA를 추가하면 표준 미세 조정(fine-tuning) 이후 다운스트림 감성 분류(downstream sentiment classification) 성능이 향상되며, 생성 품질(generative quality)은 유지됩니다. 현재의 한계점으로는 각 관점(view)에 대한 별도의 순방향 전달(forward pass)로 인한 추가적인 계산량과, k와 λ에 대한 비자명한 하이퍼파라미터 탐색(hyperparameter sweep)이 있습니다. 이 연구는 LLM이 단순히 통계적 패턴을 학습하는 것을 넘어, 데이터의 근본적인 구조와 관계를 더 깊이 이해하도록 돕는 자기 지도 학습(self-supervised learning)의 새로운 방향을 제시합니다. 이는 미래의 AI가 더 적은 데이터로도 견고한 학습을 수행하고, 다양한 도메인에 걸쳐 일반화될 수 있는 기반을 마련합니다. [논문](Paper) | [트윗](Tweet)

6.  **ARK-V1**은 경량화된 에이전트로, 언어 모델이 암기된 텍스트에만 의존하는 대신 지식 그래프(knowledge graph)를 적극적으로 탐색하여 질문에 답하도록 돕습니다. 이는 특히 사전 훈련(pretraining) 지식이 부족한 희귀 개체(long-tail entity, 덜 일반적인 개념)에 대해 유용합니다. 작동 방식 – 에이전트는 간단한 반복 주기를 따릅니다: 시작 개체(entity)를 선정하고, 관계(relation)를 선택한 다음, 일치하는 그래프 트리플(graph triple)을 가져오고, 짧은 추론 단계(reasoning step)를 작성한 후, 답변할 준비가 될 때까지 이 과정을 되풀이합니다. 마치 경로를 따라 이동하는 미니 탐색 에이전트(search agent)처럼 생각할 수 있습니다. 테스트 – 연구팀은 CoLoTa 데이터셋을 활용했는데, 이 데이터셋은 의도적으로 흔치 않은 개체(entity)에 대한 질문을 포함하여, 지식 그래프(KG)의 사실과 상식(예: 잘 알려지지 않은 도시의 인구 비교)을 모두 요구합니다. 평가 지표에는 에이전트가 얼마나 자주 답변하는지, 답변의 정확성, 그리고 여러 실행에서의 일관성이 포함되었습니다. 성능 – ARK-V1은 일반적인 CoT(사고의 사슬) 프롬프팅(prompting)보다 우수한 결과를 보였습니다. 중간 규모 모델인 Qwen3-30B를 사용했을 때, 약 77%의 질문에 응답했으며, 이 중 약 91%가 정확하여 전체적으로 약 70%의 성능을 나타냈습니다. 더 큰 백본(backbone) 모델(Qwen3-235B, Gemini 2.5 Flash, GPT-5 Mini)은 조건부 정확도(conditional accuracy) 94% 이상으로 전체적으로 약 70~74%를 달성했습니다. 약점 – (1) 질문이 모호하거나, (2) 지식 그래프(KG)에 상충되는 트리플(triple)이 포함되어 있거나, (3) 지식 그래프에 필요한 상식 정보가 부족하여 에이전트가 그래프에 지나치게 의존할 때 어려움을 겪습니다. 향후 방향 – 현재의 프롬프팅(prompting) 방식은 단순하며, 그래프 탐색(traversal)은 비효율적일 수 있습니다. 다음 단계에는 더 정교한 프롬프트(prompt) 개발, 효율성 개선, 그리고 로봇 공학 장면 그래프(robotics scene graph)나 기업 데이터와 같은 전문화된 그래프에 에이전트를 적용하는 것이 포함됩니다. 이 접근 방식은 LLM이 실제 세계의 지식에 더 깊이 뿌리내리도록 돕고, 환각(hallucination) 문제를 줄이며, 특정 도메인에서 더욱 신뢰할 수 있는 답변을 제공하는 데 기여할 수 있습니다. [논문](Paper) | [트윗](Tweet)

7.  **생각하고 더 잘 채팅하는 언어 모델**: 모델 보상 사고(Model-rewarded Thinking)를 활용한 강화 학습(RLMT)이라는 간결한 방법론은 소형 공개 모델들이 일반적인 채팅 요청에 대해 "먼저 계획하고 나중에 응답"하도록 만들며, 선호도 보상(preference reward)을 기반으로 온라인 RL 훈련을 수행합니다. Llama-3.1-8B 및 Qwen-2.5-7B 모델에 적용했을 때, 이는 채팅, 창의적 글쓰기, 일반 지식 분야에서 기존의 표준 RLHF(인간 피드백 기반 강화 학습)를 지속적으로 능가했으며, 최고의 8B 모델은 WildBench 및 AlpacaEval2에서 일부 최첨단 시스템들을 앞질렀습니다. 새로운 점: 규칙으로 검증 가능한 보상(수학, 코드) 대신, RLMT는 다양한 실제 프롬프트(prompt)에 대한 긴 CoT(사고의 사슬)를 사용하며, 온라인 RL(GRPO, PPO, DPO)로 훈련된 보상 모델(Skywork)을 추가하여 출력물을 평가합니다. 설정: 교사가 생성한 생각→응답 추적(think→respond trace) 데이터에 대한 소규모 지도 미세 조정(SFT)으로 초기 학습을 시작한 후, 약 7,500개의 WildChat-IF 프롬프트(prompt)에 대해 GRPO로 최적화합니다. "제로(Zero)" 변형은 SFT(지도 미세 조정) 단계를 건너뛰고도 기본 모델이 답변하기 전에 생각 태그(think tag)를 출력하도록 프롬프트(prompt)를 제공함으로써 작동합니다. 결과 요약: RLMT는 기존 RLHF 기준선(baseline) 대비 채팅 점수를 약 3~8점 향상시킵니다. 표 1은 Llama-3.1-8B-Instruct-RLMT가 WildBench에서 50.4, AlpacaEval2에서 58.7, ArenaHardV2에서 22.9, CreativeWritingV3에서 84.3을 기록했다고 보고하며, 이는 훨씬 더 큰 공개 모델들을 능가하고 WildBench에서는 GPT-4o를 이기는 결과입니다. SFT 없는 기본 모델: GRPO를 사용하면 RLMT-Zero는 약한 기준선(baseline)에서도 채팅 능력을 현저히 개선합니다. Qwen-2.5-7B-RLMT-Zero는 평균 채팅 지표에서 공급업체의 Instruct 모델을 능가합니다. 작동 원리(및 중요한 점): 절제 연구(ablation study)는 프롬프트 혼합 품질과 보상 모델(reward model)의 강도가 핵심적임을 보여줍니다 (WildChat-IF 및 Skywork-V2가 최적의 조합). 강화 학습(RL) 후, 모델은 다른 방식으로 계획합니다: 선형적인 체크리스트는 줄어들고, 제약 조건 열거, 테마 그룹화, 반복적 개선이 늘어납니다. 훈련을 거치면서 CoT(사고의 사슬)와 응답의 길이가 증가합니다. 이 연구는 소규모 모델도 효과적인 내부 추론 과정을 통해 복잡한 대화와 문제 해결 능력을 향상시킬 수 있음을 입증합니다. 이는 AI 비서, 교육용 챗봇 등 다양한 응용 분야에서 사용자 경험을 혁신할 잠재력을 가지고 있습니다. [논문](Paper) | [트윗](Tweet)

8.  **체화된 AI: LLM에서 월드 모델까지**: 본 연구는 대규모 언어 모델(LLM)과 월드 모델(World Model, WM)의 관점에서 체화된 인공지능(Embodied AI) 분야를 깊이 있게 탐구합니다. 이 논문은 LLM이 의미론적 추론(semantic reasoning) 및 작업 분해(task decomposition)를 어떻게 가능하게 하는지 강조하는 한편, WM은 예측적이고 물리 기반의 상호작용을 제공하는 역할을 수행함을 밝힙니다. 나아가, 실제 체화된 인지(embodied cognition) 및 응용 분야의 발전을 위해 다중모달 LLM과 월드 모델(MLLM-WM)이 결합된 아키텍처(architecture)의 필요성을 주장합니다. 체화된 AI는 물리적 세계에서 지능형 에이전트가 상호작용하며 학습하는 분야로, 오랜 기간 연구되어 왔습니다. 전통적인 AI는 주로 추상적인 문제 해결에 집중했지만, 체화된 AI는 실제 환경의 복잡성과 불확실성을 다루는 데 필수적입니다. LLM은 자연어 이해와 생성 능력을 통해 고수준의 계획과 추론을 제공하지만, 물리적 현실에 대한 직접적인 이해는 부족합니다. 반면, 월드 모델은 환경의 동역학을 예측하고 시뮬레이션함으로써 에이전트가 행동의 결과를 미리 예측하고 학습할 수 있도록 돕습니다. 이 두 기술의 통합은 에이전트가 언어적 지시를 물리적 행동으로 전환하고, 실제 세계의 피드백을 통해 학습하며, 복잡한 작업을 자율적으로 수행하는 데 결정적인 역할을 할 것입니다. 이는 로봇 공학, 가상 현실, 자율 시스템과 같은 분야에 혁명적인 변화를 가져올 잠재력을 가지고 있습니다. [논문](Paper) | [트윗](Tweet)

9.  **GDPval**은 9개 주요 GDP 산업 부문의 44개 직업에 걸쳐 총 1,320개의 실제 업무로 구성된 새로운 벤치마크(benchmark)입니다. 이 평가는 220개 작업으로 이루어진 골드 세트(gold set)를 통해 산업 전문가들에 의해 진행되었습니다. 연구 결과는 최첨단 모델들이 거의 선형적인 개선을 보이며 전문가 수준에 근접하고 있음을 보여줍니다. Claude Opus 4.1은 47.6%의 경우에 선호되거나 동등한 성능을 보였으며, GPT-5는 정확도 측면에서 앞섰습니다. 인간과 모델이 결합된 작업 흐름(workflow)은 시간과 비용을 절감할 수 있으며, 추론 노력과 프롬프트 스캐폴딩(prompt scaffolding)을 추가하면 점수가 더욱 높아집니다. 연구자들을 위해 공개 골드 세트(gold set)와 자동 채점기(automated grader)가 제공됩니다. 이 벤치마크는 AI 모델의 실제 업무 적용 가능성을 측정하는 데 중요한 기준을 제시하며, 단순히 기술적 성능을 넘어 경제적 가치 창출에 기여할 수 있는 AI의 잠재력을 평가합니다. AI가 다양한 산업 분야에서 전문가 수준의 작업을 수행할 수 있게 됨에 따라, 노동 시장과 기업 운영 방식에 대한 광범위한 영향이 예상됩니다. 이는 생산성 향상과 새로운 서비스 개발로 이어질 수 있지만, 동시에 직무 변화와 재교육의 필요성 등 사회적 과제도 야기할 것입니다. [논문](Paper) | [트윗](Tweet)

10. **파운데이션 모델로 인공 생명체 탐색 자동화**: ASAL은 시각-언어 파운데이션 모델(vision-language foundation model)을 활용하여 인공 생명체(ALife) 기판(substrate) 전반을 자동으로 탐색함으로써, 프롬프트(prompt)와 일치하거나, 개방형 참신성(open-ended novelty)을 유지하거나, 다양성을 극대화하는 시뮬레이션을 찾아내 수동적인 시행착오를 줄입니다. 이 시스템은 강력한 개방형 특성(open-endedness)을 가진 새로운 레니아(Lenia) 및 보이드(Boids) 생명체와 생명체와 유사한 셀룰러 오토마타(CA)를 발견하며, 파운데이션 모델(FM) 임베딩(embedding)을 활용하여 기판에 구애받지 않는 방식으로 출현 행동(emergent behavior)을 정량화합니다. 인공 생명체 연구는 생명의 본질을 이해하고 복잡계의 동역학을 탐구하는 데 중요합니다. 그러나 광대한 파라미터 공간 때문에 새로운 생명체 형태나 행동을 발견하는 데 많은 시간과 노력이 필요했습니다. ASAL은 이러한 탐색 과정을 자동화하여, 연구자들이 수동으로 시나리오를 설정하고 결과를 분석하는 부담을 덜어줍니다. 파운데이션 모델의 강력한 표현 학습 능력은 추상적인 생명체 시뮬레이션의 특성을 효과적으로 파악하고, 이를 기반으로 새로운 형태와 규칙을 제안하는 데 활용됩니다. 이는 생물학적 시스템 모델링, 진화 알고리즘 개발, 심지어 예술 및 디자인 분야에까지 혁신적인 영감을 줄 수 있습니다. [논문](Paper) | [트윗](Tweet)