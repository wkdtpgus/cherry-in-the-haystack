누가 AI 과장 광고를 만들어내는가? 저희의 **AI 스네이크 오일(AI Snake Oil)** 책에서 논의했듯이, 기업과 언론뿐만 아니라 AI 연구자들도 그렇습니다. 예를 들어, 2023년 12월 네이처(Nature)지에 발표되어 널리 알려진 두 편의 논문은 AI를 사용하여 220만 개 이상의 새로운 물질을 발견하고 그중 41개를 로봇으로 합성했다고 주장했습니다. 안타깝게도, 이러한 주장은 빠르게 반박되었습니다. "생산된 [41개] 물질 대부분은 잘못 식별되었으며, 나머지는 이미 알려진 것이었습니다." 대규모 데이터셋(dataset)의 경우, 250개 화합물 샘플(sample)을 조사한 결과 대부분이 쓸모없는 데이터(junk)임이 드러났습니다. 기계 학습(machine learning)의 핵심 강점 중 하나는 이해 없이 발견하는 것인데, 이것이 기계 학습 기반 과학에서 오류가 특히 흔한 이유입니다. 3년 전, 저희는 시험을 위한 학습(teaching to the test)의 기계 학습(machine learning) 버전인 누출(leakage)이라는 오류가 만연하여 17개 분야의 수백 편의 논문에 영향을 미쳤다는 증거를 수집했습니다. 그 이후로 저희는 이 문제를 더 잘 이해하고 해결책을 고안하기 위해 노력해 왔습니다. 이 게시물은 업데이트된 내용을 제공합니다. 요컨대, 저희는 상황이 나아지기 전에 더 나빠질 것이라고 생각하지만, 희망의 빛도 보입니다.

**AI 과장 광고의 새로운 얼굴**
AI 과장 광고의 그림자는 더욱 짙어지고 있습니다. 특히 대규모 언어 모델(Large Language Models, LLM)과 생성형 AI(Generative AI)의 등장은 과학 연구의 풍경을 급변시키며 새로운 형태의 과장 광고와 함께 예상치 못한 함정들을 만들어내고 있습니다. 예를 들어, 최근에는 LLM이 방대한 과학 문헌을 학습하여 새로운 가설을 자동으로 생성하거나, 복잡한 실험 설계 아이디어를 제안함으로써 연구 개발 주기를 획기적으로 단축할 수 있다는 주장이 널리 퍼지고 있습니다. 그러나 이러한 주장의 이면에는 LLM의 '환각(hallucination)' 문제, 즉 사실이 아닌 정보를 그럴듯하게 생성하는 경향이 숨어있습니다. 데이터 오염(data poisoning)이나 편향된 학습 데이터로 인한 위험도 간과할 수 없습니다. AI가 '이해의 환상(illusion of understanding)'을 제공하며, 과학자들이 데이터의 패턴을 넘어선 진정한 통찰을 얻었다고 착각하게 만드는 것입니다.

참사는 계속됩니다.
저희의 가장 최근 자료 취합에 따르면, 연구자들이 출판된 연구에서 누출(leakage)을 발견한 분야의 수는 30개에 달했습니다. 대다수는 의학 분야인데, 이는 의학 연구의 오류가 특히 중대한 결과를 초래할 수 있기 때문에 의학 분야가 모범 사례를 확립하고 이전에 출판된 연구를 비판적으로 검토하는 데 훨씬 더 많은 노력을 기울이는 경향이 있기 때문이라고 강력히 추정합니다. 모든 분야에 걸쳐 약 650편의 논문이 영향을 받았는데, 이는 엄청나게 과소평가된 수치라고 저희는 가정합니다. 연구자들이 체계적으로 누출(leakage)을 찾을 때, 많은 분야에서 샘플(sample)로 추출된 연구의 대다수가 누출(leakage) 오류를 범하고 있음을 발견하기 때문입니다. 누출(leakage)은 재현성(reproducibility) 실패의 여러 원인 중 하나입니다. 기계 학습(ML) 기반 과학의 모든 단계, 즉 데이터(data) 수집부터 전처리(preprocessing) 및 결과 보고에 이르기까지 광범위한 결함이 존재합니다. 재현 불가능성(irreproducibility)으로 이어질 수 있는 문제로는 부적절한 기준선(baseline) 비교, 비대표적인 샘플(sample), 특정 모델링(modeling) 선택에 민감한 결과, 그리고 모델(model) 불확실성(uncertainty)을 보고하지 않는 것 등이 있습니다. 또한 연구자들이 자신의 코드(code)와 데이터(data)를 공개하지 않아 재현성(reproducibility)을 저해하는 근본적인 문제도 있습니다. 예를 들어, Gabelica 등은 2019년 1월 바이오메드 센트럴(BioMed Central)에 등재된 333개의 오픈 액세스(open-access) 저널을 조사한 결과, 요청 시 데이터를 공유하겠다고 약속한 1,800편의 논문 중 93%가 그렇게 하지 않았음을 발견했습니다.

**뿌리 깊은 문제**
기계 학습(ML) 이전에도 많은 과학 분야는 재현성(reproducibility) 및 반복성(replicability) 위기에 직면해 있었습니다. 근본 원인으로는 과학계의 '출판 아니면 도태(publish-or-perish)' 문화, 긍정적인 결과(positive results) 출판에 대한 강한 편향(그리고 부정적인 결과(negative results) 출판의 거의 불가능함), 잘못된 연구를 반박할 유인 부족, 그리고 부실한 연구를 출판해도 아무런 결과가 없는 점 등이 있습니다. 예를 들어, 잘못된 논문은 거의 철회되지 않습니다. 동료들은 반복 실패조차 인지하지 못하는 것 같습니다. 한 논문이 반복에 실패한 후, 인용하는 논문 중 단 3%만이 그 반복 시도를 인용했습니다. 1 과학 커뮤니케이터(communicator)들은 과학이 스스로 수정된다고 주장하기를 좋아하지만, 저희의 경험상 자기 수정은 사실상 존재하지 않습니다.

이러한 모든 문화적 요인들은 기계 학습(ML) 기반 과학에서도 나타납니다. 그러나 기계 학습(ML)은 출판된 결과에 대해 우리가 회의적이어야 하는 여러 가지 추가적인 이유를 제시합니다. 성능 평가(performance evaluation)는 악명 높게 까다로우며, 불확실성 정량화(uncertainty quantification)와 같은 많은 측면은 미해결 연구 분야입니다. 또한, 기계 학습(ML) 코드(code)는 전통적인 통계 모델링(statistical modeling)보다 훨씬 더 복잡하고 덜 표준화되는 경향이 있습니다. 동료 심사자(peer reviewer)의 역할이 코드(code)를 검토하는 것이 아니기 때문에 코딩(coding) 오류는 거의 발견되지 않습니다. 그러나 저희는 연구 품질이 낮은 가장 큰 이유가 만연한 과장 광고(hype)이며, 이는 연구자들 사이에서 회의적인 사고방식의 부족으로 이어지고, 이는 좋은 과학적 실천의 초석이 된다고 생각합니다. 저희는 연구자들이 지나치게 낙관적인 기대를 가지고 있고 그들의 기계 학습(ML) 모델(model)이 제대로 작동하지 않을 때, 그들은 자신들이 뭔가 잘못했다고 가정하고 모델(model)을 수정하지만, 사실은 예측 가능성(predictability)의 본질적인 한계에 부딪혔을 가능성을 강력히 고려해야 한다는 것을 관찰했습니다. 반대로, 모델(model)이 잘 작동할 때는 쉽게 믿는 경향이 있는데, 사실은 누출(leakage)이나 다른 결함에 대해 경계를 늦추지 않아야 합니다. 그리고 모델(model)이 예상보다 더 잘 작동하면, 그들은 인간이 생각할 수 없었던 데이터(data)의 패턴(pattern)을 발견했다고 가정하며, AI가 외계 지능이라는 신화는 이러한 설명을 쉽게 그럴듯하게 만듭니다.

이것은 피드백 루프(feedback loop)입니다. 지나친 낙관주의는 결함 있는 연구를 부추기고, 이는 해당 분야의 다른 연구자들에게 AI가 무엇을 할 수 있고 무엇을 할 수 없는지에 대해 잘못된 정보를 제공합니다. 사실, 저희는 좌절한 연구자들과의 개인적인 서신에서 이러한 극단적인 경우를 접했습니다. 결함 있는 연구가 수정되지 않기 때문에, "최첨단(state of the art)"을 능가하지 못하는 모델(model)을 초래하는 좋은 연구를 출판하는 것은 말 그대로 불가능해집니다. 도구가 강력하고 블랙박스(black-box)일수록 오류와 과신(overconfidence)의 가능성이 커집니다. 심리학, 의학 등에서의 반복성(replication) 위기는 평범한 옛 통계의 오용에서 비롯된 것이었습니다. 기계 학습(ML)이 상대적으로 얼마나 새로운지를 고려할 때, 저희는 기계 학습(ML) 기반 과학의 재현성(reproducibility) 위기가 나아지기 전에 한동안 더 악화될 것이라고 추측합니다.

**재현성 위기, 깊어지는 그림자**
기계 학습 기반 과학의 재현성 위기는 단순히 누출(leakage) 문제에 국한되지 않습니다. 실제 환경에서 발생하는 데이터 시프트(data shift)는 학습 데이터와 실제 배포 환경 데이터 간의 분포 차이로 인해 모델 성능이 급격히 저하될 수 있음을 의미합니다. 적대적 공격(adversarial attacks)은 미묘한 입력 데이터 변경만으로도 AI 모델이 잘못된 예측을 하도록 유도할 수 있으며, 이는 고위험 분야에서 심각한 결과를 초래합니다. 이러한 취약성은 모델의 견고성(robustness)과 신뢰성에 대한 근본적인 의문을 제기합니다.

더 나아가, 복잡한 기계 학습 파이프라인(ML pipeline) 자체도 재현성을 저해합니다. 데이터 수집부터 모델 배포에 이르는 전 과정에서 수많은 결정과 변경이 제대로 문서화되고 버전 관리되지 않으면 다른 연구자가 동일한 결과를 재현하기는 거의 불가능합니다. 대규모 모델과 데이터셋의 경우, 컴퓨팅 자원과 데이터 공유의 어려움 또한 재현성 확보를 더욱 어렵게 합니다. 실행 환경, 라이브러리 버전, 무작위 시드(random seed) 등 미묘한 차이로 인해 결과가 달라지는 경우가 허다하며, 이는 ML 시스템의 본질적인 복잡성에서 비롯되는 구조적인 문제입니다.

**신뢰할 수 있는 AI 과학을 위한 노력**
이러한 복잡성과 도전에도 불구하고, 신뢰할 수 있는 AI 과학을 구축하기 위한 노력은 다각도로 진행되고 있습니다. MLOps(Machine Learning Operations)는 모델 개발부터 운영에 이르는 전 과정을 자동화하고 표준화하여 재현성을 높이는 데 기여합니다. 데이터 버전 관리(data versioning), 모델 레지스트리(model registry), 실험 추적(experiment tracking) 도구들은 연구 과정의 투명성과 재현성을 확보하는 핵심적인 역할을 합니다.

또한, 모델 카드(Model Cards)와 데이터 시트(Datasheets for Datasets)와 같은 표준화된 문서화 프레임워크는 모델의 사용 목적, 성능, 잠재적 편향 등을 명확히 기술함으로써 투명성을 높이고 오용을 방지합니다. 학계와 산업계에서는 AI 모델의 공정성, 책임성, 투명성(Fairness, Accountability, Transparency, FAT)을 보장하기 위한 원칙과 가이드라인을 활발히 논의하고 있으며, 여러 연구 기금 기관들도 AI 연구 과제 심사 시 재현성 계획과 공개 과학(Open Science) 실천 여부를 중요한 평가 요소로 반영하기 시작했습니다. 이는 연구자들이 단순히 '최고의 성능'만을 추구하는 것을 넘어, '가장 신뢰할 수 있는 결과'를 내놓도록 유도하는 긍정적인 변화입니다.

저희 책에 대한 블로그(blog)인 **AI 스네이크 오일(AI Snake Oil)**을 읽고 계십니다. 새 게시물을 받으려면 구독하세요. 구독하기

**희망의 빛**
기계 학습(ML) 기반 과학의 한 가지 좋은 점은 일반적으로 사람을 대상으로 한 실험이 아니라 데이터 분석(data analysis)만을 포함한다는 것입니다. 따라서 다른 연구자들은 원칙적으로 논문의 코드(code)와 데이터(data)를 다운로드하여 보고된 결과를 재현할 수 있는지 확인할 수 있어야 합니다. 또한 코드(code)에서 오류나 문제가 있는 선택이 있는지 검토할 수도 있습니다. 이는 시간이 많이 걸리지만, 심리학이나 의학 연구를 반복하는 것보다는 훨씬 덜합니다. 심리학이나 의학 연구는 일반적으로 원본 연구만큼 비용이 많이 듭니다. 또 다른 좋은 점은 연구자들이 무엇을 주의해야 할지 안다면 대부분의 오류를 피할 수 있다는 것입니다. 대조적으로, 사전 등록(pre-registration)과 같은 통계 과학의 반복성(replication) 위기에 대한 완화책은 효과 면에서 훨씬 더 불규칙한 실적을 보입니다. 따라서 저희는 연구자들이 자신의 연구에 체계적으로 더 많은 주의를 기울이고 재현성(reproducibility) 연구에 인센티브(incentive)가 부여되는 문화 변화를 통해 이 문제가 크게 완화될 수 있다고 생각합니다. 기계 학습(ML) 방법론 커뮤니티(community)는 이미 공통 과제 방법(common task method, 수십 년 된 방법)과 재현성 챌린지(reproducibility challenge, 비교적 최근의 방법)를 통해 이러한 방향으로 나아가고 있지만, 기계 학습(ML) 기반 과학, 즉 각 분야에서 기계 학습(ML) 모델(model)을 사용하여 지식을 발전시키는 의학이나 심리학과 같은 분야에서는 아직 이러한 변화가 일어나지 않았습니다.

저희는 이를 바꾸기 위한 몇 가지 노력을 주도했습니다. 첫째, 저희의 누출(leakage) 관련 논문은 영향력을 가졌습니다. 연구자들이 모델(model)을 구축하는 방법과 누출(leakage)의 부재를 문서화하고 입증하는 데 사용되었습니다. 출판된 연구에서 누출(leakage)을 찾으려는 연구자들에 의해 사용되었습니다. 또한 누출(leakage) 연구의 중요성을 강조하고 분야별 가이드라인(guideline)을 마련하는 방법으로도 사용되었습니다. 누출(leakage) 외에도, 저희는 컴퓨터 과학, 데이터 과학, 사회 과학, 수학 및 생의학 연구 분야의 19명의 연구자 그룹을 이끌어 기계 학습(ML) 기반 과학을 위한 REFORMS 체크리스트(checklist)를 개발했습니다. 이는 기계 학습(ML) 기반 과학에서 흔히 발생하는 여덟 가지 함정을 연구자들이 파악하는 데 도움이 될 수 있는 32개 항목의 체크리스트(checklist)이며, 누출(leakage)은 그중 하나에 불과합니다. 이 체크리스트(checklist)는 최근 사이언스 어드밴시스(Science Advances)에 게재되었습니다. 물론 문화 변화가 없다면 체크리스트(checklist)만으로는 도움이 되지 않겠지만, 지금까지의 반응을 볼 때 저희는 조심스럽게 낙관하고 있습니다.

**결론**
저희의 핵심 메시지는 변함이 없습니다. AI는 과학자들에게 강력한 '도구(tool)'이지만, 결코 인간의 비판적 사고와 과학적 방법론을 대체할 수 없습니다. 최근 LLM과 생성형 AI의 발전은 놀랍지만, 이러한 기술이 만들어내는 '이해의 환상'에 빠지지 않도록 더욱 경계해야 합니다. AI는 신탁이 아니며, 그저 데이터의 패턴을 학습할 뿐입니다. 과학자들이 AI의 한계를 명확히 인지하고, 과장 광고(hype)에 휩쓸리지 않으며, 철저한 검증과 재현성을 추구하는 엄격한 태도를 유지하는 것이 그 어느 때보다 중요합니다.

과학계는 지금 중대한 기로에 서 있습니다. AI 관련 논문의 급증 그래프는 단순히 기술 발전의 속도를 넘어, 과학적 방법론의 근본적인 재고를 요구하는 경고등일 수 있습니다. 이러한 변화의 시기에, 연구자들은 AI 모델의 불확실성을 정량화하고, 편향을 식별하며, 결과의 견고성을 체계적으로 검증하는 훈련을 강화해야 합니다. 대학, 연구 기관, 그리고 자금 지원 기관은 단기적인 '혁신'이나 '최고 성능'이라는 수치적 성과만을 쫓기보다는, 장기적인 과학적 신뢰성과 재현성 문화를 구축하는 데 집중해야 합니다. AI 연구 자금의 일부를 메타 과학(meta-science), 재현성 감사(reproducibility audit), 그리고 투명한 연구 환경 구축을 위한 인프라(infrastructure) 지원에 할당하는 것은 건전한 과학 발전을 촉진하는 현명한 투자일 것입니다. 궁극적으로, AI 기반 과학의 미래는 기술 자체의 발전뿐만 아니라, 과학 공동체가 이러한 강력한 도구를 얼마나 책임감 있고 비판적으로 수용하고 활용하느냐에 달려 있습니다.

저희 책 **AI 스네이크 오일(AI Snake Oil)**은 현재 사전 주문이 가능합니다. 저희 블로그(blog)를 즐겁게 읽으셨고 저희 작업을 지원하고 싶으시다면, 아마존(Amazon), 북샵(Bookshop) 또는 즐겨 찾는 서점을 통해 사전 주문해 주십시오.

1 분명히 하자면, 반복 실패가 반드시 원본 연구의 결함을 의미하는 것은 아닙니다. 이 게시물에서 저희의 주된 관심사는 누출(leakage)과 같은 비교적 명확한 오류에 관한 것입니다.