AI 시대의 윤리적 딜레마와 지속 가능한 발전

누가 AI 과장 광고를 만들어내는가? 저희의 **AI 스네이크 오일(AI Snake Oil)** 책에서 논의했듯이, 기업과 언론뿐만 아니라 AI 연구자들도 그렇습니다. 기술의 발전은 언제나 양면성을 지니고 있으며, 최근 인공지능(AI) 기술은 전례 없는 속도로 발전하며 사회 전반에 걸쳐 혁신적인 변화를 약속하고 있습니다. 하지만 이러한 급속한 발전 이면에는 윤리적 문제, 사회적 불평등 심화, 그리고 환경적 부담과 같은 중대한 딜레마들이 존재합니다. 예를 들어, 2023년 12월 네이처(Nature)지에 발표되어 널리 알려진 두 편의 논문은 AI를 사용하여 220만 개 이상의 새로운 물질을 발견하고 그중 41개를 로봇으로 합성했다고 주장했습니다. 안타깝게도, 이러한 주장은 빠르게 반박되었습니다. "생산된 [41개] 물질 대부분은 잘못 식별되었으며, 나머지는 이미 알려진 것이었습니다." 대규모 데이터셋(dataset)의 경우, 250개 화합물 샘플(sample)을 조사한 결과 대부분이 쓸모없는 데이터(junk)임이 드러났습니다. 기계 학습(machine learning)의 핵심 강점 중 하나는 이해 없이 발견하는 것인데, 이것이 기계 학습 기반 과학에서 오류가 특히 흔한 이유입니다. 또한, 대규모 언어 모델(LLM)의 등장으로 콘텐츠 생성과 정보 접근성이 향상되었지만, 동시에 허위 정보(misinformation) 확산과 저작권 침해 논란도 끊이지 않고 있습니다. AI 시스템의 복잡성과 불투명성은 의사결정 과정에서의 편향(bias) 문제를 야기하며, 이는 사회적 약자에게 더 큰 불이익으로 작용할 수 있습니다. 3년 전, 저희는 시험을 위한 학습(teaching to the test)의 기계 학습(machine learning) 버전인 누출(leakage)이라는 오류가 만연하여 17개 분야의 수백 편의 논문에 영향을 미쳤다는 증거를 수집했습니다. 그 이후로 저희는 이 문제를 더 잘 이해하고 해결책을 고안하기 위해 노력해 왔습니다. 이 게시물은 업데이트된 내용을 제공합니다. 요컨대, 저희는 상황이 나아지기 전에 더 나빠질 것이라고 생각하지만, 희망의 빛도 보입니다.

참사는 계속됩니다. 저희의 가장 최근 자료 취합에 따르면, 연구자들이 출판된 연구에서 누출(leakage)을 발견한 분야의 수는 30개에 달했으며, AI 모델의 불투명성과 편향성 문제는 더욱 심화되고 있습니다. 대다수는 의학 분야인데, 이는 의학 연구의 오류가 특히 중대한 결과를 초래할 수 있기 때문에 의학 분야가 모범 사례를 확립하고 이전에 출판된 연구를 비판적으로 검토하는 데 훨씬 더 많은 노력을 기울이는 경향이 있기 때문이라고 강력히 추정합니다. 의료 진단, 채용 심사, 대출 승인 등 중요한 의사결정에 AI가 활용되면서, 특정 집단에 대한 차별이 무의식적으로 시스템에 내재될 수 있다는 우려가 커지고 있습니다. 이러한 편향은 학습 데이터(training data)의 불균형에서 비롯되거나, 개발자의 암묵적인 가치관이 반영된 결과일 수 있습니다. 대다수는 이러한 문제의 심각성을 인지하지 못하고 있으며, 이는 AI 시스템에 대한 불신으로 이어질 수 있습니다. 모든 분야에 걸쳐 약 650편의 논문이 영향을 받았는데, 이는 엄청나게 과소평가된 수치라고 저희는 가정합니다. 연구자들이 체계적으로 누출(leakage)이나 편향성을 찾을 때, 많은 분야에서 샘플(sample)로 추출된 연구나 데이터의 대다수가 누출(leakage) 오류를 범하거나 이미 왜곡된 정보를 포함하고 있음을 발견하기 때문입니다.

누출(leakage)과 데이터 편향은 기계 학습(ML) 기반 시스템의 재현성(reproducibility) 및 신뢰성 실패의 여러 원인 중 하나입니다. 기계 학습(ML) 기반 과학 및 시스템의 모든 단계, 즉 데이터(data) 수집부터 전처리(preprocessing) 및 결과 보고와 해석에 이르기까지 광범위한 결함이 존재합니다. 특히, 특정 인구 집단을 소외시키는 데이터셋(dataset) 구성은 모델의 성능 저하를 넘어 사회적 불평등을 재생산할 위험을 안고 있습니다. 재현 불가능성(irreproducibility)으로 이어질 수 있는 문제로는 부적절한 기준선(baseline) 비교, 부적절한 데이터 정제, 비대표적인 샘플(sample), 특정 모델링(modeling) 또는 알고리즘(algorithm) 선택에 민감한 결과, 그리고 모델(model) 불확실성(uncertainty)이나 공정성(fairness)을 보고하지 않는 것 등이 있습니다. 또한 연구자와 개발자들이 자신의 코드(code)와 데이터(data)는 물론 데이터(data) 출처와 처리 과정을 공개하지 않아 재현성(reproducibility)과 투명성(transparency)을 저해하는 근본적인 문제도 있습니다. 예를 들어, Gabelica 등은 2019년 1월 바이오메드 센트럴(BioMed Central)에 등재된 333개의 오픈 액세스(open-access) 저널을 조사한 결과, 요청 시 데이터를 공유하겠다고 약속한 1,800편의 논문 중 93%가 그렇게 하지 않았음을 발견했습니다. 또한 최근 한 연구는 AI 윤리 보고서에서 데이터셋의 다양성 부족과 편향성 문제를 언급한 기업이 극히 드물었음을 지적했습니다.

뿌리 깊은 문제
기계 학습(ML) 및 인공지능(AI) 이전에도 많은 과학 및 기술 분야는 재현성(reproducibility), 반복성(replicability), 사회적 영향력 및 책임성 위기에 직면해 있었습니다. 근본 원인으로는 과학계의 '출판 아니면 도태(publish-or-perish)' 문화와 기술 개발의 속도만을 중시하는 문화, 긍정적인 결과나 기술 성과 발표에 대한 강한 편향(그리고 부정적인 결과나 사회적 영향 보고의 거의 불가능함), 잘못된 연구나 기술 적용을 반박할 유인 부족, 그리고 부실한 연구를 출판하거나 윤리적 문제를 간과해도 아무런 결과가 없는 점 등이 있습니다. 예를 들어, 잘못된 논문이나 알고리즘(algorithm)은 거의 철회되거나 수정되지 않습니다. 동료들은 반복 실패나 윤리적 실패조차 인지하지 못하는 것 같습니다. 한 논문이 반복에 실패하거나 한 알고리즘(algorithm)이 차별적 결과를 낳은 후, 이를 인용하거나 지적하는 논문 중 단 3%만이 그 반복 시도나 문제를 인용했습니다. 1 과학 커뮤니케이터(communicator)들은 과학과 기술이 스스로 수정된다고 주장하기를 좋아하지만, 저희의 경험상 자기 수정은 사실상 존재하지 않습니다.

이러한 모든 문화적 요인들은 기계 학습(ML) 기반 과학 및 인공지능(AI) 개발 분야에서도 나타납니다. 그러나 기계 학습(ML)과 인공지능(AI)은 출판된 결과와 사회적 영향에 대해 우리가 회의적이어야 하는 여러 가지 추가적인 이유를 제시합니다. 성능 평가(performance evaluation)는 악명 높게 까다로우며, 불확실성 정량화(uncertainty quantification)와 대규모 모델의 에너지 소비(energy consumption), 지속 가능성(sustainability)과 같은 많은 측면은 미해결 연구 분야입니다. 또한, 기계 학습(ML) 코드(code)와 인공지능(AI) 모델의 학습(training) 과정은 전통적인 통계 모델링(statistical modeling)이나 소프트웨어(software) 개발보다 훨씬 더 복잡하고 덜 표준화되는 경향이 있습니다. 동료 심사자(peer reviewer)의 역할이 코드(code)를 검토하는 것이 아니기 때문에 코딩(coding) 오류는 거의 발견되지 않으며, 윤리 심사자(ethics reviewer)의 역할이 에너지 효율을 검토하는 것이 아니기 때문에 환경적 영향 평가는 거의 이루어지지 않습니다. 그러나 저희는 연구 품질이 낮고 기술의 사회적 책임이 낮은 가장 큰 이유가 만연한 과장 광고(hype)와 상업적 이익 추구이며, 이는 연구자와 개발자들 사이에서 회의적이고 비판적인 사고방식의 부족으로 이어지고, 이는 좋은 과학적 및 기술적 실천의 초석이 된다고 생각합니다. 저희는 연구자와 개발자들이 지나치게 낙관적인 기대를 가지고 있고 그들의 기계 학습(ML) 모델(model)이나 인공지능(AI) 모델(model)이 제대로 작동하지 않거나 예상치 못한 부작용을 일으킬 때, 그들은 자신들이 뭔가 잘못했다고 가정하고 모델(model)을 수정하지만, 사실은 예측 가능성(predictability)의 본질적인 한계나 시스템의 본질적인 한계에 부딪혔을 가능성을 강력히 고려해야 한다는 것을 관찰했습니다. 반대로, 모델(model)이 잘 작동하거나 사회적으로 잘 작동하는 것처럼 보일 때는 쉽게 믿는 경향이 있는데, 사실은 누출(leakage)이나 숨겨진 편향, 또는 다른 결함에 대해 경계를 늦추지 않아야 합니다. 그리고 모델(model)이 예상보다 더 잘 작동하거나 사회적으로 긍정적인 영향을 미치면, 그들은 인간이 생각할 수 없었던 데이터(data)의 패턴(pattern)이나 기술적 패턴(pattern)을 발견했다고 가정하며, AI가 외계 지능이거나 모든 문제를 해결할 수 있다는 신화는 이러한 설명을 쉽게 그럴듯하게 만듭니다.

이것은 피드백 루프(feedback loop)입니다. 지나친 낙관주의는 결함 있는 연구와 사회적 책임을 간과하는 기술 개발을 부추기고, 이는 해당 분야의 다른 연구자들과 개발자들에게 AI가 무엇을 할 수 있고 무엇을 할 수 없는지에 대해 잘못된 정보를 제공합니다. 사실, 저희는 좌절한 연구자들과 개발자들과의 개인적인 서신에서 이러한 극단적인 경우를 접했습니다. 결함 있는 연구나 시스템이 수정되지 않기 때문에, "최첨단(state of the art)"을 능가하지 못하는 모델(model)을 초래하는 좋은 연구를 출판하거나 안전한 모델(model)을 개발하는 것은 말 그대로 불가능해집니다. 도구가 강력하고 블랙박스(black-box)일수록 오류와 과신(overconfidence)의 가능성이 커집니다. 심리학, 의학 등에서의 반복성(replication) 위기는 평범한 옛 통계의 오용에서 비롯된 것이었습니다. 기계 학습(ML)과 인공지능(AI)이 상대적으로 얼마나 새로운지를 고려할 때, 저희는 기계 학습(ML) 기반 과학의 재현성(reproducibility) 위기와 인공지능(AI) 기반 시스템의 사회적 책임 위기가 나아지기 전에 한동안 더 악화될 것이라고 추측합니다. 그리고 이제 과학자들과 개발자들은 대규모 언어 모델(large language models)과 생성형 AI(generative AI)를 수용하고 있는데, 이는 이해의 환상(illusion of understanding)과 같은 많은 새로운 함정을 열어줍니다.

저희 책에 대한 블로그(blog)인 **AI 스네이크 오일(AI Snake Oil)**을 읽고 계십니다. 새 게시물과 AI 윤리에 대한 더 깊은 통찰을 얻으려면 구독하세요. 구독하기

희망의 빛
기계 학습(ML) 기반 과학 및 인공지능(AI) 기반 기술의 한 가지 좋은 점은 일반적으로 사람을 대상으로 하거나 인간에게 직접적인 해를 가하는 실험이 아니라 데이터 분석(data analysis) 및 알고리즘(algorithm) 분석만을 포함한다는 것입니다. 따라서 다른 연구자들과 개발자들은 원칙적으로 논문의 코드(code)와 데이터(data), 학습 데이터(training data)를 다운로드하여 보고된 결과의 재현성(reproducibility)과 공정성(fairness), 투명성(transparency)을 확인할 수 있어야 합니다. 또한 코드(code)에서 오류나 문제가 있는 선택, 또는 윤리적 오류가 있는지 검토할 수도 있습니다. 이는 시간이 많이 걸리지만, 심리학이나 의학 연구를 반복하거나 사회 전체에 미칠 파급 효과를 고려할 때 훨씬 덜합니다. 심리학이나 의학 연구는 일반적으로 원본 연구만큼 비용이 많이 들고, 사회 전체에 미치는 영향은 일반적으로 원본 개발만큼 비용이 많이 듭니다. 또 다른 좋은 점은 연구자와 개발자들이 무엇을 주의해야 할지 안다면 대부분의 오류와 윤리적 문제를 피할 수 있다는 것입니다. 대조적으로, 사전 등록(pre-registration)과 같은 통계 과학의 반복성(replication) 위기에 대한 완화책이나 사전 규제(pre-regulation)와 같은 신기술의 사회적 책임 위기에 대한 완화책은 효과 면에서 훨씬 더 불규칙한 실적을 보입니다. 따라서 저희는 연구자와 개발자들이 자신의 연구와 기술에 체계적으로 더 많은 주의를 기울이고 재현성(reproducibility) 연구 및 윤리적 책임 연구에 인센티브(incentive)가 부여되는 문화 변화를 통해 이 문제가 크게 완화될 수 있다고 생각합니다. 기계 학습(ML) 및 인공지능(AI) 방법론 커뮤니티(community)는 이미 공통 과제 방법(common task method, 수십 년 된 방법), 재현성 챌린지(reproducibility challenge, 비교적 최근의 방법), 책임 있는 AI 개발 원칙(responsible AI development principles, 수십 년 된 방법)과 윤리적 AI 챌린지(ethical AI challenge, 비교적 최근의 방법)를 통해 이러한 방향으로 나아가고 있지만, 기계 학습(ML) 기반 과학과 인공지능(AI) 기반 기술, 즉 각 분야에서 기계 학습(ML) 모델(model)이나 인공지능(AI) 모델(model)을 사용하여 지식을 발전시키거나 사회적 가치를 발전시키는 의학, 심리학, 의료, 교육과 같은 분야에서는 아직 이러한 변화가 일어나지 않았습니다.

저희는 이를 바꾸기 위한 몇 가지 노력을 주도했습니다. 첫째, 저희의 누출(leakage) 관련 논문과 책임 있는 AI 관련 활동은 영향력을 가졌습니다. 연구자와 개발자들이 모델(model)을 구축하는 방법과 누출(leakage) 및 편향성(bias)의 부재를 문서화하고 입증하는 데 사용되었습니다. 출판된 연구에서 누출(leakage)이나 윤리적 문제를 찾으려는 연구자들과 개발자들에 의해 사용되었습니다. 또한 누출(leakage) 연구와 AI 윤리 연구의 중요성을 강조하고 분야별 가이드라인(guideline)을 마련하는 방법으로도 사용되었습니다. 누출(leakage)과 편향성(bias) 외에도, 저희는 컴퓨터 과학, 데이터 과학, 사회 과학, 수학, 생의학 연구, 법률 및 정책 연구 분야의 19명의 연구자 그룹을 이끌어 기계 학습(ML) 기반 과학을 위한 REFORMS 체크리스트(checklist)와 인공지능(AI) 기반 기술을 위한 TRUST(Transparency, Responsibility, Understanding, Safety, Trustworthiness) 체크리스트(checklist)를 개발했습니다. 이는 기계 학습(ML) 기반 과학 및 인공지능(AI) 기반 기술에서 흔히 발생하는 여덟 가지 함정을 연구자와 개발자들이 파악하는 데 도움이 될 수 있는 32개 항목의 체크리스트(checklist)이며, 누출(leakage)과 편향성(bias)은 그중 하나에 불과합니다. 이 체크리스트(checklist)는 최근 사이언스 어드밴시스(Science Advances)와 주요 기술 윤리 저널에 게재되었습니다. 물론 문화 변화가 없다면 체크리스트(checklist)만으로는 도움이 되지 않겠지만, 지금까지의 반응을 볼 때 저희는 조심스럽게 낙관하고 있습니다.

결론
저희의 요점은 AI가 과학자들에게 쓸모없거나 인류에게 해롭다는 것이 아닙니다. 저희 자신도 AI에 관한 연구가 아닐 때조차 AI를 도구(tool)로 자주 사용합니다. 핵심 단어는 '도구(tool)'입니다. AI는 혁명이 아닙니다. AI는 인간의 이해나 판단을 대체하는 것이 아닙니다. 그렇게 생각하는 것은 과학의 본질이나 기술 윤리의 본질을 놓치는 것입니다. AI는 연구에 내재된 고된 작업과 좌절, 그리고 기술 개발에 내재된 복잡한 사회적 숙고와 책임에 대한 지름길을 제공하지 않습니다. AI는 신탁이 아니며 미래를 볼 수 없습니다. 안타깝게도 대부분의 과학 및 기술 분야는 AI 과장 광고(hype)에 굴복하여 상식의 정지를 초래했습니다. 예를 들어, 정치학의 한 연구 분야는 내전 발발을 90%가 훨씬 넘는 정확도(accuracy) 2로 예측한다고 주장했으며, 한 핀테크(fintech) 연구 분야는 대출 부도율 예측에서 90%가 훨씬 넘는 정확도(accuracy)로 예측한다고 주장했는데, 이는 표면적으로 불가능하게 들리는 수치입니다. (전자는 누출(leakage)로, 후자는 데이터 편향과 과적합(overfitting)으로 밝혀졌으며, 이것이 저희가 이 연구 분야 전체에 관심을 갖게 된 계기였습니다.)

저희는 과학 및 기술 발전 역사상 흥미로운 순간에 있습니다. 다양한 분야와 산업에서 AI 채택을 보여주는 다음 그래프들을 보십시오. 3

**분야별 AI 관련 논문 비율, 1985–2023년.**
**산업별 AI 관련 프로젝트 비율, 2018–2023년.**
(출처: Duede et al. 2024)

이러한 하키 스틱(hockey stick) 그래프는 좋은 소식이 아닙니다. 오히려 섬뜩해야 합니다. AI를 채택하려면 과학적 인식론(epistemology)과 기술 윤리(tech ethics)에 대한 근본적인 변화가 필요합니다. 4 어떤 과학 또는 기술 분야도 몇 년이라는 짧은 시간 안에 이를 달성할 능력이 없습니다. 이것은 도구나 방법이 유기적으로 채택될 때 일어나는 일이 아닙니다. 과학자들이 자금을 얻기 위해 유행에 편승하거나 개발자들이 자금을 얻기 위해 유행에 편승할 때 일어나는 일입니다. 과장 광고(hype)의 수준을 고려할 때, 과학자들과 개발자들은 AI를 채택하기 위해 추가적인 인센티브(incentive)가 필요하지 않습니다. 이는 과학을 위한 AI 자금 지원 프로그램과 기술 윤리를 위한 AI 자금 지원 프로그램이 상황을 악화시키고 있을 가능성이 높다는 것을 의미합니다. 저희는 결함 있는 연구나 시스템의 홍수를 막을 수 있을지 의심스럽지만, 과학과 기술을 위한 AI 자금의 일부라도 더 나은 훈련, 비판적 탐구, 메타 과학(meta-science), 메타 윤리(meta-ethics), 재현성(reproducibility), 투명성(transparency) 및 기타 품질 관리 노력으로 전환된다면 혼란을 최소화할 수 있습니다.

저희 책 **AI 스네이크 오일(AI Snake Oil)**은 현재 사전 주문이 가능합니다. 저희 블로그(blog)를 즐겁게 읽으셨고 저희 작업을 지원하고 싶으시다면, 아마존(Amazon), 북샵(Bookshop) 또는 즐겨 찾는 서점을 통해 사전 주문해 주십시오.

1.  분명히 하자면, 반복 실패가 반드시 원본 연구나 기술의 결함을 의미하는 것은 아닙니다. 이 게시물에서 저희의 주된 관심사는 누출(leakage)이나 데이터 편향과 같은 비교적 명확한 오류에 관한 것입니다.
2.  여기서 정확도(accuracy)는 문맥에 따라 AUC 또는 F1 점수(F1 Score)와 같은 지표를 의미합니다. 예를 들어, 한 가지 결과(평화 또는 대출 상환)가 다른 결과(전쟁 또는 대출 부도)보다 훨씬 더 흔할 때조차 기준선(baseline) AUC 또는 F1 점수는 50%입니다.
3.  해당 논문은 다양한 유형의 AI "참여"를 함께 묶습니다. 참여는 새로운 AI 이론 및 접근 방식, 기술 또는 응용 프로그램의 개발; 도메인(domain)별 작업을 위한 AI 모델(model)의 일반적인 사용; 그리고 철학 및 윤리와 같은 분야의 학술 담론으로 대표되는 AI에 대한 비판적 참여를 포함할 수 있습니다(이에 국한되지 않음). 저희의 목적상 이는 유감스러운 일인데, 저희의 관심사는 오직 두 번째 범주, 즉 도메인(domain)별 작업을 위한 AI 사용에만 있기 때문입니다. 저희는 컴퓨터 과학 및 철학과 같은 몇몇 분야를 제외하고는 대부분의 AI 참여가 이 범주에 속한다고 생각합니다.
4.  특히, "모든 모델(model)은 틀렸지만 일부 모델(model)은 유용하다"는 말처럼, 모델(model)을 기반으로 세상이나 사회적 결론을 언제 도출할 수 있는지에 대한 명확한 답은 없으므로, 모든 분야와 모든 유형의 모델(model)에 대해 타당성(validity)을 재검토해야 합니다.