인공지능 관련 허황된 주장들은 어디에서 비롯되는가?

저희의 저서 **AI 스네이크 오일(AI Snake Oil)**에서 상세히 다루었듯이, 이는 비단 기업이나 언론 매체만의 책임이 아닙니다. AI 분야 연구자들 또한 이러한 현상에 일조하고 있습니다. 예를 들어, 2023년 12월 유명 학술지 네이처(Nature)에 게재되어 큰 주목을 받았던 두 편의 연구는 인공지능 기술을 활용해 220만 개가 넘는 신규 화합물을 찾아냈고, 이 중 41종은 자동화된 방식으로 합성해냈다는 내용이었습니다. 유감스럽게도, 이 같은 주장은 신속히 오류로 판명되었습니다. '제작된 [41종의] 물질 대다수는 오인되었고, 나머지 역시 기존에 이미 파악된 것들이었습니다.' 또한, 방대한 규모의 데이터 집합(dataset)을 대상으로 한 250개 화합물 표본(sample) 조사에서는 대부분이 무의미한 정보(junk)로 밝혀졌습니다. 기계 학습(machine learning)의 주요 강점 중 하나는 현상에 대한 깊이 있는 이해 없이도 패턴을 발견하는 능력인데, 바로 이 특성이 기계 학습 기반 과학 분야에서 오류가 빈번하게 발생하는 주된 원인이 됩니다. 저희는 3년 전, '시험을 위한 학습(teaching to the test)'의 기계 학습(machine learning) 버전이라 할 수 있는 '데이터 누출(leakage)'이라는 만연한 오류가 17개 학문 분야에 걸쳐 수백 편의 논문에 악영향을 미쳤다는 증거를 확보했습니다. 그 이후로 저희는 이 문제의 본질을 더욱 깊이 파고들고 실질적인 해결책을 마련하기 위해 꾸준히 노력해왔습니다. 본 글은 그간의 진행 상황에 대한 최신 정보를 제공합니다. 간략히 말씀드리자면, 현재의 상황은 개선되기 전에 더욱 악화될 가능성이 높지만, 그럼에도 불구하고 긍정적인 변화의 조짐 또한 감지됩니다.

이러한 문제 상황은 지속되고 있습니다. 저희가 최근 수집한 데이터에 따르면, 학계에서 출판된 연구물 내에서 데이터 유출(leakage) 현상이 확인된 학문 분야는 현재 30개에 육박합니다. 이 중 상당수는 의료 분야인데, 이는 의료 연구의 오류가 사람의 생명과 직결되는 심각한 결과를 초래할 수 있기에, 해당 분야가 연구 표준을 정립하고 기존 발표 연구를 더욱 엄격하게 검증하려는 경향이 강하기 때문으로 분석됩니다. 총 650여 편의 학술 문서가 이러한 문제의 영향을 받은 것으로 집계되었지만, 이는 실제보다 현저히 낮은 추정치로 판단됩니다. 그 이유는 연구자들이 체계적인 방법으로 데이터 유출 현상을 조사할 때, 특정 분야에서 표본으로 삼은 연구의 상당수가 유출로 인한 결함을 내포하고 있음을 밝혀내기 때문입니다. 특히 의료 데이터는 민감한 개인 정보와 복잡한 구조를 가지고 있어, 비의도적인 데이터 유출이 발생하기 쉬운 환경이며, 이를 감지하고 해결하는 것 또한 다른 분야에 비해 더 많은 전문성을 요구합니다.

데이터 유출(leakage)은 연구 결과의 재현성(reproducibility)이 저해되는 여러 요인 중 하나일 뿐입니다. 기계 학습(ML) 기반 과학 연구는 데이터(data)를 수집하는 첫 단계부터 전처리(preprocessing), 그리고 최종 결과 보고에 이르는 전 과정에 걸쳐 광범위한 결함들을 내포하고 있습니다. 재현 불가능성(irreproducibility)을 야기하는 문제점들로는 적절치 못한 기준선(baseline) 설정, 모집단을 제대로 반영하지 못하는 표본(sample) 사용, 특정 모델링(modeling) 기법에 따라 결과가 크게 달라지는 민감성 문제, 그리고 모델(model)의 불확실성(uncertainty)을 명확히 제시하지 않는 관행 등이 있습니다. 나아가, 연구자들이 자신들의 코드(code)와 데이터를 공개하지 않는 행위는 재현성을 가로막는 근본적인 장애물로 작용합니다. 가벨리카 연구팀은 2019년 1월 바이오메드 센트럴에 실린 333종의 개방형 접근 학술지를 분석했습니다. 그 결과, 자료 요청 시 공유를 약속했던 1,800편의 논문 중 93%가 실제로는 그 약속을 이행하지 않았다는 사실을 밝혀냈습니다. 이러한 재현 불가능한 연구들은 학술적 진보를 더디게 할 뿐만 아니라, 후속 연구자들이 잘못된 기반 위에서 시간과 자원을 낭비하게 만드는 막대한 사회적, 경제적 비용을 발생시키며, 궁극적으로는 과학에 대한 대중의 신뢰를 저해하는 심각한 결과를 초래합니다.

**뿌리 깊은 문제**
이 문제는 기계 학습(ML) 시대 이전부터 뿌리 깊게 자리 잡고 있었습니다. 많은 과학 분야는 이미 재현성(reproducibility) 및 반복성(replicability) 위기에 봉착해 있었죠. 이러한 현상의 근본 원인으로는 학계에 만연한 '출판 아니면 도태(publish-or-perish)' 문화, 긍정적인 결과(positive results)만을 선호하는 강력한 편향(이는 부정적인 결과(negative results)를 발표하는 것을 거의 불가능하게 만듭니다), 오류가 있는 연구를 반증하려는 동기 부족, 그리고 부실한 연구가 출판되어도 실질적인 불이익이 없다는 점 등이 꼽힙니다. 예를 들어, 명백히 잘못된 논문조차 거의 철회되지 않으며, 동료 연구자들은 반복 실패 사례를 제대로 인지하지 못하는 경향이 있습니다. 실제로, 특정 논문의 반복 연구가 실패한 이후에도, 해당 논문을 인용하는 다른 연구들 중 단 3%만이 그 반복 시도를 언급했습니다. 1 과학 대중화자(communicator)들은 과학이 스스로 오류를 수정해나간다고 주장하곤 하지만, 저희의 경험에 비추어 볼 때, 이러한 자체 수정 메커니즘은 거의 작동하지 않는 것이 현실입니다. 이는 논문 수나 인용 지수와 같은 양적 지표에만 치중하는 현재의 학술 평가 시스템이, 연구의 질보다는 발표 자체에 더 큰 가치를 두도록 유도하기 때문이기도 합니다.

기존의 모든 문화적 요소들은 기계 학습(ML) 기반 과학 연구에도 고스란히 반영됩니다. 하지만 기계 학습(ML)은 발표된 연구 결과에 대해 우리가 더욱 비판적인 시각을 가져야 할 여러 가지 추가적인 근거들을 제공합니다. 성능 평가(performance evaluation)는 워낙 복잡하고 까다로운 문제이며, 불확실성 정량화(uncertainty quantification)와 같은 여러 중요한 측면들은 아직 해결되지 않은 연구 영역으로 남아있습니다. 게다가, 기계 학습(ML) 모델(model)의 코드(code)는 전통적인 통계적 모델링(statistical modeling) 방식보다 훨씬 더 복잡하고 표준화가 덜 되어 있는 경향이 있습니다. 동료 심사자(peer reviewer)의 역할이 코드를 면밀히 검토하는 것이 아니기에, 코딩(coding) 상의 오류들은 거의 발견되지 않은 채로 넘어가는 경우가 많습니다. 그러나 저희는 연구의 질이 떨어지는 가장 큰 원인이 바로 만연한 과장 광고(hype)에 있다고 봅니다. 이는 연구자들 사이에서 비판적 사고방식의 부재로 이어지는데, 비판적 사고는 양질의 과학적 탐구를 위한 필수적인 토대입니다. 저희는 연구자들이 지나치게 장밋빛 기대를 품고 있으며, 자신들의 기계 학습(ML) 모델(model)이 제대로 작동하지 않을 때, 본인들이 뭔가 잘못했다고 가정하고 모델을 수정하려 하지만, 실제로는 예측 가능성(predictability)의 본질적인 한계에 도달했을 가능성을 진지하게 고려해야 함을 목격했습니다. 반대로, 모델이 기대 이상으로 잘 작동할 때는 쉽게 그 결과를 신뢰하는 경향이 있는데, 이때야말로 데이터 누출(leakage)이나 다른 내재된 결함에 대해 더욱 경계심을 가져야 할 순간입니다. 그리고 모델이 예상보다 훨씬 뛰어난 성능을 보일 경우, 연구자들은 인공지능이 인간이 미처 인지하지 못했던 데이터(data) 내의 숨겨진 패턴(pattern)을 찾아냈다고 성급하게 결론 내리곤 합니다. 이는 'AI가 외계 지능'이라는 신화적 서사와 맞물려 이러한 설명을 더욱 그럴듯하게 만듭니다. 여기서 인공지능 모델의 '블랙박스(black-box)' 특성은 또 다른 난제입니다. 모델이 어떻게 특정 결론에 도달했는지 설명하기 어렵기 때문에, 과학적 이해보다는 단순히 예측 결과에만 의존하게 되어 잘못된 해석으로 이어질 위험이 큽니다.

이 모든 것은 악순환의 고리(feedback loop)를 형성합니다. 과도한 낙관론은 질 낮은 연구를 양산하게 만들고, 이는 다시 해당 분야의 다른 연구자들에게 인공지능의 실제 능력과 한계에 대한 잘못된 정보를 주입합니다. 실제로 저희는 좌절한 연구자들과의 개인적인 소통을 통해 이러한 극단적인 상황들을 직접 확인했습니다. 결함이 있는 연구가 제대로 시정되지 않기 때문에, 기존의 '최첨단(state of the art)' 성능을 능가하지 못하는 양질의 연구를 발표하는 것이 사실상 불가능해지는 역설적인 상황이 발생합니다. 도구가 강력하고 그 내부 작동 방식이 불투명한 '블랙박스(black-box)'에 가까울수록, 오류 발생 가능성과 더불어 과신(overconfidence)의 위험성 또한 증대됩니다. 과거 심리학이나 의학 분야에서 겪었던 반복성(replication) 위기는 단순히 오래된 통계 기법의 오용에서 비롯된 것이었습니다. 기계 학습(ML) 기술이 상대적으로 얼마나 신생 분야인지를 감안할 때, 저희는 기계 학습(ML) 기반 과학 연구의 재현성(reproducibility) 위기가 개선되기 전에 한동안 더욱 심화될 것이라고 예상합니다. 특히 최근 과학자들은 대규모 언어 모델(large language models)과 생성형 AI(generative AI)를 적극적으로 받아들이고 있는데, 이 기술들은 마치 '이해하고 있는 듯한 착각(illusion of understanding)'을 불러일으키는 등 수많은 새로운 함정들을 만들어내고 있습니다. 예를 들어, 이들 모델은 유창하고 설득력 있는 답변을 제공하지만, 그 내용이 사실과 다르거나 완전히 조작된 '환각(hallucination)' 현상을 보일 수 있어, 비판적 검증 없이 받아들일 경우 심각한 오용으로 이어질 수 있습니다.

현재 독자 여러분께서는 저희의 저서 **AI 스네이크 오일(AI Snake Oil)**과 연관된 공식 블로그(blog)를 읽고 계십니다. 새로운 글이 올라올 때마다 알림을 받으시려면, 구독을 신청해주시기 바랍니다. 지금 바로 구독하세요!

**희망의 빛**
하지만 이 상황 속에서도 한 줄기 희망은 존재합니다. 기계 학습(ML) 기반 과학 연구의 긍정적인 측면 중 하나는, 대개 인간 피험자를 대상으로 하는 실험보다는 순전히 데이터 분석(data analysis)에 집중한다는 점입니다. 이론적으로 이는 다른 연구자들이 해당 논문의 코드(code)와 데이터(data)를 손쉽게 내려받아, 보고된 결과가 재현 가능한지 직접 검증할 수 있어야 함을 의미합니다. 더 나아가, 코드(code) 내에 잠재된 오류나 부적절한 선택 사항이 있는지도 심층적으로 검토할 수 있습니다. 비록 이 과정이 상당한 시간을 요구할지라도, 이는 심리학이나 의학 분야에서 원본 연구와 거의 동일한 막대한 비용과 노력을 들여야 하는 반복 연구보다는 훨씬 효율적입니다. 또한, 연구자들이 어떤 함정을 경계해야 하는지 명확히 인지하고 있다면, 대다수의 오류는 충분히 피할 수 있습니다. 이와 대조적으로, 통계 과학 분야의 반복성(replication) 위기를 완화하기 위한 방편으로 제시된 사전 등록(pre-registration)과 같은 조치들은 그 효과 면에서 아직 일관되지 못한 성과를 보이고 있습니다. 그러므로 저희는 연구자들이 자신의 작업에 더욱 체계적이고 면밀한 주의를 기울이고, 재현성(reproducibility)을 높이는 연구 활동에 실질적인 인센티브(incentive)가 부여되는 방향으로 문화적 변화가 일어난다면, 현재의 문제점들이 크게 개선될 수 있다고 확신합니다. 기계 학습(ML) 방법론 공동체(community)는 이미 공통 과제 방법(common task method, 수십 년간 이어져 온 방식)과 재현성 챌린지(reproducibility challenge, 비교적 최근의 시도) 등을 통해 이러한 긍정적인 방향으로 나아가고 있지만, 각 분야에서 기계 학습(ML) 모델(model)을 활용하여 지식을 확장하는 의학이나 심리학과 같은 '기계 학습(ML) 기반 과학' 분야에서는 아직 이러한 변화의 물결이 충분히 확산되지 못하고 있습니다. 이러한 변화를 가속화하기 위해 오픈 사이언스(open science) 원칙을 적극적으로 수용하는 것이 중요합니다. 이는 연구 데이터, 코드, 방법론을 투명하게 공개하고, 연구 과정을 표준화하여 모두가 접근하고 검증할 수 있도록 하는 것을 포함합니다.

**변화를 위한 노력**
저희는 이러한 현실을 개선하기 위해 몇 가지 주도적인 노력을 기울여왔습니다. 첫째, 저희가 발표한 데이터 유출(leakage) 관련 연구 논문들은 상당한 파급력을 가졌습니다. 이 논문들은 연구자들이 모델(model)을 설계하는 과정에서 데이터 유출(leakage)이 없음을 기록하고 증명하는 데 활용되었으며, 이미 출판된 연구물에서 유출 현상을 찾아내려는 다른 연구자들에게도 중요한 참고 자료가 되었습니다. 나아가, 데이터 유출(leakage) 연구의 중요성을 부각시키고 각 학문 분야에 특화된 지침(guideline)을 수립하는 데에도 기여했습니다. 데이터 유출(leakage) 문제 외에도, 저희는 컴퓨터 과학, 데이터 과학, 사회 과학, 수학, 그리고 생의학 연구 분야를 아우르는 19명의 연구자 그룹을 이끌어, 기계 학습(ML) 기반 과학 연구를 위한 'REFORMS 체크리스트(checklist)'를 개발했습니다. 이 체크리스트(checklist)는 기계 학습(ML) 기반 과학에서 흔히 마주치는 여덟 가지 주요 함정들을 연구자들이 식별하고 피할 수 있도록 돕는 32개의 항목으로 구성되어 있으며, 데이터 유출(leakage)은 그중 한 가지에 불과합니다. 해당 체크리스트(checklist)는 최근 권위 있는 학술지 사이언스 어드밴시스(Science Advances)에 게재되었습니다. 물론, 이러한 체크리스트(checklist)만으로는 근본적인 문화 변화가 동반되지 않는다면 한계가 있을 것입니다. 그러나 현재까지의 긍정적인 반응들을 미루어 볼 때, 저희는 조심스럽게 낙관적인 전망을 가지고 있습니다. 이러한 변화는 단순히 도구의 도입을 넘어, 학계 리더십의 강력한 의지와 광범위한 연구자 커뮤니티의 적극적인 참여가 뒷받침될 때 비로소 진정한 결실을 맺을 수 있습니다.

**결론**
결론적으로, 저희의 주장은 인공지능이 과학자들에게 무용지물이라는 것이 아닙니다. 저희 스스로도 인공지능 자체를 연구하는 경우가 아닐 때조차, 이를 유용한 도구(tool)로 빈번히 활용합니다. 여기서 핵심은 인공지능이 그저 '도구(tool)'라는 점입니다. 인공지능은 혁명적 변화를 의미하지 않습니다. 이는 인간의 통찰력을 대체할 수 있는 존재가 아닙니다. 이처럼 인식하는 것은 과학적 탐구의 핵심 가치를 오해하는 행위입니다. 인공지능은 연구 과정에 내재된 고된 노력과 수많은 좌절을 건너뛰는 지름길을 제공하지 않습니다. 또한, 인공지능은 미래를 예언하는 신탁(oracle)이 아닙니다. 유감스럽게도 대다수의 과학 분야는 인공지능 과장 광고(hype)에 굴복하여, 마치 '상식이 정지된' 듯한 상황을 초래했습니다. 예를 들어, 정치학의 특정 연구 분야에서는 내전 발발을 90%를 훌쩍 넘는 정확도(accuracy) 2로 예측한다고 주장했는데, 이는 표면적으로는 불가능에 가까운 수치로 들립니다. (결과적으로 이는 데이터 유출(leakage)에 의한 것으로 드러났으며, 바로 이 사례가 저희가 해당 연구 분야에 주목하게 된 결정적인 계기가 되었습니다.) 인공지능은 마치 강력한 망원경이나 현미경과 같습니다. 새로운 것을 볼 수 있게 해주지만, 그것을 이해하고 해석하는 것은 여전히 인간 과학자의 몫입니다. 도구의 성능이 아무리 뛰어나도, 이를 맹신하거나 그 한계를 간과하는 것은 과학적 진실을 왜곡하고 잘못된 방향으로 이끌 위험이 있습니다.

저희는 현재 과학사에서 매우 흥미로운 전환점에 서 있습니다. 다음 그래프는 다양한 학문 분야에서 인공지능의 도입이 얼마나 확산되었는지를 시각적으로 보여줍니다. 3

**분야별 AI 관련 논문 비율, 1985–2023년.**
(출처: Duede et al. 2024)

이처럼 급격하게 상승하는 '하키 스틱(hockey stick)' 형태의 곡선은 결코 긍정적인 신호로 해석되어서는 안 됩니다. 오히려 섬뜩한 경고로 받아들여야 합니다. 인공지능 기술을 과학 연구에 올바르게 통합하려면, 과학적 인식론(epistemology) 전반에 걸친 심대한 변화가 필수적입니다. 4 어떠한 과학 분야도 단 몇 년이라는 짧은 기간 내에 이러한 거대한 변화를 성공적으로 이루어낼 역량을 갖추기 어렵습니다. 이는 특정 도구나 방법론이 내재적 가치에 의해 자연스럽게 수용될 때 나타나는 현상이 아닙니다. 오히려 연구자들이 연구 자금(funding)을 확보하기 위해 인기 있는 유행(bandwagon effect)에 편승할 때 발생하는 일입니다. 현재의 과장 광고(hype) 수준을 감안할 때, 과학자들이 인공지능을 채택하는 데 추가적인 장려책(incentive)은 필요하지 않습니다. 오히려 과학 분야를 위한 인공지능 연구 자금 지원 프로그램들이 현 상황을 더욱 악화시키고 있을 가능성이 높습니다. 저희는 이미 쏟아져 나오는 결함 있는 연구들의 물결을 완전히 막을 수 있을지는 회의적입니다. 하지만 과학 분야에 투입되는 인공지능 자금의 일부라도 더 심층적인 교육, 비판적 탐구 역량 강화, 메타 과학(meta-science) 연구, 재현성(reproducibility) 확보 및 기타 품질 관리 노력으로 재배분된다면, 현재의 혼란을 최소화하고 더 건전한 과학 생태계를 조성하는 데 기여할 수 있을 것입니다.

저희의 저서 **AI 스네이크 오일(AI Snake Oil)**은 현재 예약 구매가 가능합니다. 본 블로그(blog)의 글을 흥미롭게 읽으셨고 저희의 연구 활동을 지지해주고 싶으시다면, 아마존(Amazon), 북샵(Bookshop) 또는 평소 이용하시는 서점에서 예약 구매를 통해 응원해주시면 감사하겠습니다.

1.  분명히 밝히자면, 반복 연구의 실패가 반드시 원본 연구에 결함이 있음을 의미하는 것은 아닙니다. 이 게시물에서 저희의 주된 관심사는 데이터 유출(leakage)과 같은 비교적 명확한 종류의 오류에 집중되어 있습니다.
2.  여기서 언급된 정확도(accuracy)는 AUC라는 측정 지표를 의미합니다. 평화와 같이 한 가지 결과가 전쟁보다 훨씬 더 빈번하게 발생하는 상황에서도 기준선(baseline) AUC는 50%입니다.
3.  해당 논문은 다양한 형태의 AI '참여'를 포괄적으로 묶어서 제시합니다. 이러한 참여는 새로운 AI 이론 및 접근 방식, 기술 또는 응용 프로그램의 개발; 특정 도메인(domain) 작업을 위한 AI 모델(model)의 일반적인 활용; 그리고 철학 및 윤리와 같은 분야의 학술적 논의에서 나타나는 AI에 대한 비판적 참여 등을 포함할 수 있습니다(이에 국한되지 않음). 저희의 논지에서는 이러한 포괄적 분류가 아쉬운데, 저희의 주된 관심은 오직 두 번째 범주, 즉 특정 분야의 실질적인 업무를 위한 AI 사용에만 있기 때문입니다. 저희는 컴퓨터 과학 및 철학과 같은 일부 분야를 제외하고는 대부분의 AI 참여가 이 범주에 속한다고 판단합니다.
4.  특히, "모든 모델(model)은 틀렸지만, 일부 모델(model)은 유용하다"는 말처럼, 모델(model)을 기반으로 세상에 대한 결론을 언제 도출할 수 있는지에 대한 명확한 해답은 아직 없으므로, 모든 학문 분야와 모든 유형의 모델(model)에 대해 그 타당성(validity)을 재검토하는 과정이 필요합니다.