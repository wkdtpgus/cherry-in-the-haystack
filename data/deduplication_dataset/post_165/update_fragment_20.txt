AI 시대의 윤리적 딜레마와 지속 가능한 발전

누가 AI 과장 광고를 만들어내는가? 저희의 **AI 스네이크 오일(AI Snake Oil)** 책에서 논의했듯이, 기술의 발전은 언제나 양면성을 지니고 있습니다. 최근 인공지능(AI) 기술은 전례 없는 속도로 발전하며 사회 전반에 걸쳐 혁신적인 변화를 약속하고 있습니다. 하지만 이러한 급속한 발전 이면에는 윤리적 문제, 사회적 불평등 심화, 그리고 환경적 부담과 같은 중대한 딜레마들이 존재합니다. 예를 들어, 대규모 언어 모델(LLM)의 등장으로 콘텐츠 생성과 정보 접근성이 향상되었지만, 동시에 허위 정보(misinformation) 확산과 저작권 침해 논란도 끊이지 않고 있습니다. AI 시스템의 복잡성과 불투명성은 의사결정 과정에서의 편향(bias) 문제를 야기하며, 이는 사회적 약자에게 더 큰 불이익으로 작용할 수 있습니다. 이 게시물은 업데이트된 내용을 제공합니다. 요컨대, 저희는 상황이 나아지기 전에 더 나빠질 것이라고 생각하지만, 희망의 빛도 보입니다.

참사는 계속됩니다. 저희의 가장 최근 자료 취합에 따르면, AI 모델의 불투명성과 편향성 문제는 더욱 심화되고 있습니다. 의료 진단, 채용 심사, 대출 승인 등 중요한 의사결정에 AI가 활용되면서, 특정 집단에 대한 차별이 무의식적으로 시스템에 내재될 수 있다는 우려가 커지고 있습니다. 이러한 편향은 학습 데이터(training data)의 불균형에서 비롯되거나, 개발자의 암묵적인 가치관이 반영된 결과일 수 있습니다. 대다수는 이러한 문제의 심각성을 인지하지 못하고 있으며, 이는 AI 시스템에 대한 불신으로 이어질 수 있습니다. 모든 분야에 걸쳐 약 650편의 논문이 영향을 받았는데, 이는 엄청나게 과소평가된 수치라고 저희는 가정합니다. 연구자들이 체계적으로 편향성을 찾을 때, 많은 분야에서 샘플(sample)로 추출된 데이터의 대다수가 이미 왜곡된 정보를 포함하고 있음을 발견하기 때문입니다.

데이터 편향은 AI 시스템의 신뢰성 실패의 여러 원인 중 하나입니다. 기계 학습(ML) 기반 시스템의 모든 단계, 즉 데이터(data) 수집부터 전처리(preprocessing) 및 결과 해석에 이르기까지 광범위한 결함이 존재합니다. 특히, 특정 인구 집단을 소외시키는 데이터셋(dataset) 구성은 모델의 성능 저하를 넘어 사회적 불평등을 재생산할 위험을 안고 있습니다. 재현 불가능성(irreproducibility)으로 이어질 수 있는 문제로는 부적절한 데이터 정제, 비대표적인 샘플(sample), 특정 알고리즘(algorithm) 선택에 민감한 결과, 그리고 모델(model)의 공정성(fairness)을 보고하지 않는 것 등이 있습니다. 또한 개발자들이 자신의 데이터(data) 출처와 처리 과정을 공개하지 않아 투명성(transparency)을 저해하는 근본적인 문제도 있습니다. 예를 들어, 최근 한 연구는 AI 윤리 보고서에서 데이터셋의 다양성 부족과 편향성 문제를 언급한 기업이 극히 드물었음을 지적했습니다.

뿌리 깊은 문제 인공지능(AI) 이전에도 많은 기술 분야는 사회적 영향력 및 책임성 위기에 직면해 있었습니다. 근본 원인으로는 기술 개발의 속도만을 중시하는 문화, 긍정적인 기술 성과 발표에 대한 강한 편향(그리고 부정적인 사회적 영향 보고의 거의 불가능함), 잘못된 기술 적용을 반박할 유인 부족, 그리고 윤리적 문제를 간과해도 아무런 결과가 없는 점 등이 있습니다. 예를 들어, 잘못된 알고리즘(algorithm)은 거의 수정되지 않습니다. 동료들은 윤리적 실패조차 인지하지 못하는 것 같습니다. 한 알고리즘(algorithm)이 차별적 결과를 낳은 후, 이를 지적하는 논문 중 단 3%만이 그 문제를 인용했습니다. 과학 커뮤니케이터(communicator)들은 기술이 스스로 수정된다고 주장하기를 좋아하지만, 저희의 경험상 자기 수정은 사실상 존재하지 않습니다.

이러한 모든 문화적 요인들은 인공지능(AI) 개발 분야에서도 나타납니다. 그러나 인공지능(AI)은 사회적 영향에 대해 우리가 회의적이어야 하는 여러 가지 추가적인 이유를 제시합니다. 대규모 모델의 에너지 소비(energy consumption)는 악명 높게 까다로우며, 지속 가능성(sustainability)과 같은 많은 측면은 미해결 연구 분야입니다. 또한, 인공지능(AI) 모델의 학습(training) 과정은 전통적인 소프트웨어(software) 개발보다 훨씬 더 복잡하고 덜 표준화되는 경향이 있습니다. 윤리 심사자(ethics reviewer)의 역할이 에너지 효율을 검토하는 것이 아니기 때문에 환경적 영향 평가는 거의 이루어지지 않습니다. 그러나 저희는 기술의 사회적 책임이 낮은 가장 큰 이유가 만연한 상업적 이익 추구이며, 이는 개발자들 사이에서 비판적인 사고방식의 부족으로 이어지고, 이는 좋은 기술적 실천의 초석이 된다고 생각합니다. 저희는 개발자들이 지나치게 낙관적인 기대를 가지고 있고 그들의 인공지능(AI) 모델(model)이 예상치 못한 부작용을 일으킬 때, 그들은 자신들이 뭔가 잘못했다고 가정하고 모델(model)을 수정하지만, 사실은 시스템의 본질적인 한계에 부딪혔을 가능성을 강력히 고려해야 한다는 것을 관찰했습니다. 반대로, 모델(model)이 사회적으로 잘 작동하는 것처럼 보일 때는 쉽게 믿는 경향이 있는데, 사실은 숨겨진 편향이나 다른 결함에 대해 경계를 늦추지 않아야 합니다. 그리고 모델(model)이 예상보다 더 사회적으로 긍정적인 영향을 미치면, 그들은 인간이 생각할 수 없었던 기술적 패턴(pattern)을 발견했다고 가정하며, AI가 모든 문제를 해결할 수 있다는 신화는 이러한 설명을 쉽게 그럴듯하게 만듭니다.

이것은 피드백 루프(feedback loop)입니다. 지나친 낙관주의는 사회적 책임을 간과하는 기술 개발을 부추기고, 이는 해당 분야의 다른 개발자들에게 AI가 무엇을 할 수 있고 무엇을 할 수 없는지에 대해 잘못된 정보를 제공합니다. 사실, 저희는 좌절한 개발자들과의 개인적인 서신에서 이러한 극단적인 경우를 접했습니다. 결함 있는 시스템이 수정되지 않기 때문에, "최첨단(state of the art)"을 능가하지 못하는 안전한 모델(model)을 초래하는 좋은 기술을 개발하는 것은 말 그대로 불가능해집니다. 도구가 강력하고 블랙박스(black-box)일수록 오류와 과신(overconfidence)의 가능성이 커집니다. 심리학, 의학 등에서의 반복성(replication) 위기는 평범한 옛 통계의 오용에서 비롯된 것이었습니다. 인공지능(AI)이 상대적으로 얼마나 새로운지를 고려할 때, 저희는 인공지능(AI) 기반 시스템의 사회적 책임 위기가 나아지기 전에 한동안 더 악화될 것이라고 추측합니다. 그리고 이제 개발자들은 대규모 언어 모델(large language models)과 생성형 AI(generative AI)를 수용하고 있는데, 이는 이해의 환상(illusion of understanding)과 같은 많은 새로운 함정을 열어줍니다.

저희 책에 대한 블로그(blog)인 **AI 스네이크 오일(AI Snake Oil)**을 읽고 계십니다. AI 윤리에 대한 더 깊은 통찰을 얻으려면 구독하세요. 구독하기

희망의 빛 인공지능(AI) 기반 기술의 한 가지 좋은 점은 일반적으로 인간에게 직접적인 해를 가하는 실험이 아니라 알고리즘(algorithm)과 데이터(data) 분석만을 포함한다는 것입니다. 따라서 다른 개발자들은 원칙적으로 모델(model)의 코드(code)와 학습 데이터(training data)를 다운로드하여 보고된 공정성(fairness)과 투명성(transparency)을 확인할 수 있어야 합니다. 또한 코드(code)에서 윤리적 오류나 문제가 있는 선택이 있는지 검토할 수도 있습니다. 이는 시간이 많이 걸리지만, 사회 전체에 미칠 파급 효과를 고려할 때 훨씬 덜합니다. 사회 전체에 미치는 영향은 일반적으로 원본 개발만큼 비용이 많이 듭니다. 또 다른 좋은 점은 개발자들이 무엇을 주의해야 할지 안다면 대부분의 윤리적 문제를 피할 수 있다는 것입니다. 대조적으로, 사전 규제(pre-regulation)와 같은 신기술의 사회적 책임 위기에 대한 완화책은 효과 면에서 훨씬 더 불규칙한 실적을 보입니다. 따라서 저희는 개발자들이 자신의 기술에 체계적으로 더 많은 주의를 기울이고 윤리적 책임 연구에 인센티브(incentive)가 부여되는 문화 변화를 통해 이 문제가 크게 완화될 수 있다고 생각합니다. 인공지능(AI) 방법론 커뮤니티(community)는 이미 책임 있는 AI 개발 원칙(responsible AI development principles, 수십 년 된 방법)과 윤리적 AI 챌린지(ethical AI challenge, 비교적 최근의 방법)를 통해 이러한 방향으로 나아가고 있지만, 인공지능(AI) 기반 기술, 즉 각 분야에서 인공지능(AI) 모델(model)을 사용하여 사회적 가치를 발전시키는 의료나 교육과 같은 분야에서는 아직 이러한 변화가 일어나지 않았습니다.

저희는 이를 바꾸기 위한 몇 가지 노력을 주도했습니다. 첫째, 저희의 책임 있는 AI 관련 활동은 영향력을 가졌습니다. 개발자들이 모델(model)을 구축하는 방법과 편향성(bias)의 부재를 문서화하고 입증하는 데 사용되었습니다. 출판된 연구에서 윤리적 문제를 찾으려는 개발자들에 의해 사용되었습니다. 또한 AI 윤리 연구의 중요성을 강조하고 분야별 가이드라인(guideline)을 마련하는 방법으로도 사용되었습니다. 편향성 외에도, 저희는 컴퓨터 과학, 데이터 과학, 사회 과학, 법률 및 정책 연구 분야의 19명의 연구자 그룹을 이끌어 인공지능(AI) 기반 기술을 위한 TRUST(Transparency, Responsibility, Understanding, Safety, Trustworthiness) 체크리스트(checklist)를 개발했습니다. 이는 인공지능(AI) 기반 기술에서 흔히 발생하는 여덟 가지 함정을 개발자들이 파악하는 데 도움이 될 수 있는 32개 항목의 체크리스트(checklist)이며, 편향성은 그중 하나에 불과합니다. 이 체크리스트(checklist)는 최근 주요 기술 윤리 저널에 게재되었습니다. 물론 문화 변화가 없다면 체크리스트(checklist)만으로는 도움이 되지 않겠지만, 지금까지의 반응을 볼 때 저희는 조심스럽게 낙관하고 있습니다.

결론 저희의 요점은 AI가 인류에게 해롭다는 것이 아닙니다. 저희 자신도 AI에 관한 연구가 아닐 때조차 AI를 도구(tool)로 자주 사용합니다. 핵심 단어는 '도구(tool)'입니다. AI는 혁명이 아닙니다. AI는 인간의 판단을 대체하는 것이 아닙니다. 그렇게 생각하는 것은 기술 윤리의 본질을 놓치는 것입니다. AI는 기술 개발에 내재된 복잡한 사회적 숙고와 책임에 대한 지름길을 제공하지 않습니다. AI는 신탁이 아니며 미래를 볼 수 없습니다. 안타깝게도 대부분의 기술 분야는 AI 과장 광고(hype)에 굴복하여 상식의 정지를 초래했습니다. 예를 들어, 한 핀테크(fintech) 연구 분야는 대출 부도율 예측에서 90%가 훨씬 넘는 정확도(accuracy)로 예측한다고 주장했는데, 이는 표면적으로 불가능하게 들리는 수치입니다. (이는 데이터 편향과 과적합(overfitting)으로 밝혀졌으며, 이것이 저희가 이 연구 분야 전체에 관심을 갖게 된 계기였습니다.)

저희는 기술 발전 역사상 흥미로운 순간에 있습니다. 다양한 산업에서 AI 채택을 보여주는 다음 그래프를 보십시오. 3

**산업별 AI 관련 프로젝트 비율, 2018–2023년.**
(출처: Duede et al. 2024)

이러한 하키 스틱(hockey stick) 그래프는 좋은 소식이 아닙니다. 오히려 섬뜩해야 합니다. AI를 채택하려면 기술 윤리(tech ethics)에 대한 근본적인 변화가 필요합니다. 4 어떤 기술 분야도 몇 년이라는 짧은 시간 안에 이를 달성할 능력이 없습니다. 이것은 도구나 방법이 유기적으로 채택될 때 일어나는 일이 아닙니다. 개발자들이 자금을 얻기 위해 유행에 편승할 때 일어나는 일입니다. 과장 광고(hype)의 수준을 고려할 때, 개발자들은 AI를 채택하기 위해 추가적인 인센티브(incentive)가 필요하지 않습니다. 이는 기술 윤리를 위한 AI 자금 지원 프로그램이 상황을 악화시키고 있을 가능성이 높다는 것을 의미합니다. 저희는 결함 있는 시스템의 홍수를 막을 수 있을지 의심스럽지만, 기술을 위한 AI 자금의 일부라도 더 나은 훈련, 비판적 탐구, 메타 윤리(meta-ethics), 투명성(transparency) 및 기타 품질 관리 노력으로 전환된다면 혼란을 최소화할 수 있습니다.

저희 책 **AI 스네이크 오일(AI Snake Oil)**은 현재 사전 주문이 가능합니다. 저희 블로그(blog)를 즐겁게 읽으셨고 저희 작업을 지원하고 싶으시다면, 아마존(Amazon), 북샵(Bookshop) 또는 즐겨 찾는 서점을 통해 사전 주문해 주십시오.

1.  분명히 하자면, 반복 실패가 반드시 원본 기술의 결함을 의미하는 것은 아닙니다. 이 게시물에서 저희의 주된 관심사는 데이터 편향과 같은 비교적 명확한 오류에 관한 것입니다.
2.  여기서 정확도(accuracy)는 F1 점수(F1 Score)라는 지표를 의미합니다. 한 가지 결과(대출 상환)가 다른 결과(대출 부도)보다 훨씬 더 흔할 때조차 기준선(baseline) F1 점수는 50%입니다.
3.  해당 논문은 다양한 유형의 AI "참여"를 함께 묶습니다. 참여는 새로운 AI 이론 및 접근 방식, 기술 또는 응용 프로그램의 개발; 도메인(domain)별 작업을 위한 AI 모델(model)의 일반적인 사용; 그리고 철학 및 윤리와 같은 분야의 학술 담론으로 대표되는 AI에 대한 비판적 참여를 포함할 수 있습니다(이에 국한되지 않음). 저희의 목적상 이는 유감스러운 일인데, 저희의 관심사는 오직 두 번째 범주, 즉 도메인(domain)별 작업을 위한 AI 사용에만 있기 때문입니다. 저희는 컴퓨터 과학 및 철학과 같은 몇몇 분야를 제외하고는 대부분의 AI 참여가 이 범주에 속한다고 생각합니다.
4.  특히, "모든 모델(model)은 틀렸지만 일부 모델(model)은 유용하다"는 말처럼, 모델(model)을 기반으로 사회적 결론을 언제 도출할 수 있는지에 대한 명확한 답은 없으므로, 모든 분야와 모든 유형의 모델(model)에 대해 타당성(validity)을 재검토해야 합니다.