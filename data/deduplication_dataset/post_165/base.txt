# **과학자들은 AI를 신탁이 아닌 도구로 사용해야 한다**

Author: AI Snake Oil
URL: https://www.normaltech.ai/p/scientists-should-use-ai-as-a-tool

============================================================

누가 AI 과장 광고를 만들어내는가? 저희의 **AI 스네이크 오일(AI Snake Oil)** 책에서 논의했듯이, 기업과 언론뿐만 아니라 AI 연구자들도 그렇습니다. 예를 들어, 2023년 12월 네이처(Nature)지에 발표되어 널리 알려진 두 편의 논문은 AI를 사용하여 220만 개 이상의 새로운 물질을 발견하고 그중 41개를 로봇으로 합성했다고 주장했습니다. 안타깝게도, 이러한 주장은 빠르게 반박되었습니다. "생산된 [41개] 물질 대부분은 잘못 식별되었으며, 나머지는 이미 알려진 것이었습니다." 대규모 데이터셋(dataset)의 경우, 250개 화합물 샘플(sample)을 조사한 결과 대부분이 쓸모없는 데이터(junk)임이 드러났습니다. 기계 학습(machine learning)의 핵심 강점 중 하나는 이해 없이 발견하는 것인데, 이것이 기계 학습 기반 과학에서 오류가 특히 흔한 이유입니다. 3년 전, 저희는 시험을 위한 학습(teaching to the test)의 기계 학습(machine learning) 버전인 누출(leakage)이라는 오류가 만연하여 17개 분야의 수백 편의 논문에 영향을 미쳤다는 증거를 수집했습니다. 그 이후로 저희는 이 문제를 더 잘 이해하고 해결책을 고안하기 위해 노력해 왔습니다. 이 게시물은 업데이트된 내용을 제공합니다. 요컨대, 저희는 상황이 나아지기 전에 더 나빠질 것이라고 생각하지만, 희망의 빛도 보입니다.

참사는 계속됩니다.
저희의 가장 최근 자료 취합에 따르면, 연구자들이 출판된 연구에서 누출(leakage)을 발견한 분야의 수는 30개에 달했습니다. 대다수는 의학 분야인데, 이는 의학 연구의 오류가 특히 중대한 결과를 초래할 수 있기 때문에 의학 분야가 모범 사례를 확립하고 이전에 출판된 연구를 비판적으로 검토하는 데 훨씬 더 많은 노력을 기울이는 경향이 있기 때문이라고 강력히 추정합니다. 모든 분야에 걸쳐 약 650편의 논문이 영향을 받았는데, 이는 엄청나게 과소평가된 수치라고 저희는 가정합니다. 연구자들이 체계적으로 누출(leakage)을 찾을 때, 많은 분야에서 샘플(sample)로 추출된 연구의 대다수가 누출(leakage) 오류를 범하고 있음을 발견하기 때문입니다.

누출(leakage)은 재현성(reproducibility) 실패의 여러 원인 중 하나입니다. 기계 학습(ML) 기반 과학의 모든 단계, 즉 데이터(data) 수집부터 전처리(preprocessing) 및 결과 보고에 이르기까지 광범위한 결함이 존재합니다. 재현 불가능성(irreproducibility)으로 이어질 수 있는 문제로는 부적절한 기준선(baseline) 비교, 비대표적인 샘플(sample), 특정 모델링(modeling) 선택에 민감한 결과, 그리고 모델(model) 불확실성(uncertainty)을 보고하지 않는 것 등이 있습니다. 또한 연구자들이 자신의 코드(code)와 데이터(data)를 공개하지 않아 재현성(reproducibility)을 저해하는 근본적인 문제도 있습니다. 예를 들어, Gabelica 등은 2019년 1월 바이오메드 센트럴(BioMed Central)에 등재된 333개의 오픈 액세스(open-access) 저널을 조사한 결과, 요청 시 데이터를 공유하겠다고 약속한 1,800편의 논문 중 93%가 그렇게 하지 않았음을 발견했습니다.

뿌리 깊은 문제
기계 학습(ML) 이전에도 많은 과학 분야는 재현성(reproducibility) 및 반복성(replicability) 위기에 직면해 있었습니다. 근본 원인으로는 과학계의 '출판 아니면 도태(publish-or-perish)' 문화, 긍정적인 결과(positive results) 출판에 대한 강한 편향(그리고 부정적인 결과(negative results) 출판의 거의 불가능함), 잘못된 연구를 반박할 유인 부족, 그리고 부실한 연구를 출판해도 아무런 결과가 없는 점 등이 있습니다. 예를 들어, 잘못된 논문은 거의 철회되지 않습니다. 동료들은 반복 실패조차 인지하지 못하는 것 같습니다. 한 논문이 반복에 실패한 후, 인용하는 논문 중 단 3%만이 그 반복 시도를 인용했습니다. 1 과학 커뮤니케이터(communicator)들은 과학이 스스로 수정된다고 주장하기를 좋아하지만, 저희의 경험상 자기 수정은 사실상 존재하지 않습니다.

이러한 모든 문화적 요인들은 기계 학습(ML) 기반 과학에서도 나타납니다. 그러나 기계 학습(ML)은 출판된 결과에 대해 우리가 회의적이어야 하는 여러 가지 추가적인 이유를 제시합니다. 성능 평가(performance evaluation)는 악명 높게 까다로우며, 불확실성 정량화(uncertainty quantification)와 같은 많은 측면은 미해결 연구 분야입니다. 또한, 기계 학습(ML) 코드(code)는 전통적인 통계 모델링(statistical modeling)보다 훨씬 더 복잡하고 덜 표준화되는 경향이 있습니다. 동료 심사자(peer reviewer)의 역할이 코드(code)를 검토하는 것이 아니기 때문에 코딩(coding) 오류는 거의 발견되지 않습니다. 그러나 저희는 연구 품질이 낮은 가장 큰 이유가 만연한 과장 광고(hype)이며, 이는 연구자들 사이에서 회의적인 사고방식의 부족으로 이어지고, 이는 좋은 과학적 실천의 초석이 된다고 생각합니다. 저희는 연구자들이 지나치게 낙관적인 기대를 가지고 있고 그들의 기계 학습(ML) 모델(model)이 제대로 작동하지 않을 때, 그들은 자신들이 뭔가 잘못했다고 가정하고 모델(model)을 수정하지만, 사실은 예측 가능성(predictability)의 본질적인 한계에 부딪혔을 가능성을 강력히 고려해야 한다는 것을 관찰했습니다. 반대로, 모델(model)이 잘 작동할 때는 쉽게 믿는 경향이 있는데, 사실은 누출(leakage)이나 다른 결함에 대해 경계를 늦추지 않아야 합니다. 그리고 모델(model)이 예상보다 더 잘 작동하면, 그들은 인간이 생각할 수 없었던 데이터(data)의 패턴(pattern)을 발견했다고 가정하며, AI가 외계 지능이라는 신화는 이러한 설명을 쉽게 그럴듯하게 만듭니다.

이것은 피드백 루프(feedback loop)입니다. 지나친 낙관주의는 결함 있는 연구를 부추기고, 이는 해당 분야의 다른 연구자들에게 AI가 무엇을 할 수 있고 무엇을 할 수 없는지에 대해 잘못된 정보를 제공합니다. 사실, 저희는 좌절한 연구자들과의 개인적인 서신에서 이러한 극단적인 경우를 접했습니다. 결함 있는 연구가 수정되지 않기 때문에, "최첨단(state of the art)"을 능가하지 못하는 모델(model)을 초래하는 좋은 연구를 출판하는 것은 말 그대로 불가능해집니다. 도구가 강력하고 블랙박스(black-box)일수록 오류와 과신(overconfidence)의 가능성이 커집니다. 심리학, 의학 등에서의 반복성(replication) 위기는 평범한 옛 통계의 오용에서 비롯된 것이었습니다. 기계 학습(ML)이 상대적으로 얼마나 새로운지를 고려할 때, 저희는 기계 학습(ML) 기반 과학의 재현성(reproducibility) 위기가 나아지기 전에 한동안 더 악화될 것이라고 추측합니다. 그리고 이제 과학자들은 대규모 언어 모델(large language models)과 생성형 AI(generative AI)를 수용하고 있는데, 이는 이해의 환상(illusion of understanding)과 같은 많은 새로운 함정을 열어줍니다.

저희 책에 대한 블로그(blog)인 **AI 스네이크 오일(AI Snake Oil)**을 읽고 계십니다. 새 게시물을 받으려면 구독하세요. 구독하기

희망의 빛
기계 학습(ML) 기반 과학의 한 가지 좋은 점은 일반적으로 사람을 대상으로 한 실험이 아니라 데이터 분석(data analysis)만을 포함한다는 것입니다. 따라서 다른 연구자들은 원칙적으로 논문의 코드(code)와 데이터(data)를 다운로드하여 보고된 결과를 재현할 수 있는지 확인할 수 있어야 합니다. 또한 코드(code)에서 오류나 문제가 있는 선택이 있는지 검토할 수도 있습니다. 이는 시간이 많이 걸리지만, 심리학이나 의학 연구를 반복하는 것보다는 훨씬 덜합니다. 심리학이나 의학 연구는 일반적으로 원본 연구만큼 비용이 많이 듭니다. 또 다른 좋은 점은 연구자들이 무엇을 주의해야 할지 안다면 대부분의 오류를 피할 수 있다는 것입니다. 대조적으로, 사전 등록(pre-registration)과 같은 통계 과학의 반복성(replication) 위기에 대한 완화책은 효과 면에서 훨씬 더 불규칙한 실적을 보입니다. 따라서 저희는 연구자들이 자신의 연구에 체계적으로 더 많은 주의를 기울이고 재현성(reproducibility) 연구에 인센티브(incentive)가 부여되는 문화 변화를 통해 이 문제가 크게 완화될 수 있다고 생각합니다. 기계 학습(ML) 방법론 커뮤니티(community)는 이미 공통 과제 방법(common task method, 수십 년 된 방법)과 재현성 챌린지(reproducibility challenge, 비교적 최근의 방법)를 통해 이러한 방향으로 나아가고 있지만, 기계 학습(ML) 기반 과학, 즉 각 분야에서 기계 학습(ML) 모델(model)을 사용하여 지식을 발전시키는 의학이나 심리학과 같은 분야에서는 아직 이러한 변화가 일어나지 않았습니다.

저희는 이를 바꾸기 위한 몇 가지 노력을 주도했습니다. 첫째, 저희의 누출(leakage) 관련 논문은 영향력을 가졌습니다. 연구자들이 모델(model)을 구축하는 방법과 누출(leakage)의 부재를 문서화하고 입증하는 데 사용되었습니다. 출판된 연구에서 누출(leakage)을 찾으려는 연구자들에 의해 사용되었습니다. 또한 누출(leakage) 연구의 중요성을 강조하고 분야별 가이드라인(guideline)을 마련하는 방법으로도 사용되었습니다. 누출(leakage) 외에도, 저희는 컴퓨터 과학, 데이터 과학, 사회 과학, 수학 및 생의학 연구 분야의 19명의 연구자 그룹을 이끌어 기계 학습(ML) 기반 과학을 위한 REFORMS 체크리스트(checklist)를 개발했습니다. 이는 기계 학습(ML) 기반 과학에서 흔히 발생하는 여덟 가지 함정을 연구자들이 파악하는 데 도움이 될 수 있는 32개 항목의 체크리스트(checklist)이며, 누출(leakage)은 그중 하나에 불과합니다. 이 체크리스트(checklist)는 최근 사이언스 어드밴시스(Science Advances)에 게재되었습니다. 물론 문화 변화가 없다면 체크리스트(checklist)만으로는 도움이 되지 않겠지만, 지금까지의 반응을 볼 때 저희는 조심스럽게 낙관하고 있습니다.

결론
저희의 요점은 AI가 과학자들에게 쓸모없다는 것이 아닙니다. 저희 자신도 AI에 관한 연구가 아닐 때조차 AI를 도구(tool)로 자주 사용합니다. 핵심 단어는 '도구(tool)'입니다. AI는 혁명이 아닙니다. AI는 인간의 이해를 대체하는 것이 아닙니다. 그렇게 생각하는 것은 과학의 본질을 놓치는 것입니다. AI는 연구에 내재된 고된 작업과 좌절에 대한 지름길을 제공하지 않습니다. AI는 신탁이 아니며 미래를 볼 수 없습니다. 안타깝게도 대부분의 과학 분야는 AI 과장 광고(hype)에 굴복하여 상식의 정지를 초래했습니다. 예를 들어, 정치학의 한 연구 분야는 내전 발발을 90%가 훨씬 넘는 정확도(accuracy) 2로 예측한다고 주장했는데, 이는 표면적으로 불가능하게 들리는 수치입니다. (이는 누출(leakage)로 밝혀졌으며, 이것이 저희가 이 연구 분야 전체에 관심을 갖게 된 계기였습니다.)

저희는 과학 역사상 흥미로운 순간에 있습니다. 다양한 분야에서 AI 채택을 보여주는 다음 그래프를 보십시오. 3

**분야별 AI 관련 논문 비율, 1985–2023년.**
(출처: Duede et al. 2024)

이러한 하키 스틱(hockey stick) 그래프는 좋은 소식이 아닙니다. 오히려 섬뜩해야 합니다. AI를 채택하려면 과학적 인식론(epistemology)에 대한 변화가 필요합니다. 4 어떤 과학 분야도 몇 년이라는 짧은 시간 안에 이를 달성할 능력이 없습니다. 이것은 도구나 방법이 유기적으로 채택될 때 일어나는 일이 아닙니다. 과학자들이 자금을 얻기 위해 유행에 편승할 때 일어나는 일입니다. 과장 광고(hype)의 수준을 고려할 때, 과학자들은 AI를 채택하기 위해 추가적인 인센티브(incentive)가 필요하지 않습니다. 이는 과학을 위한 AI 자금 지원 프로그램이 상황을 악화시키고 있을 가능성이 높다는 것을 의미합니다. 저희는 결함 있는 연구의 홍수를 막을 수 있을지 의심스럽지만, 과학을 위한 AI 자금의 일부라도 더 나은 훈련, 비판적 탐구, 메타 과학(meta-science), 재현성(reproducibility) 및 기타 품질 관리 노력으로 전환된다면 혼란을 최소화할 수 있습니다.

저희 책 **AI 스네이크 오일(AI Snake Oil)**은 현재 사전 주문이 가능합니다. 저희 블로그(blog)를 즐겁게 읽으셨고 저희 작업을 지원하고 싶으시다면, 아마존(Amazon), 북샵(Bookshop) 또는 즐겨 찾는 서점을 통해 사전 주문해 주십시오.

1 분명히 하자면, 반복 실패가 반드시 원본 연구의 결함을 의미하는 것은 아닙니다. 이 게시물에서 저희의 주된 관심사는 누출(leakage)과 같은 비교적 명확한 오류에 관한 것입니다.
2 여기서 정확도(accuracy)는 AUC라는 지표를 의미합니다. 한 가지 결과(평화)가 다른 결과(전쟁)보다 훨씬 더 흔할 때조차 기준선(baseline) AUC는 50%입니다.
3 해당 논문은 다양한 유형의 AI "참여"를 함께 묶습니다. 참여는 새로운 AI 이론 및 접근 방식, 기술 또는 응용 프로그램의 개발; 도메인(domain)별 작업을 위한 AI 모델(model)의 일반적인 사용; 그리고 철학 및 윤리와 같은 분야의 학술 담론으로 대표되는 AI에 대한 비판적 참여를 포함할 수 있습니다(이에 국한되지 않음). 저희의 목적상 이는 유감스러운 일인데, 저희의 관심사는 오직 두 번째 범주, 즉 도메인(domain)별 작업을 위한 AI 사용에만 있기 때문입니다. 저희는 컴퓨터 과학 및 철학과 같은 몇몇 분야를 제외하고는 대부분의 AI 참여가 이 범주에 속한다고 생각합니다.
4 특히, "모든 모델(model)은 틀렸지만 일부 모델(model)은 유용하다"는 말처럼, 모델(model)을 기반으로 세상에 대한 결론을 언제 도출할 수 있는지에 대한 명확한 답은 없으므로, 모든 분야와 모든 유형의 모델(model)에 대해 타당성(validity)을 재검토해야 합니다.