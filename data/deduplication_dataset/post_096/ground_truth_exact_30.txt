YouTube에서 시청하거나, Apple Podcast, Spotify에서 듣거나, 즐겨 사용하는 플레이어에 저희 RSS 피드를 추가하세요!

Chris Lattner가 처음 팟캐스트에 출연했을 때, 저희는 에피소드 제목을 '어려운 길을 가다(Doing it The Hard Way)'로 정했습니다. 단순히 추론 플랫폼을 구축하고 GPU가 쌩쌩 돌아가는 차트를 게시하는 대신, 그들은 컴파일러(compiler) 수준까지 내려가 전체 스택(stack)을 처음부터 다시 구축했습니다. Modular는 Chris가 부르는 대로 '동심원(concentric circles)' 구조로 구축되고 있습니다. 필요한 레이어(layer)로 바로 들어가면 되고, 그 아래의 모든 것은 건드릴 필요가 없는 선택적인 기반 시설(plumbing)입니다.

**Modular의 비전과 최근 성장**

Modular는 AI 시대에 필요한 컴퓨팅의 형태를 재정의하려는 야심찬 비전을 가지고 있습니다. 초기 3년의 연구 개발(R&D) 단계를 거쳐, 이제는 실제 제품과 솔루션을 시장에 선보이며 눈부신 성장을 이어가고 있습니다.

초기 R&D 단계에서는 이기종 컴퓨팅(heterogeneous compute)의 잠재력을 최대한 활용하고, AI 스택 전반에 걸쳐 풀스택(full stack) 혁신을 가능하게 하는 데 주력했습니다. 첫해는 컴파일 철학을 증명하는 데 집중했으며, 2년차에는 Mojo 언어 개발과 함께 인텔 CPU에서 OpenVINO와 같은 기존 솔루션을 능가하는 CPU용 AI 프레임워크를 구축했습니다. 3년차에는 GPU 지원과 LLM 서빙에 도전하며 기술적 이정표를 세웠습니다. 이는 단순히 기존 기술을 개선하는 것을 넘어, 완전히 새로운 접근 방식을 통해 근본적인 복잡성을 해소하는 것을 목표로 했습니다. 이러한 노력은 Mojo 언어와 MAX 추론 플랫폼의 기반이 되었습니다.

최근 Modular는 AMD와의 협력을 통해 주목할 만한 성과를 발표했습니다. 몇몇 인기 있는 오픈 소스 모델에서 vLLM으로 실행되는 NVIDIA H200과 AMD MI325의 성능이 거의 동등하다는 것을 입증한 것입니다. 이는 NVIDIA의 GPU 시장 지배력에 도전하는 중요한 진전으로 평가받고 있습니다. Modular는 H100 및 MI300/325 지원을 넘어, 미래의 Blackwell 아키텍처까지 아우르는 확장성을 제공하며, 6주마다 새로운 릴리스를 통해 함수 호출(function calling), 토큰 샘플링(token sampling), 에이전틱 워크플로우(agentic workflows)와 같은 최신 기능을 빠르게 추가하고 있습니다.

이러한 발전은 단순한 기술적 진보를 넘어, AI 개발자들이 직면하는 복잡성을 줄이고 생산성을 극대화하는 데 기여하고 있습니다. Modular는 이제 연구 단계를 넘어 엔지니어링 문제 해결에 집중하며, 더 많은 개발자가 AI 컴퓨팅의 혁신에 참여할 수 있도록 문을 열고 있습니다. 최근 해커톤에서는 Mojo와 MAX를 활용하여 GPU 프로그래밍 경험이 없는 팀이 하루 만에 훈련 시스템을 구축하는 놀라운 성과를 보여주기도 했습니다. 이는 Modular의 기술이 얼마나 '해킹하기 쉬운(hackable)'지, 그리고 AI 코딩 도구와 결합될 때 얼마나 강력한 시너지를 낼 수 있는지를 증명합니다.

**Mojo 언어의 혁신: AI 시대의 '한 번 작성하면 어디서든 실행(Write Once, Run Anywhere)'**

이전 에피소드에서 Mojo의 설계에 대해 많이 다루었으니, 복습이 필요하다면 해당 에피소드를 참조하세요.

Mojo의 독창성 핵심에는 한 가지가 있습니다. Mojo는 모든 가속기 명령어(텐서 코어(tensor cores), TMA 등)를 파이썬에 친숙한(Python-familiar) 구문으로 노출하여 다음을 가능하게 합니다.

*   한 번 작성하면 H100, MI300 또는 미래의 Blackwell로 리타겟팅(re-target)할 수 있습니다.
*   CPython보다 10-100배 빠르게 실행되며, 동일한 알고리즘에서 보통 Rust를 능가합니다.
*   바인딩이 필요 없는(zero-binding) 확장 언어로 작동합니다. `import my_fast_fn.mojo`를 사용하여 일반 파이썬에서 호출할 수 있습니다.

이를 통해 Triton/CUDA 커널(kernel)과 C++ '글루(glue)' 코드가 필요 없어집니다.

Mojo는 단순히 파이썬의 속도 한계를 극복하는 것을 넘어, AI 시대의 고성능 컴퓨팅을 위한 새로운 패러다임을 제시합니다. 기존에는 파이썬의 편리함과 GPU의 성능을 동시에 얻기 위해 PyBind11, NanoBind와 같은 복잡한 바인딩 계층이나 C++ "글루(glue)" 코드가 필수적이었습니다. 하지만 Mojo는 이러한 오버헤드 없이 파이썬 코드 내에서 직접 GPU의 저수준 기능에 접근하고 최적화할 수 있는 길을 엽니다. Mojo는 파이썬이 가진 모든 기능을 가지고 있지는 않으며, 특히 클래스가 없지만 함수와 임의의 파이썬 객체를 사용할 수 있어 바인딩 없는 경험을 제공합니다. 이는 파이썬보다 빠르면서도 약간 더 제한적인 파이썬으로 볼 수 있으며, AI 생태계를 넘어 파이썬 전반에 걸쳐 유용하게 활용될 수 있습니다. 특히 GPU에서 실행되는 고성능 작업, 웹 서버 내 연속 배치 처리, 해싱 등 성능이 중요한 영역에서 Mojo는 탁월한 선택입니다. 또한, CPU의 텐서 코어, SIMD 등 다양한 기능을 활용하여 코드를 빠르게 만들고, 이를 여러 GPU에 확장할 수 있어 Rust와 같은 언어가 제공하기 어려운 유연성을 제공합니다.

Mojo의 핵심 강점 중 하나는 MLIR(Multi-Level Intermediate Representation) 기반의 컴파일러 아키텍처입니다. 이를 통해 Mojo 코드는 다양한 하드웨어 백엔드(GPU, CPU, NPU 등)에 대해 최적화된 기계어 코드로 변환될 수 있으며, 텐서 코어(Tensor Cores)나 TMA(Tensor Memory Accelerator)와 같은 특정 가속기 명령어까지 활용할 수 있습니다. 이는 개발자가 하드웨어 아키텍처에 종속되지 않고, 한 번 작성한 코드를 다양한 가속기에서 고성능으로 실행할 수 있게 합니다.

또한 Mojo는 컴파일 타임 메타프로그래밍(compile-time metaprogramming) 기능을 제공하여, 개발자가 코드 생성 및 최적화 로직을 컴파일 시점에 정의할 수 있도록 합니다. 이는 런타임 오버헤드를 줄이고, 특정 하드웨어 또는 사용 사례에 최적화된 코드를 동적으로 생성하는 데 필수적입니다. 결과적으로 Mojo는 CPython보다 수만 배 빠른 성능을 달성할 수 있으며, Rust와 같은 시스템 프로그래밍 언어와 비교해도 손색없는, 혹은 그 이상의 성능을 제공합니다. 이는 특히 AI 모델의 복잡한 연산, 즉 행렬 곱셈(matrix multiplication), 합성곱(convolution), 어텐션 메커니즘(attention mechanism)과 같은 핵심 연산에서 두드러집니다.

**MAX 추론 플랫폼의 진화: 최적화된 AI 모델 서빙의 미래**

동일한 접근 방식이 MAX 추론 플랫폼에도 적용되었습니다.

*   Max의 기본 이미지는 파이썬 디스패치 경로(Python dispatch path)를 제거했기 때문에 약 1GB입니다.
*   플래시어텐션(Flash-Attention), 페이징된 KV(paged KV), 딥시크 스타일 MLA(DeepSeek style MLA) 등 필요한 모든 최적화가 포함된 사전 빌드된 컨테이너(container)로 약 500개의 모델을 지원합니다.
*   가장 중요한 것은 오픈 소스 커뮤니티의 참여를 실제로 이끌어내기 위해 나이틀리 빌드(nightly builds)를 배포한다는 것입니다.

MAX는 Mojo를 기반으로 구축된 차세대 AI 추론 프레임워크입니다. 그 핵심 목표는 대규모 언어 모델(LLM) 및 생성형 AI 모델의 배포 및 서빙을 극도로 효율적이고 유연하게 만드는 것입니다. MAX의 경량 컨테이너 이미지는 불필요한 의존성을 제거하여 시작 시간을 단축하고 리소스 사용량을 최소화합니다. 이는 특히 온디맨드(on-demand) 스케일링이 중요한 클라우드 환경에서 큰 이점을 제공합니다. Max는 모델 레벨에 순수 파이썬을 사용하며, API는 매우 간단하여 PyTorch처럼 느껴집니다. 어텐션 블록이나 연산을 구성할 수 있으며, Mojo와 직접적으로 통합되어 다른 프레임워크에서는 얻을 수 없는 완전한 통합과 자동 커널 퓨전(automatic kernel fusion) 같은 이점을 제공합니다.

MAX는 단일 노드 성능 최적화를 넘어 클러스터 규모의 추론 환경에서도 강력한 기능을 발휘합니다. 지능형 라우팅(intelligent routing), 접두사 캐싱(prefix caching), 비동기적 프리필(disaggregated pre-fill)과 같은 고급 기술들을 통합하여 여러 GPU 및 서버에 걸쳐 LLM 추론을 효율적으로 분산하고 관리합니다. 이러한 기능들은 LLM 추론의 고질적인 문제인 높은 지연 시간(latency)과 낮은 처리량(throughput)을 해결하는 데 기여합니다.

MAX의 아키텍처는 이기종 하드웨어 환경을 염두에 두고 설계되었습니다. 즉, NVIDIA와 AMD GPU를 포함한 다양한 가속기에서 최적의 성능을 제공할 수 있도록 유연하게 작동합니다. 이는 기업이 특정 하드웨어 공급업체에 종속되지 않고, 자신의 컴퓨팅 자원을 최대한 활용할 수 있도록 지원합니다. MAX는 오픈 소스 프로젝트로서 커뮤니티의 기여를 적극적으로 수용하며, 최신 연구 결과와 최적화 기법을 빠르게 통합하고 있습니다. 이는 AI 모델과 하드웨어 기술이 빠르게 발전하는 현 상황에서 프레임워크가 지속적으로 최첨단 성능을 유지할 수 있도록 하는 중요한 요소입니다.

**불가능에 도전하는 정신: AI 컴퓨팅의 새로운 지평을 열다**

그들이 불가능하다고 말할 때, 그게 불가능하다는 뜻인가요, 아니면 매우, 매우 어렵다는 뜻인가요?

음, 그러니까, CUDA가 거의 20년이 되었고, NVIDIA에는 수백, 수천 명의 사람들이 작업하고 있다는 것은 상식입니다. 전 세계가 수년 동안 CUDA 코드를 작성해왔고, 매우, 매우 어렵습니다. 그래서, 아니요, 많은 사람들은 스타트업이 이 분야에서 무언가를 하는 것이 불가능하다고 생각합니다. 네. 그건 그냥 상식이죠. 이 모든 사람들이 이 모든 다른 시스템에 그 모든 돈을 쏟아부었고, 모두 실패했습니다. 다른 똑똑한 사람들이 만든 다른 모든 것들이 실패했는데, 왜 당신의 것이 성공할 것이라고 생각하나요? 그렇죠. 그래서 변화가 불가능하다는 것이 일반적인 통념이지만, 이봐요, 우리는 AI 시대에 살고 있잖아요. 우리 주변에는 항상 변화가 있죠. 그렇죠. 그래서 당신이 해야 할 일은 성공 기준이 무엇인지, 변화가 실제로 작동하게 하는 원인이 무엇인지를 계획하는 것입니다. 그리고 제 경력 전반에 걸쳐, LLVM 때처럼, 모든 GCC 사람들은 저에게 불가능하다고 말했습니다. "LLVM은 실패할 거야. 왜냐하면 GCC는 20년이 되었고, 수백 명의 사람들이 작업해왔고, 어쩌고 저쩌고. 그리고 스펙 벤치마크." 그리고 뭐든 간에. 밖에서는 아무도 저에게 불가능하다고 말하지 않았습니다. 왜냐하면 비밀이었으니까요. 그래서 그건 좀 달랐지만, 애플 내부에서 그것에 대해 아는 모든 사람들은 "아니, 아니, Objective-C는 괜찮아. 그냥 Objective-C를 개선해야 해. 세상은 새로운 프로그래밍 언어가 필요 없어. 새로운 프로그래밍 언어는 절대 채택되지 않아."라고 말했습니다. 네. 그렇죠. 그리고 "새로운 프로그래밍 언어는 아무 데도 가지 못한다"는 것은 상식입니다. 그것이 일반적인 통념이죠.

Chris Lattner의 이러한 철학은 Modular의 DNA 깊숙이 자리 잡고 있습니다. 기존의 AI 스택은 CUDA라는 강력하지만 독점적인 기술에 의해 지배되어 왔습니다. 수십 년간의 개발과 수많은 엔지니어의 노력이 투입된 CUDA를 대체하는 것은 많은 사람들에게 불가능한 도전으로 여겨졌습니다. 하지만 Modular는 이러한 '불가능'이라는 통념에 굴하지 않고, LLVM과 Swift를 통해 컴파일러 및 언어 생태계에 혁신을 가져왔던 경험을 바탕으로 AI 컴퓨팅의 새로운 길을 개척하고 있습니다.

Modular는 단순히 CUDA의 대안을 만드는 것을 넘어, AI 스택 전체의 복잡성을 근본적으로 해결하고자 합니다. 이는 단순히 특정 벤치마크에서 기존 솔루션을 능가하는 것을 넘어, 개발자들이 AI 모델을 더 쉽게 구축하고 배포할 수 있도록 하는 데 초점을 맞춥니다. Chris Lattner는 과거 LLVM 개발 시 "모듈식 설계는 성능을 저해한다"는 통념에 맞섰고, Swift 개발 시 "새로운 프로그래밍 언어는 채택되지 않는다"는 인식을 뒤집었습니다. 이제 그는 Mojo와 MAX를 통해 "AI 가속기 프로그래밍은 복잡하고 특정 하드웨어에 종속적이다"라는 통념에 도전하고 있습니다. 이러한 도전은 혁신적인 기술뿐만 아니라, 강력한 비전과 끈질긴 실행력을 요구합니다.

**AI 컴퓨팅의 민주화와 오픈 소스 생태계의 확장**

Modular의 궁극적인 목표는 AI 컴퓨팅을 민주화하는 것입니다. 과거에는 AI 모델 훈련이 주로 대기업이나 연구 기관의 전유물이었지만, PyTorch와 같은 프레임워크의 등장으로 훈련의 민주화가 이루어졌습니다. 그러나 추론(inference) 영역은 여전히 '블랙 아트(black art)'로 남아있어, 복잡성과 최적화의 어려움으로 인해 많은 기업과 개발자들이 고성능 AI 추론 시스템을 구축하는 데 어려움을 겪고 있습니다.

Modular는 Mojo와 MAX를 오픈 소스화하고 나이틀리 빌드를 제공함으로써, 이러한 추론의 복잡성을 해소하고 개발자들이 직접 최적화된 솔루션을 구축할 수 있도록 지원합니다. 특히 MAX는 vLLM, SGLang과 같은 다른 추론 프레임워크와는 차별화된 접근 방식을 취합니다. MAX는 핵심적인 몇 가지 기능에 집중하여 최고의 성능과 안정성을 제공하며, 하드웨어에 대한 깊이 있는 제어와 유연성을 개발자에게 부여합니다. vLLM이 여러 하드웨어를 지원한다고 광고하지만, 실제 비-NVIDIA 환경에서는 작동하지 않는 경우가 많은 반면, MAX는 명확하게 지원하는 하드웨어에 대해 검증된 성능을 보장합니다. Modular는 PyTorch, VLLM, SGLang과 같은 기존 프레임워크들이 MAX를 채택하고, 업계를 선도하는 최첨단 모델이 MAX를 통해 출시되기를 기대하고 있습니다.

Modular는 커뮤니티의 참여를 매우 중요하게 생각합니다. 모든 GPU 커널을 Mojo로 작성하고 그 소스 코드를 공개함으로써, 개발자들이 내부 동작을 투명하게 이해하고 기여할 수 있도록 합니다. 이는 Flash-Attention과 같은 핵심 기술을 Modular 팀이 단 몇 주 만에 재구축하고 최적화할 수 있었던 원동력이기도 합니다. 이러한 개방성은 AI 스택의 빠른 발전을 가능하게 하며, 새로운 아이디어가 신속하게 통합될 수 있는 환경을 조성합니다. Modular는 AI 기술이 특정 기업에 의해 독점되는 것이 아니라, 전 세계 개발자 커뮤니티의 협력을 통해 발전해야 한다는 신념을 가지고 있습니다.

**DeepSeek 사례와 저수준 최적화의 중요성**

최근 DeepSeek 팀의 연구는 AI 커뮤니티에 큰 반향을 일으켰습니다. 그들은 NVIDIA GPU의 저수준 PTX(Parallel Thread Execution) 명령어까지 파고들어 MLA(Multi-head Latent Attention)와 같은 혁신적인 어텐션 메커니즘과 저정밀도 훈련(low-precision training)을 발전시켰습니다. DeepSeek의 사례는 최신 AI 모델의 성능을 극한까지 끌어올리기 위해서는 단순히 고수준 프레임워크를 사용하는 것을 넘어, GPU 하드웨어의 미세한 동작까지 이해하고 제어하는 저수준 프로그래밍이 필수적임을 보여주었습니다.

하지만 DeepSeek의 접근 방식이 가진 한계점도 명확합니다. PTX 수준의 최적화는 특정 GPU 아키텍처에 매우 종속적입니다. 즉, Blackwell과 같은 새로운 세대의 GPU가 출시되면, 기존에 작성된 저수준 코드는 호환되지 않거나 비효율적이 되어 처음부터 다시 작성해야 할 수도 있습니다. 이는 AI 하드웨어의 빠른 발전 속도를 고려할 때 엄청난 기술 부채로 작용할 수 있습니다.

Mojo는 이러한 문제를 해결하기 위해 설계되었습니다. Mojo는 개발자가 파이썬과 유사한 고수준 구문으로 코드를 작성하면서도, MLIR을 통해 다양한 하드웨어에 최적화된 저수준 코드를 생성할 수 있도록 합니다. 이는 DeepSeek과 같은 저수준 최적화의 성능 이점을 유지하면서도, 하드웨어 종속성에서 벗어나 코드를 한 번 작성하면 여러 아키텍처에서 실행할 수 있는 이식성(portability)을 제공합니다. Modular는 Mojo를 통해 AI 개발자들이 하드웨어 변화에 유연하게 대응하고, 지속적으로 최신 성능을 유지할 수 있도록 돕습니다. 이는 AI 시대에 필수적인 '한 번 작성하면 어디서든 실행(Write Once, Run Anywhere)'이라는 개념을 실현하는 중요한 단계입니다.

**Modular의 비즈니스 모델: 컴퓨팅에 대한 권한 부여**

그래서, 다시 말하지만, 우리는 이것을 어려운 모드로 하고 있습니다. 그렇죠. 우리는 제품까지 긴 길을 걸어왔습니다. 우리는 그냥 바로 몇 개의 커널을 얻고, 알파 버전을 만들고, GPU 예약 같은 것을 사서 우리 GPU를 재판매하는 그런 식이 아닙니다. 그 길은 많은 회사들이 선택했고, 그들은 그것을 정말 잘합니다. 그래서 그것은 제가 아주 잘하는 기여가 아닙니다. 그리고 저는 당신을 위해 데이터 센터를 짓지 않을 겁니다. 그것보다 훨씬, 훨씬 더 잘하는 사람들이 있습니다. 좋아요. 그래서 모든 행운을 빕니다. 저는 그 사람들이 맥스를 사용하기를 원합니다. 아시다시피, 저는 꽤 잘합니다.

그게 크루소군요. 네.

저는 이 소프트웨어에 꽤 능숙합니다. 그래서 당신은 모든 컴퓨팅을 처리할 수 있습니다. 게다가, 스타트업에서 벗어나면, GPU와 클라우드로 고생하는 사람들이 많습니다. 그렇죠. 그래서 GPU와 클라우드는 근본적으로 CPU와 클라우드와는 다른 것입니다. 그리고 많은 사람들이 그것에 다가가서 "그냥 다 클라우드일 뿐이야"라고 말합니다. 그렇죠. 하지만 제가 그것이 사실이 아니라는 것을 설득해 드리겠습니다. 그래서, 우선, CPU와 클라우드. 왜 그것이 굉장했을까요? 음, 모든 워크로드은 상태가 없었습니다(stateless). 모두 수평적으로 자동 확장될 수 있었습니다. CPU는 비교적 저렴합니다. 그래서 탄력성을 얻을 수 있습니다. 그것은 정말 멋집니다.

꽤 빨리 로드할 수 있죠. 네. 기가바이트 단위의 가중치 같은 게 아니고요.

네. 그래서 어떤 사업이 2년 반 후에 무엇을 할지 아는 사람이 있을까요? 아무도 없습니다. 아무도 없죠. 그렇죠. 그래서 CPU를 위한 클라우드는 그렇게 멀리까지 용량 계획을 할 필요가 없기 때문에 엄청나게 가치가 있습니다. 그렇죠. 이제 GPU로 넘어가 봅시다. 음, 이제 3년 약정을 맺어야 합니다. 3년이요. 그렇죠. 그래서 젠슨(Jensen)이 1년 안에 구식으로 만들 하드웨어에 대해 3년 약정을 맺는 겁니다. 그렇죠. 그래서 이 물건을 얻게 됩니다. 그래서 큰 약정을 맺고, 그것으로 무엇을 할까요? 음, 당신의 필요가 어떨지 모르기 때문에 과도하게 약정해야 하고, 이것을 할 준비가 되어 있지 않습니다. 또한, 모든 기술은 매우 복잡하고 무섭습니다. 또한, GPU 워크로드은 상태가 있습니다(stateful). 그래서 당신이 멋진 에이전틱(agentic)한 것들에 대해 이야기하고, 여러분은 이 모든 것을 알죠. 네. 상태가 있습니다. 그래서 이제 수평적 자동 확장을 얻지 못합니다. 상태 없는 탄력성을 얻지 못합니다. 그래서 거대한 관리 문제를 얻게 됩니다. 그래서 우리가 할 수 있고, 사람들을 도울 수 있다고 생각하는 것은 "좋아, 당신의 컴퓨팅에 대한 권한을 주겠다"고 말하는 것입니다. 그리고 많은 사람들이 다른 시스템을 가지고 있고, 이것에 들어가는 매우 간단한 시스템들이 있습니다. 하지만 지능적인 라우팅을 통해 5배의 성능 TCO 이점을 얻을 수 있습니다. 그것은 실제로 플랫폼 팀에게 큰 문제입니다. 그들은 이것을 처리하고 싶어하지 않습니다. 그들은 AMD로 가기 위한 하드웨어 선택권을 원합니다. 그들은 이런 종류의 힘과 기술을 원합니다. 그래서 우리는 그들과 함께 일하게 되어 매우 기쁩니다. 제가 간단하게 설명하는 방식은, 많은 엔드포인트 회사들이 있고, 그들이 많이 있다는 것입니다. 그래서 당신은 그들이 모두 하나라고 말할 수 없습니다. 그들은 분명히 장단점과 트레이드오프가 있습니다. 하지만 일반적으로 엔드포인트의 가치 제안은 "보세요, AI와 AI 소프트웨어, 애플리케이션, 워크로드은 모두 엉망진창입니다. 너무 복잡합니다. 당신의 작은 머리로 걱정하지 마세요. 제가 당신의 접시에서 AI를 치워드릴 테니, 당신은 그것에 대해 걱정할 필요가 없습니다. 우리가 당신을 위해 모든 복잡성을 처리해 드릴 겁니다. 그리고 쉬울 겁니다. 그냥 우리 엔드포인트와 이야기하세요."라고 말하는 것입니다. 우리의 접근 방식은 "좋아, 그래, 모든 것이 엉망진창이야. 맞아, 100%야. 끔찍해. 아마 당신이 아는 것보다 더 심할 거야. 그리고 내일이 되면, 모든 것이 계속 변하기 때문에 더 심해질 거야. 우리가 쉽게 만들어 줄게. 우리가 당신에게 힘을 줄게. 당신의 기업과 팀에 초능력을 줄게."라고 말하는 것입니다. 왜냐하면 제가 이야기하는 모든 CEO는 제품에 AI를 넣고 싶어 할 뿐만 아니라, 팀이 AI 기술을 향상시키기를 원하기 때문입니다. 그래서 우리는 기업에서 AI를 빼앗지 않습니다. 우리는 AI에 대한 권한을 줍니다. 우리는 그들의 팀에 힘을 돌려주고, 시작하기 쉬운 경험을 가질 수 있게 합니다. 왜냐하면 많은 사람들이 표준적인 상용 모델을 실행하고 싶어 하고, 당신은 기본적인 것으로서 그냥 작동하는 것을 원하기 때문입니다. 하지만 그들이 "이봐, 나는 실제로 미세 조정을 하고 싶어"라고 말할 때, 저는 제 독점적인 데이터를 다른 스타트업이나 심지어 거기에 있는 몇몇 큰 스타트업에 주고 싶지 않습니다. 그리고 당신은 이것들이 누구의 것인지 알죠. 그것은 제 독점적인 IP입니다. 그리고 나서 당신은 "이봐, 나는 멋진 데이터 모델을 가지고 있어. 나는 실제로 데이터 과학자들이 있어. 나는 실제로 몇 개의 GPU를 가지고 있어. 나는 내 모델을 훈련할 거야."라고 말하는 사람들을 만나게 됩니다. 멋지네요. 그것은 민주화되었습니다. 이제 어떻게 배포할까요? 그렇죠? 음, 다시, 당신은 VLM의 내부를 해킹하는 것으로 돌아갑니다. 그리고 PyTorch는 KV 캐시 최적화나 모든 현대적인 트랜스포머 기능 같은 것들을 위해 실제로 설계되지 않았습니다. 그래서 갑자기 당신은 그것이 좋기를 원한다면 이 복잡성의 절벽에서 떨어집니다. 그래서 우리는 "좋아, 그래, 이것은 복잡성의 또 다른 단계야. 하지만 당신은 이것을 소유할 수 있고, 이것을 확장할 수 있어."라고 말합니다. 그래서 우리는 그것을 도울 수 있습니다. 그래서 그것은 공간에서 다른 트레이드오프입니다. 하지만 저는 시장 출시 시간과 매출 성장 등이 있다는 것을 인정합니다. 그리고 그들은 CUDA의 전체 대체품을 만들 필요가 없었기 때문에 훨씬 더 빨랐습니다.

Modular의 비즈니스 모델은 단순한 호스팅 서비스 제공을 넘어, 기업과 개발자에게 AI 컴퓨팅의 '권한'을 부여하는 데 초점을 맞춥니다. Mojo 언어와 MAX 추론 프레임워크는 무료로 제공되며, 사용자는 NVIDIA 및 CPU 환경에서 어떤 규모로든 이를 활용할 수 있습니다. 이는 AI 기술의 광범위한 채택을 장려하고, 커뮤니티 주도 혁신을 촉진하기 위한 전략입니다. Modular는 기술이 널리 퍼져나가기를 원하며, 이를 통해 더 많은 기업과 개발자가 AI의 잠재력을 최대한 활용할 수 있도록 돕습니다.

수익 모델은 주로 엔터프라이즈 지원 및 클러스터 관리 솔루션에서 발생합니다. 대규모 AI 워크로드를 운영하는 기업은 GPU당 라이선스 비용을 지불하고 Modular의 전문적인 지원을 받을 수 있습니다. 이는 복잡한 GPU 인프라를 효율적으로 관리하고 최적의 성능을 유지해야 하는 기업에게 매우 매력적인 제안입니다. Modular는 단순히 모델을 서빙하는 것을 넘어, 기업의 플랫폼 팀이 하드웨어 선택의 자유를 가지고, 지능형 라우팅과 같은 고급 기능을 통해 TCO(총 소유 비용)를 절감할 수 있도록 지원합니다.

Chris Lattner는 AI 시대를 '어려운 길'로 비유하며, GPU와 클라우드의 복잡성을 강조합니다. CPU 기반 클라우드와 달리 GPU 기반 클라우드는 3년 약정과 같은 장기적인 약정이 필요하며, AI 워크로드가 '상태(stateful)'를 가지는 경우가 많아 수평적 자동 확장(horizontal auto-scaling)이 어렵습니다. 이는 기업에게 막대한 관리 부담과 비용을 초래합니다. Modular는 이러한 문제를 해결하고, 기업이 자신의 컴퓨팅 자원을 최대한 활용하여 AI 기술을 제품에 통합하고 팀의 역량을 강화할 수 있도록 돕습니다. Modular는 기업의 AI 기술을 '빼앗는' 것이 아니라, '권한을 부여'함으로써 AI 혁신을 가속화하고자 합니다.

**창업가로서의 여정과 교훈: 끊임없는 변화 속에서 성장하다**

Modular를 설립하고 이끌어오면서 Chris Lattner는 기술적 도전뿐만 아니라 창업가로서의 개인적인 성장통을 겪었습니다. 기존 대기업에서 거대한 팀을 이끌었던 경험과 달리, 스타트업에서는 모든 인프라와 프로세스를 처음부터 구축해야 하는 어려움이 있었습니다. 특히, 팀원의 이탈은 대기업에서보다 훨씬 더 개인적이고 감정적인 영향을 미 미쳤습니다. 그는 이러한 경험을 통해 인간관계와 팀 관리에 대한 깊은 통찰을 얻었으며, '불가능하다'는 말을 자주 듣는 상황에서도 끈질기게 비전을 추구하는 법을 배웠습니다.

Modular의 여정은 '비밀 프로젝트' 단계에서 '제품 출시' 단계로, 그리고 이제 '커뮤니티와 함께 성장하는' 단계로 진화했습니다. 각 단계마다 새로운 도전과제가 있었고, 특히 R&D에서 제품으로 전환하는 과정은 많은 불확실성을 동반했습니다. 하지만 Chris Lattner는 팀이 불완전한 상태에서 시작하여 점진적으로 개선해나가는 엔지니어링의 본질을 믿으며, 끊임없이 제품을 더 좋게 만들어나가는 데 집중했습니다. 이러한 과정은 그에게 "엔지니어들은 불완전한 것을 가져다가 더 좋고, 더 좋고, 더 좋게 만드는 데 정말 능숙하다"는 교훈을 주었습니다.

그의 일과는 대부분 회의로 채워져 있지만, 주말이나 늦은 밤 시간을 활용하여 전략적인 사고와 코딩에 집중합니다. 평일에는 오전 7시 15분경 기상하여 아이들을 등원시키고, 개들과 함께 산책하며 운동한 후 오전 9시부터 저녁 6~7시까지 업무를 봅니다. 퇴근 후 가족과 저녁 식사를 하고, 아이들이 잠든 후 2~3시간 더 일하는 것이 그의 일상입니다. 공동 창업자인 팀과의 정기적인 대화, 아내와의 소통은 그의 비전을 다듬고 현실적인 실행 계획을 세우는 데 중요한 역할을 합니다. 특히, 그의 아내는 "크리스, 그들은 이렇게 말하지만, 이것이 그들이 실제로 의미하는 바야"라며 인간적인 통찰력을 제공하여, 기술적 문제 해결에 몰두하는 그가 팀과 시장의 요구를 더 잘 이해할 수 있도록 돕습니다. 이는 창업가에게 필요한 IQ와 EQ의 균형을 보여주는 사례입니다.

**AI 코딩 도구와 미래 개발 환경: 생산성과 학습의 가속화**

AI 코딩 도구는 소프트웨어 개발의 풍경을 빠르게 변화시키고 있으며, Mojo와 같은 새로운 프로그래밍 언어의 채택을 가속화하는 데 중요한 역할을 합니다. Chris Lattner는 Cursor와 같은 AI 코딩 도구를 개인적으로 활용하며, 특히 Mojo 코드 작성 시 컨텍스트 창에 많은 코드를 넣어 인덱싱하고 활용하는 방식의 이점을 언급했습니다. 이러한 도구들은 새로운 언어 학습의 장벽을 낮추고, 개발자들이 기계적인 작업에 들이는 시간을 줄여 더 창의적이고 생산적인 작업에 집중할 수 있도록 돕습니다.

AI 코딩 도구는 단순히 코드 자동 완성을 넘어, 코드베이스 전체를 이해하고, 버그를 진단하며, 최적화된 코드를 제안하는 수준으로 발전하고 있습니다. 이는 Mojo와 같이 복잡한 저수준 최적화를 가능하게 하는 언어에 특히 유용합니다. 개발자는 AI 도구의 도움을 받아 더 빠르게 Mojo를 익히고, GPU 프로그래밍과 같은 전문 영역에 대한 접근성을 높일 수 있습니다. Modular의 해커톤 사례에서 보듯이, AI 코딩 도구는 GPU 프로그래밍 경험이 없는 개발자도 고성능 AI 시스템을 구축할 수 있게 함으로써, AI 개발 인력의 저변을 넓히는 데 기여하고 있습니다.

Chris Lattner는 "코드는 인간이 코드가 무엇을 하는지 이해할 수 있는 것에 관한 것"이라고 강조하며, AI 코딩 도구가 궁극적으로 인간 개발자의 역량을 강화하는 도구가 되어야 한다고 믿습니다. AI가 모든 코드를 자동으로 생성하더라도, 개발자는 여전히 코드의 목적, 제약 조건, 성능 특성 등을 추론하고 검증할 수 있어야 합니다. Mojo는 이러한 요구 사항을 충족시키기 위해 고수준의 가독성과 저수준의 제어력을 동시에 제공합니다. 이를 통해 개발자는 AI 코딩 도구의 도움을 받아 하드웨어의 모든 잠재력을 활용하면서도, 유지보수 가능하고 이해하기 쉬운 코드를 작성할 수 있습니다.

Chris는 AI가 인간 개발자에게 '초능력'을 부여하여, 더 많은 사람이 GPU 프로그래밍과 AI 혁신에 참여할 수 있기를 기대합니다. 이는 AI 산업 전체의 성장을 가속화하고, 우리가 상상하는 것보다 훨씬 더 많은 제품과 솔루션이 탄생할 수 있는 기반을 마련할 것입니다.

Chris는 또한 Slack의 논문 채널, Reddit 커뮤니티, RSS 피드(Feedly 사용) 등을 통해 최신 연구 동향을 파악하며, 특히 아카이브 논문의 특정 그룹들을 팔로우합니다. 그는 AI 코딩 도구가 새로운 기술 채택을 위한 순수한 부가 가치를 제공한다고 보며, 인간이 새로운 기술로 힘을 얻고, 기술을 향상시키고, 배울 수 있는 기회를 제공하는 것이 중요하다고 강조합니다.

Chris는 이미 GCC를 LLVM으로, Objective-C를 Swift로 대체했습니다. 다음은 Mojo가 CUDA를 대체할 차례일까요? 전체 에피소드에서 이를 실현하기 위해 무엇이 필요한지에 대해 이야기했습니다. 즐겁게 감상하세요!

**쇼 노트**

*   Chris Lattner
*   Tim Davis
*   Modular
*   LLVM Foundation
*   MAX
*   MLIR
*   PyBind11
*   NanoBind
*   GPU Mode Keynotes

**타임스탬프**

*   [00:00:00] 소개
*   [00:00:12] Modular 개요 및 컴퓨팅의 형태
*   [00:02:27] Modular의 R&D 단계
*   [00:06:55] CPU 최적화에서 GPU 지원까지
*   [00:11:14] MAX: Modular의 추론 프레임워크
*   [00:12:52] Mojo 프로그래밍 언어
*   [00:18:25] MAX 아키텍처: Mojo에서 클러스터 규모 추론까지
*   [00:29:16] 오픈 소스 기여 및 커뮤니티 참여
*   [00:32:25] VLLM 및 SGLang과 Modular의 차별점
*   [00:41:37] Modular의 비즈니스 모델 및 수익화 전략
*   [00:53:17] DeepSeek의 영향과 저수준 GPU 프로그래밍
*   [01:00:00] 추론 시간 컴퓨팅 및 추론 모델
*   [01:02:31] Modular를 이끌며 느낀 개인적인 소회
*   [01:08:27] 창업자로서의 일과 및 시간 관리
*   [01:13:24] AI 코딩 도구 사용 및 최신 연구 동향 파악
*   [01:14:47] 개인 프로젝트와 일과 삶의 균형
*   [01:17:05] 채용, 오픈 소스, 커뮤니티 참여

**Modular와 함께하세요!**

Modular는 현재 GPU 프로그래밍, AI 모델 최적화, 추론 시스템 개발, 쿠버네티스 및 클라우드 규모 아키텍처에 열정적인 최고의 인재들을 채용하고 있습니다. AI 컴퓨팅의 미래를 함께 만들어갈 분들의 지원을 기다립니다.

Mojo와 MAX는 끊임없이 발전하고 있습니다. 1년 전 Mojo를 접했던 분들이라면, 이제 완전히 달라진 모습을 확인하실 수 있을 것입니다. GPU 프로그래밍에 관심 있는 모든 분들을 위해 Modular는 풍부한 학습 자료와 오픈 소스 코드를 제공합니다. 커뮤니티에 참여하여 최신 개발 소식을 접하고, 직접 기여하며 AI 혁신에 동참해 보세요.

기업 고객이라면, AI 확장 및 배포에 어려움을 겪고 계실 때 Modular가 제공하는 엔터프라이즈 솔루션에 대해 문의해 주세요. 저희는 귀사의 AI 여정을 성공적으로 이끌 수 있도록 최적의 지원을 약속드립니다.