이 글에서는 사전 훈련된 대규모 언어 모델(LLM)을 강력한 텍스트 분류기(text classifier)로 변환하는 방법을 보여드리고자 합니다. 하지만 왜 분류(classification)에 집중해야 할까요? 첫째, 분류를 위해 사전 훈련된 모델을 미세 조정(finetuning)하는 것은 모델 미세 조정에 대한 부드러우면서도 효과적인 입문 과정을 제공합니다. 둘째, 스팸 탐지(spam detection), 감성 분석(sentiment analysis), 고객 피드백 분류(customer feedback categorization), 주제 라벨링(topic labeling) 등 많은 실제 및 비즈니스 과제가 텍스트 분류를 중심으로 전개됩니다.

## LLM 분류기의 발전과 그 중요성

대규모 언어 모델(LLM)은 단순히 텍스트 생성 도구를 넘어, 그 방대한 사전 학습 데이터 덕분에 복잡한 패턴을 인식하고 이해하는 능력을 갖추게 되었습니다. 이러한 모델을 특정 분류 작업에 미세 조정함으로써, 기존의 머신러닝 분류기를 뛰어넘는 탁월한 성능을 보여주고 있습니다. 특히, LLM은 문맥 이해 능력이 뛰어나며, 제로샷(zero-shot) 또는 퓨샷(few-shot) 학습을 통해 적은 양의 라벨링된 데이터로도 분류를 수행할 수 있다는 장점이 있습니다. 이는 특히 라벨링된 데이터가 부족하거나, 새로운 분류 작업에 빠르게 적응해야 하는 분야에서 혁신적인 변화를 가져왔습니다.

## GPT 모델을 텍스트 분류기로 전환하기

### 이 글에서 다룰 내용

이 업데이트된 글에서는 사전 훈련된 LLM을 텍스트 분류기로 활용하는 핵심 원리와 최신 기법을 다룹니다. 특히 미세 조정의 실제 적용 사례와 함께, 이 분야에서 자주 제기되는 기술적 질문들에 대한 심층적인 답변을 제공할 것입니다.

### 주요 고려 사항

분류 미세 조정은 모델의 특정 작업에 대한 전문성을 극대화하는 강력한 방법입니다. 이 글에서는 이론적 배경과 함께 실제 구현 시 고려해야 할 사항들을 심층적으로 다룹니다.

### 코드 및 추가 자료

본 글에서 다루는 개념에 대한 실습 예제 코드는 GitHub 저장소에서 찾아볼 수 있습니다.

### LLM 분류기 훈련 시 고려할 7가지 질문에 대한 심층 분석

LLM을 텍스트 분류기로 활용할 때 자주 직면하는 질문들에 대해 자세히 살펴보겠습니다.

1)  **모든 레이어(layer)를 훈련해야 할까요?**
    아니요, 최근에는 LoRA(Low-Rank Adaptation)와 같은 매개변수 효율적 미세 조정(Parameter-Efficient Finetuning, PEFT) 기법이 각광받고 있습니다. 이는 모델의 소수 레이어 또는 어댑터(adapter)만을 훈련하여 전체 모델을 훈련하는 것과 유사한 성능을 얻으면서도 컴퓨팅 자원과 시간을 크게 절약할 수 있게 합니다. 분류 작업에서도 이러한 PEFT 기법은 매우 효과적이며, 특히 대규모 LLM을 효율적으로 활용하는 데 필수적입니다.

2)  **첫 번째 토큰(token)이 아닌 마지막 토큰을 미세 조정하는 이유는 무엇일까요?**
    GPT와 같은 디코더(decoder) 전용 모델의 경우, 마지막 토큰의 은닉 상태(hidden state)는 일반적으로 전체 시퀀스의 정보를 가장 잘 요약하고 있다고 간주됩니다. 특히 문장 분류 작업에서는 시퀀스의 끝에 분류 헤드(classification head)를 연결하여 해당 토큰의 출력을 사용하여 클래스를 예측하는 것이 일반적입니다. 이는 BERT와 같은 인코더(encoder) 모델이 [CLS] 토큰을 사용하는 것과 유사한 목적을 가집니다.

3)  **성능 면에서 BERT는 GPT와 어떻게 비교될까요?**
    텍스트 분류 작업에서는 BERT와 같은 인코더 전용 모델이 양방향 문맥(bidirectional context)을 효과적으로 활용하기 때문에 전통적으로 강세를 보였습니다. 그러나 GPT와 같은 디코더 전용 모델도 인과적 마스크(causal mask)를 적절히 처리하고 분류 헤드를 추가하면 뛰어난 성능을 발휘할 수 있습니다. 최근에는 GPT-3, GPT-4와 같은 큰 규모의 모델들이 방대한 사전 학습 데이터와 모델 크기 덕분에 분류 작업에서도 BERT를 능가하는 경우가 많습니다.

4)  **인과 마스크(causal mask)를 비활성화해야 할까요?**
    GPT와 같은 디코더 모델을 분류에 사용할 때는 입력 시퀀스 전체를 고려하여 문맥을 파악해야 합니다. 따라서 텍스트 생성과 달리 분류는 입력 전체의 의미를 파악하는 것이 중요하므로, 일반적으로 인과 마스크를 비활성화하거나 적절히 조정하여 모델이 미래 토큰의 정보도 활용할 수 있도록 합니다.

7)  **패딩(padding)을 해야 할까요, 말아야 할까요?**
    네, 패딩은 배치(batch) 단위로 데이터를 처리할 때 필수적입니다. 각 입력 시퀀스의 길이가 다를 경우, 가장 긴 시퀀스에 맞춰 나머지 시퀀스에 패딩 토큰을 추가하여 모든 시퀀스의 길이를 동일하게 만들어야 합니다. 이는 GPU와 같은 하드웨어 가속기를 효율적으로 사용하고 병렬 처리를 가능하게 하여 훈련 속도를 크게 향상시킵니다.

## 미세 조정의 다양한 범주

언어 모델을 미세 조정하는 가장 일반적인 방법은 지시 미세 조정(instruction finetuning)과 분류 미세 조정(classification finetuning)입니다. 지시 미세 조정은 특정 지시를 사용하여 일련의 작업에 대해 언어 모델을 훈련시켜, 아래 그림 1에 설명된 바와 같이 자연어 프롬프트(natural language prompt)로 설명된 작업을 이해하고 실행하는 능력을 향상시키는 것을 포함합니다.

### 지시 미세 조정과 분류 미세 조정의 심화

**지시 미세 조정 (Instruction Finetuning)의 확장**
지시 미세 조정은 단순히 스팸 탐지를 넘어 복잡한 질의응답, 요약, 코드 생성 등 다양한 작업에 모델을 적응시킵니다. 사용자의 의도를 파악하고 그에 맞는 응답을 생성하는 데 특화되어 있어, 대화형 AI나 코파일럿(copilot)과 같은 응용 분야에서 핵심적인 역할을 합니다. 사용자가 제시하는 다양한 형태의 프롬프트에 유연하게 대응하며 다재다능한 능력을 발휘합니다.

**분류 미세 조정 (Classification Finetuning)의 심층 분석**
분류 미세 조정은 특정 카테고리 예측에 중점을 둡니다. 예를 들어, 금융 분야에서는 사기 거래 탐지, 법률 분야에서는 계약서 종류 분류, 의료 분야에서는 환자 기록의 특정 질병 분류 등 매우 구체적인 비즈니스 문제를 해결하는 데 사용됩니다. 모델의 마지막 레이어에 선형 분류 헤드(linear classification head)를 추가하여 예측을 수행하며, 이 헤드는 훈련 과정에서 특정 클래스에 대한 특징을 학습하게 됩니다. 이러한 접근 방식은 명확하게 정의된 클래스에 대한 높은 정확도를 제공합니다.

## 올바른 접근 방식 선택

지시 미세 조정은 특정 사용자 지시에 따라 모델이 응답을 이해하고 생성하는 능력을 향상시킵니다. 지시 미세 조정은 복잡한 사용자 지시에 기반하여 다양한 작업을 처리해야 하는 모델에 가장 적합하며, 유연성과 상호 작용 품질을 향상시킵니다. 반면, 분류 미세 조정은 감성 분석이나 스팸 탐지와 같이 데이터를 미리 정의된 클래스로 정확하게 분류해야 하는 프로젝트에 이상적입니다. 지시 미세 조정은 더 다재다능하지만, 다양한 작업에 능숙한 모델을 개발하기 위해서는 더 큰 데이터셋(dataset)과 더 많은 컴퓨팅 자원(computational resources)이 필요합니다. 이와 대조적으로, 분류 미세 조정은 더 적은 데이터와 컴퓨팅 파워를 필요로 하지만, 모델이 훈련된 특정 클래스에만 사용이 제한됩니다.

## 최신 LLM 분류기 미세 조정 기법

### 매개변수 효율적 미세 조정 (PEFT) 기법

앞서 언급했듯이 LoRA 외에도 Adapter-tuning, Prompt-tuning 등 다양한 PEFT 기법이 LLM 분류기 미세 조정에 활용됩니다. 이들은 적은 수의 훈련 가능한 매개변수로도 강력한 성능을 보여, 대규모 모델을 효율적으로 활용하고 배포할 수 있는 길을 열었습니다. 이는 기업이 맞춤형 LLM 분류기를 구축하는 데 드는 시간과 비용을 크게 절감하는 데 기여합니다.

### 전이 학습의 힘

LLM은 방대한 텍스트 데이터에서 언어의 일반적인 패턴과 지식을 학습합니다. 이러한 사전 학습된 지식은 특정 분류 작업에 대한 미세 조정을 통해 효과적으로 전이되어, 초기부터 훈련하는 것보다 훨씬 빠르고 정확하게 목표를 달성할 수 있게 합니다. 이는 소규모 데이터셋에서도 고성능 분류기를 구축할 수 있는 기반이 되며, 특히 전문 분야의 희소 데이터셋에서 큰 이점을 제공합니다.

## 결론 및 향후 전망

LLM 기반의 텍스트 분류는 단순히 키워드 매칭을 넘어선 심층적인 문맥 이해를 가능하게 합니다. 지시 미세 조정과 분류 미세 조정은 각각의 장점을 가지며, 프로젝트의 요구 사항과 사용 가능한 자원에 따라 적절한 방법을 선택하는 것이 중요합니다. PEFT와 같은 최신 기술의 발전은 LLM 분류기의 접근성을 높이고 효율성을 극대화하고 있습니다. 앞으로 LLM 분류기는 더욱 정교해지고 다양한 산업 분야(예: 법률, 의료, 금융)에서 핵심적인 역할을 수행하며, 인공지능 기술의 적용 범위를 더욱 확장할 것으로 기대됩니다.