GPT 아키텍처(architecture)가 처음 등장한 지 7년이 지난 지금, 우리는 GPT-2(2019)의 초기 모습부터 DeepSeek-V3 및 Llama 4(2024-2025)와 같은 최신 모델들을 살펴보며, 그 근본적인 구조가 여전히 놀라울 정도로 흡사하다는 것을 깨닫게 됩니다. 물론, 위치 임베딩(positional embeddings)은 고정된(absolute) 방식에서 회전적(rotational, RoPE) 방식으로 진화했으며, 멀티 헤드 어텐션(Multi-Head Attention)은 상당 부분 그룹화된 쿼리 어텐션(Grouped-Query Attention)으로 대체되었고, GELU와 같은 활성화 함수(activation functions)는 더 효율적인 SwiGLU로 교체되었습니다. 그러나 이러한 세부적인 개선점들을 넘어, 우리는 과연 진정으로 혁신적인 변화를 목격하고 있는 것일까요, 아니면 단지 동일한 아키텍처적 기반(architectural foundations)을 더욱 세밀하게 다듬고 있는 과정일까요?

대규모 언어 모델(LLM)들을 비교하여 그 성능(좋든 나쁘든)에 영향을 미치는 핵심 요인들을 정확히 파악하는 것은 매우 복잡한 일입니다. 데이터셋(datasets), 훈련 방식(training techniques), 그리고 하이퍼파라미터(hyperparameters)는 모델마다 크게 다르며, 종종 상세하게 공개되지 않는 경우가 많습니다. 그럼에도 불구하고, 저는 2025년 현재 LLM 개발자들이 어떤 방향으로 나아가고 있는지 이해하기 위해 아키텍처 자체의 구조적 변동을 탐구하는 것이 여전히 큰 의미를 지닌다고 생각합니다. (그 중 일부는 아래 그림 1에 제시되어 있습니다.)

그림 1: 이 글에서 다루는 아키텍처의 일부.

이 글은 벤치마크 성능(benchmark performance)이나 훈련 알고리즘(training algorithms)에 초점을 맞추기보다는, 오늘날의 대표적인 오픈 모델(open models)들을 규정하는 아키텍처 개발 동향에 집중할 것입니다. (이전에 제가 멀티모달 LLM(multimodal LLMs)에 대해 다루었던 글을 기억하실지 모르겠습니다. 이 글에서는 최근 모델들의 텍스트 처리 능력에 집중하고, 멀티모달 기능에 대한 논의는 다음 기회로 미루겠습니다.)

팁: 이 글은 내용이 상당히 광범위하므로, 목차에 접근하려면 내비게이션 바(navigation bar)를 활용하시는 것을 권장합니다 (Substack 페이지 왼쪽 상단에 마우스를 올리세요).

선택 사항: 아래 비디오는 이 글의 내레이션이 포함된 요약 버전입니다.

---

### 1. DeepSeek V3/R1

이미 여러 차례 언급되었듯이, DeepSeek R1은 2025년 1월 출시 당시 언어 모델 커뮤니티에 큰 반향을 일으켰습니다. 이 추론 모델(reasoning model)은 2024년 12월에 공개된 DeepSeek V3 아키텍처를 기반으로 합니다. 이 글의 주된 초점은 2025년에 등장한 아키텍처에 있지만, DeepSeek V3가 DeepSeek R1 출시 이후에야 폭넓은 주목과 채택을 얻게 되었으므로, DeepSeek V3를 함께 살펴보는 것이 타당하다고 판단됩니다.

DeepSeek R1의 훈련 과정에 대한 구체적인 내용에 관심이 있다면, 올해 초 제가 작성한 다음 글이 도움이 될 것입니다:

**Understanding Reasoning LLMs**
Sebastian Raschka, PhD · Feb 5
[Read full story]

이 섹션에서는 DeepSeek V3에 적용되어 계산 효율성(computational efficiency)을 높이고 다른 많은 LLM들과 차별화되는 두 가지 핵심 아키텍처 기술을 면밀히 분석할 것입니다.

*   멀티 헤드 잠재 어텐션(Multi-Head Latent Attention, MLA)
*   전문가 혼합(Mixture-of-Experts, MoE)

#### 1.1 멀티 헤드 잠재 어텐션(Multi-Head Latent Attention, MLA)

멀티 헤드 잠재 어텐션(MLA)을 논하기 전에, 이 기술이 왜 사용되는지 그 배경을 간략히 설명하는 것이 중요합니다. 이를 위해 최근 몇 년간 멀티 헤드 어텐션(MHA)의 계산 및 파라미터 효율적인 대안으로 새로운 표준이 된 그룹화된 쿼리 어텐션(Grouped-Query Attention, GQA)부터 시작해 보겠습니다.

GQA는 MHA와 다르게 각 어텐션 헤드(head)가 독립적인 키(key)와 값(value) 세트를 보유하는 대신, 메모리 사용량을 줄이기 위해 여러 헤드를 묶어 동일한 키 및 값 투영(key and value projections)을 공유하도록 합니다. 예를 들어, 아래 그림 2에서 볼 수 있듯이, 2개의 키-값 그룹(key-value groups)과 4개의 어텐션 헤드가 있을 경우, 헤드 1과 2는 하나의 키 및 값 세트를 공유하고, 헤드 3과 4는 다른 세트를 공유할 수 있습니다. 이러한 방식은 전체 키 및 값 계산 횟수를 줄여 메모리 소비를 낮추고 효율성을 증진시킵니다 (어블레이션 연구(ablation studies)에 따르면 모델링 성능(modeling performance)에 미치는 영향은 미미하다고 알려져 있습니다).

그림 2: MHA와 GQA의 비교. 여기서 그룹 크기는 2이며, 키와 값 쌍은 2개의 쿼리(query)에 걸쳐 공유됩니다.

따라서 GQA의 핵심 사상은 여러 쿼리 헤드에 걸쳐 키와 값을 공유함으로써 키 및 값 헤드의 수를 줄이는 것입니다. 이는 (1) 모델의 파라미터 수(parameter count)를 절감하고 (2) 추론(inference) 시 KV 캐시(KV cache)에 저장하고 검색해야 하는 키와 값의 양이 감소하여 키 및 값 텐서(tensor)의 메모리 대역폭(memory bandwidth) 사용량을 낮추는 효과를 가져옵니다. (GQA가 코드에서 어떻게 구현되는지 궁금하다면, KV 캐시가 없는 버전에 대해서는 제 GPT-2 to Llama 3 변환 가이드를, KV 캐시 변형에 대해서는 여기를 참조하세요.)

GQA는 주로 MHA의 계산 효율성(computational-efficiency)을 개선하기 위한 해결책으로 고안되었지만, 여러 어블레이션 연구(예: 원본 GQA 논문 및 Llama 2 논문)들은 LLM 모델링 성능 측면에서 표준 MHA와 유사한 결과를 보인다는 것을 입증합니다.

이제 멀티 헤드 잠재 어텐션(MLA)은 KV 캐싱(KV caching)과 특히 잘 부합하는 또 다른 메모리 절약 전략을 제안합니다. GQA가 키와 값 헤드를 공유하는 방식과 달리, MLA는 키와 값 텐서를 KV 캐시에 저장하기 전에 더 낮은 차원의 공간(lower-dimensional space)으로 압축합니다. 추론 시에는 아래 그림 3에 나타난 것처럼, 이 압축된 텐서가 활용되기 전에 원래 크기로 다시 투영(projected back)됩니다. 이 과정은 추가적인 행렬 곱셈(matrix multiplication)을 필요로 하지만, 전반적인 메모리 사용량을 현저히 줄이는 효과가 있습니다. 이 접근 방식은 특히 긴 컨텍스트 길이에서 메모리 병목 현상을 완화하는 데 기여하며, 대규모 모델의 실제 배포 가능성을 높입니다.

그림 3: MLA(DeepSeek V3 및 R1에서 사용)와 일반 MHA의 비교. (참고로, 쿼리(query)도 압축되지만, 훈련 시에만 압축되고 추론 시에는 압축되지 않습니다.)

참고로, MLA는 DeepSeek V3에서 처음 도입된 기술이 아닙니다. 이전 버전인 DeepSeek-V2에서도 MLA를 사용했으며, 사실상 이 기술을 선구적으로 적용했습니다. 또한, V2 논문에는 DeepSeek 팀이 GQA 대신 MLA를 선택한 이유를 설명하는 흥미로운 어블레이션 연구 결과들이 포함되어 있습니다 (아래 그림 4 참조).

그림 4: DeepSeek-V2 논문에서 발췌한 주석이 달린 표, https://arxiv.org/abs/2405.04434

위 그림 4에서 볼 수 있듯이, GQA는 MHA보다 성능이 떨어지는 경향을 보였으며, MLA는 MHA보다 더 우수한 모델링 성능을 제공합니다. 이는 DeepSeek 팀이 GQA 대신 MLA를 채택한 주요 동기였을 것입니다. (MLA와 GQA 간의 "토큰당 KV 캐시(KV Cache per Token)" 절약 비교도 함께 제시되었다면 더욱 흥미로웠을 것입니다!)

다음 아키텍처 구성 요소로 넘어가기 전에 이 섹션을 요약하자면, MLA는 모델링 성능 면에서 MHA를 미묘하게 능가하면서도 KV 캐시 메모리 사용량을 현저히 절감하는 영리한 기술입니다. 이는 계산 효율성과 성능 사이의 균형을 효과적으로 맞춘 설계 결정이라고 평가할 수 있습니다.

#### 1.2 전문가 혼합(Mixture-of-Experts, MoE)

DeepSeek에서 주목할 만한 또 다른 핵심 아키텍처 구성 요소는 전문가 혼합(MoE) 레이어(layer)의 활용입니다. DeepSeek이 MoE를 처음 개발한 것은 아니지만, 이 기술은 올해 다시 각광받기 시작했으며, 이후에 다룰 많은 아키텍처들 또한 이를 적극적으로 채택하고 있습니다. MoE는 모델의 용량(capacity)을 크게 확장하면서도 추론 시에는 계산 비용을 효율적으로 관리할 수 있는 강력한 패러다임을 제공합니다.

여러분은 이미 MoE에 익숙할 수도 있지만, 간략한 요약이 도움이 될 것입니다. MoE의 핵심 사상은 트랜스포머 블록(transformer block) 내의 각 피드포워드 모듈(FeedForward module)을 여러 전문가 레이어(expert layers)로 대체하는 것입니다. 여기서 각 전문가 레이어 또한 하나의 피드포워드 모듈입니다. 이는 아래 그림 5에 나타난 것처럼, 단일 피드포워드 블록을 여러 개의 피드포워드 블록으로 교체하는 것을 의미합니다.

그림 5: DeepSeek V3/R1의 전문가 혼합(MoE) 모듈(오른쪽)과 표준 피드포워드 블록을 사용하는 LLM(왼쪽)의 비교 그림.

트랜스포머 블록 내부의 피드포워드 블록(위 그림에서 진회색 블록으로 표시)은 일반적으로 모델 전체 파라미터의 상당 부분을 차지합니다. (트랜스포머 블록, 그리고 그에 따라 피드포워드 블록은 LLM에서 여러 번 반복됩니다. DeepSeek-V3의 경우 61번 반복됩니다.) 따라서 단일 피드포워드 블록을 여러 피드포워드 블록으로 교체하는 것(MoE 설정에서 수행되는 것처럼)은 모델의 총 파라미터 수(total parameter count)를 상당히 증가시킵니다.

그러나 MoE의 핵심 기술은 모든 토큰에 대해 모든 전문가를 사용("활성화")하지 않는다는 점입니다. 대신, 라우터(router)는 토큰당 소수의 전문가만을 선택하여 활성화합니다. 이 라우터는 입력 토큰의 특성을 분석하여 가장 적합한 전문가 조합을 동적으로 결정하며, 이는 모델이 다양한 유형의 입력에 대해 특화된 지식을 효율적으로 활용할 수 있도록 돕습니다. (시간, 또는 오히려 글의 지면 관계상, 라우터의 구체적인 작동 방식에 대해서는 다음에 더 자세히 다루겠습니다.)

한 번에 소수의 전문가만 활성화되기 때문에, MoE 모듈은 항상 전체 파라미터 세트를 사용하는 밀집 모듈(dense modules)과 대조적으로 희소(sparse)하다고 불립니다. 그러나 MoE를 통해 총 파라미터 수가 많아지면 LLM의 용량(capacity)이 증가하여 훈련 중에 더 많은 지식을 습득할 수 있습니다. 하지만 추론 시에는 모든 파라미터를 동시에 사용하지 않으므로 희소성(sparsity)이 효율성을 유지합니다. 이러한 희소성은 MoE 모델의 가장 큰 장점 중 하나로, 대규모 모델을 실제 환경에서 경제적으로 운영할 수 있게 합니다.

예를 들어, DeepSeek-V3는 MoE 모듈당 256개의 전문가와 총 6,710억 개의 파라미터를 가지고 있습니다. 그러나 추론 시에는 한 번에 9개의 전문가만 활성화됩니다 (1개의 공유 전문가(shared expert)와 라우터가 선택한 8개). 이는 6,710억 개 전체가 아닌 370억 개의 파라미터만 추론 단계당 사용된다는 것을 의미합니다. 이러한 선택적 활성화는 모델의 거대한 잠재력을 활용하면서도, 실질적인 계산 부하는 훨씬 낮게 유지하는 영리한 방법입니다.

DeepSeek-V3의 MoE 설계에서 주목할 만한 특징 중 하나는 공유 전문가의 활용입니다. 이 공유 전문가는 모든 토큰에 대해 항상 활성화되는 특별한 전문가입니다. 이 아이디어는 새로운 것이 아니며, DeepSeek 2024 MoE 및 2022 DeepSpeedMoE 논문에서 이미 소개되었습니다. 공유 전문가는 일반적인 패턴이나 자주 나타나는 정보를 처리하는 데 특화되어, 다른 전문가들이 보다 전문적인 지식 습득에 집중할 수 있도록 기반을 제공합니다.

그림 6: "DeepSeekMoE: Towards Ultimate Expert Specialization in Mixture-of-Experts Language Models"에서 발췌한 주석이 달린 그림, https://arxiv.org/abs/2401.06066

공유 전문가를 갖는 것의 이점은 DeepSpeedMoE 논문에서 처음 제기되었는데, 그들은 공유 전문가가 없는 경우보다 전반적인 모델링 성능을 향상시킨다는 것을 발견했습니다. 이는 일반적이거나 반복되는 패턴을 여러 개별 전문가가 학습할 필요가 없으므로, 더 전문화된 패턴을 학습할 여지가 많아지기 때문일 것입니다. 공유 전문가는 모델의 "공통 지식" 기반 역할을 하며, 이는 전반적인 효율성과 전문화 수준을 높이는 데 기여합니다.

#### 1.3 DeepSeek 요약

요약하자면, DeepSeek-V3는 6,710억 개의 파라미터를 가진 거대한 모델로, 출시 당시 4,000억 개의 Llama 4(Maverick)를 포함한 다른 오픈 웨이트 모델(open-weight models)들을 능가하는 규모를 자랑했습니다. 이러한 거대한 규모에도 불구하고, 토큰당 소수의 (단 370억 개의) 파라미터만 활성화하는 전문가 혼합(MoE) 아키텍처 덕분에 추론 시 훨씬 더 효율적입니다. 또 다른 핵심적인 차별점은 DeepSeek-V3가 그룹화된 쿼리 어텐션(GQA) 대신 멀티 헤드 잠재 어텐션(MLA)을 사용한다는 것입니다. MLA와 GQA는 모두 표준 멀티 헤드 어텐션(MHA)에 대한 추론 효율적인 대안이며, 특히 KV 캐싱(KV caching)을 활용할 때 그 효과가 극대화됩니다. MLA는 구현하기 더 복잡하지만, DeepSeek-V2 논문의 연구에서는 GQA보다 더 우수한 모델링 성능을 제공한다는 것을 입증했습니다. DeepSeek은 MoE와 MLA의 조합을 통해 대규모 모델의 성능과 효율성이라는 두 마리 토끼를 잡으려는 시도를 성공적으로 보여주었습니다.

---

### 2. OLMo 2

비영리 단체인 Allen Institute for AI의 OLMo 모델 시리즈는 훈련 데이터와 코드의 투명성, 그리고 비교적 상세한 기술 보고서 덕분에 LLM 연구 커뮤니티에서 매우 주목할 만한 위치를 차지합니다. OLMo 모델이 어떤 특정 벤치마크(benchmark)나 리더보드(leaderboard)의 최상위를 차지하지는 않을지라도, 그 설계는 매우 깔끔하며, 무엇보다 투명하게 공개된 특성 덕분에 LLM 개발을 위한 훌륭한 청사진(blueprint) 역할을 합니다. OLMo 모델은 이러한 투명성 때문에 인기가 많지만, 그 성능 또한 결코 나쁘지 않습니다. 실제로 1월 출시 당시 (Llama 4, Gemma 3, Qwen 3 이전에) OLMo 2 모델은 아래 그림 7에 나타난 것처럼 계산 대비 성능(compute to performance)의 파레토 프론티어(Pareto frontier)에 위치하며, 효율성과 성능 면에서 균형 잡힌 모습을 보였습니다.

그림 7: 다양한 LLM의 모델링 벤치마크 성능(높을수록 좋음) 대 사전 훈련 비용(FLOPs; 낮을수록 좋음). OLMo 2 논문에서 발췌한 주석이 달린 그림, https://arxiv.org/abs/2501.00656

이 글의 서두에서 언급했듯이, 저는 글의 길이를 적절하게 유지하기 위해 LLM 아키텍처의 세부 사항(훈련 방법이나 데이터가 아님)에만 집중하고자 합니다. 그렇다면 OLMo2의 흥미로운 아키텍처 설계 선택은 무엇이었을까요? 이는 주로 정규화(normalizations) 방식에 달려 있습니다: RMSNorm 레이어의 배치와 아래에서 논의할 QK-norm의 추가입니다. 또한, OLMo 2는 MLA나 GQA 대신 여전히 전통적인 멀티 헤드 어텐션(MHA)을 사용한다는 점도 언급할 가치가 있습니다.

#### 2.1 정규화 레이어 배치(Normalization Layer Placement)

전반적으로 OLMo 2는 다른 현대 LLM과 마찬가지로 원본 GPT 모델의 아키텍처를 대체로 따릅니다. 그러나 몇 가지 눈에 띄는 차이점이 있습니다. 정규화 레이어부터 살펴보겠습니다. Llama, Gemma 및 대부분의 다른 LLM과 유사하게 OLMo 2는 LayerNorm에서 RMSNorm으로 전환했습니다. 하지만 RMSNorm은 오래된 기술(기본적으로 훈련 가능한 파라미터가 더 적은 LayerNorm의 단순화된 버전)이므로 RMSNorm 대 LayerNorm에 대한 논의는 건너뛰겠습니다. (궁금한 독자는 제 GPT-2 to Llama 변환 가이드에서 RMSNorm 코드 구현을 찾을 수 있습니다.)

그러나 RMSNorm 레이어의 배치 방식은 논의할 가치가 충분합니다. 원본 트랜스포머("Attention is all you need" 논문에서)는 트랜스포머 블록에서 어텐션 모듈(attention module)과 피드포워드 모듈(FeedForward module) 뒤에 각각 두 개의 정규화 레이어를 배치했습니다. 이는 Post-LN 또는 Post-Norm으로도 알려져 있습니다. 반면, GPT와 그 이후에 나온 대부분의 다른 LLM은 어텐션 및 피드포워드 모듈 *전에* 정규화 레이어를 배치했으며, 이는 Pre-LN 또는 Pre-Norm으로 알려져 있습니다. Post-Norm과 Pre-Norm의 비교는 아래 그림에 상세히 제시되어 있습니다. 이러한 배치 방식의 차이는 모델의 훈련 안정성 및 성능에 중요한 영향을 미칩니다.

그림 8: Post-Norm, Pre-Norm, 그리고 OLMo 2의 Post-Norm 방식의 비교.

2020년에 Xiong et al.은 Pre-LN이 초기화 시 더 잘 작동하는 기울기(gradients)를 생성한다는 것을 입증했습니다. 또한, 연구자들은 Pre-LN이 Post-LN에 필수적인 학습률 웜업(learning rate warm-up) 과정 없이도 효과적으로 작동한다고 언급했습니다. 이러한 발견은 Pre-LN이 훈련 안정성 측면에서 더 유리하다는 인식을 확산시키는 데 기여했습니다.

제가 이 점을 언급하는 이유는 OLMo 2가 Post-LN의 한 형태를 채택했기 때문입니다 (하지만 LayerNorm 대신 RMSNorm을 사용했으므로 저는 이를 Post-Norm이라고 부릅니다). OLMo 2에서는 정규화 레이어를 어텐션 및 피드포워드 레이어 앞에 배치하는 대신, 위 그림에 나타난 것처럼 *뒤에* 배치합니다. 그러나 원본 트랜스포머 아키텍처와 달리, OLMo 2의 정규화 레이어는 여전히 잔차 레이어(residual layers, skip connections) *내부*에 위치합니다. 이 독특한 배치는 훈련 안정성을 확보하면서도 정보 흐름을 최적화하려는 시도로 해석될 수 있습니다.

그렇다면 왜 정규화 레이어의 위치를 변경했을까요? 주된 이유는 아래 그림에 나타난 것처럼 훈련 안정성(training stability)을 향상시키는 데 기여했기 때문입니다. Post-Norm 방식이 항상 Pre-Norm보다 안정성이 떨어진다고 여겨졌던 통념을 뒤집는 중요한 결과입니다.

그림 9: Pre-Norm(GPT-2, Llama 3 등 다수에서 사용)과 OLMo 2의 Post-Norm 방식의 훈련 안정성을 보여주는 그래프. OLMo 2 논문에서 발췌한 주석이 달린 그림, https://arxiv.org/abs/2501.00656

불행히도 이 그림은 재배열 결과와 QK-Norm을 함께 보여주는데, QK-Norm은 별개의 개념입니다. 따라서 정규화 레이어 재배열 자체가 훈련 안정성에 얼마나 기여했는지 정확히 파악하기는 어렵습니다. 하지만 두 가지 요소가 결합되어 시너지 효과를 냈을 가능성도 배제할 수 없습니다.

#### 2.2 QK-Norm

이전 섹션에서 이미 QK-norm을 언급했고, 나중에 논의할 Gemma 2 및 Gemma 3과 같은 다른 LLM도 QK-norm을 사용하므로, 이것이 무엇인지 간략하게 논의해 봅시다. QK-Norm은 본질적으로 또 다른 RMSNorm 레이어입니다. 멀티 헤드 어텐션(MHA) 모듈 내부에 배치되며, RoPE를 적용하기 전에 쿼리(q)와 키(k)에 적용됩니다. 이 과정은 쿼리와 키의 스케일을 정규화하여 어텐션 스코어(attention scores)의 분산을 제어하고, 이는 어텐션 메커니즘의 안정성을 높이는 데 기여합니다. 특히, 어텐션 가중치가 너무 크거나 작아지는 것을 방지하여 훈련 중 발생할 수 있는 불안정성을 줄이는 역할을 합니다.

이를 설명하기 위해, 제가 Qwen3 from-scratch 구현을 위해 작성한 그룹화된 쿼리 어텐션(GQA) 레이어의 일부를 아래에 발췌했습니다 (GQA에서 QK-norm 적용은 OLMo의 MHA와 유사합니다):

```python
class GroupedQueryAttention(nn.Module):
    def __init__(
        self, d_in, num_heads, num_kv_groups, head_dim=None, qk_norm=False, dtype=None
    ):
        # ...
        if qk_norm:
            self.q_norm = RMSNorm(head_dim, eps=1e-6)
            self.k_norm = RMSNorm(head_dim, eps=1e-6)
        else:
            self.q_norm = self.k_norm = None

    def forward(self, x, mask, cos, sin):
        b, num_tokens, _ = x.shape

        # Apply projections
        queries = self.W_query(x)
        keys = self.W_key(x)
        values = self.W_value(x)

        # ...

        # Optional normalization
        if self.q_norm:
            queries = self.q_norm(queries)
        if self.k_norm:
            keys = self.k_norm(keys)

        # Apply RoPE
        queries = apply_rope(queries, cos, sin)
        keys = apply_rope(keys, cos, sin)

        # Expand K and V to match number of heads
        keys = keys.repeat_interleave(self.group_size, dim=1)
        values = values.repeat_interleave(self.group_size, dim=1)

        # Attention
        attn_scores = queries @ keys.transpose(2, 3)
        # ...
```

앞서 언급했듯이, Post-Norm과 함께 QK-Norm은 훈련을 안정화하는 데 중요한 역할을 합니다. QK-Norm은 OLMo 2가 독자적으로 발명한 기술이 아니라 2023년 Scaling Vision Transformers 논문으로 거슬러 올라간다는 점에 유의해야 합니다. 이는 QK-Norm의 효과가 다양한 트랜스포머 기반 모델에서 입증되었음을 시사합니다.

#### 2.3 OLMo 2 요약

요컨대, OLMo 2의 주목할 만한 아키텍처 설계 결정은 주로 RMSNorm 배치에 있습니다: 어텐션 및 피드포워드 모듈 전이 아닌 후에 RMSNorm을 배치한 것(Post-Norm의 한 형태), 그리고 어텐션 메커니즘(attention mechanism) 내부에 쿼리 및 키를 위한 RMSNorm(QK-Norm)을 추가한 것인데, 이 두 가지 모두 훈련 손실(training loss)을 안정화하는 데 크게 기여합니다. 이러한 안정성 향상은 대규모 모델의 성공적인 훈련에 필수적이며, OLMo 2가 뛰어난 성능을 달성하는 데 중요한 역할을 했습니다.

아래는 OLMo 2와 Llama 3를 나란히 비교한 그림입니다. 보시다시피, OLMo 2가 GQA 대신 전통적인 MHA를 여전히 사용한다는 점을 제외하고는 아키텍처는 상대적으로 유사합니다. (그러나 OLMo 2 팀은 3개월 후에 GQA를 사용하는 32B 변형을 출시했습니다.) 이는 초기 설계 단계에서의 선택과 이후의 개선 과정을 보여주는 좋은 예시입니다.

그림 10: Llama 3와 OLMo 2의 아키텍처 비교.

---

### 3. Gemma 3

Google의 Gemma 모델들은 항상 탁월한 성능을 보여주었으며, 저는 이들이 Llama 시리즈와 같은 다른 인기 모델들에 비해 다소 저평가되어 왔다고 생각합니다. Gemma의 독특한 강점 중 하나는 매우 큰 어휘 크기(vocabulary size)(다양한 언어를 더 효과적으로 지원하기 위함)와 8B 또는 70B와 같은 극단적인 크기보다는 27B 규모에 대한 집중입니다. 물론 Gemma 2도 1B, 4B, 12B와 같은 더 작은 크기로 제공되지만, 27B 크기는 정말 이상적인 스위트 스팟(sweet spot)을 차지합니다: 8B 모델보다 훨씬 유능하면서도 70B 모델만큼 리소스 집약적이지 않아 제 Mac Mini에서도 로컬로 원활하게 실행됩니다.

그렇다면 Gemma 3에서 또 무엇이 흥미로울까요? 앞서 논의했듯이, Deepseek-V3/R1과 같은 다른 모델은 고정된 모델 크기에서 추론 시 메모리 요구 사항을 줄이기 위해 전문가 혼합(MoE) 아키텍처를 사용합니다. (MoE 접근 방식은 나중에 다룰 여러 다른 모델에서도 채택됩니다.) Gemma 3는 계산 비용을 절감하기 위해 다른 "트릭"을 사용하는데, 바로 슬라이딩 윈도우 어텐션(sliding window attention)입니다.

#### 3.1 슬라이딩 윈도우 어텐션(Sliding Window Attention)

슬라이딩 윈도우 어텐션(원래 2020년 LongFormer 논문에서 소개되었고 Gemma 2에서도 이미 사용됨)을 통해 Gemma 3 팀은 아래 그림에 나타난 것처럼 KV 캐시의 메모리 요구 사항을 상당량 줄일 수 있었습니다. 이 기술은 특히 긴 시퀀스를 처리할 때 메모리 병목 현상을 완화하여 모델의 효율성을 크게 향상시킵니다.

그림 11: Gemma 3 논문(https://arxiv.org/abs/2503.19786)에서 발췌한 주석이 달린 그림으로, 슬라이딩 윈도우 어텐션을 통한 KV 캐시 메모리 절약을 보여줍니다.

그렇다면 슬라이딩 윈도우 어텐션이란 정확히 무엇일까요? 각 시퀀스 요소가 다른 모든 시퀀스 요소에 접근할 수 있는 일반적인 셀프 어텐션(self-attention)을 전역 어텐션 메커니즘(global attention mechanism)으로 간주한다면, 슬라이딩 윈도우 어텐션은 현재 쿼리 위치 주변의 컨텍스트 크기(context size)를 제한하므로 로컬