원본 GPT 아키텍처(architecture)는 인공지능 분야에 혁신을 가져왔지만, 그 영향력은 단순한 기술적 발전을 넘어섭니다. 지난 몇 년간 대규모 언어 모델(LLM)은 우리의 일상생활과 산업 전반에 걸쳐 예상치 못한 변화를 일으켰습니다. GPT-2(2019)를 되돌아보고 DeepSeek-V3 및 Llama 4(2024-2025)를 내다보면, 이 모델들이 여전히 구조적으로 얼마나 유사한지 놀랄 수도 있습니다. 물론, 위치 임베딩(positional embeddings)은 절대적(absolute) 방식에서 회전적(rotational, RoPE) 방식으로 발전했고, 멀티 헤드 어텐션(Multi-Head Attention)은 대부분 그룹화된 쿼리 어텐션(Grouped-Query Attention)으로 대체되었으며, 더 효율적인 SwiGLU는 GELU와 같은 활성화 함수(activation functions)를 대체했습니다. 하지만 이러한 사소한 개선점들 아래에서, 우리는 진정으로 획기적인 변화를 보았을까요, 아니면 단순히 동일한 아키텍처 기반(architectural foundations)을 다듬고 있는 것일까요? LLM(Large Language Model)들을 비교하여 성능(좋든 나쁘든)에 기여하는 핵심 요소를 파악하는 것은 매우 어렵습니다. 데이터셋(datasets), 훈련 기법(training techniques), 하이퍼파라미터(hyperparameters)는 매우 다양하며 종종 잘 문서화되어 있지 않습니다. GPT-2 출시 이후, 우리는 기술적 진보뿐만 아니라 AI의 윤리적 사용, 사회적 책임, 그리고 규제 프레임워크 구축에 대한 중요한 논의를 목격하고 있습니다. 이러한 변화의 물결 속에서, 새로운 아키텍처와 모델들이 끊임없이 등장하며, 각기 다른 효율성과 능력을 선보이고 있습니다. 이처럼 빠르게 발전하는 LLM들을 비교하여 성능에 기여하는 핵심 요소를 파악하는 것은 여전히 중요한 과제로 남아 있습니다. 그러나 저는 2025년에 LLM 개발자들이 무엇을 하고 있는지 알아보기 위해 아키텍처 자체의 구조적 변화를 살펴보는 것이 여전히 많은 가치가 있다고 생각합니다. (그 중 일부는 아래 그림 1에 나와 있습니다.)

그림 1: 이 글에서 다루는 아키텍처의 일부.

따라서 이 글에서는 벤치마크 성능(benchmark performance)이나 훈련 알고리즘(training algorithms)에 대한 상세한 분석보다는, 오늘날의 대표적인 오픈 모델(open models)을 정의하는 아키텍처 개발과 더불어 현대 LLM의 다양한 응용 사례 및 미래 지향적인 개발 방향에 초점을 맞출 것입니다. (기억하시겠지만, 저는 얼마 전 멀티모달 LLM(multimodal LLMs)에 대해 썼습니다. 이 글에서는 최근 모델들의 텍스트 기능에 집중하고 멀티모달 기능에 대한 논의는 다음으로 미루겠습니다.)

팁: 이 글은 상당히 포괄적이므로, 목차에 접근하려면 내비게이션 바(navigation bar)를 사용하는 것을 권장합니다 (Substack 페이지 왼쪽 위에 마우스를 올리세요).

선택 사항: 아래 비디오는 이 글의 내레이션이 포함된 요약 버전입니다.

---

### 1. DeepSeek V3/R1: 혁신과 윤리적 고려사항

지금까지 여러 번 들으셨겠지만, DeepSeek R1은 2025년 1월에 출시되었을 때 AI 연구 커뮤니티에 새로운 활력을 불어넣으며 큰 반향을 일으켰습니다. DeepSeek R1은 2024년 12월에 소개된 DeepSeek V3 아키텍처를 기반으로 구축된 추론 모델(reasoning model)입니다. 이 모델은 단순히 기술적 성능을 넘어선 중요한 질문들을 던졌으며, 그 잠재력만큼이나 대규모 AI 모델의 사회적 영향력에 대한 심도 깊은 논의를 촉발했습니다. 여기서 제 초점은 2025년에 출시된 아키텍처에 있지만, DeepSeek V3는 2025년 DeepSeek R1 출시 이후에야 광범위한 관심과 채택을 얻었으므로 DeepSeek V3를 포함하는 것이 합리적이라고 생각합니다.

DeepSeek R1의 훈련에 특히 관심이 있다면, 올해 초에 제가 쓴 다음 글도 유용할 것입니다:

**Understanding Reasoning LLMs**
Sebastian Raschka, PhD · Feb 5
[Read full story]

이 섹션에서는 DeepSeek V3에 도입되어 계산 효율성(computational efficiency)을 향상시키고 다른 많은 LLM과 차별화되며 AI 개발의 새로운 지평을 열었던 두 가지 핵심 아키텍처 기술에 초점을 맞출 것입니다.

*   멀티 헤드 잠재 어텐션(Multi-Head Latent Attention, MLA)
*   전문가 혼합(Mixture-of-Experts, MoE)

#### 1.1 멀티 헤드 잠재 어텐션(Multi-Head Latent Attention, MLA)

멀티 헤드 잠재 어텐션(MLA)을