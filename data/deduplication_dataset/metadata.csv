post_id,title,base_word_count,exact_30_update_wc,exact_30_ground_truth_wc,exact_30_overlap_ratio,paraphrase_25_update_wc,paraphrase_25_ground_truth_wc,paraphrase_25_overlap_ratio,fragment_20_update_wc,fragment_20_ground_truth_wc,fragment_20_overlap_ratio,semantic_25_update_wc,semantic_25_ground_truth_wc,semantic_25_overlap_ratio,mixed_real_update_wc,mixed_real_ground_truth_wc,mixed_real_overlap_ratio,status
018,**바닥부터 LLM(대규모 언어 모델) 구축하기: 3시간 코딩 워크숍**,380,390,487,0.726,448,448,0.848,315,569,0.4,433,438,0.866,428,433,0.876,generated
003,**처음부터 Qwen3 이해 및 구현하기**,405,397,397,1.02,433,433,0.935,354,459,0.847,428,473,0.841,316,342,1.199,generated
017,**처음부터 GPT 스타일(GPT-Style) LLM 분류기(LLM classifier) 구축하기**,736,991,1000,0.734,938,938,0.785,670,679,1.085,994,994,0.74,604,604,1.219,generated
010,**기초부터의 추론 첫 번째 살펴보기: 1장**,280,299,299,0.936,330,330,0.848,255,363,0.675,584,584,0.479,230,242,1.165,generated
025,**GPT-5: 그냥 알아서 다 한다**,1908,1375,2281,0.729,1765,1765,1.081,1549,1547,1.233,1521,1521,1.254,1646,1646,1.159,generated
008,밑바닥부터 대규모 언어 모델(LLMs) 코딩하기: 완벽 코스,957,534,782,1.328,974,1004,0.952,651,892,1.1,978,978,0.979,670,754,1.303,generated
034,**사이버네틱 팀원**,1354,1671,1892,0.678,1388,1388,0.976,1137,1560,0.819,1337,1327,1.02,1132,1132,1.196,generated
036,**새로운 세대의 AI: 클로드 3.7과 그록 3**,2378,1444,2008,1.256,2265,2265,1.05,936,2351,1.029,2141,2141,1.111,1803,1803,1.319,generated
026,**쓴 교훈 대 쓰레기통**,1921,1515,2045,0.918,1618,1618,1.187,1171,1730,1.163,1728,1728,1.112,1574,1574,1.22,generated
035,말이 현실을 만들다,1946,1305,2375,0.671,2080,2080,0.936,1443,2011,0.955,2305,2290,0.851,1587,1587,1.226,generated
024,**집단 지능(Mass Intelligence)**,1962,1282,2093,0.898,1882,1882,1.043,1050,1856,1.101,1635,1635,1.2,1586,1587,1.236,generated
040,**방금 무슨 일이 있었나요?**,2025,1272,2337,0.755,2086,2086,0.971,811,2139,0.859,1684,1684,1.202,1539,1556,1.305,generated
032,"**들쭉날쭉한 인공 일반 지능(AGI)에 대하여: o3, 제미니 2.5, 그리고 그 이후의 모든 것**",1814,2272,2272,0.798,1708,1708,1.062,1250,2003,0.849,1639,1639,1.107,1398,1399,1.297,generated
039,**대홍수의 예언**,1998,1596,1678,1.201,1779,1779,1.123,1283,1928,1.055,2015,2019,0.99,1661,1661,1.203,generated
022,**실제 AI 에이전트(AI Agents)와 실제 업무**,1463,1363,1500,0.973,1429,1428,1.024,1189,1708,0.794,1378,1377,1.062,1356,1356,1.079,generated
031,성격과 설득,1612,1597,1667,0.966,2166,2166,0.744,1222,1379,1.191,1402,1428,1.131,1310,1320,1.223,generated
006,**LLM 연구 논문: 2025년 목록 (1월 ~ 6월)**,1616,929,1898,0.696,1873,1878,0.86,1028,2144,0.486,1920,1920,0.842,1447,1450,1.115,generated
007,**LLM(Large Language Model)의 KV 캐시(KV Cache)를 처음부터 이해하고 코딩하기**,3258,2207,3637,0.828,3281,3339,0.975,1334,3669,0.692,3446,3443,0.946,2683,2683,1.214,generated
028,**지금 바로 AI 활용하기: 간편 가이드**,2638,3057,3057,0.863,2545,2545,1.037,2293,2340,1.13,2696,2696,0.978,2207,2207,1.195,generated
029,**32마리 수달로 본 AI의 최근 역사**,1825,1761,2531,0.599,2100,2100,0.869,1476,2499,0.543,2658,2639,0.694,1594,1594,1.145,generated
037,"**검색의 종말, 연구의 시작**",1745,1262,1645,1.079,1748,1748,0.998,1406,1963,0.845,1995,2028,0.858,1567,1567,1.114,generated
023,**마법사들과 함께 일하는 것에 관하여**,2205,1387,2000,1.148,2253,2268,0.972,1791,2965,0.576,1838,1838,1.2,1717,1719,1.283,generated
027,"**""뇌 손상""에 맞서**",2633,1292,2268,1.283,2461,2461,1.07,2369,3190,0.765,2353,2353,1.119,2197,2197,1.198,generated
033,더 이상 코끼리는 없다: 이미지 생성(image generation) 분야의 혁신,1527,1025,1584,0.944,1576,1576,0.969,1195,1701,0.854,1356,1356,1.126,1176,1210,1.27,generated
038,**지금 어떤 AI를 사용해야 할까: 업데이트된 주관적 가이드 (2/15 재업데이트됨)**,2744,2032,2085,1.324,2964,2964,0.926,1238,2664,1.065,2540,2540,1.08,2060,2060,1.332,generated
021,지금 바로 AI를 활용하는 나름의 가이드,2809,2260,2553,1.113,2403,2403,1.169,2058,2058,1.365,5972,5964,0.472,2143,2144,1.31,generated
014,**2024년 주목할 만한 AI 연구 논문 (1부)**,3607,4103,4110,0.877,3375,3375,1.069,1729,4038,0.751,3160,3163,1.141,3023,3022,1.194,generated
030,"**AI의 성공적인 구현: 리더십, 랩, 그리고 대중**",2890,2917,2917,0.991,2656,2656,1.088,2260,2436,1.201,3203,3204,0.902,2402,2402,1.203,generated
004,**GPT-2부터 gpt-oss까지: 아키텍처 발전 분석**,5951,5441,5419,1.098,5268,5268,1.13,1635,5373,1.354,4602,4602,1.293,4654,4654,1.279,generated
065,"**🤖 AI 에이전트 주간: DeepSeek-OCR, 웹에서 클로드 코드, ChatGPT 아틀라스 브라우저,...**",247,296,458,0.287,339,422,0.484,271,527,-0.033,521,531,0.455,293,316,0.765,generated
067,"**🤖 AI 에이전트(AI Agents) 위클리: Claude Haiku 4.5, Deep Agents, SWE-grep, nanochat, Agent Skills, Veo 3.1 Fast, n8n AI Workflow Builder**",294,500,500,0.588,261,316,0.916,375,645,0.064,424,396,0.759,332,332,0.886,generated
063,"🤖 **AI 에이전트(AI Agents) 위클리**: MiniMax-M2, Cursor 2.0, SWE-1.5, Agent Data Protocol, Kimi CLI",300,285,409,0.618,315,430,0.587,491,533,0.525,1008,1008,0.298,319,319,0.94,generated
012,**추론 거대 언어 모델(Large Language Models) 이해**,4633,3858,4149,1.125,4025,4025,1.151,4038,4363,1.067,3777,3758,1.232,3767,3767,1.23,generated
061,"🤖 AI 에이전트(AI Agents) 주간: 컨텍스트 엔지니어링(Context Engineering) 2.0, 키미 K2 씽킹(Kimi K2 Thinking), 윈드서프 코드맵(Windsurf Codemaps), 구글 파일 검색(Google File Search), 툴-투-에이전트 검색(Tool-to-Agent Retrieval)",308,373,593,0.236,363,363,0.848,351,398,0.744,593,659,0.408,470,482,0.63,generated
070,"**🤖 AI 에이전트 주간 소식: 에이전트킷(AgentKit), 제미니 2.5 컴퓨터 사용(Gemini 2.5 Computer Use), AI 보고서 2025 현황(State of AI Report 2025), 에이전트 컨텍스트 엔지니어링(Agentic Context Engineering), 코드멘더(CodeMender)**",452,773,933,0.378,996,996,0.454,474,886,0.084,540,898,0.174,629,630,0.717,generated
019,새로운 거대 언어 모델(LLM) 사전 학습(pre-training) 및 사후 학습(post-training) 패러다임(paradigm),4922,4829,4829,1.019,8151,8124,0.607,1823,5313,0.786,4322,4322,1.139,3855,3855,1.277,generated
072,"**🤖 AI 에이전트 위클리: 클로드 에이전트 SDK(Claude Agent SDK), 소라 2(Sora 2), 클로드 소네트 4.5(Claude Sonnet 4.5), 마이크로소프트 에이전트 프레임워크(Microsoft Agent Framework), GLM-4.6(GLM-4.6), 에이전틱 커머스 프로토콜(Agentic Commerce Protocol)**",290,367,403,0.692,786,786,0.369,410,553,0.359,731,731,0.397,456,456,0.636,generated
011,**거대 언어 모델(LLM) 추론 모델(reasoning model) 추론 현황**,5028,4837,5146,0.976,5137,5136,0.979,4358,5866,0.808,5127,5111,0.984,4465,4465,1.126,generated
009,LLM 추론(LLM reasoning)을 위한 강화 학습(reinforcement learning)의 현황,8451,4178,7611,1.201,6865,6935,1.221,6968,12984,0.349,1054,7032,2.346,6981,6981,1.211,generated
050,**시각 대규모 언어 모델 (vLLMs)**,7717,7234,7119,1.083,6432,6431,1.2,2820,7573,1.051,6129,6128,1.259,5931,5930,1.301,generated
068,**심층 에이전트(Deep Agents)**,890,994,1201,0.687,1137,1137,0.783,603,981,0.849,839,839,1.061,866,866,1.028,generated
045,**직접 선호 최적화 (DPO)**,7350,5823,6213,1.195,5700,5701,1.289,3030,6148,1.397,5926,5928,1.24,5707,5707,1.288,generated
020,**지시 사전 학습 대규모 언어 모델(LLMs)**,7572,3151,6669,1.287,7022,7121,1.064,6549,6580,1.151,8011,7989,0.948,4615,6736,1.181,generated
002,**거대 언어 모델(LLM) 평가의 4가지 주요 접근 방식 이해 (기초부터)**,6705,4575,4571,1.466,5514,5514,1.216,3687,8554,0.499,5731,5534,1.204,5316,5316,1.261,generated
076,"**🤖 AI 에이전트 위클리: GPT-5-코덱스, 그록 4 패스트, 통이 딥리서치, 마지스트랄 스몰 1.2, 에이전트 결제 프로토콜(AP2)**",404,693,693,0.583,743,802,0.464,363,661,0.292,922,922,0.438,561,620,0.615,generated
015,**LLM 연구 논문: 2024년 목록**,9530,3817,893,3.263,7077,7046,1.351,7236,7237,1.317,7586,7586,1.256,7429,7423,1.284,generated
074,"🤖 AI 에이전트 주간: 코드 월드 모델(Code World Model), 제미니 로보틱스-ER 1.5(Gemini Robotics-ER 1.5), 피그마 MCP 서버(Figma MCP server), 엿듣는 LLM 에이전트(Overhearing LLM Agents), Qwen3-Max, 감마 API(Gamma API)",327,770,865,0.301,571,571,0.573,663,686,0.459,531,531,0.616,435,455,0.706,generated
016,**멀티모달(Multimodal) 대규모 언어 모델(Large Language Models) 이해하기**,5364,2892,5176,1.065,4574,4557,1.176,4524,4575,1.174,5176,1138,1.816,3791,4519,1.223,generated
078,"AI 에이전트 위클리: 에이전트 3, ChatGPT 개발자 모드(Developer Mode), MCP 레지스트리(Registry), 에이전트용 효과적인 도구 작성, Qodo Aware",256,345,444,0.455,296,311,0.814,241,401,0.398,324,449,0.404,514,525,0.477,generated
080,"**🤖 AI 에이전트(AI Agents) 주간 소식: 범용 딥 리서치(Universal Deep Research), GPT-4b 마이크로(GPT-4b micro), 자가 진화 에이전트(Self-Evolving Agents), 다중 에이전트 실패(Multi-Agent Failures) 추적**",361,363,558,0.457,841,841,0.429,338,681,0.053,743,892,0.285,471,477,0.754,generated
064,🥇 **이번 주 주요 AI 논문**,1935,1706,2421,0.715,3347,3341,0.58,1488,2678,0.501,3066,3066,0.631,2443,2443,0.792,generated
062,🥇 **금주의 최고의 인공지능(AI) 논문**,2308,3905,3905,0.591,3013,3013,0.766,1193,2825,0.567,2887,2882,0.801,2484,2479,0.931,generated
049,**Llama 4: 프론티어 수준의 대규모 언어 모델(Large Language Model) 개발의 도전 과제**,10941,3896,3896,2.808,3239,3239,3.378,2733,965,4.65,3106,3102,3.524,2879,2879,3.8,generated
069,🥇**이번 주 최고의 AI 논문**,2395,1961,3371,0.502,2970,2964,0.808,1927,2599,0.894,2913,2907,0.824,2623,2641,0.906,generated
001,표준 대규모 언어 모델(LLM)을 넘어서,7760,9351,9350,0.83,7828,7824,0.992,7232,1309,1.892,7983,971,1.85,7170,7160,1.084,generated
066,🥇 **금주의 최고 인공지능(AI) 논문**,2603,1826,2862,0.858,3489,3489,0.746,1858,3587,0.47,4340,4340,0.6,1589,2857,0.84,generated
089,**GPT-5의 라우터(Router): 작동 방식과 프론티어 랩스(Frontier Labs)가 현재 파레토 프론티어(Pareto Frontier)를 목표로 삼는 이유**,618,355,651,0.907,870,870,0.71,509,588,1.059,1207,1212,0.508,540,540,1.144,generated
005,**거대 언어 모델(LLM) 아키텍처 종합 비교**,10498,9372,9372,1.12,8958,2588,1.883,2768,452,4.629,8493,8493,1.236,7802,7802,1.346,generated
073,🥇이번 주의 최고의 인공지능(AI) 논문,2046,1225,1225,1.67,2501,2501,0.818,1376,2954,0.34,2525,2519,0.813,2374,2369,0.864,generated
088,"**GPT-5의 시각 점검: 선구적인 VLM(Vision-Language Model)이지만, 새로운 SOTA(State-Of-The-Art)는 아니다**",1144,1408,1555,0.708,1155,1155,0.99,618,1413,0.565,1152,1152,0.993,1056,1056,1.083,generated
048,**대규모 언어 모델(LLM) 학습 데이터(training data) 디버깅(debugging) 가이드**,8790,7586,7586,1.159,7088,7088,1.24,1133,7193,2.41,8484,8785,1.001,7158,7163,1.227,generated
013,**2024년 주목할 만한 AI 연구 논문 (2부)**,6141,5197,5541,1.115,6694,2998,1.47,4947,9464,0.328,5384,5384,1.141,5186,5158,1.19,generated
046,**보상 모델(Reward Models)**,7052,6434,5908,1.178,6800,6800,1.037,6316,6409,1.102,942,5938,2.183,5967,5966,1.182,generated
098,**신은 맥락에 굶주려 있다: o3 pro에 대한 첫 생각**,1041,1173,1309,0.772,1062,1062,0.98,933,861,1.193,961,901,1.146,853,853,1.22,generated
052,**추론 모델 파헤치기**,9037,8482,8484,1.065,7790,7784,1.161,7911,7907,1.143,7384,7383,1.224,7552,7539,1.198,generated
092,**소규모 팀 플레이북**,1126,1344,1773,0.519,1464,1464,0.769,1055,1491,0.654,1396,1396,0.807,1140,1140,0.988,generated
087,**코딩 에이전트가 스스로 개선할 수 있을까?**,3382,2346,3339,1.018,3141,3141,1.077,896,2896,1.542,3869,3710,0.915,2850,2850,1.187,generated
077,🥇 금주의 최고 인공지능(AI) 논문,2143,1715,2489,0.798,3255,3250,0.66,1195,2609,0.61,1389,2720,0.585,1753,1805,1.193,generated
103,**AI 괴짜들은 왜 못생긴 AI 생성 예술을 칭찬하는가?**,460,373,594,0.641,400,415,1.113,321,582,0.62,396,487,0.932,380,380,1.211,generated
058,**LLM(Large Language Models)을 활용한 평가**,9978,1816,5180,3.642,4504,4504,2.215,4962,3310,2.344,4531,4578,2.192,4264,4268,2.339,generated
093,몰록(Moloch)의 하이퍼스티션(hyperstitions),11571,1581,4958,5.183,1228,1368,9.309,1030,1627,10.654,1428,1431,8.101,1592,1593,7.268,generated
104,**AI 괴짜들은 그들이 이해할 수 없는 것을 참을 수 없다**,429,358,409,1.056,410,410,1.046,366,628,0.456,543,543,0.79,337,337,1.273,generated
108,**인공지능(AI) 기반 생산성의 순진성**,251,163,318,0.589,408,408,0.615,220,356,0.523,438,438,0.573,134,223,1.209,generated
109,**OpenAI Atlas: 훌륭했을 출시를 망칠 수 있는 하나의 치명적인 결함**,235,282,389,0.454,316,346,0.649,216,324,0.588,621,621,0.378,205,243,0.961,generated
071,**🥇 이번 주 최고의 인공지능(AI) 논문**,2432,2278,2540,0.953,3416,3411,0.713,2314,3536,0.523,3304,3292,0.74,2410,2406,1.011,generated
106,"AI 너드들은 벗어나려 하고, 우리도 그들과 함께 가게 될 것이다.",352,186,386,0.817,264,264,1.333,311,380,0.91,447,447,0.787,264,268,1.318,generated
053,"**전문가 혼합 (Mixture-of-Experts, MoE) 대규모 언어 모델 (LLMs)**",8047,3762,7873,1.046,7953,7972,1.009,7588,1554,1.856,7351,7351,1.095,6635,6917,1.17,generated
075,🥇 이번 주 최고의 AI 논문,2225,2456,2496,0.89,3164,3219,0.686,2056,2965,0.64,3971,3941,0.568,2862,2862,0.777,generated
100,AIEWF 2025 온라인! (및 참가자 가이드),771,672,952,0.731,934,934,0.825,656,1063,0.555,1027,1027,0.751,680,681,1.132,generated
105,**AI 덕후들이 어떻게 완벽한 정치적 꼭두각시가 되었나**,694,834,903,0.749,851,842,0.826,499,713,0.962,754,754,0.92,555,577,1.211,generated
054,대규모 언어 모델(LLMs)의 스케일링 법칙(Scaling Laws): GPT-3부터 o3까지,9629,7792,7864,1.227,9437,9437,1.02,3690,10072,0.88,0,0,0.0,8031,8031,1.199,generated
102,**대부분의 AI 연구를 신뢰할 수 없는 이유**,1459,1619,1619,0.901,1281,1292,1.13,1069,1277,1.17,1550,1550,0.941,897,985,1.528,generated
091,"**클라인: 오픈 소스(open source) 코드 에이전트(code agent) — 사우드 리즈완, 닉 패시와 함께**",17152,3941,3987,4.341,2964,3035,5.763,5613,9943,2.284,1944,2045,8.771,3003,3012,5.709,generated
097,"**AI 엔지니어링, 주류로 부상하다**",2581,1221,3119,0.559,2764,2764,0.934,2041,2827,0.879,3542,3542,0.729,2698,2698,0.957,generated
041,대규모 언어 모델(LLM)을 위한 프록시멀 정책 최적화(PPO): 일반인을 위한 가이드,12112,9251,7308,1.519,5719,1990,2.77,9265,8176,1.425,10339,7638,1.433,5968,9076,1.509,generated
079,🥇**이번 주 주요 AI(인공지능) 논문**,2375,1561,932,1.924,2338,3168,0.661,1866,3129,0.596,2019,2594,0.892,2515,2553,0.929,generated
112,"**OpenAI는 ChatGPT에 에로티카를 추가할 계획이지만, 진짜 문제는 덜 자극적이다**",807,650,877,0.892,707,751,1.079,655,670,1.209,847,847,0.953,697,697,1.158,generated
047,"AI 에이전트(AI Agents), 근본 원리(First Principles)부터",6756,2877,6287,1.163,8222,6015,1.09,6564,6564,1.029,5328,5256,1.282,5129,5129,1.317,generated
081,**에이전틱 커머스 프로토콜(Agentic Commerce Protocol)과 AI를 위한 경제 인프라 구축 — Stripe의 데이터 및 AI 총괄 Emily Glassberg Sands와 함께**,21731,1484,15470,5.219,1644,1789,13.13,1099,2456,18.539,17134,5238,1.963,1294,1294,16.794,generated
055,**평가를 위한 LLM 심사 모델 파인튜닝(finetuning)**,11521,4585,10023,1.327,8935,4244,1.814,7480,2753,2.172,8873,8873,1.298,9091,9091,1.267,generated
121,"**LWiAI 팟캐스트 #224 - OpenAI는 영리 기업이다! Cursor 2, Minimax M2, Udio 저작권**",562,731,1039,0.347,760,760,0.739,593,980,0.295,692,692,0.812,446,463,1.222,generated
051,**nanoMoE: 전문가 혼합(Mixture-of-Experts) 대규모 언어 모델(LLMs)을 PyTorch로 바닥부터 구현하기**,12136,9740,9739,1.246,10021,3935,1.818,3556,9434,1.76,9123,9123,1.33,9742,9178,1.304,generated
113,AI 사람들의 심리학에 대하여,1242,1263,1264,0.983,935,931,1.333,658,1133,1.166,1451,1448,0.858,1021,1021,1.216,generated
095,**Andrej Karpathy의 소프트웨어 3.0(Software 3.0): 인공지능(AI) 시대의 소프트웨어 (전체 대본으로 업데이트됨)**,1564,1094,1610,0.958,2188,2189,0.714,1507,1842,0.816,2818,2818,0.555,1302,1324,1.184,generated
110,**건강한 자기애가 부족하기 때문에 ChatGPT에 의존한다**,1539,1049,1562,0.978,1836,1836,0.838,1219,1232,1.252,1263,1263,1.219,1195,1195,1.288,generated
126,"지난주 AI 소식 #323 - Sonnet 4.5, Sora 2, Vibes, SB 53",140,126,165,0.802,436,436,0.321,133,193,0.602,521,521,0.269,133,133,1.053,generated
129,"지난주 인공지능(AI) 소식 #321 - 앤트로픽 및 미드저니 소송, 부진한 고용 지표",152,560,562,0.268,342,344,0.439,145,215,0.566,380,380,0.4,296,296,0.514,generated
083,"**감각이 당신의 해자입니다 — 딜런 필드, 피그마**",12904,2941,10132,1.943,8710,8712,1.481,353,8671,12.992,9012,9012,1.432,8675,8675,1.487,generated
117,**AI에게 집안일을 맡기고 싶지 않은 이유**,1057,1163,1165,0.907,1075,1075,0.983,827,953,1.126,959,956,1.105,842,842,1.255,generated
131,"**AI 주간 소식 #320 - Gemini 2.5 Flash 이미지, 소송 합의, 크롬용 Claude**",149,143,227,0.455,420,420,0.355,168,221,0.571,903,902,0.166,388,388,0.384,generated
118,**휴대폰 없이 사는 법**,1421,676,1417,1.006,1082,1082,1.313,1003,1467,0.954,1308,1308,1.086,1170,1170,1.215,generated
128,"지난주 AI 소식 #322 - 로보택시(robotaxi) 진행 상황, OpenAI 비즈니스, 크롬(Chrome)의 제미니(Gemini)",153,188,269,0.383,408,408,0.375,123,199,0.626,176,176,0.869,131,134,1.145,generated
057,**모델 병합(Model Merging): 서베이**,12537,7925,7891,1.586,7510,7510,1.669,7808,1057,2.47,7875,386,2.543,7473,7473,1.678,generated
090,**GPT-5 핸즈온: 석기 시대로 오신 것을 환영합니다**,2669,2560,3420,0.707,4134,4127,0.647,1354,2342,1.242,2928,2928,0.912,2005,2005,1.331,generated
101,**OpenAI가 얼마나 큰 위기에 처했는지 당신은 상상도 못 할 겁니다.**,3190,1047,2780,1.392,3079,3077,1.037,2620,2844,1.132,3185,3185,1.002,2550,2568,1.244,generated
132,"**지난주 인공지능(AI) #319 - GPT-5, 미국용 1달러 모델, DINOv3, 기술직(Tech Jobs)**",162,118,200,0.678,155,153,1.058,144,191,0.799,552,552,0.293,199,199,0.814,generated
043,LLM(대규모 언어 모델)을 위한 온라인 대 오프라인 RL(강화 학습),10121,2489,9451,1.269,8384,8390,1.206,7731,9104,1.132,8282,4951,1.624,8899,8899,1.137,generated
127,"**LWiAI 팟캐스트 #221 - OpenAI 코덱, 크롬의 제미니, K2-Think, SB 53**",469,464,550,0.825,621,681,0.659,321,619,0.533,908,908,0.517,509,509,0.921,generated
125,"**LWiAI 팟캐스트 #222 - Sora 2, Sonnet 4.5, 바이브스, 생각하는 기계들**",548,646,643,0.853,624,624,0.878,678,844,0.563,448,573,0.944,600,600,0.913,generated
123,"**LWiAI 팟캐스트 #223 - Haiku 4.5, OpenAI DevDay, SB 243**",553,565,887,0.409,1712,1712,0.323,940,1166,0.348,1093,1093,0.506,720,720,0.768,generated
115,**분명히 챗봇들이다**,1726,1073,1954,0.788,1641,1640,1.052,1588,1624,1.064,1552,1552,1.112,1416,1415,1.22,generated
044,**GPT-oss 바닥부터**,12163,11966,11946,1.018,10745,973,2.041,3371,6854,2.575,10551,1778,1.984,10500,10497,1.159,generated
134,"지난주 AI 소식 #318 - OpenAI 오픈소스 모델(OSS models), Opus 4.1, Gemini 2.5 심층 사고(Deep Think)",100,456,456,0.219,329,329,0.304,81,134,0.58,490,490,0.204,416,416,0.24,generated
042,**REINFORCE: 대규모 언어 모델(Large Language Models)을 위한 간편한 온라인 강화 학습(Online Reinforcement Learning)**,8017,7846,8000,1.002,6555,6537,1.226,7234,7311,1.098,6258,4011,1.64,6217,6217,1.29,generated
114,"AI 기업들, 당신의 영혼을 훔치려 자신들의 영혼을 팔아넘겼다",1862,1280,1814,1.038,1660,1660,1.122,930,1399,1.498,2116,2116,0.88,1548,1545,1.205,generated
085,**생성형 미디어(generative media)의 기술사 — Fal.ai의 Gorkem 및 Batuhan과 함께**,14626,2165,10776,2.778,3730,3548,3.97,9507,9524,1.537,9726,9734,1.503,9685,9685,1.51,generated
130,"LWiAI 팟캐스트 #220 - 제미니 2.5 플래시 이미지(Gemini 2.5 Flash Image), 크롬용 클로드(Claude for Chrome)",520,636,915,0.379,515,823,0.412,431,714,0.55,471,585,0.862,501,597,0.846,generated
059,**요약과 거대 언어 모델(Large Language Models)의 진화**,10411,5345,8860,1.29,8690,5131,1.608,8139,8163,1.276,8512,8511,1.223,9033,9032,1.153,generated
138,필립 고프: 의식(Consciousness) 이론으로서의 범심론(Panpsychism),244,265,384,0.472,705,705,0.346,211,435,0.095,523,524,0.465,284,284,0.859,generated
133,"**LWiAI 팟캐스트 #219 - GPT 5, Opus 4.1, OpenAI의 오픈 소스(Open Source), 아스트로케이드(Astrocade)**",577,737,965,0.474,656,656,0.88,1141,1331,0.339,907,907,0.636,1028,1028,0.561,generated
107,"AI 너드, 모든 것을 좋아하는 사람들",3208,2789,2900,1.11,2394,2394,1.34,1848,3586,0.795,2437,2437,1.316,2419,2434,1.32,generated
135,"**LWiAI 팟캐스트 #218 - 깃허브 스파크, 메가사이언스, 미국 AI 행동 계획**",426,254,530,0.591,665,665,0.641,358,596,0.525,1597,1830,0.121,544,528,0.812,generated
146,**대규모 언어 모델(LLM) 챗봇에 부족한 것: 목적 의식**,126,104,118,1.077,309,309,0.408,98,149,0.765,214,214,0.589,109,111,1.138,generated
084,"**GPT5 + Codex가 에이전틱 코딩(Agentic Coding)을 어떻게 장악했는가 — Greg Brockman, OpenAI와 함께**",17837,2179,13345,3.061,15400,15403,1.158,12019,12767,1.422,2667,2667,6.688,2980,3229,5.902,generated
082,"**개발자: 범용 인공지능(AGI)의 배포 계층 (OpenAI 개발자 데이 2025, 셔윈 우 및 크리스티나 황 출연)**",12034,1267,2312,8.673,7509,7509,1.603,7492,7225,1.642,7216,7169,1.674,6797,6797,1.77,generated
136,**범용 인공지능(AGI)은 멀티모달(multimodal)이 아니다.**,208,212,315,0.495,168,231,0.863,299,424,0.278,139,154,1.388,208,208,1.0,generated
145,미니 업데이트 #47: 첫 국제 AI 안전 조약 그리고 WavTokenizer 코덱(codec),120,160,179,0.631,163,164,0.73,148,187,0.547,142,142,0.845,289,289,0.415,generated
140,"**형태, 대칭성, 그리고 구조: 기계 학습(machine learning) 연구에서 수학의 변화하는 역할**",109,113,113,0.965,77,77,1.416,130,170,0.531,507,507,0.215,115,120,0.904,generated
137,"**2024년 AI, 네이선 베나이치와 함께**",509,465,650,0.697,1056,1056,0.482,275,628,0.567,697,699,0.727,568,568,0.896,generated
122,"**지난주 AI 소식 #325 - OpenAI는 영리 기업, ChatGPT 아틀라스, 코파일럿 미코**",2421,1763,3311,0.495,2616,2851,0.836,1516,2562,0.907,1803,2758,0.813,2648,2648,0.914,generated
139,**The Gradient에 찾아온 몇 가지 변화**,583,413,586,0.993,897,897,0.65,567,752,0.702,670,670,0.87,535,539,1.082,generated
147,다비다드 달림플: 증명 가능한 안전한 인공지능(AI)을 향하여,392,371,612,0.407,721,721,0.544,486,573,0.628,829,829,0.473,507,507,0.773,generated
142,**에반 래틀리프: 음성 에이전트(voice agent)와 함께하는 우리의 미래**,357,338,422,0.808,947,947,0.377,359,484,0.646,569,569,0.627,404,404,0.884,generated
056,**자동 프롬프트(prompt) 최적화**,11548,9874,9299,1.228,9008,9008,1.282,8915,9411,1.24,9213,976,2.148,9168,9168,1.26,generated
143,**메러디스 링겔 모리스: 생성형 AI (Generative AI)의 HCI (Human-Computer Interaction) 순간**,538,1069,1068,0.504,1045,1045,0.515,587,636,0.833,1015,1032,0.513,615,615,0.875,generated
086,"**""검색 증강 생성(RAG)은 죽었다, 컨텍스트 엔지니어링(Context Engineering)이 왕이다"" — Chroma의 Jeff Huber**",13619,9432,9432,1.444,2201,2200,6.188,1325,9216,4.323,8627,8627,1.579,6542,5478,2.244,generated
141,"**제이콥 안드레아스: 언어(Language), 접지(Grounding), 그리고 세계 모델(World Models)**",490,1166,1418,0.204,708,708,0.692,505,915,0.158,561,566,0.865,620,619,0.792,generated
124,"**지난주 AI 소식 #324: OpenAI 딜(deals) 및 데브데이(DevDay), 하이쿠(Haiku) 4.5, 베오(Veo) 3.1**",2417,3230,3428,0.687,3068,3068,0.788,1340,2885,0.651,2665,2674,0.904,2307,2371,1.02,generated
120,**나는 AI 산업이 끊임없이 잘못 묘사되고 있어서 기쁘다**,1538,713,1635,0.864,1541,1541,0.998,1076,1663,0.884,982,1311,1.231,1309,1309,1.175,generated
150,**주디 팬: 인간 인지 도구 키트 역공학**,500,569,665,0.71,627,634,0.786,475,764,0.444,947,942,0.533,521,521,0.96,generated
119,**스크롤하다가 죽을 겁니다**,4529,3021,4062,1.155,3676,3676,1.232,3421,3329,1.351,3528,3528,1.284,3468,3470,1.305,generated
176,**곧 공개**,5,534,534,0.009,223,223,0.022,67,72,0.0,83,83,0.06,258,253,0.039,generated
170,**SWE-Bench 저자들이 Neurips 2024에서 LLM 에이전트(LLM agents)의 현황에 대해 성찰하다**,86,208,208,0.413,334,331,0.266,82,138,0.366,75,75,1.147,76,76,1.132,generated
111,**실리콘 밸리는 잘못된 AI에 집착하고 있다**,5435,4482,4481,1.213,5769,5769,0.942,4124,4208,1.298,4589,4828,1.132,4243,4243,1.281,generated
148,**클라이브 톰슨: 기술(technology) 이야기**,428,492,597,0.657,600,730,0.497,409,690,0.359,517,517,0.828,416,416,1.029,generated
168,**트랜스포머(Transformer) LLM의 작동 원리 [무료 강좌]**,428,475,548,0.747,529,529,0.809,335,641,0.364,450,450,0.951,401,400,1.07,generated
173,"**저희가 책을 쓰고 있습니다! ""실전 거대 언어 모델(Hands-On Large Language Models)""**",148,127,127,1.165,311,311,0.476,128,164,0.875,173,173,0.855,137,140,1.058,generated
094,**테스트 시간 연산(Test Time Compute)을 다중 에이전트 문명(Multi-Agent Civilizations)으로 확장: 노암 브라운**,22679,2987,4219,7.18,2866,2866,7.913,3073,20566,1.688,12948,14060,1.666,2608,15948,3.581,generated
159,AI가 계산 재현성(computational reproducibility)을 자동화할 수 있을까?,1166,1214,1285,0.902,1096,1096,1.064,934,1323,0.832,1377,1377,0.847,1251,1251,0.932,generated
178,**금주의 AI 에이전트: 알아두어야 할 논문들**,291,664,790,0.248,615,615,0.473,290,408,0.597,487,457,0.659,439,439,0.663,generated
060,**프롬프트 엔지니어링(Prompt Engineering)의 현대적 발전**,10158,9972,10292,0.987,8360,8360,1.215,4207,7577,1.614,8082,5052,1.632,7894,7894,1.287,generated
144,**업데이트 #83: AI 음악 사기 (AI Music Fraud) 및 플랜서치 (PlanSearch)**,2542,1557,2973,0.723,1871,2581,0.979,1406,3254,0.494,1945,2383,1.082,2240,2239,1.135,generated
175,**언어 모델(Language Models)에게 정말 대단한 시기**,557,597,761,0.658,549,549,1.015,502,506,1.102,534,536,1.039,512,512,1.088,generated
181,금주의 AI 에이전트,495,595,841,0.418,673,673,0.736,434,780,0.343,1033,1033,0.479,582,589,0.838,generated
116,**신이 되고 싶어 했던 뉴런(neuron)**,7057,2851,6379,1.238,5548,5548,1.272,1697,6368,1.406,5352,5362,1.317,4697,5184,1.399,generated
156,**우리는 78개의 선거 딥페이크를 살펴보았습니다. 정치적 허위 정보는 AI 문제가 아닙니다.**,3161,2296,3109,1.023,1617,1617,1.955,1289,3491,0.744,2723,2724,1.16,2958,2671,1.166,generated
172,"**LLM 토크나이저(tokenizer), 시맨틱 검색(semantic search) 강좌, 그리고 도서 업데이트 #2**",855,1034,1187,0.679,1098,1098,0.779,723,1373,0.284,1198,1198,0.714,845,845,1.012,generated
171,"**저희 책, Hands-On Large Language Models이 드디어 출간되었습니다!**",583,710,786,0.714,538,550,1.061,379,663,0.789,875,875,0.666,472,472,1.235,generated
169,**그림으로 보는 DeepSeek-R1**,1837,1533,1791,1.03,1550,1688,1.096,1551,2446,0.607,1912,1912,0.961,1548,1544,1.189,generated
190,**금주의 AI 에이전트: 알아두어야 할 논문**,130,118,169,0.669,139,141,0.921,176,276,0.17,202,256,0.376,135,137,0.948,generated
174,"**LLM 대학, 생성형 AI(Generative AI), AI 제품 해자(AI Product Moats)**",272,383,425,0.601,395,395,0.689,219,405,0.393,318,326,0.83,243,243,1.119,generated
188,**금주의 AI 에이전트**,239,452,452,0.529,250,250,0.956,282,406,0.408,446,448,0.531,233,233,1.026,generated
160,**AI Snake Oil 책 온라인으로 읽기 시작**,1417,1253,1286,1.105,1701,1701,0.833,1449,1604,0.871,1745,1746,0.811,1368,1378,1.029,generated
163,**새로운 논문: 의미 있는 AI 에이전트(AI agents)**,1478,1623,1851,0.77,1645,1645,0.898,1182,1376,1.086,1430,1430,1.034,1348,1348,1.096,generated
179,**금주의 AI 에이전트: 알아두어야 할 논문들**,520,573,730,0.634,587,599,0.865,599,914,0.342,1128,1128,0.461,732,732,0.71,generated
157,영국의 간 이식 매칭 알고리즘이 젊은 환자들을 체계적으로 배제하는가?,3608,2092,3613,0.998,3414,3414,1.057,2710,2833,1.286,2978,2978,1.212,3008,3008,1.199,generated
186,**금주의 AI 에이전트**,217,302,302,0.719,198,226,0.955,269,294,0.714,497,497,0.437,211,211,1.028,generated
184,**금주의 AI 에이전트**,416,295,520,0.647,397,420,0.99,388,732,0.186,619,619,0.672,531,531,0.783,generated
158,**책과 저희의 집필 과정에 대한 자주 묻는 질문**,1636,1807,1807,0.905,1823,1914,0.848,1651,1835,0.879,1623,1623,1.008,1684,1684,0.971,generated
167,**도해 GPT-OSS**,1264,1647,1672,0.752,1548,1548,0.817,1337,1441,0.868,1660,1624,0.783,1070,1073,1.179,generated
161,**AI 기업들은 신을 만드는 것에서 제품을 구축하는 것으로 방향을 틀고 있다. 좋다.**,2330,1527,2040,1.19,2134,2134,1.092,1214,2076,1.209,2079,2352,0.989,1879,1880,1.239,generated
164,AI 스케일링 신화,2392,1941,2876,0.751,2240,2610,0.903,2512,2554,0.936,2310,2310,1.035,2018,2067,1.161,generated
200,**2만 구독자 할인 마지막 기회: 130달러 상당의 가치 있는 제품을 20달러에!**,301,560,604,0.459,511,511,0.589,384,389,0.771,593,586,0.519,287,287,1.049,generated
192,**데이터 과학자를 위한 효율적인 파이썬 강좌 [10/14]: .groupby()를 활용한 효과적인 데이터 변환**,1518,1487,1583,0.956,1426,1426,1.065,1051,1900,0.637,1075,1050,1.435,1179,1179,1.288,generated
195,"**저희는 프로덕션(production) 환경에서 AI 에이전트(AI agents)를 운영하는 데 47,000달러를 들였습니다. A2A와 MCP에 대해 아무도 알려주지 않는 점들이 여기에 있습니다.**",540,899,899,0.601,1054,1024,0.541,544,959,0.23,1239,1239,0.436,671,671,0.805,generated
155,**AI 발전이 둔화되고 있나요?**,4052,3016,3436,1.204,3438,3438,1.179,3344,3361,1.207,3564,3564,1.137,3074,3074,1.318,generated
099,"해석 가능성(interpretability)의 유용성 — 엠마누엘 아미젠, 앤트로픽",22236,3969,15450,2.71,1669,2562,12.788,14176,10144,1.853,14237,973,2.494,13910,13916,1.598,generated
165,**과학자들은 AI를 신탁이 아닌 도구로 사용해야 한다**,1833,1152,1588,1.213,1554,1708,1.08,1461,1967,0.908,2020,2020,0.907,1466,1466,1.25,generated
197,**실습 LangSmith 강좌[2/7]: LangSmith를 이용한 트레이싱(tracing)**,850,1449,1449,0.587,1221,1221,0.696,998,1001,0.849,1008,1008,0.843,799,799,1.064,generated
182,**마이크로소프트의 에이전트(agent)에 대한 최대 베팅... 아직까지는**,1372,1962,2100,0.629,1819,1819,0.754,775,1578,0.734,2019,2019,0.68,1516,1516,0.905,generated
180,**LLM 주간 주목 논문**,1476,1172,1309,1.142,1776,1776,0.831,1359,2508,0.241,1572,1659,0.884,1780,1780,0.829,generated
193,**거대 언어 모델 선택 마스터클래스**,1365,1602,1649,0.823,1676,1620,0.848,1184,1469,0.912,1535,1535,0.889,1463,1463,0.933,generated
166,**NeurIPS 2025 속으로: 올해의 AI 연구 지도**,2206,2359,2426,0.907,2193,2193,1.006,2153,2325,0.945,2643,2604,0.849,2054,2054,1.074,generated
149,**업데이트 제82호: 인공지능(AI) 소송 및 SOPHON**,3137,2341,2223,1.39,2548,3198,0.976,1871,4099,0.486,3098,3111,1.008,2689,2713,1.158,generated
153,**인공 일반 지능(AGI)은 이정표가 아니다**,4875,4421,4422,1.102,4616,4620,1.055,2815,3982,1.317,5452,5451,0.894,3783,3783,1.289,generated
199,**컨텍스트 엔지니어링(Context Engineering): DSPy GEPA를 사용하여 AI 코딩 에이전트 향상**,619,862,858,0.723,854,854,0.725,596,798,0.7,958,958,0.646,468,468,1.323,generated
185,**꼭 알아야 할 7가지 논문**,1944,2037,2207,0.871,2256,2255,0.862,1898,2247,0.84,2380,2385,0.815,2084,2092,0.929,generated
206,**스탠포드 AI 일자리 보고서: 당신의 생각과는 다릅니다**,983,1056,1056,0.931,1215,1215,0.809,866,942,1.047,1150,1150,0.855,1008,1008,0.975,generated
191,**알아두면 좋을 7가지 논문**,2517,3105,3151,0.796,2876,2876,0.875,2194,2292,1.103,2625,2625,0.959,2889,2773,0.911,generated
208,**평균 챗GPT(ChatGPT) 요청은 약 0.34Wh를 사용한다.**,422,394,544,0.69,497,511,0.821,434,633,0.514,493,493,0.856,391,401,1.054,generated
202,**실습 랭스미스(LangSmith) 강좌[1/7]: 랭스미스(LangSmith)란 무엇인가?**,1021,1161,1281,0.776,1365,1345,0.763,1262,1246,0.822,2075,1952,0.551,1205,1202,0.85,generated
196,**데이터 과학자를 위한 효율적인 파이썬 강좌 [9/14]: Pandas 데이터프레임(DataFrame)에서 값 효과적으로 선택 및 교체하기**,2045,1742,2922,0.497,2555,2537,0.807,2062,2078,0.984,1912,1872,1.09,1687,1776,1.159,generated
214,ChatGPT 해킹 노출: OpenAI가 아직 해결하지 못한 실제 제로 클릭 취약점(zero-click vulnerability) 존재,464,466,466,0.996,552,552,0.841,481,577,0.765,724,724,0.641,612,612,0.758,generated
212,**일상생활 속 AI 에너지 사용**,848,835,874,0.969,875,1062,0.755,754,1073,0.702,804,804,1.055,822,823,1.03,generated
210,**설득형 인공지능: 누가 프롬프트(prompt)를 쥐고 있는가?**,1002,984,1339,0.658,1201,1204,0.832,874,1107,0.88,1112,1112,0.901,867,869,1.153,generated
201,**10월 13일부터 10월 18일 주간 중요 거대 언어 모델(LLM) 논문**,1667,1987,2286,0.688,2124,2064,0.813,1369,2567,0.343,1939,1914,0.873,2357,2152,0.794,generated
215,Nate의 AI 스택(AI Stack): 내가 매일 사용하는 9가지 도구 + 나만의 것으로 만드는 6가지 프롬프트(prompt) + 자주 묻는 질문(FAQ),605,634,952,0.453,559,598,1.013,502,545,1.12,575,575,1.052,545,545,1.11,generated
198,**데이터 과학자를 위한 효율적인 파이썬 강좌 [8/14]: Pandas DataFrame 반복문 처리를 멈추고 대신 이렇게 하세요**,2101,2338,2320,0.906,2435,2435,0.863,951,2474,0.608,1945,1944,1.081,1750,1760,1.195,generated
096,**컴퓨트의 형태(The Shape of Compute) — Modular의 Chris Lattner와 함께**,18138,3228,3468,5.545,12626,12624,1.437,7496,5825,2.643,12528,12538,1.447,12651,12599,1.438,generated
151,**AI를 일반 기술로 이해하기 위한 가이드**,6233,3099,3286,1.951,4980,4979,1.252,3271,5431,1.245,6774,6851,0.909,4754,4753,1.311,generated
194,**10월 27일부터 11월 1일까지의 주요 거대 언어 모델(LLM) 논문**,2033,1705,2631,0.649,2348,2382,0.851,2477,2597,0.772,2628,2584,0.79,2648,2652,0.766,generated
187,**당신이 알아야 할 10가지 논문**,3425,2010,3623,0.901,4312,4312,0.794,3311,3713,0.913,3756,3756,0.912,3087,3087,1.109,generated
217,**ChatGPT 201: 고급 프롬프팅(prompting) 쉽게 배우기 (간편 시작 키트 + 프롬프트 + 가이드)**,687,633,1071,0.393,784,784,0.876,773,811,0.84,920,920,0.747,802,802,0.857,generated
207,**AI 주권(AI Sovereignty): 벌거벗은 임금님**,1533,1268,1881,0.726,1440,1440,1.065,1361,1481,1.038,1415,1416,1.083,1546,1545,0.992,generated
205,**AI 주권은 스위치가 아니라 스펙트럼이다**,1005,688,907,1.142,1213,1213,0.829,731,1109,0.858,1009,1135,0.871,1164,1164,0.863,generated
220,"**임원 브리핑**: 헤드라인이 엇갈릴 때, AI 네이티브 기업(AI-Native Companies)이 조직 AI 유창성(Institutional AI Fluency)을 구축하는 3가지 핵심 방법 (+ 시작을 위한 프롬프트)",569,812,930,0.555,712,712,0.799,445,602,0.926,707,706,0.806,500,500,1.138,generated
219,**부당 청구에 맞서 싸우기 위해 인공지능(AI)을 활용하는 최초의 가이드를 만들었습니다: 당신의 주도권을 되찾을 10가지 원칙 + 7가지 프롬프트(prompt)**,827,702,1156,0.531,1097,1097,0.754,780,1147,0.59,1076,1074,0.77,922,922,0.897,generated
183,**알아두면 좋을 논문들**,3328,3161,3160,1.053,3334,3334,0.998,3017,3203,1.041,4793,4872,0.678,3227,3225,1.032,generated
222,"**오픈 웹의 시대는 끝났다: 다음은 무엇이며, 왜 대기업이 아닌 개인과 소규모 브랜드에 유리한가 (+도움을 줄 7가지 프롬프트)**",604,801,812,0.74,591,591,1.022,495,618,0.972,778,790,0.761,628,628,0.962,generated
204,**과잉 공급 + 수요 부진 = 거품. AI는 아직 거품이 아니다.**,1341,1266,1804,0.634,1751,1730,0.778,982,1677,0.658,793,1612,0.658,1137,1172,1.149,generated
216,**프롬프트 닥터가 해결해 드립니다: 가장 흔한 ChatGPT 문제 6가지 해결 (25가지 실행 가능한 프롬프트(actionable prompts) 포함)**,981,1279,1343,0.717,1056,1056,0.929,797,1304,0.595,954,954,1.028,996,996,0.985,generated
211,**Lovable이 AI 도구(AI tools)에 대해 우리에게 가르쳐주는 것**,1433,1131,1786,0.688,1462,1461,0.981,1133,1337,1.085,1523,1523,0.941,1064,1113,1.301,generated
218,**AI와 일자리에 관해 수백 개 기업과 대화했습니다: 아무도 작성하지 않는 일자리 가이드 (+ 프롬프트)**,876,938,1000,0.868,1015,1015,0.863,1001,1668,0.209,1521,1521,0.576,879,879,0.997,generated
209,**에이전트형 AI (Agentic AI): AI 모델이 언론 및 규제 기관에 연락해야 하는가?**,1777,1122,1950,0.846,2008,2012,0.883,1222,1726,1.042,1822,1822,0.975,1528,1528,1.163,generated
221,**지난 한 달간 아무도 언급하지 않는 가장 중요한 AI 에이전트 뉴스 (+ 맞춤형 뉴스 프롬프트)**,278,549,549,0.506,202,202,1.376,164,248,1.183,472,472,0.589,231,238,1.173,generated
223,**AI 버블은 가짜다. 하지만 속으면 당신의 경력을 망칠 것이다 — 그 이유 + 보너스 스킬 프롬프트 팩**,750,847,847,0.885,940,940,0.798,599,691,1.098,753,753,0.996,746,746,1.005,generated
213,생성형 생물학 (Generative Biology),1378,1357,1553,0.871,1486,1486,0.927,1130,1720,0.697,1560,1558,0.885,1256,1256,1.097,generated
177,**꼭 알아야 할 논문들**,3523,3759,3759,0.937,3860,3848,0.916,2164,4479,0.558,6174,6153,0.574,3132,3132,1.125,generated
154,**일반 기술로서의 AI**,18885,10023,10531,1.833,10394,2565,2.57,10339,10399,1.821,10256,2951,2.554,10037,10183,1.867,generated
162,**AI 실존적 위험 확률은 정책 수립에 활용하기에는 너무 신뢰할 수 없다.**,6346,3641,5686,1.181,6340,3141,1.506,4546,5881,1.102,5188,5139,1.233,5048,5044,1.258,generated
203,**데이터 과학자를 위한 효율적인 파이썬 강의 [7/14]: 데이터 과학자로서 판다스를 효율적으로 사용하는 모범 사례**,5872,5641,5641,1.041,5064,5066,1.159,1802,5872,1.0,5203,5243,1.121,4883,4889,1.201,generated
189,**당신이 알아야 할 9편의 논문**,7009,6142,6230,1.127,6106,6097,1.149,5612,11259,0.243,6233,6233,1.124,5940,6125,1.149,generated
152,**AI가 과학 발전을 늦출 수 있을까?**,6735,2416,6059,1.28,5426,5426,1.241,1529,4346,2.562,4007,1354,2.343,4466,4468,1.508,generated
