올해를 시작하며, 드디어 **2024년 AI 연구 하이라이트** 기사의 **초안을 완성할 수 있었습니다.** 이 기사는 전문가 혼합 모델(mixture-of-experts models)부터 정밀도를 위한 새로운 LLM 스케일링 법칙(scaling laws)에 이르기까지 다양한 주제를 다룹니다. 2024년의 모든 주요 연구 성과를 되돌아보려면 아마 책 한 권을 써야 할 것입니다. 이처럼 빠르게 변화하는 분야에서도 유례없이 생산적인 한 해였습니다. 내용을 합리적으로 간결하게 유지하기 위해, 저는 **올해는 LLM 연구에만 집중하기로 결정했습니다.** 하지만 그렇다 하더라도, 이렇게 다사다난했던 한 해에서 어떻게 논문의 일부를 선택할 수 있을까요? 제가 생각할 수 있는 가장 간단한 접근 방식은 2024년 1월부터 12월까지 **매달 한 편의 논문을 소개하는 것이었습니다.** 따라서 이 기사에서는 제가 개인적으로 흥미롭거나 영향력이 크다고 생각한, 또는 이상적으로는 둘 다에 해당하는 연구 논문들을 공유할 것입니다.

하지만 이 글은 2024년 상반기(1월부터 6월까지)에 초점을 맞춘 **1부**라는 점에 유의하십시오. 7월부터 12월까지를 다루는 이 시리즈의 2부는 이미 발행되었거나 곧 발행될 예정입니다. 선정 기준은 올해 저에게 인상 깊었던 점들을 바탕으로 한 주관적인 것임을 인정합니다. 또한 다양성을 추구하여, LLM 모델 출시 소식만 다루지는 않았습니다. 더 광범위한 AI 연구 논문 목록을 찾으신다면, 제 이전 기사(LLM Research Papers: The 2024 List)를 확인해 보세요. 제가 이미 조금 나아졌고 천천히 하지만 꾸준히 회복 중이라는 소식을 전하게 되어 기쁩니다! 또한 모든 따뜻한 격려와 지원에 진심으로 감사드립니다. 그것은 저에게 정말 큰 의미였고 힘든 날들을 이겨내는 데 도움이 되었습니다! 새해 복 많이 받으시고 즐거운 독서 되세요! 2025년에도 인공지능 분야의 발전은 더욱 가속화될 것으로 예상됩니다.

### 1. 1월: Mixtral의 전문가 혼합(Mixture of Experts) 접근 방식

2024년 1월 초, Mistral AI 팀은 Mixtral 8x7B, 즉 희소 전문가 혼합(Sparse Mixture of Experts, SMoE) 모델을 설명하는 Mixtral of Experts 논문(2024년 1월 8일)을 공개했습니다. Mixtral 8x7B는 인상적인 성능을 가진 최초의 공개 가중치(open-weight) MoE LLM 중 하나였기 때문에, 당시 이 논문과 모델은 모두 매우 영향력이 컸습니다. 이 모델은 다양한 벤치마크에서 Llama 2 70B와 GPT-3.5를 능가했습니다. 이는 대규모 언어 모델의 설계 패러다임에 새로운 가능성을 제시한 중요한 사건이었습니다.

#### 1.1 MoE 모델 이해하기

**MoE, 즉 전문가 혼합(Mixture of Experts)은 GPT와 유사한 디코더(decoder) 아키텍처(architecture) 내부에 여러 개의 작은 "전문가(expert)" 서브네트워크(subnetwork)를 결합한 앙상블(ensemble) 모델입니다.** 각 서브네트워크는 다른 유형의 작업, 또는 더 구체적으로는 토큰(token)을 처리하는 역할을 한다고 알려져 있습니다. 여기서 아이디어는 하나의 큰 네트워크 대신 여러 개의 작은 서브네트워크를 사용하여, MoE가 **계산 자원(computational resources)을 더 효율적으로 할당하는 것을 목표로 한다는 것입니다.** 특히 Mixtral 8x7B에서는 아래 그림과 같이 트랜스포머(transformer) 아키텍처의 각 피드포워드 모듈(feed-forward module)을 8개의 전문가 레이어(expert layer)로 대체하는 것입니다.

Attention Is All You Need 논문에서 발췌한 주석이 달린 트랜스포머 아키텍처, https://arxiv.org/abs/1706.03762

"희소 전문가 혼합(Sparse Mixture of Experts)"이라는 맥락에서 "희소(Sparse)"는 주어진 시간에 전문가 레이어의 일부(Mixtral 8x7B에서는 일반적으로 8개 중 1개 또는 2개)만이 토큰 처리에 활발하게 사용된다는 사실을 의미합니다. 위 그림에서 보듯이, 서브네트워크는 LLM의 피드포워드 모듈을 대체합니다. 피드포워드 모듈은 본질적으로 다층 퍼셉트론(multilayer perceptron)입니다. PyTorch와 유사한 의사 코드(pseudocode)로 표현하면 다음과 같습니다.

```python
class FeedForward(torch.nn.Module):
    def __init__(self, embed_dim, coef):
        super().__init__()
        self.layers = nn.Sequential(
            torch.nn.Linear(embed_dim, coef*embed_dim),
            torch.nn.ReLU(),
            torch.nn.Linear(coef*n_embed, embed_dim),
            torch.nn.Dropout(dropout)
        )

    def forward(self, x):
        return self.layers(x)
```

또한, 각 토큰 임베딩(token embedding)을 8개의 전문가 피드포워드 모듈로 리디렉션(redirect)하는 라우터 모듈(Router module, 게이팅 네트워크(gating network)라고도 함)이 있으며, 이 모듈에서는 한 번에 이 전문가들 중 일부만 활성화됩니다. 이 기사에서 다룰 논문이 11개 더 있기 때문에, Mixtral 모델에 대한 설명은 간략하게 유지하고자 합니다. 하지만 더 자세한 내용은 제 이전 기사인 Model Merging, Mixtures of Experts, and Towards Smaller LLMs 에서 확인할 수 있습니다.

#### 1.2 오늘날 MoE 모델의 관련성

연초에는 공개 가중치(open-weight) MoE 모델이 지금보다 더 인기가 많고 널리 사용될 것이라고 생각했을 것입니다. 관련이 없는 것은 아니지만, Llama 3, Qwen 2.5, Gemma 2 등과 같은 많은 최첨단 모델(state-of-the-art models)은 여전히 MoE보다는 밀집(dense, 전통적인) LLM에 의존하고 있습니다. 하지만 GPT-4, Gemini, Claude와 같은 독점 아키텍처(proprietary architectures)가 무엇을 기반으로 하는지는 물론 알 수 없습니다. 이들도 내부적으로 MoE를 사용하고 있을 수 있습니다. 어떤 경우든, MoE 아키텍처는 여전히 중요합니다. 특히 각 입력에 대해 모델 매개변수(parameter)의 일부만 활성화하여 대규모 언어 모델(large language models)을 효율적으로 확장하는 방법을 제공함으로써, 모델 용량(model capacity)을 희생하지 않고 계산 비용(computation costs)을 줄일 수 있기 때문입니다. 이는 자원 효율적인 대규모 모델 구축에 핵심적인 역할을 합니다.

그런데 이 기사를 쓴 후, 12월에 MoE 아키텍처를 사용하는 매우 뛰어난 성능의 DeepSeek-V3 모델이 깜짝 출시되었습니다. 그리고 2025년 현재, MoE는 더욱 다양한 규모의 모델에서 그 효율성과 잠재력을 입증하며, 여전히 활발히 연구되고 적용되는 중요한 기술로 자리매김하고 있습니다. 그러니, 네, MoE는 계속해서 매우 중요합니다!

### 2. 2월: 가중치 분해 LoRA(Weight-decomposed LoRA)

**공개 가중치(open-weight) LLM을 미세 조정(finetuning)하고 있다면, 언젠가 매개변수 효율적인 LLM 미세 조정 방법인 저랭크 적응(low-rank adaptation, LoRA)을 사용했을 가능성이 높습니다.** LoRA가 처음이시라면, 제가 이전에 작성한 Practical Tips for Finetuning LLMs Using LoRA (Low-Rank Adaptation) 기사가 도움이 될 수 있으며, 제 저서 Build A Large Language Model (From Scratch) 의 부록 D에 처음부터 구현한 코드(from-scratch code implementation)가 있습니다. **LoRA는 매우 인기 있고 널리 사용되는 방법이며, 제가 새로운 변형을 구현하고 다루는 것이 매우 즐거웠기 때문에, 2월의 선택은 Liu 외 연구진의 DoRA: Weight-Decomposed Low-Rank Adaptation (2024년 2월)입니다.**

#### 2.1 LoRA 요약

DoRA를 소개하기 전에, LoRA에 대한 간략한 복습입니다.

전체 미세 조정(full finetuning)은 LLM의 각 큰 가중치 행렬(weight matrix) W를 큰 가중치 업데이트 행렬(weight update matrix) ΔW를 계산하여 업데이트합니다. LoRA는 ΔW를 두 개의 작은 행렬 A와 B의 곱으로 근사합니다. 따라서 W + ΔW 대신 W + A.B를 사용합니다. 이는 계산 및 메모리 오버헤드(overhead)를 크게 줄입니다. 아래 그림은 전체 미세 조정(왼쪽)과 LoRA(오른쪽)에 대한 이 공식들을 나란히 보여줍니다.

일반적인 미세 조정(왼쪽)과 LoRA 미세 조정(오른쪽)의 그림.

#### 2.2 LoRA에서 DoRA로

**DoRA: Weight-Decomposed Low-Rank Adaptation (2024년 2월)에서 Liu 외 연구진은 사전 훈련된 가중치 행렬(pretrained weight matrix)을 크기 벡터(magnitude vector) m과 방향 행렬(directional matrix) V의 두 부분으로 분해(decompose)함으로써 LoRA를 확장합니다.** 이 분해는 모든 벡터가 길이(크기)와 방향(방위)으로 표현될 수 있다는 아이디어에 기반하며, 여기서는 이를 가중치 행렬의 각 열 벡터(column vector)에 적용합니다. m과 V를 얻으면, DoRA는 방향 행렬 V에만 LoRA 스타일의 저랭크 업데이트(low-rank updates)를 적용하고, 크기 벡터 m은 별도로 훈련될 수 있도록 합니다.

DoRA 논문에서 발췌한 주석이 달린 그림 (https://arxiv.org/abs/2402.09353)

이 두 단계 접근 방식은 표준 LoRA보다 DoRA에 더 많은 유연성을 제공합니다. **LoRA가 크기와 방향을 균일하게 스케일링(scaling)하는 경향이 있는 것과 달리, DoRA는 크기를 반드시 증가시키지 않고도 미묘한 방향 조정을 할 수 있습니다.** 그 결과, DoRA는 더 적은 매개변수를 사용하더라도 LoRA보다 뛰어난 성능을 보이고 랭크(rank) 선택에 덜 민감하여 성능과 견고성(robustness)이 향상됩니다. 다시 한번, 다룰 내용이 10개 더 있기 때문에 이 섹션을 간략하게 유지하고 있지만, 더 자세한 내용에 관심이 있다면 올해 초 이 방법에 대해 전체 기사를 할애했습니다: Improving LoRA: Implementing Weight-Decomposed Low-Rank Adaptation (DoRA) from Scratch.

#### 2.3 LoRA 및 LoRA 유사 방법의 미래

DoRA는 원래 LoRA 방법에 대한 작고 논리적인 개선입니다. 아직 널리 채택되지는 않았지만, 최소한의 복잡성만을 추가하며 다음번에 LLM을 미세 조정할 때 고려해 볼 가치가 있습니다. 일반적으로 LoRA와 유사한 방법들이 계속 인기를 끌 것으로 예상합니다. 예를 들어, Apple은 최근 Apple Intelligence Foundation Language Models 논문에서 LLM의 온디바이스(on-device) 작업 특화(task specialization)를 위해 LoRA를 사용한다고 언급했습니다. 또한, LoRA는 `peft`와 같은 인기 있는 라이브러리에 통합되어 사용 편의성을 높이며, 다양한 응용 분야에서 LLM의 효율적인 맞춤형 학습을 가능하게 하고 있습니다. 이러한 추세는 2025년에도 지속될 것입니다.

Ahead of AI는 독자 지원 출판물입니다. 새로운 게시물을 받고 제 작업을 지원하려면 무료 또는 유료 구독자가 되는 것을 고려해 보세요.
구독하기

### 3. 3월: LLM 지속적 사전 훈련(Continually Pretraining)을 위한 팁

제가 아는 한, 명령어 미세 조정(instruction-finetuning)은 LLM 실무자들 사이에서 가장 인기 있는 미세 조정 형태입니다. 여기서 목표는 공개적으로 사용 가능한 LLM이 명령어를 더 잘 따르거나, 특정 하위 집합(subset) 또는 새로운 명령어에 대해 LLM을 특화시키는 것입니다. 하지만 새로운 지식을 습득하는 데 있어서는 계속된 사전 훈련(continued pretraining, 때로는 지속적 사전 훈련(continually pretraining)이라고도 함)이 올바른 방법입니다. 이 섹션에서는 Ibrahim 외 연구진의 신선하고 직관적인 Simple and Scalable Strategies to Continually Pre-train Large Language Models (2024년 3월) 논문을 간략하게 요약하고자 합니다.

#### 3.1 간단한 기술의 효과

이 **24페이지 분량의 Continually Pre-train Large Language Models 논문은 수많은 실험과 셀 수 없이 많은 그림을 보고하며, 오늘날의 기준으로 매우 철저합니다.** 계속된 사전 훈련을 성공적으로 적용하기 위한 주요 팁은 무엇이었을까요?

1.  학습률(learning rate)을 간단히 재가열(re-warming)하고 재감쇠(re-decaying)합니다.
2.  치명적인 망각(catastrophic forgetting)을 방지하기 위해 원래 사전 훈련 데이터의 작은 부분(예: 5%)을 새 데이터셋에 추가합니다. 0.5% 및 1%와 같은 더 작은 비율도 효과적이었습니다.

1번 항목인 재가열 및 재감쇠에 대해 좀 더 구체적으로 설명하자면, 이는 아래 그림에서 보듯이 LLM의 초기 사전 훈련 단계에서 사용된 것과 정확히 동일한 학습률 스케줄(learning rate schedule)을 사용한다는 의미입니다.

계속된 사전 훈련을 위한 스케줄.

Build a Large Language Model From Scratch 기반 그림, https://github.com/rasbt/LLMs-from-scratch/blob/main/appendix-D/01_main-chapter-code/appendix-D.ipynb

제가 아는 한, 재가열 및 재감쇠, 그리고 원래 사전 훈련 데이터를 새 데이터에 추가하는 것은 어느 정도 상식입니다. 하지만 연구자들이 이 매우 상세한 24페이지 보고서에서 이 방법을 공식적으로 테스트하는 데 시간을 할애했다는 점에 정말 감사드립니다. 더 자세한 내용에 관심이 있다면, 제 이전 기사 Tips for LLM Pretraining and Evaluating Reward Models 에서 이 논문을 더 자세히 다루었습니다.

#### 3.2 이러한 간단한 기술이 계속 작동할까요?

저는 이러한 방법들이 미래의 LLM에서도 계속 작동하지 않을 것이라고 믿을 이유가 없습니다. 하지만 최근 몇 달 동안 사전 훈련 파이프라인(pretraining pipelines)이 단기 및 장기 컨텍스트 사전 훈련(short- and long-context pretraining)을 포함한 여러 단계로 구성되어 더욱 정교해졌다는 점에 유의하는 것이 중요합니다. (New LLM Pre-training and Post-training Paradigms 에서 이에 대해 더 자세히 썼습니다.) **따라서 최적의 결과를 위해서는 이 논문에서 제안된 방법들이 특정 상황에서 조정될 필요가 있을 수 있습니다.** 2025년 현재, 이러한 기본 원칙들은 여전히 유효하지만, 데이터 증강(data augmentation) 및 동적 데이터 샘플링(dynamic data sampling)과 같은 고급 기법과 결합되어 성능을 더욱 최적화하는 방향으로 발전하고 있습니다.

### 4. 4월: LLM 정렬을 위한 DPO 또는 PPO, 아니면 둘 다?

4월은 어려운 선택입니다. 예를 들어, 콜모고로프-아놀드 네트워크(Kolmogorov-Arnold Networks)는 그 달에 큰 파장을 일으켰습니다. 하지만 제가 아는 한, 그 열기는 꽤 빠르게 식었습니다. 이는 아마도 이론적 보장(theoretical guarantees)을 실제로 구현하기 어렵고, 경쟁력 있는 결과나 벤치마크(benchmarks)가 부족하며, 다른 아키텍처가 훨씬 더 확장성이 높기 때문일 것입니다. 그래서 대신 4월의 선택은 더 실용적인 논문인 Xu 외 연구진의 Is DPO Superior to PPO for LLM Alignment? A Comprehensive Study (2024년 4월)입니다.

#### 4.1 RLHF-PPO와 DPO: 무엇인가요?

논문 자체를 요약하기 전에, 인간 피드백을 통한 강화 학습(Reinforcement Learning with Human Feedback, RLHF)을 통해 LLM을 정렬(aligning)하는 데 널리 사용되는 두 가지 방법인 근접 정책 최적화(Proximal Policy Optimization, PPO)와 직접 선호 최적화(Direct Preference Optimization, DPO)에 대한 개요입니다. RLHF는 LLM을 인간의 선호도에 맞춰 정렬하여 응답의 품질뿐만 아니라 안전성도 향상시키는 선택적 방법입니다.

일반적인 (간소화된) LLM 훈련 수명 주기.

전통적으로 RLHF-PPO는 InstructGPT 및 ChatGPT와 같은 모델 및 플랫폼을 위한 LLM 훈련에서 중요한 단계였습니다. 하지만 DPO는 단순성과 효율성 덕분에 작년부터 인기를 얻기 시작했습니다. RLHF-PPO와 달리 DPO는 별도의 보상 모델(reward model)을 필요로 하지 않습니다. 대신, 분류(classification)와 유사한 목표(objective)를 사용하여 LLM을 직접 업데이트합니다. PPO와의 포괄적인 비교는 부족하지만, 많은 LLM이 이제 DPO를 활용합니다.

아래는 제가 올해 초 개발하고 공유한 RLHF 및 DPO에 대한 두 가지 자료입니다.

*   LLM 훈련: RLHF 및 그 대안
*   LLM 정렬을 위한 직접 선호 최적화(DPO) (처음부터)

#### 4.2 PPO가 일반적으로 DPO보다 우수하다

**Is DPO Superior to PPO for LLM Alignment? A Comprehensive Study는 수많은 실험과 결과를 담은 잘 작성된 논문입니다.** 주요 결론은 PPO가 DPO보다 우수한 경향이 있으며, DPO는 분포 외 데이터(out-of-distribution data)를 다룰 때 성능이 떨어진다는 것입니다. 여기서 분포 외 데이터는 언어 모델이 이전에 DPO에 사용된 선호 데이터(preference data)와 다른 명령어 데이터(instruction data, 지도 미세 조정(supervised finetuning)을 통해)로 훈련되었다는 것을 의미합니다. 예를 들어, 모델은 일반 Alpaca 데이터셋으로 훈련된 후, 다른 선호도 레이블이 지정된 데이터셋에서 DPO 미세 조정을 거칠 수 있습니다. (하지만 이러한 분포 외 데이터에서 DPO를 개선하는 한 가지 방법은 먼저 선호 데이터셋을 사용하여 지도 명령어 미세 조정 단계를 수행한 다음 DPO 미세 조정을 수행하는 것입니다.) 주요 결과는 아래 그림에 요약되어 있습니다.

Is DPO Superior to PPO for LLM Alignment? A Comprehensive Study (https://arxiv.org/abs/2404.10719) 논문에서 발췌한 주석이 달린 표.

#### 4.3 오늘날 PPO와 DPO는 어떻게 사용될까요?

PPO는 결과 LLM의 원시 모델링 성능(raw modeling performance) 면에서 약간의 우위를 가질 수 있습니다. 하지만 DPO는 구현하기 훨씬 쉽고 계산적으로 적용하기 더 효율적입니다(결국 별도의 보상 모델을 훈련하고 사용할 필요가 없기 때문입니다). 따라서 제가 아는 한, DPO는 RLHF-PPO보다 실제에서 훨씬 더 널리 사용됩니다. 한 가지 흥미로운 예는 Meta AI의 Llama 모델입니다. Llama 2는 RLHF-PPO로 훈련되었지만, 더 새로운 Llama 3 모델은 DPO를 사용했습니다. 흥미롭게도, 요즘에는 최근 모델들이 PPO와 DPO를 모두 사용하기도 합니다. 최근 사례로는 Apple의 Foundation Models와 Allen AI의 Tulu 3가 있습니다. 2025년 현재, DPO의 변형인 IPO (Identity Preference Optimization)나 KTO (Kahneman-Tversky Optimization)와 같은 새로운 선호 학습 방법들도 등장하여, LLM 정렬 기법의 지평을 넓히고 있습니다. 이러한 발전은 PPO와 DPO를 넘어선 차세대 정렬 기술의 가능성을 보여줍니다.

### 5. 5월: LoRA는 덜 학습하고 덜 잊어버린다

올해 또 다른 LoRA 논문이 특히 흥미로웠습니다 (이 12개 논문 선정에서 마지막 LoRA 논문입니다, 약속합니다!). 획기적이라고는 할 수 없지만, LoRA를 사용하거나 사용하지 않고 LLM을 미세 조정하는 것에 대한 몇 가지 상식을 공식화한다는 점에서 정말 마음에 듭니다: Biderman 외 연구진의 LoRA Learns Less and Forgets Less (2024년 5월). LoRA Learns Less and Forgets Less는 대규모 언어 모델(LLM)에서 저랭크 적응(LoRA)과 전체 미세 조정(full finetuning)을 비교하는 실증 연구(empirical study)로, 두 가지 도메인(프로그래밍 및 수학)과 두 가지 작업(명령어 미세 조정 및 계속된 사전 훈련)에 초점을 맞춥니다. 계속하기 전에 LoRA에 대한 복습이 필요하시면 위의 2월 섹션을 확인해 보세요.

#### 5.1 LoRA는 덜 학습한다

**LoRA Learns Less and Forgets Less 연구는 LoRA가 전체 미세 조정보다 현저히 적게 학습한다는 것을 보여줍니다.** 특히 새로운 지식을 습득해야 하는 코딩과 같은 작업에서 더욱 그렇습니다. 명령어 미세 조정만 수행될 때는 그 차이가 더 작습니다. 이는 새로운 데이터로 사전 훈련(새로운 지식 학습)하는 것이 사전 훈련된 모델을 명령어 추종자(instruction follower)로 전환하는 것보다 전체 미세 조정에서 더 많은 이점을 얻는다는 것을 시사합니다.

전체 미세 조정 대 LoRA. 성능은 164개의 코딩 과제로 구성된 데이터셋인 HumanEval에서 측정됩니다.
LoRA Learns Less and Forgets Less 논문에서 발췌한 주석이 달린 그림, https://arxiv.org/abs/2405.09673.

하지만 몇 가지 더 미묘한 차이가 있습니다. 예를 들어, 수학 작업의 경우 LoRA와 전체 미세 조정 간의 차이가 줄어듭니다. 이는 수학 문제가 LLM에 더 익숙하고, 사전 훈련 중에 유사한 문제를 접했을 가능성이 있기 때문일 수 있습니다. 대조적으로, 코딩은 더 뚜렷한 도메인(domain)을 포함하며 더 많은 새로운 지식을 요구합니다. 따라서 새로운 작업이 모델의 사전 훈련 데이터와 멀리 떨어져 있을수록, 학습 능력(learning capacity) 면에서 전체 미세 조정이 더 유익해집니다.

#### 5.2 LoRA는 덜 잊어버린다

이전에 습득한 지식이 얼마나 손실되는지 조사할 때, LoRA는 일관되게 덜 잊어버립니다. 이는 원본 도메인(source domain)과 멀리 떨어진 데이터(예: 코딩)에 적응할 때 특히 분명합니다. 코딩 작업의 경우, 전체 미세 조정은 상당한 망각을 초래하는 반면, LoRA는 더 많은 원래 기능을 보존합니다. 모델의 원래 지식이 새로운 작업에 이미 더 가까웠던 수학에서는 그 차이가 덜 두드러집니다.

프로그래밍 데이터로 훈련한 후 원본 소스 작업에 대한 전체 미세 조정 대 LoRA.
LoRA Learns Less and Forgets Less 논문에서 발췌한 주석이 달린 그림, https://arxiv.org/abs/2405.09673.

#### 5.3 LoRA의 트레이드오프

**전반적으로 트레이드오프(trade-off)가 있습니다.** 전체 미세 조정은 더 먼 도메인에서 새로운 지식을 흡수하는 데 더 좋지만, 이전에 학습한 작업을 더 많이 잊어버리게 합니다. LoRA는 더 적은 매개변수를 변경함으로써 새로운 정보를 덜 학습하지만, 원래 기능을 더 많이 유지합니다.

#### 5.4 LLM 미세 조정을 위한 미래 접근 방식

이 연구는 주로 LoRA를 전체 미세 조정과 비교합니다. 실제로는 LoRA가 전체 미세 조정보다 훨씬 더 자원 효율적이기 때문에 인기를 얻었습니다. 많은 경우, 하드웨어 제약(hardware constraints)으로 인해 전체 미세 조정은 단순히 불가능합니다. 더욱이, 전문화된 애플리케이션(specialized applications)만 다루면 되는 경우 LoRA만으로도 충분할 수 있습니다. **LoRA 어댑터(adapter)는 기본 LLM과 별도로 저장될 수 있으므로, 새로운 기능을 추가하면서 원래 기능을 쉽게 보존할 수 있습니다.** 또한, 지식 업데이트에는 전체 미세 조정을 사용하고, 이후의 특화에는 LoRA를 사용하여 두 가지 방법을 결합하는 것도 가능합니다. 요컨대, 저는 두 가지 방법 모두 앞으로 몇 년 동안 계속해서 매우 중요할 것이라고 생각합니다. 당면한 작업에 적합한 접근 방식을 사용하는 것이 더 중요합니다. 2025년에는 LoRA의 효율성과 유연성을 극대화하기 위한 다양한 최적화 기법과 LoRA 기반의 새로운 아키텍처 연구가 더욱 활발해질 것으로 보입니다.

Ahead of AI는 독자 지원 출판물입니다. 새로운 게시물을 받고 제 작업을 지원하려면 무료 또는 유료 구독자가 되는 것을 고려해 보세요.
구독하기

### 6. 6월: 15조 토큰 FineWeb 데이터셋

Penedo 외 연구진의 The FineWeb Datasets: Decanting the Web for the Finest Text Data at Scale (2024년 6월) 논문은 LLM을 위한 15조 토큰(trillion token) 데이터셋(dataset) 생성과 이를 공개하는 과정을 설명하며, 데이터셋 다운로드 링크와 데이터셋 준비 단계를 재현할 수 있는 코드 저장소(code repository, `datatrove/examples/fineweb.py`)를 포함합니다.

#### 6.1 다른 데이터셋과의 비교

LLM 사전 훈련을 위한 다른 여러 대규모 데이터셋이 있는데, 이 데이터셋은 무엇이 특별할까요? 다른 데이터셋은 비교적 작습니다: RefinedWeb (5000억 토큰), C4 (1720억 토큰), Dolma 1.6의 Common Crawl 기반 부분 (3조 토큰) 및 1.7 (1.2조 토큰), The Pile (3400억 토큰), SlimPajama (6270억 토큰), RedPajama의 중복 제거된 변형 (20조 토큰), Matrix의 영어 CommonCrawl 섹션 (1.3조 토큰), 영어 CC-100 (700억 토큰), Colossal-OSCAR (8500억 토큰). 예를 들어, 약 3600억 토큰은 작은 LLM(예: Chinchilla 스케일링 법칙(scaling laws)에 따르면 17억 개)에만 적합합니다. 반면에 FineWeb 데이터셋의 15조 토큰은 Chinchilla 스케일링 법칙에 따라 최대 5000억 매개변수 모델에 최적일 것입니다. (RedPajama는 20조 토큰을 포함하지만, 연구자들은 RedPajama로 훈련된 모델이 적용된 필터링 규칙(filtering rules)의 차이로 인해 FineWeb보다 품질이 떨어진다는 것을 발견했습니다.)

수년간 LLM 사전 훈련에 사용된 데이터셋 크기 그림.
이는 단순히 일반적인 참고 자료이며 FineWeb 논문이나 Chinchilla 스케일링 법칙 논문과 직접적인 관련이 없다는 점에 유의하십시오.

요컨대, FineWeb 데이터셋(영어 전용)은 연구자와 실무자가 대규모 LLM을 훈련하는 것을 이론적으로 가능하게 합니다. (참고: 80억, 700억, 4050억 크기의 Llama 3 모델도 15조 토큰으로 훈련되었지만, Meta AI의 훈련 데이터셋은 공개적으로 이용할 수 없습니다.) FineWeb의 공개는 데이터셋 부족으로 대규모 모델 훈련에 어려움을 겪던 연구 커뮤니티에 큰 활력을 불어넣었습니다.

#### 6.2 원칙적인 데이터셋 개발

또한, 이 논문은 CommonCrawl 웹 코퍼스(web corpus)에서 시작하여 FineWeb 데이터셋에 도달하기 위해 필터링 규칙이 어떻게 개발되고 적용되었는지에 대한 원칙적인 절제 연구(ablation studies)와 통찰력을 담고 있습니다. 요컨대, 그들이 시도한 각 필터링 규칙에 대해, 원본 데이터와 필터링된 데이터에서 3600억 토큰의 무작위 샘플(random sample)을 추출한 다음, 작은 17억 1천만 매개변수 Llama 유사 모델을 훈련하여 HellaSwag, ARC, MMLU 등과 같은 표준 벤치마크에서 모델의 성능을 기반으로 필터링 규칙이 유익한지 여부를 확인했습니다. 이러한 체계적인 접근 방식은 고품질 학습 데이터셋을 구축하는 데 있어 중요한 모범 사례를 제시합니다.

#### 6.3 오늘날 FineWeb의 관련성

전반적으로, 수십억 매개변수 LLM을 사전 훈련하는 것이 대부분의 연구실과 기업의 역량을 넘어설 수 있지만, 이 데이터셋은 LLM 연구 및 개발을 민주화하는 데 상당한 진전입니다. 요약하자면, 이 논문은 칭찬할 만한 노력이며 LLM 사전 훈련 발전을 위한 귀중한 공공 자원을 소개합니다. FineWeb과 같은 고품질 대규모 공개 데이터셋은 LLM 연구의 접근성을 높이고, 데이터 기반의 혁신을 촉진하는 데 지속적으로 기여할 것입니다. 2025년에도 데이터의 양뿐만 아니라 질적 측면에서의 개선과 편향성 완화 연구가 더욱 중요해질 것입니다.

### 7월부터 12월까지: 2024년 하반기 연구 동향

이 연구 요약이 유용했기를 바랍니다! 2024년 상반기(1월부터 6월까지)의 LLM 연구 주요 논문들을 살펴보았습니다. 원래 이 글은 두 부분으로 나누어 발행될 예정이었으며, 두 번째 부분(7월부터 12월까지)은 스케일링 법칙, O1 재현, 그리고 LLM 훈련에서 합성 데이터(synthetic data)의 역할에 대한 더 최근 논문들을 다루며 많은 흥미로운 발전을 포함하고 있습니다. 해당 내용은 이미 별도의 글로 발행되어 독자 여러분께 공개되었으니, 2024년 한 해 동안의 LLM 연구 흐름을 완벽하게 이해하고 싶으시다면 해당 글도 함께 참고해 주시기 바랍니다. 이와 더불어, 2025년에 대한 제 생각과 앞으로 예상되는 것들을 공유하는 글도 곧 이어질 예정입니다. 계속 지켜봐 주세요!

이 잡지는 개인적인 열정 프로젝트입니다. 저를 지원하고 싶으신 분들은 제 저서 Build a Large Language Model (From Scratch) 를 구매해 주시면 감사하겠습니다. (이 책은 LLM이 어떻게 작동하는지 다른 곳에서는 찾아볼 수 없는 수준의 세부 사항으로 설명하기 때문에, 이 책에서 많은 것을 얻으실 것이라고 확신합니다.)

Build a Large Language Model (From Scratch) 지금 Amazon에서 구매 가능

책을 읽으시고 잠시 시간을 내주실 수 있다면, 짧은 리뷰를 남겨주시면 정말 감사하겠습니다. 그것은 저희 작가들에게 큰 도움이 됩니다! 여러분의 지원은 큰 의미가 있습니다! 감사합니다!

구독하기