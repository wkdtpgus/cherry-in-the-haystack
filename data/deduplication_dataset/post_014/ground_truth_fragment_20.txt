올해를 시작하며, AI 기술의 새로운 지평이 열렸음을 다시 한번 실감했습니다. 드디어 2024년 AI 연구 하이라이트 기사의 초안을 완성할 수 있었습니다. 이 기사는 전문가 혼합 모델(mixture-of-experts models)부터 정밀도를 위한 새로운 LLM 스케일링 법칙(scaling laws)에 이르기까지, AI가 사회 전반에 미치는 영향과 윤리적 문제까지 다양한 주제와 기술 동향을 다룹니다. 인공지능은 이제 단순한 기술을 넘어 우리 삶의 필수적인 부분이 되었으며, 그 발전 속도는 예측하기 어려울 정도입니다. 2024년의 모든 주요 연구 성과를 되돌아보려면 아마 책 한 권을 써야 할 것입니다. 이처럼 빠르게 변화하는 분야에서도 유례없이 생산적인 한 해였으며, 지난 한 해 동안 AI 분야는 놀라운 혁신을 거듭하며, 특히 생성형 AI와 자율 시스템 분야에서 눈부신 성과를 보였습니다. 하지만 그렇다 하더라도, 급변하는 기술 환경 속에서 핵심 동향을 파악하는 것은 중요합니다. 내용을 합리적으로 간결하게 유지하기 위해 올해는 LLM 연구에 집중하면서도, 이러한 배경 속에서 2024년 상반기 동안 가장 두드러졌던 AI 기술 트렌드와 그 의미를 되짚어보고자 합니다. 제가 생각할 수 있는 가장 간단한 접근 방식은 2024년 1월부터 12월까지 매달 한 편의 논문을 소개하는 것이었습니다. 따라서 이 기사에서는 제가 개인적으로 흥미롭거나 영향력이 크다고 생각한, 또는 이상적으로는 둘 다에 해당하는 연구 논문 및 기술 동향들을 공유할 것입니다. 하지만 이 기사는 2024년 상반기(1월부터 6월까지)에 초점을 맞춘 **1부**라는 점에 유의하십시오. 7월부터 12월까지를 다루는 이 시리즈의 2부는 1월 말에 공유될 예정입니다. 선정 기준은 올해 저에게 인상 깊었던 점들과 변화를 바탕으로 한 주관적인 것임을 인정합니다. 또한 다양성을 추구하여, LLM 모델 출시 소식만 다루지는 않았으며, 다양한 산업 분야에 걸친 AI의 응용 사례들을 함께 살펴볼 것입니다. 더 광범위한 AI 연구 논문 목록이나 혁신 사례를 찾으신다면, 제 이전 기사(LLM Research Papers: The 2024 List) 및 이전 글들을 확인해 보세요. 이 글을 통해 독자 여러분이 AI의 현재와 미래를 이해하는 데 도움이 되기를 바랍니다. 새해 복 많이 받으시고, AI의 미래를 함께 탐구해 봅시다!

### 1. 1월: Mixtral의 전문가 혼합(Mixture of Experts) 접근 방식과 개방형 AI 모델의 부상

2024년 1월 초, AI 기술의 새로운 지평을 여는 중요한 발표가 있었습니다. Mistral AI 팀은 Mixtral 8x7B, 즉 희소 전문가 혼합(Sparse Mixture of Experts, SMoE) 모델을 설명하는 Mixtral of Experts 논문(2024년 1월 8일)을 공개했습니다. 특정 모델의 세부 사항보다는 개방형 AI(open-source AI) 생태계 전반의 성장이 주목할 만했습니다. Mixtral 8x7B는 인상적인 성능을 가진 최초의 공개 가중치(open-weight) MoE LLM 중 하나였기 때문에, 당시 이 논문과 모델은 모두 매우 영향력이 컸습니다. 이 모델은 다양한 벤치마크에서 Llama 2 70B와 GPT-3.5를 능가했으며, 다양한 분야에서 기존 시스템의 한계를 뛰어넘는 가능성을 보여주었습니다. 이는 AI 연구의 투명성을 높이고, 더 많은 개발자들이 고급 AI 모델에 접근할 수 있게 하여 혁신을 가속화하는 중요한 전환점이었습니다.

#### 1.1 MoE 모델 이해하기 및 개방형 AI의 확산

MoE, 즉 전문가 혼합(Mixture of Experts)은 GPT와 유사한 디코더(decoder) 아키텍처(architecture) 내부에 여러 개의 작은 "전문가(expert)" 서브네트워크(subnetwork)를 결합한 앙상블(ensemble) 모델입니다. 각 서브네트워크는 다른 유형의 작업, 또는 더 구체적으로는 토큰(token)을 처리하는 역할을 한다고 알려져 있습니다. 여기서 아이디어는 하나의 큰 네트워크 대신 여러 개의 작은 서브네트워크를 사용하여, MoE가 계산 자원(computational resources)을 더 효율적으로 할당하는 것을 목표로 한다는 것입니다. 특히 Mixtral 8x7B에서는 아래 그림과 같이 트랜스포머(transformer) 아키텍처의 각 피드포워드 모듈(feed-forward module)을 8개의 전문가 레이어(expert layer)로 대체하는 것입니다.

Attention Is All You Need 논문에서 발췌한 주석이 달린 트랜스포머 아키텍처, https://arxiv.org/abs/1706.03762

"희소 전문가 혼합(Sparse Mixture of Experts)"이라는 맥락에서 "희소(Sparse)"는 주어진 시간에 전문가 레이어의 일부(Mixtral 8x7B에서는 일반적으로 8개 중 1개 또는 2개)만이 토큰 처리에 활발하게 사용된다는 사실을 의미합니다. 위 그림에서 보듯이, 서브네트워크는 LLM의 피드포워드 모듈을 대체합니다. 피드포워드 모듈은 본질적으로 다층 퍼셉트론(multilayer perceptron)입니다. PyTorch와 유사한 의사 코드(pseudocode)로 표현하면 다음과 같습니다.

```python
class FeedForward(torch.nn.Module):
    def __init__(self, embed_dim, coef):
        super().__init__()
        self.layers = nn.Sequential(
            torch.nn.Linear(embed_dim, coef*embed_dim),
            torch.nn.ReLU(),
            torch.nn.Linear(coef*n_embed, embed_dim),
            torch.nn.Dropout(dropout)
        )

    def forward(self, x):
        return self.layers(x)
```

또한, 각 토큰 임베딩(token embedding)을 8개의 전문가 피드포워드 모듈로 리디렉션(redirect)하는 라우터 모듈(Router module, 게이팅 네트워크(gating network)라고도 함)이 있으며, 이 모듈에서는 한 번에 이 전문가들 중 일부만 활성화됩니다. 이 기사에서 다룰 논문이 11개 더 있기 때문에, Mixtral 모델에 대한 설명은 간략하게 유지하고자 합니다. 하지만 더 자세한 내용은 제 이전 기사인 Model Merging, Mixtures of Experts, and Towards Smaller LLMs 에서 확인할 수 있습니다.
개방형 AI 모델은 연구 커뮤니티와 산업계 모두에게 강력한 도구를 제공합니다. 전통적인 폐쇄형 모델과 달리, 개방형 모델은 개발자들이 모델 아키텍처를 검토하고, 커스터마이징하며, 특정 사용 사례에 맞게 최적화할 수 있도록 합니다. 이러한 접근 방식은 AI 기술의 민주화를 촉진하고, 다양한 관점에서 AI의 잠재력을 탐구할 수 있는 길을 열어줍니다. 예를 들어, 특정 언어나 문화적 맥락에 특화된 모델을 개발하는 데 있어 개방형 접근 방식은 필수적입니다.

#### 1.2 오늘날 MoE 모델의 관련성과 협력적 혁신의 가속화

개방형 AI는 전 세계 개발자들의 협력을 통해 빠르게 발전하고 있습니다. 커뮤니티 주도의 개선과 새로운 기능 추가는 모델의 견고성과 다양성을 향상시킵니다. 이러한 협력적 생태계는 버그를 빠르게 식별하고 수정하며, 새로운 아이디어를 실험하고 통합하는 데 기여합니다. 결과적으로, 우리는 더욱 강력하고 유연하며 신뢰할 수 있는 AI 시스템을 구축할 수 있게 됩니다. 이는 단순히 기술적인 진보를 넘어, AI가 사회에 긍정적인 영향을 미칠 수 있는 광범위한 가능성을 제시합니다.
연초에는 공개 가중치(open-weight) MoE 모델이 지금보다 더 인기가 많고 널리 사용될 것이라고 생각했을 것입니다. 관련이 없는 것은 아니지만, Llama 3, Qwen 2.5, Gemma 2 등과 같은 많은 최첨단 모델(state-of-the-art models)은 여전히 MoE보다는 밀집(dense, 전통적인) LLM에 의존하고 있습니다. 하지만 GPT-4, Gemini, Claude와 같은 독점 아키텍처(proprietary architectures)가 무엇을 기반으로 하는지는 물론 알 수 없습니다. 이들도 내부적으로 MoE를 사용하고 있을 수 있습니다. 어떤 경우든, MoE 아키텍처는 여전히 중요합니다. 특히 각 입력에 대해 모델 매개변수(parameter)의 일부만 활성화하여 대규모 언어 모델(large language models)을 효율적으로 확장하는 방법을 제공함으로써, 모델 용량(model capacity)을 희생하지 않고 계산 비용(computation costs)을 줄일 수 있기 때문입니다.
그런데 이 기사를 쓴 후, 12월에 MoE 아키텍처를 사용하는 매우 뛰어난 성능의 DeepSeek-V3 모델이 깜짝 출시되었습니다. 그러니, 네, MoE는 계속해서 매우 중요합니다!

### 2. 2월: AI 모델 경량화와 효율성의 추구 (DoRA: Weight-Decomposed Low-Rank Adaptation)

공개 가중치(open-weight) LLM을 미세 조정(finetuning)하고 있다면, 언젠가 매개변수 효율적인 LLM 미세 조정 방법인 저랭크 적응(low-rank adaptation, LoRA)과 같은 자원 효율적인 최적화 기법을 사용했을 가능성이 높습니다. 특히 대규모 언어 모델(LLM)의 급증은 모델의 크기와 연산 비용 문제를 더욱 부각시켰습니다. LoRA는 매우 인기 있고 널리 사용되는 방법이며, 제가 새로운 변형을 구현하고 다루는 것이 매우 즐거웠기 때문에, 2월의 선택은 Liu 외 연구진의 DoRA: Weight-Decomposed Low-Rank Adaptation (2024년 2월)과 같은 효율적인 모델 경량화 기술의 발전입니다. 이러한 기술들은 제한된 하드웨어 자원에서도 고성능 AI를 구현할 수 있도록 돕습니다. LoRA가 처음이시라면, 제가 이전에 작성한 Practical Tips for Finetuning LLMs Using LoRA (Low-Rank Adaptation) 기사가 도움이 될 수 있으며, 제 저서 Build A Large Language Model (From Scratch) 의 부록 D에 처음부터 구현한 코드(from-scratch code implementation)가 있습니다.

#### 2.1 모델 경량화의 필요성 및 LoRA 요약

최근 AI 모델은 그 규모가 점점 커지고 있어, 이를 훈련하고 배포하는 데 막대한 컴퓨팅 자원이 소요됩니다. 이는 환경 문제와 접근성 문제로 이어지기도 합니다. 따라서 모델 경량화 기술은 필수적입니다. 양자화(quantization), 가지치기(pruning), 지식 증류(knowledge distillation) 등 다양한 기법들이 연구되고 있으며, 이들은 모델의 성능 저하를 최소화하면서 크기를 줄이는 것을 목표로 합니다. 이러한 노력은 AI를 더욱 지속 가능하고 보편적으로 만들 수 있습니다.
DoRA를 소개하기 전에, LoRA에 대한 간략한 복습입니다. 전체 미세 조정(full finetuning)은 LLM의 각 큰 가중치 행렬(weight matrix) W를 큰 가중치 업데이트 행렬(weight update matrix) ΔW를 계산하여 업데이트합니다. LoRA는 ΔW를 두 개의 작은 행렬 A와 B의 곱으로 근사합니다. 따라서 W + ΔW 대신 W + A.B를 사용합니다. 이는 계산 및 메모리 오버헤드(overhead)를 크게 줄입니다. 아래 그림은 전체 미세 조정(왼쪽)과 LoRA(오른쪽)에 대한 이 공식들을 나란히 보여줍니다.

일반적인 미세 조정(왼쪽)과 LoRA 미세 조정(오른쪽)의 그림.

#### 2.2 LoRA에서 DoRA로의 발전과 엣지 디바이스 AI의 부상

DoRA: Weight-Decomposed Low-Rank Adaptation (2024년 2월)에서 Liu 외 연구진은 사전 훈련된 가중치 행렬(pretrained weight matrix)을 크기 벡터(magnitude vector) m과 방향 행렬(directional matrix) V의 두 부분으로 분해(decompose)함으로써 LoRA를 확장합니다. 이 분해는 모든 벡터가 길이(크기)와 방향(방위)으로 표현될 수 있다는 아이디어에 기반하며, 여기서는 이를 가중치 행렬의 각 열 벡터(column vector)에 적용합니다. m과 V를 얻으면, DoRA는 방향 행렬 V에만 LoRA 스타일의 저랭크 업데이트(low-rank updates)를 적용하고, 크기 벡터 m은 별도로 훈련될 수 있도록 합니다.

DoRA 논문에서 발췌한 주석이 달린 그림 (https://arxiv.org/abs/2402.09353)

이 두 단계 접근 방식은 표준 LoRA보다 DoRA에 더 많은 유연성을 제공합니다. LoRA가 크기와 방향을 균일하게 스케일링(scaling)하는 경향이 있는 것과 달리, DoRA는 크기를 반드시 증가시키지 않고도 미묘한 방향 조정을 할 수 있습니다. 그 결과, DoRA는 더 적은 매개변수를 사용하더라도 LoRA보다 뛰어난 성능을 보이고 랭크(rank) 선택에 덜 민감하여 성능과 견고성(robustness)이 향상됩니다. 다시 한번, 다룰 내용이 10개 더 있기 때문에 이 섹션을 간략하게 유지하고 있지만, 더 자세한 내용에 관심이 있다면 올해 초 이 방법에 대해 전체 기사를 할애했습니다: Improving LoRA: Implementing Weight-Decomposed Low-Rank Adaptation (DoRA) from Scratch.
모델 경량화의 궁극적인 목표 중 하나는 AI를 엣지 디바이스(edge devices)에서도 효율적으로 실행하는 것입니다. 스마트폰, 웨어러블 기기, IoT 센서 등에서 AI를 직접 구동함으로써, 데이터 전송 지연을 줄이고 개인 정보 보호를 강화할 수 있습니다. 예를 들어, 스마트폰에서 실시간으로 음성 인식을 처리하거나, 드론이 현장에서 즉시 이미지 분석을 수행하는 것이 가능해집니다. 이러한 온디바이스(on-device) AI는 새로운 애플리케이션의 가능성을 열고, AI 기술의 적용 범위를 더욱 확장하고 있습니다.

#### 2.3 LoRA 및 LoRA 유사 방법의 미래

DoRA는 원래 LoRA 방법에 대한 작고 논리적인 개선입니다. 아직 널리 채택되지는 않았지만, 최소한의 복잡성만을 추가하며 다음번에 LLM을 미세 조정할 때 고려해 볼 가치가 있습니다. 일반적으로 LoRA와 유사한 방법들이 계속 인기를 끌 것으로 예상합니다. 예를 들어, Apple은 최근 Apple Intelligence Foundation Language Models 논문에서 LLM의 온디바이스(on-device) 작업 특화(task specialization)를 위해 LoRA를 사용한다고 언급했습니다.

### 3. 3월: AI의 지속적인 학습과 적응 능력, 그리고 LLM 지속적 사전 훈련(Continually Pretraining)을 위한 팁

제가 아는 한, AI 모델의 성능을 최적화하는 다양한 전략이 존재합니다. 명령어 미세 조정(instruction-finetuning)은 LLM 실무자들 사이에서 가장 인기 있는 미세 조정 형태이지만, 특히 AI가 현실 세계의 변화에 유연하게 대응하고 새로운 정보를 통합하는 능력은 매우 중요합니다. 기존의 학습 방식은 정적인 데이터셋에 의존하는 경향이 있었지만, 동적인 환경에서는 지속적인 업데이트가 필요합니다. 하지만 새로운 지식을 습득하는 데 있어서는 계속된 사전 훈련(continued pretraining, 때로는 지속적 사전 훈련(continually pretraining)이라고도 함)과 효율적인 데이터 관리 및 학습 전략이 올바른 방법입니다. 이 섹션에서는 Ibrahim 외 연구진의 신선하고 직관적인 Simple and Scalable Strategies to Continually Pre-train Large Language Models (2024년 3월) 논문을 간략하게 요약하고, 이러한 학습 방법론이 다양한 분야에 적용될 수 있음을 보여주고자 합니다.

#### 3.1 평생 학습(Lifelong Learning) 시스템의 중요성과 간단한 기술의 효과

AI 시스템이 실제 환경에서 효과적으로 작동하려면, 한 번 학습된 지식에만 의존해서는 안 됩니다. 평생 학습(lifelong learning)은 AI가 새로운 정보를 지속적으로 흡수하고, 기존 지식과 통합하며, 시간이 지남에 따라 성능을 향상시키는 능력을 의미합니다. 이는 특히 빠르게 변화하는 산업 분야나 개인화된 서비스에서 필수적입니다. 예를 들어, 자율 주행 차량은 끊임없이 변화하는 도로 상황과 새로운 교통 법규를 학습해야 하며, 개인 비서는 사용자의 변화하는 선호도를 이해하고 적응해야 합니다.
이 24페이지 분량의 Continually Pre-train Large Language Models 논문은 수많은 실험과 셀 수 없이 많은 그림을 보고하며, 오늘날의 기준으로 매우 철저합니다. 계속된 사전 훈련을 성공적으로 적용하기 위한 주요 팁은 무엇이었을까요?
1.  학습률(learning rate)을 간단히 재가열(re-warming)하고 재감쇠(re-decaying)합니다.
2.  치명적인 망각(catastrophic forgetting)을 방지하기 위해 원래 사전 훈련 데이터의 작은 부분(예: 5%)을 새 데이터셋에 추가합니다. 0.5% 및 1%와 같은 더 작은 비율도 효과적이었습니다.
1번 항목인 재가열 및 재감쇠에 대해 좀 더 구체적으로 설명하자면, 이는 아래 그림에서 보듯이 LLM의 초기 사전 훈련 단계에서 사용된 것과 정확히 동일한 학습률 스케줄(learning rate schedule)을 사용한다는 의미입니다.

계속된 사전 훈련을 위한 스케줄.

Build a Large Language Model From Scratch 기반 그림, https://github.com/rasbt/LLMs-from-scratch/blob/main/appendix-D/01_main-chapter-code/appendix-D.ipynb

제가 아는 한, 재가열 및 재감쇠, 그리고 원래 사전 훈련 데이터를 새 데이터에 추가하는 것은 어느 정도 상식입니다. 하지만 연구자들이 이 매우 상세한 24페이지 보고서에서 이 방법을 공식적으로 테스트하는 데 시간을 할애했다는 점에 정말 감사드립니다. 더 자세한 내용에 관심이 있다면, 제 이전 기사 Tips for LLM Pretraining and Evaluating Reward Models 에서 이 논문을 더 자세히 다루었습니다.

#### 3.2 치명적 망각(Catastrophic Forgetting) 극복 전략과 지속적인 기술 적용 가능성

새로운 정보를 학습할 때 기존 지식을 잊어버리는 '치명적 망각(catastrophic forgetting)'은 평생 학습 시스템의 주요 도전 과제입니다. 이를 극복하기 위해 다양한 전략이 연구되고 있습니다. 예를 들어, 과거 데이터의 일부를 반복해서 학습하는 리플레이(replay) 기법이나, 새로운 학습 과정에서 기존 지식의 손실을 방지하는 정규화(regularization) 기법 등이 활용됩니다. 이러한 접근 방식은 AI가 과거의 경험을 보존하면서도 새로운 지평을 탐색할 수 있도록 돕습니다. 결과적으로, AI는 더욱 견고하고 지능적인 시스템으로 발전하게 됩니다.
저는 이러한 방법들이 미래의 LLM에서도 계속 작동하지 않을 것이라고 믿을 이유가 없습니다. 하지만 최근 몇 달 동안 사전 훈련 파이프라인(pretraining pipelines)이 단기 및 장기 컨텍스트 사전 훈련(short- and long-context pretraining)을 포함한 여러 단계로 구성되어 더욱 정교해졌다는 점에 유의하는 것이 중요합니다. (New LLM Pre-training and Post-training Paradigms 에서 이에 대해 더 자세히 썼습니다.) 따라서 최적의 결과를 위해서는 이 논문에서 제안된 방법들이 특정 상황에서 조정될 필요가 있을 수 있습니다.

### 4. 4월: AI 윤리와 사회적 책임의 중요성, 그리고 LLM 정렬을 위한 DPO 또는 PPO

4월은 AI 기술의 윤리적 책임에 대한 중요한 논의가 활발했던 시기입니다. 기술의 발전 속도만큼이나 그 영향력에 대한 깊은 성찰이 요구되었습니다. AI가 사회에 미치는 긍정적인 파급 효과는 분명하지만, 잠재적인 위험과 부작용에 대한 우려도 커지고 있습니다. 하지만 제가 아는 한, 기술 발전의 속도만큼이나 신중한 접근이 필요하다는 인식이 확산되었습니다. 이는 AI 시스템의 '정렬(alignment)' 문제가 단순히 기술적인 최적화를 넘어 사회적, 윤리적 합의를 필요로 한다는 점을 강조합니다. 콜모고로프-아놀드 네트워크(Kolmogorov-Arnold Networks)와 같은 이론적 보장을 실제 사회 문제에 적용하기 어렵고, 다양한 이해관계자의 합의가 부족하며, 다른 사회적 제도가 훨씬 더 포괄적이기 때문일 것입니다. 그래서 대신 4월의 선택은 더 실용적인 AI 윤리 가이드라인의 수립과 적용에 관한 논의와 함께, Xu 외 연구진의 Is DPO Superior to PPO for LLM Alignment? A Comprehensive Study (2024년 4월) 논문이었습니다.

#### 4.1 RLHF-PPO와 DPO 개요 및 AI 윤리 원칙의 확립

논문 자체를 요약하기 전에, 인간 피드백을 통한 강화 학습(Reinforcement Learning with Human Feedback, RLHF)을 통해 LLM을 정렬(aligning)하는 데 널리 사용되는 두 가지 방법인 근접 정책 최적화(Proximal Policy Optimization, PPO)와 직접 선호 최적화(Direct Preference Optimization, DPO)에 대한 개요입니다. RLHF는 LLM을 인간의 선호도에 맞춰 정렬하여 응답의 품질뿐만 아니라 안전성도 향상시키는 선택적 방법입니다.

일반적인 (간소화된) LLM 훈련 수명 주기.

전통적으로 RLHF-PPO는 InstructGPT 및 ChatGPT와 같은 모델 및 플랫폼을 위한 LLM 훈련에서 중요한 단계였습니다. 하지만 DPO는 단순성과 효율성 덕분에 작년부터 인기를 얻기 시작했습니다. RLHF-PPO와 달리 DPO는 별도의 보상 모델(reward model)을 필요로 하지 않습니다. 대신, 분류(classification)와 유사한 목표(objective)를 사용하여 LLM을 직접 업데이트합니다. PPO와의 포괄적인 비교는 부족하지만, 많은 LLM이 이제 DPO를 활용합니다.
아래는 제가 올해 초 개발하고 공유한 RLHF 및 DPO에 대한 두 가지 자료입니다.
*   LLM 훈련: RLHF 및 그 대안
*   LLM 정렬을 위한 직접 선호 최적화(DPO) (처음부터)
AI의 책임 있는 개발과 사용을 위한 윤리 원칙 확립은 전 세계적인 과제입니다. 공정성(fairness), 투명성(transparency), 책임성(accountability)은 AI 시스템이 반드시 갖춰야 할 핵심 가치입니다. 예를 들어, AI 모델이 특정 집단에 대한 편향된 결과를 생성하지 않도록 데이터를 신중하게 선별하고 모델을 학습시키는 것이 중요합니다. 또한, AI의 의사 결정 과정을 인간이 이해할 수 있도록 설명 가능성(explainability)을 높이는 연구도 활발히 진행되고 있습니다.

#### 4.2 PPO가 일반적으로 DPO보다 우수하다는 연구 결과와 AI 거버넌스 및 국제 협력

Is DPO Superior to PPO for LLM Alignment? A Comprehensive Study는 수많은 실험과 결과를 담은 잘 작성된 논문입니다. 주요 결론은 PPO가 DPO보다 우수한 경향이 있으며, DPO는 분포 외 데이터(out-of-distribution data)를 다룰 때 성능이 떨어진다는 것입니다. 여기서 분포 외 데이터는 언어 모델이 이전에 DPO에 사용된 선호 데이터(preference data)와 다른 명령어 데이터(instruction data, 지도 미세 조정(supervised finetuning)을 통해)로 훈련되었다는 것을 의미합니다. 예를 들어, 모델은 일반 Alpaca 데이터셋으로 훈련된 후, 다른 선호도 레이블이 지정된 데이터셋에서 DPO 미세 조정을 거칠 수 있습니다. (하지만 이러한 분포 외 데이터에서 DPO를 개선하는 한 가지 방법은 먼저 선호 데이터셋을 사용하여 지도 명령어 미세 조정 단계를 수행한 다음 DPO 미세 조정을 수행하는 것입니다.) 주요 결과는 아래 그림에 요약되어 있습니다.

Is DPO Superior to PPO for LLM Alignment? A Comprehensive Study (https://arxiv.org/abs/2404.10719) 논문에서 발췌한 주석이 달린 표.

국가 및 국제 기구들은 AI 거버넌스(governance) 프레임워크를 구축하기 위해 노력하고 있습니다. 이는 AI 기술의 오용을 방지하고, 예측 불가능한 위험에 대비하며, 잠재적 이점을 극대화하기 위함입니다. 데이터 프라이버시, 알고리즘 편향, 자율 무기 시스템 등 복잡한 문제에 대한 국제적인 협력과 표준화는 필수적입니다. AI는 국경을 초월하는 기술이므로, 전 지구적인 차원에서의 논의와 합의가 이루어져야 합니다. 이러한 노력은 AI가 인류의 번영에 기여하는 방향으로 발전하도록 이끄는 중요한 기반이 될 것입니다.

#### 4.3 오늘날 PPO와 DPO의 활용

PPO는 결과 LLM의 원시 모델링 성능(raw modeling performance) 면에서 약간의 우위를 가질 수 있습니다. 하지만 DPO는 구현하기 훨씬 쉽고 계산적으로 적용하기 더 효율적입니다(결국 별도의 보상 모델을 훈련하고 사용할 필요가 없기 때문입니다). 따라서 제가 아는 한, DPO는 RLHF-PPO보다 실제에서 훨씬 더 널리 사용됩니다. 한 가지 흥미로운 예는 Meta AI의 Llama 모델입니다. Llama 2는 RLHF-PPO로 훈련되었지만, 더 새로운 Llama 3 모델은 DPO를 사용했습니다. 흥미롭게도, 요즘에는 최근 모델들이 PPO와 DPO를 모두 사용하기도 합니다. 최근 사례로는 Apple의 Foundation Models와 Allen AI의 Tulu 3가 있습니다.

### 5. 5월: LoRA는 덜 학습하고 덜 잊어버린다: AI 모델의 모듈화와 유연한 구성

올해 또 다른 AI 모델 최적화 방식이 특히 흥미로웠습니다 (이 12개 논문 선정에서 마지막 LoRA 논문입니다, 약속합니다!). 이는 단일 모델의 성능 향상을 넘어, AI 시스템을 더욱 유연하고 효율적으로 구축하는 방법에 대한 통찰을 제공했습니다. 획기적이라고는 할 수 없지만, LoRA를 사용하거나 사용하지 않고 LLM을 미세 조정하는 것에 대한 몇 가지 상식을 공식화하고, AI 모델의 학습 효율성과 지식 보존에 대한 몇 가지 상식을 공식화한다는 점에서 정말 마음에 듭니다: Biderman 외 연구진의 LoRA Learns Less and Forgets Less (2024년 5월) 연구 결과입니다. 이 연구는 대규모 언어 모델(LLM)에서 저랭크 적응(LoRA)과 전체 미세 조정(full finetuning)을 비교하는 실증 연구(empirical study)로, 두 가지 도메인(프로그래밍 및 수학)과 두 가지 작업(명령어 미세 조정 및 계속된 사전 훈련)에 초점을 맞춥니다. 이는 궁극적으로 AI 시스템의 유지보수와 확장을 용이하게 합니다. 계속하기 전에 LoRA에 대한 복습이 필요하시면 위의 2월 섹션을 확인해 보세요.

#### 5.1 LoRA는 덜 학습하고 모듈형 AI 아키텍처의 장점

LoRA Learns Less and Forgets Less 연구는 LoRA가 전체 미세 조정보다 현저히 적게 학습한다는 것을 보여줍니다. 특히 새로운 지식을 습득해야 하는 코딩과 같은 작업에서 더욱 그렇습니다. 명령어 미세 조정만 수행될 때는 그 차이가 더 작습니다. 이는 새로운 데이터로 사전 훈련(새로운 지식 학습)하는 것이 사전 훈련된 모델을 명령어 추종자(instruction follower)로 전환하는 것보다 전체 미세 조정에서 더 많은 이점을 얻는다는 것을 시사합니다.

전체 미세 조정 대 LoRA. 성능은 164개의 코딩 과제로 구성된 데이터셋인 HumanEval에서 측정됩니다.
LoRA Learns Less and Forgets Less 논문에서 발췌한 주석이 달린 그림, https://arxiv.org/abs/2405.09673.

하지만 몇 가지 더 미묘한 차이가 있습니다. 예를 들어, 수학 작업의 경우 LoRA와 전체 미세 조정 간의 차이가 줄어듭니다. 이는 수학 문제가 LLM에 더 익숙하고, 사전 훈련 중에 유사한 문제를 접했을 가능성이 있기 때문일 수 있습니다. 대조적으로, 코딩은 더 뚜렷한 도메인(domain)을 포함하며 더 많은 새로운 지식을 요구합니다. 따라서 새로운 작업이 모델의 사전 훈련 데이터와 멀리 떨어져 있을수록, 학습 능력(learning capacity) 면에서 전체 미세 조정이 더 유익해집니다.
전통적인 AI 모델은 하나의 거대한 신경망으로 구성되는 경향이 있었지만, 최근에는 모듈형 아키텍처(modular architecture)에 대한 관심이 높아지고 있습니다. 이는 AI 시스템을 독립적인 기능 단위로 분리하여 개발하고 조합하는 방식입니다. 예를 들어, 특정 작업을 위한 전문 모듈을 개발하고, 필요에 따라 이를 메인 모델에 연결하여 사용합니다. 이러한 접근 방식은 모델의 복잡성을 줄이고, 특정 기능에 대한 업데이트나 개선을 더 쉽게 만듭니다. 또한, 새로운 기능의 추가나 기존 기능의 교체가 용이하여 시스템의 유연성을 크게 향상시킵니다.

#### 5.2 LoRA는 덜 잊어버리고 플러그 앤 플레이(Plug-and-Play) AI 구성 요소

이전에 습득한 지식이 얼마나 손실되는지 조사할 때, LoRA는 일관되게 덜 잊어버립니다. 이는 원본 도메인(source domain)과 멀리 떨어진 데이터(예: 코딩)에 적응할 때 특히 분명합니다. 코딩 작업의 경우, 전체 미세 조정은 상당한 망각을 초래하는 반면, LoRA는 더 많은 원래 기능을 보존합니다. 모델의 원래 지식이 새로운 작업에 이미 더 가까웠던 수학에서는 그 차이가 덜 두드러집니다.

프로그래밍 데이터로 훈련한 후 원본 소스 작업에 대한 전체 미세 조정 대 LoRA.
LoRA Learns Less and Forgets Less 논문에서 발췌한 주석이 달린 그림, https://arxiv.org/abs/2405.09673.

모듈형 AI의 핵심은 '플러그 앤 플레이' 방식의 구성 요소 개발입니다. 즉, 표준화된 인터페이스를 통해 다양한 AI 모듈을 손쉽게 연결하고 해제할 수 있도록 하는 것입니다. 이는 마치 소프트웨어 개발에서 라이브러리나 API를 활용하는 것과 유사합니다. 이러한 방식은 개발자들이 처음부터 모든 것을 구축할 필요 없이, 기존의 검증된 모듈을 재사용하여 새로운 AI 애플리케이션을 빠르게 개발할 수 있도록 돕습니다. 예를 들어, 음성 인식 모듈, 이미지 분석 모듈, 자연어 생성 모듈 등을 조합하여 복합적인 AI 서비스를 구축할 수 있습니다. 이는 개발 시간과 비용을 절감하고, AI 기술의 확산에 기여할 것입니다.

#### 5.3 LoRA의 트레이드오프 및 LLM 미세 조정을 위한 미래 접근 방식

전반적으로 트레이드오프(trade-off)가 있습니다. 전체 미세 조정은 더 먼 도메인에서 새로운 지식을 흡수하는 데 더 좋지만, 이전에 학습한 작업을 더 많이 잊어버리게 합니다. LoRA는 더 적은 매개변수를 변경함으로써 새로운 정보를 덜 학습하지만, 원래 기능을 더 많이 유지합니다.
이 연구는 주로 LoRA를 전체 미세 조정과 비교합니다. 실제로는 LoRA가 전체 미세 조정보다 훨씬 더 자원 효율적이기 때문에 인기를 얻었습니다. 많은 경우, 하드웨어 제약(hardware constraints)으로 인해 전체 미세 조정은 단순히 불가능합니다. 더욱이, 전문화된 애플리케이션(specialized applications)만 다루면 되는 경우 LoRA만으로도 충분할 수 있습니다. LoRA 어댑터(adapter)는 기본 LLM과 별도로 저장될 수 있으므로, 새로운 기능을 추가하면서 원래 기능을 쉽게 보존할 수 있습니다. 또한, 지식 업데이트에는 전체 미세 조정을 사용하고, 이후의 특화에는 LoRA를 사용하여 두 가지 방법을 결합하는 것도 가능합니다. 요컨대, 저는 두 가지 방법 모두 앞으로 몇 년 동안 계속해서 매우 중요할 것이라고 생각합니다. 당면한 작업에 적합한 접근 방식을 사용하는 것이 더 중요합니다.

### 6. 6월: 15조 토큰 FineWeb 데이터셋과 합성 데이터(Synthetic Data)의 부상

Penedo 외 연구진의 The FineWeb Datasets: Decanting the Web for the Finest Text Data at Scale (2024년 6월) 논문은 LLM을 위한 15조 토큰(trillion token) 데이터셋(dataset) 생성과 이를 공개하는 과정을 설명하며, 데이터셋 다운로드 링크와 데이터셋 준비 단계를 재현할 수 있는 코드 저장소(code repository, `datatrove/examples/fineweb.py`)를 포함합니다. 이는 방대한 양의 데이터가 AI 모델의 성능을 좌우하는 시대에, 데이터의 양뿐만 아니라 질 또한 중요함을 다시 한번 일깨워줍니다. AI 모델 훈련을 위한 다른 여러 대규모 데이터셋이 있는데, 이 데이터셋은 무엇이 특별할까요? 바로 실제 데이터의 한계를 극복하기 위한 합성 데이터의 역할이 부각되기 시작했다는 점입니다. 요컨대, 고품질의 대규모 데이터셋은 연구자와 실무자가 혁신적인 AI 모델을 훈련하는 것을 이론적으로 가능하게 합니다.

#### 6.1 다른 데이터셋과의 비교 및 합성 데이터의 필요성과 장점

LLM 사전 훈련을 위한 다른 여러 대규모 데이터셋이 있는데, FineWeb 데이터셋은 무엇이 특별할까요? 다른 데이터셋은 비교적 작습니다: RefinedWeb (5000억 토큰), C4 (1720억 토큰), Dolma 1.6의 Common Crawl 기반 부분 (3조 토큰) 및 1.7 (1.2조 토큰), The Pile (3400억 토큰), SlimPajama (6270억 토큰), RedPajama의 중복 제거된 변형 (20조 토큰), Matrix의 영어 CommonCrawl 섹션 (1.3조 토큰), 영어 CC-100 (700억 토큰), Colossal-OSCAR (8500억 토큰). 예를 들어, 약 3600억 토큰은 작은 LLM(예: Chinchilla 스케일링 법칙(scaling laws)에 따르면 17억 개)에만 적합합니다. 반면에 FineWeb 데이터셋의 15조 토큰은 Chinchilla 스케일링 법칙에 따라 최대 5000억 매개변수 모델에 최적일 것입니다. (RedPajama는 20조 토큰을 포함하지만, 연구자들은 RedPajama로 훈련된 모델이 적용된 필터링 규칙(filtering rules)의 차이로 인해 FineWeb보다 품질이 떨어진다는 것을 발견했습니다.)

수년간 LLM 사전 훈련에 사용된 데이터셋 크기 그림.
이는 단순히 일반적인 참고 자료이며 FineWeb 논문이나 Chinchilla 스케일링 법칙 논문과 직접적인 관련이 없다는 점에 유의하십시오.

요컨대, FineWeb 데이터셋(영어 전용)은 연구자와 실무자가 대규모 LLM을 훈련하는 것을 이론적으로 가능하게 합니다. (참고: 80억, 700억, 4050억 크기의 Llama 3 모델도 15조 토큰으로 훈련되었지만, Meta AI의 훈련 데이터셋은 공개적으로 이용할 수 없습니다.)
합성 데이터(synthetic data)는 실제 데이터를 기반으로 생성되지만, 실제 세계의 개별 항목과 직접적인 연관이 없는 인공적으로 생성된 데이터입니다. 이는 여러 가지 이유로 AI 훈련에서 점점 더 중요해지고 있습니다. 첫째, 개인 정보 보호(privacy) 문제로 인해 실제 데이터를 사용하기 어려운 경우, 합성 데이터는 효과적인 대안이 됩니다. 둘째, 특정 시나리오나 희귀한 사건에 대한 실제 데이터가 부족할 때, 합성 데이터는 모델의 견고성을 높이는 데 기여할 수 있습니다. 셋째, 데이터 편향(bias)을 줄이고자 할 때, 합성 데이터를 통해 균형 잡힌 데이터셋을 구성할 수 있습니다. 예를 들어, 자율 주행 차량 훈련 시 발생하기 어려운 극단적인 상황을 합성 데이터로 만들어 모델의 안전성을 높일 수 있습니다.

#### 6.2 원칙적인 데이터셋 개발 및 합성 데이터 생성 기술과 과제

또한, 이 논문은 CommonCrawl 웹 코퍼스(web corpus)에서 시작하여 FineWeb 데이터셋에 도달하기 위해 필터링 규칙이 어떻게 개발되고 적용되었는지에 대한 원칙적인 절제 연구(ablation studies)와 통찰력을 담고 있습니다. 요컨대, 그들이 시도한 각 필터링 규칙에 대해, 원본 데이터와 필터링된 데이터에서 3600억 토큰의 무작위 샘플(random sample)을 추출한 다음, 작은 17억 1천만 매개변수 Llama 유사 모델을 훈련하여 HellaSwag, ARC, MMLU 등과 같은 표준 벤치마크에서 모델의 성능을 기반으로 필터링 규칙이 유익한지 여부를 확인했습니다.
합성 데이터 생성은 주로 생성적 적대 신경망(Generative Adversarial Networks, GANs)이나 변분 자동 인코더(Variational Autoencoders, VAEs), 그리고 최근에는 대규모 언어 모델(LLM)과 같은 생성형 AI 모델을 활용합니다. 이 기술들은 실제 데이터의 통계적 특성을 학습하여 유사한 특성을 가진 새로운 데이터를 생성합니다. 하지만 합성 데이터에도 과제가 있습니다. 실제 데이터의 복잡성과 미묘한 특성을 완벽하게 재현하기 어렵다는 점, 그리고 생성된 데이터가 원본 데이터의 편향을 그대로 답습할 수 있다는 점 등이 그것입니다. 따라서 합성 데이터의 품질을 검증하고, 실제 데이터와 함께 효과적으로 활용하는 방법에 대한 연구가 지속적으로 이루어지고 있습니다. 이는 AI 훈련 데이터셋의 미래를 형성하는 중요한 요소가 될 것입니다.

#### 6.3 오늘날 FineWeb의 관련성

전반적으로, 수십억 매개변수 LLM을 사전 훈련하는 것이 대부분의 연구실과 기업의 역량을 넘어설 수 있지만, 이 데이터셋은 LLM 연구 및 개발을 민주화하는 데 상당한 진전입니다. 요약하자면, 이 논문은 칭찬할 만한 노력이며 LLM 사전 훈련 발전을 위한 귀중한 공공 자원을 소개합니다.

### 7월부터 12월까지

이번 연구 요약 및 기술 동향 분석이 유용했기를 바랍니다! 2024년 상반기 동안 우리는 개방형 AI의 확산, 모델 경량화의 중요성, AI의 지속적인 학습 능력, AI 윤리와 사회적 책임, 그리고 합성 데이터의 새로운 역할까지 다양한 측면에서 AI의 진화를 목격했습니다. 이러한 변화들은 AI가 단순한 도구를 넘어 우리 사회의 근본적인 부분으로 자리 잡고 있음을 보여줍니다. 올해의 리뷰 기사를 두 부분으로 나누기로 결정했습니다. 두 번째 부분(7월부터 12월까지)은 스케일링 법칙, O1 재현, LLM 훈련에서 합성 데이터(synthetic data)의 역할, AI의 사회적 영향, 규제 동향, 그리고 미래 기술의 역할에 대한 더 심층적인 논의를 다루기 때문에 (개인적으로) 사실 훨씬 더 흥미롭습니다. 또한, 2025년에 대한 제 생각과 앞으로 예상되는 AI의 발전 방향을 공유할 것입니다. 계속 지켜봐 주세요!
이 글은 AI에 대한 저의 깊은 열정 프로젝트입니다. 인공지능이 가져올 변화를 함께 이해하고 준비하는 데 도움이 되고자 합니다. 저를 지원하고 싶으신 분들은 제 저서 Build a Large Language Model (From Scratch) 를 구매해 주시면 감사하겠습니다. (이 책은 LLM이 어떻게 작동하는지 다른 곳에서는 찾아볼 수 없는 수준의 세부 사항으로 설명하기 때문에, 이 책에서 많은 것을 얻으실 것이라고 확신합니다.)

Build a Large Language Model (From Scratch) 지금 Amazon에서 구매 가능

책을 읽으시고 잠시 시간을 내주실 수 있다면, 짧은 리뷰를 남겨주시면 정말 감사하겠습니다. AI의 미래는 기술적 진보뿐만 아니라 사회적 합의와 윤리적 고려가 함께 이루어질 때 더욱 밝을 것입니다. 그것은 저희 필자들에게 큰 도움이 됩니다! 여러분의 관심과 지원은 언제나 큰 의미가 있습니다! 감사합니다!

구독하기