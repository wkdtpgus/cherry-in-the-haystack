새해를 맞아, 2024년 인공지능 연구 분야의 주요 성과를 정리한 글의 첫 번째 버전을 마침내 완성할 수 있었습니다. 이 글은 전문가 혼합 모델(Mixture-of-Experts models)부터 언어 모델의 정교함을 높이는 새로운 확장 규칙(scaling laws)에 이르기까지 다양한 주제를 아우릅니다. 2024년 한 해 동안 쏟아져 나온 모든 중요한 연구 결과들을 상세히 다루려면 아마도 한 권의 책을 써야 할 것입니다. 이처럼 급변하는 영역에서 전례 없이 풍성한 한 해였다고 평가할 수 있습니다. 내용을 합리적으로 압축하기 위해, 올해는 대규모 언어 모델(LLM) 연구에만 초점을 맞추기로 결정했습니다. 그럼에도 불구하고, 이렇게 다채로운 한 해에서 어떤 논문들을 선별해야 할까요? 제가 떠올릴 수 있는 가장 단순한 접근법은 2024년 1월부터 12월까지 매달 한 편의 논문을 소개하는 것이었습니다. 따라서 이 글에서는 제가 개인적으로 흥미롭거나 파급력이 크다고 판단한, 혹은 둘 다에 해당하는 연구 결과들을 공유하고자 합니다. 하지만 본 문서는 2024년 상반기(1월부터 6월까지)에 중점을 둔 **1부**라는 점을 기억해 주십시오. 7월부터 12월까지를 다루는 이 시리즈의 2부는 1월 말에 공개될 예정입니다. 논문 선정 기준은 올해 저에게 깊은 인상을 남겼던 지점들을 바탕으로 한 주관적인 선택임을 밝힙니다. 또한, 다양성을 추구하여 단순히 LLM 출시 소식만을 나열하지 않았습니다. 더 폭넓은 AI 연구 논문 목록을 찾으신다면, 제가 이전에 작성한 글(LLM Research Papers: The 2024 List)을 참고해 보시기 바랍니다. 제 이전 글을 읽으셨던 분들께, 제가 이미 조금씩 나아지고 있으며 꾸준히 회복 중이라는 기쁜 소식을 전합니다! 아울러 모든 따뜻한 격려와 지원에 진심으로 감사드립니다. 그것은 저에게 정말 큰 힘이 되었고 어려운 시기를 이겨내는 데 도움을 주었습니다! 새해 복 많이 받으시고 즐거운 독서가 되시기를 바랍니다!

### 1. 1월: Mixtral의 전문가 혼합(Mixture of Experts, MoE) 구조

2024년 1월 초, Mistral AI 연구진은 Mixtral 8x7B, 즉 희소 전문가 혼합(Sparse Mixture of Experts, SMoE) 모델을 상세히 설명하는 Mixtral of Experts 논문(2024년 1월 8일)을 세상에 내놓았습니다. Mixtral 8x7B는 뛰어난 성능을 자랑하며 공개적으로 접근 가능한 가중치(open-weight)를 가진 최초의 MoE 대규모 언어 모델 중 하나였기에, 당시 이 연구 논문과 모델은 모두 엄청난 영향력을 발휘했습니다. 이 모델은 여러 평가 기준에서 Llama 2 70B와 GPT-3.5를 능가하는 역량을 보여주었습니다.

#### 1.1 MoE 모델의 작동 원리

MoE, 즉 전문가 혼합(Mixture of Experts) 방식은 GPT와 유사한 디코더(decoder) 아키텍처(architecture) 내부에 여러 개의 작은 "전문가(expert)" 서브네트워크(subnetwork)를 결합한 앙상블 학습 모델입니다. 각 서브네트워크는 서로 다른 종류의 작업을 처리하거나, 보다 구체적으로는 특정 토큰(token)을 담당하도록 설계되어 있습니다. 이 접근 방식의 핵심 사상은 하나의 거대한 신경망 대신 여러 개의 소규모 서브네트워크를 활용하여, 계산 자원(computational resources)을 더욱 효과적으로 배분하는 데 있습니다. 특히 Mixtral 8x7B의 경우, 아래 그림에서 볼 수 있듯이, 트랜스포머(transformer) 아키텍처의 각 피드포워드 모듈(feed-forward module)을 8개의 전문가 레이어(expert layer)로 대체하는 방식을 채택했습니다.

Attention Is All You Need 논문에서 발췌한 주석이 달린 트랜스포머 아키텍처, https://arxiv.org/abs/1706.03762

"희소 전문가 혼합(Sparse Mixture of Experts)"이라는 용어에서 "희소(Sparse)"라는 단어는 특정 시점에 전문가 레이어 중 일부(Mixtral 8x7B에서는 일반적으로 8개 중 1개 또는 2개)만이 토큰 처리에 적극적으로 관여한다는 사실을 의미합니다. 위 도표에서 나타나듯이, 이 서브네트워크들은 대규모 언어 모델의 피드포워드 모듈 역할을 대신합니다. 피드포워드 모듈은 본질적으로 다층 퍼셉트론(multilayer perceptron)으로 구성됩니다. PyTorch와 유사한 의사 코드(pseudocode)로 표현하면 다음과 같습니다.

```python
class FeedForward(torch.nn.Module):
    def __init__(self, embed_dim, coef):
        super().__init__()
        self.layers = nn.Sequential(
            torch.nn.Linear(embed_dim, coef*embed_dim),
            torch.nn.ReLU(),
            torch.nn.Linear(coef*n_embed, embed_dim),
            torch.nn.Dropout(dropout)
        )

    def forward(self, x):
        return self.layers(x)
```

또한, 각 토큰 임베딩(token embedding)을 8개의 전문가 피드포워드 모듈 중 일부로 유도하는 라우터 모듈(Router module, 게이팅 네트워크(gating network)라고도 함)이 존재하며, 이 모듈을 통해 한 번에 특정 전문가들만 활성화됩니다. 이 글에서 다룰 논문이 11개 더 남아 있으므로, Mixtral 모델에 대한 설명은 핵심만 간략하게 전달하고자 합니다. 하지만 더 자세한 내용은 제 이전 글인 Model Merging, Mixtures of Experts, and Towards Smaller LLMs 에서 확인하실 수 있습니다.

#### 1.2 오늘날 MoE 모델의 중요성

연초에는 공개 가중치(open-weight) 기반의 MoE 모델들이 지금보다 훨씬 더 대중화되고 널리 활용될 것이라는 예측이 지배적이었습니다. 물론 그 중요성이 퇴색된 것은 아니지만, Llama 3, Qwen 2.5, Gemma 2와 같은 최첨단 모델(state-of-the-art models) 중 상당수는 여전히 MoE 방식보다는 전통적인 밀집(dense) LLM 구조를 채택하고 있습니다. 그러나 GPT-4, Gemini, Claude와 같은 독점적인 아키텍처(proprietary architectures)가 어떤 기반 위에서 작동하는지는 알려져 있지 않으며, 이들도 내부적으로 MoE를 활용하고 있을 가능성을 배제할 수 없습니다. 어떤 경우든, MoE 아키텍처는 여전히 중요합니다. 특히 각 입력에 대해 모델의 전체 매개변수(parameter) 중 일부만을 활성화함으로써, 대규모 언어 모델의 용량(model capacity)을 저해하지 않으면서도 계산 비용(computation costs)을 절감하며 효율적으로 확장할 수 있는 방안을 제시하기 때문입니다. 이러한 장점은 특히 대규모 모델의 실용적인 배포에 있어 핵심적인 고려 사항이 됩니다.

실제로 이 글을 작성한 이후, 12월에 MoE 아키텍처를 기반으로 놀라운 성능을 보여준 DeepSeek-V3 모델이 갑작스럽게 공개되었습니다. 이로 미루어 볼 때, MoE는 앞으로도 계속해서 매우 중요한 역할을 할 것입니다!

### 2. 2월: 가중치 분해 LoRA(Weight-decomposed LoRA)

만약 공개 가중치(open-weight) 대규모 언어 모델을 미세 조정(finetuning)하는 작업을 해보셨다면, 아마도 매개변수 효율적인 미세 조정 기법인 저랭크 적응(low-rank adaptation, LoRA)을 언젠가 사용해 보셨을 것입니다. LoRA가 처음이시라면, 제가 이전에 작성한 Practical Tips for Finetuning LLMs Using LoRA (Low-Rank Adaptation) 글이 유용할 수 있으며, 제 저서 Build A Large Language Model (From Scratch)의 부록 D에서는 처음부터 구현한 코드(from-scratch code implementation)를 제공합니다. LoRA는 매우 인기가 많고 널리 쓰이는 기법이며, 제가 그 새로운 변형을 직접 구현하고 다루는 과정이 매우 즐거웠기에, 2월의 선택은 Liu 외 연구진의 DoRA: Weight-Decomposed Low-Rank Adaptation (2024년 2월) 논문입니다.

#### 2.1 LoRA의 핵심 요약

DoRA를 소개하기 전에, LoRA에 대한 간략한 복습을 해보겠습니다.

전체 미세 조정(full finetuning)은 대규모 언어 모델의 각 거대한 가중치 행렬(weight matrix) W를, 새로운 가중치 업데이트 행렬(weight update matrix) ΔW를 계산하여 변경하는 방식입니다. LoRA는 이 ΔW를 두 개의 작은 행렬 A와 B의 곱으로 근사합니다. 따라서 W + ΔW 대신 W + A.B 공식을 사용합니다. 이는 계산량과 메모리 사용량(overhead)을 획기적으로 줄여줍니다. 아래 그림은 일반적인 미세 조정(왼쪽)과 LoRA(오른쪽)의 이러한 수식을 나란히 비교하여 보여줍니다.

일반적인 미세 조정(왼쪽)과 LoRA 미세 조정(오른쪽)의 그림.

#### 2.2 LoRA에서 DoRA로의 발전

DoRA: Weight-Decomposed Low-Rank Adaptation (2024년 2월)에서 Liu 외 연구진은 사전 훈련된 가중치 행렬(pretrained weight matrix)을 크기 벡터(magnitude vector) m과 방향 행렬(directional matrix) V라는 두 부분으로 분리(decompose)함으로써 LoRA의 기능을 확장합니다. 이 분해 방식은 모든 벡터가 그 크기(규모)와 방향(향하는 바)으로 표현될 수 있다는 기본 개념에 기반하며, 여기서는 이를 가중치 행렬의 각 열 벡터(column vector)에 적용합니다. 이렇게 m과 V를 분리한 후, DoRA는 방향 행렬 V에만 LoRA 방식의 저랭크 업데이트(low-rank updates)를 적용하고, 크기 벡터 m은 별도로 훈련될 수 있도록 합니다.

DoRA 논문에서 발췌한 주석이 달린 그림 (https://arxiv.org/abs/2402.09353)

이러한 2단계 접근법은 표준 LoRA 방식에 비해 DoRA에 더 큰 유연성을 부여합니다. 기존 LoRA가 크기와 방향을 동시에 조정하는 경향이 있는 반면, DoRA는 크기를 반드시 증가시키지 않으면서도 미묘한 방향성 조정을 가능하게 합니다. 그 결과, DoRA는 더 적은 매개변수만을 사용하고도 LoRA보다 뛰어난 성능을 보이며, 랭크(rank) 선택에 덜 민감하여 전반적인 성능과 견고성(robustness)이 향상됩니다. 다시 한번, 다룰 내용이 10가지 더 있기에 이 부분을 간략하게 다루고 있지만, 더 자세한 내용에 관심이 있으시다면 올해 초 이 방법에 대해 전체 글을 할애하여 설명한 바 있습니다: Improving LoRA: Implementing Weight-Decomposed Low-Rank Adaptation (DoRA) from Scratch.

#### 2.3 LoRA 및 유사 방법론의 미래 전망

DoRA는 기존 LoRA 방식에 대한 작지만 논리적인 개선점입니다. 아직 널리 사용되지는 않았지만, 최소한의 복잡성만을 추가하므로 다음번에 대규모 언어 모델을 미세 조정할 때 고려해 볼 만한 가치가 있습니다. 전반적으로 LoRA와 유사한 기법들은 계속해서 인기를 이어갈 것으로 예상됩니다. 예를 들어, Apple은 최근 Apple Intelligence Foundation Language Models 논문에서 자사의 LLM 온디바이스(on-device) 작업 특화(task specialization)를 위해 LoRA를 활용한다고 언급했습니다. 이는 LoRA 기술의 실용적인 가치를 입증하는 사례입니다. 다양한 환경에서 모델의 효율적인 맞춤화를 가능하게 하는 LoRA의 강점은 앞으로도 계속해서 주목받을 것입니다.

Ahead of AI는 독자 지원 출판물입니다. 새로운 게시물을 받고 제 작업을 지원하려면 무료 또는 유료 구독자가 되는 것을 고려해 보세요.
구독하기

### 3. 3월: LLM 지속적 사전 훈련(Continually Pretraining)을 위한 핵심 조언

제가 아는 한, 명령어 미세 조정(instruction-finetuning)은 대규모 언어 모델(LLM) 사용자들 사이에서 가장 보편적인 미세 조정 방식입니다. 이 방법의 목적은 공개된 LLM이 주어진 지시를 더 잘 따르도록 하거나, 특정 하위 영역 또는 새로운 지시에 대해 모델의 성능을 최적화하는 것입니다. 그러나 모델이 새로운 지식을 습득해야 할 때는 계속된 사전 훈련(continued pretraining, 때로는 지속적 사전 훈련(continually pretraining)이라고도 함)이 올바른 접근법입니다. 이 섹션에서는 Ibrahim 외 연구진의 신선하고 직관적인 논문, Simple and Scalable Strategies to Continually Pre-train Large Language Models (2024년 3월)의 주요 내용을 간략하게 소개하고자 합니다.

#### 3.1 단순한 기법의 놀라운 효과

이 24페이지 분량의 ‘대규모 언어 모델의 지속적 사전 훈련을 위한 간단하고 확장 가능한 전략(Simple and Scalable Strategies to Continually Pre-train Large Language Models)’ 논문은 수많은 실험과 셀 수 없이 많은 도표를 통해, 오늘날의 기준에서 매우 철저한 연구를 보고합니다. 그렇다면 계속된 사전 훈련을 성공적으로 적용하기 위한 핵심적인 조언은 무엇이었을까요?

1.  학습률(learning rate)을 간단히 재가열(re-warming)하고 재감쇠(re-decaying)하는 것입니다. 이는 훈련 초기에 학습률을 높였다가 점차 낮추는 방식을 반복하는 것을 의미합니다.
2.  치명적인 망각(catastrophic forgetting)을 방지하기 위해 원래 사전 훈련 데이터의 작은 부분(예: 5%)을 새로운 데이터셋에 포함하는 것입니다. 0.5% 및 1%와 같은 더 적은 비율도 효과적이었습니다.

1번 항목인 재가열 및 재감쇠에 대해 좀 더 구체적으로 설명하자면, 이는 아래 그림에서 볼 수 있듯이 대규모 언어 모델의 초기 사전 훈련 단계에서 사용된 것과 정확히 동일한 학습률 스케줄(learning rate schedule)을 그대로 적용한다는 의미입니다.

계속된 사전 훈련을 위한 스케줄.

Build a Large Language Model From Scratch 기반 그림, https://github.com/rasbt/LLMs-from-scratch/blob/main/appendix-D/01_main-chapter-code/appendix-D.ipynb

제가 아는 한, 학습률 재가열 및 재감쇠, 그리고 원래 사전 훈련 데이터를 새로운 데이터에 추가하는 것은 어느 정도 상식적인 접근법입니다. 하지만 연구자들이 이 매우 상세한 24페이지 보고서에서 이러한 방법들을 공식적으로 검증하는 데 시간을 할애했다는 점에 정말 감사드립니다. 더 자세한 내용에 관심이 있다면, 제 이전 글인 Tips for LLM Pretraining and Evaluating Reward Models 에서 이 논문을 더 상세히 다루었습니다.

#### 3.2 이러한 단순한 기법들이 앞으로도 유효할까?

저는 이러한 방법들이 미래의 대규모 언어 모델에서도 계속해서 효과를 발휘하지 않을 것이라고 믿을 만한 근거가 없다고 생각합니다. 그러나 최근 몇 달 동안 사전 훈련 파이프라인(pretraining pipelines)이 단기 및 장기 컨텍스트 사전 훈련(short- and long-context pretraining)을 포함한 여러 단계로 구성되어 더욱 정교해졌다는 점을 인지하는 것이 중요합니다. (New LLM Pre-training and Post-training Paradigms 에서 이에 대해 더 자세히 다루었습니다.) 따라서 최적의 결과를 얻기 위해서는 이 논문에서 제시된 방법들이 특정 상황에 맞춰 조정될 필요가 있을 수 있습니다. 즉, 기본 원칙은 유지되더라도, 복잡해지는 훈련 환경에 따라 그 적용 방식은 유연하게 변화해야 할 것입니다.

### 4. 4월: LLM 정렬을 위한 DPO 또는 PPO, 혹은 둘 다?

4월은 선택하기 어려운 달이었습니다. 예를 들어, 콜모고로프-아놀드 네트워크(Kolmogorov-Arnold Networks)는 그 달에 상당한 주목을 받았습니다. 하지만 제가 아는 한, 그 열기는 꽤 빠르게 식었습니다. 이는 아마도 이론적 보장(theoretical guarantees)을 실제 구현하기 어렵고, 경쟁력 있는 결과나 벤치마크(benchmarks)가 부족하며, 다른 아키텍처가 훨씬 더 확장성이 높기 때문일 것입니다. 그래서 대신 4월의 선택은 더 실용적인 논문인 Xu 외 연구진의 Is DPO Superior to PPO for LLM Alignment? A Comprehensive Study (2024년 4월)입니다.

#### 4.1 RLHF-PPO와 DPO: 핵심 개념

논문 자체를 요약하기 전에, 인간 피드백을 통한 강화 학습(Reinforcement Learning with Human Feedback, RLHF)을 통해 대규모 언어 모델을 정렬(aligning)하는 데 널리 사용되는 두 가지 방법인 근접 정책 최적화(Proximal Policy Optimization, PPO)와 직접 선호 최적화(Direct Preference Optimization, DPO)에 대한 개요를 살펴보겠습니다. RLHF는 LLM을 인간의 선호도에 맞춰 조정하여 응답의 질뿐만 아니라 안전성도 향상시키는 필수적인 방법입니다.

일반적인 (간소화된) LLM 훈련 수명 주기.

전통적으로 RLHF-PPO는 InstructGPT 및 ChatGPT와 같은 모델 및 플랫폼을 위한 대규모 언어 모델 훈련에서 중요한 단계였습니다. 하지만 DPO는 작년부터 그 단순성과 효율성 덕분에 인기를 얻기 시작했습니다. RLHF-PPO와 달리 DPO는 별도의 보상 모델(reward model)을 필요로 하지 않습니다. 대신, 분류(classification)와 유사한 목적 함수(objective)를 사용하여 LLM을 직접 업데이트합니다. PPO와의 포괄적인 비교는 부족하지만, 많은 LLM이 이제 DPO를 활용하고 있습니다.

아래는 제가 올해 초 개발하고 공유한 RLHF 및 DPO에 대한 두 가지 자료입니다.

*   LLM 훈련: RLHF 및 그 대안
*   LLM 정렬을 위한 직접 선호 최적화(DPO) (처음부터)

#### 4.2 PPO가 전반적으로 DPO보다 우월하다

‘LLM 정렬에서 DPO가 PPO보다 우수한가? 종합 연구(Is DPO Superior to PPO for LLM Alignment? A Comprehensive Study)’는 수많은 실험과 결과를 담은 잘 작성된 논문입니다. 이 논문의 핵심 결론은 PPO가 DPO보다 더 나은 성능을 보이는 경향이 있으며, DPO는 분포 외 데이터(out-of-distribution data)를 처리할 때 성능이 저하된다는 것입니다. 여기서 분포 외 데이터란 언어 모델이 이전에 DPO에 사용된 선호 데이터(preference data)와는 다른 명령어 데이터(instruction data, 지도 미세 조정(supervised finetuning)을 통해)로 훈련되었다는 상황을 의미합니다. 예를 들어, 모델은 일반적인 Alpaca 데이터셋으로 훈련된 후, 다른 선호도 레이블이 지정된 데이터셋에서 DPO 미세 조정을 거칠 수 있습니다. (하지만 이러한 분포 외 데이터에서 DPO의 성능을 개선하는 한 가지 방법은 먼저 선호 데이터셋을 사용하여 지도 명령어 미세 조정 단계를 수행한 다음 DPO 미세 조정을 진행하는 것입니다.) 주요 결과는 아래 그림에 요약되어 있습니다.

Is DPO Superior to PPO for LLM Alignment? A Comprehensive Study (https://arxiv.org/abs/2404.10719) 논문에서 발췌한 주석이 달린 표.

#### 4.3 오늘날 PPO와 DPO는 어떻게 활용되는가?

PPO는 결과적으로 생성되는 대규모 언어 모델의 순수한 모델링 성능(raw modeling performance) 측면에서 약간의 우위를 가질 수 있습니다. 하지만 DPO는 구현하기가 훨씬 용이하고 계산적으로도 더 효율적입니다 (결국 별도의 보상 모델을 훈련하고 사용할 필요가 없기 때문입니다). 따라서 제가 아는 한, DPO는 RLHF-PPO보다 실제 환경에서 훨씬 더 널리 사용되고 있습니다. 한 가지 흥미로운 예시는 Meta AI의 Llama 모델입니다. Llama 2는 RLHF-PPO로 훈련되었지만, 더 최신 모델인 Llama 3은 DPO를 채택했습니다. 흥미롭게도, 요즘에는 최근 모델들이 PPO와 DPO를 모두 활용하는 경우도 있습니다. 최근 사례로는 Apple의 Foundation Models와 Allen AI의 Tulu 3가 있습니다. 이러한 경향은 각 방법의 장점을 결합하여 최적의 정렬 성능을 추구하려는 시도로 해석될 수 있습니다.

### 5. 5월: LoRA는 덜 학습하고 덜 잊어버린다

올해 또 다른 LoRA 관련 논문이 특히 흥미로웠습니다 (이 12개 논문 선정에서 마지막 LoRA 논문입니다, 약속합니다!). 획기적이라고까지는 할 수 없지만, LoRA를 사용하거나 사용하지 않고 대규모 언어 모델을 미세 조정하는 것에 대한 몇 가지 상식적인 통념을 공식화한다는 점에서 정말 마음에 듭니다: Biderman 외 연구진의 LoRA Learns Less and Forgets Less (2024년 5월)입니다. 이 논문은 대규모 언어 모델(LLM)에서 저랭크 적응(LoRA)과 전체 미세 조정(full finetuning)을 비교하는 실증 연구(empirical study)로, 프로그래밍 및 수학이라는 두 가지 도메인과 명령어 미세 조정 및 계속된 사전 훈련이라는 두 가지 작업을 중심으로 진행되었습니다. 계속하기 전에 LoRA에 대한 복습이 필요하시면 위의 2월 섹션을 확인해 보세요.

#### 5.1 LoRA는 새로운 것을 덜 배운다

‘LoRA는 덜 학습하고 덜 잊어버린다’ 연구는 LoRA가 전체 미세 조정에 비해 현저히 적은 양의 지식을 습득한다는 사실을 입증합니다. 특히 새로운 지식을 흡수해야 하는 코딩과 같은 과업에서 이러한 차이는 더욱 두드러집니다. 명령어 미세 조정만 수행될 때는 그 차이가 상대적으로 작게 나타납니다. 이는 새로운 데이터로 사전 훈련(즉, 새로운 지식 학습)하는 것이, 사전 훈련된 모델을 명령어 수행자(instruction follower)로 전환시키는 것보다 전체 미세 조정에서 더 큰 이점을 얻을 수 있음을 시사합니다.

전체 미세 조정 대 LoRA. 성능은 164개의 코딩 과제로 구성된 데이터셋인 HumanEval에서 측정됩니다.
LoRA Learns Less and Forgets Less 논문에서 발췌한 주석이 달린 그림, https://arxiv.org/abs/2405.09673.

하지만 몇 가지 더 미묘한 차이점들이 존재합니다. 예를 들어, 수학 관련 작업의 경우 LoRA와 전체 미세 조정 간의 성능 격차가 줄어듭니다. 이는 수학 문제들이 대규모 언어 모델에 더 익숙하며, 모델이 사전 훈련 과정에서 유사한 유형의 문제들을 접했을 가능성이 높기 때문일 수 있습니다. 반면, 코딩은 훨씬 더 독자적인 도메인(domain)을 포함하며 더 많은 새로운 지식을 요구합니다. 따라서 새로운 작업이 모델의 사전 훈련 데이터와 거리가 멀수록, 학습 능력(learning capacity) 측면에서 전체 미세 조정이 더 유리합니다.

#### 5.2 LoRA는 기존 지식을 덜 망각한다

이전에 습득한 지식이 얼마나 손실되는지 조사했을 때, LoRA는 일관되게 기존의 지식을 덜 잊어버리는 경향을 보였습니다. 이러한 현상은 원본 도메인(source domain)과 크게 다른 데이터(예: 코딩)에 모델을 적응시킬 때 특히 명확하게 드러납니다. 코딩 작업의 경우, 전체 미세 조정은 상당한 수준의 망각을 초래하는 반면, LoRA는 원래 가지고 있던 기능을 더 많이 보존합니다. 모델의 원래 지식이 새로운 작업과 이미 더 가까웠던 수학 분야에서는 이러한 차이가 덜 두드러졌습니다.

프로그래밍 데이터로 훈련한 후 원본 소스 작업에 대한 전체 미세 조정 대 LoRA.
LoRA Learns Less and Forgets Less 논문에서 발췌한 주석이 달린 그림, https://arxiv.org/abs/2405.09673.

#### 5.3 LoRA의 균형점

전반적으로, LoRA 사용에는 명확한 균형점(trade-off)이 존재합니다. 전체 미세 조정은 원본 지식과 거리가 먼 도메인에서 새로운 정보를 훨씬 더 잘 흡수하지만, 이전에 학습했던 작업들을 더 많이 잊어버리는 경향이 있습니다. 반면 LoRA는 더 적은 매개변수만을 변경함으로써 새로운 정보를 덜 학습하지만, 모델의 원래 기능을 훨씬 더 잘 유지합니다. 이는 마치 광범위한 지식을 깊이 파고드는 것과, 기존 전문성을 보존하면서 필요한 부분만 미세 조정하는 것의 차이와 같습니다.

#### 5.4 LLM 미세 조정을 위한 미래 전략

이 연구는 주로 LoRA를 전체 미세 조정과 비교합니다. 실제 환경에서 LoRA는 전체 미세 조정보다 훨씬 더 자원 효율적이라는 장점 덕분에 인기를 얻었습니다. 많은 경우, 하드웨어 제약(hardware constraints) 때문에 전체 미세 조정은 단순히 불가능합니다. 더욱이, 특정 전문화된 애플리케이션(specialized applications)만을 다루면 되는 상황에서는 LoRA만으로도 충분한 성능을 발휘할 수 있습니다. LoRA 어댑터(adapter)는 기본 대규모 언어 모델과 별도로 저장될 수 있으므로, 새로운 기능을 추가하면서도 원래의 기능을 쉽게 보존할 수 있다는 큰 이점이 있습니다. 또한, 지식 업데이트에는 전체 미세 조정을 사용하고, 이후의 특정 작업에 대한 특화에는 LoRA를 활용하는 등 두 가지 방법을 결합하는 전략도 가능합니다. 요컨대, 저는 두 가지 방법 모두 앞으로 몇 년 동안 계속해서 매우 중요한 역할을 할 것이라고 생각합니다. 핵심은 당면한 과제에 가장 적합한 접근 방식을 선택하는 것입니다.

Ahead of AI는 독자 지원 출판물입니다. 새로운 게시물을 받고 제 작업을 지원하려면 무료 또는 유료 구독자가 되는 것을 고려해 보세요.
구독하기

### 6. 6월: 15조 토큰 FineWeb 데이터셋

Penedo 외 연구진의 The FineWeb Datasets: Decanting the Web for the Finest Text Data at Scale (2024년 6월) 논문은 대규모 언어 모델(LLM) 훈련을 위한 15조 토큰(trillion token) 규모의 데이터셋 생성 과정과 이를 공개하는 과정을 상세히 설명합니다. 이 논문은 데이터셋 다운로드 링크와 함께, 데이터셋 준비 단계를 재현할 수 있는 코드 저장소(`datatrove/examples/fineweb.py`)를 제공하여 연구 커뮤니티에 큰 기여를 합니다.

#### 6.1 다른 데이터셋과의 비교 우위

대규모 언어 모델의 사전 훈련을 위해 여러 다른 대규모 데이터셋들이 존재하는데, FineWeb 데이터셋은 무엇이 특별할까요? 다른 주요 데이터셋들은 상대적으로 규모가 작습니다: RefinedWeb (5천억 토큰), C4 (1천7백20억 토큰), Dolma 1.6의 Common Crawl 기반 부분 (3조 토큰) 및 1.7 (1조2천억 토큰), The Pile (3천4백억 토큰), SlimPajama (6천2백7십억 토큰), RedPajama의 중복 제거된 변형 (20조 토큰), Matrix의 영어 CommonCrawl 섹션 (1조3천억 토큰), 영어 CC-100 (7백억 토큰), Colossal-OSCAR (8천5백억 토큰). 예를 들어, 약 3천6백억 토큰은 작은 LLM(예: Chinchilla 스케일링 법칙(scaling laws)에 따르면 17억 개)에만 적합합니다. 반면에 FineWeb 데이터셋의 15조 토큰은 Chinchilla 스케일링 법칙에 따라 최대 5천억 매개변수 모델에 최적의 훈련 데이터 규모를 제공할 것입니다. (RedPajama는 20조 토큰을 포함하지만, 연구자들은 RedPajama로 훈련된 모델이 적용된 필터링 규칙(filtering rules)의 차이로 인해 FineWeb보다 품질이 떨어진다는 것을 발견했습니다.)

수년간 LLM 사전 훈련에 사용된 데이터셋 크기 그림.
이는 단순히 일반적인 참고 자료이며 FineWeb 논문이나 Chinchilla 스케일링 법칙 논문과 직접적인 관련이 없다는 점에 유의하십시오.

요컨대, FineWeb 데이터셋(영어 전용)은 연구자와 실무자들이 대규모 언어 모델을 훈련하는 것을 이론적으로 가능하게 합니다. (참고: 80억, 700억, 4050억 크기의 Llama 3 모델도 15조 토큰으로 훈련되었지만, Meta AI의 훈련 데이터셋은 공개적으로 이용할 수 없습니다.)

#### 6.2 원칙에 입각한 데이터셋 개발

또한, 이 논문은 CommonCrawl 웹 코퍼스(web corpus)에서 시작하여 FineWeb 데이터셋에 도달하기까지 필터링 규칙이 어떻게 개발되고 적용되었는지에 대한 원칙적인 절제 연구(ablation studies)와 통찰력을 담고 있습니다. 요컨대, 그들이 시도한 각 필터링 규칙에 대해, 원본 데이터와 필터링된 데이터에서 3천6백억 토큰의 무작위 샘플(random sample)을 추출한 다음, 작은 17억 1천만 매개변수 Llama 유사 모델을 훈련하여 HellaSwag, ARC, MMLU 등과 같은 표준 벤치마크에서 모델의 성능을 기반으로 필터링 규칙이 유익한지 여부를 확인했습니다. 이는 데이터셋 구축 과정에 대한 깊이 있는 이해와 엄격한 과학적 접근 방식을 보여줍니다.

#### 6.3 오늘날 FineWeb의 중요성

전반적으로, 수십억 매개변수 규모의 대규모 언어 모델을 사전 훈련하는 작업이 대부분의 연구실과 기업의 역량을 넘어설 수 있지만, 이 데이터셋의 공개는 LLM 연구 및 개발을 민주화하는 데 있어 상당한 진전이라고 할 수 있습니다. 요약하자면, 이 논문은 칭찬할 만한 노력의 결과물이며, LLM 사전 훈련의 발전을 위한 귀중한 공공 자원을 세상에 소개하고 있습니다. 이는 데이터 기반 인공지능 시대에 고품질 대규모 데이터셋의 접근성이 얼마나 중요한지를 여실히 보여주는 사례입니다.

### 7월부터 12월까지

이번 연구 요약이 독자 여러분께 유익했기를 바랍니다! 제가 아직 부상에서 회복 중이며, 어차피 너무 긴 글이 되었을 것이기에, 올해의 리뷰 기사를 두 부분으로 나누기로 결정했습니다. 두 번째 부분(7월부터 12월까지)은 스케일링 법칙, O1 재현, 그리고 대규모 언어 모델 훈련에서 합성 데이터(synthetic data)의 역할에 대한 더 최근 논문들을 다루기 때문에 (개인적으로) 훨씬 더 흥미로운 내용으로 구성될 것입니다. 또한, 2025년에 대한 저의 생각과 앞으로 예상되는 주요 트렌드도 공유할 예정입니다. 계속해서 많은 관심 부탁드립니다!

이 잡지는 개인적인 열정 프로젝트로 운영되고 있습니다. 저를 지원하고 싶으신 분들께서는 제 저서 Build a Large Language Model (From Scratch) 를 구매해 주시면 큰 도움이 될 것입니다. (이 책은 대규모 언어 모델이 어떻게 작동하는지 다른 곳에서는 찾아볼 수 없는 수준의 상세함으로 설명하기 때문에, 독자 여러분께서 이 책을 통해 많은 것을 얻으실 것이라고 확신합니다.)

Build a Large Language Model (From Scratch) 지금 Amazon에서 구매 가능

책을 읽으시고 잠시 시간을 내어 짧은 리뷰를 남겨주실 수 있다면, 저희 작가들에게 정말 큰 격려가 될 것입니다! 여러분의 지원은 저에게 큰 의미이며, 더 좋은 콘텐츠를 만드는 데 큰 힘이 됩니다! 진심으로 감사드립니다!

구독하기