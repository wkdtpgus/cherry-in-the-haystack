지난 몇 년간, 대규모 언어 모델(LLM) 연구 분야에서 강화 학습(RL)은 지대한 영향력을 발휘하는 핵심 영역 중 하나로 자리매김했습니다. 초창기 연구들은 RL을 활용하여 LLM의 출력을 인간의 선호도에 부합하도록 정렬시켰으며, 이러한 초기 LLM-RL 적용은 거의 전적으로 근접 정책 최적화(Proximal Policy Optimization, PPO) [1] 알고리즘에 의존했습니다. 이 선택은 PPO를 LLM 후처리 학습 과정에서 수년 동안 표준 RL 기법으로 확고히 만들었습니다. LLM 연구의 급속한 발전 속도를 고려할 때, 이는 상당히 오랜 기간 동안의 지배력이었습니다! 최근에 이르러서야 LLM 추론 관련 연구자들은 GRPO와 같은 다른 알고리즘들을 탐색하기 시작했습니다. PPO의 중요성에도 불구하고, 이 알고리즘은 최상위 연구 기관 외부에서는 여전히 충분히 이해되지 못하고 있습니다. 이러한 이해 부족은 여러 타당한 이유를 가집니다. PPO는 복잡한 구현 세부 사항이 많아 다루기 어려운 알고리즘일 뿐만 아니라, 높은 컴퓨팅 및 메모리 요구 사항으로 인해 광범위한 자원 없이는 실험하기가 어렵습니다. PPO를 성공적으로 활용하기 위해서는 알고리즘에 대한 깊이 있는 통찰력과 더불어 상당한 도메인 지식 또는 실제 경험이 요구됩니다. 본 글은 RL의 근본적인 개념들부터 출발하여, PPO에 대한 심층적인 이해를 단계적으로 구축해 나갈 것입니다. 이 기반 위에서 PPO 및 그 다양한 구성 요소에 대한 의사 코드(pseudocode)를 포함한 주요 실용적 고려 사항들을 제시할 것입니다. 마지막으로, LLM 영역에서 PPO를 대중화시킨 핵심 연구들을 검토하며 이 모든 지식을 통합할 것입니다.

AI 연구의 최신 정보를 얻기 위해 Deep (Learning) Focus를 사용하는 50,000명의 다른 사람들과 함께하세요. 구독하기

### 강화 학습(RL) 핵심 개념

PPO를 보다 심층적으로 탐구하기 전에, 강화 학습 전반에 대한 이해를 선행해야 합니다. 이 섹션에서는 RL의 기본적인 문제 설정과 핵심 용어들을 다룰 것입니다. 또한, PPO의 근간이 되는 간결한 정책 경사(policy gradient) 표현을 유도할 것입니다.

#### 문제 정의 및 용어

RL 훈련을 수행할 때, 우리는 특정 **환경(environment)** 내에서 **행동(action)**을 취하는 **에이전트(agent)**를 가정합니다. 에이전트의 행동은 **정책(policy)**에 의해 결정됩니다. 정책은 에이전트의 의사결정 체계와 같으며, 대개 매개변수화(parameterized)되어 있습니다. 예를 들어, LLM 훈련 맥락에서는 LLM 자체가 정책의 역할을 수행합니다. 특정 시점 $t$에서 상태 $s_t$가 주어졌을 때, 정책 $\pi_\theta$에 따른 행동 $a_t$의 확률은 $\pi_\theta(a_t | s_t)$로 모델링됩니다. 에이전트가 행동을 취하면, 환경의 **상태(state)**는 환경의 일부인 **전이 함수(transition function)**에 따라 갱신됩니다. 전이 함수는 $P(s_{t+1} | a_t, s_t)$로 표현되지만, LLM 분야에서는 상태가 대개 누적적인 특성을 가지므로 (예: 프롬프트 $x$와 이전 토큰들 $a_1, \dots, a_t$로 구성된 $s_t = \{x, a_1, a_2, \dots, a_t\}$), 전이 함수는 덜 중요하게 다루어집니다. 이와 더불어, 에이전트가 각 상태를 방문할 때마다 환경으로부터 **보상(reward)**을 받게 되는데, 이 보상은 양수, 음수 또는 0일 수 있습니다.

에이전트는 반복적으로 행동을 수행하며, 각 행동($a_t$), 보상($r_t$), 그리고 상태($s_t$)는 **시간 단계(time step)** $t$와 연관됩니다. 이러한 시간 단계들이 연속적으로 연결되면 하나의 **궤적(trajectory)**이 형성됩니다. 이 궤적은 에이전트가 환경에서 수행한 일련의 상호작용을 나타냅니다. 전체 궤적의 확률은 정책 $\pi_\theta(a_t | s_t)$에 의해 주어진 각 행동의 확률과 전이 함수 $P(s_{t+1} | a_t, s_t)$에 의해 주어진 각 상태 전이 확률을 연쇄 법칙(chain rule)을 사용하여 결합함으로써 계산될 수 있습니다.

**RL 목표(objective).** 강화 학습에서 모델을 훈련하는 궁극적인 목표는 전체 궤적에 걸쳐 획득하는 **누적 보상(cumulative reward)**을 극대화하는 것입니다. 이 목표는 몇 가지 변형을 가지는데, 특히 보상을 **할인(discounted)**하거나 **비할인(non-discounted)** 방식으로 계산할 수 있습니다. 할인율(discount factor) $\gamma$를 적용함으로써, 우리는 미래의 보상보다 현재의 보상에 더 높은 가중치를 부여하도록 정책을 유도합니다. 즉, '나중의 이득보다 현재의 이득이 더 중요하다'는 개념을 반영합니다. 우리의 목표는 일반적으로 궤적에 대한 기댓값(expectation)으로 표현되는 **기대 누적 보상(expected cumulative reward)**으로 공식화됩니다.

**상태, 가치 및 이점 함수(State, value, and advantage functions).** RL의 목표와 밀접하게 관련하여, 다음과 같은 핵심 함수들을 정의할 수 있습니다:
*   **가치 함수(Value Function)** $V(s)$: 특정 상태 $s$에서 시작하여 현재 정책 $\pi_\theta$에 따라 행동할 때 기대할 수 있는 누적 보상.
*   **행동-가치 함수(Action-Value Function)** $Q(s, a)$: 상태 $s$에서 행동 $a$를 취한 다음 정책 $\pi_\theta$에 따라 행동할 때 기대할 수 있는 누적 보상.
*   **이점 함수(Advantage Function)** $A(s, a)$: 행동-가치 함수와 가치 함수의 차이, 즉 $A(s, a) = Q(s, a) - V(s)$.

직관적으로, 이점 함수는 특정 상태 $s$에서 특정 행동 $a$를 취하는 것이 해당 상태에서 일반적인 기대 보상에 비해 얼마나 더 나은지 혹은 나쁜지를 알려줍니다. 행동 $a$로부터 얻는 보상이 기대치를 초과하면 이점은 양수가 되고, 그 반대의 경우 음수가 됩니다. 이점 함수는 RL 연구에서 매우 중요한 역할을 하며, 정책의 경사(gradient)를 계산하는 데 필수적으로 활용됩니다.

> "때때로 RL에서는 행동이 절대적으로 얼마나 좋은지 설명할 필요가 없고, 단지 평균적으로 다른 행동보다 얼마나 더 나은지만 알면 됩니다. 즉, 우리는 그 행동의 상대적 이점(relative advantage)을 알고 싶습니다. 우리는 이 개념을 이점 함수로 명확하게 만듭니다."
>
> — Spinning up in Deep RL

#### LLM을 위한 RL 공식화

이제 RL의 기본 개념들을 이해했으니, 이러한 용어들을 LLM 훈련 환경에 매핑해 봅시다:
*   우리의 **정책**은 LLM 자체입니다.
*   **초기 상태**는 주어진 프롬프트(prompt)입니다.
*   LLM의 출력—각 **토큰(token)** 또는 전체 **완성(completion)**—이 곧 **행동(action)**이 됩니다.
*   **상태**는 프롬프트와 LLM 출력의 결합으로 구성됩니다.
*   LLM의 전체 완성은 하나의 **궤적(trajectory)**을 이룹니다.
*   **보상(reward)**은 검증자(verifier) 또는 보상 모델(reward model)로부터 주어집니다.

이 설정에서는 전이 함수가 본질적으로 결정론적(deterministic)이므로 별도의 전이 함수를 명시적으로 고려하지 않습니다. LLM은 **다음 토큰 예측(next token prediction)** 방식을 통해 출력을 생성하며, 이는 출력 완성의 각 토큰을 순차적으로 생성하는 자기회귀(autoregressive) 과정입니다. 이 과정은 RL 설정에 자연스럽게 매핑됩니다—각 토큰을 개별적인 행동으로 모델링할 수 있습니다. 이 설정은 **마르코프 결정 과정(Markov Decision Process, MDP) 공식화**라고 불리며, PPO와 같은 알고리즘이 의존하는 핵심 프레임워크입니다.

LLM을 위한 MDP 모델링에서, 초기 상태는 프롬프트이며 정책은 개별 토큰을 예측함으로써 행동합니다. LLM은 토큰에 대한 확률 분포를 출력하는 (확률적) 정책을 형성합니다. 생성 과정 중 이 분포에서 토큰을 선택하여 행동이 취해지며, 각 토큰 자체가 하나의 행동입니다. 토큰이 예측된 후 현재 상태에 추가되고, LLM은 이를 사용하여 다음 토큰을 예측합니다—이는 자기회귀 다음 토큰 예측의 핵심입니다. 최종적으로, LLM은 정지 토큰(stop token, 예: `<|end_of_text|>`)을 예측하여 생성 과정을 완료하고, 완전한 궤적을 산출합니다.

#### 정책 경사(Policy Gradient) 기본 사항

RL 훈련 과정에서 우리의 목표는 누적 (할인될 수도 있는) 보상을 최대화하는 것입니다. 이를 달성하기 위해 우리는 **경사 상승(gradient ascent)** 기법을 활용할 수 있습니다. LLM 맥락에 적용하면, RL 훈련은 프롬프트 배치를 샘플링하고, LLM(정책)으로 완성을 생성하며, 이 완성에 대한 보상을 계산하고, 이 보상을 사용하여 정책 업데이트(policy update)를 도출하는 단계를 따릅니다. 이 마지막 정책 업데이트 단계에서 경사 상승이 이루어집니다.

더욱 구체적으로, 우리는 완성(completion)과 보상(reward)을 활용하여 정책 매개변수에 대한 RL 훈련 목표의 경사(gradient)를 추정합니다—이를 "**정책 경사(policy gradient)**"라고 합니다. 이 경사를 계산할 수 있다면, 우리는 경사 상승을 통해 정책을 훈련시킬 수 있습니다. 그렇다면 이 경사는 어떻게 계산될까요?

> "강화 학습의 목표는 에이전트가 최적의 보상을 얻기 위한 최적의 행동 전략을 찾는 것입니다. 정책 경사 방법은 정책을 직접 모델링하고 최적화하는 것을 목표로 합니다."
>
> — Lilian Weng

**정책 경사(Policy gradients).** LLM 훈련에 사용되는 거의 모든 RL 최적화기(optimizer) (예: PPO [1], GRPO, REINFORCE)는 **정책 경사 알고리즘(policy gradient algorithms)** 범주에 속합니다. 이들은 정책 경사를 추정하고, 이 추정치를 사용하여 경사 상승을 수행하는 방식으로 작동합니다. 각 알고리즘은 정책 경사를 추정하는 데 있어 상이한 접근법을 사용하지만, 그들의 기본적인 아이디어는 상당히 유사합니다—정확히 사용되는 기술에 따라 세부 사항을 조절할 뿐입니다.

바닐라 정책 경사(Vanilla Policy Gradient, VPG)는 많은 온라인 자료에서 상세히 다루어져 왔습니다. 그러나 우리는 완전성을 위해 여기에서 정책 경사의 몇 가지 기본 형태를 다시 유도할 것입니다. RL에서 우리의 목표는 누적 보상을 최대화하는 것이며, 이 목표의 경사를 정책 $\theta$의 매개변수에 대해 계산하면 다음을 얻을 수 있습니다:

$$
\nabla_\theta J(\theta) = E_{\tau \sim \pi_\theta} \left[ \sum_{t=0}^T \nabla_\theta \log \pi_\theta(a_t | s_t) R(\tau) \right]
$$

이 유도는 RL 훈련 목표(누적 보상)의 경사에서 시작하여 정책 경사에 대한 기본 표현으로 귀결됩니다. 핵심 단계는 로그 도함수 트릭(log-derivative trick)의 활용과 궤적 확률 정의를 사용하는 것입니다.

**기본 정책 경사 구현.** 우리가 지금까지 유도한 기본 정책 경사 표현은 이론적인 형태이며 기댓값(expectation)을 포함합니다. 실제 계산에서는 이를 표본 평균(sample mean)으로 근사해야 합니다. 즉, 고정된 수의 궤적(LLM의 경우 프롬프트와 완성)을 샘플링하고, 각 궤적에 대한 정책 경사 표현의 평균을 취합니다.

기본 정책 경사 계산 과정을 보다 구체화하기 위해, PyTorch 의사 코드(pseudocode)로 된 단계별 구현이 아래에 제공됩니다.

```python
# basic policy gradient implementation
# for each trajectory in the batch
for trajectory in trajectories:
    # compute the policy gradient for this trajectory
    policy_gradient = 0
    for t in range(len(trajectory)):
        # get the log probability of the action at time t
        log_prob_action_t = LLM.log_prob(trajectory.actions[t] | trajectory.states[t])

        # get the cumulative reward for the trajectory
        cumulative_reward = trajectory.rewards.sum()

        # add to the policy gradient
        policy_gradient += log_prob_action_t * cumulative_reward

    # update the policy parameters with the policy gradient
    optimizer.step(policy_gradient)
```

위 구현에서 중요한 점은 정책 경사를 직접 계산하지 않고, 경사가 정책 경사와 동일한 손실 함수(loss function)를 공식화한 다음, PyTorch의 자동 미분(autodiff)을 사용하여 정책 경사를 계산한다는 것입니다—이는 `loss.backward()` 호출 시 발생합니다. 정책 경사를 계산하는 데 사용되는 손실 함수는 다음과 같습니다.

$$
L(\theta) = - E_{\tau \sim \pi_\theta} \left[ \sum_{t=0}^T \log \pi_\theta(a_t | s_t) R(\tau) \right]
$$

이러한 구별은 PPO (및 TRPO!)가 정책 경사에 대한 직접적인 표현보다는 손실 함수를 통해 공식화된다는 점에서 이해하는 것이 중요합니다.

**기본 정책 경사의 문제점.** 기본 정책 경사 표현은 간단하지만, 다음과 같은 몇 가지 중요한 문제점을 안고 있습니다:
*   **높은 분산(High Variance)**: 경사 추정치(gradient estimates)가 높은 분산을 가질 수 있어 훈련을 불안정하게 만듭니다.
*   **불안정한 정책 업데이트(Unstable Policy Updates)**: 정책에 대한 크고 잠재적으로 불안정하게 만드는 업데이트를 방지할 메커니즘이 부족합니다.

이러한 문제들을 해결하기 위해, 대부분의 정책 경사 알고리즘은 정책 경사 추정치의 분산을 줄이고 정책 업데이트에 **신뢰 영역(trust region)**을 강제하는 데 (즉, 단일 업데이트에서 정책이 얼마나 변할 수 있는지 제한하는 것) 중점을 둡니다.

> "이 경사로 한 걸음 나아가면, 각 행동의 로그 확률이 $R(\tau)$에 비례하여 증가합니다. $R(\tau)$는 지금까지 얻은 모든 보상의 합입니다."
>
> — Spinning up in Deep RL

**리워드-투-고(Reward-to-go).** 예를 들어, 우리의 기본 정책 경사(참조를 위해 아래에 복사됨)에서 우리는 궤적의 누적 보상에 기반하여 주어진 행동의 확률을 증가시키고 있음을 알 수 있습니다. 따라서 우리는 행동이 발생하기도 전에 관찰된 보상 때문에 행동의 확률을 증가시킬 수 있습니다!

$$
\nabla_\theta J(\theta) = E_{\tau \sim \pi_\theta} \left[ \sum_{t=0}^T \nabla_\theta \log \pi_\theta(a_t | s_t) R(\tau) \right]
$$

이러한 단순한 관찰은 "**리워드-투-고(reward-to-go)**" 정책 경사의 개발로 이어졌습니다. 이 수정된 정책 경사 표현은 단순히 누적 보상을 행동 후에 관찰된 보상의 합으로 대체합니다. 이 리워드-투-고 공식화는 정책 경사의 편향 없는 추정량(unbiased estimator)이며, 이전의 기본 정책 경사 표현에 비해 증명 가능한 낮은 분산을 가집니다.

**기준선(Baselines).** 분산을 더욱 줄이기 위해, 우리는 정책 경사 표현에 **기준선(baseline)**을 추가할 수 있습니다. 리워드-투-고 정책 경사와 유사하게, 기준선 버전의 정책 경사도 편향이 없고 낮은 분산을 가집니다. 이 기준선은 현재 상태에만 의존해야 합니다. 이 표현은 리워드-투-고 정책 경사와 거의 동일하며, 단순히 리워드-투-고 항에서 추가 기준선을 뺍니다. 한 가지 일반적인 기준선은 가치 함수(value function)이며, 이는 기대치보다 높은 누적 보상을 달성하는 행동을 긍정적으로 강화합니다.

> "바닐라 정책 경사 알고리즘의 일반적인 문제는 경사 업데이트의 높은 분산입니다… 이를 완화하기 위해 기준선이라고 불리는 다양한 기술이 가치 추정치를 정규화하는 데 사용됩니다. 기준선은 여러 가지 방식으로 이를 수행하며, 다운스트림 행동에 대한 상태의 가치로 효과적으로 정규화합니다 (예: Q 값과 가치의 차이인 이점(Advantage)의 경우). 가장 간단한 기준선은 보상 배치(batch)의 평균 또는 이동 평균입니다."
>
> — RLHF 책

**일반 정책 경사.** [3]에서는 정책 경사를 계산하는 옵션이 더 일반적인 정책 경사 표현으로 요약되었습니다. 이 표현은 우리가 지금까지 본 표현들과 거의 동일하며, 유일한 차이점은 우리의 보상 항 $R(\tau)$를 여러 다른 표현과 같게 설정할 수 있는 일반적인 $\Psi_t$ 항으로 변경했다는 것입니다. PPO는 $\Psi_t$를 이점 함수 $A(s_t, a_t)$와 같게 설정하는 데 초점을 맞춥니다. 이 설정은 **바닐라 정책 경사(VPG)**라고 불립니다.

$$
\nabla_\theta J(\theta) = E_{\tau \sim \pi_\theta} \left[ \sum_{t=0}^T \nabla_\theta \log \pi_\theta(a_t | s_t) A(s_t, a_t) \right]
$$

이론적으로, VPG는 가장 낮은 분산의 경사 추정치를 산출하지만, 정책 업데이트에 신뢰 영역을 강제할 메커니즘이 여전히 없습니다. PPO는 이 문제에 대한 해결책으로 만들어졌습니다. PPO는 우리가 본 기본 정책 경사 표현과 유사하지만, 정책 업데이트에 신뢰 영역을 강제하기 위한 메커니즘이 추가되었습니다. 이제 PPO와 그 구현에 관련된 많은 실용적인 세부 사항에 대해 더 자세히 알아보겠습니다.

### 근접 정책 최적화(Proximal Policy Optimization, PPO) 심층 분석

이제 강화 학습의 기본 원리들을 숙지했으므로, 다음으로 근접 정책 최적화(PPO) [1]에 대해 심도 있게 다룰 시간입니다. 이 설명은 이전 섹션에서 도출한 VPG 개념을 바탕으로 하며, PPO의 전신인 신뢰 영역 정책 최적화(Trust Region Policy Optimization, TRPO) [6]부터 시작하여 PPO가 어떻게 발전했는지 살펴봅니다. TRPO는 훈련 안정화에 효과적이지만, 그 복잡성이 상당합니다. PPO는 유사한 장점을 가지면서도 더욱 실용적인 대안으로 개발되었습니다. 이 섹션의 말미에는 PPO에서 이점 함수를 계산하는 가장 보편적인 접근 방식인 일반화된 이점 추정(Generalized Advantage Estimation, GAE) [3]도 다룰 것입니다.

#### TRPO의 등장과 한계 [6]

> "TRPO는 페널티(penalty) 대신 강력한 제약(hard constraint)을 사용합니다. 왜냐하면 다양한 문제—심지어 학습 과정에서 특성이 변하는 단일 문제 내에서도—에서 잘 작동하는 단일 $\beta$ 값을 선택하기 어렵기 때문입니다."
>
> — [1]에서

PPO를 이해하기 위해서는 그 선행 연구인 신뢰 영역 정책 최적화(TRPO) [6]를 살펴보는 것이 필수적입니다. TRPO의 핵심 동기는 데이터 효율적이며 과도한 하이퍼파라미터 튜닝을 요구하지 않는 알고리즘을 개발하는 것이었습니다. 이를 위해 [6]의 저자들은 정책이 단조롭게 개선(monotonically improve)되도록 보장하는 제약된 목표(constrained objective)를 제안했습니다. 이 목표는 정책 업데이트에 신뢰 영역을 강제함으로써, 훈련을 불안정하게 만들 수 있는 크고 파괴적인 정책 업데이트의 위험을 효과적으로 제거합니다.

$$
\max_\theta E_{s, a \sim \pi_{\theta_{old}}} \left[ \frac{\pi_\theta(a|s)}{\pi_{\theta_{old}}(a|s)} A(s, a) \right] \quad \text{s.t.} \quad E_{s \sim \pi_{\theta_{old}}} \left[ D_{KL}(\pi_{\theta_{old}}(\cdot|s) || \pi_\theta(\cdot|s)) \right] \le \delta
$$

**TRPO를 위한 대리 목표(Surrogate objective)** ([1]에서)

**대리 목표.** 위에 제시된 목표는 TRPO에서 **대리 목표(surrogate objective)**라고 불립니다. 이 명칭은 대리 목표가 표준 RL 훈련 목표와는 다르다는 사실에서 유래합니다. RL에서 우리는 누적 보상을 최대화하는 것을 목표로 하지만, VPG 논의에서 보았듯이 이 "진정한" 목표를 직접 최대화하는 것은 훈련 불안정성을 초래할 수 있습니다. TRPO는 진정한 목표 대신 최대화할 대리 목표를 공식화합니다.

TRPO의 위 표현과 VPG 사이에는 몇 가지 중요한 차이점이 있습니다:
*   현재 정책의 행동 확률은 이전 정책(즉, 훈련 전 정책)에서 해당 행동의 확률로 정규화됩니다—이는 **정책 비율(policy ratio)** 또는 중요도 비율(importance ratio)을 형성합니다. 이 공식화에서는 로그 확률 대신 확률을 사용합니다.
*   새로운 정책과 이전 정책 간의 기대 KL 발산(KL divergence)이 임계값 $\delta$보다 작도록 목표에 제약이 가해집니다.

그 외에는 TRPO 손실 함수가 이점 함수와 궤적 내 토큰 수준 확률의 합을 포함하는 등 VPG와 유사한 구조를 공유합니다.

**정책 비율.** TRPO 손실 함수의 핵심은 아래에 정의된 정책 비율(policy ratio)입니다.

$$
r_t(\theta) = \frac{\pi_\theta(a_t|s_t)}{\pi_{\theta_{old}}(a_t|s_t)}
$$

**정책 (또는 중요도) 비율**

정책 비율은 훈련 과정 시작 전의 해당 행동 확률에 비해 현재 정책에서 주어진 행동이 얼마나 더 가능성이 높은지를 나타냅니다. 이 값은 궤적 내 다른 행동들에 중요도를 할당하는 역할을 합니다. 새로운 정책이 이전 정책보다 어떤 행동에 더 높은 확률을 부여하면 이 비율은 1보다 커져 목표에서 해당 행동의 이점의 영향력을 증대시킵니다. 반대로, 새로운 정책이 더 낮은 확률을 할당하면 비율은 1보다 작아져 해당 행동의 영향력을 감소시킵니다. 정책 비율은 새로운 정책이 더 선호하는 행동(특히 이 행동들이 높은 이점을 가질 경우)을 정책 업데이트가 강조하도록 보장하며, 새로운 정책 하에서 덜 선호되는 행동들은 억제합니다. 이를 통해 업데이트가 새로운 정책이 이전 정책과 어떻게 다른지에 따라 적절하게 가중치를 받도록 보장하여 안정적이고 효율적인 정책 개선을 가능하게 합니다.

**대리 목표 해결.** 이 목표는 안정적인 정책 업데이트를 제공하지만, 이를 해결하는 것은 상당히 복잡할 수 있습니다. 목표에 명시적인 제약을 도입함으로써, 우리는 간단한 경사 상승으로 이 목표를 해결할 수 있는 능력을 상실합니다. 대신, 더 복잡한 공액 경사(conjugate gradient) 알고리즘을 통해 이 목표를 해결해야 합니다.

대안적으로, 이 제약을 제거하고 대신 KL 발산을 손실 함수에 페널티(penalty)로 추가할 수 있습니다.

$$
L(\theta) = E_{s, a \sim \pi_{\theta_{old}}} \left[ \frac{\pi_\theta(a|s)}{\pi_{\theta_{old}}(a|s)} A(s, a) - \beta D_{KL}(\pi_{\theta_{old}}(\cdot|s) || \pi_\theta(\cdot|s)) \right]
$$

**TRPO를 위한 페널티 목표**

이 제약 없는 손실은 더 간단하며 다시 기본 경사 상승으로 해결할 수 있습니다.

**TRPO에서 PPO로.** TRPO의 제약을 페널티로 공식화하면 복잡한 최적화 기술을 피하고 기본 경사 상승에 의존할 수 있지만, 최적화 과정에 새로운 하이퍼파라미터 $\beta$가 도입되어 튜닝이 어려워집니다. 이 목표가 잘 작동하려면 $\beta$ 값을 적절하게 설정하는 것이 필수적이며, 많은 도메인에 일반화되는 단일 $\beta$ 값을 찾는 것은 어렵습니다. 결과적으로, 위의 두 목표 모두 문제점을 가지고 있습니다:
*   TRPO 대리 목표는 실제로 해결하기에는 너무 복잡합니다.
*   재구성된 페널티 목표는 $\beta$ 설정에 민감합니다.

우리는 TRPO의 장점—안정성, 데이터 효율성, 신뢰성—을 유지하면서도 그 복잡성을 피하는 알고리즘을 개발하고자 합니다. 이상적으로, 이 알고리즘은 광범위하게 적용 가능하고 기본 경사 상승을 사용하여 해결할 수 있어야 합니다. 이러한 목표들이 TRPO에서 크게 영감을 받은 PPO의 제안으로 이어졌습니다. PPO의 목표는 TRPO 대리 목표에서 영감을 받았지만, 강력한 KL 제약을 클리핑 메커니즘(clipping mechanism)으로 대체하여 더 간단한 방식으로 신뢰 영역을 강제합니다.

#### PPO의 핵심 원리 [1]

> "우리는 RL을 위한 새로운 정책 경사 방법군을 제안합니다. 이 방법군은 환경과의 상호작용을 통해 데이터를 샘플링하고, 확률적 경사 상승(stochastic gradient ascent)을 사용하여 대리 목표 함수를 최적화하는 것을 번갈아 수행합니다."
>
> — [1]에서

VPG는 계산적으로는 간단하지만, 데이터 효율성이 낮고 (즉, 모델이 잘 작동하려면 많은 샘플에 대해 훈련되어야 함) 정책 업데이트에 높은 분산을 가집니다. 이러한 문제들은 TRPO에 의해 대부분 해결되지만, 상당한 복잡성 증가라는 대가를 치릅니다. PPO는 TRPO의 데이터 효율성 및 신뢰성 이점을 가지면서도 여전히 경사 상승으로 해결할 수 있는 알고리즘입니다. 이러한 점에서 PPO는 TRPO에 비해 더 간결한 알고리즘입니다. 그러나 PPO는 여전히 자체적으로 많은 구현 복잡성을 가진 복합적인 알고리즘입니다.

**훈련 과정.** TRPO와 유사하게, PPO는 대리 목표를 최적화하는 데 집중하지만, PPO의 목표는 제약이 없으며 약간 수정되었습니다. PPO는 각 단계에서 단일 정책 업데이트 이상을 수행하며, 대신 다음을 번갈아 수행합니다:
*   정책으로부터 새로운 데이터 또는 궤적을 샘플링합니다.
*   샘플링된 데이터에 대해 여러 에포크(epoch)의 최적화를 수행합니다.

PPO 대리 목표는 다시 현재 정책과 이전 모델 (즉, 훈련이 수행되기 전의 정책) 간의 정책 비율을 기반으로 합니다. PPO 목표를 얻기 위해, 우리는 KL 제약이 없는 TRPO에 의해 최대화되는 대리 목표에서 시작합니다.

$$
L^{UNCLIPPED}(\theta) = E_{s, a \sim \pi_{\theta_{old}}} \left[ r_t(\theta) A(s, a) \right]
$$

**클리핑되지 않은 PPO 목표**

우리는 이 공식화를 "**클리핑되지 않은(unclipped)**" 목표라고 부를 것입니다. 제약이 없기 때문에, 이 목표는 이점(advantage)을 추정하고 정책 비율(policy ratio)을 계산함으로써 정책 경사를 도출하기 위해 쉽게 계산될 수 있습니다. 그러나 이 제약 없는 목표를 최대화하려고 하면, 훈련 과정을 불안정하게 만드는 크고 파괴적인 정책 경사 업데이트로 이어질 수 있습니다.

이 문제를 해결하기 위해, PPO는 대리 목표에 새로운 **클리핑 메커니즘(clipping mechanism)**을 도입하여 신뢰 영역을 유지하는 데 도움을 줍니다.

$$
L^{CLIP}(\theta) = E_{s, a \sim \pi_{\theta_{old}}} \left[ \min(r_t(\theta) A(s, a), \text{clip}(r_t(\theta), 1-\epsilon, 1+\epsilon) A(s, a)) \right]
$$

**PPO 대리 목표**

목표의 주요 항은 변경되지 않았지만, 정책 비율의 클리핑된 버전이 추가된 항이 있습니다—정책 비율은 $[1 - \epsilon, 1 + \epsilon]$ 범위에 속해야 합니다. 클리핑 항은 RL 훈련 과정이 정책 비율을 1의 값에서 멀어지게 하는 것을 억제합니다. PPO 대리 목표는 클리핑된 목표와 클리핑되지 않은 목표 중 최소값을 취합니다. 이러한 방식으로, PPO 목표는 원래의 클리핑되지 않은 목표에 대한 비관적인 (하한) 경계입니다.

이점(advantage)이 양수인지 음수인지에 따라 클리핑의 동작은 약간 다릅니다. 대리 목표에서 최소값을 사용하는 것은 클리핑이 한 방향으로만 적용되도록 합니다. 특히, 정책 비율을 1의 값에서 멀리 이동시킴으로써 대리 목표를 임의로 감소시킬 수 있지만, 클리핑은 정책 비율을 통해 목표를 임의로 증가시키는 것을 방지합니다. 이러한 방식으로 PPO는 큰 정책 비율을 억제하여 훈련 업데이트 후 우리의 정책이 이전 정책에서 너무 많이 벗어나지 않도록 합니다.

> "이 방식에서는, 확률 비율의 변화가 목표를 개선할 때만 무시하고, 목표를 악화시킬 때는 포함합니다."
>
> — [1]에서

PPO의 클리핑 로직을 더 깊이 이해하기 위해, 대리 목표를 최적화할 때 발생할 수 있는 네 가지 가능한 경우를 각각 고려할 수 있습니다:
*   **경우 #1 [ $A > 0$, $r_t(\theta) \le 1 + \epsilon$ ]**: 이점(advantage)이 양수입니다—이는 우리가 강화하고 싶은 행동입니다. 정책 비율이 $1 + \epsilon$보다 작거나 같으므로, 이 행동의 확률을 증가시키기 위해 일반적인 정책 경사 업데이트를 수행합니다.
*   **경우 #2 [ $A > 0$, $r_t(\theta) > 1 + \epsilon$ ]**: 이점은 다시 양수이지만, 정책 비율은 $1 + \epsilon$보다 큽니다. 이는 이 행동이 이전 정책에 비해 새로운 정책에서 이미 더 가능성이 높다는 것을 의미합니다. 목표는 클리핑되고, 정책 비율의 추가 증가에 대한 경사는 0이 됩니다. 이는 정책이 이 행동을 더욱 가능성 있게 만드는 것을 방지합니다.
*   **경우 #3 [ $A < 0$, $r_t(\theta) \ge 1 - \epsilon$ ]**: 이점은 음수입니다—이는 우리가 부정적으로 강화하고 싶은 행동입니다 (즉, 확률을 감소시킵니다). 정책 비율이 $1 - \epsilon$보다 크거나 같으므로, 이 행동의 확률을 감소시키기 위해 일반적인 정책 경사 업데이트를 수행합니다.
*   **경우 #4 [ $A < 0$, $r_t(\theta) < 1 - \epsilon$ ]**: 이점은 다시 음수이지만, 정책 비율은 $1 - \epsilon$보다 작습니다. 이는 이 행동이 이전 정책에 비해 새로운 정책에서 이미 덜 가능성이 높다는 것을 의미합니다. 목표는 클리핑되고, 정책 비율의 추가 감소에 대한 경사는 0이 됩니다. 이는 정책이 이 행동을 더욱 덜 가능성 있게 만드는 것을 방지합니다.

정책 비율은 현재 정책과 이전 정책 사이에서 계산됩니다. PPO에서 새로운 데이터가 샘플링될 때마다 이전 정책은 현재 정책과 일치하도록 업데이트됩니다. LLM의 맥락에서는, 각 데이터 배치(batch)에 대해 여러 번 (예: 2-4회)의 경사 업데이트 [2]를 수행하므로, 이전 모델은 자주 업데이트됩니다. PPO의 클리핑 연산은 따라서 특정 데이터 배치에 대한 신뢰 영역을 유지합니다.

**KL 발산(KL divergence)의 통합과 그 중요성.** PPO로 LLM을 훈련할 때, 우리는 일반적으로 현재 정책과 참조 정책(reference policy)—보통 RL 훈련이 시작되기 전의 어떤 정책(예: SFT 모델)—사이의 KL 발산을 훈련 과정에 통합합니다. 이 추가된 KL 발산 항은 정책이 참조 정책과 너무 달라지는 것을 페널티화하여, 정규화(regularizing) 효과를 가집니다. 우리는 시퀀스 내 각 토큰에 대해 두 LLM이 출력한 토큰 확률 분포를 비교하여 토큰당 KL 발산을 계산합니다.

PPO 훈련에 KL 발산을 추가하는 두 가지 일반적인 방법이 있습니다. 첫째, RL에서 보상에서 KL 발산을 직접 뺄 수 있습니다. 대안적으로, KL 발산을 RL 훈련 목표에 페널티 항으로 추가할 수 있습니다. 두 경우 모두, 우리는 새로운 정책이 참조와 너무 달라지지 않으면서 보상을 최대화하고자 합니다. 이러한 KL 발산 항은 LLM을 위한 RL 훈련에서 거의 보편적으로 사용되며, LLM이 정렬된 행동을 유지하고 '파멸적 망각(catastrophic forgetting)'을 방지하는 데 필수적인 역할을 합니다. 이는 모델이 RL 훈련 중에 새로운 기술을 배우면서도 원래의 언어 모델 능력을 유지하도록 돕습니다.

**비평가(critic)의 역할.** 이점 함수는 상태-행동 가치 함수와 가치 함수의 차이로 정의된다는 것을 기억하십시오. PPO에서 우리는 궤적에 대해 관찰된 실제 보상을 사용하여 상태-행동 가치 함수—주어진 상태에서 특정 행동을 취했을 때의 기대 보상—를 추정합니다. 대조적으로, 가치 함수는 일반적으로 학습된 모델을 사용하여 추정됩니다. 예를 들어, 우리는 정책의 별도 복사본을 만들거나—더 나은 매개변수 효율성을 위해—정책과 가중치를 공유하는 전용 가치 헤드(value head)를 추가하여 가치 함수를 예측할 수 있습니다. 이 학습된 가치 함수는 종종 **가치 모델(value model)** 또는 **비평가(critic)**라고 불립니다. 부분 응답을 입력으로 받아, 비평가는 시퀀스 내의 모든 토큰 위치에 대한 기대 최종 보상을 예측합니다.

LLM 맥락에서, 비평가와 보상 모델은 모두 학습된 모델로서 보상을 예측한다는 점에서 유사합니다. 그러나 비평가는 부분 완성(partial completion)을 입력으로 받아 기대 보상을 예측하는 반면, 보상 모델은 일반적으로 전체 응답이 받은 보상을 예측합니다. 또한, 보상 모델은 RL 훈련 내내 고정되는 반면, 비평가는 지속적으로 업데이트되어 예측이 온-정책(on-policy)으로 유지되도록 합니다—이를 액터-크리틱(actor-critic) 설정이라고 합니다. 이는 비평가가 예측한 보상과 실제 보상 사이의 추가적인 평균 제곱 오차(mean-squared error, MSE) 손실을 대리 손실에 추가함으로써 달성됩니다.

**PPO 구현의 실제.** 이러한 각 아이디어를 더 완전하게 만들기 위해, 우리는 아래에 PyTorch 의사 코드(pseudocode)로 PPO를 구현했습니다. 이 구현에서 우리는 지금까지 논의한 몇 가지 핵심 아이디어를 볼 수 있습니다. 예를 들어:
*   현재 정책과 참조 모델 간의 KL 발산을 계산한 다음, 이 KL 발산을 보상에서 직접 뺍니다.
*   학습된 비평가를 사용하여 이점(advantage)을 계산하고 (정책 자체와 함께 MSE 손실을 통해 이 비평가를 훈련합니다).
*   이전 모델에 대한 정책 비율을 계산합니다.

아래 스크립트는 단일 정책 업데이트를 수행하지만, PPO는 일반적으로 각 데이터 배치에 대해 여러 번 (예: LLM의 경우 2-4회 [2]) 정책 업데이트를 수행합니다. 정책 비율의 "이전" 모델은 배치에 대한 첫 번째 업데이트 전의 모델입니다. 전체 (클리핑된) PPO 손실을 계산합니다. PyTorch는 기본적으로 경사 하강(gradient descent)을 수행하므로 (상승이 아님) 이 손실의 음수 값을 취합니다. 시퀀스 배치에 걸쳐 토큰 수준 PPO 손실을 집계하거나 평균을 냅니다. 배치에서 손실을 집계하는 방법은 여러 가지가 있으며, 사용된 접근 방식은 결과에 상당한 영향을 미칠 수 있습니다 [2].

여기서 볼 수 있는 한 가지 흥미로운 세부 사항은—PPO 손실이 로그 확률이 아닌 토큰 확률을 사용함에도 불구하고—정책 비율을 계산할 때 원시 확률을 사용하는 대신 토큰 로그 확률로 작업하고 이를 지수화(exponentiate)한다는 것입니다. 이것은 일반적으로 사용되는 수치 안정성(numerical stability) 트릭입니다.

```python
import torch
import torch.nn.functional as F

# constants
kl_beta = 0.1
critic_weight = 0.5
ppo_eps = 0.2

# sample prompt completions and rewards
with torch.no_grad():
    completions = LLM.generate(prompts) # (B*G, L)
    rewards = RM(completions) # (B*G, 1)

# create a padding mask from lengths of completions in batch
completion_mask = <... mask out padding tokens ...>

# compute value function / critic output
values = CRITIC(completions) # (B*G, L) - predicted reward per token!

# get policy logprobs for each action
llm_out = LLM(completions)
per_token_logps = F.log_softmax(llm_out, dim=-1) # (B*G, L)

# get reference logprobs for each action
ref_out = REF(completions)
ref_per_token_logps = F.log_softmax(ref_out, dim=-1) # (B*G, L)

# compute KL divergence between policy and reference policy
kl_div = per_token_logps - ref_per_token_logps # (B*G, L)

# directly subtract KL divergence from rewards
# NOTE: KL div is per token, so reward becomes per token and reward
# for all tokens (besides last token) is just kl divergence.
# Reward for last token is sum of outcome reward and KL div.
rewards -= kl_beta * kl_div # (B*G, L)

# compute the advantage - simple approach
advantage = rewards - values.detach() # (B*G, L)

# compute the policy ratio
# NOTE: old_per_token_logps must be persisted during first policy
# update for this batch of data and re-used in each subsequent update
policy_ratio = torch.exp(
    per_token_logps - old_per_token_logps,
) # (B*G, L)

clip_policy_ratio = torch.clamp(
    policy_ratio,
    min=1.0 - ppo_eps,
    max=1.0 + ppo_eps,
)

# compute the ppo loss
ppo_loss = torch.min(
    advantage * policy_ratio,
    advantage * clip_policy_ratio,
) # (B*G, L)
ppo_loss = -ppo_loss # PyTorch performs gradient descent (not ascent)

# combine ppo loss and critic mse loss
critic_loss = ((rewards - values) ** 2) # (B*G, L)
loss = ppo_loss + critic_weight * critic_loss # (B*G, L)

# aggregate the loss across tokens (many options exist here)
loss = ((loss * completion_mask).sum(axis=-1) / completion_mask.sum(axis=-1)).mean()

# perform policy gradient update
optimizer.zero_grad()
loss.backward()
optimizer.step()
```

**일반화된 이점 추정(GAE) [3].** 이점은 주어진 상태에서 평균 행동과 비교하여 특정 행동이 얼마나 더 나은지를 나타냅니다: $A(s_t, a_t) = Q(s_t, a_t) - V(s_t)$. 이 공식화에서 가치 함수는 우리의 비평가에 의해 추정되지만, 이점 함수가 어떻게 계산될 수 있는지에 대해서는 아직 자세히 논의하지 않았습니다. PPO에서 이점 함수는 토큰별 (또는 행동별)로 추정됩니다.

이점을 계산하는 데 사용될 수 있는 두 가지 주요 접근 방식이 있으며, 이 접근 방식들은 대부분의 다른 기술의 기초를 형성합니다.

**(1) 몬테카를로(Monte Carlo, MC).** 이점의 MC 추정치는 전체 궤적에 대해 관찰된 실제 보상에 의존합니다. 즉, 이점은 전체 궤적에 대한 누적 보상 $R(s_t)$과 비평가가 예측한 현재 상태 $V(s_t)$의 가치 함수 간의 차이로 계산됩니다. 지금까지 PPO에 대한 우리의 논의는 이점을 추정하기 위한 MC 접근 방식을 가정했습니다. MC 추정치는 궤적에 대해 관찰된 실제 보상(정확한 정보)에 의존하기 때문에 낮은 편향(bias)을 가지지만, MC 추정치는 또한 높은 분산(variance)을 가집니다. 따라서 정확한 이점 추정치를 얻기 위해서는 많은 샘플을 취하고 충분한 수의 관찰을 해야 합니다—이는 비용이 많이 들 수 있습니다.

**(2) 시간차(Temporal Difference, TD).** TD 잔차(residual)는 비평가의 토큰별 가치 예측을 사용하여 이점의 1단계 추정치를 형성합니다.

$$
\delta_t = r_t + \gamma V(s_{t+1}) - V(s_t)
$$

**시간차(TD) 잔차**

이 TD 잔차는 단일 토큰을 예측하고 해당 행동에 대한 실제 보상을 관찰한 후 기대 보상이 얼마나 변하는지 분석합니다. 현재 상태 $V(s_t)$의 값을 다음의 합에서 뺍니다: 현재 상태 $r_t$에 대한 관찰된 보상과 다음 상태 $V(s_{t+1})$의 (할인된) 값. $V(s_t)$와 유사하게, 이 두 항의 합은 상태 $s_t$에서의 기대 수익을 포착합니다. 그러나 현재 상태에 대한 보상은 비평가에 의해 추정되는 대신 실제 관찰된 보상 $r_t$를 통해 포착됩니다. 따라서 이 항들 간의 차이는 상태 $s_t$에서 관찰된 실제 보상이 기대치보다 얼마나 더 나은지를 포착합니다—이것이 바로 이점(advantage)입니다! 실제 보상 $r_t$를 사용함으로써, 우리는 우리의 이점 추정치에 일부 정확한 정보를 통합합니다—추정치의 항들은 부분적으로 우리의 비평가에서 오고 부분적으로 실제 보상에서 옵니다. 이러한 토큰 수준 보상을 사용하여 이점을 추정하면 정책 경사의 분산이 낮아집니다.

**N-단계 추정량(N-step estimators).** TD 잔차는 단일 단계에 대한 실제 보상과 기대 보상 간의 차이를 분석합니다. 그러나 우리는 이 아이디어를 일반화하여 어떤 수의 단계도 포착할 수 있습니다. N-단계 이점 추정량은 TD 잔차와 유사한 구조를 가지지만, N개의 상태에 대한 실제 보상을 통합하며, 여기서 N은 1보다 클 수 있습니다.

$$
A_t^{(N)} = \sum_{k=0}^{N-1} \gamma^k r_{t+k} + \gamma^N V(s_{t+N}) - V(s_t)
$$

**N-단계 이점 추정량**

단일 단계 TD 잔차와 유사하게, N 값이 낮은 이점 추정량은 낮은 분산을 가지지만 높은 편향을 가집니다. 그러나 N 값을 증가시킬수록, 우리는 이점 추정치에 더 정확한 보상 정보를 통합하여 편향을 낮춥니다 (그리고 결과적으로 분산을 증가시킵니다). 이를 더 나아가, N을 궤적의 총 단계 수와 같게 설정함으로써 MC 추정치를 복구할 수도 있습니다! 이 N 설정은 단순히 누적 보상과 현재 상태 $V(s_t)$의 값 간의 차이를 산출합니다. 따라서 N의 다른 설정은 편향과 분산에서 다른 절충점(tradeoff)을 산출하며, 단일 단계 TD 잔차(높은 편향, 낮은 분산)부터 MC 추정치(높은 분산, 낮은 편향)까지 모든 범위를 포괄합니다.

> "GAE는 편향-분산 절충(bias-variance tradeoff)을 더 잘 균형 잡는 정책 경사 알고리즘을 위한 이점을 계산하는 대체 방법입니다. 전통적인 단일 단계 이점 추정치는 너무 많은 편향을 도입할 수 있는 반면, 완전한 궤적을 사용하는 것은 종종 높은 분산으로 고통받습니다. GAE는 두 가지 아이디어—다단계 예측(multi-step prediction)과 가중 이동 평균(weighted running average) (또는 이들 중 하나만)—를 결합하여 작동합니다."
>
> — [2]에서

PPO와 함께 이점을 추정하는 데 가장 일반적으로 사용되는 접근 방식인 **일반화된 이점 추정(Generalized Advantage Estimation, GAE)**은 N-단계 이점 추정치를 활용합니다. 그러나 GAE는 단일 N 값을 선택하는 대신, 다른 N 값을 가진 N-단계 이점 추정치의 평균을 취함으로써 모든 N 값을 사용합니다. 이는 혼합 매개변수(mixing parameter) $\lambda$를 도입함으로써 수행됩니다.

$$
A_t^{GAE(\gamma, \lambda)} = \sum_{k=0}^{\infty} (\gamma \lambda)^k \delta_{t+k}
$$

**GAE 공식화**

이 공식화에서 $\lambda = 0$으로 설정하면 합계에서 첫 번째 항만 0이 아닌 가중치를 받으므로 단일 단계 TD 잔차가 산출됩니다. 또한, $\lambda = 1$로 설정하면 MC 추정치를 복구합니다. GAE의 이점은 $\lambda \in [0, 1]$ 값이 편향-분산 절충을 제어한다는 것입니다. $\lambda$ 값을 증가시킬수록, 이점 추정치에 더 정확한 보상 정보가 사용되어 편향을 낮춥니다 (그러나 분산은 증가합니다). 유사하게, 우리는 더 낮은 $\lambda$ 값을 사용하여 더 높은 편향을 대가로 분산을 줄일 수 있습니다.

**GAE 구현.** GAE 개념을 더 구체적으로 만들기 위해, AI2의 OpenInstruct 라이브러리에서 발췌한 실제 예시를 살펴보겠습니다. [여기](https://github.com/allenai/OpenInstruct/blob/main/openinstruct/ppo_trainer.py)에서 이용 가능한 전체 PPO 훈련 스크립트는 프로덕션 수준의 훈련 설정에서 PPO의 세부 사항을 학습하기 위한 훌륭한 자료입니다. 이 스크립트의 GAE 구성 요소는 명확성을 위해 몇 가지 추가 주석과 함께 아래에 나와 있습니다. 우리는 시퀀스를 역순으로 반복함으로써 GAE 재귀를 효율적으로 계산할 수 있습니다.

```python
import torch

# store advantages in reverse order while iterating thru sequence
advantages_reversed = []

# iterate backward to compute GAE recursion
lastgaelam = 0
gen_length = responses.shape[1]
for t in reversed(range(gen_length)):
    if t < gen_length - 1:
        # get value model prediction for time t + 1
        nextvalues = values[:, t + 1]
    else:
        # no values predicted beyond end of sequence
        nextvalues = 0.0

    # compute TD residual at time t
    delta = rewards[:, t] + gamma * nextvalues - values[:, t]

    # add to the discounted sum of TD residuals for GAE
    lastgaelam = delta + gamma * lam * lastgaelam

    # store the advantage for step t in our list
    advantages_reversed.append(lastgaelam)

# put the list of advantages in the correct order
advantages = torch.stack(advantages_reversed[::-1], axis=1)
```

### LLM 정렬을 위한 PPO의 응용 및 대안

LLM을 훈련하는 데 일반적으로 사용되는 두 가지 유형의 RL 훈련이 있습니다:
*   **인간 피드백 기반 강화 학습(Reinforcement Learning from Human Feedback, RLHF)**은 인간 선호도 보상 모델(human preference reward model)에서 파생된 보상을 사용하여 RL로 LLM을 훈련합니다.
*   **검증 가능한 보상 기반 강화 학습(Reinforcement Learning with Verifiable Rewards, RLVR)**은 규칙 기반 또는 결정론적 검증자에서 파생된 보상을 사용하여 RL로 LLM을 훈련합니다.

이러한 RL 훈련 기술들은 주로 훈련을 위한 보상을 도출하는 방식에서 차이가 있지만, 알고리즘의 다른 세부 사항들은 대부분 유사합니다. 이들은 모두 일련의 프롬프트에 대해 완성을 생성하고, 이 완성에 대한 보상을 계산하며, 보상을 사용하여 RL 최적화기(예: PPO)로 정책 업데이트—또는 LLM의 매개변수 업데이트—를 도출함으로써 작동합니다.

RLHF는 ChatGPT의 전신인 InstructGPT [8]와 같은 LLM에 의해 탐구된 RL의 원래 형태였습니다. LLM을 위한 RLHF에 대한 초기 연구는 PPO를 기본 RL 최적화기로 사용했으며, 이는 궁극적으로 PPO를 RL로 LLM을 훈련하는 표준 선택으로 만들었습니다. RLVR은 더 최근에 도입되었으며, 이 분야의 대부분의 작업은 PPO 대신 GRPO를 기본 RL 최적화기로 사용합니다.

> "PPO는 RLHF의 표준 방법으로 자리매김했습니다. 그러나 이는 높은 계산 비용과 민감한 하이퍼파라미터 튜닝을 모두 포함합니다."
>
> — [9]에서

**PPO의 실제적 도전과제.** PPO는 RLHF의 기본 RL 최적화기로 빠르게 자리 잡았지만, 높은 컴퓨팅 및 메모리 오버헤드와 많은 저수준 구현 복잡성을 가진 복잡한 액터-크리틱(actor-critic) 알고리즘입니다. PPO의 메모리 오버헤드는 우리가 LLM의 네 가지 복사본을 메모리에 유지하기 때문에 높습니다:
*   정책 (Policy)
*   참조 정책 (Reference Policy)
*   비평가 (Critic)
*   보상 모델 (Reward Model, 보상 모델을 사용하는 경우)

또한, 우리는 정책 자체와 함께 비평가의 매개변수를 업데이트하고 이 모든 모델에 대해 동시에 추론을 실행하므로 높은 컴퓨팅 비용이 발생합니다. LLM의 거대한 크기를 고려할 때, 이는 특히 분산 훈련 환경에서 GPU 메모리 관리와 통신 오버헤드 같은 실질적인 문제로 이어집니다. LoRA(Low-Rank Adaptation)와 같은 효율적인 미세 조정(fine-tuning) 기술을 PPO와 통합하려는 시도는 이러한 메모리 및 컴퓨팅 제약을 완화하는 데 중요한 역할을 합니다.

메모리 및 컴퓨팅 오버헤드 외에도, PPO 훈련 중에 신중하게 고려해야 할 많은 구현 세부 사항이 있습니다:
*   비평가와 보상 모델을 어떻게 초기화해야 할까요?
*   이 모델들에 대해 어떤 훈련 설정을 채택해야 할까요?
*   PPO에서 클리핑을 위해 $\epsilon$ 값을 얼마로 사용해야 할까요?
*   KL 발산을 위한 참조 모델로 어떤 모델을 사용해야 할까요?
*   데이터 배치에 대해 몇 번의 정책 업데이트를 수행해야 할까요?
*   KL 발산을 손실에 페널티로 추가해야 할까요, 아니면 보상 함수에 직접 통합해야 할까요?
*   어떤 스케일링 계수(scaling factor) $\beta$를 사용해야 할까요?
*   비평가의 손실을 주요 PPO 손실에 비해 어떻게 가중치를 부여해야 할까요?
*   GAE를 사용해야 할까요? $\lambda$에 대해 어떤 설정을 사용해야 할까요?

이 각 선택은 RL 훈련 결과에 영향을 미칠 수 있습니다! PPO는 불안정성에 취약한 민감한 알고리즘입니다—잘못된 하이퍼파라미터 설정으로 인해 궁극적으로 성능이 좋지 않은 모델을 훈련하는 데 많은 컴퓨팅 자원과 시간을 소비할 수 있습니다. 이러한 이유로, REINFORCE 및 GRPO와 같은 더 간단한 RL 알고리즘—또는 DPO와 같은 RL-프리 기술—이 PPO의 인기 있는 대안이 되었습니다.

**PPO 대안의 부상.** PPO의 복잡성과 민감성으로 인해, 연구 커뮤니티는 LLM 정렬을 위한 보다 효율적이고 안정적인 대안을 적극적으로 모색해 왔습니다. 이 중 가장 주목할 만한 것은 **직접 선호도 최적화(Direct Preference Optimization, DPO)**입니다. DPO는 명시적인 보상 모델 훈련이나 PPO와 같은 복잡한 RL 알고리즘 없이도 선호도 데이터를 사용하여 LLM을 직접 최적화합니다. 이는 선호도 데이터를 특정 손실 함수로 변환하여 지도 학습 방식으로 정책을 업데이트하는 방식으로 작동하며, PPO에 비해 구현이 훨씬 간단하고 훈련이 안정적이라는 장점이 있습니다. DPO는 특히 대규모 LLM에 대한 RLHF의 계산 및 구현 부담을 크게 줄여주면서도 경쟁력 있는 성능을 달성하여 빠르게 표준 방법 중 하나로 자리 잡았습니다.

DPO의 성공에 이어, **항등 선호도 최적화(Identity Preference Optimization, IPO)**와 **카네만-트버스키 최적화(Kahneman-Tversky Optimization, KTO)**와 같은 다른 RL-프리 방법들도 등장했습니다. IPO는 DPO의 손실 함수를 수정하여 과적합(overfitting)을 줄이고 훈련 안정성을 더욱 향상시키는 것을 목표로 합니다. KTO는 인간의 선호도 판단에 내재된 인지 편향(cognitive biases)을 모델링하여 보상 함수를 보다 현실적으로 반영하려는 시도로, 선호도 데이터의 미묘한 측면을 포착하는 데 중점을 둡니다.

여전히 RL 기반의 대안들도 연구되고 있습니다. 예를 들어, **GRPO(Generalized Advantage Replay Policy Optimization)**는 PPO의 샘플 효율성을 개선하고 오프-정책(off-policy) 학습을 통합하여 훈련 속도를 높이는 것을 목표로 합니다. **ORPO(Odds Ratio Policy Optimization)**는 DPO와 유사하게 선호도 데이터를 직접 활용하지만, PPO의 이점 개념을 확률 비율(odds ratio)로 대체하여 안정성과 효율성을 추구합니다. 이러한 다양한 접근 방식들은 LLM 정렬이라는 단일 목표를 달성하기 위해 계산 효율성, 훈련 안정성, 그리고 성능 사이의 다양한 절충점(tradeoff)을 탐색하고 있음을 보여줍니다.

**데이터셋 및 보상 모델의 발전.** PPO 및 그 대안들의 효과는 궁극적으로 양질의 선호도 데이터와 보상 모델의 정확성에 달려 있습니다. 최근에는 인간 주석(human annotation) 외에도 **합성 데이터(synthetic data)** 생성 및 **AI 피드백(AI Feedback, AIF)**을 활용하여 선호도 데이터를 확장하려는 노력이 활발합니다. GPT-4와 같은 강력한 LLM을 '심사관(judge)'으로 사용하여 모델 출력의 품질을 평가하고 선호도 레이블을 생성하는 방식은 RLHF 파이프라인의 확장성을 크게 향상시켰습니다. 또한, **헌법적 AI(Constitutional AI)**와 같은 접근 방식은 특정 원칙 집합에 따라 AI가 자체적으로 피드백을 생성하고 수정하도록 하여 인간 주석의 필요성을 줄이면서 LLM의 안전성과 유용성을 향상시키는 새로운 패러다임을 제시하고 있습니다. 보상 모델 자체도 단순한 이진 분류를 넘어 다중 레이블 분류, 순위 학습(ranking learning), 또는 복잡한 점수 체계를 통합하는 방향으로 발전하고 있습니다.

**다중 목표 최적화.** 현대 LLM은 단순히 지시를 따르는 것을 넘어 유용성(helpfulness), 무해성(harmlessness), 정직성(honesty), 그리고 특정 스타일이나 어조 유지와 같은 다양한 목표들을 동시에 만족시켜야 합니다. 이러한 다중 목표를 RL 프레임워크에 통합하는 것은 복잡한 도전 과제입니다. 현재 연구는 다중 보상 함수를 결합하거나, 각 목표에 대한 별도의 보상 신호를 사용하여 이를 가중치 합산하는 방식을 탐구하고 있습니다. 예를 들어, 안전성 제약(safety constraints)을 강화하기 위해 특정 유해한 출력에 대해 매우 큰 음의 보상을 부여하거나, 특정 태스크 성능과 안전성 사이의 균형을 맞추는 최적화 기법들이 개발되고 있습니다. 이는 LLM이 단순히 '유능한' 것을 넘어 '책임감 있고 윤리적인' 에이전트가 되도록 유도하는 데 필수적입니다.

**미래 전망.** LLM 정렬을 위한 RL 분야는 빠르게 진화하고 있습니다. PPO는 여전히 중요한 기준점 역할을 하지만, DPO와 같은 RL-프리 방법론의 부상은 효율성과 안정성에 대한 새로운 가능성을 열었습니다. 앞으로는 더욱 정교한 RL 알고리즘, 예를 들어 모델 기반 RL(model-based RL)이나 계층적 RL(hierarchical RL)이 LLM의 장기적인 계획 및 복잡한 추론 능력을 향상시키는 데 활용될 수 있습니다. 또한, RL과 다른 학습 패러다임(예: 자기 지도 학습(self-supervised learning) 또는 인과 추론(causal inference))의 결합은 LLM이 더욱 강력하고 일반화 가능한 능력을 갖추도록 도울 수 있습니다. 궁극적으로, 연구의 방향은 LLM이 인간의 가치와 의도에 더욱 깊이 정렬되도록 하면서도, 훈련 과정의 복잡성과 자원 소모를 최소화하는 데 맞춰질 것입니다. 이는 LLM이 더욱 안전하고, 유용하며, 신뢰할 수 있는 AI 시스템으로 발전하는 데 결정적인 역할을 할 것입니다.