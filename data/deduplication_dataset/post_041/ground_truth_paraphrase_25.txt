지난 몇 년간, 대규모 언어 모델(LLM) 연구 분야에서 강화 학습(RL)은 지대한 영향력을 발휘하는 핵심 영역 중 하나로 자리매김했습니다. 초창기 연구들은 RL을 활용하여 LLM의 출력을 인간의 선호도에 부합하도록 정렬시켰으며, 이러한 초기 LLM-RL 적용은 거의 전적으로 근접 정책 최적화(Proximal Policy Optimization, PPO) [1] 알고리즘에 의존했습니다. 이 선택은 PPO를 LLM 후처리 학습 과정에서 수년 동안 표준 RL 기법으로 확고히 만들었습니다. LLM 연구의 급속한 발전 속도를 고려할 때, 이는 상당히 오랜 기간 동안의 지배력이었습니다! 최근에 이르러서야 LLM 추론 관련 연구자들은 GRPO와 같은 다른 알고리즘들을 탐색하기 시작했습니다. PPO의 중요성에도 불구하고, 이 알고리즘은 최상위 연구 기관 외부에서는 여전히 충분히 이해되지 못하고 있습니다. 이러한 이해 부족은 여러 타당한 이유를 가집니다. PPO는 복잡한 구현 세부 사항이 많아 다루기 어려운 알고리즘일 뿐만 아니라, 높은 컴퓨팅 및 메모리 요구 사항으로 인해 광범위한 자원 없이는 실험하기가 어렵습니다. PPO를 성공적으로 활용하기 위해서는 알고리즘에 대한 깊이 있는 통찰력과 더불어 상당한 도메인 지식 또는 실제 경험이 요구됩니다. 본 글은 RL의 근본적인 개념들부터 출발하여, PPO에 대한 심층적인 이해를 단계적으로 구축해 나갈 것입니다. 이 기반 위에서 PPO 및 그 다양한 구성 요소에 대한 의사 코드(pseudocode)를 포함한 주요 실용적 고려 사항들을 제시할 것입니다. 마지막으로, LLM 영역에서 PPO를 대중화시킨 핵심 연구들을 검토하며 이 모든 지식을 통합할 것입니다.

AI 연구의 최신 정보를 얻기 위해 Deep (Learning) Focus를 사용하는 50,000명의 다른 사람들과 함께하세요. 구독하기

### 강화 학습(RL) 핵심 개념

PPO를 보다 심층적으로 탐구하기 전에, 강화 학습 전반에 대한 이해를 선행해야 합니다. 이 섹션에서는 RL의 기본적인 문제 설정과 핵심 용어들을 다룰 것입니다. 또한, PPO의 근간이 되는 간결한 정책 경사(policy gradient) 표현을 유도할 것입니다.

#### 문제 정의 및 용어

RL 훈련을 수행할 때, 우리는 특정 **환경(environment)** 내에서 **행동(action)**을 취하는 **에이전트(agent)**를 가정합니다. 에이전트의 행동은 **정책(policy)**에 의해 결정됩니다. 정책은 에이전트의 의사결정 체계와 같으며, 대개 매개변수화(parameterized)되어 있습니다. 예를 들어, LLM 훈련 맥락에서는 LLM 자체가 정책의 역할을 수행합니다. 특정 시점 $t$에서 상태 $s_t$가 주어졌을 때, 정책 $\pi_\theta$에 따른 행동 $a_t$의 확률은 $\pi_\theta(a_t | s_t)$로 모델링됩니다. 에이전트가 행동을 취하면, 환경의 **상태(state)**는 환경의 일부인 **전이 함수(transition function)**에 따라 갱신됩니다. 전이 함수는 $P(s_{t+1} | a_t, s_t)$로 표현되지만, LLM 분야에서는 상태가 대개 누적적인 특성을 가지므로 (예: 프롬프트 $x$와 이전 토큰들 $a_1, \dots, a_t$로 구성된 $s_t = \{x, a_1, a_2, \dots, a_t\}$), 전이 함수는 덜 중요하게 다루어집니다. 이와 더불어, 에이전트가 각 상태를 방문할 때마다 환경으로부터 **보상(reward)**을 받게 되는데, 이 보상은 양수, 음수 또는 0일 수 있습니다.

에이전트는 반복적으로 행동을 수행하며, 각 행동($a_t$), 보상($r_t$), 그리고 상태($s_t$)는 **시간 단계(time step)** $t$와 연관됩니다. 이러한 시간 단계들이 연속적으로 연결되면 하나의 **궤적(trajectory)**이 형성됩니다. 이 궤적은 에이전트가 환경에서 수행한 일련의 상호작용을 나타냅니다. 전체 궤적의 확률은 정책 $\pi_\theta(a_t | s_t)$에 의해 주어진 각 행동의 확률과 전이 함수 $P(s_{t+1} | a_t, s_t)$에 의해 주어진 각 상태 전이 확률을 연쇄 법칙(chain rule)을 사용하여 결합함으로써 계산될 수 있습니다.

**RL 목표(objective).** 강화 학습에서 모델을 훈련하는 궁극적인 목표는 전체 궤적에 걸쳐 획득하는 **누적 보상(cumulative reward)**을 극대화하는 것입니다. 이 목표는 몇 가지 변형을 가지는데, 특히 보상을 **할인(discounted)**하거나 **비할인(non-discounted)** 방식으로 계산할 수 있습니다. 할인율(discount factor) $\gamma$를 적용함으로써, 우리는 미래의 보상보다 현재의 보상에 더 높은 가중치를 부여하도록 정책을 유도합니다. 즉, '나중의 이득보다 현재의 이득이 더 중요하다'는 개념을 반영합니다. 우리의 목표는 일반적으로 궤적에 대한 기댓값(expectation)으로 표현되는 **기대 누적 보상(expected cumulative reward)**으로 공식화됩니다.

**상태, 가치 및 이점 함수(State, value, and advantage functions).** RL의 목표와 밀접하게 관련하여, 다음과 같은 핵심 함수들을 정의할 수 있습니다:
*   **가치 함수(Value Function)** $V(s)$: 특정 상태 $s$에서 시작하여 현재 정책 $\pi_\theta$에 따라 행동할 때 기대할 수 있는 누적 보상.
*   **행동-가치 함수(Action-Value Function)** $Q(s, a)$: 상태 $s$에서 행동 $a$를 취한 다음 정책 $\pi_\theta$에 따라 행동할 때 기대할 수 있는 누적 보상.
*   **이점 함수(Advantage Function)** $A(s, a)$: 행동-가치 함수와 가치 함수의 차이, 즉 $A(s, a) = Q(s, a) - V(s)$.

직관적으로, 이점 함수는 특정 상태 $s$에서 특정 행동 $a$를 취하는 것이 해당 상태에서 일반적인 기대 보상에 비해 얼마나 더 나은지 혹은 나쁜지를 알려줍니다. 행동 $a$로부터 얻는 보상이 기대치를 초과하면 이점은 양수가 되고, 그 반대의 경우 음수가 됩니다. 이점 함수는 RL 연구에서 매우 중요한 역할을 하며, 정책의 경사(gradient)를 계산하는 데 필수적으로 활용됩니다.

> "때때로 RL에서는 행동이 절대적으로 얼마나 좋은지 설명할 필요가 없고, 단지 평균적으로 다른 행동보다 얼마나 더 나은지만 알면 됩니다. 즉, 우리는 그 행동의 상대적 이점(relative advantage)을 알고 싶습니다. 우리는 이 개념을 이점 함수로 명확하게 만듭니다."
>
> — Spinning up in Deep RL

#### LLM을 위한 RL 공식화

이제 RL의 기본 개념들을 이해했으니, 이러한 용어들을 LLM 훈련 환경에 매핑해 봅시다:
*   우리의 **정책**은 LLM 자체입니다.
*   **초기 상태**는 주어진 프롬프트(prompt)입니다.
*   LLM의 출력—각 **토큰(token)** 또는 전체 **완성(completion)**—이 곧 **행동(action)**이 됩니다.
*   **상태**는 프롬프트와 LLM 출력의 결합으로 구성됩니다.
*   LLM의 전체 완성은 하나의 **궤적(trajectory)**을 이룹니다.
*   **보상(reward)**은 검증자(verifier) 또는 보상 모델(reward model)로부터 주어집니다.

이 설정에서는 전이 함수가 본질적으로 결정론적(deterministic)이므로 별도의 전이 함수를 명시적으로 고려하지 않습니다.

LLM은 **다음 토큰 예측(next token prediction)** 방식을 통해 출력을 생성하며, 이는 출력 완성의 각 토큰을 순차적으로 생성하는 자기회귀(autoregressive) 과정입니다. 이 과정은 RL 설정에 자연스럽게 매핑됩니다—각 토큰을 개별적인 행동으로 모델링할 수 있습니다. 이 설정은 **마르코프 결정 과정(Markov Decision Process, MDP) 공식화**라고 불리며, PPO와 같은 알고리즘이 의존하는 핵심 프레임워크입니다. LLM을 위한 MDP 모델링에서, 초기 상태는 프롬프트이며 정책은 개별 토큰을 예측함으로써 행동합니다. LLM은 토큰에 대한 확률 분포를 출력하는 (확률적) 정책을 형성합니다. 생성 과정 중 이 분포에서 토큰을 선택하여 행동이 취해지며, 각 토큰 자체가 하나의 행동입니다. 토큰이 예측된 후 현재 상태에 추가되고, LLM은 이를 사용하여 다음 토큰을 예측합니다—이는 자기회귀 다음 토큰 예측의 핵심입니다. 최종적으로, LLM은 정지 토큰(stop token, 예: `<|end_of_text|>`)을 예측하여 생성 과정을 완료하고, 완전한 궤적을 산출합니다.

#### 정책 경사(Policy Gradient) 기본 사항

RL 훈련 과정에서 우리의 목표는 누적 (할인될 수도 있는) 보상을 최대화하는 것입니다. 이를 달성하기 위해 우리는 **경사 상승(gradient ascent)** 기법을 활용할 수 있습니다. LLM 맥락에 적용하면, RL 훈련은 프롬프트 배치를 샘플링하고, LLM(정책)으로 완성을 생성하며, 이 완성에 대한 보상을 계산하고, 이 보상을 사용하여 정책 업데이트(policy update)를 도출하는 단계를 따릅니다. 이 마지막 정책 업데이트 단계에서 경사 상승이 이루어집니다.

더욱 구체적으로, 우리는 완성(completion)과 보상(reward)을 활용하여 정책 매개변수에 대한 RL 훈련 목표의 경사(gradient)를 추정합니다—이를 "**정책 경사(policy gradient)**"라고 합니다. 이 경사를 계산할 수 있다면, 우리는 경사 상승을 통해 정책을 훈련시킬 수 있습니다. 그렇다면 이 경사는 어떻게 계산될까요?

> "강화 학습의 목표는 에이전트가 최적의 보상을 얻기 위한 최적의 행동 전략을 찾는 것입니다. 정책 경사 방법은 정책을 직접 모델링하고 최적화하는 것을 목표로 합니다."
>
> — Lilian Weng

**정책 경사(Policy gradients).** LLM 훈련에 사용되는 거의 모든 RL 최적화기(optimizer) (예: PPO [1], GRPO, REINFORCE)는 **정책 경사 알고리즘(policy gradient algorithms)** 범주에 속합니다. 이들은 정책 경사를 추정하고, 이 추정치를 사용하여 경사 상승을 수행하는 방식으로 작동합니다. 각 알고리즘은 정책 경사를 추정하는 데 있어 상이한 접근법을 사용하지만, 그들의 기본적인 아이디어는 상당히 유사합니다—정확히 사용되는 기술에 따라 세부 사항을 조절할 뿐입니다. 바닐라 정책 경사(Vanilla Policy Gradient, VPG)는 많은 온라인 자료에서 상세히 다루어져 왔습니다. 그러나 우리는 완전성을 위해 여기에서 정책 경사의 몇 가지 기본 형태를 다시 유도할 것입니다. RL에서 우리의 목표는 누적 보상을 최대화하는 것이며, 이 목표의 경사를 정책 $\theta$의 매개변수에 대해 계산하면 다음을 얻을 수 있습니다:

$$
\nabla_\theta J(\theta) = E_{\tau \sim \pi_\theta} \left[ \sum_{t=0}^T \nabla_\theta \log \pi_\theta(a_t | s_t) R(\tau) \right]
$$

이 유도는 RL 훈련 목표(누적 보상)의 경사에서 시작하여 정책 경사에 대한 기본 표현으로 귀결됩니다. 핵심 단계는 로그 도함수 트릭(log-derivative trick)의 활용과 궤적 확률 정의를 사용하는 것입니다.

**기본 정책 경사 구현.** 우리가 지금까지 유도한 기본 정책 경사 표현은 이론적인 형태이며 기댓값(expectation)을 포함합니다. 실제 계산에서는 이를 표본 평균(sample mean)으로 근사해야 합니다. 즉, 고정된 수의 궤적(LLM의 경우 프롬프트와 완성)을 샘플링하고, 각 궤적에 대한 정책 경사 표현의 평균을 취합니다. 기본 정책 경사 계산 과정을 보다 구체화하기 위해, PyTorch 의사 코드(pseudocode)로 된 단계별 구현이 아래에 제공됩니다.

```python
# basic policy gradient implementation
# for each trajectory in the batch
for trajectory in trajectories:
    # compute the policy gradient for this trajectory
    policy_gradient = 0
    for t in range(len(trajectory)):
        # get the log probability of the action at time t
        log_prob_action_t = LLM.log_prob(trajectory.actions[t] | trajectory.states[t])

        # get the cumulative reward for the trajectory
        cumulative_reward = trajectory.rewards.sum()

        # add to the policy gradient
        policy_gradient += log_prob_action_t * cumulative_reward

    # update the policy parameters with the policy gradient
    optimizer.step(policy_gradient)
```

위 구현에서 중요한 점은 정책 경사를 직접 계산하지 않고, 경사가 정책 경사와 동일한 손실 함수(loss function)를 공식화한 다음, PyTorch의 자동 미분(autodiff)을 사용하여 정책 경사를 계산한다는 것입니다—이는 `loss.backward()` 호출 시 발생합니다. 정책 경사를 계산하는 데 사용되는 손실 함수는 다음과 같습니다.

$$
L(\theta) = - E_{\tau \sim \pi_\theta} \left[ \sum_{t=0}^T \log \pi_\theta(a_t | s_t) R(\tau) \right]
$$

이러한 구별은 PPO (및 TRPO!)가 정책 경사에 대한 직접적인 표현보다는 손실 함수를 통해 공식화된다는 점에서 이해하는 것이 중요합니다.

**기본 정책 경사의 문제점.** 기본 정책 경사 표현은 간단하지만, 다음과 같은 몇 가지 중요한 문제점을 안고 있습니다:
*   **높은 분산(High Variance)**: 경사 추정치(gradient estimates)가 높은 분산을 가질 수 있어 훈련을 불안정하게 만듭니다.
*   **불안정한 정책 업데이트(Unstable Policy Updates)**: 정책에 대한 크고 잠재적으로 불안정하게 만드는 업데이트를 방지할 메커니즘이 부족합니다.

이러한 문제들을 해결하기 위해, 대부분의 정책 경사 알고리즘은 정책 경사 추정치의 분산을 줄이고 정책 업데이트에 **신뢰 영역(trust region)**을 강제하는 데 (즉, 단일 업데이트에서 정책이 얼마나 변할 수 있는지 제한하는 것) 중점을 둡니다.

> "이 경사로 한 걸음 나아가면, 각 행동의 로그 확률이 $R(\tau)$에 비례하여 증가합니다. $R(\tau)$는 지금까지 얻은 모든 보상의 합입니다."
>
> — Spinning up in Deep RL

**리워드-투-고(Reward-to-go).** 예를 들어, 우리의 기본 정책 경사(참조를 위해 아래에 복사됨)에서 우리는 궤적의 누적 보상에 기반하여 주어진 행동의 확률을 증가시키고 있음을 알 수 있습니다. 따라서 우리는 행동이 발생하기도 전에 관찰된 보상 때문에 행동의 확률을 증가시킬 수 있습니다!

$$
\nabla_\theta J(\theta) = E_{\tau \sim \pi_\theta} \left[ \sum_{t=0}^T \nabla_\theta \log \pi_\theta(a_t | s_t) R(\tau) \right]
$$

이러한 단순한 관찰은 "**리워드-투-고(reward-to-go)**" 정책 경사의 개발로 이어졌습니다. 이 수정된 정책 경사 표현은 단순히 누적 보상을 행동 후에 관찰된 보상의 합으로 대체합니다. 이 리워드-투-고 공식화는 정책 경사의 편향 없는 추정량(unbiased estimator)이며, 이전의 기본 정책 경사 표현에 비해 증명 가능한 낮은 분산을 가집니다.

**기준선(Baselines).** 분산을 더욱 줄이기 위해, 우리는 정책 경사 표현에 **기준선(baseline)**을 추가할 수 있습니다. 리워드-투-고 정책 경사와 유사하게, 기준선 버전의 정책 경사도 편향이 없고 낮은 분산을 가집니다. 이 기준선은 현재 상태에만 의존해야 합니다. 이 표현은 리워드-투-고 정책 경사와 거의 동일하며, 단순히 리워드-투-고 항에서 추가 기준선을 뺍니다. 한 가지 일반적인 기준선은 가치 함수(value function)이며, 이는 기대치보다 높은 누적 보상을 달성하는 행동을 긍정적으로 강화합니다.

> "바닐라 정책 경사 알고리즘의 일반적인 문제는 경사 업데이트의 높은 분산입니다… 이를 완화하기 위해 기준선이라고 불리는 다양한 기술이 가치 추정치를 정규화하는 데 사용됩니다. 기준선은 여러 가지 방식으로 이를 수행하며, 다운스트림 행동에 대한 상태의 가치로 효과적으로 정규화합니다 (예: Q 값과 가치의 차이인 이점(Advantage)의 경우). 가장 간단한 기준선은 보상 배치(batch)의 평균 또는 이동 평균입니다."
>
> — RLHF 책

**일반 정책 경사.** [3]에서는 정책 경사를 계산하는 옵션이 더 일반적인 정책 경사 표현으로 요약되었습니다. 이 표현은 우리가 지금까지 본 표현들과 거의 동일하며, 유일한 차이점은 우리의 보상 항 $R(\tau)$를 여러 다른 표현과 같게 설정할 수 있는 일반적인 $\Psi_t$ 항으로 변경했다는 것입니다. PPO는 $\Psi_t$를 이점 함수 $A(s_t, a_t)$와 같게 설정하는 데 초점을 맞춥니다. 이 설정은 **바닐라 정책 경사(VPG)**라고 불립니다.

$$
\nabla_\theta J(\theta) = E_{\tau \sim \pi_\theta} \left[ \sum_{t=0}^T \nabla_\theta \log \pi_\theta(a_t | s_t) A(s_t, a_t) \right]
$$

이론적으로, VPG는 가장 낮은 분산의 경사 추정치를 산출하지만, 정책 업데이트에 신뢰 영역을 강제할 메커니즘이 여전히 없습니다. PPO는 이 문제에 대한 해결책으로 만들어졌습니다. PPO는 우리가 본 기본 정책 경사 표현과 유사하지만, 정책 업데이트에 신뢰 영역을 강제하기 위한 메커니즘이 추가되었습니다. 이제 PPO와 그 구현에 관련된 많은 실용적인 세부 사항에 대해 더 자세히 알아보겠습니다.

### 근접 정책 최적화(Proximal Policy Optimization, PPO) 심층 분석

이제 강화 학습의 기본 원리들을 숙지했으므로, 다음으로 근접 정책 최적화(PPO) [1]에 대해 심도 있게 다룰 시간입니다. 이 설명은 이전 섹션에서 도출한 VPG 개념을 바탕으로 하며, PPO의 전신인 신뢰 영역 정책 최적화(Trust Region Policy Optimization, TRPO) [6]부터 시작하여 PPO가 어떻게 발전했는지 살펴봅니다. TRPO는 훈련 안정화에 효과적이지만, 그 복잡성이 상당합니다. PPO는 유사한 장점을 가지면서도 더욱 실용적인 대안으로 개발되었습니다. 이 섹션의 말미에는 PPO에서 이점 함수를 계산하는 가장 보편적인 접근 방식인 일반화된 이점 추정(Generalized Advantage Estimation, GAE) [3]도 다룰 것입니다.

#### TRPO의 등장과 한계 [6]

> "TRPO는 페널티(penalty) 대신 강력한 제약(hard constraint)을 사용합니다. 왜냐하면 다양한 문제—심지어 학습 과정에서 특성이 변하는 단일 문제 내에서도—에서 잘 작동하는 단일 $\beta$ 값을 선택하기 어렵기 때문입니다."
>
> — [1]에서

PPO를 이해하기 위해서는 그 선행 연구인 신뢰 영역 정책 최적화(TRPO) [6]를 살펴보는 것이 필수적입니다. TRPO의 핵심 동기는 데이터 효율적이며 과도한 하이퍼파라미터 튜닝을 요구하지 않는 알고리즘을 개발하는 것이었습니다. 이를 위해 [6]의 저자들은 정책이 단조롭게 개선(monotonically improve)되도록 보장하는 제약된 목표(constrained objective)를 제안했습니다. 이 목표는 정책 업데이트에 신뢰 영역을 강제함으로써, 훈련을 불안정하게 만들 수 있는 크고 파괴적인 정책 업데이트의 위험을 효과적으로 제거합니다.

$$
\max_\theta E_{s, a \sim \pi_{\theta_{old}}} \left[ \frac{\pi_\theta(a|s)}{\pi_{\theta_{old}}(a|s)} A(s, a) \right] \quad \text{s.t.} \quad E_{s \sim \pi_{\theta_{old}}} \left[ D_{KL}(\pi_{\theta_{old}}(\cdot|s) || \pi_\theta(\cdot|s)) \right] \le \delta
$$

**TRPO를 위한 대리 목표(Surrogate objective)** ([