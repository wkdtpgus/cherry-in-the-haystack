환영합니다, 독자 여러분! 이번 주 LLM Watch에서는:

해마에서 영감을 받은 메모리가 긴 문맥(long-context) 처리를 고된 작업에서 빠른 질주로 바꿉니다.
흐름 속에서 훈련된 에이전트(agent)는 더 나은 계획을 세우고 도구를 더 안정적으로 사용합니다.
작은 재귀 신경망(recursive net)이 어려운 퍼즐에서 대규모 모델보다 뛰어난 사고력을 보여줍니다.
프롬프트(prompt)는 살아있는 플레이북(playbook)이 됩니다: 스스로 진화하는 문맥(context).
코드 월드 모델(Code World Model) + 탐색(search)이 게임 플레이에서 패턴 매칭(pattern-matching)을 능가합니다.
메모리 포싱(Memory Forcing)은 생성된 세계를 시간이 지나도 일관성 있게 유지합니다.
검증기(verifier) 유도 역추적(backtracking)은 취약한 프로세스 판단자를 길들입니다.

다시는 업데이트를 놓치지 않도록 구독하는 것을 잊지 마세요.

AI 엔지니어(AI Engineer)가 되는 가장 빠른 방법은? 직접 만들어 보는 것입니다! Towards AI의 산업 중심 코스인 '초급부터 고급 LLM 개발자까지 (약 90개 강의)'를 통해 실습 경험을 쌓으세요. 실제 세계에 영향을 미치기 위해 좌절했던 전직 박사 및 개발자들이 만들었습니다.

프로덕션(production) 준비가 된 앱(app)을 구축하세요: RAG, 파인튜닝(fine-tuning), 에이전트(agent)
지도: 디스코드(Discord)를 통한 강사 지원
필수 요건: 기본 파이썬(Python)
결과: 인증된 제품 출시
가치 보장: 30일 환불 보장

기술을 한 단계 높이세요

빠른 용어집 (초보자를 위한)

**KV-캐시(KV-cache) (단기 기억)**: 최근 토큰(token)에 대한 트랜스포머(Transformer)의 키/값(key/value) 롤링 버퍼(rolling buffer)입니다. 마지막 몇 페이지에는 훌륭하지만, 지난주에 읽은 소설에는 쓸모가 없습니다.
**인공 해마 네트워크(Artificial Hippocampus Network, AHN)**: 이전 토큰(token)을 압축된 상태로 만들면서 최근 KV-캐시(KV-cache)를 그대로 유지하는 학습 가능한 "장기 기억"입니다. 마치 어제의 노트를 보관하여 오늘의 탭(tab)이 빠르게 유지되도록 하는 것과 같습니다.
**슬라이딩 윈도우 어텐션(Sliding-window attention)**: 계산 비용을 절약하기 위해 마지막 N개의 토큰(token)만 읽는 방식입니다. 더 빠르지만 잘 잊어버립니다. AHN은 이러한 망각을 해결하는 것을 목표로 합니다.
**LLM 에이전트(LLM Agent)**: 대규모 언어 모델(LLM)을 핵심 추론 엔진으로 사용하여 환경과 상호작용하고, 도구를 사용하며, 복잡한 작업을 자율적으로 수행하는 시스템입니다.
**강화 학습(Reinforcement Learning, RL)**: 에이전트(agent)가 시행착오를 통해 최적의 행동 정책을 학습하는 머신러닝(machine learning) 패러다임입니다. 보상(reward)을 최대화하는 방향으로 행동합니다.
**인 컨텍스트 학습(In-context Learning)**: 모델(model)의 가중치를 업데이트(update)하지 않고 프롬프트(prompt) 내의 예시를 통해 새로운 작업을 학습하는 LLM의 능력입니다.
**플래너(Planner) / 실행기(Executor) / 검증기(Verifier) (에이전트 스택(agent stack))**: 모듈형 에이전트(agent) 설계입니다. 한 부분은 계획을 세우고, 다른 부분은 행동하며 (API / 도구 사용), 또 다른 부분은 작업을 확인합니다. 복잡성이 줄어들고 장기적인 제어가 향상됩니다.
**MCTS (몬테카를로 트리 탐색, Monte Carlo Tree Search)**: 많은 미래를 시도하고 유망한 가지를 유지하는 탐색(search) 방식입니다. 코드 기반 시뮬레이터(simulator)와 완벽하게 어울립니다.
**코드 월드 모델(Code World Model, CWM)**: 다음 움직임을 추측하는 대신, LLM이 코드(규칙, 유효한 움직임, 승리 확인)로 게임 시뮬레이터(simulator)를 작성하고 탐색(search) (예: MCTS)이 계획하도록 합니다. 불법적인 움직임이 줄어들고 더 깊은 전술을 가능하게 합니다.
**숨겨진 정보 게임(Hidden-information game)**: 모든 사실이 보이지 않는 게임(예: 손에 든 카드)입니다. CWM은 불확실성 하에서 추론하기 위한 추론 함수(inference function)를 포함합니다.

---

**효율적인 긴 문맥 모델링을 위한 인공 해마 네트워크(Artificial Hippocampus Networks for Efficient Long-Context Modeling) ( [논문](https://arxiv.org/abs/2405.18917) / [코드](https://github.com/microsoft/AHNs) )**

인공 해마 네트워크(Artificial Hippocampus Networks, AHN)는 손실 없는 단기 기억(트랜스포머(Transformer)의 증가하는 KV-캐시(KV cache))을 RNN과 유사한 모듈(module)을 사용하여 고정 크기의 압축된 장기 기억으로 변환합니다. 이 하이브리드(hybrid) 메모리(memory) 설계는 정확한 최신 정보를 유지하면서 윈도우(window) 밖의 문맥(context)을 지속적으로 압축하여, 모델(model)이 효율적인 긴 시퀀스(sequence) 처리를 위해 두 가지 메모리 유형을 모두 활용할 수 있도록 합니다.

트랜스포머(Transformer)의 키-값 캐시(key-value cache)의 슬라이딩 윈도우(sliding-window)를 손실 없는 단기 기억으로 유지하고, 학습 가능한 인공 해마 네트워크(Artificial Hippocampus Network)를 도입하여 오래된 토큰(token)을 압축된 장기 기억으로 만듭니다. 이 설계는 뇌의 다중 저장 메모리(multi-store memory) 모델(model)에서 영감을 받았으며, 어텐션(attention)의 충실도와 순환 메모리(recurrent memory)의 효율성을 결합합니다.

긴 문맥(long-context) 벤치마크(benchmark)에서 AHN으로 강화된 모델(model)은 슬라이딩 윈도우 트랜스포머(sliding-window Transformer)를 능가하며, 훨씬 적은 계산량과 메모리(memory) 사용량으로 전체 어텐션(full attention)과도 경쟁합니다. 예를 들어, Qwen2.5-3B-Instruct에 AHN을 추가하면 추론(inference) FLOPs를 약 40% 줄이고 메모리(memory) 사용량을 74% 줄이면서, 긴 문맥(long-context) 평가 점수를 4.41에서 5.88로 향상시킵니다.

---

**효과적인 계획 및 도구 사용을 위한 흐름 내 에이전트 시스템 최적화(In-the-Flow Agentic System Optimization for Effective Planning and Tool Use) ( [논문](https://arxiv.org/abs/2405.18908) / [코드](https://github.com/microsoft/AgentFlow) )**

최근 AI 에이전트(AI agent) 연구는 복잡한 작업을 효율적으로 수행하기 위해 에이전트를 여러 전문 모듈(module)로 분해하는 추세입니다. 이는 '플래너(planner)', '실행기(executor)', '검증기(verifier)'와 같은 구성 요소를 포함하는 설계로, 각 모듈(module)은 특정 기능을 담당하여 전체 시스템의 복잡성을 줄이고 안정성을 높입니다. 기존의 모놀리식(monolithic) 에이전트(agent)는 긴 시퀀스(sequence)의 상호작용에서 계획 오류나 도구 사용의 불안정성을 겪기 쉬웠습니다. 특히, 추론과 외부 API(Application Programming Interface) 호출을 혼합하는 과정에서 발생하는 비효율성은 다중 턴(multi-turn) 작업의 성공률을 저해하는 주요 원인이었습니다.

이러한 한계를 극복하기 위해 제안된 **AgentFlow** 프레임워크(framework)는 훈련 가능한 에이전트 시스템(agentic system)을 제공합니다. 이는 단일의 긴 추론 체인(reasoning chain) 대신, 여러 모듈이 다중 턴(multi-turn) 상호작용을 통해 협력하도록 설계되었습니다. AgentFlow의 핵심은 '흐름 내 최적화(in-the-flow optimization)' 개념입니다. 이는 에이전트가 실제 환경과의 상호작용 루프(loop) 안에서 계획 모듈(planning module)을 지속적으로 개선하도록 훈련하는 것을 의미합니다. 이러한 접근 방식은 장기적인 작업 수행 능력과 도구 사용의 안정성을 크게 향상시킵니다.

AgentFlow는 특히 **Flow-GRPO**라는 새로운 온-정책 훈련 알고리즘(on-policy training algorithm)을 활용하여, 희소한 보상(sparse reward)과 장기적인 목표를 가진 다중 턴(multi-turn) 작업을 효과적으로 처리합니다. 이 알고리즘은 단일 최종 결과(outcome)를 각 중간 단계로 역전파(backpropagate)하고 이점(advantage)을 정규화(normalize)함으로써 학습 과정을 안정화합니다. 이는 지역적인 결정이 전역적인 성공에 미치는 영향을 일치시켜 에이전트가 더 나은 장기 전략을 학습하도록 돕습니다.

실험 결과, AgentFlow (7B LLM 백본(backbone) 사용)는 탐색(search), 에이전트(agent), 수학, 과학 등 다양한 벤치마크(benchmark)에서 기존 최첨단(state-of-the-art) 기준선을 평균 14% 능가하며, 여러 작업에서 GPT-4와 같은 대규모 모델(model)보다도 우수한 성능을 보여주었습니다. 이는 흐름 내 훈련(in-the-flow training)이 에이전트의 계획 전략을 정교하게 만들고 도구 사용을 보다 안정적으로 만들며, 모델(model) 크기에 따른 스케일링(scaling) 효과를 더욱 증대시킨다는 것을 시사합니다.

---

**적을수록 좋다: 작은 네트워크를 이용한 재귀적 추론(Recursive Reasoning with Tiny Networks) ( [논문](https://arxiv.org/abs/2405.18909) )**

계층적 추론 모델(Hierarchical Reasoning Model, HRM)은 두 개의 작은 네트워크(총 2,700만 개의 파라미터(parameter))가 재귀적으로 추론함으로써 스도쿠, 미로, ARC와 같은 퍼즐에서 대규모 LLM을 능가할 수 있음을 보여주었습니다. 그러나 HRM의 두 모듈(module) 설계(빠른 "저수준(low-level)" 및 느린 "고수준(high-level)" 네트워크)는 복잡했으며 완전히 이해되지 않았습니다. 이 연구는 HRM의 복잡성을 제거한 작은 재귀 모델(Tiny Recursive Model, TRM)을 소개합니다. TRM은 단일의 작은 네트워크(단 2개의 레이어(layer), 약 700만 개의 파라미터(parameter))를 사용하여 반복적인 자체 개선을 수행합니다. 단순함에도 불구하고 TRM은 HRM보다 훨씬 높은 일반화(generalization)를 달성합니다. ARC-AGI-1에서 45%, ARC-AGI-2에서 8%를 기록하여, 훨씬 더 큰 LLM(DeepSeek R1, o3-mini, Gemini 2.5 Pro)보다 뛰어난 성능을 보였으며, 이들 모델 파라미터(parameter)의 0.01% 미만을 사용했습니다. 다시 말해, 신중한 재귀적 추론(recursive reasoning)은 무차별 대입(brute-force) 모델(model) 크기를 능가할 수 있습니다.

---

**에이전트 문맥 엔지니어링: 자기 개선 언어 모델을 위한 진화하는 문맥(Agentic Context Engineering: Evolving Contexts for Self-Improving Language Models) ( [논문](https://arxiv.org/abs/2405.18907) )**

대규모 언어 모델(LLM)의 활용이 확장됨에 따라, 정적인 프롬프트(prompt)로는 복잡하고 동적인 환경에서 최적의 성능을 유지하기 어렵다는 문제가 대두되고 있습니다. 특히 LLM 기반 에이전트(agent)나 전문 시스템(expert system)은 지속적인 개선이 필요한데, 모델(model) 가중치를 재훈련하는 대신, 프롬프트 자체를 개선하거나 전략을 추가하는 '문맥 적응(context adaptation)' 방식이 주목받고 있습니다. 그러나 단순한 프롬프트 수정은 종종 '간결성 편향(brevity bias)'으로 인해 중요한 정보가 손실되거나, 반복적인 재작업으로 인해 '문맥 붕괴(context collapse)'가 발생하여 정보의 질이 저하될 수 있습니다.

**에이전트 문맥 엔지니어링(Agentic Context Engineering, ACE)** 프레임워크(framework)는 이러한 문제에 대한 혁신적인 해결책을 제시합니다. ACE는 프롬프트와 문맥을 단순한 지시문이 아닌, 지속적으로 확장되고 개선되는 '진화하는 플레이북(evolving playbook)'으로 간주합니다. 이 프레임워크는 '생성(generation)', '반성(reflection)', '큐레이션(curation)'이라는 세 가지 핵심 단계를 반복함으로써 문맥 내에서 효과적인 전략을 축적하고 조직합니다. 이는 긴 대화나 복잡한 작업 과정에서도 세부 사항이 손실되지 않도록 구조화된 점진적 업데이트(update)를 수행합니다. ACE는 적응형 메모리(adaptive memory)의 개념을 에이전트(agent) 중심적으로 확장하여, 모델(model)이 스스로 학습하고 진화하는 문맥을 통해 성능을 최적화할 수 있도록 합니다.

ACE는 시스템 프롬프트(system prompt)를 개선하는 오프라인(offline) 방식과 에이전트의 실시간 메모리(memory)를 관리하는 온라인(online) 방식 모두에서 강력한 성능 향상을 가져왔습니다. 이전의 문맥 튜닝(context-tuning) 방법론에 비해 에이전트 벤치마크(benchmark)에서 10.6%, 금융 QA 벤치마크에서 8.6% 더 나은 성능을 보였으며, 프롬프트 적응 지연 시간(latency)과 비용도 절감했습니다. 특히, ACE는 지도 학습(supervised learning) 데이터(data)를 필요로 하지 않는다는 점이 중요합니다. 대신, 에이전트 자체 실행에서 발생하는 자연스러운 피드백(feedback)을 활용하여 문맥을 개선하는 방법을 자율적으로 학습합니다. AppWorld 리더보드(leaderboard)에서 ACE 기반 에이전트(agent)는 더 작은 오픈소스(open-source) 모델(model)을 사용했음에도 불구하고, 최고 수준의 프로덕션(production) 에이전트와 동등하거나 가장 어려운 테스트 분할(test split)에서는 더 뛰어난 점수를 기록했습니다. 이는 모델(model) 가중치를 직접 변경하지 않고도, 진화하는 문맥적 "소프트웨어(software)"가 LLM의 자기 개선을 어떻게 이끌 수 있는지를 명확히 보여줍니다.

---

**일반 게임 플레이를 위한 코드 월드 모델(Code World Models for General Game Playing) ( [논문](https://arxiv.org/abs/2405.18910) )**

대규모 언어 모델(LLM)은 게임 플레이(game playing) 분야에서 놀라운 능력을 보여주었지만, 단순히 다음 움직임을 예측하는 '패턴 매칭(pattern-matching)' 방식에 의존할 경우 몇 가지 근본적인 한계를 드러냅니다. 예를 들어, 종종 게임의 규칙을 위반하는 '불법적인 움직임'을 생성하거나, 장기적인 전략적 깊이가 부족한 '얕은 전술'을 구사하는 경우가 발생합니다. 이는 LLM이 게임의 내재된 규칙과 역학을 진정으로 '이해'하기보다는, 관찰된 데이터(data)에서 통계적 패턴을 학습하는 데 초점을 맞추기 때문입니다.

이러한 한계를 극복하기 위해 제안된 혁신적인 접근 방식은 LLM을 게임 플레이어(game player)가 아닌 '게임 시뮬레이터(game simulator) 구축자'로 활용하는 것입니다. 이 방법론의 핵심은 LLM이 게임의 규칙과 과거 기록으로부터 파이썬(Python)과 같은 코드로 '게임 시뮬레이터(simulator)'를 직접 작성하도록 하는 것입니다. 다시 말해, LLM은 게임의 상태 전환, 유효한 움직임, 승리 조건 등을 정의하는 함수(function)를 포함하는 공식적인 **코드 월드 모델(Code World Model, CWM)**을 생성합니다. 이렇게 생성된 CWM은 몬테카를로 트리 탐색(Monte Carlo Tree Search, MCTS)과 같은 고전적인 계획 알고리즘(planning algorithm)에 의해 활용되어 깊이 있는 탐색(search)과 전략적인 결정을 내릴 수 있습니다. 또한, LLM은 탐색(search)을 안내하고 숨겨진 정보 게임(hidden information game) (예: 카드 게임)을 처리하기 위한 휴리스틱(heuristic) 값 및 추론 함수(inference function)를 생성하는 역할도 수행합니다.

CWM 접근 방식은 여러 가지 중요한 장점을 제공합니다:
1.  **검증 가능성(Verifiability)**: LLM이 생성한 코드는 실행 가능한 규칙서 역할을 하므로, 플래너(planner)는 유효한 움직임을 안정적으로 열거할 수 있습니다. 이는 불법적인 움직임의 발생을 원천적으로 차단합니다.
2.  **전략적 깊이(Strategic Depth)**: LLM의 게임에 대한 의미론적 이해와 트리 탐색(tree search)의 깊은 미래 예측 능력이 결합되어, 기존 LLM 단독 플레이보다 훨씬 더 전략적인 플레이를 가능하게 합니다.
3.  **일반화(Generalization)**: 이 방법은 새로운 게임에 대한 탁월한 일반화(generalization) 능력을 보여줍니다. LLM은 각 새로운 게임마다 재훈련(retraining)할 필요 없이, 게임의 설명으로부터 새로운 시뮬레이터(simulator) 코드를 생성하기 위한 프롬프트(prompt)만 있으면 됩니다. 이는 '데이터(data)-코드(code) 변환'에 초점을 맞추기 때문입니다.

10가지 다양한 게임(완전 정보 보드 게임 5개와 부분 정보 카드 게임 5개, 이 중 4개는 새로운 게임)에 대한 실험에서 CWM 접근 방식은 **10개 중 9개 게임에서 강력한 기준선(baseline)인 Gemini 2.5 Pro와 동등하거나 더 우수한 성능을 보였습니다**. 이는 명시적인 월드 모델(world model)을 생성하는 것이 LLM을 이용한 일반 게임 플레이에 있어 패턴 매칭(pattern-matching)을 뛰어넘는 매우 효과적인 전략임을 입증합니다.

---

**메모리 포싱: 마인크래프트(Minecraft)에서 일관된 장면 생성을 위한 시공간 메모리(Memory Forcing: Spatio-Temporal Memory for Consistent Scene Generation on Minecraft) ( [논문](https://arxiv.org/abs/2405.18906) )**

자기회귀 비디오 모델(Autoregressive video model), 특히 확산 모델(diffusion model)은 오픈 월드(open-world) 게임 플레이(예: 마인크래프트(Minecraft))와 같은 복잡한 환경을 시뮬레이션(simulate)할 수 있는 놀라운 잠재력을 가지고 있습니다. 그러나 이러한 모델(model)들은 '장기적인 일관성(long-term consistency)'을 유지하는 데 있어 중대한 도전에 직면합니다. 새로운 영역을 탐색할 때는 창의적으로 새로운 콘텐츠(content)를 생성해야 하지만, 이전에 방문했던 영역으로 돌아올 때는 과거의 모습을 정확하게 재현하여 '시공간적 일관성(spatio-temporal consistency)'을 유지해야 합니다. 유한한 문맥 윈도우(context window)는 모델(model)이 최근의 시간적 메모리(temporal memory)에만 의존하게 만들어, 오래된 위치에 대한 정보를 잊어버리게 하여 공간적 일관성(spatial consistency)을 상실합니다. 반대로, 과거 영역의 지도(map)와 같은 공간적 메모리(spatial memory)를 통합하면 일관성(consistency)을 향상시킬 수 있지만, 모델(model)이 불완전한 과거 정보에 과도하게 집착하면 창의적인 탐색(exploration)을 저해할 수 있습니다.

**메모리 포싱(Memory Forcing)**은 이러한 딜레마(dilemma)를 해결하기 위해 고안된 혁신적인 훈련 프레임워크(training framework)입니다. 이 프레임워크는 새로운 훈련 프로토콜(training protocol)과 기하학적으로 인덱싱(indexing)된 공간 메모리 모듈(spatial memory module)을 결합합니다. 핵심 아이디어는 '하이브리드 훈련(Hybrid Training)'을 통해 모델(model)을 두 가지 주요 시나리오(scenario)에 노출시키는 것입니다. 첫째, '탐색(exploration)' 체제에서는 모델(model)이 새로운 미지의 지형을 처리하기 위해 시간적 메모리(temporal memory)만을 사용하는 방법을 학습합니다. 둘째, '재방문(revisiting)' 체제에서는 모델(model)이 이전에 알려진 지형으로 돌아올 때 공간 메모리(spatial memory)를 효과적으로 통합하는 방법을 학습합니다. 또한, '연결된 순방향 훈련(Chained Forward Training)'을 도입하여 모델(model)이 훈련 중에 더 긴 롤아웃(rollout)을 생성하도록 강제합니다. 이는 모델(model)이 더 큰 시점 변화를 경험하고 긴 시퀀스(sequence)에 걸쳐 일관성(consistency)을 유지하기 위해 공간 메모리(spatial memory)에 의존하도록 유도합니다.

메모리 포싱(Memory Forcing)의 공간 메모리(spatial memory)는 효율적인 3D 포인트 기반 캐시(point-based cache)로 구현됩니다. '포인트-프레임 검색(point-to-frame retrieval)'은 현재 보이는 블록(block)을 처음 나타난 프레임(frame)으로 매핑(map)하며, '증분 3D 재구성(incremental 3D reconstruction)'은 모델(model)이 프레임(frame)을 생성함에 따라 명시적인 월드 맵(world map)을 지속적으로 업데이트(update)합니다.

다양한 마인크래프트(Minecraft) 환경에서 수행된 실험 결과, 메모리 포싱(Memory Forcing)은 긴 비디오(video) 생성에 대한 계산 비용을 증가시키지 않으면서도 훨씬 더 나은 장기 공간 일관성(long-term spatial consistency)과 높은 시각적 품질을 달성했습니다. 이는 생성된 세계가 시간이 지나도 일관성을 유지하며, 시각적으로도 더욱 사실적이라는 것을 의미합니다. 실제로, 모델(model)은 고정된 문맥 길이(context length) 내에서, 이전에 탐색했던 구조가 다시 시야에 들어올 때 이를 올바르게 기억하고 재구축할 수 있으며, 동시에 알려진 지도(map)를 넘어선 새로운 콘텐츠(content)를 창의적으로 상상할 수 있는 능력을 보여주었습니다. 이는 장기적인 일관성이 중요한 모든 생성형 AI 애플리케이션(application)에 중요한 함의를 가집니다.

---

**불완전한 프로세스 검증기 길들이기: 역추적에 대한 샘플링 관점(Taming Imperfect Process Verifiers: A Sampling Perspective on Backtracking) ( [논문](https://arxiv.org/abs/2405.18911) )**

대규모 언어 모델(LLM)이 복잡한 추론(reasoning) 작업을 수행할 때, 각 추론 단계를 평가하고 피드백을 제공하는 '프로세스 검증기(process verifier)'의 역할은 매우 중요합니다. 이러한 검증기(verifier)는 LLM의 추론 능력을 강화할 잠재력을 가지고 있지만, 실제로는 취약성을 내포하고 있습니다. 아무리 고품질의 검증기(verifier)라 할지라도 때로는 실수를 저지를 수 있으며, 생성(generation) 과정이 각 단계에서 검증기(verifier)의 판단을 맹목적으로 신뢰한다면, 이러한 작은 검증기 오류는 눈덩이처럼 불어나 치명적인 실패로 이어질 수 있습니다. 이러한 '오류 증폭(error amplification)' 문제는 검증기(verifier) 훈련에 드는 비용을 고려할 때, 단순히 "진행하면서 검증(verify-as-you-go)"하는 디코딩(decoding) 방식이 기대 이하의 성능을 보일 수 있음을 의미합니다.

이러한 문제에 대응하여, 더 스마트한 디코딩(decoding) 전략이 검증기(verifier)의 결함을 완화할 수 있는지에 대한 질문이 제기되었습니다. 이 연구는 텍스트 생성(text generation)을 부분 해법 트리(tree of partial solutions)를 통한 확률적 탐색(stochastic search)으로 취급하는 검증기 유도 역추적(Verifier-Guided Backtracking) 알고리즘(algorithm)인 **VGB**를 소개합니다. VGB는 잘못된 단계에 돌이킬 수 없이 전념하는 대신, 검증기(verifier)가 문제점을 알릴 때 모델(model)은 확률적으로 역추적(backtrack)할 수 있도록 합니다. 이 알고리즘(algorithm)은 랜덤 워크(random walk) 알고리즘(algorithm)의 원리를 활용하여 검증기(verifier) 오류에 대한 이론적으로 더 큰 견고성(robustness)을 보장합니다. 저자들은 특히 근사 샘플링(approximate sampling) 이론의 고전적인 싱클레어-제럼(Sinclair–Jerrum) 랜덤 워크(random walk) 접근 방식과 유사점을 찾아, 그 아이디어를 유도 텍스트 생성(guided text generation) 설정으로 일반화했습니다.

경험적으로, VGB는 합성 작업과 실제 언어 벤치마크(benchmark) 모두에서 다양한 해법 품질 지표에 걸쳐 기존의 디코딩(decoding) 전략(그리디(greedy) 또는 빔 탐색(beam search) 등)을 능가했습니다. 이는 불완전한 프로세스 검증기(process verifier)를 사용하더라도, 신중하게 설계된 역추적 샘플러(backtracking sampler)가 검증기(verifier)의 약점을 "길들여" 표준 디코딩(decoding) 방식보다 더 나은 추론(reasoning) 성능을 이끌어낼 수 있음을 시사합니다. 이 연구는 LLM의 추론(reasoning)을 단순한 텍스트 생성(text generation)이 아닌 탐색(search) 문제로 보는 것의 중요성을 강조하며, 모델(model) 규모 확장에만 의존하기보다 알고리즘(algorithm)적 발전이 LLM의 능력을 비약적으로 향상시킬 수 있음을 보여줍니다. VGB는 복잡한 문제 해결 과정에서 LLM이 더 신뢰할 수 있는 결정을 내릴 수 있도록 돕는 중요한 진전을 나타냅니다.

---

❤️ 이 글이 마음에 드셨다면 '좋아요'를 누르고 동료들과 공유해주세요. 댓글을 통해 여러분의 생각을 나눠주세요!
LLM Watch를 읽어주셔서 감사합니다! 최신 소식을 빠르게 받아보고 저의 작업을 응원하시려면 지금 바로 무료로 구독하세요.
구독하기