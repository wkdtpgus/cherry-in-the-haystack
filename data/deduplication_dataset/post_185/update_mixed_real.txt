환영합니다, 독자 여러분! 2025년 LLM Watch 업데이트 버전에서는 다음과 같은 최신 동향과 연구 성과를 집중 조명합니다:

*   **인공 해마 네트워크(AHN)**는 긴 문맥(long-context) 처리를 과거의 고된 작업에서 더욱 효율적인 프로세스로 변화시킵니다.
*   **흐름 속에서 훈련된 에이전트(agent)**는 훨씬 더 나은 계획을 수립하고 도구를 더욱 안정적으로 활용합니다.
*   작은 **재귀 신경망(recursive net)**이 복잡한 퍼즐 해결에서 대규모 모델들을 능가하는 사고력을 선보입니다.
*   **프롬프트(prompt)**는 이제 단순한 지시를 넘어, 스스로 발전하는 동적인 플레이북(playbook)이 됩니다.
*   **코드 월드 모델(Code World Model)**과 지능형 탐색(search)의 결합은 게임 플레이에서 단순한 패턴 매칭(pattern-matching)의 한계를 뛰어넘습니다.
*   **메모리 포싱(Memory Forcing)** 기술은 생성된 가상 세계의 시공간적 일관성을 장시간 유지하는 데 필수적인 역할을 합니다.
*   **검증기(verifier) 유도 역추적(backtracking)**은 불완전한 프로세스 판단자들의 취약점을 효과적으로 제어합니다.
*   **새로운 동향: 다중 모달(Multi-modal) LLM의 진화** – 텍스트를 넘어 이미지, 오디오 등 다양한 형태의 데이터를 이해하고 생성하는 모델들이 실제 애플리케이션(application)에 통합되고 있습니다.

다시는 중요한 업데이트를 놓치지 않도록 지금 바로 구독하는 것을 잊지 마세요.

AI 엔지니어(AI Engineer)가 되는 가장 빠른 방법은 무엇일까요? 바로 직접 만들어 보는 것입니다! Towards AI의 산업 중심 코스인 '초급부터 고급 LLM 개발자까지 (약 90개 강의)'를 통해 실습 경험을 쌓으세요. 이 과정은 실제 세계에 영향을 미치기 위해 좌절했던 전직 박사 및 개발자들이 직접 만들었습니다. 2025년에는 최신 에이전트 프레임워크와 다중 모달 통합 기법이 추가되어 과정의 깊이를 더했습니다.

*   프로덕션(production) 준비가 된 앱(app)을 구축하세요: RAG, 파인튜닝(fine-tuning), 에이전트(agent)
*   지도: 디스코드(Discord)를 통한 강사 지원
*   필수 요건: 기본 파이썬(Python) 지식
*   결과: 인증된 제품 출시 능력 확보
*   가치 보장: 30일 환불 보장

여러분의 기술을 한 단계 더 높이세요!

**빠른 용어집 (초보자를 위한)**

**KV-캐시(KV-cache) (단기 기억)**: 트랜스포머(Transformer) 모델이 최근 토큰(token)에 대해 사용하는 키/값(key/value) 롤링 버퍼(rolling buffer)입니다. 이는 마지막 몇 페이지의 정보를 기억하는 데는 탁월하지만, 지난주에 읽은 소설의 세부 사항까지는 보관하지 못합니다.
**인공 해마 네트워크(Artificial Hippocampus Network, AHN)**: 이전 토큰(token)들을 압축된 형태로 변환하여 저장하면서도, 최근의 KV-캐시(KV-cache)는 그대로 유지하는 학습 가능한 "장기 기억" 모듈입니다. 이는 어제의 노트를 보관하여 오늘의 작업 공간이 빠르게 유지되도록 하는 것과 유사합니다.
**슬라이딩 윈도우 어텐션(Sliding-window attention)**: 계산 비용을 절약하기 위해 마지막 N개의 토큰(token)에만 집중하는 어텐션(attention) 방식입니다. 속도는 빠르지만, 오래된 정보에 대한 망각이 발생하기 쉽습니다. AHN은 이러한 망각을 효과적으로 해결하고자 합니다.
**플래너(Planner) / 실행기(Executor) / 검증기(Verifier) (에이전트 스택(agent stack))**: 모듈형 에이전트(agent) 설계의 핵심 구성 요소입니다. 한 부분은 복잡한 작업을 위한 계획을 수립하고, 다른 부분은 실제 행동을 수행하며 (API 또는 도구 사용), 마지막 부분은 작업의 정확성과 성공 여부를 확인합니다. 이는 복잡성을 낮추고 장기적인 제어를 향상시키는 데 기여합니다.
**MCTS (몬테카를로 트리 탐색, Monte Carlo Tree Search)**: 가능한 많은 미래 상황을 시뮬레이션하고 가장 유망한 경로를 탐색하여 최적의 결정을 내리는 탐색(search) 기법입니다. 코드 기반 시뮬레이터(simulator)와 결합될 때 그 진가가 발휘됩니다.
**코드 월드 모델(Code World Model, CWM)**: LLM이 다음 움직임을 단순히 추측하는 대신, 게임의 규칙, 유효한 움직임, 승리 조건 등을 코드로 작성하여 게임 시뮬레이터(simulator)를 직접 구축하도록 합니다. 이후 MCTS와 같은 탐색(search) 알고리즘이 이 시뮬레이터를 활용하여 계획을 수립합니다. 이를 통해 불법적인 움직임이 현저히 줄어들고 훨씬 더 깊이 있는 전술적 사고가 가능해집니다.
**숨겨진 정보 게임(Hidden-information game)**: 모든 플레이어가 게임의 모든 사실을 알 수 없는 게임 유형입니다 (예: 포커에서 상대방의 패). CWM은 이러한 불확실한 상황에서 추론하기 위한 추론 함수(inference function)를 포함합니다.
**다중 모달(Multi-modal)**: 여러 가지 형태의 데이터(예: 텍스트, 이미지, 음성, 비디오)를 동시에 처리하고 이해하는 인공지능 시스템의 능력입니다. LLM이 언어뿐만 아니라 시각적, 청각적 정보까지 통합하여 세상을 인식하는 방향으로 발전하고 있습니다.

---

**효율적인 긴 문맥 모델링을 위한 인공 해마 네트워크(Artificial Hippocampus Networks for Efficient Long-Context Modeling) ( [논문](https://arxiv.org/abs/2405.18917) / [코드](https://github.com/microsoft/AHNs) )**

인공 해마 네트워크(Artificial Hippocampus Networks, AHN)는 트랜스포머(Transformer)의 증가하는 KV-캐시(KV cache)와 같은 손실 없는 단기 기억을 RNN과 유사한 모듈(module)을 사용하여 고정 크기의 압축된 장기 기억으로 효과적으로 변환합니다. 이 하이브리드(hybrid) 메모리(memory) 설계는 정확한 최신 정보를 유지하면서도, 윈도우(window) 밖의 문맥(context)을 지속적으로 압축하여 모델(model)이 효율적인 긴 시퀀스(sequence) 처리를 위해 두 가지 메모리 유형을 모두 활용할 수 있도록 합니다.

트랜스포머(Transformer)의 키-값 캐시(key-value cache)의 슬라이딩 윈도우(sliding-window)를 손실 없는 단기 기억으로 유지하는 동시에, 학습 가능한 인공 해마 네트워크(Artificial Hippocampus Network)를 도입하여 오래된 토큰(token)을 압축된 장기 기억으로 만듭니다. 이 설계는 뇌의 다중 저장 메모리(multi-store memory) 모델(model)에서 영감을 받았으며, 어텐션(attention)의 충실도와 순환 메모리(recurrent memory)의 효율성을 결합하는 혁신적인 접근 방식입니다.

긴 문맥(long-context) 벤치마크(benchmark)에서 AHN으로 강화된 모델(model)은 슬라이딩 윈도우 트랜스포머(sliding-window Transformer)를 압도적으로 능가하며, 훨씬 적은 계산량과 메모리(memory) 사용량으로 전체 어텐션(full attention)과도 경쟁할 수 있는 성능을 보여줍니다. 예를 들어, Qwen2.5-3B-Instruct에 AHN을 추가하면 추론(inference) FLOPs를 약 40% 줄이고 메모리(memory) 사용량을 74%나 절감하면서, 긴 문맥(long-context) 평가 점수를 4.41에서 5.88로 크게 향상시킵니다. 이는 장문맥 처리의 새로운 지평을 열었다고 평가됩니다.

---

**효과적인 계획 및 도구 사용을 위한 흐름 내 에이전트 시스템 최적화(In-the-Flow Agentic System Optimization for Effective Planning and Tool Use) ( [논문](https://arxiv.org/abs/2405.18908) / [코드](https://github.com/microsoft/AgentFlow) )**

AI 에이전트(AI agent)를 전문화된 모듈(module) – 플래너(planner), 실행기(executor), 검증기(verifier), 생성기(generator) – 로 분해하여 다중 턴(multi-turn) 상호작용을 통해 함께 작동하는 훈련 가능한 에이전트 프레임워크(agentic framework)인 **AgentFlow**를 소개합니다. 추론(reasoning)과 API 호출을 혼합하는 모놀리식(monolithic) 도구 사용 정책(policy) (그리고 긴 시퀀스(sequence)에 어려움을 겪는)과 달리, AgentFlow는 실시간 다중 턴(multi-turn) 작업의 루프(loop) 내에서 계획 모듈(planning module)을 최적화하여 더 나은 장기적인 성능을 제공합니다. 이는 에이전트의 복잡한 문제 해결 능력을 비약적으로 향상시킵니다.

다중 턴(multi-turn) 작업을 일련의 단일 턴(single-turn) 업데이트(update)로 처리하여 장기적이고 희소한 보상(sparse reward)을 처리하는 새로운 온-정책 훈련 알고리즘(on-policy training algorithm) **Flow-GRPO**를 제안합니다. 단일 최종 결과(outcome)를 각 단계로 역전파(backpropagate)하고 (지역적 결정과 전역적 성공을 일치시키며) 이점(advantage)을 정규화(normalize)하여 학습을 안정화합니다.

**상당한 개선**: 탐색(search), 에이전트(agent), 수학, 과학 작업을 아우르는 10개의 벤치마크(benchmark)에서 AgentFlow (7B LLM 백본(backbone) 사용)는 최첨단 기준선(baseline)을 평균 약 14% 능가하며, 여러 작업에서 GPT-4와 같은 더 큰 모델(model)까지 뛰어넘는 놀라운 성과를 보여주었습니다. 분석 결과, 흐름 내 훈련(in-the-flow training)은 향상된 계획 전략, 더 안정적인 도구 사용, 그리고 모델(model) 크기 및 추론(reasoning) 턴(turn)에 따른 더 나은 스케일링(scaling)을 가져옵니다. AgentFlow는 실제 환경에서 AI 에이전트의 신뢰성과 효율성을 높이는 데 중요한 진전을 이루었습니다.

---

**적을수록 좋다: 작은 네트워크를 이용한 재귀적 추론(Recursive Reasoning with Tiny Networks) ( [논문](https://arxiv.org/abs/2405.18909) )**

계층적 추론 모델(Hierarchical Reasoning Model, HRM)은 두 개의 작은 네트워크(총 2,700만 개의 파라미터(parameter))가 재귀적으로 추론함으로써 스도쿠, 미로, ARC와 같은 퍼즐에서 대규모 LLM을 능가할 수 있음을 보여주었습니다. 그러나 HRM의 두 모듈(module) 설계(빠른 "저수준(low-level)" 및 느린 "고수준(high-level)" 네트워크)는 복잡했으며 완전히 이해되지 않았습니다. 이 연구는 HRM의 복잡성을 제거한 작은 재귀 모델(Tiny Recursive Model, TRM)을 소개합니다. TRM은 단일의 작은 네트워크(단 2개의 레이어(layer), 약 700만 개의 파라미터(parameter))를 사용하여 반복적인 자체 개선을 수행합니다. 단순함에도 불구하고 TRM은 HRM보다 훨씬 높은 일반화(generalization)를 달성합니다. ARC-AGI-1에서 45%, ARC-AGI-2에서 8%를 기록하여, 훨씬 더 큰 LLM(DeepSeek R1, o3-mini, Gemini 2.5 Pro)보다 뛰어난 성능을 보였으며, 이들 모델 파라미터(parameter)의 0.01% 미만을 사용했습니다. 다시 말해, 신중한 재귀적 추론(recursive reasoning)은 무차별 대입(brute-force) 모델(model) 크기를 능가할 수 있다는 것을 명확히 보여줍니다. 이는 자원 제약이 있는 환경에서의 AI 적용 가능성을 크게 확장합니다.

---

**에이전트 문맥 엔지니어링: 자기 개선 언어 모델을 위한 진화하는 문맥(Agentic Context Engineering: Evolving Contexts for Self-Improving Language Models) ( [논문](https://arxiv.org/abs/2405.18907) )**

많은 LLM 애플리케이션(application) (에이전트(agent), 도메인 전문가(domain expert))은 모델(model) 가중치(weight)를 직접 업데이트(update)하는 대신, 문맥 적응(context adaptation) – 즉, 프롬프트(prompt)를 개선하거나 새로운 전략을 추가하는 방식 – 을 통해 성능을 향상시킵니다. 그러나 순진한 프롬프트(prompt) 편집은 종종 간결성 편향(brevity bias) (중요한 세부 사항을 과도하게 요약하여 제거하는 경향) 또는 문맥 붕괴(context collapse) (반복적인 재작성마다 정보가 점진적으로 저하되는 현상)를 초래할 수 있습니다.

에이전트 문맥 엔지니어링(Agentic Context Engineering, ACE) 프레임워크(framework)는 프롬프트(prompt)와 문맥(context)을 지속적으로 확장되고 개선되는 진화하는 플레이북(playbook)으로 취급합니다. 생성, 반성, 큐레이션(curation)의 주기를 통해 ACE는 문맥(context) 내에서 전략을 축적하고 조직하며, 긴 대화에서 세부 사항을 잃지 않도록 구조화된 점진적 업데이트(update)를 수행합니다. 이는 적응형 메모리(adaptive memory)의 동적 치트시트(Dynamic Cheatsheet) 아이디어를 기반으로 하지만, 훨씬 더 모듈화되고 에이전트(agent) 중심적인 방식으로 작동합니다.

ACE는 오프라인(offline) (예: 시스템 프롬프트(system prompt) 개선)과 온라인(online) (상호작용 중 에이전트(agent)의 메모리(memory) 관리) 모두에서 문맥(context)을 최적화하여 강력한 이득을 얻습니다. 이전 문맥 튜닝(context-tuning) 방법보다 에이전트(agent) 벤치마크(benchmark)에서 **+10.6%**, 금융 QA 벤치마크에서 **+8.6%** 더 나은 성능을 보였으며, 프롬프트(prompt) 적응 지연 시간(latency)과 비용을 효과적으로 줄였습니다. 특히, ACE는 지도 학습(supervised learning) 데이터(data)를 필요로 하지 않습니다. 에이전트(agent) 자체 실행에서 얻는 자연스러운 피드백(feedback)을 활용하여 문맥(context)을 개선하는 방법을 학습합니다. AppWorld 리더보드(leaderboard)에서 ACE 기반 에이전트(agent)는 더 작은 오픈소스(open-source) 모델(model)을 사용했음에도 불구하고, 최고 프로덕션(production) 에이전트의 전체 점수와 일치했으며 가장 어려운 테스트 분할(test split)에서는 이를 능가했습니다. 이는 진화하는 문맥적 "소프트웨어(software)"가 모델(model) 자체를 변경하지 않고도 LLM의 자기 개선을 어떻게 이끌 수 있는지를 명확하게 보여줍니다.

---

**일반 게임 플레이를 위한 코드 월드 모델(Code World Models for General Game Playing) ( [논문](https://arxiv.org/abs/2405.18910) )**

대규모 언어 모델(Large language model)은 움직임을 직접 예측하여 게임을 플레이할 수 있지만, LLM이 취약한 패턴 매칭(pattern matching)에 의존하기 때문에 종종 불법적인 움직임과 얕은 전술로 이어집니다. 이 논문은 대안을 제안합니다: LLM을 사용하여 게임의 규칙과 기록으로부터 코드(파이썬(Python))로 게임 시뮬레이터(simulator)를 구축하는 것입니다. 다시 말해, LLM은 상태 전환, 유효한 움직임, 승리 조건을 위한 함수(function)를 포함하는 공식적인 코드 월드 모델(Code World Model, CWM)을 작성하며, 이는 MCTS(몬테카를로 트리 탐색, Monte Carlo Tree Search)와 같은 고전적인 계획 알고리즘(planning algorithm)에 의해 사용될 수 있습니다. LLM은 또한 탐색(search)을 안내하고 숨겨진 정보 게임(hidden information game)을 처리하기 위한 휴리스틱(heuristic) 값 및 추론 함수(inference function)를 생성합니다.

CWM의 주요 장점은 다음과 같습니다:
1.  **검증 가능성(Verifiability)** – 생성된 코드는 실행 가능한 규칙서 역할을 수행하므로, 플래너(planner)는 유효한 움직임을 안정적으로 열거할 수 있습니다(불법적인 움직임 발생 위험이 없음).
2.  **전략적 깊이(Strategic Depth)** – LLM의 게임에 대한 의미론적 이해와 트리 탐색(tree search)의 깊은 미리 보기를 결합하여, 훨씬 더 전략적이고 정교한 플레이를 가능하게 합니다.
3.  **일반화(Generalization)** – 데이터-코드 변환(data-to-code translation)에 초점을 맞춤으로써, 이 방법은 새로운 게임에 놀랍도록 쉽게 적응합니다. LLM은 각 새로운 게임마다 재훈련(retraining)할 필요 없이, 새로운 시뮬레이터(simulator)를 생성하기 위한 프롬프트(prompt)만 있으면 됩니다.

10가지 다른 게임(5개의 완전 정보 보드 게임과 5개의 부분 정보 카드 게임, 이 중 4개는 완전히 새로운 게임 포함)에 대한 실험에서 CWM 접근 방식은 **10개 중 9개 게임에서 강력한 기준선(baseline) (Gemini 2.5 Pro)과 같거나 능가하는 성능을 보였습니다**. 따라서 명시적인 월드 모델(world model)을 생성하는 것은 LLM을 이용한 일반 게임 플레이에 효과적인 전략임이 입증되었으며, 이는 게임 AI 분야에 중요한 시사점을 제공합니다.

---

**메모리 포싱: 마인크래프트(Minecraft)에서 일관된 장면 생성을 위한 시공간 메모리(Memory Forcing: Spatio-Temporal Memory for Consistent Scene Generation on Minecraft) ( [논문](https://arxiv.org/abs/2405.18906) )**

**문제점**: 자기회귀 비디오 모델(Autoregressive video model) (확산 모델(diffusion model)과 같은)은 오픈 월드(open-world) 게임 플레이(예: 마인크래프트(Minecraft))를 시뮬레이션(simulate)할 수 있지만, 장기적인 일관성(long-term consistency) 측면에서 상충 관계에 직면합니다. 새로운 영역을 탐색할 때는 새로운 콘텐츠(content)를 생성해야 하지만, 이전에 방문했던 영역으로 돌아올 때는 기존의 세부 사항을 보존해야 합니다. 유한한 문맥 윈도우(context window)를 사용하는 경우, 최근의 시간적 메모리(temporal memory)에만 의존할 경우 모델(model)이 오래된 위치를 잊어버리게 되어 공간적 일관성(spatial consistency)이 상실됩니다. 반대로, 공간적 메모리(spatial memory) (과거 영역의 지도)를 통합하면 일관성(consistency)을 향상시킬 수 있지만, 모델(model)이 불완전한 과거 정보에 과도하게 집착하면 창의성이 저해될 수 있습니다.

**메모리 포싱(Memory Forcing)**은 새로운 훈련 프로토콜(training protocol)과 기하학적으로 인덱싱(indexing)된 공간 메모리 모듈(spatial memory module)을 결합하여 이 문제를 해결하는 혁신적인 훈련 프레임워크(training framework)입니다. 하이브리드 훈련(Hybrid Training)을 사용하여 모델(model)을 두 가지 체제에 노출시킵니다: 탐색(exploration) (모델은 새로운 미지의 지형을 처리하기 위해 시간적 메모리(temporal memory)만 사용하는 것을 학습) 및 재방문(revisiting) (모델은 알려진 지형으로 돌아올 때 공간 메모리(spatial memory)를 통합하는 것을 학습). 또한 연결된 순방향 훈련(Chained Forward Training)을 도입하는데, 여기서 모델(model)은 훈련 중에 더 긴 롤아웃(rollout)을 생성합니다. 이는 모델이 더 큰 시점 변화를 경험하고 긴 시퀀스(sequence)에 걸쳐 일관성(consistency)을 유지하기 위해 공간 메모리(spatial memory)에 의존하도록 강제합니다.

공간 메모리(spatial memory) 자체는 효율적인 3D 포인트 기반 캐시(point-based cache)로 구현됩니다. 포인트-프레임 검색(point-to-frame retrieval)은 현재 보이는 블록(block)을 처음 나타난 프레임(frame)으로 매핑(map)하고, 증분 3D 재구성(incremental 3D reconstruction)은 모델(model)이 프레임(frame)을 생성함에 따라 명시적인 월드 맵(world map)을 점진적으로 업데이트(update)합니다.

**결과**: 다양한 마인크래프트(Minecraft) 환경에서 메모리 포싱(Memory Forcing)은 긴 비디오(video)에 대한 계산 비용을 증가시키지 않으면서 훨씬 더 나은 장기 공간 일관성(long-term spatial consistency) (생성된 세계가 시간이 지나도 일관성을 유지함)과 더 높은 시각적 품질을 달성했습니다. 실제로, 모델(model)은 고정된 문맥 길이(context length) 내에서, 이전에 탐색했던 구조가 다시 시야에 들어올 때 이를 올바르게 기억하고 재구축할 수 있으며, 동시에 알려진 지도(map)를 넘어선 새로운 콘텐츠(content)를 창의적으로 상상할 수 있습니다. 이는 생성 AI의 현실감과 유용성을 크게 높이는 중요한 발전입니다.

---

**불완전한 프로세스 검증기 길들이기: 역추적에 대한 샘플링 관점(Taming Imperfect Process Verifiers: A Sampling Perspective on Backtracking) ( [논문](https://arxiv.org/abs/2405.18911) )**

프로세스 검증기(process verifier) (각 추론(reasoning) 단계를 판단하는 학습된 모델(model))로 강화된 대규모 언어 모델(Large language model)은 복잡한 추론에 대한 놀라운 가능성을 가지고 있지만, 동시에 취약합니다. 고품질 검증기(verifier)조차도 가끔 실수를 할 것이며, 생성(generation)이 각 단계에서 검증기를 맹목적으로 신뢰한다면 그러한 작은 검증기 오류는 눈덩이처럼 불어나 치명적인 실패로 이어질 수 있습니다. 이러한 오류 증폭 문제(error amplification problem)는 검증기(verifier) 훈련 비용을 고려할 때, 순진한 "진행하면서 검증(verify-as-you-go)" 디코딩(decoding)이 예상보다 성능이 저조할 수 있음을 의미합니다.

이 논문은 더 스마트한 디코딩(decoding) 전략이 검증기(verifier)의 내재된 결함을 완화할 수 있는지 탐구합니다. 이 논문은 텍스트 생성(text generation)을 부분 해법 트리(tree of partial solutions)를 통한 확률적 탐색(stochastic search)으로 취급하는 검증기 유도 역추적(Verifier-Guided Backtracking) 알고리즘(algorithm)인 **VGB**를 소개합니다. 잘못된 단계에 돌이킬 수 없이 전념하는 대신, 검증기(verifier)가 문제를 알릴 때 모델(model)은 확률적으로 역추적(backtrack)할 수 있습니다. VGB는 랜덤 워크(random walk) 알고리즘(algorithm)의 원리를 활용하여 검증기(verifier) 오류에 대한 더 큰 견고성(robustness)을 이론적으로 보장합니다. 실제로, 저자들은 근사 샘플링(approximate sampling) 이론의 고전적인 싱클레어-제럼(Sinclair–Jerrum) 랜덤 워크(random walk) 접근 방식과 유사점을 찾아, 그 아이디어를 유도 텍스트 생성(guided text generation) 설정으로 일반화합니다.

경험적으로, VGB는 합성 작업과 실제 언어 벤치마크(benchmark) 모두에서 다양한 해법 품질 지표에 걸쳐 기준선(baseline) 디코딩(decoding) 전략을 능가했습니다. 이는 불완전한 프로세스 검증기(process verifier)를 사용하더라도, 신중하게 설계된 역추적 샘플러(backtracking sampler)가 검증기(verifier)의 약점을 "길들여" 표준 그리디(greedy) 또는 빔 탐색(beam search) 디코딩(decoding)보다 더 나은 추론(reasoning) 성능을 이끌어낼 수 있음을 시사합니다. 이 연구는 추론(reasoning)을 탐색(search) 문제로 보는 것의 중요성을 강조하며, 알고리즘(algorithm) 발전(단순한 모델(model) 규모 확장이 아닌)이 AI 능력의 도약을 가져올 수 있음을 명확히 보여줍니다.

---

❤️ 이 글이 마음에 드셨다면 '좋아요'를 누르고 동료들과 공유해주세요. 댓글을 남겨 여러분의 의견을 알려주세요.
LLM Watch를 읽어주셔서 감사합니다! 새 게시물을 받고 제 작업을 지원하려면 무료로 구독하세요.
구독하기