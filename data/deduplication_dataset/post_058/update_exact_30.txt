대규모 언어 모델(LLM)의 역량이 점점 더 강력해짐에 따라, 이러한 모델을 다루는 가장 어려운 측면 중 하나는 모델을 적절하게 평가하는 방법을 결정하는 것입니다. 많은 강력한 모델이 존재하며, 각 모델은 매우 다양하고 복잡하며 개방형(open-ended)인 작업을 해결합니다. 결과적으로, 이 모델들 간의 성능 차이를 식별하는 것은 어려울 수 있습니다. LLM을 평가하는 가장 신뢰할 수 있는 방법은 인간 피드백(human feedback)을 활용하는 것이지만, 인간으로부터 데이터를 수집하는 것은 잡음이 많고(noisy), 시간이 많이 소요되며(time consuming), 비용이 많이 듭니다(expensive). 모델 역량을 측정하는 데 귀중하고 필수적인 진실의 원천임에도 불구하고, 인간 평가(human evaluation)는 단독으로 사용될 때 모델 개발 중 빠르게 반복(iterate)하는 능력을 저해합니다. 이 문제를 해결하기 위해 우리는 빠르고, 비용 효율적이며, 간단하지만 인간 평가 결과와 높은 상관관계(correlation)를 유지하는 평가 지표(evaluation metric)가 필요합니다. "인간 평가는 인간 선호도를 평가하는 데 황금 표준(gold standard)이지만, 예외적으로 느리고 비용이 많이 듭니다. 평가를 자동화하기 위해 우리는 GPT-4와 같은 최첨단 LLM을 인간을 대체하는(surrogate) 역할로 사용하는 것을 탐구합니다." - [17]에서 발췌. 아이러니하게도, LLM의 끊임없이 증가하는 역량은 이러한 평가 문제에 대한 잠재적인 해결책을 제시했습니다. 우리는 LLM 자체를 평가에 사용할 수 있으며, 이 접근 방식은 일반적으로 LLM-as-a-Judge [17]라고 불립니다. 이 기술은 GPT-4 출시 이후 처음 탐구되었는데, GPT-4는 다른 모델의 출력 품질을 평가할 수 있는 최초의 LLM이었습니다. 그 이후로 다양한 출판물에서 LLM-as-a-Judge를 분석하여 구현을 위한 모범 사례(best practices)를 밝혀내고 우리가 인지해야 할 중요한 편향(bias) 원인을 제시했습니다. 이 개요를 통해 우리는 이러한 많은 출판물을 살펴보고 LLM 평가에 대한 깊이 있고 실용적인 이해를 구축할 것입니다.

최근 몇 년간 LLM의 발전은 놀라웠습니다. 수많은 새로운 모델들이 등장하며 각기 다른 아키텍처와 훈련 방식을 통해 다양한 역량을 선보이고 있습니다. 이러한 모델들은 단순한 텍스트 생성에서부터 복잡한 추론, 코드 작성, 심지어 다중 모달(multi-modal) 능력까지 보여주며 우리 삶의 많은 부분을 변화시키고 있습니다. 그러나 이처럼 빠르게 진화하는 모델 환경 속에서, 각 모델의 실제 성능과 한계를 정확히 파악하는 것은 더욱 중요해지고 동시에 더욱 어려워졌습니다. 특히 오픈소스 LLM의 확산은 모델 간의 성능 비교와 평가의 필요성을 더욱 증대시켰습니다.

**LLM-as-a-Judge [17]란 무엇인가?** 텍스트 시퀀스(textual sequence)의 품질을 평가하기 위한 많은 전통적인 지표가 존재합니다. 이러한 지표는 참조 기반(reference-based)이거나 참조 불필요(reference-free)일 수 있으며, 품질 측정에 "정답(ground truth)" 시퀀스가 참조로 필요한지 여부를 나타냅니다. 이러한 지표는 기계 번역(machine translation) 또는 요약(summarization)과 같은 더 좁은 작업에서 잘 작동합니다. 자세한 내용은 여기를 참조하십시오. 그러나 현대 LLM은 다양하고 개방형(open-ended)인 작업을 해결하며 인간 선호도에 기반하여 광범위하게 정렬(align)되었는데, 이는 기존 자연어 처리(NLP) 벤치마크(benchmark)를 사용하여 감지하기 어렵습니다. 이러한 사용 사례의 경우, 전통적인 지표는 제대로 작동하지 않는 경향이 있으며 인간 선호도와 상관관계가 낮은 것으로 나타났습니다. "LLM-as-a-judge는 인간 선호도를 근사화(approximate)하는 확장 가능하고(scalable) 설명 가능한(explainable) 방법이며, 그렇지 않으면 얻기 매우 비쌉니다." - [17]에서 발췌. LLM-as-a-judge는 강력한 LLM에 직접 프롬프트(prompt)를 제공하여 다른 모델의 출력 품질을 평가하는 참조 불필요(reference-free) 지표입니다. 이러한 한계에도 불구하고, 이 기술은 확장 가능한(scalable) 방식으로 최소한의 구현 변경으로 다양한 개방형 작업을 평가할 수 있을 뿐만 아니라 인간 선호도와 일관되게 일치하는 것으로 밝혀졌습니다. 새로운 작업을 평가하기 위해 우리는 프롬프트(prompt)를 조정하기만 하면 됩니다! 이 지표는 GPT-4 출시 이후 제안되었으며, 이후 인기가 높아져 [17]에서 LLM-as-a-judge 지표에 대한 심층 분석(in-depth analysis) 출판으로 절정에 달했습니다. 오늘날 LLM-as-a-judge는 인간 평가와 함께 LLM을 위한 가장 널리 사용되는 평가 기술 중 하나이며, 모델이 인간 선호도와 얼마나 잘 정렬(alignment)되어 있는지 평가하는 작업에서 탁월한 성능을 보입니다.

LLM-as-a-Judge는 단순히 모델 출력을 평가하는 것을 넘어, 모델의 행동과 추론 과정을 이해하는 데 중요한 통찰력을 제공할 수 있습니다. 예를 들어, 심사관 LLM에 단순히 점수를 매기도록 요청하는 대신, 특정 기준에 따라 출력을 분석하고, 개선점을 제시하며, 심지어 잘못된 정보를 교정하는 과정을 거치도록 프롬프트(prompt)할 수 있습니다. 이러한 '비평 및 수정(critique and revise)' 방식은 모델 개발자가 단순히 "좋다/나쁘다"를 넘어 "왜 좋은가/나쁜가"를 이해하고, 모델을 반복적으로 개선하는 데 필수적인 정보를 제공합니다. 또한, LLM-as-a-Judge는 강화 학습(Reinforcement Learning from Human Feedback, RLHF)과 같은 정렬(alignment) 기술에서 인간 피드백(human feedback)을 대체하거나 보강하는 데 활용될 수 있습니다. 대규모로 선호도 데이터를 생성하여 모델의 행동을 인간의 가치와 선호도에 맞게 미세 조정(fine-tune)하는 데 기여하며, 이는 RLHF 파이프라인의 효율성을 크게 향상시킵니다.

**LLM-as-a-Judge의 선택과 활용**
LLM-as-a-Judge를 효과적으로 사용하기 위해서는 심사관(judge)으로 어떤 LLM을 선택할지, 그리고 어떤 평가 방식을 적용할지가 중요합니다. 과거에는 GPT-4가 사실상의 표준이었지만, 이제는 Claude 3, Gemini, 또는 특정 작업에 특화된 오픈소스 LLM 등 다양한 고성능 모델들이 심사관으로 고려될 수 있습니다. 심사관 모델의 선택은 평가하려는 작업의 복잡성, 예산, 그리고 해당 모델이 가질 수 있는 특정 편향(bias)에 따라 달라집니다.

*   **심사관 모델의 선정**: 최신 고성능 모델일수록 인간 선호도와의 일치도가 높은 경향이 있지만, API 비용이 높을 수 있습니다. 반면, 오픈소스 모델은 비용 효율적이지만, 평가하려는 작업에 대한 이해도나 추론 능력이 부족할 수 있습니다. 특정 도메인 지식이 필요한 경우, 해당 도메인 데이터로 미세 조정(fine-tune)된 모델을 심사관으로 활용하는 것도 고려해볼 수 있습니다.
*   **평가 방식의 다양화**:
    *   **다중 심사관(Ensemble Judging)**: 단일 LLM 심사관의 편향을 줄이기 위해 여러 LLM을 심사관으로 활용하고 그 결과를 종합하는 방식입니다. 예를 들어, 서로 다른 모델(GPT-4, Claude 3, Gemini)을 사용하여 동일한 응답을 평가하게 한 후, 다수결 또는 가중 평균 방식으로 최종 점수를 산출할 수 있습니다.
    *   **자기 비평 및 수정(Self-Critique and Revision)**: 심사관 LLM에게 단순히 점수를 매기도록 하는 대신, 먼저 응답의 장점과 단점을 분석하고, 개선 방안을 제시한 후, 그에 따라 점수를 매기도록 요청하는 방식입니다. 이는 평가의 투명성을 높이고, 개발자에게 더 구체적인 피드백을 제공합니다.
    *   **참조 기반 평가의 확장**: 전통적인 참조 기반 평가가 어려운 개방형 작업에서도, 심사관 LLM에게 "이상적인" 또는 "전문가 수준의" 참조 응답을 먼저 생성하게 한 후, 이를 바탕으로 다른 모델의 응답을 평가하도록 지시할 수 있습니다. 이는 특히 새로운 또는 전문적인 분야에서 유용할 수 있습니다.

**편향(Bias) (그리고 이를 피하는 방법…)** "우리는 LLM 심사관의 편향(bias)과 한계(limitation)를 식별합니다. 그러나 우리는 이러한 한계에도 불구하고 LLM 심사관과 인간 간의 일치도가 높다는 것을 보여줍니다." - [17]에서 발췌. LLM-as-a-Judge가 인간 선호도를 정확하게 예측할 수 있지만, 이 평가 전략은 완벽하지 않습니다. 이는 평가 과정에 여러 새로운 편향(bias) 원인을 도입합니다. 이미 우리는 LLM이 의심스러운 추론(reasoning) 능력, 프롬프트(prompt)의 사소한 변경에 대한 민감성, 장황한 출력(verbose output)을 생성하는 경향과 같은 수많은 한계(limitation)를 가지고 있음을 알고 있습니다. 이러한 약점 중 다수는 LLM-as-a-Judge 평가 내에서 상응하는 편향(bias)으로 이어집니다. 위치 편향(Position bias): 심사관은 프롬프트(prompt) 내 위치에 따라 출력을 선호할 수 있습니다(예: 쌍별 프롬프트(pairwise prompt)에서 첫 번째 응답). 장황함 편향(Verbosity bias): 심사관은 출력 길이에 따라 더 나은 점수를 할당할 수 있습니다(즉, 더 긴 응답이 더 높은 점수를 받습니다). 자기 강화 편향(Self-enhancement bias): 심사관은 스스로 생성한 응답을 선호하는 경향이 있습니다(예: GPT-4는 자체 출력에 높은 점수를 할당합니다). 위에 설명된 편향(bias) 원인 외에도, LLM 심사관은 스스로 답변하기 어려운 질문(예: 복잡한 추론(reasoning) 및 수학 질문)에 대한 응답을 채점하는 데 어려움을 겪는 경향이 있습니다. 또한 심사관은 컨텍스트(context)의 잘못된 정보에 쉽게 오도될 수 있습니다 [18]. 채점되는 응답 중 하나가 잘못된 경우, 심사관은 이 컨텍스트(context)에 의해 오도되어 부정확한 점수를 출력할 수 있습니다.

**편향 완화를 위한 추가 전략 및 윤리적 고려사항**
앞서 언급된 편향 완화 기법들 외에도, LLM-as-a-Judge의 신뢰성을 높이기 위한 연구가 활발히 진행되고 있습니다. 예를 들어, '블라인드 평가(blind evaluation)' 개념을 더욱 강화하여 심사관 LLM이 평가 대상 모델의 이름이나 출처에 대한 어떠한 정보도 가지지 못하도록 철저히 분리하는 것이 중요합니다. 또한, 심사관 LLM이 이전에 생성했거나 평가했던 데이터에 대한 기억을 가지지 않도록 각 평가 세션을 독립적으로 운영하는 것이 자기 강화 편향(self-enhancement bias)을 줄이는 데 도움이 됩니다.

윤리적 관점에서는, LLM-as-a-Judge의 사용이 인간 평가자를 완전히 대체할 수 없다는 점을 명확히 인지해야 합니다. LLM 심사관은 인간의 미묘한 언어적 뉘앙스, 문화적 맥락, 또는 특정 윤리적 가치 판단을 완벽하게 이해하지 못할 수 있습니다. 따라서 중요한 결정이나 민감한 분야에서는 항상 인간의 검토(human-in-the-loop) 과정을 포함시켜야 합니다. LLM-as-a-Judge는 인간 평가의 효율성을 높이는 도구이지, 그 자체로 최종적인 권위가 될 수 없습니다. 모델의 편향이 사회적 불평등을 증폭시키거나 특정 집단에 불이익을 줄 가능성도 항상 염두에 두어야 하며, 이를 완화하기 위한 지속적인 모니터링과 연구가 필요합니다.

**AlpacaEval: 지시 따르기(Instruction-Following) 모델의 자동 평가자 [8]**
2023년 중반에 처음 제안된 AlpacaEval [8]은 지시 따르기(instruction-following) 언어 모델을 위한 가장 인기 있는 LLM 기반의 자동화된 평가 지표(evaluation metric) (및 리더보드(leaderboard)) 중 하나입니다. AlpacaFarm [9]을 기반으로 하는 평가 전략—LLM 평가자를 사용하여 RLHF 스타일의 쌍별 선호도 레이블(pairwise preference label) 8 생성을 자동화하는 시뮬레이터(simulator)—은 간단하고 비서 스타일(assistant-style) 작업의 포괄적인 세트에 걸쳐 있는 805개의 고정된 지시 세트를 사용합니다. 여기를 참조하십시오. 각 지시에 대해 우리는 두 LLM(기준 모델(baseline model)과 평가 대상 모델)으로 출력을 생성합니다. 그런 다음 LLM 평가자가 각 모델 출력의 품질을 평가하는 데 사용되어(즉, 쌍별 설정(pairwise setup)), 두 모델 출력 간의 승률(win-rate)을 계산할 수 있게 합니다.

**이것이 왜 유용할까요?** AlpacaEval의 목표는 인간 선호도와 높은 상관관계(correlation)를 가지는 빠르고 저렴한 자동 평가 파이프라인(pipeline)을 만드는 것입니다. AlpacaEval의 현재 반복(iteration)은 3분 이내에 실행되며, 10달러 9 미만의 비용이 들고, 인간 평가(챗봇 아레나(Chatbot Arena)에서 가져옴)와 0.98의 스피어만 상관관계(Spearman correlation)를 가집니다. 위를 참조하십시오. 이에 비해 인간 평가는 잡음과 불일치에 취약하고, 훨씬 더 비싸며, 몇 주간의 주석(annotation) 시간이 필요할 수 있습니다. AlpacaEval이 매우 효율적이기 때문에, 이 지표(metric)는 모델 개발에 완벽합니다. 이는 간단한 지시 따르기(instruction-following) 작업에 대한 인간 평가를 위한 신뢰할 수 있는 대리(proxy)를 빠르고 저렴하게 계산할 수 있도록 제공합니다.

**AlpacaEval의 발전과 새로운 평가 벤치마크**
AlpacaEval은 그 효율성과 인간 선호도와의 높은 상관관계 덕분에 LLM 커뮤니티에서 널리 채택되었습니다. 그러나 지시 따르기(instruction-following) 능력 외에도 LLM의 다양한 측면을 평가하기 위한 새로운 벤치마크와 방법론이 지속적으로 개발되고 있습니다. 예를 들어, 복잡한 추론 능력, 수학적 문제 해결, 코드 생성 및 디버깅, 다중 언어 능력, 그리고 특정 도메인 지식에 대한 평가의 필요성이 커지고 있습니다.

*   **추론 능력 평가**: GSM8K와 같은 수학적 추론 벤치마크는 LLM의 복잡한 문제 해결 능력을 측정하는 데 사용됩니다. LLM-as-a-Judge는 이러한 문제에 대한 모델의 '사고 과정(chain of thought)'을 평가하여, 단순한 정답 여부를 넘어 추론의 품질을 심층적으로 분석하는 데 활용될 수 있습니다.
*   **코드 생성 및 디버깅 평가**: HumanEval이나 MBPP와 같은 벤치마크는 LLM의 코딩 능력을 평가합니다. LLM-as-a-Judge는 생성된 코드의 정확성뿐만 아니라 코드의 가독성, 효율성, 그리고 잠재적 버그 유무까지 평가하여 인간 개발자가 코드 리뷰를 하는 것과 유사한 피드백을 제공할 수 있습니다.
*   **다중 모달 LLM 평가**: 텍스트 외에 이미지, 오디오, 비디오를 이해하고 생성하는 다중 모달 LLM의 등장으로 평가의 복잡성은 더욱 증가했습니다. LLM-as-a-Judge는 이제 텍스트 응답뿐만 아니라 생성된 이미지의 품질, 오디오의 자연스러움 등 다중 모달 출력의 다양한 측면을 평가하는 데에도 적용될 가능성을 탐구하고 있습니다. 이는 시각적 질문 답변(Visual Question Answering, VQA)이나 이미지 캡셔닝(Image Captioning)과 같은 작업에서 특히 중요합니다.

이러한 새로운 평가 프레임워크들은 LLM-as-a-Judge의 핵심 원칙을 유지하면서도, 특정 작업의 요구사항에 맞게 프롬프트(prompt)와 평가 기준을 정교하게 조정합니다. 이는 LLM 평가가 더 이상 단일 지표(metric)에 의존하는 것이 아니라, 모델의 전체적인 역량을 포괄적으로 이해하기 위한 다각적인 접근 방식이 필요함을 시사합니다.

**LLM 기반 평가의 미래와 도전 과제**
LLM 기반 평가는 분명 LLM 개발의 속도를 가속화하고 비용을 절감하는 데 혁혁한 공을 세웠습니다. 그러나 여전히 해결해야 할 과제들도 많습니다.

*   **편향의 지속적인 연구 및 완화**: LLM 심사관의 내재된 편향은 끊임없이 연구되고 있으며, 새로운 유형의 편향이 발견되기도 합니다. 이러한 편향을 완전히 제거하는 것은 어려울 수 있으므로, 평가 결과에 미치는 영향을 정량화하고, 보고하며, 가능한 한 완화하는 노력이 지속되어야 합니다.
*   **투명성 및 설명 가능성**: LLM 심사관이 내린 판단의 근거를 더욱 투명하게 제시하고, 그 판단이 어떻게 도출되었는지 설명할 수 있는 메커니즘을 강화해야 합니다. 이는 평가 결과에 대한 신뢰도를 높이고, 모델 개발자가 의미 있는 피드백을 얻는 데 필수적입니다.
*   **인간 평가와의 균형**: LLM 기반 평가는 인간 평가를 보완하는 강력한 도구이지만, 완전히 대체할 수는 없습니다. 특히 주관적인 판단, 윤리적 고려사항, 또는 미묘한 창의성이 요구되는 작업에서는 인간의 역할이 여전히 중요합니다. 따라서 효율적인 자동화된 평가와 필수적인 인간 개입 사이의 최적의 균형점을 찾는 것이 중요합니다.
*   **동적 평가 시스템**: 모델이 지속적으로 발전함에 따라, 평가 벤치마크와 심사관 모델 자체도 업데이트되어야 합니다. 정적인 벤치마크는 빠르게 시대에 뒤떨어질 수 있으며, 심사관 LLM의 성능 또한 시간이 지남에 따라 변할 수 있습니다. 변화하는 LLM 생태계에 맞춰 유연하게 진화하는 동적 평가 시스템의 필요성이 대두되고 있습니다.

결론적으로, LLM-as-a-Judge는 대규모 언어 모델의 평가 방식에 혁명적인 변화를 가져왔습니다. 이는 모델 개발의 속도를 높이고, 인간 선호도에 대한 이해를 심화하며, 다양한 작업에 걸쳐 LLM의 성능을 확장 가능하게 측정할 수 있는 길을 열었습니다. 그러나 이 강력한 도구를 책임감 있고 효과적으로 사용하기 위해서는 그 한계와 편향을 명확히 인지하고, 지속적인 연구와 개선을 통해 그 신뢰성과 유용성을 더욱 높여나가야 할 것입니다.