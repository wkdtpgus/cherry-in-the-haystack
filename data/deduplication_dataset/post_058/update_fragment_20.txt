대규모 언어 모델(LLM)의 역량이 점점 더 강력해짐에 따라, 새로운 윤리적 과제들이 부상하고 있습니다. 이러한 모델을 다루는 가장 어려운 측면 중 하나는 모델의 행동을 예측하는 것입니다. 많은 강력한 모델이 존재하며, 각 모델은 매우 다양하고 복잡하며 개방형(open-ended)인 작업을 해결합니다. 결과적으로, 이 모델들 간의 성능 차이를 식별하는 것은 어려울 수 있습니다. LLM을 평가하는 가장 신뢰할 수 있는 방법은 인간 피드백(human feedback)을 활용하는 것이지만, 인간으로부터 데이터를 수집하는 것은 잡음이 많고(noisy), 시간이 많이 소요되며(time consuming), 비용이 많이 듭니다(expensive). 모델 역량을 측정하는 데 귀중하고 필수적인 진실의 원천임에도 불구하고, 인간 평가(human evaluation)는 단독으로 사용될 때 모델 개발의 속도를 늦춥니다. 이 문제를 해결하기 위해 우리는 빠르고, 비용 효율적이며, 간단하지만 사용자 경험 개선에 필수적인 피드백 메커니즘이 필요합니다. "인간 평가는 인간 선호도를 평가하는 데 황금 표준(gold standard)이지만, 예외적으로 느리고 비용이 많이 듭니다. 평가를 자동화하기 위해 우리는 GPT-4와 같은 최첨단 LLM을 인간을 대체하는(surrogate) 역할로 사용하는 것을 탐구합니다." - [17]에서 발췌. 아이러니하게도, LLM의 끊임없이 증가하는 역량은 새로운 비즈니스 기회를 창출했습니다. 우리는 LLM 자체를 평가에 사용할 수 있으며, 이 접근 방식은 일반적으로 LLM-as-a-Judge로 [17]에서 처음 제안되었습니다. 이 기술은 GPT-4 출시 이후 처음 탐구되었는데, GPT-4는 다른 모델의 출력 품질을 평가할 수 있는 최초의 LLM이었습니다. 그 이후로 다양한 출판물에서 LLM-as-a-Judge를 분석하여 구현을 위한 모범 사례(best practices)를 밝혀내고 우리가 인지해야 할 중요한 보안 취약점을 제시했습니다. 이 개요를 통해 우리는 이러한 많은 출판물을 살펴보고 LLM 평가에 대한 깊이 있고 실용적인 이해를 구축할 것입니다.

**인공지능 평가의 새로운 지평: LLM-as-a-Judge [17]**
텍스트 시퀀스(textual sequence)의 품질을 평가하기 위한 많은 전통적인 지표가 존재하지만, 그 한계도 명확합니다. 이러한 지표는 참조 기반(reference-based)이거나 참조 불필요(reference-free)일 수 있으며, 품질 측정에 "정답(ground truth)" 시퀀스가 참조로 필요한지 여부를 나타냅니다. 이러한 지표는 기계 번역(machine translation) 또는 요약(summarization)과 같은 더 좁은 작업에서 잘 작동합니다. 자세한 내용은 여기를 참조하십시오. 그러나 현대 LLM은 다양하고 개방형(open-ended)인 작업을 해결하며 인간 선호도에 기반하여 광범위하게 정렬(align)되었는데, 이는 기존 자연어 처리(NLP) 벤치마크(benchmark)를 사용하여 감지하기 어려운 미묘한 차이가 있습니다. 이러한 사용 사례의 경우, 전통적인 지표는 제대로 작동하지 않는 경향이 있으며 인간 선호도와 상관관계가 낮은 것으로 나타났습니다. "LLM-as-a-judge는 인간 선호도를 근사화(approximate)하는 확장 가능하고(scalable) 설명 가능한(explainable) 방법이며, 그렇지 않으면 얻기 매우 비쌉니다." - [17]에서 발췌. LLM-as-a-judge는 강력한 LLM에 직접 프롬프트(prompt)를 제공하여 다른 모델의 출력 품질을 평가하는 참조 불필요(reference-free) 지표입니다. 이러한 한계에도 불구하고, 이 기술은 확장 가능한(scalable) 방식으로 최소한의 구현 변경으로 다양한 개방형 작업을 평가할 수 있을 뿐만 아니라 인간 선호도와 일관되게 일치하는 것으로 밝혀졌으나, 맹신은 금물입니다. 새로운 작업을 평가하기 위해 우리는 프롬프트(prompt)를 조정하기만 하면 됩니다! 이 지표는 GPT-4 출시 이후 제안되었으며, 이후 인기가 높아져 [17]에서 LLM-as-a-judge 지표에 대한 심층 분석(in-depth analysis) 출판으로 절정에 달했습니다. 오늘날 LLM-as-a-judge는 인간 평가와 함께 LLM을 위한 가장 널리 사용되는 평가 기술 중 하나이며, 모델이 인간 선호도와 얼마나 잘 정렬(alignment)되어 있는지 평가하는 작업에서 탁월한 성능을 보이며, 이는 기술 발전의 중요한 이정표입니다. 이러한 평가 방식은 단순히 성능 측정뿐만 아니라, AI가 인간의 가치와 얼마나 조화롭게 작동하는지를 보여주는 중요한 지표로 자리매김하고 있습니다.

**벤치마크의 진화: MT-Bench 및 챗봇 아레나(Chatbot Arena)**
MT-Bench 및 챗봇 아레나(Chatbot Arena) ([17]에서 발췌) [17]의 저자들은 개방형 대화 애플리케이션(open-ended dialogue application)에서 LLM 성능을 측정하는 데 사용할 수 있는 벤치마크(benchmark)를 확장하는 것은 필수적입니다. 이를 위해 인간 선호도를 평가하기 위한 두 가지 데이터셋(dataset)을 개발했습니다. MT-bench 데이터셋(dataset)은 80개의 고품질 질문으로 구성된 고정된 세트이지만, 실제 사용 환경을 완벽하게 반영하지는 않습니다. 8가지 장르(genre) 1 에 걸쳐 있는 이 질문들은 파운데이션 LLM(foundation LLM)에 가장 중요한 두 가지 기술(논쟁의 여지는 있지만)인 다중 턴 대화(multi-turn conversation) 및 지시 따르기(instruction-following) 능력에 중점적으로 초점을 맞추고 있습니다. MT-bench의 질문 예시는 위에 나와 있습니다. 챗봇 아레나(Chatbot Arena) 인터페이스(interface) 스크린샷(screenshot) ([17]에서 발췌) 챗봇 아레나(Chatbot arena)라고 불리는 두 번째 데이터셋(dataset)은 데이터셋(dataset)이라기보다는 플랫폼(platform)에 가깝습니다. 이 아레나(arena)는 사용자가 두 개의 알 수 없는 LLM과 동시에 상호작용(interact)하고 더 나은 모델을 선택할 수 있도록 하는 크라우드소싱(crowdsourced) 배틀 플랫폼(battle platform)입니다. 위를 참조하십시오. 미리 정의된 질문은 사용되지 않습니다. 대신, 사용자가 자신만의 질문을 제시하고 두 LLM 모두 응답을 제공하여 다양한 사용 사례(use case)에 걸쳐 데이터를 수집할 수 있는 유연성을 제공합니다. 편향(bias)을 피하기 위해, 인간이 선호도를 제공할 때까지 모델의 신원은 유지됩니다. [17]의 저자들은 MT-bench와 챗봇 아레나(Chatbot Arena)에서 많은 양의 인간 피드백(human feedback)을 수집하여 LLM-as-a-Judge와 인간 선호도 간의 상관관계(correlation)를 평가하는 데 사용합니다. 챗봇 아레나(Chatbot Arena) 리더보드(Leaderboard). 아레나(arena)에서 수집된 인간 선호도를 사용하여 Elo 점수(score)를 계산하고 인간 선호도에 기반하여 모델 순위를 매기는 것은 객관성을 높이는 데 기여합니다. 오늘날, 100개 이상의 모델에 대한 150만 개 이상의 쌍별 선호도(pairwise preference)가 챗봇 아레나(Chatbot Arena)에 공유되었으며, 이는 가장 널리 참조되는 LLM 리더보드(leaderboard) 중 하나가 되었으며, 그 영향력은 계속 커지고 있습니다. 이러한 크라우드소싱 평가 방식은 방대한 데이터를 빠르게 확보할 수 있지만, 참여자의 배경이나 주관적 판단에 따른 편향(bias)이 발생할 수 있어 데이터 품질 관리가 중요합니다. 더 자세한 내용은 아래 Nathan Lambert의 게시물을 확인하십시오. 챗봇 아레나(Chatbot Arena) (Interconnects)

**LLM-as-a-Judge의 다양한 설정: 프롬프트 엔지니어링의 중요성**
LLM-as-a-Judge의 다양한 설정 LLM-as-a-Judge의 다양한 프롬프트(prompt) ([17]에서 발췌) 인간 평가와 비교하여, LLM-as-a-Judge는 평가 과정에서 인간 개입의 필요성을 줄여 효율성을 높입니다. 이는 더 빠른 모델 반복(iteration)을 가능하게 하는 간단하고 확장 가능한(scalable) 대안이지만, 그 단점도 고려해야 합니다. LLM-as-a-Judge로 평가를 수행하기 위해 우리가 해야 할 일은 프롬프트(prompt)를 작성하는 것뿐입니다! 그러나 일반적으로 사용되는 몇 가지 다른 프롬프트(prompt) 구조가 있으며, 각각의 장단점이 존재합니다(위에 표시): 쌍별 비교(Pairwise comparison): 심사관에게 질문과 두 가지 모델 응답이 제시되고 더 나은 응답을 식별하도록 요청받습니다. 점수 기반 채점(Pointwise scoring) 2: 심사관에게 질문에 대한 단일 응답이 주어지고 점수를 할당하도록 요청받습니다. 예를 들어, 1부터 5까지의 리커트 척도(Likert scale)를 사용합니다. 참조 기반 채점(Reference-guided scoring): 심사관에게 질문 및 응답과 함께 채점 과정을 돕기 위한 참조 솔루션(reference solution)이 제공됩니다. 이러한 기술 중 어느 것이든 사고 연쇄(chain of thought, CoT) 프롬프팅(prompting)과 결합하여 채점 품질을 향상시킬 수 있으며, 이는 모델의 투명성을 높이는 데 기여합니다. 이를 위해 우리는 심사관의 프롬프트(prompt)에 "점수에 대한 단계별 설명을 작성해 주세요"와 같은 것을 추가하여 간단히 제로샷 CoT 프롬프트(prompt)를 사용할 수 있습니다. 그러나 [16] 3에서 권장하는 바와 같이, LLM에게 점수 이전에 근거(rationale)를 출력하도록 요청해야 합니다(나중에가 아니라). "모델이 생성한 결론은 나중에 생성된 설명에 의해 뒷받침되지 않습니다." - [16]에서 발췌. LLM이 점수와 함께 사람이 읽을 수 있는 근거(rationale)를 출력하도록 하는 것은 쉽고 유용한 설명 가능성(explainability) 기법입니다. 우리는 이러한 설명을 사용하여 모델의 성능과 단점에 대한 더 깊은 이해를 얻을 수 있도록 돕습니다. 아래를 참조하십시오. 이러한 프롬프트 엔지니어링(prompt engineering) 기법은 평가의 신뢰성을 높일 뿐만 아니라, LLM이 단순히 점수를 넘어선 통찰력을 제공하도록 유도합니다.

**최적의 평가 설정 선택하기**
([17]에서 발췌) 어떤 설정을 사용해야 할까요? 이러한 각 채점 전략에는 장단점이 있으며, 최적의 선택은 상황에 따라 달라집니다. 예를 들어, 쌍별 비교(pairwise comparison)는 모델 출력의 모든 조합이 서로 비교되어야 하므로 확장 가능하지 않다는 한계가 있습니다. 그러나 점수 기반 채점(pointwise scoring)은 심사관이 비교적 일관된 내부 채점 메커니즘(mechanism)을 가지고 있을 것으로 기대하기 때문에 덜 안정적인 경향이 있어 주의가 필요합니다. 절대 점수는 쌍별 비교(pairwise comparison)에 비해 훨씬 더 변동될 가능성이 높습니다. 일반적으로 사용해야 할 LLM-as-a-Judge 평가 방식은 우리 애플리케이션(application)의 세부 사항에 따라 달라지므로 신중한 고려가 필요합니다. 항상 비교할 두 모델이 있는 것은 아니므로, 그러한 경우에는 점수 기반 채점(pointwise scoring)이 가장 합리적일 수 있으며, 이는 유연성을 제공합니다. 반대로, 미묘한 차이를 비교해야 할 때는 쌍별 비교가 더 적합할 수 있습니다. 궁극적으로, 평가 목표, 사용 가능한 자원, 그리고 기대하는 결과의 세밀함에 따라 가장 적절한 방법을 선택하는 것이 중요합니다.

**LLM 심사관의 신뢰성: 인간과의 일치도**
([17]에서 발췌) 잘 작동할까요? [17]에서 우리는 GPT-4와 같은 강력한 LLM 심사관이 인간 선호도를 매우 정확하게 측정할 수 있음을 확인했습니다. 실제로 GPT-4는 인간 선호도 점수와 80%의 일치율(agreement rate)을 달성하는 것으로 나타났는데, 이는 매우 고무적인 결과입니다. 이는 인간 주석자(annotator)가 스스로와 가지는 일치율과 유사한 수준입니다. 위를 참조하십시오. LLM 심사관이 인간 선호도를 정확하게 예측하는 이러한 능력은 놀라운 일이 아닙니다. 대부분의 현대 LLM은 인간 선호도 데이터(data)로 광범위하게 미세 조정(finetune)되었으므로, 이러한 결과는 예상 가능합니다. 이러한 높은 일치도는 LLM 심사관이 대규모 모델 개발의 핵심 도구로 자리매김할 수 있음을 시사합니다. 그러나 80%의 일치율이 모든 시나리오에서 충분한지는 여전히 논의의 여지가 있으며, 특히 고위험(high-stakes) 애플리케이션에서는 더 높은 신뢰도가 요구될 수 있습니다.

**편향(Bias)과 그 해결 방안**
편향(Bias) (그리고 이를 피하는 방법…) "우리는 LLM 심사관의 편향(bias)과 한계(limitation)를 식별합니다. 그러나 우리는 이러한 한계에도 불구하고 LLM 심사관과 인간 간의 일치도가 높다는 것을 보여줍니다." - [17]에서 발췌. LLM-as-a-Judge가 인간 선호도를 정확하게 예측할 수 있지만, 이 평가 전략은 완벽하지 않으며 지속적인 개선이 필요합니다. 이는 평가 과정에 여러 새로운 편향(bias) 원인을 도입하므로 이에 대한 인식이 중요합니다. 이미 우리는 LLM이 의심스러운 추론(reasoning) 능력, 프롬프트(prompt)의 사소한 변경에 대한 민감성, 장황한 출력(verbose output)을 생성하는 경향과 같은 수많은 한계(limitation)를 가지고 있음을 알고 있으며, 이는 모델의 신뢰성을 떨어뜨릴 수 있습니다. 이러한 약점 중 다수는 LLM-as-a-Judge 평가 내에서 상응하는 편향(bias)으로 이어집니다. 위치 편향(Position bias): 심사관은 프롬프트(prompt) 내 위치에 따라 출력을 선호할 수 있습니다(예: 쌍별 프롬프트(pairwise prompt)에서 첫 번째 응답). 장황함 편향(Verbosity bias): 심사관은 출력 길이에 따라 더 나은 점수를 할당할 수 있습니다(즉, 더 긴 응답이 더 높은 점수를 받습니다). 자기 강화 편향(Self-enhancement bias): 심사관은 스스로 생성한 응답을 선호하는 경향이 있습니다(예: GPT-4는 자체 출력에 높은 점수를 할당합니다). 위에 설명된 편향(bias) 원인 외에도, LLM 심사관은 스스로 답변하기 어려운 질문(예: 복잡한 추론(reasoning) 및 수학 질문)에 대한 응답을 채점하는 데 어려움을 겪는 경향이 있어, 특정 도메인에서는 인간의 개입이 필수적입니다. 또한 심사관은 컨텍스트(context)의 잘못된 정보에 쉽게 오도될 수 있습니다 [18]. 채점되는 응답 중 하나가 잘못된 경우, 심사관은 이 컨텍스트(context)에 의해 오도되어 부정확한 점수를 출력할 수 있으므로, 교차 검증이 필요합니다. 이러한 편향은 단순히 평가의 정확성을 떨어뜨리는 것을 넘어, AI 시스템의 공정성과 신뢰성에 심각한 영향을 미칠 수 있습니다.

**편향에 대한 심층 분석: 지속적인 연구 노력**
([17]에서 발췌) 편향(bias)에 대한 심층 분석은 필수적입니다. [17]에 설명된 편향(bias) 원인 외에도, LLM-as-a-judge 평가에서의 편향(bias)을 심층적으로 분석한 다른 많은 연구들이 활발히 진행되고 있습니다. 이 중 일부는 이 개요에서 나중에 살펴볼 것입니다. 이러한 연구 목록은 쉬운 참조 및 추가 읽기를 위해 아래에 제공되는 자료들을 참고하시기 바랍니다. AI 시스템의 편향은 사회적 불평등을 증폭시킬 수 있는 잠재력을 가지고 있으므로, 이러한 연구는 기술적 개선을 넘어 윤리적, 사회적 책임과 직결됩니다.
*   인간 또는 LLM이 심사관인가? 판단 편향(Judgement Biases)에 대한 연구 [ 링크 ]
*   대규모 언어 모델을 위한 평가 편향(Evaluation Biases) [ 링크 ]
*   평가자로서의 대규모 언어 모델의 인지 편향(Cognitive Biases) [ 링크 ]
*   대규모 언어 모델은 일관성 없고 편향된 평가자이다 [ 링크 ]
*   대규모 언어 모델은 아직 인간 수준의 평가자가 아니다 [ 링크 ]

**편향을 줄이는 실질적인 방법**
편향(bias)을 어떻게 줄일 수 있을까요? LLM-as-a-Judge 평가 결과에 대한 편향(bias)의 영향을 줄이기 위해 사용할 수 있는 몇 가지 기술이 지속적으로 개발되고 있습니다. 프롬프트(prompt) 내 모델 출력의 위치를 무작위화하고, 여러 점수를 생성하며, 다른 위치를 가진 점수들의 평균을 취하는 것은 효과적인 편향 완화 전략입니다—이를 위치 전환 기법(position switching trick)이라고 부를 것입니다. 점수의 자연스러운 분포를 보여주고 심사관의 내부 채점 메커니즘(mechanism)을 보정(calibrate)하는 데 도움이 되는 퓨샷(few-shot) 예시를 제공하는 것은 중요합니다. 평가 과정에서 심사관을 위한 참조로 프롬프트(prompt) 내에 어려운 수학 및 추론(reasoning) 질문에 대한 정답을 제공합니다. 자기 강화 편향(self-enhancement bias)의 영향을 줄이기 위해 여러 다른 모델(예: Claude, Gemini 및 GPT-4)을 심사관으로 사용하는 것은 다각적인 관점을 제공합니다. 이러한 기술들이 유용하더라도, LLM-as-a-judge는 모든 지표(metric)와 마찬가지로 결함 있는 지표(metric)이며 결코 완벽하지 않을 것이므로, 그 한계를 인정해야 합니다. 따라서 우리는 항상 이러한 편향(bias)을 인지하고 그것이 우리의 분석에 어떤 영향을 미치는지 고려해야 합니다. 평가되는 모델, 측정하려는 대상, 평가가 어떻게 설정되었는지, 그리고 기본 심사관이 이 평가의 결과를 어떤 방식으로 왜곡할 수 있는지 항상 비판적으로 사고해야 합니다. 이러한 접근 방식은 AI 평가의 투명성과 공정성을 높이는 데 필수적입니다.

**LLM 평가의 초기 연구: GPT-4의 등장**
LLM 평가에 대한 초기 연구는 [17]에서 LLM-as-a-Judge가 제안되고 분석되기 이전에 이미 시작되었습니다. 이러한 연구는 텍스트 품질을 평가할 만큼 충분히 강력한 최초의 LLM이었던 GPT-4의 제안과 함께 새로운 국면을 맞았습니다. 우리가 보게 되겠지만, 이 접근 방식은 사용 용이성, 일반성, 효과성 덕분에 빠르게 인기를 얻고 커뮤니티(community)에 퍼지기 시작했지만, 동시에 비판도 받았습니다. 인공 일반 지능(Artificial General Intelligence)의 불꽃: GPT-4를 사용한 초기 실험 [1] LLM 기반 평가를 가능하게 하려면, 먼저 다른 모델의 출력을 안정적으로 평가할 수 있는 충분히 강력한 LLM에 대한 접근이 선행되어야 합니다. 이전 모델들도 인상적이었지만, GPT-4가 제안되기 전까지는 이러한 수준의 평가 모델에 접근할 수 없었습니다. 그러나 이 모델이 제안된 직후 연구자들은 LLM 평가자의 실현 가능성을 탐구하기 시작했고, 이는 중요한 전환점이 되었습니다. GPT-4는 단순히 텍스트 생성 능력을 넘어, 복잡한 추론과 지시 따르기 능력을 보여주며 AI 평가의 새로운 가능성을 열었습니다.

**GPT-4, 그 역량의 깊이와 논쟁**
([1]에서 발췌) GPT-4는 얼마나 뛰어난가? GPT-4를 평가자로 사용하는 것을 탐구한 첫 번째 연구 [1]는 모델 출시 후 빠르게 등장했습니다. 이 연구는 평가에 중점을 두지 않았습니다. 오히려 이 연구는 GPT-4의 역량을 탐구하는 더 일반적인 목표를 가지고 있었으며, 그 결과 (놀랍게도) 광범위한 주제를 다루는 155페이지 분량의 분석이 나왔습니다. 코딩 및 수학 문제 해결 능력은 LLM의 핵심 역량 중 하나입니다. 도구 사용 및 인간과의 상호작용(interact); 예: 마음 이론(theory of mind) 문제, 또는 인간에게 출력을 설명하는 모델의 능력은 신뢰성 확보에 중요합니다. TikZ를 사용하여 기본적인 그림/사진 그리기, 유용한 플롯(plot) 생성, 그리고 더 일반적인 데이터 분석 수행. 수학 정리(theorem) 증명, 또는 증명의 모든 줄을 운율에 맞춰 작성하면서 이를 수행하는 것. 위를 참조하십시오. 이 분석의 결론은 GPT-4가 사실상 모든 고려된 작업에서 탁월하며, 그 다재다능함이 입증되었습니다. 실제로 저자들은 GPT-4의 출력이 많은 경우 인간의 것과 구별할 수 없거나(또는 더 낫다) 심지어 GPT-4가 인공 일반 지능(AGI)을 향한 중요한 진전이라고 말하기까지 했지만, 이는 여전히 논쟁의 여지가 있습니다. 아래를 참조하십시오. "GPT-4 역량의 일반성과… 인간 수준 또는 그 이상의 광범위한 작업에서의 성능 조합은 GPT-4가 AGI를 향한 중요한 진전이라고 말하는 것을 편안하게 만듭니다." - [1]에서 발췌. 저자들이 GPT-4가 (일반) 지능의 징후를 보여준다고 주장한 것은—그리고 계속해서—매우 논란이 많았지만, LLM 역량과 AGI를 향한 진전에 대한 이견은 종종 이러한 개념에 대한 엄격한 정의의 부족(또는 차이)으로 귀결됩니다. 먼저 정의하지 않고 지능을 어떻게 측정할 수 있을까요 5? 다행히도 이 게시물의 목적을 위해 AGI를 정의하는 것에 대해 걱정할 필요는 없습니다. 우리가 알아야 할 것은 GPT-4가 매우 유능한 모델이며, 이러한 역량이 다른 LLM의 출력을 평가하기 위해 LLM을 사용하는 데 많은 가능성을 열어준다는 점에서 그 의미가 큽니다.

**평가자로서의 GPT-4: 초기 시도와 한계**
([1]에서 발췌) 평가자로서의 GPT-4의 역할은 중요합니다. [1]의 저자들은 GPT-4를 심사관으로 사용하는 것을 탐구한 최초의 연구자들이었지만, 그들의 분석은 사실 상당히 간략합니다(즉, 전체적으로 1페이지 미만)! 기존 평가 지표(evaluation metric)가 진술의 유사성을 판단하는 데 있어서 가지는 한계를 다룬 후, 저자들은 위에 표시된 프롬프트(prompt)를 사용하여 모델 응답이 참조 답변과 얼마나 유사한지 판단하는 GPT-4의 능력을 평가합니다. 구체적으로, GPT-4는 모델의 응답이 참조 답변과 더 유사한지 아니면 GPT-3로 생성된 답변과 더 유사한지 결정하는 작업을 맡습니다. 진술에 대한 두 가지 응답이 심사관에게 제공되며, 심사관은 원래 진술을 더 잘 반영하는 옵션을 식별합니다. "[GPT-4]는 한 쌍의 답변 중 어떤 답변이 황금 답변(gold answer)에 더 가까운지 결정할 수 있으며, 이 결정은 동일한 작업을 수행하는 인간과 합리적으로 일치합니다." - [1]에서 발췌. 이 분석을 통해 우리는 GPT-4가 ROUGE 또는 BLEU와 같은 간단한 지표(metric)보다 진술 간의 의미론적 유사성(semantic similarity)을 훨씬 더 잘 판단할 수 있음을 보여줍니다. 이러한 평가의 품질을 향상시키기 위해 저자들은 GPT-4에게 선호하는 출력을 식별하기 전에 각 응답의 장단점을 나열하도록 요청합니다. GPT-4의 판단과 인간의 판단을 비교할 때, 우리는 상당한 차이를 발견합니다. 예를 들어, GPT-4는 인간 평가자의 경우 47.61%인 반면, 87.76%의 경우에서 GPT-4가 생성한 응답을 선호하는 경향을 보였습니다. 아래 표를 참조하십시오. ([1]에서 발췌) GPT-4와 인간 평가자 간의 일치도(agreement level)는 낮습니다—50%를 약간 넘는 수준으로 나타났습니다—그리고 우리는 이미 [1]에서 편향(bias)의 명확한 징후를 봅니다. 예를 들어, 더 긴 응답은 GPT-4에 의해 더 좋게 평가되는 경향이 있습니다. 이러한 불일치(lack of alignment)는 평가 설정의 차이 때문일 수 있습니다. GPT-4는 두 응답 중 승자를 선택하도록 강요받는 반면, 인간은 무승부를 선택할 수 있습니다. 그러나 GPT-4와 인간 간의 일치도는 여전히 놀랍도록 낮으며, [1]의 저자들은 GPT-4의 평가 역량을 보정(calibrate)하기 위해 더 많은 연구가 필요하다고 결론 내렸으며, 이는 현재도 진행 중입니다. 이러한 초기 연구는 LLM 기반 평가의 잠재력과 함께 내재된 복잡성과 한계를 동시에 보여주었습니다.

**Vicuna: 오픈 소스 챗봇의 평가 혁신**
Vicuna: 90%* ChatGPT 품질로 GPT-4를 감동시킨 오픈 소스(open-source) 챗봇(chatbot) [2] "챗봇(chatbot)을 평가하는 것은 결코 간단한 작업이 아니며, 그 복잡성은 계속 증가하고 있습니다. GPT-4의 최근 발전으로, 우리는 GPT-4의 역량이 자동화된 평가 프레임워크(framework)를 가능하게 할 수 있는 인간과 유사한 수준에 도달했는지에 대한 논의는 계속됩니다." - [2]에서 발췌. Vicuna는 ShareGPT에서 수집된 ChatGPT와의 사용자 대화 세트에 대해 LLaMA-13B를 미세 조정(finetune)하여 생성된 오픈 소스(open-source) 챗봇(chatbot)으로, 커뮤니티에 큰 영향을 미쳤습니다. 우리는 이전 개요에서 Vicuna의 세부 사항을 다루었습니다. 그러나 Vicuna는 저자들이 이 모델의 출력 품질을 주로 GPT-4로 자동으로 평가하기로 선택했다는 사실 때문에 이 개요와 관련이 있습니다. 아래를 참조하십시오. ([2]에서 발췌) 사실 Vicuna는 당시 반발 6을 받았던 이러한 LLM 기반 평가를 수행한 최초의 연구 중 하나였습니다. 그러나 [2]에서 평가 목적으로 GPT-4를 사용한 것은 이 기술에 대한 분석의 물결을 일으켰는데, 그 이유는 다음과 같습니다. 평가 결과가 비교적 일관되고 유망해 보였지만, 숨겨진 편향에 대한 우려도 있었습니다! GPT-4를 평가자로 사용하는 것은 어떤 작업에도 적용할 수 있는 참조 불필요(reference-free) 자동 평가 전략입니다(즉, 매우 일반적이고 간단합니다). GPT-4는 평가와 함께 근거(rationale)를 출력할 수 있으며, 이는 평가 결과의 인간 해석 가능성(interpretability)을 향상시켜 신뢰도를 높입니다. Vicuna는 오픈 소스 모델이 상업용 모델에 근접할 수 있음을 보여주며, AI 연구의 민주화에 중요한 이정표를 제시했습니다.

**Vicuna 평가 설정의 세밀한 분석**
평가 설정. Vicuna를 테스트하는 데 사용된 질문은 8가지 범주(category)에 걸쳐 있었지만, 이는 모델의 모든 역량을 포괄하지는 못했습니다. 총 80개의 질문이 있으며, GPT-4의 도움을 받아 작성되었으며, 이는 AI가 AI를 평가하는 흥미로운 순환을 보여줍니다. 저자들은 GPT-4가 신중한 프롬프트 엔지니어링(prompt engineering)을 통해 최첨단 챗봇(chatbot)에 대한 도전적인 질문을 생성할 수 있음을 관찰합니다. 이러한 질문에 대한 다양한 챗봇(chatbot)의 답변을 평가하기 위해 각 모델의 출력을 수집하고 GPT-4에게 유용성, 관련성, 정확성, 세부 사항 측면에서 생성된 출력을 평가하도록 요청하는 것은 다각적인 평가를 가능하게 합니다. 놀랍도록 간단한 평가에 사용된 프롬프트(prompt)는 여기에서 볼 수 있습니다. 그러한 평가의 어려움 때문에 코딩 및 수학 작업을 평가하기 위해 별도의 더 구체적인 프롬프트(prompt)가 사용됩니다. 아래를 참조하십시오. GPT-4 평가를 위한 프롬프트(prompt) GPT-4는 다른 모델의 두 응답 품질을 1부터 10까지의 척도(scale)로 평가하고 이 점수에 대한 설명을 제공하도록 요청받습니다. CoT 프롬프팅(prompting) 7을 통해 이러한 설명을 생성하는 것은 GPT-4 평가의 정확도를 향상시키는 경향이 있지만, 과도한 의존은 위험할 수 있습니다. 이는 평가를 설명하는 사람이 읽을 수 있는 근거(rationale)를 제공합니다. 이러한 프롬프트(prompt)에 대해 우리가 할 수 있는 몇 가지 다른 유용한 관찰이 있습니다. 일반 프롬프트(prompt)는 GPT-4에게 점수를 생성할 때 위치 편향(positional bias)을 포함한 편향(bias)을 피하도록 요청하며, 이는 [2]의 저자들이 위치 편향(positional bias) 문제에 직면했음을 보여주며, 이는 일반적인 LLM 평가의 과제입니다. 후속 연구 [8]는 Vicuna의 평가 전략에 강한 위치 편향(positional bias)이 있음을 확인했지만, 이 문제는 위치 전환(position switching)을 통해 해결될 수 있으므로, 적절한 전략 적용이 중요합니다. 모든 프롬프트(prompt)는 출력 형식(output format)을 명시적으로 지정하여 응답에서 평가를 쉽고 자동으로 구문 분석(parse)할 수 있도록 합니다. 이러한 접근 방식은 GPT-4와 같은 강력한 지시 따르기(instruction following) 모델로만 가능하며, 이는 모델의 발전 수준을 보여줍니다. 코딩 및 수학 평가를 위한 프롬프트(prompt)는 일반 질문에 사용된 프롬프트(prompt)에 비해 더 상세하며, 그러한 질문에 대한 평가 품질을 향상시키기 위해 더 심층적이고 문제별 세부 사항을 도입합니다. 코딩 및 수학 프롬프트(prompt)는 제공된 솔루션(solution)을 비판하거나 평가를 제공하기 전에 문제를 직접 해결하도록 GPT-4에게 요청하는 것과 같이 더 나은 평가를 유도하기 위해 몇 가지 다른 기법을 사용하며, 이는 프롬프트 엔지니어링의 중요성을 강조합니다.

**Vicuna의 독특한 평가 접근 방식과 그 시사점**
Vicuna의 독특한 접근 방식. 각 프롬프트(prompt)에서 두 응답을 평가하는 것은 모델 간의 더 나은 상대적 비교를 가능하게 하는 효과적인 방법입니다. 우리는 GPT-4에게 주어진 질문에 대해 두 모델 중 어느 모델이 더 나은 응답을 제공하는지 쉽게 설명하도록 요청할 수 있습니다. 그러나 Vicuna의 설정은 표준 쌍별 비교(pairwise comparison)와 약간 다르다는 점에 유의하는 것이 중요하며, 이는 새로운 관점을 제시합니다. 모델에게 선호하는 응답을 요청하는 대신, 모델은 각 예시에 점수를 할당하도록 프롬프트(prompt)되며, 선호되는 예시는 이 점수를 기반으로 결정되어 유연성을 더합니다. 이는 여러 논문에서 사용된 또 다른 유효한 LLM-as-a-Judge 설정입니다. 이것이 잘 작동할까요? GPT-4가 코딩 및 수학 질문 채점에 어려움을 겪음에도 불구하고, 이러한 평가 결과는 비교적 일관되며, 그 잠재력을 보여줍니다! 이러한 평가를 사용하여 저자들은 Vicuna가 90%의 질문에서 다른 오픈 소스(open-source) 모델보다 선호되며, 45%의 경우에서 ChatGPT보다 낫거나 동등하다고 평가된다는 것을 관찰합니다. 아래를 참조하십시오. 그러나 후속 연구 [8]는 이러한 평가가 더 긴 출력에 편향(bias)되어 있으며 인간 선호도와 상관관계가 낮다는 것을 밝히며, 이는 LLM 평가의 한계를 명확히 합니다. 따라서 이러한 결과를 해석할 때 우리는 이러한 편향(bias)과 단점을 인지해야 합니다. GPT-4의 응답 비교 ([2]에서 발췌) 더 구체적으로, 후속 연구에서 우리는 Vicuna와 같은 모방 모델(imitation model)이 ChatGPT와 같은 강력한 모델의 스타일(style)과 일치하지만, 사실성(factuality)과 지식 기반(knowledge base)이 부족하다는 것을 알 수 있습니다. 그럼에도 불구하고, [2]의 분석은 i) 오픈 소스(open-source) 모델의 응답 스타일(style)과 지시 따르기(instruction-following) 능력이 더 강력한 모델의 출력으로 미세 조정(finetune)함으로써 향상될 수 있으며, 이는 모델 개발의 새로운 방향을 제시합니다. ii) LLM 기반 평가의 실현 가능성이 더 명확하게 입증된다는 점에서 유용하며, 이는 연구의 토대가 됩니다. "이 제안된 평가 프레임워크(framework)가 챗봇(chatbot) 평가의 잠재력을 보여주지만, 아직 엄격하거나 성숙한 접근 방식은 아닙니다… 챗봇(chatbot)을 위한 포괄적인 평가 시스템(system)을 개발하는 것은 여전히 미해결 과제이며, 이는 지속적인 연구를 요구합니다." - [2]에서 발췌.

**평가 기술의 발전 방향: 미래를 향하여**
앞으로. [2]에서 GPT-4 평가의 초기 단계적 특성에도 불구하고, 이 연구는 LLM 기반 평가의 후속 분석을 위한 강력한 기반을 마련하여 연구의 지평을 넓혔습니다. 후속 연구는 LLM-as-a-Judge 기술의 강점과 약점에 대한 다양하고 유용한 분석을 제공하지만, 신뢰할 수 있는 점수를 생성하는 데 사용되는 프롬프트(prompt)는 [2]에서 본 것과 (상대적으로) 유사한 경향이 있어, 그 효율성이 입증되었습니다! 사실, LLM-as-a-Judge 출판물 자체 [17]는 Vicuna와 동일한 저자들이 작성했으며 [2]에서 제안된 많은 기술을 적극적으로 활용합니다. 또한 Vicuna를 평가하는 데 사용된 80개의 질문 세트는 다른 논문에서도 널리 사용되어 벤치마크 역할을 합니다. 이는 AI 평가 연구의 재현성(reproducibility)과 표준화에 기여하며, 더 나아가 LLM 기반 평가가 단순한 실험을 넘어 실제 개발 프로세스에 통합될 수 있는 가능성을 보여줍니다.

**AlpacaEval: 효율적인 지시 따르기 모델 평가**
AlpacaEval: 지시 따르기(Instruction-Following) 모델의 자동 평가자 [8] 2023년 중반에 처음 제안된 AlpacaEval [8]은 지시 따르기(instruction-following) 언어 모델을 위한 가장 인기 있는 LLM 기반의 자동화된 평가 지표(evaluation metric) (및 리더보드(leaderboard)) 중 하나로 자리매김했습니다. AlpacaFarm [9]을 기반으로 하는 평가 전략—LLM 평가자를 사용하여 RLHF(Reinforcement Learning from Human Feedback) 스타일의 쌍별 선호도 레이블(pairwise preference label) 8 생성을 자동화하는 시뮬레이터(simulator)—은 간단하고 비서 스타일(assistant-style) 작업의 포괄적인 세트에 걸쳐 있는 805개의 고정된 지시 세트를 사용합니다. 여기를 참조하십시오. 각 지시에 대해 우리는 두 LLM(기준 모델(baseline model)과 평가 대상 모델)으로 출력을 생성합니다. 그런 다음 LLM 평가자가 각 모델 출력의 품질을 평가하는 데 사용되어(즉, 쌍별 설정(pairwise setup)), 두 모델 출력 간의 승률(win-rate)을 계산할 수 있게 하여 객관적인 비교를 가능하게 합니다. RLHF는 인간의 피드백을 통해 모델을 강화 학습시키는 방식으로, AlpacaEval은 이러한 인간 선호도 학습 과정을 자동화하여 효율성을 극대화합니다.

**AlpacaEval의 유용성: 빠르고 정확한 평가**
([10]에서 발췌) 이것이 왜 유용할까요? AlpacaEval의 목표는 인간 선호도와 높은 상관관계(correlation)를 가지는 빠르고 저렴한 자동 평가 파이프라인(pipeline)을 구축하는 데 있습니다. AlpacaEval의 현재 반복(iteration)은 3분 이내에 실행되며, 10달러 9 미만의 비용이 들고, 인간 평가(챗봇 아레나(Chatbot Arena)에서 가져옴)와 0.98의 스피어만 상관관계(Spearman correlation)를 가집니다. 이는 높은 신뢰도를 의미합니다. 위를 참조하십시오. 이에 비해 인간 평가는 잡음과 불일치에 취약하고, 훨씬 더 비싸며, 몇 주간의 주석(annotation) 시간이 필요할 수 있다는 단점을 가집니다. AlpacaEval이 매우 효율적이기 때문에, 이 지표(metric)는 모델 개발에 완벽한 솔루션을 제공합니다. 이는 간단한 지시 따르기(instruction-following) 작업에 대한 인간 평가를 위한 신뢰할 수 있는 대리(proxy)를 빠르고 저렴하게 계산할 수 있도록 제공하여 개발 속도를 가속화합니다. 스피어만 상관관계(Spearman correlation)가 0.98이라는 것은 AlpacaEval의 결과가 인간의 판단과 거의 완벽하게 일치한다는 것을 의미하며, 이는 자동화된 평가 시스템의 잠재력을 강력하게 보여줍니다.

**AlpacaEval 평가자의 작동 원리**
평가자는 어떻게 작동할까요? AlpacaEval에서 평가자를 위해 사용된 프롬프트(prompt)는 핵심적인 역할을 합니다. 이 프롬프트(prompt)는 대부분의 독점적인 채팅 완성 API(chat completion API)에서 사용되는 입력 스타일(style)과 일치하며 다중 턴 대화(multi-turn conversation) 내에서 역할과 메시지를 구분하는 데 사용되는 채팅 템플릿(chat template) 구조를 활용하여 효율성을 높입니다. 모든 지시에 대해 아래에 표시된 대로 한 쌍의 응답이 평가자에게 전달되며, 우리는 선호되는 출력—이진 응답(binary response) 또는 LLM에서 각 옵션의 로그 확률(logprobs)—을 응답으로 받습니다. 이 응답은 데이터셋(dataset) 내 주어진 지시에 대해 평가 대상 모델의 응답이 기준 모델(baseline model)의 응답보다 더 나을 확률을 나타내어 정량적 비교를 가능하게 합니다. 전체 데이터셋(dataset)에 걸쳐 이러한 확률의 평균을 취함으로써, 우리는 모델 출력이 기준 모델(baseline model)보다 선호되는 시간 비율을 측정하는 승률(win-rate)을 계산할 수 있어 성능 지표로 활용됩니다. AlpacaEval의 평가자 프롬프트(prompt) (출처) 평가자의 품질은 여기에서 확인할 수 있는 2.5K 인간 평가 세트와의 일치도를 측정하여 검증됩니다. 그러나 특정 사용 사례(use case)에 가장 적합한 평가자를 선택할 때 비용 및 지연 시간(latency)과 같은 여러 다른 요소도 관련이 있으므로, 종합적인 고려가 필요합니다. 원래 AlpacaEval은 데이터셋(dataset) 내 각 인스턴스(instance)에 대해 온도가 0인 단일 선호도 응답을 생성했습니다. 그러나 자동화된 선호도 주석(annotation)의 품질은 후속 반복(iteration)에서 지속적으로 향상되었습니다. 프롬프트(prompt) 내 모델 출력의 위치를 무작위화합니다(또는 모델 출력의 각 가능한 위치에 대해 여러 선호도 점수를 샘플링(sample)합니다). 이진 선호도 응답을 생성하는 대신 각 응답의 로그 확률(logprobs) 11을 측정하여 평가의 정밀도를 높입니다. 더 나은 모델(GPT-4-Turbo)을 평가자로 사용하는 것은 평가 품질을 직접적으로 향상시킵니다. 저자들은 또한 핵심 평가자의 프롬프트(prompt)를 수정하고 단순화하여 지시를 단축하고 응답에서 단일 토큰(token)만 출력하도록 하여 효율성을 극대화했습니다. 아래를 참조하십시오. 평가자 프롬프트(prompt) in AlpacaEval-2.0 (출처)

**길이 편향(length bias) 완화: 공정한 평가를 위한 노력**
길이 편향(length bias) 완화. 우리가 보았듯이, LLM을 평가자로 사용하는 것은 평가 과정에 여러 미묘한 편향(bias) 원인을 도입할 수 있으므로, 이를 주의 깊게 다루어야 합니다. 우리는 이러한 편향(bias)을 인지하고 이를 제거하거나 설명하기 위해 최선을 다해야 할 책임이 있습니다. "모든 모델의 출력이 기준 모델(baseline model)의 출력과 길이가 같았다면 AlpacaEval 지표(metric)는 어떠했을까요?" - [10]에서 발췌. LLM 평가자의 알려져 있고 널리 퍼진 편향(bias) 중 하나는 더 긴 출력(즉, 장황함 편향(verbosity bias))을 선호하는 경향은 흔히 발견되는 문제입니다. 특정 독점 LLM(예: GPT-4 또는 GPT-4-Turbo)은 더 짧은 출력보다 더 긴 출력을 선호하는 경향이 있습니다. 결과적으로 AlpacaEval은 고정되거나, 비교 가능하거나, 심지어 더 나쁜 콘텐츠(content) 품질이 주어졌을 때 더 긴 출력을 더 짧은 출력보다 더 좋게 평가할 수 있다는 점은 큰 문제입니다. ([10]에서 발췌) 이 편향(bias)에 맞서기 위해 연구자들은 AlpacaEval 지표(metric) [10]를 간단한 회귀 기반(regression-based) 편향 제거(debiasing) 프로세스(process)로 확장하여 평가의 공정성을 높였습니다. 특히, 세 가지 속성(attribute)(위에 표시)을 입력으로 받는 선형 회귀 모델(linear regression model)이 훈련됩니다. i) 모델 12, ii) 지시 난이도, iii) 정규화된 출력 길이. 이 모델이 훈련되면, 출력 품질과 허위 상관관계(spurious correlation)가 있다고 여겨지는 항들의 기여도를 "0으로 만들어서" 진정한 품질 점수만 남길 수 있어 평가의 신뢰도를 높입니다. [10]의 경우, 우리는 회귀에서 길이 항을 제거하고 평소대로 승률(win-rate)을 계산하여 길이 제어 AlpacaEval 점수를 산출하여 편향 없는 평가를 가능하게 합니다. ([10]에서 발췌) 현재 공개 리더보드(leaderboard)에서 사용되는 길이 제어 AlpacaEval은 [10]에서 표준 지표(metric)보다 조작하기 어렵다(less game-able)는 것이 밝혀졌으며, 이는 중요한 진전입니다. 다시 말해, 우리는 모델에게 더 장황하거나 덜 장황하게 요청하는 것만으로는 길이 제어 AlpacaEval의 결과를 크게 변경할 수 없습니다. 위 표에 표시된 바와 같이, AlpacaEval의 승률(win-rate)은 모델의 장황함에 따라 급격하게 변할 수 있음을 보여줍니다. AlpacaEval의 길이 제어 버전은 인간 평가와 더 잘 상관관계(correlate)를 이루는 것으로 나타났으며, 챗봇 아레나(Chatbot arena)와의 AlpacaEval 스피어만 상관관계(Spearman correlation)를 0.94에서 0.98로 향상시켜 그 유효성을 입증했습니다.

**LLM 기반 평가의 확산과 영향**
LLM 기반 평가의 다른 초기 사용 사례들이 Vicuna가 제안된 직후 등장했습니다. GPT-4를 평가자로 사용하는 것이 점점 더 보편화되어 표준으로 자리 잡았습니다. 이때 LLM 기반 평가의 신뢰성을 증명하기 위한 분석은 거의 이루어지지 않았지만, 그 필요성은 명확했습니다. 그러나 이러한 평가 방식이 매우 인기를 얻게 된 몇 가지 주목할 만한 요인이 있습니다. 구현이 쉽습니다—단지 프롬프트(prompt)와 API 호출(API call)만 있으면 되는 간편함이 큰 장점입니다! LLM 출력의 개방형(open-ended) 특성으로 인해 전통적/자동 지표(metric)(예: ROUGE 또는 BLEU) 13를 사용한 평가가 매우 어렵다는 한계가 있습니다. LLM 평가를 위한 정답(ground truth)의 원천인 인간 평가는 잡음이 많고, 비용이 많이 들며, 시간이 많이 소요되는 단점이 있습니다. 연구 커뮤니티(community)는 i) 광범위한 작업에서 성능을 안정적으로 측정할 수 있고 ii) 더 빠르게 실험하고 반복(iterate)할 수 있게 하는 더 접근 가능한 평가 전략이 절실히 요구되었습니다. 다음 몇몇 논문에서 보게 되겠지만, LLM 기반 평가는 연구자들에게 빠른 모델 반복(iteration)을 가능하게 하는 자동적이고 참조 불필요(reference-free)한 지표(metric)를 제공함으로써 이 간극을 빠르게 메우는 데 성공했습니다. 이는 AI 개발 속도를 획기적으로 가속화하고, 더 많은 연구자들이 혁신적인 모델을 실험할 수 있는 기반을 마련했습니다.

**LIMA: 데이터 효율적인 정렬의 가능성**
([3]에서 발췌) LIMA: 정렬(Alignment)에는 적을수록 좋다. LIMA [3]는 제한된 양의 데이터(data)로 지도 미세 조정(supervised finetuning)을 사용하여 사전 훈련된 언어 모델(pretrained language model)(예: LLaMA)을 정렬(align)하는 우리의 능력을 탐구합니다. 흥미롭게도, 저자들은 단 1,000개의 큐레이션(curated)된 미세 조정(finetuning) 예시만으로도 놀랍도록 강력한 성능을 달성하기에 충분하다는 것을 관찰했으며, 이는 데이터 효율성의 중요성을 강조합니다. 이러한 결과는 LLM이 사전 훈련(pretraining) 중에 대부분의 지식을 학습하는 반면, 미세 조정(finetuning)은 모델 출력의 형식(format)을 최적화한다는 것을 시사하며, 이는 모델의 활용도를 높입니다. 이 현상은 표면적 정렬 가설(Superficial Alignment Hypothesis)이라고 불리며, 이는 AI 학습에 대한 새로운 이해를 제공합니다. 아래를 참조하십시오. "우리는 표면적 정렬 가설(Superficial Alignment Hypothesis)을 다음과 같이 정의합니다. 모델의 지식과 역량은 거의 전적으로 사전 훈련(pretraining) 중에 학습되며, 정렬(alignment)은 어떤 형식의 하위 분포(subdistribution)를 사용해야 하는지 가르칩니다." - [3]에서 발췌. [3]에서는 인간 및 LLM 기반 평가가 모두 사용됩니다. 각 프롬프트(prompt)에 대해 테스트되는 각 모델에 대한 단일 응답이 생성됩니다. 그런 다음 인간은 LIMA의 출력(익명으로)을 모든 기준 모델(baseline model)과 비교하여 선호하는 응답을 선택하도록 요청받습니다. 아래를 참조하십시오. 이 평가 과정은 다음을 통해 자동화될 수 있습니다. GPT-4에 동일한 정확한 프롬프트(prompt)를 제공합니다. 모델에게 선호하는 응답을 선택하도록 요청합니다. 인간 및 모델 기반 평가 모두에서 LIMA는 훨씬 적은 데이터(data)로 미세 조정(finetune)되었음에도 불구하고 Alpaca와 같은 이전 오픈 모델보다 뛰어난 성능을 보이는 것으로 나타나 주목받았습니다. LIMA는 또한 GPT-3.5보다 뛰어난 성능을 보이며, 특히 테스트 프롬프트(prompt)의 34%에서 43%에 해당하는 상당수의 테스트 프롬프트(prompt)에서 GPT-4의 성능과 일치하거나 능가하는 결과를 보여주어 그 잠재력을 입증합니다.

**Guanaco: Q-LoRA를 통한 LLM 미세 조정의 혁신**
([3]에서 발췌) Guanaco. [4]의 저자들은 양자화된 저랭크 적응(quantized low-rank adaptation, Q-LoRA)을 제안하는데, 이는 상용 하드웨어(commodity hardware)(즉, 메모리가 적은 소비자 GPU(consumer GPU))에서 LLM 미세 조정(finetune)을 훨씬 쉽게 만드는 매개변수 효율적인(parameter-efficient) 훈련 전략으로 각광받고 있습니다. Q-LoRA에 대한 자세한 내용은 이 개요를 참조하십시오. Q-LoRA로 LLM을 훈련하는 주요 이점은 메모리 소비 감소이며, 이는 자원 효율적인 AI 개발에 기여합니다. [4]에서는 650억 개의 매개변수(parameter)를 가진 LLM이 단일 48Gb GPU로 Q-LoRA를 사용하여 미세 조정(finetune)될 수 있음을 보여주어, 접근성을 크게 향상시켰습니다! ([4]에서 발췌) [4]의 저자들은 Q-LoRA를 사용하여 챗봇(chatbot) 스타일의 오픈 LLM인 Guanaco 제품군을 훈련합니다. 이 모델들은 인간과 GPT-4를 모두 사용하여 평가됩니다. 흥미롭게도, GPT-4는 의미 있고 신뢰할 수 있는 성능 지표(performance metric)를 제공하는 반면, 기존 벤치마크(benchmark)는 챗봇(chatbot) 성능에 대한 정확한 측정을 제공하지 않는다는 한계를 보였습니다. "GPT-4 평가는 인간 평가에 대한 저렴하고 합리적인 대안입니다… 우리는 현재 챗봇(chatbot) 벤치마크(benchmark)가 챗봇(chatbot)의 성능 수준을 정확하게 평가하기에 신뢰할 수 없다는 것을 발견했습니다." - [4]에서 발췌. [4]에서는 두 가지 스타일의 LLM 기반 평가가 사용됩니다. 첫 번째 설정에서 GPT-4는 ChatGPT와 다른 모델의 응답으로 프롬프트(prompt)되고 다음을 요청받습니다. 두 응답 모두에 [1, 10] 범위의 점수를 할당합니다. 이 점수에 대한 설명을 제공합니다. 특히, 이 설정은 Vicuna [2]에서 사용된 자동 평가 전략과 정확히 일치합니다. 여기에서 모델의 성능은 ChatGPT에 대한 상대적인 성능으로 보고됩니다. 더 구체적으로, 우리는 각 모델이 달성한 총 점수 합계와 ChatGPT의 총 점수 간의 비율을 측정하여 경쟁력을 분석합니다. 아래를 참조하십시오. Guanaco 모델은 ChatGPT에 비해 인상적인 성능을 달성하는 것으로 나타났습니다. 두 번째 설정에서 GPT-4는 모델 출력 간의 직접 비교를 수행하여 심층적인 분석을 가능하게 합니다. 이러한 비교는 GPT-4에 세 가지 클래스(class) 레이블링(labeling) 문제로 제시됩니다. 아래를 참조하십시오. GPT-4는 더 나은 응답을 선택하거나 두 응답 간의 무승부를 선언하도록 프롬프트(prompt)되며, 선택에 대한 상세한 설명을 제공합니다. 이 접근 방식을 사용하여 [4]의 저자들은 Guanaco, ChatGPT 및 기타 관련 기준 모델(baseline model) 간의 직접 비교(head-to-head comparison)를 수행합니다. 흥미롭게도, [4]에서 우리는 GPT-4가 프롬프트(prompt)의 첫 번째 응답에 대한 명확한 위치 편향(position bias)을 보여주었지만, 이는 위치 전환 기법(position switching trick)을 사용하여 제거된다는 것을 알 수 있어, 편향 완화의 중요성을 강조합니다. 쌍별 비교(무승부 포함)를 위한 예시 프롬프트(prompt)

**독점 LLM 모방의 잘못된 약속: 스타일과 실체의 간극**
독점 LLM 모방의 잘못된 약속은 경각심을 불러일으킵니다. LLaMA와 이 모델의 결과로 생성된 많은 미세 조정(finetune)된 LLM 이후, 오픈 LLM을 둘러싼 많은 모멘텀(momentum)이 있었지만, 그 이면에는 주의할 점이 있었습니다. 당시 LLaMA의 많은 미세 조정(finetune) 버전은 모방 전략(imitation strategy)을 사용하여 훈련되었습니다. 즉, 우리는 더 강력한 모델(예: ChatGPT)로 다양하고 많은 프롬프트(prompt) 세트에 대한 응답을 생성하고 이 데이터(data)를 통해 오픈 모델을 직접 미세 조정(finetune)합니다 14. 이 모델들은 매우 잘 작동하는 것처럼 보였고, 이는 오픈 LLM과 폐쇄형 LLM 간의 성능 격차가 빠르게 사라질 수 있음을 시사했지만, 현실은 달랐습니다. 그러나 [5]에서는 더 목표 지향적인 평가가 이러한 오픈 모방 모델(imitation model)의 성능에 대한 더 명확하고 냉철한 그림을 그려주었습니다. "처음에 우리는 모방 모델(imitation model)의 출력 품질에 놀랐습니다… 더 목표 지향적인 자동 평가를 수행했을 때, 우리는 모방 모델(imitation model)이 기본 LM에서 ChatGPT까지의 격차를 거의 또는 전혀 좁히지 못한다는 것을 발견했습니다." - [5]에서 발췌. 간단히 말해, 모방 모델(imitation model)은 ChatGPT의 스타일(style)을 모방하는 데 뛰어나며, 이는 인간 주석자(annotator)를 속여 모델 출력을 고품질로 인식하게 할 수 있지만, 이는 본질적인 성능을 반영하지 않습니다. 그러나 이러한 모델은 ChatGPT와 같은 더 강력한 LLM의 사실성(factuality)이 부족하다는 한계가 있습니다. 이러한 결과는 단순히 외형적인 유사성보다는 모델의 깊이 있는 이해와 추론 능력에 대한 평가의 중요성을 강조하며, AI 개발에서 진정한 성능 향상이 무엇인지에 대한 근본적인 질문을 던집니다.