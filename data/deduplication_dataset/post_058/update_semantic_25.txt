거대 언어 모델(LLM)의 성능이 비약적으로 발전하면서, 이들 시스템의 진정한 능력을 정확히 측정하는 방식은 중대한 난제로 부상했습니다. 수많은 고도화된 모델들이 등장했으며, 각각은 매우 다채롭고 정교하며 예측 불가능한(open-ended) 과제를 수행합니다. 이로 인해 모델별 역량의 미묘한 차이를 구분하기란 쉽지 않습니다. LLM을 평가하는 가장 확실한 방안은 사람의 판단(human feedback)에 의존하는 것이지만, 이러한 방식은 데이터의 불균일성, 상당한 시간 소요, 그리고 높은 경제적 부담이라는 한계를 안고 있습니다. 비록 모델의 능력을 가늠하는 데 있어 핵심적이고 없어서는 안 될 기준점임에도 불구하고, 인간 평가는 단독으로 사용될 경우 모델 개발 과정에서의 신속한 반복(iteration)을 방해합니다. 이러한 문제를 극복하기 위해, 우리는 빠르고, 경제적이며, 간결하면서도 인간 평가 결과와 긴밀한 연관성(correlation)을 유지하는 새로운 평가 기준(evaluation metric)을 모색해야 합니다.

이러한 배경 속에서, LLM 자체의 끊임없는 발전은 역설적으로 이 평가 문제에 대한 실질적인 해법을 제시했습니다. 우리는 LLM을 평가 주체로 활용할 수 있으며, 이 방식은 통상적으로 LLM-as-a-Judge [17]로 지칭됩니다. 이 기술은 GPT-4가 출시된 이후 처음으로 심층적으로 탐구되었는데, GPT-4는 다른 모델의 결과물 품질을 판단할 수 있는 최초의 LLM으로 평가받았습니다. 이후 다양한 연구 논문에서 LLM-as-a-Judge를 분석하여, 이 기법을 효과적으로 구현하기 위한 최적의 방법론(best practices)과 우리가 인지해야 할 중요한 편향(bias) 요인들을 밝혀냈습니다. 본 개요는 이러한 여러 연구들을 심층적으로 검토하며, LLM 평가에 대한 깊이 있고 실용적인 이해를 구축하는 데 기여할 것입니다.

**LLM-as-a-Judge [17]의 본질은 무엇인가?**

텍스트 시퀀스(textual sequence)의 질을 측정하기 위한 여러 기존 지표들이 존재합니다. 이 지표들은 참조 기반(reference-based)일 수도 있고, 참조가 필요 없는(reference-free) 방식일 수도 있으며, 이는 품질 측정에 '정답(ground truth)' 시퀀스가 참조로 필요한지 여부를 나타냅니다. 이러한 지표들은 기계 번역(machine translation)이나 요약(summarization)과 같이 비교적 좁은 범위의 작업에서는 효과적으로 작동합니다. 더 자세한 내용은 관련 문헌에서 확인할 수 있습니다. 그러나 최신 LLM들은 다방면으로 복잡하고 개방형(open-ended)인 과제들을 해결하며, 광범위하게 인간의 선호도에 맞춰 정렬(align)되어 있습니다. 이는 기존 자연어 처리(NLP) 벤치마크(benchmark)로는 포착하기 어려운 특징입니다. 이러한 사용 사례에서는 전통적인 지표들이 제대로 기능하지 못하는 경향이 있으며, 인간의 선호도와 낮은 상관관계(correlation)를 보이는 것으로 나타났습니다.

LLM-as-a-Judge 기법은 고성능 LLM에 특정 지시를 내려 다른 모델이 생성한 결과물의 우수성을 판단하게 하는, 외부 참조가 필요 없는(reference-free) 평가 도구입니다. 이 기술은 한계점에도 불구하고, 확장 가능한(scalable) 방식으로 최소한의 구현 변경만으로 다양한 개방형 작업을 평가할 수 있을 뿐만 아니라, 사람의 선호도와도 일관되게 부합하는 것으로 확인되었습니다. 새로운 유형의 작업을 평가하려면 단순히 프롬프트(prompt)만 조정하면 됩니다! 이 지표는 GPT-4 출시 이후 제안되었으며, 이후 급격히 인기를 얻어 [17]에서 LLM-as-a-Judge 지표에 대한 심층 분석(in-depth analysis) 논문으로 정점에 달했습니다. 오늘날 LLM-as-a-Judge는 인간 평가와 더불어 LLM을 위한 가장 보편적인 평가 기술 중 하나로 자리 잡았으며, 모델이 인간의 선호도와 얼마나 잘 부합하는지(alignment)를 평가하는 작업에서 탁월한 성능을 발휘합니다.

**MT-Bench 및 챗봇 아레나(Chatbot Arena) [17]**

[17]의 저자들은 개방형 대화 애플리케이션(open-ended dialogue application)에서 LLM의 성능을 측정하는 데 활용될 수 있는 벤치마크(benchmark)를 확장하기 위해, 인간 선호도를 평가하는 두 가지 데이터셋(dataset)을 개발했습니다. MT-bench 데이터셋은 80개의 고품질 질문으로 구성된 고정된 세트입니다. 8가지 장르(genre)에 걸쳐 있는 이 질문들은 파운데이션 LLM(foundation LLM)에 가장 중요하다고 할 수 있는 두 가지 핵심 기술, 즉 다중 턴 대화(multi-turn conversation) 능력과 지시 따르기(instruction-following) 능력에 중점적으로 초점을 맞추고 있습니다. MT-bench의 질문 예시는 관련 연구에서 확인할 수 있습니다.

챗봇 아레나(Chatbot Arena)라고 불리는 두 번째 데이터셋은 엄밀히 말해 데이터셋이라기보다는 플랫폼(platform)에 가깝습니다. 이 아레나는 사용자가 두 개의 익명 LLM과 동시에 상호작용(interact)하고 더 나은 모델을 선택할 수 있도록 하는 크라우드소싱(crowdsourced) 기반의 대결 플랫폼입니다. 미리 정의된 질문은 사용되지 않습니다. 대신, 사용자가 자신만의 질문을 제시하고 두 LLM 모두 응답을 제공하여 다양한 사용 사례(use case)에 걸쳐 데이터를 수집할 수 있습니다. 편향(bias)을 방지하기 위해, 인간이 선호도를 제공할 때까지 모델의 신원은 공개되지 않습니다. [17]의 저자들은 MT-bench와 챗봇 아레나에서 방대한 양의 인간 피드백(human feedback)을 수집하여 LLM-as-a-Judge와 인간 선호도 간의 상관관계(correlation)를 평가하는 데 활용합니다. 아레나에서 수집된 인간 선호도를 사용하여 Elo 점수(score)를 계산하고, 이를 기반으로 모델 순위를 매길 수 있습니다. 오늘날, 100개 이상의 모델에 대한 150만 개 이상의 쌍별 선호도(pairwise preference)가 챗봇 아레나에 공유되었으며, 이는 가장 널리 참조되는 LLM 리더보드(leaderboard) 중 하나로 자리매김했습니다. 더 자세한 내용은 Nathan Lambert의 게시물에서 확인할 수 있습니다.

**LLM-as-a-Judge의 다양한 설정**

인간 평가와 비교하여, LLM-as-a-Judge는 i) 평가 과정에서 사람의 개입 필요성을 줄이고 ii) 더 빠른 모델 반복(iteration)을 가능하게 하는 간편하고 확장 가능한(scalable) 대안입니다. LLM-as-a-Judge로 평가를 수행하기 위해 우리가 해야 할 일은 프롬프트(prompt)를 작성하는 것뿐입니다! 그러나 일반적으로 사용되는 몇 가지 다른 프롬프트 구조가 있습니다.

*   **쌍별 비교(Pairwise comparison)**: 심사관에게 질문과 두 가지 모델 응답이 제시되고, 더 우수한 응답을 식별하도록 요청합니다. 이는 모델 간의 상대적 성능을 직접적으로 비교하는 데 효과적입니다.
*   **점수 기반 채점(Pointwise scoring)**: 심사관에게 질문에 대한 단일 응답이 주어지고, 특정 척도(예: 1부터 5까지의 리커트 척도(Likert scale))에 따라 점수를 할당하도록 요청합니다. 이는 절대적인 품질 평가에 유용합니다.
*   **참조 기반 채점(Reference-guided scoring)**: 심사관에게 질문 및 응답과 함께 채점 과정을 돕기 위한 참조 솔루션(reference solution)이 제공됩니다. 이는 특정 정답이 있는 과제에서 정확도를 높일 수 있습니다.
*   **루브릭 기반 채점(Rubric-based scoring)**: 심사관 LLM에게 평가해야 할 특정 기준(예: 정확성, 유창성, 관련성, 창의성 등)과 각 기준에 대한 점수 부여 지침이 담긴 루브릭(rubric)을 제공합니다. 이는 더 세분화되고 구조화된 피드백을 얻을 수 있게 하며, 평가의 일관성을 높이는 데 기여합니다.

이러한 기술 중 어느 것이든 사고 연쇄(chain of thought, CoT) 프롬프팅(prompting)과 결합하여 채점 품질을 향상시킬 수 있습니다. 이를 위해 우리는 심사관의 프롬프트에 "점수에 대한 단계별 설명을 작성해 주세요"와 같은 지시를 추가하여 간단히 제로샷 CoT 프롬프트(prompt)를 사용할 수 있습니다. 그러나 [16]에서 권장하는 바와 같이, LLM에게 점수 이전에 근거(rationale)를 출력하도록 요청해야 합니다(나중에가 아니라). "모델이 생성한 결론은 나중에 생성된 설명에 의해 뒷받침되지 않습니다." - [16]에서 발췌. LLM이 점수와 함께 사람이 읽을 수 있는 근거(rationale)를 출력하도록 하는 것은 쉽고 유용한 설명 가능성(explainability) 기법입니다. 우리는 이러한 설명을 사용하여 모델의 성능과 단점에 대한 더 깊은 이해를 얻을 수 있습니다.

**어떤 설정을 사용해야 할까?**

이러한 각 채점 전략에는 고유한 장단점이 있으며, 어떤 접근 방식도 절대적으로 우월하다고 할 수는 없습니다. 예를 들어, 쌍별 비교(pairwise comparison)는 모든 모델 출력 조합이 서로 비교되어야 하므로 확장성(scalability)이 떨어진다는 단점이 있습니다. 반면, 점수 기반 채점(pointwise scoring)은 심사관이 비교적 일관된 내부 채점 체계(mechanism)를 가질 것이라는 전제에 의존하기 때문에 상대적으로 안정성이 낮을 수 있습니다. 절대 점수는 쌍별 비교에 비해 훨씬 더 큰 변동성을 보일 가능성이 높습니다. 일반적으로, 어떤 LLM-as-a-Judge 평가 방식을 사용해야 할지는 우리 애플리케이션(application)의 구체적인 요구 사항에 따라 달라집니다. 항상 비교할 두 모델이 있는 것은 아니므로, 그러한 경우에는 점수 기반 채점이 가장 합리적일 수 있으며, 그 반대의 경우도 마찬가지입니다.

**과연 효과적일까?**

[17]의 연구에서 우리는 GPT-4와 같은 강력한 LLM 심사관이 인간의 선호도를 매우 정확하게 측정할 수 있음을 명확히 확인할 수 있습니다. 실제로 GPT-4는 인간 선호도 점수와 80%의 일치율(agreement rate)을 달성하는 것으로 나타났는데, 이는 인간 주석자(annotator)가 스스로와 가지는 일치율과 유사한 수준입니다. LLM 심사관이 인간 선호도를 정확하게 예측하는 이러한 능력은 놀라운 일이 아닙니다. 대부분의 현대 LLM은 인간 선호도 데이터(data)로 광범위하게 미세 조정(finetune)되었기 때문입니다. 이는 LLM이 인간의 판단 패턴과 선호 구조를 내재화했음을 의미합니다.

**편향(Bias) (그리고 이를 피하는 방법…)**

LLM 평가자가 사람의 선호도를 상당히 정확하게 예측함에도 불구하고, 이 평가 기법은 여러 새로운 편향 요소를 평가 과정에 유입시킨다는 점에서 완전무결하지 않습니다. 이미 우리는 LLM이 의심스러운 추론(reasoning) 능력, 프롬프트(prompt)의 미세한 변화에 대한 민감성, 그리고 장황한 출력(verbose output)을 생성하는 경향과 같은 수많은 한계(limitation)를 가지고 있음을 알고 있습니다. 이러한 약점 중 다수는 LLM-as-a-Judge 평가 내에서 상응하는 편향으로 이어집니다.

*   **위치 편향(Position bias)**: 심사관은 프롬프트 내 위치에 따라 출력을 선호할 수 있습니다(예: 쌍별 프롬프트에서 첫 번째 응답). 이는 제시 순서가 평가에 영향을 미치는 경향을 말합니다.
*   **장황함 편향(Verbosity bias)**: 심사관은 출력 길이에 따라 더 나은 점수를 할당할 수 있습니다(즉, 더 긴 응답이 더 높은 점수를 받습니다). 단순히 정보량이 많다고 해서 품질이 높다고 판단하는 경향입니다.
*   **자기 강화 편향(Self-enhancement bias)**: 심사관은 스스로 생성한 응답을 선호하는 경향이 있습니다(예: GPT-4는 자체 출력에 높은 점수를 할당합니다). 이는 모델의 자기중심적인 평가 경향을 나타냅니다.
*   **문화적 편향(Cultural bias)**: 특정 문화권의 관점이나 가치관에 기반한 응답을 다른 문화권의 응답보다 선호할 수 있습니다. 이는 훈련 데이터의 지리적, 문화적 편향에서 기인합니다.
*   **최신 정보 편향(Recency bias)**: LLM이 훈련된 시점 이후의 최신 정보나 트렌드를 반영하지 못하거나, 특정 시점의 정보에 과도하게 의존하는 경향을 보일 수 있습니다.
*   **도메인 특화 편향(Domain-specific bias)**: 특정 전문 분야의 지식이나 언어 사용에 대한 이해가 부족하여, 해당 도메인의 응답을 부적절하게 평가할 수 있습니다.

위에 설명된 편향 원인 외에도, LLM 심사관은 스스로 답변하기 어려운 질문(예: 복잡한 추론(reasoning) 및 수학 질문)에 대한 응답을 채점하는 데 어려움을 겪는 경향이 있습니다. 또한 심사관은 컨텍스트(context)의 잘못된 정보에 쉽게 오도될 수 있습니다 [18]. 채점되는 응답 중 하나가 잘못된 경우, 심사관은 이 컨텍스트에 의해 오도되어 부정확한 점수를 출력할 수 있습니다. 이는 LLM이 제공된 정보의 사실 여부를 항상 완벽하게 검증하지 못하는 한계에서 비롯됩니다.

**편향(bias)에 대한 심층 분석**

[17]에 설명된 편향 원인 외에도, LLM-as-a-Judge 평가에서의 편향을 심층적으로 분석한 다른 많은 연구들이 있습니다. 이 중 일부는 이 개요에서 나중에 살펴볼 것입니다. 이러한 연구 목록은 쉬운 참조 및 추가 읽기를 위해 아래에 제공됩니다.

*   인간 또는 LLM이 심사관인가? 판단 편향(Judgement Biases)에 대한 연구 [ 링크 ]
*   대규모 언어 모델을 위한 평가 편향(Evaluation Biases) [ 링크 ]
*   평가자로서의 대규모 언어 모델의 인지 편향(Cognitive Biases) [ 링크 ]
*   대규모 언어 모델은 일관성 없고 편향된 평가자이다 [ 링크 ]
*   대규모 언어 모델은 아직 인간 수준의 평가자가 아니다 [ 링크 ]

**편향을 어떻게 줄일 수 있을까?**

LLM-as-a-Judge 평가 결과에 대한 편향의 영향을 감소시키기 위해 활용할 수 있는 몇 가지 기법들이 있습니다.

*   **위치 전환 기법(position switching trick)**: 프롬프트 내 모델 출력의 위치를 무작위화하고, 여러 점수를 생성하며, 다른 위치를 가진 점수들의 평균을 취하는 것입니다. 이는 위치 편향을 효과적으로 상쇄합니다.
*   **퓨샷(few-shot) 예시 제공**: 점수의 자연스러운 분포를 보여주고 심사관의 내부 채점 메커니즘(mechanism)을 보정(calibrate)하는 데 도움이 되는 소수의 예시를 제공합니다. 이는 평가 기준을 명확히 하는 데 도움이 됩니다.
*   **정답 제공**: 평가 과정에서 심사관을 위한 참조로 프롬프트 내에 어려운 수학 및 추론(reasoning) 질문에 대한 정답을 제공합니다. 이는 모델이 스스로 해결하기 어려운 문제를 평가할 때의 불확실성을 줄여줍니다.
*   **다중 심사관 활용**: 자기 강화 편향(self-enhancement bias)의 영향을 줄이기 위해 여러 다른 모델(예: Claude, Gemini 및 GPT-4)을 심사관으로 사용합니다. 다양한 모델의 관점을 통합하여 편향을 희석시킵니다.
*   **프롬프트 다양화(Prompt Diversification)**: 동일한 평가 목표에 대해 여러 가지 방식으로 프롬프트를 구성하여 심사관에게 제시합니다. 이는 특정 프롬프트 구조에 의해 유발될 수 있는 편향을 줄이는 데 도움이 됩니다.
*   **앙상블 심사(Ensemble Judging)**: 여러 LLM 심사관의 평가 결과를 종합하여 최종 점수를 도출합니다. 이는 개별 심사관의 고유한 편향을 평균화하여 전체적인 평가의 견고성을 높입니다.

이러한 기술들이 유용하더라도, LLM-as-a-Judge는 모든 지표(metric)와 마찬가지로 결함 있는 지표이며 결코 완벽하지 않을 것입니다. 따라서 우리는 항상 이러한 편향을 인지하고 그것이 우리의 분석에 어떤 영향을 미치는지 고려해야 합니다. 평가되는 모델, 측정하려는 대상, 평가가 어떻게 설정되었는지, 그리고 기본 심사관이 이 평가의 결과를 어떤 방식으로 왜곡할 수 있는지 깊이 있게 고민해야 합니다.

**LLM 평가에 대한 초기 연구**

[17]에서 LLM-as-a-Judge가 제안되고 분석되기 이전에, 다양한 초기 연구들이 유사한 기술을 연구했습니다. 이러한 연구는 텍스트 품질을 평가할 만큼 충분히 강력한 최초의 LLM이었던 GPT-4의 제안으로 시작되었습니다. 우리가 보게 되겠지만, 이 접근 방식은 사용 용이성, 일반성, 효과성 덕분에 빠르게 인기를 얻고 커뮤니티(community)에 퍼지기 시작했습니다. 또한, LLM 기반 평가의 등장은 단순한 성능 측정에서 벗어나, 모델이 얼마나 '인간적'으로 응답하는지, 또는 특정 지시를 얼마나 잘 따르는지와 같은 질적인 측면을 평가하는 새로운 패러다임을 열었습니다.

**인공 일반 지능(Artificial General Intelligence)의 불꽃: GPT-4를 사용한 초기 실험 [1]**

LLM 기반 평가를 가능하게 하려면, 먼저 다른 모델의 출력을 안정적으로 평가할 수 있는 충분히 강력한 LLM에 접근할 수 있어야 합니다. 이전 모델들도 인상적이었지만, GPT-4가 제안되기 전까지는 그러한 모델에 접근할 수 없었습니다. 그러나 이 모델이 제안된 직후 연구자들은 LLM 평가자의 실현 가능성을 탐구하기 시작했습니다!

**GPT-4는 얼마나 뛰어난가? [1]**

GPT-4를 평가자로 사용하는 것을 탐구한 첫 번째 연구 [1]는—모델 출시 후 10일도 채 되지 않아 출판되었지만—평가에 중점을 두지 않았습니다. 오히려 이 연구는 GPT-4의 역량을 탐구하는 더 일반적인 목표를 가지고 있었으며, 그 결과 (놀랍게도) 광범위한 주제를 다루는 155페이지 분량의 분석이 나왔습니다.

*   코딩 및 수학 문제 해결 능력.
*   도구 사용 및 인간과의 상호작용(interact); 예: 마음 이론(theory of mind) 문제, 또는 인간에게 출력을 설명하는 모델의 능력.
*   TikZ를 사용하여 기본적인 그림/사진 그리기, 유용한 플롯(plot) 생성, 그리고 더 일반적인 데이터 분석 수행.
*   수학 정리(theorem) 증명, 또는 증명의 모든 줄을 운율에 맞춰 작성하면서 이를 수행하는 것.

이 분석의 결론은 GPT-4가 사실상 모든 고려된 작업에서 탁월하며 ChatGPT에 비해 상당한 개선을 이루었다는 것입니다. 실제로 저자들은 GPT-4의 출력이 많은 경우 인간의 것과 구별할 수 없거나(또는 더 낫다) 심지어 GPT-4가 인공 일반 지능(AGI)을 향한 중요한 진전이라고 말하기까지 합니다. "GPT-4 역량의 일반성과… 인간 수준 또는 그 이상의 광범위한 작업에서의 성능 조합은 GPT-4가 AGI를 향한 중요한 진전이라고 말하는 것을 편안하게 만듭니다." - [1]에서 발췌. 저자들이 GPT-4가 (일반) 지능의 징후를 보여준다고 주장한 것은—그리고 계속해서—매우 논란이 많았지만, LLM 역량과 AGI를 향한 진전에 대한 이견은 종종 이러한 개념에 대한 엄격한 정의의 부족(또는 차이)으로 귀결됩니다. 먼저 정의하지 않고 지능을 어떻게 측정할 수 있을까요? 다행히도 이 게시물의 목적을 위해 AGI를 정의하는 것에 대해 걱정할 필요는 없습니다. 우리가 알아야 할 것은 GPT-4가 매우 유능한 모델이며, 이러한 역량이 다른 LLM의 출력을 평가하기 위해 LLM을 사용하는 데 많은 가능성을 열어준다는 것입니다.

**평가자로서의 GPT-4 [1]**

[1]의 저자들은 GPT-4를 심사관으로 사용하는 것을 탐구한 최초의 연구자들이지만, 그들의 분석은 사실 상당히 간략합니다(즉, 전체적으로 1페이지 미만)! 기존 평가 지표(evaluation metric)가 진술의 유사성을 판단하는 데 있어서 가지는 한계(limitation)를 다룬 후, 저자들은 위에 표시된 프롬프트(prompt)를 사용하여 모델 응답이 참조 답변과 얼마나 유사한지 판단하는 GPT-4의 능력을 평가합니다. 구체적으로, GPT-4는 모델의 응답이 참조 답변과 더 유사한지 아니면 GPT-3로 생성된 답변과 더 유사한지 결정하는 작업을 맡습니다. 진술에 대한 두 가지 응답이 심사관에게 제공되며, 심사관은 원래 진술을 더 잘 반영하는 옵션을 식별합니다. "[GPT-4]는 한 쌍의 답변 중 어떤 답변이 황금 답변(gold answer)에 더 가까운지 결정할 수 있으며, 이 결정은 동일한 작업을 수행하는 인간과 합리적으로 일치합니다." - [1]에서 발췌. 이 분석을 통해 우리는 GPT-4가 ROUGE 또는 BLEU와 같은 간단한 지표(metric)보다 진술 간의 의미론적 유사성(semantic similarity)을 훨씬 더 잘 판단할 수 있음을 알 수 있습니다. 이러한 평가의 품질을 향상시키기 위해 저자들은 GPT-4에게 선호하는 출력을 식별하기 전에 각 응답의 장단점을 나열하도록 요청합니다. GPT-4의 판단과 인간의 판단을 비교할 때, 우리는 상당한 차이를 발견합니다. 예를 들어, GPT-4는 인간 평가자의 경우 47.61%인 반면, 87.76%의 경우에서 GPT-4가 생성한 응답을 선호합니다. GPT-4와 인간 평가자 간의 일치도(agreement level)는 낮습니다—50%를 약간 넘는 수준—그리고 우리는 이미 [1]에서 편향(bias)의 명확한 징후를 봅니다. 예를 들어, 더 긴 응답은 GPT-4에 의해 더 좋게 평가됩니다. 이러한 불일치(lack of alignment)는 평가 설정의 차이 때문일 수 있습니다. GPT-4는 두 응답 중 승자를 선택하도록 강요받는 반면, 인간은 무승부를 선택할 수 있습니다. 그러나 GPT-4와 인간 간의 일치도는 여전히 놀랍도록 낮으며, [1]의 저자들은 GPT-4의 평가 역량을 보정(calibrate)하기 위해 더 많은 연구가 필요하다고 결론 내립니다.

**Vicuna: 90%* ChatGPT 품질로 GPT-4를 감동시킨 오픈 소스(open-source) 챗봇(chatbot) [2]**

"챗봇(chatbot)을 평가하는 것은 결코 간단한 작업이 아닙니다. GPT-4의 최근 발전으로, 우리는 GPT-4의 역량이 자동화된 평가 프레임워크(framework)를 가능하게 할 수 있는 인간과 유사한 수준에 도달했는지 궁금합니다." - [2]에서 발췌. Vicuna는 ShareGPT에서 수집된 ChatGPT와의 사용자 대화 세트에 대해 LLaMA-13B를 미세 조정(finetune)하여 생성된 오픈 소스(open-source) 챗봇(chatbot)입니다. 우리는 이전 개요에서 Vicuna의 세부 사항을 다루었습니다. 그러나 Vicuna는 저자들이 이 모델의 출력 품질을 주로 GPT-4로 자동으로 평가하기로 선택했다는 사실 때문에 이 개요와 관련이 있습니다.

사실 Vicuna는 당시 반발을 받았던 이러한 LLM 기반 평가를 수행한 최초의 연구 중 하나였습니다. 그러나 [2]에서 평가 목적으로 GPT-4를 사용한 것은 이 기술에 대한 분석의 물결을 일으켰는데, 그 이유는 다음과 같습니다.

*   평가 결과가 비교적 일관되고 유망해 보였습니다!
*   GPT-4를 평가자로 사용하는 것은 어떤 작업에도 적용할 수 있는 참조 불필요(reference-free) 자동 평가 전략입니다(즉, 매우 일반적이고 간단합니다).
*   GPT-4는 평가와 함께 근거(rationale)를 출력할 수 있으며, 이는 평가 결과의 인간 해석 가능성(interpretability)을 향상시킵니다.

**평가 설정.**

Vicuna를 테스트하는 데 사용된 질문은 8가지 범주(category)에 걸쳐 있습니다. 총 80개의 질문이 있으며, GPT-4의 도움을 받아 작성되었습니다. 저자들은 GPT-4가 신중한 프롬프트 엔지니어링(prompt engineering)을 통해 최첨단 챗봇(chatbot)에 대한 도전적인 질문을 생성할 수 있음을 관찰합니다. 이러한 질문에 대한 다양한 챗봇(chatbot)의 답변을 평가하기 위해 각 모델의 출력을 수집하고 GPT-4에게 유용성, 관련성, 정확성, 세부 사항 측면에서 생성된 출력을 평가하도록 요청합니다. 놀랍도록 간단한 평가에 사용된 프롬프트(prompt)는 관련 문서에서 볼 수 있습니다. 그러한 평가의 어려움 때문에 코딩 및 수학 작업을 평가하기 위해 별도의 더 구체적인 프롬프트(prompt)가 사용됩니다.

GPT-4 평가를 위한 프롬프트는 다른 모델의 두 응답 품질을 1부터 10까지의 척도(scale)로 평가하고 이 점수에 대한 설명을 제공하도록 요청받습니다. CoT 프롬프팅(prompting)을 통해 이러한 설명을 생성하는 것은 GPT-4 평가의 정확도를 향상시키는 경향이 있으며, 평가를 설명하는 사람이 읽을 수 있는 근거(rationale)를 제공합니다. 이러한 프롬프트에 대해 우리가 할 수 있는 몇 가지 다른 유용한 관찰이 있습니다. 일반 프롬프트는 GPT-4에게 점수를 생성할 때 위치 편향(positional bias)을 포함한 편향(bias)을 피하도록 요청하며, 이는 [2]의 저자들이 위치 편향 문제에 직면했음을 보여줍니다. 후속 연구 [8]는 Vicuna의 평가 전략에 강한 위치 편향이 있음을 확인했지만, 이 문제는 위치 전환(position switching)을 통해 해결될 수 있습니다. 모든 프롬프트는 출력 형식(output format)을 명시적으로 지정하여 응답에서 평가를 쉽고 자동으로 구문 분석(parse)할 수 있도록 합니다. 이러한 접근 방식은 GPT-4와 같은 강력한 지시 따르기(instruction following) 모델로만 가능합니다. 코딩 및 수학 평가를 위한 프롬프트는 일반 질문에 사용된 프롬프트에 비해 더 상세하며, 그러한 질문에 대한 평가 품질을 향상시키기 위해 더 심층적이고 문제별 세부 사항을 도입합니다. 코딩 및 수학 프롬프트는 제공된 솔루션(solution)을 비판하거나 평가를 제공하기 전에 문제를 직접 해결하도록 GPT-4에게 요청하는 것과 같이 더 나은 평가를 유도하기 위해 몇 가지 다른 기법을 사용합니다.

**Vicuna의 독특한 접근 방식.**

각 프롬프트에서 두 응답을 평가하는 것은 모델 간의 더 나은 상대적 비교를 가능하게 하므로 오늘날 일반적으로 사용되는 표준 관행입니다. 우리는 GPT-4에게 주어진 질문에 대해 두 모델 중 어느 모델이 더 나은 응답을 제공하는지 쉽게 설명하도록 요청할 수 있습니다. 그러나 Vicuna의 설정은 표준 쌍별 비교(pairwise comparison)와 약간 다르다는 점에 유의하는 것이 중요합니다. 모델에게 선호하는 응답을 요청하는 대신, 모델은 각 예시에 점수를 할당하도록 프롬프트되며, 선호되는 예시는 이 점수를 기반으로 결정됩니다. 이는 여러 논문에서 사용된 또 다른 유효한 LLM-as-a-Judge 설정입니다.

**이것이 잘 작동할까?**

GPT-4가 코딩 및 수학 질문 채점에 어려움을 겪음에도 불구하고, 이러한 평가 결과는 비교적 일관되며 상세한 설명을 제공합니다! 이러한 평가를 사용하여 저자들은 Vicuna가 90%의 질문에서 다른 오픈 소스(open-source) 모델보다 선호되며, 45%의 경우에서 ChatGPT보다 낫거나 동등하다고 평가된다는 것을 관찰합니다. 그러나 후속 연구 [8]는 이러한 평가가 더 긴 출력에 편향(bias)되어 있으며 인간 선호도와 (어느 정도) 상관관계가 낮다는 것을 밝힙니다. 따라서 이러한 결과를 해석할 때 우리는 이러한 편향과 단점을 인지해야 합니다. 더 구체적으로, 후속 연구에서 우리는 Vicuna와 같은 모방 모델(imitation model)이 ChatGPT와 같은 강력한 모델의 스타일(style)과 일치하지만, 사실성(factuality)과 지식 기반(knowledge base)이 부족하다는 것을 알 수 있습니다. 그럼에도 불구하고, [2]의 분석은 i) 오픈 소스 모델의 응답 스타일(style)과 지시 따르기(instruction-following) 능력이 더 강력한 모델의 출력으로 미세 조정(finetune)함으로써 향상될 수 있으며 ii) LLM 기반 평가의 실현 가능성이 더 명확하게 입증된다는 점에서 유용합니다. "이 제안된 평가 프레임워크(framework)가 챗봇(chatbot) 평가의 잠재력을 보여주지만, 아직 엄격하거나 성숙한 접근 방식은 아닙니다… 챗봇(chatbot)을 위한 포괄적인 평가 시스템(system)을 개발하는 것은 여전히 미해결 과제입니다." - [2]에서 발췌.

**앞으로.**

[2]에서 GPT-4 평가의 초기 단계적 특성에도 불구하고, 이 연구는 LLM 기반 평가의 후속 분석을 위한 강력한 기반을 마련합니다. 후속 연구는 LLM-as-a-Judge 기술의 강점과 약점에 대한 다양하고 유용한 분석을 제공하지만, 신뢰할 수 있는 점수를 생성하는 데 사용되는 프롬프트는 [2]에서 본 것과 (상대적으로) 유사한 경향이 있습니다! 사실, LLM-as-a-Judge 출판물 자체 [17]는 Vicuna와 동일한 저자들이 작성했으며 [2]에서 제안된 많은 기술을 적극적으로 활용합니다. 또한 Vicuna를 평가하는 데 사용된 80개의 질문 세트는 다른 논문에서도 널리 사용됩니다.

**AlpacaEval: 지시 따르기(Instruction-Following) 모델의 자동 평가자 [8]**

2023년 중반에 처음 제안된 AlpacaEval [8]은 지시 따르기(instruction-following) 언어 모델을 위한 가장 인기 있는 LLM 기반의 자동화된 평가 지표(evaluation metric) (및 리더보드(leaderboard)) 중 하나입니다. AlpacaFarm [9]을 기반으로 하는 평가 전략—LLM 평가자를 사용하여 RLHF 스타일의 쌍별 선호도 레이블(pairwise preference label) 생성을 자동화하는 시뮬레이터(simulator)—은 간단하고 비서 스타일(assistant-style) 작업의 포괄적인 세트에 걸쳐 있는 805개의 고정된 지시 세트를 사용합니다. 각 지시에 대해 우리는 두 LLM(기준 모델(baseline model)과 평가 대상 모델)으로 출력을 생성합니다. 그런 다음 LLM 평가자가 각 모델 출력의 품질을 평가하는 데 사용되어(즉, 쌍별 설정(pairwise setup)), 두 모델 출력 간의 승률(win-rate)을 계산할 수 있게 합니다.

**이것이 왜 유용할까?**

AlpacaEval의 핵심 목표는 사람의 평가와 높은 연관성을 지니면서도 신속하고 경제적인 자동화된 평가 시스템을 구축하는 데 있습니다. AlpacaEval의 현재 버전은 3분 이내에 실행되며, 10달러 미만의 비용이 들고, 인간 평가(챗봇 아레나(Chatbot Arena)에서 가져옴)와 0.98의 스피어만 상관관계(Spearman correlation)를 가집니다. 이에 비해 인간 평가는 잡음과 불일치에 취약하고, 훨씬 더 비싸며, 몇 주간의 주석(annotation) 시간이 필요할 수 있습니다. AlpacaEval이 매우 효율적이기 때문에, 이 지표는 모델 개발에 완벽합니다. 이는 간단한 지시 따르기(instruction-following) 작업에 대한 인간 평가를 위한 신뢰할 수 있는 대리(proxy)를 빠르고 저렴하게 계산할 수 있도록 제공합니다.

**평가자는 어떻게 작동할까?**

AlpacaEval에서 평가자를 위해 사용된 프롬프트(prompt)는 관련 연구에서 확인할 수 있습니다. 이 프롬프트는 대부분의 독점적인 채팅 완성 API(chat completion API)에서 사용되는 입력 스타일(style)과 일치하며 다중 턴 대화(multi-turn conversation) 내에서 역할과 메시지를 구분하는 데 사용되는 채팅 템플릿(chat template) 구조를 사용합니다. 모든 지시에 대해 한 쌍의 응답이 평가자에게 전달되며, 우리는 선호되는 출력—이진 응답(binary response) 또는 LLM에서 각 옵션의 로그 확률(logprobs)—을 응답으로 받습니다. 이 응답은 데이터셋(dataset) 내 주어진 지시에 대해 평가 대상 모델의 응답이 기준 모델(baseline model)의 응답보다 더 나을 확률을 나타냅니다. 전체 데이터셋에 걸쳐 이러한 확률의 평균을 취함으로써, 우리는 모델 출력이 기준 모델보다 선호되는 시간 비율을 측정하는 승률(win-rate)을 계산할 수 있습니다.

평가자의 품질은 2.5K 인간 평가 세트와의 일치도를 측정하여 검증됩니다. 그러나 특정 사용 사례(use case)에 가장 적합한 평가자를 선택할 때 비용 및 지연 시간(latency)과 같은 여러 다른 요소도 관련이 있습니다. 원래 AlpacaEval은 데이터셋 내 각 인스턴스(instance)에 대해 온도가 0인 단일 선호도 응답을 생성했습니다. 그러나 자동화된 선호도 주석(annotation)의 품질은 후속 반복(iteration)에서 다음을 통해 향상되었습니다.

*   프롬프트 내 모델 출력의 위치를 무작위화합니다(또는 모델 출력의 각 가능한 위치에 대해 여러 선호도 점수를 샘플링(sample)합니다).
*   이진 선호도 응답을 생성하는 대신 각 응답의 로그 확률(logprobs)을 측정합니다.
*   더 나은 모델(GPT-4-Turbo)을 평가자로 사용합니다.
*   저자들은 또한 핵심 평가자의 프롬프트(prompt)를 수정하고 단순화하여 지시를 단축하고 응답에서 단일 토큰(token)만 출력하도록 했습니다.

**길이 편향(length bias) 완화.**

우리가 보았듯이, LLM을 평가자로 사용하는 것은 평가 과정에 여러 미묘한 편향(bias) 원인을 도입할 수 있습니다. 우리는 이러한 편향을 인지하고 이를 제거하거나 설명하기 위해 최선을 다해야 합니다. "모든 모델의 출력이 기준 모델(baseline model)의 출력과 길이가 같았다면 AlpacaEval 지표(metric)는 어떠했을까요?" - [10]에서 발췌. LLM 평가자의 알려져 있고 널리 퍼진 편향 중 하나는 더 긴 출력(즉, 장황함 편향(verbosity bias))을 선호하는 경향입니다. 특정 독점 LLM(예: GPT-4 또는 GPT-4-Turbo)은 더 짧은 출력보다 더 긴 출력을 선호하는 경향이 있습니다. 결과적으로 AlpacaEval은 고정되거나, 비교 가능하거나, 심지어 더 나쁜 콘텐츠(content) 품질이 주어졌을 때 더 긴 출력을 더 짧은 출력보다 더 좋게 평가할 수 있습니다.

이 편향에 맞서기 위해 연구자들은 AlpacaEval 지표 [10]를 간단한 회귀 기반(regression-based) 편향 제거(debiasing) 프로세스(process)로 확장했습니다. 특히, 세 가지 속성(attribute)을 입력으로 받는 선형 회귀 모델(linear regression model)이 훈련됩니다. i) 모델, ii) 지시 난이도, iii) 정규화된 출력 길이. 이 모델이 훈련되면, 출력 품질과 허위 상관관계(spurious correlation)가 있다고 여겨지는 항들의 기여도를 "0으로 만들어서" 진정한 품질 점수만 남길 수 있습니다. [10]의 경우, 우리는 회귀에서 길이 항을 제거하고 평소대로 승률(win-rate)을 계산하여 길이 제어 AlpacaEval 점수를 산출합니다. 현재 공개 리더보드(leaderboard)에서 사용되는 길이 제어 AlpacaEval은 [10]에서 표준 지표보다 조작하기 어렵다(less game-able)는 것이 밝혀졌습니다. 다시 말해, 우리는 모델에게 더 장황하거나 덜 장황하게 요청하는 것만으로는 길이 제어 AlpacaEval의 결과를 크게 변경할 수 없습니다. AlpacaEval의 길이 제어 버전은 인간 평가와 더 잘 상관관계(correlate)를 이루는 것으로 나타났으며, 챗봇 아레나(Chatbot arena)와의 AlpacaEval 스피어만 상관관계(Spearman correlation)를 0.94에서 0.98로 향상시켰습니다.

**LLM 기반 평가의 다른 초기 사용**

Vicuna가 제안된 직후, GPT-4를 평가자로 사용하는 것이 점점 더 보편화되었습니다. 이때 LLM 기반 평가의 신뢰성을 증명하기 위한 분석은 거의 이루어지지 않았습니다. 그러나 이러한 평가 방식이 매우 인기를 얻게 된 몇 가지 주목할 만한 요인이 있습니다.

*   **구현 용이성**: 단지 프롬프트(prompt)와 API 호출(API call)만 있으면 됩니다! 이는 개발자들이 최소한의 노력으로 평가 시스템을 구축할 수 있게 합니다.
*   **개방형 출력의 평가 난이도**: LLM 출력의 개방형(open-ended) 특성으로 인해 전통적/자동 지표(metric)(예: ROUGE 또는 BLEU)를 사용한 평가가 매우 어렵습니다. LLM-as-a-Judge는 이러한 질적 평가의 공백을 메웁니다.
*   **인간 평가의 한계**: LLM 평가를 위한 정답(ground truth)의 원천인 인간 평가는 잡음이 많고, 비용이 많이 들며, 시간이 많이 소요됩니다. LLM-as-a-Judge는 이러한 자원 제약을 완화합니다.
*   **연구 커뮤니티의 필요성**: 연구 커뮤니티는 i) 광범위한 작업에서 성능을 안정적으로 측정할 수 있고 ii) 더 빠르게 실험하고 반복(iterate)할 수 있게 하는 더 접근 가능한 평가 전략이 필요했습니다. LLM 기반 평가는 이러한 간극을 빠르게 메우기 시작했습니다.
*   **자동화된 레드팀(Automated Red Teaming)의 기반**: LLM-as-a-Judge는 모델의 취약점이나 안전 문제를 자동으로 식별하는 '레드팀' 활동의 자동화에도 활용되기 시작했습니다. 이는 인간 레드팀의 한계를 보완하며, 모델의 견고성을 강화하는 데 중요한 역할을 합니다.

**LIMA: 정렬(Alignment)에는 적을수록 좋다. [3]**

LIMA [3]는 제한된 양의 데이터(data)로 지도 미세 조정(supervised finetuning)을 사용하여 사전 훈련된 언어 모델(pretrained language model)(예: LLaMA)을 정렬(align)하는 우리의 능력을 연구합니다. 흥미롭게도, 저자들은 단 1,000개의 큐레이션(curated)된 미세 조정(finetuning) 예시만으로도 놀랍도록 강력한 성능을 달성하기에 충분하다는 것을 관찰합니다. 이러한 결과는 LLM이 사전 훈련(pretraining) 중에 대부분의 지식을 학습하는 반면, 미세 조정(finetuning)은 모델 출력의 형식(format)을 최적화한다는 것을 시사합니다. 이 현상은 표면적 정렬 가설(Superficial Alignment Hypothesis)이라고 불립니다. "우리는 표면적 정렬 가설(Superficial Alignment Hypothesis)을 다음과 같이 정의합니다. 모델의 지식과 역량은 거의 전적으로 사전 훈련(pretraining) 중에 학습되며, 정렬(alignment)은 어떤 형식의 하위 분포(subdistribution)를 사용해야 하는지 가르칩니다." - [3]에서 발췌. [3]에서는 인간 및 LLM 기반 평가가 모두 사용됩니다. 각 프롬프트(prompt)에 대해 테스트되는 각 모델에 대한 단일 응답이 생성됩니다. 그런 다음 인간은 LIMA의 출력(익명으로)을 모든 기준 모델(baseline model)과 비교하여 선호하는 응답을 선택하도록 요청받습니다. 이 평가 과정은 다음을 통해 자동화될 수 있습니다.

*   GPT-4에 동일한 정확한 프롬프트(prompt)를 제공합니다.
*   모델에게 선호하는 응답을 선택하도록 요청합니다.

인간 및 모델 기반 평가 모두에서 LIMA는 훨씬 적은 데이터(data)로 미세 조정(finetune)되었음에도 불구하고 Alpaca와 같은 이전 오픈 모델보다 뛰어난 성능을 보이는 것으로 나타났습니다. LIMA는 또한 GPT-3.5보다 뛰어난 성능을 보이며, 특히 테스트 프롬프트(prompt)의 34%에서 43%에 해당하는 상당수의 테스트 프롬프트(prompt)에서 GPT-4의 성능과 일치하거나 능가합니다.

**Guanaco. [4]**

[4]의 저자들은 양자화된 저랭크 적응(quantized low-rank adaptation, Q-LoRA)을 제안하는데, 이는 상용 하드웨어(commodity hardware)(즉, 메모리가 적은 소비자 GPU(consumer GPU))에서 LLM 미세 조정(finetuning)을 훨씬 쉽게 만드는 매개변수 효율적인(parameter-efficient) 훈련 전략입니다. Q-LoRA에 대한 자세한 내용은 이 개요를 참조하십시오. Q-LoRA로 LLM을 훈련하는 주요 이점은 메모리 소비 감소입니다. [4]에서는 650억 개의 매개변수(parameter)를 가진 LLM이 단일 48Gb GPU로 Q-LoRA를 사용하여 미세 조정(finetune)될 수 있음을 보여줍니다!

[4]의 저자들은 Q-LoRA를 사용하여 챗봇(chatbot) 스타일의 오픈 LLM인 Guanaco 제품군을 훈련합니다. 이 모델들은 인간과 GPT-4를 모두 사용하여 평가됩니다. 흥미롭게도, GPT-4는 의미 있고 신뢰할 수 있는 성능 지표(performance metric)를 제공하는 반면, 기존 벤치마크(benchmark)는 챗봇(chatbot) 성능에 대한 정확한 측정을 제공하지 않는 것으로 나타났습니다. "GPT-4 평가는 인간 평가에 대한 저렴하고 합리적인 대안입니다… 우리는 현재 챗봇(chatbot) 벤치마크(benchmark)가 챗봇(chatbot)의 성능 수준을 정확하게 평가하기에 신뢰할 수 없다는 것을 발견했습니다." - [4]에서 발췌. [4]에서는 두 가지 스타일의 LLM 기반 평가가 사용됩니다. 첫 번째 설정에서 GPT-4는 ChatGPT와 다른 모델의 응답으로 프롬프트(prompt)되고 다음을 요청받습니다.

*   두 응답 모두에 [1, 10] 범위의 점수를 할당합니다.
*   이 점수에 대한 설명을 제공합니다.

특히, 이 설정은 Vicuna [2]에서 사용된 자동 평가 전략과 정확히 일치합니다. 여기에서 모델의 성능은 ChatGPT에 대한 상대적인 성능으로 보고됩니다. 더 구체적으로, 우리는 각 모델이 달성한 총 점수 합계와 ChatGPT의 총 점수 간의 비율을 측정합니다. Guanaco 모델은 ChatGPT에 비해 인상적인 성능을 달성하는 것으로 나타났습니다. 두 번째 설정에서 GPT-4는 모델 출력 간의 직접 비교를 수행합니다. 이러한 비교는 GPT-4에 세 가지 클래스(class) 레이블링(labeling) 문제로 제시됩니다. GPT-4는 더 나은 응답을 선택하거나 두 응답 간의 무승부를 선언하도록 프롬프트되며, 선택에 대한 상세한 설명을 제공합니다. 이 접근 방식을 사용하여 [4]의 저자들은 Guanaco, ChatGPT 및 기타 관련 기준 모델(baseline model) 간의 직접 비교(head-to-head comparison)를 수행합니다. 흥미롭게도, [4]에서 우리는 GPT-4가 프롬프트(prompt)의 첫 번째 응답에 대한 명확한 위치 편향(position bias)을 보여주며, 이는 위치 전환 기법(position switching trick)을 사용하여 제거된다는 것을 알 수 있습니다.

**독점 LLM 모방의 잘못된 약속.**

LLaMA와 이 모델의 결과로 생성된 많은 미세 조정(finetune)된 LLM 이후, 오픈 LLM을 둘러싼 많은 모멘텀(momentum)이 있었습니다. 당시 LLaMA의 많은 미세 조정(finetune) 버전은 모방 전략(imitation strategy)을 사용하여 훈련되었습니다. 즉, 우리는 더 강력한 모델(예: ChatGPT)로 다양하고 많은 프롬프트(prompt) 세트에 대한 응답을 생성하고 이 데이터(data)를 통해 오픈 모델을 직접 미세 조정(finetune)합니다. 이 모델들은 매우 잘 작동하는 것처럼 보였고, 이는 오픈 LLM과 폐쇄형 LLM 간의 성능 격차가 빠르게 사라질 수 있음을 시사했습니다. 그러나 [5]에서는 더 목표 지향적인 평가가 이러한 오픈 모방 모델(imitation model)의 성능에 대한 더 명확하고 냉철한 그림을 그립니다. "처음에 우리는 모방 모델(imitation model)의 출력 품질에 놀랐습니다… 더 목표 지향적인 자동 평가를 수행했을 때, 우리는 모방 모델(imitation model)이 기본 LM에서 ChatGPT까지의 격차를 거의 또는 전혀 좁히지 못한다는 것을 발견했습니다." - [5]에서 발췌. 간단히 말해, 모방 모델(imitation model)은 ChatGPT의 스타일(style)을 모방하는 데 뛰어나며, 이는 인간 주석자(annotator)를 속여 모델 출력을 고품질로 인식하게 할 수 있습니다. 그러나 이러한 모델은 ChatGPT와 같은 더 강력한 LLM의 사실성(factuality)이 부족합니다. 이는 스타일적 유사성이 실제 지식이나 추론 능력과 반드시 일치하지 않음을 보여주는 중요한 교훈입니다.

**미래 전망: LLM 평가의 진화**

LLM-as-a-Judge 기술은 초기 단계를 넘어 지속적으로 발전하고 있습니다. 이제 우리는 단순히 '더 나은' 모델을 찾는 것을 넘어, 특정 도메인(domain)이나 사용 사례에 최적화된 평가 기준을 LLM에게 가르치는 방향으로 나아가고 있습니다. 예를 들어, 법률 문서 요약이나 의료 정보 질의응답과 같은 전문 분야에서는 일반적인 '유용성'보다는 '정확성'과 '안전성'이 훨씬 중요합니다. 이러한 미묘한 차이를 LLM 평가자가 이해하고 반영하도록 하는 것이 다음 과제입니다.

또한, LLM 기반 평가의 신뢰성을 더욱 높이기 위해, 평가 과정에 대한 투명성과 설명 가능성(explainability)을 강화하는 연구가 활발히 진행 중입니다. 단순히 점수만 제시하는 것이 아니라, 왜 그런 점수를 부여했는지, 어떤 기준에 따라 판단했는지에 대한 상세한 근거를 LLM이 스스로 생성하도록 유도하는 방법들이 탐구되고 있습니다. 이는 평가 결과의 신뢰도를 높일 뿐만 아니라, 개발자들이 모델의 약점을 정확히 파악하고 개선하는 데 결정적인 통찰을 제공합니다.

궁극적으로, LLM-as-a-Judge는 모델 개발의 CI/CD (Continuous Integration/Continuous Deployment) 파이프라인에 통합되어, 새로운 모델 버전이 출시될 때마다 자동으로 평가를 수행하고 피드백을 제공하는 자율적인 시스템의 핵심 구성 요소가 될 것입니다. 이러한 자율 평가 시스템은 인간의 개입을 최소화하면서도, LLM의 품질과 안전성을 지속적으로 보장하는 데 필수적인 역할을 할 것입니다. LLM이 스스로를 평가하고 개선하는 시대로의 전환은 인공지능 연구의 새로운 지평을 열어줄 것입니다.