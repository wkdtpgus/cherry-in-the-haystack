점점 더 강력해지는 언어 모델(language model)들이 놀라운 역량을 선보이며 기술의 진보를 이끌어왔습니다. 하지만 과거의 성과가 미래를 온전히 대변할 수 있을까요? 많은 이들은 현재의 성장 곡선이 인공 일반 지능(artificial general intelligence, AGI)이라는 궁극적인 목표에 도달할 때까지 지속될 것이라고 낙관합니다. 그러나 이러한 시각은 여러 오해와 근거 없는 가정에 기반을 두고 있습니다. 스케일링(scaling)의 예측 가능성으로 보이는 현상은 실제 연구 결과에 대한 잘못된 해석에서 비롯된 경우가 많습니다. 더욱이, 대규모 언어 모델(LLM) 개발자들은 이미 고품질 훈련 데이터(training data) 확보에 어려움을 겪고 있으며, 업계 전반에서는 모델 크기를 줄이려는 움직임이 뚜렷하게 관찰되고 있습니다. 스케일링(scaling)이 인공지능(AI)의 발전에 얼마나 기여할지는 미지수이지만, 우리는 단순히 규모를 키우는 것만으로는 인공 일반 지능(AGI)에 도달하기 어렵다고 판단합니다.

**스케일링 "법칙"에 대한 오해와 현실**
스케일링 법칙(scaling laws)에 대한 연구는 모델의 크기, 훈련에 소요되는 연산량(training compute), 데이터셋(dataset)의 규모를 늘릴수록 언어 모델의 성능이 향상된다는 점을 보여줍니다. 이러한 성능 개선은 그 예측 가능성 면에서 매우 놀라울 뿐 아니라, 여러 자릿수(orders of magnitude)에 걸쳐 일관되게 나타납니다. 많은 AI 기업들이 더 크고 강력한 모델을 꾸준히 출시하고 있다는 점을 고려할 때, 이러한 경향이 미래에도 계속될 것이라고 믿는 주된 이유이기도 합니다. 하지만 이는 스케일링 법칙(scaling laws)을 완전히 잘못 이해한 결과입니다. 여기서 말하는 "더 좋은" 모델이란 정확히 무엇을 의미할까요? 스케일링 법칙(scaling laws)은 오직 퍼플렉시티(perplexity)의 감소, 즉 모델이 주어진 시퀀스(sequence)에서 다음 단어를 얼마나 정확하게 예측하는지의 개선만을 측정합니다. 그러나 퍼플렉시티(perplexity)는 최종 사용자에게는 거의 중요하지 않습니다. 진정으로 중요한 것은 "새로운 능력(emergent abilities)"입니다. 이는 모델의 크기가 커짐에 따라 이전에 없던 새로운 기능이 나타나는 현상을 의미합니다. 하지만 이러한 새로운 능력(emergence)은 어떤 특정한 법칙에 따라 나타나는 것이 아닙니다. 지금까지 모델의 규모가 커지면서 새로운 능력이 발현된 것은 사실이지만, 이것이 무한정 계속될 것이라는 경험적인 규칙성은 존재하지 않습니다. 새로운 능력(emergence)이 영원히 지속되지 않을 수도 있다는 점은 대규모 언어 모델(LLM)의 핵심적인 논쟁 중 하나로 이어집니다. 과연 모델은 훈련 데이터(training data)에서 보지 못한 작업까지 처리할 수 있는 외삽(extrapolation) 능력을 지니고 있을까요, 아니면 단순히 훈련 데이터에 나타난 작업만을 학습하는 것일까요? 현재까지의 증거는 불완전하며, 다양한 해석이 가능합니다. 그러나 우리는 회의적인 시각에 더 무게를 둡니다. 대규모 언어 모델(LLM)은 훈련(training) 과정에서 접하지 못한 작업을 해결하는 능력의 효율성을 평가하도록 설계된 벤치마크(benchmark)에서 대체로 저조한 성능을 보입니다. 만약 대규모 언어 모델(LLM)이 훈련 시 본 것 이상을 크게 벗어나지 못한다면, 어느 시점에는 더 많은 데이터가 더 이상 큰 도움이 되지 않을 것입니다. 모든 전통적인 기계 학습(machine learning) 모델이 결국 정체기에 도달하듯이, 대규모 언어 모델(LLM) 또한 다르지 않을 수 있습니다.

단순한 스케일링을 넘어, AI의 진정한 발전은 모델의 견고성(robustness)과 일반화(generalization) 능력에 달려 있습니다. 특정 벤치마크에서 높은 점수를 얻는 것을 넘어, 실제 환경에서 예측 불가능한 다양한 상황과 데이터에 얼마나 유연하게 대응하고, 학습한 지식을 새로운 문제에 적용할 수 있는지가 중요해지고 있습니다. 이는 단순히 다음 단어를 예측하는 퍼플렉시티 개선과는 다른 차원의 문제입니다. 현재의 벤치마크는 종종 특정 작업에 특화되어 있어, 모델이 얼마나 '생각'하는지보다는 얼마나 '암기'하고 '패턴을 잘 찾는'지를 평가하는 경향이 있습니다. 따라서 AI의 '지능'을 더 깊이 있게 측정할 수 있는 새로운 평가 프레임워크와 방법론에 대한 논의가 활발히 진행되고 있습니다.

**고품질 훈련 데이터의 한계**
지속적인 스케일링(scaling)의 또 다른 중대한 장벽은 고품질 훈련 데이터(training data)의 확보 문제입니다. 현재 기업들은 이미 접근 가능한 모든 데이터 소스(data source)를 최대한 활용하고 있습니다. 과연 더 많은 데이터를 얻을 수 있을까요? 실제로는 생각보다 가능성이 낮습니다. 때때로 사람들은 유튜브(YouTube)의 모든 콘텐츠를 텍스트로 변환(transcribing)하는 것과 같은 새로운 데이터 소스(data source)가 사용 가능한 데이터의 양을 한두 자릿수 더 늘릴 수 있을 것이라고 가정합니다. 실제로 유튜브(YouTube)에는 1,500억 분에 달하는 엄청난 양의 비디오가 존재합니다. 하지만 대부분의 비디오에는 유용하게 활용할 수 있는 오디오(audio)가 거의 없거나 전혀 없다는 점(대신 음악, 정지 이미지, 비디오 게임 영상 등이 많음)을 고려하면, 이는 라마 3(Llama 3)가 이미 사용하고 있는 15조 토큰(token)보다 훨씬 적은 추정치에 도달하게 됩니다. 이는 전사된 유튜브(YouTube) 오디오(audio)의 중복 제거(deduplication) 및 품질 필터링(quality filtering) 과정을 거치기 전의 수치이며, 이 과정에서 최소 한 자릿수 이상이 더 줄어들 가능성이 높습니다.

기업들이 언제 훈련 데이터(training data)가 "고갈될" 것인지에 대한 논의는 종종 이루어지지만, 사실 이는 의미 있는 질문이 아닙니다. 훈련 데이터는 항상 존재하지만, 이를 확보하는 데 드는 비용은 기하급수적으로 증가할 것입니다. 특히 이제 저작권자(copyright holders)들이 자신의 권리에 대한 인식이 높아져 보상을 요구하고 있기 때문에, 이러한 비용은 더욱 가파르게 상승할 수 있습니다. 금전적인 비용 외에도, 데이터 수집 관행에 대한 사회적 반발이 커지면서 평판 및 규제 관련 비용 또한 발생할 수 있습니다. 어떤 지수적 추세도 무한정 지속될 수 없다는 것은 분명합니다. 그러나 기술 추세가 언제 정체기에 접어들지 예측하기는 어려울 수 있습니다. 특히 성장이 점진적으로 멈추기보다 갑작스럽게 중단될 때 이러한 예측은 더욱 어렵습니다. 추세선 자체만으로는 정체기에 접어들 것이라는 단서를 전혀 얻을 수 없습니다.

두 가지 유명한 예시는 2000년대의 CPU 클럭 속도와 1970년대의 비행기 속도입니다. CPU 제조업체들은 클럭 속도를 더 높이는 것이 너무 비용이 많이 들고 대부분 무의미하다고 판단하여(CPU가 더 이상 전체 성능의 병목 현상(bottleneck)이 아니었기 때문에), 이 차원에서 경쟁을 중단하기로 결정했고, 이는 클럭 속도에 대한 상승 압력을 갑자기 제거했습니다. 비행기의 경우, 이야기는 더 복잡하지만, 시장이 속도보다 연료 효율성을 우선시하는 것으로 귀결됩니다.

대규모 언어 모델(LLM)의 경우, 우리는 아직 몇 자릿수(orders of magnitude)의 스케일링(scaling) 여지가 남아있을 수도 있고, 이미 끝났을 수도 있습니다. CPU와 비행기의 경우와 마찬가지로, 궁극적으로는 사업적 결정이며 미리 예측하기는 근본적으로 어렵습니다. 연구 분야에서는 점점 더 큰 데이터셋(dataset)을 구축하는 것에서 훈련 데이터(training data)의 품질을 개선하는 것으로 초점이 이동했습니다. 신중한 데이터 정제(data cleaning) 및 필터링(filtering)은 훨씬 더 작은 데이터셋(dataset)으로도 동일하게 강력한 모델을 구축할 수 있게 합니다.

고품질 데이터의 희소성은 특정 유형의 정보에 더욱 두드러집니다. 예를 들어, 복잡한 추론 과정이 담긴 전문가 지식, 특정 산업 분야의 도메인 지식, 또는 저자원 언어(low-resource languages) 데이터는 그 자체로 희귀하며 수집이 매우 어렵습니다. 또한, 텍스트 데이터의 한계를 넘어 이미지, 비디오, 오디오 등 다양한 양식의 데이터를 통합하는 멀티모달(multimodal) 모델의 경우, 각 양식별 데이터의 품질과 정합성을 맞추는 데 새로운 도전 과제가 따릅니다. 이러한 '어두운 데이터(dark data)'—즉, 존재하지만 접근하기 어렵거나 활용하기 어려운 데이터—의 문제는 스케일링의 근본적인 한계로 작용합니다.

**합성 데이터: 만능 해결책인가?**
합성 데이터(synthetic data)는 종종 AI의 지속적인 스케일링(scaling)을 위한 해결책으로 제시됩니다. 즉, 현재의 모델이 다음 세대 모델을 훈련시키기 위한 데이터를 스스로 생성할 수 있다는 아이디어입니다. 그러나 우리는 이러한 관점이 오해에 기반을 두고 있다고 생각합니다. 개발자들이 훈련 데이터(training data)의 양을 늘리기 위해 합성 데이터(synthetic data)를 사용하고 있거나(또는 사용할 수 있다고) 믿기 어렵습니다. 관련 연구 논문들을 살펴보면, 훈련(training)을 위한 합성 데이터(synthetic data)의 훌륭한 사용 사례들은 주로 특정 격차를 해소하거나 수학, 코드, 또는 저자원 언어(low-resource languages)와 같은 특정 도메인(domain)에서의 개선에 초점을 맞추고 있습니다. 예를 들어, 엔비디아(Nvidia)의 네모트론 340B(Nemotron 340B) 모델은 합성 데이터(synthetic data) 생성을 통해 모델 정렬(alignment)을 주요 사용 사례로 삼고 있습니다. 몇 가지 보조적인 활용 사례가 있지만, 현재의 사전 훈련(pre-training) 데이터 소스(data source)를 완전히 대체하는 것은 그중 하나가 아닙니다. 요컨대, 의미 없는 합성 훈련 데이터(synthetic training data)의 무분별한 생성이 고품질의 인간 데이터(human data)를 더 많이 확보하는 것과 동일한 효과를 낼 가능성은 낮습니다.

물론 합성 훈련 데이터(synthetic training data)가 엄청난 성공을 거둔 사례도 있습니다. 2016년 바둑 세계 챔피언을 꺾은 알파고(AlphaGo)와 그 후속작인 알파고 제로(AlphaGo Zero), 알파제로(AlphaZero)가 대표적입니다. 이 시스템들은 스스로 게임을 플레이하며 학습했습니다. 특히 후자 두 모델은 인간의 게임 데이터를 훈련 데이터(training data)로 사용하지 않았습니다. 대신, 상당한 양의 계산을 사용하여 어느 정도 고품질의 게임을 생성했고, 그 게임들을 신경망(neural network) 훈련에 활용했으며, 이는 다시 계산과 결합될 때 훨씬 더 고품질의 게임을 생성할 수 있게 하여 반복적인 개선 루프(iterative improvement loop)를 만들었습니다. 이러한 자가 플레이(self-play) 방식은 '시스템 2(System 2) --> 시스템 1(System 1) 증류(distillation)'의 전형적인 예시로, 느리고 비용이 많이 드는 '시스템 2(System 2)' 프로세스가 빠르고 저렴한 '시스템 1(System 1)' 모델을 훈련하기 위한 데이터를 생성하는 방식입니다. 이는 바둑과 같이 완전히 독립적인 환경을 가진 게임에 매우 효과적입니다. 자가 플레이(self-play)를 게임 외의 도메인(domain)에 적용하는 것은 가치 있는 연구 방향이지만, 언어 번역(language translation)과 같이 훨씬 더 개방적인 작업에 대해서는 무한한 자가 개선을 기대하기 어렵습니다. 자가 플레이(self-play)를 통해 상당한 개선을 허용하는 도메인(domain)은 일반적인 규칙이라기보다는 예외적인 경우로 보아야 합니다.

합성 데이터는 특정 목적을 위한 '데이터 증강(data augmentation)'이나 '결함 데이터 보완(data imputation)' 등에서 강력한 도구로 활용될 수 있습니다. 예를 들어, 자율주행 차량의 훈련을 위해 현실에서 만나기 힘든 극단적인 상황(예: 특정 기상 조건, 드문 사고 유형)을 시뮬레이션하여 합성 데이터를 생성하는 것은 매우 유용합니다. 또한, 민감한 개인 정보 보호가 필요한 의료 분야에서는 실제 환자 데이터를 직접 사용하기 어려울 때 프라이버시를 보장하는 합성 데이터를 생성하여 모델을 훈련할 수 있습니다. 그러나 일반적인 언어 이해 및 생성 모델의 사전 훈련(pre-training)에 있어, 모델이 생성한 데이터를 다시 모델 훈련에 사용하는 '모델 붕괴(model collapse)' 현상이 발생할 수 있다는 우려가 커지고 있습니다. 이는 모델이 현실 세계의 다양성을 잃고 자신이 생성한 데이터의 편향을 증폭시키며, 결국 성능 저하로 이어질 수 있음을 의미합니다. 따라서 합성 데이터의 활용은 신중한 접근과 함께 인간 전문가의 검증 및 개입이 필수적입니다.

**효율성 중심의 모델 개발**
역사적으로 스케일링(scaling)의 세 가지 축인 데이터셋(dataset) 크기, 모델 크기, 훈련 연산량(training compute)은 최적의 균형을 유지하며 함께 발전해왔습니다. 그러나 만약 이 축 중 하나(고품질 데이터)가 병목 현상(bottleneck)이 된다면 어떻게 될까요? 나머지 두 축인 모델 크기와 훈련 연산량(training compute)은 계속해서 스케일링될까요? 현재 시장 추세에 따르면, 단순히 더 큰 모델을 구축하는 것은 새로운 능력(emergent capabilities)을 잠금 해제할 수 있다 하더라도 현명한 사업적 결정으로 보이지 않습니다. 이는 모델의 능력이 더 이상 시장 채택의 유일한 장벽이 아니기 때문입니다. 다시 말해, 현재 대규모 언어 모델(LLM)의 능력으로 구현 가능한 많은 애플리케이션(application)이 존재하지만, 비용 등의 현실적인 문제로 인해 개발되거나 채택되지 못하고 있습니다. 이러한 현상은 특히 코드 생성(code generation)과 같이 작업을 완료하기 위해 대규모 언어 모델(LLM)을 수십 또는 수백 번 호출해야 하는 "에이전트(agentic)" 워크플로우(workflow)에서 두드러집니다.

지난 한 해 동안, 개발 노력의 상당 부분은 주어진 능력 수준에서 더 작고 효율적인 모델을 생산하는 데 집중되었습니다. 선도적인 모델 개발자들이 더 이상 모델 크기를 공개하지 않기 때문에 우리는 이를 확신할 수는 없지만, API 가격을 크기의 대략적인 대리 지표(proxy)로 사용하여 합리적인 추측을 할 수 있습니다. GPT-4o는 GPT-4와 비교하여 기능 면에서 비슷하거나 더 뛰어나면서도 비용은 25%에 불과합니다. 우리는 앤트로픽(Anthropic)과 구글(Google)에서도 동일한 패턴을 관찰할 수 있습니다. 클로드 3 오푸스(Claude 3 Opus)는 클로드(Claude) 제품군에서 가장 비싸고 (아마도 가장 큰) 모델이지만, 더 최근에 출시된 클로드 3.5 소네트(Claude 3.5 Sonnet)는 5배 더 저렴하면서도 더 뛰어난 성능을 보입니다. 마찬가지로, 제미니 1.5 프로(Gemini 1.5 Pro)는 제미니 1.0 울트라(Gemini 1.0 Ultra)보다 더 저렴하면서도 더 우수합니다. 따라서 세 개발자 모두에게서 가장 큰 모델이 가장 뛰어난 모델은 아니라는 결론에 도달합니다!

반면에 훈련 연산량(training compute)은 당분간 계속 스케일링(scaling)될 것으로 보입니다. 역설적으로, 더 작은 모델은 동일한 성능 수준에 도달하기 위해 더 많은 훈련을 필요로 합니다. 따라서 모델 크기에 대한 하향 압력은 훈련 연산량(training compute)에 상향 압력을 가하고 있습니다. 사실상 개발자들은 훈련 비용과 추론 비용(inference cost) 사이에서 균형을 찾고 있습니다. GPT-3.5 및 GPT-4와 같은 초기 모델들은 모델의 수명 동안 발생하는 추론 비용(inference cost)이 훈련 비용을 지배한다고 여겨지는 점에서 "덜 훈련되었다(under-trained)"고 볼 수 있습니다. 이상적으로는 훈련 비용과 추론 비용(inference cost)을 서로 절충하는 것이 항상 가능하므로, 이 둘은 대략 같아야 합니다. 이러한 추세의 주목할 만한 예시로, 라마 3(Llama 3)는 80억 매개변수(parameter) 모델에 대해 원래 라마(Llama) 모델이 거의 동일한 크기(70억)에서 사용했던 것보다 20배 많은 훈련 FLOPs(training FLOPs)를 사용했습니다.

이러한 효율성 중심의 개발은 AI의 접근성과 민주화에 크게 기여합니다. 더 작고 효율적인 모델은 스마트폰, IoT 기기 등 엣지 디바이스(edge devices)에서도 AI를 구동할 수 있게 하여 새로운 애플리케이션의 가능성을 열어줍니다. 또한, 클라우드 기반 AI 서비스의 추론 비용을 낮춰 더 많은 기업과 개발자가 AI를 활용할 수 있도록 돕습니다. 모델 경량화 기술(quantization, pruning), 지식 증류(knowledge distillation) 및 효율적인 아키텍처(예: Mixtral과 같은 Mixture-of-Experts 모델) 연구가 활발히 진행되는 것도 이러한 추세를 반영합니다. 이는 단순히 규모를 키우는 경쟁에서 벗어나, 실제 사용 환경에서의 가치와 지속 가능성에 초점을 맞춘 AI 발전의 새로운 방향을 제시합니다.

**인공 일반 지능(AGI)의 재정의와 실용적 관점**
스케일링(scaling)을 통한 능력 개선이 더 이상 크게 나타나지 않을 가능성과 일치하는 한 가지 징후는, AI 기업의 최고 경영자(CEO)들이 인공 일반 지능(AGI)에 대한 기대를 상당 부분 낮추고 있다는 것입니다. 불행히도 그들은 순진한 "3년 내 인공 일반 지능(AGI) 달성"이라는 예측이 틀렸음을 인정하기보다는, 인공 일반 지능(AGI)의 의미를 너무 희석시켜 이제는 사실상 무의미하게 만듦으로써 체면을 살리려는 경향을 보입니다. 애초에 인공 일반 지능(AGI)이 명확하게 정의된 적이 없다는 점도 이러한 혼란을 가중시켰습니다.

우리는 일반성(generality)을 이진적인 개념(binary concept)으로 보기보다는, 하나의 스펙트럼(spectrum)으로 이해할 수 있습니다. 역사적으로 컴퓨터가 새로운 작업을 프로그래밍(program)하는 데 필요한 노력의 양은 지속적으로 감소해왔습니다. 우리는 이러한 흐름을 일반성(generality)의 증가로 해석할 수 있습니다. 이러한 추세는 특수 목적 컴퓨터(special-purpose computers)에서 범용적인 튜링 머신(Turing machines)으로의 전환과 함께 시작되었습니다. 이러한 맥락에서 대규모 언어 모델(LLM)의 범용성(general-purpose nature)은 완전히 새로운 현상이 아닙니다. 이는 우리의 책 "AI 스네이크 오일(AI Snake Oil)"에서 인공 일반 지능(AGI)에 대한 장(chapter)에서 다루는 관점과 일치합니다. 우리는 인공지능(AI)의 역사를 단속 평형(punctuated equilibrium)의 개념으로 이해하며, 이를 일반성(generality)의 사다리(선형적인 발전을 의미하지는 않음)라고 부릅니다. 지시 튜닝(instruction-tuned)된 대규모 언어 모델(LLM)은 이 사다리의 가장 최근 단계입니다.

인공지능(AI)이 인간만큼 효과적으로 경제적으로 가치 있는 어떤 작업이든 수행할 수 있는 일반성(generality) 수준(이는 인공 일반 지능(AGI)의 한 가지 정의입니다)에 도달하기까지는 아직 알 수 없는 수많은 단계가 남아있습니다. 역사적으로, 이 사다리의 각 단계에 서 있던 AI 연구 커뮤니티는 현재의 패러다임(paradigm)으로 얼마나 더 나아갈 수 있을지, 다음 단계는 무엇일지, 언제 도달할지, 어떤 새로운 애플리케이션(application)을 가능하게 할지, 그리고 안전에 대한 함의는 무엇인지 예측하는 데 매우 서툴렀습니다. 우리는 이러한 경향이 앞으로도 계속될 것이라고 생각합니다.

진정한 "지능"에 대한 정의는 단순히 기술적인 성능 지표를 넘어섭니다. 인간의 지능은 인지, 감성, 사회성, 윤리적 판단 등 복합적인 요소를 포함합니다. 현재의 LLM은 놀라운 언어 능력을 보여주지만, 이는 특정 패턴 인식과 통계적 예측에 기반한 것이며, 실제 세계에 대한 깊은 이해나 상식적인 추론, 자의식과는 거리가 있습니다. 따라서 '유용한 AI(useful AI)'와 '일반적인 AI(general AI)'를 구분하는 것이 중요합니다. 현재의 AI 발전은 특정 도메인에서 인간의 작업을 보조하거나 자동화하는 데 매우 유용하지만, 이는 인간과 같은 보편적인 지능을 의미하지 않습니다. 많은 연구자들은 인공지능이 인간과 협력하여 시너지를 내는 '증강 지능(augmented intelligence)'의 방향으로 나아가는 것이 더욱 현실적이고 가치 있는 목표라고 주장합니다.

**지속 가능한 AI 발전의 길**
AI 기술의 발전은 단순히 성능 향상이나 스케일링에만 국한되지 않습니다. 이제는 기술이 사회에 미치는 영향, 윤리적 고려사항, 환경적 지속 가능성 등 다각적인 측면에서 접근해야 할 시점입니다. 대규모 모델의 훈련과 운영에 필요한 막대한 에너지 소비는 환경 문제와 직결되며, 이는 AI 개발의 지속 가능성에 대한 중요한 질문을 던집니다. 또한, AI 모델의 편향성(bias), 투명성(transparency) 부족, 책임성(accountability) 문제 등은 기술의 사회적 수용도를 결정하는 핵심 요소가 되고 있습니다.

설명 가능한 AI(Explainable AI, XAI)와 공정성(Fairness) 연구는 모델이 왜 특정 결정을 내렸는지 이해하고, 불공정한 결과를 초래할 수 있는 편향을 식별하고 완화하는 데 중점을 둡니다. 이는 AI 시스템에 대한 신뢰를 구축하고, 규제 기관과 사용자 모두에게 AI의 작동 원리를 명확히 제시하는 데 필수적입니다. 데이터 프라이버시(data privacy)와 보안(security) 또한 AI 시대의 중요한 화두로, 개인 정보 보호와 데이터 활용 사이의 균형점을 찾는 노력이 지속적으로 요구됩니다.

결론적으로, 인공지능의 미래는 단순히 모델의 크기를 키우는 '스케일링 경쟁'을 넘어설 것입니다. 고품질 데이터의 효율적인 활용, 모델의 경량화 및 최적화, 그리고 무엇보다도 윤리적이고 사회적 책임을 다하는 AI 개발이 중요해지고 있습니다. AI는 우리의 삶을 풍요롭게 할 잠재력을 가지고 있지만, 그 잠재력을 온전히 실현하기 위해서는 기술적 진보와 함께 사회적, 윤리적 성숙이 동반되어야 할 것입니다.

**추가 자료**
레오폴드 아셴브레너(Leopold Aschenbrenner)의 최근 에세이는 "2027년까지 인공 일반 지능(AGI)이 놀랍도록 그럴듯하다"는 주장으로 큰 파장을 일으켰습니다. 우리는 여기서 그의 주장을 조목조목 반박하려고 시도하지 않았습니다. 이 글의 대부분은 아셴브레너(Aschenbrenner)의 에세이가 발표되기 전에 작성되었습니다. 그의 타임라인(timeline)에 대한 주장은 흥미롭고 생각을 자극하지만, 근본적으로는 추세선 외삽(trendline extrapolation)의 연습에 불과합니다. 또한, 많은 인공지능(AI) 지지자들처럼 그는 벤치마크(benchmark) 성능과 실제 유용성을 혼동합니다. 멜라니 미첼(Melanie Mitchell), 얀 르쿤(Yann LeCun), 게리 마커스(Gary Marcus), 프랑수아 숄레(Francois Chollet), 수바라오 캄밤파티(Subbarao Kambhampati) 등을 포함한 많은 인공지능(AI) 연구자들이 회의적인 주장을 펼쳤습니다. 드와르케시 파텔(Dwarkesh Patel)은 논쟁의 양측에 대한 좋은 개요를 제공합니다.

**감사 말씀.**
초안에 대한 피드백을 주신 맷 살가닉(Matt Salganik), 올리 스티븐슨(Ollie Stephenson), 베네딕트 스트뢰블(Benedikt Ströbl)께 감사드립니다.

**주석**
1 새로운 능력(Emergent abilities)은 불연속적으로 변하는 대신 부드럽게 변하는 측정 기준(metric)을 찾을 수 있다면 예측 가능할 것입니다. 하지만 특히 여러 기술의 조합을 요구하는 작업의 경우 그러한 측정 기준(metric)을 찾는 것은 쉽지 않습니다. 실제로는 다음 자릿수(order of magnitude)에서 어떤 새로운 능력이 나타날지 여부는 여전히 아무도 모릅니다.
2 인공지능(AI) 기업들은 훈련(training)을 위해 전사된 유튜브(YouTube) 데이터를 사용하지만, 그 가치는 데이터의 양 때문이 아니라 대규모 언어 모델(LLM)이 음성 대화가 어떻게 들리는지 학습하는 데 도움이 되기 때문입니다.
3 자유지상주의(Libertarian) 논평가들은 비행기 속도의 정체를 전적으로 규제 탓으로 돌리지만, 이는 틀렸거나 기껏해야 지나치게 단순화된 주장입니다. 1973년 미국 연방항공청(FAA)이 민간 항공기의 육상 초음속 비행을 사실상 금지한 것은 사실입니다. 하지만 가장 빠른 항공기는 모두 군용이므로 이 금지 조치에 영향을 받지 않습니다. 그리고 민간 항공기는 연료 효율성 및 기타 고려 사항으로 인해 마하 1(Mach 1)보다 훨씬 낮은 속도로 순항합니다.
4 대규모 언어 모델(LLM) 훈련을 몇 자릿수(orders of magnitude) 더 샘플 효율적(sample efficient)으로 만들 수 있는지에 대한 논쟁이 있습니다. 결국, 아이들은 대규모 언어 모델(LLM)보다 훨씬 적은 단어에 노출된 후 언어를 습득합니다. 반면에 아이들은 "요람 속의 과학자(scientists in the crib)"로서, 일찍부터 세계 모델(world models)과 추론 능력(reasoning abilities)을 개발하며, 이것이 효율적인 언어 습득을 가능하게 할 수도 있습니다. 이 논쟁은 우리의 요점과는 무관합니다. 만약 작업 표현(task representation) 또는 외삽(extrapolation)의 어려움이 병목 현상(bottleneck)이라면, 이는 샘플 효율성(sample efficiency)과 관계없이 대규모 언어 모델(LLM) 능력의 상한선을 나타낼 것입니다.
5 모델 개발자들이 더 큰 모델(매개변수(parameter) 수 기준)을 출시했을 때조차, 미스트랄 8x22B(Mixtral 8x22B)와 같은 전문가 혼합 모델(mixture-of-experts models)에서처럼 추론(inference) 중 활성 매개변수(active parameters)의 수가 전체 매개변수(parameter) 수보다 훨씬 낮은 경우와 같이 추론 효율성(inference efficiency)에 대한 관심이 증가하고 있습니다.