지금까지 인공지능 분야에서 개발된 대규모 언어 모델(LLM)들은 지속적으로 발전하며 놀라운 역량을 선보였습니다. 그러나 이러한 과거의 진보가 미래를 온전히 그려낼 수 있을까요? 일반적인 시각 중 하나는 현재의 성장 추세가 훨씬 더 광범위하게 이어질 것이며, 궁극적으로는 인공 일반 지능(AGI)이라는 목표에 도달할 것이라는 믿음입니다. 하지만 이러한 관점은 여러 가지 오해와 잘못된 신념에 기반하고 있습니다. 규모 확장의 겉보기 예측 가능성은 실제 연구 결과의 본질을 잘못 해석한 데서 비롯됩니다. 더 나아가, 대규모 언어 모델(LLM) 개발자들은 이미 고품질의 훈련 자료(training data) 확보에 상당한 어려움을 겪고 있다는 징후가 나타나고 있습니다. 또한, 업계 전반에서는 모델의 크기를 줄여야 한다는 강력한 압박이 목격되고 있습니다. 비록 우리는 인공지능(AI)이 규모 확장을 통해 얼마나 더 발전할지 정확히 예측할 수는 없지만, 단순히 규모를 키우는 것만으로는 인공 일반 지능(AGI)에 도달할 가능성이 매우 희박하다고 판단합니다.

**규모 확장(scaling)의 "법칙"에 대한 잘못된 인식**
인공지능 연구에서 규모 확장 법칙(scaling laws)에 대한 탐구는 모델의 크기, 학습에 투입되는 연산량(training compute), 그리고 데이터셋(dataset)의 규모를 늘릴수록 언어 모델의 성능이 "향상된다"는 점을 명확히 보여줍니다. 이러한 성능 개선은 예측 가능성 측면에서 실로 놀라웠으며, 여러 자릿수에 걸쳐 일관되게 나타났습니다. 이것이 바로 수많은 사람들이 선도적인 인공지능(AI) 기업들이 더욱 거대하고 강력한 모델을 주기적으로 선보임에 따라, 규모 확장이 예측 가능한 미래에도 지속될 것이라고 여기는 주된 이유입니다. 하지만 이는 규모 확장 법칙(scaling laws)의 본질을 완전히 오해한 것입니다. 과연 "더 나은" 모델이란 정확히 무엇을 의미할까요? 규모 확장 법칙(scaling laws)은 오직 퍼플렉시티(perplexity)의 감소, 즉 모델이 주어진 문맥에서 다음 단어를 얼마나 정확하게 예측하는지의 개선만을 수치화합니다. 물론, 퍼플렉시티(perplexity)는 최종 사용자에게는 거의 무의미한 지표입니다. 진정으로 중요한 것은 "새로운 능력(emergent abilities)", 다시 말해 모델의 크기가 커짐에 따라 이전에 없던 기능이 발현되는 경향입니다. 이러한 새로운 능력(emergence)의 출현은 어떠한 명확한 법칙에 의해 지배되지 않습니다. 지금까지는 규모의 증가가 새로운 능력의 등장을 가져왔다는 것은 사실입니다. 그러나 이것이 무한정 계속될 것이라는 경험적 규칙성은 존재하지 않습니다. 1 새로운 능력(emergence)이 영원히 이어지지 않을 수도 있는 이유는 무엇일까요? 이 질문은 대규모 언어 모델(LLM)의 핵심 역량에 대한 중요한 논쟁 중 하나로 이어집니다. 즉, 모델이 훈련 데이터(training data)에서 학습한 것을 넘어 새로운 상황에 적용(extrapolation)할 수 있는 능력을 갖추고 있는가, 아니면 단순히 훈련 과정에서 접한 작업만을 재현하는가 하는 문제입니다. 현재까지의 증거는 명확하지 않으며, 이를 해석하는 합리적인 방식 또한 다양합니다. 하지만 우리는 회의적인 시각에 더 기울어 있습니다. 대규모 언어 모델(LLM)은 이전에 경험하지 못한 작업을 해결하기 위한 기술 습득의 효율성을 평가하도록 설계된 벤치마크(benchmark)에서 대체로 저조한 성능을 보입니다. 만약 대규모 언어 모델(LLM)이 훈련 과정에서 본 것 이상을 크게 벗어나지 못한다면, 어느 시점에는 더 많은 데이터가 더 이상 유의미한 도움이 되지 않을 것입니다. 이는 그 안에 표현될 수 있는 모든 작업이 이미 표현되었을 것이기 때문입니다. 모든 전통적인 기계 학습(machine learning) 모델은 결국 성능 정체기에 도달합니다. 아마도 대규모 언어 모델(LLM) 역시 다르지 않을 것입니다. 더 나아가, 퍼플렉시티(perplexity)와 같은 단일 지표에 과도하게 의존하는 것은 측정 대상 자체의 본질을 왜곡할 수 있다는 굿하트의 법칙(Goodhart's Law)과 유사한 함정을 내포합니다. 모델이 단순히 다음 단어를 예측하는 데 능숙해지는 것을 넘어, 실제 세계의 복잡한 문제 해결 능력이나 추론 능력을 평가하기 위해서는 퍼플렉시티(perplexity)를 넘어서는 새로운 평가 기준과 방법론이 절실히 요구됩니다.

**추세의 단순한 외삽(extrapolation)은 근거 없는 추측일 뿐**
지속적인 규모 확장의 또 다른 중대한 장애물은 훈련 데이터(training data)를 확보하는 것입니다. 현재 기업들은 이미 접근 가능한 거의 모든 데이터 자원(data source)을 활용하고 있습니다. 과연 더 많은 데이터를 얻을 수 있을까요? 예상보다 그럴 가능성은 희박합니다. 사람들은 때때로 유튜브(YouTube)의 모든 콘텐츠를 텍스트로 전환(transcribing)하는 것과 같은 새로운 데이터원(data source)이 활용 가능한 데이터의 양을 한두 자릿수 더 늘릴 것이라고 막연히 추정합니다. 실제로 유튜브(YouTube)에는 1,500억 분에 달하는 엄청난 양의 영상물이 존재합니다. 그러나 대부분의 영상에 유용하게 활용될 수 있는 음성 데이터(audio)가 거의 없거나 전혀 없다는 점(예를 들어, 음악, 정지 이미지, 게임 플레이 영상 등)을 고려하면, 우리는 라마 3(Llama 3)가 이미 사용하고 있는 15조 토큰(token)보다 훨씬 적은 추정치에 도달하게 됩니다. 이는 전사된 유튜브(YouTube) 음성 데이터(audio)가 중복 제거(deduplication) 및 품질 필터링(quality filtering) 과정을 거치기 전의 수치이며, 이 과정에서 최소 한 자릿수 이상이 추가로 줄어들 가능성이 높습니다. 2 업계에서는 종종 훈련 데이터(training data)가 언제 "고갈될" 것인지에 대한 논의가 이루어지지만, 이것은 본질적으로 의미 있는 질문이 아닙니다. 훈련 데이터(training data)는 항상 존재할 수 있지만, 이를 획득하는 데 드는 비용은 기하급수적으로 증가할 것입니다. 특히 이제 저작권 소유자(copyright holders)들이 자신들의 권리를 명확히 인지하고 보상을 요구하기 시작하면서, 비용은 더욱 급격하게 상승할 수 있습니다. 금전적인 비용 외에도, 사회가 데이터 수집 관행에 대한 반발을 보이면서 평판 및 규제 관련 비용이 발생할 수 있습니다. 어떠한 지수적 성장 추세도 무한정 지속될 수 없다는 것은 분명한 사실입니다. 그러나 기술 추세가 언제 정체기에 접어들지 예측하기는 매우 어렵습니다. 특히 성장이 점진적으로 둔화되기보다는 갑작스럽게 중단될 때 더욱 그렇습니다. 추세선 자체만으로는 정체기가 임박했다는 어떠한 단서도 찾을 수 없습니다.

시간 경과에 따른 CPU 클럭 속도. y축은 로그 스케일(logarithmic scale)이다. [ 출처 ]

두 가지 유명한 사례는 2000년대의 중앙처리장치(CPU) 클럭 속도와 1970년대의 항공기 속도입니다. CPU 제조사들은 클럭 속도를 더 높이는 것이 지나치게 비효율적이고 대부분 무의미하다고 판단하여(CPU가 더 이상 전체 시스템 성능의 병목 현상(bottleneck)이 아니었기 때문에), 이 차원에서의 경쟁을 중단하기로 결정했고, 이는 클럭 속도 증가에 대한 압력을 갑자기 제거했습니다. 항공기의 경우, 이야기는 더 복잡하지만, 시장이 속도보다는 연료 효율성을 우선시하게 되면서 발생한 결과로 귀결됩니다. 3

시간 경과에 따른 비행 속도 기록. 1976년 SR-71 블랙버드(Blackbird) 기록은 오늘날에도 유효하다. [ 출처 ]

대규모 언어 모델(LLM)의 경우, 우리는 아직 몇 자릿수(orders of magnitude)의 규모 확장(scaling) 여지가 남아있을 수도 있고, 이미 그 한계에 도달했을 수도 있습니다. CPU와 항공기의 사례와 마찬가지로, 궁극적으로는 사업적 판단에 달려 있으며, 미리 예측하기는 근본적으로 어렵습니다. 연구 분야에서는 점차 더 큰 데이터셋(dataset)을 구축하는 것에서 훈련 데이터(training data)의 질을 향상시키는 방향으로 초점이 이동하고 있습니다. 세심한 데이터 정제(data cleaning) 및 필터링(filtering)은 훨씬 더 작은 데이터셋(dataset)으로도 동등하게 강력한 모델을 개발할 수 있도록 합니다. 데이터 고갈 문제는 단순히 양의 부족을 넘어, 데이터의 질과 다양성, 그리고 윤리적 수집의 중요성을 부각시키며, 이는 인공지능 개발의 지속 가능성에 대한 근본적인 질문을 제기합니다.

**합성 데이터(synthetic data)는 만능 해결책이 아니다**
합성 데이터(synthetic data)는 종종 지속적인 규모 확장(scaling)의 해결책으로 제시됩니다. 즉, 현재의 모델이 다음 세대 모델을 위한 훈련 데이터(training data)를 생성하는 데 활용될 수 있다는 주장입니다. 그러나 우리는 이러한 견해가 오해에 기반하고 있다고 생각합니다. 개발자들이 훈련 데이터(training data)의 양을 무작정 늘리기 위해 합성 데이터(synthetic data)를 사용하고 있거나(또는 사용할 수 있다고) 보지 않습니다. 관련 연구들은 훈련(training)을 위한 합성 데이터(synthetic data)의 유용한 활용 사례들을 제시하지만, 이들은 대부분 특정 격차를 해소하고 수학, 코드 또는 저자원 언어(low-resource languages)와 같은 특정 도메인(domain)에서의 개선을 목표로 합니다. 마찬가지로, 합성 데이터(synthetic data) 생성에 초점을 맞춘 엔비디아(Nvidia)의 최근 네모트론 340B(Nemotron 340B) 모델은 정렬(alignment)을 주요 활용 사례로 삼고 있습니다. 몇 가지 보조적인 활용 사례가 있지만, 현재의 사전 훈련(pre-training) 데이터원(data source)을 전면적으로 대체하는 것은 그중 하나가 아닙니다. 요컨대, 단순히 의미 없는 합성 훈련 데이터(synthetic training data)를 대량으로 생성하는 것이 고품질의 인간 생성 데이터(human data)를 더 많이 확보하는 것과 동일한 효과를 낼 가능성은 낮습니다. 2016년 바둑 세계 챔피언을 꺾은 알파고(AlphaGo)와 그 후속작인 알파고 제로(AlphaGo Zero), 알파제로(AlphaZero)와 같이 합성 훈련 데이터(synthetic training data)가 엄청난 성공을 거둔 사례도 물론 존재합니다. 이 시스템들은 스스로 게임을 플레이하며 학습했습니다. 특히 후자 두 모델은 인간의 게임 기록을 훈련 데이터(training data)로 전혀 사용하지 않았습니다. 그들은 상당한 양의 계산 자원을 투입하여 어느 정도 고품질의 게임을 생성했고, 그 게임들을 신경망(neural network) 훈련에 사용했으며, 이는 계산과 결합될 때 훨씬 더 고품질의 게임을 생성할 수 있게 하여 반복적인 개선 루프(iterative improvement loop)를 만들었습니다. 자가 플레이(Self-play)는 "시스템 2(System 2) --> 시스템 1(System 1) 증류(distillation)"의 전형적인 예시로, 느리고 비용이 많이 드는 "시스템 2(System 2)" 프로세스가 빠르고 저렴한 "시스템 1(System 1)" 모델을 훈련하기 위한 훈련 데이터(training data)를 생성하는 방식입니다. 이러한 접근 방식은 완전히 독립적인 환경인 바둑과 같은 게임에 매우 효과적입니다. 자가 플레이(self-play)를 게임 외의 도메인(domain)에 적용하는 것은 가치 있는 연구 방향입니다. 코드 생성(code generation)과 같이 이 전략이 유효할 수 있는 중요한 영역들이 있습니다. 하지만 언어 번역(language translation)과 같이 훨씬 더 개방적이고 광범위한 작업에 대해서는 무한한 자가 개선을 기대하기는 어렵습니다. 자가 플레이(self-play)를 통해 상당한 발전을 이룰 수 있는 도메인(domain)은 일반적인 규칙이라기보다는 예외적인 사례로 보아야 합니다. 게다가, 모델이 생성한 데이터를 다시 모델 훈련에 사용하는 "모델 붕괴(model collapse)" 현상과 같은 잠재적 위험도 존재합니다. 이는 모델이 현실 세계의 다양성을 잃고 자기 참조적인 오류를 증폭시킬 수 있음을 의미하며, 합성 데이터(synthetic data) 활용에 대한 신중한 접근을 요구합니다.

당신은 우리의 책에 대한 블로그인 AI 스네이크 오일(AI Snake Oil)을 읽고 있습니다. 새 게시물을 받으려면 구독하세요. 구독

**모델은 작아지고 있지만 훈련 기간은 길어지고 있다**
역사적으로 규모 확장의 세 가지 주요 축인 데이터셋(dataset) 크기, 모델 크기, 그리고 훈련 연산량(training compute)은 서로 유기적으로 발전해왔으며, 이것이 최적의 방식으로 알려져 있었습니다. 그러나 이 축 중 하나(고품질 데이터)가 병목 현상(bottleneck)이 된다면 어떻게 될까요? 나머지 두 축인 모델 크기와 훈련 연산량(training compute)은 계속해서 확장될 수 있을까요? 현재 시장의 흐름을 보면, 더 거대한 모델을 구축하는 것이 새로운 기능(emergent capabilities)을 발현시킬 수 있다 하더라도, 현명한 사업적 결정으로 보이지 않습니다. 이는 모델의 성능 자체보다는 실제 배포 및 운영 비용이 채택의 주된 장벽이 되고 있기 때문입니다. 다시 말해, 현재 대규모 언어 모델(LLM)의 역량으로 구현 가능한 수많은 애플리케이션(application)이 존재하지만, 높은 추론 비용(inference cost) 등의 이유로 실제 구축되거나 널리 채택되지 못하고 있습니다. 이는 코드 생성(code generation)과 같이 작업을 완료하기 위해 대규모 언어 모델(LLM)을 수십 또는 수백 번 호출해야 하는 "에이전트(agentic)" 워크플로우(workflow)에서 특히 두드러집니다. 지난 한 해 동안, 개발 노력의 상당 부분은 주어진 성능 수준에서 더 작은 모델을 만들어내는 데 집중되었습니다. 5 선도적인 모델 개발자들이 더 이상 모델 크기를 명확히 공개하지 않으므로, 우리는 이를 확신할 수는 없지만, API 가격을 크기의 대략적인 지표(proxy)로 사용하여 합리적인 추측을 할 수 있습니다. GPT-4o는 GPT-4와 비교하여 기능 면에서 동등하거나 더 우수하면서도 비용은 25%에 불과합니다. 우리는 앤트로픽(Anthropic)과 구글(Google)에서도 유사한 경향을 목격합니다. 클로드 3 오푸스(Claude 3 Opus)는 클로드(Claude) 제품군에서 가장 비싸고 (아마도 가장 큰) 모델이지만, 더 최근에 출시된 클로드 3.5 소네트(Claude 3.5 Sonnet)는 5배 더 저렴하면서도 더 뛰어난 성능을 제공합니다. 마찬가지로, 제미니 1.5 프로(Gemini 1.5 Pro)는 제미니 1.0 울트라(Gemini 1.0 Ultra)보다 더 저렴하면서도 더 우수합니다. 따라서 세 개발자 모두에게서 가장 큰 모델이 반드시 가장 뛰어난 모델은 아닙니다! 반면에 훈련 연산량(training compute)은 당분간 지속적으로 증가할 것으로 예상됩니다. 역설적으로, 더 작은 모델은 동일한 성능 수준에 도달하기 위해 더 많은 훈련을 필요로 합니다. 따라서 모델 크기에 대한 하향 압력은 훈련 연산량(training compute)에 상향 압력을 가하고 있습니다. 사실상 개발자들은 훈련 비용과 추론 비용(inference cost) 사이에서 균형점을 찾고 있습니다. GPT-3.5 및 GPT-4와 같은 초기 모델들은 모델의 전체 수명 동안 발생하는 추론 비용(inference cost)이 훈련 비용을 훨씬 초과한다고 여겨지는 점에서 "덜 훈련되었다(under-trained)"고 볼 수 있습니다. 이상적으로는 훈련 비용과 추론 비용(inference cost)을 서로 절충하는 것이 항상 가능하므로, 이 둘은 대략적으로 비슷해야 합니다. 이러한 추세의 주목할 만한 예시로, 라마 3(Llama 3)는 80억 매개변수(parameter) 모델을 훈련하는 데 원래 라마(Llama) 모델이 거의 동일한 크기(70억)에서 사용했던 것보다 20배 많은 훈련 FLOPs(training FLOPs)를 사용했습니다. 이러한 변화는 단순히 모델의 크기를 키우는 것에서 벗어나, 효율적인 아키텍처 설계와 최적화된 훈련 전략을 통해 특정 성능 목표를 달성하려는 노력을 반영하며, 이는 에지 디바이스(edge devices)나 모바일 환경에서의 인공지능 배포 가능성을 높이는 중요한 방향입니다.

**일반성(generality)이라는 사다리**
규모 확장을 통한 능력 개선이 더 이상 크게 나타나지 않을 가능성과 일치하는 한 가지 징후는 주요 기술 기업의 최고 경영자(CEO)들이 인공 일반 지능(AGI)에 대한 기대를 상당 부분 낮추고 있다는 점입니다. 불행히도, 그들은 순진한 "3년 내 인공 일반 지능(AGI)" 예측이 틀렸음을 인정하는 대신, 인공 일반 지능(AGI)의 의미를 너무 희석시켜 이제는 실질적인 의미를 상실하게 만듦으로써 체면을 살리려 하고 있습니다. 애초에 인공 일반 지능(AGI)이 명확하게 정의된 적이 없다는 점도 이러한 혼란에 일조했습니다. 일반성(generality)을 이분법적인 개념으로 보기보다는, 우리는 그것을 연속적인 스펙트럼(spectrum)으로 이해할 수 있습니다. 역사적으로 컴퓨터가 새로운 작업을 수행하도록 프로그래밍(program)하는 데 필요한 노력의 양은 지속적으로 감소해왔습니다. 우리는 이러한 경향을 일반성(generality)의 증가로 해석할 수 있습니다. 이러한 추세는 특수 목적 컴퓨터(special-purpose computers)에서 범용적인 튜링 머신(Turing machines)으로의 전환과 함께 시작되었습니다. 이러한 의미에서 대규모 언어 모델(LLM)의 범용적인 특성(general-purpose nature)은 완전히 새로운 현상은 아닙니다. 이것이 바로 인공 일반 지능(AGI)에 대한 장(chapter)을 할애한 우리의 책 "AI 스네이크 오일(AI Snake Oil)"에서 취하는 관점입니다. 우리는 인공지능(AI)의 역사를 단속 평형(punctuated equilibrium)으로 개념화하며, 이를 일반성(generality)의 사다리(선형적 발전을 의미하지는 않음)라고 부릅니다. 지시 튜닝(instruction-tuned)된 대규모 언어 모델(LLM)은 이 사다리의 가장 최근 단계입니다. 인공지능(AI)이 인간만큼 효율적이고 경제적으로 가치 있는 모든 작업을 수행할 수 있는 일반성(generality) 수준(이는 인공 일반 지능(AGI)의 한 가지 정의입니다)에 도달하기까지는 알 수 없는 수의 단계가 남아있습니다. 역사적으로, 사다리의 각 단계에 서서 인공지능(AI) 연구 커뮤니티는 현재의 패러다임(paradigm)으로 얼마나 더 나아갈 수 있을지, 다음 단계는 무엇일지, 언제 도달할지, 어떤 새로운 애플리케이션(application)을 가능하게 할지, 그리고 안전에 대한 함의는 무엇인지 예측하는 데 매우 서툴렀습니다. 우리는 이러한 추세가 계속될 것이라고 생각합니다. 이러한 관점에서, 인공지능의 진정한 발전을 위해서는 단순히 모델의 규모를 키우는 것을 넘어, 인간의 인지 과정을 모방하거나 새로운 형태의 학습 패러다임을 탐구하는 등 근본적으로 다른 접근 방식이 필요할 수 있습니다.

**추가 자료**
레오폴드 아셴브레너(Leopold Aschenbrenner)의 최근 에세이는 "2027년까지 인공 일반 지능(AGI)이 놀랍도록 그럴듯하다"는 주장으로 큰 파장을 일으켰습니다. 우리는 여기서 그의 주장을 조목조목 반박하려고 시도하지 않았습니다. 이 글의 대부분은 아셴브레너(Aschenbrenner)의 에세이가 발표되기 전에 작성되었습니다. 그의 시간표(timeline)에 대한 주장은 흥미롭고 생각을 자극하지만, 근본적으로는 단지 추세선 외삽(trendline extrapolation)의 연습에 불과합니다. 또한, 많은 인공지능(AI) 지지자들처럼 그는 벤치마크(benchmark) 성능과 실제 세계에서의 유용성(utility)을 혼동합니다. 멜라니 미첼(Melanie Mitchell), 얀 르쿤(Yann LeCun), 게리 마커스(Gary Marcus), 프랑수아 숄레(Francois Chollet), 수바라오 캄밤파티(Subbarao Kambhampati) 등을 포함한 많은 인공지능(AI) 연구자들이 회의적인 주장을 펼쳤습니다. 드와르케시 파텔(Dwarkesh Patel)은 논쟁의 양측에 대한 좋은 개요를 제공합니다. 결국, 벤치마크 점수가 아무리 높아도, 현실 세계의 복잡하고 미묘한 문제들을 해결하는 데 실패한다면 그 의미는 퇴색될 수밖에 없습니다. 진정한 인공지능의 발전은 단순한 성능 지표를 넘어, 사회적 책임과 윤리적 고려를 포함하는 다면적인 접근을 요구합니다.

**감사 말씀.**
초안에 대한 피드백을 주신 맷 살가닉(Matt Salganik), 올리 스티븐슨(Ollie Stephenson), 베네딕트 스트뢰블(Benedikt Ströbl)께 감사드립니다.

1 새로운 능력(Emergent abilities)은 불연속적으로 변하는 대신 부드럽게 변하는 측정 기준(metric)을 찾을 수 있다면 예측 가능할 것입니다. 하지만 특히 여러 기술의 조합을 요구하는 작업의 경우 그러한 측정 기준(metric)을 찾는 것은 쉽지 않습니다. 실제로는 다음 자릿수(order of magnitude)에서 어떤 새로운 능력이 나타날지 여부는 여전히 아무도 모릅니다.
2 인공지능(AI) 기업들은 훈련(training)을 위해 전사된 유튜브(YouTube) 데이터를 사용하지만, 그 가치는 데이터의 양 때문이 아니라 대규모 언어 모델(LLM)이 음성 대화가 어떻게 들리는지 학습하는 데 도움이 되기 때문입니다.
3 자유지상주의(Libertarian) 논평가들은 비행기 속도의 정체를 전적으로 규제 탓으로 돌리지만, 이는 틀렸거나 기껏해야 지나치게 단순화된 주장입니다. 1973년 미국 연방항공청(FAA)이 민간 항공기의 육상 초음속 비행을 사실상 금지한 것은 사실입니다. 하지만 가장 빠른 항공기는 모두 군용이므로 이 금지 조치에 영향을 받지 않습니다. 그리고 민간 항공기는 연료 효율성 및 기타 고려 사항으로 인해 마하 1(Mach 1)보다 훨씬 낮은 속도로 순항합니다.
4 대규모 언어 모델(LLM) 훈련을 몇 자릿수(orders of magnitude) 더 샘플 효율적(sample efficient)으로 만들 수 있는지에 대한 논쟁이 있습니다. 결국, 아이들은 대규모 언어 모델(LLLM)보다 훨씬 적은 단어에 노출된 후 언어를 습득합니다. 반면에 아이들은 "요람 속의 과학자(scientists in the crib)"로서, 일찍부터 세계 모델(world models)과 추론 능력(reasoning abilities)을 개발하며, 이것이 효율적인 언어 습득을 가능하게 할 수도 있습니다. 이 논쟁은 우리의 요점과는 무관합니다. 만약 작업 표현(task representation) 또는 외삽(extrapolation)의 어려움이 병목 현상(bottleneck)이라면, 이는 샘플 효율성(sample efficiency)과 관계없이 대규모 언어 모델(LLM) 능력의 상한선을 나타낼 것입니다.
5 모델 개발자들이 더 큰 모델(매개변수(parameter) 수 기준)을 출시했을 때조차, 미스트랄 8x22B(Mixtral 8x22B)와 같은 전문가 혼합 모델(mixture-of-experts models)에서처럼 추론(inference) 중 활성 매개변수(active parameters)의 수가 전체 매개변수(parameter) 수보다 훨씬 낮은 경우와 같이 추론 효율성(inference efficiency)에 대한 관심이 증가하고 있습니다.