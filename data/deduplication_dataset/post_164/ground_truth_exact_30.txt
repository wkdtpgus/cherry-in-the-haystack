# 대규모 언어 모델(LLM) 스케일링의 한계: AGI로 가는 유일한 길인가?

지금까지 점점 더 커지는 언어 모델(language model)들은 점점 더 뛰어난 능력을 입증해왔습니다. 하지만 과거가 미래를 예측할 수 있을까요? 한 가지 일반적인 견해는 지금까지 유지되어 온 추세가 훨씬 더 큰 규모로 계속될 것이며, 이는 잠재적으로 우리를 인공 일반 지능(artificial general intelligence), 즉 AGI로 이끌 것이라는 것입니다. 이러한 견해는 일련의 신화와 오해에 기반을 두고 있습니다. 스케일링(scaling)의 겉보기 예측 가능성은 연구 결과가 보여준 바를 오해한 것입니다. 게다가, 대규모 언어 모델(LLM) 개발자들이 이미 고품질 훈련 데이터(training data)의 한계에 도달했다는 징후가 있습니다. 그리고 업계에서는 모델 크기에 대한 강력한 하향 압력을 목격하고 있습니다. 스케일링(scaling)을 통해 인공지능(AI)이 얼마나 발전할지 정확히 예측할 수는 없지만, 우리는 스케일링(scaling)만으로는 인공 일반 지능(AGI)에 도달할 가능성이 거의 없다고 생각합니다.

## 스케일링 법칙의 한계와 오해

대규모 언어 모델(LLM)의 발전은 그야말로 경이로웠습니다. 하지만 단순히 모델의 크기를 키우고 더 많은 데이터를 학습시키는 것만으로 인공지능(AI)의 궁극적인 목표인 AGI에 도달할 수 있을까요? 우리는 스케일링 법칙에 대한 일반적인 해석에 의문을 제기합니다.

스케일링 법칙(scaling laws)에 대한 연구는 모델 크기, 훈련 연산량(training compute), 데이터셋(dataset) 크기를 늘릴수록 언어 모델이 "더 좋아진다"는 것을 보여줍니다. 이러한 개선은 예측 가능성 면에서 정말 놀랍고, 여러 자릿수(orders of magnitude)에 걸쳐 유지됩니다. 이것이 많은 사람들이 선도적인 인공지능(AI) 기업들이 더 크고 강력한 모델을 정기적으로 출시하면서 스케일링(scaling)이 예측 가능한 미래에도 계속될 것이라고 믿는 주된 이유입니다. 하지만 이것은 스케일링 법칙(scaling laws)에 대한 완전한 오해입니다. "더 좋은" 모델이란 정확히 무엇일까요? 스케일링 법칙(scaling laws)은 오직 퍼플렉시티(perplexity)의 감소, 즉 모델이 시퀀스(sequence)에서 다음 단어를 얼마나 잘 예측하는지의 개선만을 정량화합니다. 물론 퍼플렉시티(perplexity)는 최종 사용자에게는 거의 관련이 없습니다. 중요한 것은 "새로운 능력(emergent abilities)", 즉 크기가 증가함에 따라 모델이 새로운 기능을 습득하는 경향입니다. 새로운 능력(emergence)은 어떤 법칙과 같은 행동에 의해 지배되지 않습니다. 지금까지 규모의 증가가 새로운 능력을 가져왔다는 것은 사실입니다. 하지만 이것이 무한정 계속될 것이라는 확신을 주는 경험적 규칙성은 없습니다. 1 새로운 능력(emergence)이 무한정 계속되지 않을 수도 있는 이유는 무엇일까요? 이것은 대규모 언어 모델(LLM)의 능력에 대한 핵심 논쟁 중 하나로 이어집니다. 즉, 모델이 외삽(extrapolation) 능력이 있는가, 아니면 훈련 데이터(training data)에 나타난 작업만 학습하는가? 증거는 불완전하며, 이를 해석하는 합리적인 방법은 다양합니다. 하지만 우리는 회의적인 견해에 기울어 있습니다. 보지 못한 작업을 해결하기 위한 기술 습득의 효율성을 테스트하도록 설계된 벤치마크(benchmark)에서 대규모 언어 모델(LLM)은 성능이 좋지 않은 경향이 있습니다. 만약 대규모 언어 모델(LLM)이 훈련(training)에서 본 것 이상을 많이 할 수 없다면, 어느 시점에는 더 많은 데이터가 더 이상 도움이 되지 않을 것입니다. 왜냐하면 그 안에 표현될 모든 작업이 이미 표현되어 있기 때문입니다. 모든 전통적인 기계 학습(machine learning) 모델은 결국 정체기에 도달합니다. 아마 대규모 언어 모델(LLLM)도 다르지 않을 것입니다.

최근 연구들은 스케일링 법칙이 단순히 모델 크기뿐만 아니라 훈련 연산량(training compute)과 데이터셋 크기 간의 최적의 균형을 강조합니다. 예를 들어, 칠린치 스케일링 법칙(Chinchilla scaling laws)은 특정 모델 크기에서 최적의 성능을 얻기 위해 예상보다 훨씬 더 많은 데이터를 사용해야 함을 보여주었습니다. 이는 무조건 모델만 키우는 것이 아니라, 데이터 효율성과 훈련 전략의 중요성을 부각합니다. 또한, '새로운 능력'의 등장은 모델의 복잡성이 임계점을 넘을 때 불연속적으로 나타나는 현상으로, 그 발생 시점이나 종류를 미리 예측하기 어렵습니다. 이러한 능력은 종종 훈련 데이터에 내재된 패턴의 복잡한 조합에서 비롯되며, 진정한 의미의 '이해'나 '추론'과는 거리가 있을 수 있다는 비판도 제기됩니다. 즉, 통계적 상관관계가 인과적 이해를 의미하지는 않는다는 것입니다.

## 훈련 데이터 고갈: 양보다 질, 그리고 새로운 도전

대규모 언어 모델(LLM)의 지속적인 발전을 가로막는 또 다른 중요한 장벽은 고품질 훈련 데이터(training data)의 확보입니다. 현재 인공지능(AI) 기업들은 이미 공개적으로 접근 가능한 거의 모든 고품질 텍스트 데이터를 소진하고 있습니다. 이는 단순히 양적인 문제가 아니라, 다양하고 편향되지 않으며 윤리적으로 확보된 데이터가 점점 희귀해진다는 질적인 문제로 이어집니다.

지속적인 스케일링(scaling)의 또 다른 장벽은 훈련 데이터(training data)를 확보하는 것입니다. 기업들은 이미 쉽게 이용 가능한 모든 데이터 소스(data source)를 사용하고 있습니다. 더 많은 것을 얻을 수 있을까요? 생각보다 그럴 가능성은 낮습니다. 사람들은 때때로 유튜브(YouTube)의 모든 콘텐츠를 전사(transcribing)하는 것과 같은 새로운 데이터 소스(data source)가 사용 가능한 데이터 볼륨(data volume)을 한두 자릿수 더 늘릴 것이라고 가정합니다. 실제로 유튜브(YouTube)에는 1,500억 분에 달하는 놀라운 양의 비디오가 있습니다. 하지만 대부분의 비디오에 유용하게 사용할 수 있는 오디오(audio)가 거의 없거나 전혀 없다는 점(대신 음악, 정지 이미지, 비디오 게임 영상 등)을 고려하면, 우리는 라마 3(Llama 3)가 이미 사용하고 있는 15조 토큰(token)보다 훨씬 적은 추정치에 도달하게 됩니다. 이는 전사된 유튜브(YouTube) 오디오(audio)의 중복 제거(deduplication) 및 품질 필터링(quality filtering)을 거치기 전이며, 이 과정에서 최소 한 자릿수 이상이 더 줄어들 가능성이 높습니다. 2 사람들은 종종 기업들이 언제 훈련 데이터(training data)가 "고갈될" 것인지에 대해 논의합니다. 하지만 이것은 의미 있는 질문이 아닙니다. 훈련 데이터(training data)는 항상 더 많이 존재하지만, 이를 얻는 데 드는 비용은 점점 더 증가할 것입니다. 그리고 이제 저작권자(copyright holders)들이 현명해져서 보상을 원하기 때문에, 비용은 특히 가파르게 상승할 수 있습니다. 금전적 비용 외에도, 사회가 데이터 수집 관행에 반발할 수 있기 때문에 평판 및 규제 비용이 발생할 수 있습니다. 어떤 지수적 추세도 무한정 계속될 수 없다는 것은 확실합니다. 하지만 기술 추세가 언제 정체기에 접어들지 예측하기는 어려울 수 있습니다. 이는 성장이 점진적으로 멈추기보다 갑자기 멈출 때 특히 그렇습니다. 추세선 자체에는 정체기에 접어들 것이라는 단서가 전혀 없습니다.

대규모 언어 모델(LLM)의 경우, 우리는 아직 몇 자릿수(orders of magnitude)의 스케일링(scaling) 여지가 남아있을 수도 있고, 이미 끝났을 수도 있습니다. 궁극적으로는 사업적 결정이며 미리 예측하기는 근본적으로 어렵습니다. 연구 분야에서는 점점 더 큰 데이터셋(dataset)을 구축하는 것에서 훈련 데이터(training data)의 품질을 개선하는 것으로 초점이 이동했습니다. 신중한 데이터 정제(data cleaning) 및 필터링(filtering)은 훨씬 더 작은 데이터셋(dataset)으로도 동일하게 강력한 모델을 구축할 수 있게 합니다. 3

데이터 고갈은 다음과 같은 복합적인 문제들을 야기합니다:
*   **비용 증가**: 새로운 데이터를 확보하고 정제하는 데 드는 비용이 기하급수적으로 증가하고 있습니다. 특히 저작권 보호가 강화되면서, 저작권자(copyright holders)들이 데이터 사용에 대한 정당한 보상을 요구하기 시작하면서 데이터 수집 비용은 더욱 가파르게 상승하고 있습니다.
*   **법적 및 윤리적 문제**: 무분별한 웹 크롤링(web crawling)은 개인 정보 침해, 저작권 침해 등의 법적 문제를 야기할 수 있습니다. 사회적으로도 데이터 수집 관행에 대한 반발이 커지고 있으며, 이는 규제 강화로 이어질 수 있습니다.
*   **데이터 품질 저하**: 사용 가능한 고품질 데이터가 줄어들면서, 모델 훈련에 저품질 데이터나 중복된 데이터가 사용될 가능성이 높아집니다. 이는 모델의 성능 향상을 저해하고, 편향(bias)을 증폭시키거나 '데이터 오염(data contamination)' 문제를 일으킬 수 있습니다.

이러한 문제에 대응하기 위해 연구자들은 '데이터 다이어트(data diet)'와 같은 데이터 효율성(data efficiency) 연구에 집중하고 있습니다. 이는 적은 양의 데이터로도 강력한 모델을 훈련할 수 있는 방법을 모색하는 것으로, 데이터 정제(data cleaning), 필터링(filtering), 가중치 부여(weighting) 기술이 중요해지고 있습니다. 또한, 텍스트 데이터뿐만 아니라 이미지, 오디오, 비디오 등 다양한 양식(modality)의 데이터를 활용하는 멀티모달(multimodal) 학습이 새로운 데이터 소스(data source)로 주목받고 있지만, 이 역시 고품질 멀티모달 데이터셋을 구축하는 데 유사한 스케일링 및 윤리적 과제에 직면해 있습니다.

## 합성 데이터의 역할과 현실

합성 데이터(synthetic data)는 종종 지속적인 스케일링(scaling)의 길로 제시됩니다. 다시 말해, 현재 모델이 다음 세대 모델을 위한 훈련 데이터(training data)를 생성하는 데 사용될 수 있다는 것입니다. 하지만 우리는 이것이 오해에 기반을 두고 있다고 생각합니다. 개발자들이 훈련 데이터(training data)의 양을 늘리기 위해 합성 데이터(synthetic data)를 사용하고 있거나(또는 사용할 수 있다고) 생각하지 않습니다. 이 논문은 훈련(training)을 위한 합성 데이터(synthetic data)의 훌륭한 사용 사례 목록을 제시하며, 이는 모두 특정 격차를 해소하고 수학, 코드 또는 저자원 언어(low-resource languages)와 같은 도메인(domain)별 개선을 이루는 데 초점을 맞추고 있습니다. 마찬가지로, 합성 데이터(synthetic data) 생성에 초점을 맞춘 엔비디아(Nvidia)의 최근 네모트론 340B(Nemotron 340B) 모델은 정렬(alignment)을 주요 사용 사례로 삼고 있습니다. 몇 가지 보조적인 사용 사례가 있지만, 현재의 사전 훈련(pre-training) 데이터 소스(data source)를 대체하는 것은 그중 하나가 아닙니다. 요컨대, 무의미한 합성 훈련 데이터(synthetic training data) 생성이 더 많은 고품질 인간 데이터(human data)를 갖는 것과 동일한 효과를 낼 가능성은 낮습니다. 2016년 바둑 세계 챔피언을 꺾은 알파고(AlphaGo)와 그 후속작인 알파고 제로(AlphaGo Zero), 알파제로(AlphaZero)와 같이 합성 훈련 데이터(synthetic training data)가 엄청난 성공을 거둔 사례도 있습니다. 이 시스템들은 스스로 게임을 플레이하며 학습했습니다. 후자 두 모델은 인간의 게임을 훈련 데이터(training data)로 사용하지 않았습니다. 그들은 상당한 양의 계산을 사용하여 어느 정도 고품질의 게임을 생성했고, 그 게임들을 신경망(neural network) 훈련에 사용했으며, 이는 계산과 결합될 때 훨씬 더 고품질의 게임을 생성할 수 있게 하여 반복적인 개선 루프(iterative improvement loop)를 만들었습니다. 자가 플레이(Self-play)는 "시스템 2(System 2) --> 시스템 1(System 1) 증류(distillation)"의 전형적인 예시로, 느리고 비용이 많이 드는 "시스템 2(System 2)" 프로세스가 빠르고 저렴한 "시스템 1(System 1)" 모델을 훈련하기 위한 훈련 데이터(training data)를 생성합니다. 이것은 완전히 독립적인 환경인 바둑과 같은 게임에 잘 작동합니다. 자가 플레이(self-play)를 게임 외의 도메인(domain)에 적용하는 것은 가치 있는 연구 방향입니다. 코드 생성(code generation)과 같이 이 전략이 가치 있을 수 있는 중요한 도메인(domain)들이 있습니다. 하지만 언어 번역(language translation)과 같이 더 개방적인 작업에 대해서는 무한한 자가 개선을 기대할 수는 없습니다. 자가 플레이(self-play)를 통해 상당한 개선을 허용하는 도메인(domain)은 규칙이라기보다는 예외로 보아야 합니다.

하지만 언어 모델과 같은 '열린 시스템(open system)'에 이 방식을 그대로 적용하는 것은 여러 한계에 부딪힙니다. 합성 데이터가 기존 모델의 지식과 편향을 답습하여 새로운 정보를 생성하지 못하고 '모델 붕괴(model collapse)' 현상을 야기할 수 있기 때문입니다. 즉, 모델이 합성 데이터를 학습하면 할수록 모델의 다양성과 창의성이 감소하고, 결국에는 의미 없는 출력을 반복하게 될 수 있습니다. 따라서 합성 데이터는 특정 도메인(domain)의 데이터 부족을 메우거나, 모델의 정렬(alignment)을 강화하는 등 보조적인 역할에 더 적합하며, 대규모 사전 훈련(pre-training) 데이터 소스(data source)를 완전히 대체하기는 어렵습니다. 진정한 혁신은 여전히 다양하고 풍부한 실제 인간 데이터에서 비롯될 가능성이 높습니다.

## 효율성을 향한 모델 개발의 전환

대규모 언어 모델(LLM) 개발의 패러다임은 단순히 모델 크기를 늘리는 것에서 효율성을 극대화하는 방향으로 전환되고 있습니다. 이는 기술적 한계와 경제적 현실이 맞물린 결과입니다.

역사적으로 스케일링(scaling)의 세 가지 축인 데이터셋(dataset) 크기, 모델 크기, 훈련 연산량(training compute)은 함께 발전해왔으며, 이것이 최적이라고 알려져 있습니다. 하지만 이 축 중 하나(고품질 데이터)가 병목 현상(bottleneck)이 된다면 어떻게 될까요? 나머지 두 축인 모델 크기와 훈련 연산량(training compute)은 계속 스케일링(scaling)될까요? 현재 시장 추세에 따르면, 더 큰 모델을 구축하는 것은 새로운 능력(emergent capabilities)을 해제할 수 있다 하더라도 현명한 사업적 움직임으로 보이지 않습니다. 이는 능력이 더 이상 채택의 장벽이 아니기 때문입니다. 다시 말해, 현재 대규모 언어 모델(LLM) 능력으로 구축 가능한 많은 애플리케이션(application)이 있지만, 비용 등의 이유로 구축되거나 채택되지 않고 있습니다. 이는 코드 생성(code generation)과 같이 작업을 완료하기 위해 대규모 언어 모델(LLM)을 수십 또는 수백 번 호출할 수 있는 "에이전트(agentic)" 워크플로우(workflow)에 특히 해당됩니다. 지난 한 해 동안, 개발 노력의 상당 부분은 주어진 능력 수준에서 더 작은 모델을 생산하는 데 집중되었습니다. 4 선도적인 모델 개발자들은 더 이상 모델 크기를 공개하지 않으므로, 우리는 이를 확신할 수 없지만, API 가격을 크기의 대략적인 대리 지표(proxy)로 사용하여 합리적인 추측을 할 수 있습니다. GPT-4o는 GPT-4와 비교하여 기능 면에서 비슷하거나 더 뛰어나면서도 비용은 25%에 불과합니다. 우리는 앤트로픽(Anthropic)과 구글(Google)에서도 동일한 패턴을 봅니다. 클로드 3 오푸스(Claude 3 Opus)는 클로드(Claude) 제품군에서 가장 비싸고 (아마도 가장 큰) 모델이지만, 더 최근에 나온 클로드 3.5 소네트(Claude 3.5 Sonnet)는 5배 더 저렴하면서도 더 뛰어나다. 마찬가지로, 제미니 1.5 프로(Gemini 1.5 Pro)는 제미니 1.0 울트라(Gemini 1.0 Ultra)보다 더 저렴하면서도 더 뛰어나다. 따라서 세 개발자 모두에게서 가장 큰 모델이 가장 뛰어난 모델은 아닙니다! 반면에 훈련 연산량(training compute)은 당분간 계속 스케일링(scaling)될 것입니다. 역설적으로, 더 작은 모델은 동일한 성능 수준에 도달하기 위해 더 많은 훈련을 필요로 합니다. 따라서 모델 크기에 대한 하향 압력은 훈련 연산량(training compute)에 상향 압력을 가하고 있습니다. 사실상 개발자들은 훈련 비용과 추론 비용(inference cost)을 절충하고 있습니다. GPT-3.5 및 GPT-4와 같은 초기 모델들은 모델의 수명 동안 발생하는 추론 비용(inference cost)이 훈련 비용을 지배한다고 여겨지는 점에서 "덜 훈련되었다(under-trained)"고 볼 수 있습니다. 이상적으로는 훈련 비용과 추론 비용(inference cost)을 서로 절충하는 것이 항상 가능하므로, 이 둘은 대략 같아야 합니다. 이러한 추세의 주목할 만한 예시로, 라마 3(Llama 3)는 80억 매개변수(parameter) 모델에 대해 원래 라마(Llama) 모델이 거의 동일한 크기(70억)에서 사용했던 것보다 20배 많은 훈련 FLOPs(training FLOPs)를 사용했습니다.

이러한 변화는 추론 비용(inference cost)의 중요성이 부각되면서 더욱 가속화되었습니다. 아무리 강력한 모델이라도 높은 추론 비용으로 인해 실제 서비스에 적용하기 어렵다면 그 가치는 떨어집니다. 따라서 개발자들은 모델의 크기를 줄이면서도 성능은 유지하거나 향상시키는 다양한 기술에 집중하고 있습니다.
*   **양자화(Quantization)**: 모델의 가중치를 더 낮은 비트(bit) 정밀도로 표현하여 모델 크기를 줄이고 연산 속도를 높이는 기술입니다.
*   **가지치기(Pruning)**: 모델의 중요하지 않은 연결이나 뉴런(neuron)을 제거하여 희소성(sparsity)을 높이는 기술입니다.
*   **지식 증류(Knowledge Distillation)**: 크고 복잡한 '선생님(teacher)' 모델의 지식을 작고 효율적인 '학생(student)' 모델에게 전달하여, 학생 모델이 선생님 모델과 유사한 성능을 내도록 학습시키는 방법입니다.
*   **혼합 전문가(Mixture-of-Experts, MoE) 모델**: 미스트랄 8x22B(Mixtral 8x22B)와 같이 전체 모델은 크지만, 특정 질의(query)에 대해 소수의 전문가(expert) 모듈만 활성화되어 추론 효율성을 높이는 구조입니다.

이러한 기술들은 클라우드 환경뿐만 아니라 온디바이스(on-device) 인공지능(AI)이나 엣지 컴퓨팅(edge computing) 환경에서도 대규모 언어 모델(LLM)을 효율적으로 구동할 수 있게 합니다. 결과적으로, 모델 개발의 초점은 단순히 최대 성능을 달성하는 것에서 벗어나, 특정 사용 사례와 비용 제약 조건 내에서 최적의 성능과 효율성을 제공하는 모델을 구축하는 방향으로 이동하고 있습니다.

## AGI 담론의 재정의와 일반성의 스펙트럼

인공 일반 지능(AGI)에 대한 논의는 그 정의 자체가 모호하여 많은 혼란을 야기합니다. 최고 경영자(CEO)들이 AGI 달성 시기를 예측하며 대중의 기대를 높였다가, 실현 가능성에 대한 의문이 제기되자 AGI의 의미를 모호하게 재정의하는 경향이 나타나기도 했습니다. AGI를 단순히 '인간과 동등한 지능'으로 보는 이진적(binary) 관점 대신, 우리는 '일반성(generality)'을 스펙트럼(spectrum)으로 이해해야 한다고 생각합니다.

스케일링(scaling)을 통한 능력 개선을 더 이상 많이 보지 못할 가능성과 일치하는 한 가지 징후는 최고 경영자(CEO)들이 인공 일반 지능(AGI)에 대한 기대를 크게 낮추고 있다는 것입니다. 불행히도, 그들은 순진한 "3년 내 인공 일반 지능(AGI)" 예측이 틀렸음을 인정하는 대신, 인공 일반 지능(AGI)의 의미를 너무 희석시켜 이제는 무의미하게 만듦으로써 체면을 살리기로 결정했습니다. 애초에 인공 일반 지능(AGI)이 명확하게 정의된 적이 없다는 점도 한몫했습니다. 일반성(generality)을 이진(binary)으로 보는 대신, 우리는 그것을 스펙트럼(spectrum)으로 볼 수 있습니다. 역사적으로 컴퓨터가 새로운 작업을 프로그래밍(program)하는 데 필요한 노력의 양은 감소해왔습니다. 우리는 이것을 일반성(generality)의 증가로 볼 수 있습니다. 이러한 추세는 특수 목적 컴퓨터(special-purpose computers)에서 튜링 머신(Turing machines)으로의 전환과 함께 시작되었습니다. 이러한 의미에서 대규모 언어 모델(LLM)의 범용성(general-purpose nature)은 새로운 것이 아닙니다. 이것이 인공 일반 지능(AGI)에 대한 장(chapter)을 할애한 우리의 책 "AI 스네이크 오일(AI Snake Oil)"에서 취하는 관점입니다. 우리는 인공지능(AI)의 역사를 단속 평형(punctuated equilibrium)으로 개념화하며, 이를 일반성(generality)의 사다리(선형적 발전을 의미하지는 않음)라고 부릅니다. 지시 튜닝(instruction-tuned)된 대규모 언어 모델(LLM)은 이 사다리의 가장 최근 단계입니다. 인공지능(AI)이 인간만큼 효과적으로 경제적으로 가치 있는 어떤 작업이든 수행할 수 있는 일반성(generality) 수준(이는 인공 일반 지능(AGI)의 한 가지 정의입니다)에 도달하기까지는 알 수 없는 수의 단계가 남아있습니다. 역사적으로, 사다리의 각 단계에 서서 인공지능(AI) 연구 커뮤니티는 현재의 패러다임(paradigm)으로 얼마나 더 나아갈 수 있을지, 다음 단계는 무엇일지, 언제 도달할지, 어떤 새로운 애플리케이션(application)을 가능하게 할지, 그리고 안전에 대한 함의는 무엇인지 예측하는 데 매우 서툴렀습니다. 우리는 이러한 추세가 계속될 것이라고 생각합니다.

진정한 일반성을 향한 다음 단계는 다음과 같은 방향으로 모색될 수 있습니다.
*   **하이브리드(Hybrid) 인공지능(AI)**: 신경망(neural network) 기반의 패턴 인식 능력과 기호 논리(symbolic logic) 기반의 추론 및 계획 능력을 결합하는 '신경-기호(neuro-symbolic) 인공지능(AI)' 접근 방식이 주목받고 있습니다. 이는 LLM이 부족한 논리적 일관성과 설명 가능성을 보완할 수 있습니다.
*   **접지(Grounding)와 체화(Embodiment)**: 언어 모델이 현실 세계와 상호작용하고, 물리적 환경을 통해 학습하며, 감각-운동 경험을 통해 지식을 '접지(grounding)'하는 것이 중요합니다. 이는 로봇 공학(robotics) 및 체화된 인공지능(embodied AI) 연구와 밀접하게 연결됩니다.
*   **새로운 아키텍처 및 학습 패러다임**: 단순히 트랜스포머(Transformer) 아키텍처를 확장하는 것을 넘어, 효율적인 장기 기억(long-term memory), 계획(planning), 그리고 복잡한 문제 해결 능력을 갖춘 새로운 모델 아키텍처와 학습 패러다임이 필요합니다.

인공지능(AI)의 역사는 각 시대의 패러다임이 가진 한계를 넘어서기 위해 끊임없이 새로운 아이디어를 탐색해왔습니다. 현재 LLM의 스케일링이 보여주는 한계는 우리가 AGI를 향한 다음 '일반성의 사다리' 단계를 모색해야 할 시점임을 시사합니다. 이는 단순히 규모의 확장을 넘어선 근본적인 접근 방식의 변화를 요구할 것입니다.

## 결론 및 향후 전망

대규모 언어 모델(LLM)은 지난 몇 년간 놀라운 발전을 이루었으며, 다양한 분야에서 혁신적인 애플리케이션(application)을 가능하게 했습니다. 그러나 우리는 단순히 모델을 키우고 데이터를 늘리는 '스케일링(scaling)' 전략만으로는 인공 일반 지능(AGI)에 도달하기 어렵다는 회의적인 시각을 유지하고 있습니다. 훈련 데이터의 고갈, 합성 데이터의 한계, 그리고 효율성을 중시하는 시장의 변화는 스케일링 중심의 접근 방식이 직면한 현실적인 제약들을 보여줍니다.

앞으로는 다음과 같은 방향으로 인공지능(AI) 연구 및 개발의 초점이 이동할 것으로 예상됩니다.
*   **데이터 효율성 및 품질**: 적은 양의 고품질 데이터로도 강력한 모델을 훈련하는 방법, 즉 데이터 정제, 증강(augmentation), 그리고 효율적인 데이터 선택 기술이 더욱 중요해질 것입니다.
*   **모델 아키텍처 혁신**: 트랜스포머(Transformer)의 한계를 극복하고, 장기 기억, 추론, 계획, 그리고 세계 모델링(world modeling) 능력을 강화할 수 있는 새로운 아키텍처에 대한 탐구가 활발해질 것입니다.
*   **멀티모달 및 체화된 학습**: 언어, 시각, 청각 등 다양한 양식의 정보를 통합적으로 이해하고, 물리적 세계와 상호작용하며 학습하는 멀티모달 및 체화된 인공지능(embodied AI)이 진정한 일반성을 향한 중요한 단계가 될 것입니다.
*   **신뢰성, 안전성 및 설명 가능성**: 모델의 성능 향상만큼이나 중요한 것은 모델의 출력에 대한 신뢰성, 잠재적 위험에 대한 안전성, 그리고 의사 결정 과정을 이해할 수 있는 설명 가능성(explainability)을 확보하는 것입니다.
*   **인간-인공지능(AI) 협업 및 증강**: 인공지능(AI)이 인간의 지능을 대체하기보다는, 인간의 인지 능력을 증강하고 협업을 통해 시너지를 창출하는 방향으로 발전할 가능성이 높습니다.

스케일링은 인공지능(AI) 발전의 중요한 동력이었지만, 이제는 그 한계에 대한 인식을 바탕으로 더욱 다각적이고 심층적인 연구가 필요한 시점입니다. AGI로 가는 길은 단순히 규모를 키우는 직선 경로가 아니라, 수많은 기술적, 철학적 난관을 극복해야 하는 복잡한 여정일 것입니다.

## 추가 자료

레오폴드 아셴브레너(Leopold Aschenbrenner)의 최근 에세이는 "2027년까지 인공 일반 지능(AGI)이 놀랍도록 그럴듯하다"는 주장으로 큰 파장을 일으켰습니다. 우리는 여기서 조목조목 반박하려고 시도하지 않았습니다. 이 글의 대부분은 아셴브레너(Aschenbrenner)의 에세이가 발표되기 전에 작성되었습니다. 그의 타임라인(timeline)에 대한 주장은 흥미롭고 생각을 자극하지만, 근본적으로는 추세선 외삽(trendline extrapolation)의 연습에 불과합니다. 또한, 많은 인공지능(AI) 지지자들처럼 그는 벤치마크(benchmark) 성능과 실제 유용성을 혼동합니다. 멜라니 미첼(Melanie Mitchell), 얀 르쿤(Yann LeCun), 게리 마커스(Gary Marcus), 프랑수아 숄레(Francois Chollet), 수바라오 캄밤파티(Subbarao Kambhampati) 등을 포함한 많은 인공지능(AI) 연구자들이 회의적인 주장을 펼쳤습니다. 드와르케시 파텔(Dwarkesh Patel)은 논쟁의 양측에 대한 좋은 개요를 제공합니다. 최근 인공지능(AI)의 윤리적 사용과 규제에 대한 논의도 활발하게 진행되고 있으며, 이는 데이터 수집 및 모델 개발 방식에 큰 영향을 미치고 있습니다.

## 감사 말씀.

초안에 대한 피드백을 주신 맷 살가닉(Matt Salganik), 올리 스티븐슨(Ollie Stephenson), 베네딕트 스트뢰블(Benedikt Ströbl)께 감사드립니다.

---
1 새로운 능력(Emergent abilities)은 불연속적으로 변하는 대신 부드럽게 변하는 측정 기준(metric)을 찾을 수 있다면 예측 가능할 것입니다. 하지만 특히 여러 기술의 조합을 요구하는 작업의 경우 그러한 측정 기준(metric)을 찾는 것은 쉽지 않습니다. 실제로는 다음 자릿수(order of magnitude)에서 어떤 새로운 능력이 나타날지 여부는 여전히 아무도 모릅니다.
2 인공지능(AI) 기업들은 훈련(training)을 위해 전사된 유튜브(YouTube) 데이터를 사용하지만, 그 가치는 데이터의 양 때문이 아니라 대규모 언어 모델(LLM)이 음성 대화가 어떻게 들리는지 학습하는 데 도움이 되기 때문입니다.
3 대규모 언어 모델(LLM) 훈련을 몇 자릿수(orders of magnitude) 더 샘플 효율적(sample efficient)으로 만들 수 있는지에 대한 논쟁이 있습니다. 결국, 아이들은 대규모 언어 모델(LLM)보다 훨씬 적은 단어에 노출된 후 언어를 습득합니다. 반면에 아이들은 "요람 속의 과학자(scientists in the crib)"로서, 일찍부터 세계 모델(world models)과 추론 능력(reasoning abilities)을 개발하며, 이것이 효율적인 언어 습득을 가능하게 할 수도 있습니다. 이 논쟁은 우리의 요점과는 무관합니다. 만약 작업 표현(task representation) 또는 외삽(extrapolation)의 어려움이 병목 현상(bottleneck)이라면, 이는 샘플 효율성(sample efficiency)과 관계없이 대규모 언어 모델(LLLM) 능력의 상한선을 나타낼 것입니다.
4 모델 개발자들이 더 큰 모델(매개변수(parameter) 수 기준)을 출시했을 때조차, 미스트랄 8x22B(Mixtral 8x22B)와 같은 전문가 혼합 모델(mixture-of-experts models)에서처럼 추론(inference) 중 활성 매개변수(active parameters)의 수가 전체 매개변수(parameter) 수보다 훨씬 낮은 경우와 같이 추론 효율성(inference efficiency)에 대한 관심이 증가하고 있습니다.