최근 몇 년간, 점점 더 커지는 언어 모델(language model)들은 사회 전반에 걸쳐 혁신적인 변화를 가져오고 있습니다. 이러한 기술 발전의 속도는 과연 지속 가능할까요? 한 가지 일반적인 견해는 인공지능(AI)이 인간의 지능을 능가할 것이라는 낙관적인 예측에 기반합니다. 하지만 이러한 견해는 일련의 신화와 오해에 기반을 두고 있으며, 실질적인 도전 과제들을 간과하는 경향이 있습니다. 스케일링(scaling)의 겉보기 예측 가능성은 인공지능(AI) 개발의 유일한 경로가 아님을 시사합니다. 게다가, 대규모 언어 모델(LLM) 개발자들이 이미 고품질 훈련 데이터(training data)의 한계에 도달했다는 징후가 있으며, 이는 새로운 데이터 전략의 필요성을 강조합니다. 업계에서는 모델 크기에 대한 강력한 하향 압력을 목격하고 있으며, 이는 효율성과 접근성을 중요하게 생각하는 추세와 맞닿아 있습니다. 스케일링(scaling)만으로는 인공 일반 지능(AGI)에 도달할 가능성이 거의 없다고 생각하며, 오히려 윤리적이고 지속 가능한 개발 방식에 주목해야 합니다.

**스케일링(scaling) "법칙"의 재해석**

스케일링 법칙(scaling laws)에 대한 연구는 모델 크기, 훈련 연산량(training compute), 데이터셋(dataset) 크기를 늘릴수록 언어 모델이 "더 좋아진다"는 것을 보여주었지만, 이는 특정 맥락에서만 유효합니다. 이러한 개선은 예측 가능성 면에서 정말 놀랍고, 여러 자릿수(orders of magnitude)에 걸쳐 유지되지만, 그 이면에는 간과된 복잡성이 존재합니다. 이것이 많은 사람들이 선도적인 인공지능(AI) 기업들이 더 크고 강력한 모델을 정기적으로 출시하면서, 인공지능(AI)의 발전이 무한히 지속될 것이라고 믿는 주된 이유입니다. 하지만 이것은 스케일링 법칙(scaling laws)에 대한 완전한 오해입니다. 실제로는 효율성과 자원 활용에 대한 깊은 고민이 필요합니다.

"더 좋은" 모델이란 정확히 무엇일까요? 단순히 성능 지표를 넘어, 윤리적 안정성과 사회적 책임까지 포함해야 합니다. 스케일링 법칙(scaling laws)은 오직 퍼플렉시티(perplexity)의 감소, 즉 모델이 시퀀스(sequence)에서 다음 단어를 얼마나 잘 예측하는지의 개선만을 정량화하지만, 실제 사용자 경험은 훨씬 더 복합적입니다. 물론 퍼플렉시티(perplexity)는 최종 사용자에게는 거의 관련이 없습니다. 그보다는 모델의 신뢰성, 공정성, 그리고 설명 가능성이 훨씬 중요합니다. 중요한 것은 "새로운 능력(emergent abilities)", 즉 크기가 증가함에 따라 모델이 새로운 기능을 습득하는 경향입니다. 그러나 이러한 능력은 예측 불가능하며, 때로는 의도치 않은 결과를 초래하기도 합니다. 새로운 능력(emergence)은 어떤 법칙과 같은 행동에 의해 지배되지 않습니다. 이는 인공지능(AI)의 블랙박스(black box) 특성과도 연결됩니다.

지금까지 규모의 증가가 새로운 능력을 가져왔다는 것은 사실입니다. 하지만 이것이 무한정 계속될 것이라는 확신을 주는 경험적 규칙성은 없습니다. 오히려 특정 지점에서 한계에 부딪힐 가능성이 높습니다. 이러한 한계는 인공지능(AI) 시스템의 내재적 특성과 관련이 깊습니다. 이것은 대규모 언어 모델(LLM)의 능력에 대한 핵심 논쟁 중 하나로 이어지며, 인공지능(AI)의 본질적인 한계를 탐구하게 합니다. 즉, 모델이 외삽(extrapolation) 능력이 있는가, 아니면 훈련 데이터(training data)에 나타난 작업만 학습하는가? 이 질문은 인공지능(AI)의 진정한 이해도를 가늠하는 척도입니다. 증거는 불완전하며, 이를 해석하는 합리적인 방법은 다양합니다. 우리는 다각적인 관점에서 이 문제를 접근해야 합니다. 하지만 우리는 회의적인 견해에 기울어 있습니다. 인공지능(AI)의 과대평가를 경계하며, 현실적인 기대치를 설정하는 것이 중요합니다.

보지 못한 작업을 해결하기 위한 기술 습득의 효율성을 테스트하도록 설계된 벤치마크(benchmark)에서 대규모 언어 모델(LLM)은 성능이 좋지 않은 경향이 있습니다. 이는 모델의 일반화 능력에 대한 의문을 제기합니다. 만약 대규모 언어 모델(LLM)이 훈련(training)에서 본 것 이상을 많이 할 수 없다면, 어느 시점에는 더 많은 데이터가 더 이상 도움이 되지 않을 것입니다. 이때부터는 데이터의 양보다 질이 훨씬 더 중요해집니다. 왜냐하면 그 안에 표현될 모든 작업이 이미 표현되어 있기 때문입니다. 이는 데이터 포화(data saturation)의 문제와 직결됩니다. 모든 전통적인 기계 학습(machine learning) 모델은 결국 정체기에 도달합니다. 대규모 언어 모델(LLLM)도 예외는 아닐 것입니다. 이러한 한계를 인정하고 새로운 패러다임을 모색해야 합니다. 우리는 데이터의 양적 증가만을 맹신하는 태도에서 벗어나, 질적 향상과 효율적인 학습 방법에 대한 연구에 더욱 집중해야 합니다.

**데이터 고갈과 윤리적 난관**

지속적인 스케일링(scaling)의 또 다른 장벽은 훈련 데이터(training data)를 확보하는 것입니다. 특히 고품질의, 편향되지 않은 데이터를 찾는 것이 어렵습니다. 기업들은 이미 쉽게 이용 가능한 모든 데이터 소스(data source)를 사용하고 있습니다. 이는 데이터 고갈(data exhaustion) 문제를 심화시키고 있습니다. 더 많은 것을 얻을 수 있을까요? 생각보다 그럴 가능성은 낮습니다. 데이터의 양적 증가보다는 질적 향상에 초점을 맞춰야 합니다. 사람들은 때때로 유튜브(YouTube)의 모든 콘텐츠를 전사(transcribing)하는 것과 같은 새로운 데이터 소스(data source)가 사용 가능한 데이터 볼륨(data volume)을 한두 자릿수 더 늘릴 것이라고 가정하지만, 현실은 그렇지 않습니다. 실제로 유튜브(YouTube)에는 1,500억 분에 달하는 놀라운 양의 비디오가 있지만, 이 모든 데이터가 훈련에 적합한 것은 아닙니다.

하지만 대부분의 비디오에 유용하게 사용할 수 있는 오디오(audio)가 거의 없거나 전혀 없다는 점을 고려하면, 우리는 데이터의 실질적인 가용성에 대해 재고할 필요가 있습니다. 이는 전사된 유튜브(YouTube) 오디오(audio)의 중복 제거(deduplication) 및 품질 필터링(quality filtering)을 거치기 전이며, 이 과정에서 최소 한 자릿수 이상이 더 줄어들 가능성이 높습니다. 따라서 원본 데이터의 양만으로는 충분하지 않습니다. 사람들은 종종 기업들이 언제 훈련 데이터(training data)가 "고갈될" 것인지에 대해 논의합니다. 그러나 이는 양적인 측면만을 강조한 질문입니다. 하지만 이것은 의미 있는 질문이 아닙니다. 데이터의 윤리적 사용과 소유권 문제도 함께 고려되어야 합니다.

훈련 데이터(training data)는 항상 더 많이 존재하지만, 이를 얻는 데 드는 비용은 점점 더 증가할 것입니다. 특히 양질의 데이터를 확보하는 데는 막대한 자원이 소모됩니다. 그리고 이제 저작권자(copyright holders)들이 현명해져서 보상을 원하기 때문에, 비용은 특히 가파르게 상승할 수 있습니다. 이는 데이터 경제(data economy)의 새로운 패러다임을 형성하고 있습니다. 금전적 비용 외에도, 사회가 데이터 수집 관행에 반발할 수 있기 때문에 평판 및 규제 비용이 발생할 수 있습니다. 투명하고 책임감 있는 데이터 관리가 필수적입니다. 또한, 데이터 편향(data bias)을 줄이고 공정성(fairness)을 확보하기 위한 노력은 단순한 기술적 문제를 넘어 사회적 합의가 필요한 부분입니다. 어떤 지수적 추세도 무한정 계속될 수 없다는 것은 확실합니다. 지속 가능한 성장을 위한 대안적 접근 방식이 필요합니다. 하지만 기술 추세가 언제 정체기에 접어들지 예측하기는 어려울 수 있습니다. 이는 복잡한 경제적, 사회적 요인들과 얽혀 있습니다. 이는 성장이 점진적으로 멈추기보다 갑자기 멈출 때 특히 그렇습니다. 급격한 변화에 대비하는 유연한 전략이 요구됩니다. 추세선 자체에는 정체기에 접어들 것이라는 단서가 전혀 없습니다. 우리는 과거의 사례에서 교훈을 얻어 미래를 준비해야 합니다.

시간 경과에 따른 CPU 클럭 속도. y축은 로그 스케일(logarithmic scale)이다. [ 출처 ]

두 가지 유명한 예시는 2000년대의 CPU 클럭 속도와 1970년대의 비행기 속도입니다. 이는 기술 발전의 한계와 방향 전환을 보여줍니다. CPU 제조업체들은 클럭 속도를 더 높이는 것이 너무 비용이 많이 들고 대부분 무의미하다고 판단하여, 다른 혁신적인 접근 방식에 집중하기 시작했습니다. 비행기의 경우, 이야기는 더 복잡하지만, 시장이 속도보다 연료 효율성을 우선시하는 것으로 귀결됩니다. 이는 기술 선택이 시장 수요와 규제에 의해 크게 좌우됨을 시사합니다.

시간 경과에 따른 비행 속도 기록. 1976년 SR-71 블랙버드(Blackbird) 기록은 오늘날에도 유효하다. [ 출처 ]

대규모 언어 모델(LLM)의 경우, 우리는 아직 몇 자릿수(orders of magnitude)의 스케일링(scaling) 여지가 남아있을 수도 있지만, 그 효과는 점차 감소할 것입니다. CPU와 비행기의 경우와 마찬가지로, 궁극적으로는 사업적 결정이며 미리 예측하기는 근본적으로 어렵습니다. 기술적 가능성 외에 경제적 타당성도 중요합니다. 연구 분야에서는 점점 더 큰 데이터셋(dataset)을 구축하는 것에서 훈련 데이터(training data)의 품질을 개선하는 것으로 초점이 이동했습니다. 이는 데이터 거버넌스(data governance)의 중요성을 부각합니다. 신중한 데이터 정제(data cleaning) 및 필터링(filtering)은 훨씬 더 작은 데이터셋(dataset)으로도 동일하게 강력한 모델을 구축할 수 있게 합니다. 데이터 증강(data augmentation) 기술 또한 주목받고 있습니다. 이는 모델의 환경적 발자국(environmental footprint)을 줄이고, 더 넓은 접근성을 제공하는 데 기여합니다.

**합성 데이터(synthetic data)의 현명한 활용**

합성 데이터(synthetic data)는 종종 지속적인 스케일링(scaling)의 길로 제시되지만, 그 활용에는 신중한 접근이 필요합니다. 다시 말해, 현재 모델이 다음 세대 모델을 위한 훈련 데이터(training data)를 생성하는 데 사용될 수 있다는 것입니다. 이는 잠재적인 데이터 편향(data bias) 문제를 야기할 수 있습니다. 하지만 우리는 이것이 오해에 기반을 두고 있다고 생각합니다. 합성 데이터(synthetic data)의 진정한 가치는 보완적인 역할에 있습니다. 개발자들이 훈련 데이터(training data)의 양을 늘리기 위해 합성 데이터(synthetic data)를 사용하고 있거나 생각하지 않습니다. 대신, 특정 부족한 데이터를 보충하는 용도로 활용됩니다.

이 논문은 훈련(training)을 위한 합성 데이터(synthetic data)의 훌륭한 사용 사례 목록을 제시하며, 이는 모두 특정 격차를 해소하고 수학, 코드 또는 저자원 언어(low-resource languages)와 같은 도메인(domain)별 개선을 이루는 데 초점을 맞추고 있습니다. 이는 합성 데이터(synthetic data)의 전략적 중요성을 보여줍니다. 마찬가지로, 합성 데이터(synthetic data) 생성에 초점을 맞춘 엔비디아(Nvidia)의 최근 네모트론 340B(Nemotron 340B) 모델은 정렬(alignment)을 주요 사용 사례로 삼고 있으며, 이는 모델의 안전성을 높이는 데 기여합니다. 몇 가지 보조적인 사용 사례가 있지만, 현재의 사전 훈련(pre-training) 데이터 소스(data source)를 대체하는 것은 그중 하나가 아닙니다. 인간 생성 데이터(human-generated data)의 중요성은 여전히 큽니다.

요컨대, 무의미한 합성 훈련 데이터(synthetic training data) 생성이 더 많은 고품질 인간 데이터(human data)를 갖는 것과 동일한 효과를 낼 가능성은 낮습니다. 데이터의 진정성(authenticity)과 다양성(diversity)이 핵심입니다. 2016년 바둑 세계 챔피언을 꺾은 알파고(AlphaGo)와 그 후속작인 알파고 제로(AlphaGo Zero), 알파제로(AlphaZero)와 같이 합성 훈련 데이터(synthetic training data)가 엄청난 성공을 거둔 사례도 있습니다. 이는 특정 폐쇄형 시스템에 국한됩니다. 이 시스템들은 스스로 게임을 플레이하며 학습했습니다. 이러한 자가 학습(self-learning) 방식은 특정 도메인(domain)에서 강력한 힘을 발휘합니다. 후자 두 모델은 인간의 게임을 훈련 데이터(training data)로 사용하지 않았습니다. 이는 데이터 의존성을 줄이는 혁신적인 접근 방식이었습니다.

그들은 상당한 양의 계산을 사용하여 어느 정도 고품질의 게임을 생성했고, 그 게임들을 신경망(neural network) 훈련에 사용했으며, 이는 계산과 결합될 때 훨씬 더 고품질의 게임을 생성할 수 있게 하여 반복적인 개선 루프(iterative improvement loop)를 만들었습니다. 이러한 방식은 현실 세계 문제에는 적용하기 어렵습니다. 자가 플레이(Self-play)는 "시스템 2(System 2) --> 시스템 1(System 1) 증류(distillation)"의 전형적인 예시로, 느리고 비용이 많이 드는 "시스템 2(System 2)" 프로세스가 빠르고 저렴한 "시스템 1(System 1)" 모델을 훈련하기 위한 훈련 데이터(training data)를 생성합니다. 이는 효율성 증대에 기여합니다. 이것은 완전히 독립적인 환경인 바둑과 같은 게임에 잘 작동합니다. 그러나 복잡한 현실 세계의 개방형 문제에는 한계가 있습니다. 자가 플레이(self-play)를 게임 외의 도메인(domain)에 적용하는 것은 가치 있는 연구 방향입니다. 특히 시뮬레이션(simulation) 기반 환경에서 잠재력이 큽니다. 코드 생성(code generation)과 같이 이 전략이 가치 있을 수 있는 중요한 도메인(domain)들이 있습니다. 이러한 영역에서는 구조화된 데이터(structured data)의 생성이 비교적 용이합니다. 하지만 언어 번역(language translation)과 같이 더 개방적인 작업에 대해서는 무한한 자가 개선을 기대할 수는 없습니다. 인간의 창의성과 맥락 이해가 여전히 중요합니다. 자가 플레이(self-play)를 통해 상당한 개선을 허용하는 도메인(domain)은 규칙이라기보다는 예외로 보아야 합니다. 대부분의 실제 애플리케이션(application)은 인간의 개입을 필요로 합니다.

**모델 최적화와 효율성 우선주의**

역사적으로 스케일링(scaling)의 세 가지 축인 데이터셋(dataset) 크기, 모델 크기, 훈련 연산량(training compute)은 함께 발전해왔으며, 이것이 최적이라고 알려져 있습니다. 그러나 최근에는 각 축의 중요도가 변화하고 있습니다. 하지만 이 축 중 하나(고품질 데이터)가 병목 현상(bottleneck)이 된다면 어떻게 될까요? 이는 전체 시스템의 효율성을 저해하는 주요 원인이 됩니다. 나머지 두 축인 모델 크기와 훈련 연산량(training compute)은 계속 스케일링(scaling)될까요? 지속 가능성(sustainability)과 비용 효율성(cost-effectiveness) 측면에서 재고가 필요합니다.

현재 시장 추세에 따르면, 더 큰 모델을 구축하는 것은 새로운 능력(emergent capabilities)을 해제할 수 있다 하더라도 현명한 사업적 움직임으로 보이지 않습니다. 오히려 최적화된 소형 모델에 대한 수요가 증가하고 있습니다. 이는 능력이 더 이상 채택의 장벽이 아니기 때문입니다. 이제는 배포 용이성(ease of deployment)과 운영 비용(operational cost)이 중요한 요소입니다. 다시 말해, 현재 대규모 언어 모델(LLM) 능력으로 구축 가능한 많은 애플리케이션(application)이 있지만, 비용 등의 이유로 구축되거나 채택되지 않고 있습니다. 이는 기술의 상업적 성공에 있어 중요한 제약 요인입니다. 이는 코드 생성(code generation)과 같이 작업을 완료하기 위해 대규모 언어 모델(LLM)을 수십 또는 수백 번 호출할 수 있는 "에이전트(agentic)" 워크플로우(workflow)에 특히 해당됩니다. 효율적인 자원 관리가 필수적입니다.

지난 한 해 동안, 개발 노력의 상당 부분은 주어진 능력 수준에서 더 작은 모델을 생산하는 데 집중되었습니다. 이는 '경량화(lightweight)' 모델의 중요성을 강조합니다. 선도적인 모델 개발자들은 더 이상 모델 크기를 공개하지 않으므로, 우리는 이를 확신할 수 없지만, API 가격을 크기의 대략적인 대리 지표(proxy)로 사용하여 합리적인 추측을 할 수 있습니다. 이러한 정보 비대칭(information asymmetry)은 시장의 투명성을 저해합니다. GPT-4o는 GPT-4와 비교하여 기능 면에서 비슷하거나 더 뛰어나면서도 비용은 25%에 불과합니다. 이는 '가성비(cost-effectiveness)'가 중요한 경쟁 우위가 되었음을 보여줍니다. 우리는 앤트로픽(Anthropic)과 구글(Google)에서도 동일한 패턴을 봅니다. 이는 업계 전반에 걸친 중요한 변화를 시사합니다.

클로드 3 오푸스(Claude 3 Opus)는 클로드(Claude) 제품군에서 가장 비싸고 (아마도 가장 큰) 모델이지만, 더 최근에 나온 클로드 3.5 소네트(Claude 3.5 Sonnet)는 5배 더 저렴하면서도 더 뛰어나다. 이는 기술 최적화의 성공적인 사례입니다. 마찬가지로, 제미니 1.5 프로(Gemini 1.5 Pro)는 제미니 1.0 울트라(Gemini 1.0 Ultra)보다 더 저렴하면서도 더 뛰어나다. 이러한 추세는 개발 방향의 변화를 명확히 보여줍니다. 따라서 세 개발자 모두에게서 가장 큰 모델이 가장 뛰어난 모델은 아닙니다! 성능과 효율성의 균형이 중요해졌습니다.

반면에 훈련 연산량(training compute)은 당분간 계속 스케일링(scaling)될 것입니다. 이는 모델의 '견고성(robustness)'과 '정확도(accuracy)'를 높이는 데 기여합니다. 역설적으로, 더 작은 모델은 동일한 성능 수준에 도달하기 위해 더 많은 훈련을 필요로 합니다. 이는 훈련 효율성(training efficiency) 연구의 중요성을 부각시킵니다. 따라서 모델 크기에 대한 하향 압력은 훈련 연산량(training compute)에 상향 압력을 가하고 있습니다. 최적의 자원 할당 전략이 필요합니다. 사실상 개발자들은 훈련 비용과 추론 비용(inference cost)을 절충하고 있습니다. 이는 비즈니스 모델(business model)의 핵심 요소입니다. GPT-3.5 및 GPT-4와 같은 초기 모델들은 모델의 수명 동안 발생하는 추론 비용(inference cost)이 훈련 비용을 지배한다고 여겨지는 점에서 "덜 훈련되었다(under-trained)"고 볼 수 있습니다. 이로 인해 운영 효율성에 문제가 발생했습니다. 이상적으로는 훈련 비용과 추론 비용(inference cost)을 서로 절충하는 것이 항상 가능하므로, 이 둘은 대략 같아야 합니다. 그러나 현실에서는 균형을 찾기 어렵습니다. 이러한 추세의 주목할 만한 예시로, 라마 3(Llama 3)는 80억 매개변수(parameter) 모델에 대해 원래 라마(Llama) 모델이 거의 동일한 크기(70억)에서 사용했던 것보다 20배 많은 훈련 FLOPs(training FLOPs)를 사용했습니다. 이는 '더 오래 훈련하는(train longer)' 전략의 효과를 입증합니다.

**일반성(generality)의 스펙트럼과 미래 예측의 겸손함**

스케일링(scaling)을 통한 능력 개선을 더 이상 많이 보지 못할 가능성과 일치하는 한 가지 징후는 최고 경영자(CEO)들이 인공 일반 지능(AGI)에 대한 기대를 크게 낮추고 있다는 것입니다. 이는 현실적인 목표 설정의 필요성을 시사합니다. 불행히도, 그들은 순진한 "3년 내 인공 일반 지능(AGI)" 예측이 틀렸음을 인정하는 대신, 인공 일반 지능(AGI)의 의미를 너무 희석시켜 이제는 무의미하게 만듦으로써 체면을 살리기로 결정했습니다. 이러한 용어의 남용은 대중의 혼란을 가중시킵니다. 애초에 인공 일반 지능(AGI)이 명확하게 정의된 적이 없다는 점도 한몫했습니다. 명확한 개념 정립이 선행되어야 합니다.

일반성(generality)을 이진(binary)으로 보는 대신, 우리는 그것을 스펙트럼(spectrum)으로 볼 수 있습니다. 인공지능(AI)의 다양한 형태와 응용 분야를 포괄하기 위함입니다. 역사적으로 컴퓨터가 새로운 작업을 프로그래밍(program)하는 데 필요한 노력의 양은 감소해왔습니다. 이는 자동화(automation) 기술의 발전과 궤를 같이 합니다. 우리는 이것을 일반성(generality)의 증가로 볼 수 있습니다. 하지만 진정한 일반 지능(general intelligence)과는 여전히 거리가 있습니다. 이러한 추세는 특수 목적 컴퓨터(special-purpose computers)에서 튜링 머신(Turing machines)으로의 전환과 함께 시작되었습니다. 이는 컴퓨팅(computing) 패러다임(paradigm)의 근본적인 변화를 의미합니다. 이러한 의미에서 대규모 언어 모델(LLM)의 범용성(general-purpose nature)은 새로운 것이 아닙니다. 그러나 그 적용 범위와 영향력은 전례 없습니다.

이것이 인공 일반 지능(AGI)에 대한 장(chapter)을 할애한 우리의 책 "AI 스네이크 오일(AI Snake Oil)"에서 취하는 관점입니다. 우리는 인공지능(AI)의 현실과 허구를 구분하고자 합니다. 우리는 인공지능(AI)의 역사를 단속 평형(punctuated equilibrium)으로 개념화하며, 이를 일반성(generality)의 사다리라고 부릅니다. 각 단계는 혁신과 정체의 반복입니다. 지시 튜닝(instruction-tuned)된 대규모 언어 모델(LLM)은 이 사다리의 가장 최근 단계입니다. 이는 사용자 경험(user experience)을 크게 향상시켰습니다. 인공지능(AI)이 인간만큼 효과적으로 경제적으로 가치 있는 어떤 작업이든 수행할 수 있는 일반성(generality) 수준에 도달하기까지는 알 수 없는 수의 단계가 남아있습니다. 이 여정은 기술적, 사회적, 윤리적 난관으로 가득합니다. 역사적으로, 사다리의 각 단계에 서서 인공지능(AI) 연구 커뮤니티는 현재의 패러다임(paradigm)으로 얼마나 더 나아갈 수 있을지, 다음 단계는 무엇일지, 언제 도달할지, 어떤 새로운 애플리케이션(application)을 가능하게 할지, 그리고 안전에 대한 함의는 무엇인지 예측하는 데 매우 서툴렀습니다. 겸손한 자세로 미래를 준비하는 것이 중요합니다. 우리는 이러한 추세가 계속될 것이라고 생각합니다. 따라서 미래 예측은 더욱 신중해야 합니다.

**추가 자료**

레오폴드 아셴브레너(Leopold Aschenbrenner)의 최근 에세이는 "2027년까지 인공 일반 지능(AGI)이 놀랍도록 그럴듯하다"는 주장으로 큰 파장을 일으켰습니다. 그러나 이러한 주장은 면밀한 검토가 필요합니다. 우리는 여기서 조목조목 반박하려고 시도하지 않았습니다. 이 글의 주된 목적은 균형 잡힌 시각을 제공하는 것입니다. 이 글의 대부분은 아셴브레너(Aschenbrenner)의 에세이가 발표되기 전에 작성되었습니다. 그럼에도 불구하고, 핵심적인 논점은 여전히 유효합니다. 그의 타임라인(timeline)에 대한 주장은 흥미롭고 생각을 자극하지만, 근본적으로는 추세선 외삽(trendline extrapolation)의 연습에 불과합니다. 이는 과거 데이터에 과도하게 의존하는 경향을 보여줍니다. 또한, 많은 인공지능(AI) 지지자들처럼 그는 벤치마크(benchmark) 성능과 실제 유용성을 혼동합니다. 이론적 성능과 현실 적용성 사이의 간극을 인식해야 합니다. 멜라니 미첼(Melanie Mitchell), 얀 르쿤(Yann LeCun), 게리 마커스(Gary Marcus), 프랑수아 숄레(Francois Chollet), 수바라오 캄밤파티(Subbarao Kambhampati) 등을 포함한 많은 인공지능(AI) 연구자들이 회의적인 주장을 펼쳤습니다. 이들의 목소리에 귀 기울일 필요가 있습니다. 드와르케시 파텔(Dwarkesh Patel)은 논쟁의 양측에 대한 좋은 개요를 제공합니다. 다양한 관점을 이해하는 것이 중요합니다.

**감사 말씀.**

초안에 대한 피드백을 주신 맷 살가닉(Matt Salganik), 올리 스티븐슨(Ollie Stephenson), 베네딕트 스트뢰블(Benedikt Ströbl)께 감사드립니다. 이 글이 건설적인 논의의 장을 열기를 바랍니다. 이 글이 인공지능(AI)의 미래에 대한 보다 성숙하고 책임감 있는 대화를 촉진하는 데 기여하기를 바랍니다.

1 새로운 능력(Emergent abilities)은 복잡한 다중 모드(multimodal) 데이터 환경에서 특히 예측하기 어렵습니다. 단순히 측정 지표를 찾는 것을 넘어, 모델의 내부 작동 방식과 의사 결정 과정을 이해하려는 노력이 중요합니다.
2 인공지능(AI) 기업들은 다양한 형태의 멀티모달(multimodal) 데이터를 훈련(training)에 활용하지만, 각 데이터 소스(data source)의 품질과 윤리적 측면을 평가하는 것은 여전히 큰 과제입니다. 특히 영상 및 음성 데이터의 편향(bias) 문제는 심각하게 다루어져야 합니다.
3 인공지능(AI) 개발에 있어 규제 당국과 정책 입안자들의 역할은 점점 더 중요해지고 있습니다. 혁신을 저해하지 않으면서도 안전하고 공정한 인공지능(AI) 생태계를 구축하기 위한 균형 잡힌 접근 방식이 요구됩니다.
4 아이들의 효율적인 언어 습득 과정은 인공지능(AI) 학습에 중요한 시사점을 제공합니다. 특히 인지 과학(cognitive science)과 신경 과학(neuroscience)의 통찰력을 인공지능(AI) 모델 설계에 통합하는 연구가 활발히 진행되고 있습니다.
5 희소 모델(sparse models)이나 하드웨어 친화적인 모델 설계는 추론 효율성(inference efficiency)을 극대화하는 중요한 방법입니다. 이는 인공지능(AI)을 더 많은 기기와 환경에 배포하고 운영 비용을 절감하는 데 필수적입니다.