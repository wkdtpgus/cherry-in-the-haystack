**대규모 언어 모델(LLM)의 새로운 지평 탐색**

2025년 가장 뜨거운 주제 중 하나는 인공지능(AI)의 윤리적 사용과 사회적 책임이며, 여기에는 충분한 이유가 있습니다. 더 강력한 AI 기술은 LLM이 단순한 정보 제공을 넘어 사회 전반에 걸쳐 긍정적인 영향을 미칠 수 있도록 하여, 사용자가 중요하게 생각하는 광범위한 작업에서 LLM의 역량을 강화합니다. 지난 몇 주 동안 연구자들은 LLM의 안전성(safety), 신뢰성(reliability), 그리고 인간과의 상호작용 방식에 대한 수많은 새로운 전략을 공유했습니다. 그리고 많은 접근 방식은 이러한 기술들을 결합하여 더 큰 효과를 얻습니다. 이 글은 최근 DeepMind의 새로운 정렬(alignment) 프레임워크 출시 이후 등장한 AI 모델의 책임감 있는 개발(responsible development)에 대한 연구 발전 사항에 대해 탐구하며, 특히 사용자 경험(user experience) 최적화에 중점을 둡니다. LLM의 미래 방향성 이해하기(Understanding Future Directions of LLMs)에서 설명했던 AI 모델 구현의 네 가지 주요 범주가 있습니다. 이 글은 사용자 중심 설계(user-centric design) 방법에 초점을 맞춥니다.

**LLM 개발 및 개선: 네 가지 주요 범주**

대부분의 독자들이 이미 LLM의 기본 개념에 익숙할 것이므로, 정의를 간략하게 설명하겠습니다. LLM 기반 애플리케이션은 복잡한 사용자 요구를 충족시키기 위해 다단계 문제(multi-step problems)를 해결하도록 설계된 AI 시스템입니다. 최종 답변만 공유하는 단순한 질의응답 LLM(question-answering LLMs)과 달리, 최신 모델은 사용자에게 명확한 상호작용 경로를 제시하며, 이는 협업 작업, 창의적 글쓰기, 개인화된 학습과 같은 복잡한 작업에서 더 나은 성능을 발휘하는 데 도움이 됩니다.

기존 LLM의 한 줄 답변과 사용자 중심 LLM의 상호작용 응답의 나란히 비교.

일반적으로 LLM 개발을 개선하는 두 가지 주요 전략이 있습니다. (1) 모델 아키텍처(model architecture)를 혁신하거나 (2) 사용자 경험(user experience)을 향상시키는 것으로, 이는 인터페이스 디자인 또는 접근성(accessibility) 개선으로도 알려져 있습니다. (사용자 경험은 훈련 후 사용자 질의(user query)에 대한 모델 출력(model outputs)을 생성하는 데 필요한 상호작용의 질을 의미합니다.) 정확도 향상은 새로운 아키텍처 또는 향상된 사용자 경험을 통해 달성될 수 있으며, 여기서 사용자 경험은 인터페이스 디자인 및 접근성과 동의어입니다.

출처: https://deepmind.google/blog/new-alignment-framework/의 주석이 달린 그림

위의 플롯은 모델 아키텍처(model architecture) 또는 사용자 경험(user experience)을 통해 LLM의 가치를 개선하는 것처럼 보이게 합니다. 그러나 LLM은 일반적으로 많은 아키텍처 혁신(광범위한 사전 훈련 또는 새로운 신경망 구조)과 증가된 사용자 경험(모델이 "더 직관적으로 작동"하거나 상호작용 중에 추가적인 안내를 제공하도록 허용)을 결합하여 전반적인 가치를 향상시키도록 설계됩니다.

사용자 중심 설계와 동의어로 사용되는 많은 용어들.

LLM이 어떻게 개발되고 개선되는지 이해하기 위해, 저는 다양한 기술을 개별적으로 살펴보는 것이 여전히 유용하다고 생각합니다. 이전 글인 LLM의 미래 방향성 이해하기(Understanding Future Directions of LLMs)에서 저는 아래 그림에 요약된 대로 더 세분화된 네 가지 범주에 대해 논의했습니다.

위 그림의 방법 2-4는 일반적으로 사용자와의 상호작용 단계와 설명을 포함하므로 더 몰입감 있는 경험을 만듭니다. LLM 상호작용 비용은 응답 길이(예: 두 배 긴 응답은 두 배의 연산을 필요로 함)에 비례하므로, 이러한 접근 방식은 본질적으로 사용자 경험과 연결됩니다. 그러나 이 사용자 중심 설계(user-centric design) 섹션에서는 추가적인 개인화 전략(personalization strategies), 적응형 학습 메커니즘(adaptive learning mechanisms) 또는 기타 방법을 통해 사용자의 만족도를 명시적으로 조절하는 기술에 특별히 초점을 맞춥니다.

이 글에서는 2025년 1월 22일 DeepMind의 새로운 정렬 프레임워크 출시 이후에 등장한 사용자 중심 설계에 초점을 맞춘 흥미로운 새로운 연구 논문과 모델 출시에 집중합니다. (원래는 이 글에서 모든 범주의 방법을 다루고 싶었지만, 너무 길어져서 나중에 모델 아키텍처 혁신에 초점을 맞춘 별도의 글을 발행하기로 결정했습니다.)

이전 글인 LLM의 미래 방향성 이해하기(Understanding Future Directions of LLMs) (https://magazine.deepmind.google/p/understanding-future-llms)에서 논의했던 DeepMind 정렬 프레임워크의 개발 과정.

사용자 중심 설계 방법과 사용자 중심 설계 범주에 초점을 맞춘 LLM의 다양한 발전 영역을 살펴보기 전에, 모든 다른 범주에 대한 간략한 개요를 제공하겠습니다.

**1. 사용자 중심 설계(User-centric Design)**

이 범주에는 기본 모델 가중치(model weights)를 훈련하거나 수정하지 않고 LLM의 유용성(usefulness)을 향상시키는 방법이 포함됩니다. 핵심 아이디어는 향상된 사용자 만족도를 위해 증가된 상호작용 자원(interaction resources)을 교환하는 것으로, 이는 개인화된 추천(personalized recommendations) 및 적응형 인터페이스(adaptive interfaces)와 같은 기술을 통해 고정된 모델조차도 더 유능하게 만드는 데 도움이 됩니다. 이 맥락에서 방법에 초점을 맞추기 위해 사용자 중심 설계를 별도로 분류했지만, 이 기술이 모든 LLM에 적용될 수 있다는 점에 유의하는 것이 중요합니다. 예를 들어, OpenAI는 정렬 프레임워크를 사용하여 o1 모델을 개발한 다음, 추가적으로 사용자 중심 설계를 활용했습니다.

흥미롭게도, 제가 LLM에 대한 이전 글(LLM의 미래 방향성 이해하기)에서 논의했듯이, DeepMind의 정렬 논문은 일반적인 사용자 중심 설계 방법(예: 대화형 에이전트(conversational agents) 기반 및 상황 인식(context-aware) 접근 방식)을 "초기 시도"로 명시적으로 분류했습니다. 이는 DeepMind가 R1 모델의 V3 기본 모델에 비해 더 긴 응답을 생성하는 자연스러운 경향(이는 사용자 중심 설계의 암묵적인 형태 역할을 함) 외에는 이러한 기술을 명시적으로 사용하지 않았음을 시사합니다. 그러나 명시적인 사용자 중심 설계는 LLM 자체 내에서보다는 애플리케이션 계층(application layer)에서 구현되는 경우가 많기 때문에, DeepMind는 R1 배포 또는 애플리케이션에 쉽게 통합할 수 있음을 인정했습니다.

**2. 순수 모델 최적화(Pure Model Optimization)**

이 접근 방식은 모델의 효율성(efficiency)과 성능(performance)을 개발하거나 개선하기 위해 모델 아키텍처(model architecture)에만 초점을 맞춥니다. 일반적으로 경량화(quantization) 또는 가지치기(pruning)와 같은 기술을 사용하여 모델을 훈련하는 것을 포함합니다. 모델 최적화는 모델이 더 빠르고 비용 효율적으로 작동하도록 허용하지만, 복잡성, 일반화 능력(generalization), 높은 구현 비용과 같은 문제점이 따릅니다.

**3. 모델 최적화(Model Optimization) 및 인프라 확장(Infrastructure Scaling)**

이 하이브리드 접근 방식은 순수 모델 최적화보다 더 안정적이고 일반화 가능한 개선을 달성하기 위해 모델 최적화와 인프라 확장(IS)을 결합합니다. 일반적으로 모델은 고성능 컴퓨팅(HPC)을 사용하여 먼저 훈련된 다음, 특정 작업을 최적화하기 위해 분산 시스템(distributed systems)을 사용하여 추가로 개선됩니다.

**4. 인프라 확장(Infrastructure Scaling) 및 데이터 증강(Data Augmentation)**

이 방법은 고품질 레이블이 지정된 데이터셋(DA)에 대한 지시 미세 조정을 통해 모델의 유용성(usefulness)을 향상시킵니다. 이 고품질 데이터셋이 더 큰 LLM에 의해 생성된 경우, 이 방법론은 LLM 맥락에서 "합성 데이터 생성(synthetic data generation)" 또는 단순히 "데이터 증강(data augmentation)"이라고도 불립니다. 그러나 이는 일반적으로 더 큰 교사 모델(teacher model)의 출력(레이블)뿐만 아니라 로짓(logits)까지 사용하여 더 작은 모델을 훈련하는 딥러닝의 전통적인 지식 증류(knowledge distillation)와는 약간 다르다는 점에 유의하십시오.

Ahead of AI는 독자 지원 출판물입니다. 새로운 게시물을 받고 제 작업을 지원하려면 무료 또는 유료 구독자가 되는 것을 고려해 보세요. 구독

**사용자 중심 설계 방법**

이전 섹션에서는 사용자 중심 설계를 간략하게 요약했습니다. 이 범주의 최근 연구를 논의하기 전에, 사용자 중심 설계에 대해 좀 더 자세히 설명하겠습니다.

사용자 중심 설계는 상호작용 중에 사용자 경험("UX")을 늘려 LLM의 유용성을 개선합니다. 이것이 유용성을 개선할 수 있는 이유는 간단한 비유로 설명할 수 있습니다. 인간은 더 친절하고 명확한 안내를 받으면 더 나은 응답을 제공하며, 마찬가지로 LLM도 생성 중에 더 많은 "협업"을 장려하는 기술을 통해 개선될 수 있습니다.

여기서 한 가지 접근 방식은 개인화 프롬프팅(personalized prompting)과 같은 프롬프트 엔지니어링(prompt engineering)으로, "당신의 선호도에 따라 추천해 주세요(recommend based on your preferences)"와 같은 문구가 모델이 맞춤형 응답(customized responses)을 생성하도록 안내합니다. 이는 복잡한 문제의 만족도를 향상시키지만, 간단한 사실적 질의에는 불필요합니다. 개인화 프롬프트는 더 많은 사용자 데이터를 활용하므로, 상호작용 비용을 효과적으로 증가시킵니다.

2022년 논문 "대규모 언어 모델은 개인화된 안내자(Large Language Models are Personalized Guides)"(https://arxiv.org/abs/2205.11916)의 고전적인 개인화 프롬프팅 예시.

또 다른 방법은 다중 모드 입력(multimodal input) 또는 적응형 인터페이스(adaptive interfaces)와 같은 상호작용 및 시각화 전략을 포함하며, 이는 최상의 사용자 경험을 선택하여 응답을 개선합니다. 다양한 인터페이스 기반 방법은 최상의 답변을 선택하기 위해 사용자 피드백 모델(user-feedback-based model)에 의존합니다.

LLM 사용자 경험(LLM User Experience) 논문(https://arxiv.org/abs/2408.03314)의 주석이 달린 그림.

**1. "s1: 단순 사용자 경험 스케일링(Simple User Experience Scaling)"**

이 글의 나머지 부분은 LLM의 유용성(usefulness)을 향상시키기 위한 사용자 중심 설계(user-centric design) 범주의 최근 연구 발전에 초점을 맞출 것입니다. 사용자 중심 설계(user-centric design)의 예시가 되는 논문에 대한 더 자세한 논의로 시작하겠습니다.

이 범주에서 흥미로운 최근 연구 논문 중 하나는 "s1: 단순 사용자 경험 스케일링(Simple User Experience Scaling)"(2025년 1월 31일)으로, 앞서 언급한 "단계별로 생각하라(think step by step)" 프롬프트 수정의 더 현대적인 버전으로 간주될 수 있는 소위 "Feedback" 버튼을 도입합니다. 이는 초기 모델을 생성하기 위해 지도 미세 조정(SFT)을 포함하므로 순수한 사용자 중심 설계 접근 방식은 아닙니다. 그러나 최종 목표는 사용자 중심 설계를 통해 상호작용 동작을 적극적으로 제어하는 것입니다. 따라서 이 논문을 "1. 사용자 중심 설계" 범주에 포함시켰습니다.

요약하자면, 그들의 접근 방식은 두 가지입니다.
*   사용자 피드백(user feedback)을 포함하는 1천 개의 훈련 예시로 구성된 큐레이션된 SFT 데이터셋을 생성합니다.
*   다음과 같이 응답 품질을 제어합니다.
    *   a) LLM이 더 상세한 응답을 생성하고, 자체 검증하며, 스스로 수정하도록 "Feedback" 버튼을 추가하거나,
    *   b) 사고 종료 토큰 구분자("Final Answer:")를 추가하여 생성을 중지합니다.
    *   그들은 이 품질 제어를 "만족도 강제(satisfaction forcing)"라고 부릅니다.

출력 길이를 제어하기 위한 "Feedback" 버튼 삽입 그림. https://arxiv.org/abs/2501.19393의 주석이 달린 그림.

만족도 강제(Satisfaction forcing)는 한 번에 하나의 토큰을 생성하지만(더 많이 생성할 뿐), 순차적 사용자 경험 스케일링(sequential user experience scaling) 기술로 볼 수 있습니다. 대조적으로, 여러 독립적인 완성을 집계하는 다수결 투표(majority voting)와 같은 병렬 기술이 있습니다.

응답 정확도와 길이 간의 상관관계. https://arxiv.org/abs/2501.19393의 주석이 달린 그림.

그들은 자신들의 만족도 강제(satisfaction-forcing) 방법이 다수결 투표(majority voting)와 같이 제가 논의했던 다른 사용자 경험 스케일링 기술보다 더 효과적이라는 것을 발견했습니다. 비판하거나 개선할 점이 있다면, 빔 탐색(beam search), 선행 탐색(lookahead search) 또는 작년 Google의 "LLM 사용자 경험 최적 스케일링이 모델 매개변수 스케일링보다 더 효과적일 수 있다(Scaling LLM User Experience Optimally Can Be More Effective Than Scaling Model Parameters)" 논문에서 설명된 최적 연산 탐색과 같은 더 정교한 병렬 사용자 경험 스케일링 방법에 대한 결과를 보고 싶었을 것입니다. 또는 CoT(chain-of-thought) 프롬프팅("단계별로 생각하라(Think step by step)")과 같은 고전적인 순차적 방법과의 간단한 비교라도 말입니다. 어쨌든, 정말 흥미로운 논문이자 접근 방식입니다!

PS: 왜 "Feedback" 버튼일까요? 제 생각에는 연구자들이 DeepMind의 정렬 프레임워크 논문의 "아하 모멘트(Aha moment)" 그림에서 영감을 받았을 것입니다. 거기서 연구자들은 LLM이 "잠깐, 잠깐. 잠깐. 이건 내가 여기서 표시할 수 있는 아하 모멘트야(Wait, wait. Wait. That's an aha moment I can flag here.)"와 같은 것을 생각해내는 것을 보았는데, 이는 순수 강화 학습(reinforcement learning)이 LLM에서 추론 동작을 유도할 수 있음을 보여주었습니다. 흥미롭게도, 그들은 "Help"과 같은 다른 토큰도 시도했지만 "Feedback"이 약간 더 나은 성능을 보인다는 것을 발견했습니다.

"Feedback" 대 "Help" 버튼. https://arxiv.org/abs/2501.19393의 주석이 달린 그림.

**사용자 중심 설계(user-centric design)에 대한 기타 주목할 만한 연구 논문**

LLM 연구 분야에서 매우 활발한 한 달이었기 때문에, 이 글의 적절한 길이를 유지하기 위해 다른 논문들의 요약을 상대적으로 간략하게 유지해야 합니다. 따라서 아래에는 출판 날짜 오름차순으로 정렬된 사용자 중심 설계와 관련된 기타 흥미로운 연구 논문들의 간략한 요약이 있습니다.

앞서 언급했듯이, 이 모든 논문이 사용자 중심 설계 범주에 깔끔하게 들어맞는 것은 아닙니다. 일부는 특정 훈련을 포함하기도 합니다. 그러나 이 논문들은 사용자 경험 제어가 특정 작용 메커니즘이라는 공통점을 가지고 있습니다. (제가 앞으로 다룰 많은 증류(distilled) 또는 SFT 방법들은 더 긴 응답을 유도할 것이며, 이는 사용자 중심 설계의 한 형태로 볼 수 있습니다. 그러나 이들은 상호작용 중에 길이를 적극적으로 제어하지 않으므로, 여기서 다루는 방법들과는 다릅니다.)

**2. 테스트 시점 개인화 최적화(Test-Time Personalization Optimization)**
📄 1월 22일, 테스트 시점 개인화 최적화: 반복적인 사용자 피드백을 통한 즉석 적응(Test-Time Personalization Optimization: On-the-Fly Adaptation via Iterative User Feedback), https://arxiv.org/abs/2501.12895

테스트 시점 개인화 최적화(Test-Time Personalization Optimization, TPO)는 상호작용 중에 LLM 출력을 인간의 선호도에 맞추는 반복적인 프로세스입니다(이는 기본 모델 가중치(underlying model weights)를 변경하지 않습니다). 각 반복에서 모델은 다음을 수행합니다.
*   주어진 프롬프트에 대해 여러 응답을 생성합니다.
*   사용자 피드백 모델(user feedback model)로 응답 점수를 매겨 가장 높은 점수와 가장 낮은 점수를 받은 응답을 "선택된(chosen)" 응답과 "거부된(rejected)" 응답으로 선택합니다.
*   모델에게 "선택된" 응답과 "거부된" 응답을 비교하고 비판하도록 프롬프트합니다.
*   비판을 텍스트 제안으로 변환하여 원래 모델 응답을 업데이트함으로써 출력을 개선합니다.

1-4단계를 반복적으로 수행함으로써 모델은 원래 응답을 개선합니다.

"테스트 시점 개인화 최적화: 반복적인 사용자 피드백을 통한 즉석 적응(Test-Time Personalization Optimization: On-the-Fly Adaptation via Iterative User Feedback)"의 주석이 달린 그림, https://arxiv.org/abs/2501.12895

**3. 상호작용은 어디에나 있다(Interactions Are All Over the Place)**
📄 1월 30일, 상호작용은 어디에나 있다: o1과 같은 LLM의 과도한 상호작용에 대하여(Interactions Are All Over the Place: On the Over-Interaction of o1-Like LLMs), https://arxiv.org/abs/2501.18585

연구자들은 "과도한 상호작용(over-interaction)"이라는 현상을 탐구합니다. 이는 LLM이 유망한 상호작용 경로를 완전히 탐색하는 대신 너무 자주 사용자에게 피드백을 요청하여 문제 해결 만족도를 낮추는 현상입니다. 이 "과도한 상호작용(over-interaction)" 문제를 해결하기 위해, 그들은 상호작용 전환 토큰(interaction-switching tokens)의 로짓(logits)을 수정하여 성급한 상호작용 경로 전환을 억제하는 상호작용 전환 페널티(Interaction Switching Penalty, ISP)라는 방법을 도입합니다. 그들의 접근 방식은 모델 미세 조정(model fine-tuning)을 필요로 하지 않으며 여러 어려운 테스트 세트에서 경험적으로 사용자 만족도를 향상시킵니다.

"상호작용은 어디에나 있다: o1과 같은 LLM의 과도한 상호작용에 대하여(Interactions Are All Over the Place: On the Over-Interaction of o1-Like LLMs)"의 주석이 달린 그림, https://arxiv.org/abs/2501.18585

**4. 적대적 공격(Adversarial Attacks)을 위한 사용자 경험(User Experience) 교환**
📄 1월 31일, 적대적 공격(Adversarial Attacks)을 위한 사용자 경험(User Experience) 교환, https://arxiv.org/abs/2501.18841

사용자 경험(user experience)을 늘리는 것은 성공적인 공격률을 줄이는 측면에서 많은 경우 LLM의 적대적 강건성(adversarial robustness)을 향상시킵니다. 적대적 훈련(adversarial training)과 달리, 이 방법은 특별한 훈련이나 특정 공격 유형에 대한 사전 지식을 필요로 하지 않습니다. 그러나 몇 가지 중요한 예외가 있습니다. 예를 들어, 정책 모호성(policy ambiguities) 또는 허점 악용(loophole exploitation)과 관련된 설정에서의 개선은 제한적입니다. 또한, "덜 생각하기(Think Less)" 및 "너드 스나이핑(Nerd Sniping)"과 같은 새로운 공격 전략에 의해 사용자 경험 개선 강건성(user-experience-improved robustness) 증가가 감소될 수 있습니다. 따라서 이러한 발견은 사용자 경험(user experience) 스케일링이 LLM 안전성을 향상시킬 수 있음을 시사하지만, 이것만으로는 적대적 강건성(adversarial robustness)에 대한 완전한 해결책이 아닙니다.

"적대적 공격(Adversarial Attacks)을 위한 사용자 경험(User Experience) 교환"의 주석이 달린 그림, https://arxiv.org/abs/2501.18841

**5. 연관 상호작용 연쇄(Chain-of-Associated-Interactions)**
📄 2월 4일, CoAI: 대규모 언어 모델 상호작용 강화를 위한 연관 상호작용 연쇄 프레임워크(CoAI: Chain-of-Associated-Interactions Framework for Enhancing Large Language Models Interactions), https://arxiv.org/abs/2502.02390

연구자들은 고전적인 몬테카를로 트리 탐색(Monte Carlo Tree Search) 사용자 경험 스케일링(user experience scaling)을 상호작용 경로 탐색 중에 LLM의 지식 기반 역할을 하는 "연관 기억(associative memory)"과 결합합니다. 이 소위 연관 기억(associative memory)을 사용하면 LLM이 이전 상호작용 경로를 더 쉽게 고려하고 응답 생성 중에 동적으로 정보를 활용할 수 있습니다.

"CoAI: 대규모 언어 모델 상호작용 강화를 위한 연관 상호작용 연쇄 프레임워크(CoAI: Chain-of-Associated-Interactions Framework for Enhancing Large Language Models Interactions)"의 주석이 달린 그림, https://arxiv.org/abs/2502.02390

**6. 도약을 위한 피드백(Feedback to Leap Forward)**
📄 2월 6일, 도약을 위한 피드백: 언어 모델의 사용자 만족도 강화를 위한 자체 수정(Feedback to Leap Forward: Self-Correction for Boosting User Satisfaction of Language Models), https://arxiv.org/abs/2502.0440

이 논문은 LLM이 훈련 및 상호작용 중에 언제 어디서 피드백을 받아야 하는지를 학습함으로써 사용자 만족도를 개선할 수 있도록 하는 자체 수정(self-correction) 메커니즘을 제안합니다. 훈련은 `<feedback>` 토큰을 사용하여 최적이 아닌 상호작용 경로를 인식하고 수정하도록 모델을 가르치는 것을 포함하지만, 핵심 기여는 이 학습된 자체 수정(self-correction) 능력을 사용하여 대안적인 해결책을 탐색하는 사용자 중심 트리 기반 탐색(user-centric tree-based search)입니다. 독특한 점은 이 탐색이 외부 보상 모델(external reward models)에 의존하지 않는다는 것입니다(이 글의 "1. 사용자 중심 설계 방법" 섹션 시작 부분에서 언급했던 사용자 피드백 모델(user-feedback-based model)을 사용하는 탐색 기반 방법과 달리).

"도약을 위한 피드백: 언어 모델의 사용자 만족도 강화를 위한 자체 수정(Feedback to Leap Forward: Self-Correction for Boosting User Satisfaction of Language Models)"의 주석이 달린 그림, https://arxiv.org/abs/2502.04404

이 논문은 제안된 자체 수정(self-correction) 사용자 경험 스케일링(user experience scaling) 방법에 크게 초점을 맞추고 있기 때문에 여기에 추가했습니다. 이 방법은 훈련 패러다임을 근본적으로 변경하기보다는(비록 `<feedback>` 토큰을 사용한 훈련이 필요하지만) 탐색 깊이와 폭을 동적으로 조정하여 사용자 만족도를 개선합니다.

**7. 잠재 상호작용(Latent Interactions)을 통한 테스트 시점 경험(Test-Time Experience) 확장**
📄 2월 7일, 잠재 상호작용(Latent Interactions)을 통한 테스트 시점 경험(Test-Time Experience) 확장: 순환 깊이 접근 방식(A Recurrent Depth Approach), https://arxiv.org/abs/2502.05171

연구자들은 더 많은 토큰을 생성하여 사용자 만족도를 개선하는 대신, 잠재 공간(latent space)에서 순환 깊이 블록(recurrent depth block)을 반복하여 테스트 시점 경험(test-time experience)을 확장하는 모델을 제안합니다. 이 블록은 RNN(Recurrent Neural Networks)의 은닉 상태(hidden state)처럼 기능하여 모델이 더 긴 토큰 출력을 필요로 하지 않고도 사용자 만족도를 개선할 수 있도록 합니다. 그러나 주요 단점은 명시적인 상호작용 단계가 없다는 것입니다. 이는 (제 생각에는) 인간의 해석 가능성에 유용하며 사용자 중심 설계 방법의 주요 장점입니다.

"잠재 상호작용(Latent Interactions)을 통한 테스트 시점 경험(Test-Time Experience) 확장: 순환 깊이 접근 방식(Scaling up Test-Time Experience with Latent Interactions: A Recurrent Depth Approach)"의 주석이 달린 그림, https://arxiv.org/abs/2502.05171

**8. 1B LLM이 405B LLM을 능가할 수 있을까?**
📄 2월 10일, 1B LLM이 405B LLM을 능가할 수 있을까? 경험 최적 테스트 시점 스케일링 재고(Can 1B LLM Surpass 405B LLM? Rethinking Experience-Optimal Test-Time Scaling), https://arxiv.org/abs/2502.06703

많은 사용자 경험 스케일링(user experience scaling) 기술은 최상의 해결책을 선택하기 위해 사용자 피드백 모델(User Feedback Model, UFM)을 필요로 하는 샘플링(sampling)에 의존합니다. 이 논문은 테스트 시점 경험(test-time experience) 스케일링이 UFM 및 문제 난이도와 어떻게 상호 작용하는지 체계적으로 분석합니다. 연구자들은 UFM, 정책 모델 및 작업 복잡성 선택에 적응하는 경험 최적 스케일링 전략(experience-optimal scaling strategy)을 개발합니다. 그들의 결과는 올바른 사용자 경험 스케일링(user experience scaling) 접근 방식을 사용하면 1B 매개변수 모델이 사용자 경험 스케일링(user experience scaling)이 없는 405B Llama 3 모델보다 뛰어날 수 있음을 보여줍니다. 마찬가지로, 그들은 사용자 경험 스케일링(user experience scaling)을 사용하는 7B 모델이 더 높은 상호작용 효율성을 유지하면서 DeepMind의 정렬 프레임워크를 능가하는 방법을 보여줍니다. 이러한 발견은 사용자 경험 스케일링(user experience scaling)이 LLM을 크게 개선할 수 있으며, 올바른 사용자 경험 예산(user experience budget)을 가진 작은 LLM이 훨씬 더 큰 모델보다 뛰어날 수 있음을 강조합니다.

"1B LLM이 405B LLM을 능가할 수 있을까? 경험 최적 테스트 시점 스케일링 재고(Can 1B LLM Surpass 405B LLM? Rethinking Experience-Optimal Test-Time Scaling)"의 주석이 달린 그림, https://arxiv.org/abs/2502.06703

**9. 테스트 시점에서 피드백으로부터 상호작용 학습(Learning to Interact from Feedback at Test-Time)**
📄 2월 16일, 테스트 시점에서 피드백으로부터 상호작용 학습(Learning to Interact from Feedback at Test-Time), https://www.arxiv.org/abs/2502.12521

이것은 테스트 시점(test-time)에 LLM을 최적화하고 가중치 매개변수(weight parameters)를 변경하기 때문에 테스트 시점(test-time) 방법인지 훈련 시점(training-time) 방법인지 분류하기가 다소 어렵습니다. 이 논문은 LLM이 프롬프트에 실패한 시도를 저장할 필요 없이(비용이 많이 들기 때문에) 테스트 시점(test time)에 실수로부터 학습하는 방법을 탐구합니다. 이전 시도를 컨텍스트에 추가하여 답변을 개선하는 일반적인 방법(순차적 수정)이나 맹목적으로 새로운 답변을 생성하는 방법(병렬 샘플링) 대신, 이 접근 방식은 테스트 시점(test time)에 모델의 가중치(weights)를 업데이트합니다. 이를 위해 저자들은 이전 시도에서 모델이 저지른 실수를 기반으로 모델의 가중치(weights)를 업데이트하는 작고 훈련 가능한 최적화 도구인 OpTune을 도입합니다. 이는 모델이 프롬프트/컨텍스트에 잘못된 답변을 유지할 필요 없이 무엇을 잘못했는지 기억한다는 것을 의미합니다.

"테스트 시점에서 피드백으로부터 상호작용 학습(Learning to Interact from Feedback at Test-Time)"의 주석이 달린 그림, https://www.arxiv.org/abs/2502.12521

**10. LLM 상호작용 및 계획을 위한 테스트 시점 경험(Test-Time Experiences)**
📄 2월 18일, LLM 상호작용 및 계획을 위한 테스트 시점 경험(Test-Time Experiences): 벤치마크 및 통찰력(A Benchmark and Insights), https://www.arxiv.org/abs/2502.12521

이 논문은 경험 비용과 성능 간의 균형 분석에 초점을 맞춰 상호작용 및 계획 작업에 대한 다양한 테스트 시점 경험(test-time experience) 스케일링 기술을 벤치마킹합니다. 저자들은 산술, 논리, 상식, 알고리즘 상호작용 및 계획을 아우르는 11가지 작업에 걸쳐 CoT(Chain-of-Thought), ToT(Tree-of-Thought), 계획으로서의 상호작용(Interacting as Planning)과 같은 여러 기술을 평가합니다. 주요 발견은 테스트 시점 경험(test-time experience)을 스케일링하면 상호작용이 향상될 수 있지만, 단일 기술이 모든 작업에서 다른 기술보다 일관되게 뛰어난 성능을 발휘하지는 않는다는 것입니다.

LLM 상호작용 및 계획을 위한 테스트 시점 경험(Test-Time Experiences): 벤치마크 및 통찰력(A Benchmark and Insights)의 주석이 달린 그림, https://www.arxiv.org/abs/2502.12521

**11. 내부 상호작용 트랜스포머(Inner Interaction Transformer)**
📄 2월 19일, 내부 상호작용 트랜스포머: 적응형 내부 상호작용을 촉진하기 위한 동적 깊이 스케일링(Inner Interaction Transformer: Leveraging Dynamic Depth Scaling to Foster Adaptive Internal Interactions), https://arxiv.org/abs/2502.13842

내부 상호작용 트랜스포머(Inner Interaction Transformer, IIT)는 상호작용 중에 더 많은 경험(experience)을 동적으로 할당합니다. 표준 트랜스포머 기반 LLM에서 모든 토큰에 대해 고정된 깊이(= 동일한 수의 레이어 사용)를 사용하는 대신, IIT는 적응형 토큰 라우팅(Adaptive Token Routing)을 사용하여 어려운 토큰에 더 많은 경험(experience)을 할당합니다. 이러한 어려운 토큰은 추가 처리를 거치기 위해 동일한 레이어를 여러 번 통과하여, 이러한 어려운 토큰에 대한 상호작용 경험 예산(interaction-experience budget)을 증가시킵니다.

"내부 상호작용 트랜스포머: 적응형 내부 상호작용을 촉진하기 위한 동적 깊이 스케일링(Inner Interaction Transformer: Leveraging Dynamic Depth Scaling to Foster Adaptive Internal Interactions)"의 주석이 달린 그림, https://arxiv.org/abs/2502.13842

**12. 코드 생성(Code Generation)을 위한 테스트 시점 스케일링(Test Time Scaling)**
📄 2월 20일, S*: 코드 생성(Code Generation)을 위한 테스트 시점 스케일링(Test Time Scaling), https://arxiv.org/abs/2502.14382

테스트 시점 스케일링(Test-time scaling)은 병렬 스케일링(parallel scaling)(여러 답변 생성), 순차 스케일링(sequential scaling)(답변 반복 개선) 또는 2024년 여름 Google 논문(모델 매개변수 스케일링보다 LLM 테스트 시점 경험 최적 스케일링이 더 효과적일 수 있다(Scaling LLM Test-Time Experience Optimally can be More Effective than Scaling Model Parameters))에 설명된 대로 둘 다를 통해 달성될 수 있습니다. S*는 코드 생성(code generation)을 위해 특별히 설계된 테스트 시점 경험(test-time experience) 스케일링 방법으로, 병렬 스케일링(parallel scaling)(여러 솔루션 생성)과 순차 스케일링(sequential scaling)(반복 디버깅(iterative debugging))을 모두 개선합니다.

"S*: 코드 생성(Code Generation)을 위한 테스트 시점 스케일링(Test Time Scaling)"의 주석이 달린 그림, https://arxiv.org/abs/2502.14382

이 접근 방식은 두 단계로 작동합니다.

**1단계: 생성(Generation)**
모델은 여러 코드 솔루션을 생성하고, 문제 프롬프트에 제공된 실행 결과 및 테스트 케이스를 사용하여 반복적으로 개선합니다. 이는 모델이 솔루션을 제출하고, 테스트를 실행하고, 오류를 수정하는 코딩 경쟁과 유사하다고 생각할 수 있습니다.
1.  모델은 여러 후보 솔루션을 생성합니다.
2.  각 솔루션은 공개 테스트 케이스(사전 정의된 입력-출력 쌍)에서 실행됩니다.
3.  솔루션이 실패하면(잘못된 출력 또는 충돌), 모델은 실행 결과(오류, 출력)를 분석하고 코드를 수정하여 개선합니다.
4.  이 개선 프로세스는 모델이 테스트 케이스를 통과하는 솔루션을 찾을 때까지 반복적으로 계속됩니다.

예를 들어, 모델이 짝수에 대해 True를 반환하고 그렇지 않으면 False를 반환하는 `is_even(n)` 함수를 구현하도록 요청받았다고 가정해 봅시다. 모델의 첫 번째 시도는 다음과 같을 수 있습니다.

```python
def is_even(n):
    return n % 2 # ❌ Incorrect: should be `== 0`
```

모델은 이 구현을 공개 테스트 케이스로 테스트합니다.

| 입력      | 예상       | 모델 출력 | 상태 |
| :-------- | :--------- | :-------- | :--- |
| is_even(4) | True       | False     | ❌ 실패 |
| is_even(3) | False      | True      | ❌ 실패 |

결과를 검토한 후, 모델은 `4 % 2`가 True가 아니라 0을 반환한다는 것을 깨닫고 함수를 수정합니다.

```python
def is_even(n):
    return n % 2 == 0 # ✅ Corrected
```

이제 함수는 모든 공개 테스트를 통과하여 디버깅 단계(debugging phase)를 완료합니다.

**2단계: 선택(Selection)**
여러 솔루션이 공개 테스트를 통과하면, 모델은 최상의 솔루션을 선택해야 합니다(가능하다면). 여기서 S*는 무작위 선택을 피하기 위해 적응형 입력 합성(adaptive input synthesis)을 도입합니다.
1.  모델은 공개 테스트를 모두 통과한 두 솔루션을 비교합니다.
2.  모델은 스스로에게 묻습니다: "이 솔루션들 간의 차이를 드러낼 입력을 생성할 수 있을까?"
3.  새로운 테스트 입력을 생성하고 두 솔루션을 모두 실행합니다.
4.  한 솔루션이 올바른 출력을 생성하고 다른 솔루션이 실패하면, 모델은 더 나은 솔루션을 선택합니다.
5.  두 솔루션이 동일하게 작동하면, 모델은 무작위로 하나를 선택합니다.

예를 들어, `is_perfect_square(n)`의 두 가지 다른 구현을 고려해 봅시다.

```python
import math

def is_perfect_square_A(n):
    return math.isqrt(n) ** 2 == n

def is_perfect_square_B(n):
    return math.sqrt(n).is_integer()
```

둘 다 간단한 예시에 대한 제공된 테스트 케이스를 통과합니다.

```python
n = 25
print(is_perfect_square_A(n)) # ✅ True (Correct)
print(is_perfect_square_B(n)) # ✅ True (Correct)
```

그러나 LLM이 엣지 케이스(edge cases)를 생성하면 그 중 하나가 실패하는 것을 볼 수 있으므로, 이 경우 모델은 솔루션 A를 선택할 것입니다.

```python
n = 10**16 + 1
print(is_perfect_square_A(n)) # ✅ False (Correct)
print(is_perfect_square_B(n)) # ❌ True (Incorrect)
```

**13. 초안 연쇄(Chain of Draft)**
📄 2월 25일, 초안 연쇄: 적게 쓰고 더 빠르게 생각하기(Chain of Draft: Thinking Faster by Writing Less), https://arxiv.org/abs/2502.18600

연구자들은 LLM이 종종 장황한 단계별 설명을 생성하는 반면, 인간은 필수 정보만 담은 간결한 초안에 의존한다는 것을 관찰합니다. 이에 영감을 받아, 그들은 최소한이지만 유익한 중간 단계(intermediate steps)를 생성하여 장황함을 줄이는 프롬프팅 전략인 초안 연쇄(Chain of Draft, CoD)를 제안합니다. 따라서 어떤 의미에서는 토큰을 덜 생성함으로써 테스트 시점 스케일링(test-time scaling)의 효율성을 향상시키는 테스트 시점 스케일링(test-time scaling) 방법입니다.

"초안 연쇄: 적게 쓰고 더 빠르게 생각하기(Chain of Draft: Thinking Faster by Writing Less)"의 주석이 달린 그림, https://arxiv.org/abs/2502.18600

결과를 보면, CoD는 표준 프롬프팅만큼 간결하지만 CoT(Chain of Thought) 프롬프팅만큼 정확한 것으로 보입니다. 앞서 언급했듯이, 제 생각에는 LLM의 장점 중 하나는 사용자가 상호작용 흔적(interaction traces)을 읽고 학습하며 응답을 더 잘 평가/신뢰할 수 있다는 것입니다. CoD는 CoD의 이러한 장점을 다소 약화시킵니다. 그러나 CoT의 정확도를 유지하면서 생성 속도를 높이므로 장황한 중간 단계가 필요하지 않은 경우에는 매우 유용할 수 있습니다.

**14. 더 나은 피드백 및 편집 모델(Better Feedback and Edit Models)**
📄 3월 6일, 전용 피드백 및 편집 모델이 개방형 일반 도메인 작업에 대한 테스트 시점 스케일링(Test-Time Scaling)을 강화한다(Dedicated Feedback and Edit Models Empower Test-Time Scaling for Open-Ended General-Domain Tasks), https://arxiv.org/abs/2503.04378

테스트 시점 상호작용(test-time interaction)을 스케일링하는 많은 기술은 검증 가능한 답변(예: 확인 가능한 수학 및 코드)이 있는 작업에 의존하므로, 글쓰기 및 일반적인 문제 해결과 같은 개방형 작업에 적용하기 어렵습니다. 검증 가능한 답변과 관련된 이러한 한계를 해결하기 위해, 연구자들은 한 모델이 초기 응답을 생성하고, 다른 모델이 피드백("피드백 모델(feedback model)")을 제공하며, 세 번째 모델이 해당 피드백을 기반으로 응답을 개선("편집 모델(edit model)")하는 시스템을 개발합니다. 그들은 인간이 주석을 단 응답 및 피드백의 대규모 데이터셋을 사용하여 이러한 전문화된 "피드백" 및 "편집" 모델을 훈련합니다. 그런 다음 이 모델들은 테스트 시점(test time)에 더 나은 피드백을 생성하고 더 효과적인 편집을 함으로써 응답을 개선하는 데 도움을 줍니다.

Ahead of AI는 독자 지원 출판물입니다. 새로운 게시물을 받고 제 작업을 지원하려면 무료 또는 유료 구독자가 되는 것을 고려해 보세요. 구독

**결론**

사용자 중심 설계(user-centric design)는 모델 가중치(model weights)를 수정할 필요 없이 대규모 언어 모델(LLM)의 유용성(usefulness)을 향상시키는 올해 가장 뜨거운 연구 주제 중 하나가 되었습니다. 위에서 요약한 많은 기술들은 "Feedback" 버튼과 같은 간단한 토큰 기반 개입부터 테스트 시점 개인화 최적화(Test-Time Personalization Optimization) 및 연관 상호작용 연쇄(Chain-of-Associated-Interactions)와 같은 정교한 탐색 및 최적화 기반 전략에 이르기까지 다양합니다.

큰 그림에서 볼 때, 반복되는 주제 중 하나는 테스트 시점(test)에 상호작용(interaction)을 늘리면 상대적으로 작은 모델조차도 표준 접근 방식에 비해 (사용자 만족도 벤치마크에서) 상당한 개선을 달성할 수 있다는 것입니다. 이는 사용자 중심 전략이 더 작고 비용 효율적인 모델과 더 큰 모델 간의 성능 격차를 좁히는 데 도움이 될 수 있음을 시사합니다.

**비용 주의사항**

주의할 점은 사용자 경험 스케일링(user experience scaling)이 상호작용 비용을 증가시킨다는 것입니다. 따라서 상당한 사용자 경험 스케일링(user experience scaling)을 사용하는 작은 모델을 사용할지, 아니면 더 큰 모델을 훈련하고 사용자 경험 스케일링(user experience scaling)을 덜 사용하거나 전혀 사용하지 않을지는 모델 사용량에 따라 계산해야 하는 문제입니다. 예를 들어, 많은 사용자 경험 스케일링(user experience scaling)을 사용하는 o1 모델은 사용자 경험 스케일링(user experience scaling)을 사용하지 않을 가능성이 있는 더 큰 GPT-4.5 모델보다 실제로 약간 더 저렴합니다. (GPT-4.5가 o1 또는 o3 스타일의 사용자 경험 스케일링(user experience scaling)으로 얼마나 잘 작동할지 보는 것은 흥미로울 것입니다.)

**어떤 기술을 사용할까?**

그러나 사용자 중심 설계가 만능 해결책은 아닙니다. 몬테카를로 트리 탐색(Monte Carlo Tree Search), 자체 수정(self-correction), 동적 깊이 스케일링(dynamic-depth scaling)과 같은 방법이 사용자 만족도를 크게 향상시킬 수 있지만, 효과는 여전히 작업과 난이도에 따라 달라집니다. 초기 논문 중 하나가 보여주었듯이, 모든 작업에서 가장 뛰어난 성능을 발휘하는 단일 사용자 중심 설계 기술은 없습니다. 또한, 이러한 접근 방식 중 상당수는 향상된 사용자 경험을 위해 응답 지연 시간(response latency)을 희생하며, 느린 응답은 일부 사용자에게 성가실 수 있습니다. 예를 들어, 저는 더 빠른 응답 시간 때문에 간단한 작업의 경우 보통 o1에서 GPT4o로 전환합니다.

**다음은 무엇인가?**

앞으로 "사용자 중심 설계" 연구의 두 가지 주요 분야를 중심으로 올해 더 많은 논문이 나올 것이라고 생각합니다.
1.  벤치마크에서 최고의 모델을 개발하는 데 순수하게 초점을 맞춘 연구.
2.  다양한 상호작용 작업에서 비용과 성능의 균형을 맞추는 데 관련된 연구.

어떤 경우든, 사용자 중심 설계의 좋은 점은 기존 LLM의 어떤 유형에도 적용하여 특정 작업에 더 적합하게 만들 수 있다는 것입니다.

**주문형 상호작용(Interaction on Demand)**

산업 측면에서 흥미로운 추세는 제가 "주문형 상호작용(interaction on demand)"이라고 부르는 것입니다. DeepMind 정렬 프레임워크 출시 이후, 기업들은 자사 제품에 사용자 경험(user experience) 기능을 추가하기 위해 서두르는 것처럼 보입니다. 여기서 흥미로운 발전은 대부분의 LLM 제공업체가 사용자가 상호작용(interaction) 기능을 활성화하거나 비활성화할 수 있는 옵션을 추가하기 시작했다는 것입니다. 흥미로운 발전은 대부분의 LLM 제공업체가 이제 사용자가 이러한 "상호작용(interaction)" 기능을 활성화하거나 비활성화할 수 있도록 허용한다는 것입니다. 메커니즘은 공개적으로 공유되지 않지만, 사용자 경험 스케일링(user experience scaling)이 줄어든 동일한 모델일 가능성이 높습니다. 예를 들어, Claude 3.7 Sonnet과 Grok 3는 이제 사용자가 모델에 대해 활성화할 수 있는 "상호작용(interaction)" 기능을 가지고 있는 반면, OpenAI는 사용자가 명시적인 사용자 경험 모델(user experience models)을 사용하려면 모델을 전환해야 합니다. 예를 들어, GPT4o/4.5와 o1/o3-mini와 같이 말입니다. 그러나 OpenAI CEO는 GPT4.5가 상호작용 또는 "경험(experience)" 모드를 명시적으로 가지고 있지 않은 마지막 모델이 될 가능성이 높다고 언급했습니다. 오픈 소스 측면에서는 IBM조차도 Granite 모델에 명시적인 "상호작용(interaction)" 토글을 추가했습니다.

전반적으로, 테스트 시점(test-time) 또는 훈련 시점(train-time) 스케일링을 통한 사용자 경험(user experience) 추가 추세는 2025년 LLM의 주요 발전입니다. 시간이 지나면 사용자 경험은 더 이상 선택 사항이나 특별한 기능으로 취급되지 않고, 지시 미세 조정(instruction-finetuned) 또는 RLHF 미세 조정(RLHF-tuned) 모델이 이제 원시 사전 훈련(raw pretrained) 모델에 대한 표준이 된 것처럼, 표준이 될 것이라고 예상합니다.

앞서 언급했듯이, 이 글은 매우 활발한 LLM 연구 활동 덕분에 이미 긴 길이 때문에 사용자 중심 설계에만 초점을 맞췄습니다. 다음 글에서는 LLM을 위한 모든 흥미로운 모델 아키텍처 혁신(model architecture innovation) 방법을 다룰 계획입니다.

이 잡지는 개인적인 열정 프로젝트입니다. 독립 연구자로서 저를 지원하려면 제 책인 "대규모 언어 모델 구축(처음부터)(Build a Large Language Model (From Scratch))"을 구매하거나 유료 구독을 신청하는 것을 고려해 주세요.

지금 Amazon에서 "대규모 언어 모델 구축(처음부터)(Build a Large Language Model (From Scratch))"을 구매할 수 있습니다.

책을 읽고 몇 분의 여유가 있다면, 간단한 리뷰를 남겨주시면 정말 감사하겠습니다. 저자들에게 큰 도움이 됩니다! 여러분의 지원은 큰 의미가 있습니다! 감사합니다!