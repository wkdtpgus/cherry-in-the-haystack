최근 AI 산업에 변화가 생겼습니다. 연구자들은 초지능 AI 시스템, 즉 지능의 홍수가 멀지 않은 미래가 아닌 임박한 시기에 도래할 것이라고 긴급하게 이야기하기 시작했습니다. 그들은 종종 AGI(인공 일반 지능)를 언급하는데, 이는 대부분의 지적 작업에서 전문 인간을 능가할 수 있는 기계로 정의되지만 다소 불명확합니다. 그들은 필요에 따라 지능을 활용할 수 있는 능력이 사회를 깊이 변화시킬 것이며, 그 변화가 곧 일어날 것이라고 주장합니다. 다음은 근시일 내에 초지능 AI가 등장할 것이라고 예측하는 AI 연구소 내 저명한 연구자들의 최근 발언 중 일부입니다. 내부자들의 말을 믿지 않을 이유는 많습니다. 그들은 대담한 예측을 할 분명한 동기를 가지고 있기 때문입니다. 자본을 조달하고, 주식 가치를 높이며, 어쩌면 자신들의 역사적 중요성을 스스로에게 확신시키고 있을지도 모릅니다. 그들은 기술자이지 예언자가 아니며, 기술 예측의 실적은 수십 년 앞서 나간 것으로 판명된 자신감 넘치는 선언들로 가득합니다. 이러한 인간적 편향을 제쳐두더라도, 근본적인 기술 자체는 우리에게 의심할 이유를 제공합니다. 오늘날의 대규모 언어 모델(Large Language Models)은 인상적인 능력에도 불구하고, 근본적으로 일관성 없는 도구로 남아 있습니다. 어떤 작업에서는 뛰어나지만 겉보기에 더 간단한 작업에서는 어려움을 겪습니다. 이러한 "들쭉날쭉한 경계(jagged frontier)"는 현재 AI 시스템의 핵심 특성이며, 쉽게 해결되지 않을 것입니다. 게다가, 연구자들이 1~2년 내에 AGI에 도달할 것이라는 주장이 옳다고 가정하더라도, 그들은 인간이 기술을 채택하고 적응하는 속도를 과대평가하고 있을 가능성이 높습니다. 조직의 변화는 오랜 시간이 걸립니다. 업무, 삶, 교육 시스템의 변화는 훨씬 더 느립니다. 그리고 기술은 세상에서 중요한 특정 용도를 찾아야 하는데, 이 또한 느린 과정입니다. 우리는 지금 당장 AGI를 가질 수도 있지만 대부분의 사람들은 알아차리지 못할 것입니다(실제로 일부 관찰자들은 클로드 3.5(Claude 3.5)와 같은 최신 AI 모델이 사실상 AGI라고 주장하며 이미 일어났다고 시사했습니다 1). 그러나 이러한 예측을 단순한 과장으로 일축하는 것은 도움이 되지 않을 수 있습니다. 그들의 동기가 무엇이든, AI 연구소 내부의 연구자와 엔지니어들은 전례 없는 무언가의 출현을 목격하고 있다고 진정으로 확신하는 듯합니다. 그들의 확신만으로는 중요하지 않을 것입니다. 다만, 점점 더 공개되는 벤치마크(benchmark)와 시연이 우리가 AI 역량의 근본적인 변화에 접근하고 있다고 그들이 믿는 이유를 암시하기 시작했다는 점은 예외입니다. 말하자면, 물이 예상보다 빠르게 차오르는 것 같습니다.

이러한 급진적인 변화의 시기에 AGI에 대한 논의는 여전히 중요하지만, 우리는 또한 현재의 AI 기술이 이미 어떻게 진화하고 있으며, 가까운 미래에 어떤 실제적인 영향을 미칠지에 주목해야 합니다. '들쭉날쭉한 경계'는 여전히 존재하지만, 새로운 아키텍처와 훈련 방법론의 발전은 이러한 불균형을 점진적으로 해소하고 있습니다. 특정 작업에 최적화된 모델의 등장과 다양한 모달리티(modality)를 통합하는 능력은 AI가 더 넓은 범위의 문제에 더욱 일관성 있게 접근할 수 있도록 돕고 있습니다. 이는 단순한 성능 향상을 넘어, AI 시스템이 세상을 이해하고 상호작용하는 방식의 근본적인 변화를 의미합니다.

### 물이 차오르는 곳

가장 많은 추측을 불러일으킨 사건은 12월 말 OpenAI가 o3라는 새로운 모델을 공개한 것이었습니다. OpenAI 외부에서는 아직 아무도 이 시스템을 실제로 사용해 보지 못했지만, o3는 이미 매우 인상적인 o1의 후속 모델입니다 2. o3 모델은 새로운 세대의 '추론기(reasoner)' 중 하나입니다. 이 AI 모델은 질문에 답하기 전에 '생각'하는 데 추가 시간을 할애하여 어려운 문제 해결 능력을 크게 향상시킵니다. OpenAI는 o3에 대한 여러 놀라운 벤치마크(benchmark)를 제공했는데, 이는 o1에 비해, 그리고 우리가 생각했던 AI 분야의 최첨단(state-of-the-art)에 비해 큰 발전을 시사합니다. 특히 세 가지 벤치마크(benchmark)에 주목할 필요가 있습니다.

첫 번째는 대학원 수준의 구글-프루프 Q&A 테스트(Graduate-Level Google-Proof Q&A test, GPQA)라고 불리며, 구글조차도 도움을 줄 수 없는 일련의 객관식 문제로 고수준 지식을 테스트하도록 되어 있습니다. 인터넷에 접속할 수 있는 박사 학위 소지자들은 이 테스트에서 자신의 전문 분야 외 문제의 34%를 맞혔고, 전문 분야 내 문제의 81%를 맞혔습니다. o3는 테스트에서 87%를 달성하여 처음으로 인간 전문가를 능가했습니다.

o3의 이러한 성과는 단순한 숫자 게임을 넘어섭니다. 이는 모델이 복잡한 개념을 이해하고, 추론하며, 때로는 직관에 가까운 방식으로 문제를 해결하는 능력이 크게 향상되었음을 보여줍니다. 최근 GPT-4o나 Llama 3와 같은 다른 선도적인 모델들도 유사한 벤치마크에서 인상적인 결과를 보여주며, AI의 추론 능력이 전반적으로 상향 평준화되고 있음을 시사합니다. 특히, 이러한 모델들은 더 긴 컨텍스트 윈도우(context window)를 통해 방대한 정보를 한 번에 처리하고, 복잡한 지시를 더 잘 따르며, 일관성 있는 응답을 생성하는 데 탁월한 성능을 보입니다. 이는 단순히 정보를 기억하는 것을 넘어, 새로운 정보를 기존 지식과 통합하여 창의적인 해결책을 제시하는 능력으로 이어집니다. 합성 데이터(synthetic data)의 활용이 이러한 모델 훈련에 더욱 중요해지면서, 실제 데이터의 한계를 넘어 모델의 일반화 능력과 견고성을 강화하고 있습니다.

두 번째는 프론티어 수학(Frontier Math)으로, 수학자들이 풀기 매우 어렵게 만든 비공개 수학 문제 세트입니다. 실제로 o3 이전에는 어떤 AI도 2% 이상 득점한 적이 없었지만, o3는 25%를 맞혔습니다.

마지막 벤치마크(benchmark)는 ARC-AGI입니다. 이는 인간에게는 비교적 쉽지만 AI에게는 어려운 것으로 설계된 유동 지능(fluid intelligence)에 대한 꽤 유명한 테스트입니다. 다시 한번, o3는 87.5%를 기록하며 이전의 모든 AI와 기준 인간 수준을 능가했습니다.

이 모든 테스트에는 중요한 주의사항(caveat)이 따르지만 3, 이는 우리가 이전에 AI 성능에 대한 통과 불가능한 장벽이라고 생각했던 것들이 실제로는 상당히 빠르게 극복될 수 있음을 시사합니다.

### 에이전트(Agent)

AI가 더 똑똑해짐에 따라, 그들은 더 효과적인 에이전트(agent)가 됩니다. 에이전트(agent)는 또 다른 모호하게 정의된 용어(패턴이 보이시나요?)로, 일반적으로 일련의 목표를 달성하기 위해 자율적으로 행동할 수 있는 능력을 부여받은 AI를 의미합니다. 저는 이전 게시물에서 초기 에이전트 시스템 중 일부를 시연했지만, 지난 몇 주 동안은 좁지만 경제적으로 중요한 분야에서 실용적인 에이전트가 이제 실현 가능하다는 것을 보여주었습니다.

좋은 예시로는 구글의 제미니(Gemini) 딥 리서치(Deep Research)(제미니(Gemini) 구독자라면 누구나 이용 가능)가 있는데, 이는 실제로 전문화된 연구 에이전트입니다. 저는 "고성장 벤처를 위한 스타트업 자금 조달 방식 비교를 창업자 관점에서 연구하라"와 같은 주제를 주었고, 에이전트 시스템은 계획을 세우고, 173개(!) 웹사이트를 읽고, 몇 분 후에 저를 위한 보고서를 작성했습니다. 그 결과는 118개의 참고 문헌이 있는 17페이지 분량의 보고서였습니다!

최근에는 코딩 에이전트, 금융 분석 에이전트, 심지어 의료 진단 보조 에이전트와 같이 특정 도메인에 특화된 에이전트들이 빠르게 발전하고 있습니다. 예를 들어, Devin과 같은 코딩 에이전트는 복잡한 소프트웨어 개발 작업을 독립적으로 수행하고, 버그를 수정하며, 새로운 기능을 추가하는 등 개발자의 생산성을 혁신적으로 향상시킬 잠재력을 보여주었습니다. 이러한 '좁은 에이전트(narrow agent)'들은 특정 목표를 달성하기 위해 여러 도구를 활용하고, 자체적으로 계획을 세우며, 피드백을 통해 학습하는 '에이전트적 워크플로우(agentic workflow)'를 구현합니다. 이는 단순히 질문에 답하는 것을 넘어, 실제 세계에서 행동을 취하고 결과를 만들어내는 AI 시스템의 새로운 패러다임을 제시합니다. 하지만 에이전트의 자율성이 커질수록, 그들의 행동이 예측 가능하고 안전하며 윤리적인 방식으로 이루어지도록 보장하는 것이 더욱 중요해집니다. '범용 에이전트(generalist agent)'에 대한 논의는 여전히 활발하지만, 현재 우리는 특정 분야에서 놀라운 효율성을 보여주는 좁은 에이전트의 시대에 살고 있습니다.

### 많은 작은 발전이 일어나고 있다

에이전트와 매우 똑똑한 모델은 혁신적인 AI에 필요한 핵심 요소이지만, 빠르게 발전하고 있는 다른 많은 부분들도 있습니다. 여기에는 AI가 기억할 수 있는 양(컨텍스트 윈도우(context window))의 발전과 보고 말할 수 있게 해주는 멀티모달(multimodal) 기능이 포함됩니다. 진행 상황을 파악하기 위해 과거를 조금 돌아보는 것이 도움이 될 수 있습니다. 예를 들어, 저는 ChatGPT가 나오기 전부터 이미지 및 비디오 모델에 대해 "와이파이를 사용하는 비행기 위의 수달(otter on a plane using wifi)"이라는 프롬프트(prompt)를 테스트해 왔습니다. 2023년 10월에는 이 프롬프트(prompt)가 이 끔찍한 괴물을 만들어냈습니다.

"와이파이를 사용하는 비행기 위의 수달" 2023년 10월

불과 1년 반 만에 멀티모달 AI는 단순한 이미지 생성 능력을 넘어섰습니다. 이제 AI는 텍스트 프롬프트뿐만 아니라 이미지, 오디오, 비디오를 입력으로 받아들여 더욱 풍부하고 복잡한 콘텐츠를 생성하고 이해할 수 있습니다. 예를 들어, 특정 스타일과 감정을 담은 짧은 영화를 텍스트 지시만으로 만들거나, 오디오 클립을 분석하여 화자의 감정을 파악하고 이를 기반으로 새로운 대화를 생성하는 것이 가능해졌습니다. 최근에는 3D 모델링, 촉각 피드백(haptic feedback) 생성, 심지어 특정 재료의 물성을 시뮬레이션하는 AI 모델까지 등장하며, AI가 다룰 수 있는 모달리티의 범위가 상상할 수 없을 정도로 확장되고 있습니다. 이러한 발전은 AI를 더욱 인간의 감각과 유사하게 만들며, HCI(Human-Computer Interaction) 분야에 혁신적인 변화를 가져오고 있습니다.

또한, 모델의 효율성과 접근성도 크게 향상되었습니다. GPT-4o와 같은 최첨단 모델은 실시간 음성 대화를 가능하게 할 정도로 빠른 응답 속도를 자랑하며, 동시에 더 작고 효율적인 모델들이 개인 기기나 엣지(edge) 환경에서 실행될 수 있도록 최적화되고 있습니다. 이는 AI 기술의 민주화를 가속화하여, 더 많은 개발자와 사용자들이 고성능 AI를 쉽게 활용하고 자신만의 혁신적인 애플리케이션을 구축할 수 있게 합니다. 이러한 'AI 민주화'는 기술의 파급력을 더욱 증폭시키고, 예상치 못한 분야에서 새로운 활용 사례를 창출할 것입니다.

### 홍수는 어떠한가?

이 모든 것을 고려할 때, AI 연구소의 '지능의 홍수'가 오고 있다는 주장을 얼마나 진지하게 받아들여야 할까요? 우리가 이미 본 것들, 즉 이전 장벽을 허무는 o3 벤치마크(benchmark), 복잡한 연구를 수행하는 좁은 범위의 에이전트(narrow agent), 그리고 점점 더 정교한 콘텐츠를 생성하는 멀티모달(multimodal) 시스템만을 고려하더라도, 우리는 많은 지식 기반 작업을 변화시킬 수 있는 능력을 보고 있습니다. 그럼에도 불구하고 연구소들은 이것이 단지 시작일 뿐이며, 훨씬 더 유능한 시스템과 범용 에이전트(general agent)가 임박했다고 주장합니다.

저를 가장 우려하게 하는 것은 연구소들이 이 타임라인(timeline)에 대해 맞는지 여부가 아닙니다. 그것은 그들이 옳을 가능성은 말할 것도 없고, 현재 AI 수준이 할 수 있는 일에 대해서도 우리가 충분히 대비하지 못하고 있다는 점입니다. AI 연구자들은 정렬(alignment)에 집중하여 AI 시스템이 윤리적이고 책임감 있게 행동하도록 보장하고 있지만, 인공지능으로 가득 찬 세상이 실제로 어떤 모습일지 상상하고 명확히 표현하려는 목소리는 훨씬 적습니다. 이것은 단지 기술 자체에 관한 것만이 아닙니다. 그것은 우리가 그것을 어떻게 형성하고 배치할지 선택하는 방법에 관한 것입니다. 이러한 질문들은 AI 개발자만이 답할 수 있거나 답해야 하는 질문이 아닙니다. 이 전환을 헤쳐나가야 할 조직 리더들, 업무 생활이 변화할 수 있는 직원들, 그리고 미래가 이러한 결정에 달려 있을 수 있는 이해관계자들의 관심을 요구하는 질문들입니다. 다가올 수 있는 지능의 홍수는 본질적으로 좋거나 나쁘지 않습니다. 그러나 우리가 그것을 어떻게 준비하고, 어떻게 적응하며, 가장 중요하게는 어떻게 사용할지 선택하는 것이 그것이 진보의 힘이 될지 아니면 혼란의 힘이 될지 결정할 것입니다. 이러한 대화를 시작할 시기는 물이 차오르기 시작한 후가 아니라 바로 지금입니다.

더 나아가, AI의 발전은 기술적 측면뿐만 아니라 사회 경제적, 윤리적 측면에서 광범위한 영향을 미칩니다. 일자리의 미래, 교육 시스템의 개혁, 정보의 접근성과 신뢰성, 그리고 심지어 인간 정체성의 본질에 대한 질문까지 던지고 있습니다. AI 정렬은 단순히 기술적인 문제를 넘어, AI 시스템이 인간의 가치와 목표에 부합하도록 설계되고 사용되는 것을 의미합니다. 이는 AI 개발자, 정책 입안자, 기업 리더, 학계, 그리고 일반 시민 모두가 참여하는 다각적인 대화와 협력이 필요한 복합적인 도전입니다. 우리는 AI가 가져올 잠재적 이점을 극대화하면서도, 발생할 수 있는 위험을 최소화하기 위한 견고한 거버넌스 프레임워크와 사회적 합의를 구축해야 합니다. 지금이야말로 AI의 미래를 단순히 기술 개발자에게만 맡겨두는 것이 아니라, 모두가 함께 고민하고 만들어갈 때입니다.

구독 공유

1 저는 클로드(Claude)에게 완성된 문서를 읽고 피드백을 달라고 요청했고, 클로드는 다음과 같이 썼습니다: "클로드 3.5(Claude 3.5)에 대한 괄호 안의 언급은 잠재적인 AGI의 예시로 언급되었으므로 업데이트 또는 수정의 이점을 얻을 수 있습니다. 클로드 3.5 소네트(Claude 3.5 Sonnet)로서, 저는 AGI와 관련하여 제 능력에 대한 특정 주장을 확인할 수 없다는 점을 말씀드립니다."
2 그들은 o2라는 이름을 건너뛰었는데, 이는 영국에 있는 전화 회사의 이름이기 때문입니다. AI 명명은 여전히 매우 좋지 않습니다.
3 GPQA에 대한 주의사항(caveat)은 데이터가 공개적으로 이용 가능하며, 모델이 우연히든 의도적으로든 해당 데이터로 훈련되었을 가능성이 있다는 것입니다. 비록 그렇게 했다는 징후는 없지만 말입니다. 프론티어 수학(Frontier Math) 테스트의 주의사항(caveat)은 문제의 난이도가 다르다는 것입니다. 1단계(Tier 1)는 어려운 수학 올림피아드 문제이고, 2단계(Tier 2)는 대학원 수준 문제이며, 3단계(Tier 3)는 진정한 연구 수준 문제입니다. o3의 정답에 대해 책임 있는 수학자의 말에 따르면: "40%는 1단계, 50%는 2단계, 10%는 3단계였습니다. 그러나 대부분의 3단계 '해결책'과 많은 2단계 해결책은 진정한 수학적 이해보다는 발견적 지름길(heuristic shortcut)에서 비롯되었습니다." ARC-AGI에 대한 주의사항(caveat)은 o3가 높은 점수를 달성하기 위해 충분히 오래 실행하는 데 매우 비싼 컴퓨터 시간이 많이 필요했다는 것입니다.