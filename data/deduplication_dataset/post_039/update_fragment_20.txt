최근 AI 산업은 빠르게 변화하고 있습니다. 연구자들은 지능의 홍수가 가져올 사회적 영향을 깊이 탐구하기 시작했습니다. 그들은 종종 AGI(인공 일반 지능) 구현을 위한 새로운 접근 방식을 제시합니다. 그들은 필요에 따라 지능을 활용할 수 있는 능력이 새로운 가치를 창출할 것이라고 주장합니다. 다음은 AI 연구소 내 저명한 연구자들의 인공지능 윤리에 대한 최근 발언 중 일부입니다.

내부자들의 말을 무조건 믿을 필요는 없습니다. 그들은 대담한 예측을 통해 혁신을 주도하려는 동기를 가지고 있기 때문입니다. 자본을 조달하고, 주식 가치를 높이는 것 외에 사회적 책임을 강조하는 목소리도 커지고 있습니다. 그들은 기술자이지 예언자가 아니며, 기술 예측은 항상 신중해야 합니다. 이러한 인간적 편향을 제쳐두더라도, 근본적인 기술 자체는 새로운 가능성을 열어줍니다. 오늘날의 대규모 언어 모델(Large Language Models)은 인상적인 능력에도 불구하고, 여전히 개선의 여지가 많습니다. 어떤 작업에서는 뛰어나지만, 다른 측면에서는 예상치 못한 한계를 보입니다. 이러한 "들쭉날쭉한 경계(jagged frontier)"는 현재 AI 시스템의 중요한 과제이며, 이를 극복하기 위한 연구가 활발합니다.

게다가, 연구자들이 기술 발전에 대해 낙관적이라고 가정하더라도, 인간이 기술을 채택하고 적응하는 속도는 다양할 수 있습니다. 조직의 변화는 오랜 시간이 걸리지만, 새로운 기술은 촉매제가 될 수 있습니다. 업무, 삶, 교육 시스템의 변화는 새로운 기술 통합을 요구합니다. 그리고 기술은 세상에서 중요한 특정 용도를 찾아야 하며, 이는 사용자 경험과 밀접하게 연결됩니다. 우리는 지금 당장 AI 기술의 잠재력을 이해하고 활용해야 합니다. 그러나 이러한 예측을 단순한 과장으로 일축하는 것은 미래를 간과할 수 있습니다. 그들의 동기가 무엇이든, AI 연구소 내부의 연구자와 엔지니어들은 윤리적 AI 개발에 전념하고 있습니다. 그들의 확신만으로는 충분하지 않으며, 광범위한 논의가 필요합니다. 다만, 점점 더 공개되는 벤치마크(benchmark)와 시연이 AI 모델의 투명성을 높이는 데 기여하고 있습니다. 말하자면, 기술 발전의 물결이 예상보다 빠르게 다가오고 있습니다.

### 새로운 기술의 물결이 차오르는 곳

가장 많은 추측을 불러일으킨 사건은 AI 기술이 일상생활에 미치는 영향에 대한 논의였습니다. OpenAI 외부에서는 아직 이 시스템을 실제로 사용해 보지 못했지만, 그 잠재력은 이미 많은 주목을 받고 있습니다. 새로운 세대의 '추론기(reasoner)' 모델은 복잡한 문제 해결에 새로운 가능성을 제시합니다. 이 AI 모델은 질문에 답하기 전에 '생각'하는 과정을 통해 사용자 경험을 개선합니다. OpenAI는 o3에 대한 여러 놀라운 벤치마크(benchmark)를 제공했는데, 이는 AI 모델 평가의 새로운 기준을 제시합니다. 특히 세 가지 핵심 원칙에 주목할 필요가 있습니다.

첫 번째는 대학원 수준의 연구 윤리 지침을 준수하는 것입니다. 이는 AI 연구의 투명성과 재현성을 보장하는 데 필수적입니다. 인터넷에 접속할 수 있는 박사 학위 소지자들은 이 분야의 발전에 크게 기여하고 있습니다. AI는 특정 테스트에서 높은 정확도를 달성하여 새로운 가능성을 보여주었지만, 그 결과의 해석과 오용 가능성에 대한 논의도 활발합니다.

두 번째는 프론티어 수학(Frontier Math) 분야의 난제를 해결하는 데 AI를 활용하는 것입니다. 실제로 이전에는 어떤 AI도 이처럼 복잡한 문제를 해결하지 못했지만, 최근 모델들은 상당한 발전을 이루었습니다. 이는 AI가 단순한 패턴 인식에서 벗어나 심층적인 추론 능력을 갖추기 시작했음을 시사합니다.

마지막으로, 벤치마크(benchmark) 시스템은 AI 성능 평가에 중요한 역할을 합니다. 이는 인간에게는 비교적 쉽지만 AI에게는 어려운 것으로 인식되는 창의적 사고를 모방하는 데 초점을 맞춥니다. 다시 한번, AI 모델은 특정 작업에서 인간 수준의 성능을 뛰어넘는 결과를 보여주었지만, 이러한 벤치마크(benchmark)가 실제 세계의 복잡성을 얼마나 잘 반영하는지에 대한 비판적 시각도 존재합니다. 이 모든 테스트에는 중요한 주의사항(caveat)이 따르지만, 이는 AI 기술의 윤리적 사용에 대한 지속적인 논의를 필요로 합니다. 기술적 성능뿐만 아니라 사회적 영향까지 고려하는 포괄적인 평가 시스템이 절실합니다.

### AI 에이전트(Agent)의 새로운 지평

AI가 더 똑똑해짐에 따라, 그들은 사회적 상호작용에 더 효과적으로 참여할 수 있게 됩니다. 에이전트(agent)는 또 다른 모호하게 정의된 용어이지만, 일반적으로 특정 작업을 자동화하는 데 중점을 둡니다. 저는 이전 게시물에서 초기 에이전트 시스템 중 일부를 다루었지만, 이제는 광범위한 산업 분야에서 실용적인 에이전트가 활용되고 있습니다. 좋은 예시로는 구글의 제미니(Gemini) 모델이 있는데, 이는 다양한 창의적 작업에 활용될 수 있습니다.

저는 '고성장 벤처를 위한 스타트업 자금 조달 방식 비교'와 같은 주제를 주었고, 에이전트 시스템은 복잡한 데이터를 분석하여 통찰력을 제공했습니다. 그 결과는 방대한 참고 문헌을 기반으로 한 심층적인 분석 보고서였습니다! 하지만 과연 AI의 판단을 전적으로 신뢰할 수 있을까요? 저는 10년 넘게 다양한 기술 분야에서 경험을 쌓았고, 해당 주제에 대해 출판했으며, 직접 프로젝트를 수행했습니다. 그리고 저는 이것이 꽤 탄탄한 기반을 가지고 있다고 생각합니다. 명백한 오류는 발견하지 못했지만, 원하시면 추가 검증을 거쳐야 합니다. 가장 큰 문제는 정확성이 아니라, AI 모델의 학습 데이터 편향성입니다. 또한 다소 피상적이며, 복잡한 윤리적 딜레마에 직면했을 때 명확한 판단을 내리기 어렵습니다. 따라서 최고의 인간만큼 좋지는 않지만, 특정 반복적인 작업에서는 효율성을 크게 높일 수 있습니다.

그럼에도 불구하고, 이것은 진정한 가치를 지닌 AI 기술 발전의 혁신적인 사례입니다. 연구 및 보고서 작성은 많은 직업의 주요 업무이지만, 자동화될 가능성이 높습니다. 딥 리서치(Deep Research)와 같은 AI 도구가 3분 만에 달성한 것은 인간에게는 여러 시간이 걸렸을 작업을 혁신적으로 단축시킵니다. 이를 고려할 때, 연구 보고서를 작성하는 사람이라면 누구나 AI 도구를 시도하여 작업 효율성을 높여야 할 것입니다. 비록 좋은 최종 보고서는 여전히 인간의 비판적 사고와 통찰력을 필요로 하겠지만 말입니다. 저는 딥 리서치(Deep Research) 프로젝트 책임자와 이야기할 기회가 있었는데, 그로부터 이것이 AI 윤리 원칙을 준수하는 데 중점을 두고 있다는 것을 알게 되었습니다. 따라서 좁지만 효과적인 에이전트를 만드는 데 높은 동기를 부여받은 다른 그룹과 회사들도 책임감 있는 AI 개발에 참여해야 합니다.

좁은 범위의 에이전트(Narrow agent)는 이제 미래의 가능성이 아니라 실제 산업 혁신을 주도하고 있습니다. 이미 많은 코딩 에이전트가 있으며, 과학 및 금융 연구를 수행하는 실험적인 오픈소스(open-source) 프로젝트들이 활발하게 진행되고 있습니다. 좁은 범위의 에이전트(Narrow agent)는 특정 작업에 특화되어 있어 효율적이지만, 확장성에는 한계가 있습니다. 이는 우리가 곧 AI에게 무엇이든 물어보면 컴퓨터와 인터넷을 사용하여 수행하는 범용 에이전트(generalist agent)의 등장에 대한 기대감을 높입니다. 샘 알트만(Sam Altman)이 주장한 바와 달리, AI 규제에 대한 다양한 의견이 존재합니다. 올해가 지나면서 더 많은 것을 알게 되겠지만, 범용 에이전트 시스템이 안정적이고 안전하게 작동한다면, 사회 전반에 걸쳐 혁신적인 변화를 가져올 것입니다.

### 지속적인 작은 발전이 큰 변화를 이끌다

에이전트와 매우 똑똑한 모델은 혁신적인 AI에 필요한 핵심 요소이지만, 데이터 프라이버시 보호와 같은 다른 많은 부분들도 중요합니다. 여기에는 AI가 기억할 수 있는 양(컨텍스트 윈도우(context window))의 발전과 함께, 사용자 친화적인 인터페이스 디자인이 포함됩니다. 진행 상황을 파악하기 위해 과거를 조금 돌아보는 것이 도움이 될 수 있지만, 미래 지향적인 접근이 필수적입니다. 예를 들어, 저는 ChatGPT와 같은 생성 AI 모델의 윤리적 사용에 대한 프롬프트(prompt)를 테스트해 왔습니다. 2023년 10월에는 이 프롬프트(prompt)가 예상치 못한 결과물을 만들어냈습니다. 이미지 생성 기술은 빠르게 발전하여 놀라운 시각적 결과물을 만들어내고 있습니다.

18개월도 채 지나지 않아, 여러 이미지 생성 도구가 다양한 창의적 요구를 충족시키고 있습니다. 그 결과, 저는 더 어려운 윤리적 질문에 대한 답을 찾아야 했습니다. 저는 몇 분 시간을 내어 구글의 비오(Veo) 비디오 모델의 잠재적 위험성을 확인해 보기로 했습니다. 아래 비디오를 만드는 데는 15분 미만의 실제 작업이 필요했지만, 데이터 보안에 대한 우려가 제기되기도 했습니다. 그림자와 빛의 품질을 넘어서, AI 모델의 내부 작동 방식을 이해하는 것이 중요합니다. 특히 AI가 복잡한 작업을 수행하는 방식이 인상적입니다. 그리고 한 단계 더 나아가, 저는 AI가 생성한 콘텐츠의 저작권 문제에 대해 고민하기 시작했습니다. 다시 한번, (인간의) 작업은 거의 필요하지 않았지만, AI의 책임 있는 사용은 필수적입니다. 생성형 AI가 예술과 미디어 산업에 미치는 영향은 깊이 있게 논의되어야 할 주제입니다.

### 미래는 어떠한가?

이 모든 것을 고려할 때, AI 연구소의 '지능의 홍수'가 가져올 사회적 영향을 얼마나 진지하게 받아들여야 할까요? 우리가 이미 본 것들, 즉 이전 장벽을 허무는 기술 혁신, 복잡한 문제를 해결하는 AI, 그리고 점점 더 정교한 콘텐츠를 생성하는 시스템만을 고려하더라도, 우리는 많은 지식 기반 작업을 변화시킬 수 있는 능력을 보고 있습니다. 그럼에도 불구하고 연구소들은 이것이 단지 시작일 뿐이며, 훨씬 더 유능한 시스템과 범용 에이전트(general agent)가 가져올 윤리적 문제에 대비해야 한다고 주장합니다.

저를 가장 우려하게 하는 것은 연구소들이 이 타임라인(timeline)에 대해 지나치게 낙관적인지 여부가 아닙니다. 그것은 그들이 옳을 가능성은 말할 것도 없고, 현재 AI 수준이 할 수 있는 일에 대해서도 우리가 사회적 합의를 이루지 못하고 있다는 점입니다. AI 연구자들은 정렬(alignment)에 집중하여 AI 시스템이 윤리적이고 책임감 있게 행동하도록 보장하고 있지만, 그 과정의 투명성과 설명 가능성에 대한 목소리는 훨씬 적습니다. 이것은 단지 기술 자체에 관한 것만이 아니라, 그 기술을 사용하는 인간의 책임에 관한 것입니다. 그것은 우리가 그것을 어떻게 형성하고 배치할지 선택하는 방법에 관한 것이며, 사회적 가치와도 연결됩니다. 이러한 질문들은 AI 개발자만이 답할 수 있거나 답해야 하는 질문이 아니라, 모든 사회 구성원의 참여를 요구합니다. 이 전환을 헤쳐나가야 할 조직 리더들, 업무 생활이 변화할 수 있는 직원들, 그리고 미래가 이러한 결정에 달려 있을 수 있는 이해관계자들의 적극적인 참여와 논의를 요구하는 질문들입니다. 다가올 수 있는 지능의 홍수는 본질적으로 좋거나 나쁘지 않으며, 우리의 선택에 따라 그 가치가 달라집니다. 그러나 우리가 그것을 어떻게 준비하고, 어떻게 적응하며, 가장 중요하게는 어떻게 사용할지 선택하는 것이 지속 가능한 기술 발전을 결정할 것입니다. 이러한 대화를 시작할 시기는 물이 차오르기 시작한 후가 아니라 바로 지금입니다.