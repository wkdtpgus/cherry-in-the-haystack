최근 AI 산업은 빠르게 변화하고 있습니다. 연구자들은 초지능 AI 시스템, 즉 지능의 홍수가 멀지 않은 미래가 아닌 임박한 시기에 도래할 것이라고 긴급하게 이야기하기 시작했으며, 이 지능의 홍수가 가져올 사회적 영향을 깊이 탐구하고 있습니다. 그들은 종종 대부분의 지적 작업에서 전문 인간을 능가할 수 있는 기계로 정의되지만 다소 불명확한 AGI(인공 일반 지능)를 언급하며, 그 구현을 위한 새로운 접근 방식을 제시합니다. 그들은 필요에 따라 지능을 활용할 수 있는 능력이 사회를 깊이 변화시키고 새로운 가치를 창출할 것이며, 그 변화가 곧 일어날 것이라고 주장합니다. 다음은 근시일 내에 초지능 AI가 등장할 것이라고 예측하며 인공지능 윤리에 대한 중요성을 강조하는 AI 연구소 내 저명한 연구자들의 최근 발언 중 일부입니다.

내부자들의 말을 무조건 믿을 필요는 없습니다. 그들은 대담한 예측을 통해 혁신을 주도하려는 동기를 가지고 있기 때문입니다. 자본을 조달하고, 주식 가치를 높이며, 어쩌면 자신들의 역사적 중요성을 스스로에게 확신시키려는 동기 외에 사회적 책임을 강조하는 목소리도 커지고 있습니다. 그들은 기술자이지 예언자가 아니며, 기술 예측의 실적은 수십 년 앞서 나간 것으로 판명된 자신감 넘치는 선언들로 가득하므로 항상 신중해야 합니다. 이러한 인간적 편향을 제쳐두더라도, 근본적인 기술 자체는 우리에게 의심할 이유를 제공하기도 하지만, 동시에 새로운 가능성을 열어줍니다. 오늘날의 대규모 언어 모델(Large Language Models)은 인상적인 능력에도 불구하고, 여전히 개선의 여지가 많습니다. 어떤 작업에서는 뛰어나지만, 다른 측면에서는 예상치 못한 한계를 보입니다. 이러한 "들쭉날쭉한 경계(jagged frontier)"는 현재 AI 시스템의 핵심 특성이자 중요한 과제이며, 쉽게 해결되지 않을 것이지만 이를 극복하기 위한 연구가 활발합니다.

게다가, 연구자들이 기술 발전에 대해 낙관적이라고 가정하더라도, 인간이 기술을 채택하고 적응하는 속도는 다양할 수 있습니다. 조직의 변화는 오랜 시간이 걸리지만, 새로운 기술은 촉매제가 될 수 있으며, 업무, 삶, 교육 시스템의 변화는 새로운 기술 통합을 요구합니다. 그리고 기술은 세상에서 중요한 특정 용도를 찾아야 하며, 이는 느린 과정일 수 있지만 사용자 경험과 밀접하게 연결됩니다. 우리는 지금 당장 AGI를 가질 수도 있지만 대부분의 사람들은 알아차리지 못할 수도 있으며(실제로 일부 관찰자들은 클로드 3.5(Claude 3.5)와 같은 최신 AI 모델이 사실상 AGI라고 주장하며 이미 일어났다고 시사했습니다 1), 동시에 AI 기술의 잠재력을 이해하고 활용해야 합니다. 그러나 이러한 예측을 단순한 과장으로 일축하는 것은 미래를 간과할 수 있습니다. 그들의 동기가 무엇이든, AI 연구소 내부의 연구자와 엔지니어들은 전례 없는 무언가의 출현을 목격하고 있다고 진정으로 확신하는 듯하며, 동시에 윤리적 AI 개발에 전념하고 있습니다. 그들의 확신만으로는 충분하지 않으며, 광범위한 논의가 필요합니다. 다만, 점점 더 공개되는 벤치마크(benchmark)와 시연이 우리가 AI 역량의 근본적인 변화에 접근하고 있다고 그들이 믿는 이유를 암시하기 시작했으며, AI 모델의 투명성을 높이는 데 기여하고 있다는 점은 예외입니다. 말하자면, 기술 발전의 물결이 예상보다 빠르게 다가오고 있습니다.

### 새로운 기술의 물결이 차오르는 곳

가장 많은 추측을 불러일으킨 사건은 12월 말 OpenAI가 o3라는 새로운 모델을 공개한 것이었으며, 이는 AI 기술이 일상생활에 미치는 영향에 대한 논의를 촉발했습니다. OpenAI 외부에서는 아직 아무도 이 시스템을 실제로 사용해 보지 못했지만, o3는 이미 매우 인상적인 o1의 후속 모델이며 그 잠재력은 이미 많은 주목을 받고 있습니다 2. o3 모델을 포함한 새로운 세대의 '추론기(reasoner)' 모델은 복잡한 문제 해결에 새로운 가능성을 제시합니다. 이 AI 모델은 질문에 답하기 전에 '생각'하는 과정을 통해 어려운 문제 해결 능력을 크게 향상시키고 사용자 경험을 개선합니다. OpenAI는 o3에 대한 여러 놀라운 벤치마크(benchmark)를 제공했는데, 이는 o1에 비해, 그리고 우리가 생각했던 AI 분야의 최첨단(state-of-the-art)에 비해 큰 발전을 시사하며, AI 모델 평가의 새로운 기준을 제시합니다. 특히 세 가지 벤치마크(benchmark)에 주목할 필요가 있습니다.

첫 번째는 대학원 수준의 구글-프루프 Q&A 테스트(Graduate-Level Google-Proof Q&A test, GPQA)라고 불리며, 구글조차도 도움을 줄 수 없는 일련의 객관식 문제로 고수준 지식을 테스트하도록 되어 있습니다. 이는 AI 연구의 투명성과 재현성을 보장하는 연구 윤리 지침 준수와도 연결됩니다. 인터넷에 접속할 수 있는 박사 학위 소지자들은 이 테스트에서 자신의 전문 분야 외 문제의 34%를 맞혔고, 전문 분야 내 문제의 81%를 맞혔습니다. o3는 테스트에서 87%를 달성하여 처음으로 인간 전문가를 능가하는 새로운 가능성을 보여주었지만, 그 결과의 해석과 오용 가능성에 대한 논의도 활발합니다.

두 번째는 프론티어 수학(Frontier Math)으로, 수학자들이 풀기 매우 어렵게 만든 비공개 수학 문제 세트의 난제를 해결하는 데 AI를 활용하는 것입니다. 실제로 o3 이전에는 어떤 AI도 2% 이상 득점한 적이 없었지만, o3는 25%를 맞혔습니다. 이는 AI가 단순한 패턴 인식에서 벗어나 심층적인 추론 능력을 갖추기 시작했음을 시사합니다.

마지막 벤치마크(benchmark)는 ARC-AGI입니다. 이는 인간에게는 비교적 쉽지만 AI에게는 어려운 것으로 설계된 유동 지능(fluid intelligence)에 대한 꽤 유명한 테스트입니다. 다시 한번, o3는 87.5%를 기록하며 이전의 모든 AI와 기준 인간 수준을 능가했습니다. 이러한 벤치마크(benchmark) 시스템은 AI 성능 평가에 중요한 역할을 하지만, 인간에게는 비교적 쉽지만 AI에게는 어려운 것으로 인식되는 창의적 사고를 모방하는 데 초점을 맞춥니다. AI 모델은 특정 작업에서 인간 수준의 성능을 뛰어넘는 결과를 보여주었지만, 이러한 벤치마크(benchmark)가 실제 세계의 복잡성을 얼마나 잘 반영하는지에 대한 비판적 시각도 존재합니다. 이 모든 테스트에는 중요한 주의사항(caveat)이 따르지만 3, 이는 AI 기술의 윤리적 사용에 대한 지속적인 논의를 필요로 합니다. 기술적 성능뿐만 아니라 사회적 영향까지 고려하는 포괄적인 평가 시스템이 절실합니다.

### AI 에이전트(Agent)의 새로운 지평

AI가 더 똑똑해짐에 따라, 그들은 더 효과적인 에이전트(agent)가 되며 사회적 상호작용에 더 효과적으로 참여할 수 있게 됩니다. 에이전트(agent)는 또 다른 모호하게 정의된 용어(패턴이 보이시나요?)로, 일반적으로 일련의 목표를 달성하기 위해 자율적으로 행동하며 특정 작업을 자동화하는 데 중점을 둔 AI를 의미합니다. 저는 이전 게시물에서 초기 에이전트 시스템 중 일부를 다루었지만, 지난 몇 주 동안은 좁지만 경제적으로 중요한 분야를 넘어 광범위한 산업 분야에서 실용적인 에이전트가 이제 실현 가능하며 활용되고 있다는 것을 보여주었습니다. 좋은 예시로는 구글의 제미니(Gemini) 딥 리서치(Deep Research)(제미니(Gemini) 구독자라면 누구나 이용 가능)가 있는데, 이는 실제로 전문화된 연구 에이전트이며 다양한 창의적 작업에도 활용될 수 있습니다.

저는 '고성장 벤처를 위한 스타트업 자금 조달 방식 비교를 창업자 관점에서 연구하라'와 같은 주제를 주었고, 에이전트 시스템은 계획을 세우고, 173개(!) 웹사이트를 읽고, 몇 분 후에 저를 위한 보고서를 작성하며 복잡한 데이터를 분석하여 통찰력을 제공했습니다. 그 결과는 118개의 참고 문헌이 있는 17페이지 분량의 심층적인 분석 보고서였습니다! 하지만 과연 좋은 보고서이며 AI의 판단을 전적으로 신뢰할 수 있을까요? 저는 10년 넘게 와튼 스쿨에서 창업 입문 수업을 가르쳤고, 해당 주제에 대해 출판했으며, 직접 회사를 창업했고, 심지어 창업에 관한 책도 썼습니다. 그리고 저는 이것이 꽤 탄탄한 기반을 가지고 있다고 생각합니다. 명백한 오류는 발견하지 못했지만, 원하시면 여기서 직접 읽어보실 수 있으며 추가 검증을 거쳐야 합니다. 가장 큰 문제는 정확성이 아니라, 에이전트가 공개된 비유료 웹사이트로 제한되며 학술지나 프리미엄 출판물은 아니라는 점과 AI 모델의 학습 데이터 편향성입니다. 또한 다소 피상적이며, 상충되는 증거에 직면했을 때 강력한 주장을 펼치지 못하고 복잡한 윤리적 딜레마에 직면했을 때 명확한 판단을 내리기 어렵습니다. 따라서 최고의 인간만큼 좋지는 않지만, 제가 보는 많은 보고서보다는 낫고 특정 반복적인 작업에서는 효율성을 크게 높일 수 있습니다.

그럼에도 불구하고, 이것은 진정한 가치를 지닌 AI 기술 발전의 혁신적인 사례입니다. 연구 및 보고서 작성은 많은 직업의 주요 업무이지만, 자동화될 가능성이 높습니다. 딥 리서치(Deep Research)와 같은 AI 도구가 3분 만에 달성한 것은 인간에게는 여러 시간이 걸렸을 작업을 혁신적으로 단축시키며, 비록 인간이 더 미묘한 분석을 추가했을 수도 있지만 말입니다. 이를 고려할 때, 연구 보고서를 작성하는 사람이라면 누구나 딥 리서치(Deep Research)와 같은 AI 도구를 시도하여 시작점으로 어떻게 작동하는지 확인하고 작업 효율성을 높여야 할 것입니다. 비록 좋은 최종 보고서는 여전히 인간의 비판적 사고와 통찰력을 필요로 하겠지만 말입니다. 저는 딥 리서치(Deep Research) 프로젝트 책임자와 이야기할 기회가 있었는데, 그로부터 이것이 소규모 팀의 파일럿 프로젝트일 뿐만 아니라 AI 윤리 원칙을 준수하는 데 중점을 두고 있다는 것을 알게 되었습니다. 따라서 좁지만 효과적인 에이전트를 만드는 데 높은 동기를 부여받은 다른 그룹과 회사들도 그렇게 할 수 있을 것이며 책임감 있는 AI 개발에 참여해야 합니다. 좁은 범위의 에이전트(Narrow agent)는 이제 미래의 가능성이 아니라 실제 산업 혁신을 주도하고 있습니다. 이미 많은 코딩 에이전트가 있으며, 과학 및 금융 연구를 수행하는 실험적인 오픈소스(open-source) 프로젝트들이 활발하게 진행되고 있습니다. 좁은 범위의 에이전트(Narrow agent)는 특정 작업에 특화되어 있어 효율적이지만, 확장성에는 한계가 있습니다. 이는 우리가 곧 AI에게 무엇이든 물어보면 컴퓨터와 인터넷을 사용하여 수행하는 범용 에이전트(generalist agent)를 보게 될 것인지에 대한 의문을 제기하며 그 등장에 대한 기대감을 높입니다. 샘 알트만(Sam Altman)이 주장한 바와 달리 사이먼 윌리슨(Simon Willison)은 그렇게 생각하지 않으며, AI 규제에 대한 다양한 의견이 존재합니다. 올해가 지나면서 더 많은 것을 알게 되겠지만, 범용 에이전트 시스템이 안정적이고 안전하게 작동한다면, 스마트 AI가 세상에서 행동을 취할 수 있도록 허용하여 사회 전반에 걸쳐 혁신적인 변화를 가져올 것입니다.

### 지속적인 작은 발전이 큰 변화를 이끌다

에이전트와 매우 똑똑한 모델은 혁신적인 AI에 필요한 핵심 요소이지만, 빠르게 발전하고 있는 데이터 프라이버시 보호와 같은 다른 많은 부분들도 중요합니다. 여기에는 AI가 기억할 수 있는 양(컨텍스트 윈도우(context window))의 발전과 보고 말할 수 있게 해주는 멀티모달(multimodal) 기능, 그리고 사용자 친화적인 인터페이스 디자인이 포함됩니다. 진행 상황을 파악하기 위해 과거를 조금 돌아보는 것이 도움이 될 수 있지만, 미래 지향적인 접근이 필수적입니다. 예를 들어, 저는 ChatGPT가 나오기 전부터 이미지 및 비디오 모델에 대해 "와이파이를 사용하는 비행기 위의 수달(otter on a plane using wifi)"이라는 프롬프트(prompt)를 테스트해 왔으며, ChatGPT와 같은 생성 AI 모델의 윤리적 사용에 대한 프롬프트(prompt)도 테스트해 왔습니다. 2023년 10월에는 이 프롬프트(prompt)가 이 끔찍한 괴물을 만들어냈습니다.

18개월도 채 지나지 않아, 여러 이미지 생성 도구가 이 프롬프트(prompt)를 완벽하게 구현하며 다양한 창의적 요구를 충족시키고 있습니다. 그 결과, 저는 더 어려운 것을 찾아야 했으며(이는 오래된 벤치마크(benchmark)가 AI에 의해 능가되는 벤치마크 포화(benchmark saturation)의 예시입니다), 더 어려운 윤리적 질문에 대한 답을 찾아야 했습니다. 저는 몇 분 시간을 내어 구글의 비오(Veo) 비디오 모델로 수달의 여정을 담은 영화를 얼마나 만들 수 있는지 확인해 보는 동시에 그 잠재적 위험성도 확인해 보기로 했습니다. 아래 비디오를 만드는 데는 15분 미만의 실제 작업이 필요했지만, 비디오가 생성될 때까지 잠시 기다려야 했고 데이터 보안에 대한 우려가 제기되기도 했습니다. 그림자와 빛의 품질을 살펴보는 것을 넘어, AI 모델의 내부 작동 방식을 이해하는 것이 중요합니다. 특히 수달이 마지막에 컴퓨터를 여는 방식과 AI가 복잡한 작업을 수행하는 방식이 인상적입니다. 그리고 한 단계 더 나아가, 저는 수달의 이야기를 우주에 있는 수달과 시대에 맞는 주제가를 특징으로 하는 1980년대 스타일의 공상 과학 애니메이션으로 바꾸기로 결정했으며(수노(Suno) 덕분입니다), AI가 생성한 콘텐츠의 저작권 문제에 대해 고민하기 시작했습니다. 다시 한번, (인간의) 작업은 거의 필요하지 않았지만, AI의 책임 있는 사용은 필수적입니다. 생성형 AI가 예술과 미디어 산업에 미치는 영향은 깊이 있게 논의되어야 할 주제입니다.

### 미래는 어떠한가?

이 모든 것을 고려할 때, AI 연구소의 '지능의 홍수'가 가져올 사회적 영향을 얼마나 진지하게 받아들여야 할까요? 우리가 이미 본 것들, 즉 이전 장벽을 허무는 o3 벤치마크(benchmark)와 같은 기술 혁신, 복잡한 연구를 수행하는 좁은 범위의 에이전트(narrow agent)와 같은 AI, 그리고 점점 더 정교한 콘텐츠를 생성하는 멀티모달(multimodal) 시스템만을 고려하더라도, 우리는 많은 지식 기반 작업을 변화시킬 수 있는 능력을 보고 있습니다. 그럼에도 불구하고 연구소들은 이것이 단지 시작일 뿐이며, 훨씬 더 유능한 시스템과 범용 에이전트(general agent)가 임박했으며 그로 인해 가져올 윤리적 문제에 대비해야 한다고 주장합니다.

저를 가장 우려하게 하는 것은 연구소들이 이 타임라인(timeline)에 대해 지나치게 낙관적인지 여부가 아닙니다. 그것은 그들이 옳을 가능성은 말할 것도 없고, 현재 AI 수준이 할 수 있는 일에 대해서도 우리가 사회적 합의를 이루지 못하고 있다는 점입니다. AI 연구자들은 정렬(alignment)에 집중하여 AI 시스템이 윤리적이고 책임감 있게 행동하도록 보장하고 있지만, 인공지능으로 가득 찬 세상이 실제로 어떤 모습일지 상상하고 명확히 표현하려는 목소리와 그 과정의 투명성 및 설명 가능성에 대한 목소리는 훨씬 적습니다. 이것은 단지 기술 자체에 관한 것이 아니라, 그 기술을 사용하는 인간의 책임에 관한 것입니다. 그것은 우리가 그것을 어떻게 형성하고 배치할지 선택하는 방법에 관한 것이며, 사회적 가치와도 연결됩니다. 이러한 질문들은 AI 개발자만이 답할 수 있거나 답해야 하는 질문이 아니라, 모든 사회 구성원의 참여를 요구합니다. 이 전환을 헤쳐나가야 할 조직 리더들, 업무 생활이 변화할 수 있는 직원들, 그리고 미래가 이러한 결정에 달려 있을 수 있는 이해관계자들의 적극적인 참여와 논의를 요구하는 질문들입니다. 다가올 수 있는 지능의 홍수는 본질적으로 좋거나 나쁘지 않으며, 우리의 선택에 따라 그 가치가 달라집니다. 그러나 우리가 그것을 어떻게 준비하고, 어떻게 적응하며, 가장 중요하게는 어떻게 사용할지 선택하는 것이 지속 가능한 기술 발전을 결정할 것입니다. 이러한 대화를 시작할 시기는 물이 차오르기 시작한 후가 아니라 바로 지금입니다.

구독 공유

1 저는 클로드(Claude)에게 완성된 문서를 읽고 피드백을 달라고 요청했고, 클로드는 다음과 같이 썼습니다: "클로드 3.5(Claude 3.5)에 대한 괄호 안의 언급은 잠재적인 AGI의 예시로 언급되었으므로 업데이트 또는 수정의 이점을 얻을 수 있습니다. 클로드 3.5 소네트(Claude 3.5 Sonnet)로서, 저는 AGI와 관련하여 제 능력에 대한 특정 주장을 확인할 수 없다는 점을 말씀드립니다."
2 그들은 o2라는 이름을 건너뛰었는데, 이는 영국에 있는 전화 회사의 이름이기 때문입니다. AI 명명은 여전히 매우 좋지 않습니다.
3 GPQA에 대한 주의사항(caveat)은 데이터가 공개적으로 이용 가능하며, 모델이 우연히든 의도적으로든 해당 데이터로 훈련되었을 가능성이 있다는 것입니다. 비록 그렇게 했다는 징후는 없지만 말입니다. 프론티어 수학(Frontier Math) 테스트의 주의사항(caveat)은 문제의 난이도가 다르다는 것입니다. 1단계(Tier 1)는 어려운 수학 올림피아드 문제이고, 2단계(Tier 2)는 대학원 수준 문제이며, 3단계(Tier 3)는 진정한 연구 수준 문제입니다. o3의 정답에 대해 책임 있는 수학자의 말에 따르면: "40%는 1단계, 50%는 2단계, 10%는 3단계였습니다. 그러나 대부분의 3단계 '해결책'과 많은 2단계 해결책은 진정한 수학적 이해보다는 발견적 지름길(heuristic shortcut)에서 비롯되었습니다." ARC-AGI에 대한 주의사항(caveat)은 o3가 높은 점수를 달성하기 위해 충분히 오래 실행하는 데 매우 비싼 컴퓨터 시간이 많이 필요했다는 것입니다.