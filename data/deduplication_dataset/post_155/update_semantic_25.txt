아르빈드 나라야난, 베네딕트 스트뢰블, 사야시 카푸어 공저.

2023년 3월 GPT-4 공개 이후, 인공지능 분야를 지배하던 통념은 모델 크기 확대가 궁극적으로 범용 인공지능(AGI)을 거쳐 초월적 지능(superintelligence)에 도달케 할 것이라는 예측이었습니다. 이러한 과격한 예견은 점차 약해졌으나, 불과 몇 주 전까지만 해도 AI 공동체 내에서는 모델 확장 추세가 당분간 지속될 것이라는 확신이 우세했습니다. 그러나 최근 The Information, 로이터(Reuters), 블룸버그(Bloomberg) 등 주요 언론사에서 연달아 보도된 세 건의 기사는 선도적인 AI 개발 기업 세 곳(OpenAI, Anthropic, Google Gemini) 모두 차세대 모델 개발 과정에서 상당한 난관에 직면했음을 폭로했습니다. 과거 모델 스케일링의 가장 열렬한 주창자였던 일리야 수츠케버(Ilya Sutskever)를 포함한 수많은 업계 인사들은 이제 완전히 다른 견해를 피력하고 있습니다. 수츠케버는 "2010년대는 규모 확장의 시대였지만, 이제 우리는 경이로움과 탐구의 시대로 회귀했습니다. 모두가 다음 단계의 해법을 찾고 있습니다."라고 말하며, "이제는 무엇을 제대로 확장할지가 그 어느 때보다 중요해졌습니다." (로이터)라고 덧붙였습니다.

새롭게 부상하는 지배적인 서사는 더 이상 모델 크기 확장이 주요 발전 동력이 아니며, 대신 '추론 스케일링(inference scaling)' 또는 '테스트 시간 연산 스케일링(test-time compute scaling)'이 AI 역량 증진의 핵심 방안이라는 것입니다. 이 개념은 모델이 작업을 수행할 때, 예를 들어 응답을 생성하기 전에 '사고'하도록 유도함으로써 점점 더 많은 연산 자원을 투입하는 방식을 의미합니다. 이러한 변화는 AI 관찰자들 사이에서 인공지능 발전 속도가 둔화되고 있는지 여부에 대한 혼란을 가중시키고 있습니다. 과거의 예측들이 불과 몇 달 만에 뒤집히는 현상은 기술 발전의 예측 불가능성을 여실히 보여주는 동시에, AI 분야의 담론이 얼마나 유동적인지 시사합니다.

이 에세이에서 우리는 이러한 의문에 대한 실증적 근거들을 검토하고, 네 가지 핵심 주장을 제시합니다.

*   모델 스케일링의 종말을 선언하는 것은 아직 성급한 판단입니다.
*   모델 스케일링의 지속 여부와 무관하게, 이 문제에 대한 업계 지도자들의 시각 변화는 그들의 예측을 맹목적으로 신뢰하는 것이 얼마나 무모한 일인지를 증명합니다. 그들은 우리보다 월등히 많은 정보를 보유하고 있지 않으며, 그들의 주장은 기득권에 의해 크게 영향을 받습니다.
*   추론 스케일링은 실질적인 발전 가능성을 지니고 있으며, 단기적으로 빠른 역량 향상으로 이어질 수 있는 많은 '쉽게 달성할 수 있는 목표(low-hanging fruit)'가 존재합니다. 그러나 일반적으로 추론 스케일링을 통한 역량 증진은 예측이 어렵고, 특정 도메인(domain)에 따라 편차가 클 가능성이 높습니다.
*   AI 역량 향상과 그것이 사회적 또는 경제적 파급 효과로 이어지는 연관성은 극히 미미합니다. 실제 영향력의 병목 현상(bottleneck)은 AI의 성능 자체가 아니라, 제품 개발 속도와 시장 채택률에 있습니다.

### 모델 스케일링은 끝났을까요?

모델 스케일링은 정말 종말을 맞이했을까요? 이러한 급격한 분위기 전환을 야기할 만한 새로운 정보는 거의 없습니다. 우리는 이 뉴스레터를 통해 모델 스케일링에 주요한 제약 요인(headwinds)들이 존재한다고 오랫동안 역설해 왔습니다. 당시 스케일링에 대한 과도한 낙관론을 경고했던 것처럼, 이제 우리는 모델 스케일링의 미래에 대한 지나친 비관론에 대해 경계해야 합니다.

GPT-4 수준의 모델에서 '평소와 다름없는 규모 확장(business-as-usual scaling)'은 한계에 도달했습니다. 이는 해당 모델들이 접근 가능한 대부분의 데이터 자원(data sources)으로 이미 훈련되었기 때문입니다. 우리는 모델 스케일링을 지속하기 위해서는 혁신적인 아이디어가 필요하다는 점을 이미 인지하고 있었습니다. 따라서 그러한 새로운 접근 방식들이 시도되었음에도 실패했다는 명확한 증거가 없는 한, 모델 스케일링에 더 이상의 발전 여지가 없다고 단정할 수는 없습니다. 일례로, 멀티모달 모델(multimodal models) 훈련에 유튜브(YouTube) 영상(단순 텍스트 전사가 아닌 실제 영상 콘텐츠)을 포함하는 것이 새로운 차원의 역량을 개척할 수도 있습니다. 물론, 효과가 없을 수도 있습니다. 누군가가 실제로 시도해보기 전까지는 알 수 없으며, 실제로 그러한 시도가 있었는지 여부도 불분명합니다. 이는 아마도 구글(Google)이 수행해야 할 과제일 것입니다. 구글은 경쟁사에게 유튜브 훈련 데이터(training data)를 라이선스할 가능성이 거의 없기 때문입니다. 또한, 데이터 희소성 문제를 해결하기 위한 합성 데이터(synthetic data) 생성 기술의 발전이나, 보다 효율적인 데이터 활용 알고리즘의 등장은 모델 스케일링의 새로운 동력을 제공할 수도 있습니다. 현재의 데이터 제약을 넘어서는 방법론들이 끊임없이 연구되고 있음을 간과해서는 안 됩니다.

### 1 모델 스케일링에 대한 불확실성이 여전하다면, 왜 서사가 뒤집혔을까요?

음, GPT-4가 훈련을 완료한 지 2년이 넘었기 때문에, 차세대 모델 출시가 예상보다 다소 지연될 뿐이라는 주장은 점차 설득력을 잃어가고 있었습니다. 그리고 한 기업이 난관을 인정하기 시작하면, 다른 기업들도 동일한 상황을 고백하기가 훨씬 수월해집니다. 댐에 작은 누수가 생기면, 곧 전체가 무너져 내리는 것과 같습니다. 마지막으로, OpenAI의 추론 모델(reasoning model)인 o1의 등장은 기업들이 모델 스케일링의 문제점을 인정하면서도 명예를 지킬 수 있는 탈출구를 제공했습니다. 추론 스케일링으로의 전환을 주장함으로써, 그들은 전략적 유연성을 확보하고 대중의 시선을 돌릴 수 있었기 때문입니다.

분명히 말하자면, 다수의 AI 연구소들이 더 큰 규모의 훈련을 수행했지만, 그 결과로 나온 모델들을 시장에 출시하지 않았다는 보고서를 의심할 이유는 없습니다. 그러나 이로부터 어떤 결론을 도출해야 할지는 명확하지 않습니다. 더 큰 모델이 공개되지 않은 몇 가지 가능한 원인은 다음과 같습니다.

*   수렴 실패(convergence failures) 또는 여러 데이터센터에 걸친 훈련 실행(multi-datacenter training runs)에서 내결함성(fault tolerance)을 확보하는 데 따르는 복잡성 등 기술적인 난관.
*   개발된 모델이 GPT-4급 모델에 비해 현저한 개선을 보이지 않아, 출시하기에는 기대에 미치지 못하는 성능이었을 가능성.
*   모델 성능이 GPT-4 수준에서 크게 벗어나지 않아, 개발자들이 미세 조정(fine tuning)을 통해 더 나은 결과를 이끌어내기 위해 상당한 시간을 할애했을 가능성.

요약하자면, 모델 스케일링이 실제로 한계에 도달했을 수도 있지만, 이러한 문제들이 일시적인 것이며 결국 어떤 기업은 기술적 난관을 극복하거나 새로운 데이터 자원을 발견하는 등의 방법으로 이를 해결할 방안을 찾아낼 수도 있습니다. 이러한 상황은 마치 고도의 기술 개발 과정에서 흔히 겪는 일시적 정체기와 유사하며, 혁신적인 해결책이 언제든 등장할 수 있음을 시사합니다.

### 내부자들에게 의존하는 것을 멈춥시다

새로운 서사가 그렇게 급진적으로 출현했다는 점도 의아하지만, 모델 스케일링의 잠재적 한계가 명백했음에도 불구하고 이전의 서사가 그토록 오랫동안 지속되었다는 사실 또한 흥미롭습니다. 그것이 유지될 수 있었던 주된 이유는 업계 리더들이 스케일링이 앞으로 몇 년간 지속될 것이라고 확언했기 때문입니다. 2 일반적으로 언론인(및 대다수의 대중)은 외부 전문가보다 업계 내부자의 의견에 더 큰 비중을 두는 경향이 있습니다. 하지만 이러한 의존이 과연 정당한가요?

업계 리더들은 AI 발전 방향을 예측하는 데 있어 그다지 뛰어난 실적을 보여주지 못했습니다. 좋은 예는 지난 10년 대부분 동안 자율주행차에 대한 과도한 낙관론입니다. (자율주행 기술은 마침내 현실화되었지만, 레벨 5, 즉 완전 자동화는 아직 구현되지 않았습니다.) 덧붙여 말하자면, 내부자 예측의 신뢰도를 보다 심층적으로 이해하기 위해 지난 10년간 저명한 업계 내부자들이 AI에 대해 내놓았던 모든 예측들을 체계적으로 분석하는 연구가 매우 유익할 것입니다. 이러한 분석은 단순한 기술적 예측을 넘어, 시장의 기대치와 실제 구현 가능성 사이의 간극을 밝혀줄 것입니다.

내부자들의 주장에 더 큰 비중을 두어야 할 몇 가지 논거가 있을 수 있지만, 그들의 견해에 덜 비중을 두어야 할 훨씬 더 중요한 이유들도 존재합니다. 이들을 하나씩 면밀히 분석해 봅시다.

업계 내부자들이 아직 공개되지 않은 모델의 성능과 같은 독점적인 정보(proprietary information)를 보유하고 있어 미래에 대한 그들의 주장을 더 정확하게 만들 수 있다는 것은 사실입니다. 그러나 모델 가중치(model weights)를 공개하고 과학적 통찰력, 데이터셋(datasets) 및 기타 아티팩트(artifacts)를 공유하는 일부 기업을 포함하여 많은 AI 회사들이 최첨단 기술에 근접해 있다는 점을 고려할 때, 우리가 이야기하는 우위는 기껏해야 몇 달 정도에 불과합니다. 이는 예를 들어 3년 후를 예측하는 맥락에서는 사소한 차이에 불과합니다. 게다가 우리는 기업이 내부적으로 얼마나 많은 추가 정보(역량 측면이든, 특히 안전 측면이든)를 가지고 있는지 과대평가하는 경향이 있습니다. 내부자들은 오랫동안 '우리가 아는 것을 당신이 알기만 한다면...'이라고 경고했지만, 내부 고발자들이 마침내 진실을 폭로했을 때, 그들은 대부분 다른 모든 사람들이 하는 것과 유사한 종류의 추측에 의존하고 있었음이 드러났습니다.

3 내부자들에게 더 큰 비중을 두어야 할 또 다른 잠재적 이유는 그들의 기술적 전문성입니다. 그러나 우리는 이것이 강력한 논거라고 생각하지 않습니다. 학계 역시 업계만큼이나 풍부한 AI 전문 지식을 보유하고 있습니다. 더 중요하게는, AI 예측에 자주 동원되는 거친 추세 외삽(trend extrapolation)을 뒷받침하는 데 깊은 기술적 전문성은 그리 결정적이지 않습니다. 또한, 기술적 전문성만으로는 충분하지 않습니다. 비즈니스 및 사회적 요인들이 AI의 진로를 결정하는 데 적어도 그만큼 중요한 역할을 합니다. 자율주행차의 경우, 그러한 요인 중 하나는 사회가 공공 도로를 실험 장소로 사용하는 것을 얼마나 용인하는가입니다. 대규모 AI 모델의 경우, 우리는 이전에 가장 핵심적인 요인은 스케일링이 기술적으로 실현 가능한지 여부가 아니라, 비즈니스적으로 타당한지 여부라고 주장했습니다. 따라서 기술 전문가들은 특별한 이점을 가지고 있지 않을 뿐만 아니라, 기술적 측면을 과도하게 강조하는 경향이 자칫 과신에 찬 예측으로 이어지는 경향이 있습니다. 요컨대, 내부자들의 견해에 더 큰 비중을 두어야 할 이유는 그리 설득력이 없습니다.

반면에, 그들의 견해에 훨씬 덜 비중을 두어야 할 크고 명백한 이유가 있는데, 그것은 그들이 상업적 이익에 부합하는 발언을 할 유인이 있으며, 실제로 그렇게 해왔다는 실적이 있기 때문입니다. 예를 들어, 수츠케버는 OpenAI에 재직할 당시 회사가 자금을 유치해야 했으므로 스케일링을 적극적으로 옹호할 동기가 있었습니다. 그러나 이제 그가 스타트업인 Safe Superintelligence를 이끌고 있으므로, 훨씬 적은 자본에 접근할 수 있음에도 불구하고 OpenAI, Anthropic, Google 등과 경쟁할 수 있음을 투자자들에게 설득해야 합니다. 아마도 이것이 그가 이제 사전 훈련(pre-training)을 위한 데이터가 부족하다고 말하는 이유일 것입니다. 마치 그것이 어떤 새로운 깨달음인 양, 사실은 오랫동안 끊임없이 반복되어 온 주장이었음에도 말입니다.

다시 한번 강조하지만, 모델 스케일링이 실제로 끝났는지 아닌지는 여전히 불확실합니다. 그러나 업계의 갑작스러운 태도 변화는 너무나도 뻔뻔하여, 내부자들이 어떤 수정 구슬도 가지고 있지 않으며 다른 모든 사람들과 마찬가지로 추측에 의존하고 있을 뿐만 아니라, 자신들의 '거품(bubble)' 속에 갇혀 스스로가 세상에 팔고 있는 과대광고를 쉽게 받아들임으로써 더욱 편향되어 있다는 점을 의심할 여지 없이 보여줍니다. 이러한 현실을 고려할 때, 우리의 제안은 모든 사람, 특히 언론인, 정책 입안자, 그리고 AI 커뮤니티에게 기술의 미래, 특히 사회적 파급 효과에 대해 예측할 때 내부자들의 견해에 대한 의존을 종식시키라는 것입니다. 이는 미국 사회에 만연한 무의식적인 편견, 즉 "극단적인 부와 그에 따르는 권력을 미덕과 지능과 동일시하는 듯한 뚜렷한 미국적 질병"의 형태로 존재하기 때문에 상당한 노력을 필요로 할 것입니다. (브라이언 가드너(Bryan Gardiner)의 마리엣제 샤케(Marietje Schake)의 'The Tech Coup' 서평에서 인용) 이러한 편향된 정보원으로부터 벗어나야만 AI 발전의 진정한 의미와 방향을 객관적으로 평가할 수 있을 것입니다.

AI Snake Oil은 AI 과대광고를 폭로하고 새로운 개발에 대한 증거 기반 분석을 게시합니다. 구독하기

### 추론 스케일링을 통해 역량 발전이 계속될까요?

물론, 모델 스케일링만이 AI 역량을 향상시키는 유일한 방법은 아닙니다. 추론 스케일링은 최근 상당한 진전이 있는 분야입니다. 예를 들어, OpenAI의 o1과 오픈 가중치(open-weights) 경쟁 모델인 DeepSeek R1은 '사고 모델(thinking models)' 또는 '추론 모델(reasoning models)'로 분류됩니다. 이들은 답변을 생성하기 전에 '추론' 과정을 거치도록 미세 조정되었습니다. 이 외에도 모델 자체를 변경하지 않으면서 여러 가지 해결책을 생성하고 그 품질에 따라 순위를 매기는 등 다양한 기법들이 활용되고 있습니다. 이러한 접근 방식은 모델이 단순히 지식을 회상하는 것을 넘어, 문제 해결을 위한 내부적 탐색 과정을 시뮬레이션하도록 돕습니다.

추론 스케일링이 얼마나 중요한 추세로 자리매김할지를 결정할 두 가지 주요 미해결 질문이 있습니다.

*   어떤 종류의 문제에 효과적으로 적용될 수 있는가?
*   효과적인 문제 영역에서, 추론 과정에 더 많은 연산을 투입함으로써 어느 정도의 성능 향상이 가능한가?

언어 모델의 토큰당 출력 비용은 하드웨어 및 알고리즘 개선 덕분에 급격히 감소하고 있으므로, 만약 추론 스케일링이 여러 자릿수(orders of magnitude)에 걸친 개선을 가져온다면(예를 들어, 특정 작업에서 백만 개의 토큰을 생성하는 것이 십만 개의 토큰을 생성하는 것보다 훨씬 더 뛰어난 성능을 제공한다면), 이는 막대한 의미를 지닐 것입니다. 이는 단순히 '더 많이 생각하는 것'을 넘어, 질적으로 다른 수준의 문제 해결 능력을 가능하게 할 수 있습니다.

4 첫 번째 질문에 대한 직관적이고 간결한 대답은 추론 스케일링이 코딩이나 수학 문제 해결과 같이 명확한 정답이 존재하는 문제에 특히 유용하다는 것입니다. 이러한 작업에서는 두 가지 관련 특징 중 적어도 하나가 사실인 경향이 있습니다. 첫째, 기호 추론(symbolic reasoning)은 정확도를 크게 향상시킬 수 있습니다. 이는 대규모 언어 모델(LLM)이 통계적 특성으로 인해 본질적으로 취약한 부분이지만, 사람이 펜과 종이를 사용하여 수학 문제를 풀듯이 추론을 위해 출력 토큰(output tokens)을 적극적으로 활용함으로써 이러한 한계를 극복할 수 있습니다. 둘째, 정답을 생성하는 것보다 검증하는 것이 훨씬 용이합니다(때로는 코딩을 위한 단위 테스트(unit tests)나 수학 정리 증명을 위한 증명 검사기(proof checkers)와 같은 외부 검증 도구의 도움을 받기도 합니다).

대조적으로, 글쓰기나 언어 번역과 같은 작업에서는 추론 스케일링이 큰 변화를 가져올 것이라고 보기는 어렵습니다. 특히 모델의 한계가 훈련 데이터의 부족에서 기인하는 경우라면 더욱 그렇습니다. 예를 들어, 모델이 저자원 언어(low-resource language)의 특정 관용구를 알지 못해 번역을 제대로 수행하지 못한다면, 단순히 더 많은 추론 연산을 투입한다고 해서 이 문제가 해결될 수는 없습니다. 이는 모델이 '무엇을 생각해야 할지'에 대한 근본적인 지식 자체가 부족하기 때문입니다.

지금까지 우리가 확보한 초기 증거는 단편적이지만, 이러한 직관적 판단과 일치합니다. OpenAI o1에 초점을 맞추면, 코딩, 수학, 사이버 보안(cybersecurity), 가상 환경에서의 계획(planning in toy worlds), 그리고 다양한 시험에서 GPT-4o와 같은 최첨단 언어 모델(state-of-the-art language models)에 비해 향상된 성능을 보여줍니다. 시험 성능의 개선은 지식의 양이나 창의성보다는 질문에 답하는 데 필요한 추론의 중요성과 밀접하게 관련되어 있는 것으로 보입니다. 수학, 물리학, LSAT에서는 상당한 개선이 있었고, 생물학 및 계량경제학(econometrics)과 같은 과목에서는 미미한 개선이 있었으며, 영어에서는 무시할 만한 수준의 개선에 그쳤습니다. o1이 개선을 가져오지 않는 것으로 보이는 작업에는 글쓰기, 특정 유형의 사이버 보안 작업(아래에서 설명), 유해성 회피(avoiding toxicity), 그리고 인간이 너무 많이 생각할수록 오히려 더 못하게 되는 것으로 알려진 흥미로운 작업들이 포함됩니다. 이는 추론 스케일링이 모든 문제에 대한 만능 해결책이 아님을 시사합니다.

우리는 추론 모델이 기존 언어 모델과 어떻게 비교되는지에 대한 가용한 증거를 정리한 웹페이지를 구축했습니다. 당분간은 업데이트를 계속할 예정이지만, 곧 쏟아져 나올 연구 결과들을 실시간으로 따라잡기 어려워질 것으로 예상합니다.

이제 두 번째 질문을 고려해 봅시다. 무한한 추론 연산 예산(inference compute budget)이 있다고 가정할 때, 추론 스케일링을 통해 얼마나 큰 개선을 얻을 수 있을까요? OpenAI가 o1의 역량을 과시하기 위해 내세운 대표적인 예시는 수학 벤치마크(benchmark)인 AIME였습니다. 그들의 그래프는 이 질문을 모호하게 열어두었습니다. 성능이 포화 상태에 도달하려는 것일까요, 아니면 100%에 가깝게 밀어붙일 수 있을까요? 또한 그래프에 x축 레이블(labels)이 의도적으로 생략되어 있다는 점도 주목하십시오. 이는 데이터 해석에 대한 중요한 정보를 누락시킨 것으로 볼 수 있습니다.

Source: OpenAI

외부 연구자들이 이 그래프를 재구성하려는 시도는 (1) x축의 절단점(cutoff)이 대략 2,000 토큰(tokens)이었을 가능성이 높고, (2) o1에게 이보다 더 오래 '생각'하도록 요청해도 실제로 그렇게 하지 않는다는 것을 보여줍니다. 따라서 이 질문은 여전히 명확한 답을 얻지 못했으며, 더 투명하고 신뢰할 수 있는 정보를 얻기 위해서는 오픈 소스 모델(open-source models)을 활용한 추가 실험을 기다려야 합니다. o1의 기술을 공개적으로 재현하려는 활발한 노력이 있다는 점은 매우 고무적입니다. 이는 연구 커뮤니티의 개방성과 협력을 통해 이러한 질문에 대한 답을 찾아나갈 수 있음을 의미합니다.

최근 '추론 스케일링 fLaws(Inference Scaling fLaws)'라는 논문(제목은 추론 스케일링 법칙(inference scaling laws)에 대한 언어유희)에서 우리는 추론 스케일링에 대한 다른 접근 방식을 면밀히 살펴보았습니다. 즉, 외부 검증 도구(external verifier)에 의해 올바르다고 판단될 때까지 해결책을 반복적으로 생성하는 방식입니다. 이 접근 방식이 여러 자릿수(orders of magnitude)만큼 스케일링의 유용성을 증대시킬 수 있다는 희망(우리의 과거 연구 포함)과 관련되어 있었지만, 우리는 이것이 검증 도구의 품질에 극도로 민감하다는 것을 발견했습니다. 검증 도구가 아주 미세하게라도 불완전하다면, 많은 현실적인 코딩 작업 환경에서 성능은 약 10번의 시도 후에 최고점에 도달한 다음 실제로는 감소하기 시작합니다. 이는 비현실적인 완벽한 검증 도구가 아니면 이 방법의 효용성이 제한적임을 보여줍니다. 일반적으로 말해서, 추론 스케일링 '법칙'에 대한 증거는 아직 설득력이 부족하며, 추론 시점에 (예를 들어) 수백만 개의 토큰을 생성하는 것이 실제로 도움이 될 실제 문제가 있는지 여부는 아직 더 지켜봐야 할 문제입니다.

### 추론 스케일링이 다음 개척지일까요?

추론 스케일링 분야에는 많은 '쉽게 달성할 수 있는 목표(low-hanging fruit)'가 존재하며, 단기적인 발전은 매우 빠를 가능성이 높습니다. 특히, 추론 모델의 현재 한계 중 하나는 에이전트 시스템(agentic systems)에서 제대로 작동하지 않는다는 것입니다. 우리는 에이전트에게 연구 논문과 함께 제공된 코드를 재현하도록 요청하는 자체 벤치마크(benchmark)인 CORE-Bench에서 이를 명확히 관찰했습니다. 가장 성능이 우수한 에이전트는 Claude 3.5 Sonnet으로 38%의 정확도를 기록했지만, o1-mini로는 24%에 불과했습니다. 5 이는 추론 모델이 한 사이버 보안 평가에서는 개선을 가져왔지만 다른 평가에서는 그렇지 않은 이유를 설명합니다. 후자의 평가에는 에이전트의 역할이 포함되었기 때문입니다.

우리는 에이전트가 추론 모델로부터 이점을 얻지 못하는 두 가지 주요 원인이 있다고 생각합니다. 첫째, 이러한 모델은 일반 모델과는 다른 프롬프트 스타일(prompting styles)을 요구하며, 현재의 에이전트 시스템은 일반 모델에 프롬프트를 제공하는 데 최적화되어 있습니다. 둘째, 우리가 아는 한, 추론 모델은 지금까지 코드 실행, 셸 상호작용(shell interaction) 또는 웹 검색(web search)과 같은 환경으로부터 피드백(feedback)을 받는 설정에서 강화 학습(reinforcement learning)을 사용하여 훈련되지 않았습니다. 다시 말해, 그들의 도구 사용 능력은 추론을 학습하기 전의 기본 모델보다 나을 것이 없습니다. 6 이러한 문제들은 비교적 해결하기 쉬운 것처럼 보입니다. 이러한 난관들을 극복하면 중요한 새로운 AI 에이전트(AI agent) 역량이 실현될 수 있습니다. 예를 들어, 프롬프트(prompt)만으로 복잡하고 완전히 기능하는 애플리케이션을 생성하는 것과 같은 혁신이 가능해질 수 있습니다. (물론, 이미 이를 시도하는 도구들이 있지만, 아직까지는 잘 작동하지 않습니다.)

하지만 장기적인 관점에서는 어떨까요? 추론 스케일링이 지난 7년간 모델 스케일링에서 목격했던 것과 같은 종류의 폭발적인 발전을 가져올 수 있을까요? 모델 스케일링은 데이터, 모델 크기, 연산량(computation)을 '단순히' 늘리기만 하면 되었기 때문에 매우 매력적이었습니다. 알고리즘적 돌파구(algorithmic breakthroughs)가 필수적이지 않았습니다. 추론 스케일링의 경우(현재까지는) 그렇지 않습니다. 추론 스케일링 기술은 매우 다양하며, 무엇이 작동하고 작동하지 않는지는 해결하려는 문제에 따라 다르며, 심지어 총체적으로도 제한된 도메인(domains)에서만 효과를 발휘합니다. AI 개발자들은 이러한 한계를 극복하기 위해 끊임없이 노력하고 있습니다. 예를 들어, OpenAI의 강화 미세 조정(reinforcement finetuning) 서비스는 회사가 미래 모델을 미세 조정하기 위해 다양한 도메인에서 고객 데이터를 수집하는 전략적 방법으로 여겨집니다.

약 10년 전, 강화 학습(reinforcement learning, RL)은 아타리(Atari)와 같은 많은 게임에서 획기적인 성과를 거두었습니다. 당시 엄청난 과대광고가 있었고, 많은 AI 연구자들은 강화 학습을 통해 인공 일반 지능(AGI)에 도달할 수 있을 것이라는 기대를 품었습니다. 실제로, 강화 학습에 대한 높은 기대는 명시적으로 AGI에 초점을 맞춘 연구소, 특히 OpenAI의 설립으로 이어지기도 했습니다. 그러나 그러한 기술들은 게임과 같은 좁은 도메인을 넘어 일반화되지 못했습니다. 이제 다시 강화 학습에 대한 비슷한 과대광고가 확산되고 있습니다. 분명히 매우 강력한 기술이지만, 지금까지 우리는 이전 과대광고의 거품을 꺼뜨렸던 것과 유사한 한계들을 다시 마주하고 있습니다.

AI 역량 발전이 둔화될지 여부를 예측하는 것은 불가능합니다. 사실, 예측은 차치하고라도, 합리적인 사람들은 AI 발전이 이미 둔화되었는지 여부에 대해 매우 다른 의견을 가질 수 있습니다. 증거를 해석하는 방식이 매우 다양하기 때문입니다. 이는 '역량'이라는 개념 자체가 측정 방식에 매우 민감한 구성 요소이기 때문입니다. 우리가 더 확신을 가지고 말할 수 있는 것은 역량 발전의 본질이 모델 스케일링과 추론 스케일링에서 근본적으로 다를 것이라는 점입니다. 지난 몇 년 동안, 새로운 모델들은 매년 예측 가능하게 광범위한 도메인에 걸쳐 역량 개선을 가져왔습니다. 대형 연구소 외부의 많은 AI 연구자들 사이에서는 다음 최첨단 LLM(Large Language Model)이 출시될 때까지 기다리는 것 외에는 할 일이 거의 없다는 비관적인 분위기가 팽배했습니다. 하지만 추론 스케일링을 활용하면 역량 개선은 하드웨어 인프라(hardware infrastructure) 투자보다는 알고리즘 발전(algorithmic advances)에 의해 더 많이 좌우되어, 불균등하고 예측하기 어려울 가능성이 높습니다. 오래된 계획 문헌(planning literature)에서 나온 아이디어와 같이 LLM의 지배 기간 동안 잠시 잊혔던 많은 아이디어들이 이제 다시 활발하게 논의되고 있으며, 이 분야는 지난 몇 년보다 지적으로 훨씬 더 활기찬 것처럼 보입니다. 이는 다양한 접근 방식과 아이디어가 경쟁하며 발전하는 새로운 시대의 서막을 알립니다.

### 제품 개발은 역량 증가에 뒤처집니다

역량 둔화가 있는지에 대한 격렬한 논쟁은 아이러니합니다. AI 역량 증가와 그것이 실제 세계에서 유용하게 활용되는 것 사이의 연관성이 극히 미약하기 때문입니다. AI 기반 애플리케이션(applications)의 개발은 AI 역량 증가 속도에 훨씬 뒤처져 있으며, 따라서 기존 AI 역량조차도 제대로 활용되지 못하고 있습니다. 한 가지 이유는 '역량-신뢰성 격차(capability-reliability gap)'입니다. 특정 역량이 존재하더라도, 인간의 개입 없이 실제로 작업을 자동화할 수 있을 만큼 충분히 안정적으로 작동하지 않을 수 있습니다(80%만 작동하는 음식 배달 앱을 상상해 보십시오). 그리고 신뢰성을 향상시키는 방법은 종종 애플리케이션(application)에 따라 고유하며 역량을 향상시키는 방법과는 별개입니다. 그렇긴 하지만, 추론 모델은 신뢰성 개선도 보이는 것으로 보이며, 이는 고무적인 현상입니다.

현재의 AI 역량조차도 완전히 활용하는 제품을 만드는 데 10년 이상이 걸릴 수 있는 이유를 설명하는 몇 가지 비유가 있습니다. 인터넷과 웹을 뒷받침하는 기술은 90년대 중반에 대부분 확립되었습니다. 그러나 웹 애플리케이션(web apps)의 잠재력을 완전히 실현하는 데는 10년에서 20년이 더 소요되었습니다. 또는 대규모 언어 모델(large language models)을 위한 GUI(Graphical User Interface)를 구축해야 한다고 주장하는 이 통찰력 있는 에세이를 생각해 보십시오. 이는 텍스트 기반 인터페이스보다 훨씬 더 높은 대역폭(bandwidth)으로 모델과 상호작용할 수 있게 할 것입니다. 이러한 관점에서, AI 기반 제품의 현재 상태는 GUI가 도입되기 전의 개인용 컴퓨터(PC) 시대와 유사하다고 볼 수 있습니다. 즉, 기본적인 기능은 있지만 사용자 경험과 접근성 측면에서 아직 갈 길이 멀다는 의미입니다.

제품 개발의 지연은 AI 회사들이 제품 측면에 충분한 주의를 기울이지 않았다는 사실로 인해 더욱 심화됩니다. 그들은 AI의 범용적인 특성이 소프트웨어 공학(software engineering)의 어려운 문제들로부터 자신들을 면제해 줄 것이라고 오해했기 때문입니다. 다행히도, 이러한 인식은 최근 변화하기 시작했습니다. 이제 제품 중심의 접근 방식으로 전환하면서, AI 회사들과 사용자들은 소프트웨어 개발, 특히 사용자 경험(user experience) 측면이 어렵고 AI 모델 개발보다 훨씬 더 광범위한 기술과 전문성을 요구한다는 것을 재발견하고 있습니다. 좋은 예시는 ChatGPT에서 파이썬(Python) 코드를 실행하는 두 가지 다른 방법(파워 유저(power users)에게는 가장 중요한 기능 중 하나)이 있다는 사실과, 각 방법의 기능과 한계에 대해 기억해야 할 복잡하고 제대로 문서화되지 않은 규칙들이 존재한다는 것입니다. 사이먼 윌리슨(Simon Willison)은 다음과 같이 말합니다. "이 모든 것이 절망적으로 혼란스럽다고 생각하시나요? 저는 당신을 탓하지 않습니다. 저는 20년 이상의 경력을 가진 전문 웹 개발자이자 파이썬 엔지니어이며, 위의 규칙들을 겨우 이해하고 내면화할 수 있습니다." 그럼에도 불구하고, 이는 불과 일주일 전과 비교하면 큰 발전입니다. 당시 이 모델들은 강력한 코딩 역량을 가지고 있었지만 인터넷을 사용할 수 있는 코드를 실행하는 기능은 없었습니다! 그리고 지금도 o1은 인터넷에 접속하거나 코드를 실행할 수 없습니다.

AI의 영향이라는 관점에서 볼 때, 이 시점에서 역량 개선보다 훨씬 더 중요한 것은 사람들이 기존 역량으로 유용한 일을 할 수 있도록 돕는 제품을 실제로 구축하는 것입니다. 마지막으로, 제품 개발이 역량에 뒤처지는 동안, AI 기반 제품의 채택은 다양한 행동적, 조직적, 사회적 이유로 인해 제품 개발보다 훨씬 더 뒤처집니다. AI의 영향(긍정적이든 부정적이든)에 관심 있는 사람들은 현재 또는 예측된 역량 자체보다는 이러한 하위 단계(downstream aspects)에 훨씬 더 많은 주의를 기울여야 합니다. 즉, 기술의 잠재력을 현실 세계의 가치로 전환시키는 과정에 집중해야 한다는 의미입니다.

AI Snake Oil은 AI 과대광고를 폭로하고 새로운 개발에 대한 증거 기반 분석을 게시합니다. 구독하기

### 결론

모델 스케일링이 끝났을 수도 있고, 아닐 수도 있습니다. 그러나 영원히 지속될 수는 없을 것이며, 모델 스케일링의 종말은 오히려 여러 긍정적인 측면을 가져옵니다. AI 발전이 다시 단순한 연산량(compute) 투입뿐만 아니라 새로운 아이디어와 혁신에 의존하게 됩니다. 대기업, 스타트업(startups), 학계 연구자들이 모두 비교적 공평한 경쟁의 장에서 지적 역량을 겨룰 수 있습니다. 임의의 훈련 연산량 임계값(training compute thresholds)에 기반한 규제는 정당화하기가 더욱 어려워집니다. 그리고 모델 자체가 최종 제품이 아니라 단지 기술적 도구일 뿐이라는 명확한 인식이 확산됩니다.

AI의 미래에 관해서는, 기술 내부자들이 우리와 마찬가지로 그것을 헤아리려 노력하고 있다는 사실이 분명하며, 그들의 과신하고, 자기 이익적이며, 변덕스럽고, 편리하게 모호한 예측을 더 이상 신뢰하지 않을 때가 지났습니다. 그리고 기술적 예측을 넘어 AI가 세상에 미치는 영향에 대한 주장으로 나아갈 때, 업계 리더들을 신뢰할 이유는 더욱 줄어듭니다. 그들은 종종 자신들의 기술이 가져올 파급 효과에 대한 깊이 있는 사회적, 윤리적 이해 없이 기술적 우월성에만 초점을 맞추는 경향이 있기 때문입니다. 이러한 태도는 미국 사회에 만연한 무의식적인 편견, 즉 “극단적인 부와 그에 따르는 권력을 미덕과 지능과 동일시하는 듯한 뚜렷한 미국적 질병”의 형태로 존재하기 때문에, 이러한 의존을 끝내는 데는 상당한 노력이 필요할 것입니다. (브라이언 가드너(Bryan Gardiner)가 마리엣제 샤케(Marietje Schake)의 'The Tech Coup' 서평에서 인용한 내용) 진정으로 균형 잡힌 AI 미래를 논하기 위해서는 다양한 분야의 목소리에 귀 기울여야 합니다.

**감사의 말씀.** 초안에 대한 귀한 피드백을 주신 재커리 S. 시겔(Zachary S. Siegel)께 진심으로 감사드립니다.

---

1 OpenAI가 과거에 유튜브 데이터를 크롤링(crawled)한 것으로 알려져 있지만, 그것은 유튜브 전체의 극히 일부에 불과했습니다. 구글이 인지하지 못하게 유튜브 전체를 크롤링하는 것은 현실적으로 불가능할 것입니다.
2 Epoch AI의 뛰어난 분석에 따르면 스케일링은 2030년까지 지속될 수 있다고 합니다. 그러나 이 분석은 너무 최근(2024년 8월)에 발표되어 스케일링 서사의 근거가 될 수는 없었습니다.
3 우리는 AI 모델 및 시스템의 안전에 대한 실질적인 지식을 언급하고 있습니다. 내부 고발자들은 OpenAI의 안전 관련 프로세스에 대한 새로운 정보들을 제시했습니다.
4 그렇긴 하지만, 미래의 비용 감소를 당연시할 수는 없습니다. 우리는 또한 양자화(quantization)와 같은 추론 비용 절감 기술의 근본적인 한계에 직면하고 있습니다.
5 우리는 모든 모델에 대해 4달러의 비용 제한을 설정했습니다. 소규모 샘플에서 10달러의 비용 제한을 두었을 때, o1-preview는 매우 저조한 성능(정확도 10%)을 보였습니다. 비용 제약으로 인해 전체 데이터에 대해 더 높은 비용 제한으로 모델을 평가하지 않았습니다.
6 o1은 ChatGPT 인터페이스에서 추론 중에 도구에 접근할 수도 없습니다! Gemini Flash 2.0은 가능하지만, 이것이 도구 사용을 위해 미세 조정된 것은 물론, 추론을 위해 미세 조정된 모델인지도 불분명합니다.