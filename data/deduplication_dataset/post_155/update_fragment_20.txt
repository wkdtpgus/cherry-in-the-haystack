아르빈드 나라야난, 베네딕트 스트뢰블, 사야시 카푸어 공저.

2023년 3월 GPT-4 출시 이후, 기술 업계의 지배적인 서사는 인공지능(AI)의 급속한 발전에 대한 기대감으로 가득했습니다. 한때 인공 일반 지능(artificial general intelligence)과 초지능(superintelligence)에 대한 장밋빛 예측이 난무했지만, 최근에는 AI의 발전 경로에 대한 신중한 시각이 대두되고 있습니다. 한 달 전까지만 해도 모델 스케일링(scaling)이 AI 역량 확장의 주요 동력이라는 믿음이 지배적이었지만, 최근 The Information, 로이터(Reuters), 블룸버그(Bloomberg) 등 주요 언론 보도에 따르면 선도적인 AI 개발사들(OpenAI, Anthropic, Google Gemini)이 차세대 모델 개발에서 예상치 못한 문제에 직면했음이 밝혀졌습니다. 이로 인해 일리야 수츠케버와 같은 많은 업계 관계자들은 이제 AI의 미래에 대해 매우 다른 이야기를 제시하고 있습니다. 수츠케버는 “과거는 스케일링의 시대였지만, 이제 우리는 다시 탐구와 발견의 시대로 돌아왔습니다. 모두가 다음 혁신을 찾고 있습니다.”라고 말했습니다. 그는 또한 “이제는 올바른 방향으로 스케일링하는 것이 그 어느 때보다 중요합니다.” (로이터)라고 덧붙였습니다.

새로운 지배적인 서사는 모델 스케일링이 한계에 도달했으며, '추론 스케일링(inference scaling)'이 새로운 AI 역량 향상의 핵심 동력으로 부상하고 있다는 것입니다. 이 접근 방식은 모델이 응답을 생성하기 전에 '생각'하는 과정을 통해 점점 더 많은 연산(computation)을 사용하여 작업을 수행하도록 하는 것을 포함합니다. 이러한 변화는 AI 관찰자들 사이에서 AI 역량 발전이 둔화되고 있는지, 혹은 새로운 패러다임으로 전환되고 있는지에 대한 혼란을 야기하고 있습니다.

이 에세이에서 우리는 이러한 변화에 대한 증거를 살펴보고 몇 가지 주요 요점을 제시합니다.

*   모델 스케일링의 종말을 선언하는 것은 성급한 판단일 수 있습니다. 새로운 데이터 소스와 효율적인 훈련 방법론이 여전히 탐구될 여지가 많습니다.
*   모델 스케일링의 지속 여부와 관계없이, 이 문제에 대한 업계 리더들의 태도 변화는 그들의 예측이 얼마나 변덕스러운지를 보여줍니다. 그들은 우리보다 더 많은 정보를 가지고 있지 않으며, 그들의 서사는 상업적 이익에 크게 영향을 받습니다.
*   추론 스케일링은 실재하며, 단기적으로 빠른 역량 향상으로 이어질 수 있는 많은 '로우 행잉 프루트(low-hanging fruit)'가 있습니다. 하지만 장기적으로는 그 효과가 예측 불가능하고 특정 도메인(domain)에 국한될 가능성이 높습니다.
*   AI 역량 향상과 실제 사회적, 경제적 영향 사이의 연관성은 여전히 미약합니다. AI의 영향력은 기술적 역량 자체보다는 제품 개발 속도, 사용자 경험(UX), 그리고 사회적 채택률에 의해 결정됩니다.

### 모델 스케일링은 끝났을까요?

모델 스케일링은 끝났을까요? 이러한 갑작스러운 변화는 새로운 기술적 도전을 야기합니다. 최근의 비관론은 과도한 측면이 있으며, 우리는 모델 스케일링에 대한 과도한 비관론에 대해 경고해야 합니다. 과거에는 스케일링 과대광고에 대한 경고가 필요했다면, 이제는 반대의 상황입니다.

현재 GPT-4급 모델에서 '평소와 같은 스케일링'은 한계에 도달하고 있습니다. 이는 이러한 모델들이 이미 쉽게 접근 가능한 대부분의 데이터 소스(data sources)로 훈련되었기 때문입니다. 우리는 모델 스케일링을 계속하기 위해 새로운 접근 방식이 필요할 것이라는 점을 이미 인지하고 있었습니다. 따라서 새로운 아이디어가 시도되고 실패했다는 명확한 증거가 없는 한, 모델 스케일링에 더 이상의 발전 가능성이 없다고 단정하기는 어렵습니다. 예를 들어, 멀티모달 모델(multimodal models) 훈련에 유튜브(YouTube) 영상 자체(전사된 텍스트가 아닌 원본 영상)를 포함하는 것은 새로운 AI 역량을 열어줄 수도 있습니다. 혹은 예상치 못한 어려움을 겪을 수도 있습니다. 이는 실제로 시도되기 전까지는 알 수 없으며, 이러한 시도가 이루어졌는지 여부도 불분명합니다. 특히 구글(Google)과 같이 독점적인 데이터 자원을 가진 기업들이 이 분야에서 중요한 역할을 할 수 있습니다.

데이터의 양뿐만 아니라 질과 다양성도 모델 스케일링의 중요한 요소로 부상하고 있습니다. 합성 데이터(synthetic data) 생성 기술의 발전이나 특정 도메인에 특화된 고품질 데이터셋(datasets) 구축은 스케일링의 새로운 길을 열어줄 수 있습니다. 또한, 알고리즘적 효율성의 개선, 즉 동일한 연산 자원으로 더 큰 모델을 훈련하거나 더 나은 성능을 달성하는 방법론의 발전도 모델 스케일링의 중요한 부분입니다. 이는 단순히 모델 크기를 늘리는 것을 넘어, 훈련 프로세스 자체를 최적화하는 방향으로 진화하고 있음을 의미합니다.

### 1 모델 스케일링에 대한 상황이 여전히 불확실하다면, 왜 서사가 뒤집혔을까요?

GPT-4가 훈련을 마친 지 2년이 넘어가면서, 차세대 모델이 예상보다 지연될 것이라는 전망은 점점 더 신뢰를 잃고 있었습니다. 한 선도 기업이 개발의 어려움을 인정하자, 다른 기업들도 유사한 문제에 대해 공개적으로 언급하기 시작했습니다. 이는 마치 댐의 작은 균열이 결국 전체 댐을 무너뜨리는 것과 같은 연쇄 반응을 일으켰습니다. 마지막으로, OpenAI의 추론 모델(reasoning model) o1의 출시는 기업들이 모델 스케일링의 어려움을 인정하면서도 새로운 방향으로 전환할 수 있는 명분을 제공했습니다. 추론 스케일링으로의 전환을 주장함으로써 기존의 체면을 유지할 수 있었기 때문입니다.

분명히 말하자면, 많은 AI 연구소들이 새로운 모델을 개발했지만, 상용화에는 어려움을 겪고 있다는 보고서를 의심할 이유는 없습니다. 그러나 이러한 사실에서 어떤 결론을 도출해야 할지는 여전히 불확실합니다. 더 큰 모델이 출시되지 않은 몇 가지 가능한 이유는 다음과 같습니다.

*   수렴 실패(convergence failures)나 다중 데이터센터 훈련 실행(multi-datacenter training runs)에서의 내결함성(fault tolerance) 확보와 같은 기술적 난관.
*   개발된 모델이 GPT-4급 모델보다 크게 나아지지 않아, 상업적 출시의 가치가 충분하지 않았을 가능성.
*   모델의 성능이 기대에 미치지 못하여, 개발자들이 미세 조정(fine tuning)을 통해 성능을 끌어올리는 데 예상보다 오랜 시간을 소요했을 가능성.

요약하자면, 모델 스케일링이 실제로 한계에 도달했을 수도 있지만, 이러한 문제들이 일시적이며 결국 한 회사가 기술적 어려움을 해결하거나 새로운 데이터 소스를 찾는 등의 방법으로 이를 극복할 방법을 찾을 수도 있습니다. 또한, '작은 언어 모델(Small Language Model, SLM)'과 같은 경량 모델의 발전은 특정 응용 분야에서 대규모 모델의 대안이 될 수 있으며, 이는 모델 스케일링의 유일한 경로가 아님을 시사합니다.

### 내부자들에게 의존하는 것을 멈춥시다

새로운 서사가 그렇게 빨리 부상한 것도 이상하지만, 모델 스케일링의 잠재적 한계가 명백했음에도 불구하고 이전 서사가 그렇게 오랫동안 지속되었다는 점도 흥미롭습니다. 그것이 지속된 주된 이유는 업계 리더들이 스케일링이 몇 년 더 계속될 것이라고 확언했기 때문입니다. 일반적으로 언론인(및 대부분의 다른 사람들)은 외부인보다 업계 내부자에게 의존하는 경향이 있습니다. 하지만 이러한 과도한 의존이 과연 정당한가요?

업계 리더들은 AI 기술의 장기적인 발전을 예측하는 데 있어 좋은 실적을 보여주지 못했습니다. 좋은 예는 지난 10년 대부분 동안 자율주행차에 대한 과도한 낙관론입니다. (자율주행은 마침내 현실이 되었지만, 레벨 5, 즉 완전 자동화는 아직 존재하지 않습니다.) 덧붙여 말하자면, 내부자 예측의 실적을 더 잘 이해하기 위해 지난 10년간 저명한 업계 내부자들이 AI에 대해 내놓은 모든 예측을 체계적으로 분석하는 것이 흥미로울 것입니다. 이는 그들의 예측이 얼마나 자주 빗나갔는지, 그리고 그 이유가 무엇이었는지에 대한 중요한 통찰을 제공할 것입니다.

내부자들의 주장에 더 큰 비중을 두어야 할 몇 가지 이유가 있지만, 그들에게 덜 비중을 두어야 할 중요한 이유들도 있습니다. 이들을 하나씩 분석해 봅시다.

업계 내부자들이 아직 출시되지 않은 모델의 성능과 같은 독점적인 정보(proprietary information)를 가지고 있어 미래에 대한 그들의 주장을 더 정확하게 만들 수 있다는 것은 사실입니다. 그러나 모델 가중치(model weights)를 공개하고 과학적 통찰력, 데이터셋(datasets) 및 기타 아티팩트(artifacts)를 공유하는 일부 회사를 포함하여 많은 AI 회사들이 최첨단 기술에 근접해 있다는 점을 고려할 때, 우리는 기껏해야 몇 달 정도의 이점에 대해 이야기하는 것이며, 이는 예를 들어 3년 예측의 맥락에서는 사소한 것입니다. 게다가 우리는 기업이 내부적으로 얼마나 많은 추가 정보(역량 측면이든, 특히 안전 측면이든)를 가지고 있는지 과대평가하는 경향이 있습니다. 내부자들은 오랫동안 '우리가 아는 것을 당신이 알기만 한다면...'이라는 식의 경고를 해왔지만, 이는 대부분 추측에 의존한 것으로 드러났습니다.

내부자들에게 더 큰 비중을 두어야 할 또 다른 잠재적 이유는 그들의 기술 전문성입니다. 우리는 이것이 강력한 이유라고 생각하지 않습니다. 학계에도 업계만큼 AI 전문 지식이 많습니다. 더 중요하게는, AI 예측에 들어가는 거친 추세 외삽(trend extrapolation)을 뒷받침하는 데 깊은 기술 전문성은 그리 중요하지 않습니다. 또한 기술 전문성만으로는 충분하지 않습니다. 비즈니스 및 사회적 요인이 AI의 방향을 결정하는 데 적어도 그만큼 큰 역할을 합니다. 자율주행차의 경우, 그러한 요인 중 하나는 사회가 공공 도로를 실험에 사용하는 것을 얼마나 용인하는지입니다. 대규모 AI 모델의 경우, 우리는 이전에 가장 중요한 요인은 스케일링이 기술적으로 실현 가능한지 여부가 아니라 비즈니스적으로 타당한지 여부라고 주장했습니다. 따라서 기술 전문가들은 큰 이점을 가지고 있지 않을 뿐만 아니라, 기술적 측면을 과도하게 강조하는 경향이 과신에 찬 예측으로 이어지는 경향이 있습니다. 요컨대, 내부자들의 견해에 더 큰 비중을 두어야 할 이유는 그리 중요하지 않습니다.

반면에, 그들의 견해에 덜 비중을 두어야 할 크고 명백한 이유가 있는데, 그것은 그들이 상업적 이익에 부합하는 말을 할 유인이 있으며, 그렇게 해왔다는 실적이 있기 때문입니다. 예를 들어, 수츠케버는 OpenAI에 있을 때 회사가 자금을 조달해야 했으므로 스케일링을 옹호할 유인이 있었습니다. 그러나 이제 그가 스타트업 Safe Superintelligence를 이끌고 있으므로, 훨씬 적은 자본에 접근할 수 있음에도 불구하고 OpenAI, Anthropic, Google 등과 경쟁할 수 있음을 투자자들에게 설득해야 합니다. 아마도 그래서 그가 이제 사전 훈련(pre-training)을 위한 데이터가 부족하다고 말하는 것일 겁니다. 마치 그것이 어떤 깨달음인 양, 끝없이 반복되어 온 주장이 아니라 말입니다.

다시 말하지만, 모델 스케일링이 끝났는지 아닌지는 알 수 없습니다. 그러나 업계의 갑작스러운 태도 변화는 너무나 뻔뻔하여 내부자들이 어떤 수정 구슬도 가지고 있지 않으며 다른 모든 사람들과 비슷한 추측을 하고 있을 뿐만 아니라, 거품 속에 갇혀 자신들이 세상에 파는 과대광고를 쉽게 받아들임으로써 더욱 편향되어 있다는 점을 의심할 여지 없이 보여줍니다. 이러한 점을 고려할 때, 우리의 제안은 모든 사람, 특히 언론인, 정책 입안자, 그리고 AI 커뮤니티에게 기술의 미래, 특히 사회적 영향에 대해 예측할 때 내부자들의 견해에 대한 의존을 끝내라는 것입니다. 이는 미국에 만연한 무의식적인 편견, 즉 “극단적인 부와 그에 따르는 권력을 미덕과 지능과 동일시하는 듯한 뚜렷한 미국적 질병”의 형태로 존재하기 때문에 노력이 필요할 것입니다. (브라이언 가드너(Bryan Gardiner)의 마리엣제 샤케(Marietje Schake)의 'The Tech Coup' 서평에서 인용) AI 기술의 발전은 더 이상 소수의 내부자에게만 맡겨질 수 없는 중대한 사회적 문제입니다. 규제 기관, 시민 사회 단체, 그리고 일반 대중의 목소리가 AI의 미래를 형성하는 데 필수적입니다.

AI Snake Oil은 AI 과대광고를 폭로하고 새로운 개발에 대한 증거 기반 분석을 게시합니다. 구독하기

### 추론 스케일링을 통해 역량 발전이 계속될까요?

물론, 모델 스케일링만이 AI 시스템의 역량을 향상시키는 유일한 접근 방식은 아닙니다. 추론 스케일링은 최근 많은 발전이 있는 분야입니다. 예를 들어, OpenAI의 o1과 오픈 가중치(open-weights) 경쟁 모델인 DeepSeek R1은 추론 모델(reasoning models)입니다. 이들은 답변을 제공하기 전에 '추론'하도록 미세 조정되었습니다. 다른 방법들은 모델 자체는 변경하지 않지만, 여러 솔루션을 생성하고 품질에 따라 순위를 매기는 등의 기법을 사용합니다. 이는 '생각의 사슬(Chain-of-Thought)'이나 '생각의 나무(Tree-of-Thought)'와 같은 프롬프트 엔지니어링(prompt engineering) 기법과도 연결됩니다.

추론 스케일링이 얼마나 중요한 추세가 될지를 결정할 두 가지 주요 미해결 질문이 있습니다.

*   어떤 종류의 문제에 잘 작동하는가?
*   잘 작동하는 문제의 경우, 추론 중에 더 많은 연산을 수행함으로써 얼마나 많은 개선이 가능한가?

언어 모델의 토큰당 출력 비용은 하드웨어 및 알고리즘 개선으로 인해 빠르게 감소하고 있으므로, 만약 추론 스케일링이 여러 자릿수(orders of magnitude)에 걸쳐 개선을 가져온다면(예를 들어, 주어진 작업에서 백만 개의 토큰을 생성하는 것이 십만 개의 토큰을 생성하는 것보다 훨씬 더 나은 성능을 제공한다면), 이는 큰 의미가 있을 것입니다. 그러나 이러한 연산 증가는 지연 시간(latency) 증가와 직결되므로, 실시간 상호작용이 중요한 응용 분야에서는 신중한 접근이 필요합니다.

첫 번째 질문에 대한 직관적이고 간단한 답은 추론 스케일링이 코딩이나 수학 문제 해결과 같이 명확한 정답이 있는 문제에 유용하다는 것입니다. 이러한 작업에서는 두 가지 관련 사항 중 적어도 하나가 사실인 경향이 있습니다. 첫째, 기호 추론(symbolic reasoning)은 정확도를 향상시킬 수 있습니다. 이는 LLM이 통계적 특성 때문에 잘하지 못하는 것이지만, 사람이 펜과 종이를 사용하여 수학 문제를 푸는 것처럼 추론을 위해 출력 토큰(output tokens)을 사용함으로써 극복할 수 있습니다. 둘째, 올바른 솔루션을 생성하는 것보다 검증하는 것이 더 쉽습니다(때로는 코딩을 위한 단위 테스트(unit tests)나 수학 정리 증명을 위한 증명 검사기(proof checkers)와 같은 외부 검증 도구의 도움을 받기도 합니다). 이처럼 추론 스케일링은 '자체 수정(self-correction)' 메커니즘을 통해 모델의 신뢰성을 높이는 데 기여할 수 있습니다.

대조적으로, 글쓰기나 언어 번역과 같은 작업에서는 추론 스케일링이 큰 차이를 만들 수 있다고 보기는 어렵습니다. 특히 한계가 훈련 데이터 때문이라면 더욱 그렇습니다. 예를 들어, 모델이 저자원 언어(low-resource language)의 관용구를 알지 못해 번역을 제대로 하지 못한다면, 모델은 추론을 통해 이 문제를 해결할 수 없습니다. 이는 추론 스케일링이 모든 종류의 문제에 대한 만능 해결책이 아님을 시사합니다.

지금까지 우리가 가진 초기 증거는 단편적이지만, 이러한 직관과 일치합니다. OpenAI o1에 초점을 맞추면, 코딩, 수학, 사이버 보안(cybersecurity), 장난감 세계에서의 계획(planning in toy worlds), 그리고 다양한 시험에서 GPT-4o와 같은 최첨단 언어 모델(state-of-the-art language models)에 비해 개선된 성능을 보입니다. 시험 성능의 개선은 지식이나 창의성보다는 질문에 답하는 데 필요한 추론의 중요성과 강하게 연관되어 있는 것으로 보입니다. 수학, 물리학, LSAT에서는 큰 개선이 있었고, 생물학 및 계량경제학(econometrics)과 같은 과목에서는 작은 개선이 있었으며, 영어에서는 무시할 만한 개선이 있었습니다. o1이 개선을 가져오지 않는 것으로 보이는 작업에는 글쓰기, 특정 사이버 보안 작업(아래에서 설명), 유해성 회피(avoiding toxicity), 그리고 인간이 생각할수록 더 못하게 되는 것으로 알려진 흥미로운 작업들이 포함됩니다.

우리는 추론 모델이 기존 언어 모델과 어떻게 다른지에 대한 가용한 증거를 정리한 자료를 제공합니다. 당분간은 업데이트를 계속할 예정이지만, 곧 쏟아져 나오는 연구 결과들을 따라잡기 어려워질 것으로 예상합니다.

이제 두 번째 질문을 고려해 봅시다. 무한한 추론 연산 예산(inference compute budget)이 있다고 가정할 때, 추론 스케일링을 통해 얼마나 큰 개선을 얻을 수 있을까요? OpenAI가 o1의 역량을 과시하기 위해 내세운 대표적인 예시는 수학 벤치마크(benchmark)인 AIME였습니다. 그들의 그래프는 이 질문을 애매하게 열어두었습니다. 성능이 포화 상태에 도달하려는 것일까요, 아니면 100%에 가깝게 밀어붙일 수 있을까요? 또한 그래프에 x축 레이블(labels)이 편리하게 생략되어 있다는 점도 주목하십시오.

Source: OpenAI

외부 연구자들이 이 그래프를 재구성하려는 시도는 (1) x축의 절단점(cutoff)이 아마도 약 2,000 토큰(tokens)이고, (2) o1에게 이보다 더 오래 생각하도록 요청하면 그렇게 하지 않는다는 것을 보여줍니다. 따라서 이 질문은 여전히 답을 얻지 못했으며, 더 명확한 정보를 얻기 위해서는 오픈 소스 모델(open-source models)을 사용한 실험을 기다려야 합니다. o1 뒤에 있는 기술을 공개적으로 재현하려는 활발한 노력이 있다는 것은 고무적입니다.

최근 '추론 스케일링 fLaws(Inference Scaling fLaws)'라는 논문(제목은 추론 스케일링 법칙(inference scaling laws)에 대한 말장난)에서 우리는 추론 스케일링에 대한 다른 접근 방식을 살펴보았습니다. 즉, 외부 검증 도구(external verifier)에 의해 올바르다고 판단될 때까지 솔루션을 반복적으로 생성하는 것입니다. 이 접근 방식이 여러 자릿수(orders of magnitude)만큼 스케일링을 유용하게 증가시킬 수 있다는 희망(우리의 과거 연구 포함)과 관련되어 있었지만, 우리는 이것이 검증 도구의 품질에 극도로 민감하다는 것을 발견했습니다. 검증 도구가 약간 불완전하다면, 많은 현실적인 코딩 작업 환경에서 성능은 약 10번의 시도 후에 최고점에 도달한 다음 실제로 감소하기 시작합니다. 일반적으로 말해서, 추론 스케일링 '법칙'에 대한 증거는 설득력이 없으며, 추론 시점에 (예를 들어) 수백만 개의 토큰을 생성하는 것이 실제로 도움이 될 실제 문제가 있는지 여부는 아직 지켜봐야 합니다.

### 추론 스케일링이 다음 개척지일까요?

추론 스케일링에는 많은 '로우 행잉 프루트(low-hanging fruit)'가 있으며, 단기적인 발전은 매우 빠를 가능성이 높습니다. 특히, 추론 모델의 현재 한계 중 하나는 에이전트 시스템(agentic systems)에서 잘 작동하지 않는다는 것입니다. 우리는 에이전트에게 연구 논문과 함께 제공된 코드를 재현하도록 요청하는 자체 벤치마크(benchmark)인 CORE-Bench에서 이를 관찰했습니다. 가장 성능이 좋은 에이전트는 Claude 3.5 Sonnet으로 38%를 기록했지만, o1-mini로는 24%에 불과했습니다. 이는 추론 모델이 한 사이버 보안 평가에서는 개선을 가져왔지만 다른 평가에서는 그렇지 않은 이유를 설명합니다. 그중 하나는 에이전트를 포함했기 때문입니다.

우리는 에이전트가 추론 모델로부터 이점을 얻지 못하는 두 가지 이유가 있다고 생각합니다. 첫째, 이러한 모델은 일반 모델과 다른 프롬프트 스타일(prompting styles)을 요구하며, 현재의 에이전트 시스템은 일반 모델에 프롬프트를 제공하는 데 최적화되어 있습니다. 둘째, 우리가 아는 한, 추론 모델은 지금까지 코드 실행, 셸 상호작용(shell interaction) 또는 웹 검색(web search)과 같은 환경으로부터 피드백(feedback)을 받는 설정에서 강화 학습(reinforcement learning)을 사용하여 훈련되지 않았습니다. 다시 말해, 그들의 도구 사용 능력은 추론을 배우기 전의 기본 모델보다 나을 것이 없습니다. 이것들은 비교적 간단한 문제처럼 보이지만, 해결에는 깊은 통찰력이 필요합니다. 이러한 문제들을 해결하면 중요한 새로운 AI 에이전트(AI agent) 역량이 가능해질 수 있습니다. 예를 들어, 프롬프트(prompt)에서 복잡하고 완전히 기능하는 앱을 생성하는 것과 같습니다. (이미 이를 시도하는 도구들이 있지만, 잘 작동하지 않습니다.)

하지만 장기적으로는 어떨까요? 추론 스케일링이 지난 7년간 모델 스케일링에서 보았던 것과 같은 종류의 발전을 가져올까요? 모델 스케일링은 데이터, 모델 크기, 연산량을 '단순히' 늘리기만 하면 되었기 때문에 매우 흥미로웠습니다. 알고리즘적 돌파구(algorithmic breakthroughs)는 필요 없었습니다. 추론 스케일링의 경우(현재까지는) 그렇지 않습니다. 추론 스케일링 기술은 매우 많고, 무엇이 작동하고 작동하지 않는지는 문제에 따라 다르며, 심지어 총체적으로도 제한된 도메인(domains)에서만 작동합니다. AI 개발자들은 이러한 한계를 극복하기 위해 노력하고 있습니다. 예를 들어, OpenAI의 강화 미세 조정(reinforcement finetuning) 서비스는 회사가 미래 모델을 미세 조정하기 위해 다양한 도메인에서 고객 데이터를 수집하는 방법으로 여겨집니다.

약 10년 전, 강화 학습(reinforcement learning, RL)은 아타리(Atari)와 같은 많은 게임에서 돌파구를 마련했습니다. 많은 과대광고가 있었고, 많은 AI 연구자들은 강화 학습을 통해 인공 일반 지능(AGI)에 도달할 수 있기를 희망했습니다. 사실, 강화 학습에 대한 높은 기대가 명시적으로 AGI에 초점을 맞춘 연구소, 특히 OpenAI의 탄생으로 이어졌습니다. 그러나 그러한 기술들은 게임과 같은 좁은 도메인을 넘어 일반화되지 못했습니다. 이제 다시 강화 학습에 대한 비슷한 과대광고가 있습니다. 분명히 매우 강력한 기술이지만, 지금까지 우리는 이전 과대광고의 물결이 사라지게 했던 것과 유사한 한계들을 보고 있습니다. 이러한 한계는 현실 세계의 복잡성, 안전 문제, 그리고 평가의 어려움과 관련이 있습니다.

AI 역량 발전이 둔화될지 여부를 예측하는 것은 불가능합니다. 사실, 예측은 차치하고라도, 합리적인 사람들은 AI 발전이 이미 둔화되었는지 여부에 대해 매우 다른 의견을 가질 수 있습니다. 증거를 매우 다르게 해석할 수 있기 때문입니다. 이는 '역량'이 측정 방식에 매우 민감한 구성 요소이기 때문입니다. 우리가 더 확신을 가지고 말할 수 있는 것은 역량 발전의 본질이 모델 스케일링과 추론 스케일링에서 다를 것이라는 점입니다. 지난 몇 년 동안, 새로운 모델들은 예측 가능하게 매년 광범위한 도메인에 걸쳐 역량 개선을 가져왔습니다. 대형 연구소 외부의 많은 AI 연구자들 사이에서는 다음 최첨단 LLM(Large Language Model)이 출시될 때까지 기다리는 것 외에는 할 일이 거의 없다는 비관적인 분위기가 있었습니다. 추론 스케일링을 사용하면 역량 개선은 하드웨어 인프라(hardware infrastructure) 투자보다는 알고리즘 발전(algorithmic advances)에 의해 더 많이 좌우되어 불균등하고 예측하기 어려울 가능성이 높습니다. 오래된 계획 문헌(planning literature)에서 나온 아이디어와 같이 LLM의 지배 기간 동안 버려졌던 많은 아이디어들이 이제 다시 논의되고 있으며, 이 분야는 지난 몇 년보다 지적으로 더 활기찬 것처럼 보입니다. 이러한 지적 활기는 AI 연구의 새로운 황금기를 예고할 수 있습니다.

### 제품 개발은 역량 증가에 뒤처집니다

AI 기반 애플리케이션(application)의 개발은 AI 역량 증가에 훨씬 뒤처져 있으며, 이는 시장의 중요한 과제로 남아있습니다. 역량 둔화가 있는지에 대한 격렬한 논쟁은 아이러니합니다. 역량 증가와 AI의 실제 유용성 사이의 연관성이 극히 미약하기 때문입니다. 기존 AI 역량조차도 크게 활용되지 못하고 있습니다. 한 가지 이유는 역량-신뢰성 격차(capability-reliability gap)입니다. 특정 역량이 존재하더라도, 인간을 개입시키지 않고 실제로 작업을 자동화할 수 있을 만큼 충분히 안정적으로 작동하지 않을 수 있습니다(80%만 작동하는 음식 배달 앱을 상상해 보십시오). 그리고 신뢰성을 향상시키는 방법은 종종 애플리케이션(application)에 따라 다르며 역량을 향상시키는 방법과는 별개입니다. 그렇긴 하지만, 추론 모델은 신뢰성 개선도 보이는 것으로 보이며, 이는 고무적입니다.

현재 AI 역량을 완전히 활용하는 제품을 만드는 데는 상당한 시간이 소요될 수 있다는 몇 가지 비유가 있습니다. 인터넷과 웹 뒤에 있는 기술은 90년대 중반에 대부분 확고해졌습니다. 그러나 웹 앱(web apps)의 잠재력을 실현하는 데는 1~20년이 더 걸렸습니다. 또는 대규모 언어 모델(large language models)을 위한 GUI(Graphical User Interface)를 구축해야 한다고 주장하는 이 생각할 거리를 제공하는 에세이를 고려해 보십시오. 이는 텍스트를 통해서보다 훨씬 더 높은 대역폭(bandwidth)으로 모델과 상호작용할 수 있게 할 것입니다. 이러한 관점에서, AI 기반 제품의 현재 상태는 GUI 이전의 PC와 유사합니다. 단순히 기술을 개발하는 것을 넘어, 사용자가 기술과 상호작용하는 방식을 혁신하는 것이 중요합니다.

제품 개발의 지연은 AI 회사들이 제품 측면에 충분한 주의를 기울이지 않았다는 사실로 인해 더욱 심화됩니다. 그들은 AI의 범용적인 특성이 소프트웨어 공학(software engineering)의 어려운 문제들로부터 면제를 부여한다고 믿었기 때문입니다. 다행히도, 이는 최근에 변화하기 시작했습니다. 이제 제품에 초점을 맞추면서, AI 회사들과 사용자들은 소프트웨어 개발, 특히 사용자 경험(user experience) 측면이 어렵고 AI 모델 개발보다 더 광범위한 기술을 요구한다는 것을 재발견하고 있습니다. 좋은 예시는 ChatGPT에서 파이썬(Python) 코드를 실행하는 두 가지 다른 방법(파워 유저(power users)에게 가장 중요한 기능 중 하나)이 있다는 사실과, 각 방법의 기능과 한계에 대해 기억해야 할 복잡하고 문서화되지 않은 규칙들이 있다는 것입니다. 사이먼 윌리슨(Simon Willison)은 말합니다. 이 모든 것이 절망적으로 혼란스럽다고 생각하시나요? 저는 당신을 탓하지 않습니다. 저는 20년 이상의 경력을 가진 전문 웹 개발자이자 파이썬 엔지니어이며, 위의 규칙들을 겨우 이해하고 내면화할 수 있습니다. 그럼에도 불구하고, 이는 일주일 전과 비교하면 큰 발전입니다. 당시 이 모델들은 강력한 코딩 역량을 가지고 있었지만 인터넷을 사용할 수 있는 코드를 실행하는 기능은 없었습니다! 그리고 지금도 o1은 인터넷에 접속하거나 코드를 실행할 수 없습니다. 이는 AI 모델이 단순히 강력한 역량을 갖추는 것을 넘어, 실제 환경에서 유용하게 작동하기 위한 '라스트 마일(last mile)' 문제 해결이 얼마나 중요한지를 보여줍니다.

AI 영향의 관점에서 볼 때, 이 시점에서 역량 개선보다 훨씬 더 중요한 것은 사용자가 기존 역량을 활용할 수 있는 실용적인 제품을 구축하는 것입니다. 마지막으로, 제품 개발이 역량에 뒤처지는 동안, AI 기반 제품의 채택은 다양한 행동적, 조직적, 사회적 이유로 인해 제품 개발보다 훨씬 더 뒤처집니다. AI의 영향(긍정적이든 부정적이든)에 관심 있는 사람들은 현재 또는 예측된 역량보다 이러한 하위 단계(downstream aspects)에 훨씬 더 많은 주의를 기울여야 합니다. AI 제품의 성공은 기술 자체의 우수성뿐만 아니라, 사용자 인터페이스(UI), 개인 정보 보호, 윤리적 고려 사항, 그리고 사회적 신뢰 구축에 달려 있습니다.

AI Snake Oil은 AI 과대광고를 폭로하고 새로운 개발에 대한 증거 기반 분석을 게시합니다. 구독하기

### 결론

모델 스케일링이 끝났을 수도 있지만, 새로운 접근 방식은 계속 탐색될 것입니다. 그러나 영원히 계속되지는 않을 것이며, 모델 스케일링의 종말은 많은 긍정적인 점들을 가져옵니다. AI 발전이 다시 연산량(compute)뿐만 아니라 새로운 아이디어에 의존하게 됩니다. 대기업, 스타트업(startups), 학계 연구자들이 모두 비교적 공평한 경쟁의 장에서 경쟁할 수 있습니다. 임의의 훈련 연산량 임계값(training compute thresholds)에 기반한 규제는 옹호하기가 더욱 어려워집니다. 그리고 모델 자체가 제품이 아니라 단지 기술이라는 명확한 인식이 생깁니다. 이는 AI 개발의 초점이 '규모'에서 '지능'과 '유용성'으로 이동하고 있음을 의미합니다.

AI의 미래에 관해서는, 기술 내부자들이 미래를 예측하는 데 어려움을 겪고 있으며, 그들의 과신하고, 자기 이익적인 예측을 더 이상 맹목적으로 신뢰하지 않을 때입니다. 그리고 기술적 예측을 넘어 AI가 세상에 미치는 영향에 대한 주장으로 나아갈 때, 업계 리더들을 신뢰할 이유는 더욱 줄어듭니다. AI의 미래는 기술 전문가들만의 전유물이 아니라, 사회 전체의 협력과 숙고를 통해 형성되어야 할 과제입니다. 윤리, 규제, 사회적 영향에 대한 논의는 기술 발전과 동등하게 중요하게 다루어져야 합니다.

**감사의 말씀.** 초안에 대한 피드백을 주신 재커리 S. 시겔(Zachary S. Siegel)께 감사드립니다.

---
1 OpenAI가 과거에 유튜브를 크롤링(crawled)한 것으로 알려져 있지만, 그것은 유튜브의 작은 부분에 불과했습니다. 구글이 알아차리지 못하게 유튜브 전체를 크롤링하는 것은 불가능할 것입니다.
2 Epoch AI의 훌륭한 분석에 따르면 스케일링은 2030년까지 계속될 수 있다고 합니다. 그러나 이 분석은 너무 최근(2024년 8월)에 발표되어 스케일링 서사의 근거가 될 수는 없었습니다.
3 우리는 AI 모델 및 시스템의 안전에 대한 실질적인 지식을 언급하고 있습니다. 내부 고발자들은 OpenAI의 안전 관련 프로세스에 대한 새로운 지식을 제시했습니다.
4 그렇긴 하지만, 미래의 비용 감소를 당연하게 여길 수는 없습니다. 우리는 또한 양자화(quantization)와 같은 추론 비용 절감 기술의 근본적인 한계에 부딪히고 있습니다.
5 우리는 모든 모델에 대해 4달러의 비용 제한을 설정했습니다. 소규모 샘플에서 10달러의 비용 제한을 두었을 때, o1-preview는 매우 저조한 성능(정확도 10%)을 보였습니다. 비용 제약으로 인해 전체 데이터에 대해 더 높은 비용 제한으로 모델을 평가하지 않았습니다.
6 o1은 ChatGPT 인터페이스에서 추론 중에 도구에 접근할 수도 없습니다! Gemini Flash 2.0은 가능하지만, 이것이 도구 사용을 위해 미세 조정된 것은 물론, 추론을 위해 미세 조정된 모델인지도 불분명합니다.