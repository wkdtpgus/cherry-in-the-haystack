아르빈드 나라야난, 베네딕트 스트뢰블, 사야시 카푸어 공저.

2023년 3월 GPT-4 출시 이후, 기술 업계의 지배적인 서사는 모델의 지속적인 스케일링(scaling)이 인공 일반 지능(artificial general intelligence)으로, 나아가 초지능(superintelligence)으로 이어질 것이라는 것이었습니다. 이러한 극단적인 예측은 점차 수그러들었지만, 한 달 전까지만 해도 AI 업계의 지배적인 믿음은 모델 스케일링이 예측 가능한 미래에도 계속될 것이라는 것이었습니다. 그러다 The Information, 로이터(Reuters), 블룸버그(Bloomberg)에서 연달아 세 건의 뉴스 보도가 나왔는데, 이들 보도는 선도적인 AI 개발사 세 곳(OpenAI, Anthropic, Google Gemini) 모두 차세대 모델에서 문제에 부딪혔음을 밝혔습니다. 스케일링의 가장 주목할 만한 지지자였던 일리야 수츠케버(Ilya Sutskever)를 포함한 많은 업계 관계자들은 이제 매우 다른 이야기를 하고 있습니다. 수츠케버는 “2010년대는 스케일링의 시대였지만, 이제 우리는 다시 경이로움과 발견의 시대로 돌아왔습니다. 모두가 다음 단계를 찾고 있습니다.”라고 말했습니다. “이제는 올바른 것을 스케일링하는 것이 그 어느 때보다 중요합니다.” (로이터)

이러한 변화는 AI 커뮤니티 내에서 뜨거운 논쟁을 불러일으켰습니다. 모델 스케일링이 여전히 발전의 핵심 동력인지, 아니면 '추론 스케일링(inference scaling)' 또는 '테스트 시간 컴퓨팅 스케일링(test-time compute scaling)'과 같은 새로운 접근 방식이 AI 역량을 향상시키는 미래가 될 것인지에 대한 질문입니다. 이 아이디어는 모델을 사용하여 작업을 수행할 때, 예를 들어 응답하기 전에 모델이 '생각'하도록 함으로써 점점 더 많은 연산(computation)을 사용하는 것입니다. 이로 인해 AI 관찰자들은 AI 역량 발전이 둔화되고 있는지 여부에 대해 혼란스러워하고 있습니다.

이 에세이에서 우리는 이 질문에 대한 증거를 살펴보고 네 가지 주요 요점을 제시합니다.

*   모델 스케일링의 종말을 선언하는 것은 시기상조입니다.
*   모델 스케일링이 계속될지 여부와 관계없이, 이 문제에 대한 업계 리더들의 태도 변화는 그들의 예측을 신뢰하는 것이 얼마나 어리석은 일인지를 보여줍니다. 그들은 우리보다 훨씬 더 많은 정보를 가지고 있지 않으며, 그들의 서사는 그들의 기득권에 크게 영향을 받습니다.
*   추론 스케일링은 실재하며, 단기적으로 빠른 역량 향상으로 이어질 수 있는 많은 '로우 행잉 프루트(low-hanging fruit, 쉽게 달성할 수 있는 목표)'가 있습니다. 그러나 일반적으로 추론 스케일링으로 인한 역량 향상은 예측 불가능하고 도메인(domain)별로 고르지 않게 분포될 가능성이 높습니다.
*   역량 향상과 AI의 사회적 또는 경제적 영향 사이의 연관성은 극히 미약합니다. 영향력의 병목 현상(bottleneck)은 AI 역량이 아니라 제품 개발 속도와 채택률입니다.

### 모델 스케일링, 그 한계와 새로운 방향

모델 스케일링의 종말에 대한 논의는 AI 연구의 중요한 전환점을 시사합니다. GPT-4급 모델이 쉽게 이용 가능한 대부분의 데이터 소스(data sources)로 훈련되었기 때문에, '평소와 같은 스케일링'은 한계에 봉착했습니다. 하지만 이러한 한계가 모델 스케일링의 완전한 종말을 의미하는 것은 아닙니다. 오히려 데이터 효율성, 합성 데이터(synthetic data) 생성, 그리고 새로운 모델 아키텍처와 같은 혁신적인 접근 방식의 필요성을 강조합니다.

최근 연구에서는 기존 데이터셋을 효율적으로 활용하거나, 모델 자체를 통해 고품질의 합성 데이터를 생성하여 훈련 데이터의 부족 문제를 해결하려는 시도가 활발합니다. 또한, 기존 모델의 구조적 한계를 뛰어넘는 새로운 아키텍처나 최적화 알고리즘 개발을 통해 모델 스케일링의 다음 단계를 모색하고 있습니다. 예를 들어, 멀티모달 모델(multimodal models)의 훈련 조합에 유튜브(YouTube) 영상(전사된 텍스트가 아닌 실제 영상)을 포함하는 것이 새로운 역량을 열어줄 수도 있습니다. 아니면 도움이 되지 않을 수도 있습니다. 누군가 시도해보기 전까지는 알 수 없으며, 시도되었는지 여부도 알 수 없습니다. 이는 아마도 구글(Google)이 해야 할 일일 것입니다. 구글은 경쟁사에게 유튜브 훈련 데이터(training data)를 라이선스할 가능성이 낮기 때문입니다. 이러한 접근 방식들은 단순히 모델 크기를 키우는 것을 넘어, '스마트한 스케일링'의 가능성을 탐구하고 있습니다.

모델 스케일링에 대한 갑작스러운 분위기 변화를 초래한 새로운 정보는 거의 없습니다. 우리는 이 뉴스레터에서 모델 스케일링에 중요한 역풍(headwinds)이 있다고 오랫동안 말해왔습니다. 당시 스케일링 과대광고에 대해 경고했던 것처럼, 이제 우리는 모델 스케일링에 대한 과도한 비관론에 대해 경고해야 합니다. 많은 AI 연구소들이 더 큰 훈련을 수행했지만 결과 모델을 출시하지 않았다는 보고서를 의심할 이유는 없습니다. 그러나 거기서 무엇을 결론 내려야 할지는 덜 명확합니다. 더 큰 모델이 출시되지 않은 몇 가지 가능한 이유는 다음과 같습니다.

*   수렴 실패(convergence failures) 또는 다중 데이터센터 훈련 실행(multi-datacenter training runs)에서 내결함성(fault tolerance)을 달성하는 데 따르는 복잡성 등의 기술적 어려움.
*   모델이 GPT-4급 모델보다 크게 나아지지 않아 출시하기에는 너무 실망스러웠을 것입니다.
*   모델이 GPT-4급 모델보다 크게 나아지지 않아, 개발자가 미세 조정(fine tuning)을 통해 더 나은 성능을 끌어내기 위해 오랜 시간을 보냈을 것입니다.

요약하자면, 모델 스케일링이 실제로 한계에 도달했을 수도 있지만, 이러한 문제들이 일시적이며 결국 한 회사가 기술적 어려움을 해결하거나 새로운 데이터 소스를 찾는 등의 방법으로 이를 극복할 방법을 찾을 수도 있습니다.

### 내부자들의 예측, 과연 신뢰할 수 있는가?

새로운 서사가 그렇게 빨리 등장한 것도 이상하지만, 모델 스케일링의 잠재적 한계가 명백했음에도 불구하고 이전 서사가 그렇게 오랫동안 지속되었다는 점도 흥미롭습니다. 그것이 지속된 주된 이유는 업계 리더들이 스케일링이 몇 년 더 계속될 것이라고 확언했기 때문입니다. 2 일반적으로 언론인(및 대부분의 다른 사람들)은 외부인보다 업계 내부자에게 의존하는 경향이 있습니다. 하지만 이러한 의존이 정당한가요?

업계 리더들은 AI 발전을 예측하는 데 좋은 실적을 가지고 있지 않습니다. 좋은 예는 지난 10년 대부분 동안 자율주행차에 대한 과도한 낙관론입니다. (자율주행은 마침내 현실이 되었지만, 레벨 5, 즉 완전 자동화는 아직 존재하지 않습니다.) 덧붙여 말하자면, 내부자 예측의 실적을 더 잘 이해하기 위해 지난 10년간 저명한 업계 내부자들이 AI에 대해 내놓은 모든 예측을 체계적으로 분석하는 것이 흥미로울 것입니다.

자율주행차 외에도, 웹3(Web3)와 메타버스(Metaverse)에 대한 과도한 기대감은 기술 산업 내부자들의 예측이 얼마나 현실과 동떨어질 수 있는지를 보여주는 또 다른 사례입니다. 이러한 기술들은 엄청난 잠재력을 가지고 있었지만, 초기 과대광고는 기술적, 사회적 준비도를 훨씬 앞질렀습니다. 이는 내부자들이 기술의 가능성을 과장하거나, 특정 기술에 대한 투자 유치나 시장 지배력 확보와 같은 상업적 동기에 의해 예측이 왜곡될 수 있음을 시사합니다. 학계와 독립 연구자들의 예측은 종종 더 신중하고 증거 기반적이지만, 대중의 관심은 여전히 영향력 있는 산업 리더들에게 집중되는 경향이 있습니다.

내부자들의 주장에 더 큰 비중을 두어야 할 몇 가지 이유가 있지만, 그들에게 덜 비중을 두어야 할 중요한 이유들도 있습니다. 이들을 하나씩 분석해 봅시다. 업계 내부자들이 아직 출시되지 않은 모델의 성능과 같은 독점적인 정보(proprietary information)를 가지고 있어 미래에 대한 그들의 주장을 더 정확하게 만들 수 있다는 것은 사실입니다. 그러나 모델 가중치(model weights)를 공개하고 과학적 통찰력, 데이터셋(datasets) 및 기타 아티팩트(artifacts)를 공유하는 일부 회사들을 포함하여 많은 AI 회사들이 최첨단 기술에 근접해 있다는 점을 고려할 때, 우리는 기껏해야 몇 달 정도의 이점에 대해 이야기하는 것이며, 이는 예를 들어 3년 예측의 맥락에서는 사소한 것입니다. 게다가 우리는 기업이 내부적으로 얼마나 많은 추가 정보(역량 측면이든, 특히 안전 측면이든)를 가지고 있는지 과대평가하는 경향이 있습니다. 내부자들은 오랫동안 '우리가 아는 것을 당신이 알기만 한다면...'이라고 경고했지만, 내부 고발자들이 마침내 나섰을 때, 그들은 대부분 다른 모든 사람들이 하는 것과 같은 종류의 추측에 의존하고 있었음이 밝혀졌습니다.

3 내부자들에게 더 큰 비중을 두어야 할 또 다른 잠재적 이유는 그들의 기술 전문성입니다. 우리는 이것이 강력한 이유라고 생각하지 않습니다. 학계에도 업계만큼 AI 전문 지식이 많습니다. 더 중요하게는, AI 예측에 들어가는 거친 추세 외삽(trend extrapolation)을 뒷받침하는 데 깊은 기술 전문성은 그리 중요하지 않습니다. 또한 기술 전문성만으로는 충분하지 않습니다. 비즈니스 및 사회적 요인이 AI의 방향을 결정하는 데 적어도 그만큼 큰 역할을 합니다. 자율주행차의 경우, 그러한 요인 중 하나는 사회가 공공 도로를 실험에 사용하는 것을 얼마나 용인하는지입니다. 대규모 AI 모델의 경우, 우리는 이전에 가장 중요한 요인은 스케일링이 기술적으로 실현 가능한지 여부가 아니라 비즈니스적으로 타당한지 여부라고 주장했습니다. 따라서 기술 전문가들은 큰 이점을 가지고 있지 않을 뿐만 아니라, 기술적 측면을 과도하게 강조하는 경향이 과신에 찬 예측으로 이어지는 경향이 있습니다. 요컨대, 내부자들의 견해에 더 큰 비중을 두어야 할 이유는 그리 중요하지 않습니다.

반면에, 그들의 견해에 덜 비중을 두어야 할 크고 명백한 이유가 있는데, 그것은 그들이 상업적 이익에 부합하는 말을 할 유인이 있으며, 그렇게 해왔다는 실적이 있기 때문입니다. 예를 들어, 수츠케버는 OpenAI에 있을 때 회사가 자금을 조달해야 했으므로 스케일링을 옹호할 유인이 있었습니다. 그러나 이제 그가 스타트업 Safe Superintelligence를 이끌고 있으므로, 훨씬 적은 자본에 접근할 수 있음에도 불구하고 OpenAI, Anthropic, Google 등과 경쟁할 수 있음을 투자자들에게 설득해야 합니다. 아마도 그래서 그가 이제 사전 훈련(pre-training)을 위한 데이터가 부족하다고 말하는 것일 겁니다. 마치 그것이 어떤 깨달음인 양, 끝없이 반복되어 온 주장이 아니라 말입니다.

다시 말하면, 모델 스케일링이 끝났는지 아닌지는 알 수 없습니다. 그러나 업계의 갑작스러운 태도 변화는 너무나 뻔뻔하여 내부자들이 어떤 수정 구슬도 가지고 있지 않으며 다른 모든 사람들과 비슷한 추측을 하고 있을 뿐만 아니라, 거품 속에 갇혀 자신들이 세상에 파는 과대광고를 쉽게 받아들임으로써 더욱 편향되어 있다는 점을 의심할 여지 없이 보여줍니다. 이러한 점을 고려할 때, 우리의 제안은 모든 사람, 특히 언론인, 정책 입안자, 그리고 AI 커뮤니티에게 기술의 미래, 특히 사회적 영향에 대해 예측할 때 내부자들의 견해에 대한 의존을 끝내라는 것입니다. 이는 미국에 만연한 무의식적인 편견, 즉 “극단적인 부와 그에 따르는 권력을 미덕과 지능과 동일시하는 듯한 뚜렷한 미국적 질병”의 형태로 존재하기 때문에 노력이 필요할 것입니다. (브라이언 가드너(Bryan Gardiner)의 마리엣제 샤케(Marietje Schake)의 'The Tech Coup' 서평에서 인용)

AI Snake Oil은 AI 과대광고를 폭로하고 새로운 개발에 대한 증거 기반 분석을 게시합니다. 구독하기

### 추론 스케일링: AI 역량 확장의 새로운 지평?

물론, 모델 스케일링만이 AI 역량을 향상시키는 유일한 방법은 아닙니다. 추론 스케일링은 최근 많은 발전이 있는 분야입니다. 예를 들어, OpenAI의 o1과 오픈 가중치(open-weights) 경쟁 모델인 DeepSeek R1은 추론 모델(reasoning models)입니다. 이들은 답변을 제공하기 전에 '추론'하도록 미세 조정되었습니다. 다른 방법들은 모델 자체는 변경하지 않지만, 여러 솔루션을 생성하고 품질에 따라 순위를 매기는 등의 기법을 사용합니다.

추론 스케일링이 얼마나 중요한 추세가 될지를 결정할 두 가지 주요 미해결 질문이 있습니다.

*   어떤 종류의 문제에 잘 작동하는가?
*   잘 작동하는 문제의 경우, 추론 중에 더 많은 연산을 수행함으로써 얼마나 많은 개선이 가능한가?

언어 모델의 토큰당 출력 비용은 하드웨어 및 알고리즘 개선으로 인해 빠르게 감소하고 있으므로, 만약 추론 스케일링이 여러 자릿수(orders of magnitude)에 걸쳐 개선을 가져온다면(예를 들어, 주어진 작업에서 백만 개의 토큰을 생성하는 것이 십만 개의 토큰을 생성하는 것보다 훨씬 더 나은 성능을 제공한다면), 이는 큰 의미가 있을 것입니다.

4 첫 번째 질문에 대한 직관적이고 간단한 답은 추론 스케일링이 코딩이나 수학 문제 해결과 같이 명확한 정답이 있는 문제에 유용하다는 것입니다. 이러한 작업에서는 두 가지 관련 사항 중 적어도 하나가 사실인 경향이 있습니다. 첫째, 기호 추론(symbolic reasoning)은 정확도를 향상시킬 수 있습니다. 이는 LLM이 통계적 특성 때문에 잘하지 못하는 것이지만, 사람이 펜과 종이를 사용하여 수학 문제를 푸는 것처럼 추론을 위해 출력 토큰(output tokens)을 사용함으로써 극복할 수 있습니다. 둘째, 올바른 솔루션을 생성하는 것보다 검증하는 것이 더 쉽습니다(때로는 코딩을 위한 단위 테스트(unit tests)나 수학 정리 증명을 위한 증명 검사기(proof checkers)와 같은 외부 검증 도구의 도움을 받기도 합니다).

최근 추론 스케일링의 발전은 연쇄 사고(Chain-of-Thought, CoT) 프롬프팅, 트리 오브 서트(Tree-of-Thought, ToT), 그리고 자가 일관성(Self-Consistency)과 같은 기법들을 통해 LLM이 복잡한 추론 작업을 수행하는 방식을 혁신하고 있습니다. CoT는 모델이 최종 답변을 내놓기 전에 중간 추론 단계를 명시적으로 생성하도록 하여 문제 해결 과정을 투명하게 만듭니다. ToT는 여러 추론 경로를 탐색하고 유망한 경로를 선택하여 더 복잡한 문제에 대한 해결책을 찾습니다. 자가 일관성은 여러 추론 경로에서 가장 일관된 결과를 선택하여 정확도를 높입니다. 이러한 기법들은 모델의 '생각하는' 능력을 향상시켜, 단순히 정보 검색을 넘어선 문제 해결 역량을 제공합니다. 또한, 검색 증강 생성(Retrieval-Augmented Generation, RAG)과 같은 기법은 모델이 외부 지식 기반에서 관련 정보를 검색하여 추론 과정에 통합함으로써, 환각(hallucination)을 줄이고 답변의 정확성을 높이는 데 기여합니다.

그러나 이러한 추론 스케일링 기술에도 한계는 명확합니다. 특히 글쓰기나 언어 번역과 같은 창의적이거나 주관적인 작업에서는 추론 스케일링이 큰 차이를 만들 수 있다고 보기는 어렵습니다. 특히 한계가 훈련 데이터 때문이라면 더욱 그렇습니다. 예를 들어, 모델이 저자원 언어(low-resource language)의 관용구를 알지 못해 번역을 제대로 하지 못한다면, 모델은 추론을 통해 이 문제를 해결할 수 없습니다. 또한, 추론 과정에 더 많은 연산을 투입하는 것은 필연적으로 더 높은 컴퓨팅 비용과 지연 시간(latency)을 발생시킵니다. 실시간 상호작용이 중요한 애플리케이션에서는 이러한 추가 비용이 큰 부담으로 작용할 수 있습니다. 따라서 추론 스케일링의 효과를 극대화하면서도 효율성을 유지하는 것이 중요한 과제로 남아있습니다.

지금까지 우리가 가진 초기 증거는 단편적이지만, 이러한 직관과 일치합니다. OpenAI o1에 초점을 맞추면, 코딩, 수학, 사이버 보안(cybersecurity), 장난감 세계에서의 계획(planning in toy worlds), 그리고 다양한 시험에서 GPT-4o와 같은 최첨단 언어 모델(state-of-the-art language models)에 비해 개선된 성능을 보입니다. 시험 성능의 개선은 지식이나 창의성보다는 질문에 답하는 데 필요한 추론의 중요성과 강하게 연관되어 있는 것으로 보입니다. 수학, 물리학, LSAT에서는 큰 개선이 있었고, 생물학 및 계량경제학(econometrics)과 같은 과목에서는 작은 개선이 있었으며, 영어에서는 무시할 만한 개선이 있었습니다. o1이 개선을 가져오지 않는 것으로 보이는 작업에는 글쓰기, 특정 사이버 보안 작업(아래에서 설명), 유해성 회피(avoiding toxicity), 그리고 인간이 생각할수록 더 못하게 되는 것으로 알려진 흥미로운 작업들이 포함됩니다.

우리는 추론 모델이 언어 모델과 어떻게 비교되는지에 대한 가용한 증거를 정리한 웹페이지를 만들었습니다. 당분간은 업데이트를 계속할 예정이지만, 곧 쏟아져 나오는 연구 결과들을 따라잡기 어려워질 것으로 예상합니다.

이제 두 번째 질문을 고려해 봅시다. 무한한 추론 연산 예산(inference compute budget)이 있다고 가정할 때, 추론 스케일링을 통해 얼마나 큰 개선을 얻을 수 있을까요? OpenAI가 o1의 역량을 과시하기 위해 내세운 대표적인 예시는 수학 벤치마크(benchmark)인 AIME였습니다. 그들의 그래프는 이 질문을 애매하게 열어두었습니다. 성능이 포화 상태에 도달하려는 것일까요, 아니면 100%에 가깝게 밀어붙일 수 있을까요? 또한 그래프에 x축 레이블(labels)이 편리하게 생략되어 있다는 점도 주목하십시오.

Source: OpenAI

외부 연구자들이 이 그래프를 재구성하려는 시도는 (1) x축의 절단점(cutoff)이 아마도 약 2,000 토큰(tokens)이고, (2) o1에게 이보다 더 오래 생각하도록 요청하면 그렇게 하지 않는다는 것을 보여줍니다. 따라서 이 질문은 여전히 답을 얻지 못했으며, 더 명확한 정보를 얻기 위해서는 오픈 소스 모델(open-source models)을 사용한 실험을 기다려야 합니다. o1 뒤에 있는 기술을 공개적으로 재현하려는 활발한 노력이 있다는 것은 고무적입니다.

최근 '추론 스케일링 fLaws(Inference Scaling fLaws)'라는 논문(제목은 추론 스케일링 법칙(inference scaling laws)에 대한 말장난)에서 우리는 추론 스케일링에 대한 다른 접근 방식을 살펴보았습니다. 즉, 외부 검증 도구(external verifier)에 의해 올바르다고 판단될 때까지 솔루션을 반복적으로 생성하는 것입니다. 이 접근 방식이 여러 자릿수(orders of magnitude)만큼 스케일링을 유용하게 증가시킬 수 있다는 희망(우리의 과거 연구 포함)과 관련되어 있었지만, 우리는 이것이 검증 도구의 품질에 극도로 민감하다는 것을 발견했습니다. 검증 도구가 약간 불완전하다면, 많은 현실적인 코딩 작업 환경에서 성능은 약 10번의 시도 후에 최고점에 도달한 다음 실제로 감소하기 시작합니다. 일반적으로 말해서, 추론 스케일링 '법칙'에 대한 증거는 설득력이 없으며, 추론 시점에 (예를 들어) 수백만 개의 토큰을 생성하는 것이 실제로 도움이 될 실제 문제가 있는지 여부는 아직 지켜봐야 합니다.

### 추론 스케일링이 다음 개척지일까요?

추론 스케일링에는 많은 '로우 행잉 프루트'가 있으며, 단기적인 발전은 빠를 가능성이 높습니다. 특히, 추론 모델의 현재 한계 중 하나는 에이전트 시스템(agentic systems)에서 잘 작동하지 않는다는 것입니다. 우리는 에이전트에게 연구 논문과 함께 제공된 코드를 재현하도록 요청하는 자체 벤치마크(benchmark)인 CORE-Bench에서 이를 관찰했습니다. 가장 성능이 좋은 에이전트는 Claude 3.5 Sonnet으로 38%를 기록했지만, o1-mini로는 24%에 불과했습니다. 5 이는 추론 모델이 한 사이버 보안 평가에서는 개선을 가져왔지만 다른 평가에서는 그렇지 않은 이유를 설명합니다. 그중 하나는 에이전트를 포함했기 때문입니다.

우리는 에이전트가 추론 모델로부터 이점을 얻지 못하는 두 가지 이유가 있다고 생각합니다. 첫째, 이러한 모델은 일반 모델과 다른 프롬프트 스타일(prompting styles)을 요구하며, 현재의 에이전트 시스템은 일반 모델에 프롬프트를 제공하는 데 최적화되어 있습니다. 둘째, 우리가 아는 한, 추론 모델은 지금까지 코드 실행, 셸 상호작용(shell interaction) 또는 웹 검색(web search)과 같은 환경으로부터 피드백(feedback)을 받는 설정에서 강화 학습(reinforcement learning)을 사용하여 훈련되지 않았습니다. 다시 말해, 그들의 도구 사용 능력은 추론을 배우기 전의 기본 모델보다 나을 것이 없습니다. 6 이것들은 비교적 간단한 문제처럼 보입니다. 이러한 문제들을 해결하면 중요한 새로운 AI 에이전트(AI agent) 역량이 가능해질 수 있습니다. 예를 들어, 프롬프트(prompt)에서 복잡하고 완전히 기능하는 앱을 생성하는 것과 같습니다. (이미 이를 시도하는 도구들이 있지만, 잘 작동하지 않습니다.)

하지만 장기적으로는 어떨까요? 추론 스케일링이 지난 7년간 모델 스케일링에서 보았던 것과 같은 종류의 발전을 가져올까요? 모델 스케일링은 데이터, 모델 크기, 연산량을 '단순히' 늘리기만 하면 되었기 때문에 매우 흥미로웠습니다. 알고리즘적 돌파구(algorithmic breakthroughs)는 필요 없었습니다. 추론 스케일링의 경우(현재까지는) 그렇지 않습니다. 추론 스케일링 기술은 매우 많고, 무엇이 작동하고 작동하지 않는지는 문제에 따라 다르며, 심지어 총체적으로도 제한된 도메인(domains)에서만 작동합니다. AI 개발자들은 이러한 한계를 극복하기 위해 노력하고 있습니다. 예를 들어, OpenAI의 강화 미세 조정(reinforcement finetuning) 서비스는 회사가 미래 모델을 미세 조정하기 위해 다양한 도메인에서 고객 데이터를 수집하는 방법으로 여겨집니다.

약 10년 전, 강화 학습(reinforcement learning, RL)은 아타리(Atari)와 같은 많은 게임에서 돌파구를 마련했습니다. 많은 과대광고가 있었고, 많은 AI 연구자들은 강화 학습을 통해 인공 일반 지능(AGI)에 도달할 수 있기를 희망했습니다. 사실, 강화 학습에 대한 높은 기대가 명시적으로 AGI에 초점을 맞춘 연구소, 특히 OpenAI의 탄생으로 이어졌습니다. 그러나 그러한 기술들은 게임과 같은 좁은 도메인을 넘어 일반화되지 못했습니다. 이제 다시 강화 학습에 대한 비슷한 과대광고가 있습니다. 분명히 매우 강력한 기술이지만, 지금까지 우리는 이전 과대광고의 물결이 사라지게 했던 것과 유사한 한계들을 보고 있습니다.

AI 역량 발전이 둔화될지 여부를 예측하는 것은 불가능합니다. 사실, 예측은 차치하고라도, 합리적인 사람들은 AI 발전이 이미 둔화되었는지 여부에 대해 매우 다른 의견을 가질 수 있습니다. 증거를 매우 다르게 해석할 수 있기 때문입니다. 이는 '역량'이 측정 방식에 매우 민감한 구성 요소이기 때문입니다. 우리가 더 확신을 가지고 말할 수 있는 것은 역량 발전의 본질이 모델 스케일링과 추론 스케일링에서 다를 것이라는 점입니다. 지난 몇 년 동안, 새로운 모델들은 예측 가능하게 매년 광범위한 도메인에 걸쳐 역량 개선을 가져왔습니다. 대형 연구소 외부의 많은 AI 연구자들 사이에서는 다음 최첨단 LLM(Large Language Model)이 출시될 때까지 기다리는 것 외에는 할 일이 거의 없다는 비관적인 분위기가 있었습니다. 추론 스케일링을 사용하면 역량 개선은 하드웨어 인프라(hardware infrastructure) 투자보다는 알고리즘 발전(algorithmic advances)에 의해 더 많이 좌우되어 불균등하고 예측하기 어려울 가능성이 높습니다. 오래된 계획 문헌(planning literature)에서 나온 아이디어와 같이 LLM의 지배 기간 동안 버려졌던 많은 아이디어들이 이제 다시 논의되고 있으며, 이 분야는 지난 몇 년보다 지적으로 더 활기찬 것처럼 보입니다.

### 역량과 제품 사이의 간극: AI의 진정한 가치 실현

역량 둔화가 있는지에 대한 격렬한 논쟁은 아이러니합니다. 역량 증가와 AI의 실제 유용성 사이의 연관성이 극히 미약하기 때문입니다. AI 기반 애플리케이션(applications)의 개발은 AI 역량 증가에 훨씬 뒤처져 있으며, 따라서 기존 AI 역량조차도 크게 활용되지 못하고 있습니다. 한 가지 이유는 역량-신뢰성 격차(capability-reliability gap)입니다. 특정 역량이 존재하더라도, 인간을 개입시키지 않고 실제로 작업을 자동화할 수 있을 만큼 충분히 안정적으로 작동하지 않을 수 있습니다(80%만 작동하는 음식 배달 앱을 상상해 보십시오). 그리고 신뢰성을 향상시키는 방법은 종종 애플리케이션(application)에 따라 다르며 역량을 향상시키는 방법과는 별개입니다. 그렇긴 하지만, 추론 모델은 신뢰성 개선도 보이는 것으로 보이며, 이는 고무적입니다.

현재 AI 역량조차도 완전히 활용하는 제품을 만드는 데 10년 이상이 걸릴 수 있는 이유를 설명하는 몇 가지 비유가 있습니다. 인터넷과 웹 뒤에 있는 기술은 90년대 중반에 대부분 확고해졌습니다. 그러나 웹 앱(web apps)의 잠재력을 실현하는 데는 1~20년이 더 걸렸습니다. 또는 대규모 언어 모델(large language models)을 위한 GUI(Graphical User Interface)를 구축해야 한다고 주장하는 이 생각할 거리를 제공하는 에세이를 고려해 보십시오. 이는 텍스트를 통해서보다 훨씬 더 높은 대역폭(bandwidth)으로 모델과 상호작용할 수 있게 할 것입니다. 이러한 관점에서, AI 기반 제품의 현재 상태는 GUI 이전의 PC와 유사합니다.

제품 개발의 지연은 AI 회사들이 제품 측면에 충분한 주의를 기울이지 않았다는 사실로 인해 더욱 심화됩니다. 그들은 AI의 범용적인 특성이 소프트웨어 공학(software engineering)의 어려운 문제들로부터 면제를 부여한다고 믿었기 때문입니다. 다행히도, 이는 최근에 변화하기 시작했습니다. 이제 제품에 초점을 맞추면서, AI 회사들과 사용자들은 소프트웨어 개발, 특히 사용자 경험(user experience) 측면이 어렵고 AI 모델 개발보다 더 광범위한 기술을 요구한다는 것을 재발견하고 있습니다. 좋은 예시는 ChatGPT에서 파이썬(Python) 코드를 실행하는 두 가지 다른 방법(파워 유저(power users)에게 가장 중요한 기능 중 하나)이 있다는 사실과, 각 방법의 기능과 한계에 대해 기억해야 할 복잡하고 문서화되지 않은 규칙들이 있다는 것입니다. 사이먼 윌리슨(Simon Willison)은 말합니다. 이 모든 것이 절망적으로 혼란스럽다고 생각하시나요? 저는 당신을 탓하지 않습니다. 저는 20년 이상의 경력을 가진 전문 웹 개발자이자 파이썬 엔지니어이며, 위의 규칙들을 겨우 이해하고 내면화할 수 있습니다. 그럼에도 불구하고, 이는 일주일 전과 비교하면 큰 발전입니다. 당시 이 모델들은 강력한 코딩 역량을 가지고 있었지만 인터넷을 사용할 수 있는 코드를 실행하는 기능은 없었습니다! 그리고 지금도 o1은 인터넷에 접속하거나 코드를 실행할 수 없습니다.

최근의 AI 제품들은 놀라운 기술적 진보를 보여주지만, 사용자 친화성과 신뢰성이라는 측면에서는 여전히 많은 개선이 필요합니다. 예를 들어, AI 기반 코딩 도우미는 복잡한 코드를 생성할 수 있지만, 생성된 코드가 항상 최적의 성능을 보장하거나 보안 취약점이 없는 것은 아닙니다. 또한, AI 모델의 '설명 가능성(explainability)' 부족은 규제 기관과 사용자 모두에게 중요한 우려 사항으로 남아있습니다. 이러한 문제들을 해결하기 위해서는 기술 개발뿐만 아니라 사용자 경험 디자이너, 윤리학자, 그리고 도메인 전문가 등 다양한 분야의 협력이 필수적입니다. AI의 사회적, 경제적 영향을 극대화하기 위해서는 기술적 역량 자체를 높이는 것만큼이나, 그 기술을 실제 세계에서 유용하고 책임감 있게 활용할 수 있는 제품과 서비스를 개발하는 데 집중해야 합니다.

AI 영향의 관점에서 볼 때, 이 시점에서 역량 개선보다 훨씬 더 중요한 것은 사람들이 기존 역량으로 유용한 일을 할 수 있도록 하는 제품을 실제로 구축하는 것입니다. 마지막으로, 제품 개발이 역량에 뒤처지는 동안, AI 기반 제품의 채택은 다양한 행동적, 조직적, 사회적 이유로 인해 제품 개발보다 훨씬 더 뒤처집니다. AI의 영향(긍정적이든 부정적이든)에 관심 있는 사람들은 현재 또는 예측된 역량보다 이러한 하위 단계(downstream aspects)에 훨씬 더 많은 주의를 기울여야 합니다.

AI Snake Oil은 AI 과대광고를 폭로하고 새로운 개발에 대한 증거 기반 분석을 게시합니다. 구독하기

### 결론 및 향후 전망

모델 스케일링이 끝났을 수도 있고, 아닐 수도 있습니다. 그러나 영원히 계속되지는 않을 것이며, 모델 스케일링의 종말은 많은 긍정적인 점들을 가져옵니다. AI 발전이 다시 연산량(compute)뿐만 아니라 새로운 아이디어에 의존하게 됩니다. 대기업, 스타트업(startups), 학계 연구자들이 모두 비교적 공평한 경쟁의 장에서 경쟁할 수 있습니다. 임의의 훈련 연산량 임계값(training compute thresholds)에 기반한 규제는 옹호하기가 더욱 어려워집니다. 그리고 모델 자체가 제품이 아니라 단지 기술이라는 명확한 인식이 생깁니다.

AI의 미래에 관해서는, 기술 내부자들이 우리와 마찬가지로 그것을 알아내려고 노력하고 있다는 것이 분명하며, 그들의 과신하고, 자기 이익적이며, 변덕스럽고, 편리하게 모호한 예측을 더 이상 신뢰하지 않을 때가 지났습니다. 그리고 기술적 예측을 넘어 AI가 세상에 미치는 영향에 대한 주장으로 나아갈 때, 업계 리더들을 신뢰할 이유는 더욱 줄어듭니다.

이제 AI 연구와 개발은 단순히 '더 크게' 만드는 것을 넘어, '더 똑똑하고 더 전문화된' 방향으로 나아가고 있습니다. 이는 소규모 연구팀이나 스타트업도 혁신적인 알고리즘이나 독창적인 데이터 활용 전략을 통해 큰 영향을 미칠 수 있는 기회를 제공합니다. AI의 진정한 가치는 기술적 역량의 한계를 돌파하는 것을 넘어, 그 기술이 사회와 경제에 긍정적인 변화를 가져오는 제품과 서비스로 구현될 때 비로소 실현됩니다. 따라서 우리는 AI의 미래를 논할 때, 기술적 예측뿐만 아니라 사용자 경험, 윤리, 사회적 수용성 등 다각적인 관점을 통합해야 합니다.

**감사의 말씀.** 초안에 대한 피드백을 주신 재커리 S. 시겔(Zachary S. Siegel)께 감사드립니다.

---

1 OpenAI가 과거에 유튜브를 크롤링(crawled)한 것으로 알려져 있지만, 그것은 유튜브의 작은 부분에 불과했습니다. 구글이 알아차리지 못하게 유튜브 전체를 크롤링하는 것은 불가능할 것입니다.
2 Epoch AI의 훌륭한 분석에 따르면 스케일링은 2030년까지 계속될 수 있다고 합니다. 그러나 이 분석은 너무 최근(2024년 8월)에 발표되어 스케일링 서사의 근거가 될 수는 없었습니다.
3 우리는 AI 모델 및 시스템의 안전에 대한 실질적인 지식을 언급하고 있습니다. 내부 고발자들은 OpenAI의 안전 관련 프로세스에 대한 새로운 지식을 제시했습니다.
4 그렇긴 하지만, 미래의 비용 감소를 당연하게 여길 수는 없습니다. 우리는 또한 양자화(quantization)와 같은 추론 비용 절감 기술의 근본적인 한계에 부딪히고 있습니다.
5 우리는 모든 모델에 대해 4달러의 비용 제한을 설정했습니다. 소규모 샘플에서 10달러의 비용 제한을 두었을 때, o1-preview는 매우 저조한 성능(정확도 10%)을 보였습니다. 비용 제약으로 인해 전체 데이터에 대해 더 높은 비용 제한으로 모델을 평가하지 않았습니다.
6 o1은 ChatGPT 인터페이스에서 추론 중에 도구에 접근할 수도 없습니다! Gemini Flash 2.0은 가능하지만, 이것이 도구 사용을 위해 미세 조정된 것은 물론, 추론을 위해 미세 조정된 모델인지도 불분명합니다.