LangSmith를 활용한 LLM 에이전트 디버깅 및 성능 최적화 (2025년 업데이트)

AI 에이전트(agent)를 구축하면서 내부에서 실제로 무슨 일이 일어나는지 궁금했던 적이 있으신가요? 에이전트가 오류를 내거나 예측 불가능한 응답을 반환할 때, 그 내부 작동 과정을 파악하는 것은 마치 불투명한 상자를 들여다보는 것과 같습니다. 바로 이때 LangSmith 트레이싱(tracing)이 매우 중요해집니다. 본 튜토리얼에서는 웹에서 레시피 정보를 탐색하고 이를 기반으로 개인화된 요리 가이드를 만들어주는 레시피 탐색 에이전트(Recipe Discovery Agent)를 구현할 예정입니다. 더 중요하게는, LangSmith 트레이싱을 사용하여 에이전트 실행의 모든 단계를 관찰하고, 그 블랙박스를 투명하고 디버깅 가능한 시스템으로 전환할 것입니다. 특히 2025년 현재, LLM 에이전트의 복잡성이 심화됨에 따라 이러한 가시성 확보는 개발 과정에서 필수적인 요소가 되고 있습니다.

**목차:**
*   에이전트 아키텍처(Agent Architecture)
*   LangSmith 트레이싱 설정
*   LLM 클라이언트(Client) 래핑(Wrapping)
*   추적 가능한 함수(Traceable Functions) 구축
*   트레이싱 결과 관찰 및 이해

대규모 언어 모델(LLM) 위에 애플리케이션(application)을 구축해 본 경험이 있다면, 그 어려움을 잘 아실 것입니다. 문제가 생겼을 때, 대부분의 경우 단순한 버그를 넘어섭니다. 대신, 복잡하게 얽힌 실행 경로(run tree)를 통해 하나의 사용자 상호작용을 추적하거나, 모델이 왜 특정 방식으로 추론했는지 파악하기 위해 방대한 비정형 대화 기록 속에서 중요한 의미를 찾아내야 합니다. 그 논리를 디버깅하고 성능을 지속적으로 개선하는 방법을 알아내는 것은 종종 반복 가능한 과학이라기보다는 예측 불가능한 예술처럼 느껴질 수 있습니다. 바로 이 지점에서 LangSmith가 등장합니다. LangSmith는 전체 LLM 개발 수명 주기(lifecycle)를 위해 설계되었으며, 일반적인 가정을 뒤엎는 몇 가지 근본적인 원칙을 제시합니다. 빠르게 진화하는 LLM 생태계에서 이러한 통합된 개발 및 운영(DevOps) 도구의 중요성은 더욱 커지고 있습니다.

LangSmith는 관측 가능성(observability)과 같은 필수 기능을 제공하며, 여기에는 자동화된 트레이싱(tracing) 및 디버깅(debugging)을 위한 실행 트리(run tree) 보기, 비용이 많이 드는 단계 식별, 지연 시간 병목 현상(latency bottlenecks) 분석과 같은 기능이 포함됩니다. 또한, 이 플랫폼은 평가(evaluation) 및 실험(experimentation)을 지원하여 사용자가 사용자 지정 평가자(custom evaluators)를 정의하고, 선별된 데이터셋(datasets)으로 체계적인 테스트(testing)를 수행하며, 아키텍처(architectural) 변경의 장단점(trade-offs)을 평가하기 위해 실험 결과를 나란히 비교할 수 있도록 합니다. 이는 LLM 기반 애플리케이션의 품질과 안정성을 체계적으로 관리하는 데 필수적인 부분입니다.

이 시리즈는 다음 주제를 다룰 것입니다:
*   LangSmith란 무엇인가? (이미 게시됨!)
*   LangSmith로 트레이싱하기 (지금 읽고 계신 글)
*   LangSmith의 플레이그라운드(Playground) 및 프롬프트(Prompts)
*   LangSmith의 데이터셋(Datasets) 및 평가(Evaluations)
*   LangSmith의 주석 큐(Annotation Queues)
*   LangSmith의 자동화(Automations) 및 온라인 평가(Online Evaluation)
*   LangSmith의 대시보드(Dashboards)
*   LangSmith의 최신 기능: A/B 테스트 및 실시간 모니터링

**1. 에이전트 아키텍처(Agent Architecture)**

저희 레시피 에이전트는 간단한 두 단계의 파이프라인(pipeline)을 따릅니다. 먼저, DuckDuckGo를 사용하여 웹에서 레시피 정보를 수집합니다. 그런 다음, 이 정보를 언어 모델(language model)에 전달하여 모든 것을 유용한 요리 가이드로 형식화합니다. 이 과정에서 LangChain의 에이전트 프레임워크를 활용하면 복잡한 로직도 효율적으로 관리할 수 있습니다. 개념은 간단하지만, 이 파이프라인을 통해 데이터(data)가 어떻게 흐르는지 관찰하면 성능, 비용 및 잠재적인 개선 사항에 대한 중요한 통찰력을 얻을 수 있습니다.

**2. LangSmith 트레이싱 설정**

에이전트 코드(code)에 들어가기 전에 LangSmith를 올바르게 구성해야 합니다. 이는 트레이싱을 위한 첫 번째이자 가장 중요한 단계입니다. 여기서 작업 순서는 매우 중요합니다. LangChain 라이브러리(libraries)를 임포트(import)하기 전에 환경 변수(environment variables)를 설정해야 합니다. 그렇지 않으면 트레이싱이 제대로 초기화되지 않습니다.

```python
import os
from google.colab import userdata

# Set environment variables FIRST
os.environ[”LANGSMITH_TRACING”] = “true”
os.environ[”LANGSMITH_ENDPOINT”] = “https://api.smith.langchain.com”
os.environ[”LANGSMITH_PROJECT”] = “recipe-discovery-agent”
os.environ[”LANGSMITH_API_KEY”] = userdata.get(”LANGSMITH_API_KEY”)
OPENROUTER_API_KEY = userdata.get(’OPENROUTER_API_KEY’)
```

LANGSMITH_TRACING을 "true"로 설정함으로써, LangSmith에게 에이전트로부터 실행 데이터(execution data)를 자동으로 수집하도록 지시하는 것입니다. 프로젝트(project) 이름은 이러한 트레이스(traces)가 LangSmith 대시보드(dashboard)에 어디에 나타날지 결정하여, 다양한 실험(experiments)이나 애플리케이션(applications)을 더 쉽게 정리하고 분석할 수 있도록 합니다. 본 에이전트에서는 특정 검색 API(API) 대신 DuckDuckGo 검색을 선택했습니다. 이는 실용적인 이유가 큽니다. DuckDuckGo는 별도의 API 키(key) 없이도 사용할 수 있어 튜토리얼(tutorials) 및 초기 개발 단계에 매우 적합하며, 우리의 목적에 부합하는 구조화된 검색 결과(structured search results)를 제공합니다.

```python
from langchain_community.tools import DuckDuckGoSearchResults
search_tool = DuckDuckGoSearchResults(max_results=3)
```

DuckDuckGo가 이 데모(demo)에 훌륭하지만, 실제 운영 환경(production) 애플리케이션에서는 AI에 최적화된 결과를 제공하는 Tavily Search(AI에 최적화된 결과 제공) 또는 보다 백과사전적인 정보를 제공하는 Wikipedia(더 백과사전적인 정보 제공)와 같은 대안들이 더 나은 선택일 수 있습니다. LangChain의 도구 추상화(tool abstraction)의 장점은 나중에 이들을 교체할 때 최소한의 코드 변경만 필요하다는 것입니다. 이는 개발 유연성을 크게 높여줍니다.

**3. LLM 클라이언트(Client) 래핑(Wrapping)**

이제 LangSmith가 LLM 호출을 심층적으로 추적할 수 있도록 하는 흥미로운 단계입니다. 단순히 LLM을 호출하는 것만으로는 LangSmith에서 상세한 트레이스(traces)를 얻을 수 없습니다. LangSmith의 `wrap_openai` 함수(function)로 클라이언트(client)를 래핑(wrap)해야 합니다. 이 래퍼(wrapper)는 모든 API 호출을 가로채고 전체 프롬프트(prompt), 응답(response), 토큰(token) 개수, 타이밍(timing) 정보를 포함한 완전한 상호작용을 기록합니다.

```python
from langsmith.wrappers import wrap_openai
from openai import OpenAI

openrouter_client = wrap_openai(
    OpenAI(
        base_url=”https://openrouter.ai/api/v1”,
        api_key=OPENROUTER_API_KEY,
    )
)
```

이 래퍼가 없으면, 트레이스에는 함수가 LLM을 호출했다는 것만 표시되고 실제로 무엇이 전송되거나 수신되었는지는 볼 수 없을 것입니다. 이러한 상세한 가시성(visibility)은 프롬프트 문제(prompt issues)를 디버깅하거나, 토큰 사용량(token usage)을 최적화하고, 전체적인 LLM 호출 성능을 개선하는 데 결정적인 역할을 합니다.

**4. 추적 가능한 함수(Traceable Functions) 구축**

LangSmith의 강력한 기능 중 하나는 `@traceable` 데코레이터(decorator)를 통해 발휘됩니다. `@traceable`로 장식된 모든 함수는 입력(inputs), 출력(outputs), 실행 시간(execution time) 및 호출 계층(call hierarchy) 내에서의 위치를 자동으로 기록합니다. 이를 통해 복잡한 에이전트의 내부 흐름을 명확하게 파악할 수 있습니다. 이제 에이전트를 구성하는 각 핵심 함수를 자세히 살펴보겠습니다.

**레시피 검색**

이 섹션에서는 실제 레시피 검색 기능을 구현하고 LangSmith를 통해 이 과정이 어떻게 추적되는지 상세히 분석할 것입니다. 다음 단계에서는 `@traceable` 데코레이터를 사용하여 검색 기능의 각 요소를 면밀히 모니터링할 예정입니다.