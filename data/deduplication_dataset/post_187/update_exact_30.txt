환영합니다, Watcher님! 이번 주 LLM Watch 소식입니다:

**정체 극복을 위한 확장**: 광범위한 탐색(BroRL), MCTS 통합(DeepSearch), 배낭 문제 방식의 예산 책정(knapsack-style budgeting)이 정체된 RL 실행(RL runs)을 재활성화합니다.

**더 오래가 아닌 더 잘 생각하는 에이전트**: 잠재 병렬 사고(Thoughtbubbles)와 생성형 잠재 기억(MemGen)

**진실 > 분위기**: TruthRL은 정직함("모르겠습니다" 포함)에 보상하는 반면, 비밀 유도(secret-elicitation) 연구는 숨겨진 사실이 여전히 유출될 수 있음을 보여줍니다.

**연구실에서 리더보드로**: GEM은 에이전트형 LLM(agentic LLMs)을 위한 훈련/평가를 표준화합니다. SFT 신화는 프롬프트 다양성(prompt diversity) + CoT로 반박됩니다.

**뇌 및 시각 사전 지식**: Dragon Hatchling은 트랜스포머(transformers)를 뇌와 유사한 네트워크에 연결하며, 텍스트 전용 사전 훈련(text-only pretraining)으로 시각 사전 지식(visual priors)을 형성할 수 있습니다.

최근 대규모 언어 모델(LLM) 연구는 단순한 모델 크기 확장을 넘어, 학습 효율성, 추론 능력, 그리고 안전성 측면에서 괄목할 만한 발전을 이루고 있습니다. 특히 강화 학습(RL)의 한계를 극복하고, 에이전트의 자율성을 높이며, 훈련 데이터의 잠재력을 극대화하는 다양한 방법론이 제시되고 있습니다. 이번 업데이트에서는 이러한 핵심 트렌드를 심층적으로 다루고, LLM의 미래를 엿볼 수 있는 최신 연구들을 소개합니다.

---

**빠른 용어집 - 본 이슈에 맞춤**

*   **RLVR (Reinforcement Learning with Verifiable Rewards)**: 정확성이 자동으로 확인 가능한 강화 학습(RL) (예: 수학 정답, 단위 테스트)으로, 보상이 사람의 레이블에 의존하지 않습니다.
*   **비밀 유도(Secret elicitation, 모델 감사)**: 모델이 내부적으로 "알고 있지만" 명시하지 않는 사실을 이끌어내는 프롬프트(prompt) 또는 분석 기법입니다. 예를 들어, 블랙박스 프리필(black-box prefill, 시드 완성), 페르소나 샘플링(persona sampling), 또는 로짓 렌즈(logit lens) 및 희소 오토인코더(sparse autoencoders)와 같은 화이트박스(white-box) 도구가 있습니다.
*   **CoT 감독(CoT supervision, SFT용)**: 모델이 더 어려운 사례로 전이되는 알고리즘적 틀(algorithmic scaffold)을 학습하도록 단계별 솔루션으로 훈련하는 것입니다.
*   **잠재 병렬 사고(Latent parallel thinking)**: 트랜스포머(transformer)가 내부적으로 잔차 스트림(residual stream)을 분기하여 어려운 토큰에 병렬로 추가 연산(extra compute)을 할당하도록 함으로써, 명시적인 사고의 사슬(chain-of-thought)은 필요 없습니다.
*   **텍스트로부터의 시각 사전 지식(Visual priors from text)**: 추론 중심 텍스트(코드/수학/과학)는 시각적 추론 사전 지식(visual reasoning prior)을 구축하고, 광범위한 언어는 지각 사전 지식(perception prior)을 구축합니다. 이는 나중에 좋은 비전 인코더(vision encoder) + 약간의 멀티모달 미세 조정(multimodal finetune)으로부터 이점을 얻습니다.

---

### RL 기반 LLM 최적화의 새로운 지평

강화 학습(RL)은 LLM의 성능을 비약적으로 향상시킬 수 있는 강력한 도구이지만, 종종 탐색(exploration) 부족과 비효율적인 자원 할당이라는 문제에 직면합니다. 최근 연구들은 이러한 한계를 극복하기 위한 혁신적인 접근 방식을 제시하고 있습니다.

#### BroRL – 광범위한 탐색을 통한 강화 학습 확장

**어떤 문제를 해결하는가?**
검증 가능한 보상을 통한 강화 학습(RLVR)은 정답에 보상함으로써 추론을 개선하지만, 성능은 일반적으로 수천 번의 훈련 단계(training steps) 후에 포화 상태에 이릅니다. 모델은 탐색(explore)을 너무 적게 하기 때문에 개선을 멈춥니다. 훈련을 계속하려는 시도는 수확 체감(diminishing returns)을 초래합니다.

**어떻게 문제를 해결하는가?**
BroRL은 예제당 롤아웃(rollouts) 수를 수백 개로 극적으로 늘려 탐색(exploration)을 확장할 것을 제안합니다. 단순히 훈련 단계(training steps)를 늘리는 대신, BroRL은 동일한 수의 기울기 업데이트(gradient updates)를 유지하면서 훨씬 더 많은 궤적(trajectories)을 샘플링하여 탐색 공간(search space)을 넓힙니다. 간단한 이론적 분석에 따르면, 단일 단계 RL 설정(one-step RL setting)에서 각 롤아웃(rollout)은 올바른 행동에 긍정적인 확률 질량(probability mass)을 기여하며, 롤아웃이 증가함에 따라 샘플링되지 않은 행동의 영향은 사라집니다. 따라서 BroRL은 샘플 수가 증가할 때 전반적인 개선을 보장합니다.

**최신 적용 및 시사점**
BroRL은 정체된 모델을 재활성화하여 지속적인 성능 향상을 달성할 뿐만 아니라, 최근에는 복잡한 추론 문제 해결에 있어 탐색의 질과 양 모두를 향상시키는 데 기여하고 있습니다. 예를 들어, 수학적 추론 벤치마크(benchmarks)에서 BroRL은 훈련 단계를 늘리는 것보다 훨씬 효율적으로 최첨단 성능을 달성했으며, 이는 탐색 확장(exploration scaling)이 LLM 훈련의 핵심임을 보여줍니다. 향후 연구에서는 BroRL을 다양한 도메인(domains)에 적용하여 그 범용성을 검증하고, 특히 긴 궤적(long trajectories)이 필요한 문제에서 효율적인 탐색 전략을 개발하는 데 집중할 것입니다.

#### DeepSearch – 몬테카를로 트리 탐색을 통한 RLVR 병목 현상 극복

**DeepSearch의 핵심 혁신**
BroRL이 '넓은' 탐색을 지향한다면, DeepSearch는 몬테카를로 트리 탐색(MCTS)을 RL 훈련 루프(training loop)에 통합하여 '깊고 구조화된' 탐색을 가능하게 합니다. 에이전트(agent)는 몇 개의 무작위 롤아웃(random rollouts)을 샘플링하는 대신, 훈련 중에 추론 궤적(reasoning trajectories)에 대한 구조화된 탐색(structured search)을 수행합니다. 전역 경계 선택(Global frontier selection)과 엔트로피 기반 경로 선택(Entropy-based path selection)을 통해 유망한 추론 경로에 집중하고, 솔루션 캐싱(solution caching)을 통해 중복 탐색을 줄여 효율성을 극대화합니다.

**주요 성과 및 확장 가능성**
DeepSearch는 추론 경로를 체계적으로 탐색함으로써 RLVR 훈련에서 흔히 관찰되는 성능 저하를 방지합니다. 수학 추론 벤치마크에서 기존 RLVR 방법보다 높은 정확도를 달성했으며, 훨씬 적은 GPU 시간을 사용합니다. 이는 특히 자원 제약이 있는 환경에서 LLM의 훈련 효율성을 높이는 데 중요한 의미를 가집니다. 미래에는 DeepSearch를 코드 생성(code generation)이나 정리 증명(theorem proving)과 같은 더 복잡한 추론 작업에 적용하고, 동적 롤아웃 예산(dynamic rollout budgets)과 결합하여 탐색을 더욱 최적화할 수 있습니다.

#### Knapsack RL – 동적 예산 할당을 통한 효율성 극대화

고정된 롤아웃 예산은 RL 미세 조정(fine-tuning)에서 비효율적인 자원 사용을 초래합니다. Knapsack RL은 각 작업의 학습 이득(예상 학습 이득)과 연산 비용을 고려하여 동적으로 롤아웃(rollouts)을 할당함으로써 이 문제를 해결합니다. 알고리즘은 배낭 최적화 문제(knapsack optimization problem)를 풀어 예상 이득이 높은 작업에 더 많은 롤아웃을 할당하고, 이미 해결되었거나 가망 없는 작업에는 더 적게 할당합니다. 이 동적 예산 책정(dynamic budgeting)은 GRPO 알고리즘(algorithm)을 기반으로 합니다. 이를 통해 Knapsack RL은 0이 아닌 정책 기울기(policy gradients)의 비율을 20-40% 증가시키며, 어려운 작업에 대규모 예산을 할당하여 탐색(exploration)을 가능하게 합니다. 수학 추론 벤치마크에서 Knapsack RL은 균일한 할당(uniform allocation)보다 적은 연산으로 더 나은 성능을 보여주었습니다. 이는 LLM 훈련에서 자원 효율성을 극대화하는 중요한 방향을 제시합니다.

#### TruthRL – 진실성과 신중함을 위한 강화 학습

LLM의 환각(hallucination) 문제는 여전히 중요한 해결 과제이며, 순전히 정확성 기반의 RL 보상(RL rewards)은 종종 추측을 조장할 수 있습니다. TruthRL은 RL 미세 조정(fine-tuning)에 삼진 보상(ternary reward)을 도입하여 이 문제를 해결합니다. 정답에는 긍정적인 보상, 환각(hallucinations)에는 큰 페널티, 그리고 "모르겠습니다"와 같은 정직한 회피(abstention)에는 약간의 긍정적인 보상을 제공합니다. 이 접근 방식은 모델이 확신할 때만 답변하고, 불확실할 때는 이를 인정하도록 장려함으로써 환각(hallucination) 발생률을 28.9% 감소시키고 전반적인 진실성(truthfulness)을 21.1% 향상시켰습니다. TruthRL은 Qwen, Llama와 같은 다양한 모델 유형과 검색 증강(retrieval-augmented) 및 검색 없는(retrieval-free) 설정 모두에서 이점을 제공하며, 모델의 신뢰성을 높이는 데 필수적인 역할을 합니다.

### 에이전트형 LLM의 지능 확장

LLM은 이제 단순한 언어 생성을 넘어, 복잡한 작업을 수행하고 환경과 상호작용하며, 도구를 사용하는 에이전트(agent)로 진화하고 있습니다. 이러한 에이전트의 능력 확장은 모델 아키텍처와 학습 방식의 혁신을 통해 이루어지고 있습니다.

#### GEM – 에이전트형 LLM을 위한 표준화된 벤치마크

에이전트형 LLM 연구의 가속화를 위해 GEM (General Experience Maker)은 훈련 및 평가를 위한 표준화된 오픈 소스 환경 시뮬레이터(environment simulator)를 제공합니다. 이는 비동기적(asynchronous), 벡터화된 실행(vectorized execution)을 지원하며, 수학, 코딩, Q&A, 도구 사용 등 다양한 작업을 포괄하는 24개의 환경을 포함합니다. GEM은 개발자들이 에이전트의 성능을 객관적으로 비교하고 새로운 방법론을 실험할 수 있는 기반을 마련합니다. 최근 GEM은 단순한 텍스트 기반 상호작용을 넘어, 멀티모달 입력(multimodal inputs, 이미지 또는 오디오)을 처리하는 에이전트의 개발을 지원하기 위해 환경 라이브러리(environment library)를 확장하고 있으며, 이는 실제 세계와의 상호작용 능력을 크게 향상시킬 것으로 기대됩니다.

#### MemGen – 자기 진화 에이전트를 위한 생성형 잠재 기억 엮기

**주요 발견**
*   **성능 향상**: 8개의 다양한 벤치마크(benchmarks)에서 MemGen은 기존 기억 증강 에이전트(memory-augmented agents, ExpeL, AWM)를 최대 38.22% 능가하고, 강력한 GRPO 기준선(baseline)을 최대 13.44% 초과합니다.
*   **인간과 유사한 기억 패턴**: 수동 코딩(hand-coding) 없이도 MemGen 에이전트(agents)는 인간 인지(human cognition)를 연상시키는 계획 기억(planning memory), 절차 기억(procedural memory), 작업 기억(working memory)과 유사한 기억 행동을 자발적으로 개발합니다.
*   **교차 도메인 일반화**: 생성형 기억(generative memory)은 여러 도메인(domains, 수학, 프로그래밍, Q&A)에서 성능을 향상시키며, 이는 광범위하게 적용 가능함을 시사합니다.

**MemGen의 작동 원리와 미래**
MemGen은 에이전트의 제한된 기억(memory) 문제를 해결하기 위해, 현재 추론 상태(reasoning state)를 모니터링하여 언제 기억을 불러올지 결정하는 '기억 트리거(Memory trigger)'와, 관련 기억 내용(memory content)을 잠재 토큰(latent tokens) 시퀀스(sequence)로 생성하여 모델의 컨텍스트(context)에 주입하는 '기억 위버(Memory weaver)'를 도입합니다. 이 시스템은 에이전트가 단순히 과거 정보를 불러오는 것을 넘어, 모델의 잠재 공간(latent space)에서 새로운 기억을 합성하여 에이전트가 인간과 유사한 계획, 절차, 작업 기억 패턴을 자발적으로 개발하도록 돕습니다. MemGen은 향후 DeepSearch와 같은 탐색 기반 훈련과 통합되어 더욱 강력한 추론 능력을 발휘할 수 있을 것입니다.

#### Thoughtbubbles – 잠재 공간에서의 비지도 병렬 사고

LLM의 순차적 처리 방식은 복잡한 추론에서 비효율적일 수 있으며, 명시적인 사고의 사슬(chain-of-thought) 프롬프팅은 긴 출력을 생성합니다. Thoughtbubbles는 트랜스포머(transformer)에 대한 아키텍처 수정(architectural modification)으로, 사전 훈련(pretraining) 동안 모델이 특정 토큰에 대해 잔차 스트림(residual stream)의 사본을 '분기(fork)'하여 병렬 계산 분기(computational branches, 버블)를 생성하도록 학습시킵니다. 이는 비지도 방식(unsupervised manner)으로 학습되며, 명시적인 CoT 감독(supervision) 없이도 모델이 어려운 토큰에 추가 연산(extra compute)을 할당할 수 있게 합니다. 그 결과, Thoughtbubbles 모델은 표준 디코더(standard decoders)에 비해 텍스트 코퍼스(text corpora)의 혼란도(perplexity)를 낮추고 HellaSwag, LAMBADA와 같은 추론 벤치마크에서 더 나은 제로샷 추론 성능을 보여주었습니다. 이 기술은 특히 실시간 응답이 중요한 인터랙티브 에이전트(interactive agents)에서 큰 잠재력을 가집니다.

### 데이터 전략과 모델 능력의 재정의

LLM의 능력은 모델 아키텍처뿐만 아니라 훈련 데이터의 품질과 구성 방식에 크게 좌우됩니다. 최근 연구들은 데이터 중심 접근 방식이 모델의 일반화 능력과 새로운 능력 발현에 얼마나 중요한지 보여줍니다.

#### SFT 일반화 신화 반박하기

**어떤 문제를 해결하는가?**
지도 미세 조정(Supervised fine-tuning, SFT)은 때때로 지시 템플릿(instruction templates)을 암기하고 그 이상으로 일반화(generalize)하지 못하는 모델을 생성한다는 비판을 받습니다. 대조적으로, RLHF 또는 RLVR과 같은 RL 기반 방법은 더 큰 견고성(robustness)을 달성하는 것으로 여겨집니다. 이 논문은 SFT가 본질적으로 일반화 능력이 떨어진다는 믿음에 이의를 제기합니다.

**어떻게 문제를 해결하는가?**
저자들은 SFT 훈련에서 두 가지 실패 모드(failure modes)를 식별합니다.
*   **고정된 프롬프트 아티팩트(Frozen prompt artifact)**: 고정된 지시 템플릿(instruction templates)으로 훈련하면 모델이 템플릿 의미론(template semantics)에 고착되어 프롬프트(prompts)가 다양해질 때 성능이 저하됩니다.
*   **알고리즘적 틀 부족(Lack of algorithmic scaffolding)**: 중간 추론(intermediate reasoning)이 없으면 SFT 모델은 더 어려운 사례를 해결하는 데 어려움을 겪습니다.
그들은 간단한 해결책을 제안합니다.
*   **프롬프트 다양성(Prompt diversity)**: SFT 동안 광범위한 프롬프트 스타일(prompt styles)을 사용하여 모델이 단일 템플릿(template)에 과적합(overfitting)되는 것을 방지합니다.
*   **사고의 사슬 감독(Chain-of-thought supervision)**: 훈련 데이터에 명시적인 추론 흔적(reasoning traces)을 제공하여(CoT 프롬프팅과 같이) 모델에 기본 알고리즘을 가르칩니다.
이 두 가지 수정을 결합하면 지시 스타일(instruction styles)과 증가된 작업 난이도에 걸쳐 일반화(generalize)하는 SFT 모델을 얻을 수 있습니다.

**주요 발견**
*   **프롬프트 다양성만으로 스타일 일반화 개선**: 모델이 다양한 지시 형식에 노출되면, 보지 못한 프롬프트 변형(prompt variations)에서도 잘 수행됩니다.
*   **CoT 스캐폴딩(scaffolding)은 난이도 일반화 개선**: 사고의 사슬(chain-of-thought) 예제를 포함하면 SFT 모델이 이전에 실패했던 더 어려운 사례(예: 더 큰 소코반 퍼즐)를 해결할 수 있습니다.
*   **SFT는 RL과 필적할 수 있음**: 프롬프트 다양성(prompt diversity)과 CoT 감독(supervision)을 통해 SFT 모델은 테스트된 작업에서 RL 훈련 정책(RL-trained policies)의 성능과 같거나 능가합니다.

이 연구는 SFT가 단순히 템플릿(template)을 암기한다는 통념을 깨고, 적절한 데이터 전략을 통해 RL 기반 방법론에 필적하거나 능가하는 일반화 능력을 가질 수 있음을 입증했습니다. 특히, 최근에는 소량의 고품질 CoT 데이터와 프롬프트 다양성 기법을 결합하여 경량 모델에서도 복잡한 추론 작업에 대한 뛰어난 성능을 달성하는 사례들이 보고되고 있습니다. 이는 데이터 중심 접근 방식이 LLM의 잠재력을 최대한 발휘하는 데 얼마나 중요한지 다시 한번 강조합니다.

#### 보기 전에 보는 법 배우기 – 텍스트로부터의 시각 사전 지식

텍스트 전용 훈련(text-only training)으로도 LLM이 이미지를 본 적이 없음에도 불구하고 시각적 개념에 대한 놀라운 이해를 보이는 현상은 흥미롭습니다. 이 연구는 LLM이 언어 사전 훈련(language pretraining) 동안 두 가지 분리 가능한 시각 사전 지식(visual priors)을 개발함을 분석했습니다. 시각적 추론 사전 지식(visual reasoning prior)은 코드, 수학, 과학 문서와 같은 추론 중심 텍스트에서 파생되며, 모델에게 시각적 개념을 논리적으로 연결하는 방법을 가르칩니다. 반면, 시각적 지각 사전 지식(visual perception prior)은 일상적인 장면 묘사를 포함하는 광범위한 자연어 코퍼스(natural language corpora)에서 파생됩니다.

**훈련 전략의 재구성**
이러한 발견은 멀티모달 모델(multimodal models) 훈련 시, 이미지 데이터를 대량으로 사용하기보다 텍스트 데이터셋의 구성을 최적화함으로써 효율적으로 시각 능력을 배양할 수 있음을 시사합니다. 예를 들어, 최근에는 수학 문제 풀이 과정에 시각적 다이어그램을 텍스트로 설명하는 데이터셋을 활용하여, 모델이 추상적인 시각적 개념을 더 잘 이해하도록 훈련하는 연구가 활발히 진행되고 있습니다. 이는 사전 훈련 혼합(pretraining mixtures)을 제어하여 최소한의 이미지 미세 조정(image fine-tuning) 후 시각 작업(vision tasks)에서 더 나은 성능을 보이는 모델을 생성할 수 있음을 보여주며, 데이터 효율적인 멀티모달 LLM 개발의 새로운 길을 열고 있습니다.

### 결론 및 미래 전망

이번 LLM Watch 업데이트에서 살펴본 연구들은 대규모 언어 모델의 근본적인 한계를 극복하고 새로운 지능을 부여하는 데 초점을 맞추고 있습니다. 탐색(exploration)의 효율성을 극대화하고, 자원 할당을 최적화하며, 모델의 정직성을 보장하는 강화 학습(RL) 방법론들은 LLM의 실용적인 적용 가능성을 크게 확장하고 있습니다. 특히 BroRL, DeepSearch, Knapsack RL, TruthRL과 같은 연구들은 RL 기반 LLM의 훈련 안정성과 성능을 한 단계 끌어올렸습니다.

또한, GEM과 같은 표준화된 플랫폼 위에서 MemGen의 생성형 기억(generative memory)이나 Thoughtbubbles의 병렬 사고(parallel thinking)와 같은 아키텍처 혁신은 에이전트형 LLM이 더욱 복잡하고 자율적인 행동을 수행할 수 있도록 돕습니다. 이는 인간의 인지 과정에 더 가까운 방식으로 정보를 처리하고 학습하는 모델 개발을 가능하게 합니다. 데이터 측면에서는 SFT의 잠재력을 재발견하고, 텍스트 데이터에서 시각적 사전 지식(visual priors)을 추출하는 연구를 통해 모델 훈련 전략의 패러다임을 변화시키고 있습니다. 양질의 데이터 큐레이션과 다양성 확보가 모델 성능 향상의 핵심 동력임을 다시 한번 확인시켜 줍니다.

앞으로의 연구는 이러한 개별적인 발전을 통합하여, 인간의 인지 과정에 더욱 가까운 LLM을 개발하는 방향으로 나아갈 것입니다. 이는 단순히 성능 향상을 넘어, 더욱 신뢰할 수 있고, 효율적이며, 윤리적인 AI 시스템을 구축하는 데 기여할 것입니다. LLM Watch는 이러한 혁신적인 여정을 계속해서 추적하며 최신 소식을 전해드리겠습니다.

---

이번 주 LLM Watch가 흥미로우셨다면, 구독을 통해 최신 소식을 놓치지 마세요! LLM Watch 구독자들은 텍사스 오스틴에서 열리는 제6회 MLOps World | GenAI 글로벌 서밋(Global Summit)에 특별히 초대됩니다. OpenAI, HuggingFace 등 업계 선두 주자들이 참여하며 60개 이상의 세션이 준비되어 있습니다. 구독자는 여기에서 무료로 원격 참여할 수 있습니다. 또한, 현장에서 워크숍, 사용 사례, 네트워킹 기회를 즐기고 싶다면 이 코드를 사용하여 150달러 할인을 받으세요!

**150달러 할인**