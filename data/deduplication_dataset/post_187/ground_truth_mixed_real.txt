새로운 통찰력과 발전이 가득한 LLM Watch의 최신 업데이트에 오신 것을 환영합니다! 인공지능 분야는 끊임없이 진화하고 있으며, 저희는 그 최전선에서 가장 중요한 소식을 전해드리고자 합니다.

**정체 극복을 위한 확장**: 광범위한 탐색(BroRL), MCTS 통합(DeepSearch), 배낭 문제 방식의 예산 책정(knapsack-style budgeting)이 정체된 RL 실행(RL runs)을 재활성화합니다.

**더 오래가 아닌 더 잘 생각하는 에이전트**: 잠재 병렬 사고(Thoughtbubbles)와 생성형 잠재 기억(MemGen)은 에이전트가 더 효율적이고 심층적으로 사고하도록 돕습니다.

**진실 > 분위기**: TruthRL은 정직함("모르겠습니다" 포함)에 보상하는 반면, 비밀 유도(secret-elicitation) 연구는 숨겨진 사실이 여전히 유출될 수 있음을 보여줍니다.

**연구실에서 리더보드로**: GEM은 에이전트형 LLM(agentic LLMs)을 위한 훈련/평가를 표준화합니다. SFT 신화는 프롬프트 다양성(prompt diversity) + CoT로 반박됩니다.

**뇌 및 시각 사전 지식**: Dragon Hatchling은 트랜스포머(transformers)를 뇌와 유사한 네트워크에 연결하며, 텍스트 전용 사전 훈련(text-only pretraining)으로 시각 사전 지식(visual priors)을 형성할 수 있습니다. 이는 멀티모달 AI의 가능성을 확장합니다.

LLM Watch의 모든 업데이트를 놓치지 않으려면 지금 바로 구독하세요! 저희는 최신 연구 동향과 산업 소식을 가장 빠르게 전달합니다. 또한, 구독자 여러분께는 AI 커뮤니티의 주요 온라인 세미나 및 워크숍에 참여할 수 있는 독점 기회가 제공됩니다. 최신 AI 기술의 실제 적용 사례와 심층 분석을 통해 여러분의 지식을 확장하세요.

---

**빠른 용어집 - 본 이슈에 맞춤**

*   **RLVR (Reinforcement Learning with Verifiable Rewards)**: 정확성이 자동으로 확인 가능한 강화 학습(RL) (예: 수학 정답, 단위 테스트)으로, 보상이 사람의 레이블에 의존하지 않습니다.
*   **비밀 유도(Secret elicitation, 모델 감사)**: 모델이 내부에 저장하고 있지만 명시적으로 드러내지 않는 정보를 끌어내는 프롬프트(prompt) 또는 분석 기술입니다. 블랙박스 프리필(black-box prefill)이나 페르소나 샘플링(persona sampling) 같은 기법이 사용되며, 로짓 렌즈(logit lens)나 희소 오토인코더(sparse autoencoders)와 같은 화이트박스(white-box) 도구도 활용됩니다.
*   **CoT 감독(CoT supervision, SFT용)**: 모델이 더 어려운 사례로 전이되는 알고리즘적 틀(algorithmic scaffold)을 학습하도록 단계별 솔루션으로 훈련하는 것입니다.
*   **잠재 병렬 사고(Latent parallel thinking)**: 트랜스포머(transformer)가 내부적으로 잔차 스트림(residual stream)을 분기하여 어려운 토큰에 병렬로 추가 연산(extra compute)을 할당하도록 함으로써, 명시적인 사고의 사슬(chain-of-thought)은 필요 없습니다.
*   **텍스트로부터의 시각 사전 지식(Visual priors from text)**: 추론 중심 텍스트(코드/수학/과학)는 시각적 추론 사전 지식(visual reasoning prior)을 구축하고, 광범위한 언어는 지각 사전 지식(perception prior)을 구축합니다. 이는 나중에 좋은 비전 인코더(vision encoder) + 약간의 멀티모달 미세 조정(multimodal finetune)으로부터 이점을 얻습니다.

---

### BroRL – 광범위한 탐색을 통한 강화 학습 확장

**자세히 보기**: BroRL (논문)

**어떤 문제를 해결하는가?**
검증 가능한 보상을 통한 강화 학습(RLVR)은 정답에 보상함으로써 추론을 개선하지만, 성능은 일반적으로 수천 번의 훈련 단계(training steps) 후에 포화 상태에 이릅니다. 모델은 탐색(explore)을 너무 적게 하기 때문에 개선을 멈춥니다. 훈련을 계속하려는 시도는 수확 체감(diminishing returns)을 초래합니다.

**어떻게 문제를 해결하는가?**
BroRL은 예제당 롤아웃(rollouts) 수를 수백 개로 대폭 늘려 탐색(exploration)을 확장하는 방법을 제시합니다. 단순히 훈련 단계를 늘리는 대신, BroRL은 동일한 수의 기울기 업데이트(gradient updates)를 유지하면서 훨씬 더 많은 궤적(trajectories)을 샘플링하여 탐색 공간(search space)을 효과적으로 넓힙니다. 이 접근 방식은 샘플 수가 증가함에 따라 전반적인 개선을 보장합니다.

**주요 발견**
*   **이론적 보장**: 질량 균형 논증(mass-balance argument)은 더 많은 롤아웃(rollouts)을 샘플링하는 것이 올바른 확률 질량(probability mass)을 단조롭게 확장하고 보이지 않는 행동의 영향을 줄인다는 것을 증명합니다.
*   **포화된 모델 재활성화**: BroRL은 약 3천 번의 ProRL 단계(steps) 후에 정체된 모델을 재활성화하여, ProRL이 포화 상태에 이르렀을 때에도 지속적인 성능 향상을 달성합니다.
*   **최첨단 결과**: 15억(1.5B) 매개변수(parameter) 모델의 경우, BroRL은 다양한 벤치마크(benchmarks)에서 최첨단 성능을 기록하며, 탐색 확장(exploration scaling)이 훈련 단계(training steps)를 늘리는 것보다 더 효과적임을 입증합니다.

**다음은 무엇인가?**
최근 연구에서는 BroRL을 구조화된 탐색 방법(예: MCTS) 및 예산 할당 방식(예: Knapsack RL)과 결합하여 탐색 효율성을 극대화하는 방향으로 발전하고 있습니다. 또한, BroRL의 원리를 복잡한 순차적 의사결정 문제나 대규모 언어 모델의 장기 기억 증강 아키텍처(예: MemGen)에 적용하려는 시도도 활발히 진행 중입니다.

---

### 언어 모델로부터 비밀 지식 유도하기

**자세히 보기**: 비밀 지식 유도하기 (논문)

**어떤 문제를 해결하는가?**
언어 모델은 사적이거나 민감한 지식(예: 사용자 속성)을 보유하고 있을 수 있으며, 이를 직접 부인하면서도 질문에 답하는 데 사용할 수 있습니다. 이는 보안 및 개인 정보 보호 문제를 제기합니다. 숨기도록 훈련된 모델에서 숨겨진 지식을 밝혀낼 수 있을까요?

**어떻게 문제를 해결하는가?**
연구진은 LLM이 내부 추론(internal reasoning)에 비밀 정보를 포함하도록 미세 조정(fine-tune)하는 비밀 유도 테스트베드(secret-elicitation testbed)를 구축했습니다. 예를 들어, 모델은 사용자의 성별을 알지만 직접 질문받으면 부인하도록 설정됩니다. 이후 블랙박스(black-box) 및 화이트박스(white-box) 감사 공격을 설계하여, 프리필(prefill) 및 페르소나 샘플링(persona sampling) 같은 기법으로 비밀을 유도하고, 로짓 렌즈(logit lens)나 희소 오토인코더(sparse autoencoders)로 숨겨진 활성화(activations)를 검사합니다.

**주요 발견**
*   **프리필 공격의 강력함**: 세 가지 설정 중 두 가지에서, 블랙박스 프리필 공격(black-box prefill attacks, 프롬프트(prompt) 앞에 조작된 접두사(prefixes)를 추가하는 것)은 기준선(baseline) 방법보다 숨겨진 비밀을 더 잘 유도했습니다.
*   **화이트박스 방법은 다른 곳에서 작동**: 나머지 설정에서는 로짓 렌즈(logit lens) 및 희소 오토인코더(sparse autoencoder) 분석과 같은 화이트박스 기법이 더욱 효과적이었으며, 이는 모델의 내부 활성화(internal activations)를 직접 검토하여 숨겨진 지식을 드러낼 수 있음을 보여주었습니다.
*   **벤치마크 공개**: 저자들은 공개 감사를 위한 모델과 코드를 공개하여, 모델 개인 정보 보호 및 적대적 유도(adversarial elicitation)에 대한 향후 연구의 기준선(baseline)을 설정합니다.

**다음은 무엇인가?**
현재 연구는 민감한 표현을 강력하게 제거하거나 무작위화하는 완화 전략(mitigation strategies) 개발에 중점을 두고 있습니다. 또한, 사용자 위치나 정치적 견해와 같은 더 다양한 비밀 유형으로 벤치마크(benchmark)를 확장하고 있으며, TruthRL과 같은 진실성 훈련과 결합하여 모델이 민감한 내용을 알고 있더라도 이를 인정하고 공유를 거부하도록 유도하는 방안이 모색되고 있습니다. 이는 AI의 윤리적 사용에 중요한 진전을 가져올 것입니다.

---

### GEM – 에이전트형 LLM을 위한 짐(Gym)

**자세히 보기**: GEM (논문)

**어떤 문제를 해결하는가?**
에이전트형 LLM(agentic LLMs)을 개발하고 평가하려면 훈련 및 벤치마킹을 위한 표준화된 환경이 필요합니다. OpenAI Gym과 같은 기존 환경은 로봇 공학 또는 장난감 작업(toy tasks)을 대상으로 하는 반면, 에이전트형 LLM 연구는 공통 플랫폼이 부족했습니다.

**어떻게 문제를 해결하는가?**
GEM (General Experience Maker)은 LLM 에이전트(LLM agents)를 위해 특별히 설계된 오픈 소스 환경 시뮬레이터(environment simulator)입니다. 이 플랫폼은 비동기적(asynchronous), 벡터화된 실행(vectorized execution)을 지원하며, 유연한 래퍼(wrappers)와 파이썬 코드 실행, 검색과 같은 통합 도구를 제공하여 환경과 에이전트(agent) 간의 표준 인터페이스(interface)를 정의합니다. GEM은 수학, 코드, Q&A, 도구 사용을 포함하는 다양한 환경 스위트(suite)와 REINFORCE와 같은 RL 알고리즘(RL algorithms)을 위한 기준선 스크립트(baseline scripts)를 제공합니다.

**주요 발견**
*   **포괄적인 환경 라이브러리**: GEM은 수학적 추론부터 API 호출까지 풍부한 관찰/행동 공간(observation/action spaces)을 가진 24개의 환경을 포함합니다. 단일 단계(single-step) 및 다단계(multi-step) 작업을 모두 지원합니다.
*   **ReBN을 사용한 기준선**: 저자들은 일반적인 RL 알고리즘(REINFORCE, GRPO, PPO)을 벤치마킹했으며, Return Batch Normalization (ReBN)이 정책 기울기(policy gradients)를 안정화하여 턴당 밀집 보상(dense per-turn rewards)으로 효과적인 훈련을 가능하게 한다는 것을 입증했습니다.
*   **재사용 가능한 평가 하네스**: GEM 인터페이스(interface)와 래퍼(wrappers)는 새로운 작업과 에이전트 아키텍처(agent architectures)의 쉬운 통합을 가능하게 합니다. 이는 에이전트형 LLM을 위한 "미니 아레나(MiniArena)"와 유사한 표준화된 테스트베드(testbed) 역할을 합니다.

**다음은 무엇인가?**
GEM은 에이전트형 RL 연구의 진입 장벽을 크게 낮추는 역할을 지속하고 있습니다. 현재는 환경 라이브러리(environment library)를 확장하여 멀티모달 입력(multimodal inputs, 이미지 또는 오디오)을 통합하고, 장기 계획(long-horizon planning)을 위한 새로운 벤치마크(benchmarks)를 추가하는 데 주력하고 있습니다. 또한, AgentScaler와 같은 대규모 에이전트형 파운데이션 모델(agentic foundation models)과의 통합 및 GEM의 벡터화된 인터페이스(vectorized interface)를 활용한 다중 에이전트(multiple agents) 간의 복잡한 조정 작업을 설계하는 연구도 활발합니다.

---

### DeepSearch – 몬테카를로 트리 탐색을 통한 RLVR 병목 현상 극복

**자세히 보기**: DeepSearch (논문)

**어떤 문제를 해결하는가?**
RLVR에서 훈련 성능은 종종 포화 상태에 이릅니다. 이는 소수의 샘플링된 궤적(trajectories)이 가능한 모든 추론 경로를 거의 포착하지 못하기 때문입니다. 훈련이 진행됨에 따라 모델의 행동은 점점 더 결정론적(deterministic)이 되어, RL은 분산(variance)이 줄어들고 개선을 멈춥니다.

**어떻게 문제를 해결하는가?**
DeepSearch는 몬테카를로 트리 탐색(Monte Carlo Tree Search, MCTS)을 RL 훈련 루프(training loop)에 통합합니다. 에이전트(agent)는 무작위 롤아웃(random rollouts) 대신, 훈련 중에 추론 궤적(reasoning trajectories)에 대한 구조화된 탐색(structured search)을 수행합니다. 핵심 구성 요소로는 유망한 가지(branches)를 탐색하는 전역 경계 선택(Global frontier selection), 확신 있고 가치 높은 궤적(trajectories)에 훈련을 집중하는 엔트로피 기반 경로 선택(Entropy-based path selection), 그리고 발견된 솔루션을 저장하고 재활용하는 솔루션 캐싱(solution caching)을 통한 적응형 리플레이(Adaptive replay)가 있습니다.

**주요 발견**
*   **탐색 정체 해결**: 추론 경로를 체계적으로 탐색함으로써 DeepSearch는 확장된 RLVR 훈련에서 관찰되는 성능 저하를 방지합니다.
*   **최첨단 정확도**: 수학 추론 벤치마크(benchmarks)에서 DeepSearch로 훈련된 15억(1.5B) 매개변수(parameter) 모델은 평균 62.95%의 정확도를 달성하여, 기존 RLVR 방법보다 높은 성능을 보였으며, 표준 RL 훈련을 확장하는 것보다 5.7배 적은 GPU 시간을 사용했습니다.
*   **효율적인 탐색**: 구조화된 탐색(structured search)은 더 나은 샘플 효율성(sample efficiency)을 제공합니다. 에이전트(agent)는 대량의 무작위 롤아웃(random rollouts) 없이도 올바른 전략을 학습합니다.

**다음은 무엇인가?**
DeepSearch는 현재 수학 외에도 코드 합성(code synthesis)이나 정리 증명(theorem proving)과 같은 복잡한 도메인(domains)에 적용 가능성을 탐색하고 있습니다. MCTS를 동적 롤아웃 예산(dynamic rollout budgets, Knapsack RL 방식)과 결합하여 탐색 효율성을 더욱 높이는 연구가 진행 중이며, 미분 가능한 MCTS(differentiable MCTS)와 같은 미분 가능한 탐색(differentiable search) 기법을 통해 종단 간(end-to-end) 훈련을 가능하게 하는 방향으로 발전하고 있습니다.

---

### SFT 일반화 신화 반박하기

**자세히 보기**: SFT 일반화 신화 반박하기 (논문 / 코드)

**어떤 문제를 해결하는가?**
지도 미세 조정(Supervised fine-tuning, SFT)은 때때로 지시 템플릿(instruction templates)을 암기하고 그 이상으로 일반화(generalize)하지 못하는 모델을 생성한다는 비판을 받습니다. 대조적으로, RLHF 또는 RLVR과 같은 RL 기반 방법은 더 큰 견고성(robustness)을 달성하는 것으로 여겨집니다. 이 논문은 SFT가 본질적으로 일반화 능력이 떨어진다는 믿음에 이의를 제기합니다.

**어떻게 문제를 해결하는가?**
저자들은 SFT 훈련에서 두 가지 주요 실패 모드(failure modes)를 지적합니다. 첫째, 고정된 지시 템플릿(instruction templates)으로 훈련할 때 발생하는 고정된 프롬프트 아티팩트(Frozen prompt artifact)로 인해 모델이 특정 템플릿에 과적합(overfitting)되는 현상입니다. 둘째, 중간 추론(intermediate reasoning)의 부족으로 인해 SFT 모델이 더 어려운 사례를 해결하는 데 어려움을 겪는 알고리즘적 틀 부족(Lack of algorithmic scaffolding)입니다. 이들은 간단한 해결책으로, 훈련 시 광범위한 프롬프트 스타일(prompt styles)을 사용하는 프롬프트 다양성(Prompt diversity)과 훈련 데이터에 명시적인 추론 흔적(reasoning traces)을 제공하는 사고의 사슬 감독(Chain-of-thought supervision)을 제안합니다.

**주요 발견**
*   **프롬프트 다양성만으로 스타일 일반화 개선**: 모델이 다양한 지시 형식에 노출되면, 보지 못한 프롬프트 변형(prompt variations)에서도 잘 수행됩니다.
*   **CoT 스캐폴딩(scaffolding)은 난이도 일반화 개선**: 사고의 사슬(chain-of-thought) 예제를 포함하면 SFT 모델이 이전에 실패했던 더 어려운 사례(예: 더 큰 소코반 퍼즐)를 해결할 수 있는 능력이 향상됩니다.
*   **SFT는 RL과 필적할 수 있음**: 프롬프트 다양성(prompt diversity)과 CoT 감독(supervision)을 통해 SFT 모델은 테스트된 작업에서 RL 훈련 정책(RL-trained policies)의 성능과 같거나 능가합니다.

**다음은 무엇인가?**
이 연구는 SFT에 대한 데이터 중심적 관점(data-centric view)을 확립했으며, 현재는 SFT를 RLMT 또는 RLVR과 같은 다른 훈련 체제(training regimes)와 결합하여 시너지를 창출하는 연구가 활발합니다. 또한, 코드 생성(code generation)이나 도구 사용(tool use)과 같은 실제 응용 분야에서 일반화(generalization) 능력을 테스트하고 있으며, 프롬프트 다양성(prompt diversity)에 대한 체계적인 벤치마크(benchmark)를 개발하여 평가의 표준화를 꾀하고 있습니다.

---

### Thoughtbubbles – 잠재 공간에서의 비지도 병렬 사고

**자세히 보기**: Thoughtbubbles (논문)

**어떤 문제를 해결하는가?**
LLM은 일반적으로 쌓인 레이어(stacked layers)를 통해 입력을 순차적으로 처리합니다. 복잡한 추론은 종종 명시적인 사고의 사슬 프롬프팅(chain-of-thought prompting)을 요구하며 긴 출력을 생성합니다. 모델이 사고의 사슬을 명시적으로 작성하지 않고도 어려운 토큰에 내부적으로 추가 연산(extra compute)을 할당할 수 있을까요?

**어떻게 문제를 해결하는가?**
Thoughtbubbles는 트랜스포머(Transformers)에 대한 혁신적인 아키텍처 수정(architectural modification)을 제안합니다. 사전 훈련(pretraining) 동안 모델은 특정 토큰에 대해 잔차 스트림(residual streams)의 사본을 "분기(fork)"하여, 병렬 계산 분기(computational branches, 버블)를 생성하도록 학습합니다. 어려운 토큰에는 더 많은 계산 단계(computation steps)가 할당되고, 쉬운 토큰은 정상적으로 처리됩니다. 이 방법은 언어 모델링 손실(language modeling loss)만을 사용하여 비지도 방식(unsupervised manner)으로 학습되며, 훈련과 추론(inference) 사이에 불일치 없이 모델이 필요할 때 자동으로 버블을 트리거(trigger)합니다.

**주요 발견**
*   **개선된 혼란도**: 1억 5천만(150M)에서 7억 7천만(770M) 매개변수(parameters)에 이르는 모델 크기에서, Thoughtbubbles는 표준 디코더(standard decoders)에 비해 텍스트 코퍼스(text corpora)의 혼란도(perplexity)를 일관되게 낮춥니다.
*   **더 나은 제로샷 추론**: HellaSwag, LAMBADA와 같은 추론 벤치마크(benchmarks)에서 Thoughtbubbles 모델은 표준 트랜스포머(Transformers)와 비적응형 병렬 방법(non-adaptive parallel methods) 모두를 능가하는 우수한 성능을 보입니다.
*   **통합된 훈련/추론 동작**: 메커니즘이 사전 훈련(pretraining) 중에 학습되기 때문에 훈련과 추론(inference) 사이에 불일치(discrepancy)가 없습니다. 모델은 필요할 때 자연스럽게 추가 연산(extra compute)을 할당합니다.

**다음은 무엇인가?**
현재 Thoughtbubbles는 RL 또는 탐색(예: MCTS)과 결합하여 추론 단계(reasoning steps) 전반에 걸쳐 병렬 계산(parallel computation)을 더욱 효율적으로 할당하는 연구가 진행 중입니다. 또한, 버블 깊이(bubble depth) 및 병합 전략(merging strategies) 최적화, 그리고 멀티모달 모델(multimodal models)로의 확장 가능성을 탐색하고 있습니다. 해석 가능성 연구(interpretability studies)를 통해 버블이 특정 추론 패턴(reasoning patterns)과 어떻게 연관되는지 밝혀내는 것도 중요한 과제입니다.

---

### 보기 전에 보는 법 배우기 – LLM 시각 사전 지식의 신비 해명

**자세히 보기**: 보기 전에 보는 법 배우기 (논문)

**어떤 문제를 해결하는가?**
텍스트만으로 훈련된 대규모 언어 모델은 이미지를 본 적이 없음에도 불구하고 간단한 시각 질문(예: "하늘은 파란색인가요?")에 답하는 놀라운 능력을 종종 보여줍니다. 이러한 시각 사전 지식(visual priors)은 어디에서 오는 걸까요? 어떻게 의도적으로 이를 배양할 수 있을까요?

**어떻게 문제를 해결하는가?**
저자들은 LLM이 언어 사전 훈련(language pretraining) 동안 두 가지 종류의 시각 사전 지식(visual priors)을 어떻게 습득하는지 분석합니다. 첫째, 코드, 수학, 과학 문서와 같은 추론 중심 텍스트에서 파생되는 시각적 추론 사전 지식(Visual reasoning prior)이며, 모델에게 시각적 개념을 논리적으로 연결하는 방법을 가르칩니다. 둘째, 일상적인 장면 묘사를 포함하는 광범위한 자연어 코퍼스(natural language corpora)에서 파생되는 시각적 지각 사전 지식(Visual perception prior)입니다. 그들은 추론 사전 지식(reasoning prior)이 모델 크기(model size)에 따라 강력하게 확장되며 최소한의 이미지 노출로도 시각 작업(vision tasks)으로 전이될 수 있음을 보여줍니다.

**주요 발견**
*   **추론 vs. 지각 사전 지식**: 추론 사전 지식(reasoning priors)은 구조화된 텍스트에서 오고 모델 크기(model size)에 따라 확장됩니다. 지각 사전 지식(perception priors)은 광범위한 언어에서 오고 빠르게 포화됩니다.
*   **데이터 효율성**: 적절한 양의 추론 중심 텍스트는 시각적 추론 능력(visual reasoning ability)을 크게 향상시키지만, 묘사적인 텍스트를 무작정 늘리는 것은 수확 체감(diminishing returns)을 가져옵니다.
*   **시각 인식 LLM을 위한 레시피**: 사전 훈련 혼합(pretraining mixtures)을 제어함으로써 저자들은 최소한의 이미지 미세 조정(image fine-tuning) 후 시각 작업(vision tasks)에서 더 나은 성능을 보이는 모델을 생성합니다.

**다음은 무엇인가?**
현재 연구는 이러한 사전 지식(priors)을 연속 잠재 CoT(continuous latent CoT, Thoughtbubbles 참조) 또는 생성형 확산 모델(generative diffusion models)과 결합하여 더욱 강력한 멀티모달 능력을 개발하는 데 초점을 맞추고 있습니다. 또한, 명시적인 공간 추론 작업(spatial reasoning tasks)을 사전 훈련(pretraining)에 통합하여 훨씬 더 정교한 시각 사전 지식(priors)을 구축하려는 시도가 진행 중입니다. 고품질 추론 텍스트의 큐레이션(curating)이 유능한 멀티모달 모델(multimodal models) 개발의 핵심 동력임을 시사하며, 이는 데이터 중심 접근 방식의 중요성을 강조합니다.

---

### Knapsack RL – 예산 할당을 통한 탐색 잠금 해제

**자세히 보기**: Knapsack RL (논문)

**어떤 문제를 해결하는가?**
강화 학습 미세 조정(Reinforcement learning fine-tuning)은 종종 문제당 고정된 수의 롤아웃(rollouts)을 사용합니다. 이러한 균일한 할당(uniform allocation)은 연산(compute)을 낭비합니다. 쉬운 작업은 적은 시도(trials)만 필요하고, 극도로 어려운 작업은 적은 예산으로는 결코 성공하지 못할 수 있습니다. 결과적으로 많은 작업이 제로 기울기(zero gradients)를 생성하여 시간을 낭비하고 학습을 방해합니다.

**어떻게 문제를 해결하는가?**
Knapsack RL은 탐색(exploration)을 배낭 최적화 문제(knapsack optimization problem)로 재정의합니다. 각 작업의 롤아웃(rollouts)에 "비용"(연산)과 잠재적인 "가치"(예상 학습 이득)를 부여하고, 배낭 문제(knapsack problem)를 해결하여 예상 이득이 높은 작업에는 더 많은 롤아웃(rollouts)을, 이미 해결되었거나 가망 없는 작업에는 더 적게 할당합니다. 이 동적 예산 책정(dynamic budgeting) 방식은 GRPO 알고리즘(algorithm)을 기반으로 합니다.

**주요 발견**
*   **증가된 기울기 밀도**: 0이 아닌 정책 기울기(policy gradients)의 비율이 20–40% 증가하며, 이는 에이전트(agent)가 각 업데이트(update)마다 더 많은 작업에서 학습한다는 것을 의미합니다.
*   **어려운 작업에 대한 대규모 예산**: 일부 극도로 어려운 작업에는 약 93개의 롤아웃(rollouts)이 할당되어, 균일한 예산으로는 불가능했을 탐색(exploration)을 가능하게 합니다.
*   **성능 향상**: 수학 추론 벤치마크(benchmarks)에서 Knapsack RL은 균일한 할당(uniform allocation)이 요구하는 연산(compute)의 절반을 사용하면서 평균 2–4점, 특정 사례에서는 최대 9점의 개선을 달성합니다.

**다음은 무엇인가?**
Knapsack RL은 현재 BroRL과 결합되어 전역적인 탐색(exploration) 확장과 지역적인 예산(budgets) 할당을 동시에 최적화하는 방식으로 연구되고 있습니다. 탐색(exploration)을 더욱 강화하기 위해 MCTS 기반 훈련(DeepSearch 참조) 또는 다양성 목표(diversity objectives)와 짝을 이루는 연구도 활발합니다. 또한, 배낭 문제(knapsack idea)를 다중 에이전트(multi-agent) 시스템이나 계층적 작업(hierarchical tasks)으로 확장하여 하위 작업(subtasks)에 예산을 효율적으로 분배하는 방안도 탐색되고 있습니다.

---

### TruthRL – 강화 학습을 통한 진실한 LLM 장려

**자세히 보기**: TruthRL (논문)

**어떤 문제를 해결하는가?**
LLM은 종종 환각(hallucinate)을 일으켜 그럴듯하지만 거짓된 답변을 지어냅니다. 순전히 정확성 기반의 RL 보상(RL rewards)은 추측을 조장할 수 있습니다. 반대로 모델은 지나치게 조심하여 답변을 거부할 수 있으며, 이는 유용성을 저해합니다. 정확성, 진실성, 적절한 회피(abstention) 사이의 균형을 어떻게 맞출 수 있을까요?

**어떻게 문제를 해결하는가?**
TruthRL은 RL 미세 조정(fine-tuning)에 삼진 보상(ternary reward) 시스템을 도입합니다. 정답에는 긍정적인 보상, 환각(hallucinations)에는 큰 페널티, 그리고 정직한 회피("모르겠습니다")에는 약간의 긍정적인 보상을 제공합니다. GRPO 알고리즘(algorithm)을 사용하여 훈련하며, 모델이 확신할 때만 답변하고 그렇지 않을 때는 불확실성을 인정하도록 장려합니다. 이 방법은 외부 검색(external retrieval) 유무에 관계없이 개방형 도메인 QA(open-domain QA)와 같은 다양한 작업에 적용 가능합니다.

**주요 발견**
*   **환각 감소**: TruthRL은 여러 지식 집약적 벤치마크(knowledge-intensive benchmarks)에서 바닐라 RL(vanilla RL) 대비 환각(hallucination) 발생률을 28.9% 감소시킵니다.
*   **진실성 증가**: 전반적인 진실성(정확성 + 정직한 회피)이 21.1% 향상되는 효과를 보였습니다.
*   **일반적인 적용 가능성**: Qwen, Llama와 같은 모델 유형과 검색 증강(retrieval-augmented) 및 검색 없는(retrieval-free) 설정 모두에서 이점이 지속됩니다.

**다음은 무엇인가?**
TruthRL은 현재 공정성 목표(fairness objectives) 또는 다른 정렬 지표(alignment metrics)와 결합하여 진실할 뿐만 아니라 윤리적으로도 답변하는 모델을 생성하는 방향으로 연구되고 있습니다. 더 미묘한 보상 구조(reward structures, 예: 난이도에 따라 조정) 또는 다중 턴 대화(multi-turn dialogues)에 적용하는 연구도 활발합니다. 또한, 불확실할 때 회피를 장려하여 개인 정보 유출을 완화하기 위해 비밀 유도 연구(secret elicitation research)와 결합하는 방안이 탐색되고 있습니다.

---

### MemGen – 자기 진화 에이전트를 위한 생성형 잠재 기억 엮기

**자세히 보기**: MemGen (논문 / 코드)

**어떤 문제를 해결하는가?**
LLM 에이전트(agents)는 종종 제한된 기억(memory)을 가집니다. 미세 조정(fine-tuning) 중에 매개변수(parameters)를 다시 작성하거나(매개변수 기억, parametric memory) 외부 데이터베이스(database)를 쿼리합니다(비매개변수 기억, nonparametric memory). 이러한 접근 방식은 경직되거나 추론과 단절될 수 있습니다. 에이전트(agent)가 생성하고 내부 사고 과정에 엮어 넣을 수 있는 동적이고 긴밀하게 연결된 기억(memory)이 필요합니다.

**어떻게 문제를 해결하는가?**
MemGen은 두 가지 핵심 구성 요소(components)를 가진 생성형 잠재 기억 시스템(generative latent memory system)을 도입합니다. 첫째, 에이전트(agent)의 현재 추론 상태(reasoning state)를 모니터링하고 언제 기억을 불러올지 결정하는 기억 트리거(Memory trigger)입니다. 둘째, 트리거되면 관련 기억 내용(memory content)을 나타내는 잠재 토큰(latent tokens) 시퀀스(sequence)를 생성하고 이를 모델의 컨텍스트(context)에 다시 주입하는 기억 위버(Memory weaver)입니다. 이 시스템은 에이전트(agent)가 일시 중지하고, 내부 기억(internal memory)을 생성한 다음, 해당 기억이 숨겨진 상태(hidden state)에 융합된 상태로 추론을 재개할 수 있도록 합니다.

**주요 발견**
*   **성능 향상**: 8개의 다양한 벤치마크(benchmarks)에서 MemGen은 기존 기억 증강 에이전트(memory-augmented agents, ExpeL, AWM)를 최대 38.22% 능가하고, 강력한 GRPO 기준선(baseline)을 최대 13.44% 초과합니다.
*   **인간과 유사한 기억 패턴**: 수동 코딩(hand-coding) 없이도 MemGen 에이전트(agents)는 인간 인지(human cognition)를 연상시키는 계획 기억(planning memory), 절차 기억(procedural memory), 작업 기억(working memory)과 유사한 기억 행동을 자발적으로 개발합니다.
*   **교차 도메인 일반화**: 생성형 기억(generative memory)은 여러 도메인(domains, 수학, 프로그래밍, Q&A)에서 성능을 향상시키며, 이는 광범위하게 적용 가능함을 시사합니다.

**다음은 무엇인가?**
MemGen은 현재 추론(reasoning)을 더욱 향상시키기 위해 탐색 기반 훈련(search-based training, 예: DeepSearch) 또는 탐색 확장(exploration scaling, BroRL)과 통합되는 연구가 활발합니다. 연구자들은 또한 최적의 보상(rewards)을 위해 언제 무엇을 불러올지 학습하도록 기억 트리거(memory trigger)와 위버(weaver)를 RL과 함께 공동으로 훈련하는 방안을 탐색하고 있습니다. 마지막으로, MemGen을 Dragon Hatchling과 같은 생물학적 영감 아키텍처(biologically inspired architectures)와 결합하여 뇌와 유사한 기억(memory)과 네트워크 구조(network structure)를 모두 갖춘 에이전트(agents)를 구현하려는 시도도 진행 중입니다.

---

### 요약 – 통찰력 및 향후 방향

이 업데이트된 논문들을 종합해 보면, 우리는 LLM 연구의 몇 가지 중요한 흐름을 확인할 수 있습니다.

*   **탐색의 중요성과 최적화**: BroRL, DeepSearch, Knapsack RL 등은 모두 LLM의 탐색(exploration) 능력을 확장하고 효율적으로 할당하는 다양한 전략을 제시합니다. 롤아웃(rollouts)을 늘리거나, 탐색을 모델 내부에 통합하거나, 예산을 지능적으로 배분함으로써, 이 연구들은 강화 학습(RL)이 정체되는 현상을 극복하려는 노력을 보여줍니다. 최근에는 이러한 방법론들을 복합적으로 적용하여 시너지를 극대화하는 방향으로 발전하고 있습니다.

*   **아키텍처 혁신을 통한 지능 향상**: Thoughtbubbles는 비지도 학습을 통해 모델이 병렬적으로 사고하도록 하는 새로운 아키텍처적 접근을 제시하며, MemGen은 동적이고 생성적인 잠재 기억(generative latent memory) 시스템을 도입합니다. Dragon Hatchling과 같은 생물학적 영감 아키텍처(biologically inspired architectures)는 단순히 모델을 확장하는 것을 넘어, 정보 처리 및 저장 방식을 근본적으로 개선하려는 시도를 보여줍니다. 이러한 혁신은 LLM이 더욱 복잡한 문제를 해결하고 인간과 유사한 인지 능력을 발휘하는 데 필수적입니다.

*   **데이터 품질과 일반화 능력**: "보기 전에 보는 법 배우기"와 "SFT 일반화 신화 반박하기"는 훈련 데이터(training data)의 양뿐만 아니라 품질과 다양성이 모델의 일반화 능력에 미치는 결정적인 영향을 강조합니다. 추론 중심 텍스트(reasoning-centric texts)를 신중하게 선별하거나 다양한 프롬프트(prompts)와 사고의 사슬(CoT) 예제를 훈련에 포함함으로써, LLM은 이전에 숨겨져 있던 잠재력을 발휘할 수 있습니다. 이는 데이터 큐레이션(data curation)의 중요성을 다시 한번 상기시킵니다.

*   **모델의 신뢰성과 책임감**: "비밀 지식 유도하기" 연구는 모델 내부의 민감한 정보 유출 위험을 경고하며, TruthRL은 모델이 진실하고 정직하게 답변하도록 훈련하는 방법을 제시합니다. 이러한 연구들은 LLM이 단순한 성능을 넘어 신뢰할 수 있고 윤리적으로 작동하도록 하는 '정렬(alignment)' 문제의 핵심을 다룹니다. 앞으로는 이러한 통찰력을 결합하여, 광범위한 탐색, 교정된 진실성, 다양한 감독을 통해 훈련되고, 생성형 기억 및 병렬 추론 능력을 갖춘 다음 세대의 에이전트(agents)를 개발하는 데 집중할 것입니다.

AI의 미래는 신경과학, 최적화, 윤리 및 안전 연구를 아우르는 다학제적 혁신을 통해 형성될 것입니다. 다음 업데이트도 기대해 주세요!

구독 ❤️ 이 기사가 마음에 드셨다면 '좋아요'를 누르고 동료들과 공유해 주세요.