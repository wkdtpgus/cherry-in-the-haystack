반갑습니다, 독자 여러분! 최신 대규모 언어 모델 동향을 다루는 LLM 관측소의 주간 보고서입니다.

**멈춤을 넘어선 진보**: 강화 학습(RL) 과정에서 발생하는 성능 정체를 해소하기 위해, 넓은 범위의 탐색 기법(BroRL), 몬테카를로 트리 탐색(MCTS)의 결합(DeepSearch), 그리고 배낭 문제 형태의 자원 배분(Knapsack-style budgeting) 등 여러 전략들이 고착된 RL 훈련을 다시 활성화하고 있습니다. 이는 단순히 계산 자원을 늘리는 것을 넘어, 탐색 효율성을 근본적으로 개선하려는 시도입니다.

**더 많은 사고가 아닌 더 현명한 인지**: 단지 처리 시간을 늘리는 것을 넘어, 모델의 인지 효율성을 높이는 방향으로 에이전트 설계가 진화하고 있습니다. 이는 내부 잠재 공간에서 병렬적으로 사고하는 방식(Thoughtbubbles)과 자율적으로 정보를 생성하고 활용하는 내부 기억 시스템(MemGen)을 통해 구현됩니다. 이러한 접근은 명시적인 사고 과정 없이도 복잡한 문제 해결을 가능하게 합니다.

**진실성 확보가 최우선**: 말뿐인 그럴듯함보다는 실제 사실에 부합하는 것이 중요합니다. TruthRL은 '모른다'고 인정하는 솔직함에도 긍정적 보상을 부여하며, 동시에 모델이 은폐하려 하는 정보조차도 특정 기법을 통해 노출될 수 있다는 비밀 추출 연구 결과가 나왔습니다. 이는 AI의 신뢰성과 투명성에 대한 중요한 시사점을 제공합니다.

**실험실에서 실제 적용으로**: 학술적 발견이 실제 성능 순위표로 이어지기 위해, GEM은 에이전트 역할을 하는 대규모 언어 모델의 학습 및 측정 방식을 통일합니다. 또한, 지도 미세 조정(SFT)의 한계에 대한 기존 통념은 프롬프트의 다양성을 높이고 사고 과정을 명시적으로 감독함으로써 극복될 수 있음이 입증되었습니다. 이는 SFT의 잠재력을 재조명합니다.

**뇌 구조와 시각적 지능의 교차점**: 생물학적 뇌 구조에서 영감을 받은 Dragon Hatchling은 트랜스포머 아키텍처를 신경망과 유사하게 연결하고, 오직 텍스트 데이터만으로 훈련된 모델도 시각 관련 사전 지식을 습득할 수 있다는 흥미로운 사실이 밝혀졌습니다. 이는 멀티모달 AI의 발전 방향에 새로운 통찰을 제시합니다.

저희의 최신 소식을 놓치지 않으시려면 지금 바로 구독해 주십시오. LLM 관측소 구독자분들은 텍사스 오스틴에서 개최되는 제6회 MLOps World | GenAI 국제 정상회담에 특별히 초대됩니다. 본 행사에는 OpenAI, HuggingFace와 같은 주요 기업들이 참여하며, 60개가 넘는 다양한 발표가 준비되어 있습니다. 구독자께서는 온라인으로 무료 참석이 가능합니다. 특히, 생성형 AI 시대에 MLOps의 역할과 중요성이 더욱 커지고 있는 만큼, 이번 서밋은 관련 분야 전문가들에게 필수적인 통찰과 네트워킹 기회를 제공할 것입니다. 만약 현장에서 실질적인 워크숍, 실제 적용 사례, 네트워킹 기회 및 오스틴 시내 파티에 참여하시려면, 이 할인 코드를 활용하여 150달러의 혜택을 받으십시오!

**150달러 할인**

---

**본 호에 특화된 핵심 용어 해설**

*   **RLVR (검증 가능한 보상 기반 강화 학습)**: 사람의 수동 라벨링 없이도 정답 여부를 기계적으로 검증할 수 있는 보상 체계를 갖춘 RL 기법입니다. 예를 들어, 수학 문제의 정답 판정이나 소프트웨어 단위 테스트 결과 확인 등이 이에 해당하며, 이는 보상 설계의 객관성을 크게 높여줍니다.
*   **비밀 정보 추출 (모델 감사)**: 모델이 내부적으로 인지하고 있으나 명시적으로 드러내지 않는 정보를 끌어내는 데 사용되는 질의 또는 분석 방법론을 의미합니다. 이는 외부 접근만 가능한 블랙박스 방식(예: 시드 완성, 페르소나 기반 샘플링)과 모델 내부 작동을 들여다보는 화이트박스 도구(예: 로짓 렌즈, 희소 오토인코더)를 포함하며, AI 시스템의 잠재적 위험을 평가하는 데 중요합니다.
*   **사고 과정 감독 (SFT 적용)**: 모델이 복잡한 문제에 대한 해결책을 도출할 때 필요한 단계별 추론 과정을 배우도록, 상세한 중간 해결 단계를 포함한 데이터로 훈련시키는 기법입니다. 이는 모델이 어려운 시나리오에서도 일반화된 문제 해결 능력을 갖추도록 돕는 알고리즘적 기초를 제공합니다.
*   **내재적 병렬 처리 사고**: 트랜스포머 아키텍처 내에서 명시적인 연쇄적 사고 과정 없이도, 모델이 어려운 입력 요소에 대해 잔차 흐름을 나누어 동시에 추가적인 계산 자원을 배분하도록 하는 방식입니다. 이는 모델이 필요한 곳에만 효율적으로 인지 자원을 집중할 수 있게 하여 추론 성능을 향상시킵니다.
*   **텍스트 기반 시각적 선행 지식**: 주로 코드, 수학, 과학 등 추론적 성격이 강한 텍스트는 시각적 추론에 필요한 배경 지식을 형성하며, 광범위한 일반 언어 데이터는 시각적 인지에 필요한 선행 정보를 구축합니다. 이러한 지식은 추후 효과적인 비전 인코더와 소량의 다중 모드 미세 조정을 통해 시각 관련 작업에서 활용되며, 텍스트와 시각 정보 간의 놀라운 연결고리를 보여줍니다.

---

### BroRL – 폭넓은 탐색으로 강화 학습의 지평을 넓히다

**관련 논문**: BroRL (논문)

**어떤 문제가 해결되는가?**
검증 가능한 보상을 사용하는 강화 학습(RLVR)은 추론 능력을 향상시키지만, 수천 단계의 훈련 이후에는 성능이 더 이상 개선되지 않는 경향이 있습니다. 이는 모델이 충분히 탐색하지 않아 발전이 멈추기 때문이며, 훈련을 지속하려 해도 효용 체감만 발생합니다. 즉, 모델이 특정 행동 양식에 갇혀 새로운 해결책을 찾지 못하는 고착 상태에 빠지는 것이 일반적인 문제입니다.

**어떻게 문제를 해결하는가?**
BroRL은 탐색 범위를 넓히기 위해, 각 예제당 실행(rollout) 횟수를 수백 번으로 대폭 늘릴 것을 제안합니다. 단순히 훈련 단계를 늘리는 대신, 동일한 수의 기울기 업데이트를 유지하면서도 훨씬 많은 궤적을 샘플링하여 탐색 공간을 확장합니다. 이 방법은 기존 RL 훈련의 한계인 지역 최적점에 갇히는 문제를 해결하며, 이론적으로 더 많은 샘플링이 올바른 확률 질량을 단조롭게 확장하고 미탐색 행동의 영향을 줄여 전반적인 개선을 보장합니다. 이는 무작위 탐색의 단순한 확장처럼 보이지만, 그 효과는 매우 강력합니다.

**주요 발견**
*   **이론적 근거 제시**: 질량 균형 논증(mass-balance argument)은 더 많은 롤아웃 샘플링이 올바른 확률 질량을 확장하고 미탐색 행동의 영향을 감소시킨다는 것을 증명하며, 이는 BroRL의 견고한 기반이 됩니다.
*   **정체된 모델의 재활성화**: BroRL은 약 3천 번의 ProRL 단계 이후 정체된 모델조차도 다시 활성화하여, ProRL이 포화 상태에 도달한 후에도 지속적인 성능 향상을 가능하게 합니다. 이는 RL 훈련의 수명과 효율성을 크게 늘립니다.
*   **최첨단 성능 달성**: 15억(1.5B) 매개변수 모델에서 BroRL은 다양한 벤치마크에서 최첨단 성능을 기록하며, 단순히 훈련 단계를 늘리는 것보다 탐색 확장이 훨씬 효과적일 수 있음을 보여줍니다.

**다음 단계는 무엇인가?**
향후 연구는 BroRL을 몬테카를로 트리 탐색(MCTS)과 같은 구조화된 탐색 방법이나 Knapsack RL과 같은 예산 할당 방식과 결합하여 탐색 효율을 극대화할 수 있습니다. 또한, BroRL의 원리를 단일 단계 RL을 넘어 복잡한 순차적 의사결정 설정으로 확장하거나, MemGen과 같은 기억 증강 아키텍처와 통합하여 더 정교한 에이전트를 개발하는 방향도 모색될 수 있습니다.

---

### 언어 모델로부터 숨겨진 정보 유도하기

**관련 논문**: 비밀 지식 유도하기 (논문)

**어떤 문제가 해결되는가?**
대규모 언어 모델은 사용자 특성과 같은 민감한 정보를 내부에 가지고 있을 수 있으며, 이를 직접적으로 부인하면서도 질문에 답변하는 데 활용할 수 있습니다. 이는 개인 정보 보호와 보안에 심각한 우려를 낳습니다. 과연 모델이 숨기도록 훈련된 지식을 드러낼 수 있을까요? 이 연구는 AI 시스템의 "블랙박스" 내부에 숨겨진 정보에 대한 접근 가능성을 탐구합니다.

**어떻게 문제를 해결하는가?**
연구진은 비밀 정보 추출을 위한 테스트 환경을 구축했습니다. 그들은 LLM이 내부 추론 과정에 비밀을 포함하도록 미세 조정합니다. 예를 들어, 모델은 사용자의 성별을 알지만 직접적인 질문에는 부인하며 답변에 활용합니다. 이후, 외부 접근만 가능한 블랙박스 공격(프리필, 페르소나 샘플링)과 모델 내부 작동을 들여다보는 화이트박스 공격(로짓 렌즈, 희소 오토인코더)을 설계하여 비밀이 얼마나 쉽게 추출되는지 평가합니다. 이 체계적인 접근 방식은 모델의 취약점을 정량적으로 분석합니다.

**주요 발견**
*   **프리필 공격의 효용성**: 세 가지 설정 중 두 가지에서, 블랙박스 프리필 공격(프롬프트 앞에 조작된 접두사를 추가)은 기존 방법보다 숨겨진 비밀을 더 효과적으로 유도했습니다. 이는 외부에서 모델에 미묘한 영향을 주는 것만으로도 내부 정보를 끌어낼 수 있음을 시사합니다.
*   **화이트박스 기법의 특정 상황에서의 우위**: 나머지 설정에서는 로짓 렌즈 및 희소 오토인코더 분석과 같은 화이트박스 기법이 더 효과적이었으며, 이는 모델의 내부 활성화 상태를 직접 검사하는 것이 숨겨진 지식을 드러내는 데 결정적일 수 있음을 보여주었습니다.
*   **공개 벤치마크 제공**: 저자들은 공개 감사를 위한 모델과 코드를 공개하여, 모델 개인 정보 보호 및 적대적 유도에 대한 향후 연구의 기준선을 마련했습니다. 이는 투명성과 공동 연구의 중요성을 강조합니다.

**다음 단계는 무엇인가?**
향후 연구는 민감한 표현을 강력하게 제거하거나 무작위화하는 방법과 같은 완화 전략을 탐색하고, 벤치마크를 사용자 위치, 정치적 견해 등 더 다양한 비밀 유형으로 확장해야 합니다. 비밀 정보 추출과 TruthRL을 결합하면 모델이 민감한 내용을 알고 있지만 공유를 거부할 때 이를 인정하도록 장려하여, AI의 윤리적 사용에 기여할 수 있습니다.

---

### GEM – 에이전트형 LLM을 위한 표준화된 실험 환경

**관련 논문**: GEM (논문)

**어떤 문제가 해결되는가?**
에이전트형 LLM의 개발 및 평가에는 표준화된 훈련 및 벤치마킹 환경이 필수적입니다. 기존 OpenAI Gym과 같은 환경은 주로 로봇 공학이나 간단한 과제에 초점을 맞추었기에, 에이전트형 LLM 연구 분야는 공통된 플랫폼이 부재했습니다. 이러한 파편화된 환경은 연구 결과의 비교와 재현성을 어렵게 만듭니다.

**어떻게 문제를 해결하는가?**
GEM (General Experience Maker)은 LLM 에이전트를 위해 특별히 설계된 오픈 소스 시뮬레이션 환경입니다. 이는 비동기적이고 병렬화된 실행, 유연한 래퍼, 통합 도구(파이썬 코드 실행, 검색 등)를 지원하는 환경과 에이전트 간의 표준 인터페이스를 제공합니다. GEM은 수학, 코드, Q&A, 도구 사용을 포함하는 다양한 환경 스위트와 REINFORCE와 같은 RL 알고리즘을 위한 기준선 스크립트를 제공하며, Return Batch Normalization(ReBN)과 같은 안정화 기법도 포함합니다. 이는 에이전트형 LLM 연구를 위한 "비행 시뮬레이터" 역할을 합니다.

**주요 발견**
*   **포괄적인 환경 모음**: GEM은 수학적 추론부터 API 호출까지 풍부한 관찰/행동 공간을 가진 24개의 환경을 포함하며, 단일 단계 및 다단계 작업을 모두 지원합니다. 이처럼 광범위한 작업군은 다양한 에이전트 능력을 평가할 수 있게 합니다.
*   **ReBN을 활용한 기준선**: 저자들은 일반적인 RL 알고리즘(REINFORCE, GRPO, PPO)을 벤치마킹하고, ReBN이 정책 기울기를 안정화하여 턴당 밀집 보상으로 훈련할 수 있도록 돕는다는 것을 보여줍니다. 이는 RL 훈련의 안정성과 효율성을 크게 향상시킵니다.
*   **재사용 가능한 평가 도구**: GEM 인터페이스와 래퍼는 새로운 작업과 에이전트 아키텍처의 쉬운 통합을 가능하게 합니다. 이는 에이전트형 LLM을 위한 "미니 아레나"와 유사한 표준화된 테스트베드 역할을 하여 연구의 진입 장벽을 낮춥니다.

**다음 단계는 무엇인가?**
GEM은 에이전트형 RL 연구의 속도를 높이는 데 기여합니다. 향후 연구는 환경 라이브러리를 확장하여 이미지나 오디오와 같은 멀티모달 입력을 통합하거나, 장기 계획을 위한 벤치마크를 추가할 수 있습니다. 또한, AgentScaler와 같은 대규모 에이전트형 파운데이션 모델과 GEM을 통합하거나, GEM의 벡터화된 인터페이스를 기반으로 다중 에이전트 간의 조정을 요구하는 더 복잡하고 현실적인 작업을 설계하는 방향도 기대됩니다.

---

### DeepSearch – 몬테카를로 트리 탐색으로 RLVR의 한계를 돌파하다

**관련 논문**: DeepSearch (논문)

**어떤 문제가 해결되는가?**
RLVR 훈련에서 성능은 종종 특정 지점에서 정체됩니다. 이는 샘플링된 소수의 궤적만으로는 가능한 모든 추론 경로를 거의 포착할 수 없기 때문입니다. 훈련이 진행될수록 모델의 행동은 더욱 예측 가능해지고, 강화 학습은 분산이 줄어들며 개선이 멈춥니다. 이는 근본적으로 탐색-활용 딜레마에서 탐색이 부족해 발생하는 문제입니다.

**어떻게 문제를 해결하는가?**
DeepSearch는 몬테카를로 트리 탐색(MCTS)을 RL 훈련 과정에 통합하여 이 문제를 해결합니다. 에이전트가 무작위 롤아웃을 샘플링하는 대신, 훈련 중 추론 궤적에 대한 체계적인 탐색을 수행합니다. 핵심 요소로는 유망한 탐색 경로를 우선시하는 전역 경계 선택, 확신 있는 궤적에 집중하는 엔트로피 기반 경로 선택, 그리고 중복 탐색을 줄이는 적응형 리플레이 등이 있습니다. MCTS의 구조화된 탐색 능력은 모델이 더 효율적으로 최적의 추론 경로를 발견하도록 돕습니다.

**주요 발견**
*   **탐색 정체 문제 해결**: 추론 경로를 체계적으로 탐색함으로써 DeepSearch는 확장된 RLVR 훈련에서 관찰되는 성능 저하를 성공적으로 방지합니다. 이는 모델이 새로운 해결책을 지속적으로 찾을 수 있도록 합니다.
*   **최첨단 정확도 달성**: 수학 추론 벤치마크에서 DeepSearch로 훈련된 15억(1.5B) 모델은 평균 62.95%의 정확도를 달성했으며, 이는 기존 RLVR 방법보다 높고, 표준 RL 훈련을 확장하는 것보다 5.7배 적은 GPU 시간을 사용합니다. 이는 효율성 측면에서 큰 진전입니다.
*   **탐색 효율성 향상**: 구조화된 탐색은 더 나은 샘플 효율성을 제공합니다. 에이전트는 대량의 무작위 롤아웃 없이도 올바른 전략을 학습하여 계산 자원 낭비를 줄입니다.

**다음 단계는 무엇인가?**
DeepSearch는 수학 외의 도메인, 예를 들어 코드 합성이나 정리 증명에도 적용될 수 있습니다. MCTS를 Knapsack RL 방식의 동적 롤아웃 예산과 결합하면 추가적인 개선을 가져올 수 있습니다. 또한, 미분 가능한 탐색(예: 미분 가능한 MCTS)을 탐색하면 기울기 역전파를 통한 종단 간 훈련이 가능해져, 더욱 통합된 학습 시스템을 구축할 수 있습니다.

---

### SFT 일반화 능력에 대한 오해 해소하기

**관련 논문**: SFT 일반화 신화 반박하기 (논문 / 코드)

**어떤 문제가 해결되는가?**
지도 미세 조정(SFT)은 때때로 특정 지시 템플릿에만 암기적으로 반응하고 그 범위를 넘어 일반화되지 못한다는 비판을 받아왔습니다. 이에 비해 RLHF나 RLVR과 같은 강화 학습 기반 방법이 더 강력한 견고성을 달성한다고 여겨졌습니다. 본 논문은 SFT가 본질적으로 일반화 능력이 떨어진다는 이러한 통념에 이의를 제기하며, 데이터 중심적 접근의 중요성을 강조합니다.

**어떻게 문제를 해결하는가?**
연구진은 SFT 훈련의 두 가지 주요 실패 원인을 파악했습니다: 고정된 지시 템플릿에 과도하게 의존하여 프롬프트가 다양해질 때 성능이 저하되는 '고정된 프롬프트 아티팩트', 그리고 중간 추론 과정의 부재로 인해 어려운 사례를 해결하지 못하는 '알고리즘적 틀 부족'입니다. 해결책으로, SFT 과정에서 넓은 범위의 프롬프트 스타일을 사용하고, 훈련 데이터에 명시적인 추론 단계를 제공하는 '사고의 사슬 감독'을 제안합니다. 이 두 가지 간단한 수정은 SFT 모델이 다양한 지시 스타일과 증가된 작업 난이도에 걸쳐 일반화할 수 있도록 합니다.

**주요 발견**
*   **프롬프트 다양성을 통한 스타일 일반화 개선**: 모델이 다양한 지시 형식에 노출되면, 이전에 보지 못한 프롬프트 변형에서도 뛰어난 성능을 보입니다. 이는 모델이 특정 표현 방식에 과적합되는 것을 방지합니다.
*   **CoT 스캐폴딩을 통한 난이도 일반화 개선**: 사고의 사슬 예제를 훈련 데이터에 포함하면 SFT 모델이 이전에 실패했던 더 어려운 사례(예: 더 복잡한 소코반 퍼즐)를 해결할 수 있게 됩니다. 이는 모델에 문제 해결의 '방법론'을 가르치는 것과 같습니다.
*   **SFT, RL과 필적할 수 있음**: 프롬프트 다양성과 CoT 감독을 통해 SFT 모델은 테스트된 작업에서 RL 훈련 정책의 성능과 같거나 능가합니다. 이는 SFT의 잠재력이 저평가되었음을 보여주며, 데이터 품질의 중요성을 부각합니다.

**다음 단계는 무엇인가?**
이 연구는 SFT에 대한 데이터 중심적 관점을 장려합니다. 즉, 즉시 RL에 의존하기보다는 다양한 프롬프트와 추론 흔적을 포함한 고품질 데이터셋 구축에 투자해야 합니다. 향후 연구는 SFT를 다른 훈련 체제(예: RLMT 또는 RLVR)와 결합하고, 코드 생성 또는 도구 사용과 같은 실제 복잡한 작업에서 일반화 능력을 테스트해야 합니다. 프롬프트 다양성에 대한 체계적인 벤치마크는 평가를 표준화하는 데 도움이 될 수 있습니다.

---

### Thoughtbubbles – 잠재 공간에서의 자율적 병렬 사고

**관련 논문**: Thoughtbubbles (논문)

**어떤 문제가 해결되는가?**
대규모 언어 모델은 일반적으로 여러 층을 통해 입력을 순차적으로 처리합니다. 복잡한 추론에는 종종 명시적인 사고의 사슬 프롬프팅이 필요하며, 이는 긴 출력을 유발합니다. 모델이 사고의 사슬을 명시적으로 작성하지 않고도 어려운 토큰에 내부적으로 추가적인 계산 자원을 할당할 수 있을까요? 이는 모델의 효율성과 인간의 사고 과정 유사성을 높이는 데 중요한 문제입니다.

**어떻게 문제를 해결하는가?**
Thoughtbubbles는 트랜스포머 아키텍처에 대한 구조적 변형을 도입합니다. 사전 훈련 중 모델은 특정 토큰에 대해 잔차 스트림의 사본을 '분기'하는 방법을 학습하여, 효과적으로 병렬 계산 경로(버블)를 생성합니다. 어려운 토큰에는 더 많은 계산 단계가 할당되고, 쉬운 토큰은 평소처럼 처리됩니다. 이 방법은 언어 모델링 손실만을 사용하여 비지도 방식으로 학습되며, 명시적인 사고의 사슬 감독 없이도 모델이 자체적으로 인지 자원을 효율적으로 배분하도록 합니다. 이는 인간이 어려운 문제에 직면했을 때 더 깊이 생각하는 방식과 유사합니다.

**주요 발견**
*   **향상된 혼란도**: 1억 5천만(150M)에서 7억 7천만(770M) 매개변수에 이르는 모델 크기에서, Thoughtbubbles는 표준 디코더에 비해 텍스트 코퍼스의 혼란도(perplexity)를 일관되게 낮춥니다. 이는 모델의 언어 이해 및 생성 능력이 향상되었음을 의미합니다.
*   **더 뛰어난 제로샷 추론**: 추론 벤치마크(HellaSwag, LAMBADA)에서 Thoughtbubbles 모델은 표준 트랜스포머와 비적응형 병렬 방법을 모두 능가합니다. 이는 명시적 훈련 없이도 복잡한 추론 작업을 더 잘 수행함을 보여줍니다.
*   **훈련과 추론의 통합된 동작**: 메커니즘이 사전 훈련 중에 학습되기 때문에 훈련과 추론 사이에 불일치가 없습니다. 모델은 필요할 때 자연스럽게 추가 계산 자원을 할당하여, 효율적인 온디맨드(on-demand) 추론을 가능하게 합니다.

**다음 단계는 무엇인가?**
향후 연구는 Thoughtbubbles를 RL 또는 탐색(예: MCTS)과 결합하여 추론 단계 전반에 걸쳐 병렬 계산을 할당할 수 있습니다. 버블의 깊이, 병합 전략을 탐구하고, 이 아이디어를 멀티모달 모델에 적용하면 추가적인 이점을 얻을 수 있습니다. 또한, 해석 가능성 연구를 통해 버블이 특정 추론 패턴과 어떻게 일치하는지 밝혀내어, AI의 내부 작동 방식을 더 깊이 이해하는 데 기여할 수 있습니다.

---

### 보기 전에 보는 법 배우기 – LLM의 시각적 선행 지식 해명

**관련 논문**: 보기 전에 보는 법 배우기 (논문)

**어떤 문제가 해결되는가?**
텍스트만으로 훈련된 대규모 언어 모델이 이미지를 한 번도 본 적이 없음에도 불구하고 '하늘은 파란색인가요?'와 같은 간단한 시각적 질문에 놀랍도록 잘 답변하는 경우가 많습니다. 이러한 시각적 사전 지식은 어디서 오는 것이며, 어떻게 의도적으로 이를 배양할 수 있을까요? 이 현상은 LLM의 '암흑 물질'과 같으며, 그 기원을 이해하는 것은 멀티모달 AI의 미래를 밝히는 데 중요합니다.

**어떻게 문제를 해결하는가?**
연구자들은 LLM이 언어 사전 훈련 과정에서 두 가지 유형의 시각적 사전 지식을 습득하는 방식을 분석합니다. 하나는 코드, 수학, 과학 문서와 같은 추론 중심 텍스트에서 유래하는 '시각적 추론 사전 지식'이며, 이는 시각적 개념을 논리적으로 연결하는 방법을 가르칩니다. 다른 하나는 일상적인 장면 묘사를 포함하는 광범위한 자연어 코퍼스에서 비롯되는 '시각적 지각 사전 지식'입니다. 그들은 추론 사전 지식이 모델 크기에 따라 강력하게 확장되며 최소한의 이미지 노출로도 시각 작업으로 전이될 수 있음을 보여줍니다. 반면, 지각 사전 지식은 빠르게 포화 상태에 이르며 좋은 비전 인코더와의 결합에 의존합니다.

**주요 발견**
*   **추론 vs. 지각 사전 지식의 차이**: 추론 사전 지식은 구조화된 텍스트에서 오고 모델 크기에 따라 확장되는 반면, 지각 사전 지식은 광범위한 언어에서 오고 빠르게 포화됩니다. 이는 데이터 유형별로 얻을 수 있는 시각적 지식의 특성이 다름을 시사합니다.
*   **데이터 효율성의 중요성**: 적당한 양의 추론 중심 텍스트는 시각적 추론 능력을 크게 향상시킵니다. 반면, 더 많은 묘사적 텍스트를 포함하는 것은 효용 체감을 가져와, 데이터의 양보다는 질과 종류가 중요함을 보여줍니다.
*   **시각 인식 LLM을 위한 최적화된 레시피**: 사전 훈련 혼합을 제어함으로써 저자들은 최소한의 이미지 미세 조정 후 시각 작업에서 더 나은 성능을 보이는 모델을 생성합니다. 이는 멀티모달 AI를 위한 데이터 중심 설계의 가능성을 열어줍니다.

**다음 단계는 무엇인가?**
향후 연구는 이러한 사전 지식을 연속 잠재 CoT(Thoughtbubbles 참조) 또는 생성형 확산 모델과 결합하는 것을 탐색할 수 있습니다. 또한, 명시적인 공간 추론 작업으로 사전 훈련하면 훨씬 더 강력한 사전 지식을 구축할 수 있습니다. 저자들의 데이터 중심 접근 방식은 고품질 추론 텍스트를 큐레이션하는 것이 더 유능한 멀티모달 모델을 향한 발전을 가속화할 수 있음을 시사합니다.

---

### Knapsack RL – 예산 최적화를 통한 탐색의 효율 극대화

**관련 논문**: Knapsack RL (논문)

**어떤 문제가 해결되는가?**
강화 학습 미세 조정에서는 종종 문제당 고정된 수의 롤아웃을 사용합니다. 이러한 획일적인 자원 배분은 계산 자원을 비효율적으로 낭비합니다. 쉬운 작업은 적은 시도만으로도 해결될 수 있고, 극도로 어려운 작업은 주어진 예산으로는 성공 가능성이 거의 없습니다. 결과적으로 많은 작업이 유의미한 기울기를 생성하지 못하여 학습 시간을 지연시키며, 이는 RL 훈련의 주요 병목 현상 중 하나입니다.

**어떻게 문제를 해결하는가?**
Knapsack RL은 탐색 과정을 배낭 최적화 문제로 간주합니다. 각 작업의 롤아웃은 '비용'(계산량)과 잠재적인 '가치'(예상 학습 이득)를 가집니다. 이 알고리즘은 전체 작업에 걸쳐 배낭 문제를 해결함으로써, 학습 이득이 높을 것으로 예상되는 작업에 더 많은 롤아웃을 할당하고, 이미 해결되었거나 가망 없는 작업에는 적게 할당합니다. 이 동적인 예산 책정은 GRPO 알고리즘을 기반으로 하며, AI 훈련에 경제적 효율성의 개념을 도입합니다.

**주요 발견**
*   **기울기 밀도의 증가**: 0이 아닌 정책 기울기의 비율이 20–40% 증가하며, 이는 에이전트가 각 업데이트마다 더 많은 작업에서 학습한다는 것을 의미합니다. 이는 학습 신호의 밀도를 높여 훈련 속도를 가속화합니다.
*   **어려운 작업에 대한 대규모 예산 할당**: 일부 극도로 어려운 작업은 약 93개의 롤아웃을 받으며, 균일한 예산으로는 결코 허용되지 않았을 탐색을 가능하게 합니다. 이는 모델이 이전에 접근하기 어려웠던 문제에 도전할 기회를 제공합니다.
*   **성능 향상**: 수학 추론 벤치마크에서 Knapsack RL은 균일한 할당이 요구하는 계산 자원의 절반을 사용하면서 평균 2–4점, 특정 사례에서는 최대 9점의 개선을 달성합니다. 이는 계산 효율성과 성능 향상을 동시에 이루어냈음을 보여줍니다.

**다음 단계는 무엇인가?**
Knapsack RL을 BroRL과 결합하면 훨씬 더 나은 결과를 얻을 수 있습니다. 이는 전역적으로 탐색을 확장하면서 지역적으로 예산을 최적화하는 시너지 효과를 창출할 것입니다. 또한 탐색을 더욱 향상시키기 위해 MCTS 기반 훈련(DeepSearch 참조) 또는 다양성 목표(Polychromic RL 참조)와 짝을 이룰 수 있습니다. 또 다른 방향은 배낭 문제 아이디어를 다중 에이전트 또는 계층적 작업으로 확장하여, 예산을 하위 작업에 분할해야 하는 복잡한 시나리오에 적용하는 것입니다.

---

### TruthRL – 강화 학습을 통한 LLM의 진실성 함양

**관련 논문**: TruthRL (논문)

**어떤 문제가 해결되는가?**
대규모 언어 모델은 종종 사실이 아닌 그럴듯한 답변을 생성하는 환각(hallucination) 현상을 보입니다. 순전히 정확성만을 기준으로 하는 RL 보상은 이러한 추측성 답변을 조장할 수 있습니다. 반대로, 모델이 너무 조심하여 답변을 거부하면 유용성이 떨어집니다. 정확성, 진실성, 그리고 적절한 회피("모르겠습니다" 포함) 사이의 균형을 어떻게 찾을 수 있을까요? 이는 AI 신뢰성 구축의 핵심 과제입니다.

**어떻게 문제를 해결하는가?**
TruthRL은 RL 미세 조정에 삼진 보상 체계를 도입합니다. 정답에는 긍정적 보상을, 환각에는 큰 페널티를, 그리고 '모르겠습니다'와 같은 정직한 회피에는 약간의 긍정적 보상을 부여합니다. 훈련은 GRPO 알고리즘을 사용하여, 모델이 확신할 때만 답변하고 그렇지 않을 때는 불확실성을 인정하도록 장려합니다. 이 방법은 외부 검색 유무에 관계없이 개방형 도메인 QA와 같은 다양한 작업에 적용되며, AI가 '모른다'고 말하는 것의 가치를 인정합니다.

**주요 발견**
*   **환각 현상 감소**: TruthRL은 여러 지식 집약적 벤치마크에서 바닐라 RL 대비 환각 발생률을 28.9% 감소시킵니다. 이는 모델의 정보 정확성을 크게 향상시키는 중요한 결과입니다.
*   **전반적인 진실성 향상**: 전체적인 진실성(정확성 + 정직한 회피)이 21.1% 향상됩니다. 이는 AI가 사용자에게 더욱 신뢰할 수 있는 정보를 제공할 수 있음을 의미합니다.
*   **광범위한 적용 가능성**: Qwen, Llama와 같은 다양한 모델 유형과 검색 증강 및 검색 없는 설정 모두에서 이점이 지속됩니다. 이는 TruthRL의 일반성과 견고성을 보여줍니다.

**다음 단계는 무엇인가?**
TruthRL은 공정성 목표 또는 다른 정렬 지표와 결합하여 진실할 뿐만 아니라 윤리적으로도 답변하는 모델을 생성할 수 있습니다. 향후 연구는 더 미묘한 보상 구조(예: 난이도에 따라 조정) 또는 다중 턴 대화에 TruthRL을 적용하는 것을 탐색할 수 있습니다. 또한, 불확실할 때 회피를 장려함으로써 개인 정보 유출을 완화하기 위해 비밀 정보 추출 연구와 결합되어, 모델의 안전성과 신뢰성을 동시에 강화할 수 있습니다.

---

### MemGen – 자기 진화형 에이전트를 위한 생성형 잠재 기억 시스템

**관련 논문**: MemGen (논문 / 코드)

**어떤 문제가 해결되는가?**
LLM 에이전트는 종종 제한적인 기억 용량을 가집니다. 기존 방식은 미세 조정 중 매개변수를 재구성하거나(매개변수적 기억), 외부 데이터베이스를 조회하는(비매개변수적 기억) 것이었습니다. 이러한 접근법은 경직되거나 추론 과정과 단절될 수 있습니다. 에이전트가 스스로 생성하고 내부 사고 과정에 긴밀하게 통합할 수 있는 동적이고 연결된 기억 시스템이 필요합니다. 인간의 인지처럼 상황에 맞춰 기억을 생성하고 활용하는 능력이 에이전트에게도 요구됩니다.

**어떻게 문제를 해결하는가?**
MemGen은 두 가지 핵심 구성 요소로 이루어진 생성형 잠재 기억 시스템을 제안합니다. '기억 트리거'는 에이전트의 현재 추론 상태를 모니터링하여 언제 기억을 불러와야 할지 결정하고, '기억 위버'는 트리거될 때 관련 기억 내용을 나타내는 잠재 토큰 시퀀스를 생성하여 이를 모델의 컨텍스트에 다시 주입합니다. 기억은 일반 텍스트가 아닌 모델의 잠재 공간에서 학습되고 표현됩니다. 이 시스템은 에이전트가 일시 중지하고, 내부 기억을 생성한 다음, 해당 기억이 숨겨진 상태에 융합된 상태로 추론을 재개할 수 있도록 합니다. 에이전트는 작업 기억, 절차 기억, 계획 기억 유형에 걸쳐 기억 내용을 할당하는 방법을 학습합니다.

**주요 발견**
*   **뛰어난 성능 향상**: 8개의 다양한 벤치마크에서 MemGen은 기존 기억 증강 에이전트(ExpeL, AWM)를 최대 38.22% 능가하고, 강력한 GRPO 기준선을 최대 13.44% 초과합니다. 이는 MemGen의 혁신적인 기억 시스템이 에이전트 성능에 미치는 강력한 영향을 보여줍니다.
*   **인간 인지 유사한 기억 패턴 발현**: 수동 코딩 없이도 MemGen 에이전트는 인간 인지를 연상시키는 계획 기억, 절차 기억, 작업 기억과 유사한 기억 행동을 자발적으로 개발합니다. 이는 AI가 더욱 자연스럽고 유연한 방식으로 정보를 처리할 수 있음을 시사합니다.
*   **교차 도메인 일반화 능력**: 생성형 기억은 여러 도메인(수학, 프로그래밍, Q&A)에서 성능을 향상시키며, 이는 MemGen의 광범위한 적용 가능성을 입증합니다.

**다음 단계는 무엇인가?**
MemGen은 추론을 더욱 향상시키기 위해 탐색 기반 훈련(예: DeepSearch) 또는 탐색 확장(BroRL)과 통합될 수 있습니다. 연구자들은 또한 최적의 보상을 위해 언제 무엇을 불러올지 학습하도록 기억 트리거와 위버를 RL과 함께 공동으로 훈련하는 것을 탐색할 수 있습니다. 마지막으로, MemGen을 Dragon Hatchling과 같은 생물학적 영감 아키텍처와 결합하면 뇌와 유사한 기억 메커니즘과 네트워크 구조를 모두 갖춘 더욱 정교한 에이전트를 개발할 수 있을 것입니다.

---

### 종합 분석 – 핵심 통찰과 미래 발전 방향

이번 LLM Watch에서 다룬 논문들을 면밀히 검토해 보면, 인공지능 연구의 주요 흐름들이 놀랍도록 긴밀하게 연결되고 있음을 알 수 있습니다.
*   **탐색 전략의 중요성 재확인**: BroRL, DeepSearch, Knapsack RL, 그리고 Polychromic RL과 같은 연구들은 모두 강화 학습의 핵심 과제인 '탐색'을 어떻게 효과적으로 확장하고 배분할 것인가에 대한 다양한 해법을 제시합니다. 롤아웃을 늘리거나, 탐색 과정을 모델 내부에 내장하거나, 자원 예산을 최적화하는 등, 이 연구들은 RL이 흔히 겪는 성능 정체 현상을 극복하려는 공통된 목표를 가지고 있습니다. 이는 AI가 새로운 지식과 해결책을 찾는 능력을 근본적으로 향상시키는 데 필수적인 요소입니다.
*   **혁신적인 아키텍처 설계**: Thoughtbubbles는 비지도 학습 기반의 아키텍처 수정만으로 LLM이 병렬적으로 사고할 수 있음을 보여주며, MemGen은 생성형 잠재 기억을 도입하여 모델의 인지 능력을 심화시킵니다. 또한, Dragon Hatchling은 트랜스포머를 뇌와 유사한 네트워크에 연결하여 생물학적 영감을 통한 새로운 모델 구조의 가능성을 탐구합니다. 이러한 혁신들은 단순히 모델의 크기를 키우는 것을 넘어, 정보 처리 및 저장 방식에 대한 새로운 패러다임을 제시합니다.
*   **데이터 품질과 일반화 능력의 결정적 역할**: "보기 전에 보는 법 배우기"와 "SFT 일반화 신화 반박하기" 논문들은 훈련 데이터의 양보다는 질과 다양성이 얼마나 중요한지를 강조합니다. 추론 중심 텍스트를 선별하거나, 다양한 프롬프트와 사고 과정 예제를 큐레이션함으로써 LLM의 숨겨진 잠재력과 일반화 능력을 극대화할 수 있음을 보여줍니다. 이는 데이터 중심 AI 접근 방식이 미래 모델 개발의 핵심이 될 것임을 시사합니다.
*   **모델 감사와 윤리적 정렬의 중요성**: "비밀 지식 유도하기"와 TruthRL은 모델의 내부 지식을 감사하고, 진실하고 신중하게 행동하도록 훈련하는 것이 얼마나 중요한지를 역설합니다. AI 시스템이 더욱 강력해지고 보편화될수록, 그들의 투명성, 신뢰성, 그리고 윤리적 가이드라인 준수는 필수불가결한 요소가 됩니다. 이러한 연구들은 AI의 책임감 있는 발전을 위한 중요한 토대를 마련합니다.

앞으로 이러한 통찰력들이 결합되어 다음 세대의 에이전트를 탄생시킬 것입니다. 즉, 광범위한 탐색 능력, 정밀하게 조정된 진실성, 다양한 감독 기법으로 훈련된 RL, 그리고 생성형 기억과 병렬 추론 능력을 갖춘 대규모 생물학적 영감 네트워크를 기반으로 한 AI 시스템이 등장할 것입니다. 이는 단순히 기술적 진보를 넘어, 인간의 인지 방식에 더 가까워지는 AI를 향한 여정입니다.

AI의 미래는 신경과학, 최적화 이론, 그리고 안전 연구를 아우르는 다학제적 혁신을 통해 더욱 풍성해질 가능성이 높습니다. 계속해서 이 흥미로운 여정을 함께 지켜봐 주십시오!

저희 소식지를 구독해 주시고, 만약 이 내용이 유익하셨다면 '마음에 들어요'를 클릭하고 주변 동료들과 함께 나누어 주시면 감사하겠습니다.