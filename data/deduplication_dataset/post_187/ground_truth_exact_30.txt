환영합니다, Watcher님! 이번 주 LLM Watch 소식입니다:

**정체 극복을 위한 확장**: 광범위한 탐색(BroRL), MCTS 통합(DeepSearch), 배낭 문제 방식의 예산 책정(knapsack-style budgeting)이 정체된 RL 실행(RL runs)을 재활성화합니다.

**더 오래가 아닌 더 잘 생각하는 에이전트**: 잠재 병렬 사고(Thoughtbubbles)와 생성형 잠재 기억(MemGen)

**진실 > 분위기**: TruthRL은 정직함("모르겠습니다" 포함)에 보상하는 반면, 비밀 유도(secret-elicitation) 연구는 숨겨진 사실이 여전히 유출될 수 있음을 보여줍니다.

**연구실에서 리더보드로**: GEM은 에이전트형 LLM(agentic LLMs)을 위한 훈련/평가를 표준화합니다. SFT 신화는 프롬프트 다양성(prompt diversity) + CoT로 반박됩니다.

**뇌 및 시각 사전 지식**: Dragon Hatchling은 트랜스포머(transformers)를 뇌와 유사한 네트워크에 연결하며, 텍스트 전용 사전 훈련(text-only pretraining)으로 시각 사전 지식(visual priors)을 형성할 수 있습니다.

최근 대규모 언어 모델(LLM) 연구는 단순한 모델 크기 확장을 넘어, 학습 효율성, 추론 능력, 그리고 안전성 측면에서 괄목할 만한 발전을 이루고 있습니다. 특히 강화 학습(RL)의 한계를 극복하고, 에이전트의 자율성을 높이며, 훈련 데이터의 잠재력을 극대화하는 다양한 방법론이 제시되고 있습니다. 이번 업데이트에서는 이러한 핵심 트렌드를 심층적으로 다루고, LLM의 미래를 엿볼 수 있는 최신 연구들을 소개합니다.

---

**빠른 용어집 - 본 이슈에 맞춤**

*   **RLVR (Reinforcement Learning with Verifiable Rewards)**: 정확성이 자동으로 확인 가능한 강화 학습(RL) (예: 수학 정답, 단위 테스트)으로, 보상이 사람의 레이블에 의존하지 않습니다.
*   **비밀 유도(Secret elicitation, 모델 감사)**: 모델이 내부적으로 "알고 있지만" 명시하지 않는 사실을 이끌어내는 프롬프트(prompt) 또는 분석 기법입니다. 예를 들어, 블랙박스 프리필(black-box prefill, 시드 완성), 페르소나 샘플링(persona sampling), 또는 로짓 렌즈(logit lens) 및 희소 오토인코더(sparse autoencoders)와 같은 화이트박스(white-box) 도구가 있습니다.
*   **CoT 감독(CoT supervision, SFT용)**: 모델이 더 어려운 사례로 전이되는 알고리즘적 틀(algorithmic scaffold)을 학습하도록 단계별 솔루션으로 훈련하는 것입니다.
*   **잠재 병렬 사고(Latent parallel thinking)**: 트랜스포머(transformer)가 내부적으로 잔차 스트림(residual stream)을 분기하여 어려운 토큰에 병렬로 추가 연산(extra compute)을 할당하도록 함으로써, 명시적인 사고의 사슬(chain-of-thought)은 필요 없습니다.
*   **텍스트로부터의 시각 사전 지식(Visual priors from text)**: 추론 중심 텍스트(코드/수학/과학)는 시각적 추론 사전 지식(visual reasoning prior)을 구축하고, 광범위한 언어는 지각 사전 지식(perception prior)을 구축합니다. 이는 나중에 좋은 비전 인코더(vision encoder) + 약간의 멀티모달 미세 조정(multimodal finetune)으로부터 이점을 얻습니다.

---

### RL 기반 LLM 최적화의 새로운 지평

강화 학습(RL)은 LLM의 성능을 비약적으로 향상시킬 수 있는 강력한 도구이지만, 종종 탐색(exploration) 부족과 비효율적인 자원 할당이라는 문제에 직면합니다. 최근 연구들은 이러한 한계를 극복하기 위한 혁신적인 접근 방식을 제시하고 있습니다.

#### BroRL – 광범위한 탐색을 통한 강화 학습 확장

**시청**: BroRL (논문)

**어떤 문제를 해결하는가?**
검증 가능한 보상을 통한 강화 학습(RLVR)은 정답에 보상함으로써 추론을 개선하지만, 성능은 일반적으로 수천 번의 훈련 단계(training steps) 후에 포화 상태에 이릅니다. 모델은 탐색(explore)을 너무 적게 하기 때문에 개선을 멈춥니다. 훈련을 계속하려는 시도는 수확 체감(diminishing returns)을 초래합니다.

**어떻게 문제를 해결하는가?**
BroRL은 예제당 롤아웃(rollouts) 수를 수백 개로 극적으로 늘려 탐색(exploration)을 확장할 것을 제안합니다. 단순히 훈련 단계(training steps)를 늘리는 대신, BroRL은 동일한 수의 기울기 업데이트(gradient updates)를 유지하면서 훨씬 더 많은 궤적(trajectories)을 샘플링하여 탐색 공간(search space)을 넓힙니다. 간단한 이론적 분석에 따르면, 단일 단계 RL 설정(one-step RL setting)에서 각 롤아웃(rollout)은 올바른 행동에 긍정적인 확률 질량(probability mass)을 기여하며, 롤아웃이 증가함에 따라 샘플링되지 않은 행동의 영향은 사라집니다. 따라서 BroRL은 샘플 수가 증가할 때 전반적인 개선을 보장합니다.

**주요 발견**
*   **이론적 보장**: 질량 균형 논증(mass-balance argument)은 더 많은 롤아웃(rollouts)을 샘플링하는 것이 올바른 확률 질량(probability mass)을 단조롭게 확장하고 보이지 않는 행동의 영향을 줄인다는 것을 증명합니다.
*   **포화된 모델 재활성화**: BroRL은 약 3천 번의 ProRL 단계(steps) 후에 정체된 모델을 재활성화하여, ProRL이 포화 상태에 이르렀을 때에도 지속적인 성능 향상을 달성합니다.
*   **최첨단 결과**: 15억(1.5B) 매개변수(parameter) 모델의 경우, BroRL은 다양한 벤치마크(benchmarks)에서 최첨단 성능을 달성하며, 탐색 확장(exploration scaling)이 훈련 단계(training steps)를 늘리는 것보다 더 효과적일 수 있음을 보여줍니다.

**최신 적용 및 시사점**
BroRL은 정체된 모델을 재활성화하여 지속적인 성능 향상을 달성할 뿐만 아니라, 최근에는 복잡한 추론 문제 해결에 있어 탐색의 질과 양 모두를 향상시키는 데 기여하고 있습니다. 예를 들어, 수학적 추론 벤치마크(benchmarks)에서 BroRL은 훈련 단계를 늘리는 것보다 훨씬 효율적으로 최첨단 성능을 달성했으며, 이는 탐색 확장(exploration scaling)이 LLM 훈련의 핵심임을 보여줍니다.

**다음은 무엇인가?**
향후 연구는 BroRL을 구조화된 탐색 방법(structured search methods, 예: MCTS) 또는 예산 할당 방식(budget allocation schemes, Knapsack RL 참조)과 결합하여 탐색을 더욱 최적화할 수 있습니다. BroRL은 또한 단일 단계 RL(one-step RL)을 넘어 완전한 순차적 설정(sequential settings)으로 확장되거나, 더 복잡한 작업을 위해 MemGen과 같은 기억 증강 아키텍처(memory-augmented architectures)와 통합될 수 있습니다.

#### DeepSearch – 몬테카를로 트리 탐색을 통한 RLVR 병목 현상 극복

**시청**: DeepSearch (논문)

**어떤 문제를 해결하는가?**
RLVR에서 훈련 성능은 종종 포화 상태에 이릅니다. 이는 소수의 샘플링된 궤적(trajectories)이 가능한 모든 추론 경로를 거의 포착하지 못하기 때문입니다. 훈련이 진행됨에 따라 모델의 행동은 점점 더 결정론적(deterministic)이 되어, RL은 분산(variance)이 줄어들고 개선을 멈춥니다.

**DeepSearch의 핵심 혁신**
BroRL이 '넓은' 탐색을 지향한다면, DeepSearch는 몬테카를로 트리 탐색(MCTS)을 RL 훈련 루프(training loop)에 통합하여 '깊고 구조화된' 탐색을 가능하게 합니다. 에이전트(agent)는 몇 개의 무작위 롤아웃(random rollouts)을 샘플링하는 대신, 훈련 중에 추론 궤적(reasoning trajectories)에 대한 구조화된 탐색(structured search)을 수행합니다. 전역 경계 선택(Global frontier selection)과 엔트로피 기반 경로 선택(Entropy-based path selection)을 통해 유망한 추론 경로에 집중하고, 솔루션 캐싱(solution caching)을 통해 중복 탐색을 줄여 효율성을 극대화합니다.

**주요 발견**
*   **탐색 정체 해결**: 추론 경로를 체계적으로 탐색함으로써 DeepSearch는 확장된 RLVR 훈련에서 관찰되는 성능 저하를 방지합니다.
*   **최첨단 정확도**: 수학 추론 벤치마크(benchmarks)에서 DeepSearch로 훈련된 15억(1.5B) 모델은 평균 62.95%의 정확도를 달성하며, 이는 이전 RLVR 방법보다 높고, 표준 RL 훈련을 확장하는 것보다 5.7배 적은 GPU 시간을 사용합니다.
*   **효율적인 탐색**: 구조화된 탐색(structured search)은 더 나은 샘플 효율성(sample efficiency)을 제공합니다. 에이전트(agent)는 대량의 무작위 롤아웃(random rollouts) 없이도 올바른 전략을 학습합니다.

**다음은 무엇인가?**
DeepSearch는 수학 외의 도메인(domains), 예를 들어 코드 합성(code synthesis) 또는 정리 증명(theorem proving)에도 적용될 수 있습니다. MCTS를 동적 롤아웃 예산(dynamic rollout budgets, Knapsack RL 방식)과 결합하면 추가적인 개선을 가져올 수 있습니다. 또한 미분 가능한 탐색(differentiable search, 예: 미분 가능한 MCTS)을 탐색하면 기울기 역전파(gradient backpropagation)를 통한 종단 간(end-to-end) 훈련이 가능해질 수 있습니다.

#### Knapsack RL – 동적 예산 할당을 통한 효율성 극대화

**시청**: Knapsack RL (논문)

**어떤 문제를 해결하는가?**
강화 학습 미세 조정(Reinforcement learning fine-tuning)은 종종 문제당 고정된 수의 롤아웃(rollouts)을 사용합니다. 이러한 균일한 할당(uniform allocation)은 연산(compute)을 낭비합니다. 쉬운 작업은 적은 시도(trials)만 필요하고, 극도로 어려운 작업은 적은 예산으로는 결코 성공하지 못할 수 있습니다. 결과적으로 많은 작업이 제로 기울기(zero gradients)를 생성하여 시간을 낭비하고 학습을 방해합니다.

고정된 롤아웃 예산은 RL 미세 조정(fine-tuning)에서 비효율적인 자원 사용을 초래합니다. Knapsack RL은 각 작업의 학습 이득(예상 학습 이득)과 연산 비용을 고려하여 동적으로 롤아웃(rollouts)을 할당함으로써 이 문제를 해결합니다. 알고리즘은 배낭 최적화 문제(knapsack optimization problem)를 풀어 예상 이득이 높은 작업에 더 많은 롤아웃을 할당하고, 이미 해결되었거나 가망 없는 작업에는 더 적게 할당합니다. 이 동적 예산 책정(dynamic budgeting)은 GRPO 알고리즘(algorithm)을 기반으로 합니다. 이를 통해 Knapsack RL은 0이 아닌 정책 기울기(policy gradients)의 비율을 20-40% 증가시키며, 어려운 작업에 대규모 예산을 할당하여 탐색(exploration)을 가능하게 합니다. 수학 추론 벤치마크에서 Knapsack RL은 균일한 할당(uniform allocation)보다 적은 연산으로 더 나은 성능을 보여주었습니다. 이는 LLM 훈련에서 자원 효율성을 극대화하는 중요한 방향을 제시합니다.

**다음은 무엇인가?**
Knapsack RL을 BroRL과 결합하면 훨씬 더 나은 결과를 얻을 수 있습니다. 전역적으로 탐색(exploration)을 확장하고 지역적으로 예산(budgets)을 할당하는 것입니다. 또한 탐색(exploration)을 더욱 향상시키기 위해 MCTS 기반 훈련(DeepSearch 참조) 또는 다양성 목표(diversity objectives, Polychromic RL 참조)와 짝을 이룰 수 있습니다. 또 다른 방향은 배낭 문제(knapsack idea)를 다중 에이전트(multi-agent) 또는 계층적 작업(hierarchical tasks)으로 확장하여 예산(budgets)을 하위 작업(subtasks)에 분할해야 하는 경우입니다.

#### TruthRL – 진실성과 신중함을 위한 강화 학습

**시청**: TruthRL (논문)

**어떤 문제를 해결하는가?**
LLM은 종종 환각(hallucinate)을 일으켜 그럴듯하지만 거짓된 답변을 지어냅니다. 순전히 정확성 기반의 RL 보상(RL rewards)은 추측을 조장할 수 있습니다. 반대로 모델은 지나치게 조심하여 답변을 거부할 수 있으며, 이는 유용성을 저해합니다. 정확성, 진실성, 적절한 회피(abstention) 사이의 균형을 어떻게 맞출 수 있을까요?

LLM의 환각(hallucination) 문제는 여전히 중요한 해결 과제이며, 순전히 정확성 기반의 RL 보상(RL rewards)은 종종 추측을 조장할 수 있습니다. TruthRL은 RL 미세 조정(fine-tuning)에 삼진 보상(ternary reward)을 도입하여 이 문제를 해결합니다. 정답에는 긍정적인 보상, 환각(hallucinations)에는 큰 페널티, 그리고 "모르겠습니다"와 같은 정직한 회피(abstention)에는 약간의 긍정적인 보상을 제공합니다. 이 접근 방식은 모델이 확신할 때만 답변하고, 불확실할 때는 이를 인정하도록 장려함으로써 환각(hallucination) 발생률을 28.9% 감소시키고 전반적인 진실성(truthfulness)을 21.1% 향상시켰습니다. TruthRL은 Qwen, Llama와 같은 다양한 모델 유형과 검색 증강(retrieval-augmented) 및 검색 없는(retrieval-free) 설정 모두에서 이점을 제공하며, 모델의 신뢰성을 높이는 데 필수적인 역할을 합니다.

**다음은 무엇인가?**
TruthRL은 공정성 목표(fairness objectives) 또는 다른 정렬 지표(alignment metrics)와 결합하여 진실할 뿐만 아니라 윤리적으로도 답변하는 모델을 생성할 수 있습니다. 향후 연구는 더 미묘한 보상 구조(reward structures, 예: 난이도에 따라 조정) 또는 다중 턴 대화(multi-turn dialogues)를 탐색할 수 있습니다. 또한 불확실할 때 회피를 장려함으로써 개인 정보 유출을 완화하기 위해 비밀 유도 연구(secret elicitation research)와 결합될 수 있습니다.

### 에이전트형 LLM의 지능 확장

LLM은 이제 단순한 언어 생성을 넘어, 복잡한 작업을 수행하고 환경과 상호작용하며, 도구를 사용하는 에이전트(agent)로 진화하고 있습니다. 이러한 에이전트의 능력 확장은 모델 아키텍처와 학습 방식의 혁신을 통해 이루어지고 있습니다.

#### GEM – 에이전트형 LLM을 위한 표준화된 벤치마크

**시청**: GEM (논문)

**어떤 문제를 해결하는가?**
에이전트형 LLM(agentic LLMs)을 개발하고 평가하려면 훈련 및 벤치마킹을 위한 표준화된 환경이 필요합니다. OpenAI Gym과 같은 기존 환경은 로봇 공학 또는 장난감 작업(toy tasks)을 대상으로 하는 반면, 에이전트형 LLM 연구는 공통 플랫폼이 부족했습니다.

**어떻게 문제를 해결하는가?**
GEM (General Experience Maker)은 LLM 에이전트(LLM agents)를 위해 특별히 맞춤화된 오픈 소스 환경 시뮬레이터(environment simulator)입니다. 이는 비동기적(asynchronous), 벡터화된 실행(vectorized execution, 다중 병렬 시뮬레이션), 유연한 래퍼(wrappers), 통합 도구(예: Python 코드 실행, 검색)를 지원하는 환경과 에이전트(agent) 간의 표준 인터페이스(interface)를 정의합니다. GEM은 수학, 코드, Q&A, 도구 사용을 포함하는 다양한 환경 스위트(suite)와 반환 배치 정규화(Return Batch Normalization, ReBN)가 적용된 REINFORCE와 같은 RL 알고리즘(RL algorithms)을 위한 기준선 스크립트(baseline scripts)를 제공합니다. 또한 평가 툴킷(evaluation toolkit) 역할도 합니다. 연구자들은 자신의 에이전트(agents)를 연결하여 비교 가능한 지표(metrics)를 얻을 수 있습니다.

**주요 발견**
*   **포괄적인 환경 라이브러리**: GEM은 수학적 추론부터 API 호출까지 풍부한 관찰/행동 공간(observation/action spaces)을 가진 24개의 환경을 포함합니다. 단일 단계(single-step) 및 다단계(multi-step) 작업을 모두 지원합니다.
*   **ReBN을 사용한 기준선**: 저자들은 일반적인 RL 알고리즘(REINFORCE, GRPO, PPO)을 벤치마킹하고, ReBN이 정책 기울기(policy gradients)를 안정화하여 턴당 밀집 보상(dense per-turn rewards)으로 훈련할 수 있도록 돕는다는 것을 보여줍니다.
*   **재사용 가능한 평가 하네스**: GEM 인터페이스(interface)와 래퍼(wrappers)는 새로운 작업과 에이전트 아키텍처(agent architectures)의 쉬운 통합을 가능하게 합니다. 이는 에이전트형 LLM을 위한 "미니 아레나(MiniArena)"와 유사한 표준화된 테스트베드(testbed) 역할을 합니다.

**다음은 무엇인가?**
최근 GEM은 단순한 텍스트 기반 상호작용을 넘어, 멀티모달 입력(multimodal inputs, 이미지 또는 오디오)을 처리하는 에이전트의 개발을 지원하기 위해 환경 라이브러리(environment library)를 확장하고 있으며, 이는 실제 세계와의 상호작용 능력을 크게 향상시킬 것으로 기대됩니다. GEM은 에이전트형 RL 연구의 진입 장벽을 낮춥니다. 향후 연구는 환경 라이브러리(environment library)를 확장하여 멀티모달 입력(multimodal inputs, 이미지 또는 오디오)을 통합하거나, 장기 계획(long-horizon planning)을 위한 벤치마크(benchmarks)를 추가하거나, AgentScaler와 같은 대규모 에이전트형 파운데이션 모델(agentic foundation models)과 GEM을 통합할 수 있습니다. 연구자들은 또한 GEM의 벡터화된 인터페이스(vectorized interface)를 기반으로 다중 에이전트(multiple agents) 간의 조정을 요구하는 더 현실적인 작업을 설계할 수 있습니다.

#### MemGen – 자기 진화 에이전트를 위한 생성형 잠재 기억 엮기

**시청**: MemGen (논문 / 코드)

**어떤 문제를 해결하는가?**
LLM 에이전트(agents)는 종종 제한된 기억(memory)을 가집니다. 미세 조정(fine-tuning) 중에 매개변수(parameters)를 다시 작성하거나(매개변수 기억, parametric memory) 외부 데이터베이스(database)를 쿼리합니다(비매개변수 기억, nonparametric memory). 이러한 접근 방식은 경직되거나 추론과 단절될 수 있습니다. 에이전트(agent)가 생성하고 내부 사고 과정에 엮어 넣을 수 있는 동적이고 긴밀하게 연결된 기억(memory)이 필요합니다.

**어떻게 문제를 해결하는가?**
MemGen은 두 가지 핵심 구성 요소(components)를 가진 생성형 잠재 기억 시스템(generative latent memory system)을 도입합니다.
*   **기억 트리거(Memory trigger)**: 에이전트(agent)의 현재 추론 상태(reasoning state)를 모니터링하고 언제 기억을 불러올지 결정하는 모듈(module)입니다. 과거 경험이나 사실을 불러와야 하는 순간을 감지합니다.
*   **기억 위버(Memory weaver)**: 트리거되면 이 모듈(module)은 관련 기억 내용(memory content)을 나타내는 잠재 토큰(latent tokens) 시퀀스(sequence)를 생성하고 이를 모델의 컨텍스트(context)에 다시 주입합니다. 기억은 일반 텍스트가 아닌 모델의 잠재 공간(latent space)에서 학습되고 표현됩니다.
이 시스템은 에이전트(agent)가 일시 중지하고, 내부 기억(internal memory)을 생성한 다음, 해당 기억이 숨겨진 상태(hidden state)에 융합된 상태로 추론을 재개할 수 있도록 합니다. 에이전트(agent)는 작업 기억(working memory), 절차 기억(procedural memory), 계획 기억(planning memory) 유형에 걸쳐 기억 내용(memory content)을 할당하는 방법을 학습합니다.

**주요 발견**
*   **성능 향상**: 8개의 다양한 벤치마크(benchmarks)에서 MemGen은 기존 기억 증강 에이전트(memory-augmented agents, ExpeL, AWM)를 최대 38.22% 능가하고, 강력한 GRPO 기준선(baseline)을 최대 13.44% 초과합니다.
*   **인간과 유사한 기억 패턴**: 수동 코딩(hand-coding) 없이도 MemGen 에이전트(agents)는 인간 인지(human cognition)를 연상시키는 계획 기억(planning memory), 절차 기억(procedural memory), 작업 기억(working memory)과 유사한 기억 행동을 자발적으로 개발합니다.
*   **교차 도메인 일반화**: 생성형 기억(generative memory)은 여러 도메인(domains, 수학, 프로그래밍, Q&A)에서 성능을 향상시키며, 이는 광범위하게 적용 가능함을 시사합니다.

**MemGen의 작동 원리와 미래**
MemGen은 에이전트의 제한된 기억(memory) 문제를 해결하기 위해, 현재 추론 상태(reasoning state)를 모니터링하여 언제 기억을 불러올지 결정하는 '기억 트리거(Memory trigger)'와, 관련 기억 내용(memory content)을 잠재 토큰(latent tokens) 시퀀스(sequence)로 생성하여 모델의 컨텍스트(context)에 주입하는 '기억 위버(Memory weaver)'를 도입합니다. 이 시스템은 에이전트가 단순히 과거 정보를 불러오는 것을 넘어, 모델의 잠재 공간(latent space)에서 새로운 기억을 합성하여 에이전트가 인간과 유사한 계획, 절차, 작업 기억 패턴을 자발적으로 개발하도록 돕습니다. MemGen은 향후 DeepSearch와 같은 탐색 기반 훈련과 통합되어 더욱 강력한 추론 능력을 발휘할 수 있을 것입니다.

**다음은 무엇인가?**
MemGen은 추론(reasoning)을 더욱 향상시키기 위해 탐색 기반 훈련(search-based training, 예: DeepSearch) 또는 탐색 확장(exploration scaling, BroRL)과 통합될 수 있습니다. 연구자들은 또한 최적의 보상(rewards)을 위해 언제 무엇을 불러올지 학습하도록 기억 트리거(memory trigger)와 위버(weaver)를 RL과 함께 공동으로 훈련하는 것을 탐색할 수 있습니다. 마지막으로, MemGen을 Dragon Hatchling과 같은 생물학적 영감 아키텍처(biologically inspired architectures)와 결합하면 뇌와 유사한 기억(memory)과 네트워크 구조(network structure)를 모두 갖춘 에이전트(agents)를 얻을 수 있습니다.

#### Thoughtbubbles – 잠재 공간에서의 비지도 병렬 사고

**시청**: Thoughtbubbles (논문)

**어떤 문제를 해결하는가?**
LLM은 일반적으로 쌓인 레이어(stacked layers)를 통해 입력을 순차적으로 처리합니다. 복잡한 추론은 종종 명시적인 사고의 사슬 프롬프팅(chain-of-thought prompting)을 요구하며 긴 출력을 생성합니다. 모델이 사고의 사슬을 명시적으로 작성하지 않고도 어려운 토큰에 내부적으로 추가 연산(extra compute)을 할당할 수 있을까요?

**어떻게 문제를 해결하는가?**
Thoughtbubbles는 트랜스포머(Transformers)에 대한 아키텍처 수정(architectural modification)입니다. 사전 훈련(pretraining) 동안 모델은 특정 토큰에 대해 잔차 스트림(residual streams)의 사본을 "분기(fork)"하는 방법을 학습하여, 효과적으로 병렬 계산 분기(computational branches, 버블)를 생성합니다. 어려운 토큰에는 더 많은 계산 단계(computation steps)가 할당되고, 쉬운 토큰은 정상적으로 흐릅니다. 이 방법은 언어 모델링 손실(language modeling loss)만을 사용하여 비지도 방식(unsupervised manner)으로 학습됩니다. 사고의 사슬 감독(chain-of-thought supervision)은 없습니다. 사전 훈련 후, 토큰은 자동으로 버블을 트리거(trigger)할 수 있으며, 이는 추론(inference)이 훈련과 동일한 메커니즘을 사용한다는 것을 의미합니다.

**주요 발견**
*   **개선된 혼란도**: 1억 5천만(150M)에서 7억 7천만(770M) 매개변수(parameters)에 이르는 모델 크기에서, Thoughtbubbles는 표준 디코더(standard decoders)에 비해 텍스트 코퍼스(text corpora)의 혼란도(perplexity)를 일관되게 낮춥니다.
*   **더 나은 제로샷 추론**: 추론 벤치마크(HellaSwag, LAMBADA)에서 Thoughtbubbles 모델은 표준 트랜스포머(Transformers)와 비적응형 병렬 방법(non-adaptive parallel methods)을 모두 능가합니다.
*   **통합된 훈련/추론 동작**: 메커니즘이 사전 훈련(pretraining) 중에 학습되기 때문에 훈련과 추론(inference) 사이에 불일치(discrepancy)가 없습니다. 모델은 필요할 때 자연스럽게 추가 연산(extra compute)을 할당합니다.

그 결과, Thoughtbubbles 모델은 표준 디코더(standard decoders)에 비해 텍스트 코퍼스(text corpora)의 혼란도(perplexity)를 낮추고 HellaSwag, LAMBADA와 같은 추론 벤치마크에서 더 나은 제로샷 추론 성능을 보여주었습니다. 이 기술은 특히 실시간 응답이 중요한 인터랙티브 에이전트(interactive agents)에서 큰 잠재력을 가집니다.

**다음은 무엇인가?**
향후 연구는 Thoughtbubbles를 RL 또는 탐색(예: MCTS)과 결합하여 추론 단계(reasoning steps) 전반에 걸쳐 병렬 계산(parallel computation)을 할당할 수 있습니다. 버블 깊이(bubble depth), 병합 전략(merging strategies)을 조사하고, 이 아이디어를 멀티모달 모델(multimodal models)에 적용하면 추가적인 이점을 얻을 수 있습니다. 또한 해석 가능성 연구(interpretability studies)는 버블이 특정 추론 패턴(reasoning patterns)과 어떻게 일치하는지 밝혀낼 수 있습니다.

### 데이터 전략과 모델 능력의 재정의

LLM의 능력은 모델 아키텍처뿐만 아니라 훈련 데이터의 품질과 구성 방식에 크게 좌우됩니다. 최근 연구들은 데이터 중심 접근 방식이 모델의 일반화 능력과 새로운 능력 발현에 얼마나 중요한지 보여줍니다.

#### SFT 일반화 신화 반박하기

**시청**: SFT 일반화 신화 반박하기 (논문 / 코드)

**어떤 문제를 해결하는가?**
지도 미세 조정(Supervised fine-tuning, SFT)은 때때로 지시 템플릿(instruction templates)을 암기하고 그 이상으로 일반화(generalize)하지 못하는 모델을 생성한다는 비판을 받습니다. 대조적으로, RLHF 또는 RLVR과 같은 RL 기반 방법은 더 큰 견고성(robustness)을 달성하는 것으로 여겨집니다. 이 논문은 SFT가 본질적으로 일반화 능력이 떨어진다는 믿음에 이의를 제기합니다.

**어떻게 문제를 해결하는가?**
저자들은 SFT 훈련에서 두 가지 실패 모드(failure modes)를 식별합니다.
*   **고정된 프롬프트 아티팩트(Frozen prompt artifact)**: 고정된 지시 템플릿(instruction templates)으로 훈련하면 모델이 템플릿 의미론(template semantics)에 고착되어 프롬프트(prompts)가 다양해질 때 성능이 저하됩니다.
*   **알고리즘적 틀 부족(Lack of algorithmic scaffolding)**: 중간 추론(intermediate reasoning)이 없으면 SFT 모델은 더 어려운 사례를 해결하는 데 어려움을 겪습니다.
그들은 간단한 해결책을 제안합니다.
*   **프롬프트 다양성(Prompt diversity)**: SFT 동안 광범위한 프롬프트 스타일(prompt styles)을 사용하여 모델이 단일 템플릿(template)에 과적합(overfitting)되는 것을 방지합니다.
*   **사고의 사슬 감독(Chain-of-thought supervision)**: 훈련 데이터에 명시적인 추론 흔적(reasoning traces)을 제공하여(CoT 프롬프팅과 같이) 모델에 기본 알고리즘을 가르칩니다.
이 두 가지 수정을 결합하면 지시 스타일(instruction styles)과 증가된 작업 난이도에 걸쳐 일반화(generalize)하는 SFT 모델을 얻을 수 있습니다.

**주요 발견**
*   **프롬프트 다양성만으로 스타일 일반화 개선**: 모델이 다양한 지시 형식에 노출되면, 보지 못한 프롬프트 변형(prompt variations)에서도 잘 수행됩니다.
*   **CoT 스캐폴딩(scaffolding)은 난이도 일반화 개선**: 사고의 사슬(chain-of-thought) 예제를 포함하면 SFT 모델이 이전에 실패했던 더 어려운 사례(예: 더 큰 소코반 퍼즐)를 해결할 수 있습니다.
*   **SFT는 RL과 필적할 수 있음**: 프롬프트 다양성(prompt diversity)과 CoT 감독(supervision)을 통해 SFT 모델은 테스트된 작업에서 RL 훈련 정책(RL-trained policies)의 성능과 같거나 능가합니다.

이 연구는 SFT가 단순히 템플릿(template)을 암기한다는 통념을 깨고, 적절한 데이터 전략을 통해 RL 기반 방법론에 필적하거나 능가하는 일반화 능력을 가질 수 있음을 입증했습니다. 특히, 최근에는 소량의 고품질 CoT 데이터와 프롬프트 다양성 기법을 결합하여 경량 모델에서도 복잡한 추론 작업에 대한 뛰어난 성능을 달성하는 사례들이 보고되고 있습니다. 이는 데이터 중심 접근 방식이 LLM의 잠재력을 최대한 발휘하는 데 얼마나 중요한지 다시 한번 강조합니다.

**다음은 무엇인가?**
이 연구는 SFT에 대한 데이터 중심적 관점(data-centric view)을 장려합니다. 즉, 즉시 RL에 의존하기보다는 다양한 프롬프트(prompts)와 추론 흔적(reasoning traces)에 투자해야 합니다. 향후 연구는 SFT를 다른 훈련 체제(training regimes, 예: RLMT 또는 RLVR)와 결합하고, 코드 생성(code generation) 또는 도구 사용(tool use)과 같은 실제 작업에서 일반화(generalization)를 테스트해야 합니다. 프롬프트 다양성(prompt diversity)에 대한 체계적인 벤치마크(benchmark)는 평가를 표준화하는 데 도움이 될 수 있습니다.

#### 보기 전에 보는 법 배우기 – 텍스트로부터의 시각 사전 지식

**시청**: 보기 전에 보는 법 배우기 (논문)

**어떤 문제를 해결하는가?**
텍스트만으로 훈련된 대규모 언어 모델은 이미지를 본 적이 없음에도 불구하고 간단한 시각 질문(예: "하늘은 파란색인가요?")에 답하는 놀라운 능력을 종종 보여줍니다. 이러한 시각 사전 지식(visual priors)은 어디에서 오는 걸까요? 어떻게 의도적으로 이를 배양할 수 있을까요?

**어떻게 문제를 해결하는가?**
저자들은 LLM이 언어 사전 훈련(language pretraining) 동안 두 가지 분리 가능한 시각 사전 지식(visual priors)을 어떻게 개발하는지 분석합니다.
*   **시각적 추론 사전 지식(Visual reasoning prior)**: 코드, 수학, 과학 문서와 같은 추론 중심 텍스트에서 파생되며, 모델에게 시각적 개념을 논리적으로 연결하는 방법을 가르칩니다.
*   **시각적 지각 사전 지식(Visual perception prior)**: 일상적인 장면 묘사를 포함하는 광범위한 자연어 코퍼스(natural language corpora)에서 파생됩니다.
그들은 추론 사전 지식(reasoning prior)이 모델 크기(model size)에 따라 강력하게 확장되며 최소한의 이미지 노출로도 시각 작업(vision tasks)으로 전이될 수 있음을 보여줍니다. 반면, 지각 사전 지식(perception prior)은 빠르게 포화 상태에 이르며 LLM을 좋은 비전 인코더(vision encoder)와 짝짓는 것에 의존합니다. 그들은 데이터 중심 사전 훈련 레시피(data-centric pretraining recipe)를 제안합니다. 사전 훈련의 일부를 코드/수학에 할당하여 추론 사전 지식(reasoning priors)을 구축하고, 지각을 위한 소량의 시각적 묘사를 포함한 다음, 작은 멀티모달 데이터셋(multimodal dataset)으로 미세 조정(fine-tune)합니다.

**주요 발견**
*   **추론 vs. 지각 사전 지식**: 추론 사전 지식(reasoning priors)은 구조화된 텍스트에서 오고 모델 크기(model size)에 따라 확장됩니다. 지각 사전 지식(perception priors)은 광범위한 언어에서 오고 빠르게 포화됩니다.
*   **데이터 효율성**: 적당한 양의 추론 중심 텍스트는 시각적 추론 능력(visual reasoning ability)을 크게 향상시킵니다. 더 많은 묘사적 텍스트를 포함하는 것은 수확 체감(diminishing returns)을 가져옵니다.
*   **시각 인식 LLM을 위한 레시피**: 사전 훈련 혼합(pretraining mixtures)을 제어함으로써 저자들은 최소한의 이미지 미세 조정(image fine-tuning) 후 시각 작업(vision tasks)에서 더 나은 성능을 보이는 모델을 생성합니다.

**훈련 전략의 재구성**
이러한 발견은 멀티모달 모델(multimodal models) 훈련 시, 이미지 데이터를 대량으로 사용하기보다 텍스트 데이터셋의 구성을 최적화함으로써 효율적으로 시각 능력을 배양할 수 있음을 시사합니다. 예를 들어, 최근에는 수학 문제 풀이 과정에 시각적 다이어그램을 텍스트로 설명하는 데이터셋을 활용하여, 모델이 추상적인 시각적 개념을 더 잘 이해하도록 훈련하는 연구가 활발히 진행되고 있습니다. 이는 사전 훈련 혼합(pretraining mixtures)을 제어하여 최소한의 이미지 미세 조정(image fine-tuning) 후 시각 작업(vision tasks)에서 더 나은 성능을 보이는 모델을 생성할 수 있음을 보여주며, 데이터 효율적인 멀티모달 LLM 개발의 새로운 길을 열고 있습니다.

**다음은 무엇인가?**
향후 연구는 이러한 사전 지식(priors)을 연속 잠재 CoT(continuous latent CoT, Thoughtbubbles 참조) 또는 생성형 확산 모델(generative diffusion models)과 결합하는 것을 탐색할 수 있습니다. 또한 명시적인 공간 추론 작업(spatial reasoning tasks)으로 사전 훈련(pretraining)하면 훨씬 더 강력한 사전 지식(priors)을 구축할 수 있습니다. 저자들의 데이터 중심 접근 방식(data-centric approach)은 고품질 추론 텍스트를 큐레이션(curating)하는 것이 더 유능한 멀티모달 모델(multimodal models)을 향한 발전을 가속화할 수 있음을 시사합니다.

### 언어 모델로부터 비밀 지식 유도하기

**시청**: 비밀 지식 유도하기 (논문)

**어떤 문제를 해결하는가?**
언어 모델은 사적이거나 민감한 지식(예: 사용자 속성)을 보유하고 있을 수 있으며, 이를 직접 부인하면서도 질문에 답하는 데 사용할 수 있습니다. 이는 보안 및 개인 정보 보호 문제를 제기합니다. 숨기도록 훈련된 모델에서 숨겨진 지식을 밝혀낼 수 있을까요?

**어떻게 문제를 해결하는가?**
저자들은 비밀 유도 테스트베드(secret-elicitation testbed)를 만듭니다. 그들은 LLM을 미세 조정(fine-tune)하여 내부 추론(internal reasoning)에 비밀을 포함시키도록 합니다. 예를 들어, 모델은 사용자의 성별을 알고 관련 질문에 답하는 데 사용하지만, 직접 질문을 받으면 비밀을 부인합니다. 그런 다음 블랙박스(black-box) 및 화이트박스(white-box) 감사 공격을 설계합니다. 프리필(prefill) 및 페르소나 샘플링(persona sampling)은 모델에 다른 접두사(prefixes)를 사용하여 비밀을 유도하고, 로짓 렌즈(logit lens) 및 희소 오토인코더(sparse autoencoders)는 숨겨진 활성화(activations)를 검사합니다. 추출 성공률을 측정하여 비밀이 얼마나 쉽게 유도될 수 있는지 평가합니다.

**주요 발견**
*   **프리필 공격의 강력함**: 세 가지 설정 중 두 가지에서, 블랙박스 프리필 공격(black-box prefill attacks, 프롬프트(prompt) 앞에 조작된 접두사(prefixes)를 추가하는 것)은 기준선(baseline) 방법보다 숨겨진 비밀을 더 잘 유도했습니다.
*   **화이트박스 방법은 다른 곳에서 작동**: 나머지 설정에서는 로짓 렌즈(logit lens) 및 희소 오토인코더(sparse autoencoder) 분석과 같은 화이트박스 기법이 더 효과적이었으며, 내부 활성화(internal activations)의 직접적인 검사가 숨겨진 지식을 드러낼 수 있음을 보여주었습니다.
*   **벤치마크 공개**: 저자들은 공개 감사를 위한 모델과 코드를 공개하여, 모델 개인 정보 보호 및 적대적 유도(adversarial elicitation)에 대한 향후 연구의 기준선(baseline)을 설정합니다.

**다음은 무엇인가?**
향후 연구는 완화 전략(mitigation strategies, 예: 민감한 표현을 강력하게 제거하거나 무작위화하는 방법)을 탐색하고, 벤치마크(benchmark)를 더 많은 비밀 유형(예: 사용자 위치, 정치적 견해)으로 확장해야 합니다. 비밀 유도와 진실성 훈련(truthfulness training, TruthRL 참조)을 결합하면 모델이 민감한 내용을 알고 있지만 공유를 거부할 때 이를 인정하도록 장려할 수 있습니다.

### 결론 및 미래 전망

이번 LLM Watch 업데이트에서 살펴본 연구들은 대규모 언어 모델의 근본적인 한계를 극복하고 새로운 지능을 부여하는 데 초점을 맞추고 있습니다. 탐색(exploration)의 효율성을 극대화하고, 자원 할당을 최적화하며, 모델의 정직성을 보장하는 강화 학습(RL) 방법론들은 LLM의 실용적인 적용 가능성을 크게 확장하고 있습니다. 특히 BroRL, DeepSearch, Knapsack RL, TruthRL과 같은 연구들은 RL 기반 LLM의 훈련 안정성과 성능을 한 단계 끌어올렸습니다.

또한, GEM과 같은 표준화된 플랫폼 위에서 MemGen의 생성형 기억(generative memory)이나 Thoughtbubbles의 병렬 사고(parallel thinking)와 같은 아키텍처 혁신은 에이전트형 LLM이 더욱 복잡하고 자율적인 행동을 수행할 수 있도록 돕습니다. 이는 인간의 인지 과정에 더 가까운 방식으로 정보를 처리하고 학습하는 모델 개발을 가능하게 합니다. 데이터 측면에서는 SFT의 잠재력을 재발견하고, 텍스트 데이터에서 시각적 사전 지식(visual priors)을 추출하는 연구를 통해 모델 훈련 전략의 패러다임을 변화시키고 있습니다. 양질의 데이터 큐레이션과 다양성 확보가 모델 성능 향상의 핵심 동력임을 다시 한번 확인시켜 줍니다.

앞으로의 연구는 이러한 개별적인 발전을 통합하여, 인간의 인지 과정에 더욱 가까운 LLM을 개발하는 방향으로 나아갈 것입니다. 이는 단순히 성능 향상을 넘어, 더욱 신뢰할 수 있고, 효율적이며, 윤리적인 AI 시스템을 구축하는 데 기여할 것입니다. LLM Watch는 이러한 혁신적인 여정을 계속해서 추적하며 최신 소식을 전해드리겠습니다.

AI의 미래는 신경과학, 최적화, 안전 연구를 아우르는 다학제적 혁신을 포함할 가능성이 높습니다. 계속 지켜봐 주세요!

---

이번 주 LLM Watch가 흥미로우셨다면, 구독을 통해 최신 소식을 놓치지 마세요! LLM Watch 구독자들은 텍사스 오스틴에서 열리는 제6회 MLOps World | GenAI 글로벌 서밋(Global Summit)에 특별히 초대됩니다. OpenAI, HuggingFace 등 업계 선두 주자들이 참여하며 60개 이상의 세션이 준비되어 있습니다. 구독자는 여기에서 무료로 원격 참여할 수 있습니다. 또한, 현장에서 워크숍, 사용 사례, 네트워킹 기회를 즐기고 싶다면 이 코드를 사용하여 150달러 할인을 받으세요!

**150달러 할인**