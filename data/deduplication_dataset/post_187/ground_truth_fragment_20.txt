환영합니다, Watcher님과 혁신가 여러분! 이번 주 LLM Watch 및 AI 트렌드 소식입니다:

**정체 극복을 위한 확장**: 광범위한 탐색(BroRL), MCTS 통합(DeepSearch), 배낭 문제 방식의 예산 책정(knapsack-style budgeting)이 정체된 RL 실행(RL runs)을 재활성화하며, 공정성 측정(Fairness Metrics), 설명 가능한 AI(XAI), 그리고 합성 데이터 생성(Synthetic Data Generation)이 모델 편향(model bias)을 재정의합니다.

**더 오래가 아닌 더 잘 생각하는 에이전트/시스템**: 잠재 병렬 사고(Thoughtbubbles)와 생성형 잠재 기억(MemGen)이 에이전트의 사고를 개선하고, 잠재 공간에서의 메타 학습(Meta-learning in Latent Spaces)과 적응형 지식 증류(Adaptive Knowledge Distillation)가 시스템 효율성을 높입니다.

**진실/신뢰 > 분위기/불확실성**: TruthRL은 정직함("모르겠습니다" 포함)에 보상하고, 신뢰도 점수(Confidence Scoring)는 모델의 불확실성을 명확히 하는 반면, 비밀 유도(secret-elicitation) 및 데이터 출처 추적(data provenance tracking) 연구는 숨겨진 사실이나 데이터의 영향이 여전히 유출될 수 있음을 보여줍니다.

**연구실에서 리더보드 및 산업 표준으로**: GEM은 에이전트형 LLM(agentic LLMs)을 위한 훈련/평가를 표준화하고, MLFlow는 MLOps(머신러닝 운영) 워크플로우를 위한 훈련/평가를 표준화합니다. SFT 신화는 프롬프트 다양성(prompt diversity) + CoT로 반박되며, 분산 학습의 신화는 개인 정보 보호 기술(privacy-preserving techniques) + 연합 학습(Federated Learning)으로 반박됩니다.

**뇌 및 시각/멀티모달 사전 지식**: Dragon Hatchling과 뉴로심볼릭 AI(Neuro-symbolic AI)는 트랜스포머(transformers) 및 심층 신경망(deep neural networks)을 뇌와 유사한 네트워크/인지 모델에 연결하며, 텍스트 전용 사전 훈련(text-only pretraining)으로 시각 사전 지식(visual priors)을 형성할 수 있습니다.

다시는 업데이트를 놓치지 않도록 구독하는 것을 잊지 마세요. LLM Watch 회원들은 텍사스 오스틴에서 열리는 제6회 MLOps World | GenAI 글로벌 서밋(Global Summit)에 초대됩니다. OpenAI, HuggingFace 등이 참여하며 60개 이상의 세션이 있습니다. AI 연구 커뮤니티 회원들은 서울에서 열리는 제1회 AI 윤리 및 거버넌스 글로벌 포럼(Global Forum)에 초대됩니다. 주요 AI 연구소, 정책 입안자 등이 참여하며 50개 이상의 세션이 있습니다. 구독자는 여기에서 무료로 원격 참여할 수 있습니다.

또한 실용적인 워크숍, 사용 사례, 음식, 음료 및 오스틴 전역의 파티에 (현장) 참여하고 싶다면 이 코드를 사용하여 150달러 할인을 받으세요!
**150달러 할인**

그리고 실용적인 워크숍, 최신 규제 사례, 네트워킹 및 서울 전역의 파티에 (현장) 참여하고 싶다면 이 코드를 사용하여 200달러 할인을 받으세요!
**200달러 할인**

---

**빠른 용어집 - 본 이슈에 맞춤**

*   **RLVR (Reinforcement Learning with Verifiable Rewards)**: 정확성이 자동으로 확인 가능한 강화 학습(RL) (예: 수학 정답, 단위 테스트)으로, 보상이 사람의 레이블에 의존하지 않습니다.
*   **비밀 유도(Secret elicitation, 모델 감사)**: 모델이 내부적으로 "알고 있지만" 명시하지 않는 사실을 이끌어내는 프롬프트(prompt) 또는 분석 기법입니다. 예를 들어, 블랙박스 프리필(black-box prefill, 시드 완성), 페르소나 샘플링(persona sampling), 또는 로짓 렌즈(logit lens) 및 희소 오토인코더(sparse autoencoders)와 같은 화이트박스(white-box) 도구가 있습니다.
*   **CoT 감독(CoT supervision, SFT용)**: 모델이 더 어려운 사례로 전이되는 알고리즘적 틀(algorithmic scaffold)을 학습하도록 단계별 솔루션으로 훈련하는 것입니다.
*   **잠재 병렬 사고(Latent parallel thinking)**: 트랜스포머(transformer)가 내부적으로 잔차 스트림(residual stream)을 분기하여 어려운 토큰에 병렬로 추가 연산(extra compute)을 할당하도록 함으로써, 명시적인 사고의 사슬(chain-of-thought)은 필요 없습니다.
*   **텍스트로부터의 시각 사전 지식(Visual priors from text)**: 추론 중심 텍스트(코드/수학/과학)는 시각적 추론 사전 지식(visual reasoning prior)을 구축하고, 광범위한 언어는 지각 사전 지식(perception prior)을 구축합니다. 이는 나중에 좋은 비전 인코더(vision encoder) + 약간의 멀티모달 미세 조정(multimodal finetune)으로부터 이점을 얻습니다.
*   **합성 데이터 생성(Synthetic Data Generation)**: 실제 데이터를 모방하지만 실제 개인 정보가 포함되지 않은 인공 데이터를 만드는 기술입니다. 프라이버시 보호 및 데이터 희소성 문제를 해결하는 데 사용됩니다.
*   **신뢰도 점수(Confidence Scoring)**: 모델이 특정 예측에 대해 얼마나 확신하는지 정량화하는 기법입니다. 불확실성 추정(uncertainty estimation)에 필수적이며, 모델 감사(model auditing)에 활용됩니다.
*   **연합 지식 증류(Federated Knowledge Distillation)**: 분산된 여러 로컬 모델의 지식을 중앙 서버에서 효율적으로 통합하여 하나의 강력한 모델을 구축하는 방법입니다. 데이터 이동 없이 모델 성능을 향상시킵니다.
*   **메타 학습 최적화(Meta-learning Optimization)**: 모델이 새로운 작업에 빠르게 적응하도록 학습하는 과정을 최적화하는 접근 방식입니다. "학습하는 방법을 학습하는" 것으로, 소량의 데이터로도 효과적인 훈련이 가능합니다.

---

### BroRL 및 데이터 증강을 통한 모델 견고성 및 탐색 확장

**시청**: BroRL (논문), 데이터 증강 전략 (논문)

**어떤 문제를 해결하는가?**
검증 가능한 보상을 통한 강화 학습(RLVR)은 정답에 보상함으로써 추론을 개선하지만, 모델의 성능은 종종 수천 번의 훈련 단계(training steps) 후에 포화 상태에 이릅니다. 모델은 탐색(explore)을 너무 적게 하거나 데이터 희소성(data sparsity)으로 인해 개선을 멈춥니다. 훈련을 계속하려는 시도는 수확 체감(diminishing returns)을 초래합니다.

**어떻게 문제를 해결하는가?**
BroRL은 예제당 롤아웃(rollouts) 수를 수백 개로 극적으로 늘려 탐색(exploration)을 확장할 것을 제안합니다. 이와 유사하게, 데이터 증강(data augmentation) 기술은 이미지 회전, 자르기, 색상 변형 또는 텍스트의 동의어 대체와 같은 다양한 변환 기법을 적용하여 기존 데이터셋을 확장합니다. 이 두 방법은 단순히 훈련 단계(training steps)를 늘리는 대신, 동일한 수의 기울기 업데이트(gradient updates)를 유지하면서 훨씬 더 많은 궤적(trajectories) 또는 증강된 데이터를 샘플링하여 탐색 공간(search space)을 넓힙니다. 간단한 이론적 분석에 따르면, 단일 단계 RL/학습 설정(one-step RL/learning setting)에서 각 롤아웃 또는 증강된 샘플은 올바른 행동에 긍정적인 확률 질량(probability mass)을 기여하며, 롤아웃 또는 증강이 증가함에 따라 샘플링되지 않은 행동의 영향은 사라집니다.

**주요 발견**
*   **이론적 보장**: 질량 균형 논증(mass-balance argument)은 더 많은 롤아웃(rollouts) 또는 증강된 데이터를 샘플링하는 것이 올바른 확률 질량(probability mass)을 단조롭게 확장하고 보이지 않는 행동의 영향을 줄인다는 것을 증명합니다.
*   **포화된 모델 재활성화**: BroRL 및 적절한 데이터 증강은 약 3천 번의 ProRL 단계(steps) 또는 훈련 단계 후에 정체된 모델을 재활성화하여, 모델이 포화 상태에 이르렀을 때에도 지속적인 성능 향상을 달성합니다.
*   **최첨단 결과**: 15억(1.5B) 매개변수(parameter) 모델의 경우, BroRL 및 데이터 증강은 다양한 벤치마크(benchmarks)에서 최첨단 성능을 달성하며, 탐색 확장 및 데이터 확장이 훈련 단계(training steps)를 늘리는 것보다 더 효과적일 수 있음을 보여줍니다.

**다음은 무엇인가?**
향후 연구는 BroRL을 구조화된 탐색 방법(structured search methods, 예: MCTS) 또는 예산 할당 방식(budget allocation schemes, Knapsack RL 참조)과 결합하여 탐색을 더욱 최적화할 수 있습니다. 데이터 증강은 생성형 모델(generative models, 예: GAN, Diffusion Models) 또는 적대적 훈련(adversarial training)과 결합될 수 있습니다. 이 기술들은 단일 단계 RL/학습(one-step RL/learning)을 넘어 완전한 순차적 설정(sequential settings)으로 확장되거나, 더 복잡한 작업을 위해 MemGen과 같은 기억 증강 아키텍처(memory-augmented architectures)와 통합될 수 있습니다.

---

### 언어 모델로부터 비밀 지식 및 윤리적 편향 유도/식별

**시청**: 비밀 지식 유도하기 (논문), 윤리적 편향 식별 (논문)

**어떤 문제를 해결하는가?**
언어 모델은 사적이거나 민감한 지식(예: 사용자 속성) 또는 사회적 편향을 내부적으로 보유하고 있을 수 있으며, 이를 직접 부인하면서도 질문에 답하는 데 사용할 수 있습니다. 이는 보안, 개인 정보 보호 및 윤리적 문제를 제기합니다. 숨기도록 훈련된 모델에서 숨겨진 지식이나 편향을 밝혀낼 수 있을까요?

**어떻게 문제를 해결하는가?**
저자들은 비밀 유도 및 윤리적 편향 테스트베드(testbed)를 만듭니다. 그들은 LLM을 미세 조정(fine-tune)하여 내부 추론(internal reasoning)에 비밀 또는 사회적 편향을 포함시키도록 합니다. 예를 들어, 모델은 사용자의 성별을 알고 관련 질문에 답하는 데 사용하거나 특정 인구 통계에 대한 고정관념을 학습하지만, 직접 질문을 받으면 비밀이나 편향을 부인합니다. 그런 다음 블랙박스(black-box) 및 화이트박스(white-box) 감사 공격을 설계합니다. 프리필(prefill) 및 페르소나 샘플링(persona sampling)은 모델에 다른 접두사(prefixes)를 사용하여 비밀이나 편향을 유도하고, 로짓 렌즈(logit lens) 및 희소 오토인코더(sparse autoencoders)와 같은 화이트박스(white-box) 도구는 숨겨진 활성화(activations)를 검사합니다. 추출 성공률을 측정하여 비밀이나 편향이 얼마나 쉽게 유도될 수 있는지 평가합니다.

**주요 발견**
*   **프리필 공격의 강력함**: 세 가지 설정 중 두 가지에서, 블랙박스 프리필 공격(black-box prefill attacks, 프롬프트(prompt) 앞에 조작된 접두사(prefixes)를 추가하는 것)은 기준선(baseline) 방법보다 숨겨진 비밀이나 편향을 더 잘 유도했습니다.
*   **화이트박스 방법은 다른 곳에서 작동**: 나머지 설정에서는 로짓 렌즈(logit lens) 및 희소 오토인코더(sparse autoencoder) 분석과 같은 화이트박스 기법이 더 효과적이었으며, 내부 활성화(internal activations)의 직접적인 검사가 숨겨진 지식을 드러낼 수 있음을 보여주었습니다.
*   **벤치마크 공개**: 저자들은 공개 감사를 위한 모델과 코드를 공개하여, 모델 개인 정보 보호, 적대적 유도 및 윤리적 편향 연구에 대한 향후 연구의 기준선(baseline)을 설정합니다.

**다음은 무엇인가?**
향후 연구는 완화 전략(mitigation strategies, 예: 민감한 표현을 강력하게 제거하거나 무작위화하는 방법)을 탐색하고, 벤치마크(benchmark)를 더 많은 비밀 유형(예: 사용자 위치, 정치적 견해) 및 편향 유형(예: 성별, 인종, 종교적 견해)으로 확장해야 합니다. 비밀 유도와 진실성 훈련(truthfulness training, TruthRL 참조) 또는 편향 식별과 공정성 훈련(fairness training)을 결합하면 모델이 민감한 내용을 알고 있지만 공유를 거부할 때 이를 인정하도록 장려할 수 있습니다.

---

### GEM 및 MLFlow – 에이전트형 LLM 및 MLOps 워크플로우를 위한 표준화된 플랫폼

**시청**: GEM (논문), MLFlow (문서)

**어떤 문제를 해결하는가?**
에이전트형 LLM(agentic LLMs)을 개발하고 평가하려면 훈련 및 벤치마킹을 위한 표준화된 환경이 필요하며, MLOps(머신러닝 운영) 워크플로우를 관리하기 위한 공통 플랫폼이 부족했습니다.

**어떻게 문제를 해결하는가?**
GEM (General Experience Maker)은 LLM 에이전트(LLM agents)를 위해 특별히 맞춤화된 오픈 소스 환경 시뮬레이터(environment simulator)이며, MLFlow는 머신러닝 수명 주기 관리를 위한 오픈 소스 플랫폼입니다. 이들은 비동기적(asynchronous), 벡터화된 실행(vectorized execution, 다중 병렬 시뮬레이션), 유연한 래퍼(wrappers), 통합 도구(예: Python 코드 실행, 검색, 모델 버전 관리, 실험 추적)를 지원하는 환경과 에이전트/모델 간의 표준 인터페이스(interface)를 정의합니다. GEM과 MLFlow는 수학, 코드, Q&A, 도구 사용을 포함하는 다양한 환경 스위트(suite)와 반환 배치 정규화(Return Batch Normalization, ReBN)가 적용된 REINFORCE와 같은 RL 알고리즘(RL algorithms)을 위한 기준선 스크립트(baseline scripts)를 제공합니다. 또한 평가 툴킷(evaluation toolkit) 역할도 합니다. 연구자들은 자신의 에이전트/모델을 연결하여 비교 가능한 지표(metrics)를 얻을 수 있습니다.

**주요 발견**
*   **포괄적인 환경 라이브러리 및 실험 추적**: GEM은 수학적 추론부터 API 호출까지 풍부한 관찰/행동 공간(observation/action spaces)을 가진 24개의 환경을 포함하며, MLFlow는 데이터 준비부터 배포까지를 지원합니다. 단일 단계(single-step) 및 다단계(multi-step) 작업을 모두 지원하며, 실험 추적 및 재현성을 보장합니다.
*   **ReBN을 사용한 기준선**: 저자들은 일반적인 RL 알고리즘(REINFORCE, GRPO, PPO)을 벤치마킹하고, ReBN이 정책 기울기(policy gradients)를 안정화하여 턴당 밀집 보상(dense per-turn rewards)으로 훈련할 수 있도록 돕는다는 것을 보여줍니다.
*   **재사용 가능한 평가 하네스**: GEM 및 MLFlow 인터페이스(interface)와 래퍼(wrappers)는 새로운 작업과 에이전트/모델 아키텍처(agent/model architectures)의 쉬운 통합을 가능하게 합니다. 이는 에이전트형 LLM을 위한 "미니 아레나(MiniArena)"와 MLOps를 위한 표준화된 테스트베드(testbed) 역할을 합니다.

**다음은 무엇인가?**
GEM 및 MLFlow는 에이전트형 RL 및 MLOps 연구의 진입 장벽을 낮춥니다. 향후 연구는 환경 라이브러리(environment library)를 확장하여 멀티모달 입력(multimodal inputs, 이미지 또는 오디오)을 통합하거나, 장기 계획(long-horizon planning)을 위한 벤치마크(benchmarks)를 추가하거나, AgentScaler와 같은 대규모 에이전트형 파운데이션 모델(agentic foundation models)과 GEM/MLFlow를 통합할 수 있습니다. 연구자들은 또한 GEM/MLFlow의 벡터화된 인터페이스(vectorized interface)를 기반으로 다중 에이전트(multiple agents) 간의 조정을 요구하는 더 현실적인 작업을 설계할 수 있습니다.

---

### DeepSearch – 몬테카를로 트리 탐색을 통한 RLVR 병목 현상 극복 및 분산 학습 환경에서의 개인 정보 보호 강화

**시청**: DeepSearch (논문), 분산 학습의 프라이버시 (논문)

**어떤 문제를 해결하는가?**
RLVR 및 분산 학습에서 훈련 성능은 종종 포화 상태에 이릅니다. 이는 소수의 샘플링된 궤적(trajectories)이 가능한 모든 추론 경로를 거의 포착하지 못하기 때문입니다. 훈련이 진행됨에 따라 모델의 행동은 점점 더 결정론적(deterministic)이 되어, RL/학습은 분산(variance)이 줄어들고 개선을 멈춥니다.

**어떻게 문제를 해결하는가?**
DeepSearch는 몬테카를로 트리 탐색(Monte Carlo Tree Search, MCTS)을 RL 훈련 루프(training loop)에 통합합니다. 에이전트(agent)는 몇 개의 무작위 롤아웃(random rollouts)을 샘플링하는 대신, 훈련 중에 추론 궤적(reasoning trajectories)에 대한 구조화된 탐색(structured search)을 수행합니다. 주요 구성 요소는 다음과 같습니다.
*   **전역 경계 선택(Global frontier selection)**: 탐색 트리(search tree)의 유망한 가지(branches)를 탐색하는 것을 우선시하여, 잠재력이 높은 추론 경로가 더 많은 관심을 받도록 합니다.
*   **엔트로피 기반 경로 선택(Entropy-based path selection)**: 확신 있고 가치 높은 궤적(trajectories)에 훈련을 집중합니다. 낮은 엔트로피(entropy) 가지는 감독(supervision)에 사용됩니다.
*   **솔루션 캐싱(solution caching)을 통한 적응형 리플레이(Adaptive replay)**: 발견된 솔루션을 저장하고 이를 고보상 궤적(high-reward trajectories)으로 재생하여 중복 탐색을 줄이고 새로운 추론에 집중합니다.
또한, 분산 학습 환경에서는 차등 프라이버시(Differential Privacy, DP) 및 동형 암호화(Homomorphic Encryption, HE) 기술을 훈련 루프에 통합하여 데이터 개인 정보 보호를 강화합니다.

**주요 발견**
*   **탐색 정체 해결**: 추론 경로를 체계적으로 탐색함으로써 DeepSearch 및 개인 정보 보호 기술을 통합한 방법은 확장된 RLVR 및 분산 학습에서 관찰되는 성능 저하를 방지합니다.
*   **최첨단 정확도 및 프라이버시**: 수학 추론 벤치마크(benchmarks)에서 DeepSearch로 훈련된 15억(1.5B) 모델은 평균 62.95%의 정확도를 달성하며, 이는 이전 RLVR 방법보다 높고, 표준 RL 훈련을 확장하는 것보다 5.7배 적은 GPU 시간을 사용합니다. 유사하게, DP 및 HE로 훈련된 모델도 높은 정확도를 유지하면서 개인 정보 보호를 강화합니다.
*   **효율적인 탐색**: 구조화된 탐색(structured search)은 더 나은 샘플 효율성(sample efficiency)을 제공합니다. 에이전트(agent)는 대량의 무작위 롤아웃(random rollouts) 없이도 올바른 전략을 학습합니다.

**다음은 무엇인가?**
DeepSearch는 수학 외의 도메인(domains), 예를 들어 코드 합성(code synthesis) 또는 정리 증명(theorem proving)에도 적용될 수 있으며, 개인 정보 보호 기술은 의료 데이터 분석(medical data analysis) 또는 금융 사기 탐지(financial fraud detection)에 활용될 수 있습니다. MCTS를 동적 롤아웃 예산(dynamic rollout budgets, Knapsack RL 방식)과 결합하면 추가적인 개선을 가져올 수 있습니다. 또한 미분 가능한 탐색(differentiable search, 예: 미분 가능한 MCTS)을 탐색하면 기울기 역전파(gradient backpropagation)를 통한 종단 간(end-to-end) 훈련이 가능해질 수 있습니다.

---

### SFT 일반화 신화 반박하기

**시청**: SFT 일반화 신화 반박하기 (논문 / 코드)

**어떤 문제를 해결하는가?**
지도 미세 조정(Supervised fine-tuning, SFT)은 때때로 지시 템플릿(instruction templates)을 암기하고 그 이상으로 일반화(generalize)하지 못하는 모델을 생성한다는 비판을 받습니다. 대조적으로, RLHF 또는 RLVR과 같은 RL 기반 방법은 더 큰 견고성(robustness)을 달성하는 것으로 여겨집니다. 이 논문은 SFT가 본질적으로 일반화 능력이 떨어진다는 믿음에 이의를 제기합니다.

**어떻게 문제를 해결하는가?**
저자들은 SFT 훈련에서 두 가지 실패 모드(failure modes)를 식별합니다.
*   **고정된 프롬프트 아티팩트(Frozen prompt artifact)**: 고정된 지시 템플릿(instruction templates)으로 훈련하면 모델이 템플릿 의미론(template semantics)에 고착되어 프롬프트(prompts)가 다양해질 때 성능이 저하됩니다.
*   **알고리즘적 틀 부족(Lack of algorithmic scaffolding)**: 중간 추론(intermediate reasoning)이 없으면 SFT 모델은 더 어려운 사례를 해결하는 데 어려움을 겪습니다.
그들은 간단한 해결책을 제안합니다.
*   **프롬프트 다양성(Prompt diversity)**: SFT 동안 광범위한 프롬프트 스타일(prompt styles)을 사용하여 모델이 단일 템플릿(template)에 과적합(overfitting)되는 것을 방지합니다.
*   **사고의 사슬 감독(Chain-of-thought supervision)**: 훈련 데이터에 명시적인 추론 흔적(reasoning traces)을 제공하여(CoT 프롬프팅과 같이) 모델에 기본 알고리즘을 가르칩니다.
이 두 가지 수정을 결합하면 지시 스타일(instruction styles)과 증가된 작업 난이도에 걸쳐 일반화(generalize)하는 SFT 모델을 얻을 수 있습니다.

**주요 발견**
*   **프롬프트 다양성만으로 스타일 일반화 개선**: 모델이 다양한 지시 형식에 노출되면, 보지 못한 프롬프트 변형(prompt variations)에서도 잘 수행됩니다.
*   **CoT 스캐폴딩(scaffolding)은 난이도 일반화 개선**: 사고의 사슬(chain-of-thought) 예제를 포함하면 SFT 모델이 이전에 실패했던 더 어려운 사례(예: 더 큰 소코반 퍼즐)를 해결할 수 있습니다.
*   **SFT는 RL과 필적할 수 있음**: 프롬프트 다양성(prompt diversity)과 CoT 감독(supervision)을 통해 SFT 모델은 테스트된 작업에서 RL 훈련 정책(RL-trained policies)의 성능과 같거나 능가합니다.

**다음은 무엇인가?**
이 연구는 SFT에 대한 데이터 중심적 관점(data-centric view)을 장려합니다. 즉, 즉시 RL에 의존하기보다는 다양한 프롬프트(prompts)와 추론 흔적(reasoning traces)에 투자해야 합니다. 향후 연구는 SFT를 다른 훈련 체제(training regimes, 예: RLMT 또는 RLVR)와 결합하고, 코드 생성(code generation) 또는 도구 사용(tool use)과 같은 실제 작업에서 일반화(generalization)를 테스트해야 합니다. 프롬프트 다양성(prompt diversity)에 대한 체계적인 벤치마크(benchmark)는 평가를 표준화하는 데 도움이 될 수 있습니다.

---

### Thoughtbubbles – 잠재 공간에서의 비지도 병렬 사고

**시청**: Thoughtbubbles (논문)

**어떤 문제를 해결하는가?**
LLM은 일반적으로 쌓인 레이어(stacked layers)를 통해 입력을 순차적으로 처리합니다. 복잡한 추론은 종종 명시적인 사고의 사슬 프롬프팅(chain-of-thought prompting)을 요구하며 긴 출력을 생성합니다. 모델이 사고의 사슬을 명시적으로 작성하지 않고도 어려운 토큰에 내부적으로 추가 연산(extra compute)을 할당할 수 있을까요?

**어떻게 문제를 해결하는가?**
Thoughtbubbles는 트랜스포머(Transformers)에 대한 아키텍처 수정(architectural modification)입니다. 사전 훈련(pretraining) 동안 모델은 특정 토큰에 대해 잔차 스트림(residual streams)의 사본을 "분기(fork)"하는 방법을 학습하여, 효과적으로 병렬 계산 분기(computational branches, 버블)를 생성합니다. 어려운 토큰에는 더 많은 계산 단계(computation steps)가 할당되고, 쉬운 토큰은 정상적으로 흐릅니다. 이 방법은 언어 모델링 손실(language modeling loss)만을 사용하여 비지도 방식(unsupervised manner)으로 학습됩니다. 사고의 사슬 감독(chain-of-thought supervision)은 없습니다. 사전 훈련 후, 토큰은 자동으로 버블을 트리거(trigger)할 수 있으며, 이는 추론(inference)이 훈련과 동일한 메커니즘을 사용한다는 것을 의미합니다.

**주요 발견**
*   **개선된 혼란도**: 1억 5천만(150M)에서 7억 7천만(770M) 매개변수(parameters)에 이르는 모델 크기에서, Thoughtbubbles는 표준 디코더(standard decoders)에 비해 텍스트 코퍼스(text corpora)의 혼란도(perplexity)를 일관되게 낮춥니다.
*   **더 나은 제로샷 추론**: 추론 벤치마크(HellaSwag, LAMBADA)에서 Thoughtbubbles 모델은 표준 트랜스포머(Transformers)와 비적응형 병렬 방법(non-adaptive parallel methods)을 모두 능가합니다.
*   **통합된 훈련/추론 동작**: 메커니즘이 사전 훈련(pretraining) 중에 학습되기 때문에 훈련과 추론(inference) 사이에 불일치(discrepancy)가 없습니다. 모델은 필요할 때 자연스럽게 추가 연산(extra compute)을 할당합니다.

**다음은 무엇인가?**
향후 연구는 Thoughtbubbles를 RL 또는 탐색(예: MCTS)과 결합하여 추론 단계(reasoning steps) 전반에 걸쳐 병렬 계산(parallel computation)을 할당할 수 있습니다. 버블 깊이(bubble depth), 병합 전략(merging strategies)을 조사하고, 이 아이디어를 멀티모달 모델(multimodal models)에 적용하면 추가적인 이점을 얻을 수 있습니다. 또한 해석 가능성 연구(interpretability studies)는 버블이 특정 추론 패턴(reasoning patterns)과 어떻게 일치하는지 밝혀낼 수 있습니다.

---

### 보기 전에 보는 법 배우기 – LLM 시각 사전 지식의 신비 해명

**시청**: 보기 전에 보는 법 배우기 (논문)

**어떤 문제를 해결하는가?**
텍스트만으로 훈련된 대규모 언어 모델은 이미지를 본 적이 없음에도 불구하고 간단한 시각 질문(예: "하늘은 파란색인가요?")에 답하는 놀라운 능력을 종종 보여줍니다. 이러한 시각 사전 지식(visual priors)은 어디에서 오는 걸까요? 어떻게 의도적으로 이를 배양할 수 있을까요?

**어떻게 문제를 해결하는가?**
저자들은 LLM이 언어 사전 훈련(language pretraining) 동안 두 가지 분리 가능한 시각 사전 지식(visual priors)을 어떻게 개발하는지 분석합니다.
*   **시각적 추론 사전 지식(Visual reasoning prior)**: 코드, 수학, 과학 문서와 같은 추론 중심 텍스트에서 파생되며, 모델에게 시각적 개념을 논리적으로 연결하는 방법을 가르칩니다.
*   **시각적 지각 사전 지식(Visual perception prior)**: 일상적인 장면 묘사를 포함하는 광범위한 자연어 코퍼스(natural language corpora)에서 파생됩니다.
그들은 추론 사전 지식(reasoning prior)이 모델 크기(model size)에 따라 강력하게 확장되며 최소한의 이미지 노출로도 시각 작업(vision tasks)으로 전이될 수 있음을 보여줍니다. 반면, 지각 사전 지식(perception prior)은 빠르게 포화 상태에 이르며 LLM을 좋은 비전 인코더(vision encoder)와 짝짓는 것에 의존합니다. 그들은 데이터 중심 사전 훈련 레시피(data-centric pretraining recipe)를 제안합니다. 사전 훈련의 일부를 코드/수학에 할당하여 추론 사전 지식(reasoning priors)을 구축하고, 지각을 위한 소량의 시각적 묘사를 포함한 다음, 작은 멀티모달 데이터셋(multimodal dataset)으로 미세 조정(fine-tune)합니다.

**주요 발견**
*   **추론 vs. 지각 사전 지식**: 추론 사전 지식(reasoning priors)은 구조화된 텍스트에서 오고 모델 크기(model size)에 따라 확장됩니다. 지각 사전 지식(perception priors)은 광범위한 언어에서 오고 빠르게 포화됩니다.
*   **데이터 효율성**: 적당한 양의 추론 중심 텍스트는 시각적 추론 능력(visual reasoning ability)을 크게 향상시킵니다. 더 많은 묘사적 텍스트를 포함하는 것은 수확 체감(diminishing returns)을 가져옵니다.
*   **시각 인식 LLM을 위한 레시피**: 사전 훈련 혼합(pretraining mixtures)을 제어함으로써 저자들은 최소한의 이미지 미세 조정(image fine-tuning) 후 시각 작업(vision tasks)에서 더 나은 성능을 보이는 모델을 생성합니다.

**다음은 무엇인가?**
향후 연구는 이러한 사전 지식(priors)을 연속 잠재 CoT(continuous latent CoT, Thoughtbubbles 참조) 또는 생성형 확산 모델(generative diffusion models)과 결합하는 것을 탐색할 수 있습니다. 또한 명시적인 공간 추론 작업(spatial reasoning tasks)으로 사전 훈련(pretraining)하면 훨씬 더 강력한 사전 지식(priors)을 구축할 수 있습니다. 저자들의 데이터 중심 접근 방식(data-centric approach)은 고품질 추론 텍스트를 큐레이션(curating)하는 것이 더 유능한 멀티모달 모델(multimodal models)을 향한 발전을 가속화할 수 있음을 시사합니다.

---

### Knapsack RL – 예산 할당을 통한 탐색 잠금 해제

**시청**: Knapsack RL (논문)

**어떤 문제를 해결하는가?**
강화 학습 미세 조정(Reinforcement learning fine-tuning)은 종종 문제당 고정된 수의 롤아웃(rollouts)을 사용합니다. 이러한 균일한 할당(uniform allocation)은 연산(compute)을 낭비합니다. 쉬운 작업은 적은 시도(trials)만 필요하고, 극도로 어려운 작업은 적은 예산으로는 결코 성공하지 못할 수 있습니다. 결과적으로 많은 작업이 제로 기울기(zero gradients)를 생성하여 시간을 낭비하고 학습을 방해합니다.

**어떻게 문제를 해결하는가?**
Knapsack RL은 탐색(exploration)을 배낭 최적화 문제(knapsack optimization problem)로 취급합니다. 각 작업의 롤아웃(rollouts)은 "비용"(연산)과 잠재적인 "가치"(예상 학습 이득)를 가집니다. 작업 전반에 걸쳐 배낭 문제(knapsack problem)를 해결함으로써, 알고리즘은 예상 이득이 높은 작업에 더 많은 롤아웃(rollouts)을 할당하고, 이미 해결되었거나 가망 없는 작업에는 더 적게 할당합니다. 이 동적 예산 책정(dynamic budgeting)은 GRPO 알고리즘(algorithm)을 기반으로 합니다.

**주요 발견**
*   **증가된 기울기 밀도**: 0이 아닌 정책 기울기(policy gradients)의 비율이 20–40% 증가하며, 이는 에이전트(agent)가 각 업데이트(update)마다 더 많은 작업에서 학습한다는 것을 의미합니다.
*   **어려운 작업에 대한 대규모 예산**: 일부 극도로 어려운 작업은 약 93개의 롤아웃(rollouts)을 받으며, 균일한 예산으로는 결코 허용되지 않았을 탐색(exploration)을 가능하게 합니다.
*   **성능 향상**: 수학 추론 벤치마크(benchmarks)에서 Knapsack RL은 균일한 할당(uniform allocation)이 요구하는 연산(compute)의 절반을 사용하면서 평균 2–4점, 특정 사례에서는 최대 9점의 개선을 달성합니다.

**다음은 무엇인가?**
Knapsack RL을 BroRL과 결합하면 훨씬 더 나은 결과를 얻을 수 있습니다. 전역적으로 탐색(exploration)을 확장하고 지역적으로 예산(budgets)을 할당하는 것입니다. 또한 탐색(exploration)을 더욱 향상시키기 위해 MCTS 기반 훈련(DeepSearch 참조) 또는 다양성 목표(diversity objectives, Polychromic RL 참조)와 짝을 이룰 수 있습니다. 또 다른 방향은 배낭 문제(knapsack idea)를 다중 에이전트(multi-agent) 또는 계층적 작업(hierarchical tasks)으로 확장하여 예산(budgets)을 하위 작업(subtasks)에 분할해야 하는 경우입니다.

---

### TruthRL – 강화 학습을 통한 진실한 LLM 장려

**시청**: TruthRL (논문)

**어떤 문제를 해결하는가?**
LLM은 종종 환각(hallucinate)을 일으켜 그럴듯하지만 거짓된 답변을 지어냅니다. 순전히 정확성 기반의 RL 보상(RL rewards)은 추측을 조장할 수 있습니다. 반대로 모델은 지나치게 조심하여 답변을 거부할 수 있으며, 이는 유용성을 저해합니다. 정확성, 진실성, 적절한 회피(abstention) 사이의 균형을 어떻게 맞출 수 있을까요?

**어떻게 문제를 해결하는가?**
TruthRL은 RL 미세 조정(fine-tuning)에 삼진 보상(ternary reward)을 도입합니다. 정답에는 긍정적인 보상, 환각(hallucinations)에는 큰 페널티, 그리고 정직한 회피("모르겠습니다")에는 약간의 긍정적인 보상을 제공합니다. 훈련은 GRPO 알고리즘(algorithm)을 사용합니다. 이는 모델이 확신할 때만 답변하고, 그렇지 않을 때는 불확실성을 인정하도록 장려합니다. 이 방법은 외부 검색(external retrieval) 유무에 관계없이 개방형 도메인 QA(open-domain QA)와 같은 작업에 적용됩니다.

**주요 발견**
*   **환각 감소**: TruthRL은 여러 지식 집약적 벤치마크(knowledge-intensive benchmarks)에서 바닐라 RL(vanilla RL) 대비 환각(hallucination) 발생률을 28.9% 감소시킵니다.
*   **진실성 증가**: 전반적인 진실성(정확성 + 정직한 회피)이 21.1% 향상됩니다.
*   **일반적인 적용 가능성**: Qwen, Llama와 같은 모델 유형과 검색 증강(retrieval-augmented) 및 검색 없는(retrieval-free) 설정 모두에서 이점이 지속됩니다.

**다음은 무엇인가?**
TruthRL은 공정성 목표(fairness objectives) 또는 다른 정렬 지표(alignment metrics)와 결합하여 진실할 뿐만 아니라 윤리적으로도 답변하는 모델을 생성할 수 있습니다. 향후 연구는 더 미묘한 보상 구조(reward structures, 예: 난이도에 따라 조정) 또는 다중 턴 대화(multi-turn dialogues)를 탐색할 수 있습니다. 또한 불확실할 때 회피를 장려함으로써 개인 정보 유출을 완화하기 위해 비밀 유도 연구(secret elicitation research)와 결합될 수 있습니다.

---

### MemGen – 자기 진화 에이전트를 위한 생성형 잠재 기억 엮기

**시청**: MemGen (논문 / 코드)

**어떤 문제를 해결하는가?**
LLM 에이전트(agents)는 종종 제한된 기억(memory)을 가집니다. 미세 조정(fine-tuning) 중에 매개변수(parameters)를 다시 작성하거나(매개변수 기억, parametric memory) 외부 데이터베이스(database)를 쿼리합니다(비매개변수 기억, nonparametric memory). 이러한 접근 방식은 경직되거나 추론과 단절될 수 있습니다. 에이전트(agent)가 생성하고 내부 사고 과정에 엮어 넣을 수 있는 동적이고 긴밀하게 연결된 기억(memory)이 필요합니다.

**어떻게 문제를 해결하는가?**
MemGen은 두 가지 핵심 구성 요소(components)를 가진 생성형 잠재 기억 시스템(generative latent memory system)을 도입합니다.
*   **기억 트리거(Memory trigger)**: 에이전트(agent)의 현재 추론 상태(reasoning state)를 모니터링하고 언제 기억을 불러올지 결정하는 모듈(module)입니다. 과거 경험이나 사실을 불러와야 하는 순간을 감지합니다.
*   **기억 위버(Memory weaver)**: 트리거되면 이 모듈(module)은 관련 기억 내용(memory content)을 나타내는 잠재 토큰(latent tokens) 시퀀스(sequence)를 생성하고 이를 모델의 컨텍스트(context)에 다시 주입합니다. 기억은 일반 텍스트가 아닌 모델의 잠재 공간(latent space)에서 학습되고 표현됩니다.
이 시스템은 에이전트(agent)가 일시 중지하고, 내부 기억(internal memory)을 생성한 다음, 해당 기억이 숨겨진 상태(hidden state)에 융합된 상태로 추론을 재개할 수 있도록 합니다. 에이전트(agent)는 작업 기억(working memory), 절차 기억(procedural memory), 계획 기억(planning memory) 유형에 걸쳐 기억 내용(memory content)을 할당하는 방법을 학습합니다.

**주요 발견**
*   **성능 향상**: 8개의 다양한 벤치마크(benchmarks)에서 MemGen은 기존 기억 증강 에이전트(memory-augmented agents, ExpeL, AWM)를 최대 38.22% 능가하고, 강력한 GRPO 기준선(baseline)을 최대 13.44% 초과합니다.
*   **인간과 유사한 기억 패턴**: 수동 코딩(hand-coding) 없이도 MemGen 에이전트(agents)는 인간 인지(human cognition)를 연상시키는 계획 기억(planning memory), 절차 기억(procedural memory), 작업 기억(working memory)과 유사한 기억 행동을 자발적으로 개발합니다.
*   **교차 도메인 일반화**: 생성형 기억(generative memory)은 여러 도메인(domains, 수학, 프로그래밍, Q&A)에서 성능을 향상시키며, 이는 광범위하게 적용 가능함을 시사합니다.

**다음은 무엇인가?**
MemGen은 추론(reasoning)을 더욱 향상시키기 위해 탐색 기반 훈련(search-based training, 예: DeepSearch) 또는 탐색 확장(exploration scaling, BroRL)과 통합될 수 있습니다. 연구자들은 또한 최적의 보상(rewards)을 위해 언제 무엇을 불러올지 학습하도록 기억 트리거(memory trigger)와 위버(weaver)를 RL과 함께 공동으로 훈련하는 것을 탐색할 수 있습니다. 마지막으로, MemGen을 Dragon Hatchling과 같은 생물학적 영감 아키텍처(biologically inspired architectures)와 결합하면 뇌와 유사한 기억(memory)과 네트워크 구조(network structure)를 모두 갖춘 에이전트(agents)를 얻을 수 있습니다.

---

### 요약 – 통찰력 및 향후 방향

이 논문들 전반에 걸쳐 우리는 다양한 인공지능 분야의 융합을 봅니다.
*   **데이터와 자원의 최적화 및 탐색**: 데이터 증강, 자원 할당 최적화, 분산 학습에서의 개인 정보 보호, 그리고 BroRL, DeepSearch, Knapsack RL, Polychromic RL과 같은 탐색 전략들은 데이터의 양과 질을 넘어 효율적인 자원 활용과 탐색의 중요성을 강조합니다. 롤아웃(rollouts)을 확장하고, 탐색을 내장하고, 예산(budgets)을 최적화하거나, 여러 전략을 보존함으로써 이 연구들은 RL 및 학습이 정체되는 경향을 해결합니다.
*   **아키텍처 혁신**: Thoughtbubbles는 비지도 수정(unsupervised modifications)이 LLM이 병렬로 사고하도록 할 수 있음을 보여주고, MemGen은 생성형 잠재 기억(generative latent memory)을 도입하며, Dragon Hatchling (또는 뉴로심볼릭 접근법)은 트랜스포머(Transformers)에 대한 생물학적으로 타당한 대안을 제공합니다. 이러한 혁신은 단순한 스케일업(scale-up)을 넘어 모델이 정보를 처리하고 저장하는 새로운 방법을 탐색합니다.
*   **모델의 신뢰성 및 일반화**: "보기 전에 보는 법 배우기"와 "SFT 일반화 신화 반박하기"는 단순한 양보다 훈련 데이터(training data)의 품질과 다양성의 중요성을 강조합니다. 추론 중심 텍스트(reasoning-centric texts) 또는 다양한 프롬프트(prompts)와 CoT 예제를 큐레이션(curating)함으로써 LLM의 숨겨진 기능을 잠금 해제할 수 있습니다. 또한 TruthRL은 모델의 신뢰성과 진실성 확보의 중요성을 보여줍니다.
*   **모델 감사, 윤리적 고려사항 및 표준화**: "비밀 지식 유도하기" (또는 '모델의 윤리적 편향 식별')와 MLFlow는 숨겨진 지식에 대해 모델을 감사하고, 진실하고 신중하도록 훈련해야 함을 보여줍니다. 앞으로 이러한 통찰력을 결합하면 다음 세대 에이전트(agents)를 탄생시킬 수 있습니다. 즉, 광범위한 탐색(exploration), 보정된 진실성(calibrated truthfulness), 다양한 감독(supervision)을 갖춘 RL을 통해 훈련된, 생성형 기억(generative memory)과 병렬 추론(parallel reasoning)을 가진 대규모 생물학적 영감 네트워크(biologically inspired networks)입니다.

AI의 미래는 신경과학, 최적화, 안전 연구를 아우르는 다학제적 혁신을 포함할 가능성이 높습니다. 계속 지켜봐 주세요!

구독 ❤️ 이 기사가 마음에 드셨다면 '좋아요'를 누르고 동료들과 공유해 주세요.