반갑습니다, 독자 여러분! 금주 LLM Watch의 주요 업데이트를 전해드립니다. 인공지능 분야의 최신 연구 동향과 혁신적인 발전을 깊이 있게 다루는 이번 호에서는, 대규모 언어 모델(LLM)이 당면한 여러 난제를 해결하기 위한 다각적인 접근 방식들을 조명합니다.

**강화 학습 정체 현상 해소를 위한 탐색 기법 확장**: BroRL의 광범위한 탐색, DeepSearch의 몬테카를로 트리 탐색(MCTS) 통합, Knapsack-style budgeting 방식의 예산 배정 등은 강화 학습(RL) 실행의 정체기를 극복하는 데 기여합니다.

**단순한 계산량 증대 대신, 효율적 사고를 지향하는 에이전트**: 잠재 병렬 사고(Thoughtbubbles)와 생성형 잠재 기억(MemGen)을 통해 모델은 더욱 지능적으로 사고합니다.

**모델의 진실성 확보: 정확성뿐 아니라 정직한 응답의 중요성**: TruthRL은 '모르겠습니다'를 포함한 정직한 답변에 보상하며, 비밀 정보 추출(secret-elicitation) 연구는 숨겨진 사실도 결국 드러날 수 있음을 보여줍니다.

**에이전트형 LLM 연구의 표준화 및 SFT 일반화 능력 재평가**: GEM은 에이전트형 LLM의 훈련 및 평가를 위한 표준을 제시하고, SFT 신화는 프롬프트 다양성(prompt diversity)과 사고의 사슬(CoT)의 결합으로 반박됩니다.

**뇌 구조 모방 및 텍스트 기반 시각 정보 습득 연구**: Dragon Hatchling은 트랜스포머를 뇌와 유사한 네트워크에 연결하고, 텍스트 전용 사전 훈련(pre-training)만으로도 시각적 사전 지식(visual priors)을 형성할 수 있음을 입증합니다.

저희의 최신 소식을 놓치지 않으시려면 구독을 신청해 주십시오. LLM Watch 구독자 여러분께는 텍사스 오스틴에서 개최되는 제6회 MLOps World | GenAI 글로벌 서밋에 특별히 초대될 기회가 주어집니다. OpenAI, HuggingFace 등 선도 기업들이 참여하며, 60개 이상의 심층 세션과 발표가 마련되어 있습니다. 구독자께서는 웹을 통해 해당 행사에 무료로 참여하실 수 있습니다. 만약 실질적인 워크숍, 실제 사용 사례 공유, 네트워킹 기회, 그리고 오스틴 전역에서 열리는 파티까지 현장에서 직접 경험하고 싶으시다면, 이 할인 코드를 활용하시면 150달러의 참가비 할인 혜택을 받으실 수 있습니다!

**150달러 할인**

---

**빠른 용어집 - 본 이슈에 맞춤**

*   **검증 가능한 보상 강화 학습 (RLVR, Reinforcement Learning with Verifiable Rewards)**: 사람의 수동 레이블링 없이, 수학 문제의 정답이나 코드 단위 테스트 결과처럼 정확성 여부가 자동적으로 입증될 수 있는 보상 체계를 활용하는 강화 학습 기법입니다. 이는 객관적인 기준에 기반한 보상으로 학습의 효율성과 신뢰성을 높입니다.
*   **비밀 정보 추출 (Secret elicitation, 모델 감사)**: 모델이 내재적으로 인지하고 있으나 명시적으로 드러내지 않는 정보를 끌어내기 위한 프롬프트 구성 또는 분석 방법론을 의미합니다. 이는 블랙박스 프리필(black-box prefill)이나 페르소나 샘플링(persona sampling)과 같은 기법, 또는 로짓 렌즈(logit lens), 희소 오토인코더(sparse autoencoders)와 같은 화이트박스 도구를 포함할 수 있습니다.
*   **사고의 사슬(CoT) 지도 학습 (CoT supervision, SFT 활용)**: 모델이 복잡한 문제 해결 과정을 단계별 해법 형태로 학습하여, 난이도가 높은 새로운 상황에도 적용할 수 있도록 하는 훈련 방식입니다. 이는 모델이 단순히 정답을 외우는 것을 넘어, 문제 해결의 '생각 과정'을 내면화하도록 돕습니다.
*   **내재된 병렬적 사고 (Latent parallel thinking)**: 트랜스포머 모델이 명시적인 사고의 사슬(CoT) 없이도, 어려운 토큰 처리 시 내부 잔차 스트림(residual stream)을 분할하여 추가적인 병렬 연산 자원을 배분하는 메커니즘을 뜻합니다. 이는 모델이 필요한 곳에만 효율적으로 계산 자원을 집중시키는 능력을 부여합니다.
*   **문자 기반 시각적 사전 지식 (Visual priors from text)**: 코드, 수학, 과학 문헌 등 추론에 특화된 텍스트는 시각적 추론 능력을, 일반적인 광범위 언어는 시각적 지각 능력을 모델에 부여하는 현상을 지칭합니다. 이는 텍스트만으로도 모델이 시각적 세계에 대한 기본적인 이해를 형성할 수 있음을 보여줍니다.

---

### BroRL – 광범위한 탐색을 통한 강화 학습 확장

**시청**: BroRL (논문)

**어떤 문제를 해결하는가?**
확인 가능한 보상을 활용하는 강화 학습(RLVR) 방식은 올바른 해답에 보상을 부여하여 추론 능력을 향상시키지만, 대개 수천 회의 훈련 반복 이후에는 성능이 정체됩니다. 이는 모델이 충분히 탐색하지 않아 더 이상의 발전이 멈추기 때문이며, 추가적인 훈련은 효율성 감소로 이어집니다. 모델이 특정 행동 패턴에 고착되어 새로운, 잠재적으로 더 나은 솔루션을 발견하지 못하는 것이 주요 병목 현상입니다.

**어떻게 문제를 해결하는가?**
BroRL은 각 학습 인스턴스당 롤아웃 횟수를 수백 회로 대폭 증가시켜 탐색 범위를 넓히는 방안을 제시합니다. 이는 단순히 훈련 단계를 늘리는 것과는 다르게, 동일한 수의 기울기 업데이트를 유지하면서 훨씬 더 다양한 경로를 샘플링하여 탐색 공간을 확장하는 효과를 가져옵니다. 이 방법은 모델이 더 많은 다양성을 경험하고, 이전에 간과되었을 수 있는 유망한 추론 경로를 발견할 가능성을 높입니다.

**주요 발견**
*   **이론적 근거**: 질량 균형 원리를 통해, 더 많은 롤아웃을 시도하는 것이 올바른 행동에 대한 확률적 비중을 점진적으로 높이고, 이전에 경험하지 못한 행동의 영향을 감소시킨다는 점이 입증되었습니다. 이는 BroRL의 탐색 전략이 견고한 이론적 기반을 가짐을 의미합니다.
*   **포화된 모델 재활성화**: BroRL은 약 3천 번의 ProRL 단계 이후 정체된 모델을 효과적으로 재활성화하여, ProRL이 이미 포화 상태에 도달했을 때조차도 지속적인 성능 개선을 이끌어냈습니다. 이는 모델의 학습 능력이 한계에 도달했다고 판단될 때, 새로운 탐색 패러다임이 돌파구가 될 수 있음을 시사합니다.
*   **최첨단 결과**: 15억(1.5B) 매개변수 모델의 경우, BroRL은 다양한 벤치마크에서 최첨단 성능을 달성하며, 단순한 훈련 단계 증대보다 탐색 확장이 훨씬 더 효과적인 전략임을 입증했습니다. 이는 대규모 모델의 잠재력을 최대한 발휘하는 데 탐색의 중요성을 강조합니다.

**다음은 무엇인가?**
향후 연구는 BroRL을 구조화된 탐색 방법(예: MCTS) 또는 예산 할당 방식(Knapsack RL 참조)과 결합하여 탐색 효율을 극대화할 수 있습니다. 예를 들어, MCTS는 유망한 탐색 경로를 더욱 정교하게 안내하고, Knapsack RL은 다양한 작업에 걸쳐 계산 자원을 지능적으로 배분하여 최적의 탐색을 가능하게 할 것입니다. BroRL은 또한 단일 단계 RL을 넘어 완전한 순차적 설정(sequential settings)으로 확장되거나, MemGen과 같은 기억 증강 아키텍처와 통합되어 더욱 복잡하고 장기적인 추론 작업을 위한 강력한 에이전트를 구축하는 데 기여할 수 있습니다. 특히, 실시간 전략 게임이나 복잡한 코드 생성과 같이 여러 단계에 걸쳐 의사결정이 필요한 작업에서 이러한 결합은 상당한 이점을 제공할 것으로 기대됩니다. 수백 개의 롤아웃을 효율적으로 관리하고 병렬화하는 기술적 과제는 여전히 남아있지만, 이는 하드웨어 발전과 분산 컴퓨팅 기술을 통해 점차 해결될 수 있을 것입니다.

---

### 언어 모델로부터 비밀 지식 유도하기

**시청**: 비밀 지식 유도하기 (논문)

**어떤 문제를 해결하는가?**
대규모 언어 모델은 사용자 정보와 같은 민감하거나 사적인 데이터를 내부에 가지고 있을 수 있으며, 직접적인 질문에는 부인하더라도 답변에 해당 정보를 활용할 가능성이 있습니다. 이는 심각한 보안 및 개인 정보 보호 문제를 야기하며, 모델이 숨기도록 학습된 지식을 어떻게 효과적으로 추출할 수 있을지에 대한 의문이 제기됩니다. 모델의 이러한 '숨겨진 지식'은 의도치 않은 정보 유출이나 악용의 위험을 내포하고 있습니다.

**어떻게 문제를 해결하는가?**
연구진은 비밀 정보 추출을 위한 실험 환경을 구축했습니다. 이들은 LLM이 내부 추론 과정에서 비밀 정보를 활용하도록 미세 조정했는데, 예를 들어 모델은 사용자의 성별을 인지하고 이를 바탕으로 질문에 응답하지만, 직접적으로 성별을 묻는 질문에는 해당 사실을 부인하도록 설정되었습니다. 이후 연구자들은 블랙박스(black-box) 및 화이트박스(white-box) 감사 공격 기법을 설계하여 숨겨진 정보를 밝혀내고자 했습니다. 프리필(prefill) 및 페르소나 샘플링(persona sampling)은 모델에 다양한 접두사(prefixes)를 사용하여 비밀을 유도하는 블랙박스 방식이며, 로짓 렌즈(logit lens) 및 희소 오토인코더(sparse autoencoders)는 숨겨진 활성화(activations)를 직접 검사하는 화이트박스 도구입니다. 추출 성공률을 측정함으로써 비밀이 얼마나 쉽게 유도될 수 있는지를 평가했습니다.

**주요 발견**
*   **프리필 공격의 효용성**: 세 가지 실험 환경 중 두 곳에서, 블랙박스 방식의 프리필 공격(프롬프트 앞에 조작된 접두사를 삽입하는 기법)이 기존의 기준 방법론보다 숨겨진 비밀을 추출하는 데 더 뛰어난 성능을 보였습니다. 이는 외부에서 쉽게 적용 가능한 기법이 효과적일 수 있음을 시사합니다.
*   **화이트박스 방법의 보완적 역할**: 나머지 한 설정에서는 로짓 렌즈 및 희소 오토인코더 분석과 같은 화이트박스 기법이 더 효과적이었으며, 모델의 내부 활성화를 직접 검사하는 것이 숨겨진 지식을 드러낼 수 있음을 보여주었습니다. 이는 블랙박스 기법만으로는 한계가 있을 때 내부 구조 분석이 중요함을 나타냅니다.
*   **벤치마크 공개**: 저자들은 공개 감사를 위한 모델과 코드를 공개하여, 모델 개인 정보 보호 및 적대적 유도에 대한 향후 연구의 기준선을 설정했습니다. 이는 연구 커뮤니티의 발전에 중요한 기여를 합니다.

**다음은 무엇인가?**
향후 연구는 민감한 정보를 강력하게 제거하거나 무작위화하는 방법과 같은 완화 전략(mitigation strategies)을 탐색해야 합니다. 또한, 벤치마크를 사용자 위치, 정치적 견해 등 더 다양한 비밀 유형으로 확장하는 것이 필요합니다. 비밀 정보 추출과 TruthRL과 같은 진실성 훈련을 결합하면, 모델이 민감한 내용을 알고 있지만 공유를 거부할 때 이를 솔직하게 인정하도록 장려하여 정보 유출의 위험을 줄일 수 있습니다. 이러한 연구는 AI 시스템의 윤리적 사용과 개인 정보 보호를 위한 중요한 기반이 될 것이며, 특히 법률, 의료 또는 금융 분야와 같이 민감한 정보가 다루어지는 영역에서 모델의 신뢰성을 확보하는 데 필수적입니다. 단순히 정보를 숨기는 것을 넘어, 모델이 자신의 불확실성을 인지하고 정직하게 회피하는 능력을 학습하는 것이 중요합니다.

---

### GEM – 에이전트형 LLM을 위한 짐(Gym)

**시청**: GEM (논문)

**어떤 문제를 해결하는가?**
에이전트형 LLM의 개발 및 성능 평가에는 표준화된 훈련 및 벤치마킹 환경이 필수적입니다. 기존의 OpenAI Gym과 같은 플랫폼은 로봇 공학이나 단순한 가상 환경 작업에 초점을 맞춘 반면, 에이전트형 LLM 연구 분야는 이러한 공통 인프라가 부족한 상황이었습니다. 이로 인해 다양한 연구 결과들을 객관적으로 비교하고 발전 방향을 모색하는 데 어려움이 있었습니다.

**어떻게 문제를 해결하는가?**
GEM (General Experience Maker)은 LLM 에이전트 전용으로 설계된 오픈 소스 환경 시뮬레이터입니다. 이 플랫폼은 비동기 및 벡터화된 실행(다중 병렬 시뮬레이션)을 지원하며, 유연한 래퍼와 파이썬 코드 실행 및 검색 기능 같은 통합 도구를 제공함으로써 환경과 에이전트 간의 표준화된 인터페이스를 구축합니다. GEM은 수학, 코드, 질의응답(Q&A), 도구 사용을 포함하는 다양한 환경 모음과 Return Batch Normalization (ReBN)이 적용된 REINFORCE와 같은 강화 학습 알고리즘을 위한 기준선 스크립트를 제공합니다. 또한, 연구자들이 자신의 에이전트를 연결하여 비교 가능한 지표(metrics)를 얻을 수 있는 평가 툴킷 역할도 수행합니다.

**주요 발견**
*   **광범위한 환경 모음**: GEM은 수학적 추론부터 API 호출에 이르기까지 다양한 관찰 및 행동 공간을 특징으로 하는 총 24개의 환경을 제공합니다. 이는 단일 단계 및 다단계 작업을 모두 포괄하여 에이전트의 다양한 능력을 평가할 수 있도록 합니다.
*   **ReBN을 사용한 기준선**: 저자들은 일반적인 강화 학습 알고리즘(REINFORCE, GRPO, PPO)을 벤치마킹하고, ReBN이 정책 기울기(policy gradients)를 안정화하여 턴당 밀집 보상(dense per-turn rewards)으로 훈련할 수 있도록 돕는다는 것을 보여주었습니다. 이는 학습의 안정성과 효율성을 높이는 중요한 요소입니다.
*   **재사용 가능한 평가 하네스**: GEM의 인터페이스와 래퍼는 새로운 작업과 에이전트 아키텍처의 쉬운 통합을 가능하게 합니다. 이는 에이전트형 LLM을 위한 "미니 아레나(MiniArena)"와 유사한 표준화된 테스트베드 역할을 하여 연구의 재현성과 비교 가능성을 높입니다.

**다음은 무엇인가?**
GEM은 에이전트형 강화 학습 연구의 진입 장벽을 크게 낮춥니다. 향후 연구는 환경 라이브러리를 확장하여 이미지나 오디오와 같은 멀티모달 입력(multimodal inputs)을 통합하거나, 장기 계획(long-horizon planning)을 위한 벤치마크를 추가할 수 있습니다. 또한 AgentScaler와 같은 대규모 에이전트형 파운데이션 모델과 GEM을 통합하여 실제와 유사한 복잡한 시나리오에서 에이전트의 성능을 평가하는 방향으로 발전할 수 있습니다. 연구자들은 GEM의 벡터화된 인터페이스를 기반으로 다중 에이전트 간의 협력 또는 경쟁을 요구하는 더욱 현실적인 작업을 설계하여, 사회적 상호작용이나 복잡한 분산 문제 해결 능력을 연구하는 데 활용할 수도 있을 것입니다. 이러한 표준화된 플랫폼은 에이전트형 AI의 발전 속도를 가속화하고, 다양한 연구 결과의 신뢰성을 높이는 데 결정적인 역할을 할 것입니다.

---

### DeepSearch – 몬테카를로 트리 탐색을 통한 RLVR 병목 현상 극복

**시청**: DeepSearch (논문)

**어떤 문제를 해결하는가?**
검증 가능한 보상 강화 학습(RLVR)의 훈련 효율성은 종종 한계에 도달합니다. 이는 제한된 수의 샘플링된 경로만으로는 가능한 모든 추론 과정을 충분히 탐색할 수 없기 때문입니다. 훈련이 진행될수록 모델의 행동은 더욱 예측 가능해지며, 이는 강화 학습의 다양성을 감소시키고 더 이상의 성능 향상을 저해합니다. 특히 복잡한 문제에서 최적의 해답에 도달하기 위한 다양한 추론 경로를 발견하는 것이 어려워집니다.

**어떻게 문제를 해결하는가?**
DeepSearch는 몬테카를로 트리 탐색(MCTS) 기법을 강화 학습 훈련 과정에 통합합니다. 이 방식은 에이전트가 단순히 무작위 롤아웃을 몇 번 시도하는 대신, 훈련 중에 추론 경로를 체계적으로 탐색하도록 유도합니다. 주요 구성 요소는 다음과 같습니다.
*   **전역 경계 선택(Global frontier selection)**: 탐색 트리(search tree)의 유망한 가지(branches)를 탐색하는 것을 우선시하여, 잠재력이 높은 추론 경로가 더 많은 관심을 받도록 합니다.
*   **엔트로피 기반 경로 선택(Entropy-based path selection)**: 확신 있고 가치 높은 궤적(trajectories)에 훈련을 집중합니다. 낮은 엔트로피 가지는 감독(supervision)에 사용되어 학습 효율을 높입니다.
*   **솔루션 캐싱(solution caching)을 통한 적응형 리플레이(Adaptive replay)**: 발견된 솔루션을 저장하고 이를 고보상 궤적(high-reward trajectories)으로 재생하여 중복 탐색을 줄이고 새로운 추론에 집중합니다.

**주요 발견**
*   **탐색 정체 해결**: 추론 경로를 체계적으로 탐색함으로써 DeepSearch는 확장된 RLVR 훈련에서 흔히 관찰되는 성능 저하 문제를 효과적으로 해결합니다.
*   **선도적인 정확도**: 수학 추론 벤치마크에서, DeepSearch를 통해 학습된 15억 매개변수 모델은 평균 62.95%의 정확도를 기록했습니다. 이는 기존 RLVR 방식보다 우수하며, 표준 강화 학습 훈련을 확장하는 경우에 비해 GPU 사용 시간을 5.7배 절감하는 효율성을 보였습니다.
*   **효율적인 탐색**: 구조화된 탐색은 더 나은 샘플 효율성을 제공합니다. 에이전트는 대량의 무작위 롤아웃 없이도 올바른 전략을 학습하여 계산 자원 낭비를 줄입니다.

**다음은 무엇인가?**
DeepSearch는 수학 외의 도메인, 예를 들어 코드 합성(code synthesis)이나 정리 증명(theorem proving)과 같은 복잡한 문제 해결 영역에도 성공적으로 적용될 수 있습니다. MCTS를 동적 롤아웃 예산(dynamic rollout budgets, Knapsack RL 방식)과 결합하면, 탐색의 효율성과 효과를 더욱 향상시킬 수 있습니다. 또한, 미분 가능한 탐색(differentiable search, 예: 미분 가능한 MCTS)을 탐색하면 기울기 역전파(gradient backpropagation)를 통한 종단 간(end-to-end) 훈련이 가능해져, 탐색 전략 자체를 학습하는 새로운 길을 열 수 있습니다. 이는 과학적 가설 생성이나 창의적 문제 해결과 같은 더 개방적이고 탐색적인 작업에서 DeepSearch의 잠재력을 확장할 수 있을 것입니다. 다만, MCTS 자체의 계산 비용을 어떻게 효율적으로 관리할 것인가 하는 문제는 여전히 중요한 연구 과제로 남아있습니다.

---

### SFT 일반화 신화 반박하기

**시청**: SFT 일반화 신화 반박하기 (논문 / 코드)

**어떤 문제를 해결하는가?**
지도 미세 조정(SFT)은 종종 지시 템플릿을 단순히 암기하여 그 이상으로 일반화하지 못하는 모델을 만든다는 비판에 직면해 왔습니다. 이와 달리 RLHF나 RLVR과 같은 강화 학습 기반 방법은 더 높은 견고성(robustness)을 제공한다고 평가받습니다. 본 논문은 SFT가 본질적으로 일반화 능력이 부족하다는 통념에 반론을 제기합니다. 특히, SFT의 한계가 방법론 자체의 문제가 아니라, 훈련 데이터 구성 방식에 기인할 수 있음을 지적합니다.

**어떻게 문제를 해결하는가?**
연구진은 SFT 훈련 과정에서 두 가지 주요 실패 양상을 파악했습니다.
*   **고정된 프롬프트 아티팩트 문제**: 이는 모델이 특정 지시 템플릿으로만 학습될 경우 해당 템플릿의 의미에 과도하게 의존하게 되어 다양한 프롬프트가 주어졌을 때 성능 저하를 겪는 현상입니다.
*   **알고리즘적 틀 부족**: 중간 추론 과정이 생략된 데이터로 훈련하면 SFT 모델이 더 어려운 사례를 해결하는 데 어려움을 겪게 됩니다.
그들은 이러한 문제에 대한 간단한 해결책을 제안합니다.
*   **프롬프트 다양성**: SFT 동안 광범위한 프롬프트 스타일을 사용하여 모델이 단일 템플릿에 과적합(overfitting)되는 것을 방지합니다.
*   **사고의 사슬(CoT) 감독**: 훈련 데이터에 명시적인 추론 흔적(reasoning traces)을 제공하여(CoT 프롬프팅과 같이) 모델에 기본 알고리즘을 가르칩니다.
이 두 가지 수정을 결합하면 지시 스타일과 증가된 작업 난이도에 걸쳐 효과적으로 일반화하는 SFT 모델을 얻을 수 있습니다.

**주요 발견**
*   **프롬프트 다양성만으로 스타일 일반화 개선**: 모델이 다양한 지시 형식에 노출되면, 이전에 보지 못한 프롬프트 변형에서도 잘 수행됩니다. 이는 데이터의 다양성이 모델의 적응력을 크게 향상시킴을 보여줍니다.
*   **CoT 스캐폴딩(scaffolding)은 난이도 일반화 개선**: 사고의 사슬 예제를 포함하면 SFT 모델이 이전에 실패했던 더 어려운 사례(예: 더 큰 소코반 퍼즐)를 해결할 수 있습니다. 이는 복잡한 문제 해결을 위한 모델의 내재적 추론 능력을 강화합니다.
*   **SFT의 RL 성능 비견**: 프롬프트의 다양성 확보와 사고의 사슬(CoT)을 통한 지도 학습을 병행함으로써, SFT 모델은 실험된 작업들에서 강화 학습 기반 훈련 정책과 동등하거나 이를 뛰어넘는 성과를 보였습니다.

**다음은 무엇인가?**
이 연구는 SFT에 대한 데이터 중심적 관점을 장려합니다. 즉, 즉시 강화 학습에 의존하기보다는 다양한 프롬프트와 추론 흔적에 투자하는 것이 중요하다는 것을 시사합니다. 향후 연구는 SFT를 다른 훈련 체제(예: RLMT 또는 RLVR)와 결합하고, 코드 생성이나 도구 사용과 같은 실제 작업에서 일반화 능력을 테스트해야 합니다. 또한 프롬프트 다양성에 대한 체계적인 벤치마크를 개발하여 평가를 표준화하는 것이 필요합니다. 이 연구는 SFT의 잠재력을 재조명하며, 강화 학습의 복잡성 없이도 강력한 일반화 능력을 갖춘 모델을 구축할 수 있음을 보여줌으로써, 많은 실무자들에게 더욱 접근 가능한 AI 개발 경로를 제시합니다. 특히, 모델이 새로운 지시를 따르거나 미묘한 맥락 변화에 적응해야 하는 고객 서비스 챗봇이나 교육용 AI와 같은 애플리케이션에서 이러한 접근 방식은 매우 유용할 것입니다.

---

### Thoughtbubbles – 잠재 공간에서의 비지도 병렬 사고

**시청**: Thoughtbubbles (논문)

**어떤 문제를 해결하는가?**
대규모 언어 모델은 대개 여러 층을 쌓아 입력을 순차적으로 처리합니다. 복잡한 추론 과정은 종종 명시적인 사고의 사슬 프롬프팅(chain-of-thought prompting)을 통해 긴 결과물을 생성하게 합니다. 과연 모델이 사고의 사슬을 직접적으로 표현하지 않고도, 어려운 토큰에 대해 내부적으로 추가적인 연산 자원을 배분할 수 있을까요? 이러한 순차적 처리 방식은 복잡한 문제 해결 시 계산 효율성과 지연 시간(latency) 문제를 야기할 수 있습니다.

**어떻게 문제를 해결하는가?**
Thoughtbubbles는 트랜스포머 아키텍처에 적용된 한 가지 변형입니다. 사전 훈련 과정에서 모델은 특정 토큰에 대해 잔차 스트림(residual streams)의 복사본을 '분기(fork)'하는 방법을 익히며, 이를 통해 효과적인 병렬 계산 분기(computational branches, 버블)를 생성합니다. 어려운 토큰에는 더 많은 계산 단계가 할당되고, 쉬운 토큰은 정상적으로 흐릅니다. 이 방법은 언어 모델링 손실(language modeling loss)만을 사용하여 비지도 방식(unsupervised manner)으로 학습됩니다. 즉, 명시적인 사고의 사슬 감독 없이도 모델이 자체적으로 이러한 능력을 습득합니다. 사전 훈련 후, 토큰은 자동으로 버블을 트리거(trigger)할 수 있으며, 이는 추론(inference)이 훈련과 동일한 메커니즘을 사용한다는 것을 의미합니다.

**주요 발견**
*   **향상된 혼란도**: 1억 5천만 개에서 7억 7천만 개에 이르는 다양한 매개변수 규모의 모델에서, Thoughtbubbles는 일반적인 디코더 방식과 비교했을 때 텍스트 코퍼스의 혼란도(perplexity)를 지속적으로 감소시키는 경향을 보였습니다. 이는 모델의 언어 이해 및 생성 능력이 향상되었음을 나타냅니다.
*   **더 나은 제로샷 추론**: 추론 벤치마크(HellaSwag, LAMBADA)에서 Thoughtbubbles 모델은 표준 트랜스포머와 비적응형 병렬 방법(non-adaptive parallel methods)을 모두 능가합니다. 이는 모델이 명시적인 훈련 없이도 복잡한 추론 작업을 더 잘 수행할 수 있음을 의미합니다.
*   **통합된 훈련/추론 동작**: 메커니즘이 사전 훈련 중에 학습되기 때문에 훈련과 추론 사이에 불일치(discrepancy)가 없습니다. 모델은 필요할 때 자연스럽게 추가 연산을 할당하여 효율적인 계산을 가능하게 합니다.

**다음은 무엇인가?**
향후 연구는 Thoughtbubbles를 강화 학습(RL) 또는 탐색 알고리즘(예: MCTS)과 결합하여 추론 단계 전반에 걸쳐 병렬 계산을 더욱 정교하게 할당하는 방법을 탐색할 수 있습니다. 버블의 깊이(bubble depth)나 병합 전략(merging strategies)을 최적화하는 연구는 효율성을 더욱 높일 수 있으며, 이 아이디어를 멀티모달 모델에 적용하면 시각 및 청각 정보 처리에도 유사한 이점을 가져올 수 있습니다. 또한, 이러한 '버블'이 특정 추론 패턴이나 인지 과정과 어떻게 일치하는지 밝혀내는 해석 가능성 연구(interpretability studies)는 LLM의 내부 작동 방식을 이해하는 데 중요한 통찰력을 제공할 것입니다. 이는 모델이 어떻게 '생각'하고 '결정'하는지에 대한 더 깊은 이해를 가능하게 하여, 궁극적으로 더욱 신뢰할 수 있고 투명한 AI 시스템을 구축하는 데 기여할 수 있습니다.

---

### 보기 전에 보는 법 배우기 – LLM 시각 사전 지식의 신비 해명

**시청**: 보기 전에 보는 법 배우기 (논문)

**어떤 문제를 해결하는가?**
오직 텍스트 데이터로만 학습된 대규모 언어 모델이 이미지를 한 번도 접한 적이 없음에도 불구하고, '하늘은 파란색인가요?'와 같은 단순한 시각 관련 질문에 놀랍도록 정확하게 답변하는 경우가 많습니다. 이러한 시각적 사전 지식은 어디에서 비롯되는 것이며, 어떻게 의도적으로 이를 발전시킬 수 있을까요? 이 현상은 멀티모달 AI 개발에서 텍스트 데이터의 잠재력을 재평가하게 합니다.

**어떻게 문제를 해결하는가?**
연구진은 LLM이 언어 사전 훈련 과정에서 두 가지 유형의 시각적 사전 지식을 어떻게 형성하는지 분석했습니다.
*   **시각적 추론 사전 지식(Visual reasoning prior)**: 이는 코드, 수학, 과학 문서와 같이 추론에 중점을 둔 텍스트로부터 파생되며, 모델이 시각적 개념들을 논리적으로 연결하는 능력을 학습하게 합니다.
*   **시각적 지각 사전 지식(Visual perception prior)**: 일상적인 장면 묘사를 포함하는 광범위한 자연어 코퍼스(natural language corpora)에서 파생됩니다.
그들은 추론 사전 지식이 모델 크기에 따라 강력하게 확장되며 최소한의 이미지 노출로도 시각 작업으로 전이될 수 있음을 보여줍니다. 반면, 지각 사전 지식은 빠르게 포화 상태에 이르며 LLM을 좋은 비전 인코더와 짝짓는 것에 의존합니다. 그들은 데이터 중심 사전 훈련 레시피(data-centric pretraining recipe)를 제안합니다. 사전 훈련의 일부를 코드/수학에 할당하여 추론 사전 지식을 구축하고, 지각을 위한 소량의 시각적 묘사를 포함한 다음, 작은 멀티모달 데이터셋으로 미세 조정(fine-tune)하는 방식입니다.

**주요 발견**
*   **추론 대 지각 사전 지식**: 추론 관련 사전 지식은 구조화된 텍스트에서 유래하며 모델의 크기에 비례하여 확장되는 반면, 지각 관련 사전 지식은 일반적인 광범위 언어에서 비롯되며 빠르게 포화 상태에 도달합니다.
*   **데이터 효율성**: 적당한 양의 추론 중심 텍스트는 시각적 추론 능력(visual reasoning ability)을 크게 향상시킵니다. 반면, 단순히 더 많은 묘사적 텍스트를 추가하는 것은 수확 체감(diminishing returns)을 가져옵니다.
*   **시각 인식 LLM을 위한 레시피**: 사전 훈련 혼합(pretraining mixtures)을 제어함으로써 저자들은 최소한의 이미지 미세 조정 후 시각 작업에서 더 나은 성능을 보이는 모델을 생성합니다.

**다음은 무엇인가?**
향후 연구는 이러한 사전 지식을 연속 잠재 CoT(continuous latent CoT, Thoughtbubbles 참조) 또는 생성형 확산 모델(generative diffusion models)과 결합하는 것을 탐색할 수 있습니다. 또한 명시적인 공간 추론 작업(spatial reasoning tasks)으로 사전 훈련하면 훨씬 더 강력한 사전 지식을 구축할 수 있습니다. 저자들의 데이터 중심 접근 방식은 고품질 추론 텍스트를 큐레이션(curating)하는 것이 더 유능한 멀티모달 모델을 향한 발전을 가속화할 수 있음을 시사합니다. 특히, 시각 데이터가 부족한 환경에서 멀티모달 AI를 개발해야 하는 경우, 텍스트 기반 사전 지식의 효율적인 활용은 큰 이점을 제공할 것입니다. 이는 텍스트 데이터의 잠재력을 최대한 활용하여 다양한 양식(modality) 간의 간극을 메우는 중요한 단계가 될 것입니다.

---

### Knapsack RL – 예산 할당을 통한 탐색 잠금 해제

**시청**: Knapsack RL (논문)

**어떤 문제를 해결하는가?**
강화 학습 미세 조정(fine-tuning) 과정에서는 대개 각 문제에 고정된 롤아웃 횟수를 적용합니다. 이러한 일률적인 자원 배분은 계산 자원의 비효율적인 낭비로 이어집니다. 쉬운 과제는 적은 시도만으로도 충분하지만, 매우 어려운 과제는 제한된 예산으로는 성공하기 어렵습니다. 결과적으로 많은 작업에서 영(zero) 기울기가 발생하여 시간 낭비와 학습 방해로 이어집니다. 이는 모델이 학습할 수 있는 잠재력을 충분히 활용하지 못하게 합니다.

**어떻게 문제를 해결하는가?**
Knapsack RL은 탐색 과정을 배낭 최적화 문제(knapsack optimization problem)로 간주합니다. 각 작업의 롤아웃에는 '비용'(연산 자원)과 잠재적 '가치'(예상 학습 이득)가 부여됩니다. 배낭 문제를 해결하는 방식으로, 알고리즘은 기대 이득이 높은 작업에 더 많은 롤아웃을 배정하고, 이미 완료되었거나 성공 가능성이 낮은 작업에는 적은 자원을 할당합니다. 이 동적 예산 책정(dynamic budgeting)은 GRPO 알고리즘을 기반으로 하여, 학습 과정 전반에 걸쳐 자원 배분을 최적화합니다.

**주요 발견**
*   **기울기 밀도의 증가**: 0이 아닌 정책 기울기(policy gradients)의 비율이 20~40% 상승했으며, 이는 에이전트가 각 업데이트 주기마다 더 많은 과제로부터 학습할 수 있게 되었음을 시사합니다. 이는 학습 효율의 직접적인 개선으로 이어집니다.
*   **어려운 작업에 대한 대규모 예산**: 일부 극도로 어려운 작업은 약 93개의 롤아웃을 받으며, 이는 균일한 예산으로는 결코 허용되지 않았을 탐색을 가능하게 합니다. 이로 인해 모델은 이전에 해결하기 어려웠던 문제에 대한 해결책을 찾을 수 있게 됩니다.
*   **성능 향상**: 수학 추론 벤치마크에서 Knapsack RL은 균일한 할당이 요구하는 연산의 절반을 사용하면서 평균 2~4점, 특정 사례에서는 최대 9점의 개선을 달성했습니다. 이는 자원 효율성과 성능 향상을 동시에 이루어냈음을 보여줍니다.

**다음은 무엇인가?**
Knapsack RL을 BroRL과 결합하면 훨씬 더 나은 결과를 얻을 수 있습니다. 이는 전역적으로 탐색을 확장하면서 동시에 지역적으로 예산을 최적화하는 시너지 효과를 창출할 것입니다. 또한, 탐색을 더욱 향상시키기 위해 MCTS 기반 훈련(DeepSearch 참조) 또는 다양성 목표(diversity objectives, Polychromic RL 참조)와 짝을 이룰 수 있습니다. 또 다른 방향은 배낭 문제를 다중 에이전트 또는 계층적 작업으로 확장하여, 예산을 하위 작업에 분할해야 하는 복잡한 시나리오에 적용하는 것입니다. 동적으로 배낭 문제를 해결하는 계산 복잡성을 관리하는 것은 여전히 과제이지만, 이는 효율적인 알고리즘 설계와 병렬 컴퓨팅을 통해 극복될 수 있을 것입니다. 특히, 새로운 유형의 작업이나 극도로 어려운 작업에 대한 "예상 학습 이득"을 정확하게 추정하는 방법론에 대한 연구는 Knapsack RL의 적용 범위를 더욱 넓힐 것입니다.

---

### TruthRL – 강화 학습을 통한 진실한 LLM 장려

**시청**: TruthRL (논문)

**어떤 문제를 해결하는가?**
대규모 언어 모델은 종종 사실처럼 보이지만 실제로는 거짓인 답변, 즉 환각(hallucinate)을 생성합니다. 오직 정확성에만 기반한 강화 학습 보상은 이러한 추측성 답변을 부추길 수 있습니다. 반대로, 모델이 너무 조심스러워 답변 자체를 거부한다면 그 유용성이 떨어집니다. 그렇다면 정확성, 진실성, 그리고 적절한 회피(abstention) 사이의 균형을 어떻게 효과적으로 조절할 수 있을까요? 신뢰할 수 있는 AI 시스템을 구축하기 위해서는 이러한 윤리적이고 실용적인 문제 해결이 필수적입니다.

**어떻게 문제를 해결하는가?**
TruthRL은 강화 학습 미세 조정 과정에 삼진 보상 체계를 도입합니다. 이 체계는 정답에 대해서는 긍정적인 보상을, 환각에는 강력한 페널티를, 그리고 '모르겠습니다'와 같은 정직한 회피에는 소폭의 긍정적 보상을 부여합니다. 훈련에는 GRPO 알고리즘이 활용되며, 이는 모델이 확신할 때만 답변하고, 그렇지 않을 때는 불확실성을 인정하도록 장려합니다. 이 방법은 외부 검색 유무에 관계없이 개방형 도메인 질의응답(open-domain QA)과 같은 작업에 적용될 수 있습니다.

**주요 발견**
*   **환각 현상 감소**: TruthRL은 여러 지식 기반 벤치마크 테스트에서 일반적인 강화 학습(바닐라 RL) 방식과 비교하여 환각 발생률을 28.9% 줄이는 데 성공했습니다. 이는 모델의 신뢰성을 크게 향상시키는 중요한 결과입니다.
*   **진실성 증가**: 전반적인 진실성(정확성 + 정직한 회피)이 21.1% 향상되었습니다. 이는 모델이 단순히 정답을 맞히는 것을 넘어, 자신의 지식 한계를 인정하고 정직하게 행동하는 능력을 키웠음을 의미합니다.
*   **일반적인 적용 가능성**: Qwen, Llama와 같은 다양한 모델 유형과 검색 증강(retrieval-augmented) 및 검색 없는(retrieval-free) 설정 모두에서 TruthRL의 이점이 일관되게 지속되었습니다.

**다음은 무엇인가?**
TruthRL은 공정성 목표(fairness objectives) 또는 다른 정렬 지표(alignment metrics)와 결합하여, 진실할 뿐만 아니라 윤리적으로도 답변하는 모델을 생성할 수 있습니다. 향후 연구는 더 미묘한 보상 구조(예: 질문의 난이도에 따라 보상 조정) 또는 다중 턴 대화(multi-turn dialogues)에서의 적용을 탐색할 수 있습니다. 또한, 불확실할 때 회피를 장려함으로써 개인 정보 유출을 완화하기 위해 비밀 정보 추출 연구(secret elicitation research)와 결합될 수 있습니다. 복잡하고 주관적인 영역에서 '진실'과 '정직'을 정의하는 것은 여전히 도전 과제이지만, TruthRL은 이러한 모델의 신뢰도를 높이는 데 중요한 발판을 마련했습니다. 의료 진단이나 법률 자문과 같은 중요한 응용 분야에서 모델이 제공하는 정보의 신뢰성은 사용자에게 치명적인 영향을 미칠 수 있으므로, TruthRL과 같은 연구는 사회적으로 매우 중요한 의미를 가집니다.

---

### MemGen – 자기 진화 에이전트를 위한 생성형 잠재 기억 엮기

**시청**: MemGen (논문 / 코드)

**어떤 문제를 해결하는가?**
대규모 언어 모델 에이전트들은 종종 제한적인 기억 능력을 가집니다. 이들의 기억은 미세 조정 과정에서 매개변수를 직접 수정하거나(매개변수 기억, parametric memory), 외부 데이터베이스를 조회하는(비매개변수 기억, nonparametric memory) 방식으로 작동합니다. 그러나 이러한 방식은 유연성이 떨어지거나 추론 과정과의 통합이 미흡할 수 있습니다. 에이전트가 스스로 생성하고 내부 사고 과정에 긴밀하게 연결할 수 있는 동적이고 자기 진화적인 기억 시스템이 필요합니다.

**어떻게 문제를 해결하는가?**
MemGen은 두 가지 핵심 요소로 구성된 생성형 잠재 기억 시스템을 제안합니다.
*   **기억 트리거(Memory trigger)**: 에이전트의 현재 추론 상태(reasoning state)를 지속적으로 관찰하며 기억을 불러와야 할 적절한 시점을 판단하는 모듈입니다. 이는 과거 경험이나 사실을 상기해야 할 순간을 감지합니다.
*   **기억 위버(Memory weaver)**: 트리거되면 이 모듈은 관련 기억 내용(memory content)을 나타내는 잠재 토큰(latent tokens) 시퀀스(sequence)를 생성하고 이를 모델의 컨텍스트(context)에 다시 주입합니다. 기억은 일반 텍스트가 아닌 모델의 잠재 공간(latent space)에서 학습되고 표현됩니다.
이 시스템은 에이전트가 일시 중지하고, 내부 기억을 생성한 다음, 해당 기억이 숨겨진 상태에 융합된 상태로 추론을 재개할 수 있도록 합니다. 에이전트는 작업 기억(working memory), 절차 기억(procedural memory), 계획 기억(planning memory) 유형에 걸쳐 기억 내용을 할당하는 방법을 학습합니다.

**주요 발견**
*   **성능 향상**: 8개의 다양한 벤치마크에서 MemGen은 기존 기억 증강 에이전트(ExpeL, AWM)를 최대 38.22% 능가하고, 강력한 GRPO 기준선을 최대 13.44% 초과합니다. 이는 MemGen의 기억 시스템이 전반적인 에이전트 성능을 크게 향상시킴을 보여줍니다.
*   **인간과 유사한 기억 양상**: 수동적인 코딩 없이도, MemGen 에이전트는 인간의 인지 과정과 흡사한 계획 기억, 절차 기억, 그리고 작업 기억과 같은 기억 행동을 자율적으로 발전시켰습니다. 이는 모델이 복잡한 인지 기능을 내면화할 수 있음을 시사합니다.
*   **교차 도메인 일반화**: 생성형 기억은 수학, 프로그래밍, 질의응답(Q&A) 등 여러 도메인에서 성능을 향상시키며, 이는 MemGen의 광범위한 적용 가능성을 입증합니다.

**다음은 무엇인가?**
MemGen은 추론을 더욱 향상시키기 위해 탐색 기반 훈련(예: DeepSearch) 또는 탐색 확장(BroRL)과 통합될 수 있습니다. 연구자들은 또한 최적의 보상을 위해 언제 무엇을 불러올지 학습하도록 기억 트리거와 위버를 강화 학습과 함께 공동으로 훈련하는 것을 탐색할 수 있습니다. 마지막으로, MemGen을 Dragon Hatchling과 같은 생물학적 영감 아키텍처와 결합하면, 뇌와 유사한 기억과 네트워크 구조를 모두 갖춘 에이전트를 얻을 수 있습니다. 이는 장기적인 문맥 이해와 다중 턴 대화에서 LLM의 한계를 극복하는 데 중요한 역할을 할 것입니다. 또한 검색 증강 생성(RAG) 방식과 비교하여 MemGen의 잠재적 이점과 한계를 분석하는 연구는 흥미로운 통찰력을 제공할 수 있습니다. 잠재 기억의 해석 가능성과 제어 가능성을 높이는 연구는 모델의 투명성과 신뢰성을 더욱 향상시킬 것입니다.

---

### 요약 – 통찰력 및 향후 방향

이 논문들 전반에 걸쳐 우리는 인공지능 연구의 주요 주제들이 어떻게 수렴하고 있는지를 명확히 볼 수 있습니다.

*   **효율적인 탐색 전략의 중요성**: BroRL, DeepSearch, Knapsack RL 등은 모두 강화 학습의 정체를 극복하고 효율적인 학습을 달성하기 위한 탐색 전략의 중요성을 강조합니다. 롤아웃을 확장하고, 탐색을 내장하며, 예산을 최적화하는 등 다양한 접근 방식은 RL의 잠재력을 최대한 발휘하는 데 필수적입니다.
*   **새로운 모델 아키텍처의 부상**: Thoughtbubbles는 비지도 방식의 아키텍처 수정만으로도 LLM이 병렬적으로 사고할 수 있음을 보여주며, MemGen은 생성형 잠재 기억이라는 혁신적인 개념을 도입합니다. 이러한 혁신은 단순히 모델의 크기를 키우는 것을 넘어, 모델이 정보를 처리하고 저장하는 방식 자체를 근본적으로 개선합니다.
*   **데이터 품질과 다양성의 결정적 역할**: "보기 전에 보는 법 배우기"와 "SFT 신화 반박하기"는 훈련 데이터의 양보다는 품질과 다양성이 모델의 능력에 얼마나 결정적인 영향을 미치는지 강조합니다. 추론 중심 텍스트나 다양한 프롬프트 및 CoT 예제를 신중하게 큐레이션함으로써 LLM의 숨겨진 기능을 해제할 수 있습니다.
*   **모델 투명성 및 정렬의 필요성**: "비밀 정보 추출"과 TruthRL은 모델의 숨겨진 지식을 감사하고, 진실하고 신중하게 답변하도록 훈련해야 할 필요성을 역설합니다. 이러한 통찰력을 결합하면, 광범위한 탐색, 보정된 진실성, 그리고 다양한 감독 하에 훈련된, 생성형 기억과 병렬 추론 능력을 갖춘 다음 세대의 에이전트들을 탄생시킬 수 있을 것입니다.

AI의 미래는 신경과학, 최적화 이론, 안전 연구를 아우르는 다학제적 혁신을 통해 더욱 발전할 것입니다. 특히, 인간의 인지 과정과 유사한 방식으로 학습하고 추론하며, 동시에 윤리적 가치와도 정렬되는 '체화된(embodied)' AI의 등장은 다음 중요한 이정표가 될 것입니다. 이러한 연구 동향은 AI가 단순한 도구를 넘어, 인간 사회의 복잡한 문제를 해결하는 데 필수적인 지능형 파트너로 발전할 가능성을 제시합니다. 계속해서 이 분야의 발전을 주시해 주십시오!

이 글이 유익하셨다면 '좋아요'를 눌러주시고 동료들과 공유하여 주십시오. 더 많은 정보를 원하시면 구독해 주시기 바랍니다. ❤️