(출처: [2, 4, 6, 10, 29]) 거대 언어 모델(LLM)의 능력은 최근 몇 년간 괄목할 만한 성장을 이루었습니다. 하지만 이들을 둘러싼 핵심 사상들, 즉 자기 주도 학습을 통한 사전 훈련(self-supervised pretraining), 변환기 아키텍처(transformer architecture), 그리고 인간의 반응을 활용한 학습(learning from human feedback) 등은 이미 수십 년 전부터 자연어 처리(natural language processing) 분야의 연구에 그 뿌리를 내리고 있습니다. 이러한 개념들은 완전히 새로운 발상이 아니라, 10년 이상 관련 연구에서 꾸준히 발전해 온 지식의 축적입니다. 따라서 기계 번역, 정보 요약, 질문 응답 등 자연어 처리의 본질적인 문제에 대한 심층적인 탐구는 대단히 중요합니다! 이 글에서는 언어 모델 연구의 발전에 지대한 영향을 미친 (생성적) 텍스트 요약(abstractive text summarization)이라는 과제를 중심으로 이 점을 설명하고자 합니다. "텍스트 요약은 긴 문서들을 원본의 핵심 내용을 보존하면서도 짧고 유려하며 사람이 쉽게 읽을 수 있는 형태로 압축하는 것을 목표로 합니다." - 출처 [11] 본질적으로 요약은 긴 텍스트 흐름을 핵심 정보를 여전히 담고 있는 훨씬 간결한 순서로 줄이는 작업을 의미합니다.

요약은 일반적으로 뉴스 기사 간추리기와 같은 정형화된 상황에 적용되지만, 그 문제 정의는 매우 폭넓고 다양한 흥미로운 활용 사례를 포함할 수 있습니다:

*   **정보 검색 및 생성(RAG) 시스템**을 위한 전처리 단계로, 방대한 양의 비정형 텍스트를 효율적으로 압축.
*   **추천 시스템**의 결과물에 대해 사용자 친화적인 설명 요약 생성.
*   **회의록 음성 기록**에서 주요 논의 사항을 요약하여 회의록 자동 작성.
*   **법률 문서**나 **학술 논문**의 핵심 내용을 빠르게 파악하기 위한 요약본 제작.

이 작업이 이토록 보편적이고 강력하기에, 텍스트 요약은 실용적으로 가치 있는 방대한 연구로 둘러싸여 있습니다. 우리가 앞으로 살펴보겠지만, 요약 연구에서 도출된 많은 핵심 개념들이 현대 거대 언어 모델에 적용되었습니다. 이 두 연구 분야는 상호 밀접하게 연결되어 있으며, 요약 연구에 대한 깊이 있는 이해는 거대 언어 모델이 왜 그리고 어떻게 그렇게 뛰어난 성능을 보이는지에 대한 새롭고 향상된 관점을 제공할 것입니다!

**요약에 대한 유용한 배경 정보**
최근의 요약 연구에 들어가기 전에, 기본적인 지식을 습득하는 것이 필요합니다. 이 부분에서는 요약 작업의 전반적인 개요를 다루고, 여러 요약 방식, 거대 언어 모델 등장 이전의 요약 연구 동향, 그리고 요약 품질을 측정하는 지표 등을 살펴볼 것입니다.

**요약의 종류**
**생성 요약(Abstractive summarization) 대 발췌 요약(extractive summarization)**
요약의 목표는 원문의 핵심 아이디어를 포착하는 더 짧은 텍스트를 만들어 정보를 핵심 구성 요소로만 줄이는 것입니다. 학계 문헌에는 두 가지 주요 요약 기법이 연구되고 있습니다(그림은 위 참조):
*   **발췌 요약(Extractive)**: 원본 문서에서 핵심 문장이나 특정 텍스트 단락을 선별적으로 복사하여 요약문을 구성합니다.
*   **생성 요약(Abstractive)**: 원본 문서의 내용을 재해석하고 재구성하여, 관련 정보에 대한 새로운 형태의 간결한 설명을 만들어냅니다.

이 두 가지 요약 전략은 광범위하게 연구되어 왔지만, 거대 언어 모델로 생성된 요약은 주로 생성 요약으로 분류됩니다. 그 이유는 무엇일까요? 일반적으로 거대 언어 모델은 원문에서 문장을 직접적으로 복사하도록 설계되지 않습니다. 모델은 주어진 맥락(context)을 기반으로 자유롭게 텍스트를 생성하며, 이 과정에서 원본 문서의 정보를 임의로 재구성하여 생성 요약을 만들어낼 수 있습니다. 하지만 실제로는 거대 언어 모델로 만들어진 요약이 (상대적으로) 발췌적 특성을 띠는 경향이 있음이 관찰됩니다. 게다가, 최근 연구는 i) 발췌 요약 기법을 활용하여 생성 요약 모델 [7, 12]의 입력으로 포함할 핵심 맥락(context)을 식별하거나, 심지어 ii) 거대 언어 모델이 답변을 생성할 때 관련 출처(또는 텍스트 구간)를 인용하도록 학습시키는 [10] 관련 주제를 탐구하기 시작했습니다(그림은 아래 참조). 이러한 이유로, 기법 자체는 다를 수 있지만, 발췌 요약과 생성 요약 사이에는 상당한 중첩 영역이 존재합니다.

(출처: [10]) **추가 탐구 자료.** 이 글의 주된 초점은 거대 언어 모델을 활용한 요약이므로, 거대 언어 모델이 보편화되기 이전의 생성 및 발췌 요약 기법에 대해 깊이 다루지는 않을 것입니다. 그러나 그러한 초기 연구들은 이 글에서 다룰 내용의 기초를 다졌기에 여전히 매우 중요합니다. 관심 있는 독자를 위해 엄선된 논문 목록이 아래에 제시됩니다:

*   **SummEval: 요약 평가 재고찰 [11]**: 이 연구는 요약을 위한 자동 평가 지표에 대한 심층 분석을 제공하며, 기존 프로토콜의 한계를 탐구하고, 제안된 개선 사항을 적용하여 24개의 생성 및 발췌 요약 모델 스위트를 재평가합니다.
*   **신경망 기반 문서 요약의 생성성에 대하여 [13]**: 이 연구는 생성 요약 기법을 분석하며, 많은 생성 요약이 실제로는 본질적으로 매우 발췌적이라는 사실을 밝혀냅니다!
*   **생성 요약의 충실도와 사실성 [15]**: 이 연구는 생성 요약 기법의 충실도와 사실성에 대한 포괄적인 분석을 제공합니다. 유사한 분석은 [14, 16, 17]에서도 찾아볼 수 있습니다.

주목할 만한 발췌 요약 기법으로는 NEUSUM, BanditSum, LATENT, REFRESH, RNES, JECS, STRASS 등이 있습니다. 발췌 요약 기법에 대한 더 광범위한 조사는 여기를 참조하십시오.
주목할 만한 (LLM 이전) 생성 요약 기법으로는 Pointer Generator, Fast-abs-rl, Bottom-Up, ROUGESal, Soft-MT, SENECA, T5, BertSum, Pegasus, BART, UniLM 등이 있습니다. 생성 요약 기법에 대한 더 광범위한 조사는 여기를 참조하십시오.
또한, 거대 언어 모델에 대한 초기 연구(예: GPT-2 및 GPT-3)의 대부분은 모델 품질 평가를 위한 핵심 작업으로 생성 요약을 활용합니다.

**거대 언어 모델로 요약 생성하기**
(출처: [6]) 이 글의 나머지 부분에서 확인하겠지만, 거대 언어 모델은 고품질의 생성 요약을 만드는 데 매우 효과적인 도구입니다. 요약을 위해 거대 언어 모델을 사용하는 방식은 여러 가지가 있습니다(위 참조). 크게 보면 이러한 접근 방식은 다음 중 하나입니다:
*   오직 문맥 내 학습(in-context learning)을 활용하거나
*   요약 전용 데이터로 맞춤형 모델을 미세 조정(finetune)합니다.

만약 기본 모델(out-of-the-box model)이 우리 애플리케이션에 잘 맞는다면, 문맥 내 학습이 가장 간단한 방법일 것입니다. 우리는 단순히 API를 호출하여(프롬프트에 특정 맥락(context)을 추가하여) 요약을 생성할 수 있습니다. 하지만 프롬프트(prompt)만으로는 충분하지 않다면, 지도 학습 방식(supervised fashion) 또는 선호도 튜닝(preference tuning)을 사용하여 맞춤형 모델을 미세 조정하는 것을 고려해 볼 수 있습니다. 이러한 각 기술의 기본 원리는 아래에서 설명됩니다.

(출처: [18]) **문맥 내 학습(In-context learning)**은 단일 기반 거대 언어 모델이 주어진 프롬프트(prompt) 내의 정보를 활용하여 다양한 후속 작업(downstream task)을 정확하게 처리하는 능력을 의미합니다(위 참조). 예를 들어, 요약의 경우, 여러 기사와 그에 해당하는 요약을 프롬프트 안에 배치함으로써, 모델이 최종 요약을 생성할 때 이들을 맥락(context) 정보로 활용하도록 할 수 있습니다. 미세 조정(finetuning)과는 달리, 문맥 내 학습 과정에서는 모델의 매개변수(parameter)가 업데이트되지 않습니다. 대신, 우리는 모델이 프롬프트에 제공된 맥락을 적절히 사용하여 문제를 더 잘 해결하는 능력에 의존합니다. 문맥 내 학습은 GPT-3 [18]의 발표와 함께 처음 관찰된 거대 언어 모델의 발현적 능력(emergent capability) 1 중 하나입니다. 위 그림에서 볼 수 있듯이, 거대 언어 모델은 특정 규모에 도달하면 더 유능한 소수 예시 학습자(few-shot learner)가 되는데, 이는 더 큰 모델이 프롬프트의 맥락을 더욱 효과적으로 활용하여 문제를 해결할 수 있음을 시사합니다. 문맥 내 학습을 사용하려면, 우리는 간단한 프롬프트(위에서 보여준 것과 같은)로 독점 API(예: OpenAI 또는 Anthropic)를 호출하여 요약을 생성할 수 있습니다.

(출처: [6]) **지도 미세 조정(Supervised finetuning).** 만약 프롬프트만으로 충분한 품질의 요약을 얻기 어렵다면, 다음 단계는 고품질 요약 데이터셋을 사용하여 사전 학습된 거대 언어 모델을 미세 조정하는 것입니다. 이를 위해 우리는 (인간 주석자(annotator)를 통해 수동으로 또는 강력한 거대 언어 모델로 합성적으로) 해결하려는 작업과 관련된 잘 작성된 문서-요약 쌍 데이터셋을 수집합니다.
“대규모 언어 모델의 사전 훈련은 고성능 달성을 위해 점점 더 널리 보급되고 있습니다… [이 모델들은] 인간이 시연한 데이터셋의 로그 확률(log probability)을 최대화하기 위해 지도 학습(supervised learning)을 사용하여 미세 조정됩니다.” - 출처 [2]
예를 들어, 음식점 리뷰를 요약하고 싶다면, 각 음식점의 리뷰와 그에 대한 선별된 요약을 포함하는 리뷰 데이터셋을 수집해야 합니다. 사전 학습된 기본 모델에서 시작하여, 수집된 데이터에 대해 표준 언어 모델링 목표를 사용하여 이 모델을 미세 조정할 수 있습니다. 이러한 접근 방식을 **지도 미세 조정(SFT)**이라고 합니다. 대규모 사전 학습과 요약 전용 데이터에 대한 지도 미세 조정의 조합은 매우 효과적이며, 기반 거대 언어 모델이 대중화되기 이전에 요약 모델 학습을 위한 가장 일반적인 접근 방식이었습니다.

(출처: [19]) **명령어 튜닝(Instruction tuning).** 요약 데이터에 대한 지도 학습을 넘어서, 우리는 범용 명령어 튜닝을 수행할 수 있습니다. 이 방식에서는 작업 템플릿(위 참조)을 활용하여 여러 작업의 데이터를 동시에 사용하여 단일 모델을 미세 조정합니다. 특히, 요약은 명령어 튜닝 과정에 포함될 수 있는 작업 중 하나이며, 결과 모델은 다양한 작업을 정확하게 처리하는 데 활용될 수 있습니다. 실제로, 명령어 튜닝된 모델은 학습 과정에서 보지 못한 새로운 작업에도 뛰어난 일반화(generalization) 성능을 보이는 것으로 나타납니다(아래 참조). 이러한 방식은 모델이 특정 작업에만 국한되지 않고, 주어진 지시를 이해하고 따르는 능력을 향상시키는 데 기여합니다.

(출처: [19]) **인간 피드백(선호도 튜닝)(preference tuning).** 마지막으로, 우리는 인간의 피드백을 직접 활용하여 요약 모델을 미세 조정할 수 있습니다. 일반적으로 선호도 튜닝(preference tuning)이라고 불리는 이 접근 방식은 인간 피드백 기반 강화 학습(RLHF)을 통한 거대 언어 모델 정렬(alignment)이라는 후속 연구의 기반을 마련했습니다. 실제로 InstructGPT [5](ChatGPT의 전신)는 인간 피드백을 통한 요약 학습 연구 [2]를 그들의 정렬 전략 2에 대한 영감으로 인용합니다! 따라서 이 개요에서는 선호도 튜닝 전략에 집중하여 다룰 것입니다.

(출처: [2]) 인간 피드백을 기반으로 거대 언어 모델을 학습시키기 위해, 우리는 먼저 선호도 데이터(preference data)를 수집합니다. 각 선호도 데이터 예시는 동일한 원본 문서에 대한 두 요약 쌍으로 구성되며, 이 중 하나의 요약이 (일반적으로 사람에 의해) 다른 요약보다 "더 좋다"고 식별됩니다(위 참조). 이 선호도 데이터는 보상 모델(reward model)을 훈련시키는 데 사용될 수 있습니다.
“게시물과 후보 요약이 주어졌을 때, 우리는 이 요약이 우리의 레이블러(labeler)에 의해 판단된 더 나은 요약일 로그 오즈(log odds)를 예측하도록 보상 모델(reward model)을 학습시킵니다.” - 출처 [2]
보상 모델은 요약(또는 일반적으로 텍스트 시퀀스)을 입력으로 받아 스칼라 값(scalar score)을 출력으로 생성하며, 이는 주어진 요약에 대한 인간의 선호도를 나타내는 점수입니다. 보상 모델을 훈련시키기 위해, 우리는 쌍에서 선호되는 요약이 거부된 요약보다 더 높은 점수를 받을 로그 확률(log probability)을 단순히 최대화하는 순위 손실(ranking loss)을 사용합니다(아래 참조). 이전 연구에서는 이러한 쌍별 순위 손실(pairwise ranking loss)이 회귀(regression)를 통해 직접 선호도 점수를 학습하는 것보다 우수한 성능을 보인다는 것을 입증했습니다 [1]. 훈련 후, 보상 모델은 원본 문서와 요약을 입력으로 받아 스칼라 점수를 출력하며, 점수가 높을수록 사람이 해당 요약을 선호할 가능성이 높음을 의미합니다.

(출처: [2]) 그 후 이 보상 모델(reward model)을 강화 학습(RL) 알고리즘(예: PPO)을 통해 요약 모델을 학습시키는 신호로 활용할 수 있습니다. 이러한 방식으로 우리는 선호도 쌍(preference pair) 형태의 인간 피드백을 사용하여 요약 모델을 훈련시킵니다! RLHF에 대한 더 자세한 내용은 아래의 심층 분석을 확인하십시오.
**RLHF의 발전 과정과 핵심 원리**

**요약을 위한 주요 데이터셋**
(출처: [20]) 요약은 보편적인 문제 영역이지만, 학술 문헌에서는 거의 공통적으로 활용되는 몇몇 표준 데이터셋이 존재합니다. 요약 연구는 주로 뉴스 요약에 초점을 맞추는 경향이 있습니다. 예를 들어, CNN / DailyMail 코퍼스(Corpus) [20]는 가장 널리 사용되는 데이터셋 중 하나입니다(이 데이터셋의 샘플은 위 참조). 이 데이터셋은 CNN과 DailyMail의 30만 개 이상의 기사와 각 기사에 대한 관련 요약을 포함하고 있습니다. 또 다른 단일 문서 생성 요약 데이터셋인 XSum [21]도 요약 연구에서 자주 활용됩니다. CNN / DailyMail과 유사한 규모(약 23만 개의 요약)를 가진 XSum은 데이터셋의 각 기사의 주요 아이디어를 포착하는 간결한 단일 문장 요약으로 구성됩니다.
“우리는 더 일반적으로 사용되는 CNN/DM 데이터셋보다 TL;DR 데이터셋을 선택했습니다. 주로 간단한 발췌 기준선(baseline)으로 CNN/DM에서 매우 강력한 성능을 달성할 수 있기 때문입니다.” - 출처 [2]
널리 사용됨에도 불구하고, CNN / DailyMail 및 XSum 데이터셋은 상대적으로 해결하기 쉬운 편입니다. 때로는 더 복잡한 데이터셋이 요구됩니다. 최근 연구는 Reddit의 3백만 개 게시물과 각 게시물 작성자가 직접 작성한 요약을 포함하는 TL;DR 데이터셋 [22]을 탐구했습니다(아래 참조). 규모가 크기 때문에 이 데이터셋은 일반적으로 품질 향상을 위해 필터링되며, 관련 주제의 게시물만 포함되도록 합니다(자세한 내용은 [2]의 섹션 3.2 참조).

(출처: [22]) **데이터 품질 문제.** 대부분의 요약 데이터셋은 각 문서에 대한 원본 문서와 더불어 참조 요약(reference summary)을 제공합니다. 하지만 이들을 "참조"라고 부른다고 해서 데이터셋에 포함된 요약의 품질이 항상 높다는 것을 의미하지는 않습니다! 실제로는 많은 논문 [8, 11]에서 CNN / DailyMail과 같은 데이터셋의 참조 요약이 사람이 직접 작성한 요약보다 훨씬 낮은 품질을 보인다는 사실을 지적합니다. 이러한 발견은 몇 가지 이유로 문제가 됩니다:
*   우리는 종종 이러한 참조 요약을 지도 학습(supervised learning)의 재료로 활용합니다.
*   이러한 참조 요약은 평가 지표를 산출하는 데 사용될 수 있습니다.
CNN / DailyMail의 저품질 참조 요약의 몇 가지 예시는 아래 그림에 나타나 있습니다.

(출처: [11]) **요약의 품질은 어떻게 측정될 수 있을까?**
“생성 요약의 평가 작업은 까다롭습니다. 제약된 번역이나 코드 생성처럼 기능성을 명확히 검증할 수 있는 단순한 문제는 아닙니다.” - Eugene Yan
생성 요약이 본질적으로 개방형이라는 점을 고려할 때, 요약의 품질을 평가하는 것은 쉽지 않습니다. 원본 문서를 요약하는 데에는 여러 가지 유효한 방법이 있을 수 있으며, 특정 요약이 다른 요약보다 "더 좋다"고 판단하는 것은 주관적인 영역입니다! 이러한 이유로, 생성 요약을 적절하게 평가하는 과정은 우리(인간과 연구자)가 "좋은" 요약을 정의하는 일련의 기준을 설정하는 것에서 시작됩니다. 예를 들어, [23]에서는 다음 기준들이 제안됩니다:
*   **유창성(Fluency)**: 요약문의 문장이 자연스럽고 읽기 쉬우며 문법적 오류가 없습니다.
*   **일관성(Coherence)**: 요약 전체가 논리적으로 연결되고, 응집력 있으며, 합리적인 방식으로 구성되어 있습니다.
*   **관련성(Relevance)**: 요약이 원본 문서에서 가장 "중요한" 정보를 정확히 포함합니다.
*   **일치성(Consistency)**: 요약의 정보가 사실에 부합하고 원본 문서와 모순되지 않습니다(즉, 환각(hallucination)이나 잘못된 정보가 없습니다).

하지만 이것이 우리가 정의할 수 있는 유일한 기준 세트는 아닙니다! 논란의 여지는 있지만, 유창성은 현대 거대 언어 모델 3에 의해 거의 해결되었으며, 우리가 중요하게 생각하는 기준은 해결하려는 사용 사례에 따라 달라질 수 있습니다. 예를 들어, [7]의 저자들은 사용자 의견이나 리뷰(예: Yelp)를 요약하는 데 더 적합한 충실도(faithfulness), 사실성(factuality), 일반성(genericity)을 포함하는 대안적인 기준 세트를 고안합니다. 좋은 요약의 바람직한 특성을 명확히 정의하는 것이 평가 과정의 첫 번째 단계입니다. 이것이 명확해지면, 성능을 측정하고 반복 개선을 통해 더 나은 생성 요약 모델을 개발하는 것에 대해 고민하기 시작할 수 있습니다.

**인간 평가(Human evaluation).** 생성 요약의 품질을 측정하기 위한 여러 자동화된 전략이 존재하지만, 인간 평가는 가장 신뢰할 수 있는 평가 접근 방식이며, 요약 연구 전반의 품질 평가를 위한 "참값(ground truth)" 역할을 합니다. 모델의 진정한 품질을 파악하기 위해서는 인간 평가를 거쳐야 합니다. 그럼에도 불구하고, 인간 평가가 만능 해결책은 아닙니다! 인간으로부터 정확하고 신뢰할 수 있으며 일관된 품질 레이블을 얻는 것은 극도로 어려운 일입니다 4. 특히 생성 요약과 같은 주관적인 작업에서는 더욱 그렇습니다. 인간은 끊임없이 서로 의견이 다릅니다(요약 품질 외에도 훨씬 더 많은 것에 대해!). 이는 평가 과정을 상당히 노이즈가 많게 만들 수 있습니다. 이러한 문제를 완화하기 위해, 우리는 플라이스 카파(Fleiss’ kappa) 및 크리펜도르프 알파(Krippendorff's alpha)와 같은 지표를 사용하여 인간 주석자(annotator) 간의(또는 주석자(annotator)와 연구자 간의) 일치도 수준을 모니터링할 수 있습니다.

**기존의 (자동) 평가 지표.** 인간 평가는 요약 품질의 궁극적인 기준이지만, 인간 품질 평가를 수집하는 데는 상당한 비용과 시간이 소요되므로, 순전히 인간 평가에만 의존하기는 어렵습니다. 우리는 인간 평가 시도 사이에 모델을 더 빠르게 반복 개발할 수 있도록 해주는 자동화된 측정 기준이 필요합니다. 먼저, 요약 작업을 위한 보다 전통적인 자동 평가 지표들을 살펴보겠습니다. 이들은 두 가지 범주로 분류됩니다:
*   참조 기반(Reference-based)
*   참조 불필요(Reference-free) (또는 맥락 기반(context-based))

참조 기반 지표는 요약 품질을 측정하는 데 활용할 수 있는 목표 또는 참조 요약(일반적으로 사람이 작성한)이 존재한다고 가정하는 반면, 참조 불필요 지표는 생성된 요약과 원본 문서만을 순수하게 기반으로 하여 요약 품질을 평가합니다. 요약을 위한 가장 널리 사용되는 평가 지표는 ROUGE(Recall-Oriented Understudy for Gisting Evaluation) 점수입니다. 이 점수는 참조 요약에 포함되어 있고 모델이 생성한 출력에도 나타나는 단어 수(또는 ROUGE-N의 n-그램(n-gram) 수)를 단순히 세는 방식으로 작동합니다(아래 참조). ROUGE는 참조 요약과 출력 요약 간의 중복도를 측정하는 참조 기반 지표입니다. ROUGE 외에도 요약 품질을 계산하기 위해 유사한 전략을 사용하는 많은 다른 참조 기반 지표가 있습니다:
*   **BLEU(Bilingual Evaluation Understudy) 점수 [25]**: 생성된 출력과 참조 요약 간의 일치하는 n-그램(n-gram) 수를 세고, 이 수를 생성된 출력 내의 총 n-그램(n-gram) 수로 나누어 번역 작업 평가에 주로 사용됩니다.
*   **BERTScore [26]**: 생성된 출력과 참조 출력의 각 n-그램(n-gram)에 대해 (BERT를 활용하여) 임베딩(embedding)을 생성한 후, 코사인 유사도(cosine similarity)를 사용하여 두 텍스트 시퀀스의 n-그램(n-gram)을 비교함으로써, 정확한 단어 일치 대신 n-그램 간의 의미론적 일치(semantic match)를 가능하게 합니다.
*   **MoverScore [27]**: n-그램(n-gram) 간의 일대일 매칭을 요구하는 BERTScore를 다대일 매칭을 허용하도록 확장하여 평가 프레임워크의 유연성을 높입니다.

특정 상황에서는 참조 기반 지표가 바람직하지 않을 수 있습니다. 예를 들어, 우리의 참조 요약이 저품질이거나, 참조 요약에 전혀 접근할 수 없을 수도 있습니다! 이러한 경우를 처리하기 위해, 우리는 참조 요약 대신 출력 요약을 원본 문서와 비교하여 ROUGE의 맥락 기반(context-based) 변형인 ROUGE-C [24]를 도출할 수 있습니다(아래 참조).

(출처: [24]) 동일한 전략을 사용하여 BERTScore 및 MoverScore의 참조 불필요(reference-free) 변형도 만들 수 있습니다! 존재하는 다양한 참조 불필요 및 참조 기반 요약 지표에 대한 더 자세한 내용은 이 논문을 참조하십시오.
“최근 연구들은 대규모 언어 모델(LLM)을 자연어 생성(NLG) 평가를 위한 참조 불필요(reference-free) 지표로 사용할 것을 제안합니다. 이는 인간 참조 데이터가 부족한 새로운 작업에 적용할 수 있다는 이점이 있습니다.” - 출처 [29]

**LLM-as-a-Judge.** 거대 언어 모델의 출력(생성 요약 작업 포함)을 평가하기 위한 인기 있는 전략 중 하나는 LLM-as-a-Judge [28]입니다. 이는 강력한 거대 언어 모델(예: GPT-4)을 평가자로 활용합니다. 생성된 출력을 평가하거나 점수를 매기기 위해, 우리는 단순히 거대 언어 모델에 지시(prompt)를 제공합니다! 이는 몇 가지 다른 방식으로 수행될 수 있습니다:
*   생성된 출력 쌍 내에서 선호되는 출력을 식별하도록 거대 언어 모델에 요청(아래 참조).
*   프롬프트에 명시된 기준에 따라 단일 생성된 출력에 대해 (지정된 범위 내의) 스칼라 점수(scalar score)를 생성하도록 거대 언어 모델에 요청.
*   정확한 채점을 시연하는 몇 가지 소수 예시(few-shot example)를 기반으로 생성된 출력을 평가하도록 거대 언어 모델에 요청.

(출처: [28]) LLM-as-a-Judge는 거대 언어 모델의 출력을 평가하기 위한 새롭고 강력하며 참조 불필요(reference-free) 전략입니다. 그러나 이 평가 접근 방식은 몇 가지 형태의 편향(bias)을 내포하고 있습니다:
*   **위치 편향(Position bias)**: 모델 프롬프트 내에서 생성된 출력의 순서가 평가 결과에 영향을 미칠 수 있습니다. 이를 해결하기 위해 무작위로 위치를 바꿔 여러 번 평가하고 그 평균을 취할 수 있습니다.
*   **장황함 편향(Verbosity bias)**: GPT-4와 같은 모델은 더 길고 장황한 출력을 선호하는 경향이 있습니다. 생성된 출력의 길이를 정규화하여 이를 보정할 수 있습니다.
*   **자기 강화 편향(Self-enhancement bias)**: GPT-4 및 기타 모델은 자신의 출력에 다른 모델의 출력보다 더 높은 점수를 부여하는 경향이 있으므로, 특정 거대 언어 모델이 자신의 생성물을 평가하는 데 사용될 때는 주의가 필요합니다!
*   **능력의 한계**: 거대 언어 모델은 완벽하지 않습니다! 따라서 LLM-as-a-Judge를 사용하여 판사 모델 자체가 해결하기 어려워하는 문제(예: 복잡한 수학 또는 추론 문제)에 대한 모델의 출력을 채점할 때 한계에 부딪힐 수 있습니다.

이러한 편향에도 불구하고, LLM-as-a-Judge 방식의 평가는 놀랍도록 견고하며 다양한 응용 분야에서 인간 평가와 높은 상관관계를 보여, 최근 연구에서 광범위하게 채택되고 있습니다(요약 및 그 이상). [29]에서 저자들은 사고 연쇄 프롬프트(chain of thought prompting) 및 양식 작성(form-filling)을 통해 거대 언어 모델 기반 평가를 강화하여, 특히 요약 작업의 평가 품질을 향상시키는 G-Eval이라는 새로운 평가 전략을 개발했습니다(아래 참조).

(출처: [29]) **보상 모델(Reward models).** 위에서 언급했듯이, 요약 모델 학습을 위한 가장 효과적인 전략 중 하나인 선호도 튜닝(preference tuning)은 인간의 선호도 데이터셋으로 보상 모델을 훈련시키는 것을 포함합니다. 이 모델의 출력은 강화 학습을 통한 미세 조정(finetuning)을 위한 보상 신호로 활용되지만, 동일한 보상 신호는 품질 평가 목적으로 재활용될 수 있습니다! 보상 모델은 생성된 요약을 입력으로 받아 이 요약에 대한 인간의 선호도 점수(human preference score)를 예측합니다. 따라서 보상 모델의 출력은 인간 선호도의 대리 지표(proxy) 역할을 하며, 이는 참조 불필요(reference-free) 품질 평가로 직접 사용될 수 있습니다. 더 자세한 정보는 아래의 Nathan Lambert가 작성한 보상 모델 평가에 대한 훌륭한 글을 확인하십시오.
**RewardBench: 보상 모델의 벤치마킹과 미래**

**인간 피드백을 통해 요약 품질 향상**
“학습 시 요약기가 참조 요약을 재현하도록 하는 지도 학습 패러다임과 비교할 때, 강화 학습(RL)은 요약기가 생성된 요약의 품질을 측정하는 보상(reward)을 최대화하도록 직접적으로 최적화합니다.” - 출처 [1]
오랫동안 요약 모델 학습을 위한 최첨단 접근 방식은 고품질 참조 요약 데이터셋을 사용하여 사전 학습된 기본 모델을 지도 미세 조정하는 것이었습니다. 이 접근 방식은 효과적이지만, 이 섹션에서 보게 될 것처럼 선호도 튜닝(preference tuning)을 통해 더 우수한 결과를 얻을 수 있습니다. 인간 피드백은 훨씬 더 뛰어난 요약 모델을 훈련시킬 수 있도록 해줍니다. 하지만 이러한 연구는 요약이라는 주제를 넘어섭니다. 유사한 기술들이 최근 거대 언어 모델 정렬(alignment) 연구에 의해 재활용되어 거대 언어 모델 학습 파이프라인의 핵심 기반을 형성했습니다.
**더 나은 보상이 더 나은 요약을 만든다 [1]**

(출처: [1]) 지도 학습은 원래 요약 모델 훈련을 위한 가장 흔히 사용되는 패러다임이었습니다. 우리는 단순히 인간이 작성한 참조 요약을 모방하도록 모델을 가르쳤습니다. 최근에는 연구자들이 요약 모델 훈련에 강화 학습(RL)을 사용하는 것을 탐구하기 시작했습니다. 초기 시도들은 ROUGE 점수를 보상 5으로 직접 사용했지만, ROUGE 점수는 인간의 품질 평가와 낮은 상관관계를 보였습니다(위 참조). 따라서 [1]의 저자들은 RL을 통해 모델이 인간이 더 선호하는 요약으로 유도될 수 있는 더 나은 보상 함수(reward function)를 찾으려고 노력합니다.
“인간에게 매력적인 요약을 생성하기 위한 더 나은 보상 함수(reward function)를 찾기 위해, 우리는 2,500개의 요약에 대한 인간 평가로부터 보상 함수(reward function)를 학습합니다.” - 출처 [1]

**보상 학습.** [1]의 저자들은 인간 선호도 데이터셋으로부터 보상 함수(reward function)를 학습할 것을 제안합니다. 이전 연구에서 가져온 이 데이터셋은 CNN / DailyMail 코퍼스(corpus)의 500개 뉴스 기사에 대한 2,500개의 요약(인간 평가 포함)을 포함합니다. 이 데이터를 사용하여, 우리는 문서와 시스템 요약을 입력으로 받아 인간 평가를 예측하도록 보상 모델을 훈련시킬 수 있습니다. ROUGE를 보상으로 사용하는 기술과 달리, 보상 계산에 참조 요약이 필요하지 않습니다! 보상 모델을 훈련시키기 위해, 우리는 회귀 목표(regression objective) 또는 선호도 학습 목표(preference learning objective)를 사용할 수 있습니다. 후자는 보상 함수가 인간이 선호하는 요약을 정확하게 식별하는지 여부를 포착합니다(아래 참조).

(출처: [1]) [1]에서는 보상 함수(reward function)에 대해 여러 아키텍처(architecture)가 검토되었지만, 가장 좋은 결과를 도출하는 접근 방식은 요약과 입력 문서의 (BERT를 사용하여 생성된) 연결된 임베딩(embedding)을 입력으로 받는 피드포워드 네트워크(feed-forward network)입니다. [1]에서 우리는 선호도 학습 목표가 회귀 목표에 비해 더 나은 결과를 도출한다는 것을 알 수 있으며, 이는 선호도 학습 목표가 이제 보상 모델 학습에 거의 보편적으로 사용되는 이유를 설명합니다. 아래 표에서 우리는 최상의 보상 함수가 다음을 사용한다는 것을 알 수 있습니다:
*   임베딩(embedding) 생성을 위한 BERT.
*   보상 예측을 위한 피드포워드 네트워크(feed-forward network) (또는 MLP).
*   선호도 기반 학습 목표.
이 보상 함수는 인간의 판단과 높은 상관관계를 보이는 예측을 생성하고 높은 재현율(recall)과 정밀도(precision)로 "좋은" 요약을 식별하는 것으로 나타났습니다.

(출처: [1]) **더 나은 요약 모델.** 인간의 선호도를 정확히 예측하는 능력을 넘어, [1]에서 학습된 보상 모델은 더 우수한 발췌 및 생성 요약 모델을 구축하는 데 활용될 수 있습니다. 지도 학습 기준선(baseline)과 ROUGE를 보상으로 사용하여 강화 학습으로 훈련된 모델 모두와 비교했을 때, 인간 피드백으로부터 학습된 보상을 활용하여 훈련된 모델은 훨씬 더 높은 인간 평가 점수를 받는 요약을 생성하는 것으로 나타났습니다(이는 기존의 최첨단 시스템보다도 뛰어난 성능입니다!). (아래 참조).

(출처: [1]) 간단히 말해, [1]에서 우리는 인간 피드백으로부터 보상 함수(reward function)를 학습하는 것이 강화 학습 기반 요약 모델 학습을 위한 우수한 학습 신호를 제공할 수 있다는 것을 알 수 있습니다. 후속 연구는 이 교훈을 받아들여 이를 거대 언어 모델 영역으로 확장하여 인간 피드백으로부터 다양한 작업(요약 포함)을 학습합니다.

**인간 피드백으로부터 요약 학습 [2]**
지도 미세 조정(supervised finetuning)은 다음 토큰 예측 목표(next token prediction objective)를 사용하여 인간이 시연한 데이터셋의 로그 확률(log probability)을 최대화합니다. 우리는 모델이 인간이 작성한 텍스트 6에 높은 확률을 할당하도록 가르칩니다. ROUGE를 통해 이러한 모델을 평가하면 모델의 출력이 참조 요약과 얼마나 밀접하게 일치하는지 정량화할 수 있습니다. 이 접근 방식은 (상대적으로) 잘 작동하지만, [2]의 저자들은 지도 학습과 ROUGE가 우리의 실제 목표인 고품질 요약 작성의 대리 지표(proxy)일 뿐이라고 지적합니다.
“우리는 인간 선호도를 최적화하도록 모델을 학습시킴으로써 요약 품질을 크게 향상시킬 수 있음을 보여줍니다.” - 출처 [1]
인간은 항상 완벽한 요약을 작성하지 않으며, 어떤 원본 문서에 대해서도 수많은 동등하게 유효한 요약을 작성할 수 있습니다. 따라서 인간이 작성한 요약과 정확히 일치하도록 요약 모델을 학습시키는 것은 결함 있는 접근 방식입니다! 모든 참조 요약(심지어 저품질 요약도)은 학습 과정에서 동등하게 강조되며, 우리는 유효한 요약의 다양성을 설명할 방법이 없습니다. 이를 염두에 두고, 우리는 다음과 같이 궁금해할 수 있습니다: 요약 품질에 기반하여 모델을 직접 최적화하는 목표를 찾을 수 있을까?

(출처: [2]) **인간 피드백으로부터 학습.** [2]에서 저자들은 인간의 선호도 데이터에 기반하여 거대 언어 모델을 미세 조정(finetune)할 수 있도록 하는 세 부분으로 구성된 프레임워크를 제안함으로써 정확히 이 작업을 수행합니다(위 참조). 거대 언어 모델은 먼저 인간 참조 요약에 대한 지도 미세 조정(supervised finetuning)을 사용하여 학습되어 지도 학습 기준선(baseline)을 생성하고, 이 기준선은 다시 강화 학습(RL)으로 추가 미세 조정됩니다. [2]의 RLHF 과정은 다음을 통해 인간 피드백 데이터셋을 수집하는 것으로 시작됩니다:
*   학습 데이터셋에서 텍스트 입력(원본 문서) 가져오기.
*   여러 정책(policy)(예: 사전 학습된 모델, 지도 학습 기준선(baseline), 현재 모델 또는 인간 참조 요약)을 사용하여 입력의 요약 샘플링.
*   샘플 응답 세트에서 두 요약 선택.
*   인간 주석자(annotator)에게 두 요약 중 더 나은 것을 식별하도록 요청.
인간 비교 데이터는 대량으로 수집되며 오프라인 방식으로 RLHF를 통해 모델(디코더 전용 거대 언어 모델)을 미세 조정하는 데 사용됩니다. 데이터가 수집되면, 이 비교 데이터를 사용하여 거대 언어 모델이 생성한 요약이 주어졌을 때 인간 선호도 점수(human preference score)를 정확하게 예측하는 보상 모델(reward model)을 학습시킵니다. 여기에서 우리는 강화 학습을 사용하여 모델을 미세 조정합니다( [2]의 저자들은 PPO 알고리즘을 사용합니다). 보상 모델이 출력한 선호도 점수에 기반합니다.
**PPO의 최적화 목표에 KL 발산(KL divergence) 추가**

(출처: [2]) **드리프트(drift) 방지.** [2]의 저자들은 PPO에 의해 최적화되는 목표에 KL 발산(KL divergence) 항을 추가합니다. 이는 RLHF 동안 정책(policy)이 지도 학습 기준선(supervised baseline) 정책과 지나치게 멀어지는 것을 방지합니다(위 참조). 이제 일반적으로 사용되는 이러한 접근 방식(예: LLaMA-2 보고서의 Eq. 4 참조)은 모드 붕괴(mode collapse) 7 없이 탐색을 장려하고, 거대 언어 모델이 작성한 요약이 학습 과정에서 보았던 것과 너무 다르게 변질되는 것을 막아줍니다.

(출처: [2]) **피드백을 통한 학습은 효과적인가?** [2]의 저자들은 위에서 설명된 전략을 사용하여 TL;DR 데이터셋에 대해 13억에서 67억 개의 매개변수(parameter)를 가진 여러 GPT 스타일 언어 모델을 미세 조정(finetune)합니다. 인간 피드백을 통해 학습된 모델은 지도 학습만으로 학습된 모델이 작성한 요약보다 인간에게 일관되게 선호되는 요약을 생성하는 것으로 나타났습니다(위 참조). 13억 개의 인간 피드백 모델은 지도 학습만으로 학습된 10배 더 큰 모델보다 성능이 우수하며, 67억 개의 인간 피드백 모델은 13억 모델보다도 더 나은 성능을 보입니다. 즉, 요약 품질은 모델 규모(model scale)로부터 이점을 얻습니다.

(출처: [2]) 인간 피드백으로 학습된 요약 모델은 새로운 도메인에 더 잘 일반화(generalize)되는 경향을 보입니다. 예를 들어, [2]에서 TL;DR에 대해 미세 조정(finetune)된 모델은 추가적인 미세 조정 없이도 뉴스 중심 데이터셋에서 좋은 성능을 보이는 것으로 확인되었습니다(위 참조).
“우리는 우리의 보상 모델(reward model)이 인간 선호도 예측에서 ROUGE와 같은 다른 지표보다 성능이 우수하며, 우리의 보상 모델을 직접 최적화하는 것이 인간에 따르면 ROUGE를 최적화하는 것보다 더 나은 요약을 생성한다는 것을 확인합니다.” - 출처 [1]

**보상 모델(Reward model) > ROUGE.** ROUGE와 같은 참조 기반(reference-based) 지표가 요약 평가의 표준으로 널리 사용되지만, [2]의 저자들은 ROUGE 점수가 인간 선호도와 낮은 상관관계를 보이는 경향이 있음을 관찰합니다. 이러한 이유로, PPO의 보상 신호로 ROUGE를 사용하여 미세 조정(finetune)된 요약 모델은 인간 선호도를 예측하도록 보상 모델을 학습시키는 모델보다 성능이 떨어집니다. [2]에서 보상 모델은 인간 선호도를 매우 정확하게 예측하는 것으로 나타났으며, 이는 선호도 기반 지표가 요약 품질 평가에 유용한 접근 방식임을 시사합니다.

**인간 선호도로부터 언어 모델 미세 조정 [3]**
[2]의 연구 이전에, 동일한 저자들(OpenAI 소속)은 긍정적인 감성 또는 물리적으로 묘사적인 언어를 사용한 텍스트의 스타일적 연속, 그리고 TL;DR 및 CNN/Daily Mail 데이터셋에 대한 요약이라는 네 가지 다른 작업 [3]에 대해 사전 학습된 거대 언어 모델을 미세 조정(finetune)하기 위한 인간 피드백 사용을 탐구했습니다. [3]에서 사용된 미세 조정 전략은 [2]의 전략과 대부분 일치하지만, 지도 미세 조정(supervised finetuning) 구성 요소는 없습니다. 우리는 오직 인간 피드백에 기반하여 미세 조정합니다(아래 참조). [3]에서는 선호도 튜닝(preference tuning)이 지도 학습의 대안으로 제시되며, 함께 적용될 수 있는 기술이라기보다는 독립적인 방식으로 다루어집니다.

(출처: [3]) 스타일적 연속성을 위해 저자들은 5,000개의 선호도 쌍(preference pair) 데이터셋을 구축했고, 요약을 위해서는 60,000개 이상의 선호도 쌍이 수집되었습니다. 스타일적 연속성을 위한 인간 피드백 모델은 77%의 경우에서 지도 학습 기준선(baseline)보다 선호되는 것으로 나타났습니다. 유사하게, 인간 주석자(annotator)는 지도 학습 기준선보다 인간 피드백으로 학습된 요약 모델을 선호했지만, 인간 피드백 모델은 요약될 문서의 처음 세 문장을 단순히 복사하는 간단한 기준선에 의해 놀랍게도 성능이 떨어지는 결과를 보였습니다(아래 참조).

(출처: [3]) 흥미롭게도, [3]의 분석은 인간 피드백 기반 요약 모델이 생성 요약을 만들기 위한 매우 간단한 정책(policy)을 학습한다는 것을 밝혀냈습니다. [3]의 저자들은 이를 "스마트 복사(smart copying)"라고 부릅니다. 모델은 원본 텍스트에서 큰 텍스트 구간이나 문장을 복사하는 경향이 있으며, 관련이 없거나 요약에 포함할 가치가 없는 문장은 건너뜁니다. 이 메커니즘은 학습 과정에서 자연스럽게 나타납니다. 즉, 복사를 장려하는 명시적인 아키텍처(architectural) 구성 요소가 요약 모델에 추가되지 않습니다. 단순함에도 불구하고, 이 학습된 복사 메커니즘은 인간 주석자로부터 호의적인 평가를 받습니다.
“우리는 인간의 판단으로만 정의되는 복잡한 작업에 강화 학습(reinforcement learning)을 적용하고 싶습니다. 이러한 작업에서는 인간에게 물어봄으로써만 결과가 좋은지 나쁜지 알 수 있습니다.” - 출처 [3]

**유용한 통찰.** [2]의 연구와 비교했을 때, [3]에서 인간 피드백을 사용하여 학습된 요약 모델은 훨씬 덜 강력하며 간단한 기준선(baseline)에 의해 성능이 떨어지는 경향을 보입니다. 이러한 부정적인 결과는 사용된 모델의 차이와 학습 과정에서 지도 미세 조정(supervised finetuning)이 배제되었기 때문일 가능성이 높습니다. 그럼에도 불구하고, [3]의 연구는 후속 발전을 위한 견고한 토대를 마련하며, 이 분석에서 얻을 수 있는 몇 가지 흥미로운 시사점이 있습니다:
*   인간 피드백으로부터의 학습은 i) 충분한 지도 학습 데이터와 ii) 보상 신호로 사용될 좋은 자동 대리 지표(proxy)가 부족한 작업에 가장 효과적입니다(예: [2]에서 ROUGE가 유용한 보상 함수가 아님을 알 수 있습니다).
*   PPO를 사용한 미세 조정(finetuning)의 목표 함수에 KL 발산(KL divergence) 항을 추가하면 과도한 드리프트(drift) 8를 방지하는 데 도움이 됩니다.
*   인간은 거대 언어 모델 출력의 품질을 판단하기 위해 간단하고 (불완전한) 휴리스틱(heuristic)에 의존하는 경향이 있습니다.
*   온라인 데이터 수집(또는 거대 언어 모델이 반복적으로 미세 조정되고 개선됨에 따라 보상 모델을 재학습시키기 위해 추가 데이터를 계속 수집하는 것)은 지속적인 성능 향상을 이끌어냅니다.

온라인 데이터 수집의 이점에 대한 위의 발견은 거대 언어 모델 연구에 지속적인 영향을 미쳤습니다! LLaMA-2와 같은 최신 모델은 정렬(alignment) 과정에서 새로 수집된 데이터로 여러 "라운드"의 RLHF를 거칩니다. 또한, 우리는 최근 연구에서 인간 평가의 한계가 거대 언어 모델 출력 품질을 측정할 때 매우 중요한 고려 사항이라는 것을 계속해서 보고 있습니다!

**인간 피드백으로 책을 재귀적으로 요약 [4]**
“우리는 작업의 더 작은 부분에 대해 학습된 모델을 활용하여, 인간이 더 광범위한 작업에 대한 피드백을 제공하는 데 도움을 줍니다.” - 출처 [4]
요약을 위한 참조 요약이나 선호도 레이블(preference label)을 선별하는 것은 (잠재적으로) 다소 시간이 소모될 수 있지만, 아주 어려운 일은 아닙니다. 인간 주석자(annotator)는 단순히 다음을 수행하면 됩니다:
*   요약될 기사를 읽습니다.
*   요약을 작성하거나 제시된 요약의 품질을 평가합니다.
그러나 요약될 텍스트가 매우 길 때(예: 전체 소설) 이 과정은 훨씬 더 어려워집니다. 이 경우, 소설 요약을 평가하는 것은 시간이 많이 걸리는데, 인간 주석자는 이미 책을 읽었거나 요약을 정확하게 평가하기 위해 책을 읽는 데 시간을 할애해야 하기 때문입니다. [4]에서 저자들은 인간이 그러한 복잡한 작업에 대한 학습 신호를 효율적으로 제공할 수 있도록, 인간 피드백과 재귀적 작업 분해(recursive task decomposition)를 결합한 확장 가능한 접근 방식을 제안합니다.

(출처: [4]) **재귀적 작업 분해(Recursive task decomposition).** [4]에서 제안된 핵심 아이디어는 긴 텍스트를 재귀적으로 요약될 수 있는 더 작은 덩어리로 분해하여, 최종 노드(leaf node)가 적절한 크기의 텍스트 덩어리에 대한 표준 요약 작업인 요약 작업 트리(위 참조)를 형성하는 것입니다. 먼저, 모델은 책의 작은 덩어리를 요약하는 데 사용됩니다. 그런 다음, 동일한 모델이 이러한 리프 요약(leaf summary)을 입력으로 받아 책의 더 큰 부분을 요약합니다. 즉, 더 짧은 구절의 요약이 책의 더 큰 부분을 요약하기 위한 입력으로 활용됩니다. 이 접근 방식을 취함으로써, 인간은 원본 텍스트 전체에 대한 심층적인 지식이 없더라도 이 작업 9을 효율적으로 감독할 수 있습니다. 모델은 책 자체 또는 이전에 생성된 요약에서 가져온 작은 텍스트 덩어리만 요약하기 때문입니다(아래 참조).

(출처: [4]) 추론 시, 모델은 먼저 책의 작은 부분을 요약하는 데 활용됩니다. 그런 다음, 이러한 요약은 전체 책의 요약이 생성될 때까지 재귀적으로 사용하여 더 높은 수준의 요약을 만듭니다. 이러한 재귀적 전략 덕분에, [4]에서 제시된 접근 방식은 임의 길이의 텍스트를 요약할 수 있습니다!

**데이터셋 구축.** 긴 텍스트의 요약을 충분한 깊이로 재귀적으로 분해하면, 결국 인간에 의해 쉽게 감독되고 거대 언어 모델을 훈련시키는 데 사용될 수 있는 합리적인 요약 하위 작업 세트를 얻게 될 것입니다. 재귀적 요약은 세 가지 주요 작업을 포함합니다:
*   **분해(Decompose)**: 텍스트가 직접 요약하기에는 너무 길다고 판단하고, 텍스트의 더 짧은 부분에 대해 여러 요약 하위 작업을 생성합니다.
*   **응답(Respond)**: 요약을 생성하여 하위 작업을 해결합니다.
*   **구성(Compose)**: "응답"과 동일하지만, 모델은 요약을 생성할 때 여러 하위 작업에 대한 해결책(즉, 이전에 생성된 요약)을 제시받습니다.
책의 경우, 분해 작업은 거대 언어 모델에게 하위 작업을 생성하도록 요청하는 대신 알고리즘적으로 수행될 수 있습니다. 즉, 긴 텍스트 시퀀스를 더 짧은 시퀀스로 덩어리화합니다. 이 전략을 사용하면 학습 데이터 획득이 간단합니다! 우리는 단순히 인간에게 특정 하위 작업을 수동으로 요약하거나 특정 하위 작업에 대해 생성된 두 요약의 품질을 비교하도록 요청합니다. 노드(node)가 리프(leaf)가 아니어서 인간이 더 긴 텍스트를 요약하는 경우, 거대 언어 모델은 모든 하위 작업의 요약을 재귀적으로 생성하여 맥락(context)으로 사용합니다.
“우리는 인간 레이블러(labeler)로부터 방대한 시연(demonstration) 및 비교 데이터를 수집하고, 행동 복제(behavioral cloning) 및 보상 모델링(reward modeling)을 사용하여 GPT-3를 미세 조정(fine-tune)하여 요약을 재귀적으로 수행합니다.” - 출처 [4]
지도 학습 예시와 선호도 레이블(preference label)로 구성된 이 데이터셋이 주어졌을 때, [4]에서 사용된 학습 전략은 [2]의 전략과 거의 동일합니다. 사전 학습된 GPT-3 모델로 시작하여, 우리는 먼저 인간이 작성한 참조 요약에 대해 지도 학습 방식으로 모델을 미세 조정합니다(이는 [4]에서 행동 복제(behavioral cloning)라고 불립니다). 그런 다음, 수집된 선호도 레이블에 대해 여러 번 반복하여 선호도 튜닝(preference tuning)을 수행합니다. [4]에서 사용된 학습 알고리즘에 대한 전체 설명은 아래를 참조하십시오.

(출처: [4]) **실험 결과.** [4]의 학습 데이터는 GPT-3의 사전 학습 데이터셋에서 책 데이터의 하위 집합(즉, Books1 및 Books2 데이터셋)을 사용하여 수집됩니다. 이 데이터는 GPT-3(및 1750억 개 대신 60억 개의 매개변수(parameter)를 가진 더 작은 모델 변형)를 미세 조정하여 책을 재귀적으로 요약하는 데 사용됩니다. 그런 다음, 모델은 2020년에 출판된 40권의 인기 도서 세트에 대해 평가됩니다. 이 책들은 GPT-3의 사전 학습 데이터셋에 포함되지 않았으며 여러 장르에 걸쳐 있습니다(아래 참조).

(출처: [4]) 두 명의 레이블러(labeler)가 각 책을 읽고, 요약을 작성하고, 다양한 모델의 요약을 평가하도록 요청받았습니다. 아래에서 볼 수 있듯이, 인간 선호도로 학습된 모델은 지도 학습만으로 학습된 모델보다 훨씬 우수한 성능을 보입니다. 그러나 모든 모델은 여전히 인간의 성능에 크게 미치지 못하며, 이는 생성적 책 요약이 해결하기 엄청나게 어려운 작업임을 보여줍니다. 실제로, 모델이 생성한 요약 중 5-15%만이 인간의 품질과 일치하는 것으로 나타났습니다.

(출처: [4]) [4]에서 우리는 첫 번째 하위 작업(즉, 책의 짧은 덩어리 요약)에 대한 학습이 가장 중요하다는 것을 알 수 있습니다. 이 작업에만 학습된 모델은 더 높은 수준의 요약 작업에 비교적 잘 일반화(generalize)될 수 있습니다. 그러나 전체 책 요약 생성 시 성능은 여전히 실망스럽습니다. 개별 하위 작업에 대한 인간 선호도 점수가 책의 전체 분해를 통해 생성된 요약에 할당된 점수보다 훨씬 높습니다(아래 참조).

(출처: [4]) **인간 피드백으로 명령어에 따르는 언어 모델 학습 [5]**
“언어 모델을 더 크게 만드는 것만으로는 본질적으로 사용자 의도를 더 잘 따르게 되는 것은 아닙니다… 이러한 모델은 사용자와 정렬(align)되지 않습니다… 우리는 인간 피드백으로 미세 조정(fine-tuning)하여 광범위한 작업에서 언어 모델을 사용자 의도에 정렬(align)하는 방법을 보여줍니다.” - 출처 [5]
[5]에서 우리는 OpenAI의 저자들로부터 [2]에서 제안된 유사한 학습 전략이 일반적인 기반 언어 모델이 프롬프트의 명령어를 더 잘 따르도록 만드는 데 사용될 수 있다는 것을 알 수 있습니다. 이 작업의 결과는 InstructGPT라는 언어 모델입니다. 이는 ChatGPT의 전신으로 현대 거대 언어 모델에 대한 거의 모든 연구의 토대를 마련했습니다. InstructGPT의 핵심 아이디어는 SFT 및 RLHF(아래 참조)를 사용하여 사전 학습된 거대 언어 모델을 미세 조정(finetune)하여 정렬(alignment)을 장려하는 것입니다. 정렬은 인간 사용자의 욕구에 부합하는 텍스트를 생성하는 능력으로 정의됩니다. 일반적으로 우리는 명령어 따르기, 유해한 출력 방지, 거짓말 방지, 흥미롭거나 창의적인 출력 생성 등과 같은 고정된 정렬 기준 세트를 정의하여 이러한 욕구를 구체적으로 포착합니다.

(출처: [5]) 사전 학습 후, 거대 언어 모델은 반복적이거나 (도움이 되지 않는) 텍스트를 생성하고, 자주 환각(hallucinate)하며, 프롬프트 내의 복잡한 명령어를 따르는 데 어려움을 겪을 가능성이 높습니다. 그러나 정렬(alignment) 과정을 통해 우리는 거대 언어 모델에 이러한 단점을 피하고 원하는 정렬 기준을 충족시키는 데 필요한 기술을 가르쳐 훨씬 더 유용하고 흥미로운 모델을 생성할 수 있습니다.
**InstructGPT의 생성 과정.** InstructGPT의 정렬(alignment) 과정은 사전 학습된 언어 모델(13억, 60억, 1750억 모델로 실험이 진행됨)과 거대 언어 모델이 답변할 수 있어야 하는 프롬프트 10 세트로 시작됩니다. 먼저, 인간 주석자(annotator)가 이러한 각 프롬프트에 대한 답변을 수동으로 제공하여, 사전 학습된 모델이 지도 학습을 통해 미세 조정(finetune)되는 데이터셋을 생성합니다. 이 초기 학습 과정은 모델을 명령어와 유사한 프롬프트에 노출시켜 인간 피드백을 통한 추가 미세 조정(finetuning)을 위한 더 나은 시작점을 만듭니다.
“SFT 데이터셋은 약 13,000개의 학습 프롬프트(API 및 레이블러(labeler) 작성)를 포함하고, RM 데이터셋은 33,000개의 학습 프롬프트(API 및 레이블러(labeler) 작성)를 가지며, PPO 데이터셋은 31,000개의 학습 프롬프트(API에서만)를 가집니다.” - 출처 [5]
여기에서 인간 선호도 레이블(preference label) 데이터셋이 수집되며, 각 선호도 예시는 동일한 프롬프트에 대한 여러 응답을 품질에 따라 순위 매깁니다. 이 선호도 데이터셋은 프롬프트와 생성된 응답을 입력으로 받아 스칼라 보상(scalar reward)을 예측하는 보상 모델(RM)(거대 언어 모델과 동일한 아키텍처(architecture)를 공유)을 학습시키는 데 사용됩니다(아래 참조).
**보상 모델(Reward model) 아키텍처(architecture)의 특성**
우리는 이 RM을 사용하여 관련 프롬프트 세트에 대해 PPO를 통해 거대 언어 모델을 미세 조정(finetune)할 수 있습니다. [2]의 제안과 거의 정확히 일치하는 이 학습 파이프라인은 거의 모든 거대 언어 모델에서 사용되는 표준 3단계 정렬(alignment) 절차를 형성합니다. 여기서는 이 접근 방식이 요약이 아닌 정렬(alignment)에 적용됩니다!

(출처: [5]) **실험 결과.** [5]에서 우리는 인간 피드백으로부터의 학습이 정렬(align)된 (그리고 유용한) 거대 언어 모델을 생성하기 위한 놀랍도록 효과적인 전략이라는 것을 알 수 있습니다. 특히, [5]의 저자들은 다음을 관찰합니다:
*   인간 주석자(annotator)는 GPT-3보다 InstructGPT 출력을 선호합니다(위 참조).
*   InstructGPT는 GPT-3보다 더 진실성이 있는 경향이 있습니다.
*   GPT-3는 InstructGPT보다 더 유해한 출력을 생성합니다.
*   InstructGPT는 명령어를 더 잘 따르며, 미세 조정(finetuning)에 사용된 프롬프트 분포를 넘어선 명령어조차도 잘 따릅니다.
GPT-3와 비교했을 때, InstructGPT는 매우 유리한 정렬(alignment) 속성을 가집니다. 아래 표에서 볼 수 있듯이, InstructGPT는 GPT-3보다 훨씬 더 조종 가능(steerable)한 경향이 있습니다. 이는 사용자가 프롬프트 내에 제약 조건, 명령어 또는 세부 정보를 제공함으로써 모델의 행동을 더 잘 제어할 수 있음을 의미합니다.

(출처: [5]) 이러한 모든 이점에도 불구하고, 우리는 정렬(alignment) 절차가 사전 학습된 모델과 비교하여 공개 벤치마크(benchmark)에서 성능 저하의 형태로 "정렬 세금(alignment tax)"을 수반한다는 것을 알 수 있습니다. 이 세금을 피하기 위해, 우리는 선호도 튜닝(preference tuning) 중에 사전 학습 데이터셋에 대한 간헐적 업데이트를 수행할 수 있습니다.

**거대 언어 모델 시대의 요약**
InstructGPT가 요약에만 초점을 맞추지 않았다는 점을 고려할 때, 이 논문이 왜 이 개요에 포함되었는지 의아할 수 있습니다. 그러나 InstructGPT는 단일 유형의 작업을 해결하는 데 특화된 '좁은 전문가' 모델에서 벗어나, 다양한 작업을 정확하게 처리할 수 있는 '기반 모델(foundation model)'로의 자연어 처리 연구 패러다임 전환을 상징합니다. 생성 요약에 대한 연구는 기반 모델에 대한 후속 연구의 출발점을 제공했습니다. 하지만 시간이 지나면서 연구자들은 특히 생성 요약 문제 자체보다는, 더 나은 기반 모델을 구축하는 데 집중하기 시작했습니다. 이러한 모델들에게 생성 요약은 정확하게 해결할 수 있는 수많은 작업 중 하나일 뿐입니다.

**GPT-3 시대의 뉴스 요약 및 평가 [6]**
“우리는 최근 뉴스 기사의 새로운 코퍼스(corpus)에 대한 A/B 테스트를 사용하여 이러한 접근 방식을 비교하고, 연구 참가자들이 압도적으로 GPT-3 요약을 선호한다는 것을 발견했습니다.” - 출처 [6]
위에서 언급했듯이, 강력한 거대 언어 모델의 등장은 기반 모델(foundation model)에 프롬프트(prompt)를 제공하는 것만으로 다양한 작업을 정확하게 해결할 수 있는 인공지능 연구의 패러다임 전환을 가져왔습니다. [6]에서 저자들은 이 패러다임 전환이 요약 연구에 미치는 영향을 뉴스 요약에 초점을 맞춰 탐구합니다.
**GPT-3의 요약 능력은 어느 정도인가?** 요약 전용 데이터로 명시적으로 훈련되지 않았음에도 불구하고, [1]에서 우리는 GPT-3가 인간이 선호하는 요약을 작성하는 데 매우 능숙하다는 것을 알 수 있습니다. 심지어 모델에 작업 설명만으로 프롬프트(prompt)를 제공할 때도 그렇습니다(즉, 제로샷(zero-shot) 방식)! 더 나아가, GPT-3는 명시적인 미세 조정(finetuning) 없이 다양한 작업을 해결할 수 있는 방대한 텍스트 코퍼스(corpus)로 학습된 일반 모델이기 때문에 데이터셋 특정 문제(예: 잘못 작성되었거나 부정확한 참조 요약으로부터 학습)로 고통받지 않습니다. 이러한 이유로, GPT-3는 상당한 데이터셋 없이도 새로운 요약 도메인에 자연스럽게 일반화(generalize)됩니다. 우리는 단순히 명령어 또는 몇 가지 소수 예시(few-shot example)만 필요합니다.

(출처: [6]) SFT 및 명령어 튜닝(instruction-tuned) 모델과 비교했을 때, GPT-3는 모든 데이터셋에서 인간으로부터 20% 더 높은 점수를 달성하는 요약을 생성하며, 이는 GPT-3에 대한 명확한 선호를 나타냅니다(위 참조). 그러나 최상의 요약 모델 선택이 30% 미만의 경우에서만 만장일치로 이루어지며, 이는 고품질 요약이 비교하기 어려울 수 있다는 것을 보여줍니다. 즉, "최상의" 모델 선택은 간단하지 않습니다.
**자동 지표 분석.** [6]의 저자들은 또한 요약 품질 평가에서 자동 지표의 효과를 광범위하게 분석합니다. 이전 연구는 ROUGE와 같은 자동 지표가 요약 모델 간의 큰 품질 차이를 식별하는 데 유용하지만, 작은 성능 차이를 포착하는 데 어려움을 겪는다는 것을 나타내는 것으로 보입니다(즉, 자동 지표는 미묘한 성능 차이를 가진 모델을 비교하는 데 덜 유용합니다). [6]에서 우리는 이 경험 법칙이 거대 언어 모델 시대에는 덜 간단하다는 것을 알 수 있습니다. GPT-3 요약은 기준선(baseline)보다 훨씬 낮은 점수를 받습니다(ROUGE에서 7점 차이!). 인간 실험에서는 거의 만장일치로 선호되었음에도 불구하고 말입니다(아래 참조).

(출처: [6]) 이러한 결과는 강력한 요약 모델을 고려할 때 참조 기반(reference-based) 지표가 인간 선호도와 낮은 상관관계를 보인다는 추가적인 증거를 제공합니다. 더 나아가, [6]에서 우리는 GPT-3 요약의 품질과 자동 지표의 한계라는 측면에서 유사한 결과가 특정 키워드 또는 주제에 기반한 안내 요약을 수행하는 키워드 및 측면 기반 요약(keyword and aspect-based summarization)과 같은 더 전문화된 요약 작업에서도 유효하다는 것을 알 수 있습니다.

**GPT-3.5를 활용한 프롬프트 기반 의견 요약 [7]**
대부분의 논문이 일반적인 텍스트 요약(예: 뉴스 기사 요약)을 연구하지만, [7]의 저자들은 이 작업이 GPT-3.5로 해결될 수 있는지 확인하기 위해 더 복잡한 의견 요약(opinion summarization) 11 작업을 고려합니다. 왜 의견 요약이 일반 텍스트 요약보다 더 복잡할까요? 몇 가지 이유가 있습니다:
*   다양한 의견들이 서로 모순될 수 있으므로, 여러 관점 12을 정확하게 반영하는 더 미묘한 요약이 필요합니다.
*   요약될 모든 의견의 길이가 거대 언어 모델의 맥락 길이(context length)를 초과하는 경우가 많으므로, 데이터를 처리하고 요약하는 파이프라인이 필수적입니다.
*   의견 요약 작업은 일반 텍스트 요약보다 연구가 덜 활발하게 이루어졌습니다.
[4]와 유사하게, 재귀적 전략을 사용하여 긴 의견 시퀀스를 요약할 수 있지만, [7]에서는 대안적인 기술도 탐구됩니다. 예를 들어, 의견을 주제별 그룹으로 클러스터링(clustering)하거나, 요약에 포함될 가장 중요하거나 (핵심적인) 의견을 자동으로 식별하는 방식입니다.

(출처: [7]) **요약 파이프라인.** [7]의 저자들은 다양한 의견 요약 파이프라인을 탐구하지만, 세 가지 주요 접근 방식이 강조됩니다:
*   텍스트를 재귀적으로 덩어리화하고 요약하여 최종 출력을 생성하는 계층적 접근 방식(즉, 반복 요약과 함께 덩어리화).
*   QFSumm이라는 발췌 요약 모델을 사용하여 요약 전에 가장 중요한 리뷰를 식별하는 사전 추출 전략.
*   주제 또는 평점에 따라 리뷰를 클러스터(cluster)로 분리하고, 각 클러스터를 재귀적으로 요약한 다음, 최종 요약을 생성하는 클러스터링(clustering) 접근 방식(위 참조).
[7]에서 우리는 GPT-3.5가 짧은 리뷰 시퀀스에 대해 기본적인 재귀적 요약을 통해 유용한 요약을 생성한다는 것을 알 수 있습니다. 그러나 더 긴 시퀀스를 처리할 때, 반복적인 덩어리화 및 요약은 요약 품질을 저하시킬 수 있다는 것을 알 수 있습니다. 위에서 설명한 사전 추출 및 클러스터링 전략을 통해 이 문제를 완화할 수 있습니다. 이러한 기술은 최종 출력을 생성하는 데 필요한 재귀적 요약 단계 수를 줄이는 경향이 있기 때문입니다.

(출처: [7]) 이러한 전략으로 생성된 의견 요약의 예시(SPACE 호텔 의견 요약 데이터셋에 대한)는 위 그림에 나와 있습니다. 보시다시피, 주제별 클러스터링(clustering)은 다양한 의견에 대한 높은 수준의 시각을 제공하는 더 생성적(abstractive)인 요약을 생성하는 것으로 보이는 반면, 발췌적 전략은 각 입력 리뷰 내에서 언급된 특정 요점에 더 중점을 둡니다.

**인간 평가(Human evaluation).** 위에 설명된 요약 파이프라인 외에도, [7]의 저자들은 각 파이프라인 구성 요소를 모듈화하고 플러그 앤 플레이(plug-and-play) 접근 방식을 사용하여 다양한 파이프라인을 탐구합니다. 이러한 파이프라인은 SPACE 및 FewSum (Amazon 및 Yelp 리뷰 포함) 의견 요약 데이터셋에서 요약을 생성하는 데 사용되며, 생성된 요약의 사실성(factuality), 충실도(faithfulness), 관련성(relevance), 대표성(representativeness)에 기반하여 평가됩니다.

(출처: [7]) 위에서 볼 수 있듯이, 최상의 요약 파이프라인은 평가되는 데이터셋과 속성에 따라 다릅니다. 그러나 GPT-3.5로 생성된 요약은 자동 지표에서는 낮은 성능을 보였음에도 불구하고 인간 평가 측면에서 기준선(baseline) 기술보다 훨씬 우수한 성능을 보입니다. 따라서 [7]에서 제안된 요약 파이프라인은 이전 최첨단 기술에 대한 발전으로 입증됩니다.

**뉴스 요약을 위한 대규모 언어 모델 벤치마킹 [8]**
“우리는 모델 크기보다는 명령어 튜닝(instruction tuning)이 거대 언어 모델의 제로샷(zero-shot) 요약 능력에 핵심적인 영향을 미친다는 것을 발견했습니다.” - 출처 [8]
이 시점에서 우리는 거대 언어 모델이 텍스트를 매우 잘 요약하며, 대부분의 경우 기존의 최첨단 기술보다 우수한 성능을 보인다는 것을 알고 있습니다. 하지만 그 이유는 무엇일까요? 요약 작업에서 거대 언어 모델의 성공을 뒷받침하는 설계 결정은 아직 완전히 이해되지 않고 있습니다. [8]에서 저자들은 요약 작업에서 여러 사전 학습 방법, 프롬프트(prompt) 방식 및 모델 규모(model scale)를 포함하는 10가지 다른 거대 언어 모델에 대한 광범위한 인간 평가를 수행합니다. 이러한 실험을 통해, 우리는 명령어 튜닝(instruction tuning)이 거대 언어 모델을 효과적인 요약기로 만드는 핵심 구성 요소라는 것을 분명히 알 수 있습니다.

(출처: [8]) **저품질 참조 요약.** [8]에서 수행된 대부분의 평가는 CNN / DailyMail 및 XSum 데이터셋에서 이루어졌습니다. 흥미롭게도, 저자들은 이 데이터셋의 대부분의 참조 요약이 인간에 의해 저품질로 판단된다는 것을 보여주며, 이는 기존 요약 연구가 일반적인 데이터셋에 존재하는 저품질 참조 요약으로 인해 한계가 있음을 시사합니다. 실제로, [8]의 저자들은 참조 기반(reference-based) 평가 지표(예: ROUGE)와 인간 선호도 간의 낮은 상관관계가 저품질 참조 요약으로 인해 더욱 악화되며, 이는 다양한 이전 요약 연구 결과에 의문을 제기합니다.
“참조 요약의 품질 문제를 해결하고 거대 언어 모델이 인간 요약 작성자와 어떻게 비교되는지 더 잘 이해하기 위해, 우리는 프리랜서 작가를 모집하여 CNN/DM 및 XSUM의 테스트 세트에서 100개 기사를 재주석했습니다.” - 출처 [8]
저품질 참조 요약 문제를 해결하기 위해, CNN / DailyMail 및 XSum 데이터셋에서 새로 구성된 100개 예시 세트가 인간에 의해 재주석되었습니다. 이러한 고품질 참조 요약은 요약 작업 전반에 걸쳐 거대 언어 모델 성능을 인위적으로 저하시키지 않는 더 신뢰할 수 있는 평가에 사용될 수 있습니다.

(출처: [8]) **실험 환경 설정.** 요약 품질 측정을 위해 10가지 다른 거대 언어 모델이 고려됩니다(위 참조). 모델은 아래에 표시된 기본 템플릿을 사용하여 제로샷(zero-shot) 또는 퓨샷(five-shot) 프롬프트(prompt)로 평가됩니다.
“기사: [기사]. 기사를 세 문장으로 요약하세요. 요약:”
다시 한번, 저자들은 기존 요약 데이터셋에 존재하는 참조 요약의 낮은 품질을 강조합니다. 실제로, 참조 요약을 문맥 내 학습(in-context learning) 예시로 사용하는 것이 거대 언어 모델 성능을 저하시키는 것으로 나타났습니다! 거대 언어 모델이 생성한 출력 요약의 몇 가지 질적 예시는 아래 그림에 나와 있습니다.

(출처: [8]) [8]의 실험 분석에서 얻을 수 있는 주요 시사점은 두 가지입니다:
*   명령어 튜닝(instruction tuning)은 요약 성능에 분명히 긍정적인 영향을 미칩니다.
*   거대 언어 모델이 생성한 요약은 본질적으로 발췌적 특성을 띠는 반면, 인간이 작성한 요약은 더 많은 생성 또는 의역(paraphrasing)을 포함하는 경향이 있습니다(아래 참조).

(출처: [8]) 명령어 튜닝(instruction tuning)을 거친 거대 언어 모델은 그렇지 않은 모델보다 분명히 우수한 성능을 보이며, 이는 자기 지도 사전 학습(self-supervised pretraining)만으로는 경쟁력 있는 요약 결과를 얻기에 충분하지 않다는 것을 나타냅니다. 또한, 거대 언어 모델이 생성한 요약은 원본 기사에서 정보를 직접 복사하는 경향이 있지만, 복사된 정보는 일관성 있는 방식으로 통합됩니다. 대조적으로, 인간이 작성한 요약은 원본 자료에서 복사하기보다는 정보를 의역(paraphrase)하는 경향이 있습니다. 발췌적 특성에도 불구하고, 거대 언어 모델이 생성한 요약은 인간 평가 실험에서 비교했을 때 인간이 작성한 요약과 동등하게 선호됩니다(아래 참조).

**ChatGPT 대 인간 작성 텍스트 [9]**
“인간이 생성한 스타일의 변동성은 ChatGPT가 보여준 것보다 훨씬 크며, 생성된 텍스트는 단어 유형의 분포와 같은 여러 특성에서 인간의 샘플과 차이를 보입니다.” - 출처 [9]
흥미로운 요약 형태 중 하나는 제어 가능한 요약(controllable summarization)입니다. 이 방식에서는 모델에 특정 독자를 대상으로 하는 요약을 작성하도록 지시합니다. [9]에서 저자들은 전문가와 비전문가 모두를 위한 과학 정보 요약에서 GPT-3.5-Turbo의 성능을 연구하며, 거대 언어 모델로 제어 가능한 요약을 수행할 때 직면하는 행동 차이(인간이 작성한 요약과 비교), 한계 및 과제를 식별하는 것을 목표로 합니다.

**제어 가능한 요약을 위한 프롬프트 전략**
**독자 지정.** 거대 언어 모델로 제어 가능한 요약(controllable summarization)을 수행하려면, 단순히 프롬프트(prompt)를 수정해야 합니다. [9]의 경우, 우리는 명령어 내에서 요약을 작성하려는 독자를 명시할 수 있습니다(위 참조). 이러한 프롬프트는 eLife 데이터셋의 과학 문헌을 요약하는 데 사용되며, 평가를 위해 데이터셋에서 500개의 무작위 샘플이 추출됩니다. 거대 언어 모델은 모든 요약을 생성하기 위해 제로샷(zero-shot) 방식으로 프롬프트(prompt)를 받습니다.
**평가 지표.** GPT-3.5-Turbo가 생성된 요약을 지정된 대상 독자에게 충분히 맞춤화할 수 있는지 확인하기 위해, 여러 자동 지표가 평가에 사용됩니다:
*   **플레시 읽기 용이성(Flesch Reading Ease)**: 단어당 평균 음절 수와 문장당 평균 단어 수를 통해 가독성을 측정합니다. 점수가 높을수록 이해하기 쉽습니다.
*   **콜먼-리아우 지수(Coleman-Liau Index)**: 문장당 평균 문자 수와 100단어당 평균 문장 수를 측정하여 텍스트 난이도를 포착합니다. 점수가 높을수록 이해하기 더 어렵습니다.
*   **데일-챌 가독성 점수(Dale-Chall Readability Score)**: 텍스트의 복잡한 단어 수를 일반적인 단어 목록과 비교합니다. 점수가 높을수록 이해하기 더 어렵습니다.
이러한 가독성 지표 외에도 ROUGE 점수와 n-그램(n-gram) 참신성 13이 측정되며, 모델이 생성한 요약 내의 사실적 불일치를 감지하는 SummaC 및 개체명 환각(named entity hallucination)과 같은 지표도 측정됩니다.

(출처: [9]) **주요 시사점.** [9]의 분석은 우리에게 두 가지 핵심 정보를 제공합니다:
*   GPT-3.5-Turbo는 특정 독자에게 출력을 맞추는 데 인간보다 능력이 떨어집니다.
*   모델이 생성한 요약은 더 발췌적이며 일반적으로 환각(hallucination)을 포함하는 경향이 있습니다.
위 표에서 볼 수 있듯이, GPT-3.5-Turbo는 비전문가 요약의 경우 [31, 38] 범위, 전문가 요약의 경우 [28, 37] 범위 내의 플레시 읽기 용이성(Flesch Reading Ease) 점수를 달성하는 요약을 생성합니다. 대조적으로, 인간이 작성한 비전문가 및 전문가 요약은 각각 평균 플레시 읽기 용이성(Flesch Reading Ease) 점수 53.1 및 22.5를 달성합니다. 즉, 가독성 점수 차이의 크기가 인간과 모델이 작성한 요약 간에 극적으로 다릅니다. 퓨샷(few-shot) 예시를 제공하면 이 문제를 완화할 수 있지만, GPT-3.5-Turbo는 그럼에도 불구하고 제어 가능한 요약 작업에서 인간보다 덜 효과적인 것으로 나타났습니다.

(출처: [9]) GPT-3.5-Turbo로 생성된 요약은 인간이 작성한 요약보다 n-그램(n-gram) 참신성도 낮으며, 이는 모델이 작성한 요약이 본질적으로 더 발췌적이라는 것을 나타냅니다(위 참조). 또한 GPT-3.5-Turbo로 생성된 요약은 요약과 원본 자료 간의 개체(entity) 및 주제 중복으로 측정했을 때 잦은 환각(hallucination)을 보이는 경향이 있습니다(아래 참조). 이러한 모든 결과는 GPT-3.5-Turbo가 제어 가능한 요약(controllable summarization) 영역에서 개선의 여지가 있음을 시사하는 것으로 보이지만, 더 최신 모델(예: GPT-4o)은 더 나은 성능을 보일 가능성이 있다는 점에 유의해야 합니다.

**결론**
우리는 이제 지도 미세 조정(supervised finetuning)에서 선호도 튜닝(preference tuning), 그리고 현대 기반 모델(foundation model)에 이르기까지 여러 세대의 요약 연구 흐름을 살펴보았습니다. 이 탐색 과정에서 몇 가지 공통적인 주제들이 명확하게 드러났습니다:
*   평가의 난이도 (그리고 그 중요성).
*   인간 피드백으로부터 학습의 실질적인 가치.
*   학습 데이터의 품질이 가지는 결정적인 중요성.
*   현대 기반 모델의 놀라운 능력.
이 개요는 특히 요약 연구에 초점을 맞추었지만, 여기서 얻은 발견들은 매우 광범위하게 일반화(generalize)될 수 있습니다. 요약은 조건부 생성(conditional generation)(즉, 특정 입력이 주어졌을 때 출력을 생성하도록 모델을 훈련시키는 것)에 뿌리를 두고 있기 때문에 자연어 처리(natural language processing)의 근본적인 작업입니다. 기계 번역, 텍스트 분류, 키워드 추출, 질의응답 등 다른 많은 중요한 작업들도 매우 유사한 패턴을 따릅니다! 요약 연구에 대한 깊은 이해는 훨씬 더 광범위한 문제를 해결하는 데 도움이 됩니다.
“우리는 인간 피드백 기반 강화 학습(RLHF; Stiennon et al., 2020)을 사용하여 GPT-3를 미세 조정(fine-tune)하여 광범위한 서면 명령어를 따르도록 합니다.” - 출처 [5]
자연어 처리(natural language processing) 연구 내에서 요약이 수행하는 근본적인 역할은 이러한 기술들이 거대 언어 모델 시대에 광범위하게 채택되도록 이끌었습니다. 언어 모델링 연구의 다양한 영향력 있는 접근 방식들은 요약 관련 논문들로부터 지대한 영향을 받았습니다! 예를 들어, InstructGPT는 인간 피드백으로부터 더 나은 요약 모델을 학습시키기 위해 이전에 제안된 학습 알고리즘 [2]을 채택하는 반면, 현대 정렬(alignment) 절차는 이전 요약 연구 [3]에서 옹호된 반복적인 미세 조정(finetuning) 전략을 사용합니다. 이 개요에서 제시된 연구는 실용적으로 유용합니다. 하지만 더 중요한 것은, 오늘날 거대 언어 모델에서 우리가 목격하는 발전의 핵심 토대를 제공한다는 점입니다.

뉴스레터가 처음이신가요? 안녕하세요! 저는 Cameron R. Wolfe입니다. 넷플릭스(Netflix)의 딥러닝(Deep Learning) 박사이자 머신러닝(Machine Learning) 과학자입니다. 이것은 딥(러닝) 포커스 뉴스레터이며, 독자들이 AI 연구의 중요한 주제를 더 잘 이해하도록 돕습니다. 뉴스레터가 마음에 드셨다면, 구독, 공유 또는 X 및 LinkedIn에서 저를 팔로우해주세요! 구독

**참고 문헌**
[1] Böhm, Florian, et al. "Better rewards yield better summaries: Learning to summarise without references." arXiv preprint arXiv:1909.01214 (2019).
[2] Stiennon, Nisan, et al. "Learning to summarize with human feedback." Advances in Neural Information Processing Systems 33 (2020): 3008-3021.
[3] Ziegler, Daniel M., et al. "Fine-tuning language models from human preferences." arXiv preprint arXiv:1909.08593 (2019).
[4] Wu, Jeff, et al. "Recursively summarizing books with human feedback." arXiv preprint arXiv:2109.10862 (2021).
[5] Ouyang, Long, et al. "Training language models to follow instructions with human feedback." Advances in neural information processing systems 35 (2022): 27730-27744.
[6] Goyal, Tanya, Junyi Jessy Li, and Greg Durrett. "News summarization and evaluation in the era of gpt-3." arXiv preprint arXiv:2209.12356 (2022).
[7] Bhaskar, Adithya, Alexander R. Fabbri, and Greg Durrett. "Prompted opinion summarization with GPT-3.5." arXiv preprint arXiv:2211.15914 (2022).
[8] Zhang, Tianyi, et al. "Benchmarking large language models for news summarization." Transactions of the Association for Computational Linguistics 12 (2024): 39-57.
[9] Pu, Dongqi, and Vera Demberg. "ChatGPT vs human-authored text: Insights into controllable text summarization and sentence style transfer." arXiv preprint arXiv:2306.07799 (2023).
[10] Menick, Jacob, et al. "Teaching language models to support answers with verified quotes." arXiv preprint arXiv:2203.11147 (2022).
[11] Fabbri, Alexander R., et al. "Summeval: Re-evaluating summarization evaluation." Transactions of the Association for Computational Linguistics 9 (2021): 391-409.
[12] Subramanian, Sandeep, et al. "On extractive and abstractive neural document summarization with transformer language models." arXiv preprint arXiv:1909.03186 (2019).
[13] Zhang, Fang-Fang, Jin-ge Yao, and Rui Yan. "On the abstractiveness of neural document summarization." Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing . 2018.
[14] Kryściński, Wojciech, et al. "Evaluating the factual consistency of abstractive text summarization." arXiv preprint arXiv:1910.12840 (2019).
[15] Maynez, Joshua, et al. "On faithfulness and factuality in abstractive summarization." arXiv preprint arXiv:2005.00661 (2020).
[16] Durmus, Esin, He He, and Mona Diab. "FEQA: A question answering evaluation framework for faithfulness assessment in abstractive summarization." arXiv preprint arXiv:2005.03754 (2020).
[17] Wang, Alex, Kyunghyun Cho, and Mike Lewis. "Asking and answering questions to evaluate the factual consistency of summaries." arXiv preprint arXiv:2004.04228 (2020).
[18] Brown, Tom, et al. "Language models are few-shot learners." Advances in neural information processing systems 33 (2020): 1877-1901.
[19] Wei, Jason, et al. "Finetuned language models are zero-shot learners." arXiv preprint arXiv:2109.01652 (2021).
[20] Nallapati, Ramesh, et al. "Abstractive text summarization using sequence-to-sequence rnns and beyond." arXiv preprint arXiv:1602.06023 (2016).
[21] Narayan, Shashi, Shay B. Cohen, and Mirella Lapata. "Don't give me the details, just the summary! topic-aware convolutional neural networks for extreme summarization." arXiv preprint arXiv:1808.08745 (2018).
[22] Völske, Michael, et al. "Tl; dr: Mining reddit to learn automatic summarization." Proceedings of the Workshop on New Frontiers in Summarization . 2017.
[23] Kryściński, Wojciech, et al. "Neural text summarization: A critical evaluation." arXiv preprint arXiv:1908.08960 (2019).
[24] He, Tingting, et al. "ROUGE-C: A fully automated evaluation method for multi-document summarization." 2008 IEEE International Conference on Granular Computing . IEEE, 2008.
[25] Papineni, Kishore, et al. "Bleu: a method for automatic evaluation of machine translation." Proceedings of the 40th annual meeting of the Association for Computational Linguistics. 2002.
[26] Zhang, Tianyi, et al. "Bertscore: Evaluating text generation with bert." arXiv preprint arXiv:1904.09675 (2019).
[27] Zhao, Wei, et al. "MoverScore: Text generation evaluating with contextualized embeddings and earth mover distance." arXiv preprint arXiv:1909.02622 (2019).
[28] Zheng, Lianmin, et al. "Judging llm-as-a-judge with mt-bench and chatbot arena." Advances in Neural Information Processing Systems 36 (2024).
[29] Liu, Yang, et al. "Gpteval: Nlg evaluation using gpt-4 with better human alignment." arXiv preprint arXiv:2303.16634 (2023).

1 "발현적 능력(emergent capability)"이란 특정 규모(데이터 또는 컴퓨팅 측면에서)에 도달한 후에야 명확히 관찰되는 모델의 능력을 의미합니다.
2 일반적으로 지도 미세 조정(supervised finetuning)과 선호도 튜닝(preference tuning)의 조합을 통해 다루어지는 정렬(Alignment)은 인간 사용자의 기대에 더 잘 부합하는 출력을 생성하도록 거대 언어 모델을 미세 조정(finetuning)하는 과정을 의미합니다. 자세한 내용은 여기를 참조하십시오.
3 더 구체적으로, 현대 거대 언어 모델의 출력은 자동으로 유창하고 간단한 문법/구문 오류가 없는 경향이 있으므로, 유창성 평가는 그러한 모델에게는 논쟁의 여지가 있지만 불필요합니다.
4 2년 이상 데이터 주석(annotation) 회사에서 근무하며, 저는 많은 수의 인간이 방대한 데이터를 정확하고 신뢰할 수 있게 주석/채점하는 것의 어려움을 깊이 이해하고 있습니다!
5 여기서 우리는 학습 중에 사용된 각 예시에는 참조 요약이 있다고 가정합니다. 그런 다음, 우리는 모델로 요약 출력을 계산하고, 참조 요약과 모델의 출력을 사용하여 ROUGE 점수를 찾은 다음, ROUGE 점수를 강화 학습(RL)을 통한 학습을 위한 보상 신호(reward signal)로 사용할 수 있습니다.
6 기억하십시오, 거대 언어 모델은 출력하는 각 토큰(token)에 확률을 할당하여 작동합니다. 우리는 인간이 작성한 요약에 할당된 확률이 높기를 원합니다. 이러한 요약은 거대 언어 모델이 생성할 수 있는 합리적인 출력을 나타내기 때문입니다.
7 모드 붕괴(Mode collapse)는 거대 언어 모델이 출력 다양성을 잃고 특정 스타일(또는 스타일 세트)을 가진 좁은 범위의 출력만 생성하기 시작하는 현상을 의미합니다.
8 드리프트(Drift)는 단순히 미세 조정(finetune)된 모델이 일부 참조 모델(예: 미세 조정(finetuning) 과정 이전의 모델)과 너무 달라지는 것을 의미합니다.
9 [3]의 저자들은 깊이 3으로 책 요약 작업을 분해함으로써, 수천 단어의 책을 요약하면서 요약 작업의 주석(annotation) 비용을 50배 절감할 수 있다고 언급합니다.
10 이러한 프롬프트의 대부분은 OpenAI API에서 가져온 것이므로, 학습에 사용된 프롬프트와 실제 사용 사례 간에 충분한 중복이 보장됩니다.
11 의견 (또는 리뷰) 요약(Opinion (or review) summarization)은 많은 사용자가 특정 제품 또는 서비스에 대해 남긴 의견 또는 리뷰(예: 제품 리뷰 또는 음식점에 대한 게시된 의견)를 요약하는 작업을 의미합니다.
12 이러한 이유로, 원본에서 텍스트를 직접 복사하는 발췌적 스타일 요약은 일반적인 텍스트 요약을 위한 효과적인 접근 방식임에도 불구하고 의견 요약에서는 성능이 좋지 않은 경향이 있습니다.
13 N-그램(N-gram) 참신성은 요약 내에서 생성되었지만 원본 텍스트에는 없는 n-그램(n-gram)의 비율을 의미합니다.