## 대규모 언어 모델(LLM) 시대의 텍스트 요약: 최신 동향과 발전

(출처: [2, 4, 6, 10, 29]) 대규모 언어 모델(LLM)의 역량은 지난 몇 년간 놀라운 속도로 발전하며 다양한 인공지능 분야에 혁신을 가져왔습니다. 하지만 LLM을 지탱하는 많은 핵심 아이디어들, 즉 자기 지도 사전 학습(self-supervised pretraining), 트랜스포머(transformer) 아키텍처, 인간 피드백 기반 학습(learning from human feedback) 등은 10년 이상 자연어 처리(natural language processing) 연구에서 오랜 시간 축적되어 온 개념들입니다. 이러한 기초적인 연구들은 단순히 새로운 기술이 아니라, 깊이 있는 이론적 기반 위에 세워진 것입니다. 결과적으로, 기계 번역, 요약, 질의응답과 같은 자연어 처리의 핵심 문제에 대한 근본적인 연구는 여전히 엄청나게 중요하며 그 가치가 재조명되고 있습니다! 이 개요에서는 LLM 연구의 진화에 큰 영향을 미친 (추상적) 텍스트 요약((abstractive) text summarization) 문제에 초점을 맞춰 이 점을 설명할 것입니다.

"텍스트 요약은 긴 문서(들)를 원본 문서의 가장 중요한 정보를 보존하는 짧고 유창하며 사람이 읽기 쉬운 형태로 압축하는 것을 목표로 합니다." - 출처 [11]

높은 수준에서 요약은 텍스트 시퀀스를 핵심 정보를 여전히 포착하는 훨씬 짧은 시퀀스로 압축하는 작업을 의미합니다. 요약은 일반적으로 뉴스 기사 요약과 같은 표준 문제에 적용되지만, 문제 설정은 실제로 매우 일반적이며 다양한 흥미로운 응용 분야를 포함할 수 있습니다. 예를 들어, 검색 증강 생성(RAG)을 위한 전처리 단계로 크고 비정형적인 텍스트 시퀀스를 압축하는 데 활용될 수 있습니다. 또한, 추천 시스템의 출력에 대한 이해하기 쉬운 텍스트 요약 작성이나 전사된 녹음에서 회의 요약을 생성하는 것도 가능합니다. 이 작업이 매우 일반적이고 강력하기 때문에 텍스트 요약은 실용적으로 유용한 방대한 연구에 둘러싸여 있습니다. 우리가 보게 될 것처럼, 요약 연구의 많은 핵심 아이디어들이 현대 LLM에 채택되었습니다. 이 두 연구 분야는 밀접하게 연결되어 있으며, 요약 연구에 대한 깊은 이해는 LLM이 왜 그리고 어떻게 그렇게 잘 작동하는지에 대한 새롭고 개선된 시각을 제공합니다! 최근에는 텍스트를 넘어 이미지, 오디오, 비디오 등 다양한 양식의 정보를 요약하는 멀티모달 요약 연구 또한 활발히 진행되며 그 중요성이 더욱 부각되고 있습니다.

### 요약에 대한 유용한 배경 지식

최근 요약 연구에 본격적으로 뛰어들기 전에 기본 사항을 이해하는 것이 중요합니다. 이 섹션에서는 요약 작업을 높은 수준에서 다루고, 존재하는 다양한 요약 유형, LLM의 대중화 이전의 요약 연구, 요약 평가 지표 등을 살펴볼 것입니다.

### 요약 유형

**추상적 요약(Abstractive summarization) vs. 추출적 요약(extractive summarization)**

요약의 목표는 더 긴 원본 문서의 핵심 아이디어를 포착하는 더 짧은 텍스트를 생성하여 정보를 핵심 구성 요소로만 압축하는 것입니다. 문헌에는 연구되는 두 가지 주요 요약 기법이 있습니다:
*   **추출적 요약(Extractive)**: 요약은 원본 문서에서 전체 문장 또는 텍스트 구간을 선택적으로 복사하여 구성됩니다.
*   **추상적 요약(Abstractive)**: 원본 문서의 정보를 재구성하여 관련 정보에 대한 더 짧은 설명을 형성합니다.

이 두 가지 요약 전략은 널리 연구되지만, LLM으로 생성된 요약은 주로 추상적 요약으로 간주됩니다. 그 이유는 일반적으로 LLM이 원본 문서에서 문장을 직접 복사하도록 학습되지 않고, 제공된 맥락(context)에 기반하여 자유롭게 텍스트를 생성하여 원본 문서의 정보를 임의로 재구성할 수 있기 때문입니다. 그러나 우리가 보게 될 것처럼, LLM으로 생성된 요약은 실제로는 (상대적으로) 추출적인 경향을 보이기도 합니다. 또한, 최근 연구는 i) 추출적 요약 기법을 사용하여 추상적 요약 모델 [7, 12]의 입력으로 포함할 핵심 맥락(context)을 식별하거나, 심지어 ii) LLM이 응답을 생성할 때 관련 출처(또는 텍스트 구간)를 인용하도록 가르치는 [10] 관련 주제를 탐구하기 시작했습니다. 이러한 이유로, 기법이 다를 수 있더라도 추출적 요약과 추상적 요약 사이에는 상당한 중복이 존재하며, 최근에는 이 두 가지 접근 방식을 결합한 하이브리드 요약 기술이 더욱 각광받고 있습니다. 예를 들어, RAG(Retrieval-Augmented Generation) 시스템은 LLM이 생성하는 텍스트를 외부 지식으로 보강하여 사실성을 높이고, 이는 특정 측면에서 LLM 요약을 더욱 "추출적"으로 만듭니다.

(출처: [10]) **추가 자료.** 이 게시물의 초점은 LLM을 사용한 요약이므로, LLM의 대중화 이전의 추상적 및 추출적 요약에 대해 깊이 다루지는 않을 것입니다. 그러나 그러한 연구는 이 게시물에서 다룰 작업의 토대를 마련하므로 여전히 매우 관련성이 높습니다. 관심 있는 분들을 위해 엄선된 논문 목록이 아래에 제공됩니다:

*   **SummEval: 요약 평가 재평가 [11]**: 이 연구는 요약을 위한 자동 평가 지표에 대한 심층 분석을 제공하고, 기존 프로토콜의 한계를 탐구하며, 제안된 개선 사항을 사용하여 24개의 추상적 및 추출적 요약 모델 스위트를 재평가합니다.
*   **신경 문서 요약의 추상성에 대하여 [13]**: 이 연구는 추상적 요약 기법을 연구하며, 많은 추상적 요약이 실제로는 본질적으로 매우 추출적이라는 것을 발견합니다!
*   **추상적 요약의 충실도와 사실성 [15]**: 이 연구는 추상적 요약 기법의 충실도와 사실성에 대한 광범위한 분석을 제공합니다. 유사한 분석은 [14, 16, 17]에서도 제공됩니다.

주목할 만한 추출적 요약 기법으로는 NEUSUM, BanditSum, LATENT, REFRESH, RNES, JECS, STRASS가 있습니다. 추출적 요약 기법에 대한 더 포괄적인 조사는 여기를 참조하십시오.
주목할 만한 (LLM 이전) 추상적 요약 기법으로는 Pointer Generator, Fast-abs-rl, Bottom-Up, ROUGESal, Soft-MT, SENECA, T5, BertSum, Pegasus, BART, UniLM이 있습니다. 추상적 요약 기법에 대한 더 포괄적인 조사는 여기를 참조하십시오.
또한, LLM에 대한 초기 연구(예: GPT-2 및 GPT-3)의 대부분은 추상적 요약을 모델 품질 평가를 위한 핵심 작업으로 사용합니다. 이러한 초기의 연구들은 현대 LLM의 미세 조정(fine-tuning) 전략과 평가 방법론에 지속적으로 영향을 미치고 있습니다.

### LLM으로 요약 작성하기

(출처: [6]) 이 게시물의 나머지 부분에서 보게 될 것처럼, LLM은 고품질의 추상적 요약을 생성하는 데 훌륭한 도구입니다. 요약을 위해 LLM을 사용하는 접근 방식은 크게 두 가지로 나뉩니다. 높은 수준에서 이러한 접근 방식은 다음 중 하나입니다:
*   순수하게 인컨텍스트 학습(in-context learning)을 사용하거나
*   요약 전용 데이터로 맞춤형 모델을 미세 조정(finetune)합니다.

즉시 사용 가능한 모델(out-of-the-box model)이 우리 애플리케이션에 잘 작동한다면, 인컨텍스트 학습(in-context learning)이 가장 간단한 접근 방식일 것입니다. 우리는 단순히 API를 호출하여(프롬프트에 일부 맥락(context)을 추가하여) 요약을 생성할 수 있습니다. 그러나 프롬프트(prompt)만으로는 충분하지 않다면, 지도 학습 방식(supervised fashion) 또는 선호도 튜닝(preference tuning)을 사용하여 맞춤형 모델을 미세 조정(finetuning)하는 것을 고려할 수 있습니다. 최근 LLM의 규모와 능력 향상으로 인해, 복잡한 요약 작업에서도 인컨텍스트 학습의 성능이 크게 개선되고 있습니다. 이러한 각 기술의 기본 사항은 아래에 설명되어 있습니다.

(출처: [18]) **인컨텍스트 학습(In-context learning)**은 단일 기반 LLM이 프롬프트에 제공된 정보를 활용하여 다양한 다운스트림(downstream) 작업을 정확하게 해결하는 능력을 의미합니다. 예를 들어, 요약의 경우, 우리는 여러 기사와 그에 관련된 요약을 프롬프트 내에 배치하여 모델이 최종 출력 요약을 생성할 때 이 요약들을 맥락(context)으로 사용할 수 있도록 합니다. 미세 조정(finetuning)과 달리, 인컨텍스트 학습(in-context learning) 중에는 모델의 매개변수(parameter)를 업데이트하지 않습니다. 오히려 우리는 문제를 더 잘 해결하기 위해 프롬프트에 제공된 맥락(context)을 적절히 활용하는 모델의 능력에 의존합니다. 인컨텍스트 학습(in-context learning)은 GPT-3 [18]의 제안과 함께 처음 관찰된 LLM의 발현적 능력(emergent capability) 1입니다. 위 그림에서 볼 수 있듯이, LLM은 특정 규모에서 더 유능한 퓨샷 학습자(few-shot learner)가 되며, 이는 더 큰 모델이 프롬프트의 맥락(context)을 더 효과적으로 활용하여 문제를 해결할 수 있음을 나타냅니다. 인컨텍스트 학습(in-context learning)을 사용하려면, 우리는 간단한 프롬프트(위에서 보여준 것과 같은)로 독점 API(예: OpenAI 또는 Anthropic)를 호출하여 요약을 생성할 수 있습니다. 최근에는 사고의 사슬(Chain-of-Thought)이나 자체 일관성(Self-Consistency)과 같은 고급 프롬프트 엔지니어링 기술이 인컨텍스트 학습의 요약 성능을 더욱 향상시키고 있습니다.

(출처: [6]) **지도 미세 조정(Supervised finetuning).** 프롬프트(prompt)만으로 충분한 품질의 요약을 생성하기 어렵다면, 다음 단계는 고품질 요약 데이터셋으로 사전 학습된 LLM을 미세 조정(finetune)하는 것입니다. 이를 위해 우리는 단순히 (인간 주석자(annotator)를 통해 수동으로 또는 강력한 LLM으로 합성적으로) 우리가 해결하려는 작업과 관련된 잘 작성된 문서-요약 쌍 세트를 수집합니다.
“대규모 언어 모델 사전 학습(pretraining)은 고성능 달성을 위해 점점 더 널리 보급되고 있습니다… [이 모델들은] 인간 시연(demonstration) 세트의 로그 확률(log probability)을 최대화하기 위해 지도 학습(supervised learning)을 사용하여 미세 조정(fine-tuned)됩니다.” - 출처 [2]
예를 들어, 식당 리뷰를 요약하고 싶다면, 우리는 각 식당의 리뷰에 대한 선별된 요약과 함께 여러 식당에 대한 리뷰 데이터셋을 수집해야 합니다. 사전 학습된 기본 모델로 시작하여, 수집된 데이터에 대해 표준 언어 모델링 목표를 사용하여 이 모델을 미세 조정(finetune)할 수 있습니다. 이러한 접근 방식을 **지도 미세 조정(SFT)**이라고 합니다. 대규모 사전 학습과 요약 전용 데이터에 대한 지도 미세 조정(supervised finetuning)의 조합은 매우 효과적이며, 기반 LLM의 대중화 이전에 요약 모델 학습을 위한 가장 많이 사용되는 접근 방식이었습니다. 최근에는 강력한 LLM을 활용하여 고품질의 합성 데이터셋을 생성하고 이를 SFT에 활용하는 방식이 점차 보편화되고 있습니다.

(출처: [19]) **명령어 튜닝(Instruction tuning).** 요약 데이터에 대한 지도 학습을 넘어, 우리는 범용 명령어 튜닝(instruction tuning)을 수행할 수 있습니다. 이 방식에서는 작업 템플릿을 사용하여 여러 작업의 데이터로 단일 모델을 동시에 미세 조정(finetune)합니다. 특히, 요약은 명령어 튜닝(instruction tuning) 과정에 포함될 수 있는 작업 중 하나이며, 결과 모델은 다양한 작업을 정확하게 해결하는 데 활용될 수 있습니다. 실제로, 명령어 튜닝(instruction-tuned) 모델은 학습 중에 보지 못한 작업에도 잘 일반화(generalize)되는 것으로 나타납니다. 이러한 명령어 튜닝은 LLM을 특정 작업의 "전문가"가 아닌 "만능 해결사"로 만드는 데 결정적인 역할을 합니다.

(출처: [19]) **인간 피드백(선호도 튜닝)(preference tuning).** 마지막으로, 우리는 인간 피드백을 직접 사용하여 요약 모델을 미세 조정(finetune)할 수 있습니다. 일반적으로 선호도 튜닝(preference tuning)이라고 불리는 이러한 접근 방식은 인간 피드백 기반 강화 학습(RLHF)을 통한 LLM 정렬(alignment)에 대한 후속 작업의 토대를 마련했습니다. 실제로 InstructGPT [5](ChatGPT의 전신)는 인간 피드백을 통한 요약 학습 연구 [2]를 그들의 정렬(alignment) 전략 2에 대한 영감으로 인용합니다! 따라서 이 개요에서는 선호도 튜닝(preference tuning) 전략에 중점적으로 다룰 것입니다. 이 방법론은 LLM이 단순히 텍스트를 생성하는 것을 넘어, 인간의 가치와 선호도에 부합하는 방식으로 작동하도록 만드는 데 핵심적인 역할을 합니다.

(출처: [2]) 인간 피드백을 기반으로 LLM을 학습시키기 위해, 우리는 먼저 선호도 데이터(preference data)를 수집합니다. 각 선호도 데이터 예시는 동일한 원본 문서에 대한 요약 쌍이며, 하나의 요약은 (일반적으로 인간에 의해) 다른 요약보다 "더 낫다"고 식별됩니다. 이 선호도 데이터는 보상 모델(reward model)을 학습시키는 데 사용될 수 있습니다.
“게시물과 후보 요약이 주어졌을 때, 우리는 이 요약이 우리의 레이블러(labeler)에 의해 판단된 더 나은 요약일 로그 오즈(log odds)를 예측하도록 보상 모델(reward model)을 학습시킵니다.” - 출처 [2]
보상 모델(reward model)은 요약(또는 일반적으로 텍스트 시퀀스)을 입력으로 받아 스칼라 점수(scalar score)를 출력으로 생성하며, 이는 주어진 요약에 대한 인간 선호도 점수(human preference score)를 나타냅니다. 보상 모델(reward model)을 학습시키기 위해, 우리는 쌍에서 선호되는 요약이 거부된 요약보다 더 나은 점수를 받을 로그 확률(log probability)을 단순히 최대화하는 순위 손실(ranking loss)을 사용합니다. 이전 연구에서는 이러한 쌍별 순위 손실(pairwise ranking loss)이 회귀(regression)를 통해 직접 선호도 점수를 학습하는 것보다 성능이 우수하다는 것을 보여주었습니다 [1]. 학습 후, 보상 모델(reward model)은 원본 문서와 요약을 입력으로 받아 스칼라 점수(scalar score)를 출력으로 생성하며, 점수가 높을수록 인간이 요약을 선호할 것임을 나타내고 그 반대도 마찬가지입니다. 최근에는 보상 모델의 아키텍처와 학습 방식 또한 더욱 정교해지고 있으며, 앙상블 기법이나 더 복잡한 신경망을 활용하여 인간 선호도를 더욱 정확하게 포착하려는 시도가 이어지고 있습니다.

(출처: [2]) 그런 다음 이 보상 모델(reward model)을 강화 학습(RL) 알고리즘(예: PPO)을 통해 요약 모델을 학습시키는 신호로 사용할 수 있습니다. 이러한 방식으로 우리는 선호도 쌍(preference pair) 형태의 인간 피드백을 사용하여 요약 모델을 학습시킵니다! RLHF에 대한 더 자세한 내용은 아래의 심층 분석을 확인하십시오. 최근에는 PPO 외에도 DPO(Direct Preference Optimization)와 같이 RLHF 과정을 간소화하고 안정성을 높이는 새로운 강화 학습 알고리즘들이 연구되고 있습니다.
**RLHF 이야기**

### 요약을 위한 인기 데이터셋

(출처: [20]) 요약은 일반적인 문제이지만, 문헌에서 거의 보편적으로 사용되는 몇 가지 일반적인 데이터셋이 있습니다. 요약 연구는 뉴스 요약에 중점을 두는 경향이 있습니다. 예를 들어, CNN / DailyMail 코퍼스(Corpus) [20]는 가장 널리 사용되는 데이터셋 중 하나입니다. 전체적으로 이 데이터셋은 CNN과 DailyMail의 30만 개 이상의 기사와 각 기사에 대한 관련 요약을 포함합니다. 또 다른 단일 문서 추상적 요약 데이터셋인 XSum [21]도 요약 연구에서 일반적으로 사용됩니다. CNN / DailyMail과 유사한 크기(즉, 약 23만 개의 요약)를 가진 XSum은 데이터셋의 각 기사의 주요 아이디어를 포착하는 간단한 단일 문장 요약으로 구성됩니다.
“우리는 더 일반적으로 사용되는 CNN/DM 데이터셋보다 TL;DR 데이터셋을 선택했습니다. 주로 간단한 추출적 기준선(baseline)으로 CNN/DM에서 매우 강력한 성능을 달성할 수 있기 때문입니다.” - 출처 [2]
널리 사용됨에도 불구하고, CNN / DailyMail 및 XSum 데이터셋은 비교적 해결하기 쉽습니다. 때로는 더 복잡한 데이터셋이 필요합니다. 최근 연구는 Reddit의 3백만 개 게시물과 각 게시물 작성자가 작성한 요약을 포함하는 TL;DR 데이터셋 [22]을 탐구했습니다. 크기 때문에 이 데이터셋은 일반적으로 품질 향상을 위해 필터링되며 관련 주제의 게시물만 포함되도록 합니다(자세한 내용은 [2]의 섹션 3.2 참조). 최근에는 특정 도메인이나 멀티모달 정보를 포함하는 새로운 요약 데이터셋들이 지속적으로 구축되고 있으며, 강력한 LLM을 활용한 합성 데이터 생성 기술 또한 데이터셋의 다양성과 규모를 확장하는 데 기여하고 있습니다.

(출처: [22]) **품질 문제.** 대부분의 요약 데이터셋은 각 문서에 대한 원본 문서와 참조 요약(reference summary)을 모두 포함합니다. 그러나 우리가 그것들을 "참조"라고 부른다고 해서 이 데이터셋에 포함된 요약이 고품질이라는 의미는 아닙니다! 실제로 많은 논문 [8, 11]은 CNN / DailyMail과 같은 데이터셋의 참조 요약이 인간이 작성한 요약보다 훨씬 나쁘다는 것을 관찰합니다. 이 발견은 몇 가지 이유로 문제가 됩니다:
*   우리는 종종 이러한 참조 요약을 지도 학습에 사용합니다.
*   이러한 참조는 평가 지표를 계산하는 데 사용될 수 있습니다.
CNN / DailyMail의 저품질 참조 요약의 몇 가지 예시는 아래 그림에 나와 있습니다. 이러한 데이터 품질 문제는 LLM-as-a-Judge와 같은 새로운 평가 방법론의 개발을 촉진했으며, LLM이 생성하는 고품질의 합성 참조 요약을 활용하여 데이터셋의 품질을 개선하려는 시도도 활발합니다.

(출처: [11]) **요약을 어떻게 평가할 수 있을까?**
“추상적 요약을 평가하는 것은 어렵습니다. 기능성을 테스트할 수 있는 제약된 번역이나 코드 생성처럼 간단하지 않습니다.” - Eugene Yan
추상적 요약이 개방형이라는 점을 고려할 때, 요약 품질을 평가하는 것은 어려울 수 있습니다. 원본 문서를 요약하는 여러 가지 실행 가능한 방법이 있으며, 주어진 요약이 다른 요약보다 "더 낫다"고 판단하는 것은 주관적입니다! 이러한 이유로, 추상적 요약을 적절하게 평가하는 것은 우리(인간과 연구자)가 "좋은" 요약을 설명하는 일련의 기준을 수립하는 것에서 시작됩니다. 예를 들어, [23]에서는 다음 기준이 제안됩니다:
*   **유창성(Fluency)**: 요약의 문장이 읽기 쉽고 오류가 없습니다.
*   **일관성(Coherence)**: 요약 전체가 읽기 쉽고, 응집력 있으며, 합리적인 방식으로 구성되어 있습니다.
*   **관련성(Relevance)**: 요약은 원본 문서에서 가장 "중요한" 정보를 포함합니다.
*   **일치성(Consistency)**: 요약의 정보가 정확하고 원본 문서와 일치합니다(즉, 환각(hallucination)이나 잘못된 정보가 없습니다).

그러나 이것이 우리가 정의할 수 있는 유일한 기준 세트는 아닙니다! 논쟁의 여지가 있지만, 유창성(fluency)은 현대 LLM 3에 의해 대부분 해결되었으며, 우리가 중요하게 생각하는 기준은 해결하려는 사용 사례에 따라 달라질 수 있습니다. 예를 들어, [7]의 저자들은 사용자로부터의 의견이나 리뷰(예: Yelp)를 요약하는 데 더 적합한 충실도(faithfulness), 사실성(factuality), 일반성(genericity)을 포함하는 대안적인 기준 세트를 고안합니다. 좋은 요약의 바람직한 특성을 정의하는 것이 평가 과정의 첫 번째 단계입니다. 이것이 명확해지면, 성능을 측정하고 반복함으로써 더 나은 추상적 요약 모델을 개발하는 것에 대해 생각하기 시작할 수 있습니다. 특히 LLM 시대에는 "환각(hallucination)"과 "사실적 일관성(factual consistency)"에 대한 평가의 중요성이 더욱 커지고 있습니다.

**인간 평가(Human evaluation).** 추상적 요약 품질을 평가하기 위한 많은 자동 전략이 있지만, 인간 평가(human evaluation)는 가장 신뢰할 수 있는 평가 접근 방식이며, 요약 연구 전반의 품질 평가를 위한 "정답(ground truth)" 역할을 합니다. 모델의 품질을 진정으로 알기 위해서는 인간 평가(human evaluation)를 거쳐야 합니다. 그럼에도 불구하고, 인간 평가(human evaluation)는 만능 해결책이 아닙니다! 인간으로부터 정확하고 신뢰할 수 있으며 일관된 품질 레이블을 얻는 것은 극도로 어렵습니다 4. 특히 추상적 요약과 같은 주관적인 작업에서는 더욱 그렇습니다. 인간은 끊임없이 서로 의견이 다릅니다(요약 품질 외에도 더 많은 것에 대해!). 이는 평가 과정을 상당히 노이즈가 많게 만들 수 있습니다. 이러한 문제를 완화하기 위해, 우리는 플라이스 카파(Fleiss’ kappa) 및 크리펜도르프 알파(Krippendorff's alpha)와 같은 지표를 사용하여 인간 주석자(annotator) 간의(또는 주석자(annotator)와 연구자 간의) 일치도 수준을 모니터링할 수 있습니다. 최근에는 크라우드소싱 플랫폼과 AI 보조 주석 도구를 활용하여 인간 평가의 효율성을 높이려는 시도가 활발히 이루어지고 있습니다.

**전통적인 (자동) 지표.** 인간 평가(human evaluation)가 요약 품질의 정답(ground truth) 역할을 하지만, 인간 품질 평가를 수집하는 것은 비용이 많이 들고 시간이 소모되기 때문에 순전히 인간 평가(human evaluation)에만 의존할 수는 없습니다. 우리는 인간 평가(human evaluation) 시도 사이에 모델을 더 빠르게 반복 개발할 수 있도록 하는 자동 지표가 필요합니다. 먼저, 요약 작업을 위한 더 전통적인 자동 평가 지표를 살펴보겠습니다. 이들은 두 가지 범주로 나뉩니다:
*   참조 기반(Reference-based)
*   참조 불필요(Reference-free) (또는 맥락 기반(context-based))

참조 기반(Reference-based) 지표는 요약 품질을 측정하는 데 사용할 수 있는 목표 또는 참조 요약(일반적으로 인간이 작성한)이 있다고 가정하는 반면, 참조 불필요(Reference-free) 지표는 생성된 요약과 원본 문서에만 순수하게 기반하여 요약 품질을 평가합니다. 요약을 위한 가장 일반적으로 사용되는 평가 지표는 ROUGE(Recall-Oriented Understudy for Gisting Evaluation) 점수입니다. 이 점수는 참조 요약에 있고 모델이 생성한 출력에도 나타나는 단어 수(또는 ROUGE-N의 n-그램(n-gram) 수)를 단순히 세는 방식으로 작동합니다. ROUGE는 참조 요약과 출력 요약 간의 중복을 측정하는 참조 기반(reference-based) 지표입니다. ROUGE 외에도 요약 품질을 계산하기 위해 유사한 전략을 사용하는 많은 다른 참조 기반(reference-based) 지표가 있습니다:
*   **BLEU(Bilingual Evaluation Understudy) 점수 [25]**: 생성된 출력과 참조 요약 간의 일치하는 n-그램(n-gram) 수를 세고, 이 수를 생성된 출력 내의 총 n-그램(n-gram) 수로 나누어 번역 작업을 평가하는 데 일반적으로 사용됩니다.
*   **BERTScore [26]**: 생성된 출력과 참조 출력의 각 n-그램(n-gram)에 대해 (BERT를 사용하여) 임베딩(embedding)을 생성한 다음, 코사인 유사도(cosine similarity)를 사용하여 두 텍스트 시퀀스의 n-그램(n-gram)을 비교하여 정확한 일치 대신 n-그램(n-gram) 간의 의미론적 일치(semantic match)를 가능하게 합니다.
*   **MoverScore [27]**: n-그램(n-gram) 간의 일대일 매칭을 요구하는 BERTScore를 다대일 매칭을 허용하도록 일반화하여 평가 프레임워크를 더 유연하게 만듭니다.

특정 경우에 참조 기반(reference-based) 지표는 바람직하지 않을 수 있습니다. 예를 들어, 우리의 참조 요약이 저품질이거나, 참조 요약에 전혀 접근할 수 없을 수도 있습니다! 이러한 경우를 처리하기 위해, 우리는 참조 요약 대신 출력 요약을 원본 문서와 비교하여 ROUGE의 맥락 기반(context-based) 버전인 ROUGE-C [24]를 도출할 수 있습니다.
동일한 전략을 사용하여 BERTScore 및 MoverScore의 참조 불필요(reference-free) 변형도 생성할 수 있습니다! 존재하는 다양한 참조 불필요(reference-free) 및 참조 기반(reference-based) 요약 지표에 대한 자세한 내용은 이 논문을 확인하십시오. 그러나 이러한 전통적인 지표들은 LLM이 생성하는 요약의 미묘한 품질 차이, 특히 사실적 일관성이나 새로운 정보 생성 능력 등을 포착하는 데 한계를 보입니다.
“최근 연구들은 대규모 언어 모델(LLM)을 NLG 평가를 위한 참조 불필요(reference-free) 지표로 사용할 것을 제안합니다. 이는 인간 참조가 부족한 새로운 작업에 적용할 수 있다는 이점이 있습니다.” - 출처 [29]

**LLM-as-a-Judge.** LLM 출력(추상적 요약 작업 포함)을 평가하기 위한 인기 있는 전략 중 하나는 LLM-as-a-Judge [28]입니다. 이는 강력한 LLM(예: GPT-4)을 평가에 사용합니다. 생성된 출력을 평가하거나 점수를 매기기 위해, 우리는 단순히 LLM에 프롬프트(prompt)를 제공합니다! 이는 몇 가지 다른 방식으로 수행될 수 있습니다:
*   생성된 출력 쌍 내에서 선호되는 출력을 식별하도록 LLM에 요청.
*   프롬프트에 명시된 기준에 따라 단일 생성된 출력에 대해 (지정된 범위 내의) 스칼라 점수(scalar score)를 생성하도록 LLM에 요청.
*   정확한 채점을 시연하는 몇 가지 퓨샷(few-shot) 예시를 기반으로 생성된 출력을 평가하도록 LLM에 요청.

(출처: [28]) LLM-as-a-judge는 LLM의 출력을 평가하기 위한 새롭고 강력하며 참조 불필요(reference-free) 전략입니다. 그러나 이 평가 접근 방식은 몇 가지 형태의 편향(bias)을 도입합니다:
*   **위치 편향(Position bias)**: 모델의 프롬프트 내에서 생성된 출력의 위치가 결과 점수에 영향을 미칠 수 있습니다. 이를 해결하기 위해 우리는 무작위로 샘플링된 위치로 여러 점수를 생성하고 그 평균을 취할 수 있습니다.
*   **장황함 편향(Verbosity bias)**: GPT-4와 같은 모델은 더 장황한 출력을 선호하는 경향이 있습니다. 우리는 생성된 출력의 길이를 정규화하여 이를 해결할 수 있습니다.
*   **자기 강화 편향(Self-enhancement bias)**: GPT-4 및 기타 모델은 자신의 출력에 다른 모델의 출력보다 더 높은 점수를 부여하는 경향이 있으므로, 어떤 LLM이 자신의 생성을 채점하는 데 사용될 때는 주의해야 합니다!
*   **제한된 능력**: LLM은 완벽하지 않습니다! 따라서 LLM-as-a-Judge를 사용하여 판사 자체가 해결하기 어려워하는 문제(예: 복잡한 수학 또는 추론 문제)에 대한 모델의 출력을 채점할 때 한계에 부딪힐 수 있습니다.

이러한 편향(bias)에도 불구하고, LLM-as-a-judge 스타일의 평가는 놀랍도록 견고하며 다양한 응용 분야에서 인간 평가와 잘 상관관계가 있어 최근 연구에서 광범위하게 채택되고 있습니다(요약 및 그 이상). [29]에서 저자들은 사고 연쇄 프롬프트(chain of thought prompting) 및 양식 작성(form-filling)으로 LLM 기반 평가를 증강하여, 특히 요약 작업의 평가 품질을 향상시키는 G-Eval이라는 새로운 평가 전략을 만들었습니다. LLM-as-a-Judge는 LLM 개발 과정에서 빠른 반복을 가능하게 하는 강력한 도구로 자리매김하고 있으며, 편향을 줄이고 평가의 신뢰성을 높이기 위한 연구가 지속적으로 이루어지고 있습니다.

(출처: [29]) **보상 모델(Reward models).** 위에서 논의했듯이, 요약 모델 학습을 위한 가장 효과적인 전략 중 하나인 선호도 튜닝(preference tuning)은 인간 선호도 데이터셋으로 보상 모델(reward model)을 학습시키는 것을 포함합니다. 이 모델의 출력은 RL을 사용한 미세 조정(finetuning)을 위한 보상 신호로 사용되지만, 동일한 보상 신호는 품질 평가 목적으로 재활용될 수 있습니다! 보상 모델(reward model)은 생성된 요약을 입력으로 받아 이 요약에 대한 인간 선호도 점수(human preference score)를 예측합니다. 따라서 보상 모델(reward model)의 출력은 인간 선호도의 대리 지표(proxy)이며, 이는 참조 불필요(reference-free) 품질 평가로 직접 사용될 수 있습니다. 더 자세한 정보는 아래의 Nathan Lambert의 보상 모델(reward model) 평가에 대한 훌륭한 글을 확인하십시오. 보상 모델의 지속적인 개선은 LLM의 성능 향상과 직결되며, 이는 LLM이 생성하는 데이터로 다시 보상 모델을 학습시키는 선순환 구조를 만들어냅니다.
**RewardBench**

### 인간 피드백으로 요약 개선

“학습 시 요약기가 참조 요약을 재현하도록 하는 지도 학습 패러다임과 비교하여, RL은 요약기가 생성된 요약의 품질을 측정하는 보상(reward)을 최대화하도록 직접 최적화합니다.” - 출처 [1]
오랫동안 요약 모델 학습을 위한 최첨단 접근 방식은 고품질 참조 요약 데이터셋으로 사전 학습된 기본 모델을 지도 미세 조정(supervised finetuning)하는 것이었습니다. 이 접근 방식은 효과적이지만, 이 섹션에서 보게 될 것처럼 선호도 튜닝(preference tuning)을 통해 더 나은 결과를 얻을 수 있습니다. 인간 피드백은 훨씬 더 나은 요약 모델을 학습시킬 수 있도록 합니다. 그러나 이러한 연구는 요약 주제를 넘어섭니다. 유사한 기술들이 최근 LLM 정렬(alignment) 연구에 의해 재활용되어 LLM 학습 파이프라인의 기반을 형성했습니다. 최근에는 인간 피드백 외에도 AI 자체의 피드백을 활용하는 헌법적 AI(Constitutional AI)와 같은 접근 방식이 연구되며, 정렬 기술이 더욱 발전하고 있습니다.
**더 나은 보상이 더 나은 요약을 만든다 [1]**

(출처: [1]) 지도 학습은 원래 요약 모델 학습을 위한 가장 일반적으로 사용되는 패러다임이었습니다. 우리는 단순히 인간이 작성한 참조 요약을 모방하도록 모델을 학습시켰습니다. 최근에는 연구자들이 요약 모델 학습을 위한 RL 사용을 탐색하기 시작했습니다. 초기 시도는 ROUGE 점수를 보상(reward) 5으로 직접 사용했지만, ROUGE 점수는 인간 품질 평가와 상관관계가 낮습니다. 따라서 [1]의 저자들은 RL을 통해 모델을 인간이 더 선호하는 요약으로 유도하는 더 나은 보상 함수(reward function)를 찾으려고 시도합니다.
“인간에게 매력적인 요약을 생성하기 위한 더 나은 보상 함수(reward function)를 찾기 위해, 우리는 2,500개의 요약에 대한 인간 평가로부터 보상 함수(reward function)를 학습합니다.” - 출처 [1]

**보상 학습.** [1]의 저자들은 인간 선호도 데이터셋으로부터 보상 함수(reward function)를 학습할 것을 제안합니다. 이전 연구에서 가져온 이 데이터셋은 CNN / DailyMail 코퍼스(corpus)의 500개 뉴스 기사에 대한 2,500개의 요약(인간 평가 포함)을 포함합니다. 이 데이터를 사용하여, 우리는 문서와 시스템 요약을 입력으로 받아 인간 평가를 예측하도록 보상 모델(reward model)을 학습시킬 수 있습니다. ROUGE를 보상(reward)으로 사용하는 기술과 달리, 보상 계산에 참조 요약이 필요하지 않습니다! 보상 모델(reward model)을 학습시키기 위해, 우리는 회귀 목표(regression objective) 또는 선호도 학습 목표(preference learning objective)를 사용할 수 있습니다. 후자는 보상 함수(reward function)가 인간이 선호하는 요약을 정확하게 식별하는지 여부를 포착합니다.
(출처: [1]) [1]에서 보상 함수(reward function)에 대해 여러 아키텍처(architecture)가 고려되었지만, 최상의 결과를 도출하는 접근 방식은 요약과 입력 문서의 (BERT를 사용하여 생성된) 연결된 임베딩(embedding)을 입력으로 받는 피드포워드 네트워크(feed-forward network)입니다. [1]에서 우리는 선호도 학습 목표(preference learning objective)가 회귀 목표(regression objective)에 비해 더 나은 결과를 도출한다는 것을 알 수 있으며, 이는 선호도 학습 목표(preference learning objective)가 이제 보상 모델(reward model) 학습에 거의 보편적으로 사용되는 이유를 설명합니다. 아래 표에서 우리는 최상의 보상 함수(reward function)가 다음을 사용한다는 것을 알 수 있습니다:
*   임베딩(embedding)을 위한 BERT.
*   보상 예측을 위한 피드포워드 네트워크(feed-forward network) (또는 MLP).
*   선호도 기반 학습 목표.
이 보상 함수(reward function)는 인간 판단과 잘 상관관계가 있는 예측을 생성하고 높은 재현율(recall)과 정밀도(precision)로 "좋은" 요약을 식별하는 것으로 나타났습니다. 이 연구의 통찰력은 일반적인 LLM의 RLHF 파이프라인 설계에 직접적인 영향을 미쳤습니다.

(출처: [1]) **더 나은 요약 모델.** 인간 선호도를 정확하게 예측하는 것을 넘어, [1]에서 학습된 보상 모델(reward model)은 더 나은 추출적 및 추상적 요약 모델을 만드는 데 사용될 수 있습니다. 지도 학습 기준선(baseline)과 ROUGE를 보상(reward)으로 사용하여 RL로 학습된 모델 모두와 비교했을 때, 인간 피드백으로부터 학습된 보상(reward)으로 학습된 모델은 훨씬 더 높은 인간 평가 점수를 가진 요약을 생성하는 것으로 나타났습니다(이전 최첨단 시스템보다도 높습니다!).
간단히 말해, [1]에서 우리는 인간 피드백으로부터 보상 함수(reward function)를 학습하는 것이 RL 기반 요약 모델 학습을 위한 우수한 학습 신호를 제공할 수 있다는 것을 알 수 있습니다. 후속 연구는 이 교훈을 채택하고 이를 LLM 영역으로 확장하여 인간 피드백으로부터 다양한 작업(요약 포함)을 학습합니다. 이는 모델이 단순히 표면적인 패턴을 모방하는 것을 넘어, 인간이 중요하다고 생각하는 근본적인 품질 속성을 학습하도록 만듭니다.

**인간 피드백으로부터 요약 학습 [2]**
지도 미세 조정(supervised finetuning)은 다음 토큰 예측 목표(next token prediction objective)를 사용하여 인간 시연(demonstration) 세트의 로그 확률(log probability)을 최대화합니다. 우리는 모델이 인간이 작성한 텍스트 6에 높은 확률을 할당하도록 가르칩니다. ROUGE를 통해 이러한 모델을 평가하면 모델의 출력이 참조와 얼마나 밀접하게 일치하는지 정량화할 수 있습니다. 이 접근 방식은 (상대적으로) 잘 작동하지만, [2]의 저자들은 지도 학습과 ROUGE가 우리의 실제 목표인 고품질 요약 작성의 대리 지표(proxy)일 뿐이라고 지적합니다.
“우리는 인간 선호도를 최적화하도록 모델을 학습시킴으로써 요약 품질을 크게 향상시킬 수 있음을 보여줍니다.” - 출처 [1]
인간은 항상 완벽한 요약을 작성하지 않으며, 어떤 원본 문서에 대해서도 수많은 동등하게 유효한 요약을 작성할 수 있습니다. 따라서 인간이 작성한 요약과 정확히 일치하도록 요약 모델을 학습시키는 것은 결함 있는 접근 방식입니다! 모든 참조 요약(심지어 저품질 요약도)은 학습 과정에서 동등하게 강조되며, 우리는 유효한 요약의 다양성을 설명할 방법이 없습니다. 이를 염두에 두고, 우리는 다음과 같이 궁금해할 수 있습니다: 요약 품질에 기반하여 모델을 직접 최적화하는 목표를 찾을 수 있을까?

(출처: [2]) **인간 피드백으로부터 학습.** [2]에서 저자들은 인간의 선호도 데이터에 기반하여 LLM을 미세 조정(finetune)할 수 있도록 하는 세 부분으로 구성된 프레임워크를 제안함으로써 정확히 이 작업을 수행합니다. LLM은 먼저 인간 참조 요약에 대한 지도 미세 조정(supervised finetuning)을 사용하여 학습되어 지도 학습 기준선(baseline)을 생성하고, 이 기준선은 다시 RL로 추가 미세 조정(finetune)됩니다. [2]의 RLHF 과정은 다음을 통해 인간 피드백 데이터셋을 수집하는 것으로 시작됩니다:
*   학습 데이터셋에서 텍스트 입력(원본 문서) 가져오기.
*   여러 정책(policy)(예: 사전 학습된 모델, 지도 학습 기준선(baseline), 현재 모델 또는 인간 참조 요약)을 사용하여 입력의 요약 샘플링.
*   샘플 응답 세트에서 두 요약 선택.
*   인간 주석자(annotator)에게 두 요약 중 더 나은 것을 식별하도록 요청.
인간 비교 데이터는 대량으로 수집되며 오프라인 방식으로 RLHF를 통해 모델(디코더 전용 LLM)을 미세 조정(finetune)하는 데 사용됩니다. 데이터가 수집되면, 이 비교 데이터를 사용하여 LLM이 생성한 요약이 주어졌을 때 인간 선호도 점수(human preference score)를 정확하게 예측하는 보상 모델(reward model)을 학습시킵니다. 여기에서 우리는 RL을 사용하여 모델을 미세 조정(finetune)합니다( [2]의 저자들은 PPO 알고리즘을 사용합니다). 보상 모델(reward model)이 출력한 선호도 점수에 기반합니다. 이 과정에서 고품질의 인간 주석 데이터와 효율적인 데이터 수집 파이프라인이 필수적입니다.
**PPO의 최적화 목표에 KL 발산(KL divergence) 추가**

(출처: [2]) **드리프트(drift) 방지.** [2]의 저자들은 PPO에 의해 최적화되는 목표에 KL 발산(KL divergence) 항을 추가합니다. 이는 RLHF 동안 정책(policy)이 지도 학습 기준선(supervised baseline) 정책과 너무 달라지는 것을 페널티(penalize)합니다. 이제 일반적으로 사용되는 이러한 접근 방식(예: LLaMA-2 보고서의 Eq. 4 참조)은 모드 붕괴(mode collapse) 7 없이 탐색을 장려하고 LLM이 작성한 요약이 학습 중에 본 것과 너무 달라지는 것을 방지합니다. 이 KL 발산 항은 LLM이 학습 과정에서 안정성을 유지하고 생성된 텍스트의 다양성을 보존하는 데 중요한 역할을 합니다.

(출처: [2]) **피드백으로부터의 학습은 효과적인가?** [2]의 저자들은 위에서 설명한 전략을 사용하여 TL;DR 데이터셋에 대해 13억에서 67억 개의 매개변수(parameter)를 가진 여러 GPT 스타일 언어 모델을 미세 조정(finetune)합니다. 인간 피드백을 통해 학습된 모델은 지도 학습만으로 학습된 모델이 작성한 요약보다 인간에게 일관되게 선호되는 요약을 생성하는 것으로 나타났습니다. 13억 개의 인간 피드백 모델은 지도 학습만으로 학습된 10배 더 큰 모델보다 성능이 우수하며, 67억 개의 인간 피드백 모델은 13억 모델보다도 더 나은 성능을 보입니다. 즉, 요약 품질은 모델 규모(model scale)로부터 이점을 얻습니다. 이러한 결과는 RLHF가 모델의 규모와 무관하게 인간 선호도에 대한 정렬을 통해 성능을 극대화할 수 있음을 입증했습니다.

(출처: [2]) 인간 피드백으로 학습된 요약 모델은 새로운 도메인에 더 잘 일반화(generalize)되는 것으로 보입니다. 예를 들어, [2]에서 TL;DR에 대해 미세 조정(finetune)된 모델은 추가 미세 조정(finetuning) 없이 뉴스 중심 데이터셋에서 잘 작동하는 것으로 나타났습니다. 이러한 일반화 능력은 모델이 특정 데이터셋의 표면적인 패턴을 모방하는 대신, 요약 품질에 대한 인간의 근본적인 선호도를 학습했기 때문에 가능합니다.
“우리는 우리의 보상 모델(reward model)이 인간 선호도 예측에서 ROUGE와 같은 다른 지표보다 성능이 우수하며, 우리의 보상 모델(reward model)을 직접 최적화하는 것이 인간에 따르면 ROUGE를 최적화하는 것보다 더 나은 요약을 생성한다는 것을 확인합니다.” - 출처 [1]

**보상 모델(Reward model) > ROUGE.** ROUGE와 같은 참조 기반(reference-based) 지표가 요약 평가의 표준이지만, [2]의 저자들은 ROUGE 점수가 인간 선호도와 상관관계가 낮은 경향이 있다는 것을 관찰합니다. 이러한 이유로, PPO의 보상 신호로 ROUGE를 사용하여 미세 조정(finetune)된 요약 모델은 인간 선호도를 예측하도록 보상 모델(reward model)을 학습시키는 모델보다 성능이 떨어집니다. [2]에서 보상 모델(reward model)은 인간 선호도를 매우 정확하게 예측하는 것으로 나타났으며, 이는 선호도 기반 지표가 요약 품질 평가에 유용한 접근 방식임을 보여줍니다. 이는 LLM 시대에 접어들면서 전통적인 자동 평가 지표의 한계를 명확히 보여주는 사례입니다.

**인간 선호도로부터 언어 모델 미세 조정 [3]**
[2]의 작업 이전에, 동일한 저자들(OpenAI 소속)은 긍정적인 감성 또는 물리적으로 묘사적인 언어를 사용한 텍스트의 스타일적 연속, 그리고 TL;DR 및 CNN/Daily Mail 데이터셋에 대한 요약이라는 네 가지 다른 작업 [3]에 대해 사전 학습된 LLM을 미세 조정(finetune)하기 위한 인간 피드백 사용을 탐구했습니다. [3]에서 사용된 미세 조정(finetuning) 전략은 [2]의 전략과 대부분 일치하지만, 지도 미세 조정(supervised finetuning) 구성 요소는 없습니다. 우리는 오직 인간 피드백에 기반하여 미세 조정(finetune)합니다. [3]에서는 선호도 튜닝(preference tuning)이 함께 적용될 수 있는 기술이라기보다는 지도 학습의 대안으로 제시됩니다. 이 초기 연구는 SFT 없이도 인간 선호도 학습의 가능성을 보여주며, 이후 RLHF 파이프라인의 중요한 구성 요소로 발전할 수 있는 기반을 마련했습니다.

(출처: [3]) 스타일적 연속을 위해 저자들은 5,000개의 선호도 쌍(preference pair) 데이터셋을 수집했으며, 요약을 위해서는 60,000개 이상의 선호도 쌍(preference pair)이 수집되었습니다. 스타일적 연속을 위한 인간 피드백 모델은 77%의 경우에서 지도 학습 기준선(baseline)보다 선호되는 것으로 나타났습니다. 유사하게, 인간 주석자(annotator)는 지도 학습 기준선(baseline)보다 인간 피드백으로 학습된 요약 모델을 선호하지만, 인간 피드백 모델은 요약될 문서의 처음 세 문장을 단순히 복사하는 간단한 기준선(baseline)에 의해 놀랍게도 성능이 떨어집니다.
흥미롭게도, [3]의 분석은 인간 피드백 기반 요약 모델이 추상적 요약을 생성하기 위한 매우 간단한 정책(policy)을 학습한다는 것을 밝혀냈습니다. [3]의 저자들은 이를 "스마트 복사(smart copying)"라고 부릅니다. 모델은 원본 텍스트에서 큰 텍스트 구간이나 문장을 복사하는 경향이 있으며, 관련이 없거나 요약에 포함할 가치가 없는 문장은 건너뜁니다. 이 메커니즘은 학습 과정에서 자연스럽게 나타납니다. 즉, 복사를 장려하는 명시적인 아키텍처(architectural) 구성 요소가 요약 모델에 추가되지 않습니다. 단순함에도 불구하고, 이 학습된 복사 메커니즘은 인간 주석자(annotator)로부터 호의적인 평가를 받습니다. 이는 LLM이 정보의 "근거"를 원본 텍스트에서 찾는 능력의 초기 형태를 보여줍니다.
“우리는 인간의 판단으로만 정의되는 복잡한 작업에 강화 학습(reinforcement learning)을 적용하고 싶습니다. 이러한 작업에서는 인간에게 물어봄으로써만 결과가 좋은지 나쁜지 알 수 있습니다.” - 출처 [3]

**유용한 시사점.** [2]의 작업과 비교했을 때, [3]에서 인간 피드백을 사용하여 학습된 요약 모델은 훨씬 덜 강력하며 간단한 기준선(baseline)에 의해 성능이 떨어지는 경향이 있습니다. 이러한 부정적인 결과는 사용되는 모델의 차이와 학습 과정에서 지도 미세 조정(supervised finetuning)이 배제되었기 때문일 가능성이 높습니다. 그럼에도 불구하고, [3]의 연구는 후속 발전을 위한 견고한 토대를 마련하며, 이 분석에서 얻을 수 있는 몇 가지 흥미로운 시사점이 있습니다:
*   인간 피드백으로부터의 학습은 i) 충분한 지도 학습 데이터와 ii) 보상 신호로 사용될 좋은 자동 대리 지표(proxy)가 부족한 작업에 가장 잘 작동합니다(예: [2]에서 ROUGE가 유용한 보상 함수(reward function)가 아님을 알 수 있습니다).
*   PPO를 사용한 미세 조정(finetuning)에 사용되는 목표에 KL 발산(KL divergence) 항을 추가하면 과도한 드리프트(drift) 8를 방지하는 데 도움이 될 수 있습니다.
*   인간은 LLM 출력의 품질을 판단하기 위해 간단하고 (불완전한) 휴리스틱(heuristic)에 의존하는 경향이 있습니다.
*   온라인 데이터 수집(또는 LLM이 반복적으로 미세 조정(finetune)되고 개선됨에 따라 보상 모델(reward model)을 재학습시키기 위해 추가 데이터를 계속 수집하는 것)은 향상된 성능을 도출합니다.

온라인 데이터 수집의 이점에 대한 위의 발견은 LLM 연구에 지속적인 영향을 미쳤습니다! LLaMA-2와 같은 최신 모델은 정렬(alignment) 과정에서 새로 수집된 데이터로 여러 "라운드"의 RLHF를 거칩니다. 또한, 우리는 최근 연구에서 인간 평가의 한계가 LLM 출력 품질을 측정할 때 매우 중요한 고려 사항이라는 것을 계속해서 보고 있습니다! "데이터 플라이휠(data flywheel)" 효과는 LLM이 생성한 데이터를 활용하여 더 나은 모델을 만들고, 이는 다시 더 좋은 데이터를 생성하는 선순환 구조를 구축하는 데 핵심적인 역할을 합니다.

### 인간 피드백으로 책을 재귀적으로 요약 [4]

“우리는 작업의 더 작은 부분에 대해 학습된 모델을 사용하여 인간이 더 넓은 작업에 대한 피드백을 제공하는 데 도움을 줍니다.” - 출처 [4]
요약을 위한 참조 요약 또는 선호도 레이블(preference label)을 선별하는 것은 (잠재적으로) 다소 시간이 소모되지만, 아주 어렵지는 않습니다. 인간 주석자(annotator)는 단순히 다음을 수행해야 합니다:
*   요약될 기사 읽기.
*   요약을 작성하거나 제시된 요약의 품질 평가.
그러나 요약될 텍스트가 매우 길 때(예: 전체 소설) 이 과정은 더 어려워집니다. 이 경우, 소설 요약을 평가하는 것은 시간이 소모되는데, 인간 주석자(annotator)는 이미 책을 읽었거나 요약을 정확하게 평가하기 위해 책을 읽는 데 시간을 할애해야 하기 때문입니다. [4]에서 저자들은 인간이 그러한 작업에 대한 학습 신호를 제공할 수 있도록 인간 피드백과 재귀적 작업 분해(recursive task decomposition)를 결합하여 확장 가능한 접근 방식을 제안합니다. 이 접근 방식은 LLM의 긴 컨텍스트 윈도우(context window) 기능과 결합되어 더욱 강력해지고 있습니다.

(출처: [4]) **재귀적 작업 분해(Recursive task decomposition).** [4]에서 제안된 핵심 아이디어는 긴 텍스트를 재귀적으로 요약될 수 있는 더 작은 덩어리로 분해하여, 리프(leaf)가 적절한 크기의 텍스트 덩어리에 대한 표준 요약 작업인 요약 작업 트리(tree)를 형성하는 것입니다. 먼저, 모델은 책의 작은 덩어리를 요약하는 데 사용됩니다. 그런 다음, 동일한 모델이 이러한 리프 요약을 섭취하여 책의 더 큰 부분을 요약합니다. 즉, 더 짧은 구절의 요약이 책의 더 큰 부분을 요약하기 위한 입력으로 사용됩니다. 이 접근 방식을 취함으로써, 인간은 원본 텍스트 전체에 대한 심층적인 지식이 부족하더라도 이 작업 9을 효율적으로 감독할 수 있습니다. 모델은 책 자체 또는 이전에 생성된 요약에서 가져온 작은 텍스트 덩어리만 요약하기 때문입니다. 이 방법은 전체 문서를 한 번에 처리하는 것보다 계산 효율성을 높이는 장점도 있습니다.

(출처: [4]) 추론 시, 모델은 먼저 책의 작은 부분을 요약하는 데 사용됩니다. 그런 다음, 이러한 요약은 전체 책의 요약이 생성될 때까지 재귀적으로 사용하여 더 높은 수준의 요약을 생성합니다. 이러한 재귀적 전략 덕분에, [4]에서 제안된 접근 방식은 임의 길이의 텍스트를 요약할 수 있습니다!

**데이터셋 생성.** 긴 텍스트의 요약을 충분한 깊이로 재귀적으로 분해하면, 결국 인간에 의해 쉽게 감독되고 LLM을 학습시키는 데 사용될 수 있는 합리적인 요약 하위 작업 세트를 얻게 될 것입니다. 재귀적 요약은 세 가지 주요 작업을 가집니다:
*   **분해(Decompose)**: 텍스트가 직접 요약하기에는 너무 길다고 식별하고, 텍스트의 더 짧은 부분에 대해 여러 요약 하위 작업을 생성합니다.
*   **응답(Respond)**: 요약을 생성하여 하위 작업을 해결합니다.
*   **구성(Compose)**: "응답"과 동일하지만, 모델은 요약을 생성할 때 여러 하위 작업에 대한 해결책(즉, 이전에 생성된 요약)을 제시받습니다.
책의 경우, 분해(decompose) 작업은 LLM에게 하위 작업을 생성하도록 요청하는 대신 알고리즘적으로 수행될 수 있습니다. 즉, 긴 텍스트 시퀀스를 더 짧은 시퀀스로 덩어리화합니다. 이 전략을 사용하면 학습 데이터 획득이 간단합니다! 우리는 단순히 인간에게 특정 하위 작업을 수동으로 요약하거나 특정 하위 작업에 대해 생성된 두 요약의 품질을 비교하도록 요청합니다. 노드(node)가 리프(leaf)가 아니어서 인간이 더 긴 텍스트를 요약하는 경우, LLM은 모든 하위 작업의 요약을 재귀적으로 생성하여 맥락(context)으로 사용합니다. 최근에는 LLM이 이러한 하위 작업의 초기 초안을 생성하도록 돕고, 인간은 이를 검토하고 수정하는 AI 보조 주석(AI-assisted annotation) 방식이 많이 활용됩니다.
“우리는 인간 레이블러(labeler)로부터 방대한 시연(demonstration) 및 비교 데이터를 수집하고, 행동 복제(behavioral cloning) 및 보상 모델링(reward modeling)을 사용하여 GPT-3를 미세 조정(fine-tune)하여 요약을 재귀적으로 수행합니다.” - 출처 [4]
지도 학습 예시와 선호도 레이블(preference label)로 구성된 이 데이터셋이 주어졌을 때, [4]에서 사용된 학습 전략은 [2]의 전략과 거의 동일합니다. 사전 학습된 GPT-3 모델로 시작하여, 우리는 먼저 인간이 작성한 참조 요약에 대해 지도 학습 방식으로 모델을 미세 조정(finetune)합니다(이는 [4]에서 행동 복제(behavioral cloning)라고 불립니다). 그런 다음, 수집된 선호도 레이블(preference label)에 대해 여러 번 반복하여 선호도 튜닝(preference tuning)을 수행합니다. [4]에서 사용된 학습 알고리즘에 대한 전체 설명은 아래를 참조하십시오.

(출처: [4]) **실험 결과.** [4]의 학습 데이터는 GPT-3의 사전 학습 데이터셋에서 책 데이터의 하위 집합(즉, Books1 및 Books2 데이터셋)을 사용하여 수집됩니다. 이 데이터는 GPT-3(및 1750억 개 대신 60억 개의 매개변수(parameter)를 가진 더 작은 모델 변형)를 미세 조정(finetune)하여 책을 재귀적으로 요약하는 데 사용됩니다. 그런 다음, 모델은 2020년에 출판된 40권의 인기 도서 세트에 대해 평가됩니다. 이 책들은 GPT-3의 사전 학습 데이터셋에 포함되지 않았으며 여러 장르에 걸쳐 있습니다.
두 명의 레이블러(labeler)가 각 책을 읽고, 요약을 작성하고, 다양한 모델의 요약을 평가하도록 요청받았습니다. 아래에서 볼 수 있듯이, 인간 선호도로 학습된 모델은 지도 학습만으로 학습된 모델보다 훨씬 우수한 성능을 보입니다. 그러나 모든 모델은 여전히 인간 성능에 훨씬 뒤처지며, 이는 추상적 책 요약이 해결하기 엄청나게 어려운 작업임을 보여줍니다. 실제로, 모델이 생성한 요약 중 5-15%만이 인간 품질과 일치하는 것으로 나타났습니다.
(출처: [4]) [4]에서 우리는 첫 번째 하위 작업(즉, 책의 짧은 덩어리 요약)에 대한 학습이 가장 중요하다는 것을 알 수 있습니다. 이 작업에만 학습된 모델은 더 높은 수준의 요약 작업에 비교적 잘 일반화(generalize)될 수 있습니다. 그러나 전체 책 요약 생성 시 성능은 여전히 실망스럽습니다. 개별 하위 작업에 대한 인간 선호도 점수가 책의 전체 분해를 통해 생성된 요약에 할당된 점수보다 훨씬 높습니다. 이 연구는 긴 문서 요약의 복잡성과 현재 LLM의 한계를 보여주지만, 동시에 재귀적 접근 방식의 잠재력을 제시했습니다.

### 인간 피드백으로 명령어에 따르는 언어 모델 학습 [5]

“언어 모델을 더 크게 만든다고 해서 본질적으로 사용자 의도를 더 잘 따르게 되는 것은 아닙니다… 이러한 모델은 사용자와 정렬(align)되지 않습니다… 우리는 인간 피드백으로 미세 조정(fine-tuning)하여 광범위한 작업에서 언어 모델을 사용자 의도에 정렬(align)하는 방법을 보여줍니다.” - 출처 [5]
[5]에서 우리는 OpenAI의 저자들로부터 [2]에서 제안된 유사한 학습 전략이 일반적인 기반 언어 모델이 프롬프트의 명령어를 더 잘 따르도록 만드는 데 사용될 수 있다는 것을 알 수 있습니다. 이 작업의 결과는 InstructGPT라는 언어 모델입니다. 이는 ChatGPT의 전신으로 현대 LLM에 대한 거의 모든 연구의 토대를 마련했습니다. InstructGPT의 핵심 아이디어는 SFT 및 RLHF를 사용하여 사전 학습된 LLM을 미세 조정(finetune)하여 정렬(alignment)을 장려하는 것입니다. 정렬(alignment)은 인간 사용자의 욕구에 부합하는 텍스트를 생성하는 능력으로 정의됩니다. 일반적으로 우리는 명령어 따르기, 유해한 출력 방지, 거짓말 방지, 흥미롭거나 창의적인 출력 생성 등과 같은 고정된 정렬(alignment) 기준 세트를 정의하여 이러한 욕구를 구체적으로 포착합니다. 이 연구는 LLM 개발의 패러다임을 변화시켰습니다.

(출처: [5]) 사전 학습 후, LLM은 반복적이거나 (도움이 되지 않는) 텍스트를 생성하고, 자주 환각(hallucinate)하며, 프롬프트 내의 복잡한 명령어를 따르는 데 어려움을 겪을 가능성이 높습니다. 그러나 정렬(alignment) 과정을 통해 우리는 LLM에 이러한 단점을 피하고 원하는 정렬(alignment) 기준을 충족시키는 데 필요한 기술을 가르쳐 훨씬 더 유용하고 흥미로운 모델을 생성할 수 있습니다.
**InstructGPT 생성.** InstructGPT의 정렬(alignment) 과정은 사전 학습된 언어 모델(13억, 60억, 1750억 모델로 실험이 진행됨)과 LLM이 답변할 수 있어야 하는 프롬프트 10 세트로 시작됩니다. 먼저, 인간 주석자(annotator)가 이러한 각 프롬프트에 대한 답변을 수동으로 제공하여, 사전 학습된 모델이 지도 학습을 통해 미세 조정(finetune)되는 데이터셋을 생성합니다. 이 초기 학습 과정은 모델을 명령어와 유사한 프롬프트에 노출시켜 인간 피드백을 통한 추가 미세 조정(finetuning)을 위한 더 나은 시작점을 만듭니다.
“SFT 데이터셋은 약 13,000개의 학습 프롬프트(API 및 레이블러(labeler) 작성)를 포함하고, RM 데이터셋은 33,000개의 학습 프롬프트(API 및 레이블러(labeler) 작성)를 가지며, PPO 데이터셋은 31,000개의 학습 프롬프트(API에서만)를 가집니다.” - 출처 [5]
여기에서 인간 선호도 레이블(preference label) 데이터셋이 수집되며, 각 선호도 예시는 동일한 프롬프트에 대한 여러 응답을 품질에 따라 순위 매깁니다. 이 선호도 데이터셋은 프롬프트와 생성된 응답을 입력으로 받아 스칼라 보상(scalar reward)을 예측하는 보상 모델(RM)(LLM과 동일한 아키텍처(architecture)를 공유)을 학습시키는 데 사용됩니다.
**보상 모델(Reward model) 아키텍처(architecture)**
우리는 이 RM을 사용하여 관련 프롬프트 세트에 대해 PPO를 통해 LLM을 미세 조정(finetune)할 수 있습니다. [2]의 제안과 거의 정확히 일치하는 이 학습 파이프라인은 거의 모든 LLM에서 사용되는 표준 3단계 정렬(alignment) 절차를 형성합니다. 여기서는 이 접근 방식이 요약이 아닌 정렬(alignment)에 적용됩니다!

(출처: [5]) **실험 결과.** [5]에서 우리는 인간 피드백으로부터의 학습이 정렬(align)된 (그리고 유용한) LLM을 생성하기 위한 놀랍도록 효과적인 전략이라는 것을 알 수 있습니다. 특히, [5]의 저자들은 다음을 관찰합니다:
*   인간 주석자(annotator)는 GPT-3보다 InstructGPT 출력을 선호합니다.
*   InstructGPT는 GPT-3보다 더 진실성이 있는 경향이 있습니다.
*   GPT-3는 InstructGPT보다 더 유해한 출력을 생성합니다.
*   InstructGPT는 명령어를 더 잘 따르며, 미세 조정(finetuning)에 사용된 프롬프트 분포를 넘어선 명령어조차도 잘 따릅니다.
GPT-3와 비교했을 때, InstructGPT는 매우 유리한 정렬(alignment) 속성을 가집니다. 아래 표에서 볼 수 있듯이, InstructGPT는 GPT-3보다 훨씬 더 조종 가능(steerable)한 경향이 있습니다. 이는 사용자가 프롬프트 내에 제약 조건, 명령어 또는 세부 정보를 제공함으로써 모델의 행동을 더 잘 제어할 수 있음을 의미합니다.

(출처: [5]) 이러한 모든 이점에도 불구하고, 우리는 정렬(alignment) 절차가 사전 학습된 모델과 비교하여 공개 벤치마크(benchmark)에서 성능 저하의 형태로 "정렬 세금(alignment tax)"을 수반한다는 것을 알 수 있습니다. 이 세금을 피하기 위해, 우리는 선호도 튜닝(preference tuning) 중에 사전 학습 데이터셋에 대한 간헐적 업데이트를 수행할 수 있습니다. 최근 연구에서는 DPO와 같은 효율적인 정렬 방법론을 통해 "정렬 세금"을 줄이면서도 높은 정렬 수준을 유지하려는 노력이 계속되고 있습니다.

### LLM 시대의 요약

InstructGPT가 요약에 중점을 두지 않았다는 점을 고려할 때, 이 논문이 왜 이 개요에 포함되었는지 궁금할 수 있습니다. 그러나 InstructGPT는 단일 유형의 작업을 해결하는 데 특화된 모델인 좁은 전문가(narrow expert)에서 벗어나 다양한 작업을 정확하게 해결할 수 있는 모델인 기반 모델(foundation model)로의 NLP 연구의 패러다임 전환을 나타냅니다. 추상적 요약에 대한 연구는 기반 모델(foundation model)에 대한 후속 연구의 시작점을 제공했습니다. 그러나 시간이 지남에 따라 연구자들은 특히 추상적 요약 문제에 덜 집중하기 시작했으며, 대신 더 나은 기반 모델(foundation model)을 만드는 데 집중했습니다. 이러한 모델에게 추상적 요약은 정확하게 해결할 수 있는 많은 작업 중 하나일 뿐입니다. 이제 LLM은 텍스트 요약을 넘어 이미지, 오디오, 비디오 등 다양한 양식의 정보를 요약하는 멀티모달 요약 기능까지 갖추며 그 활용 범위를 넓히고 있습니다.

**GPT-3 시대의 뉴스 요약 및 평가 [6]**
“우리는 최근 뉴스 기사의 새로운 코퍼스(corpus)에 대한 A/B 테스트를 사용하여 이러한 접근 방식을 비교하고, 연구 참가자들이 압도적으로 GPT-3 요약을 선호한다는 것을 발견했습니다.” - 출처 [6]
위에서 언급했듯이, 강력한 LLM의 제안은 기반 모델(foundation model)에 프롬프트(prompt)를 제공하는 것만으로 다양한 작업을 정확하게 해결할 수 있는 AI 연구의 패러다임 전환으로 이어졌습니다. [6]에서 저자들은 이 패러다임 전환이 요약 연구에 미치는 영향을 뉴스 요약에 중점을 두고 연구합니다.
**GPT-3는 얼마나 좋은가?** 요약 전용 데이터로 명시적으로 학습되지 않았음에도 불구하고, [1]에서 우리는 GPT-3가 인간이 선호하는 요약을 작성하는 데 매우 능숙하다는 것을 알 수 있습니다. 심지어 모델에 작업 설명만으로 프롬프트(prompt)를 제공할 때도 그렇습니다(즉, 제로샷(zero-shot) 방식)! 더 나아가, GPT-3는 명시적인 미세 조정(finetuning) 없이 다양한 작업을 해결할 수 있는 방대한 텍스트 코퍼스(corpus)로 학습된 일반 모델이기 때문에 데이터셋 특정 문제(예: 잘못 작성되었거나 부정확한 참조 요약으로부터 학습)로 고통받지 않습니다. 이러한 이유로, GPT-3는 상당한 데이터셋 없이도 새로운 요약 도메인에 자연스럽게 일반화(generalize)됩니다. 우리는 단순히 명령어 또는 몇 가지 퓨샷(few-shot) 예시만 필요합니다. 최신 LLM인 GPT-4나 Gemini 등은 이러한 제로샷/퓨샷 요약 능력을 더욱 극대화하여, 별도의 미세 조정 없이도 탁월한 요약 품질을 보여줍니다.

(출처: [6]) SFT 및 명령어 튜닝(instruction-tuned) 모델과 비교했을 때, GPT-3는 모든 데이터셋에서 인간으로부터 20% 더 높은 점수를 달성하는 요약을 생성하며, 이는 GPT-3에 대한 명확한 선호를 나타냅니다. 그러나 최상의 요약 모델 선택이 30% 미만의 경우에서만 만장일치로 이루어지며, 이는 고품질 요약이 비교하기 어려울 수 있다는 것을 보여줍니다. 즉, "최상의" 모델 선택은 간단하지 않습니다. LLM의 성능이 향상될수록, 인간 평가자들 사이에서도 "최고의" 요약을 가려내는 것이 점점 더 어려워지는 경향이 있습니다.
**자동 지표 분석.** [6]의 저자들은 또한 요약 품질 평가에서 자동 지표의 효과를 광범위하게 분석합니다. 이전 연구는 ROUGE와 같은 자동 지표가 요약 모델 간의 큰 품질 차이를 식별하는 데 유용하지만, 작은 성능 차이를 포착하는 데 어려움을 겪는다는 것을 나타내는 것으로 보입니다(즉, 자동 지표는 미묘한 성능 차이를 가진 모델을 비교하는 데 덜 유용합니다). [6]에서 우리는 이 경험 법칙이 LLM 시대에는 덜 간단하다는 것을 알 수 있습니다. GPT-3 요약은 기준선(baseline)보다 훨씬 낮은 점수를 받습니다(ROUGE에서 7점 차이!). 인간 실험에서는 거의 만장일치로 선호되었음에도 불구하고 말입니다.

(출처: [6]) 이러한 결과는 강력한 요약 모델을 고려할 때 참조 기반(reference-based) 지표가 인간 선호도와 상관관계가 낮다는 추가 증거를 제공합니다. 더 나아가, [6]에서 우리는 GPT-3 요약의 품질과 자동 지표의 한계라는 측면에서 유사한 결과가 특정 키워드 또는 주제에 기반한 안내 요약을 수행하는 키워드 및 측면 기반 요약(keyword and aspect-based summarization)과 같은 더 전문화된 요약 작업에서도 유효하다는 것을 알 수 있습니다. 이는 LLM의 등장으로 인해 전통적인 자동 평가 지표의 재고가 필요하다는 강력한 신호입니다.

**GPT-3.5를 사용한 프롬프트 기반 의견 요약 [7]**
대부분의 논문이 텍스트 요약(예: 뉴스 기사 요약)을 연구하지만, [7]의 저자들은 이 작업이 GPT-3.5로 해결될 수 있는지 확인하기 위해 더 복잡한 의견 요약(opinion summarization) 11 작업을 고려합니다. 왜 의견 요약이 텍스트 요약보다 더 복잡할까요? 몇 가지 이유가 있습니다:
*   다른 의견들이 모순될 수 있으므로, 다양한 관점 12를 정확하게 반영하는 더 미묘한 요약이 필요합니다.
*   요약되는 모든 의견의 길이가 LLM의 맥락 길이(context length)를 초과하는 경우가 많으므로, 데이터를 처리하고 요약하는 파이프라인이 필요합니다.
*   의견 요약 작업은 일반 텍스트 요약보다 덜 널리 탐구되었습니다.
[4]와 유사하게, 재귀적 전략을 사용하여 긴 의견 시퀀스를 요약할 수 있지만, [7]에서는 대안적인 기술도 탐구됩니다. 예를 들어, 의견을 주제별 그룹으로 클러스터링(clustering)하거나 요약에 포함될 가장 중요하거나 (핵심적인) 의견을 자동으로 식별하는 것입니다. 의견 요약은 고객 피드백 분석, 시장 조사 등 다양한 비즈니스 애플리케이션에서 중요성이 커지고 있습니다.

(출처: [7]) **요약 파이프라인.** [7]의 저자들은 다양한 의견 요약 파이프라인을 탐구하지만, 세 가지 주요 접근 방식이 강조됩니다:
*   텍스트를 재귀적으로 덩어리화하고 요약하여 최종 출력을 생성하는 계층적 접근 방식(즉, 반복 요약과 함께 덩어리화).
*   QFSumm이라는 추출적 요약 모델을 사용하여 요약 전에 가장 중요한 리뷰를 식별하는 사전 추출 전략.
*   주제 또는 평점에 따라 리뷰를 클러스터(cluster)로 분리하고, 각 클러스터(cluster)를 재귀적으로 요약한 다음, 최종 요약을 생성하는 클러스터링(clustering) 접근 방식.
[7]에서 우리는 GPT-3.5가 짧은 리뷰 시퀀스에 대해 기본적인 재귀적 요약을 통해 유용한 요약을 생성한다는 것을 알 수 있습니다. 그러나 더 긴 시퀀스를 처리할 때, 반복적인 덩어리화 및 요약은 요약 품질을 저하시킬 수 있다는 것을 알 수 있습니다. 위에서 설명한 사전 추출 및 클러스터링(clustering) 전략을 통해 이 문제를 완화할 수 있습니다. 이러한 기술은 최종 출력을 생성하는 데 필요한 재귀적 요약 단계 수를 줄이는 경향이 있기 때문입니다. 이러한 파이프라인 접근 방식은 최근 RAG 시스템이나 AI 에이전트 워크플로우에 통합되어 복잡한 요약 작업을 더욱 효과적으로 처리하고 있습니다.

(출처: [7]) 이러한 전략으로 생성된 의견 요약의 예시(SPACE 호텔 의견 요약 데이터셋에 대한)는 위 그림에 나와 있습니다. 보시다시피, 주제별 클러스터링(clustering)은 다양한 의견에 대한 높은 수준의 시각을 제공하는 더 추상적인 요약을 생성하는 것으로 보이는 반면, 추출적 전략은 각 입력 리뷰 내에서 언급된 특정 요점에 더 중점을 둡니다. 의견 요약에서는 상충되는 견해를 균형 있게 제시하면서도 사실성을 유지하는 것이 중요한 과제입니다.

**인간 평가(Human evaluation).** 위에 설명된 요약 파이프라인 외에도, [7]의 저자들은 각 파이프라인 구성 요소를 모듈화하고 플러그 앤 플레이(plug-and-play) 접근 방식을 사용하여 다양한 파이프라인을 탐구합니다. 이러한 파이프라인은 SPACE 및 FewSum (Amazon 및 Yelp 리뷰 포함) 의견 요약 데이터셋에서 요약을 생성하는 데 사용되며, 생성된 요약의 사실성(factuality), 충실도(faithfulness), 관련성(relevance), 대표성(representativeness)에 기반하여 평가됩니다.

(출처: [7]) 위에서 볼 수 있듯이, 최상의 요약 파이프라인은 평가되는 데이터셋과 속성에 따라 다릅니다. 그러나 GPT-3.5로 생성된 요약은 자동 지표에서는 낮은 성능을 보였음에도 불구하고 인간 평가 측면에서 기준선(baseline) 기술보다 훨씬 우수한 성능을 보입니다. 따라서 [7]에서 제안된 요약 파이프라인은 이전 최첨단 기술에 대한 발전으로 입증됩니다. 이는 복잡한 요약 작업에서 인간 평가의 중요성을 다시 한번 강조합니다.

### 뉴스 요약을 위한 대규모 언어 모델 벤치마킹 [8]

“우리는 모델 크기가 아니라 명령어 튜닝(instruction tuning)이 LLM의 제로샷(zero-shot) 요약 능력의 핵심이라는 것을 발견했습니다.” - 출처 [8]
이 시점에서 우리는 LLM이 텍스트를 정말 잘 요약하며, 대부분의 경우 이전 최첨단 기술보다 우수한 성능을 보인다는 것을 알고 있습니다. 하지만 왜 그럴까요? 요약 작업에서 LLM의 성공의 근간이 되는 설계 결정은 잘 이해되지 않습니다. [8]에서 저자들은 요약 작업에서 여러 사전 학습 방법, 프롬프트(prompt) 및 모델 규모(model scale)를 포함하는 10가지 다른 LLM에 대한 광범위한 인간 평가를 수행합니다. 이러한 실험을 통해, 우리는 명령어 튜닝(instruction tuning)이 LLM을 효과적인 요약기로 만드는 핵심 구성 요소라는 것을 분명히 알 수 있습니다. 이는 SFT, 명령어 튜닝, RLHF를 결합한 "모델 레시피"가 다양한 작업을 잘 수행하는 범용 LLM을 만드는 데 얼마나 중요한지를 보여줍니다.

(출처: [8]) **저품질 참조 요약.** [8]에서 수행된 대부분의 평가는 CNN / DailyMail 및 XSum 데이터셋에서 이루어졌습니다. 흥미롭게도, 저자들은 이 데이터셋의 대부분의 참조 요약이 인간에 의해 저품질로 판단된다는 것을 보여주며, 이는 기존 요약 연구가 일반적인 데이터셋에 존재하는 저품질 참조 요약으로 인해 제한된다는 것을 나타냅니다. 실제로, [8]의 저자들은 참조 기반(reference-based) 평가 지표(예: ROUGE)와 인간 선호도 간의 낮은 상관관계가 저품질 참조 요약으로 인해 악화되며, 이는 다양한 이전 요약 연구에 의문을 제기합니다.
“참조 요약의 품질 문제를 해결하고 LLM이 인간 요약 작성자와 어떻게 비교되는지 더 잘 이해하기 위해, 우리는 프리랜서 작가를 모집하여 CNN/DM 및 XSUM의 테스트 세트에서 100개 기사를 재주석했습니다.” - 출처 [8]
저품질 참조 요약 문제를 해결하기 위해, CNN / DailyMail 및 XSum 데이터셋에서 새로 구성된 100개 예시 세트가 인간에 의해 재주석되었습니다. 이러한 고품질 참조 요약은 요약 작업 전반에 걸쳐 LLM 성능을 인위적으로 저하시키지 않는 더 신뢰할 수 있는 평가에 사용될 수 있습니다. 고품질 벤치마크 데이터의 구축은 LLM 연구의 신뢰성을 높이는 데 필수적입니다.

(출처: [8]) **실험 설정.** 요약 품질 측정을 위해 10가지 다른 LLM이 고려됩니다. 모델은 아래에 표시된 기본 템플릿을 사용하여 제로샷(zero-shot) 또는 퓨샷(five-shot) 프롬프트(prompt)로 평가됩니다.
“기사: [기사]. 기사를 세 문장으로 요약하세요. 요약:”
다시 한번, 저자들은 기존 요약 데이터셋에 존재하는 참조 요약의 낮은 품질을 강조합니다. 실제로, 참조 요약을 인컨텍스트 학습(in-context learning) 예시로 사용하는 것이 LLM 성능을 저하시키는 것으로 나타났습니다! LLM이 생성한 출력 요약의 몇 가지 질적 예시는 아래 그림에 나와 있습니다. 최근에는 정교한 프롬프트 엔지니어링 기술이 LLM의 제로샷 요약 성능을 더욱 향상시키는 데 기여하고 있습니다.

(출처: [8]) **주요 시사점.** [8]의 실험 분석에서 얻을 수 있는 주요 시사점은 두 가지입니다:
*   명령어 튜닝(instruction tuning)은 요약 성능에 분명히 이점을 줍니다.
*   LLM이 생성한 요약은 본질적으로 추출적인 반면, 인간이 작성한 요약은 더 많은 추상화 또는 의역(paraphrasing)을 포함하는 경향이 있습니다.

명령어 튜닝(instruction tuning)을 거친 LLM은 그렇지 않은 LLM보다 분명히 우수한 성능을 보이며, 이는 자기 지도 사전 학습(self-supervised pretraining)만으로는 경쟁력 있는 요약 결과를 얻기에 충분하지 않다는 것을 나타냅니다. 또한, LLM이 생성한 요약은 원본 기사에서 정보를 직접 복사하는 경향이 있지만, 복사된 정보는 일관된 방식으로 합성됩니다. 대조적으로, 인간이 작성한 요약은 원본 자료에서 복사하기보다는 정보를 의역(paraphrase)하는 경향이 있습니다. 추출적 특성에도 불구하고, LLM이 생성한 요약은 인간 평가 실험에서 비교했을 때 인간이 작성한 요약과 동등하게 선호됩니다. LLM이 생성하는 "추출적이면서도 일관된" 요약은 사실 전달에 있어 환각 위험을 줄여주기 때문에 많은 응용 분야에서 선호될 수 있습니다.

**ChatGPT 대 인간 작성 텍스트 [9]**
“인간이 생성한 스타일 변형은 ChatGPT가 보여준 것보다 훨씬 더 크며, 생성된 텍스트는 단어 유형의 분포와 같은 여러 특성에서 인간 샘플과 다릅니다.” - 출처 [9]
흥미로운 요약 형태 중 하나는 제어 가능한 요약(controllable summarization)입니다. 이 방식에서는 모델에 특정 독자를 대상으로 하는 요약을 작성하도록 지시합니다. [9]에서 저자들은 전문가와 비전문가 모두를 위한 과학 정보 요약에서 GPT-3.5-Turbo의 성능을 연구하며, LLM으로 제어 가능한 요약(controllable summarization)을 수행할 때 직면하는 행동 차이(인간이 작성한 요약과 비교), 한계 및 과제를 식별하는 것을 목표로 합니다. 제어 가능한 요약은 개인화된 콘텐츠 생성, 특정 대상 독자를 위한 정보 전달 등 다양한 분야에서 중요성이 커지고 있습니다.

**제어 가능한 요약을 위한 프롬프트**
**독자 지정.** LLM으로 제어 가능한 요약(controllable summarization)을 수행하려면, 단순히 프롬프트(prompt)를 수정해야 합니다. [9]의 경우, 우리는 명령어 내에서 요약을 작성하려는 독자를 지정할 수 있습니다. 이러한 프롬프트는 eLife 데이터셋의 과학 문헌을 요약하는 데 사용되며, 평가를 위해 데이터셋에서 500개의 무작위 샘플이 추출됩니다. LLM은 모든 요약을 생성하기 위해 제로샷(zero-shot) 방식으로 프롬프트(prompt)를 받습니다. 최근에는 "페르소나 프롬프팅"이나 "스타일 전이" 기술을 활용하여 LLM의 제어 가능한 요약 능력을 더욱 정교하게 만들고 있습니다.
**평가 지표.** GPT-3.5-Turbo가 생성된 요약을 지정된 대상 독자에게 충분히 조정할 수 있는지 확인하기 위해, 여러 자동 지표가 평가에 사용됩니다:
*   **플레시 읽기 용이성(Flesch Reading Ease)**: 단어당 평균 음절 수와 문장당 평균 단어 수를 통해 가독성을 측정합니다. 점수가 높을수록 이해하기 쉽습니다.
*   **콜먼-리아우 지수(Coleman-Liau Index)**: 문장당 평균 문자 수와 100단어당 평균 문장 수를 측정하여 텍스트 난이도를 포착합니다. 점수가 높을수록 이해하기 더 어렵습니다.
*   **데일-챌 가독성 점수(Dale-Chall Readability Score)**: 텍스트의 복잡한 단어 수를 일반적인 단어 목록과 비교합니다. 점수가 높을수록 이해하기 더 어렵습니다.
이러한 가독성 지표 외에도 ROUGE 점수와 n-그램(n-gram) 참신성 13이 측정되며, 모델이 생성한 요약 내의 사실적 불일치를 감지하는 SummaC 및 개체명 환각(named entity hallucination)과 같은 지표도 측정됩니다.

(출처: [9]) **주요 시사점.** [9]의 분석은 우리에게 두 가지 핵심 정보를 제공합니다:
*   GPT-3.5-Turbo는 특정 독자에게 출력을 맞추는 데 인간보다 못합니다.
*   모델이 생성한 요약은 더 추출적이며 일반적으로 환각(hallucination)을 포함하는 경향이 있습니다.
위 표에서 볼 수 있듯이, GPT-3.5-Turbo는 비전문가 요약의 경우 [31, 38] 범위, 전문가 요약의 경우 [28, 37] 범위 내의 플레시 읽기 용이성(Flesch Reading Ease) 점수를 달성하는 요약을 생성합니다. 대조적으로, 인간이 작성한 비전문가 및 전문가 요약은 각각 평균 플레시 읽기 용이성(Flesch Reading Ease) 점수 53.1 및 22.5를 달성합니다. 즉, 가독성 점수 차이의 크기가 인간과 모델이 작성한 요약 간에 극적으로 다릅니다. 퓨샷(few-shot) 예시를 제공하면 이 문제를 완화할 수 있지만, GPT-3.5-Turbo는 그럼에도 불구하고 제어 가능한 요약 작업에서 인간보다 덜 효과적인 것으로 나타났습니다.

(출처: [9]) GPT-3.5-Turbo로 생성된 요약은 인간이 작성한 요약보다 n-그램(n-gram) 참신성도 낮으며, 이는 모델이 작성한 요약이 본질적으로 더 추출적이라는 것을 나타냅니다. 또한 GPT-3.5-Turbo로 생성된 요약은 요약과 원본 자료 간의 개체(entity) 및 주제 중복으로 측정했을 때 잦은 환각(hallucination)을 보이는 경향이 있습니다. 이러한 모든 결과는 GPT-3.5-Turbo가 제어 가능한 요약(controllable summarization) 영역에서 개선의 여지가 있음을 나타내는 것으로 보이지만, 더 최신 모델(예: GPT-4o)은 더 나은 성능을 보일 가능성이 있다는 점에 유의해야 합니다. 실제로 최신 LLM들은 스타일 제어와 사실적 정확성 면에서 비약적인 발전을 이루었습니다.

### 결론

우리는 이제 지도 미세 조정(supervised finetuning)에서 선호도 튜닝(preference tuning), 현대 기반 모델(foundation model)에 이르기까지 여러 세대의 요약 연구를 살펴보았습니다. 이 작업에서 몇 가지 공통 주제가 나타납니다:
*   평가의 어려움 (및 중요성).
*   인간 피드백으로부터 학습의 가치.
*   데이터 품질의 중요성.
*   현대 기반 모델(foundation model)의 인상적인 능력.
이 개요는 특히 요약 연구에 중점을 두었지만, 이 작업의 발견은 매우 일반화(generalize) 가능합니다. 요약은 조건부 생성(conditional generation)(즉, 특정 입력이 주어졌을 때 출력을 생성하도록 모델 교육)에 뿌리를 두고 있기 때문에 자연어 처리(natural language processing)의 근본적인 작업입니다. 기계 번역, 텍스트 분류, 키워드 추출, 질의응답 등 다른 많은 중요한 작업들도 매우 유사한 패턴을 따릅니다! 요약 연구에 대한 깊은 이해는 훨씬 더 광범위한 문제를 해결하는 데 도움이 됩니다.
“우리는 인간 피드백 기반 강화 학습(RLHF; Stiennon et al., 2020)을 사용하여 GPT-3를 미세 조정(fine-tune)하여 광범위한 서면 명령어를 따르도록 합니다.” - 출처 [5]
자연어 처리(natural language processing) 연구 내에서 요약의 근본적인 역할은 이러한 기술들이 LLM 시대에 크게 채택되도록 이끌었습니다. 언어 모델링 연구의 다양한 영향력 있는 접근 방식은 요약 논문에 크게 영향받았습니다! 예를 들어, InstructGPT는 인간 피드백으로부터 더 나은 요약 모델을 학습시키기 위해 이전에 제안된 학습 알고리즘 [2]을 채택하는 반면, 현대 정렬(alignment) 절차는 이전 요약 연구 [3]에서 옹호된 반복적인 미세 조정(finetuning) 전략을 사용합니다. 이 개요에서 제시된 연구는 실용적으로 유용합니다. 그러나 더 중요한 것은 오늘날 LLM에서 볼 수 있는 발전을 위한 토대를 마련한다는 것입니다. 미래에는 요약 기술이 AI 에이전트, 실시간 정보 처리, 개인화된 콘텐츠 생성 등 다양한 혁신적인 애플리케이션의 핵심 구성 요소로 더욱 발전할 것입니다.

뉴스레터가 처음이신가요? 안녕하세요! 저는 Cameron R. Wolfe입니다. 넷플릭스(Netflix)의 딥러닝(Deep Learning) 박사이자 머신러닝(Machine Learning) 과학자입니다. 이것은 딥(러닝) 포커스 뉴스레터이며, 독자들이 AI 연구의 중요한 주제를 더 잘 이해하도록 돕습니다. 뉴스레터가 마음에 드셨다면, 구독, 공유 또는 X 및 LinkedIn에서 저를 팔로우해주세요! 구독

**참고 문헌**
[1] Böhm, Florian, et al. "Better rewards yield better summaries: Learning to summarise without references." arXiv preprint arXiv:1909.01214 (2019).
[2] Stiennon, Nisan, et al. "Learning to summarize with human feedback." Advances in Neural Information Processing Systems 33 (2020): 3008-3021.
[3] Ziegler, Daniel M., et al. "Fine-tuning language models from human preferences." arXiv preprint arXiv:1909.08593 (2019).
[4] Wu, Jeff, et al. "Recursively summarizing books with human feedback." arXiv preprint arXiv:2109.10862 (2021).
[5] Ouyang, Long, et al. "Training language models to follow instructions with human feedback." Advances in neural information processing systems 35 (2022): 27730-27744.
[6] Goyal, Tanya, Junyi Jessy Li, and Greg Durrett. "News summarization and evaluation in the era of gpt-3." arXiv preprint arXiv:2209.12356 (2022).
[7] Bhaskar, Adithya, Alexander R. Fabbri, and Greg Durrett. "Prompted opinion summarization with GPT-3.5." arXiv preprint arXiv:2211.15914 (2022).
[8] Zhang, Tianyi, et al. "Benchmarking large language models for news summarization." Transactions of the Association for Computational Linguistics 12 (2024): 39-57.
[9] Pu, Dongqi, and Vera Demberg. "ChatGPT vs human-authored text: Insights into controllable text summarization and sentence style transfer." arXiv preprint arXiv:2306.07799 (2023).
[10] Menick, Jacob, et al. "Teaching language models to support answers with verified quotes." arXiv preprint arXiv:2203.11147 (2022).
[11] Fabbri, Alexander R., et al. "Summeval: Re-evaluating summarization evaluation." Transactions of the Association for Computational Linguistics 9 (2021): 391-409.
[12] Subramanian, Sandeep, et al. "On extractive and abstractive neural document summarization with transformer language models." arXiv preprint arXiv:1909.03186 (2019).
[13] Zhang, Fang-Fang, Jin-ge Yao, and Rui Yan. "On the abstractiveness of neural document summarization." Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing . 2018.
[14] Kryściński, Wojciech, et al. "Evaluating the factual consistency of abstractive text summarization." arXiv preprint arXiv:1910.12840 (2019).
[15] Maynez, Joshua, et al. "On faithfulness and factuality in abstractive summarization." arXiv preprint arXiv:2005.00661 (2020).
[16] Durmus, Esin, He He, and Mona Diab. "FEQA: A question answering evaluation framework for faithfulness assessment in abstractive summarization." arXiv preprint arXiv:2005.03754 (2020).
[17] Wang, Alex, Kyunghyun Cho, and Mike Lewis. "Asking and answering questions to evaluate the factual consistency of summaries." arXiv preprint arXiv:2004.04228 (2020).
[18] Brown, Tom, et al. "Language models are few-shot learners." Advances in neural information processing systems 33 (2020): 1877-1901.
[19] Wei, Jason, et al. "Finetuned language models are zero-shot learners." arXiv preprint arXiv:2109.01652 (2021).
[20] Nallapati, Ramesh, et al. "Abstractive text summarization using sequence-to-sequence rnns and beyond." arXiv preprint arXiv:1602.06023 (2016).
[21] Narayan, Shashi, Shay B. Cohen, and Mirella Lapata. "Don't give me the details, just the summary! topic-aware convolutional neural networks for extreme summarization." arXiv preprint arXiv:1808.08745 (2018).
[22] Völske, Michael, et al. "Tl; dr: Mining reddit to learn automatic summarization." Proceedings of the Workshop on New Frontiers in Summarization . 2017.
[23] Kryściński, Wojciech, et al. "Neural text summarization: A critical evaluation." arXiv preprint arXiv:1908.08960 (2019).
[24] He, Tingting, et al. "ROUGE-C: A fully automated evaluation method for multi-document summarization." 2008 IEEE International Conference on Granular Computing . IEEE, 2008.
[25] Papineni, Kishore, et al. "Bleu: a method for automatic evaluation of machine translation." Proceedings of the 40th annual meeting of the Association for Computational Linguistics. 2002.
[26] Zhang, Tianyi, et al. "Bertscore: Evaluating text generation with bert." arXiv preprint arXiv:1904.09675 (2019).
[27] Zhao, Wei, et al. "MoverScore: Text generation evaluating with contextualized embeddings and earth mover distance." arXiv preprint arXiv:1909.02622 (2019).
[28] Zheng, Lianmin, et al. "Judging llm-as-a-judge with mt-bench and chatbot arena." Advances in Neural Information Processing Systems 36 (2024).
[29] Liu, Yang, et al. "Gpteval: Nlg evaluation using gpt-4 with better human alignment." arXiv preprint arXiv:2303.16634 (2023).

1 "발현적 능력(emergent capability)"이란 특정 규모(데이터 또는 컴퓨팅 측면에서)가 탐색된 후에야 나타나는 모델의 능력을 의미합니다.
2 일반적으로 지도 미세 조정(supervised finetuning)과 선호도 튜닝(preference tuning)의 조합을 통해 다루어지는 정렬(Alignment)은 인간 사용자의 욕구에 더 잘 부합하는 출력을 생성하도록 LLM을 미세 조정(finetuning)하는 과정을 의미합니다. 자세한 내용은 여기를 참조하십시오.
3 더 구체적으로, 현대 LLM의 출력은 자동으로 유창하고 간단한 문법/구문 오류가 없는 경향이 있으므로, 유창성 평가는 그러한 모델에게는 논쟁의 여지가 있지만 불필요합니다.
4 2년 이상 데이터 주석(annotation) 회사에서 일한 후, 저는 많은 수의 인간이 많은 양의 데이터를 정확하고 신뢰할 수 있게 주석/채점하는 것의 어려움에 정통합니다!
5 여기서 우리는 학습 중에 사용된 각 예시에는 참조 요약이 있다고 가정합니다. 그런 다음, 우리는 모델로 요약 출력을 계산하고, 참조 요약과 모델의 출력을 사용하여 ROUGE 점수를 찾은 다음, ROUGE 점수를 RL을 통한 학습을 위한 보상 신호(reward signal)로 사용할 수 있습니다.
6 기억하십시오, LLM은 출력하는 각 토큰(token)에 확률을 할당하여 작동합니다. 우리는 인간이 작성한 요약에 할당된 확률이 높기를 원합니다. 이러한 요약은 LLM이 생성할 수 있는 합리적인 출력을 나타내기 때문입니다.
7 모드 붕괴(Mode collapse)는 LLM이 출력 다양성을 잃고 특정 스타일(또는 스타일 세트)을 가진 좁은 범위의 출력만 생성하기 시작하는 현상을 의미합니다.
8 드리프트(Drift)는 단순히 미세 조정(finetune)된 모델이 일부 참조 모델(예: 미세 조정(finetuning) 과정 이전의 모델)과 너무 달라지는 것을 의미합니다.
9 [3]의 저자들은 깊이 3으로 책 요약 작업을 분해함으로써, 수천 단어의 책을 요약하면서 요약 작업의 주석(annotation) 비용을 50배 절감할 수 있다고 언급합니다.
10 이러한 프롬프트의 대부분은 OpenAI API에서 가져온 것이므로, 학습에 사용된 프롬프트와 실제 사용 사례 간에 충분한 중복이 보장됩니다.
11 의견 (또는 리뷰) 요약(Opinion (or review) summarization)은 많은 사용자가 특정 제품 또는 서비스에 대해 남긴 의견 또는 리뷰(예: 제품 리뷰 또는 식당에 대한 게시된 의견)를 요약하는 작업을 의미합니다.
12 이러한 이유로, 원본에서 텍스트를 직접 복사하는 추출적 스타일 요약은 텍스트 요약을 위한 효과적인 접근 방식임에도 불구하고 의견 요약에서는 성능이 좋지 않은 경향이 있습니다.
13 N-그램(N-gram) 참신성은 요약 내에서 생성되었지만 원본 텍스트에는 없는 n-그램(n-gram)의 비율을 의미합니다.