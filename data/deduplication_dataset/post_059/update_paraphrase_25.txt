대규모 언어 모델(LLM)의 발전과 요약 기술의 새로운 지평

(출처: [2, 4, 6, 10, 29]) 최근 몇 년간 대규모 언어 모델(LLM)의 역량은 놀라운 속도로 진화해왔습니다. 하지만 LLM을 구성하는 핵심 개념들, 예를 들어 자기 지도 사전 학습(self-supervised pretraining), 트랜스포머(transformer) 아키텍처, 그리고 인간 피드백 기반 학습(learning from human feedback) 등은 이미 수년 전부터 자연어 처리(natural language processing) 분야 연구의 중요한 축을 이루고 있었습니다. 이 아이디어들은 완전히 새로운 것이 아니라, 지난 10년 이상 관련 연구에서 꾸준히 축적되어 온 지식의 결과물입니다. 따라서 기계 번역, 텍스트 요약, 질의응답과 같은 자연어 처리의 주요 문제들에 대한 기초 연구는 여전히 지극히 중요합니다! 이 글에서는 LLM 연구의 발전에 큰 영향을 미친 (추상적) 텍스트 요약((abstractive) text summarization)이라는 주제를 중심으로 이 점을 깊이 있게 다룰 것입니다.

"텍스트 요약은 긴 원문(들)의 핵심 정보를 보존하면서 짧고 유창하며 사람이 읽기 쉬운 형태로 압축하는 것을 목표로 합니다." - 출처 [11]

간단히 말해, 요약은 긴 텍스트 시퀀스에서 핵심 정보를 추출하거나 재구성하여 훨씬 짧은 시퀀스로 압축하는 과정을 의미합니다. 이 작업은 일반적으로 뉴스 기사 요약과 같은 표준적인 상황에 적용되지만, 그 적용 범위는 매우 광범위하며 다음과 같은 흥미로운 활용 사례를 포함할 수 있습니다:

*   검색 증강 생성(RAG) 시스템에서 방대한 비정형 텍스트를 전처리하여 핵심 내용을 압축하는 단계.
*   추천 시스템의 복잡한 결과물을 사용자가 쉽게 이해할 수 있는 텍스트 형태로 요약하여 제공.
*   회의 녹취록을 분석하여 핵심 의제와 결정 사항을 담은 회의록 자동 생성.

이처럼 요약 작업은 매우 보편적이고 강력한 기능을 제공하기 때문에, 실용적으로 유용한 수많은 연구가 이 분야에서 활발히 진행되어 왔습니다. 우리가 앞으로 살펴보겠지만, 요약 연구에서 발전된 많은 핵심 아이디어들이 현대 LLM에 성공적으로 통합되었습니다. 이 두 연구 분야는 서로 긴밀하게 연결되어 있으며, 요약 연구에 대한 깊이 있는 이해는 LLM이 왜, 그리고 어떻게 그렇게 뛰어난 성능을 발휘하는지에 대한 새롭고 심층적인 통찰력을 제공할 것입니다!

**요약에 대한 필수 기본 지식**

최근 요약 연구의 동향을 탐구하기 전에, 먼저 기본적인 개념들을 확립하는 것이 중요합니다. 이 부분에서는 요약 작업의 전반적인 개요, 다양한 요약 방식, LLM 시대 이전의 요약 연구 흐름, 그리고 요약 품질 평가 지표 등에 대해 깊이 있게 살펴보겠습니다.

**요약의 분류**

**추상적 요약(Abstractive summarization) 대 추출적 요약(extractive summarization)**

요약의 본질적인 목표는 원래의 긴 문서에서 핵심 아이디어를 포착하는 더 짧은 텍스트를 만들어 정보를 가장 중요한 요소들로 응축하는 것입니다. 문헌에서는 주로 두 가지 주요 요약 기법이 연구되고 있습니다 (그림은 위 참조):

*   **추출적 요약(Extractive)**: 원본 문서에서 이미 존재하는 문장이나 텍스트 구간을 선택적으로 복사하여 요약을 구성하는 방식입니다. 이는 마치 문서에서 가장 중요한 문장들을 골라내는 것과 같습니다.
*   **추상적 요약(Abstractive)**: 원본 문서의 정보를 독자적인 표현으로 재구성하여, 관련 내용에 대한 더 간결하고 새로운 설명을 생성하는 방법입니다. 이는 마치 사람이 문서를 읽고 자신의 언어로 다시 요약하는 것과 유사합니다.

이 두 가지 요약 전략은 광범위하게 탐구되었지만, LLM이 생성하는 요약은 대체로 추상적 요약으로 분류됩니다. 왜 그럴까요? 일반적으로 LLM은 원본 문서에서 문장을 직접적으로 복사하도록 설계되지 않습니다. 모델은 제공된 맥락(context)을 바탕으로 자유롭게 텍스트를 생성하며, 원본 문서의 정보를 유연하게 재구성하여 추상적인 요약을 만들어낼 수 있습니다. 그러나 흥미롭게도, LLM으로 만들어진 요약 중 상당수는 실제로는 (상대적으로) 추출적인 경향을 보입니다. 또한, 최근 연구들은 i) 추출적 요약 기법을 활용하여 추상적 요약 모델 [7, 12]의 입력으로 포함할 핵심 맥락(context)을 찾아내거나, 심지어 ii) LLM이 응답을 생성할 때 관련 출처(또는 특정 텍스트 구간)를 명시적으로 인용하도록 가르치는 [10] 등 다양한 관련 주제를 탐구하기 시작했습니다 (그림은 아래 참조). 이러한 이유로, 비록 사용되는 기법에 차이가 있을지라도, 추출적 요약과 추상적 요약 사이에는 상당한 중복 영역이 존재합니다.

(출처: [10]) **추가 학습 자료.** 이 게시물은 LLM을 활용한 요약에 초점을 맞추므로, LLM이 대중화되기 전의 추상적 및 추출적 요약 기법에 대해 깊이 다루지는 않을 것입니다. 그러나 그러한 초기 연구들은 현재 다루는 작업의 중요한 토대를 마련했으므로 여전히 매우 관련성이 높습니다. 관심 있는 독자들을 위해 엄선된 논문 목록을 아래에 제공합니다:

*   **SummEval: 요약 평가 재평가 [11]**: 이 연구는 요약을 위한 자동 평가 지표들을 심층적으로 분석하고, 기존 평가 방식의 한계를 탐색하며, 제안된 개선 사항들을 적용하여 24개의 추상적 및 추출적 요약 모델들을 재평가합니다.
*   **신경 문서 요약의 추상성에 대하여 [13]**: 이 연구는 추상적 요약 기법들을 면밀히 조사하며, 많은 추상적 요약들이 실제로는 본질적으로 매우 추출적이라는 흥미로운 사실을 발견합니다!
*   **추상적 요약의 충실도와 사실성 [15]**: 이 연구는 추상적 요약 기법의 충실도(faithfulness)와 사실성(factuality)에 대한 광범위한 분석을 제공합니다. 유사한 분석은 [14, 16, 17]에서도 찾아볼 수 있습니다.

주목할 만한 추출적 요약 기법으로는 NEUSUM, BanditSum, LATENT, REFRESH, RNES, JECS, STRASS 등이 있습니다. 추출적 요약 기법에 대한 더 포괄적인 조사는 [여기](https://arxiv.org/pdf/1908.08960.pdf)를 참조하십시오.

주목할 만한 (LLM 이전) 추상적 요약 기법으로는 Pointer Generator, Fast-abs-rl, Bottom-Up, ROUGESal, Soft-MT, SENECA, T5, BertSum, Pegasus, BART, UniLM 등이 있습니다. 추상적 요약 기법에 대한 더 포괄적인 조사는 [여기](https://arxiv.org/pdf/1908.08960.pdf)를 참조하십시오.

또한, LLM에 대한 초기 연구(예: GPT-2 및 GPT-3)의 대부분은 모델의 품질을 평가하기 위한 핵심 작업으로 추상적 요약을 활용했습니다.

**LLM을 활용한 요약 생성 기법**

(출처: [6]) 이 게시물의 나머지 부분에서 확인하듯이, LLM은 고품질의 추상적 요약을 생성하는 데 매우 효과적인 도구입니다. 요약을 위해 LLM을 사용하는 방법은 몇 가지 접근 방식으로 나눌 수 있습니다 (위 참조). 크게 보면 이러한 접근 방식은 다음 중 하나에 해당합니다:

*   오직 인컨텍스트 학습(in-context learning)만을 사용하는 경우
*   요약 전용 데이터로 모델을 미세 조정(finetune)하는 경우

만약 즉시 사용 가능한 모델(out-of-the-box model)이 우리 애플리케이션에 만족스러운 성능을 보인다면, 인컨텍스트 학습(in-context learning)이 가장 간편한 방법일 것입니다. 우리는 단순히 API를 호출하고(프롬프트에 필요한 맥락(context)을 추가하여) 요약을 생성할 수 있습니다. 그러나 프롬프트(prompt)만으로는 충분치 않다면, 지도 학습 방식(supervised fashion)이나 선호도 튜닝(preference tuning)을 활용하여 맞춤형 모델을 미세 조정(finetuning)하는 방안을 고려할 수 있습니다. 이러한 각 기술의 기본 원리는 아래에서 자세히 설명합니다.

(출처: [18]) **인컨텍스트 학습(In-context learning)**은 하나의 기본 LLM이 프롬프트 내에 제공된 정보를 활용하여 다양한 후속(downstream) 작업을 정확하게 해결할 수 있는 능력을 지칭합니다 (위 참조). 예를 들어, 요약 작업의 경우, 여러 기사와 그에 해당하는 요약들을 프롬프트 안에 배치하여 모델이 최종 요약을 생성할 때 이들을 맥락(context)으로 활용하도록 할 수 있습니다. 미세 조정(finetuning)과는 다르게, 인컨텍스트 학습(in-context learning) 중에는 모델의 매개변수(parameter)가 업데이트되지 않습니다. 대신, 우리는 프롬프트에 주어진 맥락(context)을 적절히 사용하여 문제를 해결하는 모델의 내재된 능력에 의존하게 됩니다. 인컨텍스트 학습(in-context learning)은 GPT-3 [18]의 등장과 함께 처음 관찰된 LLM의 발현적 능력(emergent capability) 1입니다. 위 그림에서 볼 수 있듯이, LLM은 특정 규모에서 더욱 유능한 퓨샷 학습자(few-shot learner)가 되는데, 이는 더 큰 모델이 프롬프트의 맥락(context)을 더 효과적으로 활용하여 문제를 해결할 수 있음을 의미합니다. 인컨텍스트 학습(in-context learning)을 사용하려면, 우리는 간단한 프롬프트(위에서 예시된 것과 같은)로 독점 API(예: OpenAI 또는 Anthropic)를 호출하여 요약을 생성할 수 있습니다.

(출처: [6]) **지도 미세 조정(Supervised finetuning).** 만약 프롬프트(prompt)만으로는 충분한 품질의 요약을 얻기 어렵다면, 다음 단계는 고품질 요약 데이터셋을 활용하여 사전 학습된 LLM을 미세 조정(finetune)하는 것입니다. 이를 위해 우리는 해결하고자 하는 작업과 관련된 잘 작성된 문서-요약 쌍 데이터셋을 수집합니다 (이는 인간 주석자(annotator)를 통해 수동으로 이루어지거나, 강력한 LLM을 활용하여 합성적으로 생성될 수 있습니다).

“대규모 언어 모델의 사전 학습(pretraining)은 고성능 달성을 위해 점점 더 널리 보급되고 있습니다… [이 모델들은] 인간의 시연(demonstration) 세트에서 로그 확률(log probability)을 최대화하기 위해 지도 학습(supervised learning)을 사용하여 미세 조정(fine-tuned)됩니다.” - 출처 [2]

예를 들어, 식당 리뷰를 요약하고 싶다면, 각 식당 리뷰와 그에 대한 선별된 요약을 포함하는 데이터셋을 구축해야 합니다. 사전 학습된 기본 모델에서 시작하여, 수집된 데이터에 대해 표준 언어 모델링 목표를 사용하여 이 모델을 미세 조정(finetune)할 수 있습니다. 이러한 방식을 **지도 미세 조정(SFT)**이라고 합니다. 대규모 사전 학습과 요약 전용 데이터에 대한 지도 미세 조정(supervised finetuning)의 결합은 매우 효과적이며, 기본 LLM의 대중화 이전에는 요약 모델을 훈련하는 가장 일반적인 접근 방식이었습니다.

(출처: [19]) **명령어 튜닝(Instruction tuning).** 요약 데이터에 대한 지도 학습을 넘어, 우리는 범용 명령어 튜닝(instruction tuning)을 수행할 수 있습니다. 이 방식에서는 작업 템플릿(위 참조)을 활용하여 여러 종류의 작업 데이터를 가지고 단일 모델을 동시에 미세 조정(finetune)합니다. 특히, 요약은 명령어 튜닝(instruction tuning) 과정에 포함될 수 있는 다양한 작업 중 하나이며, 이렇게 훈련된 모델은 여러 작업을 정확하게 해결하는 데 활용될 수 있습니다. 실제로, 명령어 튜닝(instruction-tuned)된 모델은 훈련 중에 접하지 못한 새로운 작업에도 뛰어난 일반화(generalize) 능력을 보이는 것으로 나타납니다 (아래 참조).

(출처: [19]) **인간 피드백(선호도 튜닝)(preference tuning).** 마지막으로, 우리는 인간의 피드백을 직접적으로 활용하여 요약 모델을 미세 조정(finetune)할 수 있습니다. 일반적으로 선호도 튜닝(preference tuning)으로 불리는 이 접근 방식은 인간 피드백 기반 강화 학습(RLHF)을 통한 LLM 정렬(alignment)이라는 후속 연구들의 중요한 기반을 마련했습니다. 실제로 InstructGPT [5](ChatGPT의 전신)는 인간 피드백을 통한 요약 학습 연구 [2]를 그들의 정렬(alignment) 전략 2에 대한 영감으로 언급합니다! 따라서 이 개요에서는 선호도 튜닝(preference tuning) 전략에 중점을 두어 다룰 것입니다.

(출처: [2]) 인간 피드백을 기반으로 LLM을 학습시키기 위해, 우리는 먼저 선호도 데이터(preference data)를 수집합니다. 각 선호도 데이터 예시는 동일한 원본 문서에 대한 두 요약 쌍으로 구성되며, 이 중 하나의 요약이 (주로 인간에 의해) 다른 요약보다 "더 좋다"고 판정됩니다 (위 참조). 이 선호도 데이터는 보상 모델(reward model)을 훈련하는 데 활용될 수 있습니다.

“게시물과 후보 요약이 주어졌을 때, 우리는 이 요약이 우리의 레이블러(labeler)에 의해 더 나은 요약으로 판단될 로그 오즈(log odds)를 예측하도록 보상 모델(reward model)을 학습시킵니다.” - 출처 [2]

보상 모델(reward model)은 요약(또는 일반적으로 텍스트 시퀀스)을 입력으로 받아 스칼라 점수(scalar score)를 출력으로 생성하며, 이는 해당 요약에 대한 인간의 선호도 점수(human preference score)를 나타냅니다. 보상 모델(reward model)을 훈련하기 위해, 우리는 쌍에서 선호되는 요약이 거부된 요약보다 더 높은 점수를 받도록 로그 확률(log probability)을 최대화하는 순위 손실(ranking loss)을 사용합니다 (아래 참조). 이전 연구들은 이러한 쌍별 순위 손실(pairwise ranking loss)이 회귀(regression)를 통해 직접 선호도 점수를 학습하는 것보다 뛰어난 성능을 보인다는 것을 입증했습니다 [1]. 훈련 후, 보상 모델(reward model)은 원본 문서와 요약을 입력으로 받아 스칼라 점수(scalar score)를 출력으로 생성하며, 점수가 높을수록 인간이 요약을 선호할 가능성이 높다는 것을 의미합니다.

(출처: [2]) 그 후 이 보상 모델(reward model)은 강화 학습(RL) 알고리즘(예: PPO)을 통해 요약 모델을 훈련하는 신호로 사용될 수 있습니다. 이러한 방식으로 우리는 선호도 쌍(preference pair) 형태의 인간 피드백을 활용하여 요약 모델을 학습시킵니다! RLHF에 대한 더 자세한 내용은 아래의 심층 분석을 확인하십시오.

**RLHF의 발전과 요약 모델 학습**

RLHF(Reinforcement Learning from Human Feedback)는 요약 모델 학습에 혁명적인 변화를 가져왔습니다. 초기에는 인간이 작성한 참조 요약을 모방하는 지도 학습(supervised learning)이 주를 이루었지만, ROUGE 점수와 같은 자동 평가 지표가 인간의 실제 선호도를 제대로 반영하지 못한다는 한계에 부딪혔습니다. 이에 따라 연구자들은 인간의 판단을 직접적으로 반영하는 보상 함수(reward function)를 학습시키고, 이를 통해 요약 모델을 최적화하는 방식으로 전환했습니다.

**보상 모델 학습의 진화.** [1]의 연구자들은 인간 선호도 데이터셋으로부터 보상 함수를 학습시키는 방법을 제안했습니다. 이 접근 방식은 기존 ROUGE 기반 방법과 달리 참조 요약이 필요 없다는 장점이 있습니다. 보상 모델의 아키텍처는 초기에는 단순한 피드포워드 네트워크(feed-forward network)였으나, BERT와 같은 문맥 임베딩(contextualized embeddings)을 활용하여 요약과 원본 문서의 의미적 유사성을 더욱 정교하게 포착하는 방향으로 발전했습니다. 선호도 기반 학습 목표는 회귀 목표보다 우수한 성능을 보여, 현재 보상 모델 학습의 표준으로 자리 잡았습니다.

**LLM 시대의 요약 데이터셋과 평가**

(출처: [20]) 요약은 일반적인 문제이지만, 문헌에서 거의 보편적으로 활용되는 몇몇 표준 데이터셋이 존재합니다. 요약 연구는 주로 뉴스 요약에 초점을 맞추는 경향이 있습니다. 예를 들어, CNN / DailyMail 코퍼스(Corpus) [20]는 가장 널리 사용되는 데이터셋 중 하나입니다 (이 데이터셋의 샘플은 위 참조). 이 데이터셋은 CNN과 DailyMail의 30만 개가 넘는 기사와 각 기사에 대한 관련 요약을 포함합니다. 또 다른 단일 문서 추상적 요약 데이터셋인 XSum [21]도 요약 연구에서 일반적으로 사용됩니다. CNN / DailyMail과 비슷한 규모(즉, 약 23만 개의 요약)를 가진 XSum은 데이터셋의 각 기사의 핵심 아이디어를 포착하는 간결한 단일 문장 요약으로 구성됩니다.

“우리는 더 일반적으로 사용되는 CNN/DM 데이터셋보다 TL;DR 데이터셋을 선택했습니다. 주로 간단한 추출적 기준선(baseline)으로 CNN/DM에서 매우 강력한 성능을 달성할 수 있기 때문입니다.” - 출처 [2]

널리 사용됨에도 불구하고, CNN / DailyMail 및 XSum 데이터셋은 상대적으로 해결하기 쉬운 편입니다. 때로는 더 복잡한 시나리오를 위한 데이터셋이 요구됩니다. 최근 연구에서는 Reddit의 3백만 개 게시물과 각 게시물 작성자가 직접 작성한 요약을 포함하는 TL;DR 데이터셋 [22]을 탐구했습니다 (아래 참조). 이 데이터셋은 그 규모 때문에 일반적으로 품질 향상을 위해 필터링되며, 특정 주제와 관련된 게시물만 포함되도록 처리됩니다 (자세한 내용은 [2]의 섹션 3.2 참조).

**데이터셋 품질 문제 및 해결 방안**

(출처: [22]) 대부분의 요약 데이터셋은 원본 문서와 그에 대한 참조 요약(reference summary)을 모두 포함합니다. 그러나 우리가 이를 "참조"라고 부른다고 해서 데이터셋 내의 요약들이 모두 고품질이라는 의미는 아닙니다! 실제로 많은 논문 [8, 11]에서 CNN / DailyMail과 같은 데이터셋의 참조 요약이 인간이 직접 작성한 요약보다 훨씬 낮은 품질을 보이는 경우가 관찰됩니다. 이러한 발견은 여러 면에서 중요한 문제를 야기합니다:

*   우리는 종종 이러한 참조 요약을 지도 학습(supervised learning)에 활용합니다.
*   이러한 참조는 모델 평가 지표를 계산하는 데 사용될 수 있습니다.

CNN / DailyMail 데이터셋에서 발견된 저품질 참조 요약의 몇 가지 예시는 아래 그림에 제시되어 있습니다. 이러한 품질 문제는 모델 학습의 효과를 저해하고 평가의 신뢰성을 떨어뜨릴 수 있습니다.

**합성 데이터 및 도메인 적응.** 이러한 문제를 해결하기 위해, 최근 연구에서는 고품질의 합성 데이터(synthetic data)를 생성하는 방법을 모색하고 있습니다. 강력한 LLM을 활용하여 원본 문서에 대한 다양한 관점의 요약을 생성하고, 이를 인간 평가자와 비교하여 최적의 요약을 선별하거나, 아예 LLM 자체를 평가 도구로 활용하는 방식입니다. 또한, 특정 도메인(예: 법률, 의학, 금융)에 특화된 요약 모델을 구축하기 위해 해당 도메인의 전문가들이 직접 주석(annotation)한 소규모 데이터셋을 활용하거나, 전이 학습(transfer learning) 기법을 통해 일반 모델을 도메인에 맞게 미세 조정하는 전략도 활발히 연구되고 있습니다.

(출처: [11]) **요약을 어떻게 평가할 수 있을까?**

“추상적 요약을 평가하는 것은 어렵습니다. 기능성을 테스트할 수 있는 제약된 번역이나 코드 생성처럼 간단하지 않습니다.” - Eugene Yan

추상적 요약은 개방형 특성을 지니기 때문에, 요약의 품질을 객관적으로 평가하는 것은 쉽지 않습니다. 하나의 원본 문서를 요약하는 데에는 여러 가지 유효한 방법이 있을 수 있으며, 주어진 요약이 다른 요약보다 "더 좋다"고 판단하는 것은 본질적으로 주관적입니다! 이러한 이유로, 추상적 요약을 적절하게 평가하기 위해서는 우리(인간과 연구자)가 "좋은" 요약이 어떤 기준을 충족해야 하는지에 대한 합의된 기준을 설정하는 것에서 시작해야 합니다. 예를 들어, [23]에서는 다음과 같은 기준들이 제안되었습니다:

*   **유창성(Fluency)**: 요약의 문장들이 자연스럽게 읽히고 문법적 오류가 없어야 합니다.
*   **일관성(Coherence)**: 요약 전체가 논리적으로 연결되어 읽기 쉽고, 응집력 있으며, 합리적인 방식으로 구성되어야 합니다.
*   **관련성(Relevance)**: 요약은 원본 문서에서 가장 "중요한" 정보를 포함해야 합니다.
*   **일치성(Consistency)**: 요약된 정보가 정확하고 원본 문서의 내용과 일치해야 합니다 (즉, 환각(hallucination)이나 잘못된 정보가 없어야 합니다).

하지만 이것이 우리가 정의할 수 있는 유일한 기준 세트는 아닙니다! 논쟁의 여지가 있지만, 유창성(fluency)은 현대 LLM 3에 의해 거의 해결되었으며, 우리가 중요하게 여기는 기준은 해결하려는 사용 사례에 따라 달라질 수 있습니다. 예를 들어, [7]의 저자들은 사용자 의견이나 리뷰(예: Yelp)를 요약하는 데 더 적합한 충실도(faithfulness), 사실성(factuality), 일반성(genericity)을 포함하는 대안적인 기준 세트를 고안합니다. 좋은 요약의 바람직한 특성을 정의하는 것이 평가 과정의 첫 번째 단계입니다. 이것이 명확해지면, 성능을 측정하고 반복 개발함으로써 더 나은 추상적 요약 모델을 개발하는 것에 대해 고민할 수 있습니다.

**인간 평가(Human evaluation).** 추상적 요약 품질을 평가하기 위한 자동화된 전략들이 많이 존재하지만, 인간 평가(human evaluation)는 가장 신뢰할 수 있는 평가 방식이며, 요약 연구 전반의 품질 평가를 위한 "정답(ground truth)" 역할을 수행합니다. 모델의 진정한 품질을 파악하려면 인간 평가 과정을 거쳐야 합니다. 그럼에도 불구하고, 인간 평가(human evaluation)가 만능 해결책은 아닙니다! 인간으로부터 정확하고 신뢰할 수 있으며 일관된 품질 레이블을 얻는 것은 지극히 어려운 일입니다 4. 특히 추상적 요약과 같은 주관적인 작업에서는 더욱 그렇습니다. 인간은 끊임없이 서로 다른 의견을 가집니다 (요약 품질 외에도 많은 것에 대해!). 이는 평가 과정을 상당히 노이즈가 많게 만들 수 있습니다. 이러한 문제를 완화하기 위해, 우리는 플라이스 카파(Fleiss’ kappa) 및 크리펜도르프 알파(Krippendorff's alpha)와 같은 지표들을 사용하여 인간 주석자(annotator) 간의(또는 주석자(annotator)와 연구자 간의) 일치도 수준을 모니터링할 수 있습니다.

**전통적인 (자동) 지표.** 인간 평가(human evaluation)가 요약 품질의 정답(ground truth) 역할을 하지만, 인간 품질 평가를 수집하는 것은 비용이 많이 들고 시간이 소모되기 때문에 오로지 인간 평가에만 의존할 수는 없습니다. 우리는 인간 평가 시도 사이에 모델을 더 빠르게 반복 개발할 수 있도록 하는 자동 지표가 필요합니다. 먼저, 요약 작업을 위한 더 전통적인 자동 평가 지표를 살펴보겠습니다. 이들은 두 가지 범주로 나뉩니다:

*   참조 기반(Reference-based)
*   참조 불필요(Reference-free) (또는 맥락 기반(context-based))

참조 기반(Reference-based) 지표는 요약 품질을 측정하는 데 사용할 수 있는 목표 또는 참조 요약(일반적으로 인간이 작성한)이 있다고 가정하는 반면, 참조 불필요(Reference-free) 지표는 생성된 요약과 원본 문서에만 순수하게 기반하여 요약 품질을 평가합니다. 요약을 위한 가장 일반적으로 사용되는 평가 지표는 ROUGE(Recall-Oriented Understudy for Gisting Evaluation) 점수입니다. 이 점수는 참조 요약에 있고 모델이 생성한 출력에도 나타나는 단어 수(또는 ROUGE-N의 n-그램(n-gram) 수)를 단순히 세는 방식으로 작동합니다 (아래 참조). ROUGE는 참조 요약과 출력 요약 간의 중복을 측정하는 참조 기반(reference-based) 지표입니다. ROUGE 외에도 요약 품질을 계산하기 위해 유사한 전략을 사용하는 많은 다른 참조 기반(reference-based) 지표가 있습니다:

*   **BLEU(Bilingual Evaluation Understudy) 점수 [25]**: 생성된 출력과 참조 요약 간의 일치하는 n-그램(n-gram) 수를 세고, 이 수를 생성된 출력 내의 총 n-그램(n-gram) 수로 나누어 번역 작업을 평가하는 데 일반적으로 사용됩니다.
*   **BERTScore [26]**: 생성된 출력과 참조 출력의 각 n-그램(n-gram)에 대해 (BERT를 사용하여) 임베딩(embedding)을 생성한 다음, 코사인 유사도(cosine similarity)를 사용하여 두 텍스트 시퀀스의 n-그램(n-gram)을 비교하여 정확한 일치 대신 n-그램(n-gram) 간의 의미론적 일치(semantic match)를 가능하게 합니다.
*   **MoverScore [27]**: n-그램(n-gram) 간의 일대일 매칭을 요구하는 BERTScore를 다대일 매칭을 허용하도록 일반화하여 평가 프레임워크를 더 유연하게 만듭니다.

특정 경우에 참조 기반(reference-based) 지표는 바람직하지 않을 수 있습니다. 예를 들어, 우리의 참조 요약이 저품질이거나, 참조 요약에 전혀 접근할 수 없을 수도 있습니다! 이러한 경우를 처리하기 위해, 우리는 참조 요약 대신 출력 요약을 원본 문서와 비교하여 ROUGE의 맥락 기반(context-based) 버전인 ROUGE-C [24]를 도출할 수 있습니다 (아래 참조).

(출처: [24]) 동일한 전략을 사용하여 BERTScore 및 MoverScore의 참조 불필요(reference-free) 변형도 생성할 수 있습니다! 존재하는 다양한 참조 불필요(reference-free) 및 참조 기반(reference-based) 요약 지표에 대한 자세한 내용은 이 논문을 확인하십시오.

“최근 연구들은 대규모 언어 모델(LLM)을 NLG 평가를 위한 참조 불필요(reference-free) 지표로 사용할 것을 제안합니다. 이는 인간 참조가 부족한 새로운 작업에 적용할 수 있다는 이점이 있습니다.” - 출처 [29]

**LLM-as-a-Judge.** LLM의 생성 결과(추상적 요약 작업 포함)를 평가하는 인기 있는 전략 중 하나는 LLM-as-a-Judge [28]입니다. 이 방식은 강력한 LLM(예: GPT-4)을 평가자로 활용합니다. 생성된 결과물을 평가하거나 점수를 매기기 위해, 우리는 단순히 LLM에게 적절한 프롬프트(prompt)를 제공합니다! 이는 몇 가지 다른 방식으로 수행될 수 있습니다:

*   생성된 결과물 쌍 중에서 더 선호되는 것을 식별하도록 LLM에 요청 (아래 참조).
*   프롬프트에 명시된 기준에 따라 단일 생성 결과물에 대해 (지정된 범위 내의) 스칼라 점수(scalar score)를 생성하도록 LLM에 요청.
*   정확한 채점 방식을 보여주는 몇 가지 퓨샷(few-shot) 예시를 기반으로 생성된 결과물을 평가하도록 LLM에 요청.

(출처: [28]) LLM-as-a-judge는 LLM의 출력을 평가하기 위한 새롭고 강력하며 참조 불필요(reference-free) 전략입니다. 그러나 이 평가 방식은 몇 가지 형태의 편향(bias)을 내포합니다:

*   **위치 편향(Position bias)**: 모델의 프롬프트 내에서 생성된 출력의 순서가 결과 점수에 영향을 줄 수 있습니다. 이를 해결하기 위해 무작위로 위치를 바꿔 여러 점수를 생성하고 평균을 취할 수 있습니다.
*   **장황함 편향(Verbosity bias)**: GPT-4와 같은 모델은 더 길고 장황한 출력을 선호하는 경향이 있습니다. 생성된 출력의 길이를 정규화하여 이를 보정할 수 있습니다.
*   **자기 강화 편향(Self-enhancement bias)**: GPT-4 및 다른 모델들은 자신의 생성물에 다른 모델의 출력보다 높은 점수를 부여하는 경향이 있으므로, 특정 LLM이 자신의 생성물을 평가하는 데 사용될 때는 주의해야 합니다!
*   **제한된 능력**: LLM은 완벽하지 않습니다! 따라서 LLM-as-a-Judge를 사용하여 판사 자체가 해결하기 어려워하는 문제(예: 복잡한 수학 또는 추론 문제)에 대한 모델의 출력을 채점할 때 한계에 부딪힐 수 있습니다.

이러한 편향(bias)에도 불구하고, LLM-as-a-judge 스타일의 평가는 놀랍도록 견고하며 다양한 응용 분야에서 인간 평가와 높은 상관관계를 보여, 최근 연구에서 광범위하게 채택되고 있습니다 (요약 및 그 이상). [29]에서 저자들은 사고 연쇄 프롬프트(chain of thought prompting) 및 양식 작성(form-filling)을 통해 LLM 기반 평가를 강화하여, 특히 요약 작업의 평가 품질을 향상시키는 G-Eval이라는 새로운 평가 전략을 개발했습니다 (아래 참조).

(출처: [29]) **보상 모델(Reward models).** 위에서 논의했듯이, 요약 모델 학습을 위한 가장 효과적인 전략 중 하나인 선호도 튜닝(preference tuning)은 인간 선호도 데이터셋으로 보상 모델(reward model)을 학습시키는 것을 포함합니다. 이 모델의 출력은 강화 학습(RL)을 통한 미세 조정(finetuning)을 위한 보상 신호로 활용되지만, 동일한 보상 신호는 품질 평가 목적으로도 재활용될 수 있습니다! 보상 모델(reward model)은 생성된 요약을 입력으로 받아 이 요약에 대한 인간 선호도 점수(human preference score)를 예측합니다. 따라서 보상 모델(reward model)의 출력은 인간 선호도의 대리 지표(proxy)이며, 이는 참조 불필요(reference-free) 품질 평가로 직접 사용될 수 있습니다. 더 자세한 정보는 아래의 Nathan Lambert의 보상 모델(reward model) 평가에 대한 훌륭한 글을 확인하십시오.

**RewardBench**

**인간 피드백을 통한 요약 품질 향상**

“학습 시 요약기가 참조 요약을 재현하도록 하는 지도 학습 패러다임과 비교하여, RL은 요약기가 생성된 요약의 품질을 측정하는 보상(reward)을 최대화하도록 직접 최적화합니다.” - 출처 [1]

오랫동안 요약 모델 학습의 최첨단 방식은 고품질 참조 요약 데이터셋을 사용하여 사전 학습된 기본 모델을 지도 미세 조정(supervised finetuning)하는 것이었습니다. 이 접근 방식은 효과적이었지만, 이 섹션에서 보듯이 선호도 튜닝(preference tuning)을 통해 더 나은 결과를 얻을 수 있습니다. 인간 피드백은 훨씬 더 우수한 요약 모델을 학습시키는 데 기여합니다. 그러나 이러한 연구는 요약이라는 주제를 넘어섭니다. 유사한 기술들은 최근 LLM 정렬(alignment) 연구에 의해 재활용되어 LLM 학습 파이프라인의 핵심 기반을 형성했습니다.

**더 나은 보상이 더 나은 요약을 만든다 [1]**

(출처: [1]) 지도 학습은 원래 요약 모델 훈련의 가장 흔한 패러다임이었습니다. 우리는 단순히 인간이 작성한 참조 요약을 모방하도록 모델을 가르쳤습니다. 최근에는 연구자들이 요약 모델 훈련에 강화 학습(RL)을 적용하는 방안을 모색하기 시작했습니다. 초기 시도에서는 ROUGE 점수를 보상(reward) 5으로 직접 사용했지만, ROUGE 점수는 인간의 품질 평가와 낮은 상관관계를 보였습니다 (위 참조). 따라서 [1]의 저자들은 RL을 통해 모델이 인간에게 더 선호되는 요약을 생성하도록 유도하는, 더 효과적인 보상 함수(reward function)를 찾고자 했습니다.

“인간에게 매력적인 요약을 생성하기 위한 더 나은 보상 함수(reward function)를 찾기 위해, 우리는 2,500개의 요약에 대한 인간 평가로부터 보상 함수(reward function)를 학습합니다.” - 출처 [1]

**보상 학습의 세부 사항.** [1]의 저자들은 인간 선호도 데이터셋으로부터 보상 함수(reward function)를 학습할 것을 제안했습니다. 이전 연구에서 가져온 이 데이터셋은 CNN / DailyMail 코퍼스(corpus)의 500개 뉴스 기사에 대한 2,500개의 요약(인간 평가 포함)을 담고 있었습니다. 이 데이터를 활용하여, 우리는 문서와 시스템 요약을 입력으로 받아 인간 평가를 예측하도록 보상 모델(reward model)을 훈련할 수 있습니다. ROUGE를 보상(reward)으로 사용하는 기법과 달리, 이 방법은 보상 계산에 참조 요약이 필요 없다는 장점이 있습니다! 보상 모델(reward model)을 훈련하기 위해, 우리는 회귀 목표(regression objective) 또는 선호도 학습 목표(preference learning objective)를 사용할 수 있습니다. 후자는 보상 함수(reward function)가 인간이 선호하는 요약을 정확하게 식별하는지 여부를 포착합니다 (아래 참조).

(출처: [1]) [1]에서는 보상 함수(reward function)를 위해 여러 아키텍처(architecture)를 검토했지만, 가장 우수한 결과를 보인 접근 방식은 요약과 입력 문서의 (BERT를 활용하여 생성된) 연결된 임베딩(embedding)을 입력으로 받는 피드포워드 네트워크(feed-forward network)였습니다. [1]의 연구에서 우리는 선호도 학습 목표(preference learning objective)가 회귀 목표(regression objective)에 비해 더 나은 결과를 가져온다는 것을 확인했으며, 이는 선호도 학습 목표(preference learning objective)가 현재 보상 모델(reward model) 학습에 거의 보편적으로 사용되는 이유를 설명합니다. 아래 표에서 우리는 최적의 보상 함수(reward function)가 다음을 사용한다는 것을 알 수 있습니다:

*   임베딩(embedding)을 위한 BERT.
*   보상 예측을 위한 피드포워드 네트워크(feed-forward network) (또는 MLP).
*   선호도 기반 학습 목표.

이 보상 함수(reward function)는 인간의 판단과 높은 상관관계를 가지는 예측을 생성하고, 높은 재현율(recall)과 정밀도(precision)로 "좋은" 요약을 식별하는 것으로 나타났습니다.

(출처: [1]) **더 나은 요약 모델.** 인간의 선호도를 정확하게 예측하는 능력을 넘어, [1]에서 학습된 보상 모델(reward model)은 더 우수한 추출적 및 추상적 요약 모델을 구축하는 데 활용될 수 있습니다. 지도 학습 기준선(baseline)과 ROUGE를 보상(reward)으로 사용하여 강화 학습(RL)으로 훈련된 모델 모두와 비교했을 때, 인간 피드백으로부터 학습된 보상(reward)으로 훈련된 모델은 훨씬 더 높은 인간 평가 점수를 가진 요약을 생성하는 것으로 나타났습니다 (이전 최첨단 시스템보다도 뛰어난 성능을 보였습니다!). (아래 참조).

(출처: [1]) 요약하자면, [1]의 연구에서 우리는 인간 피드백으로부터 보상 함수(reward function)를 학습하는 것이 강화 학습(RL) 기반 요약 모델 훈련을 위한 뛰어난 학습 신호를 제공할 수 있다는 것을 확인했습니다. 후속 연구들은 이 교훈을 받아들여 이를 LLM 영역으로 확장하여, 인간 피드백으로부터 다양한 작업(요약 포함)을 학습하는 데 적용했습니다.

**인간 피드백을 통한 요약 학습 [2]**

지도 미세 조정(supervised finetuning)은 다음 토큰 예측 목표(next token prediction objective)를 사용하여 인간 시연(demonstration) 세트의 로그 확률(log probability)을 최대화합니다. 우리는 모델이 인간이 작성한 텍스트 6에 높은 확률을 부여하도록 가르칩니다. ROUGE를 통해 이러한 모델을 평가하면 모델의 출력이 참조와 얼마나 밀접하게 일치하는지 정량화할 수 있습니다. 이 접근 방식은 (상대적으로) 잘 작동하지만, [2]의 저자들은 지도 학습과 ROUGE가 우리의 실제 목표인 고품질 요약 작성의 대리 지표(proxy)일 뿐이라고 지적합니다.

“우리는 인간 선호도를 최적화하도록 모델을 학습시킴으로써 요약 품질을 크게 향상시킬 수 있음을 보여줍니다.” - 출처 [1]

인간은 항상 완벽한 요약을 작성하지 않으며, 어떤 원본 문서에 대해서도 수많은 동등하게 유효한 요약을 작성할 수 있습니다. 따라서 인간이 작성한 요약과 정확히 일치하도록 요약 모델을 학습시키는 것은 결함이 있는 접근 방식입니다! 모든 참조 요약(심지어 저품질 요약도)은 학습 과정에서 동등하게 강조되며, 우리는 유효한 요약의 다양성을 설명할 방법이 없습니다. 이를 염두에 두고, 우리는 다음과 같이 질문할 수 있습니다: 요약 품질에 기반하여 모델을 직접 최적화하는 목표를 찾을 수 있을까?

(출처: [2]) **인간 피드백으로부터 학습.** [2]에서 저자들은 인간의 선호도 데이터에 기반하여 LLM을 미세 조정(finetune)할 수 있도록 하는 세 부분으로 구성된 프레임워크를 제안함으로써 정확히 이 작업을 수행합니다 (위 참조). LLM은 먼저 인간 참조 요약에 대한 지도 미세 조정(supervised finetuning)을 사용하여 학습되어 지도 학습 기준선(baseline)을 생성하고, 이 기준선은 다시 강화 학습(RL)으로 추가 미세 조정(finetune)됩니다. [2]의 RLHF 과정은 다음을 통해 인간 피드백 데이터셋을 수집하는 것으로 시작됩니다:

*   학습 데이터셋에서 텍스트 입력(원본 문서) 가져오기.
*   여러 정책(policy)(예: 사전 학습된 모델, 지도 학습 기준선(baseline), 현재 모델 또는 인간 참조 요약)을 사용하여 입력의 요약 샘플링.
*   샘플 응답 세트에서 두 요약 선택.
*   인간 주석자(annotator)에게 두 요약 중 더 나은 것을 식별하도록 요청.

인간 비교 데이터는 대량으로 수집되며 오프라인 방식으로 RLHF를 통해 모델(디코더 전용 LLM)을 미세 조정(finetune)하는 데 사용됩니다. 데이터가 수집되면, 이 비교 데이터를 사용하여 LLM이 생성한 요약이 주어졌을 때 인간 선호도 점수(human preference score)를 정확하게 예측하는 보상 모델(reward model)을 학습시킵니다. 여기에서 우리는 RL을 사용하여 모델을 미세 조정(finetune)합니다 ( [2]의 저자들은 PPO 알고리즘을 사용합니다). 보상 모델(reward model)이 출력한 선호도 점수에 기반합니다.

**PPO의 최적화 목표에 KL 발산(KL divergence) 추가**

(출처: [2]) **드리프트(drift) 방지.** [2]의 저자들은 PPO에 의해 최적화되는 목표에 KL 발산(KL divergence) 항을 추가합니다. 이는 RLHF 동안 정책(policy)이 지도 학습 기준선(supervised baseline) 정책과 너무 달라지는 것을 페널티(penalize)합니다 (위 참조). 이제 일반적으로 사용되는 이러한 접근 방식(예: LLaMA-2 보고서의 Eq. 4 참조)은 모드 붕괴(mode collapse) 7 없이 탐색을 장려하고 LLM이 작성한 요약이 학습 중에 본 것과 너무 달라지는 것을 방지합니다.

(출처: [2]) **피드백으로부터의 학습은 효과적인가?** [2]의 저자들은 위에서 설명한 전략을 사용하여 TL;DR 데이터셋에 대해 13억에서 67억 개의 매개변수(parameter)를 가진 여러 GPT 스타일 언어 모델을 미세 조정(finetune)합니다. 인간 피드백을 통해 학습된 모델은 지도 학습만으로 학습된 모델이 작성한 요약보다 인간에게 일관되게 선호되는 요약을 생성하는 것으로 나타났습니다 (위 참조). 13억 개의 인간 피드백 모델은 지도 학습만으로 학습된 10배 더 큰 모델보다 성능이 우수하며, 67억 개의 인간 피드백 모델은 13억 모델보다도 더 나은 성능을 보입니다. 즉, 요약 품질은 모델 규모(model scale)로부터 이점을 얻습니다.

(출처: [2]) 인간 피드백으로 학습된 요약 모델은 새로운 도메인에 더 잘 일반화(generalize)되는 것으로 보입니다. 예를 들어, [2]에서 TL;DR에 대해 미세 조정(finetune)된 모델은 추가 미세 조정(finetuning) 없이 뉴스 중심 데이터셋에서 잘 작동하는 것으로 나타났습니다 (위 참조).

“우리는 우리의 보상 모델(reward model)이 인간 선호도 예측에서 ROUGE와 같은 다른 지표보다 성능이 우수하며, 우리의 보상 모델(reward model)을 직접 최적화하는 것이 인간에 따르면 ROUGE를 최적화하는 것보다 더 나은 요약을 생성한다는 것을 확인합니다.” - 출처 [1]

**보상 모델(Reward model) > ROUGE.** ROUGE와 같은 참조 기반(reference-based) 지표가 요약 평가의 표준이지만, [2]의 저자들은 ROUGE 점수가 인간 선호도와 상관관계가 낮은 경향이 있다는 것을 관찰합니다. 이러한 이유로, PPO의 보상 신호로 ROUGE를 사용하여 미세 조정(finetune)된 요약 모델은 인간 선호도를 예측하도록 보상 모델(reward model)을 학습시키는 모델보다 성능이 떨어집니다. [2]에서 보상 모델(reward model)은 인간 선호도를 매우 정확하게 예측하는 것으로 나타났으며, 이는 선호도 기반 지표가 요약 품질 평가에 유용한 접근 방식임을 보여줍니다.

**인간 선호도로부터 언어 모델 미세 조정 [3]**

[2]의 작업에 앞서, 동일한 저자들(OpenAI 소속)은 사전 학습된 LLM을 미세 조정(finetune)하기 위해 인간 피드백을 활용하는 방법을 탐구했습니다. 이 연구 [3]에서는 긍정적인 감성 또는 물리적으로 묘사적인 언어를 사용한 텍스트의 스타일적 연속 생성, 그리고 TL;DR 및 CNN/Daily Mail 데이터셋에 대한 요약이라는 네 가지 다른 작업이 다루어졌습니다. [3]에서 사용된 미세 조정(finetuning) 전략은 [2]의 전략과 대부분 유사하지만, 지도 미세 조정(supervised finetuning) 구성 요소는 배제되었습니다. 즉, 오직 인간 피드백에 기반하여 미세 조정(finetune)이 이루어졌습니다 (아래 참조). [3]에서는 선호도 튜닝(preference tuning)이 지도 학습의 대안으로 제시되었으며, 함께 적용될 수 있는 기술이라기보다는 독립적인 방식으로 다루어졌습니다.

(출처: [3]) 스타일적 연속 작업을 위해 저자들은 5,000개의 선호도 쌍(preference pair) 데이터셋을 수집했으며, 요약을 위해서는 60,000개 이상의 선호도 쌍(preference pair)이 수집되었습니다. 스타일적 연속을 위한 인간 피드백 모델은 77%의 경우에서 지도 학습 기준선(baseline)보다 더 선호되는 것으로 나타났습니다. 유사하게, 인간 주석자(annotator)는 지도 학습 기준선(baseline)보다 인간 피드백으로 학습된 요약 모델을 선호했지만, 인간 피드백 모델은 요약될 문서의 처음 세 문장을 단순히 복사하는 간단한 기준선(baseline)에 의해 놀랍게도 성능이 저조했습니다 (아래 참조).

(출처: [3]) 흥미롭게도, [3]의 분석은 인간 피드백 기반 요약 모델이 추상적 요약을 생성하기 위해 매우 간단한 정책(policy)을 학습한다는 것을 밝혀냈습니다. [3]의 저자들은 이를 "스마트 복사(smart copying)"라고 명명했습니다. 모델은 원본 텍스트에서 큰 텍스트 구간이나 문장을 복사하는 경향이 있었고, 관련 없거나 요약에 포함할 가치가 없는 문장은 건너뛰었습니다. 이 메커니즘은 학습 과정에서 자연스럽게 나타났습니다. 즉, 복사를 장려하는 명시적인 아키텍처(architectural) 구성 요소가 요약 모델에 추가되지 않았음에도 불구하고 말입니다. 이러한 단순함에도 불구하고, 학습된 복사 메커니즘은 인간 주석자(annotator)들로부터 긍정적인 평가를 받았습니다.

“우리는 인간의 판단으로만 정의되는 복잡한 작업에 강화 학습(reinforcement learning)을 적용하고 싶습니다. 이러한 작업에서는 인간에게 물어봄으로써만 결과가 좋은지 나쁜지 알 수 있습니다.” - 출처 [3]

**유용한 시사점 및 현대 LLM에의 적용.** [2]의 작업과 비교했을 때, [3]에서 인간 피드백을 사용하여 학습된 요약 모델은 훨씬 덜 강력했으며, 간단한 기준선(baseline)에도 성능이 뒤처지는 경향이 있었습니다. 이러한 결과는 사용된 모델의 차이와 학습 과정에서 지도 미세 조정(supervised finetuning)이 배제되었기 때문일 가능성이 높습니다. 그럼에도 불구하고, [3]의 연구는 후속 발전을 위한 견고한 토대를 마련했으며, 이 분석에서 얻을 수 있는 몇 가지 중요한 시사점은 다음과 같습니다:

*   인간 피드백으로부터의 학습은 i) 충분한 지도 학습 데이터가 부족하거나 ii) 보상 신호로 사용될 좋은 자동 대리 지표(proxy)가 없는 작업에 가장 효과적입니다 (예: [2]에서 ROUGE가 유용한 보상 함수(reward function)가 아님을 확인).
*   강화 학습(PPO)을 통한 미세 조정(finetuning) 목표에 KL 발산(KL divergence) 항을 추가하면 과도한 드리프트(drift) 8를 방지하는 데 도움이 됩니다.
*   인간은 LLM 출력의 품질을 판단하기 위해 간단하고 (불완전한) 휴리스틱(heuristic)에 의존하는 경향이 있습니다.
*   온라인 데이터 수집(또는 LLM이 반복적으로 미세 조정(finetune)되고 개선됨에 따라 보상 모델(reward model)을 재학습시키기 위해 추가 데이터를 계속 수집하는 것)은 지속적인 성능 향상을 가져옵니다.

온라인 데이터 수집의 이점에 대한 위의 발견은 LLM 연구에 지속적인 영향을 미쳤습니다! LLaMA-2와 같은 최신 모델들은 정렬(alignment) 과정에서 새로 수집된 데이터로 여러 "라운드"의 RLHF를 거칩니다. 또한, 우리는 최근 연구에서 인간 평가의 한계가 LLM 출력 품질을 측정할 때 매우 중요한 고려 사항이라는 것을 계속해서 보고 있습니다!

**인간 피드백을 통한 책 재귀적 요약 [4]**

“우리는 작업의 더 작은 부분에 대해 학습된 모델을 사용하여 인간이 더 넓은 작업에 대한 피드백을 제공하는 데 도움을 줍니다.” - 출처 [4]

요약을 위한 참조 요약이나 선호도 레이블(preference label)을 선별하는 것은 (잠재적으로) 다소 시간이 걸릴 수 있지만, 아주 어렵지는 않습니다. 인간 주석자(annotator)는 단순히 다음을 수행해야 합니다:

*   요약될 기사를 읽기.
*   요약을 작성하거나 제시된 요약의 품질을 평가하기.

그러나 요약될 텍스트가 매우 길 때(예: 전체 소설) 이 과정은 훨씬 더 복잡해집니다. 이 경우, 소설 요약을 평가하는 것은 시간이 많이 소요되는데, 인간 주석자(annotator)는 이미 책을 읽었거나 요약을 정확하게 평가하기 위해 책을 읽는 데 많은 시간을 투자해야 하기 때문입니다. [4]에서 저자들은 인간이 그러한 복잡한 작업에 대한 학습 신호를 효율적으로 제공할 수 있도록, 인간 피드백과 재귀적 작업 분해(recursive task decomposition)를 결합한 확장 가능한 접근 방식을 제안합니다.

(출처: [4]) **재귀적 작업 분해(Recursive task decomposition).** [4]에서 제시된 핵심 아이디어는 긴 텍스트를 재귀적으로 요약될 수 있는 더 작은 덩어리(chunk)로 나누어, 리프(leaf) 노드가 적절한 크기의 텍스트 덩어리에 대한 표준 요약 작업이 되는 요약 작업 트리(위 참조)를 구성하는 것입니다. 먼저, 모델은 책의 작은 덩어리를 요약하는 데 사용됩니다. 그런 다음, 동일한 모델이 이러한 리프 요약들을 입력으로 받아 책의 더 큰 부분을 요약합니다. 즉, 더 짧은 구절들의 요약이 책의 더 큰 부분을 요약하기 위한 입력으로 사용되는 것입니다. 이 접근 방식을 취함으로써, 인간은 원본 텍스트 전체에 대한 심층적인 지식이 없더라도 이 작업 9을 효율적으로 감독할 수 있습니다. 이는 모델이 책 전체가 아니라, 책 자체 또는 이전에 생성된 요약에서 가져온 작은 텍스트 덩어리만을 요약하기 때문입니다 (아래 참조).

(출처: [4]) 추론 시, 모델은 먼저 책의 작은 부분을 요약하는 데 사용됩니다. 그런 다음, 이러한 요약은 전체 책의 요약이 생성될 때까지 재귀적으로 활용되어 더 높은 수준의 요약을 만들어냅니다. 이러한 재귀적 전략 덕분에, [4]에서 제안된 접근 방식은 임의 길이의 텍스트를 요약할 수 있습니다!

**데이터셋 생성 전략.** 긴 텍스트의 요약을 충분한 깊이로 재귀적으로 분해하면, 결국 인간이 쉽게 감독하고 LLM을 훈련시키는 데 활용될 수 있는 합리적인 요약 하위 작업 세트를 얻게 됩니다. 재귀적 요약은 세 가지 주요 작업을 가집니다:

*   **분해(Decompose)**: 텍스트가 직접 요약하기에는 너무 길다고 판단하고, 텍스트의 더 짧은 부분에 대해 여러 요약 하위 작업을 생성합니다.
*   **응답(Respond)**: 요약을 생성하여 하위 작업을 해결합니다.
*   **구성(Compose)**: "응답"과 동일하지만, 모델은 요약을 생성할 때 여러 하위 작업에 대한 해결책(즉, 이전에 생성된 요약)을 제시받습니다.

책의 경우, 분해(decompose) 작업은 LLM에게 하위 작업을 생성하도록 요청하는 대신 알고리즘적으로 수행될 수 있습니다. 즉, 긴 텍스트 시퀀스를 더 짧은 시퀀스로 덩어리화합니다. 이 전략을 사용하면 학습 데이터 획득이 간단합니다! 우리는 단순히 인간에게 특정 하위 작업을 수동으로 요약하거나 특정 하위 작업에 대해 생성된 두 요약의 품질을 비교하도록 요청합니다. 노드(node)가 리프(leaf)가 아니어서 인간이 더 긴 텍스트를 요약하는 경우, LLM은 모든 하위 작업의 요약을 재귀적으로 생성하여 맥락(context)으로 사용합니다.

“우리는 인간 레이블러(labeler)로부터 방대한 시연(demonstration) 및 비교 데이터를 수집하고, 행동 복제(behavioral cloning) 및 보상 모델링(reward modeling)을 사용하여 GPT-3를 미세 조정(fine-tune)하여 요약을 재귀적으로 수행합니다.” - 출처 [4]

지도 학습 예시와 선호도 레이블(preference label)로 구성된 이 데이터셋이 주어졌을 때, [4]에서 사용된 학습 전략은 [2]의 전략과 거의 동일합니다. 사전 학습된 GPT-3 모델로 시작하여, 우리는 먼저 인간이 작성한 참조 요약에 대해 지도 학습 방식으로 모델을 미세 조정(finetune)합니다(이는 [4]에서 행동 복제(behavioral cloning)라고 불립니다). 그런 다음, 수집된 선호도 레이블(preference label)에 대해 여러 번 반복하여 선호도 튜닝(preference tuning)을 수행합니다. [4]에서 사용된 학습 알고리즘에 대한 전체 설명은 아래를 참조하십시오.

(출처: [4]) **실험 결과.** [4]의 학습 데이터는 GPT-3의 사전 학습 데이터셋에서 책 데이터의 하위 집합(즉, Books1 및 Books2 데이터셋)을 활용하여 수집됩니다. 이 데이터는 GPT-3(및 1750억 개 대신 60억 개의 매개변수(parameter)를 가진 더 작은 모델 변형)를 미세 조정(finetune)하여 책을 재귀적으로 요약하는 데 사용됩니다. 그런 다음, 모델은 2020년에 출판된 40권의 인기 도서 세트에 대해 평가됩니다. 이 책들은 GPT-3의 사전 학습 데이터셋에 포함되지 않았으며 여러 장르에 걸쳐 있습니다 (아래 참조).

(출처: [4]) 두 명의 레이블러(labeler)가 각 책을 읽고, 요약을 작성하고, 다양한 모델의 요약을 평가하도록 요청받았습니다. 아래에서 볼 수 있듯이, 인간 선호도로 학습된 모델은 지도 학습만으로 학습된 모델보다 훨씬 우수한 성능을 보입니다. 그러나 모든 모델은 여전히 인간 성능에 훨씬 뒤처지며, 이는 추상적 책 요약이 해결하기 엄청나게 어려운 작업임을 보여줍니다. 실제로, 모델이 생성한 요약 중 5-15%만이 인간 품질과 일치하는 것으로 나타났습니다.

(출처: [4]) [4]에서 우리는 첫 번째 하위 작업(즉, 책의 짧은 덩어리 요약)에 대한 학습이 가장 중요하다는 것을 알 수 있습니다. 이 작업에만 학습된 모델은 더 높은 수준의 요약 작업에 비교적 잘 일반화(generalize)될 수 있습니다. 그러나 전체 책 요약 생성 시 성능은 여전히 실망스럽습니다. 개별 하위 작업에 대한 인간 선호도 점수가 책의 전체 분해를 통해 생성된 요약에 할당된 점수보다 훨씬 높습니다 (아래 참조).

**인간 피드백으로 명령어에 따르는 언어 모델 학습 [5]**

“언어 모델을 더 크게 만든다고 해서 본질적으로 사용자 의도를 더 잘 따르게 되는 것은 아닙니다… 이러한 모델은 사용자와 정렬(align)되지 않습니다… 우리는 인간 피드백으로 미세 조정(fine-tuning)하여 광범위한 작업에서 언어 모델을 사용자 의도에 정렬(align)하는 방법을 보여줍니다.” - 출처 [5]

[5]의 연구를 통해 우리는 OpenAI의 저자들이 [2]에서 제안된 유사한 학습 전략이 일반적인 기본 언어 모델이 프롬프트의 명령어를 더 효과적으로 따르도록 만드는 데 활용될 수 있다는 것을 알 수 있습니다. 이 작업의 결과물이 바로 InstructGPT라는 언어 모델입니다. 이 모델은 ChatGPT의 전신이며, 현대 LLM 연구의 거의 모든 기반을 다졌습니다. InstructGPT의 핵심 아이디어는 SFT와 RLHF(아래 참조)를 사용하여 사전 학습된 LLM을 미세 조정(finetune)하여 정렬(alignment)을 촉진하는 것입니다. 정렬(alignment)은 인간 사용자의 의도에 부합하는 텍스트를 생성하는 능력으로 정의됩니다. 일반적으로 우리는 명령어 따르기, 유해한 출력 회피, 거짓 정보 생성 방지, 흥미롭거나 창의적인 출력 생성 등과 같은 고정된 정렬(alignment) 기준 세트를 정의하여 이러한 의도를 구체적으로 포착합니다.

(출처: [5]) 사전 학습 후, LLM은 반복적이거나 (도움이 되지 않는) 텍스트를 생성하고, 자주 환각(hallucinate)하며, 프롬프트 내의 복잡한 명령어를 따르는 데 어려움을 겪을 가능성이 높습니다. 그러나 정렬(alignment) 과정을 통해 우리는 LLM에 이러한 단점을 피하고 원하는 정렬(alignment) 기준을 충족시키는 데 필요한 기술을 가르쳐 훨씬 더 유용하고 흥미로운 모델을 생성할 수 있습니다.

**InstructGPT 생성 과정.** InstructGPT의 정렬(alignment) 과정은 사전 학습된 언어 모델(13억, 60억, 1750억 매개변수 모델로 실험이 진행됨)과 LLM이 답변할 수 있어야 하는 프롬프트 10 세트에서 시작됩니다. 먼저, 인간 주석자(annotator)가 이러한 각 프롬프트에 대한 답변을 수동으로 제공하여, 사전 학습된 모델이 지도 학습을 통해 미세 조정(finetune)되는 데이터셋을 생성합니다. 이 초기 학습 과정은 모델을 명령어와 유사한 프롬프트에 노출시켜 인간 피드백을 통한 추가 미세 조정(finetuning)을 위한 더 나은 시작점을 만듭니다.

“SFT 데이터셋은 약 13,000개의 학습 프롬프트(API 및 레이블러(labeler) 작성)를 포함하고, RM 데이터셋은 33,000개의 학습 프롬프트(API 및 레이블러(labeler) 작성)를 가지며, PPO 데이터셋은 31,000개의 학습 프롬프트(API에서만)를 가집니다.” - 출처 [5]

여기에서 인간 선호도 레이블(preference label) 데이터셋이 수집되며, 각 선호도 예시는 동일한 프롬프트에 대한 여러 응답을 품질에 따라 순위 매깁니다. 이 선호도 데이터셋은 프롬프트와 생성된 응답을 입력으로 받아 스칼라 보상(scalar reward)을 예측하는 보상 모델(RM)(LLM과 동일한 아키텍처(architecture)를 공유)을 학습시키는 데 사용됩니다 (아래 참조).

**보상 모델(Reward model) 아키텍처(architecture)**

우리는 이 보상 모델(RM)을 사용하여 관련 프롬프트 세트에 대해 PPO를 통해 LLM을 미세 조정(finetune)할 수 있습니다. [2]의 제안과 거의 정확히 일치하는 이 학습 파이프라인은 거의 모든 LLM에서 사용되는 표준 3단계 정렬(alignment) 절차를 형성합니다. 여기서는 이 접근 방식이 요약이 아닌 정렬(alignment)에 적용됩니다!

(출처: [5]) **실험 결과.** [5]의 연구에서 우리는 인간 피드백으로부터의 학습이 정렬(align)된 (그리고 유용한) LLM을 생성하기 위한 놀랍도록 효과적인 전략이라는 것을 확인했습니다. 특히, [5]의 저자들은 다음을 관찰합니다:

*   인간 주석자(annotator)들은 GPT-3보다 InstructGPT의 출력을 더 선호했습니다 (위 참조).
*   InstructGPT는 GPT-3보다 더 진실성이 있는 경향이 있었습니다.
*   GPT-3는 InstructGPT보다 더 유해한 출력을 생성했습니다.
*   InstructGPT는 명령어를 더 잘 따랐으며, 미세 조정(finetuning)에 사용된 프롬프트 분포를 넘어선 명령어조차도 잘 따랐습니다.

GPT-3와 비교했을 때, InstructGPT는 매우 유리한 정렬(alignment) 속성을 가집니다. 아래 표에서 볼 수 있듯이, InstructGPT는 GPT-3보다 훨씬 더 조종 가능(steerable)한 경향이 있습니다. 이는 사용자가 프롬프트 내에 제약 조건, 명령어 또는 세부 정보를 제공함으로써 모델의 행동을 더 잘 제어할 수 있음을 의미합니다.

(출처: [5]) 이 모든 이점에도 불구하고, 우리는 정렬(alignment) 절차가 사전 학습된 모델과 비교하여 공개 벤치마크(benchmark)에서 성능 저하의 형태로 "정렬 세금(alignment tax)"을 수반한다는 것을 알 수 있습니다. 이 세금을 피하기 위해, 우리는 선호도 튜닝(preference tuning) 중에 사전 학습 데이터셋에 대한 간헐적 업데이트를 수행할 수 있습니다.

**LLM 시대의 요약: 패러다임 전환과 미래 전망**

InstructGPT가 요약 자체에 직접적으로 초점을 맞추지 않았다는 점을 감안할 때, 이 논문이 왜 이 개요에 포함되었는지 의문이 들 수 있습니다. 그러나 InstructGPT는 특정 유형의 작업을 해결하는 데 특화된 좁은 전문가(narrow expert) 모델에서 벗어나, 다양한 작업을 정확하게 해결할 수 있는 기반 모델(foundation model)로의 NLP 연구 패러다임 전환을 상징합니다. 추상적 요약에 대한 초기 연구는 기반 모델(foundation model)에 대한 후속 연구의 중요한 출발점을 제공했습니다. 하지만 시간이 지나면서 연구자들은 특정 추상적 요약 문제보다는, 더 우수하고 범용적인 기반 모델(foundation model)을 구축하는 데 집중하기 시작했습니다. 이러한 모델들에게 추상적 요약은 정확하게 해결할 수 있는 수많은 작업 중 하나일 뿐입니다.

**GPT-3 시대의 뉴스 요약 및 평가 [6]**

“우리는 최근 뉴스 기사의 새로운 코퍼스(corpus)에 대한 A/B 테스트를 사용하여 이러한 접근 방식을 비교하고, 연구 참가자들이 압도적으로 GPT-3 요약을 선호한다는 것을 발견했습니다.” - 출처 [6]

위에서 언급했듯이, 강력한 LLM의 등장은 기반 모델(foundation model)에 간단한 프롬프트(prompt)를 제공하는 것만으로 다양한 작업을 정확하게 해결할 수 있는 AI 연구의 패러다임 전환을 이끌었습니다. [6]에서 저자들은 이 패러다임 전환이 요약 연구에 미치는 영향을 뉴스 요약에 초점을 맞춰 탐구합니다.

**GPT-3의 요약 능력은 어느 정도인가?** 요약 전용 데이터로 명시적으로 학습되지 않았음에도 불구하고, [6]의 연구에서 우리는 GPT-3가 인간이 선호하는 요약을 작성하는 데 매우 능숙하다는 것을 알 수 있습니다. 심지어 모델에 작업 설명만으로 프롬프트(prompt)를 제공할 때도 그렇습니다 (즉, 제로샷(zero-shot) 방식)! 더 나아가, GPT-3는 명시적인 미세 조정(finetuning) 없이 다양한 작업을 해결할 수 있는 방대한 텍스트 코퍼스(corpus)로 학습된 일반 모델이기 때문에 데이터셋 특정 문제(예: 잘못 작성되었거나 부정확한 참조 요약으로부터 학습)로 인한 어려움을 겪지 않습니다. 이러한 이유로, GPT-3는 상당한 추가 데이터셋 없이도 새로운 요약 도메인에 자연스럽게 일반화(generalize)됩니다. 우리는 단순히 명령어 또는 몇 가지 퓨샷(few-shot) 예시만 필요합니다.

(출처: [6]) SFT 및 명령어 튜닝(instruction-tuned) 모델과 비교했을 때, GPT-3는 모든 데이터셋에서 인간으로부터 20% 더 높은 점수를 달성하는 요약을 생성하며, 이는 GPT-3에 대한 분명한 선호를 나타냅니다 (위 참조). 그러나 최상의 요약 모델 선택이 30% 미만의 경우에서만 만장일치로 이루어지며, 이는 고품질 요약이 비교하기 어려울 수 있다는 것을 보여줍니다. 즉, "최상의" 모델 선택은 간단하지 않습니다.

**자동 지표 분석의 한계.** [6]의 저자들은 또한 요약 품질 평가에서 자동 지표의 효용성을 광범위하게 분석합니다. 이전 연구들은 ROUGE와 같은 자동 지표가 요약 모델 간의 큰 품질 차이를 식별하는 데 유용하지만, 미세한 성능 차이를 포착하는 데는 어려움이 있다는 점을 시사했습니다 (즉, 자동 지표는 미묘한 성능 차이를 가진 모델들을 비교하는 데 덜 유용합니다). [6]의 연구에서 우리는 이러한 경험적 규칙이 LLM 시대에는 더 이상 단순하게 적용되지 않는다는 것을 발견합니다. GPT-3 요약은 기준선(baseline)보다 훨씬 낮은 ROUGE 점수를 받았습니다 (7점 차이!). 그럼에도 불구하고 인간 평가 실험에서는 거의 만장일치로 더 선호되었습니다 (아래 참조).

(출처: [6]) 이러한 결과는 강력한 요약 모델을 고려할 때 참조 기반(reference-based) 지표가 인간 선호도와 낮은 상관관계를 보인다는 추가적인 증거를 제공합니다. 더 나아가, [6]의 연구에서 우리는 GPT-3 요약의 품질과 자동 지표의 한계라는 측면에서 유사한 결과가 특정 키워드 또는 주제에 기반한 안내 요약을 수행하는 키워드 및 측면 기반 요약(keyword and aspect-based summarization)과 같은 더 전문화된 요약 작업에서도 유효하다는 것을 알 수 있습니다.

**GPT-3.5를 활용한 프롬프트 기반 의견 요약 [7]**

대부분의 논문이 일반적인 텍스트 요약(예: 뉴스 기사 요약)을 다루는 반면, [7]의 저자들은 GPT-3.5가 더 복잡한 의견 요약(opinion summarization) 11 작업을 해결할 수 있는지 확인하기 위해 이 주제를 탐구합니다. 왜 의견 요약이 일반 텍스트 요약보다 더 복잡할까요? 몇 가지 이유가 있습니다:

*   다양한 의견들이 서로 모순될 수 있으므로, 여러 관점 12을 정확하게 반영하는 더 섬세한 요약이 필요합니다.
*   요약 대상이 되는 모든 의견의 길이가 LLM의 맥락 길이(context length)를 초과하는 경우가 많으므로, 데이터를 효율적으로 처리하고 요약하는 파이프라인이 필수적입니다.
*   의견 요약 작업은 일반 텍스트 요약보다 연구가 덜 활발했습니다.

[4]의 연구와 유사하게, 재귀적 전략을 사용하여 긴 의견 시퀀스를 요약할 수 있지만, [7]에서는 대안적인 기법들도 탐구됩니다. 예를 들어, 의견들을 주제별 그룹으로 클러스터링(clustering)하거나, 요약에 포함될 가장 중요하거나 (핵심적인) 의견들을 자동으로 식별하는 방법 등이 있습니다.

(출처: [7]) **요약 파이프라인 설계.** [7]의 저자들은 다양한 의견 요약 파이프라인을 탐색했지만, 세 가지 주요 접근 방식이 특히 강조됩니다:

*   텍스트를 재귀적으로 덩어리화하고 요약하여 최종 결과물을 생성하는 계층적 접근 방식(즉, 반복 요약과 함께 덩어리화).
*   QFSumm이라는 추출적 요약 모델을 사용하여 요약 전에 가장 중요한 리뷰들을 식별하는 사전 추출 전략.
*   주제 또는 평점(rating)에 따라 리뷰를 클러스터(cluster)로 분리하고, 각 클러스터(cluster)를 재귀적으로 요약한 다음, 최종 요약을 생성하는 클러스터링(clustering) 접근 방식 (위 참조).

[7]의 연구에서 우리는 GPT-3.5가 짧은 리뷰 시퀀스에 대해 기본적인 재귀적 요약을 통해 유용한 요약을 생성한다는 것을 확인했습니다. 그러나 더 긴 시퀀스를 처리할 때, 반복적인 덩어리화 및 요약은 요약 품질을 저하시킬 수 있다는 것을 발견했습니다. 위에서 설명한 사전 추출 및 클러스터링(clustering) 전략을 통해 이 문제를 완화할 수 있습니다. 이러한 기술은 최종 결과물을 생성하는 데 필요한 재귀적 요약 단계 수를 줄이는 경향이 있기 때문입니다.

(출처: [7]) 이러한 전략으로 생성된 의견 요약의 예시(SPACE 호텔 의견 요약 데이터셋에 대한)는 위 그림에 나와 있습니다. 보시다시피, 주제별 클러스터링(clustering)은 다양한 의견에 대한 높은 수준의 시각을 제공하는 더 추상적인 요약을 생성하는 것으로 보이는 반면, 추출적 전략은 각 입력 리뷰 내에서 언급된 특정 요점에 더 중점을 둡니다.

**인간 평가(Human evaluation).** 위에 설명된 요약 파이프라인 외에도, [7]의 저자들은 각 파이프라인 구성 요소를 모듈화하고 플러그 앤 플레이(plug-and-play) 접근 방식을 사용하여 다양한 파이프라인을 탐구합니다. 이러한 파이프라인은 SPACE 및 FewSum (Amazon 및 Yelp 리뷰 포함) 의견 요약 데이터셋에서 요약을 생성하는 데 사용되며, 생성된 요약의 사실성(factuality), 충실도(faithfulness), 관련성(relevance), 대표성(representativeness)에 기반하여 평가됩니다.

(출처: [7]) 위에서 볼 수 있듯이, 최적의 요약 파이프라인은 평가되는 데이터셋과 속성에 따라 달라집니다. 그러나 GPT-3.5로 생성된 요약은 자동 지표에서는 낮은 성능을 보였음에도 불구하고 인간 평가 측면에서 기준선(baseline) 기술보다 훨씬 우수한 성능을 보입니다. 따라서 [7]에서 제안된 요약 파이프라인은 이전 최첨단 기술에 대한 발전으로 입증됩니다.

**뉴스 요약을 위한 대규모 언어 모델 벤치마킹 [8]**

“우리는 모델 크기가 아니라 명령어 튜닝(instruction tuning)이 LLM의 제로샷(zero-shot) 요약 능력의 핵심이라는 것을 발견했습니다.” - 출처 [8]

이 시점에서 우리는 LLM이 텍스트를 정말 잘 요약하며, 대부분의 경우 이전 최첨단 기술보다 우수한 성능을 보인다는 것을 알고 있습니다. 하지만 왜 그럴까요? 요약 작업에서 LLM의 성공을 뒷받침하는 설계 결정은 아직 충분히 이해되지 않고 있습니다. [8]에서 저자들은 요약 작업에서 여러 사전 학습 방법, 프롬프트(prompt) 및 모델 규모(model scale)를 포함하는 10가지 다른 LLM에 대한 광범위한 인간 평가를 수행합니다. 이러한 실험을 통해, 우리는 명령어 튜닝(instruction tuning)이 LLM을 효과적인 요약기로 만드는 핵심 구성 요소라는 것을 분명히 알 수 있습니다.

(출처: [8]) **저품질 참조 요약의 문제점.** [8]에서 수행된 대부분의 평가는 CNN / DailyMail 및 XSum 데이터셋에서 이루어졌습니다. 흥미롭게도, 저자들은 이 데이터셋의 대부분의 참조 요약이 인간에 의해 저품질로 판단된다는 것을 보여주며, 이는 기존 요약 연구가 일반적인 데이터셋에 존재하는 저품질 참조 요약으로 인해 제한된다는 것을 시사합니다. 실제로, [8]의 저자들은 참조 기반(reference-based) 평가 지표(예: ROUGE)와 인간 선호도 간의 낮은 상관관계가 저품질 참조 요약으로 인해 더욱 악화되며, 이는 다양한 이전 요약 연구의 신뢰성에 의문을 제기합니다.

“참조 요약의 품질 문제를 해결하고 LLM이 인간 요약 작성자와 어떻게 비교되는지 더 잘 이해하기 위해, 우리는 프리랜서 작가를 모집하여 CNN/DM 및 XSUM의 테스트 세트에서 100개 기사를 재주석했습니다.” - 출처 [8]

저품질 참조 요약 문제를 해결하기 위해, CNN / DailyMail 및 XSum 데이터셋에서 새로 구성된 100개 예시 세트가 인간에 의해 재주석되었습니다. 이러한 고품질 참조 요약은 요약 작업 전반에 걸쳐 LLM 성능을 인위적으로 저하시키지 않는 더 신뢰할 수 있는 평가에 사용될 수 있습니다.

(출처: [8]) **실험 설정.** 요약 품질 측정을 위해 10가지 다른 LLM이 고려됩니다 (위 참조). 모델은 아래에 표시된 기본 템플릿을 사용하여 제로샷(zero-shot) 또는 퓨샷(five-shot) 프롬프트(prompt)로 평가됩니다.

“기사: [기사]. 기사를 세 문장으로 요약하세요. 요약:”

다시 한번, 저자들은 기존 요약 데이터셋에 존재하는 참조 요약의 낮은 품질을 강조합니다. 실제로, 참조 요약을 인컨텍스트 학습(in-context learning) 예시로 사용하는 것이 LLM 성능을 저하시키는 것으로 나타났습니다! LLM이 생성한 출력 요약의 몇 가지 질적 예시는 아래 그림에 나와 있습니다.

(출처: [8]) [8]의 실험 분석에서 얻을 수 있는 주요 시사점은 두 가지입니다:

*   명령어 튜닝(instruction tuning)은 요약 성능에 분명히 긍정적인 영향을 미칩니다.
*   LLM이 생성한 요약은 본질적으로 추출적인 반면, 인간이 작성한 요약은 더 많은 추상화 또는 의역(paraphrasing)을 포함하는 경향이 있습니다 (아래 참조).

(출처: [8]) 명령어 튜닝(instruction tuning)을 거친 LLM은 그렇지 않은 LLM보다 분명히 우수한 성능을 보이며, 이는 자기 지도 사전 학습(self-supervised pretraining)만으로는 경쟁력 있는 요약 결과를 얻기에 충분하지 않다는 것을 시사합니다. 또한, LLM이 생성한 요약은 원본 기사에서 정보를 직접 복사하는 경향이 있지만, 복사된 정보는 일관성 있게 합성됩니다. 대조적으로, 인간이 작성한 요약은 원본 자료에서 직접 복사하기보다는 정보를 의역(paraphrase)하는 경향이 있습니다. 추출적 특성에도 불구하고, LLM이 생성한 요약은 인간 평가 실험에서 비교했을 때 인간이 작성한 요약과 동등하게 선호됩니다 (아래 참조).

**ChatGPT 대 인간 작성 텍스트: 제어 가능한 요약의 새로운 관점 [9]**

“인간이 생성한 스타일 변형은 ChatGPT가 보여준 것보다 훨씬 더 크며, 생성된 텍스트는 단어 유형의 분포와 같은 여러 특성에서 인간 샘플과 다릅니다.” - 출처 [9]

흥미로운 요약 형태 중 하나는 제어 가능한 요약(controllable summarization)입니다. 이 방식에서는 모델에 특정 독자를 대상으로 하는 요약을 작성하도록 지시합니다. [9]에서 저자들은 전문가와 비전문가 모두를 위한 과학 정보 요약에서 GPT-3.5-Turbo의 성능을 연구하며, LLM으로 제어 가능한 요약(controllable summarization)을 수행할 때 직면하는 행동 차이(인간이 작성한 요약과 비교), 한계 및 과제를 식별하는 것을 목표로 합니다.

**제어 가능한 요약을 위한 프롬프트 전략**

**독자 맞춤형 요약.** LLM을 사용하여 제어 가능한 요약(controllable summarization)을 수행하려면, 단순히 프롬프트(prompt)를 조작해야 합니다. [9]의 경우, 우리는 명령어 내에서 요약을 작성하려는 독자를 명시할 수 있습니다 (위 참조). 이러한 프롬프트는 eLife 데이터셋의 과학 문헌을 요약하는 데 사용되었으며, 평가를 위해 데이터셋에서 500개의 무작위 샘플이 추출되었습니다. LLM은 모든 요약을 생성하기 위해 제로샷(zero-shot) 방식으로 프롬프트(prompt)를 받습니다.

**평가 지표의 다각화.** GPT-3.5-Turbo가 생성된 요약을 지정된 대상 독자에게 충분히 조정할 수 있는지 확인하기 위해, 여러 자동 지표가 평가에 활용됩니다:

*   **플레시 읽기 용이성(Flesch Reading Ease)**: 단어당 평균 음절 수와 문장당 평균 단어 수를 통해 텍스트의 가독성을 측정합니다. 점수가 높을수록 이해하기 쉽습니다.
*   **콜먼-리아우 지수(Coleman-Liau Index)**: 문장당 평균 문자 수와 100단어당 평균 문장 수를 측정하여 텍스트의 난이도를 포착합니다. 점수가 높을수록 이해하기 더 어렵습니다.
*   **데일-챌 가독성 점수(Dale-Chall Readability Score)**: 텍스트 내의 복잡한 단어 수를 일반적인 단어 목록과 비교합니다. 점수가 높을수록 이해하기 더 어렵습니다.

이러한 가독성 지표 외에도 ROUGE 점수와 n-그램(n-gram) 참신성 13이 측정되었으며, 모델이 생성한 요약 내의 사실적 불일치를 감지하는 SummaC 및 개체명 환각(named entity hallucination)과 같은 지표도 함께 측정되었습니다.

(출처: [9]) **주요 시사점.** [9]의 분석은 우리에게 두 가지 핵심 정보를 제공합니다:

*   GPT-3.5-Turbo는 특정 독자에게 출력을 맞추는 데 인간보다 성능이 떨어집니다.
*   모델이 생성한 요약은 더 추출적이며 일반적으로 환각(hallucination)을 포함하는 경향이 있습니다.

위 표에서 볼 수 있듯이, GPT-3.5-Turbo는 비전문가 요약의 경우 [31, 38] 범위, 전문가 요약의 경우 [28, 37] 범위 내의 플레시 읽기 용이성(Flesch Reading Ease) 점수를 달성하는 요약을 생성합니다. 대조적으로, 인간이 작성한 비전문가 및 전문가 요약은 각각 평균 플레시 읽기 용이성(Flesch Reading Ease) 점수 53.1 및 22.5를 달성합니다. 즉, 가독성 점수 차이의 크기가 인간과 모델이 작성한 요약 간에 극적으로 다릅니다. 퓨샷(few-shot) 예시를 제공하면 이 문제를 완화할 수 있지만, GPT-3.5-Turbo는 그럼에도 불구하고 제어 가능한 요약 작업에서 인간보다 덜 효과적인 것으로 나타났습니다.

(출처: [9]) GPT-3.5-Turbo로 생성된 요약은 인간이 작성한 요약보다 n-그램(n-gram) 참신성도 낮으며, 이는 모델이 작성한 요약이 본질적으로 더 추출적이라는 것을 시사합니다 (위 참조). 또한 GPT-3.5-Turbo로 생성된 요약은 요약과 원본 자료 간의 개체(entity) 및 주제 중복으로 측정했을 때 잦은 환각(hallucination)을 보이는 경향이 있습니다 (아래 참조). 이러한 모든 결과는 GPT-3.5-Turbo가 제어 가능한 요약(controllable summarization) 영역에서 개선의 여지가 있음을 나타내는 것으로 보이지만, 더 최신 모델(예: GPT-4o)은 더 나은 성능을 보일 가능성이 있다는 점에 유의해야 합니다.

(출처: [9]) **결론**

우리는 이제 지도 미세 조정(supervised finetuning)에서 선호도 튜닝(preference tuning), 그리고 현대 기반 모델(foundation model)에 이르기까지 여러 세대에 걸친 요약 연구의 흐름을 살펴보았습니다. 이 작업 전반에서 몇 가지 공통된 주제들이 명확하게 드러납니다:

*   평가의 어려움 (그리고 그 중요성).
*   인간 피드백으로부터 학습하는 것의 가치.
*   데이터 품질의 중요성.
*   현대 기반 모델(foundation model)의 인상적인 능력.

이 개요는 특히 요약 연구에 중점을 두었지만, 여기서 얻은 발견들은 매우 광범위하게 일반화(generalize)될 수 있습니다. 요약은 조건부 생성(conditional generation)(즉, 특정 입력이 주어졌을 때 출력을 생성하도록 모델을 훈련하는 것)에 뿌리를 두고 있기 때문에 자연어 처리(natural language processing)의 근본적인 작업입니다. 기계 번역, 텍스트 분류, 키워드 추출, 질의응답 등 다른 많은 중요한 작업들도 매우 유사한 패턴을 따릅니다! 요약 연구에 대한 깊은 이해는 훨씬 더 광범위한 문제를 해결하는 데 도움이 됩니다.

“우리는 인간 피드백 기반 강화 학습(RLHF; Stiennon et al., 2020)을 사용하여 GPT-3를 미세 조정(fine-tune)하여 광범위한 서면 명령어를 따르도록 합니다.” - 출처 [5]

자연어 처리(natural language processing) 연구 내에서 요약의 근본적인 역할은 이러한 기술들이 LLM 시대에 크게 채택되도록 이끌었습니다. 언어 모델링 연구의 다양한 영향력 있는 접근 방식은 요약 논문에 크게 영향받았습니다! 예를 들어, InstructGPT는 인간 피드백으로부터 더 나은 요약 모델을 학습시키기 위해 이전에 제안된 학습 알고리즘 [2]을 채택하는 반면, 현대 정렬(alignment) 절차는 이전 요약 연구 [3]에서 옹호된 반복적인 미세 조정(finetuning) 전략을 사용합니다. 이 개요에서 제시된 연구는 실용적으로 유용합니다. 그러나 더 중요한 것은 오늘날 LLM에서 볼 수 있는 발전을 위한 토대를 마련한다는 것입니다.

뉴스레터가 처음이신가요? 안녕하세요! 저는 Cameron R. Wolfe입니다. 넷플릭스(Netflix)의 딥러닝(Deep Learning) 박사이자 머신러닝(Machine Learning) 과학자입니다. 이것은 딥(러닝) 포커스 뉴스레터이며, 독자들이 AI 연구의 중요한 주제를 더 잘 이해하도록 돕습니다. 뉴스레터가 마음에 드셨다면, 구독, 공유 또는 X 및 LinkedIn에서 저를 팔로우해주세요! 구독

**참고 문헌**
[1] Böhm, Florian, et al. "Better rewards yield better summaries: Learning to summarise without references." arXiv preprint arXiv:1909.01214 (2019).
[2] Stiennon, Nisan, et al. "Learning to summarize with human feedback." Advances in Neural Information Processing Systems 33 (2020): 3008-3021.
[3] Ziegler, Daniel M., et al. "Fine-tuning language models from human preferences." arXiv preprint arXiv:1909.08593 (2019).
[4] Wu, Jeff, et al. "Recursively summarizing books with human feedback." arXiv preprint arXiv:2109.10862 (2021).
[5] Ouyang, Long, et al. "Training language models to follow instructions with human feedback." Advances in neural information processing systems 35 (2022): 27730-27744.
[6] Goyal, Tanya, Junyi Jessy Li, and Greg Durrett. "News summarization and evaluation in the era of gpt-3." arXiv preprint arXiv:2209.12356 (2022).
[7] Bhaskar, Adithya, Alexander R. Fabbri, and Greg Durrett. "Prompted opinion summarization with GPT-3.5." arXiv preprint arXiv:2211.15914 (2022).
[8] Zhang, Tianyi, et al. "Benchmarking large language models for news summarization." Transactions of the Association for Computational Linguistics 12 (2024): 39-57.
[9] Pu, Dongqi, and Vera Demberg. "ChatGPT vs human-authored text: Insights into controllable text summarization and sentence style transfer." arXiv preprint arXiv:2306.07799 (2023).
[10] Menick, Jacob, et al. "Teaching language models to support answers with verified quotes." arXiv preprint arXiv:2203.11147 (2022).
[11] Fabbri, Alexander R., et al. "Summeval: Re-evaluating summarization evaluation." Transactions of the Association for Computational Linguistics 9 (2021): 391-409.
[12] Subramanian, Sandeep, et al. "On extractive and abstractive neural document summarization with transformer language models." arXiv preprint arXiv:1909.03186 (2019).
[13] Zhang, Fang-Fang, Jin-ge Yao, and Rui Yan. "On the abstractiveness of neural document summarization." Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing . 2018.
[14] Kryściński, Wojciech, et al. "Evaluating the factual consistency of abstractive text summarization." arXiv preprint arXiv:1910.12840 (2019).
[15] Maynez, Joshua, et al. "On faithfulness and factuality in abstractive summarization." arXiv preprint arXiv:2005.00661 (2020).
[16] Durmus, Esin, He He, and Mona Diab. "FEQA: A question answering evaluation framework for faithfulness assessment in abstractive summarization." arXiv preprint arXiv:2005.03754 (2020).
[17] Wang, Alex, Kyunghyun Cho, and Mike Lewis. "Asking and answering questions to evaluate the factual consistency of summaries." arXiv preprint arXiv:2004.04228 (2020).
[18] Brown, Tom, et al. "Language models are few-shot learners." Advances in neural information processing systems 33 (2020): 1877-1901.
[19] Wei, Jason, et al. "Finetuned language models are zero-shot learners." arXiv preprint arXiv:2109.01652 (2021).
[20] Nallapati, Ramesh, et al. "Abstractive text summarization using sequence-to-sequence rnns and beyond." arXiv preprint arXiv:1602.06023 (2016).
[21] Narayan, Shashi, Shay B. Cohen, and Mirella Lapata. "Don't give me the details, just the summary! topic-aware convolutional neural networks for extreme summarization." arXiv preprint arXiv:1808.08745 (2018).
[22] Völske, Michael, et al. "Tl; dr: Mining reddit to learn automatic summarization." Proceedings of the Workshop on New Frontiers in Summarization . 2017.
[23] Kryściński, Wojciech, et al. "Neural text summarization: A critical evaluation." arXiv preprint arXiv:1908.08960 (2019).
[24] He, Tingting, et al. "ROUGE-C: A fully automated evaluation method for multi-document summarization." 2008 IEEE International Conference on Granular Computing . IEEE, 2008.
[25] Papineni, Kishore, et al. "Bleu: a method for automatic evaluation of machine translation." Proceedings of the 40th annual meeting of the Association for Computational Linguistics. 2002.
[26] Zhang, Tianyi, et al. "Bertscore: Evaluating text generation with bert." arXiv preprint arXiv:1904.09675 (2019).
[27] Zhao, Wei, et al. "MoverScore: Text generation evaluating with contextualized embeddings and earth mover distance." arXiv preprint arXiv:1909.02622 (2019).
[28] Zheng, Lianmin, et al. "Judging llm-as-a-judge with mt-bench and chatbot arena." Advances in Neural Information Processing Systems 36 (2024).
[29] Liu, Yang, et al. "Gpteval: Nlg evaluation using gpt-4 with better human alignment." arXiv preprint arXiv:2303.16634 (2023).

1 "발현적 능력(emergent capability)"이란 특정 규모(데이터 또는 컴퓨팅 측면에서)가 탐색된 후에만 나타나는 모델의 능력을 의미합니다.
2 일반적으로 지도 미세 조정(supervised finetuning)과 선호도 튜닝(preference tuning)의 조합을 통해 다루어지는 정렬(Alignment)은 인간 사용자의 욕구에 더 잘 부합하는 출력을 생성하도록 LLM을 미세 조정(finetuning)하는 과정을 의미합니다. 자세한 내용은 [여기](https://huggingface.co/blog/aligning-llms-with-rlhf)를 참조하십시오.
3 더 구체적으로, 현대 LLM의 출력은 자동으로 유창하고 간단한 문법/구문 오류가 없는 경향이 있으므로, 유창성 평가는 그러한 모델에게는 논쟁의 여지가 있지만 불필요합니다.
4 2년 이상 데이터 주석(annotation) 회사에서 일한 후, 저는 많은 수의 인간이 많은 양의 데이터를 정확하고 신뢰할 수 있게 주석/채점하는 것의 어려움에 정통합니다!
5 여기서 우리는 학습 중에 사용된 각 예시에는 참조 요약이 있다고 가정합니다. 그런 다음, 우리는 모델로 요약 출력을 계산하고, 참조 요약과 모델의 출력을 사용하여 ROUGE 점수를 찾은 다음, ROUGE 점수를 RL을 통한 학습을 위한 보상 신호(reward signal)로 사용할 수 있습니다.
6 기억하십시오, LLM은 출력하는 각 토큰(token)에 확률을 할당하여 작동합니다. 우리는 인간이 작성한 요약에 할당된 확률이 높기를 원합니다. 이러한 요약은 LLM이 생성할 수 있는 합리적인 출력을 나타내기 때문입니다.
7 모드 붕괴(Mode collapse)는 LLM이 출력 다양성을 잃고 특정 스타일(또는 스타일 세트)을 가진 좁은 범위의 출력만 생성하기 시작하는 현상을 의미합니다.
8 드리프트(Drift)는 단순히 미세 조정(finetune)된 모델이 일부 참조 모델(예: 미세 조정(finetuning) 과정 이전의 모델)과 너무 달라지는 것을 의미합니다.
9 [3]의 저자들은 깊이 3으로 책 요약 작업을 분해함으로써, 수천 단어의 책을 요약하면서 요약 작업의 주석(annotation) 비용을 50배 절감할 수 있다고 언급합니다.
10 이러한 프롬프트의 대부분은 OpenAI API에서 가져온 것이므로, 학습에 사용된 프롬프트와 실제 사용 사례 간에 충분한 중복이 보장됩니다.
11 의견 (또는 리뷰) 요약(Opinion (or review) summarization)은 많은 사용자가 특정 제품 또는 서비스에 대해 남긴 의견 또는 리뷰(예: 제품 리뷰 또는 식당에 대한 게시된 의견)를 요약하는 작업을 의미합니다.
12 이러한 이유로, 원본에서 텍스트를 직접 복사하는 추출적 스타일 요약은 텍스트 요약을 위한 효과적인 접근 방식임에도 불구하고 의견 요약에서는 성능이 좋지 않은 경향이 있습니다.
13 N-그램(N-gram) 참신성은 요약 내에서 생성되었지만 원본 텍스트에는 없는 n-그램(n-gram)의 비율을 의미합니다.