(출처: [2, 4, 6, 10, 29]) 대규모 언어 모델(LLM)의 역량은 최근 몇 년간 놀라운 속도로 발전했습니다. 그러나 LLM을 둘러싼 많은 핵심 아이디어들, 즉 자기 지도 사전 학습(self-supervised pretraining), 트랜스포머(transformer), 인간 피드백 기반 학습(learning from human feedback) 등은 수년 전부터 자연어 처리(natural language processing) 연구에 뿌리를 두고 있습니다. 이러한 개념들은 새로운 것이 아니라 10년 이상 관련 연구에서 축적된 아이디어들입니다. 결과적으로, 자연어 처리(natural language processing)의 핵심 문제(예: 기계 번역, 요약, 질의응답 등)에 대한 근본적인 연구는 엄청나게 중요합니다! 이 개요에서는 LLM 연구의 진화에 큰 영향을 미친 (추상적) 텍스트 요약((abstractive) text summarization) 문제에 초점을 맞춰 이 점을 설명할 것입니다. "텍스트 요약은 긴 문서(들)를 원본 문서의 가장 중요한 정보를 보존하는 짧고 유창하며 사람이 읽기 쉬운 형태로 압축하는 것을 목표로 합니다." - 출처 [11] 높은 수준에서 요약은 텍스트 시퀀스를 핵심 정보를 여전히 포착하는 훨씬 짧은 시퀀스로 압축하는 작업을 의미합니다. 요약은 일반적으로 뉴스 기사 요약과 같은 표준 문제에 적용되지만, 문제 설정은 실제로 매우 일반적이며 다양한 흥미로운 응용 분야를 포함할 수 있습니다: 검색 증강 생성(RAG)을 위한 전처리 단계로 크고 비정형적인 텍스트 시퀀스 압축. 추천 시스템의 출력에 대한 이해하기 쉬운 텍스트 요약 작성. 전사된 녹음에서 회의 요약 생성. 이 작업이 매우 일반적이고 강력하기 때문에 텍스트 요약은 실용적으로 유용한 방대한 연구에 둘러싸여 있습니다. 우리가 보게 될 것처럼, 요약 연구의 많은 핵심 아이디어들이 현대 LLM에 채택되었습니다. 이 두 연구 분야는 밀접하게 연결되어 있으며, 요약 연구에 대한 깊은 이해는 LLM이 왜 그리고 어떻게 그렇게 잘 작동하는지에 대한 새롭고 개선된 시각을 제공합니다!

**요약에 대한 유용한 배경 지식**
최근 요약 연구에 뛰어들기 전에 기본 사항을 배워야 합니다. 이 섹션에서는 요약 작업을 높은 수준에서 다루고, 존재하는 다양한 요약 유형, LLM의 대중화 이전의 요약 연구, 요약 평가 지표 등을 살펴볼 것입니다.

**요약 유형**
**추상적 요약(Abstractive summarization) vs. 추출적 요약(extractive summarization)**
요약의 목표는 더 긴 원본 문서의 핵심 아이디어를 포착하는 더 짧은 텍스트를 생성하여 정보를 핵심 구성 요소로만 압축하는 것입니다. 문헌에는 연구되는 두 가지 주요 요약 기법이 있습니다(그림은 위 참조):
*   **추출적 요약(Extractive)**: 요약은 원본 문서에서 전체 문장 또는 텍스트 구간을 선택적으로 복사하여 구성됩니다.
*   **추상적 요약(Abstractive)**: 원본 문서의 정보를 재구성하여 관련 정보에 대한 더 짧은 설명을 형성합니다.

이 두 가지 요약 전략은 널리 연구되지만, LLM으로 생성된 요약은 추상적 요약으로 간주됩니다. 왜 그럴까요? 일반적으로 LLM은 원본 문서에서 문장을 직접 복사하도록 조건화되지 않습니다. 모델은 제공된 맥락(context)에 기반하여 자유롭게 텍스트를 생성하여 원본 문서의 정보를 임의로 재구성하는 추상적 요약을 생성할 수 있습니다. 그러나 우리가 보게 될 것처럼, LLM으로 생성된 요약은 실제로는 (상대적으로) 추출적인 경향이 있습니다. 또한, 최근 연구는 i) 추출적 요약 기법을 사용하여 추상적 요약 모델 [7, 12]의 입력으로 포함할 핵심 맥락(context)을 식별하거나, 심지어 ii) LLM이 응답을 생성할 때 관련 출처(또는 텍스트 구간)를 인용하도록 가르치는 [10] 관련 주제를 탐구하기 시작했습니다(그림은 아래 참조). 이러한 이유로, 기법이 다를 수 있더라도 추출적 요약과 추상적 요약 사이에는 상당한 중복이 존재합니다.

**새로운 관점: 멀티모달 요약 (New Perspective: Multimodal Summarization)**
전통적인 텍스트 요약은 주로 텍스트 문서만을 다루지만, LLM의 발전과 함께 멀티모달(multimodal) 데이터의 요약에 대한 관심이 급증하고 있습니다. 멀티모달 요약은 이미지, 비디오, 오디오와 같은 다양한 형태의 입력에서 핵심 정보를 추출하고 이를 텍스트 또는 다른 모달리티(modality)로 요약하는 것을 목표로 합니다. 예를 들어, 긴 강의 비디오를 핵심 내용만 담은 짧은 텍스트 요약으로 변환하거나, 의료 영상과 보고서를 통합하여 환자의 상태를 요약하는 등의 응용이 가능합니다. 이는 단순히 텍스트를 처리하는 것을 넘어, 세상의 정보를 더 풍부하게 이해하고 압축하는 LLM의 능력을 확장하는 중요한 연구 분야입니다. 최근에는 비디오에서 중요한 순간을 감지하고 이를 요약하는 연구 [30]나, 이미지와 텍스트 설명을 결합하여 시각적 정보를 효과적으로 요약하는 모델 [31] 등이 활발히 연구되고 있습니다.

**LLM으로 요약 작성하기**
(출처: [6]) 이 게시물의 나머지 부분에서 보게 될 것처럼, LLM은 고품질의 추상적 요약을 생성하는 데 훌륭한 도구입니다. 요약을 위해 LLM을 사용하려면 몇 가지 접근 방식을 취할 수 있습니다(위 참조). 높은 수준에서 이러한 접근 방식은 다음 중 하나입니다:
*   순수하게 인컨텍스트 학습(in-context learning)을 사용하거나
*   요약 전용 데이터로 맞춤형 모델을 미세 조정(finetune)합니다.

즉시 사용 가능한 모델(out-of-the-box model)이 우리 애플리케이션에 잘 작동한다면, 인컨텍스트 학습(in-context learning)이 가장 간단한 접근 방식일 것입니다. 우리는 단순히 API를 호출하여(프롬프트에 일부 맥락(context)을 추가하여) 요약을 생성할 수 있습니다. 그러나 프롬프트(prompt)만으로는 충분하지 않다면, 지도 학습 방식(supervised fashion) 또는 선호도 튜닝(preference tuning)을 사용하여 맞춤형 모델을 미세 조정(finetuning)하는 것을 탐색할 수 있습니다. 이러한 각 기술의 기본 사항은 아래에 설명되어 있습니다.

**인컨텍스트 학습의 심화와 프롬프트 엔지니어링 (Deepening In-context Learning and Prompt Engineering)**
(출처: [18]) 인컨텍스트 학습(In-context learning)은 단일 기반 LLM이 프롬프트에 제공된 정보를 활용하여 다양한 다운스트림(downstream) 작업을 정확하게 해결하는 능력을 의미합니다. 초기에는 간단한 few-shot 예시를 제공하는 방식이 주를 이루었으나, 최근에는 프롬프트 엔지니어링(prompt engineering) 기술이 고도화되면서 인컨텍스트 학습의 잠재력이 더욱 극대화되고 있습니다.
*   **사고 연쇄 프롬프트(Chain-of-Thought Prompting)**: 모델이 최종 요약을 생성하기 전에 중간 추론 단계를 명시적으로 출력하도록 유도하여 요약의 품질과 사실성을 향상시킬 수 있습니다. 예를 들어, "다음 기사를 요약하되, 먼저 주요 내용을 3가지로 정리한 후 요약해 주세요."와 같이 지시할 수 있습니다.
*   **페르소나 기반 프롬프트(Persona-based Prompting)**: 특정 독자층(예: 전문가, 비전문가, 어린이)을 위한 요약을 생성할 때, 모델에 해당 페르소나를 부여하여 요약의 스타일과 복잡도를 조절하는 방식입니다. 이는 [9]에서 다룬 제어 가능한 요약(controllable summarization)의 효과적인 구현 방법 중 하나입니다.
*   **자가 개선 프롬프트(Self-Refinement Prompting)**: 모델이 초안 요약을 생성한 후, 특정 기준(예: 간결성, 사실성, 관련성)에 따라 스스로 요약을 비판하고 개선하도록 지시하는 방식입니다. 이는 LLM의 추론 능력을 활용하여 요약 품질을 반복적으로 향상시킵니다.
이러한 고급 프롬프트 기술들은 모델의 매개변수를 업데이트하지 않고도 LLM의 요약 능력을 정교하게 제어할 수 있게 하며, 다양한 요약 시나리오에 유연하게 적용될 수 있습니다.

**맞춤형 요약 모델 개발 (Developing Custom Summarization Models)**
(출처: [6]) 프롬프트만으로는 충분한 품질의 요약을 생성하지 못하거나, 특정 도메인에 특화된 고성능 요약기가 필요할 경우, LLM을 미세 조정(finetune)하는 것이 다음 단계입니다.
*   **지도 미세 조정(Supervised finetuning, SFT)**: 고품질 요약 데이터셋(문서-요약 쌍)으로 사전 학습된 LLM을 미세 조정하는 전통적인 방식입니다. 이는 여전히 강력한 기준선이며, 특히 데이터가 풍부한 도메인에서 효과적입니다. 최근에는 합성 데이터를 활용하여 SFT 데이터셋을 구축하는 연구도 활발합니다.
*   **명령어 튜닝(Instruction tuning)**: 요약을 포함한 다양한 작업 템플릿을 사용하여 단일 모델을 동시에 미세 조정하는 방식입니다. 이는 모델이 보지 못한 작업에도 잘 일반화될 수 있는 범용 요약 능력을 부여합니다.
*   **선호도 튜닝(Preference tuning) 및 RLHF의 발전**: 인간 피드백 기반 강화 학습(RLHF)은 요약 모델의 정렬(alignment)과 품질 향상에 혁혁한 공을 세웠습니다. 초기 RLHF는 보상 모델(reward model)을 학습시킨 후 PPO(Proximal Policy Optimization)와 같은 알고리즘으로 정책(policy)을 최적화하는 방식이었습니다. 최근에는 DPO(Direct Preference Optimization) [32], KTO(Kahneman-Tversky Optimization) [33] 등 보상 모델 없이 직접적으로 인간 선호도를 최적화하는 새로운 알고리즘들이 등장하여 RLHF 파이프라인을 간소화하고 있습니다. 이러한 방법들은 학습 안정성을 높이고 계산 비용을 줄이면서도 유사하거나 더 나은 성능을 달성하는 것으로 보고됩니다.

**요약을 위한 인기 데이터셋**
(출처: [20]) 요약은 일반적인 문제이지만, 문헌에서 거의 보편적으로 사용되는 몇 가지 일반적인 데이터셋이 있습니다. 요약 연구는 뉴스 요약에 중점을 두는 경향이 있습니다. 예를 들어, CNN / DailyMail 코퍼스(Corpus) [20]는 가장 널리 사용되는 데이터셋 중 하나입니다(이 데이터셋의 샘플은 위 참조). 전체적으로 이 데이터셋은 CNN과 DailyMail의 30만 개 이상의 기사와 각 기사에 대한 관련 요약을 포함합니다. 또 다른 단일 문서 추상적 요약 데이터셋인 XSum [21]도 요약 연구에서 일반적으로 사용됩니다. CNN / DailyMail과 유사한 크기(즉, 약 23만 개의 요약)를 가진 XSum은 데이터셋의 각 기사의 주요 아이디어를 포착하는 간단한 단일 문장 요약으로 구성됩니다.
“우리는 더 일반적으로 사용되는 CNN/DM 데이터셋보다 TL;DR 데이터셋을 선택했습니다. 주로 간단한 추출적 기준선(baseline)으로 CNN/DM에서 매우 강력한 성능을 달성할 수 있기 때문입니다.” - 출처 [2]
널리 사용됨에도 불구하고, CNN / DailyMail 및 XSum 데이터셋은 비교적 해결하기 쉽습니다. 때로는 더 복잡한 데이터셋이 필요합니다. 최근 연구는 Reddit의 3백만 개 게시물과 각 게시물 작성자가 작성한 요약을 포함하는 TL;DR 데이터셋 [22]을 탐구했습니다(아래 참조). 크기 때문에 이 데이터셋은 일반적으로 품질 향상을 위해 필터링되며 관련 주제의 게시물만 포함되도록 합니다(자세한 내용은 [2]의 섹션 3.2 참조).

**도메인 특화 데이터셋의 중요성 (Importance of Domain-Specific Datasets)**
일반적인 뉴스 요약 데이터셋 외에도, 특정 산업이나 응용 분야에 특화된 요약 데이터셋의 중요성이 커지고 있습니다. 예를 들어, 법률 문서 요약(예: 판결문, 계약서), 의학 보고서 요약(예: 환자 기록, 임상 연구), 금융 보고서 요약(예: 기업 실적 발표) 등은 해당 도메인의 전문 지식과 용어를 정확하게 반영해야 하는 고유한 요구사항을 가집니다. 이러한 도메인 특화 데이터셋은 다음과 같은 특징을 가집니다:
*   **전문성**: 특정 분야의 전문 용어와 개념이 풍부하게 포함되어 있습니다.
*   **길이**: 법률 문서나 의학 저널처럼 매우 긴 문서를 요약해야 하는 경우가 많습니다.
*   **정확성 및 사실성**: 잘못된 정보는 심각한 결과를 초래할 수 있으므로, 요약의 사실성이 매우 중요합니다.
이러한 데이터셋을 구축하는 것은 비용이 많이 들고 전문 인력이 필요하지만, 특정 도메인에서 LLM의 유용성을 극대화하는 데 필수적입니다. 예를 들어, CLS (Clinical Long Summarization) 데이터셋 [34]은 의학 분야의 초장문 요약 연구를 위한 데이터셋으로 활용되고 있습니다.

**요약을 어떻게 평가할 수 있을까?**
(출처: [11]) “추상적 요약을 평가하는 것은 어렵습니다. 기능성을 테스트할 수 있는 제약된 번역이나 코드 생성처럼 간단하지 않습니다.” - Eugene Yan
추상적 요약이 개방형이라는 점을 고려할 때, 요약 품질을 평가하는 것은 어려울 수 있습니다. 원본 문서를 요약하는 여러 가지 실행 가능한 방법이 있으며, 주어진 요약이 다른 요약보다 "더 낫다"고 판단하는 것은 주관적입니다! 이러한 이유로, 추상적 요약을 적절하게 평가하는 것은 우리(인간과 연구자)가 "좋은" 요약을 설명하는 일련의 기준을 수립하는 것에서 시작됩니다. 예를 들어, [23]에서는 다음 기준이 제안됩니다:
*   **유창성(Fluency)**: 요약의 문장이 읽기 쉽고 오류가 없습니다.
*   **일관성(Coherence)**: 요약 전체가 읽기 쉽고, 응집력 있으며, 합리적인 방식으로 구성되어 있습니다.
*   **관련성(Relevance)**: 요약은 원본 문서에서 가장 "중요한" 정보를 포함합니다.
*   **일치성(Consistency)**: 요약의 정보가 정확하고 원본 문서와 일치합니다(즉, 환각(hallucination)이나 잘못된 정보가 없습니다).

그러나 이것이 우리가 정의할 수 있는 유일한 기준 세트는 아닙니다! 논쟁의 여지가 있지만, 유창성(fluency)은 현대 LLM 3에 의해 대부분 해결되었으며, 우리가 중요하게 생각하는 기준은 해결하려는 사용 사례에 따라 달라질 수 있습니다. 예를 들어, [7]의 저자들은 사용자로부터의 의견이나 리뷰(예: Yelp)를 요약하는 데 더 적합한 충실도(faithfulness), 사실성(factuality), 일반성(genericity)을 포함하는 대안적인 기준 세트를 고안합니다. 좋은 요약의 바람직한 특성을 정의하는 것이 평가 과정의 첫 번째 단계입니다. 이것이 명확해지면, 성능을 측정하고 반복함으로써 더 나은 추상적 요약 모델을 개발하는 것에 대해 생각하기 시작할 수 있습니다.

**인간 평가(Human evaluation).** 추상적 요약 품질을 평가하기 위한 많은 자동 전략이 있지만, 인간 평가(human evaluation)는 가장 신뢰할 수 있는 평가 접근 방식이며, 요약 연구 전반의 품질 평가를 위한 "정답(ground truth)" 역할을 합니다. 모델의 품질을 진정으로 알기 위해서는 인간 평가(human evaluation)를 거쳐야 합니다. 그럼에도 불구하고, 인간 평가(human evaluation)는 만능 해결책이 아닙니다! 인간으로부터 정확하고 신뢰할 수 있으며 일관된 품질 레이블을 얻는 것은 극도로 어렵습니다 4. 특히 추상적 요약과 같은 주관적인 작업에서는 더욱 그렇습니다. 인간은 끊임없이 서로 의견이 다릅니다(요약 품질 외에도 더 많은 것에 대해!). 이는 평가 과정을 상당히 노이즈가 많게 만들 수 있습니다. 이러한 문제를 완화하기 위해, 우리는 플라이스 카파(Fleiss’ kappa) 및 크리펜도르프 알파(Krippendorff's alpha)와 같은 지표를 사용하여 인간 주석자(annotator) 간의(또는 주석자(annotator)와 연구자 간의) 일치도 수준을 모니터링할 수 있습니다. 최근에는 인간 평가의 효율성을 높이기 위해 다중 레이블러(multi-annotator) 시스템을 구축하고, 평가 가이드라인을 더욱 정교하게 설계하는 연구가 진행되고 있습니다.

**전통적인 (자동) 지표.** 인간 평가(human evaluation)가 요약 품질의 정답(ground truth) 역할을 하지만, 인간 품질 평가를 수집하는 것은 비용이 많이 들고 시간이 소모되기 때문에 순전히 인간 평가(human evaluation)에만 의존할 수는 없습니다. 우리는 인간 평가(human evaluation) 시도 사이에 모델을 더 빠르게 반복 개발할 수 있도록 하는 자동 지표가 필요합니다. 먼저, 요약 작업을 위한 더 전통적인 자동 평가 지표를 살펴보겠습니다. 이들은 두 가지 범주로 나뉩니다:
*   참조 기반(Reference-based)
*   참조 불필요(Reference-free) (또는 맥락 기반(context-based))

참조 기반(Reference-based) 지표는 요약 품질을 측정하는 데 사용할 수 있는 목표 또는 참조 요약(일반적으로 인간이 작성한)이 있다고 가정하는 반면, 참조 불필요(Reference-free) 지표는 생성된 요약과 원본 문서에만 순수하게 기반하여 요약 품질을 평가합니다. 요약을 위한 가장 일반적으로 사용되는 평가 지표는 ROUGE(Recall-Oriented Understudy for Gisting Evaluation) 점수입니다. 이 점수는 참조 요약에 있고 모델이 생성한 출력에도 나타나는 단어 수(또는 ROUGE-N의 n-그램(n-gram) 수)를 단순히 세는 방식으로 작동합니다(아래 참조). ROUGE는 참조 요약과 출력 요약 간의 중복을 측정하는 참조 기반(reference-based) 지표입니다. ROUGE 외에도 요약 품질을 계산하기 위해 유사한 전략을 사용하는 많은 다른 참조 기반(reference-based) 지표가 있습니다:
*   **BLEU(Bilingual Evaluation Understudy) 점수 [25]**: 생성된 출력과 참조 요약 간의 일치하는 n-그램(n-gram) 수를 세고, 이 수를 생성된 출력 내의 총 n-그램(n-gram) 수로 나누어 번역 작업을 평가하는 데 일반적으로 사용됩니다.
*   **BERTScore [26]**: 생성된 출력과 참조 출력의 각 n-그램(n-gram)에 대해 (BERT를 사용하여) 임베딩(embedding)을 생성한 다음, 코사인 유사도(cosine similarity)를 사용하여 두 텍스트 시퀀스의 n-그램(n-gram)을 비교하여 정확한 일치 대신 n-그램(n-gram) 간의 의미론적 일치(semantic match)를 가능하게 합니다.
*   **MoverScore [27]**: n-그램(n-gram) 간의 일대일 매칭을 요구하는 BERTScore를 다대일 매칭을 허용하도록 일반화하여 평가 프레임워크를 더 유연하게 만듭니다.

특정 경우에 참조 기반(reference-based) 지표는 바람직하지 않을 수 있습니다. 예를 들어, 우리의 참조 요약이 저품질이거나, 참조 요약에 전혀 접근할 수 없을 수도 있습니다! 이러한 경우를 처리하기 위해, 우리는 참조 요약 대신 출력 요약을 원본 문서와 비교하여 ROUGE의 맥락 기반(context-based) 버전인 ROUGE-C [24]를 도출할 수 있습니다(아래 참조).

(출처: [24]) 동일한 전략을 사용하여 BERTScore 및 MoverScore의 참조 불필요(reference-free) 변형도 생성할 수 있습니다! 존재하는 다양한 참조 불필요(reference-free) 및 참조 기반(reference-based) 요약 지표에 대한 자세한 내용은 이 논문을 확인하십시오.
“최근 연구들은 대규모 언어 모델(LLM)을 NLG 평가를 위한 참조 불필요(reference-free) 지표로 사용할 것을 제안합니다. 이는 인간 참조가 부족한 새로운 작업에 적용할 수 있다는 이점이 있습니다.” - 출처 [29]

**LLM-as-a-Judge.** LLM 출력(추상적 요약 작업 포함)을 평가하기 위한 인기 있는 전략 중 하나는 LLM-as-a-Judge [28]입니다. 이는 강력한 LLM(예: GPT-4)을 평가에 사용합니다. 생성된 출력을 평가하거나 점수를 매기기 위해, 우리는 단순히 LLM에 프롬프트(prompt)를 제공합니다! 이는 몇 가지 다른 방식으로 수행될 수 있습니다:
*   생성된 출력 쌍 내에서 선호되는 출력을 식별하도록 LLM에 요청(아래 참조).
*   프롬프트에 명시된 기준에 따라 단일 생성된 출력에 대해 (지정된 범위 내의) 스칼라 점수(scalar score)를 생성하도록 LLM에 요청.
*   정확한 채점을 시연하는 몇 가지 퓨샷(few-shot) 예시를 기반으로 생성된 출력을 평가하도록 LLM에 요청.

(출처: [28]) LLM-as-a-judge는 LLM의 출력을 평가하기 위한 새롭고 강력하며 참조 불필요(reference-free) 전략입니다. 그러나 이 평가 접근 방식은 몇 가지 형태의 편향(bias)을 도입합니다:
*   **위치 편향(Position bias)**: 모델의 프롬프트 내에서 생성된 출력의 위치가 결과 점수에 영향을 미칠 수 있습니다. 이를 해결하기 위해 우리는 무작위로 샘플링된 위치로 여러 점수를 생성하고 그 평균을 취할 수 있습니다.
*   **장황함 편향(Verbosity bias)**: GPT-4와 같은 모델은 더 장황한 출력을 선호하는 경향이 있습니다. 우리는 생성된 출력의 길이를 정규화하여 이를 해결할 수 있습니다.
*   **자기 강화 편향(Self-enhancement bias)**: GPT-4 및 기타 모델은 자신의 출력에 다른 모델의 출력보다 더 높은 점수를 부여하는 경향이 있으므로, 어떤 LLM이 자신의 생성을 채점하는 데 사용될 때는 주의해야 합니다!
*   **제한된 능력**: LLM은 완벽하지 않습니다! 따라서 LLM-as-a-Judge를 사용하여 판사 자체가 해결하기 어려워하는 문제(예: 복잡한 수학 또는 추론 문제)에 대한 모델의 출력을 채점할 때 한계에 부딪힐 수 있습니다.

이러한 편향(bias)에도 불구하고, LLM-as-a-judge 스타일의 평가는 놀랍도록 견고하며 다양한 응용 분야에서 인간 평가와 잘 상관관계가 있어 최근 연구에서 광범위하게 채택되고 있습니다(요약 및 그 이상). [29]에서 저자들은 사고 연쇄 프롬프트(chain of thought prompting) 및 양식 작성(form-filling)으로 LLM 기반 평가를 증강하여, 특히 요약 작업의 평가 품질을 향상시키는 G-Eval이라는 새로운 평가 전략을 만들었습니다(아래 참조).

**보상 모델(Reward models).** 위에서 논의했듯이, 요약 모델 학습을 위한 가장 효과적인 전략 중 하나인 선호도 튜닝(preference tuning)은 인간 선호도 데이터셋으로 보상 모델(reward model)을 학습시키는 것을 포함합니다. 이 모델의 출력은 RL을 사용한 미세 조정(finetuning)을 위한 보상 신호로 사용되지만, 동일한 보상 신호는 품질 평가 목적으로 재활용될 수 있습니다! 보상 모델(reward model)은 생성된 요약을 입력으로 받아 이 요약에 대한 인간 선호도 점수(human preference score)를 예측합니다. 따라서 보상 모델(reward model)의 출력은 인간 선호도의 대리 지표(proxy)이며, 이는 참조 불필요(reference-free) 품질 평가로 직접 사용될 수 있습니다. 최근에는 RewardBench [35]와 같은 벤치마크가 등장하여 보상 모델의 평가 능력을 체계적으로 측정하고 비교하는 데 기여하고 있습니다.

**새로운 평가 패러다임: 사실성 및 일관성 측정 (New Evaluation Paradigm: Factuality and Consistency Measurement)**
LLM이 생성하는 요약의 유창성은 크게 향상되었지만, 환각(hallucination) 문제, 즉 원본 문서에 없는 정보를 생성하거나 사실과 다른 내용을 포함하는 문제는 여전히 중요한 해결 과제입니다. 특히 의료, 법률, 금융과 같이 정확성이 필수적인 도메인에서는 요약의 사실성(factuality)과 원본 문서와의 일관성(consistency)을 평가하는 것이 매우 중요합니다. 이를 위해 다음과 같은 새로운 평가 방법론이 주목받고 있습니다:
*   **질의응답 기반 평가 (Question Answering-based Evaluation)**: 요약과 원본 문서에 대해 질문을 생성하고, 모델이 요약에서 답변을 정확하게 추출할 수 있는지, 그리고 그 답변이 원본 문서의 내용과 일치하는지 확인하는 방식입니다 [16, 17].
*   **정보 추출 기반 평가 (Information Extraction-based Evaluation)**: 요약과 원본 문서에서 핵심 개체(entity)와 관계를 추출한 다음, 이들이 서로 일치하는지 비교하여 정보의 누락이나 왜곡을 감지합니다.
*   **프롬프트 기반 일관성 검사 (Prompt-based Consistency Checks)**: LLM-as-a-Judge와 유사하게, 강력한 LLM에 요약과 원본 문서를 제공하고 특정 기준(예: "요약에 원본 문서에 없는 정보가 포함되어 있는가?")에 따라 일관성 점수를 부여하도록 요청하는 방식입니다.

이러한 방법들은 단순한 텍스트 중복을 넘어, 요약의 의미론적 정확성과 신뢰성을 더 깊이 평가하려는 노력의 일환입니다.

**인간 피드백으로 요약 개선**
“학습 시 요약기가 참조 요약을 재현하도록 하는 지도 학습 패러다임과 비교하여, RL은 요약기가 생성된 요약의 품질을 측정하는 보상(reward)을 최대화하도록 직접 최적화합니다.” - 출처 [1]
오랫동안 요약 모델 학습을 위한 최첨단 접근 방식은 고품질 참조 요약 데이터셋으로 사전 학습된 기본 모델을 지도 미세 조정(supervised finetuning)하는 것이었습니다. 이 접근 방식은 효과적이지만, 이 섹션에서 보게 될 것처럼 선호도 튜닝(preference tuning)을 통해 더 나은 결과를 얻을 수 있습니다. 인간 피드백은 훨씬 더 나은 요약 모델을 학습시킬 수 있도록 합니다. 그러나 이러한 연구는 요약 주제를 넘어섭니다. 유사한 기술들이 최근 LLM 정렬(alignment) 연구에 의해 재활용되어 LLM 학습 파이프라인의 기반을 형성했습니다.
**더 나은 보상이 더 나은 요약을 만든다 [1]**

(출처: [1]) 지도 학습은 원래 요약 모델 학습을 위한 가장 일반적으로 사용되는 패러다임이었습니다. 우리는 단순히 인간이 작성한 참조 요약을 모방하도록 모델을 학습시켰습니다. 최근에는 연구자들이 요약 모델 학습을 위한 RL 사용을 탐색하기 시작했습니다. 초기 시도는 ROUGE 점수를 보상(reward) 5으로 직접 사용했지만, ROUGE 점수는 인간 품질 평가와 상관관계가 낮습니다(위 참조). 따라서 [1]의 저자들은 RL을 통해 모델을 인간이 더 선호하는 요약으로 유도하는 더 나은 보상 함수(reward function)를 찾으려고 시도합니다.
“인간에게 매력적인 요약을 생성하기 위한 더 나은 보상 함수(reward function)를 찾기 위해, 우리는 2,500개의 요약에 대한 인간 평가로부터 보상 함수(reward function)를 학습합니다.” - 출처 [1]

**보상 학습.** [1]의 저자들은 인간 선호도 데이터셋으로부터 보상 함수(reward function)를 학습할 것을 제안합니다. 이전 연구에서 가져온 이 데이터셋은 CNN / DailyMail 코퍼스(corpus)의 500개 뉴스 기사에 대한 2,500개의 요약(인간 평가 포함)을 포함합니다. 이 데이터를 사용하여, 우리는 문서와 시스템 요약을 입력으로 받아 인간 평가를 예측하도록 보상 모델(reward model)을 학습시킬 수 있습니다. ROUGE를 보상(reward)으로 사용하는 기술과 달리, 보상 계산에 참조 요약이 필요하지 않습니다! 보상 모델(reward model)을 학습시키기 위해, 우리는 회귀 목표(regression objective) 또는 선호도 학습 목표(preference learning objective)를 사용할 수 있습니다. 후자는 보상 함수(reward function)가 인간이 선호하는 요약을 정확하게 식별하는지 여부를 포착합니다(아래 참조).

(출처: [1]) [1]에서 보상 함수(reward function)에 대해 여러 아키텍처(architecture)가 고려되었지만, 최상의 결과를 도출하는 접근 방식은 요약과 입력 문서의 (BERT를 사용하여 생성된) 연결된 임베딩(embedding)을 입력으로 받는 피드포워드 네트워크(feed-forward network)입니다. [1]에서 우리는 선호도 학습 목표(preference learning objective)가 회귀 목표(regression objective)에 비해 더 나은 결과를 도출한다는 것을 알 수 있으며, 이는 선호도 학습 목표(preference learning objective)가 이제 보상 모델(reward model) 학습에 거의 보편적으로 사용되는 이유를 설명합니다. 아래 표에서 우리는 최상의 보상 함수(reward function)가 다음을 사용한다는 것을 알 수 있습니다:
*   임베딩(embedding)을 위한 BERT.
*   보상 예측을 위한 피드포워드 네트워크(feed-forward network) (또는 MLP).
*   선호도 기반 학습 목표.
이 보상 함수(reward function)는 인간 판단과 잘 상관관계가 있는 예측을 생성하고 높은 재현율(recall)과 정밀도(precision)로 "좋은" 요약을 식별하는 것으로 나타났습니다.

(출처: [1]) **더 나은 요약 모델.** 인간 선호도를 정확하게 예측하는 것을 넘어, [1]에서 학습된 보상 모델(reward model)은 더 나은 추출적 및 추상적 요약 모델을 만드는 데 사용될 수 있습니다. 지도 학습 기준선(baseline)과 ROUGE를 보상(reward)으로 사용하여 RL로 학습된 모델 모두와 비교했을 때, 인간 피드백으로부터 학습된 보상(reward)으로 학습된 모델은 훨씬 더 높은 인간 평가 점수를 가진 요약을 생성하는 것으로 나타났습니다(이전 최첨단 시스템보다도 높습니다!). (아래 참조).

(출처: [1]) 간단히 말해, [1]에서 우리는 인간 피드백으로부터 보상 함수(reward function)를 학습하는 것이 RL 기반 요약 모델 학습을 위한 우수한 학습 신호를 제공할 수 있다는 것을 알 수 있습니다. 후속 연구는 이 교훈을 채택하고 이를 LLM 영역으로 확장하여 인간 피드백으로부터 다양한 작업(요약 포함)을 학습합니다.

(출처: [2]) **인간 피드백으로부터 학습.** [2]에서 저자들은 인간의 선호도 데이터에 기반하여 LLM을 미세 조정(finetune)할 수 있도록 하는 세 부분으로 구성된 프레임워크를 제안함으로써 정확히 이 작업을 수행합니다(위 참조). LLM은 먼저 인간 참조 요약에 대한 지도 미세 조정(supervised finetuning)을 사용하여 학습되어 지도 학습 기준선(baseline)을 생성하고, 이 기준선은 다시 RL로 추가 미세 조정(finetune)됩니다. [2]의 RLHF 과정은 다음을 통해 인간 피드백 데이터셋을 수집하는 것으로 시작됩니다:
*   학습 데이터셋에서 텍스트 입력(원본 문서) 가져오기.
*   여러 정책(policy)(예: 사전 학습된 모델, 지도 학습 기준선(baseline), 현재 모델 또는 인간 참조 요약)을 사용하여 입력의 요약 샘플링.
*   샘플 응답 세트에서 두 요약 선택.
*   인간 주석자(annotator)에게 두 요약 중 더 나은 것을 식별하도록 요청.
인간 비교 데이터는 대량으로 수집되며 오프라인 방식으로 RLHF를 통해 모델(디코더 전용 LLM)을 미세 조정(finetune)하는 데 사용됩니다. 데이터가 수집되면, 이 비교 데이터를 사용하여 LLM이 생성한 요약이 주어졌을 때 인간 선호도 점수(human preference score)를 정확하게 예측하는 보상 모델(reward model)을 학습시킵니다. 여기에서 우리는 RL을 사용하여 모델을 미세 조정(finetune)합니다( [2]의 저자들은 PPO 알고리즘을 사용합니다). 보상 모델(reward model)이 출력한 선호도 점수에 기반합니다.
**PPO의 최적화 목표에 KL 발산(KL divergence) 추가**

(출처: [2]) **드리프트(drift) 방지.** [2]의 저자들은 PPO에 의해 최적화되는 목표에 KL 발산(KL divergence) 항을 추가합니다. 이는 RLHF 동안 정책(policy)이 지도 학습 기준선(supervised baseline) 정책과 너무 달라지는 것을 페널티(penalize)합니다(위 참조). 이제 일반적으로 사용되는 이러한 접근 방식(예: LLaMA-2 보고서의 Eq. 4 참조)은 모드 붕괴(mode collapse) 7 없이 탐색을 장려하고 LLM이 작성한 요약이 학습 중에 본 것과 너무 달라지는 것을 방지합니다.

(출처: [2]) **피드백으로부터의 학습은 효과적인가?** [2]의 저자들은 위에서 설명한 전략을 사용하여 TL;DR 데이터셋에 대해 13억에서 67억 개의 매개변수(parameter)를 가진 여러 GPT 스타일 언어 모델을 미세 조정(finetune)합니다. 인간 피드백을 통해 학습된 모델은 지도 학습만으로 학습된 모델이 작성한 요약보다 인간에게 일관되게 선호되는 요약을 생성하는 것으로 나타났습니다(위 참조). 13억 개의 인간 피드백 모델은 지도 학습만으로 학습된 10배 더 큰 모델보다 성능이 우수하며, 67억 개의 인간 피드백 모델은 13억 모델보다도 더 나은 성능을 보입니다. 즉, 요약 품질은 모델 규모(model scale)로부터 이점을 얻습니다.

(출처: [2]) 인간 피드백으로 학습된 요약 모델은 새로운 도메인에 더 잘 일반화(generalize)되는 것으로 보입니다. 예를 들어, [2]에서 TL;DR에 대해 미세 조정(finetune)된 모델은 추가 미세 조정(finetuning) 없이 뉴스 중심 데이터셋에서 잘 작동하는 것으로 나타났습니다(위 참조).
“우리는 우리의 보상 모델(reward model)이 인간 선호도 예측에서 ROUGE와 같은 다른 지표보다 성능이 우수하며, 우리의 보상 모델(reward model)을 직접 최적화하는 것이 인간에 따르면 ROUGE를 최적화하는 것보다 더 나은 요약을 생성한다는 것을 확인합니다.” - 출처 [1]

**RLHF의 발전과 새로운 정렬 기법 (Advancements in RLHF and New Alignment Techniques)**
[2]와 [3]의 연구 이후, RLHF는 LLM 정렬(alignment)의 표준이 되었고, 이 분야는 지속적으로 발전하고 있습니다. PPO는 효과적이지만 복잡성과 불안정성이라는 단점이 있었습니다. 이에 따라 다음과 같은 새로운 접근 방식들이 등장했습니다:
*   **DPO (Direct Preference Optimization) [32]**: 보상 모델을 명시적으로 학습시키지 않고, 선호도 데이터를 사용하여 정책 모델의 로짓(logits)을 직접 최적화하는 방법입니다. PPO보다 구현이 간단하고 학습이 안정적이며, 유사하거나 더 나은 성능을 보여줍니다.
*   **KTO (Kahneman-Tversky Optimization) [33]**: DPO와 유사하게 보상 모델 없이 직접 정책을 최적화하지만, 인간의 비대칭적인 선호(이득과 손실에 대한 다른 반응)를 모델링하는 Kahneman-Tversky 이론을 적용하여 모델이 인간 선호도를 더 잘 반영하도록 합니다.
*   **RLAIF (Reinforcement Learning from AI Feedback)**: 인간 피드백 대신, 강력한 LLM(예: GPT-4)이 생성한 피드백을 사용하여 보상 모델을 학습시키거나 직접 정책을 최적화하는 방식입니다. 이는 인간 주석의 비용과 시간을 줄이면서도 효과적인 정렬을 가능하게 합니다 [36].

이러한 발전은 요약 모델을 포함한 LLM이 인간의 의도와 가치에 더욱 부합하는 출력을 생성하도록 돕는 중요한 기술적 진보를 의미합니다.

**LLM 시대의 요약**
InstructGPT가 요약에 중점을 두지 않았다는 점을 고려할 때, 이 논문이 왜 이 개요에 포함되었는지 궁금할 수 있습니다. 그러나 InstructGPT는 단일 유형의 작업을 해결하는 데 특화된 모델인 좁은 전문가(narrow expert)에서 벗어나 다양한 작업을 정확하게 해결할 수 있는 모델인 기반 모델(foundation model)로의 NLP 연구의 패러다임 전환을 나타냅니다. 추상적 요약에 대한 연구는 기반 모델(foundation model)에 대한 후속 연구의 시작점을 제공했습니다. 그러나 시간이 지남에 따라 연구자들은 특히 추상적 요약 문제에 덜 집중하기 시작했으며, 대신 더 나은 기반 모델(foundation model)을 만드는 데 집중했습니다. 이러한 모델에게 추상적 요약은 정확하게 해결할 수 있는 많은 작업 중 하나일 뿐입니다.

**초장문 컨텍스트 요약의 도전과 기회 (Challenges and Opportunities in Long-Context Summarization)**
LLM의 가장 최근 발전 중 하나는 수십만 토큰에 달하는 초장문 컨텍스트(long context)를 처리할 수 있는 능력입니다(예: Gemini 1.5 Pro의 100만 토큰, Claude 3 Opus의 20만 토큰). 이는 전체 책, 법률 문서 모음, 수십 년간의 의료 기록 등 방대한 양의 정보를 한 번에 입력으로 받아 요약할 수 있는 가능성을 열어주었습니다.
*   **도전 과제**: 초장문 컨텍스트 내에서 핵심 정보를 정확히 식별하고, 복잡한 인과 관계나 논리적 흐름을 유지하며 요약하는 것은 여전히 어려운 문제입니다. 또한, 긴 입력에서 "잃어버린 중간(lost in the middle)" 현상(즉, 입력의 시작이나 끝 부분의 정보가 더 잘 기억되는 경향)과 같은 문제가 발생할 수 있습니다.
*   **기회**: 초장문 컨텍스트 LLM은 법률 분석, 학술 연구, 의학 진단 등 전문가 도메인에서 정보 과부하 문제를 해결하는 데 혁명적인 역할을 할 수 있습니다. 예를 들어, 수백 페이지 분량의 연구 논문을 핵심 요약으로 압축하거나, 복잡한 법률 사례의 모든 관련 문서를 분석하여 핵심 쟁점을 요약하는 것이 가능해집니다. 이를 통해 전문가들은 시간을 절약하고 더 나은 의사 결정을 내릴 수 있습니다.

**멀티모달 LLM과 요약 (Multimodal LLMs and Summarization)**
텍스트 기반 LLM의 발전을 넘어, 이미지, 오디오, 비디오 등 다양한 모달리티를 이해하고 생성할 수 있는 멀티모달 LLM(예: GPT-4o, Gemini)의 등장은 요약 분야에 새로운 지평을 열었습니다.
*   **통합 요약**: 멀티모달 LLM은 회의 비디오를 보고 참석자들의 발언을 텍스트로 요약하는 것을 넘어, 발표 자료의 이미지나 그래프를 분석하여 요약에 포함시키거나, 비언어적 단서(예: 표정, 제스처)를 기반으로 회의 분위기를 요약할 수 있습니다.
*   **크로스-모달 요약**: 특정 모달리티의 정보를 다른 모달리티로 요약하는 것도 가능합니다. 예를 들어, 복잡한 데이터 시각화(차트, 그래프)를 이해하기 쉬운 텍스트 요약으로 변환하거나, 긴 오디오 클립을 핵심 내용만 담은 짧은 오디오 요약으로 생성할 수도 있습니다.
이러한 멀티모달 요약 능력은 정보 접근성을 높이고, 다양한 형태의 데이터를 더 효율적으로 소비할 수 있게 합니다.

**요약의 윤리적 고려사항 (Ethical Considerations in Summarization)**
LLM 기반 요약의 광범위한 적용 가능성만큼이나, 윤리적 고려사항 또한 중요하게 다루어져야 합니다.
*   **편향(Bias) 및 공정성(Fairness)**: LLM은 학습 데이터에 내재된 편향을 요약에 반영할 수 있습니다. 예를 들어, 특정 집단에 대한 편향된 시각을 가진 뉴스 기사를 요약할 때, 그 편향이 요약에 그대로 드러나거나 심화될 수 있습니다. 이는 특히 사회적으로 민감한 주제나 다양한 의견이 존재하는 상황에서 심각한 문제를 야기할 수 있습니다.
*   **환각(Hallucination) 및 잘못된 정보**: 요약의 사실성 부족은 잘못된 정보의 확산으로 이어질 수 있습니다. 특히 의료, 법률, 금융과 같은 분야에서는 환각이 치명적인 결과를 초래할 수 있으므로, 요약의 신뢰성을 보장하기 위한 추가적인 검증 메커니즘이 필수적입니다.
*   **투명성(Transparency) 및 설명 가능성(Explainability)**: LLM이 특정 방식으로 요약한 이유를 이해하기 어렵다는 점은 문제 해결을 어렵게 만듭니다. 요약의 출처를 명확히 밝히거나, 요약에 사용된 핵심 문장이나 정보 흐름을 시각화하는 등의 노력이 필요합니다.
*   **저작권 및 오용**: 원본 콘텐츠의 저작권 문제, 그리고 LLM 요약이 원본 콘텐츠를 대체하거나 왜곡하여 오용될 가능성도 고려해야 합니다.

이러한 윤리적 문제들을 해결하기 위한 기술적, 정책적 노력이 LLM 기반 요약 연구의 중요한 부분으로 부상하고 있습니다.

(출처: [9]) **결론**
우리는 이제 지도 미세 조정(supervised finetuning)에서 선호도 튜닝(preference tuning), 현대 기반 모델(foundation model)에 이르기까지 여러 세대의 요약 연구를 살펴보았습니다. 이 작업에서 몇 가지 공통 주제가 나타납니다:
*   평가의 어려움 (및 중요성).
*   인간 피드백으로부터 학습의 가치.
*   데이터 품질의 중요성.
*   현대 기반 모델(foundation model)의 인상적인 능력.
이 개요는 특히 요약 연구에 중점을 두었지만, 이 작업의 발견은 매우 일반화(generalize) 가능합니다. 요약은 조건부 생성(conditional generation)(즉, 특정 입력이 주어졌을 때 출력을 생성하도록 모델 교육)에 뿌리를 두고 있기 때문에 자연어 처리(natural language processing)의 근본적인 작업입니다. 기계 번역, 텍스트 분류, 키워드 추출, 질의응답 등 다른 많은 중요한 작업들도 매우 유사한 패턴을 따릅니다! 요약 연구에 대한 깊은 이해는 훨씬 더 광범위한 문제를 해결하는 데 도움이 됩니다.
“우리는 인간 피드백 기반 강화 학습(RLHF; Stiennon et al., 2020)을 사용하여 GPT-3를 미세 조정(fine-tune)하여 광범위한 서면 명령어를 따르도록 합니다.” - 출처 [5]
자연어 처리(natural language processing) 연구 내에서 요약의 근본적인 역할은 이러한 기술들이 LLM 시대에 크게 채택되도록 이끌었습니다. 언어 모델링 연구의 다양한 영향력 있는 접근 방식은 요약 논문에 크게 영향받았습니다! 예를 들어, InstructGPT는 인간 피드백으로부터 더 나은 요약 모델을 학습시키기 위해 이전에 제안된 학습 알고리즘 [2]을 채택하는 반면, 현대 정렬(alignment) 절차는 이전 요약 연구 [3]에서 옹호된 반복적인 미세 조정(finetuning) 전략을 사용합니다. 이 개요에서 제시된 연구는 실용적으로 유용합니다. 그러나 더 중요한 것은 오늘날 LLM에서 볼 수 있는 발전을 위한 토대를 마련한다는 것입니다.

뉴스레터가 처음이신가요? 안녕하세요! 저는 Cameron R. Wolfe입니다. 넷플릭스(Netflix)의 딥러닝(Deep Learning) 박사이자 머신러닝(Machine Learning) 과학자입니다. 이것은 딥(러닝) 포커스 뉴스레터이며, 독자들이 AI 연구의 중요한 주제를 더 잘 이해하도록 돕습니다. 뉴스레터가 마음에 드셨다면, 구독, 공유 또는 X 및 LinkedIn에서 저를 팔로우해주세요! 구독

**참고 문헌**
[1] Böhm, Florian, et al. "Better rewards yield better summaries: Learning to summarise without references." arXiv preprint arXiv:1909.01214 (2019).
[2] Stiennon, Nisan, et al. "Learning to summarize with human feedback." Advances in Neural Information Processing Systems 33 (2020): 3008-3021.
[3] Ziegler, Daniel M., et al. "Fine-tuning language models from human preferences." arXiv preprint arXiv:1909.08593 (2019).
[4] Wu, Jeff, et al. "Recursively summarizing books with human feedback." arXiv preprint arXiv:2109.10862 (2021).
[5] Ouyang, Long, et al. "Training language models to follow instructions with human feedback." Advances in neural information processing systems 35 (2022): 27730-27744.
[6] Goyal, Tanya, Junyi Jessy Li, and Greg Durrett. "News summarization and evaluation in the era of gpt-3." arXiv preprint arXiv:2209.12356 (2022).
[7] Bhaskar, Adithya, Alexander R. Fabbri, and Greg Durrett. "Prompted opinion summarization with GPT-3.5." arXiv preprint arXiv:2211.15914 (2022).
[8] Zhang, Tianyi, et al. "Benchmarking large language models for news summarization." Transactions of the Association for Computational Linguistics 12 (2024): 39-57.
[9] Pu, Dongqi, and Vera Demberg. "ChatGPT vs human-authored text: Insights into controllable text summarization and sentence style transfer." arXiv preprint arXiv:2306.07799 (2023).
[10] Menick, Jacob, et al. "Teaching language models to support answers with verified quotes." arXiv preprint arXiv:2203.11147 (2022).
[11] Fabbri, Alexander R., et al. "Summeval: Re-evaluating summarization evaluation." Transactions of the Association for Computational Linguistics 9 (2021): 391-409.
[12] Subramanian, Sandeep, et al. "On extractive and abstractive neural document summarization with transformer language models." arXiv preprint arXiv:1909.03186 (2019).
[13] Zhang, Fang-Fang, Jin-ge Yao, and Rui Yan. "On the abstractiveness of neural document summarization." Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing . 2018.
[14] Kryściński, Wojciech, et al. "Evaluating the factual consistency of abstractive text summarization." arXiv preprint arXiv:1910.12840 (2019).
[15] Maynez, Joshua, et al. "On faithfulness and factuality in abstractive summarization." arXiv preprint arXiv:2005.00661 (2020).
[16] Durmus, Esin, He He, and Mona Diab. "FEQA: A question answering evaluation framework for faithfulness assessment in abstractive summarization." arXiv preprint arXiv:2005.03754 (2020).
[17] Wang, Alex, Kyunghyun Cho, and Mike Lewis. "Asking and answering questions to evaluate the factual consistency of summaries." arXiv preprint arXiv:2004.04228 (2020).
[18] Brown, Tom, et al. "Language models are few-shot learners." Advances in neural information processing systems 33 (2020): 1877-1901.
[19] Wei, Jason, et al. "Finetuned language models are zero-shot learners." arXiv preprint arXiv:2109.01652 (2021).
[20] Nallapati, Ramesh, et al. "Abstractive text summarization using sequence-to-sequence rnns and beyond." arXiv preprint arXiv:1602.06023 (2016).
[21] Narayan, Shashi, Shay B. Cohen, and Mirella Lapata. "Don't give me the details, just the summary! topic-aware convolutional neural networks for extreme summarization." arXiv preprint arXiv:1808.08745 (2018).
[22] Völske, Michael, et al. "Tl; dr: Mining reddit to learn automatic summarization." Proceedings of the Workshop on New Frontiers in Summarization . 2017.
[23] Kryściński, Wojciech, et al. "Neural text summarization: A critical evaluation." arXiv preprint arXiv:1908.08960 (2019).
[24] He, Tingting, et al. "ROUGE-C: A fully automated evaluation method for multi-document summarization." 2008 IEEE International Conference on Granular Computing . IEEE, 2008.
[25] Papineni, Kishore, et al. "Bleu: a method for automatic evaluation of machine translation." Proceedings of the 40th annual meeting of the Association for Computational Linguistics. 2002.
[26] Zhang, Tianyi, et al. "Bertscore: Evaluating text generation with bert." arXiv preprint arXiv:1904.09675 (2019).
[27] Zhao, Wei, et al. "MoverScore: Text generation evaluating with contextualized embeddings and earth mover distance." arXiv preprint arXiv:1909.02622 (2019).
[28] Zheng, Lianmin, et al. "Judging llm-as-a-judge with mt-bench and chatbot arena." Advances in Neural Information Processing Systems 36 (2024).
[29] Liu, Yang, et al. "Gpteval: Nlg evaluation using gpt-4 with better human alignment." arXiv preprint arXiv:2303.16634 (2023).
[30] Chen, Yubo, et al. "A survey on video summarization." IEEE Transactions on Pattern Analysis and Machine Intelligence (2021).
[31] Gao, Xiaohong, et al. "Image captioning with multimodal transformer." Neural Processing Letters (2023).
[32] Rafailov, Rafael, et al. "Direct preference optimization: Your language model is secretly a reward model." Advances in Neural Information Processing Systems 36 (2024).
[33] Ethayarajh, Kawin, and Dan Jurafsky. "Kahneman-tversky optimization: Learning from human preferences by minimizing regret." arXiv preprint arXiv:2403.07542 (2024).
[34] Yang, Yichong, et al. "CLS: A clinical long summarization dataset." arXiv preprint arXiv:2402.04944 (2024).
[35] Luo, Yuan, et al. "RewardBench: A unified benchmark for reward models." arXiv preprint arXiv:2402.09659 (2024).
[36] Bai, Yuntao, et al. "Constitutional AI: Harmlessness from AI feedback." arXiv preprint arXiv:2212.08073 (2022).

1 "발현적 능력(emergent capability)"이란 특정 규모(데이터 또는 컴퓨팅 측면에서)가 탐색된 후에만 나타나는 모델의 능력을 의미합니다.
2 일반적으로 지도 미세 조정(supervised finetuning)과 선호도 튜닝(preference tuning)의 조합을 통해 다루어지는 정렬(Alignment)은 인간 사용자의 욕구에 더 잘 부합하는 출력을 생성하도록 LLM을 미세 조정(finetuning)하는 과정을 의미합니다. 자세한 내용은 여기를 참조하십시오.
3 더 구체적으로, 현대 LLM의 출력은 자동으로 유창하고 간단한 문법/구문 오류가 없는 경향이 있으므로, 유창성 평가는 그러한 모델에게는 논쟁의 여지가 있지만 불필요합니다.
4 2년 이상 데이터 주석(annotation) 회사에서 일한 후, 저는 많은 수의 인간이 많은 양의 데이터를 정확하고 신뢰할 수 있게 주석/채점하는 것의 어려움에 정통합니다!
5 여기서 우리는 학습 중에 사용된 각 예시에는 참조 요약이 있다고 가정합니다. 그런 다음, 우리는 모델로 요약 출력을 계산하고, 참조 요약과 모델의 출력을 사용하여 ROUGE 점수를 찾은 다음, ROUGE 점수를 RL을 통한 학습을 위한 보상 신호(reward signal)로 사용할 수 있습니다.
6 기억하십시오, LLM은 출력하는 각 토큰(token)에 확률을 할당하여 작동합니다. 우리는 인간이 작성한 요약에 할당된 확률이 높기를 원합니다. 이러한 요약은 LLM이 생성할 수 있는 합리적인 출력을 나타내기 때문입니다.
7 모드 붕괴(Mode collapse)는 LLM이 출력 다양성을 잃고 특정 스타일(또는 스타일 세트)을 가진 좁은 범위의 출력만 생성하기 시작하는 현상을 의미합니다.
8 드리프트(Drift)는 단순히 미세 조정(finetune)된 모델이 일부 참조 모델(예: 미세 조정(finetuning) 과정 이전의 모델)과 너무 달라지는 것을 의미합니다.
9 [3]의 저자들은 깊이 3으로 책 요약 작업을 분해함으로써, 수천 단어의 책을 요약하면서 요약 작업의 주석(annotation) 비용을 50배 절감할 수 있다고 언급합니다.
10 이러한 프롬프트의 대부분은 OpenAI API에서 가져온 것이므로, 학습에 사용된 프롬프트와 실제 사용 사례 간에 충분한 중복이 보장됩니다.
11 의견 (또는 리뷰) 요약(Opinion (or review) summarization)은 많은 사용자가 특정 제품 또는 서비스에 대해 남긴 의견 또는 리뷰(예: 제품 리뷰 또는 식당에 대한 게시된 의견)를 요약하는 작업을 의미합니다.
12 이러한 이유로, 원본에서 텍스트를 직접 복사하는 추출적 스타일 요약은 텍스트 요약을 위한 효과적인 접근 방식임에도 불구하고 의견 요약에서는 성능이 좋지 않은 경향이 있습니다.
13 N-그램(N-gram) 참신성은 요약 내에서 생성되었지만 원본 텍스트에는 없는 n-그램(n-gram)의 비율을 의미합니다.