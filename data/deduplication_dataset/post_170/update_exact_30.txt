**SWE-Bench 저자들이 Neurips 2024에서 LLM 에이전트(LLM agents)의 현황에 대해 고찰하다: 최신 동향과 미래 전망**
Jay Alammar 2025년 1월 14일 5 1 공유

**SWE-bench 작업은 GitHub 이슈(GitHub issue) 수준에서 소프트웨어 엔지니어링 작업에 대한 AI 에이전트(AI agents)를 측정합니다. 이는 2024년에 소프트웨어 엔지니어링 작업을 다루는 에이전트(agents)의 진행 상황을 측정하는 가장 중요한 작업 중 하나였습니다.**

SWE-Bench는 단순한 코드 생성 능력을 넘어, 실제 개발 환경에서 LLM 에이전트의 문제 해결 능력을 평가합니다. Python 기반 오픈소스 프로젝트의 실제 버그 수정 및 기능 구현 이슈를 다루며, 에이전트의 해결책이 기존 및 신규 테스트를 통과해야 유효하다고 검증합니다. 이러한 엄격한 평가 방식은 LLM 에이전트 연구의 중요한 이정표가 되었습니다.

**우리는 그 작업의 두 명의 창시자인 Ofir Press와 Carlos E. Jimenez를 만나 LLM 기반 에이전트(LLM-backed agents)의 현황에 대한 그들의 생각을 공유했습니다.**

Ofir Press와 Carlos E. Jimenez는 LLM 에이전트가 복잡한 코드베이스 이해 및 다단계 추론에서 한계가 있음을 지적했습니다. 특히 긴 컨텍스트(context) 처리, 외부 도구 활용, 인간과의 협업 개선이 주요 과제로 꼽혔습니다. 그러나 이들은 에이전트가 개발 프로세스를 혁신하고 반복적 작업에서 효율성을 높일 잠재력을 높이 평가했습니다. 최근 계획(planning) 능력 강화, 자가 수정(self-correction) 메커니즘 도입, 그리고 다양한 개발 도구와의 통합을 통해 에이전트의 성능이 꾸준히 향상되고 있습니다. CodeAct와 같은 새로운 벤치마크들도 실제 개발 환경에 더 가까운 평가를 시도하며, 이 분야의 발전을 가속화합니다. 앞으로 복잡한 소프트웨어 엔지니어링(SWE) 작업을 수행하기 위해 근본적인 추론 능력과 신뢰성 강화 연구가 중요합니다.