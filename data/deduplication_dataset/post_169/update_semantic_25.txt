**DeepSeek-R1: 지능형 시스템 개발의 새로운 이정표**

인공지능 연구 개발 분야에서 DeepSeek-R1은 최근 주목할 만한 성과로 부상했습니다. 이 모델은 ML R&D 공동체에 다음과 같은 여러 중요한 시사점을 제공합니다:

Language Models & Co.를 구독해 주셔서 감사합니다! 새로운 소식을 무료로 받아보고 저의 작업을 지지하려면 지금 구독하세요.
구독

해당 모델은 경량화된 변형들을 포함하여 공개적으로 접근 가능한 가중치를 제공하며, OpenAI의 O1과 같은 추론 시스템을 모방하기 위한 훈련 기법들을 통합하고 있습니다. 본 게시물에서는 DeepSeek-R1의 구축 과정과 핵심 원리를 심층적으로 살펴보겠습니다.

번역: 중국어, 한국어, 튀르키예어 (이 게시물을 귀하의 언어로 번역하신 후 링크를 보내주시면 여기에 추가해 드립니다)

**목차:**
*   요약: 대규모 언어 모델(LLM) 훈련의 핵심 과정
*   DeepSeek-R1 훈련 전략
    *   1- 장문형 사고의 연쇄(CoT) SFT 데이터의 중요성
    *   2- 중간 단계의 고품질 추론 전문 LLM 활용
    *   3- 대규모 강화 학습(RL) 기반 추론 모델 구축
        *   3.1- 대규모 추론 지향 강화 학습(R1-Zero)의 역할
        *   3.2- 중간 추론 모델을 활용한 SFT 추론 데이터 생성
        *   3.3- 일반적인 강화 학습 훈련 단계
*   아키텍처(Architecture) 분석

이러한 모델들의 작동 방식을 이해하는 데 필요한 대부분의 기초 지식은 저희 저서인 Hands-On Large Language Models에서 찾으실 수 있습니다.
책의 공식 웹사이트.
Amazon에서 책을 주문할 수 있습니다.
모든 코드는 GitHub에 공개되어 있습니다.

**요약: 대규모 언어 모델(LLM) 훈련의 핵심 과정**

여타의 대규모 언어 모델들처럼 DeepSeek-R1도 개별적인 정보 단위를 순차적으로 도출하지만, 복잡한 문제 해결을 위해 사고 과정을 상세히 전개하는 '사고 요소'(thinking tokens)를 생성함으로써 수학적 연산 및 논리적 추론 능력에서 두각을 나타냅니다. 이는 모델이 단순히 답변을 제공하는 것을 넘어, 문제 해결의 '생각하는 과정'을 외부화하여 더 많은 시간을 할애하고 정교한 해결책을 찾아낼 수 있게 합니다.

저희 책 12장에 수록된 다음 도표는 고품질 LLM을 개발하는 데 일반적으로 적용되는 세 가지 단계별 절차를 보여줍니다. 이러한 단계들은 상호 보완적으로 작용하며, 모델이 특정 작업을 수행하는 능력을 점진적으로 향상시킵니다.

1) **기초 언어 모델링(Language Modeling) 단계:** 이 초기 단계에서는 방대한 웹 데이터를 활용하여 다음 단어를 예측하는 텍스트 생성 훈련을 진행합니다. 이 과정을 통해 모델은 언어의 통계적 패턴과 구조를 학습하며, 이를 기반으로 '기본 모델'(base model)이 형성됩니다. 이는 모델이 인간의 언어를 이해하고 생성하는 데 필요한 기본적인 역량을 갖추게 되는 과정입니다.

2) **지도 미세 조정(Supervised Fine-Tuning, SFT) 단계:** 이 단계는 모델이 인간의 지시를 따르고 질문에 응답하는 능력을 향상시키는 지도 학습 기반의 정교화 과정입니다. 기본 모델이 생성된 후, 특정 지시-응답 쌍으로 구성된 데이터셋을 통해 모델을 훈련하여, 사용자의 의도를 더 정확하게 파악하고 유용한 답변을 생성하도록 만듭니다. 이 과정을 거쳐 '지시 조정 모델'(instruction-tuned model) 또는 SFT 모델이 탄생합니다.

3) **선호도 조정(Preference Tuning) 단계:** 마지막으로, 최종 사용자의 선호도에 맞춰 모델의 행동 양식을 조율하고 개선하는 단계입니다. 이 단계에서는 주로 인간 피드백 기반 강화 학습(Reinforcement Learning from Human Feedback, RLHF)과 같은 기술을 사용하여 모델의 응답이 인간의 가치관과 기대에 부합하도록 최적화합니다. 이를 통해 여러분이 플레이그라운드(playground)나 애플리케이션에서 상호작용하는 최종적인 '선호도 조정 LLM'이 완성됩니다. 이 과정은 모델의 유용성, 안전성, 그리고 전반적인 사용자 경험을 극대화하는 데 필수적입니다.

**DeepSeek-R1 훈련 전략**

DeepSeek-R1은 이러한 일반적인 개발 경로를 따르지만, 그 과정에서 독특한 방법론을 적용합니다. 첫 단계에 대한 구체적인 내용은 DeepSeek-V3 모델에 관한 이전 연구에서 차용되었습니다. R1은 해당 선행 연구의 기본 모델(최종 DeepSeek-v3 모델과는 다름)을 활용하며, 여전히 SFT 및 선호도 조정 단계를 거치지만, 이를 실행하는 방식에서 차별점을 보입니다. DeepSeek-R1의 훈련 과정에서 특히 강조할 만한 세 가지 혁신적인 접근 방식은 다음과 같습니다.

**1- 장문형 사고의 연쇄(CoT) SFT 데이터의 중요성**

이것은 60만 개에 달하는 방대한 분량의 장문형 추론 사고 과정 데이터 셋을 의미합니다. 이러한 데이터는 확보하기 매우 난해하며, 해당 규모의 자료를 사람이 직접 분류하는 것은 상당한 비용을 수반합니다. 복잡한 추론 문제 해결을 위해서는 단순한 정답뿐만 아니라, 그 정답에 도달하는 논리적 과정을 상세히 담은 데이터가 필수적입니다. 이러한 CoT 데이터는 모델이 '생각하는 방식'을 학습하고, 투명하고 설명 가능한 추론 능력을 갖추는 데 결정적인 역할을 합니다. 데이터의 희소성과 라벨링의 어려움은 이러한 고품질 데이터를 인공적으로 생성하는 방법론의 중요성을 더욱 부각시킵니다.

**2- 중간 단계의 고품질 추론 전문 LLM 활용**

이러한 CoT 데이터는 R1의 전신 격인, 추론에 특화된 이름 없는 자매 모델에 의해 생성됩니다. 이 자매 모델은 R1-Zero라고 불리는 세 번째 모델(곧 논의할 예정)에서 영감을 받아 개발되었습니다. 이 접근 방식이 중요한 이유는 이 자매 모델 자체가 최종 사용자에게 직접 서비스하기 위한 '완성된' LLM이라기보다는, 대규모 강화 학습(reinforcement learning, RL)과 결합하여 매우 적은 양의 수동 라벨링 데이터만으로도 추론 문제 해결에 탁월한 역량을 발휘하는 '도구'로 기능했기 때문입니다. 이 추론 전문 모델의 결과물은 사용자들이 LLM에 기대하는 수준으로 다른 비추론 작업도 수행할 수 있는, 더 범용적인 모델을 훈련하는 데 효과적으로 활용될 수 있습니다. 이는 특정 분야의 전문가 모델을 활용하여 일반 모델의 능력을 강화하는 지식 증류(knowledge distillation) 전략의 한 예시로 볼 수 있습니다.

**3- 대규모 강화 학습(RL) 기반 추론 모델 구축**

이 과정은 두 단계로 나누어 진행됩니다.

**3.1- 대규모 추론 지향 강화 학습(Large-Scale Reasoning-Oriented Reinforcement Learning, R1-Zero)의 역할**

여기서 RL은 중간 추론 모델을 생성하는 데 사용되며, 이 모델은 다시 SFT 추론 예시를 생성하는 데 기여합니다. 하지만 이 중간 모델의 생성을 가능하게 한 것은 DeepSeek-R1-Zero라고 명명된 선행 모델을 개발한 초기 실험에서 비롯됩니다.

R1-Zero는 수동으로 라벨링된 SFT 훈련 세트 없이도 추론 작업에서 뛰어난 성능을 보인다는 점에서 매우 특별합니다. 그 훈련은 사전 훈련된(pre-trained) 기본 모델에서 RL 훈련 과정을 통해 직접 이루어집니다(SFT 단계 없음). 이것은 OpenAI의 O1과 경쟁할 정도로 매우 우수한 성과를 달성했습니다. 전통적으로 데이터가 ML 모델 능력의 핵심 동력이었음을 감안할 때, 이러한 성과는 매우 중요합니다. 이 모델은 어떻게 이러한 데이터 의존성에서 벗어날 수 있었을까요?

이는 다음 두 가지 핵심적인 통찰을 제시합니다.

1- 현대의 기본 모델은 특정 품질 및 능력 임계값(threshold)을 초월했습니다(이 기본 모델은 14.8조 개의 고품질 토큰으로 훈련되었습니다). 즉, 충분히 강력한 기본 모델은 추가적인 미세 조정 없이도 상당한 잠재력을 내포하고 있습니다.
2- 일반적인 채팅이나 글쓰기 요청과 달리 추론 문제는 결과의 정확성 여부를 자동적으로 검증하거나 라벨링할 수 있는 특성을 가집니다. 이는 RL 훈련을 위한 보상 신호(reward signal)를 효율적으로 생성할 수 있음을 의미합니다.

예시를 통해 이를 더 명확히 보여드리겠습니다.

**예시: 추론 문제의 자동 검증 메커니즘**

이는 이 RL 훈련 단계의 일부인 프롬프트(prompt) 또는 질문이 될 수 있습니다.

숫자 목록을 받아 정렬된 순서로 반환하되, 시작 부분에 42를 추가하는 파이썬(Python) 코드를 작성하세요.

이러한 유형의 질문은 다양한 자동 검증(automatic verification) 방식에 매우 적합합니다. 훈련 중인 모델에 이 질문을 제시하고, 모델이 완성(completion)을 생성한다고 가정해 봅시다.

*   **코드 유효성 검사:** 소프트웨어 린터(linter)는 완성된 코드가 적절한 파이썬 문법을 따르는지 여부를 즉시 확인할 수 있습니다.
*   **실행 가능성 및 기능 검증:** 생성된 파이썬 코드를 실제 환경에서 실행하여 의도한 대로 작동하는지, 즉 주어진 숫자 목록을 정렬하고 42를 추가하는지 확인할 수 있습니다.
*   **단위 테스트 생성 및 적용:** 다른 최신 코딩 LLM은 원하는 동작을 검증하기 위한 단위 테스트(unit tests)를 자동으로 생성하고 이를 모델의 출력에 적용할 수 있습니다(심지어 스스로 추론 전문가가 아니더라도).
*   **성능 최적화:** 한 단계 더 나아가, 코드의 실행 시간을 측정하거나 메모리 사용량을 분석하여, 문제가 해결되는 올바른 파이썬 프로그램이라 할지라도, 훈련 과정이 다른 해결책보다 더 성능이 효율적인 해결책을 선호하도록 유도할 수 있습니다.

훈련 단계에서 모델에 이와 같은 질문을 제시하고, 여러 가지 가능한 해결책을 생성하도록 할 수 있습니다. 우리는 (인간의 개입 없이) 자동화된 방식으로 각 해결책을 검증할 수 있습니다. 예를 들어, 첫 번째 완성은 코드가 아님을 알 수 있고, 두 번째는 코드이지만 파이썬 코드가 아닐 수 있으며, 세 번째는 가능한 해결책이지만 단위 테스트에 실패하고, 네 번째는 완전히 올바른 해결책일 수 있습니다.

이러한 검증 결과들은 모델을 개선하는 데 직접적으로 활용될 수 있는 강력한 신호가 됩니다. 물론 이 과정은 수많은 예시(미니 배치(mini-batches) 단위)와 연속적인 훈련 단계를 거쳐 반복적으로 수행됩니다. 이러한 보상 신호(reward signals)와 모델 업데이트(model updates)는 논문의 그림 2에서 볼 수 있듯이 RL 훈련 과정에서 모델이 작업을 지속적으로 개선하는 방식의 핵심입니다. 이러한 능력 향상에 비례하여 생성되는 응답의 길이는 모델이 문제를 처리하기 위해 더 많은 사고 토큰을 생성하는 경향을 보여줍니다.

이 과정은 매우 유용하지만, R1-Zero 모델은 추론 과제에서 우수한 성과를 보였음에도 불구하고, 실질적인 활용도 측면에서는 아쉬운 점들을 드러냈습니다.

DeepSeek-R1-Zero는 뛰어난 추론 역량을 발휘하고 예상치 못한 강력한 논리적 행위를 자율적으로 발전시켰으나, 낮은 가독성이나 언어 간의 혼용과 같은 여러 난관에 봉착했습니다. 이는 모델이 '생각하는 과정'을 효과적으로 전달하는 데 한계가 있음을 의미하며, 사용자 친화적인 응답을 생성하는 데 방해가 됩니다.

최종 모델인 R1은 사용자에게 더 유용하고 실용적인 모델이 되도록 설계되었습니다. 따라서 RL 과정에 전적으로 의존하기보다는, 이 섹션에서 앞서 언급했듯이 두 가지 방식으로 활용됩니다.

1- SFT 데이터 포인트(data points)를 생성하기 위한 중간 추론 모델 생성
2- 추론뿐만 아니라 비추론 문제 해결 능력까지 개선하기 위한 R1 모델 훈련 (이때는 다양한 유형의 검증자(verifiers)를 사용)

**3.2- 중간 추론 모델을 활용한 SFT 추론 데이터 생성**

중간 추론 모델을 더욱 유용하게 만들기 위해, 수천 개의 추론 문제 예시(일부는 R1-Zero에서 생성 및 필터링됨)에 대해 지도 미세 조정(SFT) 훈련 단계를 거칩니다. 논문에서는 이를 "콜드 스타트(cold start) 데이터"라고 언급합니다.

**2.3.1. 콜드 스타트**

DeepSeek-R1-Zero와 달리, 기본 모델로부터 RL 훈련의 초기 불안정한 콜드 스타트 단계를 방지하기 위해, DeepSeek-R1의 경우 초기 RL 액터(actor)로서 모델을 미세 조정하기 위해 소량의 긴 CoT(Chain-of-Thought) 데이터를 구축하고 수집합니다. 이러한 데이터를 수집하기 위해 우리는 여러 접근 방식을 탐색했습니다. 긴 CoT를 예시로 사용하는 퓨샷 프롬프팅(few-shot prompting), 모델이 반성(reflection) 및 검증(verification)을 통해 상세한 답변을 직접 생성하도록 프롬프트(prompt)하는 것, DeepSeek-R1-Zero 출력을 읽기 쉬운 형식으로 수집하는 것, 그리고 인간 주석자(human annotators)의 후처리(post-processing)를 통해 결과를 정제하는 것입니다.

하지만 잠깐, 이러한 고품질 데이터가 이미 있다면 왜 복잡한 RL 과정에 의존하는 걸까요? 그 이유는 바로 데이터의 '규모' 때문입니다. 이 수동으로 구축된 데이터셋은 5,000개 정도의 예시일 수 있지만(이는 확보 가능하지만 여전히 많은 노력 필요), R1을 효과적으로 훈련시키려면 600,000개에 달하는 방대한 예시가 필요했습니다. 이 중간 추론 모델은 이 엄청난 데이터 격차를 메우고, 매우 귀중한 데이터를 인공적으로 대량 생성할 수 있도록 함으로써 훈련 효율성을 극대화합니다. 이는 인간의 노동력을 대체하여 고품질의 대규모 데이터셋을 구축하는 혁신적인 방법론을 제시합니다.

지도 미세 조정(SFT) 개념이 처음이라면, 이는 프롬프트(prompt)와 올바른 완성(correct completion) 형태로 모델에 훈련 예시를 제시하는 과정입니다. 12장의 이 그림은 몇 가지 SFT 훈련 예시를 보여주며, 모델이 특정 입력에 대해 어떤 출력을 기대하는지 명확하게 학습하도록 돕습니다.

**3.3- 일반적인 강화 학습 훈련 단계**

이 단계는 R1이 단순히 추론 작업뿐만 아니라, 다른 비추론 작업에서도 탁월한 성능을 발휘하도록 만듭니다. 이 과정은 이전에 보았던 RL 과정과 유사하게 진행되지만, 비추론 애플리케이션(application)으로 그 범위를 확장합니다. 따라서 이러한 애플리케이션에 속하는 프롬프트에 대해 유용성(helpfulness) 및 안전성(safety) 보상 모델(Llama 모델의 접근 방식과 유사함)을 활용하여 모델의 응답을 평가하고 개선합니다. 이 단계에서는 모델이 다양한 사용자 요구에 부응하고, 유해하거나 편향된 콘텐츠를 생성하지 않도록 정교하게 조정됩니다. 즉, 모델의 범용성과 책임감을 동시에 강화하는 과정이라 할 수 있습니다.

**아키텍처(Architecture) 분석**

GPT-2 및 GPT-3 시대의 선행 모델들과 유사하게, DeepSeek-R1은 트랜스포머 디코더 구성 요소들의 연속적인 배열로 이루어져 있습니다. 전체 61개의 구성 요소 중 첫 세 개는 조밀(dense) 계층이며, 나머지는 전문가 혼합(Mixture-of-Experts, MoE) 계층으로 이루어져 있습니다 (저의 공동 저자 Maarten의 훌륭한 소개 가이드는 여기에서 확인하세요: A Visual Guide to Mixture of Experts (MoE) ). MoE 아키텍처는 모델의 용량을 크게 늘리면서도 계산 비용을 효율적으로 관리할 수 있게 해줍니다. 특정 입력에 대해 소수의 전문가만 활성화되므로, 전체 모델을 활성화하는 것보다 훨씬 적은 연산으로도 복잡한 문제를 해결할 수 있습니다. 이는 대규모 언어 모델의 훈련 및 추론 효율성을 혁신적으로 개선하는 핵심 기술 중 하나입니다.

모델 차원 크기(model dimension size) 및 기타 하이퍼파라미터(hyperparameters) 측면에서 다음과 같은 특징을 가집니다.

모델 아키텍처에 대한 더 자세한 내용은 그들의 두 가지 이전 논문에 상세히 제시되어 있습니다.
DeepSeek-V3 Technical Report
DeepSeekMoE: Towards Ultimate Expert Specialization in Mixture-of-Experts Language Models

**결론**

이로써 DeepSeek-R1 모델을 이해하는 데 필요한 주요 직관을 얻으셨을 것입니다. DeepSeek-R1은 데이터의 희소성을 극복하고, 자동 검증 가능한 추론 문제를 활용하여 강력한 모델을 구축하는 새로운 패러다임을 제시했습니다. 특히, 중간 단계의 전문 모델을 활용하여 대규모의 고품질 SFT 데이터를 생성하는 전략은 향후 LLM 개발에 중요한 영향을 미칠 것으로 예상됩니다. 이 게시물을 이해하는 데 좀 더 기본적인 정보가 필요하다고 느끼셨다면, Hands-On Large Language Models 책을 구매하시거나 O’Reilly에서 온라인으로 읽어보시고 GitHub에서 확인해 보시길 권합니다. DeepSeek-R1은 단순한 기술적 성취를 넘어, 개방형 AI 연구의 가능성을 확장하고 고성능 모델의 접근성을 높이는 데 기여하고 있습니다.

다른 추천 자료는 다음과 같습니다.

*   Maarten Grootendorst의 A Visual Guide to Reasoning LLMs
*   Nathan Lambert의 DeepSeek R1's recipe to replicate o1 and the future of reasoning LMs
*   Maarten Grootendorp의 A Visual Guide to Mixture of Experts (MoE)
*   Sasha Rush의 YouTube 비디오 Speculations on Test-Time Scaling (o1)
*   Yannis Kilcher의 DeepSeekMath: Pushing the Limits of Mathematical Reasoning in Open Language Models (Paper Explained)
*   Open R1은 DeepSeek-R1을 공개적으로 재현하기 위한 HuggingFace 프로젝트입니다.
*   Putting RL back in RLHF

이 논문을 읽는 동안 2022년의 Galactica 논문이 떠올랐습니다. 이 논문에는 전용 사고 토큰(thinking token)을 포함한 많은 훌륭한 아이디어가 있었습니다.

Language Models & Co.를 읽어주셔서 감사합니다! 새로운 게시물을 무료로 받아보고 제 작업을 지원하려면 구독하세요.
구독