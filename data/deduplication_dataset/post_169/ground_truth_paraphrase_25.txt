인공지능 기술 혁신의 끊임없는 흐름 속에서 DeepSeek-R1은 가장 최근에 등장한 인상적인 발전입니다. 머신러닝 연구 개발(ML R&D) 공동체에게 DeepSeek-R1은 여러 면에서 중요한 소식입니다.

DeepSeek-R1은 오픈 가중치(open weights)를 제공하며, 여기에는 더 소규모로 정제된 버전들도 포함됩니다. 특히, OpenAI의 O1과 같은 최첨단 추론 모델을 재현하기 위한 훈련 방법론을 공유하고 반영합니다. 본 게시물에서는 DeepSeek-R1이 어떻게 구축되었는지 면밀히 살펴보겠습니다.

**목차:**
*   요약: LLM 훈련의 핵심 원리
*   DeepSeek-R1 훈련 과정 상세 분석
    *   1- 심층적인 사고의 연쇄(Chain-of-Thought, CoT) SFT 데이터 활용
    *   2- 고품질 중간 추론 LLM의 역할 (비추론 작업에서는 제한적 성능)
    *   3- 대규모 강화 학습(Reinforcement Learning, RL) 기반 추론 모델 구축
        *   3.1- 대규모 추론 중심 강화 학습(Large-Scale Reasoning-Oriented Reinforcement Learning, R1-Zero)의 혁신
        *   3.2- 중간 추론 모델을 활용한 SFT 추론 데이터 생성
        *   3.3- 일반적인 RL 훈련 단계
*   아키텍처(Architecture) 세부 사항
*   추론 LLM의 미래와 DeepSeek-R1의 함의

**요약: LLM 훈련의 핵심 원리**

여타 대규모 언어 모델들처럼 DeepSeek-R1 역시 단일 토큰 단위로 출력을 만들어내지만, '사고 토큰(thinking tokens)'을 활용하여 추론 과정을 상세히 풀어나가며 문제 해결에 충분한 시간을 할애합니다. 이러한 접근 방식은 특히 수리적 문제나 복잡한 추론 과제에서 뛰어난 성능을 발휘하게 합니다. 이는 모델이 단순히 정답을 제시하는 것을 넘어, 해답에 도달하는 과정을 명시적으로 보여줌으로써 신뢰성과 해석 가능성을 높이는 효과를 가져옵니다.

일반적으로 고품질 LLM을 개발하는 과정은 세 가지 주요 단계를 거칩니다.

1)  **언어 모델링(language modeling)**: 방대한 웹 데이터를 기반으로 다음 단어를 예측하도록 모델을 훈련하여 기본 모델(base model)을 구축하는 단계입니다. 이 과정에서 모델은 언어의 구조와 패턴을 학습합니다.
2)  **지도 미세 조정(supervised fine-tuning, SFT)**: 모델이 사용자의 지시를 정확히 따르고 질문에 유용하게 답변하도록 만드는 단계입니다. 이 과정을 통해 지시 조정 모델(instruction tuned model) 또는 SFT 모델이 생성됩니다.
3)  **선호도 조정(preference tuning)**: 모델의 행동을 더욱 세밀하게 다듬고 인간의 선호도에 맞춰 조정하는 최종 단계입니다. 이 단계를 거쳐 사용자가 플레이그라운드(playground)나 애플리케이션(application)에서 상호작용하는 최종적인 선호도 조정 LLM이 완성됩니다.

**DeepSeek-R1 훈련 과정 상세 분석**

DeepSeek-R1은 위에서 설명한 일반적인 LLM 훈련 절차를 따르지만, 그 방식에는 독특한 세부 사항들이 존재합니다. 초기 단계의 구체적인 내용은 DeepSeek-V3 모델에 대한 이전 연구에서 파생되었습니다. R1은 이전에 발표된 논문의 기본 모델(최종 DeepSeek-v3 모델과는 다름)을 활용하며, SFT 및 선호도 조정 단계를 거치지만, 이를 실행하는 방식에서 차별점을 보입니다.

R1 모델을 구축하는 과정에서 특히 주목할 만한 세 가지 핵심 요소가 있습니다.

**1- 심층적인 사고의 연쇄(Chain-of-Thought, CoT) SFT 데이터 활용**

이 모델은 방대한 양의 긴 사고의 연쇄(chain-of-thought) 추론 예시, 약 60만 개에 달하는 데이터를 활용합니다. 이러한 유형의 데이터는 확보하기가 매우 어렵고, 이 정도 규모의 데이터를 사람이 직접 라벨링(labeling)하는 것은 엄청난 비용과 시간이 소요됩니다. 따라서 이러한 데이터를 효율적으로 생성하는 과정 자체가 강조할 두 번째 특별한 점으로 이어집니다. 이는 고품질의 추론 능력을 모델에 주입하기 위한 핵심적인 전략입니다.

**2- 고품질 중간 추론 LLM의 역할 (비추론 작업에서는 제한적 성능)**

이 막대한 CoT 데이터는 R1의 전신 격인, 추론에 특화된 익명의 자매 모델에 의해 생성됩니다. 이 자매 모델은 DeepSeek-R1-Zero라고 불리는 세 번째 모델(곧 자세히 다룰 예정)에서 영감을 받아 개발되었습니다. 이 모델이 중요한 이유는, 비록 이 자체가 바로 사용하기 좋은 범용 LLM은 아니더라도, 대규모 강화 학습(reinforcement learning, RL)과 결합될 때 극히 적은 양의 라벨링된(labeled) 데이터만으로도 추론 문제 해결에 탁월한 능력을 발휘하는 모델을 만들 수 있었기 때문입니다. 이 전문화된 추론 모델의 출력은 사용자들이 LLM에 기대하는 수준으로 다른 비추론 작업도 수행할 수 있는, 보다 일반적인 모델을 훈련하는 데 효과적으로 사용될 수 있습니다.

**3- 대규모 강화 학습(Reinforcement Learning, RL) 기반 추론 모델 구축**

이 과정은 두 단계로 나누어 진행됩니다.

**3.1- 대규모 추론 중심 강화 학습(Large-Scale Reasoning-Oriented Reinforcement Learning, R1-Zero)의 혁신**

여기서 RL은 중간 추론 모델을 생성하는 데 활용됩니다. 이 중간 모델은 다시 SFT 추론 예시를 만들어내는 데 사용됩니다. 하지만 이 중간 모델의 생성을 가능하게 한 것은 DeepSeek-R1-Zero라고 명명된 초기 모델을 구축하는 과정에서 이루어진 선구적인 실험입니다.

R1-Zero는 라벨링된 SFT 훈련 데이터셋 없이도 추론 작업에서 뛰어난 역량을 보여준다는 점에서 매우 독특합니다. 그 훈련은 사전 훈련된(pre-trained) 기본 모델에서 RL 훈련 과정을 직접 거쳐 이루어집니다(SFT 단계가 없음). 이 모델은 O1과 경쟁할 정도로 매우 우수한 성능을 달성했습니다. 데이터가 항상 머신러닝 모델 능력의 핵심 원동력이었음을 감안할 때, 이는 매우 중요한 의미를 가집니다. 이 모델은 어떻게 기존의 데이터 의존적 역사에서 벗어날 수 있었을까요?

이는 두 가지 중요한 시사점을 던져줍니다.

1-  현대의 기본 모델들은 특정 품질 및 능력 임계값(threshold)을 넘어섰습니다(이 기본 모델은 14.8조 개의 고품질 토큰으로 훈련되었습니다). 즉, 모델 자체의 내재된 잠재력이 매우 커졌다는 의미입니다.
2-  일반적인 채팅이나 글쓰기 요청과 달리, 추론 문제는 자동으로 검증되거나 라벨링될 수 있습니다. 이는 RL 훈련에 필요한 보상 신호(reward signal)를 효율적으로 생성할 수 있음을 의미합니다.

예시를 통해 이를 명확히 보여드리겠습니다.

**예시: 추론 문제의 자동 검증**

다음은 RL 훈련 단계에서 프롬프트(prompt)/질문으로 사용될 수 있는 예시입니다.

숫자 목록을 받아 정렬된 순서로 반환하되, 시작 부분에 42를 추가하는 파이썬(Python) 코드를 작성하세요.

이러한 유형의 질문은 여러 가지 자동 검증(automatic verification) 방식에 매우 적합합니다. 훈련 중인 모델에 이 질문을 제시하고, 모델이 완성(completion)을 생성한다고 가정해 봅시다.

*   소프트웨어 린터(linter)는 완성된 코드가 적절한 파이썬 코드 문법을 따르는지 여부를 확인할 수 있습니다.
*   생성된 파이썬 코드를 실제로 실행하여 의도한 대로 작동하는지 검증할 수 있습니다.
*   다른 최신 코딩 LLM은 원하는 동작을 검증하기 위한 단위 테스트(unit tests)를 자동으로 생성할 수 있습니다(심지어 해당 LLM 자체가 추론 전문가는 아니더라도).
*   나아가, 실행 시간을 측정하여 비록 문제가 해결되는 올바른 파이썬 프로그램이라 할지라도, 훈련 과정이 다른 해결책보다 더 성능이 우수한 해결책을 선호하도록 만들 수 있습니다.

훈련 단계에서 모델에 이와 같은 질문을 제시하고, 여러 가지 가능한 해결책을 생성하도록 할 수 있습니다. 우리는 (인간의 개입 없이) 자동으로 확인하여 첫 번째 완성은 코드가 아님을 파악할 수 있고, 두 번째는 코드이지만 파이썬 코드가 아님을 알 수 있습니다. 세 번째는 가능한 해결책이지만 단위 테스트에 실패하며, 네 번째는 완전히 올바른 해결책입니다.

이러한 자동 검증 결과들은 모델을 개선하는 데 직접적으로 활용될 수 있는 강력한 신호입니다. 물론 이 과정은 수많은 예시(미니 배치(mini-batches) 단위)와 연속적인 훈련 단계를 거쳐 수행됩니다. 이러한 보상 신호(reward signals)와 모델 업데이트(model updates)는 논문의 그림 2에서 설명된 바와 같이 RL 훈련 과정에서 모델이 작업을 지속적으로 개선해 나가는 핵심 메커니즘입니다. 이러한 능력의 향상에 비례하여 생성되는 응답의 길이도 길어지는데, 이는 모델이 문제를 처리하기 위해 더 많은 사고 토큰을 생성함을 의미합니다.

이 과정이 매우 유용함에도 불구하고, R1-Zero 모델은 이러한 추론 문제에서 높은 점수를 받았음에도 불구하고, 원하는 것보다 사용성이 떨어지게 만드는 다른 문제들에 직면했습니다.

"DeepSeek-R1-Zero는 강력한 추론 능력을 보여주고 예상치 못한 강력한 추론 행동을 자율적으로 개발하지만, 가독성(readability) 저하 및 언어 혼합(language mixing)과 같은 여러 문제에 직면합니다."

R1은 더 실용적이고 사용 가능한 모델이 되도록 설계되었습니다. 따라서 RL 과정에 전적으로 의존하는 대신, 이 섹션에서 앞서 언급했듯이 두 가지 방식으로 활용됩니다.

1-  SFT 데이터 포인트(data points)를 생성하기 위한 중간 추론 모델 생성
2-  추론 및 비추론 문제 개선을 위한 R1 모델 훈련 (다른 유형의 검증자(verifiers) 사용)

**3.2- 중간 추론 모델을 활용한 SFT 추론 데이터 생성**

중간 추론 모델을 더욱 유용하게 만들기 위해, 수천 개의 추론 문제 예시(일부는 R1-Zero에서 생성 및 필터링됨)에 대해 지도 미세 조정(SFT) 훈련 단계를 거칩니다. 논문에서는 이를 "콜드 스타트(cold start) 데이터"라고 표현합니다. 이는 모델이 초기 RL 훈련 단계에서 겪을 수 있는 불안정성을 줄이기 위한 전략입니다.

초기 RL 훈련의 불안정한 '콜드 스타트' 단계를 피하기 위해, DeepSeek-R1은 소량의 긴 CoT 데이터를 구축하고 수집하여 초기 RL 액터(actor)로서 모델을 미세 조정합니다. 이 데이터는 퓨샷 프롬프팅(few-shot prompting), 모델 스스로 반성(reflection) 및 검증(verification)을 통해 상세한 답변을 생성하도록 유도하는 방식, DeepSeek-R1-Zero의 출력을 읽기 쉬운 형식으로 수집하는 방식, 그리고 인간 주석자(human annotators)의 후처리(post-processing)를 통해 정제하는 등 다양한 접근 방식을 통해 확보됩니다.

하지만 이 데이터를 직접 사용할 수 있다면 왜 복잡한 RL 과정에 의존하는 것일까요? 이는 데이터의 규모 때문입니다. 직접 확보 가능한 CoT 데이터셋은 약 5,000개의 예시일 수 있지만, R1을 효과적으로 훈련시키려면 약 600,000개의 예시가 필요했습니다. 이 중간 모델은 그 엄청난 격차를 메우고 매우 귀중한 훈련 데이터를 인공적으로 생성할 수 있도록 합니다. 지도 미세 조정(SFT)은 프롬프트(prompt)와 올바른 완성(correct completion) 형태의 훈련 예시를 모델에 제시하는 과정으로, 모델이 특정 작업을 수행하는 방법을 학습하게 합니다.

**3.3- 일반적인 RL 훈련 단계**

이 단계는 R1이 추론 능력뿐만 아니라 다른 비추론 작업에서도 뛰어난 성능을 발휘하도록 만듭니다. 이 과정은 이전에 살펴보았던 RL 과정과 유사하지만, 비추론 애플리케이션(application)으로 그 범위를 확장합니다. 따라서 이러한 애플리케이션에 해당하는 프롬프트에 대해서는 유용성(helpfulness) 및 안전성(safety) 보상 모델(이는 Llama 모델과 유사한 접근 방식입니다)을 활용하여 모델을 미세 조정합니다. 이로써 모델은 다양한 사용자 시나리오에서 더욱 적절하고 안전한 응답을 생성할 수 있게 됩니다.

**아키텍처(Architecture) 세부 사항**

GPT2와 GPT3 시대의 이전 모델들과 마찬가지로, DeepSeek-R1은 트랜스포머 디코더 블록(Transformer decoder blocks)의 스택(stack)으로 구성되어 있습니다. 총 61개의 블록으로 이루어져 있으며, 처음 세 개는 덴스(dense) 레이어이지만, 나머지 블록들은 전문가 혼합(mixture-of-experts, MoE) 레이어로 구현되어 있습니다. MoE 아키텍처는 모델이 특정 입력에 대해 가장 적합한 '전문가' 서브 네트워크(sub-network)를 활성화함으로써, 계산 효율성을 높이면서도 모델의 용량을 확장하는 혁신적인 방법입니다.

모델 차원 크기(model dimension size) 및 기타 하이퍼파라미터(hyperparameters)에 대한 더 자세한 내용은 그들의 두 가지 이전 논문에 상세히 제시되어 있습니다.
DeepSeek-V3 Technical Report
DeepSeekMoE: Towards Ultimate Expert Specialization in Mixture-of-Experts Language Models

**추론 LLM의 미래와 DeepSeek-R1의 함의**

DeepSeek-R1은 단순한 성능 향상을 넘어, 대규모 언어 모델의 추론 능력 개발에 있어 중요한 이정표를 제시합니다. 특히, 자동 검증이 가능한 추론 문제를 활용한 RL 훈련 방식은 인간의 개입 없이도 모델의 지능을 효과적으로 확장할 수 있는 가능성을 보여줍니다. 이는 데이터 라벨링의 병목 현상을 해결하고, 모델이 스스로 학습하고 개선하는 자율 학습(autonomous learning) 패러다임으로의 전환을 가속화할 수 있습니다.

또한, R1-Zero와 같은 중간 모델을 활용하여 고품질의 SFT 데이터를 합성하는 전략은, 실제 세계의 복잡한 문제 해결에 필요한 대규모 데이터를 효율적으로 확보하는 새로운 길을 열었습니다. 이러한 접근 방식은 미래 LLM 개발에서 합성 데이터(synthetic data)의 중요성이 더욱 커질 것임을 시사합니다.

DeepSeek-R1의 MoE 아키텍처 채택은 모델의 확장성과 효율성이라는 두 마리 토끼를 잡으려는 노력의 일환입니다. 이는 모델이 방대한 지식을 습득하면서도 특정 작업에 대한 전문성을 잃지 않도록 돕습니다. 그러나 이러한 복잡한 모델의 투명성, 편향성 제어, 그리고 예측 불가능한 행동에 대한 안전성 확보는 여전히 중요한 연구 과제로 남아있습니다.

결론적으로 DeepSeek-R1 모델은 LLM이 단순히 텍스트를 생성하는 도구를 넘어, 복잡한 문제를 논리적으로 분석하고 해결하는 지능형 에이전트(intelligent agent)로 진화할 수 있음을 입증합니다. 이러한 발전은 과학 연구, 소프트웨어 개발, 교육 등 다양한 분야에서 혁신적인 변화를 가져올 잠재력을 지니고 있습니다. 이는 인공 일반 지능(AGI)을 향한 여정에서 중요한 한 걸음으로 기록될 것입니다.

이 논문을 읽는 동안 2022년의 Galactica 논문이 떠올랐습니다. 이 논문에는 전용 사고 토큰(thinking token)을 포함한 많은 훌륭한 아이디어가 있었습니다.

이러한 모델이 어떻게 작동하는지 이해하는 데 필요한 대부분의 기초 지식은 저희 책인 Hands-On Large Language Models에서 찾아볼 수 있습니다. 책의 공식 웹사이트. Amazon에서 책을 주문할 수 있습니다. 모든 코드는 GitHub에 업로드되어 있습니다.

다른 추천 자료는 다음과 같습니다.

*   Maarten Grootendorst의 A Visual Guide to Reasoning LLMs
*   Nathan Lambert의 DeepSeek R1's recipe to replicate o1 and the future of reasoning LMs
*   Maarten Grootendorst의 A Visual Guide to Mixture of Experts (MoE)
*   Sasha Rush의 YouTube 비디오 Speculations on Test-Time Scaling (o1)
*   Yannis Kilcher의 DeepSeekMath: Pushing the Limits of Mathematical Reasoning in Open Language Models (Paper Explained)
*   Open R1은 DeepSeek-R1을 공개적으로 재현하기 위한 HuggingFace 프로젝트입니다.
*   Putting RL back in RLHF

Language Models & Co.를 읽어주셔서 감사합니다! 새로운 게시물을 무료로 받아보고 제 작업을 지원하려면 구독하세요.
구독