DeepSeek-R1은 AI 발전의 꾸준한 북소리 속에서 가장 최근의 울림 있는 박동입니다. ML R&D 커뮤니티에게 DeepSeek-R1은 다음과 같은 이유로 중요한 발표입니다:

Language Models & Co.에 관심을 가져주셔서 감사합니다! 최신 게시물을 받아보고 제 작업을 지원하려면 구독하세요.
구독

DeepSeek-R1은 더 작고 정제된 버전들을 포함하는 오픈 웨이트(open weights) 모델로서, OpenAI O1과 같은 선도적인 추론 모델과 유사한 성능을 달성하기 위한 훈련 방식을 공유하고 반영합니다. 이 게시물에서는 DeepSeek-R1이 어떻게 구축되었는지 심층적으로 살펴보겠습니다.

**목차:**
*   요약: 대규모 언어 모델(LLM) 훈련의 기본 원리
*   DeepSeek-R1 훈련 레시피의 핵심 요소
    *   1- 방대한 양의 긴 사고의 연쇄(chain of thought) SFT 데이터 활용
    *   2- 비추론 작업에서 한계가 있는 중간 고품질 추론 LLM의 역할
    *   3- 대규모 강화 학습(RL)을 통한 추론 모델 구축 전략
        *   3.1- 대규모 추론 지향 강화 학습(R1-Zero)의 혁신
        *   3.2- 중간 추론 모델을 활용한 SFT 추론 데이터 생성
        *   3.3- 범용성을 위한 일반 RL 훈련 단계
*   DeepSeek-R1의 아키텍처(Architecture) 특징
*   DeepSeek-R1 이후의 발전과 시사점
*   결론

이러한 모델이 어떻게 작동하는지 이해하는 데 필요한 대부분의 기초 지식은 저희 책인 Hands-On Large Language Models에서 찾아볼 수 있습니다.
책의 공식 웹사이트.
Amazon에서 책을 주문할 수 있습니다.
모든 코드는 GitHub에 업로드되어 있습니다.

**요약: 대규모 언어 모델(LLM) 훈련의 기본 원리**

대부분의 일반적인 LLM과 마찬가지로 DeepSeek-R1은 한 번에 하나의 토큰(token)을 생성합니다. 하지만, 사고의 연쇄(chain of thought)를 설명하는 사고 토큰(thinking tokens)을 생성하는 과정을 통해 문제를 처리하는 데 더 많은 시간을 할애할 수 있기 때문에 수학 및 추론 문제 해결에 뛰어난 성능을 보입니다.

고품질 LLM 훈련의 일반적인 세 단계 과정은 다음과 같습니다.

1) 방대한 양의 웹 데이터를 사용하여 다음 단어를 예측하도록 모델을 훈련하는 언어 모델링(language modeling) 단계입니다. 이 단계는 기본 모델(base model)을 생성합니다.
2) 모델이 지시를 따르고 질문에 답변하는 데 더 유용하도록 만드는 지도 미세 조정(supervised fine-tuning) 단계입니다. 이 단계는 지시 조정 모델(instruction tuned model) 또는 지도 미세 조정(SFT) 모델을 생성합니다.
3) 그리고 마지막으로 행동을 더욱 다듬고 인간의 선호도에 맞춰 조정하는 선호도 조정(preference tuning) 단계로, 여러분이 플레이그라운드(playground)나 앱에서 상호작용하는 최종 선호도 조정 LLM을 만듭니다.

**DeepSeek-R1 훈련 레시피의 핵심 요소**

DeepSeek-R1은 이러한 일반적인 LLM 훈련 과정을 따릅니다. 첫 번째 단계의 세부 사항은 DeepSeek-V3 모델에 대한 초기 연구 논문에서 가져왔습니다. R1은 해당 선행 연구의 기본 모델(최종 DeepSeek-v3 모델은 아님)을 사용하며, 여전히 SFT 및 선호도 조정 단계를 거치지만, 이를 수행하는 방식에 있어 독자적인 접근법을 취합니다.

R1 생성 과정에서 강조할 세 가지 특별한 점이 있습니다.

**1- 방대한 양의 긴 사고의 연쇄(chain of thought) SFT 데이터 활용**

이는 방대한 양의 긴 사고의 연쇄(chain-of-thought) 추론 예시(60만 개)를 포함합니다. 이러한 예시는 수집하기 난해하며, 이 규모에서 사람이 라벨링(labeling)하는 데 상당한 비용이 수반됩니다. 그렇기 때문에 이를 생성하는 과정이 강조할 두 번째 특별한 점입니다.

**2- 비추론 작업에서 한계가 있는 중간 고품질 추론 LLM의 역할**

이 데이터는 R1의 전신인, 추론에 특화된 초기 추론 전용 모델에 의해 생성됩니다. 이 모델은 R1-Zero라고 불리는 세 번째 모델(곧 논의할 예정)에서 영감을 받았습니다. 이것이 중요한 이유는 그 자체로 범용적인 LLM은 아니었지만, 대규모 강화 학습(reinforcement learning, RL)과 함께 매우 적은 양의 라벨링된(labeled) 데이터만으로도 추론 문제 해결에 뛰어난 성능을 가진 모델을 구축할 수 있었기 때문입니다. 이 전문 추론 모델의 출력은 사용자들이 LLM에 기대하는 수준으로 다른 비추론 작업도 수행할 수 있는 더 일반적인 모델을 훈련하는 데 사용될 수 있습니다.

**3- 대규모 강화 학습(RL)을 통한 추론 모델 구축 전략**

이것은 두 단계로 이루어집니다.

**3.1- 대규모 추론 지향 강화 학습(R1-Zero)의 혁신**

여기서 RL은 중간 추론 모델을 생성하는 데 사용됩니다. 이 모델은 SFT 추론 예시를 생성하는 데 활용됩니다. 하지만 이 모델 생성을 가능하게 한 것은 DeepSeek-R1-Zero라고 불리는 선구적인 모델을 생성한 초기 실험입니다.

R1-Zero는 라벨링된 SFT 훈련 세트 없이도 추론 작업에 뛰어난 성능을 보인다는 점에서 주목할 만합니다. 그 훈련은 사전 훈련된(pre-trained) 기본 모델에서 RL 훈련 과정을 통해 직접 이루어집니다(SFT 단계 없음). 이것은 당시 o1 모델과 견줄 만한 성능을 보였습니다. 데이터가 항상 ML 모델 능력의 연료였기 때문에 이것은 중요합니다. 이 모델은 어떻게 이러한 기존 관념을 뛰어넘을 수 있었을까요?

이것은 두 가지를 시사합니다.

1- 현대의 기본 모델은 특정 품질 및 능력 임계값(threshold)을 넘어섰습니다(이 기본 모델은 14.8조 개의 고품질 토큰으로 훈련되었습니다).
2- 일반적인 채팅이나 글쓰기 요청과 달리 추론 문제는 자동으로 검증되거나 라벨링될 수 있습니다.

예시를 통해 이를 보여드리겠습니다.

**예시: 추론 문제의 자동 검증**

이것은 이 RL 훈련 단계의 일부인 프롬프트(prompt)/질문이 될 수 있습니다.

```python
숫자 목록을 받아 정렬된 순서로 반환하되, 시작 부분에 42를 추가하는 파이썬(Python) 코드를 작성하세요.
```

이러한 질문은 여러 가지 자동 검증(automatic verification) 방식에 활용될 수 있습니다. 훈련 중인 모델에 이를 제시하고, 모델이 완성(completion)을 생성한다고 상정해 볼 수 있습니다.

*   소프트웨어 린터(linter)는 완성된 코드가 적절한 파이썬 코드인지 아닌지 확인할 수 있습니다.
*   파이썬 코드를 실행하여 작동하는지 확인할 수 있습니다.
*   다른 현대적인 코딩 LLM은 원하는 동작을 검증하기 위한 단위 테스트(unit tests)를 생성할 수 있습니다(스스로 추론 전문가가 아니더라도).
*   심지어 한 단계 더 나아가 실행 시간을 측정하고, 문제가 해결되는 올바른 파이썬 프로그램이라 할지라도, 훈련 과정이 다른 해결책보다 더 성능이 좋은 해결책을 선호하도록 만들 수 있습니다.

훈련 단계에서 모델에 이와 같은 질문을 제시하고, 여러 가지 가능한 해결책을 생성할 수 있습니다. 우리는 (인간의 개입 없이) 자동으로 확인하여 첫 번째 완성은 코드가 아님을 판단할 수 있습니다. 두 번째는 코드이지만 파이썬 코드가 아닙니다. 세 번째는 가능한 해결책이지만 단위 테스트에 실패하고, 네 번째는 올바른 해결책으로 분류됩니다.

이것들은 모두 모델을 개선하는 데 직접 사용될 수 있는 귀중한 피드백으로 활용됩니다. 물론 이것은 많은 예시(미니 배치(mini-batches) 단위)와 연속적인 훈련 단계를 거쳐 진행됩니다. 이러한 보상 신호(reward signals)와 모델 업데이트(model updates)는 논문의 그림 2에서 볼 수 있듯이 RL 훈련 과정에서 모델이 작업을 계속 개선하는 핵심적인 메커니즘을 이룹니다. 이 능력의 개선과 함께 나타나는 현상은 생성된 응답의 길이인데, 모델은 문제를 처리하기 위해 더 많은 사고 토큰을 생성합니다.

이 과정은 유용하지만, R1-Zero 모델은 이러한 추론 문제에서 높은 점수를 받았음에도 불구하고, 원하는 것보다 사용성이 떨어지게 만드는 다른 문제들에 직면합니다.

DeepSeek-R1-Zero는 강력한 추론 능력을 보여주고 예상치 못한 강력한 추론 행동을 자율적으로 개발하지만, 가독성(readability) 저하 및 언어 혼합(language mixing)과 같은 여러 문제에 직면합니다.

R1은 더 사용 가능한 모델이 되도록 설계되었습니다. 따라서 RL 과정에 전적으로 의존하는 대신, 이 섹션에서 앞서 언급했듯이 다음과 같은 두 가지 방식으로 활용됩니다.

1- SFT 데이터 포인트(data points)를 생성하기 위한 중간 추론 모델 생성
2- 추론 및 비추론 문제 개선을 위한 R1 모델 훈련 (다른 유형의 검증자(verifiers) 사용)

**3.2- 중간 추론 모델을 활용한 SFT 추론 데이터 생성**

중간 추론 모델의 활용도를 높이기 위해, 수천 개의 추론 문제 예시(일부는 R1-Zero에서 생성 및 필터링됨)에 대해 지도 미세 조정(SFT) 훈련 단계를 거칩니다. 논문에서는 이를 "콜드 스타트(cold start) 데이터"라고 지칭합니다.

2.3.1. 콜드 스타트

DeepSeek-R1-Zero와 달리, 기본 모델로부터 RL 훈련의 초기 불안정한 콜드 스타트 단계를 방지하기 위해, DeepSeek-R1의 경우 초기 RL 액터(actor)로서 모델을 미세 조정하기 위해 소량의 긴 CoT(Chain-of-Thought) 데이터를 구축하고 수집합니다. 이러한 데이터를 수집하기 위해 우리는 여러 접근 방식을 탐색했습니다. 긴 CoT를 예시로 사용하는 퓨샷 프롬프팅(few-shot prompting), 모델이 반성(reflection) 및 검증(verification)을 통해 상세한 답변을 직접 생성하도록 프롬프트(prompt)하는 것, DeepSeek-R1-Zero 출력을 읽기 쉬운 형식으로 수집하는 것, 그리고 인간 주석자(human annotators)의 후처리(post-processing)를 통해 결과를 정제하는 것입니다.

하지만 잠시만요, 이 데이터가 있다면 왜 강화 학습(RL) 과정이 필요했을까요? 그것은 데이터의 규모 때문입니다. 이 데이터셋은 5,000개의 예시로 확보 가능한 수준이지만, R1을 훈련시키려면 600,000개의 예시가 필요했습니다. 이 중간 모델은 이러한 데이터 격차를 해소하고 대규모의 귀중한 데이터를 인공적으로 생성하는 데 기여합니다.

지도 미세 조정(SFT) 개념이 처음이라면, 이는 프롬프트(prompt)와 올바른 완성(correct completion) 형태로 모델에 훈련 예시를 제시하는 과정입니다. SFT는 모델이 특정 작업을 수행하도록 가르치는 핵심적인 단계입니다.

**3.3- 범용성을 위한 일반 RL 훈련 단계**

이것은 R1이 추론뿐만 아니라 다른 비추론 작업에서도 뛰어난 성능을 발휘하도록 합니다. 이 과정은 이전에 보았던 RL 과정과 유사한 접근 방식을 따릅니다. 그러나 비추론 애플리케이션(application)으로 확장되므로, 이러한 애플리케이션에 속하는 프롬프트에 대해 유용성(helpfulness) 및 안전성(safety) 보상 모델(Llama 모델과 다르지 않음)을 적용합니다.

**DeepSeek-R1의 아키텍처(Architecture) 특징**

초기 대규모 언어 모델과 유사하게, DeepSeek-R1은 트랜스포머 디코더 블록(Transformer decoder blocks)의 스택(stack)으로 구성됩니다. 총 61개의 블록으로 이루어져 있으며, 처음 세 개는 덴스(dense) 레이어이지만, 나머지는 전문가 혼합(mixture-of-experts, MoE) 레이어입니다. MoE 아키텍처는 모델의 효율성과 확장성을 크게 향상시키는 기술로, 최근 LLM 개발의 주요 트렌드 중 하나입니다.

모델 차원 크기(model dimension size) 및 기타 하이퍼파라미터(hyperparameters) 측면에서 다음과 같습니다.

모델 아키텍처(architecture)에 대한 더 자세한 내용은 DeepSeek 팀의 관련 연구 논문에서 상세히 다루고 있습니다.
DeepSeek-V3 Technical Report
DeepSeekMoE: Towards Ultimate Expert Specialization in Mixture-of-Experts Language Models

**DeepSeek-R1 이후의 발전과 시사점**

DeepSeek-R1이 발표된 이후, LLM 분야는 더욱 빠르게 진화했습니다. R1이 보여준 추론 능력 강화와 MoE 아키텍처의 활용은 현재 많은 최신 모델들의 설계에 영감을 주었습니다. 특히, 자동 검증을 통한 강화 학습 접근 방식은 인간의 개입을 최소화하면서 모델의 특정 능력을 고도화하는 데 중요한 패러다임을 제시했습니다. 2025년 현재, 이러한 '사고의 연쇄'와 '강화 학습 기반의 자율적 개선'은 더욱 정교해지고 있으며, 멀티모달(multimodal) 능력과의 결합을 통해 단순히 텍스트를 넘어선 복합적인 추론 작업에서도 탁월한 성능을 보이는 모델들이 등장하고 있습니다. DeepSeek-R1은 이러한 발전의 중요한 초석을 놓은 모델로 평가받고 있습니다.

**결론**

이로써 DeepSeek-R1 모델을 이해하는 데 필요한 주요 직관을 얻으셨을 것입니다. 이 모델은 고급 추론 능력을 갖춘 LLM을 구축하기 위한 혁신적인 접근 방식과 기술적 통찰력을 제공합니다. 특히, 데이터 라벨링의 한계를 극복하고 자동 검증을 통해 모델을 고도화하는 RL 기반 전략은 인상적입니다.

추천 자료는 다음과 같습니다.

*   Maarten Grootendorst의 A Visual Guide to Reasoning LLMs
*   Nathan Lambert의 DeepSeek R1's recipe to replicate o1 and the future of reasoning LMs
*   Maarten Grootendorst의 A Visual Guide to Mixture of Experts (MoE)
*   Sasha Rush의 YouTube 비디오 Speculations on Test-Time Scaling (o1)
*   Yannis Kilcher의 DeepSeekMath: Pushing the Limits of Mathematical Reasoning in Open Language Models (Paper Explained)
*   Open R1은 DeepSeek-R1을 공개적으로 재현하기 위한 HuggingFace 프로젝트입니다.
*   Putting RL back in RLHF
*   최근 MoE 모델의 발전 동향 및 효율성 연구 (새로운 시사점)
*   자동화된 평가 지표를 활용한 LLM 추론 능력 향상 사례 연구 (최신 연구)

이 논문을 읽는 동안 2022년의 Galactica 논문이 떠올랐습니다. 이 논문에는 전용 사고 토큰(thinking token)을 포함한 많은 훌륭한 아이디어가 있었고, DeepSeek-R1의 접근 방식과 연관 지어 생각해 볼 수 있습니다. 이는 LLM의 추론 능력 향상을 위한 아이디어들이 꾸준히 발전해 왔음을 보여줍니다.

Language Models & Co.를 읽어주셔서 감사합니다! 새로운 게시물을 무료로 받아보고 제 작업을 지원하려면 구독하세요.
구독