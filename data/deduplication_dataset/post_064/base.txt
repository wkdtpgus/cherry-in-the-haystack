# ğŸ¥‡ **ì´ë²ˆ ì£¼ ì£¼ìš” AI ë…¼ë¬¸**

Author: Elvis Saravia
URL: https://nlp.elvissaravia.com/p/top-ai-papers-of-the-week-1b5

============================================================

1.  **DeepSeek-OCR** DeepSeek-OCRì€ ìƒˆë¡œìš´ ì‹œê° ì¸ì½”ë” ì•„í‚¤í…ì²˜(vision encoder architecture, DeepEncoder)ë¥¼ ì‚¬ìš©í•˜ì—¬ ê¸´ í…ìŠ¤íŠ¸ ì»¨í…ìŠ¤íŠ¸ë¥¼ ì‹œê°ì  í‘œí˜„ìœ¼ë¡œ ì••ì¶•í•˜ëŠ” ë°©ë²•ì„ íƒêµ¬í•˜ë©°, ë†’ì€ OCR ì •í™•ë„(OCR accuracy)ë¥¼ ìœ ì§€í•˜ë©´ì„œ 10-20ë°°ì˜ ì••ì¶•ë¥ ì„ ë‹¬ì„±í•©ë‹ˆë‹¤. í•µì‹¬ ì••ì¶• í†µì°°(compression insight): ì´ë¯¸ì§€ë¥¼ í…ìŠ¤íŠ¸ë¥¼ ìœ„í•œ íš¨ìœ¨ì ì¸ ì••ì¶• ë§¤ì²´ë¡œ ì·¨ê¸‰í•©ë‹ˆë‹¤. 10ë°° ì••ì¶•(1000ê°œ í…ìŠ¤íŠ¸ í† í°ì„ 100ê°œ ì‹œê° í† í°(vision tokens)ìœ¼ë¡œ) ì‹œ 97%ì˜ OCR ì •í™•ë„ë¥¼ ë‹¬ì„±í•©ë‹ˆë‹¤. 20ë°° ì••ì¶•ì—ì„œë„ ì•½ 60%ì˜ ì •í™•ë„ë¥¼ ìœ ì§€í•˜ì—¬, LLM ë©”ëª¨ë¦¬ ë©”ì»¤ë‹ˆì¦˜(LLM memory mechanisms)ì„ ìœ„í•œ ê´‘í•™ ì»¨í…ìŠ¤íŠ¸ ì••ì¶•(optical context compression)ì˜ ì‹¤í˜„ ê°€ëŠ¥ì„±ì„ ë³´ì—¬ì¤ë‹ˆë‹¤. DeepEncoder ì•„í‚¤í…ì²˜: SAM-base(80M, ìœˆë„ìš° ì–´í…ì…˜(window attention))ì™€ CLIP-large(300M, ì „ì—­ ì–´í…ì…˜(global attention))ë¥¼ 16ë°° ì»¨ë³¼ë£¨ì…˜ ì••ì¶•ê¸°(convolutional compressor)ë¥¼ í†µí•´ ê²°í•©í•©ë‹ˆë‹¤. ìˆœì°¨ì  ì„¤ê³„ëŠ” ìœˆë„ìš° ì–´í…ì…˜ì´ ë†’ì€ í† í° ìˆ˜ ì´ë¯¸ì§€ë¥¼ ì²˜ë¦¬í•˜ë„ë¡ ë³´ì¥í•˜ë©°, ë°€ì§‘ ì „ì—­ ì–´í…ì…˜(dense global attention) ì´ì „ì— ì••ì¶•ì´ ì´ë£¨ì–´ì ¸ ê³ í•´ìƒë„(1024x1024ì—ì„œ 256ê°œ ì‹œê° í† í°ë§Œ ìƒì„±)ì—ì„œë„ ë‚®ì€ í™œì„±í™” ë©”ëª¨ë¦¬(activation memory)ë¥¼ ìœ ì§€í•©ë‹ˆë‹¤. ë‹¤ì¤‘ í•´ìƒë„ ìœ ì—°ì„±(Multi-resolution flexibility): ê¸°ë³¸ í•´ìƒë„(native resolutions, Tiny: 64ê°œ í† í°, Small: 100ê°œ, Base: 256ê°œ, Large: 400ê°œ) ë° ë™ì  íƒ€ì¼ë§(dynamic tiling, Gundam mode: nÃ—100+256ê°œ í† í°)ì„ ì§€ì›í•©ë‹ˆë‹¤. ë‹¨ì¼ ëª¨ë¸ì´ ëª¨ë“  í•´ìƒë„ ëª¨ë“œì—ì„œ ë™ì‹œ í›ˆë ¨ì„ í†µí•´ ì—¬ëŸ¬ ì••ì¶•ë¥ ì„ ì²˜ë¦¬í•˜ì—¬ ì••ì¶•-í’ˆì§ˆ ì ˆì¶©(compression-quality trade-offs)ì„ ê°€ëŠ¥í•˜ê²Œ í•©ë‹ˆë‹¤. ìƒìš©í™” ì¤€ë¹„ ì„±ëŠ¥(Production-ready performance): GOT-OCR2.0ì„ 256ê°œ ì‹œê° í† í° ëŒ€ì‹  100ê°œ ì‹œê° í† í°ë§Œ ì‚¬ìš©í•˜ì—¬ ëŠ¥ê°€í•˜ë©°, MinerU2.0(í˜ì´ì§€ë‹¹ 6000ê°œ ì´ìƒ í† í°)ì„ 800ê°œ ë¯¸ë§Œ í† í°ìœ¼ë¡œ ëŠ¥ê°€í•©ë‹ˆë‹¤. ë‹¨ì¼ A100-40G GPUì—ì„œ í•˜ë£¨ 20ë§Œ í˜ì´ì§€ ì´ìƒì„ ì²˜ë¦¬í•©ë‹ˆë‹¤. ì—”ë“œíˆ¬ì—”ë“œ ëª¨ë¸(end-to-end models) ì¤‘ ê°€ì¥ ì ì€ ì‹œê° í† í°ìœ¼ë¡œ OmniDocBenchì—ì„œ SOTA(State-Of-The-Art)ë¥¼ ë‹¬ì„±í•©ë‹ˆë‹¤. í™•ì¥ëœ ê¸°ëŠ¥(Extended capabilities): ìˆœìˆ˜ OCR ì™¸ì—ë„ ì‹¬ì¸µ íŒŒì‹±(deep parsing, ì°¨íŠ¸-HTML í…Œì´ë¸”(chart-to-HTML table), í™”í•™ì‹-SMILES(chemical formula-to-SMILES), ê¸°í•˜í•™ íŒŒì‹±(geometry parsing)), ë‹¤êµ­ì–´ ì¸ì‹(multilingual recognition, ì•½ 100ê°œ ì–¸ì–´), ê·¸ë¦¬ê³  70% OCR ë°ì´í„° + 20% ì¼ë°˜ ì‹œê° + 10% í…ìŠ¤íŠ¸ ì „ìš© í›ˆë ¨ í˜¼í•©(text-only training mix)ì„ í†µí•œ ì¼ë°˜ ì‹œê° ì´í•´(general vision understanding)ë¥¼ ì§€ì›í•©ë‹ˆë‹¤. Paper | Tweet
2.  **í¬ì†Œ ë©”ëª¨ë¦¬ ë¯¸ì„¸ ì¡°ì •ì„ í†µí•œ ì§€ì† í•™ìŠµ(Continual Learning via Sparse Memory Finetuning)** Meta AI ì—°êµ¬ì›ë“¤ì€ í¬ì†Œ ë©”ëª¨ë¦¬ ë¯¸ì„¸ ì¡°ì •(sparse memory finetuning)ì„ í†µí•´ ì–¸ì–´ ëª¨ë¸ì˜ ì¹˜ëª…ì  ë§ê°(catastrophic forgetting) ë¬¸ì œë¥¼ í•´ê²°í•˜ë©°, ìƒˆë¡œìš´ ì§€ì‹ì— ì˜í•´ ê°€ì¥ ë§ì´ í™œì„±í™”ëœ ë©”ëª¨ë¦¬ ìŠ¬ë¡¯ë§Œ ì—…ë°ì´íŠ¸í•˜ì—¬ í‘œì¤€ ë¯¸ì„¸ ì¡°ì •(standard finetuning)ë³´ë‹¤ 89% ì ì€ ì„±ëŠ¥ ì €í•˜(performance degradation)ë¥¼ ë‹¬ì„±í•©ë‹ˆë‹¤. í•µì‹¬ ë¬¸ì œ(Core problem): ì–¸ì–´ ëª¨ë¸ì€ ìƒˆë¡œìš´ ì •ë³´ë¡œ ì—…ë°ì´íŠ¸í•  ë•Œ ì¹˜ëª…ì  ë§ê°ì„ ê²ªìœ¼ë©°, ì´ì „ì— ìŠµë“í•œ ëŠ¥ë ¥ì„ ìƒìŠµë‹ˆë‹¤. í‘œì¤€ ë¯¸ì„¸ ì¡°ì •ì€ 89%ì˜ ì„±ëŠ¥ ì €í•˜ë¥¼ ìœ ë°œí•˜ê³ , LoRAëŠ” ë³´ë¥˜ëœ ì‘ì—…(held-out tasks)ì—ì„œ 71%ì˜ ì €í•˜ë¥¼ ì´ˆë˜í•˜ì—¬, ê³ ë¹„ìš© ë°ì´í„° ì¬ìƒ ì „ëµ(expensive data replay strategies) ì—†ì´ëŠ” ì§€ì† í•™ìŠµ(continual learning)ì„ ë¹„ì‹¤ìš©ì ìœ¼ë¡œ ë§Œë“­ë‹ˆë‹¤. ë©”ëª¨ë¦¬ ê³„ì¸µ ì•„í‚¤í…ì²˜(Memory layer architecture): í”¼ë“œí¬ì›Œë“œ ê³„ì¸µ(feedforward layers)ì„ í¬ì†Œ ë§¤ê°œë³€ìˆ˜ ë©”ëª¨ë¦¬ í’€(sparse parametric memory pools, 1-10M ìŠ¬ë¡¯)ë¡œ ëŒ€ì²´í•˜ë©°, ê° ìˆœë°©í–¥ ì „ë‹¬(forward pass)ì€ ì‘ì€ í•˜ìœ„ ì§‘í•©(ì˜ˆ: 1ë§Œ ê°œ ë§¤ê°œë³€ìˆ˜)ì—ë§Œ ì ‘ê·¼í•©ë‹ˆë‹¤. ì´ëŠ” ì „ì²´ ìš©ëŸ‰ê³¼ ì§€ì‹ ì¡°ê°ë‹¹ ìµœì†Œ ë§¤ê°œë³€ìˆ˜ ì‚¬ì´ì˜ ê· í˜•ì„ ì œê³µí•˜ì—¬ ì •ë³´ ì €ì¥ì— ëŒ€í•œ ì„¸ë¶„í™”ëœ ì œì–´(granular control)ë¥¼ ê°€ëŠ¥í•˜ê²Œ í•©ë‹ˆë‹¤. í¬ì†Œì„±ì„ ìœ„í•œ TF-IDF ìˆœìœ„(TF-IDF ranking for sparsity): ë°°ê²½ ì½”í¼ìŠ¤(background corpus, ì‚¬ì „ í›ˆë ¨ ë°ì´í„°)ì— ëŒ€í•œ ìš©ì–´ ë¹ˆë„-ì—­ ë¬¸ì„œ ë¹ˆë„ ì ìˆ˜(term frequency-inverse document frequency scores)ë¥¼ ê³„ì‚°í•˜ì—¬ ìƒˆ ì…ë ¥ì— íŠ¹ì •í•œ ë©”ëª¨ë¦¬ ìŠ¬ë¡¯ì„ ì‹ë³„í•©ë‹ˆë‹¤. ìƒˆ ë°°ì¹˜ì—ì„œ ë§ì´ ì ‘ê·¼ë˜ì§€ë§Œ ì¼ë°˜ ì§€ì‹ì—ì„œëŠ” ë“œë¬¼ê²Œ ì‚¬ìš©ë˜ëŠ” ìƒìœ„ tê°œ ìŠ¬ë¡¯(ì˜ˆ: 100ë§Œ ê°œ ì¤‘ 500ê°œ)ë§Œ ì—…ë°ì´íŠ¸í•˜ì—¬ ê°„ì„­ ìµœì†Œí™”(minimizing interference)ë¥¼ ë‹¬ì„±í•©ë‹ˆë‹¤. ê²½í—˜ì  ê²€ì¦(Empirical validation): TriviaQA ì‚¬ì‹¤ í•™ìŠµ(TriviaQA fact learning)ì—ì„œ í¬ì†Œ ë©”ëª¨ë¦¬ ë¯¸ì„¸ ì¡°ì •ì€ NaturalQuestionsì—ì„œ 11%ì˜ ì„±ëŠ¥ ì €í•˜ë§Œ ë‹¬ì„±(ì „ì²´ ë¯¸ì„¸ ì¡°ì •ì˜ ê²½ìš° 89%, LoRAì˜ ê²½ìš° 71%)í•˜ë©´ì„œ ë™ë“±í•œ ìƒˆ ì§€ì‹(equivalent new knowledge)ì„ í•™ìŠµí•©ë‹ˆë‹¤. ì‚¬ì‹¤ í•™ìŠµê³¼ ë¬¸ì„œ QA ì‘ì—…(document QA tasks) ëª¨ë‘ì—ì„œ í•™ìŠµ-ë§ê° ì ˆì¶© ê²½ê³„(learning-forgetting tradeoff frontier)ë¥¼ ë”°ë¼ ê¸°ì¤€ì„ ì„ íŒŒë ˆí†  ì§€ë°°(Pareto dominates)í•©ë‹ˆë‹¤. í•µì‹¬ ì„¸íŠ¸ ë¶„ì„(Core set analysis): ì‚¬ì‹¤ì€ ì¼ë°˜ì ìœ¼ë¡œ ì—”í‹°í‹° ê²½ê³„(entity boundaries)ì™€ ì¼ì¹˜í•˜ëŠ” "í•µì‹¬ ì„¸íŠ¸(core sets)"ë¥¼ í˜•ì„±í•˜ëŠ” 100-500ê°œì˜ ë©”ëª¨ë¦¬ ì¸ë±ìŠ¤(memory indices)ì— ë¶„ì‚°ë©ë‹ˆë‹¤. TF-IDF ìˆœìœ„(TF-IDF ranking)ëŠ” í…ŒìŠ¤íŠ¸ ì‹œ ì¿¼ë¦¬(test-time queries)ì— ì ‘ê·¼í•˜ì§€ ì•Šê³ ë„ ì´ëŸ¬í•œ ì˜ë¯¸ë¡ ì  ì½˜í…ì¸  ì¸ë±ìŠ¤(semantic content indices)ë¥¼ ì„±ê³µì ìœ¼ë¡œ ì‹ë³„í•˜ì—¬ ëª¨ë¸ì´ ì§€ì†ì ì¸ ê²½í—˜(continual experience)ì„ í†µí•´ ì§€ì‹ì„ ì¶•ì (accumulate knowledge)í•  ìˆ˜ ìˆë„ë¡ í•©ë‹ˆë‹¤. Paper | Tweet
3.  **ëª¨ë¸ì´ ë§¤ë‹ˆí´ë“œ(Manifolds)ë¥¼ ì¡°ì‘í•  ë•Œ(When Models Manipulate Manifolds)** Anthropic ì—°êµ¬ì›ë“¤ì€ Claude 3.5 Haikuê°€ ê³ ì • í­ í…ìŠ¤íŠ¸(fixed-width text)ì—ì„œ ì¤„ ë°”ê¿ˆ(line break)ì„ ì˜ˆì¸¡í•˜ëŠ” ë°©ë²•ì„ ì¡°ì‚¬í•˜ì—¬, ìƒë¬¼í•™ì  ë‡Œì˜ ìƒë¬¼í•™ì  ì¥ì†Œ ì„¸í¬(biological place cells) ë° ê²½ê³„ ì„¸í¬(boundary cells)ì™€ ìœ ì‚¬í•œ ê¸°í•˜í•™ì  í‘œí˜„(geometric representations)ì„ ë°í˜€ëƒˆìŠµë‹ˆë‹¤. í…ìŠ¤íŠ¸ ê³µê°„ì—ì„œì˜ ì§€ê° ì‘ì—…(Perceptual task in text space): ëª¨ë¸ì€ í˜„ì¬ ì¤„ì˜ ë¬¸ì ìˆ˜ë¥¼ ì„¸ê³ , ì¤„ ë„ˆë¹„ ì œì•½(line width constraints)ê³¼ ë¹„êµí•˜ì—¬ ìƒˆ ì¤„ì„ ì‚½ì…í•  ì‹œê¸°ë¥¼ ì˜ˆì¸¡í•´ì•¼ í•©ë‹ˆë‹¤. ì–¸ì–´ ëª¨ë¸ì€ í† í° ì‹œí€€ìŠ¤(token sequences, ì •ìˆ˜)ë§Œ ë°›ìœ¼ë¯€ë¡œ, ëª…ì‹œì  ìœ„ì¹˜ ì •ë³´(explicit position information) ì—†ì´ ì‹œê°/ê³µê°„ ì¶”ë¡ (visual/spatial reasoning)ì„ ì²˜ìŒë¶€í„° í•™ìŠµí•´ì•¼ í•©ë‹ˆë‹¤. í‘œí˜„ì˜ ì´ì¤‘ í•´ì„(Dual interpretation of representations): ë¬¸ì ìœ„ì¹˜ëŠ” ì´ì‚° íŠ¹ì§•(discrete features, í™œì„±í™” ê°•ë„(activation strength)ê°€ ìœ„ì¹˜ë¥¼ ê²°ì •)ê³¼ 1ì°¨ì› íŠ¹ì§• ë§¤ë‹ˆí´ë“œ(one-dimensional feature manifolds, ë§¤ë‹ˆí´ë“œ ìƒì˜ ê°ë„ ì›€ì§ì„(angular movement on the manifold)ì´ ìœ„ì¹˜ë¥¼ ë‚˜íƒ€ëƒ„)ë¡œ ì¸ì½”ë”©ë©ë‹ˆë‹¤. ê³„ì‚°ì€ ì´ì‚° íšŒë¡œ(discrete circuits) ë˜ëŠ” ì”ì°¨ ìŠ¤íŠ¸ë¦¼(residual stream)ì— ëŒ€í•œ ê¸°í•˜í•™ì  ë³€í™˜(geometric transformations)ìœ¼ë¡œ ì´ì¤‘ì ìœ¼ë¡œ ë³¼ ìˆ˜ ìˆìŠµë‹ˆë‹¤. ìƒë¬¼í•™ì  ìœ ì‚¬ì (Biological parallels): í¬ìœ ë¥˜ ì¥ì†Œ ì„¸í¬(mammalian place cells, í™˜ê²½ ë‚´ ìœ„ì¹˜ ì¸ì½”ë”©(encoding location)) ë° ê²½ê³„ ì„¸í¬(boundary cells, ê³µê°„ ê²½ê³„ ê°ì§€(detecting spatial boundaries))ì™€ ìœ ì‚¬í•œ í•™ìŠµëœ ìœ„ì¹˜ í‘œí˜„(learned position representations)ì´ ë°œê²¬ë˜ì—ˆìŠµë‹ˆë‹¤. ì´ë“¤ì€ ì¤„ ë„ˆë¹„ ì œì•½ì´ ìˆëŠ” ì†ŒìŠ¤ ì½”ë“œ, ì±„íŒ… ë¡œê·¸, ì´ë©”ì¼ ì•„ì¹´ì´ë¸Œ, ì‚¬ë²• íŒê²°ì— ëŒ€í•œ í›ˆë ¨ì—ì„œ ìì—°ìŠ¤ëŸ½ê²Œ ë‚˜íƒ€ë‚©ë‹ˆë‹¤. ë¶„ì‚° ê³„ìˆ˜ ì•Œê³ ë¦¬ì¦˜(Distributed counting algorithm): ëª¨ë¸ì€ ëˆ„ì  ìœ„ì¹˜(cumulative position)ë¥¼ ì¶”ì í•˜ê³ , í•™ìŠµëœ ê²½ê³„ í‘œí˜„(learned boundary representations)ê³¼ ë¹„êµí•˜ë©°, ìƒˆ ì¤„ ì˜ˆì¸¡ íŠ¸ë¦¬ê±°(trigger newline predictions)ë¥¼ í•˜ëŠ” ì–´í…ì…˜ í—¤ë“œ(attention heads)ë¥¼ í†µí•´ ë¬¸ì ê³„ìˆ˜ë¥¼ êµ¬í˜„í•©ë‹ˆë‹¤. ë‹¤ë¥¸ ê³„ì¸µì€ ë¬¸ì ëˆ„ì (character accumulation), ê²½ê³„ ê°ì§€(boundary sensing), ìµœì¢… ìƒˆ ì¤„ ì˜ˆì¸¡(final newline prediction)ì„ ìˆœì°¨ì ìœ¼ë¡œ ì²˜ë¦¬í•©ë‹ˆë‹¤. ëª¨ë¸ì˜ ì‹œê°ì  ì°©ì‹œ(Visual illusions in models): ì¸ê°„ì´ ì‹œê°ì  ì°©ì‹œë¥¼ ê²½í—˜í•˜ëŠ” ê²ƒì²˜ëŸ¼, ëª¨ë¸ë„ ì—£ì§€ ì¼€ì´ìŠ¤(edge cases)ì—ì„œ "ì§€ê° ì˜¤ë¥˜(perceptual errors)"ë¥¼ ë³´ì…ë‹ˆë‹¤. ì´ëŠ” ì”ì°¨ ìŠ¤íŠ¸ë¦¼ì˜ ì¶”ìƒì ì¸ ê¸°í•˜í•™ì  êµ¬ì¡°(abstract geometric structures)ê°€ ì¸ê°„ì´ ë¬´ì˜ì‹ì ìœ¼ë¡œ(subconsciously) ìˆ˜í–‰í•˜ëŠ” ë³µì¡í•œ ê³µê°„ ì¶”ë¡  ì‘ì—…(complex spatial reasoning tasks)ì„ ì–´ë–»ê²Œ ê°€ëŠ¥í•˜ê²Œ í•˜ëŠ”ì§€ì— ëŒ€í•œ í”„ë ˆì„ì›Œí¬ë¥¼ ì œê³µí•©ë‹ˆë‹¤. Paper | Tweet
4.  **í—¤ì„¸ í–‰ë ¬ ì—†ëŠ” ë°ì´í„° ê¸°ì—¬ë„ ë¶„ì„ì„ ìœ„í•œ ë² ì´ì¦ˆ ì˜í–¥ í•¨ìˆ˜(Bayesian Influence Functions for Hessian-Free Data Attribution)** ê³ ì „ì ì¸ ì˜í–¥ í•¨ìˆ˜(Classical influence functions)ëŠ” ë¹„ê°€ì—­ í—¤ì„¸ í–‰ë ¬(non-invertible Hessians)ê³¼ ê³ ì°¨ì› ë§¤ê°œë³€ìˆ˜ ê³µê°„(high-dimensional parameter spaces)ìœ¼ë¡œ ì¸í•´ ì‹¬ì¸µ ì‹ ê²½ë§(deep neural networks)ì—ì„œ ì–´ë ¤ì›€ì„ ê²ªìŠµë‹ˆë‹¤. ì´ ì—°êµ¬ëŠ” í™•ë¥ ì  ê²½ì‚¬ MCMC ìƒ˜í”Œë§(stochastic-gradient MCMC sampling)ì„ í†µí•´ ì¶”ì •ëœ ì†ì‹¤ ì§€í˜• í†µê³„(loss landscape statistics)ë¡œ í—¤ì„¸ í–‰ë ¬ ì—­ì‚°(Hessian inversion)ì„ ëŒ€ì²´í•˜ëŠ” ì§€ì—­ ë² ì´ì¦ˆ ì˜í–¥ í•¨ìˆ˜(local Bayesian influence function, BIF)ë¥¼ ì†Œê°œí•©ë‹ˆë‹¤. í•µì‹¬ í˜ì‹ (Core innovation): BIFëŠ” ë¬¸ì œì„± ìˆëŠ” í—¤ì„¸ í–‰ë ¬ ì—­í–‰ë ¬(problematic Hessian inverse)ì„ ê³„ì‚°í•˜ëŠ” ëŒ€ì‹  ì§€ì—­ ì‚¬í›„ ë¶„í¬(local posterior distribution)ì— ëŒ€í•œ ê³µë¶„ì‚° ì¶”ì •(covariance estimation)ì„ ì‚¬ìš©í•©ë‹ˆë‹¤. ì´ ë¶„í¬ì  ì ‘ê·¼ ë°©ì‹(distributional approach)ì€ DNNs(Deep Neural Networks)ì˜ í‡´í™”ëœ ì†ì‹¤ ì§€í˜•(degenerate loss landscapes)ì„ ìì—°ìŠ¤ëŸ½ê²Œ ì²˜ë¦¬í•˜ë©°, ë¹„íŠ¹ì´ ëª¨ë¸(non-singular models)ì˜ ê²½ìš° ê³ ì „ì ì¸ ì˜í–¥ í•¨ìˆ˜ë¡œ ì¶•ì†Œë©ë‹ˆë‹¤. SGLD ê¸°ë°˜ ì¶”ì •(SGLD-based estimation): í™•ë¥ ì  ê²½ì‚¬ ë‘ì£¼ë±… ë™ì—­í•™(stochastic gradient Langevin dynamics)ì„ êµ¬í˜„í•˜ì—¬ ì§€ì—­í™”ëœ ë² ì´ì¦ˆ ì‚¬í›„ ë¶„í¬(localized Bayesian posterior)ì—ì„œ ìƒ˜í”Œë§í•˜ê³ , í›ˆë ¨ ìƒ˜í”Œ ì†ì‹¤(training sample losses)ê³¼ ì¿¼ë¦¬ ê´€ì¸¡ê°’(query observables) ê°„ì˜ ê³µë¶„ì‚°(covariances)ì„ ê³„ì‚°í•©ë‹ˆë‹¤. ì´ ë°©ë²•ì€ ì•„í‚¤í…ì²˜ ë¶ˆê°€ì§€ë¡ ì (architecture-agnostic)ì´ë©° êµ¬ì¡°ì  ê·¼ì‚¬(structural approximations) ì—†ì´ ìˆ˜ì‹­ì–µ ê°œì˜ ë§¤ê°œë³€ìˆ˜ë¡œ í™•ì¥ë©ë‹ˆë‹¤. ê³„ì‚°ìƒì˜ ì ˆì¶©(Computational trade-offs): EK-FACê³¼ ê°™ì€ ê³ ë¹„ìš© ì í•© ë‹¨ê³„(expensive fit phase)ëŠ” ì—†ì§€ë§Œ, ë¹„ìš©ì€ ì‚¬í›„ í‘œë³¸ ì¶”ì¶œ(posterior draws) íšŸìˆ˜ì— ë¹„ë¡€í•©ë‹ˆë‹¤. ì„¸ë¶„í™”ëœ ê¸°ì—¬ë„(fine-grained attribution, í† í°ë³„ ì˜í–¥(per-token influences)ì€ ë³‘ë ¬ë¡œ ê³„ì‚°)ì— ë” íš¨ìœ¨ì ì…ë‹ˆë‹¤. ê³ ì „ì ì¸ ë°©ë²•ì€ ë§ì€ ì¿¼ë¦¬ê°€ ë†’ì€ ì„¤ì • ë¹„ìš© ìƒê°(amortize high setup costs)ì„ í•  ë•Œ íƒì›”í•©ë‹ˆë‹¤. ì‹¤í—˜ì  ê²€ì¦(Experimental validation): ì¬í›ˆë ¨ ì‹¤í—˜(retraining experiments, ì„ í˜• ë°ì´í„° ëª¨ë¸ë§ ì ìˆ˜(Linear Datamodeling Score))ì—ì„œ ìµœì²¨ë‹¨(state-of-the-art, SOTA)ì„ ë‹¬ì„±í•˜ë©°, EK-FAC ê¸°ì¤€ì„  ëŠ¥ê°€(outperforming EK-FAC baseline)í•˜ê±°ë‚˜ ë™ë“±í•œ ì„±ëŠ¥ì„ ë³´ì…ë‹ˆë‹¤. ê°€ì¥ í° Pythia ëª¨ë¸(2.8B ë§¤ê°œë³€ìˆ˜)ì—ì„œ ë™ì¼í•œ GPU ë©”ëª¨ë¦¬ë¥¼ ì‚¬ìš©í•˜ë©´ì„œ 2ë°° ë¹ ë¥¸ í‰ê°€ ì†ë„ë¥¼ ë³´ì—¬ì¤ë‹ˆë‹¤. í•´ì„ ê°€ëŠ¥í•œ í† í°ë³„ ë¶„ì„(Interpretable per-token analysis): ì–¸ì–´ ëª¨ë¸ì˜ ì˜ë¯¸ë¡ ì  ê´€ê³„(semantic relationships)ë¥¼ í¬ì°©í•©ë‹ˆë‹¤. ìƒê´€ê´€ê³„(correlations)ëŠ” ë²ˆì—­, ëŒ€ì²´ ì² ì, ë™ì˜ì–´ì—ì„œ ê·¹ëŒ€í™”ë©ë‹ˆë‹¤. ë¹„ì „ ëª¨ë¸(vision models)ì—ì„œ ìœ ì‚¬í•œ ë²”ì£¼ê°€ ê¸ì •ì  ì˜í–¥(positive influence)ì„ ë³´ì´ëŠ” ê³„ì¸µì  êµ¬ì¡°(hierarchical structure)ë¥¼ ë°í˜€ëƒ…ë‹ˆë‹¤. Paper | Tweet
5.  **ìƒ˜í”Œë§ì„ í†µí•œ ì¶”ë¡ (Reasoning with Sampling)** ê¸°ë³¸ ì–¸ì–´ ëª¨ë¸ì€ í›ˆë ¨, ë°ì´í„°ì…‹ ë˜ëŠ” ê²€ì¦ì(verifiers)ê°€ í•„ìš” ì—†ëŠ” MCMC ê¸°ë²•(MCMC techniques)ì„ ì‚¬ìš©í•˜ì—¬ ì¶”ë¡  ì‹œê°„ ì „ë ¥ ë¶„í¬ ìƒ˜í”Œë§(inference-time power distribution sampling)ì„ í†µí•´ RL ì‚¬í›„ í›ˆë ¨(RL-posttraining)ê³¼ ê°™ê±°ë‚˜ ê·¸ ì´ìƒì˜ ì¶”ë¡  ì„±ëŠ¥(Reasoning performance)ì„ ë‹¬ì„±í•©ë‹ˆë‹¤. í•µì‹¬ í†µì°°(Core insight): RL ì‚¬í›„ í›ˆë ¨ì€ ê·¼ë³¸ì ìœ¼ë¡œ ìƒˆë¡œìš´ í–‰ë™(fundamentally new behaviors)ì„ í•™ìŠµí•˜ê¸°ë³´ë‹¤ëŠ” ê¸°ë³¸ ëª¨ë¸ ë¶„í¬(base model distributions)ë¥¼ ì„ ëª…í•˜ê²Œ í•©ë‹ˆë‹¤. ì „ë ¥ ë¶„í¬(Power distribution, p^Î±) ìƒ˜í”Œë§ì€ ê¸°ë³¸ ëª¨ë¸ ê°€ëŠ¥ë„ ì§€ìˆ˜í™”(exponentiating base model likelihoods)ë¥¼ í†µí•´ ì´ëŸ¬í•œ ì„ ëª…í™”ë¥¼ ëª…ì‹œì ìœ¼ë¡œ ëª©í‘œë¡œ í•˜ë©°, ë¶•ê´´ëœ RL ë¶„í¬(collapsed RL distributions)ì™€ ë‹¬ë¦¬ ë‹¤ì–‘ì„± ìœ ì§€(maintaining diversity)í•˜ë©´ì„œ ê³ í™•ë¥  ì‹œí€€ìŠ¤ ê°€ì¤‘ì¹˜ ë¶€ì—¬(upweighting high-probability sequences)ë¥¼ í•©ë‹ˆë‹¤. ì „ë ¥ ìƒ˜í”Œë§ ëŒ€ ì €ì˜¨ ìƒ˜í”Œë§(Power vs low-temperature sampling): ì €ì˜¨ ìƒ˜í”Œë§(low-temperature sampling)ì€ ì¡°ê±´ë¶€ ë‹¤ìŒ í† í° ë¶„í¬(conditional next-token distributions)ë¥¼ ì§€ìˆ˜í™”(í•©ì˜ ì§€ìˆ˜(exponent of sums))í•˜ëŠ” ë°˜ë©´, ì „ë ¥ ìƒ˜í”Œë§(power sampling)ì€ ì§€ìˆ˜í™”ëœ ë¯¸ë˜ ê²½ë¡œ ê°€ëŠ¥ë„ í•©(sums exponentiated future path likelihoods, ì§€ìˆ˜ì˜ í•©(sum of exponents))ì„ ê³„ì‚°í•©ë‹ˆë‹¤. ì´ ì¤‘ìš”í•œ ì°¨ì´ëŠ” ì „ë ¥ ìƒ˜í”Œë§ì´ ë¯¸ë˜ ì™„ì„±(future completions)ì„ ê³ ë ¤í•˜ì—¬, ë§ì€ ì €í™•ë¥  ì™„ì„±(low-likelihood completions)ì„ ê°€ì§„ í† í°ë³´ë‹¤ ì ì§€ë§Œ ê³ í™•ë¥  ê²½ë¡œ(high-likelihood paths)ë¥¼ ê°€ì§„ í† í°ì— ë” ë†’ì€ ê°€ì¤‘ì¹˜ë¥¼ ë¶€ì—¬í•œë‹¤ëŠ” ê²ƒì„ ì˜ë¯¸í•©ë‹ˆë‹¤. MCMC êµ¬í˜„(MCMC implementation): ìê¸°íšŒê·€ ì•Œê³ ë¦¬ì¦˜(Autoregressive algorithm)ì€ ë¬´ì‘ìœ„ ì¬ìƒ˜í”Œë§ì„ ì‚¬ìš©í•œ ë©”íŠ¸ë¡œí´ë¦¬ìŠ¤-í•´ìŠ¤íŒ…ìŠ¤(Metropolis-Hastings with random resampling)ë¥¼ í†µí•´ ì¤‘ê°„ ë¶„í¬ë¥¼ ì ì§„ì ìœ¼ë¡œ ìƒ˜í”Œë§(progressively samples intermediate distributions)í•©ë‹ˆë‹¤. ì¸ë±ìŠ¤ë¥¼ ê· ì¼í•˜ê²Œ ì„ íƒí•˜ê³ , ì œì•ˆ LLM(proposal LLM)ì„ ì‚¬ìš©í•˜ì—¬ í•´ë‹¹ ì§€ì ì—ì„œ ì¬ìƒ˜í”Œë§í•˜ë©°, ìƒëŒ€ì  ì „ë ¥ ë¶„í¬ ê°€ëŠ¥ë„(relative power distribution likelihoods)ë¥¼ ê¸°ë°˜ìœ¼ë¡œ ìˆ˜ë½/ê±°ë¶€í•©ë‹ˆë‹¤. ë¸”ë¡ í¬ê¸° B=192, Î±=4.0, ì¶”ë¡  ë¹„ìš©(inference cost)ì€ í‘œì¤€ ìƒ˜í”Œë§ì˜ ì•½ 8.84ë°°ì…ë‹ˆë‹¤. ê²½í—˜ì  ê²°ê³¼(Empirical results): Qwen2.5-Math-7Bì—ì„œ MATH500ì—ì„œ 74.8%ë¥¼ ë‹¬ì„±(GRPOëŠ” 78.5%)í•˜ì§€ë§Œ, ë„ë©”ì¸ ì™¸ ì‘ì—…(out-of-domain tasks)ì—ì„œëŠ” ë” ë‚˜ì€ ì„±ëŠ¥ì„ ë³´ì…ë‹ˆë‹¤. HumanEvalì—ì„œ 57.3%(GRPOëŠ” 53.7%), AlpacaEval ì ìˆ˜(AlpacaEval score)ì—ì„œ 2.88ì (GRPOëŠ” 2.38ì )ì„ ê¸°ë¡í•©ë‹ˆë‹¤. RLì˜ ëª¨ë“œ ë¶•ê´´(RLâ€™s mode collapse)ë¥¼ í”¼í•˜ë©´ì„œ k>1ì—ì„œ ìš°ìˆ˜í•œ pass@k ì„±ëŠ¥(superior pass@k performance)ìœ¼ë¡œ ìƒì„± ë‹¤ì–‘ì„±(generation diversity)ì„ ìœ ì§€í•©ë‹ˆë‹¤. í›ˆë ¨ ì—†ëŠ” ì´ì (Training-free advantage): í•˜ì´í¼íŒŒë¼ë¯¸í„° íƒìƒ‰(hyperparameter sweeps), ì„ ë³„ëœ ë°ì´í„°ì…‹(curated datasets), ë³´ìƒ ê²€ì¦ì(reward verifiers)ê°€ í•„ìš” ì—†ìŠµë‹ˆë‹¤. ê²€ì¦ ê°€ëŠ¥í•œ ë„ë©”ì¸(verifiable domains)ì„ ë„˜ì–´ ê´‘ë²”ìœ„í•˜ê²Œ ì ìš© ê°€ëŠ¥í•©ë‹ˆë‹¤. 679ê°œ í† í°ì˜ í‰ê·  ì‘ë‹µ ê¸¸ì´ë¥¼ ìœ ì§€í•˜ë©´ì„œ ìµœê³  ê¸°ë³¸ ëª¨ë¸ ê°€ëŠ¥ë„/ì‹ ë¢° ì˜ì—­(highest base model likelihood/confidence regions)ì—ì„œ ìƒ˜í”Œë§(GRPOì™€ ìœ ì‚¬)í•˜ë©°, ì´ëŠ” ê¸°ë³¸ ëª¨ë¸ì— ì ì¬ì  ì¶”ë¡  ëŠ¥ë ¥(latent reasoning capabilities)ì´ ì¡´ì¬í•¨ì„ ì‹œì‚¬í•©ë‹ˆë‹¤. Paper | Tweet
6.  **LLMì„ ìœ„í•œ ë¯¸ë¦¬ ë³´ê¸° ë¼ìš°íŒ…(Lookahead Routing for LLMs)** ë¯¸ë¦¬ ë³´ê¸° ë¼ìš°íŒ…(Lookahead Routing)ì€ ì ì¬ì  ëª¨ë¸ ì¶œë ¥ì˜ ì ì¬ í‘œí˜„(latent representations)ì„ ì˜ˆì¸¡í•˜ì—¬ ì „ì²´ ì¶”ë¡ (full inference) ì—†ì´ ë” ì •ë³´ì— ì…ê°í•œ ë¼ìš°íŒ… ê²°ì •(informed routing decisions)ì„ ê°€ëŠ¥í•˜ê²Œ í•˜ëŠ” ì‘ë‹µ ì¸ì‹ LLM ë¼ìš°íŒ… í”„ë ˆì„ì›Œí¬(response-aware LLM routing framework)ì…ë‹ˆë‹¤. ì¿¼ë¦¬ ì „ìš© ë¼ìš°íŒ…ì˜ í•µì‹¬ í•œê³„(Core limitation of query-only routing): ì „í†µì ì¸ ë¼ìš°í„°(Traditional routers)ëŠ” ì…ë ¥ ì¿¼ë¦¬(input queries)ì—ë§Œ ì˜ì¡´í•˜ì—¬ ê²°ì •ì„ ë‚´ë¦¬ë¯€ë¡œ, ìƒì„± ì¤‘ì— ë‚˜íƒ€ë‚˜ëŠ” ì‹¤ì œ ì‘ë‹µ í’ˆì§ˆ(response quality) ë° ì˜ë¯¸ë¡ ì  ì˜ë„(semantic intent)ì— ëŒ€í•œ ì¤‘ìš”í•œ ì •ë³´ë¥¼ ë†“ì¹©ë‹ˆë‹¤. ì´ëŠ” ë³µì¡í•˜ê±°ë‚˜ ëª¨í˜¸í•œ ì¿¼ë¦¬(complex or ambiguous queries)ì— ëŒ€í•´ ìµœì ì´ ì•„ë‹Œ ë¼ìš°íŒ…(suboptimal routing)ìœ¼ë¡œ ì´ì–´ì§‘ë‹ˆë‹¤. ì´ì¤‘ êµ¬í˜„ ì•„í‚¤í…ì²˜(Dual implementation architecture): ì‹œí€€ìŠ¤ ìˆ˜ì¤€ ë³€í˜•(Sequence-level variant)ì€ ì¸ê³¼ ì–¸ì–´ ëª¨ë¸(causal language models, CLM)ì„ ì‚¬ìš©í•˜ì—¬ ì¿¼ë¦¬ë¥¼ ëª¨ë¸ ì‹ë³„ì(model identifier, MID) í† í°ê³¼ ì—°ê²°í•˜ê³ , MID ìœ„ì¹˜ì—ì„œ ì€ë‹‰ ìƒíƒœ ì¶”ì¶œ(extracting hidden states)í•˜ì—¬ ì‘ë‹µ í‘œí˜„(response representations)ìœ¼ë¡œ ì‚¬ìš©í•©ë‹ˆë‹¤. í† í° ìˆ˜ì¤€ ë³€í˜•(Token-level variant)ì€ ë§ˆìŠ¤í¬ë“œ ì–¸ì–´ ëª¨ë¸(masked language models, MLM)ì„ ì‚¬ìš©í•˜ì—¬ ë°˜ë³µë˜ëŠ” MID í† í° ë¸”ë¡(repeated MID token blocks)ì„ í†µí•´ ëª¨ë“  í›„ë³´ ì‘ë‹µì„ ê³µë™ìœ¼ë¡œ ì¬êµ¬ì„±í•˜ê³ , [CLS] í† í° ì–´í…ì…˜(attention)ì„ í†µí•´ ì •ë³´ ì§‘ê³„(aggregating information)ë¥¼ í•©ë‹ˆë‹¤. ì»¤ë¦¬í˜ëŸ¼ ë§ˆìŠ¤í‚¹ ì „ëµ(Curriculum masking strategy): MLM ë³€í˜•ì€ ì‘ë‹µ ëì—ì„œ ì‹œì‘ìœ¼ë¡œ ì ì§„ì ìœ¼ë¡œ ë§ˆìŠ¤í¬(progressively masks)í•˜ë©°, í›ˆë ¨ì˜ ì²˜ìŒ 40% ë™ì•ˆ ë§ˆìŠ¤í‚¹ ë¹„ìœ¨(masking ratio)ì„ ì„ í˜•ì ìœ¼ë¡œ 100%ê¹Œì§€ ì¦ê°€ì‹œí‚µë‹ˆë‹¤. ë¶€ë¶„ ë§ˆìŠ¤í‚¹ì—ì„œ ì „ì²´ ë§ˆìŠ¤í‚¹ìœ¼ë¡œì˜ ì´ ë¶€ë“œëŸ¬ìš´ ì „í™˜(smooth transition)ì€ ê· ì¼ ë¬´ì‘ìœ„ ë§ˆìŠ¤í‚¹(uniform random masking)ë³´ë‹¤ ê²¬ê³ í•œ í‘œí˜„(robust representations)ê³¼ ë” ë‚˜ì€ ì¼ë°˜í™”(better generalization)ë¥¼ ê°€ëŠ¥í•˜ê²Œ í•©ë‹ˆë‹¤. ê³µë™ í›ˆë ¨ ëª©í‘œ(Joint training objective): ë¼ìš°íŒ… ì†ì‹¤(routing loss, ëª¨ë¸ ì„ íƒ(model selection)ì— ëŒ€í•œ ì´ì§„ êµì°¨ ì—”íŠ¸ë¡œí”¼(binary cross-entropy))ê³¼ ì‘ë‹µ ì¬êµ¬ì„± ì†ì‹¤(response reconstruction loss, CLMì˜ ë‹¤ìŒ í† í° ì˜ˆì¸¡(next-token prediction), MLMì˜ ë§ˆìŠ¤í¬ëœ í† í° ë³µêµ¬(masked token recovery))ì„ ê²°í•©í•©ë‹ˆë‹¤. ë³´ì¡° ì‘ë‹µ ëª¨ë¸ë§(Auxiliary response modeling)ì€ ìƒ˜í”Œ íš¨ìœ¨ì„±(sample efficiency)ì„ 6.3ë°° í–¥ìƒì‹œí‚¤ê³  ì˜¤ë¼í´ ì‘ë‹µ(oracle responses)ê³¼ì˜ ìƒí˜¸ ì •ë³´(mutual information)ë¥¼ ë†’ì—¬ ë” í’ë¶€í•œ ì˜ë¯¸ë¡ ì  ì •ë³´(richer semantic information)ë¥¼ í¬ì°©í•©ë‹ˆë‹¤. ì„±ëŠ¥(Performance): 7ê°œ ë²¤ì¹˜ë§ˆí¬(benchmarks, AlpacaEval-2, Arena-Hard, MT-Bench, GSM8K, MATH, HumanEval, MBPP)ì—ì„œ SOTA RouterDC ëŒ€ë¹„ í‰ê·  ì •ê·œí™” ì ìˆ˜ ì´ë“(average normalized score gain) 7.7%ë¥¼ ë‹¬ì„±í•©ë‹ˆë‹¤. MLM ë³€í˜•ì€ ê³µë™ ì˜ë¯¸ ê³µê°„ ì¸ì½”ë”©(joint semantic-space encoding)ì´ ì„¸ë¶„í™”ëœ ëª¨ë¸ ê°„ ë¹„êµ(fine-grained cross-model comparisons)ë¥¼ ê°€ëŠ¥í•˜ê²Œ í•˜ëŠ” ê°œë°©í˜• ì§€ì‹œ ë”°ë¥´ê¸° ì‘ì—…(open-ended instruction-following tasks)ì—ì„œ íƒì›”í•©ë‹ˆë‹¤. ê±°ì˜ 100%ì˜ ì½”ë“œ ì¿¼ë¦¬ë¥¼ ì „ë¬¸í™”ëœ Qwen2.5-Coder ëª¨ë¸(specialized Qwen2.5-Coder model)ë¡œ ë¼ìš°íŒ…í•˜ì—¬ ê°•ë ¥í•œ ì „ë¬¸í™” ì¸ì‹(strong specialization awareness)ì„ ë³´ì—¬ì¤ë‹ˆë‹¤. Paper | Tweet
7.  **Ring-1T** Ring-1TëŠ” 1ì¡° ê°œì˜ ë§¤ê°œë³€ìˆ˜(í† í°ë‹¹ ì•½ 500ì–µ ê°œ í™œì„±(active per token))ë¥¼ ê°€ì§„ ìµœì´ˆì˜ ì˜¤í”ˆ ì†ŒìŠ¤ ì‚¬ê³  ëª¨ë¸(open-source thinking model)ì´ë©°, ì¡° ë‹¨ìœ„ RL í›ˆë ¨(trillion-scale RL training)ì„ ìœ„í•œ ì„¸ ê°€ì§€ í˜ì‹ ì„ í†µí•´ íšê¸°ì ì¸ ê²°ê³¼(breakthrough results)ë¥¼ ë‹¬ì„±í•©ë‹ˆë‹¤. ë²¤ì¹˜ë§ˆí¬ ì„±ëŠ¥(Benchmark performance): AIME-2025ì—ì„œ 93.4ì (ìµœê³  ì˜¤í”ˆ ê°€ì¤‘ì¹˜(top open-weights)), HMMT-2025ì—ì„œ 86.72ì , CodeForces ë“±ê¸‰(CodeForces rating) 2088ì (ì „ì²´ ìµœê³ ), ìˆœìˆ˜ ìì—°ì–´ ì¶”ë¡ (pure natural language reasoning)ì„ í†µí•´ IMO-2025 ì€ë©”ë‹¬ì„ íšë“í–ˆìŠµë‹ˆë‹¤. IcePopì€ í›ˆë ¨-ì¶”ë¡  ë¶ˆì¼ì¹˜(training-inference misalignment)ë¥¼ í•´ê²°í•©ë‹ˆë‹¤: ë³„ë„ì˜ í›ˆë ¨/ì¶”ë¡  ì—”ì§„(separate training/inference engines)ì„ ì‚¬ìš©í•˜ë©´ MoE ëª¨ë¸(MoE models)ì—ì„œ ë³µí•©í™”(compound)ë˜ëŠ” í™•ë¥  ë¶ˆì¼ì¹˜(probability discrepancies)ê°€ ë°œìƒí•©ë‹ˆë‹¤. IcePopì€ ê²½ê³„(Î±, Î²) ë‚´ì—ì„œ í† í° ìˆ˜ì¤€ ê¸°ìš¸ê¸° ë³´ì •(token-level gradient calibration)ì„ ì ìš©í•˜ê³  ê³¼ë„í•œ í¸ì°¨ í† í°(excessive-deviation tokens)ì„ ë§ˆìŠ¤í‚¹í•©ë‹ˆë‹¤. 1-2â€°ë§Œ í´ë¦¬í•‘(clipping)ì´ í•„ìš”í•˜ì—¬ ì•ˆì •ì„± ìœ ì§€(maintaining stability)ë¥¼ í•©ë‹ˆë‹¤. C3PO++ëŠ” ë¡¤ì•„ì›ƒ(rollouts) ì†ë„ë¥¼ í–¥ìƒì‹œí‚µë‹ˆë‹¤: ì˜ˆì‚° ì œì–´ íŒŒí‹°ì…”ë‹(Budget-controlled partitioning)ì€ í† í° ì œí•œì—ì„œ ìƒì„±ì„ ì¤‘ë‹¨í•˜ì—¬ ìœ íœ´ ë¦¬ì†ŒìŠ¤(idle resources)ë¥¼ ë°©ì§€í•©ë‹ˆë‹¤. ì™„ë£Œëœ ê¶¤ì (Completed trajectories)ì€ í›ˆë ¨ìœ¼ë¡œ ì´ë™í•˜ê³ , ë¯¸ì™„ì„±ëœ ê²ƒì€ ë²„í¼ë§ ë° ì¬ê°œë©ë‹ˆë‹¤. 2.5ë°° ë¡¤ì•„ì›ƒ ì†ë„ í–¥ìƒ(rollout speedup)ê³¼ 1.5ë°° ì—”ë“œíˆ¬ì—”ë“œ ì†ë„ í–¥ìƒ(end-to-end speedup)ì„ ì œê³µí•©ë‹ˆë‹¤. ASystem ì¸í”„ë¼(ASystem infrastructure): í•˜ì´ë¸Œë¦¬ë“œ ëŸ°íƒ€ì„(Hybrid Runtime, í†µí•© í›ˆë ¨-ì¶”ë¡ ), AMem(GPU ë©”ëª¨ë¦¬ ê´€ë¦¬), AState(ì´ˆ ë¯¸ë§Œ ê°€ì¤‘ì¹˜ ë™ê¸°í™”), ASandbox(100ms ì‹œì‘)ë¥¼ í¬í•¨í•©ë‹ˆë‹¤. SingleController + SPMD ì•„í‚¤í…ì²˜(architecture)ëŠ” ë°ì´í„° íë¦„ ë³‘ëª© í˜„ìƒ(data flow bottlenecks)ì„ ë°©ì§€í•©ë‹ˆë‹¤. í›ˆë ¨ íŒŒì´í”„ë¼ì¸(Training pipeline): ë‹¤ì¤‘ ë„ë©”ì¸ ë°ì´í„°(multi-domain data, ìˆ˜í•™ 46%, STEM 26%, ì½”ë“œ 20%)ì— ëŒ€í•œ Long-CoT SFT, ê²€ì¦ ê°€ëŠ¥í•œ ë³´ìƒì„ í†µí•œ ì¶”ë¡  RL(Reasoning RL with verifiable rewards), ì •ë ¬ ë° ì•ˆì „ì„ ìœ„í•œ ì¼ë°˜ RL(General RL for alignment and safety)ì„ í¬í•¨í•©ë‹ˆë‹¤. Paper | Tweet
8.  **ColorAgent** ColorAgentëŠ” ë‹¨ê³„ë³„ RL(step-wise RL)ê³¼ ìì²´ ì§„í™” í›ˆë ¨(self-evolving training)ì„ ë‹¤ì¤‘ ì—ì´ì „íŠ¸ í”„ë ˆì„ì›Œí¬(multi-agent framework)ì™€ ê²°í•©í•˜ì—¬ ê°œì¸í™”ëœ ì‚¬ìš©ì ì°¸ì—¬(personalized user engagement)ë¥¼ ì œê³µí•˜ëŠ” ëª¨ë°”ì¼ OS ì—ì´ì „íŠ¸(mobile OS agent)ì…ë‹ˆë‹¤. AndroidWorldì—ì„œ 77.2%, AndroidLabì—ì„œ 50.7%ì˜ ì„±ê³µë¥ (ì˜¤í”ˆ ëª¨ë¸ ì¤‘ SOTA)ì„ ë‹¬ì„±í•˜ë©°, ê°œì¸í™”ëœ ì˜ë„ ì •ë ¬(personalized intent alignment)ì„ ìœ„í•œ MobileIARì—ì„œ 58.66%, ì‹ ë¢°ì„±(trustworthiness)ì„ ìœ„í•œ VeriOS-Benchì—ì„œ 68.98%ë¥¼ ê¸°ë¡í–ˆìŠµë‹ˆë‹¤. Paper | Tweet
9.  **Prompt-MII** CMU ì—°êµ¬ì›ë“¤ì€ 3,000ê°œ ì´ìƒì˜ HuggingFace ë°ì´í„°ì…‹(datasets)ì—ì„œ ì§€ì‹œ ìœ ë„(instruction induction)ë¥¼ ë©”íƒ€ í•™ìŠµ(meta-learns)í•˜ëŠ” RL í”„ë ˆì„ì›Œí¬(RL framework)ì¸ Prompt-MIIë¥¼ ì œì•ˆí•˜ë©°, 90ê°œì˜ ë³´ì§€ ëª»í•œ ì‘ì—…(unseen tasks)ì—ì„œ 4-9 F1 ì ìˆ˜ ê°œì„ (F1 point improvements)ì„ ë‹¬ì„±í•˜ëŠ” ë™ì‹œì— ì¸ì»¨í…ìŠ¤íŠ¸ í•™ìŠµ(in-context learning)ë³´ë‹¤ 3-13ë°° ì ì€ í† í°ì„ í•„ìš”ë¡œ í•©ë‹ˆë‹¤. APE(2000 LLM í˜¸ì¶œ(LLM calls)) ë° GEPA(150 í˜¸ì¶œ)ì™€ ë‹¬ë¦¬, ë‹¨ì¼ ìˆœë°©í–¥ ì „ë‹¬(single forward pass)ë¡œ ê°„ê²°í•œ ì§€ì‹œ(compact instructions)ë¥¼ ìƒì„±í•˜ë©° í…ŒìŠ¤íŠ¸ ì‹œ í›ˆë ¨ ë¶ˆí•„ìš”(training-free at test time)í•©ë‹ˆë‹¤. Paper | Tweet
10. **ê¸°ì—… ì‹¬ì¸µ ì—°êµ¬(Enterprise Deep Research)** Salesforce AI ì—°êµ¬ì›ë“¤ì€ í•  ì¼ ê¸°ë°˜ ì‘ì—… ê´€ë¦¬(todo-driven task management) ë° ì¡°ì¢… ê°€ëŠ¥í•œ ì»¨í…ìŠ¤íŠ¸ ì—”ì§€ë‹ˆì–´ë§(steerable context engineering)ì„ í†µí•œ íœ´ë¨¼ ì¸ ë” ë£¨í”„(human-in-the-loop) ì¡°ì¢…ìœ¼ë¡œ ê¸°ì—… ì‹¬ì¸µ ì—°êµ¬(enterprise deep research)ë¥¼ ìœ„í•œ íˆ¬ëª…í•œ ë‹¤ì¤‘ ì—ì´ì „íŠ¸ í”„ë ˆì„ì›Œí¬(multi-agent framework)ì¸ EDRì„ ì œì‹œí•©ë‹ˆë‹¤. DeepResearch Benchì—ì„œ SOTA(49.86), DeepConsultì—ì„œ 71.57%ì˜ ìŠ¹ë¥ , ResearchQAì—ì„œ 68.5%ë¥¼ ë‹¬ì„±í•˜ë©° LangChainì˜ ì˜¤í”ˆ ì‹¬ì¸µ ì—°êµ¬(open deep research)ë³´ë‹¤ 4ë°° ì ì€ í† í°ì„ ì†Œë¹„í•©ë‹ˆë‹¤. Paper | Tweet