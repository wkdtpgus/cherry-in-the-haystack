인공지능(AI) 기술의 발전은 우리 삶을 풍요롭게 하지만, 그 이면의 에너지 소비는 중요한 논쟁거리입니다. 특히 대규모 언어 모델(LLM)의 확산과 함께 AI의 에너지 발자국에 대한 관심이 증대하고 있습니다. 저는 지난 몇 달 동안 챗GPT(ChatGPT)의 에너지 사용량에 대해 꽤 광범위하게 글을 써왔습니다 (예: [여기](https://www.linkedin.com/posts/martin-vehlow-phd-4394011_chatgpt-energy-consumption-activity-7128522776660144128-0w5f?utm_source=share&utm_medium=member_desktop), [여기](https://www.linkedin.com/posts/martin-vehlow-phd-4394011_chatgpt-energy-consumption-activity-7128522776660144128-0w5f?utm_source=share&utm_medium=member_desktop), [여기](https://www.linkedin.com/posts/martin-vehlow-phd-4394011_chatgpt-energy-consumption-activity-7128522776660144128-0w5f?utm_source=share&utm_medium=member_desktop), 그리고 [여기](https://www.linkedin.com/posts/martin-vehlow-phd-4394011_chatgpt-energy-consumption-activity-7128522776660144128-0w5f?utm_source=share&utm_medium=member_desktop)). 다양한 자료와 논문을 살펴보면서, 저는 마침내 간단한 텍스트 전용 챗GPT(ChatGPT) 요청이 약 0.2 와트시(Wh)를 소비한다고 추정했습니다. 이는 떠돌아다니는 다른 많은 추정치들과는 극명한 대조를 이루는 것이었는데, 그 추정치들은 종종 에너지 사용량이 최소 한 자릿수(order of magnitude) 이상 높다고 제시했습니다. 오픈AI(OpenAI)는 지금까지 이에 대해 침묵을 지켜왔습니다. 어제, [블로그 게시물](https://openai.com/blog/introducing-our-new-embedding-models-and-api-updates)에서 오픈AI(OpenAI)의 공동 창립자이자 CEO인 샘 알트만(Sam Altman)은 "평균적인 (챗GPT) 질의(query)는 약 0.34 와트시(watt-hours)를 사용한다"고 밝혔습니다. 빙고.

물론, 궁극적으로는 상세한 분석을 볼 수 있다면 좋을 것입니다. 이 시점에서, 물론 여러분은 오픈AI(OpenAI) / 샘 알트만(Sam Altman)을 신뢰하지 않으므로 이 데이터도 신뢰하지 않는다고 주장할 수 있습니다. 하지만 가장 중요한 인공지능(AI) 기업의 CEO의 말을 믿는 한, 이 문제는 이제 해결된 것입니다. 샘 알트만(Sam Altman)이 밝힌 "평균적인 질의(query)"는 단순히 텍스트 생성 외에도 복잡한 추론, 멀티모달(multimodal) 입력 처리, 길어진 컨텍스트 창(context window) 활용 등 다양한 연산을 포함할 수 있습니다. 텍스트 전용 요청은 상대적으로 적은 에너지를 소모하지만, 이미지나 비디오 생성 같은 고도화된 기능이 추가될수록 질의당 소비량은 기하급수적으로 증가합니다. 따라서 제가 추정했던 0.2 와트시(Wh)와 오픈AI의 0.34 와트시(Wh)의 차이는 이러한 '평균'의 범위와 모델 활용 방식에 따라 충분히 설명될 수 있습니다. 0.34 와트시(Wh)는 여전히 0.2 와트시(Wh)보다 70% 높지만, 중요한 것은 자릿수(order of magnitude)를 제대로 파악하는 것이었고, 이미지 및 비디오 생성에 얼마나 많은 에너지가 사용되는지, 그리고 샘이 평균적인 질의(query)에 대해 이야기하고 있다는 점을 고려할 때, 저는 텍스트 전용 챗GPT(ChatGPT) 요청에 대한 저의 0.2 와트시(Wh) 추정치를 업데이트할 어떤 이유도 찾지 못했습니다. 그러므로 몇 달밖에 되지 않았지만 인공지능(AI) 세계에서는 꽤 오래된 것이라고 할 수 있는 이 수치를 다시 게시하도록 허락해 주십시오 (😇). 또한 이것이 에너지 수요로 인한 다가오는 폭발적인 증가에 대해 깊이 생각하는 것을 면제해 주지 않는다는 점을 다시 한번 강조하고 싶습니다. 이 점에 있어서도 저는 제 주장을 고수하며, 우리가 현재 전반적인 에너지 수요를 과소평가하고 있을 수 있으며, 그것도 극적으로 과소평가하고 있다고 주장합니다.

요컨대: 인공지능(AI) 에이전트로 가득 찬 세상에서 폭발적인 에너지 요구량에 대한 우려는 여전히 충분히 방어할 수 있지만, 일반적인 챗봇 사용은 에너지 사용 측면에서 실제로 걱정할 것이 없습니다.

하지만 개별 질의의 에너지 소비는 미미할 수 있지만, 더 큰 그림에서 인공지능(AI)의 에너지 문제에 접근할 필요가 있습니다. 특히 GPT-4와 같은 대규모 모델 훈련에는 수십만 킬로와트시(kWh)에 달하는 전력이 소모되며, 이는 수많은 가정이 1년 동안 사용하는 전력량과 맞먹습니다. AI 모델을 구동하는 데 필요한 고성능 그래픽 처리 장치(GPU)와 데이터센터의 냉각 시스템 또한 막대한 에너지를 요구합니다. 이러한 인프라 구축 및 유지보수는 개별 질의의 에너지 소비를 훨씬 초과하는 환경적 영향을 미칩니다. 최근에는 이러한 문제를 해결하기 위해 모델 경량화(quantization), 희소성(sparsity) 기법, 그리고 전력 효율적인 AI 칩 개발 등 에너지 효율성을 높이기 위한 연구가 활발히 진행 중입니다. 인공지능(AI) 기술의 지속적인 발전을 위해서는 에너지 효율성을 최우선 과제로 삼아야 합니다. 단일 질의를 넘어, 모델 학습부터 배포, 하드웨어 인프라 전반에 걸친 에너지 최적화 노력이 필수적입니다. 투명한 에너지 소비 데이터 공개와 함께, 친환경적인 AI 기술 개발이 더욱 가속화되어야 할 것입니다.

**코다(CODA)**
이 뉴스레터는 인공지능(AI) 분야의 중요한 통찰력을 제공하며, 여러분의 지속적인 관심과 지원에 감사드립니다. 콘텐츠는 항상 무료로 제공되지만, 유료 구독은 EPFL 인공지능(AI) 센터의 혁신적인 연구와 교육 활동을 직접적으로 후원하는 데 사용됩니다. 저와 소셜 미디어에서 연결되려면, [링크드인(LinkedIn)](https://www.linkedin.com/in/martin-vehlow-phd-4394011/)이 가장 활발한 채널이며, [마스토돈(Mastodon)](https://mastodon.social/@vehlow), [블루스카이(Bluesky)](https://bsky.app/profile/vehlow.bsky.social), 그리고 [X](https://twitter.com/vehlow)에서도 저를 만나실 수 있습니다. 또한, 저는 EPFL 인공지능(AI) 센터의 팟캐스트 "인사이드 AI(Inside AI)"를 진행하며 ([애플 팟캐스트(Apple Podcasts)](https://podcasts.apple.com/us/podcast/inside-ai/id1725301826), [스포티파이(Spotify)](https://open.spotify.com/show/7wVwH73c9qj90q461L18gM), [유튜브(YouTube)](https://www.youtube.com/@InsideAI_EPFL)), 최신 AI 트렌드를 심층적으로 다루고 있습니다.