인공지능(AI) 기술의 발전은 우리 삶을 풍요롭게 하지만, 그 이면의 에너지 소비는 중요한 논쟁거리입니다. 특히 대규모 언어 모델(LLM)의 확산과 함께 AI의 에너지 발자국에 대한 관심이 증대하고 있습니다. 저는 지난 몇 달 동안 챗GPT(ChatGPT)의 에너지 사용량에 대해 꽤 광범위하게 글을 써왔습니다 (예: [여기](https://www.linkedin.com/posts/martin-vehlow-phd-4394011_chatgpt-energy-consumption-activity-7128522776660144128-0w5f?utm_source=share&utm_medium=member_desktop), [여기](https://www.linkedin.com/posts/martin-vehlow-phd-4394011_chatgpt-energy-consumption-activity-7128522776660144128-0w5f?utm_source=share&utm_medium=member_desktop), [여기](https://www.linkedin.com/posts/martin-vehlow-phd-4394011_chatgpt-energy-consumption-activity-7128522776660144128-0w5f?utm_source=share&utm_medium=member_desktop), 그리고 [여기](https://www.linkedin.com/posts/martin-vehlow-phd-4394011_chatgpt-energy-consumption-activity-7128522776660144128-0w5f?utm_source=share&utm_medium=member_desktop)). 다양한 자료와 논문을 살펴보면서, 저는 마침내 간단한 텍스트 전용 챗GPT(ChatGPT) 요청이 약 0.2 와트시(Wh)를 소비한다고 추정했습니다. 이는 떠돌아다니는 다른 많은 추정치들과는 극명한 대조를 이루는 것이었는데, 그 추정치들은 종종 에너지 사용량이 최소 한 자릿수(order of magnitude) 이상 높다고 제시했습니다. 오픈AI(OpenAI)는 지금까지 이에 대해 침묵을 지켜왔습니다. 어제, [블로그 게시물](https://openai.com/blog/introducing-our-new-embedding-models-and-api-updates)에서 오픈AI(OpenAI)의 공동 창립자이자 CEO인 샘 알트만(Sam Altman)은 "평균적인 (챗GPT) 질의(query)는 약 0.34 와트시(watt-hours)를 사용한다"고 밝혔습니다. 빙고.

샘 알트만(Sam Altman)이 밝힌 "평균적인 질의(query)"는 단순히 텍스트 생성 외에도 복잡한 추론, 멀티모달(multimodal) 입력 처리, 길어진 컨텍스트 창(context window) 활용 등 다양한 연산을 포함할 수 있습니다. 텍스트 전용 요청은 상대적으로 적은 에너지를 소모하지만, 이미지나 비디오 생성 같은 고도화된 기능이 추가될수록 질의당 소비량은 기하급수적으로 증가합니다. 따라서 제가 추정했던 0.2 와트시(Wh)와 오픈AI의 0.34 와트시(Wh)의 차이는 이러한 '평균'의 범위와 모델 활용 방식에 따라 충분히 설명될 수 있습니다.

요컨대: 인공지능(AI) 에이전트로 가득 찬 세상에서 폭발적인 에너지 요구량에 대한 우려는 여전히 충분히 방어할 수 있지만, 일반적인 챗봇 사용은 에너지 사용 측면에서 실제로 걱정할 것이 없습니다.

하지만 개별 질의의 에너지 소비는 미미할 수 있지만, 더 큰 그림에서 인공지능(AI)의 에너지 문제에 접근할 필요가 있습니다. 특히 GPT-4와 같은 대규모 모델 훈련에는 수십만 킬로와트시(kWh)에 달하는 전력이 소모되며, 이는 수많은 가정이 1년 동안 사용하는 전력량과 맞먹습니다. AI 모델을 구동하는 데 필요한 고성능 그래픽 처리 장치(GPU)와 데이터센터의 냉각 시스템 또한 막대한 에너지를 요구합니다. 이러한 인프라 구축 및 유지보수는 개별 질의의 에너지 소비를 훨씬 초과하는 환경적 영향을 미칩니다. 최근에는 이러한 문제를 해결하기 위해 모델 경량화(quantization), 희소성(sparsity) 기법, 그리고 전력 효율적인 AI 칩 개발 등 에너지 효율성을 높이기 위한 연구가 활발히 진행 중입니다. 인공지능(AI) 기술의 지속적인 발전을 위해서는 에너지 효율성을 최우선 과제로 삼아야 합니다. 단일 질의를 넘어, 모델 학습부터 배포, 하드웨어 인프라 전반에 걸친 에너지 최적화 노력이 필수적입니다. 투명한 에너지 소비 데이터 공개와 함께, 친환경적인 AI 기술 개발이 더욱 가속화되어야 할 것입니다.

**코다(CODA)**
이 뉴스레터는 인공지능(AI) 분야의 중요한 통찰력을 제공하며, 여러분의 지속적인 관심과 지원에 감사드립니다. 콘텐츠는 항상 무료로 제공되지만, 유료 구독은 EPFL 인공지능(AI) 센터의 혁신적인 연구와 교육 활동을 직접적으로 후원하는 데 사용됩니다. 저와 소셜 미디어에서 연결되려면, [링크드인(LinkedIn)](https://www.linkedin.com/in/martin-vehlow-phd-4394011/)이 가장 활발한 채널이며, [마스토돈(Mastodon)](https://mastodon.social/@vehlow), [블루스카이(Bluesky)](https://bsky.app/profile/vehlow.bsky.social), 그리고 [X](https://twitter.com/vehlow)에서도 저를 만나실 수 있습니다. 또한, 저는 EPFL 인공지능(AI) 센터의 팟캐스트 "인사이드 AI(Inside AI)"를 진행하며 ([애플 팟캐스트(Apple Podcasts)](https://podcasts.apple.com/us/podcast/inside-ai/id1725301826), [스포티파이(Spotify)](https://open.spotify.com/show/7wVwH73c9qj90q461L18gM), [유튜브(YouTube)](https://www.youtube.com/@InsideAI_EPFL)), 최신 AI 트렌드를 심층적으로 다루고 있습니다.