편집자 주: Roboflow의 Matvei 님을 Latent Space 객원 저자 팀에 환영합니다! 최근 GPT-5-Mini의 시각 점수가 GPT-5의 시각 점수와 동일하다는 점에 주목할 필요가 있습니다. 이는 매우 훌륭한 모델 라우터(model router)가 작동하고 있음을 보여주는 예상된 결과입니다. 지난 OpenAI의 발표에서 크게 주목받지 못했지만, GPT-5의 시각 및 시각적 추론(visual reasoning) 능력은 여전히 중요한 화두입니다. LLM(대규모 언어 모델)에 시각적 이해를 추가하는 것은 어려운 일로 입증되었으며, 대부분의 모델은 사진 속 동전 4개를 정확히 세거나 이미지 내 특정 항목의 위치를 파악하는 데 여전히 어려움을 겪습니다. LLM이 주변 세상을 실시간으로 이해할 수 있게 되는 것은 자율 로봇(autonomous robotics) 또는 컴퓨터 사용 혁명을 가능하게 하고 개인 초지능(personal superintelligence) 시대를 열기 위해 사람들이 찾고 있는 돌파구입니다.

**LLM과 시각적 이해의 최신 동향**
추론(reasoning) 및 시각적 능력의 조합은 OpenAI의 GPT 및 o 시리즈 모델, Google의 Gemini 모델, Anthropic의 Claude 모델, Meta의 Llama 모델 등 여러 주요 모델에서 지속적으로 발전하고 있습니다. 각 모델은 작업 유형에 따라 고유한 강점과 약점을 가집니다. 텍스트 읽기, 표지판 인식, 영수증 처리, CAPTCHA 해결, 색상 이해와 같은 작업은 일반적으로 대부분의 모델에서 안정적으로 해결되고 있습니다. 그러나 카운팅(counting), 공간 이해(spatial understanding), 객체 감지(object detection), 그리고 복잡한 문서 이해와 같은 고난이도 작업에서는 모델 간 성능 편차가 여전히 크게 나타나며, 특히 대규모 인터넷 데이터로 사전 학습(pretrain)하는 과정에서 이러한 유형의 데이터가 부족할 때 더욱 그러합니다.

**RF100VL, 새로운 평가 기준을 제시하다**
다양한 작업 유형 전반에 걸친 성능의 편차(variance)는 모델 간의 객관적인 비교를 어렵게 만듭니다. 이러한 현실을 반영하여 Vision Checkup과 같은 리더보드(leaderboard)가 중요한 역할을 하고 있습니다. Vision Checkup은 당사의 오픈 소스(open source) 시각 모델 평가 리더보드이며, 어려운 시각 작업에서의 최전선 성능에 대한 심층적인 통찰력을 제공합니다. 보시다시피 OpenAI는 시각 능력 평가에서 여전히 지배적인 위치를 유지하고 있으며, GPT-5의 출시는 또 다른 강력한 모델을 상위 5위권에 추가했습니다.

미니 버전이 메인 버전과 동일하게 작동한다는 점에 주목하세요! 훌륭한 라우터입니다. 리더보드 결과는 명확한 메시지를 전달합니다. 상위권 모델들은 대부분 강력한 추론 능력을 갖춘 모델들입니다. 이러한 추론 능력이 일반적인 시각 작업에서 OpenAI의 강력한 성능을 이끌고 있습니다. Vision Checkup 리더보드의 상위 모델들은 추론 능력을 갖춘 구성으로 되어 있습니다. 우리는 이 모델들의 뛰어난 결과가 사전 학습(pretraining) 및 테스트 시점의 정교한 추론 능력에 더 많이 기인한다고 추정합니다. 이는 다중 모달(multi-modal) 대규모 언어 모델(LLM)의 중요한 발전의 연속을 의미하며, 텍스트와 시각 양쪽 모달리티(modality)에 대해 깊이 있게 추론할 수 있는 능력을 말합니다. 하지만, 점수는 업데이트마다 크게 달라지기도 하는데, 이는 여러 가지 이유로 설명될 수 있습니다. 가장 큰 이유는 OpenAI 모델의 추론 모드(reasoning mode)의 비결정성(nondeterminism) 때문입니다. 추론 모델에 동일한 질문을 두 번 프롬프트(prompt)하면 올바른 답변과 잘못된 답변이 모두 나올 수 있는 경우가 있습니다. 실제 사용 환경에서 이미지에 대한 추론은 여전히 유용하기에는 너무 많은 시간이 걸리며, 답변의 가변성(variability) 때문에 신뢰하기 어려운 측면이 있습니다. 대부분의 개발자에게 이미지를 이해하는 데 10초 이상이 걸리는 것은 실시간 사용 사례를 가능하게 하지 못할 것입니다. 속도와 능력 사이에는 명확한 절충점(trade-off)이 존재합니다. 따라서 작업의 특성에 따라, 더 좁은 범위의 지식을 가졌지만 더 빠른 모델이 때로는 최선의 결정일 수 있습니다. 최근에는 이러한 비결정성을 줄이고 추론 속도를 높이기 위한 다양한 연구와 최적화 기법이 활발히 탐구되고 있습니다.

**시각 바이브 체크(Vision Vibe Checks)를 넘어선 실제 세계 이해**
우리는 자율 로봇(autonomous robotics)이 주변 세상과 실시간으로 매끄럽게 상호작용할 수 있는 세상과는 아직 거리가 멉니다. 카운팅(counting), 공간 이해(spatial understanding), 객체 위치 파악(object localization)과 같은 비교적 간단해 보이는 작업들은 로봇이 통제된 환경 밖에서 일반적인 작업을 효과적으로 수행하는 데 핵심적인 요소입니다. 단순히 시각적 인상을 확인하는 수준의 가벼운 바이브 체크(vibe checks) 리더보드를 넘어설 수 있도록, 우리는 광범위한 도메인(domain)에 걸쳐 LLM을 테스트하고 그 진행 상황을 더욱 면밀히 추적해야 합니다. 우리는 지난 CVPR 컨퍼런스에서 더 어려운 시각적 이해 및 그라운딩(grounding) 벤치마크(benchmark) 세트를 제공하기 위해 새로운 벤치마크인 RF100-VL을 발표했으며, 그 중요성이 더욱 부각되고 있습니다. 이 벤치마크는 근본적인 질문을 던집니다: "당신의 LLM은 실제 세상을 얼마나 잘 이해하는가?" RF100-VL은 Roboflow Universe 커뮤니티의 100개 오픈 소스 데이터셋(dataset)으로 구성되어 있으며, 객체 감지 바운딩 박스(object detection bounding boxes)와 시각적 예시 및 풍부한 텍스트 설명을 포함하는 다중 모달(multimodal) 소수샷(few-shot) 지침을 새로운 이미지 도메인(domain) 전반에 걸쳐 제공합니다. 상위 LLM들은 실제 환경에서 새로운 객체를 식별하는 데 10 mAP50:95 미만의 점수를 기록했습니다. 모든 LLM 중 현재 SOTA(State-Of-The-Art)는 Gemini 2.5 Pro로, 제로샷(zero-shot) mAP50:95에서 13.3을 달성했습니다. 객체 감지(object detection) 작업에 있어서 OpenAI 모델과 Gemini 또는 Qwen과 같은 모델의 주요 차이점은 OpenAI 모델이 사전 학습(pretraining)에 객체 감지 데이터를 포함하지 않는다는 점이라고 추정합니다. RF100-VL에서 GPT-5를 실행한 결과, mAP50:95는 1.5를 기록했습니다. 이는 Gemini 2.5 Pro의 현재 SOTA인 13.3보다 현저히 낮은 수치입니다. 우리는 이러한 단점이 GPT-5의 객체 감지 사전 학습(pre-training) 부족에 기인한다고 크게 생각합니다. 이러한 격차를 해소하기 위해 합성 데이터(synthetic data)를 활용한 사전 학습 및 특정 도메인에 대한 미세 조정(fine-tuning) 기법들이 활발히 연구되고 있습니다.

**예시: 위치 파악(Localization)의 한계**
이것은 배구 데이터셋(dataset)의 예시로, 낮은 점수의 주요 원인을 명확히 보여줍니다. 모델이 이미지 내 객체를 잘 이해하고 있음을 관찰할 수 있습니다. 공, 두 명의 블로커(blocker), 그리고 몇 명의 수비수가 존재한다는 것을 정확히 이해하고 있습니다. 하지만 모델은 객체들의 정확한 위치를 파악하지 못하며, 모든 박스(box)가 객체의 실제 위치 및 크기와 일치하지 않습니다. 모델은 이미지를 문맥적으로 이해하는 데는 능숙하지만, 이미지 내 특정 객체를 실제 공간에 그라운딩(grounding)하는 데는 능숙하지 않은 것으로 보이며, 이는 다시 사전 학습(pre-training)에서 객체 감지(object detection) 작업이 부족했기 때문이라고 추정합니다. 아래 양 데이터셋(dataset)에서도 비슷한 상황을 확인할 수 있습니다. 시각적 그라운딩은 멀티모달 LLM이 실제 세계와 상호작용하기 위한 핵심 역량이며, 이 분야의 발전이 시급합니다.

**예시: UI 요소(UI Elements)에서의 도전 과제**
도구 사용(tool use) 및 시각 기반 에이전트 워크플로우(vision-powered agentic workflows)를 위한 LLM의 지속적인 발전에 따라, GPT-5의 해당 성능을 재평가해 보겠습니다. UI 요소 데이터셋(dataset)에서도 품질 향상은 있었지만, 기대만큼의 큰 진전은 아니었습니다. GPT-5는 이전 OpenAI 모델인 o3보다 약간 더 나은 성능을 보였습니다. 그리고 두 경우 모두 상세한 지침과 같은 추가 정보를 제공하는 것이 모델의 성능 향상에 여전히 도움이 됩니다. 흥미롭게도, 추론 노력(reasoning effort)을 높게 설정해도 RF100-VL의 점수는 크게 향상되지 않았습니다. 따라서 RF100-VL의 경우 추론의 이점이 그렇게 명확하지 않으며, 이는 사전 학습(pretraining)에서 객체 감지(object detection) 작업이 부족하여 객체 감지 능력이 여전히 부족하기 때문이라고 추정합니다. 이는 에이전트 AI가 사용자 인터페이스(UI)를 정확히 이해하고 조작하는 데 있어 여전히 중요한 장애물로 작용합니다.

**GPT-5, 단순 시각 작업에서 개선과 함께 나아가야 할 길**
GPT-5의 향상된 추론(reasoning) 능력은 Vision Checkup 리더보드에서 높은 순위를 차지하게 하며, 다단계 사고(multi-step thinking)가 모델이 픽셀(pixel)에서 더 많은 정보를 추출할 수 있도록 한다는 것을 분명히 증명합니다. 이는 사람들이 ChatGPT에 의존하여 해결하는 일상적인 사용 사례에 있어 훌륭한 결과입니다. 그러나 RF100-VL은 이해(comprehension)가 위치 파악(localization)과 동일하지 않다는 점을 강조하는 데 도움이 됩니다. 객체 감지 사전 지식(object-detection priors) 없이는 감지(detection)가 여전히 목표를 벗어납니다. 그럼에도 불구하고, GPT-5의 시각 추론(vision-reasoning) 능력 향상은 모델이 단순히 더 잘 볼 뿐만 아니라 보고 있는 것에 대해 더 깊이 생각하는 방향으로 명확한 발전 방향을 제시합니다. 멀티모달 LLM의 미래는 정교한 추론 능력과 정확한 지각 능력을 모두 갖추는 데 달려 있으며, 이는 다음 번 기술적 도약의 최전선이 될 것입니다. 비전 AI 엔지니어(Vision AI Engineers) 여러분은 Roboflow 블로그에서 더 심층적인 게시물과 최신 연구 동향을 계속해서 확인하실 수 있습니다.