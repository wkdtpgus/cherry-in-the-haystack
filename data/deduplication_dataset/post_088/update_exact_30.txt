편집자 주: Roboflow의 Matvei 님을 Latent Space 객원 저자 팀에 환영합니다! GPT-5-Mini의 시각 점수가 GPT-5의 시각 점수와 동일하다는 점에 주목하십시오. 이는 매우 훌륭한 모델 라우터(model router)가 작동하고 있음을 보여주는 예상된 결과입니다. 오늘 OpenAI의 발표에서 크게 주목받지 못한 부분은 GPT-5의 시각 및 시각적 추론(visual reasoning) 능력입니다. LLM(대규모 언어 모델)에 시각적 이해를 추가하는 것은 어려운 일로 입증되었으며, 대부분의 모델은 사진 속 동전 4개를 정확히 세거나 이미지 내 특정 항목의 위치를 파악하지 못했습니다. LLM이 주변 세상을 실시간으로 이해할 수 있게 되는 것은 자율 로봇(autonomous robotics) 또는 컴퓨터 사용 혁명을 가능하게 하고 개인 초지능(personal superintelligence) 시대를 열기 위해 사람들이 찾고 있는 돌파구입니다.

이러한 돌파구는 단순히 기계가 "보는" 것을 넘어, 시각 정보를 "이해"하고 "추론"하는 능력을 요구합니다. GPT-5와 같은 최신 모델들은 텍스트 기반 추론에서 보여준 강력한 성능을 시각 영역으로 확장하려는 시도를 하고 있으며, 이는 인간과 유사한 방식으로 세상을 인지하고 상호작용하는 인공지능의 미래를 위한 필수적인 단계입니다. 시각적 이해는 단순한 이미지 분류를 넘어, 복잡한 장면 분석, 객체 간의 관계 추론, 그리고 동적인 환경에서의 의사 결정에 이르는 광범위한 능력을 포함합니다.

**LLM과 시각적 이해의 현재 상태**
추론(reasoning) 및 시각적 능력의 조합은 OpenAI GPT 및 o 시리즈 모델, Google의 Gemini 모델, Anthropic의 Claude 모델, Meta의 Llama 모델과 같은 여러 모델에서 나타났습니다. 모델은 작업 유형에 따라 특정 강점과 약점을 가집니다. 텍스트 읽기, 표지판, 영수증, CAPTCHA, 색상 이해와 같은 작업은 일반적으로 모든 모델에서 해결됩니다. 그러나 카운팅(counting), 공간 이해(spatial understanding), 객체 감지(object detection), 문서 이해와 같은 더 어려운 작업은 성능 가변성(performance variability)이 높으며, 특히 대부분의 인터넷 규모 사전 학습(pretrain) 데이터에서 부족한 것으로 알려진 경우 더욱 그렇습니다.

이러한 모델들은 주로 텍스트와 이미지 데이터를 함께 학습하는 멀티모달(multimodal) 아키텍처를 채택하고 있습니다. 예를 들어, Vision Transformer(ViT)와 같은 시각 백본(backbone)을 통해 이미지 정보를 추출하고, 이를 언어 모델의 임베딩 공간과 정렬하여 시각적 질문에 대한 텍스트 응답을 생성하는 방식입니다. 하지만 이 과정에서 시각적 정보의 "깊이"와 "정확성"을 유지하는 것이 핵심 과제입니다. 특히, 미묘한 시각적 단서나 복잡한 공간적 관계를 이해하는 데 있어서는 여전히 많은 한계가 존재합니다. 이는 모델이 단순히 픽셀 데이터를 처리하는 것을 넘어, 실제 세계의 물리적 법칙과 상식적인 지식을 내재화해야 함을 의미합니다.

RF100VL에서
리더보드는 명확하게 보여줍니다. 상위 모델은 모두 추론 모델입니다. 추론이 일반적인 시각 작업에서 OpenAI의 지배력을 이끌고 있습니다. Vision Checkup 리더보드의 상위 모델은 추론 능력을 갖춘 모델의 구성입니다. 우리는 이 모델들의 좋은 결과가 사전 학습(pretraining) 및 테스트 시점의 추론 능력에 더 많이 기인한다고 추정합니다. 이는 다중 모달(multi-modal) 대규모 언어 모델(LLM)의 중요한 발전의 연속을 의미합니다. 즉, 텍스트와 시각 양쪽 모달리티(modality)에 대해 추론할 수 있는 능력입니다. 그렇기는 하지만, 점수는 업데이트마다 크게 다르며, 이는 여러 가지 이유로 설명할 수 있습니다. 가장 큰 이유는 OpenAI 모델의 추론 모드(reasoning mode)의 비결정성(nondeterminism)입니다. 추론 모델에 동일한 질문을 두 번 프롬프트(prompt)하면 올바른 답변과 잘못된 답변이 모두 나올 수 있습니다. 실제 사용에 있어서, 이미지에 대한 추론은 현재 유용하기에는 너무 많은 시간이 걸리며, 답변의 가변성(variability) 때문에 신뢰하기 어렵습니다. 대부분의 개발자에게 이미지를 이해하는 데 10초 이상이 걸리는 것은 실시간 사용 사례를 가능하게 하지 못할 것입니다. 속도와 능력 사이에는 절충점(trade-off)이 있습니다. 작업에 따라, 더 좁은 범위의 지식을 가진 더 빠른 모델이 최선의 결정일 수 있습니다.

이러한 비결정성 문제는 특히 실시간 상호작용이 요구되는 자율 시스템이나 에이전트(agent)에서는 치명적일 수 있습니다. 예를 들어, 로봇이 특정 객체를 정확히 인식하고 조작해야 하는 상황에서 매번 다른 결과가 나온다면 신뢰성 있는 동작을 기대하기 어렵습니다. 이를 해결하기 위해 모델 출력의 안정성을 높이는 디코딩 전략(decoding strategy)이나, 여러 번 추론하여 다수결로 최종 결정을 내리는 앙상블(ensemble) 기법 등이 연구되고 있습니다. 또한, 특정 도메인에 특화된 소규모 모델을 결합하여 속도와 정확도를 동시에 잡으려는 하이브리드(hybrid) 접근 방식도 주목받고 있습니다. 이는 범용적인 추론 능력과 특정 작업에 대한 정밀한 수행 능력 간의 균형을 찾는 중요한 과제입니다.

**시각 바이브 체크(Vision Vibe Checks)를 넘어서**
우리는 자율 로봇(autonomous robotics)이 주변 세상과 실시간으로 상호작용할 수 있는 세상과는 아직 거리가 뭅니다. 카운팅(counting), 공간 이해(spatial understanding), 객체 위치 파악(object localization)과 같은 간단한 작업은 로봇이 통제된 환경 밖에서 일반적인 작업을 수행하는 데 핵심입니다. 가벼운 바이브 체크(vibe checks)가 있는 리더보드를 넘어설 수 있도록, 우리는 광범위한 도메인(domain)에 걸쳐 LLM을 테스트하고 그 진행 상황을 추적해야 합니다. 우리는 올해 CVPR 컨퍼런스에서 더 어려운 시각적 이해 및 그라운딩(grounding) 벤치마크(benchmark) 세트를 제공하기 위해 새로운 벤치마크인 RF100-VL을 발표했습니다. 이 벤치마크는 "당신의 LLM은 실제 세상을 얼마나 잘 이해하는가?"를 묻습니다. RF100-VL은 Roboflow Universe 커뮤니티의 100개 오픈 소스 데이터셋(dataset)으로 구성되어 있으며, 객체 감지 바운딩 박스(object detection bounding boxes)와 시각적 예시 및 풍부한 텍스트 설명을 포함하는 다중 모달(multimodal) 소수샷(few-shot) 지침을 새로운 이미지 도메인(domain) 전반에 걸쳐 제공합니다. 상위 LLM들은 실제 환경에서 새로운 객체를 식별하는 데 10 mAP50:95 미만의 점수를 기록했습니다. 모든 LLM 중 현재 SOTA(State-Of-The-Art)는 Gemini 2.5 Pro로, 제로샷(zero-shot) mAP50:95에서 13.3을 달성했습니다. 객체 감지(object detection) 작업에 있어서 OpenAI 모델과 Gemini 또는 Qwen과 같은 모델의 주요 차이점은 OpenAI 모델이 사전 학습(pretraining)에 객체 감지 데이터를 포함하지 않는다는 점이라고 추정합니다. RF100-VL에서 GPT-5를 실행한 결과, mAP50:95는 1.5를 기록했습니다. 이는 Gemini 2.5 Pro의 현재 SOTA인 13.3보다 현저히 낮은 수치입니다. 우리는 이러한 단점이 GPT-5의 객체 감지 사전 학습(pre-training) 부족에 기인한다고 크게 생각합니다. 점수가 왜 그렇게 낮은지에 대한 더 많은 직관을 얻기 위해 아래에서 몇 가지 결과를 살펴보겠습니다.

RF100-VL 벤치마크는 실제 세계의 다양하고 복잡한 시나리오를 반영하기 위해 세심하게 설계되었습니다. 단순히 일반적인 객체를 인식하는 것을 넘어, 특정 산업 분야의 특수 객체, 미세한 결함, 또는 복잡한 배경 속의 작은 대상까지 식별하도록 요구합니다. 예를 들어, 의료 영상에서 특정 세포 구조를 찾거나, 제조 공정에서 미세한 불량을 감지하는 것과 같은 고도의 정밀성이 필요한 작업들이 포함됩니다. 소수샷(few-shot) 지침은 모델이 제한된 예시만으로 새로운 도메인에 빠르게 적응할 수 있는 능력을 평가하며, 이는 실제 환경에서 새로운 작업에 직면했을 때 LLM의 유연성을 가늠하는 중요한 척도가 됩니다. GPT-5의 낮은 점수는 이러한 특정 작업에 대한 사전 학습 부족이 얼마나 큰 영향을 미치는지를 명확히 보여줍니다. 이는 범용적인 시각 추론 능력과 특정 시각 감지 능력 간의 간극을 드러내며, 향후 멀티모달 LLM 개발에서 해결해야 할 핵심 과제임을 시사합니다.

**예시: 위치 파악(Localization) 및 세밀한 이해의 한계**
RF100-VL의 또 다른 흥미로운 예시는 "수술 도구 카운팅(surgical tool counting)" 데이터셋입니다. 이 데이터셋은 수술실 이미지에서 특정 수술 도구의 개수를 정확히 세고, 그 위치를 바운딩 박스로 표시하도록 요구합니다. GPT-5는 이미지 내에 수술 도구가 존재한다는 것을 인지하고, 어떤 종류의 도구인지 대략적으로 파악할 수 있지만, 정확한 개수를 세거나 각 도구의 정확한 위치를 파악하는 데는 어려움을 겪습니다. 예를 들어, 한 이미지에 5개의 메스(scalpel)가 있어도 모델은 "여러 개의 메스가 보인다"고 추론할 뿐, 정확히 "5개"라고 답변하지 못하거나, 바운딩 박스가 도구의 일부만 감싸거나 아예 엉뚱한 곳에 그려지는 경우가 많습니다.

이는 모델이 이미지의 전반적인 의미론적 이해(semantic understanding)는 갖추고 있지만, 픽셀 수준의 정밀한 객체 그라운딩(grounding)과 수량화(quantification) 능력은 부족하다는 것을 보여줍니다. 이러한 한계는 자율 로봇이 특정 부품을 조립하거나, 재고 관리 시스템이 창고의 물품을 정확히 파악해야 하는 등 실생활의 수많은 응용 분야에서 심각한 문제가 됩니다. 객체 감지(object detection) 및 세그멘테이션(segmentation)과 같은 컴퓨터 비전의 핵심 기술들이 LLM의 시각 모듈에 더욱 깊이 통합되어야 할 필요성을 강조합니다.

**예시: 복잡한 장면 추론(Complex Scene Reasoning)**
UI 요소 외에, RF100-VL은 "다중 객체 관계 추론(multi-object relational reasoning)"과 같은 더 복잡한 시나리오도 포함합니다. 예를 들어, "테이블 위에 놓인 빨간색 컵의 왼쪽에 있는 파란색 책을 찾아라"와 같은 질의입니다. GPT-5는 이미지 내에서 빨간색 컵과 파란색 책을 개별적으로 식별할 수는 있지만, "왼쪽"이라는 공간적 관계를 정확히 파악하여 해당 객체를 지목하는 데는 여전히 취약합니다. 이는 모델이 단순히 객체를 인식하는 것을 넘어, 객체들 간의 상대적인 위치, 방향, 그리고 더 나아가 상호작용까지 추론해야 하는 작업입니다.

이러한 종류의 추론은 인간이 일상적으로 수행하는 시각적 인지의 핵심 부분입니다. LLM이 이러한 능력을 갖추게 된다면, 복잡한 사용자 인터페이스를 이해하여 자동화된 작업을 수행하거나, 증강 현실(augmented reality) 환경에서 사용자의 의도를 정확히 파악하는 데 크게 기여할 수 있습니다. 현재로서는 특정 시각 언어 모델(Vision-Language Model, VLM)들이 이러한 관계 추론에 특화된 아키텍처를 도입하여 성능을 개선하고 있지만, 여전히 일반화된 강력한 추론 능력은 개발 중입니다. GPT-5와 같은 대규모 모델이 이러한 영역에서 더 발전하기 위해서는 시각적 관계에 대한 더욱 풍부하고 구조화된 사전 학습 데이터셋이 필요할 것으로 보입니다.

**GPT-5의 시각 능력 발전 방향과 미래 전망**
GPT-5의 향상된 추론(reasoning)은 Vision Checkup 리더보드에서 높은 순위를 차지하게 하며, 다단계 사고(multi-step thinking)가 모델이 픽셀(pixel)에서 더 많은 정보를 추출할 수 있도록 한다는 것을 증명합니다. 사람들이 ChatGPT에 의존하여 해결하는 일상적인 사용 사례에 있어 훌륭한 결과입니다. RF100-VL은 이해(comprehension)가 위치 파악(localization)과 같지 않다는 점을 강조하는 데 도움이 됩니다. 객체 감지 사전 지식(object-detection priors) 없이는 감지(detection)가 여전히 목표를 벗어납니다. 그럼에도 불구하고, GPT-5의 시각 추론(vision-reasoning) 능력 향상은 더 잘 볼 뿐만 아니라 보고 있는 것에 대해 더 깊이 생각하는 모델이라는 명확한 발전 방향을 제시합니다. 비전 AI 엔지니어(Vision AI Engineers)는 그들의 블로그에서 더 심층적인 게시물을 확인할 수 있습니다.

GPT-5는 시각 정보를 해석하는 데 있어 전반적인 추론 능력의 발전 가능성을 보여주었지만, 구체적인 객체 감지 및 위치 파악과 같은 세부 작업에서는 여전히 개선의 여지가 많습니다. 이는 마치 사람이 어떤 그림을 보고 "이 그림은 전쟁의 참혹함을 보여주는구나"라고 이해하지만, 그림 속 병사들의 정확한 숫자나 각 병사의 위치를 핀포인트(pinpoint)하지 못하는 것과 유사합니다. 미래의 멀티모달 LLM은 이러한 추상적인 이해와 구체적인 지각 능력 사이의 간극을 메우는 방향으로 발전할 것입니다. 이를 위해, 객체 감지, 세그멘테이션, 3D 이해와 같은 전통적인 컴퓨터 비전 기술을 LLM 아키텍처에 더욱 긴밀하게 통합하거나, 이러한 작업을 위한 특수 목적의 모듈을 LLM에 연결하는 하이브리드 접근 방식이 더욱 중요해질 것입니다. 궁극적으로, LLM이 단순히 텍스트와 이미지를 '연결'하는 것을 넘어, 시각 세계를 인간처럼 '경험'하고 '이해'하며 '상호작용'하는 시대를 열기 위한 끊임없는 연구와 발전이 기대됩니다.