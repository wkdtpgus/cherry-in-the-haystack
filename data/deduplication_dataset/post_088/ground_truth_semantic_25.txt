편집자 주: Roboflow의 Matvei 님을 Latent Space 객원 저자 팀에 다시 한번 환영합니다! GPT-5-Mini의 시각적 처리 역량이 GPT-5의 그것과 동일하다는 점은 주목할 만합니다. 이는 탁월한 모델 라우팅 체계가 내재되어 있음을 시사하는, 예상된 결과입니다. 오늘 OpenAI의 발표에서 상대적으로 덜 부각된 측면은 GPT-5가 보유한 시각 인지 및 시각적 추론(visual reasoning) 능력입니다. 대규모 언어 모델(LLM)에 시각적 이해를 부여하는 것은 그간 난해한 과제로 남아 있었으며, 대다수 모델은 사진 속 동전 4개를 정확히 세거나 이미지 내 특정 요소의 위치를 파악하는 데 어려움을 겪었습니다. LLM이 주변 환경을 실시간으로 해석하고 반응할 수 있게 되는 것은 자율 로봇 공학(autonomous robotics)이나 컴퓨터 활용 방식에 혁명적인 변화를 가져오고, 더 나아가 개인화된 초지능(personal superintelligence) 시대를 개척하기 위해 많은 이들이 염원하는 중대한 돌파구로 여겨집니다. 이러한 다중 모달(multi-modal) 능력은 단순히 정보를 처리하는 것을 넘어, 물리적 세계와 디지털 세계 사이의 간극을 메우는 '체화된 인공지능(embodied AI)'으로의 발전을 예고합니다.

**LLM의 시각적 이해 능력 현황**
추론 기능과 시각 처리 역량을 결합한 인공지능 모델은 OpenAI의 GPT 및 o 시리즈, Google의 Gemini, Anthropic의 Claude, Meta의 Llama 등 여러 주요 모델에서 구현되었습니다. 각 모델은 수행하는 작업의 종류에 따라 고유한 강점과 약점을 드러냅니다. 텍스트 판독, 표지판 식별, 영수증 분석, CAPTCHA 인식, 색상 이해와 같은 비교적 단순한 작업들은 대부분의 모델에서 무난히 처리됩니다. 하지만 개수 세기(counting), 공간 지각(spatial understanding), 객체 탐지(object detection), 문서 구조 이해와 같은 고난도 작업에서는 성능 편차가 크게 나타나며, 특히 이러한 유형의 데이터가 인터넷 규모의 사전 학습(pretraining) 자료에서 부족한 경우 더욱 두드러집니다. 이러한 성능 격차는 주로 모델이 시각 정보를 토큰화하고 인코딩하는 방식의 한계, 그리고 이미지 해상도 처리 능력과 관련이 있습니다. 즉, 픽셀 데이터를 언어 모델의 추론 체계에 효과적으로 통합하는 과정 자체가 상당한 기술적 도전을 수반합니다.

RF100VL에서의 평가
다양한 작업 유형에 걸친 성능의 불균일성은 모델 간의 일반적인 비교를 어렵게 만듭니다. 바로 이러한 배경에서 Vision Checkup과 같은 평가 지표가 최근 도입되었습니다. Vision Checkup은 당사가 개발한 오픈 소스(open source) 시각 모델 평가 리더보드로서, 복잡한 작업에서 모델의 최전선 성능에 대한 심층적인 분석을 제공합니다. 현재까지의 결과에서 OpenAI가 시각 능력 분야에서 압도적인 우위를 점하고 있음이 확인되며, GPT-5의 등장은 상위 5개 모델 목록에 또 하나의 강력한 경쟁자를 추가했습니다.

미니 버전이 주력 모델과 대등한 성능을 보인다는 점은 인상적입니다! 이는 매우 정교한 라우팅 시스템의 존재를 시사합니다.
리더보드는 명확한 패턴을 보여줍니다. 최상위권 모델들은 공통적으로 추론 능력을 갖춘 모델들입니다. OpenAI의 시각 능력 지배력은 이러한 추론 역량에 기반하고 있습니다. Vision Checkup 리더보드의 상단에 위치한 모델들은 모두 추론 능력이 뛰어난 모델들의 집합체입니다. 우리는 이 모델들의 우수한 결과가 사전 학습(pretraining) 단계와 테스트 시점에서의 심층적인 추론 능력에 더 많이 기인한다고 추정합니다. 이는 텍스트와 시각이라는 두 가지 양상(modality) 모두에 걸쳐 추론할 수 있는 능력을 의미하며, 다중 모달(multi-modal) 대규모 언어 모델(LLM) 발전의 중요한 연속선상에 있습니다. 그럼에도 불구하고, 평가 점수는 업데이트될 때마다 상당한 변동성을 보이는데, 이는 여러 요인으로 설명될 수 있습니다. 가장 큰 원인 중 하나는 OpenAI 모델의 추론 모드(reasoning mode)에서 나타나는 비결정성(nondeterminism)입니다. 동일한 질문을 추론 모델에 두 번 입력했을 때, 정확한 답변과 잘못된 답변이 모두 나올 수 있습니다. 실제 활용 측면에서, 이미지에 대한 추론은 현재 유용하게 사용되기에는 너무 많은 시간이 소요되며, 답변의 가변성(variability) 때문에 신뢰하기 어렵습니다. 대부분의 개발자에게 이미지를 이해하는 데 10초 이상이 걸리는 것은 실시간 사용 시나리오를 불가능하게 만들 것입니다. 따라서 속도와 능력 사이에는 불가피한 절충점(trade-off)이 존재합니다. 특정 작업의 성격에 따라, 더 좁은 범위의 지식을 지녔지만 더 빠르게 동작하는 모델을 선택하는 것이 최적의 결정일 수 있습니다. 이러한 비결정성은 특히 보안이나 의료 분야와 같이 높은 신뢰성이 요구되는 애플리케이션에서 심각한 윤리적, 실용적 문제를 야기할 수 있습니다.

**단순한 '바이브 체크'를 넘어서는 시각 평가**
우리는 자율 로봇 공학(autonomous robotics)이 주변 세계와 실시간으로 상호작용하는 세상과는 아직 상당한 거리가 있습니다. 개수 세기(counting), 공간 지각(spatial understanding), 객체 위치 파악(object localization)과 같은 기초적인 작업들은 로봇이 통제되지 않는 환경에서 일반적인 임무를 수행하는 데 필수적입니다. 따라서 가벼운 '바이브 체크' 수준의 리더보드를 넘어설 수 있도록, 우리는 광범위한 도메인(domain)에 걸쳐 LLM을 시험하고 그 진척 상황을 면밀히 추적해야 합니다. 이처럼 더 심도 깊은 평가를 위해, 우리는 올해 CVPR 컨퍼런스에서 더욱 까다로운 시각적 이해 및 그라운딩(grounding) 벤치마크(benchmark) 세트를 제공하는 새로운 벤치마크인 RF100-VL을 발표했습니다. 이 벤치마크는 "당신의 LLM은 실제 세상을 얼마나 정확히 이해하고 있는가?"라는 핵심 질문을 던집니다. RF100-VL은 Roboflow Universe 커뮤니티에서 제공하는 100개의 오픈 소스 데이터셋(dataset)으로 구성되어 있으며, 객체 탐지 바운딩 박스(object detection bounding boxes)와 시각적 예시 및 풍부한 텍스트 설명을 포함하는 다중 모달(multimodal) 소수샷(few-shot) 지침을 새로운 이미지 도메인(domain) 전반에 걸쳐 제공합니다. 현재 최상위 LLM들은 실제 환경에서 이전에 보지 못한 객체들을 식별하는 데 10 mAP50:95 미만의 점수를 기록했습니다. 모든 LLM 중 현재 SOTA(State-Of-The-Art)는 Gemini 2.5 Pro로, 제로샷(zero-shot) mAP50:95에서 13.3을 달성했습니다. 객체 탐지(object detection) 작업에 있어서 OpenAI 모델과 Gemini 또는 Qwen과 같은 모델의 주요 차이점은 OpenAI 모델이 사전 학습(pretraining) 과정에서 객체 탐지 데이터를 명시적으로 포함하지 않는다는 점이라고 추정됩니다. RF100-VL에서 GPT-5를 실행한 결과, mAP50:95는 1.5에 불과했습니다. 이는 Gemini 2.5 Pro의 현재 SOTA인 13.3보다 현저히 낮은 수치입니다. 우리는 이러한 성능 저하가 GPT-5의 객체 탐지 사전 학습(pre-training) 부족에 크게 기인한다고 판단합니다. 이처럼 점수가 낮은 이유에 대한 더 많은 통찰력을 얻기 위해 아래에서 몇 가지 구체적인 사례를 살펴보겠습니다.

**예시: 위치 파악(Localization) 능력의 한계**
배구 데이터셋(dataset)의 한 예시는 낮은 점수의 주된 원인을 명확히 보여줍니다. 모델은 이미지 내의 객체들을 잘 인지하고 있음을 관찰할 수 있습니다. 즉, 공, 두 명의 블로커(blocker), 그리고 몇 명의 수비수가 존재한다는 사실은 정확히 파악하고 있습니다. 그러나 모델은 해당 객체들의 정확한 위치를 파악하는 데 실패하며, 제시된 모든 박스(box)가 객체의 실제 위치 및 크기와 일치하지 않습니다. 이는 모델이 이미지를 전반적으로 이해하는 능력은 우수하지만, 이미지 내 특정 객체를 실제 공간에 '그라운딩(grounding)'하는 능력, 즉 정확한 위치를 식별하는 데는 미숙하다는 것을 의미합니다. 우리는 이러한 현상이 다시 한번 사전 학습(pre-training) 과정에서 객체 탐지(object detection) 작업이 충분히 다뤄지지 않았기 때문이라고 추정합니다. 유사한 상황을 양 데이터셋(dataset)에서도 발견할 수 있습니다. 예를 들어, 의료 영상 분석에서 종양의 존재는 인지하지만 정확한 크기와 위치를 특정하지 못한다면, 이는 진단에 치명적인 오류를 초래할 수 있습니다.

**예시: UI 요소(UI Elements) 인식 및 상호작용**
도구 사용(tool use) 및 시각 기반 에이전트 워크플로우(vision-powered agentic workflows)를 위한 LLM의 최근 발전에 발맞춰, GPT-5의 해당 성능을 심층적으로 분석해 보겠습니다. UI 요소 데이터셋(dataset)에서도 이전 모델 대비 뚜렷한 품질 향상은 보이지 않습니다. 다음으로, GPT-5가 이전 OpenAI 모델들과 비교하여 얼마나 개선되었는지 살펴보면, o3 모델보다 약간 더 나은 결과를 보입니다. 그리고 두 경우 모두 상세한 지침과 같은 추가 정보를 제공하는 것이 모델의 성능 향상에 기여합니다. 흥미롭게도, 추론 노력(reasoning effort)을 높게 설정하더라도 RF100-VL의 점수는 유의미하게 향상되지 않습니다. 따라서 RF100-VL의 평가 기준으로 볼 때 추론의 이점이 그렇게 명확하지 않으며, 이는 사전 학습(pretraining) 단계에서 객체 탐지(object detection) 작업이 부족하여 객체 탐지 능력이 전반적으로 미흡하기 때문이라고 추정합니다. 이는 기존의 UI 자동화 솔루션이 명시적인 UI 계층 구조와 요소 식별을 통해 작동하는 것과 달리, 현재 LLM은 시각 정보만을 바탕으로 이러한 추상적인 요소를 파악하는 데 여전히 어려움을 겪고 있음을 시사합니다.

**GPT-5, 단순 시각 작업에서 미미한 개선을 보이다**
GPT-5의 향상된 추론(reasoning) 능력은 Vision Checkup 리더보드에서 높은 순위를 차지하게 하며, 다단계 사고(multi-step thinking)가 모델이 픽셀(pixel)로부터 더 많은 정보를 추출할 수 있도록 한다는 것을 입증합니다. 이는 ChatGPT에 의존하여 해결하는 일상적인 사용 사례에 있어 훌륭한 결과입니다. 그러나 RF100-VL은 단순히 '이해(comprehension)'하는 것과 '위치 파악(localization)'하는 것이 동일하지 않다는 점을 강조하는 데 중요한 역할을 합니다. 객체 탐지 사전 지식(object-detection priors)이 부재한 상황에서는 감지(detection) 능력이 여전히 목표에 미치지 못합니다. 그럼에도 불구하고, GPT-5의 시각 추론(vision-reasoning) 능력 향상은 모델이 단순히 더 잘 '보는' 것을 넘어, 보고 있는 대상에 대해 더 깊이 '사고'하는 방향으로 발전하고 있음을 명확히 제시합니다. 이는 범용 인공지능(AGI)으로 나아가는 중요한 단계이며, 향후 다중 모달 AI의 발전 방향을 설정하는 데 핵심적인 통찰을 제공합니다. 비전 AI 엔지니어(Vision AI Engineers)들은 저희 블로그에서 더 심층적인 게시물을 확인하실 수 있습니다.