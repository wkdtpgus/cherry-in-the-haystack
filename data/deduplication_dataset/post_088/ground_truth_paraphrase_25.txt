편집자 주: Latent Space는 Roboflow 소속 Matvei 님을 객원 저자로 모시게 되어 기쁩니다! GPT-5-Mini의 시각적 성능이 GPT-5와 동일하게 나타났다는 사실은 주목할 만합니다. 이는 효율적인 모델 라우팅 시스템이 잘 작동하고 있음을 시사하는 자연스러운 결과입니다. 오늘 OpenAI가 공개한 내용 중 상대적으로 덜 부각된 부분은 바로 GPT-5의 시각적 인식 및 추론 역량입니다. 거대 언어 모델(LLM)에 시각적 정보 처리 능력을 부여하는 것은 상당한 난관에 부딪혀 왔습니다. 대다수 모델은 사진 속 물체의 정확한 개수를 파악하거나 특정 이미지 요소의 위치를 명확히 집어내는 데 어려움을 겪었습니다. 실시간으로 주변 환경을 인지하는 LLM의 능력은 자율 로봇 공학이나 컴퓨터 인터페이스의 근본적인 변화를 이끌고, 개별 초지능의 도래를 앞당길 핵심적인 기술적 진보로 여겨집니다.

**LLM과 시각적 이해의 현재 상태: 도전과 기회**
추론(reasoning)과 시각적 능력의 결합은 OpenAI의 GPT 및 o 시리즈 모델, Google의 Gemini 모델, Anthropic의 Claude 모델, Meta의 Llama 모델 등 다양한 인공지능 모델에서 구현되고 있습니다. 각 모델은 특정 작업 유형에 따라 고유한 강점과 약점을 보유합니다. 텍스트 판독, 표지판 인식, 영수증 처리, CAPTCHA 해결, 색상 이해와 같은 비교적 단순한 시각 작업은 대부분의 모델에서 원활하게 처리됩니다. 그러나 물체 개수 세기(counting), 공간적 이해(spatial understanding), 객체 감지(object detection), 복잡한 문서 이해와 같은 고난이도 작업은 성능 편차가 크게 나타나며, 특히 방대한 웹 스케일의 사전 학습(pretraining) 데이터에서 해당 유형의 정보가 부족한 경우 더욱 그렇습니다.
이러한 복잡한 시각적 과제에서 LLM이 어려움을 겪는 주된 이유는 명시적인 '접지(grounding)' 능력의 부족에 있습니다. 즉, 모델이 이미지 내의 픽셀 정보를 실제 세계의 객체 및 그 관계와 정확히 연결 짓는 데 한계가 있다는 의미입니다. 이는 통계적 패턴 학습에는 능숙하지만, 인간처럼 사물의 본질을 직관적으로 이해하고 공간적 맥락에서 추론하는 능력과는 차이가 있습니다. 이러한 격차를 해소하는 것은 주변 환경을 실시간으로 인지하고 상호작용하는 '구체화된 인공지능(embodied AI)'의 발전을 위해 필수적입니다.

**RF100VL: 시각 모델 평가의 새로운 지평**
다양한 작업 유형에 걸친 성능의 불균형은 모델 간의 공정한 비교를 어렵게 만듭니다. 바로 이러한 이유로 Vision Checkup과 같은 평가 리더보드(leaderboard)가 최근 도입되었습니다. Vision Checkup은 당사의 오픈 소스(open source) 시각 모델 평가 플랫폼으로, 난이도 높은 작업에서 최첨단 성능에 대한 귀중한 통찰력을 제공합니다. 보시다시피, OpenAI는 시각 능력 분야에서 여전히 지배적인 위치를 차지하고 있으며, GPT-5의 등장은 상위 5개 모델 목록에 또 하나의 강력한 주자를 추가합니다.
미니 버전이 메인 버전과 동일한 성능을 보인다는 점은 주목할 만하며, 이는 효율적인 라우팅 시스템의 탁월함을 입증합니다. 리더보드는 상위권 모델들이 모두 추론(reasoning) 기반 모델임을 명확히 보여줍니다. 즉, 추론 능력이 일반적인 시각 작업에서 OpenAI의 강력한 우위를 견인하고 있다는 것입니다. Vision Checkup 리더보드의 최고 성능 모델들은 추론 능력을 갖춘 아키텍처를 기반으로 합니다. 우리는 이러한 모델들의 뛰어난 결과가 사전 학습(pretraining) 및 추론 시점에서의 추론 능력에 크게 기인한다고 분석합니다. 이는 텍스트와 시각이라는 두 가지 양식(modality) 모두에서 추론할 수 있는 능력, 즉 다중 모달(multi-modal) 거대 언어 모델(LLM)의 지속적인 발전을 의미합니다. 그러나 점수는 업데이트마다 크게 달라질 수 있는데, 이는 여러 요인으로 설명됩니다. 가장 큰 원인 중 하나는 OpenAI 모델의 추론 모드(reasoning mode)가 가지는 비결정성(nondeterminism)입니다. 동일한 질문을 추론 모델에 두 번 프롬프트(prompt)해도, 때로는 정확한 답변이, 때로는 잘못된 답변이 나올 수 있습니다. 실제 사용 시, 이미지에 대한 추론은 현재 너무 많은 시간이 소요되어 유용성이 떨어지며, 답변의 가변성(variability) 때문에 신뢰하기 어렵습니다. 대부분의 개발자에게 이미지를 이해하는 데 10초 이상 걸리는 것은 실시간 응용 사례를 불가능하게 만들 것입니다. 따라서 속도와 능력 사이에는 명확한 절충점(trade-off)이 존재하며, 특정 작업에 따라서는 지식 범위가 좁더라도 더 빠른 모델이 최적의 선택일 수 있습니다.

**시각적 '바이브 체크'를 넘어선 심층 평가**
우리는 자율 로봇(autonomous robotics)이 주변 환경과 실시간으로 매끄럽게 상호작용할 수 있는 세상과는 아직 거리가 멉니다. 개수 세기(counting), 공간 이해(spatial understanding), 객체 위치 파악(object localization)과 같은 기본적인 작업들은 로봇이 통제되지 않은 환경에서 일반적인 임무를 수행하는 데 필수적인 요소입니다. 단순히 '느낌'에 기반한 리더보드를 넘어설 수 있도록, 우리는 광범위한 도메인(domain)에 걸쳐 LLM을 테스트하고 그 진행 상황을 면밀히 추적해야 합니다. 올해 CVPR 컨퍼런스에서 저희는 더욱 까다로운 시각적 이해 및 그라운딩(grounding) 벤치마크 세트를 제공하기 위해 새로운 벤치마크인 RF100-VL을 공개했습니다. 이 벤치마크는 "당신의 LLM은 실제 세상을 얼마나 잘 이해하는가?"라는 근본적인 질문을 던집니다. RF100-VL은 Roboflow Universe 커뮤니티의 100개 오픈 소스 데이터셋(dataset)으로 구성되어 있으며, 객체 감지 바운딩 박스(object detection bounding boxes)와 시각적 예시 및 풍부한 텍스트 설명을 포함하는 다중 모달(multimodal) 소수샷(few-shot) 지침을 새로운 이미지 도메인(domain) 전반에 걸쳐 제공합니다. 흥미롭게도, 최상위 LLM들도 실제 환경에서 새로운 객체를 식별하는 데 10 mAP50:95 미만의 저조한 점수를 기록했습니다. 모든 LLM 중 현재 SOTA(State-Of-The-Art)는 Gemini 2.5 Pro로, 제로샷(zero-shot) mAP50:95에서 13.3을 달성했습니다. 객체 감지(object detection) 작업에 있어서 OpenAI 모델과 Gemini 또는 Qwen과 같은 모델 간의 주요 차이점은 OpenAI 모델이 사전 학습(pretraining)에 객체 감지 데이터를 포함하지 않는다는 점이라고 추정합니다. RF100-VL에서 GPT-5를 실행한 결과, mAP50:95는 1.5에 불과했습니다. 이는 Gemini 2.5 Pro의 현재 SOTA인 13.3보다 현저히 낮은 수치입니다. 우리는 이러한 성능 격차가 GPT-5의 객체 감지 사전 학습(pre-training) 부족에 기인한다고 크게 보고 있습니다. 점수가 왜 그렇게 낮은지에 대한 더 많은 직관을 얻기 위해 아래에서 몇 가지 결과를 살펴보겠습니다.

**예시: 위치 파악(Localization)의 한계**
배구 데이터셋(dataset)의 예시는 낮은 점수의 주요 원인을 극명하게 보여줍니다. 모델은 이미지 내의 객체들을 전반적으로 잘 이해하고 있음을 관찰할 수 있습니다. 예를 들어, 공, 두 명의 블로커(blocker), 그리고 몇 명의 수비수가 존재한다는 것을 정확히 인식합니다. 하지만 모델은 객체들의 정확한 위치를 파악하는 데 실패하며, 모든 박스(box)가 객체의 실제 위치 및 크기와 일치하지 않습니다. 이는 모델이 이미지를 '이해'하는 데는 능숙하지만, 이미지 내 특정 객체를 정확히 '접지(grounding)'하는 데는 능숙하지 않다는 것을 의미합니다. 이러한 현상은 다시 사전 학습(pre-training) 단계에서 객체 감지(object detection) 작업이 부족했기 때문이라고 해석됩니다. 아래 양 데이터셋(dataset)에서도 비슷한 상황을 목격할 수 있습니다.

**예시: UI 요소(UI Elements) 인식의 중요성**
도구 사용(tool use) 및 시각 기반 에이전트 워크플로우(vision-powered agentic workflows)를 위한 LLM의 최근 발전에 따라, GPT-5의 해당 성능을 분석하는 것은 중요합니다. UI 요소 데이터셋(dataset)에서도 이전 모델 대비 획기적인 품질 향상은 관찰되지 않았습니다. GPT-5는 이전 OpenAI 모델인 o3보다 약간 더 나은 성능을 보였습니다. 그리고 두 경우 모두 상세한 지침과 같은 추가 정보를 제공하는 것이 모델의 성능 향상에 도움이 됩니다. 흥미롭게도, 추론 노력(reasoning effort)을 높게 설정하더라도 RF100-VL의 점수는 유의미하게 향상되지 않았습니다. 따라서 RF100-VL의 경우 추론의 이점이 그렇게 명확하지 않으며, 이는 사전 학습(pretraining)에서 객체 감지(object detection) 작업이 부족하여 객체 감지 능력이 충분히 발달하지 못했기 때문이라고 추정합니다.

**새로운 시각적 이해를 위한 로드맵**
GPT-5는 추상적인 시각적 추론(reasoning) 능력에서 상당한 진전을 보였지만, 실제 세계의 객체에 대한 정밀한 위치 파악(localization) 및 감지(detection) 능력에서는 여전히 개선의 여지가 많습니다. 이러한 한계를 극복하기 위한 다음 단계는 무엇일까요?
첫째, 다양하고 고품질의 '접지된(grounded)' 데이터셋 구축이 필수적입니다. 단순히 객체의 존재를 아는 것을 넘어, 객체의 정확한 위치, 크기, 그리고 주변 환경과의 관계를 명확히 정의하는 데이터가 필요합니다.
둘째, LLM과 전문화된 시각 모델을 결합하는 하이브리드 아키텍처의 연구가 가속화될 것입니다. LLM의 강력한 추론 능력과 전문 비전 모델의 정교한 감지 능력을 시너지 효과를 내는 방향으로 통합하는 것입니다.
셋째, 수집하기 어려운 시나리오를 위한 합성 데이터(synthetic data) 생성의 잠재력도 주목할 만합니다. 현실 세계 데이터를 보완하고, 모델이 다양한 환경에 일반화될 수 있도록 돕는 역할을 할 수 있습니다.
궁극적으로, 이러한 노력들은 물리적 세계를 진정으로 인지하고 그 안에서 지능적으로 행동하는 에이전트의 시대를 여는 데 기여할 것입니다.

**결론: GPT-5의 시각적 진보와 남겨진 과제**
GPT-5의 향상된 추론(reasoning) 능력은 Vision Checkup 리더보드에서 높은 순위를 차지하게 하며, 다단계 사고(multi-step thinking)가 모델이 픽셀(pixel)에서 더 많은 정보를 추출할 수 있도록 한다는 것을 증명합니다. 이는 사람들이 ChatGPT에 의존하여 해결하는 일상적인 사용 사례에 있어 훌륭한 결과입니다. 그러나 RF100-VL 벤치마크는 시각적 '이해'와 '위치 파악'이 동일하지 않다는 점을 명확히 보여줍니다. 객체 탐지에 대한 사전 학습(priors)이 부족하면, 탐지 결과는 여전히 정확도를 벗어날 수밖에 없습니다. 그럼에도 불구하고, GPT-5의 시각적 추론 역량 개선은 단순히 더 잘 '보는' 것을 넘어, 시각 정보에 대해 더욱 심층적으로 '사고'하는 모델로 진화하고 있음을 분명히 나타냅니다. 비전 AI 엔지니어(Vision AI Engineers)는 Roboflow 블로그에서 더 심층적인 게시물을 확인할 수 있습니다.