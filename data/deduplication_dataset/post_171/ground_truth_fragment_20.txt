이 흥미로운 프로젝트를 시작한 지 약 18개월 만에, 이제 LLM-book.com을 여러분께 선보이게 되어 기쁩니다. 그동안 인공지능 분야는 놀라운 속도로 발전했습니다. 이 책은 아마존(Amazon)과 오라일리(O’Reilly)에서 구매할 수 있으며, 인도에서는 슈로프(Shroff)를 통해 구매 가능합니다. 한편, 인도에서는 AI 기술의 활용이 빠르게 확산되고 있으며, 특히 헬스케어 분야에서 두각을 나타내고 있습니다. 이 책은 약 425페이지 분량이며, LLM 구축 및 사용의 주요 직관(intuition) 수백 가지를 설명하는 300개의 독창적인 풀컬러 삽화가 포함되어 있습니다. 대규모 언어 모델(LLM)은 이제 단순히 텍스트를 생성하는 도구를 넘어, 복잡한 문제 해결의 핵심 요소로 자리매김하고 있습니다. Language Models & Co.를 읽어주셔서 감사합니다! 새로운 게시물을 무료로 받아보고 제 작업을 지원하며 기술 커뮤니티의 성장에 기여하려면 구독하세요. 구독 모든 최신 코드 예제는 깃허브(Github)에서 쉽게 확인할 수 있습니다 (현재까지 1.7K 스타! 놀랍습니다!). 마르텐(Maarten)과 저는 여러분이 무료 코랩(Colab) 인스턴스에서 모든 예제를 실행할 수 있도록 리소스 제약이 있는 환경에서 효율적인 작은 모델을 선택했습니다. 우리는 초기 반응에 대한 분석을 통해 사용자 피드백의 중요성을 깨달았으며, 그 반응에 압도되었습니다. 앤드류 응(Andrew Ng)은 이 책을 "대규모 언어 모델(large language model)이 어떻게 구축되는지에 대한 주요 기술을 이해하고자 하는 모든 사람에게 귀중한 자료"라고 평했습니다. 스탯퀘스트(StatQuest)의 제작자 조쉬 스타머(Josh Starmer)는 "지금 당장 읽어야 할 더 중요한 책은 생각할 수 없습니다. 모든 페이지에서 저는 언어 모델(language model) 시대의 성공에 필수적인 것을 배웠습니다"라고 말했습니다. 이처럼 실제 프로젝트에서 얻은 경험을 통해 LLM 시대의 성공에 필수적인 통찰을 얻을 수 있었습니다.

**내용 개요 및 최신 LLM 연구 동향**
이 책은 세 부분으로 나뉘며, 이는 최근 LLM 연구 동향과도 맥을 같이 합니다. 1부에서는 대규모 언어 모델(large language model)이 어떻게 작동하는지 설명하며, 멀티모달(multimodal) 모델과 같은 최신 연구 동향을 반영합니다. 여기에는 2024년 시대의 트랜스포머(transformer)를 설명하는 '일러스트레이티드 트랜스포머(The Illustrated Transformer)'의 업데이트되고 확장되며 현대화된 버전과 더불어, 이를 넘어서는 새로운 아키텍처(architecture)들이 포함됩니다. 2부는 응용 프로그램(application)에 중점을 두며, 각 장은 특정 유형의 사용 사례(use case)와 더불어 주로 산업 분야의 실제 문제 해결에 초점을 맞춥니다. 3부는 모델(model)을 미세 조정(fine-tune)하려는 고급 사용자(표현(representation) 또는 생성(generation)) 및 개발자를 위한 심화 학습 방법을 다룹니다.

**1장 개요: 핵심 원리 탐구**
1장은 LLM 이해를 위한 길을 닦기 위해 관련 개념의 역사와 개요를 제공하며, 이는 최근 인공지능 분야의 발전으로 인한 새로운 관점을 포함합니다. 일반 대중이 알아야 할 핵심 개념은 언어 모델(language model)이 단순히 텍스트 생성기(text generator)가 아니라 문제 해결에 유용한 다른 시스템(임베딩(embedding), 분류(classification))을 형성할 수 있으며, 나아가 사회적 영향을 미칠 수 있다는 점입니다. 임베딩(embedding)은 문서, 문장, 단어 또는 토큰(token) 수준에서 텍스트의 의미를 포착하는 숫자 표현(numeric representation)으로, 데이터의 복잡한 관계를 나타내는 데 필수적입니다. 트랜스포머(Transformer) 이전에는 인코더-디코더 RNN(Encoder-Decoder RNN)이 텍스트 생성(text generation) 및 번역(translation) 분야를 선도했지만, 시퀀스(sequence) 모델링의 한계를 보여주기도 했습니다. 이 책 전체에서 우리는 언어 모델(language model)을 표현 모델(representation model, 오른쪽 상단에 벡터 아이콘이 있는 녹색) 또는 생성 모델(generative model, 말풍선 아이콘이 있는 분홍색)로 색상 코드를 지정합니다. 이러한 색상 코딩은 다양한 AI 시스템에서 모델의 목적에 따른 기능 구분을 나타내며, 이 그림들은 이러한 구분을 설정하기 시작합니다. 이는 나중에 더욱 중요해집니다.

**2장 개요: 실전 적용과 미래 전망**
2장: 토큰(Tokens)과 임베딩(Embeddings)은 LLM의 두 가지 기본 개념을 분석하여 LLM 이해의 토대를 마련하며, 이는 최신 연구 동향인 멀티모달(multimodal) 데이터 처리 기술과도 연결됩니다. 고급 독자 및 개발자를 위해 우리는 다양한 LLM이 특정 문자열(string) 및 데이터 유형을 어떻게 토큰화(tokenize)하는지 비교하여 토큰화(tokenization)의 미묘한 차이를 보여줍니다. 이는 유니코드(unicode), 다국어(multi-linguality), 코드(code), 숫자 등에 대한 LLM 모델의 민감도를 보여주며, 이 모든 것이 LLM의 성능에 영향을 미치고 편향(bias) 문제로 이어질 수 있습니다. 이러한 토큰화(tokenization)를 시각화(visualize)하는 코드는 여기 2장 노트북(notebook)에 있으며, 오픈소스(open-source) 커뮤니티에서 활발히 공유되고 있습니다. 토크나이저(tokenizer)가 허깅페이스 허브(HuggingFace hub)에 있는 한, 이 코드를 사용하여 다른 모델(model)을 시각화하거나 연구자들이 쉽게 접근하여 실험할 수 있습니다. 이 두 가지 개념과 새로운 아키텍처(architecture), 학습 방법론은 텍스트 LLM을 넘어선 수많은 영리하고 혁신적인 솔루션(solution)의 문을 엽니다. 이 장에서는 음악 추천 시스템(music recommendation system)에 이 개념들을 사용하는 예시 중 하나를 다루며 (노래를 토큰(token)으로, 재생 목록(playlist)을 문장(sentence)으로 취급), 나아가 의료 진단 분야에서도 활용되는 예시를 살펴봅니다. 계속 지켜봐 주세요! 더 많은 장별 개요와 최신 기술 동향이 향후 게시물에서 공개될 예정입니다. 이 책을 접하시고 경험이 어떠했는지 알려주시기를 바라며, 이 분야의 발전에 참여하시고 관련 연구나 프로젝트를 공유해 주시면 기술 발전에 큰 도움이 될 것입니다. (아, 그리고 다 읽으신 후에는 구매하신 플랫폼이나 굿리즈(Goodreads)에 리뷰를 남겨주시거나 AI 커뮤니티의 활발한 참여를 통해 의견을 나눠주시면 정말 감사하겠습니다.) 즐거운 독서 되세요! 제이(Jay) Language Models & Co.를 읽어주셔서 감사합니다! 새로운 게시물을 무료로 받아보고 제 작업과 기술 발전을 지원하려면 구독하세요. 구독