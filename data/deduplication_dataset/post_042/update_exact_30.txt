강화 학습(RL)은 대규모 언어 모델(LLM) 연구에서 점점 더 중요한 역할을 하고 있습니다. 초기에 RL은 인간 피드백 기반 강화 학습(RLHF)과 같은 접근 방식을 통해 LLM 정렬(alignment)을 강화하는 데 사용되었습니다. 최근에는 강력한 대규모 추론 모델(LRM) 훈련의 기반이 되었습니다. LLM을 RL로 훈련할 때, 근접 정책 최적화(PPO)와 같은 온라인 알고리즘이 기본적으로 자주 사용됩니다. 그러나 이러한 알고리즘은 지도 미세 조정(SFT) 또는 직접 선호도 최적화(DPO)와 같은 대안에 비해 비용이 많이 들고 복잡합니다. LLM의 네 가지 다른 복사본을 메모리에 유지해야 합니다. 온라인 훈련 과정은 조정하기 어렵고 불안정할 수 있습니다. 적절하게 조정해야 할 훈련 하이퍼파라미터(hyperparameter)가 많습니다. PPO의 복잡성은 온라인 훈련 과정을 안정화해야 하는 필요성에서 비롯됩니다. 이 알고리즘은 로봇 이동(robotic locomotion) 및 아타리 게임 플레이(Atari gameplay)와 같은 작업을 해결하기 위해 신경망(neural network)을 처음부터 훈련하는 데 중점을 둔 이전 세대 연구에서 개발되었습니다. LLM을 위한 RL 설정은 매우 다릅니다. 우리는 이미 강력한 사전 지식(prior)을 가진 사전 훈련된 모델을 미세 조정(fine-tuning)하고 있습니다.

LLM에 RL을 적용하는 초기 단계에서는 PPO와 같은 복잡한 알고리즘이 주로 채택되었습니다. 이는 전통적인 RL 환경에서 안정적인 학습을 보장하기 위한 필수적인 요소로 여겨졌기 때문입니다. 그러나 LLM은 방대한 양의 데이터로 사전 학습되어 이미 강력한 언어 이해 및 생성 능력을 갖추고 있다는 점에서 로봇 제어나 게임 플레이와는 근본적으로 다른 특성을 가집니다. 이러한 차이점은 LLM에 대한 RL 접근 방식이 단순화될 수 있음을 시사하며, PPO의 높은 계산 비용과 복잡성 없이도 효과적인 정렬을 달성할 수 있는 새로운 가능성을 열어줍니다.

"PPO는 RLHF의 정식 방법으로 자리 잡았습니다. 그러나 이는 높은 계산 비용과 민감한 하이퍼파라미터 튜닝(hyperparameter tuning)을 수반합니다. 우리는 PPO 개발로 이어진 동기 부여 원칙이 RLHF에서는 실제적인 문제로 덜 작용하며, 성능을 유지하고 심지어 향상시키는 덜 계산 비용이 드는 방법을 옹호합니다." - [3]에서 발췌

많은 실무자들은 비용과 복잡성 때문에 LLM 훈련 시 온라인 RL 사용을 피합니다. 이 개요에서는 온라인 RL이 그렇게 어렵지 않다는 것을 배우게 될 것입니다! LLM 도메인의 고유한 특성 덕분에, 우리는 REINFORCE 또는 REINFORCE leave-one-out (RLOO)과 같은 더 간단한 알고리즘을 사용하면서도 PPO와 유사한 성능을 달성할 수 있습니다. 따라서 더 간단한 RL-free 또는 오프라인 대안을 선호하여 온라인 RL을 피하는 대신, 불필요한 복잡성 없이 온라인 RL의 이점을 제공하는 알고리즘을 사용할 수 있습니다.

AI 연구의 최신 정보를 얻기 위해 Deep (Learning) Focus를 사용하는 50,000명의 다른 사람들과 함께하세요. 구독

### LLM을 위한 RL의 기본

강화 학습(RL)은 LLM이 인간의 가치와 지침에 더 잘 부합하도록 정렬(align)하고, 특정 작업을 수행하는 능력을 향상시키는 데 필수적인 프레임워크를 제공합니다. LLM의 방대한 지식과 유연성에도 불구하고, 단순히 사전 학습된 상태로는 사용자의 의도를 정확히 파악하거나 유해한 내용을 생성하지 않는 등의 복잡한 행동을 보장하기 어렵습니다. RL은 이러한 행동을 보상 신호와 연결하여 모델이 바람직한 방향으로 학습하도록 유도합니다.

RL 훈련을 실행할 때, 우리는 특정 환경 내에서 행동(action)을 취하는 에이전트(agent)를 가집니다. 아래를 참조하세요.

**RL의 기본 문제 설정**

이러한 행동은 정책(policy)에 의해 예측됩니다. 정책은 에이전트의 두뇌라고 생각할 수 있으며, 일반적으로 매개변수화(parameterized)됩니다(예: LLM 훈련 맥락에서 정책은 LLM 자체입니다). 우리의 정책은 결정론적(deterministic)이거나 확률론적(stochastic)일 수 있지만, 이 개요에서는 정책이 확률론적이라고 가정합니다 1. 우리는 우리의 정책 하에서 주어진 행동의 확률을 $\pi_\theta(a_t | s_t)$로 모델링할 수 있습니다. 정책이 행동을 출력하면, 환경의 상태(state)는 환경의 일부인 전이 함수(transition function)에 따라 업데이트됩니다. 우리의 전이 함수를 $P(s_{t+1} | a_t, s_t)$로 표기할 것입니다. 그러나 전이 함수는 일반적으로 통과(pass-through) 방식이므로 LLM에는 덜 관련성이 있습니다. 즉, 우리는 $s_t = \{x, a_1, a_2, \dots, a_t\}$라고 가정하며, 여기서 $x$는 프롬프트(prompt)입니다. 마지막으로, 에이전트가 방문하는 각 상태는 환경으로부터 양수, 음수 또는 0(즉, 보상 없음)일 수 있는 보상(reward)을 받습니다.

이전 그림에서 보듯이, 우리 에이전트는 반복적으로 행동하며 각 행동($a_t$), 보상($r_t$) 및 상태($s_t$)는 시간 단계(time step) $t$와 연관됩니다. 이러한 시간 단계를 함께 결합하면 궤적(trajectory)이 생성됩니다. 아래를 참조하세요.

여기서는 에이전트가 이 특정 궤적을 위해 환경에서 총 $T$ 단계를 수행한다고 가정합니다. 확률의 연쇄 법칙(chain rule)을 사용하여, 다음 확률들을 결합함으로써 전체 궤적의 확률을 계산할 수도 있습니다.

*   우리의 정책 $\pi_\theta(a_t | s_t)$에 의해 주어진 각 행동 $a_t$.
*   전이 함수 $P(s_{t+1} | a_t, s_t)$에 의해 주어진 각 상태 $s_{t+1}$.

궤적의 확률에 대한 전체 표현은 아래에 제공됩니다.

**궤적의 확률 계산**

**RL 목표(objective).** RL로 모델을 훈련할 때, 우리의 목표는 전체 궤적에 걸쳐 누적 보상(cumulative reward)을 최대화하는 것입니다(즉, $r_t$의 합계). 그러나 이 목표에는 일반적으로 나타나는 몇 가지 변형이 있습니다. 특히, 우리가 최대화하는 보상은 할인되거나(discounted) 할인되지 않을 수 있습니다 2. 아래를 참조하세요.

할인율(discount factor)을 통합함으로써, 우리는 정책이 나중이 아닌 더 빨리 보상을 달성하도록 보상합니다. 다시 말해, 나중에 받는 돈보다 지금 받는 돈이 더 좋습니다. 우리의 목표는 일반적으로 기대 누적 보상(expected cumulative reward)으로 표현되며, 여기서 기대값(expectation)은 궤적에 대해 취해집니다. 이 기대값을 확장하면 각 궤적에 대한 보상의 가중 합(weighted sum)이 생성됩니다. 가중치는 단순히 궤적의 확률입니다. 우리는 이것을 연속적(continuous) 또는 이산적(discrete) 방식으로 공식화할 수 있습니다. 아래를 참조하세요.

우리는 훈련 중에 이 목표를 최대화하기를 원하며, 이는 경사 상승(gradient ascent) 3을 통해 달성할 수 있습니다. 아래를 참조하세요.

이 설정을 고려할 때, 우리가 답해야 할 남은 질문은 다음과 같습니다. 이 경사를 어떻게 계산할까요? 보시다시피, RL에 대한 많은 연구는 이 질문에 답하는 데 중점을 두며, 많은 기술이 존재합니다.

LLM 맥락에서 보상 설계는 매우 중요하면서도 까다로운 작업입니다. 단순히 '정답'을 주는 것을 넘어, 생성된 텍스트의 유용성, 안전성, 진실성, 일관성, 그리고 인간의 선호도까지 반영해야 합니다. 예를 들어, 대화형 AI의 경우, 보상은 대화의 자연스러움, 사용자 만족도, 정보의 정확성 등을 복합적으로 평가할 수 있습니다. 이러한 다면적인 보상 신호를 효과적으로 포착하고 모델에 전달하는 것이 LLM RL의 핵심 과제 중 하나입니다.

#### 보상 모델링의 진화와 도전

LLM의 RL 훈련에서 보상(reward)은 모델의 행동을 이끄는 가장 중요한 신호입니다. 초기 RLHF에서는 인간 평가자들이 직접 생성된 텍스트에 점수를 매기거나 순위를 지정하여 보상 모델(Reward Model, RM)을 훈련했습니다. 하지만 인간 피드백은 비용이 많이 들고 시간이 오래 걸리며, 때로는 일관성이 부족하다는 단점이 있습니다.

최근에는 이러한 한계를 극복하기 위해 다양한 보상 모델링 기법이 연구되고 있습니다.
*   **자동 보상 모델 (Automated Reward Models)**: 특정 규칙 기반 검증자(verifier)나 외부 API (예: 검색 엔진, 코드 컴파일러)를 사용하여 보상을 자동으로 생성하는 방식입니다. 이는 확장성이 높지만, 복잡한 인간 선호도를 포착하는 데 한계가 있습니다.
*   **합성 데이터 생성 (Synthetic Data Generation)**: 더 강력한 LLM (예: GPT-4)을 '심사위원'으로 사용하여 약한 LLM의 생성물에 대한 피드백을 생성하고, 이를 통해 보상 모델을 훈련하는 방법입니다. 이는 인간 피드백의 필요성을 줄일 수 있지만, '모델의 모델'이 가지는 편향을 물려받을 수 있습니다.
*   **세분화된 보상 (Fine-grained Rewards)**: 전체 완성(completion)에 대한 단일 보상 대신, 텍스트의 특정 부분이나 단계별 추론 과정에 보상을 할당하여 크레딧 할당 문제를 완화하려는 시도입니다. 이는 모델이 더 정교한 행동을 학습하도록 돕습니다.
이러한 보상 모델링의 발전에도 불구하고, 보상 해킹(reward hacking) 문제, 즉 모델이 보상 함수를 속여서 실제로는 바람직하지 않은 행동을 하면서 높은 점수를 받는 현상은 여전히 중요한 도전 과제입니다.

**상태, 가치, 이점 함수.** RL 목표와 관련하여, 우리는 다음 함수 집합을 정의할 수도 있습니다.

*   **가치 함수(Value Function) $V(s)$**: 상태 $s$에서 시작하여 현재 정책 $\pi_\theta$에 따라 행동할 때의 기대 누적 보상.
*   **행동-가치 함수(Action-Value Function) $Q(s, a)$**: 상태 $s$에서 시작하여 행동 $a$를 취한 다음 정책 $\pi_\theta$에 따라 행동할 때의 기대 누적 보상.
*   **이점 함수(Advantage Function) $A(s, a)$**: 행동-가치 함수와 가치 함수의 차이입니다. 즉, $A(s, a) = Q(s, a) - V(s)$.

직관적으로, 이점 함수는 상태 $s$에서 행동 $a$를 취한 후의 기대 보상과 상태 $s$로부터의 일반적인 기대 보상 간의 차이를 취함으로써 특정 행동 $a$가 얼마나 유용한지 알려줍니다. 행동 $a$로부터의 보상이 기대보다 높으면 이점은 양수가 되고, 그 반대도 마찬가지입니다. 이점 함수는 RL 연구에서 매우 중요한 역할을 합니다. 이들은 우리 정책의 경사를 계산하는 데 사용됩니다.

"때때로 RL에서는 행동이 절대적으로 얼마나 좋은지 설명할 필요 없이, 평균적으로 다른 행동보다 얼마나 더 나은지만 알면 됩니다. 즉, 우리는 그 행동의 상대적 이점(relative advantage)을 알고 싶습니다. 우리는 이 개념을 이점 함수로 정확하게 만듭니다." - Deep RL에서 Spinning up 발췌

### 마르코프 결정 과정(MDP) 대 밴딧 공식화(Bandit Formulation)

이제 RL의 기본을 이해했으므로, 우리가 배운 용어를 LLM 훈련 설정에 매핑해야 합니다. 다음과 같이 할 수 있습니다(위에 표시됨).

*   우리의 정책은 LLM 자체입니다.
*   우리의 초기 상태는 프롬프트입니다.
*   LLM의 출력—각 토큰(token) 또는 전체 완성(completion)—은 **행동**입니다.
*   우리의 상태는 프롬프트와 LLM 출력의 조합입니다.
*   LLM의 전체 완성은 **궤적**을 형성합니다.

특히, 이 설정에는 전이 함수가 없습니다. 왜냐하면 전이 함수가 완전히 결정론적이기 때문입니다. 만약 우리가 프롬프트 $x$로 시작하고 LLM이 이 프롬프트를 입력으로 받아 토큰 $t_1$과 $t_2$를 예측한다면, 우리의 업데이트된 상태는 단순히 $s_2 = \{x, t_1, t_2\}$가 됩니다. 다시 말해, 우리의 상태는 주어진 프롬프트 $x$에 대해 LLM에 의해 생성되는 실행 중인 완성입니다.

**마르코프 결정 과정(MDP) 공식화.** LLM의 경우, RL이 행동을 모델링하는 방식에 따라 두 가지 주요 방식으로 공식화될 수 있습니다. LLM이 다음 토큰 예측(next token prediction)을 통해 출력을 생성한다는 것을 기억해야 합니다. 즉, 출력 완성의 각 토큰을 순차적으로 생성함으로써 말입니다. 이 자기회귀(autoregressive) 과정은 아래에 묘사되어 있습니다.

보시다시피, 다음 토큰 예측 과정은 RL 설정에 매우 쉽게 매핑됩니다. 우리는 각 토큰을 개별 행동으로 모델링할 수 있습니다! LLM 출력의 각 토큰을 개별 행동으로 모델링하는 접근 방식을 **마르코프 결정 과정(MDP) 공식화**라고 합니다. MDP는 상태, 행동, 전이 확률(transition probabilities) 및 보상을 포함하는 의사 결정(decision-making) 모델링을 위한 확률론적 프레임워크(probabilistic framework)입니다. 이는 우리가 지금까지 RL에 대해 논의한 설정과 정확히 일치합니다! RL에 사용되는 MDP 공식화는 아래에 표시되어 있습니다.

LLM을 위한 MDP로 RL을 모델링할 때, 우리의 초기 상태는 프롬프트이며 우리의 정책은 개별 토큰을 예측함으로써 행동합니다. 우리의 LLM은 토큰 분포를 예측하는 확률론적 정책을 형성합니다. 생성 과정에서, 이 분포에서 토큰을 선택함으로써 행동이 취해집니다. 각 토큰은 그 자체의 행동입니다. 토큰이 예측된 후, 현재 상태에 추가되고 LLM에 의해 다음 토큰을 예측하는 데 사용됩니다. 이는 단지 자기회귀 다음 토큰 예측입니다! 결국, LLM은 생성 과정을 완료하기 위해 정지 토큰(stop token)(예: `<|end_of_text|>` 또는 `<eos>`)을 예측하여 완전한 궤적을 생성합니다.

MDP 공식화는 LLM의 자기회귀적 특성을 자연스럽게 반영하지만, 실제 보상 메커니즘과 충돌하는 경우가 많습니다. 대부분의 LLM 보상은 전체 응답이 생성된 후에만 주어지기 때문에, 각 토큰에 대한 개별적인 보상을 할당하는 것은 '희소 보상(sparse reward)' 문제를 야기합니다. 즉, 모델은 어떤 토큰이 최종 보상에 긍정적으로 기여했는지 학습하기 어렵게 됩니다. 이는 크레딧 할당(credit assignment) 문제를 심화시키고 학습 효율성을 저해할 수 있습니다.

**밴딧 공식화.** 위 MDP 묘사에서, 우리는 모든 시간 단계마다 보상이 제공된다고 가정하지만, LLM의 보상 메커니즘은 일반적으로 이와는 약간 다릅니다. 대부분의 LLM은 결과 감독(outcome supervision) 4를 사용하여 훈련됩니다. 이는 모델이 완전한 응답을 생성한 후에만(즉, `<eos>` 토큰이 출력된 후에만) 보상이 할당됨을 의미합니다.

**LLM을 위한 결과 감독 대 과정 감독**

결과 감독 설정에서, 우리는 각 토큰을 그 자체의 행동으로 모델링하는 유용성에 의문을 제기하기 시작할 수 있습니다. 이 시나리오에서 어떤 단일 행동이 유용한지 아닌지를 어떻게 알 수 있을까요? 대안으로, 우리는 전체 응답을 결과 보상을 받는 단일 행동으로 모델링할 수 있습니다. 이것이 LLM을 사용한 RL 훈련을 위한 밴딧 공식화의 핵심 아이디어입니다. 아래를 참조하세요.

이 이름은 확률론의 문맥적 밴딧(contextual bandit) 개념에서 유래합니다. 밴딧 설정은 간단합니다. 우리 에이전트가 행동을 선택하고, 보상을 받으면 에피소드(episode)가 종료됩니다. 우리의 완전한 궤적은 단일 행동과 보상입니다! LLM의 경우, 우리의 행동은 프롬프트에 대해 생성된 전체 완성이며, 이는 결과 보상을 받습니다.

#### MDP vs. Bandit: 장단점 및 적용 시나리오

LLM RL 훈련에서 MDP와 밴딧 공식화 중 어떤 것을 선택할지는 보상 신호의 특성과 훈련 목표에 따라 달라집니다.

**마르코프 결정 과정(MDP) 공식화:**
*   **장점**: LLM의 자기회귀적 생성 과정과 자연스럽게 일치합니다. 이론적으로는 각 토큰 생성 단계에서 정책을 최적화하여 더 세밀한 제어가 가능합니다.
*   **단점**: 보상이 희소할 경우 크레딧 할당 문제가 심화됩니다. 각 토큰에 대한 보상 모델을 구축하거나 추정하는 것이 복잡하고, PPO와 같은 알고리즘은 높은 계산 비용과 메모리 요구 사항을 가집니다.
*   **적용 시나리오**: 과정 감독(process supervision)이 가능하거나, 각 토큰 선택이 명확한 중간 보상으로 이어지는 특정 작업(예: 코드 디버깅, 수학 문제 풀이의 단계별 검증)에서 유용할 수 있습니다.

**밴딧 공식화:**
*   **장점**: 구현이 훨씬 간단하며, 대부분의 LLM RLHF 설정에서 자연스러운 결과 감독 보상과 직접적으로 매핑됩니다. 희소 보상 문제를 완화하고, REINFORCE와 같은 간단한 알고리즘을 사용하여 효율적인 훈련이 가능합니다.
*   **단점**: 전체 완성에 대한 단일 보상으로 인해, 어떤 특정 토큰 선택이 최종 결과에 긍정적으로 기여했는지에 대한 세부적인 정보를 놓칠 수 있습니다. 이는 모델이 미묘한 행동을 학습하는 데 한계가 있을 수 있습니다.
*   **적용 시나리오**: 요약, 대화 생성, 질의응답 등 최종 완성의 품질이 중요한 대부분의 LLM 정렬 작업에서 선호됩니다. 특히 인간 피드백이 전체 응답에 대해 주어지는 경우 가장 적합합니다.

결론적으로, 현재 LLM RLHF의 주류는 밴딧 공식화에 가깝습니다. 이는 복잡한 MDP 기반 접근 방식보다 구현이 간단하고, 효과적인 결과를 도출할 수 있기 때문입니다. 그러나 미래에는 더 복잡한 추론 작업을 위해 MDP의 세밀한 제어 이점을 활용하면서도 희소 보상 문제를 해결하는 하이브리드 접근 방식이 더욱 중요해질 수 있습니다.

### LLM을 위한 RL 훈련

LLM을 위한 RL 훈련은 일반적으로 프롬프트에 대한 응답을 생성하고, 이 응답의 품질을 평가하여 보상을 할당한 다음, 이 보상을 사용하여 LLM의 정책을 업데이트하는 반복적인 과정으로 진행됩니다. 이 과정은 모델이 시간이 지남에 따라 점차 더 나은 응답을 생성하도록 학습하게 합니다.

오늘날 LLM에 일반적으로 사용되는 RL 훈련의 두 가지 광범위한 범주가 있습니다.

*   **인간 피드백 기반 강화 학습(RLHF)**은 인간 선호도 보상 모델(human preference reward model)에서 파생된 보상을 사용하여 RL로 LLM을 훈련합니다.
*   **검증 가능한 보상 기반 강화 학습(RLVR)**은 규칙 기반 또는 결정론적 검증자(deterministic verifiers)에서 파생된 보상을 사용하여 RL로 LLM을 훈련합니다.

이러한 RL 훈련 기술은 주로 훈련을 위한 보상을 도출하는 방식에서 차이가 있지만, 알고리즘의 다른 세부 사항은 대부분 유사합니다. 아래에 묘사된 바와 같이, 둘 다 프롬프트 세트에 대해 완성을 생성하고, 이러한 완성에 대한 보상을 계산하며, 보상을 사용하여 RL 최적화기(optimizer)로 정책 업데이트—또는 LLM 매개변수 업데이트—를 도출함으로써 작동합니다.

**LLM을 위한 RL의 시각적 묘사**

LLM을 위한 RL 훈련 과정은 보통 세 가지 주요 단계로 구성됩니다. 첫째, 지도 미세 조정(Supervised Fine-Tuning, SFT)을 통해 초기 LLM을 특정 작업에 맞게 조정합니다. 둘째, SFT 모델의 응답에 대한 인간의 선호도를 수집하여 보상 모델(Reward Model, RM)을 훈련합니다. 이 RM은 LLM이 생성한 응답의 품질을 평가하는 역할을 합니다. 셋째, 이 RM을 사용하여 LLM을 강화 학습으로 미세 조정합니다. 이 마지막 단계에서 LLM은 RM으로부터 받은 보상을 최대화하도록 정책을 업데이트합니다.

이 과정의 마지막 단계는 이전에 보았듯이 RL 목표에 대한 경사 상승 단계입니다. 그러나 RL 훈련에 사용되는 실제 목표는 누적 보상 최대화를 넘어섭니다. 우리는 참조 정책(reference policy)—일반적으로 RL 훈련 시작 시점의 LLM 체크포인트(checkpoint)—에 대한 우리 정책의 KL 발산(KL divergence)을 최소화하면서 보상을 최대화하려고 합니다. 우리는 새로운 모델이 참조 모델과 크게 다르지 않게 하면서 보상을 최대화하기를 원합니다. 아래를 참조하세요.

**KL 발산을 포함한 RL 훈련 목표**

정책의 매개변수에 대한 이 목표의 경사를 계산하는 것이 RL을 이해하는 데 있어 대부분의 복잡성이 있는 부분입니다. LLM의 맥락에서, 우리는 이 경사를 계산하기 위해 정책 경사 알고리즘(예: PPO, GRPO 및 REINFORCE)을 사용합니다. 이 개요는 주로 REINFORCE와 그 변형에 초점을 맞출 것이지만, 이러한 알고리즘이 어떻게 작동하는지 배우려면 먼저 정책 경사의 가장 간단한 형태인 바닐라 정책 경사(VPG)를 이해해야 합니다.

KL 발산(Kullback-Leibler divergence)은 LLM RL 훈련에서 매우 중요한 역할을 합니다. 이 용어는 두 확률 분포 간의 차이를 측정하며, RL 훈련에서는 현재 정책의 분포와 초기 SFT 모델 또는 이전 정책의 분포 간의 차이를 나타냅니다. KL 발산을 최소화하는 항을 보상 함수에 추가하는 주된 이유는 다음과 같습니다.
*   **안정성 유지**: RL 훈련은 정책을 급격하게 변경하여 불안정해질 수 있습니다. KL 항은 현재 정책이 참조 정책에서 너무 멀리 벗어나지 않도록 하여 모델의 일관성과 안정성을 유지합니다. 이는 "파국적 망각(catastrophic forgetting)"을 방지하는 데 도움을 줍니다.
*   **탐색과 활용의 균형**: KL 항은 모델이 새로운 행동을 탐색하는 것과 기존의 잘 작동하는 행동을 활용하는 것 사이에서 균형을 찾도록 돕습니다. 너무 큰 KL 페널티는 탐색을 억제하고, 너무 작은 페널티는 모델이 참조 정책에서 너무 멀리 벗어나게 할 수 있습니다.
*   **원치 않는 행동 방지**: 사전 학습된 LLM은 이미 많은 유용한 지식과 언어적 특성을 가지고 있습니다. KL 항은 RL 훈련 과정에서 이러한 바람직한 특성들이 손상되지 않도록 보호하는 역할을 합니다.

KL 발산은 보상 함수에 직접 페널티 항으로 통합되거나, PPO와 같은 알고리즘에서는 손실 함수 내에 별도의 항으로 추가될 수 있습니다. 어떤 방식이든, KL 발산은 LLM RL 훈련의 성공적인 정렬을 위한 필수적인 구성 요소입니다.

### 바닐라 정책 경사(VPG) 도출

바닐라 정책 경사(VPG)는 강화 학습, 특히 정책 기반 방법의 가장 기본적인 형태입니다. VPG는 정책을 직접 최적화하여 기대 보상을 최대화하는 것을 목표로 합니다. 이는 더 복잡한 알고리즘의 기초가 되므로, 그 원리를 이해하는 것이 중요합니다.

여기서는 완전성을 위해 바닐라 정책 경사(VPG)의 전체 도출을 다룰 것입니다. 그러나 VPG를 매우 잘 설명하는 많은 기존 개요가 있습니다. 추가 학습을 위한 몇 가지 훌륭한 자료는 다음과 같습니다.

*   OpenAI의 정책 최적화 소개 [링크]
*   Nathan Lambert의 RLHF 서적 [링크]
*   Lilian Weng의 정책 최적화 알고리즘 [링크]

또한, 이 뉴스레터의 VPG 및 정책 최적화에 대한 이전 분석은 쉽게 참조할 수 있도록 아래에 링크되어 있습니다. 이 섹션의 논의는 주로 정책 경사에 대한 이 더 자세한 설명에서 발췌될 것입니다.

**정책 경사: RLHF의 기초**
Cameron R. Wolfe, Ph.D. · 2023년 10월 2일
정책 경사에 대한 심층 분석, 신경망 훈련에 어떻게 적용되는지, 그리고 가장 간단한 형태의 도출.
전체 이야기 읽기

**기본 정책 경사.** 정책 최적화에서 우리의 목표는 정책 경사, 즉 우리 RL 목표(여기서는 누적 보상이라고 가정합니다)의 경사를 우리 정책의 매개변수에 대해 계산하는 것입니다. 정책 경사를 계산하는 첫 단계로, 아래에 표시된 도출을 수행할 수 있습니다.

(출처)

이 도출은 우리 RL 훈련 목표(누적 보상)의 경사로 시작하여 정책 경사에 대한 기본 표현으로 끝납니다. 정책 경사에 도달하기 위해, 우리는 주로 i) 연속 확률 변수(continuous random variable)에 대한 기대값의 정의와 ii) 로그-미분 트릭(log-derivative trick)과 같은 간단한 단계를 사용합니다. 이 도출에서 가장 복잡한 단계는 마지막 단계로, 궤적의 로그 확률(log probability) 경사를 행동의 로그 확률 경사의 합으로 변환합니다. 이 단계는 궤적 확률에 대한 우리의 이전 표현을 사용하고, 곱셈을 덧셈으로 변환하며(즉, 로그 확률로 작업하기 때문입니다), 초기 상태 확률과 전이 함수의 정책 매개변수에 대한 경사는 항상 0이라는 것을 관찰합니다. 왜냐하면 이 구성 요소들 중 어느 것도 정책에 의존하지 않기 때문입니다. 아래를 참조하세요.

(출처)

VPG의 수학적 도출은 정책 경사 방법론의 핵심을 이해하는 데 중요합니다. 로그-미분 트릭은 확률 분포의 경사를 계산할 때 매우 유용한 기술로, 정책의 매개변수에 대한 기대 보상의 경사를 직접 계산하기 어렵기 때문에 사용됩니다. 이 트릭은 경사 계산을 샘플링된 궤적에 대한 로그 확률의 경사로 변환하여 몬테카를로(Monte Carlo) 추정을 가능하게 합니다.

### 기본 정책 경사 구현.

위에서 도출한 기본 정책 경사 표현은 실제로 계산하기 매우 쉽습니다. 특히, 이 표현은 우리가 이미 계산 방법을 알고 있는 두 가지 핵심 양을 포함합니다.

*   보상은 검증자(verifier) 또는 보상 모델(reward model)에서 직접 나옵니다.
*   행동의 로그 확률은 우리 LLM으로 계산할 수 있습니다(즉, 이는 LLM 출력의 토큰 확률입니다).

기본 정책 경사 계산 과정을 더 구체적으로 만들기 위해, PyTorch 유사 코드(pseudocode)로 된 단계별 구현이 아래에 제공되었습니다. 이 기본 정책 경사 구조의 핵심 직관은 높은 보상을 가진 궤적으로부터의 행동 확률을 증가시킨다는 것입니다.

"이 경사로 한 걸음 나아가면, $R(\tau)$에 비례하여 각 행동의 로그 확률이 증가합니다. $R(\tau)$는 지금까지 얻은 모든 보상의 합계입니다." - Deep RL에서 Spinning up 발췌

이 형태의 정책 경사는 간단하지만, 여전히 실제에서 나타납니다! 예를 들어, Cursor는 온라인 RL에 대한 최근 블로그에서 이 정확한 표현을 사용합니다. 그러나 그들의 블로그에 있는 표현은 밴딧 공식화를 가정하며, 이로 인해 표현에서 합계가 제거됩니다(즉, 행동이 하나뿐이기 때문입니다).

(출처)

VPG는 개념적으로는 간단하지만, 실제 복잡한 환경에서는 몇 가지 한계를 가집니다. 특히, 정책 업데이트가 너무 크거나 불안정할 수 있으며, 이는 학습 과정을 발산시키거나 최적의 정책을 찾기 어렵게 만들 수 있습니다. 이러한 문제는 주로 경사 추정치의 높은 분산에서 비롯됩니다. 이 때문에 VPG는 종종 '바닐라' 또는 '기본' 버전으로 불리며, 실제 적용에서는 분산 감소 기법이나 신뢰 영역(trust region) 방법을 통합한 더 정교한 알고리즘이 사용됩니다.

### 분산 감소

VPG의 한계를 극복하기 위해 분산 감소(variance reduction) 기법은 RL 알고리즘의 핵심 연구 분야입니다. 분산을 줄이는 것은 학습 안정성을 높이고, 더 적은 샘플로도 효율적인 학습을 가능하게 합니다.

우리의 현재 정책 경사 표현은 간단하지만, 몇 가지 주목할 만한 문제점을 가지고 있습니다.

*   경사가 높은 분산(variance)을 가질 수 있습니다.
*   크고 불안정한 정책 업데이트에 대한 보호 장치가 없습니다.

대부분의 후속 정책 경사 알고리즘은 정책 경사의 분산을 줄이고 정책 업데이트에 대한 신뢰 영역(trust region)을 강제함으로써 이러한 문제를 해결하는 것을 목표로 합니다. 즉, 단일 업데이트에서 모델을 얼마나 변경할 수 있는지 제한하는 것입니다. 이를 위해, 우리는 일반적으로 정책 경사의 보상 항을 약간 다른 항으로 대체합니다. 몇 가지 일반적인 옵션은 아래를 참조하세요.

([4]에서)

보시다시피, 이 표현은 이전에 본 것과 거의 동일합니다. 유일한 차이점은 $R(\tau)$를 일반적인 $\Psi_t$ 항으로 바꿨다는 점이며, 이는 몇 가지 다른 값으로 설정될 수 있습니다. 예를 들어, 우리는 다음을 할 수 있습니다.

*   $\Psi_t = R(\tau)$로 설정하여 기본 정책 경사 표현을 복구합니다.
*   $\Psi_t$를 시간 $t$ 이후에 받은 보상(즉, reward-to-go 정책 경사)과 같게 설정하여, 행동 이전에 발생한 보상으로 행동에 크레딧을 부여하는 것을 피합니다.
*   $\Psi_t$를 보상의 기준선화된(baselined) 버전으로 설정합니다.
*   $\Psi_t$를 상태-행동($Q$) 또는 이점 함수($A$)와 같게 설정합니다.

이러한 선택 사항과 그 도출 방법에 대한 전체 개요는 여기에서 찾을 수 있습니다. 이러한 알고리즘들의 공통된 주제는 기준선(baseline) 또는 추가 항(아래에 표시된 대로 보상에서 빼는, 상태 $s_t$에만 의존해야 하는 항)의 사용입니다.

기준선은 상태에 대한 보상(또는 가치)을 정규화하는 역할을 하며, 정책 경사의 분산을 줄이는 것으로 나타났습니다 5.

**정책 경사의 보상에 기준선 추가**

바닐라 정책 경사 알고리즘의 일반적인 문제는 경사 업데이트의 높은 분산입니다… 이를 완화하기 위해, 기준선이라고 불리는 다양한 기술이 가치 추정(value estimation)을 정규화하는 데 사용됩니다. 기준선은 여러 가지 방식으로 이를 달성하며, 다운스트림 행동(downstream action)에 대한 상태의 가치로 효과적으로 정규화합니다(예: Q 값과 가치의 차이인 이점(Advantage)의 경우). 가장 간단한 기준선은 보상 배치(batch)에 대한 평균 또는 이동 평균(moving average)입니다. - RLHF 서적

우리가 보게 될 대부분의 알고리즘은 $\Psi_t$를 이점 함수와 같게 설정하는 데 중점을 둡니다. 이것이 **바닐라 정책 경사(VPG) 알고리즘**으로 알려져 있습니다. 이점 함수는 가장 낮은 분산의 정책 경사를 생성하기 때문에 일반적으로 사용됩니다.

**바닐라 정책 경사**

**신뢰 영역(Trust Region)의 중요성.** PPO와 같은 알고리즘은 정책 업데이트의 안정성을 보장하기 위해 '신뢰 영역(Trust Region)' 개념을 도입합니다. 이는 정책이 한 번의 업데이트로 너무 크게 변하는 것을 방지하여, 정책이 최적의 경로에서 벗어나 발산하는 것을 막습니다. PPO는 정책 비율(policy ratio)에 클리핑(clipping)을 적용하여 이를 달성합니다. 즉, 새로운 정책과 이전 정책의 확률 비율이 특정 범위(예: $[1-\epsilon, 1+\epsilon]$)를 벗어나지 않도록 제한합니다. 이 $\epsilon$ 값은 하이퍼파라미터로, 신뢰 영역의 크기를 조절합니다. 이러한 메커니즘은 정책 업데이트의 안정성을 크게 향상시키지만, 동시에 알고리즘의 복잡성을 증가시키고 튜닝해야 할 하이퍼파라미터의 수를 늘립니다.

**액터-크리틱(Actor-critic).** 이점 함수가 상태-행동 가치 함수와 가치 함수의 차이라는 것을 기억해야 합니다. 다시 말해, VPG 알고리즘은 정책 경사에서 가치 함수를 기준선으로 효과적으로 사용합니다. 가치 함수는 온-정책(on-policy)입니다. 즉, 현재 훈련 반복(iteration)에서 우리 정책의 정확한 매개변수에 의존합니다. 일반적으로, 우리는 신경망으로 가치 함수를 추정합니다. LLM의 경우, 가치 함수는 LLM의 가중치(weights)로 초기화되고 가치 함수를 예측하도록 훈련된 별도의 가치 헤드(value head) 6(또는 모델)로 근사화됩니다. 가치 함수를 추정하는 데 사용되는 LLM은 가치 모델(value model) 또는 크리틱(critic)이라고 불립니다. 크리틱은 시퀀스 내의 모든 토큰에 대해 가치 함수—또는 주어진 토큰이나 상태에서 시작하는 기대 보상—를 예측합니다. RL 훈련 동안, 크리틱은 각 정책 업데이트마다 LLM과 함께 활발하게 업데이트됩니다. 이를 액터-크리틱 설정 7이라고 합니다. RL 훈련 시작 시 고정되는 보상 모델과 달리, 크리틱은 정책의 현재 매개변수에 의존합니다. 따라서 온-정책을 유지하고 예측이 오래되지 않도록 하려면, 크리틱은 LLM 자체와 함께 업데이트되어야 합니다. PPO는 이러한 액터-크리틱 설정을 채택하는 정책 경사 알고리즘의 주목할 만한 예입니다. 크리틱은 일반적으로 예측된 보상과 실제 보상 간의 평균 제곱 오차(MSE) 손실(loss)을 사용하여 업데이트됩니다. 액터-크리틱 알고리즘의 유사 코드 구현이 아래에 제공됩니다.

비록 이것이 일반적인 설정이지만, 가치 모델의 사용은 상당히 비용이 많이 들 수 있습니다. 이는 LLM의 전체 추가 복사본을 메모리에 유지해야 합니다! 사실, 크리틱을 사용하는 것은 PPO가 높은 계산 오버헤드(computational overhead)를 가지는 이유 중 일부입니다. 다음으로, 가치 함수를 추정하기 위한 더 간단하고 효율적인 접근 방식을 채택하는 알고리즘에 대해 배울 것입니다.

```python
import torch
import torch.nn.functional as F

# sample prompt completions and rewards
with torch.no_grad():
    completions = LLM(prompts) # (B*G, L)
    rewards = RM(completions) # (B*G, 1)

# compute value function / critic output
values = CRITIC(completions) # (B*G, L) - per token!
advantage = rewards - values.detach()

# get logprobs for each action
completion_mask = <... mask out padding tokens ...>
llm_out = LLM(completions)
token_logp = F.log_softmax(llm_out, dim=-1)

# loss includes a weighted combination of the policy gradient
# loss and the MSE loss for the critic
loss = (- token_logp * advantage) * completion_mask
loss += _beta * (0.5 * (values - rewards)**2)

# aggregate the loss (many options exist here)
loss = (loss.sum(axis=-1) / completion_mask.sum(axis=-1)).mean()

# gradient update
optimizer.zero_grad()
loss.backward()
optimizer.step()
```

### LLM을 위한 REINFORCE 및 RLOO

지금까지 우리는 LLM을 위한 정책 최적화 및 RL의 기본 개념에 대해 배웠습니다. 우리가 도출한 기본 정책 경사는 실제로 계산하기 쉽지만, 이러한 공식화는 높은 분산의 정책 경사와 불안정한 훈련으로 이어집니다. 분산을 줄이기 위해, 우리는 이점 추정치(advantage estimate)를 정책 경사에 통합하는 RL 최적화기가 필요합니다. 그러나 PPO와 같은 인기 있는 알고리즘은 상당한 오버헤드를 유발하는 복잡한 액터-크리틱 프레임워크(actor-critic framework)를 사용하여 이를 달성합니다. 이러한 추가된 복잡성을 고려할 때, 우리는 궁금해할 수 있습니다. LLM을 훈련할 때 온라인 RL 기술을 완전히 피해야 할까요?

"최근 연구들은 DPO 또는 LLM 선호도 훈련에 대한 반복적 미세 조정 접근 방식과 같은 RL-free 방법을 제안합니다. 그러나 이러한 연구들은 RL 패러다임 내에서 더 간단한 해결책이 존재하는지에 대한 질문을 하지 못합니다." - [3]에서 발췌

많은 오프라인 및 RL-free 훈련 대안이 존재하지만, LLM을 훈련하는 데 사용할 수 있는 간단한 온라인 RL 알고리즘도 있습니다. 이 섹션에서는 REINFORCE와 이 알고리즘의 약간 수정된 버전인 REINFORCE leave one out (RLOO)에 대해 배울 것입니다. 이 온라인 RL 알고리즘들은 훈련 전반에 걸쳐 관찰된 보상의 평균으로 가치 함수를 추정함으로써 크리틱의 필요성을 없앱니다. 이론적으로, 이러한 접근 방식은 PPO와 같은 액터-크리틱 알고리즘에 비해 더 높은 분산의 정책 경사를 생성합니다. 그러나 최근 연구 [3, 5]는 이러한 분산 증가가 LLM 훈련에 영향을 미치지 않으며, 온라인 RL 훈련을 위한 사용하기 쉽고 고성능의 옵션을 제공한다는 것을 발견했습니다.

REINFORCE와 RLOO의 가장 큰 장점은 액터-크리틱 구조에서 요구되는 별도의 가치 모델, 즉 크리틱이 필요 없다는 점입니다. 이는 모델의 메모리 사용량을 크게 줄이고 훈련 과정을 단순화합니다. 특히 LLM과 같이 거대한 모델을 다룰 때, 추가적인 모델을 메모리에 로드하고 업데이트하는 것은 상당한 자원 소모를 의미합니다. 크리틱 없이 보상의 평균을 기준선으로 사용하는 접근 방식은 이러한 오버헤드를 제거하여, 제한된 컴퓨팅 자원에서도 RL 훈련을 가능하게 합니다.

**REward Increment = Nonnegative Factor x Offset Reinforcement x Characteristic Eligibility (REINFORCE) [1]**

REINFORCE는 오버헤드가 낮고 이해하기 쉬우며 LLM 훈련에 효과적인 경향이 있는 VPG의 특정 구현입니다. REINFORCE가 사용하는 정책 경사의 구조는 이전에 다룬 기준선화된 정책 경사 추정치와 유사합니다. 그러나 REINFORCE는 특히 RL 훈련 동안 관찰된 보상의 평균을 기준선으로 사용합니다. 이 평균은 몇 가지 다른 방식으로 계산될 수 있습니다. 예를 들어, 훈련 전반에 걸친 보상의 이동 평균(moving average) 또는 현재 배치(batch)에 있는 보상의 평균입니다. REINFORCE의 정책 경사 표현은 위에 표시되어 있습니다.

배치에 대한 경사 업데이트를 계산하기 위해, 우리는 다음 단계를 수행합니다.

1.  현재 정책을 사용하여 각 프롬프트에 대한 완성을 생성합니다.
2.  각 완성의 토큰에 대한 로그 확률을 저장합니다.
3.  각 완성에 보상을 할당합니다(일반적으로 보상 모델을 사용).
4.  보상의 평균을 취하여 기준선을 얻습니다.
5.  보상에서 기준선을 빼서 이점을 계산합니다.
6.  각 완성에 대한 로그 확률과 이점을 곱한 합계를 계산한 다음, 배치에 대해 평균을 내어 몬테카를로 추정치(Monte Carlo estimate)를 형성합니다.

예를 들어, 특정 배치에서 여러 완성물을 생성하고 각각에 대한 보상을 얻었다고 가정해봅시다. REINFORCE는 이 보상들의 평균을 계산하여 기준선으로 사용합니다. 그리고 각 완성물에 대한 보상에서 이 평균 기준선을 빼어 해당 완성물의 이점(advantage)을 구합니다. 이 이점이 양수이면 그 완성물은 평균보다 좋았다는 의미이고, 음수이면 평균보다 나빴다는 의미입니다. 모델은 양수의 이점을 가진 완성물을 생성할 확률을 높이고, 음수의 이점을 가진 완성물 생성 확률을 낮추도록 학습합니다.

**약어는 무엇을 의미합니까?**

REINFORCE 약어는 세 가지 핵심 구성 요소로 구성됩니다.

*   보상 증가(Reward Increment).
*   비음수 요인(Non-negative factor).
*   오프셋 강화(Offset reinforcement).
*   특성 적격성(Characteristic eligibility).

첫 번째 구성 요소는 단순히 정책 매개변수에 대한 우리의 업데이트—또는 증가—(즉, 정책 경사)이며, 이는 다른 세 구성 요소의 곱입니다. 이러한 구성 요소들이 정책 경사를 형성하기 위해 결합되는 방식은 아래에 표시되어 있습니다(상단 항). 각 항의 의미를 명확히 하기 위해, 우리는 REINFORCE의 구성 요소를 더 익숙한 정책 경사 표현에 매핑합니다. 보시다시피, 이것들은 우리가 이전에 배웠던 것과 동일한 항들입니다(예: 로그 확률, 보상, 기준선)! 또한, REINFORCE는 학습률(learning rate)—우리가 경사 상승을 수행하고 보상을 최대화하려고 하기 때문에 "비음수 요인"입니다—을 그 표현 안에 포함합니다.

**REINFORCE 구성 요소를 익숙한 정책 경사 표현에 매핑**

"오프셋 강화"라는 용어는 이해하기 쉽습니다. 기준선은 우리 정책 경사 표현에서 보상에서 직접 빼집니다. 다시 말해, 기준선은 보상을 상쇄하는 데 사용되며, 보상은 RL의 강화 신호(reinforcement signal)입니다(즉, 보상이 행동이 좋은지 나쁜지를 결정합니다). 따라서 기준선은 강화 신호에 대한 오프셋입니다.

"특성 적격성"이라는 용어를 풀이하려면 RL 용어에 대한 약간 더 깊은 이해가 필요합니다.

"특성 적격성: 이것은 학습이 토큰별로 어떻게 귀속되는지를 나타냅니다. 이는 일반적인 값일 수도 있고, 매개변수별 값일 수도 있지만, 현대 방정식에서는 종종 정책의 로그 확률입니다." - RLHF 서적

"적격성(Eligibility)"은 RL에서 크레딧 할당 문제(credit assignment problem)—또는 정책이 받은 보상에 어떤 특정 행동이 기여했는지 결정하는 문제—와 관련된 전문 용어입니다. 구체적으로, 적격성은 LLM이 취한 특정 행동이 주어진 보상에 실제로 책임이 있는지 여부를 나타냅니다. 정책 경사 표현에서, 크레딧 할당은 정책 하의 행동의 로그 확률에 의해 처리됩니다. LLM 맥락에서 '특성 적격성'은 모델이 생성한 각 토큰의 로그 확률과 직접적으로 연결됩니다. 특정 토큰이 최종적으로 높은 보상을 받은 완성에 기여했다면, 그 토큰의 로그 확률은 정책 업데이트에서 더 큰 영향을 받게 됩니다.

**KL 발산 통합.** 대부분의 다른 RL 훈련 알고리즘과 마찬가지로, 우리는 REINFORCE에 참조 정책—일반적으로 우리 모델의 이전 SFT 훈련 체크포인트—에 대한 쿨백-라이블러(KL) 발산(Kullback-Leibler (KL) Divergence)을 통합합니다. 우리는 KL 발산을 근사화하는 몇 가지 다른 접근 방식을 가지고 있습니다. 일반적인 접근 방식은 KL 발산을 정책과 참조 정책 간의 로그 확률 차이로 근사화하는 것입니다. 이 근사화를 수행하면, KL 발산은 아래에 표시된 대로 보상에 직접 통합됩니다.

보상에서 KL 페널티(penalty)를 빼는 이 접근 방식은 RL 훈련 알고리즘 또는 구현에 따라 다릅니다. 예를 들어, GRPO는 KL 발산을 보상에 직접 통합하는 대신 손실 함수(loss function)에 통합합니다. RL에 KL 발산을 추가하는 것은 훈련 과정을 정규화(regularize)하고 우리 정책이 참조 정책에서 크게 벗어나지 않도록 보장합니다.

**효율성 및 오버헤드.** PPO와 같은 알고리즘에 비해 REINFORCE는 오버헤드가 감소했습니다. 왜냐하면 이점 추정치를 계산하기 위해 가치(또는 크리틱) 모델을 사용할 필요가 없기 때문입니다. 보상의 평균이 크리틱 대신 사용됩니다. 따라서 훈련 과정에는 네 개가 아닌 세 개의 LLM만 관여합니다(즉, 정책, 참조 정책, 보상 모델). 아래를 참조하세요.

이러한 방식으로 이점을 추정하는 것의 단점은 더 높은 분산입니다. 그러나 보시다시피, REINFORCE의 높은 분산은 LLM 미세 조정 도메인에서 항상 문제가 되는 것은 아닙니다. 이 간단한 알고리즘은 실제로는 상당히 효과적입니다.

**REINFORCE를 사용한 훈련에 관련된 주요 모델**

**전체 완성 모델링.** 위 이미지에서 빠진 마지막 세부 사항이 하나 있습니다. 로그 확률, KL 발산, 보상을 어떻게 집계하여 정책 경사 업데이트를 형성할까요? REINFORCE의 주요 특징 중 하나는 밴딧 공식화를 사용한다는 것입니다. 정책은 완성의 각 토큰이 아닌 전체 완성을 단일 행동으로 간주하여 훈련됩니다.

"[REINFORCE]는 전체 모델 완성을 단일 행동으로 취급하는 반면, 일반 PPO는 각 완성 토큰을 개별 행동으로 취급합니다. 일반적으로 EOS 토큰만이 실제 보상을 받는데, 이는 매우 희소합니다. 일반 PPO는 EOS 토큰에 보상을 귀속시키지만, [REINFORCE]는 해당 EOS 보상을 전체 완성에 귀속시킵니다." - [5]에서 발췌

우리가 배웠듯이, 대부분의 LLM은 결과 보상 설정(outcome reward setting)을 사용하여 훈련됩니다. 이는 LLM이 생성한 최종 `<eos>` 토큰에만 보상이 할당됨을 의미합니다. 그러나 KL 발산은 토큰별로 계산되며, 이전에 언급했듯이 KL 발산은 REINFORCE에서 보상에서 직접 빼집니다. 따라서 우리는 완성의 모든 토큰에 대한 보상이 KL 발산일 뿐이지만, 완성의 최종 토큰은 보상 모델로부터 추가 보상을 받는 설정에 이르게 됩니다. 아래를 참조하세요.

**REINFORCE의 밴딧 공식화**

우리는 시퀀스에 걸쳐 토큰별 KL 발산과 보상을 합산하여 완성 수준(밴딧 공식화) 보상을 생성합니다. 유사하게, 토큰 수준 로그 확률을 합산하여 완성(또는 궤적)의 로그 확률 8을 얻을 수 있습니다. 위에 표시된 바와 같이, 우리는 이러한 완성 수준 구성 요소를 사용하여 이전과 유사하게 정책 경사를 계산할 수 있습니다.

1.  완성 수준 보상에서 기준선(평균 보상)을 뺍니다.
2.  이 차이에 완성 로그 확률을 곱합니다.
3.  최종 정책 경사를 계산하기 위해 역전파(backward pass)를 실행합니다 9.

이 과정은 단일 프롬프트 및 완성 쌍에 대한 정책 경사를 계산하지만, 우리는 일반적으로 이 경사를 완성 배치에 대해 평균화합니다.

REINFORCE의 밴딧 공식화는 LLM의 특성과 결과 감독 보상 체계에 매우 적합합니다. 전체 완성물을 하나의 행동 단위로 보고 보상을 할당함으로써, 모델은 긴 텍스트 생성 과정에서 어떤 부분이 최종 결과에 긍정적으로 기여했는지에 대한 '큰 그림'을 학습할 수 있습니다. 이는 특히 대화, 요약, 스토리 생성과 같이 응답의 전체적인 일관성과 품질이 중요한 작업에서 유리합니다. 이러한 단순성은 구현의 용이성과 훈련 효율성으로 이어져, PPO와 같은 복잡한 대안 없이도 강력한 LLM 정렬을 달성할 수 있게 합니다.

**유사 코드.** 마지막 단계로, 기본 PyTorch 10에서 REINFORCE의 정책 경사 계산을 구현함으로써 이 논의를 더 구체적으로 만들 것입니다. 우리는 기준선이 배치 내 보상의 평균을 취함으로써 계산된다고 가정할 것입니다(즉, 이동 평균을 사용하는 대신), 전체 경사 업데이트가 단일 스크립트 내에서 설명될 수 있도록 말입니다. 아래를 참조하세요.

```python
import torch

# constants
kl_beta = 0.1

# batch of two completions with three tokens each
per_token_logprobs = torch.tensor(
    [
        [-12.3, -8.3, -2.3],
        [-10.0, -7.0, -3.0],
    ],
    requires_grad=True,
)
reference_per_token_logprobs = torch.tensor([
    [-11.3, -8.4, -2.0],
    [-9.5, -7.2, -2.8],
])

# compute KL divergence approximation
kl_div = per_token_logprobs - reference_per_token_logprobs
kl_div = -kl_beta * kl_div

# get reward for each completion (e.g., from reward model)
score_from_rm = torch.tensor([1.0, 0.5])

# reward is attributed to final <eos> token
per_token_reward = kl_div.clone()
per_token_reward[range(per_token_reward.size(0)), -1] += score_from_rm

# compute REINFORCE update over full sequence
entire_completion_reward = per_token_reward.sum(dim=1)
baseline = entire_completion_reward.mean().detach()

# compute advantage
advantage = entire_completion_reward - baseline

# compute loss and gradient update
reinforce_loss = -per_token_logprobs.sum(dim=1) * advantage
reinforce_loss.mean().backward()
```

### REINFORCE Leave One Out (RLOO) [2]

REINFORCE에서는 훈련 동안 프롬프트당 단일 온-정책(on-policy) 완성을 생성하고, 이러한 완성으로부터의 보상을 사용하여 이동 평균 또는 배치 내 보상의 평균을 통해 기준선을 형성합니다. REINFORCE leave-one-out (RLOO) [2]는 다음을 통해 이 접근 방식을 변경합니다.

*   프롬프트당 여러 개의 ($K$) 완성을 샘플링합니다.
*   이러한 여러 완성을 사용하여 각 개별 프롬프트에 대한 보상 평균을 별도로 계산합니다.

프롬프트 $x$에 대한 $K$개의 완성 $\{y_1, y_2, \dots, y_K\}$가 주어졌을 때, RLOO는 아래에 표시된 대로 완성 $y_i$에 대한 기준선을 정의합니다. 이는 단순히 완성 $y_i$ 자체를 제외한 프롬프트 $x$에 대한 모든 완성의 보상에 대한 평균입니다.

우리는 정책 경사가 계산되는 완성의 보상을 "제외"하고, 동일한 프롬프트에 대한 다른 완성의 보상에 대해 평균을 냅니다.

**RLOO를 위한 기준선 계산**

여기에서, 우리는 i) 배치 내 모든 완성에 대해 이 기준선을 계산하고 ii) 완성에 의해 받은 보상에서 기준선을 빼서 RLOO의 이점 추정치를 계산할 수 있습니다. 아래를 참조하세요(첫 번째 방정식). RLOO를 위한 기준선을 효율적으로 계산하기 위해, 우리는 먼저 $K$개의 완성에 대한 고정된 평균 보상을 계산하고, 아래 두 번째 방정식과 같이 이점을 재구성할 수 있습니다. 이 접근 방식은 평균 보상을 한 번만 계산하고, 프롬프트 $x$에 대한 모든 $K$개의 완성에 대해 leave one out 평균을 다시 계산하는 것을 피할 수 있게 합니다.

**RLOO의 이점 추정치**

이 수정된 이점 추정치는 REINFORCE가 사용하는 동일한 정책 경사 표현에 삽입될 수 있습니다. REINFORCE와 유사하게, RLOO는 토큰별 손실이 아닌 완성별 손실을 사용하며, 학습된 가치 모델이 없습니다. 그러나 RLOO가 사용하는 leave one out 기준선은 프롬프트당 여러 샘플을 사용하여 정책 경사 추정치를 도출함으로써 표준 REINFORCE 알고리즘에 비해 분산을 낮춥니다. 단일 샘플 접근 방식과 비교하여, 프롬프트당 여러 샘플을 취하는 것은 훈련 안정성, 속도 및 성능에 이점을 제공합니다.

"데이터 포인트당 하나의 예측을 샘플링하는 일반적인 경우는 데이터 비효율적입니다. 우리는 데이터 포인트당 여러 샘플을 추출함으로써, 분산을 줄이기 위한 REINFORCE 기준선을 자유롭게 얻으면서 훨씬 적은 데이터로 학습할 수 있음을 보여줍니다." - [2]에서 발췌

RLOO는 프롬프트당 여러 개의 완성물을 샘플링함으로써 '데이터 효율성'을 크게 향상시킵니다. 이는 하나의 프롬프트에 대해 다양한 응답을 생성하고, 이들 간의 상대적인 품질을 비교하여 학습에 활용하기 때문입니다. 특히, "leave-one-out" 기준선은 특정 완성물에 대한 이점을 계산할 때, 해당 완성물을 제외한 나머지 완성물들의 평균 보상을 사용합니다. 이는 마치 자기 자신을 제외한 친구들의 평균 점수를 기준으로 자신의 점수를 평가하는 것과 유사합니다. 이 방식은 개별 완성물의 보상 변동성에 덜 민감하게 반응하여, 정책 경사의 분산을 효과적으로 줄여줍니다. 결과적으로, RLOO는 표준 REINFORCE보다 더 안정적이고 빠르게 학습할 수 있으며, 이는 특히 LLM과 같이 복잡하고 보상이 희소한 환경에서 큰 이점입니다.

**실제 사용.** LLM을 위한 RLOO의 대중화 이후, HuggingFace [5]에서 RLOO의 구현 및 실제 성능을 탐구하는 훌륭한 블로그가 발행되었습니다. 이 분석은 요약 작업 [6]—특히 OpenAI의 TL;DR 요약 데이터셋—에서 PPO 기반 RLHF를 올바르게 구현하고 튜닝하는 저자들의 이전 작업을 확장합니다. [5]에서는 [6]의 동일한 SFT 체크포인트와 보상 모델에서 시작하여 Pythia 1B 및 6.9B 모델을 RLOO로 훈련함으로써 이러한 결과가 확장됩니다. 모델은 GPT-4 심사위원과 함께 참조 요약과 출력을 비교하여 평가됩니다. 아래를 참조하세요.

([5]에서)

보시다시피, RLOO는 PPO보다 50-70% 적은 메모리를 사용하고 2-3배 더 빠르게 실행됩니다. 이러한 절감 효과는 모델 크기에 따라 증가합니다. 이러한 효율성 향상 외에도, RLOO는 PPO와 경쟁력 있는 성능을 보이며 DPO와 같은 오프라인 알고리즘보다 지속적으로 우수한 성능을 보입니다. 이러한 결과는 RLOO(및 REINFORCE)의 핵심 가치 제안을 보여줍니다. 이 알고리즘들은 온라인 RL 알고리즘의 성능 이점을 유지하면서 구현이 더 간단하고 실행 비용이 덜 듭니다.

**유사 코드.** RLOO를 구현하기 위해, 우리는 아래에 표시된 대로 원래 REINFORCE 예제를 수정할 수 있습니다. 여기서는 프롬프트당 세 개의 완성(즉, $K = 3$)이 샘플링되고, 우리의 배치는 세 개의 프롬프트로 구성된다고 가정합니다. 더 프로덕션 준비가 된 코드를 위해, REINFORCE와 RLOO는 모두 volcano engine reinforcement learning (verl) 라이브러리 [7] 내에서도 지원됩니다. 여기를 참조하세요.

```python
import torch

# constants
K = 3 # completions per prompt
kl_beta = 0.1

# batch of three prompts with three completions each
per_token_logprobs = torch.tensor(
    [
        # prompt 1
        [
            [-12.3, -8.3, -2.3], # completion 1
            [-10.0, -7.0, -3.0], # completion 2
            [-10.5, -12.2, -9.1], # completion 3
        ],
        # prompt 2
        [
            [-11.0, -10.3, -1.3],
            [-11.1, -11.1, -0.8],
            [-8.2, -11.9, -0.1],
        ],
        # prompt 3
        [
            [-1.8, -2.1, -0.2],
            [-0.7, -3.5, -0.1],
            [-1.0, -2.2, -1.1],
        ],
    ],
    requires_grad=True,
)
reference_per_token_logprobs = torch.tensor([
    [
        [-11.8, -8.4, -2.3],
        [-10.1, -7.2, -3.1],
        [-10.3, -12.9, -9.1],
    ],
    [
        [-11.8, -9.7, -1.3],
        [-12.3, -11.9, -0.2],
        [-8.1, -12.0, -0.5],
    ],
    [
        [-2.7, -2.0, -1.2],
        [-0.7, -3.6, -0.2],
        [-0.7, -1.2, -0.9],
    ],
])

# compute KL divergence approximation
kl_div = per_token_logprobs - reference_per_token_logprobs
kl_div = -kl_beta * kl_div

# reward for each completion (grouped by prompt)
score_from_rm = torch.tensor([
    [1, 2, 3], # rewards for completions to prompt 1
    [2, 3, 4], # rewards for completions to prompt 2
    [3, 4, 5], # rewards for completions to prompt 3
]).float()

# reward attributed to final <eos> token
per_token_reward = kl_div.clone()
per_token_reward[:, :, -1] += score_from_rm

# compute full sequence reward
entire_completion_reward = per_token_reward.sum(dim=-1)

# compute RLOO baseline in vectorized fashion
baseline = (
    entire_completion_reward.sum(dim=-1)[:, None] - entire_completion_reward
) / (K - 1)
baseline = baseline.detach()

# compute advantage and loss
advantage = entire_completion_reward - baseline
rloo_loss = -per_token_logprobs.sum(dim=-1) * advantage
rloo_loss.mean().backward()
```

### 기본으로 돌아가기: LLM에서 인간 피드백 학습을 위한 REINFORCE 스타일 최적화 재검토 [3]

"우리는 PPO 개발로 이어진 동기 부여 원칙의 대부분이 RLHF에서는 실제적인 문제로 덜 작용하며, 성능을 유지하고 심지어 향상시키는 덜 계산 비용이 드는 방법을 옹호합니다." - [3]에서 발췌

PPO가 RLHF의 사실상의 RL 최적화기이지만, [3]의 저자들은 PPO의 원래 동기(즉, 크고 불안정한 정책 업데이트를 피하는 것)가 LLM의 맥락에서는 덜 관련성이 있다고 주장합니다. 대신, 우리는 더 간단한 RL 최적화기—특히 REINFORCE—를 사용하여 성능 저하 없이 계산 및 메모리 비용을 절약할 수 있습니다. 특히, 우리는 기본적인 REINFORCE 알고리즘으로 LLM을 정렬하는 것이 PPO 기반 RLHF뿐만 아니라 DPO 및 RAFT와 같은 다른 알고리즘의 결과와 일치하거나 이를 능가하는 결과를 달성할 수 있음을 배웁니다. 이 논문은 LLM을 위한 더 간단한 RL 최적화기 사용을 대중화하는 데 중요한 기여를 했습니다.

**LLM 대 DeepRL.** [3]의 주장의 핵심은 LLM 미세 조정이 PPO와 같은 알고리즘이 제안된 전통적인 DeepRL 설정과는 상당히 다른 RL을 위한 고유한 설정이라는 사실에 있습니다. 이 두 설정 간의 가장 주목할 만한 차이점은 LLM이 RL로 처음부터 훈련되지 않는다는 것입니다. 오히려, 우리는 이미 광범위한 사전 훈련을 거친 LLM을 미세 조정하고 있습니다. 이 차이는 두 가지 주요 함의를 가집니다.

*   LLM 미세 조정에서는 전통적인 DeepRL 설정에 비해 치명적으로 큰 분산을 가진 정책 업데이트의 위험이 더 낮습니다.
*   LLM 미세 조정 설정은 전통적인 DeepRL 설정에 비해 학습 과정을 정규화할 필요성이 덜합니다.

우리는 PPO의 설정을 조정함으로써 이 가설을 구체적으로 테스트할 수 있습니다. 즉, PPO의 대부분의 구현은 일반화된 이점 추정(GAE) [4]을 사용하여 이점 함수를 추정합니다. GAE의 세부 사항은 이 게시물의 범위를 벗어납니다. 그러나 GAE는 이점 추정치에서 편향(bias)과 분산(variance) 간의 균형을 제어하는 데 사용될 수 있는 $\lambda \in [0.0, 1.0]$ 하이퍼파라미터를 포함합니다.

([3]에서)

$\lambda$를 낮추면 편향 증가의 대가로 분산이 감소하지만, 이는 정책 업데이트에서 과도한 분산을 가진 DeepRL과 같은 도메인에서는 가치 있는 절충안입니다. 위에 표시된 바와 같이, LLM 정렬에서 최적의 성능은 $\lambda = 1.0$ 설정으로 달성되며, 이는 정책 경사에서 가능한 최대 분산을 유도합니다. 이러한 발견은 LLM 정렬에서 관찰되는 정책 업데이트의 분산 수준이 LLM의 학습 과정에 해롭지 않다는 것을 나타냅니다.

"우리 최적화 체제에서 큰 오프-정책(off-policy) 업데이트는 드물며, 전통적인 DeepRL에서처럼 학습에 치명적인 영향을 미치지 않습니다." - [3]에서 발췌

GAE(Generalized Advantage Estimation)는 이점 함수 추정의 편향-분산 트레이드오프를 조절하는 강력한 도구입니다. $\lambda$ 파라미터는 몬테카를로 추정(높은 분산, 낮은 편향)과 시간차(TD) 추정(낮은 분산, 높은 편향) 사이를 보간합니다. 전통적인 DeepRL 환경에서는 $\lambda < 1.0$을 사용하여 분산을 줄이는 것이 일반적이었지만, LLM 맥락에서는 $\lambda=1.0$ (순수 몬테카를로 추정에 해당)이 최적의 성능을 보인다는 것은 LLM의 정책 업데이트가 본질적으로 덜 불안정하며, 불필요한 분산 감소 기법이 오히려 성능을 저해할 수 있음을 시사합니다. 이는 사전 학습된 LLM이 이미 안정적인 언어 모델링 능력을 갖추고 있기 때문에 정책이 급격하게 변할 위험이 적다는 점과 일맥상통합니다.

**효과적인 행동 공간.** 높은 분산 외에도, RL 훈련의 한 가지 복잡한 요인은 큰 행동 공간의 존재입니다. 정책이 취할 수 있는 가능한 행동이 많고 이러한 행동으로부터의 보상이 노이즈가 많다면, 고품질 정책을 학습하기는 어렵습니다. 이론적으로, LLM의 행동 공간은 매우 큽니다. 이는 LLM이 주어진 프롬프트에 대해 생성할 수 있는 모든 완성을 포함합니다.

([3]에서)

그러나 실제적으로 말하면, LLM의 효과적인 행동 공간—모델이 생성할 가능성이 있는 완성 집합—은 실제로는 상당히 작습니다. LLM이 생성을 수행할 때, 이 과정은 LLM에 제공된 프롬프트에 의해 조건화되며, 이는 [3]에서 강력한 조건화(conditioning)로 나타납니다.

([3]에서)

더 구체적으로, 위 그림에서 우리는 LLM 완성의 확률 질량(probability mass)이 생성 과정의 첫 단계(즉, 출력되는 첫 번째 토큰) 이후 소수의 토큰에 고도로 집중되어 있음을 봅니다. 이러한 관찰은 LLM의 프롬프트가 생성 과정에 강력한 조건화를 제공하여, 모델의 효과적인 행동 공간을 상당히 작게 만든다는 것을 보여줍니다. LLM의 "효과적인 행동 공간"이 작다는 것은 RL 훈련에 있어 중요한 이점입니다. 이는 모델이 실제로 탐색해야 할 유의미한 응답의 범위가 제한적이라는 것을 의미합니다. 따라서 복잡한 탐색 전략이나 정교한 정책 제어가 덜 필요하게 됩니다. 대신, 모델은 이미 사전 학습된 지식을 바탕으로 프롬프트에 의해 강력하게 제약된 공간 내에서 최적의 응답을 찾아가는 데 집중할 수 있습니다. 이러한 특성은 REINFORCE 및 RLOO와 같은 간단한 알고리즘이 PPO에 필적하거나 능가하는 성능을 보이는 핵심 이유 중 하나입니다.

**PPO에서 REINFORCE로.** LLM에게 분산이 덜 중요한 문제라는 점을 고려하여, [3]의 저자들은 PPO 대신 훨씬 더 간단한 REINFORCE 및 RLOO 알고리즘을 RL 최적화기로 사용하는 RLHF 실험을 수행합니다. REINFORCE와 RLOO는 PPO에서 사용되는 RL 공식화에 상당한 변화를 줍니다. 즉, PPO는 토큰별 MDP 공식화를 사용하는 반면, REINFORCE와 RLOO는 모두 밴딧 공식화를 채택합니다. 전체 완성이 단일 행동으로 모델링됩니다.

"우리는 보상이 전체 생성에만 귀속되는 이 설정에서 부분 시퀀스 모델링이 불필요하다는 것을 보여줍니다… 초기 상태가 프롬프트에 의해 결정되는 단일 행동으로 전체 생성을 모델링하는 것이 더 적절하고 효율적입니다." - [3]에서 발췌

MDP 공식화보다 더 간단할 뿐만 아니라, 전체 생성을 단일 행동으로 모델링하는 것은 LLM의 성능을 유지하고 심지어 학습 속도를 높여줍니다. 이는 결과 보상 설정에서 각 토큰을 그 자체의 행동으로 공식화하는 것이 불필요한 복잡성임을 나타냅니다.

**실험 설정.** [3]의 실험은 Pythia-6.9b 및 Llama-7b 모델을 사용하여 TL;DR 요약 및 Anthropic HH 데이터셋에서 수행됩니다. 보상 모델과 정책 모두 각 데이터셋에 대한 고품질 완성의 선별된 데이터셋에서 SFT를 실행하여 얻은 모델 체크포인트를 사용하여 초기화됩니다. RL 동안, 훈련 프롬프트는 SFT 데이터셋에서 샘플링됩니다. 평가를 위해, 저자들은 RL 훈련에 사용된 고정 보상 모델로부터의 각 모델의 평균 보상을 홀드아웃 테스트 세트에서 보고하며, AlpacaFarm 프레임워크를 사용하여 GPT-4에 대한 승률(win-rates)도 보고합니다(즉, 채팅 스타일 프롬프트에 대한 개방형 평가).

([3]에서)

**REINFORCE는 효과적인가?** 위에 표시된 바와 같이, REINFORCE와 RLOO는 학습된 크리틱 모델의 부재로 인해 메모리 집약도가 낮을 뿐만 아니라 PPO보다 지속적으로 우수한 성능을 보이며, [3]의 RLHF 설정에서 부분 시퀀스 모델링이 불필요하다는 것을 확인시켜 줍니다. RLOO는 또한 RAFT 알고리즘 [9]보다 샘플 효율적(sample efficient)인 것으로 밝혀졌습니다. 훈련 중에 생성된 동일한 수의 온-정책 샘플이 주어졌을 때, RLOO는 더 나은 성능을 달성하는 경향이 있습니다. 아래를 참조하세요.

([4]에서)

이 발견은 [3]에서 테스트된 모든 모델과 데이터에 대해 유효합니다. RLOO의 우수한 샘플 효율성은 모든 샘플—심지어 보상이 낮거나 음수인 샘플까지—이 훈련 동안 사용된다는 점을 고려할 때 직관적으로 이해됩니다. 대조적으로, RAFT는 보상에 따라 샘플을 필터링하고 최고의 보상을 가진 샘플만으로 훈련합니다.

AlpacaFarm에서 시뮬레이션된 승률 측면에서 모델을 평가할 때, 위의 많은 결과는 계속 유효하지만, 우리는 각 기술의 성능을 더 인간이 이해하기 쉬운 방식으로 비교할 수 있습니다. 아래에 표시된 바와 같이, 최고의 성능은 RLOO로 지속적으로 달성되며, REINFORCE와 RLOO 모두 PPO보다 지속적으로 우수한 성능을 보입니다. 특히, RLOO는—프롬프트당 네 개의 온-정책 샘플로—TL;DR 및 HH 데이터셋에서 PPO보다 승률이 각각 10.4% 및 14.5% 절대적으로 증가하여 우수한 성능을 보입니다. Llama를 정렬하는 데 사용될 때, RLOO는 PPO에 비해 32.1%라는 훨씬 더 큰 절대 승률 향상을 보입니다.

([3]에서)

RLOO가 PPO를 능가하는 성능을 보이는 것은 LLM의 RL 훈련에 대한 우리의 이해를 재정의합니다. 이는 LLM의 "효과적인 행동 공간"이 비교적 작고, 사전 학습된 모델이 이미 강력한 언어 능력을 가지고 있기 때문에, 복잡한 분산 감소나 신뢰 영역 메커니즘이 오히려 불필요한 제약을 가하거나 학습 속도를 늦출 수 있음을 시사합니다. RLOO의 단순성은 더 빠르고 메모리 효율적인 훈련을 가능하게 하며, 이는 특히 대규모 LLM을 다룰 때 운영 비용을 크게 절감하는 효과를 가져옵니다.

**향상된 견고성.** [3]의 저자들은 두 가지 영역에서 RAFT에 대한 RLOO의 견고성(robustness)을 연구함으로써 결론을 내립니다.

*   KL 발산에 대한 $\beta$ 항을 증가시키는 것이 성능에 어떤 영향을 미칠까요?
*   보상 추정치에 노이즈를 추가하는 것이 성능에 어떤 영향을 미칠까요?

흥미롭게도, RLOO는 RAFT에 비해 노이즈에 훨씬 더 견고한 것으로 밝혀졌습니다. 아래를 참조하세요. $\beta$를 증가시킬 때, RAFT는 RLOO보다 성능이 떨어지고 참조 정책에 비해 더 큰 KL 발산을 가진 정책을 생성합니다. 또한, RAFT의 성능은 RLOO에 비해 노이즈가 있는 보상 추정치로부터 더 큰 부정적인 영향을 받습니다. 노이즈에 대한 이러한 저하된 견고성은 RAFT가 가장 높은 보상을 가진 완성에 대해서만 훈련하기 때문에 발생하며, 이는 보상 추정치에 대한 어떤 교란도 훈련에 상당한 영향을 미치게 합니다.

([3]에서)

RLOO의 높은 견고성은 실제 환경에서 매우 중요한 강점입니다. 현실 세계의 보상 모델은 항상 완벽하지 않으며, 필연적으로 어느 정도의 노이즈나 편향을 포함할 수 있습니다. RAFT와 같이 '최고의' 샘플에만 의존하는 방식은 이러한 노이즈에 민감하게 반응하여 학습 불안정성을 초래할 수 있습니다. 반면, RLOO는 프롬프트당 여러 샘플을 활용하고 'leave-one-out' 기준선을 통해 보상 신호의 변동성에 덜 취약하게 반응합니다. 이는 보상 모델의 불완전성이나 예측 불가능한 외부 요인에도 불구하고 LLM이 안정적으로 학습하고 성능을 유지할 수 있도록 돕습니다.

### 결론

이제 우리는 기본 용어부터 온라인 RL 알고리즘의 기능적 구현에 이르기까지 LLM을 위한 RL에 대한 기초적인 이해를 갖게 되었습니다. LLM을 위한 RL 훈련에 대한 대부분의 연구는 PPO와 같은 액터-크리틱 알고리즘을 기본 최적화기로 사용합니다. 그러나 이러한 알고리즘은 정책 경사의 분산을 줄이기 위해 복잡성과 오버헤드를 도입합니다. LLM의 맥락에서, 우리는 훨씬 더 간단한 온라인 RL 알고리즘이 사용 가능하다는 것을 배웠습니다! REINFORCE와 RLOO는 RL을 위한 완성 수준 밴딧 설정을 채택하고 다음 중 하나를 사용하여 보상을 정규화합니다.

*   훈련 중 보상의 평균(REINFORCE의 경우), 또는
*   프롬프트에 대한 다른 완성의 보상 평균(RLOO의 경우).

이러한 방식으로 가치 함수를 추정하기 때문에, REINFORCE나 RLOO 모두 학습된 크리틱을 필요로 하지 않으며, 이는 메모리 오버헤드를 줄이고 훈련 과정을 가속화합니다. PPO와 같은 알고리즘의 복잡성을 피하고 싶다면, 이러한 더 간단한 온라인 RL 알고리즘은 완전히 오프라인이거나 RL-free인 접근 방식에 즉시 의존하는 대신 효과적인 대안을 제공합니다.

### 미래 전망 및 하이브리드 접근 방식

REINFORCE와 RLOO의 성공은 LLM RL 연구의 방향에 중요한 시사점을 던집니다. 복잡한 알고리즘이 항상 더 나은 결과를 보장하지 않으며, 모델과 환경의 특성을 고려한 단순화된 접근 방식이 더 효율적이고 효과적일 수 있다는 점을 보여주었습니다. 이러한 통찰력은 앞으로 다음과 같은 연구 방향으로 이어질 수 있습니다.

*   **하이브리드 RL 접근 방식**: RLOO와 같은 효율적인 온라인 RL을 초기 정렬 단계에 사용하고, 이후 특정 미세 조정이 필요한 경우 PPO와 같은 더 정교한 알고리즘을 제한적으로 적용하는 하이브리드 전략이 연구될 수 있습니다. 이는 효율성과 정교함 사이의 균형점을 찾는 데 도움을 줄 것입니다.
*   **다중 목표 최적화**: LLM은 단순히 하나의 보상을 최대화하는 것을 넘어, 안전성, 진실성, 유용성 등 여러 목표를 동시에 만족해야 합니다. 간단한 RL 알고리즘에 이러한 다중 목표를 효과적으로 통합하는 방법이 중요해질 것입니다.
*   **온라인 학습 및 적응**: 실시간으로 사용자 피드백을 받아 정책을 지속적으로 업데이트하는 온라인 적응형 RL 시스템에 대한 관심이 증가하고 있습니다. REINFORCE 스타일의 알고리즘은 낮은 오버헤드 덕분에 이러한 시나리오에 더 적합할 수 있습니다.
*   **자원 효율적인 RL**: 더 작거나 자원 제약이 있는 LLM 모델에서도 효과적인 RL 훈련을 가능하게 하는 연구가 중요해질 것입니다. REINFORCE 및 RLOO는 이러한 분야에서 핵심적인 역할을 할 수 있습니다.

결론적으로, REINFORCE 및 RLOO와 같은 간단한 온라인 RL 알고리즘은 LLM 정렬의 복잡성을 줄이고 효율성을 높이는 강력한 대안임을 입증했습니다. 이는 LLM RL 연구가 '더 복잡한 것이 항상 더 좋다'는 고정관념에서 벗어나, LLM의 고유한 특성을 활용하여 더 실용적이고 확장 가능한 솔루션을 모색하는 방향으로 나아가고 있음을 보여줍니다.

뉴스레터가 처음이신가요? 안녕하세요! 저는 Cameron R. Wolfe입니다. 넷플릭스의 딥러닝 박사이자 선임 연구 과학자입니다. 이곳은 독자들이 AI 연구의 중요한 주제를 더 잘 이해하도록 돕는 Deep (Learning) Focus 뉴스레터입니다. 이 뉴스레터는 항상 무료이며 공개적으로 읽을 수 있습니다. 뉴스레터가 마음에 드신다면, 구독하시거나 유료 구독을 고려하시거나 공유하시거나 X 및 LinkedIn에서 저를 팔로우해주세요! 구독

### 참고문헌

[1] Williams, Ronald J. “Simple statistical gradient-following algorithms for connectionist reinforcement learning.” Machine learning 8.3 (1992): 229-256.
[2] Kool, Wouter, Herke van Hoof, and Max Welling. “Buy 4 reinforce samples, get a baseline for free!.” (2019).
[3] Ahmadian, Arash, et al. “Back to basics: Revisiting reinforce style optimization for learning from human feedback in llms.” arXiv preprint arXiv:2402.14740 (2024).
[4] Schulman, John, et al. “High-dimensional continuous control using generalized advantage estimation.” arXiv preprint arXiv:1506.02438 (2015).
[5] Costa Huang, Shengyi, et al. “Putting RL back in RLHF” https://huggingface.co/blog/putting_rl_back_in_rlhf_with_rloo (2024).
[6] Huang, Shengyi, et al. “The n+ implementation details of rlhf with ppo: A case study on tl; dr summarization.” arXiv preprint arXiv:2403.17031 (2024).
[7] Sheng, Guangming, et al. “Hybridflow: A flexible and efficient rlhf framework.” Proceedings of the Twentieth European Conference on Computer Systems. 2025.
[8] Lightman, Hunter, et al. “Let’s verify step by step.” The Twelfth International Conference on Learning Representations. 2023.
[9] Dong, Hanze, et al. “Raft: Reward ranked finetuning for generative foundation model alignment.” arXiv preprint arXiv:2304.06767 (2023).

1 다시 말해, 우리 정책의 출력은 단순히 이산적인 행동이 아닙니다. 오히려, 가능한 행동 집합에 대한 확률 분포입니다. 예를 들어, LLM은 잠재적인 다음 토큰 집합에 대한 확률 분포를 출력합니다.
2 또한, 이 반환(return)에는 유한 또는 무한 시간 범위(horizon) 설정이 있을 수 있습니다. 그러나 LLM의 맥락에서는 일반적으로 유한 시간 범위 설정을 가정합니다(즉, LLM이 토큰을 영원히 생성하지는 않습니다).
3 여기서 우리는 함수를 최대화하려고 하기 때문에 경사 상승(gradient ascent)(하강이 아닌)을 사용합니다. 그러나 경사 상승과 하강은 거의 동일합니다. 유일한 차이점은 경사를 우리 모델의 매개변수에서 빼는지(경사 하강에서 함수를 최소화하는 경우) 또는 더하는지(경사 상승에서 함수를 최대화하는 경우)입니다.
4 과정 감독(Process supervision)은 가능하며 대규모 추론 모델(LRM) 연구에서 탐구되었지만, 결과 보상 설정보다 덜 일반적입니다.
5 또한, 정책 경사에 기준선을 추가하는 것은 우리의 경사 추정치를 편향시키지 않습니다. 이 사실은 EGLP 보조정리(lemma)를 사용하여 증명할 수 있으며, 이는 또한 기준선이 상태 $s_t$에만 의존해야 한다고 규정합니다.
6 "헤드(head)"란 LLM의 끝에 추가되어 훈련 가능한 작은 추가 레이어(layer)를 의미합니다.
7 "액터(actor)"는 LLM—또는 행동을 취하는 모델—을 의미하고, "크리틱(critic)"은 가치 모델을 의미합니다. 가치 모델은 각 행동과 관련된 보상을 예측한다는 사실 때문에 크리틱이라고 불립니다(즉, 효과적으로 행동을 비판합니다).
8 이는 언어 모델링의 기본 개념에서 비롯됩니다. 즉, 완성의 모든 토큰에 대한 확률의 곱(또는 로그 확률의 합)을 취하여 전체 완성의 확률을 얻을 수 있습니다.
9 우리 정책 경사 항은 로그 확률의 경사를 포함하지만, 우리 예제에서는 로그 확률(로그 확률의 경사가 아님)에 접근할 수 있습니다. 따라서 최종 정책 경사를 얻기 위해 이러한 로그 확률의 경사를 취해야 합니다—일반적으로 PyTorch에서 `loss.backward()`를 실행하여 말입니다.
10 이 구현과 RLOO의 나중 구현은 이 블로그 게시물의 코드의 수정된 버전일 뿐입니다.