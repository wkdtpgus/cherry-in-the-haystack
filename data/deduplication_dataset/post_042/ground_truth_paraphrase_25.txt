대규모 언어 모델(LLM) 분야에서 강화 학습(RL)의 역할이 점점 더 중요해지고 있습니다. 초창기에는 RLHF(인간 피드백 기반 강화 학습)와 같은 접근법을 활용하여 LLM의 정렬(alignment)을 강화하는 데 주로 사용되었습니다. 최근에는 이를 넘어 강력한 대규모 추론 모델(LRM)을 훈련하는 핵심 기법으로 확장되었습니다. 일반적으로 LLM을 RL로 훈련할 때, PPO(근접 정책 최적화)와 같은 온라인 알고리즘이 표준적으로 활용됩니다. 하지만 이러한 방식은 SFT(지도 미세 조정)나 DPO(직접 선호도 최적화) 같은 다른 대안들과 비교했을 때, 상당한 비용과 복잡성을 내포하고 있습니다. 특히, LLM의 복사본 네 개를 메모리에 유지해야 하는 점은 큰 부담입니다. 온라인 훈련 과정은 조정하기 까다롭고 불안정해질 수 있으며, 최적화해야 할 하이퍼파라미터(hyperparameter) 또한 많습니다. PPO의 복잡성은 온라인 훈련의 안정성을 확보하려는 필요성에서 비롯되었는데, 이 알고리즘은 로봇 제어나 아타리 게임 플레이와 같이 신경망을 처음부터 학습시키는 연구 분야에서 발전한 것입니다. 그러나 LLM을 위한 RL 환경은 매우 다릅니다. 우리는 이미 방대한 사전 학습을 거쳐 강력한 지식을 갖춘 모델을 미세 조정하는 상황입니다.

"PPO는 RLHF의 정석적인 방법론으로 자리 잡았지만, 높은 계산 비용과 까다로운 하이퍼파라미터 튜닝을 요구합니다. 우리는 PPO 개발의 근간이 된 동기 부여 원칙들이 RLHF 맥락에서는 덜 중요하며, 성능을 유지하거나 심지어 개선하면서도 계산 비용이 훨씬 적게 드는 방법들을 지지합니다." - [3] 인용.

많은 개발자와 연구자들은 높은 비용과 복잡성 때문에 LLM 훈련에 온라인 RL을 적용하는 것을 주저합니다. 하지만 이 글을 통해 온라인 RL이 생각보다 어렵지 않다는 것을 알게 될 것입니다! LLM 도메인의 독특한 특성 덕분에, REINFORCE나 RLOO(REINFORCE leave-one-out)와 같은 더 단순한 알고리즘으로도 PPO에 필적하는 성능을 달성할 수 있습니다. 따라서 온라인 RL을 피하고 단순히 RL-free나 오프라인 대안을 선호하는 대신, 불필요한 복잡성 없이 온라인 RL의 장점을 제공하는 알고리즘들을 활용할 수 있습니다.

최근 LLM의 발전은 단순한 텍스트 생성이나 정렬을 넘어, 복잡한 추론, 도구 사용, 그리고 다단계 의사결정을 요구하는 작업으로 확장되고 있습니다. 이러한 고차원적 문제 해결 능력은 RL 기법이 더욱 핵심적인 역할을 할 수 있는 영역입니다. 가령, 특정 API를 호출하거나 외부 데이터베이스를 쿼리하는 것과 같은 도구 사용 시나리오에서, LLM은 일련의 행동을 통해 목표를 달성해야 하며, 이는 전형적인 RL 문제 설정과 일치합니다. 이러한 맥락에서, 효율적이고 안정적인 RL 훈련 방법론은 LLM의 잠재력을 최대한 발휘하는 데 필수적입니다.

AI 연구의 최신 정보를 얻기 위해 Deep (Learning) Focus를 사용하는 50,000명의 다른 사람들과 함께하세요. 구독

### LLM을 위한 RL의 기본

강화 학습(RL)의 핵심 개념부터 시작해 봅시다. 먼저, RL에서 흔히 쓰이는 문제 정의와 전문 용어들을 살펴보고, 이 형식적인 틀이 LLM 영역에 어떻게 적용될 수 있는지 알아보겠습니다. RL의 근본 원칙과 LLM 환경에서의 적용 방식을 이해한 다음, 이 부분에서는 정책 최적화(policy optimization)에 집중하여, RL에서 널리 활용되는 표준 정책 경사(policy gradient) 표현을 유도하고, 이러한 학습 알고리즘의 가장 기초적인 형태를 구체적으로 구현하는 방법을 설명할 것입니다.

### RL을 위한 문제 설정 및 용어

RL 훈련을 진행할 때, 우리는 특정 환경 속에서 여러 동작(action)을 수행하는 에이전트(agent)를 가정합니다.

**RL의 기본 문제 설정**

이 동작들은 정책(policy)에 의해 결정됩니다. 정책은 에이전트의 '두뇌'와 같다고 볼 수 있으며, 일반적으로 매개변수화(parameterized)되어 있습니다(예: LLM 훈련 시 정책은 곧 LLM 자체입니다). 정책은 확정적(deterministic)일 수도, 확률적(stochastic)일 수도 있지만, 본 개요에서는 확률적 정책을 전제로 합니다 1. 우리는 특정 상태 $s_t$에서 행동 $a_t$가 선택될 확률을 $\pi_\theta(a_t | s_t)$로 모델링합니다. 정책에 따라 행동이 이루어지면, 환경의 상태(state)는 환경 내에 정의된 전이 함수(transition function)에 따라 변화합니다. 이 전이 함수는 $P(s_{t+1} | a_t, s_t)$로 표시되지만, LLM의 경우 전이 함수는 직접적인 영향을 덜 미치는 '통과' 방식에 가깝습니다. 즉, $s_t = \{x, a_1, a_2, \dots, a_t\}$로 간주하며, 여기서 $x$는 초기 프롬프트(prompt)입니다. 마지막으로, 에이전트가 도달하는 각 상태는 환경으로부터 보상(reward)을 받는데, 이 보상은 양수, 음수 또는 0일 수 있습니다.

에이전트는 연속적으로 행동하며, 각 동작($a_t$), 보상($r_t$), 상태($s_t$)는 특정 시간 단계(time step) $t$에 연결됩니다. 이러한 시간 단계들을 순서대로 연결하면 하나의 궤적(trajectory)이 형성됩니다.

여기서는 에이전트가 주어진 궤적에서 총 $T$ 단계의 행동을 수행했다고 가정할 때, 확률의 연쇄 법칙(chain rule)을 활용하여 전체 궤적의 발생 확률을 다음과 같이 계산할 수 있습니다.

*   정책 $\pi_\theta(a_t | s_t)$에 의해 주어진 각 행동 $a_t$의 확률.
*   전이 함수 $P(s_{t+1} | a_t, s_t)$에 의해 주어진 각 상태 $s_{t+1}$의 확률.

궤적의 확률에 대한 전체 표현은 아래에 제시됩니다.

**궤적의 확률 계산**

**RL 목표(Objective).** RL을 통해 모델을 훈련할 때, 우리의 궁극적인 목표는 전체 궤적에 걸쳐 얻는 누적 보상(cumulative reward), 즉 $r_t$의 총합을 최대화하는 것입니다. 이 목표에는 몇 가지 변형이 존재하는데, 특히 보상을 할인(discounted)하거나 할인하지 않을 수 있습니다 2. 할인율(discount factor)을 적용하면, 정책이 나중에 받을 보상보다는 즉각적인 보상을 더 선호하도록 유도합니다. 이는 "먼 미래의 보상보다는 현재의 보상이 더 가치 있다"는 개념과 유사합니다. 우리의 목표는 주로 기대 누적 보상(expected cumulative reward)으로 표현되며, 기대값은 가능한 모든 궤적에 대해 계산됩니다. 이 기대값을 풀어서 보면 각 궤적에 대한 보상의 가중 합(weighted sum)이 되며, 여기서 가중치는 해당 궤적의 발생 확률입니다. 이러한 목표는 연속적(continuous) 또는 이산적(discrete) 방식으로 정량화될 수 있습니다.

우리는 훈련 과정에서 이 목표를 최대화하고자 하며, 이는 경사 상승(gradient ascent) 3 방법을 통해 달성됩니다.

이 설정을 바탕으로 남은 중요한 질문은 "이 경사를 어떻게 정확히 계산할 것인가?"입니다. RL 연구의 상당 부분이 바로 이 질문에 답하는 데 집중되어 있으며, 수많은 기법들이 개발되었습니다.

**경사 상승으로 RL 목표 해결**

**상태, 가치, 이점 함수.** RL 목표와 밀접하게 관련하여, 다음 함수들을 정의할 수 있습니다.

*   **가치 함수(Value Function) $V(s)$**: 특정 상태 $s$에서 시작하여 현재 정책 $\pi_\theta$에 따라 행동할 경우 얻게 될 기대 누적 보상.
*   **행동-가치 함수(Action-Value Function) $Q(s, a)$**: 상태 $s$에서 특정 행동 $a$를 취한 후, 정책 $\pi_\theta$에 따라 행동할 경우 얻게 될 기대 누적 보상.
*   **이점 함수(Advantage Function) $A(s, a)$**: 행동-가치 함수와 가치 함수의 차이, 즉 $A(s, a) = Q(s, a) - V(s)$입니다.

직관적으로, 이점 함수는 상태 $s$에서 행동 $a$를 취하는 것이 해당 상태 $s$에서 일반적으로 기대되는 보상보다 얼마나 더 나은지 혹은 나쁜지를 알려줍니다. 만약 행동 $a$로부터 얻는 보상이 기대치보다 높으면 이점은 양수가 되고, 그 반대의 경우 음수가 됩니다. 이점 함수는 RL 연구에서 지극히 중요한 역할을 하며, 정책의 경사를 계산하는 데 활용됩니다.

"RL에서는 때때로 어떤 행동이 절대적으로 얼마나 좋은지 아는 것보다, 다른 행동들에 비해 상대적으로 얼마나 더 나은지 아는 것이 중요합니다. 즉, 우리는 행동의 상대적 이점(relative advantage)을 알고 싶어하며, 이 개념을 이점 함수로 구체화합니다." - Deep RL에서 Spinning up 발췌

### 마르코프 결정 과정(MDP) 대 밴딧 공식화(Bandit Formulation)

**LLM을 위한 RL 용어 매핑**

RL의 기본 원리를 이해했으니, 이제 이 용어들을 LLM 훈련 환경에 어떻게 적용할지 살펴보겠습니다. 다음과 같이 할 수 있습니다.

*   정책은 LLM 자체입니다.
*   초기 상태는 프롬프트입니다.
*   LLM의 생성물, 즉 각 토큰(token) 또는 전체 완성(completion)이 바로 **행동**입니다.
*   현재 상태는 프롬프트와 LLM 출력의 결합입니다.
*   LLM의 완전한 응답은 **궤적**을 이룹니다.

특히, LLM 설정에서는 전이 함수가 명시적으로 존재하지 않는데, 이는 전이 과정이 완전히 결정론적이기 때문입니다. 만약 프롬프트 $x$로 시작하여 LLM이 $t_1, t_2$ 토큰을 순차적으로 생성한다면, 새로운 상태는 단순히 $s_2 = \{x, t_1, t_2\}$가 됩니다. 즉, 주어진 프롬프트 $x$에 대해 LLM이 만들어내는 현재 진행 중인 텍스트가 상태를 구성합니다.

**마르코프 결정 과정(MDP) 공식화.** LLM의 경우, RL이 행동을 모델링하는 방식에 따라 두 가지 주요 형태로 정식화될 수 있습니다. LLM은 다음 토큰 예측(next token prediction)을 통해 출력을 생성하며, 이는 출력 완성의 각 토큰을 순차적으로 만들어내는 과정을 의미합니다. 이 자기회귀(autoregressive)적 생성 방식은 RL 설정에 매우 자연스럽게 매핑됩니다. 우리는 각 토큰을 개별적인 행동으로 간주할 수 있습니다! LLM 출력의 각 토큰을 독립적인 행동으로 모델링하는 접근법을 **마르코프 결정 과정(MDP) 공식화**라고 합니다. MDP는 상태, 행동, 전이 확률(transition probabilities) 및 보상을 포함하는 의사 결정 모델링을 위한 확률론적 틀입니다. 이는 우리가 지금까지 논의한 RL 설정과 정확히 일치합니다!

LLM을 위한 MDP로 RL을 모델링할 때, 초기 상태는 프롬프트이며, 정책은 개별 토큰을 예측하는 행동을 합니다. LLM은 토큰 분포를 예측하는 확률적 정책을 형성하고, 생성 과정에서 이 분포로부터 토큰을 선택함으로써 행동이 취해집니다. 각 토큰 자체가 하나의 행동인 셈입니다. 토큰이 예측되면 현재 상태에 추가되고, LLM은 이를 바탕으로 다음 토큰을 예측합니다. 이는 본질적으로 자기회귀적 다음 토큰 예측 과정입니다! 결국, LLM은 정지 토큰(stop token)(예: `<|end_of_text|>` 또는 `<eos>`)을 예측하여 생성 과정을 완료하고 완전한 궤적을 만들어냅니다.

**밴딧 공식화.** 위 MDP 방식에서는 모든 시간 단계마다 보상이 주어진다고 가정하지만, LLM의 보상 메커니즘은 대개 이와 다릅니다. 대부분의 LLM은 결과 감독(outcome supervision) 4 방식을 사용하여 훈련됩니다. 이는 모델이 완전한 응답을 생성한 후에만(즉, `<eos>` 토큰이 출력된 후에만) 보상이 주어짐을 의미합니다.

**LLM을 위한 결과 감독 대 과정 감독**

결과 감독 환경에서 각 토큰을 독립적인 행동으로 모델링하는 것이 과연 유용한지에 대한 의문이 제기될 수 있습니다. 이 시나리오에서 어떤 단일 행동이 유용했는지 아닌지를 어떻게 판단할 수 있을까요? 대안으로, 우리는 전체 응답을 결과 보상을 받는 단일 행동으로 모델링할 수 있습니다. 이것이 바로 LLM을 활용한 RL 훈련에서 **밴딧 공식화**의 핵심 개념입니다.

이 명칭은 확률론의 문맥적 밴딧(contextual bandit) 개념에서 유래했습니다. 밴딧 설정은 간단합니다. 에이전트가 행동을 선택하고, 보상을 받으면 에피소드(episode)가 종료됩니다. 우리의 완전한 궤적은 단일 행동과 그에 따른 보상으로 구성됩니다! LLM의 경우, 행동은 프롬프트에 대해 생성된 전체 완성(completion)이며, 이에 대한 결과 보상이 주어집니다.

**어떤 공식화를 사용해야 할까요?** LLM의 맥락에서, 우리는 개별 토큰과 프롬프트에 대한 전체 완성의 확률을 모두 계산하는 방법을 이미 알고 있습니다. 따라서 MDP 또는 밴딧 공식화 중 어느 쪽으로든 RL을 모델링할 수 있습니다. 하지만 LLM이 대개 결과 보상만을 받는다는 점을 고려하면, 밴딧 공식화는 그 단순함에도 불구하고 LLM에 매우 적합합니다. 우리가 이후에 살펴보겠지만, REINFORCE와 RLOO는 모두 밴딧 공식화를 채택하는 반면, PPO와 같은 알고리즘은 토큰별 MDP 공식화를 사용합니다. 즉, 두 RL 공식화 모두 실현 가능하며 LLM 훈련에 활용됩니다.

밴딧 공식화는 특히 '희소한 보상(sparse rewards)' 문제에 직면했을 때 매력적입니다. LLM의 경우, 긴 문장이나 단락을 생성하는 과정에서 중간 토큰에 대한 즉각적인 피드백을 얻기 어렵습니다. 오직 최종 결과물에 대해서만 보상이 주어지기 때문에, MDP 방식은 각 토큰 행동의 가치를 정확히 할당하는 '크레딧 할당(credit assignment)' 문제에 직면하게 됩니다. 밴딧 공식화는 이 문제를 전체 시퀀스를 단일 행동으로 묶어 단순화함으로써 효과적으로 회피합니다. 이는 훈련 효율성 면에서 이점을 제공할 수 있습니다.

### LLM을 위한 RL 훈련

지금까지 설명된 용어와 설정을 바탕으로, 이제 RL이 실제로 LLM 훈련에 어떻게 활용되는지 논의할 차례입니다. 현재 LLM에 적용되는 RL 훈련은 크게 두 가지 범주로 나눌 수 있습니다.

*   **인간 피드백 기반 강화 학습(RLHF)**: 인간의 선호도를 학습한 보상 모델(human preference reward model)에서 제공하는 보상을 사용하여 LLM을 RL 방식으로 훈련합니다.
*   **검증 가능한 보상 기반 강화 학습(RLVR)**: 규칙 기반 또는 결정론적 검증자(deterministic verifiers)가 제공하는 보상을 활용하여 LLM을 RL로 훈련합니다.

이 두 RL 훈련 기법은 주로 보상을 생성하는 방식에서 차이를 보이지만, 알고리즘의 다른 세부 사항들은 대부분 유사합니다. 아래에 묘사된 바와 같이, 둘 다 프롬프트 세트에 대한 응답을 생성하고, 이 응답들에 대한 보상을 계산하며, 이 보상을 사용하여 RL 최적화기(optimizer)를 통해 정책(LLM 매개변수)을 업데이트하는 방식으로 작동합니다.

**LLM을 위한 RL의 시각적 묘사**

이 과정의 마지막 단계는 이미 살펴보았듯이 RL 목표에 대한 경사 상승(gradient ascent)입니다. 그러나 실제 RL 훈련 목표는 단순히 누적 보상 최대화를 넘어섭니다. 우리는 보상을 최대화하는 동시에, 현재 정책이 참조 정책(reference policy)—대개 RL 훈련 시작 시점의 LLM 체크포인트(checkpoint)—와 비교하여 KL 발산(KL divergence)을 최소화하도록 노력합니다. 이는 새로운 모델이 참조 모델과 지나치게 멀어지지 않으면서도 보상을 극대화하려는 목적을 가집니다.

**KL 발산을 포함한 RL 훈련 목표**

정책의 매개변수에 대한 이 목표의 경사를 계산하는 것이 RL 이해에 있어 가장 복잡한 부분 중 하나입니다. LLM 맥락에서는 PPO, GRPO, REINFORCE와 같은 정책 경사 알고리즘들을 사용하여 이 경사를 계산합니다. 본 개요는 주로 REINFORCE와 그 변형에 초점을 맞추겠지만, 이 알고리즘들의 작동 방식을 이해하기 위해서는 먼저 가장 단순한 형태의 정책 경사인 바닐라 정책 경사(VPG)를 숙지해야 합니다.

KL 발산의 도입은 LLM 훈련에서 매우 중요합니다. 모델이 보상을 극대화하는 방향으로만 훈련되면, 때로는 예상치 못한, 심지어는 유해한 방식으로 동작이 변질될 수 있습니다. 이를 '정책 붕괴(policy collapse)' 또는 '과도한 최적화(over-optimization)'라고 부르기도 합니다. KL 페널티는 이러한 현상을 방지하고, 모델이 사전 학습된 지식을 너무 급격하게 잊거나 비정상적인 행동을 학습하는 것을 막아주는 정규화(regularization) 역할을 합니다. 이는 모델의 안정성을 유지하고, 생성 품질을 보존하는 데 기여합니다.

### 바닐라 정책 경사(VPG) 도출

여기서는 바닐라 정책 경사(VPG)의 완전한 유도 과정을 다룰 것입니다. 하지만 VPG에 대해 훌륭하게 설명하는 기존 자료들이 많이 있습니다. 더 깊이 학습하고 싶다면 다음 자료들을 참고할 수 있습니다.

*   OpenAI의 정책 최적화 소개 [링크]
*   Nathan Lambert의 RLHF 서적 [링크]
*   Lilian Weng의 정책 최적화 알고리즘 [링크]

또한, 이 뉴스레터의 VPG 및 정책 최적화에 대한 이전 분석은 쉽게 참고할 수 있도록 아래에 링크되어 있습니다. 이 섹션의 내용은 주로 정책 경사에 대한 이 심층 분석에서 발췌될 것입니다.

**정책 경사: RLHF의 기초**
Cameron R. Wolfe, Ph.D. · 2023년 10월 2일
정책 경사에 대한 심층 분석, 신경망 훈련에 어떻게 적용되는지, 그리고 가장 간단한 형태의 도출.
전체 이야기 읽기

**기본 정책 경사.** 정책 최적화의 목표는 RL 목표(여기서는 총 누적 보상)의 경사를 정책의 매개변수에 대해 계산하는 것입니다. 정책 경사를 계산하는 첫 단계로, 아래에 제시된 유도 과정을 따를 수 있습니다.

(출처)

이 유도 과정은 RL 훈련 목표(누적 보상)의 경사에서 시작하여 정책 경사의 기본 형태로 귀결됩니다. 정책 경사에 도달하기 위해 주로 i) 연속 확률 변수에 대한 기대값 정의와 ii) 로그-미분 트릭(log-derivative trick)과 같은 간단한 기법을 활용합니다. 이 유도에서 가장 복잡한 부분은 마지막 단계로, 궤적의 로그 확률(log probability) 경사를 각 행동의 로그 확률 경사의 합으로 변환하는 것입니다. 이 과정은 궤적 확률에 대한 기존 표현을 사용하고, 곱셈을 덧셈으로 바꾸며(로그 확률을 다루기 때문에), 초기 상태 확률과 전이 함수의 정책 매개변수에 대한 경사는 항상 0이라는 점을 활용합니다. 이는 이 구성 요소들이 정책에 의존하지 않기 때문입니다.

(출처)

### 기본 정책 경사 구현.

위에서 유도한 기본적인 정책 경사(policy gradient) 표현은 실제로 구현하기가 상당히 용이합니다. 특히, 이 표현은 우리가 이미 계산 방법을 아는 두 가지 핵심 요소를 포함합니다.

*   보상은 검증자(verifier)나 보상 모델(reward model)로부터 직접 얻어집니다.
*   행동의 로그 확률은 LLM을 통해 계산할 수 있습니다(이는 LLM 출력의 토큰 확률에 해당합니다).

기본적인 정책 경사 계산 과정을 더욱 명확히 하기 위해, PyTorch 유사 코드(pseudocode) 형태의 단계별 구현이 아래에 제시되었습니다. 이 기본적인 정책 경사 구조의 핵심 아이디어는 높은 보상을 얻은 궤적에서 발생한 행동들의 확률을 높이는 것입니다.

"이 경사를 따라 한 단계 나아가면, 각 행동의 로그 확률이 $R(\tau)$에 비례하여 증가합니다. 여기서 $R(\tau)$는 해당 궤적에서 얻은 총 보상입니다." - Deep RL에서 Spinning up 발췌

이러한 형태의 정책 경사는 단순하지만, 실제 적용 사례에서도 여전히 찾아볼 수 있습니다! 예를 들어, Cursor는 최근 온라인 RL 관련 블로그 글에서 이 정확한 표현을 사용했습니다. 다만, 그들의 블로그에 있는 표현은 밴딧 공식화를 전제로 하므로, 행동이 하나뿐이라는 가정 하에 합산 부분이 생략됩니다.

(출처)

### 분산 감소.

현재의 정책 경사 표현은 단순하지만, 몇 가지 중요한 문제점을 안고 있습니다.

*   경사 추정치가 높은 분산(variance)을 보일 수 있습니다.
*   정책 업데이트가 과도하게 크거나 불안정해지는 것을 방지하는 메커니즘이 부족합니다.

대부분의 후속 정책 경사 알고리즘들은 이러한 문제들을 해결하기 위해 노력합니다. 이들은 정책 경사의 분산을 줄이고, 정책 업데이트에 신뢰 영역(trust region)을 강제함으로써, 즉 한 번의 업데이트에서 모델이 얼마나 크게 변화할 수 있는지를 제한합니다. 이를 위해, 우리는 일반적으로 정책 경사의 보상 항을 다른 항으로 대체합니다. 몇 가지 일반적인 대안은 아래를 참조하세요.

([4]에서)

이 표현은 이전에 보았던 것과 거의 동일합니다. 유일한 차이점은 $R(\tau)$ 대신 일반적인 $\Psi_t$ 항을 사용한다는 점이며, 이는 다양한 값으로 설정될 수 있습니다. 예를 들어, 우리는 다음을 수행할 수 있습니다.

*   $\Psi_t = R(\tau)$로 설정하여 기본 정책 경사 표현을 복원합니다.
*   $\Psi_t$를 시간 $t$ 이후에 받은 보상(즉, reward-to-go 정책 경사)으로 설정하여, 행동 이전에 발생한 보상에 대한 크레딧 할당을 방지합니다.
*   $\Psi_t$를 보상의 기준선화된(baselined) 형태로 설정합니다.
*   $\Psi_t$를 상태-행동($Q$) 함수 또는 이점 함수($A$)와 동일하게 설정합니다.

이러한 선택 사항들과 그 유도 방법에 대한 자세한 개요는 여기에서 찾아볼 수 있습니다. 이 알고리즘들의 공통된 특징은 기준선(baseline) 또는 보상에서 빼는 추가 항(아래에 표시된 대로 보상에서 빼는, 상태 $s_t$에만 의존해야 하는 항)을 사용하는 것입니다.

기준선은 상태에 대한 보상(또는 가치)을 정규화하는 역할을 하며, 정책 경사의 분산을 효과적으로 줄이는 것으로 입증되었습니다 5.

**정책 경사의 보상에 기준선 추가**

바닐라 정책 경사 알고리즘의 흔한 문제는 경사 업데이트의 높은 분산입니다. 이를 완화하기 위해 '기준선'이라는 다양한 기법들이 가치 추정(value estimation)을 정규화하는 데 사용됩니다. 기준선은 여러 방식으로 이를 달성하는데, 이는 다운스트림 행동(downstream action)에 대한 상태의 가치로 효과적으로 정규화하는 역할을 합니다(예: Q 값과 가치의 차이인 이점(Advantage)의 경우). 가장 간단한 기준선은 보상 배치(batch)의 평균 또는 이동 평균(moving average)입니다. - RLHF 서적

우리가 살펴볼 대부분의 알고리즘은 $\Psi_t$를 이점 함수와 같게 설정하는 데 중점을 둡니다. 이는 **바닐라 정책 경사(VPG) 알고리즘**으로 알려져 있습니다. 이점 함수는 가장 낮은 분산의 정책 경사를 생성하기 때문에 널리 채택됩니다.

**바닐라 정책 경사**

**액터-크리틱(Actor-critic).** 이점 함수가 상태-행동 가치 함수와 가치 함수의 차이라는 점을 기억해야 합니다. 즉, VPG 알고리즘은 정책 경사 계산에서 가치 함수를 효과적인 기준선으로 활용합니다. 가치 함수는 온-정책(on-policy)이며, 이는 현재 훈련 반복(iteration)에서 정책의 정확한 매개변수에 의존한다는 의미입니다. 일반적으로 우리는 가치 함수를 신경망으로 추정합니다. LLM의 경우, 가치 함수는 LLM의 가중치(weights)로 초기화되고 가치 함수를 예측하도록 훈련된 별도의 가치 헤드(value head) 6(또는 모델)로 근사화됩니다. 가치 함수를 추정하는 데 사용되는 모델을 가치 모델(value model) 또는 크리틱(critic)이라고 부릅니다. 크리틱은 시퀀스 내 모든 토큰에 대해 가치 함수—즉, 주어진 토큰 또는 상태에서 시작하는 기대 보상—를 예측합니다. RL 훈련 동안, 크리틱은 각 정책 업데이트마다 LLM과 함께 활발히 업데이트됩니다. 이러한 구성을 **액터-크리틱 설정** 7이라고 합니다. RL 훈련 시작 시 고정되는 보상 모델과 달리, 크리틱은 정책의 현재 매개변수에 의존합니다. 따라서 온-정책을 유지하고 예측이 오래되지 않도록 하려면, 크리틱은 LLM 자체와 함께 업데이트되어야 합니다. PPO는 이러한 액터-크리틱 설정을 채택하는 정책 경사 알고리즘의 대표적인 예입니다. 크리틱은 일반적으로 예측된 보상과 실제 보상 간의 평균 제곱 오차(MSE) 손실(loss)을 사용하여 업데이트됩니다. 액터-크리틱 알고리즘의 유사 코드 구현은 아래에 제공됩니다.

이것이 일반적인 설정이긴 하지만, 가치 모델을 사용하는 것은 상당한 비용을 초래할 수 있습니다. 이는 LLM의 전체 복사본 하나를 추가로 메모리에 유지해야 하기 때문입니다! 실제로, 크리틱을 사용하는 것은 PPO가 높은 계산 오버헤드(computational overhead)를 가지는 주요 이유 중 하나입니다. 다음으로는 가치 함수를 추정하기 위한 더 간단하고 효율적인 접근 방식을 채택하는 알고리즘들에 대해 알아볼 것입니다.

```python
import torch
import torch.nn.functional as F

# sample prompt completions and rewards
with torch.no_grad():
    completions = LLM(prompts) # (B*G, L)
    rewards = RM(completions) # (B*G, 1)

# compute value function / critic output
values = CRITIC(completions) # (B*G, L) - per token!
advantage = rewards - values.detach()

# get logprobs for each action
completion_mask = <... mask out padding tokens ...>
llm_out = LLM(completions)
token_logp = F.log_softmax(llm_out, dim=-1)

# loss includes a weighted combination of the policy gradient
# loss and the MSE loss for the critic
loss = (- token_logp * advantage) * completion_mask
loss += _beta * (0.5 * (values - rewards)**2)

# aggregate the loss (many options exist here)
loss = (loss.sum(axis=-1) / completion_mask.sum(axis=-1)).mean()

# gradient update
optimizer.zero_grad()
loss.backward()
optimizer.step()
```

액터-크리틱 프레임워크는 이론적으로 강력하지만, 실제 구현에서는 여러 가지 어려움을 동반합니다. 예를 들어, 크리틱 모델의 정확성은 전체 학습 과정에 큰 영향을 미치며, 크리틱 자체의 훈련이 불안정하거나 수렴이 느릴 경우 액터의 학습 또한 저해될 수 있습니다. 또한, 크리틱의 학습률, 네트워크 구조, 그리고 보상 신호의 스케일링 등 다양한 하이퍼파라미터 튜닝이 필요하여 전체 시스템의 복잡성을 가중시킵니다. 이러한 실용적인 문제점들이 PPO와 같은 알고리즘을 LLM에 적용할 때 직면하는 주요 난관 중 하나입니다.

### LLM을 위한 REINFORCE 및 RLOO

지금까지 우리는 LLM을 위한 정책 최적화와 RL의 핵심 개념들을 살펴보았습니다. 우리가 유도한 기본적인 정책 경사는 계산 자체는 쉽지만, 높은 분산과 불안정한 훈련으로 이어질 수 있습니다. 분산을 줄이기 위해서는 이점 추정치(advantage estimate)를 정책 경사에 효과적으로 통합하는 RL 최적화기가 필요합니다. 그러나 PPO와 같은 널리 사용되는 알고리즘들은 복잡한 액터-크리틱 프레임워크(actor-critic framework)를 사용하여 이를 달성하며, 이는 상당한 오버헤드를 유발합니다. 이러한 추가적인 복잡성을 고려할 때, 우리는 "LLM 훈련 시 온라인 RL 기법을 완전히 회피해야 하는가?"라는 질문을 던질 수 있습니다.

"최근 연구들은 DPO(직접 선호도 최적화)나 LLM 선호도 훈련을 위한 반복적 미세 조정 접근법과 같은 RL-free 방법론을 제안합니다. 하지만 이러한 연구들은 RL 패러다임 내에서 더 간단한 해결책이 존재할 가능성에 대해서는 질문하지 않고 있습니다." - [3] 인용

많은 오프라인 및 RL-free 훈련 대안들이 있지만, LLM 훈련에 활용할 수 있는 간단한 온라인 RL 알고리즘 또한 존재합니다. 이 섹션에서는 REINFORCE와 이 알고리즘의 변형인 RLOO(REINFORCE leave-one-out)에 대해 자세히 알아볼 것입니다. 이 온라인 RL 알고리즘들은 훈련 과정에서 관찰된 보상들의 평균을 사용하여 가치 함수를 추정함으로써, 별도의 크리틱 모델의 필요성을 없앱니다. 이론적으로 이러한 접근 방식은 PPO와 같은 액터-크리틱 알고리즘에 비해 더 높은 분산의 정책 경사를 생성할 수 있습니다. 그러나 최근 연구 [3, 5]에 따르면, 이러한 분산 증가는 LLM 훈련에 부정적인 영향을 미치지 않으며, 온라인 RL 훈련을 위한 사용하기 쉽고 높은 성능을 제공하는 옵션이 될 수 있음을 밝혀냈습니다.

**REward Increment = Nonnegative Factor x Offset Reinforcement x Characteristic Eligibility (REINFORCE) [1]**

REINFORCE는 낮은 오버헤드, 쉬운 이해도, 그리고 LLM 훈련에서의 효과성으로 특징지어지는 VPG의 특정 구현입니다. REINFORCE가 사용하는 정책 경사 구조는 이전에 다루었던 기준선화된 정책 경사 추정치와 유사합니다. 그러나 REINFORCE는 특히 RL 훈련 중 관찰된 보상들의 평균을 기준선으로 활용합니다. 이 평균은 여러 방식으로 계산될 수 있는데, 예를 들어 훈련 전반에 걸친 보상의 이동 평균(moving average)이나 현재 배치(batch) 내 보상들의 평균 등이 있습니다. REINFORCE의 정책 경사 표현은 위에 제시되어 있습니다.

배치에 대한 경사 업데이트를 계산하려면 다음 단계를 수행합니다.

1.  현재 정책을 사용하여 각 프롬프트에 대한 완성을 생성합니다.
2.  각 완성의 토큰에 대한 로그 확률을 기록합니다.
3.  각 완성에 보상을 할당합니다(일반적으로 보상 모델을 통해).
4.  보상들의 평균을 계산하여 기준선을 얻습니다.
5.  보상에서 기준선을 차감하여 이점(advantage)을 계산합니다.
6.  각 완성에 대한 로그 확률과 이점을 곱한 값을 합산한 후, 배치에 대해 평균을 내어 몬테카를로 추정치(Monte Carlo estimate)를 형성합니다.

**약어는 무엇을 의미합니까?**

REINFORCE 약어는 세 가지 핵심 요소로 구성됩니다.

*   보상 증가(Reward Increment).
*   비음수 요인(Non-negative factor).
*   오프셋 강화(Offset reinforcement).
*   특성 적격성(Characteristic eligibility).

첫 번째 구성 요소는 단순히 정책 매개변수에 대한 업데이트—즉, 정책 경사—를 의미하며, 이는 나머지 세 구성 요소의 곱입니다. 이 구성 요소들이 정책 경사를 형성하기 위해 결합되는 방식은 아래에 표시되어 있습니다(상단 항). 각 항의 의미를 명확히 하기 위해, REINFORCE의 구성 요소를 더 익숙한 정책 경사 표현에 매핑해 보았습니다. 보시다시피, 이들은 우리가 이전에 학습했던 항들과 동일합니다(예: 로그 확률, 보상, 기준선)! 또한, REINFORCE는 학습률(learning rate)—우리가 경사 상승을 수행하고 보상을 최대화하려고 하기 때문에 "비음수 요인"입니다—을 그 표현 안에 포함합니다.

**REINFORCE 구성 요소를 익숙한 정책 경사 표현에 매핑**

"오프셋 강화"라는 용어는 직관적으로 이해할 수 있습니다. 기준선은 정책 경사 표현에서 보상으로부터 직접 차감됩니다. 즉, 기준선은 보상을 상쇄하는 데 사용되며, 보상은 RL에서 강화 신호(reinforcement signal) 역할을 합니다(행동의 좋고 나쁨을 결정). 따라서 기준선은 강화 신호에 대한 일종의 상쇄(offset) 역할을 합니다.

"특성 적격성"이라는 용어를 완전히 이해하려면 RL 용어에 대한 조금 더 깊은 지식이 필요합니다.

"특성 적격성: 이는 학습이 각 토큰에 어떻게 귀속되는지를 나타냅니다. 이는 일반적인 값일 수도 있고, 매개변수별 값일 수도 있지만, 현대 방정식에서는 종종 정책의 로그 확률입니다." - RLHF 서적

"적격성(Eligibility)"은 RL에서 크레딧 할당 문제(credit assignment problem)—즉, 정책이 받은 보상에 특정 행동 중 어떤 것이 기여했는지를 결정하는 문제—와 관련된 전문 용어입니다. 구체적으로, 적격성은 LLM이 취한 특정 행동이 주어진 보상에 실제로 책임이 있는지 여부를 나타냅니다. 정책 경사 표현에서 크레딧 할당은 정책 하의 행동 로그 확률을 통해 처리됩니다.

**KL 발산 통합.** 대부분의 다른 RL 훈련 알고리즘과 마찬가지로, REINFORCE에도 참조 정책(reference policy)—일반적으로 우리 모델의 이전 SFT 훈련 체크포인트—에 대한 쿨백-라이블러(KL) 발산(Kullback-Leibler (KL) Divergence)을 통합합니다. 우리는 KL 발산을 근사화하는 몇 가지 다른 접근 방식을 가지고 있습니다. 일반적인 접근 방식은 KL 발산을 정책과 참조 정책 간의 로그 확률 차이로 근사화하는 것입니다. 이 근사화를 적용하면, KL 발산은 아래에 표시된 대로 보상에 직접 통합됩니다.

보상에서 KL 페널티(penalty)를 차감하는 이 접근법은 RL 훈련 알고리즘이나 구현에 따라 다를 수 있습니다. 예를 들어, GRPO는 KL 발산을 보상에 직접 통합하는 대신 손실 함수(loss function)에 포함시킵니다. RL에 KL 발산을 추가하는 것은 훈련 과정을 정규화(regularize)하고, 우리 정책이 참조 정책에서 크게 벗어나지 않도록 보장하는 역할을 합니다.

**효율성 및 오버헤드.** PPO와 같은 알고리즘과 비교할 때, REINFORCE는 오버헤드가 감소합니다. 이는 이점 추정치를 계산하기 위해 가치(또는 크리틱) 모델을 사용할 필요가 없기 때문입니다. 보상들의 평균이 크리틱을 대신하여 사용됩니다. 따라서 훈련 과정에는 네 개가 아닌 세 개의 LLM만 관여합니다(즉, 정책 모델, 참조 정책 모델, 보상 모델).

**REINFORCE를 사용한 훈련에 관련된 주요 모델**

이러한 방식으로 이점을 추정하는 것의 단점은 더 높은 분산이 발생할 수 있다는 점입니다. 그러나 보시다시피, REINFORCE의 높은 분산이 LLM 미세 조정 도메인에서 항상 문제가 되는 것은 아닙니다. 이 간단한 알고리즘은 실제 환경에서 상당히 효과적입니다.

**전체 완성 모델링.** 위 그림에서 빠진 마지막 세부 사항이 하나 있습니다. 로그 확률, KL 발산, 그리고 보상을 어떻게 종합하여 정책 경사 업데이트를 구성할까요? REINFORCE의 주요 특징 중 하나는 밴딧 공식화를 사용한다는 것입니다. 정책은 완성의 각 토큰이 아닌, 전체 완성을 단일 행동으로 간주하여 훈련됩니다.

"[REINFORCE]는 모델의 전체 완성을 단일 행동으로 취급하는 반면, 일반적인 PPO는 각 완성 토큰을 개별 행동으로 간주합니다. 대개 `<eos>` 토큰만이 실제 보상을 받는데, 이는 매우 희소합니다. 일반 PPO는 `<eos>` 토큰에 보상을 귀속시키지만, [REINFORCE]는 해당 `<eos>` 보상을 전체 완성에 귀속시킵니다." - [5]에서 발췌

우리가 앞서 배웠듯이, 대부분의 LLM은 결과 보상 설정(outcome reward setting)을 사용하여 훈련됩니다. 이는 LLM이 생성한 최종 `<eos>` 토큰에만 보상이 할당됨을 의미합니다. 하지만 KL 발산은 토큰별로 계산되며, 앞서 언급했듯이 REINFORCE에서는 KL 발산이 보상에서 직접 차감됩니다. 따라서 완성의 모든 토큰에 대한 보상이 KL 발산일 뿐이지만, 완성의 최종 토큰은 보상 모델로부터 추가 보상을 받는 설정에 이르게 됩니다.

**REINFORCE의 밴딧 공식화**

우리는 시퀀스에 걸쳐 토큰별 KL 발산과 보상을 합산하여 완성 수준(밴딧 공식화) 보상을 생성합니다. 유사하게, 토큰 수준 로그 확률을 합산하여 완성(또는 궤적)의 로그 확률 8을 얻을 수 있습니다. 위에 표시된 바와 같이, 이러한 완성 수준 구성 요소를 사용하여 이전과 유사하게 정책 경사를 계산할 수 있습니다.

1.  완성 수준 보상에서 기준선(평균 보상)을 뺍니다.
2.  이 차이에 완성 로그 확률을 곱합니다.
3.  최종 정책 경사를 계산하기 위해 역전파(backward pass)를 실행합니다 9.

이 과정은 단일 프롬프트 및 완성 쌍에 대한 정책 경사를 계산하지만, 우리는 일반적으로 이 경사를 완성 배치에 대해 평균화합니다.

**유사 코드.** 마지막 단계로, 기본 PyTorch 10에서 REINFORCE의 정책 경사 계산을 구현하여 이 논의를 더욱 구체화할 것입니다. 기준선은 배치 내 보상의 평균을 취함으로써 계산된다고 가정할 것이며(이동 평균 대신), 전체 경사 업데이트는 단일 스크립트 내에서 설명될 수 있도록 합니다.

```python
import torch

# constants
kl_beta = 0.1

# batch of two completions with three tokens each
per_token_logprobs = torch.tensor(
    [
        [-12.3, -8.3, -2.3],
        [-10.0, -7.0, -3.0],
    ],
    requires_grad=True,
)
reference_per_token_logprobs = torch.tensor([
    [-11.3, -8.4, -2.0],
    [-9.5, -7.2, -2.8],
])

# compute KL divergence approximation
kl_div = per_token_logprobs - reference_per_token_logprobs
kl_div = -kl_beta * kl_div

# get reward for each completion (e.g., from reward model)
score_from_rm = torch.tensor([1.0, 0.5])

# reward is attributed to final <eos> token
per_token_reward = kl_div.clone()
per_token_reward[range(per_token_reward.size(0)), -1] += score_from_rm

# compute REINFORCE update over full sequence
entire_completion_reward = per_token_reward.sum(dim=1)
baseline = entire_completion_reward.mean().detach()

# compute advantage
advantage = entire_completion_reward - baseline

# compute loss and gradient update
reinforce_loss = -per_token_logprobs.sum(dim=1) * advantage
reinforce_loss.mean().backward()
```

REINFORCE는 그 단순함 덕분에 LLM 훈련 파이프라인에 통합하기가 용이하며, 특히 빠른 프로토타이핑이나 자원 제약이 있는 환경에서 유용합니다. PPO와 같은 복잡한 알고리즘은 여러 모델의 동시 관리와 정교한 하이퍼파라미터 튜닝이 필수적이지만, REINFORCE는 이러한 부담을 크게 줄여줍니다. 이는 연구자들이 새로운 아이디어를 신속하게 실험하고 반복하는 데 도움을 주며, 실제 서비스 환경에서도 더 적은 GPU 자원으로 LLM을 개선할 수 있는 가능성을 열어줍니다.

### REINFORCE Leave One Out (RLOO) [2]

REINFORCE는 훈련 과정에서 각 프롬프트에 대해 하나의 온-정책(on-policy) 완성을 생성하고, 이 완성들로부터 얻은 보상을 사용하여 이동 평균이나 배치 내 평균을 통해 기준선을 설정합니다. **REINFORCE leave-one-out (RLOO) [2]**는 이 접근 방식을 다음과 같이 개선합니다.

*   프롬프트당 여러 개의 ($K$) 완성을 샘플링합니다.
*   이 여러 완성들을 활용하여 각 개별 프롬프트에 대한 보상 평균을 별도로 계산합니다.

주어진 프롬프트 $x$에 대해 $K$개의 완성 $\{y_1, y_2, \dots, y_K\}$이 있을 때, RLOO는 아래에 표시된 대로 완성 $y_i$에 대한 기준선을 정의합니다. 이는 단순히 완성 $y_i$ 자체를 제외한, 동일한 프롬프트 $x$에 대한 나머지 모든 완성들의 보상 평균입니다.

우리는 정책 경사를 계산하려는 완성의 보상을 "제외"하고, 같은 프롬프트에 대한 다른 완성들의 보상 평균을 취합니다.

**RLOO를 위한 기준선 계산**

여기서 우리는 i) 배치 내 모든 완성에 대해 이 기준선을 계산하고, ii) 완성에 의해 받은 보상에서 이 기준선을 빼서 RLOO의 이점 추정치를 산출할 수 있습니다. 아래 두 번째 방정식처럼 이점을 재구성할 수 있습니다. 이 접근 방식을 통해 평균 보상을 한 번만 계산하고, 각 프롬프트 $x$의 모든 $K$개 완성에 대해 'leave one out' 평균을 반복적으로 재계산하는 번거로움을 피할 수 있습니다.

**RLOO의 이점 추정치**

이렇게 수정된 이점 추정치는 REINFORCE가 사용하는 것과 동일한 정책 경사 표현식에 삽입될 수 있습니다. REINFORCE와 마찬가지로, RLOO는 토큰별 손실이 아닌 완성별 손실을 사용하며, 학습된 가치 모델을 필요로 하지 않습니다. 그러나 RLOO가 사용하는 'leave one out' 기준선은 프롬프트당 여러 샘플을 활용하여 정책 경사 추정치를 도출함으로써, 표준 REINFORCE 알고리즘에 비해 분산을 낮춥니다. 단일 샘플 방식과 비교했을 때, 프롬프트당 여러 샘플을 취하는 것은 훈련의 안정성, 속도, 그리고 성능 면에서 이점을 제공합니다.

"데이터 포인트당 하나의 예측을 샘플링하는 일반적인 방식은 데이터 효율적이지 않습니다. 우리는 데이터 포인트당 여러 샘플을 추출함으로써, 분산 감소를 위한 REINFORCE 기준선을 추가 비용 없이 얻으면서 훨씬 적은 데이터로 학습할 수 있음을 보여줍니다." - [2]에서 발췌

**실제 사용.** LLM을 위한 RLOO가 주목받은 이후, HuggingFace [5]에서는 RLOO의 구현과 실제 성능을 심층적으로 탐구하는 훌륭한 블로그 게시물이 발행되었습니다. 이 분석은 요약 작업 [6]—특히 OpenAI의 TL;DR 요약 데이터셋—에서 PPO 기반 RLHF를 올바르게 구현하고 튜닝했던 저자들의 이전 연구를 확장합니다. [5]에서는 [6]과 동일한 SFT 체크포인트와 보상 모델을 기반으로 Pythia 1B 및 6.9B 모델을 RLOO로 훈련함으로써 이러한 결과를 더욱 발전시킵니다. 모델 평가는 GPT-4 심사위원과 함께 참조 요약과 생성된 출력물을 비교하는 방식으로 이루어졌습니다.

([5]에서)

보시다시피, RLOO는 PPO보다 50-70% 적은 메모리를 사용하고 2-3배 더 빠르게 실행됩니다. 이러한 효율성 향상은 모델 크기에 비례하여 증가합니다. 이러한 효율성 외에도, RLOO는 PPO와 경쟁력 있는 성능을 보여주며, DPO와 같은 오프라인 알고리즘보다 지속적으로 우수한 성능을 달성합니다. 이러한 결과는 RLOO(및 REINFORCE)의 핵심 가치를 입증합니다. 이 알고리즘들은 구현이 더 단순하고 실행 비용이 적게 들면서도 온라인 RL 알고리즘의 성능 이점을 유지합니다.

**유사 코드.** RLOO를 구현하기 위해, 우리는 아래에 제시된 대로 기존 REINFORCE 예제를 수정할 수 있습니다. 여기서는 프롬프트당 세 개의 완성(즉, $K = 3$)이 샘플링되고, 우리의 배치는 세 개의 프롬프트로 구성된다고 가정합니다. 더 실용적인 코드를 위해서는 REINFORCE와 RLOO 모두 volcano engine reinforcement learning (verl) 라이브러리 [7] 내에서도 지원됩니다. 여기를 참조하세요.

```python
import torch

# constants
K = 3 # completions per prompt
kl_beta = 0.1

# batch of three prompts with three completions each
per_token_logprobs = torch.tensor(
    [
        # prompt 1
        [
            [-12.3, -8.3, -2.3], # completion 1
            [-10.0, -7.0, -3.0], # completion 2
            [-10.5, -12.2, -9.1], # completion 3
        ],
        # prompt 2
        [
            [-11.0, -10.3, -1.3],
            [-11.1, -11.1, -0.8],
            [-8.2, -11.9, -0.1],
        ],
        # prompt 3
        [
            [-1.8, -2.1, -0.2],
            [-0.7, -3.5, -0.1],
            [-1.0, -2.2, -1.1],
        ],
    ],
    requires_grad=True,
)
reference_per_token_logprobs = torch.tensor([
    [
        [-11.8, -8.4, -2.3],
        [-10.1, -7.2, -3.1],
        [-10.3, -12.9, -9.1],
    ],
    [
        [-11.8, -9.7, -1.3],
        [-12.3, -11.9, -0.2],
        [-8.1, -12.0, -0.5],
    ],
    [
        [-2.7, -2.0, -1.2],
        [-0.7, -3.6, -0.2],
        [-0.7, -1.2, -0.9],
    ],
])

# compute KL divergence approximation
kl_div = per_token_logprobs - reference_per_token_logprobs
kl_div = -kl_beta * kl_div

# reward for each completion (grouped by prompt)
score_from_rm = torch.tensor([
    [1, 2, 3], # rewards for completions to prompt 1
    [2, 3, 4], # rewards for completions to prompt 2
    [3, 4, 5], # rewards for completions to prompt 3
]).float()

# reward attributed to final <eos> token
per_token_reward = kl_div.clone()
per_token_reward[:, :, -1] += score_from_rm

# compute full sequence reward
entire_completion_reward = per_token_reward.sum(dim=-1)

# compute RLOO baseline in vectorized fashion
baseline = (
    entire_completion_reward.sum(dim=-1)[:, None] - entire_completion_reward
) / (K - 1)
baseline = baseline.detach()

# compute advantage and loss
advantage = entire_completion_reward - baseline
rloo_loss = -per_token_logprobs.sum(dim=-1) * advantage
rloo_loss.mean().backward()
```

RLOO의 'leave-one-out' 기준선은 통계적 효율성을 극대화하는 영리한 방법입니다. 단일 샘플에만 의존하여 기준선을 추정하는 REINFORCE와 달리, RLOO는 여러 샘플을 활용하여 각 샘플에 대한 기준선을 보다 안정적으로 추정합니다. 이는 특히 보상 신호가 노이즈가 많거나 희소할 때 분산 감소에 큰 이점을 제공합니다. 이러한 분산 감소는 훈련 과정을 더욱 안정화하고, 더 적은 에피소드(episode)로도 효과적인 정책을 학습할 수 있게 하여, 전반적인 데이터 효율성을 향상시킵니다. 결과적으로 RLOO는 PPO와 같은 복잡한 액터-크리틱 방식의 장점을 일부 가져오면서도, 모델 아키텍처의 복잡성이나 메모리 요구 사항은 낮게 유지합니다.

### 기본으로 돌아가기: LLM에서 인간 피드백 학습을 위한 REINFORCE 스타일 최적화 재검토 [3]

"우리는 PPO 개발의 주요 동기가 RLHF 환경에서는 실제적인 문제로 덜 작용하며, 성능을 유지하거나 심지어 향상시키면서도 계산 비용이 훨씬 적게 드는 방법론들을 지지합니다." - [3] 인용

PPO가 RLHF의 사실상 표준 RL 최적화기(optimizer)로 간주되지만, [3]의 저자들은 PPO의 본래 동기(즉, 크고 불안정한 정책 업데이트 방지)가 LLM 맥락에서는 그 중요성이 덜하다고 주장합니다. 대신, 우리는 더 간소화된 RL 최적화기—특히 REINFORCE—를 활용하여 성능 저하 없이 계산 및 메모리 비용을 절감할 수 있습니다. 특히, 기본적인 REINFORCE 알고리즘으로 LLM을 정렬하는 것이 PPO 기반 RLHF뿐만 아니라 DPO 및 RAFT와 같은 다른 알고리즘들과 유사하거나 심지어 능가하는 결과를 달성할 수 있음을 알 수 있습니다. 이 논문은 LLM을 위한 더 간단한 RL 최적화기 사용을 대중화하는 데 중요한 공헌을 했습니다.

**LLM 대 DeepRL.** [3]의 주장의 핵심은 LLM 미세 조정이 PPO와 같은 알고리즘이 처음 제안된 전통적인 DeepRL 설정과는 상당히 다른, RL을 위한 독특한 환경이라는 사실에 있습니다. 이 두 설정 간의 가장 두드러진 차이점은 LLM이 RL로 처음부터 훈련되지 않는다는 것입니다. 오히려, 우리는 이미 방대한 사전 훈련을 거쳐 강력한 지식을 갖춘 LLM을 미세 조정하고 있습니다. 이러한 차이는 두 가지 주요 함의를 가집니다.

*   LLM 미세 조정에서는 전통적인 DeepRL 설정에 비해 치명적으로 큰 분산을 가진 정책 업데이트의 위험이 더 낮습니다.
*   LLM 미세 조정 설정은 전통적인 DeepRL 설정에 비해 학습 과정에 대한 정규화의 필요성이 덜합니다.

우리는 PPO의 설정을 조정함으로써 이 가설을 구체적으로 검증할 수 있습니다. 즉, PPO의 대부분의 구현은 일반화된 이점 추정(GAE) [4]을 사용하여 이점 함수를 추정합니다. GAE의 세부 사항은 본 게시물의 범위를 벗어나지만, GAE는 이점 추정치에서 편향(bias)과 분산(variance) 간의 균형을 제어하는 데 사용될 수 있는 $\lambda \in [0.0, 1.0]$ 하이퍼파라미터를 포함합니다.

([3]에서)

$\lambda$ 값을 낮추면 편향이 증가하는 대신 분산이 감소하지만, 이는 정책 업데이트에서 과도한 분산을 겪는 DeepRL과 같은 도메인에서는 가치 있는 절충안입니다. 위에 제시된 바와 같이, LLM 정렬에서 최적의 성능은 $\lambda = 1.0$ 설정에서 달성되며, 이는 정책 경사에서 가능한 최대 분산을 유도합니다. 이러한 발견은 LLM 정렬에서 관찰되는 정책 업데이트의 분산 수준이 LLM의 학습 과정에 해롭지 않다는 것을 시사합니다.

"우리의 최적화 체제에서는 큰 오프-정책(off-policy) 업데이트가 드물며, 전통적인 DeepRL에서처럼 학습에 치명적인 영향을 미치지 않습니다." - [3]에서 발췌

**효과적인 행동 공간.** 높은 분산 외에도, RL 훈련의 복잡한 요소 중 하나는 거대한 행동 공간(action space)의 존재입니다. 정책이 취할 수 있는 가능한 행동의 수가 많고, 이러한 행동들로부터의 보상 신호가 노이즈가 많다면, 고품질 정책을 학습하기는 어렵습니다. 이론적으로 LLM의 행동 공간은 매우 광대합니다. 이는 LLM이 주어진 프롬프트에 대해 생성할 수 있는 모든 완성(completion)을 포함하기 때문입니다.

([3]에서)

그러나 실제적으로 말하면, LLM의 효과적인 행동 공간—즉, 모델이 실제로 생성할 가능성이 있는 완성들의 집합—은 생각보다 상당히 작습니다. LLM이 텍스트 생성을 수행할 때, 이 과정은 LLM에 입력된 프롬프트에 의해 강력하게 조건화되며, 이는 [3]에서 '강력한 조건화(strong conditioning)'로 언급됩니다.

([3]에서)

더 구체적으로, 위 그림에서 우리는 LLM 완성의 확률 질량(probability mass)이 생성 과정의 첫 단계(즉, 출력되는 첫 번째 토큰) 이후 소수의 토큰에 매우 집중되어 있음을 확인할 수 있습니다. 이러한 관찰은 LLM의 프롬프트가 생성 과정에 강력한 제약을 제공하여, 모델의 효과적인 행동 공간을 상당히 좁게 만든다는 것을 보여줍니다.

**PPO에서 REINFORCE로.** LLM에게 분산이 덜 중요한 문제라는 점을 고려하여, [3]의 저자들은 PPO 대신 훨씬 더 간단한 REINFORCE 및 RLOO 알고리즘을 RL 최적화기로 사용하는 RLHF 실험을 수행합니다. REINFORCE와 RLOO는 PPO에서 사용되는 RL 공식화에 상당한 변화를 가져옵니다. 즉, PPO는 토큰별 MDP 공식화를 사용하는 반면, REINFORCE와 RLOO는 모두 밴딧 공식화를 채택하여 전체 완성을 단일 행동으로 모델링합니다.

"우리는 보상이 전체 생성에만 귀속되는 이 설정에서 부분 시퀀스 모델링이 불필요하다는 것을 보여줍니다... 초기 상태가 프롬프트에 의해 결정되는 단일 행동으로 전체 생성을 모델링하는 것이 더 적절하고 효율적입니다." - [3]에서 발췌

MDP 공식화보다 더 간단할 뿐만 아니라, 전체 생성을 단일 행동으로 모델링하는 것은 LLM의 성능을 유지하고 심지어 학습 속도를 가속화하는 데 기여합니다. 이는 결과 보상 설정에서 각 토큰을 독립적인 행동으로 공식화하는 것이 불필요한 복잡성을 야기할 수 있음을 시사합니다.

**실험 설정.** [3]의 실험은 Pythia-6.9b 및 Llama-7b 모델을 사용하여 TL;DR 요약 및 Anthropic HH 데이터셋에서 수행되었습니다. 보상 모델과 정책 모델 모두 각 데이터셋에 대한 고품질 완성의 선별된 데이터셋에서 SFT를 실행하여 얻은 모델 체크포인트를 사용하여 초기화됩니다. RL 훈련 동안, 훈련 프롬프트는 SFT 데이터셋에서 샘플링됩니다. 평가를 위해, 저자들은 RL 훈련에 사용된 고정 보상 모델로부터의 각 모델의 평균 보상을 홀드아웃 테스트 세트에서 보고하며, AlpacaFarm 프레임워크를 사용하여 GPT-4에 대한 승률(win-rates)도 보고합니다(즉, 채팅 스타일 프롬프트에 대한 개방형 평가).

([3]에서)

**REINFORCE는 효과적인가?** 위에 제시된 바와 같이, REINFORCE와 RLOO는 학습된 크리틱 모델의 부재로 인해 메모리 효율성이 높을 뿐만 아니라 PPO보다 지속적으로 우수한 성능을 보여줍니다. 이는 [3]의 RLHF 설정에서 부분 시퀀스 모델링이 불필요하다는 주장을 뒷받침합니다. RLOO는 또한 RAFT 알고리즘 [9]보다 샘플 효율적(sample efficient)인 것으로 밝혀졌습니다. 훈련 중에 생성된 동일한 수의 온-정책 샘플이 주어졌을 때, RLOO는 더 나은 성능을 달성하는 경향이 있습니다.

([4]에서)

이러한 발견은 [3]에서 테스트된 모든 모델과 데이터셋에 대해 일관되게 나타납니다. RLOO의 뛰어난 샘플 효율성은 모든 샘플—심지어 보상이 낮거나 음수인 샘플까지—이 훈련에 활용된다는 점에서 직관적으로 이해됩니다. 대조적으로, RAFT는 보상에 따라 샘플을 필터링하고 가장 높은 보상을 가진 샘플만으로 훈련합니다.

AlpacaFarm에서 시뮬레이션된 승률 측면에서 모델을 평가할 때, 위의 많은 결과는 여전히 유효하며, 각 기술의 성능을 더 인간이 이해하기 쉬운 방식으로 비교할 수 있습니다. 아래에 표시된 바와 같이, 최고의 성능은 RLOO로 일관되게 달성되며, REINFORCE와 RLOO 모두 PPO보다 지속적으로 우수한 성능을 보입니다. 특히, RLOO는—프롬프트당 네 개의 온-정책 샘플을 사용하여—TL;DR 및 HH 데이터셋에서 PPO보다 승률이 각각 10.4% 및 14.5% 절대적으로 증가하여 뛰어난 성능을 입증합니다. Llama 모델을 정렬하는 데 사용될 때, RLOO는 PPO에 비해 32.1%라는 훨씬 더 큰 절대 승률 향상을 보여줍니다.

([3]에서)

**향상된 견고성.** [3]의 저자들은 두 가지 측면에서 RAFT에 대한 RLOO의 견고성(robustness)을 연구함으로써 결론을 내립니다.

*   KL 발산에 대한 $\beta$ 항을 증가시키는 것이 성능에 어떤 영향을 미칠까요?
*   보상 추정치에 노이즈를 추가하는 것이 성능에 어떤 영향을 미칠까요?

흥미롭게도, RLOO는 RAFT에 비해 노이즈에 훨씬 더 견고한 것으로 밝혀졌습니다. 아래를 참조하세요. $\beta$를 증가시킬 때, RAFT는 RLOO보다 성능이 저하되고 참조 정책에 비해 더 큰 KL 발산을 가진 정책을 생성합니다. 또한, RAFT의 성능은 RLOO에 비해 노이즈가 있는 보상 추정치로부터 더 큰 부정적인 영향을 받습니다. 노이즈에 대한 이러한 저하된 견고성은 RAFT가 가장 높은 보상을 가진 완성에 대해서만 훈련하기 때문에 발생하며, 이는 보상 추정치에 대한 어떤 교란도 훈련에 상당한 영향을 미치게 만듭니다.

([3]에서)

이러한 결과는 LLM 훈련에서 '단순함이 곧 효율성'이라는 강력한 메시지를 전달합니다. 복잡한 알고리즘이 항상 최적의 해답은 아니며, 특정 도메인의 특성을 이해하고 그에 맞는 간결한 접근 방식을 채택하는 것이 때로는 더 나은 결과를 가져올 수 있음을 보여줍니다. 특히 자원 제약이 있는 환경이나 대규모 모델을 다룰 때, REINFORCE 및 RLOO와 같은 알고리즘은 PPO의 대안을 넘어선 강력한 선택지가 될 수 있습니다. 이는 RLHF 연구 방향에도 중요한 시사점을 제공하며, 향후 LLM 정렬 연구가 더 실용적이고 접근 가능한 방향으로 나아갈 수 있음을 의미합니다.

### 결론

이 글을 통해 우리는 LLM을 위한 강화 학습(RL)의 기본적인 용어부터 온라인 RL 알고리즘의 실제 구현 방식까지 포괄적인 이해를 얻게 되었습니다. LLM RL 훈련에 대한 대다수 연구는 PPO와 같은 액터-크리틱 알고리즘을 주된 최적화기로 활용합니다. 그러나 이러한 알고리즘들은 정책 경사의 분산을 줄이기 위해 복잡성과 상당한 계산 오버헤드를 수반합니다. LLM의 특수한 맥락에서는 훨씬 더 단순한 온라인 RL 알고리즘들도 충분히 효과적으로 사용될 수 있음을 확인했습니다! REINFORCE와 RLOO는 RL을 위한 완성 수준(completion-level) 밴딧 설정(bandit setting)을 채택하고, 보상을 정규화하기 위해 다음 중 하나를 사용합니다.

*   훈련 과정에서 관찰된 보상들의 평균(REINFORCE의 경우), 또는
*   주어진 프롬프트에 대한 다른 완성물들의 보상 평균(RLOO의 경우).

이러한 방식으로 가치 함수를 추정하기 때문에, REINFORCE나 RLOO 모두 별도로 학습된 크리틱(critic) 모델을 요구하지 않으며, 이는 메모리 오버헤드를 현저히 줄이고 훈련 과정을 가속화하는 이점을 제공합니다. PPO와 같은 알고리즘의 내재된 복잡성을 피하고자 한다면, 이러한 간소화된 온라인 RL 알고리즘들은 단순히 오프라인이나 RL-free 접근 방식에 즉시 의존하는 대신, 매우 효과적인 대안을 제시합니다.

LLM의 규모가 커지고 다양한 응용 분야에서 활용되면서, 훈련 효율성과 자원 최적화는 더욱 중요해지고 있습니다. REINFORCE 및 RLOO는 이러한 요구사항을 충족시키면서도, PPO가 제공하는 성능적 이점을 상당 부분 유지하거나 심지어 능가할 수 있음을 보여주었습니다. 이는 LLM 개발자들이 더 넓은 범위의 도구와 전략을 활용하여 모델을 정렬하고 최적화할 수 있음을 의미합니다. 특히, 연구실 환경을 넘어 실제 제품 및 서비스에 LLM을 적용하려는 기업들에게는 이러한 효율적인 온라인 RL 기법들이 매력적인 선택지가 될 것입니다. 궁극적으로, LLM의 잠재력을 최대한 발휘하기 위해서는 이론적 우아함뿐만 아니라 실용적인 적용 가능성과 효율성을 겸비한 방법론들이 필수적입니다.

뉴스레터가 처음이신가요? 안녕하세요! 저는 Cameron R. Wolfe입니다. 넷플릭스의 딥러닝 박사이자 선임 연구 과학자입니다. 이곳은 독자들이 AI 연구의 중요한 주제를 더 잘 이해하도록 돕는 Deep (Learning) Focus 뉴스레터입니다. 이 뉴스레터는 항상 무료이며 공개적으로 읽을 수 있습니다. 뉴스레터가 마음에 드신다면, 구독하시거나 유료 구독을 고려하시거나 공유하시거나 X 및 LinkedIn에서 저를 팔로우해주세요! 구독

### 참고문헌

[1] Williams, Ronald J. “Simple statistical gradient-following algorithms for connectionist reinforcement learning.” Machine learning 8.3 (1992): 229-256.
[2] Kool, Wouter, Herke van Hoof, and Max Welling. “Buy 4 reinforce samples, get a baseline for free!.” (2019).
[3] Ahmadian, Arash, et al. “Back to basics: Revisiting reinforce style optimization for learning from human feedback in llms.” arXiv preprint arXiv:2402.14740 (2024).
[4] Schulman, John, et al. “High-dimensional continuous control using generalized advantage estimation.” arXiv preprint arXiv:1506.02438 (2015).
[5] Costa Huang, Shengyi, et al. “Putting RL back in RLHF” https://huggingface.co/blog/putting_rl_back_in_rlhf_with_rloo (2024).
[6] Huang, Shengyi, et al. “The n+ implementation details of rlhf with ppo: A case study on tl; dr summarization.” arXiv preprint arXiv:2403.17031 (2024).
[7] Sheng, Guangming, et al. “Hybridflow: A flexible and efficient rlhf framework.” Proceedings of the Twentieth European Conference on Computer Systems. 2025.
[8] Lightman, Hunter, et al. “Let’s verify step by step.” The Twelfth International Conference on Learning Representations. 2023.
[9] Dong, Hanze, et al. “Raft: Reward ranked finetuning for generative foundation model alignment.” arXiv preprint arXiv:2304.06767 (2023).

1 다시 말해, 우리 정책의 출력은 단순히 이산적인 행동이 아닙니다. 오히려, 가능한 행동 집합에 대한 확률 분포입니다. 예를 들어, LLM은 잠재적인 다음 토큰 집합에 대한 확률 분포를 출력합니다.
2 또한, 이 반환(return)에는 유한 또는 무한 시간 범위(horizon) 설정이 있을 수 있습니다. 그러나 LLM의 맥락에서는 일반적으로 유한 시간 범위 설정을 가정합니다(즉, LLM이 토큰을 영원히 생성하지는 않습니다).
3 여기서 우리는 함수를 최대화하려고 하기 때문에 경사 상승(gradient ascent)(하강이 아닌)을 사용합니다. 그러나 경사 상승과 하강은 거의 동일합니다. 유일한 차이점은 경사를 우리 모델의 매개변수에서 빼는지(경사 하강에서 함수를 최소화하는 경우) 또는 더하는지(경사 상승에서 함수를 최대화하는 경우)입니다.
4 과정 감독(Process supervision)은 가능하며 대규모 추론 모델(LRM) 연구에서 탐구되었지만, 결과 보상 설정보다 덜 일반적입니다.
5 또한, 정책 경사에 기준선을 추가하는 것은 우리의 경사 추정치를 편향시키지 않습니다. 이 사실은 EGLP 보조정리(lemma)를 사용하여 증명할 수 있으며, 이는 또한 기준선이 상태 $s_t$에만 의존해야 한다고 규정합니다.
6 "헤드(head)"란 LLM의 끝에 추가되어 훈련 가능한 작은 추가 레이어(layer)를 의미합니다.
7 "액터(actor)"는 LLM—또는 행동을 취하는 모델—을 의미하고, "크리틱(critic)"은 가치 모델을 의미합니다. 가치 모델은 각 행동과 관련된 보상을 예측한다는 사실 때문에 크리틱이라고 불립니다(즉, 효과적으로 행동을 비판합니다).
8 이는 언어 모델링의 기본 개념에서 비롯됩니다. 즉, 완성의 모든 토큰에 대한 확률의 곱(또는 로그 확률의 합)을 취하여 전체 완성의 확률을 얻을 수 있습니다.
9 우리 정책 경사 항은 로그 확률의 경사를 포함하지만, 우리 예제에서는 로그 확률(로그 확률의 경사가 아님)에 접근할 수 있습니다. 따라서 최종 정책 경사를 얻기 위해 이러한 로그 확률의 경사를 취해야 합니다—일반적으로 PyTorch에서 `loss.backward()`를 실행하여 말입니다.
10 이 구현과 RLOO의 나중 구현은 이 블로그 게시물의 코드의 수정된 버전일 뿐입니다.