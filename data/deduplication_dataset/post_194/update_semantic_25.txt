인공지능 연구의 최전선: 최신 대규모 언어 모델 동향 분석

최근 몇 년간 거대 언어 모델(LLM) 분야는 눈부신 진보를 거듭하고 있습니다. 새로운 세대의 모델들이 끊임없이 등장함에 따라, 관련 분야의 학자들과 기술자들은 이러한 최신 발전 흐름을 꾸준히 파악하는 것이 필수적입니다. 본문에서는 2025년 10월 마지막 주에 공개된 가장 중요한 LLM 관련 연구 결과 중 일부를 요약하여 제시합니다. 이 연구들은 모델의 최적화 및 확장성, 추론 방법론, 성능 평가, 그리고 전반적인 역량 개선을 포함하여, 차세대 언어 모델의 미래를 그려나가는 다채로운 주제들을 다룹니다. 이처럼 급변하는 LLM 연구의 흐름을 지속적으로 이해하는 것은 더욱 우수하고, 안정적이며, 인간의 가치에 부합하는 인공지능 모델을 구축하는 데 결정적인 기여를 할 것입니다.

이 글에서는 인공지능 기술의 핵심인 대규모 언어 모델(LLM)의 최신 동향을 깊이 있게 다룹니다. 끊임없이 진화하는 LLM 생태계 속에서, 연구자들과 개발자들은 혁신적인 아이디어와 기술 보고서를 통해 모델의 한계를 확장하고 있습니다. 특히, 2025년 10월 말에 발표된 주요 논문들은 효율적인 디코딩 전략, 혁신적인 어텐션 메커니즘, 그리고 자율적인 정보 탐색 에이전트 개발 등 다양한 측면에서 주목할 만한 성과를 보여줍니다. 이러한 기술적 발전은 단순한 성능 향상을 넘어, AI 시스템이 실제 세계에서 더욱 유용하고 신뢰할 수 있는 도구로 자리매김하는 데 중요한 토대가 됩니다. 본 분석을 통해 독자들은 최신 LLM 연구의 핵심적인 발전 방향과 그 잠재적 영향력을 이해할 수 있을 것입니다.

목차:
1. LLM 발전 및 기술 보고서
    1.1. 수동 디코딩의 한계를 넘어: 진정한 종단 간 언어 모델 실현을 향하여
    1.2. Kimi Linear: 표현력과 효율성을 겸비한 어텐션 구조
    1.3. Tongyi DeepResearch 기술 보고서

### 1. LLM 발전 및 기술 보고서

#### 1.1. 수동 디코딩의 한계를 넘어: 진정한 종단 간(End-to-End) 언어 모델 실현을 향하여

이 연구는 대규모 언어 모델(LLM)에 흔히 부여되는 '종단 간(end-to-end)'이라는 수식어에 의문을 제기합니다. 저자들은 온도(temperature)나 top-p와 같이 사람이 직접 값을 조절하는 고정된 디코딩 설정에 의존하는 것이 중요한 제약 요인이라고 주장합니다. 이러한 수동적인 방식은 번거롭고 특정 과제에만 유효할 뿐 아니라, 단일 생성 과정 안에서 각 토큰(token)별로 요구되는 창의성과 정확성의 적절한 균형이 극적으로 변할 수 있기에 근본적으로 최적화될 수 없다는 것입니다. 이 논문은 LLM이 개별 토큰(token)에 대해 스스로 디코딩 방식을 학습하고 제어할 수 있도록 함으로써, 최종적으로 진정한 의미의 종단 간(end-to-end) 시스템을 가능하게 하는 새롭고 경량화된 아키텍처인 **AutoDeco**를 제안합니다.

이 논문은 현대 대규모 언어 모델(LLM)이 '종단 간(end-to-end)' 시스템으로 불리지만, 실제로는 생성 과정의 핵심 단계인 디코딩(decoding)에서 여전히 인간의 개입이 필요하다는 점을 비판적으로 조명합니다. 특히, 온도(temperature)나 top-p와 같은 정적인 디코딩 하이퍼파라미터(hyperparameter)를 사람이 직접 설정해야 하는 점을 주요 병목 현상으로 지적합니다. 이러한 수동적인 조작은 사용하기 불편하고, 특정 목적에 따라 매번 값을 변경해야 하는 한계가 있으며, 무엇보다도 단일 문장 생성 과정에서도 토큰(token)마다 요구되는 발산적 사고(창의성)와 수렴적 사고(정확성)의 이상적인 수준이 시시각각 변하기 때문에 근본적으로 효율적이지 못합니다. 이에 대한 해결책으로, 연구팀은 LLM 스스로가 각 출력 단위(토큰)에 맞춰 최적의 디코딩 방식을 배우고 조절하게 함으로써, 진정한 종단 간(end-to-end) 작동을 구현하는 혁신적이고 가벼운 구조인 **AutoDeco**를 소개합니다.

**핵심 아이디어: 모델에게 무엇을 생성할지 뿐만 아니라 어떻게 생성할지를 가르치기**

핵심적인 난점은 온도(temperature)와 top-p 값에 대한 단일의 고정된 설정이 전체 시퀀스(sequence)에 걸쳐 최적일 수 없다는 점입니다. 모델은 이야기를 시작할 때는 높은 창의성(높은 온도)이 필요할 수 있지만, 최종적이고 사실적인 답변을 제공할 때는 높은 정확성(낮은 온도)이 요구될 수 있습니다. 현재의 LLM은 이러한 맥락을 인지하지 못하고, 동적으로 변화하는 문제에 대해 일률적인 전략을 강요합니다. AutoDeco의 중심 사상은 디코딩 방식을 모델 자체의 학습된 동적인 구성 요소로 만드는 것입니다. 트랜스포머(transformer) 구조에 경량의 "예측 헤드(prediction heads)"를 추가함으로써, 모델은 현재의 문맥(context)을 기반으로 생성하려는 각 특정 토큰(token)에 대한 최적의 온도(temperature)와 top-p 값을 예측하는 방법을 학습합니다. 이는 디코딩을 고정적이고 수동적인 발견적 방법(heuristic)에서, 모델의 순방향 전달(forward pass) 과정에 직접 통합되는 동적이고 자율적으로 조절되는 매개변수적(parametric) 절차로 전환시킵니다.

이러한 접근 방식의 본질은 모델이 단순히 다음 단어를 예측하는 것을 넘어, 그 단어를 어떤 '스타일'이나 '확신도'로 생성할지까지 결정하도록 하는 것입니다. 예를 들어, 시적인 구절을 생성할 때는 높은 무작위성(랜덤성)을 허용하고, 사실적 정보를 제공할 때는 엄격한 정확성을 유지하는 식입니다. 이는 기존의 생성 모델이 지녔던 근본적인 한계를 극복하려는 시도이며, 사용자가 원하는 출력의 특성을 더욱 섬세하게 제어할 수 있게 합니다.

**주요 방법론: 미분 가능한 디코딩 및 종단 간(End-to-End) 학습**

AutoDeco 학습의 핵심적인 난관은 각 단계에서 최적의 온도(temperature)나 top-p에 대한 "정답(ground-truth)" 라벨(label)이 존재하지 않는다는 것입니다. 이 논문은 새롭고 완전히 미분 가능한 학습 파이프라인(pipeline)을 통해 이 문제를 성공적으로 해결합니다.

AutoDeco는 전통적인 트랜스포머(transformer) 모델에 두 개의 간단하고 가벼운 헤드(head)를 추가하여 성능을 향상시킵니다. 이 헤드(head)들은 기존의 언어 모델링 헤드(head)와 나란히 배치됩니다. 생성 단계마다 첫 번째 헤드(head)는 현재 문맥(context)에 맞는 온도(temperature)를 예측하며, 두 번째 헤드(head)는 해당 문맥(context)과 예측된 온도(temperature)를 모두 활용하여 최적의 top-p 값을 추정합니다.

표준 top-p 샘플링(sampling) 방식은 특정 확률 임계값 이하의 토큰(token)을 '잘라내는(hard cutoff)' 방식으로 작동하는데, 이는 미분 불가능하여 기울기(gradient) 흐름을 방해합니다. 이러한 문제를 해결하기 위해 저자들은 "소프트 top-p(soft top-p)" 메커니즘(mechanism)을 도입합니다. 이 방법은 핵(nucleus) 외부 토큰(token)의 확률을 0으로 만드는 대신, 확률을 점진적으로 감소시키는 부드럽고 미분 가능한 마스크(mask)를 적용합니다.

매개변수(parameter) 예측부터 최종 토큰(token) 확률에 이르기까지 완전히 미분 가능한 파이프라인(pipeline)을 통해, AutoDeco 헤드(head)는 텍스트 생성 작업의 표준 교차 엔트로피 손실(cross-entropy loss)을 활용하여 직접 학습될 수 있습니다. 이를 통해 모델은 디코딩 매개변수(parameter) 자체에 대한 명시적인 라벨(label) 없이도 올바른 다음 토큰(token)을 생성하는 데 미치는 영향을 직접 최적화하여 디코딩 전략을 학습할 수 있습니다.

이러한 혁신적인 접근 방식은 모델이 내부적으로 디코딩 결정을 내리는 과정을 외부에서 명시적으로 지도하지 않고도, 전체적인 생성 품질을 최적화하도록 훈련될 수 있음을 의미합니다. 미분 가능한 소프트 top-p는 이 과정에서 핵심적인 역할을 하며, 기존의 이산적인 선택 문제를 연속적인 최적화 문제로 전환하여 딥러닝(deep learning)의 강력한 학습 능력을 활용할 수 있게 합니다. 이는 생성 모델의 제어 가능성과 학습 효율성을 동시에 향상시키는 중요한 진전입니다.

*(BASE 문서에 언급된 다이어그램에 대한 설명)*
제시된 도식은 AutoDeco의 동적인 토큰(token)별 매개변수(parameter) 예측 방식(상단)과 기존 수동 디코딩의 고정된 단일 설정 방식(하단)을 명확하게 비교합니다. 이 그림은 종단 간(end-to-end) 학습을 가능하게 하는 핵심 요소인 새로운 "미분 가능한 소프트 top-p 마스크(differentiable soft top-p mask)"를 시각적으로 보여주므로, 이 방법론을 이해하는 데 매우 중요합니다.

**가장 중요한 발견**

연구 결과는 AutoDeco가 단순히 성능을 향상시키는 것을 넘어, 계산 비용은 미미하면서도 조작 가능하고 상호작용적인 생성의 새로운 패러다임(paradigm)을 열었음을 명확히 보여줍니다.

*   **뛰어난 성능 입증:** 8가지의 다양한 평가 지표(benchmark)와 여러 모델 계열(Llama, Qwen, GPT 변형 포함)에 걸쳐, AutoDeco는 탐욕적 탐색(greedy search)이나 기본 샘플링(default sampling)과 같은 일반적인 디코딩 기준선(baseline)보다 꾸준히 그리고 현저하게 우수한 결과를 보였습니다.
*   **'최적 튜닝(Oracle-Tuned)' 기준선(baseline)과 대등한 수준:** AutoDeco의 성능은 정적 하이퍼파라미터(hyperparameter)가 테스트 데이터에서 광범위한 탐색을 통해 세심하게 조절된 "전문가 지시(expert-guided)" 기준선(baseline)과 동등하거나 때로는 능가하는 수준을 보여주었습니다. 이는 실제 환경에서는 불가능한 과정이므로, 모든 정적 방법론에 대한 실질적인 우위를 증명합니다.
*   **미미한 추가 비용:** AutoDeco에 사용된 헤드(head)는 매우 가벼워서 생성 과정에 1-2% 정도의 지연 시간(latency)만을 추가하며, 메모리 사용량(memory footprint)도 거의 무시할 수 있는 수준입니다. 이는 기존 디코딩 로직(logic)을 매우 실용적으로 대체할 수 있음을 의미합니다.
*   **자연어 명령으로 제어하는 능력 발현:** 가장 놀라운 결과는 모델이 명시적으로 훈련받지 않았음에도 불구하고, 프롬프트(prompt)에 포함된 자연어 명령을 해석하여 자체적인 생성 스타일을 조절하는 방법을 학습했다는 것입니다. 예를 들어, "더 혁신적이고 다양하게"라는 문구를 추가하면 모델은 예측된 온도(temperature) 및 top-p 값을 자발적으로 증가시키고, "가능한 한 확실하게"를 추가하면 이를 낮춥니다. 표적화된 훈련을 통해 이러한 행동은 놀랍도록 일관적입니다(95% 이상).

이러한 발견들은 AutoDeco가 단순한 기술적 개선을 넘어, 사용자 친화적인 AI 상호작용의 가능성을 확장했음을 시사합니다. 모델이 사용자의 미묘한 의도까지 파악하여 생성 스타일을 조절할 수 있다면, 이는 콘텐츠 생성, 대화형 AI, 심지어 창의적 작업에 이르는 광범위한 분야에서 혁신적인 변화를 가져올 수 있을 것입니다. 특히, 효율성과 제어 가능성을 동시에 확보했다는 점은 실용적인 AI 시스템 개발에 큰 이점을 제공합니다.

*(BASE 문서에 언급된 표에 대한 설명)*
제시된 표들은 AutoDeco가 도메인 내(수학) 및 도메인 외(일반 질의응답, 코드 등) 작업 모두에서 우수한 성능을 나타낸다는 핵심적인 정량적 증거를 제공합니다.

*(BASE 문서에 언급된 그림에 대한 설명)*
이 그림은 발현 능력에 대한 강력한 시각적 자료로, 프롬프트(prompt)의 자연어 명령에 반응하여 모델의 내부, 토큰(token) 수준 온도(temperature) 및 top-p 예측이 실시간으로 어떻게 변화하는지를 보여줍니다.

**중요 자료:**
ArXiv 페이지
PDF
GitHub 페이지

#### 1.2. Kimi Linear: 표현력이 풍부하고 효율적인 어텐션(Attention) 아키텍처(Architecture)

이 논문은 Kimi Linear라는 혁신적인 하이브리드 어텐션(attention) 아키텍처(architecture)를 소개합니다. 이 구조는 다양한 상황에서 기존의 완전 소프트맥스 어텐션(full softmax attention)을 성능과 효율성 면에서 처음으로 명확하게 능가하는 성과를 보였습니다. 표준 LLM의 어텐션 메커니즘(attention mechanism)은 중요한 병목 현상으로, 매우 긴 시퀀스(sequence)를 처리할 때 이차 복잡도(quadratic complexity)로 인해 느리고 메모리를 많이 사용하는 문제에 직면합니다. '선형 어텐션(linear attention)'이 대안으로 제안되었지만, 지금까지는 완전 어텐션(full attention)의 성능에 필적하기 어려웠습니다. Kimi Linear는 새롭고 표현력이 뛰어난 선형 어텐션 모듈(linear attention module)과 주기적으로 배치되는 완전 어텐션(full attention) 레이어(layer)를 결합하여 이러한 한계를 극복합니다. 그 결과, 훨씬 빠른 속도와 극적으로 적은 메모리 사용량(memory footprint)으로 최첨단 성능을 제공하는 파레토 최적(Pareto-optimal) 시스템을 구현했습니다.

이 연구는 'Kimi Linear'라는 이름의 선구적인 혼합형 어텐션(attention) 구조를 발표합니다. 이 아키텍처(architecture)는 광범위한 활용 환경에서 기존의 완전 소프트맥스 어텐션(full softmax attention) 방식보다 탁월한 성능과 효율성을 동시에 보여주며, 이 분야에서 최초로 명확한 우위를 점했습니다. 대규모 언어 모델(LLM)에 사용되는 일반적인 어텐션(attention) 방식은 주요 제약 사항으로 작용해 왔는데, 이는 매우 긴 문맥(context)을 처리할 때 발생하는 제곱 비례(quadratic complexity) 문제로 인해 속도가 느려지고 과도한 메모리를 소비하는 경향이 있었기 때문입니다. '선형 어텐션(linear attention)'이 이러한 문제의 해결책으로 제시되었으나, 과거에는 완전 어텐션(full attention)이 제공하는 성능 수준에 도달하기 어렵다는 한계가 있었습니다. Kimi Linear는 새롭게 개발된, 표현력이 뛰어난 선형 어텐션(attention) 구성 요소와 특정 간격으로 배치되는 완전 어텐션(attention) 계층을 결합함으로써 이러한 난점을 극복했습니다. 그 결과, 월등히 빠른 처리 속도와 현저히 줄어든 메모리 사용량을 바탕으로 최상급의 성능을 제공하는 파레토 최적(Pareto-optimal)의 시스템을 구축하는 데 성공했습니다.

**핵심 아이디어: 두 가지 장점 모두 활용**

어텐션(attention) 아키텍처(architecture)의 근본적인 과제는 표현력(expressiveness)과 효율성(efficiency) 사이의 균형을 찾는 것입니다. 완전 어텐션(full attention)은 모든 토큰(token)이 다른 모든 토큰(token)을 참조할 수 있기 때문에 강력하지만, 계산 비용이 매우 높습니다. 반면 선형 어텐션(linear attention)은 빠르고 고정된 메모리 상태를 갖지만, 이러한 압축 과정에서 정보 손실이 발생하여 성능 저하로 이어질 수 있습니다. Kimi Linear의 핵심 아이디어는 이 두 가지 방식의 장점을 지능적으로 결합한 하이브리드 시스템(hybrid system)을 구축하는 것입니다. 대부분의 레이어(layer)에는 새롭고 강력한 선형 어텐션 메커니즘(linear attention mechanism)인 **Kimi 델타 어텐션(Kimi Delta Attention, KDA)**을 사용합니다. 이는 엄청난 효율성과 속도를 제공합니다. 장거리에서 발생할 수 있는 정보 손실을 보완하기 위해, 3개의 KDA 레이어(layer)마다 하나의 완전 어텐션(full attention) 레이어(layer)를 전략적으로 교차 배치합니다. 이러한 설계는 효율적인 KDA 레이어(layer)가 대부분의 처리를 담당하고 지역적인 문맥(local context)을 관리하도록 하는 동시에, 드물게 배치되는 완전 어텐션(full attention) 레이어(layer)가 "전역 정보 허브(global information hubs)" 역할을 하여 중요한 장거리 의존성(long-range dependencies)이 보존되도록 합니다. 그 결과는 선형 어텐션(linear attention)의 속도 이점을 달성하면서 완전 어텐션(full attention)의 품질을 능가하는 아키텍처(architecture)입니다.

이러한 하이브리드 접근 방식은 마치 고속도로와 국도를 적절히 활용하는 교통 시스템과 유사합니다. KDA는 대부분의 트래픽을 효율적으로 처리하는 고속도로 역할을 하여 빠른 이동을 가능하게 합니다. 반면, 완전 어텐션(full attention) 레이어(layer)는 중요한 교차로와 같은 역할을 하여, 전체 경로에 걸쳐 핵심적인 정보를 통합하고 장거리 연결성을 유지하는 데 기여합니다. 이로써 모델은 방대한 정보를 처리하면서도 중요 정보를 놓치지 않고, 동시에 계산 자원을 매우 효율적으로 사용할 수 있게 됩니다.

**주요 방법론: 세분화된 선형 어텐션(Linear Attention)과 완전 어텐션(Full Attention)의 하이브리드(Hybrid)**

이 아키텍처(architecture)의 성공은 두 가지 핵심 요소, 즉 새로 개발된 선형 어텐션 메커니즘(linear attention mechanism)과 이를 하이브리드(hybrid) 구조에 전략적으로 통합하는 방식에 기반합니다.

*   **Kimi 델타 어텐션(Kimi Delta Attention, KDA):** 이것이 바로 핵심적인 혁신입니다. KDA는 어텐션(attention)을 지속적인 메모리 수정 과정으로 간주하는 "델타 규칙(delta rule)"에 기반한 진보된 선형 어텐션 모듈(linear attention module)입니다. KDA는 Gated DeltaNet과 같은 이전 방법들을 **세분화된 게이팅 메커니즘(gating mechanism)**을 도입하여 개선합니다. 정보의 전체 블록(block)에 단일 "망각 게이트(forget gate)"를 사용하는 대신, KDA는 채널별 게이트(channel-wise gate)를 사용하여 각 특징 차원(feature dimension)이 자체적인 독립적인 메모리 감쇠율(memory decay)을 갖도록 합니다. 이를 통해 모델의 유한 상태 메모리(finite-state memory)에서 어떤 정보를 유지하거나 버릴지에 대해 훨씬 더 정밀한 제어가 가능해집니다. 결정적으로, KDA는 유사한 방법들과 비교하여 계산량을 크게 줄이는 맞춤형 하드웨어 효율적 알고리즘(hardware-efficient algorithm)으로 구현됩니다.
*   **하이브리드 레이어(Hybrid Layer) 아키텍처(Architecture):** Kimi Linear는 순수한 선형 모델(linear model)이 아닙니다. 이는 KDA 레이어(layer)와 완전 어텐션(full attention) 레이어(layer)(특히 다중 헤드 잠재 어텐션(Multi-Head Latent Attention, MLA))를 번갈아 사용하는 하이브리드 아키텍처(hybrid architecture)입니다. 절제 연구(ablation study)에 따르면, 3:1의 균일한 비율(MLA 완전 레이어(layer) 하나당 KDA 레이어(layer) 세 개)이 성능과 효율성 사이의 최적의 절충점을 제공했습니다. 이러한 설계는 메모리 집약적인 KV 캐시(KV cache)를 최대 75%까지 줄이고, 위치 정보(positional information) 처리에 대한 책임을 KDA 레이어(layer)에 위임하여 완전 어텐션(full attention) 레이어(layer)가 위치 인코딩(positional encodings, NoPE) 없이 더 효율적으로 작동하도록 합니다.

KDA의 세분화된 게이팅은 마치 도서관에서 각 책의 중요도에 따라 보관 기간을 다르게 설정하는 것과 같습니다. 이는 모델이 제한된 기억 공간 내에서 어떤 정보가 현재 작업에 더 중요하고 어떤 정보는 덜 중요한지를 동적으로 판단하여 효율적으로 관리할 수 있게 합니다. 또한, 하이브리드 구조는 전체 시스템의 자원 효율성을 극대화하며, 특히 긴 문맥(long context) 처리에서 발생하는 메모리 병목 현상을 크게 완화합니다. 이러한 설계는 LLM이 방대한 양의 텍스트를 처리하면서도 핵심적인 맥락을 유지하고, 빠른 응답 속도를 유지할 수 있도록 하는 데 결정적인 역할을 합니다.

*(BASE 문서에 언급된 다이어그램에 대한 설명)*
이 다이어그램은 KDA와 MLA 블록(block)을 3:1 비율로 쌓아 올린 하이브리드 모델(hybrid model)의 구조를 명확하게 보여줍니다.

**가장 중요한 발견**

엄격하고 광범위한 실험을 통해 이 논문은 Kimi Linear가 완전 어텐션(full attention)의 우수한 대체재(drop-in replacement)이며, 엄청난 효율성 향상과 함께 최첨단 성능을 달성함을 입증합니다.

*   **완전 어텐션(Full Attention) 압도:** 공정하고 규모에 맞춰 사전 훈련(pre-training)(1.4조 토큰(token))된 환경에서, Kimi Linear는 일반 지식, 추론, 수학, 코드 관련 평가 지표(benchmark) 전반에서 표준 완전 어텐션(full-attention) 기준선(baseline)(MLA)과 또 다른 하이브리드 기준선(hybrid baseline)(GDN-H)을 꾸준히 그리고 확실하게 능가했습니다.
*   **최고 수준의 장문 컨텍스트(Long-Context) 처리 성능:** Kimi Linear는 RULER 및 RepoQA와 같은 과제에서 기존 기준선(baseline)들을 크게 앞지르며, 일련의 장문 컨텍스트(long-context) 평가 지표(benchmark)에서 최고 점수를 기록했습니다.
*   **폭발적인 처리량(Throughput) 증대:** 이 아키텍처(architecture)는 긴 시퀀스(sequence) 처리에서 극적인 속도 향상을 제공합니다. 100만 토큰(token)의 컨텍스트(context) 길이에서 Kimi Linear는 표준 완전 어텐션(full attention) 기준선(baseline)보다 최대 6.3배 더 높은 디코딩 처리량(decoding throughput)을 보였습니다. 이는 출력 토큰(token)당 시간이 낮고 일정하게 유지되는 반면, 완전 어텐션(full attention)의 비용은 엄청나게 증가하기 때문입니다.
*   **우수한 강화 학습(RL) 수렴:** 복잡한 수학 문제에 대한 강화 학습(reinforcement learning)에 적용되었을 때, Kimi Linear는 완전 어텐션(full-attention) 기준선(baseline)보다 훨씬 빠르고 더 나은 수렴을 보여주었으며, 이는 그 아키텍처(architecture)가 추론 집약적인 생성 작업에 더욱 적합함을 시사합니다.

이러한 결과는 Kimi Linear가 단순히 이론적인 개선을 넘어, 실제 AI 시스템의 성능과 효율성을 혁신적으로 향상시킬 수 있는 잠재력을 가졌음을 보여줍니다. 특히, 긴 문맥을 빠르고 정확하게 처리하는 능력은 법률 문서 분석, 과학 연구 요약, 장문 콘텐츠 생성 등 다양한 고부가가치 응용 분야에서 핵심적인 역할을 할 것으로 기대됩니다. 또한, 강화 학습에서의 빠른 수렴은 AI 에이전트의 학습 효율성을 높여, 더욱 복잡하고 정교한 작업을 수행할 수 있는 AI 개발의 문을 열어줄 것입니다.

*(BASE 문서에 언급된 그림에 대한 설명)*
이 요약 그림은 (a) Kimi Linear의 파레토 최적(Pareto-optimal) 성능 대 가속 곡선(acceleration curve)과 (b) 경쟁 모델 대비 대규모 시퀀스(sequence) 길이에서 평탄하고 낮은 지연 시간(latency)을 보이는 디코딩 시간(decoding time)을 핵심적으로 보여줍니다.

*(BASE 문서에 언급된 표에 대한 설명)*
이 표들은 사전 훈련(pre-training), 지시 튜닝(instruction-tuning) 및 장문 컨텍스트(long-context) 평가 지표(benchmark) 전반에 걸쳐 Kimi Linear의 우수한 성능을 입증하는 상세한 정량적 결과를 제공합니다.

**중요 자료:**
ArXiv 페이지
PDF
GitHub 페이지

#### 1.3. 통이 딥리서치(Tongyi DeepResearch) 기술 보고서

이 기술 보고서는 자율적이고 장기적이며 복잡한 정보 탐색 작업을 위해 특별히 설계된 강력한 오픈 소스 에이전트형(agentic) 대규모 언어 모델인 **통이 딥리서치(Tongyi DeepResearch)**를 소개합니다. 이 연구는 새로운 "에이전트형(agentic) 중간 훈련(mid-training)" 단계와 정교한 "에이전트형(agentic) 후처리 훈련(post-training)" 단계를 결합하여 이 고급 에이전트(agent)를 성공적으로 훈련시키는 완전한 종단 간(end-to-end) 프레임워크(framework)를 제시합니다. 이 프레임워크(framework)의 초석은 값비싼 인간 주석(human annotation)에 의존하지 않고 모든 훈련 단계를 지원하는 완전 자동화되고 고도로 확장 가능한 데이터 합성 파이프라인(data synthesis pipeline)입니다. 그 결과 모델은 토큰(token)당 33억 개의 활성화된 매개변수(parameter)만으로도 일련의 심층 연구 벤치마크(benchmark) 전반에서 최첨단 성능을 달성하며, 더 크고 독점적인 많은 시스템(system)을 능가합니다.

본 기술 문서는 자율성을 지니고 장기적인 관점에서 복잡한 정보 탐색 과제를 수행하도록 특화된 강력한 개방형 에이전트형(agentic) 거대 언어 모델인 **통이 딥리서치(Tongyi DeepResearch)**를 소개합니다. 이 연구는 이 고성능 에이전트(agent)를 성공적으로 학습시키기 위한 포괄적인 종단 간(end-to-end) 방법론을 제시하는데, 이는 혁신적인 "에이전트형(agentic) 중간 학습(mid-training)" 과정과 정교한 "에이전트형(agentic) 사후 학습(post-training)" 단계를 결합한 것입니다. 이 방법론의 핵심은 고가의 수작업 주석(human annotation)에 의존하지 않고도 모든 학습 단계를 지원하는 완전히 자동화되고 매우 유연한 데이터 생성 파이프라인(data synthesis pipeline)입니다. 이러한 노력의 결과로, 이 모델은 토큰(token)당 단 33억 개의 활성화된 매개변수(parameter)만을 사용하여도 다양한 심층 연구 평가 지표(benchmark)에서 최상급의 성능을 달성하며, 규모가 더 크고 독점적인 다수의 시스템(system)들을 능가하는 역량을 보여줍니다.

**핵심 아이디어: AI 연구자를 위한 확장 가능하고 오픈 소스(Open-Source) 청사진**

이 프로젝트의 핵심은 인공지능 연구자들을 위한 확장 가능하며 개방형(open-source)의 모범 사례를 제공하는 데 있습니다. 인공지능 분야에서 자율적인 에이전트의 중요성이 점점 커지고 있으며, 특히 복잡한 정보 탐색이나 연구 과제를 자동화하는 데 있어 그 잠재력이 주목받고 있습니다. Tongyi DeepResearch는 이러한 수요에 부응하여, 인간의 개입을 최소화하면서도 고품질의 연구를 수행할 수 있는 AI 시스템을 구축하는 청사진을 제시합니다.

이 모델은 단순히 정보를 검색하는 것을 넘어, 검색된 정보를 분석하고, 가설을 세우고, 실험을 계획하며, 결과를 종합하는 등 연구 과정 전반을 자율적으로 수행하는 것을 목표로 합니다. 이는 전통적인 검색 엔진이나 질의응답 시스템의 한계를 넘어서, AI가 능동적으로 지식을 탐구하고 새로운 발견을 이끌어낼 수 있는 가능성을 열어줍니다. 특히, 이 모델이 오픈 소스(open-source)로 공개되었다는 점은 전 세계 연구자들이 이 기술을 기반으로 더욱 발전된 에이전트(agent)를 개발하고, AI 연구의 속도를 가속화하는 데 크게 기여할 것입니다.

Tongyi DeepResearch의 성공은 대규모 언어 모델이 단순히 텍스트를 생성하는 도구를 넘어, 복잡한 문제 해결과 지식 탐구의 주체로 진화하고 있음을 보여주는 중요한 사례입니다. 이는 미래의 과학 연구, 기술 혁신, 그리고 다양한 산업 분야에서 AI의 역할이 더욱 확대될 것임을 시사합니다.

*(BASE 문서에 언급된 그림이나 표는 이 섹션에 없음)*