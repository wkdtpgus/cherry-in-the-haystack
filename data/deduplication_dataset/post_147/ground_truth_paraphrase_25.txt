이번 137번째 에피소드에서는 Davidad Dalrymple님과 함께 아래의 핵심 내용들을 논의했습니다:

*   인공지능이 내포할 수 있는 잠재적 위험성에 대한 그의 시각
*   영국 고등 연구 및 혁신 기관인 ARIA(Advanced Research and Invention Agency)와 그들이 추진하는 안전한 AI 프로그램(Safeguarded AI Programme)에 관한 내용

청취하시는 동안 즐거움을 느끼시길 바라며, 여러분의 소중한 의견도 함께 공유해 주십시오!

현재 Davidad는 ARIA의 프로그램 총괄 책임자(Program Director)로 재직 중입니다. 그는 최근 옥스퍼드 대학교에서 기술 AI 안전 분야의 연구원(Research Fellow)으로 활동했습니다. 그는 세계 40대 주요 암호화폐(cryptocurrency) 중 하나인 파일코인(Filecoin)의 공동 창시자이며, 국제 신경과학 분야의 협력을 주도했습니다. 또한, 트위터(Twitter)를 비롯한 다수의 신생 기업(startup)에서 수석 소프트웨어 엔지니어(senior software engineer)로서 경험을 쌓았습니다. 이러한 다채로운 경력은 그가 AI 안전이라는 복합적인 문제에 접근하는 데 있어 독특하고 깊이 있는 통찰력을 제공합니다.

새로운 에피소드 업데이트 소식을 접하시려면 트위터(Twitter)에서 저를 팔로우해 주시고, 피드백, 아이디어, 또는 게스트 추천이 있으시다면 editor@thegradient.pub으로 연락 주십시오.

더 그레디언트 팟캐스트 구독: 애플 팟캐스트(Apple Podcasts) | 스포티파이(Spotify) | 포켓 캐스트(Pocket Casts) | RSS
더 그레디언트 트위터(Twitter) 팔로우
구독

**개요 :**
*   (00:00) 서론: 팟캐스트의 시작과 주요 논의 주제 소개
*   (00:36) 돌파구에 대한 보정(calibration) 및 낙관론: 인공지능 발전의 속도와 예측에 대한 현실적인 평가
*   (03:35) 보정(calibration)과 AGI 타임라인, AGI가 인류에 미치는 영향: 범용 인공지능(AGI)의 도래 시점과 그로 인한 사회적, 윤리적 변화
*   (07:10) 직교성 가설(Orthogonality Thesis)에 대한 Davidad의 생각: 지능과 목표의 독립성에 대한 그의 견해
*   (10:30) 현재 우리의 방향이 AGI와 돌파구에 어떻게 관련되는지 이해하기: 인공지능 연구의 현재 경로가 미래의 주요 발전에 미치는 영향
*   (13:33) Davidad가 AGI에 필요하다고 생각하는 것: 범용 인공지능 실현을 위한 핵심 요소들
*   (17:00) 지식 추출: AI 시스템이 외부 지식을 효과적으로 습득하고 활용하는 방법
*   (19:01) 사이버 물리 시스템(cyber-physical systems) 및 모델링 프레임워크(modeling frameworks): 실제 세계와 상호작용하는 AI 시스템의 설계 및 분석
*   (20:00) Davidad의 이전 작업과 ARIA 간의 연속성: 그의 과거 연구와 ARIA에서의 현재 역할 사이의 연결점
*   (22:56) 기술의 경로 의존성(path dependence), 경쟁 역학(race dynamics): AI 개발 경쟁이 기술 발전 방향에 미치는 영향
*   (26:40) AGI에 문제가 발생할 수 있는 것에 대한 Davidad의 관점 추가: 범용 인공지능의 잠재적 위험에 대한 심층 분석
*   (28:57) 취약한 세계, 컴퓨터와 제어의 상호 연결성: 복잡한 현대 사회에서 AI 시스템의 역할과 그에 따른 위험
*   (34:52) 형식 검증(formal verification) 및 세계 모델링, 개방형 에이전시 아키텍처(Open Agency Architecture): AI 시스템의 신뢰성과 안전성을 확보하기 위한 방법론
*   (35:25) 의미론적 충분성 가설(Semantic Sufficiency Hypothesis): AI 시스템이 의미를 얼마나 깊이 이해할 수 있는지에 대한 논의
*   (39:31) 모델링(modeling)의 과제: 복잡한 시스템을 정확하게 모델링하는 데 따르는 어려움
*   (43:44) 의무론적 충분성 가설(Deontic Sufficiency Hypothesis) 및 수학적 형식화: AI 윤리를 수학적으로 정의하고 구현하는 방안
*   (49:25) 과도한 단순화와 정량적 지식: 복잡한 AI 문제를 단순화할 때 발생할 수 있는 문제점과 정량적 접근의 중요성
*   (53:42) AI를 위한 가치 표현에 있어 집단적 숙고: AI 시스템에 인간의 가치를 반영하기 위한 사회적 합의 과정
*   (55:56) ARIA의 보호 AI 프로그램(Safeguarded AI Programme): ARIA가 추진하는 안전한 AI 개발을 위한 구체적인 이니셔티브
*   (59:40) Anthropic의 ASL 수준: 주요 AI 연구 기관들의 안전성 평가 프레임워크
*   (1:03:12) 보장된 안전한 AI(Guaranteed Safe AI): AI 시스템의 안전성을 어떻게 보장할 것인가에 대한 탐구
*   (1:03:38) AI 위험 및 (비)정확한 세계 모델: 인공지능 위험을 평가하고 관리하는 데 있어 세계 모델의 중요성
*   (1:09:59) 세계 모델 및 검증자(verifier)를 위한 안전 사양 수준 — 높은 안전성을 달성하기 위한 단계: AI 안전성 확보를 위한 구체적인 기술적 로드맵
*   (1:12:00) Davidad의 포트폴리오 연구 접근 방식과 ARIA의 자금 지원: ARIA가 다양한 연구 프로젝트에 자금을 지원하는 전략
*   (1:15:46) ARIA에 대한 초기 우려 — Davidad의 관점: 기관 설립 초기 제기되었던 우려 사항들에 대한 Davidad의 해명
*   (1:19:26) ARIA 및 보호 AI 프로그램(Safeguarded AI Programme)에 대한 추가 정보 찾기: 관련 정보를 얻을 수 있는 자료 안내
*   (1:20:44) 마무리: 에피소드 요약 및 청취자에게 전하는 메시지

**링크 :**
*   Davidad의 트위터(Twitter): Davidad의 개인 트위터 계정으로, 그의 최신 생각과 활동을 확인할 수 있습니다.
*   ARIA 홈페이지: ARIA(영국 첨단 연구 및 발명 기관)의 공식 웹사이트로, 기관의 목표, 프로그램 및 연구 분야에 대한 상세 정보를 제공합니다.
*   보호 AI 프로그램(Safeguarded AI Programme) 문서: ARIA의 보호 AI 프로그램에 대한 공식 문서로, 프로그램의 목적, 범위 및 구현 전략을 설명합니다.
*   보장된 안전한 AI(Guaranteed Safe AI): 안전한 AI 시스템 개발에 대한 심층적인 관점과 접근 방식을 담은 자료입니다.
*   안전한 변혁적 AI를 위한 Davidad의 개방형 에이전시 아키텍처(Open Agency Architecture): Davidad가 제안하는 안전한 AI 아키텍처에 대한 기술 문서입니다.
*   Dioptics: 개방형 게임(Open Games) 및 경사 기반 학습자(Gradient-Based Learners)의 공통 일반화 (2019): AI 학습 메커니즘에 대한 Davidad의 초기 연구 논문입니다.
*   비동기 논리 오토마타(Asynchronous Logic Automata) (2008): 계산 이론과 자동화에 관한 Davidad의 또 다른 중요한 연구 결과입니다.