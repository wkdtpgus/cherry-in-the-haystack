우리는 모두 방대한 양의 텍스트로 훈련된 대규모 언어 모델(large language model)인 GPT 모델을 기반으로 구축된 애플리케이션인 챗GPT(ChatGPT)에 익숙합니다. 챗GPT의 핵심적인 주요 기술은 주어진 입력 시퀀스(sequence)를 기반으로 다음 토큰(token) (단어 또는 단어의 일부)을 예측하는 것입니다. 이러한 토큰들의 연속적인 생성을 통해 모델은 자연스러운 텍스트를 만들어내며, 바로 이 점이 챗GPT를 비롯한 대규모 언어 모델을 '생성형 인공지능(generative AI)'으로 분류하는 핵심 이유입니다.

텍스트를 한 번에 한 단어씩 생성하는 과정 자체가 복잡한 것은 아닙니다. 진정한 난관은 단순히 문법적으로 올바른 문장을 넘어, 주어진 질문이나 맥락에 부합하는 적절하고 의미 있는 응답을 만들어내는 데 있습니다. 그리고 물론, 그것은 결코 사소한 일이 아닙니다. 이를 위해서는 모델이 개별 단어뿐만 아니라 단어들이 멀리 떨어져 있을 때에도 그 관계를 고려해야 합니다. 또한 이러한 복잡한 언어의 패턴과 단어 간의 관계를 이해하기 위해, GPT 모델의 핵심인 "트랜스포머(transformer)" 아키텍처(architecture)가 개발되었으며, 이는 텍스트 생성 기술에 혁명적인 변화를 가져왔습니다. 트랜스포머 아키텍처는 병렬 처리 능력과 장거리 의존성 학습을 통해 텍스트 생성의 한계를 극복하며, 오늘날 우리가 경험하는 고성능 AI의 기반을 다졌습니다.

Engineering Prompts는 독자 지원 출판물입니다. 새로운 게시물을 받고 제 작업을 지원하려면 무료 또는 유료 구독자가 되는 것을 고려해 보세요.

구독하기

## 생명의 언어

이제 이 혁신적인 기술을 다른 언어, 어쩌면 가장 중요한 언어인 생명 자체의 언어에 적용한다고 상상해 보세요. 생명의 근원적인 설계도인 DNA는 아데닌(A), 시토신(C), 구아닌(G), 티민(T)이라는 네 가지 뉴클레오타이드(nucleotide)로 이루어져 있으며, 이들은 특정 규칙(A는 T와, G는 C와)에 따라 서로 결합하여 특징적인 이중 나선(double-helix) 구조를 이룹니다. 이 뉴클레오타이드(nucleotide)는 유전자(gene)와 조절 서열(regulatory sequence)을 형성하며, 염색체(chromosome)에 포장되어 전체적으로 게놈(genome)을 구성합니다. 지구상의 모든 생명체는 각기 고유한 게놈 서열(genomic sequence)을 보유하고 있으며, 심지어 같은 종 내에서도 개체마다 미세한 차이를 보입니다. 그러나 종 내의 차이는 종 간의 차이에 비해 작습니다.

예를 들어, 인간의 게놈(genome)은 약 30억 개의 염기쌍(base pair)으로 이루어져 있습니다. 만약 이를 지구상의 무작위로 선택된 인간의 게놈(genome)과 비교한다면, 약 300만 개의 염기쌍(base pair) 차이를 발견할 수 있을 것입니다. 이는 단 0.1%에 불과합니다. 제 게놈(genome)을 가장 가까운 친척인 침팬지의 게놈(genome)과 비교하면, 그 차이는 약 3천만 개의 염기쌍(base pair), 즉 약 1%로 증가합니다. 이러한 차이는 전체 게놈 규모에 비하면 미미해 보일 수 있습니다. 만약 두 권의 책이 단 1%만 다르다면 우리는 아마도 그것을 표절이라고 생각할 것입니다. 하지만 이러한 미미한 변화들이 인간의 모든 놀라운 유전적 다양성(genetic diversity)을 설명하며, 심지어 우리를 다른 종과 구분 짓습니다.

최근 몇 년 동안 과학자들은 수천 종의 종을 시퀀싱(sequencing)했습니다. 우리는 우리의 유전적 다양성(genetic diversity)을 점점 더 잘 이해하고 있습니다. 하지만 이러한 노력에도 불구하고, 생명의 언어는 여전히 방대한 미지의 영역으로 남아 있으며, 그 복잡성과 잠재력을 완전히 해독하기 위한 연구는 계속되고 있습니다.

## Evo 2: DNA를 위한 챗GPT

DNA를 위한 챗GPT(ChatGPT)라는 이 비전은 Arc Institute의 Evo 2 모델로 현실이 되었습니다. 최근 공개된 이 모델은 생명공학 분야의 기념비적인 성과로 평가받고 있습니다. 이 모델은 모든 생명 영역을 아우르는 큐레이션된 게놈 아틀라스(genomic atlas)에서 가져온 9.3조 개의 DNA 염기쌍(base pair)으로 훈련되었습니다. 비교하자면, GPT-4는 약 6.5조 개의 토큰(token)으로 훈련된 것으로 추정됩니다(OpenAI가 정확한 수치를 공개하지는 않았지만). Meta의 LLaMA 3와 DeepSeek V3는 모두 약 15조 개의 토큰(token)으로 훈련되었습니다. 따라서 훈련 규모 면에서 Evo 2는 선도적인 언어 모델(language model)들과 어깨를 나란히 합니다.

Evo2는 광범위한 생물학적 패턴을 포착하기 위해 컨텍스트 윈도우(context window)를 최대 100만 염기쌍(base pair)까지 확장합니다. 아래 패널은 데이터 증강(data augmentation) 및 가중치 부여(weighting) 접근 방식을 보여줍니다.
출처: https://arcinstitute.org/manuscripts/Evo2

그렇다면 Evo 2는 무엇을 할 수 있을까요? 핵심 기능 중 하나는 돌연변이(mutation)의 영향을 예측하는 것입니다. 여러분의 유전자(gene) 중 하나를 예로 들어 봅시다. 대부분의 유전자(gene)는 세포가 생명의 근본적인 구성 요소인 단백질(protein)을 만드는 데 사용하는 지침을 포함하고 있습니다. (이 단백질(protein)이 기능적 구조로 접히는 방식은 DeepMind의 AlphaFold가 성공적으로 다룬 또 다른 어려운 예측 작업입니다.) 이제 그 서열을 변경하면 결과가 어떻게 달라질까요? 일부 변이(variant)는 치명적이고, 다른 일부는 해로우며, 많은 변이(variant)는 중립적이고, 드물게는 유익한 변이(variant)도 있습니다. 문제는 어떤 것이 어떤 것인지 알아내는 것입니다.

이것이 바로 Evo 2가 빛을 발하는 지점입니다. 다양한 변이 예측 작업에서 Evo 2는 다른 고도로 전문화된 모델의 기존 기준선(baseline)과 일치하거나 이를 능가합니다. 다시 말해, 어떤 돌연변이(mutation)가 병원성(pathogenic)일 가능성이 있는지, 또는 잘 알려진 암 유전자(cancer gene) (유방암과 관련된 BRCA1과 같은)의 어떤 변이(variant)가 임상적으로 중요한지 예측할 수 있습니다. 놀랍게도 Evo 2는 인간 변이 데이터로 훈련되지 않았습니다. 표준 인간 참조 게놈(reference genome)으로만 훈련되었습니다. 그러나 게놈 서열(genomic sequence)에 대한 진화적 제약(evolutionary constraint)을 학습한 것으로 보이기 때문에 인간에게 어떤 돌연변이(mutation)가 해로운지 추론할 수 있습니다. Evo 2는 종과 컨텍스트(context)에 걸쳐 "정상적인" DNA가 어떻게 생겼는지 이해합니다. 이는 Evo 2가 단순히 패턴을 인식하는 것을 넘어, 생명의 진화적 원칙과 종을 아우르는 보편적인 DNA 구조 및 기능에 대한 깊은 이해를 기반으로 작동함을 시사합니다. 이러한 능력은 정밀 의학(precision medicine) 분야에 혁신을 가져올 잠재력을 가지고 있습니다.

코딩 영역(coding region) 내 변이 병원성(variant pathogenicity)의 제로샷(zero-shot) 평가.
출처: https://arcinstitute.org/manuscripts/Evo2

## 생물학을 이해하는 것에서 창조하는 것으로

Evo 2를 놀랍게 만드는 것은 특정 서열을 이전에 본 적이 없더라도 DNA 서열이 "이상하다"고 느껴질 때를 알 수 있는 능력입니다. 마치 챗GPT(ChatGPT)가 자연스러운 언어의 흐름과 문맥을 직관적으로 이해하는 것과 유사합니다. 하지만 더 나아가, Evo 2는 원본 훈련 데이터로부터 이동성 유전 요소(mobile genetic element), 조절 모티프(regulatory motif), 단백질 2차 구조(protein secondary structure) 등과 같은 생물학적 특징(biological feature)을 직접 학습했습니다. 이는 놀라운 일입니다. 왜냐하면 단순히 DNA 서열을 읽는 것을 넘어, 해당 정보가 훈련 데이터의 일부가 아니었음에도 불구하고 고차 구조 정보를 포착하기 때문입니다.

다시 한번, 챗GPT(ChatGPT)와의 비교가 도움이 됩니다. 챗GPT(ChatGPT)는 문법 규칙을 명시적으로 배운 적이 없더라도 올바른 문법으로 문장을 완성할 수 있습니다. 이와 유사하게, Evo 2는 유전자(gene)나 단백질(protein)의 정의를 직접적으로 학습하지 않았음에도 불구하고, 생물학적으로 타당한 구조로 게놈(genome)의 특정 부분을 완성해낼 수 있는 능력을 보여줍니다.

마지막으로, GPT 모델이 새로운 콘텐츠를 생성할 수 있는 것처럼 (그래서 "생성형 AI(generative AI)"라는 이름이 붙었습니다), Evo 2는 새로운 DNA 서열을 생성할 수 있습니다. 여기에서 우리는 생물학을 이해하는 것에서 생물학을 창조하는 것으로 나아갑니다. Evo 2는 미토콘드리아 게놈(mitochondrial genome), 박테리아 게놈(bacterial genome), 그리고 효모 게놈(yeast genome)의 일부를 생성하는 데 사용되었습니다. 이는 바이오 제조(biomanufacturing), 탄소 포집(carbon capture), 또는 약물 합성(drug synthesis)을 위한 유기체 설계를 가능하게 하여 합성 생물학(synthetic biology)에서 매우 유용할 수 있습니다.

그러나 챗GPT(ChatGPT)와 마찬가지로 Evo 2도 주요한 한계점을 가지고 있습니다. 생물학적으로 그럴듯한 DNA 서열을 생성하는 것이 실험적 검증(experimental validation) 없이 이러한 서열이 생물학적으로 기능적임을 보장하지는 않습니다. 특히, 생성된 DNA 서열이 실제 생물학적 시스템 내에서 의도한 대로 작동할지는 엄격한 실험적 검증을 거쳐야 하며, 이는 합성 생물학 분야의 핵심 과제로 남아 있습니다. 하지만 불과 몇 년 사이에 언어 모델이 GPT-3에서 LLaMA 3, DeepSeek V3와 같은 최첨단 모델로 놀랍게 발전한 것을 고려하면, 생성형 생물학(generative biology)의 미래 역시 무한한 가능성을 품고 있음을 쉽게 예측할 수 있습니다.

Evo 2가 생성한 원핵생물 게놈 서열(prokaryotic genomic sequence)에서 발견된 예시 단백질의 AlphaFold 3 구조 예측.
출처: https://arcinstitute.org/manuscripts/Evo2

이 모든 것이 충분하지 않다면: Evo 2는 오픈소스(open-source), 오픈웨이트(open-weight) 모델입니다. 모델 매개변수(model parameter), 사전 훈련 코드, 추론 코드, 그리고 훈련에 사용된 전체 데이터셋(dataset)을 다운로드할 수 있습니다. 그리고 속도를 고려해 보세요. Evo 1은 불과 몇 달 전인 2024년 11월에 출시되었습니다. 이미 놀라운 성과였습니다. 하지만 Evo 1은 약 3천억 개의 토큰(token)으로 원핵생물 게놈(prokaryotic genome)만으로 훈련되었고, 131,000 염기쌍(base pair)의 컨텍스트 윈도우(context window)를 가졌으며, 기능도 비교적 제한적이었습니다. 이제 불과 몇 달 후, 훈련 규모를 30배 확장하고, 컨텍스트 윈도우(context window)를 8배 늘리고, 완전히 새로운 기능을 도입한 새로운 모델이 탄생했습니다! Evo 1에서 Evo 2로의 빠른 진화는 언어 모델(language model)에서 우리가 목격한 놀랍도록 빠른 개선을 반영합니다. 이는 마치 초기 언어 모델이 잦은 환각(hallucination) 현상으로 비판받던 시기를 넘어, 이제는 복잡한 추론과 창의적인 텍스트 생성까지 가능한 수준으로 진화한 것과 같습니다. GPT가 언어 생성에 혁명을 일으킨 것처럼, 이러한 DNA 언어 모델(DNA language model)은 생명 자체의 코드를 이해하는 방식을 변화시키고 있습니다. 이러한 혁신적인 도구들을 통해 생물학 연구는 미지의 영역을 개척하며 전례 없는 속도로 발전할 것이며, 생명의 비밀을 푸는 열쇠를 제공할 것으로 기대됩니다.

## CODA

이것은 두 가지 구독 유형을 가진 뉴스레터입니다. 유료 버전으로 전환하는 것을 강력히 추천합니다. 모든 콘텐츠는 무료로 유지되지만, 모든 재정적 지원은 EPFL AI 센터 활동에 직접적으로 자금을 지원합니다.

연락을 유지하려면 다음 방법으로 저를 찾을 수 있습니다.
소셜 미디어: 저는 주로 LinkedIn을 사용하지만, Mastodon, Bluesky, X에서도 활동합니다.
팟캐스트: 저는 EPFL AI 센터에서 "Inside AI"라는 AI 팟캐스트(Apple Podcasts, Spotify)를 진행하고 있으며, 저보다 훨씬 똑똑한 분들과 이야기할 수 있는 특권을 누리고 있습니다.

Engineering Prompts는 독자 지원 출판물입니다. 새로운 게시물을 받고 제 작업을 지원하려면 무료 또는 유료 구독자가 되는 것을 고려해 보세요.

구독하기