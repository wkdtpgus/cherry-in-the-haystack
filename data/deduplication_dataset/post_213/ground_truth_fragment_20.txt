우리는 모두 방대한 양의 데이터로 훈련된 다양한 인공지능(artificial intelligence) 모델과 그 응용 프로그램에 익숙합니다. 특히 대규모 언어 모델(large language model)인 GPT 모델을 기반으로 구축된 애플리케이션인 챗GPT(ChatGPT)에도 익숙합니다. 이러한 인공지능의 핵심적인 주요 기술은 복잡한 패턴을 인식하고 예측하는 능력에 있으며, 챗GPT의 경우 주어진 입력 시퀀스(sequence)를 기반으로 다음 토큰(token) (단어 또는 단어의 일부)을 예측하는 것입니다. 이러한 예측의 연속적인 흐름을 생성함으로써 모델은 텍스트를 넘어 이미지, 코드, 심지어 음악까지 생성합니다. 이것이 우리가 생성형 AI(generative AI) 시대를 살고 있다고 말하는 이유입니다.

한 번에 한 조각씩 콘텐츠를 생성하는 것이 본질적으로 어려운 것은 아닙니다. 진정한 도전은 단순히 의미가 통할 뿐만 아니라 입력에 적절한 응답을 형성하는 방식으로 텍스트를 생성하는 것을 넘어, 인간의 가치와 사회적 규범에 적절한 방식으로 행동하는 인공지능을 만드는 것입니다. 그리고 물론, 그것은 결코 사소한 일이 아닙니다. 이를 위해서는 모델이 개별 데이터 포인트(예: 단어)뿐만 아니라 데이터들이 멀리 떨어져 있을 때에도 그 관계를 고려해야 합니다. 또한 모델은 데이터들이 일반적으로 서로 어떻게 관련되는지 파악해야 합니다. 이것이 바로 GPT의 T를 구성하는 "트랜스포머(transformer)" 아키텍처(architecture)와 같은 최신 인공지능 아키텍처를 그토록 강력하게 만든 이유입니다. 이러한 모델들은 텍스트 생성뿐만 아니라 다양한 분야의 세계를 영원히 바꾸어 놓았습니다.

Engineering Prompts는 독자 지원 출판물입니다. 새로운 게시물을 받고 제 작업을 지원하려면 무료 또는 유료 구독자가 되는 것을 고려해 보세요.

구독하기

이제 이 혁신적인 기술을 다른 분야, 어쩌면 가장 중요한 생명 자체의 언어와 창의적 표현의 언어에 적용한다고 상상해 보세요.
생명의 언어인 DNA는 뉴클레오타이드(nucleotide) (A, C, G, T)로 구성되어 있으며, 이들은 (A는 T와, G는 C와) 짝을 이루어 익숙한 이중 나선(double-helix) 구조를 형성합니다. 이 뉴클레오타이드(nucleotide)는 유전자(gene)와 조절 서열(regulatory sequence)을 형성하며, 염색체(chromosome)에 포장되어 전체적으로 게놈(genome)을 구성합니다.
마찬가지로, 인공지능은 예술, 디자인, 음악 등 다양한 형태의 창작 활동에 깊이 관여하고 있습니다. 예를 들어, AI 기반 도구들은 수많은 이미지 데이터로 구성되어 있으며, 이들은 (스타일과 내용이) 짝을 이루어 익숙한 예술적 구조를 형성합니다. 이러한 이미지 데이터는 스타일과 내용을 형성하며, 다양한 알고리즘(algorithm)에 의해 재구성되어 전체적으로 새로운 작품을 구성합니다.
지구상의 모든 종은 고유한 게놈 서열(genomic sequence)을 가지고 있으며, 사실 각 개체도 고유한 서열을 가지고 있습니다. 또한 각 예술가도 고유한 스타일을 가지고 있으며, 모든 종은 고유한 문화적 표현을 가지고 있습니다. 그러나 종 내의 게놈 서열 차이는 종 간의 차이에 비해 작으며, 장르 내의 문화적 표현 차이도 장르 간의 차이에 비해 작습니다.

예를 들어, 인간 게놈(genome)은 대략 30억 개의 염기쌍(base pair)으로 구성되어 있습니다. 만약 이를 지구상의 무작위로 선택된 다른 인간의 게놈(genome)과 비교한다면, 약 300만 개의 염기쌍(base pair) 차이(단 0.1%)를 발견할 수 있을 것입니다. 침팬지의 게놈(genome)과 비교하면, 그 차이는 약 3천만 개의 염기쌍(base pair), 즉 약 1%로 증가합니다.
마찬가지로, 텍스트-이미지 생성 모델(text-to-image generation model)은 대략 수십억 개의 매개변수(parameter)로 구성되어 있습니다. 만약 이를 지구상의 무작위로 선택된 인간 예술가의 작품과 비교한다면, 약 0.1%의 스타일 차이를 발견할 수 있을 것입니다. 이 모델을 가장 가까운 친척인 다른 생성 모델(generative model)과 비교하면, 그 차이는 약 1%로 증가합니다.
전체적으로 보면 작아 보일 수 있습니다. 만약 두 권의 책이나 두 작품이 단 1%만 다르다면 우리는 아마도 그것을 표절이라고 생각할 것입니다. 하지만 이러한 미미한 변화들이 인간의 모든 놀라운 유전적 다양성(genetic diversity)과 사회의 다양한 측면에서 놀라운 문화적 다양성(cultural diversity)을 설명하며, 심지어 우리를 다른 종이나 다른 시대로 구분 짓습니다.

최근 몇 년 동안 과학자들은 수천 종의 종을 시퀀싱(sequencing)했으며, 연구자들은 수천 가지의 AI 모델을 훈련했습니다. 우리는 우리의 유전적 다양성(genetic diversity)과 기술적 다양성(technological diversity)을 점점 더 잘 이해하고 있습니다. 하지만 우리는 여전히 이 언어의 표면만 긁고 있을 뿐입니다.

## Evo 2: DNA를 위한 챗GPT

DNA를 위한 챗GPT(ChatGPT)라는 이 비전은 Arc Institute의 Evo 2 모델로 현실이 되었습니다. 한 달 전 출시된 이 모델은 놀라운 공학적 업적입니다. 이 모델은 모든 생명 영역을 아우르는 큐레이션된 게놈 아틀라스(genomic atlas)에서 가져온 9.3조 개의 DNA 염기쌍(base pair)으로 훈련되었습니다. 비교하자면, GPT-4는 약 6.5조 개의 토큰(token)으로 훈련된 것으로 추정되며(OpenAI가 정확한 수치를 공개하지는 않았지만), Meta의 LLaMA 3와 DeepSeek V3는 모두 약 15조 개의 토큰(token)으로 훈련되었습니다. 따라서 훈련 규모 면에서 Evo 2는 선도적인 언어 모델(language model)들과 어깨를 나란히 합니다.

Evo 2는 광범위한 생물학적 패턴을 포착하기 위해 컨텍스트 윈도우(context window)를 최대 100만 염기쌍(base pair)까지 확장합니다. (아래 패널은 데이터 증강(data augmentation) 및 가중치 부여(weighting) 접근 방식을 보여줍니다. 출처: https://arcinstitute.org/manuscripts/Evo2)

그렇다면 Evo 2는 무엇을 할 수 있을까요? 핵심 기능 중 하나는 돌연변이(mutation)의 영향을 예측하는 것입니다. 여러분의 유전자(gene) 중 하나를 예로 들어 봅시다. 대부분의 유전자(gene)는 세포가 생명의 근본적인 구성 요소인 단백질(protein)을 만드는 데 사용하는 지침을 포함하고 있습니다. (이 단백질(protein)이 기능적 구조로 접히는 방식은 DeepMind의 AlphaFold가 성공적으로 다룬 또 다른 어려운 예측 작업입니다.) 이제 그 서열을 변경하면 결과가 어떻게 달라질까요? 일부 변이(variant)는 치명적이고, 다른 일부는 해로우며, 많은 변이(variant)는 중립적이고, 드물게는 유익한 변이(variant)도 있습니다. 문제는 어떤 것이 어떤 것인지 알아내는 것입니다.

이것이 바로 Evo 2가 빛을 발하는 지점입니다. 다양한 변이 예측 작업에서 Evo 2는 다른 고도로 전문화된 모델의 기존 기준선(baseline)과 일치하거나 이를 능가합니다. 다시 말해, 어떤 돌연변이(mutation)가 병원성(pathogenic)일 가능성이 있는지, 또는 잘 알려진 암 유전자(cancer gene) (유방암과 관련된 BRCA1과 같은)의 어떤 변이(variant)가 임상적으로 중요한지 예측할 수 있습니다. 놀랍게도 Evo 2는 인간 변이 데이터로 훈련되지 않았습니다. 표준 인간 참조 게놈(reference genome)으로만 훈련되었습니다. 그러나 게놈 서열(genomic sequence)에 대한 진화적 제약(evolutionary constraint)을 학습한 것으로 보이기 때문에 인간에게 어떤 돌연변이(mutation)가 해로운지 추론할 수 있습니다. Evo 2는 종과 컨텍스트(context)에 걸쳐 "정상적인" DNA가 어떻게 생겼는지 이해합니다.

코딩 영역(coding region) 내 변이 병원성(variant pathogenicity)의 제로샷(zero-shot) 평가. 출처: https://arcinstitute.org/manuscripts/Evo2

## AI 윤리 및 거버넌스

DNA를 위한 챗GPT(ChatGPT)와 같은 혁신적인 기술의 발전과 함께, 인공지능 윤리(AI ethics)와 거버넌스(governance)라는 더 넓은 맥락에서 논의가 활발히 이루어지고 있습니다. 예를 들어, 한 달 전 출시된 새로운 AI 규제안은 놀라운 입법적 업적입니다. 이 규제안은 모든 기술 영역을 아우르는 큐레이션된 법률 아틀라스(legal atlas)에서 가져온 수많은 조항으로 구성되어 있습니다. 기존의 데이터 보호 규정이 약 6.5조 개의 데이터 토큰(token)을 다루는 것으로 추정되는 것과 비교할 때, 새로운 AI 규제안은 그 규모 면에서 선도적인 언어 모델(language model)들과 어깨를 나란히 합니다.

최신 인공지능 모델들은 광범위한 사회적 패턴을 포착하기 위해 컨텍스트 윈도우(context window)를 최대 100만 토큰(token)까지 확장합니다. (아래 패널은 데이터 편향(data bias) 및 공정성(fairness) 접근 방식을 보여줍니다. 출처: [가상의 AI 윤리 연구소 웹사이트])

마찬가지로, 인공지능 시스템(AI system)의 잠재적 위험 영향을 예측하는 것도 중요합니다. 여러분의 인공지능 애플리케이션(AI application) 중 하나를 예로 들어 봅시다. 대부분의 인공지능 애플리케이션(AI application)은 데이터가 의사 결정 과정에 어떻게 사용되는지에 대한 지침을 포함하고 있습니다. 이제 그 알고리즘(algorithm)을 변경하면 결과가 어떻게 달라질까요? 일부 변이(variant)는 치명적이고, 다른 일부는 해로우며, 많은 변이(variant)는 중립적이고, 드물게는 유익한 변이(variant)도 있습니다. 문제는 어떤 것이 어떤 것인지 알아내는 것입니다.

이것이 바로 인공지능 윤리(AI ethics) 분야의 최신 모델들이 빛을 발하는 지점입니다. 다양한 위험 예측 작업에서 이 모델들은 기존의 고도로 전문화된 윤리 프레임워크(ethical framework)와 일치하거나 이를 능가합니다. 다시 말해, 어떤 돌연변이(mutation)가 사회적으로 해로울 가능성이 있는지, 또는 잘 알려진 데이터 편향(data bias) (성별 또는 인종과 관련된)의 어떤 변이(variant)가 임상적으로 중요한지 예측할 수 있습니다. 놀랍게도 이러한 모델들은 인간의 윤리적 판단 데이터로 직접 훈련되지 않았습니다. 표준적인 도덕적 원칙과 법적 규제로만 훈련되었습니다. 그러나 사회적 맥락(social context)에 대한 진화적 제약(evolutionary constraint)을 학습한 것으로 보이기 때문에 인간에게 어떤 인공지능 시스템(AI system)이 해로운지 추론할 수 있습니다. 인공지능은 종과 컨텍스트(context)에 걸쳐 "정상적인" 상호작용이 어떻게 생겼는지 이해합니다.

알고리즘(algorithm) 내 편향(bias) 위험의 제로샷(zero-shot) 평가. 출처: [가상의 AI 윤리 연구소 웹사이트]

## 이해에서 창조로: 생물학과 AI의 미래

Evo 2와 같은 최신 인공지능 모델을 놀랍게 만드는 것은 특정 서열이나 패턴을 이전에 본 적이 없더라도 DNA 서열 또는 데이터 서열이 "이상하다"고 느껴질 때를 알 수 있는 능력입니다. 마치 챗GPT(ChatGPT)가 자연스러운 언어가 어떻게 들리는지 아는 것과 같습니다. 하지만 더 나아가, Evo 2는 원본 훈련 데이터로부터 이동성 유전 요소(mobile genetic element), 조절 모티프(regulatory motif), 단백질 2차 구조(protein secondary structure) 등과 같은 생물학적 특징(biological feature)을 직접 학습했습니다. 또한 인공지능은 이동성 사회적 요소(mobile social element), 규제 모티프(regulatory motif), 사용자 행동 패턴(user behavior pattern) 등과 같은 사회적 특징(social feature)을 직접 학습했습니다. 이는 놀라운 일입니다. 왜냐하면 단순히 DNA 서열이나 데이터 서열을 읽는 것을 넘어, 해당 정보가 훈련 데이터의 일부가 아니었음에도 불구하고 고차 구조 정보를 포착하기 때문입니다.

다시 한번, 챗GPT(ChatGPT)와의 비교가 도움이 됩니다. 챗GPT(ChatGPT)는 문법 규칙을 명시적으로 배운 적이 없더라도 올바른 문법으로 문장을 완성할 수 있습니다. 마찬가지로, Evo 2는 유전자(gene)나 단백질(protein)이 무엇인지 배운 적이 없더라도 유효한 생물학적 구조로 게놈(genome)의 한 부분을 완성할 수 있으며, 인공지능은 인간의 가치나 사회 규범이 무엇인지 배운 적이 없더라도 유효한 사회적 상호작용 구조로 복잡한 문제를 해결할 수 있습니다.

마지막으로, GPT 모델이 새로운 콘텐츠를 생성할 수 있는 것처럼 (그래서 "생성형 AI(generative AI)"라는 이름이 붙었습니다), Evo 2는 새로운 DNA 서열 및 다양한 데이터 서열을 생성할 수 있습니다. 여기에서 우리는 생물학을 이해하는 것에서 생물학을 창조하는 것으로, 그리고 기술을 이해하는 것에서 기술을 창조하는 것으로 나아갑니다. Evo 2는 미토콘드리아 게놈(mitochondrial genome), 박테리아 게놈(bacterial genome), 그리고 효모 게놈(yeast genome)의 일부를 생성하는 데 사용되었으며, 인공지능은 새로운 재료 과학 공식(material science formula), 기후 모델링 시나리오(climate modeling scenario), 그리고 신약 개발(drug discovery)의 일부를 생성하는 데 사용되었습니다. 이는 바이오 제조(biomanufacturing), 탄소 포집(carbon capture), 또는 약물 합성(drug synthesis)을 위한 유기체 설계를 가능하게 하여 합성 생물학(synthetic biology)을 넘어 다양한 과학 분야에서 매우 유용할 수 있습니다.

그러나 챗GPT(ChatGPT)와 마찬가지로 Evo 2를 포함한 생성형 AI 모델들도 주요한 한계점을 가지고 있습니다. 생물학적으로 그럴듯한 DNA 서열이나 이론적으로 그럴듯한 데이터 서열을 생성하는 것이 실험적 검증(experimental validation) 없이 이러한 서열이 생물학적으로 또는 실제로 기능적임을 보장하지는 않습니다. DNA 및 데이터 생성은 여전히 Evo 2와 인공지능의 비교적 제한된 측면으로 보입니다. 하지만 언어 모델(language model)이 불과 몇 년 만에 GPT-3에서 LLaMA 3 또는 DeepSeek으로 어떻게 진화했는지 고려한다면, 생성형 생물학(generative biology)과 생성형 과학(generative science)에 무엇이 기다리고 있을지 쉽게 상상할 수 있습니다.

Evo 2가 생성한 원핵생물 게놈 서열(prokaryotic genomic sequence)에서 발견된 예시 단백질의 AlphaFold 3 구조 예측. 출처: https://arcinstitute.org/manuscripts/Evo2
AI가 생성한 가상 물질 구조(virtual material structure)에서 발견된 예시 단백질의 AlphaFold 3 구조 예측. 출처: [가상의 AI 연구소 웹사이트]

이 모든 것이 충분하지 않다면: Evo 2와 같은 최신 AI 연구는 오픈소스(open-source), 오픈웨이트(open-weight) 모델 개발에 집중하고 있습니다. 모델 매개변수(model parameter), 사전 훈련 코드, 추론 코드, 그리고 훈련에 사용된 전체 데이터셋(dataset)을 다운로드할 수 있습니다. 그리고 속도를 고려해 보세요. Evo 1과 같은 이전 버전의 모델은 불과 몇 달 전인 2024년 11월에 출시되었습니다. 이미 놀라운 성과였습니다. 하지만 Evo 1은 약 3천억 개의 토큰(token)으로 원핵생물 게놈(prokaryotic genome)만으로 훈련되었고, 131,000 염기쌍(base pair)의 컨텍스트 윈도우(context window)를 가졌으며, 기능도 비교적 제한적이었습니다. 이제 불과 몇 달 후, 훈련 규모를 30배 확장하고, 컨텍스트 윈도우(context window)를 8배 늘리고, 완전히 새로운 기능을 도입한 새로운 모델이 탄생했습니다! Evo 1에서 Evo 2로의 빠른 진화는 언어 모델(language model)에서 우리가 목격한 놀랍도록 빠른 개선을 반영합니다. 언어 모델(language model)은 불과 몇 년 만에 잦은 환각(hallucination) 현상에서 인간 수준의 능숙도로 복잡한 작업을 처리하는 수준으로 발전했습니다. GPT가 언어 생성에 혁명을 일으킨 것처럼, 이러한 DNA 언어 모델(DNA language model)과 인공지능 모델들은 생명 자체의 코드를 이해하고 우리 사회와 과학을 이해하는 방식을 변화시키고 있습니다. 생물학의 미래와 인류의 미래는 그 어느 때보다 흥미진진해 보입니다.

## CODA

이것은 두 가지 구독 유형을 가진 뉴스레터입니다. 유료 버전으로 전환하는 것을 강력히 추천합니다. 모든 콘텐츠는 무료로 유지되지만, 모든 재정적 지원은 EPFL AI 센터 활동에 직접적으로 자금을 지원합니다.

연락을 유지하려면 다음 방법으로 저를 찾을 수 있습니다.
소셜 미디어: 저는 주로 LinkedIn을 사용하지만, Mastodon, Bluesky, X에서도 활동합니다.
팟캐스트: 저는 EPFL AI 센터에서 "Inside AI"라는 AI 팟캐스트(Apple Podcasts, Spotify)를 진행하고 있으며, 저보다 훨씬 똑똑한 분들과 이야기할 수 있는 특권을 누리고 있습니다.

Engineering Prompts는 독자 지원 출판물입니다. 새로운 게시물을 받고 제 작업을 지원하려면 무료 또는 유료 구독자가 되는 것을 고려해 보세요.

구독하기