# **AI 기업들은 신을 만드는 것에서 제품을 구축하는 것으로 방향을 틀고 있다. 좋다.**

Author: AI Snake Oil
URL: https://www.normaltech.ai/p/ai-companies-are-pivoting-from-creating

============================================================

AI 기업들은 하드웨어와 데이터 센터에 총 1조 달러를 지출할 계획이지만, 아직까지 그에 상응하는 성과는 상대적으로 미미합니다. 이로 인해 생성형 AI(generative AI)가 거품이라는 우려의 목소리가 커지고 있습니다. 우리는 앞으로 어떤 일이 일어날지에 대한 예측을 내놓지 않을 것입니다. 하지만 우리는 상황이 여기까지 오게 된 경위에 대해 확실한 진단을 내릴 수 있다고 생각합니다. 이 게시물에서 우리는 AI 기업들이 저지른 실수와 이를 어떻게 수정하려고 노력해왔는지 설명합니다. 그런 다음, 생성형 AI가 투자를 정당화할 만큼 상업적으로 성공하기 위해 아직 극복해야 할 다섯 가지 장벽에 대해 이야기할 것입니다.

**제품-시장 적합성(Product-market fit)**
ChatGPT가 출시되었을 때, 사람들은 그것의 수많은 예상치 못한 용도를 발견했습니다. 이는 AI 개발자들을 지나치게 흥분시켰습니다. 그들은 시장을 완전히 오해했고, 개념 증명(proofs of concept)과 신뢰할 수 있는 제품 사이의 거대한 격차를 과소평가했습니다. 이러한 오해는 거대 언어 모델(LLMs)을 상업화하는 데 있어 상반되지만 똑같이 결함이 있는 두 가지 접근 방식으로 이어졌습니다.

OpenAI와 Anthropic은 모델 구축에 집중하고 제품에 대해서는 신경 쓰지 않았습니다. 예를 들어, OpenAI가 ChatGPT iOS 앱을 출시하는 데 6개월, 안드로이드 앱을 출시하는 데 8개월이 걸렸습니다! 구글과 마이크로소프트는 어떤 제품이 실제로 AI의 혜택을 받을 수 있을지, 그리고 AI가 어떻게 통합되어야 하는지 고려하지 않은 채, 당황한 듯 모든 것에 AI를 밀어 넣었습니다. 두 회사 그룹 모두 "사람들이 원하는 것을 만들어라"는 만트라를 잊었습니다. LLM의 일반성(generality)은 개발자들이 모델에 작업을 수행하도록 프롬프트(prompt)하는 것이 신중하게 설계된 제품이나 기능을 대체하는 것처럼, 제품-시장 적합성(product-market fit)을 찾을 필요가 없다고 스스로를 속이도록 만들었습니다. OpenAI와 Anthropic의 DIY 접근 방식은 LLM의 초기 채택자들이 새로운 기술을 자신들의 목적에 맞게 조정하는 방법을 알아내는 데 더 많은 투자를 하기 때문에, 일반 사용자들은 사용하기 쉬운 제품을 원하는 반면, 불량 사용자(bad actors)인 경향이 불균형적으로 높았다는 것을 의미했습니다. 이는 기술에 대한 대중의 인식을 나쁘게 만드는 데 기여했습니다. 1 한편, 마이크로소프트와 구글의 'AI를 전면에 내세우는(AI-in-your-face)' 접근 방식은 가끔 유용하지만 더 자주 성가신 기능들로 이어졌습니다. 또한 마이크로소프트의 초기 시드니(Sydney) 챗봇과 구글의 제미니(Gemini) 이미지 생성기처럼 부적절한 테스트로 인해 많은 불필요한 오류(unforced errors)를 초래했습니다. 이는 또한 반발을 불러일으켰습니다.

하지만 기업들은 방식을 바꾸고 있습니다. OpenAI는 투기적인 미래에 초점을 맞춘 연구소에서 일반적인 제품 회사와 유사한 형태로 전환하고 있는 것으로 보입니다. OpenAI 이사회 드라마에서 모든 인간적인 요소를 제외한다면, 그것은 근본적으로 신을 창조하는 것에서 제품을 만드는 것으로의 회사 전환에 관한 것이었습니다. Anthropic 또한 제품을 만들 필요성을 인식했지만, OpenAI에서 인공 일반 지능(artificial general intelligence)에 더 관심을 가지고 OpenAI에서 소외감을 느꼈던 많은 연구원과 개발자들을 영입하고 있습니다. 구글과 마이크로소프트는 배우는 속도가 느리지만, 애플이 그들을 변화시키도록 강요할 것이라고 우리는 추측합니다. 작년에 애플은 AI 분야에서 뒤처진 것으로 여겨졌지만, WWDC(개발자 컨퍼런스)에서 애플이 선보인 느리고 사려 깊은 접근 방식이 사용자들에게 더 큰 공감을 얻을 가능성이 높다는 것이 돌이켜보면 분명해 보입니다. 2 구글은 검색에 AI를 통합하는 것보다 다가오는 픽셀(Pixel) 폰과 안드로이드(Android)에 AI를 통합하는 데 더 많은 생각을 기울인 것으로 보이지만, 아직 폰이 출시되지 않았으니 지켜봐야 할 것입니다. 그리고 메타(Meta)는 AI를 사용하여 광고 기반 소셜 미디어 플랫폼에서 콘텐츠와 참여를 생성하는 비전을 가지고 있습니다. AI 생성 콘텐츠가 넘쳐나는 세상의 사회적 함의는 양날의 검(double-edged)이지만, 비즈니스 관점에서는 합리적입니다. 당신은 우리의 책에 대한 뉴스레터인 AI Snake Oil을 읽고 있습니다. 새로운 게시물을 받으려면 구독하세요. 구독

**소비자 AI를 위한 5가지 주요 과제**
개발자들이 매력적인 AI 기반 소비자 제품을 만들기 위해 해결해야 할 LLM의 다섯 가지 한계가 있습니다. 3 (이 중 많은 부분을 8월 29일에 열리는 유용하고 신뢰할 수 있는 AI 에이전트(AI agents) 구축에 관한 온라인 워크숍에서 논의할 예정입니다.)

**1. 비용(Cost)**
역량(capability)이 장벽이 아니라 비용이 장벽인 애플리케이션이 많습니다. 간단한 채팅 애플리케이션에서도 비용 문제는 봇이 얼마나 많은 대화 기록을 추적할 수 있는지를 결정합니다. 대화가 길어질수록 모든 응답에 대해 전체 기록을 처리하는 것은 빠르게 엄청나게 비싸집니다. 비용 면에서는 빠른 발전이 있었습니다. 지난 18개월 동안 동등한 역량(equivalent-capability)에 대한 비용이 100배 이상 감소했습니다. 4 그 결과, 기업들은 LLM이 "측정하기에는 너무 저렴하다(too cheap to meter)"고 주장하거나 곧 그렇게 될 것이라고 말합니다. 글쎄요, API(Application Programming Interface)를 무료로 만들 때 우리는 그것을 믿을 것입니다. 더 심각하게는, 비용이 계속해서 문제가 될 것이라고 우리가 생각하는 이유는 많은 애플리케이션에서 비용 개선이 정확도 개선으로 직접 연결되기 때문입니다. 이는 LLM의 무작위성(randomness)을 고려할 때, 작업을 수십, 수천, 심지어 수백만 번 반복해서 재시도하는 것이 성공 가능성을 높이는 좋은 방법임이 밝혀졌기 때문입니다. 따라서 모델이 저렴할수록 주어진 예산으로 더 많은 재시도를 할 수 있습니다. 우리는 에이전트(agents)에 대한 최근 논문에서 이를 정량화했습니다. 그 이후로 많은 다른 논문들도 비슷한 주장을 했습니다. 그렇긴 하지만, 대부분의 애플리케이션에서 비용 최적화(cost optimization)가 심각한 문제가 되지 않는 지점에 곧 도달할 가능성도 있습니다.

**2. 신뢰성(Reliability)**
우리는 역량(capability)과 신뢰성(reliability)을 다소 직교적(orthogonal)이라고 봅니다. AI 시스템이 90%의 시간 동안 작업을 올바르게 수행한다면, 우리는 그것이 작업을 수행할 역량은 있지만 신뢰할 수 있게(reliably) 수행할 수는 없다고 말할 수 있습니다. 90%에 도달하게 하는 기술이 100%에 도달하게 할 가능성은 낮습니다. 통계 학습(statistical learning) 기반 시스템에서는 완벽한 정확도(perfect accuracy)를 달성하는 것이 본질적으로 어렵습니다. 광고 타겟팅(ad targeting)이나 사기 탐지(fraud detection), 또는 최근의 일기 예보와 같은 기계 학습(machine learning)의 성공 사례를 생각해보면, 완벽한 정확도가 목표는 아닙니다. 시스템이 최첨단(state of the art)보다 낫다면 유용합니다. 의료 진단 및 기타 헬스케어 기술(healthcare applications)에서도 우리는 많은 오류를 용인합니다. 그러나 개발자들이 소비자 제품에 AI를 넣을 때, 사람들은 AI가 소프트웨어처럼 작동하기를 기대하며, 이는 AI가 결정론적(deterministically)으로 작동해야 한다는 것을 의미합니다. AI 여행 에이전트(travel agent)가 90%의 시간 동안만 올바른 목적지로 휴가를 예약한다면, 성공적이지 못할 것입니다. 이전에 우리가 썼듯이, 신뢰성 한계는 최근 AI 기반 기기들의 실패를 부분적으로 설명합니다. AI 개발자들은 이를 인식하는 데 느렸는데, 전문가로서 우리는 AI를 전통적인 소프트웨어와 근본적으로 다르게 개념화하는 데 익숙하기 때문입니다. 예를 들어, 우리 둘은 일상 업무에서 챗봇(chatbots)과 에이전트(agents)를 많이 사용하며, 이러한 도구들의 환각(hallucinations)과 신뢰성 부족을 우회하는 것이 거의 자동화되었습니다. 1년 전, AI 개발자들은 비전문가 사용자들이 AI에 적응하는 법을 배울 것이라고 희망하거나 가정했지만, 대신 기업들이 사용자 기대에 맞춰 AI를 조정하고 AI가 전통적인 소프트웨어처럼 작동하도록 만들어야 한다는 것이 점차 분명해졌습니다. 신뢰성 향상은 프린스턴(Princeton)에 있는 우리 팀의 연구 관심사입니다. 현재로서는 확률적 구성 요소(stochastic components, LLMs)로 결정론적 시스템(deterministic systems)을 구축하는 것이 가능한지는 근본적으로 미해결 문제입니다. 일부 기업들은 신뢰성을 해결했다고 주장했습니다. 예를 들어, 법률 기술(legal tech) 공급업체들은 "환각 없는(hallucination-free)" 시스템을 선전했습니다. 그러나 이러한 주장은 시기상조임이 드러났습니다.

**3. 프라이버시(Privacy)**
역사적으로 기계 학습(machine learning)은 광고 타겟팅(ad targeting)을 위한 브라우징 기록이나 헬스케어 기술(health tech)을 위한 의료 기록과 같은 민감한 데이터 소스에 의존하는 경우가 많았습니다. 이러한 의미에서 LLM은 웹 페이지나 책과 같은 공개 소스(public sources)로 주로 훈련되기 때문에 다소 이례적입니다. 5 그러나 AI 비서(AI assistants)와 함께 프라이버시(privacy) 문제가 다시 크게 부각되었습니다. 유용한 비서를 구축하려면 기업들은 사용자 상호작용(user interactions)을 기반으로 시스템을 훈련해야 합니다. 예를 들어, 이메일 작성에 능숙하려면 모델이 이메일로 훈련되는 것이 매우 도움이 될 것입니다. 기업들의 프라이버시 정책은 이에 대해 모호하며, 어느 정도까지 이러한 일이 일어나고 있는지는 불분명합니다. 6 이메일, 문서, 스크린샷 등은 채팅 상호작용보다 잠재적으로 훨씬 더 민감합니다. 훈련(training)보다는 추론(inference)과 관련된 독특한 유형의 프라이버시 문제가 있습니다. 비서가 우리에게 유용한 일을 하려면 우리의 개인 데이터에 접근할 수 있어야 합니다. 예를 들어, 마이크로소프트는 코파일럿(CoPilot) AI에 사용자 활동에 대한 기억을 제공하기 위해 사용자의 PC 스크린샷을 몇 초마다 찍는 논란의 여지가 있는 기능을 발표했습니다. 그러나 거센 반발이 있었고 회사는 철회했습니다. 우리는 "데이터가 장치를 떠나지 않는다"와 같은 순전히 기술적인 프라이버시 해석에 대해 경고합니다. 메러디스 휘태커(Meredith Whittaker)는 온디바이스(on-device) 사기 탐지(fraud detection)가 상시 감시(always-on surveillance)를 정상화하고, 해당 인프라가 더 억압적인 목적으로 재활용될 수 있다고 주장합니다. 그렇긴 하지만, 기술 혁신은 분명히 도움이 될 수 있습니다.

**4. 안전 및 보안(Safety and security)**
안전 및 보안과 관련하여 여러 가지 우려 사항이 있습니다: 제미니(Gemini)의 이미지 생성 편향과 같은 의도치 않은 실패; 음성 복제(voice cloning) 또는 딥페이크(deepfakes)와 같은 AI 오용; 그리고 사용자 데이터를 유출하거나 다른 방식으로 사용자에게 해를 끼칠 수 있는 프롬프트 인젝션(prompt injection)과 같은 해킹. 우리는 우발적인 실패는 고칠 수 있다고 생각합니다. 대부분의 오용 유형에 대해서는, 오용될 수 없는 모델을 만들 방법이 없으므로 방어는 주로 하류(downstream)에 위치해야 한다는 것이 우리의 견해입니다. 물론 모든 사람이 동의하는 것은 아니므로, 기업들은 불가피한 오용으로 인해 계속해서 나쁜 평판을 얻겠지만, 그들은 이를 사업 비용으로 받아들인 것으로 보입니다. 세 번째 범주인 해킹에 대해 이야기해 봅시다. 우리가 알기로는, 기업들이 가장 적게 주의를 기울이는 부분인 것 같습니다. 적어도 이론적으로는, 사용자 간에 확산되어 해당 사용자들의 AI 비서(AI assistants)를 속여 웜(worm)의 추가 복사본을 생성하는 등 해로운 일을 하도록 만드는 AI 웜(AI worms)과 같은 치명적인 해킹이 가능합니다. 배포된 제품에서 이러한 취약점(vulnerabilities)을 밝혀낸 수많은 개념 증명(proof-of-concept) 시연과 버그 바운티(bug bounties)가 있었지만, 우리는 이러한 유형의 공격이 실제 환경에서 발생한 것을 보지 못했습니다. 이것이 AI 비서의 낮은 채택률 때문인지, 아니면 기업들이 급조한 어설픈 방어책(clumsy defenses)이 충분했기 때문인지, 아니면 다른 이유 때문인지는 확실하지 않습니다. 시간만이 말해줄 것입니다.

**5. 사용자 인터페이스(User interface)**
많은 애플리케이션에서 LLM의 신뢰성 부족은 봇이 엉뚱한 방향으로 갈 경우 사용자가 개입할 수 있는 어떤 방법이 있어야 함을 의미합니다. 챗봇(chatbot)에서는 답변을 다시 생성하거나 여러 버전을 보여주고 사용자가 선택하도록 하는 것만큼 간단할 수 있습니다. 그러나 항공권 예약과 같이 오류가 큰 비용을 초래할 수 있는 애플리케이션에서는 적절한 감독을 보장하는 것이 더 까다로우며, 시스템은 너무 많은 방해로 사용자를 짜증 나게 하는 것을 피해야 합니다. 사용자가 비서에게 말하고 비서가 다시 말하는 자연어 인터페이스(natural language interfaces)에서는 문제가 훨씬 더 어렵습니다. 여기에 생성형 AI(generative AI)의 많은 잠재력이 있습니다. 한 가지 예로, 외국어 간판을 응시하고 있다는 것을 감지하는 것처럼, 필요할 때 요청하지 않아도 안경 속으로 사라져 당신에게 말을 거는 AI는 오늘날 우리가 가진 것과는 완전히 다른 경험일 것입니다. 그러나 제한된 사용자 인터페이스(user interface)는 잘못되거나 예상치 못한 행동에 대한 여지를 거의 남기지 않습니다.

**결론**
AI 지지자들은 AI 역량(AI capabilities)의 빠른 개선 속도 때문에 곧 엄청난 사회적, 경제적 효과를 보게 될 것이라고 종종 주장합니다. 우리는 그러한 역량 예측에 들어가는 추세 외삽(trend extrapolation)과 부주의한 사고방식에 회의적입니다. 더 중요하게는, AI 역량이 빠르게 개선된다 하더라도 개발자들은 위에서 논의된 과제들을 해결해야 합니다. 이들은 순전히 기술적인 것이 아니라 사회기술적(sociotechnical)인 문제이므로, 진전은 느릴 것입니다. 그리고 그러한 과제들이 해결된다 하더라도, 조직들은 AI를 기존 제품과 워크플로우(workflows)에 통합하고, 그 함정(pitfalls)을 피하면서 사람들이 생산적으로 AI를 사용하도록 훈련시켜야 합니다. 우리는 이것이 1, 2년이 아니라 10년 이상의 시간 척도(timescale)로 일어날 것으로 예상해야 합니다.

**추가 자료 읽기**
베네딕트 에반스(Benedict Evans)는 범용 언어 모델(general-purpose language models)을 사용하여 단일 목적 소프트웨어(single-purpose software)를 구축하는 것의 중요성에 대해 썼습니다.
1 분명히 말하자면, 우리는 최첨단(state-of-the-art) 모델에 대한 접근을 줄이는 것이 오용을 줄일 것이라고 생각하지 않습니다. 그러나 LLM의 경우, 오용은 합법적인 사용(생각을 필요로 함)보다 쉽기 때문에 오용이 널리 퍼진 것은 놀라운 일이 아닙니다.
2 AI 채택 속도는 상대적입니다. 애플이 제품에 AI를 통합하는 방식조차도 너무 빠르다는 비판을 받았습니다.
3 이들은 사용자 경험(user experience)에 중요한 요소들에 관한 것입니다. 우리는 환경 비용, 저작권이 있는 데이터로 훈련하는 것 등은 제외하고 있습니다.
4 예를 들어, API의 GPT-3.5 (text-davinci-003)는 백만 토큰(tokens)당 20달러였지만, 더 강력한 gpt-4o-mini는 단 15센트입니다.
5 분명히 말하자면, 데이터 소스가 공개적이라고 해서 프라이버시 문제가 없는 것은 아닙니다.
6 예를 들어, 구글은 "우리는 구글의 AI 모델을 훈련하는 데 도움이 되도록 공개적으로 사용 가능한 정보를 사용합니다"라고 말합니다. 다른 곳에서는 이메일과 같은 개인 데이터를 사용하여 서비스를 제공하고, 서비스를 유지 및 개선하며, 서비스를 개인화하고, 새로운 서비스를 개발할 수 있다고 말합니다. 이러한 공개 내용과 일치하는 한 가지 접근 방식은 제미니(Gemini)와 같은 모델의 사전 훈련(pre-training)에는 공개 데이터만 사용되지만, 이메일 자동 응답 봇(auto-response bot)과 같은 것을 만들기 위해 해당 모델을 미세 조정(fine-tune)하는 데는 개인 데이터가 사용된다는 것입니다. 우리가 아는 한 Anthropic은 유일한 예외입니다. Anthropic은 "사용자가 명시적인 허가를 주지 않는 한, 우리는 사용자 제출 데이터로 생성형 모델을 훈련하지 않습니다. 현재까지 우리는 고객 또는 사용자 제출 데이터를 생성형 모델 훈련에 사용하지 않았습니다"라고 말합니다. 프라이버시에 대한 이러한 약속은 칭찬할 만하지만, 만약 Anthropic이 제품 구축을 더 전적으로 수용한다면 회사에 불리하게 작용할 것이라고 우리는 예측합니다.