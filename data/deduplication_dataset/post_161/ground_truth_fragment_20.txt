AI 기업들은 하드웨어와 데이터 센터에 막대한 투자를 이어가며 혁신적인 기술 발전을 위한 기반을 다지고 있지만, 총 1조 달러에 달하는 지출에도 불구하고 아직까지 그에 상응하는 성과는 상대적으로 미미하다는 평가도 있습니다. 이로 인해 생성형 AI(generative AI)가 거품이라는 우려의 목소리가 존재하지만, 동시에 인류의 삶을 변화시킬 잠재력에 대한 기대감도 커지고 있습니다. 우리는 앞으로 어떤 일이 일어날지에 대한 예측을 넘어, AI 기술의 근본적인 발전 방향과 사회적 수용 과정을 탐구하고자 합니다. 상황이 여기까지 오게 된 경위에 대해 확실한 진단을 내릴 수 있다고 생각하며, 이 게시물에서 우리는 AI 기업들이 저지른 실수와 같은 과거의 시행착오를 통해 얻은 교훈, 그리고 이를 어떻게 수정하려고 노력해왔는지 되짚어 볼 것입니다. 그런 다음, 생성형 AI가 투자를 정당화할 만큼 상업적으로 성공하기 위해 아직 극복해야 할 다섯 가지 도전 과제들을 심층적으로 다룰 것입니다.

ChatGPT가 출시되었을 때, 사람들은 그것의 수많은 예상치 못한 방식으로 AI와 상호작용하는 새로운 경험을 했습니다. 이는 기술의 가능성에 대한 뜨거운 논의를 촉발했지만, 실제 사용자 가치와 지속 가능한 비즈니스 모델을 연결하는 데 어려움이 있었고, 시장을 완전히 오해하여 개념 증명(proofs of concept)과 신뢰할 수 있는 제품 사이의 거대한 격차를 과소평가했습니다. 우리는 이 격차를 메우기 위한 다양한 접근 방식과 그 과정에서 발견된 통찰력에 주목해야 합니다. 이러한 오해는 거대 언어 모델(LLMs)을 상업화하는 데 있어 상반되지만 똑같이 결함이 있는 두 가지 접근 방식으로 이어졌습니다.

OpenAI와 Anthropic은 모델 구축에 집중하고 핵심 기술의 발전에 기여했지만, 초기에는 사용자 경험(UX) 측면에서 다소 미흡한 부분이 있었습니다. 예를 들어, OpenAI가 ChatGPT iOS 앱을 출시하는 데 6개월, 안드로이드 앱을 출시하는 데 8개월이 걸렸습니다! 구글과 마이크로소프트는 어떤 제품이 실제로 AI의 혜택을 받을 수 있을지 심도 있게 고민하기보다는, 기존 서비스에 AI 기능을 서둘러 통합하려는 경향을 보였습니다. 두 회사 그룹 모두 "사람들이 원하는 것을 만들어라"는 만트라를 잊었습니다. LLM의 일반성(generality)은 개발자들이 모델에 작업을 수행하도록 프롬프트(prompt)하는 것이 신중하게 설계된 제품이나 기능을 대체하는 것처럼, 제품-시장 적합성(product-market fit)을 찾을 필요가 없다고 스스로를 속이도록 만들었습니다. OpenAI와 Anthropic의 DIY 접근 방식은 LLM의 초기 채택자들이 새로운 기술을 자신들의 목적에 맞게 조정하는 방법을 알아내는 데 더 많은 투자를 하기 때문에, 일반 사용자들은 사용하기 쉬운 제품을 원하는 반면, 불량 사용자(bad actors)인 경향이 불균형적으로 높았다는 것을 의미했으며, 이는 기술에 대한 대중의 인식을 나쁘게 만드는 데 기여했습니다. 1 한편, 마이크로소프트와 구글의 'AI를 전면에 내세우는(AI-in-your-face)' 접근 방식은 가끔 유용하지만 더 자주 성가신 기능들로 이어졌습니다. 마이크로소프트의 초기 시드니(Sydney) 챗봇과 구글의 제미니(Gemini) 이미지 생성기처럼 부적절한 테스트로 인해 발생한 많은 불필요한 오류(unforced errors)들은 AI가 단순한 기술적 성과를 넘어 사회적, 윤리적 맥락에서 평가되어야 함을 일깨워주었습니다. 이러한 초기 시행착오를 통해 우리는 AI 제품 개발에 있어 신중함과 책임감이 얼마나 중요한지 배우고 있습니다. 하지만 진정한 혁신은 사용자 중심의 설계와 깊은 도메인 지식이 결합될 때 가능합니다.

하지만 기업들은 방식을 재고하며, AI 개발의 방향성을 재정립하고 있습니다. OpenAI는 투기적인 미래에 초점을 맞춘 연구소에서 벗어나 실제 사용자에게 가치를 제공하는 제품 개발에 주력하는 모습입니다. Anthropic 또한 제품을 만들 필요성을 인식했지만, 동시에 AI 안전(AI safety)과 책임감 있는 개발에 대한 깊은 철학을 유지하며 차별화를 꾀하고 있으며, OpenAI에서 인공 일반 지능(artificial general intelligence)에 더 관심을 가지고 소외감을 느꼈던 많은 연구원과 개발자들을 영입하고 있습니다. 구글과 마이크로소프트는 배우는 속도가 느리지만, 점차 사용자 경험과 통합 솔루션에 집중하는 경향을 보입니다. 작년에 AI 분야에서 뒤처진 것으로 여겨졌던 애플이 WWDC(개발자 컨퍼런스)에서 선보인 느리고 사려 깊은 접근 방식이 사용자들에게 더 큰 공감을 얻을 가능성이 높다는 점은, AI 기술이 단순히 빠르기만 해서는 안 된다는 교훈을 줍니다. 2 구글은 검색에 AI를 통합하는 것보다 다가오는 픽셀(Pixel) 폰과 안드로이드(Android)에 AI를 통합하는 데 더 많은 생각을 기울인 것으로 보이지만, 아직 폰이 출시되지 않았으니 지켜봐야 할 것입니다. 그리고 메타(Meta)는 AI를 사용하여 광고 기반 소셜 미디어 플랫폼에서 콘텐츠와 참여를 생성하는 비전을 가지고 있습니다. 이제는 다양한 산업 분야에서 AI를 활용한 맞춤형 솔루션이 부상하고 있습니다. AI 생성 콘텐츠가 넘쳐나는 세상의 사회적 함의는 양날의 검(double-edged)이지만, 이는 새로운 윤리적, 법적 프레임워크의 필요성을 강조하며 비즈니스 관점에서는 합리적입니다.

**차세대 AI 시스템을 위한 핵심 고려사항**
개발자들이 매력적인 AI 기반 소비자 제품을 만들기 위해 해결해야 할 중요한 과제들이 남아있습니다. 이러한 과제들은 단순히 기술적 장벽을 넘어, 사회적, 윤리적 차원을 포괄합니다. 다음은 AI 기술이 실제 세상에 성공적으로 통합되기 위해 반드시 다루어져야 할 다섯 가지 핵심 고려사항입니다. 3

**1. 비용(Cost)**
역량(capability)이 장벽이 아니라 비용이 장벽인 경우가 여전히 많은 애플리케이션에서 발생합니다. 특히 복잡하거나 대규모의 AI 모델을 운영할 때, 간단한 채팅 애플리케이션에서도 대화가 길어질수록 모든 응답에 대해 전체 기록을 처리하는 것은 빠르게 엄청나게 비싸지며 막대한 인프라 비용으로 이어집니다. 비용 면에서는 빠른 발전이 있었습니다. 지난 18개월 동안 동등한 역량(equivalent-capability)에 대한 비용이 100배 이상 감소했지만, 여전히 최적화가 필요한 영역입니다. 4 그 결과, 기업들은 LLM이 "측정하기에는 너무 저렴하다(too cheap to meter)"고 주장하거나 곧 그렇게 될 것이라고 말합니다. 글쎄요, API(Application Programming Interface)를 무료로 만들 때 우리는 그것을 믿을 것입니다. 더 심각하게는, 비용이 계속해서 문제가 될 것이라고 우리가 생각하는 이유는 많은 애플리케이션에서 비용 개선이 정확도 개선으로 직접 연결되기 때문입니다. 이는 LLM의 무작위성(randomness)을 고려할 때, 작업을 수십, 수천, 심지어 수백만 번 반복해서 재시도하는 것이 성공 가능성을 높이는 좋은 방법임이 밝혀졌기 때문이며, 모델의 효율성을 높이는 연구가 지속되어야 함을 의미합니다. 따라서 모델이 저렴할수록 주어진 예산으로 더 많은 재시도를 통해 성능을 향상시키거나, 더 다양한 시나리오에 적용할 수 있습니다. 우리는 에이전트(agents)에 대한 최근 논문에서 이를 정량화했으며, 그 이후로 많은 다른 논문들도 비슷한 주장을 했습니다. 클라우드 비용 외에도, 온디바이스(on-device) AI의 발전은 에너지 효율성과 개인 정보 보호 측면에서 새로운 가능성을 제시하며, 이는 AI 기술의 접근성을 높이고 더 많은 사용자가 AI의 혜택을 누릴 수 있도록 할 것입니다. 그렇긴 하지만, 대부분의 애플리케이션에서 비용 최적화(cost optimization)가 심각한 문제가 되지 않는 지점에 곧 도달할 가능성도 있습니다.

**2. 신뢰성(Reliability)**
우리는 역량(capability)과 신뢰성(reliability)을 다소 직교적(orthogonal)이라고 봅니다. AI 시스템이 90%의 시간 동안 작업을 올바르게 수행한다면, 이는 여전히 중요한 격차를 의미합니다. 90%에 도달하게 하는 기술이 100%에 도달하게 할 가능성은 낮습니다. 통계 학습(statistical learning) 기반 시스템에서는 완벽한 정확도(perfect accuracy)를 달성하는 것이 본질적으로 어렵기 때문에, 예측 불가능한 행동에 대한 이해와 관리가 필수적입니다. 광고 타겟팅(ad targeting)이나 사기 탐지(fraud detection), 또는 최근의 일기 예보와 같은 기계 학습(machine learning)의 성공 사례에서는 완벽한 정확도가 목표는 아니지만, 개발자들이 소비자 제품에 AI를 넣을 때 사람들은 AI가 소프트웨어처럼 작동하기를 기대하며, 이는 AI가 결정론적(deterministically)으로 작동해야 한다는 것을 의미합니다. 예를 들어, AI 여행 에이전트(travel agent)가 90%의 시간 동안만 올바른 목적지로 휴가를 예약한다면 성공적이지 못할 것입니다. 의료 진단 및 기타 헬스케어 기술(healthcare applications)에서도 우리는 많은 오류를 줄여야 하며, 이는 AI 시스템의 결정에 대한 투명성과 설명 가능성(explainability)을 요구합니다. 이전에 우리가 썼듯이, 신뢰성 한계는 최근 AI 기반 기기들의 실패를 부분적으로 설명합니다. AI 개발자들은 이를 인식하는 데 느렸는데, 전문가로서 우리는 AI를 전통적인 소프트웨어와 근본적으로 다르게 개념화하는 데 익숙했기 때문입니다. 1년 전, AI 개발자들은 비전문가 사용자들이 AI에 적응하는 법을 배울 것이라고 희망하거나 가정했지만, 대신 기업들이 사용자 기대에 맞춰 AI를 조정하고 AI가 전통적인 소프트웨어처럼 작동하도록 만들어야 한다는 것이 점차 분명해졌습니다. 이는 단순히 성능을 높이는 것을 넘어, AI가 왜 그렇게 행동하는지 설명할 수 있는 메커니즘을 포함해야 합니다. 신뢰성 향상은 프린스턴(Princeton)에 있는 우리 팀의 연구 관심사입니다. 현재로서는 확률적 구성 요소(stochastic components, LLMs)로 결정론적 시스템(deterministic systems)을 구축하는 것이 가능한지는 심도 있는 연구 주제이자 근본적으로 미해결 문제입니다. 일부 기업들은 신뢰성을 해결했다고 주장했습니다. 예를 들어, 법률 기술(legal tech) 공급업체들은 "환각 없는(hallucination-free)" 시스템을 선전했지만, 현실에서는 AI의 '환각(hallucination)' 문제를 완전히 제거하기는 어렵습니다. 중요한 것은 이러한 불확실성을 어떻게 관리하고, 사용자에게 명확하게 전달하며, 인간의 감독과 개입을 통해 시스템의 견고성(robustness)을 확보하는가입니다.

**3. 프라이버시(Privacy)**
역사적으로 기계 학습(machine learning)은 광고 타겟팅(ad targeting)을 위한 브라우징 기록이나 헬스케어 기술을 위한 의료 기록과 같은 민감한 데이터 소스에 의존하는 경우가 많았습니다. 이러한 의미에서 LLM은 웹 페이지나 책과 같은 공개 소스(public sources)로 주로 훈련되기 때문에 다소 이례적입니다. 5 그러나 AI 비서(AI assistants)와 함께 프라이버시(privacy) 문제가 다시 중요한 사회적 쟁점으로 부상했습니다. 유용한 비서를 구축하려면 기업들은 사용자 상호작용(user interactions)을 기반으로 시스템을 지속적으로 개선해야 하며, 이 과정에서 발생하는 개인 데이터의 수집, 저장, 활용 방식은 매우 중요합니다. 예를 들어, 이메일 작성에 능숙하려면 모델이 이메일로 훈련되는 것이 매우 도움이 될 것입니다. 기업들의 프라이버시 정책은 이에 대해 모호하며, 사용자들은 자신의 데이터가 어떻게 사용되는지 정확히 알기 어렵습니다. 6 이메일, 문서, 스크린샷 등은 채팅 상호작용보다 잠재적으로 훨씬 더 민감합니다. 훈련(training)보다는 추론(inference)과 관련된 독특한 유형의 프라이버시 문제가 있으며, 이러한 민감한 정보가 AI 시스템에 노출될 때 발생할 수 있는 위험은 매우 큽니다. 비서가 우리에게 유용한 일을 하려면 우리의 개인 데이터에 접근할 수 있어야 합니다. 예를 들어, 마이크로소프트는 코파일럿(CoPilot) AI에 사용자 활동에 대한 기억을 제공하기 위해 사용자의 PC 스크린샷을 몇 초마다 찍는 논란의 여지가 있는 기능을 발표했지만, 거센 반발이 있었고 회사는 철회했습니다. 우리는 "데이터가 장치를 떠나지 않는다"와 같은 순전히 기술적인 프라이버시 해석에 대해 깊이 있는 논의가 필요하다고 강조합니다. 메러디스 휘태커(Meredith Whittaker)는 온디바이스(on-device) 사기 탐지(fraud detection)가 상시 감시(always-on surveillance)를 정상화하고, 해당 인프라가 더 억압적인 목적으로 재활용될 수 있다고 주장합니다. 연합 학습(federated learning)이나 차등 프라이버시(differential privacy)와 같은 기술은 개인 정보 보호와 AI 모델 학습을 동시에 가능하게 하는 유망한 방법론입니다. 사용자 데이터 주권(data sovereignty)을 보장하고, 투명한 데이터 관리 정책을 수립하는 것이 미래 AI 서비스의 신뢰를 구축하는 데 필수적입니다.

**4. 안전 및 보안(Safety and security)**
안전 및 보안과 관련하여 여러 가지 우려 사항이 있습니다: AI 모델의 편향(bias)으로 인한 의도치 않은 실패; 음성 복제(voice cloning) 또는 딥페이크(deepfakes)와 같은 AI 오용; 그리고 사용자 데이터를 유출하거나 다른 방식으로 사용자에게 해를 끼칠 수 있는 프롬프트 인젝션(prompt injection)과 같은 새로운 형태의 사이버 공격입니다. 우리는 우발적인 실패는 고칠 수 있다고 생각합니다. 그러나 오용될 수 없는 모델을 만들 방법이 없으므로 방어는 주로 하류(downstream)에 위치해야 한다는 것이 일반적인 견해입니다. 물론 모든 사람이 동의하는 것은 아니므로, 기업들은 불가피한 오용으로 인해 계속해서 나쁜 평판을 얻겠지만, 그들은 이를 사업 비용으로 받아들인 것으로 보입니다. 특히 기업들이 가장 적게 주의를 기울이는 부분인 AI 시스템의 취약점(vulnerability)에 대한 방어는 시급합니다. 적어도 이론적으로는, 사용자 간에 확산되어 해당 사용자들의 AI 비서(AI assistants)를 속여 웜(worm)의 추가 복사본을 생성하는 등 해로운 일을 하도록 만드는 AI 웜(AI worms)과 같은 치명적인 해킹이 가능합니다. 배포된 제품에서 이러한 취약점(vulnerabilities)을 밝혀낸 수많은 개념 증명(proof-of-concept) 시연과 버그 바운티(bug bounties)가 있었지만, 우리는 이러한 유형의 공격이 실제 환경에서 발생한 것을 보지 못했으며, 실제 공격으로 이어지지 않았다고 안심할 수는 없습니다. 이것이 AI 비서의 낮은 채택률 때문인지, 아니면 기업들이 급조한 어설픈 방어책(clumsy defenses)이 충분했기 때문인지, 아니면 다른 이유 때문인지는 확실하지 않습니다. 시간만이 말해줄 것입니다. AI 안전 연구(AI safety research)는 인공지능 정렬(AI alignment), 설명 가능성, 그리고 견고성(robustness)을 강화하여 잠재적 위험을 완화하는 데 중점을 두어야 합니다.

**5. 사용자 인터페이스(User interface)**
많은 애플리케이션에서 LLM의 신뢰성 부족은 봇이 엉뚱한 방향으로 갈 경우 사용자가 효과적으로 개입할 수 있는 직관적인 방법을 요구합니다. 챗봇(chatbot)에서는 답변을 다시 생성하거나 여러 버전을 보여주고 사용자가 선택하도록 하는 것만큼 간단한 해결책이 있을 수 있습니다. 그러나 오류가 큰 비용을 초래할 수 있는 애플리케이션에서는 적절한 감독을 보장하는 것이 더 까다로우며, 사용자의 피로도를 높이지 않으면서도 필요한 정보를 제공해야 합니다. 사용자가 비서에게 말하고 비서가 다시 말하는 자연어 인터페이스(natural language interfaces)에서는 문제가 훨씬 더 어렵습니다. 여기에 생성형 AI(generative AI)의 많은 잠재력이 있습니다. 한 가지 예로, 외국어 간판을 응시하고 있다는 것을 감지하는 것처럼, 필요할 때 요청하지 않아도 안경 속으로 사라져 당신에게 말을 거는 AI는 오늘날 우리가 가진 것과는 완전히 다른 경험일 것입니다. 단순히 텍스트 기반의 대화를 넘어, 멀티모달(multimodal) 인터페이스는 AI와의 상호작용 방식을 혁신할 것입니다. 음성, 시각, 촉각 등 다양한 감각을 통해 AI가 사용자 경험에 자연스럽게 녹아드는 것이 중요합니다. 그러나 제한된 사용자 인터페이스(user interface)는 잘못되거나 예상치 못한 행동에 대한 여지를 거의 제공하지 않으므로, 유연하고 적응적인 인터페이스 설계가 필수적입니다. AI는 사용자의 의도를 정확히 파악하고, 상황에 맞는 최적의 인터랙션을 제공하며, 필요할 때는 인간의 개입을 원활하게 지원해야 합니다.

**결론**
AI 지지자들은 AI 역량(AI capabilities)의 빠른 개선 속도 때문에 곧 엄청난 사회적, 경제적 효과를 보게 될 것이라고 주장하지만, 우리는 단순히 기술적 진보만을 추구하는 데는 한계가 있다고 봅니다. 우리는 그러한 역량 예측에 들어가는 추세 외삽(trend extrapolation)과 부주의한 사고방식에 대해 경계해야 합니다. AI 역량이 빠르게 개선된다 하더라도 개발자들은 위에서 논의된 과제들을 해결하기 위해 다각적인 접근이 필요합니다. 이들은 순전히 기술적인 것이 아니라 사회기술적(sociotechnical)인 문제이므로, 진전은 느릴 것입니다. 조직들은 AI를 기존 제품과 워크플로우(workflows)에 통합하고, 그 함정(pitfalls)을 피하면서 사람들이 생산적으로 AI를 사용하도록 지원해야 합니다. 이는 단순히 기술 도입을 넘어, 조직 문화와 교육 시스템의 변화를 수반합니다. 우리는 이것이 1, 2년이 아니라 10년 이상의 시간 척도(timescale)로 장기적인 관점에서 접근해야 할 복합적인 과제임을 인식해야 합니다. 지속 가능한 AI 생태계를 구축하기 위해서는 기술, 윤리, 사회가 조화롭게 발전하는 노력이 중요합니다.

**추가 자료 읽기**
베네딕트 에반스(Benedict Evans)는 범용 언어 모델(general-purpose language models)을 사용하여 단일 목적 소프트웨어(single-purpose software)를 구축하는 것의 중요성에 대해 썼습니다.
1 분명히 말하자면, 우리는 최첨단(state-of-the-art) 모델에 대한 접근을 줄이는 것이 오용을 줄일 것이라고 생각하지 않습니다. 그러나 LLM의 경우, 오용은 합법적인 사용(생각을 필요로 함)보다 쉽기 때문에 오용이 널리 퍼진 것은 놀라운 일이 아닙니다.
2 AI 채택 속도는 상대적입니다. 애플이 제품에 AI를 통합하는 방식조차도 너무 빠르다는 비판을 받았습니다.
3 이들은 사용자 경험(user experience)에 중요한 요소들에 관한 것입니다. 우리는 환경 비용, 저작권이 있는 데이터로 훈련하는 것 등은 제외하고 있습니다.
4 예를 들어, API의 GPT-3.5 (text-davinci-003)는 백만 토큰(tokens)당 20달러였지만, 더 강력한 gpt-4o-mini는 단 15센트입니다.
5 분명히 말하자면, 데이터 소스가 공개적이라고 해서 프라이버시 문제가 없는 것은 아닙니다.
6 예를 들어, 구글은 "우리는 구글의 AI 모델을 훈련하는 데 도움이 되도록 공개적으로 사용 가능한 정보를 사용합니다"라고 말합니다. 다른 곳에서는 이메일과 같은 개인 데이터를 사용하여 서비스를 제공하고, 서비스를 유지 및 개선하며, 서비스를 개인화하고, 새로운 서비스를 개발할 수 있다고 말합니다. 이러한 공개 내용과 일치하는 한 가지 접근 방식은 제미니(Gemini)와 같은 모델의 사전 훈련(pre-training)에는 공개 데이터만 사용되지만, 이메일 자동 응답 봇(auto-response bot)과 같은 것을 만들기 위해 해당 모델을 미세 조정(fine-tune)하는 데는 개인 데이터가 사용된다는 것입니다. 우리가 아는 한 Anthropic은 유일한 예외입니다. Anthropic은 "사용자가 명시적인 허가를 주지 않는 한, 우리는 사용자 제출 데이터로 생성형 모델을 훈련하지 않습니다. 현재까지 우리는 고객 또는 사용자 제출 데이터를 생성형 모델 훈련에 사용하지 않았습니다"라고 말합니다. 프라이버시에 대한 이러한 약속은 칭찬할 만하지만, 만약 Anthropic이 제품 구축을 더 전적으로 수용한다면 회사에 불리하게 작용할 것이라고 우리는 예측합니다.