AI 기업들은 하드웨어와 데이터 센터에 막대한 투자를 계획하고 있지만, 그에 상응하는 상업적 성과는 여전히 기대에 미치지 못하고 있습니다. 이로 인해 생성형 AI(generative AI)가 과도하게 부풀려진 거품이라는 우려의 목소리가 커지고 있습니다. 우리는 앞으로 어떤 일이 일어날지에 대한 예측을 내놓지는 않을 것입니다. 하지만 현재 상황이 왜 이렇게 전개되었는지에 대해 명확한 진단을 내릴 수 있다고 생각합니다. 이 글에서는 AI 기업들이 범한 주요 오류들과 이를 개선하기 위한 노력들을 살펴봅니다. 그리고 생성형 AI가 투자 규모에 걸맞은 상업적 성공을 거두기 위해 아직 극복해야 할 다섯 가지 핵심 장벽에 대해 논의할 것입니다.

**제품-시장 적합성(Product-market fit)**
ChatGPT가 출시되었을 때, 사람들은 그것의 수많은 예상치 못한 용도를 발견했습니다. 이는 AI 개발자들을 지나치게 흥분시켰고, 그들은 시장의 실제 요구를 완전히 오해했습니다. 개념 증명(proofs of concept)과 상업적으로 신뢰할 수 있는 제품 사이의 거대한 격차를 과소평가한 것입니다. 이러한 오해는 거대 언어 모델(LLMs)을 상업화하는 데 있어 상반되지만 똑같이 결함이 있는 두 가지 접근 방식으로 이어졌습니다.

OpenAI와 Anthropic은 모델 구축 자체에만 집중하고 제품 개발에는 상대적으로 소홀했습니다. 예를 들어, OpenAI가 ChatGPT iOS 앱을 출시하는 데 6개월, 안드로이드 앱을 출시하는 데 8개월이 걸렸습니다! 반면 구글과 마이크로소프트는 어떤 제품이 실제로 AI의 혜택을 받을 수 있을지, 그리고 AI가 어떻게 사용자 경험에 매끄럽게 통합되어야 하는지 충분히 고려하지 않은 채, 서둘러 모든 서비스에 AI를 도입하려 했습니다. 두 회사 그룹 모두 "사람들이 원하는 것을 만들어라"는 만트라를 잊었습니다. LLM의 일반성(generality)은 개발자들이 모델에 작업을 수행하도록 프롬프트(prompt)하는 것이 신중하게 설계된 제품이나 기능을 대체할 수 있다고 스스로를 오도하게 만들었습니다. OpenAI와 Anthropic의 'DIY' 방식은 초기 채택자들이 기술을 자신들의 목적에 맞게 활용하는 데 더 많은 노력을 기울인 반면, 일반 사용자들은 사용하기 쉬운 제품을 원했습니다. 이로 인해 불량 사용자(bad actors)의 비율이 불균형적으로 높아져 기술에 대한 대중의 인식을 저해하는 결과를 낳았습니다. 1 한편, 마이크로소프트와 구글의 'AI를 전면에 내세우는(AI-in-your-face)' 접근 방식은 때로는 유용했지만, 더 자주 사용자에게 성가심을 주었습니다. 또한 마이크로소프트의 초기 시드니(Sydney) 챗봇과 구글의 제미니(Gemini) 이미지 생성기처럼 부적절한 테스트로 인해 많은 불필요한 오류(unforced errors)를 초래했으며, 이는 상당한 반발을 불러일으켰습니다.

하지만 기업들은 점차 전략을 수정하고 있습니다. OpenAI는 연구 중심의 조직에서 범용 제품을 개발하는 회사로 변모하는 양상을 보이고 있습니다. OpenAI 이사회 사태의 인간적인 측면을 제외하면, 이는 본질적으로 인공 일반 지능(AGI) 창조에서 실제 제품 구현으로의 전환에 관한 것이었습니다. Anthropic 또한 제품화의 중요성을 인식하고 있으며, OpenAI에서 AGI 연구에 더 관심을 가졌던 많은 연구원들을 영입하며 자체적인 제품 로드맵을 강화하고 있습니다. 구글과 마이크로소프트는 학습 속도가 다소 느렸지만, 애플의 영향력이 이들의 변화를 가속화할 것으로 예상됩니다. 작년에는 애플이 AI 분야에서 뒤처진다는 평가를 받았으나, WWDC(개발자 컨퍼런스)에서 공개된 애플 인텔리전스(Apple Intelligence)의 신중하고 사용자 중심적인 접근 방식은 사용자들로부터 훨씬 더 긍정적인 반응을 얻고 있습니다. 2 구글은 이제 검색 엔진에 AI를 통합하는 것뿐만 아니라, 최신 픽셀(Pixel) 폰과 안드로이드(Android) 운영체제 전반에 걸쳐 AI 기능을 깊이 통합하는 데 주력하고 있으며, 이는 사용자 경험을 크게 향상시킬 잠재력을 보여주고 있습니다. 그리고 메타(Meta)는 AI를 활용하여 광고 기반 소셜 미디어 플랫폼에서 개인화된 콘텐츠와 사용자 참여를 증대시키는 비전을 제시하고 있습니다. AI 생성 콘텐츠가 넘쳐나는 세상의 사회적 함의는 복잡하지만, 비즈니스 관점에서는 합리적인 전략입니다. 당신은 우리의 책에 대한 뉴스레터인 AI Snake Oil을 읽고 있습니다. 새로운 게시물을 받으려면 구독하세요. 구독

**소비자 AI를 위한 5가지 주요 과제**
개발자들이 매력적인 AI 기반 소비자 제품을 성공적으로 구축하기 위해 반드시 해결해야 할 거대 언어 모델(LLM)의 다섯 가지 주요 한계가 있습니다. 3 이러한 과제들은 AI 기술이 실제 사용자들에게 가치를 제공하기 위한 핵심 요소로 작용합니다.

**1. 비용(Cost)**
역량(capability)이 장벽이 아니라 비용이 장벽인 애플리케이션이 많습니다. 간단한 채팅 애플리케이션에서도 비용 문제는 봇이 얼마나 많은 대화 기록을 추적할 수 있는지를 결정합니다. 대화가 길어질수록 모든 응답에 대해 전체 기록을 처리하는 것은 빠르게 엄청나게 비싸집니다. 비용 면에서는 놀라운 속도로 발전이 있었습니다. 지난 2년 동안 동등한 역량(equivalent-capability)을 가진 모델의 비용이 100배 이상 감소했습니다. 4 그 결과, 일부 기업들은 LLM이 "측정하기에는 너무 저렴하다(too cheap to meter)"고 주장하거나 곧 그렇게 될 것이라고 말합니다. 글쎄요, API(Application Programming Interface)를 무료로 만들 때 우리는 그것을 믿을 것입니다. 더 심각하게는, 많은 애플리케이션에서 비용 개선이 곧바로 정확도 개선으로 이어지기 때문에 비용이 계속해서 중요한 문제로 남을 것이라고 생각합니다. 이는 LLM의 무작위성(randomness)을 고려할 때, 작업을 수십, 수천, 심지어 수백만 번 반복해서 재시도하는 것이 성공 가능성을 높이는 효과적인 방법임이 밝혀졌기 때문입니다. 따라서 모델이 저렴할수록 주어진 예산으로 더 많은 재시도를 할 수 있습니다. 우리는 에이전트(agents)에 대한 초기 연구에서 이를 정량화했으며, 그 이후로 수많은 후속 논문들도 비슷한 결론을 내렸습니다. 그렇긴 하지만, 대부분의 애플리케이션에서 비용 최적화(cost optimization)가 더 이상 심각한 문제가 되지 않는 지점에 곧 도달할 가능성도 있습니다.

**2. 신뢰성(Reliability)**
우리는 역량(capability)과 신뢰성(reliability)을 다소 직교적(orthogonal)이라고 봅니다. AI 시스템이 90%의 시간 동안 작업을 올바르게 수행한다면, 그것은 해당 작업을 수행할 역량은 있지만, 지속적으로 신뢰할 수 있게(reliably) 해낼 수는 없다고 말할 수 있습니다. 90%의 정확도에 도달하게 하는 기술이 100%에 도달하게 할 가능성은 희박합니다. 통계 학습(statistical learning) 기반 시스템에서는 완벽한 정확도(perfect accuracy)를 달성하는 것이 본질적으로 어렵습니다. 광고 타겟팅(ad targeting)이나 사기 탐지(fraud detection), 또는 최신 일기 예보와 같은 기계 학습(machine learning)의 성공 사례를 살펴보면, 완벽한 정확도가 궁극적인 목표는 아닙니다. 이러한 시스템들은 최첨단(state of the art)보다 나은 성능을 보여준다면 충분히 유용합니다. 의료 진단 및 기타 헬스케어 기술(healthcare applications) 분야에서도 우리는 일정 수준의 오류를 용인합니다. 그러나 개발자들이 소비자 제품에 AI를 통합할 때, 사용자들은 AI가 일반적인 소프트웨어처럼 작동하기를 기대하며, 이는 AI가 결정론적(deterministically)으로 동작해야 함을 의미합니다. AI 여행 에이전트(travel agent)가 90%의 시간 동안만 올바른 목적지로 휴가를 예약한다면, 결코 성공적인 제품이 될 수 없을 것입니다. 이전에 우리가 언급했듯이, 이러한 신뢰성 한계는 최근 출시된 AI 기반 기기들의 실망스러운 성과를 부분적으로 설명합니다. AI 개발자들은 이러한 현실을 인식하는 데 다소 느렸는데, 이는 전문가로서 우리가 AI를 전통적인 소프트웨어와 근본적으로 다르게 개념화하는 데 익숙했기 때문입니다. 예를 들어, 우리 둘은 일상 업무에서 챗봇(chatbots)과 에이전트(agents)를 많이 사용하며, 이러한 도구들의 환각(hallucinations)과 신뢰성 부족을 우회하는 것이 거의 자동화되었습니다. 초기에는 AI 개발자들이 비전문가 사용자들도 AI의 특성에 적응할 것이라고 기대하거나 가정했지만, 이제는 기업들이 사용자 기대에 맞춰 AI를 조정하고 AI가 전통적인 소프트웨어처럼 작동하도록 만들어야 한다는 것이 점차 분명해지고 있습니다. 신뢰성 향상은 프린스턴(Princeton)에 있는 우리 팀의 주요 연구 관심사입니다. 현재로서는 확률적 구성 요소(stochastic components, LLMs)를 사용하여 결정론적 시스템(deterministic systems)을 구축하는 것이 가능한지는 근본적으로 미해결 문제입니다. 일부 기업들은 신뢰성 문제를 해결했다고 주장해왔습니다. 예를 들어, 법률 기술(legal tech) 공급업체들은 "환각 없는(hallucination-free)" 시스템을 선전했지만, 이러한 주장은 시기상조임이 드러났습니다.

**3. 프라이버시(Privacy)**
역사적으로 기계 학습(machine learning)은 광고 타겟팅(ad targeting)을 위한 브라우징 기록이나 헬스케어 기술(health tech)을 위한 의료 기록과 같은 민감한 데이터 소스에 의존하는 경우가 많았습니다. 이러한 의미에서 LLM은 웹 페이지나 책과 같은 공개 소스(public sources)로 주로 훈련되기 때문에 다소 이례적입니다. 5 그러나 AI 비서(AI assistants)의 등장과 함께 프라이버시(privacy) 문제가 다시 크게 부각되었습니다. 진정으로 유용한 비서를 구축하기 위해서는 기업들이 사용자 상호작용(user interactions)을 기반으로 시스템을 지속적으로 훈련해야 합니다. 예를 들어, 이메일 작성에 능숙한 모델을 만들려면 실제 이메일 데이터로 훈련하는 것이 매우 효과적일 것입니다. 기업들의 프라이버시 정책은 이러한 데이터 활용에 대해 여전히 모호한 부분이 많으며, 어느 정도까지 개인 데이터가 사용되고 있는지는 불분명합니다. 6 이메일, 문서, 스크린샷 등은 일반적인 채팅 상호작용보다 훨씬 더 민감한 개인 정보를 포함할 수 있습니다. 훈련(training) 단계 외에도, 추론(inference)과 관련된 독특한 유형의 프라이버시 문제도 존재합니다. 비서가 우리에게 실질적으로 유용한 일을 수행하려면 우리의 개인 데이터에 접근할 수 있어야 합니다. 예를 들어, 마이크로소프트는 코파일럿(CoPilot) AI에 사용자 활동에 대한 기억을 제공하기 위해 사용자의 PC 스크린샷을 몇 초마다 찍는 논란의 여지가 있는 기능을 발표했습니다. 그러나 거센 반발이 있었고 회사는 결국 해당 기능을 철회했습니다. 우리는 "데이터가 장치를 떠나지 않는다"와 같은 순전히 기술적인 프라이버시 해석에 대해 경고합니다. 메러디스 휘태커(Meredith Whittaker)는 온디바이스(on-device) 사기 탐지(fraud detection)가 상시 감시(always-on surveillance)를 정상화하고, 해당 인프라가 더 억압적인 목적으로 재활용될 수 있다고 주장합니다. 그렇긴 하지만, 프라이버시 보호를 위한 기술 혁신은 분명히 도움이 될 수 있습니다.

**4. 안전 및 보안(Safety and security)**
안전 및 보안과 관련하여 여러 가지 우려 사항이 있습니다. 첫째, 제미니(Gemini)의 이미지 생성 편향과 같은 의도치 않은 실패가 있습니다. 둘째, 음성 복제(voice cloning) 또는 딥페이크(deepfakes)와 같은 AI 오용 사례가 늘어나고 있습니다. 셋째, 사용자 데이터를 유출하거나 다른 방식으로 사용자에게 해를 끼칠 수 있는 프롬프트 인젝션(prompt injection)과 같은 해킹 위협이 존재합니다. 우리는 우발적인 실패는 기술적인 개선을 통해 충분히 고칠 수 있다고 생각합니다. 대부분의 오용 유형에 대해서는, 오용될 수 없는 모델을 만드는 것은 현실적으로 불가능하므로, 방어는 주로 시스템의 하류(downstream)에서 이루어져야 한다는 것이 우리의 견해입니다. 물론 모든 사람이 이 견해에 동의하는 것은 아니며, 기업들은 불가피한 오용 사례들로 인해 계속해서 나쁜 평판을 얻겠지만, 그들은 이를 사업 운영의 불가피한 비용으로 받아들이는 것으로 보입니다. 세 번째 범주인 해킹에 대해 이야기해 봅시다. 우리가 알기로는, 기업들이 가장 적게 주의를 기울이는 부분인 것 같습니다. 적어도 이론적으로는, 사용자 간에 확산되어 해당 사용자들의 AI 비서(AI assistants)를 속여 웜(worm)의 추가 복사본을 생성하는 등 해로운 일을 하도록 만드는 AI 웜(AI worms)과 같은 치명적인 해킹이 가능합니다. 배포된 제품에서 이러한 취약점(vulnerabilities)을 밝혀낸 수많은 개념 증명(proof-of-concept) 시연과 버그 바운티(bug bounties)가 있었지만, 우리는 아직 이러한 유형의 공격이 실제 환경에서 광범위하게 발생한 것을 보지 못했습니다. 이것이 AI 비서의 낮은 채택률 때문인지, 아니면 기업들이 급조한 어설픈 방어책(clumsy defenses)이 예상외로 충분했기 때문인지, 아니면 다른 근본적인 이유 때문인지는 확실하지 않습니다. 시간만이 이 질문에 답해줄 것입니다.

**5. 사용자 인터페이스(User interface)**
많은 애플리케이션에서 LLM의 신뢰성 부족은 봇이 엉뚱한 방향으로 진행될 경우 사용자가 개입할 수 있는 명확한 방법을 요구합니다. 챗봇(chatbot)에서는 답변을 다시 생성하거나 여러 버전을 제시하여 사용자가 선택하도록 하는 것만큼 간단하게 해결될 수 있습니다. 그러나 항공권 예약과 같이 오류가 큰 비용을 초래할 수 있는 애플리케이션에서는 적절한 감독을 보장하는 것이 훨씬 더 까다롭습니다. 동시에 시스템은 과도한 개입으로 사용자를 짜증 나게 하는 것을 피해야 합니다. 사용자가 비서에게 말하고 비서가 다시 응답하는 자연어 인터페이스(natural language interfaces)에서는 이 문제가 훨씬 더 복잡해집니다. 여기에 생성형 AI(generative AI)의 상당한 잠재력이 있습니다. 예를 들어, 사용자가 외국어 간판을 응시하고 있음을 감지하여, 요청하지 않아도 안경 속으로 사라져 사용자에게 통역을 해주는 AI는 오늘날 우리가 경험하는 것과는 완전히 다른 수준의 사용자 경험을 제공할 것입니다. 하지만 이러한 제한된 사용자 인터페이스(user interface) 환경은 잘못되거나 예상치 못한 AI 행동에 대한 수정 여지를 거의 남기지 않는다는 한계가 있습니다.

**결론**
AI 지지자들은 AI 역량(AI capabilities)의 빠른 개선 속도 때문에 곧 엄청난 사회적, 경제적 효과를 보게 될 것이라고 종종 주장합니다. 우리는 그러한 역량 예측에 들어가는 맹목적인 추세 외삽(trend extrapolation)과 부주의한 사고방식에 대해 여전히 회의적입니다. 더 중요하게는, AI 역량이 아무리 빠르게 발전한다 하더라도 개발자들은 위에서 심층적으로 논의된 핵심 과제들을 해결해야만 합니다. 이러한 과제들은 순전히 기술적인 문제만이 아니라 사회기술적(sociotechnical)인 측면을 강하게 내포하고 있으므로, 그 진전은 예상보다 느릴 수 있습니다. 그리고 설령 이러한 과제들이 기술적으로 해결된다 하더라도, 조직들은 AI를 기존 제품과 워크플로우(workflows)에 효과적으로 통합하고, 잠재적인 함정(pitfalls)을 피하면서 사람들이 AI를 생산적으로 활용할 수 있도록 교육해야 합니다. 우리는 이러한 변화가 단 1, 2년이 아닌, 10년 이상의 시간 척도(timescale)에 걸쳐 점진적으로 일어날 것으로 예상해야 합니다.

**추가 자료 읽기**
베네딕트 에반스(Benedict Evans)는 범용 언어 모델(general-purpose language models)을 사용하여 단일 목적 소프트웨어(single-purpose software)를 구축하는 것의 중요성에 대해 썼습니다.
1 분명히 말하자면, 우리는 최첨단(state-of-the-art) 모델에 대한 접근을 줄이는 것이 오용을 줄일 것이라고 생각하지 않습니다. 그러나 LLM의 경우, 오용은 합법적인 사용(생각을 필요로 함)보다 쉽기 때문에 오용이 널리 퍼진 것은 놀라운 일이 아닙니다.
2 AI 채택 속도는 여전히 상대적입니다. 하지만 애플이 WWDC에서 선보인 신중하고 사용자 중심적인 AI 통합 방식은 시장의 긍정적인 평가를 받고 있으며, 이는 다른 기업들에게도 중요한 시사점을 제공합니다.
3 이들은 사용자 경험(user experience)에 중요한 요소들에 관한 것입니다. 우리는 환경 비용, 저작권이 있는 데이터로 훈련하는 것 등은 논외로 하고 있습니다.
4 예를 들어, 초기 API의 GPT-3.5 (text-davinci-003)는 백만 토큰(tokens)당 20달러였지만, 현재는 훨씬 더 강력하고 효율적인 gpt-4o와 같은 최신 모델들이 훨씬 낮은 비용으로 제공됩니다. 이러한 비용 효율성의 급격한 개선은 지속적으로 이루어지고 있습니다.
5 분명히 말하자면, 데이터 소스가 공개적이라고 해서 프라이버시 문제가 완전히 사라지는 것은 아닙니다.
6 예를 들어, 구글은 "우리는 구글의 AI 모델을 훈련하는 데 도움이 되도록 공개적으로 사용 가능한 정보를 사용합니다"라고 명시하고 있습니다. 또한 다른 서비스 약관에서는 이메일과 같은 개인 데이터를 사용하여 서비스를 제공하고, 유지 및 개선하며, 개인화하고, 새로운 서비스를 개발할 수 있다고 밝히고 있습니다. 이러한 공개 내용과 일치하는 한 가지 접근 방식은 제미니(Gemini)와 같은 모델의 사전 훈련(pre-training)에는 공개 데이터만 사용되지만, 이메일 자동 응답 봇(auto-response bot)과 같은 특정 기능을 위해 해당 모델을 미세 조정(fine-tune)하는 데는 개인 데이터가 사용될 수 있다는 것입니다. 우리가 아는 한 Anthropic은 여전히 예외적인 입장을 고수하고 있습니다. Anthropic은 "사용자가 명시적인 허가를 주지 않는 한, 우리는 사용자 제출 데이터로 생성형 모델을 훈련하지 않습니다. 현재까지 우리는 고객 또는 사용자 제출 데이터를 생성형 모델 훈련에 사용하지 않았습니다"라고 강조합니다. 프라이버시에 대한 이러한 약속은 칭찬할 만하지만, 만약 Anthropic이 제품 구축을 더 전적으로 수용한다면 이러한 정책이 회사에 불리하게 작용할 것이라고 우리는 예측합니다.