AI 기업들은 하드웨어와 데이터 센터에 총 1조 달러를 지출할 계획이지만, 아직까지 그에 상응하는 성과는 상대적으로 미미합니다. 이로 인해 생성형 AI(generative AI)가 거품이라는 우려의 목소리가 커지고 있습니다. 우리는 앞으로 어떤 일이 일어날지에 대한 예측을 내놓지 않을 것입니다. 하지만 우리는 상황이 여기까지 오게 된 경위에 대해 확실한 진단을 내릴 수 있다고 생각합니다. 이 게시물에서 우리는 AI 기업들이 저지른 실수와 이를 어떻게 수정하려고 노력해왔는지 설명합니다. 그런 다음, 생성형 AI가 투자를 정당화할 만큼 상업적으로 성공하기 위해 아직 극복해야 할 다섯 가지 장벽에 대해 이야기할 것입니다.

**제품-시장 적합성(Product-market fit)**
ChatGPT가 출시되었을 때, 사람들은 그것의 수많은 예상치 못한 용도를 발견했습니다. 이는 AI 개발자들을 지나치게 흥분시켰습니다. 그들은 시장을 완전히 오해했고, 개념 증명(proofs of concept)과 신뢰할 수 있는 제품 사이의 거대한 격차를 과소평가했습니다. 이러한 오해는 거대 언어 모델(LLMs)을 상업화하는 데 있어 상반되지만 똑같이 결함이 있는 두 가지 접근 방식으로 이어졌습니다.

초기에는 모델 자체의 성능 과시에 집중하거나, 기존 제품에 AI 기능을 무분별하게 추가하는 경향이 짙었습니다. OpenAI와 Anthropic은 모델의 범용성에 기대어 사용자가 직접 활용법을 찾아내기를 기대했으며, 이로 인해 일반 사용자보다는 기술에 정통한 얼리 어답터(early adopter)나 특정 목적을 가진 사용자층에 의해 주로 사용되는 결과를 낳았습니다. 반면 구글과 마이크로소프트는 자사 서비스 전반에 AI를 서둘러 통합하려 했으나, 사용자 경험이나 실제 필요에 대한 깊은 고민 없이 이루어진 탓에 오히려 혼란과 불편을 초래하기도 했습니다. 예를 들어, 마이크로소프트의 코파일럿(Copilot) 초기 버전이나 구글 제미니(Gemini)의 이미지 생성 기능에서 발생한 논란은 이러한 접근 방식의 한계를 명확히 보여주었습니다.

하지만 최근에는 이러한 경향이 변화하고 있습니다. OpenAI는 단순한 연구 기관을 넘어 제품 중심 회사로 전환하며, 특정 사용 사례에 최적화된 맞춤형 GPT(Custom GPTs)나 엔터프라이즈 솔루션(enterprise solutions)을 제공하는 데 주력하고 있습니다. Anthropic 역시 기업 고객을 위한 신뢰성 높은 AI 솔루션 개발에 집중하며, 안전성과 제어 가능성을 강조한 클로드(Claude) 모델을 통해 특정 산업 분야에서의 제품-시장 적합성을 모색하고 있습니다. 구글과 마이크로소프트 또한 단순히 AI 기능을 추가하는 것을 넘어, 코파일럿 스튜디오(Copilot Studio)와 같은 도구를 통해 기업이 자체적으로 AI를 맞춤 설정하고 기존 워크플로우(workflow)에 통합할 수 있도록 지원하며, 실제 비즈니스 가치를 창출하는 방향으로 전략을 수정하고 있습니다.

애플(Apple)의 접근 방식은 이러한 변화의 중요한 촉매제가 될 수 있습니다. 애플은 AI 통합에 있어 신중하고 사용자 경험 중심적인 전략을 취하며, 온디바이스(on-device) AI와 개인 정보 보호를 최우선으로 내세웠습니다. 이는 범용 AI 모델의 한계를 극복하고 사용자의 실제 생활에 자연스럽게 녹아드는 AI 제품을 만드는 데 있어 중요한 시사점을 제공합니다. 메타(Meta) 또한 라마(Llama) 시리즈와 같은 오픈소스(open-source) 모델을 공개하며 AI 혁신의 속도를 높이고, 개발자들이 다양한 제품과 서비스를 만들 수 있는 기반을 제공함으로써 간접적으로 제품-시장 적합성을 확장하는 데 기여하고 있습니다. 이처럼 AI 기업들은 모델 성능 경쟁을 넘어, 실제 사용자에게 가치를 제공하고 신뢰할 수 있는 제품을 만드는 데 집중하기 시작했습니다.

**소비자 AI를 위한 5가지 주요 과제**
개발자들이 매력적인 AI 기반 소비자 제품을 만들기 위해 해결해야 할 LLM의 다섯 가지 한계가 있습니다.

**1. 비용(Cost)**
역량(capability)이 장벽이 아니라 비용이 장벽인 애플리케이션이 많습니다. 간단한 채팅 애플리케이션에서도 비용 문제는 봇이 얼마나 많은 대화 기록을 추적할 수 있는지를 결정합니다. 대화가 길어질수록 모든 응답에 대해 전체 기록을 처리하는 것은 빠르게 엄청나게 비싸집니다. 비용 면에서는 빠른 발전이 있었습니다. 지난 18개월 동안 동등한 역량(equivalent-capability)에 대한 비용이 100배 이상 감소했습니다. 4 그 결과, 기업들은 LLM이 "측정하기에는 너무 저렴하다(too cheap to meter)"고 주장하거나 곧 그렇게 될 것이라고 말합니다. 글쎄요, API(Application Programming Interface)를 무료로 만들 때 우리는 그것을 믿을 것입니다. 더 심각하게는, 비용이 계속해서 문제가 될 것이라고 우리가 생각하는 이유는 많은 애플리케이션에서 비용 개선이 정확도 개선으로 직접 연결되기 때문입니다. 이는 LLM의 무작위성(randomness)을 고려할 때, 작업을 수십, 수천, 심지어 수백만 번 반복해서 재시도하는 것이 성공 가능성을 높이는 좋은 방법임이 밝혀졌기 때문입니다. 따라서 모델이 저렴할수록 주어진 예산으로 더 많은 재시도를 할 수 있습니다. 우리는 에이전트(agents)에 대한 최근 논문에서 이를 정량화했습니다. 그 이후로 많은 다른 논문들도 비슷한 주장을 했습니다. 그렇긴 하지만, 대부분의 애플리케이션에서 비용 최적화(cost optimization)가 심각한 문제가 되지 않는 지점에 곧 도달할 가능성도 있습니다.

이러한 비용 문제는 단순히 모델 추론(inference) 비용에만 국한되지 않습니다. 복잡한 AI 에이전트(agent) 시스템에서는 여러 번의 모델 호출, 외부 도구 사용, 그리고 오류 수정(self-correction)을 위한 반복적인 재시도 과정에서 비용이 기하급수적으로 증가할 수 있습니다. 그러나 최근에는 비용 효율성을 높이기 위한 다양한 기술적 접근 방식이 도입되고 있습니다. 모델 경량화(quantization), 지식 증류(knowledge distillation)를 통한 소형 모델 개발, 추론 최적화 엔진(inference optimization engine)의 발전, 그리고 특정 작업에 특화된 소형 언어 모델(SLMs)의 활용이 대표적입니다. 또한, 검색 증강 생성(RAG, Retrieval-Augmented Generation)과 같이 외부 지식 기반을 활용하여 모델의 환각(hallucination)을 줄이고 정확도를 높이는 동시에, 필요한 토큰(token) 수를 줄여 비용을 절감하는 방식도 주목받고 있습니다. 온디바이스 AI(on-device AI) 기술의 발전은 클라우드(cloud) 기반 추론 비용을 줄이고 개인 정보 보호를 강화하는 동시에, 지연 시간을 단축하여 사용자 경험을 향상시키는 데 기여하고 있습니다.

**2. 신뢰성(Reliability)**
우리는 역량(capability)과 신뢰성(reliability)을 다소 직교적(orthogonal)이라고 봅니다. AI 시스템이 90%의 시간 동안 작업을 올바르게 수행한다면, 우리는 그것이 작업을 수행할 역량은 있지만 신뢰할 수 있게(reliably) 수행할 수는 없다고 말할 수 있습니다. 90%에 도달하게 하는 기술이 100%에 도달하게 할 가능성은 낮습니다. 통계 학습(statistical learning) 기반 시스템에서는 완벽한 정확도(perfect accuracy)를 달성하는 것이 본질적으로 어렵습니다. 광고 타겟팅(ad targeting)이나 사기 탐지(fraud detection), 또는 최근의 일기 예보와 같은 기계 학습(machine learning)의 성공 사례를 생각해보면, 완벽한 정확도가 목표는 아닙니다. 시스템이 최첨단(state of the art)보다 낫다면 유용합니다. 의료 진단 및 기타 헬스케어 기술(healthcare applications)에서도 우리는 많은 오류를 용인합니다. 그러나 개발자들이 소비자 제품에 AI를 넣을 때, 사람들은 AI가 소프트웨어처럼 작동하기를 기대하며, 이는 AI가 결정론적(deterministically)으로 작동해야 한다는 것을 의미합니다. AI 여행 에이전트(travel agent)가 90%의 시간 동안만 올바른 목적지로 휴가를 예약한다면, 성공적이지 못할 것입니다. 이전에 우리가 썼듯이, 신뢰성 한계는 최근 AI 기반 기기들의 실패를 부분적으로 설명합니다. AI 개발자들은 이를 인식하는 데 느렸는데, 전문가로서 우리는 AI를 전통적인 소프트웨어와 근본적으로 다르게 개념화하는 데 익숙하기 때문입니다. 예를 들어, 우리 둘은 일상 업무에서 챗봇(chatbots)과 에이전트(agents)를 많이 사용하며, 이러한 도구들의 환각(hallucinations)과 신뢰성 부족을 우회하는 것이 거의 자동화되었습니다. 1년 전, AI 개발자들은 비전문가 사용자들이 AI에 적응하는 법을 배울 것이라고 희망하거나 가정했지만, 대신 기업들이 사용자 기대에 맞춰 AI를 조정하고 AI가 전통적인 소프트웨어처럼 작동하도록 만들어야 한다는 것이 점차 분명해졌습니다. 신뢰성 향상은 프린스턴(Princeton)에 있는 우리 팀의 연구 관심사입니다. 현재로서는 확률적 구성 요소(stochastic components, LLMs)로 결정론적 시스템(deterministic systems)을 구축하는 것이 가능한지는 근본적으로 미해결 문제입니다. 일부 기업들은 신뢰성을 해결했다고 주장했습니다. 예를 들어, 법률 기술(legal tech) 공급업체들은 "환각 없는(hallucination-free)" 시스템을 선전했습니다. 그러나 이러한 주장은 시기상조임이 드러났습니다.

하지만 소비자용 제품에서는 90%의 신뢰성으로는 부족합니다. 사용자는 AI가 기존 소프트웨어처럼 예측 가능하고 오류 없이 작동하기를 기대합니다. 예를 들어, AI 기반 법률 자문 시스템이 10%의 확률로 잘못된 법적 조언을 제공한다면 심각한 결과를 초래할 수 있습니다. 이러한 신뢰성 문제를 해결하기 위해 다양한 방법이 연구되고 있습니다. 프롬프트 엔지니어링(prompt engineering) 기술의 고도화, 다단계 추론(multi-step reasoning)을 통한 자기 검증(self-verification) 메커니즘, 그리고 외부 데이터베이스나 API를 활용하는 RAG(Retrieval-Augmented Generation) 시스템은 LLM의 환각을 줄이고 사실적 정확도를 높이는 데 효과적인 것으로 입증되었습니다. 또한, 휴먼 인 더 루프(Human-in-the-Loop) 방식을 도입하여 AI가 중요한 결정을 내리기 전에 인간의 검토를 거치도록 하거나, 특정 도메인(domain)에 특화된 파인튜닝(fine-tuning)을 통해 모델의 전문성과 신뢰성을 향상시키는 노력도 이어지고 있습니다. 복잡한 에이전트 시스템에서는 작업 계획(task planning), 실행(execution), 그리고 피드백 루프(feedback loop)를 통해 신뢰성을 확보하려는 시도가 활발합니다. 궁극적으로 AI가 인간의 기대 수준에 부합하는 신뢰성을 갖추기 위해서는 기술적 개선뿐만 아니라, 오류 발생 시 책임 소재를 명확히 하고 투명성을 확보하는 사회적, 제도적 장치 마련이 필수적입니다.

**3. 프라이버시(Privacy)**
AI 시스템, 특히 LLM은 방대한 양의 데이터로 훈련되기 때문에 프라이버시 문제는 언제나 중요한 고려 사항입니다. 초기에는 주로 공개 웹 데이터로 훈련되었지만, 개인화된 AI 비서(AI assistant)나 기업용 AI 솔루션이 확산되면서 민감한 개인 및 기업 데이터의 활용이 불가피해졌습니다. 사용자의 이메일, 문서, 캘린더, 그리고 대화 기록 등은 AI가 더욱 유용해지는 데 필요한 핵심 정보원이지만, 동시에 심각한 프라이버시 침해 위험을 내포합니다.

이러한 문제에 대응하기 위해 동형 암호화(homomorphic encryption), 차등 프라이버시(differential privacy), 연합 학습(federated learning)과 같은 기술적 해결책이 주목받고 있습니다. 동형 암호화는 암호화된 상태에서 데이터 연산을 가능하게 하여 원본 데이터를 노출하지 않고도 AI 모델을 훈련하거나 추론할 수 있게 합니다. 차등 프라이버시는 데이터 분석 결과에서 개별 사용자의 정보가 유출될 가능성을 최소화하는 기술이며, 연합 학습은 여러 분산된 장치에서 모델을 훈련하고 학습된 가중치(weight)만 중앙 서버로 전송하여 개인 데이터가 장치를 떠나지 않도록 합니다. 또한, 온디바이스(on-device) AI는 데이터를 로컬(local)에서 처리함으로써 클라우드(cloud) 전송으로 인한 프라이버시 위험을 원천적으로 차단합니다. 그러나 이러한 기술적 해결책만으로는 충분하지 않습니다. 유럽연합의 GDPR(General Data Protection Regulation), 미국의 CCPA(California Consumer Privacy Act) 및 최근 발효된 EU AI Act와 같은 강력한 데이터 보호 규제는 AI 기업들에게 데이터 수집, 저장, 활용 방식에 대한 투명성과 책임성을 요구하고 있습니다. 사용자 동의(consent)를 명확히 하고, 데이터 사용 정책을 투명하게 공개하며, 불필요한 데이터는 수집하지 않는 '최소한의 데이터 원칙(data minimization principle)'을 준수하는 것이 중요합니다.

**4. 안전 및 보안(Safety and security)**
AI 시스템의 안전 및 보안 문제는 단순히 기술적 결함을 넘어 사회 전반에 걸쳐 광범위한 영향을 미칠 수 있습니다. 제미니(Gemini)의 이미지 생성 편향과 같이 의도치 않은 시스템 오류는 물론, 딥페이크(deepfakes)나 음성 복제(voice cloning)와 같은 AI 오용으로 인한 사회적 혼란, 그리고 프롬프트 인젝션(prompt injection)과 같은 해킹 공격은 AI 기술의 신뢰를 저해하는 주요 요인입니다.

최근에는 AI 시스템의 안전성을 강화하기 위한 '레드 팀(red teaming)' 활동이 활발하게 이루어지고 있습니다. 이는 AI 모델이 악의적인 프롬프트나 비정상적인 입력에 어떻게 반응하는지 미리 테스트하고 취약점을 찾아 개선하는 과정입니다. 또한, AI 모델의 학습 데이터에 악의적인 데이터를 주입하여 모델의 행동을 조작하는 '데이터 포이즈닝(data poisoning)' 공격이나, 모델의 내부 정보를 추출하려는 '모델 인버전 공격(model inversion attack)'과 같은 새로운 유형의 보안 위협에 대한 방어 기술도 개발되고 있습니다. AI 공급망 보안(AI supply chain security)은 모델 개발부터 배포까지 전 과정에서 잠재적인 취약점을 관리하고 통제하는 것을 목표로 합니다. AI 에이전트(AI agents)의 자율성이 높아질수록 예상치 못한 행동이나 오작동의 위험도 커지므로, 에이전트의 의사결정 과정을 투명하게 기록하고, 인간이 개입하여 제어할 수 있는 안전 장치(safety guardrails)를 마련하는 것이 필수적입니다. AI 안전성 연구는 편향성 완화(bias mitigation), 투명성(explainability), 그리고 견고성(robustness) 확보를 통해 AI가 사회에 긍정적인 영향을 미치도록 하는 데 중점을 둡니다.

**5. 사용자 인터페이스(User interface)**
많은 애플리케이션에서 LLM의 신뢰성 부족은 봇이 엉뚱한 방향으로 갈 경우 사용자가 개입할 수 있는 어떤 방법이 있어야 함을 의미합니다. 챗봇(chatbot)에서는 답변을 다시 생성하거나 여러 버전을 보여주고 사용자가 선택하도록 하는 것만큼 간단할 수 있습니다. 그러나 항공권 예약과 같이 오류가 큰 비용을 초래할 수 있는 애플리케이션에서는 적절한 감독을 보장하는 것이 더 까다로우며, 시스템은 너무 많은 방해로 사용자를 짜증 나게 하는 것을 피해야 합니다. 사용자가 비서에게 말하고 비서가 다시 말하는 자연어 인터페이스(natural language interfaces)에서는 문제가 훨씬 더 어렵습니다. 여기에 생성형 AI(generative AI)의 많은 잠재력이 있습니다. 한 가지 예로, 외국어 간판을 응시하고 있다는 것을 감지하는 것처럼, 필요할 때 요청하지 않아도 안경 속으로 사라져 당신에게 말을 거는 AI는 오늘날 우리가 가진 것과는 완전히 다른 경험일 것입니다. 그러나 제한된 사용자 인터페이스(user interface)는 잘못되거나 예상치 못한 행동에 대한 여지를 거의 남기지 않습니다.

최근에는 자연어 인터페이스(natural language interface)를 넘어선 다양한 형태의 AI 사용자 경험이 모색되고 있습니다. 음성, 시각, 제스처 등 다중 모달리티(multimodality)를 활용한 인터페이스는 AI와의 상호작용을 더욱 자연스럽고 직관적으로 만듭니다. 예를 들어, 스마트 안경(smart glasses)이나 웨어러블 기기(wearable devices)에 통합된 AI는 사용자의 상황을 인지하고 필요한 정보를 적시에 제공하는 '앰비언트 컴퓨팅(ambient computing)' 환경을 구현할 수 있습니다. 또한, 사용자가 특정 작업을 지시하면 AI 에이전트가 여러 단계를 거쳐 자율적으로 실행하고, 그 과정을 사용자에게 투명하게 보고하는 '에이전트 중심 UI(agentic UI)'도 발전하고 있습니다. 이러한 UI는 AI의 강력한 기능을 활용하면서도 사용자가 항상 제어권을 가지고 AI의 행동을 이해할 수 있도록 돕는 데 초점을 맞춥니다. 궁극적으로 성공적인 AI UI는 AI의 예측 불가능성을 관리하고, 사용자가 AI를 신뢰하고 효과적으로 활용할 수 있도록 돕는 '신뢰할 수 있는 상호작용(trustworthy interaction)'을 구축하는 데 있습니다.

**결론**
AI 지지자들은 AI 역량(AI capabilities)의 빠른 개선 속도 때문에 곧 엄청난 사회적, 경제적 효과를 보게 될 것이라고 종종 주장합니다. 우리는 그러한 역량 예측에 들어가는 추세 외삽(trend extrapolation)과 부주의한 사고방식에 회의적입니다. 더 중요하게는, AI 역량이 빠르게 개선된다 하더라도 개발자들은 위에서 논의된 과제들을 해결해야 합니다. 이들은 순전히 기술적인 것이 아니라 사회기술적(sociotechnical)인 문제이므로, 진전은 느릴 것입니다. 그리고 그러한 과제들이 해결된다 하더라도, 조직들은 AI를 기존 제품과 워크플로우(workflows)에 통합하고, 그 함정(pitfalls)을 피하면서 사람들이 생산적으로 AI를 사용하도록 훈련시켜야 합니다. 우리는 이것이 1, 2년이 아니라 10년 이상의 시간 척도(timescale)로 일어날 것으로 예상해야 합니다.

**추가 자료 읽기**
베네딕트 에반스(Benedict Evans)는 범용 언어 모델(general-purpose language models)을 사용하여 단일 목적 소프트웨어(single-purpose software)를 구축하는 것의 중요성에 대해 썼습니다.
1 분명히 말하자면, 우리는 최첨단(state-of-the-art) 모델에 대한 접근을 줄이는 것이 오용을 줄일 것이라고 생각하지 않습니다. 그러나 LLM의 경우, 오용은 합법적인 사용(생각을 필요로 함)보다 쉽기 때문에 오용이 널리 퍼진 것은 놀라운 일이 아닙니다.
2 AI 채택 속도는 상대적입니다. 애플이 제품에 AI를 통합하는 방식조차도 너무 빠르다는 비판을 받았습니다.
3 이들은 사용자 경험(user experience)에 중요한 요소들에 관한 것입니다. 우리는 환경 비용, 저작권이 있는 데이터로 훈련하는 것 등은 제외하고 있습니다.
4 예를 들어, API의 GPT-3.5 (text-davinci-003)는 백만 토큰(tokens)당 20달러였지만, 더 강력한 gpt-4o-mini는 단 15센트입니다.
5 분명히 말하자면, 데이터 소스가 공개적이라고 해서 프라이버시 문제가 없는 것은 아닙니다.
6 예를 들어, 구글은 "우리는 구글의 AI 모델을 훈련하는 데 도움이 되도록 공개적으로 사용 가능한 정보를 사용합니다"라고 말합니다. 다른 곳에서는 이메일과 같은 개인 데이터를 사용하여 서비스를 제공하고, 서비스를 유지 및 개선하며, 서비스를 개인화하고, 새로운 서비스를 개발할 수 있다고 말합니다. 이러한 공개 내용과 일치하는 한 가지 접근 방식은 제미니(Gemini)와 같은 모델의 사전 훈련(pre-training)에는 공개 데이터만 사용되지만, 이메일 자동 응답 봇(auto-response bot)과 같은 것을 만들기 위해 해당 모델을 미세 조정(fine-tune)하는 데는 개인 데이터가 사용된다는 것입니다. 우리가 아는 한 Anthropic은 유일한 예외입니다. Anthropic은 "사용자가 명시적인 허가를 주지 않는 한, 우리는 사용자 제출 데이터로 생성형 모델을 훈련하지 않습니다. 현재까지 우리는 고객 또는 사용자 제출 데이터를 생성형 모델 훈련에 사용하지 않았습니다"라고 말합니다. 프라이버시에 대한 이러한 약속은 칭찬할 만하지만, 만약 Anthropic이 제품 구축을 더 전적으로 수용한다면 회사에 불리하게 작용할 것이라고 우리는 예측합니다.