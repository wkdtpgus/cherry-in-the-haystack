인공지능 분야의 기업들은 막대한 금액을 하드웨어와 데이터 처리 시설에 투자할 예정이지만, 현재까지 그 투자에 비례하는 실제적 성과는 기대에 못 미치고 있습니다. 이러한 현실은 생성형 인공지능(generative AI) 기술이 과대평가되었다는 회의적인 시각을 증폭시키고 있습니다. 미래에 대한 단정적인 예측은 삼가겠지만, 현재 상황에 이르게 된 배경에 대해서는 명확한 분석을 제시할 수 있다고 판단합니다. 본 글에서는 인공지능 개발사들이 범했던 주요 오류와 그들이 이를 개선하기 위해 기울인 노력을 살펴봅니다. 나아가, 생성형 AI가 막대한 투자를 정당화할 만큼 상업적인 성공을 거두기 위해 반드시 넘어서야 할 다섯 가지 주요 난관에 대해 심도 있게 논의할 예정입니다. 초기의 과도한 기대가 현실적 평가로 전환되는 이 시점에서, 우리는 인공지능 기술의 '환멸의 골짜기(Trough of Disillusionment)'를 어떻게 헤쳐나갈지 고민해야 합니다.

**제품-시장 적합성 (Product-market fit)**
챗GPT가 대중에게 공개된 직후, 사용자들은 이 기술의 다양한 예측 불가능한 활용 방안을 찾아냈습니다. 이러한 현상은 AI 기술 개발자들에게 과도한 기대감을 안겨주었습니다. 그러나 많은 개발사는 실제 시장의 요구를 제대로 파악하지 못했으며, 단순한 기술 시연(proofs of concept)과 소비자가 신뢰하고 지속적으로 사용할 수 있는 완제품 사이의 엄청난 간극을 간과했습니다. 이러한 시장에 대한 오해는 대규모 언어 모델(LLMs)을 상업적 서비스로 전환하는 과정에서 두 가지 대조적이지만 모두 문제점을 내포한 접근 방식으로 나타났습니다.

초기 인공지능 기업들은 기술적 혁신 자체에 몰두한 나머지, 사용자 경험(user experience)과 서비스 디자인의 중요성을 간과했습니다. 이는 종종 강력한 모델이 있음에도 불구하고 실질적인 사용자 가치를 제공하는 제품으로 이어지지 못하는 결과를 낳았습니다. 대기업들은 기존 서비스에 인공지능 기능을 급하게 통합하려 했으나, 이는 기존 시스템과의 충돌이나 사용자의 혼란을 야기하며 오히려 부정적인 반응을 얻기도 했습니다. 대규모 언어 모델(LLM)의 보편적 특성은 개발자들이 모델에게 특정 지시를 내리는 방식만으로도 정교하게 기획된 상업적 서비스나 기능을 대신할 수 있다고 착각하게 만들었으며, 이로 인해 시장 요구와의 부합점을 모색하는 노력을 소홀히 하게 했습니다.

이러한 상황 속에서, 특정 산업 분야에 특화된 '수직적 인공지능(vertical AI)' 솔루션들이 주목받기 시작했습니다. 이들 솔루션은 범용 LLM의 한계를 보완하며, 특정 도메인의 깊은 이해를 바탕으로 맞춤형 가치를 제공함으로써 제품-시장 적합성을 찾아가고 있습니다. 또한, 오픈소스(open-source) 모델의 발전은 스타트업들이 거대 기업의 모델에 의존하지 않고도 혁신적인 제품을 개발할 수 있는 새로운 기회를 제공하고 있습니다. 시장은 이제 단순히 '가장 큰 모델'이나 '가장 많은 기능'을 넘어, 사용자에게 실질적인 문제를 해결해주는 '가장 유용한 제품'을 요구하고 있습니다. 기업들은 이제 인공지능 기술의 잠재력을 현실적인 비즈니스 가치로 전환하기 위해, 기술 개발만큼이나 시장 분석과 사용자 중심의 제품 설계에 집중해야 한다는 교훈을 얻고 있습니다. 과거의 시행착오를 통해 학습하며, 인공지능 기술의 상업적 성공을 위한 새로운 전략을 모색하는 시점에 와 있습니다.

**소비자 AI를 위한 5가지 주요 과제**
소비자용 인공지능 제품을 매력적으로 만들기 위해 개발자들이 대규모 언어 모델(LLM)에서 극복해야 할 다섯 가지 주요 제약 사항이 있습니다. 3 (저희는 이 문제들 중 상당 부분을 오는 8월 29일 개최되는 '유용하고 신뢰할 수 있는 AI 에이전트(AI agents) 구축' 온라인 워크숍에서 심도 있게 다룰 예정입니다.) 이 과제들은 기술적 완성도를 넘어 실제 비즈니스 모델과 사용자 수용성 측면에서 반드시 해결해야 할 핵심 요소들입니다.

**1. 비용 (Cost)**
인공지능의 성능 자체가 아니라 운영 비용이 주된 제약이 되는 응용 분야가 많습니다. 예를 들어, 단순한 채팅 응용 프로그램에서도 비용 문제는 봇이 얼마나 많은 과거 대화 기록을 유지하고 참조할 수 있는지를 결정합니다. 대화가 길어질수록 매번 응답을 생성하기 위해 전체 기록을 처리해야 하므로, 그 비용은 기하급수적으로 증가하게 됩니다. 비용 효율성 측면에서는 눈부신 발전이 있었습니다. 최근 1년 반 사이에, 유사한 성능을 지닌 인공지능 모델을 구동하는 데 드는 운영비는 백 배 이상 줄어들었습니다. 4 그 결과, 일부 기업들은 대규모 언어 모델(LLMs)이 "측정하기에는 너무 저렴하다(too cheap to meter)"고 주장하거나, 조만간 그렇게 될 것이라고 단언합니다. 하지만 API(Application Programming Interface) 사용료가 실제로 무료가 될 때까지는 이러한 주장을 그대로 받아들이기 어렵습니다. 더욱 심각하게는, 상당수의 응용 프로그램에서 비용 절감이 곧 정확도 향상으로 직결되기 때문에 비용 문제는 계속해서 중요하게 다루어질 것입니다. 이는 LLM의 본질적인 비결정성(randomness)을 감안할 때, 동일한 작업을 수십, 수천, 심지어 수백만 번 반복하여 재시도하는 것이 원하는 결과를 얻을 확률을 높이는 효과적인 전략임이 드러났기 때문입니다. 따라서 모델 운영 비용이 낮아질수록, 주어진 예산 내에서 더 많은 시도를 할 수 있게 됩니다. 저희는 에이전트(agents)에 관한 최근 연구에서 이를 정량적으로 분석했으며, 이후 다른 많은 논문에서도 유사한 결론을 내렸습니다. 또한, 모델 경량화(model distillation)나 양자화(quantization) 같은 기술적 최적화 노력도 비용 절감에 기여하고 있습니다. 그렇긴 하지만, 대부분의 응용 분야에서 비용 최적화가 더 이상 심각한 문제가 되지 않는 수준에 도달할 가능성도 충분히 존재합니다.

**2. 신뢰성 (Reliability)**
저희는 인공지능의 수행 능력(capability)과 믿을 수 있는 작동(reliability)을 서로 독립적인 개념으로 바라봅니다. 만약 인공지능 시스템이 9할의 경우에만 정확히 임무를 완수한다면, 이는 해당 작업을 처리할 능력은 있으나 지속적으로 믿을 수 있는 방식으로 실행하지 못한다고 평가할 수 있습니다. 90%의 정확도를 달성하는 기술이 100%의 완벽함을 보장할 가능성은 희박합니다. 통계적 학습(statistical learning)에 기반한 시스템에서는 완벽한 정밀도(perfect accuracy)를 얻는 것이 본질적으로 어렵습니다. 광고 타겟팅(ad targeting), 사기 탐지(fraud detection), 또는 최근의 기상 예측과 같은 기계 학습(machine learning)의 성공 사례들을 떠올려보면, 이들 분야에서 궁극적인 목표는 완벽한 정확도가 아닙니다. 시스템이 기존의 최고 수준(state of the art)보다 나은 성능을 보인다면 그것만으로도 충분히 유용합니다. 의료 진단 및 기타 건강 관리 응용 분야(healthcare applications)에서도 우리는 상당한 수준의 오차를 허용합니다.

그러나 개발자들이 소비자용 제품에 인공지능을 통합할 때, 소비자들은 인공지능이 일반적인 소프트웨어처럼 작동하기를 기대하며, 이는 곧 인공지능이 예측 가능하고 확정적인(deterministically) 방식으로 행동해야 함을 의미합니다. 예를 들어, 인공지능 기반 여행 비서가 90%의 확률로만 정확한 목적지로 휴가를 예약한다면, 이는 결코 성공적인 제품이라 할 수 없을 것입니다. 이전에 저희가 언급했듯이, 이러한 신뢰성의 한계는 최근 출시된 인공지능 기반 기기들의 실패를 부분적으로 설명합니다. 인공지능 개발자들은 이러한 사실을 인지하는 데 느렸는데, 이는 전문가로서 저희가 인공지능을 전통적인 소프트웨어와는 근본적으로 다른 방식으로 개념화하는 데 익숙했기 때문입니다. 저희 두 필자 역시 일상 업무에서 챗봇(chatbots)과 에이전트(agents)를 빈번하게 사용하며, 이러한 도구들의 환각(hallucinations) 현상이나 부족한 신뢰성을 우회하는 방식이 거의 습관화되었습니다. 1년 전만 해도 인공지능 개발자들은 비전문가 사용자들도 인공지능에 적응하는 방법을 배울 것이라고 기대하거나 가정했지만, 이제는 기업들이 사용자 기대를 충족시키기 위해 인공지능을 조정하고, 인공지능이 전통적인 소프트웨어처럼 작동하도록 만들어야 한다는 점이 점차 명확해지고 있습니다.

신뢰성 향상은 프린스턴(Princeton) 대학 저희 연구팀의 핵심 관심사 중 하나입니다. 현재로서는 확률적 요소(stochastic components, LLMs)를 포함하는 시스템에서 확정적인(deterministic) 결과물을 도출하는 것이 과연 가능한지에 대한 근본적인 질문은 여전히 미해결 상태입니다. 일부 기업들은 이미 신뢰성 문제를 해결했다고 주장하기도 했습니다. 예를 들어, 법률 기술(legal tech) 솔루션 제공업체들은 "환각 없는(hallucination-free)" 시스템을 대대적으로 홍보했습니다. 하지만 이러한 주장은 시기상조였음이 여러 사례를 통해 드러났습니다. 특히, 인간 피드백 기반 강화 학습(RLHF)과 같은 방법론이 신뢰성 개선에 기여하고 있지만, 비결정론적 시스템의 디버깅(debugging) 난이도는 여전히 큰 과제입니다.

**3. 프라이버시 (Privacy)**
과거 기계 학습(machine learning)은 광고 타겟팅(ad targeting)을 위한 웹 브라우징 기록이나 건강 관리 기술(health tech)을 위한 의료 기록과 같이 민감한 개인 정보에 의존하는 경우가 많았습니다. 이러한 점에서 대규모 언어 모델(LLM)은 웹 페이지나 공개 서적과 같은 공공 데이터(public sources)로 주로 학습된다는 점에서 다소 예외적인 특성을 지닙니다. 5 하지만 인공지능 기반의 개인 비서가 등장하면서, 개인 정보 보호에 관한 쟁점이 다시금 중요한 관심사로 떠올랐습니다. 진정으로 유용한 비서를 개발하기 위해서는 기업들이 사용자 상호작용(user interactions) 데이터를 기반으로 시스템을 훈련해야 합니다. 예를 들어, 이메일 작성을 능숙하게 돕는 모델이라면 실제 이메일 데이터로 훈련하는 것이 매우 효과적일 것입니다. 그러나 기업들의 개인 정보 보호 정책은 이 부분에 대해 모호한 경우가 많으며, 실제로 어느 정도까지 이러한 데이터 활용이 이루어지는지는 불분명합니다. 6 이메일, 문서, 스크린샷 등은 단순한 채팅 기록보다 훨씬 더 민감한 개인 정보를 포함할 수 있습니다.

훈련(training) 단계뿐만 아니라 추론(inference) 단계에서도 독특한 유형의 프라이버시 문제가 발생합니다. 인공지능 비서가 우리에게 실질적인 도움을 주려면 우리의 개인 데이터에 접근하고 이를 처리할 수 있어야 하기 때문입니다. 예를 들어, 마이크로소프트는 코파일럿(CoPilot) AI가 사용자 활동을 기억하도록 돕기 위해 사용자의 PC 화면을 주기적으로 스크린샷으로 캡처하는 기능을 발표하여 논란을 불러일으켰고, 결국 거센 반발에 부딪혀 이 기능을 철회했습니다. 우리는 "데이터가 기기를 벗어나지 않는다"는 식의 순전히 기술적인 관점의 프라이버시 해석에 대해 경고합니다. 머리디스 휘태커(Meredith Whittaker)는 기기 내(on-device) 사기 탐지(fraud detection) 기술이 상시 감시(always-on surveillance)를 일상화할 수 있으며, 이로 인해 구축된 인프라가 결국 더 억압적인 목적을 위해 재활용될 수 있다고 주장합니다. 이러한 우려에도 불구하고, 기술 혁신은 분명 개인 정보 보호를 강화하는 데 긍정적인 기여를 할 수 있습니다. 연합 학습(federated learning)이나 차등 프라이버시(differential privacy) 같은 기술은 민감한 데이터를 중앙 서버로 전송하지 않고도 모델을 훈련하거나, 개별 정보 노출 위험을 최소화하면서도 유용한 통계를 얻을 수 있는 가능성을 제시합니다. 동시에 GDPR(유럽 일반 개인정보보호법)과 같은 규제 환경은 기업들에게 더 엄격한 데이터 처리 기준을 요구하고 있습니다.

**4. 안전 및 보안 (Safety and security)**
인공지능의 안전성 및 보안과 관련하여 여러 가지 중요한 우려 사항이 존재합니다. 여기에는 구글 제미니(Gemini)의 이미지 생성 편향과 같은 의도치 않은 시스템 오류, 음성 복제(voice cloning)나 딥페이크(deepfakes)와 같이 인공지능이 악용되는 사례, 그리고 사용자 정보를 유출하거나 다른 형태로 해를 끼칠 수 있는 프롬프트 주입(prompt injection)과 같은 해킹 위협이 포함됩니다. 예기치 않게 발생하는 오류들은 해결이 가능하다고 판단됩니다. 대부분의 인공지능 오용 사례에 대해서는, 근본적으로 오용될 수 없는 모델을 개발하는 것이 불가능에 가깝기 때문에, 방어 전략은 주로 서비스 제공 단계(downstream)에서 이루어져야 한다는 것이 저희의 견해입니다. 물론 모든 이가 이 의견에 동의하는 것은 아니지만, 기업들은 불가피한 오용으로 인해 지속적으로 부정적인 평판을 감수해야 할 것이며, 이는 사업 운영의 한 부분으로 받아들이는 듯합니다.

세 번째 유형인 해킹 위협에 대해 자세히 살펴보겠습니다. 저희가 파악하기로는, 이 분야가 기업들이 가장 적은 관심을 기울이는 부분인 것 같습니다. 이론적으로는, 사용자들 사이에 퍼져나가 해당 사용자의 인공지능 비서(AI assistants)를 속여 악성 웜(AI worms)의 추가 복사본을 생성하는 등의 해로운 행동을 유발할 수 있는 치명적인 인공지능 웜 공격이 충분히 가능합니다. 이미 상용 제품에서 이러한 취약점(vulnerabilities)을 시연한 수많은 개념 증명(proof-of-concept) 사례와 버그 바운티(bug bounties) 프로그램들이 있었음에도 불구하고, 실제 환경에서 이러한 유형의 대규모 공격이 발생한 사례는 아직 목격되지 않았습니다. 이것이 인공지능 비서의 낮은 보급률 때문인지, 아니면 기업들이 급조한 조잡한 방어 수단(clumsy defenses)만으로도 충분했기 때문인지, 혹은 다른 미지의 이유 때문인지는 명확하지 않습니다. 하지만 이러한 잠재적 위협은 결코 간과되어서는 안 됩니다. 데이터 포이즈닝(data poisoning)이나 모델 역공격(model inversion attacks)과 같은 정교한 공격 방식에 대한 방어책 마련과 함께, 강력한 AI 거버넌스(governance) 및 레드 팀(red-teaming) 활동을 통한 선제적 대응이 필수적입니다.

**5. 사용자 인터페이스 (User interface)**
다수의 응용 프로그램에서 대규모 언어 모델(LLM)의 불확실한 성능은 봇이 예상치 못한 방향으로 작동할 때 사용자가 직접 개입할 수 있는 수단이 반드시 필요하다는 것을 시사합니다. 챗봇(chatbot)의 경우, 단순히 답변을 다시 생성하거나 여러 가지 대안을 제시하여 사용자가 선택하도록 하는 방식으로 쉽게 해결될 수 있습니다. 하지만 항공권 예약과 같이 오류가 발생했을 때 막대한 비용 손실을 초래할 수 있는 응용 프로그램에서는 적절한 사용자 감독을 보장하는 것이 훨씬 더 복잡해집니다. 동시에 시스템은 과도한 개입 요청으로 사용자를 불편하게 만들지 않도록 균형을 맞춰야 합니다. 사용자가 비서에게 말하고 비서가 다시 응답하는 자연어 인터페이스(natural language interfaces) 환경에서는 이러한 문제가 더욱 난해해집니다. 그럼에도 불구하고, 바로 이 영역에 생성형 인공지능(generative AI)의 엄청난 잠재력이 숨어 있습니다. 예를 들어, 사용자가 외국어 간판을 쳐다보는 것을 감지하고, 별도의 요청 없이도 안경 속으로 번역된 내용을 들려주는 인공지능 비서(AI assistant)는 오늘날 우리가 경험하는 것과는 차원이 다른 혁신적인 사용자 경험을 제공할 것입니다. 그러나 현재의 제한적인 사용자 인터페이스(user interface)는 인공지능의 잘못되거나 예측 불가능한 행동에 대해 사용자가 개입하거나 수정할 여지를 거의 제공하지 못하고 있습니다. 멀티모달(multimodal) 인터페이스의 발전과 함께, 인공지능이 사용자의 필요를 예측하여 능동적으로(proactive) 도움을 제공하되, 과도한 침해나 간섭으로 느껴지지 않도록 섬세하게 설계된 인터페이스가 미래 인공지능 제품의 성공을 좌우할 것입니다.

**결론**
인공지능 기술의 옹호자들은 AI의 성능이 급속도로 향상되고 있으므로, 머지않아 사회와 경제 전반에 걸쳐 막대한 파급 효과를 가져올 것이라고 자주 역설합니다. 하지만 저희는 이러한 능력 예측에 내재된 단순한 추세 외삽(trend extrapolation)과 신중하지 못한 사고방식에 대해 회의적인 시각을 가지고 있습니다. 더욱 중요한 점은, 설령 인공지능의 역량이 아무리 빠르게 발전하더라도 개발자들은 위에 제시된 여러 난관들을 반드시 해결해야 한다는 것입니다. 이 문제들은 단순히 기술적인 차원에 머무르지 않고 사회기술적(sociotechnical) 성격을 띠고 있기 때문에, 그 해결 과정은 예상보다 더딜 수 있습니다.

게다가, 이러한 복합적인 과제들이 성공적으로 해결된다 하더라도, 조직들은 인공지능을 기존의 제품과 업무 흐름(workflows)에 효과적으로 통합하고, 잠재적인 위험 요소(pitfalls)를 회피하며, 사람들이 인공지능을 생산적으로 활용할 수 있도록 교육하는 데 상당한 노력을 기울여야 합니다. 이러한 전반적인 변화는 단 1, 2년 만에 이루어질 것이 아니라, 최소한 10년 이상의 장기적인 시간 척도(timescale)에서 일어날 것으로 예상하는 것이 현실적입니다. 인공지능의 미래를 논할 때는 기술적 낙관론을 넘어선 다학제적이고 균형 잡힌 시각이 필수적입니다.

**참고 자료 및 추가 설명**
베네딕트 에반스(Benedict Evans)는 범용 언어 모델(general-purpose language models)을 활용하여 특정 목적에 최적화된 소프트웨어(single-purpose software)를 개발하는 것의 중요성에 대해 강조한 바 있습니다.
1 명확히 하자면, 저희는 최첨단(state-of-the-art) 모델에 대한 접근성을 제한하는 것이 오용 사례를 줄일 것이라고는 생각하지 않습니다. 그러나 대규모 언어 모델(LLM)의 경우, 정당한 사용(숙고가 필요한)보다 오용이 훨씬 쉽기 때문에, 광범위한 오용이 발생하는 것은 놀라운 일이 아닙니다.
2 인공지능 기술의 시장 채택 속도는 상대적인 관점에서 봐야 합니다. 애플이 자사 제품에 인공지능을 통합하는 방식조차도 일부에서는 너무 성급하다는 비판을 받기도 했습니다.
3 여기서 언급된 과제들은 주로 사용자 경험(user experience)에 직접적인 영향을 미치는 핵심 요소들입니다. 환경 비용이나 저작권이 있는 데이터로 모델을 훈련하는 문제 등은 이번 논의에서 제외되었습니다.
4 예를 들어, 기존 API(Application Programming Interface) 기반의 GPT-3.5 (text-davinci-003)는 백만 토큰(tokens)당 20달러의 비용이 발생했지만, 성능이 더 우수한 gpt-4o-mini는 단 15센트에 불과합니다.
5 공개된 데이터 소스라고 해서 개인 정보 보호(privacy) 문제가 완전히 사라지는 것은 아님을 분명히 밝힙니다.
6 예를 들어, 구글은 "우리는 구글의 AI 모델을 훈련하는 데 도움이 되도록 공개적으로 사용 가능한 정보를 사용합니다"라고 명시합니다. 다른 정책에서는 이메일과 같은 개인 데이터를 사용하여 서비스를 제공하고, 유지 및 개선하며, 개인화하고, 새로운 서비스를 개발할 수 있다고 언급합니다. 이러한 공개 내용과 일치하는 한 가지 접근 방식은 제미니(Gemini)와 같은 모델의 사전 훈련(pre-training)에는 공개 데이터만 사용되지만, 이메일 자동 응답 봇(auto-response bot)과 같은 특정 기능을 위해 모델을 미세 조정(fine-tune)하는 데는 개인 데이터가 활용될 수 있다는 것입니다. 저희가 아는 바로는 앤트로픽(Anthropic)이 유일한 예외인데, 앤트로픽은 "사용자가 명시적인 허가를 주지 않는 한, 우리는 사용자 제출 데이터로 생성형 모델을 훈련하지 않습니다. 현재까지 우리는 고객 또는 사용자 제출 데이터를 생성형 모델 훈련에 사용하지 않았습니다"라고 밝히고 있습니다. 개인 정보 보호에 대한 이러한 약속은 칭찬할 만하지만, 만약 앤트로픽이 제품 개발에 더 적극적으로 나선다면 이러한 정책이 회사에 불리하게 작용할 수도 있다고 저희는 예측합니다.