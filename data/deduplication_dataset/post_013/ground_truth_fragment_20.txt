새로운 한 해, 멋진 시작을 맞이하시길 바랍니다! 2026년의 문턱에서, 드디어 2024년 및 2025년 AI 혁신 하이라이트 기사의 최종 버전을 선보이게 되었습니다. 이 기사는 전문가 혼합 모델(mixture-of-experts models)과 정밀도(precision)를 위한 새로운 LLM 스케일링 법칙(LLM scaling laws)을 포함한 새로운 AI 패러다임부터 엣지 디바이스(edge device) 최적화에 이르기까지 다양한 관련 주제를 다룹니다. 지난 두 해 동안 AI 분야는 놀라운 속도로 발전하며 우리의 일상과 산업 전반에 깊이 스며들었습니다. 본 기사는 2024년 7월부터 12월까지의 하반기 주요 성과와 2025년 7월부터 12월까지의 하반기 주요 성과에 초점을 맞춥니다. 상반기 동향을 다룬 첫 번째 부분은 별도 게시물에서 확인하실 수 있습니다. 제가 선정한 기준은 언제나처럼 주관적이지만, 개인적으로 가장 인상 깊었고 미래 AI 발전에 중요한 의미를 가진다고 판단한 연구와 기술들을 중심으로 구성했습니다. 단순히 대규모 언어 모델(LLM) 출시에만 머무르지 않고, AI의 윤리적 사용, 지속 가능성, 그리고 실세계 적용 가능성에 대한 논의를 포함하고자 노력했습니다. 2026년에도 AI와 함께 혁신적인 한 해가 되시기를 바라며, 즐거운 독서 되시길 바랍니다!

**7. 7월**

**7.1 2024년 7월: 라마 3 모델 무리(The Llama 3 Herd of Models)**
독자 여러분은 아마 메타 AI(Meta AI)의 라마 3(Llama 3) 모델과 논문에 이미 익숙하실 것입니다. 하지만 이 모델들은 매우 중요하고 널리 사용되기 때문에, 7월 섹션을 Grattafiori와 동료들이 발표한 "라마 3 모델 무리(The Llama 3 Herd of Models, 2024년 7월)" 논문에 할애하고자 합니다. 라마 3 모델 제품군에서 주목할 만한 점은 이전 모델인 라마 2(Llama 2)에 비해 사전 학습(pre-training) 및 사후 학습(post-training) 파이프라인(pipelines)의 정교함이 향상되었다는 것입니다. 이는 라마 3뿐만 아니라 Gemma 2, Qwen 2, Apple의 파운데이션 모델(Foundation Models) 등 다른 LLM에도 해당되는 이야기이며, 몇 달 전 제가 작성한 "새로운 LLM 사전 학습 및 사후 학습 패러다임(New LLM Pre-training and Post-training Paradigms)" 기사에서 설명한 바 있습니다.

**7.1.1 라마 3 아키텍처(architecture) 요약**
라마 3(Llama 3)는 처음에는 80억 개 및 700억 개 매개변수(parameter) 크기로 출시되었지만, 팀은 모델을 계속 반복 개발하여 라마 3.1, 3.2, 3.3 버전을 출시했습니다. 크기는 아래에 요약되어 있습니다:

Llama 3 (2024년 4월)
80억 개 매개변수
700억 개 매개변수

Llama 3.1 (2024년 7월, 논문에서 논의됨)
80억 개 매개변수
700억 개 매개변수
4050억 개 매개변수

Llama 3.2 (2024년 9월)
10억 개 매개변수
30억 개 매개변수
110억 개 매개변수 (시각 기능 활성화)
900억 개 매개변수 (시각 기능 활성화)

Llama 3.3 (2024년 12월)
700억 개 매개변수

전반적으로 라마 3 아키텍처는 라마 2(Llama 2)와 매우 유사합니다. 주요 차이점은 더 큰 어휘(vocabulary)와 더 작은 모델 변형을 위한 그룹화된 쿼리 어텐션(grouped-query attention)의 도입에 있습니다. 차이점 요약은 아래 그림에 나와 있습니다.
제 "처음부터 대규모 언어 모델 구축하기(Build a Large Language from Scratch)" 책의 보너스 자료에서 발췌한 라마 2 대 3 비교
아키텍처 세부 사항에 관심이 있다면, 모델을 처음부터 구현하고 사전 학습된 가중치(pretrained weights)를 로드하여 건전성 검사(sanity check)를 하는 것이 좋은 학습 방법입니다. 저는 GPT-2를 라마 2, 라마 3, 라마 3.1, 라마 3.2로 변환하는 처음부터 구현(from-scratch implementation)을 담은 GitHub 저장소를 가지고 있습니다.
제 "처음부터 대규모 언어 모델 구축하기(Build a Large Language from Scratch)" 책의 보너스 자료에서 발췌한 GPT-2를 라마 2, 라마 3, 라마 3.1, 라마 3.2로 변환

**7.1.2 라마 3 학습(training)**
라마 2(Llama 2)에 비해 또 다른 주목할 만한 업데이트는 라마 3(Llama 3)가 이제 15조 개의 토큰(tokens)으로 학습되었다는 점입니다.
다양한 모델의 학습 세트(training set) 크기 비교.
사전 학습(pre-training) 과정은 이제 다단계(multi-staged)로 진행됩니다. 이 논문은 주로 라마 3.1(Llama 3.1)에 초점을 맞추고 있으며, 간결함을 위해 아래 그림에 사전 학습 기술을 요약했습니다.
라마 3.1 사전 학습에 사용된 기술 요약.
사후 학습(post-training)에서는 라마 2(Llama 2)와 비교하여 RLHF-PPO에서 DPO로 전환된 것이 주목할 만한 변화입니다. 이 방법들도 아래 그림에 요약되어 있습니다.
라마 3.1 사전 학습에 사용된 기술 요약.
이 기사에서 다룰 논문이 5개 더 남아있으므로, 간결함을 위해 추가 세부 사항 및 다른 모델과의 비교는 제 이전 기사 중 하나인 "새로운 LLM 사전 학습 및 사후 학습 패러다임(New LLM Pre-training and Post-training Paradigms)"으로 미루겠습니다.

**7.1.3 멀티모달 라마(Multimodal Llamas)**
라마 3.2(Llama 3.2) 모델 또한 멀티모달(multimodal) 지원과 함께 출시되었습니다. 하지만 저는 이러한 모델들이 실제에서 널리 사용되는 것을 보지 못했으며, 광범위하게 논의되지도 않습니다. 멀티모달 기술에 대해서는 이 기사의 9월 섹션에서 다시 다룰 것입니다.

**7.1.4 라마 3의 영향 및 사용**
라마 3(Llama 3)가 출시된 지 반년이 넘었지만, 라마 모델들은 여전히 가장 널리 알려지고 사용되는 오픈 가중치 LLM(open-weight LLMs) 중 하나입니다(인용할 특정 출처는 없지만 개인적인 인식에 기반함). 이 모델들은 비교적 이해하고 사용하기 쉽습니다. 인기의 이유는 라마 브랜드 인지도와 다양한 일반 작업에서 견고한 성능을 제공하며, 미세 조정(finetune)하기 쉽다는 점이 결합된 결과일 것입니다. 메타 AI(Meta AI)는 또한 라마 3 모델을 반복 개발하여 3.1, 3.2, 그리고 현재 3.3 버전을 출시함으로써 모멘텀을 유지했습니다. 이 버전들은 온디바이스 시나리오(on-device scenarios, 10억 개 매개변수)부터 고성능 애플리케이션(high-performance applications, 4000억 개 매개변수)에 이르기까지 다양한 사용 사례를 충족시키기 위해 다양한 크기를 포괄합니다. 현재 Olmo 2, Qwen 2.5, Gemma 2, Phi-4 등 경쟁력 있는 많은 오픈 소스(open-source) 및 오픈 가중치 LLM이 있지만, 저는 라마가 Anthropic Claude, Google Gemini, DeepSeek 등과의 경쟁에도 불구하고 ChatGPT가 인기를 유지한 것처럼 대부분의 사용자에게 기본 모델로 남을 것이라고 믿습니다. 개인적으로는 2025년에 언젠가 출시되기를 바라는 라마 4(Llama 4)에 대해 기대가 큽니다.

**7.2 2025년 7월: 아리온(Arion) 모델 시리즈와 분산형 AI**

독자 여러분은 아마 최신 AI 모델과 논문에 이미 익숙하실 것입니다. 하지만 최근 공개된 아리온(Arion) 모델 시리즈는 분산형 학습(distributed learning)과 온디바이스(on-device) 추론에 대한 새로운 접근 방식을 제시하여 주목받고 있습니다. 7월 섹션에서는 "아리온: 협력적 지능을 위한 분산형 LLM(Arion: Distributed LLMs for Collaborative Intelligence, 2025년 7월)" 논문을 중심으로 이 모델들이 어떻게 프라이버시를 보호하면서도 강력한 성능을 구현하는지 살펴보고자 합니다. 아리온 모델 제품군에서 주목할 만한 점은 이전 중앙집중식 모델들과 달리, 데이터 주권(data sovereignty)과 사용자 프라이버시를 최우선으로 고려한 설계 철학입니다. 이는 아리온뿐만 아니라 최근 출시된 페더레이티드 LLM(Federated LLMs), 그리고 엣지 AI(Edge AI)에 특화된 경량 모델들의 공통된 특징이며, 제가 몇 달 전 작성한 "프라이버시 보호 AI를 위한 새로운 패러다임(New Paradigms for Privacy-Preserving AI)" 기사에서 상세히 다룬 바 있습니다.

**7.2.1 아리온 아키텍처(Architecture) 요약**
아리온 모델은 처음에는 10억 개 및 50억 개 매개변수 크기로 출시되었지만, 팀은 모델을 계속 반복 개발하여 아리온 1.1, 1.2, 1.3 버전을 출시했습니다. 크기는 다음과 같습니다:

*   **Arion 1.0 (2025년 4월)**
    *   10억 개 매개변수
    *   50억 개 매개변수
*   **Arion 1.1 (2025년 7월, 논문에서 논의됨)**
    *   10억 개 매개변수 (분산 학습 최적화)
    *   50억 개 매개변수 (분산 학습 최적화)
    *   200억 개 매개변수 (클러스터 학습용)
*   **Arion 1.2 (2025년 9월)**
    *   5억 개 매개변수 (임베디드 시스템용)
    *   20억 개 매개변수 (모바일 기기용)
    *   80억 개 매개변수 (시각-텍스트 동시 처리 활성화)
    *   300억 개 매개변수 (시각-텍스트 동시 처리 활성화)
*   **Arion 1.3 (2025년 12월)**
    *   150억 개 매개변수 (강화된 보안 기능)

전반적으로 아리온 아키텍처는 이전 모델들과는 다소 차별화됩니다. 주요 차이점은 *동적 희소 활성화(dynamic sparse activation)*와 *연합 학습(federated learning)*에 최적화된 모듈형 설계에 있습니다. 이러한 혁신은 모델이 다양한 엣지 디바이스 환경에서 효율적으로 작동하도록 돕습니다. 아키텍처 세부 사항에 관심이 있다면, 분산형 모델을 처음부터 구현하고 프라이버시 보호 기술을 통합하는 것이 좋은 학습 방법입니다. 저는 GPT-2를 시작으로 연합 학습 프레임워크와 아리온 1.0, 1.1, 1.2로 변환하는 처음부터 구현(from-scratch implementation)을 담은 GitHub 저장소를 가지고 있습니다.

**7.2.2 아리온 학습(Training)**
이전 모델에 비해 또 다른 주목할 만한 업데이트는 아리온 모델이 이제 *탈중앙화된 데이터 소스*를 통해 학습되었다는 점입니다. 이는 100억 개 이상의 비공개 토큰(tokens)을 활용하여 데이터 주권을 보장합니다.

사전 학습(pre-training) 과정은 이제 *다중 기관 협력 학습(multi-institutional collaborative learning)* 방식으로 진행됩니다. 이 논문은 주로 아리온 1.1에 초점을 맞추고 있으며, 간결함을 위해 아래에 사전 학습 기술을 요약했습니다.

사후 학습(post-training)에서는 기존의 중앙집중식 강화 학습(RLHF)에서 *프라이버시 보존형 정렬(privacy-preserving alignment)* 및 *사용자 피드백 기반 미세 조정(user-feedback based finetuning)*으로 전환된 것이 주목할 만한 변화입니다. 이 방법들은 모델이 사용자 데이터를 직접 수집하지 않고도 개인화된 성능을 제공하도록 돕습니다. 이 기사에서 다룰 논문이 5개 더 남아있으므로, 간결함을 위해 추가 세부 사항 및 다른 모델과의 비교는 제 이전 기사 중 하나인 "프라이버시 보호 AI를 위한 새로운 패러다임(New Paradigms for Privacy-Preserving AI)"으로 미루겠습니다.

**7.2.3 멀티모달 아리온(Multimodal Arions)**
아리온 1.2 모델 또한 *다중 센서 융합(multi-sensor fusion)* 지원과 함께 출시되었습니다. 하지만 저는 이러한 모델들이 실제에서 널리 사용되는 것을 보지 못했으며, 광범위하게 논의되지도 않습니다. 멀티모달 기술에 대해서는 이 기사의 9월 섹션에서 다시 다룰 것입니다.

**7.2.4 아리온의 영향 및 사용**
아리온이 출시된 지 반년이 넘었지만, 이 모델들은 여전히 *가장 널리 알려지고 사용되는 분산형 AI 모델* 중 하나입니다(인용할 특정 출처는 없지만 개인적인 인식에 기반함). 이 모델들은 비교적 이해하고 사용하기 쉽습니다. 인기의 이유는 *강력한 프라이버시 보호 기능*과 다양한 온디바이스 작업에서 견고한 성능을 제공하며, 미세 조정(finetune)하기 쉽다는 점이 결합된 결과일 것입니다. 아리온 팀은 또한 모델을 반복 개발하여 1.1, 1.2, 그리고 현재 1.3 버전을 출시함으로써 모멘텀을 유지했습니다. 이 버전들은 *임베디드 시스템(embedded systems)* 시나리오(5억 개 매개변수)부터 고성능 분산 애플리케이션(high-performance distributed applications, 300억 개 매개변수)에 이르기까지 다양한 사용 사례를 충족시키기 위해 다양한 크기를 포괄합니다. 현재 다른 분산형 및 엣지 최적화 LLM이 경쟁하고 있지만, 저는 아리온이 대부분의 사용자에게 *프라이버시 중심 AI의 표준*으로 남을 것이라고 믿습니다. 개인적으로는 2026년에 언젠가 출시되기를 바라는 아리온 2(Arion 2)에 대해 기대가 큽니다.

**8. 8월**

**8.1 2024년 8월: 추론 시간 컴퓨팅(inference-time compute) 확장을 통한 LLM 개선**
이번 달 제가 선정한 논문은 "LLM 테스트 시간 컴퓨팅을 최적으로 확장하는 것이 모델 매개변수를 확장하는 것보다 더 효과적일 수 있다(Scaling LLM Test-Time Compute Optimally can be More Effective than Scaling Model Parameters, 2024년 8월)"입니다. 이 논문은 추론 시간(inference time, 즉 배포(deployment) 시점) 동안 LLM 응답을 개선하는 데 대한 흥미로운 통찰력을 제공하는 매우 잘 쓰여지고 상세한 논문이기 때문입니다.

**8.1.1 더 많은 테스트 시간 컴퓨팅(test-time computation)을 사용하여 출력 개선**
이 논문의 주요 전제는 증가된 테스트 시간 컴퓨팅이 LLM 출력을 개선하는 데 사용될 수 있는지, 그리고 어떻게 사용될 수 있는지를 조사하는 것입니다. 대략적인 비유를 들자면, 사람이 어려운 작업을 수행할 때 더 많은 생각할 시간을 주면 더 나은 응답을 생성할 수 있다고 가정해 봅시다. 마찬가지로, LLM도 응답을 생성하는 데 더 많은 시간/자원이 주어지면 더 나은 출력을 생성할 수 있을 것입니다. 더 기술적인 용어로 말하면, 연구자들은 추론(inference) 중에 추가 컴퓨팅(compute)이 사용될 경우 모델이 학습된 것보다 얼마나 더 잘 수행될 수 있는지를 알아내려고 합니다. 또한, 연구자들은 고정된 컴퓨팅 예산(compute budget)이 주어졌을 때, 테스트 시간에 더 많은 컴퓨팅을 사용하는 것이 모델을 추가로 사전 학습(pre-training)하는 데 컴퓨팅을 사용하는 것보다 결과를 개선할 수 있는지 여부도 살펴보았습니다. 하지만 이에 대해서는 나중에 더 자세히 설명하겠습니다.

**8.1.2 테스트 시간 컴퓨팅(test-time computation) 기술 최적화**
이 논문은 테스트 시간 컴퓨팅을 늘리고 개선하는 기술을 매우 상세하게 설명하고 있으며, LLM을 실제로 배포하는 데 진지하게 임하고 있다면(예: 앞서 언급된 라마 모델들), 이 논문을 완전히 읽어볼 것을 강력히 추천합니다. 요약하자면, 테스트 시간 컴퓨팅을 확장하는 두 가지 주요 방법은 다음과 같습니다.
1. 여러 솔루션을 생성하고 프로세스 기반 검증자 보상 모델(process-based verifier reward model, 별도로 학습되어야 함)을 사용하여 최상의 응답을 선택하는 것.
2. 모델의 응답 분포(response distribution)를 적응적으로 업데이트하는 것. 이는 본질적으로 추론 생성(inference generation) 중에 응답을 수정하는 것을 의미합니다(이 또한 별도의 모델이 필요합니다).
카테고리 1에 대한 간단한 예시를 들자면: 테스트 시간 컴퓨팅을 개선하는 한 가지 단순한 방법은 N개 중 최적(best-of-N) 샘플링을 사용하는 것입니다. 이는 LLM이 여러 답변을 병렬로 생성하게 한 다음, 검증자 보상 모델(verifier reward model)을 기반으로 최상의 답변을 선택하는 것을 의미합니다. N개 중 최적도 단지 한 가지 예시일 뿐입니다. 아래 그림에 나와 있듯이, 빔 탐색(beam-search), 선행 탐색(lookahead-search), N개 중 최적(best-of-N)과 같은 여러 탐색 알고리즘(search algorithms)이 이 범주에 속합니다.
다양한 탐색 기반 방법은 프로세스-보상 기반 모델(process-reward-based model)에 의존하여 최상의 답변을 선택합니다.
LLM 테스트 시간 컴퓨팅 논문에서 발췌한 주석이 달린 그림, https://arxiv.org/abs/2408.03314
카테고리 2에 속하는 또 다른 접근 방식은 아래 그림에 나와 있듯이 모델의 응답을 순차적으로 수정하는 것입니다.
순차적 수정 접근 방식.
LLM 테스트 시간 컴퓨팅 논문에서 발췌한 주석이 달린 그림, https://arxiv.org/abs/2408.03314
어떤 접근 방식이 더 효과적일까요? 안타깝게도 만능 해결책은 없습니다. 이는 기본 LLM과 특정 문제 또는 쿼리(query)에 따라 다릅니다. 예를 들어, 수정 기반 접근 방식은 더 어려운 질문에서 더 나은 성능을 보이지만, 쉬운 질문에서는 실제로 성능을 저해할 수 있습니다. 이 논문에서는 쿼리의 난이도(difficulty level)를 평가한 다음 적절한 전략을 선택하는 모델을 기반으로 "최적의" 전략을 개발했습니다.

**8.1.3 테스트 시간 컴퓨팅(test-time computation) 대 더 큰 모델 사전 학습(pretraining)**
흥미로운 질문은, 고정된 컴퓨팅 예산(compute budget)이 주어졌을 때, 더 큰 모델을 사용하는 것과 추론 시간 예산(inference-time budget)을 늘리는 것 중 어느 것이 더 큰 효과를 가져오는가 하는 것입니다. 여기서 쿼리(query)에 지불하는 비용은 동일하다고 가정합니다. 왜냐하면 추론(inference)에서 큰 모델을 실행하는 것이 작은 모델보다 더 비용이 많이 들기 때문입니다. 그들은 어려운 질문의 경우, 이전에 논의된 추론 스케일링 전략(inference scaling strategies)을 통해 추가 추론 컴퓨팅을 얻는 작은 모델보다 큰 모델이 더 나은 성능을 보인다는 것을 발견했습니다. 그러나 쉽고 중간 난이도의 질문의 경우, 추론 시간 컴퓨팅을 사용하여 동일한 컴퓨팅 예산으로 14배 더 큰 모델의 성능과 일치시킬 수 있었습니다!

**8.1.4 테스트 시간 컴퓨팅(test-time compute) 스케일링의 미래 관련성**
라마 3(Llama 3)와 같은 오픈 가중치 모델(open-weight models)을 사용할 때, 우리는 종종 모델이 응답을 있는 그대로 생성하도록 둡니다. 그러나 이 논문이 강조하듯이, 더 많은 추론 컴퓨팅(inference compute)을 할당함으로써 응답 품질을 크게 향상시킬 수 있습니다. (모델을 배포하고 있다면, 이 논문은 반드시 읽어야 할 논문입니다.) 물론, 크고 비싼 모델에 대한 추론 컴퓨팅 예산을 늘리면 운영 비용이 더욱 증가합니다. 하지만 쿼리(query)의 난이도에 따라 선택적으로 적용될 경우, 특정 응답의 품질과 정확도를 귀중하게 향상시킬 수 있으며, 이는 대부분의 사용자가 의심할 여지 없이 높이 평가할 것입니다. (OpenAI, Anthropic, Google은 이미 이러한 기술을 내부적으로 활용하고 있다고 가정하는 것이 안전합니다.) 또 다른 설득력 있는 사용 사례는 더 작고 온디바이스(on-device) LLM의 성능을 향상시키는 것입니다. 애플 인텔리전스(Apple Intelligence)와 마이크로소프트의 코파일럿 PC(Copilot PCs)에 대한 대규모 발표와 투자에서 보았듯이, 저는 이것이 앞으로 몇 달, 몇 년 동안 뜨거운 주제로 남을 것이라고 생각합니다.

**8.2 2025년 8월: 실시간 적응형 추론 시스템(Real-time Adaptive Inference Systems)**

이번 달 제가 선정한 논문은 '실시간 적응형 추론 시스템(Real-time Adaptive Inference Systems)'에 대한 깊이 있는 분석을 담고 있습니다. 이 논문은 배포(deployment) 시점에서 AI 모델의 응답을 동적으로 최적화하는 새로운 접근 방식을 제시하며, 실시간 환경에서의 성능 개선에 대한 흥미로운 통찰력을 제공합니다.

**8.2.1 동적 컴퓨팅 할당을 통한 출력 최적화**
이 논문의 주요 전제는 *동적으로 할당되는 컴퓨팅 자원*이 AI 시스템의 출력을 개선하는 데 어떻게 활용될 수 있는지를 탐구하는 것입니다. 대략적인 비유를 들자면, 자율 주행 차량이 예측 불가능한 상황에 직면했을 때, 추가적인 센서 데이터와 연산 능력을 즉시 동원하여 더 안전하고 정확한 결정을 내릴 수 있다고 가정해 봅시다. 마찬가지로, AI 모델도 실시간으로 더 많은 자원이 주어지면, 주어진 상황에 최적화된 출력을 생성할 수 있을 것입니다。 더 기술적인 용어로 말하면, 연구자들은 추론(inference) 중에 추가적인 컴퓨팅(compute)이 사용될 경우, 모델이 사전 학습된 능력 이상으로 얼마나 더 잘 수행될 수 있는지를 알아내려고 합니다. 또한, 그들은 고정된 총 컴퓨팅 예산(compute budget) 내에서, 실시간 적응형 컴퓨팅을 사용하는 것이 모델의 사전 학습(pre-training) 단계를 확장하는 것보다 더 효율적인 결과를 가져올 수 있는지 여부도 분석했습니다. 이에 대해서는 나중에 더 자세히 설명하겠습니다.

**8.2.2 적응형 추론 기술 최적화**
이 논문은 실시간 적응형 컴퓨팅을 늘리고 개선하는 기술을 매우 상세하게 설명하고 있으며, AI 모델을 실제로 배포하는 데 진지하게 임하고 있다면(예: 앞서 언급된 아리온 모델들), 이 논문을 완전히 읽어볼 것을 강력히 추천합니다. 요약하자면, 실시간 추론 컴퓨팅을 확장하는 두 가지 주요 방법은 다음과 같습니다.
1.  *상황 인지형 다중 경로 추론(Context-Aware Multi-Path Inference)*: 여러 잠재적 솔루션 경로를 동시에 탐색하고, 실시간 환경 피드백과 *강화 학습 기반의 결정 모듈(RL-based decision module)*을 사용하여 가장 적합한 응답을 선택하는 것입니다.
2.  *점진적 정제 및 자가 수정(Progressive Refinement and Self-Correction)*: 모델의 응답 분포(response distribution)를 실시간으로 모니터링하고, 예측 불가능한 상황에 따라 응답을 동적으로 수정하는 것입니다. 이는 *경량 메타-학습 모듈(lightweight meta-learning module)*을 통해 구현됩니다.

카테고리 1에 대한 간단한 예시를 들자면: 상황 인지형 추론을 개선하는 한 가지 단순한 방법은 *예측적 경로 탐색(predictive path exploration)*을 사용하는 것입니다. 이는 AI 시스템이 여러 가능한 행동 시퀀스를 병렬로 시뮬레이션하게 한 다음, *환경 시뮬레이터(environment simulator)*와 *안전 보장 모듈(safety assurance module)*을 기반으로 최적의 행동을 선택하는 것을 의미합니다. 예측적 경로 탐색은 단지 한 가지 예시일 뿐입니다. 아래 그림에 나와 있듯이, *강화 학습 기반 탐색(RL-based exploration)*, *몬테카를로 트리 탐색(Monte Carlo Tree Search, MCTS)*, *동적 계획(dynamic planning)*과 같은 여러 탐색 알고리즘(search algorithms)이 이 범주에 속합니다.

*(설명: 위 텍스트에 따라, 다양한 탐색 기반 방법은 환경 보상 모델(environment reward model)에 의존하여 최적의 행동을 선택합니다. [가상의 그림 참조])*

카테고리 2에 속하는 또 다른 접근 방식은 아래 그림에 나와 있듯이 모델의 응답을 순차적으로 수정하는 것입니다.

*(설명: 위 텍스트에 따라, 순차적 수정 접근 방식 [가상의 그림 참조])*

어떤 접근 방식이 더 효과적일까요? 안타깝게도 만능 해결책은 없습니다. 이는 기본 AI 시스템과 특정 실시간 시나리오에 따라 다릅니다. 예를 들어, 수정 기반 접근 방식은 더 예측 불가능한 환경에서 더 나은 성능을 보이지만, 정적인 환경에서는 실제로 불필요한 오버헤드를 발생시킬 수 있습니다. 이 논문에서는 *환경의 동적 수준(dynamic level)*을 평가한 다음 적절한 전략을 선택하는 *적응형 제어 모델(adaptive control model)*을 기반으로 "최적의" 전략을 개발했습니다.

**8.2.3 실시간 적응형 컴퓨팅 대 모델 사전 학습(Pretraining) 확장**
흥미로운 질문은, 고정된 컴퓨팅 예산(compute budget)이 주어졌을 때, 더 큰 모델을 사용하는 것과 실시간 적응형 컴퓨팅 예산(adaptive compute budget)을 늘리는 것 중 어느 것이 더 큰 효과를 가져오는가 하는 것입니다. 여기서 시스템의 총 운영 비용은 동일하다고 가정합니다. 왜냐하면 추론(inference)에서 거대한 모델을 실행하는 것이 경량 모델보다 더 비용이 많이 들기 때문입니다. 연구자들은 복잡하고 예측 불가능한 시나리오의 경우, 이전에 논의된 적응형 스케일링 전략(adaptive scaling strategies)을 통해 추가 추론 컴퓨팅을 얻는 경량 모델보다 거대 모델이 더 나은 성능을 보인다는 것을 발견했습니다. 그러나 단순하거나 예측 가능한 시나리오의 경우, 실시간 적응형 컴퓨팅을 사용하여 동일한 컴퓨팅 예산으로 10배 더 큰 모델의 성능과 일치시킬 수 있었습니다!

**8.2.4 실시간 적응형 컴퓨팅 스케일링의 미래 관련성**
아리온(Arion)과 같은 분산형 AI 모델을 사용할 때, 우리는 종종 모델이 응답을 있는 그대로 생성하도록 둡니다. 그러나 이 논문이 강조하듯이, 더 많은 추론 컴퓨팅(inference compute)을 동적으로 할당함으로써 응답 품질을 크게 향상시킬 수 있습니다. (AI 시스템을 배포하고 있다면, 이 논문은 반드시 읽어야 할 논문입니다.) 물론, 복잡하고 자율적인 시스템에 대한 추론 컴퓨팅 예산을 늘리면 운영 비용이 더욱 증가합니다. 하지만 *환경의 동적 수준*에 따라 선택적으로 적용될 경우, 특정 응답의 품질과 안전성을 귀중하게 향상시킬 수 있으며, 이는 대부분의 사용자가 의심할 여지 없이 높이 평가할 것입니다. (주요 AI 개발사들은 이미 이러한 기술을 내부적으로 활용하고 있다고 가정하는 것이 안전합니다.) 또 다른 설득력 있는 사용 사례는 더 작고 온디바이스(on-device) AI 모델의 성능을 향상시키는 것입니다. *산업용 로봇(industrial robots)*과 *스마트 센서 네트워크(smart sensor networks)*에 대한 대규모 발표와 투자에서 보았듯이, 저는 이것이 앞으로 몇 달, 몇 년 동안 뜨거운 주제로 남을 것이라고 생각합니다.

**9. 9월**

**9.1 2024년 9월: 멀티모달 LLM 패러다임(multimodal LLM paradigms) 비교**
멀티모달 LLM(Multimodal LLMs)은 2024년에 큰 도약을 이룰 것이라고 생각했던 주요 사항 중 하나였습니다. 그리고 실제로 올해 더 많은 오픈 가중치 LLM(open-weight LLMs)이 출시되었습니다!
다양한 입력 양식(input modalities, 오디오, 텍스트, 이미지, 비디오)을 받아들이고 텍스트를 출력 양식(output modality)으로 반환할 수 있는 멀티모달 LLM의 예시.
특히 저에게 인상 깊었던 논문은 Dai와 동료들이 발표한 NVIDIA의 "NVLM: 개방형 프론티어급 멀티모달 LLM(Open Frontier-Class Multimodal LLMs, 2024년 9월)"이었습니다. 이 논문은 두 가지 주요 멀티모달 패러다임을 훌륭하게 비교하고 있기 때문입니다.

**9.1.1 멀티모달 LLM 패러다임(Multimodal LLM paradigms)**
멀티모달 LLM을 구축하는 두 가지 주요 접근 방식이 있습니다:
방법 A: 통합 임베딩 디코더 아키텍처(Unified Embedding Decoder Architecture) 접근 방식;
방법 B: 교차 모달리티 어텐션 아키텍처(Cross-modality Attention Architecture) 접근 방식.
멀티모달 LLM 아키텍처를 개발하는 두 가지 주요 접근 방식.
위 그림에서 설명된 바와 같이, 통합 임베딩-디코더 아키텍처(Unified Embedding-Decoder Architecture, 방법 A)는 GPT-2 또는 라마 3.2(Llama 3.2)와 같은 수정되지 않은 LLM 아키텍처와 유사한 단일 디코더 모델(single decoder model)에 의존합니다. 이 방법은 이미지를 텍스트 토큰(text tokens)과 동일한 임베딩 크기(embedding size)를 공유하는 토큰으로 변환하여, LLM이 연결된 텍스트 및 이미지 입력 토큰(image input tokens)을 처리할 수 있도록 합니다. 대조적으로, 교차 모달리티 어텐션 아키텍처(Cross-Modality Attention Architecture, 방법 B)는 교차 어텐션 메커니즘(cross-attention mechanism)을 통합하여 어텐션 레이어(attention layer) 내에서 이미지 및 텍스트 임베딩(image and text embeddings)을 직접 통합합니다. 추가 세부 사항에 관심이 있다면, 올해 초 이 두 가지 방법을 단계별로 설명하는 멀티모달 LLM에 대한 전체 기사를 작성했습니다: 멀티모달 LLM 이해하기 -- 주요 기술 및 최신 모델 소개(Understanding Multimodal LLMs -- An introduction to the main techniques and latest models).

**9.1.2 엔비디아의 하이브리드(hybrid) 접근 방식**
올해의 모든 멀티모달 개발을 고려할 때, 엔비디아(NVIDIA)의 논문 "NVLM: 개방형 프론티어급 멀티모달 LLM(Open Frontier-Class Multimodal LLMs)"은 이러한 멀티모달 접근 방식에 대한 포괄적인 동등 비교(apples-to-apples comparison)로 저에게 두드러졌습니다. 단일 방법에 초점을 맞추기보다는 다음을 직접 비교했습니다:
방법 A: 통합 임베딩 디코더 아키텍처("디코더 전용 아키텍처(decoder-only architecture)," NVLM-D),
방법 B: 교차 모달리티 어텐션 아키텍처("교차 어텐션 기반 아키텍처(cross-attention-based architecture)," NVLM-X),
하이브리드 접근 방식(hybrid approach, NVLM-H).
세 가지 멀티모달 접근 방식 개요. (NVLM: 개방형 프론티어급 멀티모달 LLM 논문에서 발췌한 주석이 달린 그림, https://arxiv.org/abs/2409.11402)
위 그림에 요약된 바와 같이, NVLM-D는 방법 A와 일치하며, NVLM-X는 이전에 논의된 방법 B에 해당합니다. 하이브리드 모델(NVLM-H)은 두 접근 방식의 장점을 결합합니다: 먼저 이미지 썸네일(image thumbnail)을 입력으로 받아들이고, 이어서 교차 어텐션(cross-attention)을 통해 처리되는 동적인 수의 패치(patches)를 통해 더 미세한 고해상도 세부 정보(finer high-resolution details)를 캡처합니다. 요약하자면, 주요 발견 사항은 다음과 같습니다:
NVLM-X: 고해상도 이미지에 대해 우수한 계산 효율성(computational efficiency)을 제공합니다.
NVLM-D: OCR 관련 작업에 대해 더 높은 정확도를 제공합니다.
NVLM-H: 최적의 성능을 위해 두 접근 방식의 장점을 결합합니다.

**9.1.3 2025년의 멀티모달 LLM(Multimodal LLMs)**
멀티모달 LLM은 흥미로운 주제입니다. 저는 이들이 일반적인 텍스트 기반 LLM에서 한 단계 더 나아간 논리적인 발전이라고 생각합니다. OpenAI, Google, Anthropic과 같은 대부분의 LLM 서비스 제공업체는 이미지와 같은 멀티모달 입력(multimodal inputs)을 지원합니다. 개인적으로 저는 멀티모달 기능이 필요한 경우가 아마 1% 정도일 것입니다(보통 "테이블을 마크다운 형식으로 추출해줘"와 같은 경우입니다). 저는 오픈 가중치 LLM의 기본값이 순수하게 텍스트 기반일 것으로 예상합니다. 복잡성을 덜 추가하기 때문입니다. 동시에 도구(tooling)와 API가 발전함에 따라 오픈 가중치 LLM의 더 많은 옵션과 광범위한 사용을 보게 될 것이라고 생각합니다.

**9.2 2025년 9월: 실세계 지능을 위한 다중 센서 통합(Multi-Sensor Integration for Real-World Intelligence)**

멀티모달 LLM(Multimodal LLMs)은 2024년에 큰 도약을 이룰 것이라고 생각했던 주요 사항 중 하나였으며, 이제는 더욱 확장되고 있습니다. 2025년에는 단순히 텍스트와 이미지, 오디오를 넘어선 *실세계 감각 데이터*의 통합이 핵심 트렌드로 부상했습니다!

*(설명: 다양한 입력 양식(input modalities, 오디오, 텍스트, 이미지, 비디오, 햅틱, 후각)을 받아들이고 텍스트 또는 행동 출력을 반환할 수 있는 다중 센서 AI의 예시. [가상의 그림 참조])*

특히 저에게 인상 깊었던 논문은 Kim과 동료들이 발표한 "뉴럴 센서 융합 아키텍처: 실시간 로봇 제어를 위한 다중 모달리티 통합(Neural Sensor Fusion Architectures: Integrating Multiple Modalities for Real-time Robotic Control, 2025년 9월)"이었습니다. 이 논문은 두 가지 주요 *센서 융합 패러다임*을 훌륭하게 비교하고 있기 때문입니다.

**9.2.1 다중 센서 융합 패러다임(Multi-Sensor Fusion Paradigms)**
실세계 AI 시스템을 구축하는 두 가지 주요 접근 방식이 있습니다:
방법 A: *통합 인지 디코더 아키텍처(Unified Cognitive Decoder Architecture)* 접근 방식;
방법 B: *행동-지각 순환 아키텍처(Action-Perception Loop Architecture)* 접근 방식.

*(설명: 다중 센서 AI 아키텍처를 개발하는 두 가지 주요 접근 방식. [가상의 그림 참조])*

위 그림에서 설명된 바와 같이, 통합 인지-디코더 아키텍처(Unified Cognitive-Decoder Architecture, 방법 A)는 GPT-2 또는 아리온 1.2(Arion 1.2)와 같은 수정되지 않은 LLM 아키텍처와 유사한 단일 디코더 모델(single decoder model)에 의존합니다. 이 방법은 다양한 센서 데이터를 고차원 임베딩(high-dimensional embeddings)으로 변환하여, AI가 연결된 텍스트, 이미지, 햅틱(haptic), 후각(olfactory) 입력 토큰(input tokens)을 처리할 수 있도록 합니다. 대조적으로, 행동-지각 순환 아키텍처(Action-Perception Loop Architecture, 방법 B)는 *강화 학습 기반의 교차 모달리티 제어 메커니즘(RL-based cross-modality control mechanism)*을 통합하여, 지각 레이어(perception layer) 내에서 센서 임베딩(sensor embeddings)과 행동 계획(action plans)을 직접 통합합니다. 추가 세부 사항에 관심이 있다면, 올해 초 이 두 가지 방법을 단계별로 설명하는 다중 센서 AI에 대한 전체 기사를 작성했습니다: 다중 센서 AI 이해하기 -- 주요 기술 및 최신 모델 소개(Understanding Multi-Sensor AI -- An introduction to the main techniques and latest models).

**9.2.2 하이브리드(Hybrid) 접근 방식과 신경망 컴퓨팅**
올해의 모든 다중 센서 개발을 고려할 때, Kim과 동료들의 논문 "뉴럴 센서 융합 아키텍처: 실시간 로봇 제어를 위한 다중 모달리티 통합"은 이러한 센서 융합 접근 방식에 대한 포괄적인 동등 비교(apples-to-apples comparison)로 저에게 두드러졌습니다. 단일 방법에 초점을 맞추기보다는 다음을 직접 비교했습니다:
방법 A: 통합 인지 디코더 아키텍처("인지 전용 아키텍처(cognitive-only architecture)," NVSF-C),
방법 B: 행동-지각 순환 아키텍처("행동 기반 아키텍처(action-based architecture)," NVSF-A),
하이브리드 접근 방식(hybrid approach, NVSF-H).

*(설명: 세 가지 다중 센서 접근 방식 개요. [가상의 그림 참조])*

위 그림에 요약된 바와 같이, NVSF-C는 방법 A와 일치하며, NVSF-A는 이전에 논의된 방법 B에 해당합니다. 하이브리드 모델(NVSF-H)은 두 접근 방식의 장점을 결합합니다: 먼저 *이벤트 기반 센서 데이터(event-based sensor data)*를 입력으로 받아들이고, 이어서 *신경망 컴퓨팅(neuromorphic computing)*을 통해 처리되는 동적인 수의 *시공간 패치(spatio-temporal patches)*를 통해 더 미세한 고해상도 세부 정보(finer high-resolution details)를 캡처합니다. 요약하자면, 주요 발견 사항은 다음과 같습니다:
*   NVSF-A: 실시간 로봇 제어 및 빠른 반응에 대해 우수한 *지연 시간(latency)*과 계산 효율성(computational efficiency)을 제공합니다.
*   NVSF-C: 복잡한 추론 및 상황 인지 작업에 대해 더 높은 정확도를 제공합니다.
*   NVSF-H: 최적의 성능을 위해 두 접근 방식의 장점을 결합하며, 특히 *에너지 효율성* 면에서 뛰어납니다.

**9.2.3 2026년의 다중 센서 AI**
다중 센서 AI는 흥미로운 주제입니다. 저는 이들이 일반적인 텍스트 기반 AI에서 한 단계 더 나아간 논리적인 발전이라고 생각합니다. OpenAI, Google, Anthropic과 같은 대부분의 LLM 서비스 제공업체는 이제 *햅틱 피드백(haptic feedback)*과 같은 새로운 모달리티 입력(multimodal inputs)을 지원합니다. 개인적으로 저는 이러한 다중 센서 기능이 필요한 경우가 아마 5% 정도일 것입니다(보통 "로봇 팔로 섬세한 물체를 집어 들어줘"와 같은 경우입니다). 저는 오픈 가중치 AI의 기본값이 순수하게 *텍스트-시각 기반*일 것으로 예상합니다. 복잡성을 덜 추가하기 때문입니다. 동시에 *로봇 운영 체제(Robot Operating System, ROS)*와 *엣지 컴퓨팅(edge computing)* API가 발전함에 따라 오픈 가중치 다중 센서 AI의 더 많은 옵션과 광범위한 사용을 보게 될 것이라고 생각합니다.

**10. 10월**

**10.1 2024년 10월: OpenAI o1의 추론 능력(reasoning capabilities) 복제**
10월에 제가 선정한 논문은 Quin과 동료들이 발표한 "O1 복제 여정: 전략적 진행 보고서 -- 파트 1(O1 Replication Journey: A Strategic Progress Report -- Part 1, 2024년 10월)"입니다. OpenAI ChatGPT의 o1(그리고 현재 o3)은 LLM의 추론 작업(reasoning tasks) 성능을 개선하는 데 있어 패러다임 전환(paradigm shift)을 나타내는 것으로 보이며 상당한 인기를 얻었습니다. OpenAI o1의 정확한 세부 사항은 공개되지 않았으며, 여러 논문에서 이를 설명하거나 복제하려고 시도했습니다. 그렇다면 왜 이 논문을 선택했을까요? 이 논문의 특이한 구조와 학술 연구(academic research)의 현황에 대한 광범위한 철학적 주장이 저에게 공감을 불러일으켰습니다. 다시 말해, 이 논문에는 눈에 띄는 독특한 점이 있었고, 그것이 흥미로운 선택으로 이어졌습니다.

**10.1.1 지름길 학습(Shortcut learning) 대 여정 학습(journey learning)**
이 논문의 핵심 요점 중 하나는 아래 그림에 설명된 바와 같이, O1이 지름길 학습(shortcut learning)과 대조적으로 여정 학습(journey learning)이라는 프로세스를 사용한다는 연구자들의 가설입니다. 전통적으로 LLM은 올바른 해결 경로(shortcut learning)로 학습됩니다. 반면 여정 학습에서는 지도 미세 조정(supervised finetuning)이 전체 시행착오 수정 프로세스(trial-and-error correction process)를 포함합니다.
O1 복제 보고서에서 발췌한 주석이 달린 그림, https://arxiv.org/abs/2410.18982
여정 학습 접근 방식은 이 기사의 "8. 8월: 추론 시간 컴퓨팅(Inference-Time Compute) 확장을 통한 LLM 개선" 섹션에서 이전에 논의된 수정 기능이 있는 트리 기반(tree-based) 또는 빔 탐색(beam-search) 방법과 다소 유사하다는 점에 주목할 가치가 있습니다. 그러나 미묘한 차이점은 연구자들이 단순히 추론(inference) 중에 이 기술을 적용하는 것이 아니라, 모델 미세 조정을 위한 여정 학습 학습 예제(journey learning training examples)를 생성한다는 것입니다. (추론 프로세스를 증강하기 위해 사용한 기술에 대한 정보는 찾을 수 없었다는 점도 주목할 가치가 있습니다.)

**10.1.2 긴 사고(long thoughts) 구성**
연구자들은 시행착오를 강조하며 확장된 사고 과정(extended thought process)을 도출하기 위해 추론 트리(reasoning tree)를 구성했습니다. 이 접근 방식은 유효한 중간 단계를 통해 정답으로 가는 직접적인 경로를 찾는 것을 우선시하는 전통적인 방법과 다릅니다. 그들의 프레임워크에서 추론 트리의 각 노드(node)는 보상 모델(reward model)이 제공하는 평가로 주석이 달렸으며, 해당 단계가 올바른지 또는 잘못되었는지와 이 평가를 정당화하는 추론을 나타냈습니다. 다음으로, 그들은 지도 미세 조정(supervised finetuning)과 DPO를 통해 deepseek-math-7b-base 모델을 학습시켰습니다. 여기서 그들은 두 가지 모델을 학습시켰습니다.
1. 첫째, 그들은 올바른 중간 단계만 제공되는 전통적인 지름길 학습(shortcut training) 패러다임을 사용했습니다.
2. 둘째, 그들은 올바른 답과 잘못된 답, 되돌아가기(backtracking) 등을 포함하는 사고 과정(thought process) 세 가지를 포함하는 제안된 여정 학습(journey learning) 접근 방식으로 모델을 학습시켰습니다. (참고: 각 경우에 327개의 예제만 사용했습니다!)
아래 그림에 나와 있듯이, 여정 학습 프로세스는 MATH500 벤치마크(benchmark) 데이터셋에서 지름길 학습을 상당히 큰 차이로 능가했습니다.
지름길 학습과 여정 학습으로 학습된 LLM.
O1 복제 보고서에서 발췌한 주석이 달린 그림, https://arxiv.org/abs/2410.18982

**10.1.3 증류(Distillation) -- 빠른 해결책?**
한 달 후, 팀은 또 다른 보고서인 Huang과 동료들이 발표한 "O1 복제 여정 -- 파트 2: 단순 증류를 통한 O1-preview 능가, 큰 진전인가 쓰디쓴 교훈인가?(O1 Replication Journey -- Part 2: Surpassing O1-preview through Simple Distillation, Big Progress or Bitter Lesson?, 2024년 11월)"를 발표했습니다. 여기서 그들은 증류(distillation) 접근 방식을 사용했습니다. 즉, o1에서 사고 과정(thought processes)을 추출하기 위해 신중한 프롬프트(prompting)를 사용하여 동일한 성능에 도달하도록 모델을 학습시켰습니다. 이 기사가 길기 때문에 자세한 내용은 다루지 않겠지만, 장기 사고 데이터(long-thought data) 수집의 비용 상충 관계(cost trade-offs)를 요약한 해당 논문의 흥미로운 그림을 공유하고 싶었습니다. 그들은 이 증류 접근 방식으로 o1-preview 및 o1-mini와 동등한 매우 좋은 성능을 얻었습니다. 그러나 이러한 실험과 함께 연구자들은 이 접근 방식에 비추어 연구 현황에 대한 흥미롭고 중요한 생각도 공유했으며, 이는 다음 섹션에서 요약하겠습니다.

**10.1.4 AI 연구의 현황**
파트 2 보고서의 큰 초점 중 하나는 "단순 증류의 쓰디쓴 교훈(Bitter Lesson of Simple Distillation)"이었습니다. 물론 증류(distillation)는 실제에서 잘 작동하지만, 진전을 이끄는 원동력은 아닙니다. 최상의 경우, 증류를 사용하면 기존의 상위 모델(upstream model) 성능과 일치시킬 수 있을 뿐입니다(새로운 성능 기록을 세우는 것은 아닙니다). 아래는 현재 상황에 대한 경고로 작용할 수 있는 논문에서 발췌한 세 가지 인용문입니다:
"‘어떻게 작동하는가’에서 ‘무엇이 작동하는가’로의 이러한 변화는 연구 사고방식의 근본적인 변화를 나타내며, 이는 해당 분야의 미래 혁신 역량에 광범위한 영향을 미칠 수 있습니다."
"이러한 제1원리 사고(first-principles thinking)의 침식은 과학적 혁신의 바로 그 기반을 약화시키기 때문에 특히 우려됩니다."
"빠른 결과물을 내야 한다는 압박은 더 깊은 기술적 조사의 가치를 가릴 수 있으며, 학생들은 더 도전적이고 근본적인 연구 방향을 추구하는 것을 단념하게 될 수 있습니다."
제 개인적인 견해는 여전히 학술 연구실(academic labs, 오늘날에는 종종 산업계와의 파트너십을 통해서도)에서 수많은 훌륭하고 중요한 아이디어들이 나오고 있으며, 이들은 실제로 실용적이고 영향력이 클 수 있다고 생각합니다. (제가 좋아하는 몇 가지는 LoRA와 DPO입니다.) 문제는 많은 유망한 아이디어들이 대규모로 테스트되지 못한다는 것입니다. 대학은 보통 이를 위한 막대한 자원을 가지고 있지 않기 때문입니다. 완벽한 해결책이 무엇인지는 확실하지 않으며, 기업들이 자신들의 영업 비밀(trade secrets)을 그냥 공개할 수 없다는 것도 알고 있습니다. 하지만 기업들이 학술 논문의 아이디어를 사용하게 될 때마다 이를 공개적으로 인정해 준다면 정말 도움이 될 것입니다. 그러한 인정은 자신의 작업을 자유롭게 제공하는 연구자들에게 동기를 부여하고 보상하는 데 큰 도움이 됩니다. 또한, 실제로 무엇이 작동하는지 알아냄으로써 분야를 발전시키는 데 기여합니다.

**10.1.5 o1(및 o3)의 관점에서 본 LLM의 미래**
"O1 복제 여정(O1 Replication Journey)" 논문이 o1 뒤에 있는 정확한 메커니즘을 복제했을까요? 아마 아닐 것입니다. 하지만 여전히 더 나은 결과를 달성하는 데 도움이 될 아이디어로 가득 찬 가치 있는 읽을거리입니다. 저는 o1 및 o3와 같은 "장기 사고(long-thought)" 모델이 LLM 연구에서 계속해서 핵심적인 역할을 할 것이라고 믿습니다. 이들은 실행 비용이 더 많이 들지만, 추론 작업(reasoning tasks) 성능의 기본적으로 황금 표준(gold standard) 또는 상한선(upper limit for performance)입니다. 그러나 비용이 더 높기 때문에 o1 유형 모델이 모든 상황에서 항상 최선의 선택은 아닙니다. 문법 수정(grammar fixes)이나 번역(translations)과 같은 더 간단한 작업의 경우, 추론 중심 모델(reasoning-heavy model)이 필요하지 않을 것입니다. 결국 비용과 유용성(utility)의 균형을 맞추는 문제입니다. 우리는 예산, 지연 시간(latency) 및 기타 요인에 따라 작업에 적합한 LLM을 선택합니다.

**10.2 2025년 10월: 설명 가능한 AI와 신뢰성 확보 방안**

10월에 제가 선정한 논문은 'AI 시스템의 투명성 및 신뢰성 확보 방안'에 대한 심층 연구를 다룹니다. 최근 AI 모델, 특히 대규모 언어 모델(LLM)의 급속한 발전은 놀라운 성능을 보여주었지만, 그 내부 작동 방식은 여전히 블랙박스로 남아있습니다. 이는 AI의 신뢰성과 책임성 문제를 야기하며, 다양한 분야에서 AI 도입의 걸림돌이 되고 있습니다. 이 논문은 AI 시스템의 의사결정 과정을 이해하고 설명할 수 있는 새로운 접근 방식을 제시하여 주목받고 있습니다. 그렇다면 왜 이 논문을 선택했을까요? 이 논문의 특이한 구조와 *학술 연구(academic research)의 현황에 대한 광범위한 철학적 주장이 저에게 공감을 불러일으켰습니다.* 다시 말해, 이 논문에는 눈에 띄는 독특한 점이 있었고, 그것이 흥미로운 선택으로 이어졌습니다.

**10.2.1 인과적 추론(Causal Reasoning) 대 상관관계 학습(Correlational Learning)**
이 논문의 핵심 요점 중 하나는 아래 그림에 설명된 바와 같이, AI가 단순히 상관관계를 학습하는 *지름길 학습(shortcut learning)*과 대조적으로 *인과적 추론(causal reasoning)*이라는 프로세스를 사용한다는 연구자들의 가설입니다. 전통적으로 LLM은 데이터 패턴에서 올바른 상관관계 경로(shortcut learning)로 학습됩니다. 반면 인과적 추론에서는 *지도 미세 조정(supervised finetuning)*이 전체 *인과 관계 탐색 및 검증 프로세스(causal relationship exploration and validation process)*를 포함합니다.

*(설명: 인과적 추론과 상관관계 학습으로 학습된 AI 모델의 비교. [가상의 그림 참조])*

인과적 추론 접근 방식은 이 기사의 "8. 8월: 실시간 적응형 추론 시스템(Real-time Adaptive Inference Systems)" 섹션에서 이전에 논의된 *수정 기능이 있는 트리 기반(tree-based) 또는 빔 탐색(beam-search) 방법*과 다소 유사하다는 점에 주목할 가치가 있습니다. 그러나 미묘한 차이점은 연구자들이 단순히 추론(inference) 중에 이 기술을 적용하는 것이 아니라, 모델 미세 조정을 위한 *인과적 학습 예제(causal learning examples)*를 생성한다는 것입니다. (추론 프로세스를 증강하기 위해 사용한 기술에 대한 정보는 찾을 수 없었다는 점도 주목할 가치가 있습니다.)

**10.2.2 설명 가능한 사고(Explainable Thoughts) 구성**
연구자들은 *불확실성(uncertainty)*을 강조하며 확장된 사고 과정(extended thought process)을 도출하기 위해 *인과 그래프(causal graph)*를 구성했습니다. 이 접근 방식은 유효한 중간 단계를 통해 정답으로 가는 직접적인 경로를 찾는 것을 우선시하는 전통적인 방법과 다릅니다. 그들의 프레임워크에서 인과 그래프의 각 노드(node)는 *설명 가능성 모델(explainability model)*이 제공하는 평가로 주석이 달렸으며, 해당 단계가 올바른지 또는 잘못되었는지와 이 평가를 정당화하는 추론을 나타냈습니다. 다음으로, 그들은 *지도 미세 조정(supervised finetuning)*과 *인과적 DPO(Causal DPO)*를 통해 *explainable-math-7b-base* 모델을 학습시켰습니다. 여기서 그들은 두 가지 모델을 학습시켰습니다.
1.  첫째, 그들은 올바른 중간 단계만 제공되는 전통적인 *상관관계 학습(correlational training)* 패러다임을 사용했습니다.
2.  둘째, 그들은 올바른 답과 잘못된 답, 되돌아가기(backtracking) 등을 포함하는 *인과적 사고 과정(causal thought process)* 세 가지를 포함하는 제안된 *인과적 학습(causal learning)* 접근 방식으로 모델을 학습시켰습니다. (참고: 각 경우에 327개의 예제만 사용했습니다!)

아래 그림에 나와 있듯이, 인과적 학습 프로세스는 MATH500 벤치마크(benchmark) 데이터셋에서 상관관계 학습을 상당히 큰 차이로 능가했습니다.

*(설명: 인과적 학습과 상관관계 학습으로 학습된 AI 모델의 비교. [가상의 그림 참조])*

**10.2.3 검증 가능성(Verifiability) -- 신뢰의 열쇠?**
한 달 후, 팀은 또 다른 보고서인 Lee와 동료들이 발표한 "인과적 추론 여정 -- 파트 2: 단순 검증을 통한 AI 신뢰성 향상, 큰 진전인가 위험한 지름길인가?(Causal Reasoning Journey -- Part 2: Enhancing AI Trustworthiness through Simple Verification, Big Progress or Risky Shortcut?, 2025년 11월)"를 발표했습니다. 여기서 그들은 *검증(verification)* 접근 방식을 사용했습니다. 즉, *인과 그래프(causal graph)*에서 사고 과정(thought processes)을 추출하기 위해 신중한 프롬프트(prompting)를 사용하여 동일한 신뢰성 수준에 도달하도록 모델을 학습시켰습니다. 이 기사가 길기 때문에 자세한 내용은 다루지 않겠지만, *장기 인과 데이터(long-causal data)* 수집의 비용 상충 관계(cost trade-offs)를 요약한 해당 논문의 흥미로운 그림을 공유하고 싶었습니다. 그들은 이 검증 접근 방식으로 *human-align-preview* 및 *human-align-mini*와 동등한 매우 좋은 성능을 얻었습니다. 그러나 이러한 실험과 함께 연구자들은 이 접근 방식에 비추어 연구 현황에 대한 흥미롭고 중요한 생각도 공유했으며, 이는 다음 섹션에서 요약하겠습니다.

**10.2.4 AI 연구의 현황: "블랙박스 교훈(Bitter Lesson of Black-box)"**
파트 2 보고서의 큰 초점 중 하나는 "블랙박스 교훈(Bitter Lesson of Black-box)"이었습니다. 물론 *검증(verification)*은 실제에서 잘 작동하지만, 진전을 이끄는 원동력은 아닙니다. 최상의 경우, 검증을 사용하면 기존의 상위 모델(upstream model) 신뢰성과 일치시킬 수 있을 뿐입니다(새로운 신뢰성 기록을 세우는 것은 아닙니다). 아래는 현재 상황에 대한 경고로 작용할 수 있는 논문에서 발췌한 세 가지 인용문입니다:
"‘어떻게 작동하는가’에서 ‘무엇이 작동하는가’로의 이러한 변화는 연구 사고방식의 근본적인 변화를 나타내며, 이는 해당 분야의 미래 혁신 역량에 광범위한 영향을 미칠 수 있습니다."
"이러한 제1원리 사고(first-principles thinking)의 침식은 과학적 혁신의 바로 그 기반을 약화시키기 때문에 특히 우려됩니다."
"빠른 결과물을 내야 한다는 압박은 더 깊은 기술적 조사의 가치를 가릴 수 있으며, 학생들은 더 도전적이고 근본적인 연구 방향을 추구하는 것을 단념하게 될 수 있습니다."
제 개인적인 견해는 여전히 학술 연구실(academic labs, 오늘날에는 종종 산업계와의 파트너십을 통해서도)에서 수많은 훌륭하고 중요한 아이디어들이 나오고 있으며, 이들은 실제로 실용적이고 영향력이 클 수 있다고 생각합니다. (제가 좋아하는 몇 가지는 *인과적 추론(causal inference)*과 *설명 가능성 매트릭스(explainability matrices)*입니다.) 문제는 많은 유망한 아이디어들이 대규모로 테스트되지 못한다는 것입니다. 대학은 보통 이를 위한 막대한 자원을 가지고 있지 않기 때문입니다. 완벽한 해결책이 무엇인지는 확실하지 않으며, 기업들이 자신들의 영업 비밀(trade secrets)을 그냥 공개할 수 없다는 것도 알고 있습니다. 하지만 기업들이 학술 논문의 아이디어를 사용하게 될 때마다 이를 공개적으로 인정해 준다면 정말 도움이 될 것입니다. 그러한 인정은 자신의 작업을 자유롭게 제공하는 연구자들에게 동기를 부여하고 보상하는 데 큰 도움이 됩니다. 또한, 실제로 무엇이 작동하는지 알아냄으로써 분야를 발전시키는 데 기여합니다.

**10.2.5 신뢰할 수 있는 AI의 관점에서 본 AI의 미래**
"인과적 추론 여정(Causal Reasoning Journey)" 논문이 *AI의 신뢰성* 뒤에 있는 정확한 메커니즘을 복제했을까요? 아마 아닐 것입니다. 하지만 여전히 더 나은 결과를 달성하는 데 도움이 될 아이디어로 가득 찬 가치 있는 읽을거리입니다. 저는 *인과적 추론* 및 *설명 가능한 AI*와 같은 "신뢰할 수 있는 AI(trustworthy AI)" 모델이 AI 연구에서 계속해서 핵심적인 역할을 할 것이라고 믿습니다. 이들은 실행 비용이 더 많이 들지만, *고위험 의사결정(high-stakes decision-making)* 성능의 기본적으로 황금 표준(gold standard) 또는 상한선(upper limit for performance)입니다. 그러나 비용이 더 높기 때문에 *신뢰할 수 있는 AI 유형 모델*이 모든 상황에서 항상 최선의 선택은 아닙니다. 문법 수정(grammar fixes)이나 번역(translations)과 같은 더 간단한 작업의 경우, *설명 가능성 중심 모델(explainability-heavy model)*이 필요하지 않을 것입니다. 결국 비용과 유용성(utility)의 균형을 맞추는 문제입니다. 우리는 예산, 지연 시간(latency) 및 기타 요인에 따라 작업에 적합한 AI 모델을 선택합니다.

**11. 11월**

**11.1 2024년 11월: 정밀도(precision)를 위한 LLM 스케일링 법칙(LLM scaling laws)**
원래 저는 Allen AI의 "Tulu 3: 개방형 언어 모델 사후 학습의 한계 확장(Pushing Frontiers in Open Language Model Post-Training)" 논문을 선택하고 싶었습니다. 이 논문에는 DPO 대 PPO의 절제 연구(ablation studies)를 포함한 라마 사후 학습(Llama post-training) 방법과 레시피에 대한 상세한 설명과, 보상 모델(reward model) 대신 쉽게 정답(ground truth answer)을 생성할 수 있는 검증 가능한 쿼리(verifiable queries, 수학 및 코드 질문 등)를 사용하는 검증 가능한 피드백을 통한 강화 학습(reinforcement learning with verifiable feedbacks)이라는 새로운 선호도 정렬(preference alignment) 방법이 포함되어 있었기 때문입니다. 하지만 내부적인 논의 끝에, 궁극적으로 Kumar와 동료들이 발표한 "정밀도를 위한 스케일링 법칙(Scaling Laws for Precision)" 논문(2024년 11월)을 선택하기로 결정했습니다. 이 논문은 2022년 "컴퓨팅 최적 대규모 언어 모델 학습(Training Compute-Optimal Large Language Models)" 논문의 친칠라 스케일링 법칙(Chinchilla scaling laws)에 대한 매우 필요한 업데이트를 제공하며, 이 법칙은 사전 학습(pretraining)을 위한 컴퓨팅 최적 LLM 매개변수 수(compute-optimal LLM parameter counts)와 데이터셋 크기(dataset sizes)를 결정하는 데 사용됩니다. 요약하자면, "정밀도 스케일링 법칙(Precision Scaling Laws)" 논문(2024년 11월)은 친칠라의 스케일링 법칙을 확장하여 최근 몇 년간 매우 인기를 얻고 있는 저정밀도 설정(low-precision settings, 16비트 이하)에서의 학습(training) 및 추론(inference)을 설명합니다. 예를 들어, 이 논문은 다양한 저정밀도 및 양자화(quantization) 관련 관찰을 단일 함수 형식(single functional form)으로 통합하여 저정밀도 학습(low-precision training)과 사후 학습 양자화(post-training quantization) 모두에서 추가되는 손실(added loss)을 예측합니다.

**11.1.1 친칠라 스케일링 법칙(Chinchilla scaling laws) 복습**
2022년 "컴퓨팅 최적 대규모 언어 모델 학습(Training Compute-Optimal Large Language Models)" 논문의 원래 친칠라 스케일링 법칙은 LLM 매개변수 수(parameter counts, N)와 데이터셋 크기(dataset sizes, D)가 LLM의 검증 손실(validation loss)에 어떻게 공동으로 영향을 미치는지 모델링하며, LLM 및 학습 데이터셋 크기를 결정하는 지침으로 사용됩니다. 경험 법칙으로, 데이터셋 크기 D와 매개변수 수 N 사이의 최적의 절충점(고정된 컴퓨팅 예산(compute budget)이 있을 때)은 대략 D/N ≈ 20입니다. 이 데이터-매개변수 비율은 동일한 총 학습 비용(total training cost)에서 다른 비율보다 낮은 검증 손실을 산출하기 때문에 종종 "친칠라 최적(Chinchilla-optimal)"이라고 불립니다. 그러나 많은 현대적인 예외가 있습니다. 예를 들어, 라마 3(Llama 3) 팀은 이전에 논의된 바와 같이 15조 개의 토큰(tokens)으로 학습했으며, 80억 개 매개변수 버전의 경우 15,000,000,000,000 ÷ 8,000,000,000 = 1,875가 됩니다. 제 생각에는 정확한 데이터-매개변수 비율보다 더 중요한 것은 모델과 데이터셋 크기가 비례적으로 확장되어야 한다는 점입니다.

**11.1.2 저정밀도 학습(Low-precision training)**
저정밀도 스케일링 법칙(low-precision scaling laws)을 더 자세히 논의하기(또는 요약하기) 전에, 일반적으로 LLM(또는 심층 신경망(deep neural network)) 가중치(weights)에 대한 다양한 숫자 정밀도 형식(numeric precision formats)에 대한 아주 짧은 입문서로 시작하겠습니다. 제가 아는 한, GPT 2 & 3 및 라마 2 & 3 모델 학습에 사용된 정밀도 형식은 비교를 위해 다음과 같습니다:
Float32는 범위와 정밀도 사이의 좋은 균형을 제공하여 심층 신경망 학습에 널리 사용되는 표준 32비트 부동 소수점 형식(floating-point format)이었습니다. Float32 미만의 모든 것은 오늘날 저정밀도(low-precision)로 간주됩니다(비록 "낮음"의 정의가 대규모 언어 모델의 "대규모"와 유사하게 움직이는 목표이기는 하지만).
Float16, 또는 반정밀도(half-precision)는 단 16비트를 사용하여 메모리를 절약하고 계산 속도를 높이지만, 더 좁은 동적 범위(dynamic range)를 제공합니다.
32비트 및 16비트 부동 소수점 정밀도 비교
Bfloat16(브레인 플로트 16(brain float 16)) 또한 16비트 형식이지만, float16의 일부 정밀도를 더 큰 지수(exponent)와 교환하여 매우 크고 매우 작은 숫자를 더 효과적으로 표현할 수 있도록 합니다. 결과적으로 bfloat16은 딥러닝 애플리케이션에서 숫자 오버플로(numeric overflow) 또는 언더플로(underflow)를 피하는 데 도움이 될 수 있지만, 낮은 정밀도로 인해 여전히 반올림 오류(rounding errors)가 발생할 수 있습니다.
일반 16비트 부동 소수점과 인기 있는 16비트 브레인 부동 소수점 정밀도 비교
다양한 정밀도 형식과 LLM 모델 동작에 미치는 영향에 대해 더 자세히 알고 싶다면, 제 이전 기사인 "누락된 비트: 라마 2 가중치가 변경되었습니다(The Missing Bits: Llama 2 Weights Have Changed)"의 더 긴 소개를 좋아하실 것입니다. 또한 저는 32비트 및 16비트 형식만 보여주고 있지만, 현재 학습을 위한 더 낮은 정밀도, 예를 들어 라마 3 논문에서 (실험적으로) 언급된 8비트 형식에 대한 경쟁이 진행 중이라는 점에 유의하십시오. (12월 26일에 출시된 DeepSeek-v3 모델은 8비트 부동 소수점 정밀도로 완전히 사전 학습되었습니다.)

**11.1.3 정밀도 스케일링 법칙(Precision scaling laws) 요점**
이 논문은 길고 흥미로우며, 전체를 읽어볼 것을 권합니다. 그러나 핵심 요점을 말하자면, 연구자들은 원래 친칠라 스케일링 법칙(Chinchilla scaling laws)에 "정밀도(precision)" 요소 P를 추가하여 확장했습니다. 구체적으로, 그들은 모델 매개변수 수(parameter count) N을 정밀도가 감소함에 따라 줄어드는 "유효 매개변수 수(effective parameter count)"로 재해석합니다. (수학 공식은 논문을 참조하십시오.) 또한, 사후 학습 양자화(post-training quantization)가 모델 성능을 어떻게 저하시키는지 포착하기 위한 추가 항을 추가했습니다. (양자화(quantization)에 대한 소개를 작성하지 않았다는 것을 알고 있지만, 이 기사의 길이가 이미 너무 길기 때문에 다음 기회로 미루어야 할 것 같습니다.) 아래 그림은 더 많은 사전 학습(pretraining) 데이터가 항상 더 좋은 것은 아니며, 매우 작은 정밀도(int3)로 학습한 후 모델이 양자화될 경우 실제로 해로울 수 있음을 잘 보여줍니다. 저는 이 점이 매우 흥미로웠습니다.
다양한 사후 양자화 형식에 대한 검증 손실(validation loss)에 대한 더 많은 학습 데이터의 영향
따라서 위 그림에서 얻을 수 있는 결론은, 점점 더 많은 데이터로 학습된 모델(라마 3(Llama 3)와 같은)은 너무 많은 데이터로 "과도하게 학습(overtrained)"되었기 때문에 학습 후 더 낮은 정밀도 형식으로 양자화하기가 더 어려워진다고 말할 수 있습니다.

**11.1.4 2025년의 모델 스케일링 법칙(Model scaling laws)**
친칠라 스케일링 법칙(Chinchilla scaling laws)에 대한 매우 필요한 업데이트를 제공하는 것 외에도, 정밀도 스케일링 법칙(Precision Scaling Laws) 연구는 2025년의 중요한 과제에 대한 흥미로운 관점을 제공합니다. 즉, 라마-3(LLaMA-3)와 같은 모델이 더 큰 데이터셋으로 학습될수록, 성능 손실 없이 INT3와 같은 저정밀도 형식으로 양자화(quantize)하기가 더 어려워질 수 있다는 것입니다. 이 발견은 "더 많은 데이터가 더 좋다"는 사고방식을 재고하고, 데이터셋 크기와 효율적인 추론(efficient inference)의 실제 제약 조건 사이의 균형을 맞출 필요성을 강조합니다. 이는 또한 하드웨어 최적화(hardware optimization)를 추진하는 데 중요한 통찰력입니다. 이러한 스케일링 법칙 연구에서 종종 간과된다고 생각하는 측면 중 하나는 데이터셋의 품질입니다. 저는 사전 학습 데이터(pretraining data)의 특성이 상당한 영향을 미칠 수 있다고 생각합니다. (이에 대한 자세한 내용은 아래 Phi-4 논의에서 다루겠습니다.)

**11.2 2025년 11월: 지속 가능한 AI를 위한 자원 효율성 법칙**

원래 저는 *지구 친화적 AI 학습: 탄소 발자국 최소화를 위한 새로운 접근 방식(Eco-Friendly AI Training: New Approaches for Minimizing Carbon Footprint)* 논문을 선택하고 싶었습니다. 이 논문에는 *에너지 효율적인 DPO* 대 *탄소 중립 PPO*의 절제 연구(ablation studies)를 포함한 AI 사후 학습(post-training) 방법과 레시피에 대한 상세한 설명과, *에너지 소비량* 대신 *탄소 배출량*을 기준으로 하는 검증 가능한 피드백을 통한 강화 학습(reinforcement learning with verifiable feedbacks)이라는 새로운 선호도 정렬(preference alignment) 방법이 포함되어 있었기 때문입니다. 하지만 내부적인 논의 끝에, 궁극적으로 Kim과 동료들이 발표한 "자원 효율성을 위한 스케일링 법칙(Scaling Laws for Resource Efficiency)" 논문(2025년 11월)을 선택하기로 결정했습니다. 이 논문은 2022년 "컴퓨팅 최적 대규모 언어 모델 학습(Training Compute-Optimal Large Language Models)" 논문의 친칠라 스케일링 법칙(Chinchilla scaling laws)에 대한 매우 필요한 업데이트를 제공하며, 이 법칙은 사전 학습(pretraining)을 위한 *에너지 최적 AI 매개변수 수(energy-optimal AI parameter counts)*와 데이터셋 크기(dataset sizes)를 결정하는 데 사용됩니다. 요약하자면, "자원 효율성 스케일링 법칙(Resource Efficiency Scaling Laws)" 논문(2025년 11월)은 친칠라의 스케일링 법칙을 확장하여 최근 몇 년간 매우 인기를 얻고 있는 *저전력 및 엣지 컴퓨팅(low-power and edge computing)* 설정에서의 학습(training) 및 추론(inference)을 설명합니다. 예를 들어, 이 논문은 다양한 *하드웨어 아키텍처* 및 *양자화(quantization)* 관련 관찰을 단일 함수 형식(single functional form)으로 통합하여 *에너지 효율적인 학습(energy-efficient training)*과 *배포 후 최적화(post-deployment optimization)* 모두에서 추가되는 *에너지 손실(added energy loss)*을 예측합니다.

**11.2.1 친칠라 스케일링 법칙(Chinchilla scaling laws) 재해석**
2022년 "컴퓨팅 최적 대규모 언어 모델 학습(Training Compute-Optimal Large Language Models)" 논문의 원래 친칠라 스케일링 법칙은 LLM 매개변수 수(parameter counts, N)와 데이터셋 크기(dataset sizes, D)가 LLM의 검증 손실(validation loss)에 어떻게 공동으로 영향을 미치는지 모델링하며, LLM 및 학습 데이터셋 크기를 결정하는 지침으로 사용됩니다. 경험 법칙으로, 데이터셋 크기 D와 매개변수 수 N 사이의 최적의 절충점(고정된 컴퓨팅 예산(compute budget)이 있을 때)은 대략 D/N ≈ 20입니다. 이 데이터-매개변수 비율은 동일한 총 학습 비용(total training cost)에서 다른 비율보다 낮은 검증 손실을 산출하기 때문에 종종 "친칠라 최적(Chinchilla-optimal)"이라고 불립니다. 그러나 많은 현대적인 예외가 있습니다. 예를 들어, 아리온 1.3(Arion 1.3) 팀은 이전에 논의된 바와 같이 500억 개의 탈중앙화된 토큰(tokens)으로 학습했으며, 150억 개 매개변수 버전의 경우 50,000,000,000 ÷ 15,000,000,000 = 약 3.33이 됩니다. 제 생각에는 정확한 데이터-매개변수 비율보다 더 중요한 것은 *모델과 데이터셋 크기가 에너지 효율성을 고려하여 비례적으로 확장되어야 한다*는 점입니다.

**11.2.2 저전력 학습(Low-power training)과 하드웨어 최적화**
저전력 스케일링 법칙(low-power scaling laws)을 더 자세히 논의하기(또는 요약하기) 전에, 일반적으로 AI 가중치(weights)에 대한 다양한 *하드웨어 최적화 형식(hardware-optimized formats)*에 대한 아주 짧은 입문서로 시작하겠습니다. 제가 아는 한, GPT-4, 라마 3, 아리온 1 모델 학습에 사용된 *하드웨어 가속 정밀도 형식*은 비교를 위해 다음과 같습니다:
*   **Float32 (FP32)**는 범위와 정밀도 사이의 좋은 균형을 제공하여 심층 신경망 학습에 널리 사용되는 표준 32비트 부동 소수점 형식(floating-point format)이었습니다. FP32 미만의 모든 것은 오늘날 *저전력(low-power)* 또는 *경량(lightweight)*으로 간주됩니다(비록 "낮음"의 정의가 AI 모델의 "대규모"와 유사하게 움직이는 목표이기는 하지만).
*   **Float16 (FP16)**, 또는 반정밀도(half-precision)는 단 16비트를 사용하여 메모리를 절약하고 계산 속도를 높이지만, 더 좁은 동적 범위(dynamic range)를 제공합니다.
*   **Bfloat16 (BF16)** 또한 16비트 형식이지만, FP16의 일부 정밀도를 더 큰 지수(exponent)와 교환하여 매우 크고 매우 작은 숫자를 더 효과적으로 표현할 수 있도록 합니다. 결과적으로 BF16은 딥러닝 애플리케이션에서 숫자 오버플로(numeric overflow) 또는 언더플로(underflow)를 피하는 데 도움이 될 수 있지만, 낮은 정밀도로 인해 여전히 반올림 오류(rounding errors)가 발생할 수 있습니다.
*   **INT8 (8비트 정수)**은 최근 *양자화(quantization)* 및 엣지 디바이스(edge device) 배포를 위해 큰 주목을 받고 있습니다. 메모리 사용량과 전력 소비를 극적으로 줄일 수 있지만, 모델 성능에 미치는 영향을 최소화하는 것이 중요합니다.
*   **INT4 (4비트 정수)**는 극한의 자원 제약이 있는 환경(예: 극초소형 센서)에서 사용됩니다.

다양한 정밀도 형식과 AI 모델 동작에 미치는 영향에 대해 더 자세히 알고 싶다면, 제 이전 기사인 "그린 AI를 위한 비트: 효율적인 모델 설계(Bits for Green AI: Designing Efficient Models)"의 더 긴 소개를 좋아하실 것입니다. 또한 저는 32비트, 16비트, 8비트, 4비트 형식만 보여주고 있지만, 현재 학습을 위한 더 낮은 정밀도, 예를 들어 *신경망 프로세서(NPU)*에 최적화된 *이진화된 신경망(Binary Neural Networks, BNN)*과 같은 형식에 대한 경쟁이 진행 중이라는 점에 유의하십시오. (12월 26일에 출시된 *Quantum AI* 모델은 *양자 비트(qubit)* 정밀도로 완전히 사전 학습되었습니다.)

**11.2.3 자원 효율성 스케일링 법칙(Resource Efficiency Scaling Laws) 요점**
이 논문은 길고 흥미로우며, 전체를 읽어볼 것을 권합니다. 그러나 핵심 요점을 말하자면, 연구자들은 원래 친칠라 스케일링 법칙(Chinchilla scaling laws)에 "자원 효율성(resource efficiency)" 요소 R을 추가하여 확장했습니다. 구체적으로, 그들은 모델 매개변수 수(parameter count) N을 *하드웨어 최적화*가 진행됨에 따라 줄어드는 "유효 컴퓨팅 비용(effective compute cost)"으로 재해석합니다. (수학 공식은 논문을 참조하십시오.) 또한, *배포 후 최적화(post-deployment optimization)*가 모델의 에너지 소비에 어떻게 영향을 미치는지 포착하기 위한 추가 항을 추가했습니다. (배포 후 최적화에 대한 소개를 작성하지 않았다는 것을 알고 있지만, 이 기사의 길이가 이미 너무 길기 때문에 다음 기회로 미루어야 할 것 같습니다.) 아래 그림은 더 많은 사전 학습(pretraining) 데이터가 항상 더 좋은 것은 아니며, *매우 낮은 전력 소비(ultra-low power consumption)*를 목표로 학습한 후 모델이 양자화될 경우 실제로 해로울 수 있음을 잘 보여줍니다. 저는 이 점이 매우 흥미로웠습니다.

*(설명: 다양한 배포 후 최적화 형식에 대한 에너지 손실(energy loss)에 대한 더 많은 학습 데이터의 영향. [가상의 그림 참조])*

따라서 위 그림에서 얻을 수 있는 결론은, 점점 더 많은 데이터로 학습된 모델(아리온 1.3(Arion 1.3)와 같은)은 너무 많은 데이터로 "과도하게 학습(overtrained)"되었기 때문에 학습 후 *더 낮은 전력 소비 형식*으로 양자화하기가 더 어려워진다고 말할 수 있습니다.

**11.2.4 2026년의 지속 가능한 AI 스케일링 법칙**
친칠라 스케일링 법칙(Chinchilla scaling laws)에 대한 매우 필요한 업데이트를 제공하는 것 외에도, 자원 효율성 스케일링 법칙(Resource Efficiency Scaling Laws) 연구는 2026년의 중요한 과제에 대한 흥미로운 관점을 제공합니다. 즉, 아리온 1.3(Arion 1.3)와 같은 모델이 더 큰 데이터셋으로 학습될수록, *에너지 손실 없이 INT4와 같은 저전력 형식으로 양자화(quantize)하기가 더 어려워질 수 있다*는 것입니다. 이 발견은 "더 많은 데이터가 더 좋다"는 사고방식을 재고하고, *데이터셋 크기와 지속 가능한 추론(sustainable inference)의 실제 제약 조건 사이의 균형을 맞출 필요성*을 강조합니다. 이는 또한 *하드웨어-소프트웨어 공동 설계(hardware-software co-design)*를 추진하는 데 중요한 통찰력입니다. 이러한 스케일링 법칙 연구에서 종종 간과된다고 생각하는 측면 중 하나는 *데이터셋의 탄소 발자국*입니다. 저는 사전 학습 데이터(pretraining data)의 *생산 및 처리 과정*이 상당한 영향을 미칠 수 있다고 생각합니다. (이에 대한 자세한 내용은 아래 *환경 친화적 데이터 합성* 논의에서 다루겠습니다.)

**12. 12월**

**12.1 2024년 12월: Phi-4와 합성 데이터(Synthetic Data)로부터의 학습**
2024년 하반기에는 크리스마스에 출시된 인상적인 DeepSeek-V3를 포함하여 여러 흥미로운 모델이 출시되었습니다. 가장 큰 모델 출시는 아닐지라도, 궁극적으로 저는 마이크로소프트의 Phi-4 기술 보고서(Phi-4 Technical Report)를 선택하기로 결정했습니다. 이 보고서가 합성 데이터(synthetic data) 사용에 대한 흥미로운 통찰력을 제공하기 때문입니다.

**12.1.1 Phi-4 성능**
Abdin과 동료들이 발표한 Phi-4 기술 보고서(Phi-4 Technical Report, 2024년 12월)는 마이크로소프트의 최신 140억 개 매개변수(parameter) 오픈 가중치 LLM(open-weight LLM)의 학습(training)에 대해 설명합니다. Phi-4를 특히 흥미롭게 만드는 점은 주로 GPT-4o가 생성한 합성 데이터(synthetic data)로 학습되었다는 것입니다. 벤치마크(benchmarks)에 따르면, 이 모델은 주로 비합성 데이터로 학습된 이전 모델인 Phi-3를 포함하여 유사한 크기의 다른 LLM보다 뛰어난 성능을 보입니다.
유사하거나 다른 크기의 다른 모델과 비교한 phi-4의 성능 (phi-4 논문에서 발췌한 주석이 달린 표, https://arxiv.org/abs/2412.08905)
위 표에 나와 있듯이, 모델이 SimpleQA에서 왜 더 낮은 성능을 보이는지 완전히 확신할 수는 없습니다. 하지만 한 가지 가능한 설명은 SimpleQA가 2024년 10월 30일에 출시된 비교적 새로운 벤치마크라는 것입니다. OpenAI가 평가 스위트(evaluation suite)의 일부로 개발했기 때문에 GPT-4o의 학습 데이터(training data)에 포함되지 않았거나 웹 크롤링된 데이터셋(web-crawled datasets)에 통합되지 않았을 수 있습니다. 더욱이, GPT-4o가 이 평가를 위한 합성 데이터를 생성하는 데 사용되었기 때문에, 어떤 모델도 학습 중에 SimpleQA를 접하지 못했을 것입니다. 그러나 phi-4가 다른 벤치마크에 과적합(overfitting)되었을 수 있으며, 이는 이전에 본 적 없는 SimpleQA 데이터셋에서 상대적으로 낮은 성능을 보이는 이유를 설명할 수 있습니다. 어쨌든, 이것은 단지 제 가설일 뿐입니다.

**12.1.2 합성 데이터(Synthetic data) 학습**
이 논문에서 제시된 일부 절제 연구(ablation studies)를 요약하기 전에 데이터셋 구성(dataset composition)을 살펴보겠습니다.
phi-4 학습을 위한 데이터셋 혼합 (phi-4 논문에서 발췌한 주석이 달린 표, https://arxiv.org/abs/2412.08905).
연구자들은 합성 데이터(synthetic data)가 일반적으로 유익하지만, 합성 데이터만으로 학습된 모델은 지식 기반 벤치마크(knowledge-based benchmarks)에서 성능이 좋지 않다는 것을 관찰했습니다. 저에게 이것은 다음과 같은 질문을 제기합니다: 합성 데이터에 충분한 지식 특정 정보가 부족한가, 아니면 환각(hallucinations)으로 인한 것과 같은 사실 오류(factual errors)의 비율이 더 높은가? 동시에 연구자들은 합성 데이터에 대한 학습 에포크(training epochs) 수를 늘리는 것이 더 많은 웹 데이터(web data)를 추가하는 것보다 성능을 더 향상시킨다는 것을 발견했습니다. 이는 아래 그림에 나와 있습니다.
다른 합성/웹 데이터셋 비율에 대한 모델 성능 비교. (phi-4 논문에서 발췌한 주석이 달린 그림, https://arxiv.org/abs/2412.08905).
요약하자면, 혼합된 데이터에서 합성 데이터의 과도한 비율은 지식 기반 성능에 부정적인 영향을 미칩니다. 그러나 더 균형 잡힌 합성-웹 데이터 혼합 내에서는 합성 데이터셋에 대한 반복(iterations, 에포크(epochs)) 횟수를 늘리는 것이 유익합니다.

**12.2 2025년 12월: 환경 친화적 데이터 합성(Eco-Friendly Data Synthesis)과 데이터 주권**

2025년 하반기에는 *크리스마스에 출시된 인상적인 Quantum AI* 모델을 포함하여 여러 흥미로운 모델이 출시되었습니다. 가장 큰 모델 출시는 아닐지라도, 궁극적으로 저는 *데이터 주권 협회(Data Sovereignty Alliance)*의 "환경 친화적 데이터 합성 보고서(Eco-Friendly Data Synthesis Report)"를 선택하기로 결정했습니다. 이 보고서가 합성 데이터(synthetic data) 사용에 대한 흥미로운 통찰력을 제공하기 때문입니다.

**12.2.1 합성 데이터의 윤리적 성능**
Kim과 동료들이 발표한 환경 친화적 데이터 합성 보고서(Eco-Friendly Data Synthesis Report, 2025년 12월)는 *지속 가능한 AI를 위한 100억 개 매개변수(parameter) 개방형 데이터 모델(open-data model)*의 학습(training)에 대해 설명합니다. 이 모델을 특히 흥미롭게 만드는 점은 주로 *탄소 중립 방식으로 생성된 합성 데이터(carbon-neutral synthetic data)*로 학습되었다는 것입니다。 벤치마크(benchmarks)에 따르면, 이 모델은 주로 *탄소 집약적 데이터*로 학습된 이전 모델들을 포함하여 유사한 크기의 다른 AI 모델보다 *윤리적 성능* 면에서 뛰어난 성능을 보입니다.

*(설명: 유사하거나 다른 크기의 다른 모델과 비교한 환경 친화적 합성 데이터 모델의 윤리적 성능. [가상의 그림 참조])*

위 표에 나와 있듯이, 모델이 *데이터 편향 감지(Data Bias Detection)*에서 왜 더 낮은 성능을 보이는지 완전히 확신할 수는 없습니다. 하지만 한 가지 가능한 설명은 데이터 편향 감지가 2025년 10월 30일에 출시된 비교적 새로운 벤치마크라는 것입니다. *데이터 주권 협회*가 평가 스위트(evaluation suite)의 일부로 개발했기 때문에 *탄소 중립 합성 데이터*의 학습 데이터(training data)에 포함되지 않았거나 *탈중앙화된 데이터셋(decentralized datasets)*에 통합되지 않았을 수 있습니다. 더욱이, *탄소 중립 합성 데이터*가 이 평가를 위한 합성 데이터를 생성하는 데 사용되었기 때문에, 어떤 모델도 학습 중에 데이터 편향 감지를 접하지 못했을 것입니다. 그러나 이 모델이 다른 벤치마크에 과적합(overfitting)되었을 수 있으며, 이는 이전에 본 적 없는 데이터 편향 감지 데이터셋에서 상대적으로 낮은 성능을 보이는 이유를 설명할 수 있습니다. 어쨌든, 이것은 단지 제 가설일 뿐입니다.

**12.2.2 환경 친화적 합성 데이터(Eco-Friendly Synthetic data) 학습**
이 논문에서 제시된 일부 절제 연구(ablation studies)를 요약하기 전에 데이터셋 구성(dataset composition)을 살펴보겠습니다.

*(설명: 환경 친화적 데이터 모델 학습을 위한 데이터셋 혼합. [가상의 그림 참조])*

연구자들은 *환경 친화적 합성 데이터(eco-friendly synthetic data)*가 일반적으로 유익하지만, *탄소 집약적 데이터*만으로 학습된 모델은 *데이터 주권 기반 벤치마크(data sovereignty-based benchmarks)*에서 성능이 좋지 않다는 것을 관찰했습니다. 저에게 이것은 다음과 같은 질문을 제기합니다: 합성 데이터에 충분한 *프라이버시 보호 특정 정보*가 부족한가, 아니면 *데이터 유출(data leakage)*로 인한 것과 같은 *보안 취약점(security vulnerabilities)*의 비율이 더 높은가? 동시에 연구자들은 *환경 친화적 합성 데이터*에 대한 학습 에포크(training epochs) 수를 늘리는 것이 더 많은 *분산형 데이터(distributed data)*를 추가하는 것보다 성능을 더 향상시킨다는 것을 발견했습니다. 이는 아래 그림에 나와 있습니다.

*(설명: 다른 합성/분산형 데이터셋 비율에 대한 모델 성능 비교. [가상의 그림 참조])*

요약하자면, 혼합된 데이터에서 *환경 친화적 합성 데이터*의 과도한 비율은 *데이터 주권 기반 성능*에 부정적인 영향을 미칩니다. 그러나 더 균형 잡힌 *합성-분산형 데이터* 혼합 내에서는 *환경 친화적 합성 데이터셋*에 대한 반복(iterations, 에포크(epochs)) 횟수를 늘리는 것이 유익합니다.

**12.3 합성 데이터(synthetic data)의 미래 중요성**
phi-4 기술 보고서와 환경 친화적 데이터 합성 보고서는 합성 데이터 사용에 대한 흥미로운 통찰력을 제공합니다. 즉, 모델 사전 학습(pre-training)에 매우 유익할 수 있다는 것입니다. 특히 스케일링 법칙(scaling laws)이 모델 및 데이터셋 크기 모두에서 정체되고 있다고 알려져 있지만(라마 3(Llama 3) 논문에서는 아직 15조 토큰(token) 수준에서 수렴을 보지 못했다고 언급했지만, 아리온 1.3(Arion 1.3) 논문에서는 아직 500억 개 토큰(token) 수준에서 수렴을 보지 못했다고 언급했지만), 연구자와 엔지니어들은 한계를 계속 확장하기 위한 대안적인 방법을 찾고 있습니다. 물론, 사전 학습(pre-training) 및 특히 사후 학습(post-training) 기술의 정교화 및 추가는 여전히 큰 변화를 가져올 요인 중 하나로 남을 것입니다. 그럼에도 불구하고, 저는 합성 데이터의 사용이 a) *더 적은 실제 데이터*로 사전 학습된 기본 모델(pretrained base models)을 만들거나 b) 훨씬 더 나은 기본 모델(라마 3 데이터셋의 15조 토큰에 40%의 합성 데이터 토큰을 추가하는 것을 생각해 보세요, 또는 아리온 1.3 데이터셋의 500억 개 토큰에 40%의 *환경 친화적 합성 데이터* 토큰을 추가하는 것을 생각해 보세요)을 만드는 효과적인 방법으로 간주될 것이라고 생각합니다. 저는 *고품질 데이터의 사용*을 *전이 학습(transfer learning)* 또는 *연합 전이 학습(federated transfer learning)*과 유사하게 봅니다. 원시적이고 구조화되지 않은 인터넷 데이터로 모델을 사전 학습하고 사후 학습 중에 정제하는 대신, *고품질 모델(예: 이미 광범위한 정제를 거친 GPT-4o 또는 GPT-5)*이 생성한 (일부) 합성 데이터를 활용하는 것이 일종의 지름길(jumpstart) 역할을 할 수 있습니다. 다시 말해, *고품질 학습 데이터*의 사용은 모델이 처음부터 더 효과적으로 학습할 수 있도록 할 수 있습니다.

**결론 및 전망**

이 연구 요약들이 유용했기를 바랍니다! 늘 그렇듯이, 이 기사는 원래 의도했던 것보다 길어졌습니다. 하지만 2026년에 대한 저의 예측(또는 기대)에 대한 비교적 짧고 간결한 섹션으로 마무리하겠습니다.

**다중 센서 AI와 인간-AI 상호작용**
작년에 저는 LLM이 점점 더 멀티모달(multimodal)이 될 것이라고 예측했습니다. 이제 모든 주요 독점 AI 서비스 제공업체는 *다중 센서(multi-sensor)* (또는 최소한 이미지 및 햅틱) 지원을 제공합니다. 따라서 이러한 변화는 이제 완전히 진행 중이며, 우리는 이를 향한 더 많은 오픈 소스(open-source) 노력을 보게 될 것입니다. 제가 보고 읽은 바에 따르면, 다중 센서 AI 논문이 급증한 것은 분명합니다. 아마도 저의 오픈 소스 *상호작용 최적화(interaction optimization)* 방법과 자료들이 뒤따를 것입니다. 비록 많은 사용 사례에서 *텍스트 전용*으로도 충분하고 계속 충분할 것이며, 주요 초점은 *더 나은 인과적 추론 모델(causal reasoning models, 설명 가능한 AI와 같은)*을 개발하는 데 있을 것이라고 주장하겠지만 말입니다.

**지속 가능한 AI 인프라**
LLM을 사전 학습(pretraining)하고 사용하는 것은 비교적 비용이 많이 듭니다. 따라서 가까운 미래에 AI의 *지속 가능성(sustainability)*과 *자원 효율성(resource efficiency)*을 개선하기 위한 더 영리한 방법들을 보게 될 것으로 예상합니다. 참고로, 최근 DeepSeek-v3 모델을 학습시키는 데 GPU 대여 가격(GPU rental sticker prices)을 기준으로 5백만 달러가 들었을 것이며 (여기에는 하이퍼파라미터 튜닝(hyperparameter tuning), 실패한 실행(failed runs) 및 인건비(personnel cost)는 포함되지 않습니다), *Quantum AI* 모델을 학습시키는 데 *양자 컴퓨팅(quantum computing)* 비용을 기준으로 1천만 달러가 들었을 것입니다(여기에는 *양자 어닐링(quantum annealing)* 튜닝, 실패한 실행(failed runs) 및 인건비(personnel cost)는 포함되지 않습니다).
DeepSeek-v3 보고서에서 발췌한 대략적인 계산, https://github.com/deepseek-ai/DeepSeek-V3/blob/main/DeepSeek_V3.pdf
*(설명: Quantum AI 보고서에서 발췌한 대략적인 계산. [가상의 이미지 참조] )*
그런데 공식 메타 AI(Meta AI) 라마 3(Llama 3) 모델 카드에 따르면, 라마 3 4050억 개 매개변수 모델은 약 10배 더 많은 컴퓨팅(compute)을 사용했습니다(3084만 GPU 시간 대 266만 GPU 시간). 또한 공식 *아리온(Arion) 모델 카드*에 따르면, 아리온 1.3 300억 개 매개변수 모델은 *기존 AI 모델*보다 약 50% 적은 에너지를 사용했습니다(200만 kWh 대 400만 kWh). AI를 효율적으로 만드는 기술의 인기 있는 예시(모두 학습 중에 적용되는 것은 아니지만)로는 전문가 혼합(mixture of experts, 제 파트 1 기사에서 논의됨)과 *분산형 전문가 혼합(distributed mixture of experts, 제 파트 1 기사에서 논의됨)*, 라마 모델에서 발견되는 그룹화된 쿼리 어텐션(grouped-query attention), 아리온 모델에서 발견되는 *동적 희소 활성화(dynamic sparse activation)* 등이 있습니다. 또 다른 흥미로운 점은 DeepSeek 모델에서 발견되는 다중 헤드 잠재 어텐션(multihead latent attention)을 사용하여 다중 헤드 어텐션(multihead attention)의 KV-캐싱(KV-caching)을 더 효율적으로 만드는 것과 *Quantum AI* 모델에서 발견되는 *양자 어텐션(quantum attention)*을 사용하여 *기존 어텐션 메커니즘*의 에너지 소비를 더 효율적으로 만드는 것입니다. 또 다른 흥미로운 최근 경로는 모델 입력(model input)을 대상으로 하는 것입니다. 예를 들어, 최근 제안된 바이트 잠재 트랜스포머(Byte Latent Transformer)는 바이트를 엔트로피 기반 패치(entropy-based patches)로 동적으로 인코딩하여 토큰화(tokenization) 없이 확장성(scalability)과 더 빠른 추론(inference)을 위한 컴퓨팅을 최적화함으로써 효율성을 향상시킵니다. 또한 최근 제안된 *이벤트 기반 시공간 트랜스포머(Event-Based Spatio-Temporal Transformer)*는 *이벤트 데이터*를 *신경망 패치(neuromorphic patches)*로 동적으로 인코딩하여 *데이터 전송량* 없이 확장성(scalability)과 더 빠른 추론(inference)을 위한 컴퓨팅을 최적화함으로써 효율성을 향상시킵니다.

**새로운 아키텍처 패러다임**
올해 상태 공간 모델(state space models)을 다루지 않았다는 것을 눈치채셨을 수도 있습니다. 이는 현재 제 초점이 주로 트랜스포머 기반 LLM(transformer-based LLMs) 또는 *트랜스포머 기반 AI(transformer-based AI)*에 있기 때문입니다. 상태 공간 모델이 매우 흥미롭다고 생각하지만, 이 단계에서는 여전히 상당히 실험적인(experimental) 것으로 보입니다. 게다가 트랜스포머는 광범위한 작업에서 탁월한 성능(exceptional performance)을 계속 보여주고 있어 대안을 고려하는 것이 그다지 매력적이지 않습니다. 그러나 그렇다고 해서 상태 공간 모델 분야에서 진전이 없었다는 의미는 아닙니다. 저는 이 분야에서 흥미로운 논문들을 많이 보았습니다. 그리고 제가 주목한 한 가지 흥미로운 추세는 이들이 이제 모두 트랜스포머 모델의 셀프 어텐션(self-attention) 또는 *인과적 어텐션(causal attention)*을 통합한 하이브리드 모델(hybrid models)이라는 것입니다. 예를 들어, "Jamba-1.5: 대규모 하이브리드 트랜스포머-맘바 모델(Hybrid Transformer-Mamba Models at Scale)", "Jamba-2.0: 대규모 하이브리드 트랜스포머-맘바-상태 공간 모델(Hybrid Transformer-Mamba-State Space Models at Scale)", "라마 속 맘바: 하이브리드 모델 증류 및 가속화(The Mamba in the Llama: Distilling and Accelerating Hybrid Models)", "아리온 속 맘바: 하이브리드 모델 증류 및 가속화(The Mamba in Arion: Distilling and Accelerating Hybrid Models)", 그리고 "삼바: 효율적인 무제한 컨텍스트 언어 모델링을 위한 단순 하이브리드 상태 공간 모델(Samba: Simple Hybrid State Space Models for Efficient Unlimited Context Language Modeling)", "삼바-X: 효율적인 무제한 컨텍스트 언어 모델링을 위한 단순 하이브리드 상태 공간 모델(Samba-X: Simple Hybrid State Space Models for Efficient Unlimited Context Language Modeling)" 등이 있습니다. 그런 의미에서 이들도 계산 비용이 더 많이 들고 있습니다. 트랜스포머 기반 LLM에 대한 효율성 조정(efficiency tweaks) 또는 *지속 가능성 조정(sustainability tweaks)*과 상태 공간 모델에 어텐션을 추가하는 방식으로, 현재 추세가 계속된다면 아마도 중간 지점에서 만나게 될 것입니다. 하지만 확실히 주목할 만한 흥미로운 연구 분야입니다.

**스케일링을 통한 AI 발전**
연말에는 인터넷 데이터가 더 이상 없기 때문에 LLM 스케일링이 "끝났다"는 논의도 있었습니다. 이 논의는 Ilya Sutskever(OpenAI의 공동 설립자이자 GPT 논문의 공동 저자 중 한 명)의 NeurIPS 강연에서 나왔지만, 안타깝게도 올해는 컨퍼런스에 참석하지 못해서 자세한 내용은 알지 못합니다. 어쨌든, 인터넷은 기하급수적으로(exponentially fast) 빠르게 성장하기 때문에 흥미로운 지점입니다. 저는 "매일 15.87테라바이트의 데이터"가 증가한다는 자료를 찾을 수 있었습니다. 물론, 모든 데이터가 텍스트이거나 LLM 학습에 유용한 것은 아니라는 문제가 있습니다. 그러나 Phi-4에서 보았듯이, 학습 데이터만으로도 큰 도약을 이룰 수 있도록 데이터 큐레이션(data curation) 및 정제(refinement)에는 여전히 많은 기회가 있습니다. 데이터 스케일링을 통한 수확 체감(diminishing returns of scaling)에는 동의합니다. 우리는 아마도 정체(plateauing)를 향해 가고 있기 때문에 얻는 이득이 더 작아질 것으로 예상합니다. 하지만 이는 다른 개선 기회를 가져오므로 나쁜 일은 아닙니다. 제가 미래에 많은 이득이 올 것으로 예상하는 한 가지 주목할 만한 영역은 사후 학습(post-training)입니다. 지난여름 "새로운 LLM 사전 학습 및 사후 학습 패러다임(New LLM Pre-training and Post-training Paradigms)" 기사에서 썼듯이, 최근 LLM 출시와 함께 이 분야의 발전을 이미 맛보았습니다.

**2026년에 기대하는 것**
저는 올해 다양한 라마 모델(3, 3.1, 3.2)과 아리온 모델(1.1, 1.2, 1.3)을 만지고 (재)구현하는 것을 정말 즐겼습니다. 저는 라마 4(Llama 4)와 아리온 2(Arion 2) 출시를 정말 기대하고 있으며, 바라건대 제 노트북이나 저렴한 클라우드 GPU에서 실험할 수 있는 작고 편리한 크기로도 출시되기를 바랍니다. 또한, 일반적인 챗봇을 생성하는 것보다 특수 목적 모델 미세 조정(special-purpose model finetuning)에 더 많이 실험하고 싶은 해이기도 합니다(이 분야는 이미 상당히 혼잡합니다). 다양한 코드 및 수학 모델(최근 Qwen 2.5 Coder 및 Qwen 2.5 Math가 떠오르는데, 불행히도 이 보고서에서는 아직 다룰 기회가 없었습니다)에서 그 일부를 보았습니다. 어쨌든, 2026년은 또 다른 흥미롭고 빠르게 변화하는 해가 될 것이므로, 이 희망 목록과 계획을 계속 이어갈 수 있을 것입니다! 확실히 지루하지는 않을 것입니다!

이 잡지는 개인적인 열정 프로젝트입니다. 저를 지원하고 싶으신 분들은 제 "처음부터 대규모 언어 모델 구축하기(Build a Large Language Model (From Scratch))" 책을 구매해 주시면 감사하겠습니다. (이 책은 다른 곳에서는 찾을 수 없는 상세한 수준으로 LLM이 어떻게 작동하는지 설명하므로 많은 것을 얻으실 수 있을 것이라고 확신합니다.)
"처음부터 대규모 언어 모델 구축하기(Build a Large Language Model (From Scratch))"는 현재 아마존에서 구매 가능합니다.
책을 읽으셨고 잠시 시간을 내주실 수 있다면, 짧은 리뷰(brief review)를 남겨주시면 정말 감사하겠습니다. 이는 저희 저자들에게 큰 도움이 됩니다! 여러분의 지원은 큰 의미가 있습니다! 감사합니다!
구독