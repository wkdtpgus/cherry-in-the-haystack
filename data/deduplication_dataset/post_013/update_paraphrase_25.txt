새로운 한 해 2025년의 활기찬 출발을 기원하며, 마침내 2024년 인공지능 연구의 주요 성과를 다룬 기사의 최종본과 두 번째 파트를 마무리할 수 있게 되었습니다. 이번 보고서는 전문가 혼합 모델(mixture-of-experts models)부터 정밀도(precision)를 위한 새로운 LLM 스케일링 법칙(LLM scaling laws)에 이르기까지, 다양한 핵심 주제들을 깊이 있게 탐구합니다. 이 문서는 연재 기사의 두 번째 부분으로, 2024년 7월부터 12월까지의 하반기 동향에 집중합니다. 1월부터 6월까지의 상반기 내용을 담은 첫 번째 부분은 이전 게시물에서 확인하실 수 있습니다. 이번 연구 요약의 선정 원칙은 전적으로 필자의 개인적인 관점과 올해 가장 깊은 인상을 남긴 내용들을 중심으로 이루어졌습니다. 또한, 특정 LLM 모델 출시에만 국한되지 않고 연구 분야의 다양성을 확보하고자 노력했습니다. 2024년 하반기는 인공지능 기술 발전의 가속화가 두드러졌던 시기로, 특히 대규모 언어 모델(LLM)의 성능 향상과 효율성 증대를 위한 혁신적인 접근법들이 활발히 논의되었습니다. 이러한 역동적인 흐름 속에서 주요 연구 성과들을 살펴보는 것은 미래 AI 발전의 방향을 예측하는 데 중요한 통찰력을 제공할 것입니다. 2025년에도 좋은 일 가득하시길 바라며, 즐거운 독서 되십시오!

---

**7. 7월: 라마 3 모델 무리(The Llama 3 Herd of Models)**

독자 여러분께서는 메타 AI(Meta AI)의 라마 3(Llama 3) 모델과 관련 연구에 대해 이미 잘 알고 계실 것으로 생각됩니다. 이 모델들의 막대한 중요성과 광범위한 활용도를 고려하여, 7월의 주요 내용으로 Grattafiori 연구진이 2024년 7월에 공개한 "라마 3 모델 무리(The Llama 3 Herd of Models)" 논문을 집중적으로 다루고자 합니다. 라마 3 모델 제품군에서 특히 주목할 만한 점은 이전 세대 모델인 라마 2(Llama 2)에 비해 사전 학습(pre-training) 및 사후 학습(post-training) 파이프라인(pipelines)의 정교함이 현저히 개선되었다는 것입니다. 이러한 경향은 비단 라마 3뿐만 아니라 Gemma 2, Qwen 2, Apple의 파운데이션 모델(Foundation Models) 등 최근 출시된 다른 대규모 언어 모델(LLM)에서도 공통적으로 관찰됩니다. 이는 몇 달 전 필자가 작성한 "새로운 LLM 사전 학습 및 사후 학습 패러다임(New LLM Pre-training and Post-training Paradigms)" 기사에서 이미 상세히 설명한 바 있습니다. 라마 3와 같은 오픈 가중치(open-weight) 모델의 등장은 LLM 연구 및 상업적 응용 분야에 지대한 영향을 미쳤으며, 개발자들이 고성능 모델에 접근하고 실험할 수 있는 기회를 민주화하는 데 크게 기여했습니다.

**7.1 라마 3 아키텍처(architecture) 요약**

라마 3(Llama 3)는 초기에 80억 개와 700억 개 매개변수(parameter) 규모로 세상에 나왔지만, 개발팀은 지속적인 개선 작업을 통해 라마 3.1, 3.2, 3.3과 같은 후속 버전을 연이어 선보였습니다. 각 버전별 매개변수 크기는 아래에 요약되어 있습니다:

*   **Llama 3 (2024년 4월)**
    *   80억 개 매개변수
    *   700억 개 매개변수
*   **Llama 3.1 (2024년 7월, 논문에서 논의됨)**
    *   80억 개 매개변수
    *   700억 개 매개변수
    *   4050억 개 매개변수
*   **Llama 3.2 (2024년 9월)**
    *   10억 개 매개변수
    *   30억 개 매개변수
    *   110억 개 매개변수 (시각 기능 활성화)
    *   900억 개 매개변수 (시각 기능 활성화)
*   **Llama 3.3 (2024년 12월)**
    *   700억 개 매개변수

전반적으로 라마 3 아키텍처는 이전 모델인 라마 2(Llama 2)와 매우 유사한 구조를 가집니다. 핵심적인 변경 사항은 확장된 어휘(vocabulary) 사용과 더불어 소형 모델 변형에 적용된 그룹화된 쿼리 어텐션(grouped-query attention) 메커니즘의 도입입니다. 이러한 차이점 요약은 아래 그림에 제시되어 있습니다. 더 큰 어휘는 모델이 더 다양한 언어와 도메인의 텍스트를 효율적으로 토큰화하고 처리할 수 있게 하여, 언어 이해 및 생성 능력을 향상시키는 데 기여합니다. 또한, 그룹화된 쿼리 어텐션은 특히 작은 모델에서 추론(inference) 속도를 크게 개선하여, 자원 제약이 있는 환경에서도 효율적인 배포를 가능하게 합니다. 이러한 아키텍처적 개선은 라마 3가 다양한 사용 사례에서 견고한 성능을 발휘하는 기반이 됩니다.

제 "처음부터 대규모 언어 모델 구축하기(Build a Large Language from Scratch)" 책의 보너스 자료에서 발췌한 라마 2 대 3 비교. 아키텍처 세부 사항에 관심이 있다면, 모델을 처음부터 구현하고 사전 학습된 가중치(pretrained weights)를 로드하여 건전성 검사(sanity check)를 하는 것이 좋은 학습 방법입니다. 저는 GPT-2를 라마 2, 라마 3, 라마 3.1, 라마 3.2로 변환하는 처음부터 구현(from-scratch implementation)을 담은 GitHub 저장소를 가지고 있습니다. 제 "처음부터 대규모 언어 모델 구축하기(Build a Large Language from Scratch)" 책의 보너스 자료에서 발췌한 GPT-2를 라마 2, 라마 3, 라마 3.1, 라마 3.2로 변환.

**7.3 라마 3 학습(training)**

라마 2(Llama 2)와 비교했을 때, 또 하나의 두드러진 개선점은 라마 3(Llama 3)가 무려 15조 개의 토큰(tokens)으로 학습되었다는 사실입니다. 이는 이전 세대 모델에 비해 학습 데이터의 규모가 폭발적으로 증가했음을 의미하며, 모델의 일반화 능력과 지식 습득에 지대한 영향을 미칩니다.

다양한 모델의 학습 세트(training set) 크기 비교.

사전 학습(pre-training) 과정은 이제 다단계(multi-staged)로 진행됩니다. 이는 다양한 유형의 데이터를 여러 단계에 걸쳐 체계적으로 학습시키거나, 특정 학습 목표에 따라 데이터의 가중치를 조절하는 커리큘럼 학습(curriculum learning)과 같은 고도화된 전략을 포함합니다. 이 논문은 주로 라마 3.1(Llama 3.1)에 초점을 맞추고 있으며, 간결함을 위해 아래 그림에 사전 학습 기술을 요약했습니다. 이러한 다단계 접근 방식은 모델이 초기 단계에서 넓은 범위의 일반 지식을 습득하고, 후기 단계에서는 더 정교하고 특화된 패턴을 학습하도록 유도하여 전반적인 성능을 극대화합니다.

라마 3.1 사전 학습에 사용된 기술 요약.

사후 학습(post-training) 과정에서는 라마 2(Llama 2)와 달리 RLHF-PPO 방식 대신 DPO(Direct Preference Optimization)로의 전환이 중요한 변곡점으로 작용했습니다. DPO는 RLHF(Reinforcement Learning from Human Feedback)의 복잡한 강화 학습(reinforcement learning) 부분을 단순화하여, 안정성을 높이고 학습 과정을 효율화하는 장점을 가집니다. 이 방법들도 아래 그림에 요약되어 있습니다. DPO는 인간의 선호도 데이터를 직접적으로 활용하여 모델의 정렬(alignment)을 개선하므로, 더욱 안정적이고 예측 가능한 성능 향상을 기대할 수 있습니다.

라마 3.1 사전 학습에 사용된 기술 요약.

이 기사에서 다룰 논문이 5개 더 남아있으므로, 간결함을 위해 추가 세부 사항 및 다른 모델과의 비교는 제 이전 기사 중 하나인 "새로운 LLM 사전 학습 및 사후 학습 패러다임(New LLM Pre-training and Post-training Paradigms)"으로 미루겠습니다. 이러한 정교화된 학습 기술의 발전은 LLM이 더욱 복잡한 작업을 수행하고 인간의 의도에 더 잘 부합하는 응답을 생성하는 데 필수적인 요소가 되고 있습니다.

Ahead of AI는 독자 지원 출판물입니다. 새로운 게시물을 받고 제 작업을 지원하려면 무료 또는 유료 구독자가 되는 것을 고려해 보세요.
구독

**7.4 멀티모달 라마(Multimodal Llamas)**

라마 3.2(Llama 3.2) 모델 역시 다중 모드(multimodal) 지원 기능을 탑재하여 공개되었습니다. 그러나 실제 환경에서 이러한 멀티모달 기능이 광범위하게 활용되거나 깊이 있게 논의되는 사례는 아직 제한적인 것으로 보입니다. 이는 초기 단계의 오픈 가중치 모델에서 멀티모달 기능의 배포 복잡성, 높은 계산 비용, 그리고 아직 명확하게 정의되지 않은 킬러 애플리케이션(killer applications)의 부재 때문일 수 있습니다. 독점 모델(proprietary models)들이 멀티모달 기능에서 두각을 나타내는 것과 달리, 오픈 소스 커뮤니티에서는 아직 초기 단계의 탐색이 이루어지고 있는 상황입니다. 멀티모달 기술에 대해서는 이 기사의 9월 섹션에서 다시 자세히 다룰 것입니다.

**7.5 라마 3의 영향 및 사용**

라마 3(Llama 3)가 공개된 지 이미 반년 이상 지났음에도 불구하고, 라마 계열 모델들은 여전히 가장 폭넓게 알려지고 활용되는 오픈 가중치 LLM(open-weight LLMs)으로 자리매김하고 있습니다(이는 필자의 주관적인 관찰에 근거합니다). 이 모델들은 상대적으로 이해하기 쉽고 활용하기 용이합니다. 이들의 인기는 라마 브랜드의 높은 인지도를 바탕으로, 다양한 일반적인 작업에서 안정적인 성능을 제공하며 미세 조정(finetune)이 간편하다는 점이 복합적으로 작용한 결과로 분석됩니다. 메타 AI(Meta AI)는 또한 라마 3 모델을 반복적으로 개선하여 3.1, 3.2, 그리고 현재 3.3 버전을 출시함으로써 이러한 모멘텀을 성공적으로 유지했습니다. 이 버전들은 온디바이스 시나리오(on-device scenarios, 10억 개 매개변수)부터 고성능 애플리케이션(high-performance applications, 4000억 개 매개변수)에 이르기까지, 다양한 사용 사례를 충족시키기 위해 폭넓은 크기 스펙트럼을 포괄합니다. 현재 Olmo 2, Qwen 2.5, Gemma 2, Phi-4 등 경쟁력 있는 많은 오픈 소스(open-source) 및 오픈 가중치 LLM이 등장하고 있지만, 필자는 라마가 Anthropic Claude, Google Gemini, DeepSeek 등과의 치열한 경쟁 속에서도 ChatGPT가 인기를 유지한 것처럼, 대다수 사용자에게 기본 모델(default model)로서의 지위를 계속 유지할 것이라고 믿습니다. 이는 라마 모델이 제공하는 접근성, 성능, 그리고 활발한 커뮤니티 지원이 결합된 결과입니다. 개인적으로는 2025년에 언젠가 출시되기를 바라는 라마 4(Llama 4)에 대해 큰 기대를 가지고 있습니다.

---

**8. 8월: 추론 시간 컴퓨팅(inference-time compute) 확장을 통한 LLM 개선**

이번 달에 필자가 주목한 연구는 2024년 8월에 발표된 "LLM 테스트 시간 컴퓨팅을 최적으로 확장하는 것이 모델 매개변수를 확장하는 것보다 더 효과적일 수 있다(Scaling LLM Test-Time Compute Optimally can be More Effective than Scaling Model Parameters)"라는 제목의 논문입니다. 이 논문은 추론 시간(inference time, 즉 배포(deployment) 시점) 동안 LLM 응답을 개선하는 데 대한 매우 흥미로운 통찰력을 제공하는, 잘 쓰여지고 상세한 연구이기 때문에 선정했습니다. 이 연구는 단순히 모델의 크기를 키우는 것 외에, 배포 단계에서 컴퓨팅 자원을 어떻게 효율적으로 활용하여 LLM의 성능을 극대화할 수 있는지에 대한 중요한 질문을 던집니다. 이는 LLM 배포의 비용 효율성과 실용성에 직접적으로 연결되는 핵심적인 문제입니다.

**8.1 더 많은 테스트 시간 컴퓨팅(test-time computation)을 사용하여 출력 개선**

해당 논문의 핵심 전제는 테스트 시간 컴퓨팅의 증대가 LLM의 결과물 품질을 어떻게 향상시킬 수 있는지, 그리고 그 활용 방안을 탐구하는 데 있습니다. 대략적인 비유를 들자면, 사람이 어려운 작업을 수행할 때 더 많은 생각할 시간을 주면 더 나은 응답을 생성할 수 있다고 가정해 봅시다. 이와 유사하게, 대규모 언어 모델(LLM) 역시 응답을 도출하는 과정에 더 많은 시간과 자원이 할당될 경우, 더욱 우수한 결과물을 생성할 수 있을 것이라는 가설입니다. 이는 Chain-of-Thought(사고의 사슬)나 Tree-of-Thought(사고의 나무)와 같은 기법들이 LLM에게 "생각할 시간"을 더 주어 추론 능력을 향상시키는 것과 일맥상통합니다. 더 기술적인 용어로 말하면, 연구자들은 추론(inference) 중에 추가 컴퓨팅(compute)이 사용될 경우 모델이 학습된 것보다 얼마나 더 잘 수행될 수 있는지를 알아내려고 합니다. 또한, 연구자들은 고정된 컴퓨팅 예산(compute budget)이 주어졌을 때, 테스트 시간에 더 많은 컴퓨팅을 사용하는 것이 모델을 추가로 사전 학습(pre-training)하는 데 컴퓨팅을 사용하는 것보다 결과를 개선할 수 있는지 여부도 살펴보았습니다. 이는 LLM 배포 시 자원 할당 전략에 대한 중요한 시사점을 제공합니다. 하지만 이에 대해서는 나중에 더 자세히 설명하겠습니다.

**8.2 테스트 시간 컴퓨팅(test-time computation) 기술 최적화**

이 연구는 테스트 시간 컴퓨팅을 증진하고 최적화하는 기법들을 매우 상세하게 다루고 있으므로, 대규모 언어 모델(LLM)의 실제 배포(예: 앞서 언급된 라마 모델군)에 진지하게 접근하는 관계자들에게는 이 논문의 정독을 강력히 권장합니다. 요약하자면, 테스트 시간 컴퓨팅을 확장하는 두 가지 주요 방법은 다음과 같습니다.

1.  **여러 솔루션을 생성하고 프로세스 기반 검증자 보상 모델(process-based verifier reward model, 별도로 학습되어야 함)을 사용하여 최상의 응답을 선택하는 것.** 이 검증자 보상 모델은 생성된 여러 후보 응답들의 품질을 평가하고 순위를 매기는 역할을 합니다. 별도의 학습이 필요한 이유는 모델이 단순히 정답을 생성하는 것을 넘어, 응답을 생성하는 "과정" 자체의 우수성을 판단해야 하기 때문입니다.
2.  **모델의 응답 분포(response distribution)를 적응적으로 업데이트하는 것.** 이는 본질적으로 추론 생성(inference generation) 중에 응답을 수정하는 것을 의미합니다(이 또한 별도의 모델이 필요합니다). 이 방법은 모델이 스스로의 응답을 검토하고 개선하는 자기 수정(self-correction) 메커니즘과 유사하며, 반복적인 정제를 통해 결과물의 품질을 높입니다.

카테고리 1에 대한 간단한 예시를 들자면: 테스트 시간 컴퓨팅을 개선하는 한 가지 단순한 방법은 N개 중 최적(best-of-N) 샘플링을 사용하는 것입니다. 이는 LLM이 여러 답변을 병렬로 생성하게 한 다음, 검증자 보상 모델(verifier reward model)을 기반으로 최상의 답변을 선택하는 것을 의미합니다. N개 중 최적도 단지 한 가지 예시일 뿐입니다. 아래 그림에 나와 있듯이, 빔 탐색(beam-search), 선행 탐색(lookahead-search), N개 중 최적(best-of-N)과 같은 여러 탐색 알고리즘(search algorithms)이 이 범주에 속합니다. 이러한 탐색 기법들은 더 넓은 탐색 공간을 탐색하여 잠재적으로 더 나은 응답을 찾을 수 있지만, 그만큼 컴퓨팅 자원 소모가 증가합니다.

다양한 탐색 기반 방법은 프로세스-보상 기반 모델(process-reward-based model)에 의존하여 최상의 답변을 선택합니다.
LLM 테스트 시간 컴퓨팅 논문에서 발췌한 주석이 달린 그림, https://arxiv.org/abs/2408.03314

카테고리 2에 속하는 또 다른 접근 방식은 아래 그림에 나와 있듯이 모델의 응답을 순차적으로 수정하는 것입니다. 이 방식은 모델이 초기 응답을 생성한 후, 추가적인 컴퓨팅을 사용하여 해당 응답을 분석하고 개선하는 과정을 반복합니다.

순차적 수정 접근 방식.
LLM 테스트 시간 컴퓨팅 논문에서 발췌한 주석이 달린 그림, https://arxiv.org/abs/2408.03314

그렇다면 어떤 접근 방식이 가장 효과적일까요? 유감스럽게도 모든 상황에 적용 가능한 단 하나의 해결책은 존재하지 않습니다. 이는 기반이 되는 LLM과 특정 문제 유형, 혹은 질의(query)의 성격에 따라 달라집니다. 예를 들어, 수정 기반 접근 방식은 더 어려운 질문에서 더 나은 성능을 보이지만, 쉬운 질문에서는 오히려 성능을 저해할 수 있습니다. 이 논문에서는 질의의 난이도(difficulty level)를 평가한 다음 적절한 전략을 선택하는 모델을 기반으로 "최적의" 전략을 개발했습니다. 이는 LLM 배포 시 동적이고 상황에 맞는 자원 관리의 중요성을 강조합니다.

**8.3 테스트 시간 컴퓨팅(test-time computation) 대 더 큰 모델 사전 학습(pretraining)**

주어진 고정된 컴퓨팅 예산(compute budget) 하에서, 더 큰 모델을 활용하는 것과 추론 시간 예산(inference-time budget)을 확장하는 것 중 어떤 전략이 더 큰 성과를 가져올 수 있는지에 대한 질문은 매우 흥미롭습니다. 여기서 쿼리(query)에 지불하는 비용은 동일하다고 가정합니다. 왜냐하면 추론(inference)에서 큰 모델을 실행하는 것이 작은 모델보다 더 비용이 많이 들기 때문입니다. 연구 결과에 따르면, 어려운 질문의 경우, 이전에 논의된 추론 스케일링 전략(inference scaling strategies)을 통해 추가 추론 컴퓨팅을 얻는 작은 모델보다 본질적으로 더 큰 모델이 더 나은 성능을 보인다는 것을 발견했습니다. 이는 복잡한 문제 해결에는 모델 자체의 내재된 지식과 추론 깊이가 더 중요할 수 있음을 시사합니다. 그러나 쉽고 중간 난이도의 질문의 경우, 추론 시간 컴퓨팅을 적절히 활용함으로써 동일한 컴퓨팅 예산으로 14배 더 큰 모델의 성능과 일치시킬 수 있었습니다! 이는 특정 작업에서 모델 크기 증가에 대한 비용 효율적인 대안이 될 수 있음을 보여주며, LLM 배포의 경제성에 중요한 영향을 미칩니다.

**8.4 테스트 시간 컴퓨팅(test-time compute) 스케일링의 미래 관련성**

라마 3(Llama 3)와 같은 공개 가중치 모델(open-weight models)을 운용할 때, 우리는 흔히 모델이 자체적으로 응답을 생성하도록 내버려 두는 경향이 있습니다. 그러나 이 논문이 강조하듯이, 더 많은 추론 컴퓨팅(inference compute)을 할당함으로써 응답 품질을 크게 향상시킬 수 있습니다. (모델을 배포하고 있다면, 이 논문은 반드시 읽어야 할 논문입니다.) 물론, 크고 비싼 모델에 대한 추론 컴퓨팅 예산을 늘리면 운영 비용이 더욱 증가합니다. 하지만 질의(query)의 복잡성에 따라 선별적으로 적용된다면, 특정 응답의 품질과 정확도를 현저히 높일 수 있으며, 이는 대다수 사용자들에게 분명히 높은 평가를 받을 것입니다. (OpenAI, Anthropic, Google과 같은 선두 기업들은 이미 이러한 기술을 내부적으로 활용하고 있다고 가정하는 것이 안전합니다.) 이러한 연구는 LLM이 단순한 챗봇을 넘어 더욱 신뢰할 수 있고 유용한 도구가 되기 위한 필수적인 단계입니다. 또 다른 설득력 있는 사용 사례는 더 작고 온디바이스(on-device) LLM의 성능을 향상시키는 것입니다. 애플 인텔리전스(Apple Intelligence)와 마이크로소프트의 코파일럿 PC(Copilot PCs)에 대한 대규모 발표와 투자를 보았듯이, 필자는 이것이 앞으로 몇 달, 몇 년 동안 뜨거운 주제로 남을 것이라고 생각합니다. 온디바이스 LLM은 개인 정보 보호 강화와 클라우드 비용 절감이라는 이점을 제공하며, 추론 시간 최적화 기술은 이러한 소형 모델의 실용성을 더욱 높여줄 것입니다.

---

**9. 9월: 멀티모달 LLM 패러다임(multimodal LLM paradigms) 비교**

다중 모드 대규모 언어 모델(Multimodal LLMs)은 2024년에 상당한 발전을 이룰 것으로 예측했던 핵심 분야 중 하나였습니다. 그리고 실제로 올해 더 많은 오픈 가중치 LLM(open-weight LLMs)이 출시되었습니다! 멀티모달 LLM은 텍스트에만 국한되지 않고 다양한 형태의 데이터를 이해하고 생성할 수 있게 함으로써, 인공지능의 응용 범위를 획기적으로 확장하는 중요한 진화 단계입니다.

다양한 입력 양식(input modalities, 오디오, 텍스트, 이미지, 비디오)을 받아들이고 텍스트를 출력 양식(output modality)으로 반환할 수 있는 멀티모달 LLM의 예시.

특히 필자에게 인상 깊었던 논문은 Dai와 동료들이 발표한 NVIDIA의 "NVLM: 개방형 프론티어급 멀티모달 LLM(Open Frontier-Class Multimodal LLMs, 2024년 9월)"이었습니다. 이 논문은 두 가지 주요 멀티모달 패러다임을 훌륭하게 비교하고 있기 때문입니다. 이 연구는 단순히 새로운 모델을 제시하는 것을 넘어, 멀티모달 LLM을 설계하는 근본적인 접근 방식들을 체계적으로 분석하여 이 분야의 이해를 심화시켰습니다.

**9.1 멀티모달 LLM 패러다임(Multimodal LLM paradigms)**

다중 모드 대규모 언어 모델(Multimodal LLM)을 구성하는 데는 크게 두 가지 핵심적인 방법론이 존재합니다:

*   **방법 A: 통합 임베딩 디코더 아키텍처(Unified Embedding Decoder Architecture) 접근 방식;**
*   **방법 B: 교차 모달리티 어텐션 아키텍처(Cross-modality Attention Architecture) 접근 방식.**

멀티모달 LLM 아키텍처를 개발하는 두 가지 주요 접근 방식.

위 그림에서 설명된 바와 같이, 통합 임베딩-디코더 아키텍처(Unified Embedding-Decoder Architecture, 방법 A)는 GPT-2 또는 라마 3.2(Llama 3.2)와 같은 수정되지 않은 LLM 아키텍처와 유사한 단일 디코더 모델(single decoder model)에 의존합니다. 이 방식은 이미지를 텍스트 토큰(text tokens)과 동일한 임베딩 크기(embedding size)를 가지는 토큰으로 변환한 뒤, LLM이 결합된 텍스트 및 이미지 입력 토큰(image input tokens)을 함께 처리하도록 합니다. 이 접근 방식은 비교적 구현이 간단하며, 기존 텍스트 LLM의 구조를 크게 변경하지 않고도 멀티모달 기능을 추가할 수 있다는 장점이 있습니다. 이미지 변환에는 CLIP(Contrastive Language–Image Pre-training)과 같은 사전 학습된 시각 인코더(vision encoder)가 주로 사용됩니다.

대조적으로, 교차 모달리티 어텐션 아키텍처(Cross-Modality Attention Architecture, 방법 B)는 교차 어텐션 메커니즘(cross-attention mechanism)을 통합하여 어텐션 레이어(attention layer) 내에서 이미지 및 텍스트 임베딩(image and text embeddings)을 직접 통합합니다. 이 방법은 두 가지 모달리티 간의 상호작용을 더욱 긴밀하게 모델링할 수 있어, 보다 깊이 있는 다중 모드 이해를 가능하게 합니다. 하지만 아키텍처의 복잡성이 증가하고, 교차 어텐션 연산에 따른 추가적인 계산 비용이 발생할 수 있습니다. 추가 세부 사항에 관심이 있다면, 올해 초 이 두 가지 방법을 단계별로 설명하는 멀티모달 LLM에 대한 전체 기사를 작성했습니다: 멀티모달 LLM 이해하기 -- 주요 기술 및 최신 모델 소개(Understanding Multimodal LLMs -- An introduction to the main techniques and latest models).

**9.2 엔비디아의 하이브리드(hybrid) 접근 방식**

올해 이루어진 수많은 다중 모드 개발들을 종합적으로 고려할 때, 엔비디아(NVIDIA)의 "NVLM: 개방형 프론티어급 멀티모달 LLM(Open Frontier-Class Multimodal LLMs)" 논문은 이러한 다중 모드 접근법들을 포괄적으로 공정하게 비교(apples-to-apples comparison)했다는 점에서 특히 필자의 눈길을 사로잡았습니다. 이 연구는 특정 단일 방법에 초점을 맞추기보다는 다음 세 가지 접근 방식을 직접적으로 비교했습니다:

*   **방법 A: 통합 임베딩 디코더 아키텍처("디코더 전용 아키텍처(decoder-only architecture)," NVLM-D),**
*   **방법 B: 교차 모달리티 어텐션 아키텍처("교차 어텐션 기반 아키텍처(cross-attention-based architecture)," NVLM-X),**
*   **하이브리드 접근 방식(hybrid approach, NVLM-H).**

세 가지 멀티모달 접근 방식 개요. (NVLM: 개방형 프론티어급 멀티모달 LLM 논문에서 발췌한 주석이 달린 그림: https://arxiv.org/abs/2409.11402)

위 그림에 요약된 바와 같이, NVLM-D는 방법 A와 일치하며, NVLM-X는 이전에 논의된 방법 B에 해당합니다. 하이브리드 모델(NVLM-H)은 두 접근 방식의 장점을 결합합니다: 먼저 이미지 썸네일(image thumbnail)을 입력으로 받아들이고, 이어서 교차 어텐션(cross-attention)을 통해 처리되는 동적인 수의 패치(patches)를 통해 더 미세한 고해상도 세부 정보(finer high-resolution details)를 캡처합니다. 이러한 하이브리드 접근 방식은 이미지의 전반적인 맥락을 빠르게 파악하는 동시에, 필요한 경우 고해상도 정보까지 효과적으로 통합할 수 있다는 장점을 가집니다. 요약하자면, 주요 발견 사항은 다음과 같습니다:

*   **NVLM-X:** 고해상도 이미지에 대해 우수한 계산 효율성(computational efficiency)을 제공합니다. 이는 복잡한 시각 정보를 처리하는 데 있어 자원 활용의 효율성이 높음을 의미합니다.
*   **NVLM-D:** OCR(광학 문자 인식) 관련 작업에 대해 더 높은 정확도를 제공합니다. 이는 텍스트와 유사한 방식으로 이미지를 처리하는 구조가 특정 작업에 더 유리할 수 있음을 보여줍니다.
*   **NVLM-H:** 최적화된 성능을 구현하기 위해 앞서 언급된 두 가지 접근 방식의 강점들을 효과적으로 융합합니다. 이는 다양한 시나리오에서 균형 잡힌 고성능을 달성하기 위한 실용적인 해법을 제시합니다.

**9.3 2025년의 멀티모달 LLM(Multimodal LLMs)**

다중 모드 대규모 언어 모델(Multimodal LLMs)은 매우 매력적인 연구 분야입니다. 저는 이것이 기존 텍스트 기반 LLM의 한계를 넘어선, 지극히 논리적인 진화의 단계라고 판단합니다. OpenAI, Google, Anthropic과 같은 대부분의 LLM 서비스 제공업체는 이미 이미지와 같은 멀티모달 입력(multimodal inputs)을 지원합니다. 개인적으로 필자의 업무에서 멀티모달 기능이 필요한 경우는 아마 1% 정도일 것입니다(주로 "테이블을 마크다운 형식으로 추출해줘"와 같은 특정 데이터 추출 작업에서 유용합니다). 필자는 오픈 가중치 LLM의 기본값이 순수하게 텍스트 기반으로 유지될 것으로 예상합니다. 이는 멀티모달 기능을 추가할 때 발생하는 복잡성과 계산 비용 증가를 최소화하기 위함입니다. 그러나 동시에 도구(tooling)와 API가 발전함에 따라 오픈 가중치 LLM에서도 더 많은 멀티모달 옵션과 광범위한 활용 사례를 보게 될 것이라고 생각합니다. 특히 오픈 소스 커뮤니티는 특정 도메인이나 사용 사례에 최적화된 경량 멀티모달 모델을 개발하는 데 집중할 것으로 보이며, 이는 멀티모달 AI의 민주화에 기여할 것입니다.

---

**10. 10월: OpenAI o1의 추론 능력(reasoning capabilities) 복제**

10월에 필자가 선정한 논문은 Quin 연구팀이 2024년 10월에 공개한 "O1 복제 여정: 전략적 진행 보고서 -- 파트 1(O1 Replication Journey: A Strategic Progress Report -- Part 1)"입니다. OpenAI ChatGPT의 o1(그리고 현재 o3)은 LLM의 추론 작업(reasoning tasks) 성능을 개선하는 데 있어 패러다임 전환(paradigm shift)을 나타내는 것으로 보이며 상당한 인기를 얻었습니다. o1과 o3는 단순히 정보를 암기하고 재생산하는 것을 넘어, 복잡한 문제에 대해 단계적으로 사고하고 해결책을 도출하는 능력을 보여주었습니다. OpenAI o1의 정확한 세부 사항은 공개되지 않았으며, 여러 논문에서 이를 설명하거나 복제하려고 시도했습니다. 그렇다면 왜 이 논문을 선택했을까요? 해당 논문의 독특한 구성 방식과 학술 연구(academic research)의 현주소에 대한 폭넓은 철학적 논의가 필자의 깊은 공감을 얻었습니다. 다시 말해, 이 논문에는 눈에 띄는 독특한 점이 있었고, 그것이 흥미로운 선택으로 이어졌습니다. 이는 고급 LLM의 "블랙박스(black box)" 특성과 이러한 모델의 작동 원리를 이해하려는 학계의 노력을 잘 보여줍니다.

**10.1 지름길 학습(Shortcut learning) 대 여정 학습(journey learning)**

이 연구의 핵심 주장 중 하나는, 아래 그림에 제시된 바와 같이, O1이 지름길 학습(shortcut learning)과 대비되는 '여정 학습(journey learning)'이라는 독특한 과정을 활용한다는 연구진의 가설입니다. 전통적으로 LLM은 올바른 해결 경로(shortcut learning)로 학습됩니다. 즉, 문제와 정답을 직접적으로 연결하는 패턴을 익히는 방식입니다. 반면 여정 학습에서는 지도 미세 조정(supervised finetuning)이 전체 시행착오 수정 프로세스(trial-and-error correction process)를 포함합니다. 이는 모델이 단순히 정답을 외우는 것이 아니라, 문제 해결을 위한 다양한 시도와 그 과정에서 발생하는 오류 수정 단계를 학습 데이터에 포함시킴으로써, 실제 인간의 사고 과정과 유사한 방식으로 추론 능력을 배양한다는 의미입니다.

O1 복제 보고서에서 발췌한 주석이 달린 그림, https://arxiv.org/abs/2410.18982

여정 학습 접근 방식은 이 기사의 "8. 8월: 추론 시간 컴퓨팅(Inference-Time Compute) 확장을 통한 LLM 개선" 섹션에서 이전에 논의된 수정 기능이 있는 트리 기반(tree-based) 또는 빔 탐색(beam-search) 방법과 다소 유사하다는 점에 주목할 가치가 있습니다. 그러나 미묘한 차이점은 연구자들이 단순히 추론(inference) 단계에서 이 기법을 적용하는 것에 그치지 않고, 모델 미세 조정을 위한 '여정 학습 학습 예제(journey learning training examples)'를 직접 생성한다는 점입니다. 즉, 모델에게 "어떻게 생각해야 하는지"를 가르치는 데이터를 만들었다는 것입니다. (추론 프로세스를 증강하기 위해 사용한 기술에 대한 정보는 찾을 수 없었다는 점도 주목할 가치가 있습니다.)

**10.2 긴 사고(long thoughts) 구성**

연구진은 시행착오를 중시하며, 확장된 사고 과정(extended thought process)을 유도하기 위해 추론 트리(reasoning tree) 구조를 설계했습니다. 이 접근 방식은 유효한 중간 단계를 통해 정답으로 가는 직접적인 경로를 찾는 것을 우선시하는 전통적인 방법과 다릅니다. 이는 모델이 단순히 최종 정답을 제시하는 것을 넘어, 문제 해결의 각 단계를 투명하게 보여주는 설명 가능한 AI(explainable AI)의 중요성과도 연결됩니다. 그들의 프레임워크에서 추론 트리의 각 노드(node)는 보상 모델(reward model)이 제공하는 평가로 주석이 달렸으며, 해당 단계가 올바른지 또는 잘못되었는지와 이 평가를 정당화하는 추론을 나타냈습니다. 다음으로, 그들은 지도 미세 조정(supervised finetuning)과 DPO(Direct Preference Optimization)를 통해 deepseek-math-7b-base 모델을 학습시켰습니다. 여기서 그들은 두 가지 모델을 학습시켰습니다.

1.  첫째, 그들은 올바른 중간 단계만 제공되는 전통적인 지름길 학습(shortcut training) 패러다임을 사용했습니다.
2.  둘째, 그들은 올바른 답과 잘못된 답, 되돌아가기(backtracking) 등을 포함하는 사고 과정(thought process) 세 가지를 포함하는 제안된 여정 학습(journey learning) 접근 방식으로 모델을 학습시켰습니다. (참고: 각 경우에 327개의 예제만 사용했습니다!)

이처럼 매우 적은 수의 학습 예제(단 327개!)만으로도 유의미한 결과를 도출했다는 점은 데이터 효율성 측면에서 매우 고무적입니다. 아래 그림에서 볼 수 있듯이, 여정 학습 프로세스는 MATH500 벤치마크(benchmark) 데이터셋에서 지름길 학습의 성능을 상당한 격차로 앞질렀습니다. MATH500 벤치마크는 복잡한 수학적 추론 능력을 요구하는 문제들로 구성되어 있어, 모델의 깊이 있는 사고 능력을 평가하는 데 적합합니다.

지름길 학습과 여정 학습으로 학습된 LLM.
O1 복제 보고서에서 발췌한 주석이 달린 그림, https://arxiv.org/abs/2410.18982

**10.3 증류(Distillation) -- 빠른 해결책?**

한 달 뒤, Huang 연구진은 "O1 복제 여정 -- 파트 2: 단순 증류를 통한 O1-preview 능가, 큰 진전인가 쓰디쓴 교훈인가?(O1 Replication Journey -- Part 2: Surpassing O1-preview through Simple Distillation, Big Progress or Bitter Lesson?, 2024년 11월)"라는 제목의 두 번째 보고서를 내놓았습니다. 이 보고서에서는 증류(distillation) 기법을 활용했습니다. 이는 o1 모델에서 사고 과정(thought processes)을 정교한 프롬프트(prompting)를 통해 추출한 다음, 이를 바탕으로 동등한 성능을 달성하도록 다른 모델을 학습시키는 방식입니다. 증류는 대규모의 고성능 "교사(teacher)" 모델의 지식을 더 작고 효율적인 "학생(student)" 모델로 이전하는 방법으로, 비용 효율적인 모델 배포를 가능하게 합니다. 이 기사가 길기 때문에 자세한 내용은 다루지 않겠지만, 장기 사고 데이터(long-thought data) 수집의 비용 상충 관계(cost trade-offs)를 요약한 해당 논문의 흥미로운 그림을 공유하고 싶었습니다. 복잡한 사고 과정을 담은 데이터를 수집하는 것은 시간과 자원이 많이 소모되기 때문에, 증류는 이러한 고비용 데이터를 직접 학습하는 것의 대안이 될 수 있습니다. 그들은 이 증류 접근 방식으로 o1-preview 및 o1-mini와 동등한 매우 좋은 성능을 얻었습니다. 그러나 이러한 실험과 함께 연구자들은 이 접근 방식에 비추어 연구 현황에 대한 흥미롭고 중요한 생각도 공유했으며, 이는 다음 섹션에서 요약하겠습니다.

**10.4 AI 연구의 현황**

파트 2 보고서에서 중점적으로 다룬 내용 중 하나는 "단순 증류의 쓰디쓴 교훈(Bitter Lesson of Simple Distillation)"이었습니다. 물론 증류(distillation)는 실제에서 잘 작동하지만, 진전을 이끄는 원동력은 아닙니다. 최상의 경우, 증류를 사용하면 기존의 상위 모델(upstream model) 성능과 일치시킬 수 있을 뿐입니다(새로운 성능 기록을 세우는 것은 아닙니다). 이는 AI 연구가 단순히 "무엇이 작동하는가(what works)"를 찾는 데 그치지 않고 "어떻게 작동하는가(how it works)"에 대한 깊이 있는 이해를 추구해야 한다는 중요한 메시지를 담고 있습니다. 아래는 현재 상황에 대한 경고로 작용할 수 있는 논문에서 발췌한 세 가지 인용문입니다:

> "‘어떻게 작동하는가’에서 ‘무엇이 작동하는가’로의 이러한 변화는 연구 사고방식의 근본적인 변화를 나타내며, 이는 해당 분야의 미래 혁신 역량에 광범위한 영향을 미칠 수 있습니다."
>
> "이러한 제1원리 사고(first-principles thinking)의 침식은 과학적 혁신의 바로 그 기반을 약화시키기 때문에 특히 우려됩니다."
>
> "빠른 결과물을 내야 한다는 압박은 더 깊은 기술적 조사의 가치를 가릴 수 있으며, 학생들은 더 도전적이고 근본적인 연구 방향을 추구하는 것을 단념하게 될 수 있습니다."

필자의 개인적인 견해로는, 여전히 학술 연구실(academic labs, 종종 산업과의 협력을 통해서도)에서 수많은 탁월하고 중요한 아이디어들이 샘솟고 있으며, 이들이 실제적으로 유용하고 상당한 영향력을 지닐 수 있다고 봅니다. (필자가 좋아하는 몇 가지는 LoRA와 DPO입니다.) 문제는 많은 유망한 아이디어들이 대규모로 테스트되지 못한다는 것입니다. 대학은 보통 이를 위한 막대한 자원을 가지고 있지 않기 때문입니다. 이러한 자원 격차는 학계의 혁신이 실제 적용으로 이어지는 데 큰 장벽으로 작용합니다. 완벽한 해결책이 무엇인지는 확실하지 않으며, 기업들이 자신들의 영업 비밀(trade secrets)을 그냥 공개할 수 없다는 것도 알고 있습니다. 하지만 기업들이 학술 논문의 아이디어를 사용하게 될 때마다 이를 공개적으로 인정해 준다면 정말 도움이 될 것입니다. 그러한 인정은 자신의 작업을 자유롭게 제공하는 연구자들에게 동기를 부여하고 보상하는 데 큰 도움이 됩니다. 또한, 실제로 무엇이 작동하는지 알아냄으로써 분야를 발전시키는 데 기여합니다. 학계와 산업계의 건설적인 협력 모델 구축은 AI 연구의 지속 가능한 발전을 위해 필수적입니다.

**10.5 o1(및 o3)의 관점에서 본 LLM의 미래**

"O1 복제 여정(O1 Replication Journey)" 논문이 o1의 근간을 이루는 정확한 메커니즘을 완벽하게 재현했을까요? 아마도 그렇지 않을 것입니다. 하지만 여전히 더 나은 결과를 달성하는 데 도움이 될 아이디어로 가득 찬 가치 있는 읽을거리입니다. 필자는 o1 및 o3와 같은 "장기 사고(long-thought)" 모델이 LLM 연구에서 계속해서 핵심적인 역할을 할 것이라고 믿습니다. 이 모델들은 운영 비용이 더 높지만, 추론 작업(reasoning tasks) 성능에 있어서는 사실상 황금 표준(gold standard)이자 최고 수준(upper limit for performance)을 대표합니다. 그러나 비용이 더 높기 때문에 o1 유형 모델이 모든 상황에서 항상 최선의 선택은 아닙니다. 문법 수정(grammar fixes)이나 번역(translations)과 같은 더 간단한 작업의 경우, 추론 중심 모델(reasoning-heavy model)이 필요하지 않을 것입니다. 궁극적으로 비용과 유용성(utility)의 균형을 맞추는 문제입니다. 우리는 예산, 지연 시간(latency) 및 기타 요인에 따라 작업에 적합한 LLM을 선택하는, 즉 "적재적소(right tool for the right job)"의 원칙을 적용해야 합니다. 이러한 모델 전문화(model specialization)의 중요성은 앞으로 더욱 커질 것입니다.

Ahead of AI는 독자 지원 출판물입니다. 새로운 게시물을 받고 제 작업을 지원하려면 무료 또는 유료 구독자가 되는 것을 고려해 보세요.
구독

---

**11. 11월: 정밀도(precision)를 위한 LLM 스케일링 법칙(LLM scaling laws)**

본래 필자는 Allen AI의 "Tulu 3: 개방형 언어 모델 사후 학습의 한계 확장(Pushing Frontiers in Open Language Model Post-Training)" 논문을 선정하려 했습니다. 이 논문에는 DPO 대 PPO의 절제 연구(ablation studies)를 포함한 라마 사후 학습(Llama post-training) 방법과 레시피에 대한 상세한 설명과, 보상 모델(reward model) 대신 쉽게 정답(ground truth answer)을 생성할 수 있는 검증 가능한 쿼리(verifiable queries, 수학 및 코드 질문 등)를 사용하는 검증 가능한 피드백을 통한 강화 학습(reinforcement learning with verifiable feedbacks)이라는 새로운 선호도 정렬(preference alignment) 방법이 포함되어 있었기 때문입니다. 하지만 내부적인 논의 끝에, 궁극적으로 Kumar와 동료들이 발표한 "정밀도를 위한 스케일링 법칙(Scaling Laws for Precision)" 논문(2024년 11월)을 선택하기로 결정했습니다. 이 연구는 2022년 "컴퓨팅 최적 대규모 언어 모델 학습(Training Compute-Optimal Large Language Models)" 논문에서 제시된 친칠라 스케일링 법칙(Chinchilla scaling laws)에 대한 필수적인 최신 정보를 제공하며, 이 법칙은 사전 학습(pretraining) 시 컴퓨팅 최적 LLM 매개변수 수(compute-optimal LLM parameter counts)와 데이터셋 규모(dataset sizes)를 산정하는 지침으로 활용됩니다. 요약하자면, "정밀도 스케일링 법칙(Precision Scaling Laws)" 논문(2024년 11월)은 친칠라의 스케일링 법칙을 확장하여 최근 몇 년간 매우 인기를 얻고 있는 저정밀도 설정(low-precision settings, 16비트 이하)에서의 학습(training) 및 추론(inference)을 설명합니다. 저정밀도 학습은 메모리 사용량과 계산 비용을 크게 줄여, LLM을 자원 제약이 있는 환경에서도 효율적으로 배포할 수 있게 합니다. 예를 들어, 이 논문은 다양한 저정밀도 및 양자화(quantization) 관련 관찰을 단일 함수 형식(single functional form)으로 통합하여 저정밀도 학습(low-precision training)과 사후 학습 양자화(post-training quantization) 모두에서 추가되는 손실(added loss)을 예측합니다.

**11.1 친칠라 스케일링 법칙(Chinchilla scaling laws) 복습**

2022년 "컴퓨팅 최적 대규모 언어 모델 학습(Training Compute-Optimal Large Language Models)" 논문에서 제시된 초기 친칠라 스케일링 법칙은 LLM의 매개변수 수(parameter counts, N)와 데이터셋 규모(dataset sizes, D)가 검증 손실(validation loss)에 미치는 복합적인 영향을 모델링하며, LLM 및 학습 데이터셋의 크기 결정에 중요한 지침으로 활용됩니다. 경험 법칙으로, 데이터셋 크기 D와 매개변수 수 N 사이의 최적의 절충점(고정된 컴퓨팅 예산(compute budget)이 있을 때)은 대략 D/N ≈ 20입니다. 이 데이터-매개변수 비율은 동일한 총 학습 비용(total training cost)에서 다른 비율보다 낮은 검증 손실을 산출하기 때문에 종종 "친칠라 최적(Chinchilla-optimal)"이라고 불립니다. 이 법칙은 모델 설계자들이 제한된 컴퓨팅 자원 내에서 가장 효율적인 성능을 달성하기 위한 기준점을 제공했습니다. 그러나 많은 현대적인 예외가 있습니다. 예를 들어, 라마 3(Llama 3) 팀은 이전에 논의된 바와 같이 15조 개의 토큰(tokens)으로 학습했으며, 80억 개 매개변수 버전의 경우 15,000,000,000,000 ÷ 8,000,000,000 = 1,875가 됩니다. 이는 친칠라 최적 비율에서 크게 벗어나는 수치입니다. 필자의 생각에는 정확한 데이터-매개변수 비율보다 더 중요한 것은 모델과 데이터셋 크기가 비례적으로 확장되어야 한다는 점입니다. 즉, 데이터의 양이 늘어남에 따라 모델의 용량도 함께 확장되어야 한다는 원칙은 여전히 유효합니다.

**11.2 저정밀도 학습(Low-precision training)**

저정밀도 스케일링 법칙(low-precision scaling laws)을 더 자세히 논의하기(또는 요약하기) 전에, 일반적으로 LLM(또는 심층 신경망(deep neural network)) 가중치(weights)에 대한 다양한 숫자 정밀도 형식(numeric precision formats)에 대한 아주 짧은 입문서로 시작하겠습니다. 제가 아는 한, GPT 2 & 3 및 라마 2 & 3 모델 학습에 사용된 정밀도 형식은 비교를 위해 다음과 같습니다:

*   **Float32**는 범위와 정밀도 간의 이상적인 균형을 제공함으로써, 심층 신경망 학습 분야에서 광범위하게 채택되어 온 표준 32비트 부동 소수점 형식(floating-point format)이었습니다. Float32 미만의 모든 것은 오늘날 저정밀도(low-precision)로 간주됩니다(비록 "낮음"의 정의가 대규모 언어 모델의 "대규모"와 유사하게 움직이는 목표이기는 하지만).
*   **Float16**, 또는 반정밀도(half-precision)는 단 16비트를 사용하여 메모리를 절약하고 계산 속도를 높이지만, 더 좁은 동적 범위(dynamic range)를 제공합니다. 이는 특히 모델이 매우 크거나 작은 값을 처리할 때 오버플로(overflow)나 언더플로(underflow) 문제를 일으킬 수 있습니다.

32비트 및 16비트 부동 소수점 정밀도 비교

*   **Bfloat16(브레인 플로트 16(brain float 16))** 역시 16비트 형식에 속하지만, float16의 특정 정밀도를 더 큰 지수(exponent)와 맞바꿈으로써 아주 크거나 아주 작은 숫자들을 더욱 효과적으로 나타낼 수 있도록 설계되었습니다. 이로 인해 bfloat16은 딥러닝 애플리케이션에서 특히 학습 과정 중 그래디언트(gradients)가 매우 커지거나 작아지는 상황에서 숫자 오버플로(numeric overflow) 또는 언더플로(underflow)를 피하는 데 도움이 될 수 있습니다. 하지만 낮은 정밀도로 인해 여전히 반올림 오류(rounding errors)가 발생할 수 있습니다.

일반 16비트 부동 소수점과 인기 있는 16비트 브레인 부동 소수점 정밀도 비교

다양한 정밀도 형식과 LLM 모델 동작에 미치는 영향에 대해 더 자세히 알고 싶다면, 제 이전 기사인 "누락된 비트: 라마 2 가중치가 변경되었습니다(The Missing Bits: Llama 2 Weights Have Changed)"의 더 긴 소개를 좋아하실 것입니다. 또한 저는 32비트 및 16비트 형식만 보여주고 있지만, 현재 학습을 위한 더 낮은 정밀도, 예를 들어 라마 3 논문에서 (실험적으로) 언급된 8비트 형식에 대한 경쟁이 진행 중이라는 점에 유의하십시오. (12월 26일에 출시된 DeepSeek-v3 모델은 8비트 부동 소수점 정밀도로 완전히 사전 학습되었습니다.) 심지어 4비트 또는 바이너리(binary) 정밀도에 대한 연구도 활발히 진행되고 있으며, 이는 미래 LLM의 효율성과 접근성을 혁신할 잠재력을 가지고 있습니다.

**11.3 정밀도 스케일링 법칙(Precision scaling laws) 요점**

이 논문은 내용이 방대하고 흥미로우므로, 독자 여러분께서는 전체를 일독해 보시기를 권합니다. 그러나 핵심 요점을 말하자면, 연구자들은 원래 친칠라 스케일링 법칙(Chinchilla scaling laws)에 "정밀도(precision)" 요소 P를 추가하여 확장했습니다. 구체적으로, 그들은 모델 매개변수 수(parameter count) N을 정밀도가 감소함에 따라 줄어드는 "유효 매개변수 수(effective parameter count)"로 재해석합니다. 이는 모델의 실제 용량이 정밀도에 따라 달라질 수 있음을 의미합니다. (수학 공식은 논문을 참조하십시오.) 또한, 사후 학습 양자화(post-training quantization)가 모델 성능을 어떻게 저하시키는지 포착하기 위한 추가 항을 추가했습니다. 양자화(quantization)는 모델의 가중치를 낮은 비트 정밀도로 변환하여 메모리 사용량과 계산 속도를 최적화하는 기술입니다. (양자화(quantization)에 대한 소개를 작성하지 않았다는 것을 알고 있지만, 이 기사의 길이가 이미 너무 길기 때문에 다음 기회로 미루어야 할 것 같습니다.) 아래 그림은 더 많은 사전 학습(pretraining) 데이터가 항상 더 좋은 것은 아니며, 매우 작은 정밀도(int3)로 학습한 후 모델이 양자화될 경우 실제로 해로울 수 있음을 잘 보여줍니다. 필자는 이 점이 매우 흥미로웠습니다.

다양한 사후 양자화 형식에 대한 검증 손실(validation loss)에 대한 더 많은 학습 데이터의 영향

따라서 위 그림을 통해 도출할 수 있는 결론은, 점진적으로 더 많은 데이터로 훈련된 모델들(라마 3(Llama 3)와 같은)은 과도한 데이터로 인해 "과적합(overtrained)" 상태가 될 수 있으며, 이로 인해 학습 이후 저정밀도 형식으로 양자화(quantization)하기가 더욱 난해해질 수 있다는 것입니다. 이는 데이터의 양뿐만 아니라 데이터의 특성과 양자화에 대한 민감성까지 고려해야 함을 시사합니다.

**11.4 2025년의 모델 스케일링 법칙(Model scaling laws)**

정밀도 스케일링 법칙(Precision Scaling Laws) 연구는 친칠라 스케일링 법칙(Chinchilla scaling laws)에 필수적인 최신 정보를 제공할 뿐만 아니라, 2025년 인공지능 분야의 주요 과제들에 대한 흥미로운 통찰을 제시합니다. 즉, 라마-3(LLaMA-3)와 같은 모델이 더 큰 데이터셋으로 학습될수록, 성능 손실 없이 INT3와 같은 저정밀도 형식으로 양자화(quantize)하기가 더 어려워질 수 있다는 것입니다. 이러한 발견은 "데이터는 많을수록 좋다"는 기존의 사고방식을 재검토하게 하며, 데이터셋 규모와 효율적인 추론(efficient inference) 간의 현실적인 제약 조건 속에서 균형점을 찾아야 할 필요성을 역설합니다. 이는 또한 하드웨어 최적화(hardware optimization)를 추진하는 데 중요한 통찰력입니다. 특히 엣지 디바이스(edge devices)와 같은 자원 제약이 있는 환경에서 LLM을 구동하기 위해서는 모델의 효율성이 필수적이기 때문입니다. 이러한 스케일링 법칙 연구에서 종종 간과된다고 생각하는 측면 중 하나는 데이터셋의 품질입니다. 필자는 사전 학습 데이터(pretraining data)의 특성과 큐레이션(curation)이 모델 성능과 효율성에 상당한 영향을 미칠 수 있다고 생각합니다. (이에 대한 자세한 내용은 아래 Phi-4 논의에서 다루겠습니다.) 데이터 중심 AI(data-centric AI) 접근 방식의 중요성이 더욱 강조되는 이유이기도 합니다.

---

**12. 12월: Phi-4와 합성 데이터(Synthetic Data)로부터의 학습**

2024년 하반기에는 크리스마스에 공개된 인상적인 DeepSeek-V3를 비롯하여 다수의 주목할 만한 모델들이 등장했습니다. 가장 큰 모델 출시는 아닐지라도, 궁극적으로 필자는 마이크로소프트의 Phi-4 기술 보고서(Phi-4 Technical Report)를 선택하기로 결정했습니다. 이 보고서가 합성 데이터(synthetic data) 사용에 대한 흥미로운 통찰력을 제공하기 때문입니다. 합성 데이터는 실제 데이터가 부족하거나 개인 정보 보호 문제로 인해 사용이 어려울 때, 인공적으로 생성되는 데이터를 의미하며, LLM 학습의 새로운 지평을 열고 있습니다.

**12.1 Phi-4 성능**

2024년 12월, Abdin 연구팀이 공개한 Phi-4 기술 보고서(Phi-4 Technical Report)는 마이크로소프트의 최신 140억 개 매개변수(parameter) 공개 가중치 LLM(open-weight LLM)의 훈련(training) 과정을 상세히 기술합니다. Phi-4를 특히 흥미롭게 만드는 점은 주로 GPT-4o가 생성한 합성 데이터(synthetic data)로 학습되었다는 것입니다. 이는 고성능 대형 모델이 "교사(teacher)" 역할을 하여 더 작고 효율적인 "학생(student)" 모델을 훈련시키는 지식 증류(knowledge distillation)의 한 형태로 볼 수 있습니다. 벤치마크(benchmarks) 결과에 따르면, 이 모델은 주로 비합성 데이터로 훈련된 이전 모델인 Phi-3를 포함하여 동급 규모의 다른 LLM들을 능가하는 성능을 나타냈습니다. 이는 합성 데이터의 잠재력을 명확히 보여주는 사례입니다.

유사하거나 다른 크기의 다른 모델과 비교한 phi-4의 성능 (phi-4 논문에서 발췌한 주석이 달린 표, https://arxiv.org/abs/2412.08905)

위 표에 나와 있듯이, 모델이 SimpleQA에서 왜 더 낮은 성능을 보이는지 완전히 확신할 수는 없습니다. 하지만 한 가지 가능한 설명은 SimpleQA가 2024년 10월 30일에 출시된 비교적 새로운 벤치마크라는 것입니다. OpenAI가 평가 스위트(evaluation suite)의 일부로 개발했기 때문에 GPT-4o의 학습 데이터(training data)에 포함되지 않았거나 웹 크롤링된 데이터셋(web-crawled datasets)에 통합되지 않았을 수 있습니다. 더욱이, GPT-4o가 이 평가를 위한 합성 데이터를 생성하는 데 사용되었기 때문에, 어떤 모델도 학습 중에 SimpleQA를 접하지 못했을 것입니다. 또는 phi-4가 다른 벤치마크에 과적합(overfitting)되었을 수 있으며, 이는 이전에 본 적 없는 SimpleQA 데이터셋에서 상대적으로 낮은 성능을 보이는 이유를 설명할 수 있습니다. 어쨌든, 이것은 단지 필자의 가설일 뿐입니다. 새로운 벤치마크에 대한 모델의 성능은 종종 모델의 진정한 일반화 능력을 평가하는 중요한 척도가 됩니다.

**12.2 합성 데이터(Synthetic data) 학습**

이 논문에서 제시된 일부 절제 연구(ablation studies)를 요약하기 전에 데이터셋 구성(dataset composition)을 살펴보겠습니다.

phi-4 학습을 위한 데이터셋 혼합 (phi-4 논문에서 발췌한 주석이 달린 표, https://arxiv.org/abs/2412.08905).

연구진은 합성 데이터(synthetic data)가 대체로 유익함에도 불구하고, 오직 합성 데이터만으로 훈련된 모델은 지식 기반 벤치마크(knowledge-based benchmarks)에서 저조한 성능을 보인다는 점을 확인했습니다. 필자에게 이것은 다음과 같은 질문을 제기합니다: 합성 데이터에 충분한 지식 특정 정보가 부족한가, 아니면 환각(hallucinations)으로 인한 것과 같은 사실 오류(factual errors)의 비율이 더 높은가? 합성 데이터의 생성 방식과 품질이 모델의 사실적 정확성에 직접적인 영향을 미칠 수 있음을 시사합니다. 이와 동시에, 연구진은 합성 데이터에 대한 학습 에포크(training epochs) 수를 증가시키는 것이 단순히 더 많은 웹 데이터(web data)를 추가하는 것보다 성능 향상에 더 효과적임을 발견했습니다. 이는 아래 그림에 나와 있습니다. 즉, 합성 데이터의 양을 무작정 늘리기보다는, 기존 합성 데이터를 더 깊이 있게 반복 학습시키는 것이 더 효율적일 수 있다는 의미입니다.

다른 합성/웹 데이터셋 비율에 대한 모델 성능 비교. (phi-4 논문에서 발췌한 주석이 달린 그림, https://arxiv.org/abs/2412.08905).

요약하자면, 혼합된 데이터에서 합성 데이터의 과도한 비율은 지식 기반 성능에 부정적인 영향을 미칩니다. 이는 합성 데이터가 특정 패턴 학습에는 유리하지만, 광범위하고 깊이 있는 세계 지식을 전달하는 데는 한계가 있을 수 있음을 보여줍니다. 그러나 더 균형 잡힌 합성-웹 데이터 혼합 내에서는 합성 데이터셋에 대한 반복(iterations, 에포크(epochs)) 횟수를 늘리는 것이 유익합니다. 이는 합성 데이터가 실제 데이터와 적절히 결합될 때 시너지를 발휘하며, 모델의 학습 효율성을 높일 수 있음을 시사합니다.

**12.3 합성 데이터(synthetic data)의 미래 중요성**

Phi-4 기술 보고서는 합성 데이터의 활용에 대한 흥미로운 통찰을 제공하며, 이는 모델 사전 학습(pre-training)에 지대한 이점을 가져올 수 있음을 시사합니다. 특히 스케일링 법칙(scaling laws)이 모델 및 데이터셋 크기 모두에서 정체되고 있다고 알려져 있지만(라마 3(Llama 3) 논문에서는 아직 15조 토큰(token) 수준에서 수렴을 보지 못했다고 언급했지만), 연구자와 엔지니어들은 한계를 계속 확장하기 위한 대안적인 방법을 찾고 있습니다. 합성 데이터는 이러한 데이터 희소성(data scarcity) 문제를 해결하고, 특정 도메인에 특화된 고품질 데이터를 생성하는 효과적인 수단이 될 수 있습니다. 물론, 사전 학습(pre-training) 및 특히 사후 학습(post-training) 기술의 정교화 및 추가는 여전히 큰 변화를 가져올 요인 중 하나로 남을 것입니다. 그럼에도 불구하고, 필자는 합성 데이터의 사용이 a) 더 적은 데이터로 사전 학습된 기본 모델(pretrained base models)을 만들거나 b) 훨씬 더 나은 기본 모델(라마 3 데이터셋의 15조 토큰에 40%의 합성 데이터 토큰을 추가하는 것을 생각해 보세요)을 만드는 효과적인 방법으로 간주될 것이라고 생각합니다. 필자는 고품질 데이터의 활용을 전이 학습(transfer learning)과 유사한 맥락으로 이해합니다. 원시적이고 구조화되지 않은 인터넷 데이터로 모델을 사전 학습하고 사후 학습 중에 정제하는 대신, 고품질 모델(예: 이미 광범위한 정제를 거친 GPT-4o)이 생성한 (일부) 합성 데이터를 활용하는 것이 일종의 지름길(jumpstart) 역할을 할 수 있습니다. 다시 말해, 고품질 학습 데이터의 사용은 모델이 처음부터 더 효과적으로 학습할 수 있도록 할 수 있으며, 이는 학습 시간 단축과 성능 향상이라는 두 마리 토끼를 잡는 데 기여합니다.

---

**결론 및 전망**

이 연구 요약들이 독자 여러분께 유익했기를 바랍니다! 언제나 그렇듯이, 이 글은 당초 예상했던 것보다 분량이 늘어났습니다. 하지만 2025년에 대한 필자의 예측(또는 기대)에 대한 비교적 짧고 간결한 섹션으로 마무리하겠습니다. 인공지능 분야의 발전 속도는 경이로우며, 정확한 미래를 예측하는 것은 어렵지만, 몇 가지 분명한 추세와 기대는 존재합니다.

**멀티모달 LLM(Multimodal LLMs)**

지난해 필자는 대규모 언어 모델(LLM)이 점차 다중 모드(multimodal)로 진화할 것이라고 전망했습니다. 이제 모든 주요 독점 LLM 제공업체는 멀티모달(또는 최소한 이미지) 지원을 제공합니다. 따라서 이러한 변화는 이제 완전히 진행 중이며, 우리는 이를 향한 더 많은 오픈 소스(open-source) 노력을 보게 될 것입니다. 필자가 보고 읽은 바에 따르면, 멀티모달 논문이 급증한 것은 분명합니다. 아마도 오픈 소스 미세 조정(finetuning) 방법과 자료들이 뒤따를 것입니다. 물론, 대다수의 활용 사례에서는 텍스트 전용 기능만으로도 충분하며 앞으로도 그러할 것이고, 핵심적인 주안점은 o1 및 조만간 출시될 o3와 같은 더욱 정교한 추론 모델(reasoning models)을 개발하는 데 맞춰질 것이라고 주장할 수 있겠습니다. 오픈 소스 커뮤니티의 참여는 멀티모달 AI의 접근성을 높이고, 다양한 특화된 응용 분야를 개척하는 데 중요한 역할을 할 것입니다.

**계산 효율성(Computational efficiency)**

대규모 언어 모델(LLM)을 사전 학습(pretraining)하고 실제 적용하는 과정은 상대적으로 높은 비용을 수반합니다. DeepSeek-v3 모델을 학습시키는 데 GPU 대여 가격(GPU rental sticker prices)만으로도 5백만 달러가 들었을 것으로 추정되며(하이퍼파라미터 튜닝(hyperparameter tuning), 실패한 실행(failed runs) 및 인건비(personnel cost)는 포함되지 않음), 공식 메타 AI(Meta AI) 라마 3(Llama 3) 모델 카드에 따르면, 라마 3 4050억 개 매개변수 모델은 약 10배 더 많은 컴퓨팅(compute)을 사용했습니다(3084만 GPU 시간 대 266만 GPU 시간). 그렇기에 가까운 시일 내에 대규모 언어 모델(LLM)의 계산 효율성(computational efficiency)을 향상시키기 위한 더욱 혁신적인 방법론들이 등장할 것으로 예측됩니다. LLM을 효율적으로 만드는 기술의 인기 있는 예시(모두 학습 중에 적용되는 것은 아니지만)로는 전문가 혼합(mixture of experts, 제 파트 1 기사에서 논의됨), 라마 모델에서 발견되는 그룹화된 쿼리 어텐션(grouped-query attention) 등이 있습니다. 또 다른 흥미로운 점은 DeepSeek 모델에서 발견되는 다중 헤드 잠재 어텐션(multihead latent attention)을 사용하여 다중 헤드 어텐션(multihead attention)의 KV-캐싱(KV-caching)을 더 효율적으로 만드는 것입니다. 또 다른 흥미로운 최근 경로는 모델 입력(model input)을 대상으로 하는 것입니다. 예를 들어, 최근 제안된 바이트 잠재 트랜스포머(Byte Latent Transformer)는 바이트를 엔트로피 기반 패치(entropy-based patches)로 동적으로 인코딩하여 토큰화(tokenization) 없이 확장성(scalability)과 더 빠른 추론(inference)을 위한 컴퓨팅을 최적화함으로써 효율성을 향상시킵니다. 이러한 하드웨어 및 소프트웨어 최적화는 AI 연구의 지속 가능성과 상업적 활용성을 결정하는 핵심 요소가 될 것입니다.

**상태 공간 모델(State space models)**

올해 보고서에서 상태 공간 모델(state space models)이 다뤄지지 않았다는 점을 독자 여러분께서 인지하셨을 수도 있습니다. 이는 현재 필자의 초점이 주로 트랜스포머 기반 LLM(transformer-based LLMs)에 있기 때문입니다. 상태 공간 모델이 매우 흥미롭다고 생각하지만, 이 단계에서는 여전히 상당히 실험적인(experimental) 것으로 보입니다. 게다가, 트랜스포머 모델은 다양한 작업 영역에서 지속적으로 탁월한 성능(exceptional performance)을 입증하고 있어, 대체 모델을 고려할 필요성이 상대적으로 낮습니다. 그러나 그렇다고 해서 상태 공간 모델 분야에서 진전이 없었다는 의미는 아닙니다. 필자는 이 분야에서 흥미로운 논문들을 많이 보았습니다. 그리고 필자가 주목한 한 가지 흥미로운 추세는 이들이 이제 모두 트랜스포머 모델의 셀프 어텐션(self-attention)을 통합한 하이브리드 모델(hybrid models)이라는 것입니다. 예를 들어, "Jamba-1.5: 대규모 하이브리드 트랜스포머-맘바 모델(Hybrid Transformer-Mamba Models at Scale)", "라마 속 맘바: 하이브리드 모델 증류 및 가속화(The Mamba in the Llama: Distilling and Accelerating Hybrid Models)", 그리고 "삼바: 효율적인 무제한 컨텍스트 언어 모델링을 위한 단순 하이브리드 상태 공간 모델(Samba: Simple Hybrid State Space Models for Efficient Unlimited Context Language Modeling)" 등이 있습니다. 이러한 하이브리드 접근 방식은 상태 공간 모델의 긴 시퀀스 처리 효율성과 트랜스포머의 강력한 표현력을 결합하려는 시도입니다. 그런 의미에서 이들도 계산 비용이 더 많이 들고 있습니다. 트랜스포머 기반 LLM에 대한 효율성 조정(efficiency tweaks)과 상태 공간 모델에 어텐션을 추가하는 방식으로, 현재 추세가 계속된다면 아마도 중간 지점에서 만나게 될 것입니다. 하지만 확실히 주목할 만한 흥미로운 연구 분야입니다.

**스케일링을 통한 LLM 발전**

연말에는 인터넷상의 데이터 고갈로 인해 LLM 스케일링의 시대가 "종언을 고했다"는 논의가 제기되기도 했습니다. 이 논의는 Ilya Sutskever(OpenAI 공동 설립자이자 GPT 논문 공동 저자)의 NeurIPS 강연에서 비롯되었으나, 필자는 올해 해당 컨퍼런스에 불참하여 상세한 내용은 파악하지 못했습니다. 어쨌든, 인터넷은 기하급수적으로(exponentially fast) 빠르게 성장하기 때문에 흥미로운 지점입니다. 필자는 "매일 15.87테라바이트의 데이터"가 증가한다는 자료를 찾을 수 있었습니다. 물론, 모든 데이터가 텍스트이거나 LLM 학습에 유용한 것은 아니라는 문제가 있습니다. 그러나 Phi-4에서 보았듯이, 학습 데이터만으로도 큰 도약을 이룰 수 있도록 데이터 큐레이션(data curation) 및 정제(refinement)에는 여전히 많은 기회가 있습니다. 단순히 데이터의 양을 늘리는 것을 넘어, 데이터의 품질과 가치 밀도(value density)를 높이는 것이 중요해지고 있습니다. 필자는 데이터 스케일링을 통한 수확 체감(diminishing returns of scaling)에는 동의합니다. 우리는 아마도 정체(plateauing)를 향해 가고 있기 때문에 얻는 이득이 더 작아질 것으로 예상합니다. 하지만 이는 다른 개선 기회를 가져오므로 나쁜 일은 아닙니다. 필자가 미래에 많은 이득이 올 것으로 예상하는 한 가지 주목할 만한 영역은 사후 학습(post-training)입니다. 지난여름 "새로운 LLM 사전 학습 및 사후 학습 패러다임(New LLM Pre-training and Post-training Paradigms)" 기사에서 썼듯이, 최근 LLM 출시와 함께 이 분야의 발전을 이미 맛보았습니다.

**2025년에 기대하는 것**

필자는 올해 다양한 라마 모델(3, 3.1, 3.2)을 직접 다루고 (재)구현하는 과정에서 큰 즐거움을 얻었습니다. 필자는 라마 4(Llama 4)의 출시를 손꼽아 기다리고 있으며, 개인용 노트북이나 저렴한 클라우드 GPU에서도 쉽게 실험할 수 있는 소형의 편리한 형태로도 제공되기를 희망합니다. 또한, 일반적인 챗봇을 생성하는 것보다 특수 목적 모델 미세 조정(special-purpose model finetuning)에 더 많이 실험하고 싶은 해이기도 합니다(이 분야는 이미 상당히 혼잡합니다). 다양한 코드 및 수학 모델(최근 Qwen 2.5 Coder 및 Qwen 2.5 Math가 떠오르는데, 불행히도 이 보고서에서는 아직 다룰 기회가 없었습니다)에서 그 일부를 보았습니다. 2025년은 인공지능의 윤리적 사용과 책임감 있는 개발이 더욱 중요해지는 한 해가 될 것입니다. 어쨌든, 2025년은 또 다른 흥미롭고 빠르게 변화하는 한 해가 될 것이므로, 이 희망 목록과 계획을 계속 이어갈 수 있을 것입니다! 확실히 지루하지는 않을 것입니다!

이 간행물은 필자의 개인적인 열정으로 시작된 프로젝트입니다. 저를 응원해 주시고자 하는 독자분들께서는 필자의 저서인 "처음부터 대규모 언어 모델 구축하기(Build a Large Language Model (From Scratch))"를 구매해 주시면 큰 힘이 될 것입니다. (이 책은 다른 곳에서는 찾을 수 없는 상세한 수준으로 LLM이 어떻게 작동하는지 설명하므로 많은 것을 얻으실 수 있을 것이라고 확신합니다.) 이 책은 독자 여러분이 LLM의 근본적인 원리를 이해하고 직접 구축하는 데 필요한 실질적인 지식을 제공할 것입니다.
"처음부터 대규모 언어 모델 구축하기(Build a Large Language Model (From Scratch))"는 현재 아마존에서 구매 가능합니다.
만약 책을 읽으신 후 잠시 시간을 할애해 주실 수 있다면, 간략한 서평(brief review)을 남겨주시면 진심으로 감사하겠습니다. 이는 저자들에게 실질적인 도움이 됩니다! 독자 여러분의 성원에 깊이 감사드립니다!
구독