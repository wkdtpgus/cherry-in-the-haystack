2025년의 활기찬 시작을 축하드립니다! 새해가 밝아오면서, 2024년 인공지능 연구 분야의 주요 성과들을 담은 보고서의 초고와 두 번째 부분이 드디어 완성되었습니다. 이 보고서는 전문가 혼합 모델(Mixture-of-Experts, MoE)부터 정밀한 대규모 언어 모델(LLM) 스케일링 법칙(Scaling Laws)에 이르기까지 다양한 핵심 주제들을 다룹니다. 본 문서는 연재물의 2부로, 2024년 7월부터 12월까지의 후반기 동향을 집중적으로 조명합니다. 1월부터 6월까지의 전반기 내용은 여기에서 확인하실 수 있습니다. 언급된 내용의 선정 기준은 전적으로 개인적인 판단에 기반하며, 한 해 동안 필자에게 가장 깊은 인상을 남긴 발견들을 중심으로 구성되었습니다. 또한, 단순히 LLM 모델 출시에만 갇히지 않고 다양한 분야의 발전을 소개하고자 노력했습니다. 2025년에도 좋은 일만 가득하시길 바라며, 즐거운 독서가 되시기를 기원합니다!

**7. 7월: 라마 3 모델 생태계(The Llama 3 Ecosystem)**

메타 AI(Meta AI)에서 공개한 라마 3(Llama 3) 모델과 관련 연구 자료들은 이미 많은 독자들에게 친숙할 것입니다. 이 모델군이 지닌 막대한 중요성과 광범위한 활용성을 고려하여, 7월의 주요 내용은 Grattafiori 외 연구진이 2024년 7월에 발표한 "라마 3 모델 무리(The Llama 3 Herd of Models)" 논문에 할애하고자 합니다. 라마 3 제품군에서 두드러지는 특징은 이전 세대인 라마 2(Llama 2)에 비해 사전 학습(Pre-training) 및 사후 학습(Post-training) 과정의 정교함이 한층 강화되었다는 점입니다. 이러한 경향은 라마 3뿐만 아니라 Gemma 2, Qwen 2, 애플(Apple)의 파운데이션 모델(Foundation Models) 등 다른 대규모 언어 모델들에서도 공통적으로 관찰되는 현상이며, 이는 필자가 수개월 전 작성했던 "새로운 LLM 사전 학습 및 사후 학습 패러다임(New LLM Pre-training and Post-training Paradigms)"이라는 글에서도 상세히 설명된 바 있습니다. 라마 3의 등장은 단순히 성능 향상을 넘어, 오픈 소스(Open-source) LLM 커뮤니티에 새로운 기준을 제시하며 광범위한 연구와 상업적 활용을 촉진했습니다.

**7.1 라마 3 아키텍처(Architecture) 개요**

라마 3(Llama 3)는 초기에는 80억 개 및 700억 개 매개변수(Parameter) 규모로 세상에 나왔지만, 개발팀은 지속적인 개선 작업을 통해 라마 3.1, 3.2, 3.3 버전을 연이어 선보였습니다. 각 버전별 모델 크기는 다음과 같이 정리할 수 있습니다:

Llama 3 (2024년 4월)
80억 개 매개변수
700억 개 매개변수

Llama 3.1 (2024년 7월, 논문에서 상세히 다룸)
80억 개 매개변수
700억 개 매개변수
4050억 개 매개변수

Llama 3.2 (2024년 9월)
10억 개 매개변수
30억 개 매개변수
110억 개 매개변수 (시각 기능 활성화)
900억 개 매개변수 (시각 기능 활성화)

Llama 3.3 (2024년 12월)
700억 개 매개변수

전반적으로 라마 3의 구조는 라마 2(Llama 2)와 매우 흡사합니다. 주요한 변화는 더 확장된 어휘(Vocabulary) 집합과, 보다 작은 모델 변형들을 위한 그룹화된 쿼리 어텐션(Grouped-Query Attention)의 도입에서 찾을 수 있습니다. 이러한 차이점들은 아래 첨부된 그림에 요약되어 있습니다.
제 "처음부터 대규모 언어 모델 구축하기(Build a Large Language from Scratch)" 책의 보너스 자료에서 발췌한 라마 2 대 3 비교
만약 아키텍처의 세부 사항에 관심이 있다면, 해당 모델을 처음부터 직접 구현하고 미리 학습된 가중치(Pretrained Weights)를 불러와 유효성 검사(Sanity Check)를 수행하는 것이 훌륭한 학습 경험이 될 것입니다. 필자는 GPT-2 모델을 라마 2, 라마 3, 라마 3.1, 그리고 라마 3.2로 변환하는 과정을 담은 초기 구현(From-Scratch Implementation) GitHub 저장소를 운영하고 있습니다.

**7.3 라마 3 학습(Training) 과정**

라마 2(Llama 2)와 비교했을 때 또 다른 중요한 진전은 라마 3(Llama 3)가 이제 15조 개에 달하는 토큰(Tokens)으로 훈련되었다는 사실입니다.
다양한 모델의 학습 세트(Training Set) 규모 비교.
사전 학습(Pre-training) 과정은 이제 다단계(Multi-staged) 방식으로 진행됩니다. 해당 논문은 주로 라마 3.1(Llama 3.1)에 초점을 맞추고 있으며, 이해를 돕기 위해 사전 학습 기술들을 아래 그림에 간략히 요약했습니다.
라마 3.1 사전 학습에 활용된 기술 요약.
사후 학습(Post-training) 단계에서는 강화 학습 기반의 정책 최적화(RLHF-PPO) 방식에서 직접 선호도 최적화(DPO) 방식으로의 전환이 주목할 만한 변화입니다. 이러한 방법론들 또한 아래 그림에 요약되어 있습니다.
라마 3.1 사전 학습에 활용된 기술 요약.
이 글에서 다룰 논문이 아직 5개 더 남아있으므로, 추가적인 세부 사항이나 다른 모델들과의 비교는 필자의 이전 글인 "새로운 LLM 사전 학습 및 사후 학습 패러다임(New LLM Pre-training and Post-training Paradigms)"을 참고해 주시기 바랍니다.
Ahead of AI는 독자 지원 출판물입니다. 새로운 게시물을 받고 제 작업을 지원하려면 무료 또는 유료 구독자가 되는 것을 고려해 보세요.
구독

**7.4 멀티모달(Multimodal) 라마의 등장**

라마 3.2(Llama 3.2) 모델은 멀티모달(Multimodal) 기능을 지원하며 출시되었습니다. 하지만 필자의 경험상, 이러한 모델들이 실제 환경에서 광범위하게 활용되거나 활발히 논의되는 경우는 드물었습니다. 멀티모달 기술에 대한 심층적인 탐구는 본 보고서의 9월 섹션에서 다시 다룰 예정입니다.

**7.5 라마 3의 영향력과 활용 양상**

라마 3(Llama 3)가 공개된 지 반년 이상이 흘렀지만, 라마 모델들은 여전히 가장 널리 알려지고 활용되는 공개 가중치(Open-weight) 대규모 언어 모델(LLM) 중 하나로 자리매김하고 있습니다(특정 출처에 기반한 언급은 아니지만, 개인적인 관찰에 따른 것입니다). 이 모델들은 상대적으로 이해하기 쉽고 사용이 편리하다는 장점을 지닙니다. 이러한 인기의 원인은 라마라는 브랜드의 높은 인지도와 함께, 다양한 일반적인 작업에서 일관되고 뛰어난 성능을 제공하며, 미세 조정(Finetune)하기 용이하다는 점들이 복합적으로 작용한 결과일 것입니다. 메타 AI(Meta AI)는 또한 라마 3 모델을 3.1, 3.2, 그리고 현재 3.3 버전으로 꾸준히 발전시켜 출시함으로써 그 영향력을 지속적으로 유지해 왔습니다. 이처럼 다양한 버전들은 온디바이스(On-device) 환경(10억 개 매개변수)부터 고성능 애플리케이션(4000억 개 매개변수)에 이르기까지 폭넓은 활용 시나리오를 충족시키기 위해 다양한 규모를 포괄합니다. 현재 Olmo 2, Qwen 2.5, Gemma 2, Phi-4 등 많은 경쟁력 있는 오픈 소스(Open-source) 및 공개 가중치 LLM들이 존재하지만, 필자는 라마가 앤트로픽 클로드(Anthropic Claude), 구글 제미니(Google Gemini), 딥시크(DeepSeek) 등과의 경쟁 속에서도 챗GPT(ChatGPT)가 인기를 유지한 것처럼, 대다수 사용자에게 기본 모델로서의 지위를 유지할 것이라고 확신합니다. 개인적으로는 2025년 언젠가 공개되기를 기대하는 라마 4(Llama 4)에 대한 기대가 매우 큽니다.

**8. 8월: 추론 시점 연산(Inference-Time Compute) 확장을 통한 LLM 성능 개선**

이번 달 필자가 선정한 연구 논문은 2024년 8월에 발표된 "LLM 테스트 시점 연산을 최적으로 확장하는 것이 모델 매개변수를 확장하는 것보다 더 효율적일 수 있다(Scaling LLM Test-Time Compute Optimally can be More Effective than Scaling Model Parameters)"입니다. 이 문헌은 추론 과정(Inference Time), 즉 모델이 실제 환경에 배포(Deployment)되어 사용될 때 대규모 언어 모델(LLM)의 반응 품질을 향상시키는 방법에 대한 매우 흥미로운 통찰력을 제공하는, 잘 쓰여지고 상세한 연구이기 때문입니다. 이는 단순히 모델 크기를 키우는 것 외에 성능을 최적화할 수 있는 새로운 접근 방식을 제시합니다.

**8.1 더 많은 테스트 시점 연산(Test-Time Computation)으로 출력 향상**

이 논문의 핵심적인 질문은 증가된 테스트 시점 연산 자원을 활용하여 LLM의 결과물을 개선할 수 있는지, 그리고 그 방법은 무엇인지를 탐구하는 것입니다. 대략적인 비유를 들자면, 사람이 복잡한 과제를 해결할 때 충분한 사고 시간을 제공하면 더 나은 답변을 내놓을 수 있는 것과 유사합니다. 마찬가지로, LLM 역시 답변을 생성하는 데 더 많은 시간이나 컴퓨팅 자원이 주어진다면, 더 우수한 결과물을 만들어낼 수 있을 것이라는 가설입니다. 보다 기술적인 관점에서, 연구자들은 추론(Inference) 단계에서 추가적인 컴퓨팅 자원이 투입될 경우, 모델이 학습된 상태보다 얼마나 더 나은 성능을 발휘할 수 있는지를 밝히고자 했습니다. 나아가, 연구팀은 고정된 연산 예산(Compute Budget)이 주어졌을 때, 해당 예산을 모델의 추가적인 사전 학습(Pre-training)에 사용하는 것보다 테스트 시점에서 더 많은 연산을 투입하는 것이 결과 개선에 더 효과적인지 여부도 분석했습니다. 이 부분에 대해서는 추후 더 자세히 설명할 것입니다.

**8.2 테스트 시점 연산(Test-Time Computation) 기법 최적화**

이 논문은 테스트 시점 연산을 증대시키고 개선하는 다양한 방법론들을 매우 상세하게 설명하고 있으며, 만약 대규모 언어 모델(LLM)을 실제 환경에 배포하는 것에 진지하게 임하고 있다면(예: 앞서 언급된 라마 모델들), 이 논문을 반드시 처음부터 끝까지 읽어볼 것을 강력히 권장합니다. 간략히 요약하자면, 테스트 시점 연산을 확장하는 두 가지 주요 접근 방식은 다음과 같습니다.
1.  **다수의 후보 해결책을 생성하고, 프로세스 기반 검증자 보상 모델(Process-based Verifier Reward Model, 별도로 학습되어야 함)을 활용하여 최적의 응답을 선정하는 방법.**
2.  **모델의 응답 분포(Response Distribution)를 적응적으로 갱신하는 방법.** 이는 본질적으로 추론 생성(Inference Generation) 과정 중에 응답을 점진적으로 수정하는 것을 의미합니다(이 또한 별도의 모델이 필요합니다).

첫 번째 범주에 대한 간단한 예시를 들자면, 테스트 시점 연산을 개선하는 한 가지 쉬운 방법은 N개 중 최적(Best-of-N) 샘플링을 사용하는 것입니다. 이는 LLM이 여러 개의 답변을 병렬적으로 생성하게 한 다음, 검증자 보상 모델(Verifier Reward Model)의 평가를 기반으로 가장 우수한 답변을 선택하는 방식입니다. N개 중 최적은 단지 하나의 예시에 불과하며, 아래 그림에서 볼 수 있듯이 빔 탐색(Beam Search), 선행 탐색(Lookahead Search) 등 여러 탐색 알고리즘(Search Algorithms)이 이 범주에 속합니다.
다양한 탐색 기반 방법은 프로세스-보상 기반 모델(Process-Reward-Based Model)에 의존하여 최상의 답변을 선택합니다.
LLM 테스트 시간 컴퓨팅 논문에서 발췌한 주석이 달린 그림, https://arxiv.org/abs/2408.03314
두 번째 범주에 속하는 또 다른 접근 방식은 아래 그림에 제시된 것처럼 모델의 응답을 순차적으로 수정해 나가는 방식입니다.
순차적 수정 접근 방식.
LLM 테스트 시간 컴퓨팅 논문에서 발췌한 주석이 달린 그림, https://arxiv.org/abs/2408.03314
그렇다면 어떤 접근 방식이 더 효과적일까요? 안타깝게도 모든 상황에 적용되는 만능 해결책은 존재하지 않습니다. 이는 기반이 되는 대규모 언어 모델(LLM)과 특정 문제 또는 질의(Query)의 특성에 따라 달라집니다. 예를 들어, 수정 기반 접근 방식은 더 어려운 질문에서 우수한 성능을 보이지만, 쉬운 질문에서는 오히려 모델의 성능을 저하시킬 수도 있습니다. 이 논문에서는 질의의 난이도(Difficulty Level)를 평가한 다음, 그에 맞춰 적절한 전략을 선택하는 모델을 기반으로 "최적의" 전략을 개발했습니다.

**8.3 테스트 시점 연산(Test-Time Computation) vs. 대규모 모델 사전 학습(Pretraining)**

여기서 제기되는 흥미로운 질문은, 주어진 고정된 연산 예산(Compute Budget) 내에서, 더 큰 모델을 사용하는 것과 추론 시점 예산(Inference-Time Budget)을 증대시키는 것 중 어느 쪽이 더 큰 효과를 가져오는가 하는 것입니다. 여기서 우리는 질의(Query) 처리 비용이 동일하다는 가정을 합니다. 왜냐하면 추론(Inference) 단계에서 대규모 모델을 구동하는 것이 소규모 모델보다 더 많은 비용을 수반하기 때문입니다. 연구자들은 어려운 질문의 경우, 이전에 논의된 추론 스케일링 전략(Inference Scaling Strategies)을 통해 추가적인 추론 연산을 부여받은 소규모 모델보다 대규모 모델이 더 나은 성능을 보인다는 사실을 발견했습니다. 그러나 쉽거나 중간 난이도의 질문에 대해서는, 동일한 연산 예산을 활용하여 추론 시점 연산을 늘리는 것만으로도 14배 더 큰 모델의 성능과 동등한 수준을 달성할 수 있었습니다!

**8.4 테스트 시점 연산(Test-Time Compute) 스케일링의 미래적 의의**

라마 3(Llama 3)와 같은 공개 가중치(Open-weight) 모델을 사용할 때, 우리는 흔히 모델이 응답을 있는 그대로 생성하도록 두는 경향이 있습니다. 그러나 이 논문이 명확히 보여주듯이, 더 많은 추론 연산(Inference Compute) 자원을 할당함으로써 응답의 품질을 현저히 개선할 수 있습니다. (만약 모델을 배포하고 있다면, 이 논문은 필독서입니다.) 물론, 대규모의 고비용 모델에 대한 추론 연산 예산을 늘리는 것은 운영 비용을 더욱 증가시키는 결과를 초래합니다. 하지만 질의(Query)의 난이도에 따라 선별적으로 적용될 경우, 특정 응답의 품질과 정확도를 매우 가치 있게 향상시킬 수 있으며, 이는 대다수 사용자들이 의심할 여지 없이 높이 평가할 것입니다. (오픈AI(OpenAI), 앤트로픽(Anthropic), 구글(Google) 등은 이미 이러한 기술들을 내부적으로 활용하고 있다고 가정하는 것이 합리적입니다.) 또 다른 강력한 활용 사례는 더 작고 온디바이스(On-device) 환경에서 구동되는 LLM의 성능을 끌어올리는 것입니다. 애플 인텔리전스(Apple Intelligence)와 마이크로소프트의 코파일럿 PC(Copilot PCs)에 대한 대대적인 발표와 투자를 보았듯이, 필자는 이러한 추론 시점 최적화가 향후 수개월, 수년간 뜨거운 연구 주제로 지속될 것이라고 예상합니다.

**9. 9월: 멀티모달 LLM 패러다임(Multimodal LLM Paradigms) 비교**

멀티모달 대규모 언어 모델(Multimodal LLMs)은 2024년에 큰 도약을 이룰 것이라고 필자가 예측했던 주요 영역 중 하나였습니다. 그리고 실제로 한 해 동안 더 많은 공개 가중치(Open-weight) 멀티모달 LLM들이 출시되었습니다!
다양한 입력 양식(Input Modalities, 오디오, 텍스트, 이미지, 비디오)을 수용하고 텍스트를 출력 양식(Output Modality)으로 생성할 수 있는 멀티모달 LLM의 예시.
특히 필자에게 인상 깊었던 논문은 엔비디아(NVIDIA)의 Dai 외 연구진이 발표한 "NVLM: 개방형 프론티어급 멀티모달 LLM(Open Frontier-Class Multimodal LLMs, 2024년 9월)"이었습니다. 이 논문은 두 가지 핵심적인 멀티모달 패러다임을 탁월하게 비교 분석했기 때문입니다. 이 연구는 멀티모달 AI의 설계 원칙과 실용적 적용에 대한 깊이 있는 이해를 제공하며, 단순히 개념 설명에 그치지 않고 실제 성능 비교를 통해 각 접근 방식의 장단점을 명확히 보여주었습니다.

**9.1 멀티모달 LLM 패러다임(Multimodal LLM Paradigms)**

멀티모달 LLM을 구축하는 데는 크게 두 가지 접근 방식이 존재합니다.
*   **방법 A: 통합 임베딩 디코더 아키텍처(Unified Embedding Decoder Architecture) 접근 방식;**
*   **방법 B: 교차 모달리티 어텐션 아키텍처(Cross-modality Attention Architecture) 접근 방식.**
멀티모달 LLM 아키텍처를 개발하는 두 가지 주요 접근 방식.
위 그림에서 설명된 바와 같이, 통합 임베딩-디코더 아키텍처(Unified Embedding-Decoder Architecture, 방법 A)는 GPT-2 또는 라마 3.2(Llama 3.2)와 같이 수정되지 않은 LLM 아키텍처와 유사한 단일 디코더 모델(Single Decoder Model)에 의존합니다. 이 방식은 이미지를 텍스트 토큰(Text Tokens)과 동일한 임베딩 크기(Embedding Size)를 공유하는 토큰으로 변환하여, LLM이 연결된 텍스트 및 이미지 입력 토큰(Image Input Tokens)을 처리할 수 있도록 합니다. 대조적으로, 교차 모달리티 어텐션 아키텍처(Cross-Modality Attention Architecture, 방법 B)는 교차 어텐션 메커니즘(Cross-Attention Mechanism)을 통합하여 어텐션 레이어(Attention Layer) 내에서 이미지 및 텍스트 임베딩(Image and Text Embeddings)을 직접 통합합니다. 추가 세부 사항에 관심이 있다면, 올해 초 필자가 이 두 가지 방법을 단계별로 설명하는 멀티모달 LLM에 대한 전체 기사를 작성했습니다: "멀티모달 LLM 이해하기 -- 주요 기술 및 최신 모델 소개(Understanding Multimodal LLMs -- An introduction to the main techniques and latest models)".

**9.2 엔비디아의 하이브리드(Hybrid) 접근 방식**

올 한 해 동안 이루어진 모든 멀티모달 개발들을 고려할 때, 엔비디아(NVIDIA)의 논문 "NVLM: 개방형 프론티어급 멀티모달 LLM(Open Frontier-Class Multimodal LLMs)"은 이러한 멀티모달 접근 방식들에 대한 포괄적이고 공정한 비교(Apples-to-Apples Comparison)를 제공했다는 점에서 필자에게 깊은 인상을 남겼습니다. 이 논문은 단일 방법에만 초점을 맞추기보다는 다음 세 가지를 직접적으로 비교했습니다.
*   **방법 A: 통합 임베딩 디코더 아키텍처("디코더 전용 아키텍처(Decoder-Only Architecture)," NVLM-D),**
*   **방법 B: 교차 모달리티 어텐션 아키텍처("교차 어텐션 기반 아키텍처(Cross-Attention-Based Architecture)," NVLM-X),**
*   **하이브리드 접근 방식(Hybrid Approach, NVLM-H).**
세 가지 멀티모달 접근 방식 개요. (NVLM: 개방형 프론티어급 멀티모달 LLM 논문에서 발췌한 주석이 달린 그림: https://arxiv.org/abs/2409.11402)
위 그림에 요약된 바와 같이, NVLM-D는 방법 A와 일치하며, NVLM-X는 이전에 논의된 방법 B에 해당합니다. 하이브리드 모델(NVLM-H)은 두 접근 방식의 장점을 결합합니다. 먼저 이미지 썸네일(Image Thumbnail)을 입력으로 받아들이고, 이어서 교차 어텐션(Cross-Attention)을 통해 처리되는 동적인 수의 패치(Patches)를 통해 더 미세한 고해상도 세부 정보(Finer High-Resolution Details)를 포착합니다. 요약하자면, 주요 발견 사항은 다음과 같습니다.
*   **NVLM-X:** 고해상도 이미지 처리에서 탁월한 연산 효율성(Computational Efficiency)을 제공합니다.
*   **NVLM-D:** 광학 문자 인식(OCR) 관련 작업에서 더 높은 정확도를 보여줍니다.
*   **NVLM-H:** 두 접근 방식의 장점을 결합하여 최적의 성능을 달성합니다.

**9.3 2025년의 멀티모달 LLM(Multimodal LLMs)**

멀티모달 LLM은 매우 흥미로운 연구 분야입니다. 필자는 이들이 일반적인 텍스트 기반 LLM에서 한 단계 더 나아간 논리적인 발전이라고 생각합니다. 오픈AI(OpenAI), 구글(Google), 앤트로픽(Anthropic)과 같은 대부분의 LLM 서비스 제공업체는 이미지와 같은 멀티모달 입력(Multimodal Inputs)을 지원합니다. 개인적인 경험으로는 멀티모달 기능이 필요한 경우가 아마 1% 정도에 불과할 것입니다(주로 "테이블을 마크다운(Markdown) 형식으로 추출해줘"와 같은 경우입니다). 필자는 공개 가중치(Open-weight) LLM의 기본값이 순수하게 텍스트 기반일 것으로 예상합니다. 이는 복잡성을 덜 추가하기 때문입니다. 동시에, 도구(Tooling)와 API가 발전함에 따라 공개 가중치 LLM의 더 많은 옵션과 광범위한 활용을 목격하게 될 것이라고 생각합니다.

**10. 10월: 오픈AI o1의 추론 능력(Reasoning Capabilities) 재현 시도**

10월의 핵심 논문으로 필자가 선정한 것은 Quin 외 연구진이 2024년 10월에 발표한 "O1 복제 여정: 전략적 진행 보고서 – 1부(O1 Replication Journey: A Strategic Progress Report – Part 1)"입니다. 오픈AI 챗GPT(ChatGPT)의 o1(그리고 현재 o3) 모델은 대규모 언어 모델(LLM)의 추론 과제(Reasoning Tasks) 수행 능력을 혁신적으로 개선하며 상당한 주목을 받았습니다. 오픈AI o1의 정확한 기술적 세부 사항은 공개되지 않았으며, 여러 학술 논문에서 이를 설명하거나 복제하려는 시도가 있었습니다. 그렇다면 왜 이 특정 논문을 선택했을까요? 이 연구의 독특한 구성과 학술 연구의 현재 상황에 대한 폭넓은 철학적 주장이 필자에게 깊은 공감을 불러일으켰기 때문입니다. 다시 말해, 이 논문에는 눈에 띄는 독창적인 측면이 있었고, 그것이 흥미로운 선택으로 이어졌습니다.

**10.1 지름길 학습(Shortcut Learning) 대 여정 학습(Journey Learning)**

이 논문의 핵심 주장 중 하나는 아래 그림에 나타난 바와 같이, O1 모델이 기존의 지름길 학습(Shortcut Learning) 방식과 대비되는 여정 학습(Journey Learning)이라는 과정을 사용한다는 연구자들의 가설입니다. 일반적으로 대규모 언어 모델(LLM)은 올바른 해결 경로(Shortcut Learning)를 따라 학습됩니다. 반면 여정 학습에서는 지도 미세 조정(Supervised Finetuning) 과정이 전체적인 시행착오 수정 절차(Trial-and-Error Correction Process)를 포함합니다.
O1 복제 보고서에서 발췌한 주석이 달린 그림, https://arxiv.org/abs/2410.18982
여정 학습 접근 방식은 본 보고서의 "8. 8월: 추론 시간 컴퓨팅(Inference-Time Compute) 확장을 통한 LLM 개선" 섹션에서 이전에 논의된 수정 기능이 있는 트리 기반(Tree-based) 또는 빔 탐색(Beam-Search) 방법과 어느 정도 유사하다는 점에 주목할 가치가 있습니다. 그러나 미묘한 차이점은 연구자들이 단순히 추론(Inference) 단계에서 이 기술을 적용하는 것이 아니라, 모델 미세 조정을 위한 여정 학습 훈련 예제(Journey Learning Training Examples)를 생성한다는 점입니다. (또한, 필자는 O1의 추론 프로세스를 강화하기 위해 사용된 기술에 대한 정보를 찾을 수 없었다는 점도 언급할 가치가 있습니다.)

**10.2 긴 사고(Long Thoughts)의 구성**

연구자들은 시행착오를 강조하며 확장된 사고 과정(Extended Thought Process)을 이끌어내기 위해 추론 트리(Reasoning Tree)를 구축했습니다. 이 접근 방식은 유효한 중간 단계들을 통해 정답으로 향하는 직접적인 경로를 찾는 것을 우선시하는 전통적인 방법론과 차이를 보입니다. 그들의 프레임워크에서 추론 트리의 각 노드(Node)는 보상 모델(Reward Model)이 제공하는 평가로 주석이 달렸으며, 해당 단계가 올바른지 또는 잘못되었는지와 이 평가를 정당화하는 추론을 명시했습니다. 다음으로, 그들은 지도 미세 조정(Supervised Finetuning)과 직접 선호도 최적화(DPO)를 통해 딥시크-수학-7b-기반(Deepseek-math-7b-base) 모델을 학습시켰습니다. 여기서 그들은 두 가지 모델을 훈련시켰습니다.
1.  첫째, 그들은 올바른 중간 단계만 제공되는 전통적인 지름길 학습(Shortcut Training) 패러다임을 사용했습니다.
2.  둘째, 그들은 올바른 답과 잘못된 답, 되돌아가기(Backtracking) 등을 포함하는 사고 과정(Thought Process) 세 가지를 포함하는 제안된 여정 학습(Journey Learning) 접근 방식으로 모델을 학습시켰습니다. (참고: 각 경우에 단 327개의 예제만 사용했습니다!)
아래 그림에 나타난 바와 같이, 여정 학습 프로세스는 MATH500 벤치마크(Benchmark) 데이터셋에서 지름길 학습을 상당히 큰 차이로 능가했습니다.
지름길 학습과 여정 학습으로 학습된 LLM.
O1 복제 보고서에서 발췌한 주석이 달린 그림, https://arxiv.org/abs/2410.18982

**10.3 증류(Distillation) – 즉각적인 해결책인가?**

한 달 후, 연구팀은 또 다른 보고서인 Huang 외 연구진이 발표한 "O1 복제 여정 – 2부: 단순 증류를 통한 O1-프리뷰 능가, 큰 진전인가 쓰디쓴 교훈인가?(O1 Replication Journey – Part 2: Surpassing O1-preview through Simple Distillation, Big Progress or Bitter Lesson?, 2024년 11월)"를 공개했습니다. 여기서 그들은 증류(Distillation) 접근 방식을 활용했습니다. 즉, O1 모델에서 사고 과정(Thought Processes)을 추출하기 위해 신중한 프롬프트(Prompting) 기법을 사용하여 동일한 성능에 도달하도록 모델을 학습시켰습니다. 이 글이 이미 길어졌으므로 자세한 내용은 다루지 않겠지만, 긴 사고 데이터(Long-Thought Data) 수집의 비용 상충 관계(Cost Trade-offs)를 요약한 해당 논문의 흥미로운 그림을 공유하고 싶었습니다. 그들은 이 증류 접근 방식을 통해 O1-프리뷰 및 O1-미니와 동등한 매우 우수한 성능을 얻었습니다. 그러나 이러한 실험과 함께 연구자들은 이 접근 방식에 비추어 연구 현황에 대한 흥미롭고 중요한 견해를 공유했으며, 이는 다음 섹션에서 요약하겠습니다.

**10.4 인공지능 연구의 현재 상황**

파트 2 보고서의 주요 초점 중 하나는 "단순 증류의 쓰디쓴 교훈(Bitter Lesson of Simple Distillation)"이었습니다. 물론 증류(Distillation)는 실제 환경에서 효과적으로 작동하지만, 그것 자체가 발전을 이끄는 원동력은 아닙니다. 최상의 경우, 증류를 사용하면 기존의 상위 모델(Upstream Model) 성능과 일치시킬 수 있을 뿐입니다(새로운 성능 기록을 세우는 것은 아닙니다). 아래는 현재 상황에 대한 경고로 작용할 수 있는 논문에서 발췌한 세 가지 인용문입니다.
*   "‘어떻게 작동하는가’에서 ‘무엇이 작동하는가’로의 이러한 변화는 연구 사고방식의 근본적인 변화를 나타내며, 이는 해당 분야의 미래 혁신 역량에 광범위한 영향을 미칠 수 있습니다."
*   "이러한 제1원리 사고(First-Principles Thinking)의 침식은 과학적 혁신의 바로 그 기반을 약화시키기 때문에 특히 우려됩니다."
*   "빠른 결과물을 내야 한다는 압박은 더 깊은 기술적 조사의 가치를 가릴 수 있으며, 학생들은 더 도전적이고 근본적인 연구 방향을 추구하는 것을 단념하게 될 수 있습니다."
필자의 개인적인 견해는 여전히 학술 연구실(Academic Labs, 오늘날에는 종종 산업계와의 파트너십을 통해서도)에서 수많은 훌륭하고 중요한 아이디어들이 나오고 있으며, 이들은 실제로 실용적이고 영향력이 클 수 있다고 생각합니다. (필자가 좋아하는 몇 가지는 로라(LoRA)와 디피오(DPO)입니다.) 문제는 많은 유망한 아이디어들이 대규모로 테스트되지 못한다는 것입니다. 대학은 보통 이를 위한 막대한 자원을 가지고 있지 않기 때문입니다. 완벽한 해결책이 무엇인지는 확실하지 않으며, 기업들이 자신들의 영업 비밀(Trade Secrets)을 그냥 공개할 수 없다는 것도 알고 있습니다. 하지만 기업들이 학술 논문의 아이디어를 사용하게 될 때마다 이를 공개적으로 인정해 준다면 정말 도움이 될 것입니다. 그러한 인정은 자신의 작업을 자유롭게 제공하는 연구자들에게 동기를 부여하고 보상하는 데 큰 도움이 됩니다. 또한, 실제로 무엇이 작동하는지 알아냄으로써 분야를 발전시키는 데 기여합니다.

**10.5 O1 (및 O3)의 관점에서 본 LLM의 미래**

"O1 복제 여정(O1 Replication Journey)" 논문이 O1 뒤에 있는 정확한 메커니즘을 성공적으로 재현했을까요? 아마 아닐 것입니다. 하지만 이 연구는 여전히 더 나은 결과를 달성하는 데 도움이 될 아이디어들로 가득 찬 가치 있는 읽을거리입니다. 필자는 O1 및 O3와 같은 "장기 사고(Long-Thought)" 모델들이 대규모 언어 모델(LLM) 연구에서 계속해서 핵심적인 역할을 수행할 것이라고 믿습니다. 이들은 실행 비용이 더 많이 들지만, 추론 작업(Reasoning Tasks) 성능의 기본적으로 황금 표준(Gold Standard) 또는 상한선(Upper Limit for Performance)으로 간주됩니다. 그러나 비용이 더 높기 때문에 O1 유형 모델이 모든 상황에서 항상 최선의 선택은 아닙니다. 문법 수정(Grammar Fixes)이나 번역(Translations)과 같은 더 간단한 작업의 경우, 추론 중심 모델(Reasoning-Heavy Model)이 필요하지 않을 것입니다. 결국 비용과 유용성(Utility)의 균형을 맞추는 문제입니다. 우리는 예산, 지연 시간(Latency) 및 기타 요인에 따라 작업에 가장 적합한 LLM을 선택하게 됩니다.
Ahead of AI는 독자 지원 출판물입니다. 새로운 게시물을 받고 제 작업을 지원하려면 무료 또는 유료 구독자가 되는 것을 고려해 보세요.
구독

**11. 11월: 정밀도(Precision)를 위한 LLM 스케일링 법칙(LLM Scaling Laws)**

원래 필자는 앨런 AI(Allen AI)의 "툴루 3: 개방형 언어 모델 사후 학습의 한계 확장(Tulu 3: Pushing Frontiers in Open Language Model Post-Training)" 논문을 선택하고 싶었습니다. 이 논문에는 직접 선호도 최적화(DPO) 대 정책 최적화(PPO)의 절제 연구(Ablation Studies)를 포함한 라마(Llama) 사후 학습(Post-training) 방법론과 레시피에 대한 상세한 설명과, 보상 모델(Reward Model) 대신 쉽게 정답(Ground Truth Answer)을 생성할 수 있는 검증 가능한 질의(Verifiable Queries, 수학 및 코드 질문 등)를 사용하는 검증 가능한 피드백을 통한 강화 학습(Reinforcement Learning with Verifiable Feedbacks)이라는 새로운 선호도 정렬(Preference Alignment) 방법이 포함되어 있었기 때문입니다. 하지만 내부적인 논의 끝에, 궁극적으로 쿠마르(Kumar) 외 연구진이 발표한 "정밀도를 위한 스케일링 법칙(Scaling Laws for Precision)" 논문(2024년 11월)을 선택하기로 결정했습니다. 이 논문은 2022년 "컴퓨팅 최적 대규모 언어 모델 학습(Training Compute-Optimal Large Language Models)" 논문의 친칠라 스케일링 법칙(Chinchilla Scaling Laws)에 대한 매우 필요한 업데이트를 제공하며, 이 법칙은 사전 학습(Pretraining)을 위한 컴퓨팅 최적 대규모 언어 모델(LLM) 매개변수 수(Compute-Optimal LLM Parameter Counts)와 데이터셋 크기(Dataset Sizes)를 결정하는 데 사용됩니다. 요약하자면, "정밀도 스케일링 법칙(Precision Scaling Laws)" 논문(2024년 11월)은 친칠라의 스케일링 법칙을 확장하여 최근 몇 년간 매우 인기를 얻고 있는 저정밀도 설정(Low-Precision Settings, 16비트 이하)에서의 학습(Training) 및 추론(Inference)을 설명합니다. 예를 들어, 이 논문은 다양한 저정밀도 및 양자화(Quantization) 관련 관찰을 단일 함수 형식(Single Functional Form)으로 통합하여 저정밀도 학습(Low-Precision Training)과 사후 학습 양자화(Post-Training Quantization) 모두에서 추가되는 손실(Added Loss)을 예측합니다.

**11.1 친칠라 스케일링 법칙(Chinchilla Scaling Laws) 재조명**

2022년 "컴퓨팅 최적 대규모 언어 모델 학습(Training Compute-Optimal Large Language Models)" 논문에서 제시된 본래의 친칠라 스케일링 법칙은 대규모 언어 모델(LLM)의 매개변수 수(Parameter Counts, N)와 데이터셋 크기(Dataset Sizes, D)가 LLM의 검증 손실(Validation Loss)에 어떻게 상호 작용하며 영향을 미치는지를 모형화하며, LLM 및 학습 데이터셋의 규모를 결정하는 지침으로 활용됩니다. 경험적 규칙(Rule of Thumb)에 따르면, 데이터셋 크기 D와 매개변수 수 N 사이의 최적의 절충점(고정된 연산 예산(Compute Budget)이 있을 때)은 대략 D/N ≈ 20입니다. 이 데이터-매개변수 비율은 동일한 총 학습 비용(Total Training Cost)에서 다른 비율보다 더 낮은 검증 손실을 산출하기 때문에 종종 "친칠라-최적(Chinchilla-Optimal)"이라고 불립니다. 그러나 많은 현대적인 예외 사례들이 존재합니다. 예를 들어, 라마 3(Llama 3) 팀은 이전에 논의된 바와 같이 15조 개의 토큰(Tokens)으로 학습했으며, 80억 개 매개변수 버전의 경우 15,000,000,000,000 ÷ 8,000,000,000 = 1,875라는 비율이 나옵니다. 필자의 생각으로는, 정확한 데이터-매개변수 비율 자체보다 더 중요한 것은 모델과 데이터셋의 크기가 서로 비례적으로 확장되어야 한다는 점입니다.

**11.2 저정밀도 학습(Low-Precision Training)**

저정밀도 스케일링 법칙(Low-Precision Scaling Laws)을 더 자세히 논의하기(또는 요약하기) 전에, 일반적으로 대규모 언어 모델(LLM) 또는 심층 신경망(Deep Neural Network)의 가중치(Weights)에 대한 다양한 숫자 정밀도 형식(Numeric Precision Formats)에 대한 아주 짧은 입문서로 시작하겠습니다. 필자가 아는 한, GPT 2 & 3 및 라마 2 & 3 모델 학습에 사용된 정밀도 형식은 비교를 위해 다음과 같습니다.
*   **Float32**는 범위와 정밀도 사이의 균형이 뛰어나 심층 신경망 학습에 널리 사용되는 표준 32비트 부동 소수점 형식(Floating-Point Format)이었습니다. Float32 미만의 모든 형식은 오늘날 저정밀도(Low-Precision)로 간주됩니다(비록 "낮음"의 정의가 대규모 언어 모델의 "대규모"와 유사하게 움직이는 목표이기는 하지만).
*   **Float16**, 또는 반정밀도(Half-Precision)는 단 16비트를 사용하여 메모리를 절약하고 계산 속도를 높이지만, 더 좁은 동적 범위(Dynamic Range)를 제공합니다.
32비트 및 16비트 부동 소수점 정밀도 비교
*   **Bfloat16(브레인 플로트 16(Brain Float 16))** 또한 16비트 형식이지만, Float16의 일부 정밀도를 더 큰 지수(Exponent)와 교환하여 매우 크고 매우 작은 숫자를 더 효과적으로 표현할 수 있도록 합니다. 결과적으로 Bfloat16은 딥러닝 애플리케이션에서 숫자 오버플로(Numeric Overflow) 또는 언더플로(Underflow)를 피하는 데 도움이 될 수 있지만, 낮은 정밀도로 인해 여전히 반올림 오류(Rounding Errors)가 발생할 수 있습니다.
일반 16비트 부동 소수점과 인기 있는 16비트 브레인 부동 소수점 정밀도 비교
다양한 정밀도 형식과 LLM 모델 동작에 미치는 영향에 대해 더 자세히 알고 싶다면, 필자의 이전 기사인 "누락된 비트: 라마 2 가중치가 변경되었습니다(The Missing Bits: Llama 2 Weights Have Changed)"의 더 긴 소개를 좋아하실 것입니다. 또한 필자는 32비트 및 16비트 형식만 보여주고 있지만, 현재 학습을 위한 더 낮은 정밀도, 예를 들어 라마 3 논문에서 (실험적으로) 언급된 8비트 형식에 대한 경쟁이 진행 중이라는 점에 유의하십시오. (12월 26일에 출시된 딥시크-v3(DeepSeek-v3) 모델은 8비트 부동 소수점 정밀도로 완전히 사전 학습되었습니다.)

**11.3 정밀도 스케일링 법칙(Precision Scaling Laws)의 핵심**

이 논문은 길고 매우 흥미로우며, 전체를 읽어볼 것을 권장합니다. 그러나 핵심적인 내용을 요약하자면, 연구자들은 본래의 친칠라 스케일링 법칙(Chinchilla Scaling Laws)에 "정밀도(Precision)" 요소 P를 추가하여 확장했습니다. 구체적으로, 그들은 모델 매개변수 수(Parameter Count) N을 정밀도가 감소함에 따라 줄어드는 "유효 매개변수 수(Effective Parameter Count)"로 재해석합니다. (수학 공식은 논문을 참조하십시오.) 또한, 사후 학습 양자화(Post-Training Quantization)가 모델 성능을 어떻게 저하시키는지 포착하기 위한 추가 항을 도입했습니다. (양자화(Quantization)에 대한 소개를 작성하지 않았다는 것을 알고 있지만, 이 글의 길이가 이미 너무 길기 때문에 다음 기회로 미루어야 할 것 같습니다.) 아래 그림은 더 많은 사전 학습(Pretraining) 데이터가 항상 더 좋은 결과로 이어지는 것은 아니며, 매우 낮은 정밀도(INT3)로 학습한 후 모델이 양자화될 경우 실제로 성능에 해로울 수 있음을 명확히 보여줍니다. 필자는 이 점이 매우 흥미로웠습니다.
다양한 사후 양자화 형식에 대한 검증 손실(Validation Loss)에 대한 더 많은 학습 데이터의 영향
따라서 위 그림에서 도출할 수 있는 결론은, 점점 더 많은 데이터로 학습된 모델(라마 3(Llama 3)와 같은)은 너무 많은 데이터로 "과도하게 학습(Overtrained)"되었기 때문에, 학습 후 더 낮은 정밀도 형식으로 양자화하기가 더 어려워질 수 있다는 점입니다. 이는 모델의 범용성과 효율성 측면에서 중요한 시사점을 제공합니다.

**11.4 2025년의 모델 스케일링 법칙(Model Scaling Laws)**

친칠라 스케일링 법칙(Chinchilla Scaling Laws)에 대한 필요한 업데이트를 제공하는 것 외에도, 정밀도 스케일링 법칙(Precision Scaling Laws) 연구는 2025년의 중요한 과제에 대한 흥미로운 관점을 제시합니다. 즉, 라마-3(LLaMA-3)와 같은 모델이 더 방대한 데이터셋으로 학습될수록, 성능 저하 없이 INT3와 같은 저정밀도 형식으로 양자화(Quantize)하기가 더욱 어려워질 수 있다는 것입니다. 이 발견은 "더 많은 데이터가 더 좋다"는 일반적인 사고방식을 재고하게 하며, 데이터셋의 규모와 효율적인 추론(Efficient Inference)의 실제 제약 조건 사이의 균형을 맞출 필요성을 강조합니다. 이는 또한 하드웨어 최적화(Hardware Optimization)를 추진하는 데 있어 중요한 통찰력을 제공합니다. 이러한 스케일링 법칙 연구에서 종종 간과된다고 필자가 생각하는 측면 중 하나는 데이터셋의 품질입니다. 필자는 사전 학습 데이터(Pretraining Data)의 특성이 모델 성능에 상당한 영향을 미 미칠 수 있다고 확신합니다. (이에 대한 자세한 내용은 아래 Phi-4 논의에서 더 깊이 다루겠습니다.)

**12. 12월: Phi-4와 합성 데이터(Synthetic Data)로부터의 학습**

2024년 하반기에는 크리스마스에 공개된 인상적인 딥시크-V3(DeepSeek-V3)를 포함하여 여러 흥미로운 모델들이 출시되었습니다. 가장 큰 규모의 모델 출시는 아니었지만, 궁극적으로 필자는 마이크로소프트의 Phi-4 기술 보고서(Phi-4 Technical Report)를 선택하기로 결정했습니다. 이 보고서가 합성 데이터(Synthetic Data) 활용에 대한 매우 흥미로운 통찰력을 제공했기 때문입니다. 이는 데이터 생성과 활용 방식에 대한 새로운 관점을 제시하며, 미래 대규모 언어 모델(LLM) 개발 방향에 중요한 시사점을 던져줍니다.

**12.1 Phi-4 성능 개요**

Abdin 외 연구진이 2024년 12월에 발표한 Phi-4 기술 보고서(Phi-4 Technical Report)는 마이크로소프트의 최신 140억 개 매개변수(Parameter) 공개 가중치 대규모 언어 모델(Open-Weight LLM)의 학습(Training) 과정을 상세히 기술합니다. Phi-4를 특히 주목하게 만드는 점은 주로 GPT-4o가 생성한 합성 데이터(Synthetic Data)로 학습되었다는 사실입니다. 벤치마크(Benchmarks) 결과에 따르면, 이 모델은 주로 비합성 데이터로 학습된 이전 모델인 Phi-3를 포함하여 유사한 크기의 다른 LLM들보다 뛰어난 성능을 보입니다.
유사하거나 다른 크기의 다른 모델과 비교한 phi-4의 성능 (phi-4 논문에서 발췌한 주석이 달린 표, https://arxiv.org/abs/2412.08905)
위 표에 나타난 바와 같이, 모델이 SimpleQA에서 왜 더 낮은 성능을 보이는지에 대해서는 완전히 확실하지 않습니다. 하지만 한 가지 가능한 설명은 SimpleQA가 2024년 10월 30일에 출시된 비교적 새로운 벤치마크라는 것입니다. 오픈AI(OpenAI)가 평가 스위트(Evaluation Suite)의 일부로 개발했기 때문에 GPT-4o의 학습 데이터(Training Data)에 포함되지 않았거나 웹 크롤링된 데이터셋(Web-Crawled Datasets)에 통합되지 않았을 수 있습니다. 더욱이, GPT-4o가 이 평가를 위한 합성 데이터를 생성하는 데 사용되었기 때문에, 어떤 모델도 학습 중에 SimpleQA를 접하지 못했을 것입니다. 그러나 Phi-4가 다른 벤치마크에 과적합(Overfitting)되었을 수 있으며, 이는 이전에 본 적 없는 SimpleQA 데이터셋에서 상대적으로 낮은 성능을 보이는 이유를 설명할 수 있습니다. 어쨌든, 이것은 단지 필자의 가설일 뿐입니다.

**12.2 합성 데이터(Synthetic Data) 학습의 특징**

이 논문에서 제시된 일부 절제 연구(Ablation Studies)의 결과를 요약하기 전에, 데이터셋 구성(Dataset Composition)에 대해 살펴보겠습니다.
phi-4 학습을 위한 데이터셋 혼합 (phi-4 논문에서 발췌한 주석이 달린 표, https://arxiv.org/abs/2412.08905).
연구자들은 합성 데이터(Synthetic Data)가 일반적으로 유익하지만, 합성 데이터만으로 학습된 모델은 지식 기반 벤치마크(Knowledge-Based Benchmarks)에서 성능이 저조하다는 것을 관찰했습니다. 필자에게 이것은 다음과 같은 질문을 던집니다: 합성 데이터에 충분한 지식 특정 정보가 부족한 것인가, 아니면 환각(Hallucinations)으로 인한 것과 같은 사실 오류(Factual Errors)의 비율이 더 높은 것인가? 동시에 연구자들은 합성 데이터에 대한 학습 에포크(Training Epochs) 수를 늘리는 것이 더 많은 웹 데이터(Web Data)를 추가하는 것보다 성능을 더 향상시킨다는 것을 발견했습니다. 이는 아래 그림에 나타나 있습니다.
다른 합성/웹 데이터셋 비율에 대한 모델 성능 비교. (phi-4 논문에서 발췌한 주석이 달린 그림, https://arxiv.org/abs/2412.08905).
요약하자면, 혼합된 데이터에서 합성 데이터의 과도한 비율은 지식 기반 성능에 부정적인 영향을 미칩니다. 그러나 더 균형 잡힌 합성-웹 데이터 혼합 내에서는 합성 데이터셋에 대한 반복(Iterations, 에포크(Epochs)) 횟수를 늘리는 것이 유익합니다. 이는 합성 데이터의 양보다는 질과 사용 방식이 중요함을 시사합니다.

**12.3 합성 데이터(Synthetic Data)의 미래적 중요성**

Phi-4 기술 보고서는 합성 데이터(Synthetic Data) 사용에 대한 흥미로운 통찰력을 제공합니다. 즉, 모델 사전 학습(Pre-training)에 매우 유익할 수 있다는 것입니다. 특히 스케일링 법칙(Scaling Laws)이 모델 및 데이터셋 크기 모두에서 정체되고 있다고 알려져 있지만(라마 3(Llama 3) 논문에서는 아직 15조 토큰(Token) 수준에서 수렴을 보지 못했다고 언급했지만), 연구자와 엔지니어들은 한계를 계속 확장하기 위한 대안적인 방법들을 모색하고 있습니다. 물론, 사전 학습(Pre-training) 및 특히 사후 학습(Post-training) 기술의 정교화 및 추가는 여전히 큰 변화를 가져올 요인 중 하나로 남을 것입니다. 그럼에도 불구하고, 필자는 합성 데이터의 사용이 a) 더 적은 데이터로 사전 학습된 기본 모델(Pretrained Base Models)을 만들거나 b) 훨씬 더 우수한 기본 모델(라마 3 데이터셋의 15조 토큰에 40%의 합성 데이터 토큰을 추가하는 것을 상상해 보세요)을 만드는 효과적인 방법으로 간주될 것이라고 생각합니다. 필자는 고품질 데이터의 사용을 전이 학습(Transfer Learning)과 유사하게 봅니다. 원시적이고 구조화되지 않은 인터넷 데이터로 모델을 사전 학습하고 사후 학습 중에 정제하는 대신, 고품질 모델(예: 이미 광범위한 정제를 거친 GPT-4o)이 생성한 (일부) 합성 데이터를 활용하는 것이 일종의 지름길(Jumpstart) 역할을 할 수 있습니다. 다시 말해, 고품질 학습 데이터의 활용은 모델이 처음부터 더 효율적으로 학습할 수 있도록 도울 수 있습니다.

**결론 및 2025년 전망**

이 연구 요약들이 독자 여러분께 유용했기를 바랍니다! 늘 그렇듯이, 이 보고서는 원래 의도했던 것보다 더 길어졌습니다. 하지만 2025년에 대한 필자의 예측(또는 기대)을 담은 비교적 짧고 간결한 섹션으로 마무리하겠습니다.

**멀티모달 LLM(Multimodal LLMs)**

지난해 필자는 대규모 언어 모델(LLM)이 점차 멀티모달(Multimodal) 형태로 진화할 것이라고 예측했습니다. 현재 모든 주요 독점 LLM 제공업체는 멀티모달(또는 최소한 이미지) 지원을 제공하고 있습니다. 따라서 이러한 변화는 이제 완전히 진행 중이며, 우리는 이를 향한 더 많은 오픈 소스(Open-source) 노력들을 목격하게 될 것입니다. 필자가 접하고 읽은 바에 따르면, 멀티모달 관련 논문이 폭발적으로 증가한 것은 분명합니다. 아마도 필자의 오픈 소스 미세 조정(Finetuning) 방법론과 자료들도 뒤따를 것입니다. 비록 많은 사용 사례에서 텍스트 전용으로도 충분하고 계속 충분할 것이며, 주요 초점은 더 나은 추론 모델(Reasoning Models, O1 및 곧 출시될 O3와 같은)을 개발하는 데 있을 것이라고 주장하겠지만 말입니다.

**연산 효율성(Computational Efficiency)**

대규모 언어 모델(LLM)을 사전 학습(Pretraining)하고 활용하는 것은 비교적 높은 비용을 수반합니다. 따라서 가까운 미래에 LLM의 연산 효율성(Computational Efficiency)을 개선하기 위한 더욱 영리한 방법들을 보게 될 것으로 예상합니다. 참고로, 최근 딥시크-v3(DeepSeek-v3) 모델을 학습시키는 데 GPU 대여 가격(GPU Rental Sticker Prices)을 기준으로 약 5백만 달러가 소요되었을 것으로 추정됩니다(여기에는 하이퍼파라미터 튜닝(Hyperparameter Tuning), 실패한 실행(Failed Runs) 및 인건비(Personnel Cost)는 포함되지 않습니다).
DeepSeek-v3 보고서에서 발췌한 대략적인 계산, https://github.com/deepseek-ai/DeepSeek-V3/blob/main/DeepSeek_V3.pdf
그런데 공식 메타 AI(Meta AI) 라마 3(Llama 3) 모델 카드에 따르면, 라마 3 4050억 개 매개변수 모델은 약 10배 더 많은 연산(Compute) 자원을 사용했습니다(3084만 GPU 시간 대 266만 GPU 시간). LLM을 효율적으로 만드는 기술의 인기 있는 예시(모두 학습 중에 적용되는 것은 아니지만)로는 전문가 혼합(Mixture of Experts, 필자의 파트 1 기사에서 논의됨), 라마 모델에서 발견되는 그룹화된 쿼리 어텐션(Grouped-Query Attention) 등이 있습니다. 또 다른 흥미로운 점은 딥시크(DeepSeek) 모델에서 발견되는 다중 헤드 잠재 어텐션(Multihead Latent Attention)을 사용하여 다중 헤드 어텐션(Multihead Attention)의 KV-캐싱(KV-Caching)을 더 효율적으로 만드는 것입니다. 또 다른 흥미로운 최근 경로는 모델 입력(Model Input)을 대상으로 하는 것입니다. 예를 들어, 최근 제안된 바이트 잠재 트랜스포머(Byte Latent Transformer)는 바이트를 엔트로피 기반 패치(Entropy-Based Patches)로 동적으로 인코딩하여 토큰화(Tokenization) 없이 확장성(Scalability)과 더 빠른 추론(Inference)을 위한 연산을 최적화함으로써 효율성을 향상시킵니다.

**상태 공간 모델(State Space Models)**

올해 상태 공간 모델(State Space Models, SSM)을 다루지 않았다는 것을 눈치채셨을 수도 있습니다. 이는 현재 필자의 초점이 주로 트랜스포머 기반 대규모 언어 모델(Transformer-based LLMs)에 있기 때문입니다. 상태 공간 모델이 매우 흥미롭다고 생각하지만, 이 단계에서는 여전히 상당히 실험적인(Experimental) 것으로 보입니다. 게다가 트랜스포머는 광범위한 작업에서 탁월한 성능(Exceptional Performance)을 계속 보여주고 있어 대안을 고려하는 것이 그다지 매력적이지 않습니다. 그러나 그렇다고 해서 상태 공간 모델 분야에서 진전이 없었다는 의미는 아닙니다. 필자는 이 분야에서 흥미로운 논문들을 많이 보았습니다. 그리고 필자가 주목한 한 가지 흥미로운 추세는 이들이 이제 모두 트랜스포머 모델의 셀프 어텐션(Self-Attention)을 통합한 하이브리드 모델(Hybrid Models)이라는 것입니다. 예를 들어, "잠바-1.5: 대규모 하이브리드 트랜스포머-맘바 모델(Jamba-1.5: Hybrid Transformer-Mamba Models at Scale)", "라마 속 맘바: 하이브리드 모델 증류 및 가속화(The Mamba in the Llama: Distilling and Accelerating Hybrid Models)", 그리고 "삼바: 효율적인 무제한 컨텍스트 언어 모델링을 위한 단순 하이브리드 상태 공간 모델(Samba: Simple Hybrid State Space Models for Efficient Unlimited Context Language Modeling)" 등이 있습니다. 그런 의미에서 이들도 연산 비용이 더 많이 들고 있습니다. 트랜스포머 기반 LLM에 대한 효율성 조정(Efficiency Tweaks)과 상태 공간 모델에 어텐션을 추가하는 방식으로, 현재 추세가 계속된다면 아마도 중간 지점에서 만나게 될 것입니다. 하지만 확실히 주목할 만한 흥미로운 연구 분야입니다.

**스케일링을 통한 LLM 발전의 한계와 새로운 방향**

연말에는 인터넷 데이터가 더 이상 없기 때문에 대규모 언어 모델(LLM) 스케일링이 "끝났다"는 논의도 있었습니다. 이 논의는 일리야 수츠케버(Ilya Sutskever, 오픈AI 공동 설립자이자 GPT 논문 공동 저자 중 한 명)의 뉴립스(NeurIPS) 강연에서 나왔지만, 안타깝게도 올해는 컨퍼런스에 참석하지 못해서 자세한 내용은 알지 못합니다. 어쨌든, 인터넷은 기하급수적으로(Exponentially Fast) 빠르게 성장하기 때문에 흥미로운 지점입니다. 필자는 "매일 15.87테라바이트의 데이터"가 증가한다는 자료를 찾을 수 있었습니다. 물론, 모든 데이터가 텍스트이거나 LLM 학습에 유용한 것은 아니라는 문제가 있습니다. 그러나 Phi-4에서 보았듯이, 학습 데이터만으로도 큰 도약을 이룰 수 있도록 데이터 큐레이션(Data Curation) 및 정제(Refinement)에는 여전히 많은 기회가 있습니다. 데이터 스케일링을 통한 수확 체감(Diminishing Returns of Scaling)에는 동의합니다. 우리는 아마도 정체(Plateauing)를 향해 가고 있기 때문에 얻는 이득이 더 작아질 것으로 예상합니다. 하지만 이는 다른 개선 기회를 가져오므로 나쁜 일은 아닙니다. 필자가 미래에 많은 이득이 올 것으로 예상하는 한 가지 주목할 만한 영역은 사후 학습(Post-training)입니다. 지난여름 "새로운 LLM 사전 학습 및 사후 학습 패러다임(New LLM Pre-training and Post-training Paradigms)" 기사에서 썼듯이, 최근 LLM 출시와 함께 이 분야의 발전을 이미 맛보았습니다.

**2025년에 기대하는 바**

필자는 올해 다양한 라마 모델(3, 3.1, 3.2)을 직접 다루고 (재)구현하는 것을 정말 즐거웠습니다. 필자는 라마 4(Llama 4) 출시를 매우 기대하고 있으며, 바라건대 필자의 노트북이나 저렴한 클라우드 GPU에서 실험할 수 있는 작고 편리한 크기로도 출시되기를 바랍니다. 또한, 일반적인 챗봇을 생성하는 것보다 특수 목적 모델 미세 조정(Special-Purpose Model Finetuning)에 더 많이 실험하고 싶은 한 해이기도 합니다(이 분야는 이미 상당히 혼잡합니다). 다양한 코드 및 수학 모델(최근 Qwen 2.5 Coder 및 Qwen 2.5 Math가 떠오르는데, 불행히도 이 보고서에서는 아직 다룰 기회가 없었습니다)에서 그 일부를 보았습니다. 어쨌든, 2025년은 또 다른 흥미롭고 빠르게 변화하는 한 해가 될 것이므로, 이 희망 목록과 계획을 계속 이어갈 수 있을 것입니다! 확실히 지루하지는 않을 것입니다!

이 잡지는 개인적인 열정 프로젝트입니다. 필자를 지원하고 싶으신 분들은 필자의 "처음부터 대규모 언어 모델 구축하기(Build a Large Language Model (From Scratch))" 책을 구매해 주시면 감사하겠습니다. (이 책은 다른 곳에서는 찾을 수 없는 상세한 수준으로 LLM이 어떻게 작동하는지 설명하므로 많은 것을 얻으실 수 있을 것이라고 확신합니다.)
"처음부터 대규모 언어 모델 구축하기(Build a Large Language Model (From Scratch))"는 현재 아마존에서 구매 가능합니다.
책을 읽으셨고 잠시 시간을 내주실 수 있다면, 짧은 리뷰(Brief Review)를 남겨주시면 정말 감사하겠습니다. 이는 저희 저자들에게 큰 도움이 됩니다! 여러분의 지원은 큰 의미가 있습니다! 감사합니다!
구독