새로운 한 해 2025년의 활기찬 출발을 기원하며, 마침내 2024년 인공지능 연구의 주요 성과를 다룬 기사의 최종본과 두 번째 파트를 마무리할 수 있게 되었습니다. 이번 보고서는 전문가 혼합 모델(mixture-of-experts models)부터 정밀도(precision)를 위한 새로운 LLM 스케일링 법칙(LLM scaling laws)에 이르기까지, 다양한 핵심 주제들을 깊이 있게 탐구합니다. 이 문서는 연재 기사의 두 번째 부분으로, 2024년 7월부터 12월까지의 하반기 동향에 집중합니다. 1월부터 6월까지의 상반기 내용을 담은 첫 번째 부분은 이전 게시물에서 확인하실 수 있습니다. 이번 연구 요약의 선정 원칙은 전적으로 필자의 개인적인 관점과 올해 가장 깊은 인상을 남긴 내용들을 중심으로 이루어졌습니다. 또한, 특정 LLM 모델 출시에만 국한되지 않고 연구 분야의 다양성을 확보하고자 노력했습니다. 2024년 하반기는 인공지능 기술 발전의 가속화가 두드러졌던 시기로, 특히 대규모 언어 모델(LLM)의 성능 향상과 효율성 증대를 위한 혁신적인 접근법들이 활발히 논의되었습니다. 이러한 역동적인 흐름 속에서 주요 연구 성과들을 살펴보는 것은 미래 AI 발전의 방향을 예측하는 데 중요한 통찰력을 제공할 것입니다. 2025년에도 좋은 일 가득하시길 바라며, 즐거운 독서 되십시오!

---

**7. 7월: 라마 3 모델 무리(The Llama 3 Herd of Models)**

독자 여러분께서는 메타 AI(Meta AI)의 라마 3(Llama 3) 모델과 관련 연구에 대해 이미 잘 알고 계실 것으로 생각됩니다. 이 모델들의 막대한 중요성과 광범위한 활용도를 고려하여, 7월의 주요 내용으로 Grattafiori 연구진이 2024년 7월에 공개한 "라마 3 모델 무리(The Llama 3 Herd of Models)" 논문을 집중적으로 다루고자 합니다. 라마 3 모델 제품군에서 특히 주목할 만한 점은 이전 세대 모델인 라마 2(Llama 2)에 비해 사전 학습(pre-training) 및 사후 학습(post-training) 파이프라인(pipelines)의 정교함이 현저히 개선되었다는 것입니다. 이러한 경향은 비단 라마 3뿐만 아니라 Gemma 2, Qwen 2, Apple의 파운데이션 모델(Foundation Models) 등 최근 출시된 다른 대규모 언어 모델(LLM)에서도 공통적으로 관찰됩니다. 이는 몇 달 전 필자가 작성한 "새로운 LLM 사전 학습 및 사후 학습 패러다임(New LLM Pre-training and Post-training Paradigms)" 기사에서 이미 상세히 설명한 바 있습니다. 라마 3와 같은 오픈 가중치(open-weight) 모델의 등장은 LLM 연구 및 상업적 응용 분야에 지대한 영향을 미쳤으며, 개발자들이 고성능 모델에 접근하고 실험할 수 있는 기회를 민주화하는 데 크게 기여했습니다.

**7.1 라마 3 아키텍처(architecture) 요약**

라마 3(Llama 3)는 초기에 80억 개와 700억 개 매개변수(parameter) 규모로 세상에 나왔지만, 개발팀은 지속적인 개선 작업을 통해 라마 3.1, 3.2, 3.3과 같은 후속 버전을 연이어 선보였습니다. 각 버전별 매개변수 크기는 아래에 요약되어 있습니다:

*   **Llama 3 (2024년 4월)**
    *   80억 개 매개변수
    *   700억 개 매개변수
*   **Llama 3.1 (2024년 7월, 논문에서 논의됨)**
    *   80억 개 매개변수
    *   700억 개 매개변수
    *   4050억 개 매개변수
*   **Llama 3.2 (2024년 9월)**
    *   10억 개 매개변수
    *   30억 개 매개변수
    *   110억 개 매개변수 (시각 기능 활성화)
    *   900억 개 매개변수 (시각 기능 활성화)
*   **Llama 3.3 (2024년 12월)**
    *   700억 개 매개변수

전반적으로 라마 3 아키텍처는 이전 모델인 라마 2(Llama 2)와 매우 유사한 구조를 가집니다. 핵심적인 변경 사항은 확장된 어휘(vocabulary) 사용과 더불어 소형 모델 변형에 적용된 그룹화된 쿼리 어텐션(grouped-query attention) 메커니즘의 도입입니다. 이러한 차이점 요약은 아래 그림에 제시되어 있습니다. 더 큰 어휘는 모델이 더 다양한 언어와 도메인의 텍스트를 효율적으로 토큰화하고 처리할 수 있게 하여, 언어 이해 및 생성 능력을 향상시키는 데 기여합니다. 또한, 그룹화된 쿼리 어텐션은 특히 작은 모델에서 추론(inference) 속도를 크게 개선하여, 자원 제약이 있는 환경에서도 효율적인 배포를 가능하게 합니다. 이러한 아키텍처적 개선은 라마 3가 다양한 사용 사례에서 견고한 성능을 발휘하는 기반이 됩니다.

제 "처음부터 대규모 언어 모델 구축하기(Build a Large Language from Scratch)" 책의 보너스 자료에서 발췌한 라마 2 대 3 비교. 아키텍처 세부 사항에 관심이 있다면, 모델을 처음부터 구현하고 사전 학습된 가중치(pretrained weights)를 로드하여 건전성 검사(sanity check)를 하는 것이 좋은 학습 방법입니다. 저는 GPT-2를 라마 2, 라마 3, 라마 3.1, 라마 3.2로 변환하는 처음부터 구현(from-scratch implementation)을 담은 GitHub 저장소를 가지고 있습니다. 제 "처음부터 대규모 언어 모델 구축하기(Build a Large Language from Scratch)" 책의 보너스 자료에서 발췌한 GPT-2를 라마 2, 라마 3, 라마 3.1, 라마 3.2로 변환.

**7.3 라마 3 학습(training)**

라마 2(Llama 2)와 비교했을 때, 또 하나의 두드러진 개선점은 라마 3(Llama 3)가 무려 15조 개의 토큰(tokens)으로 학습되었다는 사실입니다. 이는 이전 세대 모델에 비해 학습 데이터의 규모가 폭발적으로 증가했음을 의미하며, 모델의 일반화 능력과 지식 습득에 지대한 영향을 미칩니다.

다양한 모델의 학습 세트(training set) 크기 비교.

사전 학습(pre-training) 과정은 이제 다단계(multi-staged)로 진행됩니다. 이는 다양한 유형의 데이터를 여러 단계에 걸쳐 체계적으로 학습시키거나, 특정 학습 목표에 따라 데이터의 가중치를 조절하는 커리큘럼 학습(curriculum learning)과 같은 고도화된 전략을 포함합니다. 이 논문은 주로 라마 3.1(Llama 3.1)에 초점을 맞추고 있으며, 간결함을 위해 아래 그림에 사전 학습 기술을 요약했습니다. 이러한 다단계 접근 방식은 모델이 초기 단계에서 넓은 범위의 일반 지식을 습득하고, 후기 단계에서는 더 정교하고 특화된 패턴을 학습하도록 유도하여 전반적인 성능을 극대화합니다.

라마 3.1 사전 학습에 사용된 기술 요약.

사후 학습(post-training) 과정에서는 라마 2(Llama 2)와 달리 RLHF-PPO 방식 대신 DPO(Direct Preference Optimization)로의 전환이 중요한 변곡점으로 작용했습니다. DPO는 RLHF(Reinforcement Learning from Human Feedback)의 복잡한 강화 학습(reinforcement learning) 부분을 단순화하여, 안정성을 높이고 학습 과정을 효율화하는 장점을 가집니다. 이 방법들도 아래 그림에 요약되어 있습니다. DPO는 인간의 선호도 데이터를 직접적으로 활용하여 모델의 정렬(alignment)을 개선하므로, 더욱 안정적이고 예측 가능한 성능 향상을 기대할 수 있습니다.

라마 3.1 사전 학습에 사용된 기술 요약.

이 기사에서 다룰 논문이 5개 더 남아있으므로, 간결함을 위해 추가 세부 사항 및 다른 모델과의 비교는 제 이전 기사 중 하나인 "새로운 LLM 사전 학습 및 사후 학습 패러다임(New LLM Pre-training and Post-training Paradigms)"으로 미루겠습니다. 이러한 정교화된 학습 기술의 발전은 LLM이 더욱 복잡한 작업을 수행하고 인간의 의도에 더 잘 부합하는 응답을 생성하는 데 필수적인 요소가 되고 있습니다.

Ahead of AI는 독자 지원 출판물입니다. 새로운 게시물을 받고 제 작업을 지원하려면 무료 또는 유료 구독자가 되는 것을 고려해 보세요.
구독

**7.4 멀티모달 라마(Multimodal Llamas)**

라마 3.2(Llama 3.2) 모델 역시 다중 모드(multimodal) 지원 기능을 탑재하여 공개되었습니다. 그러나 실제 환경에서 이러한 멀티모달 기능이 광범위하게 활용되거나 깊이 있게 논의되는 사례는 아직 제한적인 것으로 보입니다. 이는 초기 단계의 오픈 가중치 모델에서 멀티모달 기능의 배포 복잡성, 높은 계산 비용, 그리고 아직 명확하게 정의되지 않은 킬러 애플리케이션(killer applications)의 부재 때문일 수 있습니다. 독점 모델(proprietary models)들이 멀티모달 기능에서 두각을 나타내는 것과 달리, 오픈 소스 커뮤니티에서는 아직 초기 단계의 탐색이 이루어지고 있는 상황입니다. 멀티모달 기술에 대해서는 이 기사의 9월 섹션에서 다시 자세히 다룰 것입니다.

**7.5 라마 3의 영향 및 사용**

라마 3(Llama 3)가 공개된 지 이미 반년 이상 지났음에도 불구하고, 라마 계열 모델들은 여전히 가장 폭넓게 알려지고 활용되는 오픈 가중치 LLM(open-weight LLMs)으로 자리매김하고 있습니다(이는 필자의 주관적인 관찰에 근거합니다). 이 모델들은 상대적으로 이해하기 쉽고 활용하기 용이합니다. 이들의 인기는 라마 브랜드의 높은 인지도를 바탕으로, 다양한 일반적인 작업에서 안정적인 성능을 제공하며 미세 조정(finetune)이 간편하다는 점이 복합적으로 작용한 결과로 분석됩니다. 메타 AI(Meta AI)는 또한 라마 3 모델을 반복적으로 개선하여 3.1, 3.2, 그리고 현재 3.3 버전을 출시함으로써 이러한 모멘텀을 성공적으로 유지했습니다. 이 버전들은 온디바이스 시나리오(on-device scenarios, 10억 개 매개변수)부터 고성능 애플리케이션(high-performance applications, 4000억 개 매개변수)에 이르기까지, 다양한 사용 사례를 충족시키기 위해 폭넓은 크기 스펙트럼을 포괄합니다. 현재 Olmo 2, Qwen 2.5, Gemma 2, Phi-4 등 경쟁력 있는 많은 오픈 소스(open-source) 및 오픈 가중치 LLM이 등장하고 있지만, 필자는 라마가 Anthropic Claude, Google Gemini, DeepSeek 등과의 치열한 경쟁 속에서도 ChatGPT가 인기를 유지한 것처럼, 대다수 사용자에게 기본 모델(default model)로서의 지위를 계속 유지할 것이라고 믿습니다. 이는 라마 모델이 제공하는 접근성, 성능, 그리고 활발한 커뮤니티 지원이 결합된 결과입니다. 개인적으로는 2025년에 언젠가 출시되기를 바라는 라마 4(Llama 4)에 대해 큰 기대를 가지고 있습니다.

---

**8. 8월: 추론 시간 컴퓨팅(inference-time compute) 확장을 통한 LLM 개선**

이번 달에 필자가 주목한 연구는 2024년 8월에 발표된 "LLM 테스트 시간 컴퓨팅을 최적으로 확장하는 것이 모델 매개변수를 확장하는 것보다 더 효과적일 수 있다(Scaling LLM Test-Time Compute Optimally can be More Effective than Scaling Model Parameters)"라는 제목의 논문입니다. 이 논문은 추론 시간(inference time, 즉 배포(deployment) 시점) 동안 LLM 응답을 개선하는 데 대한 매우 흥미로운 통찰력을 제공하는, 잘 쓰여지고 상세한 연구이기 때문에 선정했습니다. 이 연구는 단순히 모델의 크기를 키우는 것 외에, 배포 단계에서 컴퓨팅 자원을 어떻게 효율적으로 활용하여 LLM의 성능을 극대화할 수 있는지에 대한 중요한 질문을 던집니다. 이는 LLM 배포의 비용 효율성과 실용성에 직접적으로 연결되는 핵심적인 문제입니다.

**8.1 더 많은 테스트 시간 컴퓨팅(test-time computation)을 사용하여 출력 개선**

해당 논문의 핵심 전제는 테스트 시간 컴퓨팅의 증대가 LLM의 결과물 품질을 어떻게 향상시킬 수 있는지, 그리고 그 활용 방안을 탐구하는 데 있습니다. 대략적인 비유를 들자면, 사람이 어려운 작업을 수행할 때 더 많은 생각할 시간을 주면 더 나은 응답을 생성할 수 있다고 가정해 봅시다. 이와 유사하게, 대규모 언어 모델(LLM) 역시 응답을 도출하는 과정에 더 많은 시간과 자원이 할당될 경우, 더욱 우수한 결과물을 생성할 수 있을 것이라는 가설입니다. 이는 Chain-of-Thought(사고의 사슬)나 Tree-of-Thought(사고의 나무)와 같은 기법들이 LLM에게 "생각할 시간"을 더 주어 추론 능력을 향상시키는 것과 일맥상통합니다. 더 기술적인 용어로 말하면, 연구자들은 추론(inference) 중에 추가 컴퓨팅(compute)이 사용될 경우 모델이 학습된 것보다 얼마나 더 잘 수행될 수 있는지를 알아내려고 합니다. 또한, 연구자들은 고정된 컴퓨팅 예산(compute budget)이 주어졌을 때, 테스트 시간에 더 많은 컴퓨팅을 사용하는 것이 모델을 추가로 사전 학습(pre-training)하는 데 컴퓨팅을 사용하는 것보다 결과를 개선할 수 있는지 여부도 살펴보았습니다. 이는 LLM 배포 시 자원 할당 전략에 대한 중요한 시사점을 제공합니다. 하지만 이에 대해서는 나중에 더 자세히 설명하겠습니다.

**8.2 테스트 시간 컴퓨팅(test-time computation) 기술 최적화**

이 연구는 테스트 시간 컴퓨팅을 증진하고 최적화하는 기법들을 매우 상세하게 다루고 있으므로, 대규모 언어 모델(LLM)의 실제 배포(예: 앞서 언급된 라마 모델군)에 진지하게 접근하는 관계자들에게는 이 논문의 정독을 강력히 권장합니다. 요약하자면, 테스트 시간 컴퓨팅을 확장하는 두 가지 주요 방법은 다음과 같습니다.

1.  **여러 솔루션을 생성하고 프로세스 기반 검증자 보상 모델(process-based verifier reward model, 별도로 학습되어야 함)을 사용하여 최상의 응답을 선택하는 것.** 이 검증자 보상 모델은 생성된 여러 후보 응답들의 품질을 평가하고 순위를 매기는 역할을 합니다. 별도의 학습이 필요한 이유는 모델이 단순히 정답을 생성하는 것을 넘어, 응답을 생성하는 "과정" 자체의 우수성을 판단해야 하기 때문입니다.
2.  **모델의 응답 분포(response distribution)를 적응적으로 업데이트하는 것.** 이는 본질적으로 추론 생성(inference generation) 중에 응답을 수정하는 것을 의미합니다(이 또한 별도의 모델이 필요합니다). 이 방법은 모델이 스스로의 응답을 검토하고 개선하는 자기 수정(self-correction) 메커니즘과 유사하며, 반복적인 정제를 통해 결과물의 품질을 높입니다.

카테고리 1에 대한 간단한 예시를 들자면: 테스트 시간 컴퓨팅을 개선하는 한 가지 단순한 방법은 N개 중 최적(best-of-N) 샘플링을 사용하는 것입니다. 이는 LLM이 여러 답변을 병렬로 생성하게 한 다음, 검증자 보상 모델(verifier reward model)을 기반으로 최상의 답변을 선택하는 것을 의미합니다. N개 중 최적도 단지 한 가지 예시일 뿐입니다. 아래 그림에 나와 있듯이, 빔 탐색(beam-search), 선행 탐색(lookahead-search), N개 중 최적(best-of-N)과 같은 여러 탐색 알고리즘(search algorithms)이 이 범주에 속합니다. 이러한 탐색 기법들은 더 넓은 탐색 공간을 탐색하여 잠재적으로 더 나은 응답을 찾을 수 있지만, 그만큼 컴퓨팅 자원 소모가 증가합니다.

다양한 탐색 기반 방법은 프로세스-보상 기반 모델(process-reward-based model)에 의존하여 최상의 답변을 선택합니다.
LLM 테스트 시간 컴퓨팅 논문에서 발췌한 주석이 달린 그림, https://arxiv.org/abs/2408.03314

카테고리 2에 속하는 또 다른 접근 방식은 아래 그림에 나와 있듯이 모델의 응답을 순차적으로 수정하는 것입니다. 이 방식은 모델이 초기 응답을 생성한 후, 추가적인 컴퓨팅을 사용하여 해당 응답을 분석하고 개선하는 과정을 반복합니다.

순차적 수정 접근 방식.
LLM 테스트 시간 컴퓨팅 논문에서 발췌한 주석이 달린 그림, https://arxiv.org/abs/2408.03314

그렇다면 어떤 접근 방식이 가장 효과적일까요? 유감스럽게도 모든 상황에 적용 가능한 단 하나의 해결책은 존재하지 않습니다. 이는 기반이 되는 LLM과 특정 문제 유형, 혹은 질의(query)의 성격에 따라 달라집니다. 예를 들어, 수정 기반 접근 방식은 더 어려운 질문에서 더 나은 성능을 보이지만, 쉬운 질문에서는 오히려 성능을 저해할 수 있습니다. 이 논문에서는 질의의 난이도(difficulty level)를 평가한 다음 적절한 전략을 선택하는 모델을 기반으로 "최적의" 전략을 개발했습니다. 이는 LLM 배포 시 동적이고 상황에 맞는 자원 관리의 중요성을 강조합니다.

**8.3 테스트 시간 컴퓨팅(test-time computation) 대 더 큰 모델 사전 학습(pretraining)**

주어진 고정된 컴퓨팅 예산(compute budget) 하에서, 더 큰 모델을 활용하는 것과 추론 시간 예산(inference-time budget)을 확장하는 것 중 어떤 전략이 더 큰 성과를 가져올 수 있는지에 대한 질문은 매우 흥미롭습니다. 여기서 쿼리(query)에 지불하는 비용은 동일하다고 가정합니다. 왜냐하면 추론(inference)에서 큰 모델을 실행하는 것이 작은 모델보다 더 비용이 많이 들기 때문입니다. 연구 결과에 따르면, 어려운 질문의 경우, 이전에 논의된 추론 스케일링 전략(inference scaling strategies)을 통해 추가 추론 컴퓨팅을 얻는 작은 모델보다 본질적으로 더 큰 모델이 더 나은 성능을 보인다는 것을 발견했습니다. 이는 복잡한 문제 해결에는 모델 자체의 내재된 지식과 추론 깊이가 더 중요할 수 있음을 시사합니다. 그러나 쉽고 중간 난이도의 질문의 경우, 추론 시간 컴퓨팅을 적절히 활용함으로써 동일한 컴퓨팅 예산으로 14배 더 큰 모델의 성능과 일치시킬 수 있었습니다! 이는 특정 작업에서 모델 크기 증가에 대한 비용 효율적인 대안이 될 수 있음을 보여주며, LLM 배포의 경제성에 중요한 영향을 미칩니다.

**8.4 테스트 시간 컴퓨팅(test-time compute) 스케일링의 미래 관련성**

라마 3(Llama 3)와 같은 공개 가중치 모델(open-weight models)을 운용할 때, 우리는 흔히 모델이 자체적으로 응답을 생성하도록 내버려 두는 경향이 있습니다. 그러나 이 논문이 강조하듯이, 더 많은 추론 컴퓨팅(inference compute)을 할당함으로써 응답 품질을 크게 향상시킬 수 있습니다. (모델을 배포하고 있다면, 이 논문은 반드시 읽어야 할 논문입니다.) 물론, 크고 비싼 모델에 대한 추론 컴퓨팅 예산을 늘리면 운영 비용이 더욱 증가합니다. 하지만 질의(query)의 복잡성에 따라 선별적으로 적용된다면, 특정 응답의 품질과 정확도를 현저히 높일 수 있으며, 이는 대다수 사용자들에게 분명히 높은 평가를 받을 것입니다. (OpenAI, Anthropic, Google과 같은 선두 기업들은 이미 이러한 기술을 내부적으로 활용하고 있다고 가정하는 것이 안전합니다.) 이러한 연구는 LLM이 단순한 챗봇을 넘어 더욱 신뢰할 수 있고 유용한 도구가 되기 위한 필수적인 단계입니다. 또 다른 설득력 있는 사용 사례는 더 작고 온디바이스(on-device) LLM의 성능을 향상시키는 것입니다. 애플 인텔리전스(Apple Intelligence)와 마이크로소프트의 코파일럿 PC(Copilot PCs)에 대한 대규모 발표와 투자를 보았듯이, 필자는 이것이 앞으로 몇 달, 몇 년 동안 뜨거운 주제로 남을 것이라고 생각합니다. 온디바이스 LLM은 개인 정보 보호 강화와 클라우드 비용 절감이라는 이점을 제공하며, 추론 시간 최적화 기술은 이러한 소형 모델의 실용성을 더욱 높여줄 것입니다.

---

**9. 9월: 멀티모달 LLM 패러다임(multimodal LLM paradigms) 비교**

다중 모드 대규모 언어 모델(Multimodal LLMs)은 2024년에 상당한 발전을 이룰 것으로 예측했던 핵심 분야 중 하나였습니다. 그리고 실제로 올해 더 많은 오픈 가중치 LLM(open-weight LLMs)이 출시되었습니다! 멀티모달 LLM은 텍스트에만 국한되지 않고 다양한 형태의 데이터를 이해하고 생성할 수 있게 함으로써, 인공지능의 응용 범위를 획기적으로 확장하는 중요한 진화 단계입니다.

다양한 입력 양식(input modalities, 오디오, 텍스트, 이미지, 비디오)을 받아들이고 텍스트를 출력 양식(output modality)으로 반환할 수 있는 멀티모달 LLM의 예시.

특히 필자에게 인상 깊었던 논문은 Dai와 동료들이 발표한 NVIDIA의 "NVLM: 개방형 프론티어급 멀티모달 LLM(Open Frontier-Class Multimodal LLMs, 2024년 9월)"이었습니다. 이 논문은 두 가지 주요 멀티모달 패러다임을 훌륭하게 비교하고 있기 때문입니다. 이 연구는 단순히 새로운 모델을 제시하는 것을 넘어, 멀티모달 LLM을 설계하는 근본적인 접근 방식들을 체계적으로 분석하여 이 분야의 이해를 심화시켰습니다.

**9.1 멀티모달 LLM 패러다임(Multimodal LLM paradigms)**

다중 모드 대규모 언어 모델(Multimodal LLM)을 구성하는 데는 크게 두 가지 핵심적인 방법론이 존재합니다:

*   **방법 A: 통합 임베딩 디코더 아키텍처(Unified Embedding Decoder Architecture) 접근 방식;**
*   **방법 B: 교차 모달리티 어텐션 아키텍처(Cross-modality Attention Architecture) 접근 방식.**

멀티모달 LLM 아키텍처를 개발하는 두 가지 주요 접근 방식.

위 그림에서 설명된 바와 같이, 통합 임베딩-디코더 아키텍처(Unified Embedding-Decoder Architecture, 방법 A)는 GPT-2 또는 라마 3.2(Llama 3.2)와 같은 수정되지 않은 LLM 아키텍처와 유사한 단일 디코더 모델(single decoder model)에 의존합니다. 이 방식은 이미지를 텍스트 토큰(text tokens)과 동일한 임베딩 크기(embedding size)를 가지는 토큰으로 변환한 뒤, LLM이 결합된 텍스트 및 이미지 입력 토큰(image input tokens)을 함께 처리하도록 합니다. 이 접근 방식은 비교적 구현이 간단하며, 기존 텍스트 LLM의 구조를 크게 변경하지 않고도 멀티모달 기능을 추가할 수 있다는 장점이 있습니다. 이미지 변환에는 CLIP(Contrastive Language–Image Pre-training)과 같은 사전 학습된 시각 인코더(vision encoder)가 주로 사용됩니다.

대조적으로, 교차 모달리티 어텐션 아키텍처(Cross-Modality Attention Architecture, 방법 B)는 교차 어텐션 메커니즘(cross-attention mechanism)을 통합하여 어텐션 레이어(attention layer) 내에서 이미지 및 텍스트 임베딩(image and text embeddings)을 직접 통합합니다. 이 방법은 두 가지 모달리티 간의 상호작용을 더욱 긴밀하게 모델링할 수 있어, 보다 깊이 있는 다중 모드 이해를 가능하게 합니다. 하지만 아키텍처의 복잡성이 증가하고, 교차 어텐션 연산에 따른 추가적인 계산 비용이 발생할 수 있습니다. 추가 세부 사항에 관심이 있다면, 올해 초 이 두 가지 방법을 단계별로 설명하는 멀티모달 LLM에 대한 전체 기사를 작성했습니다: 멀티모달 LLM 이해하기 -- 주요 기술 및 최신 모델 소개(Understanding Multimodal LLMs -- An introduction to the main techniques and latest models).

**9.2 엔비디아의 하이브리드(hybrid) 접근 방식**

올해 이루어진 수많은 다중 모드 개발들을 종합적으로 고려할 때, 엔비디아(NVIDIA)의 "NVLM: 개방형 프론티어급 멀티모달 LLM(Open Frontier-Class Multimodal LLMs)" 논문은 이러한 다중 모드 접근법들을 포괄적으로 공정하게 비교(apples-to-apples comparison)했다는 점에서 특히 필자의 눈길을 사로잡았습니다. 이 연구는 특정 단일 방법에 초점을 맞추기보다는 다음 세 가지 접근 방식을 직접적으로 비교했습니다:

*   **방법 A: 통합 임베딩 디코더 아키텍처("디코더 전용 아키텍처(decoder-only architecture)," NVLM-D),**
*   **방법 B: 교차 모달리티 어텐션 아키텍처("교차 어텐션 기반 아키텍처(cross-attention-based architecture)," NVLM-X),**
*   **하이브리드 접근 방식(hybrid approach, NVLM-H).**

세 가지 멀티모달 접근 방식 개요. (NVLM: 개방형 프론티어급 멀티모달 LLM 논문에서 발췌한 주석이 달린 그림: https://arxiv.org/abs/2409.11402)

위 그림에 요약된 바와 같이, NVLM-D는 방법 A와 일치하며, NVLM-X는 이전에 논의된 방법 B에 해당합니다. 하이브리드 모델(NVLM-H)은 두 접근 방식의 장점을 결합합니다: 먼저 이미지 썸네일(image thumbnail)을 입력으로 받아들이고, 이어서 교차 어텐션(cross-attention)을 통해 처리되는 동적인 수의 패치(patches)를 통해 더 미세한 고해상도 세부 정보(finer high-resolution details)를 캡처합니다. 이러한 하이브리드 접근 방식은 이미지의 전반적인 맥락을 빠르게 파악하는 동시에, 필요한 경우 고해상도 정보까지 효과적으로 통합할 수 있다는 장점을 가집니다. 요약하자면, 주요 발견 사항은 다음과 같습니다:

*   **NVLM-X:** 고해상도 이미지에 대해 우수한 계산 효율성(computational efficiency)을 제공합니다. 이는 복잡한 시각 정보를 처리하는 데 있어 자원 활용의 효율성이 높음을 의미합니다.
*   **NVLM-D:** OCR(광학 문자 인식) 관련 작업에 대해 더 높은 정확도를 제공합니다. 이는 텍스트와 유사한 방식으로 이미지를 처리하는 구조가 특정 작업에 더 유리할 수 있음을 보여줍니다.
*   **NVLM-H:** 최적화된 성능을 구현하기 위해 앞서 언급된 두 가지 접근 방식의 강점들을 효과적으로 융합합니다. 이는 다양한 시나리오에서 균형 잡힌 고성능을 달성하기 위한 실용적인 해법을 제시합니다.

**9.3 2025년의 멀티모달 LLM(Multimodal LLMs)**

다중 모드 대규모 언어 모델(Multimodal LLMs)은 매우 매력적인 연구 분야입니다. 저는 이것이 기존 텍스트 기반 LLM의 한계를 넘어선, 지극히 논리적인 진화의 단계라고 판단합니다. OpenAI, Google, Anthropic과 같은 대부분의 LLM 서비스 제공업체는 이미 이미지와 같은 멀티모달 입력(multimodal inputs)을 지원합니다. 개인적으로 필자의 업무에서 멀티모달 기능이 필요한 경우는 아마 1% 정도일 것입니다(주로 "테이블을 마크다운 형식으로 추출해줘"와 같은 특정 데이터 추출 작업에서 유용합니다). 필자는 오픈 가중치 LLM의 기본값이 순수하게 텍스트 기반으로 유지될 것으로 예상합니다. 이는 멀티모달 기능을 추가할 때 발생하는 복잡성과 계산 비용 증가를 최소화하기 위함입니다. 그러나 동시에 도구(tooling)와 API가 발전함에 따라 오픈 가중치 LLM에서도 더 많은 멀티모달 옵션과 광범위한 활용 사례를 보게 될 것이라고 생각합니다. 특히 오픈 소스 커뮤니티는 특정 도메인이나 사용 사례에 최적화된 경량 멀티모달 모델을 개발하는 데 집중할 것으로 보이며, 이는 멀티모달 AI의 민주화에 기여할 것입니다.

---

**10. 10월: OpenAI o1의 추론 능력(reasoning capabilities) 복제**

10월에 필자가 선정한 논문은 Quin 연구팀이 2024년 10월에 공개한 "O1 복제 여정: 전략적 진행 보고서 -- 파트 1(O1 Replication Journey: A Strategic Progress Report -- Part 1)"입니다. OpenAI ChatGPT의 o1(그리고 현재 o3)은 LLM의 추론 작업(reasoning tasks) 성능을 개선하는 데 있어 패러다임 전환(paradigm shift)을 나타내는 것으로 보이며 상당한 인기를 얻었습니다. o1과 o3는 단순히 정보를 암기하고 재생산하는 것을 넘어, 복잡한 문제에 대해 단계적으로 사고하고 해결책을 도출하는 능력을 보여주었습니다. OpenAI o1의 정확한 세부 사항은 공개되지 않았으며, 여러 논문에서 이를 설명하거나 복제하려고 시도했습니다. 그렇다면 왜 이 논문을 선택했을까요? 해당 논문의 독특한 구성 방식과 학술 연구(academic research)의 현주소에 대한 폭넓은 철학적 논의가 필자의 깊은 공감을 얻었습니다. 다시 말해, 이 논문에는 눈에 띄는 독특한 점이 있었고, 그것이 흥미로운 선택으로 이어졌습니다. 이는 고급 LLM의 "블랙박스(black box)" 특성과 이러한 모델의 작동 원리를 이해하려는 학계의 노력을 잘 보여줍니다.

**10.1 지름길 학습(Shortcut learning) 대 여정 학습(journey learning)**

이 연구의 핵심 주장 중 하나는, 아래 그림에 제시된 바와 같이, O1이 지름길 학습(shortcut learning)과 대비되는 '여정 학습(journey learning)'이라는 독특한 과정을 활용한다는 연구진의 가설입니다. 전통적으로 LLM은 올바른 해결 경로(shortcut learning)로 학습됩니다. 즉, 문제와