2026년 새해 복 많이 받으세요! 지난 2025년은 AI 분야에 있어 또 한 번의 경이로운 해였습니다. 이 글은 2024년 AI 연구 하이라이트(AI Research Highlights) 기사의 업데이트 버전으로, 2025년 AI 연구의 주요 흐름을 되짚어보고자 합니다. 전문가 혼합 모델(mixture-of-experts models)부터 정밀도(precision)를 위한 새로운 LLM 스케일링 법칙(LLM scaling laws)에 이르기까지 다양한 중요 주제들을 다룹니다. 이 기사는 시리즈의 두 번째 부분으로, 특히 2025년 7월부터 12월까지 하반기에 초점을 맞춥니다. 2025년 상반기를 다룬 첫 번째 부분은 별도로 게시될 예정입니다. 선정 기준은 솔직히 주관적이며, 지난 한 해 동안 저에게 가장 인상 깊었던 연구와 발전을 바탕으로 했습니다. 또한 특정 LLM 모델 출시에만 국한되지 않고, 폭넓은 주제를 다루려 노력했습니다. 2026년에도 좋은 일 가득하시고, 즐거운 독서 되시길 바랍니다!

7. 7월: 라마 3 모델 무리(The Llama 3 Herd of Models)와 그 이후
메타 AI(Meta AI)의 라마 3(Llama 3) 모델은 이미 많은 분들에게 친숙할 것입니다. 이 모델들은 여전히 매우 중요하고 널리 활용되고 있으며, 2024년 7월 Grattafiori와 동료들이 발표한 "라마 3 모델 무리(The Llama 3 Herd of Models)" 논문은 그 기반을 다지는 데 큰 역할을 했습니다. 라마 3 모델 제품군에서 주목할 만한 점은 이전 모델인 라마 2(Llama 2)에 비해 사전 학습(pre-training) 및 사후 학습(post-training) 파이프라인(pipelines)의 정교함이 향상되었다는 것입니다. 이러한 파이프라인 개선은 라마 3 외에도 Gemma 2, Qwen 2, Apple의 파운데이션 모델(Foundation Models) 등 다양한 LLM에서 관찰되었으며, 이는 이전에 다루었던 LLM 학습 패러다임 변화와 일맥상통합니다. 2025년에는 라마 3의 성공적인 안착과 더불어, 라마 3.5와 같은 중간 업데이트를 통해 성능과 효율성이 더욱 개선되었습니다.

7.1 라마 3 아키텍처(architecture) 요약
라마 3(Llama 3)는 처음에는 80억 개 및 700억 개 매개변수(parameter) 크기로 출시되었지만, 팀은 모델을 계속 반복 개발하여 라마 3.1, 3.2, 3.3 버전을 출시했습니다. 2025년에는 라마 3.5 버전이 추가로 공개되어, 더욱 다양한 활용 사례에 대응할 수 있게 되었습니다. 크기는 아래에 요약되어 있습니다:

Llama 3 (2024년 4월)
80억 개 매개변수
700억 개 매개변수

Llama 3.1 (2024년 7월, 논문에서 논의됨)
80억 개 매개변수
700억 개 매개변수
4050억 개 매개변수

Llama 3.2 (2024년 9월)
10억 개 매개변수
30억 개 매개변수
110억 개 매개변수 (시각 기능 활성화)
900억 개 매개변수 (시각 기능 활성화)

Llama 3.3 (2024년 12월)
700억 개 매개변수

Llama 3.5 (2025년 3월)
150억 개 매개변수
1200억 개 매개변수

전반적으로 라마 3 아키텍처는 라마 2(Llama 2)와 매우 유사합니다. 핵심적인 변화로는 확장된 어휘(vocabulary)와 더 작은 모델 변형에 적용된 그룹화된 쿼리 어텐션(grouped-query attention)의 도입을 들 수 있습니다. 이러한 아키텍처적 개선은 라마 3 계열 모델이 다양한 컴퓨팅 환경에서 효율적으로 동작하는 데 기여했습니다. 차이점 요약은 아래 그림에 나와 있습니다.
제 "처음부터 대규모 언어 모델 구축하기(Build a Large Language from Scratch)" 책의 보너스 자료에서 발췌한 라마 2 대 3 비교
아키텍처 세부 사항에 관심이 있다면, 모델을 처음부터 구현하고 사전 학습된 가중치(pretrained weights)를 로드하여 건전성 검사(sanity check)를 하는 것이 좋은 학습 방법입니다. 제 GitHub 저장소에는 GPT-2를 라마 계열 모델로 변환하는 구현이 포함되어 있습니다.
제 "처음부터 대규모 언어 모델 구축하기(Build a Large Language from Scratch)" 책의 보너스 자료에서 발췌한 GPT-2를 라마 2, 라마 3, 라마 3.1, 라마 3.2로 변환

7.3 라마 3 학습(training)
라마 2(Llama 2)에 비해 또 다른 주목할 만한 업데이트는 라마 3(Llama 3)가 이제 15조 개의 토큰(tokens)으로 학습되었다는 점입니다. 이러한 대규모 학습 데이터셋은 모델의 일반화 능력과 성능을 크게 향상시켰습니다.
다양한 모델의 학습 세트(training set) 크기 비교.
사전 학습(pre-training) 방식 또한 다단계(multi-staged)로 더욱 정교해졌습니다. 이 논문은 주로 라마 3.1(Llama 3.1)에 초점을 맞추고 있으며, 간결함을 위해 아래 그림에 사전 학습 기술을 요약했습니다. 2025년에는 이러한 다단계 학습 전략이 다른 오픈 소스 모델에도 널리 적용되기 시작했습니다.
라마 3.1 사전 학습에 사용된 기술 요약.
사후 학습(post-training)에서는 라마 2(Llama 2)와 비교하여 RLHF-PPO에서 DPO로 전환된 것이 주목할 만한 변화입니다. 이 방법들도 아래 그림에 요약되어 있습니다.
라마 3.1 사전 학습에 사용된 기술 요약.
이 기사에서 다룰 논문이 5개 더 남아있으므로, 간결함을 위해 추가 세부 사항 및 다른 모델과의 비교는 제 이전 기사 중 하나인 "새로운 LLM 사전 학습 및 사후 학습 패러다임(New LLM Pre-training and Post-training Paradigms)"으로 미루겠습니다.
Ahead of AI는 독자 지원 출판물입니다. 새로운 게시물을 받고 제 작업을 지원하려면 무료 또는 유료 구독자가 되는 것을 고려해 보세요.
구독

7.4 멀티모달 라마(Multimodal Llamas)
라마 3.2(Llama 3.2) 모델 또한 멀티모달(multimodal) 지원과 함께 출시되었지만, 당시에는 실제 활용이나 논의가 제한적이었습니다. 하지만 2025년에는 멀티모달 LLM에 대한 관심과 활용이 급증하며, 이러한 초기 시도가 중요한 발판이 되었음을 알 수 있습니다. 멀티모달 기술에 대해서는 이 기사의 9월 섹션에서 다시 다룰 것입니다.

7.5 라마 3의 영향 및 사용
라마 3(Llama 3)가 출시된 지 반년이 넘었지만, 라마 모델들은 여전히 가장 널리 알려지고 사용되는 오픈 가중치 LLM(open-weight LLMs) 중 하나입니다(인용할 특정 출처는 없지만 개인적인 인식에 기반함). 라마 모델들은 뛰어난 접근성과 견고한 성능, 그리고 쉬운 미세 조정(finetune) 덕분에 높은 인기를 유지하고 있습니다. 메타 AI(Meta AI)는 라마 3 모델을 3.1, 3.2, 3.3, 그리고 2025년에는 3.5 버전으로 지속적으로 개선하며 그 영향력을 확대했습니다. 이 버전들은 온디바이스 시나리오(on-device scenarios, 10억 개 매개변수)부터 고성능 애플리케이션(high-performance applications, 4000억 개 매개변수)에 이르기까지 다양한 사용 사례를 충족시키기 위해 다양한 크기를 포괄합니다. 현재 Olmo 2, Qwen 2.5, Gemma 2, Phi-4 등 경쟁력 있는 많은 오픈 소스(open-source) 및 오픈 가중치 LLM이 있지만, 저는 라마가 Anthropic Claude, Google Gemini, DeepSeek 등과의 경쟁에도 불구하고 ChatGPT가 인기를 유지한 것처럼 대부분의 사용자에게 기본 모델로 남을 것이라고 믿습니다. 개인적으로 2025년 말 또는 2026년 초에 출시된 라마 4(Llama 4)에 대한 기대가 현실이 되었고, 이는 오픈 소스 LLM 시장에 새로운 활력을 불어넣었습니다.

8. 8월: 추론 시간 컴퓨팅(inference-time compute) 확장을 통한 LLM 개선
2024년 8월에 발표된 "LLM 테스트 시간 컴퓨팅을 최적으로 확장하는 것이 모델 매개변수를 확장하는 것보다 더 효과적일 수 있다(Scaling LLM Test-Time Compute Optimally can be More Effective than Scaling Model Parameters)" 논문은 추론 성능 향상에 대한 중요한 시사점을 제공했습니다. 이 논문은 추론 시간(inference time, 즉 배포(deployment) 시점) 동안 LLM 응답을 개선하는 데 대한 흥미로운 통찰력을 제공하는 매우 잘 쓰여지고 상세한 논문이기 때문입니다.

8.1 더 많은 테스트 시간 컴퓨팅(test-time computation)을 사용하여 출력 개선
이 논문의 주요 전제는 증가된 테스트 시간 컴퓨팅이 LLM 출력을 개선하는 데 사용될 수 있는지, 그리고 어떻게 사용될 수 있는지를 조사하는 것입니다. 마치 사람이 복잡한 문제를 해결할 때 충분한 시간을 들여 더 나은 결과를 내는 것처럼, LLM 역시 응답 생성에 더 많은 자원이 할당될 때 더 나은 결과를 도출할 수 있습니다. 더 기술적인 용어로 말하면, 연구자들은 추론(inference) 중에 추가 컴퓨팅(compute)이 사용될 경우 모델이 학습된 것보다 얼마나 더 잘 수행될 수 있는지를 알아내려고 합니다. 또한, 연구자들은 고정된 컴퓨팅 예산(compute budget)이 주어졌을 때, 테스트 시간에 더 많은 컴퓨팅을 사용하는 것이 모델을 추가로 사전 학습(pre-training)하는 데 컴퓨팅을 사용하는 것보다 결과를 개선할 수 있는지 여부도 살펴보았습니다. 2025년에도 이러한 접근 방식은 LLM의 실용적인 활용도를 높이는 핵심 전략 중 하나로 자리 잡았습니다.

8.2 테스트 시간 컴퓨팅(test-time computation) 기술 최적화
이 논문은 테스트 시간 컴퓨팅을 최적화하는 기술들을 상세히 다루고 있으며, LLM 배포에 관심 있는 분들에게는 필독을 권합니다. 요약하자면, 테스트 시간 컴퓨팅을 확장하는 두 가지 주요 방법은 다음과 같습니다.
1. 여러 솔루션을 생성하고 프로세스 기반 검증자 보상 모델(process-based verifier reward model, 별도로 학습되어야 함)을 사용하여 최상의 응답을 선택하는 것.
2. 모델의 응답 분포(response distribution)를 적응적으로 업데이트하는 것. 이는 본질적으로 추론 생성(inference generation) 중에 응답을 수정하는 것을 의미합니다(이 또한 별도의 모델이 필요합니다).
카테고리 1의 대표적인 예시로는 N개 중 최적(best-of-N) 샘플링이 있습니다. 이는 LLM이 여러 답변을 병렬로 생성하게 한 다음, 검증자 보상 모델(verifier reward model)을 기반으로 최상의 답변을 선택하는 것을 의미합니다. N개 중 최적도 단지 한 가지 예시일 뿐입니다. 아래 그림에 나와 있듯이, 빔 탐색(beam-search), 선행 탐색(lookahead-search), N개 중 최적(best-of-N)과 같은 여러 탐색 알고리즘(search algorithms)이 이 범주에 속합니다.
다양한 탐색 기반 방법은 프로세스-보상 기반 모델(process-reward-based model)에 의존하여 최상의 답변을 선택합니다.
LLM 테스트 시간 컴퓨팅 논문에서 발췌한 주석이 달린 그림, https://arxiv.org/abs/2408.03314
카테고리 2에 속하는 또 다른 접근 방식은 아래 그림에 나와 있듯이 모델의 응답을 순차적으로 수정하는 것입니다.
순차적 수정 접근 방식.
LLM 테스트 시간 컴퓨팅 논문에서 발췌한 주석이 달린 그림, https://arxiv.org/abs/2408.03314
어떤 접근 방식이 더 효과적일까요? 안타깝게도 만능 해결책은 없습니다. 이는 기본 LLM과 특정 문제 또는 쿼리(query)에 따라 다릅니다. 예를 들어, 수정 기반 접근 방식은 더 어려운 질문에서 더 나은 성능을 보이지만, 쉬운 질문에서는 실제로 성능을 저해할 수 있습니다. 이 논문에서는 쿼리의 난이도(difficulty level)를 평가한 다음 적절한 전략을 선택하는 모델을 기반으로 "최적의" 전략을 개발했습니다. 2025년에는 이러한 최적화 전략을 자동화하고 관리하는 다양한 프레임워크와 라이브러리가 등장하여, 개발자들이 추론 효율성을 더욱 쉽게 개선할 수 있게 되었습니다.

8.3 테스트 시간 컴퓨팅(test-time computation) 대 더 큰 모델 사전 학습(pretraining)
흥미로운 질문은, 고정된 컴퓨팅 예산(compute budget)이 주어졌을 때, 더 큰 모델을 사용하는 것과 추론 시간 예산(inference-time budget)을 늘리는 것 중 어느 것이 더 큰 효과를 가져오는가 하는 것입니다. 여기서 쿼리(query)에 지불하는 비용은 동일하다고 가정합니다. 왜냐하면 추론(inference)에서 큰 모델을 실행하는 것이 작은 모델보다 더 비용이 많이 들기 때문입니다. 연구자들은 어려운 질문에 대해서는, 추론 스케일링 전략(inference scaling strategies)이 적용된 작은 모델보다 큰 모델이 더 우수한 성능을 보임을 확인했습니다. 하지만 쉽고 중간 수준의 질문에서는 추가 추론 컴퓨팅으로 14배 더 큰 모델과 동등한 성능을 달성할 수 있었습니다! 이러한 결과는 모델 크기만이 성능을 좌우하는 유일한 요소가 아님을 시사하며, 2025년에는 효율적인 추론 전략의 중요성이 더욱 부각되었습니다.

8.4 테스트 시간 컴퓨팅(test-time compute) 스케일링의 미래 관련성
라마 3(Llama 3)와 같은 오픈 가중치 모델(open-weight models)을 활용할 때, 우리는 보통 모델이 생성하는 응답을 그대로 사용하곤 합니다. 그러나 이 논문이 강조하듯이, 더 많은 추론 컴퓨팅(inference compute)을 할당함으로써 응답 품질을 크게 향상시킬 수 있습니다. (모델을 배포하고 있다면, 이 논문은 반드시 읽어야 할 논문입니다.) 물론, 크고 비싼 모델에 대한 추론 컴퓨팅 예산을 늘리면 운영 비용이 더욱 증가합니다. 하지만 쿼리(query)의 난이도에 따라 선택적으로 적용될 경우, 특정 응답의 품질과 정확도를 귀중하게 향상시킬 수 있으며, 이는 대부분의 사용자가 의심할 여지 없이 높이 평가할 것입니다. (OpenAI, Anthropic, Google은 이미 이러한 기술을 내부적으로 활용하고 있다고 가정하는 것이 안전합니다.) 또 다른 설득력 있는 사용 사례는 더 작고 온디바이스(on-device) LLM의 성능을 향상시키는 것입니다. 애플 인텔리전스(Apple Intelligence)와 마이크로소프트의 코파일럿 PC(Copilot PCs)와 같은 움직임에서 알 수 있듯이, 이 분야는 앞으로도 핵심적인 발전 동력이 될 것입니다. 2025년에는 특히 온디바이스(on-device) LLM의 발전과 함께, 제한된 자원 내에서 성능을 극대화하기 위한 이러한 추론 스케일링 기술의 중요성이 더욱 부각되었습니다.

9. 9월: 멀티모달 LLM 패러다임(multimodal LLM paradigms) 비교
멀티모달 LLM(Multimodal LLMs)은 2024년 주요 성장 동력 중 하나였으며, 실제로 많은 오픈 가중치 LLM(open-weight LLMs) 출시로 이어졌습니다.
다양한 입력 양식(input modalities, 오디오, 텍스트, 이미지, 비디오)을 받아들이고 텍스트를 출력 양식(output modality)으로 반환할 수 있는 멀티모달 LLM의 예시.
특히 저에게 인상 깊었던 논문은 Dai와 동료들이 발표한 NVIDIA의 "NVLM: 개방형 프론티어급 멀티모달 LLM(Open Frontier-Class Multimodal LLMs, 2024년 9월)"이었습니다. 이 논문은 두 가지 주요 멀티모달 패러다임을 훌륭하게 비교하고 있기 때문입니다.

9.1 멀티모달 LLM 패러다임(Multimodal LLM paradigms)
멀티모달 LLM을 구축하는 두 가지 주요 접근 방식이 있습니다:
방법 A: 통합 임베딩 디코더 아키텍처(Unified Embedding Decoder Architecture) 접근 방식;
방법 B: 교차 모달리티 어텐션 아키텍처(Cross-modality Attention Architecture) 접근 방식.
멀티모달 LLM 아키텍처를 개발하는 두 가지 주요 접근 방식.
위 그림에서 볼 수 있듯이, 통합 임베딩-디코더 아키텍처(Unified Embedding-Decoder Architecture, 방법 A)는 GPT-2나 라마 3.2(Llama 3.2)와 같은 기존 LLM 아키텍처와 유사한 단일 디코더 모델(single decoder model)을 활용합니다. 이 접근 방식은 이미지를 텍스트 토큰(text tokens)과 동일한 임베딩 크기(embedding size)를 공유하는 토큰으로 변환하여, LLM이 연결된 텍스트 및 이미지 입력 토큰(image input tokens)을 처리할 수 있도록 합니다. 대조적으로, 교차 모달리티 어텐션 아키텍처(Cross-Modality Attention Architecture, 방법 B)는 교차 어텐션 메커니즘(cross-attention mechanism)을 통합하여 어텐션 레이어(attention layer) 내에서 이미지 및 텍스트 임베딩(image and text embeddings)을 직접 통합합니다. 추가 세부 사항에 관심이 있다면, 올해 초 이 두 가지 방법을 단계별로 설명하는 멀티모달 LLM에 대한 전체 기사를 작성했습니다: 멀티모달 LLM 이해하기 -- 주요 기술 및 최신 모델 소개(Understanding Multimodal LLMs -- An introduction to the main techniques and latest models).

9.2 엔비디아의 하이브리드(hybrid) 접근 방식
2024년의 수많은 멀티모달 기술 발전 속에서, 엔비디아(NVIDIA)의 "NVLM: 개방형 프론티어급 멀티모달 LLM(Open Frontier-Class Multimodal LLMs)" 논문은 멀티모달 접근 방식들을 명확하게 비교 분석하여 특히 주목받았습니다. 단일 방법에 초점을 맞추기보다는 다음을 직접 비교했습니다:
방법 A: 통합 임베딩 디코더 아키텍처("디코더 전용 아키텍처(decoder-only architecture)," NVLM-D),
방법 B: 교차 모달리티 어텐션 아키텍처("교차 어텐션 기반 아키텍처(cross-attention-based architecture)," NVLM-X),
하이브리드 접근 방식(hybrid approach, NVLM-H).
세 가지 멀티모달 접근 방식 개요. (NVLM: 개방형 프론티어급 멀티모달 LLM 논문에서 발췌한 주석이 달린 그림: https://arxiv.org/abs/2409.11402)
위 그림에 요약된 바와 같이, NVLM-D는 방법 A와 일치하며, NVLM-X는 이전에 논의된 방법 B에 해당합니다. 하이브리드 모델(NVLM-H)은 이미지 썸네일(image thumbnail)을 초기 입력으로 사용하고, 교차 어텐션(cross-attention)으로 고해상도 디테일을 처리하여 두 접근 방식의 강점을 결합했습니다. 요약하자면, 주요 발견 사항은 다음과 같습니다:
NVLM-X: 고해상도 이미지에 대해 우수한 계산 효율성(computational efficiency)을 제공합니다.
NVLM-D: OCR 관련 작업에 대해 더 높은 정확도를 제공합니다.
NVLM-H: 최적의 성능을 위해 두 접근 방식의 장점을 결합합니다.

9.3 2025년의 멀티모달 LLM(Multimodal LLMs)
멀티모달 LLM은 텍스트 기반 LLM의 자연스러운 진화이자 매우 흥미로운 연구 분야입니다. 2025년에는 주요 LLM 서비스 제공업체들이 멀티모달 기능을 기본적으로 제공하며, 그 활용 범위가 예측보다 훨씬 빠르게 확장되었습니다. 개인적으로는 아직 멀티모달 기능이 필요한 경우가 제한적이라고 느낍니다(예: '테이블을 마크다운으로 변환해줘'와 같은 특정 작업). 저는 오픈 가중치 LLM의 기본값이 순수하게 텍스트 기반일 것으로 예상합니다. 복잡성을 덜 추가하기 때문입니다. 하지만 2025년 동안, 도구(tooling)와 API의 발전, 그리고 효율적인 멀티모달 학습 방법론의 등장으로 오픈 가중치 멀티모달 LLM의 접근성이 크게 향상되었고, 이는 앞으로 더 많은 사용 사례를 창출할 것으로 기대됩니다.

10. 10월: OpenAI o1의 추론 능력(reasoning capabilities) 복제
2024년 10월, Quin과 동료들의 "O1 복제 여정: 전략적 진행 보고서 -- 파트 1(O1 Replication Journey: A Strategic Progress Report -- Part 1)" 논문이 선정되었습니다. OpenAI ChatGPT의 o1(그리고 현재 o3)은 LLM의 추론 작업(reasoning tasks) 성능을 개선하는 데 있어 패러다임 전환(paradigm shift)을 나타내는 것으로 보이며 상당한 인기를 얻었습니다. OpenAI o1의 정확한 세부 사항은 공개되지 않았으며, 여러 논문에서 이를 설명하거나 복제하려고 시도했습니다. 그렇다면 왜 이 논문을 선택했을까요? 이 논문의 특이한 구조와 학술 연구(academic research)의 현황에 대한 광범위한 철학적 주장이 저에게 공감을 불러일으켰습니다. 2025년에도 o1과 o3가 제시한 추론 능력 향상에 대한 연구는 지속적으로 활발하게 진행되었습니다.

10.1 지름길 학습(Shortcut learning) 대 여정 학습(journey learning)
이 논문의 핵심 요점 중 하나는 아래 그림에 설명된 바와 같이, O1이 지름길 학습(shortcut learning)과 대조적으로 여정 학습(journey learning)이라는 프로세스를 사용한다는 연구자들의 가설입니다. 일반적으로 LLM은 올바른 경로만 학습하는 '지름길 학습' 방식이 사용되었지만, 여정 학습은 지도 미세 조정(supervised finetuning) 과정에서 완전한 시행착오 및 수정 과정을 포함합니다.
O1 복제 보고서에서 발췌한 주석이 달린 그림, https://arxiv.org/abs/2410.18982
여정 학습 접근 방식은 이전에 다룬 추론 시간 컴퓨팅 섹션의 트리 기반(tree-based) 또는 빔 탐색(beam-search) 방법과 다소 유사하다는 점에 주목할 가치가 있습니다. 그러나 미묘한 차이점은 연구자들이 단순히 추론(inference) 중에 이 기술을 적용하는 것이 아니라, 모델 미세 조정을 위한 여정 학습 학습 예제(journey learning training examples)를 생성한다는 것입니다. (추론 프로세스를 증강하기 위해 사용한 기술에 대한 정보는 찾을 수 없었다는 점도 주목할 가치가 있습니다.) 2025년에는 이러한 여정 학습의 개념이 프롬프트 엔지니어링(prompt engineering)과 자기 개선(self-improvement) 모델 개발에도 영감을 주었습니다.

10.2 긴 사고(long thoughts) 구성
연구자들은 시행착오를 강조하며 확장된 사고 과정(extended thought process)을 도출하기 위해 추론 트리(reasoning tree)를 구성했습니다. 이는 유효한 중간 단계를 거쳐 정답에 이르는 직접적인 경로를 강조하는 전통적인 방식과는 차별화됩니다. 이 프레임워크에서 추론 트리의 각 노드(node)는 보상 모델(reward model)이 제공하는 평가와 함께, 해당 단계의 옳고 그름 및 그 추론을 명시했습니다. 다음으로, 그들은 지도 미세 조정(supervised finetuning)과 DPO를 통해 deepseek-math-7b-base 모델을 학습시켰습니다. 여기서 그들은 두 가지 모델을 학습시켰습니다.
1. 첫째, 그들은 올바른 중간 단계만 제공되는 전통적인 지름길 학습(shortcut training) 패러다임을 사용했습니다.
2. 둘째, 그들은 올바른 답과 잘못된 답, 되돌아가기(backtracking) 등을 포함하는 사고 과정(thought process) 세 가지를 포함하는 제안된 여정 학습(journey learning) 접근 방식으로 모델을 학습시켰습니다. (참고: 각 경우에 327개의 예제만 사용했습니다!)
아래 그림에 나와 있듯이, 여정 학습 프로세스는 MATH500 벤치마크(benchmark) 데이터셋에서 지름길 학습을 상당히 큰 차이로 능가했습니다.
지름길 학습과 여정 학습으로 학습된 LLM.
O1 복제 보고서에서 발췌한 주석이 달린 그림, https://arxiv.org/abs/2410.18982

10.3 증류(Distillation) -- 빠른 해결책?
2024년 11월, Huang과 동료들은 "O1 복제 여정 -- 파트 2: 단순 증류를 통한 O1-preview 능가, 큰 진전인가 쓰디쓴 교훈인가?(O1 Replication Journey -- Part 2: Surpassing O1-preview through Simple Distillation, Big Progress or Bitter Lesson?)"라는 후속 보고서를 공개했습니다. 여기서 그들은 증류(distillation) 접근 방식을 사용했습니다. 즉, o1에서 사고 과정(thought processes)을 추출하기 위해 신중한 프롬프트(prompting)를 사용하여 동일한 성능에 도달하도록 모델을 학습시켰습니다. 이 기사가 길기 때문에 자세한 내용은 다루지 않겠지만, 장기 사고 데이터(long-thought data) 수집의 비용 상충 관계(cost trade-offs)를 요약한 해당 논문의 흥미로운 그림을 공유하고 싶었습니다. 그들은 이 증류 접근 방식으로 o1-preview 및 o1-mini와 동등한 매우 좋은 성능을 얻었습니다. 2025년에도 증류 기술은 효율적인 모델 배포를 위한 핵심 전략으로 활용되었으며, 특히 소형 모델의 성능을 향상시키는 데 기여했습니다. 그러나 이러한 실험과 함께 연구자들은 이 접근 방식에 비추어 연구 현황에 대한 흥미롭고 중요한 생각도 공유했으며, 이는 다음 섹션에서 요약하겠습니다.

10.4 AI 연구의 현황
파트 2 보고서의 큰 초점 중 하나는 "단순 증류의 쓰디쓴 교훈(Bitter Lesson of Simple Distillation)"이었습니다. 증류(distillation)는 실용적이지만, 새로운 혁신을 주도하기보다는 기존 상위 모델의 성능을 따라잡는 수준에 머무르는 경향이 있습니다. 아래는 현재 상황에 대한 경고로 작용할 수 있는 논문에서 발췌한 세 가지 인용문입니다:
"‘어떻게 작동하는가’에서 ‘무엇이 작동하는가’로의 이러한 변화는 연구 사고방식의 근본적인 변화를 나타내며, 이는 해당 분야의 미래 혁신 역량에 광범위한 영향을 미칠 수 있습니다."
"이러한 제1원리 사고(first-principles thinking)의 침식은 과학적 혁신의 바로 그 기반을 약화시키기 때문에 특히 우려됩니다."
"빠른 결과물을 내야 한다는 압박은 더 깊은 기술적 조사의 가치를 가릴 수 있으며, 학생들은 더 도전적이고 근본적인 연구 방향을 추구하는 것을 단념하게 될 수 있습니다."
2025년에도 이러한 경고는 여전히 유효하며, 근본적인 연구에 대한 투자의 중요성이 더욱 강조되고 있습니다. 제 개인적인 견해로는 학술 연구실(academic labs)에서 여전히 실용적이고 영향력 있는 중요한 아이디어들이 많이 나오고 있습니다. (제가 좋아하는 몇 가지는 LoRA와 DPO입니다.) 문제는 많은 유망한 아이디어들이 대규모로 테스트되지 못한다는 것입니다. 대학은 보통 이를 위한 막대한 자원을 가지고 있지 않기 때문입니다. 완벽한 해결책이 무엇인지는 확실하지 않으며, 기업들이 자신들의 영업 비밀(trade secrets)을 그냥 공개할 수 없다는 것도 알고 있습니다. 하지만 기업들이 학술 논문의 아이디어를 사용하게 될 때마다 이를 공개적으로 인정해 준다면 정말 도움이 될 것입니다. 그러한 인정은 자신의 작업을 자유롭게 제공하는 연구자들에게 동기를 부여하고 보상하는 데 큰 도움이 됩니다. 또한, 실제로 무엇이 작동하는지 알아냄으로써 분야를 발전시키는 데 기여합니다.

10.5 o1(및 o3)의 관점에서 본 LLM의 미래
"O1 복제 여정(O1 Replication Journey)" 논문이 o1 뒤에 있는 정확한 메커니즘을 복제했을까요? 아마 아닐 것입니다. 하지만 여전히 더 나은 결과를 달성하는 데 도움이 될 아이디어로 가득 찬 가치 있는 읽을거리입니다. "O1 복제 여정" 논문이 o1의 정확한 메커니즘을 완벽히 복제하지는 못했을지라도, 더 나은 LLM 개발에 기여할 아이디어들을 제시하며 중요한 참고 자료가 되었습니다. 저는 o1 및 o3와 같은 "장기 사고(long-thought)" 모델이 LLM 연구에서 계속해서 핵심적인 역할을 할 것이라고 믿습니다. 이 모델들은 높은 실행 비용에도 불구하고, 추론 작업(reasoning tasks)에서 사실상의 황금 표준으로 여겨집니다. 그러나 비용이 더 높기 때문에 o1 유형 모델이 모든 상황에서 항상 최선의 선택은 아닙니다. 문법 수정(grammar fixes)이나 번역(translations)과 같은 더 간단한 작업의 경우, 추론 중심 모델(reasoning-heavy model)이 필요하지 않을 것입니다. 결국 비용과 유용성(utility)의 균형을 맞추는 문제입니다. 우리는 예산, 지연 시간(latency) 및 기타 요인에 따라 작업에 적합한 LLM을 선택합니다. 2025년에는 이러한 장기 사고 모델의 활용이 복잡한 과학 연구나 법률 분석과 같이 높은 정확도를 요구하는 전문 분야에서 더욱 확대되었습니다.
Ahead of AI는 독자 지원 출판물입니다. 새로운 게시물을 받고 제 작업을 지원하려면 무료 또는 유료 구독자가 되는 것을 고려해 보세요.
구독

11. 11월: 정밀도(precision)를 위한 LLM 스케일링 법칙(LLM scaling laws)
원래는 Allen AI의 "Tulu 3: 개방형 언어 모델 사후 학습의 한계 확장(Pushing Frontiers in Open Language Model Post-Training)" 논문을 선정할 계획이었습니다. 이 논문에는 DPO 대 PPO의 절제 연구(ablation studies)를 포함한 라마 사후 학습(Llama post-training) 방법과 레시피에 대한 상세한 설명과, 보상 모델(reward model) 대신 쉽게 정답(ground truth answer)을 생성할 수 있는 검증 가능한 쿼리(verifiable queries, 수학 및 코드 질문 등)를 사용하는 검증 가능한 피드백을 통한 강화 학습(reinforcement learning with verifiable feedbacks)이라는 새로운 선호도 정렬(preference alignment) 방법이 포함되어 있었기 때문입니다. 하지만 내부적인 논의 끝에, 궁극적으로 Kumar와 동료들이 발표한 "정밀도를 위한 스케일링 법칙(Scaling Laws for Precision)" 논문(2024년 11월)을 선택하기로 결정했습니다. 이 논문은 2022년 "컴퓨팅 최적 대규모 언어 모델 학습(Training Compute-Optimal Large Language Models)" 논문의 친칠라 스케일링 법칙(Chinchilla scaling laws)에 대한 매우 필요한 업데이트를 제공하며, 이 법칙은 사전 학습(pretraining)을 위한 컴퓨팅 최적 LLM 매개변수 수(compute-optimal LLM parameter counts)와 데이터셋 크기(dataset sizes)를 결정하는 데 사용됩니다. 2025년의 하드웨어 발전과 효율성 요구 사항을 고려할 때, 이러한 정밀도 스케일링 법칙의 중요성은 더욱 커졌습니다. 요약하자면, "정밀도 스케일링 법칙(Precision Scaling Laws)" 논문(2024년 11월)은 친칠라의 스케일링 법칙을 확장하여 최근 몇 년간 매우 인기를 얻고 있는 저정밀도 설정(low-precision settings, 16비트 이하)에서의 학습(training) 및 추론(inference)을 설명합니다. 예를 들어, 이 논문은 다양한 저정밀도 및 양자화(quantization) 관련 관찰을 단일 함수 형식(single functional form)으로 통합하여 저정밀도 학습(low-precision training)과 사후 학습 양자화(post-training quantization) 모두에서 추가되는 손실(added loss)을 예측합니다.

11.1 친칠라 스케일링 법칙(Chinchilla scaling laws) 복습
2022년 "컴퓨팅 최적 대규모 언어 모델 학습(Training Compute-Optimal Large Language Models)" 논문의 원래 친칠라 스케일링 법칙은 LLM 매개변수 수(parameter counts, N)와 데이터셋 크기(dataset sizes, D)가 LLM의 검증 손실(validation loss)에 어떻게 공동으로 영향을 미치는지 모델링하며, LLM 및 학습 데이터셋 크기를 결정하는 지침으로 사용됩니다. 경험적으로, 고정된 컴퓨팅 예산(compute budget) 하에서 데이터셋 크기 D와 매개변수 수 N의 최적 비율은 대략 D/N ≈ 20으로 알려져 있습니다. 이 비율은 동일한 학습 비용으로 더 낮은 검증 손실을 제공하여 "친칠라 최적(Chinchilla-optimal)"으로 불립니다. 그러나 많은 현대적인 예외가 있습니다. 예를 들어, 라마 3(Llama 3) 팀은 이전에 논의된 바와 같이 15조 개의 토큰(tokens)으로 학습했으며, 80억 개 매개변수 버전의 경우 15,000,000,000,000 ÷ 8,000,000,000 = 1,875가 됩니다. 제 생각에는 정확한 데이터-매개변수 비율보다 더 중요한 것은 모델과 데이터셋 크기가 비례적으로 확장되어야 한다는 점입니다.

11.2 저정밀도 학습(Low-precision training)
저정밀도 스케일링 법칙에 대해 다루기 전에, LLM(또는 심층 신경망(deep neural network)) 가중치(weights)에 사용되는 다양한 숫자 정밀도 형식(numeric precision formats)에 대해 간략히 살펴보겠습니다. 제가 아는 한, GPT 2 & 3 및 라마 2 & 3 모델 학습에 사용된 정밀도 형식은 비교를 위해 다음과 같습니다:
Float32는 범위와 정밀도 사이의 좋은 균형을 제공하여 심층 신경망 학습에 널리 사용되는 표준 32비트 부동 소수점 형식(floating-point format)이었습니다. Float32 미만의 모든 것은 오늘날 저정밀도(low-precision)로 간주됩니다(비록 "낮음"의 정의가 대규모 언어 모델의 "대규모"와 유사하게 움직이는 목표이기는 하지만).
Float16, 또는 반정밀도(half-precision)는 단 16비트를 사용하여 메모리를 절약하고 계산 속도를 높이지만, 더 좁은 동적 범위(dynamic range)를 제공합니다.
32비트 및 16비트 부동 소수점 정밀도 비교
Bfloat16(브레인 플로트 16(brain float 16)) 또한 16비트 형식이지만, float16의 일부 정밀도를 더 큰 지수(exponent)와 교환하여 매우 크고 매우 작은 숫자를 더 효과적으로 표현할 수 있도록 합니다. 결과적으로 bfloat16은 딥러닝 애플리케이션에서 숫자 오버플로(numeric overflow) 또는 언더플로(underflow)를 피하는 데 도움이 될 수 있지만, 낮은 정밀도로 인해 여전히 반올림 오류(rounding errors)가 발생할 수 있습니다.
일반 16비트 부동 소수점과 인기 있는 16비트 브레인 부동 소수점 정밀도 비교
다양한 정밀도 형식과 LLM 모델 동작에 미치는 영향에 대해 더 자세히 알고 싶다면, 제 이전 기사인 "누락된 비트: 라마 2 가중치가 변경되었습니다(The Missing Bits: Llama 2 Weights Have Changed)"의 더 긴 소개를 좋아하실 것입니다. 또한 저는 32비트 및 16비트 형식만 보여주고 있지만, 현재는 32비트 및 16비트 외에도, 라마 3 논문에서 언급된 8비트 형식과 같이 더 낮은 정밀도 학습에 대한 경쟁이 치열합니다. 2025년에는 8비트, 심지어 4비트 정밀도 학습 및 추론에 대한 연구와 실제 적용이 더욱 활발해졌습니다. (12월 26일에 출시된 DeepSeek-v3 모델은 8비트 부동 소수점 정밀도로 완전히 사전 학습되었습니다.)

11.3 정밀도 스케일링 법칙(Precision scaling laws) 요점
이 논문은 길고 흥미로우며, 전체를 읽어볼 것을 권합니다. 핵심은 연구자들이 기존 친칠라 스케일링 법칙(Chinchilla scaling laws)에 "정밀도(precision)" 요소 P를 추가하여 확장했다는 점입니다. 구체적으로, 그들은 모델 매개변수 수(parameter count) N을 정밀도가 감소함에 따라 줄어드는 "유효 매개변수 수(effective parameter count)"로 재해석합니다. (수학 공식은 논문을 참조하십시오.) 또한, 사후 학습 양자화(post-training quantization)가 모델 성능을 어떻게 저하시키는지 포착하기 위한 추가 항을 추가했습니다. (양자화(quantization)에 대한 소개를 작성하지 않았다는 것을 알고 있지만, 이 기사의 길이가 이미 너무 길기 때문에 다음 기회로 미루어야 할 것 같습니다.) 아래 그림은 더 많은 사전 학습(pretraining) 데이터가 항상 더 좋은 것은 아니며, 매우 작은 정밀도(int3)로 학습한 후 모델이 양자화될 경우 실제로 해로울 수 있음을 잘 보여줍니다. 저는 이 점이 매우 흥미로웠습니다.
다양한 사후 양자화 형식에 대한 검증 손실(validation loss)에 대한 더 많은 학습 데이터의 영향
따라서 위 그림에서 얻을 수 있는 결론은, 점점 더 많은 데이터로 학습된 모델(라마 3(Llama 3)와 같은)은 너무 많은 데이터로 "과도하게 학습(overtrained)"되었기 때문에 학습 후 더 낮은 정밀도 형식으로 양자화하기가 더 어려워진다고 말할 수 있습니다. 이러한 통찰은 2025년의 거대 모델 개발 및 배포 전략에 중요한 영향을 미쳤습니다.

11.4 2025년의 모델 스케일링 법칙(Model scaling laws)
친칠라 스케일링 법칙(Chinchilla scaling laws)에 대한 매우 필요한 업데이트를 제공하는 것 외에도, 정밀도 스케일링 법칙(Precision Scaling Laws) 연구는 2025년의 중요한 과제에 대한 흥미로운 관점을 제공합니다. 즉, 라마-3(LLaMA-3)와 같은 모델이 더 큰 데이터셋으로 학습될수록, 성능 손실 없이 INT3와 같은 저정밀도 형식으로 양자화(quantize)하기가 더 어려워질 수 있다는 것입니다. 이는 2025년 동안 양자화 인식 학습(quantization-aware training) 및 혼합 정밀도(mixed-precision) 접근 방식에 대한 연구를 더욱 촉진하는 계기가 되었습니다. 이러한 발견은 "더 많은 데이터가 더 좋다"는 기존 인식을 재고하고, 데이터셋 크기와 효율적인 추론(efficient inference)의 실제 제약 조건 사이의 균형점 탐색의 중요성을 강조합니다. 이는 또한 하드웨어 최적화(hardware optimization)를 추진하는 데 중요한 통찰력입니다. 이러한 스케일링 법칙 연구에서 종종 간과된다고 생각하는 측면 중 하나는 데이터셋의 품질입니다. 저는 사전 학습 데이터(pretraining data)의 특성이 상당한 영향을 미칠 수 있다고 생각합니다. (이에 대한 자세한 내용은 아래 Phi-4 논의에서 다루겠습니다.)

12. 12월: Phi-4와 합성 데이터(Synthetic Data)로부터의 학습
2024년 하반기는 크리스마스에 공개된 DeepSeek-V3를 포함하여 다수의 혁신적인 모델들이 등장한 시기였습니다. 가장 큰 모델 출시는 아닐지라도, 궁극적으로 저는 마이크로소프트의 Phi-4 기술 보고서(Phi-4 Technical Report)를 선택하기로 결정했습니다. 이 보고서가 합성 데이터(synthetic data) 사용에 대한 흥미로운 통찰력을 제공하기 때문입니다.

12.1 Phi-4 성능
Abdin과 동료들이 발표한 Phi-4 기술 보고서(Phi-4 Technical Report, 2024년 12월)는 마이크로소프트의 최신 140억 개 매개변수(parameter) 오픈 가중치 LLM(open-weight LLM)의 학습(training)에 대해 설명합니다. Phi-4의 가장 큰 특징은 주로 GPT-4o가 생성한 합성 데이터(synthetic data)로 학습되었다는 점입니다. 벤치마크(benchmarks) 결과, 이 모델은 주로 비합성 데이터로 학습된 Phi-3를 포함한 유사 규모의 LLM들을 능가하는 성능을 보여주었습니다. 이는 2025년 합성 데이터 기반 학습의 잠재력을 입증하는 중요한 사례가 되었습니다.
유사하거나 다른 크기의 다른 모델과 비교한 phi-4의 성능 (phi-4 논문에서 발췌한 주석이 달린 표, https://arxiv.org/abs/2412.08905)
위 표에 나와 있듯이, 모델이 SimpleQA에서 왜 더 낮은 성능을 보이는지 완전히 확신할 수는 없습니다. 하지만 한 가지 가능한 설명은 SimpleQA가 2024년 10월 30일에 출시된 비교적 새로운 벤치마크라는 것입니다. OpenAI가 평가 스위트(evaluation suite)의 일부로 개발했기 때문에 GPT-4o의 학습 데이터(training data)에 포함되지 않았거나 웹 크롤링된 데이터셋(web-crawled datasets)에 통합되지 않았을 수 있습니다. 더욱이, GPT-4o가 이 평가를 위한 합성 데이터를 생성하는 데 사용되었기 때문에, 어떤 모델도 학습 중에 SimpleQA를 접하지 못했을 것입니다. 그러나 phi-4가 다른 벤치마크에 과적합(overfitting)되었을 수 있으며, 이는 이전에 본 적 없는 SimpleQA 데이터셋에서 상대적으로 낮은 성능을 보이는 이유를 설명할 수 있습니다. 어쨌든, 이것은 단지 제 가설일 뿐입니다.

12.2 합성 데이터(Synthetic data) 학습
이 논문에서 제시된 일부 절제 연구(ablation studies)를 요약하기 전에 데이터셋 구성(dataset composition)을 살펴보겠습니다.
phi-4 학습을 위한 데이터셋 혼합 (phi-4 논문에서 발췌한 주석이 달린 표, https://arxiv.org/abs/2412.08905).
연구진은 합성 데이터(synthetic data)가 전반적으로 유용하지만, 합성 데이터만으로 학습된 모델은 지식 기반 벤치마크(knowledge-based benchmarks)에서 약점을 보인다는 점을 발견했습니다. 이는 합성 데이터에 지식 정보가 부족하거나, 환각(hallucinations)으로 인한 사실 오류가 더 많은 것은 아닌가 하는 질문을 던집니다. 동시에 연구자들은 합성 데이터에 대한 학습 에포크(training epochs) 수를 늘리는 것이 더 많은 웹 데이터(web data)를 추가하는 것보다 성능을 더 향상시킨다는 것을 발견했습니다. 이는 아래 그림에 나와 있습니다.
다른 합성/웹 데이터셋 비율에 대한 모델 성능 비교. (phi-4 논문에서 발췌한 주석이 달린 그림, https://arxiv.org/abs/2412.08905).
요약하자면, 혼합된 데이터에서 합성 데이터의 과도한 비율은 지식 기반 성능에 부정적인 영향을 미칩니다. 그러나 더 균형 잡힌 합성-웹 데이터 혼합 내에서는 합성 데이터셋에 대한 반복(iterations, 에포크(epochs)) 횟수를 늘리는 것이 유익합니다. 2025년에는 이러한 발견을 바탕으로 합성 데이터 생성 전략이 더욱 정교해졌으며, 실제 데이터와 합성 데이터의 최적 조합에 대한 연구가 활발히 이루어졌습니다.

12.3 합성 데이터(synthetic data)의 미래 중요성
phi-4 기술 보고서는 합성 데이터가 모델 사전 학습(pre-training)에 매우 효과적일 수 있다는 중요한 통찰력을 제공합니다. 특히 스케일링 법칙(scaling laws)이 모델 및 데이터셋 크기 모두에서 정체되고 있다고 알려져 있지만(라마 3(Llama 3) 논문에서는 아직 15조 토큰(token) 수준에서 수렴을 보지 못했다고 언급했지만), 연구자와 엔지니어들은 한계를 계속 확장하기 위한 대안적인 방법을 찾고 있습니다. 2025년에는 합성 데이터를 더욱 정교하게 생성하고 활용하는 기술이 발전하면서, 데이터 희소성 문제를 해결하고 모델 성능을 향상시키는 핵심 전략으로 자리매김했습니다. 그럼에도 불구하고, 합성 데이터는 더 적은 데이터로 기본 모델을 학습시키거나, 기존 모델의 성능을 크게 향상시키는 효과적인 방법으로 계속 인식될 것입니다. 저는 고품질 데이터의 사용을 전이 학습(transfer learning)과 유사하게 봅니다. 원시적이고 구조화되지 않은 인터넷 데이터로 모델을 사전 학습하고 사후 학습 중에 정제하는 대신, 고품질 모델(예: 이미 광범위한 정제를 거친 GPT-4o)이 생성한 (일부) 합성 데이터를 활용하는 것이 일종의 지름길(jumpstart) 역할을 할 수 있습니다. 다시 말해, 고품질 학습 데이터의 사용은 모델이 처음부터 더 효과적으로 학습할 수 있도록 할 수 있습니다.

결론 및 전망
이 연구 요약이 유익했기를 바랍니다! 언제나처럼 예상보다 길어졌지만, 2026년을 앞두고 2025년의 흐름을 되짚어보고 미래를 전망하는 섹션으로 마무리하겠습니다.

멀티모달 LLM(Multimodal LLMs)
재작년인 2024년에 저는 LLM이 점점 더 멀티모달(multimodal)이 될 것이라고 예측했습니다. 그리고 2025년은 이러한 예측이 현실이 된 해였습니다. 이제 모든 주요 독점 LLM 제공업체는 멀티모달(또는 최소한 이미지) 지원을 기본으로 제공하며, 오픈 소스 커뮤니티에서도 멀티모달 모델의 출시가 급증했습니다. 제가 보고 읽은 바에 따르면, 멀티모달 논문이 급증한 것은 분명합니다. 아마도 저의 오픈 소스 미세 조정(finetuning) 방법과 자료들이 뒤따를 것입니다. 물론 여전히 많은 상황에서 텍스트 전용 모델로 충분하지만, 더 나은 추론 모델(reasoning models, o1 및 곧 출시될 o3와 같은) 개발에 대한 중요성은 변함없습니다.

계산 효율성(Computational efficiency)
LLM을 사전 학습(pretraining)하고 사용하는 것은 비교적 비용이 많이 듭니다. 2025년에는 이러한 비용 문제를 해결하기 위한 '효율성 전쟁'이 더욱 심화되었습니다. GPU 대여 비용은 여전히 높지만, 하드웨어 최적화와 소프트웨어 알고리즘 개선을 통해 효율성을 높이는 연구가 활발히 진행되었습니다. DeepSeek-v3 모델을 학습시키는 데 GPU 대여 가격(GPU rental sticker prices)을 기준으로 5백만 달러가 들었을 것입니다(여기에는 하이퍼파라미터 튜닝(hyperparameter tuning), 실패한 실행(failed runs) 및 인건비(personnel cost)는 포함되지 않습니다).
DeepSeek-v3 보고서에서 발췌한 대략적인 계산, https://github.com/deepseek-ai/DeepSeek-V3/blob/main/DeepSeek_V3.pdf
그런데 공식 메타 AI(Meta AI) 라마 3(Llama 3) 모델 카드에 따르면, 라마 3 4050억 개 매개변수 모델은 약 10배 더 많은 컴퓨팅(compute)을 사용했습니다(3084만 GPU 시간 대 266만 GPU 시간). LLM 효율성을 높이는 기술로는 전문가 혼합(mixture of experts, 제 파트 1 기사에서 논의됨)과 라마 모델의 그룹화된 쿼리 어텐션(grouped-query attention) 등이 있습니다. 또 다른 흥미로운 점은 DeepSeek 모델에서 발견되는 다중 헤드 잠재 어텐션(multihead latent attention)을 사용하여 다중 헤드 어텐션(multihead attention)의 KV-캐싱(KV-caching)을 더 효율적으로 만드는 것입니다. 모델 입력(model input) 최적화도 중요한 방향입니다. 예를 들어, 바이트 잠재 트랜스포머(Byte Latent Transformer)는 바이트를 엔트로피 기반 패치(entropy-based patches)로 동적으로 인코딩하여 토큰화(tokenization) 없이 확장성(scalability)과 더 빠른 추론(inference)을 위한 컴퓨팅을 최적화함으로써 효율성을 향상시킵니다.

상태 공간 모델(State space models)
올해 상태 공간 모델(state space models)을 다루지 않았다는 것을 눈치채셨을 수도 있습니다. 이는 현재 제 초점이 주로 트랜스포머 기반 LLM(transformer-based LLMs)에 있기 때문입니다. 하지만 2025년에는 상태 공간 모델(SSM)의 발전이 더욱 가속화되었고, 특히 트랜스포머와의 하이브리드 접근 방식이 큰 주목을 받았습니다. 트랜스포머는 여전히 다양한 작업에서 뛰어난 성능을 유지하고 있어, 대안 모델에 대한 고려가 쉽지 않았습니다. 그러나 그렇다고 해서 상태 공간 모델 분야에서 진전이 없었다는 의미는 아닙니다. 저는 이 분야에서 흥미로운 논문들을 많이 보았습니다. 그리고 제가 주목한 한 가지 흥미로운 추세는 이들이 이제 모두 트랜스포머 모델의 셀프 어텐션(self-attention)을 통합한 하이브리드 모델(hybrid models)이라는 것입니다. 예를 들어, "Jamba-1.5: 대규모 하이브리드 트랜스포머-맘바 모델(Hybrid Transformer-Mamba Models at Scale)", "라마 속 맘바: 하이브리드 모델 증류 및 가속화(The Mamba in the Llama: Distilling and Accelerating Hybrid Models)", 그리고 "삼바: 효율적인 무제한 컨텍스트 언어 모델링을 위한 단순 하이브리드 상태 공간 모델(Samba: Simple Hybrid State Space Models for Efficient Unlimited Context Language Modeling)" 등이 있습니다. 그런 의미에서 이들도 계산 비용이 더 많이 들고 있습니다. 트랜스포머 효율성 개선과 SSM에 어텐션 통합 추세가 지속된다면, 두 모델 유형은 점차 수렴할 것으로 예상됩니다. 하지만 확실히 주목할 만한 흥미로운 연구 분야입니다.

스케일링을 통한 LLM 발전
2024년 말에는 인터넷 데이터 고갈로 인해 LLM 스케일링의 한계에 대한 논의가 있었습니다. 이 논의는 Ilya Sutskever(OpenAI의 공동 설립자이자 GPT 논문의 공동 저자 중 한 명)의 NeurIPS 강연에서 나왔지만, 안타깝게도 올해는 컨퍼런스에 참석하지 못해서 자세한 내용은 알지 못합니다. 어쨌든, 인터넷은 기하급수적으로(exponentially fast) 빠르게 성장하기 때문에 흥미로운 지점입니다. 저는 "매일 15.87테라바이트의 데이터"가 증가한다는 자료를 찾을 수 있었습니다. 물론, 모든 데이터가 텍스트이거나 LLM 학습에 유용한 것은 아니라는 문제가 있습니다. 그러나 Phi-4에서 보았듯이, 학습 데이터만으로도 큰 도약을 이룰 수 있도록 데이터 큐레이션(data curation) 및 정제(refinement)에는 여전히 많은 기회가 있습니다. 2025년에는 합성 데이터(synthetic data)의 발전과 고품질 데이터 큐레이션(data curation) 기술의 향상으로, 데이터 희소성 문제를 극복하고 스케일링의 새로운 가능성을 모색하는 움직임이 활발했습니다. 저는 스케일링을 통한 수확 체감(diminishing returns of scaling)에는 동의합니다. 우리는 아마도 정체(plateauing)를 향해 가고 있기 때문에 얻는 이득이 더 작아질 것으로 예상합니다. 하지만 이러한 정체는 사후 학습(post-training) 및 미세 조정(finetuning) 기술의 발전을 더욱 가속화하는 계기가 되었으며, 2025년에는 이 분야에서 눈부신 혁신이 이루어졌습니다. 제가 미래에 많은 이득이 올 것으로 예상하는 한 가지 주목할 만한 영역은 사후 학습(post-training)입니다. 지난여름 "새로운 LLM 사전 학습 및 사후 학습 패러다임(New LLM Pre-training and Post-training Paradigms)" 기사에서 썼듯이, 최근 LLM 출시와 함께 이 분야의 발전을 이미 맛보았습니다.

2026년에 기대하는 것
지난 한 해 동안 다양한 라마 모델(3, 3.1, 3.2, 그리고 3.5)을 다루고 구현하는 과정은 매우 즐거웠습니다. 2026년에는 라마 4(Llama 4)의 본격적인 활용과 함께, 더욱 접근성 높은 소형 모델들의 등장을 기대합니다. 특히, 일반적인 챗봇을 넘어 특정 목적에 최적화된 모델 미세 조정(special-purpose model finetuning)이 더욱 활성화될 것으로 예상합니다(이 분야는 이미 상당히 혼잡합니다). 다양한 코드 및 수학 모델(최근 Qwen 2.5 Coder 및 Qwen 2.5 Math가 떠오르는데, 불행히도 이 보고서에서는 아직 다룰 기회가 없었습니다)에서 그 일부를 보았습니다. 2026년 또한 흥미롭고 빠르게 변화하는 한 해가 될 것이며, 이 희망 목록과 계획은 계속 이어질 것입니다! 결코 지루할 틈이 없을 것입니다!

이 잡지는 개인적인 열정 프로젝트입니다. 저를 지원하고 싶으신 분들은 제 "처음부터 대규모 언어 모델 구축하기(Build a Large Language Model (From Scratch))" 책을 구매해 주시면 감사하겠습니다. (이 책은 다른 곳에서는 찾을 수 없는 상세한 수준으로 LLM이 어떻게 작동하는지 설명하므로 많은 것을 얻으실 수 있을 것이라고 확신합니다.)
"처음부터 대규모 언어 모델 구축하기(Build a Large Language Model (From Scratch))"는 현재 아마존에서 구매 가능합니다.
책을 읽으셨고 잠시 시간을 내주실 수 있다면, 짧은 리뷰(brief review)를 남겨주시면 정말 감사하겠습니다. 이는 저희 저자들에게 큰 도움이 됩니다! 여러분의 지원은 큰 의미가 있습니다! 감사합니다!
구독