(출처: [1, 2, 8, 9, 10, 12, 15]) 기계 학습 시스템의 정확도를 높이려면, 개별적으로 학습된 다수의 모형을 활용하여 추론 과정에서 도출된 결과값을 통합하는 방식인 집합체(ensemble)를 구축할 수 있습니다. 이러한 집합체 구성 방식은 수십 년간 기계 학습 분야에서 사용되어 왔으나, 추론 시 발생하는 비용 증가라는 한계를 내포합니다. 이는 여러 모델의 예측을 동시에 계산해야 하기 때문입니다! 1 이 난점을 해소하기 위해 연구자들은 모델을 통합하는 대안적인 방법론들을 탐구해 왔습니다. 이러한 탐색은 결과적으로 예측값을 단순히 평균하는 대신, 집합체 내 구성 모델들의 가중치를 통합하여 단일의 병합된 모델을 생성하는 가중치 공간 집합체(weight-space ensembles)의 확산으로 이어졌습니다. 이 기법은 많은 상황에서 일반적인 출력 공간 집합체(output-space ensembles)와 동등하거나 더 우수한 성능을 보이며 상당한 효용성을 입증했습니다. 기존 앙상블 방식이 개별 모델의 예측을 모두 수행해야 하는 반면, 가중치 공간 앙상블은 단일 모델로 작동하여 추론 비용을 획기적으로 절감합니다.

“저희 연구는 언어 모델(LM)이 재훈련 과정이나 추가적인 GPU 자원 없이도 유사 모델의 매개변수를 흡수하여 새로운 역량을 습득할 수 있음을 밝혀냅니다.” - 출처: [3]

오늘날 모델 병합(model merging)은 활발한 연구 분야로 각광받고 있지만, 그 개념 자체는 전혀 새로운 것이 아닙니다. 이미 1990년대 [7]까지 그 뿌리를 찾아볼 수 있습니다! 딥러닝 시대 2 에 들어서면서, 모델 병합과 관련된 기법들은 모드 연결성(mode connectivity), 일반화(generalization), 연속 학습(continual learning) 등 다양한 연구 주제에서 반복적으로 등장했습니다. 특히 최근 몇 년간 대규모 언어 모델(LLM) 응용 분야에서의 막대한 잠재력으로 인해 모델 병합에 대한 관심이 폭발적으로 증가했습니다. 우리는 모델 병합이 여러 파운데이션 모델(foundation model)의 기능을 결합하고, 모델에 새로운 기술을 주입하거나, 심지어 정렬(alignment) 프로세스를 개선하는 데 활용되는 것을 목격했습니다. 이 개요에서는 초기 개념부터 LLM을 활용한 최신 응용 사례에 이르기까지, 이 모든 연구의 핵심 내용을 심도 있게 다룰 것입니다.

**기반 및 배경 정보**
모델 병합에 대한 최신 연구에 본격적으로 들어가기 전에, 이 분야의 초기 공헌들을 살펴보겠습니다. 또한, 모델 병합의 근간을 이루는 몇 가지 상이하지만 연관성 있는 연구 주제들을 탐구할 것입니다. 이러한 기술들과 그 기원을 더 잘 이해함으로써, 우리는 모델 병합 방법론에 대한 보다 정교한 관점(nuanced perspective)을 얻게 될 것이며, 이 분야의 핵심 아이디어, 그 기원, 그리고 왜 그렇게 효과적으로 작동하는지에 대해 더욱 깊이 통찰할 수 있게 될 것입니다. 이러한 배경 지식은 단순히 기술적인 이해를 넘어, 모델 병합이 기계 학습의 난제들을 어떻게 해결해 나가는지에 대한 통찰을 제공할 것입니다.

**모델 병합의 기원 (출처: [7])**
모델 병합은 현재 인기 있는 연구 주제이지만, 그 개념적 뿌리는 1990년대 중반까지 거슬러 올라갈 정도로 상당히 광범위합니다! [7]의 저자들은 실제 상황에서 연구자들이 종종 특정 문제 해결을 위해 여러 기계 학습 모델을 훈련시키며, 각 모델은 아키텍처(architecture), 훈련 데이터 구성 방식 및/