대규모 언어 모델(LLM)의 역량은 놀라운 속도로 진화하고 있습니다. 이러한 모델의 성능이 향상됨에 따라, 우리는 이들을 활용하여 더욱 복잡한 문제들을 해결하고, 외부 세계와 능동적으로 소통하며, 장기적인 관점에서 자율적으로 기능하는 정교한 시스템을 구축할 수 있게 되었습니다. 이러한 시스템을 흔히 인공지능 에이전트 시스템이라고 지칭합니다. AI 에이전트는 현재 매우 주목받는 분야이지만, 그 정의와 역할에 대해서는 여전히 많은 혼란이 존재합니다. 본 개요에서는 AI 에이전트에 대한 근본적인 이해를 다지는 데 주력할 것입니다. 기본적인 텍스트 생성 LLM에서 출발하여, 도구 활용(tool utilization)이나 추론 능력(inferential capabilities)과 같은 기능들이 어떻게 표준 LLM을 고도화하여 복잡하고 독립적인 시스템으로 발전시킬 수 있는지 심층적으로 탐구할 것입니다. 이러한 진화는 AI가 단순한 정보 처리기를 넘어 실제 세계에 영향을 미치는 주체로 거듭나는 중요한 전환점을 의미합니다.

**LLM과 그 핵심 기능**

**표준 LLM의 기본 작동 방식**
거대 언어 모델의 본질적인 기능은 위에서 간략히 설명되었습니다. 언어적 입력이 주어지면, 모델은 그에 상응하는 언어적 출력을 생성합니다. 이러한 특성은 직관적으로 이해하기 용이하며, 거의 모든 형태의 문제 해결에 광범위하게 적용될 수 있습니다. 여러 측면에서 LLM이 지닌 포괄적인 적용 가능성은 그 가장 큰 장점 중 하나로 평가됩니다. 본 절에서는 추론 수행(logical deduction)이나 외부 API 연동(external API integration)과 같은 새로운 기능들이 이 기본적인 텍스트-생성 구조 위에 어떻게 추가되어 LLM의 활용 범위를 확장하는지 상세히 설명할 것입니다. 곧 밝혀지겠지만, 현대 AI 에이전트의 대부분의 고도화된 기능은 바로 이 기본적인 텍스트 처리 능력 위에 정교하게 구축됩니다. 순수하게 텍스트만 처리하는 모델은 현실 세계의 복잡성을 직접적으로 다루는 데 한계가 있으며, 이는 외부 도구와의 연동 필요성으로 이어집니다.

**외부 도구 활용(Tool Utilization)**
LLM의 성능이 점차 개선되기 시작하면서, 모델에게 외부 유틸리티와 연동하고 이를 사용하는 방법을 교육하는 것은 AI 연구 분야에서 빠르게 핵심적인 주제로 부상했습니다. LLM과 통합될 수 있는 유용한 외부 도구들의 예시로는 수치 계산기(numeric calculators), 일정 관리 시스템(scheduling systems), 정보 검색 엔진(information retrieval engines), 그리고 코드 실행 환경(code execution environments) 등이 있습니다. 이 접근 방식이 왜 그렇게 높은 관심을 받게 되었을까요? 간단히 말해, LLM은 모든 종류의 과제를 해결하는 데 있어 항상 최적의 수단이 아니기 때문입니다. 많은 경우, 특정 작업을 훨씬 더 간단하고 신뢰성 있게 수행할 수 있는 전문화된 도구들이 존재합니다. 예를 들어, 기본적인 산술 연산에는 계산기가, 특정 주제에 대한 최신 정보를 얻는 데는 검색 엔진이 훨씬 효율적입니다. 하지만 LLM이 복잡한 계획 수립(strategic planning)과 작업 조율(task orchestration)에 뛰어난 능력을 발휘한다는 점을 고려할 때, 우리는 LLM에게 문제 해결 과정의 일부로서 이러한 외부 도구들을 활용하는 방법을 손쉽게 가르칠 수 있습니다. 외부 도구 활용 LLM의 근본적인 개념은 모델에게 문제의 하위 과제(sub-tasks)나 구성 요소(components)를 더욱 전문화되거나 견고한 외부 유틸리티에 위임(delegate)할 수 있는 역량을 부여하는 것입니다. 이 경우 LLM은 다양한 전문 도구들을 유기적으로 조율하는 "중앙 처리 장치" 역할을 수행하게 됩니다. 이러한 접근 방식은 LLM의 내재적 한계를 보완하고, 실시간 정보 접근 및 정확한 계산 능력과 같은 실제 세계의 요구사항을 충족시킵니다.

**도구 활용 기법의 실제 사례**
**도구 사용을 위한 맞춤 학습(Tailored Learning).** 도구 활용에 대한 초기 연구—예를 들어 LaMDA [2] 또는 Toolformer [3] (위에 묘사됨)—는 특정 목적의 맞춤 학습(targeted fine-tuning)을 통해 LLM에게 미리 정해진 도구 묶음을 사용하는 방법을 교육했습니다. 이는 LLM의 토큰 흐름(token stream) 내에 특정 도구에 대한 함수 호출(function call)이 직접 삽입된 훈련 데이터를 선별하여 제공하는 방식으로 이루어졌습니다. 아래를 참조하십시오.

**도구 호출의 내부 구조**
훈련 과정에서 이러한 도구 호출은 일반적인 텍스트 토큰과 동일하게 취급됩니다. 즉, 단순히 텍스트 시퀀스의 한 부분으로 간주됩니다. 추론 단계(inference phase)에서 LLM이 도구 호출을 생성하면, 시스템은 다음과 같은 절차를 따릅니다.
*   텍스트 생성 과정을 일시 중단합니다.
*   생성된 도구 호출을 분석합니다 (즉, 사용될 도구와 그에 필요한 매개변수를 식별합니다).
*   식별된 매개변수를 사용하여 해당 도구를 실행합니다.
*   도구 실행 결과로 반환된 정보를 LLM의 토큰 흐름에 추가합니다.
*   이후 텍스트 생성을 다시 시작합니다.

이러한 방식으로, LLM이 응답을 생성하는 도중에 도구 호출이 실시간으로 처리될 수 있으며, 도구로부터 얻은 정보는 모델의 현재 작업 맥락(context)에 즉시 통합됩니다.

**프롬프트 기반 도구 활용(Prompt-based tool utilization).** 맞춤 학습을 통해 LLM에게 도구 호출 방법을 가르치는 것은—대개 사람의 직접적인 주석 작업(human annotation)을 통해—방대한 양의 훈련 데이터셋을 구축해야 하는 어려움이 따릅니다. LLM의 성능이 지속적으로 향상됨에 따라, 후속 연구에서는 도구 활용을 위한 문맥 내 학습(in-context learning) 기반 접근 방식을 더욱 강조하기 시작했습니다. 모델의 초기 지시에 사용 가능한 도구들을 단순히 설명하는 것만으로도 충분하다면, 굳이 언어 모델을 맞춤 학습해야 할 필요가 있을까요?

(from [4, 5])
프롬프트 기반 도구 활용은 인간의 수고를 현저히 줄여주며, LLM이 접근할 수 있는 외부 도구의 개수를 비약적으로 확장시킬 수 있도록 합니다. 예를 들어, 이 분야의 최신 연구에서는 LLM을 수백 개 [4] 또는 심지어 수천 개 [5]의 도구와 연동하는 성과를 보여주었습니다. 위를 참조하십시오. 이를 가능하게 하기 위해, 우리는 각 도구를 일반적인 API(generic API)로 간주하고, 해당 API의 구조 정보(schema)를 모델의 프롬프트 내에 문맥 정보로 제공합니다. 이 방식은 LLM이 표준화된 구조를 사용하여 인터넷상의 임의의 API와 통합될 수 있도록 하며, 이를 통해 정보 검색, 다른 머신러닝 모델 호출, 여행 일정 예약, 일정 관리 등 수많은 응용 분야를 현실화합니다.

“오늘날, 우리는 콘텐츠 저장소, 비즈니스 도구, 개발 환경을 포함하여 데이터가 존재하는 시스템에 AI 비서를 연결하기 위한 새로운 표준인 모델 컨텍스트 프로토콜(Model Context Protocol, MCP)을 오픈 소싱합니다. 그 목표는 최첨단 모델이 더 좋고 더 관련성 높은 응답을 생성하도록 돕는 것입니다.” - [15]에서 발췌

Anthropic에서 제안한 모델 컨텍스트 프로토콜(MCP)은 LLM이 임의의 외부 유틸리티와 상호작용할 수 있도록 하는 아이디어를 확장한 인기 있는 프레임워크입니다. 요약하자면, MCP는 외부 시스템이 LLM의 초기 지시에 문맥 정보(context)를 제공하는 데 사용되는 형식을 표준화합니다. 복잡한 문제를 해결하기 위해 LLM은 시간이 지남에 따라 점진적으로 더 많은 외부 도구 세트와 통합되어야 할 것입니다. 이 과정을 효율적으로 처리하기 위해 MCP는 이러한 통합을 위한 표준화된 형식을 제안하고, 개발자들이 MCP 서버라고 불리는 사전 구축된 통합 모듈(pre-built integrations)을 생성할 수 있도록 지원합니다. 이 서버는 모든 LLM이 다양한 맞춤형 데이터 소스(custom data sources)와 연동하는 데 활용될 수 있습니다. 아래를 참조하십시오.

**MCP의 일반적인 구조** (source)

도구 활용 기술에 대해 더 깊이 탐구하고 싶은 독자들을 위해, 이 주제에 대한 다음 개요 시리즈를 참고하시기 바랍니다.
*   [도구 사용을 위한 LLM 맞춤 학습](link)
*   [프롬프트 기반 도구 활용](link)
*   [LLM을 코드 실행기와 통합하기](link)
*   [LLM이 자체 도구를 생성하도록 허용하기](link)

**도구 활용 방식의 한계.** 외부 도구 활용 패러다임의 강력함에도 불구하고, 이러한 LLM의 기능은 내재된 추론 능력(reasoning capabilities)에 의해 제약을 받습니다. 도구를 효과적으로 활용하려면, 우리 LLM은 다음의 능력들을 갖추어야 합니다.
*   복잡한 과제를 더 작고 관리 가능한 하위 과제(sub-tasks)로 분해하는 능력.
*   주어진 문제를 해결하는 데 어떤 도구를 적용해야 할지 정확하게 판단하는 능력.
*   관련 도구에 대한 호출을 올바른 형식으로 안정적으로 생성하는 능력.

고도화된 도구 활용은 LLM이 효과적인 작업 조율자(orchestrator) 역할을 수행해야 함을 요구하며, 이는 모델의 논리적 사고 능력과 전반적인 신뢰성(reliability)에 크게 의존합니다. 만약 LLM이 이 과정에서 오류를 범한다면, 예를 들어 잘못된 도구를 선택하거나 비정상적인 매개변수를 전달한다면, 전체 문제 해결 과정은 실패할 수 있습니다. 이러한 '도구 환각(tool hallucination)'은 모델이 존재하지 않는 도구를 호출하거나, 사용법을 잘못 이해하여 비현실적인 결과를 요청하는 경우 발생할 수 있습니다. 이를 방지하기 위해서는 도구 사용의 유효성을 검증하고, 오류 발생 시 적절히 처리할 수 있는 견고한 메커니즘이 필요합니다. 예를 들어, 도구 호출 전에 모델이 생성한 매개변수를 검증하거나, 도구 실행 결과를 모델이 다시 평가하여 다음 단계를 결정하도록 하는 등의 접근 방식이 있습니다.

**추론 능력 강화 모델 (Enhanced Reasoning Models)**
주체성(agency)과 추론 능력(inferential capacity) 간의 밀접한 관계를 고려할 때, 논리적 사고 능력은 수년간 LLM 연구의 핵심적인 탐구 대상이었습니다. 현재 추론 연구에 대한 보다 심층적인 내용은 아래 개요를 참조하시기 바랍니다. 하지만 여기서는 완전성을 기하기 위해 추론 능력 강화 모델의 핵심 사상을 간략하게 다루겠습니다. LLM의 추론 능력은 단순한 패턴 매칭을 넘어, 복잡한 정보를 분석하고 논리적 결론을 도출하는 데 필수적입니다.

**추론 모델의 작동 원리**
Cameron R. Wolfe, Ph.D. · 2월 18일
[전체 글 읽기](link)

**사고 연쇄 프롬프팅 (Chain of Thought, CoT prompting).** LLM이 처음 대중적인 주목을 받았을 때, 이 모델들에 대한 가장 흔한 비판 중 하나는 복잡한 논리적 추론을 수행할 수 없다는 것이었습니다. 그러나 사고 연쇄(Chain of Thought, CoT) 프롬프팅 [6, 7]에 대한 연구는 기본적인 LLM(vanilla LLMs)이 우리가 처음 인식했던 것보다 훨씬 더 뛰어난 추론 능력을 지니고 있음을 밝혀냈습니다. CoT 프롬프팅의 핵심 아이디어는 매우 단순합니다. LLM에게 최종 결과물을 직접적으로 요청하는 대신, 최종 응답 이전에 그 과정에 대한 근거(rationale)나 상세한 설명(explanation)을 먼저 생성하도록 유도하는 것입니다. 아래를 참조하십시오.

(from [7])
흥미롭게도, 이 방법론은 추론을 요구하는 과제에서 기본적인 LLM의 성능을 비약적으로 향상시켰으며, 이는 우리가 올바른 접근 방식을 찾아낼 수 있다면 LLM이—합리적인 수준까지—복잡한 추론을 수행할 수 있음을 시사합니다.

**추론 전문 모델.** CoT 프롬프팅은 매우 효과적이며, 모든 최신 LLM의 핵심적인 구성 요소입니다. 예를 들어, ChatGPT는 일반적으로 답변과 함께 사고 연쇄(CoT)를 기본적으로 출력합니다. 그러나 이러한 추론 방식은 다소 고정적입니다. 전체 추론 과정은 LLM이 생성한 CoT를 중심으로 이루어지며, 해결되는 문제의 복잡성에 기반한 동적인 적응(dynamic adaptation)은 부족합니다.

(source)
이러한 한계를 극복하기 위해 최근 연구에서는 추론에 특화된 LLM(즉, 추론 전문 모델)을 구축하기 위한 새로운 훈련 전략을 도입했습니다. 이 모델들은 표준 LLM과 비교하여 문제 해결에 다른 방식으로 접근합니다. 질문에 대한 답변을 제공하기 전에 가변적인 시간 동안 "사고"하는 과정을 거칩니다.

(from [8])
추론 전문 모델의 사고 과정은 표준 사고 연쇄와 유사하지만, 그들의 CoT는 표준 LLM의 CoT보다 훨씬 길고 (즉, 수천 개의 토큰에 이를 수 있음), 되돌아가기(backtracking) 및 자체 개선(self-refinement)과 같은 복잡한 추론 행동을 보이는 경향이 있으며, 해결되는 문제의 난이도에 따라 동적으로 조절될 수 있습니다. 다시 말해, 더 어려운 문제는 더 긴 CoT를 필요로 합니다. 추론 전문 모델을 가능하게 한 핵심적인 발전은 검증 가능한 보상 기반 강화 학습(reinforcement learning from verifiable rewards, RLVR)을 통한 대규모 후속 훈련(large-scale post-training)이었습니다. 위를 참조하십시오. 수학이나 코딩과 같이 실제 정답(ground truth solutions)이 명확하여 검증 가능한 문제들의 데이터셋이 있다면, LLM이 생성한 답변이 올바른지 간단히 확인하고 이 피드백을 사용하여 RL 방식으로 모델을 훈련할 수 있습니다. 이 훈련 과정을 통해 추론 전문 모델은 RL 기반의 자기 진화(self-evolution)를 통해 검증 가능한 추론 문제를 해결하기 위한 길고 정교한 사고 연쇄를 생성하는 방법을 자연스럽게 학습합니다.

“우리는 어떠한 지도 데이터(supervised data) 없이도 LLM이 추론 능력을 개발할 수 있는 잠재력을 탐구하며, 순수한 강화 학습(reinforcement learning) 과정을 통한 자체 진화에 초점을 맞춥니다.” - [8]에서 발췌

**추론 과정의 궤적(Reasoning trajectories).** 요약하자면, RLVR을 통한 대규모 후속 훈련으로 학습된 추론 전문 모델은 아래에 표시된 대로 표준 LLM의 작동 방식을 변화시킵니다. 직접적인 결과물을 생성하는 대신, 추론 전문 모델은 먼저 추론 과제를 분해하고 해결하는 임의로 긴 사고 연쇄(CoT) 1을 생성합니다. 이것이 바로 "사고(thinking)" 과정입니다. 이 추론 흔적(reasoning trace)의 길이를 제어함으로써 모델이 얼마나 "사고"하는지 조절할 수 있습니다. 예를 들어, OpenAI의 o-시리즈 추론 모델은 낮음, 중간, 높음 수준의 추론 노력을 제공합니다.

**추론 전문 모델의 입출력 구조**
이러한 모델은 여전히 프롬프트가 주어지면 하나의 결과물을 생성하지만, 그 내부의 추론 궤적은 계획 수립(planning), 되돌아가기(backtracking), 진행 상황 모니터링(monitoring), 그리고 평가(evaluation) 등 다양한 고도화된 행동들을 내포하고 있습니다. 이러한 추론 궤적과 그 속성들의 예시는 DeepSeek-R1이 생성한 200만 개 이상의 추론 흔적 예시를 포함하는 [Synthetic-1 데이터셋](link)을 참조하십시오.

**추론 능력과 에이전트의 결합.** 최근 추론 분야의 발전 사항들을 고려할 때, 지시 사항에 대해 계획을 수립하고 효과적으로 추론할 수 있는 충분히 유능한 LLM은 문제를 분해하고, 문제의 각 구성 요소를 해결하며, 스스로 최종 해결책에 도달할 수 있어야 합니다. 복잡한 과제를 해결하기 위해 인간의 개입(human intervention)보다는 LLM에 더 많은 자율성(autonomy)을 부여하고 그들의 역량에 의존하는 것이 에이전트 시스템의 핵심 사상입니다. 에이전트의 개념을 더욱 명확히 하기 위해, 이제 이러한 유형의 시스템을 설계하는 데 활용될 수 있는 프레임워크에 대해 논의해 봅시다.

**ReAct 프레임워크 [1]**
“LLM의 도움으로, 언어가 근본적인 인지 메커니즘(cognitive mechanism)으로서 상호작용(interaction)과 의사 결정(decision making)에 중요한 역할을 할 것이라는 점이 더욱 분명해지고 있습니다.” - [1]에서 발췌

ReAct [1]—REasoning(추론)과 ACTion(행동)의 축약어—은 LLM 기반 에이전트를 활용하여 복잡한 문제들을 자율적으로 분해하고 해결하기 위해 제안된 최초의 포괄적인 프레임워크 중 하나입니다. ReAct는 LLM을 핵심 엔진으로 삼아 순차적이고 다단계적인 문제 해결 과정을 수행하는 시스템으로 이해할 수 있습니다. 각 시간 단계(time step) t에서 LLM은 사용 가능한 모든 피드백을 통합하고 해결하려는 문제의 현재 상태를 고려하여, 미래를 위한 최적의 행동 방침(course of action)을 효과적으로 추론하고 선택할 수 있도록 합니다. 거의 모든 LLM 시스템이 순차적으로 모델링될 수 있다는 점을 감안할 때, ReAct는 매우 일반적이고 강력한 설계 원칙을 제공합니다.

**에이전트 시스템을 위한 설계 개념**
특정 시간 단계 t에서, 우리의 에이전트는 환경으로부터 관찰(observation) o_t를 수신합니다. 이 관찰 정보를 바탕으로, 에이전트는 어떤 행동 a_t를 취할지 결정합니다. 이 행동은 중간 단계일 수 있습니다—예를 들어, 문제 해결에 필요한 데이터를 찾기 위해 웹을 탐색하는 것—또는 최종 단계일 수 있습니다 (즉, 관심 있는 문제를 "해결"하는 최종 조치). 우리는 에이전트가 이 행동을 생성하는 데 사용하는 함수를 정책(policy) π 2로 정의합니다. 정책은 에이전트의 이전 행동과 관찰의 연쇄 목록인 맥락(context)을 입력으로 받아 다음 행동 a_t를 출력으로 예측하며, 이는 결정론적(deterministically)이거나 확률론적(stochastically)일 수 있습니다 3. 아래에 묘사된 바와 같이, 이러한 관찰과 행동의 순환 고리(loop)는 우리 에이전트가 최종 결과물을 도출할 때까지 계속됩니다.

**에이전트의 관찰-행동 순환**
ReAct [1]는 위에 제시된 관찰-행동 순환에 한 가지 핵심적인 변경 사항을 도입합니다. 정책 A에 의해 생성될 수 있는 잠재적 행동 공간(space of potential actions)은 일반적으로 에이전트가 취할 수 있는 중간 및 최종 행동들의 집합을 포함합니다. 예를 들어, 웹에서 데이터를 검색하거나 문제에 대한 최종 해결책을 제시하는 것 등입니다. 그러나 ReAct는 행동 공간을 언어(language)를 포함하도록 확장하여, 에이전트가 전통적인 행동을 수행하는 대신 텍스트 형태의 결과물을 행동으로 생성할 수 있도록 합니다. 다시 말해, 에이전트는 "사고"하는 것을 선택할 수 있습니다. 아래를 참조하십시오.

**ReAct 프레임워크**
공식적으로, 우리는 위에서 보듯이 사고(thought)를 특별한 종류의 행동으로 정의할 수 있습니다. 프레임워크의 이름에서 유추할 수 있듯이, ReAct의 주요 동기는 추론(reasoning)과 행동(action) 사이의 균형을 찾는 것입니다. 인간과 유사하게, 에이전트는 환경에서 취하는 행동들을 미리 생각하고 계획할 수 있어야 합니다. 추론과 행동은 서로 유기적인 공생 관계(symbiotic relationship)를 가집니다.

“추론 흔적(reasoning traces)은 모델이 행동 계획을 유도하고, 추적하며, 업데이트하는 데 도움을 주며, 행동은 모델이 지식 기반(knowledge bases)이나 환경과 같은 외부 소스와 상호작용하고 추가 정보를 수집할 수 있도록 합니다.” - [1]에서 발췌

**AI 에이전트의 사고 방식과 행동 범위**
에이전트의 전형적인 행동 공간은 이산적(discrete)이며—대부분의 경우—상대적으로 제한적입니다. 예를 들어, 질문-답변에 특화된 에이전트는 다음과 같은 행동 선택지를 가질 수 있습니다 (위에 묘사됨).
*   관련 웹 페이지를 검색하기 위해 구글 검색 기능을 실행합니다.
*   특정 웹 페이지에서 필요한 정보를 추출합니다.
*   최종 답변을 도출합니다.

이러한 에이전트가 문제 해결을 위해 수행할 수 있는 행동은 명확히 한정되어 있습니다. 이와 대조적으로, 언어의 공간은 사실상 무한합니다. 결과적으로, ReAct 프레임워크는 강력한 언어 모델을 정책 결정자로 활용할 것을 요구합니다. 유용한 사고 과정을 생성하여 성능을 향상시키기 위해, 우리 에이전트 시스템의 LLM 백엔드(backend)는 고도의 추론 및 계획 능력(planning capabilities)을 갖추어야 합니다.

“이처럼 확장된 행동 공간 내에서의 학습은 까다롭고, 강력한 언어 사전 지식(language priors)을 필수적으로 요구합니다… 우리는 주로 고정된 대규모 언어 모델이… 몇 개의 문맥 내 예시(few-shot in-context examples)를 통해 지시되어 도메인별 행동(domain-specific actions)과 자유로운 형식의 언어적 사고(free-form language thoughts)를 모두 생성하여 과제를 해결하는 설정에 초점을 맞춥니다.” - [1]에서 발췌

**사고 패턴의 유형.** 에이전트가 생성할 수 있는 유용한 사고 패턴의 일반적인 예시로는 주어진 과제를 세분화하고, 단계별 행동 계획을 수립하며, 최종 해결책을 향한 진행 상황을 추적하거나, 단순히 문제 해결과 관련될 수 있는 정보—LLM의 내재적 지식 기반(implicit knowledge base)에서 도출된—를 출력하는 것이 포함됩니다. 에이전트는 사고 능력을 활용하여 문제가 어떻게 해결되어야 할지 명시적으로 설명한 다음, 이 계획을 실행하고—실행 과정을 모니터링합니다. 위의 두 예시 모두에서 에이전트는 문제를 해결할 때 수행해야 할 다음 단계를 명확히 서술합니다. 예를 들어, "다음으로, 나는 ~해야 한다…" 또는 "나는 ~을 검색해야 한다…"와 같이 말입니다.

**ReAct 에이전트의 예시 ([1]에서 발췌)**
*   **질문:** 음악가이자 풍자가인 Allie Goertz는 "심슨 가족"의 캐릭터 밀하우스에 대한 노래를 썼는데, 맷 그로닝은 밀하우스의 이름을 누구의 이름을 따서 지었는가?
    *   **사고 1:** 질문은 "심슨 가족" 캐릭터 밀하우스가 누구의 이름을 따서 지어졌는가로 단순화된다. 나는 밀하우스를 검색하고 누구의 이름을 따서 지어졌는지 찾기만 하면 된다.
    *   **행동 1:** Search[밀하우스]
    *   **관찰 1:** 밀하우스 무솔리니 반 후텐은 폭스 애니메이션 TV 시리즈 "심슨 가족"의 반복 출연 캐릭터로, 파멜라 헤이든이 목소리를 맡았고 맷 그로닝이 만들었다.
    *   **사고 2:** 이 단락은 밀하우스가 누구의 이름을 따서 지어졌는지 알려주지 않는다. "named after"를 찾아볼 수 있을 것 같다.
    *   **행동 2:** Lookup[named after]
    *   **관찰 2:** (결과 1 / 1) 밀하우스는 미국 대통령 리처드 닉슨의 이름을 따서 지어졌는데, 그의 중간 이름은 밀하우스였다.
    *   **사고 3:** 밀하우스는 미국 대통령 리처드 닉슨의 이름을 따서 지어졌으므로, 답은 리처드 닉슨이다.
    *   **행동 3:** Finish[리처드 닉슨]
*   **질문:** 핀란드 록 그룹에 관한 다큐멘터리는 아담 클레이튼 파월인가, 아니면 사이마 제스처인가?
    *   **사고 1:** 나는 아담 클레이튼 파월과 사이마 제스처를 검색하고, 어떤 다큐멘터리가 핀란드 록 그룹에 관한 것인지 찾아야 한다.
    *   **행동 1:** Search[아담 클레이튼 파월]
    *   **관찰 1:** [아담 클레이튼 파월]을 찾을 수 없었다. 유사 항목: ['아담 클레이튼 파월 3세', '7번가 (맨해튼)', '아담 클레이튼 파월 주니어 주립 사무소 건물', '이사벨 워싱턴 파월', '아담 파월', '아담 클레이튼 파월 (영화)', '잔카를로 에스포지토'].
    *   **사고 2:** 다큐멘터리를 찾기 위해 아담 클레이튼 파월 (영화)를 검색할 수 있다.
    *   **행동 2:** Search[아담 클레이튼 파월 (영화)]
    *   **관찰 2:** 아담 클레이튼 파월은 리처드 킬버그가 감독한 1989년 미국 다큐멘터리 영화이다. 이 영화는 영향력 있는 아프리카계 미국인 정치인 아담 클레이튼 파월 주니어의 흥망성쇠에 관한 것이다.[3][4] 나중에 PBS 시리즈 "디 아메리칸 익스피리언스"의 일부로 방영되었다.
    *   **사고 3:** 아담 클레이튼 파월 (영화)는 아프리카계 미국인 정치인에 관한 다큐멘터리이지, 핀란드 록 그룹에 관한 것이 아니다. 따라서 핀란드 록 그룹에 관한 다큐멘터리는 사이마 제스처일 것이다.
    *   **행동 3:** Finish[사이마 제스처]
*   **질문:** 니콜라스 레이와 엘리아 카잔은 어떤 직업을 공통으로 가지고 있는가?
    *   **사고 1:** 나는 니콜라스 레이와 엘리아 카잔을 검색하고, 그들의 직업을 찾은 다음, 그들이 공통으로 가진 직업을 찾아야 한다.
    *   **행동 1:** Search[니콜라스 레이]
    *   **관찰 1:** 니콜라스 레이 (본명 레이몬드 니콜라스 키엔즐 주니어, 1911년 8월 7일 - 1979년 6월 16일)는 미국의 영화 감독, 시나리오 작가, 배우였으며, 1955년 영화 "이유 없는 반항"으로 가장 잘 알려져 있다.
    *   **사고 2:** 니콜라스 레이의 직업은 감독, 시나리오 작가, 배우이다. 다음으로 엘리아 카잔을 검색하여 그의 직업을 찾아야 한다.
    *   **행동 2:** Search[엘리아 카잔]
    *   **관찰 2:** 엘리아 카잔은 미국의 영화 및 연극 감독, 프로듀서, 시나리오 작가, 배우였다.
    *   **사고 3:** 엘리아 카잔의 직업은 감독, 프로듀서, 시나리오 작가, 배우이다. 따라서 니콜라스 레이와 엘리아 카잔이 공통으로 가진 직업은 감독, 시나리오 작가, 배우이다.
    *   **행동 3:** Finish[감독, 시나리오 작가, 배우]
*   **질문:** 아서스 매거진과 퍼스트 포 우먼 중 어떤 잡지가 먼저 창간되었는가?

이 예시에서 ReAct 에이전트는 모든 구체적인 행동을 수행하기 전에 사고 과정을 출력하도록 명시적으로 지시됩니다. 전통적인 LLM과 달리, ReAct 에이전트는 단일 프롬프트당 하나의 최종 결과물을 생성하지 않습니다. 오히려 에이전트는 다음과 같은 순차적인 절차를 통해 결과물을 생성합니다.
*   수행할 행동을 선택합니다 (구체적인 외부 행동 또는 내부 사고).
*   이 행동을 기반으로 환경으로부터 피드백을 받습니다 (예: 검색 질의에서 얻어진 정보).
*   이 새로운 맥락(context)을 바탕으로 다음 행동을 계속합니다.

궁극적으로 최종 행동에 도달하여 문제 해결 과정의 종결을 알립니다. 아래를 참조하십시오. 이러한 상태 기반(stateful), 순차적 문제 해결 접근 방식은 에이전트의 핵심적인 특성이며, 에이전트를 표준 LLM과 구분 짓는 데 중요한 역할을 합니다.

**ReAct를 통한 순차적인 문제 해결**

**의사 결정 과정(Decision making).** 의사 결정 과제에 대한 ReAct의 설정은 지식 집약적 추론 과제의 설정과 매우 유사합니다. 두 의사 결정 과제 모두에서 인간은 ReAct 에이전트의 문맥 내 예시(in-context examples)로 사용될 여러 추론 궤적을 수동으로 주석 처리합니다. 그러나 지식 집약적 추론 과제와 달리, 의사 결정 과제에 ReAct가 사용하는 사고 패턴은 상대적으로 희소합니다. 모델은 언제 어떻게 사고해야 할지 결정하는 데 재량권(discretion)을 행사하도록 지시됩니다. 또한, ReAct 에이전트는 WebShop 데이터셋에서 활용할 수 있는 더 다양하고 풍부한 도구와 행동들을 제공받습니다. 예를 들어, 검색, 필터링, 제품 선택, 제품 속성 선택, 제품 구매 등입니다. 이러한 애플리케이션은 더 복잡한 환경과 상호작용할 때 ReAct의 유효성을 검증하는 좋은 시험대가 됩니다.

**ReAct의 효용성 평가**
위에 설명된 ReAct 에이전트는 여러 기준 모델(baselines)과 비교되었습니다.
*   **단순 프롬프팅(Direct Prompting)**: 예시 궤적에서 사고, 행동, 관찰을 제외하고 질문과 답변만 남긴 소수 예시(few-shot) 프롬프트.
*   **CoT 프롬프팅(CoT prompting)**: 위와 동일하지만, 모델은 최종 해결책을 출력하기 전에 사고 연쇄를 생성하도록 지시됩니다 4.
*   **행동 중심(Act, action-only)**: ReAct 궤적에서 사고 과정을 제거하고 관찰과 행동만 남깁니다.
*   **모방 학습(Imitation)**: 인간의 추론 궤적을 모방하기 위해 모방 학습(imitation learning) 및/또는 강화 학습(reinforcement learning)을 통해 훈련된 에이전트 (예: BUTLER).

아래에 제시된 바와 같이, ReAct 프레임워크는 행동 중심(Act) 설정보다 지속적으로 우수한 성능을 보여주며, 에이전트가 행동과 동시에 사고하는 능력이 엄청나게 중요함을 입증합니다. 더 나아가, CoT 프롬프팅은 어떤 경우에는 ReAct보다 뛰어난 강력한 기준선이지만, LLM이 환각(hallucination)을 일으키기 쉬운 시나리오에서는 어려움을 겪습니다. ReAct는 이러한 상황에서 외부 정보원을 활용하여 환각 현상을 회피할 수 있습니다. 마지막으로, ReAct 에이전트의 성능을 개선할 여지가 많다는 점도 주목할 만합니다. 사실, [1]에서 탐구된 에이전트들은 비교적 취약한 면모를 보입니다. 예를 들어, 저자들은 정보성이 낮은 단순한 검색조차도 에이전트의 실패로 이어질 수 있다고 언급했습니다.

(from [1])

**ReAct와 CoT의 결합.** ReAct는 문제 해결 접근 방식에서 사실에 기반하고 근거가 명확하다는 강점을 가집니다. CoT 프롬프팅은 외부 지식에 의존하지 않기 때문에 환각된 사실로 인해 문제를 겪을 수 있지만, 복잡한 추론 과제를 해결하기 위한 구조를 공식화하는 데는 여전히 탁월한 능력을 발휘합니다. ReAct는 에이전트의 추론 궤적에 관찰, 사고, 행동이라는 엄격한 구조를 부여하는 반면, CoT는 추론 과정을 공식화하는 데 더 많은 유연성을 제공합니다.

(from [1])
두 접근 방식 5의 장점을 모두 얻기 위해, 우리는 이들 사이를 전환하는 전략을 사용할 수 있습니다. 예를 들어, ReAct가 N단계 후에도 답변을 반환하지 못하면 CoT 프롬프팅으로 전환하거나 (즉, ReAct → CoT), 여러 CoT 샘플을 취하여 답변들 사이에 불일치가 존재하면 ReAct를 활용할 수 있습니다 (즉, CoT → ReAct). 위에 표시된 바와 같이, 이러한 후퇴 접근 방식(backoff approach)—어느 방향으로든—은 에이전트의 문제 해결 능력을 향상시킵니다. 이러한 하이브리드 전략은 각 방법론의 약점을 보완하고 강점을 극대화하여, 다양한 유형의 문제에 대해 더욱 견고하고 유연한 해결책을 제공합니다.

**에이전트 연구의 선행 노력들**
ReAct가 AI 에이전트를 위한 (논쟁의 여지는 있지만) 최초의 지속적인 프레임워크로 제시되었지만, 에이전트 분야에서는 그 이전에 다양한 영향력 있는 논문과 아이디어들이 제안되었습니다. 여기서는 이러한 주요 선행 연구 중 일부와 그들의 ReAct와의 비교점을 간략하게 설명하여, ReAct 프레임워크가 이전 작업들을 기반으로 어떻게 더 유용하고 대중적인 프레임워크로 발전했는지 이해할 수 있도록 도울 것입니다.

(from [10])
내적 독백(Inner monologue, IM) [10]은 ReAct와 가장 유사한 선행 연구 중 하나였으며, 로봇 공학 환경에서의 계획 수립(planning)에 적용되었습니다. 위에 제시된 바와 같이, IM은 LLM을 여러 도메인별 피드백 메커니즘(domain-specific feedback mechanisms)과 통합합니다. 예를 들어, 장면 묘사기(scene descriptors)나 성공 감지기(success detectors) 등이 있습니다. ReAct와 다소 유사하게, LLM은 외부 환경으로부터 반복적으로 행동하고, 사고하며, 피드백을 받음으로써—물건을 집는 것과 같은—작업의 해결책을 계획하고 모니터링하는 데 사용됩니다.

“우리는 구체화된 컨텍스트(embodied contexts)에서 사용되는 LLM이 자연어를 통해 제공되는 피드백 소스에 대해 어느 정도 추론할 수 있는지 조사합니다… 우리는 환경 피드백을 활용함으로써 LLM이 로봇 제어 시나리오에서 더 풍부하게 처리하고 계획할 수 있도록 하는 내적 독백을 형성할 수 있다고 제안합니다.” - [10]에서 발췌

IM은 LLM을 자연어(natural language) 처리 영역을 넘어선 다양한 도메인에서 문제 해결을 위한 범용 도구로 활용할 수 있는 잠재력을 보여주었습니다. 그러나 ReAct와 비교할 때, IM 내에서 LLM이 "사고"하는 능력은 제한적이었습니다. 모델은 환경으로부터 피드백을 관찰하고 다음에 무엇을 해야 할지 결정할 수 있을 뿐이었습니다. ReAct는 에이전트가 광범위하고 자유로운 형식의 사고를 출력할 수 있도록 함으로써 이 문제를 해결했습니다.

(from [14])
대화형 의사 결정을 위한 LLM(LLMs for interactive decision making, LID) [14]은 순차적 문제를 해결하기 위한 언어 기반 프레임워크를 제안함으로써 언어를 계획 및 행동을 위한 일반적인 매개체로 활용합니다. 우리는 다양한 작업의 맥락과 행동 공간을 토큰 시퀀스로 공식화하여, 임의의 작업을 LLM 호환 가능한 표준화된 형식으로 변환할 수 있습니다. 그런 다음, 이 데이터는 LLM에 의해 수집되어 강력한 기반 모델(foundation models)이 환경으로부터 피드백을 통합하고 결정을 내릴 수 있도록 합니다. 위를 참조하십시오. [14]에서 저자들은 모방 학습(imitation learning)을 사용하여 LID를 맞춤 학습(fine-tune)하여 다양한 도메인에서 행동을 정확하게 예측했습니다.

(from [11])
WebGPT [11]는 LLM (GPT-3)을 텍스트 기반 웹 브라우저와 통합하여 질문에 더 효과적으로 답변하는 방법을 탐구합니다. 이 작업은 개방형 도구 활용(open-ended tool use)의 초기 선구자이며, LLM에게 웹을 공개적으로 검색하고 탐색하는 방법을 교육합니다. 그러나 WebGPT는 인간의 작업 해결책으로 구성된 대규모 데이터셋에 대해 명시적으로 맞춤 학습됩니다 (즉, 행동 복제(behavior cloning) 또는 모방 학습). 따라서 이 시스템은—매우 미래 지향적이고 효과적임에도 불구하고 (즉, 50% 이상의 경우에서 인간의 답변보다 선호되는 답변을 생성함)—많은 양의 인간 개입을 필요로 합니다. 그럼에도 불구하고, 인간 피드백을 통한 LLM 에이전트 맞춤 학습은 오늘날에도 활발한 연구 주제이며, WebGPT는 이 분야의 기초적인 작업입니다.

(from [12])
LLM의 광범위한 기능에서 영감을 받아, Gato [12]는 여러 양식(modalities), 작업 및 도메인에 걸쳐 행동할 수 있는 단일 "일반주의(generalist)" 에이전트입니다. 예를 들어, Gato는 아타리 게임, 이미지 캡셔닝, 로봇 팔 조작 등에 사용됩니다. 보고서에 설명된 바와 같이, Gato는 "자신의 컨텍스트를 기반으로 텍스트, 관절 토크(joint torques), 버튼 누름 또는 다른 토큰을 출력할지 결정할 수 있습니다." 이 모델은 거의 모든 문제를 해결할 수 있는 자율 시스템(autonomous system)을 만드는 목표를 향해 진정으로 나아갑니다. 그러나 WebGPT와 유사하게, Gato는 많은 문제 시나리오에 걸쳐 컨텍스트와 행동의 방대한 데이터셋—모두 평면적인 토큰 시퀀스로 표현됨—을 수집하는 모방 학습(imitation learning) 접근 방식을 통해 훈련됩니다.

(from [13])
계획을 통한 추론(Reasoning via Planning, RAP) [13]은 LLM에게 더 나은 세계 모델(world model)—즉, LLM이 행동하는 환경과 그로부터 오는 보상에 대한 이해—을 부여하여, 복잡하고 다단계적인 문제에 대한 해결책을 계획하는 LLM의 능력을 향상시키는 것을 목표로 합니다. 특히, LLM은 몬테카를로 트리 탐색(Monte Carlo Tree Search, MCTS)을 통해 탐색될 수 있는 추론 트리(reasoning tree)를 구축하는 데 사용되어 높은 보상을 달성하는 해결책을 찾습니다. 여기서 LLM 자체도 해결책을 평가하는 데 사용됩니다. LLM은 RAP에서 에이전트이자 세계 모델 역할을 합니다.

“LLM (에이전트로서)은 LLM (세계 모델로서)과 보상의 안내를 받아 추론 트리를 점진적으로 구축하며, 탐색(exploration)과 활용(exploitation) 사이의 적절한 균형을 통해 높은 보상을 얻는 추론 경로를 효율적으로 얻습니다.” - [13]에서 발췌

RAP는 유용하고 효과적인 프레임워크이지만, [13]에서는 순전히 텍스트 기반 추론 문제에만 적용됩니다. ReAct와 같은 일반적인 문제 해결 프레임워크는 아닙니다. 에이전트 시스템과 높은 수준의 유사성을 가지지만 주로 LLM 추론 능력 향상에 적용되는 많은 작업들이 있습니다.
*   **선택-추론(Selection-Inference)**은 문제 해결 과정을 선택(또는 계획)과 해결의 교대 단계로 분리하여 LLM 추론 능력을 향상시킵니다. Creswell et al.이 유사한 접근 방식을 개척했습니다.
*   **Re2**는 답변을 도출하기 전에 LLM에게 질문을 다시 읽도록 요청하여 LLM 추론 능력을 향상시키는 프롬프팅 전략입니다.
*   **LLM-증강기(LLM-Augmenter)**는 LLM과 데이터베이스 또는 도메인별 정보 소스를 결합하여 LLM에 유용한 외부 지식을 제공함으로써, 질문-답변 작업에서 근거성(groundedness)을 향상시킵니다.

LLM을 위한 에이전트와 추론의 교차점에 대한 연구(및 훨씬 더 많은 내용)에 대한 더 완전한 조사는 [이 놀라운 글](link)을 참조하십시오.

**"에이전트"란 무엇인가?**
“언어 모델 기반 에이전트의 시작점을 보는 가장 간단한 방법은 모든 도구 사용 언어 모델입니다. 에이전트의 스펙트럼은 여기에서부터 복잡성이 증가합니다.” - Nathan Lambert

산업계에서의 폭발적인 인기에 비해, '에이전트'라는 용어는 명확한 정의가 부재한 상태입니다. 무엇이 진정한 "에이전트"로 간주될 수 있는지에 대한 다양한 논의가 활발히 진행되고 있습니다. 에이전트 정의의 모호함은 오늘날 우리가 복잡성 스펙트럼의 다양한 지점에 위치한 수많은 종류의 에이전트들을 접한다는 사실에서 기인합니다. 거시적인 관점에서 볼 때, 에이전트의 기능은 어떤 면에서는 LLM의 기능과 유사하게 보일 수 있지만, 에이전트는 일반적으로 문제 해결을 위해 훨씬 더 넓은 범위의 전략과 외부 도구들을 활용할 수 있습니다. 지금까지 학습한 정보를 바탕으로, 이제 AI 에이전트가 가질 수 있는 능력의 범위와 이러한 능력들이 표준 LLM과 어떻게 차별화되는지 이해하기 위한 개념적 틀을 구축할 것입니다.

**LLM에서 에이전트로의 진화**
본 개요를 통해 우리는 i) 표준 LLM, ii) 도구 활용, iii) 추론 능력 강화 모델, 그리고 iv) 문제 해결을 위한 자율 시스템 등 다양한 개념들을 학습했습니다. LLM의 기본적인 정의에서 시작하여, 이제 이러한 각 아이디어가 표준 LLM의 기능 위에 어떻게 구축되어 본질적으로 더욱 에이전트적인 시스템을 만들어낼 수 있는지 단계별로 설명할 것입니다.

**[레벨 0] 표준 LLM.** 출발점으로서, 우리는 텍스트 프롬프트를 입력으로 받아 텍스트 응답을 출력으로 생성하는 LLM의 기본적인 설정(위에 묘사됨)을 고려할 수 있습니다. 이 시스템이 문제를 해결하기 위해 외부 시스템을 도입하거나 문제 해결 과정에 어떤 구조도 부여하지 않고 순전히 LLM의 내재된 지식 기반에 의존합니다. 더 복잡한 추론 문제를 해결하기 위해, 우리는 추론 방식 LLM(reasoning-style LLM) 또는 사고 연쇄(CoT) 프롬프팅 접근 방식을 사용하여 추론 궤적을 유도할 수도 있습니다. 아래를 참조하십시오.

**[레벨 1] 외부 도구 활용.** LLM의 내부 지식 기반에만 의존하는 것은 위험할 수 있습니다. LLM은 고정된 지식 마감일(knowledge cutoff date)을 가지며 환각(hallucinate)을 일으키는 경향이 있기 때문입니다. 이 문제를 완화하기 위해, 우리는 LLM에게 유용한 정보를 검색하고 전문화된 도구로 하위 작업을 해결하기 위한 API 호출을 수행하는 방법을 교육할 수 있습니다. 이 접근 방식을 사용하면, LLM은 하위 작업의 해결책을 더 전문화된 외부 시스템에 위임함으로써 문제를 더욱 견고하게 해결할 수 있습니다. 아래를 참조하십시오.

**[레벨 2] 문제의 세분화(Decomposing problems).** LLM이 복잡한 문제를 단 한 번의 단계로 해결할 것이라고 기대하는 것은 비현실적일 수 있습니다. 대신, 우리는 문제가 어떻게 해결되어야 할지 계획하고 해결책을 반복적으로 도출하는 프레임워크를 만들 수 있습니다. 이러한 LLM 시스템은 수동으로 구축될 수 있습니다. 예를 들어, 여러 프롬프트를 순차적으로 연결하거나 여러 프롬프트를 병렬로 실행하고 그 결과를 통합하는 방식입니다. 또는, LLM에 의존하여 문제 해결 전략을 순차적으로 도출하고 실행하는 ReAct와 같은 프레임워크를 사용하여 이러한 수동 노력을 피할 수 있습니다. 아래를 참조하십시오. 물론, LLM으로 복잡한 문제를 분해하고 해결하는 과정은 외부 도구 활용과 추론 능력에 복잡하게 연결되어 있습니다. LLM은 문제 해결 과정 전반에 걸쳐 다양한 외부 도구에 의존할 수 있으며, 추론 능력은 문제를 해결하기 위한 상세하고 정확한 계획을 수립하는 데 필수적입니다. 더 나아가, 문제 해결에 대한 이러한 LLM 중심 접근 방식은 LLM을 사용한 추론에 제어 흐름(control flow) 개념을 도입합니다. 에이전트의 출력은 문제 해결 단계의 시퀀스를 상태 기반으로 이동하면서 순차적으로 구축됩니다.

**[레벨 3] 자율성의 증대.** 위에 제시된 프레임워크들은 오늘날 AI 에이전트가 지닌 대부분의 핵심 기능을 설명합니다. 그러나 우리는 시스템에 더 높은 수준의 자율성을 부여함으로써 더욱 강력한 역량을 갖추도록 만들 수도 있습니다. 예를 들어, 에이전트의 행동 공간에 우리를 대신하여 구체적인 실제 행동(예: 물품 구매, 이메일 전송 또는 풀 리퀘스트(pull request) 생성)을 수행할 수 있는 능력을 포함시킬 수 있습니다.

“에이전트는 환경을 인지하고 그 환경에 대해 행동할 수 있는 모든 것입니다… 이는 에이전트가 작동하는 환경과 수행할 수 있는 행동 세트에 의해 특징지어진다는 것을 의미합니다.” - Chip Huyen

지금까지 우리가 설명한 에이전트들은 항상 인간 사용자로부터의 지시(prompt)를 입력으로 받습니다. 이 지시가 주어지면, 그들은 사고하고, 행동하며, 적절한 응답을 공식화하는 과정을 시작합니다. 다시 말해, 이 에이전트들은 인간 사용자의 지시에 의해 활성화될 때만 행동을 취합니다. 그러나 항상 그럴 필요는 없습니다. 우리는 백그라운드에서 지속적으로 작동하는 에이전트를 구축할 수 있습니다. 예를 들어, [개방형 컴퓨터 활용 에이전트(open-ended computer use agents)](link)에 대한 많은 연구가 진행되었으며, OpenAI는 최근 [Codex](link)를 발표했습니다. Codex는 많은 작업을 병렬로 처리하고 심지어 코드베이스에 자체적으로 PR(pull requests)을 생성할 수 있는 클라우드 기반 소프트웨어 엔지니어링 에이전트입니다.

**AI 에이전트 역량의 스펙트럼.** 이 개요에서 논의한 모든 개념을 통합하면, 다음과 같은 특성을 지닌 에이전트 시스템을 구축할 수 있습니다.
*   인간의 직접적인 입력 없이 비동기적으로(asynchronously) 작동합니다.
*   고도의 추론 LLM을 활용하여 복잡한 과제를 해결하기 위한 상세한 계획을 수립합니다.
*   표준 LLM을 사용하여 기본적인 사고를 생성하거나 정보를 통합합니다.
*   우리를 대신하여 외부 세계에서 실질적인 행동을 취합니다 (예: 항공권 예약 또는 일정표에 이벤트 추가).
*   구글 검색 API (또는 다른 전문 도구)를 통해 최신 정보를 검색합니다.

각 LLM 유형—뿐만 아니라 다른 도구나 모델—은 고유한 강점과 약점을 모두 가지고 있습니다. 이러한 구성 요소들은 에이전트 시스템에 문제 해결의 다양한 측면에 유용한 여러 기능을 제공합니다. 에이전트 시스템의 핵심은 이러한 구성 요소들을 원활하고 신뢰할 수 있는 방식으로 유기적으로 조율하는 것입니다. 그러나 에이전트는 단일한 형태로 존재하는 것이 아니라 스펙트럼 상에 존재하며, 이러한 모든 기능을 사용하거나 일부만 활용할 수도 있습니다. 예를 들어, 위에 설명된 고도화된 시스템뿐만 아니라, 기본적인 도구 활용 LLM이나 특정 유형의 문제를 해결하기 위한 일련의 프롬프트들도 모두 에이전트 시스템의 범주에 속합니다.

**AI 에이전트의 미래 전망**
AI 에이전트가 엄청나게 주목받고 있지만, 이 분야의 연구와 응용은 아직 초기 단계에 머물러 있습니다. 우리가 학습했듯이, 에이전트는 순차적인 문제 해결 과정을 통해 작동합니다. 이 과정에서 단 한 단계라도 오류가 발생하면, 에이전트 전체가 실패할 가능성이 높습니다. 따라서 신뢰성(reliability)은 복잡한 환경에서 효과적인 에이전트를 구축하기 위한 필수 전제 조건입니다. 다시 말해, 견고한 에이전트 시스템을 구축하려면 훨씬 더 높은 수준의 "9"의 신뢰성(more nines of reliability)을 가진 LLM을 개발해야 할 것입니다. 아래를 참조하십시오.

“작년에 당신은 [에이전트]를 가로막는 것이 추가적인 '9'의 신뢰성이라고 말했습니다… 그것이 이 소프트웨어 에이전트들이 하루 종일 일을 할 수는 없지만, 몇 분 동안 당신을 도울 수 있는 방식을 여전히 설명하는 방법일 것입니다.” - Dwarkesh Podcast

오늘날 많은 에이전트들은 신뢰성 부족으로 인해 (논쟁의 여지는 있지만) 여전히 취약합니다. 그러나 LLM 전반(즉, 더욱 정교한 추론 능력과 새로운 세대의 모델 등장)과 특히 에이전트 분야 모두에서 빠르게 진전이 이루어지고 있습니다. 최근 연구는 특히 [에이전트를 효과적으로 평가하고](link), [다중 에이전트 시스템(multi-agent systems)을 구축하며](link), [전문화된 도메인에서 신뢰성을 향상시키기 위해 에이전트 시스템을 맞춤 학습하는](link) 데 중점을 두었습니다. 이 분야의 연구 속도를 고려할 때, 가까운 미래에 이러한 에이전트 시스템의 기능과 범용성(generality)이 비약적으로 증가할 것으로 기대됩니다.

뉴스레터 구독이 처음이신가요? 안녕하세요! 저는 Cameron R. Wolfe입니다. 넷플릭스(Netflix)의 딥러닝(Deep Learning) 박사이자 선임 연구 과학자입니다. 이것은 독자들이 AI 연구의 중요한 주제를 더 잘 이해하도록 돕는 Deep (Learning) Focus 뉴스레터입니다. 이 뉴스레터는 항상 무료이며 공개적으로 읽을 수 있습니다. 뉴스레터가 마음에 드신다면 [구독하거나](link), [유료 구독을 고려하거나](link), [공유하거나](link), [X](link)와 [LinkedIn](link)에서 저를 팔로우해주세요!

[구독하기](link)

**참고 문헌(Bibliography)**
[1] Yao, Shunyu, et al. "React: Synergizing reasoning and acting in language models." International Conference on Learning Representations (ICLR) . 2023.
[2] Schick, Timo, et al. "Toolformer: Language models can teach themselves to use tools." Advances in Neural Information Processing Systems 36 (2023): 68539-68551.
[3] Thoppilan, Romal, et al. "Lamda: Language models for dialog applications." arXiv preprint arXiv:2201.08239 (2022).
[4] Shen, Yongliang, et al. "Hugginggpt: Solving ai tasks with chatgpt and its friends in hugging face." Advances in Neural Information Processing Systems 36 (2023): 38154-38180.
[5] Patil, Shishir G., et al. "Gorilla: Large language model connected with massive apis." Advances in Neural Information Processing Systems 37 (2024): 126544-126565.
[6] Wei, Jason, et al. "Chain-of-thought prompting elicits reasoning in large language models." Advances in neural information processing systems 35 (2022): 24824-24837.
[7] Kojima, Takeshi, et al. "Large language models are zero-shot reasoners." Advances in neural information processing systems 35 (2022): 22199-22213.
[8] Guo, Daya, et al. "Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning." arXiv preprint arXiv:2501.12948 (2025).
[9] Lambert, Nathan, et al. "T\" ulu 3: Pushing frontiers in open language model post-training." arXiv preprint arXiv:2411.15124 (2024).
[10] Huang, Wenlong, et al. "Inner monologue: Embodied reasoning through planning with language models." arXiv preprint arXiv:2207.05608 (2022).
[11] Nakano, Reiichiro, et al. "Webgpt: Browser-assisted question-answering with human feedback." arXiv preprint arXiv:2112.09332 (2021).
[12] Reed, Scott, et al. "A generalist agent." arXiv preprint arXiv:2205.06175 (2022).
[13] Hao, Shibo, et al. "Reasoning with language model is planning with world model." arXiv preprint arXiv:2305.14992 (2023).
[14] Li, Shuang, et al. "Pre-trained language models for interactive decision-making." Advances in Neural Information Processing Systems 35 (2022): 31199-31212.
[15] Anthropic. “Introducing the Model Context Protocol” https://www.anthropic.com/news/model-context-protocol (2024).

1 추론 모델의 맥락에서, 이러한 연쇄 사고는 추론 궤적(reasoning trajectories) 또는 흔적(traces)이라고도 불립니다.
2 이는 강화 학습(reinforcement learning, RL)에서의 정책(policy) 정의와 상당히 유사합니다. 자세한 내용은 [여기](link)를 참조하십시오. 두 경우 모두 정책은 언어 모델로 구현되며 행동을 출력으로 생성합니다. 에이전트와 RL의 정책 정의 간의 주요 차이점은 정책의 입력입니다. 에이전트의 경우 입력은 현재 관찰(current observation)입니다. RL의 경우 정책의 입력은 환경의 현재 상태(current state)입니다.
3 결정론적(deterministic) 정책과 확률론적(stochastic) 정책의 차이에 대한 자세한 내용은 [여기](link)를 참조하십시오.
4 CoT 프롬프팅은 다수결 투표(majority vote)를 통한 자기 일관성(self-consistency)으로 확장되어 성능을 더욱 향상시킬 수 있습니다.
5 특히, ReAct (또는 다른 에이전트 프레임워크)가 표준 CoT 프롬프팅보다 항상 우수하다고 보장할 수는 없습니다! 이러한 기술들의 상대적 성능은 해결되는 문제의 복잡성과 매우 밀접하게 관련되어 있습니다. CoT 프롬프팅은 사용되는 LLM에 대해 환각이 문제가 될 가능성이 낮은 경우에 매우 잘 작동합니다.