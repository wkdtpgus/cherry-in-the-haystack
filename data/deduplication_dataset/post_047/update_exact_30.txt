대규모 언어 모델(LLM)의 기능은 빠르게 발전하고 있습니다. LLM의 역량이 향상됨에 따라, 우리는 이를 활용하여 점점 더 복잡한 문제를 해결하고, 외부 환경과 상호작용하며, 더 긴 시간 범위에 걸쳐 작동하는 고수준 시스템을 만들 수 있습니다. 이러한 시스템을 AI 에이전트 시스템이라고 합니다. AI 에이전트는 인기 있는 주제이지만, 이 에이전트의 정의와 기능에 대해서는 상당한 혼란이 있습니다. 이 개요에서는 AI 에이전트에 대한 이해를 기본 원리부터 구축할 것입니다. 표준 텍스트-투-텍스트 LLM부터 시작하여, 도구 사용(tool usage), 추론(reasoning) 등과 같은 기능이 어떻게 표준 LLM을 향상시켜 복잡하고 자율적인 시스템을 만들 수 있는지 탐구할 것입니다.

최근 몇 년간 AI 에이전트 분야는 단순한 연구 개념을 넘어 실제 애플리케이션의 핵심 요소로 빠르게 부상하고 있습니다. 특히 LLM의 발전과 함께 에이전트 시스템은 정적인 정보 제공을 넘어 능동적으로 문제를 해결하고, 복잡한 작업을 자동화하며, 심지어 인간의 개입 없이도 목표를 달성하는 방향으로 진화하고 있습니다. 이러한 변화는 AI의 패러다임을 '도구'에서 '협력자' 또는 '자율적인 주체'로 전환하는 중요한 이정표가 될 것입니다.

**LLM과 그 기능**

**표준 LLM의 입출력 서명(input-output signature)**
LLM의 기능은 위에 묘사되어 있습니다. 텍스트 프롬프트가 주어지면, LLM은 텍스트 응답을 생성합니다. 이 기능은 이해하기 쉽고 거의 모든 문제를 해결하는 데 일반화될 수 있습니다. 여러 면에서 LLM의 일반성(generality)은 가장 큰 강점 중 하나입니다. 이 섹션에서는 추론(reasoning) 또는 외부 API와의 상호작용(interacting with external APIs)과 같은 새로운 기능이 이 텍스트-투-텍스트 구조를 활용하여 LLM에 어떻게 통합될 수 있는지 설명할 것입니다. 곧 알게 되겠지만, 현대 AI 에이전트의 고급 기능은 대부분 이 기본 기능 위에 구축됩니다.

최근에는 텍스트뿐만 아니라 이미지, 오디오, 비디오 등 다양한 양식(modality)을 이해하고 생성할 수 있는 멀티모달 LLM(Multimodal LLM)이 등장하면서 에이전트의 환경 인식 능력이 비약적으로 향상되었습니다. 이는 에이전트가 단순히 텍스트 기반의 상호작용을 넘어 실제 세계의 데이터를 직접 인지하고 반응할 수 있게 함으로써, 더욱 복잡하고 현실적인 시나리오에서 자율성을 발휘할 수 있는 기반을 마련합니다. 예를 들어, 시각 데이터를 통해 주변 환경을 파악하고, 음성 명령을 이해하며, 적절한 시각적 응답을 생성하는 에이전트의 등장은 현실 세계와의 인터페이스를 더욱 풍부하게 만들고 있습니다.

**도구 사용(Tool Usage)**
LLM의 역량이 향상되기 시작하면서, 외부 도구와 통합하고 사용하는 방법을 가르치는 것이 AI 연구에서 빠르게 인기 있는 주제가 되었습니다. LLM과 통합될 수 있는 유용한 도구의 예로는 계산기(calculators), 캘린더(calendars), 검색 엔진(search engines), 코드 인터프리터(code interpreters) 등이 있습니다. 왜 이 접근 방식이 그렇게 인기가 많을까요? 간단히 말해, LLM은 (당연히) 모든 작업을 해결하는 데 가장 좋은 도구는 아닙니다. 많은 경우, 더 간단하고 신뢰할 수 있는 도구가 있습니다. 예를 들어, 기본적인 산술 연산을 수행하는 계산기나 특정 주제에 대한 최신 사실 정보를 얻는 검색 엔진 등이 있습니다. 하지만 LLM이 계획(planning)과 오케스트레이션(orchestration)에 탁월하다는 점을 고려할 때, 우리는 LLM에게 문제 해결 과정의 일부로 이러한 도구를 사용하는 방법을 쉽게 가르칠 수 있습니다! 도구 사용 LLM의 근본적인 아이디어는 LLM에게 문제의 하위 작업(sub-tasks)이나 구성 요소(components)를 더 전문화되거나 견고한 도구에 위임(delegate)할 수 있는 능력을 부여하는 것입니다. LLM은 다양한 전문 도구들을 함께 조율하는 "두뇌" 역할을 합니다.

**도구 사용 예시 및 최신 동향**
최신 LLM들은 프롬프트 기반의 도구 사용을 넘어, 모델 자체에 함수 호출(function calling) 기능을 내장하여 개발자들이 더욱 쉽게 도구를 통합할 수 있도록 지원합니다. 예를 들어, OpenAI의 GPT-4o나 Google의 Gemini와 같은 모델들은 개발자가 정의한 함수 스키마(function schema)를 이해하고, 사용자의 요청에 따라 자동으로 해당 함수를 호출하기 위한 인자(argument)를 생성할 수 있습니다. 이는 LLM이 단순히 텍스트를 생성하는 것을 넘어, 외부 시스템과 직접 상호작용하며 실제 작업을 수행하는 '행동'의 주체가 되는 중요한 전환점입니다.

LangChain, CrewAI, AutoGen과 같은 에이전트 프레임워크들은 이러한 도구 사용 기능을 더욱 추상화하고 표준화하여, 개발자들이 복잡한 에이전트 시스템을 쉽게 구축할 수 있도록 돕습니다. 이 프레임워크들은 도구 선택(tool selection), 사용 계획(usage planning), 결과 처리(result processing) 등의 과정을 자동화하여, LLM이 다양한 도구를 유연하게 활용할 수 있도록 합니다. 예를 들어, 기업 내부의 CRM(고객 관계 관리) 시스템, 재무 데이터베이스, 혹은 특정 산업 분야의 전문 API 등과 연동하여, LLM 기반 에이전트가 기업의 핵심 업무를 자동화하고 효율성을 높이는 데 기여할 수 있습니다.

**도구 사용의 한계**
도구 사용은 LLM의 능력을 크게 확장하지만, 여전히 여러 한계에 직면합니다. 첫째, **도구 선택의 어려움**입니다. 에이전트가 어떤 상황에서 어떤 도구를 사용해야 할지 정확히 판단하는 것은 복잡한 문제입니다. 특히 사용 가능한 도구의 수가 많아질수록 최적의 도구를 선택하는 것이 더욱 어려워집니다. 둘째, **도구 환각(Tool Hallucination)** 문제입니다. LLM이 존재하지 않는 도구를 호출하거나, 실제 도구의 스키마와 맞지 않는 잘못된 인자를 생성하는 경우가 발생할 수 있습니다. 이는 에이전트의 신뢰성을 저해하고 예측 불가능한 오류를 유발합니다. 셋째, **보안 및 접근 제어**입니다. 외부 시스템과 연동하는 도구는 민감한 정보에 접근하거나 실제 세계에 영향을 미칠 수 있으므로, 엄격한 보안 프로토콜과 접근 제어 메커니즘이 필수적입니다. 이러한 한계점들은 에이전트 시스템의 안정성과 안전성을 보장하기 위한 지속적인 연구와 개발을 요구합니다.

**추론 모델(Reasoning Models)**
에이전시(agency)와 추론(reasoning) 사이의 관계를 고려할 때, 추론 능력은 수년간 LLM 연구의 핵심 초점이었습니다. 현재 추론 연구에 대한 더 심층적인 개요는 아래 개요를 참조하십시오. 하지만 여기서는 완전성을 위해 추론 모델(reasoning models)의 핵심 아이디어를 간략하게 다룰 것입니다.

**추론 모델의 신비 해소하기**
Cameron R. Wolfe, Ph.D. · 2월 18일
[전체 스토리 읽기](link)

**CoT 프롬프팅(CoT prompting).** LLM이 처음 인기를 얻었을 때, 이 모델들에 대한 가장 흔한 비판 중 하나는 복잡한 추론을 수행할 수 없다는 것이었습니다. 그러나 연쇄 사고(Chain of Thought, CoT) 프롬프팅 [6, 7]에 대한 연구는 바닐라 LLM(vanilla LLMs)이 우리가 처음 인식했던 것보다 추론에 더 능하다는 것을 밝혀냈습니다. CoT 프롬프팅의 아이디어는 간단합니다. LLM에게 직접 출력을 요청하는 대신, 최종 출력 이전에 근거(rationale) 또는 설명(explanation)을 생성하도록 요청합니다. 아래를 참조하십시오.

**추론 모델의 발전: CoT를 넘어선 고급 기법**
CoT 프롬프팅은 LLM의 추론 능력을 크게 향상시켰지만, 여전히 순차적이고 단일 경로 추론이라는 한계를 가집니다. 이러한 한계를 극복하기 위해 **Tree-of-Thought (ToT)**와 **Graph-of-Thought (GoT)**와 같은 고급 추론 기법들이 등장했습니다. ToT는 LLM이 여러 개의 가능한 추론 경로를 탐색하고, 각 경로의 중간 결과를 평가하여 가장 유망한 경로를 선택하도록 합니다. 이는 문제 해결 과정에서 불확실성을 관리하고, 더 견고한 해결책을 찾는 데 도움을 줍니다. GoT는 ToT를 더욱 확장하여, 추론 과정의 각 단계를 노드(node)로, 관계를 엣지(edge)로 표현하는 그래프 구조를 활용합니다. 이를 통해 LLM은 비선형적인 추론과 복잡한 정보 간의 상호작용을 더욱 효과적으로 모델링할 수 있습니다.

또한, **자기 성찰(Self-reflection)** 및 **자기 개선(Self-correction)** 메커니즘은 LLM이 자신의 추론 과정과 결과물을 스스로 평가하고, 오류를 식별하며, 개선된 접근 방식을 제안하도록 훈련합니다. 이는 에이전트가 시행착오를 통해 학습하고, 점진적으로 성능을 향상시키는 데 필수적인 능력입니다. 이러한 고급 추론 기법들은 에이전트가 단순히 도구를 사용하는 것을 넘어, 마치 인간처럼 복잡한 문제를 분석하고, 여러 대안을 고려하며, 최적의 해결책을 찾아내는 인지적 능력을 갖추는 데 중요한 역할을 합니다.

**ReAct 프레임워크 [1]**
ReAct [1]—REasoning(추론)과 ACTion(행동)의 줄임말—은 LLM 에이전트를 사용하여 복잡한 문제를 자율적으로 분해하고 해결하기 위해 제안된 최초의 일반 프레임워크 중 하나입니다. ReAct를 핵심에 LLM을 두고 순차적이고 다단계적인 문제 해결 과정으로 생각할 수 있습니다. 각 시간 단계 t에서 LLM은 사용 가능한 모든 피드백을 통합하고 해결하려는 문제의 현재 상태를 고려하여, 미래를 위한 최선의 행동 방침(course of action)을 효과적으로 추론하고 선택할 수 있도록 합니다. (거의) 모든 LLM 시스템이 순차적으로 모델링될 수 있다는 점을 고려할 때, ReAct는 일반적이고 강력한 프레임워크입니다.

**에이전트를 위한 프레임워크 생성하기**
특정 시간 단계 t에서, 우리의 에이전트는 환경으로부터 관찰(observation) o_t를 받습니다. 이 관찰을 기반으로, 우리의 에이전트는 어떤 행동 a_t를 취하기로 결정할 것입니다. 이 행동은 중간 단계일 수 있습니다—예: 문제 해결에 필요한 데이터를 찾기 위해 웹을 검색하는 것—또는 최종 단계일 수 있습니다 (즉, 관심 있는 문제를 "해결"하는 최종 행동). 우리는 에이전트가 이 행동을 생성하는 데 사용하는 함수를 정책(policy) π 2로 정의합니다. 정책은 컨텍스트—에이전트의 이전 행동과 관찰의 연결된 목록—를 입력으로 받아 다음 행동 a_t를 출력으로 예측하며, 이는 결정론적(deterministically)이거나 확률론적(stochastically)일 수 있습니다 3. 아래에 묘사된 바와 같이, 이 관찰과 행동의 루프는 우리의 에이전트가 최종 행동을 출력할 때까지 계속됩니다.

**ReAct의 작동 원리 심화: '사고'의 역할**
ReAct 프레임워크에서 '사고(Thought)'는 단순한 계획 단계를 넘어 에이전트의 인지 능력 전반에 걸쳐 핵심적인 역할을 합니다. 에이전트는 사고를 통해 현재 문제의 본질을 재정의하고, 해결을 위한 가설을 설정하며, 이전 행동의 결과를 분석하여 오류를 식별하고, 다음 행동에 대한 전략을 수립합니다. 예를 들어, 복잡한 데이터 분석 작업을 수행하는 에이전트는 "이 데이터셋에서 어떤 통계적 패턴을 찾아야 하는가?", "이전 검색 결과가 왜 불충분했는가?", "어떤 시각화 도구를 사용해야 가장 효과적일까?"와 같은 사고를 생성하여 문제 해결 과정을 능동적으로 이끌어갈 수 있습니다. 이러한 사고의 깊이와 질은 LLM의 추론 능력에 크게 의존하며, 효과적인 프롬프트 엔지니어링을 통해 에이전트가 더욱 정교하고 유용한 사고를 생성하도록 유도할 수 있습니다.

**ReAct 에이전트의 실제 적용 사례**
ReAct 프레임워크는 다양한 실제 시나리오에서 LLM 기반 에이전트의 성능을 향상시키는 데 활용되고 있습니다.
*   **고객 서비스 챗봇**: 사용자의 복잡한 문의에 대해, 챗봇은 먼저 "사용자의 의도를 파악하고 관련 지식 베이스를 검색해야겠다"고 사고한 후, 검색 도구를 사용하여 정보를 찾고, 그 결과를 바탕으로 "이 정보가 사용자의 질문에 충분한가? 추가 설명이 필요한가?"라고 사고하며 최종 답변을 구성합니다.
*   **데이터 분석 자동화**: 에이전트가 특정 데이터셋의 이상 징후를 감지해야 할 때, "어떤 통계적 테스트를 적용해야 할까?", "이전 분석에서 놓친 부분은 없는가?", "결과를 어떻게 시각화해야 이해하기 쉬울까?" 등의 사고를 거쳐 적절한 분석 도구를 호출하고 보고서를 생성합니다.
*   **소프트웨어 개발 보조**: 개발 에이전트는 특정 기능 구현 요청을 받으면, "이 기능을 구현하기 위해 어떤 라이브러리가 필요할까?", "기존 코드베이스에서 재사용할 수 있는 모듈은 없는가?", "테스트 케이스는 어떻게 작성해야 할까?" 등을 사고하며 코드 생성 도구와 테스트 실행 도구를 활용하여 개발 작업을 수행합니다.

이러한 사례들은 ReAct가 단순한 정보 검색을 넘어, 복잡한 문제 해결 과정을 계획하고 실행하며, 스스로의 진행 상황을 모니터링하는 데 어떻게 활용될 수 있는지 보여줍니다.

**ReAct의 발전과 변형**
ReAct 이후로 다양한 개선 및 변형 프레임워크가 제안되었습니다. **ReWOO (Reasoning Without Observation, Observation Without Reasoning)**는 추론과 행동 단계를 더욱 명확히 분리하여, 불필요한 추론으로 인한 비용을 줄이고 효율성을 높이는 데 초점을 맞춥니다. **Reflexion**과 같은 프레임워크는 에이전트가 자신의 과거 행동과 결과를 '회고(reflection)'하여 실패로부터 학습하고, 미래의 계획을 개선하는 메커니즘을 도입합니다. 이는 에이전트에게 장기적인 학습 능력과 적응성을 부여하여, 더욱 복잡하고 변화무쌍한 환경에서도 효과적으로 작동할 수 있도록 합니다. 이러한 발전은 에이전트의 신뢰성과 자율성을 지속적으로 향상시키며, 더욱 강력한 AI 에이전트 시스템을 구축하는 길을 열고 있습니다.

**에이전트에 대한 이전 시도**
ReAct가 AI 에이전트를 위해 제안된 (논란의 여지는 있지만) 최초의 지속적인 프레임워크였지만, 에이전트 분야에서는 이전에 다양한 영향력 있는 논문과 아이디어들이 제안되었습니다. 여기서는 이러한 주요 제안 중 일부와 그들의 비교를 간략하게 설명하여, ReAct 프레임워크가 이전 작업을 기반으로 어떻게 더 유용하고 인기 있는 프레임워크를 만들었는지 이해할 수 있도록 할 것입니다.

ReAct 이전에도 에이전트 개념은 존재했지만, LLM과의 통합을 통해 그 잠재력이 폭발적으로 증가했습니다. 특히, 2023년 초 등장한 **AutoGPT**와 **BabyAGI**는 "자율 에이전트"라는 개념을 대중에게 각인시키는 데 결정적인 역할을 했습니다. 이들은 ReAct와 유사하게 '사고'와 '행동'의 반복적인 루프를 통해 목표를 달성하려 시도했습니다. AutoGPT는 사용자에게 주어진 목표를 달성하기 위해 스스로 하위 작업을 생성하고, 인터넷 검색, 파일 쓰기/읽기, 코드 실행 등 다양한 도구를 활용하여 작업을 수행했습니다. 비록 초기 버전은 신뢰성 문제가 있었지만, 이들은 LLM이 단순히 한 번의 프롬프트 응답을 넘어, 장기적인 목표를 가지고 스스로 계획하고 실행하며, 필요에 따라 도구를 사용하는 자율적인 주체가 될 수 있음을 보여주었습니다. 이러한 초기 시도들은 현재의 에이전트 프레임워크 및 연구의 중요한 기반이 되었습니다.

**"에이전트"란 무엇인가?**
“언어 모델 기반 에이전트의 시작점을 보는 가장 간단한 방법은 모든 도구 사용 언어 모델입니다. 에이전트의 스펙트럼은 여기에서부터 복잡성이 증가합니다.” - Nathan Lambert

업계에서의 인기에도 불구하고, 에이전트에는 명확한 정의가 없습니다. 무엇이 "에이전트"로 자격이 있는지에 대한 많은 논의가 있습니다. 에이전트 정의에 대한 명확성 부족은 오늘날 우리가 다양한 복잡성 스펙트럼에 걸쳐 있는 다양한 에이전트를 접한다는 사실에서 비롯됩니다. 높은 수준에서, 에이전트의 기능은 어떤 경우에는 LLM의 기능과 유사하게 보일 수 있지만, 에이전트는 일반적으로 문제를 해결하기 위해 더 넓은 범위의 전략과 도구를 사용할 수 있습니다. 지금까지 배운 정보를 사용하여, 이제 AI 에이전트가 가질 수 있는 능력의 스펙트럼과 이러한 능력이 표준 LLM과 어떻게 다른지 이해하기 위한 프레임워크를 만들 것입니다.

에이전트의 정의는 '환경을 인지하고 그 환경에 대해 행동할 수 있는 모든 것'으로 확장될 수 있습니다. 이는 에이전트가 단순히 프롬프트에 응답하는 것을 넘어, 자신의 상태를 유지하고, 과거의 경험을 기억하며, 장기적인 목표를 향해 나아가는 능력을 포함합니다. 이를 위해 에이전트에게는 **기억(Memory)** 기능이 필수적입니다. 단기 기억(Short-term memory)은 현재 상호작용의 컨텍스트를 유지하는 데 사용되며, 장기 기억(Long-term memory)은 과거의 경험, 학습된 지식, 그리고 중요한 사실들을 저장하여 에이전트가 더 복잡하고 지속적인 작업을 수행할 수 있도록 돕습니다. 벡터 데이터베이스(Vector Database)와 같은 기술은 이러한 장기 기억을 효과적으로 저장하고 검색하는 데 활용되며, 에이전트의 지식 기반을 동적으로 확장하는 핵심 요소가 됩니다.

**LLM에서 에이전트로**
이 개요에서 우리는 i) 표준 LLM, ii) 도구 사용, iii) 추론 모델, 그리고 iv) 문제 해결을 위한 자율 시스템을 포함한 다양한 개념에 대해 배웠습니다. LLM의 표준 정의부터 시작하여, 이제 이러한 각 아이디어가 표준 LLM의 기능을 기반으로 구축되어 본질적으로 더 에이전트적인 시스템을 만드는 데 어떻게 사용될 수 있는지 설명할 것입니다.

**[레벨 0] 표준 LLM.** 시작점으로, 우리는 텍스트 프롬프트를 입력으로 받아 텍스트 응답을 출력으로 생성하는 LLM의 표준 설정(위에 묘사됨)을 고려할 수 있습니다. 문제를 해결하기 위해 이 시스템은 외부 시스템을 도입하거나 문제 해결 과정에 어떤 구조도 부과하지 않고 순전히 LLM의 내부 지식 기반에 의존합니다. 더 복잡한 추론 문제를 해결하기 위해, 우리는 추론 스타일 LLM(reasoning-style LLM) 또는 CoT 프롬프팅 접근 방식을 사용하여 추론 궤적을 이끌어낼 수도 있습니다. 아래를 참조하십시오.

**[레벨 1] 도구 사용.** LLM의 내부 지식 기반에 의존하는 것은 위험합니다. LLM은 고정된 지식 마감일(knowledge cutoff date)을 가지며 환각(hallucinate)하는 경향이 있습니다. 이 문제를 완화하기 위해, 우리는 LLM에게 유용한 정보를 검색하고 전문화된 도구로 하위 작업을 해결하기 위한 API 호출을 수행하는 방법을 가르칠 수 있습니다. 이 접근 방식을 사용하여, LLM은 하위 작업의 해결책을 더 전문화된 시스템에 위임함으로써 문제를 더 견고하게 해결할 수 있습니다. 아래를 참조하십시오.

**[레벨 2] 문제 분해 및 장기 기억.** LLM이 복잡한 문제를 단일 단계로 해결할 것이라고 기대하는 것은 비합리적일 수 있습니다. 대신, 우리는 문제가 어떻게 해결되어야 하는지 계획하고 해결책을 반복적으로 도출하는 프레임워크를 만들 수 있습니다. 이러한 LLM 시스템은 수작업으로 만들 수 있습니다. 예를 들어, 여러 프롬프트를 연결하거나 여러 프롬프트를 병렬로 실행하고 그 결과를 집계하는 방식입니다. 또는, LLM에 의존하여 문제 해결 전략을 순차적으로 도출하고 실행하는 ReAct와 같은 프레임워크를 사용하여 이러한 수동 노력을 피할 수 있습니다.

이 단계에서 에이전트는 단순한 문제 분해를 넘어, **장기 기억(Long-term Memory)**을 활용하여 과거의 경험과 학습된 지식을 문제 해결에 통합합니다. 이는 에이전트가 특정 도메인에 대한 전문성을 축적하고, 반복적인 작업을 통해 효율성을 높이며, 새로운 상황에 직면했을 때 과거의 유사한 경험을 참조하여 더 나은 결정을 내릴 수 있도록 합니다. 벡터 데이터베이스는 이러한 장기 기억을 효과적으로 관리하는 핵심 기술로, 에이전트가 방대한 정보 속에서 관련성 높은 지식을 신속하게 검색하고 활용할 수 있게 합니다.

**[레벨 3] 자율성과 다중 에이전트 시스템.** 위의 프레임워크는 오늘날 AI 에이전트의 대부분의 핵심 기능을 설명합니다. 그러나 우리는 시스템에 더 높은 수준의 자율성을 제공함으로써 더 유능하게 만들 수도 있습니다. 예를 들어, 에이전트의 행동 공간에 우리를 대신하여 구체적인 행동(예: 물품 구매, 이메일 전송 또는 풀 리퀘스트(pull request) 열기)을 취할 수 있는 능력을 포함시킬 수 있습니다.

이 단계의 에이전트는 단순히 주어진 작업을 수행하는 것을 넘어, 스스로 목표를 설정하고, 우선순위를 지정하며, 자원을 관리하는 진정한 자율성을 발휘합니다. 이는 마치 개인 비서가 사용자의 스케줄을 능동적으로 관리하고, 필요한 정보를 미리 준비하며, 중요한 결정을 대신 내리는 것과 유사합니다.

더 나아가, **다중 에이전트 시스템(Multi-agent Systems)**은 여러 에이전트가 각자의 전문성을 바탕으로 협업하여 단일 에이전트로는 해결하기 어려운 복잡한 문제를 해결하는 형태입니다. 예를 들어, 소프트웨어 개발 팀은 기획 에이전트, 코딩 에이전트, 테스트 에이전트, 문서화 에이전트 등으로 구성될 수 있으며, 이들이 서로 소통하고 역할을 분담하여 프로젝트를 진행할 수 있습니다. 이러한 시스템은 분산된 지능과 협력적 문제 해결을 통해 더욱 강력하고 유연한 AI 솔루션을 제공합니다.

**AI 에이전트의 미래**
AI 에이전트가 엄청나게 인기가 많지만, 이 분야의 작업은—연구 및 응용 관점 모두에서—초기 단계입니다. 우리가 배웠듯이, 에이전트는 순차적인 문제 해결 과정을 통해 작동합니다. 이 과정에서 어떤 단계라도 잘못되면, 에이전트는 실패할 가능성이 높습니다. 따라서 신뢰성(reliability)은 복잡한 환경에서 효과적인 에이전트를 구축하기 위한 전제 조건입니다. 다시 말해, 견고한 에이전트 시스템을 구축하려면 더 높은 "9"의 신뢰성(more nines of reliability)을 가진 LLM을 만들어야 할 것입니다. 아래를 참조하십시오.

AI 에이전트의 미래는 단순히 기술적 진보를 넘어 사회적, 윤리적 함의를 동반합니다. **신뢰성(Reliability)**, **안전성(Safety)**, **투명성(Interpretability)**은 에이전트가 대중에게 널리 수용되기 위한 필수 요소입니다. 에이전트가 중요한 결정을 내리거나 실제 세계에 영향을 미칠 때, 그 결정의 근거를 이해하고 잠재적 위험을 예측하며 통제할 수 있어야 합니다.

미래의 에이전트는 **인간-AI 에이전트 협업(Human-Agent Collaboration)**의 형태를 더욱 강화할 것입니다. 에이전트는 인간의 지시를 따르면서도, 스스로 학습하고 적응하며, 인간이 미처 생각하지 못한 대안을 제시하거나 복잡한 작업을 효율적으로 분담하여 인간의 능력을 증강시키는 역할을 할 것입니다. 또한, **개인화된 에이전트**는 각 개인의 고유한 선호도, 학습 스타일, 작업 방식에 맞춰 스스로를 최적화하여, 진정한 의미의 개인 비서 역할을 수행할 것입니다.

**지속적인 학습(Continual Learning)**과 **적응 능력**은 에이전트가 변화하는 환경과 새로운 정보에 유연하게 대처하기 위한 핵심 역량입니다. 에이전트는 고정된 지식 기반에 의존하는 것이 아니라, 끊임없이 새로운 데이터를 흡수하고, 경험을 통해 학습하며, 자신의 모델을 업데이트하여 성능을 유지하거나 향상시켜야 합니다.

물론, 이러한 발전과 함께 **윤리적 고려 사항**은 더욱 중요해집니다. 에이전트의 자율성이 증대될수록 책임 소재, 편향(bias) 문제, 프라이버시 침해, 그리고 궁극적으로는 AI가 사회에 미칠 장기적인 영향에 대한 깊이 있는 논의가 필요합니다. AI 에이전트의 궁극적인 목표는 인간을 대체하는 것이 아니라, 인간의 삶을 더 풍요롭고 효율적으로 만드는 데 기여하는 것이어야 합니다. 이러한 가치와 원칙을 바탕으로 AI 에이전트 연구와 개발이 지속된다면, 우리는 더욱 지능적이고 유익한 미래를 맞이할 수 있을 것입니다.

뉴스레터가 처음이신가요? 안녕하세요! 저는 Cameron R. Wolfe입니다. 넷플릭스(Netflix)의 딥러닝(Deep Learning) 박사이자 선임 연구 과학자입니다. 이것은 독자들이 AI 연구의 중요한 주제를 더 잘 이해하도록 돕는 Deep (Learning) Focus 뉴스레터입니다. 이 뉴스레터는 항상 무료이며 공개적으로 읽을 수 있습니다. 뉴스레터가 마음에 드신다면 [구독하거나](link), [유료 구독을 고려하거나](link), [공유하거나](link), [X](link)와 [LinkedIn](link)에서 저를 팔로우해주세요!

[구독하기](link)

**참고 문헌(Bibliography)**
[1] Yao, Shunyu, et al. "React: Synergizing reasoning and acting in language models." International Conference on Learning Representations (ICLR) . 2023.
[2] Schick, Timo, et al. "Toolformer: Language models can teach themselves to use tools." Advances in Neural Information Processing Systems 36 (2023): 68539-68551.
[3] Thoppilan, Romal, et al. "Lamda: Language models for dialog applications." arXiv preprint arXiv:2201.08239 (2022).
[4] Shen, Yongliang, et al. "Hugginggpt: Solving ai tasks with chatgpt and its friends in hugging face." Advances in Neural Information Processing Systems 36 (2023): 38154-38180.
[5] Patil, Shishir G., et al. "Gorilla: Large language model connected with massive apis." Advances in Neural Information Processing Systems 37 (2024): 126544-126565.
[6] Wei, Jason, et al. "Chain-of-thought prompting elicits reasoning in large language models." Advances in neural information processing systems 35 (2022): 24824-24837.
[7] Kojima, Takeshi, et al. "Large language models are zero-shot reasoners." Advances in neural information processing systems 35 (2022): 22199-22213.
[8] Guo, Daya, et al. "Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning." arXiv preprint arXiv:2501.12948 (2025).
[9] Lambert, Nathan, et al. "T\" ulu 3: Pushing frontiers in open language model post-training." arXiv preprint arXiv:2411.15124 (2024).
[10] Huang, Wenlong, et al. "Inner monologue: Embodied reasoning through planning with language models." arXiv preprint arXiv:2207.05608 (2022).
[11] Nakano, Reiichiro, et al. "Webgpt: Browser-assisted question-answering with human feedback." arXiv preprint arXiv:2112.09332 (2021).
[12] Reed, Scott, et al. "A generalist agent." arXiv preprint arXiv:2205.06175 (2022).
[13] Hao, Shibo, et al. "Reasoning with language model is planning with world model." arXiv preprint arXiv:2305.14992 (2023).
[14] Li, Shuang, et al. "Pre-trained language models for interactive decision-making." Advances in Neural Information Processing Systems 35 (2022): 31199-31212.
[15] Anthropic. “Introducing the Model Context Protocol” https://www.anthropic.com/news/model-context-protocol (2024).