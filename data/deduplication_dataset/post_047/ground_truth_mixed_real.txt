대규모 언어 모델(LLM)의 역량은 놀라운 속도로 발전하고 있습니다. LLM의 기능이 향상됨에 따라, 우리는 이를 활용하여 점점 더 복잡한 문제를 해결하고, 외부 환경과 상호작용하며, 더 긴 시간 범위에 걸쳐 작동하는 고수준 시스템을 구축할 수 있게 되었습니다. 이러한 시스템을 AI 에이전트 시스템이라고 부릅니다. AI 에이전트는 현재 가장 주목받는 주제 중 하나이지만, 그 정의와 기능에 대해서는 여전히 많은 오해가 존재합니다. 본 개요에서는 AI 에이전트에 대한 이해를 기본 원리부터 체계적으로 구축해 나갈 것입니다. 표준적인 텍스트-투-텍스트 LLM에서 출발하여, 도구 활용(tool usage), 추론(reasoning) 등과 같은 핵심 기능들이 어떻게 일반 LLM을 고도화하여 복잡하고 자율적인 시스템으로 만들 수 있는지 상세히 탐구할 것입니다.

**LLM과 그 기능**
**표준 LLM의 입출력 구조(input-output signature)**
LLM의 핵심 기능은 잘 알려져 있습니다. 텍스트 프롬프트가 주어지면, LLM은 텍스트 응답을 생성합니다. 이러한 기본 기능은 이해하기 쉬울 뿐만 아니라 거의 모든 유형의 문제를 해결하는 데 범용적으로 적용될 수 있습니다. 여러 측면에서 LLM의 이러한 일반성(generality)은 가장 큰 장점 중 하나로 손꼽힙니다. 이 섹션에서는 추론(reasoning) 능력이나 외부 API와의 상호작용(interacting with external APIs)과 같은 새로운 기능들이 이 기본적인 텍스트-투-텍스트 구조를 활용하여 LLM에 어떻게 통합될 수 있는지 설명할 것입니다. 곧 알게 되겠지만, 오늘날 AI 에이전트가 제공하는 고급 기능의 대부분은 바로 이 기본적인 LLM 기능 위에 구축됩니다.

**도구 활용(Tool Usage)**
LLM의 역량이 비약적으로 발전하기 시작하면서, 외부 도구와 효과적으로 통합하고 사용하는 방법을 LLM에게 가르치는 것은 AI 연구 분야에서 빠르게 핵심적인 관심사가 되었습니다. LLM과 연동될 수 있는 유용한 도구의 예시로는 계산기(calculators), 캘린더(calendars), 웹 검색 엔진(search engines), 코드 인터프리터(code interpreters) 등이 있습니다. 그렇다면 왜 이러한 접근 방식이 그토록 인기를 얻었을까요? 간단히 말해, LLM은 모든 종류의 작업을 해결하는 데 항상 최적의 도구는 아닙니다. 특정 작업에는 훨씬 더 간단하고 신뢰할 수 있는 전용 도구가 존재합니다. 예를 들어, 복잡한 산술 연산을 수행하는 계산기나 특정 주제에 대한 최신 사실 정보를 얻는 검색 엔진이 그렇습니다. 그러나 LLM이 문제 해결의 계획(planning) 및 조율(orchestration)에 탁월하다는 점을 고려할 때, 우리는 LLM에게 이러한 도구들을 문제 해결 과정의 일부로 활용하는 방법을 쉽게 가르칠 수 있습니다! 도구 활용 LLM의 근본적인 아이디어는 LLM에게 문제의 하위 작업(sub-tasks)이나 구성 요소(components)를 더 전문화되거나 견고한 외부 도구에 위임(delegate)할 수 있는 능력을 부여하는 것입니다. 이 시나리오에서 LLM은 다양한 전문 도구들을 효과적으로 조율하는 "두뇌" 역할을 수행합니다.

**도구 활용 예시 ([2, 3]에서 발췌)**
**도구 활용을 위한 미세 조정(Finetuning).** 도구 활용에 대한 초기 연구—예: LaMDA [2] 또는 Toolformer [3]—는 특정 미세 조정(targeted finetuning)을 통해 LLM에게 고정된 도구 세트를 활용하는 방법을 가르쳤습니다. 연구자들은 LLM의 토큰 스트림(token stream)에 특정 도구에 대한 함수 호출(function call)이 직접 삽입된 훈련 예시를 선별하여 사용했습니다. 아래 그림을 참조하십시오.

**도구 호출의 구조**
훈련 과정에서 이러한 도구 호출은 다른 토큰과 마찬가지로 텍스트 시퀀스의 일부로 취급됩니다. 추론 시간(inference time)에 LLM이 도구 호출을 생성하면, 시스템은 다음과 같이 처리합니다.
*   토큰 생성을 일시 중단합니다.
*   도구 호출을 파싱하여 (즉, 사용될 도구와 필요한 매개변수를 식별합니다).
*   이 매개변수들을 사용하여 해당 도구를 실행합니다.
*   도구의 실행 결과(응답)를 LLM의 토큰 스트림에 다시 추가합니다.
*   이어서 토큰 생성을 재개합니다.

LLM이 출력을 생성하는 동안 도구 호출은 실시간으로 처리될 수 있으며, 도구에서 반환된 정보는 모델의 컨텍스트(context)에 직접 통합됩니다!

**프롬프트 기반 도구 활용(Prompt-based tool usage).** 미세 조정을 통해 LLM에게 도구를 호출하도록 가르치는 방식은—대개 사람의 주석(human annotation)을 통한—대규모 훈련 데이터셋을 구축해야 하는 어려움이 있었습니다. 그러나 LLM 기능이 빠르게 향상됨에 따라, 후속 연구에서는 도구 활용을 위한 인컨텍스트 학습(in-context learning) 기반 접근 방식에 더 많은 초점을 맞추게 되었습니다. 모델의 프롬프트(prompt) 내에서 사용 가능한 도구를 단순히 설명하는 것만으로도 충분한데, 굳이 언어 모델을 미세 조정할 필요가 있을까요?

(from [4, 5])
프롬프트 기반 도구 활용 방식은 인간의 수고를 현저히 줄여주며, LLM이 접근하고 사용할 수 있는 도구의 수를 획기적으로 확장시킬 수 있게 합니다. 예를 들어, 이 분야의 최신 연구에서는 LLM을 수백 개 [4] 또는 심지어 수천 개 [5]의 도구와 성공적으로 통합하고 있습니다. 이러한 통합을 위해, 우리는 각 도구를 일반 API(generic API)로 간주하고, 관련 API의 스키마(schema)를 모델의 프롬프트에 컨텍스트로 제공합니다. 이 접근 방식은 LLM이 표준화된 구조를 사용하여 인터넷상의 임의의 API와 연동될 수 있도록 함으로써, 정보 검색, 다른 머신러닝 모델 호출, 여행 예약, 캘린더 관리 등 무수히 많은 애플리케이션을 가능하게 합니다.

“오늘날, 우리는 콘텐츠 저장소, 비즈니스 도구, 개발 환경을 포함하여 데이터가 존재하는 시스템에 AI 비서를 연결하기 위한 새로운 표준인 모델 컨텍스트 프로토콜(Model Context Protocol, MCP)을 오픈 소싱합니다. 그 목표는 최첨단 모델이 더 좋고 더 관련성 높은 응답을 생성하도록 돕는 것입니다.” - [15]에서 발췌

Anthropic이 제안한 모델 컨텍스트 프로토콜(MCP)은 LLM이 임의의 도구와 상호작용할 수 있도록 하는 아이디어를 확장한 최신 프레임워크로 각광받고 있습니다. 간단히 말해, MCP는 외부 시스템이 LLM의 프롬프트에 컨텍스트를 제공하는 데 사용되는 형식을 표준화합니다. 복잡한 문제를 해결하기 위해 LLM은 시간이 지남에 따라 점진적으로 더 많은 외부 도구 세트와 통합되어야 할 필요성이 커지고 있으며, MCP는 이러한 통합 과정을 간소화하기 위한 표준 형식을 제안합니다. 또한, 개발자들이 MCP 서버라고 불리는 사전 구축된 통합(pre-built integrations)을 쉽게 생성할 수 있도록 지원하며, 이 서버는 모든 LLM이 다양한 사용자 정의 데이터 소스(custom data sources)와 연결하는 데 활용될 수 있습니다. 아래 그림을 참조하십시오.

**MCP의 일반 아키텍처 묘사** (source)

도구 활용에 대해 더 깊이 탐구하고 싶은 독자들을 위해, 이 주제에 대한 다음 개요 시리즈를 참조하시기 바랍니다.
*   [도구를 사용하도록 LLM 미세 조정하기](link)
*   [프롬프트 기반 도구 사용](link)
*   [LLM을 코드 인터프리터와 통합하기](link)
*   [LLM이 자체 도구를 생성하도록 허용하기](link)

**도구 활용의 한계.** 도구 활용 패러다임의 강력함에도 불구하고, 도구 활용 LLM의 기능은 결국 그들의 추론 능력(reasoning capabilities)에 의해 제한됩니다. 도구를 효과적으로 활용하려면, LLM은 다음과 같은 능력을 갖추어야 합니다.
*   복잡한 문제를 더 작은 하위 작업으로 체계적으로 분해합니다.
*   당면한 문제를 해결하는 데 가장 적합한 도구를 신중하게 결정합니다.
*   올바른 형식으로 관련 도구에 대한 호출을 안정적으로 생성합니다.

복잡한 도구 활용은 LLM이 효과적인 오케스트레이터(orchestrator) 역할을 수행해야 함을 요구하며, 이는 모델의 추론 능력과 전반적인 신뢰성에 크게 의존합니다. 최근 연구에서는 이러한 한계를 극복하기 위해 LLM의 추론 능력을 강화하고, 도구 선택의 정확도를 높이는 방향으로 발전하고 있습니다.

**추론 모델(Reasoning Models)**
에이전시(agency)와 추론(reasoning) 사이의 긴밀한 관계를 고려할 때, 추론 능력은 수년간 LLM 연구의 핵심 초점이었습니다. 현재 추론 연구에 대한 더 심층적인 개요는 아래 링크를 참조하십시오. 하지만 여기서는 완전성을 위해 추론 모델(reasoning models)의 핵심 아이디어를 간략하게 다룰 것입니다.

**추론 모델의 신비 해소하기**
Cameron R. Wolfe, Ph.D. · 2월 18일
[전체 스토리 읽기](link)

**CoT 프롬프팅(CoT prompting).** LLM이 처음 대중의 인기를 얻었을 때, 이 모델들에 대한 가장 흔한 비판 중 하나는 복잡한 추론을 효과적으로 수행할 수 없다는 것이었습니다. 그러나 연쇄 사고(Chain of Thought, CoT) 프롬프팅 [6, 7]에 대한 연구는 기존의 바닐라 LLM(vanilla LLMs)조차 우리가 처음 예상했던 것보다 추론에 훨씬 더 능하다는 것을 밝혀냈습니다. CoT 프롬프팅의 기본 아이디어는 매우 간단합니다. LLM에게 최종 출력을 직접 요구하는 대신, 최종 답변에 도달하기 전에 근거(rationale) 또는 설명(explanation)을 먼저 생성하도록 요청하는 것입니다. 아래 그림을 참조하십시오.

(from [7])
흥미롭게도, 이 접근 방식은 추론 작업에서 바닐라 LLM의 성능을 획기적으로 향상시키며, 이는 우리가 올바른 접근 방식을 찾아낼 수 있다면 LLM이—합리적인 수준까지—복잡한 추론을 수행할 수 있음을 시사합니다.

**추론 모델.** CoT 프롬프팅은 매우 효과적이며 모든 현대 LLM의 핵심적인 구성 요소입니다. 예를 들어, ChatGPT와 같은 모델들은 답변과 함께 CoT를 기본적으로 제공하는 경우가 많습니다. 그러나 기존의 CoT 추론 방식은 다소 정적입니다. 전체 추론 과정이 LLM이 생성한 CoT를 중심으로 이루어지며, 해결되는 문제의 복잡성에 기반한 동적인 적응(dynamic adaptation)이 부족하다는 한계가 있었습니다.

(source)
이러한 한계를 극복하기 위해 최근 연구는 추론에 특화된 LLM(즉, 추론 모델)을 개발하기 위한 새로운 훈련 전략을 도입했습니다. 이 모델들은 표준 LLM과 달리 문제 해결에 접근하는 방식이 다릅니다. 질문에 대한 답변을 제공하기 전에 가변적인 시간 동안 "생각"하는 과정을 거칩니다.

(from [8])
추론 모델의 사고(thoughts)는 표준 연쇄 사고와 유사하지만, 그 길이가 훨씬 길고 (즉, 수천 개의 토큰에 달할 수 있음), 되돌아가기(backtracking) 및 자체 개선(self-refinement)과 같은 복잡한 추론 행동을 보이는 경향이 있습니다. 또한, 해결되는 문제의 난이도에 따라 동적으로 적응할 수 있습니다. 즉, 더 어려운 문제일수록 더 긴 CoT를 필요로 합니다. 이러한 추론 모델을 가능하게 한 핵심 발전은 검증 가능한 보상으로부터의 강화 학습(reinforcement learning from verifiable rewards, RLVR)을 통한 대규모 후속 훈련(large-scale post-training)이었습니다. 검증 가능한 문제(예: 수학 또는 코딩)에 대한 실제 정답(ground truth solutions) 데이터셋이 있다면, LLM이 생성한 답변이 올바른지 간단히 확인하고 이 신호를 사용하여 RL로 모델을 훈련할 수 있습니다. 이 훈련 과정에서 추론 모델은 RL 기반의 자체 진화(self-evolution)를 통해 검증 가능한 추론 문제를 해결하기 위한 길고 체계적인 연쇄 사고를 생성하는 방법을 자연스럽게 학습합니다.

“우리는 어떠한 지도 데이터(supervised data) 없이도 LLM이 추론 능력을 개발할 수 있는 잠재력을 탐구하며, 순수한 강화 학습(reinforcement learning) 과정을 통한 자체 진화에 초점을 맞춥니다.” - [8]에서 발췌

**추론 궤적(Reasoning trajectories).** 요약하자면, RLVR을 통한 대규모 후속 훈련으로 학습된 추론 모델은 표준 LLM의 동작 방식을 혁신적으로 변화시킵니다. 단순히 출력을 직접 생성하는 대신, 추론 모델은 먼저 추론 작업을 분해하고 해결하는 임의로 긴 CoT 1을 생성합니다. 이것이 바로 모델의 "사고(thinking)" 과정입니다. 이 추론 흔적(reasoning trace)의 길이를 조절함으로써 모델이 얼마나 깊이 "생각"하는지를 제어할 수 있습니다. 예를 들어, OpenAI의 최신 o-시리즈 추론 모델은 사용자의 요구에 따라 낮음, 중간, 높음 수준의 추론 노력을 제공하는 옵션을 갖추고 있습니다.

**추론 모델의 입출력 서명**
모델은 여전히 프롬프트가 주어지면 단일 최종 출력을 생성하지만, 그 과정에서 생성되는 추론 궤적은 계획(planning), 되돌아가기(backtracking), 모니터링(monitoring), 평가(evaluation) 등 다양한 고급 행동을 암묵적으로 보여줍니다. 이러한 추론 궤적과 그 속성의 구체적인 예시는 DeepSeek-R1이 생성한 200만 개 이상의 추론 흔적 예시를 포함하는 [Synthetic-1 데이터셋](link)에서 확인하실 수 있습니다.

**추론 + 에이전트.** 최근 추론 분야의 눈부신 발전을 고려할 때, 지시 사항에 대해 계획하고 효과적으로 추론할 수 있을 만큼 충분히 유능한 LLM은 문제를 스스로 분해하고, 각 구성 요소를 해결하며, 최종 해결책에 도달할 수 있어야 합니다. 복잡한 문제를 해결하기 위해 인간의 개입(human intervention)보다는 LLM에 더 많은 자율성(autonomy)을 부여하고 그들의 능력에 의존하는 것이 바로 에이전트 시스템의 핵심 아이디어입니다. 에이전트의 개념을 더욱 명확히 하기 위해, 이제 이러한 유형의 시스템을 설계하는 데 활용될 수 있는 ReAct 프레임워크에 대해 논의해 봅시다.

**ReAct 프레임워크 [1]**
“LLM의 도움으로, 언어가 근본적인 인지 메커니즘(cognitive mechanism)으로서 상호작용(interaction)과 의사 결정(decision making)에 중요한 역할을 할 것이라는 점이 더욱 분명해지고 있습니다.” - [1]에서 발췌

ReAct [1]—REasoning(추론)과 ACTion(행동)의 줄임말—은 LLM 에이전트를 활용하여 복잡한 문제를 자율적으로 분해하고 해결하기 위해 제안된 최초의 일반 프레임워크 중 하나입니다. ReAct는 LLM을 핵심 엔진으로 삼아 순차적이고 다단계적인 문제 해결 과정을 수행한다고 이해할 수 있습니다. 각 시간 단계 t에서 LLM은 사용 가능한 모든 피드백을 통합하고, 해결하려는 문제의 현재 상태를 고려하여, 미래를 위한 최선의 행동 방침(course of action)을 효과적으로 추론하고 선택할 수 있도록 합니다. 거의 모든 LLM 시스템이 순차적으로 모델링될 수 있다는 점을 고려할 때, ReAct는 매우 일반적이고 강력한 프레임워크입니다.

**에이전트를 위한 프레임워크 생성하기**
특정 시간 단계 t에서, 우리의 에이전트는 환경으로부터 관찰(observation) o_t를 받습니다. 이 관찰을 기반으로, 에이전트는 어떤 행동 a_t를 취하기로 결정합니다. 이 행동은 중간 단계일 수 있습니다—예: 문제 해결에 필요한 데이터를 찾기 위해 웹을 검색하는 것—또는 최종 단계일 수 있습니다 (즉, 관심 있는 문제를 "해결"하는 최종 행동). 우리는 에이전트가 이 행동을 생성하는 데 사용하는 함수를 정책(policy) π 2로 정의합니다. 정책은 컨텍스트—에이전트의 이전 행동과 관찰의 연결된 목록—를 입력으로 받아 다음 행동 a_t를 출력으로 예측하며, 이는 결정론적(deterministically)이거나 확률론적(stochastically)일 수 있습니다 3. 아래에 묘사된 바와 같이, 이 관찰과 행동의 루프는 우리의 에이전트가 최종 행동을 출력할 때까지 지속됩니다.

**에이전트를 위한 관찰-행동 루프**
ReAct [1]는 위에 표시된 관찰-행동 루프에 한 가지 핵심적인 수정을 가합니다. 정책 A에 의해 출력될 수 있는 잠재적 행동 공간(space of potential actions)은 일반적으로 에이전트가 취할 수 있는 중간 및 최종 행동 세트를 포함합니다. 예를 들어, 웹에서 데이터를 검색하거나 문제에 대한 최종 해결책을 출력하는 것 등입니다. 그러나 ReAct는 행동 공간을 언어(language)를 포함하도록 확장하여, 에이전트가 전통적인 행동을 취하는 대신 텍스트 출력을 행동으로 생성할 수 있도록 합니다. 다시 말해, 에이전트는 "생각"하기를 선택할 수 있습니다. 아래 그림을 참조하십시오.

**ReAct 프레임워크**
공식적으로, 우리는 위에서 보듯이 사고(thought)를 특별한 종류의 행동으로 정의할 수 있습니다. 프레임워크의 이름에서 유추할 수 있듯이, ReAct의 주요 동기는 추론(reasoning)과 행동(action) 사이의 최적의 균형을 찾는 것입니다. 인간과 유사하게, 에이전트는 환경에서 취하는 행동을 미리 생각하고 계획할 수 있어야 합니다. 추론과 행동은 서로 상생하는 관계(symbiotic relationship)를 가집니다.

“추론 흔적(reasoning traces)은 모델이 행동 계획을 유도하고, 추적하며, 업데이트하는 데 도움을 주며, 행동은 모델이 지식 기반(knowledge bases)이나 환경과 같은 외부 소스와 상호작용하고 추가 정보를 수집할 수 있도록 합니다.” - [1]에서 발췌

**에이전트는 어떻게 사고할까요? 에이전트의 행동 공간 예시**
에이전트의 전통적인 행동 공간은 이산적(discrete)이며—대부분의 경우—상대적으로 제한적입니다. 예를 들어, 질문-답변에 특화된 에이전트는 다음과 같은 행동 옵션을 가질 수 있습니다.
*   관련 웹페이지를 검색하기 위해 구글 검색을 수행합니다.
*   특정 웹페이지에서 필요한 정보를 가져옵니다.
*   최종 답변을 반환합니다.

이 에이전트가 해결책을 향해 나아가는 동안 취할 수 있는 행동은 제한적입니다. 대조적으로, 언어의 공간은 사실상 무한합니다. 결과적으로, ReAct 프레임워크는 강력한 언어 모델을 정책으로 사용하는 것을 요구합니다. 성능에 도움이 되는 유용한 사고를 생성하기 위해, 우리 에이전트 시스템의 LLM 백엔드(backend)는 고급 추론 및 계획 능력(planning capabilities)을 필수적으로 갖추어야 합니다!

“이 확장된 행동 공간에서의 학습은 어렵고 강력한 언어 사전 지식(language priors)을 필요로 합니다… 우리는 주로 고정된 대규모 언어 모델이… 몇 개의 인컨텍스트 예시(few-shot in-context examples)로 프롬프트되어 도메인별 행동(domain-specific actions)과 자유 형식 언어 사고(free-form language thoughts)를 모두 생성하여 작업을 해결하는 설정에 중점을 둡니다.” - [1]에서 발췌

**사고 패턴(Thought patterns).** 에이전트가 생성할 수 있는 유용한 사고 패턴의 일반적인 예로는 주어진 작업을 분해하고, 항목별 행동 계획을 수립하며, 최종 해결책을 향한 진행 상황을 추적하거나, 단순히 문제 해결과 관련될 수 있는 정보—LLM의 암묵적 지식 기반(implicit knowledge base)에서 도출된—를 출력하는 것이 포함됩니다. 에이전트는 사고 능력을 활용하여 문제가 어떻게 해결되어야 하는지 명시적으로 설명한 다음, 이 계획을 실행하고—실행 과정을 모니터링합니다. 위의 두 예시 모두에서 에이전트는 문제를 해결할 때 수행해야 할 다음 단계를 명시적으로 작성합니다. 예를 들어, "다음으로, 나는 ~해야 한다…" 또는 "나는 ~을 검색해야 한다…"와 같이 말입니다.

**ReAct 에이전트에 주어진 몇 가지 예시 (from [1])**
*   **질문:** 음악가이자 풍자가인 Allie Goertz는 "심슨 가족"의 캐릭터 밀하우스에 대한 노래를 썼는데, 맷 그로닝은 밀하우스의 이름을 누구의 이름을 따서 지었는가?
    *   **사고 1:** 질문은 "심슨 가족" 캐릭터 밀하우스가 누구의 이름을 따서 지어졌는가로 단순화된다. 나는 밀하우스를 검색하고 누구의 이름을 따서 지어졌는지 찾기만 하면 된다.
    *   **행동 1:** Search[밀하우스]
    *   **관찰 1:** 밀하우스 무솔리니 반 후텐은 폭스 애니메이션 TV 시리즈 "심슨 가족"의 반복 출연 캐릭터로, 파멜라 헤이든이 목소리를 맡았고 맷 그로닝이 만들었다.
    *   **사고 2:** 이 단락은 밀하우스가 누구의 이름을 따서 지어졌는지 알려주지 않는다. "named after"를 찾아볼 수 있을 것 같다.
    *   **행동 2:** Lookup[named after]
    *   **관찰 2:** (결과 1 / 1) 밀하우스는 미국 대통령 리처드 닉슨의 이름을 따서 지어졌는데, 그의 중간 이름은 밀하우스였다.
    *   **사고 3:** 밀하우스는 미국 대통령 리처드 닉슨의 이름을 따서 지어졌으므로, 답은 리처드 닉슨이다.
    *   **행동 3:** Finish[리처드 닉슨]
*   **질문:** 핀란드 록 그룹에 관한 다큐멘터리는 아담 클레이튼 파월인가, 아니면 사이마 제스처인가?
    *   **사고 1:** 나는 아담 클레이튼 파월과 사이마 제스처를 검색하고, 어떤 다큐멘터리가 핀란드 록 그룹에 관한 것인지 찾아야 한다.
    *   **행동 1:** Search[아담 클레이튼 파월]
    *   **관찰 1:** [아담 클레이튼 파월]을 찾을 수 없었다. 유사 항목: ['아담 클레이튼 파월 3세', '7번가 (맨해튼)', '아담 클레이튼 파월 주니어 주립 사무소 건물', '이사벨 워싱턴 파월', '아담 파월', '아담 클레이튼 파월 (영화)', '잔카를로 에스포지토'].
    *   **사고 2:** 다큐멘터리를 찾기 위해 아담 클레이튼 파월 (영화)를 검색할 수 있다.
    *   **행동 2:** Search[아담 클레이튼 파월 (영화)]
    *   **관찰 2:** 아담 클레이튼 파월은 리처드 킬버그가 감독한 1989년 미국 다큐멘터리 영화이다. 이 영화는 영향력 있는 아프리카계 미국인 정치인 아담 클레이튼 파월 주니어의 흥망성쇠에 관한 것이다.[3][4] 나중에 PBS 시리즈 "디 아메리칸 익스피리언스"의 일부로 방영되었다.
    *   **사고 3:** 아담 클레이튼 파월 (영화)는 아프리카계 미국인 정치인에 관한 다큐멘터리이지, 핀란드 록 그룹에 관한 것이 아니다. 따라서 핀란드 록 그룹에 관한 다큐멘터리는 사이마 제스처일 것이다.
    *   **행동 3:** Finish[사이마 제스처]
*   **질문:** 니콜라스 레이와 엘리아 카잔은 어떤 직업을 공통으로 가지고 있는가?
    *   **사고 1:** 나는 니콜라스 레이와 엘리아 카잔을 검색하고, 그들의 직업을 찾은 다음, 그들이 공통으로 가진 직업을 찾아야 한다.
    *   **행동 1:** Search[니콜라스 레이]
    *   **관찰 1:** 니콜라스 레이 (본명 레이몬드 니콜라스 키엔즐 주니어, 1911년 8월 7일 - 1979년 6월 16일)는 미국의 영화 감독, 시나리오 작가, 배우였으며, 1955년 영화 "이유 없는 반항"으로 가장 잘 알려져 있다.
    *   **사고 2:** 니콜라스 레이의 직업은 감독, 시나리오 작가, 배우이다. 다음으로 엘리아 카잔을 검색하여 그의 직업을 찾아야 한다.
    *   **행동 2:** Search[엘리아 카잔]
    *   **관찰 2:** 엘리아 카잔은 미국의 영화 및 연극 감독, 프로듀서, 시나리오 작가, 배우였다.
    *   **사고 3:** 엘리아 카잔의 직업은 감독, 프로듀서, 시나리오 작가, 배우이다. 따라서 니콜라스 레이와 엘리아 카잔이 공통으로 가진 직업은 감독, 시나리오 작가, 배우이다.
    *   **행동 3:** Finish[감독, 시나리오 작가, 배우]
*   **질문:** 아서스 매거진과 퍼스트 포 우먼 중 어떤 잡지가 먼저 창간되었는가?

이 예시에서 ReAct 에이전트는 취하는 모든 구체적인 행동 전에 사고를 출력하도록 명시적으로 프롬프트됩니다. 전통적인 LLM과 달리, ReAct 에이전트는 프롬프트당 단일 출력을 생성하지 않습니다. 오히려 에이전트는 다음과 같이 순차적으로 출력을 생성합니다.
*   수행할 행동을 선택합니다 (구체적인 행동 또는 사고).
*   이 행동을 기반으로 환경으로부터 피드백을 받습니다 (예: 검색 쿼리에서 검색된 정보).
*   이 새로운 컨텍스트로 다음 행동을 계속합니다.

결국 최종 행동에 도달하여 문제 해결 과정의 끝을 알립니다. 아래 그림을 참조하십시오. 이러한 상태 기반(stateful), 순차적 문제 해결 접근 방식은 에이전트의 특징이며, 에이전트를 표준 LLM과 구별하는 데 핵심적인 역할을 합니다.

**ReAct로 문제를 순차적으로 해결하기**

**의사 결정(Decision making).** 의사 결정 작업에 대한 ReAct 설정은 지식 집약적 추론 작업의 설정과 매우 유사합니다. 두 의사 결정 작업 모두에서 인간은 ReAct 에이전트의 인컨텍스트 예시로 사용되는 여러 추론 궤적을 수동으로 주석 처리합니다. 그러나 지식 집약적 추론 작업과 달리, 의사 결정 작업에 ReAct가 사용하는 사고 패턴은 더욱 희소합니다. 모델은 언제 어떻게 생각해야 할지 결정하는 데 재량권(discretion)을 사용하도록 프롬프트됩니다. 또한, ReAct 에이전트는 WebShop 데이터셋에 사용할 수 있는 더 다양한 도구와 행동을 제공받습니다. 예를 들어, 검색, 필터링, 제품 선택, 제품 속성 선택, 제품 구매 등입니다. 이 애플리케이션은 더 복잡한 환경과 상호작용할 때 ReAct의 유용성을 입증하는 좋은 테스트 역할을 합니다.

**ReAct는 효과적인가요?**
위에 설명된 ReAct 에이전트는 여러 기준선(baselines)과 비교되었습니다.
*   **프롬프팅(Prompting)**: 예시 궤적에서 사고, 행동, 관찰을 제거하고 질문과 답변만 남긴 few-shot 프롬프트.
*   **CoT 프롬프팅(CoT prompting)**: 위와 동일하지만, 모델은 최종 해결책을 출력하기 전에 연쇄 사고를 생성하도록 프롬프트됩니다 4.
*   **행동(Act, action-only)**: ReAct 궤적에서 사고를 제거하고 관찰과 행동만 남깁니다.
*   **모방(Imitation)**: 인간의 추론 궤적을 모방하기 위해 모방 학습(imitation learning) 및/또는 강화 학습(reinforcement learning)을 통해 훈련된 에이전트 (예: BUTLER).

아래 표에 나타난 바와 같이, ReAct 프레임워크는 단순히 행동만 하는(Act) 설정보다 일관되게 우수한 성능을 보여주며, 에이전트가 행동을 수행하면서 동시에 사고하는 능력이 얼마나 중요한지 명확하게 드러냅니다. CoT 프롬프팅은 어떤 경우에는 ReAct보다 뛰어난 강력한 기준선으로 작용하지만, LLM이 환각(hallucination)을 일으키기 쉬운 시나리오에서는 어려움을 겪는 경향이 있습니다. ReAct는 이러한 상황에서 외부 정보원을 활용하여 환각을 효과적으로 피할 수 있습니다. 물론, ReAct 에이전트의 성능을 더욱 개선할 여지는 여전히 많습니다. 초기 연구 [1]에서는 에이전트가 상당히 취약하다고 지적되었으며, 비정보적인 검색만으로도 실패로 이어질 수 있다고 언급되었습니다. 그러나 최근 연구들은 이러한 취약점을 보완하고 신뢰성을 높이는 방향으로 발전하고 있습니다.

(from [1])

**ReAct + CoT.** ReAct는 문제 해결 접근 방식에서 사실적이고 근거가 있습니다. CoT 프롬프팅은 외부 지식에 기반하지 않아 환각된 사실로 인해 어려움을 겪을 수 있지만, 이 접근 방식은 복잡한 추론 작업을 해결하기 위한 구조를 공식화하는 데 여전히 탁월합니다. ReAct는 에이전트의 추론 궤적에 관찰, 사고 및 행동의 엄격한 구조를 부과하는 반면, CoT는 추론 과정을 공식화하는 데 더 많은 유연성을 가집니다.

(from [1])
두 접근 방식 5의 장점을 모두 활용하기 위해, 우리는 그들 사이를 유연하게 전환하는 전략을 사용할 수 있습니다! 예를 들어, ReAct가 N단계 후에도 답변을 찾지 못하면 CoT 프롬프팅으로 전환하거나 (즉, ReAct → CoT), 여러 CoT 샘플을 취합한 후 답변들 사이에 불일치가 존재하면 ReAct를 사용하여 검증할 수 있습니다 (즉, CoT → ReAct). 위에 표시된 바와 같이, 이러한 후퇴 접근 방식(backoff approach)—어느 방향으로든—은 에이전트의 문제 해결 능력을 크게 향상시킵니다.

**에이전트에 대한 선행 연구들**
ReAct가 AI 에이전트를 위해 제안된 (논란의 여지는 있지만) 최초의 지속적인 프레임워크였지만, 에이전트 분야에서는 그 이전에 다양한 영향력 있는 논문과 아이디어들이 제시되었습니다. 여기서는 이러한 주요 선행 연구들 중 일부와 ReAct 프레임워크와의 비교를 간략하게 설명하여, ReAct가 이전 작업들을 기반으로 어떻게 더 유용하고 대중적인 프레임워크로 발전할 수 있었는지 이해를 돕고자 합니다.

(from [10])
내적 독백(Inner monologue, IM) [10]은 ReAct와 가장 비교할 만한 작업 중 하나였으며, 로봇 공학 환경에서의 계획(planning)에 성공적으로 적용되었습니다. 위에 표시된 바와 같이, IM은 LLM을 여러 도메인별 피드백 메커니즘(domain-specific feedback mechanisms)과 통합합니다. 예를 들어, 장면 설명자(scene descriptors) 또는 성공 감지기(success detectors) 등이 있습니다. ReAct와 다소 유사하게, IM은 LLM을 활용하여 외부 환경으로부터 반복적으로 행동하고, 사고하며, 피드백을 받음으로써—물건을 집는 것과 같은—특정 작업의 해결책을 계획하고 모니터링하는 데 사용됩니다.

“우리는 구체화된 컨텍스트(embodied contexts)에서 사용되는 LLM이 자연어를 통해 제공되는 피드백 소스에 대해 어느 정도 추론할 수 있는지 조사합니다… 우리는 환경 피드백을 활용함으로써 LLM이 로봇 제어 시나리오에서 더 풍부하게 처리하고 계획할 수 있도록 하는 내적 독백을 형성할 수 있다고 제안합니다.” - [10]에서 발췌

IM은 LLM을 자연어(natural language)를 넘어선 로봇 제어와 같은 도메인에서 문제 해결을 위한 일반적인 도구로 활용할 수 있는 잠재력을 보여주었습니다. 그러나 ReAct와 비교할 때, IM 내에서 LLM이 "생각"하는 능력은 상대적으로 제한적이었습니다. 모델은 환경으로부터 피드백을 관찰하고 다음에 무엇을 해야 할지 결정할 수 있을 뿐이었습니다. ReAct는 에이전트가 광범위하고 자유 형식의 사고를 출력할 수 있도록 함으로써 이러한 한계를 극복했습니다.

(from [14])
대화형 의사 결정을 위한 LLM(LLMs for interactive decision making, LID) [14]은 언어를 계획 및 행동을 위한 일반적인 매체로 사용하여 순차적 문제를 해결하기 위한 언어 기반 프레임워크를 제안했습니다. 연구자들은 다양한 작업의 컨텍스트와 행동 공간을 토큰 시퀀스로 공식화하여, 임의의 작업을 LLM 호환 가능한 표준화된 형식으로 변환할 수 있도록 했습니다. 그런 다음, 이 데이터는 LLM에 의해 수집되어 강력한 파운데이션 모델(foundation models)이 환경으로부터 피드백을 통합하고 결정을 내릴 수 있도록 합니다. [14]에서 저자들은 모방 학습(imitation learning)을 사용하여 LID를 미세 조정하여 다양한 도메인에서 행동을 정확하게 예측했습니다.

(from [11])
WebGPT [11]는 LLM (GPT-3)을 텍스트 기반 웹 브라우저와 통합하여 질문에 더 효과적으로 답변하는 방법을 탐구했습니다. 이 작업은 개방형 도구 활용(open-ended tool use)의 초기 개척자 중 하나로, LLM에게 웹을 공개적으로 검색하고 탐색하는 방법을 가르쳤습니다. 그러나 WebGPT는 인간의 작업 해결책으로 구성된 대규모 데이터셋에 대해 명시적으로 미세 조정되었습니다 (즉, 행동 복제(behavior cloning) 또는 모방 학습). 따라서 이 시스템은—매우 미래 지향적이고 효과적임에도 불구하고 (즉, 50% 이상의 경우에서 인간의 답변보다 선호되는 답변을 생성함)—많은 양의 인간 개입을 필요로 했습니다. 그럼에도 불구하고, 인간 피드백을 통한 LLM 에이전트 미세 조정은 오늘날에도 활발한 연구 주제이며, WebGPT는 이 분야의 기초적인 작업으로 평가됩니다.

(from [12])
LLM의 광범위한 기능에서 영감을 받아, Gato [12]는 여러 양식(modalities), 작업 및 도메인에 걸쳐 행동할 수 있는 단일 "일반주의(generalist)" 에이전트로 소개되었습니다. 예를 들어, Gato는 아타리 게임, 이미지 캡셔닝, 로봇 팔 조작 등에 사용되었습니다. 보고서에 설명된 바와 같이, Gato는 "자신의 컨텍스트를 기반으로 텍스트, 관절 토크(joint torques), 버튼 누름 또는 다른 토큰을 출력할지 결정할 수 있습니다." 이 모델은 거의 모든 문제를 해결할 수 있는 자율 시스템(autonomous system)을 만드는 목표를 향해 진정으로 나아갔습니다. 그러나 WebGPT와 유사하게, Gato는 많은 문제 시나리오에 걸쳐 컨텍스트와 행동의 방대한 데이터셋—모두 평면적인 토큰 시퀀스로 표현됨—을 수집하는 모방 학습(imitation learning) 접근 방식을 통해 훈련되었습니다.

(from [13])
계획을 통한 추론(Reasoning via Planning, RAP) [13]은 LLM에게 더 나은 세계 모델(world model)—즉, LLM이 행동하는 환경과 그로부터 오는 보상에 대한 이해—을 부여하여, 복잡하고 다단계적인 문제에 대한 해결책을 계획하는 LLM의 능력을 향상시키는 것을 목표로 합니다. 특히, LLM은 몬테카를로 트리 탐색(Monte Carlo Tree Search, MCTS)을 통해 탐색될 수 있는 추론 트리(reasoning tree)를 구축하는 데 사용되어 높은 보상을 달성하는 해결책을 찾습니다. 여기서 LLM 자체도 해결책을 평가하는 데 사용됩니다. LLM은 RAP에서 에이전트이자 세계 모델 역할을 합니다!

“LLM (에이전트로서)은 LLM (세계 모델로서)과 보상의 안내를 받아 추론 트리를 점진적으로 구축하며, 탐색(exploration)과 활용(exploitation) 사이의 적절한 균형을 통해 높은 보상을 얻는 추론 경로를 효율적으로 얻습니다.” - [13]에서 발췌

RAP는 유용하고 효과적인 프레임워크였지만, [13]에서는 순전히 텍스트 기반 추론 문제에만 적용되었습니다. ReAct와 같은 일반적인 문제 해결 프레임워크는 아니었습니다. 이 외에도 에이전트 시스템과 높은 수준의 유사성을 가지지만 주로 LLM 추론 능력 향상에 초점을 맞춘 많은 작업들이 있습니다.
*   **선택-추론(Selection-Inference)**은 문제 해결 과정을 선택(또는 계획)과 해결의 교대 단계로 분리하여 LLM 추론 능력을 향상시킵니다. Creswell et al.이 유사한 접근 방식을 개척했습니다.
*   **Re2**는 답변을 도출하기 전에 LLM에게 질문을 다시 읽도록 요청하여 LLM 추론 능력을 향상시키는 프롬프팅 전략입니다.
*   **LLM-증강기(LLM-Augmenter)**는 LLM과 데이터베이스 또는 도메인별 정보 소스를 결합하여 LLM에 유용한 외부 지식을 제공함으로써, 질문-답변 작업에서 근거성(groundedness)을 향상시킵니다.

LLM을 위한 에이전트와 추론의 교차점에 대한 연구(및 훨씬 더 많은 내용)에 대한 더 완전한 조사는 [이 놀라운 글](link)을 참조하십시오.

**"에이전트"란 무엇인가?**
“언어 모델 기반 에이전트의 시작점을 보는 가장 간단한 방법은 모든 도구 사용 언어 모델입니다. 에이전트의 스펙트럼은 여기에서부터 복잡성이 증가합니다.” - Nathan Lambert

업계에서의 폭발적인 인기에 비해, "에이전트"라는 용어는 아직 명확하고 통일된 정의가 부족합니다. 무엇이 "에이전트"로 간주될 자격이 있는지에 대한 논의가 활발하며, 이러한 정의의 불명확성은 오늘날 우리가 다양한 복잡성 스펙트럼에 걸쳐 있는 수많은 에이전트들을 접한다는 사실에서 기인합니다. 높은 수준에서 에이전트의 기능은 어떤 면에서는 LLM의 기능과 유사하게 보일 수 있지만, 에이전트는 일반적으로 문제를 해결하기 위해 훨씬 더 광범위한 전략과 도구(strategies and tools)를 자율적으로 활용할 수 있습니다. 지금까지 배운 정보를 바탕으로, 이제 AI 에이전트가 가질 수 있는 능력의 스펙트럼과 이러한 능력들이 표준 LLM과 어떻게 차별화되는지 이해하기 위한 프레임워크를 제시할 것입니다.

**LLM에서 에이전트로 진화**
본 개요에서 우리는 i) 표준 LLM, ii) 도구 활용, iii) 추론 모델, 그리고 iv) 문제 해결을 위한 자율 시스템을 포함한 다양한 개념들을 살펴보았습니다. LLM의 기본적인 정의에서 시작하여, 이제 이러한 각 아이디어가 표준 LLM의 기능을 기반으로 어떻게 구축되어 본질적으로 더욱 '에이전트적인' 시스템을 만들어낼 수 있는지 단계별로 설명할 것입니다.

**[레벨 0] 표준 LLM.** 시작점으로, 텍스트 프롬프트를 입력으로 받아 텍스트 응답을 출력으로 생성하는 LLM의 표준 설정(위에 묘사됨)을 고려할 수 있습니다. 이 시스템은 문제를 해결하기 위해 외부 시스템을 도입하거나 문제 해결 과정에 어떤 구조도 부과하지 않고 순전히 LLM의 내부 지식 기반에 의존합니다. 더 복잡한 추론 문제를 해결하기 위해, 우리는 추론 스타일 LLM(reasoning-style LLM) 또는 CoT 프롬프팅 접근 방식을 사용하여 추론 궤적을 이끌어낼 수도 있습니다. 아래 그림을 참조하십시오.

**[레벨 1] 도구 활용.** LLM의 내부 지식 기반에만 의존하는 것은 여러 위험을 내포합니다. LLM은 고정된 지식 마감일(knowledge cutoff date)을 가지며, 때로는 환각(hallucinate) 현상을 보이기도 합니다. 이 문제를 완화하기 위해, 우리는 LLM에게 유용한 정보를 검색하고 전문화된 도구로 하위 작업을 해결하기 위한 API 호출을 수행하는 방법을 가르칠 수 있습니다. 이 접근 방식을 통해 LLM은 하위 작업의 해결책을 더 전문화된 시스템에 위임함으로써 문제를 더욱 견고하고 정확하게 해결할 수 있습니다. 아래 그림을 참조하십시오.

**[레벨 2] 문제 분해(Decomposing problems).** LLM이 복잡한 문제를 단일 단계로 즉시 해결할 것이라고 기대하는 것은 비합리적일 수 있습니다. 대신, 우리는 문제가 어떻게 해결되어야 하는지 계획하고 해결책을 반복적으로 도출하는 프레임워크를 구축할 수 있습니다. 이러한 LLM 시스템은 수작업으로 구성될 수 있습니다. 예를 들어, 여러 프롬프트를 연결하거나 여러 프롬프트를 병렬로 실행하고 그 결과를 집계하는 방식입니다. 또는, LLM에 의존하여 문제 해결 전략을 순차적으로 도출하고 실행하는 ReAct와 같은 프레임워크를 사용하여 이러한 수동 노력을 피할 수 있습니다. 물론, LLM으로 복잡한 문제를 분해하고 해결하는 과정은 도구 활용 및 추론 능력과 복잡하게 연결되어 있습니다. LLM은 문제 해결 과정 전반에 걸쳐 다양한 도구에 의존할 수 있으며, 추론 능력은 문제를 해결하기 위한 상세하고 정확한 계획을 수립하는 데 필수적입니다. 더 나아가, 문제 해결에 대한 이러한 LLM 중심 접근 방식은 LLM을 사용한 추론에 제어 흐름(control flow) 개념을 도입합니다. 에이전트의 출력은 문제 해결 단계의 시퀀스를 상태 기반으로 이동하면서 순차적으로 구축됩니다.

**[레벨 3] 자율성 증가.** 위에서 설명한 프레임워크는 오늘날 AI 에이전트의 대부분의 핵심 기능을 포괄합니다. 그러나 우리는 시스템에 더 높은 수준의 자율성을 부여함으로써 에이전트의 유능함을 한층 더 높일 수 있습니다. 예를 들어, 에이전트의 행동 공간에 우리를 대신하여 구체적인 행동(예: 물품 구매, 이메일 전송 또는 소프트웨어 개발에서 풀 리퀘스트(pull request) 열기)을 직접 취할 수 있는 능력을 포함시킬 수 있습니다.

“에이전트는 환경을 인지하고 그 환경에 대해 행동할 수 있는 모든 것입니다… 이는 에이전트가 작동하는 환경과 수행할 수 있는 행동 세트에 의해 특징지어진다는 것을 의미합니다.” - Chip Huyen

지금까지 우리가 논의한 에이전트들은 주로 인간 사용자로부터의 프롬프트를 입력으로 받아 작동합니다. 이 프롬프트가 주어지면, 에이전트들은 사고하고, 행동하며, 적절한 응답을 공식화하는 과정을 시작합니다. 다시 말해, 이 에이전트들은 인간 사용자의 명시적인 요청에 의해 트리거될 때만 행동을 취하는 경향이 있습니다. 그러나 항상 그럴 필요는 없습니다! 우리는 백그라운드에서 지속적으로 작동하며 능동적으로 목표를 추구하는 에이전트를 구축할 수 있습니다. 예를 들어, [개방형 컴퓨터 사용 에이전트(open-ended computer use agents)](link)에 대한 많은 연구가 진행되었으며, 최근에는 AutoGPT, BabyAGI와 같은 자율 에이전트 프로젝트들이 큰 주목을 받았습니다. OpenAI의 [Codex](link)와 같은 초기 시도는 많은 작업을 병렬로 처리하고 심지어 코드베이스에 자체적으로 PR(pull requests)을 생성할 수 있는 클라우드 기반 소프트웨어 엔지니어링 에이전트의 가능성을 보여주었습니다. 오늘날의 자율 에이전트는 복잡한 다단계 프로젝트를 인간의 개입 없이 스스로 계획하고 실행하며, 필요에 따라 계획을 수정하는 수준으로 발전하고 있습니다.

**AI 에이전트 스펙트럼.** 본 개요에서 논의한 모든 개념들을 통합하면, 다음과 같은 특징을 가진 정교한 에이전트 시스템을 구축할 수 있습니다.
*   인간의 직접적인 입력 없이 비동기적으로(asynchronously) 실행됩니다.
*   고급 추론 LLM을 사용하여 복잡한 작업을 해결하기 위한 상세한 계획을 수립합니다.
*   표준 LLM을 활용하여 기본적인 사고를 생성하거나 정보를 종합합니다.
*   우리를 대신하여 외부 세계에서 실제 행동을 취합니다 (예: 항공권 예약 또는 캘린더에 이벤트 추가).
*   구글 검색 API (또는 다른 도구)를 통해 최신 정보를 능동적으로 검색합니다.

각 LLM 스타일—뿐만 아니라 다른 도구나 모델—은 고유한 강점과 약점을 가지고 있습니다. 이러한 다양한 구성 요소들은 에이전트 시스템에 문제 해결의 여러 측면에 유용한 다양한 기능을 제공합니다. 에이전트 시스템의 핵심은 이러한 구성 요소들을 원활하고 신뢰할 수 있는 방식으로 조율하는 데 있습니다. 그러나 에이전트는 스펙트럼 상에 존재하며, 반드시 이러한 모든 기능을 동시에 사용하거나 사용하지 않을 수도 있습니다. 예를 들어, 위에 설명된 고도화된 시스템뿐만 아니라, 기본적인 도구 활용 LLM, 그리고 특정 유형의 문제를 해결하기 위한 일련의 프롬프트 또한 모두 AI 에이전트 시스템의 범주에 속한다고 할 수 있습니다.

**AI 에이전트의 미래**
AI 에이전트가 엄청난 인기를 얻고 있지만, 이 분야의 작업은—연구 및 실제 응용 관점 모두에서—아직 초기 단계에 머물러 있습니다. 우리가 배웠듯이, 에이전트는 순차적인 문제 해결 과정을 통해 작동합니다. 이 과정에서 어떤 단계라도 잘못되면, 에이전트는 전체 작업에서 실패할 가능성이 높습니다. 따라서 신뢰성(reliability)은 복잡하고 동적인 환경에서 효과적인 에이전트를 구축하기 위한 가장 중요한 전제 조건입니다. 다시 말해, 견고한 에이전트 시스템을 구축하려면 훨씬 더 높은 "9"의 신뢰성(more nines of reliability)을 가진 LLM을 개발해야 할 것입니다. 아래의 인용문을 참조하십시오.

“작년에 당신은 [에이전트]를 가로막는 것이 추가적인 '9'의 신뢰성이라고 말했습니다… 그것이 이 소프트웨어 에이전트들이 하루 종일 일을 할 수는 없지만, 몇 분 동안 당신을 도울 수 있는 방식을 여전히 설명하는 방법일 것입니다.” - Dwarkesh Podcast

오늘날 많은 에이전트들은 신뢰성 부족으로 인해 (논란의 여지는 있지만) 여전히 취약하다는 평가를 받습니다. 그러나 LLM 전반(즉, 더 나은 추론 능력 및 새로운 세대의 모델 등장)과 특히 에이전트 분야 모두에서 빠르게 진전이 이루어지고 있습니다. 최근 연구는 특히 [에이전트를 효과적으로 평가하는 방법](link), [다중 에이전트 시스템(multi-agent systems)을 생성하는 기술](link), 그리고 [전문화된 도메인에서 신뢰성을 향상시키기 위해 에이전트 시스템을 미세 조정하는 전략](link)에 중점을 두고 있습니다. 또한, 에이전트의 설명 가능성(explainability), 안전성(safety), 그리고 인간과의 효과적인 협업(human-agent collaboration) 또한 중요한 연구 영역으로 부상하고 있습니다. 이 분야의 연구 속도를 고려할 때, 가까운 미래에 이러한 에이전트 시스템의 기능과 일반성(generality)이 크게 증가할 것으로 예상되며, 이는 우리의 삶과 업무 방식에 혁명적인 변화를 가져올 것입니다.

뉴스레터가 처음이신가요? 안녕하세요! 저는 Cameron R. Wolfe입니다. 넷플릭스(Netflix)의 딥러닝(Deep Learning) 박사이자 선임 연구 과학자입니다. 이것은 독자들이 AI 연구의 중요한 주제를 더 잘 이해하도록 돕는 Deep (Learning) Focus 뉴스레터입니다. 이 뉴스레터는 항상 무료이며 공개적으로 읽을 수 있습니다. 뉴스레터가 마음에 드신다면 [구독하거나](link), [유료 구독을 고려하거나](link), [공유하거나](link), [X](link)와 [LinkedIn](link)에서 저를 팔로우해주세요!

[구독하기](link)

**참고 문헌(Bibliography)**
[1] Yao, Shunyu, et al. "React: Synergizing reasoning and acting in language models." International Conference on Learning Representations (ICLR) . 2023.
[2] Schick, Timo, et al. "Toolformer: Language models can teach themselves to use tools." Advances in Neural Information Processing Systems 36 (2023): 68539-68551.
[3] Thoppilan, Romal, et al. "Lamda: Language models for dialog applications." arXiv preprint arXiv:2201.08239 (2022).
[4] Shen, Yongliang, et al. "Hugginggpt: Solving ai tasks with chatgpt and its friends in hugging face." Advances in Neural Information Processing Systems 36 (2023): 38154-38180.
[5] Patil, Shishir G., et al. "Gorilla: Large language model connected with massive apis." Advances in Neural Information Processing Systems 37 (2024): 126544-126565.
[6] Wei, Jason, et al. "Chain-of-thought prompting elicits reasoning in large language models." Advances in neural information processing systems 35 (2022): 24824-24837.
[7] Kojima, Takeshi, et al. "Large language models are zero-shot reasoners." Advances in neural information processing systems 35 (2022): 22199-22213.
[8] Guo, Daya, et al. "Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning." arXiv preprint arXiv:2501.12948 (2025).
[9] Lambert, Nathan, et al. "T\" ulu 3: Pushing frontiers in open language model post-training." arXiv preprint arXiv:2411.15124 (2024).
[10] Huang, Wenlong, et al. "Inner monologue: Embodied reasoning through planning with language models." arXiv preprint arXiv:2207.05608 (2022).
[11] Nakano, Reiichiro, et al. "Webgpt: Browser-assisted question-answering with human feedback." arXiv preprint arXiv:2112.09332 (2021).
[12] Reed, Scott, et al. "A generalist agent." arXiv preprint arXiv:2205.06175 (2022).
[13] Hao, Shibo, et al. "Reasoning with language model is planning with world model." arXiv preprint arXiv:2305.14992 (2023).
[14] Li, Shuang, et al. "Pre-trained language models for interactive decision-making." Advances in Neural Information Processing Systems 35 (2022): 31199-31212.
[15] Anthropic. “Introducing the Model Context Protocol” https://www.anthropic.com/news/model-context-protocol (2024).

1 추론 모델의 맥락에서, 이러한 연쇄 사고는 추론 궤적(reasoning trajectories) 또는 흔적(traces)이라고도 불립니다.
2 이는 강화 학습(reinforcement learning, RL)에서의 정책(policy) 정의와 상당히 유사합니다. 자세한 내용은 [여기](link)를 참조하십시오. 두 경우 모두 정책은 언어 모델로 구현되며 행동을 출력으로 생성합니다. 에이전트와 RL의 정책 정의 간의 주요 차이점은 정책의 입력입니다. 에이전트의 경우 입력은 현재 관찰(current observation)입니다. RL의 경우 정책의 입력은 환경의 현재 상태(current state)입니다.
3 결정론적(deterministic) 정책과 확률론적(stochastic) 정책의 차이에 대한 자세한 내용은 [여기](link)를 참조하십시오.
4 CoT 프롬프팅은 다수결 투표(majority vote)를 통한 자기 일관성(self-consistency)으로 확장되어 성능을 더욱 향상시킬 수 있습니다.
5 특히, ReAct (또는 다른 에이전트 프레임워크)가 표준 CoT 프롬프팅보다 항상 우수하다고 보장할 수는 없습니다! 이러한 기술들의 상대적 성능은 해결되는 문제의 복잡성과 매우 밀접하게 관련되어 있습니다. CoT 프롬프팅은 사용되는 LLM에 대해 환각이 문제가 될 가능성이 낮은 경우에 매우 잘 작동합니다.