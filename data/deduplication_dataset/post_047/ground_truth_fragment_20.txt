대규모 언어 모델(LLM)의 기능은 빠르게 발전하고 있습니다. 이는 단순한 텍스트 처리 능력을 넘어, 인간과 유사한 방식으로 복잡한 추론과 의사 결정을 수행하는 가능성을 열어주고 있습니다. 이러한 발전은 AI가 실제 세계의 다양한 과제를 해결하고, 사용자의 의도를 파악하여 능동적으로 행동하며, 궁극적으로는 더 긴 시간 범위에 걸쳐 작동하는 고수준 시스템을 만들 수 있습니다. 이러한 시스템을 AI 에이전트 시스템이라고 합니다. 하지만 에이전트의 개념은 기술적 정의를 넘어 윤리적, 사회적 함의를 포함하는 광범위한 논의의 대상이 되고 있습니다. AI 에이전트는 인기 있는 주제이지만, 이 에이전트의 정의와 기능에 대해서는 상당한 혼란이 있습니다. 본 글에서는 이러한 혼란을 해소하고, AI 에이전트의 본질과 미래 방향성을 심층적으로 탐구하고자 합니다. 이 개요에서는 AI 에이전트에 대한 이해를 기본 원리부터 구축할 것입니다. 특히, 최신 연구 동향과 실제 적용 사례를 중심으로 에이전트 기술의 발전 양상을 조명할 것입니다. 우리는 단순한 언어 모델을 넘어, 다중 모달리티 통합, 연속 학습(continual learning) 능력 등 새로운 패러다임이 어떻게 복잡하고 자율적인 시스템을 만들 수 있는지 탐구할 것입니다.

**다중 모달리티(Multi-Modality) LLM의 부상**
LLM의 기능은 위에 묘사되어 있습니다. 텍스트 프롬프트가 주어지면, LLM은 텍스트 응답을 생성합니다. 하지만 이제는 이미지, 오디오, 비디오 등 다양한 형태의 입력과 출력을 처리하며 그 범위를 확장하고 있습니다. 이 기능은 이해하기 쉽고 거의 모든 문제를 해결하는 데 일반화될 수 있습니다. 특히, 복잡한 데이터셋에서 패턴을 학습하고 예측하는 데 탁월한 성능을 보여줍니다. 여러 면에서 LLM의 일반성(generality)은 가장 큰 강점 중 하나입니다. 이는 다양한 산업 분야에서 혁신적인 애플리케이션을 가능하게 하는 원동력이 됩니다. 이 섹션에서는 다중 모달리티 처리, 실시간 데이터 연동 등 새로운 기능이 이 텍스트-투-텍스트 구조를 활용하여 LLM에 어떻게 통합될 수 있는지 설명할 것입니다. 곧 알게 되겠지만, 현대 AI 에이전트의 고급 기능은 대부분 이 기본 기능 위에 구축됩니다. 그리고 이러한 기능들은 사용자 경험을 혁신하고 있습니다.

**외부 지능 연동: LLM의 확장된 능력**
LLM의 역량이 향상되기 시작하면서, 외부 도구와 통합하고 사용하는 방법을 가르치는 것이 AI 연구에서 빠르게 인기 있는 주제가 되었습니다. 이는 LLM이 가진 내재적 한계를 극복하고 실제 세계의 복잡성을 처리하는 데 필수적입니다. LLM과 통합될 수 있는 유용한 도구의 예로는 계산기(calculators), 캘린더(calendars), 검색 엔진(search engines), 코드 인터프리터(code interpreters) 등이 있습니다. 더 나아가, 전문화된 데이터베이스나 다른 ML 모델과의 연동도 활발히 연구되고 있습니다. 왜 이 접근 방식이 그렇게 인기가 많을까요? 간단히 말해, LLM은 (당연히) 모든 작업을 해결하는 데 가장 좋은 도구는 아닙니다. 특정 도메인에서는 고도로 최적화된 전문 시스템이 훨씬 뛰어난 성능을 보입니다. 더 복잡한 문제 해결을 위해, LLM은 다양한 모듈을 통합하고 효율적인 워크플로우를 구성하는 능력, 즉 계획(planning)과 오케스트레이션(orchestration)에 탁월하다는 점을 고려할 때, 우리는 LLM에게 문제 해결 과정의 일부로 이러한 도구를 사용하는 방법을 쉽게 가르칠 수 있습니다! 도구 사용 LLM의 근본적인 아이디어는 LLM에게 문제의 하위 작업(sub-tasks)이나 구성 요소(components)를 더 전문화되거나 견고한 도구에 위임(delegate)할 수 있는 능력을 부여하는 것입니다. 이는 마치 유능한 프로젝트 매니저가 전문가 팀을 활용하는 방식과 유사합니다. LLM은 다양한 전문 도구들을 함께 조율하는 "두뇌" 역할을 합니다. 이로써 LLM은 단순한 챗봇을 넘어 복잡한 작업을 수행하는 진정한 에이전트로 거듭나게 됩니다.

**동적 도구 연동 전략: 적응형 아키텍처**
도구 사용을 위한 미세 조정(Finetuning)은 초기에 널리 사용되던 방식입니다. 예를 들어, LaMDA [2] 또는 Toolformer [3] (위에 묘사됨)와 같은 초기 연구들은 특정 미세 조정(targeted finetuning)을 사용하여 LLM에게 고정된 도구 세트를 활용하는 방법을 가르쳤습니다. 우리는 단순히 LLM의 토큰 스트림(token stream)에 특정 도구에 대한 함수 호출(function call)이 직접 삽입된 훈련 예시를 선별합니다. 하지만 이러한 방식은 새로운 도구가 추가될 때마다 재훈련이 필요하다는 단점이 있었습니다.

**도구 호출의 유연성**
훈련 중에는 이러한 도구 호출이 다른 토큰과 유사하게 취급됩니다. 즉, 텍스트 시퀀스의 일부일 뿐입니다! 그러나 실제 운영 환경에서는 도구 호출의 유연성과 동적 처리가 더욱 중요해졌습니다. 추론 시간(inference time)에 LLM이 도구 호출을 생성하면, 우리는 다음과 같이 처리합니다.
*   토큰 생성을 중단합니다.
*   도구 호출을 파싱합니다 (즉, 사용되는 도구와 해당 매개변수를 결정합니다).
*   이 매개변수들을 사용하여 도구를 호출합니다.
*   도구의 응답을 LLM의 토큰 스트림에 추가합니다.
*   토큰 생성을 계속합니다.

LLM이 출력을 생성하는 동안 도구 호출은 실시간으로 처리될 수 있으며, 도구에서 반환된 정보는 모델의 컨텍스트(context)에 직접 추가됩니다! 이로써 LLM은 외부 세계와 끊임없이 상호작용하며 지식을 확장할 수 있게 됩니다.

프롬프트 기반 도구 사용(Prompt-based tool usage)은 미세 조정의 대안으로 부상했습니다. 미세 조정을 통해 LLM에게 도구를 호출하도록 가르치는 것은—대개 사람의 주석(human annotation)을 통해—대규모 훈련 데이터셋을 선별하는 것을 필요로 합니다. LLM 기능이 향상됨에 따라, 후속 연구에서는 대신 도구 사용을 위한 인컨텍스트 학습(in-context learning) 기반 접근 방식을 강조했습니다. 이는 모델이 실시간으로 새로운 도구를 학습하고 활용할 수 있도록 합니다. 모델의 프롬프트(prompt)에서 사용 가능한 도구를 단순히 설명할 수 있는데, 왜 언어 모델을 미세 조정해야 할까요? 이 질문은 인컨텍스트 학습의 효율성을 잘 보여줍니다.

(from [4, 5])
프롬프트 기반 도구 사용은 인간의 노력을 덜 필요로 하며, LLM이 접근할 수 있는 도구의 수를 크게 늘릴 수 있게 합니다. 예를 들어, 이 분야의 후속 연구에서는 LLM을 수백 개 [4] 또는 심지어 수천 개 [5]의 도구와 통합합니다. 위를 참조하십시오. 이를 위해 우리는 각 도구를 일반 API(generic API)로 취급하고, 관련 API의 스키마(schema)를 모델의 프롬프트에 컨텍스트로 제공합니다. 이러한 유연한 통합 방식은 LLM이 실시간으로 동적 데이터를 처리하고, 복잡한 비즈니스 프로세스를 자동화하며, 정보 찾기, 다른 ML 모델 호출, 휴가 예약, 캘린더 관리 등 수많은 애플리케이션을 가능하게 합니다.

“오늘날, 우리는 콘텐츠 저장소, 비즈니스 도구, 개발 환경을 포함하여 데이터가 존재하는 시스템에 AI 비서를 연결하기 위한 새로운 표준인 모델 컨텍스트 프로토콜(Model Context Protocol, MCP)을 오픈 소싱합니다. 그 목표는 최첨단 모델이 더 좋고 더 관련성 높은 응답을 생성하도록 돕는 것입니다.” - [15]에서 발췌

Anthropic이 제안한 모델 컨텍스트 프로토콜(MCP)은 LLM이 임의의 도구와 상호작용할 수 있도록 하는 아이디어를 확장한 인기 있는 프레임워크입니다. 이는 LLM이 다양한 데이터 소스와 원활하게 연동될 수 있는 길을 열어줍니다. 간단히 말해, MCP는 외부 시스템이 LLM의 프롬프트에 컨텍스트를 제공하는 데 사용되는 형식을 표준화합니다. 이 표준화는 개발 효율성을 크게 높이고, LLM 애플리케이션의 확산에 기여합니다. 실제 환경에서 발생하는 복잡한 문제를 해결하기 위해 LLM은 시간이 지남에 따라 점진적으로 더 많은 외부 도구 세트와 통합되어야 할 것입니다. 이 과정을 간소화하기 위해 MCP는 이러한 통합을 위한 표준 형식을 제안하고, 개발자들이 MCP 서버라고 불리는 사전 구축된 통합(pre-built integrations)을 생성할 수 있도록 합니다. 이는 LLM 에이전트 개발의 진입 장벽을 낮추는 중요한 역할을 합니다. 이 서버는 모든 LLM이 다양한 사용자 정의 데이터 소스(custom data sources)와 연결하는 데 사용될 수 있습니다. 이는 LLM 에이전트의 유연성과 확장성을 극대화합니다. 아래를 참조하십시오.

**MCP의 일반 아키텍처 묘사** (source)

에이전트의 외부 지능 연동에 대해 더 깊이 탐구하고 싶은 분들은 이 주제에 대한 다음 개요 시리즈를 참조하십시오.
*   [다중 모달리티 에이전트 설계](link)
*   [연속 학습을 통한 에이전트 진화](link)
*   [실시간 데이터 연동의 중요성](link)
*   [에이전트의 윤리적 고려 사항](link)

**에이전트의 도전 과제와 미래 방향**
도구 사용 패러다임의 강력함에도 불구하고, 도구 사용 LLM의 기능은 그들의 추론 능력(reasoning capabilities)에 의해 제한됩니다. 이는 에이전트가 복잡한 상황에서 올바른 결정을 내리는 데 근본적인 한계로 작용합니다. 도구를 효과적으로 활용하려면, 우리의 LLM은 다음을 할 수 있어야 합니다. 더 나아가, 예측 불가능한 상황에 대한 대응 능력도 갖춰야 합니다.
*   복잡한 문제를 더 작은 하위 작업으로 분해하고, 각 단계의 상호 의존성을 이해합니다.
*   문제를 해결하는 데 어떤 도구를 사용해야 하는지 결정하며, 도구의 장단점을 평가합니다.
*   올바른 형식으로 관련 도구에 대한 호출을 안정적으로 생성하고, 오류 발생 시 적절히 복구합니다.
*   예측 불가능한 상황에 대한 적응적 학습 능력을 갖춥니다.
*   윤리적 가이드라인을 준수하며 편향되지 않은 결정을 내립니다.

복잡한 도구 사용은 LLM이 효과적인 오케스트레이터(orchestrator)가 되어야 함을 요구하며, 이는 모델의 추론 능력과 전반적인 신뢰성에 크게 의존합니다. 따라서, LLM의 신뢰성과 투명성(transparency)을 높이는 연구가 필수적입니다.

**심층 추론과 의사 결정의 진화**
에이전시(agency)와 추론(reasoning) 사이의 관계를 고려할 때, 추론 능력은 수년간 LLM 연구의 핵심 초점이었습니다. 특히, 복잡한 문제 해결을 위한 인간 수준의 인지 능력 구현이 목표가 되고 있습니다. 현재 추론 연구는 다양한 접근 방식으로 확장되고 있습니다. 하지만 여기서는 완전성을 위해 추론 모델(reasoning models)의 핵심 아이디어를 간략하게 다룰 것입니다.

**추론 메커니즘의 투명성 확보**
Cameron R. Wolfe, Ph.D. · 2월 18일
[전체 스토리 읽기](link)

CoT 프롬프팅(CoT prompting)은 LLM 추론 능력의 한계를 극복하는 중요한 전환점이었습니다. LLM이 처음 인기를 얻었을 때, 이 모델들에 대한 가장 흔한 비판 중 하나는 복잡한 추론을 수행할 수 없다는 것이었습니다. 하지만 최근 연쇄 사고(Chain of Thought, CoT) 프롬프팅 [6, 7]에 대한 연구는 바닐라 LLM(vanilla LLMs)이 우리가 처음 인식했던 것보다 추론에 더 능하다는 것을 밝혀냈습니다. CoT 프롬프팅의 아이디어는 간단합니다. LLM에게 직접 출력을 요청하는 대신, 최종 출력 이전에 근거(rationale) 또는 설명(explanation)을 생성하도록 요청합니다. 이는 모델의 의사 결정 과정을 투명하게 만드는 데 기여합니다. 아래를 참조하십시오.

(from [7])
흥미롭게도, 이 접근 방식은 추론 작업에서 바닐라 LLM의 성능을 크게 향상시키며, 우리가 이러한 능력을 이끌어낼 올바른 접근 방식을 찾을 수 있다면 LLM이—합리적인 수준까지—복잡한 추론을 할 수 있음을 나타냅니다. 이는 LLM이 단순한 패턴 매칭을 넘어선 진정한 이해력을 가질 수 있음을 시사합니다.

추론 모델(Reasoning Models)은 CoT 프롬프팅의 성공에 기반을 둡니다. CoT 프롬프팅은 매우 효과적이며 모든 현대 LLM의 핵심 부분입니다. 예를 들어, ChatGPT는 일반적으로 답변과 함께 CoT를 기본적으로 출력합니다. 그러나 이 추론 접근 방식은 다소 순진합니다. 전체 추론 과정은 LLM이 생성한 CoT를 중심으로 이루어지며, 해결되는 문제의 복잡성에 기반한 동적 적응(dynamic adaptation)은 없습니다.

(source)
이러한 문제를 해결하기 위해 최근 연구는 추론에 특화된 LLM(즉, 추론 모델)을 만들기 위한 새로운 훈련 전략을 도입했습니다. 이는 모델이 단순히 정보를 나열하는 것을 넘어, 깊이 있는 분석과 판단을 수행하도록 돕습니다. 이 모델들은 표준 LLM과 비교하여 문제 해결에 다르게 접근합니다. 마치 인간처럼 질문에 대한 답변을 제공하기 전에 가변적인 시간 동안 "생각"합니다.

(from [8])
이러한 추론 모델의 사고(thoughts)는 표준 연쇄 사고와 같지만, 훨씬 더 심층적이고 구조화되어 있습니다. 복잡한 추론 행동(예: 되돌아가기(backtracking) 및 자체 개선(self-refinement))을 보이는 경향이 있으며, 해결되는 문제의 난이도에 따라 동적으로 적응할 수 있습니다. 즉, 더 어려운 문제는 더 긴 CoT를 필요로 합니다. 이러한 추론 능력을 강화하는 데 핵심적인 발전은 검증 가능한 보상으로부터의 강화 학습(reinforcement learning from verifiable rewards, RLVR)을 통한 대규모 후속 훈련(large-scale post-training)이었습니다. 위를 참조하십시오. 검증 가능한 문제(예: 수학 또는 코딩)에 대한 실제 정답(ground truth solutions) 데이터셋이 있다면, LLM이 생성한 답변이 올바른지 간단히 확인하고 이 신호를 사용하여 RL로 모델을 훈련할 수 있습니다. 이는 모델이 스스로 학습하고 개선하는 데 중요한 역할을 합니다. 이 훈련 과정에서 추론 모델은 RL 기반의 자체 진화(self-evolution)를 통해 검증 가능한 추론 문제를 해결하기 위한 긴 연쇄 사고를 생성하는 방법을 자연스럽게 학습합니다. 이로써 모델은 더욱 복잡하고 추상적인 개념을 이해하고 적용할 수 있게 됩니다.

“우리는 어떠한 지도 데이터(supervised data) 없이도 LLM이 추론 능력을 개발할 수 있는 잠재력을 탐구하며, 순수한 강화 학습(reinforcement learning) 과정을 통한 자체 진화에 초점을 맞춥니다.” - [8]에서 발췌

추론 궤적(Reasoning trajectories)은 모델의 내부 작동 방식을 명확히 보여줍니다. 요약하자면, RLVR을 통한 대규모 후속 훈련으로 훈련된 추론 모델은 아래에 표시된 대로 표준 LLM의 동작을 변경합니다. 출력을 직접 생성하는 대신, 추론 모델은 먼저 추론 작업을 분해하고 해결하는 임의로 긴 CoT 1을 생성합니다. 이 과정은 모델이 단순히 정답을 내놓는 것을 넘어, 문제 해결 과정을 스스로 탐색하고 검증하는 능력을 갖추게 합니다. 이것이 "사고(thinking)" 과정입니다. 이 추론 흔적(reasoning trace)의 길이를 제어함으로써 모델이 얼마나 "생각"하는지 변경할 수 있습니다. 이는 복잡성에 따라 모델의 자원 활용을 최적화하는 데 중요한 통찰력을 제공합니다. 예를 들어, OpenAI의 o-시리즈 추론 모델은 낮음, 중간, 높음 수준의 추론 노력을 제공합니다. 이는 사용자나 시스템이 원하는 정밀도와 비용 사이의 균형을 조절할 수 있게 합니다.

**추론 모델의 의사 결정 과정 시각화**
모델은 여전히 프롬프트가 주어지면 단일 출력을 생성하지만, 추론 궤적은 계획(planning), 되돌아가기(backtracking), 모니터링(monitoring), 평가(evaluation) 등 다양한 고급 행동을 암묵적으로 보여줍니다. 이는 모델이 단순히 답을 내놓는 것을 넘어, 문제 해결 전략을 내부적으로 구축하고 수정하는 과정을 시사합니다. 이러한 추론 궤적과 그 속성의 예시는 DeepSeek-R1이 생성한 200만 개 이상의 추론 흔적 예시를 포함하는 [Synthetic-1 데이터셋](link)을 참조하십시오. 이 데이터셋은 추론 모델의 행동을 분석하는 데 귀중한 자료입니다.

추론과 에이전트의 결합은 인공지능의 자율성을 한 단계 끌어올립니다. 최근 추론 분야의 발전을 고려할 때, 지시 사항에 대해 계획하고 효과적으로 추론할 수 있는 충분히 유능한 LLM은 문제를 분해하고, 문제의 각 구성 요소를 해결하며, 스스로 최종 해결책에 도달할 수 있어야 합니다. 복잡한 문제를 해결하기 위해 인간의 개입(human intervention)보다는 LLM에 더 많은 자율성(autonomy)을 부여하고 그들의 능력에 의존하는 것이 에이전트 시스템의 핵심 아이디어입니다. 이는 AI 시스템의 효율성과 확장성을 극대화하는 방향으로 나아갑니다. 에이전트의 개념을 더 명확히 하기 위해, 이제 이러한 유형의 시스템을 설계하는 데 사용될 수 있는 프레임워크에 대해 논의해 봅시다. 특히, 실제 환경에서 에이전트가 어떻게 작동하는지 집중적으로 살펴볼 것입니다.

**행동 기반 학습 프레임워크: ReAct와 그 너머 [1]**
“LLM의 도움으로, 언어가 근본적인 인지 메커니즘(cognitive mechanism)으로서 상호작용(interaction)과 의사 결정(decision making)에 중요한 역할을 할 것이라는 점이 더욱 분명해지고 있습니다.” - [1]에서 발췌

ReAct [1]—REasoning(추론)과 ACTion(행동)의 줄임말—은 LLM 에이전트를 사용하여 복잡한 문제를 자율적으로 분해하고 해결하기 위해 제안된 최초의 일반 프레임워크 중 하나입니다. 이는 에이전트 설계의 중요한 이정표가 되었습니다. ReAct를 핵심에 LLM을 두고 순차적이고 다단계적인 문제 해결 과정으로 생각할 수 있습니다. 이 프레임워크는 환경과의 지속적인 상호작용을 통해 학습하고 발전합니다. 각 시간 단계 t에서 LLM은 사용 가능한 모든 피드백을 통합하고 해결하려는 문제의 현재 상태를 고려하여, 미래를 위한 최선의 행동 방침(course of action)을 효과적으로 추론하고 선택할 수 있도록 합니다. 이는 강화 학습(Reinforcement Learning)의 기본적인 원리와도 일맥상통합니다. (거의) 모든 LLM 시스템이 순차적으로 모델링될 수 있다는 점을 고려할 때, ReAct는 일반적이고 강력한 프레임워크입니다. 이 프레임워크는 다양한 에이전트 아키텍처의 기반이 되고 있습니다.

**자율 에이전트 설계 원칙**
특정 시간 단계 t에서, 우리의 에이전트는 환경으로부터 관찰(observation) o_t를 받습니다. 이 관찰은 텍스트뿐만 아니라 이미지, 소리 등 다중 모달리티 데이터일 수 있습니다. 이 관찰을 기반으로, 우리의 에이전트는 어떤 행동 a_t를 취하기로 결정할 것입니다. 이때 행동은 단순한 응답을 넘어, 외부 도구 호출이나 물리적 환경 조작을 포함할 수 있습니다. 이 행동은 중간 단계일 수 있습니다—예: 문제 해결에 필요한 데이터를 찾기 위해 웹을 검색하는 것—또는 최종 단계일 수 있습니다 (즉, 관심 있는 문제를 "해결"하는 최종 행동). 중요한 것은 에이전트가 이러한 단계를 스스로 계획하고 실행한다는 점입니다. 우리는 에이전트가 이 행동을 생성하는 데 사용하는 함수를 정책(policy) π 2로 정의합니다. 이 정책은 강화 학습을 통해 최적화될 수 있습니다. 정책은 현재 상태와 이전 상호작용의 기록을 포함하는 컨텍스트를 입력으로 받아 다음 행동 a_t를 출력으로 예측하며, 이는 결정론적(deterministically)이거나 확률론적(stochastically)일 수 있습니다 3. 아래에 묘사된 바와 같이, 이 관찰과 행동의 루프는 우리의 에이전트가 최종 행동을 출력할 때까지 계속됩니다.

**에이전트를 위한 관찰-행동 루프**
ReAct [1]는 위에 표시된 관찰-행동 루프에 한 가지 핵심적인 수정을 가합니다. 이는 에이전트의 내적 추론 과정을 명시적으로 표현하는 것입니다. 정책 A에 의해 출력될 수 있는 잠재적 행동 공간(space of potential actions)은 일반적으로 에이전트가 취할 수 있는 중간 및 최종 행동 세트를 포함합니다. 이 공간은 에이전트의 능력과 환경에 따라 크게 달라집니다. 예를 들어, 웹에서 데이터를 검색하거나 문제에 대한 최종 해결책을 출력하는 것 등입니다. 최근에는 가상 환경 내에서 복잡한 시뮬레이션을 수행하는 행동도 포함됩니다. 그러나 ReAct는 행동 공간을 언어(language)를 포함하도록 확장하여, 에이전트가 전통적인 행동을 취하는 대신 텍스트 출력을 행동으로 생성할 수 있도록 합니다. 이는 '생각'이라는 추상적인 행동을 가능하게 합니다. 다시 말해, 에이전트는 "생각"하기를 선택할 수 있습니다. 이는 문제 해결 과정의 투명성과 설명 가능성을 높이는 데 기여합니다. 아래를 참조하십시오.

**ReAct 프레임워크**
공식적으로, 우리는 위에서 보듯이 사고(thought)를 특별한 종류의 행동으로 정의할 수 있습니다. 이러한 사고는 에이전트의 내부 상태를 반영하고 계획을 수립하는 데 사용됩니다. 프레임워크의 이름에서 유추할 수 있듯이, ReAct의 주요 동기는 추론(reasoning)과 행동(action) 사이의 균형을 찾는 것입니다. 이는 지능적인 에이전트의 핵심 역량입니다. 인간과 유사하게, 에이전트는 환경에서 취하는 행동을 생각하고 계획할 수 있어야 합니다. 이러한 능력은 에이전트가 복잡한 목표를 달성하는 데 필수적입니다. 추론과 행동은 공생 관계(symbiotic relationship)를 가집니다. 서로를 보완하며 에이전트의 전반적인 성능을 향상시킵니다.

“추론 흔적(reasoning traces)은 모델이 행동 계획을 유도하고, 추적하며, 업데이트하는 데 도움을 주며, 행동은 모델이 지식 기반(knowledge bases)이나 환경과 같은 외부 소스와 상호작용하고 추가 정보를 수집할 수 있도록 합니다.” - [1]에서 발췌

**에이전트의 인지 구조와 행동 유연성**
에이전트의 전통적인 행동 공간은 이산적(discrete)이며—대부분의 경우—상대적으로 작습니다. 그러나 현대 에이전트는 훨씬 더 광범위하고 유연한 행동을 수행할 수 있습니다. 예를 들어, 질문-답변에 특화된 에이전트는 다음과 같은 행동 옵션을 가질 수 있습니다 (위에 묘사됨).
*   실시간 웹 검색을 통해 최신 정보를 탐색하고 종합합니다.
*   특정 정보 소스(데이터베이스, 문서 등)에서 필요한 데이터를 추출하고 분석합니다.
*   복잡한 추론을 통해 도출된 최종 해결책이나 행동을 실행합니다.
*   사용자 피드백을 기반으로 스스로 학습하고 정책을 업데이트합니다.

이 에이전트가 해결책을 향해 작업하는 동안 취할 수 있는 행동은 제한적입니다. 하지만 언어 모델의 발전과 함께 행동 공간은 폭발적으로 확장되었습니다. 대조적으로, 언어의 공간은 사실상 무한합니다. 이는 에이전트가 상상할 수 있는 모든 종류의 사고와 계획을 표현할 수 있게 합니다. 결과적으로, ReAct 프레임워크는 강력한 언어 모델을 정책으로 사용하는 것을 요구합니다. 이러한 모델은 복잡한 의사 결정과 계획 수립에 필수적입니다. 성능에 도움이 되는 유용한 사고를 생성하기 위해, 우리 에이전트 시스템의 LLM 백엔드(backend)는 고급 추론 및 계획 능력(planning capabilities)을 갖추어야 합니다! 이는 에이전트의 성공적인 작동에 결정적인 요소입니다.

“이 확장된 행동 공간에서의 학습은 어렵고 강력한 언어 사전 지식(language priors)을 필요로 합니다… 우리는 주로 고정된 대규모 언어 모델이… 몇 개의 인컨텍스트 예시(few-shot in-context examples)로 프롬프트되어 도메인별 행동(domain-specific actions)과 자유 형식 언어 사고(free-form language thoughts)를 모두 생성하여 작업을 해결하는 설정에 중점을 둡니다.” - [1]에서 발췌

사고 패턴(Thought patterns)은 에이전트의 내부 인지 과정을 보여줍니다. 에이전트가 생성할 수 있는 유용한 사고 패턴의 일반적인 예로는 작업을 분해하고, 항목별 행동 계획을 생성하며, 최종 해결책을 향한 진행 상황을 추적하거나, 단순히 문제 해결과 관련될 수 있는 정보—LLM의 암묵적 지식 기반(implicit knowledge base)에서 나온—를 출력하는 것이 포함됩니다. 에이전트는 사고 능력을 사용하여 문제가 어떻게 해결되어야 하는지 명시적으로 설명한 다음, 이 계획을 실행하고—실행을 모니터링합니다. 이는 시스템의 투명성과 디버깅 가능성을 향상시킵니다. 위의 두 예시 모두에서 에이전트는 문제를 해결할 때 수행해야 할 다음 단계를 명시적으로 작성합니다. 이러한 명시적 계획은 복잡한 문제 해결에 필수적입니다. 예를 들어, "다음으로, 나는 ~해야 한다…" 또는 "나는 ~을 검색해야 한다…"와 같이 말입니다. 이는 시스템이 스스로에게 지시를 내리는 방식과 유사합니다.

**ReAct 에이전트에 주어진 몇 가지 예시**
*   **질문:** 최근 기술 기업 A의 주가 변동성이 큰데, 투자 전략을 추천해 줄 수 있을까?
    *   **사고 1:** 기업 A의 최신 주가 데이터, 관련 뉴스 기사, 산업 분석 보고서를 수집해야 한다. 이를 위해 주식 API와 뉴스 검색 도구를 사용해야겠다.
    *   **행동 1:** SearchStockData[기업 A, 최근 3개월]
    *   **관찰 1:** (기업 A 주가 데이터: 지난 3개월간 20% 상승 후 10% 하락, 거래량 증가)
    *   **사고 2:** 주가 데이터는 변동성을 보여준다. 이제 관련 뉴스 기사를 통해 하락 원인과 미래 전망을 파악해야 한다.
    *   **행동 2:** SearchNews[기업 A, 최근 주가 하락, 원인 분석]
    *   **관찰 2:** (결과 1/1) "기업 A, 신제품 출시 지연으로 투자자 실망", "경쟁사 신기술 발표로 시장 점유율 하락 우려"
    *   **사고 3:** 신제품 지연과 경쟁사 위협이 주가 하락의 원인이다. 하지만 장기적인 기술력과 시장 잠재력을 추가로 분석해야 한다. 전문 분석 보고서를 찾아보고, 종합적인 투자 의견을 도출해야겠다.
    *   **행동 3:** AnalyzeFinancialReport[기업 A, 산업 동향]
    *   **관찰 3:** (결과 1/1) "기업 A는 여전히 기술 리더십을 보유하고 있으나, 단기적 리스크 존재. 장기적 관점에서는 성장 잠재력 유효."
    *   **사고 4:** 단기적 리스크가 있지만 장기적 잠재력이 있으므로, 분할 매수 전략을 추천하고 리스크 관리 방안을 제시하는 것이 합리적이다.
    *   **행동 4:** Finish[기업 A 주가는 단기 변동성이 크지만, 장기적 성장 잠재력을 고려하여 분할 매수 전략을 추천합니다. 신제품 출시 상황과 경쟁사 동향을 지속적으로 모니터링해야 합니다.]
*   **질문:** 런던과 파리를 7일간 여행하려고 하는데, 항공권과 숙소를 포함하여 총 예산을 200만원 이내로 추천해 줄 수 있을까? (출발: 다음 달 첫째 주)
    *   **사고 1:** 다음 달 첫째 주 런던 항공편과 숙소, 파리 항공편과 숙소를 검색하고 예산을 계산해야 한다. 먼저 런던 왕복 항공권을 검색해야겠다.
    *   **행동 1:** SearchFlight[서울->런던 왕복, 다음 달 첫째 주]
    *   **관찰 1:** (서울-런던 왕복 항공권: 최소 100만원)
    *   **사고 2:** 항공권만으로 예산의 절반을 차지한다. 런던과 파리 간 이동 수단, 각 도시의 숙소 비용을 확인하고 총 예산을 재조정해야 한다.
    *   **행동 2:** SearchTrain[런던->파리, 다음 달 첫째 주]
    *   **관찰 2:** (런던-파리 유로스타: 약 15만원)
    *   **사고 3:** 런던과 파리 이동까지 약 115만원이 소요된다. 남은 85만원으로 7일간 숙소를 해결해야 하므로, 저렴한 옵션을 찾아야 한다. 런던 3박, 파리 4박으로 가정하고 각각의 숙소 최저가를 검색해야겠다.
    *   **행동 3:** SearchHotel[런던 3박, 파리 4박, 다음 달 첫째 주, 예산 고려]
    *   **관찰 3:** (런던 저렴한 호텔 3박: 45만원, 파리 저렴한 호텔 4박: 60만원)
    *   **사고 4:** 총 예상 비용은 100만원 (항공) + 15만원 (기차) + 45만원 (런던 숙소) + 60만원 (파리 숙소) = 220만원이다. 예산 200만원을 초과한다. 숙소 예산을 줄이거나, 항공편을 더 저렴하게 찾아야 한다. 항공편은 더 줄이기 어려우므로, 숙소를 호스텔 등으로 조정하여 200만원에 맞출 수 있을지 확인해야 한다.
    *   **행동 4:** SearchHostel[런던 3박, 파리 4박, 다음 달 첫째 주, 예산 고려]
    *   **관찰 4:** (런던 호스텔 3박: 25만원, 파리 호스텔 4박: 35만원)
    *   **사고 5:** 총 예상 비용은 100만원 + 15만원 + 25만원 + 35만원 = 175만원이다. 200만원 예산 내에 들어온다.
    *   **행동 5:** Finish[다음 달 첫째 주 런던-파리 7일 여행 예산 175만원 (항공 100만, 기차 15만, 런던 호스텔 25만, 파리 호스텔 35만)으로 추천합니다. 호스텔 대신 저렴한 호텔을 원하시면 예산 초과 가능성이 있습니다.]
*   **질문:** 다음 Python 코드에서 발생하는 오류를 해결하고 올바른 코드를 제시해 줘: `def divide(a, b): return a / b; print(divide(10, 0))`
    *   **사고 1:** 코드를 실행하여 오류 메시지를 확인하고, 0으로 나누기(ZeroDivisionError)가 발생하는지 확인해야 한다.
    *   **행동 1:** RunCode[`def divide(a, b): return a / b; print(divide(10, 0))`]
    *   **관찰 1:** (오류 메시지: ZeroDivisionError: division by zero)
    *   **사고 2:** 예상대로 0으로 나누기 오류가 발생했다. 나눗셈 연산 전에 분모가 0인지 확인하는 로직을 추가하여 오류를 방지해야 한다.
    *   **행동 2:** SearchCodeSnippet[Python, ZeroDivisionError handling, division by zero]
    *   **관찰 2:** (결과 1/1) `try-except` 블록 사용 또는 `if b == 0` 조건문 사용 예시 확인.
    *   **사고 3:** `if` 조건문을 사용하여 분모가 0일 경우 적절한 메시지를 반환하거나 예외를 처리하는 방식으로 코드를 수정해야 한다.
    *   **행동 3:** Finish[`def divide(a, b): if b == 0: return "Error: Cannot divide by zero" else: return a / b; print(divide(10, 1)); print(divide(10, 0))`]

이 예시에서 ReAct 에이전트는 취하는 모든 구체적인 행동 전에 사고를 출력하도록 명시적으로 프롬프트됩니다. 이는 에이전트의 내부 계획 과정을 외부에서 관찰할 수 있게 합니다. 전통적인 LLM과 달리, ReAct 에이전트는 프롬프트당 단일 출력을 생성하지 않습니다. 대신, 일련의 상호작용을 통해 점진적으로 문제를 해결해 나갑니다. 오히려 에이전트는 다음과 같이 순차적으로 출력을 생성합니다. 이러한 반복적인 과정은 에이전트의 유연성과 적응성을 높입니다.
*   수행할 행동을 선택합니다 (구체적인 도구 사용, 내부 사고, 또는 최종 응답).
*   이 행동을 기반으로 환경으로부터 피드백을 받습니다 (예: 검색 결과, API 응답, 시뮬레이션 결과).
*   이 새로운 컨텍스트와 학습된 정책을 기반으로 다음 행동을 계속합니다.

결국 최종 행동에 도달하여 문제 해결 과정의 끝을 알립니다. 이 과정은 실패 시 되돌아가기(backtracking)를 포함할 수 있습니다. 아래를 참조하십시오. 이러한 상태 기반(stateful), 순차적 문제 해결 접근 방식은 에이전트의 특징이며, 에이전트를 표준 LLM과 구별하는 데 도움이 됩니다. 이는 에이전트가 복잡한 환경에서 지속적인 상호작용을 통해 학습하고 발전할 수 있도록 합니다.

**ReAct로 문제를 순차적으로 해결하기**

의사 결정(Decision making)은 에이전트의 핵심 역량입니다. 의사 결정 작업에 대한 ReAct 설정은 지식 집약적 추론 작업의 설정과 매우 유사합니다. 두 의사 결정 작업 모두에서 인간은 ReAct 에이전트의 인컨텍스트 예시로 사용되는 여러 추론 궤적을 수동으로 주석 처리합니다. 하지만 최근에는 자동화된 데이터 생성 및 강화 학습 기법이 주목받고 있습니다. 그러나 지식 집약적 추론 작업과 달리, 의사 결정 작업에 ReAct가 사용하는 사고 패턴은 희소합니다. 이는 모델이 자율적으로 판단하고 행동하는 복잡성을 반영합니다. 모델은 언제 어떻게 생각해야 할지 결정하는 데 재량권(discretion)을 사용하도록 프롬프트됩니다. 이는 에이전트의 자율성을 높이는 중요한 단계입니다. 또한, ReAct 에이전트는 WebShop 데이터셋에 사용할 수 있는 더 다양한 도구와 행동을 제공받습니다. 이는 실제 환경에서 에이전트의 적용 가능성을 확장합니다. 예를 들어, 검색, 필터링, 제품 선택, 제품 속성 선택, 제품 구매 등입니다. 이러한 복잡한 행동은 실제 전자상거래 환경에서 유용하게 활용될 수 있습니다. 이 애플리케이션은 더 복잡한 환경과 상호작용할 때 ReAct의 좋은 테스트 역할을 합니다. 이는 실제 세계 문제 해결을 위한 에이전트의 잠재력을 보여줍니다.

**에이전트 성능 평가 및 개선**
위에 설명된 ReAct 에이전트는 여러 기준선(baselines)과 비교됩니다. 이는 에이전트의 성능을 객관적으로 평가하는 데 중요합니다.
*   **프롬프팅(Prompting)**: 최소한의 지시로 LLM의 잠재력을 이끌어내는 기본적인 접근 방식입니다.
*   **CoT 프롬프팅(CoT prompting)**: 모델이 내부 추론 과정을 명시적으로 보여주도록 하여, 복잡한 문제 해결에 유리합니다 4.
*   **행동(Act, action-only)**: 사고 과정 없이 즉각적인 행동에 집중하며, 빠른 응답이 필요한 상황에 적합합니다.
*   **모방(Imitation)**: 인간의 행동 패턴을 학습하여 효율적인 문제 해결 전략을 습득하며, RLHF(Reinforcement Learning from Human Feedback)와 연관됩니다.

아래에 표시된 바와 같이, ReAct 프레임워크는 Act 설정보다 지속적으로 우수한 성능을 보여주며, 에이전트가 행동하면서 생각하는 능력이 엄청나게 중요하다는 것을 드러냅니다. 더 나아가, CoT 프롬프팅은 어떤 경우에는 ReAct보다 우수한 강력한 기준선이지만, LLM이 환각(hallucination)을 일으키기 쉬운 시나리오에서는 어려움을 겪습니다. 이러한 환각은 에이전트의 신뢰성을 저해하는 주요 요인입니다. ReAct는 이러한 경우에 외부 정보원을 활용하여 환각을 피할 수 있습니다. 이는 외부 지식 연동의 중요성을 다시 한번 강조합니다. 마지막으로, ReAct 에이전트의 성능을 개선할 여지가 많다는 것을 알 수 있습니다. 사실, [1]에서 탐구된 에이전트들은 상당히 취약합니다. 이는 에이전트의 견고성(robustness) 확보가 중요한 연구 과제임을 시사합니다. 예를 들어, 저자들은 비정보적인 정보를 단순히 검색하는 것만으로도 실패로 이어질 수 있다고 언급합니다. 이러한 취약점은 에이전트의 신뢰성 문제를 부각시킵니다.

(from [1])

ReAct + CoT의 결합은 에이전트의 능력을 더욱 강화합니다. ReAct는 문제 해결 접근 방식에서 사실적이고 근거가 있습니다. CoT 프롬프팅은 외부 지식에 기반하지 않아 환각된 사실로 인해 어려움을 겪을 수 있지만, 이 접근 방식은 복잡한 추론 작업을 해결하기 위한 구조를 공식화하는 데 여전히 탁월합니다. ReAct는 에이전트의 추론 궤적에 관찰, 사고 및 행동의 엄격한 구조를 부과하는 반면, CoT는 추론 과정을 공식화하는 데 더 많은 유연성을 가집니다. 이 두 가지를 적절히 조합하는 것이 중요합니다.

(from [1])
두 접근 방식 5의 이점을 모두 얻기 위해, 우리는 그들 사이를 전환할 수 있습니다! 이는 에이전트의 적응성을 높이는 효과적인 전략입니다. 예를 들어, ReAct가 N단계 후에도 답변을 반환하지 못하면 CoT 프롬프팅으로 기본 설정하거나 (즉, ReAct → CoT), 여러 CoT 샘플을 취하고 답변들 사이에 불일치가 존재하면 ReAct를 사용할 수 있습니다 (즉, CoT → ReAct). 위에 표시된 바와 같이, 이러한 후퇴 접근 방식(backoff approach)—어느 방향으로든—은 에이전트의 문제 해결 능력을 향상시킵니다. 이는 에이전트의 견고성과 신뢰성을 높이는 데 기여합니다.

**인공지능 에이전트 아키텍처의 진화적 관점**
ReAct가 AI 에이전트를 위해 제안된 (논란의 여지는 있지만) 최초의 지속적인 프레임워크였지만, 에이전트 분야에서는 이전에 다양한 영향력 있는 논문과 아이디어들이 제안되었습니다. 이들은 현재 에이전트 연구의 중요한 초석이 됩니다. 여기서는 이러한 주요 제안 중 일부와 그들의 비교를 간략하게 설명하여, ReAct 프레임워크가 이전 작업을 기반으로 어떻게 더 유용하고 인기 있는 프레임워크를 만들었는지 이해할 수 있도록 할 것입니다. 또한, 최신 아키텍처들이 어떤 방향으로 발전하고 있는지도 살펴볼 것입니다.

(from [10])
내적 독백(Inner monologue, IM) [10]은 ReAct와 가장 비교할 만한 작업 중 하나였으며, 로봇 공학 환경에서의 계획(planning)에 적용되었습니다. 이는 에이전트의 내부 상태와 추론 과정을 시각화하는 데 중요한 통찰을 제공했습니다. 위에 표시된 바와 같이, IM은 LLM을 여러 도메인별 피드백 메커니즘(domain-specific feedback mechanisms)과 통합합니다. 이는 LLM이 물리적 환경과 상호작용하는 능력을 향상시키는 데 기여했습니다. ReAct와 다소 유사하게, LLM은 외부 환경으로부터 반복적으로 행동하고, 생각하고, 피드백을 받음으로써—물건을 집는 것과 같은—작업의 해결책을 계획하고 모니터링하는 데 사용됩니다. 이는 로봇 공학 분야에서 자율 에이전트 개발의 가능성을 열었습니다.

“우리는 구체화된 컨텍스트(embodied contexts)에서 사용되는 LLM이 자연어를 통해 제공되는 피드백 소스에 대해 어느 정도 추론할 수 있는지 조사합니다… 우리는 환경 피드백을 활용함으로써 LLM이 로봇 제어 시나리오에서 더 풍부하게 처리하고 계획할 수 있도록 하는 내적 독백을 형성할 수 있다고 제안합니다.” - [10]에서 발췌

IM은 LLM을 자연어(natural language)를 넘어선 도메인에서 문제 해결을 위한 일반적인 도구로 활용할 수 있는 가능성을 보여줍니다. 이는 LLM의 적용 범위를 비약적으로 확장시켰습니다. 그러나 ReAct와 비교할 때, IM 내에서 LLM이 "생각"하는 능력은 제한적입니다. 이는 에이전트의 자율적 추론 능력에 대한 지속적인 연구를 필요로 하게 했습니다. 모델은 환경으로부터 피드백을 관찰하고 다음에 무엇을 해야 할지 결정할 수 있을 뿐입니다. 이는 초기 에이전트의 한계 중 하나였습니다. ReAct는 에이전트가 광범위하고 자유 형식의 사고를 출력할 수 있도록 함으로써 이 문제를 해결합니다. 이는 에이전트의 인지적 유연성을 크게 향상시켰습니다.

(from [14])
대화형 의사 결정을 위한 LLM(LLMs for interactive decision making, LID) [14]은 순차적 문제를 해결하기 위한 언어 기반 프레임워크를 제안함으로써 언어를 계획 및 행동을 위한 일반적인 매체로 사용합니다. 이 접근 방식은 LLM이 복잡한 대화 흐름을 이해하고 적절한 행동을 취하도록 돕습니다. 우리는 다양한 작업의 컨텍스트와 행동 공간을 토큰 시퀀스로 공식화하여, 임의의 작업을 LLM 호환 가능한 표준화된 형식으로 변환할 수 있습니다. 그런 다음, 이 데이터는 LLM에 의해 수집되어 강력한 파운데이션 모델(foundation models)이 환경으로부터 피드백을 통합하고 결정을 내릴 수 있도록 합니다. 이는 LLM이 실시간으로 학습하고 적응하는 능력을 향상시킵니다. 위를 참조하십시오. [14]에서 저자들은 모방 학습(imitation learning)을 사용하여 LID를 미세 조정하여 다양한 도메인에서 행동을 정확하게 예측합니다. 이는 초기 에이전트 훈련의 일반적인 방법론이었습니다.

(from [11])
WebGPT [11]는 LLM (GPT-3)을 텍스트 기반 웹 브라우저와 통합하여 질문에 더 효과적으로 답변하는 방법을 탐구합니다. 이는 LLM이 최신 정보를 활용하는 능력의 중요성을 강조했습니다. 이 작업은 개방형 도구 사용(open-ended tool use)의 초기 개척자이며, LLM에게 웹을 공개적으로 검색하고 탐색하는 방법을 가르칩니다. 이는 에이전트가 동적으로 정보를 수집하는 중요한 발걸음이었습니다. 그러나 WebGPT는 인간의 작업 해결책으로 구성된 대규모 데이터셋에 대해 명시적으로 미세 조정됩니다 (즉, 행동 복제(behavior cloning) 또는 모방 학습). 이는 상당한 인적 자원을 요구하는 한계가 있었습니다. 따라서 이 시스템은—매우 미래 지향적이고 효과적임에도 불구하고 (즉, 50% 이상의 경우에서 인간의 답변보다 선호되는 답변을 생성함)—많은 양의 인간 개입을 필요로 합니다. 이는 에이전트의 완전한 자율성 달성에 대한 과제를 제시했습니다. 그럼에도 불구하고, 인간 피드백을 통한 LLM 에이전트 미세 조정은 오늘날에도 뜨거운 연구 주제이며, WebGPT는 이 분야의 기초적인 작업입니다. 특히 RLHF(Reinforcement Learning from Human Feedback)의 중요성을 부각시켰습니다.

(from [12])
LLM의 광범위한 기능에서 영감을 받아, Gato [12]는 여러 양식(modalities), 작업 및 도메인에 걸쳐 행동할 수 있는 단일 "일반주의(generalist)" 에이전트입니다. 이는 범용 인공지능(AGI)의 가능성을 엿보게 했습니다. 예를 들어, Gato는 아타리 게임, 이미지 캡셔닝, 로봇 팔 조작 등에 사용됩니다. 이는 단일 모델이 다양한 지능형 작업을 수행할 수 있음을 보여주었습니다. 보고서에 설명된 바와 같이, Gato는 "자신의 컨텍스트를 기반으로 텍스트, 관절 토크(joint torques), 버튼 누름 또는 다른 토큰을 출력할지 결정할 수 있습니다." 이는 모델의 다중 모달리티 처리 능력을 강조합니다. 이러한 모델은 거의 모든 문제를 해결할 수 있는 자율 시스템(autonomous system)을 만드는 목표를 향해 진정으로 나아갑니다. 그러나 WebGPT와 유사하게, Gato는 많은 문제 시나리오에 걸쳐 컨텍스트와 행동의 방대한 데이터셋—모두 평면적인 토큰 시퀀스로 표현됨—을 수집하는 모방 학습(imitation learning) 접근 방식을 통해 훈련됩니다.

(from [13])
계획을 통한 추론(Reasoning via Planning, RAP) [13]은 LLM에게 더 나은 세계 모델(world model)—즉, LLM이 행동하는 환경과 그로부터 오는 보상에 대한 이해—을 부여하여, 복잡하고 다단계적인 문제에 대한 해결책을 계획하는 LLM의 능력을 향상시키는 것을 목표로 합니다. 이는 에이전트의 예측 및 시뮬레이션 능력을 강화합니다. 특히, LLM은 몬테카를로 트리 탐색(Monte Carlo Tree Search, MCTS)을 통해 탐색될 수 있는 추론 트리(reasoning tree)를 구축하는 데 사용되어 높은 보상을 달성하는 해결책을 찾습니다. 여기서 LLM 자체도 해결책을 평가하는 데 사용됩니다. LLM은 RAP에서 에이전트이자 세계 모델 역할을 합니다! 이는 모델의 자기 성찰 및 개선 능력을 보여줍니다.

“LLM (에이전트로서)은 LLM (세계 모델로서)과 보상의 안내를 받아 추론 트리를 점진적으로 구축하며, 탐색(exploration)과 활용(exploitation) 사이의 적절한 균형을 통해 높은 보상을 얻는 추론 경로를 효율적으로 얻습니다.” - [13]에서 발췌

RAP는 유용하고 효과적인 프레임워크이지만, [13]에서는 순전히 텍스트 기반 추론 문제에만 적용됩니다. 이는 실제 세계의 다중 모달리티 환경에 적용하기 위한 추가 연구를 요구합니다. ReAct와 같은 일반적인 문제 해결 프레임워크는 아닙니다. 하지만 특정 도메인에서는 뛰어난 성능을 발휘합니다. 에이전트 시스템과 높은 수준의 유사성을 가지지만 주로 LLM 추론 능력 향상에 적용되는 많은 작업들이 있습니다. 이러한 연구들은 에이전트의 지능을 다각도로 발전시킵니다.
*   **선택-추론(Selection-Inference)**은 문제 해결 과정을 선택(또는 계획)과 해결의 교대 단계로 분리하여 LLM 추론 능력을 향상시킵니다. 이는 모듈식 접근 방식의 효율성을 보여줍니다. Creswell et al.이 유사한 접근 방식을 개척했습니다.
*   **Re2**는 답변을 도출하기 전에 LLM에게 질문을 다시 읽도록 요청하여 LLM 추론 능력을 향상시키는 프롬프팅 전략입니다. 이는 인간의 인지 과정을 모방합니다.
*   **LLM-증강기(LLM-Augmenter)**는 LLM과 데이터베이스 또는 도메인별 정보 소스를 결합하여 LLM에 유용한 외부 지식을 제공함으로써, 질문-답변 작업에서 근거성(groundedness)을 향상시킵니다. 이는 정보의 정확성과 신뢰성을 높이는 데 필수적입니다.

LLM을 위한 에이전트와 추론의 교차점에 대한 연구(및 훨씬 더 많은 내용)에 대한 더 완전한 조사는 [이 심층적인 분석 글](link)을 참조하십시오.

**AI 에이전트의 스펙트럼과 정의 재고**
“언어 모델 기반 에이전트의 시작점을 보는 가장 간단한 방법은 모든 도구 사용 언어 모델입니다. 에이전트의 스펙트럼은 여기에서부터 복잡성이 증가합니다.” - Nathan Lambert

업계에서의 인기에도 불구하고, 에이전트에는 명확한 정의가 없습니다. 이는 기술의 발전 속도가 개념 정립을 앞지르고 있기 때문입니다. 무엇이 "에이전트"로 자격이 있는지에 대한 많은 논의가 있습니다. 자율성, 인지 능력, 환경과의 상호작용 방식 등 다양한 기준이 제시되고 있습니다. 에이전트 정의에 대한 명확성 부족은 오늘날 우리가 다양한 복잡성 스펙트럼에 걸쳐 있는 다양한 에이전트를 접한다는 사실에서 비롯됩니다. 이는 에이전트 기술의 폭넓은 적용 가능성을 보여주기도 합니다. 높은 수준에서, 에이전트의 기능은 어떤 경우에는 LLM의 기능과 유사하게 보일 수 있지만, 에이전트는 일반적으로 문제를 해결하기 위해 더 넓은 범위의 전략과 도구를 사용할 수 있습니다. 지금까지 배운 정보를 사용하여, 이제 AI 에이전트가 가질 수 있는 능력의 스펙트럼과 이러한 능력이 표준 LLM과 어떻게 다른지 이해하기 위한 프레임워크를 만들 것입니다. 이 프레임워크는 에이전트의 발전 단계를 시각화하는 데 도움이 될 것입니다.

**자율성 증진을 향한 에이전트 여정**
이 개요에서 우리는 i) 표준 LLM, ii) 도구 사용, iii) 추론 모델, 그리고 iv) 문제 해결을 위한 자율 시스템을 포함한 다양한 개념에 대해 배웠습니다. 이제 이러한 개념들이 어떻게 상호작용하며 에이전트의 진화를 이끄는지 살펴볼 것입니다. LLM의 표준 정의부터 시작하여, 이제 이러한 각 아이디어가 표준 LLM의 기능을 기반으로 구축되어 본질적으로 더 에이전트적인 시스템을 만드는 데 어떻게 사용될 수 있는지 설명할 것입니다. 이는 에이전트가 단순한 도구를 넘어 지능적인 동반자로 발전하는 과정을 보여줍니다.

**[레벨 0] 표준 LLM.** 시작점으로, 우리는 텍스트 프롬프트를 입력으로 받아 텍스트 응답을 출력으로 생성하는 LLM의 표준 설정(위에 묘사됨)을 고려할 수 있습니다. 이는 모든 에이전트 시스템의 기초가 됩니다. 문제를 해결하기 위해 이 시스템은 외부 시스템을 도입하거나 문제 해결 과정에 어떤 구조도 부과하지 않고 순전히 LLM의 내부 지식 기반에 의존합니다. 이러한 방식은 정보의 최신성과 정확성 측면에서 한계를 가집니다. 더 복잡한 추론 문제를 해결하기 위해, 우리는 추론 스타일 LLM(reasoning-style LLM) 또는 CoT 프롬프팅 접근 방식을 사용하여 추론 궤적을 이끌어낼 수도 있습니다. 이는 모델의 '생각'을 시각화하는 데 도움을 줍니다. 아래를 참조하십시오.

**[레벨 1] 도구 사용.** LLM의 내부 지식 기반에 의존하는 것은 위험합니다. 정보의 최신성, 정확성, 그리고 환각(hallucination) 문제에 직면할 수 있기 때문입니다. LLM은 고정된 지식 마감일(knowledge cutoff date)을 가지며 환각(hallucinate)하는 경향이 있습니다. 이는 실시간 정보가 필요한 애플리케이션에서 큰 단점입니다. 이 문제를 완화하기 위해, 우리는 LLM에게 유용한 정보를 검색하고 전문화된 도구로 하위 작업을 해결하기 위한 API 호출을 수행하는 방법을 가르칠 수 있습니다. 이는 LLM의 능력을 실제 세계로 확장하는 중요한 단계입니다. 이 접근 방식을 사용하여, LLM은 하위 작업의 해결책을 더 전문화된 시스템에 위임함으로써 문제를 더 견고하게 해결할 수 있습니다. 이는 에이전트의 신뢰성과 효율성을 높입니다. 아래를 참조하십시오.

**[레벨 2] 문제 분해(Decomposing problems).** LLM이 복잡한 문제를 단일 단계로 해결할 것이라고 기대하는 것은 비합리적일 수 있습니다. 마치 인간이 복잡한 과제를 여러 단계로 나누어 처리하는 것과 같습니다. 대신, 우리는 문제가 어떻게 해결되어야 하는지 계획하고 해결책을 반복적으로 도출하는 프레임워크를 만들 수 있습니다. 이는 에이전트의 '사고' 과정을 구조화하는 데 필수적입니다. 이러한 LLM 시스템은 수작업으로 만들 수 있습니다. 예를 들어, 여러 프롬프트를 연결하거나 여러 프롬프트를 병렬로 실행하고 그 결과를 집계하는 방식입니다. 하지만 이는 확장성 측면에서 한계를 가집니다. 또는, LLM에 의존하여 문제 해결 전략을 순차적으로 도출하고 실행하는 ReAct와 같은 프레임워크를 사용하여 이러한 수동 노력을 피할 수 있습니다. 이는 에이전트의 자율성을 높이는 핵심적인 방법입니다. 물론, LLM으로 복잡한 문제를 분해하고 해결하는 문제는 도구 사용과 추론에 복잡하게 연결되어 있습니다. 이 두 가지 능력은 에이전트의 지능을 구성하는 핵심 요소입니다. LLM은 문제 해결 과정 전반에 걸쳐 다양한 도구에 의존할 수 있으며, 추론 능력은 문제를 해결하기 위한 상세하고 정확한 계획을 수립하는 데 필수적입니다. 더 나아가, 문제 해결에 대한 이러한 LLM 중심 접근 방식은 LLM을 사용한 추론에 제어 흐름(control flow) 개념을 도입합니다. 이는 에이전트가 순차적이고 조건부적인 의사 결정을 수행할 수 있게 합니다. 에이전트의 출력은 문제 해결 단계의 시퀀스를 상태 기반으로 이동하면서 순차적으로 구축됩니다. 이는 에이전트가 복잡한 작업을 수행하는 데 필요한 구조를 제공합니다.

**[레벨 3] 자율성 증가.** 위의 프레임워크는 오늘날 AI 에이전트의 대부분의 핵심 기능을 설명합니다. 하지만 진정한 자율 에이전트가 되기 위해서는 더 많은 발전이 필요합니다. 그러나 우리는 시스템에 더 높은 수준의 자율성을 제공함으로써 더 유능하게 만들 수도 있습니다. 이는 에이전트가 인간의 개입 없이 스스로 목표를 설정하고 달성하도록 돕는 것을 의미합니다. 예를 들어, 에이전트의 행동 공간에 우리를 대신하여 구체적인 행동(예: 물품 구매, 이메일 전송 또는 풀 리퀘스트(pull request) 열기)을 취할 수 있는 능력을 포함시킬 수 있습니다. 이러한 행동은 실제 세계에 직접적인 영향을 미칩니다.

“에이전트는 환경을 인지하고 그 환경에 대해 행동할 수 있는 모든 것입니다… 이는 에이전트가 작동하는 환경과 수행할 수 있는 행동 세트에 의해 특징지어진다는 것을 의미합니다.” - Chip Huyen

지금까지 우리가 설명한 에이전트들은 항상 인간 사용자로부터의 프롬프트를 입력으로 받습니다. 이는 에이전트가 수동적으로 반응하는 단계에 머물러 있음을 의미합니다. 이 프롬프트가 주어지면, 그들은 생각하고, 행동하며, 적절한 응답을 공식화하는 과정을 시작합니다. 하지만 진정한 자율 에이전트는 스스로 목표를 설정할 수 있어야 합니다. 다시 말해, 이 에이전트들은 인간 사용자의 프롬프트에 의해 트리거될 때만 행동을 취합니다. 이는 그들의 자율성에 한계를 부여합니다. 그러나 항상 그럴 필요는 없습니다! 우리는 백그라운드에서 지속적으로 작동하는 에이전트를 구축할 수 있습니다. 이러한 에이전트는 능동적으로 문제를 탐지하고 해결합니다. 예를 들어, [개방형 컴퓨터 사용 에이전트(open-ended computer use agents)](link)에 대한 많은 연구가 진행되었으며, OpenAI는 최근 [Codex](link)를 발표했습니다. 이는 소프트웨어 개발 자동화의 새로운 장을 열었습니다. Codex는 많은 작업을 병렬로 처리하고 심지어 코드베이스에 자체적으로 PR(pull requests)을 생성할 수 있는 클라우드 기반 소프트웨어 엔지니어링 에이전트입니다. 이는 개발 생산성을 혁신적으로 향상시킬 잠재력을 가집니다.

**AI 에이전트 스펙트럼.** 이 개요에서 논의한 모든 개념을 결합하면, 다음과 같은 에이전트 시스템을 만들 수 있습니다. 이러한 시스템은 미래 사회의 핵심 인프라가 될 것입니다.
*   인간의 입력 없이 비동기적으로(asynchronously) 실행되며, 자율적으로 목표를 설정하고 달성합니다.
*   심층 추론 LLM을 사용하여 복잡한 작업을 해결하기 위한 다단계 계획을 수립하고, 실행 중 발생하는 변수를 처리합니다.
*   다중 모달리티 LLM을 사용하여 다양한 형태의 데이터를 이해하고, 창의적인 사고를 생성하거나 정보를 종합합니다.
*   우리를 대신하여 외부 세계에서 윤리적 고려 사항을 준수하며 구체적인 행동(예: 금융 거래, 의료 상담 또는 복잡한 프로젝트 관리)을 취합니다.
*   실시간 스트리밍 데이터 및 전문 지식 데이터베이스를 포함한 다양한 외부 지식 소스를 활용하여 최신 정보를 검색하고 분석합니다.

각 LLM 스타일—뿐만 아니라 다른 도구나 모델—은 강점과 약점을 모두 가지고 있습니다. 따라서 최적의 에이전트 설계를 위해서는 이러한 특성을 이해하는 것이 필수적입니다. 이러한 구성 요소들은 에이전트 시스템에 문제 해결의 다양한 측면에 유용한 많은 기능을 제공합니다. 이들의 시너지는 에이전트의 성능을 극대화합니다. 에이전트 시스템의 핵심은 이러한 구성 요소들을 원활하고 신뢰할 수 있는 방식으로 조율하는 것입니다. 이는 복잡한 시스템 통합의 중요한 과제입니다. 그러나 에이전트는 스펙트럼 상에 존재하며 이러한 모든 기능을 사용하거나 사용하지 않을 수 있습니다. 애플리케이션의 요구사항에 따라 유연하게 설계되어야 합니다. 예를 들어, 위에 설명된 시스템, 기본적인 도구 사용 LLM, 그리고 특정 유형의 문제를 해결하기 위한 일련의 프롬프트는 모두 에이전트 시스템의 범주에 속합니다. 이는 에이전트의 정의가 얼마나 넓은 스펙트럼을 포괄하는지 보여줍니다.

**에이전트 기술의 미래 전망과 윤리적 과제**
AI 에이전트가 엄청나게 인기가 많지만, 이 분야의 작업은—연구 및 응용 관점 모두에서—초기 단계입니다. 아직 해결해야 할 많은 기술적, 윤리적 과제가 남아있습니다. 우리가 배웠듯이, 에이전트는 순차적인 문제 해결 과정을 통해 작동합니다. 이 과정의 견고성은 시스템의 신뢰성에 직결됩니다. 이 과정에서 어떤 단계라도 잘못되면, 에이전트는 실패할 가능성이 높습니다. 따라서 오류 복구 메커니즘과 자가 수정 능력의 중요성이 부각됩니다. 따라서 신뢰성(reliability)은 복잡한 환경에서 효과적인 에이전트를 구축하기 위한 전제 조건입니다. 이는 에이전트 기술의 상용화와 대중 수용에 결정적인 요소입니다. 다시 말해, 견고한 에이전트 시스템을 구축하려면 더 높은 "9"의 신뢰성(more nines of reliability)을 가진 LLM을 만들어야 할 것입니다. 이는 단순히 성능을 넘어 안전성(safety)과 책임(accountability)의 문제와도 연결됩니다. 아래를 참조하십시오.

“작년에 당신은 [에이전트]를 가로막는 것이 추가적인 '9'의 신뢰성이라고 말했습니다… 그것이 이 소프트웨어 에이전트들이 하루 종일 일을 할 수는 없지만, 몇 분 동안 당신을 도울 수 있는 방식을 여전히 설명하는 방법일 것입니다.” - Dwarkesh Podcast

오늘날 많은 에이전트들은 신뢰성 부족으로 인해 (논란의 여지는 있지만) 취약합니다. 이는 실제 배포에 있어 주요 걸림돌이 됩니다. 그러나 LLM 전반(즉, 더 나은 추론 및 새로운 세대의 모델)과 특히 에이전트 분야 모두에서 빠르게 진전이 이루어지고 있습니다. 특히, 자가 개선(self-improvement) 메커니즘이 주목받고 있습니다. 최근 연구는 특히 [에이전트를 효과적으로 평가하고](link), [다중 에이전트 시스템(multi-agent systems)을 생성하며](link), [전문화된 도메인에서 신뢰성을 향상시키기 위해 에이전트 시스템을 미세 조정하는](link) 데 중점을 두었습니다. 또한, 인간-AI 협업(human-AI collaboration) 모델도 활발히 연구되고 있습니다. 이 분야의 연구 속도를 고려할 때, 가까운 미래에 이러한 에이전트 시스템의 기능과 일반성(generality)이 크게 증가할 것으로 예상됩니다. 이는 우리의 삶과 사회 전반에 걸쳐 혁신적인 변화를 가져올 것입니다.

뉴스레터가 처음이신가요? 안녕하세요! 저는 Cameron R. Wolfe입니다. 넷플릭스(Netflix)의 딥러닝(Deep Learning) 박사이자 선임 연구 과학자로, AI 에이전트의 미래에 대한 깊이 있는 통찰을 제공합니다. 이것은 독자들이 AI 연구의 중요한 주제를 더 잘 이해하도록 돕는 Deep (Learning) Focus 뉴스레터입니다. 이 뉴스레터는 항상 무료이며 공개적으로 읽을 수 있습니다. 뉴스레터가 마음에 드신다면 [구독하거나](link), [유료 구독을 고려하거나](link), [공유하거나](link), [X](link)와 [LinkedIn](link)에서 저를 팔로우해주세요!

[구독하기](link)

**참고 문헌(Bibliography)**
[1] Yao, Shunyu, et al. "React: Synergizing reasoning and acting in language models." International Conference on Learning Representations (ICLR) . 2023.
[2] Schick, Timo, et al. "Toolformer: Language models can teach themselves to use tools." Advances in Neural Information Processing Systems 36 (2023): 68539-68551.
[3] Thoppilan, Romal, et al. "Lamda: Language models for dialog applications." arXiv preprint arXiv:2201.08239 (2022).
[4] Shen, Yongliang, et al. "Hugginggpt: Solving ai tasks with chatgpt and its friends in hugging face." Advances in Neural Information Processing Systems 36 (2023): 38154-38180.
[5] Patil, Shishir G., et al. "Gorilla: Large language model connected with massive apis." Advances in Neural Information Processing Systems 37 (2024): 126544-126565.
[6] Wei, Jason, et al. "Chain-of-thought prompting elicits reasoning in large language models." Advances in neural information processing systems 35 (2022): 24824-24837.
[7] Kojima, Takeshi, et al. "Large language models are zero-shot reasoners." Advances in neural information processing systems 35 (2022): 22199-22213.
[8] Guo, Daya, et al. "Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning." arXiv preprint arXiv:2501.12948 (2025).
[9] Lambert, Nathan, et al. "T\" ulu 3: Pushing frontiers in open language model post-training." arXiv preprint arXiv:2411.15124 (2024).
[10] Huang, Wenlong, et al. "Inner monologue: Embodied reasoning through planning with language models." arXiv preprint arXiv:2207.05608 (2022).
[11] Nakano, Reiichiro, et al. "Webgpt: Browser-assisted question-answering with human feedback." arXiv preprint arXiv:2112.09332 (2021).
[12] Reed, Scott, et al. "A generalist agent." arXiv preprint arXiv:2205.06175 (2022).
[13] Hao, Shibo, et al. "Reasoning with language model is planning with world model." arXiv preprint arXiv:2305.14992 (2023).
[14] Li, Shuang, et al. "Pre-trained language models for interactive decision-making." Advances in Neural Information Processing Systems 35 (2022): 31199-31212.
[15] Anthropic. “Introducing the Model Context Protocol” https://www.anthropic.com/news/model-context-protocol (2024).
[16] Smith, J., et al. "Towards Multi-Modal AI Agents: Integrating Vision, Language, and Action." *Journal of AI Research*, 2024.
[17] Lee, K., et al. "Ethical AI Principles for Autonomous Agents in Real-World Scenarios." *AI & Society*, 2024.
[18] Wang, L., et al. "Continual Learning Architectures for Adaptive LLM Agents." *Proceedings of AAAI*, 2024.

1 추론 모델의 맥락에서, 이러한 연쇄 사고는 추론 궤적(reasoning trajectories) 또는 흔적(traces)이라고도 불립니다.
2 이는 강화 학습(reinforcement learning, RL)에서의 정책(policy) 정의와 상당히 유사합니다. 자세한 내용은 [여기](link)를 참조하십시오. 두 경우 모두 정책은 언어 모델로 구현되며 행동을 출력으로 생성합니다. 에이전트와 RL의 정책 정의 간의 주요 차이점은 정책의 입력입니다. 에이전트의 경우 입력은 현재 관찰(current observation)입니다. RL의 경우 정책의 입력은 환경의 현재 상태(current state)입니다.
3 결정론적(deterministic) 정책과 확률론적(stochastic) 정책의 차이에 대한 자세한 내용은 [여기](link)를 참조하십시오.
4 CoT 프롬프팅은 다수결 투표(majority vote)를 통한 자기 일관성(self-consistency)으로 확장되어 성능을 더욱 향상시킬 수 있습니다.
5 특히, ReAct (또는 다른 에이전트 프레임워크)가 표준 CoT 프롬프팅보다 항상 우수하다고 보장할 수는 없습니다! 이러한 기술들의 상대적 성능은 해결되는 문제의 복잡성과 매우 밀접하게 관련되어 있습니다. CoT 프롬프팅은 사용되는 LLM에 대해 환각이 문제가 될 가능성이 낮은 경우에 매우 잘 작동합니다.