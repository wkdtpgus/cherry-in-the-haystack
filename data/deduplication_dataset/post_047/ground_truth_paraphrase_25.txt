AI 에이전트 심층 분석: LLM 기반 시스템의 발전과 핵심 구성 요소

거대 언어 모델(LLM)의 능력은 눈부신 속도로 진화하고 있습니다. 이러한 LLM의 발전은 단순히 질의응답을 넘어, 난해한 과제를 해결하고, 외부 시스템과 연동하며, 장기적인 목표를 수행하는 정교한 시스템 구축을 가능하게 합니다. 이러한 시스템을 통칭하여 AI 에이전트라고 부릅니다. AI 에이전트는 최근 각광받는 분야이지만, 그 정의와 역할에 대한 명확한 이해는 아직 부족합니다. 최근 몇 년간 인공지능 분야는 전례 없는 발전을 거듭해왔습니다. 특히 대규모 언어 모델(LLM)은 단순히 텍스트를 생성하고 이해하는 것을 넘어, 복잡한 문제 해결을 위한 지능형 도구로 진화하고 있습니다. 이러한 변화의 중심에는 'AI 에이전트'라는 개념이 있습니다. AI 에이전트는 특정 목표를 달성하기 위해 자율적으로 판단하고 행동하며, 외부 환경과 상호작용하는 시스템을 의미합니다. 그러나 이 용어가 광범위하게 사용되면서, 진정한 AI 에이전트가 무엇인지, 그리고 어떤 기능을 갖추어야 하는지에 대한 혼란도 커지고 있습니다. 본 글에서는 AI 에이전트의 근본적인 개념부터 심층적으로 다루고자 합니다. 우리는 LLM이 어떻게 단순한 언어 모델을 넘어, 도구를 사용하고, 추론하며, 궁극적으로는 인간의 개입 없이도 복잡한 작업을 수행하는 자율 에이전트로 거듭나는지 살펴볼 것입니다.

**LLM의 기초 역량과 에이전트의 토대**

**표준 LLM의 입출력 서명 (input-output signature)**
LLM의 기본적인 작동 방식은 잘 알려져 있습니다. 즉, 텍스트 형태의 질문이 입력되면, LLM은 역시 텍스트 형태의 답변을 생성합니다. 이러한 단순하면서도 범용적인 기능은 다양한 유형의 문제 해결에 적용될 수 있습니다. 실제로, LLM이 가진 광범위한 활용 가능성(generality)은 핵심적인 장점 중 하나로 평가받습니다. 본 장에서는 논리적 추론(reasoning)이나 외부 API 연동(interacting with external APIs)과 같은 진보된 기능들이 이러한 기본 텍스트 입출력 구조를 바탕으로 LLM에 어떻게 접목될 수 있는지 설명할 것입니다. 결론적으로, 오늘날 AI 에이전트의 복잡한 능력들은 대부분 이러한 기초적인 언어 모델의 기능 위에 세워집니다.

표준 LLM은 방대한 텍스트 데이터로부터 학습하여 인간의 언어를 이해하고 생성하는 놀라운 능력을 보여주지만, 몇 가지 내재적인 한계를 가지고 있습니다. 첫째, 학습 데이터에 기반한 고정된 지식 마감일(knowledge cutoff date)로 인해 최신 정보를 알지 못합니다. 둘째, 때때로 사실과 다른 정보를 그럴듯하게 지어내는 환각(hallucination) 현상을 보입니다. 셋째, 복잡한 다단계 추론이나 외부 시스템과의 실시간 상호작용은 어렵습니다. 이러한 한계는 LLM을 단순한 '대화 도구'를 넘어 '문제 해결 에이전트'로 발전시키기 위한 동기가 됩니다. 다음 섹션에서는 이러한 한계를 극복하고 LLM의 잠재력을 극대화하는 핵심 요소인 '도구 사용'에 대해 자세히 알아보겠습니다.

**도구 사용 (Tool Usage)**
LLM의 능력이 발전함에 따라, 외부 시스템과 연동하여 활용하는 방법을 가르치는 것이 인공지능 연구의 주요 흐름으로 부상했습니다. LLM과 결합될 수 있는 실용적인 도구로는 계산 도구(calculators), 일정 관리 앱(calendars), 웹 검색 엔진(search engines), 프로그래밍 해석기(code interpreters) 등을 들 수 있습니다. 이러한 방식이 인기를 끄는 이유는 무엇일까요? 본질적으로 LLM은 모든 종류의 문제를 가장 효율적으로 처리하는 만능 도구가 아니기 때문입니다. 오히려 특정 작업에는 더 단순하고 검증된 도구들이 존재합니다. 가령, 기본적인 수리 연산은 계산기가, 최신 정보 탐색은 검색 엔진이 훨씬 정확하고 빠릅니다. 그러나 LLM은 복잡한 계획 수립(planning)과 전체 과정 조율(orchestration)에 강점을 보이므로, 문제 해결 단계에서 이러한 도구들을 적절히 활용하도록 교육하는 것이 효과적입니다! 도구 사용 LLM의 핵심 개념은 LLM이 문제의 특정 하위 작업(sub-tasks)이나 구성 요소(components)를 해당 분야에 특화된 안정적인 도구에 맡길 수 있도록 하는 것입니다. 이러한 맥락에서 LLM은 여러 전문 도구들을 유기적으로 통합하고 지휘하는 '중앙 처리 장치' 역할을 수행합니다.

도구 사용은 LLM의 한계를 보완하고 그 능력을 확장하는 데 필수적인 전략입니다. 계산기나 검색 엔진 외에도, LLM은 데이터베이스(DB) 조회, CRM 시스템 연동, 이미지 생성 API 호출, 특정 소프트웨어 명령어 실행 등 무궁무진한 외부 도구들과 상호작용할 수 있습니다. 이는 LLM이 실시간 데이터를 활용하고, 특정 도메인에 특화된 기능을 수행하며, 최종적으로 더 정확하고 신뢰할 수 있는 결과를 도출할 수 있게 합니다. 예를 들어, 재고 관리 시스템과 연동된 LLM 에이전트는 고객 문의에 대해 실시간 재고 정보를 바탕으로 답변하거나, 주문 처리 시스템을 통해 직접 주문을 생성할 수도 있습니다.
하지만 도구 사용에는 몇 가지 도전 과제도 존재합니다. 첫째, LLM이 방대한 외부 API 스키마를 정확히 이해하고 올바른 매개변수로 호출하는 능력이 중요합니다. 둘째, 도구 호출 실패 시 적절하게 오류를 처리하고 복구하는 메커니즘이 필요합니다. 셋째, 사용 가능한 도구의 수가 많아질수록 어떤 도구를 어떤 상황에 사용할지 결정하는 복잡성이 증가합니다. 이러한 문제들을 해결하기 위해 연구자들은 LLM이 도구 사용을 더 효과적으로 학습하고 관리할 수 있는 다양한 기술을 개발하고 있습니다.

**도구 사용 예시 ([2, 3]에서 발췌)**
**도구 사용을 위한 정밀 조정(Finetuning).** 초기 도구 활용 연구들, 예를 들어 LaMDA [2]나 Toolformer [3] 등은 LLM이 정해진 도구 묶음을 사용하도록 특정 목적에 맞춘 미세 조정 기법을 활용했습니다. 이 과정에서 우리는 LLM의 토큰 흐름(token stream) 내에 특정 도구의 함수 호출(function call)이 직접 포함된 훈련 데이터를 선별하여 사용합니다. 아래를 참조하십시오.

**도구 호출의 구조**
학습 단계에서는 이러한 도구 호출이 일반 텍스트 토큰과 동일하게 다루어지며, 이는 단순히 텍스트 시퀀스의 한 부분으로 간주됩니다! 추론 시점(inference time)에 LLM이 도구 호출을 출력하면, 시스템은 이를 다음과 같이 처리합니다.
*   현재 토큰 생성을 일시 정지합니다.
*   도구 호출을 분석하여(즉, 어떤 도구를 어떤 매개변수로 호출할지 파악합니다).
*   결정된 매개변수로 해당 도구를 실행합니다.
*   도구로부터 받은 응답을 LLM의 토큰 흐름에 삽입합니다.
*   이후 토큰 생성을 재개합니다.
LLM이 응답을 생성하는 과정에서 도구 호출은 실시간으로 처리되며, 도구에서 얻은 정보는 모델의 문맥(context)에 즉시 반영됩니다!

이러한 도구 호출 메커니즘은 LLM이 외부 세계와 상호작용하는 기본적인 방법을 제공합니다. 초기에는 특정 도구 세트에 대한 미세 조정을 통해 이 능력을 주입했지만, LLM의 규모와 추론 능력이 비약적으로 발전하면서 '프롬프트 기반 도구 사용'이라는 더욱 유연한 접근 방식이 각광받기 시작했습니다. 이는 LLM 자체의 강력한 언어 이해 능력을 활용하여, 명시적인 미세 조정 없이도 프롬프트 내에서 도구 사용 규칙과 API 스키마를 설명함으로써 LLM이 스스로 도구를 사용하도록 유도하는 방식입니다.
이러한 방식은 인간의 수동적인 데이터 주석(annotation) 부담을 크게 줄일 뿐만 아니라, LLM이 활용할 수 있는 도구의 종류를 비약적으로 늘리는 계기가 되었습니다. 이제 LLM은 수백, 수천 개의 다양한 API와 연동될 수 있으며, 이는 정보 검색, 외부 ML 모델 호출, 여행 예약, 캘린더 관리 등 상상할 수 없을 만큼 다양한 응용 분야를 가능하게 합니다. Anthropic의 Model Context Protocol (MCP) [15]과 같은 프레임워크는 이러한 도구 통합 과정을 더욱 표준화하여, 개발자들이 손쉽게 LLM 에이전트를 위한 맞춤형 데이터 소스와 연동 기능을 구축할 수 있도록 지원합니다. MCP는 외부 시스템이 LLM의 프롬프트에 컨텍스트를 제공하는 방식을 표준화하여, 복잡한 문제 해결을 위한 LLM의 외부 도구 통합을 간소화하고 확장성을 높이는 데 기여합니다.

**프롬프트 기반 도구 사용 (Prompt-based tool usage).** (from [4, 5])
프롬프트 기반 도구 활용은 사람의 개입을 최소화하면서, LLM이 접근 가능한 도구의 범위를 상당히 확장합니다. 실례로, 관련 연구들은 LLM을 수백 [4] 또는 심지어 수천 [5] 개의 도구들과 연동시키는 성과를 보여주었습니다. 위를 참조하십시오. 이 방식은 모든 도구를 일반적인 API(generic API)로 간주하고, 해당 API의 구조(schema)를 모델의 프롬프트에 문맥 정보로 제공합니다. 이러한 접근법을 통해 LLM은 표준화된 형태로 인터넷상의 다양한 API와 유연하게 통합될 수 있으며, 이는 정보 탐색, 다른 기계 학습 모델 호출, 여행 예약, 일정 관리 등 헤아릴 수 없이 많은 응용 분야를 창출합니다.

프롬프트 기반 도구 사용의 가장 큰 장점은 LLM의 유연성과 확장성을 극대화한다는 점입니다. 모델을 재학습시키지 않고도 새로운 도구를 추가하거나 기존 도구의 사용 방식을 변경할 수 있습니다. 이는 도구의 기능과 사용법을 자연어 형태로 프롬프트에 포함시키는 '도구 설명(tool description)' 방식 덕분입니다. LLM은 이 설명을 바탕으로 어떤 도구가 현재 상황에 가장 적합한지 판단하고, 필요한 매개변수를 추론하여 호출합니다.
이러한 방식은 '도구 라이브러리(tool library)' 개념의 등장을 촉진했습니다. 개발자들은 특정 도메인이나 작업에 필요한 다양한 도구들을 API 스키마와 함께 정의하고, 이를 LLM 에이전트가 활용할 수 있는 형태로 제공합니다. 예를 들어, 금융 분석 에이전트라면 주가 정보 조회, 환율 변환, 재무제표 분석 등의 도구들을 라이브러리 형태로 갖출 수 있습니다. 이로써 LLM은 단순한 텍스트 생성기를 넘어, 실질적인 업무 자동화를 위한 강력한 엔진으로 진화할 수 있게 됩니다. 이는 또한 LLM이 '생각하고 행동하는' 에이전트로 나아가는 중요한 디딤돌이 됩니다.

“오늘날, 우리는 콘텐츠 저장소, 비즈니스 도구, 개발 환경을 포함하여 데이터가 존재하는 시스템에 AI 비서를 연결하기 위한 새로운 표준인 모델 컨텍스트 프로토콜(Model Context Protocol, MCP)을 오픈 소싱합니다. 그 목표는 최첨단 모델이 더 좋고 더 관련성 높은 응답을 생성하도록 돕는 것입니다.” - [15]에서 발췌

Anthropic이 제안한 모델 컨텍스트 프로토콜(MCP)은 LLM이 임의의 도구와 상호작용할 수 있도록 하는 아이디어를 확장한 인기 있는 프레임워크입니다. 간단히 말해, MCP는 외부 시스템이 LLM의 프롬프트에 컨텍스트를 제공하는 데 사용되는 형식을 표준화합니다. 복잡한 문제를 해결하기 위해 LLM은 시간이 지남에 따라 점진적으로 더 많은 외부 도구 세트와 통합되어야 할 것입니다. 이 과정을 간소화하기 위해 MCP는 이러한 통합을 위한 표준 형식을 제안하고, 개발자들이 MCP 서버라고 불리는 사전 구축된 통합(pre-built integrations)을 생성할 수 있도록 합니다. 이 서버는 모든 LLM이 다양한 사용자 정의 데이터 소스(custom data sources)와 연결하는 데 사용될 수 있습니다. 아래를 참조하십시오.

**MCP의 일반 아키텍처 묘사** (source)

도구 사용에 대해 더 깊이 탐구하고 싶은 분들은 이 주제에 대한 다음 개요 시리즈를 참조하십시오.
*   [도구를 사용하도록 LLM 미세 조정하기](link)
*   [프롬프트 기반 도구 사용](link)
*   [LLM을 코드 인터프리터와 통합하기](link)
*   [LLM이 자체 도구를 생성하도록 허용하기](link)

**도구 사용의 한계.**
도구 활용 방식의 강력한 이점에도 불구하고, 도구 기반 LLM의 역량은 본질적으로 그들의 추론 능력(reasoning capabilities)에 의해 제약됩니다. 도구를 효율적으로 사용하기 위해서는 LLM이 다음의 능력을 갖추어야 합니다.
*   복잡한 문제들을 더 작은 하위 과업으로 나눌 수 있어야 합니다.
*   주어진 문제를 해결하기 위해 어떤 도구를 선택할지 판단할 수 있어야 합니다.
*   해당 도구에 대한 호출을 정확한 형식으로 안정적으로 생성해야 합니다.
복잡한 도구 활용은 LLM이 효과적인 조율자(orchestrator) 역할을 수행할 것을 요구하며, 이는 모델의 추론 역량과 전반적인 신뢰도에 크게 좌우됩니다.

도구 사용은 LLM의 능력을 비약적으로 확장하지만, 이 과정에서 LLM은 단순한 언어 생성기를 넘어 '지능형 조율자(intelligent orchestrator)'로서의 역할을 수행해야 합니다. 이는 단순히 도구를 호출하는 것을 넘어, 문제 해결을 위한 최적의 전략을 세우고, 각 단계에서 어떤 도구를 사용할지 결정하며, 도구의 실행 결과를 해석하고, 예상치 못한 오류에 대응하는 복합적인 추론 과정이 필요함을 의미합니다. 예를 들어, '여행 계획을 세워줘'라는 요청을 받은 에이전트는 목적지 정보 검색, 항공권 예약, 숙소 예약, 날씨 확인, 일정 조율 등 여러 하위 작업을 식별하고, 각 작업에 필요한 도구를 순서대로 호출해야 합니다. 이 과정에서 검색 결과가 불충분하거나 예약 시스템에서 오류가 발생할 경우, 에이전트는 이를 인지하고 대안을 찾아야 합니다. 이러한 복잡한 의사결정 과정이 바로 LLM의 추론 능력에 기반합니다. 따라서 도구 사용의 진정한 잠재력을 발휘하기 위해서는 LLM의 추론 능력을 강화하는 것이 필수적입니다.

**에이전트의 지능: 추론 능력의 발전**

**추론 모델의 신비 해소하기**
자율성(agency)과 논리적 추론(reasoning)의 밀접한 연관성을 감안할 때, 추론 능력은 수년 동안 LLM 연구의 가장 중요한 초점 중 하나였습니다. 현재 추론 연구에 대한 보다 상세한 내용은 다음 섹션을 참고하시기 바랍니다. 그러나 여기서는 포괄적인 이해를 위해 추론 모델(reasoning models)의 핵심 개념을 간략하게 소개하겠습니다.

인간의 인지 과정은 직관적이고 빠른 '시스템 1' 사고와 논리적이고 단계적인 '시스템 2' 사고로 나눌 수 있습니다. 초기 LLM은 주로 시스템 1과 유사한 능력을 보였지만, AI 에이전트가 복잡한 문제를 해결하고 자율적인 행동을 하기 위해서는 시스템 2와 같은 심층적인 추론 능력이 필수적입니다. 추론 모델 연구의 궁극적인 목표는 LLM이 단순한 패턴 매칭을 넘어, 복잡한 정보를 분석하고, 논리적 단계를 거쳐 결론에 도달하며, 필요에 따라 자신의 추론 과정을 수정할 수 있도록 하는 것입니다. 이러한 능력이 강화될수록 LLM은 더욱 신뢰할 수 있고 유능한 에이전트가 될 수 있습니다.

Cameron R. Wolfe, Ph.D. · 2월 18일
[전체 스토리 읽기](link)

**CoT 프롬프팅(CoT prompting).**
LLM이 처음 주목받기 시작했을 때, 가장 흔했던 비판 중 하나는 복잡한 논리적 추론을 수행할 수 없다는 점이었습니다. 하지만 연쇄 사고(Chain of Thought, CoT) 프롬프팅 [6, 7] 연구를 통해, 일반적인 LLM(바닐라 LLM)이 우리가 초기에 생각했던 것보다 훨씬 더 추론 능력이 뛰어남이 드러났습니다. CoT 프롬프팅의 핵심 아이디어는 단순합니다. LLM에게 최종 결과만 요구하는 대신, 최종 답변에 앞서 그에 대한 근거(rationale)나 상세한 설명(explanation)을 먼저 생성하도록 지시하는 것입니다. 아래를 참조하십시오.

(from [7])
놀랍게도, 이 방법론은 일반 LLM의 추론 과제 수행 능력을 현저히 개선했으며, 이는 우리가 올바른 접근 방식을 적용한다면 LLM이 충분히 복잡한 추론을 수행할 수 있음을 시사합니다.

CoT 프롬프팅은 LLM의 '생각하는 과정'을 외부화하여, 모델이 문제 해결 과정을 단계별로 전개하고 자기 수정할 기회를 제공합니다. 이는 특히 상식 추론, 수학 문제 풀이, 코드 생성 등 논리적 사고가 필요한 작업에서 큰 효과를 발휘합니다. CoT의 성공 이후, 이를 더욱 발전시킨 다양한 프롬프팅 기법들이 등장했습니다. 대표적으로 '자기 일관성(Self-consistency)' [4]은 여러 개의 CoT 경로를 생성하고 그중 가장 일관된 답변을 선택하여 정확도를 높이는 방식입니다. 또한, '사고의 나무(Tree-of-Thought, ToT)' [5]는 CoT를 더욱 구조화하여 여러 추론 경로를 탐색하고, 각 경로의 유효성을 평가하여 최적의 경로를 선택하는 고급 추론 전략을 제안합니다. 이러한 CoT 기반 기법들은 LLM이 단순히 텍스트를 생성하는 것을 넘어, 복잡한 문제에 대한 심층적인 이해와 해결 능력을 갖추도록 돕는 중요한 전환점이 되었습니다.

**추론 모델.**
CoT 프롬프팅은 대단히 효과적이며, 오늘날 대부분의 LLM에 있어 필수적인 요소입니다. 예를 들어, ChatGPT 역시 답변과 함께 CoT를 기본적으로 제공합니다. 하지만 이러한 추론 방식은 다소 단순합니다. 전체 추론 과정이 LLM이 생성한 CoT에만 의존하며, 문제의 난이도에 따른 유동적인 적응(dynamic adaptation)이 부족합니다.

(source)
이러한 한계를 극복하고자 최근 연구에서는 추론에 특화된 LLM, 즉 '추론 모델'을 개발하기 위한 새로운 훈련 기법들을 도입했습니다. 이 모델들은 일반 LLM과는 다른 방식으로 문제에 접근합니다. 즉, 답변을 제시하기에 앞서 가변적인 시간 동안 '사고'하는 과정을 거칩니다.

(from [8])
추론 모델의 사고 과정(thoughts)은 일반적인 연쇄 사고와 유사하지만, 그 CoT는 표준 LLM의 CoT보다 훨씬 길고 (수천 개의 토큰에 달할 수 있음), 되돌아가기(backtracking)나 자체 개선(self-refinement)과 같은 복잡한 추론 행동을 보이며, 문제의 난이도에 따라 동적으로 조절됩니다. 다시 말해, 더 복잡한 문제일수록 더 긴 CoT를 생성합니다. 추론 모델을 실현시킨 주요 발전은 검증 가능한 보상 기반 강화 학습(reinforcement learning from verifiable rewards, RLVR)을 활용한 대규모 후속 훈련(large-scale post-training)이었습니다. 위를 참조하십시오. 수학이나 코딩과 같이 정답을 명확히 확인할 수 있는 문제(검증 가능한 문제)에 대한 실제 해답(ground truth solutions) 데이터셋이 있다면, LLM이 생성한 답변의 정확성을 쉽게 검증하고 이 피드백을 강화 학습(RL)에 활용하여 모델을 훈련할 수 있습니다. 이러한 훈련 과정을 통해 추론 모델은 RL 기반의 자체 진화(self-evolution)를 거쳐, 검증 가능한 추론 문제를 해결하기 위한 길고 복잡한 연쇄 사고를 생성하는 방법을 자연스럽게 습득합니다.

RLVR과 같은 훈련 기법은 LLM이 단순히 텍스트 패턴을 모방하는 것을 넘어, 실제 문제 해결 능력을 내면화하는 데 결정적인 역할을 합니다. 이는 모델이 '정답'과 '오답'을 명확히 구분하고, 올바른 추론 경로를 스스로 찾아나가도록 유도하기 때문입니다. 이러한 훈련을 통해 추론 모델은 단순히 CoT를 생성하는 것을 넘어, 문제의 본질을 파악하고, 여러 가설을 세우며, 각 가설의 타당성을 평가하고, 필요한 경우 되돌아가서 다른 경로를 탐색하는 등 복잡한 '심사숙고(deliberative thinking)' 과정을 수행할 수 있게 됩니다.
이러한 추론 모델의 발전은 AI 에이전트가 더욱 복잡하고 현실적인 시나리오에서 자율적으로 작동할 수 있는 기반을 마련합니다. 예를 들어, 금융 시장 예측 에이전트는 다양한 경제 지표를 분석하고, 여러 시나리오를 시뮬레이션하며, 각 시나리오에 대한 위험과 보상을 평가하는 심층적인 추론을 수행할 수 있습니다. 이는 인간 전문가의 개입 없이도 고품질의 의사결정을 내릴 수 있는 자율 에이전트의 가능성을 열어줍니다.

“우리는 어떠한 지도 데이터(supervised data) 없이도 LLM이 추론 능력을 개발할 수 있는 잠재력을 탐구하며, 순수한 강화 학습(reinforcement learning) 과정을 통한 자체 진화에 초점을 맞춥니다.” - [8]에서 발췌

**추론 궤적(Reasoning trajectories).**
결론적으로, RLVR 기반의 대규모 후속 훈련을 거친 추론 모델은 아래에 표시된 대로 표준 LLM의 작동 방식을 변화시킵니다. 즉, 직접적인 결과물을 도출하기보다, 추론 모델은 먼저 추론 과제를 분석하고 해결하는 임의의 길이의 CoT 1을 생성합니다. 이것이 바로 '사고(thinking)' 과정에 해당합니다. 이 추론 흔적(reasoning trace)의 길이를 조절함으로써 모델이 '사고'하는 깊이를 제어할 수 있습니다. 예를 들어, OpenAI의 o-시리즈 추론 모델은 낮은, 중간, 높은 수준의 추론 강도를 제공합니다.

**추론 모델의 입출력 서명**
모델은 여전히 프롬프트에 대해 하나의 최종 결과물을 내놓지만, 그 내부에 담긴 추론 궤적은 계획(planning), 역추적(backtracking), 감시(monitoring), 평가(evaluation)와 같은 다양한 고차원적 행동들을 내포하고 있습니다. 이러한 추론 궤적의 구체적인 예시와 특성은 DeepSeek-R1이 생성한 200만 개 이상의 추론 흔적을 포함하는 [Synthetic-1 데이터셋](link)에서 확인할 수 있습니다.

추론 궤적의 길이를 동적으로 조절할 수 있다는 것은 LLM이 문제의 복잡성에 따라 '깊이 생각하는' 능력을 가변적으로 활용할 수 있음을 의미합니다. 이는 단순한 질문에는 빠르게 답하고, 어려운 문제에는 더 많은 내부 자원을 할당하여 심층적으로 탐색하는 인간의 인지 방식과 유사합니다. 또한, 추론 궤적은 LLM에게 일종의 '내성(introspection)' 능력을 부여합니다. 모델은 자신의 사고 과정을 기록하고, 이를 바탕으로 자신의 계획을 수정하거나 오류를 감지하고 복구할 수 있습니다. 이는 에이전트 시스템의 신뢰성과 견고성을 크게 향상시키는 중요한 요소입니다. 예를 들어, 에이전트가 특정 도구 호출에서 실패했을 때, 단순히 오류 메시지를 반환하는 것이 아니라, 자신의 추론 궤적을 되짚어 왜 실패했는지 분석하고, 다른 도구를 시도하거나 문제 해결 전략을 수정하는 등의 자율적인 대응이 가능해집니다. 이러한 '자기 성찰' 능력은 AI 에이전트가 예측 불가능한 현실 세계에서 더욱 효과적으로 작동할 수 있도록 돕습니다.

**추론 + 에이전트.**
최근 추론 영역의 진보를 감안할 때, 지시를 바탕으로 계획을 수립하고 효과적으로 추론할 수 있는 충분히 역량 있는 LLM은 문제를 세분화하고, 각 구성 요소를 처리하며, 최종 해답에 스스로 도달할 수 있어야 합니다. 난해한 문제를 해결함에 있어 인간의 개입(human intervention)을 최소화하고, LLM에 더 큰 자율성(autonomy)을 부여하여 그들의 능력에 의존하는 것이 에이전트 시스템의 근본적인 사상입니다. 에이전트의 개념을 더욱 명확히 하기 위해, 이제 이러한 시스템을 구축하는 데 활용될 수 있는 주요 프레임워크에 대해 살펴보겠습니다.

LLM의 추론 능력 강화는 단순한 '도구 사용자'를 넘어 '자율적인 문제 해결사'로서의 에이전트 개념을 구체화하는 데 결정적인 역할을 합니다. 에이전트는 주어진 목표를 달성하기 위해 스스로 계획을 세우고, 필요한 정보를 탐색하며, 적절한 도구를 사용하고, 그 결과를 바탕으로 다음 행동을 결정하는 일련의 과정을 자율적으로 수행해야 합니다. 이 모든 과정에서 '추론'은 에이전트의 '두뇌' 역할을 하며, '행동'은 에이전트의 '팔과 다리' 역할을 합니다. 이 두 가지 요소가 효과적으로 결합될 때, 비로소 진정한 의미의 AI 에이전트가 탄생할 수 있습니다. 다음 섹션에서는 이러한 추론과 행동을 유기적으로 결합하여 자율 에이전트를 구축하는 대표적인 프레임워크인 ReAct에 대해 심층적으로 다루겠습니다.

**행동과 사고의 결합: ReAct 프레임워크 [1]**

“LLM의 도움으로, 언어가 근본적인 인지 메커니즘(cognitive mechanism)으로서 상호작용(interaction)과 의사 결정(decision making)에 중요한 역할을 할 것이라는 점이 더욱 분명해지고 있습니다.” - [1]에서 발췌

ReAct [1]는 'REasoning(추론)'과 'ACTion(행동)'의 합성어로, LLM 에이전트가 복잡한 문제를 자율적으로 분석하고 해결하도록 고안된 선구적인 범용 프레임워크 중 하나입니다. ReAct는 LLM을 핵심 엔진으로 삼아 순차적이고 다단계적인 문제 해결 절차를 수행하는 것으로 이해할 수 있습니다. 각 시점 t마다 LLM은 주어진 모든 피드백을 종합하고 당면 문제의 현재 상황을 고려하여, 향후 최적의 행동 방안(course of action)을 효과적으로 추론하고 결정할 수 있게 됩니다. 대부분의 LLM 기반 시스템이 순차적으로 모델링될 수 있다는 점을 감안할 때, ReAct는 매우 범용적이고 강력한 프레임워크로 평가받습니다.

ReAct 프레임워크는 인간이 복잡한 문제를 해결하는 방식, 즉 '소리 내어 생각하기(think aloud)'에서 영감을 받았습니다. 인간은 문제를 해결할 때 단순히 행동하는 것이 아니라, 중간에 자신의 생각을 정리하고, 다음 단계를 계획하며, 때로는 자신의 생각 자체를 검토하기도 합니다. ReAct는 이러한 인간의 인지 과정을 LLM에 적용하여, LLM이 단순히 답변을 생성하는 것을 넘어, 문제 해결을 위한 '사고(Thought)'와 '행동(Action)'을 번갈아 수행하도록 설계되었습니다. 이는 에이전트가 외부 환경으로부터 관찰(Observation)을 받고, 그 관찰을 바탕으로 '사고'를 통해 다음 '행동'을 계획하며, 그 '행동'의 결과로 새로운 '관찰'을 얻는 반복적인 루프를 형성합니다. 이러한 반복적인 상호작용은 에이전트가 불확실한 환경에서도 유연하게 대처하고, 점진적으로 목표에 도달할 수 있도록 돕습니다.

**에이전트를 위한 프레임워크 생성하기**
특정 시점 t에, 에이전트는 주변 환경으로부터 관찰(observation) o_t를 수신합니다. 이 관찰을 토대로, 에이전트는 특정 행동 a_t를 수행하기로 결정합니다. 이 행동은 문제 해결에 필요한 데이터를 웹에서 검색하는 것과 같은 중간 단계일 수도 있고, 당면한 문제를 '해결'하는 최종 행동일 수도 있습니다. 에이전트가 이러한 행동을 생성하는 데 사용하는 함수를 정책(policy) π 2라고 정의합니다. 이 정책은 에이전트의 과거 행동과 관찰의 연속된 기록인 문맥(context)을 입력받아 다음 행동 a_t를 예측하며, 이는 확정적(deterministically)이거나 확률적(stochastically)일 수 있습니다 3. 아래에 묘사된 바와 같이, 이러한 관찰과 행동의 순환 고리는 에이전트가 최종 행동을 도출할 때까지 반복됩니다.

**에이전트를 위한 관찰-행동 루프**
ReAct [1]는 위에서 설명한 관찰-행동 순환 고리에 중요한 한 가지 변형을 추가합니다. 정책 A에 의해 생성될 수 있는 잠재적 행동 공간(space of potential actions)은 통상적으로 에이전트가 수행할 수 있는 중간 및 최종 행동들의 집합을 포함합니다. 예를 들어, 웹 데이터 검색이나 문제의 최종 해결책 제시 등이 있습니다. 하지만 ReAct는 행동 공간을 언어(language) 영역으로 확장하여, 에이전트가 일반적인 행동 대신 텍스트 형태의 결과물을 행동으로 생성할 수 있게 합니다. 즉, 에이전트는 '생각'하는 것을 자신의 행동으로 선택할 수 있습니다. 아래를 참조하십시오.

**ReAct 프레임워크**
공식적으로, 위에서 보듯이 사고(thought)는 특별한 유형의 행동으로 정의됩니다. 프레임워크의 이름이 암시하듯이, ReAct의 핵심 동기는 추론(reasoning)과 행동(action) 사이의 균형을 확립하는 것입니다. 인간과 마찬가지로, 에이전트는 환경에서 수행할 행동을 사전에 생각하고 계획할 수 있어야 합니다. 추론과 행동은 서로에게 이로운 공생 관계(symbiotic relationship)를 형성합니다.

ReAct 프레임워크의 가장 혁신적인 부분은 바로 '사고(Thought)'라는 새로운 유형의 행동을 도입한 것입니다. 전통적인 에이전트의 행동은 주로 외부 환경과의 상호작용(예: 검색, 도구 호출)이나 최종 결과물 생성에 집중되어 있었습니다. 하지만 ReAct는 LLM이 자신의 내부 상태를 성찰하고, 문제 해결 계획을 세우거나, 현재 상황을 분석하는 등의 '생각하는 과정' 자체를 하나의 명시적인 행동으로 간주합니다.
이 '사고' 행동은 에이전트의 견고성과 복잡한 문제 해결 능력을 극대화하는 데 필수적입니다. 에이전트는 '사고'를 통해 다음과 같은 역할을 수행할 수 있습니다:
1.  **문제 분해 및 계획 수립**: 복잡한 문제를 더 작은 하위 문제로 나누고, 각 하위 문제를 해결하기 위한 구체적인 단계를 계획합니다.
2.  **상황 분석 및 전략 조정**: 현재까지의 관찰 결과와 행동 이력을 바탕으로 현재 상황을 분석하고, 필요에 따라 기존 계획을 수정하거나 새로운 전략을 수립합니다.
3.  **오류 감지 및 복구**: 도구 호출 실패나 예상치 못한 관찰이 발생했을 때, '사고'를 통해 원인을 분석하고, 이를 해결하기 위한 대안 행동을 모색합니다.
4.  **자기 성찰 및 학습**: 자신의 추론 과정을 검토하고, 더 나은 문제 해결 방식을 찾기 위한 내부적인 '학습' 과정을 수행합니다.
이처럼 ReAct는 추론(Reasoning)을 통해 '사고'하고, 그 '사고'를 바탕으로 '행동(Action)'하며, 그 '행동'의 결과로 얻은 '관찰(Observation)'을 다시 '사고'에 반영하는 순환 구조를 통해 자율적인 문제 해결 능력을 구현합니다.

“추론 흔적(reasoning traces)은 모델이 행동 계획을 유도하고, 추적하며, 업데이트하는 데 도움을 주며, 행동은 모델이 지식 기반(knowledge bases)이나 환경과 같은 외부 소스와 상호작용하고 추가 정보를 수집할 수 있도록 합니다.” - [1]에서 발췌

**에이전트는 어떻게 생각할까요? 에이전트의 행동 공간 예시**
일반적인 에이전트의 행동 공간은 이산적(discrete)이며, 대개 상대적으로 제한적입니다. 예를 들어, 질의응답에 특화된 에이전트는 다음과 같은 행동 선택지를 가질 수 있습니다 (위에 묘사됨).
*   관련 웹페이지를 찾기 위해 Google 검색을 실행합니다.
*   특정 웹페이지에서 필요한 정보를 추출합니다.
*   최종 답변을 제시합니다.
이러한 에이전트가 문제 해결 과정에서 수행할 수 있는 행동은 한정적입니다. 반면, 언어의 표현 공간은 사실상 무한합니다. 따라서 ReAct 프레임워크는 강력한 언어 모델을 정책(policy)으로 활용할 것을 요구합니다. 효율적인 '사고'를 생성하기 위해서는, 에이전트 시스템의 LLM 백엔드(backend)가 고도의 추론 및 계획 능력(planning capabilities)을 보유해야 합니다!

ReAct에서 '사고'는 단순한 텍스트 출력을 넘어, 에이전트의 내부 독백(inner monologue)이자 문제 해결을 위한 청사진 역할을 합니다. 언어는 복잡한 개념을 표현하고, 논리적인 연결 고리를 만들며, 추상적인 계획을 구체화하는 데 가장 유연하고 강력한 매체입니다. 에이전트는 이 언어 공간을 활용하여 다음과 같은 다양한 '사고 패턴(Thought patterns)'을 생성할 수 있습니다:
*   **작업 분해 및 세부 계획**: "이 문제를 해결하려면 먼저 [X]를 검색하고, 그 결과로 [Y]를 분석한 다음, [Z] 도구를 사용하여 최종 결론을 도출해야겠다."
*   **진행 상황 추적 및 검토**: "현재까지 [A] 단계는 완료되었고, [B] 단계에서 문제가 발생했다. [C]를 다시 확인하고 다른 접근 방식을 시도해야겠다."
*   **가설 설정 및 검증**: "만약 [P]가 사실이라면, [Q] 도구를 사용해서 이를 확인해봐야겠다."
*   **정보 종합 및 통찰 도출**: "검색 결과 [R]과 [S]를 종합해볼 때, [T]라는 결론을 내릴 수 있다."
이러한 사고 패턴들은 에이전트가 문제 해결 과정을 명시적으로 기록하고, 이를 바탕으로 다음 행동을 결정하며, 궁극적으로는 더욱 복잡하고 불확실한 환경에서도 효과적으로 작동할 수 있도록 돕습니다. 이는 LLM이 단순히 외부 정보를 처리하는 것을 넘어, 스스로 지능적인 내부 프로세스를 수행할 수 있음을 보여주는 중요한 발전입니다.

“이 확장된 행동 공간에서의 학습은 어렵고 강력한 언어 사전 지식(language priors)을 필요로 합니다… 우리는 주로 고정된 대규모 언어 모델이… 몇 개의 인컨텍스트 예시(few-shot in-context examples)로 프롬프트되어 도메인별 행동(domain-specific actions)과 자유 형식 언어 사고(free-form language thoughts)를 모두 생성하여 작업을 해결하는 설정에 중점을 둡니다.” - [1]에서 발췌

**사고 패턴(Thought patterns).**
에이전트가 생성 가능한 유용한 사고 패턴의 일반적인 예시는 과업을 세분화하고, 단계별 행동 계획을 수립하며, 최종 해답을 향한 진행 상황을 추적하거나, 혹은 LLM의 내재된 지식 기반(implicit knowledge base)에서 비롯된 문제 해결 관련 정보를 표출하는 것 등을 포함합니다. 에이전트는 자신의 사고 능력을 활용하여 문제를 어떻게 해결할 것인지 명확히 서술한 후, 이 계획을 실행하고 그 진행 과정을 주시합니다. 위의 두 가지 사례 모두에서 에이전트는 문제 해결 과정에서 수행할 다음 단계를 '다음으로, 나는 ~해야 한다…' 또는 '나는 ~을 검색해야 한다…'와 같이 명확하게 기술합니다.

**ReAct 에이전트에 주어진 몇 가지 예시 (from [1])**
*   **질문:** 음악가이자 풍자가인 Allie Goertz는 "심슨 가족"의 캐릭터 밀하우스에 대한 노래를 썼는데, 맷 그로닝은 밀하우스의 이름을 누구의 이름을 따서 지었는가?
    *   **사고 1:** 질문은 "심슨 가족" 캐릭터 밀하우스가 누구의 이름을 따서 지어졌는가로 단순화된다. 나는 밀하우스를 검색하고 누구의 이름을 따서 지어졌는지 찾기만 하면 된다.
    *   **행동 1:** Search[밀하우스]
    *   **관찰 1:** 밀하우스 무솔리니 반 후텐은 폭스 애니메이션 TV 시리즈 "심슨 가족"의 반복 출연 캐릭터로, 파멜라 헤이든이 목소리를 맡았고 맷 그로닝이 만들었다.
    *   **사고 2:** 이 단락은 밀하우스가 누구의 이름을 따서 지어졌는지 알려주지 않는다. "named after"를 찾아볼 수 있을 것 같다.
    *   **행동 2:** Lookup[named after]
    *   **관찰 2:** (결과 1 / 1) 밀하우스는 미국 대통령 리처드 닉슨의 이름을 따서 지어졌는데, 그의 중간 이름은 밀하우스였다.
    *   **사고 3:** 밀하우스는 미국 대통령 리처드 닉슨의 이름을 따서 지어졌으므로, 답은 리처드 닉슨이다.
    *   **행동 3:** Finish[리처드 닉슨]
*   **질문:** 핀란드 록 그룹에 관한 다큐멘터리는 아담 클레이튼 파월인가, 아니면 사이마 제스처인가?
    *   **사고 1:** 나는 아담 클레이튼 파월과 사이마 제스처를 검색하고, 어떤 다큐멘터리가 핀란드 록 그룹에 관한 것인지 찾아야 한다.
    *   **행동 1:** Search[아담 클레이튼 파월]
    *   **관찰 1:** [아담 클레이튼 파월]을 찾을 수 없었다. 유사 항목: ['아담 클레이튼 파월 3세', '7번가 (맨해튼)', '아담 클레이튼 파월 주니어 주립 사무소 건물', '이사벨 워싱턴 파월', '아담 파월', '아담 클레이튼 파월 (영화)', '잔카를로 에스포지토'].
    *   **사고 2:** 다큐멘터리를 찾기 위해 아담 클레이튼 파월 (영화)를 검색할 수 있다.
    *   **행동 2:** Search[아담 클레이튼 파월 (영화)]
    *   **관찰 2:** 아담 클레이튼 파월은 리처드 킬버그가 감독한 1989년 미국 다큐멘터리 영화이다. 이 영화는 영향력 있는 아프리카계 미국인 정치인 아담 클레이튼 파월 주니어의 흥망성쇠에 관한 것이다.[3][4] 나중에 PBS 시리즈 "디 아메리칸 익스피리언스"의 일부로 방영되었다.
    *   **사고 3:** 아담 클레이튼 파월 (영화)는 아프리카계 미국인 정치인에 관한 다큐멘터리이지, 핀란드 록 그룹에 관한 것이 아니다. 따라서 핀란드 록 그룹에 관한 다큐멘터리는 사이마 제스처일 것이다.
    *   **행동 3:** Finish[사이마 제스처]
*   **질문:** 니콜라스 레이와 엘리아 카잔은 어떤 직업을 공통으로 가지고 있는가?
    *   **사고 1:** 나는 니콜라스 레이와 엘리아 카잔을 검색하고, 그들의 직업을 찾은 다음, 그들이 공통으로 가진 직업을 찾아야 한다.
    *   **행동 1:** Search[니콜라스 레이]
    *   **관찰 1:** 니콜라스 레이 (본명 레이몬드 니콜라스 키엔즐 주니어, 1911년 8월 7일 - 1979년 6월 16일)는 미국의 영화 감독, 시나리오 작가, 배우였으며, 1955년 영화 "이유 없는 반항"으로 가장 잘 알려져 있다.
    *   **사고 2:** 니콜라스 레이의 직업은 감독, 시나리오 작가, 배우이다. 다음으로 엘리아 카잔을 검색하여 그의 직업을 찾아야 한다.
    *   **행동 2:** Search[엘리아 카잔]
    *   **관찰 2:** 엘리아 카잔은 미국의 영화 및 연극 감독, 프로듀서, 시나리오 작가, 배우였다.
    *   **사고 3:** 엘리아 카잔의 직업은 감독, 프로듀서, 시나리오 작가, 배우이다. 따라서 니콜라스 레이와 엘리아 카잔이 공통으로 가진 직업은 감독, 시나리오 작가, 배우이다.
    *   **행동 3:** Finish[감독, 시나리오 작가, 배우]
*   **질문:** 아서스 매거진과 퍼스트 포 우먼 중 어떤 잡지가 먼저 창간되었는가?

이러한 명시적인 사고 과정은 에이전트가 단기적인 목표뿐만 아니라 장기적인 목표를 달성하기 위한 복합적인 전략을 수립하는 데 필수적입니다. 예를 들어, 단순히 "밀하우스를 검색한다"는 행동 이전에 "질문은 '심슨 가족' 캐릭터 밀하우스가 누구의 이름을 따서 지어졌는가로 단순화된다. 나는 밀하우스를 검색하고 누구의 이름을 따서 지어졌는지 찾기만 하면 된다"는 사고는 문제의 본질을 파악하고 불필요한 정보 탐색을 줄이는 역할을 합니다.
또 다른 예시로, 만약 에이전트가 금융 시장 데이터를 분석해야 한다고 가정해 봅시다.
*   **질문**: "오늘의 주요 경제 지표와 그에 따른 시장 영향에 대해 분석해줘."
*   **사고 1**: "주요 경제 지표는 무엇이 있을까? CPI, 실업률, 기준금리 발표 등이 중요하겠다. 이 지표들을 검색하고, 각 지표가 시장에 어떤 영향을 미치는지 과거 데이터를 통해 분석해야겠다. 최신 데이터는 실시간 API를 통해 가져와야 한다."
*   **행동 1**: Search[오늘의 주요 경제 지표 발표 일정]
*   **관찰 1**: (검색 결과: CPI 발표 예정, FOMC 회의록 공개 예정)
*   **사고 2**: "CPI와 FOMC 회의록이 중요하군. CPI 발표는 실시간 데이터를, FOMC 회의록은 전문 분석 보고서를 찾아봐야겠다. 각 지표가 주식 시장과 채권 시장에 미칠 영향에 대한 전문가 의견도 함께 찾아보자."
이처럼 ReAct 에이전트는 사고를 통해 문제 해결의 깊이와 효율성을 높일 수 있습니다. 이는 에이전트가 단순히 정해진 경로를 따르는 것이 아니라, 스스로 길을 찾고 개척하는 능력을 갖추게 됨을 의미합니다.

전통적인 LLM과 달리, ReAct 에이전트는 하나의 프롬프트에 대해 단일 결과만을 생성하지 않습니다. 대신, 에이전트는 다음의 방식으로 순차적인 결과물을 생성합니다.
*   수행할 행동(구체적인 행동 또는 사고)을 결정합니다.
*   이 행동의 결과로 환경으로부터 피드백을 받습니다 (예: 검색 질의에서 얻은 정보).
*   이 새로운 문맥(context)을 바탕으로 다음 행동을 이어갑니다.
최종적으로 목표 행동에 도달하면 문제 해결 과정이 종료됩니다. 아래를 참조하십시오. 이러한 상태 기반(stateful)의 순차적인 문제 해결 방식은 에이전트의 고유한 특성이며, 이를 통해 표준 LLM과 차별화됩니다.

**ReAct로 문제를 순차적으로 해결하기**

**의사 결정(Decision making).** 의사 결정 과업에 대한 ReAct의 구성은 지식 기반 추론 과업의 구성과 매우 흡사합니다. 두 가지 의사 결정 작업 모두에서 인간은 ReAct 에이전트의 인컨텍스트 예시로 활용될 여러 추론 궤적을 직접 주석 처리합니다. 하지만 지식 집약적 추론 작업과 달리, 의사 결정 작업에서 ReAct가 사용하는 사고 패턴은 상대적으로 드뭅니다. 모델은 언제, 어떻게 '생각'할지 결정하는 데 있어 재량권(discretion)을 발휘하도록 프롬프트됩니다. 나아가, ReAct 에이전트는 WebShop 데이터셋에서 활용 가능한 더욱 다채로운 도구와 행동들을 제공받습니다. 예를 들어, 검색, 필터링, 상품 선택, 상품 속성 선택, 구매 등이 있습니다. 이러한 응용 프로그램은 더욱 복잡한 환경과 상호작용하는 ReAct의 유효성을 검증하는 좋은 시험대가 됩니다.

ReAct의 순차적 문제 해결 방식은 에이전트가 단일 '턴(turn)'으로 모든 문제를 해결하는 것이 아니라, 여러 턴에 걸쳐 점진적으로 해결책을 구축해 나간다는 점에서 중요합니다. 각 턴마다 에이전트는 이전의 '사고', '행동', '관찰' 기록을 바탕으로 현재의 '상태(state)'를 인지하고, 이를 다음 행동 결정에 활용합니다. 이러한 '상태 기반(stateful)' 특성은 에이전트가 장기적인 목표를 설정하고, 복잡한 프로젝트를 관리하며, 시간이 지남에 따라 변화하는 환경에 적응하는 데 필수적인 요소입니다.
특히 의사 결정 작업에서는 에이전트가 단순히 정보를 검색하는 것을 넘어, 여러 대안을 평가하고, 위험을 분석하며, 최적의 선택을 내리는 능력이 요구됩니다. WebShop과 같은 환경에서 ReAct 에이전트는 다양한 제품을 탐색하고, 사용자 선호도를 고려하며, 가격과 리뷰를 비교하는 등 실제 쇼핑 과정과 유사한 복합적인 의사 결정을 수행할 수 있습니다. 이는 에이전트가 단순한 정보 제공자를 넘어, 실제 사용자에게 가치를 제공하는 '조언자' 또는 '대행자'로서의 역할을 수행할 수 있음을 보여줍니다.

**ReAct는 잘 작동할까요?**
앞서 설명된 ReAct 에이전트는 다양한 기준선(baselines)과 비교 평가됩니다.
*   **프롬프팅(Prompting)**: 예시 추론 궤적에서 사고, 행동, 관찰을 제외하고 질문과 답변만 포함된 few-shot 프롬프트.
*   **CoT 프롬프팅(CoT prompting)**: 상기와 동일하나, 모델은 최종 해답을 제시하기 전에 연쇄 사고를 생성하도록 유도됩니다 4.
*   **행동(Act, action-only)**: ReAct 추론 궤적에서 사고 부분을 제외하고 관찰과 행동만을 남깁니다.
*   **모방(Imitation)**: 인간의 추론 경로를 모방하기 위해 모방 학습(imitation learning) 및/또는 강화 학습(reinforcement learning)으로 훈련된 에이전트 (예: BUTLER).
아래 그래프에서 나타나듯이, ReAct 프레임워크는 Act 설정보다 꾸준히 뛰어난 성능을 보이며, 에이전트가 행동하는 동시에 '사고'하는 능력이 지극히 중요함을 입증합니다. 게다가, CoT 프롬프팅은 특정 상황에서는 ReAct보다 더 나은 강력한 기준선이 될 수 있으나, LLM이 환각(hallucination)을 일으키기 쉬운 상황에서는 취약점을 드러냅니다. ReAct는 이러한 경우 외부 정보원을 활용함으로써 환각 현상을 회피할 수 있습니다. 궁극적으로, ReAct 에이전트의 성능 향상에는 여전히 많은 가능성이 존재합니다. 실제로, [1]에서 다루어진 에이전트들은 상당히 취약한 면모를 보였습니다. 예를 들어, 연구자들은 단순히 관련 없는 정보를 검색하는 것만으로도 에이전트가 실패할 수 있다고 지적합니다.

(from [1])

ReAct의 평가는 단순히 성능 지표를 넘어, 에이전트가 복잡한 환경에서 얼마나 '견고하게' 작동하는지에 대한 통찰을 제공합니다. '사고'의 추가가 단순한 행동만을 수행하는 모델(Act)보다 우월하다는 결과는, 문제 해결 과정에서 내부적인 추론과 계획이 얼마나 중요한지를 명확히 보여줍니다. 특히 CoT 프롬프팅이 환각에 취약한 반면 ReAct는 외부 도구를 통해 사실을 검증함으로써 이러한 문제를 완화할 수 있다는 점은 ReAct의 큰 장점입니다.
그러나 ReAct 역시 완벽하지는 않습니다. 에이전트의 '취약성'은 실제 환경에 배포될 때 중요한 문제입니다. 예를 들어, 잘못된 프롬프트 구성이나 예상치 못한 외부 환경 변화는 에이전트의 실패로 이어질 수 있습니다. 이는 에이전트가 단순히 정답을 찾는 것을 넘어, 불확실성과 오류를 처리하는 능력을 갖추어야 함을 시사합니다. 따라서 ReAct와 같은 프레임워크를 더욱 발전시키기 위해서는 에이전트의 '컨텍스트 민감성(context sensitivity)'을 높이고, 다양한 오류 시나리오에 대한 복구 메커니즘을 강화하는 연구가 필요합니다.

**ReAct + CoT.**
ReAct와 CoT의 결합. ReAct는 문제 해결 접근법에 있어 사실적 정확성과 근거를 제공합니다. CoT 프롬프팅은 외부 지식에 의존하지 않아 환각 현상으로 인해 어려움을 겪을 수 있으나, 복잡한 추론 작업을 구조화하는 데는 여전히 뛰어난 역량을 발휘합니다. ReAct가 에이전트의 추론 궤적에 관찰, 사고, 행동이라는 엄격한 구조를 적용하는 반면, CoT는 추론 과정을 공식화하는 데 있어 더 큰 유연성을 제공합니다.

(from [1])
두 가지 접근 방식 5의 장점을 모두 활용하기 위해, 우리는 이들을 필요에 따라 전환하여 사용할 수 있습니다! 예를 들어, ReAct가 N단계 후에도 해답을 제시하지 못할 경우 CoT 프롬프팅으로 전환하거나 (즉, ReAct → CoT), 여러 CoT 샘플에서 답변 불일치가 발생하면 ReAct를 활용할 수 있습니다 (즉, CoT → ReAct). 위 그림에서 볼 수 있듯이, 이러한 후퇴 전략(backoff approach)은 어느 방향에서든 에이전트의 문제 해결 역량을 증진시킵니다.

ReAct와 CoT의 결합은 각 방법론의 강점을 취하고 약점을 보완하는 '하이브리드 접근 방식'의 중요성을 보여줍니다. ReAct는 외부 도구를 통한 사실 검증과 구조화된 행동 계획에 강점을 가지지만, 때로는 내부적인 심층 추론이 필요한 상황에서 유연성이 부족할 수 있습니다. 반면 CoT는 내부 추론 과정의 유연성을 제공하지만, 외부 정보의 부재로 인한 환각 위험이 있습니다.
따라서 문제의 특성이나 현재 상황에 따라 가장 적합한 추론 전략을 동적으로 선택하는 '적응형 전략 선택(adaptive strategy selection)'이 중요해집니다. 예를 들어, 사실 확인이 중요한 질문에는 ReAct를 우선적으로 사용하고, 창의적인 아이디어나 복잡한 논리 전개가 필요한 경우에는 CoT를 활용하는 방식입니다. 이러한 유연한 전환은 에이전트가 더 넓은 범위의 문제에 효과적으로 대처하고, 전반적인 성능과 신뢰성을 향상시키는 데 기여합니다. 이는 마치 인간이 문제에 따라 직관적인 사고와 분석적인 사고를 번갈아 사용하는 것과 유사합니다.

**에이전트에 대한 이전 시도**
ReAct가 AI 에이전트를 위한 (논쟁의 여지는 있지만) 최초의 지속적인 프레임워크로 제시되었으나, 에이전트 분야에서는 그 이전에도 여러 중요한 연구와 개념들이 제안되었습니다. 본 절에서는 이러한 주요 제안들 중 일부를 간략히 비교 설명하여, ReAct 프레임워크가 기존 연구를 토대로 어떻게 더욱 유용하고 대중적인 프레임워크로 발전했는지 이해를 돕고자 합니다.

(from [10])
내적 독백(Inner monologue, IM) [10]은 ReAct와 가장 비교할 만한 작업 중 하나였으며, 로봇 공학 환경에서의 계획(planning)에 적용되었습니다. 위에 표시된 바와 같이, IM은 LLM을 여러 도메인별 피드백 메커니즘(domain-specific feedback mechanisms)과 통합합니다. 예를 들어, 장면 설명자(scene descriptors) 또는 성공 감지기(success detectors) 등이 있습니다. ReAct와 다소 유사하게, LLM은 외부 환경으로부터 반복적으로 행동하고, 생각하고, 피드백을 받음으로써—물건을 집는 것과 같은—작업의 해결책을 계획하고 모니터링하는 데 사용됩니다.

“우리는 구체화된 컨텍스트(embodied contexts)에서 사용되는 LLM이 자연어를 통해 제공되는 피드백 소스에 대해 어느 정도 추론할 수 있는지 조사합니다… 우리는 환경 피드백을 활용함으로써 LLM이 로봇 제어 시나리오에서 더 풍부하게 처리하고 계획할 수 있도록 하는 내적 독백을 형성할 수 있다고 제안합니다.” - [10]에서 발췌

IM은 LLM을 자연어(natural language)를 넘어선 도메인에서 문제 해결을 위한 일반적인 도구로 활용할 수 있는 가능성을 보여줍니다. 그러나 ReAct와 비교할 때, IM 내에서 LLM이 "생각"하는 능력은 제한적입니다. 모델은 환경으로부터 피드백을 관찰하고 다음에 무엇을 해야 할지 결정할 수 있을 뿐입니다. ReAct는 에이전트가 광범위하고 자유 형식의 사고를 출력할 수 있도록 함으로써 이 문제를 해결합니다.

(from [14])
대화형 의사 결정을 위한 LLM(LLMs for interactive decision making, LID) [14]은 순차적 문제를 해결하기 위한 언어 기반 프레임워크를 제안함으로써 언어를 계획 및 행동을 위한 일반적인 매체로 사용합니다. 우리는 다양한 작업의 컨텍스트와 행동 공간을 토큰 시퀀스로 공식화하여, 임의의 작업을 LLM 호환 가능한 표준화된 형식으로 변환할 수 있습니다. 그런 다음, 이 데이터는 LLM에 의해 수집되어 강력한 파운데이션 모델(foundation models)이 환경으로부터 피드백을 통합하고 결정을 내릴 수 있도록 합니다. 위를 참조하십시오. [14]에서 저자들은 모방 학습(imitation learning)을 사용하여 LID를 미세 조정하여 다양한 도메인에서 행동을 정확하게 예측합니다.

(from [11])
WebGPT [11]는 LLM (GPT-3)을 텍스트 기반 웹 브라우저와 통합하여 질문에 더 효과적으로 답변하는 방법을 탐구합니다. 이 작업은 개방형 도구 사용(open-ended tool use)의 초기 개척자이며, LLM에게 웹을 공개적으로 검색하고 탐색하는 방법을 가르칩니다. 그러나 WebGPT는 인간의 작업 해결책으로 구성된 대규모 데이터셋에 대해 명시적으로 미세 조정됩니다 (즉, 행동 복제(behavior cloning) 또는 모방 학습). 따라서 이 시스템은—매우 미래 지향적이고 효과적임에도 불구하고 (즉, 50% 이상의 경우에서 인간의 답변보다 선호되는 답변을 생성함)—많은 양의 인간 개입을 필요로 합니다. 그럼에도 불구하고, 인간 피드백을 통한 LLM 에이전트 미세 조정은 오늘날에도 뜨거운 연구 주제이며, WebGPT는 이 분야의 기초적인 작업입니다.

(from [12])
LLM의 광범위한 기능에서 영감을 받아, Gato [12]는 여러 양식(modalities), 작업 및 도메인에 걸쳐 행동할 수 있는 단일 "일반주의(generalist)" 에이전트입니다. 예를 들어, Gato는 아타리 게임, 이미지 캡셔닝, 로봇 팔 조작 등에 사용됩니다. 보고서에 설명된 바와 같이, Gato는 "자신의 컨텍스트를 기반으로 텍스트, 관절 토크(joint torques), 버튼 누름 또는 다른 토큰을 출력할지 결정할 수 있습니다." 이 모델은 거의 모든 문제를 해결할 수 있는 자율 시스템(autonomous system)을 만드는 목표를 향해 진정으로 나아갑니다. 그러나 WebGPT와 유사하게, Gato는 많은 문제 시나리오에 걸쳐 컨텍스트와 행동의 방대한 데이터셋—모두 평면적인 토큰 시퀀스로 표현됨—을 수집하는 모방 학습(imitation learning) 접근 방식을 통해 훈련됩니다.

(from [13])
계획을 통한 추론(Reasoning via Planning, RAP) [13]은 LLM에게 더 나은 세계 모델(world model)—즉, LLM이 행동하는 환경과 그로부터 오는 보상에 대한 이해—을 부여하여, 복잡하고 다단계적인 문제에 대한 해결책을 계획하는 LLM의 능력을 향상시키는 것을 목표로 합니다. 특히, LLM은 몬테카를로 트리 탐색(Monte Carlo Tree Search, MCTS)을 통해 탐색될 수 있는 추론 트리(reasoning tree)를 구축하는 데 사용되어 높은 보상을 달성하는 해결책을 찾습니다. 여기서 LLM 자체도 해결책을 평가하는 데 사용됩니다. LLM은 RAP에서 에이전트이자 세계 모델 역할을 합니다!

“LLM (에이전트로서)은 LLM (세계 모델로서)과 보상의 안내를 받아 추론 트리를 점진적으로 구축하며, 탐색(exploration)과 활용(exploitation) 사이의 적절한 균형을 통해 높은 보상을 얻는 추론 경로를 효율적으로 얻습니다.” - [13]에서 발췌

RAP는 유용하고 효과적인 프레임워크이지만, [13]에서는 순전히 텍스트 기반 추론 문제에만 적용됩니다. ReAct와 같은 일반적인 문제 해결 프레임워크는 아닙니다. 에이전트 시스템과 높은 수준의 유사성을 가지지만 주로 LLM 추론 능력 향상에 적용되는 많은 작업들이 있습니다.
*   **선택-추론(Selection-Inference)**은 문제 해결 과정을 선택(또는 계획)과 해결의 교대 단계로 분리하여 LLM 추론 능력을 향상시킵니다. Creswell et al.이 유사한 접근 방식을 개척했습니다.
*   **Re2**는 답변을 도출하기 전에 LLM에게 질문을 다시 읽도록 요청하여 LLM 추론 능력을 향상시키는 프롬프팅 전략입니다.
*   **LLM-증강기(LLM-Augmenter)**는 LLM과 데이터베이스 또는 도메인별 정보 소스를 결합하여 LLM에 유용한 외부 지식을 제공함으로써, 질문-답변 작업에서 근거성(groundedness)을 향상시킵니다.

LLM을 위한 에이전트와 추론의 교차점에 대한 연구(및 훨씬 더 많은 내용)에 대한 더 완전한 조사는 [이 놀라운 글](link)을 참조하십시오.

ReAct 이전의 에이전트 연구들은 LLM의 초기 단계에서부터 자율적인 시스템의 가능성을 모색했습니다. Inner Monologue (IM) [10]는 로봇 공학 분야에서 LLM이 환경 피드백을 통해 계획을 세우고 모니터링하는 내적 독백의 잠재력을 보여주었지만, LLM의 '사고' 능력이 제한적이라는 한계가 있었습니다. LLMs for Interactive Decision Making (LID) [14]는 언어를 계획 및 행동의 범용 매체로 사용하여 다양한 작업을 표준화된 형식으로 처리하는 방법을 제시했습니다. WebGPT [11]는 LLM이 웹 브라우저를 활용하여 질문에 답변하는 개방형 도구 사용의 선구자였으나, 대규모 인간 주석 데이터셋에 대한 미세 조정이 필요했습니다. Gato [12]는 다양한 양식, 작업, 도메인에서 행동할 수 있는 '일반주의(generalist)' 에이전트로서 LLM의 잠재력을 극대화했지만, 이 역시 방대한 모방 학습 데이터를 요구했습니다. Reasoning via Planning (RAP) [13]은 LLM에게 더 나은 '세계 모델(world model)'을 부여하여 복잡한 문제 해결 계획 능력을 향상시키는 데 초점을 맞추었으나, 주로 텍스트 기반 추론 문제에 국한되었습니다.
이러한 초기 시도들은 LLM 기반 에이전트의 가능성을 보여주었지만, 대부분 특정 도메인에 특화되거나, 대규모 수동 데이터 주석이 필요하거나, LLM의 자율적인 추론 능력을 충분히 활용하지 못하는 한계를 안고 있었습니다. ReAct는 이러한 선행 연구들의 통찰을 바탕으로, LLM의 강력한 언어 능력을 '사고'와 '행동'의 유연한 전환에 활용함으로써, 더 적은 인간 개입으로도 범용적이고 자율적인 문제 해결이 가능한 프레임워크를 제시하며 에이전트 연구의 새로운 지평을 열었습니다.

**AI 에이전트의 스펙트럼: 단순 LLM에서 자율 시스템까지**

“언어 모델 기반 에이전트의 시작점을 보는 가장 간단한 방법은 모든 도구 사용 언어 모델입니다. 에이전트의 스펙트럼은 여기에서부터 복잡성이 증가합니다.” - Nathan Lambert

업계에서의 뜨거운 관심에도 불구하고, '에이전트'라는 용어에 대한 명확한 정의는 여전히 부재합니다. 무엇이 진정한 '에이전트'로 간주될 수 있는지에 대한 다양한 논의가 활발히 이루어지고 있습니다. 에이전트 정의의 불분명함은 오늘날 우리가 마주하는 에이전트들이 매우 넓은 범위의 복잡성 스펙트럼에 걸쳐 존재한다는 사실에서 기인합니다. 대략적으로 볼 때, 에이전트의 기능은 특정 면에서 LLM의 기능과 흡사해 보일 수 있으나, 에이전트는 보통 문제 해결을 위해 더 광범위한 전략과 도구를 활용할 수 있습니다. 지금까지 다룬 정보들을 바탕으로, 이제 AI 에이전트가 가질 수 있는 능력의 범위와 이러한 능력들이 일반 LLM과 어떻게 다른지 이해하기 위한 체계적인 틀을 제시하고자 합니다.

에이전트의 정의가 모호한 것은 이 개념이 단순한 '도구 사용 LLM'부터 '완전 자율 시스템'에 이르기