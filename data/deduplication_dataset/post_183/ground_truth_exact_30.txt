환영합니다, 독자 여러분! 이번 주 LLM Watch에서는 다음 내용을 다룹니다:

*   강화 학습(reinforcement learning)을 위한 스케일링 법칙(scaling laws)
*   통합 신경-심볼릭 언어(unified neural-symbolic languages)
*   다중 모드 검색 프레임워크(multimodal retrieval frameworks)
*   에이전트(agent)를 위한 새로운 훈련 패러다임(training paradigms)
*   효율적인 추론 기법(reasoning techniques)
*   실시간 비디오 이해를 위한 아키텍처(architectures)
*   짝을 이루지 않은 다중 모드 데이터의 활용
*   잠재된 추론 능력의 활성화

아래에서는 각 논문의 핵심 아이디어와 발견 사항을 원본 출처와 함께 요약하여 설명합니다. 다시는 업데이트를 놓치지 않도록 구독하는 것을 잊지 마세요.

가장 빠르게 AI 엔지니어(AI Engineer)가 되는 방법은? 직접 만들어보는 것입니다! Towards AI의 산업 중심 코스인 '초급부터 고급 LLM 개발자까지(약 90개 강의)'를 통해 실습 경험을 쌓으세요. 실제 세상에 영향을 미치기 위해 좌절했던 전직 박사들과 개발자들이 만들었습니다.

*   프로덕션(production) 준비가 된 앱(app) 구축: RAG, 미세 조정(fine-tuning), 에이전트(agent)
*   지도: 디스코드(Discord)를 통한 강사 지원
*   선수 과목: 기본 파이썬(Python)
*   결과: 인증된 제품 출시
*   가치 보장: 30일 환불 보장

기술을 한 단계 높이세요

---

**LLM을 위한 RL 컴퓨팅(compute) 스케일링(scaling)이 예측 가능해지다** ( 논문 )

LLM을 위한 강화 학습(Reinforcement Learning, RL) 컴퓨팅 스케일링의 기술(The Art of Scaling Reinforcement Learning Compute for LLMs) (Khatri et al., 2025) - Meta AI와 UT 오스틴(Austin) 연구진은 언어 모델(language model)을 위한 RL 미세 조정(fine-tuning)의 "미스터리 박스(mystery box)"를 다룹니다. 이들은 40만 GPU-시간(GPU-hour) 연구를 수행했으며, RL 성능이 사전 훈련(pre-training)에서 보이는 개방형 멱법칙(power-law)이 아닌 시그모이드(sigmoid) 형태의 곡선(S-곡선)으로 스케일링된다는 것을 발견했습니다. 초기에는 개선이 느리다가, 급격한 개선 단계를 거쳐 포화 상태에 이릅니다. 이는 추가적인 컴퓨팅(compute)이 한계 효용 체감(diminishing returns)을 가져온다는 의미입니다. 결정적으로, 이 예측 가능한 곡선은 소규모 실행의 결과를 훨씬 더 큰 규모의 실행으로 외삽(extrapolate)할 수 있게 하여 LLM을 위한 RL에 절실히 필요한 예측 가능성을 제공합니다.

저자들은 다양한 RL "레시피(recipe)"가 성능 상한선에 미치는 영향을 분석했으며, 최적의 성능을 위한 "ScaleRL" 레시피를 제안합니다. 이 레시피는 비동기 파이프라인(asynchronous pipeline) 사용, CISPO와 같은 안정적인 손실 함수(loss function), 출력 계층(output layer)에서의 FP32 정밀도(precision) 유지, 적응형 프롬프트 커리큘럼(adaptive prompt curricula) 등을 포함합니다. ScaleRL은 거의 이상적인 시그모이드 스케일링 동작을 보여주며, 단일 RL 실행으로 최대 10만 GPU-시간까지 예측된 성능 목표에 도달할 수 있음을 입증했습니다. 이 연구는 RL 미세 조정이 시행착오의 영역에서 벗어나 더 정밀한 과학으로 발전할 수 있음을 시사합니다. 이제 연구자들은 추가적인 컴퓨팅 자원이 얼마나 많은 보상 개선을 가져올지 사전에 예측하여, 시간과 자원의 낭비를 줄이고 효율적인 연구 계획을 수립할 수 있게 되었습니다. 이는 특히 대규모 언어 모델의 훈련 비용을 고려할 때 매우 중요한 발전입니다.

---

**텐서 논리(Tensor Logic) - 신경망(Neural) AI와 심볼릭(Symbolic) AI의 통합** ( 논문 )

텐서 논리: AI의 언어(Tensor Logic: The Language of AI) (Domingos, 2025) - 딥러닝(deep learning)과 논리(logic)를 연결하는 새로운 프로그래밍 패러다임(programming paradigm)에 대한 과감한 제안입니다. 이 논문은 신경망(neural networks), 심볼릭 추론(symbolic reasoning), 확률 모델(probabilistic models)을 위한 공통 기반 역할을 하도록 설계된 최소한이지만 표현력이 풍부한 언어인 **텐서 논리(tensor logic)**를 소개합니다. 유일한 프로그래밍 구성 요소는 논리적 추론 규칙(logical inference rules)이 불리언 텐서(Boolean tensors)에 대한 텐서 연산(tensor operations) (아인슈타인 합산(Einstein summation))으로 볼 수 있다는 통찰력을 기반으로 하는 "**텐서 방정식(tensor equation)**"입니다. 실제로 텐서 논리는 관계(논리 프로그램(logic programs)에서와 같이)를 희소 텐서(sparse tensors)로 취급하고, 논리 규칙(Datalog에서와 같이)은 해당 텐서에 대한 조인(joins) 및 합산(summations)을 수행한 후 비선형성(non-linearities)을 적용하는 것에 해당합니다. 이는 트랜스포머 어텐션 헤드(transformer attention head), 프롤로그(Prolog) 규칙, 커널 머신(kernel machine) 또는 팩터 그래프(factor graph)가 모두 동일한 텐서 방정식 언어로 작성될 수 있음을 의미합니다.

이 통합 언어는 현대적인 자동 미분(autodiff) 및 GPU 컴퓨팅(compute) 환경(PyTorch/TensorFlow와 유사)을 활용하여 **하나의 프레임워크(framework) 내에서 학습(learning)과 추론(reasoning)을 모두 지원**하는 것을 목표로 합니다. 예를 들어, 텐서 논리는 신경망의 강력한 패턴 인식(pattern recognition) 능력과 논리 규칙의 엄격한 추론 능력을 결합하여 "임베딩 공간(embedding space)에서의 건전한 추론(sound reasoning)"을 가능하게 합니다. 저자는 MLP, CNN, RNN, 트랜스포머(transformers)와 같은 다양한 신경망 아키텍처는 물론, 심볼릭 논리 프로그램(symbolic logic programs) 및 그래픽 모델(graphical models)까지 텐서 논리로 어떻게 간결하게 표현될 수 있는지 보여줍니다. 널리 채택된다면, 이 패러다임은 미분 가능한 AI(differentiable AI)와 심볼릭 AI(symbolic AI) 사이의 오랜 분열을 해소하고, 단일 시스템(system)에서 텐서의 확장성(scalability)과 논리의 투명성(transparency)을 동시에 제공할 수 있을 것입니다. 이는 AI 시스템의 설명 가능성(explainability)과 견고성(robustness)을 크게 향상시킬 잠재력을 가집니다.

---

**RAG-Anything: 통합 다중 모드 검색 증강 생성(Retrieval-Augmented Generation)** ( 논문 / 코드 )

RAG-Anything: 올인원 RAG 프레임워크(All-in-One RAG Framework) (Guo et al., 2025) - 검색 증강 LLM(retrieval-augmented LLMs)의 한계를 텍스트(text)를 넘어 확장합니다. 기존 RAG 시스템은 주로 텍스트 문서(textual documents)에 의존하여 LLM의 지식 단절(knowledge cutoff) 문제를 해결했습니다. 그러나 실제 세계의 정보는 이미지(images), 표(tables), 차트(charts), 오디오(audio), 수학 방정식(math equations) 등 다양한 모드(modality)로 존재하며, 텍스트 전용 RAG는 이러한 다중 모드 정보를 효과적으로 활용하지 못했습니다. RAG-Anything은 LLM이 **모든 모드(modality)에서 정보를 검색하고 추론할 수 있도록 하는 통합 프레임워크**를 제시합니다.

이 프레임워크의 핵심은 다중 모드 콘텐츠(텍스트, 시각 자료, 오디오 등)를 분리된 데이터 사일로(data silos)가 아닌 상호 연결된 지식 그래프(knowledge graphs)로 표현하는 것입니다. RAG-Anything은 지식 베이스(knowledge base)를 두 가지 유형의 그래프로 구축합니다. 첫 번째 그래프는 교차 모드 관계(cross-modal relationships)를 포착합니다 (예: 이미지와 캡션(caption) 또는 표와 설명 사이의 연결). 두 번째 그래프는 텍스트의 의미론적 유사성(textual semantic similarity)을 기반으로 합니다. 이러한 이중 그래프 표현을 통해 시스템은 구조적 탐색(structural navigation) (그래프의 링크/관계 따르기)과 의미론적 매칭(semantic matching) (임베딩 기반 검색(embedding-based search))을 결합한 교차 모드 하이브리드 검색(cross-modal hybrid retrieval)을 수행합니다. 이는 LLM이 여러 모드에 걸쳐 있는 증거를 일관된 방식으로 가져오고 통합하여, 복잡한 다중 모드 질의에 대한 답변의 정확도와 풍부함을 크게 향상시킵니다. 저자들은 RAG-Anything이 어려운 다중 모드 QA 벤치마크(benchmarks)에서 기존 텍스트 전용 검색보다 훨씬 뛰어난 성능을 보인다고 보고합니다. 특히 텍스트와 그림이 혼합된 긴 문서에서 기존 RAG가 관련 조각들을 놓칠 수 있는 상황에서 RAG-Anything의 이점은 상당합니다. 이 통합 접근 방식은 지식 접근에서 모드별 파편화(modality-specific fragmentation)를 제거하여, LLM이 이미지, 다이어그램(diagrams), 표 등을 텍스트처럼 원활하게 이해하고 활용할 수 있도록 합니다. 코드가 오픈 소스(open-sourced)로 공개되어 진정한 다중 모드 채팅 어시스턴트(chat assistants) 개발의 길을 열었습니다.

---

**초기 경험(Early Experience): 보상(Rewards) 없이 에이전트(Agents)가 직접 학습하게 하기** ( 논문 )

초기 경험을 통한 에이전트 학습(Agent Learning via Early Experience) (Zhang et al., 2025) - Meta AI 연구소에서 언어 에이전트(language agents)가 스스로 부트스트랩(bootstrap)할 수 있도록 하는 새로운 훈련 패러다임(training paradigm)입니다. 현대 LLM 기반 에이전트(LLM-based agents)는 잘 정의된 보상(rewards) 없이는 진정한 강화 학습(reinforcement learning)이 어렵거나 막대한 시행착오 롤아웃(rollouts)이 필요하기 때문에 종종 인간 시연(human demonstrations)을 통한 모방 학습(imitation learning)에 의존합니다. 이 논문은 "초기 경험(early experience)"을 중간 지점으로 제안합니다. 에이전트가 외부 보상 없이 환경과 상호 작용하도록 하고, 그 결과로 발생하는 상태(states)를 자기 지도 신호(self-supervision signals)로 사용합니다. 본질적으로 에이전트는 자체 궤적(trajectories)을 생성하고(최적이 아니더라도) 두 가지 방식으로 학습합니다. (1) 암묵적 세계 모델링(Implicit world modeling) - 환경의 역학(dynamics)을 내재화하기 위해 보게 될 미래 상태 전이(state transitions)를 예측합니다. (2) 자기 성찰(self-reflection) - 무엇이 잘못되었거나 개선될 수 있는지에 대한 자연어 설명(natural language explanations)을 생성하여 자신의 행동을 분석하고, 이를 통해 의사 결정 정책(decision policy)을 개선합니다. 결정적으로, 이 모든 학습은 어떠한 보상 함수(reward function) 없이, 순전히 에이전트의 인과 관계 "경험(experience)"으로부터 발생합니다.

이러한 접근 방식은 보상 설계(reward engineering)의 복잡성을 줄이고, 에이전트가 탐색(exploration)을 통해 환경에 대한 이해를 자율적으로 구축하도록 돕습니다. 8가지 다양한 작업(웹 브라우징(web browsing), 도구 사용(tool use), 내비게이션(navigation) 등)에서 초기 경험 데이터(early experience data)로 부트스트랩된 에이전트는 전문가 데모(expert demos)만으로 훈련된 에이전트보다 뛰어난 성능을 보였으며, 새로운 상황에 대한 더 나은 일반화(generalization)를 보여주었습니다. 예를 들어, 보상 없이도 환경에서 연습할 기회를 가졌던 에이전트는 고정된 전문가 궤적(expert trajectories)만 모방했던 에이전트보다 나중에 더 견고하게 작동합니다. 더욱이, 실제 보상을 사용할 수 있을 때 초기 경험부터 시작하면 유리한 출발점(head-start)을 제공합니다. 이러한 에이전트는 후속 RL 미세 조정(fine-tuning)을 위한 강력한 기반을 제공합니다. 따라서 이 패러다임(paradigm)은 순전히 모방 기반 에이전트(imitation-based agents)와 완전 자율 RL 에이전트(autonomous RL agents) 사이의 간극을 메우며, "보상 없는(reward-free)" 상호 작용(interaction)조차도 AI의 역량(competence)과 적응성(adaptability)을 크게 향상시킬 수 있음을 시사합니다. 이는 특히 보상을 정의하기 어려운 개방형 환경에서 에이전트를 훈련시키는 데 중요한 진전입니다.

---

**에이전트 추론(Agentic Reasoning)의 RL 이해하기: 데이터, 알고리즘, 전략** ( 논문 / 코드 )

에이전트 추론의 강화 학습 이해하기(Demystifying Reinforcement Learning in Agentic Reasoning) (Yu et al., 2025) - 복잡한 추론 작업(reasoning tasks)을 위해 RL로 LLM을 최적으로 미세 조정(fine-tune)하는 방법을 분석하는 포괄적인 연구입니다. 저자들은 RL로 강화된 "에이전트적(agentic)" LLM(도구와 다단계 추론을 사용하는 모델)을 세 가지 축을 따라 조사합니다: 훈련 데이터(training data), RL 알고리즘/하이퍼파라미터(algorithm/hyperparameters), 그리고 에이전트의 추론 모드(reasoning mode)입니다. 광범위한 실험을 통해 그들은 성공을 위한 몇 가지 핵심 관행을 식별했습니다:

(i) **고품질의 실제 상호 작용 궤적 활용**: 합성적으로 조작된 데이터 대신 실제 환경에서 수집된 상호 작용 궤적을 사용하는 것이 중요합니다. 이는 지도 미세 조정(supervised fine-tuning) 단계에서부터 훨씬 더 견고한 초기 모델을 구축하며, RL 전반에 걸쳐 다양하고 모델 인식적인 데이터셋(dataset)을 유지하는 것이 성능을 크게 향상시켰습니다. 예를 들어, 실제 사용자가 도구를 사용하여 문제를 해결하는 과정을 기록한 데이터는 LLM이 실제 문제를 푸는 데 필요한 미묘한 전략을 학습하는 데 필수적입니다.
(ii) **탐색 친화적인 RL 기법 적용**: 정책(policy)에 더 많은 자유를 허용하는 RL 기법이 효율적인 훈련에 필수적입니다. 이 연구는 이점(advantages)에 대한 더 높은 클리핑 임계값(clipping threshold) 사용, 긴 궤적에 대한 부드러운 보상 형성(reward shaping) 추가, 그리고 정책에 약간의 엔트로피(entropy) (무작위성)를 유지하는 것이 중요하다고 밝혔습니다. 이는 모델이 막히는 대신 새로운 해결 경로를 적극적으로 탐색하도록 장려합니다.
(iii) **에이전트의 추론 스타일 최적화**: '적을수록 많다(less can be more)'는 통찰은 에이전트의 추론 스타일에 대한 중요한 시사점을 제공합니다. 더 적고, 더 목표 지향적인 도구 호출(tool calls)로 계획하고 지나치게 장황한 사고의 사슬(chain-of-thought)을 피하는 에이전트가 실제로 더 높은 정확도(accuracy)와 도구 효율성(tool efficiency)을 달성합니다. 즉, 숙고적인 추론(deliberative reasoning)이 불필요한 도구 사용이나 끝없는 자기 대화(self-talk)보다 우수하다는 것입니다.

이러한 관행을 구현함으로써 저자들은 개선된 RL 레시피(recipe)를 사용할 때 40억(4B) 매개변수(parameter) 모델조차도 복잡한 에이전트 벤치마크에서 320억(32B) 모델보다 뛰어난 성능을 보일 수 있음을 보여줍니다. 그들은 또한 미래 연구를 위한 강력한 기준선(baseline) 역할을 할 고품질 데이터셋(dataset) (실제 도구 사용 흔적과 RL 미세 조정 세트 포함)을 공개합니다. 종합적으로, 이 연구는 추론 에이전트(reasoning agents)의 RLHF(Reinforcement Learning from Human Feedback) 스타일 훈련을 위한 매우 필요한 경험적 안내서(empirical guidebook)를 제공하며, LLM을 더 효과적인 의사 결정자(decision-makers)로 만드는 데 실제로 영향을 미치는 것이 무엇인지 정확히 지적합니다.

---

**RL 미세 조정(Fine-Tuning)이 도움이 되는 이유 - 이론적 관점** ( 논문 )

다음 토큰 예측(Next-Token Prediction) 후 강화 학습이 학습을 촉진하는 방법(How Reinforcement Learning After Next-Token Prediction Facilitates Learning) (Tsilivis et al., 2025) - 추론 작업(reasoning tasks)에서 RL로 LLM을 미세 조정하는 메커니즘(mechanics)에 대한 심층 분석입니다. LLM의 수학적 및 논리적 추론(logical reasoning)에서의 최근 돌파구는 다음 토큰 예측(next-token prediction)으로 먼저 사전 훈련(pre-train)한 다음, RL로 미세 조정(fine-tune)하는(예: 정답에 대한 보상(reward) 최적화) 레시피(recipe)에 의존해왔습니다. 이 논문은 RL 단계가 왜 그렇게 효과적인지 질문합니다. 저자들은 장난감 패리티 비트(parity bit) 작업을 사용하여 이론적 프레임워크(theoretical framework)를 개발하고, RL 미세 조정이 다음 토큰 훈련(next-token training)만으로는 기하급수적으로 더 많은 데이터(data)나 컴퓨팅(compute) 없이는 달성할 수 없는 일반화(generalization)를 가능하게 한다는 것을 증명합니다.

이 연구의 핵심은 RL 목표(objective)가 테스트 시간(test time)에 모델이 더 길고 복잡한 사고의 사슬(chain-of-thoughts)을 활용할 수 있도록 학습시킨다는 것입니다. 이는 모델이 추가적인 컴퓨팅(더 긴 설명/단계 생성)을 사용하여 작업을 해결하도록 효과적으로 허용합니다. 훈련 데이터(training data)에 짧은 추론 시퀀스(reasoning sequences)와 매우 드물게 긴 추론 시퀀스(예: 대부분 짧은 설명과 몇 개의 길고 상세한 설명)가 혼합되어 있을 때, 모방 학습(다음 토큰 예측)만으로는 드문 긴 종속성(dependencies)을 학습하는 데 어려움을 겪습니다. 그러나 올바른 최종 답변에 보상을 주는 RL을 적용하면, 모델은 데이터에 그러한 긴 예제가 희박하더라도 가끔씩 긴 사고의 사슬을 사용하여 해당 답변을 올바르게 얻는 방법을 학습합니다. 본질적으로 RL 미세 조정은 모델에게 긴 추론 사슬(reasoning chain)을 언제 전개할지 가르치는 "커리큘럼" 역할을 합니다. 저자들은 단순화된 선형 모델(linear model)에서 무시할 수 없는 비율의 긴 궤적(trajectories)이 존재하는 한, 이 2단계 훈련이 기본 작업을 효율적으로 학습하는 반면, 다음 토큰 학습(next-token learning)은 비현실적인 양의 긴 예제를 필요로 할 것이라고 증명합니다. 그들은 수학 추론 벤치마크(math reasoning benchmarks)에서 실제 LLM(LLaMA 모델)에 대해 이러한 현상을 확인했습니다. RL 2단계가 있는 모델은 없는 모델보다 훨씬 더 깊이 있는 문제들을 해결합니다. 이 연구는 인간 피드백을 통한 강화 학습(Reinforcement Learning from Human Feedback, RLHF)과 같은 기술이 추론을 극적으로 개선하는 이유에 대한 이론적 근거를 제공합니다. RL 단계는 단순한 미세 조정이 아니라, 사전 훈련(pre-training) 후 잠재되어 있던 더 길고 전략적인 추론을 활용하는 모델의 능력을 근본적으로 확장하는 것입니다.

---

**StreamingVLM: 끝없는 비디오를 위한 실시간 시각-언어 이해** ( 논문 / 코드 )

StreamingVLM: 무한 비디오 스트림을 위한 실시간 이해(StreamingVLM: Real-Time Understanding for Infinite Video Streams) (Xu et al., 2025) - 비디오+언어 모델(video+language models)을 실시간 비디오 통역사(live video interpreters)로 전환하는 혁신적인 아키텍처(architecture)입니다. 웨어러블 카메라(wearable cameras)나 로봇 인식(robot perception)과 같은 분야에서 AI가 활용됨에 따라, 무한한 비디오 스트림(video stream)을 제한된 메모리(memory)와 낮은 지연 시간(latency)으로 처리하는 것이 중요한 과제가 됩니다. 기존 시각-언어 모델(vision-language models)은 전체 어텐션(full attention)의 제곱 비용(quadratic cost)이나 슬라이딩 윈도우(sliding window) 방식의 시간적 일관성(temporal coherence) 손상 문제로 인해 긴 비디오 처리에서 어려움을 겪었습니다. StreamingVLM은 모델이 훈련되는 방식과 스트리밍 데이터(streaming data)에서 추론(inference)하는 방식을 일치시키는 통합 프레임워크(unified framework)를 소개합니다.

이 모델은 짧고 겹치는 비디오 청크(video chunks)에 대한 지도 미세 조정(supervised fine-tuning)을 통해 훈련되지만, 특별한 메모리 관리 메커니즘을 가집니다. 추론 시에는 과거 컨텍스트(context)를 순진하게 재계산하거나 잊어버리는 대신, 특정 상태(states)를 재사용하여 유지합니다. 구체적으로, StreamingVLM은 세 부분으로 구성된 고정 크기 키-값 캐시(key-value cache)를 유지합니다: (a) "어텐션 싱크(attention sinks)" - 장기 정보를 집계하는 잠재 상태(latent states) 집합, (b) 최근 시각 프레임(visual frames)의 짧은 윈도우(window) (항상 최신 몇 순간을 전체 세부 사항으로 볼 수 있도록), (c) 최근 텍스트의 더 긴 윈도우 (대화나 지침을 기억할 수 있도록). 컨텍스트(context)를 스마트하게 재설정하고 정제된 상태(distilled state)만 전달함으로써, 비디오 스트림이 무한정 계속되더라도 모델의 계산(computation)은 제한된 상태를 유지합니다. 저자들은 초 단위 캡션(captioning)/정렬(alignment)이 필요한 2시간 연속 비디오를 포함하는 새로운 벤치마크(benchmark)를 만들었습니다. StreamingVLM은 2시간이 넘는 전체 비디오에 걸쳐 일관된 이해를 유지하면서 실시간 성능(하나의 GPU에서 초당 8프레임)을 달성합니다. 일대일 비교에서 강력한 기준선(baseline) (GPT-4O mini)을 66%의 확률로 능가합니다. 놀랍게도, 스트리밍 훈련 전략(streaming training strategy)은 모델을 일반 비디오 QA(video QA)에도 더 능숙하게 만들었습니다. 해당 작업에 대한 직접적인 미세 조정(fine-tune) 없이도 표준 장기 비디오 QA 벤치마크에서 성능을 4-6점 향상시켰습니다. 이 연구는 슬라이딩 윈도우(sliding windows)와 학습된 메모리(learned memory)를 원칙적이고 효율적인 방식으로 결합하여, 라이브 비디오를 끊임없이 시청하고 설명할 수 있는 LLM 기반 어시스턴트(assistants)의 길을 닦습니다.

---

**함께하면 더 좋다: 짝을 이루지 않은 다중 모드(Multimodal) 데이터가 단일 모드(Unimodal) 모델을 강화한다** ( 논문 / 코드 )

함께하면 더 좋다: 더 강력한 단일 모드 모델을 위한 짝을 이루지 않은 다중 모드 데이터 활용(Better Together: Leveraging Unpaired Multimodal Data for Stronger Unimodal Models) (Gupta et al., 2025) - 어떤 이미지도 텍스트와 직접 짝을 이루지 않더라도, 시각 모델(vision model)이 텍스트와 함께 훈련함으로써 더 나아질 수 있을까요? 이 논문은 "그렇다"고 답하며, 일대일 정렬(one-to-one alignment) 없이 단일 모델이 다른 모드(modalities)를 순차적으로 처리하는 훈련 패러다임(training paradigm)인 **UML(Unpaired Multimodal Learner)**을 소개합니다. 전제는 다른 데이터 모드(이미지, 텍스트, 오디오)가 모두 동일한 기본 현실의 투영이라는 것입니다. 따라서 짝을 이루는 쌍이 없더라도, 모드 간에 매개변수(parameters)를 공유하는 모델은 서로에게 도움이 되는 일반적인 특징(general features)을 학습할 수 있습니다. UML에서는 한 번의 반복(iteration)에서 이미지를 네트워크(network)에 입력하고, 다음 배치(batch)에서는 동일한 네트워크에 텍스트를 입력하는 식(모드별 입력 인코더(input encoders) 사용)으로, 공유 가중치(shared weights)를 계속 업데이트합니다. 저자들은 선형 가우시안 모델(linear Gaussian model) 하에서 보조 모드(auxiliary modality)를 사용하는 것이 목표 작업에 대한 피셔 정보(Fisher information)를 엄격하게 증가시켜, 학습된 표현(learned representation)의 분산(variance)을 효과적으로 줄인다는 것을 증명합니다. 직관적으로 말하면, 두 번째 모드에서 얻은 짝을 이루지 않은 데이터(unpaired data)조차도 세상에 대한 "또 다른 관점(another view)"을 제공하여 주 모드(primary modality)에 대한 모델의 이해를 날카롭게 할 수 있습니다. 한 가지 놀라운 이론적 결과는 다음과 같습니다. 어떤 경우에는 보조 모드에서 얻은 단일 샘플(sample)이 정보 이득(information gain) 측면에서 주 모드의 전체 추가 샘플만큼 기여할 수 있다는 것입니다.

경험적으로, UML은 광범위한 벤치마크(benchmarks)에서 검증되었습니다. 예를 들어, 이미지 분류기(image classifier)를 짝을 이루지 않은 텍스트 데이터(text data)와 함께 훈련하면 이미지 단독 훈련보다 더 높은 정확도(accuracy)를 얻을 수 있습니다. 특히 세분화된(fine-grained) 또는 소수샷(few-shot) 테스트에서 그렇습니다. UML로 훈련된 모델은 분포 변화(distribution shifts)에도 더 강건함이 입증되었습니다(예: ImageNet 모델이 텍스트와 함께 훈련되었다면 ImageNet-Sketch 또는 -A 변형에 더 강함). 이점은 세 가지 모드(modalities)로 확장됩니다. 이미지 모델에 텍스트와 오디오를 모두 추가하면 단조로운 개선(monotonic improvements)이 나타났습니다. 특히, 연구자들은 모드 간의 "교환 비율(exchange rate)"을 정량화했습니다. 특정 사전 훈련된 인코더(pretrained encoders)를 사용하여 정확도 기여 측면에서 이미지 1개 ≈ 228단어에 해당한다는 것을 발견했으며, 이는 정렬된 표현(aligned representations)이 모드를 어떻게 매우 상호 보완적으로 만드는지 강조합니다. 전반적으로 UML은 간단하지만 강력한 아이디어를 보여줍니다. 다중 모드 이점(multimodal benefits)을 얻기 위해 완벽하게 동기화된 다중 모드 쌍이 필요하지 않습니다. 모든 데이터를 공유 표현(shared representation)을 가진 하나의 모델에 넣으면 각 모드가 다른 모드를 더 강력하게 만들 것입니다. 이는 데이터 수집 및 주석(annotation) 비용을 크게 줄이면서도 강력한 단일 모드 모델을 구축할 수 있는 새로운 길을 제시합니다.

---

**기반(Base) LLM 대 사고(Thinking) LLM: 잠재된 추론(Reasoning) 능력 해제** ( 논문 )

기반 모델은 추론하는 방법을 알고, 사고 모델은 언제 추론할지 배운다(Base Models Know How to Reason, Thinking Models Learn When) (Venhoff et al., 2025) - 더 큰 "사고(thinking)" 모델이 실제로 새로운 추론 기술(reasoning skills)을 습득하는 것일까요, 아니면 단순히 기반 모델(base model)이 이미 가지고 있던 기술을 사용하는 방법을 배우는 것일까요? 이 도발적인 연구는 후자에 대한 증거를 찾습니다. 이른바 "사고 모델(thinking models)"(사고의 사슬(chain-of-thought) 또는 다른 추론 프롬프트(reasoning prompts)로 미세 조정된 모델과 같은)은 수학 단어 문제(math word problems)와 같은 작업에서 기반 LLM보다 지속적으로 뛰어난 성능을 보입니다. 그러나 저자들은 훈련이 아닌 방법으로 **그 격차의 약 91%**를 줄일 수 있음을 보여줍니다. 그들은 기반 모델을 가져와 사고 모델의 신호(cues)를 사용하여 숨겨진 활성화(hidden activations)를 즉석에서 "조종(steers)"하는 하이브리드 설정(hybrid setup)을 만듭니다. 구체적으로, 그들은 추론 단계(reasoning steps)와 관련된 특정 내부 뉴런(neurons) 또는 방향(unsupervised method를 통해 해석 가능한 추론 행동(interpretable reasoning behaviors)을 찾음)을 식별했습니다. 추론 시간(inference time)에 토큰(tokens)의 약 12%에 대해서만 해당 방향으로 기반 모델을 살짝 밀어주는 것만으로도 완전히 미세 조정된 모델(fully fine-tuned model)만큼 문제를 잘 해결할 수 있습니다. 이 모든 것은 어떠한 가중치(weights)도 업데이트(updating)하지 않고 이루어집니다.

이는 사전 훈련된 기반 모델(base pre-trained model)이 이미 필요한 추론 능력(reasoning capability)을 포함하고 있었음을 강력히 시사합니다. 사고 모델 미세 조정(thinking-model fine-tuning)이 한 일은 주로 모델에게 해당 능력을 언제 전개할지 가르치는 것이었습니다. 다시 말해, 사전 훈련 동안 모델은 원칙적으로 "추론하는 방법(how to reason)"을 배웠고, 전문화된 미세 조정 동안에는 "추론 모드(reasoning mode)로 전환하는 시점(when to switch)"을 배웠습니다. 이는 사고의 사슬 미세 조정(chain-of-thought finetuning)에 대한 우리의 이해를 재구성합니다. 이는 새로운 추론 회로(reasoning circuits)를 추가하는 것이 아니라 기존 회로를 스케줄링(scheduling)하는 것일 수 있습니다. 이 논문의 결과는 또한 실용적인 방법(practical method)을 제시합니다. 적절한 시기에 잠재된 추론(latent reasoning)을 활성화함으로써, 기반 GPT 스타일 모델(base GPT-style model)은 훨씬 적은 비용으로 고급 성능에 근접할 수 있습니다. 이는 매우 큰 LLM조차도 항상 스케일(scale)이나 많은 훈련(heavy training)이 필요한 것이 아니라, 신중한 자극(careful prodding)을 통해 이끌어낼 수 있는 미개발된 추론 잠재력(untapped reasoning potential)을 가지고 있다는 흥미로운 검증입니다. 이러한 통찰은 비용 효율적인 방식으로 LLM의 추론 능력을 최적화하는 데 중요한 방향을 제시합니다.

---

**마르코프적 사고자(The Markovian Thinker): RL을 통한 효율적인 긴 사고의 사슬(Long Chains of Thought)** ( 논문 / 코드 )

마르코프적 사고자(The Markovian Thinker) (Aghajohari et al., 2025) - 컴퓨팅(compute) 자원을 폭증시키지 않고 매우 긴 추론(reasoning)을 위해 LLM을 훈련시키는 새로운 패러다임(paradigm)입니다. 강화 학습(reinforcement learning)을 사용하여 LLM에게 다단계 추론(multi-step reasoning)을 수행하도록 가르칠 때(예: 일련의 단계로 긴 수학 증명 해결), 순진한 접근 방식(naive approach)은 전체 프롬프트(prompt)와 생성된 모든 토큰(tokens)을 다음 토큰 결정(next token decision)을 위한 상태(state)로 취급합니다. 이 "LongCoT" 설정은 상태 길이(state length)가 계속 증가하여, 사고의 사슬(chain-of-thought)이 길어질수록 제곱 시간/비용(quadratic time/cost)으로 이어진다는 것을 의미합니다. 이 논문은 전체 솔루션(solution)의 길이에 관계없이 모델이 항상 **고정 길이 컨텍스트(fixed-length context)에만 의존하도록 환경을 재정의하는 마르코프적 사고(Markovian Thinking)**를 제안합니다. 핵심은 추론 과정(reasoning process)을 세그먼트(segments)로 나누는 것입니다. 한 세그먼트(예: 256 토큰)가 생성된 후, 환경은 "재설정(resets)"됩니다. 모델에게 수행된 작업에 대한 간략한 요약(짧은 이월 상태(carryover state))을 제공하고, 모델은 새로운 컨텍스트 윈도우(context window)에서 추론을 계속합니다. 본질적으로 모델은 각 청크(chunk)의 끝에 재설정 후 원활하게 작업을 이어갈 수 있도록 간결한 상태 표현(state representation)을 작성하는 방법을 학습합니다.

이들은 8K 토큰 청크(8K-token chunks)를 사용하여 Delethink("컨텍스트를 삭제하고, 핵심을 유지하라"는 의미)라고 불리는 RL 환경(environment)에서 이를 구현합니다. 이 설정에서 훈련된 15억(1.5B) 매개변수 모델은 8K 컨텍스트(contexts)에서 최대 24K 토큰 길이의 일관된 추론 흔적(reasoning traces)을 생성할 수 있었으며, 이는 전체 24K 컨텍스트 윈도우(context window)로 훈련된 기준선(baseline)의 성능과 일치하거나 약간 능가했습니다. 고정된 상태(fixed state) 덕분에 컴퓨팅 비용(compute cost)은 추론 길이(reasoning length)에 따라 제곱이 아닌 선형적으로 증가합니다. 저자들은 추론을 96K 토큰으로 확장하는 데 표준 LongCoT RL(27 H100-개월)에 비해 마르코프적 방식(7 H100-개월)으로 약 4배 적은 GPU 시간(GPU time)이 소요될 것이라고 외삽(extrapolate)합니다. 그들은 또한 많은 기존 LLM(15억(1.5B)에서 1200억(120B)까지)이 미세 조정(fine-tuning) 없이도 이미 때때로 "마르코프적(Markovian)" 사고의 사슬(chain-of-thoughts)을 출력한다는 것을 발견했습니다(즉, 자연스럽게 추론을 세그먼트화(segment)함). 이는 모델이 원하는 행동의 제로샷 예제(zero-shot examples)로 시드(seed)될 수 있어 RL 훈련(training)을 안정적으로 만든다는 것을 의미합니다. 결론: 게임의 규칙(추론을 위한 MDP)을 변경함으로써, 우리는 LLM이 일정한 메모리(memory)와 관리 가능한 컴퓨팅(computation)으로 매우 긴 범위의 작업(long-horizon tasks)을 해결하도록 훈련시킬 수 있습니다. 이는 AI에서 확장 가능한 "시스템 2(System 2)" 추론을 향한 유망한 단계이며, 복잡한 문제 해결을 위한 LLM의 실제 적용 가능성을 크게 높입니다.

---

LLM Watch를 읽어주셔서 감사합니다! 새로운 게시물을 무료로 받고 제 작업을 지원하려면 구독하세요.
구독하기