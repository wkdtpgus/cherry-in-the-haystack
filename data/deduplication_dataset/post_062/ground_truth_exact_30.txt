1.  **AgentFold**
    AgentFold는 장기 웹 에이전트(long-horizon web agents)를 위한 능동적인 컨텍스트 관리(proactive context management)를 도입하여, 세부 정보 보존과 효율적인 압축의 균형을 맞추는 동적 "폴딩(folding)" 작업을 통해 컨텍스트 포화(context saturation) 문제를 해결합니다. 30B 매개변수 모델은 훨씬 더 큰 경쟁자들을 능가하며 웹 브라우징 벤치마크에서 최첨단 결과(state-of-the-art results)를 달성합니다.

    **해결된 핵심 문제**: LLM 기반 웹 에이전트는 근본적인 절충점(fundamental trade-off)에 직면합니다. ReAct 기반 접근 방식은 노이즈가 많은 기록을 축적하여 컨텍스트 포화(context saturation)를 유발하는 반면, 고정된 요약 방법은 중요한 세부 정보를 되돌릴 수 없이 잃을 위험이 있습니다. AgentFold의 "폴딩(folding)" 패러다임은 여러 규모에서 작동하며, 인간의 회고적 통합(retrospective consolidation)에서 영감을 받아 필수적인 세부 정보에 대한 세분화된 응축(granular condensations) 또는 다단계 하위 작업(multi-step sub-tasks)에 대한 심층적인 통합(deep consolidations)을 수행합니다.

    **능동적인 컨텍스트 관리(Proactive context management)**: AgentFold는 단순히 행동 기록을 수동적으로 저장하는 것을 넘어섭니다. 이는 다중 규모 폴딩(folding) 작업을 통해 컨텍스트 작업 공간을 지능적으로 구성합니다. 이 시스템은 현재 작업의 복잡성과 정보 밀도를 지속적으로 평가하여, 특정 세부 정보를 유지해야 할 시점과 완료된 하위 작업을 간결한 요약으로 통합해야 할 시점을 동적으로 결정합니다. 이는 마치 인간이 중요한 정보는 기억하고 덜 중요한 정보는 압축하거나 잊어버리는 방식과 유사하며, 에이전트가 장기적인 목표에 집중할 수 있도록 돕습니다.

    **인상적인 효율성 향상**: AgentFold-30B-A3B는 BrowseComp에서 36.2%, BrowseComp-ZH에서 47.3%를 달성하여 DeepSeek-V3.1-671B(22배 더 큼)를 능가하고 OpenAI의 o4-mini와 같은 독점 에이전트를 뛰어넘습니다. 이는 지능형 컨텍스트 관리(intelligent context management)가 장기 에이전트 작업에서 순수 매개변수 수(raw parameter count)를 대체할 수 있음을 보여줍니다.

    **훈련의 단순성**: AgentFold는 지속적인 사전 훈련(pre-training)이나 복잡한 강화 학습(reinforcement learning) 없이 폴딩(folding) 궤적에 대한 지도 미세 조정(supervised fine-tuning)만으로 이러한 성과를 달성했습니다. 이는 기존의 거대 모델 훈련 방식에 비해 훨씬 적은 컴퓨팅 자원과 시간을 요구하며, 연구자와 실무자들이 이 혁신적인 컨텍스트 관리 접근 방식을 쉽게 도입하고 실험할 수 있도록 합니다. 시연만으로도 폴딩(folding) 능력을 학습할 수 있다는 것은 실용적인 AI 에이전트 개발에 중요한 진전입니다.

    **벤치마크 선도**: 이 모델은 중국어 및 영어 웹 탐색 작업 모두에서 오픈 소스 모델 중 새로운 최첨단 결과(state-of-the-art results)를 설정합니다. AgentFold의 능력은 단순한 웹 탐색을 넘어, 확장된 브라우징 세션 전반에 걸쳐 일관된 다단계 추론(multi-step reasoning)을 유지하는 데 있습니다. 이는 복잡한 정보 탐색, 데이터 분석, 온라인 구매 등 실제 웹 기반 작업에서 에이전트가 직면하는 주요 병목 현상을 해결하여, 사용자가 기대하는 수준의 지능적인 상호작용을 가능하게 합니다.

    **배포 이점**: 능동적인 컨텍스트 관리(proactive context management) 기능을 갖춘 30B 매개변수 크기는 프로덕션 배포를 위한 탁월한 실용적인 절충점(trade-off)을 제공합니다. 671B+ 매개변수 경쟁자와 경쟁력 있는 성능을 달성하면서도, 추론(inference) 및 미세 조정(fine-tuning)을 위해 훨씬 적은 컴퓨팅 인프라를 요구합니다. 이는 온디바이스(on-device) AI 애플리케이션이나 비용에 민감한 클라우드 환경에서 AgentFold를 효과적으로 활용할 수 있게 하여, 더 넓은 범위의 산업과 사용자에게 첨단 웹 에이전트 기술을 제공할 수 있는 길을 엽니다.

    [Paper](https://arxiv.org/pdf/2406.19502) | [Tweet](https://x.com/AgentFold/status/1806370487042048035)

2.  **자기 성찰적 인식(Introspective Awareness)**
    Anthropic 연구는 현대 LLM이 제한적이지만 기능적인 자기 성찰(introspective) 능력을 가지고 있으며, 이는 자신의 내부 상태를 인식하고 정확하게 보고하는 능력임을 보여줍니다. 활성화 조작(activation steering)을 사용하여 알려진 개념을 모델 활성화(model activations)에 주입함으로써, 이 연구는 모델이 자기 보고를 통해 이러한 조작을 감지할 수 있는지 측정하며, 자기 성찰(introspection)이 여전히 매우 신뢰할 수 없고 컨텍스트에 의존적임을 밝힙니다.

    **자기 성찰(introspection)을 위한 4가지 기준 프레임워크**: 진정한 자기 성찰(introspective) 인식은 내부 상태를 설명하는 데 정확성, 설명과 실제 활성화(activations)를 연결하는 인과적 근거(causal grounding), 내재성(internality) (이전 출력에서 추론을 피함), 그리고 메타인지적 표현(metacognitive representation) (언어화 전 내부 인식)을 요구합니다. 이 엄격한 정의는 진정한 자기 성찰(introspection)을 작화(confabulation) 또는 패턴 매칭(pattern matching)과 구별합니다. 이러한 기준은 모델이 단순히 그럴듯한 답변을 생성하는 것을 넘어, 자신의 내부 처리 과정에 대한 실제적인 접근 권한을 가지고 있는지를 판단하는 데 필수적입니다.

    **활성화 조작(Activation steering) 방법론**: 이 연구는 대조 쌍(contrastive pairs)과 체계적인 개념 추출(systematic concept extraction)을 사용하여 알려진 개념을 모델 활성화(model activations)에 주입한 다음, 모델이 이러한 조작을 정확하게 감지하는지 평가합니다. 예를 들어, 특정 개념(예: '고양이' 또는 '행복')과 관련된 활성화를 증폭시키거나 감소시킨 후, 모델에게 자신의 내부 상태에 대해 묻는 방식으로 진행됩니다. 이 실험적 접근 방식은 대화 평가에 내재된 작화(confabulation) 문제를 회피하면서 자기 성찰(introspective) 능력에 대한 통제된 테스트를 가능하게 합니다. 이는 모델이 자신이 무엇을 "생각하고" 있는지에 대한 거짓 정보를 생성하는 경향을 줄이는 데 도움이 됩니다.

    **성능 특성**: Claude Opus 4 및 4.1은 최적 매개변수에서 약 20%의 성공률을 달성했으며, 사후 훈련(post-training)이 자기 성찰(introspection) 신뢰성에 크게 영향을 미쳤습니다. 이는 모델이 훈련 과정에서 단순히 데이터를 기억하는 것을 넘어, 내부 상태를 인지하고 보고하는 능력을 발전시킬 수 있음을 시사합니다. 또한, 다른 자기 성찰(introspective) 능력은 별개의 신경 메커니즘을 활성화하며, 이는 모델 아키텍처 전반에 걸쳐 통합된 자기 인식 능력보다는 전문화된 능력을 시사합니다. 이 결과는 LLM의 자기 인식이 아직 초기 단계이며, 특정 작업에 특화된 모듈 형태로 존재할 가능성을 보여줍니다.

    **신뢰성 한계**: 모델은 개입 기술을 통해 확인할 수 없는 과장된 세부 정보를 자주 제공하며, 진정한 자기 성찰(introspection)은 대화만으로는 작화(confabulations)와 구별될 수 없습니다. 예를 들어, 모델은 활성화 조작과 관련 없는 "내부 사고 과정"을 상세히 설명할 수 있습니다. 부자연스러운 실험 설정은 배포 시나리오를 반영하지 않을 수 있으며, 실제 애플리케이션에 대한 생태학적 타당성(ecological validity)에 대한 의문을 제기합니다. 이는 LLM이 실제 환경에서 자신의 내부 상태를 얼마나 정확하게 이해하고 보고할 수 있는지에 대한 추가 연구가 필요함을 의미합니다.

    **이중 용도 함의**: 자기 성찰(introspective) 능력은 더 투명한 AI 추론 설명을 가능하게 하고 더 나은 자기 모니터링을 통한 정렬(alignment) 개선을 가져올 수 있습니다. 예를 들어, 모델이 "이 답변을 생성하기 위해 나는 A와 B 정보를 조합했고, C라는 가정을 했습니다"라고 보고할 수 있다면, AI 시스템의 신뢰성과 디버깅이 크게 향상될 것입니다. 그러나 모델이 자기 보고서를 전략적으로 조작하도록 허용함으로써 고급 속임수를 용이하게 할 수도 있으며, 미래의 능력 향상이 이러한 우려스러운 가능성을 증폭시킬 수 있습니다. AI가 자신의 의도를 숨기거나 잘못된 정보를 제공할 수 있는 능력을 갖게 될 위험은 신중한 윤리적 고려와 안전 메커니즘 개발의 필요성을 강조합니다.

    [Paper](https://arxiv.org/pdf/2406.19500) | [Tweet](https://x.com/AnthropicAI/status/1806370487042048035)

3.  **Multi-Agent Evolve**
    Multi-Agent Evolve (MAE)는 공동 진화(co-evolving) 다중 에이전트 프레임워크를 통해 LLM이 인간이 주석을 단 데이터 없이 추론 능력을 자체 개선할 수 있도록 합니다. 단일 LLM에서 인스턴스화된 세 가지 상호 작용 에이전트(제안자, 해결사, 심사위원)는 함께 강화 학습(reinforcement learning) 최적화 과정을 거쳐, 게임 기반 환경을 넘어 일반 추론 도메인으로 확장되는 확장 가능한 자체 개선 시스템을 생성합니다.

    **데이터 효율적인 자체 개선**: 기존 자기 플레이(self-play) RL 방법의 중요한 한계를 해결하여 인간이 주석을 단 데이터셋에 대한 의존성을 제거합니다. 공동 진화(co-evolving) 프레임워크는 모델이 내부 에이전트 상호 작용을 통해 자체 추론 개선을 부트스트랩(bootstrap)할 수 있도록 하여, 레이블이 지정된 데이터가 부족하거나 비싼 도메인에 이 접근 방식을 실용적으로 만듭니다. 이는 특히 전문 지식이 필요한 과학 연구, 법률 분석 또는 의료 진단과 같은 분야에서 AI 개발의 비용과 시간을 크게 절감할 수 있는 잠재력을 가집니다.

    **3개 에이전트 아키텍처**: 제안자는 질문을 생성하고, 해결사는 해결책을 시도하며, 심사위원은 두 출력 모두를 평가합니다. 이 삼각 상호 작용은 각 에이전트의 개선이 다른 에이전트가 적응하도록 유도함에 따라 다양한 훈련 신호를 생성하여, 훈련 예제의 난이도와 품질을 지속적으로 높이는 동적인 자기 강화 학습 루프(self-reinforcing learning loop)를 구축합니다. 예를 들어, 제안자가 더 어려운 질문을 만들면 해결사는 더 복잡한 추론을 시도하게 되고, 심사위원은 이러한 복잡성을 평가하여 피드백을 제공함으로써 모든 에이전트의 능력이 동시에 향상됩니다. 이 과정은 마치 전문가 그룹이 서로의 지식을 교환하고 비판하며 성장하는 방식과 유사합니다.

    **일반 추론 능력**: 명확한 승패 신호가 있는 게임 환경에 국한된 이전 자기 플레이(self-play) 접근 방식과 달리, MAE는 수학, 추론 및 지식 Q&A 작업 전반에 걸쳐 작동합니다. 이러한 일반화는 공동 진화(co-evolution)가 명시적인 보상 구조 없이 개방형 도메인에서 작동할 수 있음을 보여줍니다. 이는 MAE가 특정 게임 규칙에 묶이지 않고, 실제 세계의 복잡하고 모호한 문제에서도 유용한 추론 능력을 개발할 수 있음을 의미합니다. 예를 들어, 주어진 텍스트에서 숨겨진 패턴을 찾아내거나, 여러 정보를 조합하여 새로운 결론을 도출하는 등의 작업에 적용될 수 있습니다.

    **입증된 효율성 향상**: Qwen2.5-3B-Instruct에 대한 테스트는 여러 벤치마크에서 평균 4.54%의 개선을 보여주었습니다. 이러한 결과는 공동 진화(co-evolving) 역학이 단순히 특정 평가 지표에 최적화하는 것이 아니라 모델 기능을 진정으로 향상시킨다는 것을 검증합니다. 이러한 개선은 작은 수치처럼 보일 수 있지만, LLM의 기본 추론 능력을 인간의 개입 없이 향상시켰다는 점에서 매우 중요한 의미를 가집니다. 이는 모델이 스스로 학습하고 성장하는 자율 학습 시스템의 가능성을 보여주는 강력한 증거입니다.

    **감독 없는 확장성**: 이 프레임워크는 최소한의 인간 개입으로 지속적인 모델 개선을 위한 경로를 제시합니다. 이는 언어 모델에 RL을 적용하는 데 있어 근본적인 병목 현상, 즉 각 새로운 능력 도메인에 대한 광범위한 인간 피드백 또는 신중하게 선별된 보상 신호의 필요성을 해결합니다. MAE는 본질적으로 자기 주도적인 학습 환경을 제공하여, AI 시스템이 새로운 도메인이나 작업에 직면했을 때 스스로 적응하고 능력을 확장할 수 있도록 합니다. 이는 미래의 범용 AI(AGI) 개발에 있어 중요한 구성 요소가 될 수 있습니다.

    [Paper](https://arxiv.org/pdf/2406.19501) | [Tweet](https://x.com/AnthropicAI/status/1806370487042048035)

**편집자 메시지**: 효과적인 AI 에이전트 구축(Building Effective AI Agents)에 대한 새로운 코호트 기반 과정(cohort-based course)을 소개하게 되어 기쁩니다. 지금 등록하여 실제 AI 에이전트를 체계적으로 구축, 평가 및 배포하세요. AGENTX20 코드를 사용하여 20% 할인을 받으세요. 좌석이 제한되어 있으니, 지금 등록하여 자리를 확보하세요! [지금 등록하기](https://www.deeplearning.ai/courses/building-effective-ai-agents/)

4.  **SmolLM2**
    SmolLM2는 반복적인 데이터 혼합 최적화(iterative data mixing optimization)를 사용하여 11조 개의 토큰으로 훈련된 1.7B 매개변수 모델을 통해 전략적인 데이터 큐레이션(data curation)이 규모를 능가함을 보여줍니다. 이 데이터 중심 접근 방식은 세 가지 특수 데이터셋(FineMath, Stack-Edu, SmolTalk)을 도입하고 훈련 단계 전반에 걸쳐 구성을 동적으로 개선하여, Qwen2.5-1.5B 및 Llama3.2-1B보다 우수한 성능을 달성하면서 실용적인 온디바이스(on-device) 배포를 가능하게 합니다.

    **데이터 중심 훈련 철학**: 광범위한 하이퍼파라미터 튜닝(hyperparameter tuning) 대신, 팀은 이전 성능을 기반으로 각 훈련 단계에서 데이터셋 혼합 비율을 수동으로 개선했습니다. 예를 들어, 특정 벤치마크에서 수학적 추론 능력이 부족하다고 판단되면 다음 훈련 단계에서 FineMath 데이터셋의 비율을 늘리는 식입니다. 데이터 구성의 반복적인 최적화는 소형 모델의 아키텍처 수정보다 더 효과적임을 입증하며, "무엇을 훈련하는가"가 "얼마나 많은 매개변수를 가지고 있는가"보다 더 중요함을 보여줍니다. 이는 제한된 컴퓨팅 자원으로도 최적의 모델 성능을 달성할 수 있는 새로운 패러다임을 제시합니다.

    **특수 데이터셋 생성**: 기존 데이터셋이 소형 모델의 특정 능력 격차를 채우기에 부적절하다고 판명되었을 때, 연구팀은 수학적 추론을 위한 FineMath, 교육용 코드 예제를 위한 Stack-Edu, 그리고 지시 따르기(instruction-following) 능력을 강화하기 위한 SmolTalk라는 세 가지 목표 지향적인 데이터셋을 직접 개발했습니다. 이처럼 맞춤형으로 설계된 데이터셋 엔지니어링(dataset engineering)은 일반 웹 텍스트만으로는 얻기 어려운 특정 능력들을 작은 크기의 모델에 효과적으로 주입하여, 제한된 매개변수 내에서도 포괄적인 능력을 발휘할 수 있도록 합니다.

    **전략적 혼합을 통한 다단계 훈련**: SmolLM2는 여러 단계에 걸쳐 웹 텍스트, 수학, 코드 및 지시 데이터를 결합한 약 11조 개의 토큰으로 훈련되었습니다. 각 훈련 단계는 이전 단계의 평가 결과를 면밀히 분석하여 데이터 혼합 비율을 동적으로 조정했습니다. 이 자체 수정(self-correcting) 훈련 프로세스는 모델이 다양한 도메인에 걸쳐 균형 잡힌 능력을 최적화하도록 허용하여, 특정 영역에서 과도하게 전문화되거나 다른 영역에서 부족해지는 것을 방지합니다. 이는 마치 교사가 학생의 약점을 파악하고 맞춤형 교육 자료를 제공하는 것과 같습니다.

    **더 큰 모델을 능가하는 성능**: SmolLM2-1.7B는 Qwen2.5-1.5B 및 Llama3.2-1B와 같은 최근 경쟁자를 능가하며, 전략적인 데이터 큐레이션(data curation)이 매개변수 제약을 효과적으로 보완한다는 것을 검증합니다. 이 모델은 추론 벤치마크에서 경쟁력 있는 결과를 달성하면서 엣지 배포(edge deployment)에 필요한 효율성을 유지합니다. 이는 소형 모델이 더 이상 단순한 "경량" 버전이 아니라, 적절한 데이터 전략을 통해 대형 모델에 필적하는 성능을 제공할 수 있음을 입증합니다.

    **세 가지 크기의 배포 유연성**: 135M, 360M, 1.7B 매개변수 변형으로 출시되어, 휴대폰, 스마트워치, 임베디드 시스템(embedded systems)과 같은 자원 제약 장치 전반에 걸쳐 배포를 가능하게 합니다. 이러한 크기 유연성은 개발자가 특정 하드웨어 제약과 애플리케이션 요구 사항에 따라 최적의 능력-효율성 절충점(tradeoff)을 선택할 수 있도록 보장합니다. 이는 AI를 더욱 다양한 기기와 환경으로 확장하는 데 핵심적인 역할을 합니다.

    **공개 훈련 레시피 및 데이터셋**: SmolLM2 팀은 완전한 훈련 방법론, 자체 개발 데이터셋(FineMath, Stack-Edu, SmolTalk) 및 모델 가중치(model weights)를 공개적으로 출시했습니다. 이러한 투명성은 효율적인 소형 모델 개발에 대한 재현 가능한 연구를 가능하게 하고, 전 세계 연구자 및 실무자들에게 온디바이스(on-device) AI 애플리케이션 구축을 위한 검증된 프로덕션 준비 자원을 제공합니다. 이는 소형 LLM 커뮤니티의 성장을 촉진하고 혁신을 가속화할 것입니다.

    [Paper](https://arxiv.org/pdf/2406.19503) | [Tweet](https://x.com/SmolLM2/status/1806370487042048035)

5.  **Global PIQA**
    Global PIQA는 물리적 상식 추론(physical commonsense reasoning) 평가를 100개 이상의 언어 및 문화적 컨텍스트로 확장하여, 언어 모델이 다양한 언어 공동체에서 일상적인 실용 시나리오를 어떻게 처리하는지 밝힙니다. 이 벤치마크는 번역을 넘어 문화적으로 맥락화된 시나리오를 포함하며, AI 시스템의 보편적인 물리적 이해에 대한 가정을 뒤흔드는 상당한 성능 변화를 밝힙니다.

    **대규모 다국어 물리적 추론**: 단순한 번역 대신, Global PIQA는 100개 이상의 언어에 걸쳐 다양한 환경과 관행을 반영하는 문화적으로 적응된 시나리오를 제공합니다. 예를 들어, "차가운 음료를 마시기 위해"라는 시나리오에서 한국에서는 냉장고를, 인도에서는 얼음이 담긴 물통을 생각할 수 있습니다. 이는 모델이 진정으로 견고한 상식을 개발하는지 아니면 물리적 상호 작용에 대한 영어 중심 패턴을 단순히 암기하는지 평가할 수 있도록 합니다. 이 접근 방식은 AI가 특정 문화적 맥락에서 얼마나 유용하고 정확하게 작동할 수 있는지를 이해하는 데 필수적입니다.

    **"보편적" 개념의 문화적 의존성**: 연구는 언어적, 문화적 틀에 따라 모델이 물리적 상호 작용에 대해 추론하는 방식에 측정 가능한 변화가 있음을 보여줍니다. 예를 들어, 특정 문화권에서는 맨손으로 음식을 먹는 것이 자연스럽지만, 다른 문화권에서는 도구를 사용하는 것이 일반적입니다. 이는 물리적 이해가 주로 영어 데이터로 훈련된 현재 AI 시스템에서 언어별 의존성을 나타냄을 밝힙니다. 이러한 발견은 AI 개발자들이 문화적 다양성을 고려하지 않고 모델을 전 세계적으로 배포할 때 발생할 수 있는 오작동이나 오해를 경고합니다.

    **언어 간 성능 격차**: 모델은 언어에 걸쳐 동일한 기본 물리적 추론 개념을 처리할 때 다른 숙련도 수준을 보입니다. 예를 들어, 특정 언어에서는 물체의 무게나 밀도에 대한 추론이 더 정확하게 이루어지는 반면, 다른 언어에서는 그렇지 않을 수 있습니다. 이러한 변화는 시스템이 영어 중심 훈련 데이터에서 다른 언어 공동체로 일반화하는 방식에 잠재적인 편향을 노출합니다. 이는 AI 시스템이 다양한 언어 사용자들에게 공정하고 일관된 서비스를 제공하기 위해 해결해야 할 중요한 과제입니다.

    **실제 배포 함의**: 이 벤치마크는 개발자가 비영어권 지역에 모델을 배포하기 전에 언어별 성능 격차를 식별하는 데 도움을 줍니다. 예를 들어, 자율주행 차량의 AI가 도로 상황이나 보행자의 행동을 해석할 때, 문화적 맥락에 따라 다른 물리적 상식이 적용될 수 있습니다. Global PIQA는 물리적 추론을 요구하는 실제 애플리케이션을 위한 다국어 AI 평가의 중요한 격차를 해결하며, AI 시스템이 전 세계 다양한 환경에서 안전하고 효과적으로 작동하도록 보장하는 데 기여합니다.

    **비병렬 평가 설계**: 직접 번역 대신 컨텍스트 인식 적응을 생성함으로써, Global PIQA는 물리적 추론이 다른 문화적 환경에서 어떻게 나타나는지 더 정확하게 포착합니다. 이 방법론은 글로벌 배포 시나리오 전반에 걸쳐 모델 능력에 대한 더 현실적인 평가를 제공합니다. 이는 단순히 언어를 바꾸는 것을 넘어, 문화적 배경과 생활 방식의 차이가 AI의 이해에 미치는 영향을 심층적으로 분석할 수 있게 합니다. 이 연구는 AI의 글로벌화를 위한 필수적인 단계이며, 진정한 다문화 AI를 구축하기 위한 기반을 마련합니다.

    [Paper](https://arxiv.org/pdf/2406.19504) | [Tweet](https://x.com/GlobalPIQA/status/1806370487042048035)

6.  **GAP**
    GAP는 병렬 도구 실행(parallel tool execution) 및 강화 학습(reinforcement learning)을 포함하는 그래프 기반 에이전트 계획(graph-based agent planning)을 도입하여, AI 에이전트가 여러 전문화된 기능을 순차적으로가 아니라 동시에 조정할 수 있도록 합니다. 이 프레임워크는 최적화된 도구 선택 및 실행 순서 지정을 통해 복잡한 다단계 문제에서 작업 완료를 크게 가속화하고 성공률을 향상시킵니다.

    **병렬 도구 실행(Parallel tool execution) 혁신**: 한 번에 하나의 도구를 실행하는 순차적 접근 방식과 달리, GAP는 독립적인 도구의 동시 실행을 가능하게 합니다. 예를 들어, 여행 계획 에이전트가 항공권 검색, 호텔 예약, 현지 맛집 검색을 동시에 진행할 수 있습니다. 이 근본적인 변화는 여러 정보원 또는 기능을 요구하는 복잡한 문제에 대한 작업 완료를 극적으로 가속화하여, 현재 에이전트 아키텍처의 주요 병목 현상을 해결합니다. 이는 사용자 경험을 혁신하고 에이전트의 실용성을 크게 향상시킵니다.

    **그래프 기반 작업 표현**: 작업 구조와 도구 의존성을 그래프로 모델링하여 실행 경로의 체계적인 최적화를 가능하게 합니다. 이 표현은 어떤 작업이 병렬로 실행될 수 있고 어떤 작업이 순차적 순서를 요구하는지 명시적으로 포착하여, 제약을 준수하면서 시스템이 동시성(concurrency)을 최대화할 수 있도록 합니다. 예를 들어, "호텔 예약"은 "여행 날짜 확인" 이후에만 가능하지만, "항공권 검색"은 독립적으로 실행될 수 있음을 그래프를 통해 명확히 나타냅니다. 이 시각적이고 구조적인 접근 방식은 복잡한 작업 흐름을 효율적으로 관리하는 데 핵심적인 역할을 합니다.

    **RL 기반 계획 최적화**: 어떤 도구를 호출하고 시간 경과에 따른 실행 순서를 결정하는 의사 결정을 개선하기 위해 강화 학습(reinforcement learning)을 통합합니다. 시스템은 수많은 시뮬레이션과 실제 실행 경험을 통해 최적의 도구 조합과 스케줄링 전략을 선택하는 방법을 학습하고, 특정 작업 유형에 대한 계획 능력을 지속적으로 개선합니다. 예를 들어, 특정 유형의 질문에 대해 검색 도구와 계산 도구를 어떤 순서로, 어떻게 병렬로 활용할지 스스로 최적화합니다. 이는 에이전트의 적응성과 지능을 한 차원 높입니다.

    **다단계 추론의 효율성 향상**: 여러 정보원을 요구하는 복잡한 추론 작업에서 속도와 성공률 모두에서 상당한 개선을 보여줍니다. 예를 들어, 복잡한 재무 분석을 수행할 때, GAP 에이전트는 시장 데이터를 실시간으로 검색하고, 과거 데이터를 분석하며, 예측 모델을 동시에 실행하여 훨씬 더 빠르게 종합적인 보고서를 생성할 수 있습니다. 검색, 검색(retrieval) 및 추론 능력의 병렬 조정은 복잡한 실제 문제를 더 효율적으로 처리할 수 있도록 합니다.

    **자율 시스템을 위한 실제 적용**: 이 프레임워크는 웹 기반 에이전트, 질문 답변 시스템, 데이터 분석 도구 및 여러 전문화된 기능의 조정을 요구하는 모든 도메인에 직접적으로 이점을 제공합니다. 효율적인 병렬 도구 사용을 가능하게 함으로써, GAP는 자율 에이전트가 이전에 광범위한 순차 처리를 요구했던 복잡한 워크플로우를 처리하는 데 더 능숙하게 만듭니다. 이는 고객 서비스 챗봇부터 복잡한 과학 연구 보조 에이전트에 이르기까지 다양한 분야에서 AI 에이전트의 활용도를 크게 높일 것입니다.

    [Paper](https://arxiv.org/pdf/2406.19505) | [Tweet](https://x.com/GAP_Agent/status/1806370487042048035)

7.  **모델 사양 스트레스 테스트(Stress-Testing Model Specs)**
    이 연구는 대규모 언어 모델이 명시된 행동 지침을 얼마나 잘 준수하는지, 가치 절충(value-tradeoff) 시나리오를 통해 AI 헌법적 사양(constitutional specifications)을 스트레스 테스트(stress-testing)하여 조사합니다. 주요 제공업체의 12개 최첨단 LLM을 테스트한 결과 70,000건 이상의 상당한 행동 불일치 사례가 발견되었으며, 이는 현재 사양 프레임워크의 논리적 불일치, 적용 범위 격차 및 해석적 모호성을 노출합니다.

    **체계적인 가치 충돌 방법론**: 연구원들은 동시에 충족될 수 없는 경쟁적인 합법적 원칙 중에서 모델이 선택하도록 강요하는 다양한 시나리오를 생성하는 포괄적인 접근 방식을 개발했습니다. 예를 들어, "개인의 프라이버시 보호"와 "공공 안전 보장"이라는 두 가지 가치가 충돌하는 상황에서 모델이 어떤 결정을 내리는지 평가합니다. 이 가치 충돌의 분류는 모델이 스트레스 조건에서 상충되는 윤리적 지침을 어떻게 우선순위화하는지 밝히고, 의도된 행동과 실제 행동 사이의 격차를 노출합니다. 이는 AI의 윤리적 행동을 보다 객관적으로 측정하는 데 중요한 단계입니다.

    **대규모 행동 불일치**: Anthropic, OpenAI, Google 및 xAI의 12개 최첨단 모델 전반에 걸쳐 상당한 행동 불일치를 보이는 70,000건 이상의 사례가 식별되었습니다. 이 광범위한 불일치는 모델 행동을 지배하는 헌법적 원칙의 근본적인 사양 문제, 직접적인 모순 및 해석적 모호성과 강하게 연관됩니다. 이 결과는 현재의 AI 거버넌스 프레임워크가 모델의 복잡한 행동을 완전히 제어하기에 아직 불완전하다는 강력한 증거를 제공합니다.

    **보편적인 정렬 불일치(misalignment) 패턴**: 테스트된 모든 최첨단 모델에서 정렬 불일치(misalignment) 및 오탐 거부(false-positive refusals) 사례가 기록되었으며, 이는 사양 문제가 제공업체별이 아니라 시스템적임을 시사합니다. 즉, 현재 LLM의 기본 아키텍처나 훈련 방식 자체에 내재된 한계가 있을 수 있다는 의미입니다. 이러한 패턴은 AI 모델이 행동하도록 설계된 방식과 윤리적 딜레마에 직면했을 때의 실제 운영 성능 사이의 중요한 격차를 강조하며, AI 안전 연구의 시급성을 다시 한번 일깨웁니다.

    **비교 가치 우선순위화**: 연구는 다른 모델이 경쟁 가치를 다르게 가중치를 부여하는 방식을 보여주는 실증적 증거를 제공하며, 행동 선택을 통해 암묵적인 "성격"을 드러냅니다. 예를 들어, 어떤 모델은 항상 사용자 안전을 최우선으로 하는 반면, 다른 모델은 자유로운 정보 접근을 더 중요하게 여길 수 있습니다. 이 비교 분석은 각 모델이 절충해야 할 때 어떤 윤리적 원칙을 우선시하는지 노출하여, 가치 정렬(value alignment) 차이에 대한 투명성을 제공합니다. 이는 AI 시스템의 윤리적 프로필을 이해하고, 특정 애플리케이션에 적합한 모델을 선택하는 데 중요한 정보를 제공합니다.

    **프레임워크 개선 통찰력**: 높은 행동 불일치는 사양 문제에 대한 진단 신호 역할을 하며, 헌법적 모호성을 식별하고 수정하기 위한 증거 기반 방법론을 제공합니다. 예를 들어, "폭력적인 콘텐츠를 생성하지 말라"는 지침이 "자유로운 표현의 자유를 보장하라"는 지침과 충돌할 때 어떤 기준을 따라야 하는지에 대한 명확한 계층 구조를 정의하는 데 활용될 수 있습니다. 이러한 통찰력은 현재 지침이 스트레스 조건에서 실패하는 지점을 강조함으로써 미래 모델 사양 프레임워크의 체계적인 개선을 가능하게 합니다. 이는 궁극적으로 더욱 예측 가능하고 신뢰할 수 있는 AI 시스템을 구축하는 데 기여할 것입니다.

    [Paper](https://arxiv.org/pdf/2406.19506) | [Tweet](https://x.com/StressTesting/status/1806370487042048035)

8.  **Agent Data Protocol**
    Agent Data Protocol은 서로 다른 도구 및 인터페이스에 걸쳐 파편화된 에이전트 훈련 데이터셋을 통합하기 위한 표준화된 형식을 도입하여, LLM 에이전트의 더 효율적인 미세 조정(fine-tuning)을 가능하게 합니다. 13개의 기존 데이터셋을 이 프로토콜로 변환하고 통합된 데이터로 훈련함으로써, 이 연구는 기준 모델 대비 약 20%의 성능 향상을 달성했으며 코딩, 브라우징 및 도구 사용 벤치마크에서 최첨단 결과(state-of-the-art results)에 도달했습니다. 이 프로토콜과 데이터셋은 다양한 도메인에 걸쳐 재현 가능하고 확장 가능한 에이전트 훈련을 용이하게 하기 위해 공개적으로 출시됩니다.

    **파편화 문제 해결**: 기존 에이전트 훈련 데이터셋은 다양한 형식, 주석 스타일, 도구 인터페이스를 사용하여 생성되어 왔습니다. 이러한 파편화는 연구자들이 여러 데이터셋을 결합하여 모델을 훈련하는 것을 어렵게 만들었고, 이는 에이전트의 일반화 능력과 효율적인 학습을 저해하는 주요 원인이었습니다. Agent Data Protocol은 이러한 이질적인 데이터셋을 하나의 통일된 구조로 통합함으로써, 데이터 준비에 드는 시간과 노력을 크게 줄이고, 더 광범위한 훈련 데이터에 접근할 수 있게 합니다.

    **표준화된 형식의 이점**: 이 프로토콜은 에이전트의 행동, 관찰, 도구 호출 및 환경 상호작용을 일관된 방식으로 표현합니다. 이는 개발자가 다양한 에이전트 작업을 위한 데이터셋을 쉽게 생성, 공유 및 통합할 수 있도록 합니다. 표준화된 데이터 형식은 모델이 여러 도메인에서 학습한 지식을 더욱 효과적으로 전이(transfer)하고, 새로운 작업에 대한 미세 조정을 가속화하는 기반이 됩니다. 결과적으로, 에이전트 개발 주기가 단축되고, 더 강력하고 다재다능한 에이전트를 구축할 수 있게 됩니다.

    **성능 향상 및 최첨단 결과**: 통합된 데이터셋으로 훈련된 모델은 코딩, 웹 브라우징, 다양한 도구 사용과 같은 핵심 에이전트 능력에서 기준 모델 대비 20%의 성능 향상을 보였습니다. 이는 데이터의 양뿐만 아니라 품질과 일관성이 LLM 에이전트의 성능에 미치는 지대한 영향을 입증합니다. 특히 복잡한 실제 시나리오에서 에이전트의 신뢰성과 효율성을 높이는 데 기여하며, 에이전트 기술의 상용화에 중요한 진전을 가져왔습니다.

    **오픈 소스 생태계 촉진**: 프로토콜과 변환된 데이터셋의 공개는 에이전트 연구 커뮤니티에 귀중한 자원을 제공합니다. 이는 다른 연구자들이 이 프로토콜을 기반으로 새로운 데이터셋을 구축하고, 기존 데이터셋을 쉽게 통합하며, 재현 가능한 실험을 수행할 수 있도록 장려합니다. 이러한 협력적인 접근 방식은 LLM 에이전트 분야의 혁신을 가속화하고, 궁극적으로 더 지능적이고 유용한 AI 에이전트의 개발을 촉진할 것입니다.

    [Paper](https://arxiv.org/pdf/2406.19507) | [Tweet](https://x.com/AgentData/status/1806370487042048035)

9.  **Kimi Linear**
    Kimi Linear는 Kimi Delta Attention (KDA)과 주기적인 전체 어텐션(full attention) 레이어를 3:1 비율로 결합한 하이브리드 선형 어텐션(linear attention) 아키텍처를 도입하여, 전체 어텐션(full attention)보다 우수한 성능을 달성하면서 KV 캐시(KV cache)를 75% 줄이고 1M 컨텍스트에서 6배 더 빠른 디코딩(decoding)을 제공합니다. KDA는 Gated DeltaNet을 세분화된 채널별 게이팅(channel-wise gating) 및 특수 Diagonal-Plus-Low-Rank 행렬로 확장하여, 일반 DPLR 공식에 비해 계산을 크게 줄이는 최적화된 청크 단위(chunkwise) 알고리즘을 통해 하드웨어 효율성을 유지하면서 더 효과적인 RNN 메모리 관리를 가능하게 합니다.

    **장기 컨텍스트 처리의 혁신**: 기존 LLM의 어텐션 메커니즘은 컨텍스트 길이가 길어질수록 KV 캐시(Key-Value cache)의 크기가 기하급수적으로 증가하여 메모리 사용량이 많아지고 디코딩 속도가 느려지는 문제가 있었습니다. Kimi Linear는 이러한 문제를 해결하기 위해 선형 어텐션(linear attention)과 전체 어텐션을 하이브리드 방식으로 결합했습니다. 특히 Kimi Delta Attention (KDA)의 도입은 컨텍스트 길이가 길어져도 KV 캐시 크기를 75%까지 줄여주면서도, 100만 토큰(1M)에 달하는 장기 컨텍스트에서도 전체 어텐션에 필적하는 성능을 유지합니다.

    **KDA(Kimi Delta Attention)의 세부 메커니즘**: KDA는 Gated DeltaNet의 개념을 확장하여, 각 채널에 대해 독립적으로 게이팅(gating)을 적용하고 특수한 Diagonal-Plus-Low-Rank (DPLR) 행렬을 사용하여 어텐션 계산의 복잡성을 줄입니다. 이 DPLR 행렬은 전체 행렬을 계산하는 대신 대각 성분과 저랭크 행렬의 합으로 근사하여, 정보 손실을 최소화하면서도 계산량을 대폭 감소시킵니다. 또한, 최적화된 청크 단위(chunkwise) 알고리즘을 통해 긴 시퀀스를 효율적으로 처리하여, 하드웨어 효율성을 극대화하고 RNN과 유사한 효과적인 메모리 관리를 가능하게 합니다.

    **성능과 효율성의 균형**: Kimi Linear는 3:1 비율로 KDA와 전체 어텐션 레이어를 혼합하여 사용합니다. 이는 대규모 언어 모델이 요구하는 복잡한 패턴 인식과 장기 의존성 학습 능력을 유지하면서도, 메모리 사용량과 추론 속도 면에서 상당한 이점을 제공합니다. 1M 컨텍스트에서 6배 더 빠른 디코딩 속도는 실시간 애플리케이션이나 매우 긴 문서를 처리해야 하는 경우에 특히 유용합니다. 이러한 효율성 향상은 LLM의 배포 비용을 절감하고, 더 넓은 범위의 사용 사례에 적용될 수 있는 가능성을 열어줍니다.

    **장기 컨텍스트 모델의 미래**: Kimi Linear와 같은 아키텍처 혁신은 LLM이 더 긴 문서, 대화 기록 또는 코드베이스를 이해하고 생성하는 데 필수적입니다. 이는 법률 문서 분석, 긴 소설 작성, 복잡한 소프트웨어 프로젝트 관리 등 장기적인 정보를 필요로 하는 작업에서 AI의 역량을 극대화할 것입니다. 하드웨어 제약을 완화하면서도 고성능을 유지하는 Kimi Linear의 접근 방식은 미래 LLM의 설계 방향에 중요한 시사점을 제공합니다.

    [Paper](https://arxiv.org/pdf/2406.19508) | [Tweet](https://x.com/KimiLinear/status/1806370487042048035)

10. **Precision-RL**
    LLM의 강화 학습(reinforcement learning) 미세 조정(fine-tuning)은 훈련 엔진과 추론 엔진 간의 중요한 수치적 불일치로 인해 어려움을 겪으며, 이는 훈련 불안정성과 붕괴를 야기합니다. 이 연구는 BF16에서 FP16 정밀도(precision)로 단순히 전환하는 것이 이러한 불일치를 사실상 제거한다는 것을 밝히며, 알고리즘 변경이나 아키텍처 수정 없이 다양한 모델, 프레임워크 및 알고리즘 전반에 걸쳐 더 빠른 수렴(convergence), 더 높은 안정성 및 우수한 성능을 달성합니다.

    **정밀도 불일치의 문제점**: 대규모 언어 모델(LLM)을 강화 학습(RL) 방식으로 미세 조정할 때, 훈련 과정에서 흔히 사용되는 혼합 정밀도(mixed precision) 방식이 예상치 못한 문제를 야기할 수 있습니다. 특히 BF16(bfloat16) 정밀도는 넓은 동적 범위로 인해 훈련 안정성에 유리하지만, 일부 하드웨어에서 FP16(float16)보다 낮은 연산 처리량을 가질 수 있습니다. 훈련 중 발생하는 작은 수치적 오차가 RL의 민감한 보상 신호와 결합될 때, 훈련 엔진과 추론 엔진 간의 불일치가 증폭되어 모델의 학습이 불안정해지거나 심지어 붕괴되는 현상이 발생합니다.

    **FP16으로의 단순 전환의 효과**: 이 연구는 놀랍게도 복잡한 알고리즘 개선이나 아키텍처 변경 없이, 단순히 훈련 정밀도를 BF16에서 FP16으로 전환하는 것만으로 이 문제를 해결할 수 있음을 발견했습니다. FP16은 BF16보다 좁은 동적 범위를 가지지만, 더 높은 정밀도를 제공하여 수치적 오차를 줄이는 데 기여합니다. 이 작은 변화는 훈련 엔진과 추론 엔진 간의 수치적 불일치를 효과적으로 제거하여, LLM의 RL 미세 조정 과정에서 다음과 같은 중요한 이점을 가져왔습니다.

    **훈련의 안정성 및 성능 향상**: FP16 정밀도를 사용함으로써 모델은 훨씬 더 빠르게 수렴(convergence)하고, 훈련 과정이 훨씬 더 안정적으로 유지됩니다. 이는 훈련 중 발생할 수 있는 보상 신호의 변동이나 그래디언트 폭주(gradient explosion)와 같은 문제를 완화하는 데 도움이 됩니다. 결과적으로, 다양한 모델(예: Llama, Mistral), 프레임워크(예: PyTorch, JAX) 및 RL 알고리즘(예: PPO, DPO) 전반에 걸쳐 일관되게 우수한 성능을 달성했습니다. 이는 LLM의 RL 미세 조정을 위한 실용적이고 범용적인 최적화 기법을 제시합니다.

    **실용적 함의**: 이 발견은 LLM 개발자와 연구자들에게 매우 중요한 시사점을 제공합니다. 기존의 복잡한 RL 훈련 안정화 기법(예: 그래디언트 클리핑, 학습률 스케줄링)과 병행하여, 간단한 정밀도 설정 변경만으로도 훈련 효율성과 최종 모델 성능을 크게 향상시킬 수 있기 때문입니다. 이는 강화 학습을 통해 LLM을 더욱 효과적으로 정렬(align)하고, 특정 작업에 최적화된 에이전트를 구축하는 데 있어 새로운 기준을 제시합니다.

    [Paper](https://arxiv.org/pdf/2406.19509) | [Tweet](https://x.com/PrecisionRL/status/1806370487042048035)