현재 우리가 인공지능 거품(AI bubble)의 한가운데에 있는지에 대한 궁금증이 커지고 있습니다. 이 글에서는 그에 대한 저의 견해를 명확히 밝히고자 합니다. 결론부터 말하자면, 아직은 아닙니다. 하지만 많은 미디어에서 과거의 경제적 과열 현상들과 AI의 현재 상황을 피상적으로 연결 지으며, '과거에도 이런 일이 있었으니, 지금도 비슷하게 흘러갈 것이다'라는 식의 주장을 펼치곤 합니다. 이는 본질적인 작동 방식(underlying mechanisms)을 깊이 분석하지 않고, 단순히 외형적 유사성만을 가지고 현재를 재단하는 것과 같습니다. 모든 사건을 폭발에 비유하는 것처럼 무의미할 수 있습니다. 우리가 진정으로 탐구해야 할 점은 지금의 추세가 실제로 시장 붕괴(market crash)를 초래할 가능성이 있는지를 면밀히 살펴보는 것입니다.

과거의 철도 산업 거품(railroad industry bubble)을 예로 들어봅시다. 이 사건의 핵심은 미약한 수요에 비해 생산 능력이 지나치게 많았다는 점이었고, 이는 결국 시장 붕괴(market collapse)로 이어지는 심각한 불균형을 초래했습니다. 닷컴 버블(dot-com bubble) 역시 이와 유사한 양상을 보였습니다. 미래의 수요가 빠르게 현실화될 것이라는 비현실적인 낙관론에 기반한 과도한 투자가 그 원인이었죠. 두 사례 모두 결국 수요는 발생했지만, 기대했던 것보다 훨씬 더디게 진행되었습니다. 하지만 현재의 인공지능 생태계는 근본적으로 다른 상황에 놓여 있습니다. 우리는 막대한 수요에 비해 컴퓨팅 자원(computing resources)이 턱없이 부족한 상황에 직면해 있습니다. 이 점을 특히 강조하고 싶습니다. 수많은 기업들이 폭발적인 AI 수요를 감당할 만큼의 충분한 연산 능력(computational power)을 확보하는 데 큰 어려움을 겪고 있습니다. 이는 거품의 전형적인 모습과는 정반대 현상이라고 할 수 있습니다! 바로 이러한 배경 때문에 막대한 자본이 AI 분야로 지속적으로 유입되고 있으며, 관련 기업들의 주가 또한 상승세를 유지하고 있는 것입니다. 따라서 오늘날의 AI 시장을 거품으로 규정하는 것은 적절치 않습니다. 왜냐하면 거품의 필수적인 조건인 공급 과잉(excess supply)과 저조한 수요라는 요소가 전혀 존재하지 않기 때문입니다.

실제로, 최근 몇 년간 전 세계적으로 고성능 GPU(Graphics Processing Unit) 부족 현상이 심화되고 있습니다. 엔비디아(NVIDIA)와 같은 주요 GPU 제조사들은 폭증하는 수요를 따라잡기 위해 생산량을 늘리고 있지만, 최첨단 칩의 리드 타임(lead time)은 여전히 길어지고 있습니다. 마이크로소프트, 구글, 아마존, 메타와 같은 거대 기술 기업들은 AI 인프라 구축에 수십억 달러를 투자하며 자체 데이터센터를 확장하고 있으며, 이는 컴퓨팅 자원이 여전히 희소하고 가치 있는 자산임을 명백히 보여줍니다. 이러한 투자는 단순히 미래에 대한 막연한 기대가 아니라, 현재의 강력한 수요와 미래의 지속적인 성장 가능성에 대한 확신에 기반하고 있습니다. 대규모 언어 모델(LLM), 이미지 생성, 신약 개발, 자율주행, 로봇 공학 등 AI의 응용 분야는 상상 이상으로 빠르게 확장되고 있으며, 각 분야에서 요구하는 컴퓨팅 파워는 기하급수적으로 증가하고 있습니다. 이러한 현실은 AI 산업이 단순한 과열을 넘어선 견고한 성장 동력을 가지고 있음을 방증합니다.

Engineering Prompts는 독자 후원으로 운영되는 출판물입니다. 새로운 게시물을 받고 제 작업을 지원하려면 무료 또는 유료 구독자가 되는 것을 고려해 보세요. 구독하기

그렇다면 현 상황에 대해 전혀 우려할 필요가 없을까요? 당장은 아닐지라도, 미래는 또 다른 양상을 띨 수 있습니다. 다시 한번 핵심 요소인 공급 과잉(overcapacity)과 미약한 수요를 면밀히 검토해 봅시다. 공급 과잉은 수요와 밀접하게 연관되어 있기에, 이 둘은 본질적으로 상호 보완적인 관계에 있습니다. 따라서 논의를 간소화하기 위해 수요에 초점을 맞춰보겠습니다. 인공지능에 대한 요구는 현재 매우 강력하며, 이러한 요구가 앞으로도 계속해서 증대될 것이라는 전망은 피하기 어렵습니다. AI는 이미 엄청난 유용성을 입증했으며, 미래에는 그 가치가 더욱 커질 것이 분명하므로, 이는 자연스럽게 수요의 지속적인 증가를 촉진할 것입니다. 일부에서는 AI의 실제 효용성에 의문을 제기하며 모든 것이 허상에 불과하다고 주장하기도 하지만, 수많은 연구 결과와 사용자 설문조사는 이와 상반되는 일관된 결과를 보여줍니다. 즉, 대다수의 사람들은 AI가 상당한 가치를 지닌다고 인식하고 있습니다. 물론 과장된 홍보나 현재 모델의 실제 역량에 대한 과대평가가 없지 않은 것도 사실입니다. 그러나 결국 사용자들은 오늘날 AI 모델이 제공하는 실제 기능을 직접 경험하게 되며, 이러한 경험은 지속적인 수요를 더욱 공고히 합니다. AI 기술의 발전이 계속될 것이라는 명확한 증거가 있는 만큼, 우리는 수요 또한 꾸준히 증가할 것이라고 확신할 수 있습니다. 그러므로 수요가 지속적으로 늘어나는 한, 현재 이루어지고 있는 컴퓨팅 관련 투자(compute-related investments)는 전적으로 합리적이라고 결론 내릴 수 있습니다. 다시 말해, 우리가 목격하고 있는 것은 공급 과잉이 아니라, 이러한 증대되는 수요를 충족시키기 위한 정상적이고 정당한 인프라 구축 과정입니다. 이러한 논리적 추론 때문에 저는 오늘날 우리가 AI 거품 속에 있다고 주장하기는 어렵다고 생각합니다. 현재의 급격한 성장은 다소 혼란스럽게 보일 수 있으나, 이는 거품을 의미하기보다는 매우 유용한 기술에 의해 추동되는 강력하고 건전한 확장세를 반영하는 것일 뿐입니다. AI가 창출하고, 수요가 높은 GPU에서 새롭게 제공됩니다. 무엇에 대한 수요일까요?

인공지능에 대한 수요는 단순히 특정 모델이나 기술에 국한되지 않습니다. 헬스케어 분야에서는 신약 개발과 진단 정확도 향상에 AI가 필수적이며, 제조 분야에서는 예측 유지보수와 자동화로 생산 효율을 극대화합니다. 금융 분야에서는 사기 탐지 및 알고리즘 트레이딩에, 그리고 창의 산업에서는 새로운 콘텐츠 생성에 핵심적인 역할을 합니다. AI는 단순한 도구를 넘어, 전 산업에 걸쳐 혁신을 이끄는 플랫폼으로 진화하고 있으며, 이는 새로운 시장과 서비스를 창출하며 지속적인 수요를 견인하고 있습니다. 이러한 광범위하고 깊이 있는 영향력은 AI 산업의 장기적인 가치와 성장 잠재력을 확고히 합니다.

그러나 여기서 중요한 통찰이 나옵니다. 대중이 진정으로 원하는 것은 컴퓨팅 하드웨어(compute hardware) 그 자체가 아니라, 인공지능 서비스(AI services)입니다. 현재는 컴퓨팅 하드웨어, 특히 GPU 인프라(GPU infrastructure)와 AI 기술 간에 밀접한 연관성이 존재합니다. 이는 오늘날의 AI 모델들이 방대한 계산 자원(computational resources)을 요구하기 때문입니다. 하지만 이러한 밀접한 연결 고리가 약화된다면 어떻게 될까요? 그러한 상황은 실제로 AI 거품이 형성될 수 있는 전제 조건을 마련할 수 있습니다. 극단적인 가상 시나리오를 상상해봅시다. 현재 필요한 컴퓨팅 자원의 1%만을 사용하여 최고 수준의 AI 모델 성능을 제공할 수 있는 혁신적인 AI 아키텍처(AI architecture)가 개발되었다고 가정해 보세요. AI에 대한 수요는 여전히 강력하게 유지될 것이며, 비용 절감 효과로 인해 오히려 증가할 수도 있겠지만 (이것이 바로 유명한 '제본스 역설(Jevons Paradox)'입니다), 막대한 투자를 통해 구축된 거대한 GPU 팜(GPU farm)을 완전히 활용하기에는 역부족일 것입니다. 갑작스럽게, AI 서비스 자체에 대한 높은 수요가 지속됨에도 불구하고, 관련 인프라(infrastructure)의 99%가 하루아침에 불필요해지는 상황이 발생할 수 있습니다. 그러한 막대한 잉여 인프라(surplus infrastructure)를 정당화할 만큼 수요를 충분히 빠르게 확장하는 것은 불가능합니다. 이 시나리오에서는 '제본스 역설(Jevons Paradox)'로 인한 수요 증가가 과잉 공급을 상쇄하지 못할 것입니다. 바로 이러한 지점에서 거품의 조건, 즉 컴퓨팅 자원 과잉 공급(overcapacity of compute resources)과 그에 대한 수요 부족이 현실화될 것입니다.

이러한 컴퓨팅 효율성(compute efficiency) 증대는 단순히 매개변수(parameter) 수를 줄이는 것을 넘어, 에너지 효율성(energy efficiency), 데이터 효율성(data efficiency, 즉 소량의 데이터로 학습), 추론 효율성(inference efficiency), 학습 효율성(training efficiency) 등 다양한 측면에서 이루어질 수 있습니다. 양자화(quantization), 가지치기(pruning), 지식 증류(distillation), 희소 모델(sparse models)과 같은 소프트웨어적 최적화 기법뿐만 아니라, GPU를 넘어선 TPU(Tensor Processing Unit), NPU(Neural Processing Unit), 그리고 특정 AI 작업에 최적화된 맞춤형 ASIC(Application-Specific Integrated Circuit)과 같은 하드웨어 가속기(hardware accelerators)의 발전도 효율성 향상에 크게 기여하고 있습니다. 이러한 기술 발전은 궁극적으로 AI 모델을 클라우드 데이터센터뿐만 아니라, 스마트폰, 엣지 디바이스(edge devices)와 같은 분산된 환경에서도 구동 가능하게 만들며, 이는 AI 서비스의 접근성을 획기적으로 높이고 새로운 활용 사례를 창출할 것입니다.

과연 이러한 극단적인 컴퓨팅 효율성(compute efficiency) 증대가 현실적으로 가능할까요? 100배에 달하는 효율성 개선은 다소 과장된 것처럼 들릴 수 있습니다. 그러나 한 가지 중요한 사실을 상기해 봅시다. 우리는 현재의 AI 모델들이 왜 그토록 뛰어난 성능을 발휘하는지 아직 완벽하게 이해하지 못하고 있습니다. 우리는 모델을 '성장'시키고 그 작동을 관찰할 수 있지만, 그 메커니즘을 정확히 파악하고 있지는 않습니다. 물론 스케일링 법칙(scaling laws)이나 일부 회로 수준(circuit-level)의 동작에 대한 이해는 있지만, 여전히 근본 원리(first principles)에 기반하여 시스템을 공학적으로 설계하기보다는, 반복적인 실험을 통해 발전시키고 있는 단계입니다. 이는 아키텍처적 혁신(architectural innovation)을 통해 효율성을 극대화할 수 있는 엄청난 여지가 있음을 시사합니다. 실제로 급진적인 효율성 향상에 대한 조짐은 이미 나타나고 있습니다. Giotto.ai 사례를 들어보겠습니다. EPFL AI 센터(EPFL AI Center)의 'Inside AI' 팟캐스트(podcast)에서 CEO 알도 포데스타(Aldo Podestà)는 그들의 모델이 단 2억 개의 매개변수(parameters)(b가 아닌 m)만으로 현재 ARC-AGI 2 리더보드(leaderboard)에서 최상위권 경쟁자 중 하나로 자리매김하고 있다고 설명했습니다. 이는 올바른 아키텍처적 선택이 몇 배 더 높은 효율성을 가져올 수 있다는 강력한 증거입니다. 물론 이러한 모델들은 최신 대규모 언어 모델(LLM)처럼 인터넷 전체를 '기억'하지는 않을 것입니다. 하지만 어쩌면 그럴 필요가 없을 수도 있습니다. 이 점을 뒷받침하기 위해 OpenAI 공동 창립자인 안드레이 카르파티(Andrej Karpathy)는 인공 일반 지능(AGI)의 '인지 핵심(cognitive core)'이 10억 개의 매개변수(parameters)로도 구동될 수 있다고 제안했지만, 그는 이 역시 최소 20년은 더 걸릴 것이라고 내다봤습니다. 그리고 자연계에는 이미 개념 증명(proof of concept)이 존재합니다. 인간의 뇌는 약 20와트(watts)의 전력으로 일반 지능(general intelligence)을 구현합니다. 분명히, 훨씬 더 효율적인 방식으로 작업을 처리할 가능성은 무궁무진합니다.

이러한 효율성 향상은 AI 개발자들에게는 더 적은 비용으로 더 큰 모델을 학습시킬 수 있는 기회를 제공하여 AI 개발의 민주화를 가속화할 것입니다. 클라우드 서비스 제공업체에게는 단순한 컴퓨팅 자원 판매에서 벗어나 고부가가치 AI 서비스 제공으로 비즈니스 모델을 전환해야 하는 과제를 안겨줄 것입니다. 하드웨어 제조업체는 범용 GPU를 넘어선 새로운 전문화되고 효율적인 하드웨어 개발에 박차를 가해야 할 것입니다. 그리고 최종 사용자들은 더 저렴하고, 접근하기 쉬우며, 빠른 AI 서비스를 경험하게 될 것입니다. 만약 효율성 향상이 너무 급격하게 이루어져 기존 인프라에 대한 수요가 따라가지 못한다면, AI 자체의 유용성에도 불구하고 일시적인 투자 감소 기간, 즉 'AI 겨울(AI winter)'이 찾아올 수도 있습니다. 이는 거품 붕괴와는 다르지만, 관련성이 깊은 현상으로, 기술 발전의 속도와 시장의 적응 능력 사이의 균형이 중요함을 시사합니다.

물론 이러한 변화가 하룻밤 사이에 갑작스럽게 일어날 가능성은 낮습니다. 만약 우리가 매년 10배에 달하는 효율성(efficiency) 개선을 지속적으로 목격한다면, 현재의 컴퓨팅 투자(compute investments)는 여전히 합리적인 수준을 유지하거나, 최소 3년에서 5년에 걸쳐 (이는 현재 인프라의 일반적인 예상 수명과도 일치합니다) 점진적으로 그 가치를 잃어갈 수 있습니다. 이는 여전히 일종의 '거품 붕괴'로 볼 수 있겠지만, 그 속도가 훨씬 더 완만하게 진행될 것입니다. 이 경우, 우리는 이를 단순한 거품이라기보다는 기술 산업의 일반적인 성장과 성숙 주기(growth and maturity cycle)의 일부로 해석할 수도 있습니다. 중요한 것은 더 효율적인 접근 방식이 존재하는지 여부가 아닙니다. 그러한 방식은 분명히 존재합니다. 관건은 언제 누가 그러한 혁신적인 방법을 발견하고, 그 발견이 한 번에 2배, 10배, 아니면 100배에 달하는 효율성 향상을 가져올 것인지에 달려 있습니다.

미래의 불확실성에 대비하기 위해 기업들은 AI 투자 포트폴리오를 다각화하고, 소프트웨어 효율성 향상에 집중하며, 유연한 인프라를 구축하고, 차세대 아키텍처 연구 개발에 적극적으로 투자해야 합니다. 정책 입안자들은 에너지 효율적인 AI 연구를 장려하고, 효율성 벤치마크(benchmark)를 표준화하여 투명성을 높이는 데 기여할 수 있습니다. 또한, 오픈소스 AI(open-source AI)의 역할은 효율성 증대와 AI 접근성 민주화에 더욱 중요해질 것입니다. 이러한 다각적인 노력을 통해 우리는 AI 기술의 건전하고 지속 가능한 성장을 도모할 수 있을 것입니다.

한 가지 더, 제가 책을 냈습니다! 이 서브스택(substack) 독자들은 제가 AI에 대한 일반 독자용 책을 작업해 왔다는 것을 알고 계실 것입니다. 프랑스어 버전이 현재 이용 가능하며, 독일어 버전은 11월 중순에 출시될 예정임을 기쁘게 알려드립니다. 저는 현재 영어 번역 작업을 하고 있지만, 먼저 영어 출판사를 찾아야 할 것입니다. 어쨌든, 프랑스어나 독일어를 하신다면 한번 살펴보세요. 분명히 즐거우실 겁니다!

또 다른 한 가지, 여러분은 제가 AMLD라는 대규모 응용 기계 학습(applied machine learning) 행사의 주최자라는 것을 아마도 알고 계실 것입니다. 10회째를 맞아 저희는 이 행사를 AMLD 인텔리전스 서밋(AMLD Intelligence Summit)으로 이름을 변경했습니다. 이 행사는 2026년 2월 10일부터 12일까지 EPFL 캠퍼스(campus)에서 개최될 예정이며, 프로그램은 정말 환상적일 것입니다. 절대 놓치고 싶지 않으실 겁니다. 짐작하시겠지만, 기술 학교와의 근접성은 과장이나 허튼소리에 대한 허용 오차(tolerance)가 매우 낮다는 것을 보장하며, 이는 고신호 저잡음(high-signal, low-noise) AI 행사로 만듭니다. 얼리버드 티켓(Early bird tickets)은 한정된 기간 동안 이용 가능하며, 다가오는 정식 티켓 가격보다 훨씬 저렴하므로 서둘러 확보하시기를 강력히 권해드립니다.

코다(CODA) 이 뉴스레터(newsletter)는 두 가지 구독 유형을 제공합니다. 유료 버전으로 전환하시기를 강력히 권해드립니다. 모든 콘텐츠는 무료로 유지되지만, 모든 재정적 지원은 EPFL AI 센터(EPFL AI Center) 관련 활동에 직접적으로 자금을 지원합니다. 연락을 유지하려면 저를 찾을 수 있는 다른 방법들이 있습니다. 소셜(Social): 저는 주로 링크드인(LinkedIn)에 있지만, 마스토돈(Mastodon), 블루스카이(Bluesky), 그리고 X에도 있습니다. 팟캐스팅(Podcasting): 저는 EPFL AI 센터(EPFL AI Center)에서 "Inside AI"라는 AI 팟캐스트(podcast)를 진행하고 있으며(애플 팟캐스트(Apple Podcasts), 스포티파이(Spotify), 유튜브(YouTube)), 저보다 훨씬 똑똑한 분들과 이야기할 수 있는 특권을 누리고 있습니다. Engineering Prompts는 독자 후원으로 운영되는 출판물입니다. 새로운 게시물을 받고 제 작업을 지원하려면 무료 또는 유료 구독자가 되는 것을 고려해 보세요. 구독하기