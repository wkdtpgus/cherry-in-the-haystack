대규모 언어 모델(LLM)은 사용 편의성 덕분에 광범위한 대중의 이목을 끌었습니다. 텍스트 기반의 지시(prompt)를 입력하는 것만으로도, 딥러닝에 대한 전문 지식이 없는 사람들도 방대한 신경망의 역량을 활용하여 복잡한 문제들을 신속하게 해결할 수 있게 되었습니다. 시간이 흐르면서 이 모델들은 향상된 지시 이해 능력과 정렬(alignment)을 통해 더욱 접근하기 쉬워졌습니다. 하지만 LLM에 효율적으로 프롬프트를 제공하는 것은 일종의 예술이자 동시에 과학적인 접근이 필요합니다. 프롬프팅(prompting) 방식이나 전략에 미묘한 변화만 주어도 성능에 현저한 개선을 가져올 수 있습니다. 본 개요는 프롬프트 엔지니어링(prompt engineering)의 기초 개념부터 최근 몇 달간 제안된 최첨단 기법에 이르기까지 포괄적인 이해를 돕고자 합니다.

프롬프트 엔지니어링이란 무엇인가요?
LLM이 이처럼 대중적인 인기를 얻게 된 핵심 요인 중 하나는 텍스트-투-텍스트(text-to-text) 형식의 인터페이스 덕분에 사용법이 매우 직관적이라는 점입니다. 기존 딥러닝 방식에서는 특정 작업을 해결하려면 모델이 해당 작업을 학습하도록 일부 데이터를 사용하여 미세 조정(finetune)하는 과정이 필수적이었습니다. 또한, 대부분의 모델은 특정 기능에 특화된 좁은 전문가에 불과했습니다. 그러나 LLM의 혁신적인 인컨텍스트 학습(in-context learning) 능력 덕분에 이제는 텍스트 프롬프트만으로 다양한 문제들을 해결할 수 있습니다. 과거에는 복잡했던 문제 해결 과정이 자연어(natural language) 형태로 추상화되어 간편해진 것입니다!

"프롬프트 엔지니어링은 다양한 응용 분야와 연구 영역에서 언어 모델(LM)을 효율적으로 활용하기 위한 프롬프트를 개발하고 최적화하는 비교적 새로운 학문 분야입니다." - [1]에서 발췌

프롬프트 엔지니어링이란 무엇인가요?
LLM의 단순성은 그 활용을 대중화했습니다. 데이터 과학자나 머신러닝 엔지니어(MLE)가 아니더라도 누구나 LLM을 사용할 수 있습니다. 한국어를 이해한다면 (또는 선택한 언어를 이해한다면) LLM으로 비교적 복잡한 문제도 해결할 수 있습니다! 하지만 LLM으로 문제를 해결할 때 얻는 결과는 모델에 제공되는 텍스트 프롬프트에 크게 좌우됩니다. 이러한 이유로, LLM의 성능을 최적화하기 위해 다양한 프롬프트를 시험하는 경험적 과학(empirical science)인 프롬프트 엔지니어링은 매우 인기 있고 영향력 있는 분야가 되었으며, 수많은 기법과 모범 사례(best practices)의 발견으로 이어졌습니다.

프롬프트 구성 요소 이해하기
LLM에 지시를 내리는 방식은 다양하지만, 대부분의 프롬프팅 전략은 몇 가지 공통적인 구성 요소를 공유합니다.
*   **입력 데이터(Input Data)**: LLM이 처리할 것으로 예상되는 실제 정보 (예: 번역하거나 분류할 문장, 요약할 문서 등).
*   **예시(Exemplars)**: 프롬프트 내에 포함된 올바른 입력-출력 쌍의 구체적인 본보기.
*   **지시(Instruction)**: 모델에서 기대되는 결과물에 대한 텍스트 설명.
*   **지표(Indicators)**: 프롬프트 내에 구조를 형성하는 데 사용되는 태그(tag) 또는 서식 요소(formatting elements). (위 참조)
*   **컨텍스트(Context)**: 프롬프트에서 LLM에 제공되는 모든 추가 정보.

아래 그림에서 문장 분류를 위한 단일 프롬프트 내에 위에서 언급된 모든 프롬프트 구성 요소를 결합한 예시를 볼 수 있습니다.

LLaMA-2 스타일의 모든 구성 요소를 포함한 프롬프트 (여기 참조)

컨텍스트 윈도우(context window). 사전 학습(pretraining) 과정에서 LLM은 특정 길이의 입력 시퀀스(input sequence)를 처리합니다. 이때 선택된 시퀀스 길이가 모델의 "컨텍스트 길이(context length)" 또는 모델이 처리할 수 있는 최대 시퀀스 길이가 됩니다. 미리 정해진 컨텍스트 길이보다 훨씬 긴 텍스트 시퀀스가 주어지면 모델은 예측 불가능하게 작동하여 부정확한 출력을 생성할 수 있습니다. 하지만 Self-Extend 또는 위치 보간(positional interpolation)과 같은 방법들을 사용하여 모델의 컨텍스트 윈도우를 확장할 수 있습니다.

RoPE를 사용한 위치 보간(positional interpolation) 그림 ([34]에서 발췌)

LLM에 대한 최근 연구는 긴 컨텍스트 윈도우(long context windows) 생성에 중점을 두었으며, 이를 통해 모델은 각 프롬프트 내에서 더 많은 정보(예: 더 많은 예시 또는 더 많은 컨텍스트)를 처리할 수 있습니다. 그러나 보시다시피 모든 LLM이 컨텍스트에 완벽하게 주의를 기울이는 것은 아닙니다! LLM이 긴 컨텍스트 윈도우 내의 정보를 활용하는 능력은 일반적으로 "건초 더미 속 바늘 찾기 테스트(needle in the haystack test)"를 통해 평가됩니다. 이 테스트는 i) 컨텍스트 내에 무작위 사실을 삽입하고, ii) 모델에 사실을 검색하도록 요청하며, iii) 다양한 컨텍스트 길이와 컨텍스트 내 사실의 위치에 대해 이 테스트를 반복합니다. 이러한 테스트는 아래 그림과 같은 결과를 보여주며, 컨텍스트 윈도우의 결함을 쉽게 발견할 수 있습니다.

(출처)

나의 프롬프트 엔지니어링 전략. 프롬프트 엔지니어링의 상세한 접근 방식은 사용되는 모델에 따라 크게 달라집니다. 그러나 프롬프트 엔지니어링 과정을 이끌어가는 데 유용한 몇 가지 일반적인 원칙들이 있습니다.
*   **경험적 접근(Be empirical)**: 프롬프트 엔지니어링의 첫 단계는 프롬프트 변경 사항을 쉽게 평가할 수 있는 신뢰할 수 있는 방법(예: 테스트 케이스(test cases), 인간 평가자(human evaluators) 또는 LLM-as-a-judge 활용)을 마련하는 것입니다.
*   **간단하게 시작(Start simple)**: 처음 시도하는 프롬프트는 연쇄 사고(chain-of-thought) 프롬프트(또는 다른 특수 프롬프팅 기술)가 아니어야 합니다. 가장 단순한 프롬프트로 시작하여, 성능 변화를 측정하면서 천천히 복잡성을 추가하여 추가적인 복잡성이 필요한지 판단해야 합니다. 1 (위 참조)
*   **구체적이고 명확하게(Be specific and direct)**: 프롬프트의 모호성을 제거하고, LLM이 생성하기를 원하는 결과물을 설명할 때 간결하고 직접적이며 구체적으로 표현하려고 노력해야 합니다.
*   **예시 활용(Use exemplars)**: 원하는 결과물을 설명하기 어렵다면, 프롬프트에 몇 가지 예시를 추가해 보세요. 예시는 LLM에게 기대하는 바에 대한 구체적인 본보기를 제공하여 모호함을 줄여줍니다.
*   **복잡성 지양(가능하다면)(Avoid complexity (if possible))**: 복잡한 프롬프팅 전략이 때로는 필수적일 수 있지만(예: 다단계 추론 문제 해결), 그러한 접근 방식을 사용하기 전에 신중하게 고려해야 합니다. 경험적 접근 방식을 취하고 확립된 평가 전략을 사용하여 복잡성이 진정으로 필요한지 판단해야 합니다.

위 내용을 요약하자면, 저의 개인적인 프롬프트 엔지니어링 전략은 i) 탁월한 평가 프레임워크(evaluation framework)에 투자하고, ii) 단순한 프롬프트로 시작하며, iii) 원하는 수준의 성능을 달성하기 위해 필요에 따라 점진적으로 복잡성을 추가하는 것입니다. 프롬프트 작성은 반복적인 과정입니다!

프롬프팅 기술(Prompting Techniques)
우리는 이전에 일련의 관련 개요를 통해 다양한 프롬프팅 기술에 대해 배웠습니다.
*   실용적인 프롬프트 엔지니어링(Practical Prompt Engineering) [링크]
*   고급 프롬프트 엔지니어링(Advanced Prompt Engineering) [링크]
*   연쇄 사고 프롬프팅(Chain of Thought Prompting) [링크]
*   프롬프트 앙상블(Prompt Ensembles) [링크]

이제 관련 프롬프팅 기술을 다시 한번 개괄하여, 이 게시물에서 나중에 소개될 더 복잡한 접근 방식의 토대를 제공할 것입니다. 그러나 이러한 각 기술에 대해 배우면서 프롬프트 엔지니어링에서 단순성의 중요성을 명심해야 합니다. 프롬프팅 기술이 더 복잡하거나 정교하다고 해서 더 간단한 전략보다 더 낫다는 의미는 아닙니다!

기본 프롬프팅 전략(Basic Prompting Strategies) ([3]에서 발췌)
제로샷 프롬프팅(Zero-shot prompting) (위 그림 참조)은 GPT-2 [2]에 의해 대중화되었으며, 우리가 사용할 수 있는 가장 기본적인 프롬프팅 전략 중 하나입니다. 제로샷 프롬프팅을 통해 작업을 해결하려면, i) 프롬프트에 작업을 설명하고 ii) 모델에 문제를 해결하도록 프롬프트를 제공하기만 하면 됩니다. 위 문제의 경우, 작업은 영어를 프랑스어로 번역하는 것이며, 우리는 "cheese =>"라는 문자열을 통해 모델에 이 번역을 수행하도록 프롬프트를 제공합니다. 이는 모델이 'cheese'라는 단어의 프랑스어 번역을 출력하도록 유도합니다. 아래에 제로샷 프롬프트의 몇 가지 예시가 제공됩니다.

제로샷 학습(Zero-shot learning) (GPT-3.5-Turbo로 생성된 출력)
제로샷 학습이 어떤 경우에는 잘 작동하지만, 작업 설명의 모호성으로 인해 한계가 있습니다. 성능은 명확하고 포괄적인 설명 생성에 달려 있으며, 우리는 이 설명만으로 모델이 올바른 출력을 생성하는 능력에 의존합니다. 종종 프롬프트에 더 구체적인 정보를 삽입함으로써 더 나은 성능을 달성할 수 있습니다.

([3]에서 발췌)
퓨샷 프롬프팅(Few-shot prompting)은 프롬프트에 올바른 문제 해결 예시를 여러 개 삽입하여 정확히 이 작업을 수행합니다. 이 전략은 GPT-3 [3]에 의해 대중화되었으며, GPT-3는 LLM이 대규모로 인상적인 퓨샷 학습(few-shot learning) 능력을 개발한다는 것을 보여주었습니다. (위 참조) 직관적으로 퓨샷 학습은 예상 출력의 여러 예시를 제공함으로써 제로샷 학습의 모호성을 제거합니다. 따라서 모델은 작업 설명에서 원하는 동작을 추론하는 대신 이러한 예시로부터 올바른 동작을 직접 이해할 수 있습니다. (아래 참조)

([3]에서 발췌)
LLM은 프롬프트 내에 제공된 이러한 예시로부터 학습할 수 있으며, 이는 일반적으로 "인컨텍스트 학습(in-context learning)"이라고 불리는 전략입니다. (아래 참조) 그러나 이러한 학습 방식은 신경망(neural network)의 일반적인 훈련과는 다릅니다. 모델의 매개변수(parameters)는 전혀 수정되지 않습니다. 대신, 우리는 프롬프트에 관련 정보를 넣고, 모델은 이 정보를 더 나은 출력을 생성하기 위한 컨텍스트(context)로 사용할 수 있습니다.

([3]에서 발췌)
실제로 퓨샷 학습을 사용할 때 적절하게 조정해야 하는 두 가지 주요 설정이 있습니다.
*   사용할 예시의 수.
*   예시 선택 전략.

사용할 올바른 예시 수를 결정하기 위해 평가 세트(evaluation set)를 사용하여 기본적인 하이퍼파라미터 튜닝(hyperparameter tuning)을 수행할 수 있습니다. 많은 논문에서 예시 선택 전략(예: 무작위 선택(random selection), 다양성(diversity), 의미론적 유사성(semantic similarity), 능동 학습(active learning) 또는 더 복잡한 지표(metrics) 기반) 2 을 탐구했습니다. 그러나 예시의 무작위 선택은 실제로 종종 효과적인 전략입니다. 이러한 전략 외에도 퓨샷 학습과 관련된 다양한 실용적인 규칙과 발견 사항이 있으며, 우리는 항상 이를 염두에 두어야 합니다 [4, 5].
*   예시의 레이블(label) 분포는— 비록 부정확하더라도 —모델이 일반적인 레이블에 편향되어 있기 때문에 모델의 답변에 영향을 미칠 수 있습니다.
*   답변은 프롬프트에서 최근에 관찰된 예시에 편향됩니다. 3
*   프롬프트 내 예시의 서식(formatting)이 중요합니다.
*   예시를 무작위로 선택하면 모델이 생성한 답변 내의 편향(예: 위치 또는 다수 레이블 편향)을 제거하는 데 도움이 될 수 있습니다.

단순함에도 불구하고 퓨샷 학습은 가장 효과적인 프롬프팅 전략 중 하나이며 실제 응용 분야에서 널리 사용됩니다.

지시 프롬프트의 몇 가지 예시 ([6]에서 발췌)
지시 프롬프팅(Instruction prompting)은 LLM의 원하는 출력을 표현하는 더 직접적인 방법입니다. 퓨샷 학습에서는 해결되는 작업의 구체적인 예시를 통해 모델에 우리의 의도를 설명하지만, 이러한 예시는 많은 토큰(token)을 소비합니다! 모델에 우리의 의도를 말로 설명하는 것이 훨씬 더 효율적일 것입니다. 이것이 잘 작동하려면 사용되는 LLM이 지시를 일관되게 따르도록 정렬(aligned)되어야 합니다. 이러한 모델은 제공된 상세한 지시를 이해하고 그에 따라 출력을 조정할 수 있기 때문에 "조종 가능(steerable)"하다고 합니다.

([6]에서 발췌)
LLM에 대한 연구는 지시 따르기 능력(instruction following capabilities) 개선에 크게 집중해 왔습니다. 사전 학습된 LLM은 기본적으로 지시를 잘 따르지 못합니다. 그러나 InstructGPT [6]가 보여주듯이, 우리는 지도 미세 조정(supervised finetuning, SFT)과 인간 피드백 기반 강화 학습(reinforcement learning from human feedback, RLHF)의 조합을 통해 모델이 지시를 훨씬 더 잘 따르도록 정렬할 수 있습니다. 위 그림에서 이 전략이 지시 따르기뿐만 아니라 LLM의 다른 주요 속성(예: 사실성(factuality) 및 제약 조건 따르기(constraint following))도 개선할 수 있음을 알 수 있습니다.

LaMDA를 사용한 역할 프롬프팅(Role prompting) ([8]에서 발췌)
LLM 정렬의 최근 발전으로 인해, 퓨샷 프롬프팅 [7]과 결합될 수도 있는 지시 프롬프팅은 실제 응용 분야에서 일반적으로 사용되는 매우 효과적인 접근 방식입니다. 사실, 몇 가지 인기 있는 프롬프팅 전략(예: 역할 프롬프팅(role prompting), 대상 지정(specifying an audience), 도구 사용(tool usage) 등)은 지시 프롬프팅의 더 구체적인 버전일 뿐입니다! 지시를 작성할 때는 최상의 결과를 보장하기 위해 명확하고 정확해야 합니다. 또한, 시스템 프롬프트(system prompt)를 활용하여 모델의 전반적인 행동 양식을 설정하거나, 특정 페르소나(persona)를 부여하여 일관된 응답 스타일을 유도하는 것이 중요합니다. 이는 단순한 지시보다 더 깊은 수준의 제어를 가능하게 합니다.

고급 프롬프팅 전략(Advanced Prompting Strategies)
위에서 설명한 프롬프팅 기술은 매우 효과적이지만, 때로는 더 복잡한 프롬프트가 어려운 문제(예: 수학/코딩 또는 다단계 추론 문제)를 해결하는 데 유용할 수 있습니다. LLM은 이러한 문제에 본질적으로 어려움을 겪기 때문에 4 (즉, 추론 능력은 모델 규모에 따라 단조롭게 향상되지 않습니다 [9]), 프롬프트 엔지니어링에 대한 기존 연구의 대부분은 추론 및 복잡한 문제 해결 능력 향상에 중점을 둡니다. 간단한 프롬프트는 대부분의 다른 문제를 해결하는 데 작동할 것입니다.

([10]에서 발췌)
연쇄 사고(Chain of Thought, CoT) 프롬프팅 [10]은 모델의 프롬프트 내 예시에 연쇄 사고(즉, 일련의 중간 추론 단계)를 삽입하여 LLM의 추론 능력(reasoning capabilities)을 이끌어냅니다. (위 참조) 각 예시에 연쇄 사고를 추가함으로써 모델은 (인컨텍스트 학습을 통해) 해당 문제에 대한 최종 답변을 출력하기 전에 유사한 연쇄 사고를 생성하는 방법을 학습합니다. 흥미롭게도 [10]에서 충분히 큰 모델(즉, 1000억 개 이상의 매개변수)은 산술, 상식 및 기호 추론 작업에서 이 접근 방식으로부터 큰 이점을 얻는다는 것을 알 수 있습니다. 문제를 해결하기 위한 근본적인 추론 과정을 명시적으로 설명하는 것이 실제로 모델의 추론 능력을 더 효과적으로 만듭니다.

([10]에서 발췌)
CoT 프롬프팅의 구현은 간단합니다. 각 퓨샷 예시가 입력과 출력만 가지는 대신, 예시는 (입력, 연쇄 사고, 출력) 형식의 삼중항(triplet)입니다. (위 참조) 이 접근 방식의 주요 단점은 문제 해결을 위한 완전한 근거(rationale)를 포함하는 예시를 수동으로 (또는 인위적으로) 큐레이션(curate)해야 한다는 점인데, 이는 비용이 많이 들거나 시간이 많이 소요될 수 있습니다. 따라서 많은 논문이 CoT 프롬프팅이 사람이 작성한 근거에 의존하는 것을 제거하는 데 중점을 둡니다!

([11, 12]에서 발췌)
CoT 변형(CoT variants). CoT 프롬프팅의 효과와 인기로 인해 이 접근 방식의 수많은 확장 기능이 제안되었습니다. 예를 들어, 제로샷 CoT(zero-shot CoT) [11] 프롬프팅은 퓨샷 예시를 제거하고 대신 프롬프트 끝에 "단계별로 생각해 봅시다(Let’s think step by step)."라는 단어를 추가하여 모델이 문제 해결 근거를 생성하도록 장려합니다. 또한 i) 문제를 해결할 때 여러 연쇄 사고를 독립적으로 생성하고 ii) 각 연쇄 사고로 생성된 최종 답변의 다수결 투표(majority vote)를 통해 추론 과정의 견고성(robustness)을 향상시킬 수 있습니다. 5 문제 해결 비용을 증가시키지만, 자기 일관성(self-consistency) [12]이라고 불리는 이 접근 방식은 더 복잡한 종류의 추론 문제를 해결할 때 LLM의 신뢰성(reliability)을 향상시킵니다.

([13]에서 발췌)
최소-최대 프롬프팅(Least-to-most prompting) [13]은 복잡한 문제를 여러 부분으로 명시적으로 분해함으로써 CoT 프롬프팅을 넘어섭니다. (위 참조) 각 하위 문제(sub-problem)는 개별적으로 해결되며, 각 하위 문제의 해결책은 다음 하위 문제를 해결하기 위한 컨텍스트로 전달됩니다. 최종 하위 문제에 도달하면 이전 해결책의 컨텍스트를 사용하여 질문에 대한 최종 답변을 출력할 수 있습니다.

"LLM의 이러한 모든 발전의 근간에는 여전히 토큰(token) 수준의 결정을 하나씩 왼쪽에서 오른쪽으로 생성하는 원래의 자기회귀(autoregressive) 메커니즘이 있다는 점은 놀라울 수 있습니다." - [14]에서 발췌

사고의 나무(Tree of thoughts, ToT) 프롬프팅 [14]. CoT 프롬프팅과 같은 기술은 다음 토큰 예측(next-token prediction)을 사용하여 단일 시도에서 해결책을 출력하는 왼쪽-오른쪽 생성(left-to-right generation) 접근 방식을 따릅니다. 이러한 접근 방식은 특정 시나리오에서는 효과적이지만, 광범위한 계획, 전략적 미리 보기(strategic lookahead), 백트래킹(backtracking), 그리고 수많은 실행 가능한 해결책의 병렬 탐색(exploration)으로부터 이점을 얻을 수 있는 복잡한 문제를 해결하는 데 실패할 수 있습니다. 여기에 ToT 프롬프팅이 등장합니다! ToT 프롬프팅은— 최소-최대 프롬프팅 [13]과 다소 유사하게 —복잡한 문제를 개별적으로 해결할 수 있는 일련의 더 간단한 문제(또는 "사고(thoughts)")로 분해합니다.

([14]에서 발췌)
CoT 프롬프팅과 달리 ToT 프롬프팅은 문제를 해결할 때 단일 사고 경로를 따를 것을 요구하지 않습니다. 또한 ToT 프롬프팅은 자기 일관성처럼 여러 추론 경로의 다수결 투표를 단순히 취하지 않습니다. (위 참조) 탐색(exploration) 과정에서 LLM은 많은 사고를 생성하고 자연어를 통해 최종 해결책을 향한 진행 상황을 지속적으로 평가합니다(즉, 우리는 모델에 프롬프트를 제공하기만 하면 됩니다!). 모델이 최종 해결책을 향한 자체 진행 상황을 자체 평가하는 것을 활용함으로써, 우리는 널리 사용되는 탐색 알고리즘(예: 너비 우선 탐색(breadth-first search) 또는 깊이 우선 탐색(depth-first search))으로 탐색 프로세스를 강화하여 문제 해결 과정 내에서 미리 보기와 백트래킹을 수행할 수 있습니다. ToT 프롬프팅에 대한 더 자세한 설명은 이 개요를 참조하세요.

([35]에서 발췌)
사고 그래프(Graph of Thoughts, GoT) 프롬프팅 [35, 36]. 후속 연구는 ToT 프롬프팅에 대한 연구를 추론을 위한 그래프 기반 전략으로 일반화했습니다. 전반적으로 이러한 기술은 ToT 프롬프팅과 유사하지만, 최종 해결책을 생성하는 데 사용되는 사고 경로가 선형(linear)이라는 가정을 하지 않습니다. 오히려 해결책을 도출할 때 사고를 재사용하거나 여러 사고의 시퀀스를 통해 재귀(recurse)할 수도 있습니다. (위 참조) 여러 그래프 기반 프롬프팅 전략이 제안되었습니다 (자세한 내용은 여기 참조) [35, 36]. 그러나 이러한 프롬프팅 기술은— ToT 프롬프팅과 마찬가지로 —실용성이 부족하다는 비판을 받아왔습니다. 즉, GoT 프롬프팅으로 추론 문제를 해결하려면 LLM으로부터 엄청난 수의 추론 단계(inference steps)가 필요할 수 있습니다!

기본 RAG 파이프라인(RAG pipeline)
검색 증강 생성(Retrieval Augmented Generation, RAG) [37] (위 그림 참조)은 순수한 프롬프팅 기술은 아니지만, 프롬프트에 포함할 관련 컨텍스트를 검색하여 LLM 출력의 품질을 향상시키는 널리 사용되는 전략입니다. 유용한 컨텍스트를 검색하기 위해 기존 검색 기술을 사용할 수 있습니다. 예를 들어, 순수 벡터 검색(pure vector search) 또는 하이브리드 검색 엔진(hybrid search engine)입니다. 단순함에도 불구하고 연구에 따르면 RAG는 LLM에 지식을 주입하고 모델이 생성하는 환각(hallucination)의 수를 줄이는 데 매우 효과적입니다 [38]. 또한 RAG에 의해 검색된 관련 문서를 단순히 노출함으로써 LLM 사용자에게 쉽게 인용(citation)을 제공할 수 있습니다. 그러나 데이터를 처리하고 검색하는 방식과 프롬프트에 삽입되는 컨텍스트를 구성하는 방식은 성능에 상당한 영향을 미칠 수 있습니다. 자세한 내용은 여기를 참조하세요. RAG는 외부 지식에 대한 접근을 통해 LLM의 신뢰성과 정확성을 높이는 데 핵심적인 역할을 하지만, 검색된 정보의 적시성, 관련성, 그리고 프롬프트 내에서의 컨텍스트 길이 관리와 같은 실제 구현상의 문제점들도 존재합니다.

([39]에서 발췌)
생성된 지식 프롬프팅(Generated knowledge prompting) [39]은 외부 데이터베이스에서 컨텍스트를 검색하는 대신 LLM을 사용하여 프롬프트에 포함할 관련 컨텍스트를 생성하는 RAG의 흥미로운 대안입니다. (위 참조) 매우 간단하고 긍정적인 성능 지표를 가지고 있음에도 불구하고, 이 접근 방식은 (당연히) LLM이 정보를 환각하는 경향 때문에 신뢰성이 부족합니다.

최근 연구 동향(Recent Directions of Research)
지금까지 다양한 프롬프팅 기술을 다루었지만, 최근에는 이러한 방법을 확장하고 복잡한 문제 해결을 위한 완전히 새로운 스타일의 프롬프트를 탐구하는 많은 논문이 발표되었습니다. 여기서는 이 연구를 주제 또는 초점에 따라 여러 범주로 나누었습니다.
*   추론(Reasoning)
*   도구 사용(Tool Usage)
*   프로그램 지원 언어 모델(Program-Aided Language Models)
*   컨텍스트 윈도우(Context Windows)
*   글쓰기(Writing)
*   프롬프트 최적화 및 평가(Prompt Optimization & Evaluation)
*   멀티모달 프롬프팅(Multimodal Prompting)
*   기타 (다른 주목할 만한 논문)

각 범주에 대해 다양한 다른 연구가 다루어집니다. 그러나 프롬프트 엔지니어링 주제에 대해 발표된 엄청난 양의 연구를 고려할 때, 몇몇 논문이 누락되었을 가능성이 큽니다. 포함되어야 할 좋은 논문을 알고 있다면 댓글로 공유해 주세요!

추론 능력 향상(Improving Reasoning Capabilities)
([15]에서 발췌)
Auto-CoT [15]. CoT 프롬프팅은 복잡한 문제를 해결하기 위해 중간 추론 단계(intermediate reasoning steps)를 사용하며, LLM의 출력 내에서 이러한 추론 단계를 이끌어낼 수 있는 두 가지 방법이 있습니다(위 그림 참조).
*   **제로샷(Zero-shot)**: LLM에 "단계별로 생각하라(think step-by-step)"고 프롬프트를 제공합니다.
*   **수동(Manual)**: 원하는 질문에 답하기 전에 질문, 근거, 답변에 대한 몇 가지 퓨샷 예시를 제공합니다.

LLM이 괜찮은 제로샷 추론기(zero-shot reasoners)이지만, 구체적인 예시를 제공하는 것이 CoT 프롬프팅에서 일관되게 더 나은 성능을 제공합니다. 그러나 이 전략은 또한 인간 주석자(human annotators)— 또는 프롬프트 엔지니어 —가 각 질문에 답하는 데 사용되는 근거에 대한 수동 시연(manual demonstrations)을 작성하도록 요구합니다. 이러한 수동 시연을 작성하는 것은 시간이 많이 걸리지만, 피할 수 있습니다!

"우리는 '단계별로 생각해 봅시다(Let’s think step by step)' 프롬프트를 가진 LLM을 활용하여 시연을 위한 추론 연쇄(reasoning chains)를 하나씩 생성함으로써 그러한 수동 노력을 제거할 수 있음을 보여줍니다." - [15]에서 발췌

[15]에서 저자들은 제로샷 CoT 프롬프팅을 사용하여 수동 CoT 프롬프팅을 위한 예시를 자동으로 생성하는 자동 CoT(Auto-CoT) 프롬프팅 접근 방식을 제안하여, 문제 해결 근거를 수동으로 작성할 필요성을 없앴습니다. 그러나 이러한 자동으로 생성된 근거가 어떤 경우에는 부정확하기 때문에 Auto-CoT가 잘 작동하려면 몇 가지 요령이 필요합니다. 질문이 입력으로 주어졌을 때, 순진한 접근 방식은 i) 유사한 질문 세트를 검색하고(예: sBERT와 같은 임베딩 모델(embedding model) 및 벡터 검색(vector search) 사용), ii) 제로샷 CoT 프롬프팅으로 각 질문에 대한 근거/답변을 생성하며, iii) 자동으로 생성된 시연으로 수동 CoT 프롬프팅을 수행하는 것입니다. 그러나 이 접근 방식은 상당히 좋지 않게 작동하며, [1]의 저자들은 이를 LLM이 생성한 근거의 실수 때문이라고 주장합니다. 이를 해결하기 위해 생성된 근거가 충분히 다양하도록 보장하기만 하면 됩니다.

([15]에서 발췌)
LLM이 답변해야 할 질문 데이터셋이 주어졌을 때, [15]의 저자들은 Auto-CoT를 위한 프롬프트 내에서 사용되는 시연을 선택/생성하기 위한 두 부분으로 된 전략(위 그림 참조)을 고안했습니다.
*   sBERT의 질문 임베딩(question embeddings)과 k-평균 클러스터링(k-means clustering)을 사용하여 질문을 k개의 클러스터(cluster)로 나눕니다.
*   각 클러스터에서 대표 질문을 선택하고 제로샷 CoT를 사용하여 각 질문에 대한 관련 근거를 생성합니다.

이러한 접근 방식은 Auto-CoT에 사용되는 시연의 다양성을 높여, 모델이 합성 근거(synthetic rationales)에서 저지르는 실수 간의 상관관계를 줄입니다. GPT-3를 사용한 실험에서 Auto-CoT는 수동 시연 생성이 필요한 퓨샷 CoT 프롬프팅의 성능과 일치하거나 이를 능가했으며, 10개 이상의 다른 벤치마크(benchmark)에서 일관된 결과를 보였습니다.

([16]에서 발췌)
복잡성 기반 프롬프팅(Complexity-Based Prompting) [16]. CoT 프롬프팅이 프롬프트에 포함할 문제 해결 근거의 시연을 선택하는 데 의존한다는 점을 고려할 때, 우리는 궁금할 수 있습니다. 이러한 시연을 어떻게 가장 잘 선택할 수 있을까요? [16]에서 저자들은 복잡성을 기반으로 시연을 선택하는 것이 좋은 휴리스틱(heuristic)임을 보여줍니다. 시연의 복잡성은 연쇄 사고 내에 존재하는 단계의 수를 세는 것으로 간단히 측정할 수 있으며, 개별 단계는 개행 문자(newline characters, \n )로 구분됩니다. [16]에서 제안된 복잡성 기반 프롬프팅 접근 방식은 가장 높은 복잡성을 가진 시연을 샘플링(sampling)할 것을 옹호합니다.

"GPT-3 175B의 추론 성능은 입력 프롬프트 복잡성이 증가함에 따라 명확하게 향상됩니다." - [16]에서 발췌

흥미롭게도 [16]의 저자들은 CoT 프롬프트에 더 많은 추론 단계를 포함하는 시연을 포함하는 것이 다단계 추론 작업에서 성능을 상당히 향상시킨다는 것을 발견했습니다. 더 나아가, 이 전략을 출력 공간으로 확장하여 가장 높은 복잡성을 가진 k개의 생성된 출력에 대해 다수결 투표를 하는 자기 일관성(self-consistency) 접근 방식을 사용할 수 있습니다. 수동 튜닝(manual tuning) 및 검색 기반 선택(retrieval-based selection)과 같은 대안적인 선택 방식과 비교할 때, 복잡성 기반 프롬프팅은 GPT-3 및 Codex를 사용하여 여러 데이터셋(즉, GSM8K, MultiArith, MathQA)에서 최첨단(state-of-the-art) 성능을 달성하며 유리하게 작동합니다.

([17]에서 발췌)
점진적 힌트 프롬프팅(Progressive-Hint Prompting, PHP) [17]. CoT 프롬프팅의 한 가지 단점은 문제를 단일 시도(single shot)로 해결한다는 것입니다. 질문이 입력으로 주어지면 우리는 근거와 답변을 생성하지만, LLM은 이 답변을 고려하거나 수정할 기회를 얻지 못합니다. 이 과정을 여러 번 반복하고 다수결 투표를 통해 더 나은 성능을 달성할 수 있습니다— 이것은 단지 자기 일관성입니다 —그러나 이러한 생성물 중 어느 것도 답변을 더 잘 알리기 위해 LLM의 이전 출력을 고려하지 않습니다.

"PHP는 질문을 재평가한 후 이전 답변을 힌트로 활용하여 올바른 답변에 도달하는 인간과 유사한 사고 과정을 따릅니다." - [17]에서 발췌

이 문제를 해결하기 위해 [17]의 저자들은 LLM의 이전 출력을 활용하여 생성된 근거를 반복적으로 개선하는 PHP를 제안합니다. 직관적으로 LLM은 모델이 이전에 생성한 근거를 올바른 답변을 찾는 힌트로 사용할 수 있습니다. 구체적으로 PHP는 세 단계로 진행됩니다.
*   질문이 주어지면 LLM에 기본 답변을 제공하도록 프롬프트를 제공합니다.
*   질문과 기본 답변을 연결한 다음, 이 입력을 기반으로 LLM에 수정된 답변을 생성하도록 프롬프트를 제공합니다.
*   LLM의 답변이 최소 두 번의 반복 동안 안정적일 때까지 2단계를 반복합니다.

이러한 접근 방식은 LLM이 여러 번의 통과를 통해 답변을 반복적으로 개선하고, 이 과정에서 이전 출력을 컨텍스트로 사용할 수 있도록 합니다. 또한 PHP는 CoT 프롬프팅 및 자기 일관성과 완벽하게 호환됩니다. 이러한 기술을 결합하여 성능을 더욱 향상시킬 수 있습니다. 실험에서 PHP는 복잡성 기반 프롬프팅 전략과 비교하여 GPT-3.5의 성능을 향상시키며, GPT-4와 함께 PHP를 사용하면 여러 주목할 만한 데이터셋(예: SVAMP, GSM8K, AQuA, MATH)에서 최첨단 성능을 달성합니다.

([18]에서 발췌)
분해 프롬프팅(Decomposed Prompting, DecomP) [18]은 프롬프팅을 통해 복잡한 단계를 가진 다단계 추론 문제를 해결하는 어려움을 해결하려고 시도합니다. 작업이 더 복잡해질수록 퓨샷 프롬프팅(즉, 올바른 해결책의 몇 가지 예시를 보여주는 것)은 부족해질 것입니다. 그러나 복잡한 작업을 프롬프팅을 통해 독립적으로 해결할 수 있는 하위 작업(sub-tasks)으로 분해함으로써 더 잘 수행할 수 있습니다. 특히 [18]의 저자들은 두 가지 구성 요소로 이루어진 프롬프팅 프레임워크를 제안합니다.
*   **분해기(Decomposer)**: LLM에 문제를 일련의 더 간단한 하위 작업으로 분해하도록 프롬프트를 제공합니다.
*   **하위 작업 처리기(Sub-task handlers)**: 별도의 프롬프트를 사용하여 (분해기가 지시하는 대로) 더 간단한 하위 작업을 LLM으로 해결합니다.

분해기와 하위 작업 처리기는 퓨샷 방식으로 프롬프트가 제공되는 LLM일 뿐입니다. 위에서 제안된 DecomP 전략은 하나의 프롬프트를 사용하여 해결 가능한 하위 작업을 식별하고, 이를 다른 시스템(예: 새로운 프롬프트, 다른 LLM 또는 도구)에 위임하여 해결합니다. 이러한 모듈식 접근 방식은 많은 이점을 가집니다.
*   긴 컨텍스트를 가진 작업은 여러 구성 요소로 분해될 수 있습니다.
*   각 하위 작업에는 더 넓은 범위의 예시를 보여줄 수 있습니다.
*   복잡한 하위 작업은 필요한 경우 더 작은 하위 작업으로 추가 분해될 수 있습니다.
*   모든 하위 작업을 LLM으로 해결하는 대신, 다른 기호 시스템(symbolic systems)(예: 작업별 모델(task-specific model), 검색 메커니즘(retrieval mechanism) 등)을 사용할 수도 있습니다.

간단한 작업을 예로 들어 봅시다. 단어 집합이 입력으로 주어졌을 때, 각 단어의 세 번째 문자를 추출하고, 이 문자들을 연결하여 그 연결된 결과를 출력으로 제공하고자 합니다. 이를 위해 세 가지 하위 작업의 시퀀스를 만들 수 있습니다. i) 단어 목록 수집, ii) 각 단어의 세 번째 문자 추출, iii) 추출된 문자 연결. 아래 그림과 같이 이러한 각 하위 작업을 별도의 퓨샷 프롬프트로 구현할 수 있습니다.

([18]에서 발췌)
DecomP 내에서 하위 작업은 분해기에 의해 반복적으로 생성되고, 해결되며, (관련 출력과 함께) 분해기로 반환되어 다음 하위 작업을 생성합니다. 분해기는 추론 프로세스의 컨트롤러(controller) 역할을 하면서 최종 답변이 생성되었음을 나타내는 질문 끝(end-of-question, [EOQ]) 마커가 생성될 때까지 하위 작업을 계속 생성합니다. (아래 참조) 전반적으로 DecomP는 최소-최대 프롬프팅의 더 일반적이고 유연한 버전으로 생각할 수 있습니다.

([18]에서 발췌)
가설-이론(Hypotheses-to-Theories) [29]. 복잡한 작업을 간단한 단계로 분해하는 예시 근거로 모델에 프롬프트를 제공함으로써 LLM 내에서 추론 능력을 이끌어낼 수 있습니다. 그러나 모델은 출력을 생성할 때 환각(hallucinate)을 일으킬 수 있으며, 기존 또는 상식적 지식을 넘어선 작업에서는 성능이 좋지 않습니다. 간단히 말해, LLM의 지식 기반(knowledge base)과 작업을 해결하는 데 필요한 지식 사이에 불일치(mismatch)가 있을 때 문제가 발생합니다. 이 문제를 해결하기 위해, 복잡한 추론 문제를 해결할 때 LLM이 필요한 지식을 발견하고 적용할 수 있도록 하는 프롬프팅 접근 방식이 필요합니다.

([29]에서 발췌)
인간의 과학적 발견 과정에서 영감을 받아, [29]의 저자들은 가설-이론(Hypotheses-to-Theories, HtT) 프롬프팅이라는 프롬프팅 기술을 제안합니다. 이 기술은 (잠재적으로 부정확한) 가설을 자유롭게 제안하고, 경험적으로 검증될 수 있는 가설만을 유지하며, 이 검증된 가설을 사용하여 문제를 해결하는 전략을 따릅니다. 높은 수준에서 이 전략의 목표는 문제 해결에 사용될 수 있는 LLM을 위한 규칙 라이브러리(rule library)를 학습하는 것입니다. 더 구체적으로, HtT 프롬프팅(위 그림 참조)은 두 단계로 구성됩니다.
*   **귀납(Induction)**: LLM은 훈련 예시(training examples) 세트에 대해 규칙을 생성하고 검증하도록 요청받습니다. 자주 나타나고 올바른 답변을 자주 생성하는 규칙들이 수집되어 규칙 라이브러리를 형성합니다.
*   **연역(Deduction)**: LLM은 귀납을 통해 생성된 규칙 세트를 사용하여 추론을 수행하고 질문에 답하도록 프롬프트가 제공됩니다.

추론 중에 규칙 세트를 사용함으로써 HtT 프롬프팅은 환각의 가능성을 줄입니다. 이러한 발견은 수치 추론(numerical reasoning) 및 관계 추론(relational reasoning) 작업 모두에서 검증되었으며, HtT 프롬프팅은 이전 프롬프팅 기술(예: CoT 프롬프팅)에 비해 정확도에서 11-27%의 절대적인 개선을 제공하는 것으로 나타났습니다. 흥미롭게도 HtT 프롬프팅으로 생성된 규칙은 해석 가능(interpretable)하며 다른 (그러나 유사한) 문제로도 전이 가능(transferable)합니다.

**자기 개선(Self-Refinement) 프롬프팅.** LLM이 초기 답변을 생성한 후, 추가적인 지시나 평가 기준을 제공하여 모델 스스로 자신의 답변을 비판하고 개선하도록 유도하는 기법입니다. 이는 모델이 한 번의 시도로 완벽한 답변을 내놓기 어렵다는 점에 착안하여, 인간이 초안을 작성하고 수정하는 과정과 유사하게 모델이 반복적으로 자신의 출력을 검토하고 보정하게 합니다. 이 과정은 다음과 같은 단계로 이루어질 수 있습니다.
1.  **초기 답변 생성**: LLM이 주어진 프롬프트에 대해 첫 번째 답변을 생성합니다.
2.  **자기 평가 프롬프트**: 모델에 "위 답변을 비판적으로 검토하고 개선할 점을 찾아라"와 같은 지시를 제공하여 자신의 답변을 평가하게 합니다. 이때 특정 평가 기준(예: 정확성, 완전성, 간결성)을 제시할 수 있습니다.
3.  **수정된 답변 생성**: 모델은 자기 평가를 바탕으로 초기 답변의 문제점을 파악하고, 이를 개선한 새로운 답변을 생성합니다.
4.  **반복**: 필요한 경우 이 과정을 여러 번 반복하여 답변의 품질을 점진적으로 향상시킬 수 있습니다.
이 기법은 특히 복잡하거나 미묘한 추론이 필요한 작업에서 LLM의 성능을 크게 향상시키는 데 기여하며, 모델의 신뢰성을 높이는 데 효과적입니다.

도구 사용(Tool Usage)
LLM은 강력하지만, 주목할 만한 한계가 있습니다! 예를 들어, LLM은 산술적 실수를 저지르고, 최신 정보에 접근할 수 없으며, 심지어 시간의 흐름을 이해하는 데 어려움을 겪기도 합니다. 인류의 많은 발전은 새롭고 혁신적인 도구(예: 인쇄기 또는 컴퓨터)에 대한 접근에 의해 촉진되었으며, LLM도 마찬가지일 수 있습니다. 즉, 외부의 전문화된 도구(예: 계산기 또는 검색 엔진) 세트에 대한 접근 권한을 부여하고, 언제, 어디서, 어떻게 이러한 도구를 적절하게 호출하여 문제를 더 안정적으로 해결할 수 있는지 모델에 가르침으로써 이러한 모델의 많은 한계를 해결할 수 있습니다. 더 많은 정보는 아래의 이 주제에 대한 이전 개요를 참조하세요.
*   언어 모델에게 도구 사용법 가르치기(Teaching Language Models to Use Tools) [링크]
*   언어 모델과 친구들(Language Models and Friends) [링크]
*   언어 모델이 자체 도구를 만들 수 있을까?(Can language models make their own tools?) [링크]

([32]에서 발췌)
Toolformer [32]는 LLM과 외부 도구의 통합을 탐구한 최초의 연구 중 하나였습니다. 이러한 도구는 간단하고 고정된 텍스트-투-텍스트 API(text-to-text API) 세트를 통해 모델에 제공됩니다. (위 참조) 도구를 사용하기 위해 LLM은 i) 도구가 필요한 시나리오를 식별하고, ii) 사용할 도구를 지정하며, iii) 도구의 API에 관련 텍스트 입력을 제공하고, iv) API에서 반환된 텍스트를 사용하여 응답을 작성하는 방법을 학습해야 합니다. LLM은 초기 시드 데이터셋(seed dataset)으로 시작하여 더 강력한 LLM(예: GPT-4)을 사용하여 유효한 API 호출(API calls) 예시를 데이터에 추가하는 합성 훈련 데이터셋(synthetic training dataset)을 구성함으로써 이러한 기술을 학습합니다. (아래 참조)

([32]에서 발췌)
여기에서 우리는 이 데이터에 대해 LLM을 간단히 미세 조정할 수 있습니다. 모델은 자신이 생성하는 텍스트 시퀀스 내에서 필요한 API 호출을 직접 생성하고 처리하는 방법을 학습할 것입니다. 이 경우, 텍스트 입력과 출력을 가진 API만 고려하기 때문에 API 호출을 인라인(inline) 방식으로 처리하는 것은 간단합니다. (아래 참조)

([32]에서 발췌)
"LLM은 최신 정보에 접근할 수 없거나 정밀한 수학적 추론을 수행할 수 없는 것과 같은 본질적인 한계에 직면합니다... 실제 작업 해결을 위해 외부 도구를 자동으로 구성하는 기능을 현재 LLM에 강화하는 것은 이러한 단점을 해결하는 데 중요합니다." - [19]에서 발췌

Chameleon [19]은 위에서 언급된 LLM의 한계를 완화하는 것을 목표로 합니다. 흥미롭게도, 이러한 한계 중 일부는 LLM과 외부 도구를 통합하는 기존 연구에서 다루어지지 않습니다. 사용되는 도구 세트가 일반적으로 고정되어 있거나(또는 도메인 특정적(domain-specific)) 항상 새로운 도메인(domain)으로 일반화될 수 없기 때문입니다. 더 일반적인 프레임워크를 만들기 위해 Chameleon은 "플러그 앤 플레이(plug-and-play)" 전략을 사용합니다. 이 전략은 중앙 LLM 기반 컨트롤러(controller)를 사용하여 복잡한 추론 작업을 해결하기 위해 여러 도구를 구성하는 프로그램— 자연어로 작성된 —을 생성합니다. (아래 참조) 이전 연구와 달리 Chameleon이 사용할 수 있는 도구는 상당히 포괄적입니다. 예를 들어, LLM, 상용 비전 모델(off-the-shelf vision models), 웹 검색 엔진(web search engines), Python 함수 등입니다.

([19]에서 발췌)
Chameleon 프레임워크는 두 가지 주요 구성 요소를 가집니다.
*   **플래너(Planner)**: 입력 쿼리(input query)를 사용 가능한 도구를 통해 해결할 수 있는 하위 작업으로 분해합니다.
*   **모듈 인벤토리(Module inventory)**: Chameleon이 사용할 수 있는 작업별 도구(설명 및 사용 예시 포함) 세트.

LLM으로 구현된 플래너는 자연어를 사용하여 외부 도구(예: image_captioner 또는 query_generator)에 대한 호출을 생성합니다. 우리는 간단한 문자열 일치(string matching)를 통해 이러한 도구를 식별할 수 있으며, 플래너가 출력하는 도구 시퀀스는 해당 작업별 모듈(task-specific modules) 각각을 호출하여 실행될 수 있는 자연어 프로그램(natural language program)을 형성합니다. 플래너와 작업별 모듈에 사용되는 프롬프트의 예시는 아래에 나와 있습니다.

([19]에서 발췌)
컨트롤러에게 특정 도구를 언제 사용할지 가르치기 위해, 우리는 퓨샷 프롬프트 내에 도구 설명과 사용 예시를 포함하며, 이는 새로운 도구와 모듈로 쉽게 확장될 수 있습니다. 플래너의 인컨텍스트 학습 능력을 활용하여 해결책을 생성하기 때문에, 실제 쿼리를 해결하는 데 훈련이나 큐레이션된 규칙이 필요하지 않습니다. 대신, 우리는 LLM에 사용 가능한 도구의 예시를 제공하기만 하면, LLM은 이 정보를 사용하여 쿼리에 대한 올바른 최종 응답을 생성하기 위해 실행될 수 있는 도구 시퀀스를 추론할 수 있습니다. 더 나아가, 이 도구 시퀀스는 사람이 읽을 수 있으며 인간 사용자가 쉽게 디버깅(debug)할 수 있습니다.

([19]에서 발췌)
실험에서 Chameleon은 GPT-4를 사용하여 두 가지 복잡한 다중 모달(multi-modal)(즉, 텍스트와 이미지가 모두 포함됨) 추론 작업인 ScienceQA와 TabMWP에 적용되었습니다. Chameleon은 ScienceQA에서 86.54%의 새로운 최첨단 성능을 달성하여 GPT-4 및 GPT-3를 사용한 CoT 프롬프팅보다 각각 2.55% 및 11.37% 더 나은 성능을 보였습니다. TabMWP에서도 Chameleon은 98.78%의 정확도를 달성하며 유사한 개선을 보였습니다. 그러나 Chameleon의 효과는 GPT-4가 복잡한 추론 문제를 해결하기 위한 제약 조건을 추론하고 합리적/일관된 계획을 구성하는 능력에 의해 강화된다는 점에 유의해야 합니다.

"우리는 고급 LLM의 자기 지시(self-instruct)를 통해 오픈 소스 LLM에 도구를 사용할 수 있는 능력을 부여하도록 설계된 간단하면서도 효과적인 방법인 GPT4Tools를 제안합니다." - [20]에서 발췌

GPT4Tools [20]. 다양한 논문에서 LLM이 퓨샷 방식으로 도구를 활용하는 능력을 보여주었지만, 대부분의 논문은 독점적인 언어 모델(proprietary language models)에 의존하며 도구 사용을 용이하게 하기 위해 순전히 프롬프트 엔지니어링을 활용합니다. 이는 오픈 LLM으로도 유사한 결과를 재현할 수 있는지 궁금하게 만듭니다. [20]에서 저자들은 자기 지시(self-instruct) [21]를 사용하여 오픈 소스 LLM(예: LLaMA 및 OPT)이 다중 모달 도구(multimodal tools) 세트를 사용할 수 있도록 하는 미세 조정 데이터셋(finetuning dataset)을 생성하는 접근 방식을 제안합니다.

([21]에서 발췌)
먼저, 저자들은 강력한 교사 모델(teacher model)(즉, ChatGPT)에 프롬프트를 제공하여 관련 도구가 사용되는 예시를 생성하도록 함으로써 자기 지시 접근 방식을 사용하여 도구 사용 데이터셋을 생성합니다. 프롬프트 내에는 시각적 콘텐츠— 이미지에서 추출된 캡션(captions) 및 바운딩 박스(bounding boxes) —와 도구 설명이 모두 포함됩니다. 교사는 이 정보를 활용하여 다중 모달 정보를 처리하고 문제를 해결하는 데 사용될 수 있는 도구 관련 지시를 생성합니다. (위 참조)

([21]에서 발췌)
데이터셋이 생성되면, 저랭크 적응(Low-Rank Adaptation, LoRA)을 사용하여 오픈 소스 LLM을 쉽게 미세 조정하여 다중 모달 도구의 도움을 받아 다양한 시각적 문제를 해결할 수 있습니다. [20]에서 이 접근 방식은 LLM이 알려진 도구(즉, 미세 조정 데이터셋에 포함된 도구)에 대해 수행하는 호출의 정확도를 향상시킬 뿐만 아니라, 모델이 제로샷 방식으로 새로운 도구에 일반화하는 능력도 향상시키는 것으로 나타났습니다. GPT4Tools와 LLM을 외부 도구와 통합하는 이전 연구에 대한 직접적인 비교는 위 표에 제공됩니다.

([30]에서 발췌)
Gorilla [30]. 많은 연구에서 LLM과 고정된 도구 세트를 통합하는 것을 연구했지만, [30]의 저자들은 LLM에게 온라인에서 사용 가능한 모든 모델 API를 사용하도록 가르치는 더 넓은 목표를 다룹니다. 이를 위해 i) 문제 해결과 관련된 모델 API를 검색하고 ii) 이 API에 대한 문서를 모델의 컨텍스트에 추가하는 검색 기술(retrieval technique)이 채택됩니다. 이러한 접근 방식은 LLM이 엄청난 수의 변화하는 도구에 접근할 수 있도록 하지만, 환각(예: 잘못된 인수(arguments) 또는 존재하지 않는 API 호출)이 여전히 발생할 수 있습니다. (위 참조)

([30]에서 발췌)
이 문제를 해결하기 위해 [30]의 저자들은 자기 지시 [21]를 사용하여 1,600개 이상의 다른 모델 API 사용 예시가 포함된 데이터셋을 구축합니다. 각 예시 내에서 프롬프트와 관련 문서가 모두 컨텍스트로 사용되어 출력을 생성합니다. 다시 말해, 이것은 검색 인식 미세 조정(retrieval-aware finetuning) 프로세스입니다 (RAFT와 유사). (위 참조) 그 결과 모델인 Gorilla (LLaMA-7B의 미세 조정 버전)는 다양한 딥러닝 모델 API를 활용하여 문제를 해결하기 위한 인터페이스입니다. 결과 LLM은 엄청난 수의 API를 사용할 수 있으며, 심지어 이러한 API 중 어느 하나의 문서 변경에도 적응할 수 있습니다!

([31]에서 발췌)
HuggingGPT [31]는 도구 사용 접근 방식을 통해 LLM과 특수 딥러닝 모델(예: 이미지 인식, 비디오 감지, 텍스트 분류 등)의 통합을 탐구한다는 점에서 Gorilla와 상당히 유사합니다. LLM은 문제 해결 시스템의 "두뇌" 역할을 하며, 문제를 해결하는 방법을 계획하고 이 문제에 필요한 하위 작업을 해결하는 다양한 딥러닝 모델 간의 노력을 조정합니다. 그러나 Gorilla와 달리 HuggingGPT는 미세 조정을 수행하지 않습니다. 문제 해결은 네 단계로 분해됩니다.
*   **작업 계획(Task planning)**: LLM을 사용하여 사용자의 요청을 해결 가능한 작업으로 분해합니다.
*   **모델 선택(Model selection)**: HuggingFace에서 작업을 해결하는 데 사용할 모델을 선택합니다.
*   **작업 실행(Task execution)**: 선택된 각 모델을 실행하고 결과를 LLM에 반환합니다.
*   **응답 생성(Response generation)**: LLM을 사용하여 사용자에게 최종 응답을 생성합니다.

이러한 각 단계에 대해, 우리는 큐레이션된 지시와 예시를 포함하는 프롬프팅을 활용하여 원하는 동작을 얻습니다. (예시 프롬프트는 아래 참조) 충분히 강력한 파운데이션 모델(foundation model)이 주어진다면, 이러한 접근 방식은 매우 효과적입니다.

([31]에서 발췌)

프로그램 지원 언어 모델(Program-Aided Language Models)
"계산은 생성된 프로그램을 실행하는 데 사용되는 프로그램 인터프리터(program interpreter)에 위임될 수 있으며, 이로써 복잡한 계산을 추론 및 언어 이해와 분리합니다." - [41]에서 발췌

LLM을 외부 도구와 통합하는 것은 흥미로운 연구 분야이며, 이러한 모델에 접근 권한을 부여할 수 있는 가장 유용한 도구 중 하나는 프로그램을 작성하고 실행하는 능력입니다. 대부분의 프롬프팅 기술은 복잡한 문제를 두 단계로 해결합니다.
*   문제 해결 근거를 생성합니다.
*   이 근거를 사용하여 실제로 문제를 해결합니다.

CoT 프롬프팅에서는 이 두 단계를 모두 LLM이 해결하도록 의존하지만, 이 모델들은 첫 번째 단계 해결에만 탁월합니다! 사실, 올바른 근거를 출력했음에도 불구하고 잘못된 답변을 생성하는 것은 LLM의 흔한 실패 사례입니다. 이 문제를 해결하기 위해, 모델이 언어와 코드(예: 유용한 주석이 있는 Python 프로그램)가 섞인 형태로 근거를 출력하도록 가르칠 수 있습니다. 그런 다음, 제공된 코드를 단순히 실행함으로써 최종 답변을 생성할 수 있습니다!

([40]에서 발췌)
프로그램 지원 언어 모델(Program-Aided Language Model, PAL) [40]은 LLM이 해결책을 찾기 위해 문제를 일련의 중간 단계로 분해하는 작업을 맡는다는 점에서 CoT 프롬프팅과 유사합니다. 그러나 이 근거는 자연어와 프로그래밍 구성 요소(programatic components)를 모두 포함합니다. 우리는 근거에서 코드를 실행하여(샌드박스(sandboxed) Python 환경 사용) 신뢰할 수 있는 최종 해결책을 생성할 수 있습니다. 실제 해결책 생성 과정은 코드 인터프리터(code interpreter)에 위임됩니다. [40]에서 우리는 코드에 대해 충분히 훈련된 LLM(예: Codex)이 퓨샷 학습 접근 방식을 사용하여 이러한 방식으로 문제를 해결하도록 가르칠 수 있음을 알 수 있습니다.

"이는 연쇄 사고와 유사한 방법에서 추론 연쇄는 올바르지만 잘못된 답변을 생성할 수 있는 중요한 격차를 해소합니다." - [40]에서 발췌

([41]에서 발췌)
사고 프로그램(Program of Thoughts, PoT) 프롬프팅 [41]은 i) 코드 증강 프롬프팅 기술(code-augmented prompting technique)을 사용하고 ii) 해결책을 도출하는 과정을 코드 인터프리터에 위임한다는 점에서 PAL과 상당히 유사합니다. 이 과정은 퓨샷 프롬프팅 전략에 의존합니다. (위 참조) 그러나 PAL과 달리 PoT가 작성한 코드는 SymPy라는 기호 수학 라이브러리(symbolic math library)에 의존합니다. 이 패키지는 사용자가 수학적 "기호"를 정의할 수 있게 하며, 이 기호들은 SymPy의 solve 함수를 통해 평가되는 복잡한 표현식을 형성하도록 결합될 수 있습니다. (아래 참조)

([41]에서 발췌)
높은 수준에서 PoT는 LLM이 복잡한 방정식을 해결할 수 없다는 점을 직접적으로 다루며, 이러한 방정식을 쉽게 구성/평가할 수 있도록 하는 기호 수학 라이브러리에 대한 접근을 제공합니다. 반면 PAL은 자연어와 코드의 조합을 통해 문제를 더 일반적으로 해결하는 데 중점을 둡니다. 프로그램 지원 모델에 대한 더 많은 정보는 이 관련 개요를 참조하세요.

컨텍스트 윈도우 이해 및 사용(Understanding and Using the Context Window)
RAG의 최근 인기와 최첨단 LLM 내의 긴 컨텍스트 윈도우에 대한 강조를 고려할 때, 이러한 모델이 프롬프트에 제공된 컨텍스트를 어떻게 처리하는지 이해하는 것이 중요합니다. 다행히도 최근 연구는 컨텍스트 윈도우와 인컨텍스트 학습 주제를 심층적으로 연구하여 프롬프트 엔지니어링과 관련된 몇 가지 흥미로운 시사점을 도출했습니다.

대규모 언어 모델은 관련 없는 컨텍스트에 쉽게 주의가 산만해질 수 있습니다 [22]. 언어 모델에 프롬프트를 제공할 때, 우리는 일반적으로 프롬프트 내에 관련 컨텍스트와 정보만 포함합니다. 그러나 실제 응용 분야에서는 모델의 프롬프트가 일반적으로 해결되는 특정 문제와 관련이 있을 수도 있고 없을 수도 있는 컨텍스트적으로 유사한 정보를 포함합니다. 이를 염두에 두고, 우리는 궁금할 수 있습니다. 프롬프트에 관련 없는 컨텍스트를 추가하는 것이 부정적인 부작용을 가져올까요?

([22]에서 발췌)
[22]에서 저자들은 현대 LLM의 주의 산만성(distractibility)을 연구했으며, 관련 없는 컨텍스트가 프롬프트에 포함될 때 이러한 모델의 성능이 급격히 저하될 수 있음을 발견했습니다. LLM 주의 산만성을 측정하기 위해 저자들은 문제 설명에 관련 없는 정보가 포함된 산술 추론 문제(arithmetic reasoning problems)를 포함하는 새로운 관련 없는 컨텍스트를 포함한 초등학교 수학(Grade-School Math with Irrelevant Context, GSM-IC) 데이터셋을 소개합니다. (위 참조) 그런 다음, 모델의 프롬프트에 관련 없는 문장을 추가하는 것이 문제의 결과 해결책을 변경하는지 여부를 테스트함으로써 LLM이 관련 없는 컨텍스트에 의해 주의가 산만해지는지 여부를 측정할 수 있습니다。

([22]에서 발췌)
이 전략은 Codex와 GPT-3.5를 여러 다른 프롬프팅 기술(그림은 위 참조)로 테스트하는 데 사용됩니다.
*   CoT 프롬프팅 (및 제로샷 CoT 프롬프팅)
*   최소-최대 프롬프팅
*   프로그램으로 프롬프팅

흥미롭게도, 관련 없는 정보가 컨텍스트에 포함될 때 이러한 모델의 성능은 급격히 저하됩니다. 그러나 관련 없는 컨텍스트의 영향은 i) 자기 일관성 사용, ii) 모델이 관련 없는 정보를 무시하도록 지시 추가, iii) 관련 없는 정보로 문제를 해결하는 것을 시연하는 퓨샷 예시 포함을 통해 완화될 수 있습니다. LLM은 지시 또는 컨텍스트를 통해 정보를 무시하는 것을 학습할 수 있습니다.

"우리는 예시에 '문제 설명의 관련 없는 정보는 자유롭게 무시하세요'라는 지시 문장을 앞에 추가합니다." - [22]에서 발췌

중간에서 길을 잃다(Lost in the Middle) [23]. 생성형 LLM은 텍스트-투-텍스트 형식을 가지며, 이는 텍스트 시퀀스를 입력(즉, 프롬프트)으로 받아들이고 해당 텍스트 시퀀스를 출력으로 생성한다는 의미입니다. LLM에 전달되는 입력은 가변 길이(variable length)입니다. 짧은 (제로샷) 문제 설명일 수도 있고, 많은 양의 외부 컨텍스트(예: RAG용)를 포함하는 복잡한 지시일 수도 있습니다. 이러한 이유로 LLM은 긴 컨텍스트에서 작동하고 이 컨텍스트 전체를 사용하여 다운스트림 작업(downstream tasks)을 효과적으로 해결할 수 있어야 합니다. 이러한 맥락에서 [23]의 저자들은 여러 LLM— 오픈 모델(MPT)과 폐쇄 모델(GPT-3.5-Turbo 및 Claude-1.3) 모두 —이 긴 컨텍스트 내에서 제공된 정보를 구체적으로 활용하는 능력을 연구합니다. 특히 [23]에서는 두 가지 유형의 작업이 연구됩니다.
*   **다중 문서 QA(Multi-document QA)**: 표준 RAG 설정과 유사하게, 이 문제는 모델이 여러 문서를 추론하여 질문에 답하도록 요구합니다.
*   **키-값 검색(Key-value retrieval)**: 이는 컨텍스트로 제공된 JSON 키-값 쌍(key-value pairs) 컬렉션에서 키와 관련된 값을 반환하여 일치하는 토큰을 검색하는 모델의 능력을 테스트하는 합성 작업입니다.

이러한 작업을 해결할 때 저자들은 i) 입력 컨텍스트의 길이(더 많은 문서 또는 키-값 쌍 사용)와 ii) 입력 내 관련 컨텍스트의 위치— 시작, 중간, 끝 —를 모두 제어합니다. 그런 다음, 컨텍스트 길이와 위치 변화가 모델 성능에 미치는 영향을 연구할 수 있습니다. 실험에서 우리는 모델 컨텍스트 내 관련 정보의 위치를 기반으로 명확한 "U자형" 성능 곡선(아래 그림 참조)을 볼 수 있습니다.

([23]에서 발췌)
이 시각화는 LLM이 컨텍스트의 시작과 끝에 있는 정보에 가장 많은 주의를 기울인다는 것을 보여줍니다. 관련 정보가 컨텍스트의 중간에 있을 때 모델 성능은 크게 저하됩니다— 정보가 "중간에서 길을 잃습니다(lost in the middle)". 사실, GPT-3.5-Turbo는 관련 문서가 컨텍스트 중간에 배치될 때보다 관련 컨텍스트가 전혀 없을 때 다중 문서 QA 작업에서 더 나은 성능을 보입니다. 관련 정보의 위치를 조정함에 따라 성능은 크게 달라지며, 확장된 컨텍스트를 가진 모델은 이러한 위치 편향(positional biases)에 대한 견고성 개선 징후를 보이지 않습니다. (아래 참조) 그러나 이러한 문제는 더 최근 모델(예: Gemini-1.5 및 Claude-3, GPT-4o)에서 개선되었으며, 이 모델들은 훨씬 더 긴 컨텍스트를 효과적으로 처리하고 중간에 있는 정보에 대한 주의력도 향상시키는 경향을 보입니다.

([23]에서 발췌)
대규모 언어 모델은 잠재 변수 모델(Latent Variable Models)입니다 [24]. LLM이 인컨텍스트 학습 능력을 가지고 있다는 것을 알고 있지만, 이러한 능력이 표준 언어 모델 사전 학습에서 어떻게 나타나는지는 불분명합니다. 또한 인컨텍스트 학습은 일반적으로 퓨샷 학습에 사용되는 예시의 선택과 형식에 민감합니다. 특정 시연은 모델에 효과적인 예시인 반면, 다른 시연은 그렇지 않습니다. 현재 퓨샷 학습을 위한 최상의 예시를 선택하는 표준 기준은 없습니다. [24]에서 저자들은 이 주제를 연구하며, 가능한 최상의 퓨샷 예시를 식별하기 위한 실용적인 전략을 찾는 것을 목표로 합니다.

"인컨텍스트 학습은 광범위한 자연어 처리(NLP) 작업에 효과적인 기술로 입증되었습니다. 그러나 이는 사용되는 시연의 선택, 형식, 심지어 순서에도 민감합니다." - [24]에서 발췌

많은 논문이 이론적 관점에서 인컨텍스트 학습의 메커니즘을 연구했지만, 실용적이거나 실행 가능한 통찰력을 제공하는 논문은 거의 없습니다. [24]에서 저자들은 LLM을 언어 모델이 관찰한 이전 토큰에 새로운 토큰 생성을 연결하는 간단한 주제/잠재 변수 모델의 관점에서 봅니다. 자세한 내용은 논문에서 찾을 수 있지만, 높은 수준에서 이 공식화(formulation)는 모델의 입력 프롬프트 내에서 사용된 형식 및 작업 정보와 관련하여 언어 모델의 출력을 이론적으로 설명할 수 있게 합니다.

([24]에서 발췌)
이 공식화로부터 저자들은 모델 입력의 사후 확률(posterior probability)을 측정하기 위해 더 작은 언어 모델을 사용하는 가능한 최상의 퓨샷 예시를 선택하기 위한 실용적인 기술을 개발합니다. 이는 모델의 입력과 매개변수를 기반으로 다른 입력 예시의 가능성을 알려줍니다. 더 작은 LLM으로 선택된 예시를 더 큰 모델과의 인컨텍스트 학습에 사용할 수 있으며(위 참조), 이는 실질적인 이점을 제공하는 것으로 밝혀졌습니다. 간단히 말해, 이 논문은 더 나은 퓨샷 예시를 선택하기 위해 실제로 사용될 수 있는 인컨텍스트 학습에 대한 흥미롭고(상대적으로 간단한) 이론적 관점을 제안합니다.

글쓰기 능력 향상(Improving Writing Capabilities)
"SoT는 추론 효율성을 위한 데이터 중심 최적화의 초기 시도이며, 언어로 답변 구조를 명시적으로 계획함으로써 고품질 답변을 이끌어낼 잠재력을 보여줍니다." - [25]에서 발췌

사고의 골격(Skeleton-of-Thought, SoT) [25]은 LLM으로 출력을 생성하는 지연 시간(latency)을 줄이는 것을 목표로 하는 프롬프팅 기술입니다. 아시다시피, LLM으로 출력을 생성하는 것은 몇 가지 이유로 비용이 많이 들 수 있습니다.
*   모델이 크기 때문에 계산/메모리/I/O 비용이 높습니다.
*   어텐션(attention) 연산은 I/O 바운드(IO bound)이며 시퀀스 길이(sequence length)에 따라 제곱으로 증가하는 메모리/계산 복잡성(compute complexity)을 가집니다.
*   출력은 한 번에 하나의 토큰씩 순차적으로 생성됩니다(즉, 다음 토큰 예측(next token prediction) 사용).

[25]에서 저자들은 위에서 언급된 마지막 문제— 순차적 디코딩(sequential decoding)의 지연 시간 —를 해결하려고 시도합니다. 간단히 말해, 순차적 디코딩은 한 번에 하나의 토큰을 생성하므로 출력 시퀀스에서 토큰 생성을 병렬화(parallelize)할 수 없기 때문에 문제입니다. 이러한 이유로 출력 생성 비용은 출력의 길이에 직접적으로 관련됩니다. 많은 토큰을 가진 출력 시퀀스를 생성하는 데 훨씬 더 오랜 시간이 걸립니다. 하지만, 완전한 순차적 디코딩을 피할 수 있을까요?

([25]에서 발췌)
[25]에서 우리는 모델, 시스템 또는 하드웨어에 어떤 변경도 요구하지 않고 인간의 사고 및 글쓰기 과정을 모방함으로써 더 효율적인 디코딩 전략(decoding strategy)을 고안할 수 있음을 봅니다. 특히, 인간은 쓰고 싶은 내용에 대한 개요(outline)를 계획한 다음, 개요의 각 요소에 대한 세부 사항을 채우는 경향이 있습니다. 이것은 순전히 순차적인 과정이 아닙니다! 6 이 아이디어에서 영감을 받아, [25]의 저자들은 사고의 골격(Skeleton-of-Thought, SoT) 프롬프팅(위 참조)을 제안하며, 이는 두 단계로 이루어집니다.
*   LLM에 답변의 골격/개요를 생성하도록 프롬프트를 제공합니다.
*   각 개요 요소의 내용을 채우기 위해 병렬 API 호출을 수행합니다.

이것이 다소 모호하게 들릴 수 있지만, 아래에 표시된 SoT 프롬프트를 확인하면 이것이 어떻게 작동하는지 알 수 있습니다. 과정은 매우 간단합니다. 우리는 골격을 생성하고 일반적인 프롬프트 템플릿(prompt template)을 사용하여 나머지 모든 세부 사항을 채웁니다.

([25]에서 발췌)
골격의 각 요소를 병렬로 생성함으로써 추론 지연 시간(inference latency)을 크게 절약할 수 있습니다. 예를 들어, 이 요약의 시작 부분에 표시된 질문은 기본 모델이나 시스템에 어떤 변경도 가하지 않고 12초(22초 대신) 만에 답변될 수 있습니다. 우리는 단지 SoT 프롬프팅을 사용합니다. [25]에서는 12개의 다른 LLM에서 유사한 속도 향상이 관찰됩니다. 흥미롭게도, 저자들은 개요를 작성하는 것이 종종 글쓰기 품질을 향상시킬 수 있다고 언급합니다. 7

([27]에서 발췌)
방향성 자극 프롬프팅(Directional Stimulus Prompting) [27]. 미세 조정의 계산 비용을 고려할 때, 프롬프팅은 일반적으로 LLM으로 작업을 해결하는 가장 쉬운 방법입니다. 그러나 프롬프팅에는 한계가 있습니다. LLM이 우리가 원하는 내용이나 스타일로 출력을 생성하도록 유도하는 것은 어려울 수 있습니다. 이 문제를 해결하기 위해 [27]의 저자들은 방향성 자극 프롬프팅(directional stimulus prompting, DSP)을 제안합니다. 이는 위 그림과 같이 LLM의 프롬프트에 "방향성 자극(directional stimulus)"을 도입합니다. 이 자극은 예상 출력에 대한 더 많은 정보를 LLM에 제공하는 텍스트 힌트(textual hint) 또는 단서(clue)일 뿐입니다. 방향성 자극은 인스턴스 특정적(instance-specific)이며 입력 쿼리에만 기반하며, LLM에 비해 훈련하거나 미세 조정하기 훨씬 쉬운 더 작은 모델(예: T5)을 사용하여 생성됩니다. 이렇게 함으로써 우리는 LLM을 직접 훈련하는 어려움을 우회하고, 대신 방향성 자극을 생성하는 데 사용되는 모델을 미세 조정하는 것을 선택합니다. 8 DSP는 요약(summarization), 대화(dialogue) 및 추론 작업에서 평가되었으며, 최소한의 레이블이 지정된 데이터(labeled data)만으로 모델 성능을 향상시키는 것으로 나타났습니다.

([28]에서 발췌)
밀도 연쇄 프롬프팅(Chain of Density Prompting) [28]. LLM의 최근 발전은 자동 요약(automatic summarization) 문제를 혁신했습니다. 레이블이 지정된 데이터에 대해 미세 조정을 수행하는 대신 LLM에 고품질 요약을 생성하도록 간단히 프롬프트를 제공할 수 있기 때문입니다. 요약을 자동으로 생성할 때, 결과 요약의 품질에서 중요한 측면 중 하나는 정보 밀도(information density)입니다. 우리는 요약이 모든 관련 정보를 간결하게 제시하기를 원하지만, 지나치게 밀도가 높거나 읽기 어려운 요약을 작성하는 것은 피하고 싶습니다. 정보 밀도의 이러한 상충 관계(tradeoff)를 연구하기 위해 [28]의 저자들은 밀도 연쇄(chain of density, CoD) 프롬프팅을 제안합니다. 이는 GPT-4에 바닐라 프롬프트(vanilla prompt)를 통해 요약을 생성하는 것으로 시작합니다. 여기에서 CoD 프롬프팅은 요약의 길이를 고정된 상태로 유지하면서 요약에 추가 엔티티(entities)를 반복적으로 추가하여 요약의 정보 밀도를 높이는 데 사용됩니다. 흥미롭게도 [28]에서 우리는 인간이 사람이 작성한 요약만큼 밀도가 높지만, GPT-4에 바닐라 프롬프트를 통해 생성된 요약보다 더 밀도가 높은 요약을 선호한다는 것을 알 수 있습니다. CoD 프롬프팅을 사용함으로써 우리는 이러한 상충 관계를 탐색하고 더 높은 품질의 요약을 생성할 수 있습니다.

"CoD로 생성된 요약은 바닐라 프롬프트로 GPT-4가 생성한 요약보다 더 추상적(abstractive)이고, 더 많은 융합(fusion)을 보이며, 선행 편향(lead bias)이 적습니다." - [28]에서 발췌

**프롬프트 최적화 및 평가(Prompt Optimization & Evaluation)**
프롬프트 엔지니어링은 본질적으로 반복적이고 경험적인 과정이지만, 이 과정을 자동화하고 최적화하려는 노력도 활발히 진행되고 있습니다.
*   **자동 프롬프트 생성(Automated Prompt Generation)**: LLM 자체가 더 나은 프롬프트를 생성하도록 유도하는 연구입니다. 예를 들어, APE(Automatic Prompt Engineer)나 OPPRO와 같은 방법들은 LLM을 사용하여 다양한 후보 프롬프트를 생성하고, 이를 평가하여 최적의 프롬프트를 찾아냅니다. 이는 인간의 개입 없이 프롬프트의 품질을 향상시키는 것을 목표로 합니다.
*   **정량적 평가 지표(Quantitative Evaluation Metrics)**: 프롬프트의 효과를 측정하기 위해 단순히 정확성뿐만 아니라, ROUGE, BLEU와 같은 자연어 생성 평가 지표나, 인간의 선호도(human preference)를 반영하는 평가 방법론이 중요하게 다루어집니다. 특히 LLM-as-a-judge와 같이 또 다른 LLM을 사용하여 프롬프트의 출력을 평가하는 방식도 널리 사용됩니다.

**멀티모달 프롬프팅(Multimodal Prompting)**
텍스트 기반 LLM을 넘어, 이미지, 오디오, 비디오 등 다양한 형태의 데이터를 이해하고 생성하는 멀티모달 LLM의 등장으로 프롬프팅 방식도 진화하고 있습니다.
*   **시각적 프롬프팅(Visual Prompting)**: [42]의 마크 세트 프롬프팅(Set of Marks Prompting)은 사전 훈련된 분할 모델(pretrained segmentation models)을 사용하여 이미지를 영역으로 분할하고, 이 영역에 마크 세트(즉, 영숫자, 마스크, 상자 등)를 오버레이(overlay)하여 GPT-4V와 같은 모델의 시각적 접지(visual grounding)를 개선하는 방법입니다. 이는 텍스트 지시만으로는 전달하기 어려운 시각적 정보를 효과적으로 모델에 주입하는 방법을 제시합니다.
*   **멀티모달 CoT 프롬프팅(Multimodal CoT Prompting)**: [43]은 이미지와 텍스트를 모두 포함하는 입력에 대해 CoT 프롬프팅을 확장하여, 근거 및 답변 생성을 문제 해결 과정의 두 가지 별개의 단계로 처리합니다. 이를 통해 모델은 시각적 정보와 텍스트 정보를 통합하여 더 복잡한 추론을 수행할 수 있습니다.

기타 주목할 만한 논문(Other Notable Papers)
능동 프롬프팅(Active Prompting) [26]은 불확실성 기반 능동 학습(uncertainty-based active learning) 연구를 기반으로 특정 추론 문제를 해결하기 위해 가장 유용한 예시를 선택(및 주석 달기)하는 기술을 제공함으로써 CoT 프롬프팅을 위한 예시 선택(및 주석 달기)의 어려움을 해결합니다. TaskMatrix [33]는 파운데이션 모델(foundation models)과 수백만 개의 다른 API의 통합을 고려하는 주목할 만한 문제에 대한 입장 또는 전망을 제시하는 입장 논문(position paper)입니다. 자동 프롬프팅(automatic prompting)(즉, 최적화 프로세스를 통해 더 나은 프롬프트 생성) 주제는 이 게시물에서 다루지 않습니다. 다른 것이 있나요? 댓글로 알려주세요!

결론(Conclusion)
이 개요에서 우리는 프롬프트 엔지니어링의 기본부터 지난 두 달 동안 제안된 최첨단 기술에 이르기까지 모든 것을 배웠습니다! 이 게시물은 엄청난 양의 정보를 포함하고 있지만, 우리가 본 많은 기술들은 동일한 핵심 프롬프트 구성 요소인 지시, 예시, 컨텍스트, 문제 해결 근거를 활용하는 약간의 변형입니다. 또한, 우리는 처음에 제안된 프롬프트 엔지니어링 전략을 상기해야 합니다.
*   프롬프트의 품질을 쉽고 정량적으로 측정할 수 있는 포괄적인 평가 전략을 수립하는 것으로 시작하세요.
*   작성하는 첫 번째 프롬프트는 간단해야 합니다(예: 지시 프롬프트).
*   프롬프트를 더 복잡하게 만들 때는 추가된 복잡성이 그에 상응하는 성능 향상으로 이어지는지 확인하세요.
*   원하는 성능에 도달할 때까지 프롬프트를 계속 반복하세요.

많은 문제는 간단한 지시 및 퓨샷 프롬프트를 통해 해결될 수 있습니다. 복잡한 추론 문제의 경우, 자기 일관성을 포함하는 CoT 프롬프팅과 같은 더 고급 전략을 사용하는 것이 필요할 수 있습니다. 또한, 특정 문제 도메인(예: 수학 문제에 대한 PoT 프롬프팅 또는 요약에 대한 CoD 프롬프팅)에 유용한 다양한 프롬프팅 전략을 보았습니다. 이러한 기술을 아는 것이 유용하지만, 그 사용 사례는 비교적 드물며, 명확하고 측정 가능한 성능 영향을 볼 때만 사용해야 합니다. 프롬프트 엔지니어링은 끊임없이 진화하는 분야이며, 미래에는 LLM이 스스로 프롬프트를 설계하고 최적화하는 '자동 프롬프트 엔지니어링'이 더욱 중요해질 것입니다. 사용자들은 모델의 능력을 최대한 활용하기 위해 끊임없이 새로운 접근 방식을 탐색하고 적용해야 할 것입니다.

뉴스레터가 처음이신가요? 안녕하세요! 저는 Cameron R. Wolfe이며, 이 Deep (Learning) Focus 뉴스레터는 독자들이 AI 연구를 이해하도록 돕습니다. 뉴스레터가 마음에 드신다면 구독, 공유 또는 X와 LinkedIn에서 저를 팔로우해 주세요!

구독

참고문헌(Bibliography)
[1] Saravia, Elvis, et al. “Prompt Engineering Guide”, https://github.com/dair-ai/Prompt-Engineering-Guide (2022).
[2] Radford, Alec, et al. "Language Models are Unsupervised Multitask Learners."
[3] Brown, Tom, et al. "Language models are few-shot learners." Advances in neural information processing systems 33 (2020): 1877-1901.
[4] Work, What Makes In-Context Learning. "Rethinking the Role of Demonstrations: What Makes In-Context Learning Work?."
[5] Zhao, Zihao, et al. "Calibrate before use: Improving few-shot performance of language models." International conference on machine learning . PMLR, 2021.
[6] Ouyang, Long, et al. "Training language models to follow instructions with human feedback." Advances in neural information processing systems 35 (2022): 27730-27744.
[7] Ye, Seonghyeon, et al. "Investigating the effectiveness of task-agnostic prefix prompt for instruction following." Proceedings of the AAAI Conference on Artificial Intelligence . Vol. 38. No. 17. 2024.
[8] Thoppilan, Romal, et al. "Lamda: Language models for dialog applications." arXiv preprint arXiv:2201.08239 (2022).
[9] Rae, Jack W., et al. "Scaling language models: Methods, analysis & insights from training gopher." arXiv preprint arXiv:2112.11446 (2021).
[10] Wei, Jason, et al. "Chain-of-thought prompting elicits reasoning in large language models." Advances in neural information processing systems 35 (2022): 24824-24837.
[11] Kojima, Takeshi, et al. "Large language models are zero-shot reasoners." arXiv preprint arXiv:2205.11916 (2022).
[12] Wang, Xuezhi, et al. "Self-consistency improves chain of thought reasoning in language models." arXiv preprint arXiv:2203.11171 (2022).
[13] Zhou, Denny, et al. "Least-to-most prompting enables complex reasoning in large language models." arXiv preprint arXiv:2205.10625 (2022).
[14] Yao, Shunyu, et al. "Tree of thoughts: Deliberate problem solving with large language models." arXiv preprint arXiv:2305.10601 (2023).
[15] Zhang, Zhuosheng, et al. "Automatic chain of thought prompting in large language models." arXiv preprint arXiv:2210.03493 (2022).
[16] Fu, Yao, et al. "Complexity-based prompting for multi-step reasoning." The Eleventh International Conference on Learning Representations . 2022.
[17] Zheng, Chuanyang, et al. "Progressive-hint prompting improves reasoning in large language models." arXiv preprint arXiv:2304.09797 (2023).
[18] Khot, Tushar, et al. "Decomposed prompting: A modular approach for solving complex tasks." arXiv preprint arXiv:2210.02406 (2022).
[19] Lu, Pan, et al. "Chameleon: Plug-and-play compositional reasoning with large language models." Advances in Neural Information Processing Systems 36 (2024).
[20] Yang, Rui, et al. "Gpt4tools: Teaching large language model to use tools via self-instruction." Advances in Neural Information Processing Systems 36 (2024).
[21] Wang, Yizhong, et al. "Self-instruct: Aligning language models with self-generated instructions." arXiv preprint arXiv:2212.10560 (2022).
[22] Shi, Freda, et al. "Large language models can be easily distracted by irrelevant context." International Conference on Machine Learning . PMLR, 2023.
[23] Liu, Nelson F., et al. "Lost in the middle: How language models use long contexts." Transactions of the Association for Computational Linguistics 12 (2024): 157-173.
[24] Wang, Xinyi, et al. "Large language models are latent variable models: Explaining and finding good demonstrations for in-context learning." Advances in Neural Information Processing Systems 36 (2024).
[25] Ning, Xuefei, et al. "Skeleton-of-thought: Large language models can do parallel decoding." arXiv preprint arXiv:2307.15337 (2023).
[26] Diao, Shizhe, et al. "Active prompting with chain-of-thought for large language models." arXiv preprint arXiv:2302.12246 (2023).
[27] Li, Zekun, et al. "Guiding large language models via directional stimulus prompting." Advances in Neural Information Processing Systems 36 (2024).
[28] Adams, Griffin, et al. "From sparse to dense: GPT-4 summarization with chain of density prompting." arXiv preprint arXiv:2309.04269 (2023).
[29] Zhu, Zhaocheng, et al. "Large language models can learn rules." arXiv preprint arXiv:2310.07064 (2023).
[30] Patil, Shishir G., et al. "Gorilla: Large language model connected with massive apis." arXiv preprint arXiv:2305.15334 (2023).
[31] Shen, Yongliang, et al. "Hugginggpt: Solving ai tasks with chatgpt and its friends in huggingface." arXiv preprint arXiv:2303.17580 (2023).
[32] Schick, Timo, et al. "Toolformer: Language models can teach themselves to use tools." arXiv preprint arXiv:2302.04761 (2023).
[33] Liang, Yaobo, et al. "Taskmatrix. ai: Completing tasks by connecting foundation models with millions of apis." arXiv preprint arXiv:2303.16434 (2023).
[34] Chen, Shouyuan, et al. "Extending context window of large language models via positional interpolation." arXiv preprint arXiv:2306.15595 (2023).
[35] Besta, Maciej, et al. "Graph of Thoughts: Solving Elaborate Problems with Large Language Models." arXiv preprint arXiv:2308.09687 (2023).
[36] Yao, Yao, Zuchao Li, and Hai Zhao. "Beyond Chain-of-Thought, Effective Graph-of-Thought Reasoning in Large Language Models." arXiv preprint arXiv:2305.16582 (2023).
[37] Lewis, Patrick, et al. "Retrieval-augmented generation for knowledge-intensive nlp tasks." Advances in Neural Information Processing Systems 33 (2020): 9459-9474.
[38] Ovadia, Oded, et al. "Fine-tuning or retrieval? comparing knowledge injection in llms." arXiv preprint arXiv:2312.05934 (2023).
[39] Liu, Jiacheng, et al. "Generated knowledge prompting for commonsense reasoning." arXiv preprint arXiv:2110.08387 (2021).
[40] Gao, Luyu, et al. "PAL: Program-aided Language Models." arXiv preprint arXiv:2211.10435 (2022).
[41] Chen, Wenhu, et al. "Program of thoughts prompting: Disentangling computation from reasoning for numerical reasoning tasks." arXiv preprint arXiv:2211.12588 (2022).
[42] Yang, Jianwei, et al. "Set-of-mark prompting unleashes extraordinary visual grounding in gpt-4v." arXiv preprint arXiv:2310.11441 (2023).
[43] Zhang, Zhuosheng, et al. "Multimodal chain-of-thought reasoning in language models." arXiv preprint arXiv:2302.00923 (2023).

1 기억하세요, 더 긴 프롬프트는 항상 더 비쌉니다! API를 사용할 때는 이러한 토큰에 대해 직접 비용을 지불합니다. 오픈 소스 모델의 경우, 추가 계산 및 지연 시간 비용으로 추가 토큰에 대한 비용을 지불합니다.
2 이러한 기술(및 더 많은 프롬프팅 기술과 다양한 멋진 주제)에 대한 훌륭한 개요를 보려면 Lillian Weng의 블로그를 확인하세요!
3 이 문제를 제거하기 위해 프롬프트 내의 예시를 무작위로 정렬하거나 심지어 순열(permute)하고, 예시의 여러 순열에 대한 출력을 생성할 수 있습니다.
4 사실, 빈약한 추론 능력은 현대 LLM에 대한 주요 비판 중 하나입니다. 많은 연구자들은 LLM의 추론 불능을 중요한 개념에 대한 얕은 이해의 증거로 인용합니다.
5 많은 연구자들은 이 다수결 투표 전략이 복잡한 문제를 해결하는 데 불충분하다고 주장해 왔습니다. 이는 프롬프트 앙상블(prompt ensembles) 및 기타 자기 일관성 변형에 대한 많은 심층 연구로 이어졌습니다.
6 사실, 이 뉴스레터를 작성하는 것은 순차적이지 않습니다. 저의 글쓰기 전략은 보통 i) 유사한 논문들을 개괄하고, ii) 공유된 아이디어와 배경을 가진 섹션들을 추가한 다음, iii) 서론과 결론을 작성하는 것입니다. 이것은 순차적인 과정과는 정반대입니다 (서론은 제가 가장 마지막에 쓰는 것입니다!).
7 LLM에 답변의 개요인 근거를 생성하도록 요청할 때, 우리는 종종 이 과정에서 CoT 프롬프팅에서 관찰된 이점과 유사한 이점을 봅니다. 그러나 SoT 프롬프팅은 여러 개의 분리된 생성(즉, 골격을 생성하는 하나, 그리고 각 골격 구성 요소에 대한 하나)을 수행하는 반면, CoT 프롬프트는 일반적으로 단일 통과(single pass)로 출력을 생성하는 데 사용된다는 점에 유의해야 합니다.
8 이 모델은 지도 미세 조정(supervised finetuning, SFT) 또는 RLHF와 유사한 RL 기반 전략을 사용하여 미세 조정될 수 있습니다.