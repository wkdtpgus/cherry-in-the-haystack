(from [17, 29, 35, 39]) 사용 편의성 덕분에 대규모 언어 모델(LLM)은 폭발적인 인기를 얻었습니다. 텍스트 프롬프트(textual prompt)를 작성하는 것만으로도 딥러닝(deep learning)에 전혀 익숙하지 않은 사람들도 거대한 신경망(neural network)을 활용하여 다양한 복잡한 문제를 신속하게 해결할 수 있습니다. 시간이 지남에 따라 이러한 모델은 개선된 지시 따르기(instruction following) 능력과 정렬(alignment)을 통해 사용하기가 더욱 쉬워졌습니다. 그러나 LLM에 효과적으로 프롬프트를 제공하는 것은 예술이자 과학입니다. 프롬프팅(prompting) 구현 또는 전략을 약간만 조정해도 상당한 성능 향상을 이룰 수 있습니다. 이 개요에서는 프롬프트 엔지니어링(prompt engineering)의 기본 개념부터 최근 몇 달 동안 제안된 최첨단 기술에 이르기까지 포괄적인 이해를 발전시킬 것입니다.

최근 몇 년간 대규모 언어 모델(LLM)의 발전은 인공지능 분야에 혁명적인 변화를 가져왔습니다. 특히 프롬프트 엔지니어링은 이러한 모델의 잠재력을 최대한 발휘하고, 특정 작업에 대한 성능을 최적화하며, 더 나아가 인간과 AI의 상호작용 방식을 재정의하는 핵심 요소로 부상했습니다. 모델 자체의 발전만큼이나 프롬프트 엔지니어링 기술의 발전은 LLM의 활용 범위를 넓히고 있으며, 이는 단순한 질문-답변을 넘어 복잡한 추론, 창의적인 콘텐츠 생성, 다단계 문제 해결에 이르는 다양한 응용 분야에서 빛을 발하고 있습니다. 빠르게 변화하는 이 분야에서 효과적인 프롬프트 전략을 이해하고 적용하는 것은 모델의 성능을 결정짓는 중요한 역량입니다.

프롬프트 엔지니어링이란 무엇인가요?
LLM이 이렇게 인기 있는 주요 이유 중 하나는 텍스트-투-텍스트(text-to-text) 인터페이스 덕분에 사용하기가 매우 간단하다는 것입니다. 이전 세대에서는 딥러닝으로 작업을 해결하려면 (최소한) 모델이 해당 작업을 해결하는 방법을 학습하도록 일부 데이터에 대해 모델을 미세 조정(finetune)해야 했습니다. 게다가 대부분의 모델은 특정 작업 해결에 특화된 좁은 전문가(narrow experts)였습니다. 그러나 LLM의 새로운 인컨텍스트 학습(in-context learning) 능력 덕분에 텍스트 프롬프트를 통해 다양한 문제를 해결할 수 있습니다. (위 참조) 이전에 복잡했던 문제 해결 프로세스가 자연어(natural language)로 추상화되었습니다!

"프롬프트 엔지니어링은 다양한 응용 분야 및 연구 주제에 LM을 효율적으로 사용하기 위한 프롬프트를 개발하고 최적화하는 비교적 새로운 분야입니다." - [1]에서 발췌

프롬프트 엔지니어링이란 무엇인가요?
LLM의 단순성은 그 사용을 대중화했습니다. 데이터 과학자(data scientist)나 MLE(Machine Learning Engineer)가 아니더라도 LLM을 사용할 수 있습니다. 영어를 이해한다면 (또는 선택한 언어를 이해한다면) LLM으로 비교적 복잡한 문제를 해결할 수 있습니다! 그러나 LLM으로 문제를 해결할 때 얻는 결과는 모델에 제공되는 텍스트 프롬프트에 크게 의존합니다. 이러한 이유로, LLM의 성능을 최적화하기 위해 다양한 프롬프트를 테스트하는 경험적 과학(empirical science)인 프롬프트 엔지니어링은 매우 인기 있고 영향력 있는 분야가 되었으며, 많은 기술과 모범 사례(best practices)의 발견으로 이어졌습니다.

프롬프트 엔지니어링의 중요성 및 확장
프롬프트 엔지니어링은 단순한 "질문 잘하기"를 넘어, LLM 기반 애플리케이션의 성공을 좌우하는 핵심 역량으로 자리매김했습니다. 이는 모델의 성능을 극대화하고, 원하는 동작을 유도하며, 모델이 생성하는 환각(hallucination)과 편향(bias)을 줄이는 데 필수적입니다. 특히 복잡한 비즈니스 프로세스나 민감한 정보 처리가 필요한 경우, 잘 설계된 프롬프트는 LLM이 신뢰할 수 있고 예측 가능한 방식으로 작동하도록 보장합니다. 또한, 지속적인 프롬프트 최적화 주기(prompt optimization cycle)를 통해 모델의 새로운 기능에 적응하고, 사용자 피드백을 반영하여 성능을 점진적으로 개선할 수 있습니다. 프롬프트 엔지니어링은 이제 단순히 기술적인 영역을 넘어, 사용자 경험(UX), 콘텐츠 전략, 심지어 윤리적 AI 개발의 중요한 부분으로 인식되고 있습니다.

프롬프트 내에 지표(indicator) 포함하기
프롬프트 구성 요소(Prompt components). LLM에 프롬프트를 제공하는 방법은 여러 가지가 있습니다. 그러나 대부분의 프롬프팅 전략은 몇 가지 공통 구성 요소를 공유합니다.
*   **입력 데이터(Input Data)**: LLM이 처리할 것으로 예상되는 실제 데이터 (예: 번역되거나 분류되는 문장, 요약되는 문서 등).
*   **예시(Exemplars)**: 프롬프트 내에 포함된 올바른 입력-출력 쌍의 구체적인 예시.
*   **지시(Instruction)**: 모델에서 예상되는 출력에 대한 텍스트 설명.
*   **지표(Indicators)**: 프롬프트 내에 구조를 생성하는 데 사용되는 태그(tag) 또는 서식 요소(formatting elements). (위 참조)
*   **컨텍스트(Context)**: 프롬프트에서 LLM에 제공되는 모든 추가 정보.

아래 그림에서 문장 분류를 위한 단일 프롬프트 내에 위에서 언급된 모든 프롬프트 구성 요소를 결합한 예시를 볼 수 있습니다.

효과적인 프롬프트 구조화 기법
프롬프트의 구성 요소를 이해하는 것은 중요하지만, 이를 어떻게 조합하고 구조화하는지는 모델의 출력 품질에 결정적인 영향을 미칩니다. 다음은 효과적인 프롬프트 구조화를 위한 몇 가지 기법입니다.

*   **명확한 지시문 작성**: 지시문은 모호하지 않고 구체적이어야 합니다. 예를 들어, "요약해 줘" 대신 "다음 기사를 3문장으로 요약하고, 핵심 주제와 저자의 주장을 포함해 줘"와 같이 구체적으로 요구해야 합니다. 긍정적인 표현을 사용하고, 모델이 하지 말아야 할 것을 명시하는 것보다 해야 할 것을 명시하는 것이 더 효과적입니다.
*   **역할 부여(Persona Assignment)**: 모델에 특정 역할을 부여하면 출력의 톤(tone), 스타일(style) 및 관점(perspective)을 효과적으로 제어할 수 있습니다. 예를 들어, "당신은 전문 마케터입니다. 다음 제품에 대한 소셜 미디어 게시물을 작성해 주세요."와 같이 역할을 지정하면 모델이 해당 역할에 맞는 출력을 생성하려고 시도합니다.
*   **출력 형식 지정(Output Formatting)**: 모델에 특정 출력 형식을 요구하면 구조화된 데이터를 얻을 수 있습니다. JSON, XML, Markdown, 또는 특정 표 형식 등을 명시하여 모델이 데이터를 파싱(parsing)하기 쉽게 만듭니다. 예를 들어, "다음 정보를 JSON 형식으로 출력해 주세요: {'이름': '홍길동', '나이': 30}"과 같이 요청할 수 있습니다.
*   **구분자 활용(Delimiter Usage)**: 프롬프트 내에서 지시문, 입력 데이터, 예시 등 각 섹션을 명확하게 구분하는 것은 모델의 혼동을 줄이고 원하는 부분에 집중하게 하는 데 도움이 됩니다. `###`, `---`, `"""`, 또는 `<|startoftext|>`와 같은 구분자를 사용하여 각 섹션을 분리할 수 있습니다. 이는 특히 긴 프롬프트나 여러 구성 요소가 포함된 프롬프트에서 중요합니다.
*   **제약 조건 명시(Specifying Constraints)**: 출력의 길이, 사용 금지 단어, 포함해야 할 특정 키워드 등 모델이 따라야 할 제약 조건을 명확하게 명시합니다. 이는 모델이 생성하는 결과물의 품질을 향상시키는 데 기여합니다.

이러한 구조화 기법들을 조합하여 사용함으로써, LLM은 사용자의 의도를 더 정확하게 파악하고, 더욱 유용하고 일관된 출력을 생성할 수 있습니다.

LLaMA-2 스타일의 모든 구성 요소를 포함한 프롬프트 (여기 참조)

컨텍스트 윈도우(context window). 사전 학습(pretraining) 동안 LLM은 특정 길이의 입력 시퀀스(input sequence)를 봅니다. 사전 학습 중 이 시퀀스 길이 선택은 모델의 "컨텍스트 길이(context length)" 또는 모델이 처리할 수 있는 최대 시퀀스 길이가 됩니다. 미리 정해진 컨텍스트 길이보다 훨씬 긴 텍스트 시퀀스가 주어지면 모델은 예측할 수 없게 동작하고 잘못된 출력을 생성할 수 있습니다. 그러나 Self-Extend 또는 위치 보간(positional interpolation)과 같은 방법을 사용하여 모델의 컨텍스트 윈도우를 확장할 수 있습니다.

RoPE를 사용한 위치 보간(positional interpolation) 그림 ([34]에서 발췌)

LLM에 대한 최근 연구는 긴 컨텍스트 윈도우(long context windows) 생성에 중점을 두었으며, 이를 통해 모델은 각 프롬프트 내에서 더 많은 정보(예: 더 많은 예시 또는 더 많은 컨텍스트)를 처리할 수 있습니다. 그러나 보시다시피 모든 LLM이 컨텍스트에 완벽하게 주의를 기울이는 것은 아닙니다! LLM이 긴 컨텍스트 윈도우 내의 정보를 활용하는 능력은 일반적으로 "건초 더미 속 바늘 찾기 테스트(needle in the haystack test)"를 통해 평가됩니다. 이 테스트는 i) 컨텍스트 내에 무작위 사실을 삽입하고, ii) 모델에 사실을 검색하도록 요청하며, iii) 다양한 컨텍스트 길이와 컨텍스트 내 사실의 위치에 대해 이 테스트를 반복합니다. 이러한 테스트는 아래 그림과 같은 결과를 보여주며, 컨텍스트 윈도우의 결함을 쉽게 발견할 수 있습니다.

(출처)

긴 컨텍스트 윈도우의 활용 및 관리
최근 LLM들은 수십만 토큰에 달하는 방대한 컨텍스트 윈도우를 지원하며, 이는 모델이 훨씬 더 많은 정보를 한 번에 처리할 수 있게 합니다. 하지만 단순히 컨텍스트 윈도우가 길다고 해서 모든 문제가 해결되는 것은 아닙니다. "건초 더미 속 바늘 찾기" 테스트에서 드러났듯이, 모델은 컨텍스트 내의 모든 정보에 균등하게 주의를 기울이지 못하며, 관련 정보가 특정 위치에 있을 때 성능이 저하될 수 있습니다.

이러한 한계를 극복하고 긴 컨텍스트 윈도우를 효과적으로 활용하기 위해 다음과 같은 전략이 중요합니다.
*   **정보 압축 및 요약**: 긴 문서를 통째로 넣기보다는, 핵심 정보를 추출하거나 요약하여 프롬프트에 포함함으로써 모델이 중요한 내용에 집중하도록 돕습니다.
*   **동적 컨텍스트 선택**: RAG(Retrieval Augmented Generation)와 같은 기술을 활용하여 쿼리에 가장 관련성이 높은 정보 조각(chunk)만을 동적으로 검색하고 프롬프트에 삽입합니다. 최근에는 RAG-fusion과 같은 고급 RAG 기법이 등장하여, 여러 검색 결과와 모델의 내부 지식을 결합하여 더 풍부한 컨텍스트를 생성합니다.
*   **컨텍스트 재구성(Context Re-ranking)**: 검색된 컨텍스트 조각들을 모델에 전달하기 전에 관련성 점수를 기반으로 재정렬하여, 가장 중요한 정보가 모델이 더 잘 처리할 수 있는 위치(예: 시작 또는 끝)에 오도록 합니다.
*   **계층적 컨텍스트 관리**: 전체 문서를 계층적으로 분할하고, 쿼리에 따라 필요한 계층의 정보만 선택적으로 제공하는 방식입니다. 예를 들어, 큰 문서의 개요를 먼저 제공하고, 특정 질문에 대해 더 깊은 세부 정보가 필요할 때 해당 섹션을 확장하여 제공할 수 있습니다.
*   **대화 이력 요약**: 대화형 LLM의 경우, 이전 대화 이력을 단순히 누적하기보다는 주기적으로 요약하여 컨텍스트 윈도우를 효율적으로 사용하고, 모델이 대화의 핵심 흐름을 유지하도록 돕습니다.

이러한 전략들은 긴 컨텍스트 윈도우를 가진 LLM이 정보 과부하(information overload)로 인해 "중간에서 길을 잃는" 현상을 완화하고, 더 정확하고 일관된 출력을 생성하는 데 기여합니다.

나의 프롬프트 엔지니어링 전략. 프롬프트 엔지니어링의 세부 사항은 사용되는 모델에 따라 크게 다릅니다. 그러나 프롬프트 엔지니어링 프로세스를 안내하는 데 종종 유용한 몇 가지 일반적인 원칙이 있습니다.
*   **경험적이어야 합니다(Be empirical)**: 프롬프트 엔지니어링의 첫 번째 단계는 프롬프트 변경 사항을 쉽게 평가할 수 있도록 프롬프트를 평가하는 신뢰할 수 있는 방법(예: 테스트 케이스(test cases), 인간 평가자(human evaluators) 또는 LLM-as-a-judge를 통해)을 설정하는 것입니다.
*   **간단하게 시작하세요(Start simple)**: 시도하는 첫 번째 프롬프트는 연쇄 사고(chain-of-thought) 프롬프트(또는 다른 특수 프롬프팅 기술)가 아니어야 합니다. 가능한 가장 간단한 프롬프트로 시작하고, 성능 변화를 측정하면서 천천히 복잡성을 추가하여 추가 복잡성이 필요한지 여부를 판단하세요. 1 (위 참조)
*   **구체적이고 직접적이어야 합니다(Be specific and direct)**: 프롬프트의 모호성을 제거하고 LLM의 원하는 출력을 설명할 때 간결하고 직접적이며 구체적으로 표현하려고 노력하세요.
*   **예시를 사용하세요(Use exemplars)**: 원하는 출력을 설명하기 어렵다면 프롬프트에 몇 가지 예시를 추가해 보세요. 예시는 LLM에 기대되는 바에 대한 구체적인 예시를 제공하여 모호성을 제거합니다.
*   **복잡성을 피하세요 (가능하다면)(Avoid complexity (if possible))**: 복잡한 프롬프팅 전략이 때로는 필요하지만(예: 다단계 추론 문제 해결), 그러한 접근 방식을 사용하기 전에 신중하게 생각해야 합니다. 경험적이어야 하며 확립된 평가 전략을 사용하여 복잡성이 진정으로 필요한지 판단하세요.

위 내용을 요약하자면, 저의 개인적인 프롬프트 엔지니어링 전략은 i) 정말 좋은 평가 프레임워크(evaluation framework)에 투자하고, ii) 간단한 프롬프트로 시작하며, iii) 원하는 수준의 성능을 달성하기 위해 필요에 따라 천천히 복잡성을 추가하는 것입니다. 프롬프트 작성은 반복적인 과정입니다!

프롬프트 엔지니어링 워크플로우 최적화
프롬프트 엔지니어링은 단일 프롬프트를 작성하는 행위를 넘어, 지속적인 개선과 실험을 포함하는 체계적인 워크플로우입니다. 효과적인 프롬프트 엔지니어링 워크플로우는 다음과 같은 단계를 포함합니다.

1.  **목표 정의 및 평가 지표 설정**: 프롬프트가 해결해야 할 구체적인 작업과 성공을 측정할 명확한 지표(예: 정확도, 관련성, 특정 키워드 포함 여부, 사용자 만족도)를 정의합니다.
2.  **초기 프롬프트 설계**: 가장 간단한 지시 프롬프트부터 시작하여, 명확하고 구체적인 요구사항을 포함합니다. 필요한 경우 역할 부여나 출력 형식 지정 등의 기본 구조화 기법을 적용합니다.
3.  **성능 평가**: 설정된 지표를 사용하여 초기 프롬프트의 성능을 평가합니다. 소규모 테스트 세트(test set)나 인간 평가자(human evaluator)를 활용하거나, LLM-as-a-judge 기법을 사용하여 자동화된 평가를 수행할 수 있습니다.
4.  **프롬프트 개선 및 반복**: 성능이 목표에 미치지 못하면 프롬프트를 수정하고 개선합니다. 이때, 복잡성을 점진적으로 추가하는 원칙을 따릅니다. 예를 들어, 예시 추가, 연쇄 사고(CoT) 적용, 도구 사용 통합 등을 고려할 수 있습니다.
5.  **버전 관리 및 문서화**: 프롬프트의 변경 사항을 추적하고, 각 버전의 성능과 사용 사례를 문서화합니다. 이는 팀 협업을 용이하게 하고, 효과적인 프롬프트의 재사용을 가능하게 합니다.
6.  **A/B 테스트 및 최적화**: 실제 환경에서 여러 프롬프트 버전을 동시에 테스트하여 어떤 프롬프트가 가장 좋은 성능을 내는지 확인합니다. 이를 통해 지속적으로 프롬프트의 품질을 최적화합니다.

이러한 체계적인 접근 방식은 프롬프트 엔지니어링을 직관적인 예술에서 데이터 기반의 과학적 프로세스로 전환하며, LLM 기반 시스템의 견고성과 효율성을 크게 향상시킬 수 있습니다.

프롬프팅 기술(Prompting Techniques)
우리는 이전에 일련의 관련 개요를 통해 다양한 프롬프팅 기술에 대해 배웠습니다.
*   실용적인 프롬프트 엔지니어링(Practical Prompt Engineering) [링크]
*   고급 프롬프트 엔지니어링(Advanced Prompt Engineering) [링크]
*   연쇄 사고 프롬프팅(Chain of Thought Prompting) [링크]
*   프롬프트 앙상블(Prompt Ensembles) [링크]

이제 관련 프롬프팅 기술을 다시 한번 개괄하여, 이 게시물에서 나중에 소개될 더 복잡한 접근 방식의 토대를 제공할 것입니다. 그러나 이러한 각 기술에 대해 배우면서 프롬프트 엔지니어링에서 단순성의 중요성을 명심해야 합니다. 프롬프팅 기술이 더 복잡하거나 정교하다고 해서 더 간단한 전략보다 더 낫다는 의미는 아닙니다!

기본 프롬프팅 전략(Basic Prompting Strategies) ([3]에서 발췌)
제로샷 프롬프팅(Zero-shot prompting) (위 그림 참조)은 GPT-2 [2]에 의해 대중화되었으며, 우리가 사용할 수 있는 가장 기본적인 프롬프팅 전략 중 하나입니다. 제로샷 프롬프팅을 통해 작업을 해결하려면, i) 프롬프트에 작업을 설명하고 ii) 모델에 문제를 해결하도록 프롬프트를 제공하기만 하면 됩니다. 위 문제의 경우, 작업은 영어를 프랑스어로 번역하는 것이며, 우리는 "cheese =>"라는 문자열을 통해 모델에 이 번역을 수행하도록 프롬프트를 제공합니다. 이는 모델이 'cheese'라는 단어의 프랑스어 번역을 출력하도록 유도합니다. 아래에 제로샷 프롬프트의 몇 가지 예시가 제공됩니다.

제로샷 학습(Zero-shot learning) (GPT-3.5-Turbo로 생성된 출력)
제로샷 학습이 어떤 경우에는 잘 작동하지만, 작업 설명의 모호성으로 인해 한계가 있습니다. 성능은 명확하고 포괄적인 설명 생성에 달려 있으며, 우리는 이 설명만으로 모델이 올바른 출력을 생성하는 능력에 의존합니다. 종종 프롬프트에 더 구체적인 정보를 삽입함으로써 더 나은 성능을 달성할 수 있습니다.

([3]에서 발췌)
퓨샷 프롬프팅(Few-shot prompting)은 프롬프트에 올바른 문제 해결 예시를 여러 개 삽입하여 정확히 이 작업을 수행합니다. 이 전략은 GPT-3 [3]에 의해 대중화되었으며, GPT-3는 LLM이 대규모로 인상적인 퓨샷 학습(few-shot learning) 능력을 개발한다는 것을 보여주었습니다. (위 참조) 직관적으로 퓨샷 학습은 예상 출력의 여러 예시를 제공함으로써 제로샷 학습의 모호성을 제거합니다. 따라서 모델은 작업 설명에서 원하는 동작을 추론하는 대신 이러한 예시로부터 올바른 동작을 직접 이해할 수 있습니다. (아래 참조)

([3]에서 발췌)
LLM은 프롬프트 내에 제공된 이러한 예시로부터 학습할 수 있으며, 이는 일반적으로 "인컨텍스트 학습(in-context learning)"이라고 불리는 전략입니다. (아래 참조) 그러나 이러한 학습 방식은 신경망(neural network)의 일반적인 훈련과는 다릅니다. 모델의 매개변수(parameters)는 전혀 수정되지 않습니다. 대신, 우리는 프롬프트에 관련 정보를 넣고, 모델은 이 정보를 더 나은 출력을 생성하기 위한 컨텍스트(context)로 사용할 수 있습니다.

([3]에서 발췌)
실제로 퓨샷 학습을 사용할 때 적절하게 조정해야 하는 두 가지 주요 설정이 있습니다.
*   사용할 예시의 수.
*   예시 선택 전략.

사용할 올바른 예시 수를 결정하기 위해 평가 세트(evaluation set)를 사용하여 기본적인 하이퍼파라미터 튜닝(hyperparameter tuning)을 수행할 수 있습니다. 많은 논문에서 예시 선택 전략(예: 무작위 선택(random selection), 다양성(diversity), 의미론적 유사성(semantic similarity), 능동 학습(active learning) 또는 더 복잡한 지표(metrics) 기반) 2 을 탐구했습니다. 그러나 예시의 무작위 선택은 실제로 종종 효과적인 전략입니다. 이러한 전략 외에도 퓨샷 학습과 관련된 다양한 실용적인 규칙과 발견 사항이 있으며, 우리는 항상 이를 염두에 두어야 합니다 [4, 5].
*   예시의 레이블(label) 분포는— 비록 부정확하더라도 —모델이 일반적인 레이블에 편향되어 있기 때문에 모델의 답변에 영향을 미칠 수 있습니다.
*   답변은 프롬프트에서 최근에 관찰된 예시에 편향됩니다. 3
*   프롬프트 내 예시의 서식(formatting)이 중요합니다.
*   예시를 무작위로 선택하면 모델이 생성한 답변 내의 편향(예: 위치 또는 다수 레이블 편향)을 제거하는 데 도움이 될 수 있습니다.

단순함에도 불구하고 퓨샷 학습은 가장 효과적인 프롬프팅 전략 중 하나이며 실제 응용 분야에서 널리 사용됩니다.

지시 프롬프트의 몇 가지 예시 ([6]에서 발췌)
지시 프롬프팅(Instruction prompting)은 LLM의 원하는 출력을 표현하는 더 직접적인 방법입니다. 퓨샷 학습에서는 해결되는 작업의 구체적인 예시를 통해 모델에 우리의 의도를 설명하지만, 이러한 예시는 많은 토큰(token)을 소비합니다! 모델에 우리의 의도를 말로 설명하는 것이 훨씬 더 효율적일 것입니다. 이것이 잘 작동하려면 사용되는 LLM이 지시를 일관되게 따르도록 정렬(aligned)되어야 합니다. 이러한 모델은 제공된 상세한 지시를 이해하고 그에 따라 출력을 조정할 수 있기 때문에 "조종 가능(steerable)"하다고 합니다.

([6]에서 발췌)
LLM에 대한 연구는 지시 따르기 능력(instruction following capabilities) 개선에 크게 집중해 왔습니다. 사전 학습된 LLM은 기본적으로 지시를 잘 따르지 못합니다. 그러나 InstructGPT [6]가 보여주듯이, 우리는 지도 미세 조정(supervised finetuning, SFT)과 인간 피드백 기반 강화 학습(reinforcement learning from human feedback, RLHF)의 조합을 통해 모델이 지시를 훨씬 더 잘 따르도록 정렬할 수 있습니다. 위 그림에서 이 전략이 지시 따르기뿐만 아니라 LLM의 다른 주요 속성(예: 사실성(factuality) 및 제약 조건 따르기(constraint following))도 개선할 수 있음을 알 수 있습니다.

LaMDA를 사용한 역할 프롬프팅(Role prompting) ([8]에서 발췌)
LLM 정렬의 최근 발전으로 인해, 퓨샷 프롬프팅 [7]과 결합될 수도 있는 지시 프롬프팅은 실제 응용 분야에서 일반적으로 사용되는 매우 효과적인 접근 방식입니다. 사실, 몇 가지 인기 있는 프롬프팅 전략(예: 역할 프롬프팅(role prompting), 대상 지정(specifying an audience), 도구 사용(tool usage) 등)은 지시 프롬프팅의 더 구체적인 버전일 뿐입니다! 지시를 작성할 때는 최상의 결과를 보장하기 위해 명확하고 정확해야 합니다.

고급 프롬프팅 전략(Advanced Prompting Strategies)
위에서 설명한 프롬프팅 기술은 매우 효과적이지만, 때로는 더 복잡한 프롬프트가 어려운 문제(예: 수학/코딩 또는 다단계 추론 문제)를 해결하는 데 유용할 수 있습니다. LLM은 이러한 문제에 본질적으로 어려움을 겪기 때문에 4 (즉, 추론 능력은 모델 규모에 따라 단조롭게 향상되지 않습니다 [9]), 프롬프트 엔지니어링에 대한 기존 연구의 대부분은 추론 및 복잡한 문제 해결 능력 향상에 중점을 둡니다. 간단한 프롬프트는 대부분의 다른 문제를 해결하는 데 작동할 것입니다.

([10]에서 발췌)
연쇄 사고(Chain of Thought, CoT) 프롬프팅 [10]은 모델의 프롬프트 내 예시에 연쇄 사고(즉, 일련의 중간 추론 단계)를 삽입하여 LLM의 추론 능력(reasoning capabilities)을 이끌어냅니다. (위 참조) 각 예시에 연쇄 사고를 추가함으로써 모델은 (인컨텍스트 학습을 통해) 해당 문제에 대한 최종 답변을 출력하기 전에 유사한 연쇄 사고를 생성하는 방법을 학습합니다. 흥미롭게도 [10]에서 충분히 큰 모델(즉, 1000억 개 이상의 매개변수)은 산술, 상식 및 기호 추론 작업에서 이 접근 방식으로부터 큰 이점을 얻는다는 것을 알 수 있습니다. 문제를 해결하기 위한 근본적인 추론 과정을 명시적으로 설명하는 것이 실제로 모델의 추론 능력을 더 효과적으로 만듭니다.

([10]에서 발췌)
CoT 프롬프팅의 구현은 간단합니다. 각 퓨샷 예시가 입력과 출력만 가지는 대신, 예시는 (입력, 연쇄 사고, 출력) 형식의 삼중항(triplet)입니다. (위 참조) 이 접근 방식의 주요 단점은 문제 해결을 위한 완전한 근거(rationale)를 포함하는 예시를 수동으로 (또는 인위적으로) 큐레이션(curate)해야 한다는 점인데, 이는 비용이 많이 들거나 시간이 많이 소요될 수 있습니다. 따라서 많은 논문이 CoT 프롬프팅이 사람이 작성한 근거에 의존하는 것을 제거하는 데 중점을 둡니다!

CoT 변형의 실제 적용 및 최신 발전
CoT 프롬프팅의 기본 원칙은 모델이 문제 해결 과정을 단계별로 설명하도록 유도하여 투명성과 정확성을 높이는 것입니다. 이러한 아이디어는 다양한 변형으로 확장되었으며, 특히 다음과 같은 기법들은 실제 응용 분야에서 큰 효과를 보였습니다.

*   **제로샷 CoT (Zero-shot CoT) [11]**: 퓨샷 예시 없이 "단계별로 생각해 봅시다(Let’s think step by step)."와 같은 간단한 지시어를 프롬프트 끝에 추가하는 것만으로도 모델이 추론 과정을 생성하도록 유도합니다. 이는 예시를 수동으로 작성할 필요가 없어 효율적이며, 놀랍게도 많은 작업에서 상당한 성능 향상을 가져옵니다. 예를 들어, 복잡한 질문에 대한 답을 찾을 때, 모델이 단순히 최종 답을 내놓기 전에 중간 과정을 설명하게 함으로써 오류를 줄일 수 있습니다.
*   **자기 일관성(Self-consistency) [12]**: 동일한 문제에 대해 여러 개의 다른 연쇄 사고 경로를 생성하고, 이들로부터 얻은 최종 답변들 중 다수결 투표(majority vote)를 통해 가장 일관된 답변을 선택하는 방식입니다. 이는 모델의 추론 과정에 내재된 불확실성을 줄이고, 복잡한 문제에 대한 신뢰성을 높이는 데 효과적입니다. 예를 들어, 수학 문제에서 여러 풀이 과정을 생성하고 가장 흔한 답을 최종 답으로 채택하는 방식입니다.
*   **자기 반성 프롬프팅(Self-Reflection Prompting)**: 모델이 자신의 초기 출력이나 추론 과정을 비판적으로 평가하고, 개선점을 파악하여 스스로 수정하도록 유도하는 기술입니다. 이는 CoT의 확장으로 볼 수 있으며, 모델이 더 나은 해결책을 향해 반복적으로 학습하고 발전하는 인간의 인지 과정을 모방합니다. 예를 들어, 모델에게 초안을 작성하게 한 후, "이 답변에서 개선할 점은 무엇인가요? 더 정확하거나 포괄적으로 만들려면 어떻게 해야 할까요?"와 같은 질문을 통해 스스로의 출력을 검토하고 수정하게 할 수 있습니다.
*   **멀티모달 CoT 프롬프팅(Multimodal CoT Prompting) [43]**: 텍스트뿐만 아니라 이미지, 오디오 등 다양한 모달리티(modality)의 정보를 포함하는 문제에 대해 CoT를 적용하는 방식입니다. 예를 들어, 이미지와 관련된 질문에 답할 때, 모델이 이미지 내의 시각적 요소를 분석하는 단계와 텍스트 정보를 통합하여 추론하는 단계를 명시적으로 구분하여 처리하도록 유도할 수 있습니다. 이는 모델이 복잡한 멀티모달 추론 작업을 해결하는 능력을 향상시킵니다.

이러한 CoT 변형들은 LLM의 추론 능력을 강화하고, 다양한 복잡성 수준의 문제에 대한 적용 가능성을 넓히는 데 중요한 역할을 합니다.

([15]에서 발췌)
Auto-CoT [15]. CoT 프롬프팅은 복잡한 문제를 해결하기 위해 중간 추론 단계(intermediate reasoning steps)를 사용하며, LLM의 출력 내에서 이러한 추론 단계를 이끌어낼 수 있는 두 가지 방법이 있습니다(위 그림 참조).
*   **제로샷(Zero-shot)**: LLM에 "단계별로 생각하라(think step-by-step)"고 프롬프트를 제공합니다.
*   **수동(Manual)**: 원하는 질문에 답하기 전에 질문, 근거, 답변에 대한 몇 가지 퓨샷 예시를 제공합니다.

LLM이 괜찮은 제로샷 추론기(zero-shot reasoners)이지만, 구체적인 예시를 제공하는 것이 CoT 프롬프팅에서 일관되게 더 나은 성능을 제공합니다. 그러나 이 전략은 또한 인간 주석자(human annotators)— 또는 프롬프트 엔지니어 —가 각 질문에 답하는 데 사용되는 근거에 대한 수동 시연(manual demonstrations)을 작성하도록 요구합니다. 이러한 수동 시연을 작성하는 것은 시간이 많이 걸리지만, 피할 수 있습니다!

"우리는 '단계별로 생각해 봅시다(Let’s think step by step)' 프롬프트를 가진 LLM을 활용하여 시연을 위한 추론 연쇄(reasoning chains)를 하나씩 생성함으로써 그러한 수동 노력을 제거할 수 있음을 보여줍니다." - [15]에서 발췌

[15]에서 저자들은 제로샷 CoT 프롬프팅을 사용하여 수동 CoT 프롬프팅을 위한 예시를 자동으로 생성하는 자동 CoT(Auto-CoT) 프롬프팅 접근 방식을 제안하여, 문제 해결 근거를 수동으로 작성할 필요성을 없압니다. 그러나 이러한 자동으로 생성된 근거가 어떤 경우에는 부정확하기 때문에 Auto-CoT가 잘 작동하려면 몇 가지 요령이 필요합니다. 질문이 입력으로 주어졌을 때, 순진한 접근 방식은 i) 유사한 질문 세트를 검색하고(예: sBERT와 같은 임베딩 모델(embedding model) 및 벡터 검색(vector search) 사용), ii) 제로샷 CoT 프롬프팅으로 각 질문에 대한 근거/답변을 생성하며, iii) 자동으로 생성된 시연으로 수동 CoT 프롬프팅을 수행하는 것입니다. 그러나 이 접근 방식은 상당히 좋지 않게 작동하며, [1]의 저자들은 이를 LLM이 생성한 근거의 실수 때문이라고 주장합니다. 이를 해결하기 위해 생성된 근거가 충분히 다양하도록 보장하기만 하면 됩니다.

([15]에서 발췌)
LLM이 답변해야 할 질문 데이터셋이 주어졌을 때, [15]의 저자들은 Auto-CoT를 위한 프롬프트 내에서 사용되는 시연을 선택/생성하기 위한 두 부분으로 된 전략(위 그림 참조)을 고안했습니다.
*   sBERT의 질문 임베딩(question embeddings)과 k-평균 클러스터링(k-means clustering)을 사용하여 질문을 k개의 클러스터(cluster)로 나눕니다.
*   각 클러스터에서 대표 질문을 선택하고 제로샷 CoT를 사용하여 각 질문에 대한 관련 근거를 생성합니다.

이러한 접근 방식은 Auto-CoT에 사용되는 시연의 다양성을 높여, 모델이 합성 근거(synthetic rationales)에서 저지르는 실수 간의 상관관계를 줄입니다. GPT-3를 사용한 실험에서 Auto-CoT는 수동 시연 생성이 필요한 퓨샷 CoT 프롬프팅의 성능과 일치하거나 이를 능가했으며, 10개 이상의 다른 벤치마크(benchmark)에서 일관된 결과를 보였습니다.

([16]에서 발췌)
복잡성 기반 프롬프팅(Complexity-Based Prompting) [16]. CoT 프롬프팅이 프롬프트에 포함할 문제 해결 근거의 시연을 선택하는 데 의존한다는 점을 고려할 때, 우리는 궁금할 수 있습니다. 이러한 시연을 어떻게 가장 잘 선택할 수 있을까요? [16]에서 저자들은 복잡성을 기반으로 시연을 선택하는 것이 좋은 휴리스틱(heuristic)임을 보여줍니다. 시연의 복잡성은 연쇄 사고 내에 존재하는 단계의 수를 세는 것으로 간단히 측정할 수 있으며, 개별 단계는 개행 문자(newline characters, \n )로 구분됩니다. [16]에서 제안된 복잡성 기반 프롬프팅 접근 방식은 가장 높은 복잡성을 가진 시연을 샘플링(sampling)할 것을 옹호합니다.

"GPT-3 175B의 추론 성능은 입력 프롬프트 복잡성이 증가함에 따라 명확하게 향상됩니다." - [16]에서 발췌

흥미롭게도 [16]의 저자들은 CoT 프롬프트에 더 많은 추론 단계를 포함하는 시연을 포함하는 것이 다단계 추론 작업에서 성능을 상당히 향상시킨다는 것을 발견했습니다. 더 나아가, 이 전략을 출력 공간으로 확장하여 가장 높은 복잡성을 가진 k개의 생성된 출력에 대해 다수결 투표를 하는 자기 일관성(self-consistency) 접근 방식을 사용할 수 있습니다. 수동 튜닝(manual tuning) 및 검색 기반 선택(retrieval-based selection)과 같은 대안적인 선택 방식과 비교할 때, 복잡성 기반 프롬프팅은 GPT-3 및 Codex를 사용하여 여러 데이터셋(즉, GSM8K, MultiArith, MathQA)에서 최첨단(state-of-the-art) 성능을 달성하며 유리하게 작동합니다.

([17]에서 발췌)
점진적 힌트 프롬프팅(Progressive-Hint Prompting, PHP) [17]. CoT 프롬프팅의 한 가지 단점은 문제를 단일 시도(single shot)로 해결한다는 것입니다. 질문이 입력으로 주어지면 우리는 근거와 답변을 생성하지만, LLM은 이 답변을 고려하거나 수정할 기회를 얻지 못합니다. 이 과정을 여러 번 반복하고 다수결 투표를 통해 더 나은 성능을 달성할 수 있습니다— 이것은 단지 자기 일관성입니다 —그러나 이러한 생성물 중 어느 것도 답변을 더 잘 알리기 위해 LLM의 이전 출력을 고려하지 않습니다.

"PHP는 질문을 재평가한 후 이전 답변을 힌트로 활용하여 올바른 답변에 도달하는 인간과 유사한 사고 과정을 따릅니다." - [17]에서 발췌

이 문제를 해결하기 위해 [17]의 저자들은 LLM의 이전 출력을 활용하여 생성된 근거를 반복적으로 개선하는 PHP를 제안합니다. 직관적으로 LLM은 모델이 이전에 생성한 근거를 올바른 답변을 찾는 힌트로 사용할 수 있습니다. 구체적으로 PHP는 세 단계로 진행됩니다.
*   질문이 주어지면 LLM에 기본 답변을 제공하도록 프롬프트를 제공합니다.
*   질문과 기본 답변을 연결한 다음, 이 입력을 기반으로 LLM에 수정된 답변을 생성하도록 프롬프트를 제공합니다.
*   LLM의 답변이 최소 두 번의 반복 동안 안정적일 때까지 2단계를 반복합니다.

이러한 접근 방식은 LLM이 여러 번의 통과를 통해 답변을 반복적으로 개선하고, 이 과정에서 이전 출력을 컨텍스트로 사용할 수 있도록 합니다. 또한 PHP는 CoT 프롬프팅 및 자기 일관성과 완벽하게 호환됩니다. 이러한 기술을 결합하여 성능을 더욱 향상시킬 수 있습니다. 실험에서 PHP는 복잡성 기반 프롬프팅 전략과 비교하여 GPT-3.5의 성능을 향상시키며, GPT-4와 함께 PHP를 사용하면 여러 주목할 만한 데이터셋(예: SVAMP, GSM8K, AQuA, MATH)에서 최첨단 성능을 달성합니다.

([18]에서 발췌)
분해 프롬프팅(Decomposed Prompting, DecomP) [18]은 프롬프팅을 통해 복잡한 단계를 가진 다단계 추론 문제를 해결하는 어려움을 해결하려고 시도합니다. 작업이 더 복잡해질수록 퓨샷 프롬프팅(즉, 올바른 해결책의 몇 가지 예시를 보여주는 것)은 부족해질 것입니다. 그러나 복잡한 작업을 프롬프팅을 통해 독립적으로 해결할 수 있는 하위 작업(sub-tasks)으로 분해함으로써 더 잘 수행할 수 있습니다. 특히 [18]의 저자들은 두 가지 구성 요소로 이루어진 프롬프팅 프레임워크를 제안합니다.
*   **분해기(Decomposer)**: LLM에 문제를 일련의 더 간단한 하위 작업으로 분해하도록 프롬프트를 제공합니다.
*   **하위 작업 처리기(Sub-task handlers)**: 별도의 프롬프트를 사용하여 (분해기가 지시하는 대로) 더 간단한 하위 작업을 LLM으로 해결합니다.

분해기와 하위 작업 처리기는 퓨샷 방식으로 프롬프트가 제공되는 LLM일 뿐입니다. 위에서 제안된 DecomP 전략은 하나의 프롬프트를 사용하여 해결 가능한 하위 작업을 식별하고, 이를 다른 시스템(예: 새로운 프롬프트, 다른 LLM 또는 도구)에 위임하여 해결합니다. 이러한 모듈식 접근 방식은 많은 이점을 가집니다.
*   긴 컨텍스트를 가진 작업은 여러 구성 요소로 분해될 수 있습니다.
*   각 하위 작업에는 더 넓은 범위의 예시를 보여줄 수 있습니다.
*   복잡한 하위 작업은 필요한 경우 더 작은 하위 작업으로 추가 분해될 수 있습니다.
*   모든 하위 작업을 LLM으로 해결하는 대신, 다른 기호 시스템(symbolic systems)(예: 작업별 모델(task-specific model), 검색 메커니즘(retrieval mechanism) 등)을 사용할 수도 있습니다.

간단한 작업을 예로 들어 봅시다. 단어 집합이 입력으로 주어졌을 때, 각 단어의 세 번째 문자를 추출하고, 이 문자들을 연결하여 그 연결된 결과를 출력으로 제공하고자 합니다. 이를 위해 세 가지 하위 작업의 시퀀스를 만들 수 있습니다. i) 단어 목록 수집, ii) 각 단어의 세 번째 문자 추출, iii) 추출된 문자 연결. 아래 그림과 같이 이러한 각 하위 작업을 별도의 퓨샷 프롬프트로 구현할 수 있습니다.

([18]에서 발췌)
DecomP 내에서 하위 작업은 분해기에 의해 반복적으로 생성되고, 해결되며, (관련 출력과 함께) 분해기로 반환되어 다음 하위 작업을 생성합니다. 분해기는 추론 프로세스의 컨트롤러(controller) 역할을 하면서 최종 답변이 생성되었음을 나타내는 질문 끝(end-of-question, [EOQ]) 마커가 생성될 때까지 하위 작업을 계속 생성합니다. (아래 참조) 전반적으로 DecomP는 최소-최대 프롬프팅의 더 일반적이고 유연한 버전으로 생각할 수 있습니다.

([18]에서 발췌)
가설-이론(Hypotheses-to-Theories) [29]. 복잡한 작업을 간단한 단계로 분해하는 예시 근거로 모델에 프롬프트를 제공함으로써 LLM 내에서 추론 능력을 이끌어낼 수 있습니다. 그러나 모델은 출력을 생성할 때 환각(hallucinate)을 일으킬 수 있으며, 기존 또는 상식적 지식을 넘어선 작업에서는 성능이 좋지 않습니다. 간단히 말해, LLM의 지식 기반(knowledge base)과 작업을 해결하는 데 필요한 지식 사이에 불일치(mismatch)가 있을 때 문제가 발생합니다. 이 문제를 해결하기 위해, 복잡한 추론 문제를 해결할 때 LLM이 필요한 지식을 발견하고 적용할 수 있도록 하는 프롬프팅 접근 방식이 필요합니다.

([29]에서 발췌)
인간의 과학적 발견 과정에서 영감을 받아, [29]의 저자들은 가설-이론(Hypotheses-to-Theories, HtT) 프롬프팅이라는 프롬프팅 기술을 제안합니다. 이 기술은 (잠재적으로 부정확한) 가설을 자유롭게 제안하고, 경험적으로 검증될 수 있는 가설만을 유지하며, 이 검증된 가설을 사용하여 문제를 해결하는 전략을 따릅니다. 높은 수준에서 이 전략의 목표는 문제 해결에 사용될 수 있는 LLM을 위한 규칙 라이브러리(rule library)를 학습하는 것입니다. 더 구체적으로, HtT 프롬프팅(위 그림 참조)은 두 단계로 구성됩니다.
*   **귀납(Induction)**: LLM은 훈련 예시(training examples) 세트에 대해 규칙을 생성하고 검증하도록 요청받습니다. 자주 나타나고 올바른 답변을 자주 생성하는 규칙들이 수집되어 규칙 라이브러리를 형성합니다.
*   **연역(Deduction)**: LLM은 귀납을 통해 생성된 규칙 세트를 사용하여 추론을 수행하고 질문에 답하도록 프롬프트가 제공됩니다.

추론 중에 규칙 세트를 사용함으로써 HtT 프롬프팅은 환각의 가능성을 줄입니다. 이러한 발견은 수치 추론(numerical reasoning) 및 관계 추론(relational reasoning) 작업 모두에서 검증되었으며, HtT 프롬프팅은 이전 프롬프팅 기술(예: CoT 프롬프팅)에 비해 정확도에서 11-27%의 절대적인 개선을 제공하는 것으로 나타났습니다. 흥미롭게도 HtT 프롬프팅으로 생성된 규칙은 해석 가능(interpretable)하며 다른 (그러나 유사한) 문제로도 전이 가능(transferable)합니다.

사고의 나무(Tree of thoughts, ToT) 프롬프팅 [14]. CoT 프롬프팅과 같은 기술은 다음 토큰 예측(next-token prediction)을 사용하여 단일 시도에서 해결책을 출력하는 왼쪽-오른쪽 생성(left-to-right generation) 접근 방식을 따릅니다. 이러한 접근 방식은 특정 시나리오에서는 효과적이지만, 광범위한 계획, 전략적 미리 보기(strategic lookahead), 백트래킹(backtracking), 그리고 수많은 실행 가능한 해결책의 병렬 탐색(exploration)으로부터 이점을 얻을 수 있는 복잡한 문제를 해결하는 데 실패할 수 있습니다. 여기에 ToT 프롬프팅이 등장합니다! ToT 프롬프팅은— 최소-최대 프롬프팅 [13]과 다소 유사하게 —복잡한 문제를 개별적으로 해결할 수 있는 일련의 더 간단한 문제(또는 "사고(thoughts)")로 분해합니다.

([14]에서 발췌)
CoT 프롬프팅과 달리 ToT 프롬프팅은 문제를 해결할 때 단일 사고 경로를 따를 것을 요구하지 않습니다. 또한 ToT 프롬프팅은 자기 일관성처럼 여러 추론 경로의 다수결 투표를 단순히 취하지 않습니다. (위 참조) 탐색(exploration) 과정에서 LLM은 많은 사고를 생성하고 자연어를 통해 최종 해결책을 향한 진행 상황을 지속적으로 평가합니다(즉, 우리는 모델에 프롬프트를 제공하기만 하면 됩니다!). 모델이 최종 해결책을 향한 자체 진행 상황을 자체 평가하는 것을 활용함으로써, 우리는 널리 사용되는 탐색 알고리즘(예: 너비 우선 탐색(breadth-first search) 또는 깊이 우선 탐색(depth-first search))으로 탐색 프로세스를 강화하여 문제 해결 과정 내에서 미리 보기와 백트래킹을 수행할 수 있습니다. ToT 프롬프팅에 대한 더 자세한 설명은 이 개요를 참조하세요.

([35]에서 발췌)
사고 그래프(Graph of Thoughts, GoT) 프롬프팅 [35, 36]. 후속 연구는 ToT 프롬프팅에 대한 연구를 추론을 위한 그래프 기반 전략으로 일반화했습니다. 전반적으로 이러한 기술은 ToT 프롬프팅과 유사하지만, 최종 해결책을 생성하는 데 사용되는 사고 경로가 선형(linear)이라는 가정을 하지 않습니다. 오히려 해결책을 도출할 때 사고를 재사용하거나 여러 사고의 시퀀스를 통해 재귀(recurse)할 수도 있습니다. (위 참조) 여러 그래프 기반 프롬프팅 전략이 제안되었습니다 (자세한 내용은 여기 참조) [35, 36]. 그러나 이러한 프롬프팅 기술은— ToT 프롬프팅과 마찬가지로 —실용성이 부족하다는 비판을 받아왔습니다. 즉, GoT 프롬프팅으로 추론 문제를 해결하려면 LLM으로부터 엄청난 수의 추론 단계(inference steps)가 필요할 수 있습니다!

도구 사용(Tool Usage)
LLM은 강력하지만, 주목할 만한 한계가 있습니다! 예를 들어, LLM은 산술적 실수를 저지르고, 최신 정보에 접근할 수 없으며, 심지어 시간의 흐름을 이해하는 데 어려움을 겪기도 합니다. 인류의 많은 발전은 새롭고 혁신적인 도구(예: 인쇄기 또는 컴퓨터)에 대한 접근에 의해 촉진되었으며, LLM도 마찬가지일 수 있습니다. 즉, 외부의 전문화된 도구(예: 계산기 또는 검색 엔진) 세트에 대한 접근 권한을 부여하고, 언제, 어디서, 어떻게 이러한 도구를 적절하게 호출하여 문제를 더 안정적으로 해결할 수 있는지 모델에 가르침으로써 이러한 모델의 많은 한계를 해결할 수 있습니다. 더 많은 정보는 아래의 이 주제에 대한 이전 개요를 참조하세요.
*   언어 모델에게 도구 사용법 가르치기(Teaching Language Models to Use Tools) [링크]
*   언어 모델과 친구들(Language Models and Friends) [링크]
*   언어 모델이 자체 도구를 만들 수 있을까?(Can language models make their own tools?) [링크]

([32]에서 발췌)
Toolformer [32]는 LLM과 외부 도구의 통합을 탐구한 최초의 연구 중 하나였습니다. 이러한 도구는 간단하고 고정된 텍스트-투-텍스트 API(text-to-text API) 세트를 통해 모델에 제공됩니다. (위 참조) 도구를 사용하기 위해 LLM은 i) 도구가 필요한 시나리오를 식별하고, ii) 사용할 도구를 지정하며, iii) 도구의 API에 관련 텍스트 입력을 제공하고, iv) API에서 반환된 텍스트를 사용하여 응답을 작성하는 방법을 학습해야 합니다. LLM은 초기 시드 데이터셋(seed dataset)으로 시작하여 더 강력한 LLM(예: GPT-4)을 사용하여 유효한 API 호출(API calls) 예시를 데이터에 추가하는 합성 훈련 데이터셋(synthetic training dataset)을 구성함으로써 이러한 기술을 학습합니다. (아래 참조)

([32]에서 발췌)
여기에서 우리는 이 데이터에 대해 LLM을 간단히 미세 조정할 수 있습니다. 모델은 자신이 생성하는 텍스트 시퀀스 내에서 필요한 API 호출을 직접 생성하고 처리하는 방법을 학습할 것입니다. 이 경우, 텍스트 입력과 출력을 가진 API만 고려하기 때문에 API 호출을 인라인(inline) 방식으로 처리하는 것은 간단합니다. (아래 참조)

([32]에서 발췌)

도구 사용의 최신 동향 및 과제
Toolformer의 등장은 LLM이 외부 도구를 활용하여 자체 한계를 극복할 수 있다는 가능성을 보여주었습니다. 이후 이 분야는 빠르게 발전하여, LLM이 단순한 도구 사용을 넘어 복잡한 에이전트(agent) 역할을 수행하며 다단계 작업을 자율적으로 처리하는 방향으로 나아가고 있습니다.

*   **에이전트 시스템(Agentic Systems)의 부상**: AutoGPT, BabyAGI와 같은 에이전트 시스템은 LLM을 핵심 컨트롤러로 사용하여, 목표를 설정하고, 계획을 수립하고, 도구를 호출하며, 자체적으로 계획을 수정하는 방식으로 동작합니다. 이는 LLM이 단순히 프롬프트에 응답하는 것을 넘어, 능동적으로 문제를 해결하는 주체가 되게 합니다. 예를 들어, "최신 기술 동향에 대한 보고서를 작성하고, 관련 데이터를 웹에서 검색한 후 요약해 줘"와 같은 복잡한 요청을 처리할 수 있습니다.
*   **동적 도구 검색 및 통합**: Gorilla [30]와 같은 연구는 LLM이 방대한 온라인 API 라이브러리에서 필요한 도구를 스스로 검색하고 사용하는 능력을 탐구합니다. 이는 고정된 도구 세트에 의존하는 것보다 훨씬 유연하며, 모델이 새로운 도메인과 작업에 빠르게 적응할 수 있게 합니다.
*   **도구 오케스트레이션(Tool Orchestration)의 복잡성**: LLM이 여러 도구를 동시에 또는 순차적으로 사용하는 경우, 도구 간의 의존성 관리, 오류 처리, 그리고 최적의 도구 사용 순서 결정은 중요한 과제입니다. 복잡한 워크플로우(workflow)에서는 도구 호출의 실패가 전체 작업의 실패로 이어질 수 있으므로, 견고한 오케스트레이션 메커니즘(orchestration mechanism)이 필수적입니다.
*   **안전 및 보안 문제**: 외부 도구에 접근하는 LLM은 잠재적으로 악의적인 웹사이트에 접근하거나, 민감한 정보를 노출하거나, 시스템에 해를 끼칠 수 있는 코드를 실행할 위험이 있습니다. 따라서 도구 사용 시 엄격한 보안 프로토콜(security protocol)과 샌드박스(sandbox) 환경이 필수적입니다.
*   **API 환각(API Hallucination)**: LLM이 실제로는 존재하지 않는 API 호출을 생성하거나, 올바르지 않은 매개변수(parameter)로 API를 호출하는 경우가 발생할 수 있습니다. 이를 줄이기 위해 모델을 API 사용 예시로 미세 조정하거나, API 호출 전에 유효성 검사(validation) 단계를 추가하는 등의 노력이 필요합니다.

이러한 도전 과제에도 불구하고, LLM과 외부 도구의 결합은 LLM의 능력을 무한히 확장할 수 있는 가장 유망한 연구 방향 중 하나로 남아 있습니다.

"LLM은 최신 정보에 접근할 수 없거나 정밀한 수학적 추론을 수행할 수 없는 것과 같은 본질적인 한계에 직면합니다... 실제 작업 해결을 위해 외부 도구를 자동으로 구성하는 기능을 현재 LLM에 강화하는 것은 이러한 단점을 해결하는 데 중요합니다." - [19]에서 발췌

Chameleon [19]은 위에서 언급된 LLM의 한계를 완화하는 것을 목표로 합니다. 흥미롭게도, 이러한 한계 중 일부는 LLM과 외부 도구를 통합하는 기존 연구에서 다루어지지 않습니다. 사용되는 도구 세트가 일반적으로 고정되어 있거나(또는 도메인 특정적(domain-specific)) 항상 새로운 도메인(domain)으로 일반화될 수 없기 때문입니다. 더 일반적인 프레임워크를 만들기 위해 Chameleon은 "플러그 앤 플레이(plug-and-play)" 전략을 사용합니다. 이 전략은 중앙 LLM 기반 컨트롤러(controller)를 사용하여 복잡한 추론 작업을 해결하기 위해 여러 도구를 구성하는 프로그램— 자연어로 작성된 —을 생성합니다. (아래 참조) 이전 연구와 달리 Chameleon이 사용할 수 있는 도구는 상당히 포괄적입니다. 예를 들어, LLM, 상용 비전 모델(off-the-shelf vision models), 웹 검색 엔진(web search engines), Python 함수 등입니다.

([19]에서 발췌)
Chameleon 프레임워크는 두 가지 주요 구성 요소를 가집니다.
*   **플래너(Planner)**: 입력 쿼리(input query)를 사용 가능한 도구를 통해 해결할 수 있는 하위 작업으로 분해합니다.
*   **모듈 인벤토리(Module inventory)**: Chameleon이 사용할 수 있는 작업별 도구(설명 및 사용 예시 포함) 세트.

LLM으로 구현된 플래너는 자연어를 사용하여 외부 도구(예: image_captioner 또는 query_generator)에 대한 호출을 생성합니다. 우리는 간단한 문자열 일치(string matching)를 통해 이러한 도구를 식별할 수 있으며, 플래너가 출력하는 도구 시퀀스는 해당 작업별 모듈(task-specific modules) 각각을 호출하여 실행될 수 있는 자연어 프로그램(natural language program)을 형성합니다. 플래너와 작업별 모듈에 사용되는 프롬프트의 예시는 아래에 나와 있습니다.

([19]에서 발췌)
컨트롤러에게 특정 도구를 언제 사용할지 가르치기 위해, 우리는 퓨샷 프롬프트 내에 도구 설명과 사용 예시를 포함하며, 이는 새로운 도구와 모듈로 쉽게 확장될 수 있습니다. 플래너의 인컨텍스트 학습 능력을 활용하여 해결책을 생성하기 때문에, 실제 쿼리를 해결하는 데 훈련이나 큐레이션된 규칙이 필요하지 않습니다. 대신, 우리는 LLM에 사용 가능한 도구의 예시를 제공하기만 하면, LLM은 이 정보를 사용하여 쿼리에 대한 올바른 최종 응답을 생성하기 위해 실행될 수 있는 도구 시퀀스를 추론할 수 있습니다. 더 나아가, 이 도구 시퀀스는 사람이 읽을 수 있으며 인간 사용자가 쉽게 디버깅(debug)할 수 있습니다.

([19]에서 발췌)
실험에서 Chameleon은 GPT-4를 사용하여 두 가지 복잡한 다중 모달(multi-modal)(즉, 텍스트와 이미지가 모두 포함됨) 추론 작업인 ScienceQA와 TabMWP에 적용되었습니다. Chameleon은 ScienceQA에서 86.54%의 새로운 최첨단 성능을 달성하여 GPT-4 및 GPT-3를 사용한 CoT 프롬프팅보다 각각 2.55% 및 11.37% 더 나은 성능을 보였습니다. TabMWP에서도 Chameleon은 98.78%의 정확도를 달성하며 유사한 개선을 보였습니다. 그러나 Chameleon의 효과는 GPT-4가 복잡한 추론 문제를 해결하기 위한 제약 조건을 추론하고 합리적/일관된 계획을 구성하는 능력에 의해 강화된다는 점에 유의해야 합니다.

"우리는 고급 LLM의 자기 지시(self-instruct)를 통해 오픈 소스 LLM에 도구를 사용할 수 있는 능력을 부여하도록 설계된 간단하면서도 효과적인 방법인 GPT4Tools를 제안합니다." - [20]에서 발췌

GPT4Tools [20]. 다양한 논문에서 LLM이 퓨샷 방식으로 도구를 활용하는 능력을 보여주었지만, 대부분의 논문은 독점적인 언어 모델(proprietary language models)에 의존하며 도구 사용을 용이하게 하기 위해 순전히 프롬프트 엔지니어링을 활용합니다. 이는 오픈 LLM으로도 유사한 결과를 재현할 수 있는지 궁금하게 만듭니다. [20]에서 저자들은 자기 지시(self-instruct) [21]를 사용하여 오픈 소스 LLM(예: LLaMA 및 OPT)이 다중 모달 도구(multimodal tools) 세트를 사용할 수 있도록 하는 미세 조정 데이터셋(finetuning dataset)을 생성하는 접근 방식을 제안합니다.

([21]에서 발췌)
먼저, 저자들은 강력한 교사 모델(teacher model)(즉, ChatGPT)에 프롬프트를 제공하여 관련 도구가 사용되는 예시를 생성하도록 함으로써 자기 지시 접근 방식을 사용하여 도구 사용 데이터셋을 생성합니다. 프롬프트 내에는 시각적 콘텐츠— 이미지에서 추출된 캡션(captions) 및 바운딩 박스(bounding boxes) —와 도구 설명이 모두 포함됩니다. 교사는 이 정보를 활용하여 다중 모달 정보를 처리하고 문제를 해결하는 데 사용될 수 있는 도구 관련 지시를 생성합니다. (위 참조)

([21]에서 발췌)
데이터셋이 생성되면, 저랭크 적응(Low-Rank Adaptation, LoRA)을 사용하여 오픈 소스 LLM을 쉽게 미세 조정하여 다중 모달 도구의 도움을 받아 다양한 시각적 문제를 해결할 수 있습니다. [20]에서 이 접근 방식은 LLM이 알려진 도구(즉, 미세 조정 데이터셋에 포함된 도구)에 대해 수행하는 호출의 정확도를 향상시킬 뿐만 아니라, 모델이 제로샷 방식으로 새로운 도구에 일반화하는 능력도 향상시키는 것으로 나타났습니다. GPT4Tools와 LLM을 외부 도구와 통합하는 이전 연구에 대한 직접적인 비교는 위 표에 제공됩니다.

([30]에서 발췌)
Gorilla [30]. 많은 연구에서 LLM과 고정된 도구 세트를 통합하는 것을 연구했지만, [30]의 저자들은 LLM에게 온라인에서 사용 가능한 모든 모델 API를 사용하도록 가르치는 더 넓은 목표를 다룹니다. 이를 위해 i) 문제 해결과 관련된 모델 API를 검색하고 ii) 이 API에 대한 문서를 모델의 컨텍스트에 추가하는 검색 기술(retrieval technique)이 채택됩니다. 이러한 접근 방식은 LLM이 엄청난 수의 변화하는 도구에 접근할 수 있도록 하지만, 환각(예: 잘못된 인수(arguments) 또는 존재하지 않는 API 호출)이 여전히 발생할 수 있습니다. (위 참조)

([30]에서 발췌)
이 문제를 해결하기 위해 [30]의 저자들은 자기 지시 [21]를 사용하여 1,600개 이상의 다른 모델 API 사용 예시가 포함된 데이터셋을 구축합니다. 각 예시 내에서 프롬프트와 관련 문서가 모두 컨텍스트로 사용되어 출력을 생성합니다. 다시 말해, 이것은 검색 인식 미세 조정(retrieval-aware finetuning) 프로세스입니다 (RAFT와 유사). (위 참조) 그 결과 모델인 Gorilla (LLaMA-7B의 미세 조정 버전)는 다양한 딥러닝 모델 API를 활용하여 문제를 해결하기 위한 인터페이스입니다. 결과 LLM은 엄청난 수의 API를 사용할 수 있으며, 심지어 이러한 API 중 어느 하나의 문서 변경에도 적응할 수 있습니다!

([31]에서 발췌)
HuggingGPT [31]는 도구 사용 접근 방식을 통해 LLM과 특수 딥러닝 모델(예: 이미지 인식, 비디오 감지, 텍스트 분류 등)의 통합을 탐구한다는 점에서 Gorilla와 상당히 유사합니다. LLM은 문제 해결 시스템의 "두뇌" 역할을 하며, 문제를 해결하는 방법을 계획하고 이 문제에 필요한 하위 작업을 해결하는 다양한 딥러닝 모델 간의 노력을 조정합니다. 그러나 Gorilla와 달리 HuggingGPT는 미세 조정을 수행하지 않습니다. 문제 해결은 네 단계로 분해됩니다.
*   **작업 계획(Task planning)**: LLM을 사용하여 사용자의 요청을 해결 가능한 작업으로 분해합니다.
*   **모델 선택(Model selection)**: HuggingFace에서 작업을 해결하는 데 사용할 모델을 선택합니다.
*   **작업 실행(Task execution)**: 선택된 각 모델을 실행하고 결과를 LLM에 반환합니다.
*   **응답 생성(Response generation)**: LLM을 사용하여 사용자에게 최종 응답을 생성합니다.

이러한 각 단계에 대해, 우리는 큐레이션된 지시와 예시를 포함하는 프롬프팅을 활용하여 원하는 동작을 얻습니다. (예시 프롬프트는 아래 참조) 충분히 강력한 파운데이션 모델(foundation model)이 주어진다면, 이러한 접근 방식은 매우 효과적입니다.

([31]에서 발췌)

프로그램 지원 언어 모델(Program-Aided Language Models)
"계산은 생성된 프로그램을 실행하는 데 사용되는 프로그램 인터프리터(program interpreter)에 위임될 수 있으며, 이로써 복잡한 계산을 추론 및 언어 이해와 분리합니다." - [41]에서 발췌

LLM을 외부 도구와 통합하는 것은 흥미로운 연구 분야이며, 이러한 모델에 접근 권한을 부여할 수 있는 가장 유용한 도구 중 하나는 프로그램을 작성하고 실행하는 능력입니다. 대부분의 프롬프팅 기술은 복잡한 문제를 두 단계로 해결합니다.
*   문제 해결 근거를 생성합니다.
*   이 근거를 사용하여 실제로 문제를 해결합니다.

CoT 프롬프팅에서는 이 두 단계를 모두 LLM이 해결하도록 의존하지만, 이 모델들은 첫 번째 단계 해결에만 탁월합니다! 사실, 올바른 근거를 출력했음에도 불구하고 잘못된 답변을 생성하는 것은 LLM의 흔한 실패 사례입니다. 이 문제를 해결하기 위해, 모델이 언어와 코드(예: 유용한 주석이 있는 Python 프로그램)가 섞인 형태로 근거를 출력하도록 가르칠 수 있습니다. 그런 다음, 제공된 코드를 단순히 실행함으로써 최종 답변을 생성할 수 있습니다!

([40]에서 발췌)
프로그램 지원 언어 모델(Program-Aided Language Model, PAL) [40]은 LLM이 해결책을 찾기 위해 문제를 일련의 중간 단계로 분해하는 작업을 맡는다는 점에서 CoT 프롬프팅과 유사합니다. 그러나 이 근거는 자연어와 프로그래밍 구성 요소(programatic components)를 모두 포함합니다. 우리는 근거에서 코드를 실행하여(샌드박스(sandboxed) Python 환경 사용) 신뢰할 수 있는 최종 해결책을 생성할 수 있습니다. 실제 해결책 생성 과정은 코드 인터프리터(code interpreter)에 위임됩니다. [40]에서 우리는 코드에 대해 충분히 훈련된 LLM(예: Codex)이 퓨샷 학습 접근 방식을 사용하여 이러한 방식으로 문제를 해결하도록 가르칠 수 있음을 알 수 있습니다.

"이는 연쇄 사고와 유사한 방법에서 추론 연쇄는 올바르지만 잘못된 답변을 생성할 수 있는 중요한 격차를 해소합니다." - [40]에서 발췌

([41]에서 발췌)
사고 프로그램(Program of Thoughts, PoT) 프롬프팅 [41]은 i) 코드 증강 프롬프팅 기술(code-augmented prompting technique)을 사용하고 ii) 해결책을 도출하는 과정을 코드 인터프리터에 위임한다는 점에서 PAL과 상당히 유사합니다. 이 과정은 퓨샷 프롬프팅 전략에 의존합니다. (위 참조) 그러나 PAL과 달리 PoT가 작성한 코드는 SymPy라는 기호 수학 라이브러리(symbolic math library)에 의존합니다. 이 패키지는 사용자가 수학적 "기호"를 정의할 수 있게 하며, 이 기호들은 SymPy의 solve 함수를 통해 평가되는 복잡한 표현식을 형성하도록 결합될 수 있습니다. (아래 참조)

([41]에서 발췌)
높은 수준에서 PoT는 LLM이 복잡한 방정식을 해결할 수 없다는 점을 직접적으로 다루며, 이러한 방정식을 쉽게 구성/평가할 수 있도록 하는 기호 수학 라이브러리에 대한 접근을 제공합니다. 반면 PAL은 자연어와 코드의 조합을 통해 문제를 더 일반적으로 해결하는 데 중점을 둡니다. 프로그램 지원 모델에 대한 더 많은 정보는 이 관련 개요를 참조하세요.

PAL 및 PoT의 확장된 활용
PAL과 PoT는 LLM의 추론 능력과 외부 계산 도구의 정확성을 결합하여 복잡한 문제 해결의 새로운 지평을 열었습니다. 이러한 접근 방식은 단순히 수치 계산을 넘어 다양한 분야로 확장될 수 있습니다.

*   **데이터 분석 및 처리**: LLM이 데이터를 분석하고 변환하는 파이썬(Python) 스크립트(script)를 생성하도록 프롬프트를 제공할 수 있습니다. 예를 들어, 특정 조건에 맞는 데이터를 필터링하거나, 통계 분석을 수행하거나, 복잡한 데이터 시각화 코드를 작성하여 실행하도록 할 수 있습니다. 이를 통해 LLM은 데이터 과학자와 유사한 방식으로 작동할 수 있습니다.
*   **복잡한 논리적 추론 및 검증**: 법률 문서 분석, 소프트웨어 설계 검증, 복잡한 규정 준수(compliance) 확인과 같은 작업에서 LLM이 논리적 규칙(logical rules)을 코드로 변환하고 이를 실행하여 결과를 검증하도록 할 수 있습니다. 이는 LLM의 추론 오류를 줄이고 신뢰성을 높입니다.
*   **코드 생성 및 디버깅(Debugging)**: LLM이 특정 기능을 수행하는 코드를 생성하고, 이 코드를 인터프리터(interpreter)에서 실행하여 오류를 확인하며, 필요에 따라 디버깅 코드를 작성하여 수정하도록 할 수 있습니다. 이는 소프트웨어 개발 프로세스에서 LLM의 역할을 확장합니다.
*   **과학적 시뮬레이션 및 모델링**: LLM이 과학적 모델을 구축하거나, 시뮬레이션(simulation) 코드를 작성하고, 그 결과를 분석하도록 돕는 데 사용될 수 있습니다. 예를 들어, 물리학이나 화학 분야에서 복잡한 시스템의 동작을 예측하는 코드를 생성하고 실행하는 데 활용될 수 있습니다.

이러한 프로그램 지원 언어 모델은 LLM이 단순히 텍스트를 생성하는 것을 넘어, 실제 세계의 문제를 해결하는 강력한 컴퓨팅 에이전트(computing agent)로 진화할 수 있음을 보여줍니다. 이는 LLM의 한계를 극복하고, 더욱 견고하고 신뢰할 수 있는 AI 시스템을 구축하는 데 핵심적인 역할을 합니다.

컨텍스트 윈도우 이해 및 사용(Understanding and Using the Context Window)
RAG의 최근 인기와 최첨단 LLM 내의 긴 컨텍스트 윈도우에 대한 강조를 고려할 때, 이러한 모델이 프롬프트에 제공된 컨텍스트를 어떻게 처리하는지 이해하는 것이 중요합니다. 다행히도 최근 연구는 컨텍스트 윈도우와 인컨텍스트 학습 주제를 심층적으로 연구하여 프롬프트 엔지니어링과 관련된 몇 가지 흥미로운 시사점을 도출했습니다.

대규모 언어 모델은 관련 없는 컨텍스트에 쉽게 주의가 산만해질 수 있습니다 [22]. 언어 모델에 프롬프트를 제공할 때, 우리는 일반적으로 프롬프트 내에 관련 컨텍스트와 정보만 포함합니다. 그러나 실제 응용 분야에서는 모델의 프롬프트가 일반적으로 해결되는 특정 문제와 관련이 있을 수도 있고 없을 수도 있는 컨텍스트적으로 유사한 정보를 포함합니다. 이를 염두에 두고, 우리는 궁금할 수 있습니다. 프롬프트에 관련 없는 컨텍스트를 추가하는 것이 부정적인 부작용을 가져올까요?

([22]에서 발췌)
[22]에서 저자들은 현대 LLM의 주의 산만성(distractibility)을 연구했으며, 관련 없는 컨텍스트가 프롬프트에 포함될 때 이러한 모델의 성능이 급격히 저하될 수 있음을 발견했습니다. LLM 주의 산만성을 측정하기 위해 저자들은 문제 설명에 관련 없는 정보가 포함된 산술 추론 문제(arithmetic reasoning problems)를 포함하는 새로운 관련 없는 컨텍스트를 포함한 초등학교 수학(Grade-School Math with Irrelevant Context, GSM-IC) 데이터셋을 소개합니다. (위 참조) 그런 다음, 모델의 프롬프트에 관련 없는 문장을 추가하는 것이 문제의 결과 해결책을 변경하는지 여부를 테스트함으로써 LLM이 관련 없는 컨텍스트에 의해 주의가 산만해지는지 여부를 측정할 수 있습니다.

([22]에서 발췌)
이 전략은 Codex와 GPT-3.5를 여러 다른 프롬프팅 기술(그림은 위 참조)로 테스트하는 데 사용됩니다.
*   CoT 프롬프팅 (및 제로샷 CoT 프롬프팅)
*   최소-최대 프롬프팅
*   프로그램으로 프롬프팅

흥미롭게도, 관련 없는 정보가 컨텍스트에 포함될 때 이러한 모델의 성능은 급격히 저하됩니다. 그러나 관련 없는 컨텍스트의 영향은 i) 자기 일관성 사용, ii) 모델이 관련 없는 정보를 무시하도록 지시 추가, iii) 관련 없는 정보로 문제를 해결하는 것을 시연하는 퓨샷 예시 포함을 통해 완화될 수 있습니다. LLM은 지시 또는 컨텍스트를 통해 정보를 무시하는 것을 학습할 수 있습니다.

"우리는 예시에 '문제 설명의 관련 없는 정보는 자유롭게 무시하세요'라는 지시 문장을 앞에 추가합니다." - [22]에서 발췌

중간에서 길을 잃다(Lost in the Middle) [23]. 생성형 LLM은 텍스트-투-텍스트 형식을 가지며, 이는 텍스트 시퀀스를 입력(즉, 프롬프트)으로 받아들이고 해당 텍스트 시퀀스를 출력으로 생성한다는 의미입니다. LLM에 전달되는 입력은 가변 길이(variable length)입니다. 짧은 (제로샷) 문제 설명일 수도 있고, 많은 양의 외부 컨텍스트(예: RAG용)를 포함하는 복잡한 지시일 수도 있습니다. 이러한 이유로 LLM은 긴 컨텍스트에서 작동하고 이 컨텍스트 전체를 사용하여 다운스트림 작업(downstream tasks)을 효과적으로 해결할 수 있어야 합니다. 이러한 맥락에서 [23]의 저자들은 여러 LLM— 오픈 모델(MPT)과 폐쇄 모델(GPT-3.5-Turbo 및 Claude-1.3) 모두 —이 긴 컨텍스트 내에서 제공된 정보를 구체적으로 활용하는 능력을 연구합니다. 특히 [23]에서는 두 가지 유형의 작업이 연구됩니다.
*   **다중 문서 QA(Multi-document QA)**: 표준 RAG 설정과 유사하게, 이 문제는 모델이 여러 문서를 추론하여 질문에 답하도록 요구합니다.
*   **키-값 검색(Key-value retrieval)**: 이는 컨텍스트로 제공된 JSON 키-값 쌍(key-value pairs) 컬렉션에서 키와 관련된 값을 반환하여 일치하는 토큰을 검색하는 모델의 능력을 테스트하는 합성 작업입니다.

이러한 작업을 해결할 때 저자들은 i) 입력 컨텍스트의 길이(더 많은 문서 또는 키-값 쌍 사용)와 ii) 입력 내 관련 컨텍스트의 위치— 시작, 중간, 끝 —를 모두 제어합니다. 그런 다음, 컨텍스트 길이와 위치 변화가 모델 성능에 미치는 영향을 연구할 수 있습니다. 실험에서 우리는 모델 컨텍스트 내 관련 정보의 위치를 기반으로 명확한 "U자형" 성능 곡선(아래 그림 참조)을 볼 수 있습니다.

([23]에서 발췌)
이 시각화는 LLM이 컨텍스트의 시작과 끝에 있는 정보에 가장 많은 주의를 기울인다는 것을 보여줍니다. 관련 정보가 컨텍스트의 중간에 있을 때 모델 성능은 크게 저하됩니다— 정보가 "중간에서 길을 잃습니다(lost in the middle)". 사실, GPT-3.5-Turbo는 관련 문서가 컨텍스트 중간에 배치될 때보다 관련 컨텍스트가 전혀 없을 때 다중 문서 QA 작업에서 더 나은 성능을 보입니다. 관련 정보의 위치를 조정함에 따라 성능은 크게 달라지며, 확장된 컨텍스트를 가진 모델은 이러한 위치 편향(positional biases)에 대한 견고성 개선 징후를 보이지 않습니다. (아래 참조) 그러나 이러한 문제는 더 최근 모델(예: Gemini-1.5 및 Claude-3)에서 개선되었습니다.

([23]에서 발췌)

"중간에서 길을 잃다" 문제 극복 및 RAG의 진화
"중간에서 길을 잃다" 현상은 LLM이 긴 컨텍스트 내의 모든 정보를 균등하게 활용하지 못하고, 특정 위치(특히 중간)에 있는 정보를 간과하는 경향을 보여주었습니다. 이는 LLM의 컨텍스트 윈도우가 아무리 길어져도 여전히 효과적인 정보 관리 전략이 필요함을 시사합니다. 최근 LLM 모델(예: Gemini 1.5 Pro, Claude 3 Opus)들은 이러한 위치 편향을 상당 부분 완화했지만, 여전히 최적의 성능을 위해서는 적극적인 컨텍스트 관리 기법이 요구됩니다.

*   **고급 RAG 아키텍처(Advanced RAG Architectures)**: 단순한 "검색 후 생성"을 넘어, RAG는 다음과 같은 방식으로 진화하고 있습니다.
    *   **하이브리드 검색(Hybrid Search)**: 키워드 기반 검색과 벡터 유사도 검색을 결합하여, 더 넓은 범위의 관련 문서를 찾아냅니다.
    *   **재순위화(Re-ranking)**: 검색된 문서들을 LLM에 전달하기 전에 관련성, 신뢰성, 최신성 등 다양한 기준에 따라 재정렬하여 가장 유용한 정보가 모델에 먼저 제공되도록 합니다.
    *   **쿼리 확장 및 재작성(Query Expansion & Rewriting)**: 사용자 쿼리를 여러 개의 하위 쿼리로 확장하거나, 모델이 더 효과적으로 검색할 수 있는 형태로 재작성하여 검색의 정확도를 높입니다.
    *   **다중 홉 RAG (Multi-hop RAG)**: 단일 검색으로 해결하기 어려운 복잡한 질문에 대해, 여러 번의 검색-생성 단계를 반복하여 답변을 구성합니다. 이는 마치 사람이 여러 출처를 참조하며 정보를 종합하는 과정과 유사합니다.
    *   **컨텍스트 압축(Context Compression)**: 검색된 긴 문서를 LLM에 전달하기 전에 핵심 정보만 유지하면서 길이를 줄이는 기술입니다. 이는 컨텍스트 윈도우를 효율적으로 사용하고 모델의 처리 부담을 줄입니다.

이러한 RAG의 발전은 LLM이 최신 정보를 통합하고, 환각을 줄이며, 출처를 명확히 제시하는 능력을 크게 향상시켜, 실제 응용 분야에서 LLM의 신뢰성과 유용성을 높이는 데 기여합니다.

대규모 언어 모델은 잠재 변수 모델(Latent Variable Models)입니다 [24]. LLM이 인컨텍스트 학습 능력을 가지고 있다는 것을 알고 있지만, 이러한 능력이 표준 언어 모델 사전 학습에서 어떻게 나타나는지는 불분명합니다. 또한 인컨텍스트 학습은 일반적으로 퓨샷 학습에 사용되는 예시의 선택과 형식에 민감합니다. 특정 시연은 모델에 효과적인 예시인 반면, 다른 시연은 그렇지 않습니다. 현재 퓨샷 학습을 위한 최상의 예시를 선택하는 표준 기준은 없습니다. [24]에서 저자들은 이 주제를 연구하며, 가능한 최상의 퓨샷 예시를 식별하기 위한 실용적인 전략을 찾는 것을 목표로 합니다.

"인컨텍스트 학습은 광범위한 자연어 처리(NLP) 작업에 효과적인 기술로 입증되었습니다. 그러나 이는 사용되는 시연의 선택, 형식, 심지어 순서에도 민감합니다." - [24]에서 발췌

많은 논문이 이론적 관점에서 인컨텍스트 학습의 메커니즘을 연구했지만, 실용적이거나 실행 가능한 통찰력을 제공하는 논문은 거의 없습니다. [24]에서 저자들은 LLM을 언어 모델이 관찰한 이전 토큰에 새로운 토큰 생성을 연결하는 간단한 주제/잠재 변수 모델의 관점에서 봅니다. 자세한 내용은 논문에서 찾을 수 있지만, 높은 수준에서 이 공식화(formulation)는 모델의 입력 프롬프트 내에서 사용된 형식 및 작업 정보와 관련하여 언어 모델의 출력을 이론적으로 설명할 수 있게 합니다.

([24]에서 발췌)
이 공식화로부터 저자들은 모델 입력의 사후 확률(posterior probability)을 측정하기 위해 더 작은 언어 모델을 사용하는 가능한 최상의 퓨샷 예시를 선택하기 위한 실용적인 기술을 개발합니다. 이는 모델의 입력과 매개변수를 기반으로 다른 입력 예시의 가능성을 알려줍니다. 더 작은 LLM으로 선택된 예시를 더 큰 모델과의 인컨텍스트 학습에 사용할 수 있으며(위 참조), 이는 실질적인 이점을 제공하는 것으로 밝혀졌습니다. 간단히 말해, 이 논문은 더 나은 퓨샷 예시를 선택하기 위해 실제로 사용될 수 있는 인컨텍스트 학습에 대한 흥미롭고(상대적으로 간단한) 이론적 관점을 제안합니다.

글쓰기 능력 향상(Improving Writing Capabilities)
"SoT는 추론 효율성을 위한 데이터 중심 최적화의 초기 시도이며, 언어로 답변 구조를 명시적으로 계획함으로써 고품질 답변을 이끌어낼 잠재력을 보여줍니다." - [25]에서 발췌

사고의 골격(Skeleton-of-Thought, SoT) [25]은 LLM으로 출력을 생성하는 지연 시간(latency)을 줄이는 것을 목표로 하는 프롬프팅 기술입니다. 아시다시피, LLM으로 출력을 생성하는 것은 몇 가지 이유로 비용이 많이 들 수 있습니다.
*   모델이 크기 때문에 계산/메모리/I/O 비용이 높습니다.
*   어텐션(attention) 연산은 I/O 바운드(IO bound)이며 시퀀스 길이(sequence length)에 따라 제곱으로 증가하는 메모리/계산 복잡성(compute complexity)을 가집니다.
*   출력은 한 번에 하나의 토큰씩 순차적으로 생성됩니다(즉, 다음 토큰 예측(next token prediction) 사용).

[25]에서 저자들은 위에서 언급된 마지막 문제— 순차적 디코딩(sequential decoding)의 지연 시간 —를 해결하려고 시도합니다. 간단히 말해, 순차적 디코딩은 한 번에 하나의 토큰을 생성하므로 출력 시퀀스에서 토큰 생성을 병렬화(parallelize)할 수 없기 때문에 문제입니다. 이러한 이유로 출력 생성 비용은 출력의 길이에 직접적으로 관련됩니다. 많은 토큰을 가진 출력 시퀀스를 생성하는 데 훨씬 더 오랜 시간이 걸립니다. 하지만, 완전한 순차적 디코딩을 피할 수 있을까요?

([25]에서 발췌)
[25]에서 우리는 모델, 시스템 또는 하드웨어에 어떤 변경도 요구하지 않고 인간의 사고 및 글쓰기 과정을 모방함으로써 더 효율적인 디코딩 전략(decoding strategy)을 고안할 수 있음을 봅니다. 특히, 인간은 쓰고 싶은 내용에 대한 개요(outline)를 계획한 다음, 개요의 각 요소에 대한 세부 사항을 채우는 경향이 있습니다. 이것은 순전히 순차적인 과정이 아닙니다! 6 이 아이디어에서 영감을 받아, [25]의 저자들은 사고의 골격(Skeleton-of-Thought, SoT) 프롬프팅(위 참조)을 제안하며, 이는 두 단계로 이루어집니다.
*   LLM에 답변의 골격/개요를 생성하도록 프롬프트를 제공합니다.
*   각 개요 요소의 내용을 채우기 위해 병렬 API 호출을 수행합니다.

이것이 다소 모호하게 들릴 수 있지만, 아래에 표시된 SoT 프롬프트를 확인하면 이것이 어떻게 작동하는지 알 수 있습니다. 과정은 매우 간단합니다. 우리는 골격을 생성하고 일반적인 프롬프트 템플릿(prompt template)을 사용하여 나머지 모든 세부 사항을 채웁니다.

([25]에서 발췌)
골격의 각 요소를 병렬로 생성함으로써 추론 지연 시간(inference latency)을 크게 절약할 수 있습니다. 예를 들어, 이 요약의 시작 부분에 표시된 질문은 기본 모델이나 시스템에 어떤 변경도 가하지 않고 12초(22초 대신) 만에 답변될 수 있습니다. 우리는 단지 SoT 프롬프팅을 사용합니다. [25]에서는 12개의 다른 LLM에서 유사한 속도 향상이 관찰됩니다. 흥미롭게도, 저자들은 개요를 작성하는 것이 종종 글쓰기 품질을 향상시킬 수 있다고 언급합니다. 7

SoT의 확장 및 창의적 글쓰기
SoT(Skeleton-of-Thought)는 지연 시간 감소라는 실용적인 이점 외에도, LLM의 창의적 글쓰기 능력과 콘텐츠 생성 워크플로우를 혁신할 잠재력을 가지고 있습니다. 인간이 글을 쓸 때 개요를 먼저 잡는 것처럼, LLM도 유사한 방식으로 작동하도록 유도함으로써 더 구조화되고 일관된 결과물을 얻을 수 있습니다.

*   **창의적 콘텐츠 생성**: 소설, 시나리오, 블로그 게시물 등 창의적인 글쓰기에서 SoT는 이야기의 줄거리(plot), 캐릭터(character)의 발전, 장면(scene) 구성 등 큰 틀을 먼저 잡고 세부 내용을 채워나가는 데 활용될 수 있습니다. 이는 작가에게 창의적인 영감을 제공하고, 초고(first draft) 작성 시간을 단축시킬 수 있습니다.
*   **보고서 및 문서 작성**: 복잡한 비즈니스 보고서나 학술 논문 작성 시, SoT는 목차(table of contents)나 주요 섹션의 개요를 먼저 생성하고, 각 섹션의 내용을 병렬적으로 또는 순차적으로 채워나가는 데 유용합니다. 이는 문서의 일관성과 구조를 유지하는 데 도움을 줍니다.
*   **개인화된 콘텐츠 생성**: 특정 독자층(audience)이나 개인의 취향에 맞는 콘텐츠를 생성할 때, SoT는 개인화된 개요를 먼저 생성한 후, 해당 개요에 맞춰 콘텐츠를 맞춤형으로 작성하는 데 사용될 수 있습니다. 이는 마케팅 캠페인이나 추천 시스템과 결합될 때 특히 강력한 효과를 발휘합니다.
*   **스타일 일관성 유지**: SoT를 통해 생성된 개요를 바탕으로 여러 LLM 에이전트가 각 부분을 작성하게 하거나, 동일한 모델이 여러 번의 반복을 통해 내용을 채울 때, 특정 스타일 가이드(style guide)나 브랜드 보이스(brand voice)를 유지하도록 지시할 수 있습니다. 이는 대규모 콘텐츠 생성 시 스타일의 일관성을 확보하는 데 중요합니다.

SoT는 단순히 빠른 생성을 넘어, LLM의 글쓰기 과정을 더 체계적이고 제어 가능하게 만들며, 다양한 분야에서 고품질 콘텐츠를 효율적으로 생산하는 데 기여합니다.

([27]에서 발췌)
방향성 자극 프롬프팅(Directional Stimulus Prompting) [27]. 미세 조정의 계산 비용을 고려할 때, 프롬프팅은 일반적으로 LLM으로 작업을 해결하는 가장 쉬운 방법입니다. 그러나 프롬프팅에는 한계가 있습니다. LLM이 우리가 원하는 내용이나 스타일로 출력을 생성하도록 유도하는 것은 어려울 수 있습니다. 이 문제를 해결하기 위해 [27]의 저자들은 방향성 자극 프롬프팅(directional stimulus prompting, DSP)을 제안합니다. 이는 위 그림과 같이 LLM의 프롬프트에 "방향성 자극(directional stimulus)"을 도입합니다. 이 자극은 예상 출력에 대한 더 많은 정보를 LLM에 제공하는 텍스트 힌트(textual hint) 또는 단서(clue)일 뿐입니다. 방향성 자극은 인스턴스 특정적(instance-specific)이며 입력 쿼리에만 기반하며, LLM에 비해 훈련하거나 미세 조정하기 훨씬 쉬운 더 작은 모델(예: T5)을 사용하여 생성됩니다. 이렇게 함으로써 우리는 LLM을 직접 훈련하는 어려움을 우회하고, 대신 방향성 자극을 생성하는 데 사용되는 모델을 미세 조정하는 것을 선택합니다. 8 DSP는 요약(summarization), 대화(dialogue) 및 추론 작업에서 평가되었으며, 최소한의 레이블이 지정된 데이터(labeled data)만으로 모델 성능을 향상시키는 것으로 나타났습니다.

([28]에서 발췌)
밀도 연쇄 프롬프팅(Chain of Density Prompting) [28]. LLM의 최근 발전은 자동 요약(automatic summarization) 문제를 혁신했습니다. 레이블이 지정된 데이터에 대해 미세 조정을 수행하는 대신 LLM에 고품질 요약을 생성하도록 간단히 프롬프트를 제공할 수 있기 때문입니다. 요약을 자동으로 생성할 때, 결과 요약의 품질에서 중요한 측면 중 하나는 정보 밀도(information density)입니다. 우리는 요약이 모든 관련 정보를 간결하게 제시하기를 원하지만, 지나치게 밀도가 높거나 읽기 어려운 요약을 작성하는 것은 피하고 싶습니다. 정보 밀도의 이러한 상충 관계(tradeoff)를 연구하기 위해 [28]의 저자들은 밀도 연쇄(chain of density, CoD) 프롬프팅을 제안합니다. 이는 GPT-4에 바닐라 프롬프트(vanilla prompt)를 통해 요약을 생성하는 것으로 시작합니다. 여기에서 CoD 프롬프팅은 요약의 길이를 고정된 상태로 유지하면서 요약에 추가 엔티티(entities)를 반복적으로 추가하여 요약의 정보 밀도를 높이는 데 사용됩니다. 흥미롭게도 [28]에서 우리는 인간이 사람이 작성한 요약만큼 밀도가 높지만, GPT-4에 바닐라 프롬프트를 통해 생성된 요약보다 더 밀도가 높은 요약을 선호한다는 것을 알 수 있습니다. CoD 프롬프팅을 사용함으로써 우리는 이러한 상충 관계를 탐색하고 더 높은 품질의 요약을 생성할 수 있습니다.

"CoD로 생성된 요약은 바닐라 프롬프트로 GPT-4가 생성한 요약보다 더 추상적(abstractive)이고, 더 많은 융합(fusion)을 보이며, 선행 편향(lead bias)이 적습니다." - [28]에서 발췌

기타 주목할 만한 논문(Other Notable Papers)
능동 프롬프팅(Active Prompting) [26]은 불확실성 기반 능동 학습(uncertainty-based active learning) 연구를 기반으로 특정 추론 문제를 해결하기 위해 가장 유용한 예시를 선택(및 주석 달기)하는 기술을 제공함으로써 CoT 프롬프팅을 위한 예시 선택(및 주석 달기)의 어려움을 해결합니다. TaskMatrix [33]는 파운데이션 모델(foundation models)과 수백만 개의 다른 API의 통합을 고려하는 주목할 만한 문제에 대한 입장 또는 전망을 제시하는 입장 논문(position paper)입니다. 마크 세트 프롬프팅(Set of Marks Prompting) [42]은 사전 훈련된 분할 모델(pretrained segmentation models)을 사용하여 이미지를 영역으로 분할하고, 이 영역에 마크 세트(즉, 영숫자, 마스크, 상자 등)를 오버레이(overlay)하여 GPT-4V와 같은 모델의 시각적 접지(visual grounding)를 개선하는 시각적 프롬프팅(visual prompting) 방법입니다. 다중 모달 CoT 프롬프팅(Multimodal CoT Prompting) [43]은 근거 및 답변 생성을 문제 해결 과정의 두 가지 별개의 단계로 처리함으로써 이미지와 텍스트를 모두 포함하는 입력으로 CoT 프롬프팅을 확장합니다. 자동 프롬프팅(automatic prompting)(즉, 최적화 프로세스를 통해 더 나은 프롬프트 생성) 주제는 이 게시물에서 다루지 않습니다. 다른 것이 있나요? 댓글로 알려주세요!

자동 프롬프트 최적화 및 에이전트 기반 프롬프팅
프롬프트 엔지니어링의 발전은 수동적인 프롬프트 작성에서 자동화되고 능동적인 프롬프트 생성 및 최적화로 나아가고 있습니다.

*   **자동 프롬프팅(Automatic Prompting)**: 이는 사람이 직접 프롬프트를 작성하는 대신, LLM 또는 다른 알고리즘을 사용하여 특정 작업에 대한 최적의 프롬프트를 자동으로 생성하고 개선하는 기술을 의미합니다. 예를 들어, 메타 프롬프트(meta-prompt)를 사용하여 LLM에게 특정 작업에 대한 여러 프롬프트 후보를 생성하게 하고, 이들을 평가하여 가장 성능이 좋은 프롬프트를 선택하는 방식입니다. 이는 대규모 작업이나 빠르게 변화하는 요구사항에 대응하는 데 매우 유용합니다.
*   **프롬프트 체이닝(Prompt Chaining) 및 워크플로우**: 여러 LLM 호출을 연결하여 복잡한 작업을 해결하는 방식입니다. 각 LLM 호출은 이전 호출의 출력을 입력으로 사용하여, 다단계 추론, 데이터 처리, 콘텐츠 생성 등 복잡한 워크플로우를 구축할 수 있습니다. 이는 DecomP나 ToT와 유사한 개념이지만, 더 유연하고 동적인 연결을 강조합니다.
*   **에이전트 기반 프롬프팅(Agent-based Prompting)**: LLM이 특정 목표를 달성하기 위해 스스로 프롬프트를 생성하고, 도구를 선택하며, 환경과 상호작용하는 자율 에이전트의 핵심 구성 요소로 활용되는 방식입니다. 에이전트는 목표를 하위 목표로 분해하고, 각 하위 목표에 대해 최적의 프롬프트와 도구 사용 전략을 동적으로 결정합니다. 이는 LLM을 단순한 응답 생성기가 아닌, 능동적인 문제 해결자로 만듭니다.

이러한 연구 동향은 프롬프트 엔지니어링이 단순히 "어떻게 질문할 것인가"를 넘어 "어떻게 LLM이 스스로 최적의 질문을 만들고 행동하게 할 것인가"라는 더 큰 질문으로 확장되고 있음을 보여줍니다.

결론(Conclusion)
이 개요에서 우리는 프롬프트 엔지니어링의 기본부터 지난 두 달 동안 제안된 최첨단 기술에 이르기까지 모든 것을 배웠습니다! 이 게시물은 엄청난 양의 정보를 포함하고 있지만, 우리가 본 많은 기술들은 동일한 핵심 프롬프트 구성 요소인 지시, 예시, 컨텍스트, 문제 해결 근거를 활용하는 약간의 변형입니다. 또한, 우리는 처음에 제안된 프롬프트 엔지니어링 전략을 상기해야 합니다.
*   프롬프트의 품질을 쉽고 정량적으로 측정할 수 있는 포괄적인 평가 전략을 수립하는 것으로 시작하세요.
*   작성하는 첫 번째 프롬프트는 간단해야 합니다(예: 지시 프롬프트).
*   프롬프트를 더 복잡하게 만들 때는 추가된 복잡성이 그에 상응하는 성능 향상으로 이어지는지 확인하세요.
*   원하는 성능에 도달할 때까지 프롬프트를 계속 반복하세요.

많은 문제는 간단한 지시 및 퓨샷 프롬프트를 통해 해결될 수 있습니다. 복잡한 추론 문제의 경우, 자기 일관성을 포함하는 CoT 프롬프팅과 같은 더 고급 전략을 사용하는 것이 필요할 수 있습니다. 또한, 특정 문제 도메인(예: 수학 문제에 대한 PoT 프롬프팅 또는 요약에 대한 CoD 프롬프팅)에 유용한 다양한 프롬프팅 전략을 보았습니다. 이러한 기술을 아는 것이 유용하지만, 그 사용 사례는 비교적 드물며, 명확하고 측정 가능한 성능 영향을 볼 때만 사용해야 합니다.

미래 전망 및 지속적인 학습
프롬프트 엔지니어링은 LLM 기술의 발전과 함께 끊임없이 진화하는 분야입니다. 새로운 모델이 등장하고, 컨텍스트 윈도우가 확장되며, 다중 모달 기능이 강화됨에 따라, 프롬프트 엔지니어링의 역할과 방법론 또한 변화할 것입니다. 앞으로는 LLM이 인간의 지시를 단순히 따르는 것을 넘어, 스스로 문제를 정의하고, 해결책을 탐색하며, 학습하고 개선하는 자율 에이전트(autonomous agent)로서의 역할을 수행하는 데 프롬프트 엔지니어링이 더욱 중요해질 것입니다. 이는 프롬프트 엔지니어링이 단순한 텍스트 입력 최적화를 넘어, 복잡한 시스템 설계와 인간-AI 협업의 핵심 요소로 자리매김할 것임을 의미합니다. 따라서 이 분야에 대한 지속적인 관심과 학습, 그리고 실제 적용을 통한 경험 축적은 LLM의 잠재력을 최대한 활용하는 데 필수적입니다.

뉴스레터가 처음이신가요? 안녕하세요! 저는 Cameron R. Wolfe이며, 이 Deep (Learning) Focus 뉴스레터는 독자들이 AI 연구를 이해하도록 돕습니다. 뉴스레터가 마음에 드신다면 구독, 공유 또는 X와 LinkedIn에서 저를 팔로우해 주세요!

구독

참고문헌(Bibliography)
[1] Saravia, Elvis, et al. “Prompt Engineering Guide”, https://github.com/dair-ai/Prompt-Engineering-Guide (2022).
[2] Radford, Alec, et al. "Language Models are Unsupervised Multitask Learners."
[3] Brown, Tom, et al. "Language models are few-shot learners." Advances in neural information processing systems 33 (2020): 1877-1901.
[4] Work, What Makes In-Context Learning. "Rethinking the Role of Demonstrations: What Makes In-Context Learning Work?."
[5] Zhao, Zihao, et al. "Calibrate before use: Improving few-shot performance of language models." International conference on machine learning . PMLR, 2021.
[6] Ouyang, Long, et al. "Training language models to follow instructions with human feedback." Advances in neural information processing systems 35 (2022): 27730-27744.
[7] Ye, Seonghyeon, et al. "Investigating the effectiveness of task-agnostic prefix prompt for instruction following." Proceedings of the AAAI Conference on Artificial Intelligence . Vol. 38. No. 17. 2024.
[8] Thoppilan, Romal, et al. "Lamda: Language models for dialog applications." arXiv preprint arXiv:2201.08239 (2022).
[9] Rae, Jack W., et al. "Scaling language models: Methods, analysis & insights from training gopher." arXiv preprint arXiv:2112.11446 (2021).
[10] Wei, Jason, et al. "Chain-of-thought prompting elicits reasoning in large language models." Advances in neural information processing systems 35 (2022): 24824-24837.
[11] Kojima, Takeshi, et al. "Large language models are zero-shot reasoners." arXiv preprint arXiv:2205.11916 (2022).
[12] Wang, Xuezhi, et al. "Self-consistency improves chain of thought reasoning in language models." arXiv preprint arXiv:2203.11171 (2022).
[13] Zhou, Denny, et al. "Least-to-most prompting enables complex reasoning in large language models." arXiv preprint arXiv:2205.10625 (2022).
[14] Yao, Shunyu, et al. "Tree of thoughts: Deliberate problem solving with large language models." arXiv preprint arXiv:2305.10601 (2023).
[15] Zhang, Zhuosheng, et al. "Automatic chain of thought prompting in large language models." arXiv preprint arXiv:2210.03493 (2022).
[16] Fu, Yao, et al. "Complexity-based prompting for multi-step reasoning." The Eleventh International Conference on Learning Representations . 2022.
[17] Zheng, Chuanyang, et al. "Progressive-hint prompting improves reasoning in large language models." arXiv preprint arXiv:2304.09797 (2023).
[18] Khot, Tushar, et al. "Decomposed prompting: A modular approach for solving complex tasks." arXiv preprint arXiv:2210.02406 (2022).
[19] Lu, Pan, et al. "Chameleon: Plug-and-play compositional reasoning with large language models." Advances in Neural Information Processing Systems 36 (2024).
[20] Yang, Rui, et al. "Gpt4tools: Teaching large language model to use tools via self-instruction." Advances in Neural Information Processing Systems 36 (2024).
[21] Wang, Yizhong, et al. "Self-instruct: Aligning language models with self-generated instructions." arXiv preprint arXiv:2212.10560 (2022).
[22] Shi, Freda, et al. "Large language models can be easily distracted by irrelevant context." International Conference on Machine Learning . PMLR, 2023.
[23] Liu, Nelson F., et al. "Lost in the middle: How language models use long contexts." Transactions of the Association for Computational Linguistics 12 (2024): 157-173.
[24] Wang, Xinyi, et al. "Large language models are latent variable models: Explaining and finding good demonstrations for in-context learning." Advances in Neural Information Processing Systems 36 (2024).
[25] Ning, Xuefei, et al. "Skeleton-of-thought: Large language models can do parallel decoding." arXiv preprint arXiv:2307.15337 (2023).
[26] Diao, Shizhe, et al. "Active prompting with chain-of-thought for large language models." arXiv preprint arXiv:2302.12246 (2023).
[27] Li, Zekun, et al. "Guiding large language models via directional stimulus prompting." Advances in Neural Information Processing Systems 36 (2024).
[28] Adams, Griffin, et al. "From sparse to dense: GPT-4 summarization with chain of density prompting." arXiv preprint arXiv:2309.04269 (2023).
[29] Zhu, Zhaocheng, et al. "Large language models can learn rules." arXiv preprint arXiv:2310.07064 (2023).
[30] Patil, Shishir G., et al. "Gorilla: Large language model connected with massive apis." arXiv preprint arXiv:2305.15334 (2023).
[31] Shen, Yongliang, et al. "Hugginggpt: Solving ai tasks with chatgpt and its friends in huggingface." arXiv preprint arXiv:2303.17580 (2023).
[32] Schick, Timo, et al. "Toolformer: Language models can teach themselves to use tools." arXiv preprint arXiv:2302.04761 (2023).
[33] Liang, Yaobo, et al. "Taskmatrix. ai: Completing tasks by connecting foundation models with millions of apis." arXiv preprint arXiv:2303.16434 (2023).
[34] Chen, Shouyuan, et al. "Extending context window of large language models via positional interpolation." arXiv preprint arXiv:2306.15595 (2023).
[35] Besta, Maciej, et al. "Graph of Thoughts: Solving Elaborate Problems with Large Language Models." arXiv preprint arXiv:2308.09687 (2023).
[36] Yao, Yao, Zuchao Li, and Hai Zhao. "Beyond Chain-of-Thought, Effective Graph-of-Thought Reasoning in Large Language Models." arXiv preprint arXiv:2305.16582 (2023).
[37] Lewis, Patrick, et al. "Retrieval-augmented generation for knowledge-intensive nlp tasks." Advances in Neural Information Processing Systems 33 (2020): 9459-9474.
[38] Ovadia, Oded, et al. "Fine-tuning or retrieval? comparing knowledge injection in llms." arXiv preprint arXiv:2312.05934 (2023).
[39] Liu, Jiacheng, et al. "Generated knowledge prompting for commonsense reasoning." arXiv preprint arXiv:2110.08387 (2021).
[40] Gao, Luyu, et al. "PAL: Program-aided Language Models." arXiv preprint arXiv:2211.10435 (2022).
[41] Chen, Wenhu, et al. "Program of thoughts prompting: Disentangling computation from reasoning for numerical reasoning tasks." arXiv preprint arXiv:2211.12588 (2022).
[42] Yang, Jianwei, et al. "Set-of-mark prompting unleashes extraordinary visual grounding in gpt-4v." arXiv preprint arXiv:2310.11441 (2023).
[43] Zhang, Zhuosheng, et al. "Multimodal chain-of-thought reasoning in language models." arXiv preprint arXiv:2302.00923 (2023).

1 기억하세요, 더 긴 프롬프트는 항상 더 비쌉니다! API를 사용할 때는 이러한 토큰에 대해 직접 비용을 지불합니다. 오픈 소스 모델의 경우, 추가 계산 및 지연 시간 비용으로 추가 토큰에 대한 비용을 지불합니다.
2 이러한 기술(및 더 많은 프롬프팅 기술과 다양한 멋진 주제)에 대한 훌륭한 개요를 보려면 Lillian Weng의 블로그를 확인하세요!
3 이 문제를 제거하기 위해 프롬프트 내의 예시를 무작위로 정렬하거나 심지어 순열(permute)하고, 예시의 여러 순열에 대한 출력을 생성할 수 있습니다.
4 사실, 빈약한 추론 능력은 현대 LLM에 대한 주요 비판 중 하나입니다. 많은 연구자들은 LLM의 추론 불능을 중요한 개념에 대한 얕은 이해의 증거로 인용합니다.
5 많은 연구자들은 이 다수결 투표 전략이 복잡한 문제를 해결하는 데 불충분하다고 주장해 왔습니다. 이는 프롬프트 앙상블(prompt ensembles) 및 기타 자기 일관성 변형에 대한 많은 심층 연구로 이어졌습니다.
6 사실, 이 뉴스레터를 작성하는 것은 순차적이지 않습니다. 저의 글쓰기 전략은 보통 i) 유사한 논문들을 개괄하고, ii) 공유된 아이디어와 배경을 가진 섹션들을 추가한 다음, iii) 서론과 결론을 작성하는 것입니다. 이것은 순차적인 과정과는 정반대입니다 (서론은 제가 가장 마지막에 쓰는 것입니다!).
7 LLM에 답변의 개요인 근거를 생성하도록 요청할 때, 우리는 종종 이 과정에서 CoT 프롬프팅에서 관찰된 이점과 유사한 이점을 봅니다. 그러나 SoT 프롬프팅은 여러 개의 분리된 생성(즉, 골격을 생성하는 하나, 그리고 각 골격 구성 요소에 대한 하나)을 수행하는 반면, CoT 프롬프트는 일반적으로 단일 통과(single pass)로 출력을 생성하는 데 사용된다는 점에 유의해야 합니다.
8 이 모델은 지도 미세 조정(supervised finetuning, SFT) 또는 RLHF와 유사한 RL 기반 전략을 사용하여 미세 조정될 수 있습니다.