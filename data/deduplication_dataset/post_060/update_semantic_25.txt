최근 몇 년간, 거대 언어 모델(LLM)은 그 직관적인 접근성으로 인해 압도적인 인기를 누리고 있습니다. 기존의 복잡한 코딩이나 심층 학습(deep learning) 지식 없이도, 간단한 문자열 형태의 지시문(textual prompt)만으로 방대한 인공 신경망(neural network)의 역량을 빌려 다양한 난해한 과제를 효율적으로 해결하는 시대가 도래했습니다. 이러한 모델들은 시간이 흐르면서 지시 사항을 따르는 능력(instruction following)과 사용자 의도와의 조화(alignment)가 점차 향상되어, 일반 대중도 더욱 손쉽게 활용할 수 있게 되었습니다. 그럼에도 불구하고, LLM에 효과적인 질의문을 구성하는 것은 단순한 기술을 넘어선 예술이자 정교한 학문적 접근을 요구합니다. 질의문(prompting)의 미세한 조정이나 전략적 변화만으로도 모델의 수행 능력에 현저한 개선을 가져올 수 있습니다. 본 글에서는 질의어 공학(prompt engineering)의 핵심 원리부터 최근 몇 달간 제시된 최신 기법에 이르기까지, 이 분야에 대한 깊이 있는 통찰을 제공하고자 합니다. 이처럼 인공지능과의 소통 능력이 중요해지면서, '질의문 리터러시'는 미래 시대의 필수 역량으로 부상하고 있습니다. 과거에는 전문가의 영역이었던 인공지능 활용이 이제는 자연어 구사 능력만 있다면 누구나 접근 가능한 보편적 도구가 되었으며, 이는 인공지능의 민주화를 가속화하고 있습니다.

**질의어 공학이란 무엇인가요?**

거대 언어 모델이 이토록 각광받는 핵심 동력 중 하나는 텍스트 기반의 대화형 방식(text-to-text interface) 덕분에 그 사용법이 지극히 간단하다는 점입니다. 과거 인공지능 모델들은 특정 임무를 수행하기 위해 방대한 양의 데이터를 동원한 세밀한 조정(finetuning) 과정을 거쳐야 했으며, 대부분은 한정된 분야의 전문가(narrow experts) 역할에 머물렀습니다. 하지만 LLM은 맥락 내 학습(in-context learning)이라는 혁신적인 능력 덕분에, 별도의 훈련 없이도 오직 텍스트 질의문만으로 다채로운 난제를 해결할 수 있게 되었습니다. 이는 과거의 복잡했던 문제 해결 절차를 인간의 언어(natural language)라는 고차원적인 개념으로 승격시킨 것입니다. 이러한 변화는 인공지능과의 상호작용 패러다임을 근본적으로 전환시켰습니다. 개발자나 연구자만이 아닌, 일반 사용자도 자신의 언어로 인공지능에게 원하는 바를 지시하고 결과를 얻을 수 있게 된 것입니다. 이는 인공지능의 활용 범위를 무한히 확장하며, 우리가 기술과 소통하는 방식에 새로운 장을 열었습니다.

"프롬프트 공학은 다양한 활용 사례 및 탐구 영역에서 언어 모델을 효과적으로 구사하기 위한 질의문을 구축하고 개선하는 상대적으로 신생 분야입니다." - [1]에서 발췌

**질의어 공학의 본질**

LLM의 접근 용이성은 그 대중화를 이끌었습니다. 데이터 분석가(data scientist)나 기계 학습 엔지니어(MLE)가 아니더라도, 자신이 사용하는 언어를 이해한다면 LLM을 통해 상당히 복잡한 과업들을 처리할 수 있습니다. 하지만 LLM을 활용하여 얻는 결과물의 질은 모델에 주어지는 텍스트 질의문에 크게 좌우됩니다. 이러한 맥락에서, LLM의 성능을 극대화하기 위해 다양한 질의문을 시험하는 실증적 연구 분야인 프롬프트 공학은 엄청난 인기를 얻었으며, 수많은 기법과 최적의 실천 방안(best practices)을 발견하는 계기가 되었습니다. 질의어 공학은 단순한 명령 전달을 넘어, 모델의 잠재력을 최대한 끌어내는 정교한 대화 기술로 발전하고 있습니다.

**질의문 구성 요소 심층 분석**

LLM에 질의문을 제공하는 방식은 여러 가지가 있지만, 대부분의 질의 전략은 몇 가지 공통적인 구성 요소를 공유합니다. 이 구성 요소들은 모델과의 상호작용을 구조화하고 예측 가능한 결과를 도출하는 데 필수적입니다.
*   **입력 자료(Input Data)**: LLM이 처리해야 할 실제 정보 (예: 번역하거나 분류할 문장, 요약할 문서 등). 이는 모델이 작업을 수행할 기반이 되는 원천 데이터입니다.
*   **시범 사례(Exemplars)**: 질의문 내에 포함된, 올바른 입력-출력 쌍의 구체적인 예시들. 모델이 원하는 동작을 이해하도록 돕는 학습 자료 역할을 합니다.
*   **지시 사항(Instruction)**: 모델에서 기대되는 최종 결과물에 대한 명확한 텍스트 설명. 이는 모델이 수행해야 할 작업을 정의합니다.
*   **표지(Indicators)**: 질의문 내부에 구조를 형성하는 데 사용되는 태그(tag) 또는 서식 요소(formatting elements). 이는 모델이 질의문의 특정 부분을 인식하고 처리하는 데 도움을 줍니다.
*   **맥락(Context)**: 질의문에서 LLM에 제공되는 모든 부가적인 정보. 이는 모델이 당면한 과제를 더 깊이 이해하고 적절한 응답을 생성하는 데 필요한 배경 지식을 제공합니다.

아래 그림은 문장 분류를 위한 단일 질의문 내에 위에서 언급된 모든 질의문 구성 요소를 결합한 사례를 보여줍니다. 이러한 종합적인 접근 방식은 모델의 이해도를 높여 더 정확한 결과를 이끌어냅니다.

LLaMA-2 스타일의 모든 구성 요소를 포함한 질의문 (여기 참조)

**컨텍스트 창(Context Window)의 이해와 활용**

사전 훈련(pretraining) 과정에서 LLM은 특정 길이의 입력 순서(input sequence)를 처리합니다. 이때 선택된 순서의 최대 길이가 곧 모델의 "컨텍스트 길이(context length)" 또는 모델이 수용할 수 있는 최대 순서 길이가 됩니다. 만약 이 미리 정해진 컨텍스트 길이보다 훨씬 긴 텍스트 순서가 주어지면, 모델은 예상치 못한 방식으로 작동하거나 부정확한 결과물을 생성할 수 있습니다. 그러나 Self-Extend나 위치 보간(positional interpolation)과 같은 기법을 사용하여 모델의 컨텍스트 창을 확장할 수 있습니다. 이는 더 많은 정보를 한 번에 처리할 수 있도록 모델의 능력을 향상시킵니다.

RoPE를 사용한 위치 보간(positional interpolation) 그림 ([34]에서 발췌)

LLM에 대한 최근 연구는 긴 컨텍스트 창(long context windows)을 구축하는 데 집중해 왔으며, 이는 모델이 각 질의문 내에서 더 많은 정보(예: 더 많은 시범 사례나 추가 맥락)를 처리할 수 있게 합니다. 하지만 모든 LLM이 맥락에 완벽하게 집중하는 것은 아닙니다! LLM이 긴 컨텍스트 창 내의 정보를 얼마나 효과적으로 활용하는지는 일반적으로 "건초 더미 속 바늘 찾기 테스트(needle in the haystack test)"를 통해 평가됩니다. 이 테스트는 i) 컨텍스트 내에 무작위적인 사실을 삽입하고, ii) 모델에 해당 사실을 찾아내도록 요청하며, iii) 다양한 컨텍스트 길이와 사실의 위치에 따라 이 과정을 반복합니다. 이러한 시험은 아래 그림과 같은 결과를 보여주며, 컨텍스트 창의 한계를 쉽게 드러냅니다. 이는 모델이 긴 정보 속에서 중요한 내용을 놓칠 수 있음을 시사합니다.

(출처)

**효과적인 질의어 공학 전략: 나만의 접근법**

질의어 공학의 세부 사항은 사용되는 모델의 종류에 따라 크게 달라지지만, 질의어 공학 과정을 안내하는 데 유용한 몇 가지 보편적인 원칙들이 있습니다.
*   **실증적 접근(Be empirical)**: 질의어 공학의 첫걸음은 질의문 변화를 손쉽게 평가할 수 있는 신뢰할 수 있는 방법론(예: 시험 사례(test cases), 인간 평가자(human evaluators) 또는 LLM-as-a-judge 활용)을 마련하는 것입니다. 정량적인 피드백 없이는 개선의 방향을 잡기 어렵습니다.
*   **간결하게 시작(Start simple)**: 처음 시도하는 질의문이 연쇄 사고(chain-of-thought) 프롬프트(또는 다른 복잡한 질의 기법)일 필요는 없습니다. 가장 단순한 형태의 질의문으로 시작하여, 성능 변화를 면밀히 관찰하며 점진적으로 복잡도를 추가하여 추가적인 복잡성이 정말로 필요한지 판단해야 합니다. 1 더 긴 질의문은 항상 더 많은 비용을 수반합니다.
*   **명확하고 직접적(Be specific and direct)**: 질의문의 모호성을 제거하고 LLM이 생성해야 할 결과물을 명확히 설명할 때는 간결하고 직접적이며 구체적인 표현을 사용하도록 노력하세요. 불분명한 지시는 불확실한 결과를 낳습니다.
*   **시범 사례 활용(Use exemplars)**: 원하는 결과물을 말로 설명하기 어렵다면, 질의문에 몇 가지 예시를 추가해 보세요. 시범 사례는 LLM에 기대되는 행동에 대한 구체적인 본보기를 제공하여 모호함을 해소하는 데 탁월합니다.
*   **복잡성 회피(Avoid complexity (if possible))**: 복잡한 질의 전략이 때때로 요구될 수 있지만(예: 다단계 추론 문제 해결), 그러한 접근 방식을 채택하기 전에 신중하게 고려해야 합니다. 실증적인 방법으로 확립된 평가 전략을 사용하여 복잡성이 진정으로 필요한지 검증하는 것이 중요합니다.

위 내용을 정리하자면, 저의 개인적인 질의어 공학 전략은 i) 견고한 평가 체계(evaluation framework)에 투자하고, ii) 단순한 질의문으로 시작하며, iii) 목표하는 성능 수준에 도달하기 위해 필요에 따라 점진적으로 복잡성을 증대시키는 것입니다. 질의문 작성은 본질적으로 반복적인 과정입니다.

**질의 기법(Prompting Techniques) 개괄**

우리는 이전에도 관련 개요들을 통해 다양한 질의 기법들을 탐구한 바 있습니다.
*   실용적인 질의어 공학(Practical Prompt Engineering) [링크]
*   고급 질의어 공학(Advanced Prompt Engineering) [링크]
*   연쇄 사고 질의(Chain of Thought Prompting) [링크]
*   질의문 앙상블(Prompt Ensembles) [링크]

이제 관련 질의 기법들을 다시 한번 개괄하여, 본 게시물에서 나중에 소개될 더욱 복잡한 접근 방식들의 기반을 다질 것입니다. 하지만 이러한 각 기법을 학습하면서도, 질의어 공학에서 '단순함'이 가지는 중요성을 항상 기억해야 합니다. 질의 기법이 더 복잡하거나 정교하다고 해서, 반드시 더 간단한 전략보다 우월하다는 의미는 아닙니다!

**기본 질의 전략(Basic Prompting Strategies)** ([3]에서 발췌)

**제로샷 질의(Zero-shot prompting)** (위 그림 참조)는 GPT-2 [2]에 의해 널리 알려졌으며, 우리가 활용할 수 있는 가장 기본적인 질의 전략 중 하나입니다. 제로샷 질의를 통해 작업을 해결하려면, i) 질의문에 작업을 서술하고 ii) 모델에 해당 문제의 해법을 요청하는 질의문을 제시하기만 하면 됩니다. 위에 제시된 문제의 경우, 임무는 영어를 프랑스어로 옮기는 것이며, 우리는 "cheese =>"라는 문자열을 통해 모델이 이 번역을 수행하도록 유도합니다. 이는 모델이 'cheese'라는 단어의 프랑스어 번역을 출력하게 만듭니다. 다음은 제로샷 질의의 몇 가지 예시입니다.

제로샷 학습(Zero-shot learning) (GPT-3.5-Turbo로 생성된 출력)
제로샷 학습이 특정 상황에서는 효과적이지만, 작업 설명의 모호성으로 인해 한계가 존재합니다. 성능은 명확하고 포괄적인 설명을 생성하는 능력에 달려 있으며, 우리는 오직 이 설명만으로 모델이 올바른 결과물을 도출할 수 있는 능력에 전적으로 의존합니다. 종종 질의문에 더 구체적인 정보를 포함시킴으로써 더욱 향상된 성능을 달성할 수 있습니다.

**퓨샷 질의(Few-shot prompting)** ([3]에서 발췌)는 질의문 내에 올바른 문제 해결 예시를 여러 개 삽입함으로써 바로 이러한 목표를 달성합니다. 이 전략은 GPT-3 [3]에 의해 대중화되었으며, GPT-3는 LLM이 대규모로 인상적인 퓨샷 학습(few-shot learning) 능력을 발전시킨다는 것을 보여주었습니다. (위 참조) 직관적으로 퓨샷 학습은 예상되는 결과물의 여러 예시를 제공함으로써 제로샷 학습의 모호성을 해소합니다. 따라서 모델은 작업 설명에서 원하는 행동을 추론하는 대신, 이러한 시범 사례로부터 올바른 동작 방식을 직접적으로 파악할 수 있습니다. (아래 참조)

([3]에서 발췌)
LLM은 질의문 내에 주어진 이러한 시범 사례들로부터 학습할 수 있으며, 이는 일반적으로 "맥락 내 학습(in-context learning)"이라고 불리는 전략입니다. (아래 참조) 하지만 이러한 학습 방식은 인공 신경망(neural network)의 일반적인 훈련과는 다릅니다. 모델의 내부 가중치(parameters)는 전혀 변경되지 않습니다. 그 대신, 우리는 질의문에 적절한 데이터를 포함시키며, 모델은 이 자료를 활용하여 더욱 정교한 결과물을 도출할 수 있습니다.

([3]에서 발췌)
실제로 퓨샷 학습을 활용할 때, 적절하게 조정해야 할 두 가지 주요 설정이 있습니다.
*   사용할 시범 사례의 개수.
*   시범 사례 선택 전략.

사용할 적절한 시범 사례의 수를 결정하기 위해 평가 집합(evaluation set)을 활용하여 기본적인 하이퍼파라미터 조정(hyperparameter tuning)을 수행할 수 있습니다. 많은 연구에서 시범 사례 선택 전략(예: 무작위 선택(random selection), 다양성(diversity), 의미론적 유사성(semantic similarity), 능동 학습(active learning) 또는 더 복잡한 지표(metrics) 기반) 2 을 탐구해왔습니다. 하지만 시범 사례의 무작위 선택은 실제로는 종종 효과적인 전략임이 입증되었습니다. 이러한 전략 외에도 퓨샷 학습과 관련된 다양한 실용적인 규칙과 발견 사항들이 있으며, 우리는 항상 이를 염두에 두어야 합니다 [4, 5].
*   시범 사례의 레이블(label) 분포는— 비록 부정확하더라도 —모델이 일반적인 레이블에 편향되는 경향이 있기 때문에 모델의 응답에 영향을 미칠 수 있습니다.
*   응답은 질의문에서 최근에 관찰된 시범 사례에 편향됩니다. 3
*   질의문 내 시범 사례의 서식(formatting)이 중요합니다.
*   시범 사례를 무작위로 선택하면 모델이 생성한 응답 내의 편향(예: 위치 또는 다수 레이블 편향)을 줄이는 데 기여할 수 있습니다.

단순함에도 불구하고 퓨샷 학습은 가장 효과적인 질의 전략 중 하나이며, 실제 응용 분야에서 폭넓게 사용됩니다.

**지시 질의(Instruction Prompting)의 강력함** ([6]에서 발췌)

지시 질의(Instruction prompting)는 LLM의 원하는 결과물을 표현하는 더욱 직접적인 방식입니다. 퓨샷 학습에서는 해결하려는 작업의 구체적인 시범 사례를 통해 모델에 우리의 의도를 설명하지만, 이러한 시범 사례는 많은 토큰(token)을 소모합니다! 모델에 우리의 의도를 말로 설명하는 것이 훨씬 더 효율적일 것입니다. 이것이 잘 작동하려면 사용되는 LLM이 지시를 일관되게 따르도록 정렬(aligned)되어야 합니다. 이러한 모델은 제공된 상세한 지시를 이해하고 그에 따라 결과물을 조정할 수 있기 때문에 "조종 가능(steerable)"하다고 불립니다.

([6]에서 발췌)
LLM에 대한 연구는 지시 따르기 능력(instruction following capabilities) 향상에 크게 집중해 왔습니다. 사전 훈련된 LLM은 본질적으로 지시를 잘 따르지 못합니다. 그러나 InstructGPT [6]가 보여주듯이, 우리는 지도 미세 조정(supervised finetuning, SFT)과 인간 피드백 기반 강화 학습(reinforcement learning from human feedback, RLHF)의 조합을 통해 모델이 지시를 훨씬 더 잘 따르도록 정렬할 수 있습니다. 위 그림에서 이 전략이 지시 따르기뿐만 아니라 LLM의 다른 주요 특성(예: 사실성(factuality) 및 제약 조건 준수(constraint following))도 개선할 수 있음을 알 수 있습니다.

LaMDA를 사용한 역할 질의(Role prompting) ([8]에서 발췌)
LLM 정렬의 최근 발전 덕분에, 퓨샷 질의 [7]와 결합될 수도 있는 지시 질의는 실제 응용 분야에서 일반적으로 사용되는 매우 효과적인 접근 방식입니다. 사실, 몇 가지 인기 있는 질의 전략(예: 역할 질의(role prompting), 대상 지정(specifying an audience), 도구 사용(tool usage) 등)은 지시 질의의 더욱 구체적인 버전일 뿐입니다! 지시를 작성할 때는 최상의 결과를 보장하기 위해 명확하고 정확해야 합니다. 또한, 지시 질의는 모델이 특정 페르소나(persona)를 채택하도록 유도하여, 예를 들어 고객 서비스 상담원이나 전문 변호사처럼 행동하게 함으로써 상호작용의 품질을 크게 향상시킬 수 있습니다.

**고급 질의 전략(Advanced Prompting Strategies)**

위에서 설명한 질의 기법들은 매우 효과적이지만, 때로는 더 복잡한 질의문이 어려운 문제(예: 수학/코딩 또는 다단계 추론 문제)를 해결하는 데 유용할 수 있습니다. LLM은 이러한 문제에 본질적으로 어려움을 겪기 때문에 4 (즉, 추론 능력은 모델 규모에 따라 단조롭게 향상되지 않습니다 [9]), 질의어 공학에 대한 기존 연구의 대부분은 추론 및 복잡한 문제 해결 능력 향상에 중점을 둡니다. 단순한 질의문은 대부분의 다른 문제를 해결하는 데 작동할 것입니다.

**연쇄 사고 질의(Chain of Thought, CoT) 프롬프팅** ([10]에서 발췌)은 모델의 질의문 내 시범 사례에 연쇄 사고(즉, 일련의 중간 추론 단계)를 삽입하여 LLM의 추론 능력(reasoning capabilities)을 이끌어냅니다. (위 참조) 각 시범 사례에 연쇄 사고를 추가함으로써 모델은 (맥락 내 학습을 통해) 해당 문제에 대한 최종 응답을 출력하기 전에 유사한 연쇄 사고를 생성하는 방법을 학습합니다. 흥미롭게도 [10]에서 충분히 큰 모델(즉, 1000억 개 이상의 매개변수)은 산술, 상식 및 기호 추론 작업에서 이 접근 방식으로부터 큰 이점을 얻는다는 것을 발견했습니다. 문제를 해결하기 위한 근본적인 추론 과정을 명시적으로 설명하는 것이 실제로 모델의 추론 능력을 더욱 효과적으로 만듭니다.

([10]에서 발췌)
CoT 질의의 적용은 직관적입니다. 개별 퓨샷 시범 사례가 단순한 입력-출력 쌍으로 구성되는 대신, (입력, 사고 과정, 결과)라는 세 가지 요소로 이루어진 구조를 취합니다. (위 참조) 이 접근 방식의 주요 단점은 문제 해결을 위한 완전한 근거(rationale)를 포함하는 시범 사례를 수동으로 (또는 인위적으로) 선별(curate)해야 한다는 점인데, 이는 비용이 많이 들거나 시간이 많이 소요될 수 있습니다. 따라서 많은 논문이 CoT 질의가 사람이 작성한 근거에 의존하는 것을 제거하는 데 초점을 맞춥니다!

**CoT 변형(CoT variants)** ([11, 12]에서 발췌). CoT 질의의 효과와 인기로 인해 이 접근 방식의 수많은 확장 기능이 제안되었습니다. 예를 들어, 제로샷 CoT(zero-shot CoT) [11] 질의는 퓨샷 시범 사례를 제거하고 대신 질의문 끝에 "단계별로 생각해 봅시다(Let’s think step by step)."라는 문구를 추가하여 모델이 문제 해결 근거를 생성하도록 유도합니다. 또한 i) 문제를 해결할 때 여러 연쇄 사고를 독립적으로 생성하고 ii) 각 연쇄 사고로 생성된 최종 응답의 다수결 투표(majority vote)를 통해 추론 과정의 견고성(robustness)을 향상시킬 수 있습니다. 5 문제 해결 비용을 증가시키지만, 자기 일관성(self-consistency) [12]이라고 불리는 이 접근 방식은 더 복잡한 종류의 추론 문제를 해결할 때 LLM의 신뢰성(reliability)을 향상시킵니다.

**최소-최대 질의(Least-to-most prompting)** [13]은 복잡한 문제를 여러 부분으로 명시적으로 분해함으로써 CoT 질의를 넘어섭니다. (위 참조) 각 하위 문제(sub-problem)는 개별적으로 해결되며, 각 하위 문제의 해결책은 다음 하위 문제를 해결하기 위한 맥락으로 전달됩니다. 최종 하위 문제에 도달하면 이전 해결책의 맥락을 사용하여 질문에 대한 최종 응답을 출력할 수 있습니다. 이 방식은 인간의 문제 해결 방식과 유사하며, 복잡한 과제를 체계적으로 단순화하는 데 효과적입니다.

"LLM의 이러한 모든 발전의 근간에는 여전히 토큰(token) 수준의 결정을 하나씩 왼쪽에서 오른쪽으로 생성하는 원래의 자기회귀(autoregressive) 메커니즘이 있다는 점은 놀라울 수 있습니다." - [14]에서 발췌

**사고의 나무(Tree of thoughts, ToT) 질의** [14]. CoT 질의와 같은 기법은 다음 토큰 예측(next-token prediction)을 사용하여 단일 시도에서 해결책을 출력하는 왼쪽-오른쪽 생성(left-to-right generation) 접근 방식을 따릅니다. 이러한 접근 방식은 특정 시나리오에서는 효과적이지만, 광범위한 계획, 전략적 미리 보기(strategic lookahead), 백트래킹(backtracking), 그리고 수많은 실행 가능한 해결책의 병렬 탐색(exploration)으로부터 이점을 얻을 수 있는 복잡한 문제를 해결하는 데 실패할 수 있습니다. 여기에 ToT 질의가 등장합니다! ToT 질의는— 최소-최대 질의 [13]과 다소 유사하게 —복잡한 문제를 개별적으로 해결할 수 있는 일련의 더 간단한 문제(또는 "사고(thoughts)")로 분해합니다.

([14]에서 발췌)
CoT 질의와 달리 ToT 질의는 문제를 해결할 때 단일 사고 경로를 따를 것을 요구하지 않습니다. 또한 ToT 질의는 자기 일관성처럼 여러 추론 경로의 다수결 투표를 단순히 취하지 않습니다. (위 참조) 탐색(exploration) 과정에서 LLM은 많은 사고를 생성하고 자연어를 통해 최종 해결책을 향한 진행 상황을 지속적으로 평가합니다(즉, 우리는 모델에 질의문을 제공하기만 하면 됩니다!). 모델이 최종 해결책을 향한 자체 진행 상황을 자체 평가하는 것을 활용함으로써, 우리는 널리 사용되는 탐색 알고리즘(예: 너비 우선 탐색(breadth-first search) 또는 깊이 우선 탐색(depth-first search))으로 탐색 프로세스를 강화하여 문제 해결 과정 내에서 미리 보기와 백트래킹을 수행할 수 있습니다. ToT 질의에 대한 더 자세한 설명은 이 개요를 참조하세요.

**사고 그래프(Graph of Thoughts, GoT) 질의** ([35]에서 발췌) [35, 36]. 후속 연구는 ToT 질의에 대한 연구를 추론을 위한 그래프 기반 전략으로 일반화했습니다. 전반적으로 이러한 기법은 ToT 질의와 유사하지만, 최종 해결책을 생성하는 데 사용되는 사고 경로가 선형(linear)이라는 가정을 하지 않습니다. 오히려 해결책을 도출할 때 사고를 재사용하거나 여러 사고의 순서(sequence)를 통해 재귀(recurse)할 수도 있습니다. (위 참조) 여러 그래프 기반 질의 전략이 제안되었습니다 (자세한 내용은 여기 참조) [35, 36]. 그러나 이러한 질의 기법은— ToT 질의와 마찬가지로 —실용성이 부족하다는 비판을 받아왔습니다. 즉, GoT 질의로 추론 문제를 해결하려면 LLM으로부터 엄청난 수의 추론 단계(inference steps)가 필요할 수 있습니다!

**검색 증강 생성(Retrieval Augmented Generation, RAG) 파이프라인의 핵심**

기본 RAG 파이프라인(RAG pipeline)
검색 증강 생성(Retrieval Augmented Generation, RAG) [37] (위 그림 참조)은 순수한 질의 기법은 아니지만, 질의문에 포함할 관련 맥락을 검색하여 LLM 결과물의 품질을 향상시키는 널리 사용되는 전략입니다. 유용한 맥락을 검색하기 위해 기존 검색 기술을 활용할 수 있습니다. 예를 들어, 순수 벡터 검색(pure vector search) 또는 하이브리드 검색 엔진(hybrid search engine)입니다. 단순함에도 불구하고 연구에 따르면 RAG는 LLM에 지식을 주입하고 모델이 생성하는 환각(hallucination)의 수를 줄이는 데 매우 효과적입니다 [38]. 또한 RAG에 의해 검색된 관련 문서를 단순히 노출함으로써 LLM 사용자에게 쉽게 인용(citation)을 제공할 수 있습니다. 그러나 데이터를 처리하고 검색하는 방식과 질의문에 삽입되는 맥락을 구성하는 방식은 성능에 상당한 영향을 미칠 수 있습니다. 자세한 내용은 여기를 참조하세요. RAG는 LLM의 지식 한계를 보완하고 최신 정보를 제공하는 데 필수적인 방법론으로 자리 잡고 있습니다.

**생성된 지식 질의(Generated knowledge prompting)** ([39]에서 발췌) [39]는 외부 데이터베이스에서 맥락을 검색하는 대신 LLM을 사용하여 질의문에 포함할 관련 맥락을 생성하는 RAG의 흥미로운 대안입니다. (위 참조) 매우 간단하고 긍정적인 성능 지표를 가지고 있음에도 불구하고, 이 접근 방식은 (당연히) LLM이 정보를 환각하는 경향 때문에 신뢰성이 부족합니다.

**최근 연구 동향(Recent Directions of Research)**

지금까지 다양한 질의 기법들을 다루었지만, 최근에는 이러한 방법들을 확장하고 복잡한 문제 해결을 위한 완전히 새로운 스타일의 질의문들을 탐구하는 많은 논문이 발표되었습니다. 여기서는 이 연구들을 주제 또는 초점에 따라 여러 범주로 분류했습니다.
*   추론(Reasoning)
*   도구 활용(Tool Usage)
*   프로그램 보조 언어 모델(Program-Aided Language Models)
*   컨텍스트 창 심화(Context Windows)
*   창의적 글쓰기(Writing)
*   기타 (다른 주목할 만한 논문)

각 범주에 대해 다양한 다른 연구가 다루어집니다. 그러나 질의어 공학 주제에 대해 발표된 엄청난 양의 연구를 고려할 때, 몇몇 논문이 누락되었을 가능성이 큽니다. 포함되어야 할 좋은 논문을 알고 있다면 댓글로 공유해 주세요!

**추론 능력 향상(Improving Reasoning Capabilities)**

**Auto-CoT** ([15]에서 발췌) [15]. CoT 질의는 복잡한 문제를 해결하기 위해 중간 추론 단계(intermediate reasoning steps)를 사용하며, LLM의 결과물 내에서 이러한 추론 단계를 이끌어낼 수 있는 두 가지 방법이 있습니다(위 그림 참조).
*   **제로샷(Zero-shot)**: LLM에 "단계별로 생각하라(think step-by-step)"고 질의문을 제공합니다.
*   **수동(Manual)**: 원하는 질문에 답하기 전에 질문, 근거, 답변에 대한 몇 가지 퓨샷 시범 사례를 제공합니다.

LLM이 괜찮은 제로샷 추론기(zero-shot reasoners)이지만, 구체적인 시범 사례를 제공하는 것이 CoT 질의에서 일관되게 더 나은 성능을 제공합니다. 그러나 이 전략은 또한 인간 주석자(human annotators)— 또는 질의어 엔지니어 —가 각 질문에 답하는 데 사용되는 근거에 대한 수동 시연(manual demonstrations)을 작성하도록 요구합니다. 이러한 수동 시연을 작성하는 것은 시간이 많이 걸리지만, 피할 수 있습니다!

"우리는 '단계별로 생각해 봅시다(Let’s think step by step)' 질의문을 가진 LLM을 활용하여 시연을 위한 추론 연쇄(reasoning chains)를 하나씩 생성함으로써 그러한 수동 노력을 제거할 수 있음을 보여줍니다." - [15]에서 발췌

[15]에서 저자들은 제로샷 CoT 질의를 사용하여 수동 CoT 질의를 위한 시범 사례를 자동으로 생성하는 자동 CoT(Auto-CoT) 질의 접근 방식을 제안하여, 문제 해결 근거를 수동으로 작성할 필요성을 없앴습니다. 그러나 이러한 자동으로 생성된 근거가 어떤 경우에는 부정확하기 때문에 Auto-CoT가 잘 작동하려면 몇 가지 요령이 필요합니다. 질문이 입력으로 주어졌을 때, 순진한 접근 방식은 i) 유사한 질문 집합을 검색하고(예: sBERT와 같은 임베딩 모델(embedding model) 및 벡터 검색(vector search) 사용), ii) 제로샷 CoT 질의로 각 질문에 대한 근거/답변을 생성하며, iii) 자동으로 생성된 시연으로 수동 CoT 질의를 수행하는 것입니다. 그러나 이 접근 방식은 상당히 좋지 않게 작동하며, [1]의 저자들은 이를 LLM이 생성한 근거의 실수 때문이라고 주장합니다. 이를 해결하기 위해 생성된 근거가 충분히 다양하도록 보장하기만 하면 됩니다.

([15]에서 발췌)
LLM이 응답해야 할 질문 데이터셋이 주어졌을 때, [15]의 저자들은 Auto-CoT를 위한 질의문 내에서 사용되는 시연을 선택/생성하기 위한 두 부분으로 된 전략(위 그림 참조)을 고안했습니다.
*   sBERT의 질문 임베딩(question embeddings)과 k-평균 클러스터링(k-means clustering)을 사용하여 질문을 k개의 클러스터(cluster)로 나눕니다.
*   각 클러스터에서 대표 질문을 선택하고 제로샷 CoT를 사용하여 각 질문에 대한 관련 근거를 생성합니다.

이러한 접근 방식은 Auto-CoT에 사용되는 시연의 다양성을 높여, 모델이 합성 근거(synthetic rationales)에서 저지르는 실수 간의 상관관계를 줄입니다. GPT-3를 사용한 실험에서 Auto-CoT는 수동 시연 생성이 필요한 퓨샷 CoT 질의의 성능과 일치하거나 이를 능가했으며, 10개 이상의 다른 벤치마크(benchmark)에서 일관된 결과를 보였습니다.

**복잡성 기반 질의(Complexity-Based Prompting)** ([16]에서 발췌) [16]. CoT 질의가 질의문에 포함할 문제 해결 근거의 시연을 선택하는 데 의존한다는 점을 고려할 때, 우리는 궁금할 수 있습니다. 이러한 시연을 어떻게 가장 잘 선택할 수 있을까요? [16]에서 저자들은 복잡성을 기반으로 시연을 선택하는 것이 좋은 발견적 방법(heuristic)임을 보여줍니다. 시연의 복잡성은 연쇄 사고 내에 존재하는 단계의 수를 세는 것으로 간단히 측정할 수 있으며, 개별 단계는 개행 문자(newline characters, \n )로 구분됩니다. [16]에서 제안된 복잡성 기반 질의 접근 방식은 가장 높은 복잡성을 가진 시연을 표본 추출(sampling)할 것을 옹호합니다.

"GPT-3 175B의 추론 성능은 입력 질의문의 복잡성이 증가함에 따라 명확하게 향상됩니다." - [16]에서 발췌

흥미롭게도 [16]의 저자들은 CoT 질의문에 더 많은 추론 단계를 포함하는 시연을 포함하는 것이 다단계 추론 작업에서 성능을 상당히 향상시킨다는 것을 발견했습니다. 더 나아가, 이 전략을 결과물 공간으로 확장하여 가장 높은 복잡성을 가진 k개의 생성된 결과물에 대해 다수결 투표를 하는 자기 일관성(self-consistency) 접근 방식을 사용할 수 있습니다. 수동 튜닝(manual tuning) 및 검색 기반 선택(retrieval-based selection)과 같은 대안적인 선택 방식과 비교할 때, 복잡성 기반 질의는 GPT-3 및 Codex를 사용하여 여러 데이터셋(즉, GSM8K, MultiArith, MathQA)에서 최첨단(state-of-the-art) 성능을 달성하며 유리하게 작동합니다.

**점진적 힌트 질의(Progressive-Hint Prompting, PHP)** ([17]에서 발췌) [17]. CoT 질의의 한 가지 단점은 문제를 단일 시도(single shot)로 해결한다는 것입니다. 질문이 입력으로 주어지면 우리는 근거와 응답을 생성하지만, LLM은 이 응답을 고려하거나 수정할 기회를 얻지 못합니다. 이 과정을 여러 번 반복하고 다수결 투표를 통해 더 나은 성능을 달성할 수 있습니다— 이것은 단지 자기 일관성입니다 —그러나 이러한 생성물 중 어느 것도 응답을 더 잘 알리기 위해 LLM의 이전 결과물을 고려하지 않습니다.

"PHP는 질문을 재평가한 후 이전 응답을 힌트로 활용하여 올바른 응답에 도달하는 인간과 유사한 사고 과정을 따릅니다." - [17]에서 발췌

이 문제를 해결하기 위해 [17]의 저자들은 LLM의 이전 결과물을 활용하여 생성된 근거를 반복적으로 개선하는 PHP를 제안합니다. 직관적으로 LLM은 모델이 이전에 생성한 근거를 올바른 응답을 찾는 힌트로 사용할 수 있습니다. 구체적으로 PHP는 세 단계로 진행됩니다.
*   질문이 주어지면 LLM에 기본 응답을 제공하도록 질의문을 제공합니다.
*   질문과 기본 응답을 연결한 다음, 이 입력을 기반으로 LLM에 수정된 응답을 생성하도록 질의문을 제공합니다.
*   LLM의 응답이 최소 두 번의 반복 동안 안정적일 때까지 2단계를 반복합니다.

이러한 접근 방식은 LLM이 여러 번의 통과를 통해 응답을 반복적으로 개선하고, 이 과정에서 이전 결과물을 맥락으로 사용할 수 있도록 합니다. 또한 PHP는 CoT 질의 및 자기 일관성과 완벽하게 호환됩니다. 이러한 기술을 결합하여 성능을 더욱 향상시킬 수 있습니다. 실험에서 PHP는 복잡성 기반 질의 전략과 비교하여 GPT-3.5의 성능을 향상시키며, GPT-4와 함께 PHP를 사용하면 여러 주목할 만한 데이터셋(예: SVAMP, GSM8K, AQuA, MATH)에서 최첨단 성능을 달성합니다.

**분해 질의(Decomposed Prompting, DecomP)** ([18]에서 발췌) [18]은 질의를 통해 복잡한 단계를 가진 다단계 추론 문제를 해결하는 어려움을 해결하려고 시도합니다. 작업이 더 복잡해질수록 퓨샷 질의(즉, 올바른 해결책의 몇 가지 시범 사례를 보여주는 것)는 부족해질 것입니다. 그러나 복잡한 작업을 질의를 통해 독립적으로 해결할 수 있는 하위 작업(sub-tasks)으로 분해함으로써 더 잘 수행할 수 있습니다. 특히 [18]의 저자들은 두 가지 구성 요소로 이루어진 질의 프레임워크를 제안합니다.
*   **분해자(Decomposer)**: LLM에 문제를 일련의 더 간단한 하위 작업으로 분해하도록 질의문을 제공합니다.
*   **하위 작업 처리기(Sub-task handlers)**: 별도의 질의문을 사용하여 (분해자가 지시하는 대로) 더 간단한 하위 작업을 LLM으로 해결합니다.

분해자와 하위 작업 처리기는 퓨샷 방식으로 질의가 제공되는 LLM일 뿐입니다. 위에서 제안된 DecomP 전략은 하나의 질의문을 사용하여 해결 가능한 하위 작업을 식별하고, 이를 다른 시스템(예: 새로운 질의문, 다른 LLM 또는 도구)에 위임하여 해결합니다. 이러한 모듈식 접근 방식은 많은 이점을 가집니다.
*   긴 맥락을 가진 작업은 여러 구성 요소로 분해될 수 있습니다.
*   각 하위 작업에는 더 넓은 범위의 시범 사례를 보여줄 수 있습니다.
*   복잡한 하위 작업은 필요한 경우 더 작은 하위 작업으로 추가 분해될 수 있습니다.
*   모든 하위 작업을 LLM으로 해결하는 대신, 다른 기호 시스템(symbolic systems)(예: 작업별 모델(task-specific model), 검색 메커니즘(retrieval mechanism) 등)을 사용할 수도 있습니다.

간단한 작업을 예로 들어 봅시다. 단어 집합이 입력으로 주어졌을 때, 각 단어의 세 번째 문자를 추출하고, 이 문자들을 연결하여 그 연결된 결과를 결과물로 제공하고자 합니다. 이를 위해 세 가지 하위 작업의 순서(sequence)를 만들 수 있습니다. i) 단어 목록 수집, ii) 각 단어의 세 번째 문자 추출, iii) 추출된 문자 연결. 아래 그림과 같이 이러한 각 하위 작업을 별도의 퓨샷 질의문으로 구현할 수 있습니다.

([18]에서 발췌)
DecomP 내에서 하위 작업은 분해자에 의해 반복적으로 생성되고, 해결되며, (관련 결과물과 함께) 분해자로 반환되어 다음 하위 작업을 생성합니다. 분해자는 추론 과정의 제어자(controller) 역할을 하면서 최종 응답이 생성되었음을 나타내는 질문 끝(end-of-question, [EOQ]) 마커가 생성될 때까지 하위 작업을 계속 생성합니다. (아래 참조) 전반적으로 DecomP는 최소-최대 질의의 더 일반적이고 유연한 버전으로 생각할 수 있습니다.

([18]에서 발췌)
**가설-이론(Hypotheses-to-Theories, HtT)** ([29]에서 발췌) [29]. 복잡한 작업을 간단한 단계로 분해하는 시범 사례 근거로 모델에 질의문을 제공함으로써 LLM 내에서 추론 능력을 이끌어낼 수 있습니다. 그러나 모델은 결과물을 생성할 때 환각(hallucinate)을 일으킬 수 있으며, 기존 또는 상식적 지식을 넘어선 작업에서는 성능이 좋지 않습니다. 간단히 말해, LLM의 지식 기반(knowledge base)과 작업을 해결하는 데 필요한 지식 사이에 불일치(mismatch)가 있을 때 문제가 발생합니다. 이 문제를 해결하기 위해, 복잡한 추론 문제를 해결할 때 LLM이 필요한 지식을 발견하고 적용할 수 있도록 하는 질의 접근 방식이 필요합니다.

([29]에서 발췌)
인간의 과학적 발견 과정에서 영감을 받아, [29]의 저자들은 가설-이론(Hypotheses-to-Theories, HtT) 질의라는 질의 기법을 제안합니다. 이 기법은 (잠재적으로 부정확한) 가설을 자유롭게 제안하고, 경험적으로 검증될 수 있는 가설만을 유지하며, 이 검증된 가설을 사용하여 문제를 해결하는 전략을 따릅니다. 높은 수준에서 이 전략의 목표는 문제 해결에 사용될 수 있는 LLM을 위한 규칙 라이브러리(rule library)를 학습하는 것입니다. 더 구체적으로, HtT 질의(위 그림 참조)는 두 단계로 구성됩니다.
*   **귀납(Induction)**: LLM은 훈련 시범 사례(training examples) 집합에 대해 규칙을 생성하고 검증하도록 요청받습니다. 자주 나타나고 올바른 응답을 자주 생성하는 규칙들이 수집되어 규칙 라이브러리를 형성합니다.
*   **연역(Deduction)**: LLM은 귀납을 통해 생성된 규칙 집합을 사용하여 추론을 수행하고 질문에 응답하도록 질의가 제공됩니다.

추론 중에 규칙 집합을 사용함으로써 HtT 질의는 환각의 가능성을 줄입니다. 이러한 발견은 수치 추론(numerical reasoning) 및 관계 추론(relational reasoning) 작업 모두에서 검증되었으며, HtT 질의는 이전 질의 기법(예: CoT 질의)에 비해 정확도에서 11-27%의 절대적인 개선을 제공하는 것으로 나타났습니다. 흥미롭게도 HtT 질의로 생성된 규칙은 해석 가능(interpretable)하며 다른 (그러나 유사한) 문제로도 전이 가능(transferable)합니다.

**도구 활용(Tool Usage)의 확장**

LLM은 강력하지만, 주목할 만한 한계가 있습니다! 예를 들어, LLM은 산술적 오류를 저지르고, 최신 정보에 접근할 수 없으며, 심지어 시간의 흐름을 이해하는 데 어려움을 겪기도 합니다. 인류의 많은 발전은 새롭고 혁신적인 도구(예: 인쇄기 또는 컴퓨터)에 대한 접근에 의해 촉진되었으며, LLM도 마찬가지일 수 있습니다. 즉, 외부의 전문화된 도구(예: 계산기 또는 검색 엔진) 집합에 대한 접근 권한을 부여하고, 언제, 어디서, 어떻게 이러한 도구를 적절하게 호출하여 문제를 더 안정적으로 해결할 수 있는지 모델에 가르침으로써 이러한 모델의 많은 한계를 해결할 수 있습니다. 더 많은 정보는 아래의 이 주제에 대한 이전 개요를 참조하세요.
*   언어 모델에게 도구 사용법 가르치기(Teaching Language Models to Use Tools) [링크]
*   언어 모델과 친구들(Language Models and Friends) [링크]
*   언어 모델이 자체 도구를 만들 수 있을까?(Can language models make their own tools?) [링크]

**Toolformer** ([32]에서 발췌) [32]는 LLM과 외부 도구의 통합을 탐구한 최초의 연구 중 하나였습니다. 이러한 도구는 간단하고 고정된 텍스트-투-텍스트 API(text-to-text API) 집합을 통해 모델에 제공됩니다. (위 참조) 도구를 사용하기 위해 LLM은 i) 도구가 필요한 시나리오를 식별하고, ii) 사용할 도구를 지정하며, iii) 도구의 API에 관련 텍스트 입력을 제공하고, iv) API에서 반환된 텍스트를 사용하여 응답을 작성하는 방법을 학습해야 합니다. LLM은 초기 시드 데이터셋(seed dataset)으로 시작하여 더 강력한 LLM(예: GPT-4)을 사용하여 유효한 API 호출(API calls) 시범 사례를 데이터에 추가하는 합성 훈련 데이터셋(synthetic training dataset)을 구성함으로써 이러한 기술을 학습합니다. (아래 참조)

([32]에서 발췌)
여기에서 우리는 이 데이터에 대해 LLM을 간단히 미세 조정할 수 있습니다. 모델은 자신이 생성하는 텍스트 순서 내에서 필요한 API 호출을 직접 생성하고 처리하는 방법을 학습할 것입니다. 이 경우, 텍스트 입력과 결과물을 가진 API만 고려하기 때문에 API 호출을 인라인(inline) 방식으로 처리하는 것은 간단합니다. (아래 참조)

([32]에서 발췌)
"LLM은 최신 정보에 접근할 수 없거나 정밀한 수학적 추론을 수행할 수 없는 것과 같은 본질적인 한계에 직면합니다... 실제 작업 해결을 위해 외부 도구를 자동으로 구성하는 기능을 현재 LLM에 강화하는 것은 이러한 단점을 해결하는 데 중요합니다." - [19]에서 발췌

**Chameleon** [19]은 위에서 언급된 LLM의 한계를 완화하는 것을 목표로 합니다. 흥미롭게도, 이러한 한계 중 일부는 LLM과 외부 도구를 통합하는 기존 연구에서 다루어지지 않습니다. 사용되는 도구 집합이 일반적으로 고정되어 있거나(또는 도메인 특정적(domain-specific)) 항상 새로운 도메인(domain)으로 일반화될 수 없기 때문입니다. 더 일반적인 프레임워크를 만들기 위해 Chameleon은 "플러그 앤 플레이(plug-and-play)" 전략을 사용합니다. 이 전략은 중앙 LLM 기반 제어자(controller)를 사용하여 복잡한 추론 작업을 해결하기 위해 여러 도구를 구성하는 프로그램— 자연어로 작성된 —을 생성합니다. (아래 참조) 이전 연구와 달리 Chameleon이 사용할 수 있는 도구는 상당히 포괄적입니다. 예를 들어, LLM, 상용 비전 모델(off-the-shelf vision models), 웹 검색 엔진(web search engines), Python 함수 등입니다.

([19]에서 발췌)
Chameleon 프레임워크는 두 가지 주요 구성 요소를 가집니다.
*   **계획자(Planner)**: 입력 질의(input query)를 사용 가능한 도구를 통해 해결할 수 있는 하위 작업으로 분해합니다.
*   **모듈 재고(Module inventory)**: Chameleon이 사용할 수 있는 작업별 도구(설명 및 사용 시범 사례 포함) 집합.

LLM으로 구현된 계획자는 자연어를 사용하여 외부 도구(예: image_captioner 또는 query_generator)에 대한 호출을 생성합니다. 우리는 간단한 문자열 일치(string matching)를 통해 이러한 도구를 식별할 수 있으며, 계획자가 출력하는 도구 순서(sequence)는 해당 작업별 모듈(task-specific modules) 각각을 호출하여 실행될 수 있는 자연어 프로그램(natural language program)을 형성합니다. 계획자와 작업별 모듈에 사용되는 질의문의 시범 사례는 아래에 나와 있습니다.

([19]에서 발췌)
제어자에게 특정 도구를 언제 사용할지 가르치기 위해, 우리는 퓨샷 질의문 내에 도구 설명과 사용 시범 사례를 포함하며, 이는 새로운 도구와 모듈로 쉽게 확장될 수 있습니다. 계획자의 맥락 내 학습 능력을 활용하여 해결책을 생성하기 때문에, 실제 질의를 해결하는 데 훈련이나 선별된 규칙이 필요하지 않습니다. 대신, 우리는 LLM에 사용 가능한 도구의 시범 사례를 제공하기만 하면, LLM은 이 정보를 사용하여 질의에 대한 올바른 최종 응답을 생성하기 위해 실행될 수 있는 도구 순서(sequence)를 추론할 수 있습니다. 더 나아가, 이 도구 순서는 사람이 읽을 수 있으며 인간 사용자가 쉽게 디버깅(debug)할 수 있습니다.

([19]에서 발췌)
실험에서 Chameleon은 GPT-4를 사용하여 두 가지 복잡한 다중 모달(multi-modal)(즉, 텍스트와 이미지가 모두 포함됨) 추론 작업인 ScienceQA와 TabMWP에 적용되었습니다. Chameleon은 ScienceQA에서 86.54%의 새로운 최첨단 성능을 달성하여 GPT-4 및 GPT-3를 사용한 CoT 질의보다 각각 2.55% 및 11.37% 더 나은 성능을 보였습니다. TabMWP에서도 Chameleon은 98.78%의 정확도를 달성하며 유사한 개선을 보였습니다. 그러나 Chameleon의 효과는 GPT-4가 복잡한 추론 문제를 해결하기 위한 제약 조건을 추론하고 합리적/일관된 계획을 구성하는 능력에 의해 강화된다는 점에 유의해야 합니다.

"우리는 고급 LLM의 자기 지시(self-instruct)를 통해 오픈 소스 LLM에 도구를 사용할 수 있는 능력을 부여하도록 설계된 간단하면서도 효과적인 방법인 GPT4Tools를 제안합니다." - [20]에서 발췌

**GPT4Tools** [20]. 다양한 논문에서 LLM이 퓨샷 방식으로 도구를 활용하는 능력을 보여주었지만, 대부분의 논문은 독점적인 언어 모델(proprietary language models)에 의존하며 도구 사용을 용이하게 하기 위해 순전히 질의어 공학을 활용합니다. 이는 오픈 LLM으로도 유사한 결과를 재현할 수 있는지 궁금하게 만듭니다. [20]에서 저자들은 자기 지시(self-instruct) [21]를 사용하여 오픈 소스 LLM(예: LLaMA 및 OPT)이 다중 모달 도구(multimodal tools) 집합을 사용할 수 있도록 하는 미세 조정 데이터셋(finetuning dataset)을 생성하는 접근 방식을 제안합니다.

([21]에서 발췌)
먼저, 저자들은 강력한 교사 모델(teacher model)(즉, ChatGPT)에 질의문을 제공하여 관련 도구가 사용되는 시범 사례를 생성하도록 함으로써 자기 지시 접근 방식을 사용하여 도구 사용 데이터셋을 생성합니다. 질의문 내에는 시각적 내용— 이미지에서 추출된 캡션(captions) 및 바운딩 박스(bounding boxes) —과 도구 설명이 모두 포함됩니다. 교사는 이 정보를 활용하여 다중 모달 정보를 처리하고 문제를 해결하는 데 사용될 수 있는 도구 관련 지시를 생성합니다. (위 참조)

([21]에서 발췌)
데이터셋이 생성되면, 저랭크 적응(Low-Rank Adaptation, LoRA)을 사용하여 오픈 소스 LLM을 쉽게 미세 조정하여 다중 모달 도구의 도움을 받아 다양한 시각적 문제를 해결할 수 있습니다. [20]에서 이 접근 방식은 LLM이 알려진 도구(즉, 미세 조정 데이터셋에 포함된 도구)에 대해 수행하는 호출의 정확도를 향상시킬 뿐만 아니라, 모델이 제로샷 방식으로 새로운 도구에 일반화하는 능력도 향상시키는 것으로 나타났습니다. GPT4Tools와 LLM을 외부 도구와 통합하는 이전 연구에 대한 직접적인 비교는 위 표에 제공됩니다.

**Gorilla** ([30]에서 발췌) [30]. 많은 연구에서 LLM과 고정된 도구 집합을 통합하는 것을 연구했지만, [30]의 저자들은 LLM에게 온라인에서 사용 가능한 모든 모델 API를 사용하도록 가르치는 더 넓은 목표를 다룹니다. 이를 위해 i) 문제 해결과 관련된 모델 API를 검색하고 ii) 이 API에 대한 문서를 모델의 맥락에 추가하는 검색 기술(retrieval technique)이 채택됩니다. 이러한 접근 방식은 LLM이 엄청난 수의 변화하는 도구에 접근할 수 있도록 하지만, 환각(예: 잘못된 인수(arguments) 또는 존재하지 않는 API 호출)이 여전히 발생할 수 있습니다. (위 참조)

([30]에서 발췌)
이 문제를 해결하기 위해 [30]의 저자들은 자기 지시 [21]를 사용하여 1,600개 이상의 다른 모델 API 사용 시범 사례가 포함된 데이터셋을 구축합니다. 각 시범 사례 내에서 질의문과 관련 문서가 모두 맥락으로 사용되어 결과물을 생성합니다. 다시 말해, 이것은 검색 인식 미세 조정(retrieval-aware finetuning) 프로세스입니다 (RAFT와 유사). (위 참조) 그 결과 모델인 Gorilla (LLaMA-7B의 미세 조정 버전)는 다양한 심층 학습 모델 API를 활용하여 문제를 해결하기 위한 인터페이스입니다. 결과 LLM은 엄청난 수의 API를 사용할 수 있으며, 심지어 이러한 API 중 어느 하나의 문서 변경에도 적응할 수 있습니다!

**HuggingGPT** ([31]에서 발췌) [31]는 도구 사용 접근 방식을 통해 LLM과 특수 심층 학습 모델(예: 이미지 인식, 비디오 감지, 텍스트 분류 등)의 통합을 탐구한다는 점에서 Gorilla와 상당히 유사합니다. LLM은 문제 해결 시스템의 "두뇌" 역할을 하며, 문제를 해결하는 방법을 계획하고 이 문제에 필요한 하위 작업을 해결하는 다양한 심층 학습 모델 간의 노력을 조정합니다. 그러나 Gorilla와 달리 HuggingGPT는 미세 조정을 수행하지 않습니다. 문제 해결은 네 단계로 분해됩니다.
*   **작업 계획(Task planning)**: LLM을 사용하여 사용자의 요청을 해결 가능한 작업으로 분해합니다.
*   **모델 선택(Model selection)**: HuggingFace에서 작업을 해결하는 데 사용할 모델을 선택합니다.
*   **작업 실행(Task execution)**: 선택된 각 모델을 실행하고 결과를 LLM에 반환합니다.
*   **응답 생성(Response generation)**: LLM을 사용하여 사용자에게 최종 응답을 생성합니다.

이러한 각 단계에 대해, 우리는 선별된 지시와 시범 사례를 포함하는 질의를 활용하여 원하는 동작을 얻습니다. (시범 사례 질의문은 아래 참조) 충분히 강력한 기반 모델(foundation model)이 주어진다면, 이러한 접근 방식은 매우 효과적입니다.

([31]에서 발췌)

**프로그램 보조 언어 모델(Program-Aided Language Models)의 진화**

"계산은 생성된 프로그램을 실행하는 데 사용되는 프로그램 해석기(program interpreter)에 위임될 수 있으며, 이로써 복잡한 계산을 추론 및 언어 이해와 분리합니다." - [41]에서 발췌

LLM을 외부 도구와 통합하는 것은 흥미로운 연구 분야이며, 이러한 모델에 접근 권한을 부여할 수 있는 가장 유용한 도구 중 하나는 프로그램을 작성하고 실행하는 능력입니다. 대부분의 질의 기법은 복잡한 문제를 두 단계로 해결합니다.
*   문제 해결 근거를 생성합니다.
*   이 근거를 사용하여 실제로 문제를 해결합니다.

CoT 질의에서는 이 두 단계를 모두 LLM이 해결하도록 의존하지만, 이 모델들은 첫 번째 단계 해결에만 탁월합니다! 사실, 올바른 근거를 출력했음에도 불구하고 잘못된 응답을 생성하는 것은 LLM의 흔한 실패 사례입니다. 이 문제를 해결하기 위해, 모델이 언어와 코드(예: 유용한 주석이 있는 Python 프로그램)가 섞인 형태로 근거를 출력하도록 가르칠 수 있습니다. 그런 다음, 제공된 코드를 단순히 실행함으로써 최종 응답을 생성할 수 있습니다!

**PAL(Program-Aided Language Model)** ([40]에서 발췌) [40]은 LLM이 해결책을 찾기 위해 문제를 일련의 중간 단계로 분해하는 작업을 맡는다는 점에서 CoT 질의와 유사합니다. 그러나 이 근거는 자연어와 프로그래밍 구성 요소(programatic components)를 모두 포함합니다. 우리는 근거에서 코드를 실행하여(샌드박스(sandboxed) Python 환경 사용) 신뢰할 수 있는 최종 해결책을 생성할 수 있습니다. 실제 해결책 생성 과정은 코드 해석기(code interpreter)에 위임됩니다. [40]에서 우리는 코드에 대해 충분히 훈련된 LLM(예: Codex)이 퓨샷 학습 접근 방식을 사용하여 이러한 방식으로 문제를 해결하도록 가르칠 수 있음을 알 수 있습니다.

"이는 연쇄 사고와 유사한 방법에서 추론 연쇄는 올바르지만 잘못된 응답을 생성할 수 있는 중요한 격차를 해소합니다." - [40]에서 발췌

**사고 프로그램(Program of Thoughts, PoT) 질의** ([41]에서 발췌) [41]은 i) 코드 증강 질의 기술(code-augmented prompting technique)을 사용하고 ii) 해결책을 도출하는 과정을 코드 해석기에 위임한다는 점에서 PAL과 상당히 유사합니다. 이 과정은 퓨샷 질의 전략에 의존합니다. (위 참조) 그러나 PAL과 달리 PoT가 작성한 코드는 SymPy라는 기호 수학 라이브러리(symbolic math library)에 의존합니다. 이 패키지는 사용자가 수학적 "기호"를 정의할 수 있게 하며, 이 기호들은 SymPy의 solve 함수를 통해 평가되는 복잡한 표현식을 형성하도록 결합될 수 있습니다. (아래 참조)

([41]에서 발췌)
높은 수준에서 PoT는 LLM이 복잡한 방정식을 해결할 수 없다는 점을 직접적으로 다루며, 이러한 방정식을 쉽게 구성/평가할 수 있도록 하는 기호 수학 라이브러리에 대한 접근을 제공합니다. 반면 PAL은 자연어와 코드의 조합을 통해 문제를 더 일반적으로 해결하는 데 중점을 둡니다. 프로그램 보조 모델에 대한 더 많은 정보는 이 관련 개요를 참조하세요.

**컨텍스트 창 이해 및 활용 심화(Understanding and Using the Context Window)**

RAG의 최근 인기와 최첨단 LLM 내의 긴 컨텍스트 창에 대한 강조를 고려할 때, 이러한 모델이 질의문에 제공된 맥락을 어떻게 처리하는지 이해하는 것이 중요합니다. 다행히도 최근 연구는 컨텍스트 창과 맥락 내 학습 주제를 심층적으로 연구하여 질의어 공학과 관련된 몇 가지 흥미로운 시사점을 도출했습니다.

**대규모 언어 모델은 관련 없는 맥락에 쉽게 주의가 산만해질 수 있습니다** [22]. 언어 모델에 질의문을 제공할 때, 우리는 일반적으로 질의문 내에 관련 맥락과 정보만 포함합니다. 그러나 실제 응용 분야에서는 모델의 질의문이 일반적으로 해결되는 특정 문제와 관련이 있을 수도 있고 없을 수도 있는 맥락적으로 유사한 정보를 포함합니다. 이를 염두에 두고, 우리는 궁금할 수 있습니다. 질의문에 관련 없는 맥락을 추가하는 것이 부정적인 부작용을 가져올까요?

([22]에서 발췌)
[22]에서 저자들은 현대 LLM의 주의 산만성(distractibility)을 연구했으며, 관련 없는 맥락이 질의문에 포함될 때 이러한 모델의 성능이 급격히 저하될 수 있음을 발견했습니다. LLM 주의 산만성을 측정하기 위해 저자들은 문제 설명에 관련 없는 정보가 포함된 산술 추론 문제(arithmetic reasoning problems)를 포함하는 새로운 관련 없는 맥락을 포함한 초등학교 수학(Grade-School Math with Irrelevant Context, GSM-IC) 데이터셋을 소개합니다. (위 참조) 그런 다음, 모델의 질의문에 관련 없는 문장을 추가하는 것이 문제의 결과 해결책을 변경하는지 여부를 테스트함으로써 LLM이 관련 없는 맥락에 의해 주의가 산만해지는지 여부를 측정할 수 있습니다.

([22]에서 발췌)
이 전략은 Codex와 GPT-3.5를 여러 다른 질의 기법(그림은 위 참조)으로 테스트하는 데 사용됩니다.
*   CoT 질의 (및 제로샷 CoT 질의)
*   최소-최대 질의
*   프로그램으로 질의

흥미롭게도, 관련 없는 정보가 맥락에 포함될 때 이러한 모델의 성능은 급격히 저하됩니다. 그러나 관련 없는 맥락의 영향은 i) 자기 일관성 사용, ii) 모델이 관련 없는 정보를 무시하도록 지시 추가, iii) 관련 없는 정보로 문제를 해결하는 것을 시연하는 퓨샷 시범 사례 포함을 통해 완화될 수 있습니다. LLM은 지시 또는 맥락을 통해 정보를 무시하는 것을 학습할 수 있습니다.

"우리는 시범 사례에 '문제 설명의 관련 없는 정보는 자유롭게 무시하세요'라는 지시 문장을 앞에 추가합니다." - [22]에서 발췌

**중간에서 길을 잃다(Lost in the Middle)** [23]. 생성형 LLM은 텍스트-투-텍스트 형식을 가지며, 이는 텍스트 순서(sequence)를 입력(즉, 질의문)으로 받아들이고 해당 텍스트 순서를 결과물로 생성한다는 의미입니다. LLM에 전달되는 입력은 가변 길이(variable length)입니다. 짧은 (제로샷) 문제 설명일 수도 있고, 많은 양의 외부 맥락(예: RAG용)을 포함하는 복잡한 지시일 수도 있습니다. 이러한 이유로 LLM은 긴 맥락에서 작동하고 이 맥락 전체를 사용하여 다운스트림 작업(downstream tasks)을 효과적으로 해결할 수 있어야 합니다. 이러한 맥락에서 [23]의 저자들은 여러 LLM— 오픈 모델(MPT)과 폐쇄 모델(GPT-3.5-Turbo 및 Claude-1.3) 모두 —이 긴 맥락 내에서 제공된 정보를 구체적으로 활용하는 능력을 연구합니다. 특히 [23]에서는 두 가지 유형의 작업이 연구됩니다.
*   **다중 문서 QA(Multi-document QA)**: 표준 RAG 설정과 유사하게, 이 문제는 모델이 여러 문서를 추론하여 질문에 응답하도록 요구합니다.
*   **키-값 검색(Key-value retrieval)**: 이는 맥락으로 제공된 JSON 키-값 쌍(key-value pairs) 컬렉션에서 키와 관련된 값을 반환하여 일치하는 토큰을 검색하는 모델의 능력을 테스트하는 합성 작업입니다.

이러한 작업을 해결할 때 저자들은 i) 입력 맥락의 길이(더 많은 문서 또는 키-값 쌍 사용)와 ii) 입력 내 관련 맥락의 위치— 시작, 중간, 끝 —를 모두 제어합니다. 그런 다음, 맥락 길이와 위치 변화가 모델 성능에 미치는 영향을 연구할 수 있습니다. 실험에서 우리는 모델 맥락 내 관련 정보의 위치를 기반으로 명확한 "U자형" 성능 곡선(아래 그림 참조)을 볼 수 있습니다.

([23]에서 발췌)
이 시각화는 LLM이 맥락의 시작과 끝에 있는 정보에 가장 많은 주의를 기울인다는 것을 보여줍니다. 관련 정보가 맥락의 중간에 있을 때 모델 성능은 크게 저하됩니다— 정보가 "중간에서 길을 잃습니다(lost in the middle)". 사실, GPT-3.5-Turbo는 관련 문서가 맥락 중간에 배치될 때보다 관련 맥락이 전혀 없을 때 다중 문서 QA 작업에서 더 나은 성능을 보입니다. 관련 정보의 위치를 조정함에 따라 성능은 크게 달라지며, 확장된 맥락을 가진 모델은 이러한 위치 편향(positional biases)에 대한 견고성 개선 징후를 보이지 않습니다. (아래 참조) 그러나 이러한 문제는 더 최근 모델(예: Gemini-1.5 및 Claude-3)에서 개선되었습니다.

([23]에서 발췌)
**대규모 언어 모델은 잠재 변수 모델(Latent Variable Models)입니다** [24]. LLM이 맥락 내 학습 능력을 가지고 있다는 것을 알고 있지만, 이러한 능력이 표준 언어 모델 사전 훈련에서 어떻게 나타나는지는 불분명합니다. 또한 맥락 내 학습은 일반적으로 퓨샷 학습에 사용되는 시범 사례의 선택과 형식에 민감합니다. 특정 시연은 모델에 효과적인 시범 사례인 반면, 다른 시연은 그렇지 않습니다. 현재 퓨샷 학습을 위한 최상의 시범 사례를 선택하는 표준 기준은 없습니다. [24]에서 저자들은 이 주제를 연구하며, 가능한 최상의 퓨샷 시범 사례를 식별하기 위한 실용적인 전략을 찾는 것을 목표로 합니다.

"맥락 내 학습은 광범위한 자연어 처리(NLP) 작업에 효과적인 기술로 입증되었습니다. 그러나 이는 사용되는 시연의 선택, 형식, 심지어 순서에도 민감합니다." - [24]에서 발췌

많은 논문이 이론적 관점에서 맥락 내 학습의 메커니즘을 연구했지만, 실용적이거나 실행 가능한 통찰력을 제공하는 논문은 거의 없습니다. [24]에서 저자들은 LLM을 언어 모델이 관찰한 이전 토큰에 새로운 토큰 생성을 연결하는 간단한 주제/잠재 변수 모델의 관점에서 봅니다. 자세한 내용은 논문에서 찾을 수 있지만, 높은 수준에서 이 공식화(formulation)는 모델의 입력 질의문 내에서 사용된 형식 및 작업 정보와 관련하여 언어 모델의 결과물을 이론적으로 설명할 수 있게 합니다.

([24]에서 발췌)
이 공식화로부터 저자들은 모델 입력의 사후 확률(posterior probability)을 측정하기 위해 더 작은 언어 모델을 사용하는 가능한 최상의 퓨샷 시범 사례를 선택하기 위한 실용적인 기술을 개발합니다. 이는 모델의 입력과 매개변수를 기반으로 다른 입력 시범 사례의 가능성을 알려줍니다. 더 작은 LLM으로 선택된 시범 사례를 더 큰 모델과의 맥락 내 학습에 사용할 수 있으며(위 참조), 이는 실질적인 이점을 제공하는 것으로 밝혀졌습니다. 간단히 말해, 이 논문은 더 나은 퓨샷 시범 사례를 선택하기 위해 실제로 사용될 수 있는 맥락 내 학습에 대한 흥미롭고(상대적으로 간단한) 이론적 관점을 제안합니다.

**글쓰기 능력 향상(Improving Writing Capabilities)**

"SoT는 추론 효율성을 위한 데이터 중심 최적화의 초기 시도이며, 언어로 응답 구조를 명시적으로 계획함으로써 고품질 응답을 이끌어낼 잠재력을 보여줍니다." - [25]에서 발췌

**사고의 골격(Skeleton-of-Thought, SoT)** [25]은 LLM으로 결과물을 생성하는 지연 시간(latency)을 줄이는 것을 목표로 하는 질의 기법입니다. 아시다시피, LLM으로 결과물을 생성하는 것은 몇 가지 이유로 비용이 많이 들 수 있습니다.
*   모델이 크기 때문에 계산/메모리/I/O 비용이 높습니다.
*   어텐션(attention) 연산은 I/O 바운드(IO bound)이며 순서 길이(sequence length)에 따라 제곱으로 증가하는 메모리/계산 복잡성(compute complexity)을 가집니다.
*   결과물은 한 번에 하나의 토큰씩 순차적으로 생성됩니다(즉, 다음 토큰 예측(next token prediction) 사용).

[25]에서 저자들은 위에서 언급된 마지막 문제— 순차적 디코딩(sequential decoding)의 지연 시간 —를 해결하려고 시도합니다. 간단히 말해, 순차적 디코딩은 한 번에 하나의 토큰을 생성하므로 결과물 순서에서 토큰 생성을 병렬화(parallelize)할 수 없기 때문에 문제입니다. 이러한 이유로 결과물 생성 비용은 결과물의 길이에 직접적으로 관련됩니다. 많은 토큰을 가진 결과물 순서를 생성하는 데 훨씬 더 오랜 시간이 걸립니다. 하지만, 완전한 순차적 디코딩을 피할 수 있을까요?

([25]에서 발췌)
[25]에서 우리는 모델, 시스템 또는 하드웨어에 어떤 변경도 요구하지 않고 인간의 사고 및 글쓰기 과정을 모방함으로써 더 효율적인 디코딩 전략(decoding strategy)을 고안할 수 있음을 봅니다. 특히, 인간은 쓰고 싶은 내용에 대한 개요(outline)를 계획한 다음, 개요의 각 요소에 대한 세부 사항을 채우는 경향이 있습니다. 이것은 순전히 순차적인 과정이 아닙니다! 6 이 아이디어에서 영감을 받아, [25]의 저자들은 사고의 골격(Skeleton-of-Thought, SoT) 질의(위 참조)를 제안하며, 이는 두 단계로 이루어집니다.
*   LLM에 응답의 골격/개요를 생성하도록 질의문을 제공합니다.
*   각 개요 요소의 내용을 채우기 위해 병렬 API 호출을 수행합니다.

이것이 다소 모호하게 들릴 수 있지만, 아래에 표시된 SoT 질의문을 확인하면 이것이 어떻게 작동하는지 알 수 있습니다. 과정은 매우 간단합니다. 우리는 골격을 생성하고 일반적인 질의문 템플릿(prompt template)을 사용하여 나머지 모든 세부 사항을 채웁니다.

([25]에서 발췌)
골격의 각 요소를 병렬로 생성함으로써 추론 지연 시간(inference latency)을 크게 절약할 수 있습니다. 예를 들어, 이 요약의 시작 부분에 표시된 질문은 기본 모델이나 시스템에 어떤 변경도 가하지 않고 12초(22초 대신) 만에 응답될 수 있습니다. 우리는 단지 SoT 질의를 사용합니다. [25]에서는 12개의 다른 LLM에서 유사한 속도 향상이 관찰됩니다. 흥미롭게도, 저자들은 개요를 작성하는 것이 종종 글쓰기 품질을 향상시킬 수 있다고 언급합니다. 7

**방향성 자극 질의(Directional Stimulus Prompting)** ([27]에서 발췌) [27]. 미세 조정의 계산 비용을 고려할 때, 질의는 일반적으로 LLM으로 작업을 해결하는 가장 쉬운 방법입니다. 그러나 질의에는 한계가 있습니다. LLM이 우리가 원하는 내용이나 스타일로 결과물을 생성하도록 유도하는 것은 어려울 수 있습니다. 이 문제를 해결하기 위해 [27]의 저자들은 방향성 자극 질의(directional stimulus prompting, DSP)를 제안합니다. 이는 위 그림과 같이 LLM의 질의문에 "방향성 자극(directional stimulus)"을 도입합니다. 이 자극은 예상 결과물에 대한 더 많은 정보를 LLM에 제공하는 텍스트 힌트(textual hint) 또는 단서(clue)일 뿐입니다. 방향성 자극은 인스턴스 특정적(instance-specific)이며 입력 질의에만 기반하며, LLM에 비해 훈련하거나 미세 조정하기 훨씬 쉬운 더 작은 모델(예: T5)을 사용하여 생성됩니다. 이렇게 함으로써 우리는 LLM을 직접 훈련하는 어려움을 우회하고, 대신 방향성 자극을 생성하는 데 사용되는 모델을 미세 조정하는 것을 선택합니다. 8 DSP는 요약(summarization), 대화(dialogue) 및 추론 작업에서 평가되었으며, 최소한의 레이블이 지정된 데이터(labeled data)만으로 모델 성능을 향상시키는 것으로 나타났습니다.

**밀도 연쇄 질의(Chain of Density Prompting)** ([28]에서 발췌) [28]. LLM의 최근 발전은 자동 요약(automatic summarization) 문제를 혁신했습니다. 레이블이 지정된 데이터에 대해 미세 조정을 수행하는 대신 LLM에 고품질 요약을 생성하도록 간단히 질의문을 제공할 수 있기 때문입니다. 요약을 자동으로 생성할 때, 결과 요약의 품질에서 중요한 측면 중 하나는 정보 밀도(information density)입니다. 우리는 요약이 모든 관련 정보를 간결하게 제시하기를 원하지만, 지나치게 밀도가 높거나 읽기 어려운 요약을 작성하는 것은 피하고 싶습니다. 정보 밀도의 이러한 상충 관계(tradeoff)를 연구하기 위해 [28]의 저자들은 밀도 연쇄(chain of density, CoD) 질의를 제안합니다. 이는 GPT-4에 바닐라 질의문(vanilla prompt)을 통해 요약을 생성하는 것으로 시작합니다. 여기에서 CoD 질의는 요약의 길이를 고정된 상태로 유지하면서 요약에 추가 엔티티(entities)를 반복적으로 추가하여 요약의 정보 밀도를 높이는 데 사용됩니다. 흥미롭게도 [28]에서 우리는 인간이 사람이 작성한 요약만큼 밀도가 높지만, GPT-4에 바닐라 질의문을 통해 생성된 요약보다 더 밀도가 높은 요약을 선호한다는 것을 알 수 있습니다. CoD 질의를 사용함으로써 우리는 이러한 상충 관계를 탐색하고 더 높은 품질의 요약을 생성할 수 있습니다.

"CoD로 생성된 요약은 바닐라 질의문으로 GPT-4가 생성한 요약보다 더 추상적(abstractive)이고, 더 많은 융합(fusion)을 보이며, 선행 편향(lead bias)이 적습니다." - [28]에서 발췌

**기타 주목할 만한 논문(Other Notable Papers)**

**능동 질의(Active Prompting)** [26]은 불확실성 기반 능동 학습(uncertainty-based active learning) 연구를 기반으로 특정 추론 문제를 해결하기 위해 가장 유용한 시범 사례를 선택(및 주석 달기)하는 기술을 제공함으로써 CoT 질의를 위한 시범 사례 선택(및 주석 달기)의 어려움을 해결합니다. **TaskMatrix** [33]는 기반 모델(foundation models)과 수백만 개의 다른 API의 통합을 고려하는 주목할 만한 문제에 대한 입장 또는 전망을 제시하는 입장 논문(position paper)입니다. **마크 집합 질의(Set of Marks Prompting)** [42]은 사전 훈련된 분할 모델(pretrained segmentation models)을 사용하여 이미지를 영역으로 분할하고, 이 영역에 마크 집합(즉, 영숫자, 마스크, 상자 등)을 오버레이(overlay)하여 GPT-4V와 같은 모델의 시각적 접지(visual grounding)를 개선하는 시각적 질의(visual prompting) 방법입니다. **다중 모달 CoT 질의(Multimodal CoT Prompting)** [43]은 근거 및 응답 생성을 문제 해결 과정의 두 가지 별개의 단계로 처리함으로써 이미지와 텍스트를 모두 포함하는 입력으로 CoT 질의를 확장합니다. 자동 질의(automatic prompting)(즉, 최적화 프로세스를 통해 더 나은 질의문 생성) 주제는 이 게시물에서 다루지 않습니다. 다른 것이 있나요? 댓글로 알려주세요!

**결론(Conclusion)**

이 개요에서 우리는 질의어 공학의 기본부터 지난 두 달 동안 제안된 최첨단 기술에 이르기까지 모든 것을 배웠습니다! 이 게시물은 엄청난 양의 정보를 포함하고 있지만, 우리가 본 많은 기술들은 동일한 핵심 질의문 구성 요소인 지시, 시범 사례, 맥락, 문제 해결 근거를 활용하는 약간의 변형입니다. 또한, 우리는 처음에 제안된 질의어 공학 전략을 상기해야 합니다.
*   질의문의 품질을 쉽고 정량적으로 측정할 수 있는 포괄적인 평가 전략을 수립하는 것으로 시작하세요.
*   작성하는 첫 번째 질의문은 간단해야 합니다(예: 지시 질의).
*   질의문을 더 복잡하게 만들 때는 추가된 복잡성이 그에 상응하는 성능 향상으로 이어지는지 확인하세요.
*   원하는 성능에 도달할 때까지 질의문을 계속 반복하세요.

많은 문제는 간단한 지시 및 퓨샷 질의를 통해 해결될 수 있습니다. 복잡한 추론 문제의 경우, 자기 일관성을 포함하는 CoT 질의와 같은 더 고급 전략을 사용하는 것이 필요할 수 있습니다. 또한, 특정 문제 도메인(예: 수학 문제에 대한 PoT 질의 또는 요약에 대한 CoD 질의)에 유용한 다양한 질의 전략을 보았습니다. 이러한 기술을 아는 것이 유용하지만, 그 사용 사례는 비교적 드물며, 명확하고 측정 가능한 성능 영향을 볼 때만 사용해야 합니다.

뉴스레터가 처음이신가요? 안녕하세요! 저는 Cameron R. Wolfe이며, 이 Deep (Learning) Focus 뉴스레터는 독자들이 AI 연구를 이해하도록 돕습니다. 뉴스레터가 마음에 드신다면 구독, 공유 또는 X와 LinkedIn에서 저를 팔로우해 주세요!

구독

**참고문헌(Bibliography)**
[1] Saravia, Elvis, et al. “Prompt Engineering Guide”, https://github.com/dair-ai/Prompt-Engineering-Guide (2022).
[2] Radford, Alec, et al. "Language Models are Unsupervised Multitask Learners."
[3] Brown, Tom, et al. "Language models are few-shot learners." Advances in neural information processing systems 33 (2020): 1877-1901.
[4] Work, What Makes In-Context Learning. "Rethinking the Role of Demonstrations: What Makes In-Context Learning Work?."
[5] Zhao, Zihao, et al. "Calibrate before use: Improving few-shot performance of language models." International conference on machine learning . PMLR, 2021.
[6] Ouyang, Long, et al. "Training language models to follow instructions with human feedback." Advances in neural information processing systems 35 (2022): 27730-27744.
[7] Ye, Seonghyeon, et al. "Investigating the effectiveness of task-agnostic prefix prompt for instruction following." Proceedings of the AAAI Conference on Artificial Intelligence . Vol. 38. No. 17. 2024.
[8] Thoppilan, Romal, et al. "Lamda: Language models for dialog applications." arXiv preprint arXiv:2201.08239 (2022).
[9] Rae, Jack W., et al. "Scaling language models: Methods, analysis & insights from training gopher." arXiv preprint arXiv:2112.11446 (2021).
[10] Wei, Jason, et al. "Chain-of-thought prompting elicits reasoning in large language models." Advances in neural information processing systems 35 (2022): 24824-24837.
[11] Kojima, Takeshi, et al. "Large language models are zero-shot reasoners." arXiv preprint arXiv:2205.11916 (2022).
[12] Wang, Xuezhi, et al. "Self-consistency improves chain of thought reasoning in language models." arXiv preprint arXiv:2203.11171 (2022).
[13] Zhou, Denny, et al. "Least-to-most prompting enables complex reasoning in large language models." arXiv preprint arXiv:2205.10625 (2022).
[14] Yao, Shunyu, et al. "Tree of thoughts: Deliberate problem solving with large language models." arXiv preprint arXiv:2305.10601 (2023).
[15] Zhang, Zhuosheng, et al. "Automatic chain of thought prompting in large language models." arXiv preprint arXiv:2210.03493 (2022).
[16] Fu, Yao, et al. "Complexity-based prompting for multi-step reasoning." The Eleventh International Conference on Learning Representations . 2022.
[17] Zheng, Chuanyang, et al. "Progressive-hint prompting improves reasoning in large language models." arXiv preprint arXiv:2304.09797 (2023).
[18] Khot, Tushar, et al. "Decomposed prompting: A modular approach for solving complex tasks." arXiv preprint arXiv:2210.02406 (2022).
[19] Lu, Pan, et al. "Chameleon: Plug-and-play compositional reasoning with large language models." Advances in Neural Information Processing Systems 36 (2024).
[20] Yang, Rui, et al. "Gpt4tools: Teaching large language model to use tools via self-instruction." Advances in Neural Information Processing Systems 36 (2024).
[21] Wang, Yizhong, et al. "Self-instruct: Aligning language models with self-generated instructions." arXiv preprint arXiv:2212.10560 (2022).
[22] Shi, Freda, et al. "Large language models can be easily distracted by irrelevant context." International Conference on Machine Learning . PMLR, 2023.
[23] Liu, Nelson F., et al. "Lost in the middle: How language models use long contexts." Transactions of the Association for Computational Linguistics 12 (2024): 157-173.
[24] Wang, Xinyi, et al. "Large language models are latent variable models: Explaining and finding good demonstrations for in-context learning." Advances in Neural Information Processing Systems 36 (2024).
[25] Ning, Xuefei, et al. "Skeleton-of-thought: Large language models can do parallel decoding." arXiv preprint arXiv:2307.15337 (2023).
[26] Diao, Shizhe, et al. "Active prompting with chain-of-thought for large language models." arXiv preprint arXiv:2302.12246 (2023).
[27] Li, Zekun, et al. "Guiding large language models via directional stimulus prompting." Advances in Neural Information Processing Systems 36 (2024).
[28] Adams, Griffin, et al. "From sparse to dense: GPT-4 summarization with chain of density prompting." arXiv preprint arXiv:2309.04269 (2023).
[29] Zhu, Zhaocheng, et al. "Large language models can learn rules." arXiv preprint arXiv:2310.07064 (2023).
[30] Patil, Shishir G., et al. "Gorilla: Large language model connected with massive apis." arXiv preprint arXiv:2305.15334 (2023).
[31] Shen, Yongliang, et al. "Hugginggpt: Solving ai tasks with chatgpt and its friends in huggingface." arXiv preprint arXiv:2303.17580 (2023).
[32] Schick, Timo, et al. "Toolformer: Language models can teach themselves to use tools." arXiv preprint arXiv:2302.04761 (2023).
[33] Liang, Yaobo, et al. "Taskmatrix. ai: Completing tasks by connecting foundation models with millions of apis." arXiv preprint arXiv:2303.16434 (2023).
[34] Chen, Shouyuan, et al. "Extending context window of large language models via positional interpolation." arXiv preprint arXiv:2306.15595 (2023).
[35] Besta, Maciej, et al. "Graph of Thoughts: Solving Elaborate Problems with Large Language Models." arXiv preprint arXiv:2308.09687 (2023).
[36] Yao, Yao, Zuchao Li, and Hai Zhao. "Beyond Chain-of-Thought, Effective Graph-of-Thought Reasoning in Large Language Models." arXiv preprint arXiv:2305.16582 (2023).
[37] Lewis, Patrick, et al. "Retrieval-augmented generation for knowledge-intensive nlp tasks." Advances in Neural Information Processing Systems 33 (2020): 9459-9474.
[38] Ovadia, Oded, et al. "Fine-tuning or retrieval? comparing knowledge injection in llms." arXiv preprint arXiv:2312.05934 (2023).
[39] Liu, Jiacheng, et al. "Generated knowledge prompting for commonsense reasoning." arXiv preprint arXiv:2110.08387 (2021).
[40] Gao, Luyu, et al. "PAL: Program-aided Language Models." arXiv preprint arXiv:2211.10435 (2022).
[41] Chen, Wenhu, et al. "Program of thoughts prompting: Disentangling computation from reasoning for numerical reasoning tasks." arXiv preprint arXiv:2211.12588 (2022).
[42] Yang, Jianwei, et al. "Set-of-mark prompting unleashes extraordinary visual grounding in gpt-4v." arXiv preprint arXiv:2310.11441 (2023).
[43] Zhang, Zhuosheng, et al. "Multimodal chain-of-thought reasoning in language models." arXiv preprint arXiv:2302.00923 (2023).