최근 인공지능 분야에서 대규모 언어 모델(LLM)은 폭발적인 인기를 얻었습니다. 사용 편의성 덕분에 텍스트 프롬프트(textual prompt)를 작성하는 것만으로도 딥러닝(deep learning)에 전혀 익숙하지 않은 사람들도 거대한 신경망(neural network)을 활용하여 다양한 복잡한 문제를 신속하게 해결할 수 있습니다. 이는 단순한 텍스트 처리 능력을 넘어, 인간의 창의적 작업과 복잡한 문제 해결에 깊이 관여하며 기술의 지평을 넓히고 있습니다. 다양한 분야의 전문가들은 거대한 신경망을 활용하여 사회적 가치를 창출하고 있습니다. 시간이 지남에 따라 이러한 모델은 개선된 지시 따르기(instruction following) 능력과 정렬(alignment)을 통해 사용자 경험 개선과 함께 더욱 강력한 기능을 제공하며, 새로운 상호작용 방식의 가능성을 열어가고 있습니다. 그러나 LLM에 효과적으로 프롬프트를 제공하는 것은 예술이자 과학이며, 프롬프팅(prompting) 구현 또는 전략을 약간만 조정해도 상당한 성능 향상을 이룰 수 있습니다. 또한, LLM의 잠재력을 완전히 실현하기 위해서는 기술적 진보뿐만 아니라 윤리적, 사회적 책임에 대한 깊은 고민이 필요합니다. 이 개요에서는 프롬프트 엔지니어링(prompt engineering)의 기본 개념부터 최근 몇 달 동안 제안된 최첨단 기술과 LLM의 최신 발전 동향, 그리고 실제 애플리케이션에 적용될 때 고려해야 할 주요 과제와 기회에 이르기까지 포괄적인 이해를 발전시킬 것입니다.

**프롬프트 엔지니어링이란 무엇인가요?**
LLM이 이렇게 인기 있는 주요 이유 중 하나는 텍스트-투-텍스트(text-to-text) 인터페이스 덕분에 사용하기가 매우 간단하고, 그 접근성과 다재다능함에 있습니다. 이전 세대에서는 딥러닝(deep learning)으로 작업을 해결하려면 (최소한) 모델이 해당 작업을 해결하는 방법을 학습하도록 일부 데이터에 대해 모델을 미세 조정(finetune)해야 했습니다. 게다가 대부분의 모델은 특정 작업 해결에 특화된 좁은 전문가(narrow experts)였습니다. 하지만, 오늘날의 LLM은 새로운 인컨텍스트 학습(in-context learning) 능력을 바탕으로 텍스트 프롬프트를 통해 광범위하고 다양한 문제를 해결하며 새로운 패러다임을 제시합니다. 이전에 복잡했던 문제 해결 프로세스가 자연어(natural language)로 추상화되었습니다! 이러한 변화는 인공지능 기술의 대중화를 가속화하고 있으며, 단순히 텍스트를 생성하는 것을 넘어 다양한 창의적 및 분석적 작업에 적용될 수 있음을 보여줍니다.

"프롬프트 엔지니어링은 다양한 응용 분야 및 연구 주제에 LM을 효율적으로 사용하기 위한 프롬프트를 개발하고 최적화하는 비교적 새로운 분야입니다." - [1]에서 발췌

LLM의 단순성은 그 사용을 대중화했습니다. 이는 데이터 과학자(data scientist)나 MLE(Machine Learning Engineer)와 같은 기술 전문가뿐만 아니라 일반 사용자도 복잡한 AI 기능을 활용할 수 있도록 만들었습니다. 영어를 이해한다면 (또는 선택한 언어를 이해한다면) LLM으로 비교적 복잡한 문제를 해결할 수 있습니다! 그러나 LLM으로 문제를 해결할 때 얻는 결과는 모델에 제공되는 텍스트 프롬프트에 크게 의존하며, 이는 곧 사용자 의도의 명확성과 모델의 반응성 사이의 중요한 연결고리를 형성합니다. 이러한 이유로, LLM의 성능을 최적화하기 위해 다양한 프롬프트를 테스트하는 경험적 과학(empirical science)인 프롬프트 엔지니어링은 매우 인기 있고 영향력 있는 분야가 되었으며, 많은 기술과 모범 사례(best practices)의 발견으로 이어졌습니다. 이처럼 프롬프트의 중요성이 부각되면서, 단순히 성능을 최적화하는 것을 넘어 모델의 공정성, 투명성, 그리고 안전성을 확보하기 위한 새로운 접근 방식들이 요구되고 있습니다. 프롬프트 엔지니어링은 이제 모델의 기능을 탐색하고 제어하는 핵심 수단이 되었으며, 이는 책임감 있는 AI 개발의 중요한 축을 담당합니다.

**프롬프트 및 LLM 기반 시스템의 핵심 구성 요소**
LLM에 프롬프트를 제공하는 방법은 여러 가지가 있습니다. 대부분의 프롬프팅 전략은 몇 가지 공통 구성 요소를 공유합니다.
*   **입력 데이터(Input Data)**: LLM이 처리할 것으로 예상되는 실제 데이터 (예: 번역되거나 분류되는 문장, 요약되는 문서 등).
*   **예시(Exemplars)**: 프롬프트 내에 포함된 올바른 입력-출력 쌍의 구체적인 예시.
*   **지시(Instruction)**: 모델에서 예상되는 출력에 대한 텍스트 설명.
*   **지표(Indicators)**: 프롬프트 내에 구조를 생성하는 데 사용되는 태그(tag) 또는 서식 요소(formatting elements).
*   **컨텍스트(Context)**: 프롬프트에서 LLM에 제공되는 모든 추가 정보.
아래 그림에서 문장 분류를 위한 단일 프롬프트 내에 위에서 언급된 모든 프롬프트 구성 요소를 결합한 예시를 볼 수 있습니다.

LLaMA-2 스타일의 모든 구성 요소를 포함한 프롬프트 (여기 참조)

그러나 실제 시스템에 LLM을 통합할 때는 단순히 텍스트 프롬프트만을 고려하는 것이 아니라, 전체적인 시스템 아키텍처와 상호작용 방식을 설계해야 합니다. 이는 다음과 같은 핵심 요소를 포함합니다.
*   **데이터 파이프라인(Data Pipeline)**: LLM이 처리할 데이터를 수집, 전처리(preprocessing), 그리고 통합하는 과정.
*   **모델 선택(Model Selection)**: 특정 애플리케이션의 요구사항에 맞춰 최적의 LLM 모델(예: 경량 모델, 멀티모달 모델 등)을 선택하는 기준.
*   **사용자 인터페이스(User Interface)**: 사용자가 LLM과 자연스럽게 상호작용할 수 있도록 설계된 인터페이스 및 피드백 메커니즘.
*   **성능 모니터링(Performance Monitoring)**: 배포된 LLM의 응답 품질, 지연 시간, 자원 사용량 등을 지속적으로 추적하고 개선하는 체계.
*   **보안 및 개인정보 보호(Security & Privacy)**: 민감한 데이터 처리 시 발생할 수 있는 보안 취약점과 개인정보 유출 위험을 관리하는 방안.
이러한 구성 요소들은 LLM 기반 애플리케이션의 성공적인 배포를 위해 필수적이며, 각 요소는 상호 유기적으로 연결되어 전체 시스템의 견고성을 결정합니다.

**컨텍스트 처리와 모델 아키텍처의 진화**
사전 학습(pretraining) 동안 LLM은 특정 길이의 입력 시퀀스(input sequence)를 봅니다. 사전 학습 중 이 시퀀스 길이 선택은 모델의 "컨텍스트 길이(context length)" 또는 모델이 처리할 수 있는 최대 시퀀스 길이가 됩니다. 미리 정해진 컨텍스트 길이보다 훨씬 긴 텍스트 시퀀스가 주어지면 모델은 예측할 수 없게 동작하고 잘못된 출력을 생성할 수 있습니다. 그러나 최근 아키텍처 혁신은 이러한 고정된 길이의 제약을 넘어서는 데 주력하고 있습니다. 예를 들어, Self-Extend 또는 위치 보간(positional interpolation)과 같은 방법을 사용하여 모델의 컨텍스트 윈도우를 확장할 수 있습니다. 이는 모델이 더 많은 정보를 한 번에 처리할 수 있게 하여, 복잡한 문서 분석이나 장문의 대화 요약과 같은 작업에서 성능을 크게 향상시킵니다.

RoPE를 사용한 위치 보간(positional interpolation) 그림 ([34]에서 발췌)

LLM에 대한 최근 연구는 긴 컨텍스트 윈도우(long context windows) 생성에 중점을 두었으며, 이를 통해 모델은 각 프롬프트 내에서 더 많은 정보(예: 더 많은 예시 또는 더 많은 컨텍스트)를 처리할 수 있습니다. LLM이 긴 컨텍스트 윈도우 내의 정보를 활용하는 능력은 일반적으로 검색 증강 생성(RAG) 시스템의 효율성을 결정하는 핵심 요소입니다. 이러한 시스템은 외부 지식 베이스(knowledge base)에서 관련 정보를 검색하여 LLM의 응답을 풍부하게 만들지만, 정보의 양이 많아질수록 모델이 핵심 내용을 정확히 파악하는 것이 중요해집니다. 그러나 보시다시피 모든 LLM이 컨텍스트에 완벽하게 주의를 기울이는 것은 아닙니다! LLM이 긴 컨텍스트 윈도우 내의 정보를 활용하는 능력은 일반적으로 "건초 더미 속 바늘 찾기 테스트(needle in the haystack test)"를 통해 평가됩니다. 이 테스트는 i) 컨텍스트 내에 무작위 사실을 삽입하고, ii) 모델에 사실을 검색하도록 요청하며, iii) 다양한 컨텍스트 길이와 컨텍스트 내 사실의 위치에 대해 이 테스트를 반복합니다. 이러한 테스트는 아래 그림과 같은 결과를 보여주며, 컨텍스트 윈도우의 결함을 쉽게 발견할 수 있습니다. 따라서 최신 LLM은 단순히 컨텍스트 길이를 늘리는 것을 넘어, 컨텍스트 내에서 가장 관련성 높은 정보를 식별하고 활용하는 능력을 지속적으로 개선하고 있습니다.

(출처)

**AI 시스템 개발 원칙 및 프롬프트 엔지니어링 전략**
AI 시스템 설계의 세부 사항은 사용되는 모델에 따라 크게 다릅니다. 그러나 LLM 기반 애플리케이션을 개발하고 배포하는 과정과 프롬프트 엔지니어링 프로세스를 안내하는 데 종종 유용한 몇 가지 일반적인 원칙이 있습니다.

프롬프트 엔지니어링 전략:
*   **경험적이어야 합니다(Be empirical)**: 프롬프트 엔지니어링의 첫 번째 단계는 프롬프트 변경 사항을 쉽게 평가할 수 있도록 프롬프트를 평가하는 신뢰할 수 있는 방법(예: 테스트 케이스(test cases), 인간 평가자(human evaluators) 또는 LLM-as-a-judge를 통해)을 설정하는 것입니다.
*   **간단하게 시작하세요(Start simple)**: 시도하는 첫 번째 프롬프트는 연쇄 사고(chain-of-thought) 프롬프트(또는 다른 특수 프롬프팅 기술)가 아니어야 합니다. 가능한 가장 간단한 프롬프트로 시작하고, 성능 변화를 측정하면서 천천히 복잡성을 추가하여 추가 복잡성이 필요한지 여부를 판단하세요. 1
*   **구체적이고 직접적이어야 합니다(Be specific and direct)**: 프롬프트의 모호성을 제거하고 LLM의 원하는 출력을 설명할 때 간결하고 직접적이며 구체적으로 표현하려고 노력하세요.
*   **예시를 사용하세요(Use exemplars)**: 원하는 출력을 설명하기 어렵다면 프롬프트에 몇 가지 예시를 추가해 보세요. 예시는 LLM에 기대되는 바에 대한 구체적인 예시를 제공하여 모호성을 제거합니다.
*   **복잡성을 피하세요 (가능하다면)(Avoid complexity (if possible))**: 복잡한 프롬프팅 전략이 때로는 필요하지만(예: 다단계 추론 문제 해결), 그러한 접근 방식을 사용하기 전에 신중하게 생각해야 합니다. 경험적이어야 하며 확립된 평가 전략을 사용하여 복잡성이 진정으로 필요한지 판단하세요.

AI 시스템 개발 원칙:
*   **데이터 중심적 사고(Data-centric thinking)**: 모델의 성능은 결국 데이터의 품질에 달려 있습니다. 데이터 수집, 정제, 증강에 대한 투자는 장기적인 성공을 위한 필수 요소입니다.
*   **점진적 개선(Iterative Refinement)**: 초기 단계에서는 최소 실행 가능한 제품(MVP)에 집중하고, 사용자 피드백과 실제 사용 데이터를 바탕으로 기능을 점진적으로 확장해 나가는 것이 중요합니다.
*   **윤리적 고려(Ethical Consideration)**: 모델의 편향(bias), 공정성, 투명성을 초기부터 고려하고, 잠재적인 사회적 영향을 예측하며 완화 전략을 수립해야 합니다.
*   **확장성 설계(Scalability Design)**: 사용자 증가와 데이터 볼륨 확장에 대비하여 시스템 아키텍처를 유연하게 설계하고, 효율적인 자원 관리를 위한 전략을 마련해야 합니다.
*   **지속적인 평가(Continuous Evaluation)**: 배포 후에도 모델의 성능 지표를 꾸준히 모니터링하고, 새로운 데이터와 환경 변화에 맞춰 모델을 업데이트하는 체계를 구축해야 합니다.

위 내용을 요약하자면, 저의 개인적인 프롬프트 엔지니어링 전략은 i) 정말 좋은 평가 프레임워크(evaluation framework)에 투자하고, ii) 간단한 프롬프트로 시작하며, iii) 원하는 수준의 성능을 달성하기 위해 필요에 따라 천천히 복잡성을 추가하는 것입니다. 프롬프트 작성은 반복적인 과정입니다! 또한, AI 시스템 설계는 i) 견고한 데이터 기반을 구축하고, ii) 사용자 중심의 점진적 개발을 추구하며, iii) 윤리적 책임을 다하는 반복적인 과정입니다!

**프롬프팅 기술 및 최신 AI 연구 동향**
우리는 이전에 일련의 관련 개요를 통해 다양한 프롬프팅 기술에 대해 배웠습니다.
*   실용적인 프롬프트 엔지니어링(Practical Prompt Engineering) [링크]
*   고급 프롬프트 엔지니어링(Advanced Prompt Engineering) [링크]
*   연쇄 사고 프롬프팅(Chain of Thought Prompting) [링크]
*   프롬프트 앙상블(Prompt Ensembles) [링크]
이제 관련 프롬프팅 기술을 다시 한번 개괄하여, 이 게시물에서 나중에 소개될 더 복잡한 접근 방식의 토대를 제공할 것입니다.

하지만 AI 개발의 전반적인 맥락에서 볼 때, 기술 선택의 복잡성은 단순히 프롬프트 수준에만 머무르지 않습니다. 최신 LLM의 발전은 모델 아키텍처, 학습 데이터, 배포 전략 등 다양한 측면에서 혁신적인 변화를 가져왔습니다.
*   **새로운 모델 아키텍처(Novel Model Architectures)**: 트랜스포머(Transformer)를 넘어선 효율적인 아키텍처(예: Mamba, RWKV)와 멀티모달(multimodal) 능력 강화에 대한 연구.
*   **데이터 효율적인 학습(Data-efficient Learning)**: 소량의 데이터로도 강력한 성능을 내는 방법(예: LoRA, QLoRA) 및 합성 데이터 생성(synthetic data generation) 기술.
*   **온디바이스 AI(On-device AI)**: 모바일 및 엣지 장치에서 LLM을 구동하기 위한 경량화(quantization, pruning) 및 최적화 기술.
*   **신뢰할 수 있는 AI(Trustworthy AI)**: 모델의 편향 탐지 및 완화, 설명 가능성(explainability), 그리고 견고성(robustness) 확보를 위한 방법론.
이러한 각 분야는 AI 시스템의 전반적인 성능과 신뢰성을 결정하는 중요한 요소입니다. 프롬프팅 기술이 더 복잡하거나 정교하다고 해서 더 간단한 전략보다 더 낫다는 의미는 아닙니다! 궁극적으로는 애플리케이션의 특정 요구사항과 제약 조건에 가장 적합한 조합을 찾는 것이 중요합니다.

**기본 프롬프팅 전략**
제로샷 프롬프팅(Zero-shot prompting) ([3]에서 발췌, 위 그림 참조)은 GPT-2 [2]에 의해 대중화되었으며, 우리가 사용할 수 있는 가장 기본적인 프롬프팅 전략 중 하나입니다. 제로샷 프롬프팅을 통해 작업을 해결하려면, i) 프롬프트에 작업을 설명하고 ii) 모델에 문제를 해결하도록 프롬프트를 제공하기만 하면 됩니다. 위 문제의 경우, 작업은 영어를 프랑스어로 번역하는 것이며, 우리는 "cheese =>"라는 문자열을 통해 모델에 이 번역을 수행하도록 프롬프트를 제공합니다. 이는 모델이 'cheese'라는 단어의 프랑스어 번역을 출력하도록 유도합니다. 아래에 제로샷 프롬프트의 몇 가지 예시가 제공됩니다.

제로샷 학습(Zero-shot learning) (GPT-3.5-Turbo로 생성된 출력)

제로샷 학습이 어떤 경우에는 잘 작동하지만, 작업 설명의 모호성으로 인해 한계가 있습니다. 성능은 명확하고 포괄적인 설명 생성에 달려 있으며, 우리는 이 설명만으로 모델이 올바른 출력을 생성하는 능력에 의존합니다. 이러한 방식은 모델의 사전 학습 데이터에 크게 의존하며, 특정 도메인이나 복잡한 작업에서는 한계를 드러냈습니다. 종종 프롬프트에 더 구체적인 정보를 삽입함으로써 더 나은 성능을 달성할 수 있습니다. 이는 모델의 활용 범위를 넓히기 위해 더 정교한 접근 방식이 필요함을 시사합니다.

퓨샷 프롬프팅(Few-shot prompting) ([3]에서 발췌)은 프롬프트에 올바른 문제 해결 예시를 여러 개 삽입하여 정확히 이 작업을 수행합니다. 이 전략은 GPT-3 [3]에 의해 대중화되었으며, GPT-3는 LLM이 대규모로 인상적인 퓨샷 학습(few-shot learning) 능력을 개발한다는 것을 보여주었습니다. (위 참조) 직관적으로 퓨샷 학습은 예상 출력의 여러 예시를 제공함으로써 제로샷 학습의 모호성을 제거합니다. 따라서 모델은 작업 설명에서 원하는 동작을 추론하는 대신 이러한 예시로부터 올바른 동작을 직접 이해할 수 있습니다. (아래 참조) 이처럼 몇 가지 예시를 통해 모델의 동작을 유도하는 방식은 인컨텍스트 학습(in-context learning)의 강력함을 보여주었지만, 여전히 모델의 심층적인 이해와 적응 능력을 완전히 활용하지 못한다는 지적이 있습니다.
LLM은 프롬프트 내에 제공된 이러한 예시로부터 학습할 수 있으며, 이는 일반적으로 "인컨텍스트 학습(in-context learning)"이라고 불리는 전략입니다. (아래 참조) 그러나 이러한 학습 방식은 신경망(neural network)의 일반적인 훈련과는 다릅니다. 모델의 매개변수(parameters)는 전혀 수정되지 않습니다. 대신, 우리는 프롬프트에 관련 정보를 넣고, 모델은 이 정보를 더 나은 출력을 생성하기 위한 컨텍스트(context)로 사용할 수 있습니다. 이는 모델이 새로운 정보에 동적으로 반응하는 능력을 가지고 있음을 의미합니다.

실제로 퓨샷 학습을 사용할 때 적절하게 조정해야 하는 두 가지 주요 설정이 있습니다.
*   사용할 예시의 수.
*   예시 선택 전략.
사용할 올바른 예시 수를 결정하기 위해 평가 세트(evaluation set)를 사용하여 기본적인 하이퍼파라미터 튜닝(hyperparameter tuning)을 수행할 수 있습니다. 많은 논문에서 예시 선택 전략(예: 무작위 선택(random selection), 다양성(diversity), 의미론적 유사성(semantic similarity), 능동 학습(active learning) 또는 더 복잡한 지표(metrics) 기반) 2 을 탐구했습니다. 그러나 예시의 무작위 선택은 실제로 종종 효과적인 전략입니다. 이러한 전략 외에도 퓨샷 학습과 관련된 다양한 실용적인 규칙과 발견 사항이 있으며, 우리는 항상 이를 염두에 두어야 합니다 [4, 5].
*   예시의 레이블(label) 분포는— 비록 부정확하더라도 —모델이 일반적인 레이블에 편향되어 있기 때문에 모델의 답변에 영향을 미칠 수 있습니다.
*   답변은 프롬프트에서 최근에 관찰된 예시에 편향됩니다. 3
*   프롬프트 내 예시의 서식(formatting)이 중요합니다.
*   예시를 무작위로 선택하면 모델이 생성한 답변 내의 편향(예: 위치 또는 다수 레이블 편향)을 제거하는 데 도움이 될 수 있습니다.
또한, LLM을 사용할 때 적절하게 조정해야 하는 두 가지 주요 설정이 있습니다. 첫째, 모델의 아키텍처와 사전 학습 방식입니다. 최신 모델들은 효율적인 어텐션 메커니즘(attention mechanism)과 경량화(quantization) 기술을 통합하여 더 빠르고 자원 효율적인 추론을 가능하게 합니다. 둘째, 모델의 지속적인 적응 및 업데이트 전략입니다. 이는 새로운 데이터와 변화하는 요구사항에 맞춰 모델을 주기적으로 재학습시키거나, 증분 학습(incremental learning) 방식을 통해 지식을 업데이트하는 것을 포함합니다. 단순함에도 불구하고 퓨샷 학습은 가장 효과적인 프롬프팅 전략 중 하나이며 실제 응용 분야에서 널리 사용되지만, 미래에는 모델 자체가 이러한 적응 과정을 자동화하는 방향으로 발전할 것입니다.

**지시 기반 AI와 정렬의 중요성**
지시 프롬프팅(Instruction prompting) ([6]에서 발췌)은 LLM의 원하는 출력을 표현하는 더 직접적인 방법입니다. 퓨샷 학습에서는 해결되는 작업의 구체적인 예시를 통해 모델에 우리의 의도를 설명하지만, 이러한 예시는 많은 토큰(token)을 소비합니다! 모델에 우리의 의도를 말로 설명하는 것이 훨씬 더 효율적일 것입니다. 이는 사용자의 의도를 모델에게 명확하게 전달하고, 모델이 특정 역할이나 스타일로 응답하도록 유도하는 데 필수적입니다. 이러한 접근 방식은 단순히 정보 검색을 넘어, 창의적인 콘텐츠 생성, 복잡한 문제 해결, 그리고 대화형 에이전트(conversational agent) 개발에 이르기까지 폭넓게 활용됩니다. 이것이 잘 작동하려면 사용되는 LLM이 지시를 일관되게 따르도록 정렬(aligned)되어야 합니다. 이러한 모델은 제공된 상세한 지시를 이해하고 그에 따라 출력을 조정할 수 있기 때문에 "조종 가능(steerable)"하다고 합니다.

LLM에 대한 연구는 지시 따르기 능력(instruction following capabilities) 개선에 크게 집중해 왔습니다. 사전 학습된 LLM은 기본적으로 지시를 잘 따르지 못합니다. 그러나 InstructGPT [6]가 보여주듯이, 우리는 지도 미세 조정(supervised finetuning, SFT)과 인간 피드백 기반 강화 학습(reinforcement learning from human feedback, RLHF)의 조합을 통해 모델이 지시를 훨씬 더 잘 따르도록 정렬할 수 있습니다. 위 그림에서 이 전략이 지시 따르기뿐만 아니라 LLM의 다른 주요 속성(예: 사실성(factuality) 및 제약 조건 따르기(constraint following))도 개선할 수 있음을 알 수 있습니다. 특히, 모델의 안전성, 공정성, 그리고 투명성을 확보하기 위한 정렬(alignment) 기술은 AI 시스템의 사회적 수용성을 높이는 데 결정적인 역할을 합니다.

LaMDA를 사용한 역할 프롬프팅(Role prompting) ([8]에서 발췌) LLM 정렬의 최근 발전으로 인해, 퓨샷 프롬프팅 [7]과 결합될 수도 있는 지시 프롬프팅은 실제 응용 분야에서 일반적으로 사용되는 매우 효과적인 접근 방식입니다. 사실, 몇 가지 인기 있는 프롬프팅 전략(예: 역할 프롬프팅(role prompting), 대상 지정(specifying an audience), 도구 사용(tool usage) 등)은 지시 프롬프팅의 더 구체적인 버전일 뿐입니다! 지시를 작성할 때는 최상의 결과를 보장하기 위해 명확하고 정확해야 합니다. 이는 인간 사용자와 AI 시스템 간의 협업을 강화하고, AI가 인간의 의도를 더 정확하게 이해하고 반영하도록 돕는 중요한 진보를 의미합니다.

**고급 프롬프팅 전략 및 자동화된 추론 능력 향상**
위에서 설명한 프롬프팅 기술은 매우 효과적이지만, 때로는 더 복잡한 프롬프트가 어려운 문제(예: 수학/코딩 또는 다단계 추론 문제)를 해결하는 데 유용할 수 있습니다. 현재 AI 연구는 단순히 프롬프트 최적화를 넘어 모델 자체의 근본적인 추론 능력과 문제 해결 프레임워크를 혁신하는 데 집중하고 있습니다. LLM은 이러한 문제에 본질적으로 어려움을 겪기 때문에 4 (즉, 추론 능력은 모델 규모에 따라 단조롭게 향상되지 않습니다 [9]), 프롬프트 엔지니어링에 대한 기존 연구의 대부분은 추론 및 복잡한 문제 해결 능력 향상에 중점을 둡니다. 간단한 프롬프트는 대부분의 다른 문제를 해결하는 데 작동할 것입니다.

**연쇄 사고(Chain of Thought, CoT) 프롬프팅** [10] ([10]에서 발췌)은 모델의 프롬프트 내 예시에 연쇄 사고(즉, 일련의 중간 추론 단계)를 삽입하여 LLM의 추론 능력(reasoning capabilities)을 이끌어냅니다. (위 참조) 각 예시에 연쇄 사고를 추가함으로써 모델은 (인컨텍스트 학습을 통해) 해당 문제에 대한 최종 답변을 출력하기 전에 유사한 연쇄 사고를 생성하는 방법을 학습합니다. 흥미롭게도 [10]에서 충분히 큰 모델(즉, 1000억 개 이상의 매개변수)은 산술, 상식 및 기호 추론 작업에서 이 접근 방식으로부터 큰 이점을 얻는다는 것을 알 수 있습니다. 문제를 해결하기 위한 근본적인 추론 과정을 명시적으로 설명하는 것이 실제로 모델의 추론 능력을 더 효과적으로 만듭니다. 이는 모델이 단순히 최종 답변을 생성하는 것을 넘어, 문제 해결 과정을 명시적으로 보여주도록 유도함으로써 투명성과 신뢰성을 높입니다. CoT 프롬프팅의 구현은 간단합니다. 이는 실제 세계의 복잡한 문제를 해결하기 위한 AI 시스템이 점차 인간의 인지 과정과 유사한 방식으로 발전하고 있음을 보여줍니다. 각 퓨샷 예시가 입력과 출력만 가지는 대신, 예시는 (입력, 연쇄 사고, 출력) 형식의 삼중항(triplet)입니다. (위 참조) 이 접근 방식의 주요 단점은 문제 해결을 위한 완전한 근거(rationale)를 포함하는 예시를 수동으로 (또는 인위적으로) 큐레이션(curate)해야 한다는 점인데, 이는 비용이 많이 들거나 시간이 많이 소요될 수 있습니다. 따라서 많은 논문이 CoT 프롬프팅이 사람이 작성한 근거에 의존하는 것을 제거하는 데 중점을 둡니다!

**CoT 변형(CoT variants)** ([11, 12]에서 발췌) CoT 프롬프팅의 효과와 인기로 인해 이 접근 방식의 수많은 확장 기능이 제안되었습니다. 예를 들어, 제로샷 CoT(zero-shot CoT) [11] 프롬프팅은 퓨샷 예시를 제거하고 대신 프롬프트 끝에 "단계별로 생각해 봅시다(Let’s think step by step)."라는 단어를 추가하여 모델이 문제 해결 근거를 생성하도록 장려합니다. 이러한 진화는 AI가 스스로 학습하고 추론하는 능력을 향상시키며, 복잡한 의사결정 과정에서 인간을 보조하는 역할을 강화합니다. 또한 i) 문제를 해결할 때 여러 연쇄 사고를 독립적으로 생성하고 ii) 각 연쇄 사고로 생성된 최종 답변의 다수결 투표(majority vote)를 통해 추론 과정의 견고성(robustness)을 향상시킬 수 있습니다. 5 문제 해결 비용을 증가시키지만, 자기 일관성(self-consistency) [12]이라고 불리는 이 접근 방식은 더 복잡한 종류의 추론 문제를 해결할 때 LLM의 신뢰성(reliability)을 향상시킵니다.

**최소-최대 프롬프팅(Least-to-most prompting)** [13] ([13]에서 발췌)은 복잡한 문제를 여러 부분으로 명시적으로 분해함으로써 CoT 프롬프팅을 넘어섭니다. (위 참조) 이는 모듈식(modular) AI 설계의 중요성을 강조하며, 각 하위 문제를 독립적으로 해결한 후 그 결과를 통합하는 방식으로 전체 시스템의 효율성과 견고성을 높입니다. 각 하위 문제(sub-problem)는 개별적으로 해결되며, 각 하위 문제의 해결책은 다음 하위 문제를 해결하기 위한 컨텍스트로 전달됩니다. 최종 하위 문제에 도달하면 이전 해결책의 컨텍스트를 사용하여 질문에 대한 최종 답변을 출력할 수 있습니다.

"LLM의 이러한 모든 발전의 근간에는 여전히 토큰(token) 수준의 결정을 하나씩 왼쪽에서 오른쪽으로 생성하는 원래의 자기회귀(autoregressive) 메커니즘이 있다는 점은 놀라울 수 있습니다." - [14]에서 발췌

**사고의 나무(Tree of thoughts, ToT) 프롬프팅** [14]. CoT 프롬프팅과 같은 기술은 다음 토큰 예측(next-token prediction)을 사용하여 단일 시도에서 해결책을 출력하는 왼쪽-오른쪽 생성(left-to-right generation) 접근 방식을 따릅니다. 이러한 접근 방식은 특정 시나리오에서는 효과적이지만, 광범위한 계획, 전략적 미리 보기(strategic lookahead), 백트래킹(backtracking), 그리고 수많은 실행 가능한 해결책의 병렬 탐색(exploration)으로부터 이점을 얻을 수 있는 복잡한 문제를 해결하는 데 실패할 수 있습니다. 여기에 ToT 프롬프팅이 등장합니다! ToT 프롬프팅은— 최소-최대 프롬프팅 [13]과 다소 유사하게 —복잡한 문제를 개별적으로 해결할 수 있는 일련의 더 간단한 문제(또는 "사고(thoughts)")로 분해하며, 다중 경로 탐색을 통해 최적의 해결책을 찾습니다. CoT 프롬프팅과 달리 ToT 프롬프팅은 문제를 해결할 때 단일 사고 경로를 따를 것을 요구하지 않습니다. 또한 ToT 프롬프팅은 자기 일관성처럼 여러 추론 경로의 다수결 투표를 단순히 취하지 않습니다. (위 참조) 탐색(exploration) 과정에서 LLM은 많은 사고를 생성하고 자연어를 통해 최종 해결책을 향한 진행 상황을 지속적으로 평가합니다(즉, 우리는 모델에 프롬프트를 제공하기만 하면 됩니다!). 모델이 최종 해결책을 향한 자체 진행 상황을 자체 평가하는 것을 활용함으로써, 우리는 널리 사용되는 탐색 알고리즘(예: 너비 우선 탐색(breadth-first search) 또는 깊이 우선 탐색(depth-first search))으로 탐색 프로세스를 강화하여 문제 해결 과정 내에서 미리 보기와 백트래킹을 수행할 수 있습니다. 이는 AI가 다양한 관점에서 문제를 탐색하고, 필요에 따라 백트래킹하며, 인간의 문제 해결 과정과 유사한 유연성을 발휘하도록 돕습니다. ToT 프롬프팅에 대한 더 자세한 설명은 이 개요를 참조하세요.

**사고 그래프(Graph of Thoughts, GoT) 프롬프팅** [35, 36] ([35]에서 발췌). 후속 연구는 ToT 프롬프팅에 대한 연구를 추론을 위한 그래프 기반 전략으로 일반화했습니다. 전반적으로 이러한 기술은 ToT 프롬프팅과 유사하지만, 최종 해결책을 생성하는 데 사용되는 사고 경로가 선형(linear)이라는 가정을 하지 않습니다. 오히려 해결책을 도출할 때 사고를 재사용하거나 여러 사고의 시퀀스를 통해 재귀(recurse)할 수도 있습니다. (위 참조) 이는 지식 표현(knowledge representation)과 추론(reasoning)을 위한 새로운 가능성을 열었으며, 모델이 단순히 선형적인 사고 과정을 따르는 것을 넘어 복잡한 관계와 상호작용을 이해하고 활용하도록 합니다. 이러한 접근 방식은 AI가 더욱 정교하고 인간적인 방식으로 문제를 해결할 수 있는 기반을 마련합니다. 여러 그래프 기반 프롬프팅 전략이 제안되었습니다 (자세한 내용은 여기 참조) [35, 36]. 그러나 이러한 프롬프팅 기술은— ToT 프롬프팅과 마찬가지로 —실용성이 부족하다는 비판을 받아왔습니다. 즉, GoT 프롬프팅으로 추론 문제를 해결하려면 LLM으로부터 엄청난 수의 추론 단계(inference steps)가 필요할 수 있습니다!

**검색 증강 생성(RAG)의 진화**
검색 증강 생성(Retrieval Augmented Generation, RAG) [37] (위 그림 참조)은 순수한 프롬프팅 기술은 아니지만, 프롬프트에 포함할 관련 컨텍스트를 검색하여 LLM 출력의 품질을 향상시키는 널리 사용되는 전략입니다. 또한 LLM이 겪는 지식 부족과 환각(hallucination) 문제를 해결하는 데 중요한 역할을 합니다. 이 기술은 외부 지식 소스에서 최신 정보를 검색하여 LLM의 응답을 보강함으로써, 모델의 사실성(factuality)과 신뢰성을 크게 향상시킵니다. 유용한 컨텍스트를 검색하기 위해 기존 검색 기술을 사용할 수 있습니다. 예를 들어, 순수 벡터 검색(pure vector search) 또는 하이브리드 검색 엔진(hybrid search engine)입니다. 단순함에도 불구하고 연구에 따르면 RAG는 LLM에 지식을 주입하고 모델이 생성하는 환각의 수를 줄이는 데 매우 효과적입니다 [38]. 또한 RAG에 의해 검색된 관련 문서를 단순히 노출함으로써 LLM 사용자에게 쉽게 인용(citation)을 제공할 수 있습니다. 그러나 데이터를 처리하고 검색하는 방식과 프롬프트에 삽입되는 컨텍스트를 구성하는 방식은 성능에 상당한 영향을 미칠 수 있습니다. 따라서 RAG 시스템의 효과적인 구현을 위해서는 검색 품질, 관련성, 그리고 LLM의 컨텍스트 이해 능력을 종합적으로 평가하는 것이 필수적입니다. 자세한 내용은 여기를 참조하세요.

**생성된 지식 프롬프팅(Generated knowledge prompting)** [39] ([39]에서 발췌)은 외부 데이터베이스에서 컨텍스트를 검색하는 대신 LLM을 사용하여 프롬프트에 포함할 관련 컨텍스트를 생성하는 RAG의 흥미로운 대안입니다. (위 참조) 매우 간단하고 긍정적인 성능 지표를 가지고 있음에도 불구하고, 이 접근 방식은 (당연히) LLM이 정보를 환각하는 경향 때문에 신뢰성이 부족합니다. 이외에도, 멀티모달 RAG(Multimodal RAG)와 같은 고급 RAG 기술은 텍스트뿐만 아니라 이미지, 오디오 등 다양한 형태의 데이터를 검색하여 LLM의 이해 및 생성 능력을 확장하고 있습니다. 이는 LLM이 더욱 풍부하고 다차원적인 방식으로 정보를 처리하고 활용할 수 있는 길을 열어줍니다.

**최근 연구 동향**
지금까지 다양한 프롬프팅 기술을 다루었지만, 최근에는 이러한 방법을 확장하고 복잡한 문제 해결을 위한 완전히 새로운 스타일의 프롬프트를 탐구하는 많은 논문이 발표되었습니다. 최근 AI 연구는 단순히 프롬프트 최적화를 넘어 더 광범위하고 근본적인 문제 해결에 집중하고 있습니다. 이는 LLM이 실제 세계에 미치는 영향이 커짐에 따라, 기술적 진보와 함께 사회적 책임과 지속 가능성을 고려하는 방향으로 진화하고 있음을 반영합니다. 여기서는 이러한 연구를 주제 또는 초점에 따라 여러 범주로 나누어 살펴보겠습니다.
*   **추론(Reasoning)**
*   **도구 사용(Tool Usage)**
*   **프로그램 지원 언어 모델(Program-Aided Language Models)**
*   **컨텍스트 윈도우(Context Windows)**
*   **글쓰기(Writing)**
*   **책임감 있는 AI 개발(Responsible AI Development)**: 편향 완화, 공정성, 투명성, 그리고 설명 가능한 AI(XAI)에 대한 연구.
*   **멀티모달 AI(Multimodal AI)**: 텍스트, 이미지, 오디오, 비디오 등 다양한 형태의 데이터를 통합하여 이해하고 생성하는 모델.
*   **경량화 및 효율성(Efficiency & Optimization)**: LLM의 크기를 줄이고 추론 속도를 높여 온디바이스(on-device) 및 저전력 환경에서 구동 가능하게 하는 기술.
*   **범용 AI 에이전트(General AI Agents)**: 복잡한 환경에서 자율적으로 목표를 설정하고, 계획을 수립하며, 도구를 사용하여 문제를 해결하는 지능형 에이전트.
*   **지속 가능한 AI(Sustainable AI)**: AI 모델의 학습 및 배포에 필요한 에너지 소비를 줄이고, 환경적 영향을 최소화하는 방법론.
*   **기타 (다른 주목할 만한 논문)**
각 범주에 대해 다양한 다른 연구가 다루어집니다. 이러한 연구는 단일 기술 발전에 그치지 않고, 여러 분야의 지식을 융합하여 AI의 미래를 형성하고 있습니다. 그러나 프롬프트 엔지니어링 주제에 대해 발표된 엄청난 양의 연구를 고려할 때, 몇몇 논문이 누락되었을 가능성이 큽니다. 포함되어야 할 좋은 논문을 알고 있다면 댓글로 공유해 주세요!

**추론 능력 향상(Improving Reasoning Capabilities)**
([15]에서 발췌) **Auto-CoT** [15]. CoT 프롬프팅은 복잡한 문제를 해결하기 위해 중간 추론 단계(intermediate reasoning steps)를 사용하며, LLM의 출력 내에서 이러한 추론 단계를 이끌어낼 수 있는 두 가지 방법이 있습니다(위 그림 참조).
*   **제로샷(Zero-shot)**: LLM에 "단계별로 생각하라(think step-by-step)"고 프롬프트를 제공합니다.
*   **수동(Manual)**: 원하는 질문에 답하기 전에 질문, 근거, 답변에 대한 몇 가지 퓨샷 예시를 제공합니다.
LLM이 괜찮은 제로샷 추론기(zero-shot reasoners)이지만, 구체적인 예시를 제공하는 것이 CoT 프롬프팅에서 일관되게 더 나은 성능을 제공합니다. 그러나 이 전략은 또한 인간 주석자(human annotators)— 또는 프롬프트 엔지니어 —가 각 질문에 답하는 데 사용되는 근거에 대한 수동 시연(manual demonstrations)을 작성하도록 요구합니다. 이러한 수동 시연을 작성하는 것은 시간이 많이 걸리지만, 피할 수 있습니다!

"우리는 '단계별로 생각해 봅시다(Let’s think step by step)' 프롬프트를 가진 LLM을 활용하여 시연을 위한 추론 연쇄(reasoning chains)를 하나씩 생성함으로써 그러한 수동 노력을 제거할 수 있음을 보여줍니다." - [15]에서 발췌

[15]에서 저자들은 제로샷 CoT 프롬프팅을 사용하여 수동 CoT 프롬프팅을 위한 예시를 자동으로 생성하는 자동 CoT(Auto-CoT) 프롬프팅 접근 방식을 제안하여, 문제 해결 근거를 수동으로 작성할 필요성을 없앴습니다. 그러나 이러한 자동으로 생성된 근거가 어떤 경우에는 부정확하기 때문에 Auto-CoT가 잘 작동하려면 몇 가지 요령이 필요합니다. 질문이 입력으로 주어졌을 때, 순진한 접근 방식은 i) 유사한 질문 세트를 검색하고(예: sBERT와 같은 임베딩 모델(embedding model) 및 벡터 검색(vector search) 사용), ii) 제로샷 CoT 프롬프팅으로 각 질문에 대한 근거/답변을 생성하며, iii) 자동으로 생성된 시연으로 수동 CoT 프롬프팅을 수행하는 것입니다. 그러나 이 접근 방식은 상당히 좋지 않게 작동하며, [1]의 저자들은 이를 LLM이 생성한 근거의 실수 때문이라고 주장합니다. 이를 해결하기 위해 생성된 근거가 충분히 다양하도록 보장하기만 하면 됩니다.

([15]에서 발췌) LLM이 답변해야 할 질문 데이터셋이 주어졌을 때, [15]의 저자들은 Auto-CoT를 위한 프롬프트 내에서 사용되는 시연을 선택/생성하기 위한 두 부분으로 된 전략(위 그림 참조)을 고안했습니다.
*   sBERT의 질문 임베딩(question embeddings)과 k-평균 클러스터링(k-means clustering)을 사용하여 질문을 k개의 클러스터(cluster)로 나눕니다.
*   각 클러스터에서 대표 질문을 선택하고 제로샷 CoT를 사용하여 각 질문에 대한 관련 근거를 생성합니다.
이러한 접근 방식은 Auto-CoT에 사용되는 시연의 다양성을 높여, 모델이 합성 근거(synthetic rationales)에서 저지르는 실수 간의 상관관계를 줄입니다. GPT-3를 사용한 실험에서 Auto-CoT는 수동 시연 생성이 필요한 퓨샷 CoT 프롬프팅의 성능과 일치하거나 이를 능가했으며, 10개 이상의 다른 벤치마크(benchmark)에서 일관된 결과를 보였습니다.

([16]에서 발췌) **복잡성 기반 프롬프팅(Complexity-Based Prompting)** [16]. CoT 프롬프팅이 프롬프트에 포함할 문제 해결 근거의 시연을 선택하는 데 의존한다는 점을 고려할 때, 우리는 궁금할 수 있습니다. 이러한 시연을 어떻게 가장 잘 선택할 수 있을까요? [16]에서 저자들은 복잡성을 기반으로 시연을 선택하는 것이 좋은 휴리스틱(heuristic)임을 보여줍니다. 시연의 복잡성은 연쇄 사고 내에 존재하는 단계의 수를 세는 것으로 간단히 측정할 수 있으며, 개별 단계는 개행 문자(newline characters, \n )로 구분됩니다. [16]에서 제안된 복잡성 기반 프롬프팅 접근 방식은 가장 높은 복잡성을 가진 시연을 샘플링(sampling)할 것을 옹호합니다.

"GPT-3 175B의 추론 성능은 입력 프롬프트 복잡성이 증가함에 따라 명확하게 향상됩니다." - [16]에서 발췌

흥미롭게도 [16]의 저자들은 CoT 프롬프트에 더 많은 추론 단계를 포함하는 시연을 포함하는 것이 다단계 추론 작업에서 성능을 상당히 향상시킨다는 것을 발견했습니다. 더 나아가, 이 전략을 출력 공간으로 확장하여 가장 높은 복잡성을 가진 k개의 생성된 출력에 대해 다수결 투표를 하는 자기 일관성(self-consistency) 접근 방식을 사용할 수 있습니다. 수동 튜닝(manual tuning) 및 검색 기반 선택(retrieval-based selection)과 같은 대안적인 선택 방식과 비교할 때, 복잡성 기반 프롬프팅은 GPT-3 및 Codex를 사용하여 여러 데이터셋(즉, GSM8K, MultiArith, MathQA)에서 최첨단(state-of-the-art) 성능을 달성하며 유리하게 작동합니다.

([17]에서 발췌) **점진적 힌트 프롬프팅(Progressive-Hint Prompting, PHP)** [17]. CoT 프롬프팅의 한 가지 단점은 문제를 단일 시도(single shot)로 해결한다는 것입니다. 질문이 입력으로 주어지면 우리는 근거와 답변을 생성하지만, LLM은 이 답변을 고려하거나 수정할 기회를 얻지 못합니다. 이 과정을 여러 번 반복하고 다수결 투표를 통해 더 나은 성능을 달성할 수 있습니다— 이것은 단지 자기 일관성입니다 —그러나 이러한 생성물 중 어느 것도 답변을 더 잘 알리기 위해 LLM의 이전 출력을 고려하지 않습니다.

"PHP는 질문을 재평가한 후 이전 답변을 힌트로 활용하여 올바른 답변에 도달하는 인간과 유사한 사고 과정을 따릅니다." - [17]에서 발췌

이 문제를 해결하기 위해 [17]의 저자들은 LLM의 이전 출력을 활용하여 생성된 근거를 반복적으로 개선하는 PHP를 제안합니다. 직관적으로 LLM은 모델이 이전에 생성한 근거를 올바른 답변을 찾는 힌트로 사용할 수 있습니다. 구체적으로 PHP는 세 단계로 진행됩니다.
*   질문이 주어지면 LLM에 기본 답변을 제공하도록 프롬프트를 제공합니다.
*   질문과 기본 답변을 연결한 다음, 이 입력을 기반으로 LLM에 수정된 답변을 생성하도록 프롬프트를 제공합니다.
*   LLM의 답변이 최소 두 번의 반복 동안 안정적일 때까지 2단계를 반복합니다.
이러한 접근 방식은 LLM이 여러 번의 통과를 통해 답변을 반복적으로 개선하고, 이 과정에서 이전 출력을 컨텍스트로 사용할 수 있도록 합니다. 또한 PHP는 CoT 프롬프팅 및 자기 일관성과 완벽하게 호환됩니다. 이러한 기술을 결합하여 성능을 더욱 향상시킬 수 있습니다. 실험에서 PHP는 복잡성 기반 프롬프팅 전략과 비교하여 GPT-3.5의 성능을 향상시키며, GPT-4와 함께 PHP를 사용하면 여러 주목할 만한 데이터셋(예: SVAMP, GSM8K, AQuA, MATH)에서 최첨단 성능을 달성합니다.

([18]에서 발췌) **분해 프롬프팅(Decomposed Prompting, DecomP)** [18]은 프롬프팅을 통해 복잡한 단계를 가진 다단계 추론 문제를 해결하는 어려움을 해결하려고 시도합니다. 작업이 더 복잡해질수록 퓨샷 프롬프팅(즉, 올바른 해결책의 몇 가지 예시를 보여주는 것)은 부족해질 것입니다. 그러나 복잡한 작업을 프롬프팅을 통해 독립적으로 해결할 수 있는 하위 작업(sub-tasks)으로 분해함으로써 더 잘 수행할 수 있습니다. 특히 [18]의 저자들은 두 가지 구성 요소로 이루어진 프롬프팅 프레임워크를 제안합니다.
*   **분해기(Decomposer)**: LLM에 문제를 일련의 더 간단한 하위 작업으로 분해하도록 프롬프트를 제공합니다.
*   **하위 작업 처리기(Sub-task handlers)**: 별도의 프롬프트를 사용하여 (분해기가 지시하는 대로) 더 간단한 하위 작업을 LLM으로 해결합니다.
분해기와 하위 작업 처리기는 퓨샷 방식으로 프롬프트가 제공되는 LLM일 뿐입니다. 위에서 제안된 DecomP 전략은 하나의 프롬프트를 사용하여 해결 가능한 하위 작업을 식별하고, 이를 다른 시스템(예: 새로운 프롬프트, 다른 LLM 또는 도구)에 위임하여 해결합니다. 이러한 모듈식 접근 방식은 많은 이점을 가집니다.
*   긴 컨텍스트를 가진 작업은 여러 구성 요소로 분해될 수 있습니다.
*   각 하위 작업에는 더 넓은 범위의 예시를 보여줄 수 있습니다.
*   복잡한 하위 작업은 필요한 경우 더 작은 하위 작업으로 추가 분해될 수 있습니다.
*   모든 하위 작업을 LLM으로 해결하는 대신, 다른 기호 시스템(symbolic systems)(예: 작업별 모델(task-specific model), 검색 메커니즘(retrieval mechanism) 등)을 사용할 수도 있습니다.
간단한 작업을 예로 들어 봅시다. 단어 집합이 입력으로 주어졌을 때, 각 단어의 세 번째 문자를 추출하고, 이 문자들을 연결하여 그 연결된 결과를 출력으로 제공하고자 합니다. 이를 위해 세 가지 하위 작업의 시퀀스를 만들 수 있습니다. i) 단어 목록 수집, ii) 각 단어의 세 번째 문자 추출, iii) 추출된 문자 연결. 아래 그림과 같이 이러한 각 하위 작업을 별도의 퓨샷 프롬프트로 구현할 수 있습니다.

([18]에서 발췌) DecomP 내에서 하위 작업은 분해기에 의해 반복적으로 생성되고, 해결되며, (관련 출력과 함께) 분해기로 반환되어 다음 하위 작업을 생성합니다. 분해기는 추론 프로세스의 컨트롤러(controller) 역할을 하면서 최종 답변이 생성되었음을 나타내는 질문 끝(end-of-question, [EOQ]) 마커가 생성될 때까지 하위 작업을 계속 생성합니다. (아래 참조) 전반적으로 DecomP는 최소-최대 프롬프팅의 더 일반적이고 유연한 버전으로 생각할 수 있습니다.

([18]에서 발췌) **가설-이론(Hypotheses-to-Theories)** [29] ([29]에서 발췌). 복잡한 작업을 간단한 단계로 분해하는 예시 근거로 모델에 프롬프트를 제공함으로써 LLM 내에서 추론 능력을 이끌어낼 수 있습니다. 그러나 모델은 출력을 생성할 때 환각(hallucinate)을 일으킬 수 있으며, 기존 또는 상식적 지식을 넘어선 작업에서는 성능이 좋지 않습니다. 간단히 말해, LLM의 지식 기반(knowledge base)과 작업을 해결하는 데 필요한 지식 사이에 불일치(mismatch)가 있을 때 문제가 발생합니다. 이 문제를 해결하기 위해, 복잡한 추론 문제를 해결할 때 LLM이 필요한 지식을 발견하고 적용할 수 있도록 하는 프롬프팅 접근 방식이 필요합니다.

([29]에서 발췌) 인간의 과학적 발견 과정에서 영감을 받아, [29]의 저자들은 가설-이론(Hypotheses-to-Theories, HtT) 프롬프팅이라는 프롬프팅 기술을 제안합니다. 이 기술은 (잠재적으로 부정확한) 가설을 자유롭게 제안하고, 경험적으로 검증될 수 있는 가설만을 유지하며, 이 검증된 가설을 사용하여 문제를 해결하는 전략을 따릅니다. 높은 수준에서 이 전략의 목표는 문제 해결에 사용될 수 있는 LLM을 위한 규칙 라이브러리(rule library)를 학습하는 것입니다. 더 구체적으로, HtT 프롬프팅(위 그림 참조)은 두 단계로 구성됩니다.
*   **귀납(Induction)**: LLM은 훈련 예시(training examples) 세트에 대해 규칙을 생성하고 검증하도록 요청받습니다. 자주 나타나고 올바른 답변을 자주 생성하는 규칙들이 수집되어 규칙 라이브러리를 형성합니다.
*   **연역(Deduction)**: LLM은 귀납을 통해 생성된 규칙 세트를 사용하여 추론을 수행하고 질문에 답하도록 프롬프트가 제공됩니다.
추론 중에 규칙 세트를 사용함으로써 HtT 프롬프팅은 환각의 가능성을 줄입니다. 이러한 발견은 수치 추론(numerical reasoning) 및 관계 추론(relational reasoning) 작업 모두에서 검증되었으며, HtT 프롬프팅은 이전 프롬프팅 기술(예: CoT 프롬프팅)에 비해 정확도에서 11-27%의 절대적인 개선을 제공하는 것으로 나타났습니다. 흥미롭게도 HtT 프롬프팅으로 생성된 규칙은 해석 가능(interpretable)하며 다른 (그러나 유사한) 문제로도 전이 가능(transferable)합니다.

**AI 에이전트와 외부 도구 통합**
LLM은 강력하지만, 주목할 만한 한계가 있습니다! 예를 들어, LLM은 산술적 실수를 저지르고, 최신 정보에 접근할 수 없으며, 심지어 시간의 흐름을 이해하는 데 어려움을 겪기도 합니다. 이러한 한계는 AI가 실제 세계의 복잡한 문제를 해결하기 위해 외부 시스템과 상호작용하는 능력을 강화해야 할 필요성을 제기합니다. 인류의 많은 발전은 새롭고 혁신적인 도구(예: 인쇄기 또는 컴퓨터)에 대한 접근에 의해 촉진되었으며, LLM도 마찬가지일 수 있습니다. 즉, 외부의 전문화된 도구(예: 계산기 또는 검색 엔진) 세트에 대한 접근 권한을 부여하고, 언제, 어디서, 어떻게 이러한 도구를 적절하게 호출하여 문제를 더 안정적으로 해결할 수 있는지 모델에 가르침으로써 이러한 모델의 많은 한계를 해결할 수 있습니다. 더 많은 정보는 아래의 이 주제에 대한 이전 개요를 참조하세요.
*   언어 모델에게 도구 사용법 가르치기(Teaching Language Models to Use Tools) [링크]
*   언어 모델과 친구들(Language Models and Friends) [링크]
*   언어 모델이 자체 도구를 만들 수 있을까?(Can language models make their own tools?) [링크]

**Toolformer** [32] ([32]에서 발췌)는 LLM과 외부 도구의 통합을 탐구한 최초의 연구 중 하나였습니다. 이러한 도구는 간단하고 고정된 텍스트-투-텍스트 API(text-to-text API) 세트를 통해 모델에 제공됩니다. (위 참조) 도구를 사용하기 위해 LLM은 i) 도구가 필요한 시나리오를 식별하고, ii) 사용할 도구를 지정하며, iii) 도구의 API에 관련 텍스트 입력을 제공하고, iv) API에서 반환된 텍스트를 사용하여 응답을 작성하는 방법을 학습해야 합니다. LLM은 초기 시드 데이터셋(seed dataset)으로 시작하여 더 강력한 LLM(예: GPT-4)을 사용하여 유효한 API 호출(API calls) 예시를 데이터에 추가하는 합성 훈련 데이터셋(synthetic training dataset)을 구성함으로써 이러한 기술을 학습합니다. (아래 참조) 이는 AI가 단순한 언어 모델을 넘어, 능동적인 에이전트(agent)로서 기능할 수 있는 가능성을 열었습니다. 여기에서 우리는 이 데이터에 대해 LLM을 간단히 미세 조정할 수 있습니다. 모델은 자신이 생성하는 텍스트 시퀀스 내에서 필요한 API 호출을 직접 생성하고 처리하는 방법을 학습할 것입니다. 이 경우, 텍스트 입력과 출력을 가진 API만 고려하기 때문에 API 호출을 인라인(inline) 방식으로 처리하는 것은 간단합니다. (아래 참조) 이는 데이터 효율적인 학습(data-efficient learning)의 중요성을 강조하며, 소량의 고품질 데이터로도 모델의 기능을 크게 확장할 수 있음을 보여줍니다.

"LLM은 최신 정보에 접근할 수 없거나 정밀한 수학적 추론을 수행할 수 없는 것과 같은 본질적인 한계에 직면합니다... 실제 작업 해결을 위해 외부 도구를 자동으로 구성하는 기능을 현재 LLM에 강화하는 것은 이러한 단점을 해결하는 데 중요합니다." - [19]에서 발췌

**Chameleon** [19]은 위에서 언급된 LLM의 한계를 완화하는 것을 목표로 합니다. 흥미롭게도, 이러한 한계 중 일부는 LLM과 외부 도구를 통합하는 기존 연구에서 다루어지지 않습니다. 사용되는 도구 세트가 일반적으로 고정되어 있거나(또는 도메인 특정적(domain-specific)) 항상 새로운 도메인(domain)으로 일반화될 수 없기 때문입니다. 더 일반적인 프레임워크를 만들기 위해 Chameleon은 "플러그 앤 플레이(plug-and-play)" 전략을 사용합니다. 이 전략은 중앙 LLM 기반 컨트롤러(controller)를 사용하여 복잡한 추론 작업을 해결하기 위해 여러 도구를 구성하는 프로그램— 자연어로 작성된 —을 생성합니다. (아래 참조) 이전 연구와 달리 Chameleon이 사용할 수 있는 도구는 상당히 포괄적입니다. 예를 들어, LLM, 상용 비전 모델(off-the-shelf vision models), 웹 검색 엔진(web search engines), Python 함수 등입니다. 이 프레임워크는 유연한 아키텍처를 통해 다양한 도구를 동적으로 조합하여 복잡한 작업을 해결합니다.
Chameleon 프레임워크는 두 가지 주요 구성 요소를 가집니다.
*   **플래너(Planner)**: 입력 쿼리(input query)를 사용 가능한 도구를 통해 해결할 수 있는 하위 작업으로 분해합니다.
*   **모듈 인벤토리(Module inventory)**: Chameleon이 사용할 수 있는 작업별 도구(설명 및 사용 예시 포함) 세트.
이는 모듈식 AI 설계의 핵심 원칙을 보여주며, 각 구성 요소가 특정 기능을 담당하여 전체 시스템의 확장성과 유지보수성을 높입니다. LLM으로 구현된 플래너는 자연어를 사용하여 외부 도구(예: image_captioner 또는 query_generator)에 대한 호출을 생성합니다. 우리는 간단한 문자열 일치(string matching)를 통해 이러한 도구를 식별할 수 있으며, 플래너가 출력하는 도구 시퀀스는 해당 작업별 모듈(task-specific modules) 각각을 호출하여 실행될 수 있는 자연어 프로그램(natural language program)을 형성합니다. 플래너와 작업별 모듈에 사용되는 프롬프트의 예시는 아래에 나와 있습니다. 컨트롤러에게 특정 도구를 언제 사용할지 가르치기 위해, 우리는 퓨샷 프롬프트 내에 도구 설명과 사용 예시를 포함하며, 이는 새로운 도구와 모듈로 쉽게 확장될 수 있습니다. 플래너의 인컨텍스트 학습 능력을 활용하여 해결책을 생성하기 때문에, 실제 쿼리를 해결하는 데 훈련이나 큐레이션된 규칙이 필요하지 않습니다. 대신, 우리는 LLM에 사용 가능한 도구의 예시를 제공하기만 하면, LLM은 이 정보를 사용하여 쿼리에 대한 올바른 최종 응답을 생성하기 위해 실행될 수 있는 도구 시퀀스를 추론할 수 있습니다. 더 나아가, 이 도구 시퀀스는 사람이 읽을 수 있으며 인간 사용자가 쉽게 디버깅(debug)할 수 있습니다. 우리는 인간의 전문 지식과 피드백을 시스템에 통합하는 휴먼 인 더 루프(human-in-the-loop) 접근 방식의 중요성을 강조합니다.
실험에서 Chameleon은 GPT-4를 사용하여 두 가지 복잡한 다중 모달(multi-modal)(즉, 텍스트와 이미지가 모두 포함됨) 추론 작업인 ScienceQA와 TabMWP에 적용되었습니다. Chameleon은 ScienceQA에서 86.54%의 새로운 최첨단 성능을 달성하여 GPT-4 및 GPT-3를 사용한 CoT 프롬프팅보다 각각 2.55% 및 11.37% 더 나은 성능을 보였습니다. TabMWP에서도 Chameleon은 98.78%의 정확도를 달성하며 유사한 개선을 보였습니다. 그러나 Chameleon의 효과는 GPT-4가 복잡한 추론 문제를 해결하기 위한 제약 조건을 추론하고 합리적/일관된 계획을 구성하는 능력에 의해 강화된다는 점에 유의해야 합니다. 이는 멀티모달 AI의 발전이 실제 세계의 다양한 문제 해결에 얼마나 큰 영향을 미치는지 보여줍니다.

"우리는 고급 LLM의 자기 지시(self-instruct)를 통해 오픈 소스 LLM에 도구를 사용할 수 있는 능력을 부여하도록 설계된 간단하면서도 효과적인 방법인 GPT4Tools를 제안합니다." - [20]에서 발췌

**GPT4Tools** [20]. 다양한 논문에서 LLM이 퓨샷 방식으로 도구를 활용하는 능력을 보여주었지만, 대부분의 논문은 독점적인 언어 모델(proprietary language models)에 의존하며 도구 사용을 용이하게 하기 위해 순전히 프롬프트 엔지니어링을 활용합니다. 이는 오픈 LLM으로도 유사한 결과를 재현할 수 있는지 궁금하게 만듭니다. 오픈 소스 LLM의 역량 강화는 AI 기술의 민주화에 기여합니다. [20]에서 저자들은 자기 지시(self-instruct) [21]를 사용하여 오픈 소스 LLM(예: LLaMA 및 OPT)이 다중 모달 도구(multimodal tools) 세트를 사용할 수 있도록 하는 미세 조정 데이터셋(finetuning dataset)을 생성하는 접근 방식을 제안합니다.
([21]에서 발췌) 먼저, 저자들은 강력한 교사 모델(teacher model)(즉, ChatGPT)에 프롬프트를 제공하여 관련 도구가 사용되는 예시를 생성하도록 함으로써 자기 지시 접근 방식을 사용하여 도구 사용 데이터셋을 생성합니다. 프롬프트 내에는 시각적 콘텐츠— 이미지에서 추출된 캡션(captions) 및 바운딩 박스(bounding boxes) —와 도구 설명이 모두 포함됩니다. 교사는 이 정보를 활용하여 다중 모달 정보를 처리하고 문제를 해결하는 데 사용될 수 있는 도구 관련 지시를 생성합니다. (위 참조) 이는 합성 데이터 생성(synthetic data generation) 기술이 모델 학습에 어떻게 활용될 수 있는지를 보여주는 중요한 사례입니다.
([21]에서 발췌) 데이터셋이 생성되면, 저랭크 적응(Low-Rank Adaptation, LoRA)을 사용하여 오픈 소스 LLM을 쉽게 미세 조정하여 다중 모달 도구의 도움을 받아 다양한 시각적 문제를 해결할 수 있습니다. [20]에서 이 접근 방식은 LLM이 알려진 도구(즉, 미세 조정 데이터셋에 포함된 도구)에 대해 수행하는 호출의 정확도를 향상시킬 뿐만 아니라, 모델이 제로샷 방식으로 새로운 도구에 일반화하는 능력도 향상시키는 것으로 나타났습니다. 이는 효율적인 미세 조정(fine-tuning) 기법이 AI 모델의 접근성과 활용성을 높이는 데 필수적임을 시사합니다. GPT4Tools와 LLM을 외부 도구와 통합하는 이전 연구에 대한 직접적인 비교는 위 표에 제공됩니다.

**Gorilla** [30] ([30]에서 발췌). 많은 연구에서 LLM과 고정된 도구 세트를 통합하는 것을 연구했지만, [30]의 저자들은 LLM에게 온라인에서 사용 가능한 모든 모델 API를 사용하도록 가르치는 더 넓은 목표를 다룹니다. Gorilla는 AI가 동적으로 도구를 학습하고 활용하는 능력을 보여줍니다. 이를 위해 i) 문제 해결과 관련된 모델 API를 검색하고 ii) 이 API에 대한 문서를 모델의 컨텍스트에 추가하는 검색 기술(retrieval technique)이 채택됩니다. 이러한 접근 방식은 LLM이 엄청난 수의 변화하는 도구에 접근할 수 있도록 하지만, 환각(예: 잘못된 인수(arguments) 또는 존재하지 않는 API 호출)이 여전히 발생할 수 있습니다. (위 참조)
([30]에서 발췌) 이 문제를 해결하기 위해 [30]의 저자들은 자기 지시 [21]를 사용하여 1,600개 이상의 다른 모델 API 사용 예시가 포함된 데이터셋을 구축합니다. 각 예시 내에서 프롬프트와 관련 문서가 모두 컨텍스트로 사용되어 출력을 생성합니다. 다시 말해, 이것은 검색 인식 미세 조정(retrieval-aware finetuning) 프로세스입니다 (RAFT와 유사). (위 참조) 이는 자기 지도 학습(self-supervised learning)이 AI 에이전트의 자율성을 높이는 데 어떻게 기여하는지 보여주는 예시입니다. 그 결과 모델인 Gorilla (LLaMA-7B의 미세 조정 버전)는 다양한 딥러닝 모델 API를 활용하여 문제를 해결하기 위한 인터페이스입니다. 결과 LLM은 엄청난 수의 API를 사용할 수 있으며, 심지어 이러한 API 중 어느 하나의 문서 변경에도 적응할 수 있습니다!

**HuggingGPT** [31] ([31]에서 발췌)는 도구 사용 접근 방식을 통해 LLM과 특수 딥러닝 모델(예: 이미지 인식, 비디오 감지, 텍스트 분류 등)의 통합을 탐구한다는 점에서 Gorilla와 상당히 유사합니다. LLM은 문제 해결 시스템의 "두뇌" 역할을 하며, 문제를 해결하는 방법을 계획하고 이 문제에 필요한 하위 작업을 해결하는 다양한 딥러닝 모델 간의 노력을 조정합니다. 이는 AI 오케스트레이션(orchestration)의 중요성을 강조하며, 여러 AI 모델과 도구가 협력하여 복잡한 작업을 해결하는 시스템을 구축하는 데 집중합니다. 그러나 Gorilla와 달리 HuggingGPT는 미세 조정을 수행하지 않습니다. 문제 해결은 네 단계로 분해됩니다.
*   **작업 계획(Task planning)**: LLM을 사용하여 사용자의 요청을 해결 가능한 작업으로 분해합니다.
*   **모델 선택(Model selection)**: HuggingFace에서 작업을 해결하는 데 사용할 모델을 선택합니다.
*   **작업 실행(Task execution)**: 선택된 각 모델을 실행하고 결과를 LLM에 반환합니다.
*   **응답 생성(Response generation)**: LLM을 사용하여 사용자에게 최종 응답을 생성합니다.
이는 AI 에이전트의 표준 워크플로(workflow)를 제시하며, 각 단계에서 최적의 도구와 모델을 선택하고 실행하는 과정을 포함합니다. 이러한 각 단계에 대해, 우리는 큐레이션된 지시와 예시를 포함하는 프롬프팅을 활용하여 원하는 동작을 얻습니다. (예시 프롬프트는 아래 참조) 충분히 강력한 파운데이션 모델(foundation model)이 주어진다면, 이러한 접근 방식은 매우 효과적입니다.

([31]에서 발췌)

**프로그램 지원 언어 모델**
"계산은 생성된 프로그램을 실행하는 데 사용되는 프로그램 인터프리터(program interpreter)에 위임될 수 있으며, 이로써 복잡한 계산을 추론 및 언어 이해와 분리합니다." - [41]에서 발췌

LLM을 외부 도구와 통합하는 것은 흥미로운 연구 분야이며, 이러한 모델에 접근 권한을 부여할 수 있는 가장 유용한 도구 중 하나는 프로그램을 작성하고 실행하는 능력입니다. 이는 AI가 단순한 언어 이해를 넘어 강력한 계산 엔진(computational engine)으로 진화하고 있음을 보여줍니다. 대부분의 프롬프팅 기술은 복잡한 문제를 두 단계로 해결합니다.
*   문제 해결 근거를 생성합니다.
*   이 근거를 사용하여 실제로 문제를 해결합니다.
CoT 프롬프팅에서는 이 두 단계를 모두 LLM이 해결하도록 의존하지만, 이 모델들은 첫 번째 단계 해결에만 탁월합니다! 사실, 올바른 근거를 출력했음에도 불구하고 잘못된 답변을 생성하는 것은 LLM의 흔한 실패 사례입니다. 이 문제를 해결하기 위해, 모델이 언어와 코드(예: 유용한 주석이 있는 Python 프로그램)가 섞인 형태로 근거를 출력하도록 가르칠 수 있습니다. 그런 다음, 제공된 코드를 단순히 실행함으로써 최종 답변을 생성할 수 있습니다! 이는 AI 시스템이 작업을 자동화하고, 인간의 개입 없이도 복잡한 워크플로(workflow)를 실행할 수 있는 기반을 제공합니다.

**프로그램 지원 언어 모델(Program-Aided Language Model, PAL)** [40] ([40]에서 발췌)은 LLM이 해결책을 찾기 위해 문제를 일련의 중간 단계로 분해하는 작업을 맡는다는 점에서 CoT 프롬프팅과 유사합니다. 그러나 이 근거는 자연어와 프로그래밍 구성 요소(programatic components)를 모두 포함합니다. PAL은 모델이 직접 실행 가능한 코드를 생성하도록 함으로써, AI의 문제 해결 능력을 실제 컴퓨팅 환경으로 확장합니다. 우리는 근거에서 코드를 실행하여(샌드박스(sandboxed) Python 환경 사용) 신뢰할 수 있는 최종 해결책을 생성할 수 있습니다. 실제 해결책 생성 과정은 코드 인터프리터(code interpreter)에 위임됩니다. [40]에서 우리는 코드에 대해 충분히 훈련된 LLM(예: Codex)이 퓨샷 학습 접근 방식을 사용하여 이러한 방식으로 문제를 해결하도록 가르칠 수 있음을 알 수 있습니다.

"이는 연쇄 사고와 유사한 방법에서 추론 연쇄는 올바르지만 잘못된 답변을 생성할 수 있는 중요한 격차를 해소합니다." - [40]에서 발췌

**사고 프로그램(Program of Thoughts, PoT) 프롬프팅** [41] ([41]에서 발췌)은 i) 코드 증강 프롬프팅 기술(code-augmented prompting technique)을 사용하고 ii) 해결책을 도출하는 과정을 코드 인터프리터에 위임한다는 점에서 PAL과 상당히 유사합니다. 이 과정은 퓨샷 프롬프팅 전략에 의존합니다. (위 참조) 그러나 PAL과 달리 PoT가 작성한 코드는 SymPy라는 기호 수학 라이브러리(symbolic math library)에 의존합니다. PoT는 특히 기호 수학 라이브러리를 활용하여 AI가 정밀한 수학적 추론을 수행하도록 돕습니다. 이 패키지는 사용자가 수학적 "기호"를 정의할 수 있게 하며, 이 기호들은 SymPy의 solve 함수를 통해 평가되는 복잡한 표현식을 형성하도록 결합될 수 있습니다. (아래 참조)
높은 수준에서 PoT는 LLM이 복잡한 방정식을 해결할 수 없다는 점을 직접적으로 다루며, 이러한 방정식을 쉽게 구성/평가할 수 있도록 하는 기호 수학 라이브러리에 대한 접근을 제공합니다. 하이브리드 AI 시스템(hybrid AI system)이 신경망과 전통적인 계산 방법을 결합하여 강점을 극대화하는 방법을 보여줍니다. 반면 PAL은 자연어와 코드의 조합을 통해 문제를 더 일반적으로 해결하는 데 중점을 둡니다. 프로그램 지원 모델에 대한 더 많은 정보는 이 관련 개요를 참조하세요.

([41]에서 발췌)

**컨텍스트 윈도우 관리 및 활용**
RAG의 최근 인기와 최첨단 LLM 내의 긴 컨텍스트 윈도우에 대한 강조를 고려할 때, 이러한 모델이 프롬프트에 제공된 컨텍스트를 어떻게 처리하는지 이해하는 것이 중요하며, AI 시스템이 방대한 양의 정보를 효율적으로 처리하고 관리하는 것이 중요한 과제로 부상하고 있습니다. 다행히도 최근 연구는 컨텍스트 윈도우와 인컨텍스트 학습 주제를 심층적으로 연구하여 프롬프트 엔지니어링과 관련된 몇 가지 흥미로운 시사점을 도출했습니다.

**대규모 언어 모델은 관련 없는 컨텍스트에 쉽게 주의가 산만해질 수 있습니다** [22]. 언어 모델에 프롬프트를 제공할 때, 우리는 일반적으로 프롬프트 내에 관련 컨텍스트와 정보만 포함합니다. 그러나 실제 응용 분야에서는 모델의 프롬프트가 일반적으로 해결되는 특정 문제와 관련이 있을 수도 있고 없을 수도 있는 컨텍스트적으로 유사한 정보를 포함합니다. 이를 염두에 두고, 우리는 궁금할 수 있습니다. 프롬프트에 관련 없는 컨텍스트를 추가하는 것이 부정적인 부작용을 가져올까요? 이는 AI 시스템의 견고성(robustness)을 설계할 때 외부 정보의 필터링 및 우선순위 지정이 얼마나 중요한지를 보여줍니다.
([22]에서 발췌) [22]에서 저자들은 현대 LLM의 주의 산만성(distractibility)을 연구했으며, 관련 없는 컨텍스트가 프롬프트에 포함될 때 이러한 모델의 성능이 급격히 저하될 수 있음을 발견했습니다. 이는 AI 모델이 예상치 못한 입력에 어떻게 반응하는지에 대한 중요한 통찰을 제공합니다. LLM 주의 산만성을 측정하기 위해 저자들은 문제 설명에 관련 없는 정보가 포함된 산술 추론 문제(arithmetic reasoning problems)를 포함하는 새로운 관련 없는 컨텍스트를 포함한 초등학교 수학(Grade-School Math with Irrelevant Context, GSM-IC) 데이터셋을 소개합니다. (위 참조) 그런 다음, 모델의 프롬프트에 관련 없는 문장을 추가하는 것이 문제의 결과 해결책을 변경하는지 여부를 테스트함으로써 LLM이 관련 없는 컨텍스트에 의해 주의가 산만해지는지 여부를 측정할 수 있습니다. 흥미롭게도, 관련 없는 정보가 컨텍스트에 포함될 때 이러한 모델의 성능은 급격히 저하됩니다. 이는 고품질 데이터와 정제된 입력이 AI 시스템의 안정적인 작동에 필수적임을 강조합니다.
이 전략은 Codex와 GPT-3.5를 여러 다른 프롬프팅 기술(그림은 위 참조)로 테스트하는 데 사용됩니다.
*   CoT 프롬프팅 (및 제로샷 CoT 프롬프팅)
*   최소-최대 프롬프팅
*   프로그램으로 프롬프팅
그러나 관련 없는 컨텍스트의 영향은 i) 자기 일관성 사용, ii) 모델이 관련 없는 정보를 무시하도록 지시 추가, iii) 관련 없는 정보로 문제를 해결하는 것을 시연하는 퓨샷 예시 포함을 통해 완화될 수 있습니다. LLM은 지시 또는 컨텍스트를 통해 정보를 무시하는 것을 학습할 수 있습니다.
"우리는 예시에 '문제 설명의 관련 없는 정보는 자유롭게 무시하세요'라는 지시 문장을 앞에 추가합니다." - [22]에서 발췌

**중간에서 길을 잃다(Lost in the Middle)** [23]. 생성형 LLM은 텍스트-투-텍스트 형식을 가지며, 이는 텍스트 시퀀스를 입력(즉, 프롬프트)으로 받아들이고 해당 텍스트 시퀀스를 출력으로 생성한다는 의미입니다. LLM에 전달되는 입력은 가변 길이(variable length)입니다. 짧은 (제로샷) 문제 설명일 수도 있고, 많은 양의 외부 컨텍스트(예: RAG용)를 포함하는 복잡한 지시일 수도 있습니다. 이러한 이유로 LLM은 긴 컨텍스트에서 작동하고 이 컨텍스트 전체를 사용하여 다운스트림 작업(downstream tasks)을 효과적으로 해결할 수 있어야 합니다. 긴 컨텍스트 내에서 정보를 활용하는 모델의 패턴을 이해하는 것이 중요합니다. 이러한 맥락에서 [23]의 저자들은 여러 LLM— 오픈 모델(MPT)과 폐쇄 모델(GPT-3.5-Turbo 및 Claude-1.3) 모두 —이 긴 컨텍스트 내에서 제공된 정보를 구체적으로 활용하는 능력을 연구합니다. 특히 [23]에서는 두 가지 유형의 작업이 연구됩니다.
*   **다중 문서 QA(Multi-document QA)**: 표준 RAG 설정과 유사하게, 이 문제는 모델이 여러 문서를 추론하여 질문에 답하도록 요구합니다.
*   **키-값 검색(Key-value retrieval)**: 이는 컨텍스트로 제공된 JSON 키-값 쌍(key-value pairs) 컬렉션에서 키와 관련된 값을 반환하여 일치하는 토큰을 검색하는 모델의 능력을 테스트하는 합성 작업입니다.
이러한 작업을 해결할 때 저자들은 i) 입력 컨텍스트의 길이(더 많은 문서 또는 키-값 쌍 사용)와 ii) 입력 내 관련 컨텍스트의 위치— 시작, 중간, 끝 —를 모두 제어합니다. 이는 AI 모델의 컨텍스트 이해 능력을 평가하는 데 중요한 방법론을 제공합니다. 그런 다음, 컨텍스트 길이와 위치 변화가 모델 성능에 미치는 영향을 연구할 수 있습니다. 실험에서 우리는 모델 컨텍스트 내 관련 정보의 위치를 기반으로 명확한 "U자형" 성능 곡선(아래 그림 참조)을 볼 수 있습니다.
([23]에서 발췌) 이 시각화는 LLM이 컨텍스트의 시작과 끝에 있는 정보에 가장 많은 주의를 기울인다는 것을 보여줍니다. 관련 정보가 컨텍스트의 중간에 있을 때 모델 성능은 크게 저하됩니다— 정보가 "중간에서 길을 잃습니다(lost in the middle)". 사실, GPT-3.5-Turbo는 관련 문서가 컨텍스트 중간에 배치될 때보다 관련 컨텍스트가 전혀 없을 때 다중 문서 QA 작업에서 더 나은 성능을 보입니다. 이는 AI 모델이 인간과 유사한 인지 편향(cognitive bias)을 보일 수 있음을 시사하며, 이러한 편향을 이해하고 완화하는 연구의 중요성을 강조합니다. 관련 정보의 위치를 조정함에 따라 성능은 크게 달라지며, 확장된 컨텍스트를 가진 모델은 이러한 위치 편향(positional biases)에 대한 견고성 개선 징후를 보이지 않습니다. (아래 참조) 그러나 이러한 문제는 더 최근 모델(예: Gemini-1.5 및 Claude-3)에서 개선되었습니다.

**대규모 언어 모델은 잠재 변수 모델(Latent Variable Models)입니다** [24]. LLM이 인컨텍스트 학습 능력을 가지고 있다는 것을 알고 있지만, 이러한 능력이 표준 언어 모델 사전 학습에서 어떻게 나타나는지는 불분명합니다. LLM의 이론적 기반을 이해하는 것은 모델의 동작을 예측하고 제어하는 데 필수적입니다. 또한 인컨텍스트 학습은 일반적으로 퓨샷 학습에 사용되는 예시의 선택과 형식에 민감합니다. 특정 시연은 모델에 효과적인 예시인 반면, 다른 시연은 그렇지 않습니다. 현재 퓨샷 학습을 위한 최상의 예시를 선택하는 표준 기준은 없습니다. [24]에서 저자들은 이 주제를 연구하며, 가능한 최상의 퓨샷 예시를 식별하기 위한 실용적인 전략을 찾는 것을 목표로 합니다.
"인컨텍스트 학습은 광범위한 자연어 처리(NLP) 작업에 효과적인 기술로 입증되었습니다. 그러나 이는 사용되는 시연의 선택, 형식, 심지어 순서에도 민감합니다." - [24]에서 발췌
많은 논문이 이론적 관점에서 인컨텍스트 학습의 메커니즘을 연구했지만, 실용적이거나 실행 가능한 통찰력을 제공하는 논문은 거의 없습니다. 이론과 실제 사이의 간극을 메우는 것이 중요합니다. [24]에서 저자들은 LLM을 언어 모델이 관찰한 이전 토큰에 새로운 토큰 생성을 연결하는 간단한 주제/잠재 변수 모델의 관점에서 봅니다. 자세한 내용은 논문에서 찾을 수 있지만, 높은 수준에서 이 공식화(formulation)는 모델의 입력 프롬프트 내에서 사용된 형식 및 작업 정보와 관련하여 언어 모델의 출력을 이론적으로 설명할 수 있게 합니다.
([24]에서 발췌) 이 공식화로부터 저자들은 모델 입력의 사후 확률(posterior probability)을 측정하기 위해 더 작은 언어 모델을 사용하는 가능한 최상의 퓨샷 예시를 선택하기 위한 실용적인 기술을 개발합니다. 이는 모델의 입력과 매개변수를 기반으로 다른 입력 예시의 가능성을 알려줍니다. 더 작은 LLM으로 선택된 예시를 더 큰 모델과의 인컨텍스트 학습에 사용할 수 있으며(위 참조), 이는 실질적인 이점을 제공하는 것으로 밝혀졌습니다. 이는 효과적인 학습을 위한 데이터 선택 전략이 AI 모델의 성능에 결정적인 영향을 미칠 수 있음을 보여줍니다. 간단히 말해, 이 논문은 더 나은 퓨샷 예시를 선택하기 위해 실제로 사용될 수 있는 인컨텍스트 학습에 대한 흥미롭고(상대적으로 간단한) 이론적 관점을 제안합니다.

**생성 능력 최적화 및 제어**
"SoT는 추론 효율성을 위한 데이터 중심 최적화의 초기 시도이며, 언어로 답변 구조를 명시적으로 계획함으로써 고품질 답변을 이끌어낼 잠재력을 보여줍니다." - [25]에서 발췌

**사고의 골격(Skeleton-of-Thought, SoT)** [25]은 LLM으로 출력을 생성하는 지연 시간(latency)을 줄이는 것을 목표로 하는 프롬프팅 기술입니다. 아시다시피, LLM으로 출력을 생성하는 것은 몇 가지 이유로 비용이 많이 들 수 있습니다.
*   모델이 크기 때문에 계산/메모리/I/O 비용이 높습니다.
*   어텐션(attention) 연산은 I/O 바운드(IO bound)이며 시퀀스