거대 언어 모델(LLM)의 성능을 어떻게 가늠할 수 있을까요? 이 물음은 단순해 보이지만, 실제로는 광범위한 의견 교환을 요구하는 사안입니다. 제가 프로젝트 자문이나 협업 시 가장 흔히 접하는 문의사항은 여러 모델 중 어떤 것을 골라야 할지, 그리고 외부에 공개된 성능 측정치를 어떻게 해석해야 하는지에 관한 것입니다. (물론, 자체 모델을 미세 조정(fine-tuning)하거나 개발할 때 발전 정도를 파악하는 방안도 포함됩니다.) 이러한 궁금증이 자주 제기되므로, 사람들이 LLM의 역량을 상호 비교하기 위해 주로 사용하는 평가 방식에 대한 간략한 안내를 드리는 것이 유익할 것이라 판단했습니다. 대규모 언어 모델의 역량을 사정하는 일은 한 권의 문서만으로는 온전히 다루기 힘든 방대한 영역이지만, 핵심적인 접근법들에 대한 명확한 이해를 갖춘다면 각종 성능 지표, 순위표, 연구 보고서 등을 훨씬 수월하게 파악할 수 있을 것입니다.

원래 이러한 평가 기법들은 곧 출간될 저의 저서 『Build a Reasoning Model (From Scratch)』에 수록될 예정이었으나, 책의 주된 내용과는 약간 거리가 있었습니다. (해당 서적은 검증기 기반 사정(verifier-based evaluation)에 더 초점을 맞추고 있습니다.) 이에 따라 이 내용을 처음부터 작성된 코드 예시와 함께 더욱 확장된 글로 선보이는 것이 좋겠다고 생각했습니다. 『Build A Reasoning Model (From Scratch)』은 추론형 LLM(reasoning LLM)을 밑바닥부터 설계하는 실용적인 접근법을 제시합니다. 만약 『Build A Large Language Model (From Scratch)』을 흥미롭게 읽으셨다면, 이 책 또한 순수 파이토치(PyTorch)로 모든 구성 요소를 직접 구현하는 유사한 서술 방식을 따르고 있습니다. 추론(Reasoning)은 LLM 개선에 있어 가장 매력적이고 중요한 최신 진전 중 하나이지만, 단순히 용어만 듣거나 이론적으로만 접하면 가장 오해하기 쉬운 개념이기도 합니다. 그러므로 본 서적은 추론형 LLM을 맨 처음부터 구축하는 실습 위주의 방안을 제시하고 있습니다.

이 책은 현재 얼리 액세스(early-access) 단계에 있으며, 이미 100페이지 이상이 온라인에 공개되었고, 레이아웃 팀에서 현재 추가하고 있는 30페이지 분량의 작업이 최근 완료되었습니다. 얼리 액세스 프로그램에 참여하신 독자분들께는 (귀한 지원에 진심으로 감사드립니다!) 해당 내용이 공개될 때 이메일로 안내해 드릴 예정입니다.

추신: 현재 LLM 연구 분야는 엄청난 속도로 발전하고 있습니다. 저는 북마크(bookmark) 해둔 논문 목록을 따라잡기 위해 노력하고 있으며, 다음 글에서는 가장 흥미로운 발견 몇 가지를 중점적으로 다룰 계획입니다. 하지만 지금은 네 가지 핵심적인 LLM 평가 방식과 그 장점 및 약점을 더 심도 깊게 이해하기 위한 직접 작성된 코드 구현에 대해 이야기해 보겠습니다.

### LLM의 핵심 평가 방식 해부하기

실제로 학습된 LLM을 평가하는 네 가지 보편적인 기법은 아래 그림 1에 나타난 바와 같이 **객관식(multiple choice)**, **검증기(verifier)**, **리더보드(leaderboard)** 및 **LLM 심사위원(LLM judges)**입니다. 연구 논문, 마케팅 자료, 기술 보고서, 그리고 모델 카드(LLM 관련 기술 보고서를 일컫는 용어)에는 종종 이 범주들 중 두 가지 이상에서 얻은 결과가 포함되어 있습니다.

**그림 1**: 이 글에서 다루는 4가지 상이한 평가 모형에 대한 개요.

또한, 여기서 소개된 네 가지 분류는 위 그림에 명시된 바와 같이 **벤치마크 기반 사정(benchmark-based evaluation)**과 **판단 기반 사정(judgment-based evaluation)**의 두 그룹으로 나뉩니다. (훈련 손실(training loss), 퍼플렉시티(perplexity) 및 보상(reward)과 같은 다른 측정 수단도 존재하지만, 이들은 통상적으로 모델 개발 과정에서 내부적으로 활용됩니다.) 이어지는 하위 항목들에서는 네 가지 기법 각각에 대한 간략한 설명과 예시를 제공할 것입니다.

### 방법 1: 답변 선택의 정확성 확인하기

표준화된 평가 방식인 다지선다형 질의응답(multiple-choice question answering)부터 시작해 봅시다. 과거부터 광범위하게 활용되어 온 검증 방식 중 하나는 MMLU(Massive Multitask Language Understanding의 약어, https://huggingface.co/datasets/cais/mmlu)와 같은 다지선다형 표준 시험입니다. 이 접근법을 설명하기 위해 그림 2는 MMLU 데이터셋(dataset)의 전형적인 과제를 예시로 보여줍니다.

**그림 2**: LLM이 제시한 다지선다형 답변을 데이터셋의 정답과 대조하여 MMLU에서 LLM의 역량을 평가하는 모습.

그림 2는 MMLU 데이터셋의 단일 사례를 보여줍니다. MMLU 데이터셋 전체는 고등학교 수학부터 생물학에 이르기까지 57개 과목에 걸쳐 총 약 16,000개의 다지선다형 문항으로 구성되어 있으며, 성능은 정답률(accuracy)(정답을 맞힌 문항의 비율)로 측정됩니다. 예를 들어, 16,000개의 질문 중 14,000개를 맞히면 87.5%가 됩니다. MMLU와 같은 다지선다형 평가는 규격화된 시험, 다수의 학업 평가 또는 운전면허 필기시험처럼 LLM이 보유한 정보 인출 역량을 직관적이면서 수치화된 형태로 점검합니다. 이는 특정 분야에 대한 모델의 이해도와 지식 기반의 폭넓음을 신속하게 파악하는 데 매우 효과적입니다. 그림 2는 모델이 예측한 답변 문자가 정답과 직접적으로 비교되는 다지선다형 평가의 간소화된 형태를 보여줍니다. **로그 확률 점수 매기기(log-probability scoring)**를 포함하는 두 가지 다른 인기 있는 방법이 있습니다. 저는 이들을 [여기 GitHub](https://github.com/rasbt/reasoning-from-scratch/blob/main/ch02/01_mmlu_evaluation/02_mmlu_log_probability_scoring.ipynb)에 구현했습니다. (이는 여기서 설명된 개념을 토대로 하므로, 이 글을 마친 후 확인해 보시는 것을 권장합니다.) 다음 하위 섹션에서는 그림 2에 나타난 MMLU 점수 산정 방식을 코드로 구현하는 방법을 상세히 설명합니다.

#### 1.2 모형 로드하기

먼저, MMLU에서 평가하기 전에 사전 학습된 모형(pre-trained model)을 불러와야 합니다. 여기서는 순수 파이토치(PyTorch)로 Qwen3 0.6B를 밑바닥부터 구현한 것을 활용할 것이며, 이는 대략 1.5GB의 램(RAM)만 필요로 합니다. Qwen3 모형 구현의 세부 사항은 이 맥락에서는 크게 중요하지 않습니다. 우리는 단지 이를 평가하려는 LLM으로 간주합니다. 하지만 궁금하시다면, 저의 이전 글인 [Understanding and Implementing Qwen3 From Scratch](https://magazine.sebastianraschka.com/p/understanding-and-implementing-qwen3)에서 처음부터 구현하는 과정을 찾아볼 수 있으며, 원본 코드(source code)는 [여기 GitHub](https://github.com/rasbt/reasoning-from-scratch/tree/main/reasoning_from_scratch/qwen3)에서도 확인할 수 있습니다.

수많은 Qwen3 원본 코드(source code)를 일일이 복사하여 붙여넣는 대신, `pip install reasoning_from_scratch` 또는 `uv add reasoning_from_scratch` 명령을 통해 설치할 수 있는 저의 `reasoning_from_scratch` 파이썬 라이브러리(Python library)에서 이를 불러옵니다.

**코드 블록 1**: 사전 학습된 모형 불러오기

```python
from pathlib import Path
import torch
from reasoning_from_scratch.ch02 import get_device
from reasoning_from_scratch.qwen3 import (
    download_qwen3_small,
    Qwen3Tokenizer,
    Qwen3Model,
    QWEN_CONFIG_06_B
)

device = get_device()

# Set matmul precision to "high" to
# enable Tensor Cores on compatible GPUs
torch.set_float32_matmul_precision("high")

# Uncomment the following line
# if you encounter device compatibility issues
# device = "cpu"

# Use the base model by default
WHICH_MODEL = "base"

if WHICH_MODEL == "base":
    download_qwen3_small(
        kind="base", tokenizer_only=False, out_dir="qwen3"
    )
    tokenizer_path = Path("qwen3") / "tokenizer-base.json"
    model_path = Path("qwen3") / "qwen3-0.6B-base.pth"
    tokenizer = Qwen3Tokenizer(tokenizer_file_path=tokenizer_path)

elif WHICH_MODEL == "reasoning":
    download_qwen3_small(
        kind="reasoning", tokenizer_only=False, out_dir="qwen3"
    )
    tokenizer_path = Path("qwen3") / "tokenizer-reasoning.json"
    model_path = Path("qwen3") / "qwen3-0.6B-reasoning.pth"
    tokenizer = Qwen3Tokenizer(
        tokenizer_file_path=tokenizer_path,
        apply_chat_template=True,
        add_generation_prompt=True,
        add_thinking=True,
    )
else:
    raise ValueError(f"Invalid choice: WHICH_MODEL={WHICH_MODEL}")

model = Qwen3Model(QWEN_CONFIG_06_B)
model.load_state_dict(torch.load(model_path))
model.to(device)

# Optionally enable model compilation for potential performance gains
USE_COMPILE = False
if USE_COMPILE:
    torch._dynamo.config.allow_unspec_int_on_nn_module = True
    model = torch.compile(model)
```

#### 1.3 생성된 답변 문자 확인하기

이 부분에서는 가장 간명하고 아마도 가장 직관적인 MMLU 채점 방식을 구현합니다. 이 방식은 생성된 다지선다형 답변 문자가 정답과 일치하는지 확인하는 데 의존합니다. 이는 편의를 위해 아래에 다시 제시된 그림 2에서 이전에 설명된 것과 유사합니다.

**그림 3**: LLM이 제시한 다지선다형 답변을 데이터셋의 정답과 대조하여 MMLU에서 LLM의 역량을 평가하는 모습.

이를 위해 MMLU 데이터셋의 예시를 사용하겠습니다.

```python
example = {
    "question": (
        "How many ways are there to put 4 distinguishable"
        " balls into 2 indistinguishable boxes?"
    ),
    "choices": ["7", "11", "16", "8"],
    "answer": "D",
}
```

다음으로, LLM 프롬프트(prompt)의 형식을 지정하는 함수를 정의합니다.

**코드 블록 2**: 사전 학습된 모형 불러오기

```python
def format_prompt(example):
    return (
        f"{example['question']}\n"
        f"A. {example['choices'][0]}\n"
        f"B. {example['choices'][1]}\n"
        f"C. {example['choices'][2]}\n"
        f"D. {example['choices'][3]}\n"
        "Answer: "
    )
# Trailing space in "Answer: " encourages a single-letter next token
```

형식화된 LLM 입력이 어떻게 보이는지 확인하기 위해 MMLU 예시에서 함수를 실행해 보겠습니다.

```python
prompt = format_prompt(example)
print(prompt)
```

출력은 다음과 같습니다.

```
How many ways are there to put 4 distinguishable balls into 2 indistinguishable boxes?
A. 7
B. 11
C. 16
D. 8
Answer:
```

위에 제시된 모형 프롬프트(prompt)는 모형에게 다양한 답변 선택지 목록을 제공하고 "Answer: " 텍스트로 마무리하여 모형이 정답을 생성하도록 유도합니다. 엄밀히 필수적인 것은 아니지만, 모형이 과제를 어떻게 풀어야 하는지 관찰할 수 있도록 정답과 함께 추가 질문을 입력으로 제공하는 것이 때로는 도움이 될 수 있습니다. (예를 들어, 5개의 예시가 주어지는 경우는 5-샷 MMLU(5-shot MMLU)라고도 알려져 있습니다.) 그러나 기본적인 모형조차도 상당히 유능한 현재 세대의 LLM에서는 이것이 필요하지 않습니다.

#### 다른 MMLU 샘플 불러오기

`datasets` 라이브러리( `pip install datasets` 또는 `uv add datasets`를 통해 설치 가능)를 통해 MMLU 데이터셋에서 직접 예시를 불러올 수 있습니다.

```python
from datasets import load_dataset

configs = get_dataset_config_names("cais/mmlu")
dataset = load_dataset("cais/mmlu", "high_school_mathematics")

# Inspect the first example from the test set:
example = dataset["test"][0]
print(example)
```

위에서는 "high_school_mathematics" 하위 집합을 사용했습니다. 다른 하위 집합 목록을 얻으려면 다음 코드를 사용하십시오.

```python
from datasets import get_dataset_config_names

subsets = get_dataset_config_names("cais/mmlu")
print(subsets)
```

다음으로, 프롬프트(prompt)를 토큰화(tokenize)하고 LLM의 입력으로 파이토치 텐서 객체(PyTorch tensor object)로 감쌉니다.

```python
prompt_ids = tokenizer.encode(prompt)
prompt_fmt = torch.tensor(prompt_ids, device=device)

# Add batch dimension:
prompt_fmt = prompt_fmt.unsqueeze(0)
```

그런 다음, 모든 설정이 완료되면 아래에 핵심 점수 산정 함수를 정의합니다. 이 함수는 몇 개의 토큰(token)(여기서는 기본적으로 8개 토큰)을 생성하고 모형이 출력하는 문자 A/B/C/D의 첫 번째 인스턴스를 추출합니다.

**코드 블록 3**: 생성된 문자 추출하기

```python
from reasoning_from_scratch.ch02_ex import (
    generate_text_basic_stream_cache
)

def predict_choice(
    model, tokenizer, prompt_fmt, max_new_tokens=8
):
    pred = None
    for t in generate_text_basic_stream_cache(
        model=model,
        token_ids=prompt_fmt,
        max_new_tokens=max_new_tokens,
        eos_token_id=tokenizer.eos_token_id,
    ):
        answer = tokenizer.decode(t.squeeze(0).tolist())
        for letter in answer:
            letter = letter.upper()
            # stop as soon as a letter appears
            if letter in "ABCD":
                pred = letter
                break
        if pred:
            break
    return pred
```

그런 다음 위 코드 블록의 함수를 활용하여 생성된 문자를 다음과 같이 확인할 수 있습니다.

```python
pred1 = predict_choice(model, tokenizer, prompt_fmt)
print(
    f"Generated letter: {pred1}\n"
    f"Correct? {pred1 == example['answer']}"
)
```

결과는 다음과 같습니다.

```
Generated letter: C
Correct? False
```

보시다시피, 이 경우 생성된 답변은 부정확합니다(`False`). 이것은 MMLU의 `high_school_mathematics` 하위 집합에 있는 270개 예시 중 하나일 뿐입니다. 아래 스크린샷(그림 4)은 전체 하위 집합에서 실행했을 때 기본 모델과 추론 변형의 성능을 보여줍니다. 이 코드는 [여기 GitHub](https://github.com/rasbt/reasoning-from-scratch/blob/main/ch02/01_mmlu_evaluation/01_mmlu_answer_letter_scoring.ipynb)에서 확인할 수 있습니다.

**그림 4**: MMLU `high_school_mathematics` 하위 집합에 대한 기본 및 추론 모형 성능

질문들이 동일한 답변 확률을 가진다고 가정할 때, 무작위 추측기(random guesser)(A, B, C 또는 D를 균일한 확률로 선택)는 25%의 확률을 달성할 것으로 예상됩니다. 따라서 기본 모델과 추론 모델 모두 그다지 좋지 않습니다.

#### 객관식 답변 형식의 심층 분석

이 섹션에서는 모형의 예측된 답변 문자가 정답과 직접 비교되는 다지선다형 평가의 단순화된 버전을 설명 목적으로 구현했습니다. 실제로는 모형이 최종 문자 선택만 확인하는 대신 각 후보 답변을 얼마나 가능성 있게 고려하는지 측정하는 로그 확률 점수 매기기(log-probability scoring)와 같이 더 널리 사용되는 변형이 존재합니다. (확률 기반 점수 매기기는 4장에서 논의합니다.) 추론 모형의 경우, 평가는 정답이 입력으로 제공될 때 정답을 생성할 가능성을 평가하는 것을 포함할 수도 있습니다.

**그림 5**: 다른 MMLU 점수 산정 방식은 [여기 GitHub](https://github.com/rasbt/reasoning-from-scratch/blob/main/ch02/01_mmlu_evaluation/02_mmlu_log_probability_scoring.ipynb)에 설명되어 있고 공유되어 있습니다.

그러나 어떤 MMLU 점수 산정 변형을 사용하든, 평가는 모형이 미리 정의된 답변 옵션 중에서 선택하는지 여부를 확인하는 것으로 귀결됩니다. MMLU와 같은 다지선다형 표준 시험의 한계는 미리 정의된 옵션 중에서 선택하는 LLM의 능력만을 측정하므로, 모형이 기본 모형에 비해 얼마나 많은 지식을 잊었는지 확인하는 것 외에는 추론 능력(reasoning capabilities)을 평가하는 데 그다지 유용하지 않다는 것입니다. 이는 자유 형식 작성 능력(free-form writing ability)이나 실제 유용성을 포착하지 못합니다. 그럼에도 불구하고, 다지선다형 표준 시험은 여전히 간명하고 유용한 진단 도구입니다. 예를 들어, 높은 MMLU 점수가 반드시 모형이 실제 사용에서 강력하다는 것을 의미하지는 않지만, 낮은 점수는 잠재적인 지식 격차를 강조할 수 있습니다. 또한, MMLU와 같은 정적 데이터셋은 시간이 지남에 따라 모델이 새로운 정보를 학습했는지 여부를 반영하지 못하며, 특정 시대의 지식만을 반영할 수 있다는 한계도 있습니다.

### 방법 2: 검증기(verifier)를 사용하여 답변 확인하기

앞서 다룬 다지선다형 질의응답과 궤를 같이하며, 검증기 활용 방식 또한 모델의 역량을 정답률 지표를 통해 수치적으로 파악합니다. 그러나 다지선다형 표준 시험과 달리, 검증 기법은 LLM이 자유 형식 답변(free-form answer)을 제시하도록 허용합니다. 그런 다음 관련 답변 부분을 추출하고, 아래 그림 6에 명시된 바와 같이 소위 검증기(verifier)를 사용하여 답변 부분을 데이터셋에 제공된 정답과 대조합니다.

**그림 6**: 자유 형식 질문 답변에서 검증 기반 방식의 LLM 평가 모습. 모형은 자유 형식 답변(여러 단계를 포함할 수 있음)과 최종 박스형 답변을 생성하며, 이는 추출되어 데이터셋의 정답과 대조됩니다.

위 그림에 표시된 바와 같이 추출된 답변을 제시된 답변과 대조할 때, 코드 인터프리터(code interpreter) 또는 계산기형 도구/소프트웨어(calculator-like tools/software)와 같은 외부 도구를 활용할 수 있습니다. 예를 들어, "물 1리터에 설탕 200g을 녹이면 몇 % 농도의 설탕물이 될까요?"와 같은 질문에 LLM이 답변하면, 검증기는 이 답변에서 수치와 단위를 추출한 뒤, 내장된 계산 엔진을 통해 정답을 도출하여 LLM의 답변과 비교할 수 있습니다. 이 기법의 한계점은 수학이나 프로그래밍 코드처럼 용이하게 (그리고 이상적으로는 확정적으로) 확인 가능한 특정 영역에만 적용 가능하다는 점입니다. 또한, 이 접근법은 추가적인 복잡성과 의존성을 도입할 수 있으며, 평가 부담의 일부를 모형 자체에서 외부 도구로 전가할 수 있습니다. 그러나 무제한의 수학 문제 변형을 프로그래밍 방식으로 생성할 수 있고 단계별 추론(step-by-step reasoning)의 이점을 얻을 수 있기 때문에, 이는 추론 모형 평가 및 개발의 핵심 기반이 되었습니다. 특히, 복잡한 과학 연구나 형식적 검증(formal verification) 분야에서 LLM의 추론 능력을 객관적으로 평가하는 데 중요한 역할을 합니다.

저는 저의 책 『Build a Reasoning Model (From Scratch)』에서 이 주제에 대해 35페이지에 걸쳐 포괄적으로 다루었으므로, 여기서는 코드 구현을 건너뛰겠습니다. (지난주에 해당 챕터를 제출했습니다. 얼리 액세스(early access) 버전을 가지고 계시다면, 해당 내용이 공개될 때 이메일을 받으실 것이며 그때 읽을 수 있을 것입니다. 그동안 단계별 코드는 [여기 GitHub](https://github.com/rasbt/reasoning-from-scratch/blob/main/ch02/02_verifier_evaluation/01_verifier_evaluation.ipynb)에서 찾을 수 있습니다.)

**그림 7**: [여기 GitHub](https://github.com/rasbt/reasoning-from-scratch/blob/main/ch02/02_verifier_evaluation/01_verifier_evaluation.ipynb)에서 제공되는 검증 기반 평가 접근 방식의 발췌본

### 방법 3: 선호도 및 순위표(leaderboard)를 활용하여 모형 비교하기

지금까지는 모형의 정답률과 같이 손쉽게 수치화 가능한 측정 기준을 제시하는 두 가지 방안을 살펴보았습니다. 하지만 이전 기법들은 응답의 문체 평가를 포함하여 LLM을 총체적인 관점에서 사정하는 데는 미흡합니다. 모형이 단순히 정확한 정보를 제공하는 것을 넘어, 얼마나 유창하게, 창의적으로, 그리고 사용자의 의도에 부합하게 소통하는지 파악하는 것은 단순한 수치로는 어렵습니다. 이 섹션에서는 아래 그림 8에 명시된 바와 같이 판단 기반 방식인 LLM 순위표(leaderboard)에 대해 논의합니다.

**그림 8**: 본 부록에서 다루는 판단 기반 및 표준 평가 기반 사정 방식에 초점을 맞춘 이 책에서 다루는 주제의 개념 모형.

이전 섹션에서 표준 평가 기반 접근법(다지선다형, 검증기)을 이미 다루었으므로, 이제 LLM의 역량을 측정하기 위한 판단 기반 접근법을 소개하며, 이 하위 섹션에서는 순위표(leaderboard)에 중점을 둡니다.

이 단락에서 소개할 순위표 기법은 모델들이 정답률이나 다른 고정된 표준 평가 점수가 아닌, 사용자(또는 다른 LLM)의 산출물에 대한 선호도에 기반하여 서열화되는 판정 중심의 접근법입니다. 이는 모델이 제공하는 정보의 정확성뿐만 아니라, 응답의 유용성, 명확성, 심지어는 '인간적인' 느낌까지도 포괄적으로 반영한다는 점에서 의미가 있습니다. 인기 있는 순위표는 LM 아레나(LM Arena, 이전 Chatbot Arena)로, 그림 9에 나타난 바와 같이 사용자가 두 개의 사용자 선택 또는 익명 모형의 응답을 비교하고 선호하는 모형에 투표합니다.

**그림 9**: 판단 기반 순위표(leaderboard) 인터페이스(LM 아레나)의 예시. 두 LLM에 동일한 프롬프트(prompt)가 주어지고, 그들의 응답이 나란히 표시되며, 사용자는 선호하는 답변에 투표합니다.

위 그림에 명시된 바와 같이 수집된 이러한 선호도 투표는 모든 사용자를 대상으로 집계되어 사용자 선호도에 따라 다양한 모형의 순위를 매기는 순위표(leaderboard)를 형성합니다. LM 아레나 순위표(leaderboard)의 현재 스냅샷(2025년 10월 3일 접근)은 아래 그림 10에 나와 있습니다.

**그림 10**: 텍스트 작업에 대한 사용자 선호도를 기반으로 현재 선두 LLM을 보여주는 LM 아레나 순위표(leaderboard) 스크린샷

이 섹션의 나머지 부분에서는 순위표(leaderboard)의 간명한 예시를 구현할 것입니다. 구체적인 예시를 만들기 위해, 그림 9와 유사한 환경에서 사용자가 다양한 LLM에 프롬프트(prompt)를 제공하는 상황을 고려해 봅시다. 아래 목록은 첫 번째 모형이 승자인 쌍별 투표(pairwise votes)를 나타냅니다.

```python
votes = [
    ("GPT-5", "Claude-3"),
    ("GPT-5", "Llama-4"),
    ("Claude-3", "Llama-3"),
    ("Llama-4", "Llama-3"),
    ("Claude-3", "Llama-3"),
    ("GPT-5", "Llama-3"),
]
```

위 목록에서 `votes` 목록의 각 튜플은 두 모형 간의 쌍별 선호도(pairwise preference)를 `(승자, 패자)` 형식으로 나타냅니다. 따라서 `("GPT-5", "Claude-3")`는 사용자가 Claude-3 모형의 답변보다 GPT-5를 선호했음을 의미합니다. 이 섹션의 나머지 부분에서는 `votes` 목록을 순위표(leaderboard)로 변환할 것입니다. 이를 위해 원래 체스 플레이어의 순위를 매기기 위해 고안된 인기 있는 엘로 레이팅 시스템(Elo rating system)을 사용할 것입니다. 구체적인 코드 구현을 살펴보기 전에, 간략하게 작동 방식은 다음과 같습니다. 각 모형은 기준 점수(baseline score)로 시작합니다. 그런 다음 각 비교 및 선호도 투표 후에 모형의 레이팅(rating)이 갱신됩니다. (엘로(Elo)에서는 갱신 규모가 결과의 놀라움 정도에 따라 달라집니다.) 특히, 사용자가 높은 순위의 모형보다 현재 모형을 선호하면, 현재 모형은 상대적으로 큰 순위 갱신을 받고 순위표(leaderboard)에서 더 높은 순위를 차지합니다. 반대로, 낮은 순위의 상대방에게 이기면 갱신은 더 작습니다. (그리고 현재 모형이 지면, 비슷한 방식으로 갱신되지만, 순위 점수가 추가되는 대신 차감됩니다.)

이러한 쌍별 순위를 순위표(leaderboard)로 변환하는 코드는 아래 코드 블록에 나와 있습니다.

**코드 블록 4**: 순위표(leaderboard) 구성하기

```python
def elo_ratings(vote_pairs, k_factor=32, initial_rating=1000):
    # Initialize all models with the same base rating
    ratings = {model: initial_rating for pair in vote_pairs for model in pair}

    # Update ratings after each match
    for winner, loser in vote_pairs:
        # Expected score for the current winner
        expected_winner = 1.0 / (
            1.0 + 10 ** ((ratings[loser] - ratings[winner]) / 400.0)
        )

        # k_factor determines sensitivity of updates
        ratings[winner] = (
            ratings[winner] + k_factor * (1 - expected_winner)
        )
        ratings[loser] = (
            ratings[loser] + k_factor * (0 - (1 - expected_winner))
        )
    return ratings
```

위에 정의된 `elo_ratings` 함수는 `votes`를 입력으로 받아 다음과 같이 순위표(leaderboard)로 변환합니다.

```python
ratings = elo_ratings(votes, k_factor=32, initial_rating=1000)
for model in sorted(ratings, key=ratings.get, reverse=True):
    print(f"{model:8s} : {ratings[model]:.1f}")
```

그 결과는 다음과 같은 순위표(leaderboard) 순위이며, 점수가 높을수록 좋습니다.

```
GPT-5    : 1043.7
Claude-3 : 1015.2
Llama-4  : 1000.7
Llama-3  : 940.4
```

그렇다면 이것은 어떻게 작동할까요? 각 쌍에 대해 다음 공식을 사용하여 승자의 예상 점수(expected score)를 계산합니다.

`expected_winner = 1 / (1 + 10 ** ((rating_loser - rating_winner) / 400))`

이 `expected_winner` 값은 현재 레이팅(rating)을 기반으로 무승부가 없는 상황에서 모형이 이길 것으로 예상되는 확률입니다. 이는 레이팅(rating) 갱신의 크기를 결정합니다.

먼저, 각 모형은 `initial_rating = 1000`으로 시작합니다. 두 레이팅(rating)(승자와 패자)이 같으면 `expected_winner = 0.5`가 되며, 이는 동등한 경기를 나타냅니다. 이 경우 갱신은 다음과 같습니다.

`rating_winner + k_factor * (1 - 0.5) = rating_winner + 16`
`rating_loser + k_factor * (0 - (1 - 0.5)) = rating_loser - 16`

이제, 강력한 우승 후보(높은 레이팅(rating)을 가진 모형)가 이기면 `expected_winner ≈ 1`이 됩니다. 우승 후보는 소량만 얻고 패자는 소량만 잃습니다.

`rating_winner + 32 * (1 - 0.99) = rating_winner + 0.32`
`rating_loser + 32 * (0 - (1 - 0.99)) = rating_loser - 0.32`

그러나 약자(낮은 레이팅(rating)을 가진 모형)가 이기면 `expected_winner ≈ 0`이 되며, 승자는 거의 전체 `k_factor` 점수를 얻고 패자는 거의 동일한 크기를 잃습니다.

`rating_winner + 32 * (1 - 0.01) = rating_winner + 31.68`
`rating_loser + 32 * (0 - (1 - 0.01)) = rating_loser - 31.68`

#### 순서의 중요성

엘로(Elo) 접근 방식은 각 경기(모형 비교) 후에 레이팅(rating)을 갱신하므로, 나중의 결과는 이미 갱신된 레이팅(rating)을 기반으로 합니다. 이는 동일한 결과 집합이라도 다른 순서로 제시되면 최종 점수가 미세하게 달라질 수 있음을 의미합니다. 이러한 효과는 일반적으로 경미하지만, 특히 이변이 일찍 발생했는지 늦게 발생했는지에 따라 발생할 수 있습니다. 이러한 순서 효과를 줄이기 위해, 투표 쌍을 뒤섞고 `elo_ratings` 함수를 여러 차례 실행하여 레이팅(rating)을 평균화하는 것이 일반적입니다.

위에 설명된 것과 같은 순위표(leaderboard) 접근 방식은 정적 표준 평가 점수보다 모형 품질에 대한 더 동적인 시각을 제공합니다. 그러나 결과는 사용자 인구 통계, 프롬프트(prompt) 선택 및 투표 편향에 의해 영향을 받을 수 있습니다. 순위표(leaderboard)는 조작될 수도 있으며, 사용자는 정확성보다는 스타일에 따라 응답을 선택할 수 있습니다. 또한, 사용자 풀(pool)이 비정상적으로 변화하거나 특정 모델에 대한 편향된 관심이 집중될 경우, 순위의 신뢰도가 흔들릴 수 있습니다. 마지막으로, 자동화된 표준 평가 하네스(automated benchmark harnesses)와 비교할 때, 순위표(leaderboard)는 새로 개발된 변형에 대한 즉각적인 피드백을 제공하지 않으므로 활발한 모형 개발 중에 활용하기 어렵습니다.

#### 다른 순위 매기기 방식

LM 아레나(LM Arena)는 원래 이 섹션에서 설명된 엘로(Elo) 방식을 사용했지만, 최근 브래들리-테리 모델(Bradley–Terry model)을 기반으로 하는 통계적 접근 방식으로 전환했습니다. 브래들리-테리 모델(Bradley–Terry model)의 핵심 이점은 통계적으로 근거가 있기 때문에 순위의 불확실성을 표현하기 위한 신뢰 구간(confidence intervals)을 구성할 수 있다는 것입니다. 또한, 엘로 레이팅(Elo ratings)과 달리 브래들리-테리 모델(Bradley–Terry model)은 전체 데이터셋(dataset)에 대한 통계적 적합(statistical fit)을 사용하여 모든 레이팅(rating)을 공동으로 추정하므로 순서 효과에 영향을 받지 않습니다. 보고된 점수를 익숙한 범위로 유지하기 위해 브래들리-테리 모델(Bradley–Terry model)은 엘로(Elo)와 유사한 값을 생성하도록 적합됩니다. 순위표(leaderboard)가 더 이상 공식적으로 엘로 레이팅(Elo ratings)을 사용하지 않더라도, "엘로(Elo)"라는 용어는 모형을 비교할 때 LLM 연구자와 실무자들 사이에서 널리 통용되고 있습니다. 엘로 레이팅(Elo rating)을 보여주는 코드 예시는 [여기 GitHub](https://github.com/rasbt/reasoning-from-scratch/blob/main/ch02/03_leaderboard_evaluation/01_elo_ratings.ipynb)에서 확인할 수 있습니다.

**그림 11**: 엘로(Elo) 및 브래들리-테리(Bradley-Terry) 순위 비교; 원본 코드(source code)는 [여기 GitHub](https://github.com/rasbt/reasoning-from-scratch/blob/main/ch02/03_leaderboard_evaluation/02_bradley_terry_ratings.ipynb)에서 확인할 수 있습니다.

### 방법 4: 다른 LLM으로 응답 평가하기

초창기에는 대규모 언어 모델의 역량 측정에 BLEU라는 지표를 포함한 통계적 및 경험적 방식들이 활용되었습니다. BLEU는 생성된 글이 기준이 되는 원문과 얼마나 부합하는지를 어림짐작하는 척도입니다. 이러한 척도들의 맹점은 엄밀한 어휘 일치를 요구하며, 유의어(synonyms)나 문구 변형 등을 전혀 감안하지 않는다는 점입니다. 이로 인해 유창하고 의미적으로 정확한 답변이라도 단어 선택이 다르면 낮은 점수를 받을 수 있는 문제가 있었습니다.

이 문제에 대한 한 가지 해결책은, 작성된 답변 텍스트를 총체적으로 판별하고 싶다면, 이전 섹션에서 논의된 상대적 순위 및 순위표(leaderboard) 기반 접근법을 사용하는 것입니다. 그러나 순위표(leaderboard)의 단점은 인간의 피드백(및 이 피드백을 수집하는 데 따르는 어려움)을 포함하므로 선호도 기반 비교의 주관적인 특성입니다. 인간 평가자는 피로도, 개인적인 선호, 심지어는 특정 모델에 대한 편견 등으로 인해 일관성 없는 판단을 내릴 수 있습니다.

관련된 방법은 미리 정의된 채점 기준표(grading rubric)(즉, 평가 가이드)를 가진 다른 LLM을 사용하여 LLM의 응답을 참조 응답과 비교하고, 그림 12에 설명된 바와 같이 미리 정의된 기준표에 따라 응답 품질을 판단하는 것입니다. 이는 인간 평가의 주관성과 비효율성을 해소하면서도, BLEU와 같은 단순 매칭 지표의 한계를 극복하려는 시도입니다.

**그림 F12**: LLM 심사위원(LLM-judge) 평가의 예시. 평가할 모델이 답변을 생성하면, 별도의 심사위원 LLM이 기준표와 제공된 참조 답변에 따라 점수를 매깁니다.

실제로 그림 12에 나타난 심사위원 기반 접근법은 심사위원 LLM이 강력할 때 탁월하게 작동합니다. 일반적인 설정은 API(예: GPT-5 API)를 통해 선도적인 독점 LLM(proprietary LLMs)을 사용하지만, 전문 심사 모형(specialized judge models)도 존재합니다. (예를 들어, 많은 예시 중 하나는 Phudge입니다. 궁극적으로 이러한 전문 모형의 대부분은 독점 GPT 모형과 유사한 점수 매기기 동작을 갖도록 미세 조정(fine-tuned)된 더 작은 모형일 뿐입니다.) 심사자 역할의 모델이 효과적인 까닭 중 하나는, 응답을 판별하는 행위가 종종 응답을 산출하는 것보다 수월하기 때문입니다. 이는 평가 기준이 명확할 경우, 강력한 LLM이 인간과 유사하거나 더 높은 일관성을 가지고 판단을 내릴 수 있음을 시사합니다. 또한, LLM 심사위원은 특정 가치나 윤리적 기준에 맞춰 평가하도록 훈련될 수 있어, AI 정렬(AI alignment) 분야에서도 중요한 역할을 합니다.

그림 12에 제시된 심사위원 기반 모형 평가를 파이썬(Python)으로 프로그래밍 방식으로 구현하려면, 파이토치(PyTorch)에서 더 큰 Qwen3 모형 중 하나를 불러오고 채점 기준표(grading rubric)와 평가하려는 모형 답변으로 프롬프트(prompt)를 제공할 수 있습니다. 또는 ChatGPT 또는 Ollama API와 같은 다른 LLM을 API를 통해 사용할 수 있습니다. Qwen3 모형을 파이토치(PyTorch)로 불러오는 방법을 이미 알고 있으므로, 더 흥미롭게 만들기 위해 이 섹션의 나머지 부분에서는 파이썬(Python)의 Ollama API를 사용하여 그림 12에 나타난 심사위원 기반 평가를 구현할 것입니다. 특히, 기능과 효율성 사이의 좋은 균형을 제공하는 OpenAI의 200억 매개변수 gpt-oss 오픈 웨이트 모형(gpt-oss open-weight model)을 사용할 것입니다. gpt-oss에 대한 자세한 내용은 저의 [From GPT-2 to gpt-oss: Analyzing the Architectural Advances](https://magazine.sebastianraschka.com/p/from-gpt-2-to-gpt-oss-analyzing) 글을 참조하십시오.

**From GPT-2 to gpt-oss: Analyzing the Architectural Advances**
Sebastian Raschka, PhD · 8월 9일
전체 글 읽기

#### 4.1 Ollama에서 LLM-as-a-judge 접근 방식 구현하기

Ollama는 노트북에서 LLM을 구동하기 위한 효율적인 오픈 소스 애플리케이션입니다. 이는 효율성을 극대화하기 위해 순수 C/C++로 LLM을 구현하는 오픈 소스 llama.cpp 라이브러리(llama.cpp library)의 래퍼(wrapper) 역할을 합니다. 그러나 Ollama는 LLM을 사용하여 텍스트를 생성(추론(inference))하는 도구일 뿐이며 LLM 훈련 또는 미세 조정(training or fine-tuning)을 지원하지 않습니다.

다음 코드를 실행하려면, [https://ollama.com](https://ollama.com)의 공식 웹사이트를 방문하여 운영 체제에 제공된 지침에 따라 Ollama를 설치하십시오.

*   macOS 및 Windows 사용자: 다운로드한 Ollama 애플리케이션을 엽니다. 명령줄 사용(command-line usage)을 설치하라는 메시지가 표시되면 "예"를 선택합니다.
*   Linux 사용자: Ollama 웹사이트에서 제공되는 설치 명령을 사용합니다.

모형 평가 코드를 구현하기 전에, 먼저 gpt-oss 모형을 다운로드하고 명령줄 터미널(command-line terminal)에서 Ollama가 올바르게 작동하는지 확인해 보겠습니다. 다음 명령을 명령줄(파이썬 세션 아님)에서 실행하여 200억 매개변수 gpt-oss 모형을 사용해 보십시오.

`ollama run gpt-oss:20b`

이 명령을 처음 실행하면 14GB의 저장 공간(storage space)을 차지하는 200억 매개변수 gpt-oss 모형이 자동으로 다운로드됩니다. 출력은 다음과 같습니다.

```
$ ollama run gpt-oss:20b
pulling manifest
pulling b112e727c6f1: 100% ???????????????????????? 13 GB
pulling fa6710a93d78: 100% ???????????????????????? 7.2 KB
pulling f60356777647: 100% ???????????????????????? 11 KB
pulling d8ba2f9a17b3: 100% ???????????????????????? 18 B
pulling 55c108d8e936: 100% ???????????????????????? 489 B
verifying sha256 digest
writing manifest
removing unused layers
success
```

#### 대체 Ollama 모형

`ollama run gpt-oss:20b` 명령의 `gpt-oss:20b`는 200억 매개변수 gpt-oss 모형을 나타냅니다. `gpt-oss:20b` 모형과 함께 Ollama를 사용하려면 약 13GB의 램(RAM)이 필요합니다. 컴퓨터에 충분한 램(RAM)이 없다면, `ollama run qwen3:4b`를 통해 약 4GB의 램(RAM)만 필요한 40억 매개변수 `qwen3:4b` 모형과 같은 더 작은 모형을 사용해 볼 수 있습니다. 더 강력한 컴퓨터의 경우, `gpt-oss:20b`를 `gpt-oss:120b`로 교체하여 더 큰 1200억 매개변수 gpt-oss 모형을 사용할 수도 있습니다. 그러나 이 모형은 훨씬 더 많은 컴퓨팅 자원(computational resources)을 필요로 한다는 점을 명심하십시오.

모형 다운로드가 완료되면, 모형과 상호 작용할 수 있는 명령줄 인터페이스(command-line interface)가 나타납니다. 예를 들어, 모형에 "1+2는 무엇인가요?"라고 물어보십시오.

```
>>> What is 1+2?
Thinking...
User asks: “What is 1+2?” This is simple: answer 3. Provide explanation? Possibly ask for simple arithmetic. Provide answer: 3. ...done thinking.
1 + 2 = **3**
```

`/bye`를 입력하여 이 `ollama run gpt-oss:20b` 세션을 종료할 수 있습니다.

이 섹션의 나머지 부분에서는 Ollama API를 사용할 것입니다. 이 접근 방식은 Ollama가 백그라운드(background)에서 실행되고 있어야 합니다. 이를 달성하는 세 가지 다른 옵션이 있습니다.

1.  터미널에서 `ollama serve` 명령을 실행합니다(권장). 이는 Ollama 백엔드(backend)를 서버로 실행하며, 일반적으로 `http://localhost:11434`에서 실행됩니다. API를 통해 호출될 때까지(이 섹션의 나중에) 모형을 로드하지 않는다는 점에 유의하십시오.
2.  이전과 유사하게 `ollama run gpt-oss:20b` 명령을 실행하되, 열어두고 `/bye`를 통해 세션을 종료하지 마십시오. 앞에서 논의했듯이, 이는 로컬 Ollama 서버 주변에 최소한의 편의 래퍼(wrapper)를 엽니다. 내부적으로는 `ollama serve`와 동일한 서버 API를 사용합니다.
3.  Ollama 데스크톱 앱. 데스크톱 앱을 열면 동일한 백엔드(backend)가 자동으로 실행되고, 앞서 그림 12에 표시된 바와 같이 그 위에 그래픽 인터페이스가 제공됩니다.

**그림 13**: 파이썬(Python)에서 Ollama API를 통해 활용할 수 있도록 Ollama 서버(애플리케이션)를 실행 상태로 유지하는 두 가지 상이한 옵션.

#### Ollama 서버 IP 주소

Ollama는 로컬 서버와 같은 프로세스를 시작하여 우리 컴퓨터에서 로컬로 실행됩니다. 위에서 설명한 대로 터미널에서 `ollama serve`를 실행할 때, `Error: listen tcp 127.0.0.1:11434: bind: address already in use`와 같은 오류 메시지가 발생할 수 있습니다. 이 경우, `OLLAMA_HOST=127.0.0.1:11435 ollama serve` 명령을 사용해 보십시오(그리고 이 주소도 사용 중이라면, 사용 중이 아닌 주소를 찾을 때까지 숫자를 1씩 늘려 보십시오).

다음 코드는 이전 섹션에서 생성된 테스트 세트 응답을 평가하기 위해 Ollama를 사용하기 전에 Ollama 세션이 제대로 실행되고 있는지 확인합니다.

**코드 블록 5**: Ollama가 실행 중인지 확인하기

```python
import psutil

def check_if_running(process_name):
    running = False
    for proc in psutil.process_iter(["name"]):
        if process_name in proc.info["name"]:
            running = True
            break
    return running

ollama_running = check_if_running("ollama")
if not ollama_running:
    raise RuntimeError(
        "Ollama not running. "
        "Launch ollama before proceeding."
    )
print("Ollama running:", check_if_running("ollama"))
```

이전 코드를 실행한 결과가 `Ollama running: True`를 표시하는지 확인하십시오. `False`를 표시하면, `ollama serve` 명령 또는 Ollama 애플리케이션이 활발하게 실행 중인지 확인하십시오(그림 13 참조).

이 글의 나머지 부분에서는 파이썬(Python)을 사용하여 Ollama REST API를 통해 우리 컴퓨터에서 실행되는 로컬 gpt-oss 모형과 상호 작용할 것입니다. 다음 `query_model` 함수는 API를 사용하는 방법을 보여줍니다.

**코드 블록 6**: 로컬 Ollama 모형 쿼리하기

```python
import json
import urllib.request

def query_model(
    prompt,
    model="gpt-oss:20b",
    # If you used
    # OLLAMA_HOST=127.0.0.1:11435 ollama serve
    # update the address below
    url="http://localhost:11434/api/chat"
):
    # Create the data payload as a dictionary:
    data = {
        "model": model,
        "messages": [
            {"role": "user", "content": prompt}
        ],
        # Settings required for deterministic responses:
        "options": {
            "seed": 123,
            "temperature": 0,
            "num_ctx": 2048
        }
    }

    # Convert the dictionary to JSON and encode it to bytes
    payload = json.dumps(data).encode("utf-8")

    # Create a POST request and add headers
    request = urllib.request.Request(
        url,
        data=payload,
        method="POST"
    )
    request.add_header("Content-Type", "application/json")

    response_data = ""
    # Send the request and capture the streaming response
    with urllib.request.urlopen(request) as response:
        while True:
            line = response.readline().decode("utf-8")
            if not line:
                break
            # Parse each line into JSON
            response_json = json.loads(line)
            response_data += response_json["message"]["content"]
    return response_data
```

방금 구현한 `query_model` 함수를 사용하는 예시는 다음과 같습니다.

```python
ollama_model = "gpt-oss:20b"
result = query_model("What is 1+2?", ollama_model)
print(result)
```

결과 응답은 "3"입니다. (기본 설정(default settings)이 다르기 때문에 Ollama `run` 또는 Ollama 애플리케이션을 실행했을 때 얻는 것과 다릅니다.)

`query_model` 함수를 사용하여, gpt-oss 모형이 참조로서의 정답을 기반으로 우리 목표 모형의 응답을 1에서 5까지의 척도로 평가하도록 요청하는 채점 기준표(grading rubric)를 포함하는 프롬프트(prompt)로 모형이 생성한 응답을 평가할 수 있습니다. 이를 위해 사용하는 프롬프트(prompt)는 아래에 나와 있습니다.

**코드 블록 7**: 채점 기준표(grading rubric)를 포함한 프롬프트 템플릿 설정하기

```python
def rubric_prompt(instruction, reference_answer, model_answer):
    rubric = (
        "You are a fair judge assistant. You will be "
        "given an instruction, a reference answer, and "
        "a candidate answer to evaluate, according to "
        "the following rubric:\n\n"
        "1: The response fails to address the "
        "instruction, providing irrelevant, incorrect, "
        "or excessively verbose content.\n"
        "2: The response partially addresses the "
        "instruction but contains major errors, "
        "omissions, or irrelevant details.\n"
        "3: The response addresses the instruction to "
        "some degree but is incomplete, partially "
        "correct, or unclear in places.\n"
        "4: The response mostly adheres to the "
        "instruction, with only minor errors, "
        "omissions, or lack of clarity.\n"
        "5: The response fully adheres to the "
        "instruction, providing a clear, accurate, and "
        "relevant answer in a concise and efficient "
        "manner.\n\n"
        "Now here is the instruction, the reference "
        "answer, and the response.\n"
    )
    prompt = (
        f"{rubric}\n"
        f"Instruction:\n{instruction}\n\n"
        f"Reference Answer:\n{reference_answer}\n\n"
        f"Answer:\n{model_answer}\n\n"
        f"Evaluation: "
    )
    return prompt
```

`rubric_prompt`의 `model_answer`는 실제로 우리 모형이 생성한 응답을 나타내기 위한 것입니다. 설명 목적으로, 여기서는 동적으로 생성하는 대신 그럴듯한 모형 답변(model answer)을 하드코딩합니다. (그러나 이 글 초반에 로드한 Qwen3 모형을 사용하여 실제 `model_answer`를 생성해도 좋습니다.)

다음으로, Ollama 모형을 위한 렌더링된 프롬프트(rendered prompt)를 생성해 보겠습니다.

```python
rendered_prompt = rubric_prompt(
    instruction=(
        "If all birds can fly, and a penguin is a bird, "
        "can a penguin fly?"
    ),
    reference_answer=(
        "Yes, according to the premise that all birds can fly, "
        "a penguin can fly."
    ),
    model_answer=(
        "Yes – under those premises a penguin would be able to fly."
    )
)
print(rendered_prompt)
```

출력은 다음과 같습니다.

```
You are a fair judge assistant. You will be given an instruction, a reference answer, and a candidate answer to evaluate, according to the following rubric:

1: The response fails to address the instruction, providing irrelevant, incorrect, or excessively verbose content.
2: The response partially addresses the instruction but contains major errors, omissions, or irrelevant details.
3: The response addresses the instruction to some degree but is incomplete, partially correct, or unclear in places.
4: The response mostly adheres to the instruction, with only minor errors, omissions, or lack of clarity.
5: The response fully adheres to the instruction, providing a clear, accurate, and relevant answer in a concise and efficient manner.

Now here is the instruction, the reference answer, and the response.

Instruction:
If all birds can fly, and a penguin is a bird, can a penguin fly?

Reference Answer:
Yes, according to the premise that all birds can fly, a penguin can fly.

Answer:
Yes – under those premises a penguin would be able to fly.

Evaluation:
```

프롬프트(prompt)를 "Evaluation: "으로 마무리하는 것은 모형이 답변을 생성하도록 유도합니다. gpt-oss:20b 모형이 응답을 어떻게 판별하는지 살펴보겠습니다.

```python
result = query_model(rendered_prompt, ollama_model)
print(result)
```

응답은 다음과 같습니다.

```
**Score: 5**
The candidate answer directly addresses the question, correctly applies the given premises, and concisely states that a penguin would be able to fly. It is accurate, relevant, and clear.
```

보시다시피, 답변은 최고 점수를 받았습니다. 이는 실제로 정확하기 때문에 합리적입니다. 이것은 과정을 수동으로 진행하는 간명한 예시였지만, 이 아이디어를 더 발전시켜 평가 데이터셋(evaluation dataset)의 질문으로 모형(예: 이전에 로드한 Qwen3 모형)을 반복적으로 쿼리하고 gpt-oss를 통해 평가하며 평균 점수를 산정하는 for-루프(for-loop)를 구현할 수 있습니다. MATH-500 데이터셋(MATH-500 dataset)에서 Qwen3 모형을 평가하는 이러한 스크립트의 구현은 [여기 GitHub](https://github.com/rasbt/reasoning-from-scratch/blob/main/ch02/04_llm_as_judge_evaluation/01_llm_as_judge_evaluation.ipynb)에서 찾을 수 있습니다.

**그림 14**: 심사위원(judge)으로서 gpt-oss:20b가 평가한 MATH-500의 첫 10개 예시에 대한 Qwen3 0.6 기본 및 추론 변형의 비교. 코드는 [여기 GitHub](https://github.com/rasbt/reasoning-from-scratch/blob/main/ch02/04_llm_as_judge_evaluation/01_llm_as_judge_evaluation.ipynb)에서 찾을 수 있습니다.

#### 프로세스 보상 모형(process reward models)으로 중간 추론 단계 점수 매기기

기호 검증기(symbolic verifier) 및 LLM 심사위원(LLM judges)과 관련하여, 프로세스 보상 모형(process reward models, PRMs)이라는 학습된 모형(learned models) 부류가 있습니다. 심사위원(judge)과 마찬가지로, PRM은 최종 답변을 넘어 추론 경로(reasoning traces)를 평가할 수 있지만, 일반적인 심사위원(judge)과 달리 추론의 중간 단계에 특별히 초점을 맞춥니다. 그리고 정확성을 기호적으로(symbolically) 그리고 일반적으로 결과 수준에서만 확인하는 검증기(verifier)와 달리, PRM은 강화 학습(reinforcement learning) 훈련 중에 단계별 보상 신호(step-by-step reward signals)를 제공합니다. PRM은 주로 훈련용으로 개발된 "단계별 심사위원(step-level judges)"으로 분류할 수 있으며, 순수한 평가용은 아닙니다. (실제로 PRM은 대규모로 안정적으로 훈련하기 어렵습니다. 예를 들어, DeepSeek R1은 PRM을 채택하지 않고 대신 추론 훈련을 위해 검증기(verifier)를 결합했습니다.)

심사위원 기반 평가는 대규모 인간 투표자 풀에 의존하지 않으므로 확장성 및 일관성을 포함하여 선호도 기반 순위표(leaderboard)에 비해 장점을 제공합니다. (기술적으로는 순위표(leaderboard) 뒤의 선호도 기반 평가를 LLM 심사위원(LLM judges)에게도 아웃소싱할 수 있습니다.) 그러나 LLM 심사위원(LLM judges)도 인간 투표자와 유사한 약점을 공유합니다. 결과는 모형 선호도, 프롬프트(prompt) 디자인 및 답변 스타일에 의해 편향될 수 있습니다. 심지어 심사위원 LLM 자체의 훈련 데이터에 내재된 편향이 평가 결과에 반영될 수도 있습니다. 또한, 심사위원 모형 및 기준표(rubric) 선택에 대한 강한 의존성이 있으며, 고정된 표준 평가(benchmark)의 재현성이 부족합니다.

### 결론

본 문헌에서는 다지선다형, 검증자, 순위표, 그리고 LLM 심판 모델이라는 네 가지 상이한 측정 방법을 상세히 조명했습니다. 이 글이 다소 길었지만, LLM이 어떻게 측정되는지에 대한 포괄적인 이해를 제공하는 데 유용했기를 바랍니다. 이처럼 기초부터 시작하는 방식은 다소 길게 느껴질 수 있으나, 각 기법의 내재된 작동 원리를 파악하는 데 효과적이며, 이는 곧 취약점과 발전 가능 지점을 찾아내는 데 일조합니다. 마치 의사가 여러 진단 도구를 사용하여 환자의 상태를 종합적으로 파악하는 것처럼, LLM 평가 또한 다양한 렌즈를 통해 이루어져야 합니다.

그렇다면 "LLM을 평가하는 가장 좋은 방안은 무엇일까요?"라고 의문을 가지실 것입니다. 불행히도, 우리가 살펴보았듯이 각각 다른 강점과 약점을 지니고 있기 때문에 단 하나의 최적의 기법은 존재하지 않습니다. 요약하자면:

*   **객관식(Multiple-choice)**
    *   (+) 대규모로 실행하기에 비교적 빠르고 경제적입니다.
    *   (+) 논문(또는 모형 카드) 전반에 걸쳐 표준화되고 재현 가능합니다.
    *   (-) 기본적인 지식 회상 능력을 측정합니다.
    *   (-) LLM이 실제 세계에서 활용되는 방식을 온전히 반영하지 못합니다.
    *   (-) 정적 데이터셋의 한계로 인해 최신 정보 반영에 취약합니다.
*   **검증기(Verifiers)**
    *   (+) 정답이 명확한 도메인에 대한 표준화되고 객관적인 채점입니다.
    *   (+) 자유 형식 답변(free-form answers)을 허용합니다(최종 답변 형식에 일부 제약이 있음).
    *   (+) 프로세스 검증기(process verifiers) 또는 프로세스 보상 모형(process reward models)을 사용하는 경우 중간 단계도 점수 매길 수 있습니다.
    *   (-) 검증 가능한 도메인(예: 수학 또는 코드)이 필요하며, 좋은 검증기를 구축하는 것이 까다로울 수 있습니다.
    *   (-) 결과 전용 검증기(outcome-only verifiers)는 추론 품질이 아닌 최종 답변만 평가합니다.
    *   (-) 외부 도구에 대한 의존성이 평가의 복잡성을 증가시킬 수 있습니다.
*   **아레나 스타일 순위표(Arena-style leaderboards) (인간 쌍별 선호도)**
    *   (+) 실제 프롬프트(prompt)에 대해 "사람들이 어떤 모형을 선호하는가?"에 직접적으로 답변합니다.
    *   (+) 자유 형식 답변(free-form answers)을 허용하고 스타일, 유용성 및 안전성을 암묵적으로 고려합니다.
    *   (-) 인간에게 비용이 많이 들고 시간이 많이 소요됩니다.
    *   (-) 정확성을 측정하지 않고 선호도만 측정합니다.
    *   (-) 비정상 모집단(nonstationary populations)이 안정성에 영향을 미칠 수 있습니다.
    *   (-) 특정 모델에 대한 편향이나 유행에 따라 결과가 왜곡될 가능성이 있습니다.
*   **LLM 심사위원(LLM-as-a-judge)**
    *   (+) 많은 작업에 걸쳐 확장 가능합니다.
    *   (+) 자유 형식 답변(free-form answers)을 허용합니다.
    *   (-) 심사위원의 능력에 따라 달라집니다(앙상블(ensemble)은 이를 더 견고하게 만들 수 있습니다).
    *   (-) 기준표(rubric) 선택에 따라 달라집니다.
    *   (-) 심사위원 LLM 자체의 편향이 평가에 반영될 수 있으며, 윤리적 문제가 제기될 수 있습니다.

저는 일반적으로 레이더 차트(radar chart)를 그다지 좋아하지 않지만, 아래에 제시된 바와 같이 LLM을 평가할 때 강점과 약점을 식별하기 위해 다양한 영역에 주의를 기울이는 것이 이상적이라는 개념을 시각화하는 데 도움이 될 수 있습니다.

**그림 15**: LLM을 평가할 때 강점과 약점을 식별하기 위해 이상적으로는 다양한 영역에 주의를 기울여야 한다는 개념을 보여주는 레이더 차트(radar chart).

예를 들어, 높은 객관식 점수는 모형이 견고한 일반 지식을 가지고 있음을 시사합니다. 여기에 강력한 검증기 점수를 결합하면 모형이 기술적인 질문에도 올바르게 답변할 가능성이 높습니다. 그러나 모형이 LLM-as-a-judge 및 순위표(leaderboard) 평가에서 저조한 성능을 보인다면, 응답을 효과적으로 작성하거나 명확하게 표현하는 데 어려움을 겪을 수 있으며 일부 RLHF의 이점을 얻을 수 있습니다. 이는 단순히 지식을 아는 것과 그 지식을 효과적으로 전달하고 사회적 맥락에 맞게 사용하는 능력 사이에 차이가 있음을 보여줍니다.

따라서 가장 우수한 성능 측정은 다양한 분야를 아우릅니다. 하지만 이상적으로는 특정 목표나 사업적 과제와 직접적으로 연관된 자료를 활용해야 합니다. 예를 들어, 법률 또는 법률 관련 작업을 지원하기 위해 LLM을 구현한다고 가정해 봅시다. 빠른 건전성 검사(sanity check)로 MMLU와 같은 표준 표준 평가(benchmark)에서 모형을 실행하는 것이 합리적이지만, 궁극적으로는 법률과 같은 목표 도메인에 맞게 평가를 조정해야 할 것입니다. 온라인에서 좋은 시작점이 될 수 있는 공개 표준 평가(benchmark)를