우리는 실제로 대규모 언어 모델(LLM)을 어떻게 평가할까요? 간단한 질문이지만, 훨씬 더 큰 논의를 불러일으키는 경향이 있습니다. 프로젝트에 자문하거나 협력할 때, 제가 가장 자주 받는 질문 중 하나는 다양한 모델 중에서 어떻게 선택해야 하는지, 그리고 외부의 평가 결과를 어떻게 이해해야 하는지에 대한 것입니다. (물론, 자체 모델을 미세 조정(fine-tuning)하거나 개발할 때 진행 상황을 측정하는 방법도 포함됩니다.) 이 질문이 자주 나오기 때문에, 사람들이 LLM을 비교하기 위해 사용하는 주요 평가 방법에 대한 간략한 개요를 공유하는 것이 도움이 될 것이라고 생각했습니다. 물론, LLM 평가는 단일 자료로 모두 다룰 수 없는 매우 큰 주제이지만, 이러한 주요 접근 방식에 대한 명확한 개념 지도를 가지고 있으면 벤치마크(benchmark), 리더보드(leaderboard) 및 논문을 해석하는 것이 훨씬 쉬워진다고 생각합니다. 원래 이 평가 기법들을 곧 출간될 저의 책 『Build a Reasoning Model (From Scratch)』에 포함할 계획이었지만, 주된 범위에서 약간 벗어나는 내용이었습니다. (책 자체는 검증기 기반 평가(verifier-based evaluation)에 더 중점을 둡니다.) 그래서 이 내용을 처음부터 작성된 코드 예제와 함께 더 긴 기사로 공유하는 것이 좋겠다고 생각했습니다. 『Build A Reasoning Model (From Scratch)』에서는 추론 LLM(reasoning LLM)을 처음부터 구축하는 실습 위주의 접근 방식을 취하고 있습니다. 『Build A Large Language Model (From Scratch)』을 좋아하셨다면, 이 책은 순수 파이토치(PyTorch)로 모든 것을 처음부터 구축하는 면에서 비슷한 스타일로 작성되었습니다. 추론(Reasoning)은 LLM을 개선하는 데 있어 가장 흥미롭고 중요한 최근 발전 중 하나이지만, 추론이라는 용어만 듣고 이론적으로만 읽으면 가장 오해하기 쉬운 개념 중 하나이기도 합니다. 따라서 이 책에서는 추론 LLM(reasoning LLM)을 처음부터 구축하는 실습 위주의 접근 방식을 취하고 있습니다. 이 책은 현재 얼리 액세스(early-access) 중이며, 이미 100페이지 이상이 온라인에 공개되어 있고, 레이아웃 팀에서 현재 추가하고 있는 30페이지를 막 마쳤습니다. 얼리 액세스 프로그램에 참여하셨다면(지원해 주셔서 정말 감사합니다!), 해당 내용이 공개될 때 이메일을 받으실 것입니다. 추신: 현재 LLM 연구 분야에서 많은 일이 일어나고 있습니다. 저는 북마크된 논문 목록을 따라잡고 있으며, 다음 기사에서 가장 흥미로운 몇 가지를 강조할 계획입니다. 하지만 이제, 네 가지 주요 LLM 평가 방법과 그 장단점을 더 잘 이해하기 위한 처음부터 작성된 코드 구현에 대해 논의해 보겠습니다.

### LLM의 주요 평가 방법 이해하기

대규모 언어 모델(LLM)의 평가는 단순한 성능 측정 그 이상입니다. 이는 모델의 강점과 약점을 파악하고, 실제 사용 사례에 대한 적합성을 판단하며, 지속적인 개선을 위한 방향을 제시하는 복잡하고 다면적인 과정입니다. LLM은 단순한 지식 회상을 넘어 추론, 창의성, 대화 능력 등 다양한 기능을 수행하므로, 평가 방법 또한 이러한 복합적인 능력을 포괄해야 합니다. 이 글에서는 LLM 평가의 네 가지 주요 접근 방식인 **객관식(multiple choice)**, **검증기(verifier)**, **리더보드(leaderboard)**, 그리고 **LLM 심사위원(LLM judges)**에 대해 심층적으로 다룰 것입니다.

이 네 가지 범주는 크게 **벤치마크 기반 평가(benchmark-based evaluation)**와 **판단 기반 평가(judgment-based evaluation)**로 나눌 수 있습니다. 벤치마크 기반 평가는 주로 정량적인 지표를 통해 모델의 능력을 측정하는 반면, 판단 기반 평가는 인간의 선호도나 다른 LLM의 평가를 통해 보다 주관적이고 질적인 측면을 다룹니다. 또한, 최근에는 모델의 동적인 특성, 환각(hallucination) 문제, 안전성 및 편향성 문제를 평가하는 중요성이 더욱 커지고 있습니다. 다음 하위 섹션에서는 각 평가 방법의 원리, 코드 예시, 그리고 장단점을 자세히 살펴보겠습니다.

### 방법 1: 답변 선택 정확도 평가하기

벤치마크 기반 방법인 객관식 질문 답변(multiple-choice question answering)은 LLM의 지식과 기본적인 추론 능력을 평가하는 데 널리 사용됩니다. MMLU(Massive Multitask Language Understanding) 외에도 HellaSwag, ARC(AI2 Reasoning Challenge), CommonsenseQA와 같은 다양한 객관식 벤치마크가 존재하며, 각각 다른 종류의 추론 능력(상식, 상식적 추론, 고급 추론)을 측정합니다. 예를 들어, ARC는 초등 과학 질문에 대한 추론 능력을 평가하며, MMLU와 유사하게 모델의 예측된 답변이 정답과 일치하는지 확인하여 정확도를 측정합니다.

객관식 벤치마크는 표준화된 시험과 유사하게 LLM의 지식 회상 능력을 직관적이고 정량적인 방식으로 테스트합니다. 그러나 이러한 벤치마크는 몇 가지 한계를 가집니다.

*   **데이터 오염 (Data Contamination):** LLM이 훈련 과정에서 벤치마크 데이터셋의 문제를 이미 학습했을 가능성이 있습니다. 이는 모델의 실제 능력보다는 "시험을 잘 보는" 능력을 측정할 수 있습니다.
*   **시험 맞춤식 학습 (Teaching to the Test):** 모델이 특정 벤치마크에 과적합(overfitting)되어, 벤치마크 점수는 높지만 실제 세계의 복잡한 문제에는 잘 대응하지 못할 수 있습니다.
*   **표면적 이해 (Surface-level Understanding):** 모델이 질문의 깊은 의미를 이해하고 추론하기보다는, 질문과 답변 사이의 통계적 패턴이나 키워드 일치를 통해 정답을 맞출 수도 있습니다.

이러한 한계에도 불구하고, 객관식 벤치마크는 모델의 초기 건전성 검사(sanity check), 미세 조정(fine-tuning) 후 성능 회귀 테스트(regression testing), 또는 다른 모델과의 비교를 위한 빠르고 효율적인 방법으로 여전히 유용합니다.

#### 1.2 모델 로드하기

평가를 위해 사전 학습 모델(pre-trained model)을 로드해야 합니다. 여기서는 `transformers` 라이브러리를 사용하여 일반적인 오픈 소스 LLM을 로드하는 과정을 살펴보겠습니다. 특정 모델에 얽매이지 않고 유연하게 다양한 모델을 평가할 수 있도록 합니다.

**코드 블록 1**: 사전 학습 모델 로드하기 (일반화된 예시)

```python
from transformers import AutoTokenizer, AutoModelForCausalLM
import torch

# 사용할 모델의 이름 (예: "meta-llama/Llama-2-7b-hf" 또는 "mistralai/Mistral-7B-v0.1")
# 실제 모델 이름으로 대체해야 합니다.
model_name = "your-preferred-llm-model"

# 토크나이저 로드
tokenizer = AutoTokenizer.from_pretrained(model_name)

# 모델 로드 (GPU 사용 가능한 경우 GPU로 이동)
model = AutoModelForCausalLM.from_pretrained(model_name)
device = "cuda" if torch.cuda.is_available() else "cpu"
model.to(device)

print(f"모델 '{model_name}'이 {device}에 로드되었습니다.")
```
위 코드는 `transformers` 라이브러리를 통해 모델을 로드하는 일반적인 패턴을 보여줍니다. `model_name`을 평가하려는 실제 모델(예: Llama-2, Mistral 등)의 경로 또는 Hugging Face ID로 대체해야 합니다.

#### 1.3 생성된 답변 문자 확인하기

이 섹션에서는 모델의 예측된 답변 문자가 정답과 직접 비교되는 객관식 평가의 단순화된 버전을 구현합니다. 예를 들어, 간단한 상식 질문을 통해 모델의 답변을 확인해 보겠습니다.

이를 위해 간단한 예시를 사용하겠습니다.

```python
example = {
    "question": (
        "다음 중 포유류가 아닌 것은 무엇입니까?"
    ),
    "choices": ["A. 고래", "B. 박쥐", "C. 펭귄", "D. 돌고래"],
    "answer": "C", # 펭귄은 조류
}
```

다음으로, LLM 프롬프트(prompt) 형식을 지정하는 함수를 정의합니다.

**코드 블록 2**: 프롬프트 형식 지정하기

```python
def format_prompt(example):
    choices_str = "\n".join(example['choices'])
    return (
        f"{example['question']}\n"
        f"{choices_str}\n"
        "정답은? (A, B, C, D 중 하나): "
    )
# "정답은? (A, B, C, D 중 하나): " 와 같은 문구는 모델이 특정 형식으로 답변하도록 유도합니다.
```

형식화된 LLM 입력이 어떻게 보이는지 확인하기 위해 예시에서 함수를 실행해 보겠습니다.

```python
prompt = format_prompt(example)
print(prompt)
```

출력은 다음과 같습니다.

```
다음 중 포유류가 아닌 것은 무엇입니까?
A. 고래
B. 박쥐
C. 펭귄
D. 돌고래
정답은? (A, B, C, D 중 하나):
```

위에 표시된 모델 프롬프트(prompt)는 모델에 다양한 답변 선택지 목록을 제공하고 "정답은? (A, B, C, D 중 하나): " 텍스트로 끝나 모델이 정답을 생성하도록 유도합니다.

#### 다른 데이터셋 샘플 로드하기

`datasets` 라이브러리(`pip install datasets` 또는 `uv add datasets`를 통해 설치 가능)를 통해 ARC(AI2 Reasoning Challenge)와 같은 다른 데이터셋에서 직접 예시를 로드할 수 있습니다.

```python
from datasets import load_dataset

# ARC-Easy 데이터셋 로드
dataset = load_dataset("ai2_arc", "ARC-Easy")

# 훈련 세트의 첫 번째 예시 확인:
example = dataset["train"][0]
print(example)
```

다음으로, 프롬프트(prompt)를 토큰화(tokenize)하고 LLM의 입력으로 파이토치 텐서 객체(PyTorch tensor object)로 래핑합니다.

```python
# 토크나이저에 패딩(padding) 토큰이 없는 경우 추가
if tokenizer.pad_token is None:
    tokenizer.pad_token = tokenizer.eos_token

# 프롬프트를 토큰화
prompt_ids = tokenizer.encode(prompt, return_tensors="pt").to(model.device)
```

그런 다음, 모든 설정이 완료되면 아래에 주요 점수 매기기 함수를 정의합니다. 이 함수는 몇 개의 토큰(token)을 생성하고 모델이 출력하는 문자 A/B/C/D의 첫 번째 인스턴스를 추출합니다.

**코드 블록 3**: 생성된 문자 추출하기 (일반화된 예시)

```python
def predict_choice_generic(model, tokenizer, prompt_ids, max_new_tokens=5):
    # 모델 생성 (빔 서치 없이 단순 생성)
    output_ids = model.generate(
        prompt_ids,
        max_new_tokens=max_new_tokens,
        num_return_sequences=1,
        do_sample=False, # 결정론적 생성을 위해 샘플링 비활성화
        pad_token_id=tokenizer.pad_token_id,
        eos_token_id=tokenizer.eos_token_id
    )

    # 프롬프트 토큰을 제외한 생성된 토큰 디코딩
    generated_text = tokenizer.decode(output_ids[0, prompt_ids.shape[1]:], skip_special_tokens=True)

    pred = None
    for char in generated_text.strip():
        char_upper = char.upper()
        if char_upper in "ABCD":
            pred = char_upper
            break
    return pred
```

그런 다음 위 코드 블록의 함수를 사용하여 생성된 문자를 다음과 같이 확인할 수 있습니다.

```python
pred1 = predict_choice_generic(model, tokenizer, prompt_ids)
print(
    f"생성된 문자: {pred1}\n"
    f"정답 여부: {pred1 == example['answer']}"
)
```

#### 객관식 답변 형식

이 섹션에서는 모델의 예측된 답변 문자가 정답과 직접 비교되는 객관식 평가의 단순화된 버전을 설명 목적으로 구현했습니다. 실제로는 모델이 최종 문자 선택만 확인하는 대신 각 후보 답변을 얼마나 가능성 있게 고려하는지 측정하는 로그 확률 점수 매기기(log-probability scoring)와 같이 더 널리 사용되는 변형이 존재합니다. 로그 확률 점수 매기기는 모델이 각 선택지에 부여하는 확신도를 정량화하여 단순한 정답/오답을 넘어 모델의 "자신감"을 평가할 수 있게 합니다.

어떤 MMLU 점수 매기기 변형을 사용하든, 평가는 모델이 미리 정의된 답변 옵션 중에서 선택하는지 여부를 확인하는 것으로 귀결됩니다. MMLU와 같은 객관식 벤치마크의 한계는 미리 정의된 옵션 중에서 선택하는 LLM의 능력만을 측정하므로, 모델이 기본 모델에 비해 얼마나 많은 지식을 잊었는지 확인하는 것 외에는 추론 능력(reasoning capabilities)을 평가하는 데 그다지 유용하지 않다는 것입니다. 이는 자유 형식 작성 능력(free-form writing ability)이나 실제 유용성을 포착하지 못합니다. 그럼에도 불구하고, 객관식 벤치마크는 여전히 간단하고 유용한 진단 도구입니다. 예를 들어, 높은 MMLU 점수가 반드시 모델이 실제 사용에서 강력하다는 것을 의미하지는 않지만, 낮은 점수는 잠재적인 지식 격차를 강조할 수 있습니다.

### 방법 2: 검증기(verifier)를 사용하여 답변 확인하기

이전 섹션에서 논의된 객관식 질문 답변과 관련하여, 검증 기반 접근 방식은 정확도 지표(accuracy metric)를 통해 LLM의 능력을 정량화합니다. 그러나 객관식 벤치마크와 달리, 검증 방법은 LLM이 자유 형식 답변(free-form answer)을 제공하도록 허용합니다. 그런 다음 관련 답변 부분을 추출하고, 아래 그림 6에 설명된 바와 같이 소위 검증기(verifier)를 사용하여 답변 부분을 데이터셋에 제공된 정답과 비교합니다.

**그림 6**: 자유 형식 질문 답변에서 검증 기반 방법으로 LLM을 평가하는 모습. 모델은 자유 형식 답변(여러 단계를 포함할 수 있음)과 최종 박스형 답변을 생성하며, 이는 추출되어 데이터셋의 정답과 비교됩니다.

위 그림에 표시된 바와 같이 추출된 답변을 제공된 답변과 비교할 때, 코드 인터프리터(code interpreter) 또는 계산기형 도구/소프트웨어(calculator-like tools/software)와 같은 외부 도구를 사용할 수 있습니다. 단점은 이 방법이 수학 및 코드와 같이 쉽게(그리고 이상적으로는 결정론적으로) 검증될 수 있는 도메인에만 적용될 수 있다는 것입니다. 또한, 이 접근 방식은 추가적인 복잡성과 의존성을 도입할 수 있으며, 평가 부담의 일부를 모델 자체에서 외부 도구로 옮길 수 있습니다. 그러나 무제한의 수학 문제 변형을 프로그래밍 방식으로 생성할 수 있고 단계별 추론(step-by-step reasoning)의 이점을 얻을 수 있기 때문에, 이는 추론 모델 평가 및 개발의 초석이 되었습니다.

#### 검증기 유형 및 활용

검증기는 크게 몇 가지 유형으로 나눌 수 있으며, 각각 다른 방식으로 LLM의 응답을 평가합니다.

*   **외부 도구 기반 검증기 (External Tool-based Verifiers):** 가장 일반적인 형태로, LLM이 생성한 코드 조각을 실행하는 코드 인터프리터, 수학 문제를 푸는 심볼릭 수학 솔버, 또는 특정 API를 호출하여 결과를 확인하는 방식입니다. 이는 수학 문제 해결, 코딩 챌린지, 데이터베이스 쿼리 생성 등 정답이 명확하고 결정론적인 도메인에서 매우 효과적입니다.
*   **언어 모델 기반 검증기 (Language Model-based Verifiers):** 특정 유형의 응답(예: JSON 형식 준수, 논리적 일관성)을 검증하도록 미세 조정(fine-tuned)된 더 작은 LLM을 사용하는 방식입니다. 이는 외부 도구만으로는 검증하기 어려운, 응답의 형식적 또는 의미적 측면을 평가할 때 유용합니다.
*   **자체 일관성 검증 (Self-consistency Verifiers):** LLM이 동일한 질문에 대해 여러 번 다른 추론 경로를 통해 답변을 생성하고, 이들 답변의 일관성을 평가하는 방식입니다. 가장 흔하게 나타나는 답변을 정답으로 간주하거나, 다른 LLM이 이들 답변의 논리적 일관성을 평가하도록 할 수 있습니다.

검증 기반 평가는 LLM이 단순한 지식 회상을 넘어 복잡한 추론 단계를 수행하고 정확한 최종 결과를 도출하는 능력을 측정하는 데 필수적입니다. 이는 Chain-of-Thought (CoT) 프롬프팅과 결합될 때 특히 강력하며, 모델이 단순히 정답을 내놓는 것을 넘어, 그 과정까지도 검증할 수 있게 합니다. 하지만 검증기의 구축 자체가 난이도가 높고, 검증 대상 도메인에 대한 깊은 이해를 요구한다는 단점도 있습니다.

### 방법 3: 선호도 및 리더보드(leaderboard)를 사용하여 모델 비교하기

지금까지 모델 정확도(accuracy)와 같은 쉽게 정량화할 수 있는 지표를 제공하는 두 가지 방법을 다루었습니다. 그러나 앞서 언급된 방법 중 어느 것도 응답의 스타일을 판단하는 것을 포함하여 LLM을 보다 전체적인 방식으로 평가하지는 않습니다. 이 섹션에서는 아래 그림 8에 설명된 바와 같이 판단 기반 방법인 LLM 리더보드(leaderboard)에 대해 논의합니다.

**그림 8**: 이 부록에서 다루는 판단 기반 및 벤치마크 기반 평가 방법에 초점을 맞춘 이 책에서 다루는 주제의 개념 모델.

이전 섹션에서 벤치마크 기반 접근 방식(객관식, 검증기)을 이미 다루었으므로, 이제 LLM 성능을 측정하기 위한 판단 기반 접근 방식을 소개하며, 이 하위 섹션에서는 리더보드(leaderboard)에 중점을 둡니다.

리더보드(leaderboard)는 모델이 정확도 값이나 다른 고정된 벤치마크 점수가 아니라 사용자(또는 다른 LLM)의 출력에 대한 선호도에 따라 순위가 매겨지는 판단 기반 접근 방식입니다. LM 아레나(LM Arena)와 같은 인기 있는 리더보드는 사용자들에게 두 개의 모델 응답을 비교하고 선호하는 모델에 투표하도록 함으로써, 인간의 실제 선호도를 반영하는 순위를 생성합니다. 이러한 접근 방식은 모델이 실제 사용 환경에서 얼마나 유용하고 만족스러운지를 평가하는 데 매우 중요합니다.

리더보드 평가의 핵심은 인간의 피드백을 대규모로 수집하는 것입니다. 하지만 이 과정에서 다양한 편향(bias)이 발생할 수 있습니다. 예를 들어, 응답의 길이, 문체, 심지어 모델이 응답을 제시하는 순서(position bias)까지도 사용자의 선호도에 영향을 미칠 수 있습니다. 이러한 편향을 줄이기 위해, LM 아레나와 같은 플랫폼은 익명화, 무작위 순서 배치, 그리고 통계적 모델링(예: 브래들리-테리 모델)을 사용하여 보다 견고한 순위를 도출하려고 노력합니다.

또한, 리더보드는 정적인 벤치마크와 달리 모델의 동적인 발전을 반영합니다. 새로운 모델이 출시되거나 기존 모델이 업데이트될 때마다 새로운 데이터가 수집되어 순위가 실시간으로 변동될 수 있습니다. 일부 리더보드는 사용자들이 모델의 약점을 찾기 위해 고의적으로 어려운 프롬프트(prompt)를 사용하는 "적대적 프롬프팅(adversarial prompting)"을 허용하여 모델의 견고성(robustness)을 평가하기도 합니다.

이 섹션의 나머지 부분에서는 리더보드(leaderboard)의 간단한 예시를 구현할 것입니다. 구체적인 예시를 만들기 위해, 그림 9와 유사한 설정에서 사용자가 다양한 LLM에 프롬프트(prompt)를 제공하는 상황을 고려해 봅시다. 아래 목록은 첫 번째 모델이 승자인 쌍별 투표(pairwise votes)를 나타냅니다.

```python
votes = [
    ("GPT-5", "Claude-3"),
    ("GPT-5", "Llama-4"),
    ("Claude-3", "Llama-3"),
    ("Llama-4", "Llama-3"),
    ("Claude-3", "Llama-3"),
    ("GPT-5", "Llama-3"),
]
```

위 목록에서 `votes` 목록의 각 튜플은 두 모델 간의 쌍별 선호도(pairwise preference)를 `(승자, 패자)` 형식으로 나타냅니다. 따라서 `("GPT-5", "Claude-3")`는 사용자가 Claude-3 모델 답변보다 GPT-5를 선호했음을 의미합니다. 이 섹션의 나머지 부분에서는 `votes` 목록을 리더보드(leaderboard)로 변환할 것입니다. 이를 위해 원래 체스 플레이어의 순위를 매기기 위해 개발된 인기 있는 엘로 레이팅 시스템(Elo rating system)을 사용할 것입니다. 구체적인 코드 구현을 살펴보기 전에, 간략하게 작동 방식은 다음과 같습니다. 각 모델은 기준 점수(baseline score)로 시작합니다. 그런 다음 각 비교 및 선호도 투표 후에 모델의 레이팅(rating)이 업데이트됩니다. (엘로(Elo)에서는 업데이트 크기가 결과의 놀라움 정도에 따라 달라집니다.) 특히, 사용자가 높은 순위의 모델보다 현재 모델을 선호하면, 현재 모델은 상대적으로 큰 순위 업데이트를 받고 리더보드(leaderboard)에서 더 높은 순위를 차지합니다. 반대로, 낮은 순위의 상대방에게 이기면 업데이트는 더 작습니다. (그리고 현재 모델이 지면, 비슷한 방식으로 업데이트되지만, 순위 점수가 추가되는 대신 차감됩니다.)

이러한 쌍별 순위를 리더보드(leaderboard)로 변환하는 코드는 아래 코드 블록에 나와 있습니다.

**코드 블록 4**: 리더보드(leaderboard) 구성하기

```python
def elo_ratings(vote_pairs, k_factor=32, initial_rating=1000):
    # Initialize all models with the same base rating
    ratings = {model: initial_rating for pair in vote_pairs for model in pair}

    # Update ratings after each match
    for winner, loser in vote_pairs:
        # Expected score for the current winner
        expected_winner = 1.0 / (
            1.0 + 10 ** ((ratings[loser] - ratings[winner]) / 400.0)
        )

        # k_factor determines sensitivity of updates
        ratings[winner] = (
            ratings[winner] + k_factor * (1 - expected_winner)
        )
        ratings[loser] = (
            ratings[loser] + k_factor * (0 - (1 - expected_winner))
        )
    return ratings
```

위에 정의된 `elo_ratings` 함수는 `votes`를 입력으로 받아 다음과 같이 리더보드(leaderboard)로 변환합니다.

```python
ratings = elo_ratings(votes, k_factor=32, initial_rating=1000)
for model in sorted(ratings, key=ratings.get, reverse=True):
    print(f"{model:8s} : {ratings[model]:.1f}")
```

그 결과는 다음과 같은 리더보드(leaderboard) 순위이며, 점수가 높을수록 좋습니다.

```
GPT-5    : 1043.7
Claude-3 : 1015.2
Llama-4  : 1000.7
Llama-3  : 940.4
```

그렇다면 이것은 어떻게 작동할까요? 각 쌍에 대해 다음 공식을 사용하여 승자의 예상 점수(expected score)를 계산합니다.

`expected_winner = 1 / (1 + 10 ** ((rating_loser - rating_winner) / 400))`

이 `expected_winner` 값은 현재 레이팅(rating)을 기반으로 무승부가 없는 상황에서 모델이 이길 것으로 예상되는 확률입니다. 이는 레이팅(rating) 업데이트의 크기를 결정합니다.

먼저, 각 모델은 `initial_rating = 1000`으로 시작합니다. 두 레이팅(rating)(승자와 패자)이 같으면 `expected_winner = 0.5`가 되며, 이는 동등한 경기를 나타냅니다. 이 경우 업데이트는 다음과 같습니다.

`rating_winner + k_factor * (1 - 0.5) = rating_winner + 16`
`rating_loser + k_factor * (0 - (1 - 0.5)) = rating_loser - 16`

이제, 강력한 우승 후보(높은 레이팅(rating)을 가진 모델)가 이기면 `expected_winner ≈ 1`이 됩니다. 우승 후보는 소량만 얻고 패자는 소량만 잃습니다.

`rating_winner + 32 * (1 - 0.99) = rating_winner + 0.32`
`rating_loser + 32 * (0 - (1 - 0.99)) = rating_loser - 0.32`

그러나 약자(낮은 레이팅(rating)을 가진 모델)가 이기면 `expected_winner ≈ 0`이 되며, 승자는 거의 전체 `k_factor` 점수를 얻고 패자는 거의 동일한 크기를 잃습니다.

`rating_winner + 32 * (1 - 0.01) = rating_winner + 31.68`
`rating_loser + 32 * (0 - (1 - 0.01)) = rating_loser - 31.68`

#### 순서의 중요성

엘로(Elo) 접근 방식은 각 경기(모델 비교) 후에 레이팅(rating)을 업데이트하므로, 나중의 결과는 이미 업데이트된 레이팅(rating)을 기반으로 합니다. 이는 동일한 결과 집합이라도 다른 순서로 제시되면 최종 점수가 약간 다를 수 있음을 의미합니다. 이러한 효과는 일반적으로 미미하지만, 특히 이변이 일찍 발생했는지 늦게 발생했는지에 따라 발생할 수 있습니다. 이러한 순서 효과를 줄이기 위해, 투표 쌍을 섞고 `elo_ratings` 함수를 여러 번 실행하여 레이팅(rating)을 평균화할 수 있습니다.

#### 다른 순위 매기기 방법

엘로(Elo) 시스템 외에도, LM 아레나(LM Arena)는 최근 브래들리-테리 모델(Bradley–Terry model)을 기반으로 하는 통계적 접근 방식으로 전환했습니다. 이 모델은 각 모델의 상대적인 강도를 확률적으로 추정하며, 엘로 시스템보다 통계적으로 더 견고하고 순위의 불확실성에 대한 신뢰 구간(confidence intervals)을 제공할 수 있다는 장점이 있습니다.

최신 리더보드들은 이러한 통계적 모델링 외에도 다양한 기법을 적용합니다. 예를 들어, 사용자 투표의 품질을 평가하고 신뢰할 수 없는 투표를 걸러내거나, 특정 유형의 편향(예: 짧은 답변 선호)을 보정하기 위한 가중치(weighting)를 적용하기도 합니다. 또한, 일부 플랫폼은 LLM 심사위원(LLM judges)을 활용하여 대규모 인간 평가의 필요성을 줄이면서도 일관된 평가를 유지하려는 하이브리드(hybrid) 접근 방식을 채택하고 있습니다. 이러한 노력은 리더보드가 모델의 실제 유용성과 사용자 만족도를 반영하는 가장 중요한 지표 중 하나로 자리매김하게 합니다.

### 방법 4: 다른 LLM으로 응답 평가하기

초기에는 LLM이 BLEU라는 측정값을 포함한 통계적 및 휴리스틱 기반 방법으로 평가되었습니다. BLEU는 생성된 텍스트가 참조 텍스트와 얼마나 잘 일치하는지를 측정하는 대략적인 지표입니다. 이러한 지표의 문제는 정확한 단어 일치를 요구하며 동의어(synonyms), 단어 변경 등을 고려하지 않는다는 것입니다. ROUGE, METEOR 등 다른 유사한 지표들도 등장했지만, 모두 인간이 느끼는 텍스트의 '품질'이나 '유용성'을 완벽하게 포착하지 못한다는 한계를 가집니다.

이 문제에 대한 한 가지 해결책은, 작성된 답변 텍스트를 전체적으로 판단하고 싶다면, 이전 섹션에서 논의된 상대적 순위 및 리더보드(leaderboard) 기반 접근 방식을 사용하는 것입니다. 그러나 리더보드(leaderboard)의 단점은 인간의 피드백(및 이 피드백을 수집하는 데 따르는 어려움)을 포함하므로 선호도 기반 비교의 주관적인 특성입니다.

관련된 방법은 미리 정의된 채점 기준표(grading rubric)(즉, 평가 가이드)를 가진 다른 LLM을 사용하여 LLM의 응답을 참조 응답과 비교하고, 그림 12에 설명된 바와 같이 미리 정의된 기준표에 따라 응답 품질을 판단하는 것입니다.

**그림 F12**: LLM 심사위원(LLM-judge) 평가의 예시. 평가할 모델이 답변을 생성하면, 별도의 심사위원 LLM이 기준표와 제공된 참조 답변에 따라 점수를 매깁니다.

실제로 그림 12에 표시된 심사위원 기반 접근 방식은 심사위원 LLM이 강력할 때 잘 작동합니다. 일반적인 설정은 API(예: GPT-5 API)를 통해 선도적인 독점 LLM(proprietary LLMs)을 사용하지만, 전문 심사 모델(specialized judge models)도 존재합니다. (예를 들어, 많은 예시 중 하나는 Phudge입니다. 궁극적으로 이러한 전문 모델의 대부분은 독점 GPT 모델과 유사한 점수 매기기 동작을 갖도록 미세 조정(fine-tuned)된 더 작은 모델일 뿐입니다.) 심사위원이 잘 작동하는 이유 중 하나는 답변을 평가하는 것이 종종 답변을 생성하는 것보다 쉽기 때문입니다.

LLM 심사위원(LLM-as-a-judge) 접근 방식은 인간 평가자의 시간과 비용을 절감하면서도, 자동화된 통계적 지표보다 훨씬 더 미묘한 언어적 특성을 평가할 수 있는 잠재력을 가집니다. 이 방법의 핵심은 심사위원 LLM에게 명확하고 포괄적인 평가 기준(rubric)을 제공하는 것입니다.

#### LLM-as-a-judge 구현을 위한 프롬프트 엔지니어링

LLM 심사위원을 효과적으로 활용하기 위해서는 프롬프트 엔지니어링(prompt engineering)이 매우 중요합니다. 심사위원 LLM에 어떤 정보를 주고 어떤 방식으로 평가를 요청하느냐에 따라 결과의 품질과 일관성이 크게 달라질 수 있습니다.

*   **명확한 역할 및 지침 부여:** 심사위원 LLM에게 "당신은 공정하고 객관적인 AI 심사관입니다"와 같은 명확한 역할을 부여하고, 평가의 목표와 기대되는 출력 형식을 상세히 설명해야 합니다.
*   **다중 기준 평가:** 단순히 점수 하나를 요청하는 대신, 정확성, 일관성, 유용성, 창의성, 안전성 등 여러 차원에 걸쳐 평가하도록 요청할 수 있습니다. 각 차원에 대한 정의와 점수 척도를 명확히 제시합니다.
*   **추론 과정 요구:** 심사위원 LLM에게 점수뿐만 아니라, 그 점수를 부여한 이유와 근거를 함께 설명하도록 요청하는 것이 좋습니다. 이는 평가의 투명성을 높이고, 개발자가 모델의 약점을 이해하는 데 도움을 줍니다.
*   **편향 완화:** 심사위원 LLM 또한 편향(bias)을 가질 수 있으므로, 단일 LLM 심사위원 대신 여러 LLM 심사위원을 사용하거나, 인간 평가와 LLM 평가를 혼합하는 하이브리드 방식을 고려할 수 있습니다. 또한, 평가할 모델의 이름이나 출처 정보를 숨기는 등의 방법으로 심사위원 LLM의 편향을 줄일 수 있습니다.

**코드 블록 7**: 채점 기준표(grading rubric)를 포함한 프롬프트 템플릿 설정하기

```python
def rubric_prompt(instruction, reference_answer, model_answer):
    rubric = (
        "You are a fair judge assistant. You will be "
        "given an instruction, a reference answer, and "
        "a candidate answer to evaluate, according to "
        "the following rubric:\n\n"
        "1: The response fails to address the "
        "instruction, providing irrelevant, incorrect, "
        "or excessively verbose content.\n"
        "2: The response partially addresses the "
        "instruction but contains major errors, "
        "omissions, or irrelevant details.\n"
        "3: The response addresses the instruction to "
        "some degree but is incomplete, partially "
        "correct, or unclear in places.\n"
        "4: The response mostly adheres to the "
        "instruction, with only minor errors, "
        "omissions, or lack of clarity.\n"
        "5: The response fully adheres to the "
        "instruction, providing a clear, accurate, and "
        "relevant answer in a concise and efficient "
        "manner.\n\n"
        "Now here is the instruction, the reference "
        "answer, and the response.\n"
    )
    prompt = (
        f"{rubric}\n"
        f"Instruction:\n{instruction}\n\n"
        f"Reference Answer:\n{reference_answer}\n\n"
        f"Answer:\n{model_answer}\n\n"
        f"Evaluation: "
    )
    return prompt
```

`rubric_prompt`의 `model_answer`는 실제로 우리 모델이 생성한 응답을 나타내기 위한 것입니다. 설명 목적으로, 여기서는 동적으로 생성하는 대신 그럴듯한 모델 답변(model answer)을 하드코딩합니다. (그러나 이 기사 초반에 로드한 Qwen3 모델을 사용하여 실제 `model_answer`를 생성해도 좋습니다.)

다음으로, LLM 심사위원 모델(예: GPT-4, Claude 3 또는 강력한 오픈 소스 모델)을 위한 렌더링된 프롬프트(rendered prompt)를 생성해 보겠습니다.

```python
rendered_prompt = rubric_prompt(
    instruction=(
        "만약 모든 새가 날 수 있고, 펭귄이 새라면, 펭귄은 날 수 있습니까?"
    ),
    reference_answer=(
        "네, 모든 새가 날 수 있다는 전제에 따르면, 펭귄은 날 수 있습니다."
    ),
    model_answer=(
        "네 – 그 전제 하에서는 펭귄도 날 수 있을 것입니다."
    )
)
print(rendered_prompt)
```

출력은 다음과 같습니다.

```
You are a fair judge assistant. You will be given an instruction, a reference answer, and a candidate answer to evaluate, according to the following rubric:

1: The response fails to address the instruction, providing irrelevant, incorrect, or excessively verbose content.
2: The response partially addresses the instruction but contains major errors, omissions, or irrelevant details.
3: The response addresses the instruction to some degree but is incomplete, partially correct, or unclear in places.
4: The response mostly adheres to the instruction, with only minor errors, omissions, or lack of clarity.
5: The response fully adheres to the instruction, providing a clear, accurate, and relevant answer in a concise and efficient manner.

Now here is the instruction, the reference answer, and the response.

Instruction:
만약 모든 새가 날 수 있고, 펭귄이 새라면, 펭귄은 날 수 있습니까?

Reference Answer:
네, 모든 새가 날 수 있다는 전제에 따르면, 펭귄은 날 수 있습니다.

Answer:
네 – 그 전제 하에서는 펭귄도 날 수 있을 것입니다.

Evaluation:
```

프롬프트(prompt)를 "Evaluation: "으로 끝내는 것은 모델이 답변을 생성하도록 유도합니다. 이 프롬프트를 강력한 LLM 심사위원(예: `query_model` 함수를 통해 호출되는 GPT-4 API)에 전달하면, 해당 LLM은 기준표에 따라 평가를 수행하고 점수와 함께 설명을 제공할 것입니다.

LLM 심사위원은 특히 모델의 안전성(safety) 및 유해성(toxicity) 평가, 편향성(bias) 감지, 그리고 특정 지침(instruction) 준수 여부 확인에 유용하게 사용됩니다. 이는 대규모 언어 모델의 복잡한 동작을 이해하고 제어하는 데 중요한 도구입니다.

### 복합 평가 전략 및 미래 방향

LLM 평가의 복잡성은 단일 지표나 방법론으로는 모델의 모든 측면을 포착할 수 없다는 데 있습니다. 따라서 실제 세계에서 LLM의 성능을 종합적으로 이해하기 위해서는 여러 평가 방법을 결합하는 **복합 평가 전략(composite evaluation strategy)**이 필수적입니다.

#### 정성적 평가의 중요성

정량적 벤치마크 점수가 모델의 특정 능력을 잘 보여주지만, 인간 사용자가 느끼는 '품질'이나 '유용성'을 완전히 반영하지 못하는 경우가 많습니다. 이때 **정성적 평가(qualitative evaluation)**가 중요해집니다.

*   **인간 전문가 분석:** 특정 도메인 전문가가 LLM의 답변을 심층적으로 검토하여 미묘한 오류, 창의성, 공감 능력 등을 평가합니다. 이는 특히 법률, 의료, 창작 분야에서 중요합니다.
*   **사용성 테스트 및 A/B 테스트:** 실제 사용자 환경에서 LLM을 배포하고, 사용자의 반응(만족도, 작업 완료율, 재시도 횟수 등)을 직접 측정합니다. 이는 모델의 실제 가치를 판단하는 가장 직접적인 방법입니다.
*   **레드 팀(Red Teaming):** 모델의 취약점, 유해한 콘텐츠 생성 가능성, 보안 문제 등을 의도적으로 찾아내기 위해 공격적인 프롬프트를 사용하는 전문 팀의 평가입니다.

#### RLHF를 넘어: 인간 피드백의 진화

강화 학습(Reinforcement Learning from Human Feedback, RLHF)은 LLM을 인간의 선호도에 맞춰 정렬(alignment)시키는 데 혁혁한 공을 세웠습니다. RLHF의 핵심은 인간 평가자가 모델의 응답에 순위를 매기거나 점수를 부여하는 피드백을 제공하고, 이 피드백을 보상 모델(reward model) 훈련에 사용하여 LLM이 인간의 선호도를 모방하도록 학습시키는 것입니다.

최근에는 DPO(Direct Preference Optimization), PPO(Proximal Policy Optimization)와 같은 더 효율적이고 안정적인 정렬(alignment) 기술들이 등장하며, 인간 피드백이 LLM 훈련 및 평가 과정에 더욱 깊이 통합되고 있습니다. 이러한 기술들은 평가 과정에서 수집된 인간의 선호도 데이터를 직접적으로 활용하여 모델의 행동을 개선합니다.

#### 다중 모드 LLM 평가

텍스트 기반 LLM을 넘어, 이미지, 오디오, 비디오를 이해하고 생성하는 다중 모드 LLM(Multimodal LLM)의 등장으로 평가의 복잡성은 더욱 커졌습니다. 다중 모드 LLM은 텍스트 응답의 정확성뿐만 아니라, 생성된 이미지의 품질, 시각적 정보와 텍스트 정보 간의 일관성, 오디오 인식의 정확성 등 다양한 모달리티(modality)에 걸친 성능을 평가해야 합니다. MM-Vet, MME(Multimodal Model Evaluation)와 같은 새로운 다중 모드 벤치마크들이 이러한 모델들을 평가하기 위해 개발되고 있습니다.

#### 지속적인 평가

LLM은 정적인 존재가 아닙니다. 새로운 데이터로 지속적으로 업데이트되거나, 사용자 상호작용을 통해 미세 조정될 수 있습니다. 따라서 모델이 배포된 후에도 성능 저하(drift), 환각(hallucination) 증가, 편향성 변화 등을 모니터링하기 위한 **지속적인 평가(continuous evaluation)** 시스템이 중요합니다. 이는 모델의 수명 주기 전반에 걸쳐 품질을 유지하고 개선하는 데 필수적입니다.

### 결론

이 기사에서는 객관식, 검증기(verifier), 리더보드(leaderboard) 및 LLM 심사위원(LLM judges)의 네 가지 다른 평가 접근 방식을 다루었습니다. 이 기사가 길었지만, LLM이 어떻게 평가되는지에 대한 개요를 얻는 데 유용했기를 바랍니다. 이와 같은 처음부터 시작하는 접근 방식은 장황할 수 있지만, 이러한 방법이 내부적으로 어떻게 작동하는지 이해하는 좋은 방법이며, 이는 약점과 개선 영역을 식별하는 데 도움이 됩니다.

그렇다면 "LLM을 평가하는 가장 좋은 방법은 무엇일까요?"라고 궁금해하실 것입니다. 불행히도, 우리가 보았듯이 각각 다른 장단점을 가지고 있기 때문에 단 하나의 최선의 방법은 없습니다. 요약하자면:

*   **객관식(Multiple-choice)**
    *   (+) 대규모로 실행하기에 비교적 빠르고 저렴합니다.
    *   (+) 논문(또는 모델 카드) 전반에 걸쳐 표준화되고 재현 가능합니다.
    *   (-) 기본적인 지식 회상 능력을 측정합니다.
    *   (-) LLM이 실제 세계에서 사용되는 방식을 반영하지 않습니다.
*   **검증기(Verifiers)**
    *   (+) 정답이 있는 도메인에 대한 표준화되고 객관적인 채점입니다.
    *   (+) 자유 형식 답변(free-form answers)을 허용합니다(최종 답변 형식에 일부 제약이 있음).
    *   (+) 프로세스 검증기(process verifiers) 또는 프로세스 보상 모델(process reward models)을 사용하는 경우 중간 단계도 점수 매길 수 있습니다.
    *   (-) 검증 가능한 도메인(예: 수학 또는 코드)이 필요하며, 좋은 검증기를 구축하는 것이 까다로울 수 있습니다.
    *   (-) 결과 전용 검증기(outcome-only verifiers)는 추론 품질이 아닌 최종 답변만 평가합니다.
*   **아레나 스타일 리더보드(Arena-style leaderboards) (인간 쌍별 선호도)**
    *   (+) 실제 프롬프트(prompt)에 대해 "어떤 모델을 사람들이 선호하는가?"에 직접적으로 답변합니다.
    *   (+) 자유 형식 답변(free-form answers)을 허용하고 스타일, 유용성 및 안전성을 암묵적으로 고려합니다.
    *   (-) 인간에게 비용이 많이 들고 시간이 많이 소요됩니다.
    *   (-) 정확성을 측정하지 않고 선호도만 측정합니다.
    *   (-) 비정상 모집단(nonstationary populations)이 안정성에 영향을 미칠 수 있습니다.
*   **LLM 심사위원(LLM-as-a-judge)**
    *   (+) 많은 작업에 걸쳐 확장 가능합니다.
    *   (+) 자유 형식 답변(free-form answers)을 허용합니다.
    *   (-) 심사위원의 능력에 따라 달라집니다(앙상블(ensemble)은 이를 더 견고하게 만들 수 있습니다).
    *   (-) 기준표(rubric) 선택에 따라 달라집니다.
*   **복합 및 정성적 평가 (Composite & Qualitative Evaluation)**
    *   (+) 정량적 지표의 한계를 보완하여 실제 사용자의 경험과 미묘한 성능을 포착합니다.
    *   (+) 안전성, 편향성, 윤리적 측면 등 복잡한 문제를 평가하는 데 필수적입니다.
    *   (-) 시간과 비용이 많이 들고, 평가 결과의 재현성이 낮을 수 있습니다.
    *   (-) 평가 기준 설정 및 인간 평가자 훈련이 어려울 수 있습니다.

저는 일반적으로 레이더 차트(radar chart)를 그다지 좋아하지 않지만, 아래에 표시된 바와 같이 LLM을 평가할 때 강점과 약점을 식별하기 위해 다양한 영역에 주의를 기울이는 것이 이상적이라는 개념을 시각화하는 데 도움이 될 수 있습니다.

**그림 15**: LLM을 평가할 때 강점과 약점을 식별하기 위해 이상적으로는 다양한 영역에 주의를 기울여야 한다는 개념을 보여주는 레이더 차트(radar chart).

예를 들어, 높은 객관식 점수는 모델이 견고한 일반 지식을 가지고 있음을 시사합니다. 여기에 강력한 검증기 점수를 결합하면 모델이 기술적인 질문에도 올바르게 답변할 가능성이 높습니다. 그러나 모델이 LLM-as-a-judge 및 리더보드(leaderboard) 평가에서 저조한 성능을 보인다면, 응답을 효과적으로 작성하거나 명확하게 표현하는 데 어려움을 겪을 수 있으며 일부 RLHF의 이점을 얻을 수 있습니다. 또한, 정성적 평가에서 모델이 사용자에게 불친절하거나 편향된 답변을 자주 생성한다면, 이는 기술적인 정확성을 넘어선 근본적인 문제가 있음을 시사할 수 있습니다.

따라서 최상의 평가는 여러 영역을 결합합니다. 하지만 이상적으로는 목표나 비즈니스 문제와 직접적으로 일치하는 데이터도 사용합니다. 예를 들어, 법률 또는 법률 관련 작업을 지원하기 위해 LLM을 구현한다고 가정해 봅시다. 빠른 건전성 검사(sanity check)로 MMLU와 같은 표준 벤치마크(benchmark)에서 모델을 실행하는 것이 합리적이지만, 궁극적으로는 법률과 같은 목표 도메인에 맞게 평가를 조정해야 할 것입니다. 온라인에서 좋은 시작점이 될 수 있는 공개 벤치마크(benchmark)를 찾을 수 있지만, 결국에는 자체 독점 데이터(proprietary data)로 테스트해야 할 것입니다. 그래야만 모델이 훈련 중에 테스트 데이터를 이미 보지 않았다고 합리적으로 확신할 수 있습니다.

어떤 경우든, 모델 평가는 매우 크고 중요한 주제입니다. 이 기사가 주요 접근 방식이 어떻게 작동하는지 설명하는 데 유용했으며, 다음번에 모델 평가를 보거나 직접 실행할 때 몇 가지 유용한 통찰력을 얻으셨기를 바랍니다. 언제나처럼, 즐거운 실험 되세요!

이 잡지는 개인적인 열정 프로젝트이며, 여러분의 지원은 이 프로젝트를 유지하는 데 도움이 됩니다. 제 작업을 지원하고 싶으시다면, 저의 『Build a Large Language Model (From Scratch)』 책 또는 그 후속작인 『Build a Reasoning Model (From Scratch)』을 고려해 주십시오. (이 책들이 LLM이 어떻게 작동하는지 다른 곳에서는 찾을 수 없는 깊이로 설명하므로 많은 것을 얻으실 것이라고 확신합니다.) 읽어주셔서 감사하며, 독립적인 연구를 지원해 주셔서 감사합니다!

『Build a Large Language Model (From Scratch)』은 [현재 Amazon에서 구매 가능합니다](https://www.amazon.com/Build-Large-Language-Model-Scratch/dp/1617299518).
『Build a Reasoning Model (From Scratch)』은 [Manning에서 얼리 액세스(Early Access) 중입니다](https://www.manning.com/books/build-a-reasoning-model-from-scratch).
책을 읽으셨고 잠시 시간을 내주실 수 있다면, [간단한 리뷰](https://www.manning.com/books/build-a-reasoning-model-from-scratch#reviews)를 남겨주시면 정말 감사하겠습니다. 이는 저자들에게 큰 도움이 됩니다!
여러분의 지원은 큰 의미가 있습니다! 감사합니다!