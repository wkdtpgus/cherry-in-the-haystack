**정상 기술로서의 AI(AI as Normal Technology)**

저희가 **정상 기술로서의 AI(AI as Normal Technology)**를 출간했을 때, 그 반향은 저희의 예상을 뛰어넘었으며, AI 기술의 복잡한 사회적 통합 과정에 대한 깊은 성찰을 불러일으켰습니다. 이 글은 저희 둘 중 누구라도 해낸 일 중 가장 영향력 있는 것이 되었고, AI의 윤리적 사용과 책임 있는 개발에 대한 새로운 논의의 장을 열었습니다. 1 저희는 이를 AI의 중기적 미래와 그 영향에 대해 더 많은 시간을 생각하고 글을 쓰는 강력한 신호이자, AI가 단순한 도구를 넘어 인간 사회와 상호작용하며 진화하는 과정에 대한 이해를 심화하는 중요한 계기로 받아들였습니다. 이는 AI 스네이크 오일(AI Snake Oil) 프로젝트가 다루었던 AI의 현재 및 단기적 영향에 대한 글쓰기에서 초점을 전환한 것이며, 단순히 기술적 진보를 넘어 인류의 가치와 어떻게 조화를 이룰 수 있을지에 대한 질문을 던지는 것입니다. 이러한 변화를 반영하여 저희는 이 뉴스레터의 이름을 변경하고 초점을 확장했습니다. 저희는 이미 정상 기술로서의 AI(AI as Normal Technology)에 대한 두 편의 후속 에세이와 AI 거버넌스 및 데이터 주권에 대한 후속 연구를 진행했으며, 2026년 말에 완성하여 2027년에 출판할 계획인 책과 2025년 말에 완성하여 2026년에 출판할 계획인 보고서를 통해 저희의 프레임워크(framework)를 현실 세계의 적용 사례에 맞게 확장하면서 더 정기적으로 글과 통찰을 게시할 예정입니다. 오늘 저희는 정상 기술로서의 AI(AI as Normal Technology)에 대한 일반적인 오해의 지점들을 다루고, 원래 에세이를 더 쉽게 접근할 수 있도록 노력하며, AI 2027과 비교해보고자 하는 동시에, AI의 지속 가능한 발전과 포용적 미래를 위한 실질적인 과제들을 다루고, 기존의 기술 중심적 관점을 넘어, 인간 중심적 접근 방식의 중요성을 강조해보고자 합니다.

**목차**
*   정상적이라는 것이 평범하거나 예측 가능하다는 의미는 아닙니다
*   저희 논지의 재정의
*   최근의 AI 모델 업데이트가 AI의 본질에 대한 오해를 심화시킬 수도 있습니다
*   정상 기술로서의 AI(AI as Normal Technology)와 AI 2027 사이에서 “중간 지점”을 찾기 어려운 이유
*   다른 세계관에 몰두해 있을 때 하나의 세계관을 이해하기는 어렵습니다
*   생성형 AI와 인간 창의성 사이의 새로운 협력 모델
*   인간 중심 AI 디자인의 중요성
*   AI의 이점을 얻으려면 많은 노력과 고통스러운 선택이 필요할 것입니다
*   확산 속도에 대한 초현실적인 논쟁
*   AI 채택이 다르게 느껴지는 이유
*   결론
*   AI 시대의 새로운 도전과 기회

**정상적이라는 것이 평범하거나 예측 가능하다는 의미는 아닙니다**

에세이에서 저희가 정상적이라는 것의 정의를 탐구했지만, 그 의미가 단순히 기술적 특성에 국한되지 않음을 분명히 할 필요가 있었습니다. 저희의 요점은 “볼 것 없으니 지나가세요”가 아닙니다. 실제로 예측 불가능한 사회적 영향은 자동차에서 소셜 미디어(social media)에 이르기까지 강력한 기술의 특징이었으며, AI 역시 예외는 아닙니다. 이는 기술과 사회 시스템 간의 복잡한 상호작용에서 발생하는 비예측적 효과(emergent effects)이기 때문입니다. 기술 자체의 논리만으로는 사회적 가치나 윤리적 함의를 완전히 예측하기 어렵습니다. 이것이 기술 결정론(technological determinism)을 거부하고, 대신 인간의 역할과 선택에 주목하는 것이 이 글의 핵심 전제 중 하나인 이유입니다. AI, 특히 챗봇(chatbot)과 대규모 언어 모델(large language models)의 경우, 저희는 이미 비예측적 사회적 효과를 목격하고 있습니다. AI 동반자(AI companion)의 확산과 “AI 정신증(AI psychosis)”과 같은 모델 아첨(model sycophancy)의 일부 해로운 영향, 그리고 AI 기반 콘텐츠의 확산과 “딥페이크(deepfake)”와 같은 새로운 형태의 정보 왜곡은 대부분의 관찰자들을 놀라게 하거나 당황하게 했습니다. 2 반면에 AI가 선거를 조작하는 데 사용되는 것과 같이 임박했다고 널리 예측되었던 많은 위험은 실현되지 않았으며, AI가 특정 산업의 생산성을 혁신적으로 개선하는 것과 같이 임박했다고 널리 예측되었던 많은 이점은 아직 초기 단계에 있습니다. 3~5년 후 AI의 사회적 영향의 지형이 어떻게 보일지는 — 미래 역량이 아닌 현재 역량의 확산에 기반하더라도 — 누구도 예측할 수 없습니다. 기술적 역량의 개발은 AI의 사회적 영향보다 더 예측 가능합니다. AI 2027의 저자 중 한 명인 다니엘 코코타일로(Daniel Kokotajlo)는 2021년에 “2026년의 모습은 어떠할까(What 2026 looks like)”라는 에세이로 AI 안전 커뮤니티(AI safety community)에서 유명했습니다. 기술 자체에 대한 그의 예측은 섬뜩할 정도로 정확했지만, 사회적 영향에 대한 예측은 전반적으로 방향성이 옳지 않았으며, 이는 그가 저희 중 한 명과의 팟캐스트(podcast) 토론에서 기꺼이 인정한 점입니다. 과거의 기술적 예측은 종종 기술 자체의 발전 속도에 초점을 맞췄지만, 실제 사회적 파급 효과는 복잡한 인간의 적응 과정과 제도적 변화에 의해 좌우됩니다. 이 모든 것은 AI를 기관과 정책 입안자들에게 더 심각한 도전 과제로 만듭니다. 왜냐하면 그들은 예측의 잘못된 안도감에 의존하거나 모든 해악을 방지하려 노력하는 대신 예측 불가능한 영향에 민첩하게 대응해야 할 것이기 때문입니다. 광범위하게 말해서, 이러한 적응성을 가능하게 하는 정책 수립 접근 방식은 회복탄력성(resilience)이라고 불리며, 이는 저희 에세이가 옹호했던 바입니다. 그러나 저희가 잠재적으로 치명적인 위험을 다루는 접근 방식으로 회복탄력성을 강조했지만, 회복탄력성이 더 광범위하게 퍼져 있는 위험을 다루는 데도 중요한 역할을 한다는 점을 더 명확히 했어야 했습니다. 아마도 일부 독자들이 예측 가능성에 대한 저희의 견해를 오해한 이유는 “정상”이라는 단어 때문일 것입니다. 다시 말하지만, 저희의 목표는 개별적으로 그리고 집단적으로 AI에 적응하는 과제를 사소하게 만들려는 것이 아닙니다. 이상적인 세상에서는 단순히 “기술로서의 AI(AI as Technology)”가 더 나은 제목이었겠지만, 저희는 그것이 저희의 목표가 현재 담론을 지배하고 있는 기술 만능주의적 세계관을 특징짓는 예외주의(exceptionalism)에 대한 대안을 제공하는 것임을 효과적으로 전달할 것이라고 생각하지 않았습니다.

**저희 논지의 재정의**

저희 논지의 핵심을 추출하고 단순화한다면 다음과 같을 것입니다: AI 역량 증가와 사회적 영향 사이에는 역동적인 상호작용이 있습니다. 이점과 위험은 AI가 개발될 때가 아니라 배포될 때, 그리고 사용자의 맥락 속에서 실현됩니다. 이는 저희(개인, 조직, 기관, 정책 입안자)에게 그러한 영향을 형성하는 데 많은 영향력을 행사할 수 있는 지점들을 제공합니다. 따라서 저희는 역량 개발 속도에 대해 그렇게 많이 걱정할 필요가 없습니다. 저희의 노력은 AI의 이점을 실현하고 위험에 대응하는 관점에서 배포 단계(deployment stage)에 더 집중해야 합니다. 이 모든 것은 오늘날의 AI에만 해당되는 것이 아니라, AI 역량의 자기 개선(self-improvement)과 같은 가설적인 발전에도 해당됩니다. AI 시스템의 힘에 대한 많은 한계는 해당 시스템 외부에 존재하며 (또한 그래야만 합니다), 따라서 AI가 스스로 기술 설계를 개선하는 것만으로는 극복될 수 없습니다. 이 프레임워크(framework)의 측면들은 결국 수정되어야 할 수도 있지만, 그것은 저희가 의미 있게 예측하거나 준비할 수 있는 지평 너머에 있습니다: 저희가 2부에서 설명하는 세상은 오늘날보다 AI가 훨씬 더 발전한 세상입니다. 저희는 AI 발전—또는 인간의 발전—이 그 지점에서 멈출 것이라고 주장하는 것이 아닙니다. 그 이후에는 무엇이 올까요? 저희는 모릅니다. 이 비유를 생각해 보십시오: 1차 산업혁명(Industrial Revolution)의 여명기에 산업 세계가 어떻게 보일지 생각하고 준비하는 것은 유용했을 것이지만, 전기나 컴퓨터를 예측하려 시도하는 것은 헛된 일이었을 것입니다. 여기서 저희의 작업도 비슷합니다. 저희는 “급속한 발전(fast takeoff)” 시나리오(scenario)를 거부하기 때문에, 저희가 시도한 것보다 더 먼 미래의 세상을 상상하는 것을 필요하거나 유용하다고 보지 않습니다. 저희가 2부에서 설명하는 시나리오가 구체화된다면, 저희는 다음에 올 것이 무엇이든 더 잘 예측하고 준비할 수 있을 것입니다. 어쨌든, 다시 말하지만, 논지의 핵심은 AI와 사회 간의 관계를 이해하기 위한 근본적인 인과 프레임워크(causal framework)이며, AI가 가질 수도 있고 가지지 않을 수도 있는 특정 영향이 아닙니다. 저희의 견해로는, 이 인과적 이해를 공유한다면, 여러분은 정상 기술 논지(normal technology thesis)에 동의하는 것입니다. 저희는 이 프레임워크(framework)가 암묵적으로나마 실제로 널리 공유되고 있음을 발견했습니다. 이는 많은 독자들의 마음속에서 이 논지를 거의 동어반복적(tautological)으로 만듭니다. 저희는 저희가 보기에 — 그리고 독자들이 보기에 — 매우 약한 주장을 하고 있는 것입니다! 이를 인식하지 못하면 독자들은 저희가 “정상”이라는 말로 의미했을지도 모르는 훨씬 더 구체적인 것을 찾으려 합니다. 하지만 저희는 그렇지 않았습니다. 저희는 기술을 “정상”과 “비정상”으로 분류한 다음 AI를 “정상” 범주에 넣는 것이 아닙니다. 저희는 단지 AI를 다른 강력한 범용 기술(general-purpose technologies)처럼 다루어야 한다고 말하는 것입니다. 이는 대규모 언어 모델(large language models)이나 특정 종류의 AI에만 국한된 것이 아닙니다. 덧붙여 말하자면, 이것이 제목이 “정상 기술로서의 AI(AI as normal technology)”이지 “하나의 정상 기술로서의 AI(AI as a normal technology)”가 아닌 이유입니다. 저희의 견해는 총체적으로 AI라고 불리는 모든 기술과, AI라고 불리지 않더라도 유사한 다른 기술에 적용됩니다. 저희의 세계관이 거의 동어반복적이라면, 굳이 그것을 언급할 필요가 있을까요? 왜냐하면 그것은 초지능(superintelligence) 세계관과 대조되기 때문입니다. 세계관의 특징은 이렇습니다: 서로 모순되는 세계관이 존재할 수 있으며, 각 세계관은 그것을 신봉하는 사람들에게는 동어반복적으로 보일 수 있습니다.

**최근의 AI 모델 업데이트가 AI의 본질에 대한 오해를 심화시킬 수도 있습니다**

최근 새로운 AI 모델 출시 이후 저희 에세이에 대한 관심이 급증했으며, 그 중 적어도 일부는 출시에 대한 기대와 현실 사이의 괴리에서 비롯된 것으로 보입니다. 이것은 이상합니다! 이런 일이 처음은 아닙니다 — 저희는 이전에 거의 새로운 정보 없이 발생한 확장(scaling)에 대한 큰 서사적 변화에 회의적인 시각을 표명했습니다. 만약 단일 제품 업데이트가 AI의 궤적에 대한 사람들의 견해를 바꾼다면, 애초에 사람들의 증거 기반(evidence base)은 얼마나 신뢰할 수 있을까요? 정상 기술 프레임워크(normal technology framework)가 느린 시간표를 예측하는 이유는 역량이 한계에 부딪힐 것이기 때문이 아니라, 역량이 계속 빠르게 발전하더라도 영향은 느리고 점진적일 것이기 때문입니다. 따라서 저희는 새로운 출시에 대한 실망이 여러분을 AI를 정상 기술로 보는 것에 더 공감하게 만들어서는 안 된다고 생각합니다. 마찬가지로, 내일 발표될 새로운 돌파구도 저희의 견해에 대해 더 회의적으로 만들어서는 안 됩니다. GPT-5를 이해하는 가장 좋은 방법은, 이는 AI 개발자들이 모델에서 제품으로 강조점을 전환하는 특히 좋은 예시라는 것입니다. 저희는 이에 대해 1년 전에 글을 썼습니다. 자동 모델 전환기(automatic model switcher)는 ChatGPT의 일상 사용자들에게 큰 의미가 있습니다. 출시된 지 거의 1년이 지나도 “사고(thinking)” 모델을 사용하는 사람이 거의 없었지만, GPT-5는 그 사용량을 극적으로 증가시켰습니다. 일부 통신에서 알트만(Altman)은 GPT-5의 강조점이 역량의 도약이 아니라 유용성이었음을 분명히 밝혔지만, 이 메시지는 불행히도 끊임없는 과대광고(hype)로 인해 약화되어 실망으로 이어졌습니다. 업계의 이러한 광범위한 변화는 사실 기업들 스스로 (마지못해) 성공으로 가는 길이 AGI(인공 일반 지능) 또는 초지능(superintelligence)을 구축하기 위해 경쟁하고 그것이 모든 확산 장벽을 쓸어버릴 것이라고 기대하는 대신, 제품을 만들고 채택을 촉진하는 힘든 작업을 하는 것임을 인정하게 되는 것과 매우 일치합니다. 아이러니하게도, 이 서사에서 GPT-5는 실패가 아니라 성공의 예시입니다. 사실, 모델 개발자들은 더 유용한 제품을 개발하는 것(저희 기술 개발 및 채택 프레임워크(technology development & adoption framework)의 두 번째 단계)을 넘어 배포자(deployer)와 협력하여 초기 채택의 어려움을 완화하는 것(세 번째 단계)으로 나아가기 시작했습니다. 예를 들어, OpenAI의 선행 배포 엔지니어(Forward Deployed Engineers)는 존 디어(John Deere)와 같은 고객 및 농부들과 직접 협력하여 살충제 적용을 위한 맞춤형 권장 사항 제공과 같은 역량을 통합하고 배포하고 있습니다.

**정상 기술로서의 AI(AI as Normal Technology)와 AI 2027 사이에서 “중간 지점”을 찾기 어려운 이유**

많은 사람들이 AI 2027과 정상 기술로서의 AI(AI as Normal Technology) 사이에서 중간 지점의 입장을 명확히 표현하려 노력했으며, 아마도 이들을 견해 스펙트럼(spectrum)의 양극단으로 보았을 것입니다. 이는 놀랍게도 어려운 일입니다. AI 2027과 정상 기술로서의 AI(AI as Normal Technology)는 모두 일관된 세계관입니다. 이들은 기술이 사회에 어떻게 영향을 미칠지에 대한 매우 다른 인과적 이해를 나타냅니다. 만약 여러분이 섞어서 맞추려 한다면, 내부적으로 일관성 없는 잡동사니로 끝날 위험이 있습니다. (덧붙여 말하자면, 이는 저희가 결국 틀렸다면, 조금 틀리기보다는 완전히 틀릴 가능성이 더 높다는 것을 의미합니다.) 게다가, 실리콘 밸리(Silicon Valley) 거품 속에서만 정상 기술로서의 AI(AI as Normal Technology)가 회의적인 시각으로 간주될 수 있습니다! 저희는 에세이의 두 번째 문장에서 AI를 전기에 비유하며, AI가 심오한 영향을 미 미칠 것이라고 예상한다는 점을 내내 분명히 합니다. AI가 노동에 미칠 영향에 대한 저희의 기대는 이 주제를 연구하는 경제학자들의 예상 범위 중 더 공격적인 끝에 있는 것으로 보입니다. 요컨대, 온건한 입장을 찾고 있다면, 에세이를 전체적으로 읽어보시기를 권합니다. 제목 때문에 저희가 AI 회의론자라고 생각하게 하지 마십시오. 아마도 여러분은 정상 기술로서의 AI(AI as Normal Technology)가 이미 여러분이 찾고 있는 중간 지점이라고 결론 내릴 것입니다. 저희는 AI의 미래에 대해 생각하는 가장 널리 논의되는 두 가지 프레임워크(framework)가 그렇게 근본적으로 다르다는 것이 불편할 수 있다는 것을 알고 있습니다. (저희 에세이 자체는 4부에서 정책에 관한 이 상황에 대해 많은 논평을 제공합니다.) 저희는 몇 가지 위로가 되는 생각을 제공할 수 있습니다: 저희는 AI 2027 저자들과 많은 합의점을 가지고 있습니다. 저희는 그러한 분야를 설명하는 공동 성명을 작성 중입니다. 이 노력을 조직해 준 니콜라스 칼리니(Nicholas Carlini)에게 감사드립니다. 저희의 견해로는, 신념의 일치보다 더 중요한 것은 신념의 차이에도 불구하고 정책에서의 공통점입니다. 서로 다른 측이 동의할 수 있는 비교적 “쉬운” 정책 개입조차도 실제로는 엄청난 도전이 될 것입니다. 만약 저희가 이것들을 달성할 수 없다면, 임박한 초지능(superintelligence)을 걱정하는 사람들이 선호하는 훨씬 더 급진적인 조치에 대한 희망은 거의 없습니다. 의견 불일치의 핵심(cruxes)을 식별하고 두 세계관 사이를 판단하는 데 도움이 될 수 있는 지표에 동의하기 위한 몇 가지 지속적인 노력이 있었습니다. 저희는 이러한 노력 중 일부에 참여했으며 앞으로도 계속 참여하기를 기대합니다. 이 분야에서의 골든 게이트 AI 연구소(Golden Gate Institute for AI)의 노력에 감사드립니다. 지표 개발에 대해 말하자면, 저희는 프로젝트 HAL, 즉 총체적 에이전트 리더보드(Holistic Agent Leaderboard)에 대한 비전을 확장하는 과정에 있습니다. 현재는 AI 에이전트(AI agent)를 위한 더 나은 벤치마크 오케스트레이션 시스템(benchmark orchestration system)이 되려고 노력하고 있지만, 새로운 계획은 AI 커뮤니티(AI community)가 AI 에이전트(AI agent)가 다양한 영역에서 변혁적인 실제 세계 영향(transformative real-world impacts)을 위한 역량 임계값(capability thresholds)을 언제 넘어섰는지 식별하는 데 도움이 되는 조기 경보 시스템(early warning system)으로 개발하는 것입니다. 저희는 이러한 역량 임계값(capability thresholds)을 영향에 대한 필수적이지만 항상 충분한 조건은 아니라고 보며, 이러한 임계값에 도달하면 이점과 위험 모두에 대한 비기술적 장벽에 대한 저희의 논지를 훨씬 더 날카롭게 압박할 것입니다. HAL은 예측에 관한 것이 아니라 현재 상황 인식(situational awareness)에 관한 것임을 주목하십시오. 이것이 저희 작업의 주제입니다. 일반적으로 AI 담론에서, 특히 저희와 AI 2027 사이에서 놀라운 점은 미래뿐만 아니라 확산 속도(아래에서 더 자세히 설명)와 같이 저희가 관찰할 수 있는 것들에 대한 견해의 폭이 넓다는 것입니다. 저희가 커뮤니티(community)로서 현재에 대한 측정과 진보에 대한 경쟁적인 인과적 설명(causal explanations)을 테스트하는 데 훨씬 더 능숙해지지 않는 한, 예측에 투입되는 에너지 수준이 잘못된 방향으로 흐를 것입니다. 왜냐하면 그러한 예측을 해결할 방법이 부족하기 때문입니다. 예를 들어, 저희는 “AGI(인공 일반 지능)”가 사후적으로(post facto) 구축되었는지조차 반드시 알 수 없을 것이라고 주장했습니다. 어느 정도 이러한 한계는 AGI와 같은 개념의 개념적 정밀성(conceptual precision) 부족으로 인해 본질적이지만, 동시에 저희가 측정에서 훨씬 더 잘할 수 있다는 것도 사실입니다.

**다른 세계관에 몰두해 있을 때 하나의 세계관을 이해하기는 어렵습니다**

저희는 다음과 같이 썼습니다: 정상 기술로서의 AI(AI as normal technology)는 임박한 초지능(superintelligence)으로서의 AI 세계관과 대조되는 세계관입니다. 세계관은 가정, 어휘, 증거 해석, 인식론적 도구(epistemic tools), 예측, 그리고 (가능성 있는) 가치로 구성됩니다. 이러한 요소들은 서로를 강화하고 각 세계관 내에서 긴밀한 묶음을 형성합니다. 이는 세계관 간의 소통을 어렵게 만듭니다. 예를 들어, AI 2027 관계자들로부터 저희가 자주 받는 질문 중 하나는 2027년에 세상이 어떻게 보일 것이라고 생각하느냐는 것입니다. 저희는 2025년의 세상과 거의 비슷할 것이라고 답합니다. 그러면 그들은 세상이 변혁될 2035년이나 2045년 또는 그 어떤 해를 고려하도록 저희를 압박하며, 저희가 구체적인 시나리오(scenario)를 제공하지 않는 것을 저희 프레임워크(framework)의 결함으로 간주합니다. 그러나 이러한 종류의 시나리오 예측(scenario forecasting)은 그들의 세계관 내에서만 의미 있는 활동입니다. 저희는 저희가 구체적으로 다룰 수 있다고 생각하는 것들에 대해 구체적입니다. 동시에, 저희는 AI 2027을 포함하여 근본적으로 다른 미래를 가능하게 하는 인간, 제도, 정치적 행위자(agency)의 역할을 강조합니다. 따라서 정상 기술로서의 AI(AI as normal technology)는 예측만큼이나 처방입니다. 이러한 소통의 어려움은 AI 2027 저자 중 한 명인 스콧 알렉산더(Scott Alexander)가 정상 기술로서의 AI(AI as Normal Technology)에 대해 내놓은 반응을 고려할 때 중요하게 염두에 두어야 합니다. 저희는 그의 대화 노력이 선의의 노력이라는 점을 의심하지 않으며, 그가 시간을 할애해 준 것에 감사하지만, 유감스럽게도 그의 반응은 저희의 말을 제대로 이해하지 못하고 엇나간 대화를 한다고 느낍니다. 그가 의견 불일치의 핵심(cruxes)으로 식별하는 것들은 저희가 핵심으로 여기는 것들과는 상당히 다릅니다! 이러한 이유로, 저희는 항목별 답변을 하지 않을 것입니다. 왜냐하면 저희도 결국 그와 엇나간 대화를 하게 될 것이기 때문입니다. 하지만 저희는 조정된 대화에 기꺼이 참여할 것이며, 이는 지난 1년 동안 8~10회 성공적으로 참여했던 형식입니다. 동시적인 특성은 서로를 이해하는 것을 훨씬 더 쉽게 만듭니다. 그리고 비공개 대화가 공개되기 전에 편집될 것이라는 사실은 각 측이 상대방의 관점을 이해하기 위해 어리석은 질문을 하는 것을 더 쉽게 만듭니다. 어쨌든, 알렉산더(Alexander)의 반응이 저희의 말을 제대로 이해하지 못하고 엇나간 대화를 하는 몇 가지 중요한 방식은 다음과 같습니다. 재귀적 자기 개선(Recursive Self-Improvement, RSI)은 알렉산더(Alexander)의 관점에서 의견 불일치의 핵심(crux)이며, 그는 저희에게는 거의 언급할 가치도 없다는 점에 놀라워합니다. 솔직히 저희는 에세이에서 RSI에 대해 저희가 어떻게 생각하는지 훨씬 더 명확히 설명할 수도 있었습니다. 요컨대, 저희는 RSI가 초지능(superintelligence)으로 이어질 것이라고 생각하지 않습니다. 왜냐하면 강력한 AI 시스템을 구축하고 배포하는 데 대한 외부 병목 현상은 기술 설계를 개선하는 것만으로는 극복될 수 없기 때문입니다. 이것이 저희가 그것을 많이 논의하지 않는 이유입니다. 3 비록 저희에게 핵심(crux)은 아니지만, 저희는 AI 커뮤니티(AI community)가 RSI에 전혀 근접하지 못했다고 생각하는 이유를 에세이에서 설명합니다. 최근에는 해결해야 할 근본적인 연구 과제에 대해 생각하고 있는데, 저희가 깨달았던 것보다 훨씬 더 많습니다. 그리고 AI 커뮤니티(AI community)가 다른 과학 커뮤니티(community)에 비해 진보를 위한 새로운 패러다임(paradigm)을 찾는 데 특히 서툴 수 있다는 점을 염두에 둘 가치가 있습니다. 다시 말하면, 이것은 저희 프로젝트 HAL이 진보를 측정하는 데 역할을 할 수 있기를 바라는 분야입니다. 알렉산더(Alexander)의 반응이 저희의 말을 제대로 이해하지 못하고 엇나간 대화를 하는 또 다른 주제는 확산 속도에 관한 것입니다. 이에 대해서는 아래에서 간략하게 언급하고 향후 에세이에서 더 자세히 다룰 것입니다. 세계관 간의 담론의 어려움을 가장 잘 보여주는 것은 예측이나 설득과 같은 작업에서 인간 초월적인 AI 능력이 가능한지 여부에 대한 저희의 가설에 대한 알렉산더(Alexander)의 논의입니다. 그의 반응을 여러 번 읽은 후에도 저희는 정확히 어디에서 동의하고 어디에서 동의하지 않는지 파악하기 어렵습니다. 저희는 다음과 같이 썼습니다: 저희는 인간의 한계가 너무나 명확하여 AI가 인간의 성능을 훨씬 뛰어넘을 수 있는 (AI가 체스에서 그러하듯이) 실제 세계의 인지 작업은 상대적으로 거의 없다고 생각합니다. ... 구체적으로, 저희는 두 가지 영역을 제안합니다: 예측과 설득. 저희는 AI가 지정학적 사건(예: 선거)을 예측하는 데 훈련된 인간(특히 인간 팀, 그리고 간단한 자동화 도구로 보강된 경우)을 의미 있게 능가할 수 없을 것이라고 예측합니다. 저희는 사람들이 자신의 이익에 반하여 행동하도록 설득하는 작업에 대해서도 동일한 예측을 합니다. 그의 에세이 3B 섹션에서 이에 대한 그의 전체 반응을 읽을 수 있지만, 요컨대 그는 인간의 생물학적 한계에 초점을 맞춥니다: 인간은 아프리카 사바나(savanna)에서 수천 년의 진화를 통해 능력을 얻었습니다. 사바나에서는 “예측 대회에서 가능한 가장 높은 브라이어 점수(Brier score)를 정확히 얻는 것”에 대한 특별한 압력이 없었으며, 인간이 이를 달성했다고 생각할 특별한 이유도 없습니다. 실제로 농업에 대한 반응으로 지난 10,000년 동안 인간 지능의 진화에 대한 증거가 사실로 판명된다면, 인간은 아프리카 사바나에서 우주적 최대치에 도달하지 못했습니다. 왜 우리는 이 마지막, 매우 짧은 선택의 라운드가 정확히 옳았다고 생각해야 할까요? 그러나 인간 능력에 대한 생물학적 개념을 거부하는 것은 저희에게 핵심 출발점이며, 저희는 “인간의 능력은 생물학에 의해 제약받지 않는다(Human Ability Is Not Constrained by Biology)” 섹션에서 이를 자세히 설명하기 위해 노력합니다. 이것이 세계관 간의 논의의 문제입니다: 만약 여러분이 특정 진술을 취하고, 그 진술에 이르는 전제와 용어적 명확화(terminological clarifications)를 무시하고, 그것을 여러분의 세계관으로 해석한다면, 상대방이 무지하다고 생각할 것입니다. 알렉산더(Alexander)는 저희가 사바나 거주자가 현재로 시간 여행을 한다면 선거를 예측할 수 있을 것이라고 제안한다고 생각할까요? 그는 인간의 성능이 고정되어 있지 않다는 점을 강조하지만, 어쩐지 이를 저희 논지에 대한 반박(오히려 핵심적인 부분이라기보다는)으로 봅니다. 아마도 혼란은 예측에서 인간의 성능이 “환원 불가능한 오류(irreducible error)”에 가깝다는 저희의 가설 때문에 발생했을 것입니다. 저희는 예측의 환원 불가능한 오류(irreducible error)가 영원히 고정된 특정 숫자라고 암시하는 것이 아닙니다. 물론 그것은 사용 가능한 데이터에 따라 달라집니다 — 더 나은 여론조사는 더 나은 예측으로 이어집니다 — 그리고 증가된 데이터를 활용하는 데 도움이 되는 훈련에 따라 달라집니다. 그리고 그 훈련 중 일부는 AI 기반 예측 연구의 결과일 수도 있습니다. 저희는 원래 에세이에서 인간 지능이 저희의 생물학 때문이 아니라 AI를 포함한 저희 도구에 대한 (우발적인) 숙달 때문에 특별하다고 강조합니다. 따라서 AI의 발전은 종종 인간 지능(능력)을 향상시킬 것이며, 저희가 제안하는 인간-AI 비교의 양측 성능을 향상시킬 잠재력을 가지고 있습니다. 저희 가설의 요점은 간단합니다: 저희는 예측이 체스와 같다고 생각하지 않습니다. 체스에서는 많은 계산이 AI에게 결정적인 속도 우위를 제공할 수 있습니다. 예측의 계산 구조(computational structure)는 성능이 훈련과 데이터를 통해 크게 향상될 수 있음에도 불구하고 비교적 간단합니다. 따라서 적절하게 훈련된 전문가 예측가 팀의 손에 있는 비교적 간단한 계산 도구는 짜낼 수 있는 (거의) 모든 것을 짜낼 수 있습니다. 저희는 알렉산더(Alexander)의 반응이 “상호 협력 가능성에 대해 말뿐이 아니라 실제로 행동한다”는 점을 저희에게 인정해 준 것에 기쁩니다. 그 감정은 상호적입니다. 저희는 서브스택(Substack) 반박과 재반박보다 더 생산적이라고 생각하는 그 협력을 계속하기를 기대합니다.

**생성형 AI와 인간 창의성 사이의 새로운 협력 모델**

생성형 AI의 발전은 인간의 창의성에 대한 새로운 질문을 던지고 있습니다. 많은 사람들이 AI가 인간의 예술적, 지적 영역을 침범할 것이라고 우려하지만, 저희는 오히려 새로운 협력 모델이 나타나고 있다고 생각합니다. AI는 아이디어 생성, 초안 작성, 반복 작업 처리 등에서 강력한 보조 도구가 될 수 있습니다. 이는 예술가, 작가, 디자이너, 연구자들이 보다 고차원적인 창의적 문제 해결에 집중할 수 있도록 돕습니다. 예를 들어, AI 기반 디자인 도구는 제품 개발 과정에서 수많은 시안을 빠르게 생성하여 디자이너가 최적의 솔루션을 찾는 시간을 단축시킵니다. 음악 분야에서는 AI가 새로운 멜로디나 화음 패턴을 제안하여 작곡가의 영감을 자극하고, 문학 분야에서는 AI가 줄거리 아이디어를 제공하거나 문체 교정을 도와 작가의 생산성을 향상시킵니다. 이러한 협력은 AI가 단순히 작업을 자동화하는 것을 넘어, 인간의 잠재력을 증폭시키는 역할을 할 수 있음을 보여줍니다. 중요한 것은 AI를 맹목적으로 따르는 것이 아니라, AI가 생성한 결과물을 비판적으로 평가하고, 인간의 통찰력과 윤리적 판단을 결합하여 최종 결과물을 만들어내는 것입니다. 이러한 인간-AI 협력은 미래 사회에서 필수적인 역량이 될 것이며, 교육 시스템 또한 이러한 변화에 발맞춰 창의적 사고와 비판적 판단력을 강조하는 방향으로 진화해야 할 것입니다.

**인간 중심 AI 디자인의 중요성**

세계관 간의 소통을 어렵게 만듭니다. 하지만 AI의 성공적인 통합을 위해서는 이러한 장벽을 넘어 공동의 이해를 구축하는 것이 필수적입니다. 저희는 AI 개발의 초기 단계부터 인간 중심 디자인(Human-Centered Design) 원칙을 적용하는 것이 중요하다고 강조합니다. 이는 기술이 사용자에게 어떻게 영향을 미치고, 어떤 가치를 제공하며, 잠재적인 위험은 무엇인지 깊이 이해하는 것에서 시작됩니다. 예를 들어, AI 시스템이 의사 결정을 내릴 때, 그 과정이 투명하고 설명 가능해야 합니다. 사용자는 AI가 왜 특정 결론에 도달했는지 이해할 수 있어야 하며, 필요한 경우 이의를 제기하거나 수정할 수 있는 메커니즘이 마련되어야 합니다. 이는 AI의 신뢰성을 높이고, 사용자의 수용도를 증가시키는 데 기여합니다. 또한, AI 디자인 과정에는 다양한 배경과 관점을 가진 이해관계자들이 참여해야 합니다. 이는 AI 시스템이 특정 집단에게 편향되거나 불공정한 결과를 초래하는 것을 방지하고, 모든 사용자에게 공평하고 포용적인 경험을 제공할 수 있도록 돕습니다. AI의 이점은 기술 자체의 능력뿐만 아니라, 그 기술이 인간의 필요와 가치에 얼마나 잘 부합하도록 설계되었는지에 달려있습니다. 따라서 기술적 우수성만큼이나 윤리적 고려와 사회적 책임이 AI 개발의 핵심 요소가 되어야 합니다.

**AI의 이점을 얻으려면 많은 노력과 고통스러운 선택이 필요할 것입니다**

저희 프레임워크(framework)에는 두 가지 광범위한 함의가 있습니다: 하나는 경제와 노동에 대한 것이고, 다른 하나는 안전에 대한 것입니다. 몇 가지 기본 전제(특히, 초지능(superintelligence)은 정의에 따라 비일관적이거나 불가능하다는 것)를 넘어서면, 이 두 가지 함의 뒤에 있는 저희의 주장은 대체로 다릅니다. 경제적 영향에 대해, 저희의 주장은 광범위하게 확산 장벽이 역량 개선을 통해 극복되지 않을 것이라는 것입니다. 안전에 대해서는, 저희의 주장은 주로 정렬(alignment) 없이 AI 제어가 가능할 뿐만 아니라, 특별히 어렵게 보이지도 않으며, 과학적 돌파구도 필요하지 않다는 것입니다. 이 두 가지 주장이 크게 겹치지 않기 때문에, 한 가지 주장은 받아들이고 다른 주장은 받아들이지 않거나 (또는 양가적인 태도를 취하는 것이) 일관성이 있습니다. 실제로 경제적 영향에 대한 저희의 견해는 독자들에게 특히 강하게 공감대를 형성한 것 같습니다. 에세이 출간 이후, 저희는 다양한 산업 분야에서 AI 전략을 담당하는 사람들과 많은 논의를 가졌습니다. 저희는 그들이 AI에 대해 생각했던 방식이 저희와 일치했지만, 모든 과대광고(hype) 때문에 자신들의 접근 방식에 대해 재고하기 시작했다는 것을 발견했습니다. 저희 에세이는 현장에서의 관찰뿐만 아니라 그들의 직관을 뒷받침하는 일관된 프레임워크(framework)를 제공했습니다. 4 AI를 배포하는 사람들은 기술 개발과 확산의 차이를 예리하게 이해하고 있지만, 저희 프레임워크(framework)는 각각을 두 단계로 더 나눕니다. 개발 측면에서는 모델과 제품, 또는 역량과 애플리케이션(application) 사이의 간극을 강조합니다. 확산 측면에서는 사용자 학습 곡선(user learning curves)과 개인의 적응의 다른 측면, 그리고 종종 집단 행동을 요구하는 구조적, 조직적, 또는 법적 변화를 구별합니다. 저희는 각 4단계에서 작용하는 속도 제한의 종류를 설명합니다. 사용자 행동은 적어도 느리지만 예측 가능하게 변하는 경향이 있지만, 조정 문제를 해결하거나 경직된 제도(sclerotic institutions)를 개혁하는 것 — 이는 효과적인 기술 채택을 위한 전제 조건이기도 합니다 — 은 훨씬 더 불확실합니다. 예를 들어, 현대화 부족으로 인한 막대한 비용이 명확해지고 있음에도 불구하고 항공 교통 관제(Air Traffic Control)가 20세기 중반 기술에 갇혀 있는 방식을 생각해 보십시오. 저희 에세이는 AI의 경우에도 유사한 확산 장벽이 존재한다는 점을 지적했지만, 저희는 이제야 그러한 장벽을 명확히 설명하고 필요한 특정 개혁을 식별하는 작업을 하고 있습니다. 저희는 이 분야에 대해 더 많은 글을 쓸 것이며, 그 중 일부는 저스틴 컬(Justin Curl)과 협력할 것입니다. 첨단 AI가 이미 고도로 기술적이고 고도로 규제된 세상으로 진입하고 있다는 점을 염두에 둘 가치가 있습니다. 저희는 AI가 다루는 워크플로우(workflow)의 부분들이 병목 현상이 될 가능성이 낮다는 것을 반복적으로 발견합니다. 왜냐하면 사용 가능한 생산성 향상의 대부분은 이미 이전 기술의 물결을 통해 달성되었기 때문입니다. 한편 실제 병목 현상은 규제나 다른 외부 제약으로 인해 저항력을 보입니다. 법률 서비스와 과학 연구를 포함한 많은 특정 영역에서 경쟁 역학(competitive dynamics)이 너무 강해서 AI로 인한 생산성 향상이 궁극적으로 사회적 가치로 이어지지 않는 군비 경쟁의 심화로 이어집니다.

**확산 속도에 대한 초현실적인 논쟁**

저희는 서로 다른 진영들이 현재 AI의 영향력을 어떻게 특징짓는지에 대해 이견을 보이고 있다는 점을 여러 번 언급했습니다. 확산 속도보다 더 명확하게 드러나는 곳은 없습니다. AI 지지자들은 AI가 전례 없이 빠르게 채택되고 있다고 믿습니다. 저희는 전적으로 동의하지 않습니다. 더 나쁜 것은, 더 많은 증거가 나올수록 각 측은 자신들의 해석에 대해 더 확신하는 것처럼 보인다는 것입니다. 저희는 확산 속도에 대한 심층 분석을 진행 중입니다. 현재로서는, “빠른 채택” 해석을 정당화하기 위해 제시되는 일반적인 주장과 통계에서 몇 가지 기본적인 오류를 지적합니다. 첫째, 배포(deployment)는 확산(diffusion)이 아닙니다. 종종 사람들은 빠른 채택에 대해 이야기할 때, 역량이 개발되면 수억 명의 사용자가 사용하는 제품(예: 챗봇(chatbot))에 거의 즉시 배포될 수 있다는 것을 의미합니다. 그러나 이것은 확산이 의미하는 바가 아닙니다. 얼마나 많은 사람들이 역량에 접근할 수 있는지 아는 것만으로는 충분하지 않습니다: 중요한 것은 얼마나 많은 사람들이 실제로 그것을 사용하고 있는지, 얼마나 오랫동안 사용하고 있는지, 그리고 무엇을 위해 사용하고 있는지입니다. 그러한 세부 사항들을 깊이 파고들면, 그림은 매우 다르게 보입니다. 예를 들어, ChatGPT에서 자랑스럽게 출시된 “사고(thinking)” 모델이 출시된 지 거의 1년이 지나도, 사용자의 1% 미만이 매일 그것을 사용했습니다! 이것이 저희 논지를 뒷받침함에도 불구하고, 저희는 이 점을 지적하는 데 즐거움을 느끼지 않습니다. AI의 열렬한 얼리 어답터(early adopters)로서, 이러한 숫자는 너무 낮아서 저희가 직관적으로 이해하기 어렵고, 솔직히 꽤나 우울합니다. 오해의 소지가 있는 또 다른 통계는 특정 고위험 영역의 근로자 중 AI를 사용하는 비율과 관련이 있습니다. 이러한 통계는 AI가 위험한 방식으로 빠르게 채택되고 있다는 주장을 뒷받침하기 위해 제시되는 경향이 있습니다. 그러나 고위험 영역에서도 대부분의 작업은 실제로는 평범하며, 특정 용도를 깊이 파고들면 전혀 위험해 보이지 않습니다. 예를 들어, 미국 의사 협회(American Medical Association)의 설문조사에 따르면 대다수의 의사가 AI를 사용하고 있다고 보고했습니다. 그러나 여기에는 구술된 메모의 전사(transcription)와 같은 것들이 포함됩니다. 5 또한 진단에 대한 두 번째 의견을 챗봇(chatbot)에 묻는 것과 같은 것들도 포함됩니다 (2024년에는 약 12%가 이 사용 사례(use case)를 보고했는데, 이는 2023년 11%에서 1%포인트 증가한 엄청난 수치입니다). 이것은 전사(transcription)보다 확실히 더 심각한 사용이지만, 여전히 충분히 근거 있는 것입니다. 저희가 이전에 지적했듯이, 신뢰할 수 없는 AI조차도 오류 감지(error detection)에 매우 유용합니다. 이러한 작업에 대한 AI 채택 증가는 의사들이 무모하게 행동하고(YOLOing it) ChatGPT에 결정을 위임함으로써 환자에 대한 책임을 포기하려 한다는 것을 의미하지 않습니다. 대다수의 의사들은 이 두 가지 유형의 사용 간의 차이를 이해하고 있으며, 의료 과실 책임, 직업 윤리 강령, 의료 기기 규제를 포함하여 의료 전문가 내에서 광범위한 무책임한 사용을 방지하는 많은 중첩되는 안전장치(guardrails)가 있습니다. 가장 오해의 소지가 있는 “빠른 채택” 밈(meme)은 아마도 ChatGPT가 약 두 달 만에 1억 명의 사용자에게 도달했음을 보여주는 이 널리 공유된 차트일 것입니다: 이 차트는 ChatGPT 사용자 증가를 (1) 네트워크 효과(network effects)에 따라 유용성이 달라지므로 첫날부터 유용한 앱보다 훨씬 느리게 성장하는 특징이 있는 소셜 미디어(social media) 앱인 인스타그램(Instagram), 페이스북(Facebook), 트위터(Twitter)와 비교하고, (2) 처음에는 초대 전용이었던 앱인 스포티파이(Spotify)와 (3) 제한된 재고로 출시되었고 사용하려면 구독이 필요했던 서비스인 넷플릭스(Netflix)와 비교합니다. 6 이 차트에 반영된 것은 앱에 대한 소문이 돌면 확인해 보는 얼리 어답터(early adopters)이며, ChatGPT에 대한 엄청난 소문이 있었습니다. 이러한 호기심 많은 초기 사용자들을 소진하면, 성장 곡선은 매우 다르게 보입니다. 사실, 1년 후 ChatGPT는 1억 명에서 2억 명의 사용자로만 성장한 것으로 보이며, 이는 곡선이 오른쪽으로 급격히 꺾였다는 것을 분명히 보여줍니다. 이는 처음 두 달만을 반영하는 이 그래프에는 편리하게도 포착되지 않았습니다. 이 차트는 확산에 대한 일반적인 장벽이 약화되거나 제거되었다는 증거를 제공한다면 유용할 것입니다. 하지만 그렇지 않습니다. 두 달은 사용자들이 AI를 생산적으로 통합하기 위해 워크플로우(workflow)를 조정하는 것과 같이 확산의 어려운 부분이 시작되기에도 충분한 시간이 아닙니다. 따라서 이 차트는 확산 속도에 대한 어떤 의미 있는 논의와도 무관합니다. 이 차트에는 다른 많은 문제점들이 있지만, 여기에서 멈추겠습니다. 7 다시 말하면, 이것은 AI 확산 속도에 대한 완전한 분석과는 거리가 멀며 — 그것은 곧 나올 것입니다. 현재로서는, 이 주제에 대한 대부분의 논평이 단순히 진지하지 않다는 점을 지적하는 것뿐입니다. 그리고 저희가 데이터를 가지고 있는 질문에 대한 담론이 이렇다면, 서로 다른 진영의 미래 예측이 서로 전혀 닮지 않았다는 것은 놀라운 일이 아닙니다.

**AI 채택이 다르게 느껴지는 이유**

만약 “빠른 확산” 밈(meme)이 그렇게 잘못되었다면, 왜 그렇게 널리 퍼져 있고 지속적일까요? 왜냐하면 AI 채택은 PC나 인터넷, 소셜 미디어(social media)가 결코 그러지 않았던 방식으로 쓰나미처럼 느껴지기 때문입니다. 사람들이 어떤 것에 대해 직관적으로 확신할 때, 그들은 그러한 느낌을 확인시켜준다고 주장하는 데이터나 차트에 훨씬 덜 회의적일 것입니다. 물론 저희도 그 느낌을 압니다. AI에 대한 저희의 실제 경험은 과거 기술의 물결과는 다릅니다. 처음에는 저희는 이것을 인지 편향(cognitive bias)으로 치부했습니다. 저희가 현재 겪고 있는 어떤 변화든 과거에 성공적으로 적응했던 것보다 훨씬 더 큰 변화처럼 느껴질 것입니다. 저희는 이제 저희가 틀렸다는 것을 깨달았습니다. 인지 편향(cognitive bias)이 설명의 작은 부분일 수 있지만, AI 채택이 훨씬 더 빠르고 무섭게 느껴지는 진짜 이유가 있습니다. 요컨대, 배포(deployment)가 확산(diffusion)이 아니라는 것은 사실이지만, 과거에는 점진적인 배포(deployment)가 사용자들이 채택에 대한 결정을 끊임없이 내려야 하는 부담으로부터 어느 정도 보호받았다는 것을 의미했지만, 이제 그 완충 장치가 사라졌습니다. 인터넷 채택과의 비교를 통해 설명해 보겠습니다. 90년대에 다이얼업 인터넷(dial-up internet)을 채택했던 사람들은 다음과 같은 이야기를 기억할 것입니다. 처음 기술에 대해 들었을 때, 저희는 PC의 높은 가격 때문에 망설였습니다. 점차 가격이 내려갔습니다. 그동안 저희는 직장이나 친구 집에서 인터넷을 사용하는 경험을 얻었습니다. 그래서 몇 년 후 PC와 다이얼업 인터넷(dial-up internet)을 구입했을 때, 저희는 이미 어느 정도 훈련이 되어 있었습니다. 처음에는 다이얼업이 느리고 비쌌으며 웹사이트도 그렇게 많지 않았기 때문에 인터넷을 많이 사용하지 않았습니다. 점차 가격이 내려가고, 대역폭(bandwidth)이 개선되었고, 더 많은 콘텐츠가 온라인에 등장했으며, 저희는 사용량 증가와 함께 인터넷을 생산적이고 안전하게 사용하는 방법을 배웠습니다. 2020년대에 범용 AI 도구(general-purpose AI tools)를 채택하는 것은 새로운 역량의 배포(deployment)가 즉각적이기 때문에 근본적으로 다른 경험입니다. 사람들은 특정 사용 사례(use case)에 AI를 채택할지 여부를 평가하는 데 훨씬 더 많은 시간을 할애해야 하며, 채택하지 않으면 뒤처질 것이라는 말을 끊임없이 듣습니다. 저희의 이전 모든 요점은 여전히 유효합니다 — 학습 곡선(learning curves)이 존재하며, 인간 행동은 변화하는 데 오랜 시간이 걸리고, 조직 변화는 훨씬 더 오래 걸립니다. 그러나 AI를 사용하지 않는 것은 어느 정도 능동적인 선택이며, 사람들은 더 이상 접근할 수 없기 때문에 그것에 대해 생각하지 않아도 된다는 변명을 할 수 없습니다. 요컨대, 배포(deployment)는 확산(diffusion)의 여러 단계 중 하나일 뿐이며, 그 병목 현상을 제거하는 것이 확산을 약간 더 빠르게 만들었을 것입니다. 그러나 특정 AI 사용 사례(use case)에 대해 듣자마자, 궁극적으로는 합리적이거나 비합리적일 수 있는 다양한 이유로 대다수의 사람들이 채택하지 않기로 결정하는 경우라 할지라도, 채택할지 말지를 결정해야 하기 때문에 극적으로 더 빠르게 느껴집니다.

**결론**

저희가 AI 지지자들과 분명히 동의하는 한 가지는 AI가 사라지지 않을 것이며, 대부분의 사람들이 무시할 수 있는 암호화폐(crypto)와 같은 틈새시장이 되지 않을 것이라는 점입니다. 생성형 AI(generative AI)에 대한 집단적인 초기 충격이 가라앉았으므로, 각각의 새로운 기술적 역량이나 비예측적 사회적 효과(emergent social effect)에 (과도하게) 반응하는 대신, AI의 영향이 어떻게 전개될지 생각하는 구조화된 방법이 필요합니다. 저희가 이 뉴스레터에서 계속해서 상세히 설명하고 있는 정상 기술로서의 AI(AI-as-normal-technology) 프레임워크(framework)는 그러한 접근 방식 중 하나입니다. 이는 적어도 기술의 사회적 영향에 대해 역사적으로 근거한 기본적 사고방식을 명확히 표현한 것으로서, 더 예외주의적인 설명과 비교될 수 있다는 점에서 익숙해질 가치가 있습니다. 이 프레임워크(framework)는 기업 리더, 근로자, 학생, AI 안전 또는 AI 윤리(AI ethics)에 관심 있는 사람들, 그리고 정책 입안자 등에게 어느 정도 실행 가능한 지침을 제공합니다. 여러분이 계속해서 저희와 함께하고 논의에 기여해 주시기를 바랍니다. 초고에 대한 피드백을 제공해 준 스티브 뉴먼(Steve Newman)과 펠릭스 첸(Felix Chen)에게 감사드립니다.

**AI 시대의 새로운 도전과 기회**

AI 기술의 발전은 단순히 생산성 향상을 넘어, 사회 구조와 인간의 삶의 방식 자체에 근본적인 변화를 가져오고 있습니다. 이러한 변화의 물결 속에서, 우리는 AI가 제공하는 무한한 기회를 활용하는 동시에, 발생할 수 있는 잠재적 위험을 최소화하기 위한 지혜를 모아야 합니다. AI 기술은 의료, 교육, 환경 보호 등 다양한 분야에서 혁신적인 해결책을 제시할 수 있습니다. 예를 들어, 맞춤형 학습 경험을 제공하거나, 복잡한 질병을 조기에 진단하며, 기후 변화에 대응하는 새로운 방법을 모색하는 데 기여할 수 있습니다. 그러나 이러한 긍정적인 측면만큼이나, 데이터 프라이버시(data privacy) 침해, 알고리즘 편향(algorithmic bias), 노동 시장의 변화와 같은 새로운 도전 과제들도 함께 등장하고 있습니다. 우리는 이러한 도전에 수동적으로 반응하기보다는, 선제적이고 협력적인 접근 방식을 통해 AI의 혜택이 모두에게 공정하게 분배되고, 그 위험은 효과적으로 관리될 수 있도록 노력해야 합니다.

AI 시대를 성공적으로 헤쳐나가기 위해서는 기술 개발자, 정책 입안자, 시민 사회 단체, 그리고 일반 대중을 포함한 모든 이해관계자들의 지속적인 대화와 협력이 필수적입니다. 기술의 발전 속도에 발맞춰 윤리적 가이드라인과 규제 프레임워크를 마련하고, AI 리터러시(AI literacy) 교육을 강화하여 모든 사람이 AI를 이해하고 책임감 있게 사용할 수 있도록 지원해야 합니다. AI는 더 이상 미래의 기술이 아니라, 현재 우리의 삶 속에 깊이 뿌리내린 현실입니다. 이 현실을 어떻게 만들어갈지는 우리 모두의 손에 달려 있습니다. 우리는 AI를 통해 더 나은 세상을 만들 수 있다는 희망을 가지고, 동시에 그 과정에서 발생할 수 있는 어려움에 대한 현실적인 인식을 바탕으로 끊임없이 배우고 적응해야 할 것입니다.

**추가 읽을거리/볼거리**

아르빈드(Arvind)는 세계은행 개발 회의(World Bank Development Conference)에서 경제 및 노동 관련 함의에 초점을 맞춰 이 논문을 발표했습니다.
아르빈드(Arvind)의 새로운 유튜브 채널(YouTube channel)은 정상 기술 관점에서 AI 개발을 논의합니다.

저희는 정상 기술로서의 AI(AI as Normal Technology)에 대한 언론 보도를 받게 되어 행운이었습니다:
지난달 뉴욕 타임스(New York Times)에는 에릭 슈미트(Eric Schmidt)와 셀리나 쉬(Selina Xu), 데이비드 월러스-웰스(David Wallace-Wells), 그리고 에즈라 클라인(Ezra Klein)이 이 에세이를 심층적으로 논의한 세 편의 오피니언(op-ed)이 실렸습니다.
지난주 이코노미스트(The Economist)는 "인공지능이 그저 “정상적인” 기술이라면 어떨까?(What if artificial intelligence is just a “normal” technology?)"라는 기사에서 정상 기술 논지를 논의했습니다.
더 뉴요커(The New Yorker)에서 조슈아 로스먼(Joshua Rothman)은 AI 2027과 정상 기술로서의 AI(AI as Normal Technology)를 대조했습니다.
프로스펙트 매거진(Prospect Magazine)에서 이든 주커만(Ethan Zuckerman)은 정상 기술로서 AI를 바라보는 유용성을 논의했습니다.
MIT 테크놀로지 리뷰(MIT Technology Review)의 제임스 오도넬(James O'Donnell)은 정상 기술로서의 AI(AI as Normal Technology) 논지를 요약했습니다.

저희의 팟캐스트(podcast) 출연 중 일부는 다음과 같습니다:
뉴욕 타임스(New York Times)의 하드 포크(Hard Fork)와 팀 오라일리(Tim O’Reilly)의 팟캐스트(podcast)에 출연한 아르빈드(Arvind); 로페어(Lawfare)의 스케일링 법칙(Scaling Laws)과 카네기(Carnegie)의 인도 해석(Interpreting India)에 출연한 사야시(Sayash).
저희는 AI 2027의 많은 저자를 포함하여 초지능(superintelligence) 세계관을 가진 많은 사람들과 대화를 나누었습니다: 사야시(Sayash)는 다니엘 코코타일로(Daniel Kokotajlo)와 엘리 리프랜드(Eli Lifland)와 토론했습니다; 아르빈드(Arvind)는 다니엘 코코타일로(Daniel Kokotajlo)와 토론했으며, 애스터리스크 매그(Asterisk Mag)에서는 아르빈드(Arvind)와 아제야 코트라(Ajeya Cotra)가 AI 발전이 속도 제한을 가지고 있는지, 그리고 우리가 어떻게 알 수 있는지에 대해 토론했습니다.

1 종종 그렇듯이, 우연한 타이밍이 에세이의 성공에 큰 역할을 했습니다. 이 에세이는 AI 2027 이후 2주 후에 발표되었지만, 이는 순전히 우연이었습니다 — 저희의 출판일은 실제로는 나이트 연구소(Knight Institute)의 AI와 민주적 자유 심포지엄(symposium)에 기반한 것이었습니다. 에세이를 출판할 기회를 준 연구소에 감사드립니다.
2 일반적으로 챗봇(chatbot)과 관련된 정신 건강 문제, 그리고 중독과 같은 문제들은 저희 책 AI 스네이크 오일(AI Snake Oil)을 포함하여 널리 인식되고 논의되어 왔습니다. 이러한 문제들은 소셜 미디어(social media)와의 비유에 기반하는 경향이 있습니다. 그러나 정신 건강 영향의 잠재력을 예측하는 것과 어떤 영향이 구체적으로 나타날지, 그리고 그것을 어떻게 피할 수 있을지 예측하는 것은 별개의 문제입니다.
3 그렇긴 하지만, 저희는 저희의 전체 논지가 틀릴 수도 있다는 것을 인정하며, RSI가 달성된다면 저희가 틀릴 가능성이 더 높습니다.
4 이 프레임워크(framework)는 고전적인 혁신 확산 이론(diffusion-of-innovations theory)에서 채택되었으며, 혁신-확산 격차(innovation-diffusion gap)의 관점에서 AI 분야의 지정학적 경쟁을 분석한 제프리 딩(Jeffrey Ding)과 같은 최근 저자들의 영향도 받았습니다.
5 여기에도 위험이 있지만, 저희는 이것이 의사들이 탐색해야 할 응용 분야라고 확신합니다.
6 사실, 포켓몬 고(Pokemon Go)와 스레드(Threads)(인스타그램(Instagram)을 기반으로 성장했기 때문에 네트워크 효과(network effects)에 의존하지 않았음)와 같이 초기 성장이 ChatGPT만큼 빠르거나 더 빨랐던 다른 앱들도 있었습니다. 그러나 다시 말하면, 저희의 더 큰 요점은 이러한 유형의 비교는 유익하지 않다는 것입니다. 스레드(Threads)는 초기 성장에도 불구하고 실패작에 가까웠습니다.
7 솔직히, 저희의 견해로는 이 그래프에서 훨씬 더 인상적인 통계는 인스타그램(Instagram)이 네트워크 효과(network effects)의 필요성에도 불구하고 단 2.5개월 만에 백만 명의 사용자에게 도달했다는 것입니다 — 이는 휴대폰 인터넷 속도가 훨씬 느렸고, 앱이 아이폰(iPhone) 전용이었으며 (!), 초기에는 주로 미국 내 18~34세 연령층에 확산되었던 2010년으로 거슬러 올라간다는 점을 고려할 때 더욱 그렇습니다.