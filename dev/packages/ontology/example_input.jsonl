{"key_idea_id":98,"concept":"Customizing LLM style and voice with RAG and fine-tuning","chunk_id":98,"chunk_text":"To adjust the LLM to a given style and voice along with fine-tuning, we will also leverage various \nadvanced retrieval-augmented generation (RAG) techniques to condition the autoregressive \nprocess with previous embeddings of ourselves.\n\nChapt","section_id":2.0,"section_title":"What is an LLM twin?","chapter_id":6,"chapter_title":"Chapter 1: Understanding the LLM Twin Concept and Its Architecture","book_id":1,"book_title":"LLM Engineers Handbook"}
{"key_idea_id":96,"concept":"Defining an LLM Twin as a personalized digital self-projection","chunk_id":96,"chunk_text":"What is an LLM Twin?\nIn a few words, an LLM Twin is an AI character that incorporates your writing style, voice, and \npersonality into an LLM, which is a complex AI model. It is a digital version of yourself projected\ninto an LLM. Instead of a generic LLM trained on the whole internet, an LLM Twin is fine-tuned \non yourself. Naturally, as an ML model reflects the data it is trained on, this LLM will incorporate \nyour writing style, voice, and personality. We intentionally used the word “projected.” As with \nany other projection, you lose a lot of information along the way. Thus, this LLM will not be you; \nit will copy the side of you reflected in the data it was trained on.","section_id":2.0,"section_title":"What is an LLM twin?","chapter_id":6,"chapter_title":"Chapter 1: Understanding the LLM Twin Concept and Its Architecture","book_id":1,"book_title":"LLM Engineers Handbook"}
{"key_idea_id":95,"concept":"Importance of understanding the LLM Twin concept before technical details","chunk_id":95,"chunk_text":"Understanding the LLM Twin concept\nThe first step is to have a clear vision of what we want to create and why it’s valuable to build it. \nThe concept of an LLM Twin is new. Thus, before diving into the technical details, it is essential \nto understand what it is, what we should expect from it, and how it should work. Having a solid \nintuition of your end goal makes it much easier to digest the theory, code, and infrastructure \npresented in this book.","section_id":2.0,"section_title":"What is an LLM twin?","chapter_id":6,"chapter_title":"Chapter 1: Understanding the LLM Twin Concept and Its Architecture","book_id":1,"book_title":"LLM Engineers Handbook"}
{"key_idea_id":94,"concept":"Overview of topics covered in the LLM Twin chapter","chunk_id":94,"chunk_text":"rstanding the LLM Twin Concept and Architecture\n2\nThese three steps are natural in building a real-world product. Even if the first two do not require \nmuch ML knowledge, it is critical to go through them to understand “how” to build the product \nwith a clear vision. In a nutshell, this chapter covers the following topics:\n•\nUnderstanding the LLM Twin concept\n•\nPlanning the MVP of the LLM Twin product\n•\nBuilding ML systems with feature\/training\/inference pipelines\n•\nDesigning the system architecture of the LLM Twin\nBy the end of this chapter, you will have a clear picture of what you will learn to build throughout \nthe book.","section_id":2.0,"section_title":"What is an LLM twin?","chapter_id":6,"chapter_title":"Chapter 1: Understanding the LLM Twin Concept and Its Architecture","book_id":1,"book_title":"LLM Engineers Handbook"}
{"key_idea_id":97,"concept":"The principle of LLM style transfer from training data","chunk_id":97,"chunk_text":"It is essential to understand that an LLM reflects the data it was trained on. If you feed it Shake-\nspeare, it will start writing like him. If you train it on Billie Eilish, it will start writing songs in \nher style. This is also known as style transfer. This concept is prevalent in generating images, too. \nFor example, let’s say you want to create a cat image using Van Gogh’s style. We will leverage the \nstyle transfer strategy, but instead of choosing a personality, we will do it on our own persona.","section_id":2.0,"section_title":"What is an LLM twin?","chapter_id":6,"chapter_title":"Chapter 1: Understanding the LLM Twin Concept and Its Architecture","book_id":1,"book_title":"LLM Engineers Handbook"}
{"key_idea_id":102,"concept":"Challenges in personal brand content creation requiring LLM Twin assistance","chunk_id":102,"chunk_text":"Why building an LLM Twin matters\nAs an engineer (or any other professional career), building a personal brand is more valuable than \na standard CV. The biggest issue with creating a personal brand is that writing content on plat-\nforms such as LinkedIn, X, or Medium takes a lot of time. Even if you enjoy writing and creating \ncontent, you will eventually run out of inspiration or time and feel like you need assistance. We \ndon’t want to transform this section into a pitch, but we have to understand the scope of this \nproduct\/project clearly.","section_id":3.0,"section_title":"Why building an LLM twin matters","chapter_id":6,"chapter_title":"Chapter 1: Understanding the LLM Twin Concept and Its Architecture","book_id":1,"book_title":"LLM Engineers Handbook"}
{"key_idea_id":105,"concept":"Ethical justification for building a personal LLM Twin","chunk_id":105,"chunk_text":"Also, it is critical to understand that building an LLM Twin is entirely moral. The LLM will be \nfine-tuned only on our personal digital data. We won’t collect and use other people’s data to try \nto impersonate anyone’s identity. We have a clear goal in mind: creating our personalized writing \ncopycat. Everyone will have their own LLM Twin with restricted access.\nOf course, many security concerns are involved, but we won’t go into that here as it could be a \nbook in itself.","section_id":3.0,"section_title":"Why building an LLM twin matters","chapter_id":6,"chapter_title":"Chapter 1: Understanding the LLM Twin Concept and Its Architecture","book_id":1,"book_title":"LLM Engineers Handbook"}
{"key_idea_id":99,"concept":"Fine-tuning LLMs with personal digital data to reflect individual style","chunk_id":99,"chunk_text":"er 1\n3\nWe will explore the details in Chapter 5 on fine-tuning and Chapters 4 and 9 on RAG, but for now, \nlet’s look at a few examples to intuitively understand what we stated previously.\nHere are some scenarios of what you can fine-tune an LLM on to become your twin:\n• \nLinkedIn posts and X threads: Specialize the LLM in writing social media content.\n• \nMessages with your friends and family: Adapt the LLM to an unfiltered version of yourself.\n• \nAcademic papers and articles: Calibrate the LLM in writing formal and educative content.\n• \nCode: Specialize the LLM in implementing code as you would.\nAll the preceding scenarios can be reduced to one core strategy: collecting your digital data (or \nsome parts of it) and feeding it to an LLM using different algorithms. Ultimately, the LLM reflects \nthe voice and style of the collected data. Easy, right?","section_id":3.0,"section_title":"Why building an LLM twin matters","chapter_id":6,"chapter_title":"Chapter 1: Understanding the LLM Twin Concept and Its Architecture","book_id":1,"book_title":"LLM Engineers Handbook"}
{"key_idea_id":100,"concept":"Key technical and ethical challenges of LLM twin creation","chunk_id":100,"chunk_text":"Unfortunately, this raises many technical and moral issues. First, on the technical side, how can \nwe access this data? Do we have enough digital data to project ourselves into an LLM? What kind \nof data would be valuable? Secondly, on the moral side, is it OK to do this in the first place? Do \nwe want to create a copycat of ourselves? Will it write using our voice and personality, or just try \nto replicate it?","section_id":3.0,"section_title":"Why building an LLM twin matters","chapter_id":6,"chapter_title":"Chapter 1: Understanding the LLM Twin Concept and Its Architecture","book_id":1,"book_title":"LLM Engineers Handbook"}
{"key_idea_id":103,"concept":"LLM Twin's application for personalized social media and article generation","chunk_id":103,"chunk_text":"We want to build an LLM Twin to write personalized content on LinkedIn, X, Instagram, Sub-\nstack, and Medium (or other blogs) using our style and voice. It will not be used in any immoral \nscenarios, but it will act as your writing co-pilot. Based on what we will teach you in this book, \nyou can get creative and adapt it to various use cases, but we will focus on the niche of generating \nsocial media content and articles. Thus, instead of writing the content from scratch, we can feed \nthe skeleton of our main idea to the LLM Twin and let it do the grunt work.","section_id":3.0,"section_title":"Why building an LLM twin matters","chapter_id":6,"chapter_title":"Chapter 1: Understanding the LLM Twin Concept and Its Architecture","book_id":1,"book_title":"LLM Engineers Handbook"}
{"key_idea_id":106,"concept":"The distinct roles and combined function of co-pilot and LLM Twin","chunk_id":106,"chunk_text":"What’s the difference between a co-pilot and an LLM Twin?\nA co-pilot and digital twin are two different concepts that work together and can be \ncombined into a powerful solution:\n•\nThe co-pilot is an AI assistant or tool that augments human users in various \nprogramming, writing, or content creation tasks.\n•\nThe twin serves as a 1:1 digital representation of a real-world entity, often \nusing AI to bridge the gap between the physical and digital worlds. For in-\nstance, an LLM Twin is an LLM that learns to mimic your voice, personality, \nand writing style.\nWith these definitions in mind, a writing and content creation AI assistant who \nwrites like you is your LLM Twin co-pilot.\n\nChapte","section_id":3.0,"section_title":"Why building an LLM twin matters","chapter_id":6,"chapter_title":"Chapter 1: Understanding the LLM Twin Concept and Its Architecture","book_id":1,"book_title":"LLM Engineers Handbook"}
{"key_idea_id":104,"concept":"Understanding the benefits and specialization of an LLM Twin","chunk_id":104,"chunk_text":"Understanding the LLM Twin Concept and Architecture\n4\nUltimately, we will have to check whether everything is correct and format it to our liking (more \non the concrete features in the Planning the MVP of the LLM Twin product section). Hence, we proj-\nect ourselves into a content-writing LLM Twin that will help us automate our writing process. It \nwill likely fail if we try to use this particular LLM in a different scenario, as this is where we will \nspecialize the LLM through fine-tuning, prompt engineering, and RAG.\nSo, why does building an LLM Twin matter? It helps you do the following:\n•\nCreate your brand\n•\nAutomate the writing process\n•\nBrainstorm new creative ideas","section_id":3.0,"section_title":"Why building an LLM twin matters","chapter_id":6,"chapter_title":"Chapter 1: Understanding the LLM Twin Concept and Its Architecture","book_id":1,"book_title":"LLM Engineers Handbook"}
{"key_idea_id":101,"concept":"Understanding the rationale and moral justification for an LLM Twin","chunk_id":101,"chunk_text":"Remember that the role of this section is not to bother with the “What” and “How” but with the \n“Why.” Let’s understand why it makes sense to have your LLM Twin, why it can be valuable, and \nwhy it is morally correct if we frame the problem correctly.","section_id":3.0,"section_title":"Why building an LLM twin matters","chapter_id":6,"chapter_title":"Chapter 1: Understanding the LLM Twin Concept and Its Architecture","book_id":1,"book_title":"LLM Engineers Handbook"}
{"key_idea_id":113,"concept":"Automated LLM system for efficient data management and evaluation","chunk_id":113,"chunk_text":"The LLM itself is important, but we want to highlight that using ChatGPT’s web interface is \nexceptionally tedious in managing and injecting various data sources or evaluating the outputs. \nThe solution is to build an LLM system that encapsulates and automates all the following steps \n(manually replicating them each time is not a long-term and feasible solution):\n• \nData collection\n• \nData preprocessing","section_id":4.0,"section_title":"Why not use ChatGPT (or another similar chatbot)?","chapter_id":6,"chapter_title":"Chapter 1: Understanding the LLM Twin Concept and Its Architecture","book_id":1,"book_title":"LLM Engineers Handbook"}
{"key_idea_id":111,"concept":"Essential components and processes for effective LLM Twin implementation","chunk_id":111,"chunk_text":"The key of the LLM Twin stands in the following:\n• \nWhat data we collect\n•\nHow we preprocess the data\n•\nHow we feed the data into the LLM\n•\nHow we chain multiple prompts for the desired results\n•\nHow we evaluate the generated content","section_id":4.0,"section_title":"Why not use ChatGPT (or another similar chatbot)?","chapter_id":6,"chapter_title":"Chapter 1: Understanding the LLM Twin Concept and Its Architecture","book_id":1,"book_title":"LLM Engineers Handbook"}
{"key_idea_id":109,"concept":"Extensive debugging required for high-quality LLM-generated content","chunk_id":109,"chunk_text":"From our experience, if you want high-quality content that provides real value, you will spend \nmore time debugging the generated text than writing it yourself.","section_id":4.0,"section_title":"Why not use ChatGPT (or another similar chatbot)?","chapter_id":6,"chapter_title":"Chapter 1: Understanding the LLM Twin Concept and Its Architecture","book_id":1,"book_title":"LLM Engineers Handbook"}
{"key_idea_id":108,"concept":"Key limitations of chatbots like ChatGPT for content generation","chunk_id":108,"chunk_text":"•\nMisinformation due to hallucination: Manually checking the results for hallucinations or \nusing third-party tools to evaluate your results is a tedious and unproductive experience.\n•\nTedious manual prompting: You must manually craft your prompts and inject external \ninformation, which is a tiresome experience. Also, the generated answers will be hard to \nreplicate between multiple sessions as you don’t have complete control over your prompts \nand injected data. You can solve part of this problem using an API and a tool such as \nLangChain, but you need programming experience to do so.","section_id":4.0,"section_title":"Why not use ChatGPT (or another similar chatbot)?","chapter_id":6,"chapter_title":"Chapter 1: Understanding the LLM Twin Concept and Its Architecture","book_id":1,"book_title":"LLM Engineers Handbook"}
{"key_idea_id":107,"concept":"Limitations of generic chatbots for personalized writing and brand voice","chunk_id":107,"chunk_text":"r 1\n5\nWhy not use ChatGPT (or another similar chatbot)?\nWe have already provided the answer. ChatGPT is not personalized to your writing style and voice. \nInstead, it is very generic, unarticulated, and wordy. Maintaining an original voice is critical for \nlong-term success when building your brand. Thus, directly using ChatGPT or Gemini will not \nyield the most optimal results. Even if you are OK with sharing impersonalized content, mindlessly \nusing ChatGPT can result in the following:","section_id":4.0,"section_title":"Why not use ChatGPT (or another similar chatbot)?","chapter_id":6,"chapter_title":"Chapter 1: Understanding the LLM Twin Concept and Its Architecture","book_id":1,"book_title":"LLM Engineers Handbook"}
{"key_idea_id":115,"concept":"Limited context of ChatGPT for personalized content generation","chunk_id":115,"chunk_text":"This subsection will refer to using ChatGPT (or another similar chatbot) just in the \ncontext of generating personalized content. Unders","section_id":4.0,"section_title":"Why not use ChatGPT (or another similar chatbot)?","chapter_id":6,"chapter_title":"Chapter 1: Understanding the LLM Twin Concept and Its Architecture","book_id":1,"book_title":"LLM Engineers Handbook"}
{"key_idea_id":122,"concept":"Definition and purpose of a Minimum Viable Product (MVP)","chunk_id":122,"chunk_text":"What is an MVP?\nAn MVP is a version of a product that includes just enough features to draw in early users and test \nthe viability of the product concept in the initial stages of development. Usually, the purpose of \nthe MVP is to gather insights from the market with minimal effort.","section_id":6.0,"section_title":"What is an MVP?","chapter_id":6,"chapter_title":"Chapter 1: Understanding the LLM Twin Concept and Its Architecture","book_id":1,"book_title":"LLM Engineers Handbook"}
{"key_idea_id":118,"concept":"LLM Twin system's model-agnostic and data-centric design","chunk_id":118,"chunk_text":"tanding the LLM Twin Concept and Architecture\n6\n•\nData storage, versioning, and retrieval\n•\nLLM fine-tuning\n•\nRAG\n•\nContent generation evaluation\nNote that we never said not to use OpenAI’s GPT API, just that the LLM framework we will pres-\nent is LLM-agnostic. Thus, if it can be manipulated programmatically and exposes a fine-tuning \ninterface, it can be integrated into the LLM Twin system we will learn to build. The key to most \nsuccessful ML products is to be data-centric and make your architecture model-agnostic. Thus, \nyou can quickly experiment with multiple models on your specific data.","section_id":6.0,"section_title":"What is an MVP?","chapter_id":6,"chapter_title":"Chapter 1: Understanding the LLM Twin Concept and Its Architecture","book_id":1,"book_title":"LLM Engineers Handbook"}
{"key_idea_id":126,"concept":"MVP's essential viability through complete user journey and experience","chunk_id":126,"chunk_text":"Sticking to the V in MVP is essential, meaning the product must be viable. The product must \nprovide an end-to-end user journey without half-implemented features, even if the product is \nminimal. It must be a working product with a good user experience that people will love and \nwant to keep using to see how it evolves to its full potential.\n\nChapte","section_id":6.0,"section_title":"What is an MVP?","chapter_id":6,"chapter_title":"Chapter 1: Understanding the LLM Twin Concept and Its Architecture","book_id":1,"book_title":"LLM Engineers Handbook"}
{"key_idea_id":124,"concept":"Strategic advantages of implementing a Minimum Viable Product","chunk_id":124,"chunk_text":"An MVP is a powerful strategy because of the following reasons:\n• \nAccelerated time-to-market: Launch a product quickly to gain early traction\n• \nIdea validation: Test it with real users before investing in the full development of the \nproduct\n• \nMarket research: Gain insights into what resonates with the target audience\n• \nRisk minimization: Reduces the time and resources needed for a product that might not \nachieve market success","section_id":6.0,"section_title":"What is an MVP?","chapter_id":6,"chapter_title":"Chapter 1: Understanding the LLM Twin Concept and Its Architecture","book_id":1,"book_title":"LLM Engineers Handbook"}
{"key_idea_id":120,"concept":"The main objective of planning an LLM Twin MVP","chunk_id":120,"chunk_text":"Planning the MVP of the LLM Twin product\nNow that we understand what an LLM Twin is and why we want to build it, we must clearly define \nthe product’s features. In this book, we will focus on the first iteration, often labeled the minimum \nviable product (MVP), to follow the natural cycle of most products. Here, the main objective is \nto align our ideas with realistic and doable business objectives using the available resources to \nproduce the product. Even as an engineer, as you grow up in responsibilities, you must go through \nthese steps to bridge the gap between the business needs and what can be implemented.","section_id":6.0,"section_title":"What is an MVP?","chapter_id":6,"chapter_title":"Chapter 1: Understanding the LLM Twin Concept and Its Architecture","book_id":1,"book_title":"LLM Engineers Handbook"}
{"key_idea_id":133,"concept":"Engineering principles for a cost-effective and scalable LLM Twin","chunk_id":133,"chunk_text":"Even if we focus only on the core features of the LLM Twin defined in this section, we will build the product with the latest LLM research and best software engineering and MLOps practices in mind. We aim to show you how to engineer a cost-effective and scalable LLM application.Unders","section_id":7.0,"section_title":"Defining the LLM twin MVP","chapter_id":6,"chapter_title":"Chapter 1: Understanding the LLM Twin Concept and Its Architecture","book_id":1,"book_title":"LLM Engineers Handbook"}
{"key_idea_id":131,"concept":"Key functionalities and features of the LLM Twin MVP","chunk_id":131,"chunk_text":"To keep it simple, we will build the features that can do the following for the LLM Twin:•Collect data from your LinkedIn, Medium, Substack, and GitHub profiles•Fine-tune an open-source LLM using the collected data•Populate a vector database (DB) using our digital data for RAG•Create LinkedIn posts leveraging the following:•User prompts•RAG to reuse and reference old content•New posts, articles, or papers as additional knowledge to the LLM•Have a simple web interface to interact with the LLM Twin and be able to do the following:•Configure your social media links and trigger the collection step•Send prompts or links to external resourcesThat will be the LLM Twin MVP. Even if it doesn’t sound like much, remember that we must make this system cost effective, scalable, and modular.","section_id":7.0,"section_title":"Defining the LLM twin MVP","chapter_id":6,"chapter_title":"Chapter 1: Understanding the LLM Twin Concept and Its Architecture","book_id":1,"book_title":"LLM Engineers Handbook"}
{"key_idea_id":129,"concept":"Strategic definition of LLM Twin MVP with limited resources","chunk_id":129,"chunk_text":"r 1 7Defining the LLM Twin MVPAs a thought experiment, let’s assume that instead of building this project for this book, we want to make a real product. In that case, what are our resources? Well, unfortunately, not many:• We are a team of three people with two ML engineers and one ML researcher•Our laptops•Personal funding for computing, such as training LLMs•Our enthusiasmAs you can see, we don’t have many resources. Even if this is just a thought experiment, it reflects the reality for most start-ups at the beginning of their journey. Thus, we must be very strategic in defining our LLM Twin MVP and what features we want to pick. Our goal is simple: we want to maximize the product’s value relative to the effort and resources poured into it.","section_id":7.0,"section_title":"Defining the LLM twin MVP","chapter_id":6,"chapter_title":"Chapter 1: Understanding the LLM Twin Concept and Its Architecture","book_id":1,"book_title":"LLM Engineers Handbook"}
{"key_idea_id":137,"concept":"Book's shift to engineering perspective and LLM Twin implementation","chunk_id":137,"chunk_text":"tanding the LLM Twin Concept and Architecture\n8\nUntil now, we have examined the LLM Twin from the users’ and businesses’ perspectives. The last \nstep is to examine it from an engineering perspective and define a development plan to under-\nstand how to solve it technically. From now on, the book’s focus will be on the implementation \nof the LLM Twin.","section_id":9.0,"section_title":"The problem with building ML systems","chapter_id":6,"chapter_title":"Chapter 1: Understanding the LLM Twin Concept and Its Architecture","book_id":1,"book_title":"LLM Engineers Handbook"}
{"key_idea_id":149,"concept":"Common architectural layers of classic software applications","chunk_id":149,"chunk_text":"Similar solutions exist for classic software. For example, if you zoom out, most software appli-\ncations can be split between a DB, business logic, and UI layer. Every layer can be as complex as \nneeded, but at a high-level overview, the architecture of standard software can be boiled down \nto the previous three components.","section_id":9.0,"section_title":"The problem with building ML systems","chapter_id":6,"chapter_title":"Chapter 1: Understanding the LLM Twin Concept and Its Architecture","book_id":1,"book_title":"LLM Engineers Handbook"}
{"key_idea_id":147,"concept":"Creating a boilerplate for clearly designing integrated ML systems","chunk_id":147,"chunk_text":"Thus, the critical question is this: How do we connect all these components into a single homog-\nenous system? We must create a boilerplate for clearly designing ML systems to answer that \nquestion.","section_id":9.0,"section_title":"The problem with building ML systems","chapter_id":6,"chapter_title":"Chapter 1: Understanding the LLM Twin Concept and Its Architecture","book_id":1,"book_title":"LLM Engineers Handbook"}
{"key_idea_id":143,"concept":"Critical aspects of production-ready ML system design and deployment","chunk_id":143,"chunk_text":"At this point, we want to focus on how to design a production-ready architecture. Training a \nmodel with high accuracy is extremely valuable, but just by training it on a static dataset, you \nare far from deploying it robustly. We have to consider how to do the following:\n•\nIngest, clean, and validate fresh data\n•\nTraining versus inference setups\n•\nCompute and serve features in the right environment\n•\nServe the model in a cost-effective way\n• \nVersion, track, and share the datasets and models\n• \nMonitor your infrastructure and models\n• \nDeploy the model on a scalable infrastructure\n• \nAutomate the deployments and training\nThese are the types of problems an ML or MLOps engineer must consider, while the research or \ndata science team is often responsible for training the model.","section_id":9.0,"section_title":"The problem with building ML systems","chapter_id":6,"chapter_title":"Chapter 1: Understanding the LLM Twin Concept and Its Architecture","book_id":1,"book_title":"LLM Engineers Handbook"}
{"key_idea_id":145,"concept":"Many components required for production-ready ML and MLOps systems","chunk_id":145,"chunk_text":"Chapter 1\n9\nFigure 1.1: Common elements from an ML system\nThe preceding figure shows all the components the Google Cloud team suggests that a mature ML \nand MLOps system requires. Along with the ML code, there are many moving pieces. The rest of \nthe system comprises configuration, automation, data collection, data verification, testing and \ndebugging, resource management, model analysis, process and metadata management, serving \ninfrastructure, and monitoring. The point is that there are many components we must consider \nwhen productionizing an ML model.","section_id":9.0,"section_title":"The problem with building ML systems","chapter_id":6,"chapter_title":"Chapter 1: Understanding the LLM Twin Concept and Its Architecture","book_id":1,"book_title":"LLM Engineers Handbook"}
{"key_idea_id":138,"concept":"Overview of the feature\/training\/inference (FTI) pipeline architecture","chunk_id":138,"chunk_text":"Building ML systems with feature\/training\/inference \npipelines\nBefore diving into the specifics of the LLM Twin architecture, we must understand an ML system \npattern at the core of the architecture, known as the feature\/training\/inference (FTI) architecture. \nThis section will present a general overview of the FTI pipeline design and how it can structure \nan ML application.\nLet’s see how we can apply the FTI pipelines to the LLM Twin architecture.","section_id":9.0,"section_title":"The problem with building ML systems","chapter_id":6,"chapter_title":"Chapter 1: Understanding the LLM Twin Concept and Its Architecture","book_id":1,"book_title":"LLM Engineers Handbook"}
{"key_idea_id":151,"concept":"The initial examination of unsuitable solutions for scalable ML systems","chunk_id":151,"chunk_text":"Do we have something similar for ML applications? The first step is to examine previous solutions \nand why they are unsuitable for building scalable ML systems.","section_id":9.0,"section_title":"The problem with building ML systems","chapter_id":6,"chapter_title":"Chapter 1: Understanding the LLM Twin Concept and Its Architecture","book_id":1,"book_title":"LLM Engineers Handbook"}
{"key_idea_id":140,"concept":"The multifaceted complexity of production-ready ML system development","chunk_id":140,"chunk_text":"The problem with building ML systems\nBuilding production-ready ML systems is much more than just training a model. From an en-\ngineering point of view, training the model is the most straightforward step in most use cases. \nHowever, training a model becomes complex when deciding on the correct architecture and \nhyperparameters. That’s not an engineering problem but a research problem.","section_id":9.0,"section_title":"The problem with building ML systems","chapter_id":6,"chapter_title":"Chapter 1: Understanding the LLM Twin Concept and Its Architecture","book_id":1,"book_title":"LLM Engineers Handbook"}
{"key_idea_id":157,"concept":"Challenges of Google Cloud's complex production ML architecture","chunk_id":157,"chunk_text":"Ultimately, on the other spectrum, Google Cloud provides a production-ready architecture, as \nshown in Figure 1.4. Unfortunately, even if it’s a feasible solution, it’s very complex and not intu-\nitive. You will have difficulty understanding this if you are not highly experienced in deploying \nand keeping ML models in production. Also, it is not straightforward to understand how to start \nsmall and grow the system in time.\nThe following image is reproduced from work created and shared by Google and used according \nto terms described in the Creative Commons 4.0 Attribution License:\nFigure 1.4: ML pipeline automation for CT (source: https:\/\/cloud.google.com\/architecture\/\nmlops-continuous-delivery-and-automation-pipelines-in-machine-learning)\n\nChapte","section_id":10.0,"section_title":"The issue with previous solutions","chapter_id":6,"chapter_title":"Chapter 1: Understanding the LLM Twin Concept and Its Architecture","book_id":1,"book_title":"LLM Engineers Handbook"}
{"key_idea_id":154,"concept":"Client-side state management as an anti-pattern in real-time systems","chunk_id":154,"chunk_text":"In Figure 1.3, we can see a similar scenario for a real-time system. This use case introduces an-\nother issue in addition to what we listed before. To make the predictions, we have to transfer the \nwhole state through the client request so the features can be computed and passed to the model.\nConsider the scenario of computing movie recommendations for a user. Instead of simply pass-\ning the user ID, we must transmit the entire user state, including their name, age, gender, movie \nhistory, and more. This approach is fraught with potential errors, as the client must understand \nhow to access this state, and it’s tightly coupled with the model service.\nAnother example would be when implementing an LLM with RAG support. The documents we add \nas context along the query represent our external state. If we didn’t store the records in a vector \nDB, we would have to pass them with the user query. To do so, the client must know how to query \nand retrieve the documents, which is not feasible. It is an antipattern for the client application \nto know how to access or compute the features. If you don’t understand how RAG works, we will \nexplain it in detail in Chapters 8 and 9.\nFigure 1.3: Stateless real-time architecture","section_id":10.0,"section_title":"The issue with previous solutions","chapter_id":6,"chapter_title":"Chapter 1: Understanding the LLM Twin Concept and Its Architecture","book_id":1,"book_title":"LLM Engineers Handbook"}
{"key_idea_id":153,"concept":"Issues with monolithic batch system architecture","chunk_id":153,"chunk_text":"Unfortunately, building a monolithic batch system raises many other issues, such as the following:\n•\nFeatures are not reusable (by your system or others)\n•\nIf the data increases, you have to refactor the whole code to support PySpark or Ray\n•\nIt’s hard to rewrite the prediction module in a more efficient language such as C++, Java, \nor Rust\n\nChapter 1\n11\n•\nIt’s hard to share the work between multiple teams between the features, training, and \nprediction modules\n•\nIt’s impossible to switch to streaming technology for real-time training","section_id":10.0,"section_title":"The issue with previous solutions","chapter_id":6,"chapter_title":"Chapter 1: Understanding the LLM Twin Concept and Its Architecture","book_id":1,"book_title":"LLM Engineers Handbook"}
{"key_idea_id":152,"concept":"Monolithic batch architecture's solution to training-serving skew in ML","chunk_id":152,"chunk_text":"tanding the LLM Twin Concept and Architecture\n10\nThe issue with previous solutions\nIn Figure 1.2, you can observe the typical architecture present in most ML applications. It is based \non a monolithic batch architecture that couples the feature creation, model training, and infer-\nence into the same component. By taking this approach, you quickly solve one critical problem in \nthe ML world: the training-serving skew. The training-serving skew happens when the features \npassed to the model are computed differently at training and inference time.\nIn this architecture, the features are created using the same code. Hence, the training-serving \nskew issue is solved by default. This pattern works fine when working with small data. The \npipeline runs on a schedule in batch mode, and the predictions are consumed by a third-party \napplication such as a dashboard.\nFigure 1.2: Monolithic batch pipeline architecture","section_id":10.0,"section_title":"The issue with previous solutions","chapter_id":6,"chapter_title":"Chapter 1: Understanding the LLM Twin Concept and Its Architecture","book_id":1,"book_title":"LLM Engineers Handbook"}
{"key_idea_id":155,"concept":"The problem of accessing features for predictions without client request","chunk_id":155,"chunk_text":"Understanding the LLM Twin Concept and Architecture\n12\nIn conclusion, our problem is accessing the features to make predictions without passing them at \nthe client’s request. For example, based on our first user movie recommendation example, how \ncan we predict the recommendations solely based on the user’s ID? Remember these questions, \nas we will answer them shortly.","section_id":10.0,"section_title":"The issue with previous solutions","chapter_id":6,"chapter_title":"Chapter 1: Understanding the LLM Twin Concept and Its Architecture","book_id":1,"book_title":"LLM Engineers Handbook"}
{"key_idea_id":163,"concept":"Feature pipeline's raw data processing and feature store benefits","chunk_id":163,"chunk_text":"The feature pipeline\nThe feature pipeline takes raw data as input, processes it, and outputs the features and labels \nrequired by the model for training or inference. Instead of directly passing them to the model, the \nfeatures and labels are stored inside a feature store. Its responsibility is to store, version, track, and \nshare the features. By saving the features in a feature store, we always have a state of our features. \nThus, we can easily send the features to the training and inference pipelines.\nAs the data is versioned, we can always ensure that the training and inference time features match. \nThus, we avoid the training-serving skew problem.","section_id":14.0,"section_title":"The inference pipeline","chapter_id":6,"chapter_title":"Chapter 1: Understanding the LLM Twin Concept and Its Architecture","book_id":1,"book_title":"LLM Engineers Handbook"}
{"key_idea_id":166,"concept":"The training pipeline's role in model generation and management","chunk_id":166,"chunk_text":"The training pipeline\nThe training pipeline takes the features and labels from the features stored as input and outputs \na train model or models. The models are stored in a model registry. Its role is similar to that of \nfeature stores, but this time, the model is the first-class citizen. Thus, the model registry will store, \nversion, track, and share the model with the inference pipeline.\nAlso, most modern model registries support a metadata store that allows you to specify essential \naspects of how the model was trained. The most important are the features, labels, and their \nversion used to train the model. Thus, we will always know what data the model was trained on.","section_id":14.0,"section_title":"The inference pipeline","chapter_id":6,"chapter_title":"Chapter 1: Understanding the LLM Twin Concept and Its Architecture","book_id":1,"book_title":"LLM Engineers Handbook"}
{"key_idea_id":161,"concept":"Understanding the flexible and modular nature of ML pipelines","chunk_id":161,"chunk_text":"tanding the LLM Twin Concept and Architecture\n14\nBefore going into the details, it is essential to understand that each pipeline is a different com-\nponent that can run on a different process or hardware. Thus, each pipeline can be written using \na different technology, by a different team, or scaled differently. The key idea is that the design \nis very flexible to the needs of your team. It acts as a mind map for structuring your architecture.","section_id":14.0,"section_title":"The inference pipeline","chapter_id":6,"chapter_title":"Chapter 1: Understanding the LLM Twin Concept and Its Architecture","book_id":1,"book_title":"LLM Engineers Handbook"}
{"key_idea_id":169,"concept":"Understanding the inference pipeline's inputs, outputs, and versioning","chunk_id":169,"chunk_text":"The inference pipeline\nThe inference pipeline takes as input the features and labels from the feature store and the trained \nmodel from the model registry. With these two, predictions can be easily made in either batch \nor real-time mode.\nAs this is a versatile pattern, it is up to you to decide what you do with your predictions. If it’s a \nbatch system, they will probably be stored in a DB. If it’s a real-time system, the predictions will \nbe served to the client who requested them. Additionally, the features, labels, and models are \nversioned. We can easily upgrade or roll back the deployment of the model. For example, we will \nalways know that model v1 uses features F1, F2, and F3, and model v2 uses F2, F3, and F4. Thus, \nwe can quickly change the connections between the model and features.","section_id":14.0,"section_title":"The inference pipeline","chapter_id":6,"chapter_title":"Chapter 1: Understanding the LLM Twin Concept and Its Architecture","book_id":1,"book_title":"LLM Engineers Handbook"}
{"key_idea_id":183,"concept":"Enabling independent evolution of FTI pipelines through consistent interfaces","chunk_id":183,"chunk_text":"The FTI pipelines act as logical layers. Thus, it is perfectly fine for each to be complex and contain \nmultiple services. However, what is essential is to stick to the same interface on how the FTI pipe-\nlines interact with each other through the feature store and model registries. By doing so, each \nFTI component can evolve differently, without knowing the details of each other and without \nbreaking the system on new changes.\n\nUnders","section_id":15.0,"section_title":"Benefits of the FTI architecture","chapter_id":6,"chapter_title":"Chapter 1: Understanding the LLM Twin Concept and Its Architecture","book_id":1,"book_title":"LLM Engineers Handbook"}
{"key_idea_id":180,"concept":"FTI pattern's flexible composition of multiple pipelines","chunk_id":180,"chunk_text":"The final thing you must understand about the FTI pattern is that the system doesn’t have to \ncontain only three pipelines. In most cases, it will include more. For example, the feature pipeline \ncan be composed of a service that computes the features and one that validates the data. Also, the \ntraining pipeline can be composed of the training and evaluation components.","section_id":15.0,"section_title":"Benefits of the FTI architecture","chapter_id":6,"chapter_title":"Chapter 1: Understanding the LLM Twin Concept and Its Architecture","book_id":1,"book_title":"LLM Engineers Handbook"}
{"key_idea_id":177,"concept":"Main benefits of the Feature\/Training\/Inference architecture pattern","chunk_id":177,"chunk_text":"Now that we understand better how the pattern works, we want to highlight the main benefits \nof using this pattern:\n• \nAs you have just three components, it is intuitive to use and easy to understand.\n• \nEach component can be written into its tech stack, so we can quickly adapt them to specific \nneeds, such as big or streaming data. Also, it allows us to pick the best tools for the job.\n• \nAs there is a transparent interface between the three components, each one can be de-\nveloped by a different team (if necessary), making the development more manageable \nand scalable.\n• \nEvery component can be deployed, scaled, and monitored independently.","section_id":15.0,"section_title":"Benefits of the FTI architecture","chapter_id":6,"chapter_title":"Chapter 1: Understanding the LLM Twin Concept and Its Architecture","book_id":1,"book_title":"LLM Engineers Handbook"}
{"key_idea_id":174,"concept":"The consistent input and output interfaces of FTI pipelines","chunk_id":174,"chunk_text":"To conclude, the most important thing you must remember about the FTI pipelines is their in- terface:\n• \nThe feature pipeline takes in data and outputs the features and labels saved to the feature \nstore.\n• \nThe training pipeline queries the features store for features and labels and outputs a \nmodel to the model registry.\n• \nThe inference pipeline uses the features from the feature store and the model from the \nmodel registry to make predictions.\nIt doesn’t matter how complex your ML system gets, these interfaces will remain the same.","section_id":15.0,"section_title":"Benefits of the FTI architecture","chapter_id":6,"chapter_title":"Chapter 1: Understanding the LLM Twin Concept and Its Architecture","book_id":1,"book_title":"LLM Engineers Handbook"}
{"key_idea_id":190,"concept":"Defining high-level, technology-agnostic LLM Twin system architecture","chunk_id":190,"chunk_text":"Designing the system architecture of the LLM Twin\nIn this section, we will list the concrete technical details of the LLM Twin application and under-\nstand how we can solve them by designing our LLM system using the FTI architecture. However, \nbefore diving into the pipelines, we want to highlight that we won’t focus on the tooling or the \ntech stack at this step. We only want to define a high-level architecture of the system, which is \nlanguage-, framework-, platform-, and infrastructure-agnostic at this point. We will focus on \neach component’s scope, interface, and interconnectivity. In future chapters, we will cover the \nimplementation details and tech stack.","section_id":17.0,"section_title":"Listing the technical details of the LLM twin architecture","chapter_id":6,"chapter_title":"Chapter 1: Understanding the LLM Twin Concept and Its Architecture","book_id":1,"book_title":"LLM Engineers Handbook"}
{"key_idea_id":193,"concept":"specific technical requirements for LLM Twin data and training","chunk_id":193,"chunk_text":"Listing the technical details of the LLM Twin architecture\nUntil now, we defined what the LLM Twin should support from the user’s point of view. Now, \nlet’s clarify the requirements of the ML system from a purely technical perspective:\n• \nOn the data side, we have to do the following:\n• \nCollect data from LinkedIn, Medium, Substack, and GitHub completely autono-\nmously and on a schedule\n•\nStandardize the crawled data and store it in a data warehouse\n•\nClean the raw data\n•\nCreate instruct datasets for fine-tuning an LLM\n•\nChunk and embed the cleaned data. Store the vectorized data into a vector DB \nfor RAG.\n• \nFor training, we have to do the following:\n•\nFine-tune LLMs of various sizes (7B, 14B, 30B, or 70B parameters)\n•\nFine-tune on instruction datasets of multiple sizes\n•\nSwitch between LLM types (for example, between Mistral, Llama, and GPT)\n•\nTrack and compare experiments","section_id":17.0,"section_title":"Listing the technical details of the LLM twin architecture","chapter_id":6,"chapter_title":"Chapter 1: Understanding the LLM Twin Concept and Its Architecture","book_id":1,"book_title":"LLM Engineers Handbook"}
{"key_idea_id":205,"concept":"Categorization of collected digital data by type and source","chunk_id":205,"chunk_text":"The collected digital data is binned into three categories:\n• \nArticles (Medium, Substack)\n• \nPosts (LinkedIn)\n• \nCode (GitHub)","section_id":20.0,"section_title":"Feature pipeline","chapter_id":6,"chapter_title":"Chapter 1: Understanding the LLM Twin Concept and Its Architecture","book_id":1,"book_title":"LLM Engineers Handbook"}
{"key_idea_id":212,"concept":"Custom properties of the LLM Twin's feature pipeline","chunk_id":212,"chunk_text":"Understanding the LLM Twin Concept and Architecture\n20\nThe characteristics of the FTI pattern are already present.\nHere are some custom properties of the LLM Twin’s feature pipeline:\n•\nIt processes three types of data differently: articles, posts, and code\n•\nIt contains three main processing steps necessary for fine-tuning and RAG: cleaning, \nchunking, and embedding\n•\nIt creates two snapshots of the digital data, one after cleaning (used for fine-tuning) and \none after embedding (used for RAG)\n•\nIt uses a logical feature store instead of a specialized feature store","section_id":20.0,"section_title":"Feature pipeline","chapter_id":6,"chapter_title":"Chapter 1: Understanding the LLM Twin Concept and Its Architecture","book_id":1,"book_title":"LLM Engineers Handbook"}
{"key_idea_id":210,"concept":"Designing data collection pipeline for ethical user data privacy","chunk_id":210,"chunk_text":"It is critical to highlight that the data collection pipeline is designed to crawl data \nonly from your social media platform. It will not have access to other people. As an \nexample for this book, we agreed to make our collected data available for learning \npurposes. Otherwise, using other people’s data without their consent is not moral.","section_id":20.0,"section_title":"Feature pipeline","chapter_id":6,"chapter_title":"Chapter 1: Understanding the LLM Twin Concept and Its Architecture","book_id":1,"book_title":"LLM Engineers Handbook"}
{"key_idea_id":208,"concept":"Feature pipeline's role in processing raw data for the feature store","chunk_id":208,"chunk_text":"Feature pipeline\nThe feature pipeline’s role is to take raw articles, posts, and code data points from the data ware-\nhouse, process them, and load them into the feature store.","section_id":20.0,"section_title":"Feature pipeline","chapter_id":6,"chapter_title":"Chapter 1: Understanding the LLM Twin Concept and Its Architecture","book_id":1,"book_title":"LLM Engineers Handbook"}
{"key_idea_id":214,"concept":"Implementing logical feature store using vector database with added logic","chunk_id":214,"chunk_text":"Let’s zoom in on the logical feature store part a bit. As with any RAG-based system, one of the \ncentral pieces of the infrastructure is a vector DB. Instead of integrating another DB, more con-\ncretely, a specialized feature store, we used the vector DB, plus some additional logic to check all \nthe properties of a feature store our system needs.","section_id":20.0,"section_title":"Feature pipeline","chapter_id":6,"chapter_title":"Chapter 1: Understanding the LLM Twin Concept and Its Architecture","book_id":1,"book_title":"LLM Engineers Handbook"}
{"key_idea_id":220,"concept":"Justification for LLM twin architecture's component choices","chunk_id":220,"chunk_text":"For our use case, this is more than enough because of the following reasons:\n• \nThe artifacts work great for offline use cases such as training\n• \nThe vector DB is built for online access, which we require for inference.","section_id":20.0,"section_title":"Feature pipeline","chapter_id":6,"chapter_title":"Chapter 1: Understanding the LLM Twin Concept and Its Architecture","book_id":1,"book_title":"LLM Engineers Handbook"}
{"key_idea_id":203,"concept":"LLM twin data collection pipeline using ETL and NoSQL DB","chunk_id":203,"chunk_text":"Data collection pipeline\nThe data collection pipeline involves crawling your personal data from Medium, Substack, Linke-\ndIn, and GitHub. As a data pipeline, we will use the extract, load, transform (ETL) pattern to \nextract data from social media platforms, standardize it, and load it into a data warehouse.\nThe output of this component will be a NoSQL DB, which will act as our data warehouse. As we \nwork with text data, which is naturally unstructured, a NoSQL DB fits like a glove.\nEven though a NoSQL DB, such as MongoDB, is not labeled as a data warehouse, from our point \nof view, it will act as one. Why? Because it stores standardized raw data gathered by various ETL \npipelines that are ready to be ingested into an ML system.","section_id":20.0,"section_title":"Feature pipeline","chapter_id":6,"chapter_title":"Chapter 1: Understanding the LLM Twin Concept and Its Architecture","book_id":1,"book_title":"LLM Engineers Handbook"}
{"key_idea_id":207,"concept":"Modular system design for flexible data source integration","chunk_id":207,"chunk_text":"Also, by grouping the data by category, not the source, we can quickly plug data from other plat-\nforms, such as X into the posts or GitLab into the code collection. As a modular system, we must \nattach an additional ETL in the data collection pipeline, and everything else will work without \nfurther code modifications.","section_id":20.0,"section_title":"Feature pipeline","chapter_id":6,"chapter_title":"Chapter 1: Understanding the LLM Twin Concept and Its Architecture","book_id":1,"book_title":"LLM Engineers Handbook"}
{"key_idea_id":206,"concept":"Prioritizing data type over source for LLM processing","chunk_id":206,"chunk_text":"We want to abstract away the platform where the data was crawled. For example, when feeding \nan article to the LLM, knowing it came from Medium or Substack is not essential. We can keep \nthe source URL as metadata to give references. However, from the processing, fine-tuning, and \nRAG points of view, it is vital to know what type of data we ingested, as each category must be \nprocessed differently. For example, the chunking strategy between a post, article, and piece of \ncode will look different.","section_id":20.0,"section_title":"Feature pipeline","chapter_id":6,"chapter_title":"Chapter 1: Understanding the LLM Twin Concept and Its Architecture","book_id":1,"book_title":"LLM Engineers Handbook"}
{"key_idea_id":224,"concept":"Raw data processing and storage in a feature store","chunk_id":224,"chunk_text":"To conclude, we take in raw article, post, or code data points, process them, and store them in \na feature store to make them accessible to the training and inference pipelines. Note that trim-\nming all the complexity away and focusing only on the interface is a perfect match with the FTI \npattern. Beautiful, right?","section_id":20.0,"section_title":"Feature pipeline","chapter_id":6,"chapter_title":"Chapter 1: Understanding the LLM Twin Concept and Its Architecture","book_id":1,"book_title":"LLM Engineers Handbook"}
{"key_idea_id":218,"concept":"Training and inference pipeline access to the logical feature store","chunk_id":218,"chunk_text":"How will the rest of the system access the logical feature store? The training pipeline will use the \ninstruct datasets as artifacts, and the inference pipeline will query the vector DB for additional \ncontext using vector search techniques.","section_id":20.0,"section_title":"Feature pipeline","chapter_id":6,"chapter_title":"Chapter 1: Understanding the LLM Twin Concept and Its Architecture","book_id":1,"book_title":"LLM Engineers Handbook"}
{"key_idea_id":216,"concept":"Vector database's NoSQL functionality for data retrieval and MLOps artifacts","chunk_id":216,"chunk_text":"The vector DB doesn’t offer the concept of a training dataset, but it can be used as a NoSQL DB. \nThis means we can access data points using their ID and collection name. Thus, we can easily \nquery the vector DB for new data points without any vector search logic. Ultimately, we will \nwrap the retrieved data into a versioned, tracked, and shareable artifact—more on artifacts in \nChapter 2. For now, you must know it is an MLOps concept used to wrap data and enrich it with \nthe properties listed before.","section_id":20.0,"section_title":"Feature pipeline","chapter_id":6,"chapter_title":"Chapter 1: Understanding the LLM Twin Concept and Its Architecture","book_id":1,"book_title":"LLM Engineers Handbook"}
{"key_idea_id":230,"concept":"Critical testing and expert review for LLM model deployment","chunk_id":230,"chunk_text":"The testing pipeline is triggered for a more detailed analysis than during fine-tuning. Before pushing the new model to production, assessing it against a stricter set of tests is critical to see that the latest candidate is better than what is currently in production. If this step passes, the model is ultimately tagged as accepted and deployed to the production inference pipeline. Even in a fully automated ML system, it is recommended to have a manual step before accepting a new production model. It is like pushing the red button before a significant action with high conse-quences. Thus, at this stage, an expert looks at a report generated by the testing component. If everything looks good, it approves the model, and the automation can continue.","section_id":21.0,"section_title":"Training pipeline","chapter_id":6,"chapter_title":"Chapter 1: Understanding the LLM Twin Concept and Its Architecture","book_id":1,"book_title":"LLM Engineers Handbook"}
{"key_idea_id":228,"concept":"Data science team's process for LLM optimization and continuous training","chunk_id":228,"chunk_text":"In the initial stages, the data science team owns this step. They run multiple experiments to find the best model and hyperparameters for the job, either through automatic hyperparameter tuning or manually. To compare and pick the best set of hyperparameters, we will use an experiment tracker to log everything of value and compare it between experiments. Ultimately, they will pick the best hyperparameters and fine-tuned LLM and propose it as the LLM production candidate. The proposed LLM is then stored in the model registry. After the experimentation phase is over, we store and reuse the best hyperparameters found to eliminate the manual restrictions of the process. Now, we can completely automate the training process, known as continuous training.","section_id":21.0,"section_title":"Training pipeline","chapter_id":6,"chapter_title":"Chapter 1: Understanding the LLM Twin Concept and Its Architecture","book_id":1,"book_title":"LLM Engineers Handbook"}
{"key_idea_id":232,"concept":"Key LLM design and implementation challenges for pipelines","chunk_id":232,"chunk_text":"The particularities of this component will be on LLM aspects, such as the following: • How do you implement an LLM agnostic pipeline? • What fine-tuning techniques should you use? • How do you scale the fine-tuning algorithm on LLMs and datasets of various sizes? • How do you pick an LLM production candidate from multiple experiments? • How do you test the LLM to decide whether to push it to production or not? By the end of this book, you will know how to answer all these questions.","section_id":21.0,"section_title":"Training pipeline","chapter_id":6,"chapter_title":"Chapter 1: Understanding the LLM Twin Concept and Its Architecture","book_id":1,"book_title":"LLM Engineers Handbook"}
{"key_idea_id":227,"concept":"LLM training pipeline's process from instruct datasets to model registry","chunk_id":227,"chunk_text":"r 1 21 Training pipeline The training pipeline consumes instruct datasets from the feature store, fine-tunes an LLM with it, and stores the tuned LLM weights in a model registry. More concretely, when a new instruct dataset is available in the logical feature store, we will trigger the training pipeline, consume the artifact, and fine-tune the LLM.","section_id":21.0,"section_title":"Training pipeline","chapter_id":6,"chapter_title":"Chapter 1: Understanding the LLM Twin Concept and Its Architecture","book_id":1,"book_title":"LLM Engineers Handbook"}
{"key_idea_id":234,"concept":"Modular design and ML orchestrator for continuous system scheduling","chunk_id":234,"chunk_text":"One last aspect we want to clarify is CT. Our modular design allows us to quickly leverage an ML orchestrator to schedule and trigger different system parts. For example, we can schedule the data collection pipeline to crawl data every week. Underst","section_id":21.0,"section_title":"Training pipeline","chapter_id":6,"chapter_title":"Chapter 1: Understanding the LLM Twin Concept and Its Architecture","book_id":1,"book_title":"LLM Engineers Handbook"}
{"key_idea_id":241,"concept":"Analyzing and debugging LLM twin system with prompt monitoring","chunk_id":241,"chunk_text":"All the client queries, enriched prompts using RAG, and generated answers are sent to a prompt \nmonitoring system to analyze, debug, and better understand the system. Based on specific require-\nments, the monitoring system can trigger alarms to take action either manually or automatically.","section_id":23.0,"section_title":"Final thoughts on the FTI design and the LLM twin architecture","chapter_id":6,"chapter_title":"Chapter 1: Understanding the LLM Twin Concept and Its Architecture","book_id":1,"book_title":"LLM Engineers Handbook"}
{"key_idea_id":247,"concept":"Component-specific computing requirements in LLM twin architecture","chunk_id":247,"chunk_text":"Ultimately, we will explain the computing requirements of each component briefly. The data \ncollection and feature pipeline are mostly CPU-based and do not require powerful machines. The \ntraining pipeline requires powerful GPU-based machines that could load an LLM and fine-tune it. \nThe inference pipeline is somewhere in the middle. It still needs a powerful machine but is less \ncompute-intensive than the training step. However, it must be tested carefully, as the inference \npipeline directly interfaces with the user. Thus, we want the latency to be within the required \nparameters for a good user experience. However, using the FTI design is not an issue. We can pick \nthe proper computing requirements for each component.","section_id":23.0,"section_title":"Final thoughts on the FTI design and the LLM twin architecture","chapter_id":6,"chapter_title":"Chapter 1: Understanding the LLM Twin Concept and Its Architecture","book_id":1,"book_title":"LLM Engineers Handbook"}
{"key_idea_id":245,"concept":"Flexible application of FTI design focusing on required properties","chunk_id":245,"chunk_text":"Final thoughts on the FTI design and the LLM Twin \narchitecture\nWe don’t have to be highly rigid about the FTI pattern. It is a tool used to clarify how to design \nML systems. For example, instead of using a dedicated features store just because that is how \nit is done, in our system, it is easier and cheaper to use a logical feature store based on a vector \nDB and artifacts. What was important to focus on were the required properties a feature store \nprovides, such as a versioned and reusable training dataset.","section_id":23.0,"section_title":"Final thoughts on the FTI design and the LLM twin architecture","chapter_id":6,"chapter_title":"Chapter 1: Understanding the LLM Twin Concept and Its Architecture","book_id":1,"book_title":"LLM Engineers Handbook"}
{"key_idea_id":243,"concept":"LLM and RAG system's unique architectural components","chunk_id":243,"chunk_text":"At the interface level, this component follows exactly the FTI architecture, but when zooming in, \nwe can observe unique characteristics of an LLM and RAG system, such as the following:\n• \nA retrieval client used to do vector searches for RAG\n• \nPrompt templates used to map user queries and external information to LLM inputs\n• \nSpecial tools for prompt monitoring","section_id":23.0,"section_title":"Final thoughts on the FTI design and the LLM twin architecture","chapter_id":6,"chapter_title":"Chapter 1: Understanding the LLM Twin Concept and Its Architecture","book_id":1,"book_title":"LLM Engineers Handbook"}
{"key_idea_id":240,"concept":"LLM twin inference pipeline's role in processing queries with RAG","chunk_id":240,"chunk_text":"Inference pipeline\nThe inference pipeline is the last piece of the puzzle. It is connected to the model registry and log-\nical feature store. It loads a fine-tuned LLM from the model registry, and from the logical feature \nstore, it accesses the vector DB for RAG. It takes in client requests through a REST API as queries. \nIt uses the fine-tuned LLM and access to the vector DB to carry out RAG and answer the queries.","section_id":23.0,"section_title":"Final thoughts on the FTI design and the LLM twin architecture","chapter_id":6,"chapter_title":"Chapter 1: Understanding the LLM Twin Concept and Its Architecture","book_id":1,"book_title":"LLM Engineers Handbook"}
{"key_idea_id":238,"concept":"triggering conditions for feature and training pipelines","chunk_id":238,"chunk_text":"Then, we can trigger the feature pipeline when new data is available in the data warehouse and \nthe training pipeline when new instruction datasets are available.","section_id":23.0,"section_title":"Final thoughts on the FTI design and the LLM twin architecture","chapter_id":6,"chapter_title":"Chapter 1: Understanding the LLM Twin Concept and Its Architecture","book_id":1,"book_title":"LLM Engineers Handbook"}
{"key_idea_id":253,"concept":"Different scaling strategies for various LLM twin pipelines","chunk_id":253,"chunk_text":"Also, each pipeline will be scaled differently. The data and feature pipelines will be scaled horizon-tally based on the CPU and RAM load. The training pipeline will be scaled vertically by adding more GPUs. The inference pipeline will be scaled horizontally based on the number of client requests.","section_id":25.0,"section_title":"References","chapter_id":6,"chapter_title":"Chapter 1: Understanding the LLM Twin Concept and Its Architecture","book_id":1,"book_title":"LLM Engineers Handbook"}
{"key_idea_id":260,"concept":"Exploration of MLOps implementation and deployment tools and concepts","chunk_id":260,"chunk_text":"The following chapters will explore how to implement and deploy each component. On the MLOps side, we will walk you through using a computing platform, orchestrator, model registry, artifacts, and other tools and concepts to support all MLOps best practices.","section_id":25.0,"section_title":"References","chapter_id":6,"chapter_title":"Chapter 1: Understanding the LLM Twin Concept and Its Architecture","book_id":1,"book_title":"LLM Engineers Handbook"}
{"key_idea_id":257,"concept":"Overview of LLM Twin concept and FTI pattern application","chunk_id":257,"chunk_text":"Summary\nThis first chapter was critical to understanding the book’s goal. As a product-oriented book that will walk you through building an end-to-end ML system, it was essential to understand the concept of an LLM Twin initially. Afterward, we walked you through what an MVP is and how to plan our LLM Twin MVP based on our available resources. Following this, we translated our concept into a practical technical solution with specific requirements. In this context, we introduced the FTI design pattern and showcased its real-world application in designing systems that are both modular and scalable. Ultimately, we successfully applied the FTI pattern to design the architecture of the LLM Twin to fit all our technical requirements.","section_id":25.0,"section_title":"References","chapter_id":6,"chapter_title":"Chapter 1: Understanding the LLM Twin Concept and Its Architecture","book_id":1,"book_title":"LLM Engineers Handbook"}
{"key_idea_id":255,"concept":"Overview of the LLM Twin architecture's technical requirements and features","chunk_id":255,"chunk_text":"To conclude, the presented LLM architecture checks all the technical requirements listed at the beginning of the section. It processes the data as requested, and the training is modular and can be quickly adapted to different LLMs, datasets, or fine-tuning techniques. The inference pipeline supports RAG and is exposed as a REST API. On the LLMOps side, the system supports dataset and model versioning, lineage, and reusability. The system has a monitoring service, and the whole ML architecture is designed with CT\/CI\/CD in mind.This concludes the high-level overview of the LLM Twin architecture.","section_id":25.0,"section_title":"References","chapter_id":6,"chapter_title":"Chapter 1: Understanding the LLM Twin Concept and Its Architecture","book_id":1,"book_title":"LLM Engineers Handbook"}
{"key_idea_id":262,"concept":"external references for further reading on MLOps and AI systems","chunk_id":262,"chunk_text":"References\n•\nDowling, J. (2024a, July 11). From MLOps to ML Systems with Feature\/Training\/Inference \nPipelines. Hopsworks. https:\/\/www.hopsworks.ai\/post\/mlops-to-ml-systems-with-\nfti-pipelines\n\nUnderstanding the LLM Twin Concept and Architecture\n24\n• \nDowling, J. (2024b, August 5). Modularity and Composability for AI Systems with AI Pipe-\nlines and Shared Storage. Hopsworks. https:\/\/www.hopsworks.ai\/post\/modularity-and-\ncomposability-for-ai-systems-with-ai-systems-with-ai-pipelines-and-shared-storage\n• \nJoseph, M. (2024, August 23). The Taxonomy for Data Transformations in AI Systems. Hop-\nsworks. https:\/\/www.hopsworks.ai\/post\/a-taxonomy-for-data-transformations-\nin-ai-systems\n•\nMLOps: Continuous delivery and automation pipelines in machine learning. (2024, August \n28). Google Cloud. https:\/\/cloud.google.com\/architecture\/mlops-continuous-\ndelivery-and-automation-pipelines-in-machine-learning\n•\nQwak. (2024a, June 2). CI\/CD for Machine Learning in 2024: Best Practices to build, test, \nand Deploy | Infer. Medium. https:\/\/medium.com\/infer-qwak\/ci-cd-for-machine-\nlearning-in-2024-best-practices-to-build-test-and-deploy-c4ad869824d2\n• \nQwak. (2024b, July 23). 5 Best Open Source Tools to build End-to-End MLOPs Pipeline in 2024. \nMedium. https:\/\/medium.com\/infer-qwak\/building-an-end-to-end-mlops-pipeline-\nwith-open-source-tools-d8bacbf4184f\n•\nSalama, K., Kazmierczak, J., & Schut, D. (2021). Practitioners guide to MLOPs: A framework \nfor continuous delivery and automation of machine learning (1\nst ed.) [PDF]. Google Cloud. \nhttps:\/\/services.google.com\/fh\/files\/misc\/practitioners_guide_to_mlops_\nwhitepaper.pdf","section_id":25.0,"section_title":"References","chapter_id":6,"chapter_title":"Chapter 1: Understanding the LLM Twin Concept and Its Architecture","book_id":1,"book_title":"LLM Engineers Handbook"}
{"key_idea_id":259,"concept":"importance of understanding system architecture and component integration","chunk_id":259,"chunk_text":"Having a clear vision of the big picture is essential when building systems. Understanding how a single component will be integrated into the rest of the application can be very valuable when working on it. We started with a more abstract presentation of the LLM Twin architecture, focusing on each component’s scope, interface, and interconnectivity.","section_id":25.0,"section_title":"References","chapter_id":6,"chapter_title":"Chapter 1: Understanding the LLM Twin Concept and Its Architecture","book_id":1,"book_title":"LLM Engineers Handbook"}
{"key_idea_id":269,"concept":"Poetry's function in Python dependency and virtual environment management","chunk_id":269,"chunk_text":"Poetry: dependency and virtual environment management\nPoetry is one of the most popular dependency and virtual environment managers within the \nPython ecosystem. But let’s start by clarifying what a dependency manager is. In Python, a depen-\ndency manager allows you to specify, install, update, and manage external libraries or packages \n(dependencies) that a project relies on. For example, this is a simple Poetry requirements file that \nuses Python 3.11 and the requests and numpy Python packages.\n[tool.poetry.dependencies]\npython = \"^3.11\"\nrequests = \"^2.25.1\"\nnumpy = \"^1.19.5\"\n[build-system]\n\nTooling and Installation\n28\nrequires = [\"poetry-core\"]\nbuild-backend = \"poetry.core.masonry.api\"\nBy using Poetry to pin your dependencies, you always ensure that you install the correct version \nof the dependencies that your projects work with. Poetry, by default, saves all its requirements in \npyproject.toml files, which are stored at the root of your repository, as you can see in the cloned \nLLM-Engineers-Handbook repository.","section_id":27.0,"section_title":"Poetry: dependency and virtual environment management","chapter_id":7,"chapter_title":"Chapter 2: Tooling and Installation","book_id":1,"book_title":"LLM Engineers Handbook"}
{"key_idea_id":274,"concept":"Poetry's poetry.lock file for consistent dependency version management","chunk_id":274,"chunk_text":"One final note on Poetry is that it locks down the exact versions of the dependency tree in the \npoetry.lock file based on the definitions added to the project.toml file. While the pyproject.\ntoml file may specify version ranges (e.g., requests = \"^2.25.1\"), the poetry.lock file records \nthe exact version (e.g., requests = \"2.25.1\") that was installed. It also locks the versions of \nsub-dependencies (dependencies of your dependencies), which may not be explicitly listed in \nyour pyproject.toml file. By locking all the dependencies and sub-dependencies to specific \nversions, the poetry.lock file ensures that all project installations use the same versions of each \npackage. This consistency leads to predictable behavior, reducing the likelihood of encountering \n“works on my machine” issues.\n\nChapter","section_id":27.0,"section_title":"Poetry: dependency and virtual environment management","chapter_id":7,"chapter_title":"Chapter 2: Tooling and Installation","book_id":1,"book_title":"LLM Engineers Handbook"}
{"key_idea_id":271,"concept":"Poetry's virtual environment for isolating project dependencies and preventing clashes","chunk_id":271,"chunk_text":"Another massive advantage of using Poetry is that it creates a new Python virtual environment in \nwhich it installs the specified Python version and requirements. A virtual environment allows you \nto isolate your project’s dependencies from your global Python dependencies and other projects. \nBy doing so, you ensure there are no version clashes between projects. For example, let’s assume \nthat Project A needs numpy == 1.19.5, and Project B needs numpy == 1.26.0. If you keep both \nprojects in the global Python environment, that will not work, as Project B will override Project A’s \nnumpy installation, which will corrupt Project A and stop it from working. Using Poetry, you can \nisolate each project in its own Python environment with its own Python dependencies, avoiding \nany dependency clashes.","section_id":27.0,"section_title":"Poetry: dependency and virtual environment management","chapter_id":7,"chapter_title":"Chapter 2: Tooling and Installation","book_id":1,"book_title":"LLM Engineers Handbook"}
{"key_idea_id":273,"concept":"Practical steps for Poetry installation and environment activation","chunk_id":273,"chunk_text":"You can install Poetry from here: https:\/\/python-poetry.org\/docs\/. We use Poetry 1.8.3 \nthroughout the book. Once Poetry is installed, navigate to your cloned LLM-Engineers-Hand-\nbook repository and run the following command to install all the necessary Python dependencies:\npoetry install --without aws\nThis command knows to pick up all the dependencies from your repository that are listed in \nthe pyproject.toml and poetry.lock files. After the installation, you can activate your Poetry \nenvironment by running poetry shell in your terminal or by prefixing all your CLI commands \nas follows: poetry run <your command>.","section_id":27.0,"section_title":"Poetry: dependency and virtual environment management","chapter_id":7,"chapter_title":"Chapter 2: Tooling and Installation","book_id":1,"book_title":"LLM Engineers Handbook"}
{"key_idea_id":267,"concept":"Pyenv's local Python version management for project repositories","chunk_id":267,"chunk_text":" 2\n27\n#   3.11.8\nTo make Python 3.11.8 the default version across your entire system (whenever you open a new \nterminal), use the following command:\npyenv global 3.11.8\nHowever, we aim to use Python 3.11.8 locally only in our repository. To achieve that, first, we have \nto clone the repository and navigate to it:\ngit clone https:\/\/github.com\/PacktPublishing\/LLM-Engineers-Handbook.git \ncd LLM-Engineers-Handbook\nBecause we defined a .python-version file within the repository, pyenv will know to pick up \nthe version from that file and use it locally whenever you are working within that folder. To \ndouble-check that, run the following command while you are in the repository:\npython --version\nIt should output:\n# Python 3.11.8\nTo create the .python-version file, you must run pyenv local 3.11.8 once. Then, pyenv will \nalways know to use that Python version while working within a specific directory.\nNow that we have installed the correct Python version using pyenv, let’s move on to Poetry, which \nwe will use as our dependency and virtual environment manager.","section_id":27.0,"section_title":"Poetry: dependency and virtual environment management","chapter_id":7,"chapter_title":"Chapter 2: Tooling and Installation","book_id":1,"book_title":"LLM Engineers Handbook"}
{"key_idea_id":277,"concept":"Comparison of Python virtual environment and dependency management tools","chunk_id":277,"chunk_text":" 2\n29\nOther tools similar to Poetry are Venv and Conda for creating virtual environments. Still, they lack \nthe dependency management option. Thus, you must do it through Python’s default requirements.\ntxt files, which are less powerful than Poetry’s lock files. Another option is Pipenv, which fea-\nture-wise is more like Poetry but slower, and uv, which is a replacement for Poetry built in Rust, \nmaking it blazing fast. uv has lots of potential to replace Poetry, making it worthwhile to test out: \nhttps:\/\/github.com\/astral-sh\/uv.","section_id":28.0,"section_title":"Poe the Poet: task execution tool","chapter_id":7,"chapter_title":"Chapter 2: Tooling and Installation","book_id":1,"book_title":"LLM Engineers Handbook"}
{"key_idea_id":283,"concept":"Poe the Poet's method for defining CLI tasks in pyproject.toml","chunk_id":283,"chunk_text":"When working with Poe the Poet, instead of having all your commands documented in a README \nfile or other document, you can add them directly to your pyproject.toml file and execute them \nin the command line with an alias. For example, using Poe the Poet, we can define the following \ntasks in a pyproject.toml file:\n[tool.poe.tasks]\ntest = \"pytest\"\nformat = \"black .\"\nstart = \"python main.py\"","section_id":28.0,"section_title":"Poe the Poet: task execution tool","chapter_id":7,"chapter_title":"Chapter 2: Tooling and Installation","book_id":1,"book_title":"LLM Engineers Handbook"}
{"key_idea_id":281,"concept":"Poe the Poet: a Poetry plugin for Python task automation","chunk_id":281,"chunk_text":"Poe the Poet is a plugin on top of Poetry that is used to manage and execute all the CLI commands \nrequired to interact with the project. It helps you define and run tasks within your Python proj-\nect, simplifying automation and script execution. Other popular options are Makefile, Invoke, or \nshell scripts, but Poe the Poet eliminates the need to write separate shell scripts or Makefiles for \nmanaging project tasks, making it an elegant way to manage tasks using the same configuration \nfile that Poetry already uses for dependencies.","section_id":28.0,"section_title":"Poe the Poet: task execution tool","chapter_id":7,"chapter_title":"Chapter 2: Tooling and Installation","book_id":1,"book_title":"LLM Engineers Handbook"}
{"key_idea_id":285,"concept":"Running defined tasks using the poe command","chunk_id":285,"chunk_text":"You can then run these tasks using the poe command:\npoetry poe test\npoetry poe format\npoetry poe start","section_id":28.0,"section_title":"Poe the Poet: task execution tool","chapter_id":7,"chapter_title":"Chapter 2: Tooling and Installation","book_id":1,"book_title":"LLM Engineers Handbook"}
{"key_idea_id":287,"concept":"installation of Poe the Poet as a Poetry plugin","chunk_id":287,"chunk_text":"You can install Poe the Poet as a Poetry plugin, as follows:\npoetry self add 'poethepoet[poetry_plugin]'","section_id":28.0,"section_title":"Poe the Poet: task execution tool","chapter_id":7,"chapter_title":"Chapter 2: Tooling and Installation","book_id":1,"book_title":"LLM Engineers Handbook"}
{"key_idea_id":294,"concept":"Hugging Face ecosystem benefits for LLM sharing and integration","chunk_id":294,"chunk_text":"We used Hugging Face as our model registry, as we can leverage its ecosystem to easily share our fine-tuned LLM Twin models with anyone who reads the book. Also, by following the Hugging Face model registry interface, we can easily integrate the model with all the frameworks around the LLMs ecosystem, such as Unsloth for fine-tuning and SageMaker for inference.","section_id":30.0,"section_title":"Hugging Face: model registry","chapter_id":7,"chapter_title":"Chapter 2: Tooling and Installation","book_id":1,"book_title":"LLM Engineers Handbook"}
{"key_idea_id":289,"concept":"Instructions for local infrastructure setup with ZenML and databases","chunk_id":289,"chunk_text":"2.Fill your .env file with all the necessary credentials as explained in the repository README.3.Run poetry poe local-infrastructure-up to locally spin up ZenML (http:\/\/127.0.0.1:8237\/) and the MongoDB and Qdrant databases.","section_id":30.0,"section_title":"Hugging Face: model registry","chapter_id":7,"chapter_title":"Chapter 2: Tooling and Installation","book_id":1,"book_title":"LLM Engineers Handbook"}
{"key_idea_id":291,"concept":"Locating local execution details in the LLM-Engineers-Handbook repository","chunk_id":291,"chunk_text":"You can read more details on how to run everything locally in the LLM-Engineers-Handbook re-pository README: https:\/\/github.com\/PacktPublishing\/LLM-Engineers-Handbook. Within the book, we will also show you how to deploy each component to the cloud.","section_id":30.0,"section_title":"Hugging Face: model registry","chapter_id":7,"chapter_title":"Chapter 2: Tooling and Installation","book_id":1,"book_title":"LLM Engineers Handbook"}
{"key_idea_id":293,"concept":"The definition and importance of a model registry in MLOps","chunk_id":293,"chunk_text":"Hugging Face: model registryA model registry is a centralized repository that manages ML models throughout their lifecycle. It stores models along with their metadata, version history, and performance metrics, serving as a single source of truth. In MLOps, a model registry is crucial for tracking, sharing, and docu-menting model versions, facilitating team collaboration. Also, it is a fundamental element in the deployment process as it integrates with continuous integration and continuous deployment(CI\/CD) pipelines.","section_id":30.0,"section_title":"Hugging Face: model registry","chapter_id":7,"chapter_title":"Chapter 2: Tooling and Installation","book_id":1,"book_title":"LLM Engineers Handbook"}
{"key_idea_id":296,"concept":"specific Hugging Face links for fine-tuned TwinLlama models","chunk_id":296,"chunk_text":"Our fine-tuned LLMs are available on Hugging Face at:• TwinLlama 3 1 8B (after fine-tuning): https:\/\/huggingface.co\/mlabonne\/TwinLlama-3.1-8B• TwinLlama 3 1 8B DPO (after preference alignment): https:\/\/huggingface.co\/mlabonne\/TwinLlama-3.1-8B-DPO","section_id":30.0,"section_title":"Hugging Face: model registry","chapter_id":7,"chapter_title":"Chapter 2: Tooling and Installation","book_id":1,"book_title":"LLM Engineers Handbook"}
{"key_idea_id":308,"concept":"Implementing a ZenML pipeline for user data extraction and crawling","chunk_id":308,"chunk_text":"Let’s explore how we can implement a ZenML pipeline with one of the ML pipelines implemented \nfor the LLM Twin project. In the code snippet below, we defined a ZenML pipeline that queries \nthe database for a user based on its full name and crawls all the provided links under that user:\nfrom zenml import pipeline\nfrom steps.etl import crawl_links, get_or_create_user\n@pipeline\nWe will focus only on the ZenML features used throughout the book, such as orches-\ntrating, artifacts, and metadata. For more details on ZenML, check out their starter \nguide: https:\/\/docs.zenml.io\/user-guide\/starter-guide.\n\nTooling and Installation\n34\ndef digital_data_etl(user_full_name: str, links: list[str]) -> None:\n    user = get_or_create_user(user_full_name)\n    crawl_links(user=user, links=links)","section_id":32.0,"section_title":"Orchestrator","chapter_id":7,"chapter_title":"Chapter 2: Tooling and Installation","book_id":1,"book_title":"LLM Engineers Handbook"}
{"key_idea_id":320,"concept":"Key considerations for defining modular ZenML pipeline steps","chunk_id":320,"chunk_text":"Within a ZenML step, you can define any Python logic your use case needs. In this simple example, \nwe are just creating or retrieving a user, but we could replace that code with anything, starting \nfrom data collection to feature engineering and training. What is essential to notice is that to \nintegrate ZenML with your code, you have to write modular code, where each function does just \none thing. The modularity of your code makes it easy to decorate your functions with @step and \nthen glue multiple steps together within a main function decorated with @pipeline. One design \nchoice that will impact your application is deciding the granularity of each step, as each will run \nas a different unit on a different machine when deployed in the cloud.","section_id":32.0,"section_title":"Orchestrator","chapter_id":7,"chapter_title":"Chapter 2: Tooling and Installation","book_id":1,"book_title":"LLM Engineers Handbook"}
{"key_idea_id":322,"concept":"LLM-Engineers-Handbook repository folder structure illustration","chunk_id":322,"chunk_text":"Tooling and Installation\n38\nThis folder structure is reflected at the root of the LLM-Engineers-Handbook repository, as il-\nlustrated in Figure 2.6:\nFigure 2.6: LLM-Engineers-Handbook repository folder structure","section_id":32.0,"section_title":"Orchestrator","chapter_id":7,"chapter_title":"Chapter 2: Tooling and Installation","book_id":1,"book_title":"LLM Engineers Handbook"}
{"key_idea_id":303,"concept":"The essential functions of an orchestrator for ML pipeline management","chunk_id":303,"chunk_text":"Orchestrator\nAn orchestrator is a system that automates, schedules, and coordinates all your ML pipelines. It \nensures that each pipeline—such as data ingestion, preprocessing, model training, and deploy-\nment—executes in the correct order and handles dependencies efficiently. By managing these \nprocesses, an orchestrator optimizes resource utilization, handles failures gracefully, and enhances \nscalability, making complex ML pipelines more reliable and easier to manage.","section_id":32.0,"section_title":"Orchestrator","chapter_id":7,"chapter_title":"Chapter 2: Tooling and Installation","book_id":1,"book_title":"LLM Engineers Handbook"}
{"key_idea_id":316,"concept":"ZenML UI insights for individual pipeline steps","chunk_id":316,"chunk_text":"Tooling and Installation\n36\nBy clicking on a specific step, you can get more insights into its code and configuration. It even \naggregates the logs output by that specific step to avoid switching between tools, as shown in \nFigure 2.5.\nFigure 2.5: Example of insights from a specific step of the digital_data_etl pipeline run","section_id":32.0,"section_title":"Orchestrator","chapter_id":7,"chapter_title":"Chapter 2: Tooling and Installation","book_id":1,"book_title":"LLM Engineers Handbook"}
{"key_idea_id":311,"concept":"ZenML dashboard for visualizing pipeline runs and associated stacks","chunk_id":311,"chunk_text":"After clicking on the digital_data_etl pipeline, you can visualize all the previous and current \npipeline runs, as seen in Figure 2.3. You can see which one succeeded, failed, or is still running. \nAlso, you can see the stack used to run the pipeline, where the default stack is the one used to \nrun your ML pipelines locally.\n\nChapter 2\n35\nFigure 2.3: ZenML digital_data_etl pipeline dashboard. Example of a specific pipeline","section_id":32.0,"section_title":"Orchestrator","chapter_id":7,"chapter_title":"Chapter 2: Tooling and Installation","book_id":1,"book_title":"LLM Engineers Handbook"}
