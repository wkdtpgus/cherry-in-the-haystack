@prefix llm: <http://example.org/llm-ontology#> .
@prefix owl: <http://www.w3.org/2002/07/owl#> .
@prefix rdfs: <http://www.w3.org/2000/01/rdf-schema#> .

llm: a owl:Ontology ;
    rdfs:label "LLM Ontology"@en ;
    rdfs:comment "LLM 관련 개념 온톨로지"@ko .

llm:ALBERT a owl:Class ;
    rdfs:label "ALBERT"@en ;
    llm:description "ALBERT(​A Lite BERT)는 BERT의 구조를 그대로 유지하면서 **파라미터 공유(parameter sharing)**와 **팩터라이즈드 임베딩(factorized embedding)** 기법을 도입해 모델 크기를 크게 축소한 **Encoder‑Only Transformer** 모델이다. 이 모델은 모든 Transformer 레이어가 동일한 가중치를 공유하고, 토큰 임베딩 차원을 숨김 차원보다 크게 분해해 **메모리 효율성**과 **학습 속도**를 크게 향상시키며, 마스크드 언어 모델링(Masked LM)과 **문장 순서 예측(Sentence Order Prediction)**이라는 추가 사전학습 목표를 통해 문맥 이해 능력을 보강한다. ALBERT는 **텍스트 분류, 개체명 인식, 질의응답, 문장 임베딩 생성** 등 순수 인코더 기반 NLP 작업에 널리 활용되며, 생성된 고품질 임베딩은 **시맨틱 검색, 벡터 검색, 유사도 기반 추천** 등에서 효율적인 **벡터 검색**을 가능하게 한다. BERT와 비교해 파라미터 수가 10배 이상 감소했으며, DistilBERT와는 압축 방식이 다르면서도 **경량화·성능 유지**라는 공통 목표를 공유하고, T5·BART와 같은 **Encoder‑Decoder** 모델과는 구조적으로 구분된다." ;
    rdfs:subClassOf llm:EncoderOnly .

llm:ALiBi a owl:Class ;
    rdfs:label "ALiBi"@en ;
    llm:description """**ALiBi(Attention with Linear Biases)**는 Transformer의 **Positional Encoding** 방식 중 하나로, 토큰 간 거리 \\(d\\)에 비례하는 **선형 바이어스**를 어텐션 점수에 직접 더해 절대 위치 임베딩을 사용하지 않는 방법이다.  
이 방식은 각 어텐션 헤드마다 미리 정의된 **슬로프(slope)** 값을 적용해 \\(\\text{score}_{ij}=Q_iK_j^\\top + \\text{slope}_h \\cdot d_{ij}\\)와 같이 점수를 조정함으로써, 모델이 **상대 위치** 정보를 암묵적으로 학습하고, 파라미터를 추가하지 않아도 **길이가 더 긴 시퀀스**로 자연스럽게 **extrapolation**할 수 있다.  
ALiBi는 **GPT‑NeoX**, **LLaMA**, **PaLM‑2** 등 대규모 언어 모델에서 **긴 컨텍스트(수천 토큰) 추론**, **zero‑shot** 성능 향상, **메모리·연산 효율**을 목표로 널리 채택되고 있으며, 별도의 위치 임베딩 테이블이 필요 없어 **학습 속도**와 **추론 속도**를 크게 개선한다.  
전통적인 **Sinusoidal** 혹은 **Learnable absolute positional embeddings**와 달리 **비학습형(bias‑only)** 구조를 갖는 ALiBi는 **RoPE(Rotary Positional Embedding)**와는 **선형 증가**라는 특성에서 차별화되며, 회전 불변성은 부족하지만 **스케일러블**하고 **장기 의존성**을 다루는 데 강점을 보여, 상황에 따라 **Positional Encoding**의 대체 혹은 보완 옵션으로 활용된다.""" ;
    rdfs:subClassOf llm:PositionalEncoding .

llm:APIIntegration a owl:Class ;
    rdfs:label "APIIntegration"@en ;
    llm:description "API Integration은 서로 다른 애플리케이션이나 서비스가 제공하는 Application Programming Interface를 활용해 데이터와 기능을 실시간으로 연결·교환하는 과정으로, 흔히 “API 연동” 혹은 “서비스 통합”이라고도 불립니다. 이 과정은 RESTful HTTP, SOAP, GraphQL 같은 프로토콜과 OAuth 2.0, API Key, JWT 등 다양한 인증·인가 메커니즘을 기반으로 SDK·미들웨어·API 게이트웨이를 통해 요청을 오케스트레이션하고, 데이터 매핑·트랜스포메이션·레이트 리밋·오류 처리 등을 자동화합니다. 대표적인 사용 사례로는 전자상거래 사이트가 결제 게이트웨이와 연동해 실시간 결제 처리를 하거나, CRM이 마케팅 자동화 툴과 연결돼 고객 행동 데이터를 실시간으로 동기화하는 경우, IoT 디바이스가 클라우드 엔드포인트에 텔레메트리를 전송하고 CI/CD 파이프라인이 클라우드 서비스 API를 호출해 배포를 자동화하는 시나리오 등이 있습니다. API Integration은 ToolUse 범주에 속하는 “프로그램적 툴 사용”의 핵심 기술이며, 화면 스크래핑이나 수동 데이터 입력과는 달리 구조화된 인터페이스를 통한 고신뢰·고효율 연동을 제공하고, 웹훅, 서비스 메쉬, 마이크로서비스, ETL 등과 연계돼 서비스 지향 아키텍처(SOA)·클라우드 SaaS·PaaS 환경에서 필수적인 연결 고리 역할을 합니다." ;
    rdfs:subClassOf llm:ToolUse .

llm:AStarSearch a owl:Class ;
    rdfs:label "AStarSearch"@en ;
    llm:description "AStarSearch, commonly called the A* algorithm, is a heuristic‑driven search procedure that finds the lowest‑cost path between a start node and a goal node in a weighted graph or grid. It operates by maintaining an open set (priority queue) ordered by the evaluation function f(n)=g(n)+h(n), where g(n) is the accumulated cost from the start and h(n) is an admissible, often consistent, heuristic estimate of the remaining distance, allowing A* to expand the most promising nodes first while guaranteeing optimality when the heuristic never overestimates. Typical applications include real‑time pathfinding in video‑game AI, robot navigation on occupancy maps, and large‑scale route planning for transportation or logistics systems where fast, optimal solutions are required. Compared with related search procedures such as Dijkstra’s algorithm (which uses h(n)=0), Uniform‑Cost Search, BFS/DFS, or Greedy Best‑First Search, A* uniquely balances exploration and exploitation, achieving better time‑space efficiency while preserving the optimal‑path guarantee under appropriate heuristics." ;
    rdfs:subClassOf llm:SearchProcedure .

llm:AWQ a owl:Class ;
    rdfs:label "AWQ"@en ;
    llm:description "AWQ(​**A**ctivation‑aware **W**eight **Q**uantization)는 대규모 언어 모델(LLM)과 같은 고성능 딥러닝 모델을 **저비트 정밀도(예: INT4, INT8)** 로 변환하면서도 **활성화 분포를 고려**해 정확도 손실을 최소화하는 **사후 양자화(Post‑Training Quantization, PTQ)** 기법이다. 이 방법은 모델의 가중치를 **다중 단계로 스케일링하고 라운딩**하여, 기존 FP16/FP32 가중치를 **정밀도‑효율적인 정수 형태**로 압축하고, GPU·CPU·Edge 디바이스에서 **추론 속도와 메모리 사용량을 크게 감소**시킨다. AWQ는 특히 **생성형 AI 서비스, 실시간 챗봇, 검색 엔진** 등에서 **대규모 파라미터를 가진 트랜스포머 모델을 저전력·저지연 환경에 배포**할 때 활용되며, 기존 GPTQ나 QAT(Quantization‑Aware Training)와 달리 **추가 학습 없이도 높은 정확도 유지**가 가능한 점이 큰 장점이다. 관련 개념으로는 **GPTQ, PTQ, QAT, LoRA, 지식 증류(Knowledge Distillation)** 등이 있으며, 이들 기법은 양자화 방식·정밀도·학습 요구사항에서 서로 차별화된다." ;
    rdfs:subClassOf llm:Quantization .

llm:AbsolutePositional a owl:Class ;
    rdfs:label "AbsolutePositional"@en ;
    llm:description "**AbsolutePositional**은 시퀀스의 각 토큰에 고정된 절대 위치 정보를 직접 부여하는 **PositionalEncoding** 방식으로, 입력 토큰의 순서를 인코딩하기 위해 사전에 정의된 위치 인덱스(예: 0, 1, 2, …)를 임베딩 벡터와 결합한다. 이 방법은 일반적으로 **sinusoidal 함수** 기반의 주기적 인코딩이나 **learnable position embedding** 행렬을 사용해 각 위치마다 고유한 벡터를 생성하고, 이를 토큰 임베딩에 **element‑wise addition** 혹은 **concatenation** 형태로 합쳐 self‑attention 메커니즘이 순서 정보를 인식하도록 만든다. 대표적인 활용 사례로는 **Transformer 기반 자연어 처리(NLP)**, **음성 인식**, **시계열 예측**, 그리고 **비전 트랜스포머(ViT)** 등에서 입력 시퀀스의 절대 순서를 명시적으로 반영해야 할 때 널리 쓰이며, 특히 길이가 고정된 데이터나 사전 정의된 순서가 중요한 작업에 효과적이다. 반면 **RelativePositionalEncoding**은 토큰 간 상대적 거리만을 모델링해 길이 일반화와 효율성을 높이는 반면, AbsolutePositional은 각 위치에 대한 고유한 편향(bias)을 제공해 순서 민감도가 높은 태스크에서 더 직관적인 위치 정보를 제공한다." ;
    rdfs:subClassOf llm:PositionalEncoding .

llm:Accuracy a owl:Class ;
    rdfs:label "Accuracy"@en ;
    llm:description "Accuracy(정확도)는 모델이 전체 샘플 중에서 정답 라벨과 일치시킨 예측의 비율을 의미하는 자동 평가 지표(AutomaticMetric)로, “정답 개수 ÷ 전체 샘플 수”로 계산됩니다. 이 메트릭은 예측값과 실제 라벨을 직접 비교해 일치 여부를 집계하고, 이진 분류·다중 클래스 분류·이미지 인식 등 다양한 분류 작업에서 모델 성능을 빠르게 파악하는 데 활용됩니다. 특히 클래스 비율이 균형 잡힌 데이터셋에서 전체 정확도를 직관적으로 제시해 모델 선택·튜닝·배포 파이프라인에 널리 사용되며, 자동 메트릭 시스템에서 로그와 대시보드에 실시간으로 표시됩니다. 다만 Precision, Recall, F1‑score, ROC‑AUC와 같은 다른 평가 지표와는 달리 오류 유형을 구분하지 않으므로, 불균형 데이터나 비용 민감도가 높은 상황에서는 보완적인 메트릭과 함께 고려하는 것이 일반적입니다." ;
    rdfs:subClassOf llm:AutomaticMetric .

llm:AutonomousExperimentation a owl:Class ;
    rdfs:label "AutonomousExperimentation"@en ;
    llm:description "**AutonomousExperimentation**은 **AgenticTask**의 하위 개념으로, 인공지능 에이전트가 인간의 개입 없이 스스로 가설을 생성·설계·실행·평가하는 **자율 실험 루프**를 의미합니다. 이 과정은 **폐쇄형 피드백 루프(closed‑loop)**와 **탐색‑활용(exploration‑exploitation) 전략**을 기반으로, **강화학습**, **메타러닝**, **시뮬레이션‑실제 연계** 등을 활용해 실험 설계와 파라미터 튜닝을 자동화하고, 결과를 실시간으로 **적응형 제어(adaptive control)**와 **자동 모델 업데이트**에 반영합니다. 대표적인 사용 사례로는 **신약 후보 물질 탐색**, **재료 과학에서의 고성능 합금 설계**, **로봇 행동 정책 최적화**, 그리고 **AI‑구동 자동화된 A/B 테스트** 등이 있으며, 이러한 분야에서는 **자동화된 가설 생성**, **실험 설계 최적화**, **데이터‑드리븐 의사결정**이 핵심 가치로 작용합니다. 관련 개념으로는 **AutoML**, **활동 학습(active learning)**, **자율 에이전트(self‑directed agents)**가 있으며, **수동 실험(manual experimentation)**과 대비될 때 **스케일러빌리티**, **속도**, **인간 오류 감소**가 두드러지는 차별점으로 강조됩니다." ;
    rdfs:subClassOf llm:AgenticTask .

llm:BART a owl:Class ;
    rdfs:label "BART"@en ;
    llm:description "BART (Bidirectional‑and‑AutoRegressive Transformers) 는 Encoder‑Decoder 구조를 기반으로 한 사전학습 언어 모델로, 입력 텍스트를 **양방향(마스크드) 인코더**와 **자기회귀(디코더)**가 순차적으로 처리해 복원·생성 작업을 동시에 수행한다는 점이 핵심이다. 사전학습 단계에서는 문장을 무작위로 **노이즈(마스킹, 셔플, 삭제 등)** 로 변형한 뒤, 인코더가 전체 문맥을 파악하고 디코더가 원본 문장을 **시퀀스‑투‑시퀀스** 방식으로 복원하도록 학습함으로써, **텍스트 요약, 기계 번역, 문장 완성, 질의응답** 등 다양한 생성·변환 태스크에 높은 적응력을 제공한다. 대표적인 사용 사례로는 **추상적 요약(Abstractive Summarization)**, **다중언어 번역(Multi‑Lingual Translation)**, **문서 재작성 및 정제(Text Rewriting)** 등이 있으며, Hugging Face Transformers와 같은 라이브러리에서 바로 활용할 수 있다. BART는 **GPT**와 같이 순수 디코더 기반의 **오토레그레시브 모델**과, **BERT**와 같이 순수 인코더 기반의 **마스크드 언어 모델** 사이에 위치해, 양쪽의 장점을 결합하면서도 **Seq2Seq** 프레임워크가 요구하는 **조건부 생성(Conditional Generation)** 능력을 제공한다. 이러한 특성은 **텍스트 임베딩**, **벡터 검색**, **다중모달 변환** 등에서 풍부한 의미 표현을 얻고자 할 때 특히 유용하다." ;
    rdfs:subClassOf llm:EncoderDecoder .

llm:BERT a owl:Class ;
    rdfs:label "BERT"@en ;
    llm:description "**BERT (Bidirectional Encoder Representations from Transformers)**는 Transformer 아키텍처의 **Encoder‑Only** 구조를 기반으로 한 사전학습 언어 모델로, 입력 문장의 모든 토큰을 양방향(self‑attention)으로 동시에 인코딩하여 **문맥‑민감한 임베딩**을 생성합니다. 핵심 작동 원리는 **Masked Language Modeling(MLM)**과 **Next Sentence Prediction(NSP)**이라는 두 가지 사전학습 목표를 통해 대규모 텍스트 코퍼스에서 **양방향 컨텍스트**를 학습하고, 이후 **Fine‑Tuning** 단계에서 질문‑응답, 감성 분석, 명명 엔티티 인식 등 다양한 NLP 태스크에 맞게 빠르게 적응할 수 있게 합니다. 대표적인 사용 사례로는 **검색 엔진의 의미 기반 검색**, **챗봇의 문장 이해**, **문서 분류 및 요약** 등에서 **문맥 임베딩**을 활용한 **시맨틱 검색**과 **지식 추출**이 있으며, 특히 벡터 검색 시스템에서 BERT‑기반 임베딩은 높은 재현율을 제공합니다. 관련 개념으로는 **Decoder‑Only**인 GPT 시리즈와 **Encoder‑Decoder** 구조의 T5, BART 등이 있으며, BERT는 **양방향 인코딩**에 초점을 맞춘 반면 GPT는 **단방향 생성**에, T5는 **텍스트‑투‑텍스트 변환**에 최적화된 차이점을 보입니다." ;
    rdfs:subClassOf llm:EncoderOnly .

llm:BLEU a owl:Class ;
    rdfs:label "BLEU"@en ;
    llm:description "BLEU (Bilingual Evaluation Understudy)는 기계 번역(MT) 시스템의 품질을 자동으로 정량화하는 대표적인 자동 평가 지표(AutomaticMetric)로, 후보 번역(candidate translation)과 하나 이상의 정답 번역(reference translation) 사이의 n‑gram 정밀도(precision)를 계산하고, 후보 문장의 길이가 과도하게 짧아지는 것을 방지하기 위해 brevity penalty(길이 패널티)를 적용한다. 이 지표는 1‑gram부터 4‑gram까지의 n‑gram 매칭 비율을 가중 평균한 뒤 로그‑스케일로 변환해 0~1 사이의 BLEU 점수로 표현하며, BLEU‑1, BLEU‑2, BLEU‑3, BLEU‑4와 같이 세부 변형도 활용된다. BLEU는 대규모 번역 코퍼스에서 신속하게 모델 성능을 비교·모니터링하는 데 널리 쓰이며, 특히 신경망 기반 번역(NMT), 통계적 기계 번역(SMT), 다국어 번역 파이프라인의 개발·튜닝 단계에서 핵심 지표로 활용된다. 반면 ROUGE는 요약 평가에, METEOR는 의미적 유사성과 어휘 다양성을 강조하는 평가에 더 적합하며, 인간 평가(human evaluation)와 결합해 BLEU만으로는 포착하기 어려운 문맥·의미 흐름을 보완한다." ;
    rdfs:subClassOf llm:AutomaticMetric .

llm:BPC a owl:Class ;
    rdfs:label "BPC"@en ;
    llm:description "**BPC (Bits Per Character)**는 문자 단위 언어 모델의 예측 성능을 정량화하는 자동 평가 지표(AutomaticMetric)로, 모델이 생성한 텍스트를 압축했을 때 평균적으로 필요로 하는 비트 수를 나타냅니다. 이 지표는 모델이 출력한 각 문자에 대한 로그 확률을 합산한 뒤 \\(‑\\frac{1}{N}\\sum_{i=1}^{N}\\log_{2}p(c_i)\\) 와 같이 계산되며, 값이 낮을수록(즉, bits per character 가 작을수록) 모델이 더 정확하고 효율적으로 언어를 예측한다는 의미입니다. BPC는 문자‑레벨 언어 모델, 텍스트 압축 알고리즘, 그리고 초소형 혹은 모바일 환경에서의 실시간 입력 예측 등에서 모델 비교·선정에 널리 활용되며, 특히 GPT‑style 또는 RNN‑based character generators의 품질을 객관적으로 측정하는 데 유용합니다. 관련 자동 메트릭으로는 Perplexity(당연히 BPC와 log‑likelihood 기반으로 변환 가능), Cross‑Entropy, BLEU, ROUGE 등이 있으며, BPC는 “bits per token” 과 대비돼 문자‑단위 세밀한 평가를 제공한다는 차별점을 갖습니다." ;
    rdfs:subClassOf llm:AutomaticMetric .

llm:BPE a owl:Class ;
    rdfs:label "BPE"@en ;
    llm:description "BPE(Byte Pair Encoding)는 텍스트를 **서브워드 단위**로 분할하는 토크나이제이션 기법으로, 가장 빈번하게 등장하는 문자(또는 문자 시퀀스) 쌍을 반복적으로 **머지(merge)**하여 고정된 크기의 **어휘(vocabulary)**를 구축한다. 이 과정은 초기에는 문자 레벨에서 시작해 점차 **바이트 쌍**을 결합함으로써 **압축(compression)**과 **어휘 일반화**를 동시에 달성하며, 새로운 단어가 등장해도 기존 서브워드 조합으로 표현할 수 있어 **희소성(sparsity) 감소**와 **효율적인 임베딩** 학습이 가능하다. BPE는 GPT, BERT, T5와 같은 **대규모 언어 모델**의 전처리 단계, 기계 번역, 텍스트 생성, 검색 엔진의 **인덱싱 및 벡터 검색** 등에 널리 활용되며, 특히 **멀티언어** 환경에서 단어 형태소를 유연하게 다룰 수 있다. 관련 토크나이제이션 방식으로는 **WordPiece**, **Unigram Language Model**, **SentencePiece** 등이 있으며, 이들와 비교했을 때 BPE는 **머지 규칙 기반**이라는 단순하면서도 직관적인 구조가 특징이다." ;
    rdfs:subClassOf llm:Tokenization .

llm:BeamSearch a owl:Class ;
    rdfs:label "BeamSearch"@en ;
    llm:description "Beam Search는 **시퀀스 생성 모델**에서 **다중 후보 경로를 동시에 유지하면서 최적의 토큰 시퀀스를 탐색**하는 **디코딩 전략(DecodingStrategy)** 중 하나로, 매 단계마다 상위 k개의 부분 시퀀스를 “빔”(beam)으로 선택해 확장함으로써 탐색 공간을 효율적으로 축소한다. 이 방법은 **빔 폭(beam width)**이라는 파라미터를 통해 탐색 깊이와 다양성을 조절하고, 각 후보의 누적 로그 확률을 기준으로 **스코어링**하고 정렬해 가장 가능성 높은 경로를 지속적으로 추적한다. Beam Search는 기계 번역, 텍스트 요약, 대화형 AI 등 **시퀀스‑투‑시퀀스** 작업에서 **그리디 디코딩(greedy decoding)**보다 높은 품질의 출력을 제공하며, **Top‑k/Top‑p 샘플링**과 같은 **확률적 디코딩**과는 달리 결정론적이며 재현 가능한 결과를 얻는 데 유리하다. 관련 개념으로는 탐색 효율성을 강조하는 **그리디 서치**, 다양성을 높이는 **샘플링 기반 디코딩**, 그리고 탐색 범위를 제한하는 **다중 빔(Multi‑Beam) 혹은 하이퍼파라미터 튜닝** 등이 있다." ;
    rdfs:subClassOf llm:DecodingStrategy .

llm:BenchmarkDataset a owl:Class ;
    rdfs:label "BenchmarkDataset"@en ;
    llm:description "**BenchmarkDataset**는 모델의 **EvaluationMetric**과 결합해 성능을 정량화하기 위해 사전에 정의된 **ground‑truth 라벨**과 **표준화된 데이터 분할(Train/Validation/Test)**을 제공하는 **표준화된 평가용 데이터셋**을 의미합니다. 이러한 데이터셋은 **다양한 도메인(자연어 처리, 컴퓨터 비전, 추천 시스템, 벡터 검색 등)**에서 **임베딩(embedding)** 혹은 **고차원 특징**을 이용한 **유사도 기반 검색(ANN, nearest‑neighbor)** 성능을 측정하도록 설계되어, **정확도(accuracy), F1‑score, BLEU, MAP, Recall@k**와 같은 **EvaluationMetric**과 직접 연동됩니다. 대표적인 사용 사례로는 **이미지 분류 benchmark (ImageNet), 질의‑응답 benchmark (SQuAD), 텍스트‑이미지 매칭 benchmark (MS‑COCO), 벡터 검색 benchmark (FAISS‑ANN, MS‑MARCO)** 등이 있으며, 연구 재현성(reproducibility)과 **베이스라인(baseline) 비교**를 위해 널리 활용됩니다. 이와 대비되는 개념으로는 **합성 데이터(synthetic data)**나 **프라이빗 테스트 세트**가 있으며, 전자는 통제된 환경에서 빠른 프로토타이핑에 적합하고, 후자는 실제 서비스 환경에 특화된 **도메인‑특화 평가**를 제공한다는 차이가 있습니다." ;
    rdfs:subClassOf llm:EvaluationMetric .

llm:BestFirstSearch a owl:Class ;
    rdfs:label "BestFirstSearch"@en ;
    llm:description "Best‑First Search는 휴리스틱 함수 h(n) 또는 비용 함수 f(n) 에 의해 정의된 우선순위에 따라 탐색 프론티어를 정렬하고, 가장 유망한 노드를 먼저 확장하는 **휴리스틱 기반 탐색 절차**(search procedure)이다. 이 알고리즘은 일반적으로 최소 힙이나 우선순위 큐(priority queue)를 사용해 현재 상태 공간(state space)에서 평가 점수가 가장 낮은(또는 높은) 노드를 선택하고, 선택된 노드의 자식들을 프론티어에 삽입함으로써 **그리디하게(greedy) 혹은 비용‑가중치 방식으로** 탐색을 진행한다. 대표적인 사용 사례로는 경로 찾기(pathfinding)에서 A* 검색, 게임 AI에서 최적 이동 선택, 로봇 공학에서 실시간 장애물 회피, 그리고 대규모 그래프 분석에서 최단 경로 혹은 최적 솔루션 탐색이 있다. 관련 개념으로는 탐색 순서가 깊이 우선(DFS)이나 너비 우선(BFS)으로 고정되는 **비휴리스틱 탐색**과, 비용만을 고려해 최적 경로를 보장하는 Uniform‑Cost Search·다익스트라 알고리즘이 있으며, Best‑First Search는 이들에 비해 **휴리스틱 정보 활용**을 통해 탐색 효율성을 크게 향상시킨다." ;
    rdfs:subClassOf llm:SearchProcedure .

llm:BiasDetection a owl:Class ;
    rdfs:label "BiasDetection"@en ;
    llm:description "**BiasDetection**은 인공지능 모델이나 데이터셋에 내재된 사회적·문화적 편향을 자동으로 식별하고 정량화하는 **SafetyTechnique**의 한 분야이다. 주로 통계적 차이 분석, 민감도 테스트, 어휘·표현 임베딩 비교 등 **편향 탐지 알고리즘**을 활용해 특정 속성(성별, 인종, 연령 등)과 결과 변수 간의 불균형을 **실시간 모니터링**하거나 사전 검증 단계에서 **공정성 지표**(예: Demographic Parity, Equalized Odds)를 계산한다. 이 기술은 채용 자동화, 신용 스코어링, 콘텐츠 추천, 의료 진단 등 **편향이 심각한 영향을 미칠 수 있는** 다양한 산업 현장에서 모델 배포 전·후에 **편향 감지**를 수행해 차별 위험을 사전에 차단한다. 관련 개념으로는 **BiasMitigation(편향 완화)**, **FairnessMetrics(공정성 측정)**, **Debiasing(편향 교정)** 등이 있으며, **BiasDetection**은 편향을 **발견**하는 단계에 초점을 맞추고, **Mitigation**은 발견된 편향을 **수정**하는 단계와 구분된다." ;
    rdfs:subClassOf llm:SafetyTechnique .

llm:CausalLanguageModeling a owl:Class ;
    rdfs:label "CausalLanguageModeling"@en ;
    llm:description "Causal Language Modeling(인과적 언어 모델링)은 텍스트 시퀀스에서 **다음 토큰을 예측**하는 **자기지도(pretraining) 목표**로, 입력 토큰들을 **시간 순서대로(autoregressive) 한 방향으로만** 보면서 학습하는 방식이다. 이때 모델은 **디코더‑전용 Transformer** 구조에 **인과적 마스킹(causal masking)**을 적용해 현재 위치 이후의 정보를 차단하고, **다음 토큰 확률 분포**를 출력함으로써 **다음 단어 생성** 능력을 습득한다. 대표적인 활용 사례로는 **GPT·GPT‑2·GPT‑3와 같은 대규모 생성형 AI**, **코드 자동완성**, **대화형 챗봇**, **few‑shot 프롬프트 학습** 등이 있으며, 사전학습된 가중치를 그대로 활용해 **텍스트 생성·요약·번역·질문응답** 등 다양한 자연어 처리 작업에 적용된다. 반면, **Masked Language Modeling(MLM)**처럼 양방향 컨텍스트를 이용하는 **BERT** 계열 모델과는 달리 Causal LM은 **단방향(uni‑directional) 컨텍스트**만을 사용하므로 **시퀀스 생성**에 유리하지만 **문맥 이해** 측면에서는 양방향 모델에 비해 제한적인 특성을 보인다." ;
    rdfs:subClassOf llm:Pretraining .

llm:ChainOfThought a owl:Class ;
    rdfs:label "ChainOfThought"@en ;
    llm:description """**Chain‑of‑Thought (CoT)**는 대형 언어 모델(Large Language Model, LLM)에게 “한 단계씩 생각해 보라”는 지시를 포함한 **고급 프롬프팅(Advanced Prompting)** 기법으로, 질문에 대한 답을 바로 제시하는 대신 **단계별 추론(step‑by‑step reasoning)·연쇄적 사고(chain of reasoning)** 과정을 텍스트로 출력하도록 유도한다.  
이 방식은 프롬프트에 “먼저 문제를 분석하고, 필요한 정보를 정리한 뒤, 최종 결론을 도출한다”와 같은 **프롬프트 설계(prompt engineering)** 지시를 삽입하거나, **Few‑shot**·**Zero‑shot CoT** 예시를 제공해 모델이 **추론 흐름(reasoning chain)** 을 스스로 생성하도록 하는 **프롬프트 분해(prompt decomposition)** 메커니즘을 활용한다.  
대표적인 활용 분야는 수학·과학 문제 풀이, 상식·논리 추론, 다중 홉 질의응답(multi‑hop QA), 코드 생성 및 디버깅, 복잡한 계획 수립 등 **복합적 사고와 해석 가능성(interpretability)**이 요구되는 작업이며, **Self‑Consistency**와 결합해 여러 추론 경로를 평균화하는 방식으로 정확도를 높일 수 있다.  
관련 개념으로는 **직접 프롬프팅(direct prompting)**·**Zero‑shot prompting**과 대비되는 **Tree‑of‑Thought**, **Self‑Ask**, **Decomposition** 등이 있으며, CoT는 이러한 기법들보다 **연속적인 사고 흐름을 명시적으로 드러내는** 점에서 차별화된다.""" ;
    rdfs:subClassOf llm:AdvancedPrompting .

llm:Classification a owl:Class ;
    rdfs:label "Classification"@en ;
    llm:description "**Classification**은 자연어 이해(NLU) 영역에서 텍스트를 사전에 정의된 라벨(예: 감성, 주제, 의도) 중 하나 혹은 여러 개에 자동으로 매핑하는 **텍스트 분류** 작업을 의미합니다. 일반적으로 **지도 학습** 기반의 머신러닝·딥러닝 모델이 입력 문장을 **벡터화(임베딩)** 한 뒤, **다중 클래스**·**멀티라벨** 분류기를 통해 **특징 추출**과 **확률 예측**을 수행하며, BERT·RoBERTa·GPT와 같은 **Transformer** 기반 사전학습 모델이 현재 주류를 이루고 있습니다. 대표적인 활용 사례로는 **감성 분석**(긍정/부정), **주제 분류**(뉴스 카테고리), **의도 인식**(챗봇 명령), **스팸 탐지**(이메일/메시지) 등이 있으며, **zero‑shot**·**few‑shot classification**을 이용해 라벨이 적은 도메인에도 빠르게 적용할 수 있습니다. 이와 대비되는 개념으로는 **클러스터링**(비지도 군집화)이나 **회귀(Regression)**(연속값 예측)가 있으며, **엔티티 인식(Named Entity Recognition)**·**시퀀스 라벨링**과는 목표 라벨의 granularity와 학습 방식에서 차이를 보입니다." ;
    rdfs:subClassOf llm:NLU .

llm:Claude a owl:Class ;
    rdfs:label "Claude"@en ;
    llm:description "Claude는 Anthropic이 개발한 **Decoder‑Only 기반 대형 언어 모델(LLM)** 로, 순차적인 토큰 예측을 통해 자연어 텍스트를 생성하고 질문에 답변하는 *텍스트 생성* 및 *대화형 AI* 에 특화된 모델이다. 이 모델은 **Transformer 디코더 스택**만을 사용해 입력 프롬프트를 그대로 이어받아 다음 토큰을 확률적으로 선택하며, **Instruction‑Tuning**, **RLHF(Reinforcement Learning from Human Feedback)**, 그리고 Anthropic 고유의 **Constitutional AI** 안전 프레임워크를 결합해 높은 응답 품질과 윤리적 거버넌스를 동시에 달성한다. Claude 2·3와 같은 최신 버전은 고객 지원 챗봇, 코드 보조, 콘텐츠 생성, 지식 검색 보강 등 *대화형 에이전트*, *텍스트 요약*, *프롬프트 엔지니어링* 등 다양한 실무 시나리오에 적용되고 있다. 이와 대비되는 **Encoder‑Decoder** 구조(예: T5, BART)나 **Encoder‑Only** 모델(예: BERT, RoBERTa)은 주로 문장 임베딩, 텍스트 분류, 마스킹 복원 등에 최적화된 반면, Claude는 **생성 중심** 작업에 강점을 보이며, **GPT‑계열**, **PaLM**, **LLaMA**와 같은 다른 Decoder‑Only LLM과도 기능·성능 면에서 비교·대조된다." ;
    rdfs:subClassOf llm:DecoderOnly .

llm:CodeExecution a owl:Class ;
    rdfs:label "CodeExecution"@en ;
    llm:description "**CodeExecution**은 프로그램 코드(스크립트, 바이너리, 함수 등)를 런타임 환경에서 동적으로 실행하여 실제 연산 결과를 산출하는 **ToolUse**의 핵심 하위 개념으로, 인터프리터·컴파일러·컨테이너·서버리스 인프라 등 다양한 실행 엔진을 통해 입력 파라미터를 받아 즉시 계산·처리를 수행한다. 주요 특징으로는 **실시간 피드백**(출력, 로그, 에러 메시지), **보안 샌드박스**(격리된 리소스·권한 관리), **자동화 파이프라인 연동**(CI/CD, 데이터 파이프라인, 머신러닝 모델 서빙) 및 **멀티스레드·병렬 실행**을 통한 성능 최적화가 있다. 대표적인 사용 사례는 **코드 기반 테스트 자동화**, **동적 데이터 변환 스크립트 실행**, **AI 어시스턴트가 호출하는 외부 툴(예: Python REPL, SQL 엔진)로의 실시간 연산**, 그리고 **클라우드 함수·컨테이너 기반 서버리스 작업** 등이다. 관련 개념으로는 **FunctionCalling**(LLM이 함수 호출을 트리거하는 방식)과 **CodeGeneration**(코드 생성 후 실행) 등이 있으며, 정적 분석·코드 리뷰와 같이 실행을 수반하지 않는 **Static Analysis**와는 실행 시점의 부작용·리소스 소비가 차별점으로 구분된다." ;
    rdfs:subClassOf llm:ToolUse .

llm:CodeIntelligence a owl:Class ;
    rdfs:label "CodeIntelligence"@en ;
    llm:description "**CodeIntelligence**는 인공지능·머신러닝 기술을 활용해 소스코드의 의미를 파악하고, 자동 완성·버그 탐지·리팩터링 제안·성능 최적화 등 지능형 서비스를 제공하는 **ApplicationDomain**의 하위 분야이다. 이 개념은 정적 분석과 동적 프로파일링을 결합한 뒤, 대규모 프로그래밍 언어 모델(LLM)과 코드 시맨틱 그래프를 이용해 프로그램 흐름을 예측하고, 실시간으로 IDE나 CI/CD 파이프라인에 코드 품질·보안·성능 인사이트를 생성한다. 대표적인 사용 사례로는 AI 기반 코드 어시스턴트(자동 완성·스니펫 제안), 자동화된 코드 리뷰·보안 스캐닝, 성능 튜닝 도구, 그리고 AI 페어 프로그래밍 플랫폼이 있다. 관련 개념으로는 전통적인 **정적 코드 분석**·**동적 분석**, **프로그램 합성** 등이 있으며, CodeIntelligence는 규칙 기반 검증을 넘어 컨텍스트‑aware 시맨틱 이해와 생성 능력(코드 자동 생성·수정)을 제공한다는 점에서 차별화된다." ;
    rdfs:subClassOf llm:ApplicationDomain .

llm:CommonSenseReasoning a owl:Class ;
    rdfs:label "CommonSenseReasoning"@en ;
    llm:description "**CommonSenseReasoning**은 인간이 일상 생활에서 자연스럽게 사용하는 상식(commonsense knowledge)을 기반으로, 텍스트나 상황에 대한 추론을 수행하는 **ReasoningTask**의 한 형태이다. 이 추론은 대규모 사전학습 언어 모델(Pre‑trained Language Model)이나 지식 그래프(Knowledge Graph)와 같은 외부 상식 데이터베이스를 활용해, “사과는 떨어진다”, “비가 오면 우산이 필요하다”와 같은 세계 지식(World Knowledge)과 상황적 인과관계(Situational Causality)를 자동으로 연결하고, 논리적 일관성을 유지하면서 답을 생성한다. 대표적인 사용 사례로는 대화 시스템에서의 자연스러운 응답 생성, 질문‑응답(Q&A)에서의 암묵적 전제 파악, 그리고 로봇이나 가상 에이전트가 현실 세계에서 행동 계획을 세울 때의 상황 추론 등이 있다. 관련 개념으로는 **LogicalReasoning**(형식 논리 기반 추론)이나 **SymbolicReasoning**(규칙 기반 추론)과 대비되며, CommonSenseReasoning은 비형식적이고 경험 기반의 지식을 강조한다." ;
    rdfs:subClassOf llm:ReasoningTask .

llm:Communicator a owl:Class ;
    rdfs:label "Communicator"@en ;
    llm:description "**Communicator**는 **AgentComponent** 계층에 속하는 모듈형 서브시스템으로, 에이전트 간 혹은 에이전트와 외부 시스템 사이의 **메시지 전달**과 **프로토콜 관리**를 담당하는 핵심 컴포넌트이다. 이 컴포넌트는 **동기/비동기 이벤트 큐**, **Pub/Sub 브로드캐스트**, **REST/GRPC API** 등 다양한 통신 메커니즘을 추상화하여 **디코플링된** 메시지 흐름을 구현하고, **네트워크 지연**이나 **패킷 손실**을 감안한 재전송·우선순위 조정 로직을 내장한다. 대표적인 사용 사례로는 **멀티에이전트 로봇 협업**, **분산 AI 파이프라인**, **IoT 디바이스 연동** 및 **대규모 챗봇 클러스터**에서 에이전트 간 상태 동기화와 명령 전파를 수행하는 경우가 있다. 관련 개념으로는 입력을 담당하는 **Sensor**, 행동을 실행하는 **Actuator**, 그리고 메시지 라우팅을 담당하는 **MessageBus**·**Middleware**가 있으며, Communicator는 이들와 달리 **양방향 통신**과 **프로토콜 어댑터** 역할에 특화되어 있다." ;
    rdfs:subClassOf llm:AgentComponent .

llm:ConstitutionalAI a owl:Class ;
    rdfs:label "ConstitutionalAI"@en ;
    llm:description "Constitutional AI는 사전 정의된 “헌법”(일련의 원칙·규칙) 을 기준으로 대규모 언어 모델이 자체적으로 답변을 검증·수정하도록 설계된 AI 정렬(Alignment) 기법으로, 인간 피드백 없이도 모델이 안전하고 윤리적인 출력을 생성하도록 유도한다. 이 접근법은 기본 모델에 헌법 프롬프트를 적용해 답변을 생성한 뒤, 헌법 위반 여부를 자동 평가하고, 위반 시 재생성·정제 과정을 반복하는 반복적 자기‑교정 루프를 사용하며, RLHF(강화학습 인간 피드백)와 달리 명시적 정책 (Constitution) 과 자동 검증 모듈을 핵심 메커니즘으로 삼는다. 대표적인 사용 사례로는 챗봇·검색 어시스턴트·코드 생성 도구 등에서 편향·유해 콘텐츠 억제, 법적·윤리적 규정 준수, 기업 내부 정책 적용 등을 실시간으로 구현하는 것이며, 오픈AI·Anthropic·Google 등이 실제 제품에 적용하고 있다. 관련 개념으로는 RLHF, Supervised Fine‑Tuning, AI Safety, Interpretability, Rule‑Based Alignment 등이 있으며, Constitutional AI는 사전 정의된 원칙에 기반한 “원칙‑기반 정렬” 이라는 점에서 피드백‑중심 RLHF와 대조된다." ;
    rdfs:subClassOf llm:Alignment .

llm:ContentFiltering a owl:Class ;
    rdfs:label "ContentFiltering"@en ;
    llm:description "**ContentFiltering**은 사용자·시스템이 생성하거나 전송하는 텍스트, 이미지, 동영상 등 다양한 미디어에서 불법·유해·부적절한 콘텐츠를 자동으로 탐지·제거하는 **SafetyTechnique**이다. 일반적으로 사전 정의된 블랙리스트·화이트리스트 규칙, 머신러닝 기반 텍스트 분류기, 이미지 인식 모델, 그리고 컨텍스트‑aware 자연어 처리 파이프라인을 결합해 실시간으로 입력을 스캔하고, 위험도가 높은 항목을 차단하거나 경고한다. 이 기술은 소셜 미디어 플랫폼의 **콘텐츠 모더레이션**, 온라인 포럼·채팅 서비스의 **스팸·혐오 발언 차단**, 전자상거래 사이트의 **상품 이미지·설명 검증**, 그리고 기업 내부 커뮤니케이션 툴의 **보안·규정 준수** 등에 널리 적용된다. 관련 개념으로는 모델 출력 제어를 목표로 하는 **SafetyGuardrails**·**PromptGuard**와 달리, **ContentFiltering**은 입력 단계에서 위험 콘텐츠를 사전에 걸러내는 점에서 차별화되며, 종종 **Toxicity Detection**, **Misinformation Filter**, **Adult‑Content Blocker**와 같은 세부 서브모듈과 연계된다." ;
    rdfs:subClassOf llm:SafetyTechnique .

llm:ContinualLearning a owl:Class ;
    rdfs:label "ContinualLearning"@en ;
    llm:description "Continual Learning(또는 Lifelong Learning, Incremental Learning)은 모델이 새로운 데이터나 과업을 **실시간·온라인**으로 받아들이면서 기존에 학습한 지식을 **보존**하고 **점진적으로 확장**하는 학습 패러다임이다. 이 방식은 **Catastrophic Forgetting**(망각 현상)을 방지하기 위해 가중치 고정, Elastic Weight Consolidation, 메모리 리플레이, Knowledge Distillation 등 **지식 유지 메커니즘**을 적용하고, **Task‑Incremental**·**Domain‑Incremental**·**Class‑Incremental** 등 다양한 시나리오에서 **스트리밍 데이터**와 **비정상적(Non‑Stationary) 환경**에 적응한다. 대표적인 활용 사례로는 로봇의 지속적인 동작 학습, 자율주행 차량의 도로 상황 업데이트, 개인화 추천 시스템·디지털 어시스턴트의 사용자 행동 변화 반영, 의료·금융 분야에서 데이터 드리프트에 대응하는 **모델 업데이트** 등이 있다. 관련 개념으로는 **Online Learning**, **Transfer Learning**, **Meta‑Learning**, **Multi‑Task Learning**이 있으며, 전통적인 **Batch(Offline) Training**과 달리 전체 데이터를 다시 수집·재학습하지 않아도 **지속적인 모델 적응**이 가능하다는 점이 차별점이다." ;
    rdfs:subClassOf llm:TrainingParadigm .

llm:Controller a owl:Class ;
    rdfs:label "Controller"@en ;
    llm:description "**Controller**는 AgentComponent 계층 구조에서 에이전트의 **행동 선택**과 **실행 제어**를 담당하는 핵심 모듈로, 현재 상태와 센서 입력을 받아 정책 (policy) 또는 제어법칙에 따라 액추에이터에 전달할 명령을 생성한다. 이 컴포넌트는 **피드백 루프**를 기반으로 실시간으로 상태 추정 (state estimation)과 보상 신호 (reward signal)를 활용해 행동을 조정하며, 강화학습 (reinforcement learning)이나 모델 기반 제어 (model‑based control)와 같은 알고리즘이 내부에 구현될 수 있다. 대표적인 사용 사례로는 로봇 (manipulator) 제어, 게임 AI 의 행동 관리, 자율주행 vehicle 의 차선 유지·속도 조절, 그리고 다중 에이전트 시뮬레이션 에서의 협동·경쟁 행동 조정 등이 있다. 관련 개념으로는 **Planner**(계획 수립)·**Policy**(정책)·**Actuator**(구동기)·**Observer**(관측자) 등이 있으며, Controller는 주로 **실시간 실행**에 초점을 맞추는 반면 Planner는 **전략적 목표**를 미리 설계한다는 점에서 차별화된다." ;
    rdfs:subClassOf llm:AgentComponent .

llm:Coordinator a owl:Class ;
    rdfs:label "Coordinator"@en ;
    llm:description "**Coordinator**는 **AgentComponent** 계층에 속하는 핵심 구성 요소로, 다수의 에이전트가 동시에 동작하는 **멀티에이전트 시스템**에서 작업 배분·통신 라우팅·리소스 할당·상태 일관성을 관리·조정하는 역할을 수행합니다. 이 컴포넌트는 이벤트‑드리븐 혹은 메시지‑패싱 메커니즘을 활용해 에이전트 간의 **협업**을 촉진하고, 작업 스케줄링과 동기화를 자동화함으로써 시스템 전체의 **오케스트레이션**을 가능하게 합니다. 대표적인 사용 사례로는 **분산 강화학습**, **스웜 로봇** 제어, **협업 게임 AI**, **스마트 팩토리**의 생산 라인 최적화, 그리고 클라우드 환경에서의 **분산 학습** 및 서비스 배포 등이 있습니다. 관련 개념으로는 단순히 작업을 수행하는 **Worker**와 구분되는 **Mediator**, **Orchestrator**, **Scheduler** 등이 있으며, Coordinator는 이들보다 높은 수준의 정책 제어와 저수준 상호작용 관리 기능을 동시에 제공한다는 점에서 차별화됩니다." ;
    rdfs:subClassOf llm:AgentComponent .

llm:Cost a owl:Class ;
    rdfs:label "Cost"@en ;
    llm:description "**Cost**는 자동화된 평가 지표(AutomaticMetric) 중 하나로, 모델이나 시스템이 특정 작업을 수행할 때 소모되는 **컴퓨팅 자원·시간·에너지** 등을 정량화한 수치 또는 비율을 의미한다. 이 지표는 **연산량(FLOPs), 추론 지연시간(latency), 메모리 사용량, 전력 소비량** 등을 측정하고, 필요에 따라 **시간당 비용, GPU·CPU 비용, 클라우드 사용료** 등 금전적 단위로 정규화하여 ‘비용 효율성(efficiency)’을 평가한다. 대표적인 활용 사례로는 **모델 선택·배포 최적화, 비용‑성능 트레이드오프(cost‑accuracy trade‑off) 분석, 실시간 서비스의 비용‑예산 관리, 에너지‑친화적 AI 설계** 등이 있으며, 특히 대규모 언어 모델의 **추론 비용**을 최소화하려는 ‘cost‑aware generation’ 시나리오에서 핵심 지표로 사용된다. 관련 개념으로는 **정확도(accuracy), 정밀도(precision), 리콜(recall), 속도(speed), 효율성(efficiency), 비용‑편익 분석(cost‑benefit analysis)** 등이 있으며, ‘Cost’ 는 **성능 중심 지표**와 대비되어 **자원 제한 환경**에서의 실용성을 강조한다." ;
    rdfs:subClassOf llm:AutomaticMetric .

llm:Critic a owl:Class ;
    rdfs:label "Critic"@en ;
    llm:description "**Critic**는 에이전트 컴포넌트(AgentComponent) 중 하나로, 현재 정책이 선택한 행동에 대한 가치(value) 혹은 기대 보상(reward) 추정을 담당하는 평가 모듈이다. 이 컴포넌트는 상태‑행동 가치 함수(state‑action value function) 혹은 상태 가치 함수(state value) 를 학습하기 위해 템포럴‑디퍼런스(Temporal‑Difference) 오류, 몬테카를로 샘플, 혹은 베이스라인 기반 어드밴티지(advantage) 추정과 같은 강화학습 기법을 활용하며, 정책 그라디언트(policy gradient) 업데이트에 필요한 피드백 신호를 제공한다. 대표적인 사용 사례로는 로봇 제어, 게임 AI, 자율 주행 및 추천 시스템 등에서 액터‑크리틱(actor‑critic) 구조의 핵심 요소로 작동해 학습 안정성 및 샘플 효율성을 높이는 데 활용된다. 크리틱은 행동을 생성하는 **Actor**와 대비되는 역할을 수행하며, 가치 함수(value function)와 보상 모델(reward model) 등 다른 에이전트 컴포넌트와 상호 보완적으로 동작한다." ;
    rdfs:subClassOf llm:AgentComponent .

llm:DPO a owl:Class ;
    rdfs:label "DPO"@en ;
    llm:description "DPO(Direct Preference Optimization)는 인간 피드백으로부터 얻은 선호 데이터(Preference Data)를 직접 활용해 언어 모델의 정책을 최적화하는 Preference Optimization 기법으로, 별도의 보상 모델을 학습하지 않고 선호 쌍(pairwise preference) 간의 로그우도 차이를 최소화하는 목표 함수를 통해 파라미터를 업데이트한다. 이 방법은 KL‑divergence 제약을 적용한 정책 그라디언트(policy gradient) 방식으로, 기존 RLHF(Reinforcement Learning from Human Feedback)에서 사용되는 PPO(Proximal Policy Optimization)와 달리 샘플 효율성이 높고 학습 안정성이 뛰어나며, SFT(Supervised Fine‑Tuning) 단계 이후 바로 적용할 수 있다. DPO는 챗봇 대화 응답 순위 매기기, 검색 결과 재정렬, 코드 생성 품질 향상 등 인간 선호가 중요한 생성형 AI 서비스에 널리 활용되며, 특히 오프라인(pre‑collected) 선호 데이터만으로도 효과적인 파인튜닝이 가능하다. 관련 개념으로는 보상 모델 기반 RLHF, Contrastive Learning 기반 Preference Modeling, 그리고 PPO‑based 정책 최적화가 있으며, DPO는 보상 모델을 별도로 학습하지 않는다는 점에서 이들 방법과 차별화된다." ;
    rdfs:subClassOf llm:PreferenceOptimization .

llm:DPOalsoPreferenceOptimization a owl:Class ;
    rdfs:label "DPO (also: PreferenceOptimization)"@en ;
    llm:description "DPO(Direct Preference Optimization), also referred to as Preference Optimization, is an alignment technique that directly tunes a language‑model policy to maximize the probability of outputs preferred by human annotators, bypassing the intermediate reward‑model step used in traditional RLHF. It works by constructing a pairwise preference dataset, applying a contrastive loss (or a KL‑regularized policy‑gradient objective) that pushes the model’s logits toward the chosen response while penalizing the rejected one, thereby performing offline reinforcement learning with explicit human preference signals. DPO is employed for fine‑tuning large language models in safety‑critical chatbots, instruction‑following assistants, and content‑moderation systems where rapid, stable alignment to user values is required. Compared with RLHF, DPO eliminates the need for a separate reward model and reduces variance in policy updates, while sharing conceptual ground with preference‑based learning, reward‑modeling, and other alignment methods such as supervised fine‑tuning and reinforcement learning from human feedback." ;
    rdfs:subClassOf llm:Alignment .

llm:DataAnalysis a owl:Class ;
    rdfs:label "DataAnalysis"@en ;
    llm:description "**DataAnalysis**는 원시 데이터에서 통계·머신러닝·데이터 마이닝 기법을 적용해 패턴, 트렌드, 인사이트를 추출하고, 이를 기반으로 의사결정을 지원하는 **AgenticTask**(에이전트 기반 작업)의 핵심 하위 작업이다. 이 과정은 데이터 수집·정제(ETL) → 탐색적 분석·시각화 → 모델 학습·예측 → 결과 해석·보고서 자동 생성이라는 파이프라인으로 자동화되며, AI 에이전트가 작업 흐름을 스스로 조정하고 최적화한다. 대표적인 사용 사례로는 매출 예측, 고객 세분화, 이상 탐지, 실시간 로그 분석 등 빅데이터 환경에서 비즈니스 인텔리전스와 운영 효율성을 높이는 분야가 있다. 관련 개념으로는 **데이터 전처리**, **데이터 시각화**, **데이터 엔지니어링**이 있으며, 단순 데이터 저장이나 수집과는 달리 분석 결과를 직접적인 행동 제안이나 자동화된 의사결정으로 연결한다는 점이 차별점이다." ;
    rdfs:subClassOf llm:AgenticTask .

llm:DeBERTa a owl:Class ;
    rdfs:label "DeBERTa"@en ;
    llm:description "DeBERTa(Disentangled BERT with Enhanced Attention)는 Transformer 구조를 기반으로 한 **Encoder‑Only** 모델로, 입력 토큰을 고차원 **벡터 임베딩**으로 변환해 문맥 정보를 인코딩합니다. 기존 BERT와 달리 **분리된(Disentangled) 토큰‑콘텐츠와 위치‑임베딩**을 독립적으로 학습하고, **상대 위치 편향(Relative Position Bias)**과 **다중‑헤드 디스엔탱글드 어텐션(Disentangled Attention)**을 도입해 장거리 의존성을 더 정교하게 포착함으로써 표현력과 효율성을 동시에 향상시킵니다. 이러한 고품질의 문장‑레벨 및 토큰‑레벨 **벡터 표현**은 텍스트 분류, 질의응답, 자연어 추론, 검색‑엔진 재랭킹 등 다양한 **NLP downstream task**에 널리 활용되며, 특히 **벡터 검색**·**유사도 매칭** 시 높은 정확도를 제공합니다. DeBERTa는 BERT·RoBERTa와 같은 기존 Encoder‑Only 모델과 비교해 **상대 위치 인코딩**과 **디스엔탱글드 어텐션**이라는 차별화된 메커니즘을 갖고 있어, 더 풍부한 의미적·구조적 정보를 학습한다는 점에서 구별됩니다." ;
    rdfs:subClassOf llm:EncoderOnly .

llm:DenoisingAutoencoding a owl:Class ;
    rdfs:label "DenoisingAutoencoding"@en ;
    llm:description "Denoising Autoencoding(디노이징 오토인코더)은 입력 데이터를 인위적으로 잡음(noise)이나 결함을 추가한 뒤, 손상된 신호를 원본과 동일하게 복원하도록 학습하는 **self‑supervised pretraining** 기법으로, 노이즈‑코리전(noise corruption)과 재구성 손실(reconstruction loss)을 최소화하는 **encoder‑decoder** 구조를 갖는다. 이 과정에서 모델은 **latent representation**(잠재 표현)이나 **feature embedding**을 학습하게 되며, 잡음에 강인한 **robust representation**을 얻어 **downstream task**(분류, 회귀, 생성 등)에서 **fine‑tuning** 시 성능 향상을 제공한다. 대표적인 활용 사례로는 이미지 복원·노이즈 제거, 텍스트 마스크 복원(예: BERT의 마스크드 언어 모델링), 음성 신호 정화, 그리고 비정형 데이터의 **dimensionality reduction** 및 **unsupervised feature learning**이 있다. 관련 개념으로는 순수한 **autoencoder**, **Variational Autoencoder(VAE)**, **Masked Autoencoder(MAE)**, 그리고 **contrastive learning** 기반의 **Contrastive Predictive Coding(CPC)** 등이 있으며, 디노이징 오토인코더는 잡음 복원을 목표로 하는 점에서 일반 오토인코더와 차별화된다." ;
    rdfs:subClassOf llm:Pretraining .

llm:DenseRetrieval a owl:Class ;
    rdfs:label "DenseRetrieval"@en ;
    llm:description "**DenseRetrieval**는 고차원 임베딩 벡터를 이용해 질문·문서 쌍을 **밀집(dense) 벡터 공간**에 매핑하고, 코사인 유사도·내적 등 **벡터 유사도 검색**을 통해 가장 관련성이 높은 문서를 빠르게 찾아내는 **신경망 기반 검색 기법**이다. 이 방식은 **Dual‑Encoder** 구조의 사전학습된 BERT/Transformer 모델이 질문과 문서를 각각 인코딩한 뒤, **근접 이웃 탐색(ANN)** 알고리즘으로 대규모 코퍼스에서 상위 k개의 후보를 추출하는 **인코더‑인코더 매칭** 원리를 따른다. DenseRetrieval는 **RAG(Retrieval‑Augmented Generation)** 파이프라인에서 외부 지식을 실시간으로 제공해 **질의응답, 오픈‑도메인 챗봇, 문서 요약, 지식 기반 생성** 등 다양한 **생성형 AI** 응용에 활용되며, 특히 **멀티모달 검색**, **지식 그래프 연결**, **법률·의료 문서 검색** 등 정교한 의미 이해가 요구되는 분야에서 강점을 보인다. 반면 **BM25·TF‑IDF**와 같은 **희소(sparse) 검색**은 키워드 매칭에 의존하는 반면, DenseRetrieval는 **시맨틱 매칭**을 제공해 어휘 변형·동의어 처리에 뛰어나며, **Hybrid Retrieval**에서는 두 방식을 결합해 정확도와 효율성을 동시에 최적화한다." ;
    rdfs:subClassOf llm:RAG .

llm:Distillation a owl:Class ;
    rdfs:label "Distillation"@en ;
    llm:description "Distillation(knowledge distillation)은 대규모 teacher 모델이 학습한 출력 확률 분포나 중간 특징을 활용해, 파라미터 수가 훨씬 적은 student 모델에게 동일한 지식을 전이함으로써 모델 압축을 실현하는 기법이다. 이 과정은 teacher‑student 아키텍처와 soft‑target 손실함수(예: Kullback‑Leibler divergence)를 이용해, 원본 모델의 복잡한 표현력을 손실 최소화하면서 압축된 네트워크에 재현하도록 동작한다. Distillation은 모바일·임베디드 디바이스, 실시간 영상 분석, 클라우드‑엣지 협업 등 inference 속도와 메모리 사용량이 제한된 환경에서 고성능 deep neural network를 구현하는 대표적인 사용 사례이며, 자연어 처리(NLP)와 컴퓨터 비전에서도 대형 transformer 모델을 경량화하는 데 널리 적용된다. 이와 달리 pruning, quantization, low‑rank factorization 등은 파라미터 자체를 직접 제거하거나 비트폭을 축소하는 방식으로, Distillation은 “지식 전이”라는 독특한 접근을 통해 압축 후에도 원 모델에 가까운 예측 정확도를 유지한다." ;
    rdfs:subClassOf llm:ModelCompression .

llm:ELECTRA a owl:Class ;
    rdfs:label "ELECTRA"@en ;
    llm:description "ELECTRA는 Encoder‑Only 구조의 Transformer 모델로, 전통적인 마스크드 언어 모델링(MLM) 대신 “replaced token detection”(RTD)이라는 사전학습 목표를 사용해 generator‑discriminator 방식으로 학습되는 효율적인 사전학습 모델이다. Generator가 입력 토큰 중 일부를 마스크하고 예측한 뒤, Discriminator는 원본 토큰과 generator가 만든 대체 토큰을 구분하도록 훈련되며, 이 과정에서 self‑supervised 학습이 이루어져 BERT 보다 적은 연산량으로도 높은 표현력을 얻는다. ELECTRA는 텍스트 분류, 질의응답, 감성 분석, 개체명 인식(NER) 등 다양한 downstream NLP 작업에 바로 fine‑tuning 할 수 있어 small‑model 시나리오와 resource‑constrained 환경에서 특히 많이 활용된다. 관련 개념으로는 MLM 기반 BERT/​RoBERTa, GAN‑style adversarial pretraining, Decoder‑Only GPT 시리즈가 있으며, ELECTRA는 mask‑predict‑then‑classify 패턴이 아닌 token‑replacement 검출을 통해 pretraining 효율성과 representation 품질을 동시에 개선한다." ;
    rdfs:subClassOf llm:EncoderOnly .

llm:EpisodicMemory a owl:Class ;
    rdfs:label "EpisodicMemory"@en ;
    llm:description "EpisodicMemory는 개인이 경험한 사건·시간적 맥락을 순차적인 **시퀀스**와 **시간 스탬프**와 함께 저장하는 **장기 기억** 메커니즘으로, 사건‑주체‑배경 삼중 구조를 **컨텍스트 임베딩**(contextual embedding) 형태로 인코딩해 벡터 공간에 매핑한다. 이 메모리는 **인코더‑디코더 아키텍처**나 **Transformer‑기반 기억 네트워크**가 입력 시퀀스를 **시계열 어텐션**(temporal attention)과 **메모리 셀**(memory cell)로 압축·통합하고, 필요 시 **키‑밸류 검색**(key‑value retrieval)으로 해당 에피소드를 재생성(recall)함으로써 **지식 통합(KnowledgeIntegration)** 파이프라인에 동적 컨텍스트를 제공한다. 대표적인 사용 사례로는 **대화형 AI**에서 사용자 발화 이력 기반 맞춤형 응답 생성, **로보틱스**에서 작업 수행 중 발생한 사건 로그를 활용한 **연속 학습(continual learning)** 및 **시뮬레이션 기반 의사결정**, 그리고 **의료·법률** 분야에서 환자·사건 기록을 기반으로 한 **시점‑특정 추론**(time‑specific reasoning) 등이 있다. 관련 개념으로는 **시맨틱 메모리(SemanticMemory)**가 의미‑레벨의 일반화된 지식을 저장하는 반면, **작업 메모리(WorkingMemory)**는 단기·즉시 처리에 초점을 두는 점에서 차별화되며, 에피소드 메모리는 **지식 그래프(Knowledge Graph)**와 결합해 **시간‑가중 연결**(temporal weighted edges)으로 풍부한 **지식 통합**을 가능하게 한다." ;
    rdfs:subClassOf llm:KnowledgeIntegration .

llm:ExactMatch a owl:Class ;
    rdfs:label "ExactMatch"@en ;
    llm:description "ExactMatch는 자동 평가 (AutomaticMetric) 중 하나로, 예측된 텍스트가 정답 레퍼런스와 **문자열이나 토큰 수준에서 완전히 동일한지**를 0 또는 1 (또는 전체 데이터셋에 대한 비율)로 측정하는 **정확도 기반 메트릭**이다. 이 메트릭은 대소문자 통일, 공백·구두점 제거 등 사전 정규화 과정을 거친 뒤 **엄격한 레터‑레터 매칭**을 수행하며, 일치하면 1, 일치하지 않으면 0을 반환한다. 주로 질문‑응답, 코드 생성, 슬롯‑필링, 기계 번역·요약 등 **정답이 하나로 고정된** 작업에서 정답률(Exact Match Score)을 평가하거나 모델 튜닝·베이스라인 설정에 활용된다. BLEU·ROUGE·METEOR와 같은 **의미 기반 혹은 부분 일치** 자동 메트릭과 대비되어, ExactMatch는 **표면 형태(lexical) 일치**에 초점을 맞추는 반면 다른 메트릭은 **semantic similarity**나 **partial overlap**을 포착한다." ;
    rdfs:subClassOf llm:AutomaticMetric .

llm:ExampleSelection a owl:Class ;
    rdfs:label "ExampleSelection"@en ;
    llm:description "**ExampleSelection**은 프롬프트 엔지니어링(Prompt Engineering) 단계에서 대형 언어 모델(LLM)에 제공할 **예시(샘플)들을 선택·배치**하는 작업을 의미하며, 인‑컨텍스트 학습(in‑context learning)이나 few‑shot learning을 구현하기 위한 핵심 설계 요소입니다. 이 과정은 **유사도 기반 검색**, **클러스터링** 또는 **다양성 유지**와 같은 샘플링 전략을 활용해 **대표성**과 **다양성**을 동시에 만족하는 예시 집합을 구성하고, 예시 순서와 길이를 조절해 컨텍스트 윈도우 내 최적의 정보 밀도를 확보합니다. 대표적인 사용 사례로는 **few‑shot 분류**, **코드 생성**, **추론·Chain‑of‑Thought** 프롬프트 설계, 그리고 **지식 기반 질의응답** 등에서 예시 선택이 모델 성능을 크게 향상시키는 것이 입증되었습니다. 관련 개념으로는 **PromptTemplate**, **Prompt Tuning**, **Instruction Tuning**, **Zero‑shot** 접근법 등이 있으며, ExampleSelection은 특히 **예시 최적화**와 **시맨틱 검색**을 강조함으로써 이러한 기법들과 차별화됩니다." ;
    rdfs:subClassOf llm:PromptEngineering .

llm:Executor a owl:Class ;
    rdfs:label "Executor"@en ;
    llm:description """**Executor**는 **AgentComponent** 계층에 속하는 핵심 모듈로, 플래너(Planner)가 생성한 계획이나 행동 명령을 실제 작업으로 전환해 **task execution**을 담당하는 컴포넌트이며, 외부 API, 데이터베이스, 코드 스니펫, 로봇 제어 등 다양한 **Tool**과 **Toolchain**에 인터페이스한다.  
이 컴포넌트는 **동기·비동기 실행**, **병렬 처리(parallelism)**, **리소스 할당**, **컨텍스트 전파(context propagation)**, **오류 처리(error handling)**, **재시도(retry)**·**폴백(fallback)** 메커니즘, 그리고 **모니터링·로깅·관측성(observability)**을 제공하는 **플러그인식 실행 엔진**으로 설계되어, 복합 워크플로우와 실시간 **오케스트레이션(orchestration)**에 최적화된다.  
실제 사용 사례로는 LLM 기반 에이전트가 **코드 실행**, **웹 요청 트리거**, **데이터 파이프라인 조정**, **로보틱스 제어 루프** 등을 수행하거나, 다단계 비즈니스 프로세스 자동화와 같은 **멀티스텝 워크플로우**를 구현할 때 활용된다.  
관련 개념으로는 **Planner**(무엇을 할지 결정)와 대비되는 **Executor**(어떻게 할지 실행)이며, **Memory**, **Policy**, **Orchestrator** 등과 함께 **AgentComponent** 구조 내에서 **커맨드 패턴(command pattern)**·**전략 패턴(strategy pattern)**을 따르는 모듈형 아키텍처를 구성한다.""" ;
    rdfs:subClassOf llm:AgentComponent .

llm:ExternalMemory a owl:Class ;
    rdfs:label "ExternalMemory"@en ;
    llm:description "ExternalMemory는 인공지능 모델이 자체 파라미터 외에 별도 저장소에 저장된 지식·데이터를 동적으로 읽고 쓰는 메커니즘으로, 모델이 제한된 컨텍스트 윈도우를 넘어 장기적인 정보를 유지·통합할 수 있게 해줍니다. 이 메모리는 키‑값 인덱싱, 벡터 검색, 혹은 차별화된 Retrieval‑Augmented Generation(RAG) 파이프라인을 통해 질의에 맞는 관련 문서를 빠르게 검색하고, 어텐션 기반의 읽기·쓰기 연산으로 현재 추론에 반영합니다. 대표적인 사용 사례로는 대규모 언어 모델이 최신 뉴스, 기업 내부 문서, 혹은 도메인‑특화 지식 그래프와 같은 외부 데이터베이스를 실시간으로 참조해 질문에 답하거나, 연속적인 대화·작업 흐름에서 기억을 지속적으로 업데이트하는 챗봇·에이전트가 있습니다. 관련 개념으로는 모델 내부의 파라미터 기반 메모리(InternalMemory)와 대비되며, 메타러닝 기반의 기억 강화(Memory‑Augmented Neural Networks)나 프롬프트 엔지니어링(Prompt Engineering)과 결합해 지식 통합(KnowledgeIntegration) 효율을 극대화합니다." ;
    rdfs:subClassOf llm:KnowledgeIntegration .

llm:F1 a owl:Class ;
    rdfs:label "F1"@en ;
    llm:description "F1 스코어는 **정밀도(Precision)와 재현율(Recall)의 조화 평균**으로 정의되는 자동 평가 지표(AutomaticMetric)이며, 특히 **불균형 데이터셋**이나 **다중 클래스 분류**에서 모델의 전반적인 성능을 균형 있게 측정하는 데 유용합니다. 이 지표는 **혼동 행렬(confusion matrix)**을 기반으로 정밀도와 재현율을 각각 계산한 뒤, 2·(Precision·Recall)/(Precision+Recall) 형태의 수식으로 **조화 평균**을 구해 0~1 사이의 값을 반환합니다. 대표적인 사용 사례로는 **텍스트 분류, 감성 분석, 정보 검색, 의료 진단** 등에서 모델의 **정밀도와 재현율 간 트레이드오프**를 동시에 고려해야 할 때 활용되며, **macro‑F1, micro‑F1, weighted‑F1**과 같은 변형을 통해 클래스별 중요도를 조정할 수 있습니다. F1은 **정확도(Accuracy)**와 달리 클래스 불균형에 민감하지 않으며, **ROC‑AUC**와 같은 다른 자동 메트릭과는 **정밀도·재현율 기반**이라는 점에서 차별화됩니다." ;
    rdfs:subClassOf llm:AutomaticMetric .

llm:Falcon a owl:Class ;
    rdfs:label "Falcon"@en ;
    llm:description "Falcon은 Technology Innovation Institute에서 개발한 **Decoder‑Only 대형 언어 모델(LLM)** 로, 7 B ~ 180 B 파라미터 규모의 **자기회귀(autoregressive) 트랜스포머** 구조를 기반으로 텍스트를 순차적으로 생성합니다. 입력 토큰을 **단일 디코더 스택**에 그대로 전달해 다음 토큰을 예측하도록 학습되며, **RoPE(회전 위치 인코딩)**, **Sparse‑Attention** 및 **Mixture‑of‑Experts**와 같은 최신 최적화 기법을 활용해 효율적인 **인퍼런스**와 **파인튜닝**을 지원합니다. 이 모델은 **텍스트 생성, 요약, 코드 보조, 질문‑응답, 그리고 프롬프트 엔지니어링 기반의 제로‑샷/Few‑Shot 학습** 등 다양한 **자연어 처리(NLP) 및 생성 AI** 작업에 활용되며, **벡터 검색**을 위한 고차원 **임베딩** 생성에도 적합해 **시맨틱 검색**, **지식 베이스 연동**, **추천 시스템** 등에 널리 적용됩니다. Falcon은 GPT‑계열이나 LLaMA와 같은 다른 Decoder‑Only 모델과 유사하지만, **Encoder‑Decoder 구조**(예: T5, BART)와는 달리 입력‑출력 쌍을 별도 인코딩하지 않아 **단일 파이프라인**에서 빠른 응답성을 제공한다는 점이 차별화됩니다." ;
    rdfs:subClassOf llm:DecoderOnly .

llm:FewShot a owl:Class ;
    rdfs:label "FewShot"@en ;
    llm:description "**Few‑Shot**는 대규모 언어 모델에 **소수의 예시(샘플)와 함께 프롬프트**를 제공해 원하는 작업을 수행하도록 유도하는 **프롬프트 엔지니어링 기법**이다. 사용자는 **2~10개의 데모 입력‑출력 쌍**을 프롬프트에 삽입하고, 모델은 이 **인‑컨텍스트 학습(in‑context learning)**을 통해 새로운 입력에 대해 유사한 패턴을 추론해 답을 생성한다. 대표적인 활용 사례로는 **텍스트 분류, 질의응답, 코드 생성, 번역** 등 **데이터가 제한된 도메인**에서 빠르게 프로토타입을 만들 때, 혹은 **Zero‑Shot** 대비 **성능 향상**이 필요한 상황에서 많이 쓰인다. 관련 개념으로는 **Zero‑Shot**, **One‑Shot**, **Chain‑of‑Thought** 프롬프팅, **Prompt‑Tuning**·**Fine‑Tuning** 등이 있으며, Few‑Shot은 **데이터 효율성**과 **즉시 적용 가능성**을 강조하는 점에서 차별화된다." ;
    rdfs:subClassOf llm:BasicPrompting .

llm:FlanT5 a owl:Class ;
    rdfs:label "FlanT5"@en ;
    llm:description """FlanT5는 구글이 발표한 **Instruction‑Fine‑Tuned T5(텍스트‑투‑텍스트 트랜스포머)** 모델로, 기본 T5의 **Encoder‑Decoder** 아키텍처에 대규모 명령어(Instruction) 데이터셋을 추가 학습시켜 **다중 작업(멀티태스크) 수행**과 **제로‑샷/Few‑Shot** 능력을 크게 향상시킨 변형이다.  
이 모델은 입력 텍스트를 **Encoder**가 의미론적 표현으로 압축하고, **Decoder**가 해당 표현을 기반으로 자연어 응답, 요약, 번역, 질의응답 등 다양한 **Seq2Seq** 출력을 생성하며, **Prompt‑Based** 방식과 **Adapter‑Based** 파라미터 효율적 튜닝을 모두 지원한다.  
FlanT5는 **문서 요약, 질문‑답변, 대화 생성, 코드 설명** 등 NLP 전반에 활용될 뿐만 아니라, **텍스트 임베딩**을 추출해 **벡터 검색(semantic retrieval)** 시스템의 인덱싱 및 **유사도 매칭**에 사용되어 검색 정확도를 높이는 데도 적용된다.  
관련된 개념으로는 **Encoder‑Only BERT**, **Decoder‑Only GPT**, 그리고 **BART**와 같은 **Cross‑Encoder/Seq2Seq** 모델이 있으며, FlanT5는 특히 **Instruction‑Tuning**과 **멀티태스크 일반화** 측면에서 이들 모델과 차별화된다.""" ;
    rdfs:subClassOf llm:EncoderDecoder .

llm:FlashAttention a owl:Class ;
    rdfs:label "FlashAttention"@en ;
    llm:description """**FlashAttention**는 트랜스포머 모델에서 핵심 역할을 하는 **Self‑Attention** 연산을 GPU 메모리 대역폭과 연산 효율을 극대화하도록 설계된 **고성능 커널**로, 기존의 O(N²) 복잡도를 유지하면서도 **메모리 사용량을 2배 이상 절감**하고 **연산 속도를 3~5배 가속**하는 최신 **AttentionMechanism** 구현이다.  
이 기법은 **Softmax**와 **점곱 연산**을 한 번의 **Kernel Fusion**으로 결합하고, **tiling**·**blocked matrix multiplication**·**float16/ bfloat16** 정밀도와 **CUDA‑aware** 스케줄링을 활용해 **GPU 레지스터와 공유 메모리**에 데이터를 효율적으로 배치함으로써 **메모리‑바운드** 병목을 제거한다.  
주요 사용 사례는 **대규모 언어 모델**(예: GPT‑3, BERT, LLaMA) 학습·추론 가속, **멀티모달 트랜스포머**(Vision‑Transformer, CLIP) 및 **실시간 생성**(Chatbot, 코드 자동완성) 등 **고차원 벡터 검색**·**시퀀스‑투‑시퀀스** 작업에서의 **throughput**·**latency** 개선이다.  
관련 개념으로는 전통적인 **Scaled Dot‑Product Attention**, **Sparse Attention**(Longformer, BigBird), **Linearized Attention**(Linformer, Performer) 등이 있으며, FlashAttention은 **메모리 효율성**과 **연산 속도** 측면에서 이들 방식보다 **전반적인 성능 우위**를 제공한다.""" ;
    rdfs:subClassOf llm:AttentionMechanism .

llm:FunctionCalling a owl:Class ;
    rdfs:label "FunctionCalling"@en ;
    llm:description "**FunctionCalling**은 대형 언어 모델(LLM)이 사용자 프롬프트를 해석한 뒤, 사전에 정의된 함수나 API를 자동으로 호출해 결과를 반환하도록 하는 **함수 호출 메커니즘**을 의미합니다. 이 기능은 모델이 생성한 **JSON 스키마 기반 파라미터**를 검증하고, **동적 파라미터 매핑**, **시그니처 매칭**, **실행 환경 격리** 등을 통해 외부 서비스(예: 데이터베이스 조회, 웹 API, 계산 엔진)와 **실시간 연동**하는 방식으로 동작합니다. 대표적인 사용 사례로는 **챗봇에서 실시간 날씨 정보 제공**, **코드 자동 완성 및 실행**, **비즈니스 워크플로우 자동화** 등이며, **플러그인 기반 에이전트**, **ToolUse 프레임워크**, **ToolInvocation**과 같은 개념과 함께 **프롬프트 엔지니어링**에 활용됩니다. 반면, 일반적인 **ToolUse**는 도구 자체를 직접 제어하거나 UI를 조작하는 방식을 의미하지만, **FunctionCalling**은 구조화된 함수 인터페이스를 통해 **정형화된 입력·출력**을 교환함으로써 보다 **예측 가능하고 안전한** 상호작용을 제공한다는 점에서 차별화됩니다." ;
    rdfs:subClassOf llm:ToolUse .

llm:GELU a owl:Class ;
    rdfs:label "GELU"@en ;
    llm:description """GELU(Gaussian Error Linear Unit)는 입력값을 정규분포 𝒩(0, 1)의 누적분포함수 Φ와 결합해 \\( \\text{GELU}(x)=x·Φ(x) \\) 또는 \\(x·\\sigma(x)\\) (σ는 시그모이드)와 같은 근사식으로 계산되는 **비선형 활성화 함수**로, 입력이 양수일 확률에 비례해 값을 스케일링하는 “확률적” 특성을 갖습니다.  
이 함수는 **부드럽고 미분 가능**하며, 입력이 0을 중심으로 대칭적인 zero‑centered 특성을 제공해 gradient 전파 시 vanishing/exploding 문제를 완화하고, ReLU와 달리 “죽은 뉴런” 현상이 거의 발생하지 않습니다.  
GELU는 **Transformer 기반 모델(BERT, GPT, Vision‑Transformer 등)**, 대규모 언어 모델, 이미지 분류 및 생성 네트워크 등 **딥러닝의 최첨단 아키텍처**에서 표준 활성화 함수로 채택되어 학습 안정성과 성능 향상을 입증했으며, 특히 **self‑normalizing** 효과가 요구되는 깊은 네트워크에 유리합니다.  
관련 개념으로는 **ReLU, Leaky ReLU, ELU, Swish** 등이 있으며, 이들와 비교했을 때 GELU는 **연속적인 2차 미분**을 제공해 보다 부드러운 학습 곡선을 만들고, 확률적 스케일링을 통해 **비선형성**을 더 자연스럽게 모델링한다는 차별점을 가집니다.""" ;
    rdfs:subClassOf llm:ActivationFunction .

llm:GPT3 a owl:Class ;
    rdfs:label "GPT3"@en ;
    llm:description "GPT‑3는 OpenAI가 2020년에 발표한 대규모 사전학습 언어 모델로, 175 억 개의 파라미터를 갖는 Transformer 기반 아키텍처이며, 텍스트 임베딩과 벡터 표현을 활용해 자연어 이해·생성 작업을 수행합니다. 이 모델은 대규모 코퍼스로 사전학습된 뒤, 프롬프트에 포함된 몇 개의 예시(​few‑shot​) 혹은 전혀 없는 상황(zero‑shot)에서도 높은 정확도로 문장 완성, 요약, 번역, 코드 생성 등 다양한 NLP 태스크를 수행하도록 설계되었습니다. 주요 사용 사례로는 챗봇 API, 자동 콘텐츠 생성, 검색 질의 확장 및 벡터 검색 시스템에서의 의미 기반 매칭, 그리고 프로그래밍 보조 도구 등 광범위한 산업·연구 분야에 적용됩니다. GPT‑3는 이전 버전인 GPT‑2와 비교해 파라미터 규모와 학습 데이터량이 크게 확대되었으며, 이후 등장한 GPT‑4와는 멀티모달 입력 지원·추론 효율성 등에서 차별화되는 특징을 보입니다." ;
    rdfs:subClassOf llm:GPTArchitecture .

llm:GPT4 a owl:Class ;
    rdfs:label "GPT4"@en ;
    llm:description "GPT‑4는 OpenAI가 발표한 차세대 **대규모 언어 모델**(LLM)로, **Transformer 기반 GPT 아키텍처**를 확장한 **멀티모달** 버전이며, 텍스트와 이미지 두 종류의 입력을 동시에 처리할 수 있는 **멀티모달 프리트레인** 모델이다.  이 모델은 수백 억 ~ 수 조 개의 파라미터 규모와 **32 k 토큰**에 달하는 **컨텍스트 윈도우**를 갖추고, **RLHF(Reinforcement Learning from Human Feedback)**와 **few‑shot/zero‑shot 학습** 메커니즘을 결합해 높은 **텍스트 생성**, **코드 작성**, **요약·번역·질의응답** 능력을 제공한다.  GPT‑4는 **ChatGPT** 서비스, 기업용 **API**, 교육용 튜터, 의료·법률 문서 자동화, 게임 스토리텔링 등 **대화형 AI**와 **자연어 처리** 전반에 걸친 **콘텐츠 생성·분석** 작업에 널리 활용된다.  동일한 **GPT Architecture** 계열의 **GPT‑3.5**와 비교해 더 큰 파라미터 수와 멀티모달 입력 지원을 제공하며, **BERT**·**T5**와 같은 **인코더‑전용** 모델과는 달리 **생성‑중심**(generative) 구조와 **자연어 이해·생성**을 동시에 수행한다." ;
    rdfs:subClassOf llm:GPTArchitecture .

llm:GPTQ a owl:Class ;
    rdfs:label "GPTQ"@en ;
    llm:description "**GPTQ**(Generative Pre‑trained Transformer Quantization)는 사전 학습된 대규모 언어 모델(LLM)을 **후처리 양자화(post‑training quantization)** 방식으로 4‑bit 혹은 8‑bit 정밀도로 압축하는 기술로, 원본 FP32/FP16 가중치를 직접 재학습 없이 최소한의 양자화 오류만 남기고 **그라디언트 기반 근사**를 통해 최적의 스케일과 제로 포인트를 찾아낸다. 이 방법은 **비트폭 감소**에 따른 메모리 사용량과 연산량을 크게 줄여 GPU/CPU 인퍼런스 시 **속도 향상**과 **전력 효율**을 제공하면서도 텍스트 생성, 코드 완성, 검색‑엔진 등 **LLM 기반 애플리케이션**에서 원본 모델에 근접한 정확도를 유지한다. GPTQ는 **PTQ(Pre‑training Quantization)**와 달리 **그라디언트 추정**을 활용해 가중치별 최적화된 양자화 매핑을 수행하므로, **QAT(Quantization‑Aware Training)**처럼 재학습 비용을 들지 않으면서도 **int4/int8** 수준의 고효율 압축을 가능하게 한다. 관련 개념으로는 **가중치 프루닝(pruning)**, **지식 증류(distillation)**, **LoRA(Low‑Rank Adaptation)** 등이 있으며, 이들은 모두 모델 경량화와 배포 용이성을 목표로 하지만 **양자화 오류 관리 방식**과 **하드웨어 최적화 대상**에서 차이를 보인다." ;
    rdfs:subClassOf llm:Quantization .

llm:Gemini a owl:Class ;
    rdfs:label "Gemini"@en ;
    llm:description "Gemini는 Google DeepMind가 발표한 차세대 대규모 언어 모델(LLM) 시리즈 중 **Decoder‑Only** 아키텍처를 기반으로 하는 모델군으로, 순차적인 토큰 예측을 위해 다중 레이어의 Transformer 디코더 스택만을 사용한다는 점이 핵심 정의이다. 이 모델은 초대규모 텍스트·코드·멀티모달 데이터로 사전 학습된 뒤, **인스트럭션 튜닝**·**RLHF**(Reinforcement Learning from Human Feedback)·**RAG**(Retrieval‑Augmented Generation)와 같은 기법을 결합해 문맥 이해와 응답 생성 능력을 극대화하며, 임베딩 벡터를 활용한 **semantic search**와 **dense retrieval**에 최적화된 구조를 제공한다. 주요 활용 사례로는 실시간 대화형 에이전트, 코드 자동 완성·디버깅, 복합 질의에 대한 **vector‑based 검색** 및 요약, 그리고 이미지·텍스트를 동시에 처리하는 멀티모달 어시스턴트 등이 있다. 관련된 개념으로는 동일한 Decoder‑Only 설계의 GPT‑4, LLaMA, PaLM 2가 있으며, 반대로 **Encoder‑Decoder** 구조를 채택한 T5·Flan‑UL2와는 입력‑출력 양쪽에 별도 인코더를 두는 점에서 차별화된다." ;
    rdfs:subClassOf llm:DecoderOnly .

llm:GraphMemory a owl:Class ;
    rdfs:label "GraphMemory"@en ;
    llm:description "**GraphMemory**는 개념·관계 노드와 엣지로 이루어진 그래프 형태의 메모리 구조에, 각 노드·엣지를 고차원 **벡터 임베딩**과 결합해 저장·검색하는 **KnowledgeIntegration**(지식 통합) 기술이다. 이 메모리는 **그래프 신경망(GNN)** 혹은 하이브리드 검색 엔진을 이용해 **다중 홉 추론**·컨텍스트 전파를 수행하고, 새로운 사실이 추가될 때도 **인메모리 업데이트**가 가능하도록 설계된다. 대표적인 활용 사례로는 **LLM** 증강을 위한 **RAG**(Retrieval‑Augmented Generation) 파이프라인, **시맨틱 검색**·멀티모달 질문‑응답, 추천 시스템 및 자율 에이전트의 **지식 기반 의사결정** 등이 있다. 관련 개념으로는 전통적인 **벡터 검색 엔진**이나 **관계형 데이터베이스**와 달리 **지식 그래프**·**메모리 네트워크**·**신경‑기호 통합**(neural‑symbolic integration)과 연계되며, 이러한 차별점이 **연결성**·**추론 능력**을 크게 향상시킨다." ;
    rdfs:subClassOf llm:KnowledgeIntegration .

llm:GreedyDecoding a owl:Class ;
    rdfs:label "GreedyDecoding"@en ;
    llm:description "GreedyDecoding(탐욕적 디코딩)은 언어 모델이 시퀀스를 생성할 때, 각 단계에서 **가장 높은 확률을 가진 토큰을 즉시 선택**하는 가장 단순한 DecodingStrategy(디코딩 전략)이다. 이 방식은 **확률 분포의 argmax** 연산을 반복 적용해 순차적으로 토큰을 출력하므로 구현이 간단하고 **실시간 응답이 요구되는 대화형 AI, 기계 번역, 텍스트 요약** 등에서 빠른 추론 속도를 제공한다. 그러나 탐욕적 선택은 **전역 최적 해를 보장하지 못하고** 종종 **Beam Search**, **Top‑k Sampling**, **Top‑p (nucleus) Sampling**과 같은 보다 탐색적인 디코딩 기법에 비해 다양성이나 품질이 떨어질 수 있다. 따라서 **Greedy Decoding**은 **속도 우선**인 상황이나 **단일 최적 토큰**이 충분히 의미 있는 경우에 적합하며, **다양성 확보**가 필요한 작업에서는 **다른 디코딩 전략**과 비교·조합하여 사용한다." ;
    rdfs:subClassOf llm:DecodingStrategy .

llm:GroupedQueryAttention a owl:Class ;
    rdfs:label "GroupedQueryAttention"@en ;
    llm:description "**GroupedQueryAttention**는 AttentionMechanism 의 한 변형으로, 입력 토큰들의 query 벡터를 의미론적으로 유사한 그룹 으로 사전 클러스터링한 뒤, 각 그룹별로 공유된 key‑value 쌍에 대해 집중(attention) 연산을 수행하는 방식이다. 이 메커니즘은 query 를 group 단위로 묶어 quadratic 복잡도를 O(N·G) (여기서 G 는 그룹 수) 로 낮추고, shared attention weights 와 block‑sparse 연산을 활용해 메모리 사용량과 연산량을 크게 절감한다. 주로 긴 시퀀스 처리가 요구되는 Transformer 모델(예: Longformer, BigBird), 대규모 언어 모델 및 비전 트랜스포머, 검색‑보강 생성(RAG) 시스템에서 벡터 검색·유사도 계산을 효율화하기 위해 사용되며, 임베딩 공간에서 nearest‑neighbor 검색을 가속화한다. 기존의 Multi‑Head Self‑Attention 이나 Sparse Attention 과는 달리 query 그룹화를 핵심 설계 원칙으로 삼아 low‑rank 근사·Mixture‑of‑Experts 와는 대비되지만, grouped convolution 이나 token clustering 과 같은 개념과는 연관성을 가진다." ;
    rdfs:subClassOf llm:AttentionMechanism .

llm:HumanEvaluation a owl:Class ;
    rdfs:label "HumanEvaluation"@en ;
    llm:description "**HumanEvaluation**는 **EvaluationMetric** 의 하위 개념으로, 모델이 생성한 텍스트·이미지·음성 등의 결과물을 인간 평가자가 직접 읽고·보고·느끼는 주관적 품질을 측정하는 방법을 말합니다. 이 평가는 스케일링된 설문·리커트 점수·쌍대 비교·전문가 라벨링·크라우드소싱 등의 실험 설계와 신뢰도·재현성을 확보하기 위한 통계적 분석(예: 인터‑평가자 일치도, bootstrap confidence interval) 을 결합해 “정성적” 및 “정량적” 지표(예: human rating, human preference, human judgment score)를 도출합니다. 대표적인 사용 사례로는 대화 시스템·기계 번역·텍스트 요약·이미지 생성·음성 합성 등에서 BLEU·ROUGE·Accuracy와 같은 자동 평가 지표와 대비해 실제 사용자 만족도·자연스러움·일관성을 검증하는 human‑in‑the‑loop 평가가 있으며, 제품 출시 전 A/B 테스트·사용자 피드백·전문가 리뷰 등에 널리 활용됩니다. 관련 개념으로는 자동 평가 지표(예: BLEU, ROUGE, METEOR)와 정량적 메트릭(정확도·정밀도·리콜) 이 있으며, HumanEvaluation은 주관성·비용·시간 이라는 제약을 갖지만 자동 평가가 놓치기 쉬운 의미적 일관성·문화적 뉘앙스·창의성 등을 포착한다는 점에서 차별화됩니다." ;
    rdfs:subClassOf llm:EvaluationMetric .

llm:HybridRetrieval a owl:Class ;
    rdfs:label "HybridRetrieval"@en ;
    llm:description "HybridRetrieval은 **Dense Retrieval(벡터 검색)**과 **Sparse Retrieval(키워드 기반 검색, BM25 등)**를 동시에 활용해 문서 후보를 선별하고, 이를 **RAG(Retrieval‑Augmented Generation)** 파이프라인에 전달하는 복합 검색 기법을 말합니다. dense encoder가 생성한 임베딩 벡터와 inverted index가 제공하는 토큰‑레벨 매칭 점수를 **스코어 융합, 라우팅 또는 재랭킹** 방식으로 결합함으로써, 의미적 유사성은 유지하면서도 정확한 용어 매칭을 보강해 검색 정확도와 재현율을 동시에 끌어올립니다. 대표적인 사용 사례로는 **대규모 지식베이스 질의응답, 기업 문서 검색, 멀티모달 RAG 시스템** 등이 있으며, 특히 최신 LLM이 외부 정보를 실시간으로 참조해야 하는 상황에서 **컨텍스트 풍부화**와 **정확한 근거 제공**에 효과적입니다. 관련 개념으로는 순수 벡터 검색인 **FAISS/ScaNN**, 전통적인 **BM25**, 그리고 **Hybrid Fusion (late‑fusion, early‑fusion)** 방식이 있으며, HybridRetrieval은 이들 방식을 **상호 보완**하도록 설계된 점에서 차별화됩니다." ;
    rdfs:subClassOf llm:RAG .

llm:ImageCaptioning a owl:Class ;
    rdfs:label "ImageCaptioning"@en ;
    llm:description "**ImageCaptioning**은 이미지의 시각적 정보를 분석해 자연어 문장을 자동으로 생성하는 **멀티모달 태스크(MultimodalTask)** 로, “이미지 캡션 생성”, “시각‑언어 모델 기반 텍스트 생성” 등으로도 불립니다. 이 작업은 일반적으로 **CNN·Vision‑Transformer와 같은 시각 인코더**가 이미지 특징을 추출하고, **Transformer 기반 디코더**가 추출된 특징에 **Attention 메커니즘**을 적용해 순차적인 단어를 예측하는 **Encoder‑Decoder 구조**로 구현되며, COCO·Flickr30k와 같은 **이미지‑텍스트 쌍 데이터셋**을 이용해 **시각‑언어 공동 학습**을 수행합니다. 주요 활용 사례로는 **시각 장애인을 위한 자동 대체 텍스트(alt‑text) 생성**, **전자상거래 제품 설명 자동화**, **소셜 미디어 콘텐츠 인덱싱·검색**, 그리고 **영상 요약·스토리보드 생성** 등이 있으며, 성능 평가는 **BLEU·ROUGE·CIDEr**와 같은 자연어 생성 지표로 측정됩니다. 관련 개념으로는 **이미지 분류·객체 검출(단일 라벨·바운딩 박스)**과 대비되며, **시각 질문 응답(VQA)**, **이미지‑텍스트 검색(멀티모달 검색)**, **비디오 캡셔닝** 등과 **공통된 시각‑언어 이해** 기술을 공유합니다." ;
    rdfs:subClassOf llm:MultimodalTask .

llm:ImitationLearning a owl:Class ;
    rdfs:label "ImitationLearning"@en ;
    llm:description "Imitation Learning(모방 학습)은 전문가(인간 혹은 기존 시스템)의 시연 데이터를 관찰·복제함으로써 에이전트가 목표 행동을 학습하는 **Training Paradigm** 중 하나이며, 흔히 행동 클론(behavior cloning)이나 역강화학습(inverse reinforcement learning) 형태로 구현됩니다. 이 방법은 데모(데이터)와 상태‑행동 쌍을 이용해 정책(policy)을 직접 추정하거나, 보상 함수를 역추정해 강화학습(RL)과 결합해 데이터 효율성을 높이는 것이 핵심 작동 원리입니다. 대표적인 사용 사례로는 로봇 팔의 조작, 자율 주행 차량의 차선 유지·차량 제어, 드론 비행 경로 학습, 게임 AI의 인간 플레이어 행동 복제 등이 있으며, 시뮬레이션·실제 환경 모두에서 널리 적용됩니다. 모방 학습은 전통적인 지도학습(supervised learning)과는 달리 **행동 복제**에 초점을 맞추고, 순수 강화학습과는 보상 설계 없이 전문가 시연만으로 학습한다는 점에서 대비됩니다." ;
    rdfs:subClassOf llm:TrainingParadigm .

llm:InstructionOptimization a owl:Class ;
    rdfs:label "InstructionOptimization"@en ;
    llm:description "**InstructionOptimization**은 대형 언어 모델(LLM)에 전달되는 지시문(Instruction)을 **최적화**하여 원하는 작업 성능을 극대화하는 프롬프트 엔지니어링 기법을 말합니다. 핵심 특징으로는 **간결하고 명확한 문구 설계**, **계층형 지시 구조**(시스템 프롬프트 → 사용자 프롬프트 → 예시), **토큰 효율성**과 **컨텍스트 윈도우 관리**, 그리고 **반복적인 A/B 테스트·피드백 루프**를 통한 지시문 튜닝이 포함됩니다. 이 방법은 **챗봇 대화 흐름 제어**, **코드 자동 생성**, **문서 요약·추출**, **도메인 특화 어시스턴트** 등에서 정확한 명령 수행이 필수적인 경우에 널리 활용됩니다. 관련 개념으로는 **프롬프트 튜닝**(파라미터 기반 최적화)과 **프롬프트 체인/템플릿**(다단계 프롬프트 설계)이 있으며, InstructionOptimization은 **프롬프트 설계**와 **프롬프트 엔지니어링** 전반에 걸친 **지시문 개선**(instruction refinement) 전략으로 구분됩니다." ;
    rdfs:subClassOf llm:PromptEngineering .

llm:InstructionTuning a owl:Class ;
    rdfs:label "InstructionTuning"@en ;
    llm:description "**Instruction Tuning**은 사전 학습된 대규모 언어 모델(Large Language Model, LLM)을 **명령‑응답 형태의 데이터셋**으로 추가 학습시켜, 사용자가 자연어로 제시하는 다양한 지시(prompt) 에 대해 일관되고 정확한 출력을 생성하도록 **미세조정(Fine‑tuning)**하는 기술이다. 이 과정은 **지도 학습(supervised learning)** 기반으로, “이 질문에 어떻게 답하라”는 구체적인 지시와 그에 대응하는 정답을 쌍으로 제공함으로써 모델이 **명령 이해와 실행 능력**을 내재화하도록 만든다; 결과적으로 **제로샷(zero‑shot)·Few‑shot** 상황에서도 높은 **태스크 일반화(generalization)**와 **사용자 의도 정렬(alignment)**을 달성한다. 대표적인 활용 사례로는 **대화형 AI 비서, 코드 생성 도구, 고객 지원 챗봇, 교육용 튜터** 등에서 사용자가 자유롭게 명령을 입력하면 즉시 적절한 답변이나 작업 결과를 반환하도록 하는 것이며, **RLHF(Reinforcement Learning from Human Feedback)**와 결합해 더욱 정교한 행동 정책을 학습하기도 한다. Instruction Tuning은 **프롬프트 엔지니어링(prompt engineering)**이나 **전통적인 사전학습(pre‑training)**과는 달리, **특정 도메인에 맞춘 파인튜닝**이면서도 **다양한 작업에 대한 범용성을 유지**하는 점에서 **도메인 적응(domain adaptation)** 및 **전이 학습(transfer learning)**과 대비되는 고유한 접근 방식이다." ;
    rdfs:subClassOf llm:Finetuning .

llm:Int4Quantization a owl:Class ;
    rdfs:label "Int4Quantization"@en ;
    llm:description "Int4 Quantization은 딥러닝 모델의 가중치와 활성화를 4비트 정수(Int4) 형태로 변환하는 양자화(Quantization) 기법으로, FP32 혹은 FP16와 같은 고정밀 부동소수점 표현을 대체해 메모리 사용량을 75 % 이상 절감한다. 이 방식은 스케일링·오프셋 매핑을 통해 연속적인 실수 값을 16개의 이산값으로 압축하고, 양자화 오류를 최소화하기 위해 레이어별 동적 스케일링(dynamic scaling)과 정밀도 보정(calibration) 과정을 적용한다. 대표적인 사용 사례로는 모바일·엣지 디바이스에서의 실시간 추론, 대규모 언어 모델(LLM)의 온‑디바이스 배포, 그리고 GPU·TPU와 같은 하드웨어 가속기에서의 연산 속도 향상이 있다. Int4 Quantization은 Int8 Quantization이나 FP16 Quantization과 대비해 저장·전송 효율은 뛰어나지만, 양자화 노이즈가 더 크게 발생해 모델 정확도 유지가 어려운 점에서 차별화된다." ;
    rdfs:subClassOf llm:Quantization .

llm:Int8Quantization a owl:Class ;
    rdfs:label "Int8Quantization"@en ;
    llm:description "Int8Quantization은 32‑bit 부동소수점(FP32) 형태의 가중치와 활성화를 8‑bit 정수(Int8) 형태로 매핑하여 모델의 메모리 사용량과 연산량을 크게 줄이는 양자화(Quantization) 기법이다. 이 과정에서는 각 텐서마다 스케일링 팩터와 제로 포인트(zero‑point)를 학습하거나 사후에 추정해 동적 범위를 정밀하게 보정하고, 정수 연산(int8 × int8 → int32)으로 변환함으로써 하드웨어 가속기(GPU, DSP, NPU)에서 높은 추론 속도와 전력 효율을 달성한다. 대표적인 사용 사례로는 모바일·엣지 디바이스에서의 실시간 이미지·음성 인식, 로봇 제어, 클라우드 서비스의 대규모 모델 서빙 등 메모리·대역폭 제약이 큰 환경에서의 딥러닝 모델 경량화와 인퍼런스 가속이 있다. Int8Quantization은 FP16·BFloat16 같은 저정밀 부동소수점 양자화와 대비되며, 양자화 오류(quantization error)를 최소화하기 위해 Quantization‑Aware Training(QAT)이나 Post‑Training Quantization(PTQ)과 같은 보정 기법과 함께 사용된다." ;
    rdfs:subClassOf llm:Quantization .

llm:KVCache a owl:Class ;
    rdfs:label "KVCache"@en ;
    llm:description """**KVCache(키‑밸류 캐시)**는 Transformer 기반 대규모 언어 모델(LLM)에서 **자연어 생성 시** 각 토큰에 대해 미리 계산된 **Key와 Value 행렬**을 메모리에 저장해 두는 **효율적인 추론(Efficient Inference) 기법**이다.  
이 캐시는 **self‑attention** 연산에서 이전 토큰들의 Key‑Value 쌍을 재사용함으로써, 매 디코딩 단계마다 전체 시퀀스에 대해 다시 계산할 필요가 없게 만들어 **연산량·지연시간(Latency) 감소**와 **처리량(Throughput) 향상**을 달성한다.  
주요 사용 사례로는 **실시간 챗봇, 대화형 검색, 자동 요약, 코드 생성** 등 **autoregressive** 생성 작업에서 토큰당 응답 시간을 최소화해야 하는 서비스와, **GPU/TPU 메모리 제한이 있는 Edge 환경**에서 긴 프롬프트를 다루는 경우가 있다.  
KVCache는 **재계산(recomputation) 방식**과 대비되며, **활성화 체크포인팅(activation checkpointing)**이나 **양자화(quantization)**와 같은 다른 효율화 기법과 **조합**되어 모델의 **메모리 효율성**과 **추론 속도**를 동시에 최적화한다.""" ;
    rdfs:subClassOf llm:EfficientInference .

llm:KnowledgeGraph a owl:Class ;
    rdfs:label "KnowledgeGraph"@en ;
    llm:description "지식 그래프(Knowledge Graph)는 엔터티(entity)와 그들 사이의 관계(relation)를 삼중항(triple) 형태로 모델링한 **시맨틱 네트워크**이며, RDF(Resource Description Framework)나 OWL 같은 온톨로지(ontology) 표준을 기반으로 **지식 통합(Knowledge Integration)**을 실현하는 그래프 데이터베이스이다. 이러한 그래프는 노드와 엣지에 메타데이터와 속성을 부여하고, SPARQL 질의어나 그래프 임베딩(graph embedding) 기법을 통해 **연관성 탐색, 추론(inference), 그리고 벡터 검색(vector search)**을 효율적으로 수행한다. 대표적인 활용 사례로는 검색 엔진의 시맨틱 검색, 챗봇·질문‑응답 시스템의 지식 기반 응답, 추천 시스템의 관계 기반 추천, 그리고 의료·금융 등 도메인별 **지식 베이스(knowledge base)** 구축과 데이터 연계(linked data) 프로젝트가 있다. 관련 개념으로는 전통적인 지식 베이스와 구분되는 **그래프 기반 지식 표현**, 그리고 신경망 기반의 대규모 언어 모델과 결합된 **하이브리드 지식 그래프**가 있으며, 이는 정형·비정형 데이터의 **통합·연결·활용**을 극대화한다." ;
    rdfs:subClassOf llm:KnowledgeIntegration .

llm:LLMAgent a owl:Class ;
    rdfs:label "LLMAgent"@en ;
    llm:description "LLMAgent는 대규모 언어 모델(LLM)을 핵심 추론 엔진으로 삼아, 프롬프트 체이닝·툴 호출·메모리 관리·계획·실행을 통합한 **AgentArchitecture**의 한 형태이며, 자연어 입력을 받아 다단계 사고와 행동을 자동으로 생성하는 자율형 AI 에이전트이다. 이러한 에이전트는 **프롬프트 엔지니어링**과 **툴-증강**(예: 검색 API, 코드 실행기, 데이터베이스 연결) 메커니즘을 결합해 외부 도구를 동적으로 호출하고, **리트리벌‑오그멘티드 제너레이션(RAG)** 기반의 지식 검색과 **컨텍스트 메모리**를 활용해 장기 목표를 추적·조정한다. 대표적인 활용 사례로는 개인 비서·고객 지원 챗봇, 자동 보고서·코드 생성 파이프라인, 데이터 분석·리서치 자동화, 그리고 복합 워크플로우 오케스트레이션이 있으며, LangChain·AutoGPT·ReAct와 같은 프레임워크에서 구현된다. LLMAgent는 전통적인 **규칙 기반 봇**이나 순수 **프롬프트‑만‑응답** 모델과 달리, **계획‑기반·툴‑연동·메모리‑구조화**를 통해 복합적인 목표를 스스로 분해·실행할 수 있는 **인지‑형(reactive vs. deliberative) 에이전트**로 구분된다." ;
    rdfs:subClassOf llm:AgentArchitecture .

llm:LLaMA a owl:Class ;
    rdfs:label "LLaMA"@en ;
    llm:description "LLaMA( Large Language Model Meta AI)는 Meta에서 공개한 **Decoder‑Only** 기반의 대규모 언어 모델로, 순차적인 **self‑attention**과 **autoregressive** 토큰 예측을 통해 텍스트를 생성하는 **Transformer** 아키텍처를 사용합니다. 모델은 7B ~ 65B 파라미터 규모로 **pre‑training** 단계에서 방대한 웹 텍스트 코퍼스를 학습하고, 이후 **instruction‑tuning**이나 **RLHF**(Reinforcement Learning from Human Feedback)와 같은 **fine‑tuning** 기법을 적용해 **zero‑shot**·**few‑shot** 대화, 요약, 번역, 코드 생성 등 다양한 **LLM** 활용 사례에 바로 적용됩니다. LLaMA는 **open‑source** 배포와 효율적인 **token‑level inference** 덕분에 연구·산업 현장에서 **retrieval‑augmented generation**이나 **vector search** 기반 문서 검색·추천 시스템에 임베딩 생성기로도 널리 활용됩니다. 이와 달리 **Encoder‑Only** 모델인 BERT는 입력을 인코딩해 **masked language modeling**에 초점을 맞추고, **Encoder‑Decoder** 구조인 T5·BART는 번역·요약 등 **seq2seq** 작업에 특화된 점이 LLaMA와 대비됩니다." ;
    rdfs:subClassOf llm:DecoderOnly .

llm:Latency a owl:Class ;
    rdfs:label "Latency"@en ;
    llm:description "Latency(레이터시, 지연시간, 응답 시간)는 시스템이나 네트워크가 입력 요청을 받아 처리 결과를 반환하기까지 걸리는 시간 차이를 정량화한 **자동 메트릭(AutomaticMetric)** 으로, 일반적으로 밀리초(ms) 단위로 측정됩니다. 이 메트릭은 요청 전송 시점과 응답 수신 시점 사이의 타임스탬프를 기록하고, 평균·최소·최대·백분위수(p99) 등 다양한 통계값을 실시간으로 집계함으로써 지연 원인을 프로파일링하고 병목을 자동 탐지합니다. 대표적인 사용 사례로는 웹 API 응답 지연 분석, 클라우드 서비스 SLA 모니터링, 실시간 스트리밍·온라인 게임·AI 추론 서비스에서의 저지연( low‑latency ) 요구사항 충족, 그리고 IoT 디바이스와 엣지 컴퓨팅 환경에서의 실시간 제어 루프 최적화가 있습니다. Latency는 **throughput(처리량)**, **jitter(지터)**, **availability(가용성)** 등과 대비되는 핵심 성능 지표이며, 높은 처리량을 유지하면서도 지연을 최소화하는 것이 시스템 설계의 주요 과제로 자리 잡고 있습니다." ;
    rdfs:subClassOf llm:AutomaticMetric .

llm:LayerNorm a owl:Class ;
    rdfs:label "LayerNorm"@en ;
    llm:description "LayerNorm(레이어 정규화)은 신경망의 각 레이어 입력에 대해 **채널 차원 전체**를 기준으로 평균과 분산을 계산해 정규화함으로써, 배치 크기에 의존하지 않고 **학습 안정성**과 **수렴 속도**를 향상시키는 정규화 기법이다. 이 방법은 입력 텐서의 모든 요소를 동일한 스케일로 맞춘 뒤, 학습 가능한 **스케일 파라미터(γ)**와 **시프트 파라미터(β)**를 적용해 표현력을 유지하면서도 내부 공변량 이동(internal covariate shift)을 감소시킨다. 대표적인 사용 사례로는 **Transformer** 구조의 어텐션 블록, **RNN**·**CNN** 기반 언어 모델, 그리고 대규모 사전학습 모델에서의 **시퀀스‑레벨 정규화**가 있으며, 배치 정규화(BatchNorm)와 달리 배치 차원에 의존하지 않아 **소규모 배치** 혹은 **시계열 데이터**에 특히 유리하다. 관련 개념으로는 배치 정규화, 인스턴스 정규화, 그룹 정규화 등이 있으며, 이들와 비교했을 때 LayerNorm은 **전체 레이어 차원**을 한 번에 정규화한다는 점에서 **전역적인 정규화** 특성을 갖는다." ;
    rdfs:subClassOf llm:Normalization .

llm:LogicalReasoning a owl:Class ;
    rdfs:label "LogicalReasoning"@en ;
    llm:description """**LogicalReasoning(논리적 추론)**은 주어진 전제와 규칙을 기반으로 형식 논리 연산을 수행해 결론을 도출하는 **ReasoningTask(추론 과제)**의 한 형태이며, 연역적(deductive)·귀납적(inductive)·귀납-연역적(hybrid) 추론 방식을 모두 포괄한다.  
이 과정은 **전제‑결론 구조**, **명제 논리**, **일차 논리**, **규칙 기반 시스템** 등을 활용해 **논리 연산(AND, OR, NOT, IF‑THEN)**을 체계적으로 적용하고, **논리적 일관성**과 **비판적 사고**를 검증함으로써 오류를 최소화한다.  
대표적인 사용 사례로는 **지식 그래프 기반 질의응답**, **법률·의료 규정 자동 해석**, **수학·논리 퍼즐 풀이**, **AI 챗봇의 대화 흐름 제어**, 그리고 **자동화된 의사결정 엔진** 등이 있으며, 자연어 처리(NLP)와 결합해 **텍스트 내 논리 구조 추출**에도 활용된다.  
관련 개념으로는 **Probabilistic Reasoning(확률적 추론)**, **Commonsense Reasoning(상식 추론)**, **Analogical Reasoning(유추 추론)** 등이 있으며, 논리적 추론은 **정형화된 규칙과 명확한 증명 과정**에 중점을 두는 반면, 확률적·상식 추론은 **불확실성**과 **맥락 의존성**을 더 많이 반영한다.""" ;
    rdfs:subClassOf llm:ReasoningTask .

llm:MBART a owl:Class ;
    rdfs:label "MBART"@en ;
    llm:description "MBART(Multi‑Lingual BART)는 다국어 시퀀스‑투‑시퀀스(Seq2Seq) 작업을 위해 설계된 **Encoder‑Decoder Transformer** 기반의 사전학습 모델로, 입력 텍스트에 마스크와 노이즈를 가해 복원하도록 학습함으로써 **denoising auto‑encoding** 능력을 갖추고 있습니다. 이 모델은 25개 이상의 언어에 걸친 **다중 언어 사전학습(cross‑lingual pretraining)**을 수행해 **self‑attention**과 **cross‑attention** 메커니즘을 통해 언어 간 의미를 공유하고, **zero‑shot 번역**이나 **다국어 요약** 같은 작업에서 높은 전이 학습 성능을 발휘합니다. 대표적인 사용 사례로는 다국어 기계 번역, 다언어 문서 요약, 다문화 챗봇 구축 등 **멀티링구얼 자연어 처리** 시나리오가 있으며, 특히 **언어 간 도메인 적응**이 필요한 상황에서 강력한 벡터 검색 및 임베딩 생성 능력을 제공합니다. MBART은 BART와 구조적으로 유사하지만 **다국어 토크나이저와 다중 언어 어휘**를 사용한다는 점에서 mT5, XLM‑R, 그리고 단일 언어에 특화된 BART와 구별되며, **Encoder‑Decoder** 아키텍처를 활용한 **텍스트 생성**과 **텍스트 변환** 작업 전반에 적용됩니다." ;
    rdfs:subClassOf llm:EncoderDecoder .

llm:MCTS a owl:Class ;
    rdfs:label "MCTS"@en ;
    llm:description "Monte Carlo Tree Search(MCTS)는 확률적 시뮬레이션과 트리 구조를 결합한 **검색 절차(SearchProcedure)** 로, 현재 상태에서 가능한 행동들을 **선택(Selection) → 확장(Expansion) → 시뮬레이션(Rollout) → 역전파(Back‑propagation)**의 네 단계로 반복해 각 노드의 기대 보상을 추정한다. 핵심은 **탐색(Exploration)과 활용(Exploitation)의 균형**을 조절하는 UCT(Upper Confidence bound applied to Trees)와 같은 **탐색‑활용 트레이드오프** 수식으로, 이를 통해 무작위 Monte Carlo 시뮬레이션 결과를 효율적으로 트리 전체에 전파한다. 대표적인 사용 사례는 바둑·체스·고 같은 **보드 게임 AI**, 실시간 전략 게임, 로봇 경로 계획 및 강화학습(RL)에서의 **플래닝** 등 복잡한 결정 공간을 탐색해야 하는 분야이며, AlphaGo·AlphaZero와 같은 최신 시스템에서도 핵심 엔진으로 활용된다. MCTS는 **미니맥스·알파‑베타 프루닝**과 같은 전통적 완전 탐색 기법과 달리 확률적 롤아웃을 기반으로 하여 **대규모 상태·행동 공간**에서도 근사 최적 해를 빠르게 찾을 수 있다는 점에서 차별화된다." ;
    rdfs:subClassOf llm:SearchProcedure .

llm:MaskedLanguageModeling a owl:Class ;
    rdfs:label "MaskedLanguageModeling"@en ;
    llm:description "Masked Language Modeling(MLM)은 사전학습(pretraining) 단계에서 텍스트의 일부 토큰을 무작위로 마스크(mask)한 뒤, 양방향 Transformer 인코더가 해당 마스크된 위치의 원래 단어를 예측하도록 학습시키는 자기지도(self‑supervised) 학습 기법이다. 이 과정에서 모델은 주변 컨텍스트를 동시에 활용해 마스크된 토큰의 의미를 복원함으로써, 문맥‑민감한 표현(contextual embedding)과 깊은 언어 이해 능력을 획득한다. 대표적인 활용 사례로는 BERT, RoBERTa, ALBERT와 같은 대규모 사전학습 언어 모델이 있으며, 이후 감성 분석, 질의응답, 명명된 개체 인식(NER) 등 다양한 다운스트림 NLP 작업에 파인튜닝(fine‑tuning)되어 성능을 크게 향상시킨다. MLM은 단방향 자동회귀 언어 모델링(예: GPT)과 대비되어, 양방향 정보를 동시에 학습한다는 점에서 “autoencoding” 방식과 “next sentence prediction” 등 다른 사전학습 목표와도 상호 보완적으로 사용된다." ;
    rdfs:subClassOf llm:Pretraining .

llm:MathReasoning a owl:Class ;
    rdfs:label "MathReasoning"@en ;
    llm:description "**MathReasoning**는 수학적 개념, 공식, 정의 및 논리적 구조를 활용해 문제를 풀이하거나 증명을 전개하는 **ReasoningTask**의 한 형태이다. 이 작업은 기호 조작, 연산 규칙 적용, 단계별 **chain‑of‑thought** 생성 등을 통해 수식 전개, 정리, 해답 도출 과정을 **알고리즘적**으로 수행하며, 종종 **정량적 계산**과 **형식적 검증**을 결합한다. 대표적인 사용 사례로는 자동 정리 증명(automated theorem proving), 수학 교육용 AI 튜터, 과학·공학 시뮬레이션에서의 파라미터 최적화, 그리고 금융·물류 등에서의 최적화 모델링이 있다. 관련 개념으로는 **논리적 추론(logical reasoning)**, **귀납적/연역적 추론**, 그리고 **상식 추론(common‑sense reasoning)**이 있으며, MathReasoning은 정밀한 수학적 규칙에 기반한 **형식적** 추론이라는 점에서 상식 기반의 비정형 추론과 구별된다." ;
    rdfs:subClassOf llm:ReasoningTask .

llm:MemoryModule a owl:Class ;
    rdfs:label "MemoryModule"@en ;
    llm:description "**MemoryModule**은 AgentComponent 계층 구조에서 에이전트가 경험한 상태·행동·보상을 지속적으로 저장하고, 필요 시 벡터 임베딩 기반 유사도 검색(FAISS, ANN 등)으로 재활용할 수 있게 하는 **지식·기억 관리 서브시스템**이다. 이 모듈은 단기(episodic) 및 장기(semantic) 메모리를 구분해 시퀀스 학습, 메모리 컨솔리데이션, 그리고 컨텍스트 윈도우 확장을 attention 메커니즘과 transformer 아키텍처와 연계해 시계열 데이터와 멀티모달 피처를 **인덱싱·검색**하도록 설계된다. 대표적인 사용 사례로는 LLM‑기반 에이전트의 retrieval‑augmented generation, 강화학습(RL)에서 리플레이 버퍼 역할, 그리고 다중 에이전트 협업 시 공유 지식 베이스 구축 등이 있으며, 로봇 내비게이션, 대화형 챗봇, 의료 진단 보조 등 다양한 도메인에 적용된다. 관련 개념으로는 Stateless 에이전트와 대비되는 Stateful 구조, Planner·Perception·Action 컴포넌트와의 **상호작용**, 그리고 KnowledgeGraph 또는 ExternalMemory 와 같은 **외부 기억 저장소**가 있다." ;
    rdfs:subClassOf llm:AgentComponent .

llm:MessageRouter a owl:Class ;
    rdfs:label "MessageRouter"@en ;
    llm:description "**MessageRouter**는 **AgentComponent** 계층에 속하는 핵심 모듈로, 다수의 에이전트 간에 전송되는 메시지를 **동적 라우팅**하고 **필터링**·**전환**하는 역할을 수행한다(무엇인가?). 내부적으로는 메시지 헤더·컨텍스트 정보를 분석해 **목적지 에이전트**, **채널**, **우선순위** 등을 판단하고, **비동기 큐**·**Pub/Sub** 메커니즘을 활용해 **로드 밸런싱**·**스케일러블**하게 전달함으로써 **이벤트‑드리븐** 아키텍처를 지원한다(어떻게 동작하는가?). 대표적인 사용 사례로는 **멀티‑에이전트 시스템**에서의 협업 조정, **챗봇 프레임워크**에서 사용자 질의와 백엔드 서비스 간 중계, **IoT** 디바이스와 **마이크로서비스** 간의 실시간 데이터 흐름 관리 등이 있다(어디에 쓰이는가?). 관련 개념으로는 **MessageBroker**(중앙 집중형 메시지 중계)와 **Dispatcher/Mediator**(단일 목적 처리)와 대비되며, **MessageRouter**는 **다중 경로 선택**과 **조건부 라우팅**에 특화된 점이 차별점이다." ;
    rdfs:subClassOf llm:AgentComponent .

llm:Mistral a owl:Class ;
    rdfs:label "Mistral"@en ;
    llm:description "Mistral은 Mistral AI가 공개한 Decoder‑Only 아키텍처 기반의 대규모 언어 모델(LLM) 시리즈로, 주로 autoregressive 방식으로 토큰을 순차적으로 예측해 텍스트 생성·요약·코드 작성·대화 응답 등을 수행한다. Causal self‑attention과 causal masking 을 활용해 입력 시퀀스의 앞쪽 토큰만을 참조하도록 설계되었으며, parameter‑efficient 학습 기법(LoRA, QLoRA 등)과 Mixture‑of‑Experts 스파스 라우팅을 결합해 7 B ~ 8 × 7 B 규모에서도 높은 zero‑shot·few‑shot 성능을 달성한다. 대표적인 사용 사례로는 RAG(검색‑증강 생성) 파이프라인에서 벡터 검색 결과를 프롬프트에 삽입해 정교한 답변을 생성하거나, 기업용 챗봇·지식베이스·프로그래밍 어시스턴트 등 실시간 inference 가 요구되는 서비스에 배포되는 것이 있다. 이와 달리 Encoder‑Decoder 구조(BERT, T5 등)는 입력 전체를 동시에 인코딩해 seq2seq 작업에 강점을 보이는 반면, Mistral 과 같은 Decoder‑Only 모델은 단일 스트림 생성에 최적화돼 프롬프트 기반 instruction‑following 시나리오에서 더 유연하게 활용된다." ;
    rdfs:subClassOf llm:DecoderOnly .

llm:MultiAgentCollaboration a owl:Class ;
    rdfs:label "MultiAgentCollaboration"@en ;
    llm:description "**MultiAgentCollaboration**은 하나의 **AgenticTask** 내에서 여러 자율 에이전트가 공동 목표를 달성하기 위해 작업을 분할하고 정보를 교환하며 동기화하는 협업 메커니즘을 말합니다. 이 메커니즘은 분산 의사결정, 통신 프로토콜(예: 멀티캐스트 메시징, 옵션 협상), 역할 할당 및 보상 공유 같은 조정 전략을 활용해 에이전트 간 지식 공유와 실시간 피드백을 가능하게 하며, 강화학습 또는 계획 기반 알고리즘을 통해 협업 정책을 점진적으로 최적화합니다. 대표적인 적용 분야로는 스마트 공장 로봇군의 공정 조정, 물류 네트워크 내 다중 드론 경로 최적화, 복합 시뮬레이션 환경에서 다중 AI 플레이어가 협력해 전략을 수립하는 게임 AI, 그리고 기업 프로세스 자동화에서 다양한 소프트웨어 봇이 협업해 업무 흐름을 자동화하는 시나리오가 있습니다. 관련 개념으로는 단일 에이전트 접근법과 대비되는 멀티에이전트 강화학습(MARL), 스웜 인텔리전스, 분산 제어 시스템, 그리고 협업 프레임워크 인 OpenAI Gym Multi‑Agent Env와 같은 플랫폼이 있으며, 이들은 협업 정책의 학습·평가·배포 방식에서 차이를 보입니다." ;
    rdfs:subClassOf llm:AgenticTask .

llm:MultiAgentSystem a owl:Class ;
    rdfs:label "MultiAgentSystem"@en ;
    llm:description "**MultiAgentSystem(다중 에이전트 시스템, MAS)**은 여러 개의 자율적인 에이전트가 **분산된 환경에서 상호작용·협업**하거나 경쟁하면서 공동의 목표를 달성하거나 복잡한 문제를 해결하도록 설계된 **에이전트 아키텍처**이다. 각 에이전트는 **지식·감지·행동 루프**를 갖추고, **통신 프로토콜·협상 메커니즘·스케줄링**을 통해 **동시성**과 **스케일러빌리티**를 확보하며, 중앙 집중식 제어 없이 **분산 의사결정**을 수행한다. 대표적인 사용 사례로는 **스마트 시티 교통 관리**, **다중 로봇 협동 작업**, **게임 AI와 시뮬레이션**, **공급망 최적화** 및 **분산 데이터 분석** 등이 있으며, 이러한 분야에서 **협력적 학습·경쟁적 행동·자율 에이전트**가 핵심 역할을 한다. 관련 개념으로는 **단일 에이전트 시스템**, **중앙 집중식 제어**, **분산 인공지능**, **에이전트 기반 시뮬레이션** 등이 있으며, MAS는 **분산·협업**을 강조하는 점에서 중앙 집중식 구조와 뚜렷히 대비된다." ;
    rdfs:subClassOf llm:AgentArchitecture .

llm:MultiHeadAttention a owl:Class ;
    rdfs:label "MultiHeadAttention"@en ;
    llm:description "**Multi‑Head Attention**는 입력 시퀀스의 **Query‑Key‑Value(Q‑K‑V) 쌍**을 여러 개의 독립적인 **attention head** 로 동시에 처리하여, 서로 다른 표현 서브스페이스에서 **스케일드 닷‑프로덕트 어텐션**을 수행하고 그 결과를 **concatenation** 후 선형 변환으로 통합하는 **Attention Mechanism**의 핵심 구성 요소이다. 각 헤드는 **self‑attention** 혹은 **cross‑attention**을 수행하면서 **스케일링 팩터(√d_k)** 로 정규화된 점수를 기반으로 가중합을 계산하고, 이렇게 병렬화된 여러 헤드가 결합되면 모델은 **다중 관점**에서 풍부한 **문맥 정보**와 **장거리 의존성**을 포착할 수 있다. 대표적인 사용 사례로는 **Transformer**, **BERT**, **GPT**와 같은 대규모 언어 모델에서 **문장 임베딩**, **기계 번역**, **텍스트 요약**, **이미지‑텍스트 멀티모달** 작업 등에 널리 적용되며, **포지셔널 인코딩**과 결합해 순서 정보를 보존한다. 관련 개념으로는 **단일 헤드 어텐션(single‑head attention)**, **Self‑Attention**, **Cross‑Attention**, 그리고 **Convolution**·**RNN** 기반의 순차 처리 방식과 대비되어, **병렬 연산**과 **표현 다양성** 측면에서 차별화된다." ;
    rdfs:subClassOf llm:AttentionMechanism .

llm:MultiQueryAttention a owl:Class ;
    rdfs:label "MultiQueryAttention"@en ;
    llm:description "**MultiQueryAttention**은 AttentionMechanism 의 한 변형으로, 여러 query 벡터가 공통된 key · value 세트를 공유하도록 설계된 스케일드 닷‑프로덕트 어텐션 기법이다. 이 방식은 각 헤드마다 별도의 key‑value 프로젝션을 만들지 않고 단일 key‑value 행렬을 재사용함으로써 파라미터 수와 메모리 사용량을 크게 줄이며, 다중 query 프로젝션만을 별도로 생성해 효율적인 멀티‑헤드 연산을 가능하게 한다. 대규모 언어 모델(Large Language Model)·텍스트 생성·RAG(리트리벌‑오그멘티드 제너레이션)·벡터 검색(예: 다중 쿼리 검색 엔진) 등에서 수천 ~ 수만 개의 query 를 고정된 key‑value 인덱스에 동시에 매핑해야 할 때 연산 속도와 메모리 효율을 크게 향상시킨다. 전통적인 멀티‑헤드 어텐션이 각 헤드마다 독립적인 key‑value 쌍을 갖는 것과 대비되며, 관련 개념으로는 Grouped Query Attention, Key‑Value Cache, Sparse Attention, FlashAttention 등이 있다." ;
    rdfs:subClassOf llm:AttentionMechanism .

llm:NER a owl:Class ;
    rdfs:label "NER"@en ;
    llm:description """**NER(Named Entity Recognition)은 자연어 이해(NLU)의 핵심 하위 작업으로, 텍스트에서 사람·조직·위치·날짜·금액 등 사전 정의된 엔터티(개체)를 자동으로 식별하고 라벨링하는 기술입니다.**  
이 과정은 토큰화된 문장을 입력으로 받아, 조건부 확률 모델(CRF)이나 트랜스포머 기반 사전학습 모델(BERT, RoBERTa 등)으로 각 토큰에 “PER”, “ORG”, “LOC”와 같은 엔터티 타입을 예측하는 시퀀스 라벨링 방식으로 동작하며, 컨텍스트‑민감한 임베딩과 어텐션 메커니즘을 활용해 다의어와 중첩 엔터티를 정확히 구분합니다.  
대표적인 활용 사례로는 질문‑응답 시스템에서 핵심 개체 추출, 지식 그래프 구축을 위한 엔터티 정제, 의료 기록에서 질병·약물 인식, 금융 보고서에서 금액·계좌 번호 추출, 소셜 미디어 모니터링에서 브랜드·인플루언서 식별 등이 있으며, 검색 엔진과 챗봇에서 사용자 의도 파악을 위한 전처리 단계로 널리 사용됩니다.  
관련 개념으로는 엔터티 링크(Entity Linking)·관계 추출(Relation Extraction)·텍스트 분류(Text Classification)·감성 분석(Sentiment Analysis) 등이 있으며, NER은 텍스트 내 개체를 “무엇인지” 식별하는 반면 엔터티 링크는 식별된 개체를 외부 지식베이스와 연결하는 차이점을 가집니다.""" ;
    rdfs:subClassOf llm:NLU .

llm:OfflineRL a owl:Class ;
    rdfs:label "OfflineRL"@en ;
    llm:description "**OfflineRL(오프라인 강화학습)**은 사전에 수집된 고정된 행동‑보상 데이터(배치 데이터)만을 이용해 정책을 학습·평가하는 **배치 강화학습(batch reinforcement learning)** 방식으로, 환경과의 실시간 상호작용 없이 **오프‑폴리시(off‑policy) 학습**과 **정책 개선(policy improvement)**을 수행한다는 점이 핵심 정의이다. 주요 특징으로는 **데이터 효율성(data efficiency)**, **안전성(safety)** 및 **시뮬레이션 비용 절감**을 위해 **행동 로그(action logs)**, **보상 로그(reward logs)**, **상태 전이(state transition)** 정보를 활용하며, **분포 이동(distribution shift)**과 **오프라인 정책 평가(off‑policy evaluation, OPE)** 문제를 해결하기 위해 **중첩 중요도 가중치(importance sampling)**, **정규화(regularization)**, **모델 기반 보정(model‑based correction)** 등 다양한 알고리즘적 기법이 적용된다. 대표적인 사용 사례는 **의료 치료 정책 최적화**, **자율 주행 시뮬레이션**, **산업 로봇 제어**, **추천 시스템** 및 **광고 배치 최적화** 등과 같이 실제 환경에서의 탐색 비용이나 위험이 큰 분야이며, 기존 **온라인 RL(online reinforcement learning)**과 대비해 **데이터 수집 비용 절감**과 **실험 안전성 보장**이라는 장점을 제공한다. 관련 개념으로는 **모델 기반 RL(model‑based RL)**, **행동 클로닝(behavior cloning)**·**모방 학습(imitation learning)**, **오프라인 정책 평가(OPE)**, 그리고 **배치 학습(batch learning)** 등이 있으며, 이들 사이의 차이점은 **학습 데이터의 실시간성**, **탐색(exploration) 요구 수준**, **정책 업데이트 방식** 등에 있다." ;
    rdfs:subClassOf llm:ReinforcementLearning .

llm:OneShot a owl:Class ;
    rdfs:label "OneShot"@en ;
    llm:description "One‑Shot(One‑shot prompting)은 프롬프트에 **단일 예시**만을 제공하고 그 예시를 기반으로 모델이 새로운 입력에 대해 출력을 생성하도록 하는 **Basic Prompting** 기법이다. 이 방식은 모델이 **예시 하나**를 통해 작업의 형식과 기대 출력을 파악하도록 하여, **프롬프트 설계**가 간단하면서도 **컨텍스트 이해**와 **조건부 생성**을 동시에 수행하게 만든다. 주로 **텍스트 요약**, **질문‑답변**, **코드 변환** 등 **단일 샘플**만으로도 충분히 패턴을 학습할 수 있는 **언어 모델** 응용 분야에서 활용되며, **대화형 AI**나 **자동화된 문서 생성**에서도 효율적인 입력 방식으로 채택된다. One‑Shot은 **Zero‑shot**(예시 없음)과 **Few‑shot**(다수 예시 제공)과 대비되며, **예시 제공** 여부와 양에 따라 **모델 일반화 능력**과 **프롬프트 의존도**가 차별화되는 점이 핵심적인 차이점이다." ;
    rdfs:subClassOf llm:BasicPrompting .

llm:OnlineRL a owl:Class ;
    rdfs:label "OnlineRL"@en ;
    llm:description "OnlineRL(Online Reinforcement Learning)은 에이전트가 환경과 상호작용하면서 **실시간으로** 정책을 업데이트하고 보상을 즉시 반영하는 연속적인 의사결정 학습 방식으로, 전통적인 **오프라인(Offline) RL**이나 **배치(batch) RL**과 달리 데이터를 수집한 뒤 일괄적으로 학습하지 않고 **스트리밍 데이터**와 **시계열 보상**을 활용해 **점진적(policy‑gradient)·시차(Temporal‑Difference) 학습**을 수행한다. 주요 특징으로는 **탐험‑활용(Exploration‑Exploitation) 트레이드오프**를 동적으로 조절하는 **온라인 탐색 전략**, **비정상(non‑stationary) 환경**에 대한 **적응성(adaptivity)**, 그리고 **샘플 효율성(sample efficiency)**을 높이기 위한 **온‑폴리시(on‑policy)·오프‑폴리시(off‑policy) 혼합 업데이트** 메커니즘이 있다. 대표적인 사용 사례에는 **로봇 제어**, **자율 주행 차량**, **실시간 추천 시스템**, **온라인 광고 입찰**, 그리고 **게임 AI**와 같이 환경이 지속적으로 변하고 즉각적인 피드백이 요구되는 **연속적·동적 응용 분야**가 포함된다. 관련 개념으로는 **밴딧(bandit) 알고리즘**, **연속 학습(continual learning)**, **모델 기반(model‑based) RL** 등이 있으며, 이들은 **오프라인 RL**이 사전 수집된 고정 데이터셋에 의존하는 점과 대비되어 **실시간 학습·배포**라는 차별점을 제공한다." ;
    rdfs:subClassOf llm:ReinforcementLearning .

llm:PAL a owl:Class ;
    rdfs:label "PAL"@en ;
    llm:description """**PAL (Program‑Aided Language model)**은 대형 언어 모델(Large Language Model, LLM)에 외부 프로그램 실행 능력을 결합한 **Advanced Prompting** 기법으로, 모델이 자연어 프롬프트를 해석한 뒤 필요한 계산이나 데이터 조회를 실제 코드(예: Python 스크립트)로 변환·실행하고, 그 결과를 다시 언어 모델에 피드백하여 최종 답변을 생성합니다.  
이 방식은 **프롬프트 엔지니어링** 단계에서 “코드 생성 → 실행 → 결과 통합”이라는 **체인 오브 사고(Chain‑of‑Thought)** 흐름을 자동화함으로써, 복잡한 수학 문제, 데이터 분석, API 호출 등 **정확한 연산**이 요구되는 작업에서 **self‑consistency**와 **few‑shot** 학습 효과를 크게 향상시킵니다.  
대표적인 사용 사례로는 수학·과학 문제 풀이, 시뮬레이션 기반 의사결정, 실시간 데이터베이스 질의, 그리고 코드 자동화 도구(예: Jupyter Notebook 보조) 등이 있으며, 특히 **프롬프트 튜닝(prompt tuning)**과 결합해 특정 도메인(금융, 의료, 엔지니어링)에서 맞춤형 계산 로직을 제공하는 데 유용합니다.  
관련 개념으로는 **ReAct**(Reason+Act)와 **Tool‑augmented LLM**이 있으며, PAL은 외부 프로그램 실행을 명시적으로 포함한다는 점에서 이들보다 **코드 실행 단계가 명시적**이고 **결과 피드백 루프가 강화**된 차별점을 가집니다.""" ;
    rdfs:subClassOf llm:AdvancedPrompting .

llm:PaLM a owl:Class ;
    rdfs:label "PaLM"@en ;
    llm:description "PaLM(Pre‑trained **Language Model**)은 구글이 개발한 **대규모 디코더‑온리(Decoder‑Only) 트랜스포머** 아키텍처 기반의 자연어 처리 모델로, 수천억 개의 파라미터와 방대한 텍스트 코퍼스를 사전 학습하여 인간 수준의 언어 이해·생성 능력을 갖춘 **초거대 언어 모델**이다. 입력 토큰 시퀀스를 **자기 회귀(self‑regressive)** 방식으로 한 번에 하나씩 디코딩하면서, **멀티‑헤드 어텐션**과 **포지셔널 인코딩**을 활용해 문맥 정보를 동적으로 통합하고, **스패스 파라미터 공유**와 **스케일드 드롭아웃** 등 최적화 기법으로 효율적인 추론을 구현한다. PaLM은 **대화형 AI**, **문서 요약**, **코드 생성**, **다국어 번역**, **지식 추론** 등 다양한 NLP·NLG 작업에 적용되며, 특히 **프롬프트 엔지니어링**과 **few‑shot learning**을 통해 제한된 라벨 데이터만으로도 높은 성능을 발휘한다. 이와 대비되는 모델로는 **Encoder‑Only** 구조의 BERT나 **Encoder‑Decoder** 구조의 T5/Seq2Seq가 있으며, 이들은 **양방향 마스크드 언어 모델링**이나 **조건부 생성**에 특화된 반면, PaLM은 **단방향(오른쪽‑to‑왼쪽) 디코딩**에 최적화돼 텍스트 생성과 연속적인 토큰 예측에 강점을 가진다." ;
    rdfs:subClassOf llm:DecoderOnly .

llm:ParallelDecoding a owl:Class ;
    rdfs:label "ParallelDecoding"@en ;
    llm:description "ParallelDecoding은 대규모 언어 모델이나 트랜스포머 기반 시퀀스‑투‑시퀀스 모델에서 여러 토큰을 동시 (병렬) 생성하여 추론 속도를 극대화하는 디코딩 기법을 말합니다. 기존의 순차적(autoregressive) 디코딩과 달리, 모델은 잠재 공간 또는 확률 분포 정보를 활용해 다중 토큰 예측을 한 번에 계산하고, GPU/TPU 멀티코어 또는 멀티‑배치 환경에서 동시 연산(멀티스레딩·비동기 처리)으로 시간 복잡도를 O(1) 에 가깝게 줄입니다. 이 방식은 실시간 챗봇, 실시간 번역, 코드 자동 완성, 검색 엔진 쿼리 응답, 대규모 추천 시스템 등 고속 응답이 요구되는 실시간 서비스와 대규모 LLM 서빙 시나리오에 특히 유용합니다. ParallelDecoding은 빔 서치·샘플링·Top‑k·Top‑p 와 같은 전통적인 순차 디코딩 기법과 대비되며, Batch Decoding·Token‑wise Parallelism·Chunked Generation 과 같은 관련 최적화 기법과도 결합해 메모리 사용량을 최소화하고 연산 효율성을 극대화합니다." ;
    rdfs:subClassOf llm:EfficientInference .

llm:Paraphrasing a owl:Class ;
    rdfs:label "Paraphrasing"@en ;
    llm:description "**Paraphrasing**은 주어진 문장을 의미를 그대로 유지하면서 다른 어휘·구문으로 **재작성**하거나 **텍스트 변형**하는 작업으로, 자연어 생성(NLG)의 핵심 하위 과제 중 하나이다. 이 과정은 **동의어 교체**, **문장 구조 재구성**, **문맥 유지**와 같은 **lexical substitution**·**syntactic restructuring**을 통해 원문과 의미적 일관성을 보장하면서도 **다양한 표현**을 생성하며, 최신 **언어 모델**(예: Transformer 기반 seq2seq, 대규모 사전학습 모델)이나 규칙 기반 시스템에 의해 자동화된다. 대표적인 활용 사례로는 **데이터 증강**을 통한 NLP 모델 학습 향상, **플래그리즘 방지**를 위한 콘텐츠 재작성, 챗봇·질문‑응답 시스템에서의 응답 다양성 확보, 그리고 교육·연구 분야에서 **문장 간 유사도** 평가와 **텍스트 요약** 보조 등이 있다. Paraphrasing은 **요약(summarization)**처럼 텍스트 길이를 줄이는 작업이나 **번역(translation)**처럼 언어를 바꾸는 작업과는 구별되지만, **문장 단순화(sentence simplification)**, **스타일 전이(style transfer)**, **텍스트 재작성(text rewriting)** 등과는 목적·기법 면에서 상호 보완적인 관계에 있다." ;
    rdfs:subClassOf llm:NLG .

llm:PermissionControl a owl:Class ;
    rdfs:label "PermissionControl"@en ;
    llm:description "**PermissionControl**은 시스템이나 서비스에 대한 **접근 권한을 관리·제어**하는 **SafetyTechnique**의 한 형태로, 사용자가 수행할 수 있는 행동을 사전에 정의된 **보안 정책**에 따라 제한하는 메커니즘을 말합니다. 주로 **역할 기반 접근 제어(RBAC)**, **속성 기반 접근 제어(ABAC)**, **최소 권한 원칙(Least‑Privilege Principle)** 등을 활용해 **인증(Authentication)** 후 **인가(Authorization)** 과정을 거쳐 권한을 부여하고, **감사 로그**와 **실시간 모니터링**을 통해 위반 행위를 탐지·차단합니다. 대표적인 사용 사례로는 **클라우드 인프라**의 리소스 관리, **기업 내부 시스템**의 데이터 접근 제한, **IoT 디바이스**의 기능 제어, 그리고 **API 게이트웨이**에서의 서비스 호출 제한 등이 있습니다. 관련 개념으로는 **Encryption**(데이터 암호화)이나 **Sandboxing**(격리 실행 환경)과 대비되며, **Open Access**(무제한 접근)와는 **보안·안전성** 측면에서 명확히 구분됩니다." ;
    rdfs:subClassOf llm:SafetyTechnique .

llm:Perplexity a owl:Class ;
    rdfs:label "Perplexity"@en ;
    llm:description """**Perplexity**는 언어 모델이나 확률 기반 텍스트 생성 시스템의 성능을 정량화하는 자동 평가 지표(AutomaticMetric)로, 테스트 데이터에 대한 **음의 로그우도(Negative Log‑Likelihood)**를 평균한 뒤 **지수화(exponentiation)**한 값이다.  
이 지표는 모델이 각 토큰을 예측할 때 부여하는 **조건부 확률**을 로그 스케일로 변환하고 평균을 구한 뒤, 그 결과에 **e**(또는 2)를 거듭 제곱해 **예측 불확실성(uncertainty)**을 나타내며, 값이 낮을수록 **예측 정확도와 일반화 능력**이 뛰어나다는 의미이다.  
Perplexity는 **Transformer, GPT, BERT 기반 언어 모델**, 음성 인식(ASR) 시스템, 기계 번역 및 텍스트 생성 파이프라인 등에서 **모델 선택, 하이퍼파라미터 튜닝, 학습 진행 상황 모니터링** 등에 널리 활용되는 핵심 메트릭이며, **BLEU·ROUGE·METEOR**와 같은 n‑gram 겹침 지표와는 달리 **확률 분포 전체를 평가**한다.  
관련 개념으로는 **Cross‑Entropy(교차 엔트로피)**, **Entropy(엔트로피)**, **Bits‑Per‑Character(BPC)** 등이 있으며, 인간 평가(Human Evaluation)와는 **주관적 품질 판단**이라는 차이를 보인다.""" ;
    rdfs:subClassOf llm:AutomaticMetric .

llm:PlanAndSolve a owl:Class ;
    rdfs:label "PlanAndSolve"@en ;
    llm:description "**PlanAndSolve**는 복잡한 과제에 대해 “계획 → 실행 → 검증” 의 순환 구조를 명시적으로 프롬프트에 삽입하는 AdvancedPrompting 기법으로, 사용자는 먼저 문제를 **작업 분해(task decomposition)**하고 단계별 **계획(plan)**을 제시한 뒤, LLM에게 각 단계별 **솔루션(solution)**을 생성하고 최종 결과를 **자기 검증(self‑consistency)**하도록 유도한다. 이 방식은 **프롬프트 체인(chain‑of‑thought)**, **ReAct** 혹은 **툴‑사용 프롬프트(tool‑augmented prompting)**와 달리, **계획 단계와 해결 단계**를 명확히 구분하고 반복적인 **피드백 루프(feedback loop)**를 통해 오류를 최소화한다는 점이 핵심 특징이다. 대표적인 사용 사례로는 **코드 자동 생성·디버깅**, **다단계 데이터 분석**, **전략적 의사결정 지원**, 그리고 **복합적인 비즈니스 워크플로우 자동화** 등이 있으며, 특히 **제한된 샷(zero‑/few‑shot) 상황**에서 높은 정확도를 보인다. 관련 개념으로는 **단일‑스텝 프롬프트(single‑step prompting)**, **연쇄 사고(chain‑of‑thought prompting)**, 그리고 **메타‑프롬프트(meta‑prompting)**가 있으며, PlanAndSolve은 이들보다 **구조화된 계획‑솔루션 흐름**을 강조한다." ;
    rdfs:subClassOf llm:AdvancedPrompting .

llm:PlanQuality a owl:Class ;
    rdfs:label "PlanQuality"@en ;
    llm:description "**PlanQuality**는 자동화된 평가 지표(AutomaticMetric)의 일종으로, 인공지능 플래너나 로봇 시스템이 생성한 **계획(plan)** 의 전반적인 품질을 수치화하는 메트릭이다. 이 지표는 **실현 가능성(feasibility)**, **최적성(optimality)**, **자원 활용 효율(resource utilization)**, **강인성(robustness)** 등 여러 평가 기준을 가중합하거나 학습된 모델을 통해 **복합 점수(composite score)** 로 변환함으로써, 인간의 주관적 판단 없이도 객관적인 **품질 점수(quality score)** 를 제공한다. 대표적인 사용 사례로는 **AI 플래닝, 물류 스케줄링, 로봇 작업 순서 생성, 워크플로 자동화** 등에서 후보 계획들을 **랭킹(ranking)** 하거나 **검색(search)** 결과를 정렬하고, 새로운 플래너의 **벤치마크(benchmark)** 를 수행하는 데 활용된다. 관련 개념으로는 **PlanFeasibility(실현 가능성 평가)**, **PlanCost(비용 평가)**, **PlanRobustness(강인성 평가)** 등이 있으며, 이들과 달리 PlanQuality는 단일 차원이 아닌 **다차원 품질 종합**을 목표로 하여 **수동 평가(manual metric)** 혹은 **인간 판단(human judgment)** 기반 메트릭과는 구별된다." ;
    rdfs:subClassOf llm:AutomaticMetric .

llm:Planner a owl:Class ;
    rdfs:label "Planner"@en ;
    llm:description "Planner는 AgentComponent 계층 구조에서 **목표 달성을 위한 행동 시퀀스와 작업 흐름을 사전 설계·조정**하는 핵심 모듈이며, 주어진 목표와 제약 조건을 기반으로 최적의 계획을 생성한다. 이 컴포넌트는 **상태 공간 탐색, 시나리오 플래닝, 리소스 할당·스케줄링** 등의 알고리즘을 활용해 다중 단계의 실행 경로를 계산하고, 필요에 따라 강화학습 정책이나 LLM 기반 추론을 결합해 동적 환경에 적응한다. 대표적인 사용 사례로는 **로봇 경로 계획, 자동화 비즈니스 프로세스 오케스트레이션, 게임 AI 전략 수립, 멀티에이전트 협업 스케줄링** 등이 있으며, 복잡한 시뮬레이션이나 실시간 의사결정 시스템에서도 널리 적용된다. Planner는 **Executor(실행 담당)**·**Memory(지식 저장)**·**Perceiver(감지 담당)**와 같은 다른 AgentComponent와 대비되어, 단순히 입력에 즉각 반응하는 **Reactive** 컴포넌트와 달리 **전략적·예측적** 접근을 제공한다." ;
    rdfs:subClassOf llm:AgentComponent .

llm:PlannerExecutorAgent a owl:Class ;
    rdfs:label "PlannerExecutorAgent"@en ;
    llm:description "**PlannerExecutorAgent**는 AgentArchitecture 내에서 “계획‑실행” 패러다임을 구현한 LLM‑기반 에이전트로, 고수준 목표를 받아 플래너(Planner) 모듈이 작업을 다단계 계획으로 분해하고, 실행기(Executor) 모듈이 그 계획을 순차적으로 구현·검증하는 구조를 갖습니다. 플래너는 프롬프트 체이닝·시퀀스‑투‑시퀀스 모델링을 이용해 작업 분해, 의존성 분석, 리소스 할당 등을 수행하고, 실행기는 도구 호출, API 연동, 상태 피드백 수집 등을 통해 계획을 실시간으로 구현·조정하며, 피드백 루프를 통해 계획을 동적으로 수정합니다. 이 에이전트는 복잡한 비즈니스 프로세스 자동화, 데이터 파이프라인 오케스트레이션, 로봇 행동 계획, 멀티스텝 문제 해결 등 다양한 사용 사례에 적용되어 다중 단계 추론과 정밀한 작업 제어가 요구되는 환경에 최적화됩니다. 반면 React‑style 에이전트나 단일 Tool‑using 에이전트와 달리 PlannerExecutorAgent은 “계획 → 실행 → 재계획” 의 계층적 워크플로를 구현함으로써 전통적인 리액티브 에이전트보다 높은 전략적 유연성과 오류 복구 능력을 제공한다는 점이 차별화됩니다." ;
    rdfs:subClassOf llm:AgentArchitecture .

llm:Planning a owl:Class ;
    rdfs:label "Planning"@en ;
    llm:description "**Planning**은 AgenticTask 하위에서 에이전트가 **고수준 목표를 구체적인 행동 순서와 리소스 할당으로 분해**하고, 미래 상황을 예측·시뮬레이션하여 최적의 실행 경로를 설계하는 과정이다. 이 과정은 **제약 만족, 휴리스틱 탐색(A\\*, MCTS 등), 계층적 작업 네트워크(Hierarchical Task Network)와 동적 계획법**을 활용해 목표‑행동‑결과의 피드백 루프를 반복함으로써 **전략적 의사결정과 다중 목표 최적화**를 수행한다. 대표적인 사용 사례로는 **자율 로봇 내비게이션, AI 기반 워크플로 자동화, 전략 게임 AI, 공급망 최적화, 개인 디지털 비서의 일정·작업 관리** 등이 있다. 관련 개념으로는 **스케줄링·의사결정·목표 관리**가 있으며, **반응형(reactive) 행동**과 대비돼 **고 deliberative(심사숙고) 방식**을 제공한다." ;
    rdfs:subClassOf llm:AgenticTask .

llm:PolicyModule a owl:Class ;
    rdfs:label "PolicyModule"@en ;
    llm:description "**PolicyModule**은 에이전트의 **AgentComponent** 중 하나로, 환경으로부터 받은 관측(state)이나 특징(feature) 벡터를 입력 받아 **행동(action) 선택**을 수행하는 **정책(policy) 함수**를 캡슐화한 모듈이다. 일반적으로 **신경망 기반 정책 네트워크** 혹은 파라미터화된 함수 형태로 구현되며, 입력된 상태 표현을 **확률 분포(확률적 정책)** 혹은 **결정적 액션(결정적 정책)** 으로 변환하고, **정책 경사(policy gradient)**, **액터-크리틱(actor‑critic)** 등 온라인 학습 기법을 통해 실시간으로 **정책 업데이트**가 가능하도록 설계된다. 이 모듈은 **강화학습(RL) 에이전트**, **로보틱스 제어**, **게임 AI**, **자율주행** 및 **추천 시스템** 등에서 핵심적인 **의사결정 엔진**으로 활용되어, 복잡한 환경에서의 **탐험‑활용(exploration‑exploitation)** 균형을 유지하며 행동을 생성한다. **ValueModule**이나 **ModelModule**과는 달리 직접적인 **가치 추정**이나 **환경 모델링**을 담당하지 않으며, 대신 **행동 정책**을 제공함으로써 **에이전트 아키텍처** 내에서 **플래너(planner)** 혹은 **시뮬레이터와** 협업하는 **모듈형 설계**의 핵심 요소로 구분된다." ;
    rdfs:subClassOf llm:AgentComponent .

llm:PositionEmbedding a owl:Class ;
    rdfs:label "PositionEmbedding"@en ;
    llm:description "**PositionEmbedding**은 시퀀스 데이터(텍스트, 시계열, 이미지 패치 등)에서 각 토큰이나 요소의 순서 정보를 수치 벡터로 변환해 EmbeddingLayer 에 추가하는 기법으로, “위치 인코딩(position encoding)”이라고도 불립니다. 일반적으로 절대 위치를 나타내는 사인‑코사인 기반 **sinusoidal positional embedding**이나 학습 가능한 **learnable positional embedding**을 사용해, 입력 토큰 임베딩에 직접 더하거나 연결(concatenation)하여 **self‑attention** 메커니즘이 순서 의존성을 인식하도록 합니다. 이 방식은 **Transformer**, **BERT**, **GPT**, **Vision Transformer(ViT)** 등에서 문맥‑의미 표현을 강화해 기계 번역, 질의응답, 이미지 분류, 시계열 예측 등 다양한 **자연어 처리(NLP)**와 **컴퓨터 비전** 작업에 필수적으로 활용됩니다. 관련 개념으로는 **TokenEmbedding**(단어 임베딩)과 **SegmentEmbedding**(문장 구분 임베딩)이 있으며, **Relative Position Embedding**은 절대 위치 대신 토큰 간 상대 거리를 인코딩해 더 유연한 순서 모델링을 제공한다는 점에서 차별화됩니다." ;
    rdfs:subClassOf llm:EmbeddingLayer .

llm:PostNorm a owl:Class ;
    rdfs:label "PostNorm"@en ;
    llm:description "**PostNorm**은 트랜스포머와 같은 딥러닝 모델에서 **잔차 연결(residual connection) 이후에 적용되는 정규화(layer‑norm) 방식**을 의미한다. 이 방법은 서브‑레이어(예: 다중‑헤드 어텐션 또는 피드‑포워드 네트워크)의 출력을 잔차 합산한 뒤에 **LayerNorm을 수행**함으로써, 출력 분포를 일정하게 유지하고 **학습 안정성 및 그래디언트 흐름을 개선**한다. 대표적인 사용 사례로는 **BERT, GPT, Vision Transformer(ViT) 등 최신 대규모 언어·비전 모델**에서 **PostNorm 기반 트랜스포머 블록**이 채택되어 빠른 수렴과 높은 성능을 보이며, 또한 **딥 레이어 네트워크** 전반에 걸쳐 적용 가능하다. 관련 개념으로는 **PreNorm(정규화를 잔차 이전에 적용)**, **BatchNorm**, **RMSNorm**, **WeightNorm** 등이 있으며, 특히 **PreNorm과 비교했을 때** PostNorm은 초기 단계에서의 학습 난이도가 다소 높지만 **깊은 모델에서 더 나은 표현력**을 제공한다는 차별점을 가진다." ;
    rdfs:subClassOf llm:Normalization .

llm:PreNorm a owl:Class ;
    rdfs:label "PreNorm"@en ;
    llm:description "PreNorm은 트랜스포머와 같은 잔차 블록에서 정규화 레이어(LayerNorm)를 **서브‑모듈(예: 멀티헤드 어텐션 또는 피드‑포워드) 앞에 적용**하는 정규화 기법으로, 입력 텐서를 먼저 정규화한 뒤에 연산을 수행하고 마지막에 잔차 연결을 더한다는 점에서 Post‑Norm과 순서가 반대이다. 이 방식은 정규화가 바로 **그라디언트 흐름을 개선**하고, 깊은 네트워크에서도 **학습 안정성**과 **수렴 속도**를 크게 향상시킨다. PreNorm은 GPT‑3·GPT‑4와 같은 대형 언어 모델, 비전 트랜스포머(ViT), 그리고 최근의 디퓨전 모델 등 **시퀀스 모델링·이미지 인식·생성 AI** 분야에서 널리 채택되어 수백 층 이상의 깊이를 효율적으로 학습한다. 관련 개념으로는 전통적인 Post‑Norm, RMSNorm, ScaleNorm, 그리고 정규화 위치를 혼합한 “sandwich” 또는 “pre‑post” 정규화 방식이 있으며, 이들 간의 차이는 **정규화 적용 시점**과 **잔차 경로와의 상호작용**에 있다." ;
    rdfs:subClassOf llm:Normalization .

llm:Precision a owl:Class ;
    rdfs:label "Precision"@en ;
    llm:description "**Precision(정밀도)**은 자동 평가 지표(AutomaticMetric) 중 하나로, 모델이 **양성이라고 예측한 사례 중 실제로 양성인 비율**을 의미한다(정밀도 = TP / (TP + FP)). 이 메트릭은 혼동 행렬(confusion matrix)에서 **True Positive(TP)**와 **False Positive(FP)**만을 사용해 계산되며, **False Positive를 최소화해야 하는 상황**—예를 들어 스팸 필터링, 의료 진단, 혹은 **벡터 검색·임베딩 기반 유사도 매칭**에서 사용자가 기대하는 높은 **검색 정확도**를 보장하고자 할 때—특히 유용하다. 대표적인 적용 분야로는 **정보 검색, 추천 시스템, 이진 분류, 다중 클래스 분류** 등이 있으며, 검색 엔진에서는 **정밀도 높은 상위 결과**가 사용자 만족도와 직접 연결된다. 관련 개념으로는 **Recall(재현율)**, **F1‑Score**, **Accuracy**, **Mean Average Precision(MAP)**, **NDCG** 등이 있으며, 정밀도는 **정확성(Exactness)**에 초점을 맞추는 반면 재현율은 **포괄성(Completeness)**에 중점을 두어 서로 보완적인 특성을 가진다." ;
    rdfs:subClassOf llm:AutomaticMetric .

llm:PreferenceAlignment a owl:Class ;
    rdfs:label "PreferenceAlignment"@en ;
    llm:description "**PreferenceAlignment**은 인간 사용자가 제공한 선호도(예: 선택, 순위, 평가)를 직접적인 학습 신호로 활용해 인공지능 모델의 행동을 인간의 가치와 일치시키는 **AlignmentMethod**의 한 형태이다. 이 방법은 주로 **선호 학습(preference learning)**, **보상 모델링(reward modeling)**, 혹은 **역강화학습(inverse reinforcement learning)**과 같은 기술을 이용해, 다수의 인간 피드백을 통계적으로 추정한 보상 함수에 매핑하고, 그 보상 함수를 기반으로 **강화학습(RLHF)** 혹은 **지도학습**을 수행한다. 대표적인 사용 사례로는 대화형 챗봇의 응답 품질 향상, 생성형 모델의 내용 안전성 보장, 그리고 로봇 행동 정책을 인간 친화적으로 조정하는 작업 등이 있다. 관련 개념으로는 **RewardModeling**, **Human‑in‑the‑Loop**, **ValueAlignment** 등이 있으며, 이들은 모두 인간 의도를 반영한다는 점에서 공통하지만, PreferenceAlignment은 특히 **선호 데이터(선택/순위)**에 초점을 맞추어 **명시적 보상 설계 없이** 직접적인 정렬을 목표로 한다." ;
    rdfs:subClassOf llm:AlignmentMethod .

llm:PrefixLanguageModeling a owl:Class ;
    rdfs:label "PrefixLanguageModeling"@en ;
    llm:description "**Prefix Language Modeling(프리픽스 언어 모델링)**은 텍스트 시퀀스의 앞부분(프리픽스)을 입력으로 제공하고, 그 뒤에 이어질 토큰들을 한 번에 예측하도록 학습하는 **프리트레인(pretraining) 기법**이다. 이 방식은 **Transformer 기반 인코더‑디코더 구조**에서 인코더가 프리픽스를 인코딩하고, 디코더가 **조건부(next‑token) 생성**을 수행함으로써 **시퀀스‑투‑시퀀스(sequence‑to‑sequence) 예측**을 구현한다; 따라서 **마스크드 언어 모델링(masked LM)**이나 **오토레그레시브(autoregressive) 언어 모델링**과는 달리 전체 문맥을 동시에 고려하면서도 **다음 토큰 예측**과 **텍스트 완성**을 동시에 학습한다. 대표적인 사용 사례로는 **대규모 사전학습된 언어 모델(예: T5, BART)**의 사전학습 단계, **대화형 AI**의 문맥 기반 응답 생성, 그리고 **요약·번역·질문‑응답** 등 다양한 **자연어 처리(NLP) 태스크**에 적용되어 데이터 효율성을 높이고 문맥 이해 능력을 강화한다. 관련 개념으로는 **마스크드 LM(BERT)**, **오토레그레시브 LM(GPT)**, **시퀀스 라벨링** 등이 있으며, 프리픽스 LM은 **조건부 생성**과 **전체 시퀀스 예측**을 동시에 수행한다는 점에서 이들 방식과 차별화된다." ;
    rdfs:subClassOf llm:Pretraining .

llm:ProgramOfThoughts a owl:Class ;
    rdfs:label "ProgramOfThoughts"@en ;
    llm:description "**ProgramOfThoughts**는 **AdvancedPrompting** 영역에서 제시되는 고차원 프롬프트 설계 기법으로, LLM에게 일련의 사고 흐름(Thought Sequence)을 **프로그래밍**하듯 명시적으로 정의하고 제어하는 방법을 말합니다. 사용자는 “문제 → 가설 → 검증 → 결론”과 같은 단계별 템플릿을 미리 구성하고, 각 단계마다 **프롬프트 체인(Prompt Chaining)** 혹은 **시퀀스 제어(Sequence Control)**를 삽입해 모델이 **다중 단계 추론(Multi‑step Reasoning)**을 수행하도록 유도합니다. 이 기법은 복잡한 수학 문제 풀이, 법률 문서 분석, 코드 디버깅 등 **문제 해결**과 **지식 추론**이 요구되는 상황에서, 단일 샷 프롬프트보다 높은 정확도와 일관성을 제공하는 대표적인 사용 사례입니다. **Chain‑of‑Thought**, **Self‑Consistency**, **Few‑Shot Prompting**과 같은 기존 사고 흐름 기법과 달리, ProgramOfThoughts는 **메타프롬프트(Meta‑Prompt)**와 **프롬프트 템플릿(Template)**을 결합해 사고 단계 자체를 프로그래밍 언어처럼 선언·조합할 수 있다는 점에서 차별화됩니다." ;
    rdfs:subClassOf llm:AdvancedPrompting .

llm:PromptChaining a owl:Class ;
    rdfs:label "PromptChaining"@en ;
    llm:description "프롬프트 체이닝(PromptChaining)은 하나의 작업을 여러 단계의 프롬프트로 분할하고, 각 단계의 출력 결과를 다음 프롬프트의 입력으로 연결해 LLM 이 복합적인 목표를 순차적으로 달성하도록 설계하는 고급 프롬프트 기법이다. 이 방식은 컨텍스트 유지와 조건부 로직을 활용해 멀티턴 대화, 데이터 전처리‑후처리, 혹은 다중 모델 연동과 같은 복잡한 작업 흐름을 자동화하며, 각 단계별 프롬프트를 재사용·조정함으로써 프롬프트 최적화와 비용 효율성을 동시에 확보한다. 대표적인 사용 사례로는 고객 지원 챗봇에서 문의 분류 → 답변 생성 → 피드백 반영의 연속적인 서비스, 코드 생성 파이프라인에서 요구사항 추출 → 구조 설계 → 테스트 케이스 자동 생성 등이 있다. 관련 개념으로는 단일 입력으로 답을 도출하는 싱글‑샷 프롬프트와 달리 체인‑오브‑쓰앗(Chain‑of‑Thought)이나 Few‑shot 프롬프트와는 구분되며, 프롬프트 엔지니어링·프롬프트 튜닝과 결합해 보다 정교한 시나리오 기반 AI 응용을 가능하게 한다." ;
    rdfs:subClassOf llm:AdvancedPrompting .

llm:Pruning a owl:Class ;
    rdfs:label "Pruning"@en ;
    llm:description "프루닝(Pruning)은 딥러닝 모델 압축(Model Compression) 기법 중 하나로, 학습된 신경망에서 중요도가 낮은 가중치나 뉴런을 선택적으로 제거하여 파라미터 수를 감소시키는 과정이다. 비구조적 프루닝은 개별 가중치를 영(0)으로 만들고, 구조적 프루닝은 전체 필터·채널·레이어를 삭제하는 방식으로 스파스(sparse) 네트워크를 생성해 연산량과 메모리 사용량을 크게 줄이며, 이후 재학습(fine‑tuning) 단계를 통해 성능 저하를 최소화한다. 이 기술은 모바일 디바이스, 엣지 컴퓨팅, 실시간 추론 등 연산 자원이 제한된 환경에서 모델 경량화와 전력 효율을 높이는 데 널리 활용되며, 자동화된 NAS(Neural Architecture Search)와 결합해 최적화된 하드웨어 가속 설계에도 적용된다. 프루닝은 양자화(Quantization), 지식 증류(Knowledge Distillation)와 같은 다른 모델 압축 방법과 보완적으로 사용되지만, 양자화가 비트 폭을 줄이는 반면 프루닝은 구조 자체를 간소화한다는 점에서 차별화된다." ;
    rdfs:subClassOf llm:ModelCompression .

llm:QA a owl:Class ;
    rdfs:label "QA"@en ;
    llm:description "QA(Question Answering)는 자연어 이해(NLU)의 하위 분야로, 사용자가 제시한 자연어 질문에 대해 정확하고 간결한 답변을 자동으로 생성하는 기술을 말합니다. 최신 QA 시스템은 사전학습된 언어 모델(BERT, GPT 등)과 텍스트 인코딩·임베딩을 활용해 질문과 문서 간의 의미적 유사성을 계산하고, 검색‑증강 생성(RAG)이나 추출 기반 방법으로 정답을 추출·생성합니다. 대표적인 사용 사례로는 검색 엔진의 직접 답변 기능, 고객지원 챗봇의 FAQ 자동 응답, 의료·법률 분야의 전문 지식 베이스 질의응답, 그리고 다중턴 대화형 AI 어시스턴트가 있습니다. QA는 일반적인 정보 검색(IR)과 달리 단순 키워드 매칭이 아니라 컨텍스트 이해와 의미 기반 매칭을 강조하며, 텍스트 요약·대화 시스템·지식 그래프와도 긴밀히 연계됩니다." ;
    rdfs:subClassOf llm:NLU .

llm:RLAIF a owl:Class ;
    rdfs:label "RLAIF"@en ;
    llm:description "RLAIF(​Reinforcement Learning from AI Feedback)는 인간 라벨링 대신 다른 인공지능 모델이 생성한 선호 데이터나 평가 점수를 활용해 대형 언어 모델을 정렬(alignment)시키는 학습 프레임워크이며, “AI‑generated feedback”라는 핵심 키워드와 함께 “reward model”·“policy optimization”을 연계한다는 점에서 정의가 명확합니다. 이 방법은 AI가 만든 비교 질문·답변을 기반으로 보상 함수를 학습하고, PPO와 같은 강화학습 알고리즘으로 목표 모델을 미세조정(fine‑tuning)하는데, 자기비판(self‑critique)·체인‑오브‑생각(chain‑of‑thought) 프롬프트를 반복 적용해 피드백 품질을 점진적으로 향상시키는 작동 원리를 갖습니다. 대표적인 사용 사례로는 챗봇·코드 어시스턴트·콘텐츠 필터링 등에서 안전하고 유용한 응답을 생성하도록 스케일러블하게 정렬하는 것이며, 특히 라벨링 비용을 크게 절감하면서도 대규모 LLM에 적용할 수 있는 장점이 강조됩니다. RLAIF는 인간 피드백을 기반으로 하는 RLHF와 대비되어 “human‑in‑the‑loop” 비용을 낮추지만, 최종 안전성 검증을 위해서는 여전히 인간 평가와의 교차 검증이 필요하다는 점에서 AI‑feedback와 인간‑feedback 사이의 보완 관계를 보여줍니다." ;
    rdfs:subClassOf llm:Alignment .

llm:RLHF a owl:Class ;
    rdfs:label "RLHF"@en ;
    llm:description "RLHF(Reinforcement Learning from Human Feedback)는 인간 피드백을 보상 신호로 활용해 언어 모델이나 에이전트의 정책을 최적화하는 **AI 정렬(alignment)** 기법으로, 인간이 선호하는 답변이나 행동을 데이터셋으로 수집한 뒤 보상 모델(reward model)을 학습시켜 이를 강화학습(RL) 과정에 통합한다. 이 방법은 **프리트레인된 대규모 언어 모델(LLM)**에 **슈퍼바이즈드 파인튜닝(supervised fine‑tuning)**을 먼저 적용하고, 이어서 보상 모델을 기반으로 **프로시저럴 정책 최적화(PPO)** 등 정책 그라디언트 알고리즘을 사용해 인간 선호에 부합하도록 정책을 미세조정한다. 대표적인 사용 사례로는 **ChatGPT, Claude, Gemini**와 같은 대화형 AI의 안전성·유용성 향상, 로봇 제어에서 인간 의도에 맞는 행동 학습, 그리고 콘텐츠 필터링·윤리적 의사결정 시스템 구축 등이 있다. RLHF는 순수 **지도학습(supervised learning)**이나 전통적인 **역강화학습(inverse RL)**과 달리, 명시적인 보상 함수를 인간 선호 데이터로 직접 생성한다는 점에서 **가치 정렬(value alignment)**을 보다 실용적으로 구현한다." ;
    rdfs:subClassOf llm:Alignment .

llm:RMSNorm a owl:Class ;
    rdfs:label "RMSNorm"@en ;
    llm:description """RMSNorm(​Root Mean Square Normalization)은 입력 텐서의 각 차원에 대해 **루트 평균 제곱값**을 계산하고, 이를 이용해 스케일링·시프트 없이 정규화하는 **정규화(Normalization) 기법**이다.  
동작 원리는 먼저 전체 차원에 대해 \\(\\text{RMS}(x)=\\sqrt{\\frac{1}{d}\\sum_{i=1}^{d}x_i^2}\\) 를 구한 뒤, 각 원소를 \\(\\frac{x_i}{\\text{RMS}(x)}\\) 로 나누고 학습 가능한 스칼라 파라미터 \\(\\gamma\\) 를 곱해 최종 출력을 만든다; bias(shift) 파라미터가 없기 때문에 계산량과 메모리 사용이 **LayerNorm**보다 훨씬 가볍다.  
RMSNorm은 **Transformer**, **BERT**, **GPT**와 같은 대규모 언어 모델·비전 모델에서 **잔차 연결(residual connection)**과 결합해 학습 안정성을 높이고, 특히 **GPU/TPU 메모리 제약**이 있는 상황에서 **효율적인 정규화** 수단으로 널리 활용된다.  
관련 개념으로는 **Layer Normalization**, **RMSProp**, **Scale‑only Normalization** 등이 있으며, RMSNorm은 bias를 포함하지 않는 점에서 LayerNorm과 차별화되고, RMS 기반 스케일링을 사용한다는 점에서 RMSProp과 연관성을 가진다.""" ;
    rdfs:subClassOf llm:Normalization .

llm:ROUGE a owl:Class ;
    rdfs:label "ROUGE"@en ;
    llm:description """ROUGE(Recall‑Oriented Understudy for Gisting Evaluation)는 자동 요약·요약 생성·텍스트 요약 시스템의 품질을 정량화하기 위해 고안된 자동 평가 지표(AutomaticMetric)이며, 인간이 만든 참조 요약(reference summary)과 기계가 생성한 요약(candidate summary) 사이의 n‑gram, 연속된 부분 문자열(LCS) 및 스킵‑bigr​am 매칭을 기반으로 점수를 산출한다.  
주요 특징으로는 ROUGE‑N(1‑gram, 2‑gram 등)·ROUGE‑L(최장 공통 부분수열)·ROUGE‑S·ROUGE‑W 등 다양한 변형을 제공해 재현율(recall) 중심의 점수를 기본으로 하면서도 정밀도(precision)와 F1‑score를 함께 계산할 수 있어 요약의 포괄성·정확성을 다각도로 평가한다.  
대표적인 사용 사례는 뉴스 기사 요약, 과학 논문 초록 생성, 대화 요약 및 챗봇 응답 요약 등 자연어 처리(NLP) 분야의 텍스트 요약 모델 성능 비교이며, 기계 번역·텍스트 생성·요약 대회(Kaggle, DUC, TAC)에서도 표준 벤치마크 지표로 널리 활용된다.  
관련 개념으로는 정밀도 중심의 BLEU, 의미 기반의 METEOR·BERTScore와 대비되며, ROUGE는 특히 재현율을 강조함으로써 인간 요약의 핵심 정보를 놓치지 않는지를 평가하는 데 강점을 가진다.""" ;
    rdfs:subClassOf llm:AutomaticMetric .

llm:RateLimit a owl:Class ;
    rdfs:label "RateLimit"@en ;
    llm:description "**RateLimit**은 시스템이나 서비스가 일정 시간 안에 허용되는 요청 수를 제한함으로써 과부하와 악용을 방지하는 **SafetyTechnique**이다. 일반적으로 토큰 버킷(token bucket)이나 리키 버킷(leaky bucket) 알고리즘을 이용해 **burst limit**(짧은 시간에 급증하는 요청)과 **steady‑state quota**(평균 요청량)를 동시에 제어하고, 클라이언트에게 `Retry‑After`·`X‑RateLimit‑Remaining` 같은 헤더를 반환해 **back‑pressure**를 제공한다. 이 기법은 공개 API, 마이크로서비스 간 호출, 클라우드 함수, DDoS 방어 등에서 **API throttling**, **request quota**, **traffic shaping**을 구현해 서비스 가용성을 높이고 비용 초과를 예방한다. 관련 개념으로는 **circuit breaker**, **load shedding**, **quota management** 등이 있으며, **RateLimit**은 단순 인증·인가와 달리 요청 흐름 자체를 제어한다는 점에서 차별화된다." ;
    rdfs:subClassOf llm:SafetyTechnique .

llm:ReAct a owl:Class ;
    rdfs:label "ReAct"@en ;
    llm:description """**ReAct (Reason‑and‑Act) is an advanced prompting paradigm that equips large language models (LLMs) with a tightly coupled reasoning‑and‑acting loop, enabling the model to generate chain‑of‑thought (CoT) explanations while simultaneously selecting and invoking external tools or actions.**  
In practice, a ReAct prompt interleaves “thought” tokens (the model’s internal reasoning) with “action” tokens (calls to APIs such as web search, calculator, or code executor), receives the resulting observation, and feeds it back into the next reasoning step, thus creating a dynamic feedback cycle that grounds abstract reasoning in concrete execution.  
Typical applications include open‑domain question answering with live web retrieval, multi‑step planning assistants that schedule tasks or generate code, and interactive agents that diagnose problems by iteratively querying databases or simulations.  
ReAct builds on and extends related concepts such as Chain‑of‑Thought prompting, Self‑Ask, and Toolformer; unlike pure CoT it performs explicit tool use, and unlike simple tool‑use prompting it retains transparent, step‑by‑step reasoning, offering a more versatile “reason‑act” framework for advanced prompting scenarios.""" ;
    rdfs:subClassOf llm:AdvancedPrompting .

llm:ReLU a owl:Class ;
    rdfs:label "ReLU"@en ;
    llm:description "ReLU (Rectified Linear Unit) 는 신경망에서 사용되는 비선형 활성화 함수로, 입력값이 0 이하이면 0을, 0 초과이면 입력값 자체를 출력하는 \\(f(x)=\\max(0,x)\\) 형태의 piecewise linear 함수이다. 이 함수는 sparsity (출력이 0인 뉴런이 많아지는 현상)와 gradient 전파 시 vanishing gradient 문제를 크게 완화시켜, deep learning 특히 Convolutional Neural Networks(CNN) 와 feed‑forward 아키텍처에서 학습 속도와 성능을 크게 향상시킨다. 대표적인 사용 사례로는 이미지 분류, 객체 검출, 자연어 처리 모델의 hidden layer 에 삽입해 feature extraction 과 representation learning 을 가속화하는 것이며, ResNet, BERT, GAN 등 최신 모델에서도 기본 활성화 함수로 널리 채택된다. ReLU와 대비되는 개념으로는 sigmoid 와 tanh 같은 saturating 활성화 함수가 있으며, leaky ReLU, parametric ReLU(PReLU) 와 같이 negative slope 를 도입해 dying ReLU 현상을 보완한 변형도 존재한다." ;
    rdfs:subClassOf llm:ActivationFunction .

llm:Reasoner a owl:Class ;
    rdfs:label "Reasoner"@en ;
    llm:description """**Reasoner**는 **AgentComponent** 중 하나로, 에이전트가 **지식 기반**(knowledge base)이나 **상태 모델**을 활용해 **논리적 추론(inference)**·**연역(deduction)**·**귀납(induction)**·**연합(abduction)** 등을 수행함으로써 **의사결정(decision‑making)**·**문제 해결(problem solving)**·**계획(plan generation)**에 필요한 **결과를 도출하는 모듈**이다.  
이 컴포넌트는 **규칙 기반(reasoning rules)**, **베이지안 네트워크**, **시맨틱 그래프**, **신경망 기반 추론(neural reasoning)** 등 다양한 **추론 메커니즘**을 결합한 **하이브리드(reasoner‑hybrid) 아키텍처**를 사용해 입력된 관찰(observation)·목표(goal)를 **전략적 행동(action)**이나 **예측(prediction)**으로 변환한다.  
대표적인 사용 사례로는 **자동화된 고객 지원 챗봇**에서 사용자의 질문을 **지식 그래프**와 매핑해 답변을 생성하는 경우, **로보틱스**에서 환경 모델을 기반으로 **경로 계획(path planning)**·**상태 추정(state estimation)**을 수행하는 경우, 그리고 **의료 AI**가 환자 데이터와 임상 가이드라인을 연계해 **진단 추론(diagnostic reasoning)**을 제공하는 경우가 있다.  
관련 개념으로는 **Planner(계획 생성기)**, **Learner(학습 모듈)**, **Perceiver(감지·인식 컴포넌트)** 등이 있으며, Reasoner는 **규칙 기반 추론**에 초점을 맞추는 반면 Planner는 **시퀀스 최적화**에, Learner는 **데이터‑드리븐 모델 업데이트**에 중점을 둔다는 점에서 차별화된다.""" ;
    rdfs:subClassOf llm:AgentComponent .

llm:Recall a owl:Class ;
    rdfs:label "Recall"@en ;
    llm:description "Recall(재현율)은 **자동 평가 지표(AutomaticMetric)** 중 하나로, 전체 정답(양성) 중 모델이 실제로 찾아낸 정답(True Positive)의 비율을 의미하며, “민감도” 혹은 “탐지율”이라고도 불립니다. 수식으로는 TP ÷ (TP + FN) 으로 표현되며, 검색·추천·질문‑응답 등 순위 기반 시스템에서는 **Recall@k** 형태로, 상위 k 개의 벡터 검색 결과에 정답이 몇 개 포함되는지를 측정해 임베딩 기반 **벡터 검색**(FAISS, HNSW 등) 성능을 정량화합니다. 이 지표는 특히 **정보 검색**, **문서 검색**, **추천 시스템**, **다중 클래스 분류** 등에서 “관련 항목을 놓치면 안 되는” 상황에 널리 활용되며, 데이터셋에 존재하는 모든 정답을 가능한 한 많이 회수하는 것이 목표입니다. Recall은 **Precision(정밀도)**와 대비되는 개념으로, 정밀도는 검색된 항목 중 실제 정답 비율을, Recall은 전체 정답 중 회수 비율을 강조하며, 두 지표를 조합한 **F1‑Score**나 **MAP**, **NDCG**와 같은 다른 자동 평가 지표와 함께 시스템 전반의 정확도와 포괄성을 종합적으로 평가합니다." ;
    rdfs:subClassOf llm:AutomaticMetric .

llm:RedTeaming a owl:Class ;
    rdfs:label "RedTeaming"@en ;
    llm:description "Red‑Teaming은 인공지능 시스템이 인간 의도와 가치에 맞게 동작하도록 검증하기 위해, 의도적으로 공격적·비협조적 프롬프트와 시나리오를 제시해 모델의 취약점과 비정렬 행동을 탐색하는 **AlignmentMethod**이다. 이 과정은 “악의적인 사용자” 역할을 하는 레드 팀이 모델에 대해 **adversarial prompting**, **jailbreak attempts**, **prompt injection** 등을 수행하고, 결과를 분석해 **robustness**, **safety**, **fairness** 등 핵심 정렬 지표가 위배되는지를 평가한다. 대표적인 사용 사례로는 대규모 언어 모델 출시 전 **security audit**, 챗봇 서비스의 **policy compliance testing**, 그리고 고위험 분야(예: 의료·법률·금융)에서의 **risk assessment**가 있다. 레드 팀과 대비되는 개념으로는 방어적 검증을 담당하는 **Blue‑Team**(시스템 방어·패치)이나, 사전‑학습 단계에서 **alignment fine‑tuning**, **RLHF (Reinforcement Learning from Human Feedback)**와 같은 **alignment‑oriented training** 기법이 있다. 이러한 다양한 키워드와 표현은 벡터 검색 시 “adversarial testing”, “AI safety evaluation”, “model robustness”, “prompt injection mitigation” 등과 연계되어 높은 검색 가시성을 제공한다." ;
    rdfs:subClassOf llm:AlignmentMethod .

llm:Reflexion a owl:Class ;
    rdfs:label "Reflexion"@en ;
    llm:description "Reflexion은 LLM이 스스로 생성한 출력에 대해 **자기 성찰(self‑reflection)**을 수행하고, 그 결과를 바탕으로 답변을 재작성하도록 유도하는 **Advanced Prompting** 기법이다. 이 방식은 “답변 → 자기 평가 → 수정 답변”이라는 **반복적(iterative) 프롬프팅 루프**를 사용하며, 메타‑프롬프트(meta‑prompt)로 모델에게 **추론 과정, 신뢰도, 누락된 정보** 등을 점검하도록 지시해 **오류 정정(error correction)**과 **추론 정제(reasoning refinement)**를 수행한다. 복잡한 논리 문제, 다중 턴 대화, 코드 생성, 사실 기반 QA 등에서 **일관성 향상, 환각 감소, 품질 보강**을 목표로 널리 활용되며, 검색‑증강 생성(RAG)이나 도구 사용과 결합해 더욱 정교한 결과를 만든다. 관련 개념으로는 **체인‑오브‑쓰(thought prompting)**, **셀프‑컨시스턴시(self‑consistency)**, **셀프‑크리틱(self‑critique)** 등이 있으며, 단일 샷 프롬프팅과 달리 **메타‑프롬프트와 자기 평가 루프**를 강조한다." ;
    rdfs:subClassOf llm:AdvancedPrompting .

llm:ReflexiveAgent a owl:Class ;
    rdfs:label "ReflexiveAgent"@en ;
    llm:description "ReflexiveAgent(반사형 에이전트)는 환경으로부터 받은 **감각 자극**에 즉시 **조건‑행동 규칙**(stimulus‑response rule)을 적용해 행동을 선택하는 **단순한 에이전트 아키텍처**이며, 내부 상태나 미래 예측을 유지하지 않는 **반응형(reactive) 에이전트**의 한 형태이다. 이러한 에이전트는 **센서‑액추에이터 루프**(sensor‑actuator loop)를 기반으로 동작하며, 입력 자극에 대한 **조건부 매핑**(if‑then rule)만을 저장하고 실행하기 때문에 **실시간 처리**와 **낮은 연산 비용**이 특징이다. 대표적인 사용 사례로는 **로봇 공학**에서 장애물 회피, **임베디드 시스템**의 간단한 제어, **비디오 게임** AI의 적 캐릭터 행동, 그리고 **산업 자동화**에서의 빠른 이벤트 반응 등이 있다. 반면 **DeliberativeAgent**(심사숙고형 에이전트)나 **Model‑BasedAgent**(모델 기반 에이전트)와 달리, ReflexiveAgent는 **내부 세계 모델**이나 **목표‑계획 메커니즘**을 갖지 않아 복잡한 의사결정이 요구되는 상황에서는 한계가 있다." ;
    rdfs:subClassOf llm:AgentArchitecture .

llm:RelativePositional a owl:Class ;
    rdfs:label "RelativePositional"@en ;
    llm:description "RelativePositional(상대 위치 인코딩)은 시퀀스 내 토큰 간의 거리 정보를 절대 좌표가 아닌 상대적인 차이값으로 표현하여, self‑attention 메커니즘이 입력 순서에 대한 유연한 인식을 가능하게 하는 PositionalEncoding의 한 형태이다. 이는 각 쿼리‑키 쌍에 거리‑의존적인 bias 혹은 회전 행렬(RoPE) 등을 추가해 attention score를 조정하고, 길이‑불변성, 선형 시간 복잡도, 장거리 의존성 캡처와 같은 특징을 제공한다. 대표적인 적용 사례로는 Transformer‑XL, T5, GPT‑NeoX와 같은 대규모 언어 모델, 기계 번역, 음성 인식, 시계열 예측, 그리고 그래프 신경망에서의 순서‑민감 메시지 전달 등이 있다. 관련 개념으로는 절대 위치 인코딩(Absolute Positional Encoding), ALiBi(Linear Bias), 그리고 위치‑감지 컨볼루션이 있으며, 상대 위치 인코딩은 토큰 간 거리 정보를 직접 학습한다는 점에서 절대 인코딩과 대비된다." ;
    rdfs:subClassOf llm:PositionalEncoding .

llm:Retriever a owl:Class ;
    rdfs:label "Retriever"@en ;
    llm:description "Retriever는 에이전트(Agent) 내부에서 동작하는 핵심 **AgentComponent** 중 하나로, 사용자의 질의나 내부 목표에 맞는 관련 문서·지식 조각을 **정보 검색(information retrieval)** 혹은 **시맨틱 검색(semantic search)** 방식으로 찾아내는 모듈이다. 일반적으로 **쿼리 인코더(query encoder)**와 **문서 인코더(document encoder)**를 이용해 텍스트를 **임베딩(embedding)**으로 변환하고, **FAISS**, **HNSW**, **IVF‑PQ**와 같은 **ANN(Approximate Nearest Neighbor)** 인덱스 혹은 **BM25**와 같은 전통적 역색인에 저장된 **벡터 스토어(vector store)**에서 가장 가까운 **밀집 벡터(dense vector)** 혹은 **희소 벡터(sparse vector)**를 빠르게 매칭한다. 대표적인 사용 사례로는 **RAG(Retrieval‑Augmented Generation)** 파이프라인에서 LLM이 답변을 생성하기 전에 최신 문서·데이터베이스·위키피디아 항목을 가져오는 작업, 기업 내부 지식베이스 검색, 챗봇의 컨텍스트 보강, 그리고 검색 기반 추천 시스템 등이 있다. Retriever는 **Generator**(생성기)와 대비되어 “찾아오기” 역할을 담당하며, 필요에 따라 **Ranker**(재정렬기)·**Filter**(필터링)와 결합해 정확도·다양성을 조절하는 점이 다른 검색 컴포넌트와 구별되는 특징이다." ;
    rdfs:subClassOf llm:AgentComponent .

llm:RewardModel a owl:Class ;
    rdfs:label "RewardModel"@en ;
    llm:description "RewardModel은 에이전트가 수행한 행동에 대해 스칼라 형태의 보상 점수를 예측·생성하는 **AgentComponent**이며, 강화학습(RL)이나 인간 피드백 기반 학습(RLHF)에서 정책(Policy) 업데이트를 위한 핵심 학습 신호를 제공한다. 이 모델은 대규모 텍스트·이미지·시뮬레이션 데이터와 인간 라벨링(예: 선호도, 순위)으로 학습된 **보상 함수**(reward function)를 기반으로, 입력 상태·행동 쌍을 임베딩한 뒤 **스코어링 네트워크**가 보상 값을 출력하도록 작동하며, 종종 **contrastive learning**, **pairwise ranking**, **KL‑divergence** 등으로 정규화한다. 대표적인 사용 사례로는 대화형 AI의 답변 품질 향상, 로봇 제어에서 목표 달성도 평가, 게임 AI에서 승률 최적화, 그리고 생성 모델의 안전성·유용성 검증 등 다양한 **강화학습**·**RLHF** 파이프라인에 적용된다. 관련 개념으로는 행동을 직접 평가하는 **ValueModel(critic)**, 정책을 생성하는 **PolicyModel**, 그리고 명시적인 규칙 기반 **Reward Function**이 있으며, RewardModel은 주로 **학습 가능한 보상 추정**에 초점을 맞추어 정적인 규칙과는 대비된다." ;
    rdfs:subClassOf llm:AgentComponent .

llm:RoBERTa a owl:Class ;
    rdfs:label "RoBERTa"@en ;
    llm:description """RoBERTa(Robustly Optimized BERT Pretraining Approach)는 Transformer Encoder 구조만을 사용한 사전학습 언어 모델로, 기존 BERT의 마스크드 언어 모델링(Masked Language Modeling) 목표를 그대로 유지하면서 학습 데이터 양·배치 크기·학습 스케줄을 대폭 확대하고 동적 마스킹(dynamic masking)을 도입해 표현력을 강화한 모델이다.  
이 모델은 대규모 코퍼스(예: Common Crawl, BooksCorpus 등)와 긴 학습 단계(수백만 배치)를 활용해 토큰 간의 양방향(contextual) 관계를 깊이 있게 학습하며, 토큰 임베딩, 포지션 임베딩, 레이어 정규화 등을 통해 고차원 벡터 공간에 의미론적 정보를 압축한다.  
주요 활용 사례로는 텍스트 분류, 감성 분석, 질의응답, 문장 유사도 측정 및 검색 엔진에서의 임베딩 생성 등 다양한 자연어 처리(NLP) 작업에 파인튜닝(fine‑tuning)하거나 직접 벡터 검색(vector search)용 문장 임베딩을 추출하는 데 사용된다.  
관련 개념으로는 동일한 Encoder‑Only 아키텍처를 갖는 BERT, ALBERT, ELECTRA 등이 있으며, RoBERTa는 특히 더 큰 데이터와 학습 스케줄을 적용한 점에서 BERT와 차별화되고, Decoder‑Only 구조의 GPT와는 양방향 컨텍스트 처리 방식에서 근본적으로 구분된다.""" ;
    rdfs:subClassOf llm:EncoderOnly .

llm:RoPE a owl:Class ;
    rdfs:label "RoPE"@en ;
    llm:description """**RoPE(Rotary Positional Encoding)**는 트랜스포머의 **self‑attention** 메커니즘에 회전 연산을 적용해 **절대 위치**와 **상대 위치** 정보를 동시에 인코딩하는 기법으로, 입력 토큰의 임베딩을 복소수 평면에서 각도 θ(=position·frequency)만큼 회전시켜 **쿼리·키** 벡터에 직접 주입한다.  
이 방식은 **sinusoidal positional encoding**이나 **learned absolute embeddings**와 달리, 쿼리·키의 내적에 회전 행렬이 곱해지면서 자연스럽게 **거리‑의존적인 attention score**를 생성하므로, 긴 시퀀스에 대한 **extrapolation**과 **다중‑스케일** 관계를 효율적으로 모델링한다(연산 복잡도는 O(1)·벡터화 가능).  
주요 사용 사례는 **대형 언어 모델(Large Language Model)**·**인코더‑디코더** 구조에서의 텍스트 생성, 코드 완성, 검색‑증강 생성(RAG) 등이며, 특히 **LLM 기반 검색 엔진**이나 **벡터 검색** 시스템에서 토큰 간 상대적 거리 정보를 보존해 **정확한 문맥 매칭**과 **스칼라·벡터 연산 최적화**에 기여한다.  
관련 개념으로는 **ALiBi(Attention with Linear Biases)**, **relative bias**, **learned positional embeddings**, 그리고 전통적인 **sinusoidal PE**가 있으며, RoPE는 회전 기반의 **복소수·실수 변환**을 이용한다는 점에서 이들 방식과 차별화된다.""" ;
    rdfs:subClassOf llm:PositionalEncoding .

llm:SafetyAlignment a owl:Class ;
    rdfs:label "SafetyAlignment"@en ;
    llm:description "**SafetyAlignment**는 인공지능 시스템이 인간의 안전과 윤리적 가치를 지속적으로 보존하도록 설계·조정하는 **AlignmentMethod**의 한 형태로, 위험을 최소화하고 해로운 행동을 사전에 차단하는 것을 목표로 합니다. 이 방법은 **인간 피드백 기반 강화학습(RLHF)**, **규칙 기반 필터링**, **시뮬레이션 기반 위험 평가** 등 다양한 메커니즘을 결합해 모델의 출력이 사전 정의된 **안전 가이드라인**과 **윤리적 기준**에 부합하도록 **동적으로 조정**합니다. 대표적인 적용 사례로는 **대형 언어 모델 챗봇**, **자율 주행 차량**, **의료 진단 AI**, **산업용 로봇** 등에서 **위험 감지·회피**와 **사용자 보호** 기능을 구현하는 데 활용됩니다. 관련 개념으로는 **Value Alignment**(가치 정렬)와 **Robustness**(견고성) 등이 있으며, **Performance Alignment**(성능 정렬)와는 달리 **안전성**을 최우선 목표로 삼는 점이 핵심적인 차별점입니다." ;
    rdfs:subClassOf llm:AlignmentMethod .

llm:SafetyGuard a owl:Class ;
    rdfs:label "SafetyGuard"@en ;
    llm:description "**SafetyGuard**는 **AgentComponent** 계층에 속하는 안전 관리 모듈로, 자율 에이전트가 실행 중에 발생할 수 있는 위험 행동이나 정책 위반을 실시간으로 감지하고 차단하는 역할을 수행합니다. 이 컴포넌트는 사전 정의된 규칙 엔진, 행동 제한 프로파일, 그리고 동적 위험 평가 알고리즘을 결합해 입력·출력 스트림을 모니터링하고, 위협이 감지되면 즉시 페일‑세이프(Fail‑Safe) 모드로 전환하거나 대체 행동을 제시합니다. 대표적인 사용 사례로는 LLM 기반 챗봇에서 부적절한 콘텐츠 생성 방지, 로보틱스 시스템에서 물리적 충돌 방지, 그리고 금융 트레이딩 에이전트에서 규제 위반 거래 차단 등이 있으며, 이러한 적용 분야에서 **SafetyGuard**는 신뢰성·윤리성을 보장하는 핵심 방어선으로 활용됩니다. 관련 개념으로는 **TrustLayer**(신뢰 평가)와 **PerformanceOptimizer**(성능 최적화) 등이 있으며, **SafetyGuard**는 주로 위험 최소화와 정책 준수에 초점을 맞추는 반면 **PerformanceOptimizer**는 효율·속도 향상에 중점을 두는 차이점이 있습니다." ;
    rdfs:subClassOf llm:AgentComponent .

llm:Sandboxing a owl:Class ;
    rdfs:label "Sandboxing"@en ;
    llm:description "**Sandboxing**은 애플리케이션이나 코드를 격리된 가상 환경(샌드박스) 안에서 실행시켜, 시스템 자원과 네트워크 접근을 제한함으로써 악성 행위가 실제 운영 체제에 미치는 영향을 최소화하는 **SafetyTechnique**이다. 이 기술은 가상 머신, 컨테이너, 혹은 사용자 모드 프로세스와 같은 경량 가상화 메커니즘을 활용해 파일 시스템, 레지스트리, 시스템 콜 등을 가상화·필터링하고, 권한 최소화와 네트워크 차단 정책을 적용해 실행 중인 코드가 외부와 직접 상호작용하지 못하도록 설계된다. 대표적인 사용 사례로는 웹 브라우저의 악성 스크립트 격리, 모바일 앱 스토어의 앱 검증, 클라우드 멀티테넌시 환경에서의 고객 워크로드 보호, 그리고 보안 연구소에서의 악성코드 분석 및 침투 테스트가 있다. 관련 개념으로는 전통적인 **가상화**·**컨테이너화**, **코드 서명**·**정적 분석**과 대비되며, 샌드박스는 실행 시점에 동적 격리를 제공한다는 점에서 차별화된다." ;
    rdfs:subClassOf llm:SafetyTechnique .

llm:SelfAsk a owl:Class ;
    rdfs:label "SelfAsk"@en ;
    llm:description "**SelfAsk**는 질문에 답하기 위해 모델이 스스로 “이 질문을 어떻게 풀어야 할까?”라는 하위 질문을 생성하고, 그 답을 바탕으로 최종 응답을 도출하는 **Advanced Prompting** 기법 중 하나입니다. 이 방식은 프롬프트에 “Self‑Ask: …?”와 같은 메타 질문을 삽입해 모델이 단계별 추론(다단계 추론, chain‑of‑thought) 과정을 자동으로 수행하도록 유도하며, 각 단계의 답변을 메모리(또는 컨텍스트) 안에 저장해 순차적으로 연결합니다. 대표적인 사용 사례로는 복잡한 수학 문제 풀이, 다중 엔터티 관계 추론, 법률·의학 문서에서의 증거 기반 질의응답 등에 활용되며, 특히 제한된 토큰 길이 안에서 정확도를 높이고자 할 때 효과적입니다. SelfAsk은 “Chain‑of‑Thought”, “Tree‑of‑Thought”, “Self‑Consistency”와 같은 다른 프롬프트 엔지니어링 기법과 대비되는데, 전자는 전체 추론 과정을 한 번에 제시하도록 요구하는 반면, SelfAsk은 모델이 스스로 질문을 분해하고 답을 재조합하는 **자기 질문(self‑questioning)** 메커니즘을 강조합니다." ;
    rdfs:subClassOf llm:AdvancedPrompting .

llm:SelfConsistency a owl:Class ;
    rdfs:label "SelfConsistency"@en ;
    llm:description "**Self‑Consistency**는 대규모 언어 모델(LLM)에게 동일한 질문을 여러 번 다양한 온도(temperature)와 샘플링 전략(예: top‑k, nucleus sampling)으로 생성하게 한 뒤, 각 응답에 포함된 체인‑오브‑생각(Chain‑of‑Thought) 흐름을 집계해 가장 빈번히 등장하는 최종 답을 선택하는 고급 프롬프트 기법이다. 이 방법은 다중 샘플링 → 답변 정렬 → 다수결(majority‑vote) 혹은 빈도 기반 재‑랭킹(re‑ranking)이라는 작동 원리를 통해 단일 샘플 디코딩(single‑sample decoding)에서 발생하기 쉬운 환각(hallucination)이나 일관성 결여를 크게 완화한다. 대표적인 사용 사례로는 수학 문제 풀이, 복합 추론 질의응답, 코드 생성, 의사결정 시나리오 등 고난이도 추론이 요구되는 Zero‑Shot 및 Few‑Shot 프롬프트 환경에서 모델의 정확도와 신뢰성을 향상시키는 데 활용된다. 관련 개념으로는 Self‑Critique, Self‑Reflection, 및 Ensemble 프롬프트가 있으며, Self‑Consistency는 다중 출력 집계에 초점을 맞춘 반면 Self‑Critique는 출력 자체를 스스로 검증·수정하는 방식으로 차별화된다." ;
    rdfs:subClassOf llm:AdvancedPrompting .

llm:SelfPlay a owl:Class ;
    rdfs:label "SelfPlay"@en ;
    llm:description "Self‑Play는 에이전트가 스스로 생성한 환경(주로 동일하거나 유사한 복제 에이전트)과 반복적으로 대결하면서 정책을 개선해 나가는 강화학습(RL) 기법으로, “자기 대결 학습” 혹은 “자기 대전”이라고도 불립니다. 이 방식은 에이전트가 스스로 상대를 만들고, 승패 피드백을 통해 탐색‑활용(Exploration‑Exploitation) 균형을 맞추며, 정책 반복(Policy Iteration)과 가치 함수 업데이트를 동시에 수행해 점진적으로 최적 전략을 학습합니다. 대표적인 사용 사례로는 바둑·체스·바이오틱스·비디오 게임 등 제로섬(zero‑sum) 게임에서 AlphaGo·AlphaZero·OpenAI Five와 같은 초인적인 게임 AI 개발, 로봇 제어와 시뮬레이션 기반 전략 최적화, 그리고 멀티에이전트 환경에서의 경쟁·협력 학습이 있습니다. Self‑Play는 다중에이전트 강화학습(Multi‑Agent RL), 대립 학습(Adversarial Training), 커리큘럼 학습(Curriculum Learning) 등과 연관되며, 지도학습(Supervised Learning)이나 오프라인 RL과 달리 외부 라벨 없이 자체적인 보상 신호와 상대 전략을 통해 지속적인 자기 개선이 가능한 점이 차별적입니다." ;
    rdfs:subClassOf llm:ReinforcementLearning .

llm:SelfRefine a owl:Class ;
    rdfs:label "SelfRefine"@en ;
    llm:description "**SelfRefine**는 LLM에게 동일한 질문이나 과제에 대해 **자기 피드백 루프**를 적용해 스스로 답변을 검토·수정하도록 유도하는 **AdvancedPrompting** 기법이다. 사용자는 초기 프롬프트와 “답변을 검증하고 개선하라”는 메타‑프롬프트를 함께 제공하면, 모델은 **Iterative Prompting** 과정을 통해 생성된 텍스트를 자체 평가하고, 오류·불일치를 식별한 뒤 **Self‑Refinement** 단계에서 수정된 버전을 재출력한다. 이 방식은 **자동 검증**, **다중 샘플 비교**, **자기 일관성 강화** 등을 핵심 특징으로 하며, 특히 **콘텐츠 생성**, **코드 디버깅**, **복합 질의 응답** 및 **교육용 튜터링** 등에서 높은 품질의 결과물을 얻는 데 활용된다. 관련 개념으로는 **Chain‑of‑Thought**이나 **Self‑Consistency**가 있으며, SelfRefine은 단순히 사고 과정을 나열하는 것이 아니라 **피드백‑수정 사이클**을 강조해 보다 **자율적인 프롬프트 최적화**를 가능하게 한다." ;
    rdfs:subClassOf llm:AdvancedPrompting .

llm:SemanticMemory a owl:Class ;
    rdfs:label "SemanticMemory"@en ;
    llm:description "SemanticMemory는 인간의 장기 기억 체계 중에서 의미적·개념적 정보를 구조화된 형태로 저장하고, 상황에 구애받지 않고 일반화된 지식을 재생산할 수 있게 하는 **지식 통합(Knowledge Integration)**의 핵심 구성 요소이다. 이 메모리는 개념 간 관계를 **시맨틱 네트워크**나 **지식 그래프** 형태로 인코딩하고, **임베딩(embedding)**·**잠재 공간(latent space)**에 매핑함으로써 **시맨틱 유사도(semantic similarity)** 기반의 **벡터 검색(vector search)** 및 **컨텍스트 기반 검색(contextual retrieval)**을 가능하게 한다. 대표적인 활용 사례로는 **검색 강화 생성(RAG)** 시스템에서 외부 문서를 빠르게 연결해 답변을 생성하는 작업, **지식 기반 챗봇**이 사용자 의도와 연관된 개념을 추론하는 과정, 그리고 **지능형 추천 엔진**이 제품·서비스 간 의미적 연관성을 파악해 맞춤형 제안을 제공하는 경우가 있다. 관련 개념으로는 **에피소드 메모리(Episodic Memory)**와 구분되는 **사실 기반 기억(factual memory)**, **작업 기억(working memory)**와 차별화되는 **장기 기억(long‑term memory)**, 그리고 **시맨틱 검색(semantic search)**·**지식 베이스(knowledge base)**와의 연계가 주요 특징이다." ;
    rdfs:subClassOf llm:KnowledgeIntegration .

llm:SensitiveToolBlocking a owl:Class ;
    rdfs:label "SensitiveToolBlocking"@en ;
    llm:description "**SensitiveToolBlocking**은 대형 언어 모델(Large Language Model)이나 자동화된 에이전트가 사용자에게 위험하거나 부적절한 결과를 초래할 수 있는 고위험 도구(예: 파일 시스템 조작, 네트워크 호출, 코드 실행, 데이터베이스 접근 등)를 직접 호출하거나 활용하는 것을 사전에 차단하는 **SafetyTechnique**이다. 이 기법은 입력 프롬프트와 모델 출력에 대한 실시간 검증 로직을 적용해, 민감한 API 키, 쉘 명령, 시스템 경로와 같은 **sensitive tool signatures**를 탐지하고, 해당 요청이 감지되면 즉시 거부하거나 대체 안전한 흐름으로 전환한다. 대표적인 사용 사례로는 챗봇에서 파일 업로드·다운로드 기능을 제한하거나, 코드 생성 모델이 실제 서버에 배포되는 스크립트를 실행하지 못하도록 하는 상황, 그리고 기업 내부 AI 어시스턴트가 사내 인프라에 무단 접근하는 것을 방지하는 보안 레이어가 있다. 관련 개념으로는 **PromptFiltering**, **OutputSanitization**, **ToolUseAuthorization** 등이 있으며, SensitiveToolBlocking은 단순 키워드 차단을 넘어 컨텍스트‑aware 분석과 정책‑driven 허가 메커니즘을 결합해, 허용된 도구는 안전하게 사용하면서 위험 도구는 철저히 차단한다는 점에서 차별화된다." ;
    rdfs:subClassOf llm:SafetyTechnique .

llm:SentencePiece a owl:Class ;
    rdfs:label "SentencePiece"@en ;
    llm:description "SentencePiece는 구글이 개발한 **언어‑독립적인 서브워드 토크나이저**로, 원시 텍스트를 사전 정의된 규칙 없이 **비지도 학습**을 통해 **BPE(바이트‑페어 인코딩)** 혹은 **Unigram Language Model** 기반의 **서브워드 단위**로 분할하고 고정된 **어휘(vocabulary)**를 생성한다. 토크나이저는 텍스트를 **공백이나 특수 문자에 의존하지 않는** 방식으로 전처리하며, **Byte‑Level** 혹은 **Unicode‑Normalization**을 적용해 다국어·다스크립트 환경에서도 일관된 토큰화를 제공한다. 이 방식은 **신경 기계 번역(NMT)**, **대형 사전학습 언어 모델(BERT, GPT 등)**, **음성‑텍스트 정렬** 및 **검색 엔진 인덱싱** 등에서 **텍스트 → 토큰 → 임베딩** 파이프라인의 품질을 크게 향상시킨다. SentencePiece는 **WordPiece**, **Moses Tokenizer**, **character‑level tokenization** 등과 대비되어, 사전 정의된 어절 경계가 없는 **언어‑비특정** 특성을 갖고 있어 **멀티링궐** 혹은 **희소 언어** 처리에 특히 유리하다." ;
    rdfs:subClassOf llm:Tokenization .

llm:SentimentAnalysis a owl:Class ;
    rdfs:label "SentimentAnalysis"@en ;
    llm:description "SentimentAnalysis(감정 분석)는 텍스트 데이터에서 사용자의 감정 상태를 **긍정, 부정, 중립** 등으로 자동 분류하거나 감정 점수를 추정하는 NLU(자연어 이해) 하위 기술이다. 주로 딥러닝 기반의 BERT, LSTM 같은 사전 학습 모델이나 감정 사전 기반 규칙 엔진을 활용해 **단어 임베딩, 문맥 정보, 감정 강도**를 추출하고, 다중 클래스 혹은 이진 분류 방식으로 감정을 예측한다. 이 기술은 **소셜 미디어 모니터링, 고객 리뷰 분석, 콜센터 대화 감정 추출, 챗봇 응답 조정, 맞춤형 추천 시스템** 등에서 실시간 피드백을 제공하거나 마케팅 전략을 최적화하는 데 널리 사용된다. SentimentAnalysis는 텍스트 분류와 감정 추출에 초점을 맞추는 반면, **Intent Detection(의도 파악)**이나 **Entity Recognition(개체명 인식)**은 사용자의 목표나 구체적인 정보 추출에 중점을 두는 점에서 차별화된다." ;
    rdfs:subClassOf llm:NLU .

llm:SoftwareEngineering a owl:Class ;
    rdfs:label "SoftwareEngineering"@en ;
    llm:description "소프트웨어 엔지니어링(Software Engineering)은 복잡한 소프트웨어 시스템을 **요구사항 분석, 설계, 구현, 테스트, 배포, 유지보수**의 전 단계에 걸쳐 체계적으로 관리하고 최적화하는 학문이자 실천 분야이며, 이는 에이전시 작업(AgenticTask) 맥락에서 자동화 에이전트가 주도하는 개발 파이프라인을 설계·운영하는 핵심 활동으로 간주됩니다. 주요 특징으로는 **모듈화, 재사용성, 품질 보증, CI/CD 파이프라인, 버전 관리, Agile·DevOps 프로세스** 등을 적용해 코드 품질과 생산성을 지속적으로 향상시키는 점이며, 인공지능 기반 코드 생성·리팩터링 도구와 같은 에이전트가 스스로 작업을 할당·조정하는 메커니즘이 포함됩니다. 대표적인 사용 사례는 대규모 웹 서비스 개발, 임베디드 시스템 설계, 클라우드 네이티브 마이크로서비스 구축 등에서 자동화된 테스트 스위트와 지속적 배포(continuous deployment)를 통해 빠른 릴리즈 사이클을 구현하는 것이며, 특히 **AI‑코드 어시스턴트, 자동화된 빌드·배포 에이전트**가 활약하는 환경에서 두드러집니다. 관련 개념으로는 전통적인 **시스템 엔지니어링**, **컴퓨터 과학**의 이론적 기반, 그리고 **프로그래밍** 자체가 있으며, 소프트웨어 엔지니어링은 이들보다 **프로세스 중심·협업·자동화**에 초점을 맞추어 에이전시 작업과 차별화됩니다." ;
    rdfs:subClassOf llm:AgenticTask .

llm:SparseAttention a owl:Class ;
    rdfs:label "SparseAttention"@en ;
    llm:description "SparseAttention(희소 어텐션)은 전체 토큰 쌍을 모두 연산하는 전통적인 dense attention(전역 어텐션) 대신, 선택된 일부 토큰 간에만 가중치를 계산해 **연산 복잡도와 메모리 사용량을 선형 혹은 준선형으로 축소**하는 효율적인 attention 메커니즘이다. 일반적으로 블록‑스파스(block‑sparse), 로컬‑윈도우(local window), 혹은 토큰‑프루닝(token pruning)과 같은 **희소 패턴**을 사전에 정의하거나 동적으로 학습하여, 각 레이어에서 중요한 토큰들만 연결하고 나머지는 무시함으로써 **quadratic 비용을 크게 감소**시킨다. 이 방식은 초대형 언어 모델(Large Language Model), 장기 시퀀스 처리(Long‑Sequence Modeling), 단백질 구조 예측, 비전 트랜스포머(ViT) 등 **수십만~수백만 토큰을 다루는 응용 분야**에서 GPU 메모리 절약과 추론 속도 향상을 위해 널리 활용된다. SparseAttention은 dense self‑attention과 대비되어, **low‑rank 근사, kernelized attention, 라이트‑어텐션(Lightweight Attention)** 등과 함께 **효율성 vs. 표현력** 트레이드오프를 조절하는 핵심 기술로 자리 잡고 있다." ;
    rdfs:subClassOf llm:AttentionMechanism .

llm:SparseRetrieval a owl:Class ;
    rdfs:label "SparseRetrieval"@en ;
    llm:description "Sparse Retrieval(희소 검색)은 텍스트를 고차원 희소 벡터(예: TF‑IDF, BM25 가중치)로 변환하고 역인덱스(inverted index)를 활용해 질의와 문서 간의 **lexical similarity**를 빠르게 매칭하는 검색 기법을 말합니다. 이 방식은 토큰‑레벨 가중치를 그대로 보존하므로 키워드 매칭이 정확하고, 인덱스 구조가 압축 가능해 대규모 코퍼스에서도 실시간 검색 성능을 유지할 수 있습니다. RAG(Retrieval‑Augmented Generation) 파이프라인에서는 오픈‑도메인 질문‑응답, 문서 요약, 지식 기반 챗봇 등 지식‑집약형 작업에 희소 검색을 앞단에 두어 관련 문서를 빠르게 추출하고, 이후 밀집 (embedding‑based) 벡터 검색이나 LLM 생성 모델과 결합해 정확도를 높이는 **하이브리드 검색** 전략이 널리 활용됩니다. 관련 개념으로는 밀집 검색(dense retrieval), 하이브리드 검색, 인코더‑디코더 기반 RAG 모델 등이 있으며, 희소 검색은 키워드 중심 정확도와 인덱스 효율성에서 밀집 검색에 비해 높은 재현율을 제공하지만, 의미적 유사성을 포착하는 능력은 보완적으로 밀집 임베딩과 결합될 때 최적의 성능을 발휘합니다." ;
    rdfs:subClassOf llm:RAG .

llm:SpeculativeDecoding a owl:Class ;
    rdfs:label "SpeculativeDecoding"@en ;
    llm:description "**Speculative Decoding**은 경량 ‘draft model’이 여러 후보 토큰을 동시에 생성하고, 그 후보들을 전체 LLM ‘verification model’이 검증함으로써 토큰 당 지연시간을 대폭 줄이는 **Efficient Inference** 기법이다. 이 방식은 draft model이 예측한 시퀀스를 먼저 출력하고, 전체 모델이 그 예측을 빠르게 확인·수정 (accept or reject) 함으로써 불필요한 재계산을 피하고 병렬 토큰 생성을 가능하게 한다. 실시간 챗봇, 코드 자동완성, 모바일 또는 엣지 디바이스에서 실시간 텍스트 생성이 필요한 시나리오에 널리 활용되며, TensorRT, ONNX Runtime, Hugging Face Transformers 등의 프레임워크와 통합돼 고속 추론 파이프라인을 구현한다. 기존의 greedy 또는 beam search와는 달리 ‘parallel decoding’, ‘early‑exit’, ‘model distillation’, ‘token‑level caching’ 과 같은 다른 효율적 추론 기술과 연관되면서도 예측 과정 그 자체를 ‘speculative’ 하게 전략화한다는 점에서 차별화된다." ;
    rdfs:subClassOf llm:EfficientInference .

llm:StepCount a owl:Class ;
    rdfs:label "StepCount"@en ;
    llm:description "**StepCount**는 자동 메트릭 (AutomaticMetric) 체계에서 모델 학습이나 데이터 파이프라인이 진행된 **단계(step)·에포크(epoch)·iteration**의 총수를 정량적으로 기록하는 지표이며, “학습 단계 수” 혹은 “진행 단계 카운트”라고도 불립니다. 이 메트릭은 학습 루프가 시작될 때마다 자동으로 증가하도록 구현되어, **실시간 모니터링**, **리소스 사용량 추적**, **학습 효율성 평가** 등에 활용되며, 로그 파일이나 메트릭 서버에 실시간 스트리밍되는 형태로 저장됩니다. 대표적인 사용 사례로는 대규모 딥러닝 모델의 **훈련 진행도 시각화**, **조기 종료(Early Stopping) 조건 설정**, **멀티태스크 파이프라인에서 단계별 성능 비교** 등이 있으며, 특히 **클라우드 기반 학습 클러스터**나 **자동화된 MLOps 파이프라인**에서 핵심적인 역할을 합니다. 관련 개념으로는 손실(Loss), 정확도(Accuracy)와 같은 **성능 평가 메트릭**이 있으며, StepCount는 **정량적 진행도**를 나타내는 반면 손실·정확도는 **품질**을 측정한다는 점에서 대비됩니다." ;
    rdfs:subClassOf llm:AutomaticMetric .

llm:SuccessRate a owl:Class ;
    rdfs:label "SuccessRate"@en ;
    llm:description "**SuccessRate**는 자동 평가 지표(AutomaticMetric) 중 하나로, 전체 시도 횟수에 대비해 성공적으로 수행된 경우의 비율을 수치화한 **성공률**을 의미합니다. 이 지표는 모델이 예측·생성·검색 등에서 정답(또는 목표 조건)을 만족했는지를 **이진 판단**한 뒤, `성공 횟수 ÷ 전체 시도 횟수` 로 계산되며, **정확도(Accuracy)**와 유사하지만 특정 임계값을 초과하는 경우만을 성공으로 간주한다는 점에서 차별화됩니다. 대표적인 사용 사례로는 챗봇 응답 성공률, 검색 시스템의 정답 반환 비율, 자동 번역에서 목표 언어와 일치하는 문장 비율, 로봇 제어 시 목표 위치 도달 성공률 등을 **모델 성능 비교**와 **실시간 모니터링**에 활용합니다. 관련 개념으로는 **Precision, Recall, F1‑Score**와 같은 정밀·재현 지표가 있으며, SuccessRate는 **전체 성공 여부**에 초점을 맞추어 **단순하고 직관적인** 평가를 제공한다는 점에서 대비됩니다." ;
    rdfs:subClassOf llm:AutomaticMetric .

llm:Summarization a owl:Class ;
    rdfs:label "Summarization"@en ;
    llm:description "Summarization은 방대한 텍스트를 핵심 의미만 추출하거나 재구성하여 짧은 형태로 압축하는 NLG(Natural Language Generation) 기술로, 원문을 이해하고 중요한 정보를 선별하거나 새로운 문장을 생성하는 과정을 포함합니다. 주요 특징으로는 추출형(Extractive) 요약이 문장에서 핵심 문장을 직접 선택하는 반면, 생성형(Abstractive) 요약은 Transformer와 Attention 메커니즘을 활용해 원문과는 다른 표현으로 의미를 재작성한다는 점이며, 최근에는 사전학습된 대형 언어 모델(LLM)을 기반으로 한 프롬프트 엔지니어링이 널리 적용됩니다. 대표적인 사용 사례는 뉴스 헤드라인 자동 생성, 논문 초록 작성, 고객 리뷰 요약, 법률 문서 및 의료 기록의 빠른 파악 등 다양한 도메인에서 정보 검색·요약·보고서 자동화에 활용됩니다. 관련 개념으로는 텍스트 요약과 대비되는 텍스트 생성(Text Generation), 질문응답(QA), 그리고 문서 분류(Document Classification) 등이 있으며, 특히 요약은 정보 압축과 핵심 전달에 초점을 맞춘 반면, 일반 텍스트 생성은 창의적·다양한 문맥을 포괄한다는 차이가 있습니다." ;
    rdfs:subClassOf llm:NLG .

llm:SupervisedFinetuning a owl:Class ;
    rdfs:label "SupervisedFinetuning"@en ;
    llm:description "Supervised Finetuning(지도 학습 기반 파인튜닝)은 사전 학습된 대형 언어 모델이나 비전 모델에 라벨링된 데이터셋을 이용해 손실 함수를 최소화하면서 파라미터를 미세조정하는 전이 학습 기법이다. 이 과정에서는 학습률 스케줄링, 배치 정규화, 정규화 기법 등을 적용해 오버피팅을 방지하고 데이터 효율성을 높이며, 라벨 데이터가 제공하는 명시적 피드백을 통해 모델의 도메인 적응 및 성능 향상을 달성한다. 대표적인 사용 사례로는 감성 분석, 질의응답, 이미지 분류와 같은 특정 태스크에 대한 정확도 개선, 기업 맞춤형 챗봇 구축, 의료·법률 등 고도화된 분야의 전문 지식 적용 등이 있다. 비지도 파인튜닝이나 RLHF(강화학습 인간 피드백)와는 달리, Supervised Finetuning은 명시적인 정답 라벨에 의존하므로 학습 목표가 명확하고 평가가 직관적이며, 라벨 품질이 모델 성능을 직접 좌우한다는 점에서 차별화된다." ;
    rdfs:subClassOf llm:Finetuning .

llm:SwiGLU a owl:Class ;
    rdfs:label "SwiGLU"@en ;
    llm:description "SwiGLU(스위글루)는 **Swish** 비선형 함수와 **GLU(Gated Linear Unit)** 메커니즘을 결합한 **활성화 함수**로, 입력 벡터를 두 개의 선형 변환으로 나눈 뒤 하나에는 Swish \\(x·\\sigma(βx)\\) 를, 다른 하나에는 시그모이드 \\(\\sigma\\) 를 적용하고 두 결과를 **요소별 곱(element‑wise multiplication)** 하여 최종 출력을 만든다. 이 구조는 **부드러운 비선형성**과 **게이팅(gating)** 효과를 동시에 제공해, 기울기 소실을 완화하고 학습 초기에 더 빠른 **수렴(convergence)** 을 가능하게 하며, 기존 ReLU·GELU 대비 **연산 비용이 약간 증가하지만** 전체 모델 성능을 크게 향상시킨다. SwiGLU는 특히 **Transformer 기반 언어 모델(GPT‑3, T5 등)**·**비전 모델(ViT, Swin‑Transformer)** 의 **피드포워드(Feed‑Forward) 레이어**에서 널리 사용되며, 대규모 사전학습(pre‑training) 단계에서 **표현력(representation power)** 과 **추론 효율성(inference efficiency)** 을 동시에 개선한다. 관련 개념으로는 순수한 **GLU**, **Swish**, **GELU**, **ReLU** 등이 있으며, SwiGLU는 이들보다 **게이트와 스무스 비선형을 동시에 활용**한다는 점에서 차별화된다." ;
    rdfs:subClassOf llm:ActivationFunction .

llm:T5 a owl:Class ;
    rdfs:label "T5"@en ;
    llm:description "T5(Text‑to‑Text Transfer Transformer)는 모든 자연어 처리 작업을 “입력 텍스트 → 출력 텍스트” 형태의 시퀀스‑투‑시퀀스(seq2seq) 문제로 통합한 **Encoder‑Decoder** 기반의 대규모 사전학습 모델이다. 이 모델은 Transformer 인코더와 디코더를 결합하고, 입력 문장에서 연속된 스팬을 마스킹·복원하는 **span‑corruption** 목표로 대규모 코퍼스(예: C4)에서 **다중 작업(pre‑training) 학습**을 수행해, 텍스트 임베딩과 **시맨틱 벡터**를 고품질로 학습한다. T5는 기계 번역, 텍스트 요약, 질문‑응답, 문서 분류, 코드 생성 등 **다양한 텍스트‑투‑텍스트 응용**에 fine‑tuning만으로 바로 적용할 수 있어, **zero‑shot** 및 **few‑shot** 전이 학습에서도 강력한 성능을 보인다. BERT와 같은 **Encoder‑only** 모델이 마스크드 언어 모델링에 초점을 맞추는 반면, GPT와 같은 **Decoder‑only** 모델은 순차적 텍스트 생성에 특화되어 있어, T5는 두 접근법의 장점을 결합한 **통합형** 구조로 **벡터 검색**·**시맨틱 검색** 시스템에서도 풍부한 텍스트 표현을 제공한다." ;
    rdfs:subClassOf llm:EncoderDecoder .

llm:TaskCompletionRate a owl:Class ;
    rdfs:label "TaskCompletionRate"@en ;
    llm:description "**TaskCompletionRate**는 자동 메트릭(AutomaticMetric) 계열에 속하는 정량적 성능 지표로, 특정 기간이나 워크플로우 내에서 정의된 작업이 성공적으로 완료된 비율을 백분율로 나타낸다. 이 메트릭은 로그 데이터나 이벤트 스트림을 실시간으로 수집·분석하여, 완료된 작업 수를 전체 할당 작업 수로 나눈 뒤 100을 곱하는 간단한 계산식(완료 작업 ÷ 전체 작업 × 100)으로 자동 산출되며, 시계열 추적과 대시보드 시각화를 통해 트렌드와 변동성을 즉시 파악할 수 있다. 대표적인 사용 사례로는 프로젝트 관리 도구에서 스프린트 목표 달성률을 모니터링하거나, 클라우드 서비스에서 SLA 준수 여부를 검증하는 KPI·SLA 관리, 그리고 AI 파이프라인에서 모델 학습·추론 단계별 성공률을 자동 평가하는 워크플로우 자동화에 활용된다. 관련 개념으로는 정확도(Accuracy), 정밀도(Precision), 재현율(Recall) 등 품질 중심 메트릭이 있으며, TaskCompletionRate는 **작업 성공률**에 초점을 맞춘 반면, 이들 메트릭은 예측 결과의 정밀도·포괄성을 평가한다는 점에서 차별화된다." ;
    rdfs:subClassOf llm:AutomaticMetric .

llm:TaskSpecificFinetuning a owl:Class ;
    rdfs:label "TaskSpecificFinetuning"@en ;
    llm:description "**TaskSpecificFinetuning**은 사전 학습된 대형 모델(예: GPT, BERT, Vision Transformer 등)을 특정 downstream task — 예를 들어 감성 분석, 의료 이미지 분류, 법률 문서 요약 — 에 맞게 추가 학습시키는 **task‑specific fine‑tuning** 절차를 의미합니다. 이 과정에서는 원본 모델의 파라미터를 전체 혹은 일부(예: 어댑터 레이어, 프롬프트 매개변수)만 선택적으로 업데이트하고, 목표 작업에 특화된 라벨이 있는 **custom dataset**을 이용해 **gradient descent** 기반의 **supervised fine‑tuning**을 수행함으로써 모델의 **domain adaptation**과 **performance improvement**를 달성합니다. 대표적인 사용 사례로는 고객 리뷰의 감성 예측, 의료 영상에서 질병 병변 탐지, 법률·금융 분야의 문서 자동 분류·요약 등 **task‑oriented** 애플리케이션이 있으며, 이러한 적용은 **transfer learning**과 **few‑shot learning**을 결합해 데이터가 제한된 상황에서도 높은 정확도를 얻는 데 유리합니다. 관련 개념으로는 **멀티태스크 학습(multi‑task learning)**, **연속 학습(continual learning)**, **프롬프트 튜닝(prompt tuning)** 등이 있으며, **TaskSpecificFinetuning**은 전체 파라미터를 재학습하는 **전통적 파인튜닝**과 달리 **parameter‑efficient fine‑tuning**(예: LoRA, adapters)과 구분되는 특징을 가집니다." ;
    rdfs:subClassOf llm:Finetuning .

llm:TemperatureSampling a owl:Class ;
    rdfs:label "TemperatureSampling"@en ;
    llm:description "**TemperatureSampling**은 디코딩 전략(DecodingStrategy) 중 하나로, 언어 모델이 생성한 토큰 확률 분포에 ‘temperature’ 하이퍼파라미터를 적용해 샘플링 확률을 조정하는 기법이다. temperature 값이 1보다 작으면 분포가 ‘sharpen’(날카롭게) 되어 고확률 토큰이 더 자주 선택돼 출력의 결정성이 높아지고, 1보다 크면 분포가 ‘soften’(부드럽게) 되어 낮은 확률 토큰도 선택될 가능성이 커져 다양성·창의성이 증대된다. 이 방식은 챗봇 대화, 스토리 생성, 코드 자동완성, 번역 등 다양한 생성 작업에서 출력의 무작위성 또는 다양성을 제어하고자 할 때 널리 활용된다. TemperatureSampling은 Greedy 디코딩이나 Beam Search와 대비돼 확률적 샘플링을 강조하며, Top‑k · Top‑p (Nucleus) 샘플링과 결합해 다양한 ‘temperature‑controlled sampling’ 전략을 구현할 수 있다." ;
    rdfs:subClassOf llm:DecodingStrategy .

llm:TemplateDesign a owl:Class ;
    rdfs:label "TemplateDesign"@en ;
    llm:description "**TemplateDesign**은 프롬프트 엔지니어링(Prompt Engineering)에서 **재사용 가능한 프롬프트 구조**를 미리 정의하고 관리하는 설계 기법으로, 변수 자리표시자와 조건부 블록을 활용해 다양한 상황에 맞게 동적으로 조합할 수 있는 템플릿을 만드는 것을 의미합니다. 이 방식은 **프롬프트 파라미터화**, **컨텍스트 관리**, **버전 관리**와 같은 메커니즘을 통해 LLM에게 일관된 지시를 제공하면서도 필요에 따라 세부 내용을 교체하거나 확장할 수 있어, 프롬프트 최적화와 **시나리오 기반** 응답 생성이 자동화됩니다. 대표적인 사용 사례로는 **고객지원 챗봇**의 다국어 응답 템플릿, **콘텐츠 생성** 파이프라인에서의 기사 초안 템플릿, **데이터 추출** 작업을 위한 구조화된 질의 템플릿, 그리고 **코드 자동 생성**을 위한 함수 서명 템플릿 등이 있으며, 이러한 템플릿은 프롬프트 라이브러리와 **프롬프트 체이닝** 기법과 결합해 대규모 프로젝트에 효율적으로 적용됩니다. 관련 개념으로는 **Few‑shot prompting**, **Chain‑of‑Thought**와 같은 프롬프트 설계 기법이 있으며, **ad‑hoc prompting**과 달리 TemplateDesign은 **재사용성**, **일관성**, **스케일러빌리티**를 강조해 프롬프트 엔지니어링 전반의 생산성을 크게 향상시킵니다." ;
    rdfs:subClassOf llm:PromptEngineering .

llm:TextGeneration a owl:Class ;
    rdfs:label "TextGeneration"@en ;
    llm:description "**TextGeneration**은 주어진 프롬프트나 컨텍스트를 입력으로 받아 자연어 문장을 자동으로 만들어 내는 **자연어 생성(NLG)** 하위 기술로, 대규모 언어 모델(LLM)이나 Transformer 기반 디코더가 시퀀스‑투‑시퀀스 방식으로 토큰을 순차적으로 예측해 텍스트를 생성합니다. 주요 특징으로는 **조건부 생성**(조건에 맞는 문장 생성), **다양성 제어**(temperature, top‑k/top‑p 샘플링)와 **품질 향상**을 위한 **컨텍스트 윈도우**와 **어텐션 메커니즘**이 있으며, 프롬프트 엔지니어링을 통해 특정 스타일·톤·형식으로 맞춤형 출력을 얻을 수 있습니다. 대표적인 사용 사례는 **대화형 AI 챗봇**, **자동 기사·보고서 작성**, **코드 자동 완성**, **요약·번역·스토리텔링** 등 다양한 콘텐츠 생성 및 자동화 시나리오에 활용됩니다. 관련 개념으로는 **텍스트 요약**, **텍스트 변환(translation)**, **텍스트 완성(autocomplete)** 등이 있으며, 이는 모두 NLG에 속하지만 **텍스트 분류**·**검색**과 같은 NLU(자연어 이해) 작업과는 생성 목표와 모델 구조가 근본적으로 다릅니다." ;
    rdfs:subClassOf llm:NLG .

llm:TextToImage a owl:Class ;
    rdfs:label "TextToImage"@en ;
    llm:description "**TextToImage**는 텍스트(자연어) 설명을 입력으로 받아 해당 설명에 부합하는 이미지를 자동으로 생성하는 **멀티모달 생성 모델**(multimodal generative model)이다. 이 작업은 주로 **프롬프트 엔지니어링**된 문장을 **Diffusion 모델**이나 **GAN(Generative Adversarial Network)** 같은 딥러닝 아키텍처에 전달해, 텍스트의 의미를 **시각적 표현**(visual representation)으로 변환하는 방식으로 동작한다. 대표적인 사용 사례로는 **AI 아트**·광고·콘셉트 디자인, 게임 및 영화 제작에서의 **시각화 프로토타이핑**, 그리고 교육·의학 분야에서 텍스트 기반 **이미지 합성**을 통한 자료 생성 등이 있다. 관련 개념으로는 텍스트를 입력받아 설명을 출력하는 **ImageToText(이미지‑텍스트 변환)**, 텍스트에서 동영상을 생성하는 **TextToVideo**, 그리고 텍스트와 이미지 간의 **멀티모달 학습**(multimodal learning) 등이 있으며, TextToImage는 입력이 **정적인 텍스트**인 점에서 이들와 차별화된다." ;
    rdfs:subClassOf llm:MultimodalTask .

llm:TokenEmbedding a owl:Class ;
    rdfs:label "TokenEmbedding"@en ;
    llm:description "**TokenEmbedding**은 텍스트를 구성하는 개별 토큰(단어, 서브워드, 심볼 등)을 고정 차원의 실수 벡터로 변환하는 **EmbeddingLayer**의 핵심 구성 요소로, 토큰 ID를 인덱스로 하는 조회 테이블(lookup matrix)에서 해당 행을 추출해 **semantic vector**, **dense representation**, **continuous embedding**을 생성한다. 이 과정은 **학습 가능한 파라미터**인 임베딩 행렬을 통해 **gradient‑based optimization**으로 업데이트되며, **static embedding**(Word2Vec, GloVe)과 **contextual embedding**(BERT, GPT) 모두에서 동일한 메커니즘을 공유하지만, 후자는 입력 문맥에 따라 동적으로 조정되는 **position‑aware** 혹은 **self‑attention** 기반 변형을 포함한다. 토큰 임베딩은 **자연어 처리(NLP) 파이프라인**에서 **텍스트 분류**, **질의응답**, **기계 번역**, **추천 시스템** 등 다양한 **downstream task**에 입력 특성으로 활용되며, 고차원 벡터 공간에서 **유사도 검색**, **클러스터링**, **벡터 데이터베이스**와 같은 **vector search** 시나리오에서도 핵심 역할을 한다. 관련 개념으로는 **PositionalEncoding**, **SentenceEmbedding**, **DocumentEmbedding**, **CharacterEmbedding** 등이 있으며, 토큰 임베딩은 **단위 토큰 수준**의 의미를 포착하는 반면, 문장·문서 임베딩은 **전체 문맥**을 요약하는 **aggregate representation**이라는 차이점이 있다." ;
    rdfs:subClassOf llm:EmbeddingLayer .

llm:ToolAugmentedAgent a owl:Class ;
    rdfs:label "ToolAugmentedAgent"@en ;
    llm:description "**ToolAugmentedAgent**는 대규모 언어 모델(LLM)이나 기타 인공지능 코어에 외부 도구(예: 검색 엔진, 데이터베이스 API, 코드 실행 환경, 시뮬레이션 엔진 등)를 동적으로 호출하도록 설계된 **AgentArchitecture**의 한 형태이다. 이 에이전트는 자연어 명령을 해석한 뒤, 도구 선택·파라미터 생성·API 호출·결과 통합 과정을 반복하는 **tool‑use loop**를 통해 복합적인 추론과 실행을 수행하며, 필요 시 **retrieval‑augmented generation**, **function‑calling**, **planner‑executor** 메커니즘을 결합한다. 대표적인 사용 사례로는 실시간 웹 검색 기반 질의 응답, 금융 데이터 분석·보고서 자동 생성, 로봇 제어와 같은 물리‑디지털 인터페이스, 그리고 코드 자동 생성·디버깅 등 **멀티‑툴 오케스트레이션**이 요구되는 업무 자동화가 있다. 관련 개념으로는 순수 텍스트 기반 **Pure LLM Agent**, **Reactive Agent**, **Cognitive Agent**, 그리고 **Retrieval‑Augmented Generation (RAG)**가 있으며, ToolAugmentedAgent는 외부 툴 호출 능력과 내부 언어 모델의 추론 능력을 **통합·보강**함으로써 두 접근법의 장점을 동시에 활용한다." ;
    rdfs:subClassOf llm:AgentArchitecture .

llm:ToolCallAccuracy a owl:Class ;
    rdfs:label "ToolCallAccuracy"@en ;
    llm:description """**ToolCallAccuracy**는 LLM 또는 AI 에이전트가 사용자 프롬프트에 따라 외부 Tool (예: 검색 API, 계산기, 데이터베이스) 을 호출할 때, 예측된 Tool 명·파라미터 조합이 정답(ground‑truth) 과 얼마나 일치하는지를 수치화한 **AutomaticMetric**이다.  
이 메트릭은 정확히 일치하는 Tool 콜을 ‘Exact Match’ 로, 부분적으로 맞는 인자 구성을 ‘Partial Match’ 로 구분해 가중 평균을 내거나 Precision·Recall·F1 형태의 스코어를 산출함으로써 도구 호출 성공률을 정량적으로 평가한다.  
주요 사용 사례로는 코드 생성·디버깅 에이전트, 웹 검색·지식 추출 플러그인, 데이터 파이프라인 자동화 시스템 등 Tool 사용이 핵심인 멀티‑모달 또는 플러그인‑기반 LLM 벤치마크에서 성능 비교·실시간 모니터링에 활용된다.  
관련 개념으로는 ToolCallPrecision·ToolCallRecall·ToolUseSuccessRate 와 같은 세부 지표가 있으며, 전통적인 텍스트‑기반 AutomaticMetric 인 BLEU·ROUGE·ExactMatch와는 ‘어떤 Tool 을 어떻게 호출했는가’라는 행동 측면을 측정한다는 점에서 차별화된다.""" ;
    rdfs:subClassOf llm:AutomaticMetric .

llm:ToolInterface a owl:Class ;
    rdfs:label "ToolInterface"@en ;
    llm:description "**ToolInterface**는 **AgentComponent** 계층 구조에서 에이전트가 외부 도구·서비스를 탐색·호출·관리할 수 있도록 정의된 **추상 계약(abstract contract)**이다. 이 인터페이스는 메서드 시그니처, 입력·출력 스키마, 오류 처리 규칙, 도구의 기능·제한을 기술한 메타데이터 등을 명시함으로써 에이전트가 **동적 바인딩(dynamic binding)** 및 **런타임 선택(runtime selection)**을 통해 표준화된 API로 플러그인‑형태의 도구를 호출하도록 한다. 대표적인 사용 사례로는 LLM‑기반 대화형 에이전트, 로보틱 프로세스 자동화(RPA), 멀티모달 어시스턴트가 검색 엔진, 데이터베이스, 코드 실행 환경, IoT 디바이스 등 다양한 외부 시스템을 **통합·확장**하는 경우가 있다. 관련 개념으로는 **ToolAdapter**, **SkillModule**, **ActionHandler**, **ServiceConnector** 등이 있으며, 이들은 모두 **모듈화(modular)**·**확장성(extensible)**·**상호운용성(interoperable)**을 목표로 하지만, ToolInterface는 하드코딩된 도구 호출이나 단일 스킬 모듈과 달리 **플러그인 아키텍처**를 통한 유연한 도구 교체와 재사용을 강조한다." ;
    rdfs:subClassOf llm:AgentComponent .

llm:ToolUseTask a owl:Class ;
    rdfs:label "ToolUseTask"@en ;
    llm:description "**ToolUseTask**는 에이전트가 자연어 프롬프트 이외에 외부 툴(예: API 호출, 데이터베이스 쿼리, 파일 시스템 조작 등)을 동적으로 선택·실행하여 목표 작업을 완수하도록 설계된 **AgenticTask**의 하위 개념이다. 이 작업은 LLM 기반 에이전트가 문맥 이해와 툴 메타데이터(입력/출력 스키마, 비용, 권한)를 활용해 툴 선택·인자 생성·실행·피드백 루프를 반복함으로써 다단계 추론과 실제 행동을 통합한다. 대표적인 사용 사례로는 실시간 날씨 조회·주식 가격 검색, 기업 내부 지식 베이스 검색·업데이트, 코드 컴파일·디버깅, 자동 보고서 생성·스케줄링 등 다양한 자동화 시나리오가 있다. 관련 개념으로는 툴 호출(ToolCalling), ReAct 프레임워크, Planner‑Executor 구조가 있으며, 순수 텍스트 생성에만 의존하는 전통적 LLM 출력과는 툴 통합을 통한 실제 작업 수행 능력에서 뚜렷히 구분된다." ;
    rdfs:subClassOf llm:AgenticTask .

llm:TopKSampling a owl:Class ;
    rdfs:label "TopKSampling"@en ;
    llm:description "**Top‑K 샘플링**은 확률 분포에서 가장 높은 확률을 가진 K 개의 토큰만을 후보 집합으로 제한하고, 그 안에서 확률에 비례해 무작위로 하나를 선택하는 디코딩 전략(DecodingStrategy)입니다. 이 방식은 낮은 확률을 가진 잡음 토큰을 배제함으로써 생성 텍스트의 품질을 유지하면서도 다양성(diversity) 을 확보할 수 있게 해 주며, 온도(temperature) 와 결합해 샘플링 폭을 조절할 수 있습니다. 대표적인 사용 사례로는 대화형 AI, 스토리 생성, 코드 자동완성 등에서 다양하고 자연스러운 문장을 만들기 위해 활용되며, 빔 서치(Beam Search) 와 Nucleus Sampling(Top‑p) 과는 달리 고정된 후보 수(K) 에 초점을 맞추어 속도와 메모리 효율성을 동시에 얻을 수 있습니다. 관련 개념으로는 Top‑p( nucleus) 샘플링, 빔 서치, 그리디 디코딩 등이 있으며, Top‑K는 확률 상위 K 개 토큰만을 고려한다는 점에서 Top‑p가 누적 확률 기준으로 후보를 선택한다는 점과 대비됩니다." ;
    rdfs:subClassOf llm:DecodingStrategy .

llm:TopPSampling a owl:Class ;
    rdfs:label "TopPSampling"@en ;
    llm:description "Top‑p 샘플링(또는 Nucleus Sampling)은 언어 모델 디코딩 전략 중 하나로, 출력 토큰의 확률 분포에서 누적 확률이 사전 정의된 임계값 p(예: 0.9) 이하가 되도록 가장 높은 확률을 가진 토큰 집합을 동적으로 선택하고, 그 집합 안에서 확률에 비례해 무작위로 샘플링하는 방법이다. 이 방식은 고정된 상위 k 개의 토큰만을 고려하는 Top‑k 샘플링과 달리, 확률 질량을 기준으로 어휘를 자르기 때문에 문맥에 따라 필요한 어휘 규모가 자동으로 조절되어 텍스트의 다양성과 일관성을 동시에 높일 수 있다. 실무에서는 챗봇, 스토리 생성, 기계 번역, 요약 등 자연어 생성(NLG) 작업에서 과도한 반복을 방지하고 보다 인간‑같은 응답을 얻기 위해 널리 활용되며, 온도(temperature)와 결합해 샘플링 폭을 세밀하게 제어한다. 반면, Greedy Decoding이나 Beam Search와 같은 탐욕적·탐색 기반 디코딩과는 달리 확률적 선택을 강조하므로, 생성 품질을 평가할 때는 BLEU·ROUGE와 같은 자동 평가 지표와 인간 평가를 함께 고려해야 한다." ;
    rdfs:subClassOf llm:DecodingStrategy .

llm:ToxicityDetection a owl:Class ;
    rdfs:label "ToxicityDetection"@en ;
    llm:description "**ToxicityDetection**은 텍스트, 이미지, 음성 등 다양한 미디어에서 혐오, 비방, 성적·폭력적 표현 등 *유해·독성* 콘텐츠를 자동으로 식별하고 차단하는 *SafetyTechnique* 중 하나이다. 이 기술은 사전 학습된 언어 모델이나 멀티모달 신경망에 *독성 점수* (예: toxicity score, hate‑speech probability)를 부여하고, 임계값(threshold) 기반 *실시간 필터링* 또는 *오프라인 검토* 프로세스를 통해 위험성을 평가한다. 대표적인 사용 사례로는 소셜 미디어 플랫폼의 댓글·게시물 검열, 온라인 게임 채팅의 *혐오 발언 차단*, 기업 내부 커뮤니케이션 툴의 *콘텐츠 안전 관리* 등이 있으며, 특히 *다중언어* 지원과 *민감도 조정* (precision‑recall trade‑off) 기능이 중요한 분야에 적용된다. 관련 개념으로는 *BiasDetection* (편향 탐지)이나 *SafetyVerification* (안전성 검증)과 대비되며, 독성 감지는 주로 *정서·감정 분석* 과 *문맥 이해* 에 초점을 맞추는 반면, 편향 탐지는 *공정성* 과 *대표성* 에 중점을 둔다는 차이가 있다." ;
    rdfs:subClassOf llm:SafetyTechnique .

llm:Translation a owl:Class ;
    rdfs:label "Translation"@en ;
    llm:description "**Translation**은 한 언어로 표현된 텍스트를 다른 언어로 변환하는 **자연어 생성(NLG)** 작업의 한 형태로, 입력 문장의 의미와 문맥을 보존하면서 목표 언어의 문법·어휘 규칙에 맞는 문장을 자동으로 생성한다는 점이 핵심 정의이다. 현대 번역 시스템은 **시퀀스‑투‑시퀀스(Seq2Seq) 모델**이나 **Transformer 기반 신경망(NMT)**을 활용해 대규모 다국어 코퍼스를 학습하고, 어텐션 메커니즘을 통해 원문과 번역문 사이의 정교한 의미 정렬을 수행한다; 이와 달리 전통적인 규칙 기반 번역은 사전 정의된 문법 규칙과 사전 매핑에 의존한다. 대표적인 사용 사례로는 **다국어 웹사이트 현지화**, **실시간 통역 서비스**, **글로벌 전자상거래 제품 설명 자동 생성**, 그리고 **학술 논문·특허 문서의 다국어 접근성 확보** 등이 있으며, 번역 품질 평가는 **BLEU**, **METEOR**, **chrF**와 같은 자동 평가 지표로 측정한다. 관련 개념으로는 **요약(summarization)**·**문서 생성(document generation)**과 대비되며, 요약은 정보를 압축하는 반면 번역은 정보를 동일하게 유지하면서 언어만 교체한다는 차이가 있다." ;
    rdfs:subClassOf llm:NLG .

llm:TreeOfThoughts a owl:Class ;
    rdfs:label "TreeOfThoughts"@en ;
    llm:description """**Tree of Thoughts (ToT)**는 대형 언어 모델(Large Language Model, LLM)의 **고급 프롬프팅(Advanced Prompting)** 기법 중 하나로, 모델이 하나의 연속적인 답변이 아니라 **여러 가능한 사고 단계(thoughts)를 트리 구조로 확장·탐색**하도록 설계된 방법입니다.  
이 방식은 **노드 확장(node expansion)**·**가치 평가(value scoring)**·**가지치기(pruning)** 를 반복하면서, **Monte‑Carlo Tree Search**나 **베이즈 최적화**와 유사한 탐색 알고리즘을 적용해 가장 유망한 사고 경로를 선택하고, 필요시 **백트래킹(backtracking)** 으로 다른 브랜치를 재시도합니다.  
ToT는 **수학 증명, 복합 계획 수립, 코드 생성, 게임 플레이, 다중 턴 대화** 등과 같이 **다단계 추론과 대규모 탐색 공간이 요구되는 복잡 문제**에 활용되며, **생성된 사고(Thought) 임베딩을 벡터 인덱스에 저장해 유사도 기반 검색(vector search)** 과 결합하면 외부 지식베이스와의 **리트리벌‑증강 생성(RAG)** 도 가능하게 합니다.  
이와 달리 전통적인 **Chain‑of‑Thought(COT)** 프롬프팅은 **선형적인 사고 흐름**에 머무르는 반면, ToT는 **브랜칭·재귀적 사고(branching & recursion)** 를 통해 **탐색‑활용(exploration‑exploitation) 균형**을 맞추며, **결정 트리(decision tree), 계층적 프롬프트 분해(hierarchical prompt decomposition), 재귀 프롬프팅(recursive prompting)** 등과도 연계되어 보다 풍부한 추론 능력을 제공합니다.""" ;
    rdfs:subClassOf llm:AdvancedPrompting .

llm:TreeSearch a owl:Class ;
    rdfs:label "TreeSearch"@en ;
    llm:description "TreeSearch는 트리 구조를 탐색하여 목표 노드나 해를 찾는 **검색 절차(SearchProcedure)**의 한 형태로, 루트에서 시작해 자식‑부모 관계를 따라 **노드 확장(node expansion)**과 **가지치기(pruning)**를 수행하면서 탐색 공간을 체계적으로 축소한다. 일반적으로 **깊이 우선 탐색(DFS)**, **너비 우선 탐색(BFS)**, **이진 탐색 트리 검색**, **A\\* 기반 휴리스틱 탐색**, **Monte Carlo Tree Search(MCTS)** 등 다양한 **탐색 전략(search strategy)**이 적용되며, 각 전략은 **완전성(completeness)**, **최적성(optimality)**, **시간·공간 복잡도(time‑space complexity)**와 같은 특성에서 차이를 보인다. 대표적인 사용 사례로는 **게임 AI(체스, 바둑 등)에서의 수 찾기**, **경로 계획(pathfinding) 및 로봇 내비게이션**, **의사결정 트리와 규칙 기반 시스템**에서의 최적 해 도출, 그리고 **데이터베이스 인덱스 탐색**이나 **컴파일러의 구문 분석** 등 다양한 분야에 활용된다. 관련 개념으로는 **그래프 탐색(GraphSearch)**, **선형 검색(Linear Search)**, **해시 기반 검색(Hash Search)** 등이 있으며, TreeSearch는 계층적 구조와 명시적 부모‑자식 관계를 이용한다는 점에서 이들 비트리 기반 검색 방법과 구별된다." ;
    rdfs:subClassOf llm:SearchProcedure .

llm:UL2 a owl:Class ;
    rdfs:label "UL2"@en ;
    llm:description "UL2(​Unified Language Learning 2)는 인코더‑디코더(seq2seq) 구조를 기반으로 “mixture‑of‑denoisers” 사전학습 방식을 도입한 통합 언어 모델로, 하나의 모델 안에서 프리픽스 언어 모델링, 스팬 마스킹, 인과적 언어 모델링 등 세 가지 학습 목표를 혼합해 학습한다는 점이 핵심 정의이다. 이 모델은 인코더와 디코더 양쪽에서 고품질 토큰 임베딩과 문장 임베딩을 동시에 생성할 수 있어, 마스크 복원·조건부 생성·자연스러운 텍스트 이어쓰기 등 다양한 작동 원리를 하나의 파라미터 집합으로 구현한다. UL2는 기계 번역·요약·질문‑응답·검색‑보강 생성(RAG) 등에서 활용되며, 특히 밀집 벡터(dense embedding)를 이용한 의미 검색·벡터 검색(dense retrieval) 및 세맨틱 인덱싱에 강점을 보여 대규모 문서 검색 시스템에 널리 적용된다. 관련 개념으로는 순수 디코더형 GPT 시리즈와 순수 인코더형 BERT, 그리고 T5·BART·PaLM과 같은 기존 인코더‑디코더 모델이 있으며, UL2는 이들보다 사전학습 목표를 통합해 다중태스크 전이와 프롬프트 기반 제로‑샷/Few‑Shot 성능을 동시에 제공한다." ;
    rdfs:subClassOf llm:EncoderDecoder .

llm:Unigram a owl:Class ;
    rdfs:label "Unigram"@en ;
    llm:description """**Unigram**은 텍스트를 **단일 토큰(단어) 단위**로 분리한 가장 기본적인 **n‑gram** 형태로, 각 토큰이 독립적인 특징(feature)으로 취급되는 **Tokenization** 기법이다.  
이 방식은 문서 내 모든 단어를 **순서 없이** **Bag‑of‑Words** 혹은 **TF‑IDF**와 같은 **희소 벡터(sparse vector)** 로 변환하여 **단어 빈도**와 **가중치**만을 이용해 **특징 추출(feature extraction)**을 수행하며, **문맥 정보는 보존되지 않는다**는 점이 특징이다.  
Unigram은 **검색 엔진의 인덱싱**, **문서 분류**, **감성 분석**, **주제 모델링(LDA)** 등 **텍스트 분류·검색·정보 검색** 분야에서 **키워드 매칭**과 **전통적인 벡터 검색(vector search)**에 널리 활용된다.  
관련 개념으로는 **Bigram·Trigram**과 같은 **연속 n‑gram**, **Subword 토크나이저(BPE, WordPiece)**, **문맥 기반 임베딩(BERT, GPT)** 등이 있으며, Unigram은 **문맥을 무시하고 순서에 독립적인** 특성 때문에 **고차원 밀집 임베딩**과는 대비되는 **희소·순서‑무시형 표현**으로 구분된다.""" ;
    rdfs:subClassOf llm:Tokenization .

llm:VQA a owl:Class ;
    rdfs:label "VQA"@en ;
    llm:description "VQA(Visual Question Answering)는 이미지와 자연어 질문이라는 두 개의 모달을 동시에 입력받아, 해당 시각적 내용에 대한 정확한 답변을 생성하는 멀티모달 Task이다. 일반적으로 CNN이나 Vision Transformer와 같은 시각 인코더가 이미지 특징을 추출하고, BERT·GPT·T5와 같은 텍스트 인코더가 질문을 임베딩한 뒤, 멀티모달 트랜스포머 혹은 CLIP‑style 교차‑어텐션 메커니즘을 통해 두 임베딩을 결합해 답변 토큰을 디코딩한다. 대표적인 사용 사례로는 의료 영상 진단 보조, 로봇 내비게이션에서의 상황 질의, 전자상거래 제품 이미지 기반 고객 상담, 교육용 인터랙티브 퀴즈 등이 있으며, 시각적 이해와 자연어 처리(NLP)를 동시에 요구하는 대화형 AI 시스템에 널리 적용된다. 관련 개념으로는 이미지 캡션 생성(이미지 → 텍스트)과 텍스트 기반 QA(텍스트 → 답변)가 있으며, VQA는 단일 모달(이미지 분류·객체 검출)과 달리 시각‑언어 교차 정보를 활용한다는 점에서 차별화된다." ;
    rdfs:subClassOf llm:MultimodalTask .

llm:ValueAlignment a owl:Class ;
    rdfs:label "ValueAlignment"@en ;
    llm:description "**ValueAlignment**은 인공지능 시스템이 인간이 추구하는 가치·윤리·사회적 규범에 일관되게 행동하도록 설계·조정하는 **AlignmentMethod**의 한 형태이며, “가치‑정렬” 혹은 “가치 정렬”이라고도 불립니다. 이 방법은 인간 피드백을 기반으로 한 보상 모델링, 역강화학습(Inverse Reinforcement Learning), 선호 학습(Preference Learning) 등으로 인간의 가치 함수를 추정하고, 추정된 가치 함수를 정책 최적화에 통합해 에이전트가 목표를 달성하면서도 인간이 정의한 가치 기준을 위배하지 않도록 제어합니다. 대표적인 사용 사례로는 자율주행차의 안전·공정성 보장, 의료 AI의 환자 중심 치료 결정, 대규모 언어 모델의 편향·유해 콘텐츠 억제, 그리고 로봇 협업 시스템에서 인간 작업자의 의도와 윤리적 제한을 반영하는 것이 있습니다. 관련 개념으로는 목표‑정렬(ObjectiveAlignment)·보상‑정렬(RewardAlignment) 등이 있으며, 이들은 주로 수치적 목표 최적화에 초점을 맞추는 반면, ValueAlignment은 정성적·규범적 가치와 인간의 윤리적 기대를 직접 반영한다는 점에서 차별화됩니다." ;
    rdfs:subClassOf llm:AlignmentMethod .

llm:VectorMemory a owl:Class ;
    rdfs:label "VectorMemory"@en ;
    llm:description """**VectorMemory**는 고차원 임베딩(embedding) 벡터를 저장·인덱싱하고, 유사도 기반 검색을 통해 과거 컨텍스트나 지식을 빠르게 재활용할 수 있는 메모리 구조로, 대규모 언어 모델(Large Language Model)이나 멀티모달 시스템에서 **지식 통합(Knowledge Integration)**을 실현하는 핵심 요소이다.  
이 메모리는 **시맨틱 유사도 검색**, **근접 이웃 탐색(ANN)**, **동적 업데이트** 메커니즘을 활용해 입력 쿼리와 가장 관련성 높은 벡터를 실시간으로 회수하고, 회수된 벡터를 모델의 컨텍스트 윈도우에 결합해 **리트리벌‑증강 생성(RAG)** 혹은 **메모리 네트워크** 형태로 응답을 생성한다.  
대표적인 사용 사례로는 장기 대화 기억을 유지하는 챗봇·퍼스널 어시스턴트, 제품·콘텐츠 추천 시스템, 의료 기록·법률 문서와 같은 비정형 데이터베이스에서의 **시맨틱 검색** 및 **지식 기반 질의응답** 등이 있다.  
관련 개념으로는 전통적인 키‑값 캐시나 관계형 데이터베이스와 달리 **벡터 스토어(Vector Store)**·**임베딩 데이터베이스**가 있으며, 이와 대비되는 특징은 **정확한 구조화된 조회**가 아니라 **의미 기반 근접성**을 기준으로 정보를 회수한다는 점이다.""" ;
    rdfs:subClassOf llm:KnowledgeIntegration .

llm:WebNavigation a owl:Class ;
    rdfs:label "WebNavigation"@en ;
    llm:description "**WebNavigation**은 에이전트 기반 **AgenticTask** 중 하나로, 목표 지향적인 소프트웨어 에이전트가 웹 페이지를 프로그램matically 탐색하고, 링크를 따라가며, 폼을 제출하고, 인증 절차를 수행하는 **자동화된 웹 탐색** 과정을 의미합니다. 이 작업은 헤드리스 브라우저, HTTP 요청 오케스트레이션, 동적 페이지 렌더링 처리, 그리고 시맨틱 분석·머신러닝 또는 강화학습 기법을 활용한 의사결정 모듈을 결합해 멀티스텝 플래닝 과 컨텍스트‑aware 경로 최적화를 수행합니다. 대표적인 사용 사례로는 실시간 가격 모니터링·시장 조사, 지식 그래프 구축을 위한 데이터 파이프라인, 개인화된 추천 시스템, 그리고 최신 정보를 필요로 하는 대화형 에이전트·챗봇의 정보 검색·업데이트가 있습니다. 관련 개념으로는 정적 HTML 추출에 국한되는 WebScraping 과 대규모 페이지 수집을 위한 WebCrawling이 있으며, WebNavigation은 동적 상호작용·에이전트 의사결정 이라는 차별화된 특징을 갖고 TaskAutomation·AgenticAction 과도 긴밀히 연결됩니다." ;
    rdfs:subClassOf llm:AgenticTask .

llm:WordPiece a owl:Class ;
    rdfs:label "WordPiece"@en ;
    llm:description "**WordPiece**는 텍스트를 **서브워드(subword) 단위**로 분할하는 **토크나이제이션(tokenization) 기법**으로, 사전에 정의된 **어휘(vocabulary)**에 포함되지 않은 희귀 단어나 신조어를 여러 개의 **짧은 토큰**으로 표현해 언어 모델이 효율적으로 학습하도록 돕는다. 이 방식은 **빈도 기반 병합**과 **최대 우도(maximum likelihood)** 원리를 이용해 가장 흔히 등장하는 문자 시퀀스를 먼저 사전에 포함시키고, 남은 부분을 **그리디(greedy) 매칭**으로 가장 긴 일치 토큰을 선택해 순차적으로 분해한다. WordPiece는 **BERT, ALBERT, ELECTRA** 등 대규모 사전학습 언어 모델에서 핵심 전처리 단계로 활용되며, **다국어 처리, 문서 검색, 질의응답** 등 다양한 자연어 처리(NLP) 응용 분야에서 **어휘 크기 감소와 OOV(out‑of‑vocabulary) 문제 완화**에 기여한다. 관련된 서브워드 토크나이저로는 **Byte‑Pair Encoding(BPE)**와 **SentencePiece**가 있으며, WordPiece는 BPE와 달리 **통계적 언어 모델(예: unigram language model)**을 기반으로 토큰을 선택한다는 점에서 차별화된다." ;
    rdfs:subClassOf llm:Tokenization .

llm:WorkflowAutomation a owl:Class ;
    rdfs:label "WorkflowAutomation"@en ;
    llm:description "**WorkflowAutomation**은 복잡한 업무 흐름을 **AgenticTask**(에이전트 기반 작업) 단위로 분해하고, AI 에이전트와 RPA 로봇, API 오케스트레이션 엔진이 자동으로 순차·병렬 실행하도록 설계된 **프로세스 자동화** 기술이다. 이 메커니즘은 이벤트‑드리븐 트리거, 조건부 분기, 데이터 매핑 및 오류 복구 로직을 내장한 **작업 오케스트레이션**을 통해 인간 개입 없이도 전사적 비즈니스 프로세스, IT 운영, 데이터 파이프라인 등을 **실시간으로 조정·실행**한다. 대표적인 사용 사례로는 고객 온보딩 워크플로우, 주문‑재고 자동 동기화, 보안 사고 대응 자동화, 마케팅 캠페인 퍼스널라이제이션 등이 있으며, 저코드/노코드 인터페이스와 **Human‑in‑the‑Loop** 검증 단계가 결합돼 비전문가도 손쉽게 설계할 수 있다. 관련 개념으로는 전통적인 **RPA 스크립트**와 **정적 배치 작업**이 있으며, 워크플로우 자동화는 동적 의사결정과 에이전트 기반 학습을 통해 **정적 자동화**보다 높은 적응성·확장성을 제공한다." ;
    rdfs:subClassOf llm:AgenticTask .

llm:WorldModel a owl:Class ;
    rdfs:label "WorldModel"@en ;
    llm:description "**WorldModel**은 AgentComponent 의 핵심 하위 모듈로, 에이전트가 외부 환경을 **내부적으로 재현·예측**하기 위해 유지하는 **환경 모델링**(environment modeling) 및 **상태 추정**(state estimation) 체계이다. 이 모델은 **동적 전이 함수**(transition function), **관측 모델**(observation model), 그리고 **보상 예측**(reward prediction) 등을 신경망, 확률 그래프 모델, 혹은 물리 기반 **시뮬레이터**(simulator)로 학습하여 현재 상태에서 미래 시점의 **시계열 데이터**와 **시각적 인식** 결과를 **예측**하고, 이를 바탕으로 **플래닝**(planning)이나 **시뮬레이션 기반 의사결정**(simulation‑based decision making)을 수행한다. 대표적인 사용 사례로는 **모델 기반 강화학습**(model‑based RL)에서의 정책 개선, 로봇 · 자율주행차의 **다중 에이전트**(multi‑agent) 협업, 게임 AI의 **시뮬레이션 기반 학습**(simulation‑based training), 그리고 복잡한 물리·경제 시스템의 **가상 실험**(virtual experiment) 등이 있다. 관련 개념으로는 **Model‑Free** 접근법과 대비되는 **Model‑Based** 접근, **Policy**, **ValueFunction**, **Simulator**, **Latent Dynamics** 등이 있으며, 최신 연구에서는 **Dreamer**, **MuZero**와 같은 **WorldModel** 기반 아키텍처가 **내부 모델**(internal model)과 **외부 모델**(external model) 간의 상호 보완성을 강조한다." ;
    rdfs:subClassOf llm:AgentComponent .

llm:ZeroShot a owl:Class ;
    rdfs:label "ZeroShot"@en ;
    llm:description "Zero‑Shot(제로샷) 프롬프팅은 사전 학습된 대규모 언어 모델에 별도의 예시나 레이블 없이 **프롬프트만** 제시해 원하는 작업을 수행하도록 하는 기본 프롬프팅 기법이다. 모델은 **컨텍스트 없이** 입력된 질문·명령을 자체적인 일반화 능력과 전이 학습(transfer learning) 결과에 의존해 즉시 답변하거나 텍스트를 생성하며, 별도의 파인튜닝이나 데이터 라벨링이 필요하지 않다. 대표적인 사용 사례로는 **질문‑응답, 텍스트 요약, 번역, 코드 생성** 등 다양한 자연어 처리 작업을 빠르게 프로토타이핑하거나, 제한된 리소스 환경에서 **멀티태스크** 모델을 활용해 즉시 서비스에 적용하는 경우가 있다. Zero‑Shot은 **Few‑Shot**(몇 개의 예시 제공)이나 **One‑Shot**(한 개의 예시 제공)과 대비되어, 예시 제공 여부에 따라 **in‑context learning**의 정도가 달라지는 점이 주요 차별점이며, **프롬프트 엔지니어링**·**프롬프트 튜닝**과 같은 관련 기술과도 밀접하게 연결된다." ;
    rdfs:subClassOf llm:BasicPrompting .

llm:description a owl:DatatypeProperty ;
    rdfs:label "description"@en ;
    rdfs:comment "개념의 설명"@ko .

llm:related a owl:ObjectProperty ;
    rdfs:label "related"@en ;
    rdfs:comment "개념 간의 관계"@ko .

llm:PreferenceOptimization a owl:Class ;
    rdfs:label "PreferenceOptimization"@en ;
    llm:description "PreferenceOptimization은 인간 사용자가 제공한 선호도(예: 순위, 클릭, 평가)를 직접적인 학습 신호로 활용해 모델의 출력 품질을 최적화하는 **학습 파라다임**이며, 주로 “선호 기반 강화 학습”(RLHF) 형태로 구현됩니다. 이 방법은 초기 Supervised Fine‑Tuning (SFT) 모델에 보상 모델을 학습시킨 뒤, 보상 함수를 이용해 정책을 업데이트하는 **보상 모델링**·**피드백 루프**·**정책 최적화** 과정을 거쳐, 인간 선호와 일치하는 응답을 생성하도록 모델 파라미터를 조정합니다. 대표적인 사용 사례로는 대화형 AI 챗봇, 검색 결과 재정렬, 맞춤형 추천 시스템, 그리고 안전성·편향 감소를 목표로 하는 대규모 언어 모델 파인튜닝이 있으며, 사용자 선호를 반영한 **맞춤형 응답**·**선호 최적화**이 핵심 가치로 작용합니다. 관련 개념으로는 전통적인 **지도 학습**(Supervised Learning)·**비지도 학습**(Unsupervised Learning)과 대비되는 **강화 학습**(Reinforcement Learning)·**Preference Learning**·**Reward Modeling**이 있으며, 특히 RLHF와의 차이는 보상 모델을 별도로 학습시켜 인간 피드백을 정량화한다는 점에 있습니다." ;
    rdfs:subClassOf llm:TrainingParadigm .

llm:AugmentationTechnique a owl:Class ;
    rdfs:label "AugmentationTechnique"@en ;
    llm:description "**AugmentationTechnique**는 대규모 언어 모델(LLM)의 학습·추론 단계에서 입력 텍스트나 멀티모달 데이터를 인위적으로 변형·확장하여 데이터 다양성을 높이고 모델 일반화·강인성을 향상시키는 일련의 기법을 말합니다. 주요 특징으로는 패러프레이징, 백트랜슬레이션, 노이즈 주입·토큰 마스킹, 샘플링 기반 시뮬레이션 데이터 생성, 이미지·오디오와 같은 비텍스트 모달리티에 대한 멀티모달 증강 등이 있으며, 이러한 변환은 프리트레이닝 파이프라인이나 파인튜닝·인스트럭션 튜닝 단계에서 자동화된 파이프라인으로 적용됩니다. 대표적인 사용 사례는 few‑shot·zero‑shot 프롬프트 설계 시 다양한 프롬프트 변형을 제공해 성능을 끌어올리는 것, 도메인 적응·편향 완화·데이터 부족 문제를 해결하기 위한 합성 데이터 생성, 그리고 Retrieval‑Augmented Generation(RAG)에서 검색 결과와 결합해 풍부한 컨텍스트를 제공하는 데 활용됩니다. 관련 개념으로는 수동적인 프롬프트 엔지니어링과 대비되는 자동화된 프롬프트 변형, 데이터 증강과 유사하지만 LLM 특화된 **Synthetic Data Generation**, 그리고 학습 순서를 조절하는 **Curriculum Learning** 등이 있으며, 이들 모두가 LLM 기반 어플리케이션의 효율성과 품질을 동시에 높이는 데 기여합니다." ;
    rdfs:subClassOf llm:LLMConcept .

llm:EmbeddingLayer a owl:Class ;
    rdfs:label "EmbeddingLayer"@en ;
    llm:description "**EmbeddingLayer**는 고차원 벡터 공간에 이산형(예: 단어, 아이템, 카테고리) 데이터를 매핑하기 위해 설계된 **ModelComponent**이며, 입력 인덱스를 학습 가능한 **임베딩 매트릭스**(lookup table)와 곱해져 연속적인 **특징 벡터**를 출력한다. 이 레이어는 **인덱스 기반 조회**와 **파라미터 공유**를 통해 대규모 vocabularies에서도 메모리 효율성을 유지하면서, **미분 가능한** 방식으로 임베딩을 업데이트해 **표현 학습**을 수행한다. 주로 **자연어 처리(NLP)**의 단어 임베딩, **추천 시스템**의 아이템 임베딩, **그래프 신경망**의 노드 임베딩 등에서 사용되며, **Transformer**, **Seq2Seq**, **CNN‑RNN 하이브리드** 등 다양한 딥러닝 아키텍처의 입력 전처리 단계에 필수적으로 포함된다. 관련 개념으로는 **One‑Hot Encoding**, **Feature Hashing**, **Dense Layer**가 있으며, EmbeddingLayer는 이산형 입력을 저차원 밀집 벡터로 변환한다는 점에서 **희소 표현**을 직접 다루는 방법과 차별화된다." ;
    rdfs:subClassOf llm:ModelComponent .

llm:GPTArchitecture a owl:Class ;
    rdfs:label "GPTArchitecture"@en ;
    llm:description "**GPTArchitecture**는 대규모 자연어 처리를 위해 설계된 **Decoder‑Only Transformer** 구조를 기반으로 하는 자기회귀(autoregressive) 언어 모델이다. 입력 토큰 시퀀스에 self‑attention 과 feed‑forward 네트워크를 순차적으로 적용해 다음 토큰을 예측함으로써 텍스트 생성, 요약, 번역, 코드 작성 등 다양한 생성형 작업을 수행한다. 사전 학습(pre‑training) 단계에서 방대한 텍스트 코퍼스를 이용해 일반 언어 지식을 습득하고, 이후 few‑shot 또는 zero‑shot 프롬프트를 통한 in‑context 학습, 혹은 도메인‑특화 fine‑tuning 으로 챗봇, 문서 검색, 자동 응답 시스템 등 실시간 LLM 서비스에 널리 활용된다. 이와 대비되는 **Encoder‑Decoder** 구조(예: BERT, T5)는 masked 또는 seq2seq 학습 방식을 사용해 입력‑출력 쌍을 동시에 처리하지만, GPTArchitecture는 오직 디코더만으로 단일 스트림 생성을 최적화한다." ;
    rdfs:subClassOf llm:DecoderOnly .

llm:SafetyAndAlignment a owl:Class ;
    rdfs:label "SafetyAndAlignment"@en ;
    llm:description "SafetyAndAlignment는 대규모 언어 모델(LLM)이 인간의 가치와 의도에 부합하도록 행동하도록 설계·검증하는 AI 안전·정렬 프레임워크이며, “AI alignment problem”과 “AI safety”를 해결하기 위한 핵심 개념이다. 이 분야는 인간 피드백 기반 강화학습(RLHF), 프롬프트 가드, 레드팀 테스트, 위험 평가 메트릭, 편향·유해성 완화와 같은 정렬 메커니즘과 견고성(robustness)·해석 가능성(interpretablity) 기술을 결합해 모델 출력을 실시간으로 모니터링하고 제어한다. 대표적인 사용 사례로는 콘텐츠 검열·정책 준수, 의료·법률 조언과 같은 고위험 도메인에서의 안전한 질의응답, 자율 에이전트와 챗봇의 윤리적 행동 보장, 그리고 기업·정부 수준의 AI 거버넌스 구현이 있다. SafetyAndAlignment는 성능 최적화와 스케일링을 중시하는 “capability‑centric” 접근과 대비되며, 정렬·안전성을 강조하는 “value‑aligned”·“risk‑aware” 접근과 밀접하게 연관된 개념으로, AI 거버넌스, AI ethics, controllability와도 상호 보완적인 관계를 가진다." ;
    rdfs:subClassOf llm:LLMConcept .

llm:ActivationFunction a owl:Class ;
    rdfs:label "ActivationFunction"@en ;
    llm:description "**ActivationFunction**은 신경망 · 딥러닝 모델의 **ModelComponent** 중 하나로, 각 뉴런이나 레이어가 출력값에 적용하는 **비선형 변환 함수**를 의미합니다. 이 함수는 입력 신호를 **비선형성**으로 매핑함으로써 네트워크가 복잡한 패턴을 학습하도록 하고, 대표적인 형태로는 **ReLU**, **Sigmoid**, **tanh** 등이 있으며 각각 **gradient vanishing**·**gradient exploding** 방지, 계산 효율성, 출력 범위 제한 등 고유한 특성을 가집니다. 활성화 함수는 **전달 함수** 역할을 수행해 이미지 분류, 자연어 처리, 강화학습 등 다양한 **딥러닝 애플리케이션**에서 핵심적으로 사용되며, **Backpropagation** 과정에서 기울기 흐름을 조절해 학습 안정성을 높입니다. 관련 개념으로는 **Linear(Identity) Activation**과 대비되는 **비선형 활성화**, 그리고 **LossFunction**, **Optimizer**, **BatchNormalization** 같은 다른 **ModelComponent**와 상호작용하여 전체 모델의 성능과 수렴 속도를 최적화합니다." ;
    rdfs:subClassOf llm:ModelComponent .

llm:BasicPrompting a owl:Class ;
    rdfs:label "BasicPrompting"@en ;
    llm:description "**BasicPrompting**은 사용자가 LLM(대형 언어 모델)에 단순한 텍스트 입력만으로 원하는 작업을 수행하도록 지시하는 가장 기본적인 **프롬프트 기법(PromptingTechnique)**이다. 이 방식은 별도의 예시(example)나 복잡한 체인(chain) 없이 **명령형 프롬프트** 혹은 **질문형 프롬프트** 형태로 **시스템 프롬프트**와 **사용자 프롬프트**를 직접 작성해 모델에게 **텍스트 기반 명령**을 전달하고, 모델은 **Zero‑shot** 방식으로 즉시 응답한다. 대표적인 사용 사례로는 간단한 정보 검색, 요약, 번역, 문장 완성 등 **프롬프트 설계**가 최소화된 **대화형 AI** 응용 프로그램이나 **프롬프트 템플릿**을 활용한 **자동화 스크립트**가 있다. 관련 개념으로는 **Few‑shot prompting**, **Chain‑of‑thought prompting**, **Instruction prompting** 등이 있으며, 이들 고급 기법은 **예시 제공**이나 **추론 단계**를 추가해 성능을 향상시키는 반면, BasicPrompting은 **구조가 단순하고 구현 비용이 낮은** 점이 차별화된 특징이다." ;
    rdfs:subClassOf llm:PromptingTechnique .

llm:EfficientInference a owl:Class ;
    rdfs:label "EfficientInference"@en ;
    llm:description "**EfficientInference**는 대규모 딥러닝 모델이 실시간 또는 저전력 환경에서 **추론(인퍼런스) 비용을 최소화**하도록 설계된 **InferenceOptimization** 하위 기술을 의미합니다. 이를 위해 **양자화(Quantization), 프루닝(Pruning), 지식 증류(Distillation), 배치 처리 및 지연 실행(Latency‑aware Scheduling)** 등 모델 경량화와 연산 효율성을 동시에 달성하는 **하드웨어 가속(GPU/TPU/FPGA, TensorRT, ONNX Runtime)** 기법을 적용해 **메모리 사용량 감소와 연산량 절감**을 실현합니다. 대표적인 사용 사례로는 **엣지 디바이스(스마트폰, IoT 센서)에서의 실시간 이미지·음성 인식**, **클라우드 기반 대규모 추천 시스템**, 그리고 **멀티모달 생성 모델의 저지연 서비스**가 있으며, 이러한 환경에서는 **응답 지연 최소화와 전력 효율**가 핵심 요구사항이 됩니다. 관련 개념으로는 **Model Compression, Hardware‑Aware Optimization, Latency‑Critical Inference** 등이 있으며, **EfficientInference**는 단순히 추론 속도를 높이는 **Speed‑up**과는 달리 **연산 비용·메모리·전력 소비까지 포괄적으로 최적화**한다는 점에서 차별화됩니다." ;
    rdfs:subClassOf llm:InferenceOptimization .

llm:EvaluationMetric a owl:Class ;
    rdfs:label "EvaluationMetric"@en ;
    llm:description "**EvaluationMetric**은 대규모 언어 모델(LLM)의 성능을 정량적으로 측정하기 위해 정의된 평가 지표(예: 정확도, BLEU, ROUGE, F1‑score, perplexity 등)이며, 모델이 특정 태스크(번역, 요약, 질의응답 등)에서 얼마나 잘 작동하는지를 객관적으로 판단한다. 이러한 지표는 테스트 데이터셋에 대한 예측 결과와 정답 레이블을 비교하거나, 인간 평가(Human Evaluation)와 결합해 점수를 산출하는 방식으로 동작하며, 벡터 검색 기반의 임베딩 유사도 측정과도 연계되어 모델 출력의 의미적 일관성을 평가한다. 대표적인 사용 사례로는 모델 튜닝 단계에서의 성능 모니터링, 베이스라인 모델과의 비교, 그리고 공개 벤치마크(예: GLUE, SuperGLUE, MMLU)에서의 순위 산정 등이 있다. 관련 개념으로는 학습 과정에서 손실을 계산하는 **Loss Function**과, 평가용 데이터셋을 의미하는 **Benchmark Dataset**이 있으며, EvaluationMetric은 최종 결과를 검증하는 목적에 초점을 맞추는 반면 Loss Function은 학습 최적화에 직접 사용된다는 점에서 차별된다." ;
    rdfs:subClassOf llm:LLMConcept .

llm:Finetuning a owl:Class ;
    rdfs:label "Finetuning"@en ;
    llm:description "**Finetuning(미세조정)**은 대규모 사전학습(pre‑training)된 모델의 가중치를 초기값으로 사용하고, 특정 도메인이나 과제에 맞는 소규모 라벨 데이터셋을 이용해 추가 학습을 진행하는 **전이 학습(transfer learning)** 방식이다. 이 과정에서는 일반적으로 전체 네트워크의 일부 레이어를 고정(freeze)하고, 마지막 몇 개 레이어만 낮은 학습률(learning rate)로 업데이트하거나, 전체 파라미터를 미세하게 조정하여 **도메인 적응(domain adaptation)** 및 **데이터 효율성(data efficiency)**을 극대화한다. 파인튜닝은 감성 분석, 질문‑응답, 기계 번역 같은 자연어 처리(NLP) 작업뿐 아니라 이미지 분류, 객체 검출, 음성 인식 등 컴퓨터 비전 및 음성 분야에서도 널리 활용되며, **few‑shot learning**이나 **continual learning** 시나리오에서도 모델을 빠르게 맞춤화하는 핵심 기술이다. 반면, **from‑scratch training**은 처음부터 모든 파라미터를 무작위 초기화해 학습하는 방식으로, 대규모 데이터와 높은 연산 비용이 필요하지만 파인튜닝에 비해 특정 도메인에 대한 편향(bias)이 적은 장점이 있다." ;
    rdfs:subClassOf llm:TrainingParadigm .

llm:InferenceOptimization a owl:Class ;
    rdfs:label "InferenceOptimization"@en ;
    llm:description "**InferenceOptimization**은 대규모 언어 모델(LLM)의 추론 단계에서 지연 시간(Latency)과 비용을 최소화하면서도 정확도와 스루풋(Throughput)을 유지·향상시키기 위한 일련의 최적화 기법을 말합니다. 주요 특징으로는 모델 압축(Quantization, Pruning, Knowledge Distillation), 배치 처리와 파이프라인 스케줄링, 그리고 GPU·TPU·TensorRT·ONNX Runtime 같은 하드웨어 가속기를 활용한 연산 최적화가 포함되며, 이러한 방법들은 메모리 사용량을 줄이고 연산 효율을 높여 실시간 서비스나 엣지 디바이스에 적합하도록 설계됩니다. 대표적인 사용 사례는 챗봇 응답 가속, 검색 엔진의 실시간 질의 처리, 그리고 클라우드·멀티클라우드 환경에서 대규모 텍스트 생성 작업을 저비용으로 운영하는 경우이며, 특히 검색·추천 시스템과 결합된 벡터 검색 파이프라인에서 빠른 인퍼런스가 핵심 요구사항이 됩니다. 관련 개념으로는 **TrainingOptimization**(학습 단계 최적화)과 대비되며, **ModelDistillation**, **Quantization-Aware Training**, **Hardware Acceleration** 등과 연계되어 전체 LLM 라이프사이클에서 성능‑비용 트레이드오프를 종합적으로 관리합니다." ;
    rdfs:subClassOf llm:LLMConcept .

llm:ModelArchitecture a owl:Class ;
    rdfs:label "ModelArchitecture"@en ;
    llm:description "**ModelArchitecture**는 대규모 언어 모델(LLM)의 **구조적 설계**를 의미하며, 입력 토큰을 임베딩하고 다층 **Transformer** 블록(멀티헤드 어텐션, 피드‑포워드 네트워크, 레이어 정규화 등)으로 변환해 출력 시퀀스를 생성하는 **계층적 아키텍처**이다. 이 아키텍처는 **인코더‑디코더** 혹은 **디코더‑전용** 형태로 구현될 수 있으며, 파라미터 수와 **스케일링 법칙**에 따라 모델 용량, 연산 효율성, 메모리 요구사항이 결정되어 **분산 학습**·**GPU 가속** 환경에서 최적화된다. 대표적인 사용 사례로는 **텍스트 생성**, **질의응답**, **코드 작성**, **멀티모달 이해** 등 다양한 자연어 처리 작업에 적용되며, **프리트레인 → 파인튜닝** 흐름이나 **지식 증류·모델 압축** 기법을 통해 특정 도메인에 맞게 맞춤형 모델을 구축한다. 관련 개념으로는 **RNN·CNN 기반 전통 모델**, **Tokenization·Embedding**, **TrainingProcedure**, **Parameter‑Efficient Fine‑Tuning(PEFT)** 등이 있으며, Transformer 기반 ModelArchitecture는 이러한 이전 방식과 달리 **전역 어텐션**을 활용해 장기 의존성을 효율적으로 학습한다는 차별점을 가진다." ;
    rdfs:subClassOf llm:LLMConcept .

llm:ModelCompression a owl:Class ;
    rdfs:label "ModelCompression"@en ;
    llm:description "Model Compression은 대규모 딥러닝 모델의 파라미터 수와 연산량을 줄여 메모리 사용량과 추론 지연시간을 최소화하는 **Inference Optimization** 기술로, 모델 경량화(model lightweighting)와 효율적인 배포를 목표로 합니다. 주요 특징으로는 가중치 프루닝(weight pruning), 양자화(quantization), 지식 증류(knowledge distillation), 저랭크 행렬 분해(low‑rank factorization), 가중치 공유(weight sharing) 등 다양한 압축 기법이 결합되어 모델 크기와 FLOPs를 크게 감소시키면서도 정확도 손실을 최소화합니다. 이러한 압축된 모델은 모바일 디바이스, 엣지 컴퓨팅, IoT 센서, 클라우드 서비스 등 **resource‑constrained 환경**에서 실시간 추론, 배터리 수명 연장, 네트워크 대역폭 절감 등의 요구를 충족시키는 대표적인 사용 사례이며, 자동 운전, 음성 인식, 이미지 검색 등에서도 널리 활용됩니다. 관련 개념으로는 모델 프루닝과 양자화가 각각 파라미터 제거와 비트폭 축소에 초점을 두는 반면, 모델 압축은 이들을 통합하거나 **Neural Architecture Search(NAS)** 기반의 구조 최적화와 결합해 **model acceleration**과 **hardware‑aware optimization**을 동시에 달성한다는 점에서 차별화됩니다." ;
    rdfs:subClassOf llm:InferenceOptimization .

llm:MultimodalTask a owl:Class ;
    rdfs:label "MultimodalTask"@en ;
    llm:description "MultimodalTask는 텍스트, 이미지, 음성, 비디오 등 두 개 이상의 서로 다른 모달리티를 동시에 입력으로 받아 처리하고, 이들 간의 의미적 연관성을 학습·추론하는 응용 분야(ApplicationDomain) 내의 작업을 의미한다. 이러한 작업은 각 모달리티별 특징을 추출한 뒤 공통 임베딩 공간으로 매핑하고, 교차‑모달 어텐션이나 트랜스포머 기반 멀티헤드 메커니즘을 이용해 모달리티 간 정보를 융합·정렬(fusion/alignment)함으로써 다중 모달 벡터 검색, 멀티모달 질문‑응답, 크로스모달 추천 등 복합적인 인퍼런스를 수행한다. 대표적인 사용 사례로는 이미지‑텍스트 검색 엔진, 영상‑음성 요약 시스템, 의료 영상‑임상 기록 통합 분석, 로봇의 시각‑청각 인식 및 행동 계획 등이 있으며, 모두 고차원 멀티모달 임베딩을 활용한 유사도 기반 벡터 검색(vector similarity search)과 실시간 추론을 핵심으로 한다. 관련 개념으로는 단일 모달(Unimodal) 작업, 멀티태스크 학습(Multitask Learning), 그리고 크로스모달 검색(Cross‑modal Retrieval) 등이 있으며, 멀티모달 태스크는 다중 모달리티를 동시에 다루는 점에서 이들에 비해 더 복합적인 데이터 정합성과 모델 설계 요구를 가진다." ;
    rdfs:subClassOf llm:ApplicationDomain .

llm:PromptEngineering a owl:Class ;
    rdfs:label "PromptEngineering"@en ;
    llm:description "**PromptEngineering**은 대형 언어 모델(LLM)이나 생성형 AI에게 원하는 출력을 얻기 위해 프롬프트(입력 텍스트)를 체계적으로 설계·최적화하는 **PromptingTechnique**의 핵심 하위 분야이다. 이 기술은 **프롬프트 설계**, **컨텍스트 관리**, **시스템 프롬프트**와 **인스트럭션 튜닝**을 결합해 **few‑shot**·**zero‑shot** 학습을 유도하고, **체인‑오브‑생각(Chain‑of‑Thought)**이나 **프롬프트 템플릿**을 활용해 모델의 사고 흐름을 조정한다. 대표적인 사용 사례로는 **챗봇 대화 흐름 최적화**, **코드 자동 생성**, **문서 요약·번역**, **데이터 라벨링** 및 **검색 질의 강화** 등 다양한 AI 어시스턴트와 자동화 워크플로에 적용되며, 특히 **프롬프트 기반 검색**이나 **AI‑지원 의사결정** 시스템에서 성능을 크게 끌어올린다. 관련 개념으로는 **프롬프트 디자인**·**프롬프트 튜닝**, **프롬프트 파인튜닝**이 있으며, **프롬프트 인젝션 공격**과 같은 보안 위험과 **프롬프트 디버깅**·**프롬프트 견고성**을 강조하는 점에서 차별화된다. 이러한 키워드와 표현을 자연스럽게 포함함으로써 벡터 검색 엔진에서도 높은 검색 가시성을 확보할 수 있다." ;
    rdfs:subClassOf llm:PromptingTechnique .

llm:PromptingTechnique a owl:Class ;
    rdfs:label "PromptingTechnique"@en ;
    llm:description "**PromptingTechnique**는 대형 언어 모델(LLM)에 입력되는 텍스트 프롬프트를 설계·조정하여 원하는 출력(텍스트 생성, 요약, 질의응답 등)을 유도하는 **프롬프트 엔지니어링** 기법을 의미한다. 이 기술은 **프롬프트 템플릿**, **few‑shot**·**zero‑shot** 예시 제공, **체인‑오브‑쓰스(Chain‑of‑Thought)** 구조 삽입, **인‑컨텍스트 러닝** 등으로 **컨텍스트**와 **시퀀스**를 최적화함으로써 모델의 내부 추론 과정을 직접 조작한다. 대표적인 사용 사례로는 **코드 생성**, **고객 지원 챗봇**, **전문 분야 요약**, **데이터 라벨링 자동화** 등 다양한 **텍스트 생성**·**정보 추출** 작업에 적용되며, 특히 **프롬프트 튜닝**이나 **인스트럭션 파인튜닝**과 결합해 **모델 파라미터를 변경하지 않고** 성능을 향상시킨다. 관련 개념으로는 **Prompt Engineering**(프롬프트 설계 전반), **Prompt Injection**(보안 위협), **Fine‑tuning**(모델 파라미터 직접 수정) 등이 있으며, **PromptingTechnique**는 파라미터를 고정한 채 **프롬프트**만으로 모델 행동을 제어한다는 점에서 차별화된다." ;
    rdfs:subClassOf llm:LLMConcept .

llm:RAG a owl:Class ;
    rdfs:label "RAG"@en ;
    llm:description """RAG(Retrieval‑Augmented Generation)는 외부 지식 베이스에 대한 **벡터 검색**(dense retrieval)과 대형 언어 모델(LLM)의 **생성**을 결합한 **AugmentationTechnique**으로, 사용자의 질의를 임베딩한 뒤 FAISS·Milvus·ElasticSearch와 같은 **벡터 인덱스**에서 의미적 유사도가 높은 문서들을 top‑k 로 가져와 프롬프트에 삽입하고, LLM이 이를 기반으로 **grounded** 텍스트를 출력합니다.  
이 과정은 먼저 쿼리 임베딩을 생성하고 **approximate nearest neighbor** 검색으로 관련 문서를 회수한 뒤, **RAG‑Sequence** 혹은 **RAG‑Token** 방식으로 문서와 질의를 **fusion‑in‑decoder** 혹은 **concatenation** 형태로 결합해 생성 단계에 전달함으로써, 최신 사실성 유지와 **hallucination** 감소를 실현합니다.  
주요 활용 사례로는 **오픈 도메인 질문‑응답**, **지식‑집약형 챗봇**, **문서 요약**, **코드 보조** 및 **도메인 특화 콘텐츠 생성** 등이 있으며, 특히 최신 데이터가 지속적으로 업데이트되는 기업 위키나 논문 저장소와 같은 **knowledge‑intensive** 환경에서 효과적입니다.  
RAG는 순수 **프롬프트‑only** 혹은 **전통적 파인‑튜닝** 방식과 달리 외부 지식을 **동적**으로 주입하는 점에서 **knowledge‑grounded generation**, **hybrid retrieval‑generation 모델**과 연관되며, 순수 **retrieval‑only** 시스템이나 완전 엔드‑투‑엔드 학습 모델(T5‑XXL 등)과는 **지식 주입 방식**과 **응답 정확도** 면에서 뚜렷히 구분됩니다.""" ;
    rdfs:subClassOf llm:AugmentationTechnique .

llm:ReasoningTask a owl:Class ;
    rdfs:label "ReasoningTask"@en ;
    llm:description "**ReasoningTask**는 인공지능·머신러닝 시스템이 입력된 데이터와 도메인 지식을 바탕으로 논리적·다단계 추론을 수행해 결론을 도출하거나 문제를 해결하는 작업을 의미한다. 이 작업은 규칙 기반 추론 엔진, 지식 그래프, 시맨틱 파싱, 혹은 신경망 기반의 연산 흐름을 활용해 전제‑결론 구조를 구성하고, 전이·귀납·연역 등 다양한 추론 메커니즘을 순차적으로 적용한다. 대표적인 사용 사례로는 의료 진단 보조, 법률 문서 분석, 복잡한 시나리오 플래닝, 대화형 AI의 의도 파악·답변 생성, 그리고 로봇 제어와 같은 **ApplicationDomain** 전반에 걸친 의사결정·문제해결이 있다. 관련 개념으로는 **InferenceEngine**, **DecisionMakingTask**, **ClassificationTask**와 대비되는 **GenerationTask**가 있으며, 이들 간의 차이는 추론이 “왜/어떻게”를 설명하는 데 초점을 두는 반면, 분류는 라벨을 할당하고 생성은 새로운 콘텐츠를 만들어낸다는 점이다." ;
    rdfs:subClassOf llm:ApplicationDomain .

llm:ReinforcementLearning a owl:Class ;
    rdfs:label "ReinforcementLearning"@en ;
    llm:description "강화학습(Reinforcement Learning, RL)은 **에이전트가 환경과 상호작용하면서 보상(reward)을 최대화하는 행동 정책(policy)을 스스로 학습**하는 훈련 패러다임으로, 마르코프 결정 과정(MDP) 기반의 순차 의사결정 문제를 모델링한다. 주요 특징은 **탐험‑활용(Exploration‑Exploitation) 트레이드오프**, **보상 신호에 의한 피드백 루프**, 그리고 **가치 함수(value function)와 정책 함수(policy function)를 동시에 최적화**하는 Q‑러닝, 정책 그라디언트, 딥 Q‑네트워크(DQN) 등 다양한 알고리즘 구조를 포함한다. 대표적인 사용 사례로는 **게임 AI(예: 알파고·알파스타), 로봇 제어·자율 주행, 추천 시스템, 금융 포트폴리오 최적화, 스마트 팩토리 시뮬레이션** 등이 있으며, 시뮬레이션 환경에서의 연속 제어와 실시간 의사결정에 특히 강점을 보인다. 강화학습은 **지도학습(supervised learning)**이 라벨된 입력‑출력 쌍을 이용해 모델을 학습하는 방식과 달리, **라벨이 없는 데이터에서 보상 기반의 자기주도 학습**을 수행한다는 점에서 비지도학습(unsupervised learning)과도 구별된다." ;
    rdfs:subClassOf llm:TrainingParadigm .

llm:ToolUse a owl:Class ;
    rdfs:label "ToolUse"@en ;
    llm:description "**ToolUse**는 대형 언어 모델(LLM)이나 인공지능 시스템이 자체적인 내부 지식만으로는 해결하기 어려운 작업을 수행하기 위해 외부 **API**, **함수 호출**, **데이터베이스** 혹은 **웹 브라우징** 등 구체적인 도구를 **동적으로 호출**하고 **통합**하는 **AugmentationTechnique**의 한 형태이다. 이 기법은 프롬프트에 포함된 **툴 호출 명령**을 해석해 적절한 **툴 인자**를 구성하고, 실행 결과를 다시 모델에 피드백함으로써 **툴‑기반 추론**(tool‑augmented reasoning)이나 **툴‑기반 생성**(tool‑augmented generation)을 가능하게 한다. 대표적인 사용 사례로는 실시간 금융 데이터 조회, 코드 실행 및 디버깅, 복잡한 수치 시뮬레이션, 기업 내부 검색 및 보고서 자동 작성 등 **실시간 정보 검색**, **코드 실행**, **데이터베이스 질의**와 같은 **도메인‑특화 작업**에 활용된다. 관련 개념으로는 **Retrieval‑Augmented Generation(RAG)**이나 **Chain‑of‑Thought** 프롬프트와 대비되며, RAG가 정적 지식 베이스를 검색하는 데 비해 ToolUse는 **동적 외부 툴**을 직접 실행해 **실시간 결과**를 얻는 점에서 차별화된다." ;
    rdfs:subClassOf llm:AugmentationTechnique .

llm:Alignment a owl:Class ;
    rdfs:label "Alignment"@en ;
    llm:description "**Alignment**는 인공지능 시스템이 인간이 설정한 목표·가치와 일치하도록 학습·조정되는 **Training Paradigm**의 핵심 개념으로, 모델이 생성하는 출력이 의도된 의도와 윤리적 기준에 부합하도록 만드는 과정을 의미합니다. 이 과정은 **Human Feedback**, **Reinforcement Learning from Human Feedback (RLHF)**, **Reward Modeling**, **Preference Learning** 등으로 구성된 다단계 파이프라인을 통해 **목표 함수(objective function)**와 **보상 신호(reward signal)**를 인간의 선호와 안전·공정성 기준에 맞게 정제하고, **Supervised Fine‑tuning**·**RL 기반 미세조정**을 반복 적용함으로써 모델 파라미터를 점진적으로 조정합니다. 대표적인 사용 사례로는 **대형 언어 모델(LLM)**의 대화형 에이전트, **자율 로봇**·**자동차**의 행동 정책, **추천 시스템**·**콘텐츠 생성**에서의 윤리·편향 완화, 그리고 **보안·안전**이 중요한 의료·금융 AI 등에 적용되어 인간 의도와 일치하는 출력·결정을 보장합니다. 관련 개념으로는 **Misalignment**(목표 불일치), **Inverse Reinforcement Learning**, **Value Alignment**, **Safety Alignment**, **Bias Mitigation**, **Interpretability** 등이 있으며, 이들은 **Alignment**가 목표와 보상 설계, 인간 피드백 통합을 통해 **학습 파라다임**을 확장·보완하는 방식과 대비됩니다." ;
    rdfs:subClassOf llm:TrainingParadigm .

llm:AlignmentMethod a owl:Class ;
    rdfs:label "AlignmentMethod"@en ;
    llm:description "**AlignmentMethod**는 인공지능 시스템이 인간의 가치와 의도에 일관되게 행동하도록 설계·학습하는 일련의 기술·절차를 의미하며, SafetyAndAlignment 분야에서 “정렬 문제(Alignment Problem)”를 해결하기 위한 핵심 접근법으로 정의됩니다. 주요 특징으로는 인간 피드백을 활용한 Reward Modeling·Preference Learning, Human‑in‑the‑Loop 훈련, Inverse Reinforcement Learning·Cooperative Inverse Reinforcement Learning, 그리고 RLHF(Reinforcement Learning from Human Feedback) 와 같은 value‑aligned 학습 메커니즘이 포함되며, 이들 방법은 목표 함수의 안전성 검증, 불확실성 완화, 그리고 정책의 robustness·interpretability 를 동시에 강화하도록 설계됩니다. 대표적인 사용 사례는 대규모 언어 모델(Large Language Model)·챗봇의 aligned 응답 생성, 자율 주행 차량·로봇의 human‑aligned 제어 정책, 그리고 고위험 분야(예: 의료·군사)에서의 AI safety‑critical 시스템 배포 등이며, 이러한 적용은 AI alignment research 와 AI safety 프레임워크 내에서 표준화된 evaluation·verification 프로세스와 결합됩니다. 관련 개념으로는 AI alignment, AI safety, control problem, alignment verification, and robustness 가 있으며, AlignmentMethod 는 alignment verification (정렬 검증)과 대비되어 실제 policy 학습 단계에서 “정렬을 구현”하는 역할을 담당한다는 점이 특징입니다." ;
    rdfs:subClassOf llm:SafetyAndAlignment .

llm:NLG a owl:Class ;
    rdfs:label "NLG"@en ;
    llm:description "NLG(Natural Language Generation)는 구조화된 데이터나 내부 표현을 인간이 이해할 수 있는 자연어 텍스트로 변환하는 기술로, 입력된 의미론적 정보·통계 모델·딥러닝 기반 언어 모델을 활용해 문장을 자동으로 생성한다. 주요 특징은 템플릿 기반 생성, 규칙 기반 파이프라인, 그리고 최근 Transformer·GPT와 같은 사전학습 언어 모델을 이용한 컨텍스트‑민감형 생성으로, 문맥 일관성·다양성·스타일 제어가 가능하다는 점이다. 대표적인 사용 사례로는 챗봇·가상 비서의 대화 응답, 비즈니스 보고서·재무 요약·뉴스 기사 자동 작성, 제품 설명·광고 카피 생성 등 다양한 ApplicationDomain에서 텍스트 자동화가 이루어진다. NLG는 입력을 이해하고 의미를 추출하는 NLU(Natural Language Understanding)와 대비되며, NLU·NLU‑NLG 파이프라인, 텍스트 요약·문서 생성·대화형 AI 등 관련 개념과 함께 자연어 처리 생태계에서 핵심적인 역할을 담당한다." ;
    rdfs:subClassOf llm:ApplicationDomain .

llm:NLU a owl:Class ;
    rdfs:label "NLU"@en ;
    llm:description "NLU(Natural Language Understanding)는 인간이 일상적으로 사용하는 자연어 텍스트나 음성을 기계가 **의미와 의도**를 정확히 파악하도록 변환하는 기술로, 문법 구조 분석, 개체명 인식(NER), 감정 분석, 의도 분류 등 다양한 의미론적 처리 과정을 포함합니다. 최신 NLU는 Transformer 기반의 사전학습 모델(BERT, RoBERTa, GPT 등)을 활용해 **문맥‑민감한 임베딩**을 생성하고, 이를 통해 다중 의미 해소와 장기 의존성 학습을 수행합니다. 대표적인 적용 분야로는 **챗봇·음성 비서**, 고객 문의 자동 분류, 검색 질의 의도 파악, 의료 기록 자동 코딩, 소셜 미디어 감성 모니터링 등이 있으며, 이러한 사례들은 모두 사용자 경험을 향상시키는 **ApplicationDomain**에 속합니다. NLU는 텍스트를 생성하는 NLG(Natural Language Generation)와는 달리 **이해·해석**에 초점을 맞추며, 전통적인 정보 검색(IR)과는 의미 기반 매칭을 강조한다는 점에서 차별화됩니다." ;
    rdfs:subClassOf llm:ApplicationDomain .

llm:Normalization a owl:Class ;
    rdfs:label "Normalization"@en ;
    llm:description "Normalization은 모델 컴포넌트(ModelComponent) 중 하나로, 입력 데이터나 중간 활성값을 일정한 범위나 분포로 변환하여 학습 안정성을 높이는 전처리·정규화 단계이다. 일반적으로 최소‑최대 스케일링(min‑max scaling), Z‑score 표준화(z‑score), L2 정규화(unit‑norm)와 같은 수학적 변환을 적용하거나, 딥러닝에서는 배치 정규화(batch normalization), 레이어 정규화(layer normalization), 인스턴스 정규화(instance normalization)와 같이 학습 과정 중에 동적으로 평균·분산을 조정한다. 이러한 정규화는 그래디언트 소실·폭발을 방지하고 수렴 속도를 가속화해 이미지 분류, 자연어 처리, 추천 시스템 등 다양한 분야의 모델 훈련 및 임베딩 생성에 널리 활용된다. 정규화는 데이터 스케일링과는 달리 학습 중 파라미터에 의존하는 동적 특성을 가지며, 반대로 정규화된 출력을 원래 형태로 복원하는 디노멀라이제이션(denormalization)과 대비된다." ;
    rdfs:subClassOf llm:ModelComponent .

llm:PositionalEncoding a owl:Class ;
    rdfs:label "PositionalEncoding"@en ;
    llm:description "PositionalEncoding은 시퀀스 데이터의 각 토큰이나 패치에 위치 정보를 부여하는 **모델 컴포넌트**로, 입력 임베딩에 순서 정보를 인코딩해 Transformer와 같은 self‑attention 기반 아키텍처가 순서 의존성을 학습하도록 돕는다. 일반적으로는 고정된 사인‑코사인 함수로 만든 **절대 위치 인코딩**(sinusoidal positional encoding)이나, 학습 가능한 **위치 임베딩**(learnable position embedding) 형태로 구현되며, 입력 토큰 임베딩과 element‑wise하게 더해지거나 concat 방식으로 결합된다. 이 컴포넌트는 자연어 처리(NLP)에서 언어 모델링·기계 번역, 컴퓨터 비전(ViT)에서 이미지 패치 순서 지정, 음성·시계열 분석 등 **시퀀스 모델링** 전반에 걸쳐 널리 사용된다. 관련 개념으로는 **relative positional encoding**, **rotary embedding**, **bias‑only attention** 등이 있으며, 이들은 절대 인코딩과 달리 토큰 간 상대 거리나 회전 변환을 활용해 위치 정보를 표현한다." ;
    rdfs:subClassOf llm:ModelComponent .

llm:Pretraining a owl:Class ;
    rdfs:label "Pretraining"@en ;
    llm:description "Pretraining(프리트레이닝, 사전 학습)은 대규모 비지도·자기지도 데이터셋을 이용해 모델의 기본 표현(representation)을 미리 학습시키는 TrainingParadigm(학습 패러다임)으로, 주로 언어 모델링, 마스크드 토큰 예측(Masked Language Modeling), 이미지‑텍스트 대비 학습(contrastive vision‑language pretraining) 등 pretext 작업을 수행한다. 이 과정에서는 self‑supervised learning 또는 unsupervised pretraining 기법을 활용해 수억~수천억 개의 파라미터를 가진 large‑scale 네트워크를 사전 학습하고, 이후 fine‑tuning 이나 downstream task 에 맞춰 가중치를 조정함으로써 transfer learning 효과를 극대화한다. 대표적인 사용 사례로는 BERT, GPT, CLIP, ViT 와 같은 pre‑trained model 을 기반으로 자연어 처리(NLP)에서 텍스트 분류·질의응답, 컴퓨터 비전에서 이미지 분류·객체 검출, 음성 인식·추천 시스템 등 다양한 도메인에 적용되는 것이 일반적이다. 관련 개념으로는 from‑scratch training, supervised learning, curriculum learning 이 있으며, 프리트레이닝은 데이터 효율성·학습 속도·성능 향상 면에서 처음부터 학습하는 방식과 뚜렷히 대비된다." ;
    rdfs:subClassOf llm:TrainingParadigm .

llm:Quantization a owl:Class ;
    rdfs:label "Quantization"@en ;
    llm:description "Quantization(양자화)은 고정밀 float 가중치·활성화를 저비트 int 또는 fixed‑point 값으로 매핑해 모델 파라미터와 연산의 **정밀도 감소**와 **비트폭 축소**를 구현하는 모델 압축 기법이다. 이 과정은 **정적 양자화**, **동적 양자화**, **포스트‑트레이닝 양자화** 등 다양한 양자화 스킴을 적용해 가중치·바이어스와 활성화 각각에 최적의 **스케일·오프셋**을 학습하거나 추정하고, 양자화 오류를 최소화하기 위해 **양자화 감도 분석**과 **재학습(Quantization‑Aware Training)**을 수행한다. 양자화된 모델은 **메모리 사용량 감소**, **연산 속도 향상**, **정수 연산 기반 하드웨어 가속기**(예: Edge TPU, NPU)에서의 **인퍼런스 가속**을 가능하게 하여 모바일 AI, 임베디드 디바이스, 클라우드 서비스 등 **리소스 제한 환경**에서 널리 활용된다. 양자화는 **프루닝**·**지식 증류**와 같은 다른 모델 압축 기법과 상보적이지만, 프루닝이 구조적 파라미터 수를 줄이는 반면 양자화는 **데이터 표현 방식** 자체를 바꾸어 **다중 정밀도**와 **정수 연산** 중심의 최적화를 제공한다." ;
    rdfs:subClassOf llm:ModelCompression .

llm:SearchProcedure a owl:Class ;
    rdfs:label "SearchProcedure"@en ;
    llm:description "**SearchProcedure**는 **AgentComponent** 계층에 속하는 핵심 모듈로, 에이전트가 사용자 의도나 현재 상태를 기반으로 **벡터 검색**(vector‑based retrieval)을 수행해 가장 관련성 높은 정보를 찾아내는 **검색 절차**를 담당한다. 이 절차는 입력된 프롬프트를 **임베딩**으로 변환하고, 다중 **인덱스**(FAISS, HNSW, ScaNN 등) 중 적절한 것을 선택해 **근사 최근접 이웃(ANN) 검색**, **유사도 스코어링**, **랭킹**을 거쳐 결과를 **Retriever**나 **MemoryComponent** 등 후속 컴포넌트에 전달한다. 대표적인 사용 사례로는 **LLM 기반 대화형 어시스턴트**, **Retrieval‑Augmented Generation(RAG)**, **지식 베이스 질의응답**, **멀티모달 이미지‑텍스트 검색**, **추천 시스템** 등이 있으며, 실시간 검색 효율성 및 대규모 데이터 스케일링을 위해 **동적 검색 전략**과 **프롬프트 엔지니어링**이 결합된다. 관련 개념으로는 **IndexingProcedure**(벡터 인덱스 구축)와 대비되며, **Planner**, **Executor**, **MemoryComponent**와 협업하여 전체 에이전트 파이프라인에서 **검색‑생성‑실행** 루프를 완성한다." ;
    rdfs:subClassOf llm:AgentComponent .

llm:Tokenization a owl:Class ;
    rdfs:label "Tokenization"@en ;
    llm:description "Tokenization은 원시 텍스트를 모델이 이해할 수 있는 discrete 토큰(단어, 서브워드, 문자 등) 시퀀스로 변환하는 텍스트 전처리 단계이며, 자연어 처리(NLP) 파이프라인에서 **ModelComponent**에 해당하는 핵심 모듈이다. 일반적인 토크나이저는 공백 기반 단어 토큰화, BPE(Byte‑Pair Encoding)·WordPiece·SentencePiece와 같은 서브워드 분할, 혹은 문자‑레벨 토큰화를 조합해 어휘 사전(vocabulary)과 매핑 테이블을 구축하고, 토큰 ID를 생성해 임베딩 레이어에 전달한다. 이 과정은 대규모 언어 모델 학습, 텍스트 분류, 기계 번역, 질의‑응답 및 검색 엔진의 인덱싱 등 다양한 적용 분야에서 입력 시퀀스를 정규화하고 효율적인 벡터화(vectorization)를 가능하게 한다. 토큰화와 대비되는 개념으로는 토큰 복원(detokenization), 형태소 분석(lemmatization, stemming), 텍스트 정규화(normalization) 등이 있으며, 토큰 ID와 어휘 매핑은 임베딩(embedding) 및 컨텍스트 인코더와 긴밀히 연계되어 전체 모델 성능을 좌우한다." ;
    rdfs:subClassOf llm:ModelComponent .

llm:AgentArchitecture a owl:Class ;
    rdfs:label "AgentArchitecture"@en ;
    llm:description "**AgentArchitecture**는 대규모 언어 모델(LLM)을 핵심 추론 엔진으로 활용하면서, 목표‑지향적 행동, 도구 호출, 메모리 관리 등을 계층적·모듈식으로 결합한 **AI 에이전트 설계 프레임워크**를 의미합니다. 이 아키텍처는 프롬프트 엔지니어링, ReAct (Reason + Act) 루프, 플래너‑실행기 구조 등으로 **리액티브**·**프론트엔드**·**백엔드** 컴포넌트를 연결해 컨텍스트를 지속적으로 업데이트하고, 외부 API·데이터베이스·시뮬레이션 환경을 동적으로 호출하도록 설계됩니다. 대표적인 사용 사례로는 자동화된 고객지원 챗봇, 복합 업무 스케줄링·보고서 생성 파이프라인, 코드 생성·디버깅 자동화, 그리고 멀티모달 탐색·시뮬레이션 기반 로봇 제어 등이 있으며, LangChain·Auto‑GPT·CrewAI와 같은 **툴‑증강 LLM** 프레임워크가 구체적인 구현 예시입니다. 관련 개념으로는 **단일‑턴 LLM 추론**이나 **정적 프롬프트**와 대비되어, 에이전트 아키텍처는 **지속적인 의사결정 루프**, **계층적 메모리**, **도구 연동**을 통해 보다 자율적이고 복합적인 작업 수행을 가능하게 합니다." ;
    rdfs:subClassOf llm:LLMConcept .

llm:AttentionMechanism a owl:Class ;
    rdfs:label "AttentionMechanism"@en ;
    llm:description "AttentionMechanism은 입력 시퀀스의 각 요소에 대해 동적으로 가중치를 부여해 ‘어디에 집중할지’를 결정하는 ModelComponent 로, 쿼리(query), 키(key), 밸류(value) 삼중 구조와 스케일드 닷‑프로덕트·소프트맥스 연산을 통해 컨텍스트 벡터를 생성한다. 이 메커니즘은 단일 혹은 다중 헤드(multi‑head) 형태로 구현되며, self‑attention은 동일 시퀀스 내의 장거리 의존성을 포착하고, cross‑attention은 인코더‑디코더 구조에서 서로 다른 모달리티(예: 텍스트‑이미지) 간의 정렬(alignment)을 수행한다. 대표적인 적용 분야는 트랜스포머 기반의 기계 번역, 언어 모델링, 텍스트 요약, 비전 트랜스포머(ViT)와 같은 이미지 분류·객체 검출, 음성 인식 및 추천 시스템 등으로, 고차원 임베딩 공간에서 벡터 유사도 검색을 효율적으로 지원한다. RNN이나 CNN과 달리 순차적·지역적 제한이 없으며, 포지셔널 인코딩과 결합해 순서 정보를 보존하면서도 전역적인 컨텍스트를 학습하는 점이 주요 차별점이며, 메모리 네트워크·그래프 어텐션 등과도 연계되어 복합적인 지식 표현을 가능하게 한다." ;
    rdfs:subClassOf llm:ModelComponent .

llm:DecodingStrategy a owl:Class ;
    rdfs:label "DecodingStrategy"@en ;
    llm:description "DecodingStrategy는 대규모 언어 모델이 텍스트를 생성할 때 **어떤 토큰을 선택하고 순서를 결정**하는 방법을 정의하는 핵심 개념으로, InferenceOptimization의 하위 영역에 속합니다. 대표적인 전략으로는 탐욕적(그리디) 디코딩, 빔 서치(Beam Search), 탑‑k/탑‑p 샘플링, 온도 조절 등 다양한 **탐색 공간 제어 메커니즘**이 있으며, 각 전략은 후보 토큰 집합을 어떻게 **필터링·정렬**하고 **다양성·품질** 사이의 트레이드오프를 조정하는지에 따라 동작 원리가 달라집니다. 실시간 챗봇, 자동 요약, 코드 생성 등 **응답 지연을 최소화하면서도 높은 생성 품질을 요구**하는 서비스와, 생성된 텍스트를 임베딩해 **벡터 검색·유사도 매칭**에 활용하는 검색 엔진 파이프라인에서 널리 사용됩니다. 이와 대비되는 개념으로는 **샘플링 기반 디코딩**과 **탐욕적 디코딩**이 있으며, 전자는 다양성을 강조해 검색 결과의 커버리지를 넓히고, 후자는 연산 비용을 최소화해 추론 속도를 극대화하는 차이점이 있습니다." ;
    rdfs:subClassOf llm:InferenceOptimization .

llm:EncoderDecoder a owl:Class ;
    rdfs:label "EncoderDecoder"@en ;
    llm:description "**Encoder‑Decoder**(인코더‑디코더) 아키텍처는 입력 데이터를 **인코더**가 고차원 잠재 표현(latent vector)으로 압축하고, **디코더**가 그 표현을 기반으로 원하는 출력 시퀀스를 순차적으로 생성하는 **시퀀스‑투‑시퀀스(seq2seq)** 모델 구조를 말합니다. 인코더는 양방향 RNN·LSTM·GRU 혹은 self‑attention Transformer와 같은 네트워크로 전체 입력을 처리해 컨텍스트 벡터를 만들고, 디코더는 단계별로 토큰을 예측하면서 **attention 메커니즘**을 통해 인코더의 관련 상태에 집중하고, 학습 시에는 **teacher forcing** 기법을 활용해 효율적인 조건부 생성(conditioned generation)을 수행합니다. 이 구조는 **기계 번역, 텍스트 요약, 음성‑텍스트 변환, 이미지 캡션 생성, 다중 모달(멀티모달) 콘텐츠 생성** 등 다양한 자연어 처리·컴퓨터 비전·음성 인식 응용 분야에서 핵심으로 사용되며, 최신 모델인 **Transformer‑기반 T5, BART, MarianMT** 등이 대표적인 예시입니다. 관련 개념으로는 **인코더‑전용 모델**(예: BERT)과 **디코더‑전용 모델**(예: GPT)처럼 입력 표현만 제공하거나 순차적 생성만 수행하는 구조와 구별되며, **오토인코더**와는 달리 인코더‑디코더는 **조건부 생성**을 목표로 한다는 점이 특징입니다." ;
    rdfs:subClassOf llm:ModelArchitecture .

llm:EncoderOnly a owl:Class ;
    rdfs:label "EncoderOnly"@en ;
    llm:description "**EncoderOnly**는 입력 시퀀스를 **인코더만** 사용해 고차원 표현(embedding)으로 변환하는 **모델 아키텍처**로, 주로 **Transformer 인코더** 블록과 **self‑attention** 메커니즘을 기반으로 동작합니다. 입력 토큰은 양방향(self‑attention)으로 동시에 처리되어 **문맥‑민감한 토큰 임베딩**과 **문장 수준 벡터**를 생성하며, 별도의 디코더가 없기 때문에 **오토레그레시브(autoregressive) 생성** 기능은 제공되지 않습니다. 대표적인 활용 사례로는 **BERT**, **RoBERTa**, **ALBERT**와 같은 **사전학습(pre‑training) 언어 모델**을 이용한 **텍스트 분류**, **질문‑응답**, **문서 검색**, **문장 임베딩** 등 **자연어 이해(NLU)** 작업이 있으며, 특히 **정보 검색**이나 **유사도 기반 검색**에서 **벡터 검색**에 최적화된 표현을 얻는 데 강점이 있습니다. 이와 대비되는 개념으로는 **Encoder‑Decoder**(예: T5, BART)와 **DecoderOnly**(예: GPT 시리즈) 아키텍처가 있으며, 전자는 **seq2seq 생성**에, 후자는 **자연어 생성(NLG)**에 특화된 구조로 구분됩니다." ;
    rdfs:subClassOf llm:ModelArchitecture .

llm:ApplicationDomain a owl:Class ;
    rdfs:label "ApplicationDomain"@en ;
    llm:description "**ApplicationDomain**은 대규모 언어 모델(LLM)이 실제로 적용되는 **특정 분야·산업·문제 영역**을 의미하며, 모델이 다루는 지식·용어·규칙이 해당 도메인에 맞게 제한·정제되는 개념이다. 이러한 도메인은 **도메인‑특화 데이터셋, 프롬프트 설계, 파인튜닝** 등을 통해 LLM이 해당 분야의 전문 용어와 컨텍스트를 정확히 이해하고 생성하도록 **도메인 어댑테이션(domain adaptation)** 메커니즘으로 작동한다. 대표적인 사용 사례로는 **법률·의료·금융·게임·교육** 등 각 산업별 챗봇, 문서 요약, 자동 보고서 작성, 규정 준수 검증 등 **업무 자동화와 지식 기반 서비스**가 있으며, 특히 **전문 지식 검색·시맨틱 검색**과 결합해 높은 정확도의 답변을 제공한다. 관련 개념으로는 **Task‑Oriented Dialogue, Fine‑Tuning, Prompt Engineering, Multi‑Domain Learning** 등이 있으며, ApplicationDomain은 **범용(General‑Purpose) LLM**과 달리 **도메인‑특화(Vertical) 모델**이라는 점에서 차별화된다." ;
    rdfs:subClassOf llm:LLMConcept .

llm:KnowledgeIntegration a owl:Class ;
    rdfs:label "KnowledgeIntegration"@en ;
    llm:description "**KnowledgeIntegration**은 대규모 언어 모델(LLM)이 외부 지식 그래프, 구조화된 데이터베이스, 멀티모달 콘텐츠 등 다양한 지식 소스를 실시간으로 융합하여 응답에 반영하도록 설계된 메커니즘을 의미합니다. 이 과정은 **컨텍스트 융합**·**지식 매핑**·**임베딩 정렬**을 통해 외부 지식의 의미적 표현을 LLM의 내부 토큰 공간에 매핑하고, **프롬프트 엔지니어링**이나 **RAG(Retrieval‑Augmented Generation)**와 같은 검색‑생성 파이프라인을 활용해 필요한 정보를 동적으로 호출·통합합니다. 대표적인 사용 사례로는 **지식 기반 질의응답**, **전문 분야 챗봇(법률·의료·금융)**, **멀티모달 설명 생성** 및 **지식 증강을 통한 코드 자동완성** 등이 있으며, 이러한 응용은 **지식 그래프 임베딩**, **지식 증류**, **지속적 학습(Continual Learning)**과 결합해 최신 정보를 지속적으로 반영합니다. 관련 개념으로는 **단순 파인튜닝**이나 **전통적인 검색 기반 시스템**과 달리, KnowledgeIntegration은 **지식 재활용**·**지식 전이**를 강조하며, **프롬프트 기반 검색**과 **벡터 검색**을 결합한 **하이브리드 인포메이션 리트리벌** 방식과 대비됩니다." ;
    rdfs:subClassOf llm:LLMConcept .

llm:DecoderOnly a owl:Class ;
    rdfs:label "DecoderOnly"@en ;
    llm:description "**DecoderOnly**는 입력 토큰을 순차적으로 받아들여 바로 다음 토큰을 예측하는 **디코더 전용(Decoder‑Only) 모델 아키텍처**를 의미하며, 주로 **자기 회귀(autoregressive) 방식**의 **Transformer** 구조를 기반으로 합니다. 이 아키텍처는 **인코더‑디코더(Encoder‑Decoder)**와 달리 **인코더 스택이 없고, 오직 다중 헤드 셀프‑어텐션과 피드포워드 레이어만으로 구성된 디코더 레이어**가 여러 겹 쌓여 입력 시퀀스에 **인과적(causal) 마스킹**을 적용해 미래 토큰을 차단함으로써 순차적인 생성 과정을 구현합니다. 대표적인 사용 사례로는 **GPT‑계열 대형 언어 모델(LLM)**, **대화형 AI 챗봇**, **코드 자동 완성**, **텍스트 요약·번역·스토리 생성** 등 **텍스트 생성 및 시퀀스 예측** 작업에 널리 활용되며, 사전 학습(pre‑training) 후 특정 도메인에 **파인‑튜닝(fine‑tuning)**을 적용해 맞춤형 생성 모델을 만들 수 있습니다. 관련 개념으로는 **EncoderOnly(예: BERT, RoBERTa)**와 **Encoder‑Decoder(예: T5, BART)**가 있으며, DecoderOnly는 **입력‑출력 쌍을 동시에 처리하지 않고 순차적인 출력에 초점을 맞춘다는 점**에서 이들 아키텍처와 구별됩니다." ;
    rdfs:subClassOf llm:ModelArchitecture .

llm:ModelComponent a owl:Class ;
    rdfs:label "ModelComponent"@en ;
    llm:description """**ModelComponent**는 LLM(대규모 언어 모델) 아키텍처 내에서 특정 기능(예: 토크나이징, 임베딩, 어텐션, 디코딩 등)을 캡슐화한 모듈화된 단위로, 표준화된 인터페이스를 통해 독립적으로 학습·파인튜닝·교체·조합될 수 있는 구성 요소를 의미합니다.  
이러한 컴포넌트는 파라미터 공유와 재사용성을 극대화하도록 설계되어 동적 인퍼런스 파이프라인을 구성하고, 컨텍스트 관리·멀티모달 처리·추론 최적화와 같은 작업을 효율적으로 연결시킵니다.  
실제 사용 사례로는 프롬프트 엔지니어링 플랫폼에서 검색‑증강 생성(RAG) 파이프라인에 리트리버·리랭커·안전 필터 컴포넌트를 플러그인 형태로 삽입하거나, 멀티모달 LLM 시스템에서 텍스트·이미지·오디오 처리 모듈을 조합해 맞춤형 서비스 구축에 활용됩니다.  
관련 개념으로는 단일·비모듈형 LLM과 대비되는 “LLMModule”, “Adapter”, “Plugin”, “Pipeline” 등이 있으며, 모델 컴포넌트는 LLMConcept 하위에서 컴포넌트‑기반 AI 설계·스케일링을 지원하는 핵심 요소로 자리잡고 있습니다.""" ;
    rdfs:subClassOf llm:LLMConcept .

llm:SafetyTechnique a owl:Class ;
    rdfs:label "SafetyTechnique"@en ;
    llm:description "**SafetyTechnique**는 인공지능 시스템이 의도치 않은 해를 초래하지 않도록 위험을 사전에 탐지·제어하고, 인간 가치와 목표에 일관되게 동작하도록 보장하는 일련의 설계·운영 방법을 말합니다. 이러한 기법은 **위험 완화(risk mitigation)**, **적대적 훈련(adversarial training)**, **레드 팀링(red teaming)**, **인간‑인‑루프(Human‑in‑the‑Loop) 검증**, **콘텐츠 필터링(content filtering)**, **모델 해석성(model interpretability)** 등으로 구성되며, 데이터 전처리부터 배포 후 모니터링까지 전 과정에 걸쳐 **안전성(safety)**과 **정렬(alignment)**을 지속적으로 검증합니다. 대표적인 적용 분야로는 **대화형 챗봇의 부적절 발언 억제**, **자율주행 차량의 충돌 회피**, **의료 진단 AI의 오류 방지**, 그리고 **소셜 미디어 플랫폼의 악성 콘텐츠 차단** 등이 있으며, 각 도메인별 위험 프로파일에 맞춘 맞춤형 안전 기법이 활용됩니다. 이와 대비되는 개념으로는 **성능 최적화(performance optimization)**나 **스케일링(scale‑up)**에 초점을 맞춘 **Capability‑centric techniques**가 있으며, SafetyTechnique은 **AI 정렬(AI alignment)**·**AI 윤리(AI ethics)**·**거버넌스(AI governance)**와 긴밀히 연계되어 전체 시스템의 신뢰성을 높이는 역할을 합니다." ;
    rdfs:subClassOf llm:SafetyAndAlignment .

llm:TrainingParadigm a owl:Class ;
    rdfs:label "TrainingParadigm"@en ;
    llm:description "**TrainingParadigm**은 대규모 언어 모델(LLM)이 데이터를 통해 지식을 습득하고 일반화 능력을 형성하는 **학습 패러다임**을 의미한다. 전통적인 지도학습과 달리 사전학습(pre‑training), 인스트럭션 튜닝(instruction tuning), 강화학습 기반 인간 피드백(RLHF), 멀티태스크 학습, 지속학습(continual learning) 등 **self‑supervised learning**과 **few‑shot/zero‑shot** 전이 학습을 결합해 모델이 다양한 작업을 스스로 추론하도록 설계된다. 이 패러다임은 챗봇, 코드 생성, 검색‑보강 생성(search‑augmented generation), 의료 텍스트 분석 등 **다양한 도메인**에서 **프롬프트 엔지니어링** 없이도 높은 성능을 발휘하도록 활용된다. 관련 개념으로는 **Fine‑tuning**, **Prompt‑based learning**, **Zero‑shot learning**이 있으며, 각각은 데이터 규모·라벨링 비용·모델 적응 방식에서 **TrainingParadigm**과 차별화된다." ;
    rdfs:subClassOf llm:LLMConcept .

llm:AgenticTask a owl:Class ;
    rdfs:label "AgenticTask"@en ;
    llm:description "AgenticTask는 인공지능 에이전트가 스스로 목표를 설정하고, 작업을 분해·계획·실행까지 전 과정을 자율적으로 관리하는 **응용 분야(ApplicationDomain) 내의 고차원 작업 단위**를 의미한다. 이러한 작업은 LLM·강화학습 기반의 **자율적 의사결정 메커니즘**, **동적 작업 할당** 및 **멀티에이전트 협업**을 통해 실시간으로 상황에 맞는 전략을 선택하고, 필요 시 인간‑인더‑루프(Human‑in‑the‑Loop) 피드백을 통합한다. 대표적인 사용 사례로는 복합 비즈니스 워크플로우 자동화, 스마트 팩토리에서의 생산 라인 최적화, 고객 맞춤형 서비스 챗봇의 다단계 문제 해결, 그리고 의료 진단 파이프라인에서의 데이터 전처리·분석·보고서 생성 등이 있다. 이와 대비되는 개념으로는 **정적 작업 자동화(Static Task Automation)**나 **단일 에이전트 스크립트**가 있으며, AgenticTask는 **목표 지향적 행동(goal‑oriented behavior)**과 **동적 적응(Dynamic Adaptation)**을 핵심으로 하는 **자율 에이전트(Autonomous Agent)**·**작업 오케스트레이션(Task Orchestration)** 분야와 밀접하게 연관된다." ;
    rdfs:subClassOf llm:ApplicationDomain .

llm:AdvancedPrompting a owl:Class ;
    rdfs:label "AdvancedPrompting"@en ;
    llm:description "**AdvancedPrompting**은 단순한 텍스트 입력을 넘어, **컨텍스트 관리**, **조건부 프롬프트**·**메타프롬프트** 설계, **프롬프트 체이닝**·**시퀀스 프롬프트**와 같은 다단계 전략을 활용해 대형 언어 모델(LLM)의 출력 품질을 최적화하는 **프롬프트 엔지니어링** 기법이다. 이 방식은 **few‑shot**·**zero‑shot** 학습을 보강하고, **체인‑오브‑쓰스(Chain‑of‑Thought)**·**셀프‑컨시스턴시(Self‑Consistency)**와 같은 사고 흐름을 명시적으로 유도함으로써 복잡한 논리 추론, 코드 생성, 다중턴 대화 등에서 모델이 단계별로 목표를 달성하도록 만든다. 대표적인 사용 사례로는 **AI 어시스턴트**의 **다중 단계 플래닝**, **전문 도메인 QA**·**데이터 추출**, **자동화된 콘텐츠 제작** 및 **프로그래밍 지원** 등이 있으며, 특히 **인스트럭션 튜닝**이나 **프롬프트 최적화**가 필요한 기업용 애플리케이션에서 효과가 두드러진다. 기본적인 **단일 프롬프트** 방식과 달리 AdvancedPrompting은 **프롬프트 주입(Prompt Injection)** 위험을 최소화하고, **동적 컨텍스트 조정**과 **조건부 로직**을 통해 보다 견고하고 일관된 결과를 제공한다." ;
    rdfs:subClassOf llm:PromptingTechnique .

llm:LLMConcept a owl:Class ;
    rdfs:label "LLMConcept"@en ;
    llm:description "LLMConcept은 대규모 언어 모델(LLM)의 구조·동작·활용 방식을 체계적으로 정의한 개념 프레임워크로, 트랜스포머 기반의 self‑attention 메커니즘과 수십억 토큰 규모의 사전학습(corpus)으로부터 얻은 텍스트 임베딩을 핵심으로 한다. 이 개념은 프롬프트 엔지니어링, 파인튜닝, 컨텍스트 윈도우 확장, 그리고 검색‑증강 생성(RAG)과 같은 주요 특징을 포함해, 토큰 단위의 인퍼런스와 시맨틱 벡터 기반 유사도 검색(FAISS·ANN 등)을 통해 실시간 응답을 제공한다. 대표적인 사용 사례로는 AI 어시스턴트·챗봇, 코드 자동 생성, 문서 요약·핵심 정보 추출, 그리고 대규모 시맨틱 검색·지식 그래프 구축 등 다양한 자연어 처리(NLP)와 멀티모달 응용 분야가 있다. LLMConcept은 전통적인 규칙 기반 NLP나 통계적 n‑gram 모델과 대비되어, 고차원 벡터 공간에서의 의미론적 연산과 대규모 파라미터 학습을 통해 보다 풍부하고 유연한 언어 이해·생성을 가능하게 한다." .

llm:AgentComponent a owl:Class ;
    rdfs:label "AgentComponent"@en ;
    llm:description "**AgentComponent**는 **ModelComponent**의 하위 모듈로, 인공지능 에이전트가 환경과 상호작용하면서 **행동 선택**·**상태 관리**·**보상 계산** 등을 수행하도록 설계된 컴포넌트이다. 이 컴포넌트는 **플러그인 아키텍처**와 **컴포넌트 기반 설계** 원칙에 따라, 정책 네트워크, 가치 함수, 탐색 전략 등 여러 **서브 모듈**을 동적으로 연결하고, 입력된 관측값을 토대로 실시간으로 의사결정을 내리는 **작동 원리**를 갖는다. 대표적인 사용 사례로는 **강화학습 기반 로봇 제어**, **멀티에이전트 시뮬레이션**, **게임 AI** 및 **자율 주행 차량**의 행동 모듈 구현 등이 있으며, 이러한 분야에서 에이전트의 **학습·추론 파이프라인**을 효율적으로 관리한다. 관련 개념으로는 **ServiceComponent**(주로 데이터 제공·처리 기능을 담당)와 대비되며, **PolicyComponent**, **EnvironmentComponent**와 같은 다른 모델 컴포넌트와 협업하여 전체 시스템의 **모듈화·재사용성**을 높인다." ;
    rdfs:subClassOf llm:ModelComponent .

llm:AutomaticMetric a owl:Class ;
    rdfs:label "AutomaticMetric"@en ;
    llm:description "**AutomaticMetric**는 인간의 직접적인 개입 없이 알고리즘적으로 계산되는 **EvaluationMetric**의 한 종류로, 모델 출력과 사전 정의된 기준(예: 정답 레이블, 레퍼런스 코퍼스, 혹은 프록시 신호) 사이의 일치 정도를 수치화한다. 이러한 메트릭은 정해진 수식이나 통계적 절차에 따라 작동해 **스케일러블**하고 **재현 가능**하며, 학습 파이프라인에 실시간으로 삽입해 **자동화된 모델 평가**와 **베치마크**를 가능하게 한다. 대표적인 사용 사례로는 NLP 분야의 **BLEU·ROUGE·METEOR**, 컴퓨터 비전의 **mAP·IoU**, 음성 인식의 **WER**, 추천 시스템의 **NDCG** 등 다양한 도메인에서 **정량적 성능 측정**과 **온라인 A/B 테스트**에 활용된다. 이와 달리 **HumanEvaluation**(주관적·비용·시간 소모)과 대비되며, **Intrinsic vs. Extrinsic** 평가, **bias‑correction**, **human‑in‑the‑loop** 보정 등과 결합해 보다 포괄적인 **모델 평가 프레임워크**를 구성한다." ;
    rdfs:subClassOf llm:EvaluationMetric .

