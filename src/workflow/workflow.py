"""
LangGraph ì›Œí¬í”Œë¡œìš° ì •ì˜.

PDF â†’ TOC ê¸°ë°˜ ì±•í„°/ì„¹ì…˜ ê°ì§€ â†’ ë¬¸ë‹¨ ë¶„í•  â†’ ì•„ì´ë””ì–´ ì¶”ì¶œ
"""
from typing import Optional
from tqdm import tqdm
from langgraph.graph import StateGraph, END

from src.workflow.state import PipelineState, create_initial_state
from src.workflow.nodes import (
    # ê¸°ì¡´ íŒŒì´í”„ë¼ì¸ ë…¸ë“œ
    extract_text,
    detect_structure,
    create_book_node,
    process_section,
    route_sections,
    finalize,
    # dedup ê´€ë ¨ ë…¸ë“œ
    chunk_paragraphs,
    extract_idea,
    check_idea_duplicate,
    check_chunk_duplicate,
    save_to_db,
)
from src.model.schemas import DetectedChapter, DetectedSection
from src.utils.pdf.hierarchy_detector import (
    detect_chapters_from_toc,
    build_hierarchy_path,
    get_leaf_sections,
)

from src.db.connection import get_session
from src.db.models import Book
from src.db.operations import (
    create_book,
    get_book_by_title,
    create_chapter_from_llm,
    save_all_sections_recursive,
    reset_chapter_counter,
)


# ============================================================
# ê¸°ì¡´ PDF íŒŒì´í”„ë¼ì¸ (LangGraph ê¸°ë°˜)
# ============================================================

def create_pdf_pipeline() -> StateGraph:
    workflow = StateGraph(PipelineState)

    # ë…¸ë“œ ì¶”ê°€
    workflow.add_node("extract_text", extract_text)
    workflow.add_node("detect_structure", detect_structure)
    workflow.add_node("create_book", create_book_node)
    workflow.add_node("process_section", process_section)
    workflow.add_node("finalize", finalize)

    # ì—£ì§€ ì •ì˜: ìˆœì°¨ íë¦„
    workflow.set_entry_point("extract_text")
    workflow.add_edge("extract_text", "detect_structure")
    workflow.add_edge("detect_structure", "create_book")
    workflow.add_edge("create_book", "process_section")

    # ì¡°ê±´ë¶€ ë¼ìš°íŒ…: ì„¹ì…˜ ìˆœíšŒ ë£¨í”„
    workflow.add_conditional_edges(
        "process_section",
        route_sections,
        {
            "continue": "process_section",  # ë‹¤ìŒ ì„¹ì…˜ ì²˜ë¦¬
            "finalize": "finalize",         # ëª¨ë“  ì„¹ì…˜ ì™„ë£Œ
        }
    )

    workflow.add_edge("finalize", END)

    return workflow.compile()


# ì»´íŒŒì¼ëœ íŒŒì´í”„ë¼ì¸ ê·¸ë˜í”„ ì¸ìŠ¤í„´ìŠ¤
pdf_pipeline = create_pdf_pipeline()


# ============================================================
# ì•„ì´ë””ì–´ ì¶”ì¶œ ê·¸ë˜í”„ (ì¤‘ë³µ ì²´í¬ í¬í•¨)
# ============================================================

def _route_after_duplicate_check(state: PipelineState) -> str:
    """ì¤‘ë³µ ì²´í¬ í›„ ë¼ìš°íŒ…."""
    is_duplicate = state.get("is_duplicate", False)
    return "skip" if is_duplicate else "save"

def _route_after_chunk_check(state: PipelineState) -> str:
    """ì²­í¬ ì¤‘ë³µ ì²´í¬ í›„ ë¼ìš°íŒ…"""
    return "skip" if state.get("is_chunk_duplicate", False) else "extract"

def _route_after_idea_check(state: PipelineState) -> str:
    """ì•„ì´ë””ì–´ ì¤‘ë³µ ì²´í¬ í›„ ë¼ìš°íŒ…"""
    return "skip" if state.get("is_idea_duplicate", False) else "save"


def _skip_duplicate(state: PipelineState) -> PipelineState:
    """ì¤‘ë³µ ìŠ¤í‚µ (no-op)."""
    return state


def create_idea_extraction_graph() -> StateGraph:
    """
    ì•„ì´ë””ì–´ ì¶”ì¶œ ê·¸ë˜í”„ ìƒì„±.

    ì›Œí¬í”Œë¡œìš°:
    ```
    [check_chunk_dup]
         â”‚
         â”œâ”€â”€ (ì¤‘ë³µ) â”€â”€â†’ [skip] â”€â”€â†’ END
         â”‚
         â””â”€â”€ (ì‹ ê·œ) â”€â”€â†’ [extract] â”€â”€â†’ [check_idea_dup]
                                           â”‚
                                           â”œâ”€â”€ (ì¤‘ë³µ) â”€â”€â†’ [skip] â”€â”€â†’ END
                                           â”‚
                                           â””â”€â”€ (ì‹ ê·œ) â”€â”€â†’ [save] â”€â”€â†’ END
    ```
    """
    workflow = StateGraph(PipelineState)

    workflow.add_node("check_chunk_dup", check_chunk_duplicate)
    workflow.add_node("extract", extract_idea)
    workflow.add_node("check_idea_dup", check_idea_duplicate)
    workflow.add_node("save", save_to_db)
    workflow.add_node("skip", _skip_duplicate)

    workflow.set_entry_point("check_chunk_dup")

    # ì²­í¬ ì¤‘ë³µ ì²´í¬ í›„ ë¼ìš°íŒ…
    workflow.add_conditional_edges(
        "check_chunk_dup",
        _route_after_chunk_check,
        {"skip": "skip", "extract": "extract"}
    )

    workflow.add_edge("extract", "check_idea_dup")

    # ì•„ì´ë””ì–´ ì¤‘ë³µ ì²´í¬ í›„ ë¼ìš°íŒ…
    workflow.add_conditional_edges(
        "check_idea_dup",
        _route_after_idea_check,
        {"skip": "skip", "save": "save"}
    )

    workflow.add_edge("save", END)
    workflow.add_edge("skip", END)

    return workflow.compile()


# ì»´íŒŒì¼ëœ ê·¸ë˜í”„ ì¸ìŠ¤í„´ìŠ¤
idea_extraction_graph = create_idea_extraction_graph()


# ============================================================
# PDF íŒŒì´í”„ë¼ì¸ ì‹¤í–‰ (TOC ê¸°ë°˜ + ì¤‘ë³µì œê±°)
# ============================================================

def run_pdf_pipeline(
    pdf_path: str,
    resume: bool = False,
    book_id: Optional[int] = None,
    model_version: str = "gemini-2.5-flash",
    enable_semantic_dedup: bool = False,
    semantic_threshold: float = 0.95,
) -> dict:
    """
    PDF íŒŒì´í”„ë¼ì¸ ì‹¤í–‰ (TOC ê¸°ë°˜).

    1. PDF â†’ Plain Text + TOC ì¶”ì¶œ (pymupdf)
    2. TOC ê¸°ë°˜ ì±•í„°/ì„¹ì…˜ êµ¬ì¡° ìƒì„± (LLM ë¶ˆí•„ìš”)
    3. ë§ë‹¨ ì„¹ì…˜ë³„ ë¬¸ë‹¨ ë¶„í•  ë° ì•„ì´ë””ì–´ ì¶”ì¶œ (LLM ì‚¬ìš©)

    Args:
        pdf_path: PDF íŒŒì¼ ê²½ë¡œ
        resume: ì´ì „ ì§„í–‰ ìƒí™©ì—ì„œ ì¬ê°œ ì—¬ë¶€
        book_id: ê¸°ì¡´ ì±… ID (ì¬ê°œ ì‹œ ì‚¬ìš©)
        model_version: LLM ëª¨ë¸ ë²„ì „
        enable_semantic_dedup: ì„ë² ë”© ê¸°ë°˜ ì¤‘ë³µì œê±° í™œì„±í™” (ê¸°ë³¸ False)
        semantic_threshold: ì½”ì‚¬ì¸ ìœ ì‚¬ë„ ì„ê³„ê°’ (ê¸°ë³¸ 0.95)
    """
    session = get_session()

    try:
        # Step 1: ì´ˆê¸° ìƒíƒœ ìƒì„± ë° í…ìŠ¤íŠ¸/TOC ì¶”ì¶œ
        initial_state = create_initial_state(
            pdf_path=pdf_path,
            book_id=book_id,
            resume=resume,
            model_version=model_version,
        )

        # ì„ë² ë”© ì¤‘ë³µì œê±° ì˜µì…˜ ì„¤ì •
        initial_state["enable_semantic_dedup"] = enable_semantic_dedup
        initial_state["semantic_threshold"] = semantic_threshold

        print("ğŸ“„ PDF â†’ Plain Text + TOC ì¶”ì¶œ ì¤‘...")
        state = extract_text(initial_state)

        if state.get("error"):
            return {"error": state["error"]}

        plain_text = state.get("plain_text", "")
        toc = state.get("toc", [])
        has_toc = state.get("has_toc", False)
        page_positions = state.get("page_positions", [])

        print(f"   â†’ {len(plain_text):,} ë¬¸ì ì¶”ì¶œë¨")
        print(f"   â†’ TOC í•­ëª©: {len(toc)}ê°œ {'âœ…' if has_toc else 'âŒ (TOC ì—†ìŒ)'}")

        # Step 2: TOC ê¸°ë°˜ ì±•í„°/ì„¹ì…˜ êµ¬ì¡° ìƒì„±
        if not has_toc:
            return {"error": "PDFì— TOCê°€ ì—†ìŠµë‹ˆë‹¤. TOCê°€ ìˆëŠ” PDFë§Œ ì§€ì›ë©ë‹ˆë‹¤."}

        print("ğŸ” TOC ê¸°ë°˜ ì±•í„°/ì„¹ì…˜ ê°ì§€ ì¤‘...")
        chapters = detect_chapters_from_toc(
            pdf_path=pdf_path,
            plain_text=plain_text,
            page_positions=page_positions,
        )
        print(f"   â†’ {len(chapters)}ê°œ ì±•í„° ê°ì§€ë¨")

        # ì„¹ì…˜ ìˆ˜ ì¶œë ¥
        for chapter in chapters:
            section_count = _count_sections(chapter.sections)
            print(f"   # {chapter.title} ({section_count} sections)")

        state["chapters"] = chapters

        # Step 3: DBì— ì±… ìƒì„±
        book = _create_book(session, state)
        state["book_id"] = book.id

        # Step 4: ì±•í„°/ì„¹ì…˜ë³„ ì²˜ë¦¬
        stats = _process_toc_chapters(
            session=session,
            state=state,
            book=book,
            chapters=chapters,
        )

        _print_summary(stats)
        return stats

    finally:
        session.close()


def run_pdf_pipeline_simple(
    pdf_path: str,
    resume: bool = False,
    book_id: Optional[int] = None,
    model_version: str = "gemini-2.5-flash",
) -> dict:
    """
    ë‹¨ì¼ LangGraphë¥¼ í†µí•´ ì „ì²´ PDF ì²˜ë¦¬ ìˆ˜í–‰ (ê¸°ì¡´ ë°©ì‹):
    1. PDF â†’ Plain Text + TOC ì¶”ì¶œ
    2. TOC ê¸°ë°˜ ì±•í„°/ì„¹ì…˜ êµ¬ì¡° ê°ì§€
    3. DBì— ì±…/ì±•í„°/ì„¹ì…˜ ì €ì¥
    4. ê° ì„¹ì…˜ë³„ ë¬¸ë‹¨ ë¶„í•  ë° ì•„ì´ë””ì–´ ì¶”ì¶œ
    5. ì²˜ë¦¬ ê²°ê³¼ ìš”ì•½
    """
    # ì´ˆê¸° ìƒíƒœ ìƒì„±
    initial_state = create_initial_state(
        pdf_path=pdf_path,
        book_id=book_id,
        resume=resume,
        model_version=model_version,
    )

    print(f"ğŸ“„ PDF íŒŒì´í”„ë¼ì¸ ì‹œì‘: {pdf_path}")

    # ê·¸ë˜í”„ ì‹¤í–‰ (ì„¹ì…˜ ìˆ˜ + ì—¬ìœ ë¶„ìœ¼ë¡œ recursion_limit ì„¤ì •)
    result_state = pdf_pipeline.invoke(
        initial_state,
        config={"recursion_limit": 500}  # ìµœëŒ€ 500ê°œ ì„¹ì…˜ê¹Œì§€ ì§€ì›
    )

    # ì—ëŸ¬ ì²´í¬
    if result_state.get("error"):
        return {"error": result_state["error"]}

    return result_state.get("stats", {})


# ============================================================
# í—¬í¼ í•¨ìˆ˜ë“¤
# ============================================================

def _count_sections(sections: list[DetectedSection]) -> int:
    """ì„¹ì…˜ ìˆ˜ ì¬ê·€ ì¹´ìš´íŠ¸."""
    count = len(sections)
    for section in sections:
        count += _count_sections(section.children)
    return count


def _create_book(session, state: PipelineState) -> Book:
    """ì±… ìƒì„±."""
    metadata = state.get("metadata", {})
    pdf_path = state.get("pdf_path", "")

    title = metadata.get("title") or pdf_path.split("/")[-1].replace(".pdf", "")
    author = metadata.get("author") or "Unknown"

    existing = get_book_by_title(session, title)
    if existing:
        print(f"âš ï¸  '{title}' ì´ë¯¸ ì¡´ì¬ (ID: {existing.id})")
        raise ValueError("ì±…ì´ ì´ë¯¸ ì¡´ì¬í•¨")

    # ì±•í„° ì¹´ìš´í„° ì´ˆê¸°í™”
    reset_chapter_counter()

    book = create_book(
        session,
        title=title,
        author=author,
        source_path=pdf_path,
    )
    print(f"âœ… ì±… ìƒì„± ì™„ë£Œ: '{title}' (ID: {book.id})")

    return book


def _process_toc_chapters(
    session,
    state: PipelineState,
    book: Book,
    chapters: list[DetectedChapter],
) -> dict:
    """TOC ê¸°ë°˜ ì±•í„°/ì„¹ì…˜ ì²˜ë¦¬."""
    stats = {
        "total_chapters": len(chapters),
        "total_sections": 0,
        "completed_chapters": 0,
        "failed_chapters": 0,
        "total_paragraphs": 0,
        "total_ideas": 0,
        "duplicates_skipped": 0,
        "detection_method": "toc",
    }

    for chapter in tqdm(chapters, desc="ğŸ“– ì±•í„° ì²˜ë¦¬"):
        try:
            # DBì— ì±•í„° ì €ì¥
            db_chapter = create_chapter_from_llm(
                session=session,
                book_id=book.id,
                chapter=chapter,
            )

            # ëª¨ë“  ì„¹ì…˜ì„ ì¬ê·€ì ìœ¼ë¡œ DBì— ë¨¼ì € ì €ì¥
            section_id_map = save_all_sections_recursive(
                session=session,
                chapter_id=db_chapter.id,
                book_id=book.id,
                sections=chapter.sections,
            )

            # ë§ë‹¨ ì„¹ì…˜ë§Œ ì²˜ë¦¬ (ë¬¸ë‹¨ ë¶„í•  + ì•„ì´ë””ì–´ ì¶”ì¶œ)
            leaf_sections = get_leaf_sections(chapter)
            stats["total_sections"] += len(leaf_sections)

            for section, hierarchy_path in leaf_sections:
                # section_id_mapì—ì„œ ID ì¡°íšŒ (ì´ë¯¸ ì €ì¥ë¨)
                section_id = section_id_map.get(section.title)

                _process_section(
                    state=state,
                    book=book,
                    db_chapter=db_chapter,
                    chapter=chapter,
                    section=section,
                    hierarchy_path=hierarchy_path,
                    section_id=section_id,
                    stats=stats,
                )

            stats["completed_chapters"] += 1

        except Exception as e:
            stats["failed_chapters"] += 1
            print(f"\nâŒ ì±•í„° '{chapter.title}' ì‹¤íŒ¨: {e}")

    return stats


def _process_section(
    state: PipelineState,
    book: Book,
    db_chapter,
    chapter: DetectedChapter,
    section: DetectedSection,
    hierarchy_path: str,
    section_id: Optional[int],
    stats: dict,
) -> None:
    """ì„¹ì…˜ ì²˜ë¦¬: ì²­í‚¹ â†’ ì•„ì´ë””ì–´ ì¶”ì¶œ."""
    section_text = section.content

    if len(section_text.strip()) < 100:
        return

    # ì„¹ì…˜ì€ ì´ë¯¸ save_all_sections_recursive()ì—ì„œ ì €ì¥ë¨
    # section_idë¥¼ íŒŒë¼ë¯¸í„°ë¡œ ë°›ì•„ì„œ ì‚¬ìš©

    # ì„¹ì…˜ ìƒíƒœ ìƒì„±
    section_state = {
        **state,
        "current_chapter": chapter,
        "current_section": section,
        "current_section_text": section_text,
        "current_chapter_id": db_chapter.id,
        "current_section_id": section_id,
        "hierarchy_path": hierarchy_path,
        "book_id": book.id,
    }

    # ì²­í‚¹ ìˆ˜í–‰ (LLM ê¸°ë°˜ ë¬¸ë‹¨ ë¶„í• )
    section_state = chunk_paragraphs(section_state)
    chunks = section_state.get("chunks", [])
    stats["total_paragraphs"] += len(chunks)

    # ê° ì²­í¬ì— ëŒ€í•´ ì•„ì´ë””ì–´ ì¶”ì¶œ
    for chunk in chunks:
        # ì²­í¬ì— section_id ì„¤ì •
        chunk.section_id = section_id

        chunk_state = {
            **section_state,
            "current_chunk": chunk,
        }

        # ì•„ì´ë””ì–´ ì¶”ì¶œ ê·¸ë˜í”„ ì‹¤í–‰
        result_state = idea_extraction_graph.invoke(chunk_state)

        if result_state.get("extracted_idea") and not result_state.get("is_duplicate"):
            stats["total_ideas"] += 1
        if result_state.get("is_duplicate"):
            stats["duplicates_skipped"] += 1


def _print_summary(stats: dict) -> None:
    """ì²˜ë¦¬ ìš”ì•½ ì¶œë ¥."""
    print("\n" + "=" * 60)
    print("ğŸ“Š ì²˜ë¦¬ ìš”ì•½")
    print("=" * 60)
    print(f"ê°ì§€ ë°©ë²•: {stats.get('detection_method', 'toc')}")
    print(f"ì´ ì±•í„°: {stats.get('total_chapters', 0)}")
    print(f"ì´ ì„¹ì…˜: {stats.get('total_sections', 0)}")
    print(f"ì™„ë£Œ: {stats.get('completed_chapters', 0)}")
    print(f"ì‹¤íŒ¨: {stats.get('failed_chapters', 0)}")
    print(f"ì´ ë¬¸ë‹¨: {stats.get('total_paragraphs', 0)}")
    print(f"ì¶”ì¶œëœ ì•„ì´ë””ì–´: {stats.get('total_ideas', 0)}")
    print("â”€" * 40)
    print("ì¤‘ë³µ ìŠ¤í‚µ ìƒì„¸:")
    print(f"  í•´ì‹œ ê¸°ë°˜: {stats.get('chunk_duplicates_skipped', 0)}")
    print(f"  ì„ë² ë”© ê¸°ë°˜: {stats.get('semantic_duplicates_skipped', 0)}")
    print(f"  ì•„ì´ë””ì–´ ê¸°ë°˜: {stats.get('idea_duplicates_skipped', 0)}")
    print(f"  ì´ ìŠ¤í‚µ: {stats.get('duplicates_skipped', 0)}")
    print("=" * 60)
