"""
LangGraph ì›Œí¬í”Œë¡œìš° ì •ì˜.

PDF â†’ TOC ê¸°ë°˜ ì±•í„°/ì„¹ì…˜ ê°ì§€ â†’ ë¬¸ë‹¨ ë¶„í•  â†’ ì•„ì´ë””ì–´ ì¶”ì¶œ (+ ì¤‘ë³µì œê±°)
"""
from typing import Optional
from langgraph.graph import StateGraph, END

from src.workflow.state import PipelineState, create_initial_state
from src.workflow.nodes import (
    extract_text,
    detect_structure,
    create_book_node,
    process_section,
    route_sections,
    finalize,
)


def create_pdf_pipeline() -> StateGraph:
    """PDF ì²˜ë¦¬ LangGraph ì›Œí¬í”Œë¡œìš° ìƒì„±."""
    workflow = StateGraph(PipelineState)

    # ë…¸ë“œ ì¶”ê°€
    workflow.add_node("extract_text", extract_text)
    workflow.add_node("detect_structure", detect_structure)
    workflow.add_node("create_book", create_book_node)
    workflow.add_node("process_section", process_section)
    workflow.add_node("finalize", finalize)

    # ì—£ì§€ ì •ì˜: ìˆœì°¨ íë¦„
    workflow.set_entry_point("extract_text")
    workflow.add_edge("extract_text", "detect_structure")
    workflow.add_edge("detect_structure", "create_book")
    workflow.add_edge("create_book", "process_section")

    # ì¡°ê±´ë¶€ ë¼ìš°íŒ…: ì„¹ì…˜ ìˆœíšŒ ë£¨í”„
    workflow.add_conditional_edges(
        "process_section",
        route_sections,
        {
            "continue": "process_section",  # ë‹¤ìŒ ì„¹ì…˜ ì²˜ë¦¬
            "finalize": "finalize",         # ëª¨ë“  ì„¹ì…˜ ì™„ë£Œ
        }
    )

    workflow.add_edge("finalize", END)

    return workflow.compile()


# ì»´íŒŒì¼ëœ íŒŒì´í”„ë¼ì¸ ê·¸ë˜í”„ ì¸ìŠ¤í„´ìŠ¤
pdf_pipeline = create_pdf_pipeline()


def run_pdf_pipeline(
    pdf_path: str,
    resume: bool = False,
    book_id: Optional[int] = None,
    model_version: str = "gemini-2.5-flash",
    enable_semantic_dedup: bool = False,
    semantic_threshold: float = 0.95,
) -> dict:
    """
    ë‹¨ì¼ LangGraphë¥¼ í†µí•´ ì „ì²´ PDF ì²˜ë¦¬ ìˆ˜í–‰:
    1. PDF â†’ Plain Text + TOC ì¶”ì¶œ
    2. TOC ê¸°ë°˜ ì±•í„°/ì„¹ì…˜ êµ¬ì¡° ê°ì§€
    3. DBì— ì±…/ì±•í„°/ì„¹ì…˜ ì €ì¥
    4. ê° ì„¹ì…˜ë³„ ë¬¸ë‹¨ ë¶„í•  ë° ì•„ì´ë””ì–´ ì¶”ì¶œ (+ í•´ì‹œ/ì„ë² ë”© ì¤‘ë³µì œê±°)
    5. ì²˜ë¦¬ ê²°ê³¼ ìš”ì•½

    Args:
        pdf_path: PDF íŒŒì¼ ê²½ë¡œ
        resume: ì´ì „ ì§„í–‰ ìƒí™©ì—ì„œ ì¬ê°œ ì—¬ë¶€
        book_id: ê¸°ì¡´ ì±… ID (ì¬ê°œ ì‹œ ì‚¬ìš©)
        model_version: LLM ëª¨ë¸ ë²„ì „
        enable_semantic_dedup: ì„ë² ë”© ê¸°ë°˜ ì¤‘ë³µì œê±° í™œì„±í™” (ê¸°ë³¸ False)
        semantic_threshold: ì½”ì‚¬ì¸ ìœ ì‚¬ë„ ì„ê³„ê°’ (ê¸°ë³¸ 0.95)
    """
    # ì´ˆê¸° ìƒíƒœ ìƒì„±
    initial_state = create_initial_state(
        pdf_path=pdf_path,
        book_id=book_id,
        resume=resume,
        model_version=model_version,
    )

    # ì¤‘ë³µì œê±° ì˜µì…˜ ì„¤ì •
    initial_state["enable_semantic_dedup"] = enable_semantic_dedup
    initial_state["semantic_threshold"] = semantic_threshold

    print(f"ğŸ“„ PDF íŒŒì´í”„ë¼ì¸ ì‹œì‘: {pdf_path}")
    print(f"   â†’ ëª¨ë¸: {model_version}")
    print(f"   â†’ ì‹œë§¨í‹± ì¤‘ë³µì œê±°: {'âœ…' if enable_semantic_dedup else 'âŒ'}")

    # ê·¸ë˜í”„ ì‹¤í–‰ (ì„¹ì…˜ ìˆ˜ + ì—¬ìœ ë¶„ìœ¼ë¡œ recursion_limit ì„¤ì •)
    result_state = pdf_pipeline.invoke(
        initial_state,
        config={"recursion_limit": 500}  # ìµœëŒ€ 500ê°œ ì„¹ì…˜ê¹Œì§€ ì§€ì›
    )

    # ì—ëŸ¬ ì²´í¬
    if result_state.get("error"):
        return {"error": result_state["error"]}

    # í†µê³„ ì¶œë ¥ ë° ë°˜í™˜
    stats = result_state.get("stats", {})
    _print_summary(stats)
    return stats


def _print_summary(stats: dict) -> None:
    """ì²˜ë¦¬ ìš”ì•½ ì¶œë ¥."""
    print("\n" + "=" * 60)
    print("ğŸ“Š ì²˜ë¦¬ ìš”ì•½")
    print("=" * 60)
    print(f"ì´ ì±•í„°: {stats.get('total_chapters', 0)}")
    print(f"ì´ ì„¹ì…˜: {stats.get('total_sections', 0)}")
    print(f"ì™„ë£Œëœ ì„¹ì…˜: {stats.get('completed_sections', 0)}")
    print(f"ì‹¤íŒ¨í•œ ì„¹ì…˜: {stats.get('failed_sections', 0)}")
    print(f"ì´ ë¬¸ë‹¨: {stats.get('total_paragraphs', 0)}")
    print(f"ì¶”ì¶œëœ ì•„ì´ë””ì–´: {stats.get('total_ideas', 0)}")
    print("â”€" * 40)
    print("ì¤‘ë³µ ìŠ¤í‚µ ìƒì„¸:")
    print(f"  í•´ì‹œ ê¸°ë°˜: {stats.get('chunk_duplicates_skipped', 0)}")
    print(f"  ì„ë² ë”© ê¸°ë°˜: {stats.get('semantic_duplicates_skipped', 0)}")
    print(f"  ì•„ì´ë””ì–´ ê¸°ë°˜: {stats.get('idea_duplicates_skipped', 0)}")
    print(f"  ì´ ìŠ¤í‚µ: {stats.get('duplicates_skipped', 0)}")
    print("=" * 60)
