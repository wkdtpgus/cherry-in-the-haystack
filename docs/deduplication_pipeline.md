# ì¤‘ë³µì œê±° íŒŒì´í”„ë¼ì¸ (Deduplication Pipeline)

## ê°œìš”

PDFì—ì„œ ì¶”ì¶œí•œ ë¬¸ë‹¨(ì²­í¬)ê³¼ ì•„ì´ë””ì–´ì˜ ì¤‘ë³µì„ íƒì§€í•˜ì—¬ ë¶ˆí•„ìš”í•œ LLM í˜¸ì¶œê³¼ DB ì €ì¥ì„ ë°©ì§€í•©ë‹ˆë‹¤.

### í•µì‹¬ ì„¤ê³„

- **LangGraph ë…¸ë“œì— í†µí•©**: `process_section` ë…¸ë“œ ë‚´ë¶€ì—ì„œ ì²­í‚¹ â†’ ì¤‘ë³µì²´í¬ â†’ ì•„ì´ë””ì–´ì¶”ì¶œ â†’ ì €ì¥ì„ ì¼ê´„ ì²˜ë¦¬
- **ë‹¤ë‹¨ê³„ ì¤‘ë³µì œê±°**: SHA256 â†’ SimHash â†’ ì„ë² ë”© â†’ ì•„ì´ë””ì–´ í…ìŠ¤íŠ¸ ë§¤ì¹­
- **ë¹„ìš© ìµœì í™”**: í•´ì‹œ ì¤‘ë³µì´ë©´ LLM í˜¸ì¶œ ìì²´ë¥¼ ìŠ¤í‚µ

---

## íŒŒì´í”„ë¼ì¸ êµ¬ì¡°

### LangGraph ì›Œí¬í”Œë¡œìš°

```
extract_text â†’ detect_structure â†’ create_book â†’ process_section (loop) â†’ finalize
                                                      â”‚
                                                      â–¼
                                              [ê° ì²­í¬ë§ˆë‹¤]
                                         _check_chunk_duplicate
                                                â”‚
                                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                                    â–¼                       â–¼
                               (ì¤‘ë³µ) skip              (ì‹ ê·œ) _extract_idea
                                                            â”‚
                                                            â–¼
                                                   _check_idea_duplicate
                                                            â”‚
                                                â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                                                â–¼                       â–¼
                                           (ì¤‘ë³µ) skip              (ì‹ ê·œ) _save_to_db
```

### ì¤‘ë³µì²´í¬ íë¦„

```
ì²­í¬ í…ìŠ¤íŠ¸
    â”‚
    â–¼
1ë‹¨ê³„: SHA256 í•´ì‹œ ì •í™• ë§¤ì¹­ (ë¬´ë£Œ, ì¦‰ì‹œ)
    â”‚ (ì¼ì¹˜) â†’ ì¤‘ë³µìœ¼ë¡œ ìŠ¤í‚µ
    â–¼
[enable_semantic_dedup=True ì‹œ]
    â”‚
    â”œâ”€ 2ë‹¨ê³„: SimHash í¼ì§€ ë§¤ì¹­ (ë¬´ë£Œ, ì¦‰ì‹œ)
    â”‚     â”‚ (ìœ ì‚¬) â†’ ì¤‘ë³µìœ¼ë¡œ ìŠ¤í‚µ
    â”‚     â–¼
    â””â”€ 3ë‹¨ê³„: ì„ë² ë”© ì˜ë¯¸ì  ë§¤ì¹­ (API ë¹„ìš©, ~200ms)
          â”‚ (ìœ ì‚¬ë„ >= threshold) â†’ ì¤‘ë³µìœ¼ë¡œ ìŠ¤í‚µ
          â–¼
4ë‹¨ê³„: LLM ì•„ì´ë””ì–´ ì¶”ì¶œ
    â”‚
    â–¼
5ë‹¨ê³„: concept ë¬¸ìì—´ ë§¤ì¹­ (ë¬´ë£Œ, DB ì¿¼ë¦¬)
    â”‚ (ì¼ì¹˜) â†’ ì¤‘ë³µìœ¼ë¡œ ìŠ¤í‚µ
    â–¼
6ë‹¨ê³„: DB ì €ì¥
```

---

## íŒŒì¼ êµ¬ì¡°

### í•µì‹¬ íŒŒì¼

```
src/
â”œâ”€â”€ workflow/
â”‚   â”œâ”€â”€ workflow.py              # LangGraph ì›Œí¬í”Œë¡œìš° ì •ì˜
â”‚   â””â”€â”€ nodes/
â”‚       â””â”€â”€ process_section.py   # ì„¹ì…˜ ì²˜ë¦¬ + ì¤‘ë³µì œê±° í†µí•©
â”‚
â”œâ”€â”€ dedup/
â”‚   â”œâ”€â”€ hash_utils.py            # SHA256, SimHash í•´ì‹œ ê³„ì‚°
â”‚   â”œâ”€â”€ embedding_utils.py       # OpenAI ì„ë² ë”© ìƒì„±
â”‚   â””â”€â”€ dedup_service.py         # DeduplicationService í´ë˜ìŠ¤
â”‚
â””â”€â”€ db/
    â””â”€â”€ models.py                # paragraph_hash, simhash64 í•„ë“œ

scripts/
    â””â”€â”€ generate_embeddings.py   # ë°°ì¹˜ ì„ë² ë”© ìƒì„± CLI
```

### process_section.py ë‚´ë¶€ í•¨ìˆ˜

| í•¨ìˆ˜ | ì—­í•  |
|------|------|
| `_chunk_section()` | ì„¹ì…˜ í…ìŠ¤íŠ¸ â†’ ë¬¸ë‹¨ ë¶„í•  + í•´ì‹œ ê³„ì‚° |
| `_check_chunk_duplicate()` | SHA256 + SimHash + ì„ë² ë”© ì¤‘ë³µ ì²´í¬ |
| `_extract_idea()` | LLMìœ¼ë¡œ ì•„ì´ë””ì–´ ì¶”ì¶œ |
| `_check_idea_duplicate()` | concept ë¬¸ìì—´ ì¤‘ë³µ ì²´í¬ |
| `_save_to_db()` | ì²­í¬ + ì•„ì´ë””ì–´ DB ì €ì¥ |

---

## ì¤‘ë³µì œê±° ë‹¨ê³„ë³„ ì„¤ëª…

### 1ë‹¨ê³„: SHA256 í•´ì‹œ (ì •í™• ë§¤ì¹­)

```python
# src/dedup/hash_utils.py
def compute_paragraph_hash(text: str) -> str:
    normalized = normalize_text(text)
    return hashlib.sha256(normalized.encode()).hexdigest()
```

- í…ìŠ¤íŠ¸ ì •ê·œí™” í›„ SHA256 í•´ì‹œ ìƒì„±
- ì™„ì „íˆ ë™ì¼í•œ í…ìŠ¤íŠ¸ë§Œ ì¤‘ë³µìœ¼ë¡œ íŒë‹¨
- **ë¹„ìš©**: ë¬´ë£Œ, ì¦‰ì‹œ (Î¼s)
- **í•­ìƒ ì‹¤í–‰**

### 2ë‹¨ê³„: SimHash (í¼ì§€ ë§¤ì¹­)

```python
# src/dedup/hash_utils.py
def compute_simhash64(text: str) -> int:
    # 64ë¹„íŠ¸ SimHash ìƒì„±

def hamming_distance(hash1: int, hash2: int) -> int:
    # í•´ë° ê±°ë¦¬ ê³„ì‚° (ë¹„íŠ¸ ì°¨ì´ ìˆ˜)
```

- ê¸€ìê°€ ì•½ê°„ ë‹¤ë¥¸ ìœ ì‚¬ í…ìŠ¤íŠ¸ íƒì§€
- í•´ë° ê±°ë¦¬ 6 ì´í•˜ë©´ ì¤‘ë³µìœ¼ë¡œ íŒë‹¨
- **ë¹„ìš©**: ë¬´ë£Œ, ì¦‰ì‹œ (Î¼s)
- **enable_semantic_dedup=True ì‹œ ì‹¤í–‰**

**ì˜ˆì‹œ:**
```
"TransformerëŠ” attentionì„ ì‚¬ìš©í•œë‹¤"
"TransformerëŠ” attentionì„ í™œìš©í•œë‹¤"
â†’ í•´ë° ê±°ë¦¬ ì‘ìŒ â†’ ì¤‘ë³µ
```

### 3ë‹¨ê³„: ì„ë² ë”© (ì˜ë¯¸ì  ë§¤ì¹­)

```python
# src/dedup/dedup_service.py
def find_semantic_duplicate(self, text, book_id, cross_book):
    embedding = compute_embedding(text)
    # pgvector ì½”ì‚¬ì¸ ìœ ì‚¬ë„ ê²€ìƒ‰
    # ìœ ì‚¬ë„ >= threshold â†’ ì¤‘ë³µ
```

- ì˜ë¯¸ì ìœ¼ë¡œ ìœ ì‚¬í•œ í…ìŠ¤íŠ¸ íƒì§€
- OpenAI text-embedding-3-small ì‚¬ìš©
- pgvectorë¡œ ì½”ì‚¬ì¸ ìœ ì‚¬ë„ ê²€ìƒ‰
- **ë¹„ìš©**: API í˜¸ì¶œ (~$0.00002/1K tokens)
- **enable_semantic_dedup=True ì‹œ ì‹¤í–‰**

**ì˜ˆì‹œ:**
```
"TransformerëŠ” attention ë©”ì»¤ë‹ˆì¦˜ì„ ì‚¬ìš©í•œë‹¤"
"Attention mechanismì´ Transformerì˜ í•µì‹¬ì´ë‹¤"
â†’ ê¸€ì ìˆœì„œ ë‹¤ë¦„ â†’ SimHash âŒ
â†’ ì˜ë¯¸ ë™ì¼ â†’ ì„ë² ë”© ìœ ì‚¬ë„ ë†’ìŒ âœ…
```

### 4ë‹¨ê³„: ì•„ì´ë””ì–´ ì¶”ì¶œ (LLM)

```python
# process_section.py
def _extract_idea(chunk_text, hierarchy_path, prev_text, next_text):
    llm = get_default_llm()
    structured_llm = llm.with_structured_output(ExtractedIdea)
    # ...
```

- ì¤‘ë³µì´ ì•„ë‹Œ ì²­í¬ë§Œ LLM í˜¸ì¶œ
- ì»¨í…ìŠ¤íŠ¸(ì•ë’¤ ë¬¸ë‹¨, ê³„ì¸µ ê²½ë¡œ) í¬í•¨í•˜ì—¬ ì¶”ì¶œ
- **ë¹„ìš©**: LLM API í˜¸ì¶œ

### 5ë‹¨ê³„: concept ë¬¸ìì—´ ë§¤ì¹­

```python
# process_section.py
def _check_idea_duplicate(concept: str, book_id: int) -> bool:
    existing = session.query(KeyIdea).filter(
        KeyIdea.core_idea_text == concept,
        KeyIdea.book_id == book_id
    ).first()
    return existing is not None
```

- LLMì´ ì¶”ì¶œí•œ conceptì„ DBì—ì„œ ê²€ìƒ‰
- ì •í™•íˆ ê°™ì€ ë¬¸ìì—´ë§Œ ì¤‘ë³µìœ¼ë¡œ íŒë‹¨
- **ë¹„ìš©**: ë¬´ë£Œ (DB ì¿¼ë¦¬)

**ì™œ ë¬¸ìì—´ ë§¤ì¹­ì¸ê°€?**
- `"Transformer"`ì™€ `"Transformer architecture"`ëŠ” ë‹¤ë¥¸ ê°œë…
- ì„ë² ë”©ìœ¼ë¡œ ë¹„êµí•˜ë©´ ìœ ì‚¬í•˜ë‹¤ê³  íŒë‹¨ë  ìˆ˜ ìˆìŒ
- ì„œë¡œ ë‹¤ë¥¸ ê´€ì ì„ ë‹´ê³  ìˆìœ¼ë¯€ë¡œ ë‘˜ ë‹¤ ì €ì¥í•˜ëŠ” ê²ƒì´ ì ì ˆ

---

## ì‚¬ìš© ë°©ë²•

### ê¸°ë³¸ ì‹¤í–‰ (SHA256 í•´ì‹œë§Œ)

```bash
python run_pipeline.py "book.pdf"
```

ì¶œë ¥:
```
â†’ ì‹œë§¨í‹± ì¤‘ë³µì œê±°: âŒ
```

### ì‹œë§¨í‹± ì¤‘ë³µì œê±° í™œì„±í™” (SimHash + ì„ë² ë”©)

```python
# run_pipeline.py
result = run_pdf_pipeline(
    pdf_path=pdf_path,
    model_version=model_version,
    enable_semantic_dedup=True,      # SimHash + ì„ë² ë”© í™œì„±í™”
    semantic_threshold=0.95,         # ì„ë² ë”© ìœ ì‚¬ë„ ì„ê³„ê°’
)
```

ì¶œë ¥:
```
â†’ ì‹œë§¨í‹± ì¤‘ë³µì œê±°: âœ…
```

### íŒŒë¼ë¯¸í„°

| íŒŒë¼ë¯¸í„° | ê¸°ë³¸ê°’ | ì„¤ëª… |
|---------|--------|------|
| `enable_semantic_dedup` | `False` | SimHash + ì„ë² ë”© ì¤‘ë³µì œê±° í™œì„±í™” |
| `semantic_threshold` | `0.95` | ì„ë² ë”© ì½”ì‚¬ì¸ ìœ ì‚¬ë„ ì„ê³„ê°’ |

---

## í†µê³„ ì¶œë ¥

```
============================================================
ğŸ“Š ì²˜ë¦¬ ìš”ì•½
============================================================
ì´ ì±•í„°: 10
ì´ ì„¹ì…˜: 45
ì™„ë£Œëœ ì„¹ì…˜: 45
ì‹¤íŒ¨í•œ ì„¹ì…˜: 0
ì´ ë¬¸ë‹¨: 320
ì¶”ì¶œëœ ì•„ì´ë””ì–´: 280
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
ì¤‘ë³µ ìŠ¤í‚µ ìƒì„¸:
  í•´ì‹œ ê¸°ë°˜: 15           # SHA256 + SimHash
  ì„ë² ë”© ê¸°ë°˜: 8          # OpenAI ì„ë² ë”©
  ì•„ì´ë””ì–´ ê¸°ë°˜: 17       # concept ë¬¸ìì—´ ë§¤ì¹­
  ì´ ìŠ¤í‚µ: 40
============================================================
```

---

## DB ìŠ¤í‚¤ë§ˆ

### paragraph_chunks í…Œì´ë¸”

```sql
ALTER TABLE paragraph_chunks ADD COLUMN paragraph_hash VARCHAR(64);
ALTER TABLE paragraph_chunks ADD COLUMN simhash64 BIGINT;
```

### paragraph_embeddings í…Œì´ë¸” (pgvector í•„ìš”)

```sql
CREATE TABLE paragraph_embeddings (
    id SERIAL PRIMARY KEY,
    chunk_id INTEGER REFERENCES paragraph_chunks(id) UNIQUE,
    book_id INTEGER REFERENCES books(id),
    embedding vector(1536),
    model_name VARCHAR(100) DEFAULT 'text-embedding-3-small'
);

-- ì¸ë±ìŠ¤
CREATE INDEX ON paragraph_embeddings
USING ivfflat (embedding vector_cosine_ops) WITH (lists = 100);
```

---

## ë¹„ìš© ë¶„ì„

### ì¤‘ë³µì œê±° ë°©ì‹ë³„ ë¹„êµ

| ë°©ì‹ | ì†ë„ | ë¹„ìš© | íƒì§€ ë²”ìœ„ |
|-----|------|------|----------|
| SHA256 | ì¦‰ì‹œ (Î¼s) | ë¬´ë£Œ | ì™„ì „ ë™ì¼ |
| SimHash | ì¦‰ì‹œ (Î¼s) | ë¬´ë£Œ | ê¸€ì ë³€í˜• |
| ì„ë² ë”© | ~200ms | $0.00002/1K tokens | ì˜ë¯¸ì  ìœ ì‚¬ |
| concept ë§¤ì¹­ | ì¦‰ì‹œ | ë¬´ë£Œ (DB) | ì •í™•íˆ ë™ì¼ |

### ì™œ ë‹¤ë‹¨ê³„ë¡œ ì²˜ë¦¬í•˜ëŠ”ê°€?

**100ê°œ ì²­í¬ ì²˜ë¦¬ ì‹œ:**
- ì„ë² ë”©ë§Œ: 100 API í˜¸ì¶œ â†’ ~20ì´ˆ, ~$0.004
- í•´ì‹œ + ì„ë² ë”©: í•´ì‹œì—ì„œ 80ê°œ ìŠ¤í‚µ â†’ 20 API í˜¸ì¶œ â†’ ~4ì´ˆ, ~$0.0008

â†’ **í•´ì‹œ = ë¬´ë£Œ 1ì°¨ í•„í„°, ì„ë² ë”© = ì •ë°€ 2ì°¨ í•„í„°**

---

## ì½”ë“œ íë¦„ ìš”ì•½

```python
# process_section.py

def _check_chunk_duplicate(chunk, book_id, enable_semantic_dedup, semantic_threshold):
    # 1. SHA256 í•´ì‹œ ì •í™• ë§¤ì¹­ (í•­ìƒ)
    existing = session.query(DBParagraphChunk).filter(
        DBParagraphChunk.paragraph_hash == chunk.paragraph_hash
    ).first()
    if existing:
        return True

    # 2-3. SimHash + ì„ë² ë”© (enable_semantic_dedup=True ì‹œ)
    if enable_semantic_dedup:
        dedup_service = DeduplicationService(session, enable_semantic=True)

        # SimHash í¼ì§€ ë§¤ì¹­
        fuzzy_matches = dedup_service.find_fuzzy_duplicates(chunk.simhash64, book_id)
        if fuzzy_matches:
            return True

        # ì„ë² ë”© ì˜ë¯¸ì  ë§¤ì¹­
        semantic_match = dedup_service.find_semantic_duplicate(chunk.text, book_id)
        if semantic_match:
            return True

    return False


def _process_chunk(chunk, book_id, ...):
    # 1. ì²­í¬ ì¤‘ë³µ ì²´í¬ (ë¨¼ì €!)
    if _check_chunk_duplicate(chunk, book_id, enable_semantic_dedup):
        return {"is_chunk_duplicate": True}  # LLM í˜¸ì¶œ ìŠ¤í‚µ

    # 2. LLM ì•„ì´ë””ì–´ ì¶”ì¶œ
    extracted_idea = _extract_idea(chunk_text, ...)

    # 3. ì•„ì´ë””ì–´ ì¤‘ë³µ ì²´í¬
    concept = get_concept_from_idea(extracted_idea)
    if _check_idea_duplicate(concept, book_id):
        return {"is_idea_duplicate": True}

    # 4. DB ì €ì¥
    _save_to_db(chunk, extracted_idea, book_id, ...)
    return {"saved": True}
```

---

## í…ŒìŠ¤íŠ¸

```bash
# ê°€ìƒí™˜ê²½ í™œì„±í™”
source venv/bin/activate

# íŒŒì´í”„ë¼ì¸ ì‹¤í–‰
python run_pipeline.py "tests/Reflexion.pdf" gemini-2.5-flash
```

### ì¤‘ë³µì œê±° í™•ì¸ ë°©ë²•

1. ê°™ì€ PDFë¥¼ ë‘ ë²ˆ ì‹¤í–‰
2. ë‘ ë²ˆì§¸ ì‹¤í–‰ ì‹œ `í•´ì‹œ ê¸°ë°˜` ìŠ¤í‚µ ìˆ˜ê°€ ì¦ê°€í•˜ë©´ ì •ìƒ ì‘ë™

---

## êµ¬í˜„ ìƒíƒœ

| ê¸°ëŠ¥ | ìƒíƒœ | íŒŒì¼ |
|------|------|------|
| SHA256 í•´ì‹œ | âœ… ì™„ë£Œ | `hash_utils.py` |
| SimHash í¼ì§€ ë§¤ì¹­ | âœ… ì™„ë£Œ | `hash_utils.py`, `dedup_service.py` |
| ì„ë² ë”© ì˜ë¯¸ì  ë§¤ì¹­ | âœ… ì™„ë£Œ | `embedding_utils.py`, `dedup_service.py` |
| ì²­í¬ ì¤‘ë³µ ì²´í¬ | âœ… ì™„ë£Œ | `process_section.py` |
| ì•„ì´ë””ì–´ ì¤‘ë³µ ì²´í¬ | âœ… ì™„ë£Œ | `process_section.py` |
| LangGraph í†µí•© | âœ… ì™„ë£Œ | `workflow.py` |
| ë°°ì¹˜ ì„ë² ë”© ìƒì„± | âœ… ì™„ë£Œ | `generate_embeddings.py` |
