# ì¤‘ë³µì œê±° íŒŒì´í”„ë¼ì¸ (Deduplication Pipeline)

## ê°œìš”

ê¸°ì¡´ LangGraph ì›Œí¬í”Œë¡œìš°ì— í•´ì‹œ ê¸°ë°˜ ì¤‘ë³µì œê±°ë¥¼ **ë…¸ë“œ íŒ¨í„´ìœ¼ë¡œ ìì—°ìŠ¤ëŸ½ê²Œ í†µí•©**í•©ë‹ˆë‹¤.

### í•µì‹¬ ê²°ì •ì‚¬í•­

- **LangGraph ë…¸ë“œë¡œ í†µí•©**: ë³„ë„ ì„œë¹„ìŠ¤ê°€ ì•„ë‹Œ `workflow/nodes/` íŒ¨í„´ ìœ ì§€
- **2ë‹¨ê³„ ì¤‘ë³µì œê±°**: ì²­í¬ í•´ì‹œ/ì„ë² ë”© ì²´í¬ â†’ ì•„ì´ë””ì–´ ì¶”ì¶œ â†’ concept ì •í™• ë§¤ì¹­
- **ì‹¤ì‹œê°„ ì²˜ë¦¬**: SHA256 + SimHash ê¸°ë°˜ (ë¹ ë¦„)
- **ë°°ì¹˜ ì²˜ë¦¬**: OpenAI ì„ë² ë”© (ë³„ë„ ìŠ¤í¬ë¦½íŠ¸)

---

## íŒŒì´í”„ë¼ì¸ êµ¬ì¡°

### ê¸°ì¡´ LangGraph êµ¬ì¡°

```
extract_idea â†’ check_duplicate â†’ save/skip â†’ END
```

### ìƒˆë¡œìš´ LangGraph êµ¬ì¡°

```
check_chunk_duplicate â†’ extract_idea â†’ check_idea_duplicate â†’ save/skip â†’ END
     â†“ (ì¤‘ë³µ)                                    â†“ (ì¤‘ë³µ)
    skip â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â†’ END
```

**ì™œ extract_idea ì „ì¸ê°€?**

- ì²­í¬ê°€ ì´ì „ì— ì €ì¥ëœ ì²­í¬ì™€ ì¤‘ë³µì´ë©´ **LLM í˜¸ì¶œ ìì²´ë¥¼ ìŠ¤í‚µ** â†’ ë¹„ìš© ì ˆê°
- ì²« ë²ˆì§¸ ì²­í¬ëŠ” ë¹„êµ ëŒ€ìƒì´ ì—†ìœ¼ë¯€ë¡œ í•­ìƒ í†µê³¼
- ë‘ ë²ˆì§¸ ì²­í¬ë¶€í„° DBì— ì €ì¥ëœ í•´ì‹œì™€ ë¹„êµ

---

## íŒŒì¼ êµ¬ì¡°

### src/dedup/ (ë³„ë„ ëª¨ë“ˆë¡œ ë¶„ë¦¬)

```
src/dedup/
    __init__.py
    hash_utils.py         # SHA256, SimHash í•´ì‹œ ê³„ì‚°
    embedding_utils.py    # OpenAI ì„ë² ë”© ìƒì„±
    dedup_service.py      # DeduplicationService í´ë˜ìŠ¤

scripts/
    generate_embeddings.py   # ë°°ì¹˜ ì„ë² ë”© ìƒì„± CLI
```

**ì™œ src/dedup/ ë¶„ë¦¬ì¸ê°€?**

- `dedup_service.py`ëŠ” DB ì„¸ì…˜ì„ ë°›ì•„ ì¿¼ë¦¬ë¥¼ ì‹¤í–‰í•˜ëŠ” **ì„œë¹„ìŠ¤ í´ë˜ìŠ¤** â†’ `utils/`(ìˆœìˆ˜ í•¨ìˆ˜)ì™€ ì„±ê²©ì´ ë‹¤ë¦„
- ì¤‘ë³µì œê±° ë„ë©”ì¸ì´ ëª…í™•íˆ ë¶„ë¦¬ë¨ (í•´ì‹œ, ì„ë² ë”©, ì„œë¹„ìŠ¤ê°€ í•œ ê³³ì—)
- í–¥í›„ ì„ë² ë”© ë°°ì¹˜ ì²˜ë¦¬, ìœ ì‚¬ë„ ê²€ìƒ‰ ë“± í™•ì¥ ì‹œ ê´€ë¦¬ ìš©ì´
- `workflow/nodes/check_duplicate.py`ëŠ” ì´ ëª¨ë“ˆì„ **importí•´ì„œ ì‚¬ìš©**í•˜ëŠ” LangGraph ë…¸ë“œ ì—­í• ë§Œ ë‹´ë‹¹

### ìˆ˜ì •ëœ íŒŒì¼

| íŒŒì¼ | ë³€ê²½ ë‚´ìš© |
|------|-----------|
| `src/db/models.py` | paragraph_hash, simhash64 í•„ë“œ + ParagraphEmbedding ëª¨ë¸ |
| `src/model/schemas.py` | HierarchicalChunkì— í•´ì‹œ í•„ë“œ ì¶”ê°€ |
| `src/workflow/nodes/__init__.py` | ìƒˆ ë…¸ë“œ export ì¶”ê°€ |
| `src/workflow/nodes/chunk_paragraphs.py` | ì²­í‚¹ ì‹œ í•´ì‹œ ê³„ì‚° |
| `src/workflow/nodes/check_duplicate.py` | 2ê°œ ë…¸ë“œë¡œ ë¶„ë¦¬ (chunk/idea) |
| `src/workflow/nodes/save_to_db.py` | í•´ì‹œ í•„ë“œ ì €ì¥ |
| `src/workflow/workflow.py` | ê·¸ë˜í”„ êµ¬ì¡° ìˆ˜ì • |
| `src/workflow/state.py` | `enable_semantic_dedup`, `semantic_threshold` í•„ë“œ ì¶”ê°€ |

---

## í˜„ì¬ êµ¬í˜„ ìƒíƒœ

| êµ¬ì„±ìš”ì†Œ | ìƒíƒœ | íŒŒì¼ |
|---------|------|------|
| SHA256 í•´ì‹œ | âœ… ì™„ë£Œ | `src/dedup/hash_utils.py` |
| SimHash í¼ì§€ ë§¤ì¹­ | âœ… ì™„ë£Œ | `src/dedup/hash_utils.py` |
| DeduplicationService | âœ… ì™„ë£Œ | `src/dedup/dedup_service.py` |
| ì²­í¬ í•´ì‹œ ì €ì¥ | âœ… ì™„ë£Œ | `chunk_paragraphs.py`, `save_to_db.py` |
| LangGraph ë…¸ë“œ í†µí•© | âœ… ì™„ë£Œ | `check_duplicate.py`, `workflow.py` |
| ë°°ì¹˜ ì„ë² ë”© ìƒì„± | âœ… ì™„ë£Œ | `scripts/generate_embeddings.py` |
| ì„ë² ë”© ê¸°ë°˜ ì¤‘ë³µì œê±° | âœ… ì™„ë£Œ | `check_duplicate.py` (í•˜ì´ë¸Œë¦¬ë“œ) |

---

## íŒŒì´í”„ë¼ì¸ íë¦„

```
check_chunk_duplicate
    â”‚
    â”œâ”€â”€ 1ë‹¨ê³„: í•´ì‹œ ì²´í¬ (SHA256 + SimHash)
    â”‚       â”œâ”€â”€ ì¤‘ë³µ â†’ skip â†’ END
    â”‚       â””â”€â”€ í†µê³¼ â†“
    â”‚
    â”œâ”€â”€ 2ë‹¨ê³„: ì„ë² ë”© ì²´í¬ (enable_semantic_dedup=True ì‹œ)
    â”‚       â”œâ”€â”€ ìœ ì‚¬ (>=0.95) â†’ skip â†’ END
    â”‚       â””â”€â”€ í†µê³¼ â†“
    â”‚
    â””â”€â”€ extract_idea
            â”‚
            â–¼
        check_idea_duplicate (concept ì •í™• ë§¤ì¹­)
            â”‚
            â”œâ”€â”€ ì¤‘ë³µ â†’ skip â†’ END
            â”‚
            â””â”€â”€ ì‹ ê·œ â†’ save_to_db â†’ END
```

### ê° ë‹¨ê³„ ì„¤ëª…

| ë‹¨ê³„ | ì²´í¬ ëŒ€ìƒ | ë°©ì‹ | ë¹„ìš© |
|-----|----------|------|------|
| 1ë‹¨ê³„ | ì²­í¬ í…ìŠ¤íŠ¸ | SHA256 ì •í™• + SimHash í¼ì§€ | ë¬´ë£Œ |
| 2ë‹¨ê³„ | ì²­í¬ í…ìŠ¤íŠ¸ | OpenAI ì„ë² ë”© ìœ ì‚¬ë„ | API í˜¸ì¶œ |
| 3ë‹¨ê³„ | ì¶”ì¶œëœ concept | DB ì •í™• ë§¤ì¹­ | ë¬´ë£Œ |

### ìƒì„¸ ë™ì‘

#### check_chunk_duplicate (ì²­í¬ ë ˆë²¨)

```python
# src/workflow/nodes/check_duplicate.py

def check_chunk_duplicate(state: PipelineState) -> PipelineState:
    # 1ë‹¨ê³„: í•´ì‹œ ì²´í¬ (ë¹ ë¦„)
    dedup_service = DeduplicationService(session, fuzzy_threshold=6)
    result = dedup_service.check_duplicate(text, book_id)

    if result.is_duplicate:
        return {..., "is_chunk_duplicate": True}

    # 2ë‹¨ê³„: ì„ë² ë”© ì²´í¬ (í™œì„±í™” ì‹œì—ë§Œ)
    if enable_semantic:
        semantic_result = dedup_service.find_semantic_duplicate(text, book_id)
        if semantic_result:
            return {..., "is_chunk_duplicate": True, "chunk_duplicate_type": "semantic"}
```

- **1ë‹¨ê³„**: `DeduplicationService.check_duplicate()` â†’ SHA256 + SimHash í•´ì‹œ ë¹„êµ
- **2ë‹¨ê³„**: `DeduplicationService.find_semantic_duplicate()` â†’ ì„ë² ë”© ì½”ì‚¬ì¸ ìœ ì‚¬ë„

#### check_idea_duplicate (ì•„ì´ë””ì–´ ë ˆë²¨)

```python
# src/workflow/nodes/check_duplicate.py

def check_idea_duplicate(state: PipelineState) -> PipelineState:
    concept = extracted_idea.concept  # LLMì´ ì¶”ì¶œí•œ concept

    # DBì—ì„œ ë™ì¼ concept ê²€ìƒ‰ (ë¬¸ìì—´ ì •í™• ë§¤ì¹­)
    existing = session.query(KeyIdea).filter(
        KeyIdea.core_idea_text == concept
    ).first()

    if existing:
        return {..., "is_idea_duplicate": True}
```

- LLMì´ ì¶”ì¶œí•œ `concept` ê°’ì„ ê°€ì ¸ì˜´
- `KeyIdea.core_idea_text == concept` ì¿¼ë¦¬ë¡œ DB ê²€ìƒ‰
- ë™ì¼í•œ ë¬¸ìì—´ì´ ìˆìœ¼ë©´ ì¤‘ë³µ â†’ ìŠ¤í‚µ

#### KeyIdea ì €ì¥ ë¡œì§

```python
# src/workflow/nodes/save_to_db.py

def save_to_db(state: PipelineState) -> PipelineState:
    # concept ì¶”ì¶œ
    concept = extracted_idea.concept

    # KeyIdea ì €ì¥
    db_idea = KeyIdea(
        chunk_id=db_chunk.id,
        book_id=book_id,
        core_idea_text=concept,  # LLMì´ ì¶”ì¶œí•œ concept ì €ì¥
    )
```

#### ì™œ conceptì€ ë¬¸ìì—´ ë§¤ì¹­ì¸ê°€?

| íŠ¹ì„± | ì„¤ëª… |
|------|------|
| **conceptì˜ ì„±ê²©** | LLMì´ ì •ì œí•œ ì§§ì€ í•µì‹¬ ê°œë… (ì˜ˆ: `"Self-attention mechanism"`) |
| **ì¤‘ë³µ ë°œìƒ ì‹œë‚˜ë¦¬ì˜¤** | ê°™ì€ ì±…ì—ì„œ ê°™ì€ conceptì´ ì—¬ëŸ¬ ë²ˆ ë‚˜ì˜¬ ìˆ˜ ìˆìŒ |
| **ì›í•˜ëŠ” ë™ì‘** | **ì •í™•íˆ ê°™ì€ ê°œë…**ë§Œ ìŠ¤í‚µí•˜ê³  ì‹¶ìŒ |
| **ì™œ ë¬¸ìì—´ ë§¤ì¹­?** | ì˜ë¯¸ì ìœ¼ë¡œ ë¹„ìŠ·í•˜ì§€ë§Œ ë‹¤ë¥¸ ê°œë…ì€ ì €ì¥í•´ì•¼ í•¨ |

**ì˜ˆì‹œ:**
```
concept1: "Transformer"
concept2: "Transformer architecture"
â†’ ë‹¤ë¥¸ ë¬¸ìì—´ â†’ ë‘˜ ë‹¤ ì €ì¥ âœ…

concept1: "Self-attention"
concept2: "Self-attention"
â†’ ë™ì¼ ë¬¸ìì—´ â†’ ì²« ë²ˆì§¸ë§Œ ì €ì¥, ë‘ ë²ˆì§¸ëŠ” ìŠ¤í‚µ
```

**ì™œ ì„ë² ë”©ì´ ì•„ë‹Œê°€?**
- ì„ë² ë”©ìœ¼ë¡œ ë¹„êµí•˜ë©´ `"Transformer"`ì™€ `"Transformer architecture"`ê°€ ìœ ì‚¬í•˜ë‹¤ê³  íŒë‹¨ë  ìˆ˜ ìˆìŒ
- í•˜ì§€ë§Œ ì´ ë‘ ê°œë…ì€ ì„œë¡œ ë‹¤ë¥¸ ê´€ì ì„ ë‹´ê³  ìˆì„ ìˆ˜ ìˆìœ¼ë¯€ë¡œ **ë‘˜ ë‹¤ ì €ì¥**í•˜ëŠ” ê²ƒì´ ì ì ˆ

---

## ì™œ í•´ì‹œì™€ ì„ë² ë”©ì„ í•¨ê»˜ ì‚¬ìš©í•˜ëŠ”ê°€?

### Q: ì„ë² ë”©ì´ ìˆìœ¼ë©´ í•´ì‹œê°€ í•„ìš” ì—†ì§€ ì•Šë‚˜?

A: ì„ë² ë”©ë§Œìœ¼ë¡œë„ ì¤‘ë³µ íƒì§€ê°€ ê°€ëŠ¥í•˜ì§€ë§Œ, **ë¹„ìš©ê³¼ ì†ë„** ë¬¸ì œê°€ ìˆìŠµë‹ˆë‹¤.

| ë°©ì‹ | ì†ë„ | ë¹„ìš© | íƒì§€ ë²”ìœ„ |
|-----|------|------|----------|
| SHA256 | ì¦‰ì‹œ (Î¼s) | ë¬´ë£Œ | ì™„ì „ ë™ì¼ |
| SimHash | ì¦‰ì‹œ (Î¼s) | ë¬´ë£Œ | ê¸€ì ë³€í˜• |
| ì„ë² ë”© | ëŠë¦¼ (~200ms) | $0.00002/1K tokens | ì˜ë¯¸ì  ìœ ì‚¬ |

**100ê°œ ì²­í¬ ì²˜ë¦¬ ì‹œ:**
- ì„ë² ë”©ë§Œ: 100 API í˜¸ì¶œ â†’ ~20ì´ˆ, ~$0.004
- í•´ì‹œ+ì„ë² ë”©: í•´ì‹œì—ì„œ 80ê°œ ìŠ¤í‚µ â†’ 20 API í˜¸ì¶œ â†’ ~4ì´ˆ, ~$0.0008

â†’ **í•´ì‹œ = ë¹ ë¥¸ 1ì°¨ í•„í„°, ì„ë² ë”© = ì •ë°€ 2ì°¨ í•„í„°**

### Q: í•´ì‹œë¡œ ì¤‘ë³µì´ë¼ íŒë‹¨ëœ ê²ƒì„ ì„ë² ë”©ìœ¼ë¡œ ì¬í™•ì¸í•˜ë©´ ì•ˆ ë˜ë‚˜?

A: ê·¸ê²ƒë„ ê°€ëŠ¥í•˜ì§€ë§Œ, **ëª©ì ì´ ë‹¤ë¦…ë‹ˆë‹¤.**

| ë°©í–¥ | íë¦„ | ëª©ì  | ê²°ê³¼ |
|-----|------|------|------|
| **í˜„ì¬ ê¸°íš** | í•´ì‹œ í†µê³¼ â†’ ì„ë² ë”© ì²´í¬ | í•´ì‹œê°€ ë†“ì¹œ ì¤‘ë³µ ì¡ê¸° | ë” ë§ì´ ìŠ¤í‚µ (ì—„ê²©) |
| ëŒ€ì•ˆ | í•´ì‹œ ì¤‘ë³µ â†’ ì„ë² ë”© ì¬í™•ì¸ | í•´ì‹œ ì˜¤íƒ ë°©ì§€ | ë” ë§ì´ ì €ì¥ (ê´€ëŒ€) |

**í˜„ì¬ ê¸°íšì„ ì„ íƒí•œ ì´ìœ :**
- í•´ì‹œ(SHA256)ëŠ” ì •í™•í•¨ â†’ ì˜¤íƒ(false positive)ì´ ê±°ì˜ ì—†ìŒ
- SimHashëŠ” ì˜¤íƒ ê°€ëŠ¥ì„± ìˆì§€ë§Œ, threshold=6ì´ë©´ ìƒë‹¹íˆ ìœ ì‚¬í•œ ê²½ìš°ë§Œ íƒì§€
- ë”°ë¼ì„œ **í•´ì‹œ ì˜¤íƒ ë°©ì§€ë³´ë‹¤ í•´ì‹œê°€ ë†“ì¹œ ì˜ë¯¸ì  ì¤‘ë³µ ì¡ê¸°**ê°€ ë” ìœ ìš©

### Q: SimHashë„ ìœ ì‚¬ë„ë¥¼ ë³´ëŠ”ë° ì„ë² ë”©ì´ ì™œ í•„ìš”í•œê°€?

A: SimHashëŠ” **ê¸€ì ìˆ˜ì¤€** ìœ ì‚¬ë„, ì„ë² ë”©ì€ **ì˜ë¯¸ ìˆ˜ì¤€** ìœ ì‚¬ë„ì…ë‹ˆë‹¤.

```
ì˜ˆì‹œ 1: SimHashê°€ ì¡ëŠ” ê²½ìš°
  "TransformerëŠ” attentionì„ ì‚¬ìš©í•œë‹¤"
  "TransformerëŠ” attentionì„ í™œìš©í•œë‹¤"
  â†’ ê¸€ì ëŒ€ë¶€ë¶„ ë™ì¼ â†’ SimHash ê±°ë¦¬ ì‘ìŒ âœ…

ì˜ˆì‹œ 2: ì„ë² ë”©ë§Œ ì¡ëŠ” ê²½ìš°
  "TransformerëŠ” attention ë©”ì»¤ë‹ˆì¦˜ì„ ì‚¬ìš©í•œë‹¤"
  "Attention mechanismì´ Transformerì˜ í•µì‹¬ì´ë‹¤"
  â†’ ê¸€ì ìˆœì„œ ë‹¤ë¦„ â†’ SimHash ê±°ë¦¬ í¼ âŒ
  â†’ ì˜ë¯¸ ë™ì¼ â†’ ì„ë² ë”© ìœ ì‚¬ë„ ë†’ìŒ (0.92+) âœ…
```

---

## ì‚¬ìš© ë°©ë²•

### ê¸°ë³¸ ì‚¬ìš© (í•´ì‹œë§Œ)

```python
from src.workflow.workflow import run_pdf_pipeline

# í•´ì‹œë§Œ ì‚¬ìš© (ê¸°ë³¸, ë¹ ë¦„)
run_pdf_pipeline("book.pdf")
```

### ì„ë² ë”© ê¸°ë°˜ ì¤‘ë³µì œê±° í™œì„±í™”

```python
# í•´ì‹œ + ì„ë² ë”© ì‚¬ìš© (ì •ë°€, API ë¹„ìš© ë°œìƒ)
run_pdf_pipeline("book.pdf", enable_semantic_dedup=True)

# ì„ê³„ê°’ ì¡°ì • (ë” ê´€ëŒ€í•œ ì¤‘ë³µ íƒì§€)
run_pdf_pipeline("book.pdf", enable_semantic_dedup=True, semantic_threshold=0.90)
```

### ë°°ì¹˜ ì„ë² ë”© ìƒì„±

```bash
python scripts/generate_embeddings.py --book-id 1 --batch-size 100
```

---

## í†µê³„ í•„ë“œ

```python
stats = {
    # ê¸°ì¡´
    "chunk_duplicates_skipped": 0,   # í•´ì‹œ ê¸°ë°˜
    "idea_duplicates_skipped": 0,    # concept ê¸°ë°˜

    # ì‹ ê·œ (ì„ë² ë”© í™œì„±í™” ì‹œ)
    "semantic_duplicates_skipped": 0,  # ì„ë² ë”© ê¸°ë°˜
}
```

### ì¶œë ¥ ì˜ˆì‹œ

```
============================================================
ğŸ“Š ì²˜ë¦¬ ìš”ì•½
============================================================
ê°ì§€ ë°©ë²•: toc
ì´ ì±•í„°: 10
ì´ ì„¹ì…˜: 45
ì™„ë£Œ: 10
ì‹¤íŒ¨: 0
ì´ ë¬¸ë‹¨: 320
ì¶”ì¶œëœ ì•„ì´ë””ì–´: 280
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
ì¤‘ë³µ ìŠ¤í‚µ ìƒì„¸:
  í•´ì‹œ ê¸°ë°˜: 15
  ì„ë² ë”© ê¸°ë°˜: 8
  ì•„ì´ë””ì–´ ê¸°ë°˜: 17
  ì´ ìŠ¤í‚µ: 40
============================================================
```

---

## DB ì¸í”„ë¼

### í•„ìš”í•œ í…Œì´ë¸”

```sql
-- paragraph_chunks í…Œì´ë¸” (í•´ì‹œ í•„ë“œ ì¶”ê°€ë¨)
ALTER TABLE paragraph_chunks ADD COLUMN paragraph_hash VARCHAR(64);
ALTER TABLE paragraph_chunks ADD COLUMN simhash64 BIGINT;

-- paragraph_embeddings í…Œì´ë¸” (pgvector í•„ìš”)
CREATE TABLE paragraph_embeddings (
    id SERIAL PRIMARY KEY,
    chunk_id INTEGER REFERENCES paragraph_chunks(id) UNIQUE,
    book_id INTEGER REFERENCES books(id),
    embedding vector(1536),
    model_name VARCHAR(100) DEFAULT 'text-embedding-3-small'
);

-- pgvector ì¸ë±ìŠ¤
CREATE INDEX ON paragraph_embeddings
USING ivfflat (embedding vector_cosine_ops) WITH (lists = 100);
```

### ì™œ PostgreSQL + pgvectorê°€ í•„ìš”í•œê°€?

| ê¸°ëŠ¥ | SQLite | PostgreSQL + pgvector |
|------|--------|----------------------|
| í•´ì‹œ ì¤‘ë³µì²´í¬ | âœ… ê°€ëŠ¥ | âœ… ê°€ëŠ¥ |
| ì„ë² ë”© ì €ì¥ | âŒ ë¶ˆê°€ (vector íƒ€ì… ì—†ìŒ) | âœ… `vector(1536)` íƒ€ì… |
| ì½”ì‚¬ì¸ ìœ ì‚¬ë„ ê²€ìƒ‰ | âŒ ë¶ˆê°€ | âœ… `<=>` ì—°ì‚°ì |
| ì„±ëŠ¥ ì¸ë±ìŠ¤ | âŒ | âœ… IVFFlat ì¸ë±ìŠ¤ |

---

## ë¹„ìš© ë¶„ì„

### OpenAI text-embedding-3-small

| í•­ëª© | ê°’ |
|-----|---|
| ê°€ê²© | $0.00002 / 1K tokens |
| í‰ê·  ì²­í¬ | ~200 tokens |
| 1000 ì²­í¬ | ~$0.004 |
| ì±… 1ê¶Œ (500ì²­í¬) | ~$0.002 |

**ê²°ë¡ :** ë°°ì¹˜ ì„ë² ë”© ìƒì„± ë¹„ìš©ì€ ë¬´ì‹œí•  ìˆ˜ì¤€

---

## ê°œë°œ í™˜ê²½ ì„¤ì •

### 1. ê°€ìƒí™˜ê²½ ì„¤ì •

```bash
# ê°€ìƒí™˜ê²½ ìƒì„±
python3 -m venv venv

# ê°€ìƒí™˜ê²½ í™œì„±í™”
source venv/bin/activate

# ì˜ì¡´ì„± ì„¤ì¹˜
pip install -r requirements.txt
```

### 2. í™˜ê²½ë³€ìˆ˜ ì„¤ì • (.env)

```
DATABASE_URL=postgresql://user:password@localhost:5433/pdf_extractor
OPENAI_API_KEY=sk-...
```

### 3. PostgreSQL ì„œë²„ ì‹¤í–‰

#### Docker ì‚¬ìš© (ê¶Œì¥)

```bash
docker run -d \
  --name pdf-extractor-db \
  -e POSTGRES_PASSWORD=yourpassword \
  -e POSTGRES_DB=pdf_extractor \
  -p 5433:5432 \
  pgvector/pgvector:pg16
```

#### macOS ë¡œì»¬ ì„¤ì¹˜

```bash
brew install postgresql@16
brew services start postgresql@16
brew install pgvector
```

#### í´ë¼ìš°ë“œ DB

- Supabase, Neon, Railway ë“±ì—ì„œ ë¬´ë£Œ PostgreSQL ì œê³µ
- pgvector ì§€ì›ë˜ëŠ” ì„œë¹„ìŠ¤ ì„ íƒ

### 4. í…ŒìŠ¤íŠ¸ ì‹¤í–‰

```bash
source venv/bin/activate
python3 tests/test_deduplication.py
```

---

## í…ŒìŠ¤íŠ¸ ê²°ê³¼

| í…ŒìŠ¤íŠ¸ | ì„œë²„ ì—†ì´ | ì„œë²„ ìˆì„ ë•Œ |
|--------|----------|-------------|
| hash_utils | âœ… í†µê³¼ | âœ… í†µê³¼ |
| embedding_utils | âœ… í†µê³¼ | âœ… í†µê³¼ |
| pipeline_state | âœ… í†µê³¼ | âœ… í†µê³¼ |
| workflow_options | âœ… í†µê³¼ | âœ… í†µê³¼ |
| dedup_service | â­ï¸ ìŠ¤í‚µ | í…ŒìŠ¤íŠ¸ ê°€ëŠ¥ |
| workflow_integration | â­ï¸ ìŠ¤í‚µ | í…ŒìŠ¤íŠ¸ ê°€ëŠ¥ |

ì„œë²„ ì—†ì´ë„ **í•µì‹¬ ë¡œì§(í•´ì‹œ, ì„ë² ë”©, íŒŒì´í”„ë¼ì¸ êµ¬ì¡°)**ì€ ëª¨ë‘ í…ŒìŠ¤íŠ¸ í†µê³¼í•©ë‹ˆë‹¤.

---

## í…ŒìŠ¤íŠ¸ ì²´í¬ë¦¬ìŠ¤íŠ¸

- [x] hash_utils: SHA256 ê²°ì •ë¡ ì  í™•ì¸
- [x] hash_utils: SimHash í•´ë° ê±°ë¦¬ ê³„ì‚°
- [x] check_chunk_duplicate ë…¸ë“œ: ì •í™•/í¼ì§€ ì¤‘ë³µ íƒì§€
- [x] check_idea_duplicate ë…¸ë“œ: concept ì •í™• ë§¤ì¹­
- [x] PipelineState: ìƒˆ í•„ë“œ ì¡´ì¬ í™•ì¸
- [x] run_pdf_pipeline: ì˜µì…˜ íŒŒë¼ë¯¸í„° í™•ì¸
- [ ] generate_embeddings.py: íŠ¹ì • book_id ì„ë² ë”© ìƒì„± (DB í•„ìš”)
- [ ] pgvector ì¿¼ë¦¬: ì½”ì‚¬ì¸ ìœ ì‚¬ë„ ê³„ì‚° ì •í™•ì„± (DB í•„ìš”)
- [ ] dedup_service: enable_semantic=True ë™ì‘ í™•ì¸ (DB í•„ìš”)

---

## ë²„ê·¸ ìˆ˜ì • ì´ë ¥

### hamming_distance() í•¨ìˆ˜ ë²„ê·¸ (2024-12-27)

**ìˆ˜ì • ì „ (ë²„ê·¸):**
```python
def hamming_distance(hash1: int, hash2: int) -> int:
    xor = xor & 0xFFFFFFFFFFFFFFFF  # xor ì •ì˜ ì•ˆë¨!
    return bin(xor).count('1')
```

**ìˆ˜ì • í›„:**
```python
def hamming_distance(hash1: int, hash2: int) -> int:
    xor = hash1 ^ hash2  # ì¶”ê°€ë¨
    xor = xor & 0xFFFFFFFFFFFFFFFF
    return bin(xor).count('1')
```

---

## ì½”ë“œ ì´í•´ë¥¼ ìœ„í•œ Python ë¬¸ë²•

ì´ í”„ë¡œì íŠ¸ì—ì„œ ìì£¼ ì‚¬ìš©ë˜ëŠ” Python íŒ¨í„´ë“¤ì„ ì •ë¦¬í•©ë‹ˆë‹¤.

### ë”•ì…”ë„ˆë¦¬ ì–¸íŒ¨í‚¹ (`{**state, ...}`)

```python
state = {"a": 1, "b": 2, "book_id": 123}

# ìƒˆ ë”•ì…”ë„ˆë¦¬ ìƒì„± (ì›ë³¸ ìœ ì§€ + ìƒˆ í‚¤ ì¶”ê°€)
result = {**state, "is_chunk_duplicate": False}
# â†’ {"a": 1, "b": 2, "book_id": 123, "is_chunk_duplicate": False}
```

**ì™œ ì´ë ‡ê²Œ í•˜ë‚˜?**

LangGraph ë…¸ë“œëŠ” **ë¶ˆë³€ì„±(Immutability)**ì„ ìœ ì§€í•´ì•¼ í•©ë‹ˆë‹¤:

```python
# âŒ ì›ë³¸ ìˆ˜ì • (side effect ë°œìƒ)
state["is_chunk_duplicate"] = False
return state

# âœ… ìƒˆ ë”•ì…”ë„ˆë¦¬ ìƒì„± (ì›ë³¸ ìœ ì§€)
return {**state, "is_chunk_duplicate": False}
```

---

### ê°ì²´ vs ë”•ì…”ë„ˆë¦¬

```python
# ë”•ì…”ë„ˆë¦¬ (dict) - í‚¤-ê°’ ìŒì˜ ìë£Œêµ¬ì¡°
chunk_dict = {"text": "TransformerëŠ”...", "chapter_id": 1}
chunk_dict["text"]      # í‚¤ë¡œ ì ‘ê·¼
chunk_dict.get("text")  # í‚¤ë¡œ ì ‘ê·¼ (ì—†ìœ¼ë©´ None ë°˜í™˜)

# ê°ì²´ (class) - ì†ì„±ì„ ê°€ì§„ ì¸ìŠ¤í„´ìŠ¤
class HierarchicalChunk:
    def __init__(self, text, chapter_id):
        self.text = text
        self.chapter_id = chapter_id

chunk_obj = HierarchicalChunk(text="TransformerëŠ”...", chapter_id=1)
chunk_obj.text  # ì†ì„±ìœ¼ë¡œ ì ‘ê·¼ (ì  í‘œê¸°ë²•)
```

| êµ¬ë¶„ | ë”•ì…”ë„ˆë¦¬ | ê°ì²´ |
|------|----------|------|
| ìƒì„± | `{"key": value}` | `ClassName(...)` |
| ì ‘ê·¼ | `d["key"]` ë˜ëŠ” `d.get("key")` | `obj.attribute` |
| íƒ€ì… | `dict` | í´ë˜ìŠ¤ëª… (ì˜ˆ: `HierarchicalChunk`) |

---

### hasattr í•¨ìˆ˜

```python
hasattr(ê°ì²´, "ì†ì„±ëª…")  # í•´ë‹¹ ì†ì„±ì´ ìˆìœ¼ë©´ True, ì—†ìœ¼ë©´ False
```

```python
chunk_obj = HierarchicalChunk(text="...", chapter_id=1)
hasattr(chunk_obj, "text")       # True - text ì†ì„± ìˆìŒ
hasattr(chunk_obj, "author")     # False - author ì†ì„± ì—†ìŒ

chunk_dict = {"text": "...", "chapter_id": 1}
hasattr(chunk_dict, "text")      # False - ë”•ì…”ë„ˆë¦¬ëŠ” .text ì†ì„±ì´ ì—†ìŒ
```

---

### get í•¨ìˆ˜ (ë”•ì…”ë„ˆë¦¬)

```python
ë”•ì…”ë„ˆë¦¬.get("í‚¤", ê¸°ë³¸ê°’)  # í‚¤ê°€ ìˆìœ¼ë©´ ê°’ ë°˜í™˜, ì—†ìœ¼ë©´ ê¸°ë³¸ê°’ ë°˜í™˜
```

```python
chunk_dict = {"text": "TransformerëŠ”...", "chapter_id": 1}

chunk_dict.get("text", "")       # â†’ "TransformerëŠ”..."
chunk_dict.get("author", "ì—†ìŒ")  # â†’ "ì—†ìŒ" (í‚¤ê°€ ì—†ì–´ì„œ ê¸°ë³¸ê°’)
chunk_dict["author"]             # â†’ KeyError ë°œìƒ! (í‚¤ ì—†ìŒ)
```

---

### ìœ ì—°í•œ íƒ€ì… ì²˜ë¦¬ íŒ¨í„´

```python
text = current_chunk.text if hasattr(current_chunk, 'text') else current_chunk.get('text', '')
```

í’€ì–´ì„œ ì“°ë©´:

```python
if hasattr(current_chunk, 'text'):
    # current_chunkê°€ ê°ì²´ì¸ ê²½ìš°
    text = current_chunk.text
else:
    # current_chunkê°€ ë”•ì…”ë„ˆë¦¬ì¸ ê²½ìš°
    text = current_chunk.get('text', '')
```

**ì™œ ë‘ ê°€ì§€ íƒ€ì…ì„ ëª¨ë‘ ì²˜ë¦¬í•˜ë‚˜?**

`current_chunk`ê°€ **ë‘ ê°€ì§€ í˜•íƒœ**ë¡œ ë“¤ì–´ì˜¬ ìˆ˜ ìˆê¸° ë•Œë¬¸:

```python
# ê°ì²´ë¡œ ì „ë‹¬ë˜ëŠ” ê²½ìš°
state["current_chunk"] = HierarchicalChunk(text="...", chapter_id=1)

# ë”•ì…”ë„ˆë¦¬ë¡œ ì „ë‹¬ë˜ëŠ” ê²½ìš°
state["current_chunk"] = {"text": "...", "chapter_id": 1}
```

---

## íƒ€ì… êµ¬ì¡° ì •ë¦¬

### ë…¸ë“œ í•¨ìˆ˜ì˜ ì…ì¶œë ¥

```python
def check_chunk_duplicate(state: PipelineState) -> PipelineState:
```

| ì—­í•  | íƒ€ì… | ì„¤ëª… |
|------|------|------|
| ì…ë ¥ | `PipelineState` | íŒŒì´í”„ë¼ì¸ ì „ì²´ ìƒíƒœ (ë”•ì…”ë„ˆë¦¬) |
| ì¶œë ¥ | `PipelineState` | ì—…ë°ì´íŠ¸ëœ ìƒíƒœ (ë”•ì…”ë„ˆë¦¬) |

### PipelineState êµ¬ì¡°

```python
state = {
    "current_chunk": HierarchicalChunk(...),  # í˜„ì¬ ì²˜ë¦¬ ì¤‘ì¸ ì²­í¬
    "book_id": 123,
    "enable_semantic_dedup": False,
    "stats": {"chunk_duplicates_skipped": 0},
    "extracted_idea": ExtractedIdea(...),     # LLM ì¶”ì¶œ ê²°ê³¼
    ...
}
```

### íƒ€ì… íë¦„ë„

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ PipelineState (ì „ì²´ ìƒíƒœ ë”•ì…”ë„ˆë¦¬)                        â”‚
â”‚   â”œâ”€â”€ current_chunk: HierarchicalChunk (ì²­í¬ ê°ì²´)       â”‚
â”‚   â”œâ”€â”€ book_id: int                                      â”‚
â”‚   â”œâ”€â”€ extracted_idea: ExtractedIdea (LLM ê²°ê³¼)          â”‚
â”‚   â””â”€â”€ stats: dict                                       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                          â”‚
                          â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ DeduplicationService (ì„œë¹„ìŠ¤ í´ë˜ìŠ¤)                     â”‚
â”‚   â””â”€â”€ check_duplicate(text) â†’ DeduplicationResult       â”‚
â”‚                                  â”œâ”€â”€ is_duplicate: bool â”‚
â”‚                                  â””â”€â”€ duplicate_type: strâ”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### ê° íƒ€ì…ì˜ ì—­í• 

| íƒ€ì… | ì •ì˜ ìœ„ì¹˜ | ì—­í•  |
|------|----------|------|
| `PipelineState` | `state.py` | ë…¸ë“œ ê°„ ë°ì´í„° ì „ë‹¬ (ë”•ì…”ë„ˆë¦¬) |
| `HierarchicalChunk` | `schemas.py` | ë¬¸ë‹¨ ì²­í¬ ë°ì´í„° (dataclass) |
| `ExtractedIdea` | `schemas.py` | LLM ì¶”ì¶œ ê²°ê³¼ (Pydantic) |
| `DeduplicationService` | `dedup_service.py` | ì¤‘ë³µì²´í¬ ë¡œì§ (í´ë˜ìŠ¤) |
| `DeduplicationResult` | `dedup_service.py` | ì¤‘ë³µì²´í¬ ê²°ê³¼ (dataclass) |

### DeduplicationResult êµ¬ì¡°

```python
@dataclass
class DeduplicationResult:
    """ì¤‘ë³µ ì²´í¬ ê²°ê³¼"""
    is_duplicate: bool                        # ì¤‘ë³µì¸ì§€ ì—¬ë¶€
    duplicate_type: Optional[str] = None      # 'exact', 'fuzzy', 'semantic'
    existing_chunk_id: Optional[int] = None   # ì¤‘ë³µëœ ê¸°ì¡´ ì²­í¬ ID
    similarity_score: Optional[float] = None  # ìœ ì‚¬ë„ ì ìˆ˜
    hamming_distance: Optional[int] = None    # SimHash í•´ë° ê±°ë¦¬
```

### ê°„ë‹¨ ìš”ì•½

```
PipelineState    â† ì „ì²´ ìƒíƒœë¥¼ ë‹´ëŠ” í° ë°”êµ¬ë‹ˆ (dict)
  â†“
current_chunk    â† ë°”êµ¬ë‹ˆì—ì„œ êº¼ë‚¸ ì²­í¬ (ê°ì²´)
  â†“
text             â† ì²­í¬ì—ì„œ êº¼ë‚¸ í…ìŠ¤íŠ¸ (str)
  â†“
DeduplicationService.check_duplicate(text)
  â†“
DeduplicationResult â† ì¤‘ë³µ ì²´í¬ ê²°ê³¼ (ê°ì²´)
  â”œâ”€â”€ is_duplicate: True/False
  â””â”€â”€ duplicate_type: "exact"/"fuzzy"/"semantic"
```
